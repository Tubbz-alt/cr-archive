<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="jvmciCodeInstaller_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.</span>
<span class="line-modified">   3  * Copyright (c) 2014, 2019, Red Hat Inc. All rights reserved.</span>
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &lt;sys/types.h&gt;
  27 
  28 #include &quot;precompiled.hpp&quot;
  29 #include &quot;jvm.h&quot;
  30 #include &quot;asm/assembler.hpp&quot;
  31 #include &quot;asm/assembler.inline.hpp&quot;
  32 #include &quot;gc/shared/barrierSet.hpp&quot;
  33 #include &quot;gc/shared/cardTable.hpp&quot;
  34 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  35 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
  36 #include &quot;interpreter/interpreter.hpp&quot;
  37 #include &quot;compiler/disassembler.hpp&quot;
  38 #include &quot;memory/resourceArea.hpp&quot;

  39 #include &quot;nativeInst_aarch64.hpp&quot;
  40 #include &quot;oops/accessDecorators.hpp&quot;
  41 #include &quot;oops/compressedOops.inline.hpp&quot;
  42 #include &quot;oops/klass.inline.hpp&quot;
  43 #include &quot;runtime/biasedLocking.hpp&quot;
  44 #include &quot;runtime/icache.hpp&quot;
  45 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  46 #include &quot;runtime/jniHandles.inline.hpp&quot;
  47 #include &quot;runtime/sharedRuntime.hpp&quot;
  48 #include &quot;runtime/thread.hpp&quot;

  49 #ifdef COMPILER1
  50 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  51 #endif
  52 #ifdef COMPILER2
  53 #include &quot;oops/oop.hpp&quot;
  54 #include &quot;opto/compile.hpp&quot;
  55 #include &quot;opto/intrinsicnode.hpp&quot;
  56 #include &quot;opto/node.hpp&quot;
  57 #endif
  58 
  59 #ifdef PRODUCT
  60 #define BLOCK_COMMENT(str) /* nothing */
  61 #define STOP(error) stop(error)
  62 #else
  63 #define BLOCK_COMMENT(str) block_comment(str)
  64 #define STOP(error) block_comment(error); stop(error)
  65 #endif
  66 
  67 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  68 
</pre>
<hr />
<pre>
 454     should_not_reach_here();
 455 
 456     bind(no_reserved_zone_enabling);
 457 }
 458 
 459 int MacroAssembler::biased_locking_enter(Register lock_reg,
 460                                          Register obj_reg,
 461                                          Register swap_reg,
 462                                          Register tmp_reg,
 463                                          bool swap_reg_contains_mark,
 464                                          Label&amp; done,
 465                                          Label* slow_case,
 466                                          BiasedLockingCounters* counters) {
 467   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 468   assert_different_registers(lock_reg, obj_reg, swap_reg);
 469 
 470   if (PrintBiasedLockingStatistics &amp;&amp; counters == NULL)
 471     counters = BiasedLocking::counters();
 472 
 473   assert_different_registers(lock_reg, obj_reg, swap_reg, tmp_reg, rscratch1, rscratch2, noreg);
<span class="line-modified"> 474   assert(markOopDesc::age_shift == markOopDesc::lock_bits + markOopDesc::biased_lock_bits, &quot;biased locking makes assumptions about bit layout&quot;);</span>
 475   Address mark_addr      (obj_reg, oopDesc::mark_offset_in_bytes());
 476   Address klass_addr     (obj_reg, oopDesc::klass_offset_in_bytes());
 477   Address saved_mark_addr(lock_reg, 0);
 478 
 479   // Biased locking
 480   // See whether the lock is currently biased toward our thread and
 481   // whether the epoch is still valid
 482   // Note that the runtime guarantees sufficient alignment of JavaThread
 483   // pointers to allow age to be placed into low bits
 484   // First check to see whether biasing is even enabled for this object
 485   Label cas_label;
 486   int null_check_offset = -1;
 487   if (!swap_reg_contains_mark) {
 488     null_check_offset = offset();
 489     ldr(swap_reg, mark_addr);
 490   }
<span class="line-modified"> 491   andr(tmp_reg, swap_reg, markOopDesc::biased_lock_mask_in_place);</span>
<span class="line-modified"> 492   cmp(tmp_reg, (u1)markOopDesc::biased_lock_pattern);</span>
 493   br(Assembler::NE, cas_label);
 494   // The bias pattern is present in the object&#39;s header. Need to check
 495   // whether the bias owner and the epoch are both still current.
 496   load_prototype_header(tmp_reg, obj_reg);
 497   orr(tmp_reg, tmp_reg, rthread);
 498   eor(tmp_reg, swap_reg, tmp_reg);
<span class="line-modified"> 499   andr(tmp_reg, tmp_reg, ~((int) markOopDesc::age_mask_in_place));</span>
 500   if (counters != NULL) {
 501     Label around;
 502     cbnz(tmp_reg, around);
 503     atomic_incw(Address((address)counters-&gt;biased_lock_entry_count_addr()), tmp_reg, rscratch1, rscratch2);
 504     b(done);
 505     bind(around);
 506   } else {
 507     cbz(tmp_reg, done);
 508   }
 509 
 510   Label try_revoke_bias;
 511   Label try_rebias;
 512 
 513   // At this point we know that the header has the bias pattern and
 514   // that we are not the bias owner in the current epoch. We need to
 515   // figure out more details about the state of the header in order to
 516   // know what operations can be legally performed on the object&#39;s
 517   // header.
 518 
 519   // If the low three bits in the xor result aren&#39;t clear, that means
 520   // the prototype header is no longer biased and we have to revoke
 521   // the bias on this object.
<span class="line-modified"> 522   andr(rscratch1, tmp_reg, markOopDesc::biased_lock_mask_in_place);</span>
 523   cbnz(rscratch1, try_revoke_bias);
 524 
 525   // Biasing is still enabled for this data type. See whether the
 526   // epoch of the current bias is still valid, meaning that the epoch
 527   // bits of the mark word are equal to the epoch bits of the
 528   // prototype header. (Note that the prototype header&#39;s epoch bits
 529   // only change at a safepoint.) If not, attempt to rebias the object
 530   // toward the current thread. Note that we must be absolutely sure
 531   // that the current epoch is invalid in order to do this because
 532   // otherwise the manipulations it performs on the mark word are
 533   // illegal.
<span class="line-modified"> 534   andr(rscratch1, tmp_reg, markOopDesc::epoch_mask_in_place);</span>
 535   cbnz(rscratch1, try_rebias);
 536 
 537   // The epoch of the current bias is still valid but we know nothing
 538   // about the owner; it might be set or it might be clear. Try to
 539   // acquire the bias of the object using an atomic operation. If this
 540   // fails we will go in to the runtime to revoke the object&#39;s bias.
 541   // Note that we first construct the presumed unbiased header so we
 542   // don&#39;t accidentally blow away another thread&#39;s valid bias.
 543   {
 544     Label here;
<span class="line-modified"> 545     mov(rscratch1, markOopDesc::biased_lock_mask_in_place | markOopDesc::age_mask_in_place | markOopDesc::epoch_mask_in_place);</span>
 546     andr(swap_reg, swap_reg, rscratch1);
 547     orr(tmp_reg, swap_reg, rthread);
 548     cmpxchg_obj_header(swap_reg, tmp_reg, obj_reg, rscratch1, here, slow_case);
 549     // If the biasing toward our thread failed, this means that
 550     // another thread succeeded in biasing it toward itself and we
 551     // need to revoke that bias. The revocation will occur in the
 552     // interpreter runtime in the slow case.
 553     bind(here);
 554     if (counters != NULL) {
 555       atomic_incw(Address((address)counters-&gt;anonymously_biased_lock_entry_count_addr()),
 556                   tmp_reg, rscratch1, rscratch2);
 557     }
 558   }
 559   b(done);
 560 
 561   bind(try_rebias);
 562   // At this point we know the epoch has expired, meaning that the
 563   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
 564   // circumstances _only_, we are allowed to use the current header&#39;s
 565   // value as the comparison value when doing the cas to acquire the
</pre>
<hr />
<pre>
 610                   rscratch1, rscratch2);
 611     }
 612     bind(nope);
 613   }
 614 
 615   bind(cas_label);
 616 
 617   return null_check_offset;
 618 }
 619 
 620 void MacroAssembler::biased_locking_exit(Register obj_reg, Register temp_reg, Label&amp; done) {
 621   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 622 
 623   // Check for biased locking unlock case, which is a no-op
 624   // Note: we do not have to check the thread ID for two reasons.
 625   // First, the interpreter checks for IllegalMonitorStateException at
 626   // a higher level. Second, if the bias was revoked while we held the
 627   // lock, the object could not be rebiased toward another thread, so
 628   // the bias bit would be clear.
 629   ldr(temp_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
<span class="line-modified"> 630   andr(temp_reg, temp_reg, markOopDesc::biased_lock_mask_in_place);</span>
<span class="line-modified"> 631   cmp(temp_reg, (u1)markOopDesc::biased_lock_pattern);</span>
 632   br(Assembler::EQ, done);
 633 }
 634 
 635 static void pass_arg0(MacroAssembler* masm, Register arg) {
 636   if (c_rarg0 != arg ) {
 637     masm-&gt;mov(c_rarg0, arg);
 638   }
 639 }
 640 
 641 static void pass_arg1(MacroAssembler* masm, Register arg) {
 642   if (c_rarg1 != arg ) {
 643     masm-&gt;mov(c_rarg1, arg);
 644   }
 645 }
 646 
 647 static void pass_arg2(MacroAssembler* masm, Register arg) {
 648   if (c_rarg2 != arg ) {
 649     masm-&gt;mov(c_rarg2, arg);
 650   }
 651 }
</pre>
<hr />
<pre>
 795 
 796   // Now, create the trampoline stub&#39;s code:
 797   // - load the call
 798   // - call
 799   Label target;
 800   ldr(rscratch1, target);
 801   br(rscratch1);
 802   bind(target);
 803   assert(offset() - stub_start_offset == NativeCallTrampolineStub::data_offset,
 804          &quot;should be&quot;);
 805   emit_int64((int64_t)dest);
 806 
 807   const address stub_start_addr = addr_at(stub_start_offset);
 808 
 809   assert(is_NativeCallTrampolineStub_at(stub_start_addr), &quot;doesn&#39;t look like a trampoline&quot;);
 810 
 811   end_a_stub();
 812   return stub_start_addr;
 813 }
 814 












 815 void MacroAssembler::c2bool(Register x) {
 816   // implements x == 0 ? 0 : 1
 817   // note: must only look at least-significant byte of x
 818   //       since C-style booleans are stored in one byte
 819   //       only! (was bug)
 820   tst(x, 0xff);
 821   cset(x, Assembler::NE);
 822 }
 823 
 824 address MacroAssembler::ic_call(address entry, jint method_index) {
 825   RelocationHolder rh = virtual_call_Relocation::spec(pc(), method_index);
 826   // address const_ptr = long_constant((jlong)Universe::non_oop_word());
 827   // unsigned long offset;
 828   // ldr_constant(rscratch2, const_ptr);
 829   movptr(rscratch2, (uintptr_t)Universe::non_oop_word());
 830   return trampoline_call(Address(entry, rh));
 831 }
 832 
 833 // Implementation of call_VM versions
 834 
</pre>
<hr />
<pre>
 942 
 943 void MacroAssembler::check_and_handle_popframe(Register java_thread) { }
 944 
 945 
 946 RegisterOrConstant MacroAssembler::delayed_value_impl(intptr_t* delayed_value_addr,
 947                                                       Register tmp,
 948                                                       int offset) {
 949   intptr_t value = *delayed_value_addr;
 950   if (value != 0)
 951     return RegisterOrConstant(value + offset);
 952 
 953   // load indirectly to solve generation ordering problem
 954   ldr(tmp, ExternalAddress((address) delayed_value_addr));
 955 
 956   if (offset != 0)
 957     add(tmp, tmp, offset);
 958 
 959   return RegisterOrConstant(tmp);
 960 }
 961 
<span class="line-removed"> 962 </span>
<span class="line-removed"> 963 void MacroAssembler:: notify(int type) {</span>
<span class="line-removed"> 964   if (type == bytecode_start) {</span>
<span class="line-removed"> 965     // set_last_Java_frame(esp, rfp, (address)NULL);</span>
<span class="line-removed"> 966     Assembler:: notify(type);</span>
<span class="line-removed"> 967     // reset_last_Java_frame(true);</span>
<span class="line-removed"> 968   }</span>
<span class="line-removed"> 969   else</span>
<span class="line-removed"> 970     Assembler:: notify(type);</span>
<span class="line-removed"> 971 }</span>
<span class="line-removed"> 972 </span>
 973 // Look up the method for a megamorphic invokeinterface call.
 974 // The target method is determined by &lt;intf_klass, itable_index&gt;.
 975 // The receiver klass is in recv_klass.
 976 // On success, the result will be in method_result, and execution falls through.
 977 // On failure, execution transfers to the given label.
 978 void MacroAssembler::lookup_interface_method(Register recv_klass,
 979                                              Register intf_klass,
 980                                              RegisterOrConstant itable_index,
 981                                              Register method_result,
 982                                              Register scan_temp,
 983                                              Label&amp; L_no_such_interface,
 984                          bool return_method) {
 985   assert_different_registers(recv_klass, intf_klass, scan_temp);
 986   assert_different_registers(method_result, intf_klass, scan_temp);
 987   assert(recv_klass != method_result || !return_method,
 988      &quot;recv_klass can be destroyed when method isn&#39;t needed&quot;);
 989   assert(itable_index.is_constant() || itable_index.as_register() == method_result,
 990          &quot;caller must use same register for non-constant itable index as for method&quot;);
 991 
 992   // Compute start of first itableOffsetEntry (which is at the end of the vtable)
</pre>
<hr />
<pre>
1277   // Set NZ/Z based on last compare.
1278   repne_scan(r5, r0, r2, rscratch1);
1279 
1280   // Unspill the temp. registers:
1281   pop(pushed_registers, sp);
1282 
1283   br(Assembler::NE, *L_failure);
1284 
1285   // Success.  Cache the super we found and proceed in triumph.
1286   str(super_klass, super_cache_addr);
1287 
1288   if (L_success != &amp;L_fallthrough) {
1289     b(*L_success);
1290   }
1291 
1292 #undef IS_A_TEMP
1293 
1294   bind(L_fallthrough);
1295 }
1296 





























1297 
1298 void MacroAssembler::verify_oop(Register reg, const char* s) {
1299   if (!VerifyOops) return;
1300 
1301   // Pass register number to verify_oop_subroutine
1302   const char* b = NULL;
1303   {
1304     ResourceMark rm;
1305     stringStream ss;
1306     ss.print(&quot;verify_oop: %s: %s&quot;, reg-&gt;name(), s);
1307     b = code_string(ss.as_string());
1308   }
1309   BLOCK_COMMENT(&quot;verify_oop {&quot;);
1310 
1311   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1312   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1313 
1314   mov(r0, reg);
1315   mov(rscratch1, (address)b);
1316 
</pre>
<hr />
<pre>
1366   // cf. TemplateTable::prepare_invoke(), if (load_receiver).
1367   int stackElementSize = Interpreter::stackElementSize;
1368   int offset = Interpreter::expr_offset_in_bytes(extra_slot_offset+0);
1369 #ifdef ASSERT
1370   int offset1 = Interpreter::expr_offset_in_bytes(extra_slot_offset+1);
1371   assert(offset1 - offset == stackElementSize, &quot;correct arithmetic&quot;);
1372 #endif
1373   if (arg_slot.is_constant()) {
1374     return Address(esp, arg_slot.as_constant() * stackElementSize
1375                    + offset);
1376   } else {
1377     add(rscratch1, esp, arg_slot.as_register(),
1378         ext::uxtx, exact_log2(stackElementSize));
1379     return Address(rscratch1, offset);
1380   }
1381 }
1382 
1383 void MacroAssembler::call_VM_leaf_base(address entry_point,
1384                                        int number_of_arguments,
1385                                        Label *retaddr) {
<span class="line-removed">1386   call_VM_leaf_base1(entry_point, number_of_arguments, 0, ret_type_integral, retaddr);</span>
<span class="line-removed">1387 }</span>
<span class="line-removed">1388 </span>
<span class="line-removed">1389 void MacroAssembler::call_VM_leaf_base1(address entry_point,</span>
<span class="line-removed">1390                                         int number_of_gp_arguments,</span>
<span class="line-removed">1391                                         int number_of_fp_arguments,</span>
<span class="line-removed">1392                                         ret_type type,</span>
<span class="line-removed">1393                                         Label *retaddr) {</span>
1394   Label E, L;
1395 
1396   stp(rscratch1, rmethod, Address(pre(sp, -2 * wordSize)));
1397 
<span class="line-removed">1398   // We add 1 to number_of_arguments because the thread in arg0 is</span>
<span class="line-removed">1399   // not counted</span>
1400   mov(rscratch1, entry_point);
<span class="line-modified">1401   blrt(rscratch1, number_of_gp_arguments + 1, number_of_fp_arguments, type);</span>
1402   if (retaddr)
1403     bind(*retaddr);
1404 
1405   ldp(rscratch1, rmethod, Address(post(sp, 2 * wordSize)));
1406   maybe_isb();
1407 }
1408 
1409 void MacroAssembler::call_VM_leaf(address entry_point, int number_of_arguments) {
1410   call_VM_leaf_base(entry_point, number_of_arguments);
1411 }
1412 
1413 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0) {
1414   pass_arg0(this, arg_0);
1415   call_VM_leaf_base(entry_point, 1);
1416 }
1417 
1418 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1419   pass_arg0(this, arg_0);
1420   pass_arg1(this, arg_1);
1421   call_VM_leaf_base(entry_point, 2);
</pre>
<hr />
<pre>
1703 // not actually be used: you must use the Address that is returned.
1704 // It is up to you to ensure that the shift provided matches the size
1705 // of your data.
1706 Address MacroAssembler::form_address(Register Rd, Register base, long byte_offset, int shift) {
1707   if (Address::offset_ok_for_immed(byte_offset, shift))
1708     // It fits; no need for any heroics
1709     return Address(base, byte_offset);
1710 
1711   // Don&#39;t do anything clever with negative or misaligned offsets
1712   unsigned mask = (1 &lt;&lt; shift) - 1;
1713   if (byte_offset &lt; 0 || byte_offset &amp; mask) {
1714     mov(Rd, byte_offset);
1715     add(Rd, base, Rd);
1716     return Address(Rd);
1717   }
1718 
1719   // See if we can do this with two 12-bit offsets
1720   {
1721     unsigned long word_offset = byte_offset &gt;&gt; shift;
1722     unsigned long masked_offset = word_offset &amp; 0xfff000;
<span class="line-modified">1723     if (Address::offset_ok_for_immed(word_offset - masked_offset)</span>
1724         &amp;&amp; Assembler::operand_valid_for_add_sub_immediate(masked_offset &lt;&lt; shift)) {
1725       add(Rd, base, masked_offset &lt;&lt; shift);
1726       word_offset -= masked_offset;
1727       return Address(Rd, word_offset &lt;&lt; shift);
1728     }
1729   }
1730 
1731   // Do it the hard way
1732   mov(Rd, byte_offset);
1733   add(Rd, base, Rd);
1734   return Address(Rd);
1735 }
1736 
1737 void MacroAssembler::atomic_incw(Register counter_addr, Register tmp, Register tmp2) {
1738   if (UseLSE) {
1739     mov(tmp, 1);
1740     ldadd(Assembler::word, tmp, zr, counter_addr);
1741     return;
1742   }
1743   Label retry_load;
</pre>
<hr />
<pre>
2094     bitset &gt;&gt;= 1;
2095   }
2096   regs[count++] = zr-&gt;encoding_nocheck();
2097   count &amp;= ~1;
2098 
2099   for (int i = 2; i &lt; count; i += 2) {
2100     ldp(as_Register(regs[i]), as_Register(regs[i+1]),
2101        Address(stack, i * wordSize));
2102     words_pushed += 2;
2103   }
2104   if (count) {
2105     ldp(as_Register(regs[0]), as_Register(regs[1]),
2106        Address(post(stack, count * wordSize)));
2107     words_pushed += 2;
2108   }
2109 
2110   assert(words_pushed == count, &quot;oops, pushed != count&quot;);
2111 
2112   return count;
2113 }



























































2114 #ifdef ASSERT
2115 void MacroAssembler::verify_heapbase(const char* msg) {
2116 #if 0
2117   assert (UseCompressedOops || UseCompressedClassPointers, &quot;should be compressed&quot;);
2118   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
2119   if (CheckCompressedOops) {
2120     Label ok;
2121     push(1 &lt;&lt; rscratch1-&gt;encoding(), sp); // cmpptr trashes rscratch1
<span class="line-modified">2122     cmpptr(rheapbase, ExternalAddress((address)Universe::narrow_ptrs_base_addr()));</span>
2123     br(Assembler::EQ, ok);
2124     stop(msg);
2125     bind(ok);
2126     pop(1 &lt;&lt; rscratch1-&gt;encoding(), sp);
2127   }
2128 #endif
2129 }
2130 #endif
2131 
2132 void MacroAssembler::resolve_jobject(Register value, Register thread, Register tmp) {
2133   Label done, not_weak;
2134   cbz(value, done);           // Use NULL as-is.
2135 
2136   STATIC_ASSERT(JNIHandles::weak_tag_mask == 1u);
2137   tbz(r0, 0, not_weak);    // Test for jweak tag.
2138 
2139   // Resolve jweak.
2140   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF, value,
2141                  Address(value, -JNIHandles::weak_tag_value), tmp, thread);
2142   verify_oop(value);
2143   b(done);
2144 
2145   bind(not_weak);
2146   // Resolve (untagged) jobject.
2147   access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, 0), tmp, thread);
2148   verify_oop(value);
2149   bind(done);
2150 }
2151 
2152 void MacroAssembler::stop(const char* msg) {
2153   address ip = pc();
2154   pusha();
2155   mov(c_rarg0, (address)msg);
2156   mov(c_rarg1, (address)ip);
2157   mov(c_rarg2, sp);
2158   mov(c_rarg3, CAST_FROM_FN_PTR(address, MacroAssembler::debug64));
<span class="line-modified">2159   // call(c_rarg3);</span>
<span class="line-removed">2160   blrt(c_rarg3, 3, 0, 1);</span>
2161   hlt(0);
2162 }
2163 
2164 void MacroAssembler::warn(const char* msg) {
2165   pusha();
2166   mov(c_rarg0, (address)msg);
2167   mov(lr, CAST_FROM_FN_PTR(address, warning));
<span class="line-modified">2168   blrt(lr, 1, 0, MacroAssembler::ret_type_void);</span>
2169   popa();
2170 }
2171 
2172 void MacroAssembler::unimplemented(const char* what) {
2173   const char* buf = NULL;
2174   {
2175     ResourceMark rm;
2176     stringStream ss;
2177     ss.print(&quot;unimplemented: %s&quot;, what);
2178     buf = code_string(ss.as_string());
2179   }
2180   stop(buf);
2181 }
2182 
2183 // If a constant does not fit in an immediate field, generate some
2184 // number of MOV instructions and then perform the operation.
2185 void MacroAssembler::wrap_add_sub_imm_insn(Register Rd, Register Rn, unsigned imm,
2186                                            add_sub_imm_insn insn1,
2187                                            add_sub_reg_insn insn2) {
2188   assert(Rd != zr, &quot;Rd = zr and not setting flags?&quot;);
</pre>
<hr />
<pre>
2235 void MacroAssembler::sub(Register Rd, Register Rn, RegisterOrConstant decrement) {
2236   if (decrement.is_register()) {
2237     sub(Rd, Rn, decrement.as_register());
2238   } else {
2239     sub(Rd, Rn, decrement.as_constant());
2240   }
2241 }
2242 
2243 void MacroAssembler::subw(Register Rd, Register Rn, RegisterOrConstant decrement) {
2244   if (decrement.is_register()) {
2245     subw(Rd, Rn, decrement.as_register());
2246   } else {
2247     subw(Rd, Rn, decrement.as_constant());
2248   }
2249 }
2250 
2251 void MacroAssembler::reinit_heapbase()
2252 {
2253   if (UseCompressedOops) {
2254     if (Universe::is_fully_initialized()) {
<span class="line-modified">2255       mov(rheapbase, Universe::narrow_ptrs_base());</span>
2256     } else {
<span class="line-modified">2257       lea(rheapbase, ExternalAddress((address)Universe::narrow_ptrs_base_addr()));</span>
2258       ldr(rheapbase, Address(rheapbase));
2259     }
2260   }
2261 }
2262 
2263 // this simulates the behaviour of the x86 cmpxchg instruction using a
2264 // load linked/store conditional pair. we use the acquire/release
2265 // versions of these instructions so that we flush pending writes as
2266 // per Java semantics.
2267 
2268 // n.b the x86 version assumes the old value to be compared against is
2269 // in rax and updates rax with the value located in memory if the
2270 // cmpxchg fails. we supply a register for the old value explicitly
2271 
2272 // the aarch64 load linked/store conditional instructions do not
2273 // accept an offset. so, unlike x86, we must provide a plain register
2274 // to identify the memory word to be compared/exchanged rather than a
2275 // register+offset Address.
2276 
2277 void MacroAssembler::cmpxchgptr(Register oldv, Register newv, Register addr, Register tmp,
</pre>
<hr />
<pre>
2520       tty-&gt;print_cr(&quot;r13 = 0x%016lx&quot;, regs[13]);
2521       tty-&gt;print_cr(&quot;r14 = 0x%016lx&quot;, regs[14]);
2522       tty-&gt;print_cr(&quot;r15 = 0x%016lx&quot;, regs[15]);
2523       tty-&gt;print_cr(&quot;r16 = 0x%016lx&quot;, regs[16]);
2524       tty-&gt;print_cr(&quot;r17 = 0x%016lx&quot;, regs[17]);
2525       tty-&gt;print_cr(&quot;r18 = 0x%016lx&quot;, regs[18]);
2526       tty-&gt;print_cr(&quot;r19 = 0x%016lx&quot;, regs[19]);
2527       tty-&gt;print_cr(&quot;r20 = 0x%016lx&quot;, regs[20]);
2528       tty-&gt;print_cr(&quot;r21 = 0x%016lx&quot;, regs[21]);
2529       tty-&gt;print_cr(&quot;r22 = 0x%016lx&quot;, regs[22]);
2530       tty-&gt;print_cr(&quot;r23 = 0x%016lx&quot;, regs[23]);
2531       tty-&gt;print_cr(&quot;r24 = 0x%016lx&quot;, regs[24]);
2532       tty-&gt;print_cr(&quot;r25 = 0x%016lx&quot;, regs[25]);
2533       tty-&gt;print_cr(&quot;r26 = 0x%016lx&quot;, regs[26]);
2534       tty-&gt;print_cr(&quot;r27 = 0x%016lx&quot;, regs[27]);
2535       tty-&gt;print_cr(&quot;r28 = 0x%016lx&quot;, regs[28]);
2536       tty-&gt;print_cr(&quot;r30 = 0x%016lx&quot;, regs[30]);
2537       tty-&gt;print_cr(&quot;r31 = 0x%016lx&quot;, regs[31]);
2538       BREAKPOINT;
2539     }
<span class="line-removed">2540     ThreadStateTransition::transition(thread, _thread_in_vm, saved_state);</span>
<span class="line-removed">2541   } else {</span>
<span class="line-removed">2542     ttyLocker ttyl;</span>
<span class="line-removed">2543     ::tty-&gt;print_cr(&quot;=============== DEBUG MESSAGE: %s ================\n&quot;,</span>
<span class="line-removed">2544                     msg);</span>
<span class="line-removed">2545     assert(false, &quot;DEBUG MESSAGE: %s&quot;, msg);</span>
2546   }

2547 }
2548 
<span class="line-removed">2549 #ifdef BUILTIN_SIM</span>
<span class="line-removed">2550 // routine to generate an x86 prolog for a stub function which</span>
<span class="line-removed">2551 // bootstraps into the generated ARM code which directly follows the</span>
<span class="line-removed">2552 // stub</span>
<span class="line-removed">2553 //</span>
<span class="line-removed">2554 // the argument encodes the number of general and fp registers</span>
<span class="line-removed">2555 // passed by the caller and the callng convention (currently just</span>
<span class="line-removed">2556 // the number of general registers and assumes C argument passing)</span>
<span class="line-removed">2557 </span>
<span class="line-removed">2558 extern &quot;C&quot; {</span>
<span class="line-removed">2559 int aarch64_stub_prolog_size();</span>
<span class="line-removed">2560 void aarch64_stub_prolog();</span>
<span class="line-removed">2561 void aarch64_prolog();</span>
<span class="line-removed">2562 }</span>
<span class="line-removed">2563 </span>
<span class="line-removed">2564 void MacroAssembler::c_stub_prolog(int gp_arg_count, int fp_arg_count, int ret_type,</span>
<span class="line-removed">2565                                    address *prolog_ptr)</span>
<span class="line-removed">2566 {</span>
<span class="line-removed">2567   int calltype = (((ret_type &amp; 0x3) &lt;&lt; 8) |</span>
<span class="line-removed">2568                   ((fp_arg_count &amp; 0xf) &lt;&lt; 4) |</span>
<span class="line-removed">2569                   (gp_arg_count &amp; 0xf));</span>
<span class="line-removed">2570 </span>
<span class="line-removed">2571   // the addresses for the x86 to ARM entry code we need to use</span>
<span class="line-removed">2572   address start = pc();</span>
<span class="line-removed">2573   // printf(&quot;start = %lx\n&quot;, start);</span>
<span class="line-removed">2574   int byteCount =  aarch64_stub_prolog_size();</span>
<span class="line-removed">2575   // printf(&quot;byteCount = %x\n&quot;, byteCount);</span>
<span class="line-removed">2576   int instructionCount = (byteCount + 3)/ 4;</span>
<span class="line-removed">2577   // printf(&quot;instructionCount = %x\n&quot;, instructionCount);</span>
<span class="line-removed">2578   for (int i = 0; i &lt; instructionCount; i++) {</span>
<span class="line-removed">2579     nop();</span>
<span class="line-removed">2580   }</span>
<span class="line-removed">2581 </span>
<span class="line-removed">2582   memcpy(start, (void*)aarch64_stub_prolog, byteCount);</span>
<span class="line-removed">2583 </span>
<span class="line-removed">2584   // write the address of the setup routine and the call format at the</span>
<span class="line-removed">2585   // end of into the copied code</span>
<span class="line-removed">2586   u_int64_t *patch_end = (u_int64_t *)(start + byteCount);</span>
<span class="line-removed">2587   if (prolog_ptr)</span>
<span class="line-removed">2588     patch_end[-2] = (u_int64_t)prolog_ptr;</span>
<span class="line-removed">2589   patch_end[-1] = calltype;</span>
<span class="line-removed">2590 }</span>
<span class="line-removed">2591 #endif</span>
<span class="line-removed">2592 </span>
2593 void MacroAssembler::push_call_clobbered_registers() {
2594   int step = 4 * wordSize;
2595   push(RegSet::range(r0, r18) - RegSet::of(rscratch1, rscratch2), sp);
2596   sub(sp, sp, step);
2597   mov(rscratch1, -step);
2598   // Push v0-v7, v16-v31.
2599   for (int i = 31; i&gt;= 4; i -= 4) {
2600     if (i &lt;= v7-&gt;encoding() || i &gt;= v16-&gt;encoding())
2601       st1(as_FloatRegister(i-3), as_FloatRegister(i-2), as_FloatRegister(i-1),
2602           as_FloatRegister(i), T1D, Address(post(sp, rscratch1)));
2603   }
2604   st1(as_FloatRegister(0), as_FloatRegister(1), as_FloatRegister(2),
2605       as_FloatRegister(3), T1D, Address(sp));
2606 }
2607 
2608 void MacroAssembler::pop_call_clobbered_registers() {
2609   for (int i = 0; i &lt; 32; i += 4) {
2610     if (i &lt;= v7-&gt;encoding() || i &gt;= v16-&gt;encoding())
2611       ld1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
2612           as_FloatRegister(i+3), T1D, Address(post(sp, 4 * wordSize)));
</pre>
<hr />
<pre>
2651 Address MacroAssembler::offsetted_address(Register r, Register r1,
2652                                           Address::extend ext, int offset, int size) {
2653   if (offset || (ext.shift() % size != 0)) {
2654     lea(rscratch2, Address(r, r1, ext));
2655     return Address(rscratch2, offset);
2656   } else {
2657     return Address(r, r1, ext);
2658   }
2659 }
2660 
2661 Address MacroAssembler::spill_address(int size, int offset, Register tmp)
2662 {
2663   assert(offset &gt;= 0, &quot;spill to negative address?&quot;);
2664   // Offset reachable ?
2665   //   Not aligned - 9 bits signed offset
2666   //   Aligned - 12 bits unsigned offset shifted
2667   Register base = sp;
2668   if ((offset &amp; (size-1)) &amp;&amp; offset &gt;= (1&lt;&lt;8)) {
2669     add(tmp, base, offset &amp; ((1&lt;&lt;12)-1));
2670     base = tmp;
<span class="line-modified">2671     offset &amp;= -1&lt;&lt;12;</span>
2672   }
2673 
2674   if (offset &gt;= (1&lt;&lt;12) * size) {
2675     add(tmp, base, offset &amp; (((1&lt;&lt;12)-1)&lt;&lt;12));
2676     base = tmp;
2677     offset &amp;= ~(((1&lt;&lt;12)-1)&lt;&lt;12);
2678   }
2679 
2680   return Address(base, offset);
2681 }
2682 
2683 // Checks whether offset is aligned.
2684 // Returns true if it is, else false.
2685 bool MacroAssembler::merge_alignment_check(Register base,
2686                                            size_t size,
2687                                            long cur_offset,
2688                                            long prev_offset) const {
2689   if (AvoidUnalignedAccesses) {
2690     if (base == sp) {
2691       // Checks whether low offset if aligned to pair of registers.
</pre>
<hr />
<pre>
3653     adr = Address(rscratch2);
3654     break;
3655   }
3656   ldr(rscratch1, adr);
3657   add(rscratch1, rscratch1, src);
3658   str(rscratch1, adr);
3659 }
3660 
3661 void MacroAssembler::cmpptr(Register src1, Address src2) {
3662   unsigned long offset;
3663   adrp(rscratch1, src2, offset);
3664   ldr(rscratch1, Address(rscratch1, offset));
3665   cmp(src1, rscratch1);
3666 }
3667 
3668 void MacroAssembler::cmpoop(Register obj1, Register obj2) {
3669   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
3670   bs-&gt;obj_equals(this, obj1, obj2);
3671 }
3672 






3673 void MacroAssembler::load_klass(Register dst, Register src) {
3674   if (UseCompressedClassPointers) {
3675     ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3676     decode_klass_not_null(dst);
3677   } else {
3678     ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3679   }
3680 }
3681 
3682 // ((OopHandle)result).resolve();
3683 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
3684   // OopHandle::resolve is an indirection.
3685   access_load_at(T_OBJECT, IN_NATIVE, result, Address(result, 0), tmp, noreg);
3686 }
3687 
3688 void MacroAssembler::load_mirror(Register dst, Register method, Register tmp) {
3689   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
3690   ldr(dst, Address(rmethod, Method::const_offset()));
3691   ldr(dst, Address(dst, ConstMethod::constants_offset()));
3692   ldr(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));
3693   ldr(dst, Address(dst, mirror_offset));
3694   resolve_oop_handle(dst, tmp);
3695 }
3696 
3697 void MacroAssembler::cmp_klass(Register oop, Register trial_klass, Register tmp) {
3698   if (UseCompressedClassPointers) {
3699     ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
<span class="line-modified">3700     if (Universe::narrow_klass_base() == NULL) {</span>
<span class="line-modified">3701       cmp(trial_klass, tmp, LSL, Universe::narrow_klass_shift());</span>
3702       return;
<span class="line-modified">3703     } else if (((uint64_t)Universe::narrow_klass_base() &amp; 0xffffffff) == 0</span>
<span class="line-modified">3704                &amp;&amp; Universe::narrow_klass_shift() == 0) {</span>
3705       // Only the bottom 32 bits matter
3706       cmpw(trial_klass, tmp);
3707       return;
3708     }
3709     decode_klass_not_null(tmp);
3710   } else {
3711     ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3712   }
3713   cmp(trial_klass, tmp);
3714 }
3715 
3716 void MacroAssembler::load_prototype_header(Register dst, Register src) {
3717   load_klass(dst, src);
3718   ldr(dst, Address(dst, Klass::prototype_header_offset()));
3719 }
3720 
3721 void MacroAssembler::store_klass(Register dst, Register src) {
3722   // FIXME: Should this be a store release?  concurrent gcs assumes
3723   // klass length is valid if klass field is not null.
3724   if (UseCompressedClassPointers) {
3725     encode_klass_not_null(src);
3726     strw(src, Address(dst, oopDesc::klass_offset_in_bytes()));
3727   } else {
3728     str(src, Address(dst, oopDesc::klass_offset_in_bytes()));
3729   }
3730 }
3731 
3732 void MacroAssembler::store_klass_gap(Register dst, Register src) {
3733   if (UseCompressedClassPointers) {
3734     // Store to klass gap in destination
3735     strw(src, Address(dst, oopDesc::klass_gap_offset_in_bytes()));
3736   }
3737 }
3738 
3739 // Algorithm must match CompressedOops::encode.
3740 void MacroAssembler::encode_heap_oop(Register d, Register s) {
3741 #ifdef ASSERT
3742   verify_heapbase(&quot;MacroAssembler::encode_heap_oop: heap base corrupted?&quot;);
3743 #endif
3744   verify_oop(s, &quot;broken oop in encode_heap_oop&quot;);
<span class="line-modified">3745   if (Universe::narrow_oop_base() == NULL) {</span>
<span class="line-modified">3746     if (Universe::narrow_oop_shift() != 0) {</span>
<span class="line-modified">3747       assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
3748       lsr(d, s, LogMinObjAlignmentInBytes);
3749     } else {
3750       mov(d, s);
3751     }
3752   } else {
3753     subs(d, s, rheapbase);
3754     csel(d, d, zr, Assembler::HS);
3755     lsr(d, d, LogMinObjAlignmentInBytes);
3756 
3757     /*  Old algorithm: is this any worse?
3758     Label nonnull;
3759     cbnz(r, nonnull);
3760     sub(r, r, rheapbase);
3761     bind(nonnull);
3762     lsr(r, r, LogMinObjAlignmentInBytes);
3763     */
3764   }
3765 }
3766 
3767 void MacroAssembler::encode_heap_oop_not_null(Register r) {
3768 #ifdef ASSERT
3769   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null: heap base corrupted?&quot;);
3770   if (CheckCompressedOops) {
3771     Label ok;
3772     cbnz(r, ok);
3773     stop(&quot;null oop passed to encode_heap_oop_not_null&quot;);
3774     bind(ok);
3775   }
3776 #endif
3777   verify_oop(r, &quot;broken oop in encode_heap_oop_not_null&quot;);
<span class="line-modified">3778   if (Universe::narrow_oop_base() != NULL) {</span>
3779     sub(r, r, rheapbase);
3780   }
<span class="line-modified">3781   if (Universe::narrow_oop_shift() != 0) {</span>
<span class="line-modified">3782     assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
3783     lsr(r, r, LogMinObjAlignmentInBytes);
3784   }
3785 }
3786 
3787 void MacroAssembler::encode_heap_oop_not_null(Register dst, Register src) {
3788 #ifdef ASSERT
3789   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null2: heap base corrupted?&quot;);
3790   if (CheckCompressedOops) {
3791     Label ok;
3792     cbnz(src, ok);
3793     stop(&quot;null oop passed to encode_heap_oop_not_null2&quot;);
3794     bind(ok);
3795   }
3796 #endif
3797   verify_oop(src, &quot;broken oop in encode_heap_oop_not_null2&quot;);
3798 
3799   Register data = src;
<span class="line-modified">3800   if (Universe::narrow_oop_base() != NULL) {</span>
3801     sub(dst, src, rheapbase);
3802     data = dst;
3803   }
<span class="line-modified">3804   if (Universe::narrow_oop_shift() != 0) {</span>
<span class="line-modified">3805     assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
3806     lsr(dst, data, LogMinObjAlignmentInBytes);
3807     data = dst;
3808   }
3809   if (data == src)
3810     mov(dst, src);
3811 }
3812 
3813 void  MacroAssembler::decode_heap_oop(Register d, Register s) {
3814 #ifdef ASSERT
3815   verify_heapbase(&quot;MacroAssembler::decode_heap_oop: heap base corrupted?&quot;);
3816 #endif
<span class="line-modified">3817   if (Universe::narrow_oop_base() == NULL) {</span>
<span class="line-modified">3818     if (Universe::narrow_oop_shift() != 0 || d != s) {</span>
<span class="line-modified">3819       lsl(d, s, Universe::narrow_oop_shift());</span>
3820     }
3821   } else {
3822     Label done;
3823     if (d != s)
3824       mov(d, s);
3825     cbz(s, done);
3826     add(d, rheapbase, s, Assembler::LSL, LogMinObjAlignmentInBytes);
3827     bind(done);
3828   }
3829   verify_oop(d, &quot;broken oop in decode_heap_oop&quot;);
3830 }
3831 
3832 void  MacroAssembler::decode_heap_oop_not_null(Register r) {
3833   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
3834   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
3835   // Cannot assert, unverified entry point counts instructions (see .ad file)
3836   // vtableStubs also counts instructions in pd_code_size_limit.
3837   // Also do not verify_oop as this is called by verify_oop.
<span class="line-modified">3838   if (Universe::narrow_oop_shift() != 0) {</span>
<span class="line-modified">3839     assert(LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
<span class="line-modified">3840     if (Universe::narrow_oop_base() != NULL) {</span>
3841       add(r, rheapbase, r, Assembler::LSL, LogMinObjAlignmentInBytes);
3842     } else {
3843       add(r, zr, r, Assembler::LSL, LogMinObjAlignmentInBytes);
3844     }
3845   } else {
<span class="line-modified">3846     assert (Universe::narrow_oop_base() == NULL, &quot;sanity&quot;);</span>
3847   }
3848 }
3849 
3850 void  MacroAssembler::decode_heap_oop_not_null(Register dst, Register src) {
3851   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
3852   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
3853   // Cannot assert, unverified entry point counts instructions (see .ad file)
3854   // vtableStubs also counts instructions in pd_code_size_limit.
3855   // Also do not verify_oop as this is called by verify_oop.
<span class="line-modified">3856   if (Universe::narrow_oop_shift() != 0) {</span>
<span class="line-modified">3857     assert(LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
<span class="line-modified">3858     if (Universe::narrow_oop_base() != NULL) {</span>
3859       add(dst, rheapbase, src, Assembler::LSL, LogMinObjAlignmentInBytes);
3860     } else {
3861       add(dst, zr, src, Assembler::LSL, LogMinObjAlignmentInBytes);
3862     }
3863   } else {
<span class="line-modified">3864     assert (Universe::narrow_oop_base() == NULL, &quot;sanity&quot;);</span>
3865     if (dst != src) {
3866       mov(dst, src);
3867     }
3868   }
3869 }
3870 


































3871 void MacroAssembler::encode_klass_not_null(Register dst, Register src) {
<span class="line-modified">3872   if (Universe::narrow_klass_base() == NULL) {</span>
<span class="line-modified">3873     if (Universe::narrow_klass_shift() != 0) {</span>
<span class="line-modified">3874       assert (LogKlassAlignmentInBytes == Universe::narrow_klass_shift(), &quot;decode alg wrong&quot;);</span>
3875       lsr(dst, src, LogKlassAlignmentInBytes);
3876     } else {
3877       if (dst != src) mov(dst, src);
3878     }
<span class="line-modified">3879     return;</span>
<span class="line-removed">3880   }</span>
3881 
<span class="line-modified">3882   if (use_XOR_for_compressed_class_base) {</span>
<span class="line-modified">3883     if (Universe::narrow_klass_shift() != 0) {</span>
<span class="line-modified">3884       eor(dst, src, (uint64_t)Universe::narrow_klass_base());</span>
3885       lsr(dst, dst, LogKlassAlignmentInBytes);
3886     } else {
<span class="line-modified">3887       eor(dst, src, (uint64_t)Universe::narrow_klass_base());</span>
3888     }
<span class="line-modified">3889     return;</span>
<span class="line-removed">3890   }</span>
<span class="line-removed">3891 </span>
<span class="line-removed">3892   if (((uint64_t)Universe::narrow_klass_base() &amp; 0xffffffff) == 0</span>
<span class="line-removed">3893       &amp;&amp; Universe::narrow_klass_shift() == 0) {</span>
<span class="line-removed">3894     movw(dst, src);</span>
<span class="line-removed">3895     return;</span>
<span class="line-removed">3896   }</span>
3897 
<span class="line-modified">3898 #ifdef ASSERT</span>
<span class="line-modified">3899   verify_heapbase(&quot;MacroAssembler::encode_klass_not_null2: heap base corrupted?&quot;);</span>
<span class="line-modified">3900 #endif</span>




3901 
<span class="line-modified">3902   Register rbase = dst;</span>
<span class="line-modified">3903   if (dst == src) rbase = rheapbase;</span>
<span class="line-modified">3904   mov(rbase, (uint64_t)Universe::narrow_klass_base());</span>
<span class="line-removed">3905   sub(dst, src, rbase);</span>
<span class="line-removed">3906   if (Universe::narrow_klass_shift() != 0) {</span>
<span class="line-removed">3907     assert (LogKlassAlignmentInBytes == Universe::narrow_klass_shift(), &quot;decode alg wrong&quot;);</span>
<span class="line-removed">3908     lsr(dst, dst, LogKlassAlignmentInBytes);</span>
3909   }
<span class="line-removed">3910   if (dst == src) reinit_heapbase();</span>
3911 }
3912 
3913 void MacroAssembler::encode_klass_not_null(Register r) {
3914   encode_klass_not_null(r, r);
3915 }
3916 
3917 void  MacroAssembler::decode_klass_not_null(Register dst, Register src) {
<span class="line-removed">3918   Register rbase = dst;</span>
3919   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
3920 
<span class="line-modified">3921   if (Universe::narrow_klass_base() == NULL) {</span>
<span class="line-modified">3922     if (Universe::narrow_klass_shift() != 0) {</span>
<span class="line-modified">3923       assert(LogKlassAlignmentInBytes == Universe::narrow_klass_shift(), &quot;decode alg wrong&quot;);</span>
3924       lsl(dst, src, LogKlassAlignmentInBytes);
3925     } else {
3926       if (dst != src) mov(dst, src);
3927     }
<span class="line-modified">3928     return;</span>
<span class="line-removed">3929   }</span>
3930 
<span class="line-modified">3931   if (use_XOR_for_compressed_class_base) {</span>
<span class="line-modified">3932     if (Universe::narrow_klass_shift() != 0) {</span>
3933       lsl(dst, src, LogKlassAlignmentInBytes);
<span class="line-modified">3934       eor(dst, dst, (uint64_t)Universe::narrow_klass_base());</span>
3935     } else {
<span class="line-modified">3936       eor(dst, src, (uint64_t)Universe::narrow_klass_base());</span>
3937     }
<span class="line-modified">3938     return;</span>
<span class="line-removed">3939   }</span>
3940 
<span class="line-modified">3941   if (((uint64_t)Universe::narrow_klass_base() &amp; 0xffffffff) == 0</span>
<span class="line-modified">3942       &amp;&amp; Universe::narrow_klass_shift() == 0) {</span>
<span class="line-modified">3943     if (dst != src)</span>
<span class="line-modified">3944       movw(dst, src);</span>
<span class="line-modified">3945     movk(dst, (uint64_t)Universe::narrow_klass_base() &gt;&gt; 32, 32);</span>
<span class="line-modified">3946     return;</span>






3947   }
3948 
<span class="line-modified">3949   // Cannot assert, unverified entry point counts instructions (see .ad file)</span>
<span class="line-modified">3950   // vtableStubs also counts instructions in pd_code_size_limit.</span>
<span class="line-modified">3951   // Also do not verify_oop as this is called by verify_oop.</span>
<span class="line-removed">3952   if (dst == src) rbase = rheapbase;</span>
<span class="line-removed">3953   mov(rbase, (uint64_t)Universe::narrow_klass_base());</span>
<span class="line-removed">3954   if (Universe::narrow_klass_shift() != 0) {</span>
<span class="line-removed">3955     assert(LogKlassAlignmentInBytes == Universe::narrow_klass_shift(), &quot;decode alg wrong&quot;);</span>
<span class="line-removed">3956     add(dst, rbase, src, Assembler::LSL, LogKlassAlignmentInBytes);</span>
<span class="line-removed">3957   } else {</span>
<span class="line-removed">3958     add(dst, rbase, src);</span>
3959   }
<span class="line-removed">3960   if (dst == src) reinit_heapbase();</span>
3961 }
3962 
3963 void  MacroAssembler::decode_klass_not_null(Register r) {
3964   decode_klass_not_null(r, r);
3965 }
3966 
3967 void  MacroAssembler::set_narrow_oop(Register dst, jobject obj) {
3968 #ifdef ASSERT
3969   {
3970     ThreadInVMfromUnknown tiv;
3971     assert (UseCompressedOops, &quot;should only be used for compressed oops&quot;);
3972     assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
3973     assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
<span class="line-modified">3974     assert(Universe::heap()-&gt;is_in_reserved(JNIHandles::resolve(obj)), &quot;should be real oop&quot;);</span>
3975   }
3976 #endif
3977   int oop_index = oop_recorder()-&gt;find_index(obj);
3978   InstructionMark im(this);
3979   RelocationHolder rspec = oop_Relocation::spec(oop_index);
3980   code_section()-&gt;relocate(inst_mark(), rspec);
3981   movz(dst, 0xDEAD, 16);
3982   movk(dst, 0xBEEF);
3983 }
3984 
3985 void  MacroAssembler::set_narrow_klass(Register dst, Klass* k) {
3986   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
3987   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
3988   int index = oop_recorder()-&gt;find_index(k);
<span class="line-modified">3989   assert(! Universe::heap()-&gt;is_in_reserved(k), &quot;should not be an oop&quot;);</span>
3990 
3991   InstructionMark im(this);
3992   RelocationHolder rspec = metadata_Relocation::spec(index);
3993   code_section()-&gt;relocate(inst_mark(), rspec);
<span class="line-modified">3994   narrowKlass nk = Klass::encode_klass(k);</span>
3995   movz(dst, (nk &gt;&gt; 16), 16);
3996   movk(dst, nk &amp; 0xffff);
3997 }
3998 
3999 void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators,
4000                                     Register dst, Address src,
4001                                     Register tmp1, Register thread_tmp) {
4002   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4003   decorators = AccessInternal::decorator_fixup(decorators);
4004   bool as_raw = (decorators &amp; AS_RAW) != 0;
4005   if (as_raw) {
4006     bs-&gt;BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4007   } else {
4008     bs-&gt;load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4009   }
4010 }
4011 
4012 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators,
4013                                      Address dst, Register src,
4014                                      Register tmp1, Register thread_tmp) {
</pre>
<hr />
<pre>
4053 
4054 Address MacroAssembler::allocate_metadata_address(Metadata* obj) {
4055   assert(oop_recorder() != NULL, &quot;this assembler needs a Recorder&quot;);
4056   int index = oop_recorder()-&gt;allocate_metadata_index(obj);
4057   RelocationHolder rspec = metadata_Relocation::spec(index);
4058   return Address((address)obj, rspec);
4059 }
4060 
4061 // Move an oop into a register.  immediate is true if we want
4062 // immediate instrcutions, i.e. we are not going to patch this
4063 // instruction while the code is being executed by another thread.  In
4064 // that case we can use move immediates rather than the constant pool.
4065 void MacroAssembler::movoop(Register dst, jobject obj, bool immediate) {
4066   int oop_index;
4067   if (obj == NULL) {
4068     oop_index = oop_recorder()-&gt;allocate_oop_index(obj);
4069   } else {
4070 #ifdef ASSERT
4071     {
4072       ThreadInVMfromUnknown tiv;
<span class="line-modified">4073       assert(Universe::heap()-&gt;is_in_reserved(JNIHandles::resolve(obj)), &quot;should be real oop&quot;);</span>
4074     }
4075 #endif
4076     oop_index = oop_recorder()-&gt;find_index(obj);
4077   }
4078   RelocationHolder rspec = oop_Relocation::spec(oop_index);
4079   if (! immediate) {
4080     address dummy = address(uintptr_t(pc()) &amp; -wordSize); // A nearby aligned address
4081     ldr_constant(dst, Address(dummy, rspec));
4082   } else
4083     mov(dst, Address((address)obj, rspec));
4084 }
4085 
4086 // Move a metadata address into a register.
4087 void MacroAssembler::mov_metadata(Register dst, Metadata* obj) {
4088   int oop_index;
4089   if (obj == NULL) {
4090     oop_index = oop_recorder()-&gt;allocate_metadata_index(obj);
4091   } else {
4092     oop_index = oop_recorder()-&gt;find_index(obj);
4093   }
4094   RelocationHolder rspec = metadata_Relocation::spec(oop_index);
4095   mov(dst, Address((address)obj, rspec));
4096 }
4097 
4098 Address MacroAssembler::constant_oop_address(jobject obj) {
4099 #ifdef ASSERT
4100   {
4101     ThreadInVMfromUnknown tiv;
4102     assert(oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
<span class="line-modified">4103     assert(Universe::heap()-&gt;is_in_reserved(JNIHandles::resolve(obj)), &quot;not an oop&quot;);</span>
4104   }
4105 #endif
4106   int oop_index = oop_recorder()-&gt;find_index(obj);
4107   return Address((address)obj, oop_Relocation::spec(oop_index));
4108 }
4109 
4110 // Defines obj, preserves var_size_in_bytes, okay for t2 == var_size_in_bytes.
4111 void MacroAssembler::tlab_allocate(Register obj,
4112                                    Register var_size_in_bytes,
4113                                    int con_size_in_bytes,
4114                                    Register t1,
4115                                    Register t2,
4116                                    Label&amp; slow_case) {
4117   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4118   bs-&gt;tlab_allocate(this, obj, var_size_in_bytes, con_size_in_bytes, t1, t2, slow_case);
4119 }
4120 
4121 // Defines obj, preserves var_size_in_bytes
4122 void MacroAssembler::eden_allocate(Register obj,
4123                                    Register var_size_in_bytes,
</pre>
<hr />
<pre>
4846     cmpw(ch, ch1);
4847     br(EQ, MATCH);
4848     adds(cnt1_neg, cnt1_neg, 2);
4849     br(LT, DO1_LOOP);
4850   BIND(NOMATCH);
4851     mov(result, -1);
4852     b(DONE);
4853   BIND(MATCH);
4854     add(result, result_tmp, cnt1_neg, ASR, 1);
4855   BIND(DONE);
4856 }
4857 
4858 // Compare strings.
4859 void MacroAssembler::string_compare(Register str1, Register str2,
4860     Register cnt1, Register cnt2, Register result, Register tmp1, Register tmp2,
4861     FloatRegister vtmp1, FloatRegister vtmp2, FloatRegister vtmp3, int ae) {
4862   Label DONE, SHORT_LOOP, SHORT_STRING, SHORT_LAST, TAIL, STUB,
4863       DIFFERENCE, NEXT_WORD, SHORT_LOOP_TAIL, SHORT_LAST2, SHORT_LAST_INIT,
4864       SHORT_LOOP_START, TAIL_CHECK;
4865 
<span class="line-removed">4866   const u1 STUB_THRESHOLD = 64 + 8;</span>
4867   bool isLL = ae == StrIntrinsicNode::LL;
4868   bool isLU = ae == StrIntrinsicNode::LU;
4869   bool isUL = ae == StrIntrinsicNode::UL;
4870 





4871   bool str1_isL = isLL || isLU;
4872   bool str2_isL = isLL || isUL;
4873 
4874   int str1_chr_shift = str1_isL ? 0 : 1;
4875   int str2_chr_shift = str2_isL ? 0 : 1;
4876   int str1_chr_size = str1_isL ? 1 : 2;
4877   int str2_chr_size = str2_isL ? 1 : 2;
4878   int minCharsInWord = isLL ? wordSize : wordSize/2;
4879 
4880   FloatRegister vtmpZ = vtmp1, vtmp = vtmp2;
4881   chr_insn str1_load_chr = str1_isL ? (chr_insn)&amp;MacroAssembler::ldrb :
4882                                       (chr_insn)&amp;MacroAssembler::ldrh;
4883   chr_insn str2_load_chr = str2_isL ? (chr_insn)&amp;MacroAssembler::ldrb :
4884                                       (chr_insn)&amp;MacroAssembler::ldrh;
4885   uxt_insn ext_chr = isLL ? (uxt_insn)&amp;MacroAssembler::uxtbw :
4886                             (uxt_insn)&amp;MacroAssembler::uxthw;
4887 
4888   BLOCK_COMMENT(&quot;string_compare {&quot;);
4889 
4890   // Bizzarely, the counts are passed in bytes, regardless of whether they
4891   // are L or U strings, however the result is always in characters.
4892   if (!str1_isL) asrw(cnt1, cnt1, 1);
4893   if (!str2_isL) asrw(cnt2, cnt2, 1);
4894 
4895   // Compute the minimum of the string lengths and save the difference.
4896   subsw(result, cnt1, cnt2);
4897   cselw(cnt2, cnt1, cnt2, Assembler::LE); // min
4898 
4899   // A very short string
4900   cmpw(cnt2, minCharsInWord);
4901   br(Assembler::LE, SHORT_STRING);
4902 
4903   // Compare longwords
4904   // load first parts of strings and finish initialization while loading
4905   {
4906     if (str1_isL == str2_isL) { // LL or UU
4907       ldr(tmp1, Address(str1));
4908       cmp(str1, str2);
4909       br(Assembler::EQ, DONE);
4910       ldr(tmp2, Address(str2));
<span class="line-modified">4911       cmp(cnt2, STUB_THRESHOLD);</span>
4912       br(GE, STUB);
4913       subsw(cnt2, cnt2, minCharsInWord);
4914       br(EQ, TAIL_CHECK);
4915       lea(str2, Address(str2, cnt2, Address::uxtw(str2_chr_shift)));
4916       lea(str1, Address(str1, cnt2, Address::uxtw(str1_chr_shift)));
4917       sub(cnt2, zr, cnt2, LSL, str2_chr_shift);
4918     } else if (isLU) {
4919       ldrs(vtmp, Address(str1));
<span class="line-removed">4920       cmp(str1, str2);</span>
<span class="line-removed">4921       br(Assembler::EQ, DONE);</span>
4922       ldr(tmp2, Address(str2));
<span class="line-modified">4923       cmp(cnt2, STUB_THRESHOLD);</span>
4924       br(GE, STUB);
4925       subw(cnt2, cnt2, 4);
4926       eor(vtmpZ, T16B, vtmpZ, vtmpZ);
4927       lea(str1, Address(str1, cnt2, Address::uxtw(str1_chr_shift)));
4928       lea(str2, Address(str2, cnt2, Address::uxtw(str2_chr_shift)));
4929       zip1(vtmp, T8B, vtmp, vtmpZ);
4930       sub(cnt1, zr, cnt2, LSL, str1_chr_shift);
4931       sub(cnt2, zr, cnt2, LSL, str2_chr_shift);
4932       add(cnt1, cnt1, 4);
4933       fmovd(tmp1, vtmp);
4934     } else { // UL case
4935       ldr(tmp1, Address(str1));
<span class="line-removed">4936       cmp(str1, str2);</span>
<span class="line-removed">4937       br(Assembler::EQ, DONE);</span>
4938       ldrs(vtmp, Address(str2));
<span class="line-modified">4939       cmp(cnt2, STUB_THRESHOLD);</span>
4940       br(GE, STUB);
4941       subw(cnt2, cnt2, 4);
4942       lea(str1, Address(str1, cnt2, Address::uxtw(str1_chr_shift)));
4943       eor(vtmpZ, T16B, vtmpZ, vtmpZ);
4944       lea(str2, Address(str2, cnt2, Address::uxtw(str2_chr_shift)));
4945       sub(cnt1, zr, cnt2, LSL, str1_chr_shift);
4946       zip1(vtmp, T8B, vtmp, vtmpZ);
4947       sub(cnt2, zr, cnt2, LSL, str2_chr_shift);
4948       add(cnt1, cnt1, 8);
4949       fmovd(tmp2, vtmp);
4950     }
4951     adds(cnt2, cnt2, isUL ? 4 : 8);
4952     br(GE, TAIL);
4953     eor(rscratch2, tmp1, tmp2);
4954     cbnz(rscratch2, DIFFERENCE);
4955     // main loop
4956     bind(NEXT_WORD);
4957     if (str1_isL == str2_isL) {
4958       ldr(tmp1, Address(str1, cnt2));
4959       ldr(tmp2, Address(str2, cnt2));
</pre>
<hr />
<pre>
5613   br(Assembler::GE, loop);
5614 
5615   tbz(cnt, 0, fini);
5616   str(value, Address(post(base, 8)));
5617   bind(fini);
5618 }
5619 
5620 // Intrinsic for sun/nio/cs/ISO_8859_1$Encoder.implEncodeISOArray and
5621 // java/lang/StringUTF16.compress.
5622 void MacroAssembler::encode_iso_array(Register src, Register dst,
5623                       Register len, Register result,
5624                       FloatRegister Vtmp1, FloatRegister Vtmp2,
5625                       FloatRegister Vtmp3, FloatRegister Vtmp4)
5626 {
5627     Label DONE, SET_RESULT, NEXT_32, NEXT_32_PRFM, LOOP_8, NEXT_8, LOOP_1, NEXT_1,
5628         NEXT_32_START, NEXT_32_PRFM_START;
5629     Register tmp1 = rscratch1, tmp2 = rscratch2;
5630 
5631       mov(result, len); // Save initial len
5632 
<span class="line-removed">5633 #ifndef BUILTIN_SIM</span>
5634       cmp(len, (u1)8); // handle shortest strings first
5635       br(LT, LOOP_1);
5636       cmp(len, (u1)32);
5637       br(LT, NEXT_8);
5638       // The following code uses the SIMD &#39;uzp1&#39; and &#39;uzp2&#39; instructions
5639       // to convert chars to bytes
5640       if (SoftwarePrefetchHintDistance &gt;= 0) {
5641         ld1(Vtmp1, Vtmp2, Vtmp3, Vtmp4, T8H, src);
5642         subs(tmp2, len, SoftwarePrefetchHintDistance/2 + 16);
5643         br(LE, NEXT_32_START);
5644         b(NEXT_32_PRFM_START);
5645         BIND(NEXT_32_PRFM);
5646           ld1(Vtmp1, Vtmp2, Vtmp3, Vtmp4, T8H, src);
5647         BIND(NEXT_32_PRFM_START);
5648           prfm(Address(src, SoftwarePrefetchHintDistance));
5649           orr(v4, T16B, Vtmp1, Vtmp2);
5650           orr(v5, T16B, Vtmp3, Vtmp4);
5651           uzp1(Vtmp1, T16B, Vtmp1, Vtmp2);
5652           uzp1(Vtmp3, T16B, Vtmp3, Vtmp4);
5653           uzp2(v5, T16B, v4, v5); // high bytes
</pre>
<hr />
<pre>
5689       cbz(len, DONE);
5690 
5691     BIND(LOOP_8);
5692       cmp(len, (u1)8);
5693       br(LT, LOOP_1);
5694     BIND(NEXT_8);
5695       ld1(Vtmp1, T8H, src);
5696       uzp1(Vtmp2, T16B, Vtmp1, Vtmp1); // low bytes
5697       uzp2(Vtmp3, T16B, Vtmp1, Vtmp1); // high bytes
5698       fmovd(tmp1, Vtmp3);
5699       cbnz(tmp1, NEXT_1);
5700       strd(Vtmp2, dst);
5701 
5702       sub(len, len, 8);
5703       add(dst, dst, 8);
5704       add(src, src, 16);
5705       cmp(len, (u1)8);
5706       br(GE, NEXT_8);
5707 
5708     BIND(LOOP_1);
<span class="line-modified">5709 #endif</span>
5710     cbz(len, DONE);
5711     BIND(NEXT_1);
5712       ldrh(tmp1, Address(post(src, 2)));
5713       tst(tmp1, 0xff00);
5714       br(NE, SET_RESULT);
5715       strb(tmp1, Address(post(dst, 1)));
5716       subs(len, len, 1);
5717       br(GT, NEXT_1);
5718 
5719     BIND(SET_RESULT);
5720       sub(result, result, len); // Return index where we stopped
5721                                 // Return len == 0 if we processed all
5722                                 // characters
5723     BIND(DONE);
5724 }
5725 
5726 
5727 // Inflate byte[] array to char[].
5728 void MacroAssembler::byte_array_inflate(Register src, Register dst, Register len,
5729                                         FloatRegister vtmp1, FloatRegister vtmp2, FloatRegister vtmp3,
</pre>
<hr />
<pre>
5828                                          FloatRegister tmp3Reg, FloatRegister tmp4Reg,
5829                                          Register result) {
5830   encode_iso_array(src, dst, len, result,
5831                    tmp1Reg, tmp2Reg, tmp3Reg, tmp4Reg);
5832   cmp(len, zr);
5833   csel(result, result, zr, EQ);
5834 }
5835 
5836 // get_thread() can be called anywhere inside generated code so we
5837 // need to save whatever non-callee save context might get clobbered
5838 // by the call to JavaThread::aarch64_get_thread_helper() or, indeed,
5839 // the call setup code.
5840 //
5841 // aarch64_get_thread_helper() clobbers only r0, r1, and flags.
5842 //
5843 void MacroAssembler::get_thread(Register dst) {
5844   RegSet saved_regs = RegSet::range(r0, r1) + lr - dst;
5845   push(saved_regs, sp);
5846 
5847   mov(lr, CAST_FROM_FN_PTR(address, JavaThread::aarch64_get_thread_helper));
<span class="line-modified">5848   blrt(lr, 1, 0, 1);</span>
5849   if (dst != c_rarg0) {
5850     mov(dst, c_rarg0);
5851   }
5852 
5853   pop(saved_regs, sp);
5854 }






















</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.</span>
<span class="line-modified">   3  * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.</span>
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &lt;sys/types.h&gt;
  27 
  28 #include &quot;precompiled.hpp&quot;
  29 #include &quot;jvm.h&quot;
  30 #include &quot;asm/assembler.hpp&quot;
  31 #include &quot;asm/assembler.inline.hpp&quot;
  32 #include &quot;gc/shared/barrierSet.hpp&quot;
  33 #include &quot;gc/shared/cardTable.hpp&quot;
  34 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  35 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
  36 #include &quot;interpreter/interpreter.hpp&quot;
  37 #include &quot;compiler/disassembler.hpp&quot;
  38 #include &quot;memory/resourceArea.hpp&quot;
<span class="line-added">  39 #include &quot;memory/universe.hpp&quot;</span>
  40 #include &quot;nativeInst_aarch64.hpp&quot;
  41 #include &quot;oops/accessDecorators.hpp&quot;
  42 #include &quot;oops/compressedOops.inline.hpp&quot;
  43 #include &quot;oops/klass.inline.hpp&quot;
  44 #include &quot;runtime/biasedLocking.hpp&quot;
  45 #include &quot;runtime/icache.hpp&quot;
  46 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  47 #include &quot;runtime/jniHandles.inline.hpp&quot;
  48 #include &quot;runtime/sharedRuntime.hpp&quot;
  49 #include &quot;runtime/thread.hpp&quot;
<span class="line-added">  50 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  51 #ifdef COMPILER1
  52 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  53 #endif
  54 #ifdef COMPILER2
  55 #include &quot;oops/oop.hpp&quot;
  56 #include &quot;opto/compile.hpp&quot;
  57 #include &quot;opto/intrinsicnode.hpp&quot;
  58 #include &quot;opto/node.hpp&quot;
  59 #endif
  60 
  61 #ifdef PRODUCT
  62 #define BLOCK_COMMENT(str) /* nothing */
  63 #define STOP(error) stop(error)
  64 #else
  65 #define BLOCK_COMMENT(str) block_comment(str)
  66 #define STOP(error) block_comment(error); stop(error)
  67 #endif
  68 
  69 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  70 
</pre>
<hr />
<pre>
 456     should_not_reach_here();
 457 
 458     bind(no_reserved_zone_enabling);
 459 }
 460 
 461 int MacroAssembler::biased_locking_enter(Register lock_reg,
 462                                          Register obj_reg,
 463                                          Register swap_reg,
 464                                          Register tmp_reg,
 465                                          bool swap_reg_contains_mark,
 466                                          Label&amp; done,
 467                                          Label* slow_case,
 468                                          BiasedLockingCounters* counters) {
 469   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 470   assert_different_registers(lock_reg, obj_reg, swap_reg);
 471 
 472   if (PrintBiasedLockingStatistics &amp;&amp; counters == NULL)
 473     counters = BiasedLocking::counters();
 474 
 475   assert_different_registers(lock_reg, obj_reg, swap_reg, tmp_reg, rscratch1, rscratch2, noreg);
<span class="line-modified"> 476   assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits, &quot;biased locking makes assumptions about bit layout&quot;);</span>
 477   Address mark_addr      (obj_reg, oopDesc::mark_offset_in_bytes());
 478   Address klass_addr     (obj_reg, oopDesc::klass_offset_in_bytes());
 479   Address saved_mark_addr(lock_reg, 0);
 480 
 481   // Biased locking
 482   // See whether the lock is currently biased toward our thread and
 483   // whether the epoch is still valid
 484   // Note that the runtime guarantees sufficient alignment of JavaThread
 485   // pointers to allow age to be placed into low bits
 486   // First check to see whether biasing is even enabled for this object
 487   Label cas_label;
 488   int null_check_offset = -1;
 489   if (!swap_reg_contains_mark) {
 490     null_check_offset = offset();
 491     ldr(swap_reg, mark_addr);
 492   }
<span class="line-modified"> 493   andr(tmp_reg, swap_reg, markWord::biased_lock_mask_in_place);</span>
<span class="line-modified"> 494   cmp(tmp_reg, (u1)markWord::biased_lock_pattern);</span>
 495   br(Assembler::NE, cas_label);
 496   // The bias pattern is present in the object&#39;s header. Need to check
 497   // whether the bias owner and the epoch are both still current.
 498   load_prototype_header(tmp_reg, obj_reg);
 499   orr(tmp_reg, tmp_reg, rthread);
 500   eor(tmp_reg, swap_reg, tmp_reg);
<span class="line-modified"> 501   andr(tmp_reg, tmp_reg, ~((int) markWord::age_mask_in_place));</span>
 502   if (counters != NULL) {
 503     Label around;
 504     cbnz(tmp_reg, around);
 505     atomic_incw(Address((address)counters-&gt;biased_lock_entry_count_addr()), tmp_reg, rscratch1, rscratch2);
 506     b(done);
 507     bind(around);
 508   } else {
 509     cbz(tmp_reg, done);
 510   }
 511 
 512   Label try_revoke_bias;
 513   Label try_rebias;
 514 
 515   // At this point we know that the header has the bias pattern and
 516   // that we are not the bias owner in the current epoch. We need to
 517   // figure out more details about the state of the header in order to
 518   // know what operations can be legally performed on the object&#39;s
 519   // header.
 520 
 521   // If the low three bits in the xor result aren&#39;t clear, that means
 522   // the prototype header is no longer biased and we have to revoke
 523   // the bias on this object.
<span class="line-modified"> 524   andr(rscratch1, tmp_reg, markWord::biased_lock_mask_in_place);</span>
 525   cbnz(rscratch1, try_revoke_bias);
 526 
 527   // Biasing is still enabled for this data type. See whether the
 528   // epoch of the current bias is still valid, meaning that the epoch
 529   // bits of the mark word are equal to the epoch bits of the
 530   // prototype header. (Note that the prototype header&#39;s epoch bits
 531   // only change at a safepoint.) If not, attempt to rebias the object
 532   // toward the current thread. Note that we must be absolutely sure
 533   // that the current epoch is invalid in order to do this because
 534   // otherwise the manipulations it performs on the mark word are
 535   // illegal.
<span class="line-modified"> 536   andr(rscratch1, tmp_reg, markWord::epoch_mask_in_place);</span>
 537   cbnz(rscratch1, try_rebias);
 538 
 539   // The epoch of the current bias is still valid but we know nothing
 540   // about the owner; it might be set or it might be clear. Try to
 541   // acquire the bias of the object using an atomic operation. If this
 542   // fails we will go in to the runtime to revoke the object&#39;s bias.
 543   // Note that we first construct the presumed unbiased header so we
 544   // don&#39;t accidentally blow away another thread&#39;s valid bias.
 545   {
 546     Label here;
<span class="line-modified"> 547     mov(rscratch1, markWord::biased_lock_mask_in_place | markWord::age_mask_in_place | markWord::epoch_mask_in_place);</span>
 548     andr(swap_reg, swap_reg, rscratch1);
 549     orr(tmp_reg, swap_reg, rthread);
 550     cmpxchg_obj_header(swap_reg, tmp_reg, obj_reg, rscratch1, here, slow_case);
 551     // If the biasing toward our thread failed, this means that
 552     // another thread succeeded in biasing it toward itself and we
 553     // need to revoke that bias. The revocation will occur in the
 554     // interpreter runtime in the slow case.
 555     bind(here);
 556     if (counters != NULL) {
 557       atomic_incw(Address((address)counters-&gt;anonymously_biased_lock_entry_count_addr()),
 558                   tmp_reg, rscratch1, rscratch2);
 559     }
 560   }
 561   b(done);
 562 
 563   bind(try_rebias);
 564   // At this point we know the epoch has expired, meaning that the
 565   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
 566   // circumstances _only_, we are allowed to use the current header&#39;s
 567   // value as the comparison value when doing the cas to acquire the
</pre>
<hr />
<pre>
 612                   rscratch1, rscratch2);
 613     }
 614     bind(nope);
 615   }
 616 
 617   bind(cas_label);
 618 
 619   return null_check_offset;
 620 }
 621 
 622 void MacroAssembler::biased_locking_exit(Register obj_reg, Register temp_reg, Label&amp; done) {
 623   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 624 
 625   // Check for biased locking unlock case, which is a no-op
 626   // Note: we do not have to check the thread ID for two reasons.
 627   // First, the interpreter checks for IllegalMonitorStateException at
 628   // a higher level. Second, if the bias was revoked while we held the
 629   // lock, the object could not be rebiased toward another thread, so
 630   // the bias bit would be clear.
 631   ldr(temp_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
<span class="line-modified"> 632   andr(temp_reg, temp_reg, markWord::biased_lock_mask_in_place);</span>
<span class="line-modified"> 633   cmp(temp_reg, (u1)markWord::biased_lock_pattern);</span>
 634   br(Assembler::EQ, done);
 635 }
 636 
 637 static void pass_arg0(MacroAssembler* masm, Register arg) {
 638   if (c_rarg0 != arg ) {
 639     masm-&gt;mov(c_rarg0, arg);
 640   }
 641 }
 642 
 643 static void pass_arg1(MacroAssembler* masm, Register arg) {
 644   if (c_rarg1 != arg ) {
 645     masm-&gt;mov(c_rarg1, arg);
 646   }
 647 }
 648 
 649 static void pass_arg2(MacroAssembler* masm, Register arg) {
 650   if (c_rarg2 != arg ) {
 651     masm-&gt;mov(c_rarg2, arg);
 652   }
 653 }
</pre>
<hr />
<pre>
 797 
 798   // Now, create the trampoline stub&#39;s code:
 799   // - load the call
 800   // - call
 801   Label target;
 802   ldr(rscratch1, target);
 803   br(rscratch1);
 804   bind(target);
 805   assert(offset() - stub_start_offset == NativeCallTrampolineStub::data_offset,
 806          &quot;should be&quot;);
 807   emit_int64((int64_t)dest);
 808 
 809   const address stub_start_addr = addr_at(stub_start_offset);
 810 
 811   assert(is_NativeCallTrampolineStub_at(stub_start_addr), &quot;doesn&#39;t look like a trampoline&quot;);
 812 
 813   end_a_stub();
 814   return stub_start_addr;
 815 }
 816 
<span class="line-added"> 817 void MacroAssembler::emit_static_call_stub() {</span>
<span class="line-added"> 818   // CompiledDirectStaticCall::set_to_interpreted knows the</span>
<span class="line-added"> 819   // exact layout of this stub.</span>
<span class="line-added"> 820 </span>
<span class="line-added"> 821   isb();</span>
<span class="line-added"> 822   mov_metadata(rmethod, (Metadata*)NULL);</span>
<span class="line-added"> 823 </span>
<span class="line-added"> 824   // Jump to the entry point of the i2c stub.</span>
<span class="line-added"> 825   movptr(rscratch1, 0);</span>
<span class="line-added"> 826   br(rscratch1);</span>
<span class="line-added"> 827 }</span>
<span class="line-added"> 828 </span>
 829 void MacroAssembler::c2bool(Register x) {
 830   // implements x == 0 ? 0 : 1
 831   // note: must only look at least-significant byte of x
 832   //       since C-style booleans are stored in one byte
 833   //       only! (was bug)
 834   tst(x, 0xff);
 835   cset(x, Assembler::NE);
 836 }
 837 
 838 address MacroAssembler::ic_call(address entry, jint method_index) {
 839   RelocationHolder rh = virtual_call_Relocation::spec(pc(), method_index);
 840   // address const_ptr = long_constant((jlong)Universe::non_oop_word());
 841   // unsigned long offset;
 842   // ldr_constant(rscratch2, const_ptr);
 843   movptr(rscratch2, (uintptr_t)Universe::non_oop_word());
 844   return trampoline_call(Address(entry, rh));
 845 }
 846 
 847 // Implementation of call_VM versions
 848 
</pre>
<hr />
<pre>
 956 
 957 void MacroAssembler::check_and_handle_popframe(Register java_thread) { }
 958 
 959 
 960 RegisterOrConstant MacroAssembler::delayed_value_impl(intptr_t* delayed_value_addr,
 961                                                       Register tmp,
 962                                                       int offset) {
 963   intptr_t value = *delayed_value_addr;
 964   if (value != 0)
 965     return RegisterOrConstant(value + offset);
 966 
 967   // load indirectly to solve generation ordering problem
 968   ldr(tmp, ExternalAddress((address) delayed_value_addr));
 969 
 970   if (offset != 0)
 971     add(tmp, tmp, offset);
 972 
 973   return RegisterOrConstant(tmp);
 974 }
 975 











 976 // Look up the method for a megamorphic invokeinterface call.
 977 // The target method is determined by &lt;intf_klass, itable_index&gt;.
 978 // The receiver klass is in recv_klass.
 979 // On success, the result will be in method_result, and execution falls through.
 980 // On failure, execution transfers to the given label.
 981 void MacroAssembler::lookup_interface_method(Register recv_klass,
 982                                              Register intf_klass,
 983                                              RegisterOrConstant itable_index,
 984                                              Register method_result,
 985                                              Register scan_temp,
 986                                              Label&amp; L_no_such_interface,
 987                          bool return_method) {
 988   assert_different_registers(recv_klass, intf_klass, scan_temp);
 989   assert_different_registers(method_result, intf_klass, scan_temp);
 990   assert(recv_klass != method_result || !return_method,
 991      &quot;recv_klass can be destroyed when method isn&#39;t needed&quot;);
 992   assert(itable_index.is_constant() || itable_index.as_register() == method_result,
 993          &quot;caller must use same register for non-constant itable index as for method&quot;);
 994 
 995   // Compute start of first itableOffsetEntry (which is at the end of the vtable)
</pre>
<hr />
<pre>
1280   // Set NZ/Z based on last compare.
1281   repne_scan(r5, r0, r2, rscratch1);
1282 
1283   // Unspill the temp. registers:
1284   pop(pushed_registers, sp);
1285 
1286   br(Assembler::NE, *L_failure);
1287 
1288   // Success.  Cache the super we found and proceed in triumph.
1289   str(super_klass, super_cache_addr);
1290 
1291   if (L_success != &amp;L_fallthrough) {
1292     b(*L_success);
1293   }
1294 
1295 #undef IS_A_TEMP
1296 
1297   bind(L_fallthrough);
1298 }
1299 
<span class="line-added">1300 void MacroAssembler::clinit_barrier(Register klass, Register scratch, Label* L_fast_path, Label* L_slow_path) {</span>
<span class="line-added">1301   assert(L_fast_path != NULL || L_slow_path != NULL, &quot;at least one is required&quot;);</span>
<span class="line-added">1302   assert_different_registers(klass, rthread, scratch);</span>
<span class="line-added">1303 </span>
<span class="line-added">1304   Label L_fallthrough, L_tmp;</span>
<span class="line-added">1305   if (L_fast_path == NULL) {</span>
<span class="line-added">1306     L_fast_path = &amp;L_fallthrough;</span>
<span class="line-added">1307   } else if (L_slow_path == NULL) {</span>
<span class="line-added">1308     L_slow_path = &amp;L_fallthrough;</span>
<span class="line-added">1309   }</span>
<span class="line-added">1310   // Fast path check: class is fully initialized</span>
<span class="line-added">1311   ldrb(scratch, Address(klass, InstanceKlass::init_state_offset()));</span>
<span class="line-added">1312   subs(zr, scratch, InstanceKlass::fully_initialized);</span>
<span class="line-added">1313   br(Assembler::EQ, *L_fast_path);</span>
<span class="line-added">1314 </span>
<span class="line-added">1315   // Fast path check: current thread is initializer thread</span>
<span class="line-added">1316   ldr(scratch, Address(klass, InstanceKlass::init_thread_offset()));</span>
<span class="line-added">1317   cmp(rthread, scratch);</span>
<span class="line-added">1318 </span>
<span class="line-added">1319   if (L_slow_path == &amp;L_fallthrough) {</span>
<span class="line-added">1320     br(Assembler::EQ, *L_fast_path);</span>
<span class="line-added">1321     bind(*L_slow_path);</span>
<span class="line-added">1322   } else if (L_fast_path == &amp;L_fallthrough) {</span>
<span class="line-added">1323     br(Assembler::NE, *L_slow_path);</span>
<span class="line-added">1324     bind(*L_fast_path);</span>
<span class="line-added">1325   } else {</span>
<span class="line-added">1326     Unimplemented();</span>
<span class="line-added">1327   }</span>
<span class="line-added">1328 }</span>
1329 
1330 void MacroAssembler::verify_oop(Register reg, const char* s) {
1331   if (!VerifyOops) return;
1332 
1333   // Pass register number to verify_oop_subroutine
1334   const char* b = NULL;
1335   {
1336     ResourceMark rm;
1337     stringStream ss;
1338     ss.print(&quot;verify_oop: %s: %s&quot;, reg-&gt;name(), s);
1339     b = code_string(ss.as_string());
1340   }
1341   BLOCK_COMMENT(&quot;verify_oop {&quot;);
1342 
1343   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1344   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1345 
1346   mov(r0, reg);
1347   mov(rscratch1, (address)b);
1348 
</pre>
<hr />
<pre>
1398   // cf. TemplateTable::prepare_invoke(), if (load_receiver).
1399   int stackElementSize = Interpreter::stackElementSize;
1400   int offset = Interpreter::expr_offset_in_bytes(extra_slot_offset+0);
1401 #ifdef ASSERT
1402   int offset1 = Interpreter::expr_offset_in_bytes(extra_slot_offset+1);
1403   assert(offset1 - offset == stackElementSize, &quot;correct arithmetic&quot;);
1404 #endif
1405   if (arg_slot.is_constant()) {
1406     return Address(esp, arg_slot.as_constant() * stackElementSize
1407                    + offset);
1408   } else {
1409     add(rscratch1, esp, arg_slot.as_register(),
1410         ext::uxtx, exact_log2(stackElementSize));
1411     return Address(rscratch1, offset);
1412   }
1413 }
1414 
1415 void MacroAssembler::call_VM_leaf_base(address entry_point,
1416                                        int number_of_arguments,
1417                                        Label *retaddr) {








1418   Label E, L;
1419 
1420   stp(rscratch1, rmethod, Address(pre(sp, -2 * wordSize)));
1421 


1422   mov(rscratch1, entry_point);
<span class="line-modified">1423   blr(rscratch1);</span>
1424   if (retaddr)
1425     bind(*retaddr);
1426 
1427   ldp(rscratch1, rmethod, Address(post(sp, 2 * wordSize)));
1428   maybe_isb();
1429 }
1430 
1431 void MacroAssembler::call_VM_leaf(address entry_point, int number_of_arguments) {
1432   call_VM_leaf_base(entry_point, number_of_arguments);
1433 }
1434 
1435 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0) {
1436   pass_arg0(this, arg_0);
1437   call_VM_leaf_base(entry_point, 1);
1438 }
1439 
1440 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1441   pass_arg0(this, arg_0);
1442   pass_arg1(this, arg_1);
1443   call_VM_leaf_base(entry_point, 2);
</pre>
<hr />
<pre>
1725 // not actually be used: you must use the Address that is returned.
1726 // It is up to you to ensure that the shift provided matches the size
1727 // of your data.
1728 Address MacroAssembler::form_address(Register Rd, Register base, long byte_offset, int shift) {
1729   if (Address::offset_ok_for_immed(byte_offset, shift))
1730     // It fits; no need for any heroics
1731     return Address(base, byte_offset);
1732 
1733   // Don&#39;t do anything clever with negative or misaligned offsets
1734   unsigned mask = (1 &lt;&lt; shift) - 1;
1735   if (byte_offset &lt; 0 || byte_offset &amp; mask) {
1736     mov(Rd, byte_offset);
1737     add(Rd, base, Rd);
1738     return Address(Rd);
1739   }
1740 
1741   // See if we can do this with two 12-bit offsets
1742   {
1743     unsigned long word_offset = byte_offset &gt;&gt; shift;
1744     unsigned long masked_offset = word_offset &amp; 0xfff000;
<span class="line-modified">1745     if (Address::offset_ok_for_immed(word_offset - masked_offset, 0)</span>
1746         &amp;&amp; Assembler::operand_valid_for_add_sub_immediate(masked_offset &lt;&lt; shift)) {
1747       add(Rd, base, masked_offset &lt;&lt; shift);
1748       word_offset -= masked_offset;
1749       return Address(Rd, word_offset &lt;&lt; shift);
1750     }
1751   }
1752 
1753   // Do it the hard way
1754   mov(Rd, byte_offset);
1755   add(Rd, base, Rd);
1756   return Address(Rd);
1757 }
1758 
1759 void MacroAssembler::atomic_incw(Register counter_addr, Register tmp, Register tmp2) {
1760   if (UseLSE) {
1761     mov(tmp, 1);
1762     ldadd(Assembler::word, tmp, zr, counter_addr);
1763     return;
1764   }
1765   Label retry_load;
</pre>
<hr />
<pre>
2116     bitset &gt;&gt;= 1;
2117   }
2118   regs[count++] = zr-&gt;encoding_nocheck();
2119   count &amp;= ~1;
2120 
2121   for (int i = 2; i &lt; count; i += 2) {
2122     ldp(as_Register(regs[i]), as_Register(regs[i+1]),
2123        Address(stack, i * wordSize));
2124     words_pushed += 2;
2125   }
2126   if (count) {
2127     ldp(as_Register(regs[0]), as_Register(regs[1]),
2128        Address(post(stack, count * wordSize)));
2129     words_pushed += 2;
2130   }
2131 
2132   assert(words_pushed == count, &quot;oops, pushed != count&quot;);
2133 
2134   return count;
2135 }
<span class="line-added">2136 </span>
<span class="line-added">2137 // Push lots of registers in the bit set supplied.  Don&#39;t push sp.</span>
<span class="line-added">2138 // Return the number of words pushed</span>
<span class="line-added">2139 int MacroAssembler::push_fp(unsigned int bitset, Register stack) {</span>
<span class="line-added">2140   int words_pushed = 0;</span>
<span class="line-added">2141 </span>
<span class="line-added">2142   // Scan bitset to accumulate register pairs</span>
<span class="line-added">2143   unsigned char regs[32];</span>
<span class="line-added">2144   int count = 0;</span>
<span class="line-added">2145   for (int reg = 0; reg &lt;= 31; reg++) {</span>
<span class="line-added">2146     if (1 &amp; bitset)</span>
<span class="line-added">2147       regs[count++] = reg;</span>
<span class="line-added">2148     bitset &gt;&gt;= 1;</span>
<span class="line-added">2149   }</span>
<span class="line-added">2150   regs[count++] = zr-&gt;encoding_nocheck();</span>
<span class="line-added">2151   count &amp;= ~1;  // Only push an even number of regs</span>
<span class="line-added">2152 </span>
<span class="line-added">2153   // Always pushing full 128 bit registers.</span>
<span class="line-added">2154   if (count) {</span>
<span class="line-added">2155     stpq(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(pre(stack, -count * wordSize * 2)));</span>
<span class="line-added">2156     words_pushed += 2;</span>
<span class="line-added">2157   }</span>
<span class="line-added">2158   for (int i = 2; i &lt; count; i += 2) {</span>
<span class="line-added">2159     stpq(as_FloatRegister(regs[i]), as_FloatRegister(regs[i+1]), Address(stack, i * wordSize * 2));</span>
<span class="line-added">2160     words_pushed += 2;</span>
<span class="line-added">2161   }</span>
<span class="line-added">2162 </span>
<span class="line-added">2163   assert(words_pushed == count, &quot;oops, pushed != count&quot;);</span>
<span class="line-added">2164   return count;</span>
<span class="line-added">2165 }</span>
<span class="line-added">2166 </span>
<span class="line-added">2167 int MacroAssembler::pop_fp(unsigned int bitset, Register stack) {</span>
<span class="line-added">2168   int words_pushed = 0;</span>
<span class="line-added">2169 </span>
<span class="line-added">2170   // Scan bitset to accumulate register pairs</span>
<span class="line-added">2171   unsigned char regs[32];</span>
<span class="line-added">2172   int count = 0;</span>
<span class="line-added">2173   for (int reg = 0; reg &lt;= 31; reg++) {</span>
<span class="line-added">2174     if (1 &amp; bitset)</span>
<span class="line-added">2175       regs[count++] = reg;</span>
<span class="line-added">2176     bitset &gt;&gt;= 1;</span>
<span class="line-added">2177   }</span>
<span class="line-added">2178   regs[count++] = zr-&gt;encoding_nocheck();</span>
<span class="line-added">2179   count &amp;= ~1;</span>
<span class="line-added">2180 </span>
<span class="line-added">2181   for (int i = 2; i &lt; count; i += 2) {</span>
<span class="line-added">2182     ldpq(as_FloatRegister(regs[i]), as_FloatRegister(regs[i+1]), Address(stack, i * wordSize * 2));</span>
<span class="line-added">2183     words_pushed += 2;</span>
<span class="line-added">2184   }</span>
<span class="line-added">2185   if (count) {</span>
<span class="line-added">2186     ldpq(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(post(stack, count * wordSize * 2)));</span>
<span class="line-added">2187     words_pushed += 2;</span>
<span class="line-added">2188   }</span>
<span class="line-added">2189 </span>
<span class="line-added">2190   assert(words_pushed == count, &quot;oops, pushed != count&quot;);</span>
<span class="line-added">2191 </span>
<span class="line-added">2192   return count;</span>
<span class="line-added">2193 }</span>
<span class="line-added">2194 </span>
2195 #ifdef ASSERT
2196 void MacroAssembler::verify_heapbase(const char* msg) {
2197 #if 0
2198   assert (UseCompressedOops || UseCompressedClassPointers, &quot;should be compressed&quot;);
2199   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
2200   if (CheckCompressedOops) {
2201     Label ok;
2202     push(1 &lt;&lt; rscratch1-&gt;encoding(), sp); // cmpptr trashes rscratch1
<span class="line-modified">2203     cmpptr(rheapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));</span>
2204     br(Assembler::EQ, ok);
2205     stop(msg);
2206     bind(ok);
2207     pop(1 &lt;&lt; rscratch1-&gt;encoding(), sp);
2208   }
2209 #endif
2210 }
2211 #endif
2212 
2213 void MacroAssembler::resolve_jobject(Register value, Register thread, Register tmp) {
2214   Label done, not_weak;
2215   cbz(value, done);           // Use NULL as-is.
2216 
2217   STATIC_ASSERT(JNIHandles::weak_tag_mask == 1u);
2218   tbz(r0, 0, not_weak);    // Test for jweak tag.
2219 
2220   // Resolve jweak.
2221   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF, value,
2222                  Address(value, -JNIHandles::weak_tag_value), tmp, thread);
2223   verify_oop(value);
2224   b(done);
2225 
2226   bind(not_weak);
2227   // Resolve (untagged) jobject.
2228   access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, 0), tmp, thread);
2229   verify_oop(value);
2230   bind(done);
2231 }
2232 
2233 void MacroAssembler::stop(const char* msg) {
2234   address ip = pc();
2235   pusha();
2236   mov(c_rarg0, (address)msg);
2237   mov(c_rarg1, (address)ip);
2238   mov(c_rarg2, sp);
2239   mov(c_rarg3, CAST_FROM_FN_PTR(address, MacroAssembler::debug64));
<span class="line-modified">2240   blr(c_rarg3);</span>

2241   hlt(0);
2242 }
2243 
2244 void MacroAssembler::warn(const char* msg) {
2245   pusha();
2246   mov(c_rarg0, (address)msg);
2247   mov(lr, CAST_FROM_FN_PTR(address, warning));
<span class="line-modified">2248   blr(lr);</span>
2249   popa();
2250 }
2251 
2252 void MacroAssembler::unimplemented(const char* what) {
2253   const char* buf = NULL;
2254   {
2255     ResourceMark rm;
2256     stringStream ss;
2257     ss.print(&quot;unimplemented: %s&quot;, what);
2258     buf = code_string(ss.as_string());
2259   }
2260   stop(buf);
2261 }
2262 
2263 // If a constant does not fit in an immediate field, generate some
2264 // number of MOV instructions and then perform the operation.
2265 void MacroAssembler::wrap_add_sub_imm_insn(Register Rd, Register Rn, unsigned imm,
2266                                            add_sub_imm_insn insn1,
2267                                            add_sub_reg_insn insn2) {
2268   assert(Rd != zr, &quot;Rd = zr and not setting flags?&quot;);
</pre>
<hr />
<pre>
2315 void MacroAssembler::sub(Register Rd, Register Rn, RegisterOrConstant decrement) {
2316   if (decrement.is_register()) {
2317     sub(Rd, Rn, decrement.as_register());
2318   } else {
2319     sub(Rd, Rn, decrement.as_constant());
2320   }
2321 }
2322 
2323 void MacroAssembler::subw(Register Rd, Register Rn, RegisterOrConstant decrement) {
2324   if (decrement.is_register()) {
2325     subw(Rd, Rn, decrement.as_register());
2326   } else {
2327     subw(Rd, Rn, decrement.as_constant());
2328   }
2329 }
2330 
2331 void MacroAssembler::reinit_heapbase()
2332 {
2333   if (UseCompressedOops) {
2334     if (Universe::is_fully_initialized()) {
<span class="line-modified">2335       mov(rheapbase, CompressedOops::ptrs_base());</span>
2336     } else {
<span class="line-modified">2337       lea(rheapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));</span>
2338       ldr(rheapbase, Address(rheapbase));
2339     }
2340   }
2341 }
2342 
2343 // this simulates the behaviour of the x86 cmpxchg instruction using a
2344 // load linked/store conditional pair. we use the acquire/release
2345 // versions of these instructions so that we flush pending writes as
2346 // per Java semantics.
2347 
2348 // n.b the x86 version assumes the old value to be compared against is
2349 // in rax and updates rax with the value located in memory if the
2350 // cmpxchg fails. we supply a register for the old value explicitly
2351 
2352 // the aarch64 load linked/store conditional instructions do not
2353 // accept an offset. so, unlike x86, we must provide a plain register
2354 // to identify the memory word to be compared/exchanged rather than a
2355 // register+offset Address.
2356 
2357 void MacroAssembler::cmpxchgptr(Register oldv, Register newv, Register addr, Register tmp,
</pre>
<hr />
<pre>
2600       tty-&gt;print_cr(&quot;r13 = 0x%016lx&quot;, regs[13]);
2601       tty-&gt;print_cr(&quot;r14 = 0x%016lx&quot;, regs[14]);
2602       tty-&gt;print_cr(&quot;r15 = 0x%016lx&quot;, regs[15]);
2603       tty-&gt;print_cr(&quot;r16 = 0x%016lx&quot;, regs[16]);
2604       tty-&gt;print_cr(&quot;r17 = 0x%016lx&quot;, regs[17]);
2605       tty-&gt;print_cr(&quot;r18 = 0x%016lx&quot;, regs[18]);
2606       tty-&gt;print_cr(&quot;r19 = 0x%016lx&quot;, regs[19]);
2607       tty-&gt;print_cr(&quot;r20 = 0x%016lx&quot;, regs[20]);
2608       tty-&gt;print_cr(&quot;r21 = 0x%016lx&quot;, regs[21]);
2609       tty-&gt;print_cr(&quot;r22 = 0x%016lx&quot;, regs[22]);
2610       tty-&gt;print_cr(&quot;r23 = 0x%016lx&quot;, regs[23]);
2611       tty-&gt;print_cr(&quot;r24 = 0x%016lx&quot;, regs[24]);
2612       tty-&gt;print_cr(&quot;r25 = 0x%016lx&quot;, regs[25]);
2613       tty-&gt;print_cr(&quot;r26 = 0x%016lx&quot;, regs[26]);
2614       tty-&gt;print_cr(&quot;r27 = 0x%016lx&quot;, regs[27]);
2615       tty-&gt;print_cr(&quot;r28 = 0x%016lx&quot;, regs[28]);
2616       tty-&gt;print_cr(&quot;r30 = 0x%016lx&quot;, regs[30]);
2617       tty-&gt;print_cr(&quot;r31 = 0x%016lx&quot;, regs[31]);
2618       BREAKPOINT;
2619     }






2620   }
<span class="line-added">2621   fatal(&quot;DEBUG MESSAGE: %s&quot;, msg);</span>
2622 }
2623 












































2624 void MacroAssembler::push_call_clobbered_registers() {
2625   int step = 4 * wordSize;
2626   push(RegSet::range(r0, r18) - RegSet::of(rscratch1, rscratch2), sp);
2627   sub(sp, sp, step);
2628   mov(rscratch1, -step);
2629   // Push v0-v7, v16-v31.
2630   for (int i = 31; i&gt;= 4; i -= 4) {
2631     if (i &lt;= v7-&gt;encoding() || i &gt;= v16-&gt;encoding())
2632       st1(as_FloatRegister(i-3), as_FloatRegister(i-2), as_FloatRegister(i-1),
2633           as_FloatRegister(i), T1D, Address(post(sp, rscratch1)));
2634   }
2635   st1(as_FloatRegister(0), as_FloatRegister(1), as_FloatRegister(2),
2636       as_FloatRegister(3), T1D, Address(sp));
2637 }
2638 
2639 void MacroAssembler::pop_call_clobbered_registers() {
2640   for (int i = 0; i &lt; 32; i += 4) {
2641     if (i &lt;= v7-&gt;encoding() || i &gt;= v16-&gt;encoding())
2642       ld1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
2643           as_FloatRegister(i+3), T1D, Address(post(sp, 4 * wordSize)));
</pre>
<hr />
<pre>
2682 Address MacroAssembler::offsetted_address(Register r, Register r1,
2683                                           Address::extend ext, int offset, int size) {
2684   if (offset || (ext.shift() % size != 0)) {
2685     lea(rscratch2, Address(r, r1, ext));
2686     return Address(rscratch2, offset);
2687   } else {
2688     return Address(r, r1, ext);
2689   }
2690 }
2691 
2692 Address MacroAssembler::spill_address(int size, int offset, Register tmp)
2693 {
2694   assert(offset &gt;= 0, &quot;spill to negative address?&quot;);
2695   // Offset reachable ?
2696   //   Not aligned - 9 bits signed offset
2697   //   Aligned - 12 bits unsigned offset shifted
2698   Register base = sp;
2699   if ((offset &amp; (size-1)) &amp;&amp; offset &gt;= (1&lt;&lt;8)) {
2700     add(tmp, base, offset &amp; ((1&lt;&lt;12)-1));
2701     base = tmp;
<span class="line-modified">2702     offset &amp;= -1u&lt;&lt;12;</span>
2703   }
2704 
2705   if (offset &gt;= (1&lt;&lt;12) * size) {
2706     add(tmp, base, offset &amp; (((1&lt;&lt;12)-1)&lt;&lt;12));
2707     base = tmp;
2708     offset &amp;= ~(((1&lt;&lt;12)-1)&lt;&lt;12);
2709   }
2710 
2711   return Address(base, offset);
2712 }
2713 
2714 // Checks whether offset is aligned.
2715 // Returns true if it is, else false.
2716 bool MacroAssembler::merge_alignment_check(Register base,
2717                                            size_t size,
2718                                            long cur_offset,
2719                                            long prev_offset) const {
2720   if (AvoidUnalignedAccesses) {
2721     if (base == sp) {
2722       // Checks whether low offset if aligned to pair of registers.
</pre>
<hr />
<pre>
3684     adr = Address(rscratch2);
3685     break;
3686   }
3687   ldr(rscratch1, adr);
3688   add(rscratch1, rscratch1, src);
3689   str(rscratch1, adr);
3690 }
3691 
3692 void MacroAssembler::cmpptr(Register src1, Address src2) {
3693   unsigned long offset;
3694   adrp(rscratch1, src2, offset);
3695   ldr(rscratch1, Address(rscratch1, offset));
3696   cmp(src1, rscratch1);
3697 }
3698 
3699 void MacroAssembler::cmpoop(Register obj1, Register obj2) {
3700   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
3701   bs-&gt;obj_equals(this, obj1, obj2);
3702 }
3703 
<span class="line-added">3704 void MacroAssembler::load_method_holder(Register holder, Register method) {</span>
<span class="line-added">3705   ldr(holder, Address(method, Method::const_offset()));                      // ConstMethod*</span>
<span class="line-added">3706   ldr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*</span>
<span class="line-added">3707   ldr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*</span>
<span class="line-added">3708 }</span>
<span class="line-added">3709 </span>
3710 void MacroAssembler::load_klass(Register dst, Register src) {
3711   if (UseCompressedClassPointers) {
3712     ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3713     decode_klass_not_null(dst);
3714   } else {
3715     ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3716   }
3717 }
3718 
3719 // ((OopHandle)result).resolve();
3720 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
3721   // OopHandle::resolve is an indirection.
3722   access_load_at(T_OBJECT, IN_NATIVE, result, Address(result, 0), tmp, noreg);
3723 }
3724 
3725 void MacroAssembler::load_mirror(Register dst, Register method, Register tmp) {
3726   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
3727   ldr(dst, Address(rmethod, Method::const_offset()));
3728   ldr(dst, Address(dst, ConstMethod::constants_offset()));
3729   ldr(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));
3730   ldr(dst, Address(dst, mirror_offset));
3731   resolve_oop_handle(dst, tmp);
3732 }
3733 
3734 void MacroAssembler::cmp_klass(Register oop, Register trial_klass, Register tmp) {
3735   if (UseCompressedClassPointers) {
3736     ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
<span class="line-modified">3737     if (CompressedKlassPointers::base() == NULL) {</span>
<span class="line-modified">3738       cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());</span>
3739       return;
<span class="line-modified">3740     } else if (((uint64_t)CompressedKlassPointers::base() &amp; 0xffffffff) == 0</span>
<span class="line-modified">3741                &amp;&amp; CompressedKlassPointers::shift() == 0) {</span>
3742       // Only the bottom 32 bits matter
3743       cmpw(trial_klass, tmp);
3744       return;
3745     }
3746     decode_klass_not_null(tmp);
3747   } else {
3748     ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3749   }
3750   cmp(trial_klass, tmp);
3751 }
3752 
3753 void MacroAssembler::load_prototype_header(Register dst, Register src) {
3754   load_klass(dst, src);
3755   ldr(dst, Address(dst, Klass::prototype_header_offset()));
3756 }
3757 
3758 void MacroAssembler::store_klass(Register dst, Register src) {
3759   // FIXME: Should this be a store release?  concurrent gcs assumes
3760   // klass length is valid if klass field is not null.
3761   if (UseCompressedClassPointers) {
3762     encode_klass_not_null(src);
3763     strw(src, Address(dst, oopDesc::klass_offset_in_bytes()));
3764   } else {
3765     str(src, Address(dst, oopDesc::klass_offset_in_bytes()));
3766   }
3767 }
3768 
3769 void MacroAssembler::store_klass_gap(Register dst, Register src) {
3770   if (UseCompressedClassPointers) {
3771     // Store to klass gap in destination
3772     strw(src, Address(dst, oopDesc::klass_gap_offset_in_bytes()));
3773   }
3774 }
3775 
3776 // Algorithm must match CompressedOops::encode.
3777 void MacroAssembler::encode_heap_oop(Register d, Register s) {
3778 #ifdef ASSERT
3779   verify_heapbase(&quot;MacroAssembler::encode_heap_oop: heap base corrupted?&quot;);
3780 #endif
3781   verify_oop(s, &quot;broken oop in encode_heap_oop&quot;);
<span class="line-modified">3782   if (CompressedOops::base() == NULL) {</span>
<span class="line-modified">3783     if (CompressedOops::shift() != 0) {</span>
<span class="line-modified">3784       assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
3785       lsr(d, s, LogMinObjAlignmentInBytes);
3786     } else {
3787       mov(d, s);
3788     }
3789   } else {
3790     subs(d, s, rheapbase);
3791     csel(d, d, zr, Assembler::HS);
3792     lsr(d, d, LogMinObjAlignmentInBytes);
3793 
3794     /*  Old algorithm: is this any worse?
3795     Label nonnull;
3796     cbnz(r, nonnull);
3797     sub(r, r, rheapbase);
3798     bind(nonnull);
3799     lsr(r, r, LogMinObjAlignmentInBytes);
3800     */
3801   }
3802 }
3803 
3804 void MacroAssembler::encode_heap_oop_not_null(Register r) {
3805 #ifdef ASSERT
3806   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null: heap base corrupted?&quot;);
3807   if (CheckCompressedOops) {
3808     Label ok;
3809     cbnz(r, ok);
3810     stop(&quot;null oop passed to encode_heap_oop_not_null&quot;);
3811     bind(ok);
3812   }
3813 #endif
3814   verify_oop(r, &quot;broken oop in encode_heap_oop_not_null&quot;);
<span class="line-modified">3815   if (CompressedOops::base() != NULL) {</span>
3816     sub(r, r, rheapbase);
3817   }
<span class="line-modified">3818   if (CompressedOops::shift() != 0) {</span>
<span class="line-modified">3819     assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
3820     lsr(r, r, LogMinObjAlignmentInBytes);
3821   }
3822 }
3823 
3824 void MacroAssembler::encode_heap_oop_not_null(Register dst, Register src) {
3825 #ifdef ASSERT
3826   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null2: heap base corrupted?&quot;);
3827   if (CheckCompressedOops) {
3828     Label ok;
3829     cbnz(src, ok);
3830     stop(&quot;null oop passed to encode_heap_oop_not_null2&quot;);
3831     bind(ok);
3832   }
3833 #endif
3834   verify_oop(src, &quot;broken oop in encode_heap_oop_not_null2&quot;);
3835 
3836   Register data = src;
<span class="line-modified">3837   if (CompressedOops::base() != NULL) {</span>
3838     sub(dst, src, rheapbase);
3839     data = dst;
3840   }
<span class="line-modified">3841   if (CompressedOops::shift() != 0) {</span>
<span class="line-modified">3842     assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
3843     lsr(dst, data, LogMinObjAlignmentInBytes);
3844     data = dst;
3845   }
3846   if (data == src)
3847     mov(dst, src);
3848 }
3849 
3850 void  MacroAssembler::decode_heap_oop(Register d, Register s) {
3851 #ifdef ASSERT
3852   verify_heapbase(&quot;MacroAssembler::decode_heap_oop: heap base corrupted?&quot;);
3853 #endif
<span class="line-modified">3854   if (CompressedOops::base() == NULL) {</span>
<span class="line-modified">3855     if (CompressedOops::shift() != 0 || d != s) {</span>
<span class="line-modified">3856       lsl(d, s, CompressedOops::shift());</span>
3857     }
3858   } else {
3859     Label done;
3860     if (d != s)
3861       mov(d, s);
3862     cbz(s, done);
3863     add(d, rheapbase, s, Assembler::LSL, LogMinObjAlignmentInBytes);
3864     bind(done);
3865   }
3866   verify_oop(d, &quot;broken oop in decode_heap_oop&quot;);
3867 }
3868 
3869 void  MacroAssembler::decode_heap_oop_not_null(Register r) {
3870   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
3871   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
3872   // Cannot assert, unverified entry point counts instructions (see .ad file)
3873   // vtableStubs also counts instructions in pd_code_size_limit.
3874   // Also do not verify_oop as this is called by verify_oop.
<span class="line-modified">3875   if (CompressedOops::shift() != 0) {</span>
<span class="line-modified">3876     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
<span class="line-modified">3877     if (CompressedOops::base() != NULL) {</span>
3878       add(r, rheapbase, r, Assembler::LSL, LogMinObjAlignmentInBytes);
3879     } else {
3880       add(r, zr, r, Assembler::LSL, LogMinObjAlignmentInBytes);
3881     }
3882   } else {
<span class="line-modified">3883     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);</span>
3884   }
3885 }
3886 
3887 void  MacroAssembler::decode_heap_oop_not_null(Register dst, Register src) {
3888   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
3889   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
3890   // Cannot assert, unverified entry point counts instructions (see .ad file)
3891   // vtableStubs also counts instructions in pd_code_size_limit.
3892   // Also do not verify_oop as this is called by verify_oop.
<span class="line-modified">3893   if (CompressedOops::shift() != 0) {</span>
<span class="line-modified">3894     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
<span class="line-modified">3895     if (CompressedOops::base() != NULL) {</span>
3896       add(dst, rheapbase, src, Assembler::LSL, LogMinObjAlignmentInBytes);
3897     } else {
3898       add(dst, zr, src, Assembler::LSL, LogMinObjAlignmentInBytes);
3899     }
3900   } else {
<span class="line-modified">3901     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);</span>
3902     if (dst != src) {
3903       mov(dst, src);
3904     }
3905   }
3906 }
3907 
<span class="line-added">3908 MacroAssembler::KlassDecodeMode MacroAssembler::_klass_decode_mode(KlassDecodeNone);</span>
<span class="line-added">3909 </span>
<span class="line-added">3910 MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode() {</span>
<span class="line-added">3911   assert(UseCompressedClassPointers, &quot;not using compressed class pointers&quot;);</span>
<span class="line-added">3912   assert(Metaspace::initialized(), &quot;metaspace not initialized yet&quot;);</span>
<span class="line-added">3913 </span>
<span class="line-added">3914   if (_klass_decode_mode != KlassDecodeNone) {</span>
<span class="line-added">3915     return _klass_decode_mode;</span>
<span class="line-added">3916   }</span>
<span class="line-added">3917 </span>
<span class="line-added">3918   assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift()</span>
<span class="line-added">3919          || 0 == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);</span>
<span class="line-added">3920 </span>
<span class="line-added">3921   if (CompressedKlassPointers::base() == NULL) {</span>
<span class="line-added">3922     return (_klass_decode_mode = KlassDecodeZero);</span>
<span class="line-added">3923   }</span>
<span class="line-added">3924 </span>
<span class="line-added">3925   if (operand_valid_for_logical_immediate(</span>
<span class="line-added">3926         /*is32*/false, (uint64_t)CompressedKlassPointers::base())) {</span>
<span class="line-added">3927     const uint64_t range_mask =</span>
<span class="line-added">3928       (1UL &lt;&lt; log2_intptr(CompressedKlassPointers::range())) - 1;</span>
<span class="line-added">3929     if (((uint64_t)CompressedKlassPointers::base() &amp; range_mask) == 0) {</span>
<span class="line-added">3930       return (_klass_decode_mode = KlassDecodeXor);</span>
<span class="line-added">3931     }</span>
<span class="line-added">3932   }</span>
<span class="line-added">3933 </span>
<span class="line-added">3934   const uint64_t shifted_base =</span>
<span class="line-added">3935     (uint64_t)CompressedKlassPointers::base() &gt;&gt; CompressedKlassPointers::shift();</span>
<span class="line-added">3936   guarantee((shifted_base &amp; 0xffff0000ffffffff) == 0,</span>
<span class="line-added">3937             &quot;compressed class base bad alignment&quot;);</span>
<span class="line-added">3938 </span>
<span class="line-added">3939   return (_klass_decode_mode = KlassDecodeMovk);</span>
<span class="line-added">3940 }</span>
<span class="line-added">3941 </span>
3942 void MacroAssembler::encode_klass_not_null(Register dst, Register src) {
<span class="line-modified">3943   switch (klass_decode_mode()) {</span>
<span class="line-modified">3944   case KlassDecodeZero:</span>
<span class="line-modified">3945     if (CompressedKlassPointers::shift() != 0) {</span>
3946       lsr(dst, src, LogKlassAlignmentInBytes);
3947     } else {
3948       if (dst != src) mov(dst, src);
3949     }
<span class="line-modified">3950     break;</span>

3951 
<span class="line-modified">3952   case KlassDecodeXor:</span>
<span class="line-modified">3953     if (CompressedKlassPointers::shift() != 0) {</span>
<span class="line-modified">3954       eor(dst, src, (uint64_t)CompressedKlassPointers::base());</span>
3955       lsr(dst, dst, LogKlassAlignmentInBytes);
3956     } else {
<span class="line-modified">3957       eor(dst, src, (uint64_t)CompressedKlassPointers::base());</span>
3958     }
<span class="line-modified">3959     break;</span>







3960 
<span class="line-modified">3961   case KlassDecodeMovk:</span>
<span class="line-modified">3962     if (CompressedKlassPointers::shift() != 0) {</span>
<span class="line-modified">3963       ubfx(dst, src, LogKlassAlignmentInBytes, 32);</span>
<span class="line-added">3964     } else {</span>
<span class="line-added">3965       movw(dst, src);</span>
<span class="line-added">3966     }</span>
<span class="line-added">3967     break;</span>
3968 
<span class="line-modified">3969   case KlassDecodeNone:</span>
<span class="line-modified">3970     ShouldNotReachHere();</span>
<span class="line-modified">3971     break;</span>




3972   }

3973 }
3974 
3975 void MacroAssembler::encode_klass_not_null(Register r) {
3976   encode_klass_not_null(r, r);
3977 }
3978 
3979 void  MacroAssembler::decode_klass_not_null(Register dst, Register src) {

3980   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
3981 
<span class="line-modified">3982   switch (klass_decode_mode()) {</span>
<span class="line-modified">3983   case KlassDecodeZero:</span>
<span class="line-modified">3984     if (CompressedKlassPointers::shift() != 0) {</span>
3985       lsl(dst, src, LogKlassAlignmentInBytes);
3986     } else {
3987       if (dst != src) mov(dst, src);
3988     }
<span class="line-modified">3989     break;</span>

3990 
<span class="line-modified">3991   case KlassDecodeXor:</span>
<span class="line-modified">3992     if (CompressedKlassPointers::shift() != 0) {</span>
3993       lsl(dst, src, LogKlassAlignmentInBytes);
<span class="line-modified">3994       eor(dst, dst, (uint64_t)CompressedKlassPointers::base());</span>
3995     } else {
<span class="line-modified">3996       eor(dst, src, (uint64_t)CompressedKlassPointers::base());</span>
3997     }
<span class="line-modified">3998     break;</span>

3999 
<span class="line-modified">4000   case KlassDecodeMovk: {</span>
<span class="line-modified">4001     const uint64_t shifted_base =</span>
<span class="line-modified">4002       (uint64_t)CompressedKlassPointers::base() &gt;&gt; CompressedKlassPointers::shift();</span>
<span class="line-modified">4003 </span>
<span class="line-modified">4004     if (dst != src) movw(dst, src);</span>
<span class="line-modified">4005     movk(dst, shifted_base &gt;&gt; 32, 32);</span>
<span class="line-added">4006 </span>
<span class="line-added">4007     if (CompressedKlassPointers::shift() != 0) {</span>
<span class="line-added">4008       lsl(dst, dst, LogKlassAlignmentInBytes);</span>
<span class="line-added">4009     }</span>
<span class="line-added">4010 </span>
<span class="line-added">4011     break;</span>
4012   }
4013 
<span class="line-modified">4014   case KlassDecodeNone:</span>
<span class="line-modified">4015     ShouldNotReachHere();</span>
<span class="line-modified">4016     break;</span>







4017   }

4018 }
4019 
4020 void  MacroAssembler::decode_klass_not_null(Register r) {
4021   decode_klass_not_null(r, r);
4022 }
4023 
4024 void  MacroAssembler::set_narrow_oop(Register dst, jobject obj) {
4025 #ifdef ASSERT
4026   {
4027     ThreadInVMfromUnknown tiv;
4028     assert (UseCompressedOops, &quot;should only be used for compressed oops&quot;);
4029     assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
4030     assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
<span class="line-modified">4031     assert(Universe::heap()-&gt;is_in(JNIHandles::resolve(obj)), &quot;should be real oop&quot;);</span>
4032   }
4033 #endif
4034   int oop_index = oop_recorder()-&gt;find_index(obj);
4035   InstructionMark im(this);
4036   RelocationHolder rspec = oop_Relocation::spec(oop_index);
4037   code_section()-&gt;relocate(inst_mark(), rspec);
4038   movz(dst, 0xDEAD, 16);
4039   movk(dst, 0xBEEF);
4040 }
4041 
4042 void  MacroAssembler::set_narrow_klass(Register dst, Klass* k) {
4043   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
4044   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4045   int index = oop_recorder()-&gt;find_index(k);
<span class="line-modified">4046   assert(! Universe::heap()-&gt;is_in(k), &quot;should not be an oop&quot;);</span>
4047 
4048   InstructionMark im(this);
4049   RelocationHolder rspec = metadata_Relocation::spec(index);
4050   code_section()-&gt;relocate(inst_mark(), rspec);
<span class="line-modified">4051   narrowKlass nk = CompressedKlassPointers::encode(k);</span>
4052   movz(dst, (nk &gt;&gt; 16), 16);
4053   movk(dst, nk &amp; 0xffff);
4054 }
4055 
4056 void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators,
4057                                     Register dst, Address src,
4058                                     Register tmp1, Register thread_tmp) {
4059   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4060   decorators = AccessInternal::decorator_fixup(decorators);
4061   bool as_raw = (decorators &amp; AS_RAW) != 0;
4062   if (as_raw) {
4063     bs-&gt;BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4064   } else {
4065     bs-&gt;load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4066   }
4067 }
4068 
4069 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators,
4070                                      Address dst, Register src,
4071                                      Register tmp1, Register thread_tmp) {
</pre>
<hr />
<pre>
4110 
4111 Address MacroAssembler::allocate_metadata_address(Metadata* obj) {
4112   assert(oop_recorder() != NULL, &quot;this assembler needs a Recorder&quot;);
4113   int index = oop_recorder()-&gt;allocate_metadata_index(obj);
4114   RelocationHolder rspec = metadata_Relocation::spec(index);
4115   return Address((address)obj, rspec);
4116 }
4117 
4118 // Move an oop into a register.  immediate is true if we want
4119 // immediate instrcutions, i.e. we are not going to patch this
4120 // instruction while the code is being executed by another thread.  In
4121 // that case we can use move immediates rather than the constant pool.
4122 void MacroAssembler::movoop(Register dst, jobject obj, bool immediate) {
4123   int oop_index;
4124   if (obj == NULL) {
4125     oop_index = oop_recorder()-&gt;allocate_oop_index(obj);
4126   } else {
4127 #ifdef ASSERT
4128     {
4129       ThreadInVMfromUnknown tiv;
<span class="line-modified">4130       assert(Universe::heap()-&gt;is_in(JNIHandles::resolve(obj)), &quot;should be real oop&quot;);</span>
4131     }
4132 #endif
4133     oop_index = oop_recorder()-&gt;find_index(obj);
4134   }
4135   RelocationHolder rspec = oop_Relocation::spec(oop_index);
4136   if (! immediate) {
4137     address dummy = address(uintptr_t(pc()) &amp; -wordSize); // A nearby aligned address
4138     ldr_constant(dst, Address(dummy, rspec));
4139   } else
4140     mov(dst, Address((address)obj, rspec));
4141 }
4142 
4143 // Move a metadata address into a register.
4144 void MacroAssembler::mov_metadata(Register dst, Metadata* obj) {
4145   int oop_index;
4146   if (obj == NULL) {
4147     oop_index = oop_recorder()-&gt;allocate_metadata_index(obj);
4148   } else {
4149     oop_index = oop_recorder()-&gt;find_index(obj);
4150   }
4151   RelocationHolder rspec = metadata_Relocation::spec(oop_index);
4152   mov(dst, Address((address)obj, rspec));
4153 }
4154 
4155 Address MacroAssembler::constant_oop_address(jobject obj) {
4156 #ifdef ASSERT
4157   {
4158     ThreadInVMfromUnknown tiv;
4159     assert(oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
<span class="line-modified">4160     assert(Universe::heap()-&gt;is_in(JNIHandles::resolve(obj)), &quot;not an oop&quot;);</span>
4161   }
4162 #endif
4163   int oop_index = oop_recorder()-&gt;find_index(obj);
4164   return Address((address)obj, oop_Relocation::spec(oop_index));
4165 }
4166 
4167 // Defines obj, preserves var_size_in_bytes, okay for t2 == var_size_in_bytes.
4168 void MacroAssembler::tlab_allocate(Register obj,
4169                                    Register var_size_in_bytes,
4170                                    int con_size_in_bytes,
4171                                    Register t1,
4172                                    Register t2,
4173                                    Label&amp; slow_case) {
4174   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4175   bs-&gt;tlab_allocate(this, obj, var_size_in_bytes, con_size_in_bytes, t1, t2, slow_case);
4176 }
4177 
4178 // Defines obj, preserves var_size_in_bytes
4179 void MacroAssembler::eden_allocate(Register obj,
4180                                    Register var_size_in_bytes,
</pre>
<hr />
<pre>
4903     cmpw(ch, ch1);
4904     br(EQ, MATCH);
4905     adds(cnt1_neg, cnt1_neg, 2);
4906     br(LT, DO1_LOOP);
4907   BIND(NOMATCH);
4908     mov(result, -1);
4909     b(DONE);
4910   BIND(MATCH);
4911     add(result, result_tmp, cnt1_neg, ASR, 1);
4912   BIND(DONE);
4913 }
4914 
4915 // Compare strings.
4916 void MacroAssembler::string_compare(Register str1, Register str2,
4917     Register cnt1, Register cnt2, Register result, Register tmp1, Register tmp2,
4918     FloatRegister vtmp1, FloatRegister vtmp2, FloatRegister vtmp3, int ae) {
4919   Label DONE, SHORT_LOOP, SHORT_STRING, SHORT_LAST, TAIL, STUB,
4920       DIFFERENCE, NEXT_WORD, SHORT_LOOP_TAIL, SHORT_LAST2, SHORT_LAST_INIT,
4921       SHORT_LOOP_START, TAIL_CHECK;
4922 

4923   bool isLL = ae == StrIntrinsicNode::LL;
4924   bool isLU = ae == StrIntrinsicNode::LU;
4925   bool isUL = ae == StrIntrinsicNode::UL;
4926 
<span class="line-added">4927   // The stub threshold for LL strings is: 72 (64 + 8) chars</span>
<span class="line-added">4928   // UU: 36 chars, or 72 bytes (valid for the 64-byte large loop with prefetch)</span>
<span class="line-added">4929   // LU/UL: 24 chars, or 48 bytes (valid for the 16-character loop at least)</span>
<span class="line-added">4930   const u1 stub_threshold = isLL ? 72 : ((isLU || isUL) ? 24 : 36);</span>
<span class="line-added">4931 </span>
4932   bool str1_isL = isLL || isLU;
4933   bool str2_isL = isLL || isUL;
4934 
4935   int str1_chr_shift = str1_isL ? 0 : 1;
4936   int str2_chr_shift = str2_isL ? 0 : 1;
4937   int str1_chr_size = str1_isL ? 1 : 2;
4938   int str2_chr_size = str2_isL ? 1 : 2;
4939   int minCharsInWord = isLL ? wordSize : wordSize/2;
4940 
4941   FloatRegister vtmpZ = vtmp1, vtmp = vtmp2;
4942   chr_insn str1_load_chr = str1_isL ? (chr_insn)&amp;MacroAssembler::ldrb :
4943                                       (chr_insn)&amp;MacroAssembler::ldrh;
4944   chr_insn str2_load_chr = str2_isL ? (chr_insn)&amp;MacroAssembler::ldrb :
4945                                       (chr_insn)&amp;MacroAssembler::ldrh;
4946   uxt_insn ext_chr = isLL ? (uxt_insn)&amp;MacroAssembler::uxtbw :
4947                             (uxt_insn)&amp;MacroAssembler::uxthw;
4948 
4949   BLOCK_COMMENT(&quot;string_compare {&quot;);
4950 
4951   // Bizzarely, the counts are passed in bytes, regardless of whether they
4952   // are L or U strings, however the result is always in characters.
4953   if (!str1_isL) asrw(cnt1, cnt1, 1);
4954   if (!str2_isL) asrw(cnt2, cnt2, 1);
4955 
4956   // Compute the minimum of the string lengths and save the difference.
4957   subsw(result, cnt1, cnt2);
4958   cselw(cnt2, cnt1, cnt2, Assembler::LE); // min
4959 
4960   // A very short string
4961   cmpw(cnt2, minCharsInWord);
4962   br(Assembler::LE, SHORT_STRING);
4963 
4964   // Compare longwords
4965   // load first parts of strings and finish initialization while loading
4966   {
4967     if (str1_isL == str2_isL) { // LL or UU
4968       ldr(tmp1, Address(str1));
4969       cmp(str1, str2);
4970       br(Assembler::EQ, DONE);
4971       ldr(tmp2, Address(str2));
<span class="line-modified">4972       cmp(cnt2, stub_threshold);</span>
4973       br(GE, STUB);
4974       subsw(cnt2, cnt2, minCharsInWord);
4975       br(EQ, TAIL_CHECK);
4976       lea(str2, Address(str2, cnt2, Address::uxtw(str2_chr_shift)));
4977       lea(str1, Address(str1, cnt2, Address::uxtw(str1_chr_shift)));
4978       sub(cnt2, zr, cnt2, LSL, str2_chr_shift);
4979     } else if (isLU) {
4980       ldrs(vtmp, Address(str1));


4981       ldr(tmp2, Address(str2));
<span class="line-modified">4982       cmp(cnt2, stub_threshold);</span>
4983       br(GE, STUB);
4984       subw(cnt2, cnt2, 4);
4985       eor(vtmpZ, T16B, vtmpZ, vtmpZ);
4986       lea(str1, Address(str1, cnt2, Address::uxtw(str1_chr_shift)));
4987       lea(str2, Address(str2, cnt2, Address::uxtw(str2_chr_shift)));
4988       zip1(vtmp, T8B, vtmp, vtmpZ);
4989       sub(cnt1, zr, cnt2, LSL, str1_chr_shift);
4990       sub(cnt2, zr, cnt2, LSL, str2_chr_shift);
4991       add(cnt1, cnt1, 4);
4992       fmovd(tmp1, vtmp);
4993     } else { // UL case
4994       ldr(tmp1, Address(str1));


4995       ldrs(vtmp, Address(str2));
<span class="line-modified">4996       cmp(cnt2, stub_threshold);</span>
4997       br(GE, STUB);
4998       subw(cnt2, cnt2, 4);
4999       lea(str1, Address(str1, cnt2, Address::uxtw(str1_chr_shift)));
5000       eor(vtmpZ, T16B, vtmpZ, vtmpZ);
5001       lea(str2, Address(str2, cnt2, Address::uxtw(str2_chr_shift)));
5002       sub(cnt1, zr, cnt2, LSL, str1_chr_shift);
5003       zip1(vtmp, T8B, vtmp, vtmpZ);
5004       sub(cnt2, zr, cnt2, LSL, str2_chr_shift);
5005       add(cnt1, cnt1, 8);
5006       fmovd(tmp2, vtmp);
5007     }
5008     adds(cnt2, cnt2, isUL ? 4 : 8);
5009     br(GE, TAIL);
5010     eor(rscratch2, tmp1, tmp2);
5011     cbnz(rscratch2, DIFFERENCE);
5012     // main loop
5013     bind(NEXT_WORD);
5014     if (str1_isL == str2_isL) {
5015       ldr(tmp1, Address(str1, cnt2));
5016       ldr(tmp2, Address(str2, cnt2));
</pre>
<hr />
<pre>
5670   br(Assembler::GE, loop);
5671 
5672   tbz(cnt, 0, fini);
5673   str(value, Address(post(base, 8)));
5674   bind(fini);
5675 }
5676 
5677 // Intrinsic for sun/nio/cs/ISO_8859_1$Encoder.implEncodeISOArray and
5678 // java/lang/StringUTF16.compress.
5679 void MacroAssembler::encode_iso_array(Register src, Register dst,
5680                       Register len, Register result,
5681                       FloatRegister Vtmp1, FloatRegister Vtmp2,
5682                       FloatRegister Vtmp3, FloatRegister Vtmp4)
5683 {
5684     Label DONE, SET_RESULT, NEXT_32, NEXT_32_PRFM, LOOP_8, NEXT_8, LOOP_1, NEXT_1,
5685         NEXT_32_START, NEXT_32_PRFM_START;
5686     Register tmp1 = rscratch1, tmp2 = rscratch2;
5687 
5688       mov(result, len); // Save initial len
5689 

5690       cmp(len, (u1)8); // handle shortest strings first
5691       br(LT, LOOP_1);
5692       cmp(len, (u1)32);
5693       br(LT, NEXT_8);
5694       // The following code uses the SIMD &#39;uzp1&#39; and &#39;uzp2&#39; instructions
5695       // to convert chars to bytes
5696       if (SoftwarePrefetchHintDistance &gt;= 0) {
5697         ld1(Vtmp1, Vtmp2, Vtmp3, Vtmp4, T8H, src);
5698         subs(tmp2, len, SoftwarePrefetchHintDistance/2 + 16);
5699         br(LE, NEXT_32_START);
5700         b(NEXT_32_PRFM_START);
5701         BIND(NEXT_32_PRFM);
5702           ld1(Vtmp1, Vtmp2, Vtmp3, Vtmp4, T8H, src);
5703         BIND(NEXT_32_PRFM_START);
5704           prfm(Address(src, SoftwarePrefetchHintDistance));
5705           orr(v4, T16B, Vtmp1, Vtmp2);
5706           orr(v5, T16B, Vtmp3, Vtmp4);
5707           uzp1(Vtmp1, T16B, Vtmp1, Vtmp2);
5708           uzp1(Vtmp3, T16B, Vtmp3, Vtmp4);
5709           uzp2(v5, T16B, v4, v5); // high bytes
</pre>
<hr />
<pre>
5745       cbz(len, DONE);
5746 
5747     BIND(LOOP_8);
5748       cmp(len, (u1)8);
5749       br(LT, LOOP_1);
5750     BIND(NEXT_8);
5751       ld1(Vtmp1, T8H, src);
5752       uzp1(Vtmp2, T16B, Vtmp1, Vtmp1); // low bytes
5753       uzp2(Vtmp3, T16B, Vtmp1, Vtmp1); // high bytes
5754       fmovd(tmp1, Vtmp3);
5755       cbnz(tmp1, NEXT_1);
5756       strd(Vtmp2, dst);
5757 
5758       sub(len, len, 8);
5759       add(dst, dst, 8);
5760       add(src, src, 16);
5761       cmp(len, (u1)8);
5762       br(GE, NEXT_8);
5763 
5764     BIND(LOOP_1);
<span class="line-modified">5765 </span>
5766     cbz(len, DONE);
5767     BIND(NEXT_1);
5768       ldrh(tmp1, Address(post(src, 2)));
5769       tst(tmp1, 0xff00);
5770       br(NE, SET_RESULT);
5771       strb(tmp1, Address(post(dst, 1)));
5772       subs(len, len, 1);
5773       br(GT, NEXT_1);
5774 
5775     BIND(SET_RESULT);
5776       sub(result, result, len); // Return index where we stopped
5777                                 // Return len == 0 if we processed all
5778                                 // characters
5779     BIND(DONE);
5780 }
5781 
5782 
5783 // Inflate byte[] array to char[].
5784 void MacroAssembler::byte_array_inflate(Register src, Register dst, Register len,
5785                                         FloatRegister vtmp1, FloatRegister vtmp2, FloatRegister vtmp3,
</pre>
<hr />
<pre>
5884                                          FloatRegister tmp3Reg, FloatRegister tmp4Reg,
5885                                          Register result) {
5886   encode_iso_array(src, dst, len, result,
5887                    tmp1Reg, tmp2Reg, tmp3Reg, tmp4Reg);
5888   cmp(len, zr);
5889   csel(result, result, zr, EQ);
5890 }
5891 
5892 // get_thread() can be called anywhere inside generated code so we
5893 // need to save whatever non-callee save context might get clobbered
5894 // by the call to JavaThread::aarch64_get_thread_helper() or, indeed,
5895 // the call setup code.
5896 //
5897 // aarch64_get_thread_helper() clobbers only r0, r1, and flags.
5898 //
5899 void MacroAssembler::get_thread(Register dst) {
5900   RegSet saved_regs = RegSet::range(r0, r1) + lr - dst;
5901   push(saved_regs, sp);
5902 
5903   mov(lr, CAST_FROM_FN_PTR(address, JavaThread::aarch64_get_thread_helper));
<span class="line-modified">5904   blr(lr);</span>
5905   if (dst != c_rarg0) {
5906     mov(dst, c_rarg0);
5907   }
5908 
5909   pop(saved_regs, sp);
5910 }
<span class="line-added">5911 </span>
<span class="line-added">5912 void MacroAssembler::cache_wb(Address line) {</span>
<span class="line-added">5913   assert(line.getMode() == Address::base_plus_offset, &quot;mode should be base_plus_offset&quot;);</span>
<span class="line-added">5914   assert(line.index() == noreg, &quot;index should be noreg&quot;);</span>
<span class="line-added">5915   assert(line.offset() == 0, &quot;offset should be 0&quot;);</span>
<span class="line-added">5916   // would like to assert this</span>
<span class="line-added">5917   // assert(line._ext.shift == 0, &quot;shift should be zero&quot;);</span>
<span class="line-added">5918   if (VM_Version::supports_dcpop()) {</span>
<span class="line-added">5919     // writeback using clear virtual address to point of persistence</span>
<span class="line-added">5920     dc(Assembler::CVAP, line.base());</span>
<span class="line-added">5921   } else {</span>
<span class="line-added">5922     // no need to generate anything as Unsafe.writebackMemory should</span>
<span class="line-added">5923     // never invoke this stub</span>
<span class="line-added">5924   }</span>
<span class="line-added">5925 }</span>
<span class="line-added">5926 </span>
<span class="line-added">5927 void MacroAssembler::cache_wbsync(bool is_pre) {</span>
<span class="line-added">5928   // we only need a barrier post sync</span>
<span class="line-added">5929   if (!is_pre) {</span>
<span class="line-added">5930     membar(Assembler::AnyAny);</span>
<span class="line-added">5931   }</span>
<span class="line-added">5932 }</span>
</pre>
</td>
</tr>
</table>
<center><a href="jvmciCodeInstaller_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>