<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="macroAssembler_aarch64.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_aarch64_log.cpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 1,8 ***</span>
  /*
   * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
<span class="line-modified">!  * Copyright (c) 2014, 2015, Red Hat Inc. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
<span class="line-new-header">--- 1,8 ---</span>
  /*
   * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
<span class="line-modified">!  * Copyright (c) 2014, 2019, Red Hat Inc. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 25,10 ***</span>
<span class="line-new-header">--- 25,12 ---</span>
  
  #ifndef CPU_AARCH64_MACROASSEMBLER_AARCH64_HPP
  #define CPU_AARCH64_MACROASSEMBLER_AARCH64_HPP
  
  #include &quot;asm/assembler.hpp&quot;
<span class="line-added">+ #include &quot;oops/compressedOops.hpp&quot;</span>
<span class="line-added">+ #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  
  // MacroAssembler extends Assembler by frequently used macros.
  //
  // Instructions for which a &#39;better&#39; code sequence exists depending
  // on arguments should also go in here.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 77,21 ***</span>
      bool     check_exceptions          // whether to check for pending exceptions after return
    );
  
    void call_VM_helper(Register oop_result, address entry_point, int number_of_arguments, bool check_exceptions = true);
  
<span class="line-modified">!   // True if an XOR can be used to expand narrow klass references.</span>
<span class="line-modified">!   bool use_XOR_for_compressed_class_base;</span>
  
   public:
<span class="line-modified">!   MacroAssembler(CodeBuffer* code) : Assembler(code) {</span>
<span class="line-removed">-     use_XOR_for_compressed_class_base</span>
<span class="line-removed">-       = (operand_valid_for_logical_immediate(false /*is32*/,</span>
<span class="line-removed">-                                              (uint64_t)Universe::narrow_klass_base())</span>
<span class="line-removed">-          &amp;&amp; ((uint64_t)Universe::narrow_klass_base()</span>
<span class="line-removed">-              &gt; (1UL &lt;&lt; log2_intptr(Universe::narrow_klass_range()))));</span>
<span class="line-removed">-   }</span>
  
   // These routines should emit JVMTI PopFrame and ForceEarlyReturn handling code.
   // The implementation is only non-empty for the InterpreterMacroAssembler,
   // as only the interpreter handles PopFrame and ForceEarlyReturn requests.
   virtual void check_and_handle_popframe(Register java_thread);
<span class="line-new-header">--- 79,24 ---</span>
      bool     check_exceptions          // whether to check for pending exceptions after return
    );
  
    void call_VM_helper(Register oop_result, address entry_point, int number_of_arguments, bool check_exceptions = true);
  
<span class="line-modified">!   enum KlassDecodeMode {</span>
<span class="line-modified">!     KlassDecodeNone,</span>
<span class="line-added">+     KlassDecodeZero,</span>
<span class="line-added">+     KlassDecodeXor,</span>
<span class="line-added">+     KlassDecodeMovk</span>
<span class="line-added">+   };</span>
<span class="line-added">+ </span>
<span class="line-added">+   KlassDecodeMode klass_decode_mode();</span>
<span class="line-added">+ </span>
<span class="line-added">+  private:</span>
<span class="line-added">+   static KlassDecodeMode _klass_decode_mode;</span>
  
   public:
<span class="line-modified">!   MacroAssembler(CodeBuffer* code) : Assembler(code) {}</span>
  
   // These routines should emit JVMTI PopFrame and ForceEarlyReturn handling code.
   // The implementation is only non-empty for the InterpreterMacroAssembler,
   // as only the interpreter handles PopFrame and ForceEarlyReturn requests.
   virtual void check_and_handle_popframe(Register java_thread);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 130,10 ***</span>
<span class="line-new-header">--- 135,24 ---</span>
      InstructionMark im(this);
      code_section()-&gt;relocate(inst_mark(), a.rspec());
      a.lea(this, r);
    }
  
<span class="line-added">+   /* Sometimes we get misaligned loads and stores, usually from Unsafe</span>
<span class="line-added">+      accesses, and these can exceed the offset range. */</span>
<span class="line-added">+   Address legitimize_address(const Address &amp;a, int size, Register scratch) {</span>
<span class="line-added">+     if (a.getMode() == Address::base_plus_offset) {</span>
<span class="line-added">+       if (! Address::offset_ok_for_immed(a.offset(), exact_log2(size))) {</span>
<span class="line-added">+         block_comment(&quot;legitimize_address {&quot;);</span>
<span class="line-added">+         lea(scratch, a);</span>
<span class="line-added">+         block_comment(&quot;} legitimize_address&quot;);</span>
<span class="line-added">+         return Address(scratch);</span>
<span class="line-added">+       }</span>
<span class="line-added">+     }</span>
<span class="line-added">+     return a;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
    void addmw(Address a, Register incr, Register scratch) {
      ldrw(scratch, a);
      addw(scratch, scratch, incr);
      strw(scratch, a);
    }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 167,17 ***</span>
    void build_frame(int framesize);
    void remove_frame(int framesize);
  
    virtual void _call_Unimplemented(address call_site) {
      mov(rscratch2, call_site);
<span class="line-removed">-     haltsim();</span>
    }
  
  #define call_Unimplemented() _call_Unimplemented((address)__PRETTY_FUNCTION__)
  
<span class="line-removed">-   virtual void notify(int type);</span>
<span class="line-removed">- </span>
    // aliases defined in AARCH64 spec
  
    template&lt;class T&gt;
    inline void cmpw(Register Rd, T imm)  { subsw(zr, Rd, imm); }
  
<span class="line-new-header">--- 186,14 ---</span>
</pre>
<hr />
<pre>
<span class="line-old-header">*** 442,16 ***</span>
<span class="line-new-header">--- 458,22 ---</span>
    void mov_immediate32(Register dst, u_int32_t imm32);
  
    int push(unsigned int bitset, Register stack);
    int pop(unsigned int bitset, Register stack);
  
<span class="line-added">+   int push_fp(unsigned int bitset, Register stack);</span>
<span class="line-added">+   int pop_fp(unsigned int bitset, Register stack);</span>
<span class="line-added">+ </span>
    void mov(Register dst, Address a);
  
  public:
    void push(RegSet regs, Register stack) { if (regs.bits()) push(regs.bits(), stack); }
    void pop(RegSet regs, Register stack) { if (regs.bits()) pop(regs.bits(), stack); }
  
<span class="line-added">+   void push_fp(RegSet regs, Register stack) { if (regs.bits()) push_fp(regs.bits(), stack); }</span>
<span class="line-added">+   void pop_fp(RegSet regs, Register stack) { if (regs.bits()) pop_fp(regs.bits(), stack); }</span>
<span class="line-added">+ </span>
    // Push and pop everything that might be clobbered by a native
    // runtime call except rscratch1 and rscratch2.  (They are always
    // scratch, so we don&#39;t have to protect them.)  Only save the lower
    // 64 bits of each vector register.
    void push_call_clobbered_registers();
</pre>
<hr />
<pre>
<span class="line-old-header">*** 605,10 ***</span>
<span class="line-new-header">--- 627,11 ---</span>
  
    static int patch_oop(address insn_addr, address o);
    static int patch_narrow_klass(address insn_addr, narrowKlass n);
  
    address emit_trampoline_stub(int insts_call_instruction_offset, address target);
<span class="line-added">+   void emit_static_call_stub();</span>
  
    // The following 4 methods return the offset of the appropriate move instruction
  
    // Support for fast byte/short loading with zero extension (depending on particular CPU)
    int load_unsigned_byte(Register dst, Address src);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 784,10 ***</span>
<span class="line-new-header">--- 807,12 ---</span>
    void resolve_jobject(Register value, Register thread, Register tmp);
  
    // C &#39;boolean&#39; to Java boolean: x == 0 ? 0 : 1
    void c2bool(Register x);
  
<span class="line-added">+   void load_method_holder(Register holder, Register method);</span>
<span class="line-added">+ </span>
    // oop manipulations
    void load_klass(Register dst, Register src);
    void store_klass(Register dst, Register src);
    void cmp_klass(Register oop, Register trial_klass, Register tmp);
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 922,10 ***</span>
<span class="line-new-header">--- 947,15 ---</span>
    void check_klass_subtype(Register sub_klass,
                             Register super_klass,
                             Register temp_reg,
                             Label&amp; L_success);
  
<span class="line-added">+   void clinit_barrier(Register klass,</span>
<span class="line-added">+                       Register thread,</span>
<span class="line-added">+                       Label* L_fast_path = NULL,</span>
<span class="line-added">+                       Label* L_slow_path = NULL);</span>
<span class="line-added">+ </span>
    Address argument_address(RegisterOrConstant arg_slot, int extra_slot_offset = 0);
  
  
    // Debugging
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1174,32 ***</span>
    // bootstraps into the generated ARM code which directly follows the
    // stub
    //
  
    public:
<span class="line-removed">-   // enum used for aarch64--x86 linkage to define return type of x86 function</span>
<span class="line-removed">-   enum ret_type { ret_type_void, ret_type_integral, ret_type_float, ret_type_double};</span>
<span class="line-removed">- </span>
<span class="line-removed">- #ifdef BUILTIN_SIM</span>
<span class="line-removed">-   void c_stub_prolog(int gp_arg_count, int fp_arg_count, int ret_type, address *prolog_ptr = NULL);</span>
<span class="line-removed">- #else</span>
<span class="line-removed">-   void c_stub_prolog(int gp_arg_count, int fp_arg_count, int ret_type) { }</span>
<span class="line-removed">- #endif</span>
<span class="line-removed">- </span>
<span class="line-removed">-   // special version of call_VM_leaf_base needed for aarch64 simulator</span>
<span class="line-removed">-   // where we need to specify both the gp and fp arg counts and the</span>
<span class="line-removed">-   // return type so that the linkage routine from aarch64 to x86 and</span>
<span class="line-removed">-   // back knows which aarch64 registers to copy to x86 registers and</span>
<span class="line-removed">-   // which x86 result register to copy back to an aarch64 register</span>
<span class="line-removed">- </span>
<span class="line-removed">-   void call_VM_leaf_base1(</span>
<span class="line-removed">-     address  entry_point,             // the entry point</span>
<span class="line-removed">-     int      number_of_gp_arguments,  // the number of gp reg arguments to pass</span>
<span class="line-removed">-     int      number_of_fp_arguments,  // the number of fp reg arguments to pass</span>
<span class="line-removed">-     ret_type type,                    // the return type for the call</span>
<span class="line-removed">-     Label*   retaddr = NULL</span>
<span class="line-removed">-   );</span>
  
    void ldr_constant(Register dest, const Address &amp;const_addr) {
      if (NearCpool) {
        ldr(dest, const_addr);
      } else {
<span class="line-new-header">--- 1204,10 ---</span>
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1358,10 ***</span>
<span class="line-new-header">--- 1366,13 ---</span>
        spill(tmp1, true, dst_offset);
        unspill(tmp1, true, src_offset+8);
        spill(tmp1, true, dst_offset+8);
      }
    }
<span class="line-added">+ </span>
<span class="line-added">+   void cache_wb(Address line);</span>
<span class="line-added">+   void cache_wbsync(bool is_pre);</span>
  };
  
  #ifdef ASSERT
  inline bool AbstractAssembler::pd_check_instruction_mark() { return false; }
  #endif
</pre>
<center><a href="macroAssembler_aarch64.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_aarch64_log.cpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>