<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c1_FrameMap_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_LIRAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.</span>
   3  * Copyright (c) 2014, Red Hat Inc. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;asm/assembler.hpp&quot;
  29 #include &quot;c1/c1_CodeStubs.hpp&quot;
  30 #include &quot;c1/c1_Compilation.hpp&quot;
  31 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  32 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  33 #include &quot;c1/c1_Runtime1.hpp&quot;
  34 #include &quot;c1/c1_ValueStack.hpp&quot;
  35 #include &quot;ci/ciArrayKlass.hpp&quot;
  36 #include &quot;ci/ciInstance.hpp&quot;
<span class="line-modified">  37 #include &quot;gc/shared/barrierSet.hpp&quot;</span>
<span class="line-removed">  38 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;</span>
  39 #include &quot;gc/shared/collectedHeap.hpp&quot;
  40 #include &quot;nativeInst_aarch64.hpp&quot;
  41 #include &quot;oops/objArrayKlass.hpp&quot;
  42 #include &quot;runtime/frame.inline.hpp&quot;
  43 #include &quot;runtime/sharedRuntime.hpp&quot;

  44 #include &quot;vmreg_aarch64.inline.hpp&quot;
  45 
  46 
<span class="line-removed">  47 </span>
  48 #ifndef PRODUCT
  49 #define COMMENT(x)   do { __ block_comment(x); } while (0)
  50 #else
  51 #define COMMENT(x)
  52 #endif
  53 
  54 NEEDS_CLEANUP // remove this definitions ?
  55 const Register IC_Klass    = rscratch2;   // where the IC klass is cached
  56 const Register SYNC_header = r0;   // synchronization header
  57 const Register SHIFT_count = r0;   // where count for shift operations must be
  58 
  59 #define __ _masm-&gt;
  60 
  61 
  62 static void select_different_registers(Register preserve,
  63                                        Register extra,
  64                                        Register &amp;tmp1,
  65                                        Register &amp;tmp2) {
  66   if (tmp1 == preserve) {
  67     assert_different_registers(tmp1, tmp2, extra);
</pre>
<hr />
<pre>
 122 address LIR_Assembler::double_constant(double d) {
 123   address const_addr = __ double_constant(d);
 124   if (const_addr == NULL) {
 125     bailout(&quot;const section overflow&quot;);
 126     return __ code()-&gt;consts()-&gt;start();
 127   } else {
 128     return const_addr;
 129   }
 130 }
 131 
 132 address LIR_Assembler::int_constant(jlong n) {
 133   address const_addr = __ long_constant(n);
 134   if (const_addr == NULL) {
 135     bailout(&quot;const section overflow&quot;);
 136     return __ code()-&gt;consts()-&gt;start();
 137   } else {
 138     return const_addr;
 139   }
 140 }
 141 
<span class="line-removed"> 142 void LIR_Assembler::set_24bit_FPU() { Unimplemented(); }</span>
<span class="line-removed"> 143 </span>
<span class="line-removed"> 144 void LIR_Assembler::reset_FPU() { Unimplemented(); }</span>
<span class="line-removed"> 145 </span>
<span class="line-removed"> 146 void LIR_Assembler::fpop() { Unimplemented(); }</span>
<span class="line-removed"> 147 </span>
<span class="line-removed"> 148 void LIR_Assembler::fxch(int i) { Unimplemented(); }</span>
<span class="line-removed"> 149 </span>
<span class="line-removed"> 150 void LIR_Assembler::fld(int i) { Unimplemented(); }</span>
<span class="line-removed"> 151 </span>
<span class="line-removed"> 152 void LIR_Assembler::ffree(int i) { Unimplemented(); }</span>
<span class="line-removed"> 153 </span>
 154 void LIR_Assembler::breakpoint() { Unimplemented(); }
 155 
 156 void LIR_Assembler::push(LIR_Opr opr) { Unimplemented(); }
 157 
 158 void LIR_Assembler::pop(LIR_Opr opr) { Unimplemented(); }
 159 
 160 bool LIR_Assembler::is_literal_address(LIR_Address* addr) { Unimplemented(); return false; }
 161 //-------------------------------------------
 162 
 163 static Register as_reg(LIR_Opr op) {
 164   return op-&gt;is_double_cpu() ? op-&gt;as_register_lo() : op-&gt;as_register();
 165 }
 166 
 167 static jlong as_long(LIR_Opr data) {
 168   jlong result;
 169   switch (data-&gt;type()) {
 170   case T_INT:
 171     result = (data-&gt;as_jint());
 172     break;
 173   case T_LONG:
</pre>
<hr />
<pre>
 298   __ inline_cache_check(receiver, ic_klass);
 299 
 300   // if icache check fails, then jump to runtime routine
 301   // Note: RECEIVER must still contain the receiver!
 302   Label dont;
 303   __ br(Assembler::EQ, dont);
 304   __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
 305 
 306   // We align the verified entry point unless the method body
 307   // (including its inline cache check) will fit in a single 64-byte
 308   // icache line.
 309   if (! method()-&gt;is_accessor() || __ offset() - start_offset &gt; 4 * 4) {
 310     // force alignment after the cache check.
 311     __ align(CodeEntryAlignment);
 312   }
 313 
 314   __ bind(dont);
 315   return start_offset;
 316 }
 317 











 318 
 319 void LIR_Assembler::jobject2reg(jobject o, Register reg) {
 320   if (o == NULL) {
 321     __ mov(reg, zr);
 322   } else {
 323     __ movoop(reg, o, /*immediate*/true);
 324   }
 325 }
 326 
 327 void LIR_Assembler::deoptimize_trap(CodeEmitInfo *info) {
 328   address target = NULL;
 329   relocInfo::relocType reloc_type = relocInfo::none;
 330 
 331   switch (patching_id(info)) {
 332   case PatchingStub::access_field_id:
 333     target = Runtime1::entry_for(Runtime1::access_field_patching_id);
 334     reloc_type = relocInfo::section_word_type;
 335     break;
 336   case PatchingStub::load_klass_id:
 337     target = Runtime1::entry_for(Runtime1::load_klass_patching_id);
</pre>
<hr />
<pre>
 699 }
 700 
 701 void LIR_Assembler::reg2reg(LIR_Opr src, LIR_Opr dest) {
 702   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 703   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 704 
 705   // move between cpu-registers
 706   if (dest-&gt;is_single_cpu()) {
 707     if (src-&gt;type() == T_LONG) {
 708       // Can do LONG -&gt; OBJECT
 709       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
 710       return;
 711     }
 712     assert(src-&gt;is_single_cpu(), &quot;must match&quot;);
 713     if (src-&gt;type() == T_OBJECT) {
 714       __ verify_oop(src-&gt;as_register());
 715     }
 716     move_regs(src-&gt;as_register(), dest-&gt;as_register());
 717 
 718   } else if (dest-&gt;is_double_cpu()) {
<span class="line-modified"> 719     if (src-&gt;type() == T_OBJECT || src-&gt;type() == T_ARRAY) {</span>
 720       // Surprising to me but we can see move of a long to t_object
 721       __ verify_oop(src-&gt;as_register());
 722       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
 723       return;
 724     }
 725     assert(src-&gt;is_double_cpu(), &quot;must match&quot;);
 726     Register f_lo = src-&gt;as_register_lo();
 727     Register f_hi = src-&gt;as_register_hi();
 728     Register t_lo = dest-&gt;as_register_lo();
 729     Register t_hi = dest-&gt;as_register_hi();
 730     assert(f_hi == f_lo, &quot;must be same&quot;);
 731     assert(t_hi == t_lo, &quot;must be same&quot;);
 732     move_regs(f_lo, t_lo);
 733 
 734   } else if (dest-&gt;is_single_fpu()) {
 735     __ fmovs(dest-&gt;as_float_reg(), src-&gt;as_float_reg());
 736 
 737   } else if (dest-&gt;is_double_fpu()) {
 738     __ fmovd(dest-&gt;as_double_reg(), src-&gt;as_double_reg());
 739 
 740   } else {
 741     ShouldNotReachHere();
 742   }
 743 }
 744 
 745 void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {
 746   if (src-&gt;is_single_cpu()) {
<span class="line-modified"> 747     if (type == T_ARRAY || type == T_OBJECT) {</span>
 748       __ str(src-&gt;as_register(), frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 749       __ verify_oop(src-&gt;as_register());
<span class="line-modified"> 750     } else if (type == T_METADATA || type == T_DOUBLE) {</span>
 751       __ str(src-&gt;as_register(), frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 752     } else {
 753       __ strw(src-&gt;as_register(), frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 754     }
 755 
 756   } else if (src-&gt;is_double_cpu()) {
 757     Address dest_addr_LO = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), lo_word_offset_in_bytes);
 758     __ str(src-&gt;as_register_lo(), dest_addr_LO);
 759 
 760   } else if (src-&gt;is_single_fpu()) {
 761     Address dest_addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 762     __ strs(src-&gt;as_float_reg(), dest_addr);
 763 
 764   } else if (src-&gt;is_double_fpu()) {
 765     Address dest_addr = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
 766     __ strd(src-&gt;as_double_reg(), dest_addr);
 767 
 768   } else {
 769     ShouldNotReachHere();
 770   }
 771 
 772 }
 773 
 774 
 775 void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide, bool /* unaligned */) {
 776   LIR_Address* to_addr = dest-&gt;as_address_ptr();
 777   PatchingStub* patch = NULL;
 778   Register compressed_src = rscratch1;
 779 
 780   if (patch_code != lir_patch_none) {
 781     deoptimize_trap(info);
 782     return;
 783   }
 784 
<span class="line-modified"> 785   if (type == T_ARRAY || type == T_OBJECT) {</span>
 786     __ verify_oop(src-&gt;as_register());
 787 
 788     if (UseCompressedOops &amp;&amp; !wide) {
 789       __ encode_heap_oop(compressed_src, src-&gt;as_register());
 790     } else {
 791       compressed_src = src-&gt;as_register();
 792     }
 793   }
 794 
 795   int null_check_here = code_offset();
 796   switch (type) {
 797     case T_FLOAT: {
 798       __ strs(src-&gt;as_float_reg(), as_Address(to_addr));
 799       break;
 800     }
 801 
 802     case T_DOUBLE: {
 803       __ strd(src-&gt;as_double_reg(), as_Address(to_addr));
 804       break;
 805     }
</pre>
<hr />
<pre>
 840 
 841     case T_CHAR:    // fall through
 842     case T_SHORT:
 843       __ strh(src-&gt;as_register(), as_Address(to_addr));
 844       break;
 845 
 846     default:
 847       ShouldNotReachHere();
 848   }
 849   if (info != NULL) {
 850     add_debug_info_for_null_check(null_check_here, info);
 851   }
 852 }
 853 
 854 
 855 void LIR_Assembler::stack2reg(LIR_Opr src, LIR_Opr dest, BasicType type) {
 856   assert(src-&gt;is_stack(), &quot;should not call otherwise&quot;);
 857   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 858 
 859   if (dest-&gt;is_single_cpu()) {
<span class="line-modified"> 860     if (type == T_ARRAY || type == T_OBJECT) {</span>
 861       __ ldr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 862       __ verify_oop(dest-&gt;as_register());
<span class="line-modified"> 863     } else if (type == T_METADATA) {</span>
 864       __ ldr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 865     } else {
 866       __ ldrw(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 867     }
 868 
 869   } else if (dest-&gt;is_double_cpu()) {
 870     Address src_addr_LO = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), lo_word_offset_in_bytes);
 871     __ ldr(dest-&gt;as_register_lo(), src_addr_LO);
 872 
 873   } else if (dest-&gt;is_single_fpu()) {
 874     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
 875     __ ldrs(dest-&gt;as_float_reg(), src_addr);
 876 
 877   } else if (dest-&gt;is_double_fpu()) {
 878     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
 879     __ ldrd(dest-&gt;as_double_reg(), src_addr);
 880 
 881   } else {
 882     ShouldNotReachHere();
 883   }
</pre>
<hr />
<pre>
 990 
 991     case T_BYTE:
 992       __ ldrsb(dest-&gt;as_register(), as_Address(from_addr));
 993       break;
 994     case T_BOOLEAN: {
 995       __ ldrb(dest-&gt;as_register(), as_Address(from_addr));
 996       break;
 997     }
 998 
 999     case T_CHAR:
1000       __ ldrh(dest-&gt;as_register(), as_Address(from_addr));
1001       break;
1002     case T_SHORT:
1003       __ ldrsh(dest-&gt;as_register(), as_Address(from_addr));
1004       break;
1005 
1006     default:
1007       ShouldNotReachHere();
1008   }
1009 
<span class="line-modified">1010   if (type == T_ARRAY || type == T_OBJECT) {</span>
1011     if (UseCompressedOops &amp;&amp; !wide) {
1012       __ decode_heap_oop(dest-&gt;as_register());
1013     }
<span class="line-modified">1014     __ verify_oop(dest-&gt;as_register());</span>




1015   } else if (type == T_ADDRESS &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1016     if (UseCompressedClassPointers) {
1017       __ decode_klass_not_null(dest-&gt;as_register());
1018     }
1019   }
1020 }
1021 
1022 
1023 int LIR_Assembler::array_element_size(BasicType type) const {
1024   int elem_size = type2aelembytes(type);
1025   return exact_log2(elem_size);
1026 }
1027 
1028 
1029 void LIR_Assembler::emit_op3(LIR_Op3* op) {
1030   switch (op-&gt;code()) {
1031   case lir_idiv:
1032   case lir_irem:
1033     arithmetic_idiv(op-&gt;code(),
1034                     op-&gt;in_opr1(),
</pre>
<hr />
<pre>
1053   }
1054 }
1055 
1056 void LIR_Assembler::emit_opBranch(LIR_OpBranch* op) {
1057 #ifdef ASSERT
1058   assert(op-&gt;block() == NULL || op-&gt;block()-&gt;label() == op-&gt;label(), &quot;wrong label&quot;);
1059   if (op-&gt;block() != NULL)  _branch_target_blocks.append(op-&gt;block());
1060   if (op-&gt;ublock() != NULL) _branch_target_blocks.append(op-&gt;ublock());
1061 #endif
1062 
1063   if (op-&gt;cond() == lir_cond_always) {
1064     if (op-&gt;info() != NULL) add_debug_info_for_branch(op-&gt;info());
1065     __ b(*(op-&gt;label()));
1066   } else {
1067     Assembler::Condition acond;
1068     if (op-&gt;code() == lir_cond_float_branch) {
1069       bool is_unordered = (op-&gt;ublock() == op-&gt;block());
1070       // Assembler::EQ does not permit unordered branches, so we add
1071       // another branch here.  Likewise, Assembler::NE does not permit
1072       // ordered branches.
<span class="line-modified">1073       if (is_unordered &amp;&amp; op-&gt;cond() == lir_cond_equal</span>
<span class="line-modified">1074           || !is_unordered &amp;&amp; op-&gt;cond() == lir_cond_notEqual)</span>
1075         __ br(Assembler::VS, *(op-&gt;ublock()-&gt;label()));
1076       switch(op-&gt;cond()) {
1077       case lir_cond_equal:        acond = Assembler::EQ; break;
1078       case lir_cond_notEqual:     acond = Assembler::NE; break;
1079       case lir_cond_less:         acond = (is_unordered ? Assembler::LT : Assembler::LO); break;
1080       case lir_cond_lessEqual:    acond = (is_unordered ? Assembler::LE : Assembler::LS); break;
1081       case lir_cond_greaterEqual: acond = (is_unordered ? Assembler::HS : Assembler::GE); break;
1082       case lir_cond_greater:      acond = (is_unordered ? Assembler::HI : Assembler::GT); break;
1083       default:                    ShouldNotReachHere();
1084         acond = Assembler::EQ;  // unreachable
1085       }
1086     } else {
1087       switch (op-&gt;cond()) {
1088         case lir_cond_equal:        acond = Assembler::EQ; break;
1089         case lir_cond_notEqual:     acond = Assembler::NE; break;
1090         case lir_cond_less:         acond = Assembler::LT; break;
1091         case lir_cond_lessEqual:    acond = Assembler::LE; break;
1092         case lir_cond_greaterEqual: acond = Assembler::GE; break;
1093         case lir_cond_greater:      acond = Assembler::GT; break;
1094         case lir_cond_belowEqual:   acond = Assembler::LS; break;
</pre>
<hr />
<pre>
1194                                InstanceKlass::init_state_offset()));
1195     __ cmpw(rscratch1, InstanceKlass::fully_initialized);
1196     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
1197     __ br(Assembler::NE, *op-&gt;stub()-&gt;entry());
1198   }
1199   __ allocate_object(op-&gt;obj()-&gt;as_register(),
1200                      op-&gt;tmp1()-&gt;as_register(),
1201                      op-&gt;tmp2()-&gt;as_register(),
1202                      op-&gt;header_size(),
1203                      op-&gt;object_size(),
1204                      op-&gt;klass()-&gt;as_register(),
1205                      *op-&gt;stub()-&gt;entry());
1206   __ bind(*op-&gt;stub()-&gt;continuation());
1207 }
1208 
1209 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
1210   Register len =  op-&gt;len()-&gt;as_register();
1211   __ uxtw(len, len);
1212 
1213   if (UseSlowPath ||
<span class="line-modified">1214       (!UseFastNewObjectArray &amp;&amp; (op-&gt;type() == T_OBJECT || op-&gt;type() == T_ARRAY)) ||</span>
<span class="line-modified">1215       (!UseFastNewTypeArray   &amp;&amp; (op-&gt;type() != T_OBJECT &amp;&amp; op-&gt;type() != T_ARRAY))) {</span>
1216     __ b(*op-&gt;stub()-&gt;entry());
1217   } else {
1218     Register tmp1 = op-&gt;tmp1()-&gt;as_register();
1219     Register tmp2 = op-&gt;tmp2()-&gt;as_register();
1220     Register tmp3 = op-&gt;tmp3()-&gt;as_register();
1221     if (len == tmp1) {
1222       tmp1 = tmp3;
1223     } else if (len == tmp2) {
1224       tmp2 = tmp3;
1225     } else if (len == tmp3) {
1226       // everything is ok
1227     } else {
1228       __ mov(tmp3, len);
1229     }
1230     __ allocate_array(op-&gt;obj()-&gt;as_register(),
1231                       len,
1232                       tmp1,
1233                       tmp2,
1234                       arrayOopDesc::header_size(op-&gt;type()),
1235                       array_element_size(op-&gt;type()),
</pre>
<hr />
<pre>
1728       case lir_mul: __ mul (dest-&gt;as_register_lo(), lreg_lo, rreg_lo); break;
1729       case lir_div: __ corrected_idivq(dest-&gt;as_register_lo(), lreg_lo, rreg_lo, false, rscratch1); break;
1730       case lir_rem: __ corrected_idivq(dest-&gt;as_register_lo(), lreg_lo, rreg_lo, true, rscratch1); break;
1731       default:
1732         ShouldNotReachHere();
1733       }
1734 
1735     } else if (right-&gt;is_constant()) {
1736       jlong c = right-&gt;as_constant_ptr()-&gt;as_jlong();
1737       Register dreg = as_reg(dest);
1738       switch (code) {
1739         case lir_add:
1740         case lir_sub:
1741           if (c == 0 &amp;&amp; dreg == lreg_lo) {
1742             COMMENT(&quot;effective nop elided&quot;);
1743             return;
1744           }
1745           code == lir_add ? __ add(dreg, lreg_lo, c) : __ sub(dreg, lreg_lo, c);
1746           break;
1747         case lir_div:
<span class="line-modified">1748           assert(c &gt; 0 &amp;&amp; is_power_of_2_long(c), &quot;divisor must be power-of-2 constant&quot;);</span>
1749           if (c == 1) {
1750             // move lreg_lo to dreg if divisor is 1
1751             __ mov(dreg, lreg_lo);
1752           } else {
1753             unsigned int shift = exact_log2_long(c);
1754             // use rscratch1 as intermediate result register
1755             __ asr(rscratch1, lreg_lo, 63);
1756             __ add(rscratch1, lreg_lo, rscratch1, Assembler::LSR, 64 - shift);
1757             __ asr(dreg, rscratch1, shift);
1758           }
1759           break;
1760         case lir_rem:
<span class="line-modified">1761           assert(c &gt; 0 &amp;&amp; is_power_of_2_long(c), &quot;divisor must be power-of-2 constant&quot;);</span>
1762           if (c == 1) {
1763             // move 0 to dreg if divisor is 1
1764             __ mov(dreg, zr);
1765           } else {
1766             // use rscratch1 as intermediate result register
1767             __ negs(rscratch1, lreg_lo);
1768             __ andr(dreg, lreg_lo, c - 1);
1769             __ andr(rscratch1, rscratch1, c - 1);
1770             __ csneg(dreg, dreg, rscratch1, Assembler::MI);
1771           }
1772           break;
1773         default:
1774           ShouldNotReachHere();
1775       }
1776     } else {
1777       ShouldNotReachHere();
1778     }
1779   } else if (left-&gt;is_single_fpu()) {
1780     assert(right-&gt;is_single_fpu(), &quot;right hand side of float arithmetics needs to be float register&quot;);
1781     switch (code) {
1782     case lir_add: __ fadds (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
1783     case lir_sub: __ fsubs (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;

1784     case lir_mul: __ fmuls (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;

1785     case lir_div: __ fdivs (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
1786     default:
1787       ShouldNotReachHere();
1788     }
1789   } else if (left-&gt;is_double_fpu()) {
1790     if (right-&gt;is_double_fpu()) {
<span class="line-modified">1791       // cpu register - cpu register</span>
1792       switch (code) {
1793       case lir_add: __ faddd (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
1794       case lir_sub: __ fsubd (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;

1795       case lir_mul: __ fmuld (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;

1796       case lir_div: __ fdivd (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
1797       default:
1798         ShouldNotReachHere();
1799       }
1800     } else {
1801       if (right-&gt;is_constant()) {
1802         ShouldNotReachHere();
1803       }
1804       ShouldNotReachHere();
1805     }
1806   } else if (left-&gt;is_single_stack() || left-&gt;is_address()) {
1807     assert(left == dest, &quot;left and dest must be equal&quot;);
1808     ShouldNotReachHere();
1809   } else {
1810     ShouldNotReachHere();
1811   }
1812 }
1813 
1814 void LIR_Assembler::arith_fpu_implementation(LIR_Code code, int left_index, int right_index, int dest_index, bool pop_fpu_stack) { Unimplemented(); }
1815 
</pre>
<hr />
<pre>
1911       }
1912     }
1913   } else {
1914     Register rreg = right-&gt;as_register();
1915     __ corrected_idivl(dreg, lreg, rreg, is_irem, rscratch1);
1916   }
1917 }
1918 
1919 
1920 void LIR_Assembler::comp_op(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Op2* op) {
1921   if (opr1-&gt;is_constant() &amp;&amp; opr2-&gt;is_single_cpu()) {
1922     // tableswitch
1923     Register reg = as_reg(opr2);
1924     struct tableswitch &amp;table = switches[opr1-&gt;as_constant_ptr()-&gt;as_jint()];
1925     __ tableswitch(reg, table._first_key, table._last_key, table._branches, table._after);
1926   } else if (opr1-&gt;is_single_cpu() || opr1-&gt;is_double_cpu()) {
1927     Register reg1 = as_reg(opr1);
1928     if (opr2-&gt;is_single_cpu()) {
1929       // cpu register - cpu register
1930       Register reg2 = opr2-&gt;as_register();
<span class="line-modified">1931       if (opr1-&gt;type() == T_OBJECT || opr1-&gt;type() == T_ARRAY) {</span>
1932         __ cmpoop(reg1, reg2);
1933       } else {
<span class="line-modified">1934         assert(opr2-&gt;type() != T_OBJECT &amp;&amp; opr2-&gt;type() != T_ARRAY, &quot;cmp int, oop?&quot;);</span>
1935         __ cmpw(reg1, reg2);
1936       }
1937       return;
1938     }
1939     if (opr2-&gt;is_double_cpu()) {
1940       // cpu register - cpu register
1941       Register reg2 = opr2-&gt;as_register_lo();
1942       __ cmp(reg1, reg2);
1943       return;
1944     }
1945 
1946     if (opr2-&gt;is_constant()) {
1947       bool is_32bit = false; // width of register operand
1948       jlong imm;
1949 
1950       switch(opr2-&gt;type()) {
1951       case T_INT:
1952         imm = opr2-&gt;as_constant_ptr()-&gt;as_jint();
1953         is_32bit = true;
1954         break;
1955       case T_LONG:
1956         imm = opr2-&gt;as_constant_ptr()-&gt;as_jlong();
1957         break;
1958       case T_ADDRESS:
1959         imm = opr2-&gt;as_constant_ptr()-&gt;as_jint();
1960         break;



1961       case T_OBJECT:
1962       case T_ARRAY:
1963         jobject2reg(opr2-&gt;as_constant_ptr()-&gt;as_jobject(), rscratch1);
1964         __ cmpoop(reg1, rscratch1);
1965         return;
1966       default:
1967         ShouldNotReachHere();
1968         imm = 0;  // unreachable
1969         break;
1970       }
1971 
1972       if (Assembler::operand_valid_for_add_sub_immediate(imm)) {
1973         if (is_32bit)
1974           __ cmpw(reg1, imm);
1975         else
1976           __ subs(zr, reg1, imm);
1977         return;
1978       } else {
1979         __ mov(rscratch1, imm);
1980         if (is_32bit)
</pre>
<hr />
<pre>
2046 }
2047 
2048 
2049 /* Currently, vtable-dispatch is only enabled for sparc platforms */
2050 void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {
2051   ShouldNotReachHere();
2052 }
2053 
2054 
2055 void LIR_Assembler::emit_static_call_stub() {
2056   address call_pc = __ pc();
2057   address stub = __ start_a_stub(call_stub_size());
2058   if (stub == NULL) {
2059     bailout(&quot;static call stub overflow&quot;);
2060     return;
2061   }
2062 
2063   int start = __ offset();
2064 
2065   __ relocate(static_stub_Relocation::spec(call_pc));
<span class="line-modified">2066   __ mov_metadata(rmethod, (Metadata*)NULL);</span>
<span class="line-removed">2067   __ movptr(rscratch1, 0);</span>
<span class="line-removed">2068   __ br(rscratch1);</span>
2069 
<span class="line-modified">2070   assert(__ offset() - start &lt;= call_stub_size(), &quot;stub too big&quot;);</span>

2071   __ end_a_stub();
2072 }
2073 
2074 
2075 void LIR_Assembler::throw_op(LIR_Opr exceptionPC, LIR_Opr exceptionOop, CodeEmitInfo* info) {
2076   assert(exceptionOop-&gt;as_register() == r0, &quot;must match&quot;);
2077   assert(exceptionPC-&gt;as_register() == r3, &quot;must match&quot;);
2078 
2079   // exception object is not added to oop map by LinearScan
2080   // (LinearScan assumes that no oops are in fixed registers)
2081   info-&gt;add_register_oop(exceptionOop);
2082   Runtime1::StubID unwind_id;
2083 
2084   // get current pc information
2085   // pc is only needed if the method has an exception handler, the unwind code does not need it.
2086   int pc_for_athrow_offset = __ offset();
2087   InternalAddress pc_for_athrow(__ pc());
2088   __ adr(exceptionPC-&gt;as_register(), pc_for_athrow);
2089   add_call_info(pc_for_athrow_offset, info); // for exception handler
2090 
</pre>
<hr />
<pre>
2207 
2208 
2209 // This code replaces a call to arraycopy; no exception may
2210 // be thrown in this code, they must be thrown in the System.arraycopy
2211 // activation frame; we could save some checks if this would not be the case
2212 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
2213   ciArrayKlass* default_type = op-&gt;expected_type();
2214   Register src = op-&gt;src()-&gt;as_register();
2215   Register dst = op-&gt;dst()-&gt;as_register();
2216   Register src_pos = op-&gt;src_pos()-&gt;as_register();
2217   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
2218   Register length  = op-&gt;length()-&gt;as_register();
2219   Register tmp = op-&gt;tmp()-&gt;as_register();
2220 
2221   __ resolve(ACCESS_READ, src);
2222   __ resolve(ACCESS_WRITE, dst);
2223 
2224   CodeStub* stub = op-&gt;stub();
2225   int flags = op-&gt;flags();
2226   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
<span class="line-modified">2227   if (basic_type == T_ARRAY) basic_type = T_OBJECT;</span>
2228 
2229   // if we don&#39;t know anything, just go through the generic arraycopy
2230   if (default_type == NULL // || basic_type == T_OBJECT
2231       ) {
2232     Label done;
2233     assert(src == r1 &amp;&amp; src_pos == r2, &quot;mismatch in calling convention&quot;);
2234 
2235     // Save the arguments in case the generic arraycopy fails and we
2236     // have to fall back to the JNI stub
2237     __ stp(dst,     dst_pos, Address(sp, 0*BytesPerWord));
2238     __ stp(length,  src_pos, Address(sp, 2*BytesPerWord));
2239     __ str(src,              Address(sp, 4*BytesPerWord));
2240 
2241     address copyfunc_addr = StubRoutines::generic_arraycopy();
2242     assert(copyfunc_addr != NULL, &quot;generic arraycopy stub required&quot;);
2243 
2244     // The arguments are in java calling convention so we shift them
2245     // to C convention
2246     assert_different_registers(c_rarg0, j_rarg1, j_rarg2, j_rarg3, j_rarg4);
2247     __ mov(c_rarg0, j_rarg0);
</pre>
<hr />
<pre>
2251     __ mov(c_rarg2, j_rarg2);
2252     assert_different_registers(c_rarg3, j_rarg4);
2253     __ mov(c_rarg3, j_rarg3);
2254     __ mov(c_rarg4, j_rarg4);
2255 #ifndef PRODUCT
2256     if (PrintC1Statistics) {
2257       __ incrementw(ExternalAddress((address)&amp;Runtime1::_generic_arraycopystub_cnt));
2258     }
2259 #endif
2260     __ far_call(RuntimeAddress(copyfunc_addr));
2261 
2262     __ cbz(r0, *stub-&gt;continuation());
2263 
2264     // Reload values from the stack so they are where the stub
2265     // expects them.
2266     __ ldp(dst,     dst_pos, Address(sp, 0*BytesPerWord));
2267     __ ldp(length,  src_pos, Address(sp, 2*BytesPerWord));
2268     __ ldr(src,              Address(sp, 4*BytesPerWord));
2269 
2270     // r0 is -1^K where K == partial copied count
<span class="line-modified">2271     __ eonw(rscratch1, r0, 0);</span>
2272     // adjust length down and src/end pos up by partial copied count
2273     __ subw(length, length, rscratch1);
2274     __ addw(src_pos, src_pos, rscratch1);
2275     __ addw(dst_pos, dst_pos, rscratch1);
2276     __ b(*stub-&gt;entry());
2277 
2278     __ bind(*stub-&gt;continuation());
2279     return;
2280   }
2281 
2282   assert(default_type != NULL &amp;&amp; default_type-&gt;is_array_klass() &amp;&amp; default_type-&gt;is_loaded(), &quot;must be true at this point&quot;);
2283 
2284   int elem_size = type2aelembytes(basic_type);
2285   int shift_amount;
2286   int scale = exact_log2(elem_size);
2287 
2288   Address src_length_addr = Address(src, arrayOopDesc::length_offset_in_bytes());
2289   Address dst_length_addr = Address(dst, arrayOopDesc::length_offset_in_bytes());
2290   Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());
2291   Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());
</pre>
<hr />
<pre>
2849   assert(tmp-&gt;is_illegal(), &quot;wasting a register if tmp is allocated&quot;);
2850 
2851   if (left-&gt;is_single_cpu()) {
2852     assert(dest-&gt;is_single_cpu(), &quot;expect single result reg&quot;);
2853     __ negw(dest-&gt;as_register(), left-&gt;as_register());
2854   } else if (left-&gt;is_double_cpu()) {
2855     assert(dest-&gt;is_double_cpu(), &quot;expect double result reg&quot;);
2856     __ neg(dest-&gt;as_register_lo(), left-&gt;as_register_lo());
2857   } else if (left-&gt;is_single_fpu()) {
2858     assert(dest-&gt;is_single_fpu(), &quot;expect single float result reg&quot;);
2859     __ fnegs(dest-&gt;as_float_reg(), left-&gt;as_float_reg());
2860   } else {
2861     assert(left-&gt;is_double_fpu(), &quot;expect double float operand reg&quot;);
2862     assert(dest-&gt;is_double_fpu(), &quot;expect double float result reg&quot;);
2863     __ fnegd(dest-&gt;as_double_reg(), left-&gt;as_double_reg());
2864   }
2865 }
2866 
2867 
2868 void LIR_Assembler::leal(LIR_Opr addr, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
<span class="line-modified">2869   assert(patch_code == lir_patch_none, &quot;Patch code not supported&quot;);</span>




2870   __ lea(dest-&gt;as_register_lo(), as_Address(addr-&gt;as_address_ptr()));
2871 }
2872 
2873 
2874 void LIR_Assembler::rt_call(LIR_Opr result, address dest, const LIR_OprList* args, LIR_Opr tmp, CodeEmitInfo* info) {
2875   assert(!tmp-&gt;is_valid(), &quot;don&#39;t need temporary&quot;);
2876 
2877   CodeBlob *cb = CodeCache::find_blob(dest);
2878   if (cb) {
2879     __ far_call(RuntimeAddress(dest));
2880   } else {
2881     __ mov(rscratch1, RuntimeAddress(dest));
<span class="line-modified">2882     int len = args-&gt;length();</span>
<span class="line-removed">2883     int type = 0;</span>
<span class="line-removed">2884     if (! result-&gt;is_illegal()) {</span>
<span class="line-removed">2885       switch (result-&gt;type()) {</span>
<span class="line-removed">2886       case T_VOID:</span>
<span class="line-removed">2887         type = 0;</span>
<span class="line-removed">2888         break;</span>
<span class="line-removed">2889       case T_INT:</span>
<span class="line-removed">2890       case T_LONG:</span>
<span class="line-removed">2891       case T_OBJECT:</span>
<span class="line-removed">2892         type = 1;</span>
<span class="line-removed">2893         break;</span>
<span class="line-removed">2894       case T_FLOAT:</span>
<span class="line-removed">2895         type = 2;</span>
<span class="line-removed">2896         break;</span>
<span class="line-removed">2897       case T_DOUBLE:</span>
<span class="line-removed">2898         type = 3;</span>
<span class="line-removed">2899         break;</span>
<span class="line-removed">2900       default:</span>
<span class="line-removed">2901         ShouldNotReachHere();</span>
<span class="line-removed">2902         break;</span>
<span class="line-removed">2903       }</span>
<span class="line-removed">2904     }</span>
<span class="line-removed">2905     int num_gpargs = 0;</span>
<span class="line-removed">2906     int num_fpargs = 0;</span>
<span class="line-removed">2907     for (int i = 0; i &lt; args-&gt;length(); i++) {</span>
<span class="line-removed">2908       LIR_Opr arg = args-&gt;at(i);</span>
<span class="line-removed">2909       if (arg-&gt;type() == T_FLOAT || arg-&gt;type() == T_DOUBLE) {</span>
<span class="line-removed">2910         num_fpargs++;</span>
<span class="line-removed">2911       } else {</span>
<span class="line-removed">2912         num_gpargs++;</span>
<span class="line-removed">2913       }</span>
<span class="line-removed">2914     }</span>
<span class="line-removed">2915     __ blrt(rscratch1, num_gpargs, num_fpargs, type);</span>
2916   }
2917 
2918   if (info != NULL) {
2919     add_call_info_here(info);
2920   }
2921   __ maybe_isb();
2922 }
2923 
2924 void LIR_Assembler::volatile_move_op(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info) {
2925   if (dest-&gt;is_address() || src-&gt;is_address()) {
2926     move_op(src, dest, type, lir_patch_none, info,
2927             /*pop_fpu_stack*/false, /*unaligned*/false, /*wide*/false);
2928   } else {
2929     ShouldNotReachHere();
2930   }
2931 }
2932 
2933 #ifdef ASSERT
2934 // emit run-time assertion
2935 void LIR_Assembler::emit_assert(LIR_OpAssert* op) {
</pre>
<hr />
<pre>
3124         // Insert the tableswitch instruction
3125         inst-&gt;insert_before(start_insn,
3126                             new LIR_Op2(lir_cmp, lir_cond_always,
3127                                         LIR_OprFact::intConst(tableswitch_count),
3128                                         reg_opr));
3129         inst-&gt;insert_before(start_insn + 1, new LIR_OpLabel(&amp;sw-&gt;_branches));
3130         tableswitch_count++;
3131       }
3132       reg = noreg;
3133       last_key = -2147483648;
3134     }
3135   next_state:
3136     ;
3137   }
3138 #endif
3139 }
3140 
3141 void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp_op) {
3142   Address addr = as_Address(src-&gt;as_address_ptr());
3143   BasicType type = src-&gt;type();
<span class="line-modified">3144   bool is_oop = type == T_OBJECT || type == T_ARRAY;</span>
3145 
3146   void (MacroAssembler::* add)(Register prev, RegisterOrConstant incr, Register addr);
3147   void (MacroAssembler::* xchg)(Register prev, Register newv, Register addr);
3148 
3149   switch(type) {
3150   case T_INT:
3151     xchg = &amp;MacroAssembler::atomic_xchgalw;
3152     add = &amp;MacroAssembler::atomic_addalw;
3153     break;
3154   case T_LONG:
3155     xchg = &amp;MacroAssembler::atomic_xchgal;
3156     add = &amp;MacroAssembler::atomic_addal;
3157     break;
3158   case T_OBJECT:
3159   case T_ARRAY:
3160     if (UseCompressedOops) {
3161       xchg = &amp;MacroAssembler::atomic_xchgalw;
3162       add = &amp;MacroAssembler::atomic_addalw;
3163     } else {
3164       xchg = &amp;MacroAssembler::atomic_xchgal;
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   3  * Copyright (c) 2014, Red Hat Inc. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;asm/assembler.hpp&quot;
  29 #include &quot;c1/c1_CodeStubs.hpp&quot;
  30 #include &quot;c1/c1_Compilation.hpp&quot;
  31 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  32 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  33 #include &quot;c1/c1_Runtime1.hpp&quot;
  34 #include &quot;c1/c1_ValueStack.hpp&quot;
  35 #include &quot;ci/ciArrayKlass.hpp&quot;
  36 #include &quot;ci/ciInstance.hpp&quot;
<span class="line-modified">  37 #include &quot;code/compiledIC.hpp&quot;</span>

  38 #include &quot;gc/shared/collectedHeap.hpp&quot;
  39 #include &quot;nativeInst_aarch64.hpp&quot;
  40 #include &quot;oops/objArrayKlass.hpp&quot;
  41 #include &quot;runtime/frame.inline.hpp&quot;
  42 #include &quot;runtime/sharedRuntime.hpp&quot;
<span class="line-added">  43 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  44 #include &quot;vmreg_aarch64.inline.hpp&quot;
  45 
  46 

  47 #ifndef PRODUCT
  48 #define COMMENT(x)   do { __ block_comment(x); } while (0)
  49 #else
  50 #define COMMENT(x)
  51 #endif
  52 
  53 NEEDS_CLEANUP // remove this definitions ?
  54 const Register IC_Klass    = rscratch2;   // where the IC klass is cached
  55 const Register SYNC_header = r0;   // synchronization header
  56 const Register SHIFT_count = r0;   // where count for shift operations must be
  57 
  58 #define __ _masm-&gt;
  59 
  60 
  61 static void select_different_registers(Register preserve,
  62                                        Register extra,
  63                                        Register &amp;tmp1,
  64                                        Register &amp;tmp2) {
  65   if (tmp1 == preserve) {
  66     assert_different_registers(tmp1, tmp2, extra);
</pre>
<hr />
<pre>
 121 address LIR_Assembler::double_constant(double d) {
 122   address const_addr = __ double_constant(d);
 123   if (const_addr == NULL) {
 124     bailout(&quot;const section overflow&quot;);
 125     return __ code()-&gt;consts()-&gt;start();
 126   } else {
 127     return const_addr;
 128   }
 129 }
 130 
 131 address LIR_Assembler::int_constant(jlong n) {
 132   address const_addr = __ long_constant(n);
 133   if (const_addr == NULL) {
 134     bailout(&quot;const section overflow&quot;);
 135     return __ code()-&gt;consts()-&gt;start();
 136   } else {
 137     return const_addr;
 138   }
 139 }
 140 












 141 void LIR_Assembler::breakpoint() { Unimplemented(); }
 142 
 143 void LIR_Assembler::push(LIR_Opr opr) { Unimplemented(); }
 144 
 145 void LIR_Assembler::pop(LIR_Opr opr) { Unimplemented(); }
 146 
 147 bool LIR_Assembler::is_literal_address(LIR_Address* addr) { Unimplemented(); return false; }
 148 //-------------------------------------------
 149 
 150 static Register as_reg(LIR_Opr op) {
 151   return op-&gt;is_double_cpu() ? op-&gt;as_register_lo() : op-&gt;as_register();
 152 }
 153 
 154 static jlong as_long(LIR_Opr data) {
 155   jlong result;
 156   switch (data-&gt;type()) {
 157   case T_INT:
 158     result = (data-&gt;as_jint());
 159     break;
 160   case T_LONG:
</pre>
<hr />
<pre>
 285   __ inline_cache_check(receiver, ic_klass);
 286 
 287   // if icache check fails, then jump to runtime routine
 288   // Note: RECEIVER must still contain the receiver!
 289   Label dont;
 290   __ br(Assembler::EQ, dont);
 291   __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
 292 
 293   // We align the verified entry point unless the method body
 294   // (including its inline cache check) will fit in a single 64-byte
 295   // icache line.
 296   if (! method()-&gt;is_accessor() || __ offset() - start_offset &gt; 4 * 4) {
 297     // force alignment after the cache check.
 298     __ align(CodeEntryAlignment);
 299   }
 300 
 301   __ bind(dont);
 302   return start_offset;
 303 }
 304 
<span class="line-added"> 305 void LIR_Assembler::clinit_barrier(ciMethod* method) {</span>
<span class="line-added"> 306   assert(VM_Version::supports_fast_class_init_checks(), &quot;sanity&quot;);</span>
<span class="line-added"> 307   assert(!method-&gt;holder()-&gt;is_not_initialized(), &quot;initialization should have been started&quot;);</span>
<span class="line-added"> 308 </span>
<span class="line-added"> 309   Label L_skip_barrier;</span>
<span class="line-added"> 310 </span>
<span class="line-added"> 311   __ mov_metadata(rscratch2, method-&gt;holder()-&gt;constant_encoding());</span>
<span class="line-added"> 312   __ clinit_barrier(rscratch2, rscratch1, &amp;L_skip_barrier /*L_fast_path*/);</span>
<span class="line-added"> 313   __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));</span>
<span class="line-added"> 314   __ bind(L_skip_barrier);</span>
<span class="line-added"> 315 }</span>
 316 
 317 void LIR_Assembler::jobject2reg(jobject o, Register reg) {
 318   if (o == NULL) {
 319     __ mov(reg, zr);
 320   } else {
 321     __ movoop(reg, o, /*immediate*/true);
 322   }
 323 }
 324 
 325 void LIR_Assembler::deoptimize_trap(CodeEmitInfo *info) {
 326   address target = NULL;
 327   relocInfo::relocType reloc_type = relocInfo::none;
 328 
 329   switch (patching_id(info)) {
 330   case PatchingStub::access_field_id:
 331     target = Runtime1::entry_for(Runtime1::access_field_patching_id);
 332     reloc_type = relocInfo::section_word_type;
 333     break;
 334   case PatchingStub::load_klass_id:
 335     target = Runtime1::entry_for(Runtime1::load_klass_patching_id);
</pre>
<hr />
<pre>
 697 }
 698 
 699 void LIR_Assembler::reg2reg(LIR_Opr src, LIR_Opr dest) {
 700   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 701   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 702 
 703   // move between cpu-registers
 704   if (dest-&gt;is_single_cpu()) {
 705     if (src-&gt;type() == T_LONG) {
 706       // Can do LONG -&gt; OBJECT
 707       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
 708       return;
 709     }
 710     assert(src-&gt;is_single_cpu(), &quot;must match&quot;);
 711     if (src-&gt;type() == T_OBJECT) {
 712       __ verify_oop(src-&gt;as_register());
 713     }
 714     move_regs(src-&gt;as_register(), dest-&gt;as_register());
 715 
 716   } else if (dest-&gt;is_double_cpu()) {
<span class="line-modified"> 717     if (is_reference_type(src-&gt;type())) {</span>
 718       // Surprising to me but we can see move of a long to t_object
 719       __ verify_oop(src-&gt;as_register());
 720       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
 721       return;
 722     }
 723     assert(src-&gt;is_double_cpu(), &quot;must match&quot;);
 724     Register f_lo = src-&gt;as_register_lo();
 725     Register f_hi = src-&gt;as_register_hi();
 726     Register t_lo = dest-&gt;as_register_lo();
 727     Register t_hi = dest-&gt;as_register_hi();
 728     assert(f_hi == f_lo, &quot;must be same&quot;);
 729     assert(t_hi == t_lo, &quot;must be same&quot;);
 730     move_regs(f_lo, t_lo);
 731 
 732   } else if (dest-&gt;is_single_fpu()) {
 733     __ fmovs(dest-&gt;as_float_reg(), src-&gt;as_float_reg());
 734 
 735   } else if (dest-&gt;is_double_fpu()) {
 736     __ fmovd(dest-&gt;as_double_reg(), src-&gt;as_double_reg());
 737 
 738   } else {
 739     ShouldNotReachHere();
 740   }
 741 }
 742 
 743 void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {
 744   if (src-&gt;is_single_cpu()) {
<span class="line-modified"> 745     if (is_reference_type(type)) {</span>
 746       __ str(src-&gt;as_register(), frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 747       __ verify_oop(src-&gt;as_register());
<span class="line-modified"> 748     } else if (type == T_METADATA || type == T_DOUBLE || type == T_ADDRESS) {</span>
 749       __ str(src-&gt;as_register(), frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 750     } else {
 751       __ strw(src-&gt;as_register(), frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 752     }
 753 
 754   } else if (src-&gt;is_double_cpu()) {
 755     Address dest_addr_LO = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), lo_word_offset_in_bytes);
 756     __ str(src-&gt;as_register_lo(), dest_addr_LO);
 757 
 758   } else if (src-&gt;is_single_fpu()) {
 759     Address dest_addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 760     __ strs(src-&gt;as_float_reg(), dest_addr);
 761 
 762   } else if (src-&gt;is_double_fpu()) {
 763     Address dest_addr = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
 764     __ strd(src-&gt;as_double_reg(), dest_addr);
 765 
 766   } else {
 767     ShouldNotReachHere();
 768   }
 769 
 770 }
 771 
 772 
 773 void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide, bool /* unaligned */) {
 774   LIR_Address* to_addr = dest-&gt;as_address_ptr();
 775   PatchingStub* patch = NULL;
 776   Register compressed_src = rscratch1;
 777 
 778   if (patch_code != lir_patch_none) {
 779     deoptimize_trap(info);
 780     return;
 781   }
 782 
<span class="line-modified"> 783   if (is_reference_type(type)) {</span>
 784     __ verify_oop(src-&gt;as_register());
 785 
 786     if (UseCompressedOops &amp;&amp; !wide) {
 787       __ encode_heap_oop(compressed_src, src-&gt;as_register());
 788     } else {
 789       compressed_src = src-&gt;as_register();
 790     }
 791   }
 792 
 793   int null_check_here = code_offset();
 794   switch (type) {
 795     case T_FLOAT: {
 796       __ strs(src-&gt;as_float_reg(), as_Address(to_addr));
 797       break;
 798     }
 799 
 800     case T_DOUBLE: {
 801       __ strd(src-&gt;as_double_reg(), as_Address(to_addr));
 802       break;
 803     }
</pre>
<hr />
<pre>
 838 
 839     case T_CHAR:    // fall through
 840     case T_SHORT:
 841       __ strh(src-&gt;as_register(), as_Address(to_addr));
 842       break;
 843 
 844     default:
 845       ShouldNotReachHere();
 846   }
 847   if (info != NULL) {
 848     add_debug_info_for_null_check(null_check_here, info);
 849   }
 850 }
 851 
 852 
 853 void LIR_Assembler::stack2reg(LIR_Opr src, LIR_Opr dest, BasicType type) {
 854   assert(src-&gt;is_stack(), &quot;should not call otherwise&quot;);
 855   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 856 
 857   if (dest-&gt;is_single_cpu()) {
<span class="line-modified"> 858     if (is_reference_type(type)) {</span>
 859       __ ldr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 860       __ verify_oop(dest-&gt;as_register());
<span class="line-modified"> 861     } else if (type == T_METADATA || type == T_ADDRESS) {</span>
 862       __ ldr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 863     } else {
 864       __ ldrw(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 865     }
 866 
 867   } else if (dest-&gt;is_double_cpu()) {
 868     Address src_addr_LO = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), lo_word_offset_in_bytes);
 869     __ ldr(dest-&gt;as_register_lo(), src_addr_LO);
 870 
 871   } else if (dest-&gt;is_single_fpu()) {
 872     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
 873     __ ldrs(dest-&gt;as_float_reg(), src_addr);
 874 
 875   } else if (dest-&gt;is_double_fpu()) {
 876     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
 877     __ ldrd(dest-&gt;as_double_reg(), src_addr);
 878 
 879   } else {
 880     ShouldNotReachHere();
 881   }
</pre>
<hr />
<pre>
 988 
 989     case T_BYTE:
 990       __ ldrsb(dest-&gt;as_register(), as_Address(from_addr));
 991       break;
 992     case T_BOOLEAN: {
 993       __ ldrb(dest-&gt;as_register(), as_Address(from_addr));
 994       break;
 995     }
 996 
 997     case T_CHAR:
 998       __ ldrh(dest-&gt;as_register(), as_Address(from_addr));
 999       break;
1000     case T_SHORT:
1001       __ ldrsh(dest-&gt;as_register(), as_Address(from_addr));
1002       break;
1003 
1004     default:
1005       ShouldNotReachHere();
1006   }
1007 
<span class="line-modified">1008   if (is_reference_type(type)) {</span>
1009     if (UseCompressedOops &amp;&amp; !wide) {
1010       __ decode_heap_oop(dest-&gt;as_register());
1011     }
<span class="line-modified">1012 </span>
<span class="line-added">1013     if (!UseZGC) {</span>
<span class="line-added">1014       // Load barrier has not yet been applied, so ZGC can&#39;t verify the oop here</span>
<span class="line-added">1015       __ verify_oop(dest-&gt;as_register());</span>
<span class="line-added">1016     }</span>
1017   } else if (type == T_ADDRESS &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1018     if (UseCompressedClassPointers) {
1019       __ decode_klass_not_null(dest-&gt;as_register());
1020     }
1021   }
1022 }
1023 
1024 
1025 int LIR_Assembler::array_element_size(BasicType type) const {
1026   int elem_size = type2aelembytes(type);
1027   return exact_log2(elem_size);
1028 }
1029 
1030 
1031 void LIR_Assembler::emit_op3(LIR_Op3* op) {
1032   switch (op-&gt;code()) {
1033   case lir_idiv:
1034   case lir_irem:
1035     arithmetic_idiv(op-&gt;code(),
1036                     op-&gt;in_opr1(),
</pre>
<hr />
<pre>
1055   }
1056 }
1057 
1058 void LIR_Assembler::emit_opBranch(LIR_OpBranch* op) {
1059 #ifdef ASSERT
1060   assert(op-&gt;block() == NULL || op-&gt;block()-&gt;label() == op-&gt;label(), &quot;wrong label&quot;);
1061   if (op-&gt;block() != NULL)  _branch_target_blocks.append(op-&gt;block());
1062   if (op-&gt;ublock() != NULL) _branch_target_blocks.append(op-&gt;ublock());
1063 #endif
1064 
1065   if (op-&gt;cond() == lir_cond_always) {
1066     if (op-&gt;info() != NULL) add_debug_info_for_branch(op-&gt;info());
1067     __ b(*(op-&gt;label()));
1068   } else {
1069     Assembler::Condition acond;
1070     if (op-&gt;code() == lir_cond_float_branch) {
1071       bool is_unordered = (op-&gt;ublock() == op-&gt;block());
1072       // Assembler::EQ does not permit unordered branches, so we add
1073       // another branch here.  Likewise, Assembler::NE does not permit
1074       // ordered branches.
<span class="line-modified">1075       if ((is_unordered &amp;&amp; op-&gt;cond() == lir_cond_equal)</span>
<span class="line-modified">1076           || (!is_unordered &amp;&amp; op-&gt;cond() == lir_cond_notEqual))</span>
1077         __ br(Assembler::VS, *(op-&gt;ublock()-&gt;label()));
1078       switch(op-&gt;cond()) {
1079       case lir_cond_equal:        acond = Assembler::EQ; break;
1080       case lir_cond_notEqual:     acond = Assembler::NE; break;
1081       case lir_cond_less:         acond = (is_unordered ? Assembler::LT : Assembler::LO); break;
1082       case lir_cond_lessEqual:    acond = (is_unordered ? Assembler::LE : Assembler::LS); break;
1083       case lir_cond_greaterEqual: acond = (is_unordered ? Assembler::HS : Assembler::GE); break;
1084       case lir_cond_greater:      acond = (is_unordered ? Assembler::HI : Assembler::GT); break;
1085       default:                    ShouldNotReachHere();
1086         acond = Assembler::EQ;  // unreachable
1087       }
1088     } else {
1089       switch (op-&gt;cond()) {
1090         case lir_cond_equal:        acond = Assembler::EQ; break;
1091         case lir_cond_notEqual:     acond = Assembler::NE; break;
1092         case lir_cond_less:         acond = Assembler::LT; break;
1093         case lir_cond_lessEqual:    acond = Assembler::LE; break;
1094         case lir_cond_greaterEqual: acond = Assembler::GE; break;
1095         case lir_cond_greater:      acond = Assembler::GT; break;
1096         case lir_cond_belowEqual:   acond = Assembler::LS; break;
</pre>
<hr />
<pre>
1196                                InstanceKlass::init_state_offset()));
1197     __ cmpw(rscratch1, InstanceKlass::fully_initialized);
1198     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
1199     __ br(Assembler::NE, *op-&gt;stub()-&gt;entry());
1200   }
1201   __ allocate_object(op-&gt;obj()-&gt;as_register(),
1202                      op-&gt;tmp1()-&gt;as_register(),
1203                      op-&gt;tmp2()-&gt;as_register(),
1204                      op-&gt;header_size(),
1205                      op-&gt;object_size(),
1206                      op-&gt;klass()-&gt;as_register(),
1207                      *op-&gt;stub()-&gt;entry());
1208   __ bind(*op-&gt;stub()-&gt;continuation());
1209 }
1210 
1211 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
1212   Register len =  op-&gt;len()-&gt;as_register();
1213   __ uxtw(len, len);
1214 
1215   if (UseSlowPath ||
<span class="line-modified">1216       (!UseFastNewObjectArray &amp;&amp; is_reference_type(op-&gt;type())) ||</span>
<span class="line-modified">1217       (!UseFastNewTypeArray   &amp;&amp; !is_reference_type(op-&gt;type()))) {</span>
1218     __ b(*op-&gt;stub()-&gt;entry());
1219   } else {
1220     Register tmp1 = op-&gt;tmp1()-&gt;as_register();
1221     Register tmp2 = op-&gt;tmp2()-&gt;as_register();
1222     Register tmp3 = op-&gt;tmp3()-&gt;as_register();
1223     if (len == tmp1) {
1224       tmp1 = tmp3;
1225     } else if (len == tmp2) {
1226       tmp2 = tmp3;
1227     } else if (len == tmp3) {
1228       // everything is ok
1229     } else {
1230       __ mov(tmp3, len);
1231     }
1232     __ allocate_array(op-&gt;obj()-&gt;as_register(),
1233                       len,
1234                       tmp1,
1235                       tmp2,
1236                       arrayOopDesc::header_size(op-&gt;type()),
1237                       array_element_size(op-&gt;type()),
</pre>
<hr />
<pre>
1730       case lir_mul: __ mul (dest-&gt;as_register_lo(), lreg_lo, rreg_lo); break;
1731       case lir_div: __ corrected_idivq(dest-&gt;as_register_lo(), lreg_lo, rreg_lo, false, rscratch1); break;
1732       case lir_rem: __ corrected_idivq(dest-&gt;as_register_lo(), lreg_lo, rreg_lo, true, rscratch1); break;
1733       default:
1734         ShouldNotReachHere();
1735       }
1736 
1737     } else if (right-&gt;is_constant()) {
1738       jlong c = right-&gt;as_constant_ptr()-&gt;as_jlong();
1739       Register dreg = as_reg(dest);
1740       switch (code) {
1741         case lir_add:
1742         case lir_sub:
1743           if (c == 0 &amp;&amp; dreg == lreg_lo) {
1744             COMMENT(&quot;effective nop elided&quot;);
1745             return;
1746           }
1747           code == lir_add ? __ add(dreg, lreg_lo, c) : __ sub(dreg, lreg_lo, c);
1748           break;
1749         case lir_div:
<span class="line-modified">1750           assert(c &gt; 0 &amp;&amp; is_power_of_2(c), &quot;divisor must be power-of-2 constant&quot;);</span>
1751           if (c == 1) {
1752             // move lreg_lo to dreg if divisor is 1
1753             __ mov(dreg, lreg_lo);
1754           } else {
1755             unsigned int shift = exact_log2_long(c);
1756             // use rscratch1 as intermediate result register
1757             __ asr(rscratch1, lreg_lo, 63);
1758             __ add(rscratch1, lreg_lo, rscratch1, Assembler::LSR, 64 - shift);
1759             __ asr(dreg, rscratch1, shift);
1760           }
1761           break;
1762         case lir_rem:
<span class="line-modified">1763           assert(c &gt; 0 &amp;&amp; is_power_of_2(c), &quot;divisor must be power-of-2 constant&quot;);</span>
1764           if (c == 1) {
1765             // move 0 to dreg if divisor is 1
1766             __ mov(dreg, zr);
1767           } else {
1768             // use rscratch1 as intermediate result register
1769             __ negs(rscratch1, lreg_lo);
1770             __ andr(dreg, lreg_lo, c - 1);
1771             __ andr(rscratch1, rscratch1, c - 1);
1772             __ csneg(dreg, dreg, rscratch1, Assembler::MI);
1773           }
1774           break;
1775         default:
1776           ShouldNotReachHere();
1777       }
1778     } else {
1779       ShouldNotReachHere();
1780     }
1781   } else if (left-&gt;is_single_fpu()) {
1782     assert(right-&gt;is_single_fpu(), &quot;right hand side of float arithmetics needs to be float register&quot;);
1783     switch (code) {
1784     case lir_add: __ fadds (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
1785     case lir_sub: __ fsubs (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
<span class="line-added">1786     case lir_mul_strictfp: // fall through</span>
1787     case lir_mul: __ fmuls (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
<span class="line-added">1788     case lir_div_strictfp: // fall through</span>
1789     case lir_div: __ fdivs (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
1790     default:
1791       ShouldNotReachHere();
1792     }
1793   } else if (left-&gt;is_double_fpu()) {
1794     if (right-&gt;is_double_fpu()) {
<span class="line-modified">1795       // fpu register - fpu register</span>
1796       switch (code) {
1797       case lir_add: __ faddd (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
1798       case lir_sub: __ fsubd (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
<span class="line-added">1799       case lir_mul_strictfp: // fall through</span>
1800       case lir_mul: __ fmuld (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
<span class="line-added">1801       case lir_div_strictfp: // fall through</span>
1802       case lir_div: __ fdivd (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
1803       default:
1804         ShouldNotReachHere();
1805       }
1806     } else {
1807       if (right-&gt;is_constant()) {
1808         ShouldNotReachHere();
1809       }
1810       ShouldNotReachHere();
1811     }
1812   } else if (left-&gt;is_single_stack() || left-&gt;is_address()) {
1813     assert(left == dest, &quot;left and dest must be equal&quot;);
1814     ShouldNotReachHere();
1815   } else {
1816     ShouldNotReachHere();
1817   }
1818 }
1819 
1820 void LIR_Assembler::arith_fpu_implementation(LIR_Code code, int left_index, int right_index, int dest_index, bool pop_fpu_stack) { Unimplemented(); }
1821 
</pre>
<hr />
<pre>
1917       }
1918     }
1919   } else {
1920     Register rreg = right-&gt;as_register();
1921     __ corrected_idivl(dreg, lreg, rreg, is_irem, rscratch1);
1922   }
1923 }
1924 
1925 
1926 void LIR_Assembler::comp_op(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Op2* op) {
1927   if (opr1-&gt;is_constant() &amp;&amp; opr2-&gt;is_single_cpu()) {
1928     // tableswitch
1929     Register reg = as_reg(opr2);
1930     struct tableswitch &amp;table = switches[opr1-&gt;as_constant_ptr()-&gt;as_jint()];
1931     __ tableswitch(reg, table._first_key, table._last_key, table._branches, table._after);
1932   } else if (opr1-&gt;is_single_cpu() || opr1-&gt;is_double_cpu()) {
1933     Register reg1 = as_reg(opr1);
1934     if (opr2-&gt;is_single_cpu()) {
1935       // cpu register - cpu register
1936       Register reg2 = opr2-&gt;as_register();
<span class="line-modified">1937       if (is_reference_type(opr1-&gt;type())) {</span>
1938         __ cmpoop(reg1, reg2);
1939       } else {
<span class="line-modified">1940         assert(!is_reference_type(opr2-&gt;type()), &quot;cmp int, oop?&quot;);</span>
1941         __ cmpw(reg1, reg2);
1942       }
1943       return;
1944     }
1945     if (opr2-&gt;is_double_cpu()) {
1946       // cpu register - cpu register
1947       Register reg2 = opr2-&gt;as_register_lo();
1948       __ cmp(reg1, reg2);
1949       return;
1950     }
1951 
1952     if (opr2-&gt;is_constant()) {
1953       bool is_32bit = false; // width of register operand
1954       jlong imm;
1955 
1956       switch(opr2-&gt;type()) {
1957       case T_INT:
1958         imm = opr2-&gt;as_constant_ptr()-&gt;as_jint();
1959         is_32bit = true;
1960         break;
1961       case T_LONG:
1962         imm = opr2-&gt;as_constant_ptr()-&gt;as_jlong();
1963         break;
1964       case T_ADDRESS:
1965         imm = opr2-&gt;as_constant_ptr()-&gt;as_jint();
1966         break;
<span class="line-added">1967       case T_METADATA:</span>
<span class="line-added">1968         imm = (intptr_t)(opr2-&gt;as_constant_ptr()-&gt;as_metadata());</span>
<span class="line-added">1969         break;</span>
1970       case T_OBJECT:
1971       case T_ARRAY:
1972         jobject2reg(opr2-&gt;as_constant_ptr()-&gt;as_jobject(), rscratch1);
1973         __ cmpoop(reg1, rscratch1);
1974         return;
1975       default:
1976         ShouldNotReachHere();
1977         imm = 0;  // unreachable
1978         break;
1979       }
1980 
1981       if (Assembler::operand_valid_for_add_sub_immediate(imm)) {
1982         if (is_32bit)
1983           __ cmpw(reg1, imm);
1984         else
1985           __ subs(zr, reg1, imm);
1986         return;
1987       } else {
1988         __ mov(rscratch1, imm);
1989         if (is_32bit)
</pre>
<hr />
<pre>
2055 }
2056 
2057 
2058 /* Currently, vtable-dispatch is only enabled for sparc platforms */
2059 void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {
2060   ShouldNotReachHere();
2061 }
2062 
2063 
2064 void LIR_Assembler::emit_static_call_stub() {
2065   address call_pc = __ pc();
2066   address stub = __ start_a_stub(call_stub_size());
2067   if (stub == NULL) {
2068     bailout(&quot;static call stub overflow&quot;);
2069     return;
2070   }
2071 
2072   int start = __ offset();
2073 
2074   __ relocate(static_stub_Relocation::spec(call_pc));
<span class="line-modified">2075   __ emit_static_call_stub();</span>


2076 
<span class="line-modified">2077   assert(__ offset() - start + CompiledStaticCall::to_trampoline_stub_size()</span>
<span class="line-added">2078         &lt;= call_stub_size(), &quot;stub too big&quot;);</span>
2079   __ end_a_stub();
2080 }
2081 
2082 
2083 void LIR_Assembler::throw_op(LIR_Opr exceptionPC, LIR_Opr exceptionOop, CodeEmitInfo* info) {
2084   assert(exceptionOop-&gt;as_register() == r0, &quot;must match&quot;);
2085   assert(exceptionPC-&gt;as_register() == r3, &quot;must match&quot;);
2086 
2087   // exception object is not added to oop map by LinearScan
2088   // (LinearScan assumes that no oops are in fixed registers)
2089   info-&gt;add_register_oop(exceptionOop);
2090   Runtime1::StubID unwind_id;
2091 
2092   // get current pc information
2093   // pc is only needed if the method has an exception handler, the unwind code does not need it.
2094   int pc_for_athrow_offset = __ offset();
2095   InternalAddress pc_for_athrow(__ pc());
2096   __ adr(exceptionPC-&gt;as_register(), pc_for_athrow);
2097   add_call_info(pc_for_athrow_offset, info); // for exception handler
2098 
</pre>
<hr />
<pre>
2215 
2216 
2217 // This code replaces a call to arraycopy; no exception may
2218 // be thrown in this code, they must be thrown in the System.arraycopy
2219 // activation frame; we could save some checks if this would not be the case
2220 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
2221   ciArrayKlass* default_type = op-&gt;expected_type();
2222   Register src = op-&gt;src()-&gt;as_register();
2223   Register dst = op-&gt;dst()-&gt;as_register();
2224   Register src_pos = op-&gt;src_pos()-&gt;as_register();
2225   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
2226   Register length  = op-&gt;length()-&gt;as_register();
2227   Register tmp = op-&gt;tmp()-&gt;as_register();
2228 
2229   __ resolve(ACCESS_READ, src);
2230   __ resolve(ACCESS_WRITE, dst);
2231 
2232   CodeStub* stub = op-&gt;stub();
2233   int flags = op-&gt;flags();
2234   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
<span class="line-modified">2235   if (is_reference_type(basic_type)) basic_type = T_OBJECT;</span>
2236 
2237   // if we don&#39;t know anything, just go through the generic arraycopy
2238   if (default_type == NULL // || basic_type == T_OBJECT
2239       ) {
2240     Label done;
2241     assert(src == r1 &amp;&amp; src_pos == r2, &quot;mismatch in calling convention&quot;);
2242 
2243     // Save the arguments in case the generic arraycopy fails and we
2244     // have to fall back to the JNI stub
2245     __ stp(dst,     dst_pos, Address(sp, 0*BytesPerWord));
2246     __ stp(length,  src_pos, Address(sp, 2*BytesPerWord));
2247     __ str(src,              Address(sp, 4*BytesPerWord));
2248 
2249     address copyfunc_addr = StubRoutines::generic_arraycopy();
2250     assert(copyfunc_addr != NULL, &quot;generic arraycopy stub required&quot;);
2251 
2252     // The arguments are in java calling convention so we shift them
2253     // to C convention
2254     assert_different_registers(c_rarg0, j_rarg1, j_rarg2, j_rarg3, j_rarg4);
2255     __ mov(c_rarg0, j_rarg0);
</pre>
<hr />
<pre>
2259     __ mov(c_rarg2, j_rarg2);
2260     assert_different_registers(c_rarg3, j_rarg4);
2261     __ mov(c_rarg3, j_rarg3);
2262     __ mov(c_rarg4, j_rarg4);
2263 #ifndef PRODUCT
2264     if (PrintC1Statistics) {
2265       __ incrementw(ExternalAddress((address)&amp;Runtime1::_generic_arraycopystub_cnt));
2266     }
2267 #endif
2268     __ far_call(RuntimeAddress(copyfunc_addr));
2269 
2270     __ cbz(r0, *stub-&gt;continuation());
2271 
2272     // Reload values from the stack so they are where the stub
2273     // expects them.
2274     __ ldp(dst,     dst_pos, Address(sp, 0*BytesPerWord));
2275     __ ldp(length,  src_pos, Address(sp, 2*BytesPerWord));
2276     __ ldr(src,              Address(sp, 4*BytesPerWord));
2277 
2278     // r0 is -1^K where K == partial copied count
<span class="line-modified">2279     __ eonw(rscratch1, r0, zr);</span>
2280     // adjust length down and src/end pos up by partial copied count
2281     __ subw(length, length, rscratch1);
2282     __ addw(src_pos, src_pos, rscratch1);
2283     __ addw(dst_pos, dst_pos, rscratch1);
2284     __ b(*stub-&gt;entry());
2285 
2286     __ bind(*stub-&gt;continuation());
2287     return;
2288   }
2289 
2290   assert(default_type != NULL &amp;&amp; default_type-&gt;is_array_klass() &amp;&amp; default_type-&gt;is_loaded(), &quot;must be true at this point&quot;);
2291 
2292   int elem_size = type2aelembytes(basic_type);
2293   int shift_amount;
2294   int scale = exact_log2(elem_size);
2295 
2296   Address src_length_addr = Address(src, arrayOopDesc::length_offset_in_bytes());
2297   Address dst_length_addr = Address(dst, arrayOopDesc::length_offset_in_bytes());
2298   Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());
2299   Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());
</pre>
<hr />
<pre>
2857   assert(tmp-&gt;is_illegal(), &quot;wasting a register if tmp is allocated&quot;);
2858 
2859   if (left-&gt;is_single_cpu()) {
2860     assert(dest-&gt;is_single_cpu(), &quot;expect single result reg&quot;);
2861     __ negw(dest-&gt;as_register(), left-&gt;as_register());
2862   } else if (left-&gt;is_double_cpu()) {
2863     assert(dest-&gt;is_double_cpu(), &quot;expect double result reg&quot;);
2864     __ neg(dest-&gt;as_register_lo(), left-&gt;as_register_lo());
2865   } else if (left-&gt;is_single_fpu()) {
2866     assert(dest-&gt;is_single_fpu(), &quot;expect single float result reg&quot;);
2867     __ fnegs(dest-&gt;as_float_reg(), left-&gt;as_float_reg());
2868   } else {
2869     assert(left-&gt;is_double_fpu(), &quot;expect double float operand reg&quot;);
2870     assert(dest-&gt;is_double_fpu(), &quot;expect double float result reg&quot;);
2871     __ fnegd(dest-&gt;as_double_reg(), left-&gt;as_double_reg());
2872   }
2873 }
2874 
2875 
2876 void LIR_Assembler::leal(LIR_Opr addr, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
<span class="line-modified">2877   if (patch_code != lir_patch_none) {</span>
<span class="line-added">2878     deoptimize_trap(info);</span>
<span class="line-added">2879     return;</span>
<span class="line-added">2880   }</span>
<span class="line-added">2881 </span>
2882   __ lea(dest-&gt;as_register_lo(), as_Address(addr-&gt;as_address_ptr()));
2883 }
2884 
2885 
2886 void LIR_Assembler::rt_call(LIR_Opr result, address dest, const LIR_OprList* args, LIR_Opr tmp, CodeEmitInfo* info) {
2887   assert(!tmp-&gt;is_valid(), &quot;don&#39;t need temporary&quot;);
2888 
2889   CodeBlob *cb = CodeCache::find_blob(dest);
2890   if (cb) {
2891     __ far_call(RuntimeAddress(dest));
2892   } else {
2893     __ mov(rscratch1, RuntimeAddress(dest));
<span class="line-modified">2894     __ blr(rscratch1);</span>

































2895   }
2896 
2897   if (info != NULL) {
2898     add_call_info_here(info);
2899   }
2900   __ maybe_isb();
2901 }
2902 
2903 void LIR_Assembler::volatile_move_op(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info) {
2904   if (dest-&gt;is_address() || src-&gt;is_address()) {
2905     move_op(src, dest, type, lir_patch_none, info,
2906             /*pop_fpu_stack*/false, /*unaligned*/false, /*wide*/false);
2907   } else {
2908     ShouldNotReachHere();
2909   }
2910 }
2911 
2912 #ifdef ASSERT
2913 // emit run-time assertion
2914 void LIR_Assembler::emit_assert(LIR_OpAssert* op) {
</pre>
<hr />
<pre>
3103         // Insert the tableswitch instruction
3104         inst-&gt;insert_before(start_insn,
3105                             new LIR_Op2(lir_cmp, lir_cond_always,
3106                                         LIR_OprFact::intConst(tableswitch_count),
3107                                         reg_opr));
3108         inst-&gt;insert_before(start_insn + 1, new LIR_OpLabel(&amp;sw-&gt;_branches));
3109         tableswitch_count++;
3110       }
3111       reg = noreg;
3112       last_key = -2147483648;
3113     }
3114   next_state:
3115     ;
3116   }
3117 #endif
3118 }
3119 
3120 void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp_op) {
3121   Address addr = as_Address(src-&gt;as_address_ptr());
3122   BasicType type = src-&gt;type();
<span class="line-modified">3123   bool is_oop = is_reference_type(type);</span>
3124 
3125   void (MacroAssembler::* add)(Register prev, RegisterOrConstant incr, Register addr);
3126   void (MacroAssembler::* xchg)(Register prev, Register newv, Register addr);
3127 
3128   switch(type) {
3129   case T_INT:
3130     xchg = &amp;MacroAssembler::atomic_xchgalw;
3131     add = &amp;MacroAssembler::atomic_addalw;
3132     break;
3133   case T_LONG:
3134     xchg = &amp;MacroAssembler::atomic_xchgal;
3135     add = &amp;MacroAssembler::atomic_addal;
3136     break;
3137   case T_OBJECT:
3138   case T_ARRAY:
3139     if (UseCompressedOops) {
3140       xchg = &amp;MacroAssembler::atomic_xchgalw;
3141       add = &amp;MacroAssembler::atomic_addalw;
3142     } else {
3143       xchg = &amp;MacroAssembler::atomic_xchgal;
</pre>
</td>
</tr>
</table>
<center><a href="c1_FrameMap_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_LIRAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>