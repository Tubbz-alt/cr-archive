<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="sharedRuntime_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stubRoutines_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/stubGenerator_aarch64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.hpp&quot;
  28 #include &quot;asm/macroAssembler.inline.hpp&quot;
  29 #include &quot;gc/shared/barrierSet.hpp&quot;
  30 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  31 #include &quot;interpreter/interpreter.hpp&quot;

  32 #include &quot;nativeInst_aarch64.hpp&quot;
  33 #include &quot;oops/instanceOop.hpp&quot;
  34 #include &quot;oops/method.hpp&quot;
  35 #include &quot;oops/objArrayKlass.hpp&quot;
  36 #include &quot;oops/oop.inline.hpp&quot;
  37 #include &quot;prims/methodHandles.hpp&quot;
  38 #include &quot;runtime/frame.inline.hpp&quot;
  39 #include &quot;runtime/handles.inline.hpp&quot;
  40 #include &quot;runtime/sharedRuntime.hpp&quot;
  41 #include &quot;runtime/stubCodeGenerator.hpp&quot;
  42 #include &quot;runtime/stubRoutines.hpp&quot;
  43 #include &quot;runtime/thread.inline.hpp&quot;
  44 #include &quot;utilities/align.hpp&quot;

  45 #ifdef COMPILER2
  46 #include &quot;opto/runtime.hpp&quot;
  47 #endif
<span class="line-modified">  48 </span>
<span class="line-modified">  49 #ifdef BUILTIN_SIM</span>
<span class="line-removed">  50 #include &quot;../../../../../../simulator/simulator.hpp&quot;</span>
  51 #endif
  52 
  53 // Declaration and definition of StubGenerator (no .hpp file).
  54 // For a more detailed description of the stub routine structure
  55 // see the comment in stubRoutines.hpp
  56 
  57 #undef __
  58 #define __ _masm-&gt;
  59 #define TIMES_OOP Address::sxtw(exact_log2(UseCompressedOops ? 4 : 8))
  60 
  61 #ifdef PRODUCT
  62 #define BLOCK_COMMENT(str) /* nothing */
  63 #else
  64 #define BLOCK_COMMENT(str) __ block_comment(str)
  65 #endif
  66 
  67 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  68 
  69 // Stub Code definitions
  70 
</pre>
<hr />
<pre>
 200     const Address result_type   (rfp, result_type_off    * wordSize);
 201     const Address method        (rfp, method_off         * wordSize);
 202     const Address entry_point   (rfp, entry_point_off    * wordSize);
 203     const Address parameter_size(rfp, parameter_size_off * wordSize);
 204 
 205     const Address thread        (rfp, thread_off         * wordSize);
 206 
 207     const Address d15_save      (rfp, d15_off * wordSize);
 208     const Address d13_save      (rfp, d13_off * wordSize);
 209     const Address d11_save      (rfp, d11_off * wordSize);
 210     const Address d9_save       (rfp, d9_off * wordSize);
 211 
 212     const Address r28_save      (rfp, r28_off * wordSize);
 213     const Address r26_save      (rfp, r26_off * wordSize);
 214     const Address r24_save      (rfp, r24_off * wordSize);
 215     const Address r22_save      (rfp, r22_off * wordSize);
 216     const Address r20_save      (rfp, r20_off * wordSize);
 217 
 218     // stub code
 219 
<span class="line-removed"> 220     // we need a C prolog to bootstrap the x86 caller into the sim</span>
<span class="line-removed"> 221     __ c_stub_prolog(8, 0, MacroAssembler::ret_type_void);</span>
<span class="line-removed"> 222 </span>
 223     address aarch64_entry = __ pc();
 224 
<span class="line-removed"> 225 #ifdef BUILTIN_SIM</span>
<span class="line-removed"> 226     // Save sender&#39;s SP for stack traces.</span>
<span class="line-removed"> 227     __ mov(rscratch1, sp);</span>
<span class="line-removed"> 228     __ str(rscratch1, Address(__ pre(sp, -2 * wordSize)));</span>
<span class="line-removed"> 229 #endif</span>
 230     // set up frame and move sp to end of save area
 231     __ enter();
 232     __ sub(sp, rfp, -sp_after_call_off * wordSize);
 233 
 234     // save register parameters and Java scratch/global registers
 235     // n.b. we save thread even though it gets installed in
 236     // rthread because we want to sanity check rthread later
 237     __ str(c_rarg7,  thread);
 238     __ strw(c_rarg6, parameter_size);
 239     __ stp(c_rarg4, c_rarg5,  entry_point);
 240     __ stp(c_rarg2, c_rarg3,  result_type);
 241     __ stp(c_rarg0, c_rarg1,  call_wrapper);
 242 
 243     __ stp(r20, r19,   r20_save);
 244     __ stp(r22, r21,   r22_save);
 245     __ stp(r24, r23,   r24_save);
 246     __ stp(r26, r25,   r26_save);
 247     __ stp(r28, r27,   r28_save);
 248 
 249     __ stpd(v9,  v8,   d9_save);
</pre>
<hr />
<pre>
 280     Label parameters_done;
 281     // parameter count is still in c_rarg6
 282     // and parameter pointer identifying param 1 is in c_rarg5
 283     __ cbzw(c_rarg6, parameters_done);
 284 
 285     address loop = __ pc();
 286     __ ldr(rscratch1, Address(__ post(c_rarg5, wordSize)));
 287     __ subsw(c_rarg6, c_rarg6, 1);
 288     __ push(rscratch1);
 289     __ br(Assembler::GT, loop);
 290 
 291     __ BIND(parameters_done);
 292 
 293     // call Java entry -- passing methdoOop, and current sp
 294     //      rmethod: Method*
 295     //      r13: sender sp
 296     BLOCK_COMMENT(&quot;call Java function&quot;);
 297     __ mov(r13, sp);
 298     __ blr(c_rarg4);
 299 
<span class="line-removed"> 300     // tell the simulator we have returned to the stub</span>
<span class="line-removed"> 301 </span>
 302     // we do this here because the notify will already have been done
 303     // if we get to the next instruction via an exception
 304     //
 305     // n.b. adding this instruction here affects the calculation of
 306     // whether or not a routine returns to the call stub (used when
 307     // doing stack walks) since the normal test is to check the return
 308     // pc against the address saved below. so we may need to allow for
 309     // this extra instruction in the check.
 310 
<span class="line-removed"> 311     if (NotifySimulator) {</span>
<span class="line-removed"> 312       __ notify(Assembler::method_reentry);</span>
<span class="line-removed"> 313     }</span>
 314     // save current address for use by exception handling code
 315 
 316     return_address = __ pc();
 317 
 318     // store result depending on type (everything that is not
 319     // T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)
 320     // n.b. this assumes Java returns an integral result in r0
 321     // and a floating result in j_farg0
 322     __ ldr(j_rarg2, result);
 323     Label is_long, is_float, is_double, exit;
 324     __ ldr(j_rarg1, result_type);
 325     __ cmp(j_rarg1, (u1)T_OBJECT);
 326     __ br(Assembler::EQ, is_long);
 327     __ cmp(j_rarg1, (u1)T_LONG);
 328     __ br(Assembler::EQ, is_long);
 329     __ cmp(j_rarg1, (u1)T_FLOAT);
 330     __ br(Assembler::EQ, is_float);
 331     __ cmp(j_rarg1, (u1)T_DOUBLE);
 332     __ br(Assembler::EQ, is_double);
 333 
</pre>
<hr />
<pre>
 356 #endif
 357 
 358     // restore callee-save registers
 359     __ ldpd(v15, v14,  d15_save);
 360     __ ldpd(v13, v12,  d13_save);
 361     __ ldpd(v11, v10,  d11_save);
 362     __ ldpd(v9,  v8,   d9_save);
 363 
 364     __ ldp(r28, r27,   r28_save);
 365     __ ldp(r26, r25,   r26_save);
 366     __ ldp(r24, r23,   r24_save);
 367     __ ldp(r22, r21,   r22_save);
 368     __ ldp(r20, r19,   r20_save);
 369 
 370     __ ldp(c_rarg0, c_rarg1,  call_wrapper);
 371     __ ldrw(c_rarg2, result_type);
 372     __ ldr(c_rarg3,  method);
 373     __ ldp(c_rarg4, c_rarg5,  entry_point);
 374     __ ldp(c_rarg6, c_rarg7,  parameter_size);
 375 
<span class="line-removed"> 376 #ifndef PRODUCT</span>
<span class="line-removed"> 377     // tell the simulator we are about to end Java execution</span>
<span class="line-removed"> 378     if (NotifySimulator) {</span>
<span class="line-removed"> 379       __ notify(Assembler::method_exit);</span>
<span class="line-removed"> 380     }</span>
<span class="line-removed"> 381 #endif</span>
 382     // leave frame and return to caller
 383     __ leave();
 384     __ ret(lr);
 385 
 386     // handle return types different from T_INT
 387 
 388     __ BIND(is_long);
 389     __ str(r0, Address(j_rarg2, 0));
 390     __ br(Assembler::AL, exit);
 391 
 392     __ BIND(is_float);
 393     __ strs(j_farg0, Address(j_rarg2, 0));
 394     __ br(Assembler::AL, exit);
 395 
 396     __ BIND(is_double);
 397     __ strd(j_farg0, Address(j_rarg2, 0));
 398     __ br(Assembler::AL, exit);
 399 
 400     return start;
 401   }
 402 
 403   // Return point for a Java call if there&#39;s an exception thrown in
 404   // Java code.  The exception is caught and transformed into a
 405   // pending exception stored in JavaThread that can be tested from
 406   // within the VM.
 407   //
 408   // Note: Usually the parameters are removed by the callee. In case
 409   // of an exception crossing an activation frame boundary, that is
 410   // not the case if the callee is compiled code =&gt; need to setup the
 411   // rsp.
 412   //
 413   // r0: exception oop
 414 
<span class="line-removed"> 415   // NOTE: this is used as a target from the signal handler so it</span>
<span class="line-removed"> 416   // needs an x86 prolog which returns into the current simulator</span>
<span class="line-removed"> 417   // executing the generated catch_exception code. so the prolog</span>
<span class="line-removed"> 418   // needs to install rax in a sim register and adjust the sim&#39;s</span>
<span class="line-removed"> 419   // restart pc to enter the generated code at the start position</span>
<span class="line-removed"> 420   // then return from native to simulated execution.</span>
<span class="line-removed"> 421 </span>
 422   address generate_catch_exception() {
 423     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;catch_exception&quot;);
 424     address start = __ pc();
 425 
 426     // same as in generate_call_stub():
 427     const Address sp_after_call(rfp, sp_after_call_off * wordSize);
 428     const Address thread        (rfp, thread_off         * wordSize);
 429 
 430 #ifdef ASSERT
 431     // verify that threads correspond
 432     {
 433       Label L, S;
 434       __ ldr(rscratch1, thread);
 435       __ cmp(rthread, rscratch1);
 436       __ br(Assembler::NE, S);
 437       __ get_thread(rscratch1);
 438       __ cmp(rthread, rscratch1);
 439       __ br(Assembler::EQ, L);
 440       __ bind(S);
 441       __ stop(&quot;StubRoutines::catch_exception: threads must correspond&quot;);
</pre>
<hr />
<pre>
 562   address generate_verify_oop() {
 563 
 564     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;verify_oop&quot;);
 565     address start = __ pc();
 566 
 567     Label exit, error;
 568 
 569     // save c_rarg2 and c_rarg3
 570     __ stp(c_rarg3, c_rarg2, Address(__ pre(sp, -16)));
 571 
 572     // __ incrementl(ExternalAddress((address) StubRoutines::verify_oop_count_addr()));
 573     __ lea(c_rarg2, ExternalAddress((address) StubRoutines::verify_oop_count_addr()));
 574     __ ldr(c_rarg3, Address(c_rarg2));
 575     __ add(c_rarg3, c_rarg3, 1);
 576     __ str(c_rarg3, Address(c_rarg2));
 577 
 578     // object is in r0
 579     // make sure object is &#39;reasonable&#39;
 580     __ cbz(r0, exit); // if obj is NULL it is OK
 581 










 582     // Check if the oop is in the right area of memory
 583     __ mov(c_rarg3, (intptr_t) Universe::verify_oop_mask());
 584     __ andr(c_rarg2, r0, c_rarg3);
 585     __ mov(c_rarg3, (intptr_t) Universe::verify_oop_bits());
 586 
 587     // Compare c_rarg2 and c_rarg3.  We don&#39;t use a compare
 588     // instruction here because the flags register is live.
 589     __ eor(c_rarg2, c_rarg2, c_rarg3);
 590     __ cbnz(c_rarg2, error);
 591 
 592     // make sure klass is &#39;reasonable&#39;, which is not zero.
 593     __ load_klass(r0, r0);  // get klass
 594     __ cbz(r0, error);      // if klass is NULL it is broken
 595 
 596     // return if everything seems ok
 597     __ bind(exit);
 598 
 599     __ ldp(c_rarg3, c_rarg2, Address(__ post(sp, 16)));
 600     __ ret(lr);
 601 
 602     // handle errors
 603     __ bind(error);
 604     __ ldp(c_rarg3, c_rarg2, Address(__ post(sp, 16)));
 605 
 606     __ push(RegSet::range(r0, r29), sp);
 607     // debug(char* msg, int64_t pc, int64_t regs[])
 608     __ mov(c_rarg0, rscratch1);      // pass address of error message
 609     __ mov(c_rarg1, lr);             // pass return address
 610     __ mov(c_rarg2, sp);             // pass address of regs on stack
 611 #ifndef PRODUCT
 612     assert(frame::arg_reg_save_area_bytes == 0, &quot;not expecting frame reg save area&quot;);
 613 #endif
 614     BLOCK_COMMENT(&quot;call MacroAssembler::debug&quot;);
 615     __ mov(rscratch1, CAST_FROM_FN_PTR(address, MacroAssembler::debug64));
<span class="line-modified"> 616     __ blrt(rscratch1, 3, 0, 1);</span>

 617 
 618     return start;
 619   }
 620 
 621   void array_overlap_test(Label&amp; L_no_overlap, Address::sxtw sf) { __ b(L_no_overlap); }
 622 
 623   // The inner part of zero_words().  This is the bulk operation,
 624   // zeroing words in blocks, possibly using DC ZVA to do it.  The
 625   // caller is responsible for zeroing the last few words.
 626   //
 627   // Inputs:
 628   // r10: the HeapWord-aligned base address of an array to zero.
 629   // r11: the count in HeapWords, r11 &gt; 0.
 630   //
 631   // Returns r10 and r11, adjusted for the caller to clear.
 632   // r10: the base address of the tail of words left to clear.
 633   // r11: the number of words in the tail.
 634   //      r11 &lt; MacroAssembler::zero_words_block_size.
 635 
 636   address generate_zero_blocks() {
</pre>
<hr />
<pre>
1346     __ align(CodeEntryAlignment);
1347     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1348     address start = __ pc();
1349     __ enter();
1350 
1351     if (entry != NULL) {
1352       *entry = __ pc();
1353       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1354       BLOCK_COMMENT(&quot;Entry:&quot;);
1355     }
1356 
1357     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;
1358     if (dest_uninitialized) {
1359       decorators |= IS_DEST_UNINITIALIZED;
1360     }
1361     if (aligned) {
1362       decorators |= ARRAYCOPY_ALIGNED;
1363     }
1364 
1365     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
<span class="line-modified">1366     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, d, count, saved_reg);</span>
1367 
1368     if (is_oop) {
1369       // save regs before copy_memory
1370       __ push(RegSet::of(d, count), sp);
1371     }
<span class="line-modified">1372     copy_memory(aligned, s, d, count, rscratch1, size);</span>





1373 
1374     if (is_oop) {
1375       __ pop(RegSet::of(d, count), sp);
1376       if (VerifyOops)
1377         verify_oop_array(size, d, count, r16);
<span class="line-removed">1378       __ sub(count, count, 1); // make an inclusive end pointer</span>
<span class="line-removed">1379       __ lea(count, Address(d, count, Address::lsl(exact_log2(size))));</span>
1380     }
1381 
1382     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, d, count, rscratch1, RegSet());
1383 
1384     __ leave();
1385     __ mov(r0, zr); // return 0
1386     __ ret(lr);
<span class="line-removed">1387 #ifdef BUILTIN_SIM</span>
<span class="line-removed">1388     {</span>
<span class="line-removed">1389       AArch64Simulator *sim = AArch64Simulator::get_current(UseSimulatorCache, DisableBCCheck);</span>
<span class="line-removed">1390       sim-&gt;notifyCompile(const_cast&lt;char*&gt;(name), start);</span>
<span class="line-removed">1391     }</span>
<span class="line-removed">1392 #endif</span>
1393     return start;
1394   }
1395 
1396   // Arguments:
1397   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1398   //             ignored
1399   //   is_oop  - true =&gt; oop array, so generate store check code
1400   //   name    - stub name string
1401   //
1402   // Inputs:
1403   //   c_rarg0   - source array address
1404   //   c_rarg1   - destination array address
1405   //   c_rarg2   - element count, treated as ssize_t, can be zero
1406   //
1407   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1408   // the hardware handle it.  The two dwords within qwords that span
1409   // cache line boundaries will still be loaded and stored atomicly.
1410   //
1411   address generate_conjoint_copy(size_t size, bool aligned, bool is_oop, address nooverlap_target,
1412                                  address *entry, const char *name,
</pre>
<hr />
<pre>
1420     if (entry != NULL) {
1421       *entry = __ pc();
1422       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1423       BLOCK_COMMENT(&quot;Entry:&quot;);
1424     }
1425 
1426     // use fwd copy when (d-s) above_equal (count*size)
1427     __ sub(rscratch1, d, s);
1428     __ cmp(rscratch1, count, Assembler::LSL, exact_log2(size));
1429     __ br(Assembler::HS, nooverlap_target);
1430 
1431     DecoratorSet decorators = IN_HEAP | IS_ARRAY;
1432     if (dest_uninitialized) {
1433       decorators |= IS_DEST_UNINITIALIZED;
1434     }
1435     if (aligned) {
1436       decorators |= ARRAYCOPY_ALIGNED;
1437     }
1438 
1439     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
<span class="line-modified">1440     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, d, count, saved_regs);</span>
1441 
1442     if (is_oop) {
1443       // save regs before copy_memory
1444       __ push(RegSet::of(d, count), sp);
1445     }
<span class="line-modified">1446     copy_memory(aligned, s, d, count, rscratch1, -size);</span>





1447     if (is_oop) {
1448       __ pop(RegSet::of(d, count), sp);
1449       if (VerifyOops)
1450         verify_oop_array(size, d, count, r16);
<span class="line-removed">1451       __ sub(count, count, 1); // make an inclusive end pointer</span>
<span class="line-removed">1452       __ lea(count, Address(d, count, Address::lsl(exact_log2(size))));</span>
1453     }
1454     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, d, count, rscratch1, RegSet());
1455     __ leave();
1456     __ mov(r0, zr); // return 0
1457     __ ret(lr);
<span class="line-removed">1458 #ifdef BUILTIN_SIM</span>
<span class="line-removed">1459     {</span>
<span class="line-removed">1460       AArch64Simulator *sim = AArch64Simulator::get_current(UseSimulatorCache, DisableBCCheck);</span>
<span class="line-removed">1461       sim-&gt;notifyCompile(const_cast&lt;char*&gt;(name), start);</span>
<span class="line-removed">1462     }</span>
<span class="line-removed">1463 #endif</span>
1464     return start;
1465 }
1466 
1467   // Arguments:
1468   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1469   //             ignored
1470   //   name    - stub name string
1471   //
1472   // Inputs:
1473   //   c_rarg0   - source array address
1474   //   c_rarg1   - destination array address
1475   //   c_rarg2   - element count, treated as ssize_t, can be zero
1476   //
1477   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1478   // we let the hardware handle it.  The one to eight bytes within words,
1479   // dwords or qwords that span cache line boundaries will still be loaded
1480   // and stored atomically.
1481   //
1482   // Side Effects:
1483   //   disjoint_byte_copy_entry is set to the no-overlap entry point  //
</pre>
<hr />
<pre>
1775 
1776      // Empty array:  Nothing to do.
1777     __ cbz(count, L_done);
1778 
1779     __ push(RegSet::of(r18, r19, r20, r21), sp);
1780 
1781 #ifdef ASSERT
1782     BLOCK_COMMENT(&quot;assert consistent ckoff/ckval&quot;);
1783     // The ckoff and ckval must be mutually consistent,
1784     // even though caller generates both.
1785     { Label L;
1786       int sco_offset = in_bytes(Klass::super_check_offset_offset());
1787       __ ldrw(start_to, Address(ckval, sco_offset));
1788       __ cmpw(ckoff, start_to);
1789       __ br(Assembler::EQ, L);
1790       __ stop(&quot;super_check_offset inconsistent&quot;);
1791       __ bind(L);
1792     }
1793 #endif //ASSERT
1794 
<span class="line-modified">1795     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST;</span>
1796     bool is_oop = true;
1797     if (dest_uninitialized) {
1798       decorators |= IS_DEST_UNINITIALIZED;
1799     }
1800 
1801     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
<span class="line-modified">1802     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, to, count, wb_pre_saved_regs);</span>
1803 
1804     // save the original count
1805     __ mov(count_save, count);
1806 
1807     // Copy from low to high addresses
1808     __ mov(start_to, to);              // Save destination array start address
1809     __ b(L_load_element);
1810 
1811     // ======== begin loop ========
1812     // (Loop is rotated; its entry is L_load_element.)
1813     // Loop control:
1814     //   for (; count != 0; count--) {
1815     //     copied_oop = load_heap_oop(from++);
1816     //     ... generate_type_check ...;
1817     //     store_heap_oop(to++, copied_oop);
1818     //   }
1819     __ align(OptoLoopAlignment);
1820 
1821     __ BIND(L_store_element);
1822     __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, AS_RAW);  // store the oop
</pre>
<hr />
<pre>
1825 
1826     // ======== loop entry is here ========
1827     __ BIND(L_load_element);
1828     __ load_heap_oop(copied_oop, __ post(from, UseCompressedOops ? 4 : 8), noreg, noreg, AS_RAW); // load the oop
1829     __ cbz(copied_oop, L_store_element);
1830 
1831     __ load_klass(r19_klass, copied_oop);// query the object klass
1832     generate_type_check(r19_klass, ckoff, ckval, L_store_element);
1833     // ======== end loop ========
1834 
1835     // It was a real error; we must depend on the caller to finish the job.
1836     // Register count = remaining oops, count_orig = total oops.
1837     // Emit GC store barriers for the oops we have copied and report
1838     // their number to the caller.
1839 
1840     __ subs(count, count_save, count);     // K = partially copied oop count
1841     __ eon(count, count, zr);                   // report (-1^K) to caller
1842     __ br(Assembler::EQ, L_done_pop);
1843 
1844     __ BIND(L_do_card_marks);
<span class="line-modified">1845     __ add(to, to, -heapOopSize);         // make an inclusive end pointer</span>
<span class="line-removed">1846     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, start_to, to, rscratch1, wb_post_saved_regs);</span>
1847 
1848     __ bind(L_done_pop);
1849     __ pop(RegSet::of(r18, r19, r20, r21), sp);
1850     inc_counter_np(SharedRuntime::_checkcast_array_copy_ctr);
1851 
1852     __ bind(L_done);
1853     __ mov(r0, count);
1854     __ leave();
1855     __ ret(lr);
1856 
1857     return start;
1858   }
1859 
1860   // Perform range checks on the proposed arraycopy.
1861   // Kills temp, but nothing else.
1862   // Also, clean the sign bits of src_pos and dst_pos.
1863   void arraycopy_range_checks(Register src,     // source array oop (c_rarg0)
1864                               Register src_pos, // source position (c_rarg1)
1865                               Register dst,     // destination array oo (c_rarg2)
1866                               Register dst_pos, // destination position (c_rarg3)
</pre>
<hr />
<pre>
2356         break;
2357       case T_SHORT:
2358         __ tbz(count, 0, L_fill_4);
2359         __ strh(value, Address(__ post(to, 2)));
2360         __ bind(L_fill_4);
2361         __ tbz(count, 1, L_exit2);
2362         __ strw(value, Address(to));
2363         break;
2364       case T_INT:
2365         __ cbzw(count, L_exit2);
2366         __ strw(value, Address(to));
2367         break;
2368       default: ShouldNotReachHere();
2369     }
2370     __ bind(L_exit2);
2371     __ leave();
2372     __ ret(lr);
2373     return start;
2374   }
2375 






































2376   void generate_arraycopy_stubs() {
2377     address entry;
2378     address entry_jbyte_arraycopy;
2379     address entry_jshort_arraycopy;
2380     address entry_jint_arraycopy;
2381     address entry_oop_arraycopy;
2382     address entry_jlong_arraycopy;
2383     address entry_checkcast_arraycopy;
2384 
2385     generate_copy_longs(copy_f, r0, r1, rscratch2, copy_forwards);
2386     generate_copy_longs(copy_b, r0, r1, rscratch2, copy_backwards);
2387 
2388     StubRoutines::aarch64::_zero_blocks = generate_zero_blocks();
2389 
2390     //*** jbyte
2391     // Always need aligned and unaligned versions
2392     StubRoutines::_jbyte_disjoint_arraycopy         = generate_disjoint_byte_copy(false, &amp;entry,
2393                                                                                   &quot;jbyte_disjoint_arraycopy&quot;);
2394     StubRoutines::_jbyte_arraycopy                  = generate_conjoint_byte_copy(false, entry,
2395                                                                                   &amp;entry_jbyte_arraycopy,
</pre>
<hr />
<pre>
3092     __ addv(v0, __ T4S, v0, v2);
3093     __ addv(v1, __ T4S, v1, v3);
3094 
3095     if (multi_block) {
3096       __ add(ofs, ofs, 64);
3097       __ cmp(ofs, limit);
3098       __ br(Assembler::LE, sha1_loop);
3099       __ mov(c_rarg0, ofs); // return ofs
3100     }
3101 
3102     __ ldpd(v10, v11, Address(sp, 16));
3103     __ ldpd(v8, v9, __ post(sp, 32));
3104 
3105     __ stpq(v0, v1, state);
3106 
3107     __ ret(lr);
3108 
3109     return start;
3110   }
3111 
<span class="line-removed">3112 #ifndef BUILTIN_SIM</span>
3113   // Safefetch stubs.
3114   void generate_safefetch(const char* name, int size, address* entry,
3115                           address* fault_pc, address* continuation_pc) {
3116     // safefetch signatures:
3117     //   int      SafeFetch32(int*      adr, int      errValue);
3118     //   intptr_t SafeFetchN (intptr_t* adr, intptr_t errValue);
3119     //
3120     // arguments:
3121     //   c_rarg0 = adr
3122     //   c_rarg1 = errValue
3123     //
3124     // result:
3125     //   PPC_RET  = *adr or errValue
3126 
3127     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3128 
3129     // Entry point, pc or function descriptor.
3130     *entry = __ pc();
3131 
3132     // Load *adr into c_rarg1, may fault.
3133     *fault_pc = __ pc();
3134     switch (size) {
3135       case 4:
3136         // int32_t
3137         __ ldrw(c_rarg1, Address(c_rarg0, 0));
3138         break;
3139       case 8:
3140         // int64_t
3141         __ ldr(c_rarg1, Address(c_rarg0, 0));
3142         break;
3143       default:
3144         ShouldNotReachHere();
3145     }
3146 
3147     // return errValue or *adr
3148     *continuation_pc = __ pc();
3149     __ mov(r0, c_rarg1);
3150     __ ret(lr);
3151   }
<span class="line-removed">3152 #endif</span>
3153 
3154   /**
3155    *  Arguments:
3156    *
3157    * Inputs:
3158    *   c_rarg0   - int crc
3159    *   c_rarg1   - byte* buf
3160    *   c_rarg2   - int length
3161    *
3162    * Ouput:
3163    *       rax   - int crc result
3164    */
3165   address generate_updateBytesCRC32() {
3166     assert(UseCRC32Intrinsics, &quot;what are we doing here?&quot;);
3167 
3168     __ align(CodeEntryAlignment);
3169     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesCRC32&quot;);
3170 
3171     address start = __ pc();
3172 
</pre>
<hr />
<pre>
3979         tmpC4, tmp1, tmp2, tmp3, tmp4, tmp5);
3980     return entry;
3981   }
3982 
3983   // code for comparing 16 bytes of strings with same encoding
3984   void compare_string_16_bytes_same(Label &amp;DIFF1, Label &amp;DIFF2) {
3985     Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, tmp1 = r10, tmp2 = r11;
3986     __ ldr(rscratch1, Address(__ post(str1, 8)));
3987     __ eor(rscratch2, tmp1, tmp2);
3988     __ ldr(cnt1, Address(__ post(str2, 8)));
3989     __ cbnz(rscratch2, DIFF1);
3990     __ ldr(tmp1, Address(__ post(str1, 8)));
3991     __ eor(rscratch2, rscratch1, cnt1);
3992     __ ldr(tmp2, Address(__ post(str2, 8)));
3993     __ cbnz(rscratch2, DIFF2);
3994   }
3995 
3996   // code for comparing 16 characters of strings with Latin1 and Utf16 encoding
3997   void compare_string_16_x_LU(Register tmpL, Register tmpU, Label &amp;DIFF1,
3998       Label &amp;DIFF2) {
<span class="line-modified">3999     Register cnt1 = r2, tmp1 = r10, tmp2 = r11, tmp3 = r12;</span>
4000     FloatRegister vtmp = v1, vtmpZ = v0, vtmp3 = v2;
4001 
4002     __ ldrq(vtmp, Address(__ post(tmp2, 16)));
4003     __ ldr(tmpU, Address(__ post(cnt1, 8)));
4004     __ zip1(vtmp3, __ T16B, vtmp, vtmpZ);
4005     // now we have 32 bytes of characters (converted to U) in vtmp:vtmp3
4006 
4007     __ fmovd(tmpL, vtmp3);
4008     __ eor(rscratch2, tmp3, tmpL);
4009     __ cbnz(rscratch2, DIFF2);
4010 
4011     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4012     __ umov(tmpL, vtmp3, __ D, 1);
4013     __ eor(rscratch2, tmpU, tmpL);
4014     __ cbnz(rscratch2, DIFF1);
4015 
4016     __ zip2(vtmp, __ T16B, vtmp, vtmpZ);
4017     __ ldr(tmpU, Address(__ post(cnt1, 8)));
4018     __ fmovd(tmpL, vtmp);
4019     __ eor(rscratch2, tmp3, tmpL);
</pre>
<hr />
<pre>
4022     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4023     __ umov(tmpL, vtmp, __ D, 1);
4024     __ eor(rscratch2, tmpU, tmpL);
4025     __ cbnz(rscratch2, DIFF1);
4026   }
4027 
4028   // r0  = result
4029   // r1  = str1
4030   // r2  = cnt1
4031   // r3  = str2
4032   // r4  = cnt2
4033   // r10 = tmp1
4034   // r11 = tmp2
4035   address generate_compare_long_string_different_encoding(bool isLU) {
4036     __ align(CodeEntryAlignment);
4037     StubCodeMark mark(this, &quot;StubRoutines&quot;, isLU
4038         ? &quot;compare_long_string_different_encoding LU&quot;
4039         : &quot;compare_long_string_different_encoding UL&quot;);
4040     address entry = __ pc();
4041     Label SMALL_LOOP, TAIL, TAIL_LOAD_16, LOAD_LAST, DIFF1, DIFF2,
<span class="line-modified">4042         DONE, CALCULATE_DIFFERENCE, LARGE_LOOP_PREFETCH, SMALL_LOOP_ENTER,</span>
4043         LARGE_LOOP_PREFETCH_REPEAT1, LARGE_LOOP_PREFETCH_REPEAT2;
4044     Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, cnt2 = r4,
4045         tmp1 = r10, tmp2 = r11, tmp3 = r12, tmp4 = r14;
4046     FloatRegister vtmpZ = v0, vtmp = v1, vtmp3 = v2;
4047     RegSet spilled_regs = RegSet::of(tmp3, tmp4);
4048 
<span class="line-modified">4049     int prefetchLoopExitCondition = MAX(32, SoftwarePrefetchHintDistance/2);</span>
4050 
4051     __ eor(vtmpZ, __ T16B, vtmpZ, vtmpZ);
4052     // cnt2 == amount of characters left to compare
4053     // Check already loaded first 4 symbols(vtmp and tmp2(LU)/tmp1(UL))
4054     __ zip1(vtmp, __ T8B, vtmp, vtmpZ);
4055     __ add(str1, str1, isLU ? wordSize/2 : wordSize);
4056     __ add(str2, str2, isLU ? wordSize : wordSize/2);
4057     __ fmovd(isLU ? tmp1 : tmp2, vtmp);
4058     __ subw(cnt2, cnt2, 8); // Already loaded 4 symbols. Last 4 is special case.
<span class="line-removed">4059     __ add(str1, str1, cnt2, __ LSL, isLU ? 0 : 1);</span>
4060     __ eor(rscratch2, tmp1, tmp2);
<span class="line-removed">4061     __ add(str2, str2, cnt2, __ LSL, isLU ? 1 : 0);</span>
4062     __ mov(rscratch1, tmp2);
4063     __ cbnz(rscratch2, CALCULATE_DIFFERENCE);
<span class="line-modified">4064     Register strU = isLU ? str2 : str1,</span>
<span class="line-removed">4065              strL = isLU ? str1 : str2,</span>
<span class="line-removed">4066              tmpU = isLU ? rscratch1 : tmp1, // where to keep U for comparison</span>
4067              tmpL = isLU ? tmp1 : rscratch1; // where to keep L for comparison
4068     __ push(spilled_regs, sp);
<span class="line-modified">4069     __ sub(tmp2, strL, cnt2); // strL pointer to load from</span>
<span class="line-modified">4070     __ sub(cnt1, strU, cnt2, __ LSL, 1); // strU pointer to load from</span>
4071 
4072     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4073 
4074     if (SoftwarePrefetchHintDistance &gt;= 0) {
4075       __ subs(rscratch2, cnt2, prefetchLoopExitCondition);
<span class="line-modified">4076       __ br(__ LT, SMALL_LOOP);</span>
4077       __ bind(LARGE_LOOP_PREFETCH);
4078         __ prfm(Address(tmp2, SoftwarePrefetchHintDistance));
4079         __ mov(tmp4, 2);
4080         __ prfm(Address(cnt1, SoftwarePrefetchHintDistance));
4081         __ bind(LARGE_LOOP_PREFETCH_REPEAT1);
4082           compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4083           __ subs(tmp4, tmp4, 1);
4084           __ br(__ GT, LARGE_LOOP_PREFETCH_REPEAT1);
4085           __ prfm(Address(cnt1, SoftwarePrefetchHintDistance));
4086           __ mov(tmp4, 2);
4087         __ bind(LARGE_LOOP_PREFETCH_REPEAT2);
4088           compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4089           __ subs(tmp4, tmp4, 1);
4090           __ br(__ GT, LARGE_LOOP_PREFETCH_REPEAT2);
4091           __ sub(cnt2, cnt2, 64);
4092           __ subs(rscratch2, cnt2, prefetchLoopExitCondition);
4093           __ br(__ GE, LARGE_LOOP_PREFETCH);
4094     }
4095     __ cbz(cnt2, LOAD_LAST); // no characters left except last load

4096     __ subs(cnt2, cnt2, 16);
4097     __ br(__ LT, TAIL);
<span class="line-modified">4098     __ b(SMALL_LOOP_ENTER);</span>
4099     __ bind(SMALL_LOOP); // smaller loop
4100       __ subs(cnt2, cnt2, 16);
<span class="line-removed">4101     __ bind(SMALL_LOOP_ENTER);</span>
4102       compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4103       __ br(__ GE, SMALL_LOOP);
<span class="line-modified">4104       __ cbz(cnt2, LOAD_LAST);</span>
<span class="line-modified">4105     __ bind(TAIL); // 1..15 characters left</span>
<span class="line-modified">4106       __ subs(zr, cnt2, -8);</span>
<span class="line-modified">4107       __ br(__ GT, TAIL_LOAD_16);</span>
<span class="line-modified">4108       __ ldrd(vtmp, Address(tmp2));</span>
<span class="line-modified">4109       __ zip1(vtmp3, __ T8B, vtmp, vtmpZ);</span>
<span class="line-modified">4110 </span>
<span class="line-removed">4111       __ ldr(tmpU, Address(__ post(cnt1, 8)));</span>
<span class="line-removed">4112       __ fmovd(tmpL, vtmp3);</span>
<span class="line-removed">4113       __ eor(rscratch2, tmp3, tmpL);</span>
<span class="line-removed">4114       __ cbnz(rscratch2, DIFF2);</span>
<span class="line-removed">4115       __ umov(tmpL, vtmp3, __ D, 1);</span>
<span class="line-removed">4116       __ eor(rscratch2, tmpU, tmpL);</span>
<span class="line-removed">4117       __ cbnz(rscratch2, DIFF1);</span>
<span class="line-removed">4118       __ b(LOAD_LAST);</span>
<span class="line-removed">4119     __ bind(TAIL_LOAD_16);</span>
<span class="line-removed">4120       __ ldrq(vtmp, Address(tmp2));</span>
<span class="line-removed">4121       __ ldr(tmpU, Address(__ post(cnt1, 8)));</span>
<span class="line-removed">4122       __ zip1(vtmp3, __ T16B, vtmp, vtmpZ);</span>
<span class="line-removed">4123       __ zip2(vtmp, __ T16B, vtmp, vtmpZ);</span>
<span class="line-removed">4124       __ fmovd(tmpL, vtmp3);</span>
<span class="line-removed">4125       __ eor(rscratch2, tmp3, tmpL);</span>
<span class="line-removed">4126       __ cbnz(rscratch2, DIFF2);</span>
<span class="line-removed">4127 </span>
<span class="line-removed">4128       __ ldr(tmp3, Address(__ post(cnt1, 8)));</span>
<span class="line-removed">4129       __ umov(tmpL, vtmp3, __ D, 1);</span>
<span class="line-removed">4130       __ eor(rscratch2, tmpU, tmpL);</span>
<span class="line-removed">4131       __ cbnz(rscratch2, DIFF1);</span>
<span class="line-removed">4132 </span>
<span class="line-removed">4133       __ ldr(tmpU, Address(__ post(cnt1, 8)));</span>
<span class="line-removed">4134       __ fmovd(tmpL, vtmp);</span>
<span class="line-removed">4135       __ eor(rscratch2, tmp3, tmpL);</span>
<span class="line-removed">4136       __ cbnz(rscratch2, DIFF2);</span>
<span class="line-removed">4137 </span>
<span class="line-removed">4138       __ umov(tmpL, vtmp, __ D, 1);</span>
<span class="line-removed">4139       __ eor(rscratch2, tmpU, tmpL);</span>
<span class="line-removed">4140       __ cbnz(rscratch2, DIFF1);</span>
4141       __ b(LOAD_LAST);
4142     __ bind(DIFF2);
4143       __ mov(tmpU, tmp3);
4144     __ bind(DIFF1);
4145       __ pop(spilled_regs, sp);
4146       __ b(CALCULATE_DIFFERENCE);
4147     __ bind(LOAD_LAST);



4148       __ pop(spilled_regs, sp);
4149 
<span class="line-modified">4150       __ ldrs(vtmp, Address(strL));</span>
<span class="line-modified">4151       __ ldr(tmpU, Address(strU));</span>
4152       __ zip1(vtmp, __ T8B, vtmp, vtmpZ);
4153       __ fmovd(tmpL, vtmp);
4154 
4155       __ eor(rscratch2, tmpU, tmpL);
4156       __ cbz(rscratch2, DONE);
4157 
4158     // Find the first different characters in the longwords and
4159     // compute their difference.
4160     __ bind(CALCULATE_DIFFERENCE);
4161       __ rev(rscratch2, rscratch2);
4162       __ clz(rscratch2, rscratch2);
4163       __ andr(rscratch2, rscratch2, -16);
4164       __ lsrv(tmp1, tmp1, rscratch2);
4165       __ uxthw(tmp1, tmp1);
4166       __ lsrv(rscratch1, rscratch1, rscratch2);
4167       __ uxthw(rscratch1, rscratch1);
4168       __ subw(result, tmp1, rscratch1);
4169     __ bind(DONE);
4170       __ ret(lr);
4171     return entry;
</pre>
<hr />
<pre>
4193     // to prefetch memory behind array border
4194     int largeLoopExitCondition = MAX(64, SoftwarePrefetchHintDistance)/(isLL ? 1 : 2);
4195     // cnt1/cnt2 contains amount of characters to compare. cnt1 can be re-used
4196     // update cnt2 counter with already loaded 8 bytes
4197     __ sub(cnt2, cnt2, wordSize/(isLL ? 1 : 2));
4198     // update pointers, because of previous read
4199     __ add(str1, str1, wordSize);
4200     __ add(str2, str2, wordSize);
4201     if (SoftwarePrefetchHintDistance &gt;= 0) {
4202       __ bind(LARGE_LOOP_PREFETCH);
4203         __ prfm(Address(str1, SoftwarePrefetchHintDistance));
4204         __ prfm(Address(str2, SoftwarePrefetchHintDistance));
4205         compare_string_16_bytes_same(DIFF, DIFF2);
4206         compare_string_16_bytes_same(DIFF, DIFF2);
4207         __ sub(cnt2, cnt2, isLL ? 64 : 32);
4208         compare_string_16_bytes_same(DIFF, DIFF2);
4209         __ subs(rscratch2, cnt2, largeLoopExitCondition);
4210         compare_string_16_bytes_same(DIFF, DIFF2);
4211         __ br(__ GT, LARGE_LOOP_PREFETCH);
4212         __ cbz(cnt2, LAST_CHECK_AND_LENGTH_DIFF); // no more chars left?
<span class="line-removed">4213         // less than 16 bytes left?</span>
<span class="line-removed">4214         __ subs(cnt2, cnt2, isLL ? 16 : 8);</span>
<span class="line-removed">4215         __ br(__ LT, TAIL);</span>
4216     }




4217     __ bind(SMALL_LOOP);
4218       compare_string_16_bytes_same(DIFF, DIFF2);
4219       __ subs(cnt2, cnt2, isLL ? 16 : 8);
4220       __ br(__ GE, SMALL_LOOP);
4221     __ bind(TAIL);
4222       __ adds(cnt2, cnt2, isLL ? 16 : 8);
4223       __ br(__ EQ, LAST_CHECK_AND_LENGTH_DIFF);
4224       __ subs(cnt2, cnt2, isLL ? 8 : 4);
4225       __ br(__ LE, CHECK_LAST);
4226       __ eor(rscratch2, tmp1, tmp2);
4227       __ cbnz(rscratch2, DIFF);
4228       __ ldr(tmp1, Address(__ post(str1, 8)));
4229       __ ldr(tmp2, Address(__ post(str2, 8)));
4230       __ sub(cnt2, cnt2, isLL ? 8 : 4);
4231     __ bind(CHECK_LAST);
4232       if (!isLL) {
4233         __ add(cnt2, cnt2, cnt2); // now in bytes
4234       }
4235       __ eor(rscratch2, tmp1, tmp2);
4236       __ cbnz(rscratch2, DIFF);
</pre>
<hr />
<pre>
4325     bool isL = str1_isL &amp;&amp; str2_isL;
4326    // parameters
4327     Register result = r0, str2 = r1, cnt1 = r2, str1 = r3, cnt2 = r4;
4328     // temporary registers
4329     Register tmp1 = r20, tmp2 = r21, tmp3 = r22, tmp4 = r23;
4330     RegSet spilled_regs = RegSet::range(tmp1, tmp4);
4331     // redefinitions
4332     Register ch1 = rscratch1, ch2 = rscratch2, first = tmp3;
4333 
4334     __ push(spilled_regs, sp);
4335     Label L_LOOP, L_LOOP_PROCEED, L_SMALL, L_HAS_ZERO,
4336         L_HAS_ZERO_LOOP, L_CMP_LOOP, L_CMP_LOOP_NOMATCH, L_SMALL_PROCEED,
4337         L_SMALL_HAS_ZERO_LOOP, L_SMALL_CMP_LOOP_NOMATCH, L_SMALL_CMP_LOOP,
4338         L_POST_LOOP, L_CMP_LOOP_LAST_CMP, L_HAS_ZERO_LOOP_NOMATCH,
4339         L_SMALL_CMP_LOOP_LAST_CMP, L_SMALL_CMP_LOOP_LAST_CMP2,
4340         L_CMP_LOOP_LAST_CMP2, DONE, NOMATCH;
4341     // Read whole register from str1. It is safe, because length &gt;=8 here
4342     __ ldr(ch1, Address(str1));
4343     // Read whole register from str2. It is safe, because length &gt;=8 here
4344     __ ldr(ch2, Address(str2));

4345     __ andr(first, ch1, str1_isL ? 0xFF : 0xFFFF);
4346     if (str1_isL != str2_isL) {
4347       __ eor(v0, __ T16B, v0, v0);
4348     }
4349     __ mov(tmp1, str2_isL ? 0x0101010101010101 : 0x0001000100010001);
4350     __ mul(first, first, tmp1);
4351     // check if we have less than 1 register to check
4352     __ subs(cnt2, cnt2, wordSize/str2_chr_size - 1);
4353     if (str1_isL != str2_isL) {
4354       __ fmovd(v1, ch1);
4355     }
4356     __ br(__ LE, L_SMALL);
4357     __ eor(ch2, first, ch2);
4358     if (str1_isL != str2_isL) {
4359       __ zip1(v1, __ T16B, v1, v0);
4360     }
4361     __ sub(tmp2, ch2, tmp1);
4362     __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4363     __ bics(tmp2, tmp2, ch2);
4364     if (str1_isL != str2_isL) {
</pre>
<hr />
<pre>
4796     // lr and fp are already in place
4797     __ sub(sp, rfp, ((unsigned)framesize-4) &lt;&lt; LogBytesPerInt); // prolog
4798 
4799     int frame_complete = __ pc() - start;
4800 
4801     // Set up last_Java_sp and last_Java_fp
4802     address the_pc = __ pc();
4803     __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);
4804 
4805     // Call runtime
4806     if (arg1 != noreg) {
4807       assert(arg2 != c_rarg1, &quot;clobbered&quot;);
4808       __ mov(c_rarg1, arg1);
4809     }
4810     if (arg2 != noreg) {
4811       __ mov(c_rarg2, arg2);
4812     }
4813     __ mov(c_rarg0, rthread);
4814     BLOCK_COMMENT(&quot;call runtime_entry&quot;);
4815     __ mov(rscratch1, runtime_entry);
<span class="line-modified">4816     __ blrt(rscratch1, 3 /* number_of_arguments */, 0, 1);</span>
4817 
4818     // Generate oop map
4819     OopMap* map = new OopMap(framesize, 0);
4820 
4821     oop_maps-&gt;add_gc_map(the_pc - start, map);
4822 
4823     __ reset_last_Java_frame(true);
4824     __ maybe_isb();
4825 
4826     __ leave();
4827 
4828     // check for pending exceptions
4829 #ifdef ASSERT
4830     Label L;
4831     __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));
4832     __ cbnz(rscratch1, L);
4833     __ should_not_reach_here();
4834     __ bind(L);
4835 #endif // ASSERT
4836     __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
</pre>
<hr />
<pre>
5770 
5771     if (UseMulAddIntrinsic) {
5772       StubRoutines::_mulAdd = generate_mulAdd();
5773     }
5774 
5775     if (UseMontgomeryMultiplyIntrinsic) {
5776       StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;montgomeryMultiply&quot;);
5777       MontgomeryMultiplyGenerator g(_masm, /*squaring*/false);
5778       StubRoutines::_montgomeryMultiply = g.generate_multiply();
5779     }
5780 
5781     if (UseMontgomerySquareIntrinsic) {
5782       StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;montgomerySquare&quot;);
5783       MontgomeryMultiplyGenerator g(_masm, /*squaring*/true);
5784       // We use generate_multiply() rather than generate_square()
5785       // because it&#39;s faster for the sizes of modulus we care about.
5786       StubRoutines::_montgomerySquare = g.generate_multiply();
5787     }
5788 #endif // COMPILER2
5789 
<span class="line-removed">5790 #ifndef BUILTIN_SIM</span>
5791     // generate GHASH intrinsics code
5792     if (UseGHASHIntrinsics) {
5793       StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();
5794     }
5795 




5796     if (UseAESIntrinsics) {
5797       StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();
5798       StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();
5799       StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();
5800       StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt();
5801     }
5802 
5803     if (UseSHA1Intrinsics) {
5804       StubRoutines::_sha1_implCompress     = generate_sha1_implCompress(false,   &quot;sha1_implCompress&quot;);
5805       StubRoutines::_sha1_implCompressMB   = generate_sha1_implCompress(true,    &quot;sha1_implCompressMB&quot;);
5806     }
5807     if (UseSHA256Intrinsics) {
5808       StubRoutines::_sha256_implCompress   = generate_sha256_implCompress(false, &quot;sha256_implCompress&quot;);
5809       StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true,  &quot;sha256_implCompressMB&quot;);
5810     }
5811 
5812     // generate Adler32 intrinsics code
5813     if (UseAdler32Intrinsics) {
5814       StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();
5815     }
5816 
5817     // Safefetch stubs.
5818     generate_safefetch(&quot;SafeFetch32&quot;, sizeof(int),     &amp;StubRoutines::_safefetch32_entry,
5819                                                        &amp;StubRoutines::_safefetch32_fault_pc,
5820                                                        &amp;StubRoutines::_safefetch32_continuation_pc);
5821     generate_safefetch(&quot;SafeFetchN&quot;, sizeof(intptr_t), &amp;StubRoutines::_safefetchN_entry,
5822                                                        &amp;StubRoutines::_safefetchN_fault_pc,
5823                                                        &amp;StubRoutines::_safefetchN_continuation_pc);
<span class="line-removed">5824 #endif</span>
5825     StubRoutines::aarch64::set_completed();
5826   }
5827 
5828  public:
5829   StubGenerator(CodeBuffer* code, bool all) : StubCodeGenerator(code) {
5830     if (all) {
5831       generate_all();
5832     } else {
5833       generate_initial();
5834     }
5835   }
5836 }; // end class declaration
5837 

5838 void StubGenerator_generate(CodeBuffer* code, bool all) {



5839   StubGenerator g(code, all);
5840 }
</pre>
</td>
<td>
<hr />
<pre>
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.hpp&quot;
  28 #include &quot;asm/macroAssembler.inline.hpp&quot;
  29 #include &quot;gc/shared/barrierSet.hpp&quot;
  30 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  31 #include &quot;interpreter/interpreter.hpp&quot;
<span class="line-added">  32 #include &quot;memory/universe.hpp&quot;</span>
  33 #include &quot;nativeInst_aarch64.hpp&quot;
  34 #include &quot;oops/instanceOop.hpp&quot;
  35 #include &quot;oops/method.hpp&quot;
  36 #include &quot;oops/objArrayKlass.hpp&quot;
  37 #include &quot;oops/oop.inline.hpp&quot;
  38 #include &quot;prims/methodHandles.hpp&quot;
  39 #include &quot;runtime/frame.inline.hpp&quot;
  40 #include &quot;runtime/handles.inline.hpp&quot;
  41 #include &quot;runtime/sharedRuntime.hpp&quot;
  42 #include &quot;runtime/stubCodeGenerator.hpp&quot;
  43 #include &quot;runtime/stubRoutines.hpp&quot;
  44 #include &quot;runtime/thread.inline.hpp&quot;
  45 #include &quot;utilities/align.hpp&quot;
<span class="line-added">  46 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  47 #ifdef COMPILER2
  48 #include &quot;opto/runtime.hpp&quot;
  49 #endif
<span class="line-modified">  50 #if INCLUDE_ZGC</span>
<span class="line-modified">  51 #include &quot;gc/z/zThreadLocalData.hpp&quot;</span>

  52 #endif
  53 
  54 // Declaration and definition of StubGenerator (no .hpp file).
  55 // For a more detailed description of the stub routine structure
  56 // see the comment in stubRoutines.hpp
  57 
  58 #undef __
  59 #define __ _masm-&gt;
  60 #define TIMES_OOP Address::sxtw(exact_log2(UseCompressedOops ? 4 : 8))
  61 
  62 #ifdef PRODUCT
  63 #define BLOCK_COMMENT(str) /* nothing */
  64 #else
  65 #define BLOCK_COMMENT(str) __ block_comment(str)
  66 #endif
  67 
  68 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  69 
  70 // Stub Code definitions
  71 
</pre>
<hr />
<pre>
 201     const Address result_type   (rfp, result_type_off    * wordSize);
 202     const Address method        (rfp, method_off         * wordSize);
 203     const Address entry_point   (rfp, entry_point_off    * wordSize);
 204     const Address parameter_size(rfp, parameter_size_off * wordSize);
 205 
 206     const Address thread        (rfp, thread_off         * wordSize);
 207 
 208     const Address d15_save      (rfp, d15_off * wordSize);
 209     const Address d13_save      (rfp, d13_off * wordSize);
 210     const Address d11_save      (rfp, d11_off * wordSize);
 211     const Address d9_save       (rfp, d9_off * wordSize);
 212 
 213     const Address r28_save      (rfp, r28_off * wordSize);
 214     const Address r26_save      (rfp, r26_off * wordSize);
 215     const Address r24_save      (rfp, r24_off * wordSize);
 216     const Address r22_save      (rfp, r22_off * wordSize);
 217     const Address r20_save      (rfp, r20_off * wordSize);
 218 
 219     // stub code
 220 



 221     address aarch64_entry = __ pc();
 222 





 223     // set up frame and move sp to end of save area
 224     __ enter();
 225     __ sub(sp, rfp, -sp_after_call_off * wordSize);
 226 
 227     // save register parameters and Java scratch/global registers
 228     // n.b. we save thread even though it gets installed in
 229     // rthread because we want to sanity check rthread later
 230     __ str(c_rarg7,  thread);
 231     __ strw(c_rarg6, parameter_size);
 232     __ stp(c_rarg4, c_rarg5,  entry_point);
 233     __ stp(c_rarg2, c_rarg3,  result_type);
 234     __ stp(c_rarg0, c_rarg1,  call_wrapper);
 235 
 236     __ stp(r20, r19,   r20_save);
 237     __ stp(r22, r21,   r22_save);
 238     __ stp(r24, r23,   r24_save);
 239     __ stp(r26, r25,   r26_save);
 240     __ stp(r28, r27,   r28_save);
 241 
 242     __ stpd(v9,  v8,   d9_save);
</pre>
<hr />
<pre>
 273     Label parameters_done;
 274     // parameter count is still in c_rarg6
 275     // and parameter pointer identifying param 1 is in c_rarg5
 276     __ cbzw(c_rarg6, parameters_done);
 277 
 278     address loop = __ pc();
 279     __ ldr(rscratch1, Address(__ post(c_rarg5, wordSize)));
 280     __ subsw(c_rarg6, c_rarg6, 1);
 281     __ push(rscratch1);
 282     __ br(Assembler::GT, loop);
 283 
 284     __ BIND(parameters_done);
 285 
 286     // call Java entry -- passing methdoOop, and current sp
 287     //      rmethod: Method*
 288     //      r13: sender sp
 289     BLOCK_COMMENT(&quot;call Java function&quot;);
 290     __ mov(r13, sp);
 291     __ blr(c_rarg4);
 292 


 293     // we do this here because the notify will already have been done
 294     // if we get to the next instruction via an exception
 295     //
 296     // n.b. adding this instruction here affects the calculation of
 297     // whether or not a routine returns to the call stub (used when
 298     // doing stack walks) since the normal test is to check the return
 299     // pc against the address saved below. so we may need to allow for
 300     // this extra instruction in the check.
 301 



 302     // save current address for use by exception handling code
 303 
 304     return_address = __ pc();
 305 
 306     // store result depending on type (everything that is not
 307     // T_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)
 308     // n.b. this assumes Java returns an integral result in r0
 309     // and a floating result in j_farg0
 310     __ ldr(j_rarg2, result);
 311     Label is_long, is_float, is_double, exit;
 312     __ ldr(j_rarg1, result_type);
 313     __ cmp(j_rarg1, (u1)T_OBJECT);
 314     __ br(Assembler::EQ, is_long);
 315     __ cmp(j_rarg1, (u1)T_LONG);
 316     __ br(Assembler::EQ, is_long);
 317     __ cmp(j_rarg1, (u1)T_FLOAT);
 318     __ br(Assembler::EQ, is_float);
 319     __ cmp(j_rarg1, (u1)T_DOUBLE);
 320     __ br(Assembler::EQ, is_double);
 321 
</pre>
<hr />
<pre>
 344 #endif
 345 
 346     // restore callee-save registers
 347     __ ldpd(v15, v14,  d15_save);
 348     __ ldpd(v13, v12,  d13_save);
 349     __ ldpd(v11, v10,  d11_save);
 350     __ ldpd(v9,  v8,   d9_save);
 351 
 352     __ ldp(r28, r27,   r28_save);
 353     __ ldp(r26, r25,   r26_save);
 354     __ ldp(r24, r23,   r24_save);
 355     __ ldp(r22, r21,   r22_save);
 356     __ ldp(r20, r19,   r20_save);
 357 
 358     __ ldp(c_rarg0, c_rarg1,  call_wrapper);
 359     __ ldrw(c_rarg2, result_type);
 360     __ ldr(c_rarg3,  method);
 361     __ ldp(c_rarg4, c_rarg5,  entry_point);
 362     __ ldp(c_rarg6, c_rarg7,  parameter_size);
 363 






 364     // leave frame and return to caller
 365     __ leave();
 366     __ ret(lr);
 367 
 368     // handle return types different from T_INT
 369 
 370     __ BIND(is_long);
 371     __ str(r0, Address(j_rarg2, 0));
 372     __ br(Assembler::AL, exit);
 373 
 374     __ BIND(is_float);
 375     __ strs(j_farg0, Address(j_rarg2, 0));
 376     __ br(Assembler::AL, exit);
 377 
 378     __ BIND(is_double);
 379     __ strd(j_farg0, Address(j_rarg2, 0));
 380     __ br(Assembler::AL, exit);
 381 
 382     return start;
 383   }
 384 
 385   // Return point for a Java call if there&#39;s an exception thrown in
 386   // Java code.  The exception is caught and transformed into a
 387   // pending exception stored in JavaThread that can be tested from
 388   // within the VM.
 389   //
 390   // Note: Usually the parameters are removed by the callee. In case
 391   // of an exception crossing an activation frame boundary, that is
 392   // not the case if the callee is compiled code =&gt; need to setup the
 393   // rsp.
 394   //
 395   // r0: exception oop
 396 







 397   address generate_catch_exception() {
 398     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;catch_exception&quot;);
 399     address start = __ pc();
 400 
 401     // same as in generate_call_stub():
 402     const Address sp_after_call(rfp, sp_after_call_off * wordSize);
 403     const Address thread        (rfp, thread_off         * wordSize);
 404 
 405 #ifdef ASSERT
 406     // verify that threads correspond
 407     {
 408       Label L, S;
 409       __ ldr(rscratch1, thread);
 410       __ cmp(rthread, rscratch1);
 411       __ br(Assembler::NE, S);
 412       __ get_thread(rscratch1);
 413       __ cmp(rthread, rscratch1);
 414       __ br(Assembler::EQ, L);
 415       __ bind(S);
 416       __ stop(&quot;StubRoutines::catch_exception: threads must correspond&quot;);
</pre>
<hr />
<pre>
 537   address generate_verify_oop() {
 538 
 539     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;verify_oop&quot;);
 540     address start = __ pc();
 541 
 542     Label exit, error;
 543 
 544     // save c_rarg2 and c_rarg3
 545     __ stp(c_rarg3, c_rarg2, Address(__ pre(sp, -16)));
 546 
 547     // __ incrementl(ExternalAddress((address) StubRoutines::verify_oop_count_addr()));
 548     __ lea(c_rarg2, ExternalAddress((address) StubRoutines::verify_oop_count_addr()));
 549     __ ldr(c_rarg3, Address(c_rarg2));
 550     __ add(c_rarg3, c_rarg3, 1);
 551     __ str(c_rarg3, Address(c_rarg2));
 552 
 553     // object is in r0
 554     // make sure object is &#39;reasonable&#39;
 555     __ cbz(r0, exit); // if obj is NULL it is OK
 556 
<span class="line-added"> 557 #if INCLUDE_ZGC</span>
<span class="line-added"> 558     if (UseZGC) {</span>
<span class="line-added"> 559       // Check if mask is good.</span>
<span class="line-added"> 560       // verifies that ZAddressBadMask &amp; r0 == 0</span>
<span class="line-added"> 561       __ ldr(c_rarg3, Address(rthread, ZThreadLocalData::address_bad_mask_offset()));</span>
<span class="line-added"> 562       __ andr(c_rarg2, r0, c_rarg3);</span>
<span class="line-added"> 563       __ cbnz(c_rarg2, error);</span>
<span class="line-added"> 564     }</span>
<span class="line-added"> 565 #endif</span>
<span class="line-added"> 566 </span>
 567     // Check if the oop is in the right area of memory
 568     __ mov(c_rarg3, (intptr_t) Universe::verify_oop_mask());
 569     __ andr(c_rarg2, r0, c_rarg3);
 570     __ mov(c_rarg3, (intptr_t) Universe::verify_oop_bits());
 571 
 572     // Compare c_rarg2 and c_rarg3.  We don&#39;t use a compare
 573     // instruction here because the flags register is live.
 574     __ eor(c_rarg2, c_rarg2, c_rarg3);
 575     __ cbnz(c_rarg2, error);
 576 
 577     // make sure klass is &#39;reasonable&#39;, which is not zero.
 578     __ load_klass(r0, r0);  // get klass
 579     __ cbz(r0, error);      // if klass is NULL it is broken
 580 
 581     // return if everything seems ok
 582     __ bind(exit);
 583 
 584     __ ldp(c_rarg3, c_rarg2, Address(__ post(sp, 16)));
 585     __ ret(lr);
 586 
 587     // handle errors
 588     __ bind(error);
 589     __ ldp(c_rarg3, c_rarg2, Address(__ post(sp, 16)));
 590 
 591     __ push(RegSet::range(r0, r29), sp);
 592     // debug(char* msg, int64_t pc, int64_t regs[])
 593     __ mov(c_rarg0, rscratch1);      // pass address of error message
 594     __ mov(c_rarg1, lr);             // pass return address
 595     __ mov(c_rarg2, sp);             // pass address of regs on stack
 596 #ifndef PRODUCT
 597     assert(frame::arg_reg_save_area_bytes == 0, &quot;not expecting frame reg save area&quot;);
 598 #endif
 599     BLOCK_COMMENT(&quot;call MacroAssembler::debug&quot;);
 600     __ mov(rscratch1, CAST_FROM_FN_PTR(address, MacroAssembler::debug64));
<span class="line-modified"> 601     __ blr(rscratch1);</span>
<span class="line-added"> 602     __ hlt(0);</span>
 603 
 604     return start;
 605   }
 606 
 607   void array_overlap_test(Label&amp; L_no_overlap, Address::sxtw sf) { __ b(L_no_overlap); }
 608 
 609   // The inner part of zero_words().  This is the bulk operation,
 610   // zeroing words in blocks, possibly using DC ZVA to do it.  The
 611   // caller is responsible for zeroing the last few words.
 612   //
 613   // Inputs:
 614   // r10: the HeapWord-aligned base address of an array to zero.
 615   // r11: the count in HeapWords, r11 &gt; 0.
 616   //
 617   // Returns r10 and r11, adjusted for the caller to clear.
 618   // r10: the base address of the tail of words left to clear.
 619   // r11: the number of words in the tail.
 620   //      r11 &lt; MacroAssembler::zero_words_block_size.
 621 
 622   address generate_zero_blocks() {
</pre>
<hr />
<pre>
1332     __ align(CodeEntryAlignment);
1333     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1334     address start = __ pc();
1335     __ enter();
1336 
1337     if (entry != NULL) {
1338       *entry = __ pc();
1339       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1340       BLOCK_COMMENT(&quot;Entry:&quot;);
1341     }
1342 
1343     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;
1344     if (dest_uninitialized) {
1345       decorators |= IS_DEST_UNINITIALIZED;
1346     }
1347     if (aligned) {
1348       decorators |= ARRAYCOPY_ALIGNED;
1349     }
1350 
1351     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
<span class="line-modified">1352     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, s, d, count, saved_reg);</span>
1353 
1354     if (is_oop) {
1355       // save regs before copy_memory
1356       __ push(RegSet::of(d, count), sp);
1357     }
<span class="line-modified">1358     {</span>
<span class="line-added">1359       // UnsafeCopyMemory page error: continue after ucm</span>
<span class="line-added">1360       bool add_entry = !is_oop &amp;&amp; (!aligned || sizeof(jlong) == size);</span>
<span class="line-added">1361       UnsafeCopyMemoryMark ucmm(this, add_entry, true);</span>
<span class="line-added">1362       copy_memory(aligned, s, d, count, rscratch1, size);</span>
<span class="line-added">1363     }</span>
1364 
1365     if (is_oop) {
1366       __ pop(RegSet::of(d, count), sp);
1367       if (VerifyOops)
1368         verify_oop_array(size, d, count, r16);


1369     }
1370 
1371     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, d, count, rscratch1, RegSet());
1372 
1373     __ leave();
1374     __ mov(r0, zr); // return 0
1375     __ ret(lr);






1376     return start;
1377   }
1378 
1379   // Arguments:
1380   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1381   //             ignored
1382   //   is_oop  - true =&gt; oop array, so generate store check code
1383   //   name    - stub name string
1384   //
1385   // Inputs:
1386   //   c_rarg0   - source array address
1387   //   c_rarg1   - destination array address
1388   //   c_rarg2   - element count, treated as ssize_t, can be zero
1389   //
1390   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1391   // the hardware handle it.  The two dwords within qwords that span
1392   // cache line boundaries will still be loaded and stored atomicly.
1393   //
1394   address generate_conjoint_copy(size_t size, bool aligned, bool is_oop, address nooverlap_target,
1395                                  address *entry, const char *name,
</pre>
<hr />
<pre>
1403     if (entry != NULL) {
1404       *entry = __ pc();
1405       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1406       BLOCK_COMMENT(&quot;Entry:&quot;);
1407     }
1408 
1409     // use fwd copy when (d-s) above_equal (count*size)
1410     __ sub(rscratch1, d, s);
1411     __ cmp(rscratch1, count, Assembler::LSL, exact_log2(size));
1412     __ br(Assembler::HS, nooverlap_target);
1413 
1414     DecoratorSet decorators = IN_HEAP | IS_ARRAY;
1415     if (dest_uninitialized) {
1416       decorators |= IS_DEST_UNINITIALIZED;
1417     }
1418     if (aligned) {
1419       decorators |= ARRAYCOPY_ALIGNED;
1420     }
1421 
1422     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
<span class="line-modified">1423     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, s, d, count, saved_regs);</span>
1424 
1425     if (is_oop) {
1426       // save regs before copy_memory
1427       __ push(RegSet::of(d, count), sp);
1428     }
<span class="line-modified">1429     {</span>
<span class="line-added">1430       // UnsafeCopyMemory page error: continue after ucm</span>
<span class="line-added">1431       bool add_entry = !is_oop &amp;&amp; (!aligned || sizeof(jlong) == size);</span>
<span class="line-added">1432       UnsafeCopyMemoryMark ucmm(this, add_entry, true);</span>
<span class="line-added">1433       copy_memory(aligned, s, d, count, rscratch1, -size);</span>
<span class="line-added">1434     }</span>
1435     if (is_oop) {
1436       __ pop(RegSet::of(d, count), sp);
1437       if (VerifyOops)
1438         verify_oop_array(size, d, count, r16);


1439     }
1440     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, d, count, rscratch1, RegSet());
1441     __ leave();
1442     __ mov(r0, zr); // return 0
1443     __ ret(lr);






1444     return start;
1445 }
1446 
1447   // Arguments:
1448   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1449   //             ignored
1450   //   name    - stub name string
1451   //
1452   // Inputs:
1453   //   c_rarg0   - source array address
1454   //   c_rarg1   - destination array address
1455   //   c_rarg2   - element count, treated as ssize_t, can be zero
1456   //
1457   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1458   // we let the hardware handle it.  The one to eight bytes within words,
1459   // dwords or qwords that span cache line boundaries will still be loaded
1460   // and stored atomically.
1461   //
1462   // Side Effects:
1463   //   disjoint_byte_copy_entry is set to the no-overlap entry point  //
</pre>
<hr />
<pre>
1755 
1756      // Empty array:  Nothing to do.
1757     __ cbz(count, L_done);
1758 
1759     __ push(RegSet::of(r18, r19, r20, r21), sp);
1760 
1761 #ifdef ASSERT
1762     BLOCK_COMMENT(&quot;assert consistent ckoff/ckval&quot;);
1763     // The ckoff and ckval must be mutually consistent,
1764     // even though caller generates both.
1765     { Label L;
1766       int sco_offset = in_bytes(Klass::super_check_offset_offset());
1767       __ ldrw(start_to, Address(ckval, sco_offset));
1768       __ cmpw(ckoff, start_to);
1769       __ br(Assembler::EQ, L);
1770       __ stop(&quot;super_check_offset inconsistent&quot;);
1771       __ bind(L);
1772     }
1773 #endif //ASSERT
1774 
<span class="line-modified">1775     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT;</span>
1776     bool is_oop = true;
1777     if (dest_uninitialized) {
1778       decorators |= IS_DEST_UNINITIALIZED;
1779     }
1780 
1781     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
<span class="line-modified">1782     bs-&gt;arraycopy_prologue(_masm, decorators, is_oop, from, to, count, wb_pre_saved_regs);</span>
1783 
1784     // save the original count
1785     __ mov(count_save, count);
1786 
1787     // Copy from low to high addresses
1788     __ mov(start_to, to);              // Save destination array start address
1789     __ b(L_load_element);
1790 
1791     // ======== begin loop ========
1792     // (Loop is rotated; its entry is L_load_element.)
1793     // Loop control:
1794     //   for (; count != 0; count--) {
1795     //     copied_oop = load_heap_oop(from++);
1796     //     ... generate_type_check ...;
1797     //     store_heap_oop(to++, copied_oop);
1798     //   }
1799     __ align(OptoLoopAlignment);
1800 
1801     __ BIND(L_store_element);
1802     __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, AS_RAW);  // store the oop
</pre>
<hr />
<pre>
1805 
1806     // ======== loop entry is here ========
1807     __ BIND(L_load_element);
1808     __ load_heap_oop(copied_oop, __ post(from, UseCompressedOops ? 4 : 8), noreg, noreg, AS_RAW); // load the oop
1809     __ cbz(copied_oop, L_store_element);
1810 
1811     __ load_klass(r19_klass, copied_oop);// query the object klass
1812     generate_type_check(r19_klass, ckoff, ckval, L_store_element);
1813     // ======== end loop ========
1814 
1815     // It was a real error; we must depend on the caller to finish the job.
1816     // Register count = remaining oops, count_orig = total oops.
1817     // Emit GC store barriers for the oops we have copied and report
1818     // their number to the caller.
1819 
1820     __ subs(count, count_save, count);     // K = partially copied oop count
1821     __ eon(count, count, zr);                   // report (-1^K) to caller
1822     __ br(Assembler::EQ, L_done_pop);
1823 
1824     __ BIND(L_do_card_marks);
<span class="line-modified">1825     bs-&gt;arraycopy_epilogue(_masm, decorators, is_oop, start_to, count_save, rscratch1, wb_post_saved_regs);</span>

1826 
1827     __ bind(L_done_pop);
1828     __ pop(RegSet::of(r18, r19, r20, r21), sp);
1829     inc_counter_np(SharedRuntime::_checkcast_array_copy_ctr);
1830 
1831     __ bind(L_done);
1832     __ mov(r0, count);
1833     __ leave();
1834     __ ret(lr);
1835 
1836     return start;
1837   }
1838 
1839   // Perform range checks on the proposed arraycopy.
1840   // Kills temp, but nothing else.
1841   // Also, clean the sign bits of src_pos and dst_pos.
1842   void arraycopy_range_checks(Register src,     // source array oop (c_rarg0)
1843                               Register src_pos, // source position (c_rarg1)
1844                               Register dst,     // destination array oo (c_rarg2)
1845                               Register dst_pos, // destination position (c_rarg3)
</pre>
<hr />
<pre>
2335         break;
2336       case T_SHORT:
2337         __ tbz(count, 0, L_fill_4);
2338         __ strh(value, Address(__ post(to, 2)));
2339         __ bind(L_fill_4);
2340         __ tbz(count, 1, L_exit2);
2341         __ strw(value, Address(to));
2342         break;
2343       case T_INT:
2344         __ cbzw(count, L_exit2);
2345         __ strw(value, Address(to));
2346         break;
2347       default: ShouldNotReachHere();
2348     }
2349     __ bind(L_exit2);
2350     __ leave();
2351     __ ret(lr);
2352     return start;
2353   }
2354 
<span class="line-added">2355   address generate_data_cache_writeback() {</span>
<span class="line-added">2356     const Register line        = c_rarg0;  // address of line to write back</span>
<span class="line-added">2357 </span>
<span class="line-added">2358     __ align(CodeEntryAlignment);</span>
<span class="line-added">2359 </span>
<span class="line-added">2360     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_data_cache_writeback&quot;);</span>
<span class="line-added">2361 </span>
<span class="line-added">2362     address start = __ pc();</span>
<span class="line-added">2363     __ enter();</span>
<span class="line-added">2364     __ cache_wb(Address(line, 0));</span>
<span class="line-added">2365     __ leave();</span>
<span class="line-added">2366     __ ret(lr);</span>
<span class="line-added">2367 </span>
<span class="line-added">2368     return start;</span>
<span class="line-added">2369   }</span>
<span class="line-added">2370 </span>
<span class="line-added">2371   address generate_data_cache_writeback_sync() {</span>
<span class="line-added">2372     const Register is_pre     = c_rarg0;  // pre or post sync</span>
<span class="line-added">2373 </span>
<span class="line-added">2374     __ align(CodeEntryAlignment);</span>
<span class="line-added">2375 </span>
<span class="line-added">2376     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_data_cache_writeback_sync&quot;);</span>
<span class="line-added">2377 </span>
<span class="line-added">2378     // pre wbsync is a no-op</span>
<span class="line-added">2379     // post wbsync translates to an sfence</span>
<span class="line-added">2380 </span>
<span class="line-added">2381     Label skip;</span>
<span class="line-added">2382     address start = __ pc();</span>
<span class="line-added">2383     __ enter();</span>
<span class="line-added">2384     __ cbnz(is_pre, skip);</span>
<span class="line-added">2385     __ cache_wbsync(false);</span>
<span class="line-added">2386     __ bind(skip);</span>
<span class="line-added">2387     __ leave();</span>
<span class="line-added">2388     __ ret(lr);</span>
<span class="line-added">2389 </span>
<span class="line-added">2390     return start;</span>
<span class="line-added">2391   }</span>
<span class="line-added">2392 </span>
2393   void generate_arraycopy_stubs() {
2394     address entry;
2395     address entry_jbyte_arraycopy;
2396     address entry_jshort_arraycopy;
2397     address entry_jint_arraycopy;
2398     address entry_oop_arraycopy;
2399     address entry_jlong_arraycopy;
2400     address entry_checkcast_arraycopy;
2401 
2402     generate_copy_longs(copy_f, r0, r1, rscratch2, copy_forwards);
2403     generate_copy_longs(copy_b, r0, r1, rscratch2, copy_backwards);
2404 
2405     StubRoutines::aarch64::_zero_blocks = generate_zero_blocks();
2406 
2407     //*** jbyte
2408     // Always need aligned and unaligned versions
2409     StubRoutines::_jbyte_disjoint_arraycopy         = generate_disjoint_byte_copy(false, &amp;entry,
2410                                                                                   &quot;jbyte_disjoint_arraycopy&quot;);
2411     StubRoutines::_jbyte_arraycopy                  = generate_conjoint_byte_copy(false, entry,
2412                                                                                   &amp;entry_jbyte_arraycopy,
</pre>
<hr />
<pre>
3109     __ addv(v0, __ T4S, v0, v2);
3110     __ addv(v1, __ T4S, v1, v3);
3111 
3112     if (multi_block) {
3113       __ add(ofs, ofs, 64);
3114       __ cmp(ofs, limit);
3115       __ br(Assembler::LE, sha1_loop);
3116       __ mov(c_rarg0, ofs); // return ofs
3117     }
3118 
3119     __ ldpd(v10, v11, Address(sp, 16));
3120     __ ldpd(v8, v9, __ post(sp, 32));
3121 
3122     __ stpq(v0, v1, state);
3123 
3124     __ ret(lr);
3125 
3126     return start;
3127   }
3128 

3129   // Safefetch stubs.
3130   void generate_safefetch(const char* name, int size, address* entry,
3131                           address* fault_pc, address* continuation_pc) {
3132     // safefetch signatures:
3133     //   int      SafeFetch32(int*      adr, int      errValue);
3134     //   intptr_t SafeFetchN (intptr_t* adr, intptr_t errValue);
3135     //
3136     // arguments:
3137     //   c_rarg0 = adr
3138     //   c_rarg1 = errValue
3139     //
3140     // result:
3141     //   PPC_RET  = *adr or errValue
3142 
3143     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3144 
3145     // Entry point, pc or function descriptor.
3146     *entry = __ pc();
3147 
3148     // Load *adr into c_rarg1, may fault.
3149     *fault_pc = __ pc();
3150     switch (size) {
3151       case 4:
3152         // int32_t
3153         __ ldrw(c_rarg1, Address(c_rarg0, 0));
3154         break;
3155       case 8:
3156         // int64_t
3157         __ ldr(c_rarg1, Address(c_rarg0, 0));
3158         break;
3159       default:
3160         ShouldNotReachHere();
3161     }
3162 
3163     // return errValue or *adr
3164     *continuation_pc = __ pc();
3165     __ mov(r0, c_rarg1);
3166     __ ret(lr);
3167   }

3168 
3169   /**
3170    *  Arguments:
3171    *
3172    * Inputs:
3173    *   c_rarg0   - int crc
3174    *   c_rarg1   - byte* buf
3175    *   c_rarg2   - int length
3176    *
3177    * Ouput:
3178    *       rax   - int crc result
3179    */
3180   address generate_updateBytesCRC32() {
3181     assert(UseCRC32Intrinsics, &quot;what are we doing here?&quot;);
3182 
3183     __ align(CodeEntryAlignment);
3184     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesCRC32&quot;);
3185 
3186     address start = __ pc();
3187 
</pre>
<hr />
<pre>
3994         tmpC4, tmp1, tmp2, tmp3, tmp4, tmp5);
3995     return entry;
3996   }
3997 
3998   // code for comparing 16 bytes of strings with same encoding
3999   void compare_string_16_bytes_same(Label &amp;DIFF1, Label &amp;DIFF2) {
4000     Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, tmp1 = r10, tmp2 = r11;
4001     __ ldr(rscratch1, Address(__ post(str1, 8)));
4002     __ eor(rscratch2, tmp1, tmp2);
4003     __ ldr(cnt1, Address(__ post(str2, 8)));
4004     __ cbnz(rscratch2, DIFF1);
4005     __ ldr(tmp1, Address(__ post(str1, 8)));
4006     __ eor(rscratch2, rscratch1, cnt1);
4007     __ ldr(tmp2, Address(__ post(str2, 8)));
4008     __ cbnz(rscratch2, DIFF2);
4009   }
4010 
4011   // code for comparing 16 characters of strings with Latin1 and Utf16 encoding
4012   void compare_string_16_x_LU(Register tmpL, Register tmpU, Label &amp;DIFF1,
4013       Label &amp;DIFF2) {
<span class="line-modified">4014     Register cnt1 = r2, tmp2 = r11, tmp3 = r12;</span>
4015     FloatRegister vtmp = v1, vtmpZ = v0, vtmp3 = v2;
4016 
4017     __ ldrq(vtmp, Address(__ post(tmp2, 16)));
4018     __ ldr(tmpU, Address(__ post(cnt1, 8)));
4019     __ zip1(vtmp3, __ T16B, vtmp, vtmpZ);
4020     // now we have 32 bytes of characters (converted to U) in vtmp:vtmp3
4021 
4022     __ fmovd(tmpL, vtmp3);
4023     __ eor(rscratch2, tmp3, tmpL);
4024     __ cbnz(rscratch2, DIFF2);
4025 
4026     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4027     __ umov(tmpL, vtmp3, __ D, 1);
4028     __ eor(rscratch2, tmpU, tmpL);
4029     __ cbnz(rscratch2, DIFF1);
4030 
4031     __ zip2(vtmp, __ T16B, vtmp, vtmpZ);
4032     __ ldr(tmpU, Address(__ post(cnt1, 8)));
4033     __ fmovd(tmpL, vtmp);
4034     __ eor(rscratch2, tmp3, tmpL);
</pre>
<hr />
<pre>
4037     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4038     __ umov(tmpL, vtmp, __ D, 1);
4039     __ eor(rscratch2, tmpU, tmpL);
4040     __ cbnz(rscratch2, DIFF1);
4041   }
4042 
4043   // r0  = result
4044   // r1  = str1
4045   // r2  = cnt1
4046   // r3  = str2
4047   // r4  = cnt2
4048   // r10 = tmp1
4049   // r11 = tmp2
4050   address generate_compare_long_string_different_encoding(bool isLU) {
4051     __ align(CodeEntryAlignment);
4052     StubCodeMark mark(this, &quot;StubRoutines&quot;, isLU
4053         ? &quot;compare_long_string_different_encoding LU&quot;
4054         : &quot;compare_long_string_different_encoding UL&quot;);
4055     address entry = __ pc();
4056     Label SMALL_LOOP, TAIL, TAIL_LOAD_16, LOAD_LAST, DIFF1, DIFF2,
<span class="line-modified">4057         DONE, CALCULATE_DIFFERENCE, LARGE_LOOP_PREFETCH, NO_PREFETCH,</span>
4058         LARGE_LOOP_PREFETCH_REPEAT1, LARGE_LOOP_PREFETCH_REPEAT2;
4059     Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, cnt2 = r4,
4060         tmp1 = r10, tmp2 = r11, tmp3 = r12, tmp4 = r14;
4061     FloatRegister vtmpZ = v0, vtmp = v1, vtmp3 = v2;
4062     RegSet spilled_regs = RegSet::of(tmp3, tmp4);
4063 
<span class="line-modified">4064     int prefetchLoopExitCondition = MAX(64, SoftwarePrefetchHintDistance/2);</span>
4065 
4066     __ eor(vtmpZ, __ T16B, vtmpZ, vtmpZ);
4067     // cnt2 == amount of characters left to compare
4068     // Check already loaded first 4 symbols(vtmp and tmp2(LU)/tmp1(UL))
4069     __ zip1(vtmp, __ T8B, vtmp, vtmpZ);
4070     __ add(str1, str1, isLU ? wordSize/2 : wordSize);
4071     __ add(str2, str2, isLU ? wordSize : wordSize/2);
4072     __ fmovd(isLU ? tmp1 : tmp2, vtmp);
4073     __ subw(cnt2, cnt2, 8); // Already loaded 4 symbols. Last 4 is special case.

4074     __ eor(rscratch2, tmp1, tmp2);

4075     __ mov(rscratch1, tmp2);
4076     __ cbnz(rscratch2, CALCULATE_DIFFERENCE);
<span class="line-modified">4077     Register tmpU = isLU ? rscratch1 : tmp1, // where to keep U for comparison</span>


4078              tmpL = isLU ? tmp1 : rscratch1; // where to keep L for comparison
4079     __ push(spilled_regs, sp);
<span class="line-modified">4080     __ mov(tmp2, isLU ? str1 : str2); // init the pointer to L next load</span>
<span class="line-modified">4081     __ mov(cnt1, isLU ? str2 : str1); // init the pointer to U next load</span>
4082 
4083     __ ldr(tmp3, Address(__ post(cnt1, 8)));
4084 
4085     if (SoftwarePrefetchHintDistance &gt;= 0) {
4086       __ subs(rscratch2, cnt2, prefetchLoopExitCondition);
<span class="line-modified">4087       __ br(__ LT, NO_PREFETCH);</span>
4088       __ bind(LARGE_LOOP_PREFETCH);
4089         __ prfm(Address(tmp2, SoftwarePrefetchHintDistance));
4090         __ mov(tmp4, 2);
4091         __ prfm(Address(cnt1, SoftwarePrefetchHintDistance));
4092         __ bind(LARGE_LOOP_PREFETCH_REPEAT1);
4093           compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4094           __ subs(tmp4, tmp4, 1);
4095           __ br(__ GT, LARGE_LOOP_PREFETCH_REPEAT1);
4096           __ prfm(Address(cnt1, SoftwarePrefetchHintDistance));
4097           __ mov(tmp4, 2);
4098         __ bind(LARGE_LOOP_PREFETCH_REPEAT2);
4099           compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4100           __ subs(tmp4, tmp4, 1);
4101           __ br(__ GT, LARGE_LOOP_PREFETCH_REPEAT2);
4102           __ sub(cnt2, cnt2, 64);
4103           __ subs(rscratch2, cnt2, prefetchLoopExitCondition);
4104           __ br(__ GE, LARGE_LOOP_PREFETCH);
4105     }
4106     __ cbz(cnt2, LOAD_LAST); // no characters left except last load
<span class="line-added">4107     __ bind(NO_PREFETCH);</span>
4108     __ subs(cnt2, cnt2, 16);
4109     __ br(__ LT, TAIL);
<span class="line-modified">4110     __ align(OptoLoopAlignment);</span>
4111     __ bind(SMALL_LOOP); // smaller loop
4112       __ subs(cnt2, cnt2, 16);

4113       compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2);
4114       __ br(__ GE, SMALL_LOOP);
<span class="line-modified">4115       __ cmn(cnt2, (u1)16);</span>
<span class="line-modified">4116       __ br(__ EQ, LOAD_LAST);</span>
<span class="line-modified">4117     __ bind(TAIL); // 1..15 characters left until last load (last 4 characters)</span>
<span class="line-modified">4118       __ add(cnt1, cnt1, cnt2, __ LSL, 1); // Address of 32 bytes before last 4 characters in UTF-16 string</span>
<span class="line-modified">4119       __ add(tmp2, tmp2, cnt2); // Address of 16 bytes before last 4 characters in Latin1 string</span>
<span class="line-modified">4120       __ ldr(tmp3, Address(cnt1, -8));</span>
<span class="line-modified">4121       compare_string_16_x_LU(tmpL, tmpU, DIFF1, DIFF2); // last 16 characters before last load</span>






























4122       __ b(LOAD_LAST);
4123     __ bind(DIFF2);
4124       __ mov(tmpU, tmp3);
4125     __ bind(DIFF1);
4126       __ pop(spilled_regs, sp);
4127       __ b(CALCULATE_DIFFERENCE);
4128     __ bind(LOAD_LAST);
<span class="line-added">4129       // Last 4 UTF-16 characters are already pre-loaded into tmp3 by compare_string_16_x_LU.</span>
<span class="line-added">4130       // No need to load it again</span>
<span class="line-added">4131       __ mov(tmpU, tmp3);</span>
4132       __ pop(spilled_regs, sp);
4133 
<span class="line-modified">4134       // tmp2 points to the address of the last 4 Latin1 characters right now</span>
<span class="line-modified">4135       __ ldrs(vtmp, Address(tmp2));</span>
4136       __ zip1(vtmp, __ T8B, vtmp, vtmpZ);
4137       __ fmovd(tmpL, vtmp);
4138 
4139       __ eor(rscratch2, tmpU, tmpL);
4140       __ cbz(rscratch2, DONE);
4141 
4142     // Find the first different characters in the longwords and
4143     // compute their difference.
4144     __ bind(CALCULATE_DIFFERENCE);
4145       __ rev(rscratch2, rscratch2);
4146       __ clz(rscratch2, rscratch2);
4147       __ andr(rscratch2, rscratch2, -16);
4148       __ lsrv(tmp1, tmp1, rscratch2);
4149       __ uxthw(tmp1, tmp1);
4150       __ lsrv(rscratch1, rscratch1, rscratch2);
4151       __ uxthw(rscratch1, rscratch1);
4152       __ subw(result, tmp1, rscratch1);
4153     __ bind(DONE);
4154       __ ret(lr);
4155     return entry;
</pre>
<hr />
<pre>
4177     // to prefetch memory behind array border
4178     int largeLoopExitCondition = MAX(64, SoftwarePrefetchHintDistance)/(isLL ? 1 : 2);
4179     // cnt1/cnt2 contains amount of characters to compare. cnt1 can be re-used
4180     // update cnt2 counter with already loaded 8 bytes
4181     __ sub(cnt2, cnt2, wordSize/(isLL ? 1 : 2));
4182     // update pointers, because of previous read
4183     __ add(str1, str1, wordSize);
4184     __ add(str2, str2, wordSize);
4185     if (SoftwarePrefetchHintDistance &gt;= 0) {
4186       __ bind(LARGE_LOOP_PREFETCH);
4187         __ prfm(Address(str1, SoftwarePrefetchHintDistance));
4188         __ prfm(Address(str2, SoftwarePrefetchHintDistance));
4189         compare_string_16_bytes_same(DIFF, DIFF2);
4190         compare_string_16_bytes_same(DIFF, DIFF2);
4191         __ sub(cnt2, cnt2, isLL ? 64 : 32);
4192         compare_string_16_bytes_same(DIFF, DIFF2);
4193         __ subs(rscratch2, cnt2, largeLoopExitCondition);
4194         compare_string_16_bytes_same(DIFF, DIFF2);
4195         __ br(__ GT, LARGE_LOOP_PREFETCH);
4196         __ cbz(cnt2, LAST_CHECK_AND_LENGTH_DIFF); // no more chars left?



4197     }
<span class="line-added">4198     // less than 16 bytes left?</span>
<span class="line-added">4199     __ subs(cnt2, cnt2, isLL ? 16 : 8);</span>
<span class="line-added">4200     __ br(__ LT, TAIL);</span>
<span class="line-added">4201     __ align(OptoLoopAlignment);</span>
4202     __ bind(SMALL_LOOP);
4203       compare_string_16_bytes_same(DIFF, DIFF2);
4204       __ subs(cnt2, cnt2, isLL ? 16 : 8);
4205       __ br(__ GE, SMALL_LOOP);
4206     __ bind(TAIL);
4207       __ adds(cnt2, cnt2, isLL ? 16 : 8);
4208       __ br(__ EQ, LAST_CHECK_AND_LENGTH_DIFF);
4209       __ subs(cnt2, cnt2, isLL ? 8 : 4);
4210       __ br(__ LE, CHECK_LAST);
4211       __ eor(rscratch2, tmp1, tmp2);
4212       __ cbnz(rscratch2, DIFF);
4213       __ ldr(tmp1, Address(__ post(str1, 8)));
4214       __ ldr(tmp2, Address(__ post(str2, 8)));
4215       __ sub(cnt2, cnt2, isLL ? 8 : 4);
4216     __ bind(CHECK_LAST);
4217       if (!isLL) {
4218         __ add(cnt2, cnt2, cnt2); // now in bytes
4219       }
4220       __ eor(rscratch2, tmp1, tmp2);
4221       __ cbnz(rscratch2, DIFF);
</pre>
<hr />
<pre>
4310     bool isL = str1_isL &amp;&amp; str2_isL;
4311    // parameters
4312     Register result = r0, str2 = r1, cnt1 = r2, str1 = r3, cnt2 = r4;
4313     // temporary registers
4314     Register tmp1 = r20, tmp2 = r21, tmp3 = r22, tmp4 = r23;
4315     RegSet spilled_regs = RegSet::range(tmp1, tmp4);
4316     // redefinitions
4317     Register ch1 = rscratch1, ch2 = rscratch2, first = tmp3;
4318 
4319     __ push(spilled_regs, sp);
4320     Label L_LOOP, L_LOOP_PROCEED, L_SMALL, L_HAS_ZERO,
4321         L_HAS_ZERO_LOOP, L_CMP_LOOP, L_CMP_LOOP_NOMATCH, L_SMALL_PROCEED,
4322         L_SMALL_HAS_ZERO_LOOP, L_SMALL_CMP_LOOP_NOMATCH, L_SMALL_CMP_LOOP,
4323         L_POST_LOOP, L_CMP_LOOP_LAST_CMP, L_HAS_ZERO_LOOP_NOMATCH,
4324         L_SMALL_CMP_LOOP_LAST_CMP, L_SMALL_CMP_LOOP_LAST_CMP2,
4325         L_CMP_LOOP_LAST_CMP2, DONE, NOMATCH;
4326     // Read whole register from str1. It is safe, because length &gt;=8 here
4327     __ ldr(ch1, Address(str1));
4328     // Read whole register from str2. It is safe, because length &gt;=8 here
4329     __ ldr(ch2, Address(str2));
<span class="line-added">4330     __ sub(cnt2, cnt2, cnt1);</span>
4331     __ andr(first, ch1, str1_isL ? 0xFF : 0xFFFF);
4332     if (str1_isL != str2_isL) {
4333       __ eor(v0, __ T16B, v0, v0);
4334     }
4335     __ mov(tmp1, str2_isL ? 0x0101010101010101 : 0x0001000100010001);
4336     __ mul(first, first, tmp1);
4337     // check if we have less than 1 register to check
4338     __ subs(cnt2, cnt2, wordSize/str2_chr_size - 1);
4339     if (str1_isL != str2_isL) {
4340       __ fmovd(v1, ch1);
4341     }
4342     __ br(__ LE, L_SMALL);
4343     __ eor(ch2, first, ch2);
4344     if (str1_isL != str2_isL) {
4345       __ zip1(v1, __ T16B, v1, v0);
4346     }
4347     __ sub(tmp2, ch2, tmp1);
4348     __ orr(ch2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4349     __ bics(tmp2, tmp2, ch2);
4350     if (str1_isL != str2_isL) {
</pre>
<hr />
<pre>
4782     // lr and fp are already in place
4783     __ sub(sp, rfp, ((unsigned)framesize-4) &lt;&lt; LogBytesPerInt); // prolog
4784 
4785     int frame_complete = __ pc() - start;
4786 
4787     // Set up last_Java_sp and last_Java_fp
4788     address the_pc = __ pc();
4789     __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);
4790 
4791     // Call runtime
4792     if (arg1 != noreg) {
4793       assert(arg2 != c_rarg1, &quot;clobbered&quot;);
4794       __ mov(c_rarg1, arg1);
4795     }
4796     if (arg2 != noreg) {
4797       __ mov(c_rarg2, arg2);
4798     }
4799     __ mov(c_rarg0, rthread);
4800     BLOCK_COMMENT(&quot;call runtime_entry&quot;);
4801     __ mov(rscratch1, runtime_entry);
<span class="line-modified">4802     __ blr(rscratch1);</span>
4803 
4804     // Generate oop map
4805     OopMap* map = new OopMap(framesize, 0);
4806 
4807     oop_maps-&gt;add_gc_map(the_pc - start, map);
4808 
4809     __ reset_last_Java_frame(true);
4810     __ maybe_isb();
4811 
4812     __ leave();
4813 
4814     // check for pending exceptions
4815 #ifdef ASSERT
4816     Label L;
4817     __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));
4818     __ cbnz(rscratch1, L);
4819     __ should_not_reach_here();
4820     __ bind(L);
4821 #endif // ASSERT
4822     __ far_jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
</pre>
<hr />
<pre>
5756 
5757     if (UseMulAddIntrinsic) {
5758       StubRoutines::_mulAdd = generate_mulAdd();
5759     }
5760 
5761     if (UseMontgomeryMultiplyIntrinsic) {
5762       StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;montgomeryMultiply&quot;);
5763       MontgomeryMultiplyGenerator g(_masm, /*squaring*/false);
5764       StubRoutines::_montgomeryMultiply = g.generate_multiply();
5765     }
5766 
5767     if (UseMontgomerySquareIntrinsic) {
5768       StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;montgomerySquare&quot;);
5769       MontgomeryMultiplyGenerator g(_masm, /*squaring*/true);
5770       // We use generate_multiply() rather than generate_square()
5771       // because it&#39;s faster for the sizes of modulus we care about.
5772       StubRoutines::_montgomerySquare = g.generate_multiply();
5773     }
5774 #endif // COMPILER2
5775 

5776     // generate GHASH intrinsics code
5777     if (UseGHASHIntrinsics) {
5778       StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();
5779     }
5780 
<span class="line-added">5781     // data cache line writeback</span>
<span class="line-added">5782     StubRoutines::_data_cache_writeback = generate_data_cache_writeback();</span>
<span class="line-added">5783     StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();</span>
<span class="line-added">5784 </span>
5785     if (UseAESIntrinsics) {
5786       StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();
5787       StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();
5788       StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();
5789       StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt();
5790     }
5791 
5792     if (UseSHA1Intrinsics) {
5793       StubRoutines::_sha1_implCompress     = generate_sha1_implCompress(false,   &quot;sha1_implCompress&quot;);
5794       StubRoutines::_sha1_implCompressMB   = generate_sha1_implCompress(true,    &quot;sha1_implCompressMB&quot;);
5795     }
5796     if (UseSHA256Intrinsics) {
5797       StubRoutines::_sha256_implCompress   = generate_sha256_implCompress(false, &quot;sha256_implCompress&quot;);
5798       StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true,  &quot;sha256_implCompressMB&quot;);
5799     }
5800 
5801     // generate Adler32 intrinsics code
5802     if (UseAdler32Intrinsics) {
5803       StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();
5804     }
5805 
5806     // Safefetch stubs.
5807     generate_safefetch(&quot;SafeFetch32&quot;, sizeof(int),     &amp;StubRoutines::_safefetch32_entry,
5808                                                        &amp;StubRoutines::_safefetch32_fault_pc,
5809                                                        &amp;StubRoutines::_safefetch32_continuation_pc);
5810     generate_safefetch(&quot;SafeFetchN&quot;, sizeof(intptr_t), &amp;StubRoutines::_safefetchN_entry,
5811                                                        &amp;StubRoutines::_safefetchN_fault_pc,
5812                                                        &amp;StubRoutines::_safefetchN_continuation_pc);

5813     StubRoutines::aarch64::set_completed();
5814   }
5815 
5816  public:
5817   StubGenerator(CodeBuffer* code, bool all) : StubCodeGenerator(code) {
5818     if (all) {
5819       generate_all();
5820     } else {
5821       generate_initial();
5822     }
5823   }
5824 }; // end class declaration
5825 
<span class="line-added">5826 #define UCM_TABLE_MAX_ENTRIES 8</span>
5827 void StubGenerator_generate(CodeBuffer* code, bool all) {
<span class="line-added">5828   if (UnsafeCopyMemory::_table == NULL) {</span>
<span class="line-added">5829     UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);</span>
<span class="line-added">5830   }</span>
5831   StubGenerator g(code, all);
5832 }
</pre>
</td>
</tr>
</table>
<center><a href="sharedRuntime_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stubRoutines_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>