<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/aarch64/templateTable_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="templateInterpreterGenerator_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="vm_version_aarch64.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/templateTable_aarch64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  29 #include &quot;interpreter/interpreter.hpp&quot;
  30 #include &quot;interpreter/interpreterRuntime.hpp&quot;
  31 #include &quot;interpreter/interp_masm.hpp&quot;
  32 #include &quot;interpreter/templateTable.hpp&quot;
  33 #include &quot;memory/universe.hpp&quot;
  34 #include &quot;oops/methodData.hpp&quot;
  35 #include &quot;oops/method.hpp&quot;
  36 #include &quot;oops/objArrayKlass.hpp&quot;
  37 #include &quot;oops/oop.inline.hpp&quot;
  38 #include &quot;prims/methodHandles.hpp&quot;
  39 #include &quot;runtime/frame.inline.hpp&quot;
  40 #include &quot;runtime/sharedRuntime.hpp&quot;
  41 #include &quot;runtime/stubRoutines.hpp&quot;
  42 #include &quot;runtime/synchronizer.hpp&quot;

  43 
  44 #define __ _masm-&gt;
  45 
  46 // Platform-dependent initialization
  47 
  48 void TemplateTable::pd_initialize() {
  49   // No aarch64 specific initialization
  50 }
  51 
  52 // Address computation: local variables
  53 
  54 static inline Address iaddress(int n) {
  55   return Address(rlocals, Interpreter::local_offset_in_bytes(n));
  56 }
  57 
  58 static inline Address laddress(int n) {
  59   return iaddress(n + 1);
  60 }
  61 
  62 static inline Address faddress(int n) {
</pre>
<hr />
<pre>
1461   case add:
1462     // n.b. use ldrd because this is a 64 bit slot
1463     __ pop_f(v1);
1464     __ fadds(v0, v1, v0);
1465     break;
1466   case sub:
1467     __ pop_f(v1);
1468     __ fsubs(v0, v1, v0);
1469     break;
1470   case mul:
1471     __ pop_f(v1);
1472     __ fmuls(v0, v1, v0);
1473     break;
1474   case div:
1475     __ pop_f(v1);
1476     __ fdivs(v0, v1, v0);
1477     break;
1478   case rem:
1479     __ fmovs(v1, v0);
1480     __ pop_f(v0);
<span class="line-modified">1481     __ call_VM_leaf_base1(CAST_FROM_FN_PTR(address, SharedRuntime::frem),</span>
<span class="line-removed">1482                          0, 2, MacroAssembler::ret_type_float);</span>
1483     break;
1484   default:
1485     ShouldNotReachHere();
1486     break;
1487   }
1488 }
1489 
1490 void TemplateTable::dop2(Operation op)
1491 {
1492   transition(dtos, dtos);
1493   switch (op) {
1494   case add:
1495     // n.b. use ldrd because this is a 64 bit slot
1496     __ pop_d(v1);
1497     __ faddd(v0, v1, v0);
1498     break;
1499   case sub:
1500     __ pop_d(v1);
1501     __ fsubd(v0, v1, v0);
1502     break;
1503   case mul:
1504     __ pop_d(v1);
1505     __ fmuld(v0, v1, v0);
1506     break;
1507   case div:
1508     __ pop_d(v1);
1509     __ fdivd(v0, v1, v0);
1510     break;
1511   case rem:
1512     __ fmovd(v1, v0);
1513     __ pop_d(v0);
<span class="line-modified">1514     __ call_VM_leaf_base1(CAST_FROM_FN_PTR(address, SharedRuntime::drem),</span>
<span class="line-removed">1515                          0, 2, MacroAssembler::ret_type_double);</span>
1516     break;
1517   default:
1518     ShouldNotReachHere();
1519     break;
1520   }
1521 }
1522 
1523 void TemplateTable::ineg()
1524 {
1525   transition(itos, itos);
1526   __ negw(r0, r0);
1527 
1528 }
1529 
1530 void TemplateTable::lneg()
1531 {
1532   transition(ltos, ltos);
1533   __ neg(r0, r0);
1534 }
1535 
</pre>
<hr />
<pre>
1636     break;
1637   case Bytecodes::_i2s:
1638     __ sxthw(r0, r0);
1639     break;
1640   case Bytecodes::_l2i:
1641     __ uxtw(r0, r0);
1642     break;
1643   case Bytecodes::_l2f:
1644     __ scvtfs(v0, r0);
1645     break;
1646   case Bytecodes::_l2d:
1647     __ scvtfd(v0, r0);
1648     break;
1649   case Bytecodes::_f2i:
1650   {
1651     Label L_Okay;
1652     __ clear_fpsr();
1653     __ fcvtzsw(r0, v0);
1654     __ get_fpsr(r1);
1655     __ cbzw(r1, L_Okay);
<span class="line-modified">1656     __ call_VM_leaf_base1(CAST_FROM_FN_PTR(address, SharedRuntime::f2i),</span>
<span class="line-removed">1657                          0, 1, MacroAssembler::ret_type_integral);</span>
1658     __ bind(L_Okay);
1659   }
1660     break;
1661   case Bytecodes::_f2l:
1662   {
1663     Label L_Okay;
1664     __ clear_fpsr();
1665     __ fcvtzs(r0, v0);
1666     __ get_fpsr(r1);
1667     __ cbzw(r1, L_Okay);
<span class="line-modified">1668     __ call_VM_leaf_base1(CAST_FROM_FN_PTR(address, SharedRuntime::f2l),</span>
<span class="line-removed">1669                          0, 1, MacroAssembler::ret_type_integral);</span>
1670     __ bind(L_Okay);
1671   }
1672     break;
1673   case Bytecodes::_f2d:
1674     __ fcvts(v0, v0);
1675     break;
1676   case Bytecodes::_d2i:
1677   {
1678     Label L_Okay;
1679     __ clear_fpsr();
1680     __ fcvtzdw(r0, v0);
1681     __ get_fpsr(r1);
1682     __ cbzw(r1, L_Okay);
<span class="line-modified">1683     __ call_VM_leaf_base1(CAST_FROM_FN_PTR(address, SharedRuntime::d2i),</span>
<span class="line-removed">1684                          0, 1, MacroAssembler::ret_type_integral);</span>
1685     __ bind(L_Okay);
1686   }
1687     break;
1688   case Bytecodes::_d2l:
1689   {
1690     Label L_Okay;
1691     __ clear_fpsr();
1692     __ fcvtzd(r0, v0);
1693     __ get_fpsr(r1);
1694     __ cbzw(r1, L_Okay);
<span class="line-modified">1695     __ call_VM_leaf_base1(CAST_FROM_FN_PTR(address, SharedRuntime::d2l),</span>
<span class="line-removed">1696                          0, 1, MacroAssembler::ret_type_integral);</span>
1697     __ bind(L_Okay);
1698   }
1699     break;
1700   case Bytecodes::_d2f:
1701     __ fcvtd(v0, v0);
1702     break;
1703   default:
1704     ShouldNotReachHere();
1705   }
1706 }
1707 
1708 void TemplateTable::lcmp()
1709 {
1710   transition(ltos, itos);
1711   Label done;
1712   __ pop_l(r1);
1713   __ cmp(r1, r0);
1714   __ mov(r0, (u_int64_t)-1L);
1715   __ br(Assembler::LT, done);
1716   // __ mov(r0, 1UL);
</pre>
<hr />
<pre>
2306 //     memory refs that happen BEFORE the write float down to after the
2307 //     write.  It&#39;s OK for non-volatile memory refs that happen after the
2308 //     volatile write to float up before it.
2309 //
2310 // We only put in barriers around volatile refs (they are expensive),
2311 // not _between_ memory refs (that would require us to track the
2312 // flavor of the previous memory refs).  Requirements (2) and (3)
2313 // require some barriers before volatile stores and after volatile
2314 // loads.  These nearly cover requirement (1) but miss the
2315 // volatile-store-volatile-load case.  This final case is placed after
2316 // volatile-stores although it could just as well go before
2317 // volatile-loads.
2318 
2319 void TemplateTable::resolve_cache_and_index(int byte_no,
2320                                             Register Rcache,
2321                                             Register index,
2322                                             size_t index_size) {
2323   const Register temp = r19;
2324   assert_different_registers(Rcache, index, temp);
2325 
<span class="line-modified">2326   Label resolved;</span>
2327 
2328   Bytecodes::Code code = bytecode();
2329   switch (code) {
2330   case Bytecodes::_nofast_getfield: code = Bytecodes::_getfield; break;
2331   case Bytecodes::_nofast_putfield: code = Bytecodes::_putfield; break;
2332   default: break;
2333   }
2334 
2335   assert(byte_no == f1_byte || byte_no == f2_byte, &quot;byte_no out of range&quot;);
2336   __ get_cache_and_index_and_bytecode_at_bcp(Rcache, index, temp, byte_no, 1, index_size);
2337   __ subs(zr, temp, (int) code);  // have we resolved this bytecode?
2338   __ br(Assembler::EQ, resolved);
2339 
2340   // resolve first time through


2341   address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache);
2342   __ mov(temp, (int) code);
2343   __ call_VM(noreg, entry, temp);
2344 
2345   // Update registers with resolved info
2346   __ get_cache_and_index_at_bcp(Rcache, index, 1, index_size);
2347   // n.b. unlike x86 Rcache is now rcpool plus the indexed offset
2348   // so all clients ofthis method must be modified accordingly
2349   __ bind(resolved);







2350 }
2351 
2352 // The Rcache and index registers must be set before call
2353 // n.b unlike x86 cache already includes the index offset
2354 void TemplateTable::load_field_cp_cache_entry(Register obj,
2355                                               Register cache,
2356                                               Register index,
2357                                               Register off,
2358                                               Register flags,
2359                                               bool is_static = false) {
2360   assert_different_registers(cache, index, flags, off);
2361 
2362   ByteSize cp_base_offset = ConstantPoolCache::base_offset();
2363   // Field offset
2364   __ ldr(off, Address(cache, in_bytes(cp_base_offset +
2365                                           ConstantPoolCacheEntry::f2_offset())));
2366   // Flags
2367   __ ldrw(flags, Address(cache, in_bytes(cp_base_offset +
2368                                            ConstantPoolCacheEntry::flags_offset())));
2369 
</pre>
<hr />
<pre>
2868     __ pop(dtos);
2869     if (!is_static) pop_and_check_object(obj);
2870     __ access_store_at(T_DOUBLE, IN_HEAP, field, noreg /* dtos */, noreg, noreg);
2871     if (rc == may_rewrite) {
2872       patch_bytecode(Bytecodes::_fast_dputfield, bc, r1, true, byte_no);
2873     }
2874   }
2875 
2876 #ifdef ASSERT
2877   __ b(Done);
2878 
2879   __ bind(notDouble);
2880   __ stop(&quot;Bad state&quot;);
2881 #endif
2882 
2883   __ bind(Done);
2884 
2885   {
2886     Label notVolatile;
2887     __ tbz(r5, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
<span class="line-modified">2888     __ membar(MacroAssembler::StoreLoad);</span>
2889     __ bind(notVolatile);
2890   }
2891 }
2892 
2893 void TemplateTable::putfield(int byte_no)
2894 {
2895   putfield_or_static(byte_no, false);
2896 }
2897 
2898 void TemplateTable::nofast_putfield(int byte_no) {
2899   putfield_or_static(byte_no, false, may_not_rewrite);
2900 }
2901 
2902 void TemplateTable::putstatic(int byte_no) {
2903   putfield_or_static(byte_no, true);
2904 }
2905 
2906 void TemplateTable::jvmti_post_fast_field_mod()
2907 {
2908   if (JvmtiExport::can_post_field_modification()) {
</pre>
<hr />
<pre>
3012     break;
3013   case Bytecodes::_fast_sputfield:
3014     __ access_store_at(T_SHORT, IN_HEAP, field, r0, noreg, noreg);
3015     break;
3016   case Bytecodes::_fast_cputfield:
3017     __ access_store_at(T_CHAR, IN_HEAP, field, r0, noreg, noreg);
3018     break;
3019   case Bytecodes::_fast_fputfield:
3020     __ access_store_at(T_FLOAT, IN_HEAP, field, noreg /* ftos */, noreg, noreg);
3021     break;
3022   case Bytecodes::_fast_dputfield:
3023     __ access_store_at(T_DOUBLE, IN_HEAP, field, noreg /* dtos */, noreg, noreg);
3024     break;
3025   default:
3026     ShouldNotReachHere();
3027   }
3028 
3029   {
3030     Label notVolatile;
3031     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
<span class="line-modified">3032     __ membar(MacroAssembler::StoreLoad);</span>
3033     __ bind(notVolatile);
3034   }
3035 }
3036 
3037 
3038 void TemplateTable::fast_accessfield(TosState state)
3039 {
3040   transition(atos, state);
3041   // Do the JVMTI work here to avoid disturbing the register state below
3042   if (JvmtiExport::can_post_field_access()) {
3043     // Check to see if a field access watch has been set before we
3044     // take the time to call into the VM.
3045     Label L1;
3046     __ lea(rscratch1, ExternalAddress((address) JvmtiExport::get_field_access_count_addr()));
3047     __ ldrw(r2, Address(rscratch1));
3048     __ cbzw(r2, L1);
3049     // access constant pool cache entry
3050     __ get_cache_entry_pointer_at_bcp(c_rarg2, rscratch2, 1);
3051     __ verify_oop(r0);
3052     __ push_ptr(r0);  // save object pointer before call_VM() clobbers it
</pre>
<hr />
<pre>
3401   __ null_check(r2, oopDesc::klass_offset_in_bytes());
3402   __ load_klass(r3, r2);
3403 
3404   Label no_such_method;
3405 
3406   // Preserve method for throw_AbstractMethodErrorVerbose.
3407   __ mov(r16, rmethod);
3408   // Receiver subtype check against REFC.
3409   // Superklass in r0. Subklass in r3. Blows rscratch2, r13
3410   __ lookup_interface_method(// inputs: rec. class, interface, itable index
3411                              r3, r0, noreg,
3412                              // outputs: scan temp. reg, scan temp. reg
3413                              rscratch2, r13,
3414                              no_such_interface,
3415                              /*return_method=*/false);
3416 
3417   // profile this call
3418   __ profile_virtual_call(r3, r13, r19);
3419 
3420   // Get declaring interface class from method, and itable index
<span class="line-modified">3421   __ ldr(r0, Address(rmethod, Method::const_offset()));</span>
<span class="line-modified">3422   __ ldr(r0, Address(r0, ConstMethod::constants_offset()));</span>
<span class="line-removed">3423   __ ldr(r0, Address(r0, ConstantPool::pool_holder_offset_in_bytes()));</span>
3424   __ ldrw(rmethod, Address(rmethod, Method::itable_index_offset()));
3425   __ subw(rmethod, rmethod, Method::itable_index_max);
3426   __ negw(rmethod, rmethod);
3427 
3428   // Preserve recvKlass for throw_AbstractMethodErrorVerbose.
3429   __ mov(rlocals, r3);
3430   __ lookup_interface_method(// inputs: rec. class, interface, itable index
3431                              rlocals, r0, rmethod,
3432                              // outputs: method, scan temp. reg
3433                              rmethod, r13,
3434                              no_such_interface);
3435 
3436   // rmethod,: methodOop to call
3437   // r2: receiver
3438   // Check for abstract method error
3439   // Note: This should be done more efficiently via a throw_abstract_method_error
3440   //       interpreter entry point and a conditional jump to it in case of a null
3441   //       method.
3442   __ cbz(rmethod, no_such_method);
3443 
</pre>
<hr />
<pre>
3596     // zero, go directly to the header initialization.
3597     __ bind(initialize_object);
3598     __ sub(r3, r3, sizeof(oopDesc));
3599     __ cbz(r3, initialize_header);
3600 
3601     // Initialize object fields
3602     {
3603       __ add(r2, r0, sizeof(oopDesc));
3604       Label loop;
3605       __ bind(loop);
3606       __ str(zr, Address(__ post(r2, BytesPerLong)));
3607       __ sub(r3, r3, BytesPerLong);
3608       __ cbnz(r3, loop);
3609     }
3610 
3611     // initialize object header only.
3612     __ bind(initialize_header);
3613     if (UseBiasedLocking) {
3614       __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));
3615     } else {
<span class="line-modified">3616       __ mov(rscratch1, (intptr_t)markOopDesc::prototype());</span>
3617     }
3618     __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));
3619     __ store_klass_gap(r0, zr);  // zero klass gap for compressed oops
3620     __ store_klass(r0, r4);      // store klass last
3621 
3622     {
3623       SkipIfEqual skip(_masm, &amp;DTraceAllocProbes, false);
3624       // Trigger dtrace event for fastpath
3625       __ push(atos); // save the return value
3626       __ call_VM_leaf(
3627            CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), r0);
3628       __ pop(atos); // restore the return value
3629 
3630     }
3631     __ b(done);
3632   }
3633 
3634   // slow case
3635   __ bind(slow_case);
3636   __ get_constant_pool(c_rarg1);
</pre>
</td>
<td>
<hr />
<pre>
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  29 #include &quot;interpreter/interpreter.hpp&quot;
  30 #include &quot;interpreter/interpreterRuntime.hpp&quot;
  31 #include &quot;interpreter/interp_masm.hpp&quot;
  32 #include &quot;interpreter/templateTable.hpp&quot;
  33 #include &quot;memory/universe.hpp&quot;
  34 #include &quot;oops/methodData.hpp&quot;
  35 #include &quot;oops/method.hpp&quot;
  36 #include &quot;oops/objArrayKlass.hpp&quot;
  37 #include &quot;oops/oop.inline.hpp&quot;
  38 #include &quot;prims/methodHandles.hpp&quot;
  39 #include &quot;runtime/frame.inline.hpp&quot;
  40 #include &quot;runtime/sharedRuntime.hpp&quot;
  41 #include &quot;runtime/stubRoutines.hpp&quot;
  42 #include &quot;runtime/synchronizer.hpp&quot;
<span class="line-added">  43 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  44 
  45 #define __ _masm-&gt;
  46 
  47 // Platform-dependent initialization
  48 
  49 void TemplateTable::pd_initialize() {
  50   // No aarch64 specific initialization
  51 }
  52 
  53 // Address computation: local variables
  54 
  55 static inline Address iaddress(int n) {
  56   return Address(rlocals, Interpreter::local_offset_in_bytes(n));
  57 }
  58 
  59 static inline Address laddress(int n) {
  60   return iaddress(n + 1);
  61 }
  62 
  63 static inline Address faddress(int n) {
</pre>
<hr />
<pre>
1462   case add:
1463     // n.b. use ldrd because this is a 64 bit slot
1464     __ pop_f(v1);
1465     __ fadds(v0, v1, v0);
1466     break;
1467   case sub:
1468     __ pop_f(v1);
1469     __ fsubs(v0, v1, v0);
1470     break;
1471   case mul:
1472     __ pop_f(v1);
1473     __ fmuls(v0, v1, v0);
1474     break;
1475   case div:
1476     __ pop_f(v1);
1477     __ fdivs(v0, v1, v0);
1478     break;
1479   case rem:
1480     __ fmovs(v1, v0);
1481     __ pop_f(v0);
<span class="line-modified">1482     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::frem));</span>

1483     break;
1484   default:
1485     ShouldNotReachHere();
1486     break;
1487   }
1488 }
1489 
1490 void TemplateTable::dop2(Operation op)
1491 {
1492   transition(dtos, dtos);
1493   switch (op) {
1494   case add:
1495     // n.b. use ldrd because this is a 64 bit slot
1496     __ pop_d(v1);
1497     __ faddd(v0, v1, v0);
1498     break;
1499   case sub:
1500     __ pop_d(v1);
1501     __ fsubd(v0, v1, v0);
1502     break;
1503   case mul:
1504     __ pop_d(v1);
1505     __ fmuld(v0, v1, v0);
1506     break;
1507   case div:
1508     __ pop_d(v1);
1509     __ fdivd(v0, v1, v0);
1510     break;
1511   case rem:
1512     __ fmovd(v1, v0);
1513     __ pop_d(v0);
<span class="line-modified">1514     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::drem));</span>

1515     break;
1516   default:
1517     ShouldNotReachHere();
1518     break;
1519   }
1520 }
1521 
1522 void TemplateTable::ineg()
1523 {
1524   transition(itos, itos);
1525   __ negw(r0, r0);
1526 
1527 }
1528 
1529 void TemplateTable::lneg()
1530 {
1531   transition(ltos, ltos);
1532   __ neg(r0, r0);
1533 }
1534 
</pre>
<hr />
<pre>
1635     break;
1636   case Bytecodes::_i2s:
1637     __ sxthw(r0, r0);
1638     break;
1639   case Bytecodes::_l2i:
1640     __ uxtw(r0, r0);
1641     break;
1642   case Bytecodes::_l2f:
1643     __ scvtfs(v0, r0);
1644     break;
1645   case Bytecodes::_l2d:
1646     __ scvtfd(v0, r0);
1647     break;
1648   case Bytecodes::_f2i:
1649   {
1650     Label L_Okay;
1651     __ clear_fpsr();
1652     __ fcvtzsw(r0, v0);
1653     __ get_fpsr(r1);
1654     __ cbzw(r1, L_Okay);
<span class="line-modified">1655     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::f2i));</span>

1656     __ bind(L_Okay);
1657   }
1658     break;
1659   case Bytecodes::_f2l:
1660   {
1661     Label L_Okay;
1662     __ clear_fpsr();
1663     __ fcvtzs(r0, v0);
1664     __ get_fpsr(r1);
1665     __ cbzw(r1, L_Okay);
<span class="line-modified">1666     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::f2l));</span>

1667     __ bind(L_Okay);
1668   }
1669     break;
1670   case Bytecodes::_f2d:
1671     __ fcvts(v0, v0);
1672     break;
1673   case Bytecodes::_d2i:
1674   {
1675     Label L_Okay;
1676     __ clear_fpsr();
1677     __ fcvtzdw(r0, v0);
1678     __ get_fpsr(r1);
1679     __ cbzw(r1, L_Okay);
<span class="line-modified">1680     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::d2i));</span>

1681     __ bind(L_Okay);
1682   }
1683     break;
1684   case Bytecodes::_d2l:
1685   {
1686     Label L_Okay;
1687     __ clear_fpsr();
1688     __ fcvtzd(r0, v0);
1689     __ get_fpsr(r1);
1690     __ cbzw(r1, L_Okay);
<span class="line-modified">1691     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::d2l));</span>

1692     __ bind(L_Okay);
1693   }
1694     break;
1695   case Bytecodes::_d2f:
1696     __ fcvtd(v0, v0);
1697     break;
1698   default:
1699     ShouldNotReachHere();
1700   }
1701 }
1702 
1703 void TemplateTable::lcmp()
1704 {
1705   transition(ltos, itos);
1706   Label done;
1707   __ pop_l(r1);
1708   __ cmp(r1, r0);
1709   __ mov(r0, (u_int64_t)-1L);
1710   __ br(Assembler::LT, done);
1711   // __ mov(r0, 1UL);
</pre>
<hr />
<pre>
2301 //     memory refs that happen BEFORE the write float down to after the
2302 //     write.  It&#39;s OK for non-volatile memory refs that happen after the
2303 //     volatile write to float up before it.
2304 //
2305 // We only put in barriers around volatile refs (they are expensive),
2306 // not _between_ memory refs (that would require us to track the
2307 // flavor of the previous memory refs).  Requirements (2) and (3)
2308 // require some barriers before volatile stores and after volatile
2309 // loads.  These nearly cover requirement (1) but miss the
2310 // volatile-store-volatile-load case.  This final case is placed after
2311 // volatile-stores although it could just as well go before
2312 // volatile-loads.
2313 
2314 void TemplateTable::resolve_cache_and_index(int byte_no,
2315                                             Register Rcache,
2316                                             Register index,
2317                                             size_t index_size) {
2318   const Register temp = r19;
2319   assert_different_registers(Rcache, index, temp);
2320 
<span class="line-modified">2321   Label resolved, clinit_barrier_slow;</span>
2322 
2323   Bytecodes::Code code = bytecode();
2324   switch (code) {
2325   case Bytecodes::_nofast_getfield: code = Bytecodes::_getfield; break;
2326   case Bytecodes::_nofast_putfield: code = Bytecodes::_putfield; break;
2327   default: break;
2328   }
2329 
2330   assert(byte_no == f1_byte || byte_no == f2_byte, &quot;byte_no out of range&quot;);
2331   __ get_cache_and_index_and_bytecode_at_bcp(Rcache, index, temp, byte_no, 1, index_size);
2332   __ subs(zr, temp, (int) code);  // have we resolved this bytecode?
2333   __ br(Assembler::EQ, resolved);
2334 
2335   // resolve first time through
<span class="line-added">2336   // Class initialization barrier slow path lands here as well.</span>
<span class="line-added">2337   __ bind(clinit_barrier_slow);</span>
2338   address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache);
2339   __ mov(temp, (int) code);
2340   __ call_VM(noreg, entry, temp);
2341 
2342   // Update registers with resolved info
2343   __ get_cache_and_index_at_bcp(Rcache, index, 1, index_size);
2344   // n.b. unlike x86 Rcache is now rcpool plus the indexed offset
2345   // so all clients ofthis method must be modified accordingly
2346   __ bind(resolved);
<span class="line-added">2347 </span>
<span class="line-added">2348   // Class initialization barrier for static methods</span>
<span class="line-added">2349   if (VM_Version::supports_fast_class_init_checks() &amp;&amp; bytecode() == Bytecodes::_invokestatic) {</span>
<span class="line-added">2350     __ load_resolved_method_at_index(byte_no, temp, Rcache);</span>
<span class="line-added">2351     __ load_method_holder(temp, temp);</span>
<span class="line-added">2352     __ clinit_barrier(temp, rscratch1, NULL, &amp;clinit_barrier_slow);</span>
<span class="line-added">2353   }</span>
2354 }
2355 
2356 // The Rcache and index registers must be set before call
2357 // n.b unlike x86 cache already includes the index offset
2358 void TemplateTable::load_field_cp_cache_entry(Register obj,
2359                                               Register cache,
2360                                               Register index,
2361                                               Register off,
2362                                               Register flags,
2363                                               bool is_static = false) {
2364   assert_different_registers(cache, index, flags, off);
2365 
2366   ByteSize cp_base_offset = ConstantPoolCache::base_offset();
2367   // Field offset
2368   __ ldr(off, Address(cache, in_bytes(cp_base_offset +
2369                                           ConstantPoolCacheEntry::f2_offset())));
2370   // Flags
2371   __ ldrw(flags, Address(cache, in_bytes(cp_base_offset +
2372                                            ConstantPoolCacheEntry::flags_offset())));
2373 
</pre>
<hr />
<pre>
2872     __ pop(dtos);
2873     if (!is_static) pop_and_check_object(obj);
2874     __ access_store_at(T_DOUBLE, IN_HEAP, field, noreg /* dtos */, noreg, noreg);
2875     if (rc == may_rewrite) {
2876       patch_bytecode(Bytecodes::_fast_dputfield, bc, r1, true, byte_no);
2877     }
2878   }
2879 
2880 #ifdef ASSERT
2881   __ b(Done);
2882 
2883   __ bind(notDouble);
2884   __ stop(&quot;Bad state&quot;);
2885 #endif
2886 
2887   __ bind(Done);
2888 
2889   {
2890     Label notVolatile;
2891     __ tbz(r5, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
<span class="line-modified">2892     __ membar(MacroAssembler::StoreLoad | MacroAssembler::StoreStore);</span>
2893     __ bind(notVolatile);
2894   }
2895 }
2896 
2897 void TemplateTable::putfield(int byte_no)
2898 {
2899   putfield_or_static(byte_no, false);
2900 }
2901 
2902 void TemplateTable::nofast_putfield(int byte_no) {
2903   putfield_or_static(byte_no, false, may_not_rewrite);
2904 }
2905 
2906 void TemplateTable::putstatic(int byte_no) {
2907   putfield_or_static(byte_no, true);
2908 }
2909 
2910 void TemplateTable::jvmti_post_fast_field_mod()
2911 {
2912   if (JvmtiExport::can_post_field_modification()) {
</pre>
<hr />
<pre>
3016     break;
3017   case Bytecodes::_fast_sputfield:
3018     __ access_store_at(T_SHORT, IN_HEAP, field, r0, noreg, noreg);
3019     break;
3020   case Bytecodes::_fast_cputfield:
3021     __ access_store_at(T_CHAR, IN_HEAP, field, r0, noreg, noreg);
3022     break;
3023   case Bytecodes::_fast_fputfield:
3024     __ access_store_at(T_FLOAT, IN_HEAP, field, noreg /* ftos */, noreg, noreg);
3025     break;
3026   case Bytecodes::_fast_dputfield:
3027     __ access_store_at(T_DOUBLE, IN_HEAP, field, noreg /* dtos */, noreg, noreg);
3028     break;
3029   default:
3030     ShouldNotReachHere();
3031   }
3032 
3033   {
3034     Label notVolatile;
3035     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
<span class="line-modified">3036     __ membar(MacroAssembler::StoreLoad | MacroAssembler::StoreStore);</span>
3037     __ bind(notVolatile);
3038   }
3039 }
3040 
3041 
3042 void TemplateTable::fast_accessfield(TosState state)
3043 {
3044   transition(atos, state);
3045   // Do the JVMTI work here to avoid disturbing the register state below
3046   if (JvmtiExport::can_post_field_access()) {
3047     // Check to see if a field access watch has been set before we
3048     // take the time to call into the VM.
3049     Label L1;
3050     __ lea(rscratch1, ExternalAddress((address) JvmtiExport::get_field_access_count_addr()));
3051     __ ldrw(r2, Address(rscratch1));
3052     __ cbzw(r2, L1);
3053     // access constant pool cache entry
3054     __ get_cache_entry_pointer_at_bcp(c_rarg2, rscratch2, 1);
3055     __ verify_oop(r0);
3056     __ push_ptr(r0);  // save object pointer before call_VM() clobbers it
</pre>
<hr />
<pre>
3405   __ null_check(r2, oopDesc::klass_offset_in_bytes());
3406   __ load_klass(r3, r2);
3407 
3408   Label no_such_method;
3409 
3410   // Preserve method for throw_AbstractMethodErrorVerbose.
3411   __ mov(r16, rmethod);
3412   // Receiver subtype check against REFC.
3413   // Superklass in r0. Subklass in r3. Blows rscratch2, r13
3414   __ lookup_interface_method(// inputs: rec. class, interface, itable index
3415                              r3, r0, noreg,
3416                              // outputs: scan temp. reg, scan temp. reg
3417                              rscratch2, r13,
3418                              no_such_interface,
3419                              /*return_method=*/false);
3420 
3421   // profile this call
3422   __ profile_virtual_call(r3, r13, r19);
3423 
3424   // Get declaring interface class from method, and itable index
<span class="line-modified">3425 </span>
<span class="line-modified">3426   __ load_method_holder(r0, rmethod);</span>

3427   __ ldrw(rmethod, Address(rmethod, Method::itable_index_offset()));
3428   __ subw(rmethod, rmethod, Method::itable_index_max);
3429   __ negw(rmethod, rmethod);
3430 
3431   // Preserve recvKlass for throw_AbstractMethodErrorVerbose.
3432   __ mov(rlocals, r3);
3433   __ lookup_interface_method(// inputs: rec. class, interface, itable index
3434                              rlocals, r0, rmethod,
3435                              // outputs: method, scan temp. reg
3436                              rmethod, r13,
3437                              no_such_interface);
3438 
3439   // rmethod,: methodOop to call
3440   // r2: receiver
3441   // Check for abstract method error
3442   // Note: This should be done more efficiently via a throw_abstract_method_error
3443   //       interpreter entry point and a conditional jump to it in case of a null
3444   //       method.
3445   __ cbz(rmethod, no_such_method);
3446 
</pre>
<hr />
<pre>
3599     // zero, go directly to the header initialization.
3600     __ bind(initialize_object);
3601     __ sub(r3, r3, sizeof(oopDesc));
3602     __ cbz(r3, initialize_header);
3603 
3604     // Initialize object fields
3605     {
3606       __ add(r2, r0, sizeof(oopDesc));
3607       Label loop;
3608       __ bind(loop);
3609       __ str(zr, Address(__ post(r2, BytesPerLong)));
3610       __ sub(r3, r3, BytesPerLong);
3611       __ cbnz(r3, loop);
3612     }
3613 
3614     // initialize object header only.
3615     __ bind(initialize_header);
3616     if (UseBiasedLocking) {
3617       __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));
3618     } else {
<span class="line-modified">3619       __ mov(rscratch1, (intptr_t)markWord::prototype().value());</span>
3620     }
3621     __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));
3622     __ store_klass_gap(r0, zr);  // zero klass gap for compressed oops
3623     __ store_klass(r0, r4);      // store klass last
3624 
3625     {
3626       SkipIfEqual skip(_masm, &amp;DTraceAllocProbes, false);
3627       // Trigger dtrace event for fastpath
3628       __ push(atos); // save the return value
3629       __ call_VM_leaf(
3630            CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), r0);
3631       __ pop(atos); // restore the return value
3632 
3633     }
3634     __ b(done);
3635   }
3636 
3637   // slow case
3638   __ bind(slow_case);
3639   __ get_constant_pool(c_rarg1);
</pre>
</td>
</tr>
</table>
<center><a href="templateInterpreterGenerator_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="vm_version_aarch64.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>