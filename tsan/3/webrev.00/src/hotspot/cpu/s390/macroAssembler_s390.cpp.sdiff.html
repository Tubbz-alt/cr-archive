<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/s390/macroAssembler_s390.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="jniFastGetField_s390.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_s390.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/s390/macroAssembler_s390.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2016, 2018, Oracle and/or its affiliates. All rights reserved.</span>
<span class="line-modified">   3  * Copyright (c) 2016, 2018, SAP SE. All rights reserved.</span>
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/codeBuffer.hpp&quot;
  28 #include &quot;asm/macroAssembler.inline.hpp&quot;
  29 #include &quot;compiler/disassembler.hpp&quot;
  30 #include &quot;gc/shared/barrierSet.hpp&quot;
  31 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  32 #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
  33 #include &quot;interpreter/interpreter.hpp&quot;
  34 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
  35 #include &quot;memory/resourceArea.hpp&quot;
  36 #include &quot;memory/universe.hpp&quot;
  37 #include &quot;oops/accessDecorators.hpp&quot;
  38 #include &quot;oops/compressedOops.inline.hpp&quot;
  39 #include &quot;oops/klass.inline.hpp&quot;

  40 #include &quot;opto/compile.hpp&quot;
  41 #include &quot;opto/intrinsicnode.hpp&quot;
  42 #include &quot;opto/matcher.hpp&quot;

  43 #include &quot;prims/methodHandles.hpp&quot;
  44 #include &quot;registerSaver_s390.hpp&quot;
  45 #include &quot;runtime/biasedLocking.hpp&quot;
  46 #include &quot;runtime/icache.hpp&quot;
  47 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  48 #include &quot;runtime/objectMonitor.hpp&quot;
  49 #include &quot;runtime/os.hpp&quot;
  50 #include &quot;runtime/safepoint.hpp&quot;
  51 #include &quot;runtime/safepointMechanism.hpp&quot;
  52 #include &quot;runtime/sharedRuntime.hpp&quot;
  53 #include &quot;runtime/stubRoutines.hpp&quot;
  54 #include &quot;utilities/events.hpp&quot;
  55 #include &quot;utilities/macros.hpp&quot;

  56 
  57 #include &lt;ucontext.h&gt;
  58 
  59 #define BLOCK_COMMENT(str) block_comment(str)
  60 #define BIND(label)        bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  61 
  62 // Move 32-bit register if destination and source are different.
  63 void MacroAssembler::lr_if_needed(Register rd, Register rs) {
  64   if (rs != rd) { z_lr(rd, rs); }
  65 }
  66 
  67 // Move register if destination and source are different.
  68 void MacroAssembler::lgr_if_needed(Register rd, Register rs) {
  69   if (rs != rd) { z_lgr(rd, rs); }
  70 }
  71 
  72 // Zero-extend 32-bit register into 64-bit register if destination and source are different.
  73 void MacroAssembler::llgfr_if_needed(Register rd, Register rs) {
  74   if (rs != rd) { z_llgfr(rd, rs); }
  75 }
</pre>
<hr />
<pre>
1154 }
1155 
1156 // Load a 32bit constant into a 64bit register, sign-extend or zero-extend.
1157 // Patchable code sequence, but not atomically patchable.
1158 // Make sure to keep code size constant -&gt; no value-dependent optimizations.
1159 // Do not kill condition code.
1160 void MacroAssembler::load_const_32to64(Register t, int64_t x, bool sign_extend) {
1161   if (sign_extend) { Assembler::z_lgfi(t, x); }
1162   else             { Assembler::z_llilf(t, x); }
1163 }
1164 
1165 // Load narrow oop constant, no decompression.
1166 void MacroAssembler::load_narrow_oop(Register t, narrowOop a) {
1167   assert(UseCompressedOops, &quot;must be on to call this method&quot;);
1168   load_const_32to64(t, a, false /*sign_extend*/);
1169 }
1170 
1171 // Load narrow klass constant, compression required.
1172 void MacroAssembler::load_narrow_klass(Register t, Klass* k) {
1173   assert(UseCompressedClassPointers, &quot;must be on to call this method&quot;);
<span class="line-modified">1174   narrowKlass encoded_k = Klass::encode_klass(k);</span>
1175   load_const_32to64(t, encoded_k, false /*sign_extend*/);
1176 }
1177 
1178 //------------------------------------------------------
1179 //  Compare (patchable) constant with register.
1180 //------------------------------------------------------
1181 
1182 // Compare narrow oop in reg with narrow oop constant, no decompression.
1183 void MacroAssembler::compare_immediate_narrow_oop(Register oop1, narrowOop oop2) {
1184   assert(UseCompressedOops, &quot;must be on to call this method&quot;);
1185 
1186   Assembler::z_clfi(oop1, oop2);
1187 }
1188 
1189 // Compare narrow oop in reg with narrow oop constant, no decompression.
1190 void MacroAssembler::compare_immediate_narrow_klass(Register klass1, Klass* klass2) {
1191   assert(UseCompressedClassPointers, &quot;must be on to call this method&quot;);
<span class="line-modified">1192   narrowKlass encoded_k = Klass::encode_klass(klass2);</span>
1193 
1194   Assembler::z_clfi(klass1, encoded_k);
1195 }
1196 
1197 //----------------------------------------------------------
1198 //  Check which kind of load_constant we have here.
1199 //----------------------------------------------------------
1200 
1201 // Detection of CPU version dependent load_const sequence.
1202 // The detection is valid only for code sequences generated by load_const,
1203 // not load_const_optimized.
1204 bool MacroAssembler::is_load_const(address a) {
1205   unsigned long inst1, inst2;
1206   unsigned int  len1,  len2;
1207 
1208   len1 = get_instruction(a, &amp;inst1);
1209   len2 = get_instruction(a + len1, &amp;inst2);
1210 
1211   return is_z_iihf(inst1) &amp;&amp; is_z_iilf(inst2);
1212 }
</pre>
<hr />
<pre>
1268   assert(is_compare_immediate32(pos), &quot;not a compressed ptr compare&quot;);
1269 
1270   set_imm32(pos, np);
1271   return 6;
1272 }
1273 
1274 // Patching the immediate value of CPU version dependent load_narrow_oop sequence.
1275 // The passed ptr must NOT be in compressed format!
1276 int MacroAssembler::patch_load_narrow_oop(address pos, oop o) {
1277   assert(UseCompressedOops, &quot;Can only patch compressed oops&quot;);
1278 
1279   narrowOop no = CompressedOops::encode(o);
1280   return patch_load_const_32to64(pos, no);
1281 }
1282 
1283 // Patching the immediate value of CPU version dependent load_narrow_klass sequence.
1284 // The passed ptr must NOT be in compressed format!
1285 int MacroAssembler::patch_load_narrow_klass(address pos, Klass* k) {
1286   assert(UseCompressedClassPointers, &quot;Can only patch compressed klass pointers&quot;);
1287 
<span class="line-modified">1288   narrowKlass nk = Klass::encode_klass(k);</span>
1289   return patch_load_const_32to64(pos, nk);
1290 }
1291 
1292 // Patching the immediate value of CPU version dependent compare_immediate_narrow_oop sequence.
1293 // The passed ptr must NOT be in compressed format!
1294 int MacroAssembler::patch_compare_immediate_narrow_oop(address pos, oop o) {
1295   assert(UseCompressedOops, &quot;Can only patch compressed oops&quot;);
1296 
1297   narrowOop no = CompressedOops::encode(o);
1298   return patch_compare_immediate_32(pos, no);
1299 }
1300 
1301 // Patching the immediate value of CPU version dependent compare_immediate_narrow_klass sequence.
1302 // The passed ptr must NOT be in compressed format!
1303 int MacroAssembler::patch_compare_immediate_narrow_klass(address pos, Klass* k) {
1304   assert(UseCompressedClassPointers, &quot;Can only patch compressed klass pointers&quot;);
1305 
<span class="line-modified">1306   narrowKlass nk = Klass::encode_klass(k);</span>
1307   return patch_compare_immediate_32(pos, nk);
1308 }
1309 
1310 //------------------------------------------------------------------------
1311 //  Extract the constant from a load_constant instruction stream.
1312 //------------------------------------------------------------------------
1313 
1314 // Get constant from a load_const sequence.
1315 long MacroAssembler::get_const(address a) {
1316   assert(is_load_const(a), &quot;not a load of a constant&quot;);
1317   unsigned long x;
1318   x =  (((unsigned long) (get_imm32(a,0) &amp; 0xffffffff)) &lt;&lt; 32);
1319   x |= (((unsigned long) (get_imm32(a,1) &amp; 0xffffffff)));
1320   return (long) x;
1321 }
1322 
1323 //--------------------------------------
1324 //  Store a constant in memory.
1325 //--------------------------------------
1326 
</pre>
<hr />
<pre>
2908   AddressLiteral icmiss(SharedRuntime::get_ic_miss_stub());
2909 
2910   load_const_optimized(scratch, icmiss);
2911   z_br(scratch);
2912 
2913   // Fill unused space.
2914   if (requiredSize &gt; 0) {
2915     while ((offset() - startOffset) &lt; requiredSize) {
2916       if (trapMarker == 0) {
2917         z_nop();
2918       } else {
2919         z_illtrap(trapMarker);
2920       }
2921     }
2922   }
2923   BLOCK_COMMENT(&quot;} IC miss handler&quot;);
2924   return labelOffset;
2925 }
2926 
2927 void MacroAssembler::nmethod_UEP(Label&amp; ic_miss) {
<span class="line-modified">2928   Register ic_reg       = as_Register(Matcher::inline_cache_reg_encode());</span>
2929   int      klass_offset = oopDesc::klass_offset_in_bytes();
2930   if (!ImplicitNullChecks || MacroAssembler::needs_explicit_null_check(klass_offset)) {
2931     if (VM_Version::has_CompareBranch()) {
2932       z_cgij(Z_ARG1, 0, Assembler::bcondEqual, ic_miss);
2933     } else {
2934       z_ltgr(Z_ARG1, Z_ARG1);
2935       z_bre(ic_miss);
2936     }
2937   }
2938   // Compare cached class against klass from receiver.
2939   compare_klass_ptr(ic_reg, klass_offset, Z_ARG1, false);
2940   z_brne(ic_miss);
2941 }
2942 
2943 void MacroAssembler::check_klass_subtype_fast_path(Register   sub_klass,
2944                                                    Register   super_klass,
2945                                                    Register   temp1_reg,
2946                                                    Label*     L_success,
2947                                                    Label*     L_failure,
2948                                                    Label*     L_slow_path,
</pre>
<hr />
<pre>
3113 #undef final_jmp
3114   BLOCK_COMMENT(&quot;} check_klass_subtype_slow_path&quot;);
3115 }
3116 
3117 // Emitter for combining fast and slow path.
3118 void MacroAssembler::check_klass_subtype(Register sub_klass,
3119                                          Register super_klass,
3120                                          Register temp1_reg,
3121                                          Register temp2_reg,
3122                                          Label&amp;   L_success) {
3123   NearLabel failure;
3124   BLOCK_COMMENT(err_msg(&quot;check_klass_subtype(%s subclass of %s) {&quot;, sub_klass-&gt;name(), super_klass-&gt;name()));
3125   check_klass_subtype_fast_path(sub_klass, super_klass, temp1_reg,
3126                                 &amp;L_success, &amp;failure, NULL);
3127   check_klass_subtype_slow_path(sub_klass, super_klass,
3128                                 temp1_reg, temp2_reg, &amp;L_success, NULL);
3129   BIND(failure);
3130   BLOCK_COMMENT(&quot;} check_klass_subtype&quot;);
3131 }
3132 



























3133 // Increment a counter at counter_address when the eq condition code is
3134 // set. Kills registers tmp1_reg and tmp2_reg and preserves the condition code.
3135 void MacroAssembler::increment_counter_eq(address counter_address, Register tmp1_reg, Register tmp2_reg) {
3136   Label l;
3137   z_brne(l);
3138   load_const(tmp1_reg, counter_address);
3139   add2mem_32(Address(tmp1_reg), 1, tmp2_reg);
3140   z_cr(tmp1_reg, tmp1_reg); // Set cc to eq.
3141   bind(l);
3142 }
3143 
3144 // Semantics are dependent on the slow_case label:
3145 //   If the slow_case label is not NULL, failure to biased-lock the object
3146 //   transfers control to the location of the slow_case label. If the
3147 //   object could be biased-locked, control is transferred to the done label.
3148 //   The condition code is unpredictable.
3149 //
3150 //   If the slow_case label is NULL, failure to biased-lock the object results
3151 //   in a transfer of control to the done label with a condition code of not_equal.
3152 //   If the biased-lock could be successfully obtained, control is transfered to
</pre>
<hr />
<pre>
3154 //   It is mandatory to react on the condition code At the done label.
3155 //
3156 void MacroAssembler::biased_locking_enter(Register  obj_reg,
3157                                           Register  mark_reg,
3158                                           Register  temp_reg,
3159                                           Register  temp2_reg,    // May be Z_RO!
3160                                           Label    &amp;done,
3161                                           Label    *slow_case) {
3162   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
3163   assert_different_registers(obj_reg, mark_reg, temp_reg, temp2_reg);
3164 
3165   Label cas_label; // Try, if implemented, CAS locking. Fall thru to slow path otherwise.
3166 
3167   BLOCK_COMMENT(&quot;biased_locking_enter {&quot;);
3168 
3169   // Biased locking
3170   // See whether the lock is currently biased toward our thread and
3171   // whether the epoch is still valid.
3172   // Note that the runtime guarantees sufficient alignment of JavaThread
3173   // pointers to allow age to be placed into low bits.
<span class="line-modified">3174   assert(markOopDesc::age_shift == markOopDesc::lock_bits + markOopDesc::biased_lock_bits,</span>
3175          &quot;biased locking makes assumptions about bit layout&quot;);
3176   z_lr(temp_reg, mark_reg);
<span class="line-modified">3177   z_nilf(temp_reg, markOopDesc::biased_lock_mask_in_place);</span>
<span class="line-modified">3178   z_chi(temp_reg, markOopDesc::biased_lock_pattern);</span>
3179   z_brne(cas_label);  // Try cas if object is not biased, i.e. cannot be biased locked.
3180 
3181   load_prototype_header(temp_reg, obj_reg);
<span class="line-modified">3182   load_const_optimized(temp2_reg, ~((int) markOopDesc::age_mask_in_place));</span>
3183 
3184   z_ogr(temp_reg, Z_thread);
3185   z_xgr(temp_reg, mark_reg);
3186   z_ngr(temp_reg, temp2_reg);
3187   if (PrintBiasedLockingStatistics) {
3188     increment_counter_eq((address) BiasedLocking::biased_lock_entry_count_addr(), mark_reg, temp2_reg);
3189     // Restore mark_reg.
3190     z_lg(mark_reg, oopDesc::mark_offset_in_bytes(), obj_reg);
3191   }
3192   branch_optimized(Assembler::bcondEqual, done);  // Biased lock obtained, return success.
3193 
3194   Label try_revoke_bias;
3195   Label try_rebias;
3196   Address mark_addr = Address(obj_reg, oopDesc::mark_offset_in_bytes());
3197 
3198   //----------------------------------------------------------------------------
3199   // At this point we know that the header has the bias pattern and
3200   // that we are not the bias owner in the current epoch. We need to
3201   // figure out more details about the state of the header in order to
3202   // know what operations can be legally performed on the object&#39;s
3203   // header.
3204 
3205   // If the low three bits in the xor result aren&#39;t clear, that means
3206   // the prototype header is no longer biased and we have to revoke
3207   // the bias on this object.
<span class="line-modified">3208   z_tmll(temp_reg, markOopDesc::biased_lock_mask_in_place);</span>
3209   z_brnaz(try_revoke_bias);
3210 
3211   // Biasing is still enabled for this data type. See whether the
3212   // epoch of the current bias is still valid, meaning that the epoch
3213   // bits of the mark word are equal to the epoch bits of the
3214   // prototype header. (Note that the prototype header&#39;s epoch bits
3215   // only change at a safepoint.) If not, attempt to rebias the object
3216   // toward the current thread. Note that we must be absolutely sure
3217   // that the current epoch is invalid in order to do this because
3218   // otherwise the manipulations it performs on the mark word are
3219   // illegal.
<span class="line-modified">3220   z_tmll(temp_reg, markOopDesc::epoch_mask_in_place);</span>
3221   z_brnaz(try_rebias);
3222 
3223   //----------------------------------------------------------------------------
3224   // The epoch of the current bias is still valid but we know nothing
3225   // about the owner; it might be set or it might be clear. Try to
3226   // acquire the bias of the object using an atomic operation. If this
3227   // fails we will go in to the runtime to revoke the object&#39;s bias.
3228   // Note that we first construct the presumed unbiased header so we
3229   // don&#39;t accidentally blow away another thread&#39;s valid bias.
<span class="line-modified">3230   z_nilf(mark_reg, markOopDesc::biased_lock_mask_in_place | markOopDesc::age_mask_in_place |</span>
<span class="line-modified">3231          markOopDesc::epoch_mask_in_place);</span>
3232   z_lgr(temp_reg, Z_thread);
3233   z_llgfr(mark_reg, mark_reg);
3234   z_ogr(temp_reg, mark_reg);
3235 
3236   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
3237 
3238   z_csg(mark_reg, temp_reg, 0, obj_reg);
3239 
3240   // If the biasing toward our thread failed, this means that
3241   // another thread succeeded in biasing it toward itself and we
3242   // need to revoke that bias. The revocation will occur in the
3243   // interpreter runtime in the slow case.
3244 
3245   if (PrintBiasedLockingStatistics) {
3246     increment_counter_eq((address) BiasedLocking::anonymously_biased_lock_entry_count_addr(),
3247                          temp_reg, temp2_reg);
3248   }
3249   if (slow_case != NULL) {
3250     branch_optimized(Assembler::bcondNotEqual, *slow_case); // Biased lock not obtained, need to go the long way.
3251   }
3252   branch_optimized(Assembler::bcondAlways, done);           // Biased lock status given in condition code.
3253 
3254   //----------------------------------------------------------------------------
3255   bind(try_rebias);
3256   // At this point we know the epoch has expired, meaning that the
3257   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
3258   // circumstances _only_, we are allowed to use the current header&#39;s
3259   // value as the comparison value when doing the cas to acquire the
3260   // bias in the current epoch. In other words, we allow transfer of
3261   // the bias from one thread to another directly in this situation.
3262 
<span class="line-modified">3263   z_nilf(mark_reg, markOopDesc::biased_lock_mask_in_place | markOopDesc::age_mask_in_place | markOopDesc::epoch_mask_in_place);</span>
3264   load_prototype_header(temp_reg, obj_reg);
3265   z_llgfr(mark_reg, mark_reg);
3266 
3267   z_ogr(temp_reg, Z_thread);
3268 
3269   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
3270 
3271   z_csg(mark_reg, temp_reg, 0, obj_reg);
3272 
3273   // If the biasing toward our thread failed, this means that
3274   // another thread succeeded in biasing it toward itself and we
3275   // need to revoke that bias. The revocation will occur in the
3276   // interpreter runtime in the slow case.
3277 
3278   if (PrintBiasedLockingStatistics) {
3279     increment_counter_eq((address) BiasedLocking::rebiased_lock_entry_count_addr(), temp_reg, temp2_reg);
3280   }
3281   if (slow_case != NULL) {
3282     branch_optimized(Assembler::bcondNotEqual, *slow_case);  // Biased lock not obtained, need to go the long way.
3283   }
</pre>
<hr />
<pre>
3304   // removing the bias bit from the object&#39;s header.
3305   if (PrintBiasedLockingStatistics) {
3306     // z_cgr(mark_reg, temp2_reg);
3307     increment_counter_eq((address) BiasedLocking::revoked_lock_entry_count_addr(), temp_reg, temp2_reg);
3308   }
3309 
3310   bind(cas_label);
3311   BLOCK_COMMENT(&quot;} biased_locking_enter&quot;);
3312 }
3313 
3314 void MacroAssembler::biased_locking_exit(Register mark_addr, Register temp_reg, Label&amp; done) {
3315   // Check for biased locking unlock case, which is a no-op
3316   // Note: we do not have to check the thread ID for two reasons.
3317   // First, the interpreter checks for IllegalMonitorStateException at
3318   // a higher level. Second, if the bias was revoked while we held the
3319   // lock, the object could not be rebiased toward another thread, so
3320   // the bias bit would be clear.
3321   BLOCK_COMMENT(&quot;biased_locking_exit {&quot;);
3322 
3323   z_lg(temp_reg, 0, mark_addr);
<span class="line-modified">3324   z_nilf(temp_reg, markOopDesc::biased_lock_mask_in_place);</span>
3325 
<span class="line-modified">3326   z_chi(temp_reg, markOopDesc::biased_lock_pattern);</span>
3327   z_bre(done);
3328   BLOCK_COMMENT(&quot;} biased_locking_exit&quot;);
3329 }
3330 
3331 void MacroAssembler::compiler_fast_lock_object(Register oop, Register box, Register temp1, Register temp2, bool try_bias) {
3332   Register displacedHeader = temp1;
3333   Register currentHeader = temp1;
3334   Register temp = temp2;
3335   NearLabel done, object_has_monitor;
3336 
3337   BLOCK_COMMENT(&quot;compiler_fast_lock_object {&quot;);
3338 
<span class="line-modified">3339   // Load markOop from oop into mark.</span>
3340   z_lg(displacedHeader, 0, oop);
3341 
3342   if (try_bias) {
3343     biased_locking_enter(oop, displacedHeader, temp, Z_R0, done);
3344   }
3345 
3346   // Handle existing monitor.
3347   // The object has an existing monitor iff (mark &amp; monitor_value) != 0.
<span class="line-modified">3348   guarantee(Immediate::is_uimm16(markOopDesc::monitor_value), &quot;must be half-word&quot;);</span>
3349   z_lr(temp, displacedHeader);
<span class="line-modified">3350   z_nill(temp, markOopDesc::monitor_value);</span>
3351   z_brne(object_has_monitor);
3352 
<span class="line-modified">3353   // Set mark to markOop | markOopDesc::unlocked_value.</span>
<span class="line-modified">3354   z_oill(displacedHeader, markOopDesc::unlocked_value);</span>
3355 
3356   // Load Compare Value application register.
3357 
3358   // Initialize the box (must happen before we update the object mark).
3359   z_stg(displacedHeader, BasicLock::displaced_header_offset_in_bytes(), box);
3360 
3361   // Memory Fence (in cmpxchgd)
<span class="line-modified">3362   // Compare object markOop with mark and if equal exchange scratch1 with object markOop.</span>
3363 
3364   // If the compare-and-swap succeeded, then we found an unlocked object and we
3365   // have now locked it.
3366   z_csg(displacedHeader, box, 0, oop);
3367   assert(currentHeader==displacedHeader, &quot;must be same register&quot;); // Identified two registers from z/Architecture.
3368   z_bre(done);
3369 
3370   // We did not see an unlocked object so try the fast recursive case.
3371 
3372   z_sgr(currentHeader, Z_SP);
<span class="line-modified">3373   load_const_optimized(temp, (~(os::vm_page_size()-1) | markOopDesc::lock_mask_in_place));</span>
3374 
3375   z_ngr(currentHeader, temp);
3376   //   z_brne(done);
3377   //   z_release();
3378   z_stg(currentHeader/*==0 or not 0*/, BasicLock::displaced_header_offset_in_bytes(), box);
3379 
3380   z_bru(done);
3381 
3382   Register zero = temp;
<span class="line-modified">3383   Register monitor_tagged = displacedHeader; // Tagged with markOopDesc::monitor_value.</span>
3384   bind(object_has_monitor);
3385   // The object&#39;s monitor m is unlocked iff m-&gt;owner == NULL,
3386   // otherwise m-&gt;owner may contain a thread or a stack address.
3387   //
3388   // Try to CAS m-&gt;owner from NULL to current thread.
3389   z_lghi(zero, 0);
3390   // If m-&gt;owner is null, then csg succeeds and sets m-&gt;owner=THREAD and CR=EQ.
3391   z_csg(zero, Z_thread, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner), monitor_tagged);
3392   // Store a non-null value into the box.
3393   z_stg(box, BasicLock::displaced_header_offset_in_bytes(), box);
3394 #ifdef ASSERT
3395   z_brne(done);
3396   // We&#39;ve acquired the monitor, check some invariants.
3397   // Invariant 1: _recursions should be 0.
3398   asm_assert_mem8_is_zero(OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions), monitor_tagged,
3399                           &quot;monitor-&gt;_recursions should be 0&quot;, -1);
3400   z_ltgr(zero, zero); // Set CR=EQ.
3401 #endif
3402   bind(done);
3403 
</pre>
<hr />
<pre>
3412   Register currentHeader = temp2;
3413   Register temp = temp1;
3414   Register monitor = temp2;
3415 
3416   Label done, object_has_monitor;
3417 
3418   BLOCK_COMMENT(&quot;compiler_fast_unlock_object {&quot;);
3419 
3420   if (try_bias) {
3421     biased_locking_exit(oop, currentHeader, done);
3422   }
3423 
3424   // Find the lock address and load the displaced header from the stack.
3425   // if the displaced header is zero, we have a recursive unlock.
3426   load_and_test_long(displacedHeader, Address(box, BasicLock::displaced_header_offset_in_bytes()));
3427   z_bre(done);
3428 
3429   // Handle existing monitor.
3430   // The object has an existing monitor iff (mark &amp; monitor_value) != 0.
3431   z_lg(currentHeader, oopDesc::mark_offset_in_bytes(), oop);
<span class="line-modified">3432   guarantee(Immediate::is_uimm16(markOopDesc::monitor_value), &quot;must be half-word&quot;);</span>
<span class="line-modified">3433   z_nill(currentHeader, markOopDesc::monitor_value);</span>
3434   z_brne(object_has_monitor);
3435 
3436   // Check if it is still a light weight lock, this is true if we see
<span class="line-modified">3437   // the stack address of the basicLock in the markOop of the object</span>
3438   // copy box to currentHeader such that csg does not kill it.
3439   z_lgr(currentHeader, box);
3440   z_csg(currentHeader, displacedHeader, 0, oop);
3441   z_bru(done); // Csg sets CR as desired.
3442 
3443   // Handle existing monitor.
3444   bind(object_has_monitor);
3445   z_lg(currentHeader, oopDesc::mark_offset_in_bytes(), oop);    // CurrentHeader is tagged with monitor_value set.
3446   load_and_test_long(temp, Address(currentHeader, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));
3447   z_brne(done);
3448   load_and_test_long(temp, Address(currentHeader, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
3449   z_brne(done);
3450   load_and_test_long(temp, Address(currentHeader, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));
3451   z_brne(done);
3452   load_and_test_long(temp, Address(currentHeader, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));
3453   z_brne(done);
3454   z_release();
3455   z_stg(temp/*=0*/, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner), currentHeader);
3456 
3457   bind(done);
</pre>
<hr />
<pre>
3541   // Therefore we load the PC into tmp1 and let set_last_Java_frame() save
3542   // it into the frame anchor.
3543   get_PC(tmp1);
3544   set_last_Java_frame(/*sp=*/sp, /*pc=*/tmp1, allow_relocation);
3545 }
3546 
3547 void MacroAssembler::set_thread_state(JavaThreadState new_state) {
3548   z_release();
3549 
3550   assert(Immediate::is_uimm16(_thread_max_state), &quot;enum value out of range for instruction&quot;);
3551   assert(sizeof(JavaThreadState) == sizeof(int), &quot;enum value must have base type int&quot;);
3552   store_const(Address(Z_thread, JavaThread::thread_state_offset()), new_state, Z_R0, false);
3553 }
3554 
3555 void MacroAssembler::get_vm_result(Register oop_result) {
3556   verify_thread();
3557 
3558   z_lg(oop_result, Address(Z_thread, JavaThread::vm_result_offset()));
3559   clear_mem(Address(Z_thread, JavaThread::vm_result_offset()), sizeof(void*));
3560 
<span class="line-modified">3561   verify_oop(oop_result);</span>
3562 }
3563 
3564 void MacroAssembler::get_vm_result_2(Register result) {
3565   verify_thread();
3566 
3567   z_lg(result, Address(Z_thread, JavaThread::vm_result_2_offset()));
3568   clear_mem(Address(Z_thread, JavaThread::vm_result_2_offset()), sizeof(void*));
3569 }
3570 
3571 // We require that C code which does not return a value in vm_result will
3572 // leave it undisturbed.
3573 void MacroAssembler::set_vm_result(Register oop_result) {
3574   z_stg(oop_result, Address(Z_thread, JavaThread::vm_result_offset()));
3575 }
3576 
3577 // Explicit null checks (used for method handle code).
3578 void MacroAssembler::null_check(Register reg, Register tmp, int64_t offset) {
3579   if (!ImplicitNullChecks) {
3580     NearLabel ok;
3581 
</pre>
<hr />
<pre>
3589     bind(ok);
3590   } else {
3591     if (needs_explicit_null_check((intptr_t)offset)) {
3592       // Provoke OS NULL exception if reg = NULL by
3593       // accessing M[reg] w/o changing any registers.
3594       z_lg(tmp, 0, reg);
3595     }
3596     // else
3597       // Nothing to do, (later) access of M[reg + offset]
3598       // will provoke OS NULL exception if reg = NULL.
3599   }
3600 }
3601 
3602 //-------------------------------------
3603 //  Compressed Klass Pointers
3604 //-------------------------------------
3605 
3606 // Klass oop manipulations if compressed.
3607 void MacroAssembler::encode_klass_not_null(Register dst, Register src) {
3608   Register current = (src != noreg) ? src : dst; // Klass is in dst if no src provided. (dst == src) also possible.
<span class="line-modified">3609   address  base    = Universe::narrow_klass_base();</span>
<span class="line-modified">3610   int      shift   = Universe::narrow_klass_shift();</span>
3611   assert(UseCompressedClassPointers, &quot;only for compressed klass ptrs&quot;);
3612 
3613   BLOCK_COMMENT(&quot;cKlass encoder {&quot;);
3614 
3615 #ifdef ASSERT
3616   Label ok;
3617   z_tmll(current, KlassAlignmentInBytes-1); // Check alignment.
3618   z_brc(Assembler::bcondAllZero, ok);
3619   // The plain disassembler does not recognize illtrap. It instead displays
3620   // a 32-bit value. Issueing two illtraps assures the disassembler finds
3621   // the proper beginning of the next instruction.
3622   z_illtrap(0xee);
3623   z_illtrap(0xee);
3624   bind(ok);
3625 #endif
3626 
3627   if (base != NULL) {
3628     unsigned int base_h = ((unsigned long)base)&gt;&gt;32;
3629     unsigned int base_l = (unsigned int)((unsigned long)base);
3630     if ((base_h != 0) &amp;&amp; (base_l == 0) &amp;&amp; VM_Version::has_HighWordInstr()) {
</pre>
<hr />
<pre>
3638       lgr_if_needed(dst, current);
3639       z_sgr(dst, Z_R0);
3640     }
3641     current = dst;
3642   }
3643   if (shift != 0) {
3644     assert (LogKlassAlignmentInBytes == shift, &quot;decode alg wrong&quot;);
3645     z_srlg(dst, current, shift);
3646     current = dst;
3647   }
3648   lgr_if_needed(dst, current); // Move may be required (if neither base nor shift != 0).
3649 
3650   BLOCK_COMMENT(&quot;} cKlass encoder&quot;);
3651 }
3652 
3653 // This function calculates the size of the code generated by
3654 //   decode_klass_not_null(register dst, Register src)
3655 // when (Universe::heap() != NULL). Hence, if the instructions
3656 // it generates change, then this method needs to be updated.
3657 int MacroAssembler::instr_size_for_decode_klass_not_null() {
<span class="line-modified">3658   address  base    = Universe::narrow_klass_base();</span>
<span class="line-modified">3659   int shift_size   = Universe::narrow_klass_shift() == 0 ? 0 : 6; /* sllg */</span>
3660   int addbase_size = 0;
3661   assert(UseCompressedClassPointers, &quot;only for compressed klass ptrs&quot;);
3662 
3663   if (base != NULL) {
3664     unsigned int base_h = ((unsigned long)base)&gt;&gt;32;
3665     unsigned int base_l = (unsigned int)((unsigned long)base);
3666     if ((base_h != 0) &amp;&amp; (base_l == 0) &amp;&amp; VM_Version::has_HighWordInstr()) {
3667       addbase_size += 6; /* aih */
3668     } else if ((base_h == 0) &amp;&amp; (base_l != 0)) {
3669       addbase_size += 6; /* algfi */
3670     } else {
3671       addbase_size += load_const_size();
3672       addbase_size += 4; /* algr */
3673     }
3674   }
3675 #ifdef ASSERT
3676   addbase_size += 10;
3677   addbase_size += 2; // Extra sigill.
3678 #endif
3679   return addbase_size + shift_size;
3680 }
3681 
3682 // !!! If the instructions that get generated here change
3683 //     then function instr_size_for_decode_klass_not_null()
3684 //     needs to get updated.
3685 // This variant of decode_klass_not_null() must generate predictable code!
3686 // The code must only depend on globally known parameters.
3687 void MacroAssembler::decode_klass_not_null(Register dst) {
<span class="line-modified">3688   address  base    = Universe::narrow_klass_base();</span>
<span class="line-modified">3689   int      shift   = Universe::narrow_klass_shift();</span>
3690   int      beg_off = offset();
3691   assert(UseCompressedClassPointers, &quot;only for compressed klass ptrs&quot;);
3692 
3693   BLOCK_COMMENT(&quot;cKlass decoder (const size) {&quot;);
3694 
3695   if (shift != 0) { // Shift required?
3696     z_sllg(dst, dst, shift);
3697   }
3698   if (base != NULL) {
3699     unsigned int base_h = ((unsigned long)base)&gt;&gt;32;
3700     unsigned int base_l = (unsigned int)((unsigned long)base);
3701     if ((base_h != 0) &amp;&amp; (base_l == 0) &amp;&amp; VM_Version::has_HighWordInstr()) {
3702       z_aih(dst, base_h);     // Base has no set bits in lower half.
3703     } else if ((base_h == 0) &amp;&amp; (base_l != 0)) {
3704       z_algfi(dst, base_l);   // Base has no set bits in upper half.
3705     } else {
3706       load_const(Z_R0, base); // Base has set bits everywhere.
3707       z_algr(dst, Z_R0);
3708     }
3709   }
</pre>
<hr />
<pre>
3711 #ifdef ASSERT
3712   Label ok;
3713   z_tmll(dst, KlassAlignmentInBytes-1); // Check alignment.
3714   z_brc(Assembler::bcondAllZero, ok);
3715   // The plain disassembler does not recognize illtrap. It instead displays
3716   // a 32-bit value. Issueing two illtraps assures the disassembler finds
3717   // the proper beginning of the next instruction.
3718   z_illtrap(0xd1);
3719   z_illtrap(0xd1);
3720   bind(ok);
3721 #endif
3722   assert(offset() == beg_off + instr_size_for_decode_klass_not_null(), &quot;Code gen mismatch.&quot;);
3723 
3724   BLOCK_COMMENT(&quot;} cKlass decoder (const size)&quot;);
3725 }
3726 
3727 // This variant of decode_klass_not_null() is for cases where
3728 //  1) the size of the generated instructions may vary
3729 //  2) the result is (potentially) stored in a register different from the source.
3730 void MacroAssembler::decode_klass_not_null(Register dst, Register src) {
<span class="line-modified">3731   address base  = Universe::narrow_klass_base();</span>
<span class="line-modified">3732   int     shift = Universe::narrow_klass_shift();</span>
3733   assert(UseCompressedClassPointers, &quot;only for compressed klass ptrs&quot;);
3734 
3735   BLOCK_COMMENT(&quot;cKlass decoder {&quot;);
3736 
3737   if (src == noreg) src = dst;
3738 
3739   if (shift != 0) { // Shift or at least move required?
3740     z_sllg(dst, src, shift);
3741   } else {
3742     lgr_if_needed(dst, src);
3743   }
3744 
3745   if (base != NULL) {
3746     unsigned int base_h = ((unsigned long)base)&gt;&gt;32;
3747     unsigned int base_l = (unsigned int)((unsigned long)base);
3748     if ((base_h != 0) &amp;&amp; (base_l == 0) &amp;&amp; VM_Version::has_HighWordInstr()) {
3749       z_aih(dst, base_h);     // Base has not set bits in lower half.
3750     } else if ((base_h == 0) &amp;&amp; (base_l != 0)) {
3751       z_algfi(dst, base_l);   // Base has no set bits in upper half.
3752     } else {
</pre>
<hr />
<pre>
3812     // Support s = noreg.
3813     if (s != noreg) {
3814       z_st(s, Address(d, oopDesc::klass_gap_offset_in_bytes()));
3815     } else {
3816       z_mvhi(Address(d, oopDesc::klass_gap_offset_in_bytes()), 0);
3817     }
3818   }
3819 }
3820 
3821 // Compare klass ptr in memory against klass ptr in register.
3822 //
3823 // Rop1            - klass in register, always uncompressed.
3824 // disp            - Offset of klass in memory, compressed/uncompressed, depending on runtime flag.
3825 // Rbase           - Base address of cKlass in memory.
3826 // maybeNULL       - True if Rop1 possibly is a NULL.
3827 void MacroAssembler::compare_klass_ptr(Register Rop1, int64_t disp, Register Rbase, bool maybeNULL) {
3828 
3829   BLOCK_COMMENT(&quot;compare klass ptr {&quot;);
3830 
3831   if (UseCompressedClassPointers) {
<span class="line-modified">3832     const int shift = Universe::narrow_klass_shift();</span>
<span class="line-modified">3833     address   base  = Universe::narrow_klass_base();</span>
3834 
3835     assert((shift == 0) || (shift == LogKlassAlignmentInBytes), &quot;cKlass encoder detected bad shift&quot;);
3836     assert_different_registers(Rop1, Z_R0);
3837     assert_different_registers(Rop1, Rbase, Z_R1);
3838 
3839     // First encode register oop and then compare with cOop in memory.
3840     // This sequence saves an unnecessary cOop load and decode.
3841     if (base == NULL) {
3842       if (shift == 0) {
3843         z_cl(Rop1, disp, Rbase);     // Unscaled
3844       } else {
3845         z_srlg(Z_R0, Rop1, shift);   // ZeroBased
3846         z_cl(Z_R0, disp, Rbase);
3847       }
3848     } else {                         // HeapBased
3849 #ifdef ASSERT
3850       bool     used_R0 = true;
3851       bool     used_R1 = true;
3852 #endif
3853       Register current = Rop1;
</pre>
<hr />
<pre>
3946   return pow2_offset;
3947 }
3948 
3949 int MacroAssembler::get_oop_base_complement(Register Rbase, uint64_t oop_base) {
3950   int offset = get_oop_base(Rbase, oop_base);
3951   z_lcgr(Rbase, Rbase);
3952   return -offset;
3953 }
3954 
3955 // Compare compressed oop in memory against oop in register.
3956 // Rop1            - Oop in register.
3957 // disp            - Offset of cOop in memory.
3958 // Rbase           - Base address of cOop in memory.
3959 // maybeNULL       - True if Rop1 possibly is a NULL.
3960 // maybeNULLtarget - Branch target for Rop1 == NULL, if flow control shall NOT continue with compare instruction.
3961 void MacroAssembler::compare_heap_oop(Register Rop1, Address mem, bool maybeNULL) {
3962   Register Rbase  = mem.baseOrR0();
3963   Register Rindex = mem.indexOrR0();
3964   int64_t  disp   = mem.disp();
3965 
<span class="line-modified">3966   const int shift = Universe::narrow_oop_shift();</span>
<span class="line-modified">3967   address   base  = Universe::narrow_oop_base();</span>
3968 
3969   assert(UseCompressedOops, &quot;must be on to call this method&quot;);
3970   assert(Universe::heap() != NULL, &quot;java heap must be initialized to call this method&quot;);
3971   assert((shift == 0) || (shift == LogMinObjAlignmentInBytes), &quot;cOop encoder detected bad shift&quot;);
3972   assert_different_registers(Rop1, Z_R0);
3973   assert_different_registers(Rop1, Rbase, Z_R1);
3974   assert_different_registers(Rop1, Rindex, Z_R1);
3975 
3976   BLOCK_COMMENT(&quot;compare heap oop {&quot;);
3977 
3978   // First encode register oop and then compare with cOop in memory.
3979   // This sequence saves an unnecessary cOop load and decode.
3980   if (base == NULL) {
3981     if (shift == 0) {
3982       z_cl(Rop1, disp, Rindex, Rbase);  // Unscaled
3983     } else {
3984       z_srlg(Z_R0, Rop1, shift);        // ZeroBased
3985       z_cl(Z_R0, disp, Rindex, Rbase);
3986     }
3987   } else {                              // HeapBased
</pre>
<hr />
<pre>
4058                                     Register tmp1, Register tmp2, Register tmp3,
4059                                     DecoratorSet decorators) {
4060   access_store_at(T_OBJECT, IN_HEAP | decorators, a, Roop, tmp1, tmp2, tmp3);
4061 }
4062 
4063 //-------------------------------------------------
4064 // Encode compressed oop. Generally usable encoder.
4065 //-------------------------------------------------
4066 // Rsrc - contains regular oop on entry. It remains unchanged.
4067 // Rdst - contains compressed oop on exit.
4068 // Rdst and Rsrc may indicate same register, in which case Rsrc does not remain unchanged.
4069 //
4070 // Rdst must not indicate scratch register Z_R1 (Z_R1_scratch) for functionality.
4071 // Rdst should not indicate scratch register Z_R0 (Z_R0_scratch) for performance.
4072 //
4073 // only32bitValid is set, if later code only uses the lower 32 bits. In this
4074 // case we must not fix the upper 32 bits.
4075 void MacroAssembler::oop_encoder(Register Rdst, Register Rsrc, bool maybeNULL,
4076                                  Register Rbase, int pow2_offset, bool only32bitValid) {
4077 
<span class="line-modified">4078   const address oop_base  = Universe::narrow_oop_base();</span>
<span class="line-modified">4079   const int     oop_shift = Universe::narrow_oop_shift();</span>
<span class="line-modified">4080   const bool    disjoint  = Universe::narrow_oop_base_disjoint();</span>
4081 
4082   assert(UseCompressedOops, &quot;must be on to call this method&quot;);
4083   assert(Universe::heap() != NULL, &quot;java heap must be initialized to call this encoder&quot;);
4084   assert((oop_shift == 0) || (oop_shift == LogMinObjAlignmentInBytes), &quot;cOop encoder detected bad shift&quot;);
4085 
4086   if (disjoint || (oop_base == NULL)) {
4087     BLOCK_COMMENT(&quot;cOop encoder zeroBase {&quot;);
4088     if (oop_shift == 0) {
4089       if (oop_base != NULL &amp;&amp; !only32bitValid) {
4090         z_llgfr(Rdst, Rsrc); // Clear upper bits in case the register will be decoded again.
4091       } else {
4092         lgr_if_needed(Rdst, Rsrc);
4093       }
4094     } else {
4095       z_srlg(Rdst, Rsrc, oop_shift);
4096       if (oop_base != NULL &amp;&amp; !only32bitValid) {
4097         z_llgfr(Rdst, Rdst); // Clear upper bits in case the register will be decoded again.
4098       }
4099     }
4100     BLOCK_COMMENT(&quot;} cOop encoder zeroBase&quot;);
</pre>
<hr />
<pre>
4193 #endif
4194   BLOCK_COMMENT(&quot;} cOop encoder general&quot;);
4195 }
4196 
4197 //-------------------------------------------------
4198 // decode compressed oop. Generally usable decoder.
4199 //-------------------------------------------------
4200 // Rsrc - contains compressed oop on entry.
4201 // Rdst - contains regular oop on exit.
4202 // Rdst and Rsrc may indicate same register.
4203 // Rdst must not be the same register as Rbase, if Rbase was preloaded (before call).
4204 // Rdst can be the same register as Rbase. Then, either Z_R0 or Z_R1 must be available as scratch.
4205 // Rbase - register to use for the base
4206 // pow2_offset - offset of base to nice value. If -1, base must be loaded.
4207 // For performance, it is good to
4208 //  - avoid Z_R0 for any of the argument registers.
4209 //  - keep Rdst and Rsrc distinct from Rbase. Rdst == Rsrc is ok for performance.
4210 //  - avoid Z_R1 for Rdst if Rdst == Rbase.
4211 void MacroAssembler::oop_decoder(Register Rdst, Register Rsrc, bool maybeNULL, Register Rbase, int pow2_offset) {
4212 
<span class="line-modified">4213   const address oop_base  = Universe::narrow_oop_base();</span>
<span class="line-modified">4214   const int     oop_shift = Universe::narrow_oop_shift();</span>
<span class="line-modified">4215   const bool    disjoint  = Universe::narrow_oop_base_disjoint();</span>
4216 
4217   assert(UseCompressedOops, &quot;must be on to call this method&quot;);
4218   assert(Universe::heap() != NULL, &quot;java heap must be initialized to call this decoder&quot;);
4219   assert((oop_shift == 0) || (oop_shift == LogMinObjAlignmentInBytes),
4220          &quot;cOop encoder detected bad shift&quot;);
4221 
4222   // cOops are always loaded zero-extended from memory. No explicit zero-extension necessary.
4223 
4224   if (oop_base != NULL) {
4225     unsigned int oop_base_hl = ((unsigned int)((uint64_t)(intptr_t)oop_base &gt;&gt; 32)) &amp; 0xffff;
4226     unsigned int oop_base_hh = ((unsigned int)((uint64_t)(intptr_t)oop_base &gt;&gt; 48)) &amp; 0xffff;
4227     unsigned int oop_base_hf = ((unsigned int)((uint64_t)(intptr_t)oop_base &gt;&gt; 32)) &amp; 0xFFFFffff;
4228     if (disjoint &amp;&amp; (oop_base_hl == 0 || oop_base_hh == 0)) {
4229       BLOCK_COMMENT(&quot;cOop decoder disjointBase {&quot;);
4230       // We do not need to load the base. Instead, we can install the upper bits
4231       // with an OR instead of an ADD.
4232       Label done;
4233 
4234       // Rsrc contains a narrow oop. Thus we are sure the leftmost &lt;oop_shift&gt; bits will never be set.
4235       if (maybeNULL) {  // NULL ptr must be preserved!
</pre>
<hr />
<pre>
4322 #endif
4323       BLOCK_COMMENT(&quot;} cOop decoder general&quot;);
4324     }
4325   } else {
4326     BLOCK_COMMENT(&quot;cOop decoder zeroBase {&quot;);
4327     if (oop_shift == 0) {
4328       lgr_if_needed(Rdst, Rsrc);
4329     } else {
4330       z_sllg(Rdst, Rsrc, oop_shift);
4331     }
4332     BLOCK_COMMENT(&quot;} cOop decoder zeroBase&quot;);
4333   }
4334 }
4335 
4336 // ((OopHandle)result).resolve();
4337 void MacroAssembler::resolve_oop_handle(Register result) {
4338   // OopHandle::resolve is an indirection.
4339   z_lg(result, 0, result);
4340 }
4341 
<span class="line-modified">4342 void MacroAssembler::load_mirror(Register mirror, Register method) {</span>
<span class="line-modified">4343   mem2reg_opt(mirror, Address(method, Method::const_offset()));</span>
<span class="line-removed">4344   mem2reg_opt(mirror, Address(mirror, ConstMethod::constants_offset()));</span>
4345   mem2reg_opt(mirror, Address(mirror, ConstantPool::pool_holder_offset_in_bytes()));
4346   mem2reg_opt(mirror, Address(mirror, Klass::java_mirror_offset()));
4347   resolve_oop_handle(mirror);
4348 }
4349 






4350 //---------------------------------------------------------------
4351 //---  Operations on arrays.
4352 //---------------------------------------------------------------
4353 
4354 // Compiler ensures base is doubleword aligned and cnt is #doublewords.
4355 // Emitter does not KILL cnt and base arguments, since they need to be copied to
4356 // work registers anyway.
4357 // Actually, only r0, r1, and r5 are killed.
<span class="line-modified">4358 unsigned int MacroAssembler::Clear_Array(Register cnt_arg, Register base_pointer_arg, Register src_addr, Register src_len) {</span>
<span class="line-removed">4359   // Src_addr is evenReg.</span>
<span class="line-removed">4360   // Src_len is odd_Reg.</span>
4361 
4362   int      block_start = offset();
<span class="line-removed">4363   Register tmp_reg  = src_len; // Holds target instr addr for EX.</span>
4364   Register dst_len  = Z_R1;    // Holds dst len  for MVCLE.
4365   Register dst_addr = Z_R0;    // Holds dst addr for MVCLE.
4366 
4367   Label doXC, doMVCLE, done;
4368 
4369   BLOCK_COMMENT(&quot;Clear_Array {&quot;);
4370 
4371   // Check for zero len and convert to long.
<span class="line-modified">4372   z_ltgfr(src_len, cnt_arg);      // Remember casted value for doSTG case.</span>
4373   z_bre(done);                    // Nothing to do if len == 0.
4374 
4375   // Prefetch data to be cleared.
4376   if (VM_Version::has_Prefetch()) {
4377     z_pfd(0x02,   0, Z_R0, base_pointer_arg);
4378     z_pfd(0x02, 256, Z_R0, base_pointer_arg);
4379   }
4380 
<span class="line-modified">4381   z_sllg(dst_len, src_len, 3);    // #bytes to clear.</span>
<span class="line-modified">4382   z_cghi(src_len, 32);            // Check for len &lt;= 256 bytes (&lt;=32 DW).</span>
<span class="line-modified">4383   z_brnh(doXC);                   // If so, use executed XC to clear.</span>
4384 
4385   // MVCLE: initialize long arrays (general case).
4386   bind(doMVCLE);
4387   z_lgr(dst_addr, base_pointer_arg);
<span class="line-modified">4388   clear_reg(src_len, true, false); // Src len of MVCLE is zero.</span>
<span class="line-modified">4389 </span>
<span class="line-modified">4390   MacroAssembler::move_long_ext(dst_addr, src_addr, 0);</span>

4391   z_bru(done);
4392 
4393   // XC: initialize short arrays.
4394   Label XC_template; // Instr template, never exec directly!
4395     bind(XC_template);
4396     z_xc(0,0,base_pointer_arg,0,base_pointer_arg);
4397 
4398   bind(doXC);
<span class="line-modified">4399     add2reg(dst_len, -1);             // Get #bytes-1 for EXECUTE.</span>
4400     if (VM_Version::has_ExecuteExtensions()) {
<span class="line-modified">4401       z_exrl(dst_len, XC_template);   // Execute XC with var. len.</span>
4402     } else {
<span class="line-modified">4403       z_larl(tmp_reg, XC_template);</span>
<span class="line-modified">4404       z_ex(dst_len,0,Z_R0,tmp_reg);   // Execute XC with var. len.</span>
4405     }
4406     // z_bru(done);      // fallthru
4407 
4408   bind(done);
4409 
4410   BLOCK_COMMENT(&quot;} Clear_Array&quot;);
4411 
4412   int block_end = offset();
4413   return block_end - block_start;
4414 }
4415 
4416 // Compiler ensures base is doubleword aligned and cnt is count of doublewords.
4417 // Emitter does not KILL any arguments nor work registers.
4418 // Emitter generates up to 16 XC instructions, depending on the array length.
4419 unsigned int MacroAssembler::Clear_Array_Const(long cnt, Register base) {
4420   int  block_start    = offset();
4421   int  off;
4422   int  lineSize_Bytes = AllocatePrefetchStepSize;
4423   int  lineSize_DW    = AllocatePrefetchStepSize&gt;&gt;LogBytesPerWord;
4424   bool doPrefetch     = VM_Version::has_Prefetch();
</pre>
<hr />
<pre>
4446       z_xc(off*XC_maxlen, XC_maxlen-1, base, off*XC_maxlen, base);
4447 
4448       // Prefetch some cache lines in advance.
4449       if (doPrefetch &amp;&amp; (off &lt;= numXCInstr-AllocatePrefetchLines)) {
4450         z_pfd(0x02, (off+AllocatePrefetchLines)*lineSize_Bytes, Z_R0, base);
4451       }
4452     }
4453     if (off*XC_maxlen &lt; cnt*BytesPerWord) {
4454       z_xc(off*XC_maxlen, (cnt*BytesPerWord-off*XC_maxlen)-1, base, off*XC_maxlen, base);
4455     }
4456   }
4457   BLOCK_COMMENT(&quot;} Clear_Array_Const&quot;);
4458 
4459   int block_end = offset();
4460   return block_end - block_start;
4461 }
4462 
4463 // Compiler ensures base is doubleword aligned and cnt is #doublewords.
4464 // Emitter does not KILL cnt and base arguments, since they need to be copied to
4465 // work registers anyway.
<span class="line-modified">4466 // Actually, only r0, r1, r4, and r5 (which are work registers) are killed.</span>
4467 //
4468 // For very large arrays, exploit MVCLE H/W support.
4469 // MVCLE instruction automatically exploits H/W-optimized page mover.
4470 // - Bytes up to next page boundary are cleared with a series of XC to self.
4471 // - All full pages are cleared with the page mover H/W assist.
4472 // - Remaining bytes are again cleared by a series of XC to self.
4473 //
<span class="line-modified">4474 unsigned int MacroAssembler::Clear_Array_Const_Big(long cnt, Register base_pointer_arg, Register src_addr, Register src_len) {</span>
<span class="line-removed">4475   // Src_addr is evenReg.</span>
<span class="line-removed">4476   // Src_len is odd_Reg.</span>
4477 
4478   int      block_start = offset();
4479   Register dst_len  = Z_R1;      // Holds dst len  for MVCLE.
4480   Register dst_addr = Z_R0;      // Holds dst addr for MVCLE.
4481 
4482   BLOCK_COMMENT(&quot;Clear_Array_Const_Big {&quot;);
4483 
4484   // Get len to clear.
4485   load_const_optimized(dst_len, (long)cnt*8L);  // in Bytes = #DW*8
4486 
4487   // Prepare other args to MVCLE.
4488   z_lgr(dst_addr, base_pointer_arg);
<span class="line-modified">4489   // Indicate unused result.</span>
<span class="line-modified">4490   (void) clear_reg(src_len, true, false);  // Src len of MVCLE is zero.</span>
<span class="line-modified">4491 </span>
<span class="line-modified">4492   // Clear.</span>
<span class="line-removed">4493   MacroAssembler::move_long_ext(dst_addr, src_addr, 0);</span>
4494   BLOCK_COMMENT(&quot;} Clear_Array_Const_Big&quot;);
4495 
4496   int block_end = offset();
4497   return block_end - block_start;
4498 }
4499 
4500 // Allocator.
4501 unsigned int MacroAssembler::CopyRawMemory_AlignedDisjoint(Register src_reg, Register dst_reg,
4502                                                            Register cnt_reg,
4503                                                            Register tmp1_reg, Register tmp2_reg) {
4504   // Tmp1 is oddReg.
4505   // Tmp2 is evenReg.
4506 
4507   int block_start = offset();
4508   Label doMVC, doMVCLE, done, MVC_template;
4509 
4510   BLOCK_COMMENT(&quot;CopyRawMemory_AlignedDisjoint {&quot;);
4511 
4512   // Check for zero len and convert to long.
4513   z_ltgfr(cnt_reg, cnt_reg);      // Remember casted value for doSTG case.
</pre>
<hr />
<pre>
4546     z_pfd(1,  0,Z_R0,src_reg);
4547     z_pfd(2,  0,Z_R0,dst_reg);
4548     //    z_pfd(1,256,Z_R0,src_reg);    // Assume very short copy.
4549     //    z_pfd(2,256,Z_R0,dst_reg);
4550   }
4551 
4552   if (VM_Version::has_ExecuteExtensions()) {
4553     z_exrl(Z_R1, MVC_template);
4554   } else {
4555     z_ex(tmp1_reg, 0, Z_R0, Z_R1);
4556   }
4557 
4558   bind(done);
4559 
4560   BLOCK_COMMENT(&quot;} CopyRawMemory_AlignedDisjoint&quot;);
4561 
4562   int block_end = offset();
4563   return block_end - block_start;
4564 }
4565 

4566 //------------------------------------------------------
4567 //   Special String Intrinsics. Implementation
4568 //------------------------------------------------------
4569 
4570 // Intrinsics for CompactStrings
4571 
4572 // Compress char[] to byte[].
4573 //   Restores: src, dst
4574 //   Uses:     cnt
4575 //   Kills:    tmp, Z_R0, Z_R1.
4576 //   Early clobber: result.
4577 // Note:
4578 //   cnt is signed int. Do not rely on high word!
4579 //       counts # characters, not bytes.
4580 // The result is the number of characters copied before the first incompatible character was found.
4581 // If precise is true, the processing stops exactly at this point. Otherwise, the result may be off
4582 // by a few bytes. The result always indicates the number of copied characters.
4583 // When used as a character index, the returned value points to the first incompatible character.
4584 //
4585 // Note: Does not behave exactly like package private StringUTF16 compress java implementation in case of failure:
</pre>
<hr />
<pre>
5793   }
5794   z_brc(Assembler::bcondNotFound, Ldone);
5795   if (is_byte) {
5796     if (VM_Version::has_DistinctOpnds()) {
5797       z_sgrk(result, odd_reg, haystack);
5798     } else {
5799       z_sgr(odd_reg, haystack);
5800       z_lgr(result, odd_reg);
5801     }
5802   } else {
5803     z_slgr(odd_reg, haystack);
5804     z_srlg(result, odd_reg, exact_log2(sizeof(jchar)));
5805   }
5806 
5807   bind(Ldone);
5808   }
5809   BLOCK_COMMENT(&quot;} string_indexof_char&quot;);
5810 
5811   return offset() - block_start;
5812 }
<span class="line-modified">5813 </span>
5814 
5815 //-------------------------------------------------
5816 //   Constants (scalar and oop) in constant pool
5817 //-------------------------------------------------
5818 
5819 // Add a non-relocated constant to the CP.
5820 int MacroAssembler::store_const_in_toc(AddressLiteral&amp; val) {
5821   long    value  = val.value();
5822   address tocPos = long_constant(value);
5823 
5824   if (tocPos != NULL) {
5825     int tocOffset = (int)(tocPos - code()-&gt;consts()-&gt;start());
5826     return tocOffset;
5827   }
5828   // Address_constant returned NULL, so no constant entry has been created.
5829   // In that case, we return a &quot;fatal&quot; offset, just in case that subsequently
5830   // generated access code is executed.
5831   return -1;
5832 }
5833 
</pre>
<hr />
<pre>
5847     // Store toc_offset in relocation, used by call_far_patchable.
5848     if ((relocInfo::relocType)rel-&gt;type() == relocInfo::runtime_call_w_cp_type) {
5849       ((runtime_call_w_cp_Relocation *)(rel))-&gt;set_constant_pool_offset(tocOffset);
5850     }
5851     // Relocate at the load&#39;s pc.
5852     relocate(rsp);
5853 
5854     return tocOffset;
5855   }
5856   // Address_constant returned NULL, so no constant entry has been created
5857   // in that case, we return a &quot;fatal&quot; offset, just in case that subsequently
5858   // generated access code is executed.
5859   return -1;
5860 }
5861 
5862 bool MacroAssembler::load_const_from_toc(Register dst, AddressLiteral&amp; a, Register Rtoc) {
5863   int     tocOffset = store_const_in_toc(a);
5864   if (tocOffset == -1) return false;
5865   address tocPos    = tocOffset + code()-&gt;consts()-&gt;start();
5866   assert((address)code()-&gt;consts()-&gt;start() != NULL, &quot;Please add CP address&quot;);
<span class="line-modified">5867 </span>
5868   load_long_pcrelative(dst, tocPos);
5869   return true;
5870 }
5871 
5872 bool MacroAssembler::load_oop_from_toc(Register dst, AddressLiteral&amp; a, Register Rtoc) {
5873   int     tocOffset = store_oop_in_toc(a);
5874   if (tocOffset == -1) return false;
5875   address tocPos    = tocOffset + code()-&gt;consts()-&gt;start();
5876   assert((address)code()-&gt;consts()-&gt;start() != NULL, &quot;Please add CP address&quot;);
5877 
5878   load_addr_pcrelative(dst, tocPos);
5879   return true;
5880 }
5881 
5882 // If the instruction sequence at the given pc is a load_const_from_toc
5883 // sequence, return the value currently stored at the referenced position
5884 // in the TOC.
5885 intptr_t MacroAssembler::get_const_from_toc(address pc) {
5886 
5887   assert(is_load_const_from_toc(pc), &quot;must be load_const_from_pool&quot;);
</pre>
<hr />
<pre>
6106 void MacroAssembler::translate_to(Register r1, Register r2, uint m3) {
6107   assert(r1-&gt;encoding() % 2 == 0, &quot;dst addr/src len must be an even/odd register pair&quot;);
6108   assert((m3 &amp; 0b1110) == 0, &quot;Unused mask bits must be zero&quot;);
6109 
6110   Label retry;
6111   bind(retry);
6112   Assembler::z_trto(r1, r2, m3);
6113   Assembler::z_brc(Assembler::bcondOverflow /* CC==3 (iterate) */, retry);
6114 }
6115 
6116 void MacroAssembler::translate_tt(Register r1, Register r2, uint m3) {
6117   assert(r1-&gt;encoding() % 2 == 0, &quot;dst addr/src len must be an even/odd register pair&quot;);
6118   assert((m3 &amp; 0b1110) == 0, &quot;Unused mask bits must be zero&quot;);
6119 
6120   Label retry;
6121   bind(retry);
6122   Assembler::z_trtt(r1, r2, m3);
6123   Assembler::z_brc(Assembler::bcondOverflow /* CC==3 (iterate) */, retry);
6124 }
6125 
<span class="line-removed">6126 </span>
<span class="line-removed">6127 void MacroAssembler::generate_type_profiling(const Register Rdata,</span>
<span class="line-removed">6128                                              const Register Rreceiver_klass,</span>
<span class="line-removed">6129                                              const Register Rwanted_receiver_klass,</span>
<span class="line-removed">6130                                              const Register Rmatching_row,</span>
<span class="line-removed">6131                                              bool is_virtual_call) {</span>
<span class="line-removed">6132   const int row_size = in_bytes(ReceiverTypeData::receiver_offset(1)) -</span>
<span class="line-removed">6133                        in_bytes(ReceiverTypeData::receiver_offset(0));</span>
<span class="line-removed">6134   const int num_rows = ReceiverTypeData::row_limit();</span>
<span class="line-removed">6135   NearLabel found_free_row;</span>
<span class="line-removed">6136   NearLabel do_increment;</span>
<span class="line-removed">6137   NearLabel found_no_slot;</span>
<span class="line-removed">6138 </span>
<span class="line-removed">6139   BLOCK_COMMENT(&quot;type profiling {&quot;);</span>
<span class="line-removed">6140 </span>
<span class="line-removed">6141   // search for:</span>
<span class="line-removed">6142   //    a) The type given in Rwanted_receiver_klass.</span>
<span class="line-removed">6143   //    b) The *first* empty row.</span>
<span class="line-removed">6144 </span>
<span class="line-removed">6145   // First search for a) only, just running over b) with no regard.</span>
<span class="line-removed">6146   // This is possible because</span>
<span class="line-removed">6147   //    wanted_receiver_class == receiver_class  &amp;&amp;  wanted_receiver_class == 0</span>
<span class="line-removed">6148   // is never true (receiver_class can&#39;t be zero).</span>
<span class="line-removed">6149   for (int row_num = 0; row_num &lt; num_rows; row_num++) {</span>
<span class="line-removed">6150     // Row_offset should be a well-behaved positive number. The generated code relies</span>
<span class="line-removed">6151     // on that wrt constant code size. Add2reg can handle all row_offset values, but</span>
<span class="line-removed">6152     // will have to vary generated code size.</span>
<span class="line-removed">6153     int row_offset = in_bytes(ReceiverTypeData::receiver_offset(row_num));</span>
<span class="line-removed">6154     assert(Displacement::is_shortDisp(row_offset), &quot;Limitation of generated code&quot;);</span>
<span class="line-removed">6155 </span>
<span class="line-removed">6156     // Is Rwanted_receiver_klass in this row?</span>
<span class="line-removed">6157     if (VM_Version::has_CompareBranch()) {</span>
<span class="line-removed">6158       z_lg(Rwanted_receiver_klass, row_offset, Z_R0, Rdata);</span>
<span class="line-removed">6159       // Rmatching_row = Rdata + row_offset;</span>
<span class="line-removed">6160       add2reg(Rmatching_row, row_offset, Rdata);</span>
<span class="line-removed">6161       // if (*row_recv == (intptr_t) receiver_klass) goto fill_existing_slot;</span>
<span class="line-removed">6162       compare64_and_branch(Rwanted_receiver_klass, Rreceiver_klass, Assembler::bcondEqual, do_increment);</span>
<span class="line-removed">6163     } else {</span>
<span class="line-removed">6164       add2reg(Rmatching_row, row_offset, Rdata);</span>
<span class="line-removed">6165       z_cg(Rreceiver_klass, row_offset, Z_R0, Rdata);</span>
<span class="line-removed">6166       z_bre(do_increment);</span>
<span class="line-removed">6167     }</span>
<span class="line-removed">6168   }</span>
<span class="line-removed">6169 </span>
<span class="line-removed">6170   // Now that we did not find a match, let&#39;s search for b).</span>
<span class="line-removed">6171 </span>
<span class="line-removed">6172   // We could save the first calculation of Rmatching_row if we woud search for a) in reverse order.</span>
<span class="line-removed">6173   // We would then end up here with Rmatching_row containing the value for row_num == 0.</span>
<span class="line-removed">6174   // We would not see much benefit, if any at all, because the CPU can schedule</span>
<span class="line-removed">6175   // two instructions together with a branch anyway.</span>
<span class="line-removed">6176   for (int row_num = 0; row_num &lt; num_rows; row_num++) {</span>
<span class="line-removed">6177     int row_offset = in_bytes(ReceiverTypeData::receiver_offset(row_num));</span>
<span class="line-removed">6178 </span>
<span class="line-removed">6179     // Has this row a zero receiver_klass, i.e. is it empty?</span>
<span class="line-removed">6180     if (VM_Version::has_CompareBranch()) {</span>
<span class="line-removed">6181       z_lg(Rwanted_receiver_klass, row_offset, Z_R0, Rdata);</span>
<span class="line-removed">6182       // Rmatching_row = Rdata + row_offset</span>
<span class="line-removed">6183       add2reg(Rmatching_row, row_offset, Rdata);</span>
<span class="line-removed">6184       // if (*row_recv == (intptr_t) 0) goto found_free_row</span>
<span class="line-removed">6185       compare64_and_branch(Rwanted_receiver_klass, (intptr_t)0, Assembler::bcondEqual, found_free_row);</span>
<span class="line-removed">6186     } else {</span>
<span class="line-removed">6187       add2reg(Rmatching_row, row_offset, Rdata);</span>
<span class="line-removed">6188       load_and_test_long(Rwanted_receiver_klass, Address(Rdata, row_offset));</span>
<span class="line-removed">6189       z_bre(found_free_row);  // zero -&gt; Found a free row.</span>
<span class="line-removed">6190     }</span>
<span class="line-removed">6191   }</span>
<span class="line-removed">6192 </span>
<span class="line-removed">6193   // No match, no empty row found.</span>
<span class="line-removed">6194   // Increment total counter to indicate polymorphic case.</span>
<span class="line-removed">6195   if (is_virtual_call) {</span>
<span class="line-removed">6196     add2mem_64(Address(Rdata, CounterData::count_offset()), 1, Rmatching_row);</span>
<span class="line-removed">6197   }</span>
<span class="line-removed">6198   z_bru(found_no_slot);</span>
<span class="line-removed">6199 </span>
<span class="line-removed">6200   // Here we found an empty row, but we have not found Rwanted_receiver_klass.</span>
<span class="line-removed">6201   // Rmatching_row holds the address to the first empty row.</span>
<span class="line-removed">6202   bind(found_free_row);</span>
<span class="line-removed">6203   // Store receiver_klass into empty slot.</span>
<span class="line-removed">6204   z_stg(Rreceiver_klass, 0, Z_R0, Rmatching_row);</span>
<span class="line-removed">6205 </span>
<span class="line-removed">6206   // Increment the counter of Rmatching_row.</span>
<span class="line-removed">6207   bind(do_increment);</span>
<span class="line-removed">6208   ByteSize counter_offset = ReceiverTypeData::receiver_count_offset(0) - ReceiverTypeData::receiver_offset(0);</span>
<span class="line-removed">6209   add2mem_64(Address(Rmatching_row, counter_offset), 1, Rdata);</span>
<span class="line-removed">6210 </span>
<span class="line-removed">6211   bind(found_no_slot);</span>
<span class="line-removed">6212 </span>
<span class="line-removed">6213   BLOCK_COMMENT(&quot;} type profiling&quot;);</span>
<span class="line-removed">6214 }</span>
<span class="line-removed">6215 </span>
6216 //---------------------------------------
6217 // Helpers for Intrinsic Emitters
6218 //---------------------------------------
6219 
6220 /**
6221  * uint32_t crc;
6222  * timesXtoThe32[crc &amp; 0xFF] ^ (crc &gt;&gt; 8);
6223  */
6224 void MacroAssembler::fold_byte_crc32(Register crc, Register val, Register table, Register tmp) {
6225   assert_different_registers(crc, table, tmp);
6226   assert_different_registers(val, table);
6227   if (crc == val) {      // Must rotate first to use the unmodified value.
6228     rotate_then_insert(tmp, val, 56-2, 63-2, 2, true);  // Insert byte 7 of val, shifted left by 2, into byte 6..7 of tmp, clear the rest.
6229     z_srl(crc, 8);       // Unsigned shift, clear leftmost 8 bits.
6230   } else {
6231     z_srl(crc, 8);       // Unsigned shift, clear leftmost 8 bits.
6232     rotate_then_insert(tmp, val, 56-2, 63-2, 2, true);  // Insert byte 7 of val, shifted left by 2, into byte 6..7 of tmp, clear the rest.
6233   }
6234   z_x(crc, Address(table, tmp, 0));
6235 }
</pre>
<hr />
<pre>
6856 void MacroAssembler::asm_assert_frame_size(Register expected_size, Register tmp, const char* msg, int id) {
6857   if (tmp == noreg) {
6858     tmp = expected_size;
6859   } else {
6860     if (tmp != expected_size) {
6861       z_lgr(tmp, expected_size);
6862     }
6863     z_algr(tmp, Z_SP);
6864     z_slg(tmp, 0, Z_R0, Z_SP);
6865     asm_assert_eq(msg, id);
6866   }
6867 }
6868 #endif // !PRODUCT
6869 
6870 void MacroAssembler::verify_thread() {
6871   if (VerifyThread) {
6872     unimplemented(&quot;&quot;, 117);
6873   }
6874 }
6875 








































6876 // Plausibility check for oops.
6877 void MacroAssembler::verify_oop(Register oop, const char* msg) {
6878   if (!VerifyOops) return;
6879 
6880   BLOCK_COMMENT(&quot;verify_oop {&quot;);
<span class="line-modified">6881   Register tmp = Z_R0;</span>
<span class="line-modified">6882   unsigned int nbytes_save = 5*BytesPerWord;</span>
<span class="line-modified">6883   address entry = StubRoutines::verify_oop_subroutine_entry_address();</span>




























6884 
6885   save_return_pc();
<span class="line-modified">6886   push_frame_abi160(nbytes_save);</span>
<span class="line-modified">6887   z_stmg(Z_R1, Z_R5, frame::z_abi_160_size, Z_SP);</span>
6888 
<span class="line-modified">6889   z_lgr(Z_ARG2, oop);</span>
<span class="line-modified">6890   load_const(Z_ARG1, (address) msg);</span>
<span class="line-modified">6891   load_const(Z_R1, entry);</span>
6892   z_lg(Z_R1, 0, Z_R1);
6893   call_c(Z_R1);
6894 
<span class="line-modified">6895   z_lmg(Z_R1, Z_R5, frame::z_abi_160_size, Z_SP);</span>
6896   pop_frame();
6897   restore_return_pc();
6898 
6899   BLOCK_COMMENT(&quot;} verify_oop &quot;);
6900 }
6901 
6902 const char* MacroAssembler::stop_types[] = {
6903   &quot;stop&quot;,
6904   &quot;untested&quot;,
6905   &quot;unimplemented&quot;,
6906   &quot;shouldnotreachhere&quot;
6907 };
6908 
6909 static void stop_on_request(const char* tp, const char* msg) {
6910   tty-&gt;print(&quot;Z assembly code requires stop: (%s) %s\n&quot;, tp, msg);
6911   guarantee(false, &quot;Z assembly code requires stop: %s&quot;, msg);
6912 }
6913 
6914 void MacroAssembler::stop(int type, const char* msg, int id) {
6915   BLOCK_COMMENT(err_msg(&quot;stop: %s {&quot;, msg));
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2016, 2019, Oracle and/or its affiliates. All rights reserved.</span>
<span class="line-modified">   3  * Copyright (c) 2016, 2019, SAP SE. All rights reserved.</span>
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/codeBuffer.hpp&quot;
  28 #include &quot;asm/macroAssembler.inline.hpp&quot;
  29 #include &quot;compiler/disassembler.hpp&quot;
  30 #include &quot;gc/shared/barrierSet.hpp&quot;
  31 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  32 #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
  33 #include &quot;interpreter/interpreter.hpp&quot;
  34 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
  35 #include &quot;memory/resourceArea.hpp&quot;
  36 #include &quot;memory/universe.hpp&quot;
  37 #include &quot;oops/accessDecorators.hpp&quot;
  38 #include &quot;oops/compressedOops.inline.hpp&quot;
  39 #include &quot;oops/klass.inline.hpp&quot;
<span class="line-added">  40 #ifdef COMPILER2</span>
  41 #include &quot;opto/compile.hpp&quot;
  42 #include &quot;opto/intrinsicnode.hpp&quot;
  43 #include &quot;opto/matcher.hpp&quot;
<span class="line-added">  44 #endif</span>
  45 #include &quot;prims/methodHandles.hpp&quot;
  46 #include &quot;registerSaver_s390.hpp&quot;
  47 #include &quot;runtime/biasedLocking.hpp&quot;
  48 #include &quot;runtime/icache.hpp&quot;
  49 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  50 #include &quot;runtime/objectMonitor.hpp&quot;
  51 #include &quot;runtime/os.hpp&quot;
  52 #include &quot;runtime/safepoint.hpp&quot;
  53 #include &quot;runtime/safepointMechanism.hpp&quot;
  54 #include &quot;runtime/sharedRuntime.hpp&quot;
  55 #include &quot;runtime/stubRoutines.hpp&quot;
  56 #include &quot;utilities/events.hpp&quot;
  57 #include &quot;utilities/macros.hpp&quot;
<span class="line-added">  58 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  59 
  60 #include &lt;ucontext.h&gt;
  61 
  62 #define BLOCK_COMMENT(str) block_comment(str)
  63 #define BIND(label)        bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  64 
  65 // Move 32-bit register if destination and source are different.
  66 void MacroAssembler::lr_if_needed(Register rd, Register rs) {
  67   if (rs != rd) { z_lr(rd, rs); }
  68 }
  69 
  70 // Move register if destination and source are different.
  71 void MacroAssembler::lgr_if_needed(Register rd, Register rs) {
  72   if (rs != rd) { z_lgr(rd, rs); }
  73 }
  74 
  75 // Zero-extend 32-bit register into 64-bit register if destination and source are different.
  76 void MacroAssembler::llgfr_if_needed(Register rd, Register rs) {
  77   if (rs != rd) { z_llgfr(rd, rs); }
  78 }
</pre>
<hr />
<pre>
1157 }
1158 
1159 // Load a 32bit constant into a 64bit register, sign-extend or zero-extend.
1160 // Patchable code sequence, but not atomically patchable.
1161 // Make sure to keep code size constant -&gt; no value-dependent optimizations.
1162 // Do not kill condition code.
1163 void MacroAssembler::load_const_32to64(Register t, int64_t x, bool sign_extend) {
1164   if (sign_extend) { Assembler::z_lgfi(t, x); }
1165   else             { Assembler::z_llilf(t, x); }
1166 }
1167 
1168 // Load narrow oop constant, no decompression.
1169 void MacroAssembler::load_narrow_oop(Register t, narrowOop a) {
1170   assert(UseCompressedOops, &quot;must be on to call this method&quot;);
1171   load_const_32to64(t, a, false /*sign_extend*/);
1172 }
1173 
1174 // Load narrow klass constant, compression required.
1175 void MacroAssembler::load_narrow_klass(Register t, Klass* k) {
1176   assert(UseCompressedClassPointers, &quot;must be on to call this method&quot;);
<span class="line-modified">1177   narrowKlass encoded_k = CompressedKlassPointers::encode(k);</span>
1178   load_const_32to64(t, encoded_k, false /*sign_extend*/);
1179 }
1180 
1181 //------------------------------------------------------
1182 //  Compare (patchable) constant with register.
1183 //------------------------------------------------------
1184 
1185 // Compare narrow oop in reg with narrow oop constant, no decompression.
1186 void MacroAssembler::compare_immediate_narrow_oop(Register oop1, narrowOop oop2) {
1187   assert(UseCompressedOops, &quot;must be on to call this method&quot;);
1188 
1189   Assembler::z_clfi(oop1, oop2);
1190 }
1191 
1192 // Compare narrow oop in reg with narrow oop constant, no decompression.
1193 void MacroAssembler::compare_immediate_narrow_klass(Register klass1, Klass* klass2) {
1194   assert(UseCompressedClassPointers, &quot;must be on to call this method&quot;);
<span class="line-modified">1195   narrowKlass encoded_k = CompressedKlassPointers::encode(klass2);</span>
1196 
1197   Assembler::z_clfi(klass1, encoded_k);
1198 }
1199 
1200 //----------------------------------------------------------
1201 //  Check which kind of load_constant we have here.
1202 //----------------------------------------------------------
1203 
1204 // Detection of CPU version dependent load_const sequence.
1205 // The detection is valid only for code sequences generated by load_const,
1206 // not load_const_optimized.
1207 bool MacroAssembler::is_load_const(address a) {
1208   unsigned long inst1, inst2;
1209   unsigned int  len1,  len2;
1210 
1211   len1 = get_instruction(a, &amp;inst1);
1212   len2 = get_instruction(a + len1, &amp;inst2);
1213 
1214   return is_z_iihf(inst1) &amp;&amp; is_z_iilf(inst2);
1215 }
</pre>
<hr />
<pre>
1271   assert(is_compare_immediate32(pos), &quot;not a compressed ptr compare&quot;);
1272 
1273   set_imm32(pos, np);
1274   return 6;
1275 }
1276 
1277 // Patching the immediate value of CPU version dependent load_narrow_oop sequence.
1278 // The passed ptr must NOT be in compressed format!
1279 int MacroAssembler::patch_load_narrow_oop(address pos, oop o) {
1280   assert(UseCompressedOops, &quot;Can only patch compressed oops&quot;);
1281 
1282   narrowOop no = CompressedOops::encode(o);
1283   return patch_load_const_32to64(pos, no);
1284 }
1285 
1286 // Patching the immediate value of CPU version dependent load_narrow_klass sequence.
1287 // The passed ptr must NOT be in compressed format!
1288 int MacroAssembler::patch_load_narrow_klass(address pos, Klass* k) {
1289   assert(UseCompressedClassPointers, &quot;Can only patch compressed klass pointers&quot;);
1290 
<span class="line-modified">1291   narrowKlass nk = CompressedKlassPointers::encode(k);</span>
1292   return patch_load_const_32to64(pos, nk);
1293 }
1294 
1295 // Patching the immediate value of CPU version dependent compare_immediate_narrow_oop sequence.
1296 // The passed ptr must NOT be in compressed format!
1297 int MacroAssembler::patch_compare_immediate_narrow_oop(address pos, oop o) {
1298   assert(UseCompressedOops, &quot;Can only patch compressed oops&quot;);
1299 
1300   narrowOop no = CompressedOops::encode(o);
1301   return patch_compare_immediate_32(pos, no);
1302 }
1303 
1304 // Patching the immediate value of CPU version dependent compare_immediate_narrow_klass sequence.
1305 // The passed ptr must NOT be in compressed format!
1306 int MacroAssembler::patch_compare_immediate_narrow_klass(address pos, Klass* k) {
1307   assert(UseCompressedClassPointers, &quot;Can only patch compressed klass pointers&quot;);
1308 
<span class="line-modified">1309   narrowKlass nk = CompressedKlassPointers::encode(k);</span>
1310   return patch_compare_immediate_32(pos, nk);
1311 }
1312 
1313 //------------------------------------------------------------------------
1314 //  Extract the constant from a load_constant instruction stream.
1315 //------------------------------------------------------------------------
1316 
1317 // Get constant from a load_const sequence.
1318 long MacroAssembler::get_const(address a) {
1319   assert(is_load_const(a), &quot;not a load of a constant&quot;);
1320   unsigned long x;
1321   x =  (((unsigned long) (get_imm32(a,0) &amp; 0xffffffff)) &lt;&lt; 32);
1322   x |= (((unsigned long) (get_imm32(a,1) &amp; 0xffffffff)));
1323   return (long) x;
1324 }
1325 
1326 //--------------------------------------
1327 //  Store a constant in memory.
1328 //--------------------------------------
1329 
</pre>
<hr />
<pre>
2911   AddressLiteral icmiss(SharedRuntime::get_ic_miss_stub());
2912 
2913   load_const_optimized(scratch, icmiss);
2914   z_br(scratch);
2915 
2916   // Fill unused space.
2917   if (requiredSize &gt; 0) {
2918     while ((offset() - startOffset) &lt; requiredSize) {
2919       if (trapMarker == 0) {
2920         z_nop();
2921       } else {
2922         z_illtrap(trapMarker);
2923       }
2924     }
2925   }
2926   BLOCK_COMMENT(&quot;} IC miss handler&quot;);
2927   return labelOffset;
2928 }
2929 
2930 void MacroAssembler::nmethod_UEP(Label&amp; ic_miss) {
<span class="line-modified">2931   Register ic_reg       = Z_inline_cache;</span>
2932   int      klass_offset = oopDesc::klass_offset_in_bytes();
2933   if (!ImplicitNullChecks || MacroAssembler::needs_explicit_null_check(klass_offset)) {
2934     if (VM_Version::has_CompareBranch()) {
2935       z_cgij(Z_ARG1, 0, Assembler::bcondEqual, ic_miss);
2936     } else {
2937       z_ltgr(Z_ARG1, Z_ARG1);
2938       z_bre(ic_miss);
2939     }
2940   }
2941   // Compare cached class against klass from receiver.
2942   compare_klass_ptr(ic_reg, klass_offset, Z_ARG1, false);
2943   z_brne(ic_miss);
2944 }
2945 
2946 void MacroAssembler::check_klass_subtype_fast_path(Register   sub_klass,
2947                                                    Register   super_klass,
2948                                                    Register   temp1_reg,
2949                                                    Label*     L_success,
2950                                                    Label*     L_failure,
2951                                                    Label*     L_slow_path,
</pre>
<hr />
<pre>
3116 #undef final_jmp
3117   BLOCK_COMMENT(&quot;} check_klass_subtype_slow_path&quot;);
3118 }
3119 
3120 // Emitter for combining fast and slow path.
3121 void MacroAssembler::check_klass_subtype(Register sub_klass,
3122                                          Register super_klass,
3123                                          Register temp1_reg,
3124                                          Register temp2_reg,
3125                                          Label&amp;   L_success) {
3126   NearLabel failure;
3127   BLOCK_COMMENT(err_msg(&quot;check_klass_subtype(%s subclass of %s) {&quot;, sub_klass-&gt;name(), super_klass-&gt;name()));
3128   check_klass_subtype_fast_path(sub_klass, super_klass, temp1_reg,
3129                                 &amp;L_success, &amp;failure, NULL);
3130   check_klass_subtype_slow_path(sub_klass, super_klass,
3131                                 temp1_reg, temp2_reg, &amp;L_success, NULL);
3132   BIND(failure);
3133   BLOCK_COMMENT(&quot;} check_klass_subtype&quot;);
3134 }
3135 
<span class="line-added">3136 void MacroAssembler::clinit_barrier(Register klass, Register thread, Label* L_fast_path, Label* L_slow_path) {</span>
<span class="line-added">3137   assert(L_fast_path != NULL || L_slow_path != NULL, &quot;at least one is required&quot;);</span>
<span class="line-added">3138 </span>
<span class="line-added">3139   Label L_fallthrough;</span>
<span class="line-added">3140   if (L_fast_path == NULL) {</span>
<span class="line-added">3141     L_fast_path = &amp;L_fallthrough;</span>
<span class="line-added">3142   } else if (L_slow_path == NULL) {</span>
<span class="line-added">3143     L_slow_path = &amp;L_fallthrough;</span>
<span class="line-added">3144   }</span>
<span class="line-added">3145 </span>
<span class="line-added">3146   // Fast path check: class is fully initialized</span>
<span class="line-added">3147   z_cli(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);</span>
<span class="line-added">3148   z_bre(*L_fast_path);</span>
<span class="line-added">3149 </span>
<span class="line-added">3150   // Fast path check: current thread is initializer thread</span>
<span class="line-added">3151   z_cg(thread, Address(klass, InstanceKlass::init_thread_offset()));</span>
<span class="line-added">3152   if (L_slow_path == &amp;L_fallthrough) {</span>
<span class="line-added">3153     z_bre(*L_fast_path);</span>
<span class="line-added">3154   } else if (L_fast_path == &amp;L_fallthrough) {</span>
<span class="line-added">3155     z_brne(*L_slow_path);</span>
<span class="line-added">3156   } else {</span>
<span class="line-added">3157     Unimplemented();</span>
<span class="line-added">3158   }</span>
<span class="line-added">3159 </span>
<span class="line-added">3160   bind(L_fallthrough);</span>
<span class="line-added">3161 }</span>
<span class="line-added">3162 </span>
3163 // Increment a counter at counter_address when the eq condition code is
3164 // set. Kills registers tmp1_reg and tmp2_reg and preserves the condition code.
3165 void MacroAssembler::increment_counter_eq(address counter_address, Register tmp1_reg, Register tmp2_reg) {
3166   Label l;
3167   z_brne(l);
3168   load_const(tmp1_reg, counter_address);
3169   add2mem_32(Address(tmp1_reg), 1, tmp2_reg);
3170   z_cr(tmp1_reg, tmp1_reg); // Set cc to eq.
3171   bind(l);
3172 }
3173 
3174 // Semantics are dependent on the slow_case label:
3175 //   If the slow_case label is not NULL, failure to biased-lock the object
3176 //   transfers control to the location of the slow_case label. If the
3177 //   object could be biased-locked, control is transferred to the done label.
3178 //   The condition code is unpredictable.
3179 //
3180 //   If the slow_case label is NULL, failure to biased-lock the object results
3181 //   in a transfer of control to the done label with a condition code of not_equal.
3182 //   If the biased-lock could be successfully obtained, control is transfered to
</pre>
<hr />
<pre>
3184 //   It is mandatory to react on the condition code At the done label.
3185 //
3186 void MacroAssembler::biased_locking_enter(Register  obj_reg,
3187                                           Register  mark_reg,
3188                                           Register  temp_reg,
3189                                           Register  temp2_reg,    // May be Z_RO!
3190                                           Label    &amp;done,
3191                                           Label    *slow_case) {
3192   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
3193   assert_different_registers(obj_reg, mark_reg, temp_reg, temp2_reg);
3194 
3195   Label cas_label; // Try, if implemented, CAS locking. Fall thru to slow path otherwise.
3196 
3197   BLOCK_COMMENT(&quot;biased_locking_enter {&quot;);
3198 
3199   // Biased locking
3200   // See whether the lock is currently biased toward our thread and
3201   // whether the epoch is still valid.
3202   // Note that the runtime guarantees sufficient alignment of JavaThread
3203   // pointers to allow age to be placed into low bits.
<span class="line-modified">3204   assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits,</span>
3205          &quot;biased locking makes assumptions about bit layout&quot;);
3206   z_lr(temp_reg, mark_reg);
<span class="line-modified">3207   z_nilf(temp_reg, markWord::biased_lock_mask_in_place);</span>
<span class="line-modified">3208   z_chi(temp_reg, markWord::biased_lock_pattern);</span>
3209   z_brne(cas_label);  // Try cas if object is not biased, i.e. cannot be biased locked.
3210 
3211   load_prototype_header(temp_reg, obj_reg);
<span class="line-modified">3212   load_const_optimized(temp2_reg, ~((int) markWord::age_mask_in_place));</span>
3213 
3214   z_ogr(temp_reg, Z_thread);
3215   z_xgr(temp_reg, mark_reg);
3216   z_ngr(temp_reg, temp2_reg);
3217   if (PrintBiasedLockingStatistics) {
3218     increment_counter_eq((address) BiasedLocking::biased_lock_entry_count_addr(), mark_reg, temp2_reg);
3219     // Restore mark_reg.
3220     z_lg(mark_reg, oopDesc::mark_offset_in_bytes(), obj_reg);
3221   }
3222   branch_optimized(Assembler::bcondEqual, done);  // Biased lock obtained, return success.
3223 
3224   Label try_revoke_bias;
3225   Label try_rebias;
3226   Address mark_addr = Address(obj_reg, oopDesc::mark_offset_in_bytes());
3227 
3228   //----------------------------------------------------------------------------
3229   // At this point we know that the header has the bias pattern and
3230   // that we are not the bias owner in the current epoch. We need to
3231   // figure out more details about the state of the header in order to
3232   // know what operations can be legally performed on the object&#39;s
3233   // header.
3234 
3235   // If the low three bits in the xor result aren&#39;t clear, that means
3236   // the prototype header is no longer biased and we have to revoke
3237   // the bias on this object.
<span class="line-modified">3238   z_tmll(temp_reg, markWord::biased_lock_mask_in_place);</span>
3239   z_brnaz(try_revoke_bias);
3240 
3241   // Biasing is still enabled for this data type. See whether the
3242   // epoch of the current bias is still valid, meaning that the epoch
3243   // bits of the mark word are equal to the epoch bits of the
3244   // prototype header. (Note that the prototype header&#39;s epoch bits
3245   // only change at a safepoint.) If not, attempt to rebias the object
3246   // toward the current thread. Note that we must be absolutely sure
3247   // that the current epoch is invalid in order to do this because
3248   // otherwise the manipulations it performs on the mark word are
3249   // illegal.
<span class="line-modified">3250   z_tmll(temp_reg, markWord::epoch_mask_in_place);</span>
3251   z_brnaz(try_rebias);
3252 
3253   //----------------------------------------------------------------------------
3254   // The epoch of the current bias is still valid but we know nothing
3255   // about the owner; it might be set or it might be clear. Try to
3256   // acquire the bias of the object using an atomic operation. If this
3257   // fails we will go in to the runtime to revoke the object&#39;s bias.
3258   // Note that we first construct the presumed unbiased header so we
3259   // don&#39;t accidentally blow away another thread&#39;s valid bias.
<span class="line-modified">3260   z_nilf(mark_reg, markWord::biased_lock_mask_in_place | markWord::age_mask_in_place |</span>
<span class="line-modified">3261          markWord::epoch_mask_in_place);</span>
3262   z_lgr(temp_reg, Z_thread);
3263   z_llgfr(mark_reg, mark_reg);
3264   z_ogr(temp_reg, mark_reg);
3265 
3266   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
3267 
3268   z_csg(mark_reg, temp_reg, 0, obj_reg);
3269 
3270   // If the biasing toward our thread failed, this means that
3271   // another thread succeeded in biasing it toward itself and we
3272   // need to revoke that bias. The revocation will occur in the
3273   // interpreter runtime in the slow case.
3274 
3275   if (PrintBiasedLockingStatistics) {
3276     increment_counter_eq((address) BiasedLocking::anonymously_biased_lock_entry_count_addr(),
3277                          temp_reg, temp2_reg);
3278   }
3279   if (slow_case != NULL) {
3280     branch_optimized(Assembler::bcondNotEqual, *slow_case); // Biased lock not obtained, need to go the long way.
3281   }
3282   branch_optimized(Assembler::bcondAlways, done);           // Biased lock status given in condition code.
3283 
3284   //----------------------------------------------------------------------------
3285   bind(try_rebias);
3286   // At this point we know the epoch has expired, meaning that the
3287   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
3288   // circumstances _only_, we are allowed to use the current header&#39;s
3289   // value as the comparison value when doing the cas to acquire the
3290   // bias in the current epoch. In other words, we allow transfer of
3291   // the bias from one thread to another directly in this situation.
3292 
<span class="line-modified">3293   z_nilf(mark_reg, markWord::biased_lock_mask_in_place | markWord::age_mask_in_place | markWord::epoch_mask_in_place);</span>
3294   load_prototype_header(temp_reg, obj_reg);
3295   z_llgfr(mark_reg, mark_reg);
3296 
3297   z_ogr(temp_reg, Z_thread);
3298 
3299   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
3300 
3301   z_csg(mark_reg, temp_reg, 0, obj_reg);
3302 
3303   // If the biasing toward our thread failed, this means that
3304   // another thread succeeded in biasing it toward itself and we
3305   // need to revoke that bias. The revocation will occur in the
3306   // interpreter runtime in the slow case.
3307 
3308   if (PrintBiasedLockingStatistics) {
3309     increment_counter_eq((address) BiasedLocking::rebiased_lock_entry_count_addr(), temp_reg, temp2_reg);
3310   }
3311   if (slow_case != NULL) {
3312     branch_optimized(Assembler::bcondNotEqual, *slow_case);  // Biased lock not obtained, need to go the long way.
3313   }
</pre>
<hr />
<pre>
3334   // removing the bias bit from the object&#39;s header.
3335   if (PrintBiasedLockingStatistics) {
3336     // z_cgr(mark_reg, temp2_reg);
3337     increment_counter_eq((address) BiasedLocking::revoked_lock_entry_count_addr(), temp_reg, temp2_reg);
3338   }
3339 
3340   bind(cas_label);
3341   BLOCK_COMMENT(&quot;} biased_locking_enter&quot;);
3342 }
3343 
3344 void MacroAssembler::biased_locking_exit(Register mark_addr, Register temp_reg, Label&amp; done) {
3345   // Check for biased locking unlock case, which is a no-op
3346   // Note: we do not have to check the thread ID for two reasons.
3347   // First, the interpreter checks for IllegalMonitorStateException at
3348   // a higher level. Second, if the bias was revoked while we held the
3349   // lock, the object could not be rebiased toward another thread, so
3350   // the bias bit would be clear.
3351   BLOCK_COMMENT(&quot;biased_locking_exit {&quot;);
3352 
3353   z_lg(temp_reg, 0, mark_addr);
<span class="line-modified">3354   z_nilf(temp_reg, markWord::biased_lock_mask_in_place);</span>
3355 
<span class="line-modified">3356   z_chi(temp_reg, markWord::biased_lock_pattern);</span>
3357   z_bre(done);
3358   BLOCK_COMMENT(&quot;} biased_locking_exit&quot;);
3359 }
3360 
3361 void MacroAssembler::compiler_fast_lock_object(Register oop, Register box, Register temp1, Register temp2, bool try_bias) {
3362   Register displacedHeader = temp1;
3363   Register currentHeader = temp1;
3364   Register temp = temp2;
3365   NearLabel done, object_has_monitor;
3366 
3367   BLOCK_COMMENT(&quot;compiler_fast_lock_object {&quot;);
3368 
<span class="line-modified">3369   // Load markWord from oop into mark.</span>
3370   z_lg(displacedHeader, 0, oop);
3371 
3372   if (try_bias) {
3373     biased_locking_enter(oop, displacedHeader, temp, Z_R0, done);
3374   }
3375 
3376   // Handle existing monitor.
3377   // The object has an existing monitor iff (mark &amp; monitor_value) != 0.
<span class="line-modified">3378   guarantee(Immediate::is_uimm16(markWord::monitor_value), &quot;must be half-word&quot;);</span>
3379   z_lr(temp, displacedHeader);
<span class="line-modified">3380   z_nill(temp, markWord::monitor_value);</span>
3381   z_brne(object_has_monitor);
3382 
<span class="line-modified">3383   // Set mark to markWord | markWord::unlocked_value.</span>
<span class="line-modified">3384   z_oill(displacedHeader, markWord::unlocked_value);</span>
3385 
3386   // Load Compare Value application register.
3387 
3388   // Initialize the box (must happen before we update the object mark).
3389   z_stg(displacedHeader, BasicLock::displaced_header_offset_in_bytes(), box);
3390 
3391   // Memory Fence (in cmpxchgd)
<span class="line-modified">3392   // Compare object markWord with mark and if equal exchange scratch1 with object markWord.</span>
3393 
3394   // If the compare-and-swap succeeded, then we found an unlocked object and we
3395   // have now locked it.
3396   z_csg(displacedHeader, box, 0, oop);
3397   assert(currentHeader==displacedHeader, &quot;must be same register&quot;); // Identified two registers from z/Architecture.
3398   z_bre(done);
3399 
3400   // We did not see an unlocked object so try the fast recursive case.
3401 
3402   z_sgr(currentHeader, Z_SP);
<span class="line-modified">3403   load_const_optimized(temp, (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));</span>
3404 
3405   z_ngr(currentHeader, temp);
3406   //   z_brne(done);
3407   //   z_release();
3408   z_stg(currentHeader/*==0 or not 0*/, BasicLock::displaced_header_offset_in_bytes(), box);
3409 
3410   z_bru(done);
3411 
3412   Register zero = temp;
<span class="line-modified">3413   Register monitor_tagged = displacedHeader; // Tagged with markWord::monitor_value.</span>
3414   bind(object_has_monitor);
3415   // The object&#39;s monitor m is unlocked iff m-&gt;owner == NULL,
3416   // otherwise m-&gt;owner may contain a thread or a stack address.
3417   //
3418   // Try to CAS m-&gt;owner from NULL to current thread.
3419   z_lghi(zero, 0);
3420   // If m-&gt;owner is null, then csg succeeds and sets m-&gt;owner=THREAD and CR=EQ.
3421   z_csg(zero, Z_thread, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner), monitor_tagged);
3422   // Store a non-null value into the box.
3423   z_stg(box, BasicLock::displaced_header_offset_in_bytes(), box);
3424 #ifdef ASSERT
3425   z_brne(done);
3426   // We&#39;ve acquired the monitor, check some invariants.
3427   // Invariant 1: _recursions should be 0.
3428   asm_assert_mem8_is_zero(OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions), monitor_tagged,
3429                           &quot;monitor-&gt;_recursions should be 0&quot;, -1);
3430   z_ltgr(zero, zero); // Set CR=EQ.
3431 #endif
3432   bind(done);
3433 
</pre>
<hr />
<pre>
3442   Register currentHeader = temp2;
3443   Register temp = temp1;
3444   Register monitor = temp2;
3445 
3446   Label done, object_has_monitor;
3447 
3448   BLOCK_COMMENT(&quot;compiler_fast_unlock_object {&quot;);
3449 
3450   if (try_bias) {
3451     biased_locking_exit(oop, currentHeader, done);
3452   }
3453 
3454   // Find the lock address and load the displaced header from the stack.
3455   // if the displaced header is zero, we have a recursive unlock.
3456   load_and_test_long(displacedHeader, Address(box, BasicLock::displaced_header_offset_in_bytes()));
3457   z_bre(done);
3458 
3459   // Handle existing monitor.
3460   // The object has an existing monitor iff (mark &amp; monitor_value) != 0.
3461   z_lg(currentHeader, oopDesc::mark_offset_in_bytes(), oop);
<span class="line-modified">3462   guarantee(Immediate::is_uimm16(markWord::monitor_value), &quot;must be half-word&quot;);</span>
<span class="line-modified">3463   z_nill(currentHeader, markWord::monitor_value);</span>
3464   z_brne(object_has_monitor);
3465 
3466   // Check if it is still a light weight lock, this is true if we see
<span class="line-modified">3467   // the stack address of the basicLock in the markWord of the object</span>
3468   // copy box to currentHeader such that csg does not kill it.
3469   z_lgr(currentHeader, box);
3470   z_csg(currentHeader, displacedHeader, 0, oop);
3471   z_bru(done); // Csg sets CR as desired.
3472 
3473   // Handle existing monitor.
3474   bind(object_has_monitor);
3475   z_lg(currentHeader, oopDesc::mark_offset_in_bytes(), oop);    // CurrentHeader is tagged with monitor_value set.
3476   load_and_test_long(temp, Address(currentHeader, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));
3477   z_brne(done);
3478   load_and_test_long(temp, Address(currentHeader, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
3479   z_brne(done);
3480   load_and_test_long(temp, Address(currentHeader, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));
3481   z_brne(done);
3482   load_and_test_long(temp, Address(currentHeader, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));
3483   z_brne(done);
3484   z_release();
3485   z_stg(temp/*=0*/, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner), currentHeader);
3486 
3487   bind(done);
</pre>
<hr />
<pre>
3571   // Therefore we load the PC into tmp1 and let set_last_Java_frame() save
3572   // it into the frame anchor.
3573   get_PC(tmp1);
3574   set_last_Java_frame(/*sp=*/sp, /*pc=*/tmp1, allow_relocation);
3575 }
3576 
3577 void MacroAssembler::set_thread_state(JavaThreadState new_state) {
3578   z_release();
3579 
3580   assert(Immediate::is_uimm16(_thread_max_state), &quot;enum value out of range for instruction&quot;);
3581   assert(sizeof(JavaThreadState) == sizeof(int), &quot;enum value must have base type int&quot;);
3582   store_const(Address(Z_thread, JavaThread::thread_state_offset()), new_state, Z_R0, false);
3583 }
3584 
3585 void MacroAssembler::get_vm_result(Register oop_result) {
3586   verify_thread();
3587 
3588   z_lg(oop_result, Address(Z_thread, JavaThread::vm_result_offset()));
3589   clear_mem(Address(Z_thread, JavaThread::vm_result_offset()), sizeof(void*));
3590 
<span class="line-modified">3591   verify_oop(oop_result, FILE_AND_LINE);</span>
3592 }
3593 
3594 void MacroAssembler::get_vm_result_2(Register result) {
3595   verify_thread();
3596 
3597   z_lg(result, Address(Z_thread, JavaThread::vm_result_2_offset()));
3598   clear_mem(Address(Z_thread, JavaThread::vm_result_2_offset()), sizeof(void*));
3599 }
3600 
3601 // We require that C code which does not return a value in vm_result will
3602 // leave it undisturbed.
3603 void MacroAssembler::set_vm_result(Register oop_result) {
3604   z_stg(oop_result, Address(Z_thread, JavaThread::vm_result_offset()));
3605 }
3606 
3607 // Explicit null checks (used for method handle code).
3608 void MacroAssembler::null_check(Register reg, Register tmp, int64_t offset) {
3609   if (!ImplicitNullChecks) {
3610     NearLabel ok;
3611 
</pre>
<hr />
<pre>
3619     bind(ok);
3620   } else {
3621     if (needs_explicit_null_check((intptr_t)offset)) {
3622       // Provoke OS NULL exception if reg = NULL by
3623       // accessing M[reg] w/o changing any registers.
3624       z_lg(tmp, 0, reg);
3625     }
3626     // else
3627       // Nothing to do, (later) access of M[reg + offset]
3628       // will provoke OS NULL exception if reg = NULL.
3629   }
3630 }
3631 
3632 //-------------------------------------
3633 //  Compressed Klass Pointers
3634 //-------------------------------------
3635 
3636 // Klass oop manipulations if compressed.
3637 void MacroAssembler::encode_klass_not_null(Register dst, Register src) {
3638   Register current = (src != noreg) ? src : dst; // Klass is in dst if no src provided. (dst == src) also possible.
<span class="line-modified">3639   address  base    = CompressedKlassPointers::base();</span>
<span class="line-modified">3640   int      shift   = CompressedKlassPointers::shift();</span>
3641   assert(UseCompressedClassPointers, &quot;only for compressed klass ptrs&quot;);
3642 
3643   BLOCK_COMMENT(&quot;cKlass encoder {&quot;);
3644 
3645 #ifdef ASSERT
3646   Label ok;
3647   z_tmll(current, KlassAlignmentInBytes-1); // Check alignment.
3648   z_brc(Assembler::bcondAllZero, ok);
3649   // The plain disassembler does not recognize illtrap. It instead displays
3650   // a 32-bit value. Issueing two illtraps assures the disassembler finds
3651   // the proper beginning of the next instruction.
3652   z_illtrap(0xee);
3653   z_illtrap(0xee);
3654   bind(ok);
3655 #endif
3656 
3657   if (base != NULL) {
3658     unsigned int base_h = ((unsigned long)base)&gt;&gt;32;
3659     unsigned int base_l = (unsigned int)((unsigned long)base);
3660     if ((base_h != 0) &amp;&amp; (base_l == 0) &amp;&amp; VM_Version::has_HighWordInstr()) {
</pre>
<hr />
<pre>
3668       lgr_if_needed(dst, current);
3669       z_sgr(dst, Z_R0);
3670     }
3671     current = dst;
3672   }
3673   if (shift != 0) {
3674     assert (LogKlassAlignmentInBytes == shift, &quot;decode alg wrong&quot;);
3675     z_srlg(dst, current, shift);
3676     current = dst;
3677   }
3678   lgr_if_needed(dst, current); // Move may be required (if neither base nor shift != 0).
3679 
3680   BLOCK_COMMENT(&quot;} cKlass encoder&quot;);
3681 }
3682 
3683 // This function calculates the size of the code generated by
3684 //   decode_klass_not_null(register dst, Register src)
3685 // when (Universe::heap() != NULL). Hence, if the instructions
3686 // it generates change, then this method needs to be updated.
3687 int MacroAssembler::instr_size_for_decode_klass_not_null() {
<span class="line-modified">3688   address  base    = CompressedKlassPointers::base();</span>
<span class="line-modified">3689   int shift_size   = CompressedKlassPointers::shift() == 0 ? 0 : 6; /* sllg */</span>
3690   int addbase_size = 0;
3691   assert(UseCompressedClassPointers, &quot;only for compressed klass ptrs&quot;);
3692 
3693   if (base != NULL) {
3694     unsigned int base_h = ((unsigned long)base)&gt;&gt;32;
3695     unsigned int base_l = (unsigned int)((unsigned long)base);
3696     if ((base_h != 0) &amp;&amp; (base_l == 0) &amp;&amp; VM_Version::has_HighWordInstr()) {
3697       addbase_size += 6; /* aih */
3698     } else if ((base_h == 0) &amp;&amp; (base_l != 0)) {
3699       addbase_size += 6; /* algfi */
3700     } else {
3701       addbase_size += load_const_size();
3702       addbase_size += 4; /* algr */
3703     }
3704   }
3705 #ifdef ASSERT
3706   addbase_size += 10;
3707   addbase_size += 2; // Extra sigill.
3708 #endif
3709   return addbase_size + shift_size;
3710 }
3711 
3712 // !!! If the instructions that get generated here change
3713 //     then function instr_size_for_decode_klass_not_null()
3714 //     needs to get updated.
3715 // This variant of decode_klass_not_null() must generate predictable code!
3716 // The code must only depend on globally known parameters.
3717 void MacroAssembler::decode_klass_not_null(Register dst) {
<span class="line-modified">3718   address  base    = CompressedKlassPointers::base();</span>
<span class="line-modified">3719   int      shift   = CompressedKlassPointers::shift();</span>
3720   int      beg_off = offset();
3721   assert(UseCompressedClassPointers, &quot;only for compressed klass ptrs&quot;);
3722 
3723   BLOCK_COMMENT(&quot;cKlass decoder (const size) {&quot;);
3724 
3725   if (shift != 0) { // Shift required?
3726     z_sllg(dst, dst, shift);
3727   }
3728   if (base != NULL) {
3729     unsigned int base_h = ((unsigned long)base)&gt;&gt;32;
3730     unsigned int base_l = (unsigned int)((unsigned long)base);
3731     if ((base_h != 0) &amp;&amp; (base_l == 0) &amp;&amp; VM_Version::has_HighWordInstr()) {
3732       z_aih(dst, base_h);     // Base has no set bits in lower half.
3733     } else if ((base_h == 0) &amp;&amp; (base_l != 0)) {
3734       z_algfi(dst, base_l);   // Base has no set bits in upper half.
3735     } else {
3736       load_const(Z_R0, base); // Base has set bits everywhere.
3737       z_algr(dst, Z_R0);
3738     }
3739   }
</pre>
<hr />
<pre>
3741 #ifdef ASSERT
3742   Label ok;
3743   z_tmll(dst, KlassAlignmentInBytes-1); // Check alignment.
3744   z_brc(Assembler::bcondAllZero, ok);
3745   // The plain disassembler does not recognize illtrap. It instead displays
3746   // a 32-bit value. Issueing two illtraps assures the disassembler finds
3747   // the proper beginning of the next instruction.
3748   z_illtrap(0xd1);
3749   z_illtrap(0xd1);
3750   bind(ok);
3751 #endif
3752   assert(offset() == beg_off + instr_size_for_decode_klass_not_null(), &quot;Code gen mismatch.&quot;);
3753 
3754   BLOCK_COMMENT(&quot;} cKlass decoder (const size)&quot;);
3755 }
3756 
3757 // This variant of decode_klass_not_null() is for cases where
3758 //  1) the size of the generated instructions may vary
3759 //  2) the result is (potentially) stored in a register different from the source.
3760 void MacroAssembler::decode_klass_not_null(Register dst, Register src) {
<span class="line-modified">3761   address base  = CompressedKlassPointers::base();</span>
<span class="line-modified">3762   int     shift = CompressedKlassPointers::shift();</span>
3763   assert(UseCompressedClassPointers, &quot;only for compressed klass ptrs&quot;);
3764 
3765   BLOCK_COMMENT(&quot;cKlass decoder {&quot;);
3766 
3767   if (src == noreg) src = dst;
3768 
3769   if (shift != 0) { // Shift or at least move required?
3770     z_sllg(dst, src, shift);
3771   } else {
3772     lgr_if_needed(dst, src);
3773   }
3774 
3775   if (base != NULL) {
3776     unsigned int base_h = ((unsigned long)base)&gt;&gt;32;
3777     unsigned int base_l = (unsigned int)((unsigned long)base);
3778     if ((base_h != 0) &amp;&amp; (base_l == 0) &amp;&amp; VM_Version::has_HighWordInstr()) {
3779       z_aih(dst, base_h);     // Base has not set bits in lower half.
3780     } else if ((base_h == 0) &amp;&amp; (base_l != 0)) {
3781       z_algfi(dst, base_l);   // Base has no set bits in upper half.
3782     } else {
</pre>
<hr />
<pre>
3842     // Support s = noreg.
3843     if (s != noreg) {
3844       z_st(s, Address(d, oopDesc::klass_gap_offset_in_bytes()));
3845     } else {
3846       z_mvhi(Address(d, oopDesc::klass_gap_offset_in_bytes()), 0);
3847     }
3848   }
3849 }
3850 
3851 // Compare klass ptr in memory against klass ptr in register.
3852 //
3853 // Rop1            - klass in register, always uncompressed.
3854 // disp            - Offset of klass in memory, compressed/uncompressed, depending on runtime flag.
3855 // Rbase           - Base address of cKlass in memory.
3856 // maybeNULL       - True if Rop1 possibly is a NULL.
3857 void MacroAssembler::compare_klass_ptr(Register Rop1, int64_t disp, Register Rbase, bool maybeNULL) {
3858 
3859   BLOCK_COMMENT(&quot;compare klass ptr {&quot;);
3860 
3861   if (UseCompressedClassPointers) {
<span class="line-modified">3862     const int shift = CompressedKlassPointers::shift();</span>
<span class="line-modified">3863     address   base  = CompressedKlassPointers::base();</span>
3864 
3865     assert((shift == 0) || (shift == LogKlassAlignmentInBytes), &quot;cKlass encoder detected bad shift&quot;);
3866     assert_different_registers(Rop1, Z_R0);
3867     assert_different_registers(Rop1, Rbase, Z_R1);
3868 
3869     // First encode register oop and then compare with cOop in memory.
3870     // This sequence saves an unnecessary cOop load and decode.
3871     if (base == NULL) {
3872       if (shift == 0) {
3873         z_cl(Rop1, disp, Rbase);     // Unscaled
3874       } else {
3875         z_srlg(Z_R0, Rop1, shift);   // ZeroBased
3876         z_cl(Z_R0, disp, Rbase);
3877       }
3878     } else {                         // HeapBased
3879 #ifdef ASSERT
3880       bool     used_R0 = true;
3881       bool     used_R1 = true;
3882 #endif
3883       Register current = Rop1;
</pre>
<hr />
<pre>
3976   return pow2_offset;
3977 }
3978 
3979 int MacroAssembler::get_oop_base_complement(Register Rbase, uint64_t oop_base) {
3980   int offset = get_oop_base(Rbase, oop_base);
3981   z_lcgr(Rbase, Rbase);
3982   return -offset;
3983 }
3984 
3985 // Compare compressed oop in memory against oop in register.
3986 // Rop1            - Oop in register.
3987 // disp            - Offset of cOop in memory.
3988 // Rbase           - Base address of cOop in memory.
3989 // maybeNULL       - True if Rop1 possibly is a NULL.
3990 // maybeNULLtarget - Branch target for Rop1 == NULL, if flow control shall NOT continue with compare instruction.
3991 void MacroAssembler::compare_heap_oop(Register Rop1, Address mem, bool maybeNULL) {
3992   Register Rbase  = mem.baseOrR0();
3993   Register Rindex = mem.indexOrR0();
3994   int64_t  disp   = mem.disp();
3995 
<span class="line-modified">3996   const int shift = CompressedOops::shift();</span>
<span class="line-modified">3997   address   base  = CompressedOops::base();</span>
3998 
3999   assert(UseCompressedOops, &quot;must be on to call this method&quot;);
4000   assert(Universe::heap() != NULL, &quot;java heap must be initialized to call this method&quot;);
4001   assert((shift == 0) || (shift == LogMinObjAlignmentInBytes), &quot;cOop encoder detected bad shift&quot;);
4002   assert_different_registers(Rop1, Z_R0);
4003   assert_different_registers(Rop1, Rbase, Z_R1);
4004   assert_different_registers(Rop1, Rindex, Z_R1);
4005 
4006   BLOCK_COMMENT(&quot;compare heap oop {&quot;);
4007 
4008   // First encode register oop and then compare with cOop in memory.
4009   // This sequence saves an unnecessary cOop load and decode.
4010   if (base == NULL) {
4011     if (shift == 0) {
4012       z_cl(Rop1, disp, Rindex, Rbase);  // Unscaled
4013     } else {
4014       z_srlg(Z_R0, Rop1, shift);        // ZeroBased
4015       z_cl(Z_R0, disp, Rindex, Rbase);
4016     }
4017   } else {                              // HeapBased
</pre>
<hr />
<pre>
4088                                     Register tmp1, Register tmp2, Register tmp3,
4089                                     DecoratorSet decorators) {
4090   access_store_at(T_OBJECT, IN_HEAP | decorators, a, Roop, tmp1, tmp2, tmp3);
4091 }
4092 
4093 //-------------------------------------------------
4094 // Encode compressed oop. Generally usable encoder.
4095 //-------------------------------------------------
4096 // Rsrc - contains regular oop on entry. It remains unchanged.
4097 // Rdst - contains compressed oop on exit.
4098 // Rdst and Rsrc may indicate same register, in which case Rsrc does not remain unchanged.
4099 //
4100 // Rdst must not indicate scratch register Z_R1 (Z_R1_scratch) for functionality.
4101 // Rdst should not indicate scratch register Z_R0 (Z_R0_scratch) for performance.
4102 //
4103 // only32bitValid is set, if later code only uses the lower 32 bits. In this
4104 // case we must not fix the upper 32 bits.
4105 void MacroAssembler::oop_encoder(Register Rdst, Register Rsrc, bool maybeNULL,
4106                                  Register Rbase, int pow2_offset, bool only32bitValid) {
4107 
<span class="line-modified">4108   const address oop_base  = CompressedOops::base();</span>
<span class="line-modified">4109   const int     oop_shift = CompressedOops::shift();</span>
<span class="line-modified">4110   const bool    disjoint  = CompressedOops::base_disjoint();</span>
4111 
4112   assert(UseCompressedOops, &quot;must be on to call this method&quot;);
4113   assert(Universe::heap() != NULL, &quot;java heap must be initialized to call this encoder&quot;);
4114   assert((oop_shift == 0) || (oop_shift == LogMinObjAlignmentInBytes), &quot;cOop encoder detected bad shift&quot;);
4115 
4116   if (disjoint || (oop_base == NULL)) {
4117     BLOCK_COMMENT(&quot;cOop encoder zeroBase {&quot;);
4118     if (oop_shift == 0) {
4119       if (oop_base != NULL &amp;&amp; !only32bitValid) {
4120         z_llgfr(Rdst, Rsrc); // Clear upper bits in case the register will be decoded again.
4121       } else {
4122         lgr_if_needed(Rdst, Rsrc);
4123       }
4124     } else {
4125       z_srlg(Rdst, Rsrc, oop_shift);
4126       if (oop_base != NULL &amp;&amp; !only32bitValid) {
4127         z_llgfr(Rdst, Rdst); // Clear upper bits in case the register will be decoded again.
4128       }
4129     }
4130     BLOCK_COMMENT(&quot;} cOop encoder zeroBase&quot;);
</pre>
<hr />
<pre>
4223 #endif
4224   BLOCK_COMMENT(&quot;} cOop encoder general&quot;);
4225 }
4226 
4227 //-------------------------------------------------
4228 // decode compressed oop. Generally usable decoder.
4229 //-------------------------------------------------
4230 // Rsrc - contains compressed oop on entry.
4231 // Rdst - contains regular oop on exit.
4232 // Rdst and Rsrc may indicate same register.
4233 // Rdst must not be the same register as Rbase, if Rbase was preloaded (before call).
4234 // Rdst can be the same register as Rbase. Then, either Z_R0 or Z_R1 must be available as scratch.
4235 // Rbase - register to use for the base
4236 // pow2_offset - offset of base to nice value. If -1, base must be loaded.
4237 // For performance, it is good to
4238 //  - avoid Z_R0 for any of the argument registers.
4239 //  - keep Rdst and Rsrc distinct from Rbase. Rdst == Rsrc is ok for performance.
4240 //  - avoid Z_R1 for Rdst if Rdst == Rbase.
4241 void MacroAssembler::oop_decoder(Register Rdst, Register Rsrc, bool maybeNULL, Register Rbase, int pow2_offset) {
4242 
<span class="line-modified">4243   const address oop_base  = CompressedOops::base();</span>
<span class="line-modified">4244   const int     oop_shift = CompressedOops::shift();</span>
<span class="line-modified">4245   const bool    disjoint  = CompressedOops::base_disjoint();</span>
4246 
4247   assert(UseCompressedOops, &quot;must be on to call this method&quot;);
4248   assert(Universe::heap() != NULL, &quot;java heap must be initialized to call this decoder&quot;);
4249   assert((oop_shift == 0) || (oop_shift == LogMinObjAlignmentInBytes),
4250          &quot;cOop encoder detected bad shift&quot;);
4251 
4252   // cOops are always loaded zero-extended from memory. No explicit zero-extension necessary.
4253 
4254   if (oop_base != NULL) {
4255     unsigned int oop_base_hl = ((unsigned int)((uint64_t)(intptr_t)oop_base &gt;&gt; 32)) &amp; 0xffff;
4256     unsigned int oop_base_hh = ((unsigned int)((uint64_t)(intptr_t)oop_base &gt;&gt; 48)) &amp; 0xffff;
4257     unsigned int oop_base_hf = ((unsigned int)((uint64_t)(intptr_t)oop_base &gt;&gt; 32)) &amp; 0xFFFFffff;
4258     if (disjoint &amp;&amp; (oop_base_hl == 0 || oop_base_hh == 0)) {
4259       BLOCK_COMMENT(&quot;cOop decoder disjointBase {&quot;);
4260       // We do not need to load the base. Instead, we can install the upper bits
4261       // with an OR instead of an ADD.
4262       Label done;
4263 
4264       // Rsrc contains a narrow oop. Thus we are sure the leftmost &lt;oop_shift&gt; bits will never be set.
4265       if (maybeNULL) {  // NULL ptr must be preserved!
</pre>
<hr />
<pre>
4352 #endif
4353       BLOCK_COMMENT(&quot;} cOop decoder general&quot;);
4354     }
4355   } else {
4356     BLOCK_COMMENT(&quot;cOop decoder zeroBase {&quot;);
4357     if (oop_shift == 0) {
4358       lgr_if_needed(Rdst, Rsrc);
4359     } else {
4360       z_sllg(Rdst, Rsrc, oop_shift);
4361     }
4362     BLOCK_COMMENT(&quot;} cOop decoder zeroBase&quot;);
4363   }
4364 }
4365 
4366 // ((OopHandle)result).resolve();
4367 void MacroAssembler::resolve_oop_handle(Register result) {
4368   // OopHandle::resolve is an indirection.
4369   z_lg(result, 0, result);
4370 }
4371 
<span class="line-modified">4372 void MacroAssembler::load_mirror_from_const_method(Register mirror, Register const_method) {</span>
<span class="line-modified">4373   mem2reg_opt(mirror, Address(const_method, ConstMethod::constants_offset()));</span>

4374   mem2reg_opt(mirror, Address(mirror, ConstantPool::pool_holder_offset_in_bytes()));
4375   mem2reg_opt(mirror, Address(mirror, Klass::java_mirror_offset()));
4376   resolve_oop_handle(mirror);
4377 }
4378 
<span class="line-added">4379 void MacroAssembler::load_method_holder(Register holder, Register method) {</span>
<span class="line-added">4380   mem2reg_opt(holder, Address(method, Method::const_offset()));</span>
<span class="line-added">4381   mem2reg_opt(holder, Address(holder, ConstMethod::constants_offset()));</span>
<span class="line-added">4382   mem2reg_opt(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes()));</span>
<span class="line-added">4383 }</span>
<span class="line-added">4384 </span>
4385 //---------------------------------------------------------------
4386 //---  Operations on arrays.
4387 //---------------------------------------------------------------
4388 
4389 // Compiler ensures base is doubleword aligned and cnt is #doublewords.
4390 // Emitter does not KILL cnt and base arguments, since they need to be copied to
4391 // work registers anyway.
4392 // Actually, only r0, r1, and r5 are killed.
<span class="line-modified">4393 unsigned int MacroAssembler::Clear_Array(Register cnt_arg, Register base_pointer_arg, Register odd_tmp_reg) {</span>


4394 
4395   int      block_start = offset();

4396   Register dst_len  = Z_R1;    // Holds dst len  for MVCLE.
4397   Register dst_addr = Z_R0;    // Holds dst addr for MVCLE.
4398 
4399   Label doXC, doMVCLE, done;
4400 
4401   BLOCK_COMMENT(&quot;Clear_Array {&quot;);
4402 
4403   // Check for zero len and convert to long.
<span class="line-modified">4404   z_ltgfr(odd_tmp_reg, cnt_arg);</span>
4405   z_bre(done);                    // Nothing to do if len == 0.
4406 
4407   // Prefetch data to be cleared.
4408   if (VM_Version::has_Prefetch()) {
4409     z_pfd(0x02,   0, Z_R0, base_pointer_arg);
4410     z_pfd(0x02, 256, Z_R0, base_pointer_arg);
4411   }
4412 
<span class="line-modified">4413   z_sllg(dst_len, odd_tmp_reg, 3); // #bytes to clear.</span>
<span class="line-modified">4414   z_cghi(odd_tmp_reg, 32);         // Check for len &lt;= 256 bytes (&lt;=32 DW).</span>
<span class="line-modified">4415   z_brnh(doXC);                    // If so, use executed XC to clear.</span>
4416 
4417   // MVCLE: initialize long arrays (general case).
4418   bind(doMVCLE);
4419   z_lgr(dst_addr, base_pointer_arg);
<span class="line-modified">4420   // Pass 0 as source length to MVCLE: destination will be filled with padding byte 0.</span>
<span class="line-modified">4421   // The even register of the register pair is not killed.</span>
<span class="line-modified">4422   clear_reg(odd_tmp_reg, true, false);</span>
<span class="line-added">4423   MacroAssembler::move_long_ext(dst_addr, as_Register(odd_tmp_reg-&gt;encoding()-1), 0);</span>
4424   z_bru(done);
4425 
4426   // XC: initialize short arrays.
4427   Label XC_template; // Instr template, never exec directly!
4428     bind(XC_template);
4429     z_xc(0,0,base_pointer_arg,0,base_pointer_arg);
4430 
4431   bind(doXC);
<span class="line-modified">4432     add2reg(dst_len, -1);               // Get #bytes-1 for EXECUTE.</span>
4433     if (VM_Version::has_ExecuteExtensions()) {
<span class="line-modified">4434       z_exrl(dst_len, XC_template);     // Execute XC with var. len.</span>
4435     } else {
<span class="line-modified">4436       z_larl(odd_tmp_reg, XC_template);</span>
<span class="line-modified">4437       z_ex(dst_len,0,Z_R0,odd_tmp_reg); // Execute XC with var. len.</span>
4438     }
4439     // z_bru(done);      // fallthru
4440 
4441   bind(done);
4442 
4443   BLOCK_COMMENT(&quot;} Clear_Array&quot;);
4444 
4445   int block_end = offset();
4446   return block_end - block_start;
4447 }
4448 
4449 // Compiler ensures base is doubleword aligned and cnt is count of doublewords.
4450 // Emitter does not KILL any arguments nor work registers.
4451 // Emitter generates up to 16 XC instructions, depending on the array length.
4452 unsigned int MacroAssembler::Clear_Array_Const(long cnt, Register base) {
4453   int  block_start    = offset();
4454   int  off;
4455   int  lineSize_Bytes = AllocatePrefetchStepSize;
4456   int  lineSize_DW    = AllocatePrefetchStepSize&gt;&gt;LogBytesPerWord;
4457   bool doPrefetch     = VM_Version::has_Prefetch();
</pre>
<hr />
<pre>
4479       z_xc(off*XC_maxlen, XC_maxlen-1, base, off*XC_maxlen, base);
4480 
4481       // Prefetch some cache lines in advance.
4482       if (doPrefetch &amp;&amp; (off &lt;= numXCInstr-AllocatePrefetchLines)) {
4483         z_pfd(0x02, (off+AllocatePrefetchLines)*lineSize_Bytes, Z_R0, base);
4484       }
4485     }
4486     if (off*XC_maxlen &lt; cnt*BytesPerWord) {
4487       z_xc(off*XC_maxlen, (cnt*BytesPerWord-off*XC_maxlen)-1, base, off*XC_maxlen, base);
4488     }
4489   }
4490   BLOCK_COMMENT(&quot;} Clear_Array_Const&quot;);
4491 
4492   int block_end = offset();
4493   return block_end - block_start;
4494 }
4495 
4496 // Compiler ensures base is doubleword aligned and cnt is #doublewords.
4497 // Emitter does not KILL cnt and base arguments, since they need to be copied to
4498 // work registers anyway.
<span class="line-modified">4499 // Actually, only r0, r1, (which are work registers) and odd_tmp_reg are killed.</span>
4500 //
4501 // For very large arrays, exploit MVCLE H/W support.
4502 // MVCLE instruction automatically exploits H/W-optimized page mover.
4503 // - Bytes up to next page boundary are cleared with a series of XC to self.
4504 // - All full pages are cleared with the page mover H/W assist.
4505 // - Remaining bytes are again cleared by a series of XC to self.
4506 //
<span class="line-modified">4507 unsigned int MacroAssembler::Clear_Array_Const_Big(long cnt, Register base_pointer_arg, Register odd_tmp_reg) {</span>


4508 
4509   int      block_start = offset();
4510   Register dst_len  = Z_R1;      // Holds dst len  for MVCLE.
4511   Register dst_addr = Z_R0;      // Holds dst addr for MVCLE.
4512 
4513   BLOCK_COMMENT(&quot;Clear_Array_Const_Big {&quot;);
4514 
4515   // Get len to clear.
4516   load_const_optimized(dst_len, (long)cnt*8L);  // in Bytes = #DW*8
4517 
4518   // Prepare other args to MVCLE.
4519   z_lgr(dst_addr, base_pointer_arg);
<span class="line-modified">4520   // Pass 0 as source length to MVCLE: destination will be filled with padding byte 0.</span>
<span class="line-modified">4521   // The even register of the register pair is not killed.</span>
<span class="line-modified">4522   (void) clear_reg(odd_tmp_reg, true, false);  // Src len of MVCLE is zero.</span>
<span class="line-modified">4523   MacroAssembler::move_long_ext(dst_addr, as_Register(odd_tmp_reg-&gt;encoding() - 1), 0);</span>

4524   BLOCK_COMMENT(&quot;} Clear_Array_Const_Big&quot;);
4525 
4526   int block_end = offset();
4527   return block_end - block_start;
4528 }
4529 
4530 // Allocator.
4531 unsigned int MacroAssembler::CopyRawMemory_AlignedDisjoint(Register src_reg, Register dst_reg,
4532                                                            Register cnt_reg,
4533                                                            Register tmp1_reg, Register tmp2_reg) {
4534   // Tmp1 is oddReg.
4535   // Tmp2 is evenReg.
4536 
4537   int block_start = offset();
4538   Label doMVC, doMVCLE, done, MVC_template;
4539 
4540   BLOCK_COMMENT(&quot;CopyRawMemory_AlignedDisjoint {&quot;);
4541 
4542   // Check for zero len and convert to long.
4543   z_ltgfr(cnt_reg, cnt_reg);      // Remember casted value for doSTG case.
</pre>
<hr />
<pre>
4576     z_pfd(1,  0,Z_R0,src_reg);
4577     z_pfd(2,  0,Z_R0,dst_reg);
4578     //    z_pfd(1,256,Z_R0,src_reg);    // Assume very short copy.
4579     //    z_pfd(2,256,Z_R0,dst_reg);
4580   }
4581 
4582   if (VM_Version::has_ExecuteExtensions()) {
4583     z_exrl(Z_R1, MVC_template);
4584   } else {
4585     z_ex(tmp1_reg, 0, Z_R0, Z_R1);
4586   }
4587 
4588   bind(done);
4589 
4590   BLOCK_COMMENT(&quot;} CopyRawMemory_AlignedDisjoint&quot;);
4591 
4592   int block_end = offset();
4593   return block_end - block_start;
4594 }
4595 
<span class="line-added">4596 #ifdef COMPILER2</span>
4597 //------------------------------------------------------
4598 //   Special String Intrinsics. Implementation
4599 //------------------------------------------------------
4600 
4601 // Intrinsics for CompactStrings
4602 
4603 // Compress char[] to byte[].
4604 //   Restores: src, dst
4605 //   Uses:     cnt
4606 //   Kills:    tmp, Z_R0, Z_R1.
4607 //   Early clobber: result.
4608 // Note:
4609 //   cnt is signed int. Do not rely on high word!
4610 //       counts # characters, not bytes.
4611 // The result is the number of characters copied before the first incompatible character was found.
4612 // If precise is true, the processing stops exactly at this point. Otherwise, the result may be off
4613 // by a few bytes. The result always indicates the number of copied characters.
4614 // When used as a character index, the returned value points to the first incompatible character.
4615 //
4616 // Note: Does not behave exactly like package private StringUTF16 compress java implementation in case of failure:
</pre>
<hr />
<pre>
5824   }
5825   z_brc(Assembler::bcondNotFound, Ldone);
5826   if (is_byte) {
5827     if (VM_Version::has_DistinctOpnds()) {
5828       z_sgrk(result, odd_reg, haystack);
5829     } else {
5830       z_sgr(odd_reg, haystack);
5831       z_lgr(result, odd_reg);
5832     }
5833   } else {
5834     z_slgr(odd_reg, haystack);
5835     z_srlg(result, odd_reg, exact_log2(sizeof(jchar)));
5836   }
5837 
5838   bind(Ldone);
5839   }
5840   BLOCK_COMMENT(&quot;} string_indexof_char&quot;);
5841 
5842   return offset() - block_start;
5843 }
<span class="line-modified">5844 #endif</span>
5845 
5846 //-------------------------------------------------
5847 //   Constants (scalar and oop) in constant pool
5848 //-------------------------------------------------
5849 
5850 // Add a non-relocated constant to the CP.
5851 int MacroAssembler::store_const_in_toc(AddressLiteral&amp; val) {
5852   long    value  = val.value();
5853   address tocPos = long_constant(value);
5854 
5855   if (tocPos != NULL) {
5856     int tocOffset = (int)(tocPos - code()-&gt;consts()-&gt;start());
5857     return tocOffset;
5858   }
5859   // Address_constant returned NULL, so no constant entry has been created.
5860   // In that case, we return a &quot;fatal&quot; offset, just in case that subsequently
5861   // generated access code is executed.
5862   return -1;
5863 }
5864 
</pre>
<hr />
<pre>
5878     // Store toc_offset in relocation, used by call_far_patchable.
5879     if ((relocInfo::relocType)rel-&gt;type() == relocInfo::runtime_call_w_cp_type) {
5880       ((runtime_call_w_cp_Relocation *)(rel))-&gt;set_constant_pool_offset(tocOffset);
5881     }
5882     // Relocate at the load&#39;s pc.
5883     relocate(rsp);
5884 
5885     return tocOffset;
5886   }
5887   // Address_constant returned NULL, so no constant entry has been created
5888   // in that case, we return a &quot;fatal&quot; offset, just in case that subsequently
5889   // generated access code is executed.
5890   return -1;
5891 }
5892 
5893 bool MacroAssembler::load_const_from_toc(Register dst, AddressLiteral&amp; a, Register Rtoc) {
5894   int     tocOffset = store_const_in_toc(a);
5895   if (tocOffset == -1) return false;
5896   address tocPos    = tocOffset + code()-&gt;consts()-&gt;start();
5897   assert((address)code()-&gt;consts()-&gt;start() != NULL, &quot;Please add CP address&quot;);
<span class="line-modified">5898   relocate(a.rspec());</span>
5899   load_long_pcrelative(dst, tocPos);
5900   return true;
5901 }
5902 
5903 bool MacroAssembler::load_oop_from_toc(Register dst, AddressLiteral&amp; a, Register Rtoc) {
5904   int     tocOffset = store_oop_in_toc(a);
5905   if (tocOffset == -1) return false;
5906   address tocPos    = tocOffset + code()-&gt;consts()-&gt;start();
5907   assert((address)code()-&gt;consts()-&gt;start() != NULL, &quot;Please add CP address&quot;);
5908 
5909   load_addr_pcrelative(dst, tocPos);
5910   return true;
5911 }
5912 
5913 // If the instruction sequence at the given pc is a load_const_from_toc
5914 // sequence, return the value currently stored at the referenced position
5915 // in the TOC.
5916 intptr_t MacroAssembler::get_const_from_toc(address pc) {
5917 
5918   assert(is_load_const_from_toc(pc), &quot;must be load_const_from_pool&quot;);
</pre>
<hr />
<pre>
6137 void MacroAssembler::translate_to(Register r1, Register r2, uint m3) {
6138   assert(r1-&gt;encoding() % 2 == 0, &quot;dst addr/src len must be an even/odd register pair&quot;);
6139   assert((m3 &amp; 0b1110) == 0, &quot;Unused mask bits must be zero&quot;);
6140 
6141   Label retry;
6142   bind(retry);
6143   Assembler::z_trto(r1, r2, m3);
6144   Assembler::z_brc(Assembler::bcondOverflow /* CC==3 (iterate) */, retry);
6145 }
6146 
6147 void MacroAssembler::translate_tt(Register r1, Register r2, uint m3) {
6148   assert(r1-&gt;encoding() % 2 == 0, &quot;dst addr/src len must be an even/odd register pair&quot;);
6149   assert((m3 &amp; 0b1110) == 0, &quot;Unused mask bits must be zero&quot;);
6150 
6151   Label retry;
6152   bind(retry);
6153   Assembler::z_trtt(r1, r2, m3);
6154   Assembler::z_brc(Assembler::bcondOverflow /* CC==3 (iterate) */, retry);
6155 }
6156 


























































































6157 //---------------------------------------
6158 // Helpers for Intrinsic Emitters
6159 //---------------------------------------
6160 
6161 /**
6162  * uint32_t crc;
6163  * timesXtoThe32[crc &amp; 0xFF] ^ (crc &gt;&gt; 8);
6164  */
6165 void MacroAssembler::fold_byte_crc32(Register crc, Register val, Register table, Register tmp) {
6166   assert_different_registers(crc, table, tmp);
6167   assert_different_registers(val, table);
6168   if (crc == val) {      // Must rotate first to use the unmodified value.
6169     rotate_then_insert(tmp, val, 56-2, 63-2, 2, true);  // Insert byte 7 of val, shifted left by 2, into byte 6..7 of tmp, clear the rest.
6170     z_srl(crc, 8);       // Unsigned shift, clear leftmost 8 bits.
6171   } else {
6172     z_srl(crc, 8);       // Unsigned shift, clear leftmost 8 bits.
6173     rotate_then_insert(tmp, val, 56-2, 63-2, 2, true);  // Insert byte 7 of val, shifted left by 2, into byte 6..7 of tmp, clear the rest.
6174   }
6175   z_x(crc, Address(table, tmp, 0));
6176 }
</pre>
<hr />
<pre>
6797 void MacroAssembler::asm_assert_frame_size(Register expected_size, Register tmp, const char* msg, int id) {
6798   if (tmp == noreg) {
6799     tmp = expected_size;
6800   } else {
6801     if (tmp != expected_size) {
6802       z_lgr(tmp, expected_size);
6803     }
6804     z_algr(tmp, Z_SP);
6805     z_slg(tmp, 0, Z_R0, Z_SP);
6806     asm_assert_eq(msg, id);
6807   }
6808 }
6809 #endif // !PRODUCT
6810 
6811 void MacroAssembler::verify_thread() {
6812   if (VerifyThread) {
6813     unimplemented(&quot;&quot;, 117);
6814   }
6815 }
6816 
<span class="line-added">6817 // Save and restore functions: Exclude Z_R0.</span>
<span class="line-added">6818 void MacroAssembler::save_volatile_regs(Register dst, int offset, bool include_fp, bool include_flags) {</span>
<span class="line-added">6819   z_stmg(Z_R1, Z_R5, offset, dst); offset += 5 * BytesPerWord;</span>
<span class="line-added">6820   if (include_fp) {</span>
<span class="line-added">6821     z_std(Z_F0, Address(dst, offset)); offset += BytesPerWord;</span>
<span class="line-added">6822     z_std(Z_F1, Address(dst, offset)); offset += BytesPerWord;</span>
<span class="line-added">6823     z_std(Z_F2, Address(dst, offset)); offset += BytesPerWord;</span>
<span class="line-added">6824     z_std(Z_F3, Address(dst, offset)); offset += BytesPerWord;</span>
<span class="line-added">6825     z_std(Z_F4, Address(dst, offset)); offset += BytesPerWord;</span>
<span class="line-added">6826     z_std(Z_F5, Address(dst, offset)); offset += BytesPerWord;</span>
<span class="line-added">6827     z_std(Z_F6, Address(dst, offset)); offset += BytesPerWord;</span>
<span class="line-added">6828     z_std(Z_F7, Address(dst, offset)); offset += BytesPerWord;</span>
<span class="line-added">6829   }</span>
<span class="line-added">6830   if (include_flags) {</span>
<span class="line-added">6831     Label done;</span>
<span class="line-added">6832     z_mvi(Address(dst, offset), 2); // encoding: equal</span>
<span class="line-added">6833     z_bre(done);</span>
<span class="line-added">6834     z_mvi(Address(dst, offset), 4); // encoding: higher</span>
<span class="line-added">6835     z_brh(done);</span>
<span class="line-added">6836     z_mvi(Address(dst, offset), 1); // encoding: lower</span>
<span class="line-added">6837     bind(done);</span>
<span class="line-added">6838   }</span>
<span class="line-added">6839 }</span>
<span class="line-added">6840 void MacroAssembler::restore_volatile_regs(Register src, int offset, bool include_fp, bool include_flags) {</span>
<span class="line-added">6841   z_lmg(Z_R1, Z_R5, offset, src); offset += 5 * BytesPerWord;</span>
<span class="line-added">6842   if (include_fp) {</span>
<span class="line-added">6843     z_ld(Z_F0, Address(src, offset)); offset += BytesPerWord;</span>
<span class="line-added">6844     z_ld(Z_F1, Address(src, offset)); offset += BytesPerWord;</span>
<span class="line-added">6845     z_ld(Z_F2, Address(src, offset)); offset += BytesPerWord;</span>
<span class="line-added">6846     z_ld(Z_F3, Address(src, offset)); offset += BytesPerWord;</span>
<span class="line-added">6847     z_ld(Z_F4, Address(src, offset)); offset += BytesPerWord;</span>
<span class="line-added">6848     z_ld(Z_F5, Address(src, offset)); offset += BytesPerWord;</span>
<span class="line-added">6849     z_ld(Z_F6, Address(src, offset)); offset += BytesPerWord;</span>
<span class="line-added">6850     z_ld(Z_F7, Address(src, offset)); offset += BytesPerWord;</span>
<span class="line-added">6851   }</span>
<span class="line-added">6852   if (include_flags) {</span>
<span class="line-added">6853     z_cli(Address(src, offset), 2); // see encoding above</span>
<span class="line-added">6854   }</span>
<span class="line-added">6855 }</span>
<span class="line-added">6856 </span>
6857 // Plausibility check for oops.
6858 void MacroAssembler::verify_oop(Register oop, const char* msg) {
6859   if (!VerifyOops) return;
6860 
6861   BLOCK_COMMENT(&quot;verify_oop {&quot;);
<span class="line-modified">6862   unsigned int nbytes_save = (5 + 8 + 1) * BytesPerWord;</span>
<span class="line-modified">6863   address entry_addr = StubRoutines::verify_oop_subroutine_entry_address();</span>
<span class="line-modified">6864 </span>
<span class="line-added">6865   save_return_pc();</span>
<span class="line-added">6866 </span>
<span class="line-added">6867   // Push frame, but preserve flags</span>
<span class="line-added">6868   z_lgr(Z_R0, Z_SP);</span>
<span class="line-added">6869   z_lay(Z_SP, -((int64_t)nbytes_save + frame::z_abi_160_size), Z_SP);</span>
<span class="line-added">6870   z_stg(Z_R0, _z_abi(callers_sp), Z_SP);</span>
<span class="line-added">6871 </span>
<span class="line-added">6872   save_volatile_regs(Z_SP, frame::z_abi_160_size, true, true);</span>
<span class="line-added">6873 </span>
<span class="line-added">6874   lgr_if_needed(Z_ARG2, oop);</span>
<span class="line-added">6875   load_const_optimized(Z_ARG1, (address)msg);</span>
<span class="line-added">6876   load_const_optimized(Z_R1, entry_addr);</span>
<span class="line-added">6877   z_lg(Z_R1, 0, Z_R1);</span>
<span class="line-added">6878   call_c(Z_R1);</span>
<span class="line-added">6879 </span>
<span class="line-added">6880   restore_volatile_regs(Z_SP, frame::z_abi_160_size, true, true);</span>
<span class="line-added">6881   pop_frame();</span>
<span class="line-added">6882   restore_return_pc();</span>
<span class="line-added">6883 </span>
<span class="line-added">6884   BLOCK_COMMENT(&quot;} verify_oop &quot;);</span>
<span class="line-added">6885 }</span>
<span class="line-added">6886 </span>
<span class="line-added">6887 void MacroAssembler::verify_oop_addr(Address addr, const char* msg) {</span>
<span class="line-added">6888   if (!VerifyOops) return;</span>
<span class="line-added">6889 </span>
<span class="line-added">6890   BLOCK_COMMENT(&quot;verify_oop {&quot;);</span>
<span class="line-added">6891   unsigned int nbytes_save = (5 + 8) * BytesPerWord;</span>
<span class="line-added">6892   address entry_addr = StubRoutines::verify_oop_subroutine_entry_address();</span>
6893 
6894   save_return_pc();
<span class="line-modified">6895   unsigned int frame_size = push_frame_abi160(nbytes_save); // kills Z_R0</span>
<span class="line-modified">6896   save_volatile_regs(Z_SP, frame::z_abi_160_size, true, false);</span>
6897 
<span class="line-modified">6898   z_lg(Z_ARG2, addr.plus_disp(frame_size));</span>
<span class="line-modified">6899   load_const_optimized(Z_ARG1, (address)msg);</span>
<span class="line-modified">6900   load_const_optimized(Z_R1, entry_addr);</span>
6901   z_lg(Z_R1, 0, Z_R1);
6902   call_c(Z_R1);
6903 
<span class="line-modified">6904   restore_volatile_regs(Z_SP, frame::z_abi_160_size, true, false);</span>
6905   pop_frame();
6906   restore_return_pc();
6907 
6908   BLOCK_COMMENT(&quot;} verify_oop &quot;);
6909 }
6910 
6911 const char* MacroAssembler::stop_types[] = {
6912   &quot;stop&quot;,
6913   &quot;untested&quot;,
6914   &quot;unimplemented&quot;,
6915   &quot;shouldnotreachhere&quot;
6916 };
6917 
6918 static void stop_on_request(const char* tp, const char* msg) {
6919   tty-&gt;print(&quot;Z assembly code requires stop: (%s) %s\n&quot;, tp, msg);
6920   guarantee(false, &quot;Z assembly code requires stop: %s&quot;, msg);
6921 }
6922 
6923 void MacroAssembler::stop(int type, const char* msg, int id) {
6924   BLOCK_COMMENT(err_msg(&quot;stop: %s {&quot;, msg));
</pre>
</td>
</tr>
</table>
<center><a href="jniFastGetField_s390.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_s390.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>