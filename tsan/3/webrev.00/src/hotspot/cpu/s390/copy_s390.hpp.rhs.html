<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/s390/copy_s390.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
<a name="1" id="anc1"></a><span class="line-modified">   2  * Copyright (c) 2016, 2020, Oracle and/or its affiliates. All rights reserved.</span>
<span class="line-modified">   3  * Copyright (c) 2016, 2020 SAP SE. All rights reserved.</span>
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 // Major contributions by LS
  27 
  28 #ifndef CPU_S390_COPY_S390_HPP
  29 #define CPU_S390_COPY_S390_HPP
  30 
  31 // Inline functions for memory copy and fill.
  32 
  33 // HeapWordSize (the size of class HeapWord) is 8 Bytes (the size of a
  34 // pointer variable), since we always run the _LP64 model. As a consequence,
  35 // HeapWord* memory ranges are always assumed to be doubleword-aligned,
  36 // having a size which is an integer multiple of HeapWordSize.
  37 //
  38 // Dealing only with doubleword-aligned doubleword units has important
  39 // positive performance and data access consequences. Many of the move
  40 // instructions perform particularly well under these circumstances.
  41 // Data access is &quot;doubleword-concurrent&quot;, except for MVC and XC.
  42 // Furthermore, data access can be forced to be sequential (MVCL and MVCLE)
  43 // by use of the special padding byte 0xb1, where required. For copying,
  44 // we use padding byte 0xb0 to prevent the D-cache from being polluted.
  45 //
  46 // On z/Architecture, gcc optimizes memcpy into a series of MVC instructions.
  47 // This is optimal, even if just one HeapWord is copied. However, MVC
  48 // copying is not atomic, i.e. not &quot;doubleword concurrent&quot; by definition.
  49 //
  50 // If the -mmvcle compiler option is specified, memcpy translates into
  51 // code such that the entire memory range is copied or preset with just
  52 // one MVCLE instruction.
  53 //
  54 // *to = *from is transformed into a MVC instruction already with -O1.
  55 // Thus, for atomic copy operations, (inline) assembler code is required
  56 // to guarantee atomic data accesses.
  57 //
  58 // For large (len &gt;= MVCLEThreshold) chunks of memory, we exploit
  59 // special H/W support of z/Architecture:
  60 // 1) copy short piece of memory to page-align address(es)
  61 // 2) copy largest part (all contained full pages) of memory using mvcle instruction.
  62 //    z/Architecture processors have special H/W support for page-aligned storage
  63 //    where len is an int multiple of page size. In that case, up to 4 cache lines are
  64 //    processed in parallel and L1 cache is not polluted.
  65 // 3) copy the remaining piece of memory.
  66 //
  67 //  Measurement classifications:
  68 //  very rare - &lt;=     10.000 calls AND &lt;=     1.000 usec elapsed
  69 //       rare - &lt;=    100.000 calls AND &lt;=    10.000 usec elapsed
  70 //       some - &lt;=  1.000.000 calls AND &lt;=   100.000 usec elapsed
  71 //       freq - &lt;= 10.000.000 calls AND &lt;= 1.000.000 usec elapsed
  72 //  very freq - &gt;  10.000.000 calls OR  &gt;  1.000.000 usec elapsed
  73 
  74 #undef USE_INLINE_ASM
  75 
  76 static void copy_conjoint_jshorts_atomic(const jshort* from, jshort* to, size_t count) {
  77   if (from &gt; to) {
  78     while (count-- &gt; 0) {
  79       // Copy forwards
  80       *to++ = *from++;
  81     }
  82   } else {
  83     from += count - 1;
  84     to   += count - 1;
  85     while (count-- &gt; 0) {
  86       // Copy backwards
  87       *to-- = *from--;
  88     }
  89   }
  90 }
  91 
  92 static void copy_conjoint_jints_atomic(const jint* from, jint* to, size_t count) {
  93   if (from &gt; to) {
  94     while (count-- &gt; 0) {
  95       // Copy forwards
  96       *to++ = *from++;
  97     }
  98   } else {
  99     from += count - 1;
 100     to   += count - 1;
 101     while (count-- &gt; 0) {
 102       // Copy backwards
 103       *to-- = *from--;
 104     }
 105   }
 106 }
 107 
 108 static bool has_destructive_overlap(const char* from, char* to, size_t byte_count) {
 109   return (from &lt; to) &amp;&amp; ((to-from) &lt; (ptrdiff_t)byte_count);
 110 }
 111 
 112 #ifdef USE_INLINE_ASM
 113 
 114   //--------------------------------------------------------------
 115   // Atomic copying. Atomicity is given by the minimum of source
 116   // and target alignment. Refer to mail comm with Tim Slegel/IBM.
 117   // Only usable for disjoint source and target.
 118   //--------------------------------------------------------------
 119   #define MOVE8_ATOMIC_4(_to,_from) {                            \
 120     unsigned long toaddr;                                        \
 121     unsigned long fromaddr;                                      \
 122     asm(                                                         \
 123       &quot;LG      %[toaddr],%[to]     \n\t&quot; /* address of to area   */ \
 124       &quot;LG      %[fromaddr],%[from] \n\t&quot; /* address of from area */ \
 125       &quot;MVC     0(32,%[toaddr]),0(%[fromaddr]) \n\t&quot; /* move data */ \
 126       : [to]       &quot;+Q&quot;  (_to)          /* outputs   */          \
 127       , [from]     &quot;+Q&quot;  (_from)                                 \
 128       , [toaddr]   &quot;=a&quot;  (toaddr)                                \
 129       , [fromaddr] &quot;=a&quot;  (fromaddr)                              \
 130       :                                                          \
 131       : &quot;cc&quot;                            /* clobbered */          \
 132     );                                                           \
 133   }
 134   #define MOVE8_ATOMIC_3(_to,_from) {                            \
 135     unsigned long toaddr;                                        \
 136     unsigned long fromaddr;                                      \
 137     asm(                                                         \
 138       &quot;LG      %[toaddr],%[to]     \n\t&quot; /* address of to area   */ \
 139       &quot;LG      %[fromaddr],%[from] \n\t&quot; /* address of from area */ \
 140       &quot;MVC     0(24,%[toaddr]),0(%[fromaddr]) \n\t&quot; /* move data */ \
 141       : [to]       &quot;+Q&quot;  (_to)          /* outputs   */          \
 142       , [from]     &quot;+Q&quot;  (_from)                                 \
 143       , [toaddr]   &quot;=a&quot;  (toaddr)                                \
 144       , [fromaddr] &quot;=a&quot;  (fromaddr)                              \
 145       :                                                          \
 146       : &quot;cc&quot;                            /* clobbered */          \
 147     );                                                           \
 148   }
 149   #define MOVE8_ATOMIC_2(_to,_from) {                            \
 150     unsigned long toaddr;                                        \
 151     unsigned long fromaddr;                                      \
 152     asm(                                                         \
 153       &quot;LG      %[toaddr],%[to]     \n\t&quot; /* address of to area   */ \
 154       &quot;LG      %[fromaddr],%[from] \n\t&quot; /* address of from area */ \
 155       &quot;MVC     0(16,%[toaddr]),0(%[fromaddr]) \n\t&quot; /* move data */ \
 156       : [to]       &quot;+Q&quot;  (_to)          /* outputs   */          \
 157       , [from]     &quot;+Q&quot;  (_from)                                 \
 158       , [toaddr]   &quot;=a&quot;  (toaddr)                                \
 159       , [fromaddr] &quot;=a&quot;  (fromaddr)                              \
 160       :                                                          \
 161       : &quot;cc&quot;                            /* clobbered */          \
 162     );                                                           \
 163   }
 164   #define MOVE8_ATOMIC_1(_to,_from) {                            \
 165     unsigned long toaddr;                                        \
 166     unsigned long fromaddr;                                      \
 167     asm(                                                         \
 168       &quot;LG      %[toaddr],%[to]     \n\t&quot; /* address of to area   */ \
 169       &quot;LG      %[fromaddr],%[from] \n\t&quot; /* address of from area */ \
 170       &quot;MVC     0(8,%[toaddr]),0(%[fromaddr]) \n\t&quot;  /* move data */ \
 171       : [to]       &quot;+Q&quot;  (_to)          /* outputs   */          \
 172       , [from]     &quot;+Q&quot;  (_from)                                 \
 173       , [toaddr]   &quot;=a&quot;  (toaddr)                                \
 174       , [fromaddr] &quot;=a&quot;  (fromaddr)                              \
 175       :                                                          \
 176       : &quot;cc&quot;                            /* clobbered */          \
 177     );                                                           \
 178   }
 179 
 180   //--------------------------------------------------------------
 181   // Atomic copying of 8-byte entities.
 182   // Conjoint/disjoint property does not matter. Entities are first
 183   // loaded and then stored.
 184   // _to and _from must be 8-byte aligned.
 185   //--------------------------------------------------------------
 186   #define COPY8_ATOMIC_4(_to,_from) {                            \
 187     unsigned long toaddr;                                        \
 188     asm(                                                         \
 189       &quot;LG      3,%[from]        \n\t&quot; /* address of from area */ \
 190       &quot;LG      %[toaddr],%[to]  \n\t&quot; /* address of to area   */ \
 191       &quot;LMG     0,3,0(3)         \n\t&quot; /* load data            */ \
 192       &quot;STMG    0,3,0(%[toaddr]) \n\t&quot; /* store data           */ \
 193       : [to]     &quot;+Q&quot;  (_to)          /* outputs   */            \
 194       , [from]   &quot;+Q&quot;  (_from)        /* outputs   */            \
 195       , [toaddr] &quot;=a&quot;  (toaddr)       /* inputs    */            \
 196       :                                                          \
 197       : &quot;cc&quot;,  &quot;r0&quot;, &quot;r1&quot;, &quot;r2&quot;, &quot;r3&quot; /* clobbered */            \
 198     );                                                           \
 199   }
 200   #define COPY8_ATOMIC_3(_to,_from) {                            \
 201     unsigned long toaddr;                                        \
 202     asm(                                                         \
 203       &quot;LG      2,%[from]        \n\t&quot; /* address of from area */ \
 204       &quot;LG      %[toaddr],%[to]  \n\t&quot; /* address of to area   */ \
 205       &quot;LMG     0,2,0(2)         \n\t&quot; /* load data            */ \
 206       &quot;STMG    0,2,0(%[toaddr]) \n\t&quot; /* store data           */ \
 207       : [to]     &quot;+Q&quot;  (_to)          /* outputs   */            \
 208       , [from]   &quot;+Q&quot;  (_from)        /* outputs   */            \
 209       , [toaddr] &quot;=a&quot;  (toaddr)       /* inputs    */            \
 210       :                                                          \
 211       : &quot;cc&quot;,  &quot;r0&quot;, &quot;r1&quot;, &quot;r2&quot;       /* clobbered */            \
 212     );                                                           \
 213   }
 214   #define COPY8_ATOMIC_2(_to,_from) {                            \
 215     unsigned long toaddr;                                        \
 216     asm(                                                         \
 217       &quot;LG      1,%[from]        \n\t&quot; /* address of from area */ \
 218       &quot;LG      %[toaddr],%[to]  \n\t&quot; /* address of to area   */ \
 219       &quot;LMG     0,1,0(1)         \n\t&quot; /* load data            */ \
 220       &quot;STMG    0,1,0(%[toaddr]) \n\t&quot; /* store data           */ \
 221       : [to]     &quot;+Q&quot;  (_to)          /* outputs   */            \
 222       , [from]   &quot;+Q&quot;  (_from)        /* outputs   */            \
 223       , [toaddr] &quot;=a&quot;  (toaddr)       /* inputs    */            \
 224       :                                                          \
 225       : &quot;cc&quot;,  &quot;r0&quot;, &quot;r1&quot;             /* clobbered */            \
 226     );                                                           \
 227   }
 228   #define COPY8_ATOMIC_1(_to,_from) {                            \
 229     unsigned long addr;                                          \
 230     asm(                                                         \
 231       &quot;LG      %[addr],%[from]  \n\t&quot; /* address of from area */ \
 232       &quot;LG      0,0(0,%[addr])   \n\t&quot; /* load data            */ \
 233       &quot;LG      %[addr],%[to]    \n\t&quot; /* address of to area   */ \
 234       &quot;STG     0,0(0,%[addr])   \n\t&quot; /* store data           */ \
 235       : [to]     &quot;+Q&quot;  (_to)          /* outputs   */            \
 236       , [from]   &quot;+Q&quot;  (_from)        /* outputs   */            \
 237       , [addr]   &quot;=a&quot;  (addr)         /* inputs    */            \
 238       :                                                          \
 239       : &quot;cc&quot;,  &quot;r0&quot;                   /* clobbered */            \
 240     );                                                           \
 241   }
 242 
 243   //--------------------------------------------------------------
 244   // Atomic copying of 4-byte entities.
 245   // Exactly 4 (four) entities are copied.
 246   // Conjoint/disjoint property does not matter. Entities are first
 247   // loaded and then stored.
 248   // _to and _from must be 4-byte aligned.
 249   //--------------------------------------------------------------
 250   #define COPY4_ATOMIC_4(_to,_from) {                            \
 251     unsigned long toaddr;                                        \
 252     asm(                                                         \
 253       &quot;LG      3,%[from]        \n\t&quot; /* address of from area */ \
 254       &quot;LG      %[toaddr],%[to]  \n\t&quot; /* address of to area   */ \
 255       &quot;LM      0,3,0(3)         \n\t&quot; /* load data            */ \
 256       &quot;STM     0,3,0(%[toaddr]) \n\t&quot; /* store data           */ \
 257       : [to]     &quot;+Q&quot;  (_to)          /* outputs   */            \
 258       , [from]   &quot;+Q&quot;  (_from)        /* outputs   */            \
 259       , [toaddr] &quot;=a&quot;  (toaddr)       /* inputs    */            \
 260       :                                                          \
 261       : &quot;cc&quot;,  &quot;r0&quot;, &quot;r1&quot;, &quot;r2&quot;, &quot;r3&quot; /* clobbered */            \
 262     );                                                           \
 263   }
 264   #define COPY4_ATOMIC_3(_to,_from) {                            \
 265     unsigned long toaddr;                                        \
 266     asm(                                                         \
 267       &quot;LG      2,%[from]        \n\t&quot; /* address of from area */ \
 268       &quot;LG      %[toaddr],%[to]  \n\t&quot; /* address of to area   */ \
 269       &quot;LM      0,2,0(2)         \n\t&quot; /* load data            */ \
 270       &quot;STM     0,2,0(%[toaddr]) \n\t&quot; /* store data           */ \
 271       : [to]     &quot;+Q&quot;  (_to)          /* outputs   */            \
 272       , [from]   &quot;+Q&quot;  (_from)        /* outputs   */            \
 273       , [toaddr] &quot;=a&quot;  (toaddr)       /* inputs    */            \
 274       :                                                          \
 275       : &quot;cc&quot;,  &quot;r0&quot;, &quot;r1&quot;, &quot;r2&quot;       /* clobbered */            \
 276     );                                                           \
 277   }
 278   #define COPY4_ATOMIC_2(_to,_from) {                            \
 279     unsigned long toaddr;                                        \
 280     asm(                                                         \
 281       &quot;LG      1,%[from]        \n\t&quot; /* address of from area */ \
 282       &quot;LG      %[toaddr],%[to]  \n\t&quot; /* address of to area   */ \
 283       &quot;LM      0,1,0(1)         \n\t&quot; /* load data            */ \
 284       &quot;STM     0,1,0(%[toaddr]) \n\t&quot; /* store data           */ \
 285       : [to]     &quot;+Q&quot;  (_to)          /* outputs   */            \
 286       , [from]   &quot;+Q&quot;  (_from)        /* outputs   */            \
 287       , [toaddr] &quot;=a&quot;  (toaddr)       /* inputs    */            \
 288       :                                                          \
 289       : &quot;cc&quot;,  &quot;r0&quot;, &quot;r1&quot;             /* clobbered */            \
 290     );                                                           \
 291   }
 292   #define COPY4_ATOMIC_1(_to,_from) {                            \
 293     unsigned long addr;                                          \
 294     asm(                                                         \
 295       &quot;LG      %[addr],%[from]  \n\t&quot; /* address of from area */ \
 296       &quot;L       0,0(0,%[addr])   \n\t&quot; /* load data            */ \
 297       &quot;LG      %[addr],%[to]    \n\t&quot; /* address of to area   */ \
 298       &quot;ST      0,0(0,%[addr])   \n\t&quot; /* store data           */ \
 299       : [to]     &quot;+Q&quot;  (_to)          /* outputs   */            \
 300       , [from]   &quot;+Q&quot;  (_from)        /* outputs   */            \
 301       , [addr]   &quot;=a&quot;  (addr)         /* inputs    */            \
 302       :                                                          \
 303       : &quot;cc&quot;,  &quot;r0&quot;                   /* clobbered */            \
 304     );                                                           \
 305   }
 306 
 307 #if 0  // Waiting for gcc to support EXRL.
 308   #define MVC_MEMCOPY(_to,_from,_len)                                \
 309     if (VM_Version::has_ExecuteExtensions()) {                       \
 310       asm(&quot;\t&quot;                                                       \
 311       &quot;    LAY     1,-1(0,%[len])      \n\t&quot; /* decr for MVC  */     \
 312       &quot;    EXRL    1,1f                \n\t&quot; /* execute MVC instr */ \
 313       &quot;    BRC     15,2f               \n\t&quot; /* skip template */     \
 314       &quot;1:  MVC     0(%[len],%[to]),0(%[from]) \n\t&quot;                  \
 315       &quot;2:  BCR     0,0                 \n\t&quot;                         \
 316       : [to]   &quot;+Q&quot;  (_to)             /* outputs   */               \
 317       , [from] &quot;+Q&quot;  (_from)           /* outputs   */               \
 318       : [len]  &quot;r&quot;   (_len)            /* inputs    */               \
 319       : &quot;cc&quot;,  &quot;r1&quot;                    /* clobbered */               \
 320       );                                                             \
 321     } else {                                                         \
 322       asm(&quot;\t&quot;                                                       \
 323       &quot;    LARL    2,3f                \n\t&quot;                         \
 324       &quot;    LAY     1,-1(0,%[len])      \n\t&quot; /* decr for MVC  */     \
 325       &quot;    EX      1,0(2)              \n\t&quot; /* execute MVC instr */ \
 326       &quot;    BRC     15,4f               \n\t&quot; /* skip template */     \
 327       &quot;3:  MVC     0(%[len],%[to]),0(%[from])  \n\t&quot;                 \
 328       &quot;4:  BCR     0,0                 \n\t&quot;                         \
 329       : [to]   &quot;+Q&quot;  (_to)             /* outputs   */               \
 330       , [from] &quot;+Q&quot;  (_from)           /* outputs   */               \
 331       : [len]  &quot;r&quot;   (_len)            /* inputs    */               \
 332       : &quot;cc&quot;,  &quot;r1&quot;, &quot;r2&quot;              /* clobbered */               \
 333       );                                                             \
 334     }
 335 #else
 336   #define MVC_MEMCOPY(_to,_from,_len)                                \
 337   { unsigned long toaddr;   unsigned long tolen;                     \
 338     unsigned long fromaddr; unsigned long target;                    \
 339       asm(&quot;\t&quot;                                                       \
 340       &quot;    LTGR    %[tolen],%[len]     \n\t&quot; /* decr for MVC  */     \
 341       &quot;    BRC     8,2f                \n\t&quot; /* do nothing for l=0*/ \
 342       &quot;    AGHI    %[tolen],-1         \n\t&quot;                         \
 343       &quot;    LG      %[toaddr],%[to]     \n\t&quot;                         \
 344       &quot;    LG      %[fromaddr],%[from] \n\t&quot;                         \
 345       &quot;    LARL    %[target],1f        \n\t&quot; /* addr of MVC instr */ \
 346       &quot;    EX      %[tolen],0(%[target])         \n\t&quot; /* execute MVC instr */ \
 347       &quot;    BRC     15,2f                         \n\t&quot; /* skip template */     \
 348       &quot;1:  MVC     0(1,%[toaddr]),0(%[fromaddr]) \n\t&quot;                         \
 349       &quot;2:  BCR     0,0                 \n\t&quot; /* nop a branch target*/\
 350       : [to]       &quot;+Q&quot;  (_to)         /* outputs   */               \
 351       , [from]     &quot;+Q&quot;  (_from)                                     \
 352       , [tolen]    &quot;=a&quot;  (tolen)                                     \
 353       , [toaddr]   &quot;=a&quot;  (toaddr)                                    \
 354       , [fromaddr] &quot;=a&quot;  (fromaddr)                                  \
 355       , [target]   &quot;=a&quot;  (target)                                    \
 356       : [len]       &quot;r&quot;  (_len)        /* inputs    */               \
 357       : &quot;cc&quot;                           /* clobbered */               \
 358       );                                                             \
 359   }
 360 #endif
 361 
 362   #if 0  // code snippet to be used for debugging
 363       /* ASSERT code BEGIN */                                                \
 364       &quot;    LARL    %[len],5f       \n\t&quot;                                     \
 365       &quot;    LARL    %[mta],4f       \n\t&quot;                                     \
 366       &quot;    SLGR    %[len],%[mta]   \n\t&quot;                                     \
 367       &quot;    CGHI    %[len],16       \n\t&quot;                                     \
 368       &quot;    BRC     7,9f            \n\t&quot;      /* block size !=  16 */        \
 369                                                                              \
 370       &quot;    LARL    %[len],1f       \n\t&quot;                                     \
 371       &quot;    SLGR    %[len],%[mta]   \n\t&quot;                                     \
 372       &quot;    CGHI    %[len],256      \n\t&quot;                                     \
 373       &quot;    BRC     7,9f            \n\t&quot;      /* list len   != 256 */        \
 374                                                                              \
 375       &quot;    LGR     0,0             \n\t&quot;      /* artificial SIGILL */        \
 376       &quot;9:  BRC     7,-2            \n\t&quot;                                     \
 377       &quot;    LARL    %[mta],1f       \n\t&quot;      /* restore MVC table begin */  \
 378       /* ASSERT code END   */
 379   #endif
 380 
 381   // Optimized copying for data less than 4k
 382   // - no destructive overlap
 383   // - 0 &lt;= _n_bytes &lt;= 4096
 384   // This macro needs to be gcc-compiled with -march=z990. Otherwise, the
 385   // LAY instruction is not available.
 386   #define MVC_MULTI(_to,_from,_n_bytes)                                      \
 387   { unsigned long toaddr;                                                    \
 388     unsigned long fromaddr;                                                  \
 389     unsigned long movetable;                                                 \
 390     unsigned long len;                                                       \
 391       asm(&quot;\t&quot;                                                               \
 392       &quot;    LTGFR   %[len],%[nby]   \n\t&quot;                                     \
 393       &quot;    LG      %[ta],%[to]     \n\t&quot;      /* address of to area   */     \
 394       &quot;    BRC     8,1f            \n\t&quot;      /* nothing to copy   */        \
 395                                                                              \
 396       &quot;    NILL    %[nby],255      \n\t&quot;      /* # bytes mod 256      */     \
 397       &quot;    LG      %[fa],%[from]   \n\t&quot;      /* address of from area */     \
 398       &quot;    BRC     8,3f            \n\t&quot;      /* no rest, skip copying */    \
 399                                                                              \
 400       &quot;    LARL    %[mta],2f       \n\t&quot;      /* MVC template addr */        \
 401       &quot;    AHI     %[nby],-1       \n\t&quot;      /* adjust for EX MVC  */       \
 402                                                                              \
 403       &quot;    EX      %[nby],0(%[mta]) \n\t&quot;     /* only rightmost */           \
 404                                               /* 8 bits of nby used */       \
 405       /* Since nby is &lt;= 4096 on entry to this code, we do need */           \
 406       /* no zero extension before using it in addr calc.        */           \
 407       &quot;    LA      %[fa],1(%[nby],%[fa]) \n\t&quot;/* adjust from addr */         \
 408       &quot;    LA      %[ta],1(%[nby],%[ta]) \n\t&quot;/* adjust to   addr */         \
 409                                                                              \
 410       &quot;3:  SRAG    %[nby],%[len],8 \n\t&quot;      /* # cache lines     */        \
 411       &quot;    LARL    %[mta],1f       \n\t&quot;      /* MVC table begin   */        \
 412       &quot;    BRC     8,1f            \n\t&quot;      /* nothing to copy   */        \
 413                                                                              \
 414       /* Insert ASSERT code here if required. */                             \
 415                                                                              \
 416                                                                              \
 417       &quot;    LNGFR   %[nby],%[nby]   \n\t&quot;      /* negative offset into     */ \
 418       &quot;    SLLG    %[nby],%[nby],4 \n\t&quot;      /* MVC table 16-byte blocks */ \
 419       &quot;    BC      15,0(%[nby],%[mta]) \n\t&quot;  /* branch to block #ncl  */    \
 420                                                                              \
 421       &quot;2:  MVC     0(1,%[ta]),0(%[fa]) \n\t&quot;  /* MVC template */             \
 422                                                                              \
 423       &quot;4:  MVC     0(256,%[ta]),0(%[fa])   \n\t&quot; /* 4096 == l        */      \
 424       &quot;    LAY     %[ta],256(0,%[ta])      \n\t&quot;                             \
 425       &quot;    LA      %[fa],256(0,%[fa])      \n\t&quot;                             \
 426       &quot;5:  MVC     0(256,%[ta]),0(%[fa])   \n\t&quot; /* 3840 &lt;= l &lt; 4096 */      \
 427       &quot;    LAY     %[ta],256(0,%[ta])      \n\t&quot;                             \
 428       &quot;    LA      %[fa],256(0,%[fa])      \n\t&quot;                             \
 429       &quot;    MVC     0(256,%[ta]),0(%[fa])   \n\t&quot; /* 3548 &lt;= l &lt; 3328 */      \
 430       &quot;    LAY     %[ta],256(0,%[ta])      \n\t&quot;                             \
 431       &quot;    LA      %[fa],256(0,%[fa])      \n\t&quot;                             \
 432       &quot;    MVC     0(256,%[ta]),0(%[fa])   \n\t&quot; /* 3328 &lt;= l &lt; 3328 */      \
 433       &quot;    LAY     %[ta],256(0,%[ta])      \n\t&quot;                             \
 434       &quot;    LA      %[fa],256(0,%[fa])      \n\t&quot;                             \
 435       &quot;    MVC     0(256,%[ta]),0(%[fa])   \n\t&quot; /* 3072 &lt;= l &lt; 3328 */      \
 436       &quot;    LAY     %[ta],256(0,%[ta])      \n\t&quot;                             \
 437       &quot;    LA      %[fa],256(0,%[fa])      \n\t&quot;                             \
 438       &quot;    MVC     0(256,%[ta]),0(%[fa])   \n\t&quot; /* 2816 &lt;= l &lt; 3072 */      \
 439       &quot;    LAY     %[ta],256(0,%[ta])      \n\t&quot;                             \
 440       &quot;    LA      %[fa],256(0,%[fa])      \n\t&quot;                             \
 441       &quot;    MVC     0(256,%[ta]),0(%[fa])   \n\t&quot; /* 2560 &lt;= l &lt; 2816 */      \
 442       &quot;    LAY     %[ta],256(0,%[ta])      \n\t&quot;                             \
 443       &quot;    LA      %[fa],256(0,%[fa])      \n\t&quot;                             \
 444       &quot;    MVC     0(256,%[ta]),0(%[fa])   \n\t&quot; /* 2304 &lt;= l &lt; 2560 */      \
 445       &quot;    LAY     %[ta],256(0,%[ta])      \n\t&quot;                             \
 446       &quot;    LA      %[fa],256(0,%[fa])      \n\t&quot;                             \
 447       &quot;    MVC     0(256,%[ta]),0(%[fa])   \n\t&quot; /* 2048 &lt;= l &lt; 2304 */      \
 448       &quot;    LAY     %[ta],256(0,%[ta])      \n\t&quot;                             \
 449       &quot;    LA      %[fa],256(0,%[fa])      \n\t&quot;                             \
 450       &quot;    MVC     0(256,%[ta]),0(%[fa])   \n\t&quot; /* 1792 &lt;= l &lt; 2048 */      \
 451       &quot;    LAY     %[ta],256(0,%[ta])      \n\t&quot;                             \
 452       &quot;    LA      %[fa],256(0,%[fa])      \n\t&quot;                             \
 453       &quot;    MVC     0(256,%[ta]),0(%[fa])   \n\t&quot; /* 1536 &lt;= l &lt; 1792 */      \
 454       &quot;    LAY     %[ta],256(0,%[ta])      \n\t&quot;                             \
 455       &quot;    LA      %[fa],256(0,%[fa])      \n\t&quot;                             \
 456       &quot;    MVC     0(256,%[ta]),0(%[fa])   \n\t&quot; /* 1280 &lt;= l &lt; 1536 */      \
 457       &quot;    LAY     %[ta],256(0,%[ta])      \n\t&quot;                             \
 458       &quot;    LA      %[fa],256(0,%[fa])      \n\t&quot;                             \
 459       &quot;    MVC     0(256,%[ta]),0(%[fa])   \n\t&quot; /* 1024 &lt;= l &lt; 1280 */      \
 460       &quot;    LAY     %[ta],256(0,%[ta])      \n\t&quot;                             \
 461       &quot;    LA      %[fa],256(0,%[fa])      \n\t&quot;                             \
 462       &quot;    MVC     0(256,%[ta]),0(%[fa])   \n\t&quot; /*  768 &lt;= l &lt; 1024 */      \
 463       &quot;    LAY     %[ta],256(0,%[ta])      \n\t&quot;                             \
 464       &quot;    LA      %[fa],256(0,%[fa])      \n\t&quot;                             \
 465       &quot;    MVC     0(256,%[ta]),0(%[fa])   \n\t&quot; /*  512 &lt;= l &lt;  768 */      \
 466       &quot;    LAY     %[ta],256(0,%[ta])      \n\t&quot;                             \
 467       &quot;    LA      %[fa],256(0,%[fa])      \n\t&quot;                             \
 468       &quot;    MVC     0(256,%[ta]),0(%[fa])   \n\t&quot; /*  256 &lt;= l &lt;  512 */      \
 469       &quot;    LAY     %[ta],256(0,%[ta])      \n\t&quot;                             \
 470       &quot;    LA      %[fa],256(0,%[fa])      \n\t&quot;                             \
 471       &quot;1:  BCR     0,0                     \n\t&quot; /* nop as branch target */  \
 472       : [to]       &quot;+Q&quot;  (_to)          /* outputs   */          \
 473       , [from]     &quot;+Q&quot;  (_from)                                 \
 474       , [ta]       &quot;=a&quot;  (toaddr)                                \
 475       , [fa]       &quot;=a&quot;  (fromaddr)                              \
 476       , [mta]      &quot;=a&quot;  (movetable)                             \
 477       , [nby]      &quot;+a&quot;  (_n_bytes)                              \
 478       , [len]      &quot;=a&quot;  (len)                                   \
 479       :                                                          \
 480       : &quot;cc&quot;                            /* clobbered */          \
 481     );                                                           \
 482   }
 483 
 484   #define MVCLE_MEMCOPY(_to,_from,_len)                           \
 485     asm(                                                          \
 486       &quot;    LG      0,%[to]     \n\t&quot;   /* address of to area   */ \
 487       &quot;    LG      2,%[from]   \n\t&quot;   /* address of from area */ \
 488       &quot;    LGR     1,%[len]    \n\t&quot;   /* len of to area       */ \
 489       &quot;    LGR     3,%[len]    \n\t&quot;   /* len of from area     */ \
 490       &quot;1:  MVCLE   0,2,176     \n\t&quot;   /* copy storage, bypass cache (0xb0) */ \
 491       &quot;    BRC     1,1b        \n\t&quot;   /* retry if interrupted */ \
 492       : [to]   &quot;+Q&quot;  (_to)             /* outputs   */            \
 493       , [from] &quot;+Q&quot;  (_from)           /* outputs   */            \
 494       : [len]  &quot;r&quot;   (_len)            /* inputs    */            \
 495       : &quot;cc&quot;,  &quot;r0&quot;, &quot;r1&quot;, &quot;r2&quot;, &quot;r3&quot;  /* clobbered */            \
 496     );
 497 
 498   #define MVCLE_MEMINIT(_to,_val,_len)                            \
 499     asm(                                                          \
 500       &quot;    LG      0,%[to]       \n\t&quot; /* address of to area   */ \
 501       &quot;    LGR     1,%[len]      \n\t&quot; /* len of to area       */ \
 502       &quot;    XGR     3,3           \n\t&quot; /* from area len = 0    */ \
 503       &quot;1:  MVCLE   0,2,0(%[val]) \n\t&quot; /* init storage         */ \
 504       &quot;    BRC     1,1b          \n\t&quot; /* retry if interrupted */ \
 505       : [to]   &quot;+Q&quot;  (_to)             /* outputs   */            \
 506       : [len]  &quot;r&quot;   (_len)            /* inputs    */            \
 507       , [val]  &quot;r&quot;   (_val)            /* inputs    */            \
 508       : &quot;cc&quot;,  &quot;r0&quot;, &quot;r1&quot;, &quot;r3&quot;        /* clobbered */            \
 509     );
 510   #define MVCLE_MEMZERO(_to,_len)                                 \
 511     asm(                                                          \
 512       &quot;    LG      0,%[to]       \n\t&quot; /* address of to area   */ \
 513       &quot;    LGR     1,%[len]      \n\t&quot; /* len of to area       */ \
 514       &quot;    XGR     3,3           \n\t&quot; /* from area len = 0    */ \
 515       &quot;1:  MVCLE   0,2,0         \n\t&quot; /* clear storage        */ \
 516       &quot;    BRC     1,1b          \n\t&quot; /* retry if interrupted */ \
 517       : [to]   &quot;+Q&quot;  (_to)             /* outputs   */            \
 518       : [len]  &quot;r&quot;   (_len)            /* inputs    */            \
 519       : &quot;cc&quot;,  &quot;r0&quot;, &quot;r1&quot;, &quot;r3&quot;        /* clobbered */            \
 520     );
 521 
 522   // Clear a stretch of memory, 0 &lt;= _len &lt;= 256.
 523   // There is no alignment prereq.
 524   // There is no test for len out of range specified above.
 525   #define XC_MEMZERO_256(_to,_len)                                 \
 526 { unsigned long toaddr;   unsigned long tolen;                     \
 527   unsigned long target;                                            \
 528     asm(&quot;\t&quot;                                                       \
 529     &quot;    LTGR    %[tolen],%[len]     \n\t&quot; /* decr for MVC  */     \
 530     &quot;    BRC     8,2f                \n\t&quot; /* do nothing for l=0*/ \
 531     &quot;    AGHI    %[tolen],-1         \n\t&quot; /* adjust for EX XC  */ \
 532     &quot;    LARL    %[target],1f        \n\t&quot; /* addr of XC instr  */ \
 533     &quot;    LG      %[toaddr],%[to]     \n\t&quot; /* addr of data area */ \
 534     &quot;    EX      %[tolen],0(%[target])       \n\t&quot; /* execute MVC instr */ \
 535     &quot;    BRC     15,2f                       \n\t&quot; /* skip template */     \
 536     &quot;1:  XC      0(1,%[toaddr]),0(%[toaddr]) \n\t&quot;                         \
 537     &quot;2:  BCR     0,0                 \n\t&quot; /* nop a branch target*/\
 538     : [to]       &quot;+Q&quot;  (_to)         /* outputs   */               \
 539     , [tolen]    &quot;=a&quot;  (tolen)                                     \
 540     , [toaddr]   &quot;=a&quot;  (toaddr)                                    \
 541     , [target]   &quot;=a&quot;  (target)                                    \
 542     : [len]       &quot;r&quot;  (_len)        /* inputs    */               \
 543     : &quot;cc&quot;                           /* clobbered */               \
 544     );                                                             \
 545 }
 546 
 547   // Clear a stretch of memory, 256 &lt; _len.
 548   // XC_MEMZERO_256 may be used to clear shorter areas.
 549   //
 550   // The code
 551   // - first zeroes a few bytes to align on a HeapWord.
 552   //   This step is currently inactive because all calls seem
 553   //   to have their data aligned on HeapWord boundaries.
 554   // - then zeroes a few HeapWords to align on a cache line.
 555   // - then zeroes entire cache lines in a loop.
 556   // - then zeroes the remaining (partial) cache line.
 557 #if 1
 558   #define XC_MEMZERO_ANY(_to,_len)                                    \
 559 { unsigned long toaddr;   unsigned long tolen;                        \
 560   unsigned long len8;     unsigned long len256;                       \
 561   unsigned long target;   unsigned long lenx;                         \
 562     asm(&quot;\t&quot;                                                          \
 563     &quot;    LTGR    %[tolen],%[len]      \n\t&quot; /*                   */   \
 564     &quot;    BRC     8,2f                 \n\t&quot; /* do nothing for l=0*/   \
 565     &quot;    LG      %[toaddr],%[to]      \n\t&quot; /* addr of data area */   \
 566     &quot;    LARL    %[target],1f         \n\t&quot; /* addr of XC instr  */   \
 567     &quot; &quot;                                                               \
 568     &quot;    LCGR    %[len256],%[toaddr]  \n\t&quot; /* cache line alignment */\
 569     &quot;    NILL    %[len256],0xff       \n\t&quot;                           \
 570     &quot;    BRC     8,4f                 \n\t&quot; /* already aligned     */ \
 571     &quot;    NILH    %[len256],0x00       \n\t&quot; /* zero extend         */ \
 572     &quot;    LLGFR   %[len256],%[len256]  \n\t&quot;                           \
 573     &quot;    LAY     %[lenx],-1(,%[len256]) \n\t&quot;                         \
 574     &quot;    EX      %[lenx],0(%[target]) \n\t&quot; /* execute MVC instr   */ \
 575     &quot;    LA      %[toaddr],0(%[len256],%[toaddr]) \n\t&quot;               \
 576     &quot;    SGR     %[tolen],%[len256]   \n\t&quot; /* adjust len          */ \
 577     &quot; &quot;                                                               \
 578     &quot;4:  SRAG    %[lenx],%[tolen],8   \n\t&quot; /* # cache lines       */ \
 579     &quot;    BRC     8,6f                 \n\t&quot; /* no full cache lines */ \
 580     &quot;5:  XC      0(256,%[toaddr]),0(%[toaddr]) \n\t&quot;                  \
 581     &quot;    LA      %[toaddr],256(,%[toaddr]) \n\t&quot;                      \
 582     &quot;    BRCTG   %[lenx],5b           \n\t&quot; /* iterate             */ \
 583     &quot; &quot;                                                               \
 584     &quot;6:  NILL    %[tolen],0xff        \n\t&quot; /* leftover bytes      */ \
 585     &quot;    BRC     8,2f                 \n\t&quot; /* done if none        */ \
 586     &quot;    LAY     %[lenx],-1(,%[tolen]) \n\t&quot;                          \
 587     &quot;    EX      %[lenx],0(%[target]) \n\t&quot; /* execute MVC instr   */ \
 588     &quot;    BRC     15,2f                \n\t&quot; /* skip template       */ \
 589     &quot; &quot;                                                               \
 590     &quot;1:  XC      0(1,%[toaddr]),0(%[toaddr]) \n\t&quot;                    \
 591     &quot;2:  BCR     0,0                  \n\t&quot; /* nop a branch target */ \
 592     : [to]       &quot;+Q&quot;  (_to)         /* outputs   */               \
 593     , [lenx]     &quot;=a&quot;  (lenx)                                      \
 594     , [len256]   &quot;=a&quot;  (len256)                                    \
 595     , [tolen]    &quot;=a&quot;  (tolen)                                     \
 596     , [toaddr]   &quot;=a&quot;  (toaddr)                                    \
 597     , [target]   &quot;=a&quot;  (target)                                    \
 598     : [len]       &quot;r&quot;  (_len)        /* inputs    */               \
 599     : &quot;cc&quot;                           /* clobbered */               \
 600     );                                                             \
 601 }
 602 #else
 603   #define XC_MEMZERO_ANY(_to,_len)                                    \
 604 { unsigned long toaddr;   unsigned long tolen;                        \
 605   unsigned long len8;     unsigned long len256;                       \
 606   unsigned long target;   unsigned long lenx;                         \
 607     asm(&quot;\t&quot;                                                          \
 608     &quot;    LTGR    %[tolen],%[len]      \n\t&quot; /*                   */   \
 609     &quot;    BRC     8,2f                 \n\t&quot; /* do nothing for l=0*/   \
 610     &quot;    LG      %[toaddr],%[to]      \n\t&quot; /* addr of data area */   \
 611     &quot;    LARL    %[target],1f         \n\t&quot; /* addr of XC instr  */   \
 612     &quot; &quot;                                                               \
 613     &quot;    LCGR    %[len8],%[toaddr]    \n\t&quot; /* HeapWord alignment  */ \
 614     &quot;    NILL    %[len8],0x07         \n\t&quot;                           \
 615     &quot;    BRC     8,3f                 \n\t&quot; /* already aligned     */ \
 616     &quot;    NILH    %[len8],0x00         \n\t&quot; /* zero extend         */ \
 617     &quot;    LLGFR   %[len8],%[len8]      \n\t&quot;                           \
 618     &quot;    LAY     %[lenx],-1(,%[len8]) \n\t&quot;                           \
 619     &quot;    EX      %[lenx],0(%[target]) \n\t&quot; /* execute MVC instr */   \
 620     &quot;    LA      %[toaddr],0(%[len8],%[toaddr]) \n\t&quot;                 \
 621     &quot;    SGR     %[tolen],%[len8]     \n\t&quot; /* adjust len          */ \
 622     &quot; &quot;                                                               \
 623     &quot;3:  LCGR    %[len256],%[toaddr]  \n\t&quot; /* cache line alignment */\
 624     &quot;    NILL    %[len256],0xff       \n\t&quot;                           \
 625     &quot;    BRC     8,4f                 \n\t&quot; /* already aligned     */ \
 626     &quot;    NILH    %[len256],0x00       \n\t&quot; /* zero extend         */ \
 627     &quot;    LLGFR   %[len256],%[len256]  \n\t&quot;                           \
 628     &quot;    LAY     %[lenx],-1(,%[len256]) \n\t&quot;                         \
 629     &quot;    EX      %[lenx],0(%[target]) \n\t&quot; /* execute MVC instr   */ \
 630     &quot;    LA      %[toaddr],0(%[len256],%[toaddr]) \n\t&quot;               \
 631     &quot;    SGR     %[tolen],%[len256]   \n\t&quot; /* adjust len          */ \
 632     &quot; &quot;                                                               \
 633     &quot;4:  SRAG    %[lenx],%[tolen],8   \n\t&quot; /* # cache lines       */ \
 634     &quot;    BRC     8,6f                 \n\t&quot; /* no full cache lines */ \
 635     &quot;5:  XC      0(256,%[toaddr]),0(%[toaddr]) \n\t&quot;                  \
 636     &quot;    LA      %[toaddr],256(,%[toaddr]) \n\t&quot;                      \
 637     &quot;    BRCTG   %[lenx],5b           \n\t&quot; /* iterate             */ \
 638     &quot; &quot;                                                               \
 639     &quot;6:  NILL    %[tolen],0xff        \n\t&quot; /* leftover bytes      */ \
 640     &quot;    BRC     8,2f                 \n\t&quot; /* done if none        */ \
 641     &quot;    LAY     %[lenx],-1(,%[tolen]) \n\t&quot;                          \
 642     &quot;    EX      %[lenx],0(%[target]) \n\t&quot; /* execute MVC instr   */ \
 643     &quot;    BRC     15,2f                \n\t&quot; /* skip template       */ \
 644     &quot; &quot;                                                               \
 645     &quot;1:  XC      0(1,%[toaddr]),0(%[toaddr]) \n\t&quot;                    \
 646     &quot;2:  BCR     0,0                  \n\t&quot; /* nop a branch target */ \
 647     : [to]       &quot;+Q&quot;  (_to)         /* outputs   */               \
 648     , [lenx]     &quot;=a&quot;  (lenx)                                      \
 649     , [len8]     &quot;=a&quot;  (len8)                                      \
 650     , [len256]   &quot;=a&quot;  (len256)                                    \
 651     , [tolen]    &quot;=a&quot;  (tolen)                                     \
 652     , [toaddr]   &quot;=a&quot;  (toaddr)                                    \
 653     , [target]   &quot;=a&quot;  (target)                                    \
 654     : [len]       &quot;r&quot;  (_len)        /* inputs    */               \
 655     : &quot;cc&quot;                           /* clobbered */               \
 656     );                                                             \
 657 }
 658 #endif
 659 #endif // USE_INLINE_ASM
 660 
 661 //*************************************//
 662 //   D I S J O I N T   C O P Y I N G   //
 663 //*************************************//
 664 
 665 static void pd_aligned_disjoint_words(const HeapWord* from, HeapWord* to, size_t count) {
 666   // JVM2008: very frequent, some tests frequent.
 667 
 668   // Copy HeapWord (=DW) aligned storage. Use MVCLE in inline-asm code.
 669   // MVCLE guarantees DW concurrent (i.e. atomic) accesses if both the addresses of the operands
 670   // are DW aligned and the length is an integer multiple of a DW. Should always be true here.
 671   //
 672   // No special exploit needed. H/W discovers suitable situations itself.
 673   //
 674   // For large chunks of memory, exploit special H/W support of z/Architecture:
 675   // 1) copy short piece of memory to page-align address(es)
 676   // 2) copy largest part (all contained full pages) of memory using mvcle instruction.
 677   //    z/Architecture processors have special H/W support for page-aligned storage
 678   //    where len is an int multiple of page size. In that case, up to 4 cache lines are
 679   //    processed in parallel and L1 cache is not polluted.
 680   // 3) copy the remaining piece of memory.
 681   //
 682 #ifdef USE_INLINE_ASM
 683   jbyte* to_bytes   = (jbyte*)to;
 684   jbyte* from_bytes = (jbyte*)from;
 685   size_t len_bytes  = count*HeapWordSize;
 686 
 687   // Optimized copying for data less than 4k
 688   switch (count) {
 689     case 0: return;
 690     case 1: MOVE8_ATOMIC_1(to,from)
 691             return;
 692     case 2: MOVE8_ATOMIC_2(to,from)
 693             return;
 694 //  case 3: MOVE8_ATOMIC_3(to,from)
 695 //          return;
 696 //  case 4: MOVE8_ATOMIC_4(to,from)
 697 //          return;
 698     default:
 699       if (len_bytes &lt;= 4096) {
 700         MVC_MULTI(to,from,len_bytes)
 701         return;
 702       }
 703       // else
 704       MVCLE_MEMCOPY(to_bytes, from_bytes, len_bytes)
 705       return;
 706   }
 707 #else
 708   // Fallback code.
 709   switch (count) {
 710     case 0:
 711       return;
 712 
 713     case 1:
 714       *to = *from;
 715       return;
 716 
 717     case 2:
 718       *to++ = *from++;
 719       *to = *from;
 720       return;
 721 
 722     case 3:
 723       *to++ = *from++;
 724       *to++ = *from++;
 725       *to = *from;
 726       return;
 727 
 728     case 4:
 729       *to++ = *from++;
 730       *to++ = *from++;
 731       *to++ = *from++;
 732       *to = *from;
 733       return;
 734 
 735     default:
 736       while (count-- &gt; 0)
 737         *(to++) = *(from++);
 738       return;
 739   }
 740 #endif
 741 }
 742 
 743 static void pd_disjoint_words_atomic(const HeapWord* from, HeapWord* to, size_t count) {
 744   // JVM2008: &lt; 4k calls.
 745   assert(((((size_t)from) &amp; 0x07L) | (((size_t)to) &amp; 0x07L)) == 0, &quot;No atomic copy w/o aligned data&quot;);
 746   pd_aligned_disjoint_words(from, to, count); // Rare calls -&gt; just delegate.
 747 }
 748 
 749 static void pd_disjoint_words(const HeapWord* from, HeapWord* to, size_t count) {
 750   // JVM2008: very rare.
 751   pd_aligned_disjoint_words(from, to, count); // Rare calls -&gt; just delegate.
 752 }
 753 
 754 
 755 //*************************************//
 756 //   C O N J O I N T   C O P Y I N G   //
 757 //*************************************//
 758 
 759 static void pd_aligned_conjoint_words(const HeapWord* from, HeapWord* to, size_t count) {
 760   // JVM2008: between some and lower end of frequent.
 761 
 762 #ifdef USE_INLINE_ASM
 763   size_t  count_in = count;
 764   if (has_destructive_overlap((char*)from, (char*)to, count_in*BytesPerLong)) {
 765     switch (count_in) {
 766       case 4: COPY8_ATOMIC_4(to,from)
 767               return;
 768       case 3: COPY8_ATOMIC_3(to,from)
 769               return;
 770       case 2: COPY8_ATOMIC_2(to,from)
 771               return;
 772       case 1: COPY8_ATOMIC_1(to,from)
 773               return;
 774       case 0: return;
 775       default:
 776         from += count_in;
 777         to   += count_in;
 778         while (count_in-- &gt; 0)
 779           *(--to) = *(--from); // Copy backwards, areas overlap destructively.
 780         return;
 781     }
 782   }
 783   // else
 784   jbyte* to_bytes   = (jbyte*)to;
 785   jbyte* from_bytes = (jbyte*)from;
 786   size_t len_bytes  = count_in*BytesPerLong;
 787   MVCLE_MEMCOPY(to_bytes, from_bytes, len_bytes)
 788   return;
 789 #else
 790   // Fallback code.
 791   if (has_destructive_overlap((char*)from, (char*)to, count*BytesPerLong)) {
 792     HeapWord t1, t2, t3;
 793     switch (count) {
 794       case 0:
 795         return;
 796 
 797       case 1:
 798         *to = *from;
 799         return;
 800 
 801       case 2:
 802         t1 = *(from+1);
 803         *to = *from;
 804         *(to+1) = t1;
 805         return;
 806 
 807       case 3:
 808         t1 = *(from+1);
 809         t2 = *(from+2);
 810         *to = *from;
 811         *(to+1) = t1;
 812         *(to+2) = t2;
 813         return;
 814 
 815       case 4:
 816         t1 = *(from+1);
 817         t2 = *(from+2);
 818         t3 = *(from+3);
 819         *to = *from;
 820         *(to+1) = t1;
 821         *(to+2) = t2;
 822         *(to+3) = t3;
 823         return;
 824 
 825       default:
 826         from += count;
 827         to   += count;
 828         while (count-- &gt; 0)
 829           *(--to) = *(--from); // Copy backwards, areas overlap destructively.
 830         return;
 831     }
 832   }
 833   // else
 834   // Just delegate. HeapWords are optimally aligned anyway.
 835   pd_aligned_disjoint_words(from, to, count);
 836 #endif
 837 }
 838 
 839 static void pd_conjoint_words(const HeapWord* from, HeapWord* to, size_t count) {
 840 
 841   // Just delegate. HeapWords are optimally aligned anyway.
 842   pd_aligned_conjoint_words(from, to, count);
 843 }
 844 
 845 static void pd_conjoint_bytes(const void* from, void* to, size_t count) {
 846 
 847 #ifdef USE_INLINE_ASM
 848   size_t count_in = count;
 849   if (has_destructive_overlap((char*)from, (char*)to, count_in))
 850     (void)memmove(to, from, count_in);
 851   else {
 852     jbyte*  to_bytes   = (jbyte*)to;
 853     jbyte*  from_bytes = (jbyte*)from;
 854     size_t  len_bytes  = count_in;
 855     MVCLE_MEMCOPY(to_bytes, from_bytes, len_bytes)
 856   }
 857 #else
 858   if (has_destructive_overlap((char*)from, (char*)to, count))
 859     (void)memmove(to, from, count);
 860   else
 861     (void)memcpy(to, from, count);
 862 #endif
 863 }
 864 
 865 //**************************************************//
 866 //   C O N J O I N T  A T O M I C   C O P Y I N G   //
 867 //**************************************************//
 868 
 869 static void pd_conjoint_bytes_atomic(const void* from, void* to, size_t count) {
 870   // Call arraycopy stubs to do the job.
 871   pd_conjoint_bytes(from, to, count); // bytes are always accessed atomically.
 872 }
 873 
 874 static void pd_conjoint_jshorts_atomic(const jshort* from, jshort* to, size_t count) {
 875 
 876 #ifdef USE_INLINE_ASM
 877   size_t count_in = count;
 878   if (has_destructive_overlap((const char*)from, (char*)to, count_in*BytesPerShort)) {
 879     // Use optimizations from shared code where no z-specific optimization exists.
 880     copy_conjoint_jshorts_atomic(from, to, count);
 881   } else {
 882     jbyte* to_bytes   = (jbyte*)to;
 883     jbyte* from_bytes = (jbyte*)from;
 884     size_t len_bytes  = count_in*BytesPerShort;
 885     MVCLE_MEMCOPY(to_bytes, from_bytes, len_bytes)
 886   }
 887 #else
 888   // Use optimizations from shared code where no z-specific optimization exists.
 889   copy_conjoint_jshorts_atomic(from, to, count);
 890 #endif
 891 }
 892 
 893 static void pd_conjoint_jints_atomic(const jint* from, jint* to, size_t count) {
 894 
 895 #ifdef USE_INLINE_ASM
 896   size_t count_in = count;
 897   if (has_destructive_overlap((const char*)from, (char*)to, count_in*BytesPerInt)) {
 898     switch (count_in) {
 899       case 4: COPY4_ATOMIC_4(to,from)
 900               return;
 901       case 3: COPY4_ATOMIC_3(to,from)
 902               return;
 903       case 2: COPY4_ATOMIC_2(to,from)
 904               return;
 905       case 1: COPY4_ATOMIC_1(to,from)
 906               return;
 907       case 0: return;
 908       default:
 909         // Use optimizations from shared code where no z-specific optimization exists.
 910         copy_conjoint_jints_atomic(from, to, count_in);
 911         return;
 912     }
 913   }
 914   // else
 915   jbyte* to_bytes   = (jbyte*)to;
 916   jbyte* from_bytes = (jbyte*)from;
 917   size_t len_bytes  = count_in*BytesPerInt;
 918   MVCLE_MEMCOPY(to_bytes, from_bytes, len_bytes)
 919 #else
 920   // Use optimizations from shared code where no z-specific optimization exists.
 921   copy_conjoint_jints_atomic(from, to, count);
 922 #endif
 923 }
 924 
 925 static void pd_conjoint_jlongs_atomic(const jlong* from, jlong* to, size_t count) {
 926 
 927 #ifdef USE_INLINE_ASM
 928   size_t count_in = count;
 929   if (has_destructive_overlap((char*)from, (char*)to, count_in*BytesPerLong)) {
 930     switch (count_in) {
 931       case 4: COPY8_ATOMIC_4(to,from) return;
 932       case 3: COPY8_ATOMIC_3(to,from) return;
 933       case 2: COPY8_ATOMIC_2(to,from) return;
 934       case 1: COPY8_ATOMIC_1(to,from) return;
 935       case 0: return;
 936       default:
 937         from += count_in;
 938         to   += count_in;
 939         while (count_in-- &gt; 0) { *(--to) = *(--from); } // Copy backwards, areas overlap destructively.
 940         return;
 941     }
 942   }
 943   // else {
 944   jbyte* to_bytes   = (jbyte*)to;
 945   jbyte* from_bytes = (jbyte*)from;
 946   size_t len_bytes  = count_in*BytesPerLong;
 947   MVCLE_MEMCOPY(to_bytes, from_bytes, len_bytes)
 948 #else
 949   size_t count_in = count;
 950   if (has_destructive_overlap((char*)from, (char*)to, count_in*BytesPerLong)) {
 951     if (count_in &lt; 8) {
 952       from += count_in;
 953       to   += count_in;
 954       while (count_in-- &gt; 0)
 955          *(--to) = *(--from); // Copy backwards, areas overlap destructively.
 956       return;
 957     }
 958     // else {
 959     from += count_in-1;
 960     to   += count_in-1;
 961     if (count_in&amp;0x01) {
 962       *(to--) = *(from--);
 963       count_in--;
 964     }
 965     for (; count_in&gt;0; count_in-=2) {
 966       *to     = *from;
 967       *(to-1) = *(from-1);
 968       to     -= 2;
 969       from   -= 2;
 970     }
 971   }
 972   else
 973     pd_aligned_disjoint_words((const HeapWord*)from, (HeapWord*)to, count_in); // rare calls -&gt; just delegate.
 974 #endif
 975 }
 976 
 977 static void pd_conjoint_oops_atomic(const oop* from, oop* to, size_t count) {
 978 
 979 #ifdef USE_INLINE_ASM
 980   size_t count_in = count;
 981   if (has_destructive_overlap((char*)from, (char*)to, count_in*BytesPerOop)) {
 982     switch (count_in) {
 983       case 4: COPY8_ATOMIC_4(to,from) return;
 984       case 3: COPY8_ATOMIC_3(to,from) return;
 985       case 2: COPY8_ATOMIC_2(to,from) return;
 986       case 1: COPY8_ATOMIC_1(to,from) return;
 987       case 0: return;
 988       default:
 989         from += count_in;
 990         to   += count_in;
 991         while (count_in-- &gt; 0) { *(--to) = *(--from); } // Copy backwards, areas overlap destructively.
 992         return;
 993     }
 994   }
 995   // else
 996   jbyte* to_bytes   = (jbyte*)to;
 997   jbyte* from_bytes = (jbyte*)from;
 998   size_t len_bytes  = count_in*BytesPerOop;
 999   MVCLE_MEMCOPY(to_bytes, from_bytes, len_bytes)
1000 #else
1001   size_t count_in = count;
1002   if (has_destructive_overlap((char*)from, (char*)to, count_in*BytesPerOop)) {
1003     from += count_in;
1004     to   += count_in;
1005     while (count_in-- &gt; 0) *(--to) = *(--from); // Copy backwards, areas overlap destructively.
1006     return;
1007   }
1008   // else
1009   pd_aligned_disjoint_words((HeapWord*)from, (HeapWord*)to, count_in); // rare calls -&gt; just delegate.
1010   return;
1011 #endif
1012 }
1013 
1014 static void pd_arrayof_conjoint_bytes(const HeapWord* from, HeapWord* to, size_t count) {
1015   pd_conjoint_bytes_atomic(from, to, count);
1016 }
1017 
1018 static void pd_arrayof_conjoint_jshorts(const HeapWord* from, HeapWord* to, size_t count) {
1019   pd_conjoint_jshorts_atomic((const jshort*)from, (jshort*)to, count);
1020 }
1021 
1022 static void pd_arrayof_conjoint_jints(const HeapWord* from, HeapWord* to, size_t count) {
1023   pd_conjoint_jints_atomic((const jint*)from, (jint*)to, count);
1024 }
1025 
1026 static void pd_arrayof_conjoint_jlongs(const HeapWord* from, HeapWord* to, size_t count) {
1027   pd_conjoint_jlongs_atomic((const jlong*)from, (jlong*)to, count);
1028 }
1029 
1030 static void pd_arrayof_conjoint_oops(const HeapWord* from, HeapWord* to, size_t count) {
1031   pd_conjoint_oops_atomic((const oop*)from, (oop*)to, count);
1032 }
1033 
1034 //**********************************************//
1035 //  M E M O R Y   I N I T I A L I S A T I O N   //
1036 //**********************************************//
1037 
1038 static void pd_fill_to_bytes(void* to, size_t count, jubyte value) {
1039   // JVM2008: very rare, only in some tests.
1040 #ifdef USE_INLINE_ASM
1041   // Initialize storage to a given value. Use memset instead of copy loop.
1042   // For large chunks of memory, exploit special H/W support of z/Architecture:
1043   // 1) init short piece of memory to page-align address
1044   // 2) init largest part (all contained full pages) of memory using mvcle instruction.
1045   //    z/Architecture processors have special H/W support for page-aligned storage
1046   //    where len is an int multiple of page size. In that case, up to 4 cache lines are
1047   //    processed in parallel and L1 cache is not polluted.
1048   // 3) init the remaining piece of memory.
1049   // Atomicity cannot really be an issue since gcc implements the loop body with XC anyway.
1050   // If atomicity is a problem, we have to prevent gcc optimization. Best workaround: inline asm.
1051 
1052   jbyte*  to_bytes  = (jbyte*)to;
1053   size_t  len_bytes = count;
1054 
1055   MVCLE_MEMINIT(to_bytes, value, len_bytes)
1056 
1057 #else
1058   // Memset does the best job possible: loop over 256-byte MVCs, with
1059   // the last MVC EXecuted. With the -mmvcle option, initialization
1060   // is done using MVCLE -&gt; slight advantage for large areas.
1061   (void)memset(to, value, count);
1062 #endif
1063 }
1064 
1065 static void pd_fill_to_words(HeapWord* tohw, size_t count, juint value) {
1066   // Occurs in dbg builds only. Usually memory poisoning with BAADBABE, DEADBEEF, etc.
1067   // JVM2008: &lt; 4k calls.
1068   if (value == 0) {
1069     pd_zero_to_words(tohw, count);
1070     return;
1071   }
1072   if (value == ~(juint)(0)) {
1073     pd_fill_to_bytes(tohw, count*HeapWordSize, (jubyte)(~(juint)(0)));
1074     return;
1075   }
1076   julong* to = (julong*) tohw;
1077   julong  v  = ((julong) value &lt;&lt; 32) | value;
1078   while (count-- &gt; 0) {
1079     *to++ = v;
1080   }
1081 }
1082 
1083 static void pd_fill_to_aligned_words(HeapWord* tohw, size_t count, juint value) {
1084   // JVM2008: very frequent, but virtually all calls are with value == 0.
1085   pd_fill_to_words(tohw, count, value);
1086 }
1087 
1088 //**********************************//
1089 //  M E M O R Y   C L E A R I N G   //
1090 //**********************************//
1091 
1092 // Delegate to pd_zero_to_bytes. It also works HeapWord-atomic.
1093 // Distinguish between simple and large zero_to_words.
1094 static void pd_zero_to_words(HeapWord* tohw, size_t count) {
1095   pd_zero_to_bytes(tohw, count*HeapWordSize);
1096 }
1097 
<a name="2" id="anc2"></a>





1098 static void pd_zero_to_bytes(void* to, size_t count) {
1099   // JVM2008: some calls (generally), some tests frequent
1100 #ifdef USE_INLINE_ASM
1101   // Even zero_to_bytes() requires HeapWord-atomic, or, at least, sequential
1102   // zeroing of the memory. MVCLE is not fit for that job:
1103   //   &quot;As observed by other CPUs and by the channel subsystem,
1104   //    that portion of the first operand which is filled
1105   //    with the padding byte is not necessarily stored into in
1106   //    a left-to-right direction and may appear to be stored
1107   //    into more than once.&quot;
1108   // Therefore, implementation was changed to use (multiple) XC instructions.
1109 
1110   const long line_size = 256;
1111   jbyte* to_bytes  = (jbyte*)to;
1112   size_t len_bytes = count;
1113 
1114   if (len_bytes &lt;= line_size) {
1115     XC_MEMZERO_256(to_bytes, len_bytes);
1116   } else {
1117     XC_MEMZERO_ANY(to_bytes, len_bytes);
1118   }
1119 
1120 #else
1121   // Memset does the best job possible: loop over 256-byte MVCs, with
1122   // the last MVC EXecuted. With the -mmvcle option, initialization
1123   // is done using MVCLE -&gt; slight advantage for large areas.
1124   (void)memset(to, 0, count);
1125 #endif
1126 }
1127 
1128 #endif // CPU_S390_COPY_S390_HPP
<a name="3" id="anc3"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="3" type="hidden" />
</body>
</html>