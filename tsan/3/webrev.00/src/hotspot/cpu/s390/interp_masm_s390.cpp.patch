diff a/src/hotspot/cpu/s390/interp_masm_s390.cpp b/src/hotspot/cpu/s390/interp_masm_s390.cpp
--- a/src/hotspot/cpu/s390/interp_masm_s390.cpp
+++ b/src/hotspot/cpu/s390/interp_masm_s390.cpp
@@ -1,8 +1,8 @@
 /*
- * Copyright (c) 2016, 2018, Oracle and/or its affiliates. All rights reserved.
- * Copyright (c) 2016, 2018 SAP SE. All rights reserved.
+ * Copyright (c) 2016, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2016, 2019 SAP SE. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -31,19 +31,20 @@
 #include "gc/shared/barrierSetAssembler.hpp"
 #include "interp_masm_s390.hpp"
 #include "interpreter/interpreter.hpp"
 #include "interpreter/interpreterRuntime.hpp"
 #include "oops/arrayOop.hpp"
-#include "oops/markOop.hpp"
+#include "oops/markWord.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "runtime/basicLock.hpp"
 #include "runtime/biasedLocking.hpp"
 #include "runtime/frame.inline.hpp"
 #include "runtime/safepointMechanism.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "runtime/thread.inline.hpp"
+#include "utilities/powerOfTwo.hpp"
 
 // Implementation of InterpreterMacroAssembler.
 // This file specializes the assembler with interpreter-specific macros.
 
 #ifdef PRODUCT
@@ -412,10 +413,23 @@
     get_cache_and_index_at_bcp(cache, tmp, bcp_offset, index_size);
     add2reg_with_index(cache, in_bytes(ConstantPoolCache::base_offset()), tmp, cache);
   BLOCK_COMMENT("}");
 }
 
+void InterpreterMacroAssembler::load_resolved_method_at_index(int byte_no,
+                                                              Register cache,
+                                                              Register cpe_offset,
+                                                              Register method) {
+  const int method_offset = in_bytes(
+    ConstantPoolCache::base_offset() +
+      ((byte_no == TemplateTable::f2_byte)
+       ? ConstantPoolCacheEntry::f2_offset()
+       : ConstantPoolCacheEntry::f1_offset()));
+
+  z_lg(method, Address(cache, cpe_offset, method_offset)); // get f1 Method*
+}
+
 // Generate a subtype check: branch to ok_is_subtype if sub_klass is
 // a subtype of super_klass. Blows registers Rsuper_klass, Rsub_klass, tmp1, tmp2.
 void InterpreterMacroAssembler::gen_subtype_check(Register Rsub_klass,
                                                   Register Rsuper_klass,
                                                   Register Rtmp1,
@@ -959,13 +973,13 @@
     return;
   }
 
   // template code:
   //
-  // markOop displaced_header = obj->mark().set_unlocked();
+  // markWord displaced_header = obj->mark().set_unlocked();
   // monitor->lock()->set_displaced_header(displaced_header);
-  // if (Atomic::cmpxchg(/*ex=*/monitor, /*addr*/obj->mark_addr(), /*cmp*/displaced_header) == displaced_header) {
+  // if (Atomic::cmpxchg(/*addr*/obj->mark_addr(), /*cmp*/displaced_header, /*ex=*/monitor) == displaced_header) {
   //   // We stored the monitor address into the object's mark word.
   // } else if (THREAD->is_lock_owned((address)displaced_header))
   //   // Simple recursive case.
   //   monitor->lock()->set_displaced_header(NULL);
   // } else {
@@ -978,29 +992,29 @@
   const Register current_header   = Z_ARG5;
 
   NearLabel done;
   NearLabel slow_case;
 
-  // markOop displaced_header = obj->mark().set_unlocked();
+  // markWord displaced_header = obj->mark().set_unlocked();
 
-  // Load markOop from object into displaced_header.
+  // Load markWord from object into displaced_header.
   z_lg(displaced_header, oopDesc::mark_offset_in_bytes(), object);
 
   if (UseBiasedLocking) {
     biased_locking_enter(object, displaced_header, Z_R1, Z_R0, done, &slow_case);
   }
 
-  // Set displaced_header to be (markOop of object | UNLOCK_VALUE).
-  z_oill(displaced_header, markOopDesc::unlocked_value);
+  // Set displaced_header to be (markWord of object | UNLOCK_VALUE).
+  z_oill(displaced_header, markWord::unlocked_value);
 
   // monitor->lock()->set_displaced_header(displaced_header);
 
   // Initialize the box (Must happen before we update the object mark!).
   z_stg(displaced_header, BasicObjectLock::lock_offset_in_bytes() +
                           BasicLock::displaced_header_offset_in_bytes(), monitor);
 
-  // if (Atomic::cmpxchg(/*ex=*/monitor, /*addr*/obj->mark_addr(), /*cmp*/displaced_header) == displaced_header) {
+  // if (Atomic::cmpxchg(/*addr*/obj->mark_addr(), /*cmp*/displaced_header, /*ex=*/monitor) == displaced_header) {
 
   // Store stack address of the BasicObjectLock (this is monitor) into object.
   add2reg(object_mark_addr, oopDesc::mark_offset_in_bytes(), object);
 
   z_csg(displaced_header, monitor, 0, object_mark_addr);
@@ -1012,19 +1026,19 @@
   //   // Simple recursive case.
   //   monitor->lock()->set_displaced_header(NULL);
 
   // We did not see an unlocked object so try the fast recursive case.
 
-  // Check if owner is self by comparing the value in the markOop of object
+  // Check if owner is self by comparing the value in the markWord of object
   // (current_header) with the stack pointer.
   z_sgr(current_header, Z_SP);
 
   assert(os::vm_page_size() > 0xfff, "page size too small - change the constant");
 
   // The prior sequence "LGR, NGR, LTGR" can be done better
   // (Z_R1 is temp and not used after here).
-  load_const_optimized(Z_R0, (~(os::vm_page_size()-1) | markOopDesc::lock_mask_in_place));
+  load_const_optimized(Z_R0, (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));
   z_ngr(Z_R0, current_header); // AND sets CC (result eq/ne 0)
 
   // If condition is true we are done and hence we can store 0 in the displaced
   // header indicating it is a recursive lock and be done.
   z_brne(slow_case);
@@ -1057,22 +1071,21 @@
 //
 // Throw IllegalMonitorException if object is not locked by current thread.
 void InterpreterMacroAssembler::unlock_object(Register monitor, Register object) {
 
   if (UseHeavyMonitors) {
-    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit),
-            monitor, /*check_for_exceptions=*/ true);
+    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit), monitor);
     return;
   }
 
 // else {
   // template code:
   //
   // if ((displaced_header = monitor->displaced_header()) == NULL) {
   //   // Recursive unlock. Mark the monitor unlocked by setting the object field to NULL.
   //   monitor->set_obj(NULL);
-  // } else if (Atomic::cmpxchg(displaced_header, obj->mark_addr(), monitor) == monitor) {
+  // } else if (Atomic::cmpxchg(obj->mark_addr(), monitor, displaced_header) == monitor) {
   //   // We swapped the unlocked mark in displaced_header into the object's mark word.
   //   monitor->set_obj(NULL);
   // } else {
   //   // Slow path.
   //   InterpreterRuntime::monitorexit(THREAD, monitor);
@@ -1109,11 +1122,11 @@
   MacroAssembler::load_and_test_long(displaced_header,
                                      Address(monitor, BasicObjectLock::lock_offset_in_bytes() +
                                                       BasicLock::displaced_header_offset_in_bytes()));
   z_bre(done); // displaced_header == 0 -> goto done
 
-  // } else if (Atomic::cmpxchg(displaced_header, obj->mark_addr(), monitor) == monitor) {
+  // } else if (Atomic::cmpxchg(obj->mark_addr(), monitor, displaced_header) == monitor) {
   //   // We swapped the unlocked mark in displaced_header into the object's mark word.
   //   monitor->set_obj(NULL);
 
   // If we still have a lightweight lock, unlock the object and be done.
 
@@ -1132,12 +1145,11 @@
   //   InterpreterRuntime::monitorexit(THREAD, monitor);
 
   // The lock has been converted into a heavy lock and hence
   // we need to get into the slow case.
   z_stg(object, obj_entry);   // Restore object entry, has been cleared above.
-  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit),
-          monitor,  /*check_for_exceptions=*/false);
+  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit), monitor);
 
   // }
 
   bind(done);
 }
@@ -1651,11 +1663,11 @@
     z_brz(null_seen);
   } else {
     compareU64_and_branch(obj, (intptr_t)0, Assembler::bcondEqual, null_seen);
   }
 
-  verify_oop(obj);
+  MacroAssembler::verify_oop(obj, FILE_AND_LINE);
   load_klass(klass, obj);
 
   // Klass seen before, nothing to do (regardless of unknown bit).
   z_lgr(Z_R0, tmp);
   assert(Immediate::is_uimm(~TypeEntries::type_klass_mask, 16), "or change following instruction");
@@ -1899,11 +1911,11 @@
 
   Label has_counters;
   load_and_test_long(Rcounters, Address(Rmethod, Method::method_counters_offset()));
   z_brnz(has_counters);
 
-  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::build_method_counters), Rmethod, false);
+  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::build_method_counters), Rmethod);
   z_ltgr(Rcounters, Z_RET); // Runtime call returns MethodCounters object.
   z_brz(skip); // No MethodCounters, out of memory.
 
   bind(has_counters);
 
@@ -2060,11 +2072,11 @@
   z_sllg(index, index, LogBytesPerWord);
   mem2reg_opt(dst, Address(Z_locals, index), false);
 }
 
 void InterpreterMacroAssembler::verify_oop(Register reg, TosState state) {
-  if (state == atos) { MacroAssembler::verify_oop(reg); }
+  if (state == atos) { MacroAssembler::verify_oop(reg, FILE_AND_LINE); }
 }
 
 // Inline assembly for:
 //
 // if (thread is in interp_only_mode) {
@@ -2080,11 +2092,11 @@
   // the code to check if the event should be sent.
   if (JvmtiExport::can_post_interpreter_events()) {
     Label jvmti_post_done;
     MacroAssembler::load_and_test_int(Z_R0, Address(Z_thread, JavaThread::interp_only_mode_offset()));
     z_bre(jvmti_post_done);
-    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_method_entry), /*check_exceptions=*/false);
+    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_method_entry));
     bind(jvmti_post_done);
   }
 }
 
 // Inline assembly for:
@@ -2114,11 +2126,11 @@
   if (mode == NotifyJVMTI && JvmtiExport::can_post_interpreter_events()) {
     Label jvmti_post_done;
     MacroAssembler::load_and_test_int(Z_R0, Address(Z_thread, JavaThread::interp_only_mode_offset()));
     z_bre(jvmti_post_done);
     if (!native_method) push(state); // see frame::interpreter_frame_result()
-    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_method_exit), /*check_exceptions=*/false);
+    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_method_exit));
     if (!native_method) pop(state);
     bind(jvmti_post_done);
   }
 
 #if 0
@@ -2173,11 +2185,11 @@
   // so it is important that sender_sp is the caller's sp as TOP_IJAVA_FRAME.
 
   Register R_f1_sender_sp = tmp1;
   Register R_f2_sp = tmp2;
 
-  // Tirst check the for the interpreter frame's magic.
+  // First check for the interpreter frame's magic.
   asm_assert_ijava_state_magic(R_f2_sp/*tmp*/);
   z_lg(R_f2_sp, _z_parent_ijava_frame_abi(callers_sp), Z_fp);
   z_lg(R_f1_sender_sp, _z_ijava_state_neg(sender_sp), Z_fp);
   if (return_pc->is_valid())
     z_lg(return_pc, _z_parent_ijava_frame_abi(return_pc), Z_fp);
