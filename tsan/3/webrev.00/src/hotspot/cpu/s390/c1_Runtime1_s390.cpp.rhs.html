<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/s390/c1_Runtime1_s390.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 2016, 2018, Oracle and/or its affiliates. All rights reserved.
  3  * Copyright (c) 2016 SAP SE. All rights reserved.
  4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  5  *
  6  * This code is free software; you can redistribute it and/or modify it
  7  * under the terms of the GNU General Public License version 2 only, as
  8  * published by the Free Software Foundation.
  9  *
 10  * This code is distributed in the hope that it will be useful, but WITHOUT
 11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 13  * version 2 for more details (a copy is included in the LICENSE file that
 14  * accompanied this code).
 15  *
 16  * You should have received a copy of the GNU General Public License version
 17  * 2 along with this work; if not, write to the Free Software Foundation,
 18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 19  *
 20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 21  * or visit www.oracle.com if you need additional information or have any
 22  * questions.
 23  *
 24  */
 25 
 26 #include &quot;precompiled.hpp&quot;
 27 #include &quot;asm/macroAssembler.inline.hpp&quot;
 28 #include &quot;c1/c1_Defs.hpp&quot;
 29 #include &quot;c1/c1_MacroAssembler.hpp&quot;
 30 #include &quot;c1/c1_Runtime1.hpp&quot;
 31 #include &quot;ci/ciUtilities.hpp&quot;
 32 #include &quot;gc/shared/cardTable.hpp&quot;
 33 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
 34 #include &quot;interpreter/interpreter.hpp&quot;
<a name="1" id="anc1"></a><span class="line-added"> 35 #include &quot;memory/universe.hpp&quot;</span>
 36 #include &quot;nativeInst_s390.hpp&quot;
 37 #include &quot;oops/compiledICHolder.hpp&quot;
 38 #include &quot;oops/oop.inline.hpp&quot;
 39 #include &quot;prims/jvmtiExport.hpp&quot;
 40 #include &quot;register_s390.hpp&quot;
<a name="2" id="anc2"></a><span class="line-added"> 41 #include &quot;registerSaver_s390.hpp&quot;</span>
 42 #include &quot;runtime/sharedRuntime.hpp&quot;
 43 #include &quot;runtime/signature.hpp&quot;
 44 #include &quot;runtime/vframeArray.hpp&quot;
 45 #include &quot;utilities/macros.hpp&quot;
<a name="3" id="anc3"></a><span class="line-added"> 46 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
 47 #include &quot;vmreg_s390.inline.hpp&quot;
<a name="4" id="anc4"></a>
 48 
 49 // Implementation of StubAssembler
 50 
 51 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry_point, int number_of_arguments) {
 52   set_num_rt_args(0); // Nothing on stack.
 53   assert(!(oop_result1-&gt;is_valid() || metadata_result-&gt;is_valid()) || oop_result1 != metadata_result, &quot;registers must be different&quot;);
 54 
 55   // We cannot trust that code generated by the C++ compiler saves R14
 56   // to z_abi_160.return_pc, because sometimes it spills R14 using stmg at
 57   // z_abi_160.gpr14 (e.g. InterpreterRuntime::_new()).
 58   // Therefore we load the PC into Z_R1_scratch and let set_last_Java_frame() save
 59   // it into the frame anchor.
 60   address pc = get_PC(Z_R1_scratch);
 61   int call_offset = (int)(pc - addr_at(0));
 62   set_last_Java_frame(Z_SP, Z_R1_scratch);
 63 
 64   // ARG1 must hold thread address.
 65   z_lgr(Z_ARG1, Z_thread);
 66 
 67   address return_pc = NULL;
 68   align_call_far_patchable(this-&gt;pc());
 69   return_pc = call_c_opt(entry_point);
 70   assert(return_pc != NULL, &quot;const section overflow&quot;);
 71 
 72   reset_last_Java_frame();
 73 
 74   // Check for pending exceptions.
 75   {
 76     load_and_test_long(Z_R0_scratch, Address(Z_thread, Thread::pending_exception_offset()));
 77 
 78     // This used to conditionally jump to forward_exception however it is
 79     // possible if we relocate that the branch will not reach. So we must jump
 80     // around so we can always reach.
 81 
 82     Label ok;
 83     z_bre(ok); // Bcondequal is the same as bcondZero.
 84 
 85     // exception pending =&gt; forward to exception handler
 86 
 87     // Make sure that the vm_results are cleared.
 88     if (oop_result1-&gt;is_valid()) {
 89       clear_mem(Address(Z_thread, JavaThread::vm_result_offset()), sizeof(jlong));
 90     }
 91     if (metadata_result-&gt;is_valid()) {
 92       clear_mem(Address(Z_thread, JavaThread::vm_result_2_offset()), sizeof(jlong));
 93     }
 94     if (frame_size() == no_frame_size) {
 95       // Pop the stub frame.
 96       pop_frame();
 97       restore_return_pc();
 98       load_const_optimized(Z_R1, StubRoutines::forward_exception_entry());
 99       z_br(Z_R1);
100     } else if (_stub_id == Runtime1::forward_exception_id) {
101       should_not_reach_here();
102     } else {
103       load_const_optimized(Z_R1, Runtime1::entry_for (Runtime1::forward_exception_id));
104       z_br(Z_R1);
105     }
106 
107     bind(ok);
108   }
109 
110   // Get oop results if there are any and reset the values in the thread.
111   if (oop_result1-&gt;is_valid()) {
112     get_vm_result(oop_result1);
113   }
114   if (metadata_result-&gt;is_valid()) {
115     get_vm_result_2(metadata_result);
116   }
117 
118   return call_offset;
119 }
120 
121 
122 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1) {
123   // Z_ARG1 is reserved for the thread.
124   lgr_if_needed(Z_ARG2, arg1);
125   return call_RT(oop_result1, metadata_result, entry, 1);
126 }
127 
128 
129 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1, Register arg2) {
130   // Z_ARG1 is reserved for the thread.
131   lgr_if_needed(Z_ARG2, arg1);
132   assert(arg2 != Z_ARG2, &quot;smashed argument&quot;);
133   lgr_if_needed(Z_ARG3, arg2);
134   return call_RT(oop_result1, metadata_result, entry, 2);
135 }
136 
137 
138 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1, Register arg2, Register arg3) {
139   // Z_ARG1 is reserved for the thread.
140   lgr_if_needed(Z_ARG2, arg1);
141   assert(arg2 != Z_ARG2, &quot;smashed argument&quot;);
142   lgr_if_needed(Z_ARG3, arg2);
143   assert(arg3 != Z_ARG3, &quot;smashed argument&quot;);
144   lgr_if_needed(Z_ARG4, arg3);
145   return call_RT(oop_result1, metadata_result, entry, 3);
146 }
147 
148 
149 // Implementation of Runtime1
150 
151 #define __ sasm-&gt;
152 
153 #ifndef PRODUCT
154 #undef  __
155 #define __ (Verbose ? (sasm-&gt;block_comment(FILE_AND_LINE),sasm):sasm)-&gt;
156 #endif // !PRODUCT
157 
158 #define BLOCK_COMMENT(str) if (PrintAssembly) __ block_comment(str)
159 #define BIND(label)        bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
160 
161 static OopMap* generate_oop_map(StubAssembler* sasm) {
162   RegisterSaver::RegisterSet reg_set = RegisterSaver::all_registers;
163   int frame_size_in_slots =
164     RegisterSaver::live_reg_frame_size(reg_set) / VMRegImpl::stack_slot_size;
165   sasm-&gt;set_frame_size(frame_size_in_slots / VMRegImpl::slots_per_word);
166   return RegisterSaver::generate_oop_map(sasm, reg_set);
167 }
168 
169 static OopMap* save_live_registers(StubAssembler* sasm, bool save_fpu_registers = true, Register return_pc = Z_R14) {
170   __ block_comment(&quot;save_live_registers&quot;);
171   RegisterSaver::RegisterSet reg_set =
172     save_fpu_registers ? RegisterSaver::all_registers : RegisterSaver::all_integer_registers;
173   int frame_size_in_slots =
174     RegisterSaver::live_reg_frame_size(reg_set) / VMRegImpl::stack_slot_size;
175   sasm-&gt;set_frame_size(frame_size_in_slots / VMRegImpl::slots_per_word);
176   return RegisterSaver::save_live_registers(sasm, reg_set, return_pc);
177 }
178 
179 static OopMap* save_live_registers_except_r2(StubAssembler* sasm, bool save_fpu_registers = true) {
180   if (!save_fpu_registers) {
181     __ unimplemented(FILE_AND_LINE);
182   }
183   __ block_comment(&quot;save_live_registers&quot;);
184   RegisterSaver::RegisterSet reg_set = RegisterSaver::all_registers_except_r2;
185   int frame_size_in_slots =
186       RegisterSaver::live_reg_frame_size(reg_set) / VMRegImpl::stack_slot_size;
187   sasm-&gt;set_frame_size(frame_size_in_slots / VMRegImpl::slots_per_word);
188   return RegisterSaver::save_live_registers(sasm, reg_set);
189 }
190 
191 static void restore_live_registers(StubAssembler* sasm, bool restore_fpu_registers = true) {
192   __ block_comment(&quot;restore_live_registers&quot;);
193   RegisterSaver::RegisterSet reg_set =
194     restore_fpu_registers ? RegisterSaver::all_registers : RegisterSaver::all_integer_registers;
195   RegisterSaver::restore_live_registers(sasm, reg_set);
196 }
197 
198 static void restore_live_registers_except_r2(StubAssembler* sasm, bool restore_fpu_registers = true) {
199   if (!restore_fpu_registers) {
200     __ unimplemented(FILE_AND_LINE);
201   }
202   __ block_comment(&quot;restore_live_registers_except_r2&quot;);
203   RegisterSaver::restore_live_registers(sasm, RegisterSaver::all_registers_except_r2);
204 }
205 
206 void Runtime1::initialize_pd() {
207   // Nothing to do.
208 }
209 
210 OopMapSet* Runtime1::generate_exception_throw(StubAssembler* sasm, address target, bool has_argument) {
211   // Make a frame and preserve the caller&#39;s caller-save registers.
212   OopMap* oop_map = save_live_registers(sasm);
213   int call_offset;
214   if (!has_argument) {
215     call_offset = __ call_RT(noreg, noreg, target);
216   } else {
217     call_offset = __ call_RT(noreg, noreg, target, Z_R1_scratch, Z_R0_scratch);
218   }
219   OopMapSet* oop_maps = new OopMapSet();
220   oop_maps-&gt;add_gc_map(call_offset, oop_map);
221 
222   __ should_not_reach_here();
223   return oop_maps;
224 }
225 
226 void Runtime1::generate_unwind_exception(StubAssembler *sasm) {
227   // Incoming parameters: Z_EXC_OOP and Z_EXC_PC.
228   // Keep copies in callee-saved registers during runtime call.
229   const Register exception_oop_callee_saved = Z_R11;
230   const Register exception_pc_callee_saved = Z_R12;
231   // Other registers used in this stub.
232   const Register handler_addr = Z_R4;
233 
234   // Verify that only exception_oop, is valid at this time.
235   __ invalidate_registers(Z_EXC_OOP, Z_EXC_PC);
236 
237   // Check that fields in JavaThread for exception oop and issuing pc are set.
238   __ asm_assert_mem8_is_zero(in_bytes(JavaThread::exception_oop_offset()), Z_thread, &quot;exception oop already set : &quot; FILE_AND_LINE, 0);
239   __ asm_assert_mem8_is_zero(in_bytes(JavaThread::exception_pc_offset()), Z_thread, &quot;exception pc already set : &quot; FILE_AND_LINE, 0);
240 
241   // Save exception_oop and pc in callee-saved register to preserve it
242   // during runtime calls.
243   __ verify_not_null_oop(Z_EXC_OOP);
244   __ lgr_if_needed(exception_oop_callee_saved, Z_EXC_OOP);
245   __ lgr_if_needed(exception_pc_callee_saved, Z_EXC_PC);
246 
247   __ push_frame_abi160(0); // Runtime code needs the z_abi_160.
248 
249   // Search the exception handler address of the caller (using the return address).
250   __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), Z_thread, Z_EXC_PC);
251   // Z_RET(Z_R2): exception handler address of the caller.
252 
253   __ pop_frame();
254 
255   __ invalidate_registers(exception_oop_callee_saved, exception_pc_callee_saved, Z_RET);
256 
257   // Move result of call into correct register.
258   __ lgr_if_needed(handler_addr, Z_RET);
259 
260   // Restore exception oop and pc to Z_EXC_OOP and Z_EXC_PC (required convention of exception handler).
261   __ lgr_if_needed(Z_EXC_OOP, exception_oop_callee_saved);
262   __ lgr_if_needed(Z_EXC_PC, exception_pc_callee_saved);
263 
264   // Verify that there is really a valid exception in Z_EXC_OOP.
265   __ verify_not_null_oop(Z_EXC_OOP);
266 
267   __ z_br(handler_addr); // Jump to exception handler.
268 }
269 
270 OopMapSet* Runtime1::generate_patching(StubAssembler* sasm, address target) {
271   // Make a frame and preserve the caller&#39;s caller-save registers.
272   OopMap* oop_map = save_live_registers(sasm);
273 
274   // Call the runtime patching routine, returns non-zero if nmethod got deopted.
275   int call_offset = __ call_RT(noreg, noreg, target);
276   OopMapSet* oop_maps = new OopMapSet();
277   oop_maps-&gt;add_gc_map(call_offset, oop_map);
278 
279   // Re-execute the patched instruction or, if the nmethod was
280   // deoptmized, return to the deoptimization handler entry that will
281   // cause re-execution of the current bytecode.
282   DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
283   assert(deopt_blob != NULL, &quot;deoptimization blob must have been created&quot;);
284 
285   __ z_ltr(Z_RET, Z_RET); // return value == 0
286 
287   restore_live_registers(sasm);
288 
289   __ z_bcr(Assembler::bcondZero, Z_R14);
290 
291   // Return to the deoptimization handler entry for unpacking and
292   // rexecute if we simply returned then we&#39;d deopt as if any call we
293   // patched had just returned.
294   AddressLiteral dest(deopt_blob-&gt;unpack_with_reexecution());
295   __ load_const_optimized(Z_R1_scratch, dest);
296   __ z_br(Z_R1_scratch);
297 
298   return oop_maps;
299 }
300 
301 OopMapSet* Runtime1::generate_code_for(StubID id, StubAssembler* sasm) {
302 
303   // for better readability
304   const bool must_gc_arguments = true;
305   const bool dont_gc_arguments = false;
306 
307   // Default value; overwritten for some optimized stubs that are
308   // called from methods that do not use the fpu.
309   bool save_fpu_registers = true;
310 
311   // Stub code and info for the different stubs.
312   OopMapSet* oop_maps = NULL;
313   switch (id) {
314     case forward_exception_id:
315       {
316         oop_maps = generate_handle_exception(id, sasm);
317         // will not return
318       }
319       break;
320 
321     case new_instance_id:
322     case fast_new_instance_id:
323     case fast_new_instance_init_check_id:
324       {
325         Register klass    = Z_R11; // Incoming
326         Register obj      = Z_R2;  // Result
327 
328         if (id == new_instance_id) {
329           __ set_info(&quot;new_instance&quot;, dont_gc_arguments);
330         } else if (id == fast_new_instance_id) {
331           __ set_info(&quot;fast new_instance&quot;, dont_gc_arguments);
332         } else {
333           assert(id == fast_new_instance_init_check_id, &quot;bad StubID&quot;);
334           __ set_info(&quot;fast new_instance init check&quot;, dont_gc_arguments);
335         }
336 
337         OopMap* map = save_live_registers_except_r2(sasm);
338         int call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);
339         oop_maps = new OopMapSet();
340         oop_maps-&gt;add_gc_map(call_offset, map);
341         restore_live_registers_except_r2(sasm);
342 
<a name="5" id="anc5"></a><span class="line-modified">343         __ verify_oop(obj, FILE_AND_LINE);</span>
344         __ z_br(Z_R14);
345       }
346       break;
347 
348     case counter_overflow_id:
349       {
350         // Arguments :
351         //   bci    : stack param 0
352         //   method : stack param 1
353         //
354         Register bci = Z_ARG2, method = Z_ARG3;
355         // frame size in bytes
356         OopMap* map = save_live_registers(sasm);
357         const int frame_size = sasm-&gt;frame_size() * VMRegImpl::slots_per_word * VMRegImpl::stack_slot_size;
358         __ z_lg(bci,    0*BytesPerWord + FrameMap::first_available_sp_in_frame + frame_size, Z_SP);
359         __ z_lg(method, 1*BytesPerWord + FrameMap::first_available_sp_in_frame + frame_size, Z_SP);
360         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, counter_overflow), bci, method);
361         oop_maps = new OopMapSet();
362         oop_maps-&gt;add_gc_map(call_offset, map);
363         restore_live_registers(sasm);
364         __ z_br(Z_R14);
365       }
366       break;
367     case new_type_array_id:
368     case new_object_array_id:
369       {
370         Register length   = Z_R13; // Incoming
371         Register klass    = Z_R11; // Incoming
372         Register obj      = Z_R2;  // Result
373 
374         if (id == new_type_array_id) {
375           __ set_info(&quot;new_type_array&quot;, dont_gc_arguments);
376         } else {
377           __ set_info(&quot;new_object_array&quot;, dont_gc_arguments);
378         }
379 
380 #ifdef ASSERT
381         // Assert object type is really an array of the proper kind.
382         {
383           NearLabel ok;
384           Register t0 = obj;
385           __ mem2reg_opt(t0, Address(klass, Klass::layout_helper_offset()), false);
386           __ z_sra(t0, Klass::_lh_array_tag_shift);
387           int tag = ((id == new_type_array_id)
388                      ? Klass::_lh_array_tag_type_value
389                      : Klass::_lh_array_tag_obj_value);
390           __ compare32_and_branch(t0, tag, Assembler::bcondEqual, ok);
391           __ stop(&quot;assert(is an array klass)&quot;);
392           __ should_not_reach_here();
393           __ bind(ok);
394         }
395 #endif // ASSERT
396 
397         OopMap* map = save_live_registers_except_r2(sasm);
398         int call_offset;
399         if (id == new_type_array_id) {
400           call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_type_array), klass, length);
401         } else {
402           call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_object_array), klass, length);
403         }
404 
405         oop_maps = new OopMapSet();
406         oop_maps-&gt;add_gc_map(call_offset, map);
407         restore_live_registers_except_r2(sasm);
408 
<a name="6" id="anc6"></a><span class="line-modified">409         __ verify_oop(obj, FILE_AND_LINE);</span>
410         __ z_br(Z_R14);
411       }
412       break;
413 
414     case new_multi_array_id:
415       { __ set_info(&quot;new_multi_array&quot;, dont_gc_arguments);
416         // Z_R3,: klass
417         // Z_R4,: rank
418         // Z_R5: address of 1st dimension
419         OopMap* map = save_live_registers(sasm);
420         int call_offset = __ call_RT(Z_R2, noreg, CAST_FROM_FN_PTR(address, new_multi_array), Z_R3, Z_R4, Z_R5);
421 
422         oop_maps = new OopMapSet();
423         oop_maps-&gt;add_gc_map(call_offset, map);
424         restore_live_registers_except_r2(sasm);
425 
426         // Z_R2,: new multi array
<a name="7" id="anc7"></a><span class="line-modified">427         __ verify_oop(Z_R2, FILE_AND_LINE);</span>
428         __ z_br(Z_R14);
429       }
430       break;
431 
432     case register_finalizer_id:
433       {
434         __ set_info(&quot;register_finalizer&quot;, dont_gc_arguments);
435 
436         // Load the klass and check the has finalizer flag.
437         Register klass = Z_ARG2;
438         __ load_klass(klass, Z_ARG1);
439         __ testbit(Address(klass, Klass::access_flags_offset()), exact_log2(JVM_ACC_HAS_FINALIZER));
440         __ z_bcr(Assembler::bcondAllZero, Z_R14); // Return if bit is not set.
441 
442         OopMap* oop_map = save_live_registers(sasm);
443         int call_offset = __ call_RT(noreg, noreg,
444                                      CAST_FROM_FN_PTR(address, SharedRuntime::register_finalizer), Z_ARG1);
445         oop_maps = new OopMapSet();
446         oop_maps-&gt;add_gc_map(call_offset, oop_map);
447 
448         // Now restore all the live registers.
449         restore_live_registers(sasm);
450 
451         __ z_br(Z_R14);
452       }
453       break;
454 
455     case throw_range_check_failed_id:
456       { __ set_info(&quot;range_check_failed&quot;, dont_gc_arguments);
457         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_range_check_exception), true);
458       }
459       break;
460 
461     case throw_index_exception_id:
462       { __ set_info(&quot;index_range_check_failed&quot;, dont_gc_arguments);
463         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_index_exception), true);
464       }
465       break;
466     case throw_div0_exception_id:
467       { __ set_info(&quot;throw_div0_exception&quot;, dont_gc_arguments);
468         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_div0_exception), false);
469       }
470       break;
471     case throw_null_pointer_exception_id:
472       { __ set_info(&quot;throw_null_pointer_exception&quot;, dont_gc_arguments);
473         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_null_pointer_exception), false);
474       }
475       break;
476     case handle_exception_nofpu_id:
477     case handle_exception_id:
478       { __ set_info(&quot;handle_exception&quot;, dont_gc_arguments);
479         oop_maps = generate_handle_exception(id, sasm);
480       }
481       break;
482     case handle_exception_from_callee_id:
483       { __ set_info(&quot;handle_exception_from_callee&quot;, dont_gc_arguments);
484         oop_maps = generate_handle_exception(id, sasm);
485       }
486       break;
487     case unwind_exception_id:
488       { __ set_info(&quot;unwind_exception&quot;, dont_gc_arguments);
489         // Note: no stubframe since we are about to leave the current
490         // activation and we are calling a leaf VM function only.
491         generate_unwind_exception(sasm);
492       }
493       break;
494     case throw_array_store_exception_id:
495       { __ set_info(&quot;throw_array_store_exception&quot;, dont_gc_arguments);
496         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_array_store_exception), true);
497       }
498       break;
499     case throw_class_cast_exception_id:
500     { // Z_R1_scratch: object
501       __ set_info(&quot;throw_class_cast_exception&quot;, dont_gc_arguments);
502       oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_class_cast_exception), true);
503     }
504     break;
505     case throw_incompatible_class_change_error_id:
506       { __ set_info(&quot;throw_incompatible_class_cast_exception&quot;, dont_gc_arguments);
507         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_incompatible_class_change_error), false);
508       }
509       break;
510     case slow_subtype_check_id:
511     {
512       // Arguments :
513       //   sub  : stack param 0
514       //   super: stack param 1
515       //   raddr: Z_R14, blown by call
516       //
517       // Result : condition code 0 for match (bcondEqual will be true),
518       //          condition code 2 for miss  (bcondNotEqual will be true)
519       NearLabel miss;
520       const Register Rsubklass   = Z_ARG2; // sub
521       const Register Rsuperklass = Z_ARG3; // super
522 
523       // No args, but tmp registers that are killed.
524       const Register Rlength     = Z_ARG4; // cache array length
525       const Register Rarray_ptr  = Z_ARG5; // Current value from cache array.
526 
527       if (UseCompressedOops) {
528         assert(Universe::heap() != NULL, &quot;java heap must be initialized to generate partial_subtype_check stub&quot;);
529       }
530 
531       const int frame_size = 4*BytesPerWord + frame::z_abi_160_size;
532       // Save return pc. This is not necessary, but could be helpful
533       // in the case of crashes.
534       __ save_return_pc();
535       __ push_frame(frame_size);
536       // Save registers before changing them.
537       int i = 0;
538       __ z_stg(Rsubklass,   (i++)*BytesPerWord + frame::z_abi_160_size, Z_SP);
539       __ z_stg(Rsuperklass, (i++)*BytesPerWord + frame::z_abi_160_size, Z_SP);
540       __ z_stg(Rlength,     (i++)*BytesPerWord + frame::z_abi_160_size, Z_SP);
541       __ z_stg(Rarray_ptr,  (i++)*BytesPerWord + frame::z_abi_160_size, Z_SP);
542       assert(i*BytesPerWord + frame::z_abi_160_size == frame_size, &quot;check&quot;);
543 
544       // Get sub and super from stack.
545       __ z_lg(Rsubklass,   0*BytesPerWord + FrameMap::first_available_sp_in_frame + frame_size, Z_SP);
546       __ z_lg(Rsuperklass, 1*BytesPerWord + FrameMap::first_available_sp_in_frame + frame_size, Z_SP);
547 
548       __ check_klass_subtype_slow_path(Rsubklass, Rsuperklass, Rarray_ptr, Rlength, NULL, &amp;miss);
549 
550       // Match falls through here.
551       i = 0;
552       __ z_lg(Rsubklass,   (i++)*BytesPerWord + frame::z_abi_160_size, Z_SP);
553       __ z_lg(Rsuperklass, (i++)*BytesPerWord + frame::z_abi_160_size, Z_SP);
554       __ z_lg(Rlength,     (i++)*BytesPerWord + frame::z_abi_160_size, Z_SP);
555       __ z_lg(Rarray_ptr,  (i++)*BytesPerWord + frame::z_abi_160_size, Z_SP);
556       assert(i*BytesPerWord + frame::z_abi_160_size == frame_size, &quot;check&quot;);
557       __ pop_frame();
558       // Return pc is still in R_14.
559       __ clear_reg(Z_R0_scratch);         // Zero indicates a match. Set CC 0 (bcondEqual will be true)
560       __ z_br(Z_R14);
561 
562       __ BIND(miss);
563       i = 0;
564       __ z_lg(Rsubklass,   (i++)*BytesPerWord + frame::z_abi_160_size, Z_SP);
565       __ z_lg(Rsuperklass, (i++)*BytesPerWord + frame::z_abi_160_size, Z_SP);
566       __ z_lg(Rlength,     (i++)*BytesPerWord + frame::z_abi_160_size, Z_SP);
567       __ z_lg(Rarray_ptr,  (i++)*BytesPerWord + frame::z_abi_160_size, Z_SP);
568       assert(i*BytesPerWord + frame::z_abi_160_size == frame_size, &quot;check&quot;);
569       __ pop_frame();
570       // return pc is still in R_14
571       __ load_const_optimized(Z_R0_scratch, 1); // One indicates a miss.
572       __ z_ltgr(Z_R0_scratch, Z_R0_scratch);    // Set CC 2 (bcondNotEqual will be true).
573       __ z_br(Z_R14);
574     }
575     break;
576     case monitorenter_nofpu_id:
577     case monitorenter_id:
578       { // Z_R1_scratch : object
579         // Z_R13       : lock address (see LIRGenerator::syncTempOpr())
580         __ set_info(&quot;monitorenter&quot;, dont_gc_arguments);
581 
582         int save_fpu_registers = (id == monitorenter_id);
583         // Make a frame and preserve the caller&#39;s caller-save registers.
584         OopMap* oop_map = save_live_registers(sasm, save_fpu_registers);
585 
586         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, monitorenter), Z_R1_scratch, Z_R13);
587 
588         oop_maps = new OopMapSet();
589         oop_maps-&gt;add_gc_map(call_offset, oop_map);
590         restore_live_registers(sasm, save_fpu_registers);
591 
592         __ z_br(Z_R14);
593       }
594       break;
595 
596     case monitorexit_nofpu_id:
597     case monitorexit_id:
598       { // Z_R1_scratch : lock address
599         // Note: really a leaf routine but must setup last java sp
600         //   =&gt; Use call_RT for now (speed can be improved by
601         //      doing last java sp setup manually).
602         __ set_info(&quot;monitorexit&quot;, dont_gc_arguments);
603 
604         int save_fpu_registers = (id == monitorexit_id);
605         // Make a frame and preserve the caller&#39;s caller-save registers.
606         OopMap* oop_map = save_live_registers(sasm, save_fpu_registers);
607 
608         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, monitorexit), Z_R1_scratch);
609 
610         oop_maps = new OopMapSet();
611         oop_maps-&gt;add_gc_map(call_offset, oop_map);
612         restore_live_registers(sasm, save_fpu_registers);
613 
614         __ z_br(Z_R14);
615       }
616       break;
617 
618     case deoptimize_id:
619       { // Args: Z_R1_scratch: trap request
620         __ set_info(&quot;deoptimize&quot;, dont_gc_arguments);
621         Register trap_request = Z_R1_scratch;
622         OopMap* oop_map = save_live_registers(sasm);
623         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, deoptimize), trap_request);
624         oop_maps = new OopMapSet();
625         oop_maps-&gt;add_gc_map(call_offset, oop_map);
626         restore_live_registers(sasm);
627         DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
628         assert(deopt_blob != NULL, &quot;deoptimization blob must have been created&quot;);
629         AddressLiteral dest(deopt_blob-&gt;unpack_with_reexecution());
630         __ load_const_optimized(Z_R1_scratch, dest);
631         __ z_br(Z_R1_scratch);
632       }
633       break;
634 
635     case access_field_patching_id:
636       { __ set_info(&quot;access_field_patching&quot;, dont_gc_arguments);
637         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, access_field_patching));
638       }
639       break;
640 
641     case load_klass_patching_id:
642       { __ set_info(&quot;load_klass_patching&quot;, dont_gc_arguments);
643         // We should set up register map.
644         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_klass_patching));
645       }
646       break;
647 
648     case load_mirror_patching_id:
649       { __ set_info(&quot;load_mirror_patching&quot;, dont_gc_arguments);
650         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_mirror_patching));
651       }
652       break;
653 
654     case load_appendix_patching_id:
655       { __ set_info(&quot;load_appendix_patching&quot;, dont_gc_arguments);
656         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_appendix_patching));
657       }
658       break;
659 #if 0
660     case dtrace_object_alloc_id:
661       { // rax,: object
662         StubFrame f(sasm, &quot;dtrace_object_alloc&quot;, dont_gc_arguments);
663         // We can&#39;t gc here so skip the oopmap but make sure that all
664         // the live registers get saved.
665         save_live_registers(sasm, 1);
666 
667         __ NOT_LP64(push(rax)) LP64_ONLY(mov(c_rarg0, rax));
668         __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc)));
669         NOT_LP64(__ pop(rax));
670 
671         restore_live_registers(sasm);
672       }
673       break;
674 
675     case fpu2long_stub_id:
676       {
677         // rax, and rdx are destroyed, but should be free since the result is returned there
678         // preserve rsi,ecx
679         __ push(rsi);
680         __ push(rcx);
681         LP64_ONLY(__ push(rdx);)
682 
683         // check for NaN
684         Label return0, do_return, return_min_jlong, do_convert;
685 
686         Address value_high_word(rsp, wordSize + 4);
687         Address value_low_word(rsp, wordSize);
688         Address result_high_word(rsp, 3*wordSize + 4);
689         Address result_low_word(rsp, 3*wordSize);
690 
691         __ subptr(rsp, 32);                    // more than enough on 32bit
692         __ fst_d(value_low_word);
693         __ movl(rax, value_high_word);
694         __ andl(rax, 0x7ff00000);
695         __ cmpl(rax, 0x7ff00000);
696         __ jcc(Assembler::notEqual, do_convert);
697         __ movl(rax, value_high_word);
698         __ andl(rax, 0xfffff);
699         __ orl(rax, value_low_word);
700         __ jcc(Assembler::notZero, return0);
701 
702         __ bind(do_convert);
703         __ fnstcw(Address(rsp, 0));
704         __ movzwl(rax, Address(rsp, 0));
705         __ orl(rax, 0xc00);
706         __ movw(Address(rsp, 2), rax);
707         __ fldcw(Address(rsp, 2));
708         __ fwait();
709         __ fistp_d(result_low_word);
710         __ fldcw(Address(rsp, 0));
711         __ fwait();
712         // This gets the entire long in rax on 64bit
713         __ movptr(rax, result_low_word);
714         // testing of high bits
715         __ movl(rdx, result_high_word);
716         __ mov(rcx, rax);
717         // What the heck is the point of the next instruction???
718         __ xorl(rcx, 0x0);
719         __ movl(rsi, 0x80000000);
720         __ xorl(rsi, rdx);
721         __ orl(rcx, rsi);
722         __ jcc(Assembler::notEqual, do_return);
723         __ fldz();
724         __ fcomp_d(value_low_word);
725         __ fnstsw_ax();
726         __ testl(rax, 0x4100);  // ZF &amp; CF == 0
727         __ jcc(Assembler::equal, return_min_jlong);
728         // return max_jlong
729         __ mov64(rax, CONST64(0x7fffffffffffffff));
730         __ jmp(do_return);
731 
732         __ bind(return_min_jlong);
733         __ mov64(rax, UCONST64(0x8000000000000000));
734         __ jmp(do_return);
735 
736         __ bind(return0);
737         __ fpop();
738         __ xorptr(rax, rax);
739 
740         __ bind(do_return);
741         __ addptr(rsp, 32);
742         LP64_ONLY(__ pop(rdx);)
743         __ pop(rcx);
744         __ pop(rsi);
745         __ ret(0);
746       }
747       break;
748 #endif // TODO
749 
750     case predicate_failed_trap_id:
751       {
752         __ set_info(&quot;predicate_failed_trap&quot;, dont_gc_arguments);
753 
754         OopMap* map = save_live_registers(sasm);
755 
756         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, predicate_failed_trap));
757         oop_maps = new OopMapSet();
758         oop_maps-&gt;add_gc_map(call_offset, map);
759         restore_live_registers(sasm);
760 
761         DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
762         assert(deopt_blob != NULL, &quot;deoptimization blob must have been created&quot;);
763 
764         __ load_const_optimized(Z_R1_scratch, deopt_blob-&gt;unpack_with_reexecution());
765         __ z_br(Z_R1_scratch);
766       }
767       break;
768 
769     default:
770       {
771         __ should_not_reach_here(FILE_AND_LINE, id);
772       }
773       break;
774   }
775   return oop_maps;
776 }
777 
778 OopMapSet* Runtime1::generate_handle_exception(StubID id, StubAssembler *sasm) {
779   __ block_comment(&quot;generate_handle_exception&quot;);
780 
781   // incoming parameters: Z_EXC_OOP, Z_EXC_PC
782 
783   // Save registers if required.
784   OopMapSet* oop_maps = new OopMapSet();
785   OopMap* oop_map = NULL;
786   Register reg_fp = Z_R1_scratch;
787 
788   switch (id) {
789     case forward_exception_id: {
790       // We&#39;re handling an exception in the context of a compiled frame.
791       // The registers have been saved in the standard places. Perform
792       // an exception lookup in the caller and dispatch to the handler
793       // if found. Otherwise unwind and dispatch to the callers
794       // exception handler.
795       oop_map = generate_oop_map(sasm);
796 
797       // Load and clear pending exception oop into.
798       __ z_lg(Z_EXC_OOP, Address(Z_thread, Thread::pending_exception_offset()));
799       __ clear_mem(Address(Z_thread, Thread::pending_exception_offset()), 8);
800 
801       // Different stubs forward their exceptions; they should all have similar frame layouts
802       // (a) to find their return address (b) for a correct oop_map generated above.
803       assert(RegisterSaver::live_reg_frame_size(RegisterSaver::all_registers) ==
804              RegisterSaver::live_reg_frame_size(RegisterSaver::all_registers_except_r2), &quot;requirement&quot;);
805 
806       // Load issuing PC (the return address for this stub).
807       const int frame_size_in_bytes = sasm-&gt;frame_size() * VMRegImpl::slots_per_word * VMRegImpl::stack_slot_size;
808       __ z_lg(Z_EXC_PC, Address(Z_SP, frame_size_in_bytes + _z_abi16(return_pc)));
809       DEBUG_ONLY(__ z_lay(reg_fp, Address(Z_SP, frame_size_in_bytes));)
810 
811       // Make sure that the vm_results are cleared (may be unnecessary).
812       __ clear_mem(Address(Z_thread, JavaThread::vm_result_offset()),   sizeof(oop));
813       __ clear_mem(Address(Z_thread, JavaThread::vm_result_2_offset()), sizeof(Metadata*));
814       break;
815     }
816     case handle_exception_nofpu_id:
817     case handle_exception_id:
818       // At this point all registers MAY be live.
819       DEBUG_ONLY(__ z_lgr(reg_fp, Z_SP);)
820       oop_map = save_live_registers(sasm, id != handle_exception_nofpu_id, Z_EXC_PC);
821       break;
822     case handle_exception_from_callee_id: {
823       // At this point all registers except Z_EXC_OOP and Z_EXC_PC are dead.
824       DEBUG_ONLY(__ z_lgr(reg_fp, Z_SP);)
825       __ save_return_pc(Z_EXC_PC);
826       const int frame_size_in_bytes = __ push_frame_abi160(0);
827       oop_map = new OopMap(frame_size_in_bytes / VMRegImpl::stack_slot_size, 0);
828       sasm-&gt;set_frame_size(frame_size_in_bytes / BytesPerWord);
829       break;
830     }
831     default:  ShouldNotReachHere();
832   }
833 
834   // Verify that only Z_EXC_OOP, and Z_EXC_PC are valid at this time.
835   __ invalidate_registers(Z_EXC_OOP, Z_EXC_PC, reg_fp);
836   // Verify that Z_EXC_OOP, contains a valid exception.
837   __ verify_not_null_oop(Z_EXC_OOP);
838 
839   // Check that fields in JavaThread for exception oop and issuing pc
840   // are empty before writing to them.
841   __ asm_assert_mem8_is_zero(in_bytes(JavaThread::exception_oop_offset()), Z_thread, &quot;exception oop already set : &quot; FILE_AND_LINE, 0);
842   __ asm_assert_mem8_is_zero(in_bytes(JavaThread::exception_pc_offset()), Z_thread, &quot;exception pc already set : &quot; FILE_AND_LINE, 0);
843 
844   // Save exception oop and issuing pc into JavaThread.
845   // (Exception handler will load it from here.)
846   __ z_stg(Z_EXC_OOP, Address(Z_thread, JavaThread::exception_oop_offset()));
847   __ z_stg(Z_EXC_PC, Address(Z_thread, JavaThread::exception_pc_offset()));
848 
849 #ifdef ASSERT
850   { NearLabel ok;
851     __ z_cg(Z_EXC_PC, Address(reg_fp, _z_abi16(return_pc)));
852     __ branch_optimized(Assembler::bcondEqual, ok);
853     __ stop(&quot;use throwing pc as return address (has bci &amp; oop map)&quot;);
854     __ bind(ok);
855   }
856 #endif
857 
858   // Compute the exception handler.
859   // The exception oop and the throwing pc are read from the fields in JavaThread.
860   int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, exception_handler_for_pc));
861   oop_maps-&gt;add_gc_map(call_offset, oop_map);
862 
863   // Z_RET(Z_R2): handler address
864   //   will be the deopt blob if nmethod was deoptimized while we looked up
865   //   handler regardless of whether handler existed in the nmethod.
866 
867   // Only Z_R2, is valid at this time, all other registers have been destroyed by the runtime call.
868   __ invalidate_registers(Z_R2);
869 
870   switch(id) {
871     case forward_exception_id:
872     case handle_exception_nofpu_id:
873     case handle_exception_id:
874       // Restore the registers that were saved at the beginning.
875       __ z_lgr(Z_R1_scratch, Z_R2);   // Restoring live registers kills Z_R2.
876       restore_live_registers(sasm, id != handle_exception_nofpu_id);  // Pops as well the frame.
877       __ z_br(Z_R1_scratch);
878       break;
879     case handle_exception_from_callee_id: {
880       __ pop_frame();
881       __ z_br(Z_R2); // Jump to exception handler.
882     }
883     break;
884     default:  ShouldNotReachHere();
885   }
886 
887   return oop_maps;
888 }
889 
890 
891 #undef __
892 
893 const char *Runtime1::pd_name_for_address(address entry) {
894   return &quot;&lt;unknown function&gt;&quot;;
895 }
<a name="8" id="anc8"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="8" type="hidden" />
</body>
</html>