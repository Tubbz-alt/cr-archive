<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/arm/sharedRuntime_arm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="relocInfo_arm.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stubGenerator_arm.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/arm/sharedRuntime_arm.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2008, 2018, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/assembler.hpp&quot;
  27 #include &quot;assembler_arm.inline.hpp&quot;
  28 #include &quot;code/debugInfoRec.hpp&quot;
  29 #include &quot;code/icBuffer.hpp&quot;
  30 #include &quot;code/vtableStubs.hpp&quot;
  31 #include &quot;interpreter/interpreter.hpp&quot;
  32 #include &quot;logging/log.hpp&quot;
  33 #include &quot;memory/resourceArea.hpp&quot;
  34 #include &quot;oops/compiledICHolder.hpp&quot;

  35 #include &quot;runtime/sharedRuntime.hpp&quot;

  36 #include &quot;runtime/vframeArray.hpp&quot;
  37 #include &quot;utilities/align.hpp&quot;

  38 #include &quot;vmreg_arm.inline.hpp&quot;
  39 #ifdef COMPILER1
  40 #include &quot;c1/c1_Runtime1.hpp&quot;
  41 #endif
  42 #ifdef COMPILER2
  43 #include &quot;opto/runtime.hpp&quot;
  44 #endif
  45 
  46 #define __ masm-&gt;
  47 
  48 class RegisterSaver {
  49 public:
  50 
  51   // Special registers:
  52   //              32-bit ARM     64-bit ARM
  53   //  Rthread:       R10            R28
  54   //  LR:            R14            R30
  55 
  56   // Rthread is callee saved in the C ABI and never changed by compiled code:
  57   // no need to save it.
</pre>
<hr />
<pre>
 114 
 115 
 116 
 117 
 118 OopMap* RegisterSaver::save_live_registers(MacroAssembler* masm,
 119                                            int* total_frame_words,
 120                                            bool lr_saved) {
 121   *total_frame_words = reg_save_size;
 122 
 123   OopMapSet *oop_maps = new OopMapSet();
 124   OopMap* map = new OopMap(VMRegImpl::slots_per_word * (*total_frame_words), 0);
 125 
 126   if (lr_saved) {
 127     __ push(RegisterSet(FP));
 128   } else {
 129     __ push(RegisterSet(FP) | RegisterSet(LR));
 130   }
 131   __ push(SAVED_BASE_REGS);
 132   if (HaveVFP) {
 133     if (VM_Version::has_vfp3_32()) {
<span class="line-modified"> 134       __ fstmdbd(SP, FloatRegisterSet(D16, 16), writeback);</span>
 135     } else {
 136       if (FloatRegisterImpl::number_of_registers &gt; 32) {
 137         assert(FloatRegisterImpl::number_of_registers == 64, &quot;nb fp registers should be 64&quot;);
 138         __ sub(SP, SP, 32 * wordSize);
 139       }
 140     }
<span class="line-modified"> 141     __ fstmdbd(SP, FloatRegisterSet(D0, 16), writeback);</span>
 142   } else {
 143     __ sub(SP, SP, fpu_save_size * wordSize);
 144   }
 145 
 146   int i;
 147   int j=0;
 148   for (i = R0_offset; i &lt;= R9_offset; i++) {
 149     if (j == FP_REG_NUM) {
 150       // skip the FP register, managed below.
 151       j++;
 152     }
 153     map-&gt;set_callee_saved(VMRegImpl::stack2reg(i), as_Register(j)-&gt;as_VMReg());
 154     j++;
 155   }
 156   assert(j == R10-&gt;encoding(), &quot;must be&quot;);
 157 #if (FP_REG_NUM != 11)
 158   // add R11, if not managed as FP
 159   map-&gt;set_callee_saved(VMRegImpl::stack2reg(R11_offset), R11-&gt;as_VMReg());
 160 #endif
 161   map-&gt;set_callee_saved(VMRegImpl::stack2reg(R12_offset), R12-&gt;as_VMReg());
 162   map-&gt;set_callee_saved(VMRegImpl::stack2reg(R14_offset), R14-&gt;as_VMReg());
 163   if (HaveVFP) {
 164     for (i = 0; i &lt; (VM_Version::has_vfp3_32() ? 64 : 32); i+=2) {
 165       map-&gt;set_callee_saved(VMRegImpl::stack2reg(i), as_FloatRegister(i)-&gt;as_VMReg());
 166       map-&gt;set_callee_saved(VMRegImpl::stack2reg(i + 1), as_FloatRegister(i)-&gt;as_VMReg()-&gt;next());
 167     }
 168   }
 169 
 170   return map;
 171 }
 172 
 173 void RegisterSaver::restore_live_registers(MacroAssembler* masm, bool restore_lr) {
 174   if (HaveVFP) {
<span class="line-modified"> 175     __ fldmiad(SP, FloatRegisterSet(D0, 16), writeback);</span>
 176     if (VM_Version::has_vfp3_32()) {
<span class="line-modified"> 177       __ fldmiad(SP, FloatRegisterSet(D16, 16), writeback);</span>
 178     } else {
 179       if (FloatRegisterImpl::number_of_registers &gt; 32) {
 180         assert(FloatRegisterImpl::number_of_registers == 64, &quot;nb fp registers should be 64&quot;);
 181         __ add(SP, SP, 32 * wordSize);
 182       }
 183     }
 184   } else {
 185     __ add(SP, SP, fpu_save_size * wordSize);
 186   }
 187   __ pop(SAVED_BASE_REGS);
 188   if (restore_lr) {
 189     __ pop(RegisterSet(FP) | RegisterSet(LR));
 190   } else {
 191     __ pop(RegisterSet(FP));
 192   }
 193 }
 194 
 195 
 196 static void push_result_registers(MacroAssembler* masm, BasicType ret_type) {
 197 #ifdef __ABI_HARD__
</pre>
<hr />
<pre>
 202   }
 203 #endif // __ABI_HARD__
 204   __ raw_push(R0, R1);
 205 }
 206 
 207 static void pop_result_registers(MacroAssembler* masm, BasicType ret_type) {
 208 #ifdef __ABI_HARD__
 209   if (ret_type == T_DOUBLE || ret_type == T_FLOAT) {
 210     __ fldd(D0, Address(SP));
 211     __ add(SP, SP, 8);
 212     return;
 213   }
 214 #endif // __ABI_HARD__
 215   __ raw_pop(R0, R1);
 216 }
 217 
 218 static void push_param_registers(MacroAssembler* masm, int fp_regs_in_arguments) {
 219   // R1-R3 arguments need to be saved, but we push 4 registers for 8-byte alignment
 220   __ push(RegisterSet(R0, R3));
 221 
<span class="line-removed"> 222 #ifdef __ABI_HARD__</span>
 223   // preserve arguments
 224   // Likely not needed as the locking code won&#39;t probably modify volatile FP registers,
 225   // but there is no way to guarantee that
 226   if (fp_regs_in_arguments) {
 227     // convert fp_regs_in_arguments to a number of double registers
 228     int double_regs_num = (fp_regs_in_arguments + 1) &gt;&gt; 1;
<span class="line-modified"> 229     __ fstmdbd(SP, FloatRegisterSet(D0, double_regs_num), writeback);</span>
 230   }
<span class="line-removed"> 231 #endif // __ ABI_HARD__</span>
 232 }
 233 
 234 static void pop_param_registers(MacroAssembler* masm, int fp_regs_in_arguments) {
<span class="line-removed"> 235 #ifdef __ABI_HARD__</span>
 236   if (fp_regs_in_arguments) {
 237     int double_regs_num = (fp_regs_in_arguments + 1) &gt;&gt; 1;
<span class="line-modified"> 238     __ fldmiad(SP, FloatRegisterSet(D0, double_regs_num), writeback);</span>
 239   }
<span class="line-removed"> 240 #endif // __ABI_HARD__</span>
<span class="line-removed"> 241 </span>
 242   __ pop(RegisterSet(R0, R3));
 243 }
 244 
 245 
 246 
 247 // Is vector&#39;s size (in bytes) bigger than a size saved by default?
 248 // All vector registers are saved by default on ARM.
 249 bool SharedRuntime::is_wide_vector(int size) {
 250   return false;
 251 }
 252 
 253 size_t SharedRuntime::trampoline_size() {
 254   return 16;
 255 }
 256 
 257 void SharedRuntime::generate_trampoline(MacroAssembler *masm, address destination) {
 258   InlinedAddress dest(destination);
 259   __ indirect_jump(dest, Rtemp);
 260   __ bind_literal(dest);
 261 }
</pre>
<hr />
<pre>
 443       regs[i].set_bad();
 444       break;
 445     default:
 446       ShouldNotReachHere();
 447     }
 448   }
 449 
 450   if (slot &amp; 1) slot++;
 451   return slot;
 452 }
 453 
 454 static void patch_callers_callsite(MacroAssembler *masm) {
 455   Label skip;
 456 
 457   __ ldr(Rtemp, Address(Rmethod, Method::code_offset()));
 458   __ cbz(Rtemp, skip);
 459 
 460   // Pushing an even number of registers for stack alignment.
 461   // Selecting R9, which had to be saved anyway for some platforms.
 462   __ push(RegisterSet(R0, R3) | R9 | LR);

 463 
 464   __ mov(R0, Rmethod);
 465   __ mov(R1, LR);
 466   __ call(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite));
 467 

 468   __ pop(RegisterSet(R0, R3) | R9 | LR);
 469 
 470   __ bind(skip);
 471 }
 472 
 473 void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,
 474                                     int total_args_passed, int comp_args_on_stack,
 475                                     const BasicType *sig_bt, const VMRegPair *regs) {
 476   // TODO: ARM - May be can use ldm to load arguments
 477   const Register tmp = Rtemp; // avoid erasing R5_mh
 478 
 479   // Next assert may not be needed but safer. Extra analysis required
 480   // if this there is not enough free registers and we need to use R5 here.
 481   assert_different_registers(tmp, R5_mh);
 482 
 483   // 6243940 We might end up in handle_wrong_method if
 484   // the callee is deoptimized as we race thru here. If that
 485   // happens we don&#39;t want to take a safepoint because the
 486   // caller frame will look interpreted and arguments are now
 487   // &quot;compiled&quot; so it is much better to make this transition
</pre>
<hr />
<pre>
 735       receiver_reg = r-&gt;as_Register();
 736     }
 737   }
 738 
 739   // Figure out which address we are really jumping to:
 740   MethodHandles::generate_method_handle_dispatch(masm, iid,
 741                                                  receiver_reg, member_reg, /*for_compiler_entry:*/ true);
 742 }
 743 
 744 // ---------------------------------------------------------------------------
 745 // Generate a native wrapper for a given method.  The method takes arguments
 746 // in the Java compiled code convention, marshals them to the native
 747 // convention (handlizes oops, etc), transitions to native, makes the call,
 748 // returns to java state (possibly blocking), unhandlizes any result and
 749 // returns.
 750 nmethod* SharedRuntime::generate_native_wrapper(MacroAssembler* masm,
 751                                                 const methodHandle&amp; method,
 752                                                 int compile_id,
 753                                                 BasicType* in_sig_bt,
 754                                                 VMRegPair* in_regs,
<span class="line-modified"> 755                                                 BasicType ret_type) {</span>

 756   if (method-&gt;is_method_handle_intrinsic()) {
 757     vmIntrinsics::ID iid = method-&gt;intrinsic_id();
 758     intptr_t start = (intptr_t)__ pc();
 759     int vep_offset = ((intptr_t)__ pc()) - start;
 760     gen_special_dispatch(masm,
 761                          method,
 762                          in_sig_bt,
 763                          in_regs);
 764     int frame_complete = ((intptr_t)__ pc()) - start;  // not complete, period
 765     __ flush();
 766     int stack_slots = SharedRuntime::out_preserve_stack_slots();  // no out slots at all, actually
 767     return nmethod::new_native_nmethod(method,
 768                                        compile_id,
 769                                        masm-&gt;code(),
 770                                        vep_offset,
 771                                        frame_complete,
 772                                        stack_slots / VMRegImpl::slots_per_word,
 773                                        in_ByteSize(-1),
 774                                        in_ByteSize(-1),
 775                                        (OopMapSet*)NULL);
</pre>
<hr />
<pre>
 843   int vep_offset = __ pc() - start;
 844 
 845 
 846   if ((InlineObjectHash &amp;&amp; method-&gt;intrinsic_id() == vmIntrinsics::_hashCode) || (method-&gt;intrinsic_id() == vmIntrinsics::_identityHashCode)) {
 847     // Object.hashCode, System.identityHashCode can pull the hashCode from the header word
 848     // instead of doing a full VM transition once it&#39;s been computed.
 849     Label slow_case;
 850     const Register obj_reg = R0;
 851 
 852     // Unlike for Object.hashCode, System.identityHashCode is static method and
 853     // gets object as argument instead of the receiver.
 854     if (method-&gt;intrinsic_id() == vmIntrinsics::_identityHashCode) {
 855       assert(method-&gt;is_static(), &quot;method should be static&quot;);
 856       // return 0 for null reference input, return val = R0 = obj_reg = 0
 857       __ cmp(obj_reg, 0);
 858       __ bx(LR, eq);
 859     }
 860 
 861     __ ldr(Rtemp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
 862 
<span class="line-modified"> 863     assert(markOopDesc::unlocked_value == 1, &quot;adjust this code&quot;);</span>
<span class="line-modified"> 864     __ tbz(Rtemp, exact_log2(markOopDesc::unlocked_value), slow_case);</span>
 865 
 866     if (UseBiasedLocking) {
<span class="line-modified"> 867       assert(is_power_of_2(markOopDesc::biased_lock_bit_in_place), &quot;adjust this code&quot;);</span>
<span class="line-modified"> 868       __ tbnz(Rtemp, exact_log2(markOopDesc::biased_lock_bit_in_place), slow_case);</span>
 869     }
 870 
<span class="line-modified"> 871     __ bics(Rtemp, Rtemp, ~markOopDesc::hash_mask_in_place);</span>
<span class="line-modified"> 872     __ mov(R0, AsmOperand(Rtemp, lsr, markOopDesc::hash_shift), ne);</span>
 873     __ bx(LR, ne);
 874 
 875     __ bind(slow_case);
 876   }
 877 
 878   // Bang stack pages
 879   __ arm_stack_overflow_check(stack_size, Rtemp);
 880 
 881   // Setup frame linkage
 882   __ raw_push(FP, LR);
 883   __ mov(FP, SP);
 884   __ sub_slow(SP, SP, stack_size - 2*wordSize);
 885 
 886   int frame_complete = __ pc() - start;
 887 
 888   OopMapSet* oop_maps = new OopMapSet();
 889   OopMap* map = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
 890   const int extra_args = is_static ? 2 : 1;
 891   int receiver_offset = -1;
 892   int fp_regs_in_arguments = 0;
</pre>
<hr />
<pre>
1154 
1155   Label slow_lock, slow_lock_biased, lock_done, fast_lock;
1156   if (method-&gt;is_synchronized()) {
1157     // The first argument is a handle to sync object (a class or an instance)
1158     __ ldr(sync_obj, Address(R1));
1159     // Remember the handle for the unlocking code
1160     __ mov(sync_handle, R1);
1161 
1162     __ resolve(IS_NOT_NULL, sync_obj);
1163 
1164     if(UseBiasedLocking) {
1165       __ biased_locking_enter(sync_obj, tmp, disp_hdr/*scratched*/, false, Rtemp, lock_done, slow_lock_biased);
1166     }
1167 
1168     const Register mark = tmp;
1169     // On MP platforms the next load could return a &#39;stale&#39; value if the memory location has been modified by another thread.
1170     // That would be acceptable as either CAS or slow case path is taken in that case
1171 
1172     __ ldr(mark, Address(sync_obj, oopDesc::mark_offset_in_bytes()));
1173     __ sub(disp_hdr, FP, lock_slot_fp_offset);
<span class="line-modified">1174     __ tst(mark, markOopDesc::unlocked_value);</span>
1175     __ b(fast_lock, ne);
1176 
1177     // Check for recursive lock
1178     // See comments in InterpreterMacroAssembler::lock_object for
1179     // explanations on the fast recursive locking check.
1180     // Check independently the low bits and the distance to SP
1181     // -1- test low 2 bits
1182     __ movs(Rtemp, AsmOperand(mark, lsl, 30));
1183     // -2- test (hdr - SP) if the low two bits are 0
1184     __ sub(Rtemp, mark, SP, eq);
1185     __ movs(Rtemp, AsmOperand(Rtemp, lsr, exact_log2(os::vm_page_size())), eq);
1186     // If still &#39;eq&#39; then recursive locking OK: set displaced header to 0
1187     __ str(Rtemp, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()), eq);
1188     __ b(lock_done, eq);
1189     __ b(slow_lock);
1190 
1191     __ bind(fast_lock);
1192     __ str(mark, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));
1193 
1194     __ cas_for_lock_acquire(mark, disp_hdr, sync_obj, Rtemp, slow_lock);
</pre>
<hr />
<pre>
1200   __ add(c_rarg0, Rthread, in_bytes(JavaThread::jni_environment_offset()));
1201 
1202   // Perform thread state transition
1203   __ mov(Rtemp, _thread_in_native);
1204   __ str(Rtemp, Address(Rthread, JavaThread::thread_state_offset()));
1205 
1206   // Finally, call the native method
1207   __ call(method-&gt;native_function());
1208 
1209   // Set FPSCR/FPCR to a known state
1210   if (AlwaysRestoreFPU) {
1211     __ restore_default_fp_mode();
1212   }
1213 
1214   // Ensure a Boolean result is mapped to 0..1
1215   if (ret_type == T_BOOLEAN) {
1216     __ c2bool(R0);
1217   }
1218 
1219   // Do a safepoint check while thread is in transition state
<span class="line-removed">1220   InlinedAddress safepoint_state(SafepointSynchronize::address_of_state());</span>
1221   Label call_safepoint_runtime, return_to_java;
1222   __ mov(Rtemp, _thread_in_native_trans);
<span class="line-removed">1223   __ ldr_literal(R2, safepoint_state);</span>
1224   __ str_32(Rtemp, Address(Rthread, JavaThread::thread_state_offset()));
1225 
1226   // make sure the store is observed before reading the SafepointSynchronize state and further mem refs
1227   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreLoad | MacroAssembler::StoreStore), Rtemp);
1228 
<span class="line-modified">1229   __ ldr_s32(R2, Address(R2));</span>
1230   __ ldr_u32(R3, Address(Rthread, JavaThread::suspend_flags_offset()));
<span class="line-modified">1231   __ cmp(R2, SafepointSynchronize::_not_synchronized);</span>
<span class="line-removed">1232   __ cond_cmp(R3, 0, eq);</span>
1233   __ b(call_safepoint_runtime, ne);

1234   __ bind(return_to_java);
1235 
1236   // Perform thread state transition and reguard stack yellow pages if needed
1237   Label reguard, reguard_done;
1238   __ mov(Rtemp, _thread_in_Java);
1239   __ ldr_s32(R2, Address(Rthread, JavaThread::stack_guard_state_offset()));
1240   __ str_32(Rtemp, Address(Rthread, JavaThread::thread_state_offset()));
1241 
1242   __ cmp(R2, JavaThread::stack_guard_yellow_reserved_disabled);
1243   __ b(reguard, eq);
1244   __ bind(reguard_done);
1245 
1246   Label slow_unlock, unlock_done;
1247   if (method-&gt;is_synchronized()) {
1248     __ ldr(sync_obj, Address(sync_handle));
1249 
1250     __ resolve(IS_NOT_NULL, sync_obj);
1251 
1252     if(UseBiasedLocking) {
1253       __ biased_locking_exit(sync_obj, Rtemp, unlock_done);
</pre>
<hr />
<pre>
1284   __ ldr(Rtemp, Address(Rthread, Thread::pending_exception_offset()));
1285   __ mov(SP, FP);
1286 
1287   __ cmp(Rtemp, 0);
1288   // Pop the frame and return if no exception pending
1289   __ pop(RegisterSet(FP) | RegisterSet(PC), eq);
1290   // Pop the frame and forward the exception. Rexception_pc contains return address.
1291   __ ldr(FP, Address(SP, wordSize, post_indexed), ne);
1292   __ ldr(Rexception_pc, Address(SP, wordSize, post_indexed), ne);
1293   __ jump(StubRoutines::forward_exception_entry(), relocInfo::runtime_call_type, Rtemp);
1294 
1295   // Safepoint operation and/or pending suspend request is in progress.
1296   // Save the return values and call the runtime function by hand.
1297   __ bind(call_safepoint_runtime);
1298   push_result_registers(masm, ret_type);
1299   __ mov(R0, Rthread);
1300   __ call(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans));
1301   pop_result_registers(masm, ret_type);
1302   __ b(return_to_java);
1303 
<span class="line-removed">1304   __ bind_literal(safepoint_state);</span>
<span class="line-removed">1305 </span>
1306   // Reguard stack pages. Save native results around a call to C runtime.
1307   __ bind(reguard);
1308   push_result_registers(masm, ret_type);
1309   __ call(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));
1310   pop_result_registers(masm, ret_type);
1311   __ b(reguard_done);
1312 
1313   if (method-&gt;is_synchronized()) {
1314     // Locking slow case
1315     if(UseBiasedLocking) {
1316       __ bind(slow_lock_biased);
1317       __ sub(disp_hdr, FP, lock_slot_fp_offset);
1318     }
1319 
1320     __ bind(slow_lock);
1321 
1322     push_param_registers(masm, fp_regs_in_arguments);
1323 
1324     // last_Java_frame is already set, so do call_VM manually; no exception can occur
1325     __ mov(R0, sync_obj);
</pre>
<hr />
<pre>
1787   }
1788 
1789   OopMap* map = RegisterSaver::save_live_registers(masm, &amp;frame_size_words, !cause_return);
1790   if (!cause_return) {
1791     // update saved PC with correct value
1792     // need 2 steps because LR can be live in c2 method
1793     __ ldr(LR, Address(Rthread, JavaThread::saved_exception_pc_offset()));
1794     __ str(LR, Address(SP, RegisterSaver::LR_offset * wordSize));
1795   }
1796 
1797   __ mov(R0, Rthread);
1798   int pc_offset = __ set_last_Java_frame(SP, FP, false, Rtemp); // note: FP may not need to be saved (not on x86)
1799   assert(((__ pc()) - start) == __ offset(), &quot;warning: start differs from code_begin&quot;);
1800   __ call(call_ptr);
1801   if (pc_offset == -1) {
1802     pc_offset = __ offset();
1803   }
1804   oop_maps-&gt;add_gc_map(pc_offset, map);
1805   __ reset_last_Java_frame(Rtemp); // Rtemp free since scratched by far call
1806 
<span class="line-removed">1807   // Check for pending exception</span>
<span class="line-removed">1808   __ ldr(Rtemp, Address(Rthread, Thread::pending_exception_offset()));</span>
<span class="line-removed">1809   __ cmp(Rtemp, 0);</span>
<span class="line-removed">1810 </span>
1811   if (!cause_return) {














1812     RegisterSaver::restore_live_registers(masm, false);
1813     __ pop(PC, eq);
1814     __ pop(Rexception_pc);
1815   } else {




1816     RegisterSaver::restore_live_registers(masm);
1817     __ bx(LR, eq);
1818     __ mov(Rexception_pc, LR);
1819   }
1820 
1821   __ jump(StubRoutines::forward_exception_entry(), relocInfo::runtime_call_type, Rtemp);
1822 
1823   __ flush();
1824 
1825   return SafepointBlob::create(&amp;buffer, oop_maps, frame_size_words);
1826 }
1827 
1828 RuntimeStub* SharedRuntime::generate_resolve_blob(address destination, const char* name) {
1829   assert(StubRoutines::forward_exception_entry() != NULL, &quot;must be generated before&quot;);
1830 
1831   ResourceMark rm;
1832   CodeBuffer buffer(name, 1000, 512);
1833   int frame_size_words;
1834   OopMapSet *oop_maps;
1835   int frame_complete;
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2008, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/assembler.hpp&quot;
  27 #include &quot;assembler_arm.inline.hpp&quot;
  28 #include &quot;code/debugInfoRec.hpp&quot;
  29 #include &quot;code/icBuffer.hpp&quot;
  30 #include &quot;code/vtableStubs.hpp&quot;
  31 #include &quot;interpreter/interpreter.hpp&quot;
  32 #include &quot;logging/log.hpp&quot;
  33 #include &quot;memory/resourceArea.hpp&quot;
  34 #include &quot;oops/compiledICHolder.hpp&quot;
<span class="line-added">  35 #include &quot;oops/klass.inline.hpp&quot;</span>
  36 #include &quot;runtime/sharedRuntime.hpp&quot;
<span class="line-added">  37 #include &quot;runtime/safepointMechanism.hpp&quot;</span>
  38 #include &quot;runtime/vframeArray.hpp&quot;
  39 #include &quot;utilities/align.hpp&quot;
<span class="line-added">  40 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  41 #include &quot;vmreg_arm.inline.hpp&quot;
  42 #ifdef COMPILER1
  43 #include &quot;c1/c1_Runtime1.hpp&quot;
  44 #endif
  45 #ifdef COMPILER2
  46 #include &quot;opto/runtime.hpp&quot;
  47 #endif
  48 
  49 #define __ masm-&gt;
  50 
  51 class RegisterSaver {
  52 public:
  53 
  54   // Special registers:
  55   //              32-bit ARM     64-bit ARM
  56   //  Rthread:       R10            R28
  57   //  LR:            R14            R30
  58 
  59   // Rthread is callee saved in the C ABI and never changed by compiled code:
  60   // no need to save it.
</pre>
<hr />
<pre>
 117 
 118 
 119 
 120 
 121 OopMap* RegisterSaver::save_live_registers(MacroAssembler* masm,
 122                                            int* total_frame_words,
 123                                            bool lr_saved) {
 124   *total_frame_words = reg_save_size;
 125 
 126   OopMapSet *oop_maps = new OopMapSet();
 127   OopMap* map = new OopMap(VMRegImpl::slots_per_word * (*total_frame_words), 0);
 128 
 129   if (lr_saved) {
 130     __ push(RegisterSet(FP));
 131   } else {
 132     __ push(RegisterSet(FP) | RegisterSet(LR));
 133   }
 134   __ push(SAVED_BASE_REGS);
 135   if (HaveVFP) {
 136     if (VM_Version::has_vfp3_32()) {
<span class="line-modified"> 137       __ fpush(FloatRegisterSet(D16, 16));</span>
 138     } else {
 139       if (FloatRegisterImpl::number_of_registers &gt; 32) {
 140         assert(FloatRegisterImpl::number_of_registers == 64, &quot;nb fp registers should be 64&quot;);
 141         __ sub(SP, SP, 32 * wordSize);
 142       }
 143     }
<span class="line-modified"> 144     __ fpush(FloatRegisterSet(D0, 16));</span>
 145   } else {
 146     __ sub(SP, SP, fpu_save_size * wordSize);
 147   }
 148 
 149   int i;
 150   int j=0;
 151   for (i = R0_offset; i &lt;= R9_offset; i++) {
 152     if (j == FP_REG_NUM) {
 153       // skip the FP register, managed below.
 154       j++;
 155     }
 156     map-&gt;set_callee_saved(VMRegImpl::stack2reg(i), as_Register(j)-&gt;as_VMReg());
 157     j++;
 158   }
 159   assert(j == R10-&gt;encoding(), &quot;must be&quot;);
 160 #if (FP_REG_NUM != 11)
 161   // add R11, if not managed as FP
 162   map-&gt;set_callee_saved(VMRegImpl::stack2reg(R11_offset), R11-&gt;as_VMReg());
 163 #endif
 164   map-&gt;set_callee_saved(VMRegImpl::stack2reg(R12_offset), R12-&gt;as_VMReg());
 165   map-&gt;set_callee_saved(VMRegImpl::stack2reg(R14_offset), R14-&gt;as_VMReg());
 166   if (HaveVFP) {
 167     for (i = 0; i &lt; (VM_Version::has_vfp3_32() ? 64 : 32); i+=2) {
 168       map-&gt;set_callee_saved(VMRegImpl::stack2reg(i), as_FloatRegister(i)-&gt;as_VMReg());
 169       map-&gt;set_callee_saved(VMRegImpl::stack2reg(i + 1), as_FloatRegister(i)-&gt;as_VMReg()-&gt;next());
 170     }
 171   }
 172 
 173   return map;
 174 }
 175 
 176 void RegisterSaver::restore_live_registers(MacroAssembler* masm, bool restore_lr) {
 177   if (HaveVFP) {
<span class="line-modified"> 178     __ fpop(FloatRegisterSet(D0, 16));</span>
 179     if (VM_Version::has_vfp3_32()) {
<span class="line-modified"> 180       __ fpop(FloatRegisterSet(D16, 16));</span>
 181     } else {
 182       if (FloatRegisterImpl::number_of_registers &gt; 32) {
 183         assert(FloatRegisterImpl::number_of_registers == 64, &quot;nb fp registers should be 64&quot;);
 184         __ add(SP, SP, 32 * wordSize);
 185       }
 186     }
 187   } else {
 188     __ add(SP, SP, fpu_save_size * wordSize);
 189   }
 190   __ pop(SAVED_BASE_REGS);
 191   if (restore_lr) {
 192     __ pop(RegisterSet(FP) | RegisterSet(LR));
 193   } else {
 194     __ pop(RegisterSet(FP));
 195   }
 196 }
 197 
 198 
 199 static void push_result_registers(MacroAssembler* masm, BasicType ret_type) {
 200 #ifdef __ABI_HARD__
</pre>
<hr />
<pre>
 205   }
 206 #endif // __ABI_HARD__
 207   __ raw_push(R0, R1);
 208 }
 209 
 210 static void pop_result_registers(MacroAssembler* masm, BasicType ret_type) {
 211 #ifdef __ABI_HARD__
 212   if (ret_type == T_DOUBLE || ret_type == T_FLOAT) {
 213     __ fldd(D0, Address(SP));
 214     __ add(SP, SP, 8);
 215     return;
 216   }
 217 #endif // __ABI_HARD__
 218   __ raw_pop(R0, R1);
 219 }
 220 
 221 static void push_param_registers(MacroAssembler* masm, int fp_regs_in_arguments) {
 222   // R1-R3 arguments need to be saved, but we push 4 registers for 8-byte alignment
 223   __ push(RegisterSet(R0, R3));
 224 

 225   // preserve arguments
 226   // Likely not needed as the locking code won&#39;t probably modify volatile FP registers,
 227   // but there is no way to guarantee that
 228   if (fp_regs_in_arguments) {
 229     // convert fp_regs_in_arguments to a number of double registers
 230     int double_regs_num = (fp_regs_in_arguments + 1) &gt;&gt; 1;
<span class="line-modified"> 231     __ fpush_hardfp(FloatRegisterSet(D0, double_regs_num));</span>
 232   }

 233 }
 234 
 235 static void pop_param_registers(MacroAssembler* masm, int fp_regs_in_arguments) {

 236   if (fp_regs_in_arguments) {
 237     int double_regs_num = (fp_regs_in_arguments + 1) &gt;&gt; 1;
<span class="line-modified"> 238     __ fpop_hardfp(FloatRegisterSet(D0, double_regs_num));</span>
 239   }


 240   __ pop(RegisterSet(R0, R3));
 241 }
 242 
 243 
 244 
 245 // Is vector&#39;s size (in bytes) bigger than a size saved by default?
 246 // All vector registers are saved by default on ARM.
 247 bool SharedRuntime::is_wide_vector(int size) {
 248   return false;
 249 }
 250 
 251 size_t SharedRuntime::trampoline_size() {
 252   return 16;
 253 }
 254 
 255 void SharedRuntime::generate_trampoline(MacroAssembler *masm, address destination) {
 256   InlinedAddress dest(destination);
 257   __ indirect_jump(dest, Rtemp);
 258   __ bind_literal(dest);
 259 }
</pre>
<hr />
<pre>
 441       regs[i].set_bad();
 442       break;
 443     default:
 444       ShouldNotReachHere();
 445     }
 446   }
 447 
 448   if (slot &amp; 1) slot++;
 449   return slot;
 450 }
 451 
 452 static void patch_callers_callsite(MacroAssembler *masm) {
 453   Label skip;
 454 
 455   __ ldr(Rtemp, Address(Rmethod, Method::code_offset()));
 456   __ cbz(Rtemp, skip);
 457 
 458   // Pushing an even number of registers for stack alignment.
 459   // Selecting R9, which had to be saved anyway for some platforms.
 460   __ push(RegisterSet(R0, R3) | R9 | LR);
<span class="line-added"> 461   __ fpush_hardfp(FloatRegisterSet(D0, 8));</span>
 462 
 463   __ mov(R0, Rmethod);
 464   __ mov(R1, LR);
 465   __ call(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite));
 466 
<span class="line-added"> 467   __ fpop_hardfp(FloatRegisterSet(D0, 8));</span>
 468   __ pop(RegisterSet(R0, R3) | R9 | LR);
 469 
 470   __ bind(skip);
 471 }
 472 
 473 void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,
 474                                     int total_args_passed, int comp_args_on_stack,
 475                                     const BasicType *sig_bt, const VMRegPair *regs) {
 476   // TODO: ARM - May be can use ldm to load arguments
 477   const Register tmp = Rtemp; // avoid erasing R5_mh
 478 
 479   // Next assert may not be needed but safer. Extra analysis required
 480   // if this there is not enough free registers and we need to use R5 here.
 481   assert_different_registers(tmp, R5_mh);
 482 
 483   // 6243940 We might end up in handle_wrong_method if
 484   // the callee is deoptimized as we race thru here. If that
 485   // happens we don&#39;t want to take a safepoint because the
 486   // caller frame will look interpreted and arguments are now
 487   // &quot;compiled&quot; so it is much better to make this transition
</pre>
<hr />
<pre>
 735       receiver_reg = r-&gt;as_Register();
 736     }
 737   }
 738 
 739   // Figure out which address we are really jumping to:
 740   MethodHandles::generate_method_handle_dispatch(masm, iid,
 741                                                  receiver_reg, member_reg, /*for_compiler_entry:*/ true);
 742 }
 743 
 744 // ---------------------------------------------------------------------------
 745 // Generate a native wrapper for a given method.  The method takes arguments
 746 // in the Java compiled code convention, marshals them to the native
 747 // convention (handlizes oops, etc), transitions to native, makes the call,
 748 // returns to java state (possibly blocking), unhandlizes any result and
 749 // returns.
 750 nmethod* SharedRuntime::generate_native_wrapper(MacroAssembler* masm,
 751                                                 const methodHandle&amp; method,
 752                                                 int compile_id,
 753                                                 BasicType* in_sig_bt,
 754                                                 VMRegPair* in_regs,
<span class="line-modified"> 755                                                 BasicType ret_type,</span>
<span class="line-added"> 756                                                 address critical_entry) {</span>
 757   if (method-&gt;is_method_handle_intrinsic()) {
 758     vmIntrinsics::ID iid = method-&gt;intrinsic_id();
 759     intptr_t start = (intptr_t)__ pc();
 760     int vep_offset = ((intptr_t)__ pc()) - start;
 761     gen_special_dispatch(masm,
 762                          method,
 763                          in_sig_bt,
 764                          in_regs);
 765     int frame_complete = ((intptr_t)__ pc()) - start;  // not complete, period
 766     __ flush();
 767     int stack_slots = SharedRuntime::out_preserve_stack_slots();  // no out slots at all, actually
 768     return nmethod::new_native_nmethod(method,
 769                                        compile_id,
 770                                        masm-&gt;code(),
 771                                        vep_offset,
 772                                        frame_complete,
 773                                        stack_slots / VMRegImpl::slots_per_word,
 774                                        in_ByteSize(-1),
 775                                        in_ByteSize(-1),
 776                                        (OopMapSet*)NULL);
</pre>
<hr />
<pre>
 844   int vep_offset = __ pc() - start;
 845 
 846 
 847   if ((InlineObjectHash &amp;&amp; method-&gt;intrinsic_id() == vmIntrinsics::_hashCode) || (method-&gt;intrinsic_id() == vmIntrinsics::_identityHashCode)) {
 848     // Object.hashCode, System.identityHashCode can pull the hashCode from the header word
 849     // instead of doing a full VM transition once it&#39;s been computed.
 850     Label slow_case;
 851     const Register obj_reg = R0;
 852 
 853     // Unlike for Object.hashCode, System.identityHashCode is static method and
 854     // gets object as argument instead of the receiver.
 855     if (method-&gt;intrinsic_id() == vmIntrinsics::_identityHashCode) {
 856       assert(method-&gt;is_static(), &quot;method should be static&quot;);
 857       // return 0 for null reference input, return val = R0 = obj_reg = 0
 858       __ cmp(obj_reg, 0);
 859       __ bx(LR, eq);
 860     }
 861 
 862     __ ldr(Rtemp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
 863 
<span class="line-modified"> 864     assert(markWord::unlocked_value == 1, &quot;adjust this code&quot;);</span>
<span class="line-modified"> 865     __ tbz(Rtemp, exact_log2(markWord::unlocked_value), slow_case);</span>
 866 
 867     if (UseBiasedLocking) {
<span class="line-modified"> 868       assert(is_power_of_2(markWord::biased_lock_bit_in_place), &quot;adjust this code&quot;);</span>
<span class="line-modified"> 869       __ tbnz(Rtemp, exact_log2(markWord::biased_lock_bit_in_place), slow_case);</span>
 870     }
 871 
<span class="line-modified"> 872     __ bics(Rtemp, Rtemp, ~markWord::hash_mask_in_place);</span>
<span class="line-modified"> 873     __ mov(R0, AsmOperand(Rtemp, lsr, markWord::hash_shift), ne);</span>
 874     __ bx(LR, ne);
 875 
 876     __ bind(slow_case);
 877   }
 878 
 879   // Bang stack pages
 880   __ arm_stack_overflow_check(stack_size, Rtemp);
 881 
 882   // Setup frame linkage
 883   __ raw_push(FP, LR);
 884   __ mov(FP, SP);
 885   __ sub_slow(SP, SP, stack_size - 2*wordSize);
 886 
 887   int frame_complete = __ pc() - start;
 888 
 889   OopMapSet* oop_maps = new OopMapSet();
 890   OopMap* map = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
 891   const int extra_args = is_static ? 2 : 1;
 892   int receiver_offset = -1;
 893   int fp_regs_in_arguments = 0;
</pre>
<hr />
<pre>
1155 
1156   Label slow_lock, slow_lock_biased, lock_done, fast_lock;
1157   if (method-&gt;is_synchronized()) {
1158     // The first argument is a handle to sync object (a class or an instance)
1159     __ ldr(sync_obj, Address(R1));
1160     // Remember the handle for the unlocking code
1161     __ mov(sync_handle, R1);
1162 
1163     __ resolve(IS_NOT_NULL, sync_obj);
1164 
1165     if(UseBiasedLocking) {
1166       __ biased_locking_enter(sync_obj, tmp, disp_hdr/*scratched*/, false, Rtemp, lock_done, slow_lock_biased);
1167     }
1168 
1169     const Register mark = tmp;
1170     // On MP platforms the next load could return a &#39;stale&#39; value if the memory location has been modified by another thread.
1171     // That would be acceptable as either CAS or slow case path is taken in that case
1172 
1173     __ ldr(mark, Address(sync_obj, oopDesc::mark_offset_in_bytes()));
1174     __ sub(disp_hdr, FP, lock_slot_fp_offset);
<span class="line-modified">1175     __ tst(mark, markWord::unlocked_value);</span>
1176     __ b(fast_lock, ne);
1177 
1178     // Check for recursive lock
1179     // See comments in InterpreterMacroAssembler::lock_object for
1180     // explanations on the fast recursive locking check.
1181     // Check independently the low bits and the distance to SP
1182     // -1- test low 2 bits
1183     __ movs(Rtemp, AsmOperand(mark, lsl, 30));
1184     // -2- test (hdr - SP) if the low two bits are 0
1185     __ sub(Rtemp, mark, SP, eq);
1186     __ movs(Rtemp, AsmOperand(Rtemp, lsr, exact_log2(os::vm_page_size())), eq);
1187     // If still &#39;eq&#39; then recursive locking OK: set displaced header to 0
1188     __ str(Rtemp, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()), eq);
1189     __ b(lock_done, eq);
1190     __ b(slow_lock);
1191 
1192     __ bind(fast_lock);
1193     __ str(mark, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));
1194 
1195     __ cas_for_lock_acquire(mark, disp_hdr, sync_obj, Rtemp, slow_lock);
</pre>
<hr />
<pre>
1201   __ add(c_rarg0, Rthread, in_bytes(JavaThread::jni_environment_offset()));
1202 
1203   // Perform thread state transition
1204   __ mov(Rtemp, _thread_in_native);
1205   __ str(Rtemp, Address(Rthread, JavaThread::thread_state_offset()));
1206 
1207   // Finally, call the native method
1208   __ call(method-&gt;native_function());
1209 
1210   // Set FPSCR/FPCR to a known state
1211   if (AlwaysRestoreFPU) {
1212     __ restore_default_fp_mode();
1213   }
1214 
1215   // Ensure a Boolean result is mapped to 0..1
1216   if (ret_type == T_BOOLEAN) {
1217     __ c2bool(R0);
1218   }
1219 
1220   // Do a safepoint check while thread is in transition state

1221   Label call_safepoint_runtime, return_to_java;
1222   __ mov(Rtemp, _thread_in_native_trans);

1223   __ str_32(Rtemp, Address(Rthread, JavaThread::thread_state_offset()));
1224 
1225   // make sure the store is observed before reading the SafepointSynchronize state and further mem refs
1226   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreLoad | MacroAssembler::StoreStore), Rtemp);
1227 
<span class="line-modified">1228   __ safepoint_poll(R2, call_safepoint_runtime);</span>
1229   __ ldr_u32(R3, Address(Rthread, JavaThread::suspend_flags_offset()));
<span class="line-modified">1230   __ cmp(R3, 0);</span>

1231   __ b(call_safepoint_runtime, ne);
<span class="line-added">1232 </span>
1233   __ bind(return_to_java);
1234 
1235   // Perform thread state transition and reguard stack yellow pages if needed
1236   Label reguard, reguard_done;
1237   __ mov(Rtemp, _thread_in_Java);
1238   __ ldr_s32(R2, Address(Rthread, JavaThread::stack_guard_state_offset()));
1239   __ str_32(Rtemp, Address(Rthread, JavaThread::thread_state_offset()));
1240 
1241   __ cmp(R2, JavaThread::stack_guard_yellow_reserved_disabled);
1242   __ b(reguard, eq);
1243   __ bind(reguard_done);
1244 
1245   Label slow_unlock, unlock_done;
1246   if (method-&gt;is_synchronized()) {
1247     __ ldr(sync_obj, Address(sync_handle));
1248 
1249     __ resolve(IS_NOT_NULL, sync_obj);
1250 
1251     if(UseBiasedLocking) {
1252       __ biased_locking_exit(sync_obj, Rtemp, unlock_done);
</pre>
<hr />
<pre>
1283   __ ldr(Rtemp, Address(Rthread, Thread::pending_exception_offset()));
1284   __ mov(SP, FP);
1285 
1286   __ cmp(Rtemp, 0);
1287   // Pop the frame and return if no exception pending
1288   __ pop(RegisterSet(FP) | RegisterSet(PC), eq);
1289   // Pop the frame and forward the exception. Rexception_pc contains return address.
1290   __ ldr(FP, Address(SP, wordSize, post_indexed), ne);
1291   __ ldr(Rexception_pc, Address(SP, wordSize, post_indexed), ne);
1292   __ jump(StubRoutines::forward_exception_entry(), relocInfo::runtime_call_type, Rtemp);
1293 
1294   // Safepoint operation and/or pending suspend request is in progress.
1295   // Save the return values and call the runtime function by hand.
1296   __ bind(call_safepoint_runtime);
1297   push_result_registers(masm, ret_type);
1298   __ mov(R0, Rthread);
1299   __ call(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans));
1300   pop_result_registers(masm, ret_type);
1301   __ b(return_to_java);
1302 


1303   // Reguard stack pages. Save native results around a call to C runtime.
1304   __ bind(reguard);
1305   push_result_registers(masm, ret_type);
1306   __ call(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));
1307   pop_result_registers(masm, ret_type);
1308   __ b(reguard_done);
1309 
1310   if (method-&gt;is_synchronized()) {
1311     // Locking slow case
1312     if(UseBiasedLocking) {
1313       __ bind(slow_lock_biased);
1314       __ sub(disp_hdr, FP, lock_slot_fp_offset);
1315     }
1316 
1317     __ bind(slow_lock);
1318 
1319     push_param_registers(masm, fp_regs_in_arguments);
1320 
1321     // last_Java_frame is already set, so do call_VM manually; no exception can occur
1322     __ mov(R0, sync_obj);
</pre>
<hr />
<pre>
1784   }
1785 
1786   OopMap* map = RegisterSaver::save_live_registers(masm, &amp;frame_size_words, !cause_return);
1787   if (!cause_return) {
1788     // update saved PC with correct value
1789     // need 2 steps because LR can be live in c2 method
1790     __ ldr(LR, Address(Rthread, JavaThread::saved_exception_pc_offset()));
1791     __ str(LR, Address(SP, RegisterSaver::LR_offset * wordSize));
1792   }
1793 
1794   __ mov(R0, Rthread);
1795   int pc_offset = __ set_last_Java_frame(SP, FP, false, Rtemp); // note: FP may not need to be saved (not on x86)
1796   assert(((__ pc()) - start) == __ offset(), &quot;warning: start differs from code_begin&quot;);
1797   __ call(call_ptr);
1798   if (pc_offset == -1) {
1799     pc_offset = __ offset();
1800   }
1801   oop_maps-&gt;add_gc_map(pc_offset, map);
1802   __ reset_last_Java_frame(Rtemp); // Rtemp free since scratched by far call
1803 




1804   if (!cause_return) {
<span class="line-added">1805     if (SafepointMechanism::uses_thread_local_poll()) {</span>
<span class="line-added">1806       // If our stashed return pc was modified by the runtime we avoid touching it</span>
<span class="line-added">1807       __ ldr(R3_tmp, Address(Rthread, JavaThread::saved_exception_pc_offset()));</span>
<span class="line-added">1808       __ ldr(R2_tmp, Address(SP, RegisterSaver::LR_offset * wordSize));</span>
<span class="line-added">1809       __ cmp(R2_tmp, R3_tmp);</span>
<span class="line-added">1810       // Adjust return pc forward to step over the safepoint poll instruction</span>
<span class="line-added">1811       __ add(R2_tmp, R2_tmp, 4, eq);</span>
<span class="line-added">1812       __ str(R2_tmp, Address(SP, RegisterSaver::LR_offset * wordSize), eq);</span>
<span class="line-added">1813     }</span>
<span class="line-added">1814 </span>
<span class="line-added">1815     // Check for pending exception</span>
<span class="line-added">1816     __ ldr(Rtemp, Address(Rthread, Thread::pending_exception_offset()));</span>
<span class="line-added">1817     __ cmp(Rtemp, 0);</span>
<span class="line-added">1818 </span>
1819     RegisterSaver::restore_live_registers(masm, false);
1820     __ pop(PC, eq);
1821     __ pop(Rexception_pc);
1822   } else {
<span class="line-added">1823     // Check for pending exception</span>
<span class="line-added">1824     __ ldr(Rtemp, Address(Rthread, Thread::pending_exception_offset()));</span>
<span class="line-added">1825     __ cmp(Rtemp, 0);</span>
<span class="line-added">1826 </span>
1827     RegisterSaver::restore_live_registers(masm);
1828     __ bx(LR, eq);
1829     __ mov(Rexception_pc, LR);
1830   }
1831 
1832   __ jump(StubRoutines::forward_exception_entry(), relocInfo::runtime_call_type, Rtemp);
1833 
1834   __ flush();
1835 
1836   return SafepointBlob::create(&amp;buffer, oop_maps, frame_size_words);
1837 }
1838 
1839 RuntimeStub* SharedRuntime::generate_resolve_blob(address destination, const char* name) {
1840   assert(StubRoutines::forward_exception_entry() != NULL, &quot;must be generated before&quot;);
1841 
1842   ResourceMark rm;
1843   CodeBuffer buffer(name, 1000, 512);
1844   int frame_size_words;
1845   OopMapSet *oop_maps;
1846   int frame_complete;
</pre>
</td>
</tr>
</table>
<center><a href="relocInfo_arm.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stubGenerator_arm.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>