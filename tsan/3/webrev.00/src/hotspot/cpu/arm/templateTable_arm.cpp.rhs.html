<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/arm/templateTable_arm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
<a name="1" id="anc1"></a><span class="line-modified">   2  * Copyright (c) 2008, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.inline.hpp&quot;
  27 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  28 #include &quot;interpreter/interp_masm.hpp&quot;
  29 #include &quot;interpreter/interpreter.hpp&quot;
  30 #include &quot;interpreter/interpreterRuntime.hpp&quot;
  31 #include &quot;interpreter/templateTable.hpp&quot;
  32 #include &quot;memory/universe.hpp&quot;
  33 #include &quot;oops/cpCache.hpp&quot;
  34 #include &quot;oops/methodData.hpp&quot;
  35 #include &quot;oops/objArrayKlass.hpp&quot;
  36 #include &quot;oops/oop.inline.hpp&quot;
  37 #include &quot;prims/methodHandles.hpp&quot;
  38 #include &quot;runtime/frame.inline.hpp&quot;
  39 #include &quot;runtime/sharedRuntime.hpp&quot;
  40 #include &quot;runtime/stubRoutines.hpp&quot;
  41 #include &quot;runtime/synchronizer.hpp&quot;
<a name="2" id="anc2"></a><span class="line-added">  42 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  43 
  44 #define __ _masm-&gt;
  45 
  46 //----------------------------------------------------------------------------------------------------
  47 // Platform-dependent initialization
  48 
  49 void TemplateTable::pd_initialize() {
  50   // No arm specific initialization
  51 }
  52 
  53 //----------------------------------------------------------------------------------------------------
  54 // Address computation
  55 
  56 // local variables
  57 static inline Address iaddress(int n)            {
  58   return Address(Rlocals, Interpreter::local_offset_in_bytes(n));
  59 }
  60 
  61 static inline Address laddress(int n)            { return iaddress(n + 1); }
  62 static inline Address haddress(int n)            { return iaddress(n + 0); }
  63 
  64 static inline Address faddress(int n)            { return iaddress(n); }
  65 static inline Address daddress(int n)            { return laddress(n); }
  66 static inline Address aaddress(int n)            { return iaddress(n); }
  67 
  68 
  69 void TemplateTable::get_local_base_addr(Register r, Register index) {
  70   __ sub(r, Rlocals, AsmOperand(index, lsl, Interpreter::logStackElementSize));
  71 }
  72 
  73 Address TemplateTable::load_iaddress(Register index, Register scratch) {
  74   return Address(Rlocals, index, lsl, Interpreter::logStackElementSize, basic_offset, sub_offset);
  75 }
  76 
  77 Address TemplateTable::load_aaddress(Register index, Register scratch) {
  78   return load_iaddress(index, scratch);
  79 }
  80 
  81 Address TemplateTable::load_faddress(Register index, Register scratch) {
  82 #ifdef __SOFTFP__
  83   return load_iaddress(index, scratch);
  84 #else
  85   get_local_base_addr(scratch, index);
  86   return Address(scratch);
  87 #endif // __SOFTFP__
  88 }
  89 
  90 Address TemplateTable::load_daddress(Register index, Register scratch) {
  91   get_local_base_addr(scratch, index);
  92   return Address(scratch, Interpreter::local_offset_in_bytes(1));
  93 }
  94 
  95 // At top of Java expression stack which may be different than SP.
  96 // It isn&#39;t for category 1 objects.
  97 static inline Address at_tos() {
  98   return Address(Rstack_top, Interpreter::expr_offset_in_bytes(0));
  99 }
 100 
 101 static inline Address at_tos_p1() {
 102   return Address(Rstack_top, Interpreter::expr_offset_in_bytes(1));
 103 }
 104 
 105 static inline Address at_tos_p2() {
 106   return Address(Rstack_top, Interpreter::expr_offset_in_bytes(2));
 107 }
 108 
 109 
 110 // Loads double/long local into R0_tos_lo/R1_tos_hi with two
 111 // separate ldr instructions (supports nonadjacent values).
 112 // Used for longs in all modes, and for doubles in SOFTFP mode.
 113 void TemplateTable::load_category2_local(Register Rlocal_index, Register tmp) {
 114   const Register Rlocal_base = tmp;
 115   assert_different_registers(Rlocal_index, tmp);
 116 
 117   get_local_base_addr(Rlocal_base, Rlocal_index);
 118   __ ldr(R0_tos_lo, Address(Rlocal_base, Interpreter::local_offset_in_bytes(1)));
 119   __ ldr(R1_tos_hi, Address(Rlocal_base, Interpreter::local_offset_in_bytes(0)));
 120 }
 121 
 122 
 123 // Stores R0_tos_lo/R1_tos_hi to double/long local with two
 124 // separate str instructions (supports nonadjacent values).
 125 // Used for longs in all modes, and for doubles in SOFTFP mode
 126 void TemplateTable::store_category2_local(Register Rlocal_index, Register tmp) {
 127   const Register Rlocal_base = tmp;
 128   assert_different_registers(Rlocal_index, tmp);
 129 
 130   get_local_base_addr(Rlocal_base, Rlocal_index);
 131   __ str(R0_tos_lo, Address(Rlocal_base, Interpreter::local_offset_in_bytes(1)));
 132   __ str(R1_tos_hi, Address(Rlocal_base, Interpreter::local_offset_in_bytes(0)));
 133 }
 134 
 135 // Returns address of Java array element using temp register as address base.
 136 Address TemplateTable::get_array_elem_addr(BasicType elemType, Register array, Register index, Register temp) {
 137   int logElemSize = exact_log2(type2aelembytes(elemType));
 138   __ add_ptr_scaled_int32(temp, array, index, logElemSize);
 139   return Address(temp, arrayOopDesc::base_offset_in_bytes(elemType));
 140 }
 141 
 142 // Returns address of Java array element using temp register as offset from array base
 143 Address TemplateTable::get_array_elem_addr_same_base(BasicType elemType, Register array, Register index, Register temp) {
 144   int logElemSize = exact_log2(type2aelembytes(elemType));
 145   if (logElemSize == 0) {
 146     __ add(temp, index, arrayOopDesc::base_offset_in_bytes(elemType));
 147   } else {
 148     __ mov(temp, arrayOopDesc::base_offset_in_bytes(elemType));
 149     __ add_ptr_scaled_int32(temp, temp, index, logElemSize);
 150   }
 151   return Address(array, temp);
 152 }
 153 
 154 //----------------------------------------------------------------------------------------------------
 155 // Condition conversion
 156 AsmCondition convNegCond(TemplateTable::Condition cc) {
 157   switch (cc) {
 158     case TemplateTable::equal        : return ne;
 159     case TemplateTable::not_equal    : return eq;
 160     case TemplateTable::less         : return ge;
 161     case TemplateTable::less_equal   : return gt;
 162     case TemplateTable::greater      : return le;
 163     case TemplateTable::greater_equal: return lt;
 164   }
 165   ShouldNotReachHere();
 166   return nv;
 167 }
 168 
 169 //----------------------------------------------------------------------------------------------------
 170 // Miscelaneous helper routines
 171 
 172 // Store an oop (or NULL) at the address described by obj.
 173 // Blows all volatile registers R0-R3, Rtemp, LR).
 174 // Also destroys new_val and obj.base().
 175 static void do_oop_store(InterpreterMacroAssembler* _masm,
 176                          Address obj,
 177                          Register new_val,
 178                          Register tmp1,
 179                          Register tmp2,
 180                          Register tmp3,
 181                          bool is_null,
 182                          DecoratorSet decorators = 0) {
 183 
 184   assert_different_registers(obj.base(), new_val, tmp1, tmp2, tmp3, noreg);
 185   if (is_null) {
 186     __ store_heap_oop_null(obj, new_val, tmp1, tmp2, tmp3, decorators);
 187   } else {
 188     __ store_heap_oop(obj, new_val, tmp1, tmp2, tmp3, decorators);
 189   }
 190 }
 191 
 192 static void do_oop_load(InterpreterMacroAssembler* _masm,
 193                         Register dst,
 194                         Address obj,
 195                         DecoratorSet decorators = 0) {
 196   __ load_heap_oop(dst, obj, noreg, noreg, noreg, decorators);
 197 }
 198 
 199 Address TemplateTable::at_bcp(int offset) {
 200   assert(_desc-&gt;uses_bcp(), &quot;inconsistent uses_bcp information&quot;);
 201   return Address(Rbcp, offset);
 202 }
 203 
 204 
 205 // Blows volatile registers R0-R3, Rtemp, LR.
 206 void TemplateTable::patch_bytecode(Bytecodes::Code bc, Register bc_reg,
 207                                    Register temp_reg, bool load_bc_into_bc_reg/*=true*/,
 208                                    int byte_no) {
 209   assert_different_registers(bc_reg, temp_reg);
 210   if (!RewriteBytecodes)  return;
 211   Label L_patch_done;
 212 
 213   switch (bc) {
 214   case Bytecodes::_fast_aputfield:
 215   case Bytecodes::_fast_bputfield:
 216   case Bytecodes::_fast_zputfield:
 217   case Bytecodes::_fast_cputfield:
 218   case Bytecodes::_fast_dputfield:
 219   case Bytecodes::_fast_fputfield:
 220   case Bytecodes::_fast_iputfield:
 221   case Bytecodes::_fast_lputfield:
 222   case Bytecodes::_fast_sputfield:
 223     {
 224       // We skip bytecode quickening for putfield instructions when
 225       // the put_code written to the constant pool cache is zero.
 226       // This is required so that every execution of this instruction
 227       // calls out to InterpreterRuntime::resolve_get_put to do
 228       // additional, required work.
 229       assert(byte_no == f1_byte || byte_no == f2_byte, &quot;byte_no out of range&quot;);
 230       assert(load_bc_into_bc_reg, &quot;we use bc_reg as temp&quot;);
 231       __ get_cache_and_index_and_bytecode_at_bcp(bc_reg, temp_reg, temp_reg, byte_no, 1, sizeof(u2));
 232       __ mov(bc_reg, bc);
 233       __ cbz(temp_reg, L_patch_done);  // test if bytecode is zero
 234     }
 235     break;
 236   default:
 237     assert(byte_no == -1, &quot;sanity&quot;);
 238     // the pair bytecodes have already done the load.
 239     if (load_bc_into_bc_reg) {
 240       __ mov(bc_reg, bc);
 241     }
 242   }
 243 
 244   if (__ can_post_breakpoint()) {
 245     Label L_fast_patch;
 246     // if a breakpoint is present we can&#39;t rewrite the stream directly
 247     __ ldrb(temp_reg, at_bcp(0));
 248     __ cmp(temp_reg, Bytecodes::_breakpoint);
 249     __ b(L_fast_patch, ne);
 250     if (bc_reg != R3) {
 251       __ mov(R3, bc_reg);
 252     }
 253     __ mov(R1, Rmethod);
 254     __ mov(R2, Rbcp);
 255     // Let breakpoint table handling rewrite to quicker bytecode
 256     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::set_original_bytecode_at), R1, R2, R3);
 257     __ b(L_patch_done);
 258     __ bind(L_fast_patch);
 259   }
 260 
 261 #ifdef ASSERT
 262   Label L_okay;
 263   __ ldrb(temp_reg, at_bcp(0));
 264   __ cmp(temp_reg, (int)Bytecodes::java_code(bc));
 265   __ b(L_okay, eq);
 266   __ cmp(temp_reg, bc_reg);
 267   __ b(L_okay, eq);
 268   __ stop(&quot;patching the wrong bytecode&quot;);
 269   __ bind(L_okay);
 270 #endif
 271 
 272   // patch bytecode
 273   __ strb(bc_reg, at_bcp(0));
 274   __ bind(L_patch_done);
 275 }
 276 
 277 //----------------------------------------------------------------------------------------------------
 278 // Individual instructions
 279 
 280 void TemplateTable::nop() {
 281   transition(vtos, vtos);
 282   // nothing to do
 283 }
 284 
 285 void TemplateTable::shouldnotreachhere() {
 286   transition(vtos, vtos);
 287   __ stop(&quot;shouldnotreachhere bytecode&quot;);
 288 }
 289 
 290 
 291 
 292 void TemplateTable::aconst_null() {
 293   transition(vtos, atos);
 294   __ mov(R0_tos, 0);
 295 }
 296 
 297 
 298 void TemplateTable::iconst(int value) {
 299   transition(vtos, itos);
 300   __ mov_slow(R0_tos, value);
 301 }
 302 
 303 
 304 void TemplateTable::lconst(int value) {
 305   transition(vtos, ltos);
 306   assert((value == 0) || (value == 1), &quot;unexpected long constant&quot;);
 307   __ mov(R0_tos, value);
 308   __ mov(R1_tos_hi, 0);
 309 }
 310 
 311 
 312 void TemplateTable::fconst(int value) {
 313   transition(vtos, ftos);
 314   const int zero = 0;         // 0.0f
 315   const int one = 0x3f800000; // 1.0f
 316   const int two = 0x40000000; // 2.0f
 317 
 318   switch(value) {
 319   case 0:   __ mov(R0_tos, zero);   break;
 320   case 1:   __ mov(R0_tos, one);    break;
 321   case 2:   __ mov(R0_tos, two);    break;
 322   default:  ShouldNotReachHere();   break;
 323   }
 324 
 325 #ifndef __SOFTFP__
 326   __ fmsr(S0_tos, R0_tos);
 327 #endif // !__SOFTFP__
 328 }
 329 
 330 
 331 void TemplateTable::dconst(int value) {
 332   transition(vtos, dtos);
 333   const int one_lo = 0;            // low part of 1.0
 334   const int one_hi = 0x3ff00000;   // high part of 1.0
 335 
 336   if (value == 0) {
 337 #ifdef __SOFTFP__
 338     __ mov(R0_tos_lo, 0);
 339     __ mov(R1_tos_hi, 0);
 340 #else
 341     __ mov(R0_tmp, 0);
 342     __ fmdrr(D0_tos, R0_tmp, R0_tmp);
 343 #endif // __SOFTFP__
 344   } else if (value == 1) {
 345     __ mov(R0_tos_lo, one_lo);
 346     __ mov_slow(R1_tos_hi, one_hi);
 347 #ifndef __SOFTFP__
 348     __ fmdrr(D0_tos, R0_tos_lo, R1_tos_hi);
 349 #endif // !__SOFTFP__
 350   } else {
 351     ShouldNotReachHere();
 352   }
 353 }
 354 
 355 
 356 void TemplateTable::bipush() {
 357   transition(vtos, itos);
 358   __ ldrsb(R0_tos, at_bcp(1));
 359 }
 360 
 361 
 362 void TemplateTable::sipush() {
 363   transition(vtos, itos);
 364   __ ldrsb(R0_tmp, at_bcp(1));
 365   __ ldrb(R1_tmp, at_bcp(2));
 366   __ orr(R0_tos, R1_tmp, AsmOperand(R0_tmp, lsl, BitsPerByte));
 367 }
 368 
 369 
 370 void TemplateTable::ldc(bool wide) {
 371   transition(vtos, vtos);
 372   Label fastCase, Condy, Done;
 373 
 374   const Register Rindex = R1_tmp;
 375   const Register Rcpool = R2_tmp;
 376   const Register Rtags  = R3_tmp;
 377   const Register RtagType = R3_tmp;
 378 
 379   if (wide) {
 380     __ get_unsigned_2_byte_index_at_bcp(Rindex, 1);
 381   } else {
 382     __ ldrb(Rindex, at_bcp(1));
 383   }
 384   __ get_cpool_and_tags(Rcpool, Rtags);
 385 
 386   const int base_offset = ConstantPool::header_size() * wordSize;
 387   const int tags_offset = Array&lt;u1&gt;::base_offset_in_bytes();
 388 
 389   // get const type
 390   __ add(Rtemp, Rtags, tags_offset);
 391   __ ldrb(RtagType, Address(Rtemp, Rindex));
 392   volatile_barrier(MacroAssembler::LoadLoad, Rtemp);
 393 
 394   // unresolved class - get the resolved class
 395   __ cmp(RtagType, JVM_CONSTANT_UnresolvedClass);
 396 
 397   // unresolved class in error (resolution failed) - call into runtime
 398   // so that the same error from first resolution attempt is thrown.
 399   __ cond_cmp(RtagType, JVM_CONSTANT_UnresolvedClassInError, ne);
 400 
 401   // resolved class - need to call vm to get java mirror of the class
 402   __ cond_cmp(RtagType, JVM_CONSTANT_Class, ne);
 403 
 404   __ b(fastCase, ne);
 405 
 406   // slow case - call runtime
 407   __ mov(R1, wide);
 408   call_VM(R0_tos, CAST_FROM_FN_PTR(address, InterpreterRuntime::ldc), R1);
 409   __ push(atos);
 410   __ b(Done);
 411 
 412   // int, float, String
 413   __ bind(fastCase);
 414 
 415   __ cmp(RtagType, JVM_CONSTANT_Integer);
 416   __ cond_cmp(RtagType, JVM_CONSTANT_Float, ne);
 417   __ b(Condy, ne);
 418 
 419   // itos, ftos
 420   __ add(Rtemp, Rcpool, AsmOperand(Rindex, lsl, LogBytesPerWord));
 421   __ ldr_u32(R0_tos, Address(Rtemp, base_offset));
 422 
 423   // floats and ints are placed on stack in the same way, so
 424   // we can use push(itos) to transfer float value without VFP
 425   __ push(itos);
 426   __ b(Done);
 427 
 428   __ bind(Condy);
 429   condy_helper(Done);
 430 
 431   __ bind(Done);
 432 }
 433 
 434 // Fast path for caching oop constants.
 435 void TemplateTable::fast_aldc(bool wide) {
 436   transition(vtos, atos);
 437   int index_size = wide ? sizeof(u2) : sizeof(u1);
 438   Label resolved;
 439 
 440   // We are resolved if the resolved reference cache entry contains a
 441   // non-null object (CallSite, etc.)
 442   assert_different_registers(R0_tos, R2_tmp);
 443   __ get_index_at_bcp(R2_tmp, 1, R0_tos, index_size);
 444   __ load_resolved_reference_at_index(R0_tos, R2_tmp);
 445   __ cbnz(R0_tos, resolved);
 446 
 447   address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_ldc);
 448 
 449   // first time invocation - must resolve first
 450   __ mov(R1, (int)bytecode());
 451   __ call_VM(R0_tos, entry, R1);
 452   __ bind(resolved);
 453 
 454   { // Check for the null sentinel.
 455     // If we just called the VM, that already did the mapping for us,
 456     // but it&#39;s harmless to retry.
 457     Label notNull;
 458     Register result = R0;
 459     Register tmp = R1;
 460     Register rarg = R2;
 461 
 462     // Stash null_sentinel address to get its value later
 463     __ mov_slow(rarg, (uintptr_t)Universe::the_null_sentinel_addr());
 464     __ ldr(tmp, Address(rarg));
 465     __ cmp(result, tmp);
 466     __ b(notNull, ne);
 467     __ mov(result, 0);  // NULL object reference
 468     __ bind(notNull);
 469   }
 470 
 471   if (VerifyOops) {
 472     __ verify_oop(R0_tos);
 473   }
 474 }
 475 
 476 void TemplateTable::ldc2_w() {
 477   transition(vtos, vtos);
 478   const Register Rtags  = R2_tmp;
 479   const Register Rindex = R3_tmp;
 480   const Register Rcpool = R4_tmp;
 481   const Register Rbase  = R5_tmp;
 482 
 483   __ get_unsigned_2_byte_index_at_bcp(Rindex, 1);
 484 
 485   __ get_cpool_and_tags(Rcpool, Rtags);
 486   const int base_offset = ConstantPool::header_size() * wordSize;
 487   const int tags_offset = Array&lt;u1&gt;::base_offset_in_bytes();
 488 
 489   __ add(Rbase, Rcpool, AsmOperand(Rindex, lsl, LogBytesPerWord));
 490 
<a name="3" id="anc3"></a>


 491   // get type from tags
 492   __ add(Rtemp, Rtags, tags_offset);
 493   __ ldrb(Rtemp, Address(Rtemp, Rindex));
<a name="4" id="anc4"></a><span class="line-added"> 494 </span>
<span class="line-added"> 495   Label Condy, exit;</span>
<span class="line-added"> 496 #ifdef __ABI_HARD__</span>
<span class="line-added"> 497   Label NotDouble;</span>
 498   __ cmp(Rtemp, JVM_CONSTANT_Double);
<a name="5" id="anc5"></a><span class="line-modified"> 499   __ b(NotDouble, ne);</span>
 500   __ ldr_double(D0_tos, Address(Rbase, base_offset));
 501 
 502   __ push(dtos);
 503   __ b(exit);
<a name="6" id="anc6"></a><span class="line-modified"> 504   __ bind(NotDouble);</span>
 505 #endif
 506 
 507   __ cmp(Rtemp, JVM_CONSTANT_Long);
 508   __ b(Condy, ne);
 509   __ ldr(R0_tos_lo, Address(Rbase, base_offset + 0 * wordSize));
 510   __ ldr(R1_tos_hi, Address(Rbase, base_offset + 1 * wordSize));
 511   __ push(ltos);
 512   __ b(exit);
 513 
 514   __ bind(Condy);
 515   condy_helper(exit);
 516 
 517   __ bind(exit);
 518 }
 519 
 520 
 521 void TemplateTable::condy_helper(Label&amp; Done)
 522 {
 523   Register obj   = R0_tmp;
 524   Register rtmp  = R1_tmp;
 525   Register flags = R2_tmp;
 526   Register off   = R3_tmp;
 527 
 528   __ mov(rtmp, (int) bytecode());
 529   __ call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_ldc), rtmp);
 530   __ get_vm_result_2(flags, rtmp);
 531 
 532   // VMr = obj = base address to find primitive value to push
 533   // VMr2 = flags = (tos, off) using format of CPCE::_flags
 534   __ mov(off, flags);
 535 
 536   __ logical_shift_left( off, off, 32 - ConstantPoolCacheEntry::field_index_bits);
 537   __ logical_shift_right(off, off, 32 - ConstantPoolCacheEntry::field_index_bits);
 538 
 539   const Address field(obj, off);
 540 
 541   __ logical_shift_right(flags, flags, ConstantPoolCacheEntry::tos_state_shift);
 542   // Make sure we don&#39;t need to mask flags after the above shift
 543   ConstantPoolCacheEntry::verify_tos_state_shift();
 544 
 545   switch (bytecode()) {
 546     case Bytecodes::_ldc:
 547     case Bytecodes::_ldc_w:
 548       {
 549         // tos in (itos, ftos, stos, btos, ctos, ztos)
 550         Label notIntFloat, notShort, notByte, notChar, notBool;
 551         __ cmp(flags, itos);
 552         __ cond_cmp(flags, ftos, ne);
 553         __ b(notIntFloat, ne);
 554         __ ldr(R0_tos, field);
 555         __ push(itos);
 556         __ b(Done);
 557 
 558         __ bind(notIntFloat);
 559         __ cmp(flags, stos);
 560         __ b(notShort, ne);
 561         __ ldrsh(R0_tos, field);
 562         __ push(stos);
 563         __ b(Done);
 564 
 565         __ bind(notShort);
 566         __ cmp(flags, btos);
 567         __ b(notByte, ne);
 568         __ ldrsb(R0_tos, field);
 569         __ push(btos);
 570         __ b(Done);
 571 
 572         __ bind(notByte);
 573         __ cmp(flags, ctos);
 574         __ b(notChar, ne);
 575         __ ldrh(R0_tos, field);
 576         __ push(ctos);
 577         __ b(Done);
 578 
 579         __ bind(notChar);
 580         __ cmp(flags, ztos);
 581         __ b(notBool, ne);
 582         __ ldrsb(R0_tos, field);
 583         __ push(ztos);
 584         __ b(Done);
 585 
 586         __ bind(notBool);
 587         break;
 588       }
 589 
 590     case Bytecodes::_ldc2_w:
 591       {
 592         Label notLongDouble;
 593         __ cmp(flags, ltos);
 594         __ cond_cmp(flags, dtos, ne);
 595         __ b(notLongDouble, ne);
 596 
 597         __ add(rtmp, obj, wordSize);
 598         __ ldr(R0_tos_lo, Address(obj, off));
 599         __ ldr(R1_tos_hi, Address(rtmp, off));
 600         __ push(ltos);
 601         __ b(Done);
 602 
 603         __ bind(notLongDouble);
 604 
 605         break;
 606       }
 607 
 608     default:
 609       ShouldNotReachHere();
 610     }
 611 
 612     __ stop(&quot;bad ldc/condy&quot;);
 613 }
 614 
 615 
 616 void TemplateTable::locals_index(Register reg, int offset) {
 617   __ ldrb(reg, at_bcp(offset));
 618 }
 619 
 620 void TemplateTable::iload() {
 621   iload_internal();
 622 }
 623 
 624 void TemplateTable::nofast_iload() {
 625   iload_internal(may_not_rewrite);
 626 }
 627 
 628 void TemplateTable::iload_internal(RewriteControl rc) {
 629   transition(vtos, itos);
 630 
 631   if ((rc == may_rewrite) &amp;&amp; __ rewrite_frequent_pairs()) {
 632     Label rewrite, done;
 633     const Register next_bytecode = R1_tmp;
 634     const Register target_bytecode = R2_tmp;
 635 
 636     // get next byte
 637     __ ldrb(next_bytecode, at_bcp(Bytecodes::length_for(Bytecodes::_iload)));
 638     // if _iload, wait to rewrite to iload2.  We only want to rewrite the
 639     // last two iloads in a pair.  Comparing against fast_iload means that
 640     // the next bytecode is neither an iload or a caload, and therefore
 641     // an iload pair.
 642     __ cmp(next_bytecode, Bytecodes::_iload);
 643     __ b(done, eq);
 644 
 645     __ cmp(next_bytecode, Bytecodes::_fast_iload);
 646     __ mov(target_bytecode, Bytecodes::_fast_iload2);
 647     __ b(rewrite, eq);
 648 
 649     // if _caload, rewrite to fast_icaload
 650     __ cmp(next_bytecode, Bytecodes::_caload);
 651     __ mov(target_bytecode, Bytecodes::_fast_icaload);
 652     __ b(rewrite, eq);
 653 
 654     // rewrite so iload doesn&#39;t check again.
 655     __ mov(target_bytecode, Bytecodes::_fast_iload);
 656 
 657     // rewrite
 658     // R2: fast bytecode
 659     __ bind(rewrite);
 660     patch_bytecode(Bytecodes::_iload, target_bytecode, Rtemp, false);
 661     __ bind(done);
 662   }
 663 
 664   // Get the local value into tos
 665   const Register Rlocal_index = R1_tmp;
 666   locals_index(Rlocal_index);
 667   Address local = load_iaddress(Rlocal_index, Rtemp);
 668   __ ldr_s32(R0_tos, local);
 669 }
 670 
 671 
 672 void TemplateTable::fast_iload2() {
 673   transition(vtos, itos);
 674   const Register Rlocal_index = R1_tmp;
 675 
 676   locals_index(Rlocal_index);
 677   Address local = load_iaddress(Rlocal_index, Rtemp);
 678   __ ldr_s32(R0_tos, local);
 679   __ push(itos);
 680 
 681   locals_index(Rlocal_index, 3);
 682   local = load_iaddress(Rlocal_index, Rtemp);
 683   __ ldr_s32(R0_tos, local);
 684 }
 685 
 686 void TemplateTable::fast_iload() {
 687   transition(vtos, itos);
 688   const Register Rlocal_index = R1_tmp;
 689 
 690   locals_index(Rlocal_index);
 691   Address local = load_iaddress(Rlocal_index, Rtemp);
 692   __ ldr_s32(R0_tos, local);
 693 }
 694 
 695 
 696 void TemplateTable::lload() {
 697   transition(vtos, ltos);
 698   const Register Rlocal_index = R2_tmp;
 699 
 700   locals_index(Rlocal_index);
 701   load_category2_local(Rlocal_index, R3_tmp);
 702 }
 703 
 704 
 705 void TemplateTable::fload() {
 706   transition(vtos, ftos);
 707   const Register Rlocal_index = R2_tmp;
 708 
 709   // Get the local value into tos
 710   locals_index(Rlocal_index);
 711   Address local = load_faddress(Rlocal_index, Rtemp);
 712 #ifdef __SOFTFP__
 713   __ ldr(R0_tos, local);
 714 #else
 715   __ ldr_float(S0_tos, local);
 716 #endif // __SOFTFP__
 717 }
 718 
 719 
 720 void TemplateTable::dload() {
 721   transition(vtos, dtos);
 722   const Register Rlocal_index = R2_tmp;
 723 
 724   locals_index(Rlocal_index);
 725 
 726 #ifdef __SOFTFP__
 727   load_category2_local(Rlocal_index, R3_tmp);
 728 #else
 729   __ ldr_double(D0_tos, load_daddress(Rlocal_index, Rtemp));
 730 #endif // __SOFTFP__
 731 }
 732 
 733 
 734 void TemplateTable::aload() {
 735   transition(vtos, atos);
 736   const Register Rlocal_index = R1_tmp;
 737 
 738   locals_index(Rlocal_index);
 739   Address local = load_aaddress(Rlocal_index, Rtemp);
 740   __ ldr(R0_tos, local);
 741 }
 742 
 743 
 744 void TemplateTable::locals_index_wide(Register reg) {
 745   assert_different_registers(reg, Rtemp);
 746   __ ldrb(Rtemp, at_bcp(2));
 747   __ ldrb(reg, at_bcp(3));
 748   __ orr(reg, reg, AsmOperand(Rtemp, lsl, 8));
 749 }
 750 
 751 
 752 void TemplateTable::wide_iload() {
 753   transition(vtos, itos);
 754   const Register Rlocal_index = R2_tmp;
 755 
 756   locals_index_wide(Rlocal_index);
 757   Address local = load_iaddress(Rlocal_index, Rtemp);
 758   __ ldr_s32(R0_tos, local);
 759 }
 760 
 761 
 762 void TemplateTable::wide_lload() {
 763   transition(vtos, ltos);
 764   const Register Rlocal_index = R2_tmp;
 765   const Register Rlocal_base = R3_tmp;
 766 
 767   locals_index_wide(Rlocal_index);
 768   load_category2_local(Rlocal_index, R3_tmp);
 769 }
 770 
 771 
 772 void TemplateTable::wide_fload() {
 773   transition(vtos, ftos);
 774   const Register Rlocal_index = R2_tmp;
 775 
 776   locals_index_wide(Rlocal_index);
 777   Address local = load_faddress(Rlocal_index, Rtemp);
 778 #ifdef __SOFTFP__
 779   __ ldr(R0_tos, local);
 780 #else
 781   __ ldr_float(S0_tos, local);
 782 #endif // __SOFTFP__
 783 }
 784 
 785 
 786 void TemplateTable::wide_dload() {
 787   transition(vtos, dtos);
 788   const Register Rlocal_index = R2_tmp;
 789 
 790   locals_index_wide(Rlocal_index);
 791 #ifdef __SOFTFP__
 792   load_category2_local(Rlocal_index, R3_tmp);
 793 #else
 794   __ ldr_double(D0_tos, load_daddress(Rlocal_index, Rtemp));
 795 #endif // __SOFTFP__
 796 }
 797 
 798 
 799 void TemplateTable::wide_aload() {
 800   transition(vtos, atos);
 801   const Register Rlocal_index = R2_tmp;
 802 
 803   locals_index_wide(Rlocal_index);
 804   Address local = load_aaddress(Rlocal_index, Rtemp);
 805   __ ldr(R0_tos, local);
 806 }
 807 
 808 void TemplateTable::index_check(Register array, Register index) {
 809   // Pop ptr into array
 810   __ pop_ptr(array);
 811   index_check_without_pop(array, index);
 812 }
 813 
 814 void TemplateTable::index_check_without_pop(Register array, Register index) {
 815   assert_different_registers(array, index, Rtemp);
 816   // check array
 817   __ null_check(array, Rtemp, arrayOopDesc::length_offset_in_bytes());
 818   // check index
 819   __ ldr_s32(Rtemp, Address(array, arrayOopDesc::length_offset_in_bytes()));
 820   __ cmp_32(index, Rtemp);
 821   if (index != R4_ArrayIndexOutOfBounds_index) {
 822     // convention with generate_ArrayIndexOutOfBounds_handler()
 823     __ mov(R4_ArrayIndexOutOfBounds_index, index, hs);
 824   }
 825   __ mov(R1, array, hs);
 826   __ b(Interpreter::_throw_ArrayIndexOutOfBoundsException_entry, hs);
 827 }
 828 
 829 
 830 void TemplateTable::iaload() {
 831   transition(itos, itos);
 832   const Register Rarray = R1_tmp;
 833   const Register Rindex = R0_tos;
 834 
 835   index_check(Rarray, Rindex);
 836   Address addr = get_array_elem_addr_same_base(T_INT, Rarray, Rindex, Rtemp);
 837   __ access_load_at(T_INT, IN_HEAP | IS_ARRAY, addr, R0_tos, noreg, noreg, noreg);
 838 }
 839 
 840 
 841 void TemplateTable::laload() {
 842   transition(itos, ltos);
 843   const Register Rarray = R1_tmp;
 844   const Register Rindex = R0_tos;
 845 
 846   index_check(Rarray, Rindex);
 847 
 848   Address addr = get_array_elem_addr_same_base(T_LONG, Rarray, Rindex, Rtemp);
 849   __ access_load_at(T_LONG, IN_HEAP | IS_ARRAY, addr, noreg /* ltos */, noreg, noreg, noreg);
 850 }
 851 
 852 
 853 void TemplateTable::faload() {
 854   transition(itos, ftos);
 855   const Register Rarray = R1_tmp;
 856   const Register Rindex = R0_tos;
 857 
 858   index_check(Rarray, Rindex);
 859 
 860   Address addr = get_array_elem_addr_same_base(T_FLOAT, Rarray, Rindex, Rtemp);
 861   __ access_load_at(T_FLOAT, IN_HEAP | IS_ARRAY, addr, noreg /* ftos */, noreg, noreg, noreg);
 862 }
 863 
 864 
 865 void TemplateTable::daload() {
 866   transition(itos, dtos);
 867   const Register Rarray = R1_tmp;
 868   const Register Rindex = R0_tos;
 869 
 870   index_check(Rarray, Rindex);
 871 
 872   Address addr = get_array_elem_addr_same_base(T_DOUBLE, Rarray, Rindex, Rtemp);
 873   __ access_load_at(T_DOUBLE, IN_HEAP | IS_ARRAY, addr, noreg /* dtos */, noreg, noreg, noreg);
 874 }
 875 
 876 
 877 void TemplateTable::aaload() {
 878   transition(itos, atos);
 879   const Register Rarray = R1_tmp;
 880   const Register Rindex = R0_tos;
 881 
 882   index_check(Rarray, Rindex);
 883   do_oop_load(_masm, R0_tos, get_array_elem_addr_same_base(T_OBJECT, Rarray, Rindex, Rtemp), IS_ARRAY);
 884 }
 885 
 886 
 887 void TemplateTable::baload() {
 888   transition(itos, itos);
 889   const Register Rarray = R1_tmp;
 890   const Register Rindex = R0_tos;
 891 
 892   index_check(Rarray, Rindex);
 893   Address addr = get_array_elem_addr_same_base(T_BYTE, Rarray, Rindex, Rtemp);
 894   __ access_load_at(T_BYTE, IN_HEAP | IS_ARRAY, addr, R0_tos, noreg, noreg, noreg);
 895 }
 896 
 897 
 898 void TemplateTable::caload() {
 899   transition(itos, itos);
 900   const Register Rarray = R1_tmp;
 901   const Register Rindex = R0_tos;
 902 
 903   index_check(Rarray, Rindex);
 904   Address addr = get_array_elem_addr_same_base(T_CHAR, Rarray, Rindex, Rtemp);
 905   __ access_load_at(T_CHAR, IN_HEAP | IS_ARRAY, addr, R0_tos, noreg, noreg, noreg);
 906 }
 907 
 908 
 909 // iload followed by caload frequent pair
 910 void TemplateTable::fast_icaload() {
 911   transition(vtos, itos);
 912   const Register Rlocal_index = R1_tmp;
 913   const Register Rarray = R1_tmp;
 914   const Register Rindex = R4_tmp; // index_check prefers index on R4
 915   assert_different_registers(Rlocal_index, Rindex);
 916   assert_different_registers(Rarray, Rindex);
 917 
 918   // load index out of locals
 919   locals_index(Rlocal_index);
 920   Address local = load_iaddress(Rlocal_index, Rtemp);
 921   __ ldr_s32(Rindex, local);
 922 
 923   // get array element
 924   index_check(Rarray, Rindex);
 925   Address addr = get_array_elem_addr_same_base(T_CHAR, Rarray, Rindex, Rtemp);
 926   __ access_load_at(T_CHAR, IN_HEAP | IS_ARRAY, addr, R0_tos, noreg, noreg, noreg);
 927 }
 928 
 929 
 930 void TemplateTable::saload() {
 931   transition(itos, itos);
 932   const Register Rarray = R1_tmp;
 933   const Register Rindex = R0_tos;
 934 
 935   index_check(Rarray, Rindex);
 936   Address addr = get_array_elem_addr_same_base(T_SHORT, Rarray, Rindex, Rtemp);
 937   __ access_load_at(T_SHORT, IN_HEAP | IS_ARRAY, addr, R0_tos, noreg, noreg, noreg);
 938 }
 939 
 940 
 941 void TemplateTable::iload(int n) {
 942   transition(vtos, itos);
 943   __ ldr_s32(R0_tos, iaddress(n));
 944 }
 945 
 946 
 947 void TemplateTable::lload(int n) {
 948   transition(vtos, ltos);
 949   __ ldr(R0_tos_lo, laddress(n));
 950   __ ldr(R1_tos_hi, haddress(n));
 951 }
 952 
 953 
 954 void TemplateTable::fload(int n) {
 955   transition(vtos, ftos);
 956 #ifdef __SOFTFP__
 957   __ ldr(R0_tos, faddress(n));
 958 #else
 959   __ ldr_float(S0_tos, faddress(n));
 960 #endif // __SOFTFP__
 961 }
 962 
 963 
 964 void TemplateTable::dload(int n) {
 965   transition(vtos, dtos);
 966 #ifdef __SOFTFP__
 967   __ ldr(R0_tos_lo, laddress(n));
 968   __ ldr(R1_tos_hi, haddress(n));
 969 #else
 970   __ ldr_double(D0_tos, daddress(n));
 971 #endif // __SOFTFP__
 972 }
 973 
 974 
 975 void TemplateTable::aload(int n) {
 976   transition(vtos, atos);
 977   __ ldr(R0_tos, aaddress(n));
 978 }
 979 
 980 void TemplateTable::aload_0() {
 981   aload_0_internal();
 982 }
 983 
 984 void TemplateTable::nofast_aload_0() {
 985   aload_0_internal(may_not_rewrite);
 986 }
 987 
 988 void TemplateTable::aload_0_internal(RewriteControl rc) {
 989   transition(vtos, atos);
 990   // According to bytecode histograms, the pairs:
 991   //
 992   // _aload_0, _fast_igetfield
 993   // _aload_0, _fast_agetfield
 994   // _aload_0, _fast_fgetfield
 995   //
 996   // occur frequently. If RewriteFrequentPairs is set, the (slow) _aload_0
 997   // bytecode checks if the next bytecode is either _fast_igetfield,
 998   // _fast_agetfield or _fast_fgetfield and then rewrites the
 999   // current bytecode into a pair bytecode; otherwise it rewrites the current
1000   // bytecode into _fast_aload_0 that doesn&#39;t do the pair check anymore.
1001   //
1002   // Note: If the next bytecode is _getfield, the rewrite must be delayed,
1003   //       otherwise we may miss an opportunity for a pair.
1004   //
1005   // Also rewrite frequent pairs
1006   //   aload_0, aload_1
1007   //   aload_0, iload_1
1008   // These bytecodes with a small amount of code are most profitable to rewrite
1009   if ((rc == may_rewrite) &amp;&amp; __ rewrite_frequent_pairs()) {
1010     Label rewrite, done;
1011     const Register next_bytecode = R1_tmp;
1012     const Register target_bytecode = R2_tmp;
1013 
1014     // get next byte
1015     __ ldrb(next_bytecode, at_bcp(Bytecodes::length_for(Bytecodes::_aload_0)));
1016 
1017     // if _getfield then wait with rewrite
1018     __ cmp(next_bytecode, Bytecodes::_getfield);
1019     __ b(done, eq);
1020 
1021     // if _igetfield then rewrite to _fast_iaccess_0
1022     assert(Bytecodes::java_code(Bytecodes::_fast_iaccess_0) == Bytecodes::_aload_0, &quot;fix bytecode definition&quot;);
1023     __ cmp(next_bytecode, Bytecodes::_fast_igetfield);
1024     __ mov(target_bytecode, Bytecodes::_fast_iaccess_0);
1025     __ b(rewrite, eq);
1026 
1027     // if _agetfield then rewrite to _fast_aaccess_0
1028     assert(Bytecodes::java_code(Bytecodes::_fast_aaccess_0) == Bytecodes::_aload_0, &quot;fix bytecode definition&quot;);
1029     __ cmp(next_bytecode, Bytecodes::_fast_agetfield);
1030     __ mov(target_bytecode, Bytecodes::_fast_aaccess_0);
1031     __ b(rewrite, eq);
1032 
1033     // if _fgetfield then rewrite to _fast_faccess_0, else rewrite to _fast_aload0
1034     assert(Bytecodes::java_code(Bytecodes::_fast_faccess_0) == Bytecodes::_aload_0, &quot;fix bytecode definition&quot;);
1035     assert(Bytecodes::java_code(Bytecodes::_fast_aload_0) == Bytecodes::_aload_0, &quot;fix bytecode definition&quot;);
1036 
1037     __ cmp(next_bytecode, Bytecodes::_fast_fgetfield);
1038     __ mov(target_bytecode, Bytecodes::_fast_faccess_0, eq);
1039     __ mov(target_bytecode, Bytecodes::_fast_aload_0, ne);
1040 
1041     // rewrite
1042     __ bind(rewrite);
1043     patch_bytecode(Bytecodes::_aload_0, target_bytecode, Rtemp, false);
1044 
1045     __ bind(done);
1046   }
1047 
1048   aload(0);
1049 }
1050 
1051 void TemplateTable::istore() {
1052   transition(itos, vtos);
1053   const Register Rlocal_index = R2_tmp;
1054 
1055   locals_index(Rlocal_index);
1056   Address local = load_iaddress(Rlocal_index, Rtemp);
1057   __ str_32(R0_tos, local);
1058 }
1059 
1060 
1061 void TemplateTable::lstore() {
1062   transition(ltos, vtos);
1063   const Register Rlocal_index = R2_tmp;
1064 
1065   locals_index(Rlocal_index);
1066   store_category2_local(Rlocal_index, R3_tmp);
1067 }
1068 
1069 
1070 void TemplateTable::fstore() {
1071   transition(ftos, vtos);
1072   const Register Rlocal_index = R2_tmp;
1073 
1074   locals_index(Rlocal_index);
1075   Address local = load_faddress(Rlocal_index, Rtemp);
1076 #ifdef __SOFTFP__
1077   __ str(R0_tos, local);
1078 #else
1079   __ str_float(S0_tos, local);
1080 #endif // __SOFTFP__
1081 }
1082 
1083 
1084 void TemplateTable::dstore() {
1085   transition(dtos, vtos);
1086   const Register Rlocal_index = R2_tmp;
1087 
1088   locals_index(Rlocal_index);
1089 
1090 #ifdef __SOFTFP__
1091   store_category2_local(Rlocal_index, R3_tmp);
1092 #else
1093   __ str_double(D0_tos, load_daddress(Rlocal_index, Rtemp));
1094 #endif // __SOFTFP__
1095 }
1096 
1097 
1098 void TemplateTable::astore() {
1099   transition(vtos, vtos);
1100   const Register Rlocal_index = R1_tmp;
1101 
1102   __ pop_ptr(R0_tos);
1103   locals_index(Rlocal_index);
1104   Address local = load_aaddress(Rlocal_index, Rtemp);
1105   __ str(R0_tos, local);
1106 }
1107 
1108 
1109 void TemplateTable::wide_istore() {
1110   transition(vtos, vtos);
1111   const Register Rlocal_index = R2_tmp;
1112 
1113   __ pop_i(R0_tos);
1114   locals_index_wide(Rlocal_index);
1115   Address local = load_iaddress(Rlocal_index, Rtemp);
1116   __ str_32(R0_tos, local);
1117 }
1118 
1119 
1120 void TemplateTable::wide_lstore() {
1121   transition(vtos, vtos);
1122   const Register Rlocal_index = R2_tmp;
1123   const Register Rlocal_base = R3_tmp;
1124 
1125   __ pop_l(R0_tos_lo, R1_tos_hi);
1126 
1127   locals_index_wide(Rlocal_index);
1128   store_category2_local(Rlocal_index, R3_tmp);
1129 }
1130 
1131 
1132 void TemplateTable::wide_fstore() {
1133   wide_istore();
1134 }
1135 
1136 
1137 void TemplateTable::wide_dstore() {
1138   wide_lstore();
1139 }
1140 
1141 
1142 void TemplateTable::wide_astore() {
1143   transition(vtos, vtos);
1144   const Register Rlocal_index = R2_tmp;
1145 
1146   __ pop_ptr(R0_tos);
1147   locals_index_wide(Rlocal_index);
1148   Address local = load_aaddress(Rlocal_index, Rtemp);
1149   __ str(R0_tos, local);
1150 }
1151 
1152 
1153 void TemplateTable::iastore() {
1154   transition(itos, vtos);
1155   const Register Rindex = R4_tmp; // index_check prefers index in R4
1156   const Register Rarray = R3_tmp;
1157   // R0_tos: value
1158 
1159   __ pop_i(Rindex);
1160   index_check(Rarray, Rindex);
1161   Address addr = get_array_elem_addr_same_base(T_INT, Rarray, Rindex, Rtemp);
1162   __ access_store_at(T_INT, IN_HEAP | IS_ARRAY, addr, R0_tos, noreg, noreg, noreg, false);
1163 }
1164 
1165 
1166 void TemplateTable::lastore() {
1167   transition(ltos, vtos);
1168   const Register Rindex = R4_tmp; // index_check prefers index in R4
1169   const Register Rarray = R3_tmp;
1170   // R0_tos_lo:R1_tos_hi: value
1171 
1172   __ pop_i(Rindex);
1173   index_check(Rarray, Rindex);
1174 
1175   Address addr = get_array_elem_addr_same_base(T_LONG, Rarray, Rindex, Rtemp);
1176   __ access_store_at(T_LONG, IN_HEAP | IS_ARRAY, addr, noreg /* ltos */, noreg, noreg, noreg, false);
1177 }
1178 
1179 
1180 void TemplateTable::fastore() {
1181   transition(ftos, vtos);
1182   const Register Rindex = R4_tmp; // index_check prefers index in R4
1183   const Register Rarray = R3_tmp;
1184   // S0_tos/R0_tos: value
1185 
1186   __ pop_i(Rindex);
1187   index_check(Rarray, Rindex);
1188   Address addr = get_array_elem_addr_same_base(T_FLOAT, Rarray, Rindex, Rtemp);
1189   __ access_store_at(T_FLOAT, IN_HEAP | IS_ARRAY, addr, noreg /* ftos */, noreg, noreg, noreg, false);
1190 }
1191 
1192 
1193 void TemplateTable::dastore() {
1194   transition(dtos, vtos);
1195   const Register Rindex = R4_tmp; // index_check prefers index in R4
1196   const Register Rarray = R3_tmp;
1197   // D0_tos / R0_tos_lo:R1_to_hi: value
1198 
1199   __ pop_i(Rindex);
1200   index_check(Rarray, Rindex);
1201 
1202   Address addr = get_array_elem_addr_same_base(T_DOUBLE, Rarray, Rindex, Rtemp);
1203   __ access_store_at(T_DOUBLE, IN_HEAP | IS_ARRAY, addr, noreg /* dtos */, noreg, noreg, noreg, false);
1204 }
1205 
1206 
1207 void TemplateTable::aastore() {
1208   transition(vtos, vtos);
1209   Label is_null, throw_array_store, done;
1210 
1211   const Register Raddr_1   = R1_tmp;
1212   const Register Rvalue_2  = R2_tmp;
1213   const Register Rarray_3  = R3_tmp;
1214   const Register Rindex_4  = R4_tmp;   // preferred by index_check_without_pop()
1215   const Register Rsub_5    = R5_tmp;
1216   const Register Rsuper_LR = LR_tmp;
1217 
1218   // stack: ..., array, index, value
1219   __ ldr(Rvalue_2, at_tos());     // Value
1220   __ ldr_s32(Rindex_4, at_tos_p1());  // Index
1221   __ ldr(Rarray_3, at_tos_p2());  // Array
1222 
1223   index_check_without_pop(Rarray_3, Rindex_4);
1224 
1225   // Compute the array base
1226   __ add(Raddr_1, Rarray_3, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
1227 
1228   // do array store check - check for NULL value first
1229   __ cbz(Rvalue_2, is_null);
1230 
1231   // Load subklass
1232   __ load_klass(Rsub_5, Rvalue_2);
1233   // Load superklass
1234   __ load_klass(Rtemp, Rarray_3);
1235   __ ldr(Rsuper_LR, Address(Rtemp, ObjArrayKlass::element_klass_offset()));
1236 
1237   __ gen_subtype_check(Rsub_5, Rsuper_LR, throw_array_store, R0_tmp, R3_tmp);
1238   // Come here on success
1239 
1240   // Store value
1241   __ add(Raddr_1, Raddr_1, AsmOperand(Rindex_4, lsl, LogBytesPerHeapOop));
1242 
1243   // Now store using the appropriate barrier
1244   do_oop_store(_masm, Raddr_1, Rvalue_2, Rtemp, R0_tmp, R3_tmp, false, IS_ARRAY);
1245   __ b(done);
1246 
1247   __ bind(throw_array_store);
1248 
1249   // Come here on failure of subtype check
1250   __ profile_typecheck_failed(R0_tmp);
1251 
1252   // object is at TOS
1253   __ b(Interpreter::_throw_ArrayStoreException_entry);
1254 
1255   // Have a NULL in Rvalue_2, store NULL at array[index].
1256   __ bind(is_null);
1257   __ profile_null_seen(R0_tmp);
1258 
1259   // Store a NULL
1260   do_oop_store(_masm, Address::indexed_oop(Raddr_1, Rindex_4), Rvalue_2, Rtemp, R0_tmp, R3_tmp, true, IS_ARRAY);
1261 
1262   // Pop stack arguments
1263   __ bind(done);
1264   __ add(Rstack_top, Rstack_top, 3 * Interpreter::stackElementSize);
1265 }
1266 
1267 
1268 void TemplateTable::bastore() {
1269   transition(itos, vtos);
1270   const Register Rindex = R4_tmp; // index_check prefers index in R4
1271   const Register Rarray = R3_tmp;
1272   // R0_tos: value
1273 
1274   __ pop_i(Rindex);
1275   index_check(Rarray, Rindex);
1276 
1277   // Need to check whether array is boolean or byte
1278   // since both types share the bastore bytecode.
1279   __ load_klass(Rtemp, Rarray);
1280   __ ldr_u32(Rtemp, Address(Rtemp, Klass::layout_helper_offset()));
1281   Label L_skip;
1282   __ tst(Rtemp, Klass::layout_helper_boolean_diffbit());
1283   __ b(L_skip, eq);
1284   __ and_32(R0_tos, R0_tos, 1); // if it is a T_BOOLEAN array, mask the stored value to 0/1
1285   __ bind(L_skip);
1286   Address addr = get_array_elem_addr_same_base(T_BYTE, Rarray, Rindex, Rtemp);
1287   __ access_store_at(T_BYTE, IN_HEAP | IS_ARRAY, addr, R0_tos, noreg, noreg, noreg, false);
1288 }
1289 
1290 
1291 void TemplateTable::castore() {
1292   transition(itos, vtos);
1293   const Register Rindex = R4_tmp; // index_check prefers index in R4
1294   const Register Rarray = R3_tmp;
1295   // R0_tos: value
1296 
1297   __ pop_i(Rindex);
1298   index_check(Rarray, Rindex);
1299   Address addr = get_array_elem_addr_same_base(T_CHAR, Rarray, Rindex, Rtemp);
1300   __ access_store_at(T_CHAR, IN_HEAP | IS_ARRAY, addr, R0_tos, noreg, noreg, noreg, false);
1301 }
1302 
1303 
1304 void TemplateTable::sastore() {
1305   assert(arrayOopDesc::base_offset_in_bytes(T_CHAR) ==
1306            arrayOopDesc::base_offset_in_bytes(T_SHORT),
1307          &quot;base offsets for char and short should be equal&quot;);
1308   castore();
1309 }
1310 
1311 
1312 void TemplateTable::istore(int n) {
1313   transition(itos, vtos);
1314   __ str_32(R0_tos, iaddress(n));
1315 }
1316 
1317 
1318 void TemplateTable::lstore(int n) {
1319   transition(ltos, vtos);
1320   __ str(R0_tos_lo, laddress(n));
1321   __ str(R1_tos_hi, haddress(n));
1322 }
1323 
1324 
1325 void TemplateTable::fstore(int n) {
1326   transition(ftos, vtos);
1327 #ifdef __SOFTFP__
1328   __ str(R0_tos, faddress(n));
1329 #else
1330   __ str_float(S0_tos, faddress(n));
1331 #endif // __SOFTFP__
1332 }
1333 
1334 
1335 void TemplateTable::dstore(int n) {
1336   transition(dtos, vtos);
1337 #ifdef __SOFTFP__
1338   __ str(R0_tos_lo, laddress(n));
1339   __ str(R1_tos_hi, haddress(n));
1340 #else
1341   __ str_double(D0_tos, daddress(n));
1342 #endif // __SOFTFP__
1343 }
1344 
1345 
1346 void TemplateTable::astore(int n) {
1347   transition(vtos, vtos);
1348   __ pop_ptr(R0_tos);
1349   __ str(R0_tos, aaddress(n));
1350 }
1351 
1352 
1353 void TemplateTable::pop() {
1354   transition(vtos, vtos);
1355   __ add(Rstack_top, Rstack_top, Interpreter::stackElementSize);
1356 }
1357 
1358 
1359 void TemplateTable::pop2() {
1360   transition(vtos, vtos);
1361   __ add(Rstack_top, Rstack_top, 2*Interpreter::stackElementSize);
1362 }
1363 
1364 
1365 void TemplateTable::dup() {
1366   transition(vtos, vtos);
1367   // stack: ..., a
1368   __ load_ptr(0, R0_tmp);
1369   __ push_ptr(R0_tmp);
1370   // stack: ..., a, a
1371 }
1372 
1373 
1374 void TemplateTable::dup_x1() {
1375   transition(vtos, vtos);
1376   // stack: ..., a, b
1377   __ load_ptr(0, R0_tmp);  // load b
1378   __ load_ptr(1, R2_tmp);  // load a
1379   __ store_ptr(1, R0_tmp); // store b
1380   __ store_ptr(0, R2_tmp); // store a
1381   __ push_ptr(R0_tmp);     // push b
1382   // stack: ..., b, a, b
1383 }
1384 
1385 
1386 void TemplateTable::dup_x2() {
1387   transition(vtos, vtos);
1388   // stack: ..., a, b, c
1389   __ load_ptr(0, R0_tmp);   // load c
1390   __ load_ptr(1, R2_tmp);   // load b
1391   __ load_ptr(2, R4_tmp);   // load a
1392 
1393   __ push_ptr(R0_tmp);      // push c
1394 
1395   // stack: ..., a, b, c, c
1396   __ store_ptr(1, R2_tmp);  // store b
1397   __ store_ptr(2, R4_tmp);  // store a
1398   __ store_ptr(3, R0_tmp);  // store c
1399   // stack: ..., c, a, b, c
1400 }
1401 
1402 
1403 void TemplateTable::dup2() {
1404   transition(vtos, vtos);
1405   // stack: ..., a, b
1406   __ load_ptr(1, R0_tmp);  // load a
1407   __ push_ptr(R0_tmp);     // push a
1408   __ load_ptr(1, R0_tmp);  // load b
1409   __ push_ptr(R0_tmp);     // push b
1410   // stack: ..., a, b, a, b
1411 }
1412 
1413 
1414 void TemplateTable::dup2_x1() {
1415   transition(vtos, vtos);
1416 
1417   // stack: ..., a, b, c
1418   __ load_ptr(0, R4_tmp);  // load c
1419   __ load_ptr(1, R2_tmp);  // load b
1420   __ load_ptr(2, R0_tmp);  // load a
1421 
1422   __ push_ptr(R2_tmp);     // push b
1423   __ push_ptr(R4_tmp);     // push c
1424 
1425   // stack: ..., a, b, c, b, c
1426 
1427   __ store_ptr(2, R0_tmp);  // store a
1428   __ store_ptr(3, R4_tmp);  // store c
1429   __ store_ptr(4, R2_tmp);  // store b
1430 
1431   // stack: ..., b, c, a, b, c
1432 }
1433 
1434 
1435 void TemplateTable::dup2_x2() {
1436   transition(vtos, vtos);
1437   // stack: ..., a, b, c, d
1438   __ load_ptr(0, R0_tmp);  // load d
1439   __ load_ptr(1, R2_tmp);  // load c
1440   __ push_ptr(R2_tmp);     // push c
1441   __ push_ptr(R0_tmp);     // push d
1442   // stack: ..., a, b, c, d, c, d
1443   __ load_ptr(4, R4_tmp);  // load b
1444   __ store_ptr(4, R0_tmp); // store d in b
1445   __ store_ptr(2, R4_tmp); // store b in d
1446   // stack: ..., a, d, c, b, c, d
1447   __ load_ptr(5, R4_tmp);  // load a
1448   __ store_ptr(5, R2_tmp); // store c in a
1449   __ store_ptr(3, R4_tmp); // store a in c
1450   // stack: ..., c, d, a, b, c, d
1451 }
1452 
1453 
1454 void TemplateTable::swap() {
1455   transition(vtos, vtos);
1456   // stack: ..., a, b
1457   __ load_ptr(1, R0_tmp);  // load a
1458   __ load_ptr(0, R2_tmp);  // load b
1459   __ store_ptr(0, R0_tmp); // store a in b
1460   __ store_ptr(1, R2_tmp); // store b in a
1461   // stack: ..., b, a
1462 }
1463 
1464 
1465 void TemplateTable::iop2(Operation op) {
1466   transition(itos, itos);
1467   const Register arg1 = R1_tmp;
1468   const Register arg2 = R0_tos;
1469 
1470   __ pop_i(arg1);
1471   switch (op) {
1472     case add  : __ add_32 (R0_tos, arg1, arg2); break;
1473     case sub  : __ sub_32 (R0_tos, arg1, arg2); break;
1474     case mul  : __ mul_32 (R0_tos, arg1, arg2); break;
1475     case _and : __ and_32 (R0_tos, arg1, arg2); break;
1476     case _or  : __ orr_32 (R0_tos, arg1, arg2); break;
1477     case _xor : __ eor_32 (R0_tos, arg1, arg2); break;
1478     case shl  : __ andr(arg2, arg2, 0x1f); __ mov (R0_tos, AsmOperand(arg1, lsl, arg2)); break;
1479     case shr  : __ andr(arg2, arg2, 0x1f); __ mov (R0_tos, AsmOperand(arg1, asr, arg2)); break;
1480     case ushr : __ andr(arg2, arg2, 0x1f); __ mov (R0_tos, AsmOperand(arg1, lsr, arg2)); break;
1481     default   : ShouldNotReachHere();
1482   }
1483 }
1484 
1485 
1486 void TemplateTable::lop2(Operation op) {
1487   transition(ltos, ltos);
1488   const Register arg1_lo = R2_tmp;
1489   const Register arg1_hi = R3_tmp;
1490   const Register arg2_lo = R0_tos_lo;
1491   const Register arg2_hi = R1_tos_hi;
1492 
1493   __ pop_l(arg1_lo, arg1_hi);
1494   switch (op) {
1495     case add : __ adds(R0_tos_lo, arg1_lo, arg2_lo); __ adc (R1_tos_hi, arg1_hi, arg2_hi); break;
1496     case sub : __ subs(R0_tos_lo, arg1_lo, arg2_lo); __ sbc (R1_tos_hi, arg1_hi, arg2_hi); break;
1497     case _and: __ andr(R0_tos_lo, arg1_lo, arg2_lo); __ andr(R1_tos_hi, arg1_hi, arg2_hi); break;
1498     case _or : __ orr (R0_tos_lo, arg1_lo, arg2_lo); __ orr (R1_tos_hi, arg1_hi, arg2_hi); break;
1499     case _xor: __ eor (R0_tos_lo, arg1_lo, arg2_lo); __ eor (R1_tos_hi, arg1_hi, arg2_hi); break;
1500     default : ShouldNotReachHere();
1501   }
1502 }
1503 
1504 
1505 void TemplateTable::idiv() {
1506   transition(itos, itos);
1507   __ mov(R2, R0_tos);
1508   __ pop_i(R0);
1509   // R0 - dividend
1510   // R2 - divisor
1511   __ call(StubRoutines::Arm::idiv_irem_entry(), relocInfo::none);
1512   // R1 - result
1513   __ mov(R0_tos, R1);
1514 }
1515 
1516 
1517 void TemplateTable::irem() {
1518   transition(itos, itos);
1519   __ mov(R2, R0_tos);
1520   __ pop_i(R0);
1521   // R0 - dividend
1522   // R2 - divisor
1523   __ call(StubRoutines::Arm::idiv_irem_entry(), relocInfo::none);
1524   // R0 - remainder
1525 }
1526 
1527 
1528 void TemplateTable::lmul() {
1529   transition(ltos, ltos);
1530   const Register arg1_lo = R0_tos_lo;
1531   const Register arg1_hi = R1_tos_hi;
1532   const Register arg2_lo = R2_tmp;
1533   const Register arg2_hi = R3_tmp;
1534 
1535   __ pop_l(arg2_lo, arg2_hi);
1536 
1537   __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::lmul), arg1_lo, arg1_hi, arg2_lo, arg2_hi);
1538 }
1539 
1540 
1541 void TemplateTable::ldiv() {
1542   transition(ltos, ltos);
1543   const Register x_lo = R2_tmp;
1544   const Register x_hi = R3_tmp;
1545   const Register y_lo = R0_tos_lo;
1546   const Register y_hi = R1_tos_hi;
1547 
1548   __ pop_l(x_lo, x_hi);
1549 
1550   // check if y = 0
1551   __ orrs(Rtemp, y_lo, y_hi);
1552   __ call(Interpreter::_throw_ArithmeticException_entry, relocInfo::none, eq);
1553   __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::ldiv), y_lo, y_hi, x_lo, x_hi);
1554 }
1555 
1556 
1557 void TemplateTable::lrem() {
1558   transition(ltos, ltos);
1559   const Register x_lo = R2_tmp;
1560   const Register x_hi = R3_tmp;
1561   const Register y_lo = R0_tos_lo;
1562   const Register y_hi = R1_tos_hi;
1563 
1564   __ pop_l(x_lo, x_hi);
1565 
1566   // check if y = 0
1567   __ orrs(Rtemp, y_lo, y_hi);
1568   __ call(Interpreter::_throw_ArithmeticException_entry, relocInfo::none, eq);
1569   __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::lrem), y_lo, y_hi, x_lo, x_hi);
1570 }
1571 
1572 
1573 void TemplateTable::lshl() {
1574   transition(itos, ltos);
1575   const Register shift_cnt = R4_tmp;
1576   const Register val_lo = R2_tmp;
1577   const Register val_hi = R3_tmp;
1578 
1579   __ pop_l(val_lo, val_hi);
1580   __ andr(shift_cnt, R0_tos, 63);
1581   __ long_shift(R0_tos_lo, R1_tos_hi, val_lo, val_hi, lsl, shift_cnt);
1582 }
1583 
1584 
1585 void TemplateTable::lshr() {
1586   transition(itos, ltos);
1587   const Register shift_cnt = R4_tmp;
1588   const Register val_lo = R2_tmp;
1589   const Register val_hi = R3_tmp;
1590 
1591   __ pop_l(val_lo, val_hi);
1592   __ andr(shift_cnt, R0_tos, 63);
1593   __ long_shift(R0_tos_lo, R1_tos_hi, val_lo, val_hi, asr, shift_cnt);
1594 }
1595 
1596 
1597 void TemplateTable::lushr() {
1598   transition(itos, ltos);
1599   const Register shift_cnt = R4_tmp;
1600   const Register val_lo = R2_tmp;
1601   const Register val_hi = R3_tmp;
1602 
1603   __ pop_l(val_lo, val_hi);
1604   __ andr(shift_cnt, R0_tos, 63);
1605   __ long_shift(R0_tos_lo, R1_tos_hi, val_lo, val_hi, lsr, shift_cnt);
1606 }
1607 
1608 
1609 void TemplateTable::fop2(Operation op) {
1610   transition(ftos, ftos);
1611 #ifdef __SOFTFP__
1612   __ mov(R1, R0_tos);
1613   __ pop_i(R0);
1614   switch (op) {
1615     case add: __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_fadd_glibc), R0, R1); break;
1616     case sub: __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_fsub_glibc), R0, R1); break;
1617     case mul: __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_fmul), R0, R1); break;
1618     case div: __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_fdiv), R0, R1); break;
1619     case rem: __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::frem), R0, R1); break;
1620     default : ShouldNotReachHere();
1621   }
1622 #else
1623   const FloatRegister arg1 = S1_tmp;
1624   const FloatRegister arg2 = S0_tos;
1625 
1626   switch (op) {
1627     case add: __ pop_f(arg1); __ add_float(S0_tos, arg1, arg2); break;
1628     case sub: __ pop_f(arg1); __ sub_float(S0_tos, arg1, arg2); break;
1629     case mul: __ pop_f(arg1); __ mul_float(S0_tos, arg1, arg2); break;
1630     case div: __ pop_f(arg1); __ div_float(S0_tos, arg1, arg2); break;
1631     case rem:
1632 #ifndef __ABI_HARD__
1633       __ pop_f(arg1);
1634       __ fmrs(R0, arg1);
1635       __ fmrs(R1, arg2);
1636       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::frem), R0, R1);
1637       __ fmsr(S0_tos, R0);
1638 #else
1639       __ mov_float(S1_reg, arg2);
1640       __ pop_f(S0);
1641       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::frem));
1642 #endif // !__ABI_HARD__
1643       break;
1644     default : ShouldNotReachHere();
1645   }
1646 #endif // __SOFTFP__
1647 }
1648 
1649 
1650 void TemplateTable::dop2(Operation op) {
1651   transition(dtos, dtos);
1652 #ifdef __SOFTFP__
1653   __ mov(R2, R0_tos_lo);
1654   __ mov(R3, R1_tos_hi);
1655   __ pop_l(R0, R1);
1656   switch (op) {
1657     // __aeabi_XXXX_glibc: Imported code from glibc soft-fp bundle for calculation accuracy improvement. See CR 6757269.
1658     case add: __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_dadd_glibc), R0, R1, R2, R3); break;
1659     case sub: __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_dsub_glibc), R0, R1, R2, R3); break;
1660     case mul: __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_dmul), R0, R1, R2, R3); break;
1661     case div: __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_ddiv), R0, R1, R2, R3); break;
1662     case rem: __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::drem), R0, R1, R2, R3); break;
1663     default : ShouldNotReachHere();
1664   }
1665 #else
1666   const FloatRegister arg1 = D1_tmp;
1667   const FloatRegister arg2 = D0_tos;
1668 
1669   switch (op) {
1670     case add: __ pop_d(arg1); __ add_double(D0_tos, arg1, arg2); break;
1671     case sub: __ pop_d(arg1); __ sub_double(D0_tos, arg1, arg2); break;
1672     case mul: __ pop_d(arg1); __ mul_double(D0_tos, arg1, arg2); break;
1673     case div: __ pop_d(arg1); __ div_double(D0_tos, arg1, arg2); break;
1674     case rem:
1675 #ifndef __ABI_HARD__
1676       __ pop_d(arg1);
1677       __ fmrrd(R0, R1, arg1);
1678       __ fmrrd(R2, R3, arg2);
1679       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::drem), R0, R1, R2, R3);
1680       __ fmdrr(D0_tos, R0, R1);
1681 #else
1682       __ mov_double(D1, arg2);
1683       __ pop_d(D0);
1684       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::drem));
1685 #endif // !__ABI_HARD__
1686       break;
1687     default : ShouldNotReachHere();
1688   }
1689 #endif // __SOFTFP__
1690 }
1691 
1692 
1693 void TemplateTable::ineg() {
1694   transition(itos, itos);
1695   __ neg_32(R0_tos, R0_tos);
1696 }
1697 
1698 
1699 void TemplateTable::lneg() {
1700   transition(ltos, ltos);
1701   __ rsbs(R0_tos_lo, R0_tos_lo, 0);
1702   __ rsc (R1_tos_hi, R1_tos_hi, 0);
1703 }
1704 
1705 
1706 void TemplateTable::fneg() {
1707   transition(ftos, ftos);
1708 #ifdef __SOFTFP__
1709   // Invert sign bit
1710   const int sign_mask = 0x80000000;
1711   __ eor(R0_tos, R0_tos, sign_mask);
1712 #else
1713   __ neg_float(S0_tos, S0_tos);
1714 #endif // __SOFTFP__
1715 }
1716 
1717 
1718 void TemplateTable::dneg() {
1719   transition(dtos, dtos);
1720 #ifdef __SOFTFP__
1721   // Invert sign bit in the high part of the double
1722   const int sign_mask_hi = 0x80000000;
1723   __ eor(R1_tos_hi, R1_tos_hi, sign_mask_hi);
1724 #else
1725   __ neg_double(D0_tos, D0_tos);
1726 #endif // __SOFTFP__
1727 }
1728 
1729 
1730 void TemplateTable::iinc() {
1731   transition(vtos, vtos);
1732   const Register Rconst = R2_tmp;
1733   const Register Rlocal_index = R1_tmp;
1734   const Register Rval = R0_tmp;
1735 
1736   __ ldrsb(Rconst, at_bcp(2));
1737   locals_index(Rlocal_index);
1738   Address local = load_iaddress(Rlocal_index, Rtemp);
1739   __ ldr_s32(Rval, local);
1740   __ add(Rval, Rval, Rconst);
1741   __ str_32(Rval, local);
1742 }
1743 
1744 
1745 void TemplateTable::wide_iinc() {
1746   transition(vtos, vtos);
1747   const Register Rconst = R2_tmp;
1748   const Register Rlocal_index = R1_tmp;
1749   const Register Rval = R0_tmp;
1750 
1751   // get constant in Rconst
1752   __ ldrsb(R2_tmp, at_bcp(4));
1753   __ ldrb(R3_tmp, at_bcp(5));
1754   __ orr(Rconst, R3_tmp, AsmOperand(R2_tmp, lsl, 8));
1755 
1756   locals_index_wide(Rlocal_index);
1757   Address local = load_iaddress(Rlocal_index, Rtemp);
1758   __ ldr_s32(Rval, local);
1759   __ add(Rval, Rval, Rconst);
1760   __ str_32(Rval, local);
1761 }
1762 
1763 
1764 void TemplateTable::convert() {
1765   // Checking
1766 #ifdef ASSERT
1767   { TosState tos_in  = ilgl;
1768     TosState tos_out = ilgl;
1769     switch (bytecode()) {
1770       case Bytecodes::_i2l: // fall through
1771       case Bytecodes::_i2f: // fall through
1772       case Bytecodes::_i2d: // fall through
1773       case Bytecodes::_i2b: // fall through
1774       case Bytecodes::_i2c: // fall through
1775       case Bytecodes::_i2s: tos_in = itos; break;
1776       case Bytecodes::_l2i: // fall through
1777       case Bytecodes::_l2f: // fall through
1778       case Bytecodes::_l2d: tos_in = ltos; break;
1779       case Bytecodes::_f2i: // fall through
1780       case Bytecodes::_f2l: // fall through
1781       case Bytecodes::_f2d: tos_in = ftos; break;
1782       case Bytecodes::_d2i: // fall through
1783       case Bytecodes::_d2l: // fall through
1784       case Bytecodes::_d2f: tos_in = dtos; break;
1785       default             : ShouldNotReachHere();
1786     }
1787     switch (bytecode()) {
1788       case Bytecodes::_l2i: // fall through
1789       case Bytecodes::_f2i: // fall through
1790       case Bytecodes::_d2i: // fall through
1791       case Bytecodes::_i2b: // fall through
1792       case Bytecodes::_i2c: // fall through
1793       case Bytecodes::_i2s: tos_out = itos; break;
1794       case Bytecodes::_i2l: // fall through
1795       case Bytecodes::_f2l: // fall through
1796       case Bytecodes::_d2l: tos_out = ltos; break;
1797       case Bytecodes::_i2f: // fall through
1798       case Bytecodes::_l2f: // fall through
1799       case Bytecodes::_d2f: tos_out = ftos; break;
1800       case Bytecodes::_i2d: // fall through
1801       case Bytecodes::_l2d: // fall through
1802       case Bytecodes::_f2d: tos_out = dtos; break;
1803       default             : ShouldNotReachHere();
1804     }
1805     transition(tos_in, tos_out);
1806   }
1807 #endif // ASSERT
1808 
1809   // Conversion
1810   switch (bytecode()) {
1811     case Bytecodes::_i2l:
1812       __ mov(R1_tos_hi, AsmOperand(R0_tos, asr, BitsPerWord-1));
1813       break;
1814 
1815     case Bytecodes::_i2f:
1816 #ifdef __SOFTFP__
1817       __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_i2f), R0_tos);
1818 #else
1819       __ fmsr(S0_tmp, R0_tos);
1820       __ fsitos(S0_tos, S0_tmp);
1821 #endif // __SOFTFP__
1822       break;
1823 
1824     case Bytecodes::_i2d:
1825 #ifdef __SOFTFP__
1826       __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_i2d), R0_tos);
1827 #else
1828       __ fmsr(S0_tmp, R0_tos);
1829       __ fsitod(D0_tos, S0_tmp);
1830 #endif // __SOFTFP__
1831       break;
1832 
1833     case Bytecodes::_i2b:
1834       __ sign_extend(R0_tos, R0_tos, 8);
1835       break;
1836 
1837     case Bytecodes::_i2c:
1838       __ zero_extend(R0_tos, R0_tos, 16);
1839       break;
1840 
1841     case Bytecodes::_i2s:
1842       __ sign_extend(R0_tos, R0_tos, 16);
1843       break;
1844 
1845     case Bytecodes::_l2i:
1846       /* nothing to do */
1847       break;
1848 
1849     case Bytecodes::_l2f:
1850       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::l2f), R0_tos_lo, R1_tos_hi);
1851 #if !defined(__SOFTFP__) &amp;&amp; !defined(__ABI_HARD__)
1852       __ fmsr(S0_tos, R0);
1853 #endif // !__SOFTFP__ &amp;&amp; !__ABI_HARD__
1854       break;
1855 
1856     case Bytecodes::_l2d:
1857       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::l2d), R0_tos_lo, R1_tos_hi);
1858 #if !defined(__SOFTFP__) &amp;&amp; !defined(__ABI_HARD__)
1859       __ fmdrr(D0_tos, R0, R1);
1860 #endif // !__SOFTFP__ &amp;&amp; !__ABI_HARD__
1861       break;
1862 
1863     case Bytecodes::_f2i:
1864 #ifndef __SOFTFP__
1865       __ ftosizs(S0_tos, S0_tos);
1866       __ fmrs(R0_tos, S0_tos);
1867 #else
1868       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::f2i), R0_tos);
1869 #endif // !__SOFTFP__
1870       break;
1871 
1872     case Bytecodes::_f2l:
1873 #ifndef __SOFTFP__
1874       __ fmrs(R0_tos, S0_tos);
1875 #endif // !__SOFTFP__
1876       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::f2l), R0_tos);
1877       break;
1878 
1879     case Bytecodes::_f2d:
1880 #ifdef __SOFTFP__
1881       __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_f2d), R0_tos);
1882 #else
1883       __ convert_f2d(D0_tos, S0_tos);
1884 #endif // __SOFTFP__
1885       break;
1886 
1887     case Bytecodes::_d2i:
1888 #ifndef __SOFTFP__
1889       __ ftosizd(Stemp, D0);
1890       __ fmrs(R0, Stemp);
1891 #else
1892       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::d2i), R0_tos_lo, R1_tos_hi);
1893 #endif // !__SOFTFP__
1894       break;
1895 
1896     case Bytecodes::_d2l:
1897 #ifndef __SOFTFP__
1898       __ fmrrd(R0_tos_lo, R1_tos_hi, D0_tos);
1899 #endif // !__SOFTFP__
1900       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::d2l), R0_tos_lo, R1_tos_hi);
1901       break;
1902 
1903     case Bytecodes::_d2f:
1904 #ifdef __SOFTFP__
1905       __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_d2f), R0_tos_lo, R1_tos_hi);
1906 #else
1907       __ convert_d2f(S0_tos, D0_tos);
1908 #endif // __SOFTFP__
1909       break;
1910 
1911     default:
1912       ShouldNotReachHere();
1913   }
1914 }
1915 
1916 
1917 void TemplateTable::lcmp() {
1918   transition(ltos, itos);
1919   const Register arg1_lo = R2_tmp;
1920   const Register arg1_hi = R3_tmp;
1921   const Register arg2_lo = R0_tos_lo;
1922   const Register arg2_hi = R1_tos_hi;
1923   const Register res = R4_tmp;
1924 
1925   __ pop_l(arg1_lo, arg1_hi);
1926 
1927   // long compare arg1 with arg2
1928   // result is -1/0/+1 if &#39;&lt;&#39;/&#39;=&#39;/&#39;&gt;&#39;
1929   Label done;
1930 
1931   __ mov (res, 0);
1932   __ cmp (arg1_hi, arg2_hi);
1933   __ mvn (res, 0, lt);
1934   __ mov (res, 1, gt);
1935   __ b(done, ne);
1936   __ cmp (arg1_lo, arg2_lo);
1937   __ mvn (res, 0, lo);
1938   __ mov (res, 1, hi);
1939   __ bind(done);
1940   __ mov (R0_tos, res);
1941 }
1942 
1943 
1944 void TemplateTable::float_cmp(bool is_float, int unordered_result) {
1945   assert((unordered_result == 1) || (unordered_result == -1), &quot;invalid unordered result&quot;);
1946 
1947 
1948 #ifdef __SOFTFP__
1949 
1950   if (is_float) {
1951     transition(ftos, itos);
1952     const Register Rx = R0;
1953     const Register Ry = R1;
1954 
1955     __ mov(Ry, R0_tos);
1956     __ pop_i(Rx);
1957 
1958     if (unordered_result == 1) {
1959       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::fcmpg), Rx, Ry);
1960     } else {
1961       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::fcmpl), Rx, Ry);
1962     }
1963 
1964   } else {
1965 
1966     transition(dtos, itos);
1967     const Register Rx_lo = R0;
1968     const Register Rx_hi = R1;
1969     const Register Ry_lo = R2;
1970     const Register Ry_hi = R3;
1971 
1972     __ mov(Ry_lo, R0_tos_lo);
1973     __ mov(Ry_hi, R1_tos_hi);
1974     __ pop_l(Rx_lo, Rx_hi);
1975 
1976     if (unordered_result == 1) {
1977       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dcmpg), Rx_lo, Rx_hi, Ry_lo, Ry_hi);
1978     } else {
1979       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dcmpl), Rx_lo, Rx_hi, Ry_lo, Ry_hi);
1980     }
1981   }
1982 
1983 #else
1984 
1985   if (is_float) {
1986     transition(ftos, itos);
1987     __ pop_f(S1_tmp);
1988     __ fcmps(S1_tmp, S0_tos);
1989   } else {
1990     transition(dtos, itos);
1991     __ pop_d(D1_tmp);
1992     __ fcmpd(D1_tmp, D0_tos);
1993   }
1994 
1995   __ fmstat();
1996 
1997   // comparison result | flag N | flag Z | flag C | flag V
1998   // &quot;&lt;&quot;               |   1    |   0    |   0    |   0
1999   // &quot;==&quot;              |   0    |   1    |   1    |   0
2000   // &quot;&gt;&quot;               |   0    |   0    |   1    |   0
2001   // unordered         |   0    |   0    |   1    |   1
2002 
2003   if (unordered_result &lt; 0) {
2004     __ mov(R0_tos, 1);           // result ==  1 if greater
2005     __ mvn(R0_tos, 0, lt);       // result == -1 if less or unordered (N!=V)
2006   } else {
2007     __ mov(R0_tos, 1);           // result ==  1 if greater or unordered
2008     __ mvn(R0_tos, 0, mi);       // result == -1 if less (N=1)
2009   }
2010   __ mov(R0_tos, 0, eq);         // result ==  0 if equ (Z=1)
2011 #endif // __SOFTFP__
2012 }
2013 
2014 
2015 void TemplateTable::branch(bool is_jsr, bool is_wide) {
2016 
2017   const Register Rdisp = R0_tmp;
2018   const Register Rbumped_taken_count = R5_tmp;
2019 
2020   __ profile_taken_branch(R0_tmp, Rbumped_taken_count); // R0 holds updated MDP, Rbumped_taken_count holds bumped taken count
2021 
2022   const ByteSize be_offset = MethodCounters::backedge_counter_offset() +
2023                              InvocationCounter::counter_offset();
2024   const ByteSize inv_offset = MethodCounters::invocation_counter_offset() +
2025                               InvocationCounter::counter_offset();
2026   const int method_offset = frame::interpreter_frame_method_offset * wordSize;
2027 
2028   // Load up R0 with the branch displacement
2029   if (is_wide) {
2030     __ ldrsb(R0_tmp, at_bcp(1));
2031     __ ldrb(R1_tmp, at_bcp(2));
2032     __ ldrb(R2_tmp, at_bcp(3));
2033     __ ldrb(R3_tmp, at_bcp(4));
2034     __ orr(R0_tmp, R1_tmp, AsmOperand(R0_tmp, lsl, BitsPerByte));
2035     __ orr(R0_tmp, R2_tmp, AsmOperand(R0_tmp, lsl, BitsPerByte));
2036     __ orr(Rdisp, R3_tmp, AsmOperand(R0_tmp, lsl, BitsPerByte));
2037   } else {
2038     __ ldrsb(R0_tmp, at_bcp(1));
2039     __ ldrb(R1_tmp, at_bcp(2));
2040     __ orr(Rdisp, R1_tmp, AsmOperand(R0_tmp, lsl, BitsPerByte));
2041   }
2042 
2043   // Handle all the JSR stuff here, then exit.
2044   // It&#39;s much shorter and cleaner than intermingling with the
2045   // non-JSR normal-branch stuff occuring below.
2046   if (is_jsr) {
2047     // compute return address as bci in R1
2048     const Register Rret_addr = R1_tmp;
2049     assert_different_registers(Rdisp, Rret_addr, Rtemp);
2050 
2051     __ ldr(Rtemp, Address(Rmethod, Method::const_offset()));
2052     __ sub(Rret_addr, Rbcp, - (is_wide ? 5 : 3) + in_bytes(ConstMethod::codes_offset()));
2053     __ sub(Rret_addr, Rret_addr, Rtemp);
2054 
2055     // Load the next target bytecode into R3_bytecode and advance Rbcp
2056     __ ldrb(R3_bytecode, Address(Rbcp, Rdisp, lsl, 0, pre_indexed));
2057 
2058     // Push return address
2059     __ push_i(Rret_addr);
2060     // jsr returns vtos
2061     __ dispatch_only_noverify(vtos);
2062     return;
2063   }
2064 
2065   // Normal (non-jsr) branch handling
2066 
2067   // Adjust the bcp by the displacement in Rdisp and load next bytecode.
2068   __ ldrb(R3_bytecode, Address(Rbcp, Rdisp, lsl, 0, pre_indexed));
2069 
2070   assert(UseLoopCounter || !UseOnStackReplacement, &quot;on-stack-replacement requires loop counters&quot;);
2071   Label backedge_counter_overflow;
2072   Label profile_method;
2073   Label dispatch;
2074 
2075   if (UseLoopCounter) {
2076     // increment backedge counter for backward branches
2077     // Rdisp (R0): target offset
2078 
2079     const Register Rcnt = R2_tmp;
2080     const Register Rcounters = R1_tmp;
2081 
2082     // count only if backward branch
2083     __ tst(Rdisp, Rdisp);
2084     __ b(dispatch, pl);
2085 
2086     if (TieredCompilation) {
2087       Label no_mdo;
2088       int increment = InvocationCounter::count_increment;
2089       if (ProfileInterpreter) {
2090         // Are we profiling?
2091         __ ldr(Rtemp, Address(Rmethod, Method::method_data_offset()));
2092         __ cbz(Rtemp, no_mdo);
2093         // Increment the MDO backedge counter
2094         const Address mdo_backedge_counter(Rtemp, in_bytes(MethodData::backedge_counter_offset()) +
2095                                                   in_bytes(InvocationCounter::counter_offset()));
2096         const Address mask(Rtemp, in_bytes(MethodData::backedge_mask_offset()));
2097         __ increment_mask_and_jump(mdo_backedge_counter, increment, mask,
2098                                    Rcnt, R4_tmp, eq, &amp;backedge_counter_overflow);
2099         __ b(dispatch);
2100       }
2101       __ bind(no_mdo);
2102       // Increment backedge counter in MethodCounters*
2103       // Note Rbumped_taken_count is a callee saved registers for ARM32
2104       __ get_method_counters(Rmethod, Rcounters, dispatch, true /*saveRegs*/,
2105                              Rdisp, R3_bytecode,
2106                              noreg);
2107       const Address mask(Rcounters, in_bytes(MethodCounters::backedge_mask_offset()));
2108       __ increment_mask_and_jump(Address(Rcounters, be_offset), increment, mask,
2109                                  Rcnt, R4_tmp, eq, &amp;backedge_counter_overflow);
2110     } else {
2111       // Increment backedge counter in MethodCounters*
2112       __ get_method_counters(Rmethod, Rcounters, dispatch, true /*saveRegs*/,
2113                              Rdisp, R3_bytecode,
2114                              noreg);
2115       __ ldr_u32(Rtemp, Address(Rcounters, be_offset));           // load backedge counter
2116       __ add(Rtemp, Rtemp, InvocationCounter::count_increment);   // increment counter
2117       __ str_32(Rtemp, Address(Rcounters, be_offset));            // store counter
2118 
2119       __ ldr_u32(Rcnt, Address(Rcounters, inv_offset));           // load invocation counter
2120       __ bic(Rcnt, Rcnt, ~InvocationCounter::count_mask_value);  // and the status bits
2121       __ add(Rcnt, Rcnt, Rtemp);                                 // add both counters
2122 
2123       if (ProfileInterpreter) {
2124         // Test to see if we should create a method data oop
2125         const Address profile_limit(Rcounters, in_bytes(MethodCounters::interpreter_profile_limit_offset()));
2126         __ ldr_s32(Rtemp, profile_limit);
2127         __ cmp_32(Rcnt, Rtemp);
2128         __ b(dispatch, lt);
2129 
2130         // if no method data exists, go to profile method
2131         __ test_method_data_pointer(R4_tmp, profile_method);
2132 
2133         if (UseOnStackReplacement) {
2134           // check for overflow against Rbumped_taken_count, which is the MDO taken count
2135           const Address backward_branch_limit(Rcounters, in_bytes(MethodCounters::interpreter_backward_branch_limit_offset()));
2136           __ ldr_s32(Rtemp, backward_branch_limit);
2137           __ cmp(Rbumped_taken_count, Rtemp);
2138           __ b(dispatch, lo);
2139 
2140           // When ProfileInterpreter is on, the backedge_count comes from the
2141           // MethodData*, which value does not get reset on the call to
2142           // frequency_counter_overflow().  To avoid excessive calls to the overflow
2143           // routine while the method is being compiled, add a second test to make
2144           // sure the overflow function is called only once every overflow_frequency.
2145           const int overflow_frequency = 1024;
2146 
2147           // was &#39;__ andrs(...,overflow_frequency-1)&#39;, testing if lowest 10 bits are 0
2148           assert(overflow_frequency == (1 &lt;&lt; 10),&quot;shift by 22 not correct for expected frequency&quot;);
2149           __ movs(Rbumped_taken_count, AsmOperand(Rbumped_taken_count, lsl, 22));
2150 
2151           __ b(backedge_counter_overflow, eq);
2152         }
2153       } else {
2154         if (UseOnStackReplacement) {
2155           // check for overflow against Rcnt, which is the sum of the counters
2156           const Address backward_branch_limit(Rcounters, in_bytes(MethodCounters::interpreter_backward_branch_limit_offset()));
2157           __ ldr_s32(Rtemp, backward_branch_limit);
2158           __ cmp_32(Rcnt, Rtemp);
2159           __ b(backedge_counter_overflow, hs);
2160 
2161         }
2162       }
2163     }
2164     __ bind(dispatch);
2165   }
2166 
2167   if (!UseOnStackReplacement) {
2168     __ bind(backedge_counter_overflow);
2169   }
2170 
2171   // continue with the bytecode @ target
<a name="7" id="anc7"></a><span class="line-modified">2172   __ dispatch_only(vtos, true);</span>
2173 
2174   if (UseLoopCounter) {
2175     if (ProfileInterpreter) {
2176       // Out-of-line code to allocate method data oop.
2177       __ bind(profile_method);
2178 
2179       __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));
2180       __ set_method_data_pointer_for_bcp();
2181       // reload next bytecode
2182       __ ldrb(R3_bytecode, Address(Rbcp));
2183       __ b(dispatch);
2184     }
2185 
2186     if (UseOnStackReplacement) {
2187       // invocation counter overflow
2188       __ bind(backedge_counter_overflow);
2189 
2190       __ sub(R1, Rbcp, Rdisp);                   // branch bcp
2191       call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::frequency_counter_overflow), R1);
2192 
2193       // R0: osr nmethod (osr ok) or NULL (osr not possible)
2194       const Register Rnmethod = R0;
2195 
2196       __ ldrb(R3_bytecode, Address(Rbcp));       // reload next bytecode
2197 
2198       __ cbz(Rnmethod, dispatch);                // test result, no osr if null
2199 
2200       // nmethod may have been invalidated (VM may block upon call_VM return)
2201       __ ldrb(R1_tmp, Address(Rnmethod, nmethod::state_offset()));
2202       __ cmp(R1_tmp, nmethod::in_use);
2203       __ b(dispatch, ne);
2204 
2205       // We have the address of an on stack replacement routine in Rnmethod,
2206       // We need to prepare to execute the OSR method. First we must
2207       // migrate the locals and monitors off of the stack.
2208 
2209       __ mov(Rtmp_save0, Rnmethod);                      // save the nmethod
2210 
2211       call_VM(noreg, CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_begin));
2212 
2213       // R0 is OSR buffer
2214 
2215       __ ldr(R1_tmp, Address(Rtmp_save0, nmethod::osr_entry_point_offset()));
2216       __ ldr(Rtemp, Address(FP, frame::interpreter_frame_sender_sp_offset * wordSize));
2217 
2218       __ ldmia(FP, RegisterSet(FP) | RegisterSet(LR));
2219       __ bic(SP, Rtemp, StackAlignmentInBytes - 1);     // Remove frame and align stack
2220 
2221       __ jump(R1_tmp);
2222     }
2223   }
2224 }
2225 
2226 
2227 void TemplateTable::if_0cmp(Condition cc) {
2228   transition(itos, vtos);
2229   // assume branch is more often taken than not (loops use backward branches)
2230   Label not_taken;
2231   __ cmp_32(R0_tos, 0);
2232   __ b(not_taken, convNegCond(cc));
2233   branch(false, false);
2234   __ bind(not_taken);
2235   __ profile_not_taken_branch(R0_tmp);
2236 }
2237 
2238 
2239 void TemplateTable::if_icmp(Condition cc) {
2240   transition(itos, vtos);
2241   // assume branch is more often taken than not (loops use backward branches)
2242   Label not_taken;
2243   __ pop_i(R1_tmp);
2244   __ cmp_32(R1_tmp, R0_tos);
2245   __ b(not_taken, convNegCond(cc));
2246   branch(false, false);
2247   __ bind(not_taken);
2248   __ profile_not_taken_branch(R0_tmp);
2249 }
2250 
2251 
2252 void TemplateTable::if_nullcmp(Condition cc) {
2253   transition(atos, vtos);
2254   assert(cc == equal || cc == not_equal, &quot;invalid condition&quot;);
2255 
2256   // assume branch is more often taken than not (loops use backward branches)
2257   Label not_taken;
2258   if (cc == equal) {
2259     __ cbnz(R0_tos, not_taken);
2260   } else {
2261     __ cbz(R0_tos, not_taken);
2262   }
2263   branch(false, false);
2264   __ bind(not_taken);
2265   __ profile_not_taken_branch(R0_tmp);
2266 }
2267 
2268 
2269 void TemplateTable::if_acmp(Condition cc) {
2270   transition(atos, vtos);
2271   // assume branch is more often taken than not (loops use backward branches)
2272   Label not_taken;
2273   __ pop_ptr(R1_tmp);
2274   __ cmpoop(R1_tmp, R0_tos);
2275   __ b(not_taken, convNegCond(cc));
2276   branch(false, false);
2277   __ bind(not_taken);
2278   __ profile_not_taken_branch(R0_tmp);
2279 }
2280 
2281 
2282 void TemplateTable::ret() {
2283   transition(vtos, vtos);
2284   const Register Rlocal_index = R1_tmp;
2285   const Register Rret_bci = Rtmp_save0; // R4/R19
2286 
2287   locals_index(Rlocal_index);
2288   Address local = load_iaddress(Rlocal_index, Rtemp);
2289   __ ldr_s32(Rret_bci, local);          // get return bci, compute return bcp
2290   __ profile_ret(Rtmp_save1, Rret_bci);
2291   __ ldr(Rtemp, Address(Rmethod, Method::const_offset()));
2292   __ add(Rtemp, Rtemp, in_bytes(ConstMethod::codes_offset()));
2293   __ add(Rbcp, Rtemp, Rret_bci);
2294   __ dispatch_next(vtos);
2295 }
2296 
2297 
2298 void TemplateTable::wide_ret() {
2299   transition(vtos, vtos);
2300   const Register Rlocal_index = R1_tmp;
2301   const Register Rret_bci = Rtmp_save0; // R4/R19
2302 
2303   locals_index_wide(Rlocal_index);
2304   Address local = load_iaddress(Rlocal_index, Rtemp);
2305   __ ldr_s32(Rret_bci, local);               // get return bci, compute return bcp
2306   __ profile_ret(Rtmp_save1, Rret_bci);
2307   __ ldr(Rtemp, Address(Rmethod, Method::const_offset()));
2308   __ add(Rtemp, Rtemp, in_bytes(ConstMethod::codes_offset()));
2309   __ add(Rbcp, Rtemp, Rret_bci);
2310   __ dispatch_next(vtos);
2311 }
2312 
2313 
2314 void TemplateTable::tableswitch() {
2315   transition(itos, vtos);
2316 
2317   const Register Rindex  = R0_tos;
2318   const Register Rtemp2  = R1_tmp;
2319   const Register Rabcp   = R2_tmp;  // aligned bcp
2320   const Register Rlow    = R3_tmp;
2321   const Register Rhigh   = R4_tmp;
2322   const Register Roffset = R5_tmp;
2323 
2324   // align bcp
2325   __ add(Rtemp, Rbcp, 1 + (2*BytesPerInt-1));
2326   __ align_reg(Rabcp, Rtemp, BytesPerInt);
2327 
2328   // load lo &amp; hi
2329   __ ldmia(Rabcp, RegisterSet(Rlow) | RegisterSet(Rhigh), writeback);
2330   __ byteswap_u32(Rlow, Rtemp, Rtemp2);
2331   __ byteswap_u32(Rhigh, Rtemp, Rtemp2);
2332 
2333   // compare index with high bound
2334   __ cmp_32(Rhigh, Rindex);
2335 
2336 
2337   // if Rindex &lt;= Rhigh then calculate index in table (Rindex - Rlow)
2338   __ subs(Rindex, Rindex, Rlow, ge);
2339 
2340   // if Rindex &lt;= Rhigh and (Rindex - Rlow) &gt;= 0
2341   // (&quot;ge&quot; status accumulated from cmp and subs instructions) then load
2342   // offset from table, otherwise load offset for default case
2343 
2344   if(ProfileInterpreter) {
2345     Label default_case, continue_execution;
2346 
2347     __ b(default_case, lt);
2348     __ ldr(Roffset, Address(Rabcp, Rindex, lsl, LogBytesPerInt));
2349     __ profile_switch_case(Rabcp, Rindex, Rtemp2, R0_tmp);
2350     __ b(continue_execution);
2351 
2352     __ bind(default_case);
2353     __ profile_switch_default(R0_tmp);
2354     __ ldr(Roffset, Address(Rabcp, -3 * BytesPerInt));
2355 
2356     __ bind(continue_execution);
2357   } else {
2358     __ ldr(Roffset, Address(Rabcp, -3 * BytesPerInt), lt);
2359     __ ldr(Roffset, Address(Rabcp, Rindex, lsl, LogBytesPerInt), ge);
2360   }
2361 
2362   __ byteswap_u32(Roffset, Rtemp, Rtemp2);
2363 
2364   // load the next bytecode to R3_bytecode and advance Rbcp
2365   __ ldrb(R3_bytecode, Address(Rbcp, Roffset, lsl, 0, pre_indexed));
<a name="8" id="anc8"></a><span class="line-modified">2366   __ dispatch_only(vtos, true);</span>
2367 
2368 }
2369 
2370 
2371 void TemplateTable::lookupswitch() {
2372   transition(itos, itos);
2373   __ stop(&quot;lookupswitch bytecode should have been rewritten&quot;);
2374 }
2375 
2376 
2377 void TemplateTable::fast_linearswitch() {
2378   transition(itos, vtos);
2379   Label loop, found, default_case, continue_execution;
2380 
2381   const Register Rkey     = R0_tos;
2382   const Register Rabcp    = R2_tmp;  // aligned bcp
2383   const Register Rdefault = R3_tmp;
2384   const Register Rcount   = R4_tmp;
2385   const Register Roffset  = R5_tmp;
2386 
2387   // bswap Rkey, so we can avoid bswapping the table entries
2388   __ byteswap_u32(Rkey, R1_tmp, Rtemp);
2389 
2390   // align bcp
2391   __ add(Rtemp, Rbcp, 1 + (BytesPerInt-1));
2392   __ align_reg(Rabcp, Rtemp, BytesPerInt);
2393 
2394   // load default &amp; counter
2395   __ ldmia(Rabcp, RegisterSet(Rdefault) | RegisterSet(Rcount), writeback);
2396   __ byteswap_u32(Rcount, R1_tmp, Rtemp);
2397 
2398   __ cmp_32(Rcount, 0);
2399   __ ldr(Rtemp, Address(Rabcp, 2*BytesPerInt, post_indexed), ne);
2400   __ b(default_case, eq);
2401 
2402   // table search
2403   __ bind(loop);
2404   __ cmp_32(Rtemp, Rkey);
2405   __ b(found, eq);
2406   __ subs(Rcount, Rcount, 1);
2407   __ ldr(Rtemp, Address(Rabcp, 2*BytesPerInt, post_indexed), ne);
2408   __ b(loop, ne);
2409 
2410   // default case
2411   __ bind(default_case);
2412   __ profile_switch_default(R0_tmp);
2413   __ mov(Roffset, Rdefault);
2414   __ b(continue_execution);
2415 
2416   // entry found -&gt; get offset
2417   __ bind(found);
2418   // Rabcp is already incremented and points to the next entry
2419   __ ldr_s32(Roffset, Address(Rabcp, -BytesPerInt));
2420   if (ProfileInterpreter) {
2421     // Calculate index of the selected case.
2422     assert_different_registers(Roffset, Rcount, Rtemp, R0_tmp, R1_tmp, R2_tmp);
2423 
2424     // align bcp
2425     __ add(Rtemp, Rbcp, 1 + (BytesPerInt-1));
2426     __ align_reg(R2_tmp, Rtemp, BytesPerInt);
2427 
2428     // load number of cases
2429     __ ldr_u32(R2_tmp, Address(R2_tmp, BytesPerInt));
2430     __ byteswap_u32(R2_tmp, R1_tmp, Rtemp);
2431 
2432     // Selected index = &lt;number of cases&gt; - &lt;current loop count&gt;
2433     __ sub(R1_tmp, R2_tmp, Rcount);
2434     __ profile_switch_case(R0_tmp, R1_tmp, Rtemp, R1_tmp);
2435   }
2436 
2437   // continue execution
2438   __ bind(continue_execution);
2439   __ byteswap_u32(Roffset, R1_tmp, Rtemp);
2440 
2441   // load the next bytecode to R3_bytecode and advance Rbcp
2442   __ ldrb(R3_bytecode, Address(Rbcp, Roffset, lsl, 0, pre_indexed));
<a name="9" id="anc9"></a><span class="line-modified">2443   __ dispatch_only(vtos, true);</span>
2444 }
2445 
2446 
2447 void TemplateTable::fast_binaryswitch() {
2448   transition(itos, vtos);
2449   // Implementation using the following core algorithm:
2450   //
2451   // int binary_search(int key, LookupswitchPair* array, int n) {
2452   //   // Binary search according to &quot;Methodik des Programmierens&quot; by
2453   //   // Edsger W. Dijkstra and W.H.J. Feijen, Addison Wesley Germany 1985.
2454   //   int i = 0;
2455   //   int j = n;
2456   //   while (i+1 &lt; j) {
2457   //     // invariant P: 0 &lt;= i &lt; j &lt;= n and (a[i] &lt;= key &lt; a[j] or Q)
2458   //     // with      Q: for all i: 0 &lt;= i &lt; n: key &lt; a[i]
2459   //     // where a stands for the array and assuming that the (inexisting)
2460   //     // element a[n] is infinitely big.
2461   //     int h = (i + j) &gt;&gt; 1;
2462   //     // i &lt; h &lt; j
2463   //     if (key &lt; array[h].fast_match()) {
2464   //       j = h;
2465   //     } else {
2466   //       i = h;
2467   //     }
2468   //   }
2469   //   // R: a[i] &lt;= key &lt; a[i+1] or Q
2470   //   // (i.e., if key is within array, i is the correct index)
2471   //   return i;
2472   // }
2473 
2474   // register allocation
2475   const Register key    = R0_tos;                // already set (tosca)
2476   const Register array  = R1_tmp;
2477   const Register i      = R2_tmp;
2478   const Register j      = R3_tmp;
2479   const Register h      = R4_tmp;
2480   const Register val    = R5_tmp;
2481   const Register temp1  = Rtemp;
2482   const Register temp2  = LR_tmp;
2483   const Register offset = R3_tmp;
2484 
2485   // set &#39;array&#39; = aligned bcp + 2 ints
2486   __ add(temp1, Rbcp, 1 + (BytesPerInt-1) + 2*BytesPerInt);
2487   __ align_reg(array, temp1, BytesPerInt);
2488 
2489   // initialize i &amp; j
2490   __ mov(i, 0);                                  // i = 0;
2491   __ ldr_s32(j, Address(array, -BytesPerInt));   // j = length(array);
2492   // Convert j into native byteordering
2493   __ byteswap_u32(j, temp1, temp2);
2494 
2495   // and start
2496   Label entry;
2497   __ b(entry);
2498 
2499   // binary search loop
2500   { Label loop;
2501     __ bind(loop);
2502     // int h = (i + j) &gt;&gt; 1;
2503     __ add(h, i, j);                             // h = i + j;
2504     __ logical_shift_right(h, h, 1);             // h = (i + j) &gt;&gt; 1;
2505     // if (key &lt; array[h].fast_match()) {
2506     //   j = h;
2507     // } else {
2508     //   i = h;
2509     // }
2510     __ ldr_s32(val, Address(array, h, lsl, 1+LogBytesPerInt));
2511     // Convert array[h].match to native byte-ordering before compare
2512     __ byteswap_u32(val, temp1, temp2);
2513     __ cmp_32(key, val);
2514     __ mov(j, h, lt);   // j = h if (key &lt;  array[h].fast_match())
2515     __ mov(i, h, ge);   // i = h if (key &gt;= array[h].fast_match())
2516     // while (i+1 &lt; j)
2517     __ bind(entry);
2518     __ add(temp1, i, 1);                             // i+1
2519     __ cmp(temp1, j);                                // i+1 &lt; j
2520     __ b(loop, lt);
2521   }
2522 
2523   // end of binary search, result index is i (must check again!)
2524   Label default_case;
2525   // Convert array[i].match to native byte-ordering before compare
2526   __ ldr_s32(val, Address(array, i, lsl, 1+LogBytesPerInt));
2527   __ byteswap_u32(val, temp1, temp2);
2528   __ cmp_32(key, val);
2529   __ b(default_case, ne);
2530 
2531   // entry found
2532   __ add(temp1, array, AsmOperand(i, lsl, 1+LogBytesPerInt));
2533   __ ldr_s32(offset, Address(temp1, 1*BytesPerInt));
2534   __ profile_switch_case(R0, i, R1, i);
2535   __ byteswap_u32(offset, temp1, temp2);
2536   __ ldrb(R3_bytecode, Address(Rbcp, offset, lsl, 0, pre_indexed));
<a name="10" id="anc10"></a><span class="line-modified">2537   __ dispatch_only(vtos, true);</span>
2538 
2539   // default case
2540   __ bind(default_case);
2541   __ profile_switch_default(R0);
2542   __ ldr_s32(offset, Address(array, -2*BytesPerInt));
2543   __ byteswap_u32(offset, temp1, temp2);
2544   __ ldrb(R3_bytecode, Address(Rbcp, offset, lsl, 0, pre_indexed));
<a name="11" id="anc11"></a><span class="line-modified">2545   __ dispatch_only(vtos, true);</span>
2546 }
2547 
2548 
2549 void TemplateTable::_return(TosState state) {
2550   transition(state, state);
2551   assert(_desc-&gt;calls_vm(), &quot;inconsistent calls_vm information&quot;); // call in remove_activation
2552 
2553   if (_desc-&gt;bytecode() == Bytecodes::_return_register_finalizer) {
2554     Label skip_register_finalizer;
2555     assert(state == vtos, &quot;only valid state&quot;);
2556     __ ldr(R1, aaddress(0));
2557     __ load_klass(Rtemp, R1);
2558     __ ldr_u32(Rtemp, Address(Rtemp, Klass::access_flags_offset()));
2559     __ tbz(Rtemp, exact_log2(JVM_ACC_HAS_FINALIZER), skip_register_finalizer);
2560 
2561     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::register_finalizer), R1);
2562 
2563     __ bind(skip_register_finalizer);
2564   }
2565 
2566   // Narrow result if state is itos but result type is smaller.
2567   // Need to narrow in the return bytecode rather than in generate_return_entry
2568   // since compiled code callers expect the result to already be narrowed.
2569   if (state == itos) {
2570     __ narrow(R0_tos);
2571   }
2572   __ remove_activation(state, LR);
2573 
2574   __ interp_verify_oop(R0_tos, state, __FILE__, __LINE__);
2575 
2576   // According to interpreter calling conventions, result is returned in R0/R1,
2577   // so ftos (S0) and dtos (D0) are moved to R0/R1.
2578   // This conversion should be done after remove_activation, as it uses
2579   // push(state) &amp; pop(state) to preserve return value.
2580   __ convert_tos_to_retval(state);
2581 
2582   __ ret();
2583 
2584   __ nop(); // to avoid filling CPU pipeline with invalid instructions
2585   __ nop();
2586 }
2587 
2588 
2589 // ----------------------------------------------------------------------------
2590 // Volatile variables demand their effects be made known to all CPU&#39;s in
2591 // order.  Store buffers on most chips allow reads &amp; writes to reorder; the
2592 // JMM&#39;s ReadAfterWrite.java test fails in -Xint mode without some kind of
2593 // memory barrier (i.e., it&#39;s not sufficient that the interpreter does not
2594 // reorder volatile references, the hardware also must not reorder them).
2595 //
2596 // According to the new Java Memory Model (JMM):
2597 // (1) All volatiles are serialized wrt to each other.
2598 // ALSO reads &amp; writes act as aquire &amp; release, so:
2599 // (2) A read cannot let unrelated NON-volatile memory refs that happen after
2600 // the read float up to before the read.  It&#39;s OK for non-volatile memory refs
2601 // that happen before the volatile read to float down below it.
2602 // (3) Similar a volatile write cannot let unrelated NON-volatile memory refs
2603 // that happen BEFORE the write float down to after the write.  It&#39;s OK for
2604 // non-volatile memory refs that happen after the volatile write to float up
2605 // before it.
2606 //
2607 // We only put in barriers around volatile refs (they are expensive), not
2608 // _between_ memory refs (that would require us to track the flavor of the
2609 // previous memory refs).  Requirements (2) and (3) require some barriers
2610 // before volatile stores and after volatile loads.  These nearly cover
2611 // requirement (1) but miss the volatile-store-volatile-load case.  This final
2612 // case is placed after volatile-stores although it could just as well go
2613 // before volatile-loads.
2614 void TemplateTable::volatile_barrier(MacroAssembler::Membar_mask_bits order_constraint,
2615                                      Register tmp,
2616                                      bool preserve_flags,
2617                                      Register load_tgt) {
2618   __ membar(order_constraint, tmp, preserve_flags, load_tgt);
2619 }
2620 
2621 // Blows all volatile registers: R0-R3, Rtemp, LR.
2622 void TemplateTable::resolve_cache_and_index(int byte_no,
2623                                             Register Rcache,
2624                                             Register Rindex,
2625                                             size_t index_size) {
2626   assert_different_registers(Rcache, Rindex, Rtemp);
2627 
2628   Label resolved;
2629   Bytecodes::Code code = bytecode();
2630   switch (code) {
2631   case Bytecodes::_nofast_getfield: code = Bytecodes::_getfield; break;
2632   case Bytecodes::_nofast_putfield: code = Bytecodes::_putfield; break;
2633   default: break;
2634   }
2635 
2636   assert(byte_no == f1_byte || byte_no == f2_byte, &quot;byte_no out of range&quot;);
2637   __ get_cache_and_index_and_bytecode_at_bcp(Rcache, Rindex, Rtemp, byte_no, 1, index_size);
2638   __ cmp(Rtemp, code);  // have we resolved this bytecode?
2639   __ b(resolved, eq);
2640 
2641   // resolve first time through
2642   address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache);
2643   __ mov(R1, code);
2644   __ call_VM(noreg, entry, R1);
2645   // Update registers with resolved info
2646   __ get_cache_and_index_at_bcp(Rcache, Rindex, 1, index_size);
2647   __ bind(resolved);
2648 }
2649 
2650 
2651 // The Rcache and Rindex registers must be set before call
2652 void TemplateTable::load_field_cp_cache_entry(Register Rcache,
2653                                               Register Rindex,
2654                                               Register Roffset,
2655                                               Register Rflags,
2656                                               Register Robj,
2657                                               bool is_static = false) {
2658 
2659   assert_different_registers(Rcache, Rindex, Rtemp);
2660   assert_different_registers(Roffset, Rflags, Robj, Rtemp);
2661 
2662   ByteSize cp_base_offset = ConstantPoolCache::base_offset();
2663 
2664   __ add(Rtemp, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));
2665 
2666   // Field offset
2667   __ ldr(Roffset, Address(Rtemp,
2668            cp_base_offset + ConstantPoolCacheEntry::f2_offset()));
2669 
2670   // Flags
2671   __ ldr_u32(Rflags, Address(Rtemp,
2672            cp_base_offset + ConstantPoolCacheEntry::flags_offset()));
2673 
2674   if (is_static) {
2675     __ ldr(Robj, Address(Rtemp,
2676              cp_base_offset + ConstantPoolCacheEntry::f1_offset()));
2677     const int mirror_offset = in_bytes(Klass::java_mirror_offset());
2678     __ ldr(Robj, Address(Robj, mirror_offset));
2679     __ resolve_oop_handle(Robj);
2680   }
2681 }
2682 
2683 
2684 // Blows all volatile registers: R0-R3, Rtemp, LR.
2685 void TemplateTable::load_invoke_cp_cache_entry(int byte_no,
2686                                                Register method,
2687                                                Register itable_index,
2688                                                Register flags,
2689                                                bool is_invokevirtual,
2690                                                bool is_invokevfinal/*unused*/,
2691                                                bool is_invokedynamic) {
2692   // setup registers
2693   const Register cache = R2_tmp;
2694   const Register index = R3_tmp;
2695   const Register temp_reg = Rtemp;
2696   assert_different_registers(cache, index, temp_reg);
2697   assert_different_registers(method, itable_index, temp_reg);
2698 
2699   // determine constant pool cache field offsets
2700   assert(is_invokevirtual == (byte_no == f2_byte), &quot;is_invokevirtual flag redundant&quot;);
2701   const int method_offset = in_bytes(
2702     ConstantPoolCache::base_offset() +
2703       ((byte_no == f2_byte)
2704        ? ConstantPoolCacheEntry::f2_offset()
2705        : ConstantPoolCacheEntry::f1_offset()
2706       )
2707     );
2708   const int flags_offset = in_bytes(ConstantPoolCache::base_offset() +
2709                                     ConstantPoolCacheEntry::flags_offset());
2710   // access constant pool cache fields
2711   const int index_offset = in_bytes(ConstantPoolCache::base_offset() +
2712                                     ConstantPoolCacheEntry::f2_offset());
2713 
2714   size_t index_size = (is_invokedynamic ? sizeof(u4) : sizeof(u2));
2715   resolve_cache_and_index(byte_no, cache, index, index_size);
2716     __ add(temp_reg, cache, AsmOperand(index, lsl, LogBytesPerWord));
2717     __ ldr(method, Address(temp_reg, method_offset));
2718 
2719   if (itable_index != noreg) {
2720     __ ldr(itable_index, Address(temp_reg, index_offset));
2721   }
2722   __ ldr_u32(flags, Address(temp_reg, flags_offset));
2723 }
2724 
2725 
2726 // The registers cache and index expected to be set before call, and should not be Rtemp.
2727 // Blows volatile registers R0-R3, Rtemp, LR,
2728 // except cache and index registers which are preserved.
2729 void TemplateTable::jvmti_post_field_access(Register Rcache,
2730                                             Register Rindex,
2731                                             bool is_static,
2732                                             bool has_tos) {
2733   assert_different_registers(Rcache, Rindex, Rtemp);
2734 
2735   if (__ can_post_field_access()) {
2736     // Check to see if a field access watch has been set before we take
2737     // the time to call into the VM.
2738 
2739     Label Lcontinue;
2740 
2741     __ ldr_global_s32(Rtemp, (address)JvmtiExport::get_field_access_count_addr());
2742     __ cbz(Rtemp, Lcontinue);
2743 
2744     // cache entry pointer
2745     __ add(R2, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));
2746     __ add(R2, R2, in_bytes(ConstantPoolCache::base_offset()));
2747     if (is_static) {
2748       __ mov(R1, 0);        // NULL object reference
2749     } else {
2750       __ pop(atos);         // Get the object
2751       __ mov(R1, R0_tos);
2752       __ verify_oop(R1);
2753       __ push(atos);        // Restore stack state
2754     }
2755     // R1: object pointer or NULL
2756     // R2: cache entry pointer
2757     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_access),
2758                R1, R2);
2759     __ get_cache_and_index_at_bcp(Rcache, Rindex, 1);
2760 
2761     __ bind(Lcontinue);
2762   }
2763 }
2764 
2765 
2766 void TemplateTable::pop_and_check_object(Register r) {
2767   __ pop_ptr(r);
2768   __ null_check(r, Rtemp);  // for field access must check obj.
2769   __ verify_oop(r);
2770 }
2771 
2772 
2773 void TemplateTable::getfield_or_static(int byte_no, bool is_static, RewriteControl rc) {
2774   transition(vtos, vtos);
2775 
2776   const Register Roffset  = R2_tmp;
2777   const Register Robj     = R3_tmp;
2778   const Register Rcache   = R4_tmp;
2779   const Register Rflagsav = Rtmp_save0;  // R4/R19
2780   const Register Rindex   = R5_tmp;
2781   const Register Rflags   = R5_tmp;
2782 
2783   resolve_cache_and_index(byte_no, Rcache, Rindex, sizeof(u2));
2784   jvmti_post_field_access(Rcache, Rindex, is_static, false);
2785   load_field_cp_cache_entry(Rcache, Rindex, Roffset, Rflags, Robj, is_static);
2786 
2787   __ mov(Rflagsav, Rflags);
2788 
2789   if (!is_static) pop_and_check_object(Robj);
2790 
2791   Label Done, Lint, Ltable, shouldNotReachHere;
2792   Label Lbtos, Lztos, Lctos, Lstos, Litos, Lltos, Lftos, Ldtos, Latos;
2793 
2794   // compute type
2795   __ logical_shift_right(Rflags, Rflags, ConstantPoolCacheEntry::tos_state_shift);
2796   // Make sure we don&#39;t need to mask flags after the above shift
2797   ConstantPoolCacheEntry::verify_tos_state_shift();
2798 
2799   // There are actually two versions of implementation of getfield/getstatic:
2800   //
2801   // 1) Table switch using add(PC,...) instruction (fast_version)
2802   // 2) Table switch using ldr(PC,...) instruction
2803   //
2804   // First version requires fixed size of code block for each case and
2805   // can not be used in RewriteBytecodes and VerifyOops
2806   // modes.
2807 
2808   // Size of fixed size code block for fast_version
2809   const int log_max_block_size = 3;
2810   const int max_block_size = 1 &lt;&lt; log_max_block_size;
2811 
2812   // Decide if fast version is enabled
2813   bool fast_version = (is_static || !RewriteBytecodes) &amp;&amp; !VerifyOops;
2814 
2815   // On 32-bit ARM atos and itos cases can be merged only for fast version, because
2816   // atos requires additional processing in slow version.
2817   bool atos_merged_with_itos = fast_version;
2818 
2819   assert(number_of_states == 10, &quot;number of tos states should be equal to 9&quot;);
2820 
2821   __ cmp(Rflags, itos);
2822   if(atos_merged_with_itos) {
2823     __ cmp(Rflags, atos, ne);
2824   }
2825 
2826   // table switch by type
2827   if(fast_version) {
2828     __ add(PC, PC, AsmOperand(Rflags, lsl, log_max_block_size + Assembler::LogInstructionSize), ne);
2829   } else {
2830     __ ldr(PC, Address(PC, Rflags, lsl, LogBytesPerWord), ne);
2831   }
2832 
2833   // jump to itos/atos case
2834   __ b(Lint);
2835 
2836   // table with addresses for slow version
2837   if (fast_version) {
2838     // nothing to do
2839   } else  {
2840     __ bind(Ltable);
2841     __ emit_address(Lbtos);
2842     __ emit_address(Lztos);
2843     __ emit_address(Lctos);
2844     __ emit_address(Lstos);
2845     __ emit_address(Litos);
2846     __ emit_address(Lltos);
2847     __ emit_address(Lftos);
2848     __ emit_address(Ldtos);
2849     __ emit_address(Latos);
2850   }
2851 
2852 #ifdef ASSERT
2853   int seq = 0;
2854 #endif
2855   // btos
2856   {
2857     assert(btos == seq++, &quot;btos has unexpected value&quot;);
2858     FixedSizeCodeBlock btos_block(_masm, max_block_size, fast_version);
2859     __ bind(Lbtos);
2860     __ access_load_at(T_BYTE, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg);
2861     __ push(btos);
2862     // Rewrite bytecode to be faster
2863     if (!is_static &amp;&amp; rc == may_rewrite) {
2864       patch_bytecode(Bytecodes::_fast_bgetfield, R0_tmp, Rtemp);
2865     }
2866     __ b(Done);
2867   }
2868 
2869   // ztos (same as btos for getfield)
2870   {
2871     assert(ztos == seq++, &quot;btos has unexpected value&quot;);
2872     FixedSizeCodeBlock ztos_block(_masm, max_block_size, fast_version);
2873     __ bind(Lztos);
2874     __ access_load_at(T_BOOLEAN, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg);
2875     __ push(ztos);
2876     // Rewrite bytecode to be faster (use btos fast getfield)
2877     if (!is_static &amp;&amp; rc == may_rewrite) {
2878       patch_bytecode(Bytecodes::_fast_bgetfield, R0_tmp, Rtemp);
2879     }
2880     __ b(Done);
2881   }
2882 
2883   // ctos
2884   {
2885     assert(ctos == seq++, &quot;ctos has unexpected value&quot;);
2886     FixedSizeCodeBlock ctos_block(_masm, max_block_size, fast_version);
2887     __ bind(Lctos);
2888     __ access_load_at(T_CHAR, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg);
2889     __ push(ctos);
2890     if (!is_static &amp;&amp; rc == may_rewrite) {
2891       patch_bytecode(Bytecodes::_fast_cgetfield, R0_tmp, Rtemp);
2892     }
2893     __ b(Done);
2894   }
2895 
2896   // stos
2897   {
2898     assert(stos == seq++, &quot;stos has unexpected value&quot;);
2899     FixedSizeCodeBlock stos_block(_masm, max_block_size, fast_version);
2900     __ bind(Lstos);
2901     __ access_load_at(T_SHORT, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg);
2902     __ push(stos);
2903     if (!is_static &amp;&amp; rc == may_rewrite) {
2904       patch_bytecode(Bytecodes::_fast_sgetfield, R0_tmp, Rtemp);
2905     }
2906     __ b(Done);
2907   }
2908 
2909   // itos
2910   {
2911     assert(itos == seq++, &quot;itos has unexpected value&quot;);
2912     FixedSizeCodeBlock itos_block(_masm, max_block_size, fast_version);
2913     __ bind(Litos);
2914     __ b(shouldNotReachHere);
2915   }
2916 
2917   // ltos
2918   {
2919     assert(ltos == seq++, &quot;ltos has unexpected value&quot;);
2920     FixedSizeCodeBlock ltos_block(_masm, max_block_size, fast_version);
2921     __ bind(Lltos);
2922     __ access_load_at(T_LONG, IN_HEAP, Address(Robj, Roffset), noreg /* ltos */, noreg, noreg, noreg);
2923     __ push(ltos);
2924     if (!is_static &amp;&amp; rc == may_rewrite) {
2925       patch_bytecode(Bytecodes::_fast_lgetfield, R0_tmp, Rtemp);
2926     }
2927     __ b(Done);
2928   }
2929 
2930   // ftos
2931   {
2932     assert(ftos == seq++, &quot;ftos has unexpected value&quot;);
2933     FixedSizeCodeBlock ftos_block(_masm, max_block_size, fast_version);
2934     __ bind(Lftos);
2935     // floats and ints are placed on stack in same way, so
2936     // we can use push(itos) to transfer value without using VFP
2937     __ access_load_at(T_INT, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg);
2938     __ push(itos);
2939     if (!is_static &amp;&amp; rc == may_rewrite) {
2940       patch_bytecode(Bytecodes::_fast_fgetfield, R0_tmp, Rtemp);
2941     }
2942     __ b(Done);
2943   }
2944 
2945   // dtos
2946   {
2947     assert(dtos == seq++, &quot;dtos has unexpected value&quot;);
2948     FixedSizeCodeBlock dtos_block(_masm, max_block_size, fast_version);
2949     __ bind(Ldtos);
2950     // doubles and longs are placed on stack in the same way, so
2951     // we can use push(ltos) to transfer value without using VFP
2952     __ access_load_at(T_LONG, IN_HEAP, Address(Robj, Roffset), noreg /* ltos */, noreg, noreg, noreg);
2953     __ push(ltos);
2954     if (!is_static &amp;&amp; rc == may_rewrite) {
2955       patch_bytecode(Bytecodes::_fast_dgetfield, R0_tmp, Rtemp);
2956     }
2957     __ b(Done);
2958   }
2959 
2960   // atos
2961   {
2962     assert(atos == seq++, &quot;atos has unexpected value&quot;);
2963 
2964     // atos case for slow version on 32-bit ARM
2965     if(!atos_merged_with_itos) {
2966       __ bind(Latos);
2967       do_oop_load(_masm, R0_tos, Address(Robj, Roffset));
2968       __ push(atos);
2969       // Rewrite bytecode to be faster
2970       if (!is_static &amp;&amp; rc == may_rewrite) {
2971         patch_bytecode(Bytecodes::_fast_agetfield, R0_tmp, Rtemp);
2972       }
2973       __ b(Done);
2974     }
2975   }
2976 
2977   assert(vtos == seq++, &quot;vtos has unexpected value&quot;);
2978 
2979   __ bind(shouldNotReachHere);
2980   __ should_not_reach_here();
2981 
2982   // itos and atos cases are frequent so it makes sense to move them out of table switch
2983   // atos case can be merged with itos case (and thus moved out of table switch) on 32-bit ARM, fast version only
2984 
2985   __ bind(Lint);
2986   __ access_load_at(T_INT, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg);
2987   __ push(itos);
2988   // Rewrite bytecode to be faster
2989   if (!is_static &amp;&amp; rc == may_rewrite) {
2990     patch_bytecode(Bytecodes::_fast_igetfield, R0_tmp, Rtemp);
2991   }
2992 
2993   __ bind(Done);
2994 
2995   // Check for volatile field
2996   Label notVolatile;
2997   __ tbz(Rflagsav, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
2998 
2999   volatile_barrier(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), Rtemp);
3000 
3001   __ bind(notVolatile);
3002 }
3003 
3004 void TemplateTable::getfield(int byte_no) {
3005   getfield_or_static(byte_no, false);
3006 }
3007 
3008 void TemplateTable::nofast_getfield(int byte_no) {
3009   getfield_or_static(byte_no, false, may_not_rewrite);
3010 }
3011 
3012 void TemplateTable::getstatic(int byte_no) {
3013   getfield_or_static(byte_no, true);
3014 }
3015 
3016 
3017 // The registers cache and index expected to be set before call, and should not be R1 or Rtemp.
3018 // Blows volatile registers R0-R3, Rtemp, LR,
3019 // except cache and index registers which are preserved.
3020 void TemplateTable::jvmti_post_field_mod(Register Rcache, Register Rindex, bool is_static) {
3021   ByteSize cp_base_offset = ConstantPoolCache::base_offset();
3022   assert_different_registers(Rcache, Rindex, R1, Rtemp);
3023 
3024   if (__ can_post_field_modification()) {
3025     // Check to see if a field modification watch has been set before we take
3026     // the time to call into the VM.
3027     Label Lcontinue;
3028 
3029     __ ldr_global_s32(Rtemp, (address)JvmtiExport::get_field_modification_count_addr());
3030     __ cbz(Rtemp, Lcontinue);
3031 
3032     if (is_static) {
3033       // Life is simple.  Null out the object pointer.
3034       __ mov(R1, 0);
3035     } else {
3036       // Life is harder. The stack holds the value on top, followed by the object.
3037       // We don&#39;t know the size of the value, though; it could be one or two words
3038       // depending on its type. As a result, we must find the type to determine where
3039       // the object is.
3040 
3041       __ add(Rtemp, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));
3042       __ ldr_u32(Rtemp, Address(Rtemp, cp_base_offset + ConstantPoolCacheEntry::flags_offset()));
3043 
3044       __ logical_shift_right(Rtemp, Rtemp, ConstantPoolCacheEntry::tos_state_shift);
3045       // Make sure we don&#39;t need to mask Rtemp after the above shift
3046       ConstantPoolCacheEntry::verify_tos_state_shift();
3047 
3048       __ cmp(Rtemp, ltos);
3049       __ cond_cmp(Rtemp, dtos, ne);
3050       // two word value (ltos/dtos)
3051       __ ldr(R1, Address(SP, Interpreter::expr_offset_in_bytes(2)), eq);
3052 
3053       // one word value (not ltos, dtos)
3054       __ ldr(R1, Address(SP, Interpreter::expr_offset_in_bytes(1)), ne);
3055     }
3056 
3057     // cache entry pointer
3058     __ add(R2, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));
3059     __ add(R2, R2, in_bytes(cp_base_offset));
3060 
3061     // object (tos)
3062     __ mov(R3, Rstack_top);
3063 
3064     // R1: object pointer set up above (NULL if static)
3065     // R2: cache entry pointer
3066     // R3: value object on the stack
3067     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_modification),
3068                R1, R2, R3);
3069     __ get_cache_and_index_at_bcp(Rcache, Rindex, 1);
3070 
3071     __ bind(Lcontinue);
3072   }
3073 }
3074 
3075 
3076 void TemplateTable::putfield_or_static(int byte_no, bool is_static, RewriteControl rc) {
3077   transition(vtos, vtos);
3078 
3079   const Register Roffset  = R2_tmp;
3080   const Register Robj     = R3_tmp;
3081   const Register Rcache   = R4_tmp;
3082   const Register Rflagsav = Rtmp_save0;  // R4/R19
3083   const Register Rindex   = R5_tmp;
3084   const Register Rflags   = R5_tmp;
3085 
3086   resolve_cache_and_index(byte_no, Rcache, Rindex, sizeof(u2));
3087   jvmti_post_field_mod(Rcache, Rindex, is_static);
3088   load_field_cp_cache_entry(Rcache, Rindex, Roffset, Rflags, Robj, is_static);
3089 
3090   // Check for volatile field
3091   Label notVolatile;
3092   __ mov(Rflagsav, Rflags);
3093   __ tbz(Rflagsav, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3094 
3095   volatile_barrier(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);
3096 
3097   __ bind(notVolatile);
3098 
3099   Label Done, Lint, shouldNotReachHere;
3100   Label Ltable, Lbtos, Lztos, Lctos, Lstos, Litos, Lltos, Lftos, Ldtos, Latos;
3101 
3102   // compute type
3103   __ logical_shift_right(Rflags, Rflags, ConstantPoolCacheEntry::tos_state_shift);
3104   // Make sure we don&#39;t need to mask flags after the above shift
3105   ConstantPoolCacheEntry::verify_tos_state_shift();
3106 
3107   // There are actually two versions of implementation of putfield/putstatic:
3108   //
3109   // 32-bit ARM:
3110   // 1) Table switch using add(PC,...) instruction (fast_version)
3111   // 2) Table switch using ldr(PC,...) instruction
3112   //
3113   // First version requires fixed size of code block for each case and
3114   // can not be used in RewriteBytecodes and VerifyOops
3115   // modes.
3116 
3117   // Size of fixed size code block for fast_version (in instructions)
3118   const int log_max_block_size = 3;
3119   const int max_block_size = 1 &lt;&lt; log_max_block_size;
3120 
3121   // Decide if fast version is enabled
3122   bool fast_version = (is_static || !RewriteBytecodes) &amp;&amp; !VerifyOops;
3123 
3124   assert(number_of_states == 10, &quot;number of tos states should be equal to 9&quot;);
3125 
3126   // itos case is frequent and is moved outside table switch
3127   __ cmp(Rflags, itos);
3128 
3129   // table switch by type
3130   if (fast_version) {
3131     __ add(PC, PC, AsmOperand(Rflags, lsl, log_max_block_size + Assembler::LogInstructionSize), ne);
3132   } else  {
3133     __ ldr(PC, Address(PC, Rflags, lsl, LogBytesPerWord), ne);
3134   }
3135 
3136   // jump to itos case
3137   __ b(Lint);
3138 
3139   // table with addresses for slow version
3140   if (fast_version) {
3141     // nothing to do
3142   } else  {
3143     __ bind(Ltable);
3144     __ emit_address(Lbtos);
3145     __ emit_address(Lztos);
3146     __ emit_address(Lctos);
3147     __ emit_address(Lstos);
3148     __ emit_address(Litos);
3149     __ emit_address(Lltos);
3150     __ emit_address(Lftos);
3151     __ emit_address(Ldtos);
3152     __ emit_address(Latos);
3153   }
3154 
3155 #ifdef ASSERT
3156   int seq = 0;
3157 #endif
3158   // btos
3159   {
3160     assert(btos == seq++, &quot;btos has unexpected value&quot;);
3161     FixedSizeCodeBlock btos_block(_masm, max_block_size, fast_version);
3162     __ bind(Lbtos);
3163     __ pop(btos);
3164     if (!is_static) pop_and_check_object(Robj);
3165     __ access_store_at(T_BYTE, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg, false);
3166     if (!is_static &amp;&amp; rc == may_rewrite) {
3167       patch_bytecode(Bytecodes::_fast_bputfield, R0_tmp, Rtemp, true, byte_no);
3168     }
3169     __ b(Done);
3170   }
3171 
3172   // ztos
3173   {
3174     assert(ztos == seq++, &quot;ztos has unexpected value&quot;);
3175     FixedSizeCodeBlock ztos_block(_masm, max_block_size, fast_version);
3176     __ bind(Lztos);
3177     __ pop(ztos);
3178     if (!is_static) pop_and_check_object(Robj);
3179     __ access_store_at(T_BOOLEAN, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg, false);
3180     if (!is_static &amp;&amp; rc == may_rewrite) {
3181       patch_bytecode(Bytecodes::_fast_zputfield, R0_tmp, Rtemp, true, byte_no);
3182     }
3183     __ b(Done);
3184   }
3185 
3186   // ctos
3187   {
3188     assert(ctos == seq++, &quot;ctos has unexpected value&quot;);
3189     FixedSizeCodeBlock ctos_block(_masm, max_block_size, fast_version);
3190     __ bind(Lctos);
3191     __ pop(ctos);
3192     if (!is_static) pop_and_check_object(Robj);
3193     __ access_store_at(T_CHAR, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg, false);
3194     if (!is_static &amp;&amp; rc == may_rewrite) {
3195       patch_bytecode(Bytecodes::_fast_cputfield, R0_tmp, Rtemp, true, byte_no);
3196     }
3197     __ b(Done);
3198   }
3199 
3200   // stos
3201   {
3202     assert(stos == seq++, &quot;stos has unexpected value&quot;);
3203     FixedSizeCodeBlock stos_block(_masm, max_block_size, fast_version);
3204     __ bind(Lstos);
3205     __ pop(stos);
3206     if (!is_static) pop_and_check_object(Robj);
3207     __ access_store_at(T_SHORT, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg, false);
3208     if (!is_static &amp;&amp; rc == may_rewrite) {
3209       patch_bytecode(Bytecodes::_fast_sputfield, R0_tmp, Rtemp, true, byte_no);
3210     }
3211     __ b(Done);
3212   }
3213 
3214   // itos
3215   {
3216     assert(itos == seq++, &quot;itos has unexpected value&quot;);
3217     FixedSizeCodeBlock itos_block(_masm, max_block_size, fast_version);
3218     __ bind(Litos);
3219     __ b(shouldNotReachHere);
3220   }
3221 
3222   // ltos
3223   {
3224     assert(ltos == seq++, &quot;ltos has unexpected value&quot;);
3225     FixedSizeCodeBlock ltos_block(_masm, max_block_size, fast_version);
3226     __ bind(Lltos);
3227     __ pop(ltos);
3228     if (!is_static) pop_and_check_object(Robj);
3229     __ access_store_at(T_LONG, IN_HEAP, Address(Robj, Roffset), noreg /* ltos */, noreg, noreg, noreg, false);
3230     if (!is_static &amp;&amp; rc == may_rewrite) {
3231       patch_bytecode(Bytecodes::_fast_lputfield, R0_tmp, Rtemp, true, byte_no);
3232     }
3233     __ b(Done);
3234   }
3235 
3236   // ftos
3237   {
3238     assert(ftos == seq++, &quot;ftos has unexpected value&quot;);
3239     FixedSizeCodeBlock ftos_block(_masm, max_block_size, fast_version);
3240     __ bind(Lftos);
3241     // floats and ints are placed on stack in the same way, so
3242     // we can use pop(itos) to transfer value without using VFP
3243     __ pop(itos);
3244     if (!is_static) pop_and_check_object(Robj);
3245     __ access_store_at(T_INT, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg, false);
3246     if (!is_static &amp;&amp; rc == may_rewrite) {
3247       patch_bytecode(Bytecodes::_fast_fputfield, R0_tmp, Rtemp, true, byte_no);
3248     }
3249     __ b(Done);
3250   }
3251 
3252   // dtos
3253   {
3254     assert(dtos == seq++, &quot;dtos has unexpected value&quot;);
3255     FixedSizeCodeBlock dtos_block(_masm, max_block_size, fast_version);
3256     __ bind(Ldtos);
3257     // doubles and longs are placed on stack in the same way, so
3258     // we can use pop(ltos) to transfer value without using VFP
3259     __ pop(ltos);
3260     if (!is_static) pop_and_check_object(Robj);
3261     __ access_store_at(T_LONG, IN_HEAP, Address(Robj, Roffset), noreg /* ltos */, noreg, noreg, noreg, false);
3262     if (!is_static &amp;&amp; rc == may_rewrite) {
3263       patch_bytecode(Bytecodes::_fast_dputfield, R0_tmp, Rtemp, true, byte_no);
3264     }
3265     __ b(Done);
3266   }
3267 
3268   // atos
3269   {
3270     assert(atos == seq++, &quot;dtos has unexpected value&quot;);
3271     __ bind(Latos);
3272     __ pop(atos);
3273     if (!is_static) pop_and_check_object(Robj);
3274     // Store into the field
3275     do_oop_store(_masm, Address(Robj, Roffset), R0_tos, Rtemp, R1_tmp, R5_tmp, false);
3276     if (!is_static &amp;&amp; rc == may_rewrite) {
3277       patch_bytecode(Bytecodes::_fast_aputfield, R0_tmp, Rtemp, true, byte_no);
3278     }
3279     __ b(Done);
3280   }
3281 
3282   __ bind(shouldNotReachHere);
3283   __ should_not_reach_here();
3284 
3285   // itos case is frequent and is moved outside table switch
3286   __ bind(Lint);
3287   __ pop(itos);
3288   if (!is_static) pop_and_check_object(Robj);
3289   __ access_store_at(T_INT, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg, false);
3290   if (!is_static &amp;&amp; rc == may_rewrite) {
3291     patch_bytecode(Bytecodes::_fast_iputfield, R0_tmp, Rtemp, true, byte_no);
3292   }
3293 
3294   __ bind(Done);
3295 
3296   Label notVolatile2;
3297   if (is_static) {
3298     // Just check for volatile. Memory barrier for static final field
3299     // is handled by class initialization.
3300     __ tbz(Rflagsav, ConstantPoolCacheEntry::is_volatile_shift, notVolatile2);
3301     volatile_barrier(MacroAssembler::StoreLoad, Rtemp);
3302     __ bind(notVolatile2);
3303   } else {
3304     // Check for volatile field and final field
3305     Label skipMembar;
3306 
3307     __ tst(Rflagsav, 1 &lt;&lt; ConstantPoolCacheEntry::is_volatile_shift |
3308            1 &lt;&lt; ConstantPoolCacheEntry::is_final_shift);
3309     __ b(skipMembar, eq);
3310 
3311     __ tbz(Rflagsav, ConstantPoolCacheEntry::is_volatile_shift, notVolatile2);
3312 
3313     // StoreLoad barrier after volatile field write
3314     volatile_barrier(MacroAssembler::StoreLoad, Rtemp);
3315     __ b(skipMembar);
3316 
3317     // StoreStore barrier after final field write
3318     __ bind(notVolatile2);
3319     volatile_barrier(MacroAssembler::StoreStore, Rtemp);
3320 
3321     __ bind(skipMembar);
3322   }
3323 }
3324 
3325 void TemplateTable::putfield(int byte_no) {
3326   putfield_or_static(byte_no, false);
3327 }
3328 
3329 void TemplateTable::nofast_putfield(int byte_no) {
3330   putfield_or_static(byte_no, false, may_not_rewrite);
3331 }
3332 
3333 void TemplateTable::putstatic(int byte_no) {
3334   putfield_or_static(byte_no, true);
3335 }
3336 
3337 
3338 void TemplateTable::jvmti_post_fast_field_mod() {
3339   // This version of jvmti_post_fast_field_mod() is not used on ARM
3340   Unimplemented();
3341 }
3342 
3343 // Blows volatile registers R0-R3, Rtemp, LR,
3344 // but preserves tosca with the given state.
3345 void TemplateTable::jvmti_post_fast_field_mod(TosState state) {
3346   if (__ can_post_field_modification()) {
3347     // Check to see if a field modification watch has been set before we take
3348     // the time to call into the VM.
3349     Label done;
3350 
3351     __ ldr_global_s32(R2, (address)JvmtiExport::get_field_modification_count_addr());
3352     __ cbz(R2, done);
3353 
3354     __ pop_ptr(R3);               // copy the object pointer from tos
3355     __ verify_oop(R3);
3356     __ push_ptr(R3);              // put the object pointer back on tos
3357 
3358     __ push(state);               // save value on the stack
3359 
3360     // access constant pool cache entry
3361     __ get_cache_entry_pointer_at_bcp(R2, R1, 1);
3362 
3363     __ mov(R1, R3);
3364     assert(Interpreter::expr_offset_in_bytes(0) == 0, &quot;adjust this code&quot;);
3365     __ mov(R3, Rstack_top); // put tos addr into R3
3366 
3367     // R1: object pointer copied above
3368     // R2: cache entry pointer
3369     // R3: jvalue object on the stack
3370     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_modification), R1, R2, R3);
3371 
3372     __ pop(state);                // restore value
3373 
3374     __ bind(done);
3375   }
3376 }
3377 
3378 
3379 void TemplateTable::fast_storefield(TosState state) {
3380   transition(state, vtos);
3381 
3382   ByteSize base = ConstantPoolCache::base_offset();
3383 
3384   jvmti_post_fast_field_mod(state);
3385 
3386   const Register Rcache  = R2_tmp;
3387   const Register Rindex  = R3_tmp;
3388   const Register Roffset = R3_tmp;
3389   const Register Rflags  = Rtmp_save0; // R4/R19
3390   const Register Robj    = R5_tmp;
3391 
3392   // access constant pool cache
3393   __ get_cache_and_index_at_bcp(Rcache, Rindex, 1);
3394 
3395   __ add(Rcache, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));
3396 
3397   // load flags to test volatile
3398   __ ldr_u32(Rflags, Address(Rcache, base + ConstantPoolCacheEntry::flags_offset()));
3399 
3400   // replace index with field offset from cache entry
3401   __ ldr(Roffset, Address(Rcache, base + ConstantPoolCacheEntry::f2_offset()));
3402 
3403   // Check for volatile store
3404   Label notVolatile;
3405   __ tbz(Rflags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3406 
3407   volatile_barrier(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);
3408 
3409   __ bind(notVolatile);
3410 
3411   // Get object from stack
3412   pop_and_check_object(Robj);
3413 
3414   Address addr = Address(Robj, Roffset);
3415   // access field
3416   switch (bytecode()) {
3417     case Bytecodes::_fast_zputfield:
3418       __ access_store_at(T_BOOLEAN, IN_HEAP, addr, R0_tos, noreg, noreg, noreg, false);
3419       break;
3420     case Bytecodes::_fast_bputfield:
3421       __ access_store_at(T_BYTE, IN_HEAP, addr, R0_tos, noreg, noreg, noreg, false);
3422       break;
3423     case Bytecodes::_fast_sputfield:
3424       __ access_store_at(T_SHORT, IN_HEAP, addr, R0_tos, noreg, noreg, noreg, false);
3425       break;
3426     case Bytecodes::_fast_cputfield:
3427       __ access_store_at(T_CHAR, IN_HEAP, addr, R0_tos, noreg, noreg, noreg,false);
3428       break;
3429     case Bytecodes::_fast_iputfield:
3430       __ access_store_at(T_INT, IN_HEAP, addr, R0_tos, noreg, noreg, noreg, false);
3431       break;
3432     case Bytecodes::_fast_lputfield:
3433       __ access_store_at(T_LONG, IN_HEAP, addr, noreg, noreg, noreg, noreg, false);
3434       break;
3435     case Bytecodes::_fast_fputfield:
3436       __ access_store_at(T_FLOAT, IN_HEAP, addr, noreg, noreg, noreg, noreg, false);
3437       break;
3438     case Bytecodes::_fast_dputfield:
3439       __ access_store_at(T_DOUBLE, IN_HEAP, addr, noreg, noreg, noreg, noreg, false);
3440       break;
3441     case Bytecodes::_fast_aputfield:
3442       do_oop_store(_masm, addr, R0_tos, Rtemp, R1_tmp, R2_tmp, false);
3443       break;
3444 
3445     default:
3446       ShouldNotReachHere();
3447   }
3448 
3449   Label notVolatile2;
3450   Label skipMembar;
3451   __ tst(Rflags, 1 &lt;&lt; ConstantPoolCacheEntry::is_volatile_shift |
3452          1 &lt;&lt; ConstantPoolCacheEntry::is_final_shift);
3453   __ b(skipMembar, eq);
3454 
3455   __ tbz(Rflags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile2);
3456 
3457   // StoreLoad barrier after volatile field write
3458   volatile_barrier(MacroAssembler::StoreLoad, Rtemp);
3459   __ b(skipMembar);
3460 
3461   // StoreStore barrier after final field write
3462   __ bind(notVolatile2);
3463   volatile_barrier(MacroAssembler::StoreStore, Rtemp);
3464 
3465   __ bind(skipMembar);
3466 }
3467 
3468 void TemplateTable::fast_accessfield(TosState state) {
3469   transition(atos, state);
3470 
3471   // do the JVMTI work here to avoid disturbing the register state below
3472   if (__ can_post_field_access()) {
3473     // Check to see if a field access watch has been set before we take
3474     // the time to call into the VM.
3475     Label done;
3476     __ ldr_global_s32(R2, (address) JvmtiExport::get_field_access_count_addr());
3477     __ cbz(R2, done);
3478     // access constant pool cache entry
3479     __ get_cache_entry_pointer_at_bcp(R2, R1, 1);
3480     __ push_ptr(R0_tos);  // save object pointer before call_VM() clobbers it
3481     __ verify_oop(R0_tos);
3482     __ mov(R1, R0_tos);
3483     // R1: object pointer copied above
3484     // R2: cache entry pointer
3485     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_access), R1, R2);
3486     __ pop_ptr(R0_tos);   // restore object pointer
3487 
3488     __ bind(done);
3489   }
3490 
3491   const Register Robj    = R0_tos;
3492   const Register Rcache  = R2_tmp;
3493   const Register Rflags  = R2_tmp;
3494   const Register Rindex  = R3_tmp;
3495   const Register Roffset = R3_tmp;
3496 
3497   // access constant pool cache
3498   __ get_cache_and_index_at_bcp(Rcache, Rindex, 1);
3499   // replace index with field offset from cache entry
3500   __ add(Rtemp, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));
3501   __ ldr(Roffset, Address(Rtemp, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f2_offset()));
3502 
3503   // load flags to test volatile
3504   __ ldr_u32(Rflags, Address(Rtemp, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()));
3505 
3506   __ verify_oop(Robj);
3507   __ null_check(Robj, Rtemp);
3508 
3509   Address addr = Address(Robj, Roffset);
3510   // access field
3511   switch (bytecode()) {
3512     case Bytecodes::_fast_bgetfield:
3513       __ access_load_at(T_BYTE, IN_HEAP, addr, R0_tos, noreg, noreg, noreg);
3514       break;
3515     case Bytecodes::_fast_sgetfield:
3516       __ access_load_at(T_SHORT, IN_HEAP, addr, R0_tos, noreg, noreg, noreg);
3517       break;
3518     case Bytecodes::_fast_cgetfield:
3519       __ access_load_at(T_CHAR, IN_HEAP, addr, R0_tos, noreg, noreg, noreg);
3520       break;
3521     case Bytecodes::_fast_igetfield:
3522       __ access_load_at(T_INT, IN_HEAP, addr, R0_tos, noreg, noreg, noreg);
3523       break;
3524     case Bytecodes::_fast_lgetfield:
3525       __ access_load_at(T_LONG, IN_HEAP, addr, noreg, noreg, noreg, noreg);
3526       break;
3527     case Bytecodes::_fast_fgetfield:
3528       __ access_load_at(T_FLOAT, IN_HEAP, addr, noreg, noreg, noreg, noreg);
3529       break;
3530     case Bytecodes::_fast_dgetfield:
3531       __ access_load_at(T_DOUBLE, IN_HEAP, addr, noreg, noreg, noreg, noreg);
3532       break;
3533     case Bytecodes::_fast_agetfield:
3534       do_oop_load(_masm, R0_tos, addr);
3535       __ verify_oop(R0_tos);
3536       break;
3537     default:
3538       ShouldNotReachHere();
3539   }
3540 
3541   // Check for volatile load
3542   Label notVolatile;
3543   __ tbz(Rflags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3544 
3545   volatile_barrier(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), Rtemp);
3546 
3547   __ bind(notVolatile);
3548 }
3549 
3550 
3551 void TemplateTable::fast_xaccess(TosState state) {
3552   transition(vtos, state);
3553 
3554   const Register Robj = R1_tmp;
3555   const Register Rcache = R2_tmp;
3556   const Register Rindex = R3_tmp;
3557   const Register Roffset = R3_tmp;
3558   const Register Rflags = R4_tmp;
3559   Label done;
3560 
3561   // get receiver
3562   __ ldr(Robj, aaddress(0));
3563 
3564   // access constant pool cache
3565   __ get_cache_and_index_at_bcp(Rcache, Rindex, 2);
3566   __ add(Rtemp, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));
3567   __ ldr(Roffset, Address(Rtemp, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f2_offset()));
3568 
3569   // load flags to test volatile
3570   __ ldr_u32(Rflags, Address(Rtemp, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()));
3571 
3572   // make sure exception is reported in correct bcp range (getfield is next instruction)
3573   __ add(Rbcp, Rbcp, 1);
3574   __ null_check(Robj, Rtemp);
3575   __ sub(Rbcp, Rbcp, 1);
3576 
3577 
3578   if (state == itos) {
3579     __ access_load_at(T_INT, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg);
3580   } else if (state == atos) {
3581     do_oop_load(_masm, R0_tos, Address(Robj, Roffset));
3582     __ verify_oop(R0_tos);
3583   } else if (state == ftos) {
3584 #ifdef __SOFTFP__
3585     __ ldr(R0_tos, Address(Robj, Roffset));
3586 #else
3587     __ access_load_at(T_FLOAT, IN_HEAP, Address(Robj, Roffset), noreg /* ftos */, noreg, noreg, noreg);
3588 #endif // __SOFTFP__
3589   } else {
3590     ShouldNotReachHere();
3591   }
3592 
3593   // Check for volatile load
3594   Label notVolatile;
3595   __ tbz(Rflags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3596 
3597   volatile_barrier(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), Rtemp);
3598 
3599   __ bind(notVolatile);
3600 
3601   __ bind(done);
3602 }
3603 
3604 
3605 
3606 //----------------------------------------------------------------------------------------------------
3607 // Calls
3608 
3609 void TemplateTable::count_calls(Register method, Register temp) {
3610   // implemented elsewhere
3611   ShouldNotReachHere();
3612 }
3613 
3614 
3615 void TemplateTable::prepare_invoke(int byte_no,
3616                                    Register method,  // linked method (or i-klass)
3617                                    Register index,   // itable index, MethodType, etc.
3618                                    Register recv,    // if caller wants to see it
3619                                    Register flags    // if caller wants to test it
3620                                    ) {
3621   // determine flags
3622   const Bytecodes::Code code = bytecode();
3623   const bool is_invokeinterface  = code == Bytecodes::_invokeinterface;
3624   const bool is_invokedynamic    = code == Bytecodes::_invokedynamic;
3625   const bool is_invokehandle     = code == Bytecodes::_invokehandle;
3626   const bool is_invokevirtual    = code == Bytecodes::_invokevirtual;
3627   const bool is_invokespecial    = code == Bytecodes::_invokespecial;
3628   const bool load_receiver       = (recv != noreg);
3629   assert(load_receiver == (code != Bytecodes::_invokestatic &amp;&amp; code != Bytecodes::_invokedynamic), &quot;&quot;);
3630   assert(recv  == noreg || recv  == R2, &quot;&quot;);
3631   assert(flags == noreg || flags == R3, &quot;&quot;);
3632 
3633   // setup registers &amp; access constant pool cache
3634   if (recv  == noreg)  recv  = R2;
3635   if (flags == noreg)  flags = R3;
3636   const Register temp = Rtemp;
3637   const Register ret_type = R1_tmp;
3638   assert_different_registers(method, index, flags, recv, LR, ret_type, temp);
3639 
3640   // save &#39;interpreter return address&#39;
3641   __ save_bcp();
3642 
3643   load_invoke_cp_cache_entry(byte_no, method, index, flags, is_invokevirtual, false, is_invokedynamic);
3644 
3645   // maybe push extra argument
3646   if (is_invokedynamic || is_invokehandle) {
3647     Label L_no_push;
3648     __ tbz(flags, ConstantPoolCacheEntry::has_appendix_shift, L_no_push);
3649     __ mov(temp, index);
3650     __ load_resolved_reference_at_index(index, temp);
3651     __ verify_oop(index);
3652     __ push_ptr(index);  // push appendix (MethodType, CallSite, etc.)
3653     __ bind(L_no_push);
3654   }
3655 
3656   // load receiver if needed (after extra argument is pushed so parameter size is correct)
3657   if (load_receiver) {
3658     __ andr(temp, flags, (uintx)ConstantPoolCacheEntry::parameter_size_mask);  // get parameter size
3659     Address recv_addr = __ receiver_argument_address(Rstack_top, temp, recv);
3660     __ ldr(recv, recv_addr);
3661     __ verify_oop(recv);
3662   }
3663 
3664   // compute return type
3665   __ logical_shift_right(ret_type, flags, ConstantPoolCacheEntry::tos_state_shift);
3666   // Make sure we don&#39;t need to mask flags after the above shift
3667   ConstantPoolCacheEntry::verify_tos_state_shift();
3668   // load return address
3669   { const address table = (address) Interpreter::invoke_return_entry_table_for(code);
3670     __ mov_slow(temp, table);
3671     __ ldr(LR, Address::indexed_ptr(temp, ret_type));
3672   }
3673 }
3674 
3675 
3676 void TemplateTable::invokevirtual_helper(Register index,
3677                                          Register recv,
3678                                          Register flags) {
3679 
3680   const Register recv_klass = R2_tmp;
3681 
3682   assert_different_registers(index, recv, flags, Rtemp);
3683   assert_different_registers(index, recv_klass, R0_tmp, Rtemp);
3684 
3685   // Test for an invoke of a final method
3686   Label notFinal;
3687   __ tbz(flags, ConstantPoolCacheEntry::is_vfinal_shift, notFinal);
3688 
3689   assert(index == Rmethod, &quot;Method* must be Rmethod, for interpreter calling convention&quot;);
3690 
3691   // do the call - the index is actually the method to call
3692 
3693   // It&#39;s final, need a null check here!
3694   __ null_check(recv, Rtemp);
3695 
3696   // profile this call
3697   __ profile_final_call(R0_tmp);
3698 
3699   __ jump_from_interpreted(Rmethod);
3700 
3701   __ bind(notFinal);
3702 
3703   // get receiver klass
3704   __ null_check(recv, Rtemp, oopDesc::klass_offset_in_bytes());
3705   __ load_klass(recv_klass, recv);
3706 
3707   // profile this call
3708   __ profile_virtual_call(R0_tmp, recv_klass);
3709 
3710   // get target Method* &amp; entry point
3711   const int base = in_bytes(Klass::vtable_start_offset());
3712   assert(vtableEntry::size() == 1, &quot;adjust the scaling in the code below&quot;);
3713   __ add(Rtemp, recv_klass, AsmOperand(index, lsl, LogHeapWordSize));
3714   __ ldr(Rmethod, Address(Rtemp, base + vtableEntry::method_offset_in_bytes()));
3715   __ jump_from_interpreted(Rmethod);
3716 }
3717 
3718 void TemplateTable::invokevirtual(int byte_no) {
3719   transition(vtos, vtos);
3720   assert(byte_no == f2_byte, &quot;use this argument&quot;);
3721 
3722   const Register Rrecv  = R2_tmp;
3723   const Register Rflags = R3_tmp;
3724 
3725   prepare_invoke(byte_no, Rmethod, noreg, Rrecv, Rflags);
3726 
3727   // Rmethod: index
3728   // Rrecv:   receiver
3729   // Rflags:  flags
3730   // LR:      return address
3731 
3732   invokevirtual_helper(Rmethod, Rrecv, Rflags);
3733 }
3734 
3735 
3736 void TemplateTable::invokespecial(int byte_no) {
3737   transition(vtos, vtos);
3738   assert(byte_no == f1_byte, &quot;use this argument&quot;);
3739   const Register Rrecv  = R2_tmp;
3740   prepare_invoke(byte_no, Rmethod, noreg, Rrecv);
3741   __ verify_oop(Rrecv);
3742   __ null_check(Rrecv, Rtemp);
3743   // do the call
3744   __ profile_call(Rrecv);
3745   __ jump_from_interpreted(Rmethod);
3746 }
3747 
3748 
3749 void TemplateTable::invokestatic(int byte_no) {
3750   transition(vtos, vtos);
3751   assert(byte_no == f1_byte, &quot;use this argument&quot;);
3752   prepare_invoke(byte_no, Rmethod);
3753   // do the call
3754   __ profile_call(R2_tmp);
3755   __ jump_from_interpreted(Rmethod);
3756 }
3757 
3758 
3759 void TemplateTable::fast_invokevfinal(int byte_no) {
3760   transition(vtos, vtos);
3761   assert(byte_no == f2_byte, &quot;use this argument&quot;);
3762   __ stop(&quot;fast_invokevfinal is not used on ARM&quot;);
3763 }
3764 
3765 
3766 void TemplateTable::invokeinterface(int byte_no) {
3767   transition(vtos, vtos);
3768   assert(byte_no == f1_byte, &quot;use this argument&quot;);
3769 
3770   const Register Ritable = R1_tmp;
3771   const Register Rrecv   = R2_tmp;
3772   const Register Rinterf = R5_tmp;
3773   const Register Rindex  = R4_tmp;
3774   const Register Rflags  = R3_tmp;
3775   const Register Rklass  = R2_tmp; // Note! Same register with Rrecv
3776 
3777   prepare_invoke(byte_no, Rinterf, Rmethod, Rrecv, Rflags);
3778 
3779   // First check for Object case, then private interface method,
3780   // then regular interface method.
3781 
3782   // Special case of invokeinterface called for virtual method of
3783   // java.lang.Object.  See cpCache.cpp for details.
3784   Label notObjectMethod;
3785   __ tbz(Rflags, ConstantPoolCacheEntry::is_forced_virtual_shift, notObjectMethod);
3786   invokevirtual_helper(Rmethod, Rrecv, Rflags);
3787   __ bind(notObjectMethod);
3788 
3789   // Get receiver klass into Rklass - also a null check
3790   __ load_klass(Rklass, Rrecv);
3791 
3792   // Check for private method invocation - indicated by vfinal
3793   Label no_such_interface;
3794 
3795   Label notVFinal;
3796   __ tbz(Rflags, ConstantPoolCacheEntry::is_vfinal_shift, notVFinal);
3797 
3798   Label subtype;
3799   __ check_klass_subtype(Rklass, Rinterf, R1_tmp, R3_tmp, noreg, subtype);
3800   // If we get here the typecheck failed
3801   __ b(no_such_interface);
3802   __ bind(subtype);
3803 
3804   // do the call
3805   __ profile_final_call(R0_tmp);
3806   __ jump_from_interpreted(Rmethod);
3807 
3808   __ bind(notVFinal);
3809 
3810   // Receiver subtype check against REFC.
3811   __ lookup_interface_method(// inputs: rec. class, interface
3812                              Rklass, Rinterf, noreg,
3813                              // outputs:  scan temp. reg1, scan temp. reg2
3814                              noreg, Ritable, Rtemp,
3815                              no_such_interface);
3816 
3817   // profile this call
3818   __ profile_virtual_call(R0_tmp, Rklass);
3819 
3820   // Get declaring interface class from method
3821   __ ldr(Rtemp, Address(Rmethod, Method::const_offset()));
3822   __ ldr(Rtemp, Address(Rtemp, ConstMethod::constants_offset()));
3823   __ ldr(Rinterf, Address(Rtemp, ConstantPool::pool_holder_offset_in_bytes()));
3824 
3825   // Get itable index from method
3826   __ ldr_s32(Rtemp, Address(Rmethod, Method::itable_index_offset()));
3827   __ add(Rtemp, Rtemp, (-Method::itable_index_max)); // small negative constant is too large for an immediate on arm32
3828   __ neg(Rindex, Rtemp);
3829 
3830   __ lookup_interface_method(// inputs: rec. class, interface
3831                              Rklass, Rinterf, Rindex,
3832                              // outputs:  scan temp. reg1, scan temp. reg2
3833                              Rmethod, Ritable, Rtemp,
3834                              no_such_interface);
3835 
3836   // Rmethod: Method* to call
3837 
3838   // Check for abstract method error
3839   // Note: This should be done more efficiently via a throw_abstract_method_error
3840   //       interpreter entry point and a conditional jump to it in case of a null
3841   //       method.
3842   { Label L;
3843     __ cbnz(Rmethod, L);
3844     // throw exception
3845     // note: must restore interpreter registers to canonical
3846     //       state for exception handling to work correctly!
3847     __ restore_method();
3848     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_AbstractMethodError));
3849     // the call_VM checks for exception, so we should never return here.
3850     __ should_not_reach_here();
3851     __ bind(L);
3852   }
3853 
3854   // do the call
3855   __ jump_from_interpreted(Rmethod);
3856 
3857   // throw exception
3858   __ bind(no_such_interface);
3859   __ restore_method();
3860   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_IncompatibleClassChangeError));
3861   // the call_VM checks for exception, so we should never return here.
3862   __ should_not_reach_here();
3863 }
3864 
3865 void TemplateTable::invokehandle(int byte_no) {
3866   transition(vtos, vtos);
3867 
3868   const Register Rrecv  = R2_tmp;
3869   const Register Rmtype = R4_tmp;
3870   const Register R5_method = R5_tmp;  // can&#39;t reuse Rmethod!
3871 
3872   prepare_invoke(byte_no, R5_method, Rmtype, Rrecv);
3873   __ null_check(Rrecv, Rtemp);
3874 
3875   // Rmtype:  MethodType object (from cpool-&gt;resolved_references[f1], if necessary)
3876   // Rmethod: MH.invokeExact_MT method (from f2)
3877 
3878   // Note:  Rmtype is already pushed (if necessary) by prepare_invoke
3879 
3880   // do the call
3881   __ profile_final_call(R3_tmp);  // FIXME: profile the LambdaForm also
3882   __ mov(Rmethod, R5_method);
3883   __ jump_from_interpreted(Rmethod);
3884 }
3885 
3886 void TemplateTable::invokedynamic(int byte_no) {
3887   transition(vtos, vtos);
3888 
3889   const Register Rcallsite = R4_tmp;
3890   const Register R5_method = R5_tmp;  // can&#39;t reuse Rmethod!
3891 
3892   prepare_invoke(byte_no, R5_method, Rcallsite);
3893 
3894   // Rcallsite: CallSite object (from cpool-&gt;resolved_references[f1])
3895   // Rmethod:   MH.linkToCallSite method (from f2)
3896 
3897   // Note:  Rcallsite is already pushed by prepare_invoke
3898 
3899   if (ProfileInterpreter) {
3900     __ profile_call(R2_tmp);
3901   }
3902 
3903   // do the call
3904   __ mov(Rmethod, R5_method);
3905   __ jump_from_interpreted(Rmethod);
3906 }
3907 
3908 //----------------------------------------------------------------------------------------------------
3909 // Allocation
3910 
3911 void TemplateTable::_new() {
3912   transition(vtos, atos);
3913 
3914   const Register Robj   = R0_tos;
3915   const Register Rcpool = R1_tmp;
3916   const Register Rindex = R2_tmp;
3917   const Register Rtags  = R3_tmp;
3918   const Register Rsize  = R3_tmp;
3919 
3920   Register Rklass = R4_tmp;
3921   assert_different_registers(Rcpool, Rindex, Rtags, Rklass, Rtemp);
3922   assert_different_registers(Rcpool, Rindex, Rklass, Rsize);
3923 
3924   Label slow_case;
3925   Label done;
3926   Label initialize_header;
3927   Label initialize_object;  // including clearing the fields
3928 
3929   const bool allow_shared_alloc =
3930     Universe::heap()-&gt;supports_inline_contig_alloc();
3931 
3932   // Literals
3933   InlinedAddress Lheap_top_addr(allow_shared_alloc ? (address)Universe::heap()-&gt;top_addr() : NULL);
3934 
3935   __ get_unsigned_2_byte_index_at_bcp(Rindex, 1);
3936   __ get_cpool_and_tags(Rcpool, Rtags);
3937 
3938   // Make sure the class we&#39;re about to instantiate has been resolved.
3939   // This is done before loading InstanceKlass to be consistent with the order
3940   // how Constant Pool is updated (see ConstantPool::klass_at_put)
3941   const int tags_offset = Array&lt;u1&gt;::base_offset_in_bytes();
3942   __ add(Rtemp, Rtags, Rindex);
3943 
3944   __ ldrb(Rtemp, Address(Rtemp, tags_offset));
3945 
3946   // use Rklass as a scratch
3947   volatile_barrier(MacroAssembler::LoadLoad, Rklass);
3948 
3949   // get InstanceKlass
3950   __ cmp(Rtemp, JVM_CONSTANT_Class);
3951   __ b(slow_case, ne);
3952   __ load_resolved_klass_at_offset(Rcpool, Rindex, Rklass);
3953 
3954   // make sure klass is initialized &amp; doesn&#39;t have finalizer
3955   // make sure klass is fully initialized
3956   __ ldrb(Rtemp, Address(Rklass, InstanceKlass::init_state_offset()));
3957   __ cmp(Rtemp, InstanceKlass::fully_initialized);
3958   __ b(slow_case, ne);
3959 
3960   // get instance_size in InstanceKlass (scaled to a count of bytes)
3961   __ ldr_u32(Rsize, Address(Rklass, Klass::layout_helper_offset()));
3962 
3963   // test to see if it has a finalizer or is malformed in some way
3964   // Klass::_lh_instance_slow_path_bit is really a bit mask, not bit number
3965   __ tbnz(Rsize, exact_log2(Klass::_lh_instance_slow_path_bit), slow_case);
3966 
3967   // Allocate the instance:
3968   //  If TLAB is enabled:
3969   //    Try to allocate in the TLAB.
3970   //    If fails, go to the slow path.
3971   //  Else If inline contiguous allocations are enabled:
3972   //    Try to allocate in eden.
3973   //    If fails due to heap end, go to slow path.
3974   //
3975   //  If TLAB is enabled OR inline contiguous is enabled:
3976   //    Initialize the allocation.
3977   //    Exit.
3978   //
3979   //  Go to slow path.
3980   if (UseTLAB) {
3981     const Register Rtlab_top = R1_tmp;
3982     const Register Rtlab_end = R2_tmp;
3983     assert_different_registers(Robj, Rsize, Rklass, Rtlab_top, Rtlab_end);
3984 
3985     __ tlab_allocate(Robj, Rtlab_top, Rtlab_end, Rsize, slow_case);
3986     if (ZeroTLAB) {
3987       // the fields have been already cleared
3988       __ b(initialize_header);
3989     } else {
3990       // initialize both the header and fields
3991       __ b(initialize_object);
3992     }
3993   } else {
3994     // Allocation in the shared Eden, if allowed.
3995     if (allow_shared_alloc) {
3996       const Register Rheap_top_addr = R2_tmp;
3997       const Register Rheap_top = R5_tmp;
3998       const Register Rheap_end = Rtemp;
3999       assert_different_registers(Robj, Rklass, Rsize, Rheap_top_addr, Rheap_top, Rheap_end, LR);
4000 
4001       __ eden_allocate(Robj, Rheap_top, Rheap_top_addr, Rheap_end, Rsize, slow_case);
4002     }
4003   }
4004 
4005   if (UseTLAB || allow_shared_alloc) {
4006     const Register Rzero0 = R1_tmp;
4007     const Register Rzero1 = R2_tmp;
4008     const Register Rzero_end = R5_tmp;
4009     const Register Rzero_cur = Rtemp;
4010     assert_different_registers(Robj, Rsize, Rklass, Rzero0, Rzero1, Rzero_cur, Rzero_end);
4011 
4012     // The object is initialized before the header.  If the object size is
4013     // zero, go directly to the header initialization.
4014     __ bind(initialize_object);
4015     __ subs(Rsize, Rsize, sizeof(oopDesc));
4016     __ add(Rzero_cur, Robj, sizeof(oopDesc));
4017     __ b(initialize_header, eq);
4018 
4019 #ifdef ASSERT
4020     // make sure Rsize is a multiple of 8
4021     Label L;
4022     __ tst(Rsize, 0x07);
4023     __ b(L, eq);
4024     __ stop(&quot;object size is not multiple of 8 - adjust this code&quot;);
4025     __ bind(L);
4026 #endif
4027 
4028     __ mov(Rzero0, 0);
4029     __ mov(Rzero1, 0);
4030     __ add(Rzero_end, Rzero_cur, Rsize);
4031 
4032     // initialize remaining object fields: Rsize was a multiple of 8
4033     { Label loop;
4034       // loop is unrolled 2 times
4035       __ bind(loop);
4036       // #1
4037       __ stmia(Rzero_cur, RegisterSet(Rzero0) | RegisterSet(Rzero1), writeback);
4038       __ cmp(Rzero_cur, Rzero_end);
4039       // #2
4040       __ stmia(Rzero_cur, RegisterSet(Rzero0) | RegisterSet(Rzero1), writeback, ne);
4041       __ cmp(Rzero_cur, Rzero_end, ne);
4042       __ b(loop, ne);
4043     }
4044 
4045     // initialize object header only.
4046     __ bind(initialize_header);
4047     if (UseBiasedLocking) {
4048       __ ldr(Rtemp, Address(Rklass, Klass::prototype_header_offset()));
4049     } else {
<a name="12" id="anc12"></a><span class="line-modified">4050       __ mov_slow(Rtemp, (intptr_t)markWord::prototype().value());</span>
4051     }
4052     // mark
4053     __ str(Rtemp, Address(Robj, oopDesc::mark_offset_in_bytes()));
4054 
4055     // klass
4056     __ store_klass(Rklass, Robj); // blows Rklass:
4057     Rklass = noreg;
4058 
4059     // Note: Disable DTrace runtime check for now to eliminate overhead on each allocation
4060     if (DTraceAllocProbes) {
4061       // Trigger dtrace event for fastpath
4062       Label Lcontinue;
4063 
4064       __ ldrb_global(Rtemp, (address)&amp;DTraceAllocProbes);
4065       __ cbz(Rtemp, Lcontinue);
4066 
4067       __ push(atos);
4068       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), Robj);
4069       __ pop(atos);
4070 
4071       __ bind(Lcontinue);
4072     }
4073 
4074     __ b(done);
4075   } else {
4076     // jump over literals
4077     __ b(slow_case);
4078   }
4079 
4080   if (allow_shared_alloc) {
4081     __ bind_literal(Lheap_top_addr);
4082   }
4083 
4084   // slow case
4085   __ bind(slow_case);
4086   __ get_constant_pool(Rcpool);
4087   __ get_unsigned_2_byte_index_at_bcp(Rindex, 1);
4088   __ call_VM(Robj, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), Rcpool, Rindex);
4089 
4090   // continue
4091   __ bind(done);
4092 
4093   // StoreStore barrier required after complete initialization
4094   // (headers + content zeroing), before the object may escape.
4095   __ membar(MacroAssembler::StoreStore, R1_tmp);
4096 }
4097 
4098 
4099 void TemplateTable::newarray() {
4100   transition(itos, atos);
4101   __ ldrb(R1, at_bcp(1));
4102   __ mov(R2, R0_tos);
4103   call_VM(R0_tos, CAST_FROM_FN_PTR(address, InterpreterRuntime::newarray), R1, R2);
4104   // MacroAssembler::StoreStore useless (included in the runtime exit path)
4105 }
4106 
4107 
4108 void TemplateTable::anewarray() {
4109   transition(itos, atos);
4110   __ get_unsigned_2_byte_index_at_bcp(R2, 1);
4111   __ get_constant_pool(R1);
4112   __ mov(R3, R0_tos);
4113   call_VM(R0_tos, CAST_FROM_FN_PTR(address, InterpreterRuntime::anewarray), R1, R2, R3);
4114   // MacroAssembler::StoreStore useless (included in the runtime exit path)
4115 }
4116 
4117 
4118 void TemplateTable::arraylength() {
4119   transition(atos, itos);
4120   __ null_check(R0_tos, Rtemp, arrayOopDesc::length_offset_in_bytes());
4121   __ ldr_s32(R0_tos, Address(R0_tos, arrayOopDesc::length_offset_in_bytes()));
4122 }
4123 
4124 
4125 void TemplateTable::checkcast() {
4126   transition(atos, atos);
4127   Label done, is_null, quicked, resolved, throw_exception;
4128 
4129   const Register Robj = R0_tos;
4130   const Register Rcpool = R2_tmp;
4131   const Register Rtags = R3_tmp;
4132   const Register Rindex = R4_tmp;
4133   const Register Rsuper = R3_tmp;
4134   const Register Rsub   = R4_tmp;
4135   const Register Rsubtype_check_tmp1 = R1_tmp;
4136   const Register Rsubtype_check_tmp2 = LR_tmp;
4137 
4138   __ cbz(Robj, is_null);
4139 
4140   // Get cpool &amp; tags index
4141   __ get_cpool_and_tags(Rcpool, Rtags);
4142   __ get_unsigned_2_byte_index_at_bcp(Rindex, 1);
4143 
4144   // See if bytecode has already been quicked
4145   __ add(Rtemp, Rtags, Rindex);
4146   __ ldrb(Rtemp, Address(Rtemp, Array&lt;u1&gt;::base_offset_in_bytes()));
4147 
4148   __ cmp(Rtemp, JVM_CONSTANT_Class);
4149 
4150   volatile_barrier(MacroAssembler::LoadLoad, Rtemp, true);
4151 
4152   __ b(quicked, eq);
4153 
4154   __ push(atos);
4155   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::quicken_io_cc));
4156   // vm_result_2 has metadata result
4157   __ get_vm_result_2(Rsuper, Robj);
4158   __ pop_ptr(Robj);
4159   __ b(resolved);
4160 
4161   __ bind(throw_exception);
4162   // Come here on failure of subtype check
4163   __ profile_typecheck_failed(R1_tmp);
4164   __ mov(R2_ClassCastException_obj, Robj);             // convention with generate_ClassCastException_handler()
4165   __ b(Interpreter::_throw_ClassCastException_entry);
4166 
4167   // Get superklass in Rsuper and subklass in Rsub
4168   __ bind(quicked);
4169   __ load_resolved_klass_at_offset(Rcpool, Rindex, Rsuper);
4170 
4171   __ bind(resolved);
4172   __ load_klass(Rsub, Robj);
4173 
4174   // Generate subtype check. Blows both tmps and Rtemp.
4175   assert_different_registers(Robj, Rsub, Rsuper, Rsubtype_check_tmp1, Rsubtype_check_tmp2, Rtemp);
4176   __ gen_subtype_check(Rsub, Rsuper, throw_exception, Rsubtype_check_tmp1, Rsubtype_check_tmp2);
4177 
4178   // Come here on success
4179 
4180   // Collect counts on whether this check-cast sees NULLs a lot or not.
4181   if (ProfileInterpreter) {
4182     __ b(done);
4183     __ bind(is_null);
4184     __ profile_null_seen(R1_tmp);
4185   } else {
4186     __ bind(is_null);   // same as &#39;done&#39;
4187   }
4188   __ bind(done);
4189 }
4190 
4191 
4192 void TemplateTable::instanceof() {
4193   // result = 0: obj == NULL or  obj is not an instanceof the specified klass
4194   // result = 1: obj != NULL and obj is     an instanceof the specified klass
4195 
4196   transition(atos, itos);
4197   Label done, is_null, not_subtype, quicked, resolved;
4198 
4199   const Register Robj = R0_tos;
4200   const Register Rcpool = R2_tmp;
4201   const Register Rtags = R3_tmp;
4202   const Register Rindex = R4_tmp;
4203   const Register Rsuper = R3_tmp;
4204   const Register Rsub   = R4_tmp;
4205   const Register Rsubtype_check_tmp1 = R0_tmp;
4206   const Register Rsubtype_check_tmp2 = R1_tmp;
4207 
4208   __ cbz(Robj, is_null);
4209 
4210   __ load_klass(Rsub, Robj);
4211 
4212   // Get cpool &amp; tags index
4213   __ get_cpool_and_tags(Rcpool, Rtags);
4214   __ get_unsigned_2_byte_index_at_bcp(Rindex, 1);
4215 
4216   // See if bytecode has already been quicked
4217   __ add(Rtemp, Rtags, Rindex);
4218   __ ldrb(Rtemp, Address(Rtemp, Array&lt;u1&gt;::base_offset_in_bytes()));
4219   __ cmp(Rtemp, JVM_CONSTANT_Class);
4220 
4221   volatile_barrier(MacroAssembler::LoadLoad, Rtemp, true);
4222 
4223   __ b(quicked, eq);
4224 
4225   __ push(atos);
4226   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::quicken_io_cc));
4227   // vm_result_2 has metadata result
4228   __ get_vm_result_2(Rsuper, Robj);
4229   __ pop_ptr(Robj);
4230   __ b(resolved);
4231 
4232   // Get superklass in Rsuper and subklass in Rsub
4233   __ bind(quicked);
4234   __ load_resolved_klass_at_offset(Rcpool, Rindex, Rsuper);
4235 
4236   __ bind(resolved);
4237   __ load_klass(Rsub, Robj);
4238 
4239   // Generate subtype check. Blows both tmps and Rtemp.
4240   __ gen_subtype_check(Rsub, Rsuper, not_subtype, Rsubtype_check_tmp1, Rsubtype_check_tmp2);
4241 
4242   // Come here on success
4243   __ mov(R0_tos, 1);
4244   __ b(done);
4245 
4246   __ bind(not_subtype);
4247   // Come here on failure
4248   __ profile_typecheck_failed(R1_tmp);
4249   __ mov(R0_tos, 0);
4250 
4251   // Collect counts on whether this test sees NULLs a lot or not.
4252   if (ProfileInterpreter) {
4253     __ b(done);
4254     __ bind(is_null);
4255     __ profile_null_seen(R1_tmp);
4256   } else {
4257     __ bind(is_null);   // same as &#39;done&#39;
4258   }
4259   __ bind(done);
4260 }
4261 
4262 
4263 //----------------------------------------------------------------------------------------------------
4264 // Breakpoints
4265 void TemplateTable::_breakpoint() {
4266 
4267   // Note: We get here even if we are single stepping..
4268   // jbug inists on setting breakpoints at every bytecode
4269   // even if we are in single step mode.
4270 
4271   transition(vtos, vtos);
4272 
4273   // get the unpatched byte code
4274   __ mov(R1, Rmethod);
4275   __ mov(R2, Rbcp);
4276   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::get_original_bytecode_at), R1, R2);
4277   __ mov(Rtmp_save0, R0);
4278 
4279   // post the breakpoint event
4280   __ mov(R1, Rmethod);
4281   __ mov(R2, Rbcp);
4282   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::_breakpoint), R1, R2);
4283 
4284   // complete the execution of original bytecode
4285   __ mov(R3_bytecode, Rtmp_save0);
4286   __ dispatch_only_normal(vtos);
4287 }
4288 
4289 
4290 //----------------------------------------------------------------------------------------------------
4291 // Exceptions
4292 
4293 void TemplateTable::athrow() {
4294   transition(atos, vtos);
4295   __ mov(Rexception_obj, R0_tos);
4296   __ null_check(Rexception_obj, Rtemp);
4297   __ b(Interpreter::throw_exception_entry());
4298 }
4299 
4300 
4301 //----------------------------------------------------------------------------------------------------
4302 // Synchronization
4303 //
4304 // Note: monitorenter &amp; exit are symmetric routines; which is reflected
4305 //       in the assembly code structure as well
4306 //
4307 // Stack layout:
4308 //
4309 // [expressions  ] &lt;--- Rstack_top        = expression stack top
4310 // ..
4311 // [expressions  ]
4312 // [monitor entry] &lt;--- monitor block top = expression stack bot
4313 // ..
4314 // [monitor entry]
4315 // [frame data   ] &lt;--- monitor block bot
4316 // ...
4317 // [saved FP     ] &lt;--- FP
4318 
4319 
4320 void TemplateTable::monitorenter() {
4321   transition(atos, vtos);
4322 
4323   const Register Robj = R0_tos;
4324   const Register Rentry = R1_tmp;
4325 
4326   // check for NULL object
4327   __ null_check(Robj, Rtemp);
4328 
4329   __ resolve(IS_NOT_NULL, Robj);
4330 
4331   const int entry_size = (frame::interpreter_frame_monitor_size() * wordSize);
4332   assert (entry_size % StackAlignmentInBytes == 0, &quot;keep stack alignment&quot;);
4333   Label allocate_monitor, allocated;
4334 
4335   // initialize entry pointer
4336   __ mov(Rentry, 0);                             // points to free slot or NULL
4337 
4338   // find a free slot in the monitor block (result in Rentry)
4339   { Label loop, exit;
4340     const Register Rcur = R2_tmp;
4341     const Register Rcur_obj = Rtemp;
4342     const Register Rbottom = R3_tmp;
4343     assert_different_registers(Robj, Rentry, Rcur, Rbottom, Rcur_obj);
4344 
4345     __ ldr(Rcur, Address(FP, frame::interpreter_frame_monitor_block_top_offset * wordSize));
4346                                  // points to current entry, starting with top-most entry
4347     __ sub(Rbottom, FP, -frame::interpreter_frame_monitor_block_bottom_offset * wordSize);
4348                                  // points to word before bottom of monitor block
4349 
4350     __ cmp(Rcur, Rbottom);                       // check if there are no monitors
4351     __ ldr(Rcur_obj, Address(Rcur, BasicObjectLock::obj_offset_in_bytes()), ne);
4352                                                  // prefetch monitor&#39;s object for the first iteration
4353     __ b(allocate_monitor, eq);                  // there are no monitors, skip searching
4354 
4355     __ bind(loop);
4356     __ cmp(Rcur_obj, 0);                         // check if current entry is used
4357     __ mov(Rentry, Rcur, eq);                    // if not used then remember entry
4358 
4359     __ cmp(Rcur_obj, Robj);                      // check if current entry is for same object
4360     __ b(exit, eq);                              // if same object then stop searching
4361 
4362     __ add(Rcur, Rcur, entry_size);              // otherwise advance to next entry
4363 
4364     __ cmp(Rcur, Rbottom);                       // check if bottom reached
4365     __ ldr(Rcur_obj, Address(Rcur, BasicObjectLock::obj_offset_in_bytes()), ne);
4366                                                  // prefetch monitor&#39;s object for the next iteration
4367     __ b(loop, ne);                              // if not at bottom then check this entry
4368     __ bind(exit);
4369   }
4370 
4371   __ cbnz(Rentry, allocated);                    // check if a slot has been found; if found, continue with that one
4372 
4373   __ bind(allocate_monitor);
4374 
4375   // allocate one if there&#39;s no free slot
4376   { Label loop;
4377     assert_different_registers(Robj, Rentry, R2_tmp, Rtemp);
4378 
4379     // 1. compute new pointers
4380 
4381 
4382     __ ldr(Rentry, Address(FP, frame::interpreter_frame_monitor_block_top_offset * wordSize));
4383                                                  // old monitor block top / expression stack bottom
4384 
4385     __ sub(Rstack_top, Rstack_top, entry_size);  // move expression stack top
4386     __ check_stack_top_on_expansion();
4387 
4388     __ sub(Rentry, Rentry, entry_size);          // move expression stack bottom
4389 
4390     __ mov(R2_tmp, Rstack_top);                  // set start value for copy loop
4391 
4392     __ str(Rentry, Address(FP, frame::interpreter_frame_monitor_block_top_offset * wordSize));
4393                                                  // set new monitor block top
4394 
4395     // 2. move expression stack contents
4396 
4397     __ cmp(R2_tmp, Rentry);                                 // check if expression stack is empty
4398     __ ldr(Rtemp, Address(R2_tmp, entry_size), ne);         // load expression stack word from old location
4399     __ b(allocated, eq);
4400 
4401     __ bind(loop);
4402     __ str(Rtemp, Address(R2_tmp, wordSize, post_indexed)); // store expression stack word at new location
4403                                                             // and advance to next word
4404     __ cmp(R2_tmp, Rentry);                                 // check if bottom reached
4405     __ ldr(Rtemp, Address(R2, entry_size), ne);             // load expression stack word from old location
4406     __ b(loop, ne);                                         // if not at bottom then copy next word
4407   }
4408 
4409   // call run-time routine
4410 
4411   // Rentry: points to monitor entry
4412   __ bind(allocated);
4413 
4414   // Increment bcp to point to the next bytecode, so exception handling for async. exceptions work correctly.
4415   // The object has already been poped from the stack, so the expression stack looks correct.
4416   __ add(Rbcp, Rbcp, 1);
4417 
4418   __ str(Robj, Address(Rentry, BasicObjectLock::obj_offset_in_bytes()));     // store object
4419   __ lock_object(Rentry);
4420 
4421   // check to make sure this monitor doesn&#39;t cause stack overflow after locking
4422   __ save_bcp();  // in case of exception
4423   __ arm_stack_overflow_check(0, Rtemp);
4424 
4425   // The bcp has already been incremented. Just need to dispatch to next instruction.
4426   __ dispatch_next(vtos);
4427 }
4428 
4429 
4430 void TemplateTable::monitorexit() {
4431   transition(atos, vtos);
4432 
4433   const Register Robj = R0_tos;
4434   const Register Rcur = R1_tmp;
4435   const Register Rbottom = R2_tmp;
4436   const Register Rcur_obj = Rtemp;
4437 
4438   // check for NULL object
4439   __ null_check(Robj, Rtemp);
4440 
4441   __ resolve(IS_NOT_NULL, Robj);
4442 
4443   const int entry_size = (frame::interpreter_frame_monitor_size() * wordSize);
4444   Label found, throw_exception;
4445 
4446   // find matching slot
4447   { Label loop;
4448     assert_different_registers(Robj, Rcur, Rbottom, Rcur_obj);
4449 
4450     __ ldr(Rcur, Address(FP, frame::interpreter_frame_monitor_block_top_offset * wordSize));
4451                                  // points to current entry, starting with top-most entry
4452     __ sub(Rbottom, FP, -frame::interpreter_frame_monitor_block_bottom_offset * wordSize);
4453                                  // points to word before bottom of monitor block
4454 
4455     __ cmp(Rcur, Rbottom);                       // check if bottom reached
4456     __ ldr(Rcur_obj, Address(Rcur, BasicObjectLock::obj_offset_in_bytes()), ne);
4457                                                  // prefetch monitor&#39;s object for the first iteration
4458     __ b(throw_exception, eq);                   // throw exception if there are now monitors
4459 
4460     __ bind(loop);
4461     // check if current entry is for same object
4462     __ cmp(Rcur_obj, Robj);
4463     __ b(found, eq);                             // if same object then stop searching
4464     __ add(Rcur, Rcur, entry_size);              // otherwise advance to next entry
4465     __ cmp(Rcur, Rbottom);                       // check if bottom reached
4466     __ ldr(Rcur_obj, Address(Rcur, BasicObjectLock::obj_offset_in_bytes()), ne);
4467     __ b (loop, ne);                             // if not at bottom then check this entry
4468   }
4469 
4470   // error handling. Unlocking was not block-structured
4471   __ bind(throw_exception);
4472   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_illegal_monitor_state_exception));
4473   __ should_not_reach_here();
4474 
4475   // call run-time routine
4476   // Rcur: points to monitor entry
4477   __ bind(found);
4478   __ push_ptr(Robj);                             // make sure object is on stack (contract with oopMaps)
4479   __ unlock_object(Rcur);
4480   __ pop_ptr(Robj);                              // discard object
4481 }
4482 
4483 
4484 //----------------------------------------------------------------------------------------------------
4485 // Wide instructions
4486 
4487 void TemplateTable::wide() {
4488   transition(vtos, vtos);
4489   __ ldrb(R3_bytecode, at_bcp(1));
4490 
4491   InlinedAddress Ltable((address)Interpreter::_wentry_point);
4492   __ ldr_literal(Rtemp, Ltable);
4493   __ indirect_jump(Address::indexed_ptr(Rtemp, R3_bytecode), Rtemp);
4494 
4495   __ nop(); // to avoid filling CPU pipeline with invalid instructions
4496   __ nop();
4497   __ bind_literal(Ltable);
4498 }
4499 
4500 
4501 //----------------------------------------------------------------------------------------------------
4502 // Multi arrays
4503 
4504 void TemplateTable::multianewarray() {
4505   transition(vtos, atos);
4506   __ ldrb(Rtmp_save0, at_bcp(3));   // get number of dimensions
4507 
4508   // last dim is on top of stack; we want address of first one:
4509   // first_addr = last_addr + ndims * stackElementSize - 1*wordsize
4510   // the latter wordSize to point to the beginning of the array.
4511   __ add(Rtemp, Rstack_top, AsmOperand(Rtmp_save0, lsl, Interpreter::logStackElementSize));
4512   __ sub(R1, Rtemp, wordSize);
4513 
4514   call_VM(R0, CAST_FROM_FN_PTR(address, InterpreterRuntime::multianewarray), R1);
4515   __ add(Rstack_top, Rstack_top, AsmOperand(Rtmp_save0, lsl, Interpreter::logStackElementSize));
4516   // MacroAssembler::StoreStore useless (included in the runtime exit path)
4517 }
<a name="13" id="anc13"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="13" type="hidden" />
</body>
</html>