<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/cpu/arm/c1_Runtime1_arm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2008, 2018, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;asm/macroAssembler.inline.hpp&quot;
 27 #include &quot;c1/c1_Defs.hpp&quot;
 28 #include &quot;c1/c1_LIRAssembler.hpp&quot;
 29 #include &quot;c1/c1_MacroAssembler.hpp&quot;
 30 #include &quot;c1/c1_Runtime1.hpp&quot;
 31 #include &quot;ci/ciUtilities.hpp&quot;
 32 #include &quot;gc/shared/cardTable.hpp&quot;
 33 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
 34 #include &quot;interpreter/interpreter.hpp&quot;
 35 #include &quot;memory/universe.hpp&quot;
 36 #include &quot;nativeInst_arm.hpp&quot;
 37 #include &quot;oops/compiledICHolder.hpp&quot;
 38 #include &quot;oops/oop.inline.hpp&quot;
 39 #include &quot;prims/jvmtiExport.hpp&quot;
 40 #include &quot;register_arm.hpp&quot;
 41 #include &quot;runtime/sharedRuntime.hpp&quot;
 42 #include &quot;runtime/signature.hpp&quot;
 43 #include &quot;runtime/vframeArray.hpp&quot;
 44 #include &quot;utilities/align.hpp&quot;
 45 #include &quot;vmreg_arm.inline.hpp&quot;
 46 
 47 // Note: Rtemp usage is this file should not impact C2 and should be
 48 // correct as long as it is not implicitly used in lower layers (the
 49 // arm [macro]assembler) and used with care in the other C1 specific
 50 // files.
 51 
 52 // Implementation of StubAssembler
 53 
 54 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, int args_size) {
 55   mov(R0, Rthread);
 56 
 57   int call_offset = set_last_Java_frame(SP, FP, false, Rtemp);
 58 
 59   call(entry);
 60   if (call_offset == -1) { // PC not saved
 61     call_offset = offset();
 62   }
 63   reset_last_Java_frame(Rtemp);
 64 
 65   assert(frame_size() != no_frame_size, &quot;frame must be fixed&quot;);
 66   if (_stub_id != Runtime1::forward_exception_id) {
 67     ldr(R3, Address(Rthread, Thread::pending_exception_offset()));
 68   }
 69 
 70   if (oop_result1-&gt;is_valid()) {
 71     assert_different_registers(oop_result1, R3, Rtemp);
 72     get_vm_result(oop_result1, Rtemp);
 73   }
 74   if (metadata_result-&gt;is_valid()) {
 75     assert_different_registers(metadata_result, R3, Rtemp);
 76     get_vm_result_2(metadata_result, Rtemp);
 77   }
 78 
 79   // Check for pending exception
 80   // unpack_with_exception_in_tls path is taken through
 81   // Runtime1::exception_handler_for_pc
 82   if (_stub_id != Runtime1::forward_exception_id) {
 83     assert(frame_size() != no_frame_size, &quot;cannot directly call forward_exception_id&quot;);
 84     cmp(R3, 0);
 85     jump(Runtime1::entry_for(Runtime1::forward_exception_id), relocInfo::runtime_call_type, Rtemp, ne);
 86   } else {
 87 #ifdef ASSERT
 88     // Should not have pending exception in forward_exception stub
 89     ldr(R3, Address(Rthread, Thread::pending_exception_offset()));
 90     cmp(R3, 0);
 91     breakpoint(ne);
 92 #endif // ASSERT
 93   }
 94   return call_offset;
 95 }
 96 
 97 
 98 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1) {
 99   if (arg1 != R1) {
100     mov(R1, arg1);
101   }
102   return call_RT(oop_result1, metadata_result, entry, 1);
103 }
104 
105 
106 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1, Register arg2) {
107   assert(arg1 == R1 &amp;&amp; arg2 == R2, &quot;cannot handle otherwise&quot;);
108   return call_RT(oop_result1, metadata_result, entry, 2);
109 }
110 
111 
112 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1, Register arg2, Register arg3) {
113   assert(arg1 == R1 &amp;&amp; arg2 == R2 &amp;&amp; arg3 == R3, &quot;cannot handle otherwise&quot;);
114   return call_RT(oop_result1, metadata_result, entry, 3);
115 }
116 
117 
118 #define __ sasm-&gt;
119 
120 // TODO: ARM - does this duplicate RegisterSaver in SharedRuntime?
121 
122 enum RegisterLayout {
123   fpu_save_size = pd_nof_fpu_regs_reg_alloc,
124 #ifndef __SOFTFP__
125   D0_offset = 0,
126 #endif
127   R0_offset = fpu_save_size,
128   R1_offset,
129   R2_offset,
130   R3_offset,
131   R4_offset,
132   R5_offset,
133   R6_offset,
134 #if (FP_REG_NUM != 7)
135   R7_offset,
136 #endif
137   R8_offset,
138   R9_offset,
139   R10_offset,
140 #if (FP_REG_NUM != 11)
141   R11_offset,
142 #endif
143   R12_offset,
144   FP_offset,
145   LR_offset,
146   reg_save_size,
147   arg1_offset = reg_save_size * wordSize,
148   arg2_offset = (reg_save_size + 1) * wordSize
149 };
150 
151 
152 static OopMap* generate_oop_map(StubAssembler* sasm, bool save_fpu_registers = HaveVFP) {
153   sasm-&gt;set_frame_size(reg_save_size /* in words */);
154 
155   // Record saved value locations in an OopMap.
156   // Locations are offsets from sp after runtime call.
157   OopMap* map = new OopMap(VMRegImpl::slots_per_word * reg_save_size, 0);
158 
159   int j=0;
160   for (int i = R0_offset; i &lt; R10_offset; i++) {
161     if (j == FP_REG_NUM) {
162       // skip the FP register, saved below
163       j++;
164     }
165     map-&gt;set_callee_saved(VMRegImpl::stack2reg(i), as_Register(j)-&gt;as_VMReg());
166     j++;
167   }
168   assert(j == R10-&gt;encoding(), &quot;must be&quot;);
169 #if (FP_REG_NUM != 11)
170   // add R11, if not saved as FP
171   map-&gt;set_callee_saved(VMRegImpl::stack2reg(R11_offset), R11-&gt;as_VMReg());
172 #endif
173   map-&gt;set_callee_saved(VMRegImpl::stack2reg(FP_offset), FP-&gt;as_VMReg());
174   map-&gt;set_callee_saved(VMRegImpl::stack2reg(LR_offset), LR-&gt;as_VMReg());
175 
176   if (save_fpu_registers) {
177     for (int i = 0; i &lt; fpu_save_size; i++) {
178       map-&gt;set_callee_saved(VMRegImpl::stack2reg(i), as_FloatRegister(i)-&gt;as_VMReg());
179     }
180   }
181 
182   return map;
183 }
184 
185 static OopMap* save_live_registers(StubAssembler* sasm, bool save_fpu_registers = HaveVFP) {
186   __ block_comment(&quot;save_live_registers&quot;);
187   sasm-&gt;set_frame_size(reg_save_size /* in words */);
188 
189   __ push(RegisterSet(FP) | RegisterSet(LR));
190   __ push(RegisterSet(R0, R6) | RegisterSet(R8, R10) | R12 | altFP_7_11);
191   if (save_fpu_registers) {
192     __ fpush(FloatRegisterSet(D0, fpu_save_size / 2));
193   } else {
194     __ sub(SP, SP, fpu_save_size * wordSize);
195   }
196 
197   return generate_oop_map(sasm, save_fpu_registers);
198 }
199 
200 
201 static void restore_live_registers(StubAssembler* sasm,
202                                    bool restore_R0,
203                                    bool restore_FP_LR,
204                                    bool do_return,
205                                    bool restore_fpu_registers = HaveVFP) {
206   __ block_comment(&quot;restore_live_registers&quot;);
207 
208   if (restore_fpu_registers) {
209     __ fpop(FloatRegisterSet(D0, fpu_save_size / 2));
210     if (!restore_R0) {
211       __ add(SP, SP, (R1_offset - fpu_save_size) * wordSize);
212     }
213   } else {
214     __ add(SP, SP, (restore_R0 ? fpu_save_size : R1_offset) * wordSize);
215   }
216   __ pop(RegisterSet((restore_R0 ? R0 : R1), R6) | RegisterSet(R8, R10) | R12 | altFP_7_11);
217   if (restore_FP_LR) {
218     __ pop(RegisterSet(FP) | RegisterSet(do_return ? PC : LR));
219   } else {
220     assert (!do_return, &quot;return without restoring FP/LR&quot;);
221   }
222 }
223 
224 
225 static void restore_live_registers_except_R0(StubAssembler* sasm, bool restore_fpu_registers = HaveVFP) {
226   restore_live_registers(sasm, false, true, true, restore_fpu_registers);
227 }
228 
229 static void restore_live_registers(StubAssembler* sasm, bool restore_fpu_registers = HaveVFP) {
230   restore_live_registers(sasm, true, true, true, restore_fpu_registers);
231 }
232 
233 static void restore_live_registers_except_FP_LR(StubAssembler* sasm, bool restore_fpu_registers = HaveVFP) {
234   restore_live_registers(sasm, true, false, false, restore_fpu_registers);
235 }
236 
237 static void restore_live_registers_without_return(StubAssembler* sasm, bool restore_fpu_registers = HaveVFP) {
238   restore_live_registers(sasm, true, true, false, restore_fpu_registers);
239 }
240 
241 void StubAssembler::save_live_registers() {
242   ::save_live_registers(this);
243 }
244 
245 void StubAssembler::restore_live_registers_without_return() {
246   ::restore_live_registers_without_return(this);
247 }
248 
249 void Runtime1::initialize_pd() {
250 }
251 
252 
253 OopMapSet* Runtime1::generate_exception_throw(StubAssembler* sasm, address target, bool has_argument) {
254   OopMap* oop_map = save_live_registers(sasm);
255 
256   int call_offset;
257   if (has_argument) {
258     __ ldr(R1, Address(SP, arg1_offset));
259     __ ldr(R2, Address(SP, arg2_offset));
260     call_offset = __ call_RT(noreg, noreg, target, R1, R2);
261   } else {
262     call_offset = __ call_RT(noreg, noreg, target);
263   }
264 
265   OopMapSet* oop_maps = new OopMapSet();
266   oop_maps-&gt;add_gc_map(call_offset, oop_map);
267 
268   DEBUG_ONLY(STOP(&quot;generate_exception_throw&quot;);)  // Should not reach here
269   return oop_maps;
270 }
271 
272 
273 static void restore_sp_for_method_handle(StubAssembler* sasm) {
274   // Restore SP from its saved reg (FP) if the exception PC is a MethodHandle call site.
275   __ ldr_s32(Rtemp, Address(Rthread, JavaThread::is_method_handle_return_offset()));
276   __ cmp(Rtemp, 0);
277   __ mov(SP, Rmh_SP_save, ne);
278 }
279 
280 
281 OopMapSet* Runtime1::generate_handle_exception(StubID id, StubAssembler* sasm) {
282   __ block_comment(&quot;generate_handle_exception&quot;);
283 
284   bool save_fpu_registers = false;
285 
286   // Save registers, if required.
287   OopMapSet* oop_maps = new OopMapSet();
288   OopMap* oop_map = NULL;
289 
290   switch (id) {
291   case forward_exception_id: {
292     save_fpu_registers = HaveVFP;
293     oop_map = generate_oop_map(sasm);
294     __ ldr(Rexception_obj, Address(Rthread, Thread::pending_exception_offset()));
295     __ ldr(Rexception_pc, Address(SP, LR_offset * wordSize));
296     Register zero = __ zero_register(Rtemp);
297     __ str(zero, Address(Rthread, Thread::pending_exception_offset()));
298     break;
299   }
300   case handle_exception_id:
301     save_fpu_registers = HaveVFP;
302     // fall-through
303   case handle_exception_nofpu_id:
304     // At this point all registers MAY be live.
305     oop_map = save_live_registers(sasm, save_fpu_registers);
306     break;
307   case handle_exception_from_callee_id:
308     // At this point all registers except exception oop (R4/R19) and
309     // exception pc (R5/R20) are dead.
310     oop_map = save_live_registers(sasm);  // TODO it&#39;s not required to save all registers
311     break;
312   default:  ShouldNotReachHere();
313   }
314 
315   __ str(Rexception_obj, Address(Rthread, JavaThread::exception_oop_offset()));
316   __ str(Rexception_pc, Address(Rthread, JavaThread::exception_pc_offset()));
317 
318   __ str(Rexception_pc, Address(SP, LR_offset * wordSize)); // patch throwing pc into return address
319 
320   int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, exception_handler_for_pc));
321   oop_maps-&gt;add_gc_map(call_offset, oop_map);
322 
323   // Exception handler found
324   __ str(R0, Address(SP, LR_offset * wordSize)); // patch the return address
325 
326   // Restore the registers that were saved at the beginning, remove
327   // frame and jump to the exception handler.
328   switch (id) {
329   case forward_exception_id:
330   case handle_exception_nofpu_id:
331   case handle_exception_id:
332     restore_live_registers(sasm, save_fpu_registers);
333     // Note: the restore live registers includes the jump to LR (patched to R0)
334     break;
335   case handle_exception_from_callee_id:
336     restore_live_registers_without_return(sasm); // must not jump immediatly to handler
337     restore_sp_for_method_handle(sasm);
338     __ ret();
339     break;
340   default:  ShouldNotReachHere();
341   }
342 
343   DEBUG_ONLY(STOP(&quot;generate_handle_exception&quot;);)  // Should not reach here
344 
345   return oop_maps;
346 }
347 
348 
349 void Runtime1::generate_unwind_exception(StubAssembler* sasm) {
350   // FP no longer used to find the frame start
351   // on entry, remove_frame() has already been called (restoring FP and LR)
352 
353   // search the exception handler address of the caller (using the return address)
354   __ mov(c_rarg0, Rthread);
355   __ mov(Rexception_pc, LR);
356   __ mov(c_rarg1, LR);
357   __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), c_rarg0, c_rarg1);
358 
359   // Exception oop should be still in Rexception_obj and pc in Rexception_pc
360   // Jump to handler
361   __ verify_not_null_oop(Rexception_obj);
362 
363   // JSR292 extension
364   restore_sp_for_method_handle(sasm);
365 
366   __ jump(R0);
367 }
368 
369 
370 OopMapSet* Runtime1::generate_patching(StubAssembler* sasm, address target) {
371   OopMap* oop_map = save_live_registers(sasm);
372 
373   // call the runtime patching routine, returns non-zero if nmethod got deopted.
374   int call_offset = __ call_RT(noreg, noreg, target);
375   OopMapSet* oop_maps = new OopMapSet();
376   oop_maps-&gt;add_gc_map(call_offset, oop_map);
377 
378   DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
379   assert(deopt_blob != NULL, &quot;deoptimization blob must have been created&quot;);
380 
381   __ cmp_32(R0, 0);
382 
383   restore_live_registers_except_FP_LR(sasm);
384   __ pop(RegisterSet(FP) | RegisterSet(PC), eq);
385 
386   // Deoptimization needed
387   // TODO: ARM - no need to restore FP &amp; LR because unpack_with_reexecution() stores them back
388   __ pop(RegisterSet(FP) | RegisterSet(LR));
389 
390   __ jump(deopt_blob-&gt;unpack_with_reexecution(), relocInfo::runtime_call_type, Rtemp);
391 
392   DEBUG_ONLY(STOP(&quot;generate_patching&quot;);)  // Should not reach here
393   return oop_maps;
394 }
395 
396 
397 OopMapSet* Runtime1::generate_code_for(StubID id, StubAssembler* sasm) {
398   const bool must_gc_arguments = true;
399   const bool dont_gc_arguments = false;
400 
401   OopMapSet* oop_maps = NULL;
402   bool save_fpu_registers = HaveVFP;
403 
404   switch (id) {
405     case forward_exception_id:
406       {
407         oop_maps = generate_handle_exception(id, sasm);
408         // does not return on ARM
409       }
410       break;
411 
412     case new_instance_id:
413     case fast_new_instance_id:
414     case fast_new_instance_init_check_id:
415       {
416         const Register result = R0;
417         const Register klass  = R1;
418 
419         // If TLAB is disabled, see if there is support for inlining contiguous
420         // allocations.
421         // Otherwise, just go to the slow path.
422         if (!UseTLAB &amp;&amp; Universe::heap()-&gt;supports_inline_contig_alloc() &amp;&amp; id != new_instance_id) {
423           Label slow_case, slow_case_no_pop;
424 
425           // Make sure the class is fully initialized
426           if (id == fast_new_instance_init_check_id) {
427             __ ldrb(result, Address(klass, InstanceKlass::init_state_offset()));
428             __ cmp(result, InstanceKlass::fully_initialized);
429             __ b(slow_case_no_pop, ne);
430           }
431 
432           // Free some temporary registers
433           const Register obj_size = R4;
434           const Register tmp1     = R5;
435           const Register tmp2     = LR;
436           const Register obj_end  = Rtemp;
437 
438           __ raw_push(R4, R5, LR);
439 
440           __ ldr_u32(obj_size, Address(klass, Klass::layout_helper_offset()));
441           __ eden_allocate(result, obj_end, tmp1, tmp2, obj_size, slow_case);        // initializes result and obj_end
442           __ initialize_object(result, obj_end, klass, noreg /* len */, tmp1, tmp2,
443                                instanceOopDesc::header_size() * HeapWordSize, -1,
444                                /* is_tlab_allocated */ false);
445           __ raw_pop_and_ret(R4, R5);
446 
447           __ bind(slow_case);
448           __ raw_pop(R4, R5, LR);
449 
450           __ bind(slow_case_no_pop);
451         }
452 
453         OopMap* map = save_live_registers(sasm);
454         int call_offset = __ call_RT(result, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);
455         oop_maps = new OopMapSet();
456         oop_maps-&gt;add_gc_map(call_offset, map);
457 
458         // MacroAssembler::StoreStore useless (included in the runtime exit path)
459 
460         restore_live_registers_except_R0(sasm);
461       }
462       break;
463 
464     case counter_overflow_id:
465       {
466         OopMap* oop_map = save_live_registers(sasm);
467         __ ldr(R1, Address(SP, arg1_offset));
468         __ ldr(R2, Address(SP, arg2_offset));
469         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, counter_overflow), R1, R2);
470         oop_maps = new OopMapSet();
471         oop_maps-&gt;add_gc_map(call_offset, oop_map);
472         restore_live_registers(sasm);
473       }
474       break;
475 
476     case new_type_array_id:
477     case new_object_array_id:
478       {
479         if (id == new_type_array_id) {
480           __ set_info(&quot;new_type_array&quot;, dont_gc_arguments);
481         } else {
482           __ set_info(&quot;new_object_array&quot;, dont_gc_arguments);
483         }
484 
485         const Register result = R0;
486         const Register klass  = R1;
487         const Register length = R2;
488 
489         // If TLAB is disabled, see if there is support for inlining contiguous
490         // allocations.
491         // Otherwise, just go to the slow path.
492         if (!UseTLAB &amp;&amp; Universe::heap()-&gt;supports_inline_contig_alloc()) {
493           Label slow_case, slow_case_no_pop;
494 
495           __ cmp_32(length, C1_MacroAssembler::max_array_allocation_length);
496           __ b(slow_case_no_pop, hs);
497 
498           // Free some temporary registers
499           const Register arr_size = R4;
500           const Register tmp1     = R5;
501           const Register tmp2     = LR;
502           const Register tmp3     = Rtemp;
503           const Register obj_end  = tmp3;
504 
505           __ raw_push(R4, R5, LR);
506 
507           // Get the allocation size: round_up((length &lt;&lt; (layout_helper &amp; 0xff)) + header_size)
508           __ ldr_u32(tmp1, Address(klass, Klass::layout_helper_offset()));
509           __ mov(arr_size, MinObjAlignmentInBytesMask);
510           __ and_32(tmp2, tmp1, (unsigned int)(Klass::_lh_header_size_mask &lt;&lt; Klass::_lh_header_size_shift));
511 
512           __ add(arr_size, arr_size, AsmOperand(length, lsl, tmp1));
513 
514           __ add(arr_size, arr_size, AsmOperand(tmp2, lsr, Klass::_lh_header_size_shift));
515           __ align_reg(arr_size, arr_size, MinObjAlignmentInBytes);
516 
517           // eden_allocate destroys tmp2, so reload header_size after allocation
518           // eden_allocate initializes result and obj_end
519           __ eden_allocate(result, obj_end, tmp1, tmp2, arr_size, slow_case);
520           __ ldrb(tmp2, Address(klass, in_bytes(Klass::layout_helper_offset()) +
521                                        Klass::_lh_header_size_shift / BitsPerByte));
522           __ initialize_object(result, obj_end, klass, length, tmp1, tmp2, tmp2, -1, /* is_tlab_allocated */ false);
523           __ raw_pop_and_ret(R4, R5);
524 
525           __ bind(slow_case);
526           __ raw_pop(R4, R5, LR);
527           __ bind(slow_case_no_pop);
528         }
529 
530         OopMap* map = save_live_registers(sasm);
531         int call_offset;
532         if (id == new_type_array_id) {
533           call_offset = __ call_RT(result, noreg, CAST_FROM_FN_PTR(address, new_type_array), klass, length);
534         } else {
535           call_offset = __ call_RT(result, noreg, CAST_FROM_FN_PTR(address, new_object_array), klass, length);
536         }
537         oop_maps = new OopMapSet();
538         oop_maps-&gt;add_gc_map(call_offset, map);
539 
540         // MacroAssembler::StoreStore useless (included in the runtime exit path)
541 
542         restore_live_registers_except_R0(sasm);
543       }
544       break;
545 
546     case new_multi_array_id:
547       {
548         __ set_info(&quot;new_multi_array&quot;, dont_gc_arguments);
549 
550         // R0: klass
551         // R2: rank
552         // SP: address of 1st dimension
553         const Register result = R0;
554         OopMap* map = save_live_registers(sasm);
555 
556         __ mov(R1, R0);
557         __ add(R3, SP, arg1_offset);
558         int call_offset = __ call_RT(result, noreg, CAST_FROM_FN_PTR(address, new_multi_array), R1, R2, R3);
559 
560         oop_maps = new OopMapSet();
561         oop_maps-&gt;add_gc_map(call_offset, map);
562 
563         // MacroAssembler::StoreStore useless (included in the runtime exit path)
564 
565         restore_live_registers_except_R0(sasm);
566       }
567       break;
568 
569     case register_finalizer_id:
570       {
571         __ set_info(&quot;register_finalizer&quot;, dont_gc_arguments);
572 
573         // Do not call runtime if JVM_ACC_HAS_FINALIZER flag is not set
574         __ load_klass(Rtemp, R0);
575         __ ldr_u32(Rtemp, Address(Rtemp, Klass::access_flags_offset()));
576 
577         __ tst(Rtemp, JVM_ACC_HAS_FINALIZER);
578         __ bx(LR, eq);
579 
580         // Call VM
581         OopMap* map = save_live_registers(sasm);
582         oop_maps = new OopMapSet();
583         int call_offset = __ call_RT(noreg, noreg,
584                                      CAST_FROM_FN_PTR(address, SharedRuntime::register_finalizer), R0);
585         oop_maps-&gt;add_gc_map(call_offset, map);
586         restore_live_registers(sasm);
587       }
588       break;
589 
590     case throw_range_check_failed_id:
591       {
592         __ set_info(&quot;range_check_failed&quot;, dont_gc_arguments);
593         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_range_check_exception), true);
594       }
595       break;
596 
597     case throw_index_exception_id:
598       {
599         __ set_info(&quot;index_range_check_failed&quot;, dont_gc_arguments);
600         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_index_exception), true);
601       }
602       break;
603 
604     case throw_div0_exception_id:
605       {
606         __ set_info(&quot;throw_div0_exception&quot;, dont_gc_arguments);
607         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_div0_exception), false);
608       }
609       break;
610 
611     case throw_null_pointer_exception_id:
612       {
613         __ set_info(&quot;throw_null_pointer_exception&quot;, dont_gc_arguments);
614         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_null_pointer_exception), false);
615       }
616       break;
617 
618     case handle_exception_nofpu_id:
619     case handle_exception_id:
620       {
621         __ set_info(&quot;handle_exception&quot;, dont_gc_arguments);
622         oop_maps = generate_handle_exception(id, sasm);
623       }
624       break;
625 
626     case handle_exception_from_callee_id:
627       {
628         __ set_info(&quot;handle_exception_from_callee&quot;, dont_gc_arguments);
629         oop_maps = generate_handle_exception(id, sasm);
630       }
631       break;
632 
633     case unwind_exception_id:
634       {
635         __ set_info(&quot;unwind_exception&quot;, dont_gc_arguments);
636         generate_unwind_exception(sasm);
637       }
638       break;
639 
640     case throw_array_store_exception_id:
641       {
642         __ set_info(&quot;throw_array_store_exception&quot;, dont_gc_arguments);
643         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_array_store_exception), true);
644       }
645       break;
646 
647     case throw_class_cast_exception_id:
648       {
649         __ set_info(&quot;throw_class_cast_exception&quot;, dont_gc_arguments);
650         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_class_cast_exception), true);
651       }
652       break;
653 
654     case throw_incompatible_class_change_error_id:
655       {
656         __ set_info(&quot;throw_incompatible_class_cast_exception&quot;, dont_gc_arguments);
657         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_incompatible_class_change_error), false);
658       }
659       break;
660 
661     case slow_subtype_check_id:
662       {
663         // (in)  R0 - sub, destroyed,
664         // (in)  R1 - super, not changed
665         // (out) R0 - result: 1 if check passed, 0 otherwise
666         __ raw_push(R2, R3, LR);
667 
668         // Load an array of secondary_supers
669         __ ldr(R2, Address(R0, Klass::secondary_supers_offset()));
670         // Length goes to R3
671         __ ldr_s32(R3, Address(R2, Array&lt;Klass*&gt;::length_offset_in_bytes()));
672         __ add(R2, R2, Array&lt;Klass*&gt;::base_offset_in_bytes());
673 
674         Label loop, miss;
675         __ bind(loop);
676         __ cbz(R3, miss);
677         __ ldr(LR, Address(R2, wordSize, post_indexed));
678         __ sub(R3, R3, 1);
679         __ cmp(LR, R1);
680         __ b(loop, ne);
681 
682         // We get here if an equal cache entry is found
683         __ str(R1, Address(R0, Klass::secondary_super_cache_offset()));
684         __ mov(R0, 1);
685         __ raw_pop_and_ret(R2, R3);
686 
687         // A cache entry not found - return false
688         __ bind(miss);
689         __ mov(R0, 0);
690         __ raw_pop_and_ret(R2, R3);
691       }
692       break;
693 
694     case monitorenter_nofpu_id:
695       save_fpu_registers = false;
696       // fall through
697     case monitorenter_id:
698       {
699         __ set_info(&quot;monitorenter&quot;, dont_gc_arguments);
700         const Register obj  = R1;
701         const Register lock = R2;
702         OopMap* map = save_live_registers(sasm, save_fpu_registers);
703         __ ldr(obj, Address(SP, arg1_offset));
704         __ ldr(lock, Address(SP, arg2_offset));
705         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, monitorenter), obj, lock);
706         oop_maps = new OopMapSet();
707         oop_maps-&gt;add_gc_map(call_offset, map);
708         restore_live_registers(sasm, save_fpu_registers);
709       }
710       break;
711 
712     case monitorexit_nofpu_id:
713       save_fpu_registers = false;
714       // fall through
715     case monitorexit_id:
716       {
717         __ set_info(&quot;monitorexit&quot;, dont_gc_arguments);
718         const Register lock = R1;
719         OopMap* map = save_live_registers(sasm, save_fpu_registers);
720         __ ldr(lock, Address(SP, arg1_offset));
721         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, monitorexit), lock);
722         oop_maps = new OopMapSet();
723         oop_maps-&gt;add_gc_map(call_offset, map);
724         restore_live_registers(sasm, save_fpu_registers);
725       }
726       break;
727 
728     case deoptimize_id:
729       {
730         __ set_info(&quot;deoptimize&quot;, dont_gc_arguments);
731         OopMap* oop_map = save_live_registers(sasm);
732         const Register trap_request = R1;
733         __ ldr(trap_request, Address(SP, arg1_offset));
734         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, deoptimize), trap_request);
735         oop_maps = new OopMapSet();
736         oop_maps-&gt;add_gc_map(call_offset, oop_map);
737         restore_live_registers_without_return(sasm);
738         DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
739         assert(deopt_blob != NULL, &quot;deoptimization blob must have been created&quot;);
740         __ jump(deopt_blob-&gt;unpack_with_reexecution(), relocInfo::runtime_call_type, noreg);
741       }
742       break;
743 
744     case access_field_patching_id:
745       {
746         __ set_info(&quot;access_field_patching&quot;, dont_gc_arguments);
747         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, access_field_patching));
748       }
749       break;
750 
751     case load_klass_patching_id:
752       {
753         __ set_info(&quot;load_klass_patching&quot;, dont_gc_arguments);
754         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_klass_patching));
755       }
756       break;
757 
758     case load_appendix_patching_id:
759       {
760         __ set_info(&quot;load_appendix_patching&quot;, dont_gc_arguments);
761         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_appendix_patching));
762       }
763       break;
764 
765     case load_mirror_patching_id:
766       {
767         __ set_info(&quot;load_mirror_patching&quot;, dont_gc_arguments);
768         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_mirror_patching));
769       }
770       break;
771 
772     case predicate_failed_trap_id:
773       {
774         __ set_info(&quot;predicate_failed_trap&quot;, dont_gc_arguments);
775 
776         OopMap* oop_map = save_live_registers(sasm);
777         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, predicate_failed_trap));
778 
779         oop_maps = new OopMapSet();
780         oop_maps-&gt;add_gc_map(call_offset, oop_map);
781 
782         restore_live_registers_without_return(sasm);
783 
784         DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
785         assert(deopt_blob != NULL, &quot;deoptimization blob must have been created&quot;);
786         __ jump(deopt_blob-&gt;unpack_with_reexecution(), relocInfo::runtime_call_type, Rtemp);
787       }
788       break;
789 
790     default:
791       {
792         __ set_info(&quot;unimplemented entry&quot;, dont_gc_arguments);
793         STOP(&quot;unimplemented entry&quot;);
794       }
795       break;
796   }
797   return oop_maps;
798 }
799 
800 #undef __
801 
802 #ifdef __SOFTFP__
803 const char *Runtime1::pd_name_for_address(address entry) {
804 
805 #define FUNCTION_CASE(a, f) \
806   if ((intptr_t)a == CAST_FROM_FN_PTR(intptr_t, f))  return #f
807 
808   FUNCTION_CASE(entry, __aeabi_fadd_glibc);
809   FUNCTION_CASE(entry, __aeabi_fmul);
810   FUNCTION_CASE(entry, __aeabi_fsub_glibc);
811   FUNCTION_CASE(entry, __aeabi_fdiv);
812 
813   // __aeabi_XXXX_glibc: Imported code from glibc soft-fp bundle for calculation accuracy improvement. See CR 6757269.
814   FUNCTION_CASE(entry, __aeabi_dadd_glibc);
815   FUNCTION_CASE(entry, __aeabi_dmul);
816   FUNCTION_CASE(entry, __aeabi_dsub_glibc);
817   FUNCTION_CASE(entry, __aeabi_ddiv);
818 
819   FUNCTION_CASE(entry, __aeabi_f2d);
820   FUNCTION_CASE(entry, __aeabi_d2f);
821   FUNCTION_CASE(entry, __aeabi_i2f);
822   FUNCTION_CASE(entry, __aeabi_i2d);
823   FUNCTION_CASE(entry, __aeabi_f2iz);
824 
825   FUNCTION_CASE(entry, SharedRuntime::fcmpl);
826   FUNCTION_CASE(entry, SharedRuntime::fcmpg);
827   FUNCTION_CASE(entry, SharedRuntime::dcmpl);
828   FUNCTION_CASE(entry, SharedRuntime::dcmpg);
829 
830   FUNCTION_CASE(entry, SharedRuntime::unordered_fcmplt);
831   FUNCTION_CASE(entry, SharedRuntime::unordered_dcmplt);
832   FUNCTION_CASE(entry, SharedRuntime::unordered_fcmple);
833   FUNCTION_CASE(entry, SharedRuntime::unordered_dcmple);
834   FUNCTION_CASE(entry, SharedRuntime::unordered_fcmpge);
835   FUNCTION_CASE(entry, SharedRuntime::unordered_dcmpge);
836   FUNCTION_CASE(entry, SharedRuntime::unordered_fcmpgt);
837   FUNCTION_CASE(entry, SharedRuntime::unordered_dcmpgt);
838 
839   FUNCTION_CASE(entry, SharedRuntime::fneg);
840   FUNCTION_CASE(entry, SharedRuntime::dneg);
841 
842   FUNCTION_CASE(entry, __aeabi_fcmpeq);
843   FUNCTION_CASE(entry, __aeabi_fcmplt);
844   FUNCTION_CASE(entry, __aeabi_fcmple);
845   FUNCTION_CASE(entry, __aeabi_fcmpge);
846   FUNCTION_CASE(entry, __aeabi_fcmpgt);
847 
848   FUNCTION_CASE(entry, __aeabi_dcmpeq);
849   FUNCTION_CASE(entry, __aeabi_dcmplt);
850   FUNCTION_CASE(entry, __aeabi_dcmple);
851   FUNCTION_CASE(entry, __aeabi_dcmpge);
852   FUNCTION_CASE(entry, __aeabi_dcmpgt);
853 #undef FUNCTION_CASE
854   return &quot;&quot;;
855 }
856 #else  // __SOFTFP__
857 const char *Runtime1::pd_name_for_address(address entry) {
858   return &quot;&lt;unknown function&gt;&quot;;
859 }
860 #endif // __SOFTFP__
    </pre>
  </body>
</html>