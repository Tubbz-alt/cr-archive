diff a/src/hotspot/cpu/arm/arm.ad b/src/hotspot/cpu/arm/arm.ad
--- a/src/hotspot/cpu/arm/arm.ad
+++ b/src/hotspot/cpu/arm/arm.ad
@@ -1,7 +1,7 @@
 //
-// Copyright (c) 2008, 2018, Oracle and/or its affiliates. All rights reserved.
+// Copyright (c) 2008, 2020, Oracle and/or its affiliates. All rights reserved.
 // DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
 //
 // This code is free software; you can redistribute it and/or modify it
 // under the terms of the GNU General Public License version 2 only, as
 // published by the Free Software Foundation.
@@ -349,14 +349,11 @@
   }
   __ raw_pop(FP, LR);
 
   // If this does safepoint polling, then do it here
   if (do_polling() && ra_->C->is_method_compilation()) {
-    // mov_slow here is usually one or two instruction
-    __ mov_address(Rtemp, (address)os::get_polling_page());
-    __ relocate(relocInfo::poll_return_type);
-    __ ldr(Rtemp, Address(Rtemp));
+    __ read_polling_page(Rtemp, relocInfo::poll_return_type);
   }
 }
 
 uint MachEpilogNode::size(PhaseRegAlloc *ra_) const {
   return MachNode::size(ra_);
@@ -969,11 +966,11 @@
   }
 
   return true;  // Per default match rules are supported.
 }
 
-const bool Matcher::match_rule_supported_vector(int opcode, int vlen) {
+const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {
 
   // TODO
   // identify extra cases that we might want to provide match rules for
   // e.g. Op_ vector nodes and other intrinsics while guarding with vlen
   bool ret_value = match_rule_supported(opcode);
@@ -1075,10 +1072,28 @@
 // FIXME: does this handle vector shifts as well?
 const bool Matcher::need_masked_shift_count = true;
 
 const bool Matcher::convi2l_type_required = true;
 
+// No support for generic vector operands.
+const bool Matcher::supports_generic_vector_operands  = false;
+
+MachOper* Matcher::specialize_generic_vector_operand(MachOper* original_opnd, uint ideal_reg, bool is_temp) {
+  ShouldNotReachHere(); // generic vector operands not supported
+  return NULL;
+}
+
+bool Matcher::is_generic_reg2reg_move(MachNode* m) {
+  ShouldNotReachHere();  // generic vector operands not supported
+  return false;
+}
+
+bool Matcher::is_generic_vector(MachOper* opnd)  {
+  ShouldNotReachHere();  // generic vector operands not supported
+  return false;
+}
+
 // Should the Matcher clone shifts on addressing modes, expecting them
 // to be subsumed into complex addressing expressions or compute them
 // into registers?
 bool Matcher::clone_address_expressions(AddPNode* m, Matcher::MStack& mstack, VectorSet& address_visited) {
   return clone_base_plus_offset_address(m, mstack, address_visited);
@@ -1123,12 +1138,11 @@
 
 // No-op on ARM.
 void Matcher::pd_implicit_null_fixup(MachNode *node, uint idx) {
 }
 
-// Advertise here if the CPU requires explicit rounding operations
-// to implement the UseStrictFP mode.
+// Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.
 const bool Matcher::strict_fp_requires_explicit_rounding = false;
 
 // Are floats converted to double when stored to stack during deoptimization?
 // ARM does not handle callee-save floats.
 bool Matcher::float_in_double() {
@@ -2202,10 +2216,34 @@
 
   format %{ %}
   interface(REG_INTER);
 %}
 
+operand R8RegP() %{
+  constraint(ALLOC_IN_RC(R8_regP));
+  match(iRegP);
+
+  format %{ %}
+  interface(REG_INTER);
+%}
+
+operand R9RegP() %{
+  constraint(ALLOC_IN_RC(R9_regP));
+  match(iRegP);
+
+  format %{ %}
+  interface(REG_INTER);
+%}
+
+operand R12RegP() %{
+  constraint(ALLOC_IN_RC(R12_regP));
+  match(iRegP);
+
+  format %{ %}
+  interface(REG_INTER);
+%}
+
 operand R2RegP() %{
   constraint(ALLOC_IN_RC(R2_regP));
   match(iRegP);
 
   format %{ %}
@@ -2234,10 +2272,18 @@
 
   format %{ %}
   interface(REG_INTER);
 %}
 
+operand SPRegP() %{
+  constraint(ALLOC_IN_RC(SP_regP));
+  match(iRegP);
+
+  format %{ %}
+  interface(REG_INTER);
+%}
+
 operand LRRegP() %{
   constraint(ALLOC_IN_RC(LR_regP));
   match(iRegP);
 
   format %{ %}
@@ -4346,11 +4392,12 @@
 %}
 
 // Prefetch instructions.
 // Must be safe to execute with invalid address (cannot fault).
 
-instruct prefetchAlloc( memoryP mem ) %{
+instruct prefetchAlloc_mp( memoryP mem ) %{
+  predicate(VM_Version::has_multiprocessing_extensions());
   match( PrefetchAllocation mem );
   ins_cost(MEMORY_REF_COST);
   size(4);
 
   format %{ "PLDW $mem\t! Prefetch allocation" %}
@@ -4358,10 +4405,24 @@
     __ pldw($mem$$Address);
   %}
   ins_pipe(iload_mem);
 %}
 
+instruct prefetchAlloc_sp( memoryP mem ) %{
+  predicate(!VM_Version::has_multiprocessing_extensions());
+  match( PrefetchAllocation mem );
+  ins_cost(MEMORY_REF_COST);
+  size(4);
+
+  format %{ "PLD $mem\t! Prefetch allocation" %}
+  ins_encode %{
+    __ pld($mem$$Address);
+  %}
+  ins_pipe(iload_mem);
+%}
+
+
 //----------Store Instructions-------------------------------------------------
 // Store Byte
 instruct storeB(memoryB mem, store_RegI src) %{
   match(Set mem (StoreB mem src));
   ins_cost(MEMORY_REF_COST);
@@ -5259,10 +5320,18 @@
   ins_encode( /*empty encoding*/ );
   ins_cost(0);
   ins_pipe(empty);
 %}
 
+instruct castLL( iRegL dst ) %{
+  match(Set dst (CastLL dst));
+  format %{ "! castLL of $dst" %}
+  ins_encode( /*empty encoding*/ );
+  ins_cost(0);
+  ins_pipe(empty);
+%}
+
 //----------Arithmetic Instructions--------------------------------------------
 // Addition Instructions
 // Register Addition
 instruct addI_reg_reg(iRegI dst, iRegI src1, iRegI src2) %{
   match(Set dst (AddI src1 src2));
@@ -10547,11 +10616,11 @@
   %}
 %}
 
 instruct vsl8B_immI(vecD dst, vecD src, immI shift) %{
   predicate(n->as_Vector()->length() == 8);
-  match(Set dst (LShiftVB src shift));
+  match(Set dst (LShiftVB src (LShiftCntV shift)));
   size(4);
   ins_cost(DEFAULT_COST); // FIXME
   format %{
     "VSHL.I8 $dst.D,$src.D,$shift\t! logical left shift packed8B"
   %}
@@ -10563,11 +10632,11 @@
   ins_pipe( ialu_reg_reg ); // FIXME
 %}
 
 instruct vsl16B_immI(vecX dst, vecX src, immI shift) %{
   predicate(n->as_Vector()->length() == 16);
-  match(Set dst (LShiftVB src shift));
+  match(Set dst (LShiftVB src (LShiftCntV shift)));
   size(4);
   ins_cost(DEFAULT_COST); // FIXME
   format %{
     "VSHL.I8 $dst.Q,$src.Q,$shift\t! logical left shift packed16B"
   %}
@@ -10602,11 +10671,11 @@
   %}
 %}
 
 instruct vsl4S_immI(vecD dst, vecD src, immI shift) %{
   predicate(n->as_Vector()->length() == 4);
-  match(Set dst (LShiftVS src shift));
+  match(Set dst (LShiftVS src (LShiftCntV shift)));
   size(4);
   ins_cost(DEFAULT_COST); // FIXME
   format %{
     "VSHL.I16 $dst.D,$src.D,$shift\t! logical left shift packed4S"
   %}
@@ -10657,11 +10726,11 @@
   %}
 %}
 
 instruct vsl2I_immI(vecD dst, vecD src, immI shift) %{
   predicate(n->as_Vector()->length() == 2 && VM_Version::has_simd());
-  match(Set dst (LShiftVI src shift));
+  match(Set dst (LShiftVI src (LShiftCntV shift)));
   size(4);
   ins_cost(DEFAULT_COST); // FIXME
   format %{
     "VSHL.I32 $dst.D,$src.D,$shift\t! logical left shift packed2I"
   %}
@@ -10673,11 +10742,11 @@
   ins_pipe( ialu_reg_reg ); // FIXME
 %}
 
 instruct vsl4I_immI(vecX dst, vecX src, immI shift) %{
   predicate(n->as_Vector()->length() == 4 && VM_Version::has_simd());
-  match(Set dst (LShiftVI src shift));
+  match(Set dst (LShiftVI src (LShiftCntV shift)));
   size(4);
   ins_cost(DEFAULT_COST); // FIXME
   format %{
     "VSHL.I32 $dst.Q,$src.Q,$shift\t! logical left shift packed4I"
   %}
@@ -10701,11 +10770,11 @@
   %}
 %}
 
 instruct vsl2L_immI(vecX dst, vecX src, immI shift) %{
   predicate(n->as_Vector()->length() == 2);
-  match(Set dst (LShiftVL src shift));
+  match(Set dst (LShiftVL src (LShiftCntV shift)));
   size(4);
   ins_cost(DEFAULT_COST); // FIXME
   format %{
     "VSHL.I64 $dst.Q,$src.Q,$shift\t! logical left shift packed2L"
   %}
@@ -10724,11 +10793,11 @@
 // sign extension before a shift.
 
 // Chars vector logical right shift
 instruct vsrl4S_immI(vecD dst, vecD src, immI shift) %{
   predicate(n->as_Vector()->length() == 4);
-  match(Set dst (URShiftVS src shift));
+  match(Set dst (URShiftVS src (RShiftCntV shift)));
   size(4);
   ins_cost(DEFAULT_COST); // FIXME
   format %{
     "VSHR.U16 $dst.D,$src.D,$shift\t! logical right shift packed4S"
   %}
@@ -10740,11 +10809,11 @@
   ins_pipe( ialu_reg_reg ); // FIXME
 %}
 
 instruct vsrl8S_immI(vecX dst, vecX src, immI shift) %{
   predicate(n->as_Vector()->length() == 8);
-  match(Set dst (URShiftVS src shift));
+  match(Set dst (URShiftVS src (RShiftCntV shift)));
   size(4);
   ins_cost(DEFAULT_COST); // FIXME
   format %{
     "VSHR.U16 $dst.Q,$src.Q,$shift\t! logical right shift packed8S"
   %}
@@ -10757,11 +10826,11 @@
 %}
 
 // Integers vector logical right shift
 instruct vsrl2I_immI(vecD dst, vecD src, immI shift) %{
   predicate(n->as_Vector()->length() == 2 && VM_Version::has_simd());
-  match(Set dst (URShiftVI src shift));
+  match(Set dst (URShiftVI src (RShiftCntV shift)));
   size(4);
   ins_cost(DEFAULT_COST); // FIXME
   format %{
     "VSHR.U32 $dst.D,$src.D,$shift\t! logical right shift packed2I"
   %}
@@ -10773,11 +10842,11 @@
   ins_pipe( ialu_reg_reg ); // FIXME
 %}
 
 instruct vsrl4I_immI(vecX dst, vecX src, immI shift) %{
   predicate(n->as_Vector()->length() == 4 && VM_Version::has_simd());
-  match(Set dst (URShiftVI src shift));
+  match(Set dst (URShiftVI src (RShiftCntV shift)));
   size(4);
   ins_cost(DEFAULT_COST); // FIXME
   format %{
     "VSHR.U32 $dst.Q,$src.Q,$shift\t! logical right shift packed4I"
   %}
@@ -10790,11 +10859,11 @@
 %}
 
 // Longs vector logical right shift
 instruct vsrl2L_immI(vecX dst, vecX src, immI shift) %{
   predicate(n->as_Vector()->length() == 2);
-  match(Set dst (URShiftVL src shift));
+  match(Set dst (URShiftVL src (RShiftCntV shift)));
   size(4);
   ins_cost(DEFAULT_COST); // FIXME
   format %{
     "VSHR.U64 $dst.Q,$src.Q,$shift\t! logical right shift packed2L"
   %}
