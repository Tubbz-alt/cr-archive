<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/arm/sharedRuntime_arm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
<a name="1" id="anc1"></a><span class="line-modified">   2  * Copyright (c) 2008, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/assembler.hpp&quot;
  27 #include &quot;assembler_arm.inline.hpp&quot;
  28 #include &quot;code/debugInfoRec.hpp&quot;
  29 #include &quot;code/icBuffer.hpp&quot;
  30 #include &quot;code/vtableStubs.hpp&quot;
  31 #include &quot;interpreter/interpreter.hpp&quot;
  32 #include &quot;logging/log.hpp&quot;
  33 #include &quot;memory/resourceArea.hpp&quot;
  34 #include &quot;oops/compiledICHolder.hpp&quot;
<a name="2" id="anc2"></a><span class="line-added">  35 #include &quot;oops/klass.inline.hpp&quot;</span>
  36 #include &quot;runtime/sharedRuntime.hpp&quot;
<a name="3" id="anc3"></a><span class="line-added">  37 #include &quot;runtime/safepointMechanism.hpp&quot;</span>
  38 #include &quot;runtime/vframeArray.hpp&quot;
  39 #include &quot;utilities/align.hpp&quot;
<a name="4" id="anc4"></a><span class="line-added">  40 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  41 #include &quot;vmreg_arm.inline.hpp&quot;
  42 #ifdef COMPILER1
  43 #include &quot;c1/c1_Runtime1.hpp&quot;
  44 #endif
  45 #ifdef COMPILER2
  46 #include &quot;opto/runtime.hpp&quot;
  47 #endif
  48 
  49 #define __ masm-&gt;
  50 
  51 class RegisterSaver {
  52 public:
  53 
  54   // Special registers:
  55   //              32-bit ARM     64-bit ARM
  56   //  Rthread:       R10            R28
  57   //  LR:            R14            R30
  58 
  59   // Rthread is callee saved in the C ABI and never changed by compiled code:
  60   // no need to save it.
  61 
  62   // 2 slots for LR: the one at LR_offset and an other one at R14/R30_offset.
  63   // The one at LR_offset is a return address that is needed by stack walking.
  64   // A c2 method uses LR as a standard register so it may be live when we
  65   // branch to the runtime. The slot at R14/R30_offset is for the value of LR
  66   // in case it&#39;s live in the method we are coming from.
  67 
  68 
  69   enum RegisterLayout {
  70     fpu_save_size = FloatRegisterImpl::number_of_registers,
  71 #ifndef __SOFTFP__
  72     D0_offset = 0,
  73 #endif
  74     R0_offset = fpu_save_size,
  75     R1_offset,
  76     R2_offset,
  77     R3_offset,
  78     R4_offset,
  79     R5_offset,
  80     R6_offset,
  81 #if (FP_REG_NUM != 7)
  82     // if not saved as FP
  83     R7_offset,
  84 #endif
  85     R8_offset,
  86     R9_offset,
  87 #if (FP_REG_NUM != 11)
  88     // if not saved as FP
  89     R11_offset,
  90 #endif
  91     R12_offset,
  92     R14_offset,
  93     FP_offset,
  94     LR_offset,
  95     reg_save_size,
  96 
  97     Rmethod_offset = R9_offset,
  98     Rtemp_offset = R12_offset,
  99   };
 100 
 101   // all regs but Rthread (R10), FP (R7 or R11), SP and PC
 102   // (altFP_7_11 is the one amoung R7 and R11 which is not FP)
 103 #define SAVED_BASE_REGS (RegisterSet(R0, R6) | RegisterSet(R8, R9) | RegisterSet(R12) | R14 | altFP_7_11)
 104 
 105 
 106   //  When LR may be live in the nmethod from which we are comming
 107   //  then lr_saved is true, the return address is saved before the
 108   //  call to save_live_register by the caller and LR contains the
 109   //  live value.
 110 
 111   static OopMap* save_live_registers(MacroAssembler* masm,
 112                                      int* total_frame_words,
 113                                      bool lr_saved = false);
 114   static void restore_live_registers(MacroAssembler* masm, bool restore_lr = true);
 115 
 116 };
 117 
 118 
 119 
 120 
 121 OopMap* RegisterSaver::save_live_registers(MacroAssembler* masm,
 122                                            int* total_frame_words,
 123                                            bool lr_saved) {
 124   *total_frame_words = reg_save_size;
 125 
 126   OopMapSet *oop_maps = new OopMapSet();
 127   OopMap* map = new OopMap(VMRegImpl::slots_per_word * (*total_frame_words), 0);
 128 
 129   if (lr_saved) {
 130     __ push(RegisterSet(FP));
 131   } else {
 132     __ push(RegisterSet(FP) | RegisterSet(LR));
 133   }
 134   __ push(SAVED_BASE_REGS);
 135   if (HaveVFP) {
 136     if (VM_Version::has_vfp3_32()) {
<a name="5" id="anc5"></a><span class="line-modified"> 137       __ fpush(FloatRegisterSet(D16, 16));</span>
 138     } else {
 139       if (FloatRegisterImpl::number_of_registers &gt; 32) {
 140         assert(FloatRegisterImpl::number_of_registers == 64, &quot;nb fp registers should be 64&quot;);
 141         __ sub(SP, SP, 32 * wordSize);
 142       }
 143     }
<a name="6" id="anc6"></a><span class="line-modified"> 144     __ fpush(FloatRegisterSet(D0, 16));</span>
 145   } else {
 146     __ sub(SP, SP, fpu_save_size * wordSize);
 147   }
 148 
 149   int i;
 150   int j=0;
 151   for (i = R0_offset; i &lt;= R9_offset; i++) {
 152     if (j == FP_REG_NUM) {
 153       // skip the FP register, managed below.
 154       j++;
 155     }
 156     map-&gt;set_callee_saved(VMRegImpl::stack2reg(i), as_Register(j)-&gt;as_VMReg());
 157     j++;
 158   }
 159   assert(j == R10-&gt;encoding(), &quot;must be&quot;);
 160 #if (FP_REG_NUM != 11)
 161   // add R11, if not managed as FP
 162   map-&gt;set_callee_saved(VMRegImpl::stack2reg(R11_offset), R11-&gt;as_VMReg());
 163 #endif
 164   map-&gt;set_callee_saved(VMRegImpl::stack2reg(R12_offset), R12-&gt;as_VMReg());
 165   map-&gt;set_callee_saved(VMRegImpl::stack2reg(R14_offset), R14-&gt;as_VMReg());
 166   if (HaveVFP) {
 167     for (i = 0; i &lt; (VM_Version::has_vfp3_32() ? 64 : 32); i+=2) {
 168       map-&gt;set_callee_saved(VMRegImpl::stack2reg(i), as_FloatRegister(i)-&gt;as_VMReg());
 169       map-&gt;set_callee_saved(VMRegImpl::stack2reg(i + 1), as_FloatRegister(i)-&gt;as_VMReg()-&gt;next());
 170     }
 171   }
 172 
 173   return map;
 174 }
 175 
 176 void RegisterSaver::restore_live_registers(MacroAssembler* masm, bool restore_lr) {
 177   if (HaveVFP) {
<a name="7" id="anc7"></a><span class="line-modified"> 178     __ fpop(FloatRegisterSet(D0, 16));</span>
 179     if (VM_Version::has_vfp3_32()) {
<a name="8" id="anc8"></a><span class="line-modified"> 180       __ fpop(FloatRegisterSet(D16, 16));</span>
 181     } else {
 182       if (FloatRegisterImpl::number_of_registers &gt; 32) {
 183         assert(FloatRegisterImpl::number_of_registers == 64, &quot;nb fp registers should be 64&quot;);
 184         __ add(SP, SP, 32 * wordSize);
 185       }
 186     }
 187   } else {
 188     __ add(SP, SP, fpu_save_size * wordSize);
 189   }
 190   __ pop(SAVED_BASE_REGS);
 191   if (restore_lr) {
 192     __ pop(RegisterSet(FP) | RegisterSet(LR));
 193   } else {
 194     __ pop(RegisterSet(FP));
 195   }
 196 }
 197 
 198 
 199 static void push_result_registers(MacroAssembler* masm, BasicType ret_type) {
 200 #ifdef __ABI_HARD__
 201   if (ret_type == T_DOUBLE || ret_type == T_FLOAT) {
 202     __ sub(SP, SP, 8);
 203     __ fstd(D0, Address(SP));
 204     return;
 205   }
 206 #endif // __ABI_HARD__
 207   __ raw_push(R0, R1);
 208 }
 209 
 210 static void pop_result_registers(MacroAssembler* masm, BasicType ret_type) {
 211 #ifdef __ABI_HARD__
 212   if (ret_type == T_DOUBLE || ret_type == T_FLOAT) {
 213     __ fldd(D0, Address(SP));
 214     __ add(SP, SP, 8);
 215     return;
 216   }
 217 #endif // __ABI_HARD__
 218   __ raw_pop(R0, R1);
 219 }
 220 
 221 static void push_param_registers(MacroAssembler* masm, int fp_regs_in_arguments) {
 222   // R1-R3 arguments need to be saved, but we push 4 registers for 8-byte alignment
 223   __ push(RegisterSet(R0, R3));
 224 
<a name="9" id="anc9"></a>
 225   // preserve arguments
 226   // Likely not needed as the locking code won&#39;t probably modify volatile FP registers,
 227   // but there is no way to guarantee that
 228   if (fp_regs_in_arguments) {
 229     // convert fp_regs_in_arguments to a number of double registers
 230     int double_regs_num = (fp_regs_in_arguments + 1) &gt;&gt; 1;
<a name="10" id="anc10"></a><span class="line-modified"> 231     __ fpush_hardfp(FloatRegisterSet(D0, double_regs_num));</span>
 232   }
<a name="11" id="anc11"></a>
 233 }
 234 
 235 static void pop_param_registers(MacroAssembler* masm, int fp_regs_in_arguments) {
<a name="12" id="anc12"></a>
 236   if (fp_regs_in_arguments) {
 237     int double_regs_num = (fp_regs_in_arguments + 1) &gt;&gt; 1;
<a name="13" id="anc13"></a><span class="line-modified"> 238     __ fpop_hardfp(FloatRegisterSet(D0, double_regs_num));</span>
 239   }
<a name="14" id="anc14"></a>

 240   __ pop(RegisterSet(R0, R3));
 241 }
 242 
 243 
 244 
 245 // Is vector&#39;s size (in bytes) bigger than a size saved by default?
 246 // All vector registers are saved by default on ARM.
 247 bool SharedRuntime::is_wide_vector(int size) {
 248   return false;
 249 }
 250 
 251 size_t SharedRuntime::trampoline_size() {
 252   return 16;
 253 }
 254 
 255 void SharedRuntime::generate_trampoline(MacroAssembler *masm, address destination) {
 256   InlinedAddress dest(destination);
 257   __ indirect_jump(dest, Rtemp);
 258   __ bind_literal(dest);
 259 }
 260 
 261 int SharedRuntime::c_calling_convention(const BasicType *sig_bt,
 262                                         VMRegPair *regs,
 263                                         VMRegPair *regs2,
 264                                         int total_args_passed) {
 265   assert(regs2 == NULL, &quot;not needed on arm&quot;);
 266 
 267   int slot = 0;
 268   int ireg = 0;
 269 #ifdef __ABI_HARD__
 270   int fp_slot = 0;
 271   int single_fpr_slot = 0;
 272 #endif // __ABI_HARD__
 273   for (int i = 0; i &lt; total_args_passed; i++) {
 274     switch (sig_bt[i]) {
 275     case T_SHORT:
 276     case T_CHAR:
 277     case T_BYTE:
 278     case T_BOOLEAN:
 279     case T_INT:
 280     case T_ARRAY:
 281     case T_OBJECT:
 282     case T_ADDRESS:
 283     case T_METADATA:
 284 #ifndef __ABI_HARD__
 285     case T_FLOAT:
 286 #endif // !__ABI_HARD__
 287       if (ireg &lt; 4) {
 288         Register r = as_Register(ireg);
 289         regs[i].set1(r-&gt;as_VMReg());
 290         ireg++;
 291       } else {
 292         regs[i].set1(VMRegImpl::stack2reg(slot));
 293         slot++;
 294       }
 295       break;
 296     case T_LONG:
 297 #ifndef __ABI_HARD__
 298     case T_DOUBLE:
 299 #endif // !__ABI_HARD__
 300       assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i+1] == T_VOID, &quot;missing Half&quot; );
 301       if (ireg &lt;= 2) {
 302 #if (ALIGN_WIDE_ARGUMENTS == 1)
 303         if(ireg &amp; 1) ireg++;  // Aligned location required
 304 #endif
 305         Register r1 = as_Register(ireg);
 306         Register r2 = as_Register(ireg + 1);
 307         regs[i].set_pair(r2-&gt;as_VMReg(), r1-&gt;as_VMReg());
 308         ireg += 2;
 309 #if (ALIGN_WIDE_ARGUMENTS == 0)
 310       } else if (ireg == 3) {
 311         // uses R3 + one stack slot
 312         Register r = as_Register(ireg);
 313         regs[i].set_pair(VMRegImpl::stack2reg(slot), r-&gt;as_VMReg());
 314         ireg += 1;
 315         slot += 1;
 316 #endif
 317       } else {
 318         if (slot &amp; 1) slot++; // Aligned location required
 319         regs[i].set_pair(VMRegImpl::stack2reg(slot+1), VMRegImpl::stack2reg(slot));
 320         slot += 2;
 321         ireg = 4;
 322       }
 323       break;
 324     case T_VOID:
 325       regs[i].set_bad();
 326       break;
 327 #ifdef __ABI_HARD__
 328     case T_FLOAT:
 329       if ((fp_slot &lt; 16)||(single_fpr_slot &amp; 1)) {
 330         if ((single_fpr_slot &amp; 1) == 0) {
 331           single_fpr_slot = fp_slot;
 332           fp_slot += 2;
 333         }
 334         FloatRegister r = as_FloatRegister(single_fpr_slot);
 335         single_fpr_slot++;
 336         regs[i].set1(r-&gt;as_VMReg());
 337       } else {
 338         regs[i].set1(VMRegImpl::stack2reg(slot));
 339         slot++;
 340       }
 341       break;
 342     case T_DOUBLE:
 343       assert(ALIGN_WIDE_ARGUMENTS == 1, &quot;ABI_HARD not supported with unaligned wide arguments&quot;);
 344       if (fp_slot &lt;= 14) {
 345         FloatRegister r1 = as_FloatRegister(fp_slot);
 346         FloatRegister r2 = as_FloatRegister(fp_slot+1);
 347         regs[i].set_pair(r2-&gt;as_VMReg(), r1-&gt;as_VMReg());
 348         fp_slot += 2;
 349       } else {
 350         if(slot &amp; 1) slot++;
 351         regs[i].set_pair(VMRegImpl::stack2reg(slot+1), VMRegImpl::stack2reg(slot));
 352         slot += 2;
 353         single_fpr_slot = 16;
 354       }
 355       break;
 356 #endif // __ABI_HARD__
 357     default:
 358       ShouldNotReachHere();
 359     }
 360   }
 361   return slot;
 362 }
 363 
 364 int SharedRuntime::java_calling_convention(const BasicType *sig_bt,
 365                                            VMRegPair *regs,
 366                                            int total_args_passed,
 367                                            int is_outgoing) {
 368 #ifdef __SOFTFP__
 369   // soft float is the same as the C calling convention.
 370   return c_calling_convention(sig_bt, regs, NULL, total_args_passed);
 371 #endif // __SOFTFP__
 372   (void) is_outgoing;
 373   int slot = 0;
 374   int ireg = 0;
 375   int freg = 0;
 376   int single_fpr = 0;
 377 
 378   for (int i = 0; i &lt; total_args_passed; i++) {
 379     switch (sig_bt[i]) {
 380     case T_SHORT:
 381     case T_CHAR:
 382     case T_BYTE:
 383     case T_BOOLEAN:
 384     case T_INT:
 385     case T_ARRAY:
 386     case T_OBJECT:
 387     case T_ADDRESS:
 388       if (ireg &lt; 4) {
 389         Register r = as_Register(ireg++);
 390         regs[i].set1(r-&gt;as_VMReg());
 391       } else {
 392         regs[i].set1(VMRegImpl::stack2reg(slot++));
 393       }
 394       break;
 395     case T_FLOAT:
 396       // C2 utilizes S14/S15 for mem-mem moves
 397       if ((freg &lt; 16 COMPILER2_PRESENT(-2)) || (single_fpr &amp; 1)) {
 398         if ((single_fpr &amp; 1) == 0) {
 399           single_fpr = freg;
 400           freg += 2;
 401         }
 402         FloatRegister r = as_FloatRegister(single_fpr++);
 403         regs[i].set1(r-&gt;as_VMReg());
 404       } else {
 405         regs[i].set1(VMRegImpl::stack2reg(slot++));
 406       }
 407       break;
 408     case T_DOUBLE:
 409       // C2 utilizes S14/S15 for mem-mem moves
 410       if (freg &lt;= 14 COMPILER2_PRESENT(-2)) {
 411         FloatRegister r1 = as_FloatRegister(freg);
 412         FloatRegister r2 = as_FloatRegister(freg + 1);
 413         regs[i].set_pair(r2-&gt;as_VMReg(), r1-&gt;as_VMReg());
 414         freg += 2;
 415       } else {
 416         // Keep internally the aligned calling convention,
 417         // ignoring ALIGN_WIDE_ARGUMENTS
 418         if (slot &amp; 1) slot++;
 419         regs[i].set_pair(VMRegImpl::stack2reg(slot + 1), VMRegImpl::stack2reg(slot));
 420         slot += 2;
 421         single_fpr = 16;
 422       }
 423       break;
 424     case T_LONG:
 425       // Keep internally the aligned calling convention,
 426       // ignoring ALIGN_WIDE_ARGUMENTS
 427       if (ireg &lt;= 2) {
 428         if (ireg &amp; 1) ireg++;
 429         Register r1 = as_Register(ireg);
 430         Register r2 = as_Register(ireg + 1);
 431         regs[i].set_pair(r2-&gt;as_VMReg(), r1-&gt;as_VMReg());
 432         ireg += 2;
 433       } else {
 434         if (slot &amp; 1) slot++;
 435         regs[i].set_pair(VMRegImpl::stack2reg(slot + 1), VMRegImpl::stack2reg(slot));
 436         slot += 2;
 437         ireg = 4;
 438       }
 439       break;
 440     case T_VOID:
 441       regs[i].set_bad();
 442       break;
 443     default:
 444       ShouldNotReachHere();
 445     }
 446   }
 447 
 448   if (slot &amp; 1) slot++;
 449   return slot;
 450 }
 451 
 452 static void patch_callers_callsite(MacroAssembler *masm) {
 453   Label skip;
 454 
 455   __ ldr(Rtemp, Address(Rmethod, Method::code_offset()));
 456   __ cbz(Rtemp, skip);
 457 
 458   // Pushing an even number of registers for stack alignment.
 459   // Selecting R9, which had to be saved anyway for some platforms.
 460   __ push(RegisterSet(R0, R3) | R9 | LR);
<a name="15" id="anc15"></a><span class="line-added"> 461   __ fpush_hardfp(FloatRegisterSet(D0, 8));</span>
 462 
 463   __ mov(R0, Rmethod);
 464   __ mov(R1, LR);
 465   __ call(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite));
 466 
<a name="16" id="anc16"></a><span class="line-added"> 467   __ fpop_hardfp(FloatRegisterSet(D0, 8));</span>
 468   __ pop(RegisterSet(R0, R3) | R9 | LR);
 469 
 470   __ bind(skip);
 471 }
 472 
 473 void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,
 474                                     int total_args_passed, int comp_args_on_stack,
 475                                     const BasicType *sig_bt, const VMRegPair *regs) {
 476   // TODO: ARM - May be can use ldm to load arguments
 477   const Register tmp = Rtemp; // avoid erasing R5_mh
 478 
 479   // Next assert may not be needed but safer. Extra analysis required
 480   // if this there is not enough free registers and we need to use R5 here.
 481   assert_different_registers(tmp, R5_mh);
 482 
 483   // 6243940 We might end up in handle_wrong_method if
 484   // the callee is deoptimized as we race thru here. If that
 485   // happens we don&#39;t want to take a safepoint because the
 486   // caller frame will look interpreted and arguments are now
 487   // &quot;compiled&quot; so it is much better to make this transition
 488   // invisible to the stack walking code. Unfortunately if
 489   // we try and find the callee by normal means a safepoint
 490   // is possible. So we stash the desired callee in the thread
 491   // and the vm will find there should this case occur.
 492   Address callee_target_addr(Rthread, JavaThread::callee_target_offset());
 493   __ str(Rmethod, callee_target_addr);
 494 
 495 
 496   assert_different_registers(tmp, R0, R1, R2, R3, Rsender_sp, Rmethod);
 497 
 498   const Register initial_sp = Rmethod; // temporarily scratched
 499 
 500   // Old code was modifying R4 but this looks unsafe (particularly with JSR292)
 501   assert_different_registers(tmp, R0, R1, R2, R3, Rsender_sp, initial_sp);
 502 
 503   __ mov(initial_sp, SP);
 504 
 505   if (comp_args_on_stack) {
 506     __ sub_slow(SP, SP, comp_args_on_stack * VMRegImpl::stack_slot_size);
 507   }
 508   __ bic(SP, SP, StackAlignmentInBytes - 1);
 509 
 510   for (int i = 0; i &lt; total_args_passed; i++) {
 511     if (sig_bt[i] == T_VOID) {
 512       assert(i &gt; 0 &amp;&amp; (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), &quot;missing half&quot;);
 513       continue;
 514     }
 515     assert(!regs[i].second()-&gt;is_valid() || regs[i].first()-&gt;next() == regs[i].second(), &quot;must be ordered&quot;);
 516     int arg_offset = Interpreter::expr_offset_in_bytes(total_args_passed - 1 - i);
 517 
 518     VMReg r_1 = regs[i].first();
 519     VMReg r_2 = regs[i].second();
 520     if (r_1-&gt;is_stack()) {
 521       int stack_offset = r_1-&gt;reg2stack() * VMRegImpl::stack_slot_size;
 522       if (!r_2-&gt;is_valid()) {
 523         __ ldr(tmp, Address(initial_sp, arg_offset));
 524         __ str(tmp, Address(SP, stack_offset));
 525       } else {
 526         __ ldr(tmp, Address(initial_sp, arg_offset - Interpreter::stackElementSize));
 527         __ str(tmp, Address(SP, stack_offset));
 528         __ ldr(tmp, Address(initial_sp, arg_offset));
 529         __ str(tmp, Address(SP, stack_offset + wordSize));
 530       }
 531     } else if (r_1-&gt;is_Register()) {
 532       if (!r_2-&gt;is_valid()) {
 533         __ ldr(r_1-&gt;as_Register(), Address(initial_sp, arg_offset));
 534       } else {
 535         __ ldr(r_1-&gt;as_Register(), Address(initial_sp, arg_offset - Interpreter::stackElementSize));
 536         __ ldr(r_2-&gt;as_Register(), Address(initial_sp, arg_offset));
 537       }
 538     } else if (r_1-&gt;is_FloatRegister()) {
 539 #ifdef __SOFTFP__
 540       ShouldNotReachHere();
 541 #endif // __SOFTFP__
 542       if (!r_2-&gt;is_valid()) {
 543         __ flds(r_1-&gt;as_FloatRegister(), Address(initial_sp, arg_offset));
 544       } else {
 545         __ fldd(r_1-&gt;as_FloatRegister(), Address(initial_sp, arg_offset - Interpreter::stackElementSize));
 546       }
 547     } else {
 548       assert(!r_1-&gt;is_valid() &amp;&amp; !r_2-&gt;is_valid(), &quot;must be&quot;);
 549     }
 550   }
 551 
 552   // restore Rmethod (scratched for initial_sp)
 553   __ ldr(Rmethod, callee_target_addr);
 554   __ ldr(PC, Address(Rmethod, Method::from_compiled_offset()));
 555 
 556 }
 557 
 558 static void gen_c2i_adapter(MacroAssembler *masm,
 559                             int total_args_passed,  int comp_args_on_stack,
 560                             const BasicType *sig_bt, const VMRegPair *regs,
 561                             Label&amp; skip_fixup) {
 562   // TODO: ARM - May be can use stm to deoptimize arguments
 563   const Register tmp = Rtemp;
 564 
 565   patch_callers_callsite(masm);
 566   __ bind(skip_fixup);
 567 
 568   __ mov(Rsender_sp, SP); // not yet saved
 569 
 570 
 571   int extraspace = total_args_passed * Interpreter::stackElementSize;
 572   if (extraspace) {
 573     __ sub_slow(SP, SP, extraspace);
 574   }
 575 
 576   for (int i = 0; i &lt; total_args_passed; i++) {
 577     if (sig_bt[i] == T_VOID) {
 578       assert(i &gt; 0 &amp;&amp; (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), &quot;missing half&quot;);
 579       continue;
 580     }
 581     int stack_offset = (total_args_passed - 1 - i) * Interpreter::stackElementSize;
 582 
 583     VMReg r_1 = regs[i].first();
 584     VMReg r_2 = regs[i].second();
 585     if (r_1-&gt;is_stack()) {
 586       int arg_offset = r_1-&gt;reg2stack() * VMRegImpl::stack_slot_size + extraspace;
 587       if (!r_2-&gt;is_valid()) {
 588         __ ldr(tmp, Address(SP, arg_offset));
 589         __ str(tmp, Address(SP, stack_offset));
 590       } else {
 591         __ ldr(tmp, Address(SP, arg_offset));
 592         __ str(tmp, Address(SP, stack_offset - Interpreter::stackElementSize));
 593         __ ldr(tmp, Address(SP, arg_offset + wordSize));
 594         __ str(tmp, Address(SP, stack_offset));
 595       }
 596     } else if (r_1-&gt;is_Register()) {
 597       if (!r_2-&gt;is_valid()) {
 598         __ str(r_1-&gt;as_Register(), Address(SP, stack_offset));
 599       } else {
 600         __ str(r_1-&gt;as_Register(), Address(SP, stack_offset - Interpreter::stackElementSize));
 601         __ str(r_2-&gt;as_Register(), Address(SP, stack_offset));
 602       }
 603     } else if (r_1-&gt;is_FloatRegister()) {
 604 #ifdef __SOFTFP__
 605       ShouldNotReachHere();
 606 #endif // __SOFTFP__
 607       if (!r_2-&gt;is_valid()) {
 608         __ fsts(r_1-&gt;as_FloatRegister(), Address(SP, stack_offset));
 609       } else {
 610         __ fstd(r_1-&gt;as_FloatRegister(), Address(SP, stack_offset - Interpreter::stackElementSize));
 611       }
 612     } else {
 613       assert(!r_1-&gt;is_valid() &amp;&amp; !r_2-&gt;is_valid(), &quot;must be&quot;);
 614     }
 615   }
 616 
 617   __ ldr(PC, Address(Rmethod, Method::interpreter_entry_offset()));
 618 
 619 }
 620 
 621 AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,
 622                                                             int total_args_passed,
 623                                                             int comp_args_on_stack,
 624                                                             const BasicType *sig_bt,
 625                                                             const VMRegPair *regs,
 626                                                             AdapterFingerPrint* fingerprint) {
 627   address i2c_entry = __ pc();
 628   gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);
 629 
 630   address c2i_unverified_entry = __ pc();
 631   Label skip_fixup;
 632   const Register receiver       = R0;
 633   const Register holder_klass   = Rtemp; // XXX should be OK for C2 but not 100% sure
 634   const Register receiver_klass = R4;
 635 
 636   __ load_klass(receiver_klass, receiver);
 637   __ ldr(holder_klass, Address(Ricklass, CompiledICHolder::holder_klass_offset()));
 638   __ ldr(Rmethod, Address(Ricklass, CompiledICHolder::holder_metadata_offset()));
 639   __ cmp(receiver_klass, holder_klass);
 640 
 641   __ ldr(Rtemp, Address(Rmethod, Method::code_offset()), eq);
 642   __ cmp(Rtemp, 0, eq);
 643   __ b(skip_fixup, eq);
 644   __ jump(SharedRuntime::get_ic_miss_stub(), relocInfo::runtime_call_type, noreg, ne);
 645 
 646   address c2i_entry = __ pc();
 647   gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);
 648 
 649   __ flush();
 650   return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry);
 651 }
 652 
 653 
 654 static int reg2offset_in(VMReg r) {
 655   // Account for saved FP and LR
 656   return r-&gt;reg2stack() * VMRegImpl::stack_slot_size + 2*wordSize;
 657 }
 658 
 659 static int reg2offset_out(VMReg r) {
 660   return (r-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;
 661 }
 662 
 663 
 664 static void verify_oop_args(MacroAssembler* masm,
 665                             const methodHandle&amp; method,
 666                             const BasicType* sig_bt,
 667                             const VMRegPair* regs) {
 668   Register temp_reg = Rmethod;  // not part of any compiled calling seq
 669   if (VerifyOops) {
 670     for (int i = 0; i &lt; method-&gt;size_of_parameters(); i++) {
 671       if (sig_bt[i] == T_OBJECT || sig_bt[i] == T_ARRAY) {
 672         VMReg r = regs[i].first();
 673         assert(r-&gt;is_valid(), &quot;bad oop arg&quot;);
 674         if (r-&gt;is_stack()) {
 675           __ ldr(temp_reg, Address(SP, r-&gt;reg2stack() * VMRegImpl::stack_slot_size));
 676           __ verify_oop(temp_reg);
 677         } else {
 678           __ verify_oop(r-&gt;as_Register());
 679         }
 680       }
 681     }
 682   }
 683 }
 684 
 685 static void gen_special_dispatch(MacroAssembler* masm,
 686                                  const methodHandle&amp; method,
 687                                  const BasicType* sig_bt,
 688                                  const VMRegPair* regs) {
 689   verify_oop_args(masm, method, sig_bt, regs);
 690   vmIntrinsics::ID iid = method-&gt;intrinsic_id();
 691 
 692   // Now write the args into the outgoing interpreter space
 693   bool     has_receiver   = false;
 694   Register receiver_reg   = noreg;
 695   int      member_arg_pos = -1;
 696   Register member_reg     = noreg;
 697   int      ref_kind       = MethodHandles::signature_polymorphic_intrinsic_ref_kind(iid);
 698   if (ref_kind != 0) {
 699     member_arg_pos = method-&gt;size_of_parameters() - 1;  // trailing MemberName argument
 700     member_reg = Rmethod;  // known to be free at this point
 701     has_receiver = MethodHandles::ref_kind_has_receiver(ref_kind);
 702   } else if (iid == vmIntrinsics::_invokeBasic) {
 703     has_receiver = true;
 704   } else {
 705     fatal(&quot;unexpected intrinsic id %d&quot;, iid);
 706   }
 707 
 708   if (member_reg != noreg) {
 709     // Load the member_arg into register, if necessary.
 710     SharedRuntime::check_member_name_argument_is_last_argument(method, sig_bt, regs);
 711     VMReg r = regs[member_arg_pos].first();
 712     if (r-&gt;is_stack()) {
 713       __ ldr(member_reg, Address(SP, r-&gt;reg2stack() * VMRegImpl::stack_slot_size));
 714     } else {
 715       // no data motion is needed
 716       member_reg = r-&gt;as_Register();
 717     }
 718   }
 719 
 720   if (has_receiver) {
 721     // Make sure the receiver is loaded into a register.
 722     assert(method-&gt;size_of_parameters() &gt; 0, &quot;oob&quot;);
 723     assert(sig_bt[0] == T_OBJECT, &quot;receiver argument must be an object&quot;);
 724     VMReg r = regs[0].first();
 725     assert(r-&gt;is_valid(), &quot;bad receiver arg&quot;);
 726     if (r-&gt;is_stack()) {
 727       // Porting note:  This assumes that compiled calling conventions always
 728       // pass the receiver oop in a register.  If this is not true on some
 729       // platform, pick a temp and load the receiver from stack.
 730       assert(false, &quot;receiver always in a register&quot;);
 731       receiver_reg = j_rarg0;  // known to be free at this point
 732       __ ldr(receiver_reg, Address(SP, r-&gt;reg2stack() * VMRegImpl::stack_slot_size));
 733     } else {
 734       // no data motion is needed
 735       receiver_reg = r-&gt;as_Register();
 736     }
 737   }
 738 
 739   // Figure out which address we are really jumping to:
 740   MethodHandles::generate_method_handle_dispatch(masm, iid,
 741                                                  receiver_reg, member_reg, /*for_compiler_entry:*/ true);
 742 }
 743 
 744 // ---------------------------------------------------------------------------
 745 // Generate a native wrapper for a given method.  The method takes arguments
 746 // in the Java compiled code convention, marshals them to the native
 747 // convention (handlizes oops, etc), transitions to native, makes the call,
 748 // returns to java state (possibly blocking), unhandlizes any result and
 749 // returns.
 750 nmethod* SharedRuntime::generate_native_wrapper(MacroAssembler* masm,
 751                                                 const methodHandle&amp; method,
 752                                                 int compile_id,
 753                                                 BasicType* in_sig_bt,
 754                                                 VMRegPair* in_regs,
<a name="17" id="anc17"></a><span class="line-modified"> 755                                                 BasicType ret_type,</span>
<span class="line-added"> 756                                                 address critical_entry) {</span>
 757   if (method-&gt;is_method_handle_intrinsic()) {
 758     vmIntrinsics::ID iid = method-&gt;intrinsic_id();
 759     intptr_t start = (intptr_t)__ pc();
 760     int vep_offset = ((intptr_t)__ pc()) - start;
 761     gen_special_dispatch(masm,
 762                          method,
 763                          in_sig_bt,
 764                          in_regs);
 765     int frame_complete = ((intptr_t)__ pc()) - start;  // not complete, period
 766     __ flush();
 767     int stack_slots = SharedRuntime::out_preserve_stack_slots();  // no out slots at all, actually
 768     return nmethod::new_native_nmethod(method,
 769                                        compile_id,
 770                                        masm-&gt;code(),
 771                                        vep_offset,
 772                                        frame_complete,
 773                                        stack_slots / VMRegImpl::slots_per_word,
 774                                        in_ByteSize(-1),
 775                                        in_ByteSize(-1),
 776                                        (OopMapSet*)NULL);
 777   }
 778   // Arguments for JNI method include JNIEnv and Class if static
 779 
 780   // Usage of Rtemp should be OK since scratched by native call
 781 
 782   bool is_static = method-&gt;is_static();
 783 
 784   const int total_in_args = method-&gt;size_of_parameters();
 785   int total_c_args = total_in_args + 1;
 786   if (is_static) {
 787     total_c_args++;
 788   }
 789 
 790   BasicType* out_sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_c_args);
 791   VMRegPair* out_regs   = NEW_RESOURCE_ARRAY(VMRegPair, total_c_args);
 792 
 793   int argc = 0;
 794   out_sig_bt[argc++] = T_ADDRESS;
 795   if (is_static) {
 796     out_sig_bt[argc++] = T_OBJECT;
 797   }
 798 
 799   int i;
 800   for (i = 0; i &lt; total_in_args; i++) {
 801     out_sig_bt[argc++] = in_sig_bt[i];
 802   }
 803 
 804   int out_arg_slots = c_calling_convention(out_sig_bt, out_regs, NULL, total_c_args);
 805   int stack_slots = SharedRuntime::out_preserve_stack_slots() + out_arg_slots;
 806   // Since object arguments need to be wrapped, we must preserve space
 807   // for those object arguments which come in registers (GPR_PARAMS maximum)
 808   // plus one more slot for Klass handle (for static methods)
 809   int oop_handle_offset = stack_slots;
 810   stack_slots += (GPR_PARAMS + 1) * VMRegImpl::slots_per_word;
 811 
 812   // Plus a lock if needed
 813   int lock_slot_offset = 0;
 814   if (method-&gt;is_synchronized()) {
 815     lock_slot_offset = stack_slots;
 816     assert(sizeof(BasicLock) == wordSize, &quot;adjust this code&quot;);
 817     stack_slots += VMRegImpl::slots_per_word;
 818   }
 819 
 820   // Space to save return address and FP
 821   stack_slots += 2 * VMRegImpl::slots_per_word;
 822 
 823   // Calculate the final stack size taking account of alignment
 824   stack_slots = align_up(stack_slots, StackAlignmentInBytes / VMRegImpl::stack_slot_size);
 825   int stack_size = stack_slots * VMRegImpl::stack_slot_size;
 826   int lock_slot_fp_offset = stack_size - 2 * wordSize -
 827     lock_slot_offset * VMRegImpl::stack_slot_size;
 828 
 829   // Unverified entry point
 830   address start = __ pc();
 831 
 832   // Inline cache check, same as in C1_MacroAssembler::inline_cache_check()
 833   const Register receiver = R0; // see receiverOpr()
 834   __ load_klass(Rtemp, receiver);
 835   __ cmp(Rtemp, Ricklass);
 836   Label verified;
 837 
 838   __ b(verified, eq); // jump over alignment no-ops too
 839   __ jump(SharedRuntime::get_ic_miss_stub(), relocInfo::runtime_call_type, Rtemp);
 840   __ align(CodeEntryAlignment);
 841 
 842   // Verified entry point
 843   __ bind(verified);
 844   int vep_offset = __ pc() - start;
 845 
 846 
 847   if ((InlineObjectHash &amp;&amp; method-&gt;intrinsic_id() == vmIntrinsics::_hashCode) || (method-&gt;intrinsic_id() == vmIntrinsics::_identityHashCode)) {
 848     // Object.hashCode, System.identityHashCode can pull the hashCode from the header word
 849     // instead of doing a full VM transition once it&#39;s been computed.
 850     Label slow_case;
 851     const Register obj_reg = R0;
 852 
 853     // Unlike for Object.hashCode, System.identityHashCode is static method and
 854     // gets object as argument instead of the receiver.
 855     if (method-&gt;intrinsic_id() == vmIntrinsics::_identityHashCode) {
 856       assert(method-&gt;is_static(), &quot;method should be static&quot;);
 857       // return 0 for null reference input, return val = R0 = obj_reg = 0
 858       __ cmp(obj_reg, 0);
 859       __ bx(LR, eq);
 860     }
 861 
 862     __ ldr(Rtemp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
 863 
<a name="18" id="anc18"></a><span class="line-modified"> 864     assert(markWord::unlocked_value == 1, &quot;adjust this code&quot;);</span>
<span class="line-modified"> 865     __ tbz(Rtemp, exact_log2(markWord::unlocked_value), slow_case);</span>
 866 
 867     if (UseBiasedLocking) {
<a name="19" id="anc19"></a><span class="line-modified"> 868       assert(is_power_of_2(markWord::biased_lock_bit_in_place), &quot;adjust this code&quot;);</span>
<span class="line-modified"> 869       __ tbnz(Rtemp, exact_log2(markWord::biased_lock_bit_in_place), slow_case);</span>
 870     }
 871 
<a name="20" id="anc20"></a><span class="line-modified"> 872     __ bics(Rtemp, Rtemp, ~markWord::hash_mask_in_place);</span>
<span class="line-modified"> 873     __ mov(R0, AsmOperand(Rtemp, lsr, markWord::hash_shift), ne);</span>
 874     __ bx(LR, ne);
 875 
 876     __ bind(slow_case);
 877   }
 878 
 879   // Bang stack pages
 880   __ arm_stack_overflow_check(stack_size, Rtemp);
 881 
 882   // Setup frame linkage
 883   __ raw_push(FP, LR);
 884   __ mov(FP, SP);
 885   __ sub_slow(SP, SP, stack_size - 2*wordSize);
 886 
 887   int frame_complete = __ pc() - start;
 888 
 889   OopMapSet* oop_maps = new OopMapSet();
 890   OopMap* map = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
 891   const int extra_args = is_static ? 2 : 1;
 892   int receiver_offset = -1;
 893   int fp_regs_in_arguments = 0;
 894 
 895   for (i = total_in_args; --i &gt;= 0; ) {
 896     switch (in_sig_bt[i]) {
 897     case T_ARRAY:
 898     case T_OBJECT: {
 899       VMReg src = in_regs[i].first();
 900       VMReg dst = out_regs[i + extra_args].first();
 901       if (src-&gt;is_stack()) {
 902         assert(dst-&gt;is_stack(), &quot;must be&quot;);
 903         assert(i != 0, &quot;Incoming receiver is always in a register&quot;);
 904         __ ldr(Rtemp, Address(FP, reg2offset_in(src)));
 905         __ cmp(Rtemp, 0);
 906         __ add(Rtemp, FP, reg2offset_in(src), ne);
 907         __ str(Rtemp, Address(SP, reg2offset_out(dst)));
 908         int offset_in_older_frame = src-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
 909         map-&gt;set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));
 910       } else {
 911         int offset = oop_handle_offset * VMRegImpl::stack_slot_size;
 912         __ str(src-&gt;as_Register(), Address(SP, offset));
 913         map-&gt;set_oop(VMRegImpl::stack2reg(oop_handle_offset));
 914         if ((i == 0) &amp;&amp; (!is_static)) {
 915           receiver_offset = offset;
 916         }
 917         oop_handle_offset += VMRegImpl::slots_per_word;
 918 
 919         if (dst-&gt;is_stack()) {
 920           __ movs(Rtemp, src-&gt;as_Register());
 921           __ add(Rtemp, SP, offset, ne);
 922           __ str(Rtemp, Address(SP, reg2offset_out(dst)));
 923         } else {
 924           __ movs(dst-&gt;as_Register(), src-&gt;as_Register());
 925           __ add(dst-&gt;as_Register(), SP, offset, ne);
 926         }
 927       }
 928     }
 929 
 930     case T_VOID:
 931       break;
 932 
 933 
 934 #ifdef __SOFTFP__
 935     case T_DOUBLE:
 936 #endif
 937     case T_LONG: {
 938       VMReg src_1 = in_regs[i].first();
 939       VMReg src_2 = in_regs[i].second();
 940       VMReg dst_1 = out_regs[i + extra_args].first();
 941       VMReg dst_2 = out_regs[i + extra_args].second();
 942 #if (ALIGN_WIDE_ARGUMENTS == 0)
 943       // C convention can mix a register and a stack slot for a
 944       // 64-bits native argument.
 945 
 946       // Note: following code should work independently of whether
 947       // the Java calling convention follows C convention or whether
 948       // it aligns 64-bit values.
 949       if (dst_2-&gt;is_Register()) {
 950         if (src_1-&gt;as_Register() != dst_1-&gt;as_Register()) {
 951           assert(src_1-&gt;as_Register() != dst_2-&gt;as_Register() &amp;&amp;
 952                  src_2-&gt;as_Register() != dst_2-&gt;as_Register(), &quot;must be&quot;);
 953           __ mov(dst_2-&gt;as_Register(), src_2-&gt;as_Register());
 954           __ mov(dst_1-&gt;as_Register(), src_1-&gt;as_Register());
 955         } else {
 956           assert(src_2-&gt;as_Register() == dst_2-&gt;as_Register(), &quot;must be&quot;);
 957         }
 958       } else if (src_2-&gt;is_Register()) {
 959         if (dst_1-&gt;is_Register()) {
 960           // dst mixes a register and a stack slot
 961           assert(dst_2-&gt;is_stack() &amp;&amp; src_1-&gt;is_Register() &amp;&amp; src_2-&gt;is_Register(), &quot;must be&quot;);
 962           assert(src_1-&gt;as_Register() != dst_1-&gt;as_Register(), &quot;must be&quot;);
 963           __ str(src_2-&gt;as_Register(), Address(SP, reg2offset_out(dst_2)));
 964           __ mov(dst_1-&gt;as_Register(), src_1-&gt;as_Register());
 965         } else {
 966           // registers to stack slots
 967           assert(dst_2-&gt;is_stack() &amp;&amp; src_1-&gt;is_Register() &amp;&amp; src_2-&gt;is_Register(), &quot;must be&quot;);
 968           __ str(src_1-&gt;as_Register(), Address(SP, reg2offset_out(dst_1)));
 969           __ str(src_2-&gt;as_Register(), Address(SP, reg2offset_out(dst_2)));
 970         }
 971       } else if (src_1-&gt;is_Register()) {
 972         if (dst_1-&gt;is_Register()) {
 973           // src and dst must be R3 + stack slot
 974           assert(dst_1-&gt;as_Register() == src_1-&gt;as_Register(), &quot;must be&quot;);
 975           __ ldr(Rtemp,    Address(FP, reg2offset_in(src_2)));
 976           __ str(Rtemp,    Address(SP, reg2offset_out(dst_2)));
 977         } else {
 978           // &lt;R3,stack&gt; -&gt; &lt;stack,stack&gt;
 979           assert(dst_2-&gt;is_stack() &amp;&amp; src_2-&gt;is_stack(), &quot;must be&quot;);
 980           __ ldr(LR, Address(FP, reg2offset_in(src_2)));
 981           __ str(src_1-&gt;as_Register(), Address(SP, reg2offset_out(dst_1)));
 982           __ str(LR, Address(SP, reg2offset_out(dst_2)));
 983         }
 984       } else {
 985         assert(src_2-&gt;is_stack() &amp;&amp; dst_1-&gt;is_stack() &amp;&amp; dst_2-&gt;is_stack(), &quot;must be&quot;);
 986         __ ldr(Rtemp, Address(FP, reg2offset_in(src_1)));
 987         __ ldr(LR,    Address(FP, reg2offset_in(src_2)));
 988         __ str(Rtemp, Address(SP, reg2offset_out(dst_1)));
 989         __ str(LR,    Address(SP, reg2offset_out(dst_2)));
 990       }
 991 #else // ALIGN_WIDE_ARGUMENTS
 992       if (src_1-&gt;is_stack()) {
 993         assert(src_2-&gt;is_stack() &amp;&amp; dst_1-&gt;is_stack() &amp;&amp; dst_2-&gt;is_stack(), &quot;must be&quot;);
 994         __ ldr(Rtemp, Address(FP, reg2offset_in(src_1)));
 995         __ ldr(LR,    Address(FP, reg2offset_in(src_2)));
 996         __ str(Rtemp, Address(SP, reg2offset_out(dst_1)));
 997         __ str(LR,    Address(SP, reg2offset_out(dst_2)));
 998       } else if (dst_1-&gt;is_stack()) {
 999         assert(dst_2-&gt;is_stack() &amp;&amp; src_1-&gt;is_Register() &amp;&amp; src_2-&gt;is_Register(), &quot;must be&quot;);
1000         __ str(src_1-&gt;as_Register(), Address(SP, reg2offset_out(dst_1)));
1001         __ str(src_2-&gt;as_Register(), Address(SP, reg2offset_out(dst_2)));
1002       } else if (src_1-&gt;as_Register() == dst_1-&gt;as_Register()) {
1003         assert(src_2-&gt;as_Register() == dst_2-&gt;as_Register(), &quot;must be&quot;);
1004       } else {
1005         assert(src_1-&gt;as_Register() != dst_2-&gt;as_Register() &amp;&amp;
1006                src_2-&gt;as_Register() != dst_2-&gt;as_Register(), &quot;must be&quot;);
1007         __ mov(dst_2-&gt;as_Register(), src_2-&gt;as_Register());
1008         __ mov(dst_1-&gt;as_Register(), src_1-&gt;as_Register());
1009       }
1010 #endif // ALIGN_WIDE_ARGUMENTS
1011       break;
1012     }
1013 
1014 #if (!defined __SOFTFP__ &amp;&amp; !defined __ABI_HARD__)
1015     case T_FLOAT: {
1016       VMReg src = in_regs[i].first();
1017       VMReg dst = out_regs[i + extra_args].first();
1018       if (src-&gt;is_stack()) {
1019         assert(dst-&gt;is_stack(), &quot;must be&quot;);
1020         __ ldr(Rtemp, Address(FP, reg2offset_in(src)));
1021         __ str(Rtemp, Address(SP, reg2offset_out(dst)));
1022       } else if (dst-&gt;is_stack()) {
1023         __ fsts(src-&gt;as_FloatRegister(), Address(SP, reg2offset_out(dst)));
1024       } else {
1025         assert(src-&gt;is_FloatRegister() &amp;&amp; dst-&gt;is_Register(), &quot;must be&quot;);
1026         __ fmrs(dst-&gt;as_Register(), src-&gt;as_FloatRegister());
1027       }
1028       break;
1029     }
1030 
1031     case T_DOUBLE: {
1032       VMReg src_1 = in_regs[i].first();
1033       VMReg src_2 = in_regs[i].second();
1034       VMReg dst_1 = out_regs[i + extra_args].first();
1035       VMReg dst_2 = out_regs[i + extra_args].second();
1036       if (src_1-&gt;is_stack()) {
1037         assert(src_2-&gt;is_stack() &amp;&amp; dst_1-&gt;is_stack() &amp;&amp; dst_2-&gt;is_stack(), &quot;must be&quot;);
1038         __ ldr(Rtemp, Address(FP, reg2offset_in(src_1)));
1039         __ ldr(LR,    Address(FP, reg2offset_in(src_2)));
1040         __ str(Rtemp, Address(SP, reg2offset_out(dst_1)));
1041         __ str(LR,    Address(SP, reg2offset_out(dst_2)));
1042       } else if (dst_1-&gt;is_stack()) {
1043         assert(dst_2-&gt;is_stack() &amp;&amp; src_1-&gt;is_FloatRegister(), &quot;must be&quot;);
1044         __ fstd(src_1-&gt;as_FloatRegister(), Address(SP, reg2offset_out(dst_1)));
1045 #if (ALIGN_WIDE_ARGUMENTS == 0)
1046       } else if (dst_2-&gt;is_stack()) {
1047         assert(! src_2-&gt;is_stack(), &quot;must be&quot;); // assuming internal java convention is aligned
1048         // double register must go into R3 + one stack slot
1049         __ fmrrd(dst_1-&gt;as_Register(), Rtemp, src_1-&gt;as_FloatRegister());
1050         __ str(Rtemp, Address(SP, reg2offset_out(dst_2)));
1051 #endif
1052       } else {
1053         assert(src_1-&gt;is_FloatRegister() &amp;&amp; dst_1-&gt;is_Register() &amp;&amp; dst_2-&gt;is_Register(), &quot;must be&quot;);
1054         __ fmrrd(dst_1-&gt;as_Register(), dst_2-&gt;as_Register(), src_1-&gt;as_FloatRegister());
1055       }
1056       break;
1057     }
1058 #endif // __SOFTFP__
1059 
1060 #ifdef __ABI_HARD__
1061     case T_FLOAT: {
1062       VMReg src = in_regs[i].first();
1063       VMReg dst = out_regs[i + extra_args].first();
1064       if (src-&gt;is_stack()) {
1065         if (dst-&gt;is_stack()) {
1066           __ ldr(Rtemp, Address(FP, reg2offset_in(src)));
1067           __ str(Rtemp, Address(SP, reg2offset_out(dst)));
1068         } else {
1069           // C2 Java calling convention does not populate S14 and S15, therefore
1070           // those need to be loaded from stack here
1071           __ flds(dst-&gt;as_FloatRegister(), Address(FP, reg2offset_in(src)));
1072           fp_regs_in_arguments++;
1073         }
1074       } else {
1075         assert(src-&gt;is_FloatRegister(), &quot;must be&quot;);
1076         fp_regs_in_arguments++;
1077       }
1078       break;
1079     }
1080     case T_DOUBLE: {
1081       VMReg src_1 = in_regs[i].first();
1082       VMReg src_2 = in_regs[i].second();
1083       VMReg dst_1 = out_regs[i + extra_args].first();
1084       VMReg dst_2 = out_regs[i + extra_args].second();
1085       if (src_1-&gt;is_stack()) {
1086         if (dst_1-&gt;is_stack()) {
1087           assert(dst_2-&gt;is_stack(), &quot;must be&quot;);
1088           __ ldr(Rtemp, Address(FP, reg2offset_in(src_1)));
1089           __ ldr(LR,    Address(FP, reg2offset_in(src_2)));
1090           __ str(Rtemp, Address(SP, reg2offset_out(dst_1)));
1091           __ str(LR,    Address(SP, reg2offset_out(dst_2)));
1092         } else {
1093           // C2 Java calling convention does not populate S14 and S15, therefore
1094           // those need to be loaded from stack here
1095           __ fldd(dst_1-&gt;as_FloatRegister(), Address(FP, reg2offset_in(src_1)));
1096           fp_regs_in_arguments += 2;
1097         }
1098       } else {
1099         assert(src_1-&gt;is_FloatRegister() &amp;&amp; src_2-&gt;is_FloatRegister(), &quot;must be&quot;);
1100         fp_regs_in_arguments += 2;
1101       }
1102       break;
1103     }
1104 #endif // __ABI_HARD__
1105 
1106     default: {
1107       assert(in_sig_bt[i] != T_ADDRESS, &quot;found T_ADDRESS in java args&quot;);
1108       VMReg src = in_regs[i].first();
1109       VMReg dst = out_regs[i + extra_args].first();
1110       if (src-&gt;is_stack()) {
1111         assert(dst-&gt;is_stack(), &quot;must be&quot;);
1112         __ ldr(Rtemp, Address(FP, reg2offset_in(src)));
1113         __ str(Rtemp, Address(SP, reg2offset_out(dst)));
1114       } else if (dst-&gt;is_stack()) {
1115         __ str(src-&gt;as_Register(), Address(SP, reg2offset_out(dst)));
1116       } else {
1117         assert(src-&gt;is_Register() &amp;&amp; dst-&gt;is_Register(), &quot;must be&quot;);
1118         __ mov(dst-&gt;as_Register(), src-&gt;as_Register());
1119       }
1120     }
1121     }
1122   }
1123 
1124   // Get Klass mirror
1125   int klass_offset = -1;
1126   if (is_static) {
1127     klass_offset = oop_handle_offset * VMRegImpl::stack_slot_size;
1128     __ mov_oop(Rtemp, JNIHandles::make_local(method-&gt;method_holder()-&gt;java_mirror()));
1129     __ add(c_rarg1, SP, klass_offset);
1130     __ str(Rtemp, Address(SP, klass_offset));
1131     map-&gt;set_oop(VMRegImpl::stack2reg(oop_handle_offset));
1132   }
1133 
1134   // the PC offset given to add_gc_map must match the PC saved in set_last_Java_frame
1135   int pc_offset = __ set_last_Java_frame(SP, FP, true, Rtemp);
1136   assert(((__ pc()) - start) == __ offset(), &quot;warning: start differs from code_begin&quot;);
1137   oop_maps-&gt;add_gc_map(pc_offset, map);
1138 
1139   // Order last_Java_pc store with the thread state transition (to _thread_in_native)
1140   __ membar(MacroAssembler::StoreStore, Rtemp);
1141 
1142   // RedefineClasses() tracing support for obsolete method entry
1143   if (log_is_enabled(Trace, redefine, class, obsolete)) {
1144     __ save_caller_save_registers();
1145     __ mov(R0, Rthread);
1146     __ mov_metadata(R1, method());
1147     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::rc_trace_method_entry), R0, R1);
1148     __ restore_caller_save_registers();
1149   }
1150 
1151   const Register sync_handle = R5;
1152   const Register sync_obj    = R6;
1153   const Register disp_hdr    = altFP_7_11;
1154   const Register tmp         = R8;
1155 
1156   Label slow_lock, slow_lock_biased, lock_done, fast_lock;
1157   if (method-&gt;is_synchronized()) {
1158     // The first argument is a handle to sync object (a class or an instance)
1159     __ ldr(sync_obj, Address(R1));
1160     // Remember the handle for the unlocking code
1161     __ mov(sync_handle, R1);
1162 
1163     __ resolve(IS_NOT_NULL, sync_obj);
1164 
1165     if(UseBiasedLocking) {
1166       __ biased_locking_enter(sync_obj, tmp, disp_hdr/*scratched*/, false, Rtemp, lock_done, slow_lock_biased);
1167     }
1168 
1169     const Register mark = tmp;
1170     // On MP platforms the next load could return a &#39;stale&#39; value if the memory location has been modified by another thread.
1171     // That would be acceptable as either CAS or slow case path is taken in that case
1172 
1173     __ ldr(mark, Address(sync_obj, oopDesc::mark_offset_in_bytes()));
1174     __ sub(disp_hdr, FP, lock_slot_fp_offset);
<a name="21" id="anc21"></a><span class="line-modified">1175     __ tst(mark, markWord::unlocked_value);</span>
1176     __ b(fast_lock, ne);
1177 
1178     // Check for recursive lock
1179     // See comments in InterpreterMacroAssembler::lock_object for
1180     // explanations on the fast recursive locking check.
1181     // Check independently the low bits and the distance to SP
1182     // -1- test low 2 bits
1183     __ movs(Rtemp, AsmOperand(mark, lsl, 30));
1184     // -2- test (hdr - SP) if the low two bits are 0
1185     __ sub(Rtemp, mark, SP, eq);
1186     __ movs(Rtemp, AsmOperand(Rtemp, lsr, exact_log2(os::vm_page_size())), eq);
1187     // If still &#39;eq&#39; then recursive locking OK: set displaced header to 0
1188     __ str(Rtemp, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()), eq);
1189     __ b(lock_done, eq);
1190     __ b(slow_lock);
1191 
1192     __ bind(fast_lock);
1193     __ str(mark, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));
1194 
1195     __ cas_for_lock_acquire(mark, disp_hdr, sync_obj, Rtemp, slow_lock);
1196 
1197     __ bind(lock_done);
1198   }
1199 
1200   // Get JNIEnv*
1201   __ add(c_rarg0, Rthread, in_bytes(JavaThread::jni_environment_offset()));
1202 
1203   // Perform thread state transition
1204   __ mov(Rtemp, _thread_in_native);
1205   __ str(Rtemp, Address(Rthread, JavaThread::thread_state_offset()));
1206 
1207   // Finally, call the native method
1208   __ call(method-&gt;native_function());
1209 
1210   // Set FPSCR/FPCR to a known state
1211   if (AlwaysRestoreFPU) {
1212     __ restore_default_fp_mode();
1213   }
1214 
1215   // Ensure a Boolean result is mapped to 0..1
1216   if (ret_type == T_BOOLEAN) {
1217     __ c2bool(R0);
1218   }
1219 
1220   // Do a safepoint check while thread is in transition state
<a name="22" id="anc22"></a>
1221   Label call_safepoint_runtime, return_to_java;
1222   __ mov(Rtemp, _thread_in_native_trans);
<a name="23" id="anc23"></a>
1223   __ str_32(Rtemp, Address(Rthread, JavaThread::thread_state_offset()));
1224 
1225   // make sure the store is observed before reading the SafepointSynchronize state and further mem refs
1226   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreLoad | MacroAssembler::StoreStore), Rtemp);
1227 
<a name="24" id="anc24"></a><span class="line-modified">1228   __ safepoint_poll(R2, call_safepoint_runtime);</span>
1229   __ ldr_u32(R3, Address(Rthread, JavaThread::suspend_flags_offset()));
<a name="25" id="anc25"></a><span class="line-modified">1230   __ cmp(R3, 0);</span>

1231   __ b(call_safepoint_runtime, ne);
<a name="26" id="anc26"></a><span class="line-added">1232 </span>
1233   __ bind(return_to_java);
1234 
1235   // Perform thread state transition and reguard stack yellow pages if needed
1236   Label reguard, reguard_done;
1237   __ mov(Rtemp, _thread_in_Java);
1238   __ ldr_s32(R2, Address(Rthread, JavaThread::stack_guard_state_offset()));
1239   __ str_32(Rtemp, Address(Rthread, JavaThread::thread_state_offset()));
1240 
1241   __ cmp(R2, JavaThread::stack_guard_yellow_reserved_disabled);
1242   __ b(reguard, eq);
1243   __ bind(reguard_done);
1244 
1245   Label slow_unlock, unlock_done;
1246   if (method-&gt;is_synchronized()) {
1247     __ ldr(sync_obj, Address(sync_handle));
1248 
1249     __ resolve(IS_NOT_NULL, sync_obj);
1250 
1251     if(UseBiasedLocking) {
1252       __ biased_locking_exit(sync_obj, Rtemp, unlock_done);
1253       // disp_hdr may not have been saved on entry with biased locking
1254       __ sub(disp_hdr, FP, lock_slot_fp_offset);
1255     }
1256 
1257     // See C1_MacroAssembler::unlock_object() for more comments
1258     __ ldr(R2, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));
1259     __ cbz(R2, unlock_done);
1260 
1261     __ cas_for_lock_release(disp_hdr, R2, sync_obj, Rtemp, slow_unlock);
1262 
1263     __ bind(unlock_done);
1264   }
1265 
1266   // Set last java frame and handle block to zero
1267   __ ldr(LR, Address(Rthread, JavaThread::active_handles_offset()));
1268   __ reset_last_Java_frame(Rtemp); // sets Rtemp to 0 on 32-bit ARM
1269 
1270   __ str_32(Rtemp, Address(LR, JNIHandleBlock::top_offset_in_bytes()));
1271   if (CheckJNICalls) {
1272     __ str(__ zero_register(Rtemp), Address(Rthread, JavaThread::pending_jni_exception_check_fn_offset()));
1273   }
1274 
1275   // Unbox oop result, e.g. JNIHandles::resolve value in R0.
1276   if (ret_type == T_OBJECT || ret_type == T_ARRAY) {
1277     __ resolve_jobject(R0,      // value
1278                        Rtemp,   // tmp1
1279                        R1_tmp); // tmp2
1280   }
1281 
1282   // Any exception pending?
1283   __ ldr(Rtemp, Address(Rthread, Thread::pending_exception_offset()));
1284   __ mov(SP, FP);
1285 
1286   __ cmp(Rtemp, 0);
1287   // Pop the frame and return if no exception pending
1288   __ pop(RegisterSet(FP) | RegisterSet(PC), eq);
1289   // Pop the frame and forward the exception. Rexception_pc contains return address.
1290   __ ldr(FP, Address(SP, wordSize, post_indexed), ne);
1291   __ ldr(Rexception_pc, Address(SP, wordSize, post_indexed), ne);
1292   __ jump(StubRoutines::forward_exception_entry(), relocInfo::runtime_call_type, Rtemp);
1293 
1294   // Safepoint operation and/or pending suspend request is in progress.
1295   // Save the return values and call the runtime function by hand.
1296   __ bind(call_safepoint_runtime);
1297   push_result_registers(masm, ret_type);
1298   __ mov(R0, Rthread);
1299   __ call(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans));
1300   pop_result_registers(masm, ret_type);
1301   __ b(return_to_java);
1302 
<a name="27" id="anc27"></a>

1303   // Reguard stack pages. Save native results around a call to C runtime.
1304   __ bind(reguard);
1305   push_result_registers(masm, ret_type);
1306   __ call(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));
1307   pop_result_registers(masm, ret_type);
1308   __ b(reguard_done);
1309 
1310   if (method-&gt;is_synchronized()) {
1311     // Locking slow case
1312     if(UseBiasedLocking) {
1313       __ bind(slow_lock_biased);
1314       __ sub(disp_hdr, FP, lock_slot_fp_offset);
1315     }
1316 
1317     __ bind(slow_lock);
1318 
1319     push_param_registers(masm, fp_regs_in_arguments);
1320 
1321     // last_Java_frame is already set, so do call_VM manually; no exception can occur
1322     __ mov(R0, sync_obj);
1323     __ mov(R1, disp_hdr);
1324     __ mov(R2, Rthread);
1325     __ call(CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_locking_C));
1326 
1327     pop_param_registers(masm, fp_regs_in_arguments);
1328 
1329     __ b(lock_done);
1330 
1331     // Unlocking slow case
1332     __ bind(slow_unlock);
1333 
1334     push_result_registers(masm, ret_type);
1335 
1336     // Clear pending exception before reentering VM.
1337     // Can store the oop in register since it is a leaf call.
1338     assert_different_registers(Rtmp_save1, sync_obj, disp_hdr);
1339     __ ldr(Rtmp_save1, Address(Rthread, Thread::pending_exception_offset()));
1340     Register zero = __ zero_register(Rtemp);
1341     __ str(zero, Address(Rthread, Thread::pending_exception_offset()));
1342     __ mov(R0, sync_obj);
1343     __ mov(R1, disp_hdr);
1344     __ mov(R2, Rthread);
1345     __ call(CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C));
1346     __ str(Rtmp_save1, Address(Rthread, Thread::pending_exception_offset()));
1347 
1348     pop_result_registers(masm, ret_type);
1349 
1350     __ b(unlock_done);
1351   }
1352 
1353   __ flush();
1354   return nmethod::new_native_nmethod(method,
1355                                      compile_id,
1356                                      masm-&gt;code(),
1357                                      vep_offset,
1358                                      frame_complete,
1359                                      stack_slots / VMRegImpl::slots_per_word,
1360                                      in_ByteSize(is_static ? klass_offset : receiver_offset),
1361                                      in_ByteSize(lock_slot_offset * VMRegImpl::stack_slot_size),
1362                                      oop_maps);
1363 }
1364 
1365 // this function returns the adjust size (in number of words) to a c2i adapter
1366 // activation for use during deoptimization
1367 int Deoptimization::last_frame_adjust(int callee_parameters, int callee_locals) {
1368   int extra_locals_size = (callee_locals - callee_parameters) * Interpreter::stackElementWords;
1369   return extra_locals_size;
1370 }
1371 
1372 
1373 uint SharedRuntime::out_preserve_stack_slots() {
1374   return 0;
1375 }
1376 
1377 
1378 //------------------------------generate_deopt_blob----------------------------
1379 void SharedRuntime::generate_deopt_blob() {
1380   ResourceMark rm;
1381   CodeBuffer buffer(&quot;deopt_blob&quot;, 1024, 1024);
1382   int frame_size_in_words;
1383   OopMapSet* oop_maps;
1384   int reexecute_offset;
1385   int exception_in_tls_offset;
1386   int exception_offset;
1387 
1388   MacroAssembler* masm = new MacroAssembler(&amp;buffer);
1389   Label cont;
1390   const Register Rkind   = R9; // caller-saved
1391   const Register Rublock = R6;
1392   const Register Rsender = altFP_7_11;
1393   assert_different_registers(Rkind, Rublock, Rsender, Rexception_obj, Rexception_pc, R0, R1, R2, R3, R8, Rtemp);
1394 
1395   address start = __ pc();
1396 
1397   oop_maps = new OopMapSet();
1398   // LR saved by caller (can be live in c2 method)
1399 
1400   // A deopt is a case where LR may be live in the c2 nmethod. So it&#39;s
1401   // not possible to call the deopt blob from the nmethod and pass the
1402   // address of the deopt handler of the nmethod in LR. What happens
1403   // now is that the caller of the deopt blob pushes the current
1404   // address so the deopt blob doesn&#39;t have to do it. This way LR can
1405   // be preserved, contains the live value from the nmethod and is
1406   // saved at R14/R30_offset here.
1407   OopMap* map = RegisterSaver::save_live_registers(masm, &amp;frame_size_in_words, true);
1408   __ mov(Rkind, Deoptimization::Unpack_deopt);
1409   __ b(cont);
1410 
1411   exception_offset = __ pc() - start;
1412 
1413   // Transfer Rexception_obj &amp; Rexception_pc in TLS and fall thru to the
1414   // exception_in_tls_offset entry point.
1415   __ str(Rexception_obj, Address(Rthread, JavaThread::exception_oop_offset()));
1416   __ str(Rexception_pc, Address(Rthread, JavaThread::exception_pc_offset()));
1417   // Force return value to NULL to avoid confusing the escape analysis
1418   // logic. Everything is dead here anyway.
1419   __ mov(R0, 0);
1420 
1421   exception_in_tls_offset = __ pc() - start;
1422 
1423   // Exception data is in JavaThread structure
1424   // Patch the return address of the current frame
1425   __ ldr(LR, Address(Rthread, JavaThread::exception_pc_offset()));
1426   (void) RegisterSaver::save_live_registers(masm, &amp;frame_size_in_words);
1427   {
1428     const Register Rzero = __ zero_register(Rtemp); // XXX should be OK for C2 but not 100% sure
1429     __ str(Rzero, Address(Rthread, JavaThread::exception_pc_offset()));
1430   }
1431   __ mov(Rkind, Deoptimization::Unpack_exception);
1432   __ b(cont);
1433 
1434   reexecute_offset = __ pc() - start;
1435 
1436   (void) RegisterSaver::save_live_registers(masm, &amp;frame_size_in_words);
1437   __ mov(Rkind, Deoptimization::Unpack_reexecute);
1438 
1439   // Calculate UnrollBlock and save the result in Rublock
1440   __ bind(cont);
1441   __ mov(R0, Rthread);
1442   __ mov(R1, Rkind);
1443 
1444   int pc_offset = __ set_last_Java_frame(SP, FP, false, Rtemp); // note: FP may not need to be saved (not on x86)
1445   assert(((__ pc()) - start) == __ offset(), &quot;warning: start differs from code_begin&quot;);
1446   __ call(CAST_FROM_FN_PTR(address, Deoptimization::fetch_unroll_info));
1447   if (pc_offset == -1) {
1448     pc_offset = __ offset();
1449   }
1450   oop_maps-&gt;add_gc_map(pc_offset, map);
1451   __ reset_last_Java_frame(Rtemp); // Rtemp free since scratched by far call
1452 
1453   __ mov(Rublock, R0);
1454 
1455   // Reload Rkind from the UnrollBlock (might have changed)
1456   __ ldr_s32(Rkind, Address(Rublock, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()));
1457   Label noException;
1458   __ cmp_32(Rkind, Deoptimization::Unpack_exception);   // Was exception pending?
1459   __ b(noException, ne);
1460   // handle exception case
1461 #ifdef ASSERT
1462   // assert that exception_pc is zero in tls
1463   { Label L;
1464     __ ldr(Rexception_pc, Address(Rthread, JavaThread::exception_pc_offset()));
1465     __ cbz(Rexception_pc, L);
1466     __ stop(&quot;exception pc should be null&quot;);
1467     __ bind(L);
1468   }
1469 #endif
1470   __ ldr(Rexception_obj, Address(Rthread, JavaThread::exception_oop_offset()));
1471   __ verify_oop(Rexception_obj);
1472   {
1473     const Register Rzero = __ zero_register(Rtemp);
1474     __ str(Rzero, Address(Rthread, JavaThread::exception_oop_offset()));
1475   }
1476 
1477   __ bind(noException);
1478 
1479   // This frame is going away.  Fetch return value, so we can move it to
1480   // a new frame.
1481   __ ldr(R0, Address(SP, RegisterSaver::R0_offset * wordSize));
1482   __ ldr(R1, Address(SP, RegisterSaver::R1_offset * wordSize));
1483 #ifndef __SOFTFP__
1484   __ ldr_double(D0, Address(SP, RegisterSaver::D0_offset * wordSize));
1485 #endif
1486   // pop frame
1487   __ add(SP, SP, RegisterSaver::reg_save_size * wordSize);
1488 
1489   // Set initial stack state before pushing interpreter frames
1490   __ ldr_s32(Rtemp, Address(Rublock, Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset_in_bytes()));
1491   __ ldr(R2, Address(Rublock, Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes()));
1492   __ ldr(R3, Address(Rublock, Deoptimization::UnrollBlock::frame_sizes_offset_in_bytes()));
1493 
1494   __ add(SP, SP, Rtemp);
1495 
1496 #ifdef ASSERT
1497   // Compilers generate code that bang the stack by as much as the
1498   // interpreter would need. So this stack banging should never
1499   // trigger a fault. Verify that it does not on non product builds.
1500   // See if it is enough stack to push deoptimized frames
1501   if (UseStackBanging) {
1502     // The compiled method that we are deoptimizing was popped from the stack.
1503     // If the stack bang results in a stack overflow, we don&#39;t return to the
1504     // method that is being deoptimized. The stack overflow exception is
1505     // propagated to the caller of the deoptimized method. Need to get the pc
1506     // from the caller in LR and restore FP.
1507     __ ldr(LR, Address(R2, 0));
1508     __ ldr(FP, Address(Rublock, Deoptimization::UnrollBlock::initial_info_offset_in_bytes()));
1509     __ ldr_s32(R8, Address(Rublock, Deoptimization::UnrollBlock::total_frame_sizes_offset_in_bytes()));
1510     __ arm_stack_overflow_check(R8, Rtemp);
1511   }
1512 #endif
1513   __ ldr_s32(R8, Address(Rublock, Deoptimization::UnrollBlock::number_of_frames_offset_in_bytes()));
1514 
1515   // Pick up the initial fp we should save
1516   // XXX Note: was ldr(FP, Address(FP));
1517 
1518   // The compiler no longer uses FP as a frame pointer for the
1519   // compiled code. It can be used by the allocator in C2 or to
1520   // memorize the original SP for JSR292 call sites.
1521 
1522   // Hence, ldr(FP, Address(FP)) is probably not correct. For x86,
1523   // Deoptimization::fetch_unroll_info computes the right FP value and
1524   // stores it in Rublock.initial_info. This has been activated for ARM.
1525   __ ldr(FP, Address(Rublock, Deoptimization::UnrollBlock::initial_info_offset_in_bytes()));
1526 
1527   __ ldr_s32(Rtemp, Address(Rublock, Deoptimization::UnrollBlock::caller_adjustment_offset_in_bytes()));
1528   __ mov(Rsender, SP);
1529   __ sub(SP, SP, Rtemp);
1530 
1531   // Push interpreter frames in a loop
1532   Label loop;
1533   __ bind(loop);
1534   __ ldr(LR, Address(R2, wordSize, post_indexed));         // load frame pc
1535   __ ldr(Rtemp, Address(R3, wordSize, post_indexed));      // load frame size
1536 
1537   __ raw_push(FP, LR);                                     // create new frame
1538   __ mov(FP, SP);
1539   __ sub(Rtemp, Rtemp, 2*wordSize);
1540 
1541   __ sub(SP, SP, Rtemp);
1542 
1543   __ str(Rsender, Address(FP, frame::interpreter_frame_sender_sp_offset * wordSize));
1544   __ mov(LR, 0);
1545   __ str(LR, Address(FP, frame::interpreter_frame_last_sp_offset * wordSize));
1546 
1547   __ subs(R8, R8, 1);                               // decrement counter
1548   __ mov(Rsender, SP);
1549   __ b(loop, ne);
1550 
1551   // Re-push self-frame
1552   __ ldr(LR, Address(R2));
1553   __ raw_push(FP, LR);
1554   __ mov(FP, SP);
1555   __ sub(SP, SP, (frame_size_in_words - 2) * wordSize);
1556 
1557   // Restore frame locals after moving the frame
1558   __ str(R0, Address(SP, RegisterSaver::R0_offset * wordSize));
1559   __ str(R1, Address(SP, RegisterSaver::R1_offset * wordSize));
1560 
1561 #ifndef __SOFTFP__
1562   __ str_double(D0, Address(SP, RegisterSaver::D0_offset * wordSize));
1563 #endif // !__SOFTFP__
1564 
1565 #ifdef ASSERT
1566   // Reload Rkind from the UnrollBlock and check that it was not overwritten (Rkind is not callee-saved)
1567   { Label L;
1568     __ ldr_s32(Rtemp, Address(Rublock, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()));
1569     __ cmp_32(Rkind, Rtemp);
1570     __ b(L, eq);
1571     __ stop(&quot;Rkind was overwritten&quot;);
1572     __ bind(L);
1573   }
1574 #endif
1575 
1576   // Call unpack_frames with proper arguments
1577   __ mov(R0, Rthread);
1578   __ mov(R1, Rkind);
1579 
1580   pc_offset = __ set_last_Java_frame(SP, FP, true, Rtemp);
1581   assert(((__ pc()) - start) == __ offset(), &quot;warning: start differs from code_begin&quot;);
1582   __ call_VM_leaf(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames));
1583   if (pc_offset == -1) {
1584     pc_offset = __ offset();
1585   }
1586   oop_maps-&gt;add_gc_map(pc_offset, new OopMap(frame_size_in_words * VMRegImpl::slots_per_word, 0));
1587   __ reset_last_Java_frame(Rtemp); // Rtemp free since scratched by far call
1588 
1589   // Collect return values, pop self-frame and jump to interpreter
1590   __ ldr(R0, Address(SP, RegisterSaver::R0_offset * wordSize));
1591   __ ldr(R1, Address(SP, RegisterSaver::R1_offset * wordSize));
1592   // Interpreter floats controlled by __SOFTFP__, but compiler
1593   // float return value registers controlled by __ABI_HARD__
1594   // This matters for vfp-sflt builds.
1595 #ifndef __SOFTFP__
1596   // Interpreter hard float
1597 #ifdef __ABI_HARD__
1598   // Compiler float return value in FP registers
1599   __ ldr_double(D0, Address(SP, RegisterSaver::D0_offset * wordSize));
1600 #else
1601   // Compiler float return value in integer registers,
1602   // copy to D0 for interpreter (S0 &lt;-- R0)
1603   __ fmdrr(D0_tos, R0, R1);
1604 #endif
1605 #endif // !__SOFTFP__
1606   __ mov(SP, FP);
1607 
1608   __ pop(RegisterSet(FP) | RegisterSet(PC));
1609 
1610   __ flush();
1611 
1612   _deopt_blob = DeoptimizationBlob::create(&amp;buffer, oop_maps, 0, exception_offset,
1613                                            reexecute_offset, frame_size_in_words);
1614   _deopt_blob-&gt;set_unpack_with_exception_in_tls_offset(exception_in_tls_offset);
1615 }
1616 
1617 #ifdef COMPILER2
1618 
1619 //------------------------------generate_uncommon_trap_blob--------------------
1620 // Ought to generate an ideal graph &amp; compile, but here&#39;s some SPARC ASM
1621 // instead.
1622 void SharedRuntime::generate_uncommon_trap_blob() {
1623   // allocate space for the code
1624   ResourceMark rm;
1625 
1626   // setup code generation tools
1627   int pad = VerifyThread ? 512 : 0;
1628 #ifdef _LP64
1629   CodeBuffer buffer(&quot;uncommon_trap_blob&quot;, 2700+pad, 512);
1630 #else
1631   // Measured 8/7/03 at 660 in 32bit debug build (no VerifyThread)
1632   // Measured 8/7/03 at 1028 in 32bit debug build (VerifyThread)
1633   CodeBuffer buffer(&quot;uncommon_trap_blob&quot;, 2000+pad, 512);
1634 #endif
1635   // bypassed when code generation useless
1636   MacroAssembler* masm               = new MacroAssembler(&amp;buffer);
1637   const Register Rublock = R6;
1638   const Register Rsender = altFP_7_11;
1639   assert_different_registers(Rublock, Rsender, Rexception_obj, R0, R1, R2, R3, R8, Rtemp);
1640 
1641   //
1642   // This is the entry point for all traps the compiler takes when it thinks
1643   // it cannot handle further execution of compilation code. The frame is
1644   // deoptimized in these cases and converted into interpreter frames for
1645   // execution
1646   // The steps taken by this frame are as follows:
1647   //   - push a fake &quot;unpack_frame&quot;
1648   //   - call the C routine Deoptimization::uncommon_trap (this function
1649   //     packs the current compiled frame into vframe arrays and returns
1650   //     information about the number and size of interpreter frames which
1651   //     are equivalent to the frame which is being deoptimized)
1652   //   - deallocate the &quot;unpack_frame&quot;
1653   //   - deallocate the deoptimization frame
1654   //   - in a loop using the information returned in the previous step
1655   //     push interpreter frames;
1656   //   - create a dummy &quot;unpack_frame&quot;
1657   //   - call the C routine: Deoptimization::unpack_frames (this function
1658   //     lays out values on the interpreter frame which was just created)
1659   //   - deallocate the dummy unpack_frame
1660   //   - return to the interpreter entry point
1661   //
1662   //  Refer to the following methods for more information:
1663   //   - Deoptimization::uncommon_trap
1664   //   - Deoptimization::unpack_frame
1665 
1666   // the unloaded class index is in R0 (first parameter to this blob)
1667 
1668   __ raw_push(FP, LR);
1669   __ set_last_Java_frame(SP, FP, false, Rtemp);
1670   __ mov(R2, Deoptimization::Unpack_uncommon_trap);
1671   __ mov(R1, R0);
1672   __ mov(R0, Rthread);
1673   __ call(CAST_FROM_FN_PTR(address, Deoptimization::uncommon_trap));
1674   __ mov(Rublock, R0);
1675   __ reset_last_Java_frame(Rtemp);
1676   __ raw_pop(FP, LR);
1677 
1678 #ifdef ASSERT
1679   { Label L;
1680     __ ldr_s32(Rtemp, Address(Rublock, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()));
1681     __ cmp_32(Rtemp, Deoptimization::Unpack_uncommon_trap);
1682     __ b(L, eq);
1683     __ stop(&quot;SharedRuntime::generate_uncommon_trap_blob: expected Unpack_uncommon_trap&quot;);
1684     __ bind(L);
1685   }
1686 #endif
1687 
1688 
1689   // Set initial stack state before pushing interpreter frames
1690   __ ldr_s32(Rtemp, Address(Rublock, Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset_in_bytes()));
1691   __ ldr(R2, Address(Rublock, Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes()));
1692   __ ldr(R3, Address(Rublock, Deoptimization::UnrollBlock::frame_sizes_offset_in_bytes()));
1693 
1694   __ add(SP, SP, Rtemp);
1695 
1696   // See if it is enough stack to push deoptimized frames
1697 #ifdef ASSERT
1698   // Compilers generate code that bang the stack by as much as the
1699   // interpreter would need. So this stack banging should never
1700   // trigger a fault. Verify that it does not on non product builds.
1701   if (UseStackBanging) {
1702     // The compiled method that we are deoptimizing was popped from the stack.
1703     // If the stack bang results in a stack overflow, we don&#39;t return to the
1704     // method that is being deoptimized. The stack overflow exception is
1705     // propagated to the caller of the deoptimized method. Need to get the pc
1706     // from the caller in LR and restore FP.
1707     __ ldr(LR, Address(R2, 0));
1708     __ ldr(FP, Address(Rublock, Deoptimization::UnrollBlock::initial_info_offset_in_bytes()));
1709     __ ldr_s32(R8, Address(Rublock, Deoptimization::UnrollBlock::total_frame_sizes_offset_in_bytes()));
1710     __ arm_stack_overflow_check(R8, Rtemp);
1711   }
1712 #endif
1713   __ ldr_s32(R8, Address(Rublock, Deoptimization::UnrollBlock::number_of_frames_offset_in_bytes()));
1714   __ ldr_s32(Rtemp, Address(Rublock, Deoptimization::UnrollBlock::caller_adjustment_offset_in_bytes()));
1715   __ mov(Rsender, SP);
1716   __ sub(SP, SP, Rtemp);
1717   //  __ ldr(FP, Address(FP));
1718   __ ldr(FP, Address(Rublock, Deoptimization::UnrollBlock::initial_info_offset_in_bytes()));
1719 
1720   // Push interpreter frames in a loop
1721   Label loop;
1722   __ bind(loop);
1723   __ ldr(LR, Address(R2, wordSize, post_indexed));         // load frame pc
1724   __ ldr(Rtemp, Address(R3, wordSize, post_indexed));      // load frame size
1725 
1726   __ raw_push(FP, LR);                                     // create new frame
1727   __ mov(FP, SP);
1728   __ sub(Rtemp, Rtemp, 2*wordSize);
1729 
1730   __ sub(SP, SP, Rtemp);
1731 
1732   __ str(Rsender, Address(FP, frame::interpreter_frame_sender_sp_offset * wordSize));
1733   __ mov(LR, 0);
1734   __ str(LR, Address(FP, frame::interpreter_frame_last_sp_offset * wordSize));
1735   __ subs(R8, R8, 1);                               // decrement counter
1736   __ mov(Rsender, SP);
1737   __ b(loop, ne);
1738 
1739   // Re-push self-frame
1740   __ ldr(LR, Address(R2));
1741   __ raw_push(FP, LR);
1742   __ mov(FP, SP);
1743 
1744   // Call unpack_frames with proper arguments
1745   __ mov(R0, Rthread);
1746   __ mov(R1, Deoptimization::Unpack_uncommon_trap);
1747   __ set_last_Java_frame(SP, FP, true, Rtemp);
1748   __ call_VM_leaf(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames));
1749   //  oop_maps-&gt;add_gc_map(__ pc() - start, new OopMap(frame_size_in_words, 0));
1750   __ reset_last_Java_frame(Rtemp);
1751 
1752   __ mov(SP, FP);
1753   __ pop(RegisterSet(FP) | RegisterSet(PC));
1754 
1755   masm-&gt;flush();
1756   _uncommon_trap_blob = UncommonTrapBlob::create(&amp;buffer, NULL, 2 /* LR+FP */);
1757 }
1758 
1759 #endif // COMPILER2
1760 
1761 //------------------------------generate_handler_blob------
1762 //
1763 // Generate a special Compile2Runtime blob that saves all registers,
1764 // setup oopmap, and calls safepoint code to stop the compiled code for
1765 // a safepoint.
1766 //
1767 SafepointBlob* SharedRuntime::generate_handler_blob(address call_ptr, int poll_type) {
1768   assert(StubRoutines::forward_exception_entry() != NULL, &quot;must be generated before&quot;);
1769 
1770   ResourceMark rm;
1771   CodeBuffer buffer(&quot;handler_blob&quot;, 256, 256);
1772   int frame_size_words;
1773   OopMapSet* oop_maps;
1774 
1775   bool cause_return = (poll_type == POLL_AT_RETURN);
1776 
1777   MacroAssembler* masm = new MacroAssembler(&amp;buffer);
1778   address start = __ pc();
1779   oop_maps = new OopMapSet();
1780 
1781   if (!cause_return) {
1782     __ sub(SP, SP, 4); // make room for LR which may still be live
1783                        // here if we are coming from a c2 method
1784   }
1785 
1786   OopMap* map = RegisterSaver::save_live_registers(masm, &amp;frame_size_words, !cause_return);
1787   if (!cause_return) {
1788     // update saved PC with correct value
1789     // need 2 steps because LR can be live in c2 method
1790     __ ldr(LR, Address(Rthread, JavaThread::saved_exception_pc_offset()));
1791     __ str(LR, Address(SP, RegisterSaver::LR_offset * wordSize));
1792   }
1793 
1794   __ mov(R0, Rthread);
1795   int pc_offset = __ set_last_Java_frame(SP, FP, false, Rtemp); // note: FP may not need to be saved (not on x86)
1796   assert(((__ pc()) - start) == __ offset(), &quot;warning: start differs from code_begin&quot;);
1797   __ call(call_ptr);
1798   if (pc_offset == -1) {
1799     pc_offset = __ offset();
1800   }
1801   oop_maps-&gt;add_gc_map(pc_offset, map);
1802   __ reset_last_Java_frame(Rtemp); // Rtemp free since scratched by far call
1803 
<a name="28" id="anc28"></a>



1804   if (!cause_return) {
<a name="29" id="anc29"></a><span class="line-added">1805     if (SafepointMechanism::uses_thread_local_poll()) {</span>
<span class="line-added">1806       // If our stashed return pc was modified by the runtime we avoid touching it</span>
<span class="line-added">1807       __ ldr(R3_tmp, Address(Rthread, JavaThread::saved_exception_pc_offset()));</span>
<span class="line-added">1808       __ ldr(R2_tmp, Address(SP, RegisterSaver::LR_offset * wordSize));</span>
<span class="line-added">1809       __ cmp(R2_tmp, R3_tmp);</span>
<span class="line-added">1810       // Adjust return pc forward to step over the safepoint poll instruction</span>
<span class="line-added">1811       __ add(R2_tmp, R2_tmp, 4, eq);</span>
<span class="line-added">1812       __ str(R2_tmp, Address(SP, RegisterSaver::LR_offset * wordSize), eq);</span>
<span class="line-added">1813     }</span>
<span class="line-added">1814 </span>
<span class="line-added">1815     // Check for pending exception</span>
<span class="line-added">1816     __ ldr(Rtemp, Address(Rthread, Thread::pending_exception_offset()));</span>
<span class="line-added">1817     __ cmp(Rtemp, 0);</span>
<span class="line-added">1818 </span>
1819     RegisterSaver::restore_live_registers(masm, false);
1820     __ pop(PC, eq);
1821     __ pop(Rexception_pc);
1822   } else {
<a name="30" id="anc30"></a><span class="line-added">1823     // Check for pending exception</span>
<span class="line-added">1824     __ ldr(Rtemp, Address(Rthread, Thread::pending_exception_offset()));</span>
<span class="line-added">1825     __ cmp(Rtemp, 0);</span>
<span class="line-added">1826 </span>
1827     RegisterSaver::restore_live_registers(masm);
1828     __ bx(LR, eq);
1829     __ mov(Rexception_pc, LR);
1830   }
1831 
1832   __ jump(StubRoutines::forward_exception_entry(), relocInfo::runtime_call_type, Rtemp);
1833 
1834   __ flush();
1835 
1836   return SafepointBlob::create(&amp;buffer, oop_maps, frame_size_words);
1837 }
1838 
1839 RuntimeStub* SharedRuntime::generate_resolve_blob(address destination, const char* name) {
1840   assert(StubRoutines::forward_exception_entry() != NULL, &quot;must be generated before&quot;);
1841 
1842   ResourceMark rm;
1843   CodeBuffer buffer(name, 1000, 512);
1844   int frame_size_words;
1845   OopMapSet *oop_maps;
1846   int frame_complete;
1847 
1848   MacroAssembler* masm = new MacroAssembler(&amp;buffer);
1849   Label pending_exception;
1850 
1851   int start = __ offset();
1852 
1853   oop_maps = new OopMapSet();
1854   OopMap* map = RegisterSaver::save_live_registers(masm, &amp;frame_size_words);
1855 
1856   frame_complete = __ offset();
1857 
1858   __ mov(R0, Rthread);
1859 
1860   int pc_offset = __ set_last_Java_frame(SP, FP, false, Rtemp);
1861   assert(start == 0, &quot;warning: start differs from code_begin&quot;);
1862   __ call(destination);
1863   if (pc_offset == -1) {
1864     pc_offset = __ offset();
1865   }
1866   oop_maps-&gt;add_gc_map(pc_offset, map);
1867   __ reset_last_Java_frame(Rtemp); // Rtemp free since scratched by far call
1868 
1869   __ ldr(R1, Address(Rthread, Thread::pending_exception_offset()));
1870   __ cbnz(R1, pending_exception);
1871 
1872   // Overwrite saved register values
1873 
1874   // Place metadata result of VM call into Rmethod
1875   __ get_vm_result_2(R1, Rtemp);
1876   __ str(R1, Address(SP, RegisterSaver::Rmethod_offset * wordSize));
1877 
1878   // Place target address (VM call result) into Rtemp
1879   __ str(R0, Address(SP, RegisterSaver::Rtemp_offset * wordSize));
1880 
1881   RegisterSaver::restore_live_registers(masm);
1882   __ jump(Rtemp);
1883 
1884   __ bind(pending_exception);
1885 
1886   RegisterSaver::restore_live_registers(masm);
1887   const Register Rzero = __ zero_register(Rtemp);
1888   __ str(Rzero, Address(Rthread, JavaThread::vm_result_2_offset()));
1889   __ mov(Rexception_pc, LR);
1890   __ jump(StubRoutines::forward_exception_entry(), relocInfo::runtime_call_type, Rtemp);
1891 
1892   __ flush();
1893 
1894   return RuntimeStub::new_runtime_stub(name, &amp;buffer, frame_complete, frame_size_words, oop_maps, true);
1895 }
<a name="31" id="anc31"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="31" type="hidden" />
</body>
</html>