<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/cpu/arm/stubGenerator_arm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="sharedRuntime_arm.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stubRoutinesCrypto_arm.cpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/arm/stubGenerator_arm.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 1,7 ***</span>
  /*
<span class="line-modified">!  * Copyright (c) 2008, 2018, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
<span class="line-new-header">--- 1,7 ---</span>
  /*
<span class="line-modified">!  * Copyright (c) 2008, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 26,10 ***</span>
<span class="line-new-header">--- 26,11 ---</span>
  #include &quot;asm/assembler.hpp&quot;
  #include &quot;assembler_arm.inline.hpp&quot;
  #include &quot;gc/shared/barrierSet.hpp&quot;
  #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  #include &quot;interpreter/interpreter.hpp&quot;
<span class="line-added">+ #include &quot;memory/universe.hpp&quot;</span>
  #include &quot;nativeInst_arm.hpp&quot;
  #include &quot;oops/instanceOop.hpp&quot;
  #include &quot;oops/method.hpp&quot;
  #include &quot;oops/objArrayKlass.hpp&quot;
  #include &quot;oops/oop.inline.hpp&quot;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 38,10 ***</span>
<span class="line-new-header">--- 39,11 ---</span>
  #include &quot;runtime/handles.inline.hpp&quot;
  #include &quot;runtime/sharedRuntime.hpp&quot;
  #include &quot;runtime/stubCodeGenerator.hpp&quot;
  #include &quot;runtime/stubRoutines.hpp&quot;
  #include &quot;utilities/align.hpp&quot;
<span class="line-added">+ #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  #ifdef COMPILER2
  #include &quot;opto/runtime.hpp&quot;
  #endif
  
  // Declaration and definition of StubGenerator (no .hpp file).
</pre>
<hr />
<pre>
<span class="line-old-header">*** 176,13 ***</span>
  
      assert(frame::entry_frame_call_wrapper_offset == 0, &quot;adjust this code&quot;);
  
      __ mov(Rtemp, SP);
      __ push(RegisterSet(FP) | RegisterSet(LR));
<span class="line-modified">! #ifndef __SOFTFP__</span>
<span class="line-removed">-     __ fstmdbd(SP, FloatRegisterSet(D8, 8), writeback);</span>
<span class="line-removed">- #endif</span>
      __ stmdb(SP, RegisterSet(R0, R2) | RegisterSet(R4, R6) | RegisterSet(R8, R10) | altFP_7_11, writeback);
      __ mov(Rmethod, R3);
      __ ldmia(Rtemp, RegisterSet(R1, R3) | Rthread); // stacked arguments
  
      // XXX: TODO
<span class="line-new-header">--- 178,11 ---</span>
  
      assert(frame::entry_frame_call_wrapper_offset == 0, &quot;adjust this code&quot;);
  
      __ mov(Rtemp, SP);
      __ push(RegisterSet(FP) | RegisterSet(LR));
<span class="line-modified">!     __ fpush_hardfp(FloatRegisterSet(D8, 8));</span>
      __ stmdb(SP, RegisterSet(R0, R2) | RegisterSet(R4, R6) | RegisterSet(R8, R10) | altFP_7_11, writeback);
      __ mov(Rmethod, R3);
      __ ldmia(Rtemp, RegisterSet(R1, R3) | Rthread); // stacked arguments
  
      // XXX: TODO
</pre>
<hr />
<pre>
<span class="line-old-header">*** 240,13 ***</span>
  
      __ bind(cont);
  #endif
  
      __ pop(RegisterSet(R4, R6) | RegisterSet(R8, R10) | altFP_7_11);
<span class="line-modified">! #ifndef __SOFTFP__</span>
<span class="line-removed">-     __ fldmiad(SP, FloatRegisterSet(D8, 8), writeback);</span>
<span class="line-removed">- #endif</span>
      __ pop(RegisterSet(FP) | RegisterSet(PC));
  
      return start;
    }
  
<span class="line-new-header">--- 240,11 ---</span>
  
      __ bind(cont);
  #endif
  
      __ pop(RegisterSet(R4, R6) | RegisterSet(R8, R10) | altFP_7_11);
<span class="line-modified">!     __ fpop_hardfp(FloatRegisterSet(D8, 8));</span>
      __ pop(RegisterSet(FP) | RegisterSet(PC));
  
      return start;
    }
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 434,11 ***</span>
    // Note: JDK 9 only supports ARMv7+ so we always have ldrexd available even though the
    // code below allows for it to be otherwise. The else clause indicates an ARMv5 system
    // for which we do not support MP and so membars are not necessary. This ARMv5 code will
    // be removed in the future.
  
<span class="line-modified">!   // Support for jint Atomic::add(jint add_value, volatile jint *dest)</span>
    //
    // Arguments :
    //
    //      add_value:      R0
    //      dest:           R1
<span class="line-new-header">--- 432,12 ---</span>
    // Note: JDK 9 only supports ARMv7+ so we always have ldrexd available even though the
    // code below allows for it to be otherwise. The else clause indicates an ARMv5 system
    // for which we do not support MP and so membars are not necessary. This ARMv5 code will
    // be removed in the future.
  
<span class="line-modified">!   // Implementation of atomic_add(jint add_value, volatile jint* dest)</span>
<span class="line-added">+   // used by Atomic::add(volatile jint* dest, jint add_value)</span>
    //
    // Arguments :
    //
    //      add_value:      R0
    //      dest:           R1
</pre>
<hr />
<pre>
<span class="line-old-header">*** 484,11 ***</span>
      __ bx(LR);
  
      return start;
    }
  
<span class="line-modified">!   // Support for jint Atomic::xchg(jint exchange_value, volatile jint *dest)</span>
    //
    // Arguments :
    //
    //      exchange_value: R0
    //      dest:           R1
<span class="line-new-header">--- 483,12 ---</span>
      __ bx(LR);
  
      return start;
    }
  
<span class="line-modified">!   // Implementation of jint atomic_xchg(jint exchange_value, volatile jint* dest)</span>
<span class="line-added">+   // used by Atomic::add(volatile jint* dest, jint exchange_value)</span>
    //
    // Arguments :
    //
    //      exchange_value: R0
    //      dest:           R1
</pre>
<hr />
<pre>
<span class="line-old-header">*** 532,11 ***</span>
      __ bx(LR);
  
      return start;
    }
  
<span class="line-modified">!   // Support for jint Atomic::cmpxchg(jint exchange_value, volatile jint *dest, jint compare_value)</span>
    //
    // Arguments :
    //
    //      compare_value:  R0
    //      exchange_value: R1
<span class="line-new-header">--- 532,12 ---</span>
      __ bx(LR);
  
      return start;
    }
  
<span class="line-modified">!   // Implementation of jint atomic_cmpxchg(jint exchange_value, volatile jint *dest, jint compare_value)</span>
<span class="line-added">+   // used by Atomic::cmpxchg(volatile jint *dest, jint compare_value, jint exchange_value)</span>
    //
    // Arguments :
    //
    //      compare_value:  R0
    //      exchange_value: R1
</pre>
<hr />
<pre>
<span class="line-old-header">*** 925,11 ***</span>
    // Increases &#39;from&#39; and &#39;to&#39; by count*bytes_per_count.
    //
    // Scratches &#39;count&#39;, R3.
    // R4-R10 are preserved (saved/restored).
    //
<span class="line-modified">!   int generate_forward_aligned_copy_loop(Register from, Register to, Register count, int bytes_per_count) {</span>
      assert (from == R0 &amp;&amp; to == R1 &amp;&amp; count == R2, &quot;adjust the implementation below&quot;);
  
      const int bytes_per_loop = 8*wordSize; // 8 registers are read and written on every loop iteration
      arraycopy_loop_config *config=&amp;arraycopy_configurations[ArmCopyPlatform].forward_aligned;
      int pld_offset = config-&gt;pld_distance;
<span class="line-new-header">--- 926,11 ---</span>
    // Increases &#39;from&#39; and &#39;to&#39; by count*bytes_per_count.
    //
    // Scratches &#39;count&#39;, R3.
    // R4-R10 are preserved (saved/restored).
    //
<span class="line-modified">!   int generate_forward_aligned_copy_loop(Register from, Register to, Register count, int bytes_per_count, bool unsafe_copy = false) {</span>
      assert (from == R0 &amp;&amp; to == R1 &amp;&amp; count == R2, &quot;adjust the implementation below&quot;);
  
      const int bytes_per_loop = 8*wordSize; // 8 registers are read and written on every loop iteration
      arraycopy_loop_config *config=&amp;arraycopy_configurations[ArmCopyPlatform].forward_aligned;
      int pld_offset = config-&gt;pld_distance;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 951,111 ***</span>
      const bool prefetch_before = pld_offset &lt; 0;
      const bool prefetch_after = pld_offset &gt; 0;
  
      Label L_skip_pld;
  
<span class="line-modified">!     // predecrease to exit when there is less than count_per_loop</span>
<span class="line-modified">!     __ sub_32(count, count, count_per_loop);</span>
  
<span class="line-modified">!     if (pld_offset != 0) {</span>
<span class="line-modified">!       pld_offset = (pld_offset &lt; 0) ? -pld_offset : pld_offset;</span>
  
<span class="line-modified">!       prefetch(from, to, 0);</span>
  
<span class="line-modified">!       if (prefetch_before) {</span>
<span class="line-modified">!         // If prefetch is done ahead, final PLDs that overflow the</span>
<span class="line-modified">!         // copied area can be easily avoided. &#39;count&#39; is predecreased</span>
<span class="line-modified">!         // by the prefetch distance to optimize the inner loop and the</span>
<span class="line-modified">!         // outer loop skips the PLD.</span>
<span class="line-modified">!         __ subs_32(count, count, (bytes_per_loop+pld_offset)/bytes_per_count);</span>
  
<span class="line-modified">!         // skip prefetch for small copies</span>
<span class="line-modified">!         __ b(L_skip_pld, lt);</span>
<span class="line-modified">!       }</span>
  
<span class="line-modified">!       int offset = ArmCopyCacheLineSize;</span>
<span class="line-modified">!       while (offset &lt;= pld_offset) {</span>
<span class="line-modified">!         prefetch(from, to, offset);</span>
<span class="line-modified">!         offset += ArmCopyCacheLineSize;</span>
<span class="line-modified">!       };</span>
<span class="line-modified">!     }</span>
  
<span class="line-modified">!     {</span>
<span class="line-modified">!       // 32-bit ARM note: we have tried implementing loop unrolling to skip one</span>
<span class="line-modified">!       // PLD with 64 bytes cache line but the gain was not significant.</span>
  
<span class="line-modified">!       Label L_copy_loop;</span>
<span class="line-modified">!       __ align(OptoLoopAlignment);</span>
<span class="line-modified">!       __ BIND(L_copy_loop);</span>
  
<span class="line-modified">!       if (prefetch_before) {</span>
<span class="line-modified">!         prefetch(from, to, bytes_per_loop + pld_offset);</span>
<span class="line-modified">!         __ BIND(L_skip_pld);</span>
<span class="line-modified">!       }</span>
  
<span class="line-modified">!       if (split_read) {</span>
<span class="line-modified">!         // Split the register set in two sets so that there is less</span>
<span class="line-modified">!         // latency between LDM and STM (R3-R6 available while R7-R10</span>
<span class="line-modified">!         // still loading) and less register locking issue when iterating</span>
<span class="line-modified">!         // on the first LDM.</span>
<span class="line-modified">!         __ ldmia(from, RegisterSet(R3, R6), writeback);</span>
<span class="line-modified">!         __ ldmia(from, RegisterSet(R7, R10), writeback);</span>
<span class="line-modified">!       } else {</span>
<span class="line-modified">!         __ ldmia(from, RegisterSet(R3, R10), writeback);</span>
<span class="line-modified">!       }</span>
  
<span class="line-modified">!       __ subs_32(count, count, count_per_loop);</span>
  
<span class="line-modified">!       if (prefetch_after) {</span>
<span class="line-modified">!         prefetch(from, to, pld_offset, bytes_per_loop);</span>
<span class="line-modified">!       }</span>
  
<span class="line-modified">!       if (split_write) {</span>
<span class="line-modified">!         __ stmia(to, RegisterSet(R3, R6), writeback);</span>
<span class="line-modified">!         __ stmia(to, RegisterSet(R7, R10), writeback);</span>
<span class="line-modified">!       } else {</span>
<span class="line-modified">!         __ stmia(to, RegisterSet(R3, R10), writeback);</span>
<span class="line-modified">!       }</span>
  
<span class="line-modified">!       __ b(L_copy_loop, ge);</span>
  
<span class="line-modified">!       if (prefetch_before) {</span>
<span class="line-modified">!         // the inner loop may end earlier, allowing to skip PLD for the last iterations</span>
<span class="line-modified">!         __ cmn_32(count, (bytes_per_loop + pld_offset)/bytes_per_count);</span>
<span class="line-modified">!         __ b(L_skip_pld, ge);</span>
        }
<span class="line-modified">!     }</span>
<span class="line-modified">!     BLOCK_COMMENT(&quot;Remaining bytes:&quot;);</span>
<span class="line-removed">-     // still 0..bytes_per_loop-1 aligned bytes to copy, count already decreased by (at least) bytes_per_loop bytes</span>
  
<span class="line-modified">!     // __ add(count, count, ...); // addition useless for the bit tests</span>
<span class="line-modified">!     assert (pld_offset % bytes_per_loop == 0, &quot;decreasing count by pld_offset before loop must not change tested bits&quot;);</span>
  
<span class="line-modified">!     __ tst(count, 16 / bytes_per_count);</span>
<span class="line-modified">!     __ ldmia(from, RegisterSet(R3, R6), writeback, ne); // copy 16 bytes</span>
<span class="line-modified">!     __ stmia(to, RegisterSet(R3, R6), writeback, ne);</span>
  
<span class="line-modified">!     __ tst(count, 8 / bytes_per_count);</span>
<span class="line-modified">!     __ ldmia(from, RegisterSet(R3, R4), writeback, ne); // copy 8 bytes</span>
<span class="line-modified">!     __ stmia(to, RegisterSet(R3, R4), writeback, ne);</span>
  
<span class="line-modified">!     if (bytes_per_count &lt;= 4) {</span>
<span class="line-modified">!       __ tst(count, 4 / bytes_per_count);</span>
<span class="line-modified">!       __ ldr(R3, Address(from, 4, post_indexed), ne); // copy 4 bytes</span>
<span class="line-modified">!       __ str(R3, Address(to, 4, post_indexed), ne);</span>
<span class="line-modified">!     }</span>
  
<span class="line-modified">!     if (bytes_per_count &lt;= 2) {</span>
<span class="line-modified">!       __ tst(count, 2 / bytes_per_count);</span>
<span class="line-modified">!       __ ldrh(R3, Address(from, 2, post_indexed), ne); // copy 2 bytes</span>
<span class="line-modified">!       __ strh(R3, Address(to, 2, post_indexed), ne);</span>
<span class="line-modified">!     }</span>
  
<span class="line-modified">!     if (bytes_per_count == 1) {</span>
<span class="line-modified">!       __ tst(count, 1);</span>
<span class="line-modified">!       __ ldrb(R3, Address(from, 1, post_indexed), ne);</span>
<span class="line-modified">!       __ strb(R3, Address(to, 1, post_indexed), ne);</span>
      }
  
      __ pop(RegisterSet(R4,R10));
  
      return count_per_loop;
<span class="line-new-header">--- 952,115 ---</span>
      const bool prefetch_before = pld_offset &lt; 0;
      const bool prefetch_after = pld_offset &gt; 0;
  
      Label L_skip_pld;
  
<span class="line-modified">!     {</span>
<span class="line-modified">!       // UnsafeCopyMemory page error: continue after ucm</span>
<span class="line-added">+       UnsafeCopyMemoryMark ucmm(this, unsafe_copy, true);</span>
<span class="line-added">+       // predecrease to exit when there is less than count_per_loop</span>
<span class="line-added">+       __ sub_32(count, count, count_per_loop);</span>
  
<span class="line-modified">!       if (pld_offset != 0) {</span>
<span class="line-modified">!         pld_offset = (pld_offset &lt; 0) ? -pld_offset : pld_offset;</span>
  
<span class="line-modified">!         prefetch(from, to, 0);</span>
  
<span class="line-modified">!         if (prefetch_before) {</span>
<span class="line-modified">!           // If prefetch is done ahead, final PLDs that overflow the</span>
<span class="line-modified">!           // copied area can be easily avoided. &#39;count&#39; is predecreased</span>
<span class="line-modified">!           // by the prefetch distance to optimize the inner loop and the</span>
<span class="line-modified">!           // outer loop skips the PLD.</span>
<span class="line-modified">!           __ subs_32(count, count, (bytes_per_loop+pld_offset)/bytes_per_count);</span>
  
<span class="line-modified">!           // skip prefetch for small copies</span>
<span class="line-modified">!           __ b(L_skip_pld, lt);</span>
<span class="line-modified">!         }</span>
  
<span class="line-modified">!         int offset = ArmCopyCacheLineSize;</span>
<span class="line-modified">!         while (offset &lt;= pld_offset) {</span>
<span class="line-modified">!           prefetch(from, to, offset);</span>
<span class="line-modified">!           offset += ArmCopyCacheLineSize;</span>
<span class="line-modified">!         };</span>
<span class="line-modified">!       }</span>
  
<span class="line-modified">!       {</span>
<span class="line-modified">!         // 32-bit ARM note: we have tried implementing loop unrolling to skip one</span>
<span class="line-modified">!         // PLD with 64 bytes cache line but the gain was not significant.</span>
  
<span class="line-modified">!         Label L_copy_loop;</span>
<span class="line-modified">!         __ align(OptoLoopAlignment);</span>
<span class="line-modified">!         __ BIND(L_copy_loop);</span>
  
<span class="line-modified">!         if (prefetch_before) {</span>
<span class="line-modified">!           prefetch(from, to, bytes_per_loop + pld_offset);</span>
<span class="line-modified">!           __ BIND(L_skip_pld);</span>
<span class="line-modified">!         }</span>
  
<span class="line-modified">!         if (split_read) {</span>
<span class="line-modified">!           // Split the register set in two sets so that there is less</span>
<span class="line-modified">!           // latency between LDM and STM (R3-R6 available while R7-R10</span>
<span class="line-modified">!           // still loading) and less register locking issue when iterating</span>
<span class="line-modified">!           // on the first LDM.</span>
<span class="line-modified">!           __ ldmia(from, RegisterSet(R3, R6), writeback);</span>
<span class="line-modified">!           __ ldmia(from, RegisterSet(R7, R10), writeback);</span>
<span class="line-modified">!         } else {</span>
<span class="line-modified">!           __ ldmia(from, RegisterSet(R3, R10), writeback);</span>
<span class="line-modified">!         }</span>
  
<span class="line-modified">!         __ subs_32(count, count, count_per_loop);</span>
  
<span class="line-modified">!         if (prefetch_after) {</span>
<span class="line-modified">!           prefetch(from, to, pld_offset, bytes_per_loop);</span>
<span class="line-modified">!         }</span>
  
<span class="line-modified">!         if (split_write) {</span>
<span class="line-modified">!           __ stmia(to, RegisterSet(R3, R6), writeback);</span>
<span class="line-modified">!           __ stmia(to, RegisterSet(R7, R10), writeback);</span>
<span class="line-modified">!         } else {</span>
<span class="line-modified">!           __ stmia(to, RegisterSet(R3, R10), writeback);</span>
<span class="line-modified">!         }</span>
  
<span class="line-modified">!         __ b(L_copy_loop, ge);</span>
  
<span class="line-modified">!         if (prefetch_before) {</span>
<span class="line-modified">!           // the inner loop may end earlier, allowing to skip PLD for the last iterations</span>
<span class="line-modified">!           __ cmn_32(count, (bytes_per_loop + pld_offset)/bytes_per_count);</span>
<span class="line-modified">!           __ b(L_skip_pld, ge);</span>
<span class="line-added">+         }</span>
        }
<span class="line-modified">!       BLOCK_COMMENT(&quot;Remaining bytes:&quot;);</span>
<span class="line-modified">!       // still 0..bytes_per_loop-1 aligned bytes to copy, count already decreased by (at least) bytes_per_loop bytes</span>
  
<span class="line-modified">!       // __ add(count, count, ...); // addition useless for the bit tests</span>
<span class="line-modified">!       assert (pld_offset % bytes_per_loop == 0, &quot;decreasing count by pld_offset before loop must not change tested bits&quot;);</span>
  
<span class="line-modified">!       __ tst(count, 16 / bytes_per_count);</span>
<span class="line-modified">!       __ ldmia(from, RegisterSet(R3, R6), writeback, ne); // copy 16 bytes</span>
<span class="line-modified">!       __ stmia(to, RegisterSet(R3, R6), writeback, ne);</span>
  
<span class="line-modified">!       __ tst(count, 8 / bytes_per_count);</span>
<span class="line-modified">!       __ ldmia(from, RegisterSet(R3, R4), writeback, ne); // copy 8 bytes</span>
<span class="line-modified">!       __ stmia(to, RegisterSet(R3, R4), writeback, ne);</span>
  
<span class="line-modified">!       if (bytes_per_count &lt;= 4) {</span>
<span class="line-modified">!         __ tst(count, 4 / bytes_per_count);</span>
<span class="line-modified">!         __ ldr(R3, Address(from, 4, post_indexed), ne); // copy 4 bytes</span>
<span class="line-modified">!         __ str(R3, Address(to, 4, post_indexed), ne);</span>
<span class="line-modified">!       }</span>
  
<span class="line-modified">!       if (bytes_per_count &lt;= 2) {</span>
<span class="line-modified">!         __ tst(count, 2 / bytes_per_count);</span>
<span class="line-modified">!         __ ldrh(R3, Address(from, 2, post_indexed), ne); // copy 2 bytes</span>
<span class="line-modified">!         __ strh(R3, Address(to, 2, post_indexed), ne);</span>
<span class="line-modified">!       }</span>
  
<span class="line-modified">!       if (bytes_per_count == 1) {</span>
<span class="line-modified">!         __ tst(count, 1);</span>
<span class="line-modified">!         __ ldrb(R3, Address(from, 1, post_indexed), ne);</span>
<span class="line-modified">!         __ strb(R3, Address(to, 1, post_indexed), ne);</span>
<span class="line-added">+       }</span>
      }
  
      __ pop(RegisterSet(R4,R10));
  
      return count_per_loop;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1080,11 ***</span>
    // Decreases &#39;end_from&#39; and &#39;end_to&#39; by count*bytes_per_count.
    //
    // Scratches &#39;count&#39;, R3.
    // ARM R4-R10 are preserved (saved/restored).
    //
<span class="line-modified">!   int generate_backward_aligned_copy_loop(Register end_from, Register end_to, Register count, int bytes_per_count) {</span>
      assert (end_from == R0 &amp;&amp; end_to == R1 &amp;&amp; count == R2, &quot;adjust the implementation below&quot;);
  
      const int bytes_per_loop = 8*wordSize; // 8 registers are read and written on every loop iteration
      const int count_per_loop = bytes_per_loop / bytes_per_count;
  
<span class="line-new-header">--- 1085,11 ---</span>
    // Decreases &#39;end_from&#39; and &#39;end_to&#39; by count*bytes_per_count.
    //
    // Scratches &#39;count&#39;, R3.
    // ARM R4-R10 are preserved (saved/restored).
    //
<span class="line-modified">!   int generate_backward_aligned_copy_loop(Register end_from, Register end_to, Register count, int bytes_per_count, bool unsafe_copy = false) {</span>
      assert (end_from == R0 &amp;&amp; end_to == R1 &amp;&amp; count == R2, &quot;adjust the implementation below&quot;);
  
      const int bytes_per_loop = 8*wordSize; // 8 registers are read and written on every loop iteration
      const int count_per_loop = bytes_per_loop / bytes_per_count;
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1096,106 ***</span>
  
      // See the forward copy variant for additional comments.
  
      __ push(RegisterSet(R4,R10));
  
<span class="line-modified">!     __ sub_32(count, count, count_per_loop);</span>
  
<span class="line-modified">!     const bool prefetch_before = pld_offset &lt; 0;</span>
<span class="line-modified">!     const bool prefetch_after = pld_offset &gt; 0;</span>
  
<span class="line-modified">!     Label L_skip_pld;</span>
  
<span class="line-modified">!     if (pld_offset != 0) {</span>
<span class="line-modified">!       pld_offset = (pld_offset &lt; 0) ? -pld_offset : pld_offset;</span>
  
<span class="line-modified">!       prefetch(end_from, end_to, -wordSize);</span>
  
<span class="line-modified">!       if (prefetch_before) {</span>
<span class="line-modified">!         __ subs_32(count, count, (bytes_per_loop + pld_offset) / bytes_per_count);</span>
<span class="line-modified">!         __ b(L_skip_pld, lt);</span>
<span class="line-modified">!       }</span>
  
<span class="line-modified">!       int offset = ArmCopyCacheLineSize;</span>
<span class="line-modified">!       while (offset &lt;= pld_offset) {</span>
<span class="line-modified">!         prefetch(end_from, end_to, -(wordSize + offset));</span>
<span class="line-modified">!         offset += ArmCopyCacheLineSize;</span>
<span class="line-modified">!       };</span>
<span class="line-modified">!     }</span>
  
<span class="line-modified">!     {</span>
<span class="line-modified">!       // 32-bit ARM note: we have tried implementing loop unrolling to skip one</span>
<span class="line-modified">!       // PLD with 64 bytes cache line but the gain was not significant.</span>
  
<span class="line-modified">!       Label L_copy_loop;</span>
<span class="line-modified">!       __ align(OptoLoopAlignment);</span>
<span class="line-modified">!       __ BIND(L_copy_loop);</span>
  
<span class="line-modified">!       if (prefetch_before) {</span>
<span class="line-modified">!         prefetch(end_from, end_to, -(wordSize + bytes_per_loop + pld_offset));</span>
<span class="line-modified">!         __ BIND(L_skip_pld);</span>
<span class="line-modified">!       }</span>
  
<span class="line-modified">!       if (split_read) {</span>
<span class="line-modified">!         __ ldmdb(end_from, RegisterSet(R7, R10), writeback);</span>
<span class="line-modified">!         __ ldmdb(end_from, RegisterSet(R3, R6), writeback);</span>
<span class="line-modified">!       } else {</span>
<span class="line-modified">!         __ ldmdb(end_from, RegisterSet(R3, R10), writeback);</span>
<span class="line-modified">!       }</span>
  
<span class="line-modified">!       __ subs_32(count, count, count_per_loop);</span>
  
<span class="line-modified">!       if (prefetch_after) {</span>
<span class="line-modified">!         prefetch(end_from, end_to, -(wordSize + pld_offset), -bytes_per_loop);</span>
<span class="line-modified">!       }</span>
  
<span class="line-modified">!       if (split_write) {</span>
<span class="line-modified">!         __ stmdb(end_to, RegisterSet(R7, R10), writeback);</span>
<span class="line-modified">!         __ stmdb(end_to, RegisterSet(R3, R6), writeback);</span>
<span class="line-modified">!       } else {</span>
<span class="line-modified">!         __ stmdb(end_to, RegisterSet(R3, R10), writeback);</span>
<span class="line-modified">!       }</span>
  
<span class="line-modified">!       __ b(L_copy_loop, ge);</span>
  
<span class="line-modified">!       if (prefetch_before) {</span>
<span class="line-modified">!         __ cmn_32(count, (bytes_per_loop + pld_offset)/bytes_per_count);</span>
<span class="line-modified">!         __ b(L_skip_pld, ge);</span>
        }
<span class="line-modified">!     }</span>
<span class="line-modified">!     BLOCK_COMMENT(&quot;Remaining bytes:&quot;);</span>
<span class="line-removed">-     // still 0..bytes_per_loop-1 aligned bytes to copy, count already decreased by (at least) bytes_per_loop bytes</span>
  
<span class="line-modified">!     // __ add(count, count, ...); // addition useless for the bit tests</span>
<span class="line-modified">!     assert (pld_offset % bytes_per_loop == 0, &quot;decreasing count by pld_offset before loop must not change tested bits&quot;);</span>
  
<span class="line-modified">!     __ tst(count, 16 / bytes_per_count);</span>
<span class="line-modified">!     __ ldmdb(end_from, RegisterSet(R3, R6), writeback, ne); // copy 16 bytes</span>
<span class="line-modified">!     __ stmdb(end_to, RegisterSet(R3, R6), writeback, ne);</span>
  
<span class="line-modified">!     __ tst(count, 8 / bytes_per_count);</span>
<span class="line-modified">!     __ ldmdb(end_from, RegisterSet(R3, R4), writeback, ne); // copy 8 bytes</span>
<span class="line-modified">!     __ stmdb(end_to, RegisterSet(R3, R4), writeback, ne);</span>
  
<span class="line-modified">!     if (bytes_per_count &lt;= 4) {</span>
<span class="line-modified">!       __ tst(count, 4 / bytes_per_count);</span>
<span class="line-modified">!       __ ldr(R3, Address(end_from, -4, pre_indexed), ne); // copy 4 bytes</span>
<span class="line-modified">!       __ str(R3, Address(end_to, -4, pre_indexed), ne);</span>
<span class="line-modified">!     }</span>
  
<span class="line-modified">!     if (bytes_per_count &lt;= 2) {</span>
<span class="line-modified">!       __ tst(count, 2 / bytes_per_count);</span>
<span class="line-modified">!       __ ldrh(R3, Address(end_from, -2, pre_indexed), ne); // copy 2 bytes</span>
<span class="line-modified">!       __ strh(R3, Address(end_to, -2, pre_indexed), ne);</span>
<span class="line-modified">!     }</span>
  
<span class="line-modified">!     if (bytes_per_count == 1) {</span>
<span class="line-modified">!       __ tst(count, 1);</span>
<span class="line-modified">!       __ ldrb(R3, Address(end_from, -1, pre_indexed), ne);</span>
<span class="line-modified">!       __ strb(R3, Address(end_to, -1, pre_indexed), ne);</span>
      }
<span class="line-removed">- </span>
      __ pop(RegisterSet(R4,R10));
  
      return count_per_loop;
    }
  
<span class="line-new-header">--- 1101,109 ---</span>
  
      // See the forward copy variant for additional comments.
  
      __ push(RegisterSet(R4,R10));
  
<span class="line-modified">!     {</span>
<span class="line-added">+       // UnsafeCopyMemory page error: continue after ucm</span>
<span class="line-added">+       UnsafeCopyMemoryMark ucmm(this, unsafe_copy, true);</span>
<span class="line-added">+       __ sub_32(count, count, count_per_loop);</span>
  
<span class="line-modified">!       const bool prefetch_before = pld_offset &lt; 0;</span>
<span class="line-modified">!       const bool prefetch_after = pld_offset &gt; 0;</span>
  
<span class="line-modified">!       Label L_skip_pld;</span>
  
<span class="line-modified">!       if (pld_offset != 0) {</span>
<span class="line-modified">!         pld_offset = (pld_offset &lt; 0) ? -pld_offset : pld_offset;</span>
  
<span class="line-modified">!         prefetch(end_from, end_to, -wordSize);</span>
  
<span class="line-modified">!         if (prefetch_before) {</span>
<span class="line-modified">!           __ subs_32(count, count, (bytes_per_loop + pld_offset) / bytes_per_count);</span>
<span class="line-modified">!           __ b(L_skip_pld, lt);</span>
<span class="line-modified">!         }</span>
  
<span class="line-modified">!         int offset = ArmCopyCacheLineSize;</span>
<span class="line-modified">!         while (offset &lt;= pld_offset) {</span>
<span class="line-modified">!           prefetch(end_from, end_to, -(wordSize + offset));</span>
<span class="line-modified">!           offset += ArmCopyCacheLineSize;</span>
<span class="line-modified">!         };</span>
<span class="line-modified">!       }</span>
  
<span class="line-modified">!       {</span>
<span class="line-modified">!         // 32-bit ARM note: we have tried implementing loop unrolling to skip one</span>
<span class="line-modified">!         // PLD with 64 bytes cache line but the gain was not significant.</span>
  
<span class="line-modified">!         Label L_copy_loop;</span>
<span class="line-modified">!         __ align(OptoLoopAlignment);</span>
<span class="line-modified">!         __ BIND(L_copy_loop);</span>
  
<span class="line-modified">!         if (prefetch_before) {</span>
<span class="line-modified">!           prefetch(end_from, end_to, -(wordSize + bytes_per_loop + pld_offset));</span>
<span class="line-modified">!           __ BIND(L_skip_pld);</span>
<span class="line-modified">!         }</span>
  
<span class="line-modified">!         if (split_read) {</span>
<span class="line-modified">!           __ ldmdb(end_from, RegisterSet(R7, R10), writeback);</span>
<span class="line-modified">!           __ ldmdb(end_from, RegisterSet(R3, R6), writeback);</span>
<span class="line-modified">!         } else {</span>
<span class="line-modified">!           __ ldmdb(end_from, RegisterSet(R3, R10), writeback);</span>
<span class="line-modified">!         }</span>
  
<span class="line-modified">!         __ subs_32(count, count, count_per_loop);</span>
  
<span class="line-modified">!         if (prefetch_after) {</span>
<span class="line-modified">!           prefetch(end_from, end_to, -(wordSize + pld_offset), -bytes_per_loop);</span>
<span class="line-modified">!         }</span>
  
<span class="line-modified">!         if (split_write) {</span>
<span class="line-modified">!           __ stmdb(end_to, RegisterSet(R7, R10), writeback);</span>
<span class="line-modified">!           __ stmdb(end_to, RegisterSet(R3, R6), writeback);</span>
<span class="line-modified">!         } else {</span>
<span class="line-modified">!           __ stmdb(end_to, RegisterSet(R3, R10), writeback);</span>
<span class="line-modified">!         }</span>
  
<span class="line-modified">!         __ b(L_copy_loop, ge);</span>
  
<span class="line-modified">!         if (prefetch_before) {</span>
<span class="line-modified">!           __ cmn_32(count, (bytes_per_loop + pld_offset)/bytes_per_count);</span>
<span class="line-modified">!           __ b(L_skip_pld, ge);</span>
<span class="line-added">+         }</span>
        }
<span class="line-modified">!       BLOCK_COMMENT(&quot;Remaining bytes:&quot;);</span>
<span class="line-modified">!       // still 0..bytes_per_loop-1 aligned bytes to copy, count already decreased by (at least) bytes_per_loop bytes</span>
  
<span class="line-modified">!       // __ add(count, count, ...); // addition useless for the bit tests</span>
<span class="line-modified">!       assert (pld_offset % bytes_per_loop == 0, &quot;decreasing count by pld_offset before loop must not change tested bits&quot;);</span>
  
<span class="line-modified">!       __ tst(count, 16 / bytes_per_count);</span>
<span class="line-modified">!       __ ldmdb(end_from, RegisterSet(R3, R6), writeback, ne); // copy 16 bytes</span>
<span class="line-modified">!       __ stmdb(end_to, RegisterSet(R3, R6), writeback, ne);</span>
  
<span class="line-modified">!       __ tst(count, 8 / bytes_per_count);</span>
<span class="line-modified">!       __ ldmdb(end_from, RegisterSet(R3, R4), writeback, ne); // copy 8 bytes</span>
<span class="line-modified">!       __ stmdb(end_to, RegisterSet(R3, R4), writeback, ne);</span>
  
<span class="line-modified">!       if (bytes_per_count &lt;= 4) {</span>
<span class="line-modified">!         __ tst(count, 4 / bytes_per_count);</span>
<span class="line-modified">!         __ ldr(R3, Address(end_from, -4, pre_indexed), ne); // copy 4 bytes</span>
<span class="line-modified">!         __ str(R3, Address(end_to, -4, pre_indexed), ne);</span>
<span class="line-modified">!       }</span>
  
<span class="line-modified">!       if (bytes_per_count &lt;= 2) {</span>
<span class="line-modified">!         __ tst(count, 2 / bytes_per_count);</span>
<span class="line-modified">!         __ ldrh(R3, Address(end_from, -2, pre_indexed), ne); // copy 2 bytes</span>
<span class="line-modified">!         __ strh(R3, Address(end_to, -2, pre_indexed), ne);</span>
<span class="line-modified">!       }</span>
  
<span class="line-modified">!       if (bytes_per_count == 1) {</span>
<span class="line-modified">!         __ tst(count, 1);</span>
<span class="line-modified">!         __ ldrb(R3, Address(end_from, -1, pre_indexed), ne);</span>
<span class="line-modified">!         __ strb(R3, Address(end_to, -1, pre_indexed), ne);</span>
<span class="line-added">+       }</span>
      }
      __ pop(RegisterSet(R4,R10));
  
      return count_per_loop;
    }
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1746,21 ***</span>
    //     bytes_per_count:   size of an element
    //     forward:           specifies copy direction
    //
    // Notes:
    //     shifts &#39;from&#39; and &#39;to&#39;
<span class="line-modified">!   void copy_small_array(Register from, Register to, Register count, Register tmp, Register tmp2, int bytes_per_count, bool forward, Label &amp; entry) {</span>
      assert_different_registers(from, to, count, tmp);
  
<span class="line-modified">!     __ align(OptoLoopAlignment);</span>
<span class="line-modified">!     Label L_small_loop;</span>
<span class="line-modified">!     __ BIND(L_small_loop);</span>
<span class="line-modified">!     store_one(tmp, to, bytes_per_count, forward, al, tmp2);</span>
<span class="line-modified">!     __ BIND(entry); // entry point</span>
<span class="line-modified">!     __ subs(count, count, 1);</span>
<span class="line-modified">!     load_one(tmp, from, bytes_per_count, forward, ge, tmp2);</span>
<span class="line-modified">!     __ b(L_small_loop, ge);</span>
    }
  
    // Aligns &#39;to&#39; by reading one word from &#39;from&#39; and writting its part to &#39;to&#39;.
    //
    // Arguments:
<span class="line-new-header">--- 1754,25 ---</span>
    //     bytes_per_count:   size of an element
    //     forward:           specifies copy direction
    //
    // Notes:
    //     shifts &#39;from&#39; and &#39;to&#39;
<span class="line-modified">!   void copy_small_array(Register from, Register to, Register count, Register tmp, Register tmp2, int bytes_per_count, bool forward, Label &amp; entry, bool unsafe_copy = false) {</span>
      assert_different_registers(from, to, count, tmp);
  
<span class="line-modified">!     {</span>
<span class="line-modified">!       // UnsafeCopyMemory page error: continue after ucm</span>
<span class="line-modified">!       UnsafeCopyMemoryMark ucmm(this, unsafe_copy, true);</span>
<span class="line-modified">!       __ align(OptoLoopAlignment);</span>
<span class="line-modified">!       Label L_small_loop;</span>
<span class="line-modified">!       __ BIND(L_small_loop);</span>
<span class="line-modified">!       store_one(tmp, to, bytes_per_count, forward, al, tmp2);</span>
<span class="line-modified">!       __ BIND(entry); // entry point</span>
<span class="line-added">+       __ subs(count, count, 1);</span>
<span class="line-added">+       load_one(tmp, from, bytes_per_count, forward, ge, tmp2);</span>
<span class="line-added">+       __ b(L_small_loop, ge);</span>
<span class="line-added">+     }</span>
    }
  
    // Aligns &#39;to&#39; by reading one word from &#39;from&#39; and writting its part to &#39;to&#39;.
    //
    // Arguments:
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1873,74 ***</span>
    //     &#39;to&#39; must be aligned by bytes_per_count but must not be aligned by wordSize
    //     shifts &#39;to&#39; by the number of copied bytes
    //
    // Scratches &#39;from&#39;, &#39;count&#39;, R3 and R12.
    // R4-R10 saved for use.
<span class="line-modified">!   int align_dst_and_generate_shifted_copy_loop(Register from, Register to, Register count, int bytes_per_count, bool forward) {</span>
  
      const Register Rval = forward ? R12 : R3; // as generate_{forward,backward}_shifted_copy_loop expect
  
      int min_copy = 0;
  
      // Note: if {seq} is a sequence of numbers, L{seq} means that if the execution reaches this point,
      // then the remainder of &#39;to&#39; divided by wordSize is one of elements of {seq}.
  
      __ push(RegisterSet(R4,R10));
<span class="line-removed">-     load_one(Rval, from, wordSize, forward);</span>
  
<span class="line-modified">!     switch (bytes_per_count) {</span>
<span class="line-modified">!       case 2:</span>
<span class="line-modified">!         min_copy = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);</span>
<span class="line-modified">!         break;</span>
<span class="line-modified">!       case 1:</span>
<span class="line-modified">!       {</span>
<span class="line-modified">!         Label L1, L2, L3;</span>
<span class="line-modified">!         int min_copy1, min_copy2, min_copy3;</span>
<span class="line-modified">! </span>
<span class="line-modified">!         Label L_loop_finished;</span>
<span class="line-modified">! </span>
<span class="line-modified">!         if (forward) {</span>
<span class="line-modified">!             __ tbz(to, 0, L2);</span>
<span class="line-modified">!             __ tbz(to, 1, L1);</span>
  
<span class="line-modified">!             __ BIND(L3);</span>
<span class="line-modified">!             min_copy3 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 3, bytes_per_count, forward);</span>
<span class="line-modified">!             __ b(L_loop_finished);</span>
  
<span class="line-modified">!             __ BIND(L1);</span>
<span class="line-modified">!             min_copy1 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 1, bytes_per_count, forward);</span>
<span class="line-modified">!             __ b(L_loop_finished);</span>
  
<span class="line-modified">!             __ BIND(L2);</span>
<span class="line-modified">!             min_copy2 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);</span>
<span class="line-modified">!         } else {</span>
<span class="line-removed">-             __ tbz(to, 0, L2);</span>
<span class="line-removed">-             __ tbnz(to, 1, L3);</span>
  
<span class="line-modified">!             __ BIND(L1);</span>
<span class="line-removed">-             min_copy1 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 1, bytes_per_count, forward);</span>
<span class="line-removed">-             __ b(L_loop_finished);</span>
  
<span class="line-modified">!              __ BIND(L3);</span>
<span class="line-removed">-             min_copy3 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 3, bytes_per_count, forward);</span>
<span class="line-removed">-             __ b(L_loop_finished);</span>
  
<span class="line-modified">!            __ BIND(L2);</span>
<span class="line-removed">-             min_copy2 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);</span>
          }
<span class="line-modified">! </span>
<span class="line-modified">!         min_copy = MAX2(MAX2(min_copy1, min_copy2), min_copy3);</span>
<span class="line-modified">! </span>
<span class="line-removed">-         __ BIND(L_loop_finished);</span>
<span class="line-removed">- </span>
<span class="line-removed">-         break;</span>
        }
<span class="line-removed">-       default:</span>
<span class="line-removed">-         ShouldNotReachHere();</span>
<span class="line-removed">-         break;</span>
      }
<span class="line-removed">- </span>
      __ pop(RegisterSet(R4,R10));
  
      return min_copy;
    }
  
<span class="line-new-header">--- 1885,78 ---</span>
    //     &#39;to&#39; must be aligned by bytes_per_count but must not be aligned by wordSize
    //     shifts &#39;to&#39; by the number of copied bytes
    //
    // Scratches &#39;from&#39;, &#39;count&#39;, R3 and R12.
    // R4-R10 saved for use.
<span class="line-modified">!   int align_dst_and_generate_shifted_copy_loop(Register from, Register to, Register count, int bytes_per_count, bool forward, bool unsafe_copy = false) {</span>
  
      const Register Rval = forward ? R12 : R3; // as generate_{forward,backward}_shifted_copy_loop expect
  
      int min_copy = 0;
  
      // Note: if {seq} is a sequence of numbers, L{seq} means that if the execution reaches this point,
      // then the remainder of &#39;to&#39; divided by wordSize is one of elements of {seq}.
  
      __ push(RegisterSet(R4,R10));
  
<span class="line-modified">!     {</span>
<span class="line-modified">!       // UnsafeCopyMemory page error: continue after ucm</span>
<span class="line-modified">!       UnsafeCopyMemoryMark ucmm(this, unsafe_copy, true);</span>
<span class="line-modified">!       load_one(Rval, from, wordSize, forward);</span>
<span class="line-modified">! </span>
<span class="line-modified">!       switch (bytes_per_count) {</span>
<span class="line-modified">!         case 2:</span>
<span class="line-modified">!           min_copy = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);</span>
<span class="line-modified">!           break;</span>
<span class="line-modified">!         case 1:</span>
<span class="line-modified">!         {</span>
<span class="line-modified">!           Label L1, L2, L3;</span>
<span class="line-modified">!           int min_copy1, min_copy2, min_copy3;</span>
<span class="line-modified">! </span>
<span class="line-added">+           Label L_loop_finished;</span>
<span class="line-added">+ </span>
<span class="line-added">+           if (forward) {</span>
<span class="line-added">+               __ tbz(to, 0, L2);</span>
<span class="line-added">+               __ tbz(to, 1, L1);</span>
<span class="line-added">+ </span>
<span class="line-added">+               __ BIND(L3);</span>
<span class="line-added">+               min_copy3 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 3, bytes_per_count, forward);</span>
<span class="line-added">+               __ b(L_loop_finished);</span>
<span class="line-added">+ </span>
<span class="line-added">+               __ BIND(L1);</span>
<span class="line-added">+               min_copy1 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 1, bytes_per_count, forward);</span>
<span class="line-added">+               __ b(L_loop_finished);</span>
<span class="line-added">+ </span>
<span class="line-added">+               __ BIND(L2);</span>
<span class="line-added">+               min_copy2 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);</span>
<span class="line-added">+           } else {</span>
<span class="line-added">+               __ tbz(to, 0, L2);</span>
<span class="line-added">+               __ tbnz(to, 1, L3);</span>
  
<span class="line-modified">!               __ BIND(L1);</span>
<span class="line-modified">!               min_copy1 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 1, bytes_per_count, forward);</span>
<span class="line-modified">!               __ b(L_loop_finished);</span>
  
<span class="line-modified">!                __ BIND(L3);</span>
<span class="line-modified">!               min_copy3 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 3, bytes_per_count, forward);</span>
<span class="line-modified">!               __ b(L_loop_finished);</span>
  
<span class="line-modified">!              __ BIND(L2);</span>
<span class="line-modified">!               min_copy2 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);</span>
<span class="line-modified">!           }</span>
  
<span class="line-modified">!           min_copy = MAX2(MAX2(min_copy1, min_copy2), min_copy3);</span>
  
<span class="line-modified">!           __ BIND(L_loop_finished);</span>
  
<span class="line-modified">!           break;</span>
          }
<span class="line-modified">!         default:</span>
<span class="line-modified">!           ShouldNotReachHere();</span>
<span class="line-modified">!           break;</span>
        }
      }
      __ pop(RegisterSet(R4,R10));
  
      return min_copy;
    }
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1960,10 ***</span>
<span class="line-new-header">--- 1976,17 ---</span>
          return NULL;
      }
    }
  #endif // !PRODUCT
  
<span class="line-added">+   address generate_unsafecopy_common_error_exit() {</span>
<span class="line-added">+     address start_pc = __ pc();</span>
<span class="line-added">+       __ mov(R0, 0);</span>
<span class="line-added">+       __ ret();</span>
<span class="line-added">+     return start_pc;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
    //
    //  Generate stub for primitive array copy.  If &quot;aligned&quot; is true, the
    //  &quot;from&quot; and &quot;to&quot; addresses are assumed to be heapword aligned.
    //
    //  If &quot;disjoint&quot; is true, arrays are assumed to be disjoint, otherwise they may overlap and
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2030,12 ***</span>
          // if &#39;from&#39; is heapword aligned and HeapWordSize is divisible by 8,
          //  then from is aligned by 8
          from_is_aligned = true;
      }
  
<span class="line-modified">!     int count_required_to_align = from_is_aligned ? 0 : align_src(from, to, count, tmp1, bytes_per_count, forward);</span>
<span class="line-modified">!     assert (small_copy_limit &gt;= count_required_to_align, &quot;alignment could exhaust count&quot;);</span>
  
      // now &#39;from&#39; is aligned
  
      bool to_is_aligned = false;
  
<span class="line-new-header">--- 2053,17 ---</span>
          // if &#39;from&#39; is heapword aligned and HeapWordSize is divisible by 8,
          //  then from is aligned by 8
          from_is_aligned = true;
      }
  
<span class="line-modified">!     int count_required_to_align = 0;</span>
<span class="line-modified">!     {</span>
<span class="line-added">+       // UnsafeCopyMemoryMark page error: continue at UnsafeCopyMemory common_error_exit</span>
<span class="line-added">+       UnsafeCopyMemoryMark ucmm(this, !aligned, false);</span>
<span class="line-added">+       count_required_to_align = from_is_aligned ? 0 : align_src(from, to, count, tmp1, bytes_per_count, forward);</span>
<span class="line-added">+       assert (small_copy_limit &gt;= count_required_to_align, &quot;alignment could exhaust count&quot;);</span>
<span class="line-added">+     }</span>
  
      // now &#39;from&#39; is aligned
  
      bool to_is_aligned = false;
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2061,35 ***</span>
  
      // &#39;from&#39; and &#39;to&#39; are properly aligned
  
      int min_copy;
      if (forward) {
<span class="line-modified">!       min_copy = generate_forward_aligned_copy_loop (from, to, count, bytes_per_count);</span>
      } else {
<span class="line-modified">!       min_copy = generate_backward_aligned_copy_loop(from, to, count, bytes_per_count);</span>
      }
      assert(small_copy_limit &gt;= count_required_to_align + min_copy, &quot;first loop might exhaust count&quot;);
  
      if (status) {
        __ mov(R0, 0); // OK
      }
  
      __ ret();
  
      {
<span class="line-modified">!       copy_small_array(from, to, count, tmp1, tmp2, bytes_per_count, forward, L_small_array /* entry */);</span>
  
        if (status) {
          __ mov(R0, 0); // OK
        }
  
        __ ret();
      }
  
      if (! to_is_aligned) {
        __ BIND(L_unaligned_dst);
<span class="line-modified">!       int min_copy_shifted = align_dst_and_generate_shifted_copy_loop(from, to, count, bytes_per_count, forward);</span>
        assert (small_copy_limit &gt;= count_required_to_align + min_copy_shifted, &quot;first loop might exhaust count&quot;);
  
        if (status) {
          __ mov(R0, 0); // OK
        }
<span class="line-new-header">--- 2089,35 ---</span>
  
      // &#39;from&#39; and &#39;to&#39; are properly aligned
  
      int min_copy;
      if (forward) {
<span class="line-modified">!       min_copy = generate_forward_aligned_copy_loop(from, to, count, bytes_per_count, !aligned /*add UnsafeCopyMemory entry*/);</span>
      } else {
<span class="line-modified">!       min_copy = generate_backward_aligned_copy_loop(from, to, count, bytes_per_count, !aligned /*add UnsafeCopyMemory entry*/);</span>
      }
      assert(small_copy_limit &gt;= count_required_to_align + min_copy, &quot;first loop might exhaust count&quot;);
  
      if (status) {
        __ mov(R0, 0); // OK
      }
  
      __ ret();
  
      {
<span class="line-modified">!       copy_small_array(from, to, count, tmp1, tmp2, bytes_per_count, forward, L_small_array /* entry */, !aligned /*add UnsafeCopyMemory entry*/);</span>
  
        if (status) {
          __ mov(R0, 0); // OK
        }
  
        __ ret();
      }
  
      if (! to_is_aligned) {
        __ BIND(L_unaligned_dst);
<span class="line-modified">!       int min_copy_shifted = align_dst_and_generate_shifted_copy_loop(from, to, count, bytes_per_count, forward, !aligned /*add UnsafeCopyMemory entry*/);</span>
        assert (small_copy_limit &gt;= count_required_to_align + min_copy_shifted, &quot;first loop might exhaust count&quot;);
  
        if (status) {
          __ mov(R0, 0); // OK
        }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2870,10 ***</span>
<span class="line-new-header">--- 2898,13 ---</span>
      // lead to errors since the copy has already been performed).
  
      status = true; // generate a status compatible with C1 calls
  #endif
  
<span class="line-added">+     address ucm_common_error_exit       =  generate_unsafecopy_common_error_exit();</span>
<span class="line-added">+     UnsafeCopyMemory::set_common_exit_stub_pc(ucm_common_error_exit);</span>
<span class="line-added">+ </span>
      // these need always status in case they are called from generic_arraycopy
      StubRoutines::_jbyte_disjoint_arraycopy  = generate_primitive_copy(false, &quot;jbyte_disjoint_arraycopy&quot;,  true, 1, true);
      StubRoutines::_jshort_disjoint_arraycopy = generate_primitive_copy(false, &quot;jshort_disjoint_arraycopy&quot;, true, 2, true);
      StubRoutines::_jint_disjoint_arraycopy   = generate_primitive_copy(false, &quot;jint_disjoint_arraycopy&quot;,   true, 4, true);
      StubRoutines::_jlong_disjoint_arraycopy  = generate_primitive_copy(false, &quot;jlong_disjoint_arraycopy&quot;,  true, 8, true);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 3052,8 ***</span>
<span class="line-new-header">--- 3083,12 ---</span>
        generate_initial();
      }
    }
  }; // end class declaration
  
<span class="line-added">+ #define UCM_TABLE_MAX_ENTRIES 32</span>
  void StubGenerator_generate(CodeBuffer* code, bool all) {
<span class="line-added">+   if (UnsafeCopyMemory::_table == NULL) {</span>
<span class="line-added">+     UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);</span>
<span class="line-added">+   }</span>
    StubGenerator g(code, all);
  }
</pre>
<center><a href="sharedRuntime_arm.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stubRoutinesCrypto_arm.cpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>