<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/arm/templateTable_arm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
<a name="1" id="anc1"></a><span class="line-modified">   2  * Copyright (c) 2008, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.inline.hpp&quot;
  27 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  28 #include &quot;interpreter/interp_masm.hpp&quot;
  29 #include &quot;interpreter/interpreter.hpp&quot;
  30 #include &quot;interpreter/interpreterRuntime.hpp&quot;
  31 #include &quot;interpreter/templateTable.hpp&quot;
  32 #include &quot;memory/universe.hpp&quot;
  33 #include &quot;oops/cpCache.hpp&quot;
  34 #include &quot;oops/methodData.hpp&quot;
  35 #include &quot;oops/objArrayKlass.hpp&quot;
  36 #include &quot;oops/oop.inline.hpp&quot;
  37 #include &quot;prims/methodHandles.hpp&quot;
  38 #include &quot;runtime/frame.inline.hpp&quot;
  39 #include &quot;runtime/sharedRuntime.hpp&quot;
  40 #include &quot;runtime/stubRoutines.hpp&quot;
  41 #include &quot;runtime/synchronizer.hpp&quot;
<a name="2" id="anc2"></a>
  42 
  43 #define __ _masm-&gt;
  44 
  45 //----------------------------------------------------------------------------------------------------
  46 // Platform-dependent initialization
  47 
  48 void TemplateTable::pd_initialize() {
  49   // No arm specific initialization
  50 }
  51 
  52 //----------------------------------------------------------------------------------------------------
  53 // Address computation
  54 
  55 // local variables
  56 static inline Address iaddress(int n)            {
  57   return Address(Rlocals, Interpreter::local_offset_in_bytes(n));
  58 }
  59 
  60 static inline Address laddress(int n)            { return iaddress(n + 1); }
  61 static inline Address haddress(int n)            { return iaddress(n + 0); }
  62 
  63 static inline Address faddress(int n)            { return iaddress(n); }
  64 static inline Address daddress(int n)            { return laddress(n); }
  65 static inline Address aaddress(int n)            { return iaddress(n); }
  66 
  67 
  68 void TemplateTable::get_local_base_addr(Register r, Register index) {
  69   __ sub(r, Rlocals, AsmOperand(index, lsl, Interpreter::logStackElementSize));
  70 }
  71 
  72 Address TemplateTable::load_iaddress(Register index, Register scratch) {
  73   return Address(Rlocals, index, lsl, Interpreter::logStackElementSize, basic_offset, sub_offset);
  74 }
  75 
  76 Address TemplateTable::load_aaddress(Register index, Register scratch) {
  77   return load_iaddress(index, scratch);
  78 }
  79 
  80 Address TemplateTable::load_faddress(Register index, Register scratch) {
  81 #ifdef __SOFTFP__
  82   return load_iaddress(index, scratch);
  83 #else
  84   get_local_base_addr(scratch, index);
  85   return Address(scratch);
  86 #endif // __SOFTFP__
  87 }
  88 
  89 Address TemplateTable::load_daddress(Register index, Register scratch) {
  90   get_local_base_addr(scratch, index);
  91   return Address(scratch, Interpreter::local_offset_in_bytes(1));
  92 }
  93 
  94 // At top of Java expression stack which may be different than SP.
  95 // It isn&#39;t for category 1 objects.
  96 static inline Address at_tos() {
  97   return Address(Rstack_top, Interpreter::expr_offset_in_bytes(0));
  98 }
  99 
 100 static inline Address at_tos_p1() {
 101   return Address(Rstack_top, Interpreter::expr_offset_in_bytes(1));
 102 }
 103 
 104 static inline Address at_tos_p2() {
 105   return Address(Rstack_top, Interpreter::expr_offset_in_bytes(2));
 106 }
 107 
 108 
 109 // Loads double/long local into R0_tos_lo/R1_tos_hi with two
 110 // separate ldr instructions (supports nonadjacent values).
 111 // Used for longs in all modes, and for doubles in SOFTFP mode.
 112 void TemplateTable::load_category2_local(Register Rlocal_index, Register tmp) {
 113   const Register Rlocal_base = tmp;
 114   assert_different_registers(Rlocal_index, tmp);
 115 
 116   get_local_base_addr(Rlocal_base, Rlocal_index);
 117   __ ldr(R0_tos_lo, Address(Rlocal_base, Interpreter::local_offset_in_bytes(1)));
 118   __ ldr(R1_tos_hi, Address(Rlocal_base, Interpreter::local_offset_in_bytes(0)));
 119 }
 120 
 121 
 122 // Stores R0_tos_lo/R1_tos_hi to double/long local with two
 123 // separate str instructions (supports nonadjacent values).
 124 // Used for longs in all modes, and for doubles in SOFTFP mode
 125 void TemplateTable::store_category2_local(Register Rlocal_index, Register tmp) {
 126   const Register Rlocal_base = tmp;
 127   assert_different_registers(Rlocal_index, tmp);
 128 
 129   get_local_base_addr(Rlocal_base, Rlocal_index);
 130   __ str(R0_tos_lo, Address(Rlocal_base, Interpreter::local_offset_in_bytes(1)));
 131   __ str(R1_tos_hi, Address(Rlocal_base, Interpreter::local_offset_in_bytes(0)));
 132 }
 133 
 134 // Returns address of Java array element using temp register as address base.
 135 Address TemplateTable::get_array_elem_addr(BasicType elemType, Register array, Register index, Register temp) {
 136   int logElemSize = exact_log2(type2aelembytes(elemType));
 137   __ add_ptr_scaled_int32(temp, array, index, logElemSize);
 138   return Address(temp, arrayOopDesc::base_offset_in_bytes(elemType));
 139 }
 140 
 141 // Returns address of Java array element using temp register as offset from array base
 142 Address TemplateTable::get_array_elem_addr_same_base(BasicType elemType, Register array, Register index, Register temp) {
 143   int logElemSize = exact_log2(type2aelembytes(elemType));
 144   if (logElemSize == 0) {
 145     __ add(temp, index, arrayOopDesc::base_offset_in_bytes(elemType));
 146   } else {
 147     __ mov(temp, arrayOopDesc::base_offset_in_bytes(elemType));
 148     __ add_ptr_scaled_int32(temp, temp, index, logElemSize);
 149   }
 150   return Address(array, temp);
 151 }
 152 
 153 //----------------------------------------------------------------------------------------------------
 154 // Condition conversion
 155 AsmCondition convNegCond(TemplateTable::Condition cc) {
 156   switch (cc) {
 157     case TemplateTable::equal        : return ne;
 158     case TemplateTable::not_equal    : return eq;
 159     case TemplateTable::less         : return ge;
 160     case TemplateTable::less_equal   : return gt;
 161     case TemplateTable::greater      : return le;
 162     case TemplateTable::greater_equal: return lt;
 163   }
 164   ShouldNotReachHere();
 165   return nv;
 166 }
 167 
 168 //----------------------------------------------------------------------------------------------------
 169 // Miscelaneous helper routines
 170 
 171 // Store an oop (or NULL) at the address described by obj.
 172 // Blows all volatile registers R0-R3, Rtemp, LR).
 173 // Also destroys new_val and obj.base().
 174 static void do_oop_store(InterpreterMacroAssembler* _masm,
 175                          Address obj,
 176                          Register new_val,
 177                          Register tmp1,
 178                          Register tmp2,
 179                          Register tmp3,
 180                          bool is_null,
 181                          DecoratorSet decorators = 0) {
 182 
 183   assert_different_registers(obj.base(), new_val, tmp1, tmp2, tmp3, noreg);
 184   if (is_null) {
 185     __ store_heap_oop_null(obj, new_val, tmp1, tmp2, tmp3, decorators);
 186   } else {
 187     __ store_heap_oop(obj, new_val, tmp1, tmp2, tmp3, decorators);
 188   }
 189 }
 190 
 191 static void do_oop_load(InterpreterMacroAssembler* _masm,
 192                         Register dst,
 193                         Address obj,
 194                         DecoratorSet decorators = 0) {
 195   __ load_heap_oop(dst, obj, noreg, noreg, noreg, decorators);
 196 }
 197 
 198 Address TemplateTable::at_bcp(int offset) {
 199   assert(_desc-&gt;uses_bcp(), &quot;inconsistent uses_bcp information&quot;);
 200   return Address(Rbcp, offset);
 201 }
 202 
 203 
 204 // Blows volatile registers R0-R3, Rtemp, LR.
 205 void TemplateTable::patch_bytecode(Bytecodes::Code bc, Register bc_reg,
 206                                    Register temp_reg, bool load_bc_into_bc_reg/*=true*/,
 207                                    int byte_no) {
 208   assert_different_registers(bc_reg, temp_reg);
 209   if (!RewriteBytecodes)  return;
 210   Label L_patch_done;
 211 
 212   switch (bc) {
 213   case Bytecodes::_fast_aputfield:
 214   case Bytecodes::_fast_bputfield:
 215   case Bytecodes::_fast_zputfield:
 216   case Bytecodes::_fast_cputfield:
 217   case Bytecodes::_fast_dputfield:
 218   case Bytecodes::_fast_fputfield:
 219   case Bytecodes::_fast_iputfield:
 220   case Bytecodes::_fast_lputfield:
 221   case Bytecodes::_fast_sputfield:
 222     {
 223       // We skip bytecode quickening for putfield instructions when
 224       // the put_code written to the constant pool cache is zero.
 225       // This is required so that every execution of this instruction
 226       // calls out to InterpreterRuntime::resolve_get_put to do
 227       // additional, required work.
 228       assert(byte_no == f1_byte || byte_no == f2_byte, &quot;byte_no out of range&quot;);
 229       assert(load_bc_into_bc_reg, &quot;we use bc_reg as temp&quot;);
 230       __ get_cache_and_index_and_bytecode_at_bcp(bc_reg, temp_reg, temp_reg, byte_no, 1, sizeof(u2));
 231       __ mov(bc_reg, bc);
 232       __ cbz(temp_reg, L_patch_done);  // test if bytecode is zero
 233     }
 234     break;
 235   default:
 236     assert(byte_no == -1, &quot;sanity&quot;);
 237     // the pair bytecodes have already done the load.
 238     if (load_bc_into_bc_reg) {
 239       __ mov(bc_reg, bc);
 240     }
 241   }
 242 
 243   if (__ can_post_breakpoint()) {
 244     Label L_fast_patch;
 245     // if a breakpoint is present we can&#39;t rewrite the stream directly
 246     __ ldrb(temp_reg, at_bcp(0));
 247     __ cmp(temp_reg, Bytecodes::_breakpoint);
 248     __ b(L_fast_patch, ne);
 249     if (bc_reg != R3) {
 250       __ mov(R3, bc_reg);
 251     }
 252     __ mov(R1, Rmethod);
 253     __ mov(R2, Rbcp);
 254     // Let breakpoint table handling rewrite to quicker bytecode
 255     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::set_original_bytecode_at), R1, R2, R3);
 256     __ b(L_patch_done);
 257     __ bind(L_fast_patch);
 258   }
 259 
 260 #ifdef ASSERT
 261   Label L_okay;
 262   __ ldrb(temp_reg, at_bcp(0));
 263   __ cmp(temp_reg, (int)Bytecodes::java_code(bc));
 264   __ b(L_okay, eq);
 265   __ cmp(temp_reg, bc_reg);
 266   __ b(L_okay, eq);
 267   __ stop(&quot;patching the wrong bytecode&quot;);
 268   __ bind(L_okay);
 269 #endif
 270 
 271   // patch bytecode
 272   __ strb(bc_reg, at_bcp(0));
 273   __ bind(L_patch_done);
 274 }
 275 
 276 //----------------------------------------------------------------------------------------------------
 277 // Individual instructions
 278 
 279 void TemplateTable::nop() {
 280   transition(vtos, vtos);
 281   // nothing to do
 282 }
 283 
 284 void TemplateTable::shouldnotreachhere() {
 285   transition(vtos, vtos);
 286   __ stop(&quot;shouldnotreachhere bytecode&quot;);
 287 }
 288 
 289 
 290 
 291 void TemplateTable::aconst_null() {
 292   transition(vtos, atos);
 293   __ mov(R0_tos, 0);
 294 }
 295 
 296 
 297 void TemplateTable::iconst(int value) {
 298   transition(vtos, itos);
 299   __ mov_slow(R0_tos, value);
 300 }
 301 
 302 
 303 void TemplateTable::lconst(int value) {
 304   transition(vtos, ltos);
 305   assert((value == 0) || (value == 1), &quot;unexpected long constant&quot;);
 306   __ mov(R0_tos, value);
 307   __ mov(R1_tos_hi, 0);
 308 }
 309 
 310 
 311 void TemplateTable::fconst(int value) {
 312   transition(vtos, ftos);
 313   const int zero = 0;         // 0.0f
 314   const int one = 0x3f800000; // 1.0f
 315   const int two = 0x40000000; // 2.0f
 316 
 317   switch(value) {
 318   case 0:   __ mov(R0_tos, zero);   break;
 319   case 1:   __ mov(R0_tos, one);    break;
 320   case 2:   __ mov(R0_tos, two);    break;
 321   default:  ShouldNotReachHere();   break;
 322   }
 323 
 324 #ifndef __SOFTFP__
 325   __ fmsr(S0_tos, R0_tos);
 326 #endif // !__SOFTFP__
 327 }
 328 
 329 
 330 void TemplateTable::dconst(int value) {
 331   transition(vtos, dtos);
 332   const int one_lo = 0;            // low part of 1.0
 333   const int one_hi = 0x3ff00000;   // high part of 1.0
 334 
 335   if (value == 0) {
 336 #ifdef __SOFTFP__
 337     __ mov(R0_tos_lo, 0);
 338     __ mov(R1_tos_hi, 0);
 339 #else
 340     __ mov(R0_tmp, 0);
 341     __ fmdrr(D0_tos, R0_tmp, R0_tmp);
 342 #endif // __SOFTFP__
 343   } else if (value == 1) {
 344     __ mov(R0_tos_lo, one_lo);
 345     __ mov_slow(R1_tos_hi, one_hi);
 346 #ifndef __SOFTFP__
 347     __ fmdrr(D0_tos, R0_tos_lo, R1_tos_hi);
 348 #endif // !__SOFTFP__
 349   } else {
 350     ShouldNotReachHere();
 351   }
 352 }
 353 
 354 
 355 void TemplateTable::bipush() {
 356   transition(vtos, itos);
 357   __ ldrsb(R0_tos, at_bcp(1));
 358 }
 359 
 360 
 361 void TemplateTable::sipush() {
 362   transition(vtos, itos);
 363   __ ldrsb(R0_tmp, at_bcp(1));
 364   __ ldrb(R1_tmp, at_bcp(2));
 365   __ orr(R0_tos, R1_tmp, AsmOperand(R0_tmp, lsl, BitsPerByte));
 366 }
 367 
 368 
 369 void TemplateTable::ldc(bool wide) {
 370   transition(vtos, vtos);
 371   Label fastCase, Condy, Done;
 372 
 373   const Register Rindex = R1_tmp;
 374   const Register Rcpool = R2_tmp;
 375   const Register Rtags  = R3_tmp;
 376   const Register RtagType = R3_tmp;
 377 
 378   if (wide) {
 379     __ get_unsigned_2_byte_index_at_bcp(Rindex, 1);
 380   } else {
 381     __ ldrb(Rindex, at_bcp(1));
 382   }
 383   __ get_cpool_and_tags(Rcpool, Rtags);
 384 
 385   const int base_offset = ConstantPool::header_size() * wordSize;
 386   const int tags_offset = Array&lt;u1&gt;::base_offset_in_bytes();
 387 
 388   // get const type
 389   __ add(Rtemp, Rtags, tags_offset);
 390   __ ldrb(RtagType, Address(Rtemp, Rindex));
 391   volatile_barrier(MacroAssembler::LoadLoad, Rtemp);
 392 
 393   // unresolved class - get the resolved class
 394   __ cmp(RtagType, JVM_CONSTANT_UnresolvedClass);
 395 
 396   // unresolved class in error (resolution failed) - call into runtime
 397   // so that the same error from first resolution attempt is thrown.
 398   __ cond_cmp(RtagType, JVM_CONSTANT_UnresolvedClassInError, ne);
 399 
 400   // resolved class - need to call vm to get java mirror of the class
 401   __ cond_cmp(RtagType, JVM_CONSTANT_Class, ne);
 402 
 403   __ b(fastCase, ne);
 404 
 405   // slow case - call runtime
 406   __ mov(R1, wide);
 407   call_VM(R0_tos, CAST_FROM_FN_PTR(address, InterpreterRuntime::ldc), R1);
 408   __ push(atos);
 409   __ b(Done);
 410 
 411   // int, float, String
 412   __ bind(fastCase);
 413 
 414   __ cmp(RtagType, JVM_CONSTANT_Integer);
 415   __ cond_cmp(RtagType, JVM_CONSTANT_Float, ne);
 416   __ b(Condy, ne);
 417 
 418   // itos, ftos
 419   __ add(Rtemp, Rcpool, AsmOperand(Rindex, lsl, LogBytesPerWord));
 420   __ ldr_u32(R0_tos, Address(Rtemp, base_offset));
 421 
 422   // floats and ints are placed on stack in the same way, so
 423   // we can use push(itos) to transfer float value without VFP
 424   __ push(itos);
 425   __ b(Done);
 426 
 427   __ bind(Condy);
 428   condy_helper(Done);
 429 
 430   __ bind(Done);
 431 }
 432 
 433 // Fast path for caching oop constants.
 434 void TemplateTable::fast_aldc(bool wide) {
 435   transition(vtos, atos);
 436   int index_size = wide ? sizeof(u2) : sizeof(u1);
 437   Label resolved;
 438 
 439   // We are resolved if the resolved reference cache entry contains a
 440   // non-null object (CallSite, etc.)
 441   assert_different_registers(R0_tos, R2_tmp);
 442   __ get_index_at_bcp(R2_tmp, 1, R0_tos, index_size);
 443   __ load_resolved_reference_at_index(R0_tos, R2_tmp);
 444   __ cbnz(R0_tos, resolved);
 445 
 446   address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_ldc);
 447 
 448   // first time invocation - must resolve first
 449   __ mov(R1, (int)bytecode());
 450   __ call_VM(R0_tos, entry, R1);
 451   __ bind(resolved);
 452 
 453   { // Check for the null sentinel.
 454     // If we just called the VM, that already did the mapping for us,
 455     // but it&#39;s harmless to retry.
 456     Label notNull;
 457     Register result = R0;
 458     Register tmp = R1;
 459     Register rarg = R2;
 460 
 461     // Stash null_sentinel address to get its value later
 462     __ mov_slow(rarg, (uintptr_t)Universe::the_null_sentinel_addr());
 463     __ ldr(tmp, Address(rarg));
 464     __ cmp(result, tmp);
 465     __ b(notNull, ne);
 466     __ mov(result, 0);  // NULL object reference
 467     __ bind(notNull);
 468   }
 469 
 470   if (VerifyOops) {
 471     __ verify_oop(R0_tos);
 472   }
 473 }
 474 
 475 void TemplateTable::ldc2_w() {
 476   transition(vtos, vtos);
 477   const Register Rtags  = R2_tmp;
 478   const Register Rindex = R3_tmp;
 479   const Register Rcpool = R4_tmp;
 480   const Register Rbase  = R5_tmp;
 481 
 482   __ get_unsigned_2_byte_index_at_bcp(Rindex, 1);
 483 
 484   __ get_cpool_and_tags(Rcpool, Rtags);
 485   const int base_offset = ConstantPool::header_size() * wordSize;
 486   const int tags_offset = Array&lt;u1&gt;::base_offset_in_bytes();
 487 
 488   __ add(Rbase, Rcpool, AsmOperand(Rindex, lsl, LogBytesPerWord));
 489 
<a name="3" id="anc3"></a><span class="line-removed"> 490   Label Condy, exit;</span>
<span class="line-removed"> 491 #ifdef __ABI_HARD__</span>
<span class="line-removed"> 492   Label Long;</span>
 493   // get type from tags
 494   __ add(Rtemp, Rtags, tags_offset);
 495   __ ldrb(Rtemp, Address(Rtemp, Rindex));
<a name="4" id="anc4"></a>



 496   __ cmp(Rtemp, JVM_CONSTANT_Double);
<a name="5" id="anc5"></a><span class="line-modified"> 497   __ b(Long, ne);</span>
 498   __ ldr_double(D0_tos, Address(Rbase, base_offset));
 499 
 500   __ push(dtos);
 501   __ b(exit);
<a name="6" id="anc6"></a><span class="line-modified"> 502   __ bind(Long);</span>
 503 #endif
 504 
 505   __ cmp(Rtemp, JVM_CONSTANT_Long);
 506   __ b(Condy, ne);
 507   __ ldr(R0_tos_lo, Address(Rbase, base_offset + 0 * wordSize));
 508   __ ldr(R1_tos_hi, Address(Rbase, base_offset + 1 * wordSize));
 509   __ push(ltos);
 510   __ b(exit);
 511 
 512   __ bind(Condy);
 513   condy_helper(exit);
 514 
 515   __ bind(exit);
 516 }
 517 
 518 
 519 void TemplateTable::condy_helper(Label&amp; Done)
 520 {
 521   Register obj   = R0_tmp;
 522   Register rtmp  = R1_tmp;
 523   Register flags = R2_tmp;
 524   Register off   = R3_tmp;
 525 
 526   __ mov(rtmp, (int) bytecode());
 527   __ call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_ldc), rtmp);
 528   __ get_vm_result_2(flags, rtmp);
 529 
 530   // VMr = obj = base address to find primitive value to push
 531   // VMr2 = flags = (tos, off) using format of CPCE::_flags
 532   __ mov(off, flags);
 533 
 534   __ logical_shift_left( off, off, 32 - ConstantPoolCacheEntry::field_index_bits);
 535   __ logical_shift_right(off, off, 32 - ConstantPoolCacheEntry::field_index_bits);
 536 
 537   const Address field(obj, off);
 538 
 539   __ logical_shift_right(flags, flags, ConstantPoolCacheEntry::tos_state_shift);
 540   // Make sure we don&#39;t need to mask flags after the above shift
 541   ConstantPoolCacheEntry::verify_tos_state_shift();
 542 
 543   switch (bytecode()) {
 544     case Bytecodes::_ldc:
 545     case Bytecodes::_ldc_w:
 546       {
 547         // tos in (itos, ftos, stos, btos, ctos, ztos)
 548         Label notIntFloat, notShort, notByte, notChar, notBool;
 549         __ cmp(flags, itos);
 550         __ cond_cmp(flags, ftos, ne);
 551         __ b(notIntFloat, ne);
 552         __ ldr(R0_tos, field);
 553         __ push(itos);
 554         __ b(Done);
 555 
 556         __ bind(notIntFloat);
 557         __ cmp(flags, stos);
 558         __ b(notShort, ne);
 559         __ ldrsh(R0_tos, field);
 560         __ push(stos);
 561         __ b(Done);
 562 
 563         __ bind(notShort);
 564         __ cmp(flags, btos);
 565         __ b(notByte, ne);
 566         __ ldrsb(R0_tos, field);
 567         __ push(btos);
 568         __ b(Done);
 569 
 570         __ bind(notByte);
 571         __ cmp(flags, ctos);
 572         __ b(notChar, ne);
 573         __ ldrh(R0_tos, field);
 574         __ push(ctos);
 575         __ b(Done);
 576 
 577         __ bind(notChar);
 578         __ cmp(flags, ztos);
 579         __ b(notBool, ne);
 580         __ ldrsb(R0_tos, field);
 581         __ push(ztos);
 582         __ b(Done);
 583 
 584         __ bind(notBool);
 585         break;
 586       }
 587 
 588     case Bytecodes::_ldc2_w:
 589       {
 590         Label notLongDouble;
 591         __ cmp(flags, ltos);
 592         __ cond_cmp(flags, dtos, ne);
 593         __ b(notLongDouble, ne);
 594 
 595         __ add(rtmp, obj, wordSize);
 596         __ ldr(R0_tos_lo, Address(obj, off));
 597         __ ldr(R1_tos_hi, Address(rtmp, off));
 598         __ push(ltos);
 599         __ b(Done);
 600 
 601         __ bind(notLongDouble);
 602 
 603         break;
 604       }
 605 
 606     default:
 607       ShouldNotReachHere();
 608     }
 609 
 610     __ stop(&quot;bad ldc/condy&quot;);
 611 }
 612 
 613 
 614 void TemplateTable::locals_index(Register reg, int offset) {
 615   __ ldrb(reg, at_bcp(offset));
 616 }
 617 
 618 void TemplateTable::iload() {
 619   iload_internal();
 620 }
 621 
 622 void TemplateTable::nofast_iload() {
 623   iload_internal(may_not_rewrite);
 624 }
 625 
 626 void TemplateTable::iload_internal(RewriteControl rc) {
 627   transition(vtos, itos);
 628 
 629   if ((rc == may_rewrite) &amp;&amp; __ rewrite_frequent_pairs()) {
 630     Label rewrite, done;
 631     const Register next_bytecode = R1_tmp;
 632     const Register target_bytecode = R2_tmp;
 633 
 634     // get next byte
 635     __ ldrb(next_bytecode, at_bcp(Bytecodes::length_for(Bytecodes::_iload)));
 636     // if _iload, wait to rewrite to iload2.  We only want to rewrite the
 637     // last two iloads in a pair.  Comparing against fast_iload means that
 638     // the next bytecode is neither an iload or a caload, and therefore
 639     // an iload pair.
 640     __ cmp(next_bytecode, Bytecodes::_iload);
 641     __ b(done, eq);
 642 
 643     __ cmp(next_bytecode, Bytecodes::_fast_iload);
 644     __ mov(target_bytecode, Bytecodes::_fast_iload2);
 645     __ b(rewrite, eq);
 646 
 647     // if _caload, rewrite to fast_icaload
 648     __ cmp(next_bytecode, Bytecodes::_caload);
 649     __ mov(target_bytecode, Bytecodes::_fast_icaload);
 650     __ b(rewrite, eq);
 651 
 652     // rewrite so iload doesn&#39;t check again.
 653     __ mov(target_bytecode, Bytecodes::_fast_iload);
 654 
 655     // rewrite
 656     // R2: fast bytecode
 657     __ bind(rewrite);
 658     patch_bytecode(Bytecodes::_iload, target_bytecode, Rtemp, false);
 659     __ bind(done);
 660   }
 661 
 662   // Get the local value into tos
 663   const Register Rlocal_index = R1_tmp;
 664   locals_index(Rlocal_index);
 665   Address local = load_iaddress(Rlocal_index, Rtemp);
 666   __ ldr_s32(R0_tos, local);
 667 }
 668 
 669 
 670 void TemplateTable::fast_iload2() {
 671   transition(vtos, itos);
 672   const Register Rlocal_index = R1_tmp;
 673 
 674   locals_index(Rlocal_index);
 675   Address local = load_iaddress(Rlocal_index, Rtemp);
 676   __ ldr_s32(R0_tos, local);
 677   __ push(itos);
 678 
 679   locals_index(Rlocal_index, 3);
 680   local = load_iaddress(Rlocal_index, Rtemp);
 681   __ ldr_s32(R0_tos, local);
 682 }
 683 
 684 void TemplateTable::fast_iload() {
 685   transition(vtos, itos);
 686   const Register Rlocal_index = R1_tmp;
 687 
 688   locals_index(Rlocal_index);
 689   Address local = load_iaddress(Rlocal_index, Rtemp);
 690   __ ldr_s32(R0_tos, local);
 691 }
 692 
 693 
 694 void TemplateTable::lload() {
 695   transition(vtos, ltos);
 696   const Register Rlocal_index = R2_tmp;
 697 
 698   locals_index(Rlocal_index);
 699   load_category2_local(Rlocal_index, R3_tmp);
 700 }
 701 
 702 
 703 void TemplateTable::fload() {
 704   transition(vtos, ftos);
 705   const Register Rlocal_index = R2_tmp;
 706 
 707   // Get the local value into tos
 708   locals_index(Rlocal_index);
 709   Address local = load_faddress(Rlocal_index, Rtemp);
 710 #ifdef __SOFTFP__
 711   __ ldr(R0_tos, local);
 712 #else
 713   __ ldr_float(S0_tos, local);
 714 #endif // __SOFTFP__
 715 }
 716 
 717 
 718 void TemplateTable::dload() {
 719   transition(vtos, dtos);
 720   const Register Rlocal_index = R2_tmp;
 721 
 722   locals_index(Rlocal_index);
 723 
 724 #ifdef __SOFTFP__
 725   load_category2_local(Rlocal_index, R3_tmp);
 726 #else
 727   __ ldr_double(D0_tos, load_daddress(Rlocal_index, Rtemp));
 728 #endif // __SOFTFP__
 729 }
 730 
 731 
 732 void TemplateTable::aload() {
 733   transition(vtos, atos);
 734   const Register Rlocal_index = R1_tmp;
 735 
 736   locals_index(Rlocal_index);
 737   Address local = load_aaddress(Rlocal_index, Rtemp);
 738   __ ldr(R0_tos, local);
 739 }
 740 
 741 
 742 void TemplateTable::locals_index_wide(Register reg) {
 743   assert_different_registers(reg, Rtemp);
 744   __ ldrb(Rtemp, at_bcp(2));
 745   __ ldrb(reg, at_bcp(3));
 746   __ orr(reg, reg, AsmOperand(Rtemp, lsl, 8));
 747 }
 748 
 749 
 750 void TemplateTable::wide_iload() {
 751   transition(vtos, itos);
 752   const Register Rlocal_index = R2_tmp;
 753 
 754   locals_index_wide(Rlocal_index);
 755   Address local = load_iaddress(Rlocal_index, Rtemp);
 756   __ ldr_s32(R0_tos, local);
 757 }
 758 
 759 
 760 void TemplateTable::wide_lload() {
 761   transition(vtos, ltos);
 762   const Register Rlocal_index = R2_tmp;
 763   const Register Rlocal_base = R3_tmp;
 764 
 765   locals_index_wide(Rlocal_index);
 766   load_category2_local(Rlocal_index, R3_tmp);
 767 }
 768 
 769 
 770 void TemplateTable::wide_fload() {
 771   transition(vtos, ftos);
 772   const Register Rlocal_index = R2_tmp;
 773 
 774   locals_index_wide(Rlocal_index);
 775   Address local = load_faddress(Rlocal_index, Rtemp);
 776 #ifdef __SOFTFP__
 777   __ ldr(R0_tos, local);
 778 #else
 779   __ ldr_float(S0_tos, local);
 780 #endif // __SOFTFP__
 781 }
 782 
 783 
 784 void TemplateTable::wide_dload() {
 785   transition(vtos, dtos);
 786   const Register Rlocal_index = R2_tmp;
 787 
 788   locals_index_wide(Rlocal_index);
 789 #ifdef __SOFTFP__
 790   load_category2_local(Rlocal_index, R3_tmp);
 791 #else
 792   __ ldr_double(D0_tos, load_daddress(Rlocal_index, Rtemp));
 793 #endif // __SOFTFP__
 794 }
 795 
 796 
 797 void TemplateTable::wide_aload() {
 798   transition(vtos, atos);
 799   const Register Rlocal_index = R2_tmp;
 800 
 801   locals_index_wide(Rlocal_index);
 802   Address local = load_aaddress(Rlocal_index, Rtemp);
 803   __ ldr(R0_tos, local);
 804 }
 805 
 806 void TemplateTable::index_check(Register array, Register index) {
 807   // Pop ptr into array
 808   __ pop_ptr(array);
 809   index_check_without_pop(array, index);
 810 }
 811 
 812 void TemplateTable::index_check_without_pop(Register array, Register index) {
 813   assert_different_registers(array, index, Rtemp);
 814   // check array
 815   __ null_check(array, Rtemp, arrayOopDesc::length_offset_in_bytes());
 816   // check index
 817   __ ldr_s32(Rtemp, Address(array, arrayOopDesc::length_offset_in_bytes()));
 818   __ cmp_32(index, Rtemp);
 819   if (index != R4_ArrayIndexOutOfBounds_index) {
 820     // convention with generate_ArrayIndexOutOfBounds_handler()
 821     __ mov(R4_ArrayIndexOutOfBounds_index, index, hs);
 822   }
 823   __ mov(R1, array, hs);
 824   __ b(Interpreter::_throw_ArrayIndexOutOfBoundsException_entry, hs);
 825 }
 826 
 827 
 828 void TemplateTable::iaload() {
 829   transition(itos, itos);
 830   const Register Rarray = R1_tmp;
 831   const Register Rindex = R0_tos;
 832 
 833   index_check(Rarray, Rindex);
 834   Address addr = get_array_elem_addr_same_base(T_INT, Rarray, Rindex, Rtemp);
 835   __ access_load_at(T_INT, IN_HEAP | IS_ARRAY, addr, R0_tos, noreg, noreg, noreg);
 836 }
 837 
 838 
 839 void TemplateTable::laload() {
 840   transition(itos, ltos);
 841   const Register Rarray = R1_tmp;
 842   const Register Rindex = R0_tos;
 843 
 844   index_check(Rarray, Rindex);
 845 
 846   Address addr = get_array_elem_addr_same_base(T_LONG, Rarray, Rindex, Rtemp);
 847   __ access_load_at(T_LONG, IN_HEAP | IS_ARRAY, addr, noreg /* ltos */, noreg, noreg, noreg);
 848 }
 849 
 850 
 851 void TemplateTable::faload() {
 852   transition(itos, ftos);
 853   const Register Rarray = R1_tmp;
 854   const Register Rindex = R0_tos;
 855 
 856   index_check(Rarray, Rindex);
 857 
 858   Address addr = get_array_elem_addr_same_base(T_FLOAT, Rarray, Rindex, Rtemp);
 859   __ access_load_at(T_FLOAT, IN_HEAP | IS_ARRAY, addr, noreg /* ftos */, noreg, noreg, noreg);
 860 }
 861 
 862 
 863 void TemplateTable::daload() {
 864   transition(itos, dtos);
 865   const Register Rarray = R1_tmp;
 866   const Register Rindex = R0_tos;
 867 
 868   index_check(Rarray, Rindex);
 869 
 870   Address addr = get_array_elem_addr_same_base(T_DOUBLE, Rarray, Rindex, Rtemp);
 871   __ access_load_at(T_DOUBLE, IN_HEAP | IS_ARRAY, addr, noreg /* dtos */, noreg, noreg, noreg);
 872 }
 873 
 874 
 875 void TemplateTable::aaload() {
 876   transition(itos, atos);
 877   const Register Rarray = R1_tmp;
 878   const Register Rindex = R0_tos;
 879 
 880   index_check(Rarray, Rindex);
 881   do_oop_load(_masm, R0_tos, get_array_elem_addr_same_base(T_OBJECT, Rarray, Rindex, Rtemp), IS_ARRAY);
 882 }
 883 
 884 
 885 void TemplateTable::baload() {
 886   transition(itos, itos);
 887   const Register Rarray = R1_tmp;
 888   const Register Rindex = R0_tos;
 889 
 890   index_check(Rarray, Rindex);
 891   Address addr = get_array_elem_addr_same_base(T_BYTE, Rarray, Rindex, Rtemp);
 892   __ access_load_at(T_BYTE, IN_HEAP | IS_ARRAY, addr, R0_tos, noreg, noreg, noreg);
 893 }
 894 
 895 
 896 void TemplateTable::caload() {
 897   transition(itos, itos);
 898   const Register Rarray = R1_tmp;
 899   const Register Rindex = R0_tos;
 900 
 901   index_check(Rarray, Rindex);
 902   Address addr = get_array_elem_addr_same_base(T_CHAR, Rarray, Rindex, Rtemp);
 903   __ access_load_at(T_CHAR, IN_HEAP | IS_ARRAY, addr, R0_tos, noreg, noreg, noreg);
 904 }
 905 
 906 
 907 // iload followed by caload frequent pair
 908 void TemplateTable::fast_icaload() {
 909   transition(vtos, itos);
 910   const Register Rlocal_index = R1_tmp;
 911   const Register Rarray = R1_tmp;
 912   const Register Rindex = R4_tmp; // index_check prefers index on R4
 913   assert_different_registers(Rlocal_index, Rindex);
 914   assert_different_registers(Rarray, Rindex);
 915 
 916   // load index out of locals
 917   locals_index(Rlocal_index);
 918   Address local = load_iaddress(Rlocal_index, Rtemp);
 919   __ ldr_s32(Rindex, local);
 920 
 921   // get array element
 922   index_check(Rarray, Rindex);
 923   Address addr = get_array_elem_addr_same_base(T_CHAR, Rarray, Rindex, Rtemp);
 924   __ access_load_at(T_CHAR, IN_HEAP | IS_ARRAY, addr, R0_tos, noreg, noreg, noreg);
 925 }
 926 
 927 
 928 void TemplateTable::saload() {
 929   transition(itos, itos);
 930   const Register Rarray = R1_tmp;
 931   const Register Rindex = R0_tos;
 932 
 933   index_check(Rarray, Rindex);
 934   Address addr = get_array_elem_addr_same_base(T_SHORT, Rarray, Rindex, Rtemp);
 935   __ access_load_at(T_SHORT, IN_HEAP | IS_ARRAY, addr, R0_tos, noreg, noreg, noreg);
 936 }
 937 
 938 
 939 void TemplateTable::iload(int n) {
 940   transition(vtos, itos);
 941   __ ldr_s32(R0_tos, iaddress(n));
 942 }
 943 
 944 
 945 void TemplateTable::lload(int n) {
 946   transition(vtos, ltos);
 947   __ ldr(R0_tos_lo, laddress(n));
 948   __ ldr(R1_tos_hi, haddress(n));
 949 }
 950 
 951 
 952 void TemplateTable::fload(int n) {
 953   transition(vtos, ftos);
 954 #ifdef __SOFTFP__
 955   __ ldr(R0_tos, faddress(n));
 956 #else
 957   __ ldr_float(S0_tos, faddress(n));
 958 #endif // __SOFTFP__
 959 }
 960 
 961 
 962 void TemplateTable::dload(int n) {
 963   transition(vtos, dtos);
 964 #ifdef __SOFTFP__
 965   __ ldr(R0_tos_lo, laddress(n));
 966   __ ldr(R1_tos_hi, haddress(n));
 967 #else
 968   __ ldr_double(D0_tos, daddress(n));
 969 #endif // __SOFTFP__
 970 }
 971 
 972 
 973 void TemplateTable::aload(int n) {
 974   transition(vtos, atos);
 975   __ ldr(R0_tos, aaddress(n));
 976 }
 977 
 978 void TemplateTable::aload_0() {
 979   aload_0_internal();
 980 }
 981 
 982 void TemplateTable::nofast_aload_0() {
 983   aload_0_internal(may_not_rewrite);
 984 }
 985 
 986 void TemplateTable::aload_0_internal(RewriteControl rc) {
 987   transition(vtos, atos);
 988   // According to bytecode histograms, the pairs:
 989   //
 990   // _aload_0, _fast_igetfield
 991   // _aload_0, _fast_agetfield
 992   // _aload_0, _fast_fgetfield
 993   //
 994   // occur frequently. If RewriteFrequentPairs is set, the (slow) _aload_0
 995   // bytecode checks if the next bytecode is either _fast_igetfield,
 996   // _fast_agetfield or _fast_fgetfield and then rewrites the
 997   // current bytecode into a pair bytecode; otherwise it rewrites the current
 998   // bytecode into _fast_aload_0 that doesn&#39;t do the pair check anymore.
 999   //
1000   // Note: If the next bytecode is _getfield, the rewrite must be delayed,
1001   //       otherwise we may miss an opportunity for a pair.
1002   //
1003   // Also rewrite frequent pairs
1004   //   aload_0, aload_1
1005   //   aload_0, iload_1
1006   // These bytecodes with a small amount of code are most profitable to rewrite
1007   if ((rc == may_rewrite) &amp;&amp; __ rewrite_frequent_pairs()) {
1008     Label rewrite, done;
1009     const Register next_bytecode = R1_tmp;
1010     const Register target_bytecode = R2_tmp;
1011 
1012     // get next byte
1013     __ ldrb(next_bytecode, at_bcp(Bytecodes::length_for(Bytecodes::_aload_0)));
1014 
1015     // if _getfield then wait with rewrite
1016     __ cmp(next_bytecode, Bytecodes::_getfield);
1017     __ b(done, eq);
1018 
1019     // if _igetfield then rewrite to _fast_iaccess_0
1020     assert(Bytecodes::java_code(Bytecodes::_fast_iaccess_0) == Bytecodes::_aload_0, &quot;fix bytecode definition&quot;);
1021     __ cmp(next_bytecode, Bytecodes::_fast_igetfield);
1022     __ mov(target_bytecode, Bytecodes::_fast_iaccess_0);
1023     __ b(rewrite, eq);
1024 
1025     // if _agetfield then rewrite to _fast_aaccess_0
1026     assert(Bytecodes::java_code(Bytecodes::_fast_aaccess_0) == Bytecodes::_aload_0, &quot;fix bytecode definition&quot;);
1027     __ cmp(next_bytecode, Bytecodes::_fast_agetfield);
1028     __ mov(target_bytecode, Bytecodes::_fast_aaccess_0);
1029     __ b(rewrite, eq);
1030 
1031     // if _fgetfield then rewrite to _fast_faccess_0, else rewrite to _fast_aload0
1032     assert(Bytecodes::java_code(Bytecodes::_fast_faccess_0) == Bytecodes::_aload_0, &quot;fix bytecode definition&quot;);
1033     assert(Bytecodes::java_code(Bytecodes::_fast_aload_0) == Bytecodes::_aload_0, &quot;fix bytecode definition&quot;);
1034 
1035     __ cmp(next_bytecode, Bytecodes::_fast_fgetfield);
1036     __ mov(target_bytecode, Bytecodes::_fast_faccess_0, eq);
1037     __ mov(target_bytecode, Bytecodes::_fast_aload_0, ne);
1038 
1039     // rewrite
1040     __ bind(rewrite);
1041     patch_bytecode(Bytecodes::_aload_0, target_bytecode, Rtemp, false);
1042 
1043     __ bind(done);
1044   }
1045 
1046   aload(0);
1047 }
1048 
1049 void TemplateTable::istore() {
1050   transition(itos, vtos);
1051   const Register Rlocal_index = R2_tmp;
1052 
1053   locals_index(Rlocal_index);
1054   Address local = load_iaddress(Rlocal_index, Rtemp);
1055   __ str_32(R0_tos, local);
1056 }
1057 
1058 
1059 void TemplateTable::lstore() {
1060   transition(ltos, vtos);
1061   const Register Rlocal_index = R2_tmp;
1062 
1063   locals_index(Rlocal_index);
1064   store_category2_local(Rlocal_index, R3_tmp);
1065 }
1066 
1067 
1068 void TemplateTable::fstore() {
1069   transition(ftos, vtos);
1070   const Register Rlocal_index = R2_tmp;
1071 
1072   locals_index(Rlocal_index);
1073   Address local = load_faddress(Rlocal_index, Rtemp);
1074 #ifdef __SOFTFP__
1075   __ str(R0_tos, local);
1076 #else
1077   __ str_float(S0_tos, local);
1078 #endif // __SOFTFP__
1079 }
1080 
1081 
1082 void TemplateTable::dstore() {
1083   transition(dtos, vtos);
1084   const Register Rlocal_index = R2_tmp;
1085 
1086   locals_index(Rlocal_index);
1087 
1088 #ifdef __SOFTFP__
1089   store_category2_local(Rlocal_index, R3_tmp);
1090 #else
1091   __ str_double(D0_tos, load_daddress(Rlocal_index, Rtemp));
1092 #endif // __SOFTFP__
1093 }
1094 
1095 
1096 void TemplateTable::astore() {
1097   transition(vtos, vtos);
1098   const Register Rlocal_index = R1_tmp;
1099 
1100   __ pop_ptr(R0_tos);
1101   locals_index(Rlocal_index);
1102   Address local = load_aaddress(Rlocal_index, Rtemp);
1103   __ str(R0_tos, local);
1104 }
1105 
1106 
1107 void TemplateTable::wide_istore() {
1108   transition(vtos, vtos);
1109   const Register Rlocal_index = R2_tmp;
1110 
1111   __ pop_i(R0_tos);
1112   locals_index_wide(Rlocal_index);
1113   Address local = load_iaddress(Rlocal_index, Rtemp);
1114   __ str_32(R0_tos, local);
1115 }
1116 
1117 
1118 void TemplateTable::wide_lstore() {
1119   transition(vtos, vtos);
1120   const Register Rlocal_index = R2_tmp;
1121   const Register Rlocal_base = R3_tmp;
1122 
1123   __ pop_l(R0_tos_lo, R1_tos_hi);
1124 
1125   locals_index_wide(Rlocal_index);
1126   store_category2_local(Rlocal_index, R3_tmp);
1127 }
1128 
1129 
1130 void TemplateTable::wide_fstore() {
1131   wide_istore();
1132 }
1133 
1134 
1135 void TemplateTable::wide_dstore() {
1136   wide_lstore();
1137 }
1138 
1139 
1140 void TemplateTable::wide_astore() {
1141   transition(vtos, vtos);
1142   const Register Rlocal_index = R2_tmp;
1143 
1144   __ pop_ptr(R0_tos);
1145   locals_index_wide(Rlocal_index);
1146   Address local = load_aaddress(Rlocal_index, Rtemp);
1147   __ str(R0_tos, local);
1148 }
1149 
1150 
1151 void TemplateTable::iastore() {
1152   transition(itos, vtos);
1153   const Register Rindex = R4_tmp; // index_check prefers index in R4
1154   const Register Rarray = R3_tmp;
1155   // R0_tos: value
1156 
1157   __ pop_i(Rindex);
1158   index_check(Rarray, Rindex);
1159   Address addr = get_array_elem_addr_same_base(T_INT, Rarray, Rindex, Rtemp);
1160   __ access_store_at(T_INT, IN_HEAP | IS_ARRAY, addr, R0_tos, noreg, noreg, noreg, false);
1161 }
1162 
1163 
1164 void TemplateTable::lastore() {
1165   transition(ltos, vtos);
1166   const Register Rindex = R4_tmp; // index_check prefers index in R4
1167   const Register Rarray = R3_tmp;
1168   // R0_tos_lo:R1_tos_hi: value
1169 
1170   __ pop_i(Rindex);
1171   index_check(Rarray, Rindex);
1172 
1173   Address addr = get_array_elem_addr_same_base(T_LONG, Rarray, Rindex, Rtemp);
1174   __ access_store_at(T_LONG, IN_HEAP | IS_ARRAY, addr, noreg /* ltos */, noreg, noreg, noreg, false);
1175 }
1176 
1177 
1178 void TemplateTable::fastore() {
1179   transition(ftos, vtos);
1180   const Register Rindex = R4_tmp; // index_check prefers index in R4
1181   const Register Rarray = R3_tmp;
1182   // S0_tos/R0_tos: value
1183 
1184   __ pop_i(Rindex);
1185   index_check(Rarray, Rindex);
1186   Address addr = get_array_elem_addr_same_base(T_FLOAT, Rarray, Rindex, Rtemp);
1187   __ access_store_at(T_FLOAT, IN_HEAP | IS_ARRAY, addr, noreg /* ftos */, noreg, noreg, noreg, false);
1188 }
1189 
1190 
1191 void TemplateTable::dastore() {
1192   transition(dtos, vtos);
1193   const Register Rindex = R4_tmp; // index_check prefers index in R4
1194   const Register Rarray = R3_tmp;
1195   // D0_tos / R0_tos_lo:R1_to_hi: value
1196 
1197   __ pop_i(Rindex);
1198   index_check(Rarray, Rindex);
1199 
1200   Address addr = get_array_elem_addr_same_base(T_DOUBLE, Rarray, Rindex, Rtemp);
1201   __ access_store_at(T_DOUBLE, IN_HEAP | IS_ARRAY, addr, noreg /* dtos */, noreg, noreg, noreg, false);
1202 }
1203 
1204 
1205 void TemplateTable::aastore() {
1206   transition(vtos, vtos);
1207   Label is_null, throw_array_store, done;
1208 
1209   const Register Raddr_1   = R1_tmp;
1210   const Register Rvalue_2  = R2_tmp;
1211   const Register Rarray_3  = R3_tmp;
1212   const Register Rindex_4  = R4_tmp;   // preferred by index_check_without_pop()
1213   const Register Rsub_5    = R5_tmp;
1214   const Register Rsuper_LR = LR_tmp;
1215 
1216   // stack: ..., array, index, value
1217   __ ldr(Rvalue_2, at_tos());     // Value
1218   __ ldr_s32(Rindex_4, at_tos_p1());  // Index
1219   __ ldr(Rarray_3, at_tos_p2());  // Array
1220 
1221   index_check_without_pop(Rarray_3, Rindex_4);
1222 
1223   // Compute the array base
1224   __ add(Raddr_1, Rarray_3, arrayOopDesc::base_offset_in_bytes(T_OBJECT));
1225 
1226   // do array store check - check for NULL value first
1227   __ cbz(Rvalue_2, is_null);
1228 
1229   // Load subklass
1230   __ load_klass(Rsub_5, Rvalue_2);
1231   // Load superklass
1232   __ load_klass(Rtemp, Rarray_3);
1233   __ ldr(Rsuper_LR, Address(Rtemp, ObjArrayKlass::element_klass_offset()));
1234 
1235   __ gen_subtype_check(Rsub_5, Rsuper_LR, throw_array_store, R0_tmp, R3_tmp);
1236   // Come here on success
1237 
1238   // Store value
1239   __ add(Raddr_1, Raddr_1, AsmOperand(Rindex_4, lsl, LogBytesPerHeapOop));
1240 
1241   // Now store using the appropriate barrier
1242   do_oop_store(_masm, Raddr_1, Rvalue_2, Rtemp, R0_tmp, R3_tmp, false, IS_ARRAY);
1243   __ b(done);
1244 
1245   __ bind(throw_array_store);
1246 
1247   // Come here on failure of subtype check
1248   __ profile_typecheck_failed(R0_tmp);
1249 
1250   // object is at TOS
1251   __ b(Interpreter::_throw_ArrayStoreException_entry);
1252 
1253   // Have a NULL in Rvalue_2, store NULL at array[index].
1254   __ bind(is_null);
1255   __ profile_null_seen(R0_tmp);
1256 
1257   // Store a NULL
1258   do_oop_store(_masm, Address::indexed_oop(Raddr_1, Rindex_4), Rvalue_2, Rtemp, R0_tmp, R3_tmp, true, IS_ARRAY);
1259 
1260   // Pop stack arguments
1261   __ bind(done);
1262   __ add(Rstack_top, Rstack_top, 3 * Interpreter::stackElementSize);
1263 }
1264 
1265 
1266 void TemplateTable::bastore() {
1267   transition(itos, vtos);
1268   const Register Rindex = R4_tmp; // index_check prefers index in R4
1269   const Register Rarray = R3_tmp;
1270   // R0_tos: value
1271 
1272   __ pop_i(Rindex);
1273   index_check(Rarray, Rindex);
1274 
1275   // Need to check whether array is boolean or byte
1276   // since both types share the bastore bytecode.
1277   __ load_klass(Rtemp, Rarray);
1278   __ ldr_u32(Rtemp, Address(Rtemp, Klass::layout_helper_offset()));
1279   Label L_skip;
1280   __ tst(Rtemp, Klass::layout_helper_boolean_diffbit());
1281   __ b(L_skip, eq);
1282   __ and_32(R0_tos, R0_tos, 1); // if it is a T_BOOLEAN array, mask the stored value to 0/1
1283   __ bind(L_skip);
1284   Address addr = get_array_elem_addr_same_base(T_BYTE, Rarray, Rindex, Rtemp);
1285   __ access_store_at(T_BYTE, IN_HEAP | IS_ARRAY, addr, R0_tos, noreg, noreg, noreg, false);
1286 }
1287 
1288 
1289 void TemplateTable::castore() {
1290   transition(itos, vtos);
1291   const Register Rindex = R4_tmp; // index_check prefers index in R4
1292   const Register Rarray = R3_tmp;
1293   // R0_tos: value
1294 
1295   __ pop_i(Rindex);
1296   index_check(Rarray, Rindex);
1297   Address addr = get_array_elem_addr_same_base(T_CHAR, Rarray, Rindex, Rtemp);
1298   __ access_store_at(T_CHAR, IN_HEAP | IS_ARRAY, addr, R0_tos, noreg, noreg, noreg, false);
1299 }
1300 
1301 
1302 void TemplateTable::sastore() {
1303   assert(arrayOopDesc::base_offset_in_bytes(T_CHAR) ==
1304            arrayOopDesc::base_offset_in_bytes(T_SHORT),
1305          &quot;base offsets for char and short should be equal&quot;);
1306   castore();
1307 }
1308 
1309 
1310 void TemplateTable::istore(int n) {
1311   transition(itos, vtos);
1312   __ str_32(R0_tos, iaddress(n));
1313 }
1314 
1315 
1316 void TemplateTable::lstore(int n) {
1317   transition(ltos, vtos);
1318   __ str(R0_tos_lo, laddress(n));
1319   __ str(R1_tos_hi, haddress(n));
1320 }
1321 
1322 
1323 void TemplateTable::fstore(int n) {
1324   transition(ftos, vtos);
1325 #ifdef __SOFTFP__
1326   __ str(R0_tos, faddress(n));
1327 #else
1328   __ str_float(S0_tos, faddress(n));
1329 #endif // __SOFTFP__
1330 }
1331 
1332 
1333 void TemplateTable::dstore(int n) {
1334   transition(dtos, vtos);
1335 #ifdef __SOFTFP__
1336   __ str(R0_tos_lo, laddress(n));
1337   __ str(R1_tos_hi, haddress(n));
1338 #else
1339   __ str_double(D0_tos, daddress(n));
1340 #endif // __SOFTFP__
1341 }
1342 
1343 
1344 void TemplateTable::astore(int n) {
1345   transition(vtos, vtos);
1346   __ pop_ptr(R0_tos);
1347   __ str(R0_tos, aaddress(n));
1348 }
1349 
1350 
1351 void TemplateTable::pop() {
1352   transition(vtos, vtos);
1353   __ add(Rstack_top, Rstack_top, Interpreter::stackElementSize);
1354 }
1355 
1356 
1357 void TemplateTable::pop2() {
1358   transition(vtos, vtos);
1359   __ add(Rstack_top, Rstack_top, 2*Interpreter::stackElementSize);
1360 }
1361 
1362 
1363 void TemplateTable::dup() {
1364   transition(vtos, vtos);
1365   // stack: ..., a
1366   __ load_ptr(0, R0_tmp);
1367   __ push_ptr(R0_tmp);
1368   // stack: ..., a, a
1369 }
1370 
1371 
1372 void TemplateTable::dup_x1() {
1373   transition(vtos, vtos);
1374   // stack: ..., a, b
1375   __ load_ptr(0, R0_tmp);  // load b
1376   __ load_ptr(1, R2_tmp);  // load a
1377   __ store_ptr(1, R0_tmp); // store b
1378   __ store_ptr(0, R2_tmp); // store a
1379   __ push_ptr(R0_tmp);     // push b
1380   // stack: ..., b, a, b
1381 }
1382 
1383 
1384 void TemplateTable::dup_x2() {
1385   transition(vtos, vtos);
1386   // stack: ..., a, b, c
1387   __ load_ptr(0, R0_tmp);   // load c
1388   __ load_ptr(1, R2_tmp);   // load b
1389   __ load_ptr(2, R4_tmp);   // load a
1390 
1391   __ push_ptr(R0_tmp);      // push c
1392 
1393   // stack: ..., a, b, c, c
1394   __ store_ptr(1, R2_tmp);  // store b
1395   __ store_ptr(2, R4_tmp);  // store a
1396   __ store_ptr(3, R0_tmp);  // store c
1397   // stack: ..., c, a, b, c
1398 }
1399 
1400 
1401 void TemplateTable::dup2() {
1402   transition(vtos, vtos);
1403   // stack: ..., a, b
1404   __ load_ptr(1, R0_tmp);  // load a
1405   __ push_ptr(R0_tmp);     // push a
1406   __ load_ptr(1, R0_tmp);  // load b
1407   __ push_ptr(R0_tmp);     // push b
1408   // stack: ..., a, b, a, b
1409 }
1410 
1411 
1412 void TemplateTable::dup2_x1() {
1413   transition(vtos, vtos);
1414 
1415   // stack: ..., a, b, c
1416   __ load_ptr(0, R4_tmp);  // load c
1417   __ load_ptr(1, R2_tmp);  // load b
1418   __ load_ptr(2, R0_tmp);  // load a
1419 
1420   __ push_ptr(R2_tmp);     // push b
1421   __ push_ptr(R4_tmp);     // push c
1422 
1423   // stack: ..., a, b, c, b, c
1424 
1425   __ store_ptr(2, R0_tmp);  // store a
1426   __ store_ptr(3, R4_tmp);  // store c
1427   __ store_ptr(4, R2_tmp);  // store b
1428 
1429   // stack: ..., b, c, a, b, c
1430 }
1431 
1432 
1433 void TemplateTable::dup2_x2() {
1434   transition(vtos, vtos);
1435   // stack: ..., a, b, c, d
1436   __ load_ptr(0, R0_tmp);  // load d
1437   __ load_ptr(1, R2_tmp);  // load c
1438   __ push_ptr(R2_tmp);     // push c
1439   __ push_ptr(R0_tmp);     // push d
1440   // stack: ..., a, b, c, d, c, d
1441   __ load_ptr(4, R4_tmp);  // load b
1442   __ store_ptr(4, R0_tmp); // store d in b
1443   __ store_ptr(2, R4_tmp); // store b in d
1444   // stack: ..., a, d, c, b, c, d
1445   __ load_ptr(5, R4_tmp);  // load a
1446   __ store_ptr(5, R2_tmp); // store c in a
1447   __ store_ptr(3, R4_tmp); // store a in c
1448   // stack: ..., c, d, a, b, c, d
1449 }
1450 
1451 
1452 void TemplateTable::swap() {
1453   transition(vtos, vtos);
1454   // stack: ..., a, b
1455   __ load_ptr(1, R0_tmp);  // load a
1456   __ load_ptr(0, R2_tmp);  // load b
1457   __ store_ptr(0, R0_tmp); // store a in b
1458   __ store_ptr(1, R2_tmp); // store b in a
1459   // stack: ..., b, a
1460 }
1461 
1462 
1463 void TemplateTable::iop2(Operation op) {
1464   transition(itos, itos);
1465   const Register arg1 = R1_tmp;
1466   const Register arg2 = R0_tos;
1467 
1468   __ pop_i(arg1);
1469   switch (op) {
1470     case add  : __ add_32 (R0_tos, arg1, arg2); break;
1471     case sub  : __ sub_32 (R0_tos, arg1, arg2); break;
1472     case mul  : __ mul_32 (R0_tos, arg1, arg2); break;
1473     case _and : __ and_32 (R0_tos, arg1, arg2); break;
1474     case _or  : __ orr_32 (R0_tos, arg1, arg2); break;
1475     case _xor : __ eor_32 (R0_tos, arg1, arg2); break;
1476     case shl  : __ andr(arg2, arg2, 0x1f); __ mov (R0_tos, AsmOperand(arg1, lsl, arg2)); break;
1477     case shr  : __ andr(arg2, arg2, 0x1f); __ mov (R0_tos, AsmOperand(arg1, asr, arg2)); break;
1478     case ushr : __ andr(arg2, arg2, 0x1f); __ mov (R0_tos, AsmOperand(arg1, lsr, arg2)); break;
1479     default   : ShouldNotReachHere();
1480   }
1481 }
1482 
1483 
1484 void TemplateTable::lop2(Operation op) {
1485   transition(ltos, ltos);
1486   const Register arg1_lo = R2_tmp;
1487   const Register arg1_hi = R3_tmp;
1488   const Register arg2_lo = R0_tos_lo;
1489   const Register arg2_hi = R1_tos_hi;
1490 
1491   __ pop_l(arg1_lo, arg1_hi);
1492   switch (op) {
1493     case add : __ adds(R0_tos_lo, arg1_lo, arg2_lo); __ adc (R1_tos_hi, arg1_hi, arg2_hi); break;
1494     case sub : __ subs(R0_tos_lo, arg1_lo, arg2_lo); __ sbc (R1_tos_hi, arg1_hi, arg2_hi); break;
1495     case _and: __ andr(R0_tos_lo, arg1_lo, arg2_lo); __ andr(R1_tos_hi, arg1_hi, arg2_hi); break;
1496     case _or : __ orr (R0_tos_lo, arg1_lo, arg2_lo); __ orr (R1_tos_hi, arg1_hi, arg2_hi); break;
1497     case _xor: __ eor (R0_tos_lo, arg1_lo, arg2_lo); __ eor (R1_tos_hi, arg1_hi, arg2_hi); break;
1498     default : ShouldNotReachHere();
1499   }
1500 }
1501 
1502 
1503 void TemplateTable::idiv() {
1504   transition(itos, itos);
1505   __ mov(R2, R0_tos);
1506   __ pop_i(R0);
1507   // R0 - dividend
1508   // R2 - divisor
1509   __ call(StubRoutines::Arm::idiv_irem_entry(), relocInfo::none);
1510   // R1 - result
1511   __ mov(R0_tos, R1);
1512 }
1513 
1514 
1515 void TemplateTable::irem() {
1516   transition(itos, itos);
1517   __ mov(R2, R0_tos);
1518   __ pop_i(R0);
1519   // R0 - dividend
1520   // R2 - divisor
1521   __ call(StubRoutines::Arm::idiv_irem_entry(), relocInfo::none);
1522   // R0 - remainder
1523 }
1524 
1525 
1526 void TemplateTable::lmul() {
1527   transition(ltos, ltos);
1528   const Register arg1_lo = R0_tos_lo;
1529   const Register arg1_hi = R1_tos_hi;
1530   const Register arg2_lo = R2_tmp;
1531   const Register arg2_hi = R3_tmp;
1532 
1533   __ pop_l(arg2_lo, arg2_hi);
1534 
1535   __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::lmul), arg1_lo, arg1_hi, arg2_lo, arg2_hi);
1536 }
1537 
1538 
1539 void TemplateTable::ldiv() {
1540   transition(ltos, ltos);
1541   const Register x_lo = R2_tmp;
1542   const Register x_hi = R3_tmp;
1543   const Register y_lo = R0_tos_lo;
1544   const Register y_hi = R1_tos_hi;
1545 
1546   __ pop_l(x_lo, x_hi);
1547 
1548   // check if y = 0
1549   __ orrs(Rtemp, y_lo, y_hi);
1550   __ call(Interpreter::_throw_ArithmeticException_entry, relocInfo::none, eq);
1551   __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::ldiv), y_lo, y_hi, x_lo, x_hi);
1552 }
1553 
1554 
1555 void TemplateTable::lrem() {
1556   transition(ltos, ltos);
1557   const Register x_lo = R2_tmp;
1558   const Register x_hi = R3_tmp;
1559   const Register y_lo = R0_tos_lo;
1560   const Register y_hi = R1_tos_hi;
1561 
1562   __ pop_l(x_lo, x_hi);
1563 
1564   // check if y = 0
1565   __ orrs(Rtemp, y_lo, y_hi);
1566   __ call(Interpreter::_throw_ArithmeticException_entry, relocInfo::none, eq);
1567   __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::lrem), y_lo, y_hi, x_lo, x_hi);
1568 }
1569 
1570 
1571 void TemplateTable::lshl() {
1572   transition(itos, ltos);
1573   const Register shift_cnt = R4_tmp;
1574   const Register val_lo = R2_tmp;
1575   const Register val_hi = R3_tmp;
1576 
1577   __ pop_l(val_lo, val_hi);
1578   __ andr(shift_cnt, R0_tos, 63);
1579   __ long_shift(R0_tos_lo, R1_tos_hi, val_lo, val_hi, lsl, shift_cnt);
1580 }
1581 
1582 
1583 void TemplateTable::lshr() {
1584   transition(itos, ltos);
1585   const Register shift_cnt = R4_tmp;
1586   const Register val_lo = R2_tmp;
1587   const Register val_hi = R3_tmp;
1588 
1589   __ pop_l(val_lo, val_hi);
1590   __ andr(shift_cnt, R0_tos, 63);
1591   __ long_shift(R0_tos_lo, R1_tos_hi, val_lo, val_hi, asr, shift_cnt);
1592 }
1593 
1594 
1595 void TemplateTable::lushr() {
1596   transition(itos, ltos);
1597   const Register shift_cnt = R4_tmp;
1598   const Register val_lo = R2_tmp;
1599   const Register val_hi = R3_tmp;
1600 
1601   __ pop_l(val_lo, val_hi);
1602   __ andr(shift_cnt, R0_tos, 63);
1603   __ long_shift(R0_tos_lo, R1_tos_hi, val_lo, val_hi, lsr, shift_cnt);
1604 }
1605 
1606 
1607 void TemplateTable::fop2(Operation op) {
1608   transition(ftos, ftos);
1609 #ifdef __SOFTFP__
1610   __ mov(R1, R0_tos);
1611   __ pop_i(R0);
1612   switch (op) {
1613     case add: __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_fadd_glibc), R0, R1); break;
1614     case sub: __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_fsub_glibc), R0, R1); break;
1615     case mul: __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_fmul), R0, R1); break;
1616     case div: __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_fdiv), R0, R1); break;
1617     case rem: __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::frem), R0, R1); break;
1618     default : ShouldNotReachHere();
1619   }
1620 #else
1621   const FloatRegister arg1 = S1_tmp;
1622   const FloatRegister arg2 = S0_tos;
1623 
1624   switch (op) {
1625     case add: __ pop_f(arg1); __ add_float(S0_tos, arg1, arg2); break;
1626     case sub: __ pop_f(arg1); __ sub_float(S0_tos, arg1, arg2); break;
1627     case mul: __ pop_f(arg1); __ mul_float(S0_tos, arg1, arg2); break;
1628     case div: __ pop_f(arg1); __ div_float(S0_tos, arg1, arg2); break;
1629     case rem:
1630 #ifndef __ABI_HARD__
1631       __ pop_f(arg1);
1632       __ fmrs(R0, arg1);
1633       __ fmrs(R1, arg2);
1634       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::frem), R0, R1);
1635       __ fmsr(S0_tos, R0);
1636 #else
1637       __ mov_float(S1_reg, arg2);
1638       __ pop_f(S0);
1639       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::frem));
1640 #endif // !__ABI_HARD__
1641       break;
1642     default : ShouldNotReachHere();
1643   }
1644 #endif // __SOFTFP__
1645 }
1646 
1647 
1648 void TemplateTable::dop2(Operation op) {
1649   transition(dtos, dtos);
1650 #ifdef __SOFTFP__
1651   __ mov(R2, R0_tos_lo);
1652   __ mov(R3, R1_tos_hi);
1653   __ pop_l(R0, R1);
1654   switch (op) {
1655     // __aeabi_XXXX_glibc: Imported code from glibc soft-fp bundle for calculation accuracy improvement. See CR 6757269.
1656     case add: __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_dadd_glibc), R0, R1, R2, R3); break;
1657     case sub: __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_dsub_glibc), R0, R1, R2, R3); break;
1658     case mul: __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_dmul), R0, R1, R2, R3); break;
1659     case div: __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_ddiv), R0, R1, R2, R3); break;
1660     case rem: __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::drem), R0, R1, R2, R3); break;
1661     default : ShouldNotReachHere();
1662   }
1663 #else
1664   const FloatRegister arg1 = D1_tmp;
1665   const FloatRegister arg2 = D0_tos;
1666 
1667   switch (op) {
1668     case add: __ pop_d(arg1); __ add_double(D0_tos, arg1, arg2); break;
1669     case sub: __ pop_d(arg1); __ sub_double(D0_tos, arg1, arg2); break;
1670     case mul: __ pop_d(arg1); __ mul_double(D0_tos, arg1, arg2); break;
1671     case div: __ pop_d(arg1); __ div_double(D0_tos, arg1, arg2); break;
1672     case rem:
1673 #ifndef __ABI_HARD__
1674       __ pop_d(arg1);
1675       __ fmrrd(R0, R1, arg1);
1676       __ fmrrd(R2, R3, arg2);
1677       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::drem), R0, R1, R2, R3);
1678       __ fmdrr(D0_tos, R0, R1);
1679 #else
1680       __ mov_double(D1, arg2);
1681       __ pop_d(D0);
1682       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::drem));
1683 #endif // !__ABI_HARD__
1684       break;
1685     default : ShouldNotReachHere();
1686   }
1687 #endif // __SOFTFP__
1688 }
1689 
1690 
1691 void TemplateTable::ineg() {
1692   transition(itos, itos);
1693   __ neg_32(R0_tos, R0_tos);
1694 }
1695 
1696 
1697 void TemplateTable::lneg() {
1698   transition(ltos, ltos);
1699   __ rsbs(R0_tos_lo, R0_tos_lo, 0);
1700   __ rsc (R1_tos_hi, R1_tos_hi, 0);
1701 }
1702 
1703 
1704 void TemplateTable::fneg() {
1705   transition(ftos, ftos);
1706 #ifdef __SOFTFP__
1707   // Invert sign bit
1708   const int sign_mask = 0x80000000;
1709   __ eor(R0_tos, R0_tos, sign_mask);
1710 #else
1711   __ neg_float(S0_tos, S0_tos);
1712 #endif // __SOFTFP__
1713 }
1714 
1715 
1716 void TemplateTable::dneg() {
1717   transition(dtos, dtos);
1718 #ifdef __SOFTFP__
1719   // Invert sign bit in the high part of the double
1720   const int sign_mask_hi = 0x80000000;
1721   __ eor(R1_tos_hi, R1_tos_hi, sign_mask_hi);
1722 #else
1723   __ neg_double(D0_tos, D0_tos);
1724 #endif // __SOFTFP__
1725 }
1726 
1727 
1728 void TemplateTable::iinc() {
1729   transition(vtos, vtos);
1730   const Register Rconst = R2_tmp;
1731   const Register Rlocal_index = R1_tmp;
1732   const Register Rval = R0_tmp;
1733 
1734   __ ldrsb(Rconst, at_bcp(2));
1735   locals_index(Rlocal_index);
1736   Address local = load_iaddress(Rlocal_index, Rtemp);
1737   __ ldr_s32(Rval, local);
1738   __ add(Rval, Rval, Rconst);
1739   __ str_32(Rval, local);
1740 }
1741 
1742 
1743 void TemplateTable::wide_iinc() {
1744   transition(vtos, vtos);
1745   const Register Rconst = R2_tmp;
1746   const Register Rlocal_index = R1_tmp;
1747   const Register Rval = R0_tmp;
1748 
1749   // get constant in Rconst
1750   __ ldrsb(R2_tmp, at_bcp(4));
1751   __ ldrb(R3_tmp, at_bcp(5));
1752   __ orr(Rconst, R3_tmp, AsmOperand(R2_tmp, lsl, 8));
1753 
1754   locals_index_wide(Rlocal_index);
1755   Address local = load_iaddress(Rlocal_index, Rtemp);
1756   __ ldr_s32(Rval, local);
1757   __ add(Rval, Rval, Rconst);
1758   __ str_32(Rval, local);
1759 }
1760 
1761 
1762 void TemplateTable::convert() {
1763   // Checking
1764 #ifdef ASSERT
1765   { TosState tos_in  = ilgl;
1766     TosState tos_out = ilgl;
1767     switch (bytecode()) {
1768       case Bytecodes::_i2l: // fall through
1769       case Bytecodes::_i2f: // fall through
1770       case Bytecodes::_i2d: // fall through
1771       case Bytecodes::_i2b: // fall through
1772       case Bytecodes::_i2c: // fall through
1773       case Bytecodes::_i2s: tos_in = itos; break;
1774       case Bytecodes::_l2i: // fall through
1775       case Bytecodes::_l2f: // fall through
1776       case Bytecodes::_l2d: tos_in = ltos; break;
1777       case Bytecodes::_f2i: // fall through
1778       case Bytecodes::_f2l: // fall through
1779       case Bytecodes::_f2d: tos_in = ftos; break;
1780       case Bytecodes::_d2i: // fall through
1781       case Bytecodes::_d2l: // fall through
1782       case Bytecodes::_d2f: tos_in = dtos; break;
1783       default             : ShouldNotReachHere();
1784     }
1785     switch (bytecode()) {
1786       case Bytecodes::_l2i: // fall through
1787       case Bytecodes::_f2i: // fall through
1788       case Bytecodes::_d2i: // fall through
1789       case Bytecodes::_i2b: // fall through
1790       case Bytecodes::_i2c: // fall through
1791       case Bytecodes::_i2s: tos_out = itos; break;
1792       case Bytecodes::_i2l: // fall through
1793       case Bytecodes::_f2l: // fall through
1794       case Bytecodes::_d2l: tos_out = ltos; break;
1795       case Bytecodes::_i2f: // fall through
1796       case Bytecodes::_l2f: // fall through
1797       case Bytecodes::_d2f: tos_out = ftos; break;
1798       case Bytecodes::_i2d: // fall through
1799       case Bytecodes::_l2d: // fall through
1800       case Bytecodes::_f2d: tos_out = dtos; break;
1801       default             : ShouldNotReachHere();
1802     }
1803     transition(tos_in, tos_out);
1804   }
1805 #endif // ASSERT
1806 
1807   // Conversion
1808   switch (bytecode()) {
1809     case Bytecodes::_i2l:
1810       __ mov(R1_tos_hi, AsmOperand(R0_tos, asr, BitsPerWord-1));
1811       break;
1812 
1813     case Bytecodes::_i2f:
1814 #ifdef __SOFTFP__
1815       __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_i2f), R0_tos);
1816 #else
1817       __ fmsr(S0_tmp, R0_tos);
1818       __ fsitos(S0_tos, S0_tmp);
1819 #endif // __SOFTFP__
1820       break;
1821 
1822     case Bytecodes::_i2d:
1823 #ifdef __SOFTFP__
1824       __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_i2d), R0_tos);
1825 #else
1826       __ fmsr(S0_tmp, R0_tos);
1827       __ fsitod(D0_tos, S0_tmp);
1828 #endif // __SOFTFP__
1829       break;
1830 
1831     case Bytecodes::_i2b:
1832       __ sign_extend(R0_tos, R0_tos, 8);
1833       break;
1834 
1835     case Bytecodes::_i2c:
1836       __ zero_extend(R0_tos, R0_tos, 16);
1837       break;
1838 
1839     case Bytecodes::_i2s:
1840       __ sign_extend(R0_tos, R0_tos, 16);
1841       break;
1842 
1843     case Bytecodes::_l2i:
1844       /* nothing to do */
1845       break;
1846 
1847     case Bytecodes::_l2f:
1848       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::l2f), R0_tos_lo, R1_tos_hi);
1849 #if !defined(__SOFTFP__) &amp;&amp; !defined(__ABI_HARD__)
1850       __ fmsr(S0_tos, R0);
1851 #endif // !__SOFTFP__ &amp;&amp; !__ABI_HARD__
1852       break;
1853 
1854     case Bytecodes::_l2d:
1855       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::l2d), R0_tos_lo, R1_tos_hi);
1856 #if !defined(__SOFTFP__) &amp;&amp; !defined(__ABI_HARD__)
1857       __ fmdrr(D0_tos, R0, R1);
1858 #endif // !__SOFTFP__ &amp;&amp; !__ABI_HARD__
1859       break;
1860 
1861     case Bytecodes::_f2i:
1862 #ifndef __SOFTFP__
1863       __ ftosizs(S0_tos, S0_tos);
1864       __ fmrs(R0_tos, S0_tos);
1865 #else
1866       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::f2i), R0_tos);
1867 #endif // !__SOFTFP__
1868       break;
1869 
1870     case Bytecodes::_f2l:
1871 #ifndef __SOFTFP__
1872       __ fmrs(R0_tos, S0_tos);
1873 #endif // !__SOFTFP__
1874       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::f2l), R0_tos);
1875       break;
1876 
1877     case Bytecodes::_f2d:
1878 #ifdef __SOFTFP__
1879       __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_f2d), R0_tos);
1880 #else
1881       __ convert_f2d(D0_tos, S0_tos);
1882 #endif // __SOFTFP__
1883       break;
1884 
1885     case Bytecodes::_d2i:
1886 #ifndef __SOFTFP__
1887       __ ftosizd(Stemp, D0);
1888       __ fmrs(R0, Stemp);
1889 #else
1890       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::d2i), R0_tos_lo, R1_tos_hi);
1891 #endif // !__SOFTFP__
1892       break;
1893 
1894     case Bytecodes::_d2l:
1895 #ifndef __SOFTFP__
1896       __ fmrrd(R0_tos_lo, R1_tos_hi, D0_tos);
1897 #endif // !__SOFTFP__
1898       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::d2l), R0_tos_lo, R1_tos_hi);
1899       break;
1900 
1901     case Bytecodes::_d2f:
1902 #ifdef __SOFTFP__
1903       __ call_VM_leaf(CAST_FROM_FN_PTR(address, __aeabi_d2f), R0_tos_lo, R1_tos_hi);
1904 #else
1905       __ convert_d2f(S0_tos, D0_tos);
1906 #endif // __SOFTFP__
1907       break;
1908 
1909     default:
1910       ShouldNotReachHere();
1911   }
1912 }
1913 
1914 
1915 void TemplateTable::lcmp() {
1916   transition(ltos, itos);
1917   const Register arg1_lo = R2_tmp;
1918   const Register arg1_hi = R3_tmp;
1919   const Register arg2_lo = R0_tos_lo;
1920   const Register arg2_hi = R1_tos_hi;
1921   const Register res = R4_tmp;
1922 
1923   __ pop_l(arg1_lo, arg1_hi);
1924 
1925   // long compare arg1 with arg2
1926   // result is -1/0/+1 if &#39;&lt;&#39;/&#39;=&#39;/&#39;&gt;&#39;
1927   Label done;
1928 
1929   __ mov (res, 0);
1930   __ cmp (arg1_hi, arg2_hi);
1931   __ mvn (res, 0, lt);
1932   __ mov (res, 1, gt);
1933   __ b(done, ne);
1934   __ cmp (arg1_lo, arg2_lo);
1935   __ mvn (res, 0, lo);
1936   __ mov (res, 1, hi);
1937   __ bind(done);
1938   __ mov (R0_tos, res);
1939 }
1940 
1941 
1942 void TemplateTable::float_cmp(bool is_float, int unordered_result) {
1943   assert((unordered_result == 1) || (unordered_result == -1), &quot;invalid unordered result&quot;);
1944 
1945 
1946 #ifdef __SOFTFP__
1947 
1948   if (is_float) {
1949     transition(ftos, itos);
1950     const Register Rx = R0;
1951     const Register Ry = R1;
1952 
1953     __ mov(Ry, R0_tos);
1954     __ pop_i(Rx);
1955 
1956     if (unordered_result == 1) {
1957       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::fcmpg), Rx, Ry);
1958     } else {
1959       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::fcmpl), Rx, Ry);
1960     }
1961 
1962   } else {
1963 
1964     transition(dtos, itos);
1965     const Register Rx_lo = R0;
1966     const Register Rx_hi = R1;
1967     const Register Ry_lo = R2;
1968     const Register Ry_hi = R3;
1969 
1970     __ mov(Ry_lo, R0_tos_lo);
1971     __ mov(Ry_hi, R1_tos_hi);
1972     __ pop_l(Rx_lo, Rx_hi);
1973 
1974     if (unordered_result == 1) {
1975       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dcmpg), Rx_lo, Rx_hi, Ry_lo, Ry_hi);
1976     } else {
1977       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dcmpl), Rx_lo, Rx_hi, Ry_lo, Ry_hi);
1978     }
1979   }
1980 
1981 #else
1982 
1983   if (is_float) {
1984     transition(ftos, itos);
1985     __ pop_f(S1_tmp);
1986     __ fcmps(S1_tmp, S0_tos);
1987   } else {
1988     transition(dtos, itos);
1989     __ pop_d(D1_tmp);
1990     __ fcmpd(D1_tmp, D0_tos);
1991   }
1992 
1993   __ fmstat();
1994 
1995   // comparison result | flag N | flag Z | flag C | flag V
1996   // &quot;&lt;&quot;               |   1    |   0    |   0    |   0
1997   // &quot;==&quot;              |   0    |   1    |   1    |   0
1998   // &quot;&gt;&quot;               |   0    |   0    |   1    |   0
1999   // unordered         |   0    |   0    |   1    |   1
2000 
2001   if (unordered_result &lt; 0) {
2002     __ mov(R0_tos, 1);           // result ==  1 if greater
2003     __ mvn(R0_tos, 0, lt);       // result == -1 if less or unordered (N!=V)
2004   } else {
2005     __ mov(R0_tos, 1);           // result ==  1 if greater or unordered
2006     __ mvn(R0_tos, 0, mi);       // result == -1 if less (N=1)
2007   }
2008   __ mov(R0_tos, 0, eq);         // result ==  0 if equ (Z=1)
2009 #endif // __SOFTFP__
2010 }
2011 
2012 
2013 void TemplateTable::branch(bool is_jsr, bool is_wide) {
2014 
2015   const Register Rdisp = R0_tmp;
2016   const Register Rbumped_taken_count = R5_tmp;
2017 
2018   __ profile_taken_branch(R0_tmp, Rbumped_taken_count); // R0 holds updated MDP, Rbumped_taken_count holds bumped taken count
2019 
2020   const ByteSize be_offset = MethodCounters::backedge_counter_offset() +
2021                              InvocationCounter::counter_offset();
2022   const ByteSize inv_offset = MethodCounters::invocation_counter_offset() +
2023                               InvocationCounter::counter_offset();
2024   const int method_offset = frame::interpreter_frame_method_offset * wordSize;
2025 
2026   // Load up R0 with the branch displacement
2027   if (is_wide) {
2028     __ ldrsb(R0_tmp, at_bcp(1));
2029     __ ldrb(R1_tmp, at_bcp(2));
2030     __ ldrb(R2_tmp, at_bcp(3));
2031     __ ldrb(R3_tmp, at_bcp(4));
2032     __ orr(R0_tmp, R1_tmp, AsmOperand(R0_tmp, lsl, BitsPerByte));
2033     __ orr(R0_tmp, R2_tmp, AsmOperand(R0_tmp, lsl, BitsPerByte));
2034     __ orr(Rdisp, R3_tmp, AsmOperand(R0_tmp, lsl, BitsPerByte));
2035   } else {
2036     __ ldrsb(R0_tmp, at_bcp(1));
2037     __ ldrb(R1_tmp, at_bcp(2));
2038     __ orr(Rdisp, R1_tmp, AsmOperand(R0_tmp, lsl, BitsPerByte));
2039   }
2040 
2041   // Handle all the JSR stuff here, then exit.
2042   // It&#39;s much shorter and cleaner than intermingling with the
2043   // non-JSR normal-branch stuff occuring below.
2044   if (is_jsr) {
2045     // compute return address as bci in R1
2046     const Register Rret_addr = R1_tmp;
2047     assert_different_registers(Rdisp, Rret_addr, Rtemp);
2048 
2049     __ ldr(Rtemp, Address(Rmethod, Method::const_offset()));
2050     __ sub(Rret_addr, Rbcp, - (is_wide ? 5 : 3) + in_bytes(ConstMethod::codes_offset()));
2051     __ sub(Rret_addr, Rret_addr, Rtemp);
2052 
2053     // Load the next target bytecode into R3_bytecode and advance Rbcp
2054     __ ldrb(R3_bytecode, Address(Rbcp, Rdisp, lsl, 0, pre_indexed));
2055 
2056     // Push return address
2057     __ push_i(Rret_addr);
2058     // jsr returns vtos
2059     __ dispatch_only_noverify(vtos);
2060     return;
2061   }
2062 
2063   // Normal (non-jsr) branch handling
2064 
2065   // Adjust the bcp by the displacement in Rdisp and load next bytecode.
2066   __ ldrb(R3_bytecode, Address(Rbcp, Rdisp, lsl, 0, pre_indexed));
2067 
2068   assert(UseLoopCounter || !UseOnStackReplacement, &quot;on-stack-replacement requires loop counters&quot;);
2069   Label backedge_counter_overflow;
2070   Label profile_method;
2071   Label dispatch;
2072 
2073   if (UseLoopCounter) {
2074     // increment backedge counter for backward branches
2075     // Rdisp (R0): target offset
2076 
2077     const Register Rcnt = R2_tmp;
2078     const Register Rcounters = R1_tmp;
2079 
2080     // count only if backward branch
2081     __ tst(Rdisp, Rdisp);
2082     __ b(dispatch, pl);
2083 
2084     if (TieredCompilation) {
2085       Label no_mdo;
2086       int increment = InvocationCounter::count_increment;
2087       if (ProfileInterpreter) {
2088         // Are we profiling?
2089         __ ldr(Rtemp, Address(Rmethod, Method::method_data_offset()));
2090         __ cbz(Rtemp, no_mdo);
2091         // Increment the MDO backedge counter
2092         const Address mdo_backedge_counter(Rtemp, in_bytes(MethodData::backedge_counter_offset()) +
2093                                                   in_bytes(InvocationCounter::counter_offset()));
2094         const Address mask(Rtemp, in_bytes(MethodData::backedge_mask_offset()));
2095         __ increment_mask_and_jump(mdo_backedge_counter, increment, mask,
2096                                    Rcnt, R4_tmp, eq, &amp;backedge_counter_overflow);
2097         __ b(dispatch);
2098       }
2099       __ bind(no_mdo);
2100       // Increment backedge counter in MethodCounters*
2101       // Note Rbumped_taken_count is a callee saved registers for ARM32
2102       __ get_method_counters(Rmethod, Rcounters, dispatch, true /*saveRegs*/,
2103                              Rdisp, R3_bytecode,
2104                              noreg);
2105       const Address mask(Rcounters, in_bytes(MethodCounters::backedge_mask_offset()));
2106       __ increment_mask_and_jump(Address(Rcounters, be_offset), increment, mask,
2107                                  Rcnt, R4_tmp, eq, &amp;backedge_counter_overflow);
2108     } else {
2109       // Increment backedge counter in MethodCounters*
2110       __ get_method_counters(Rmethod, Rcounters, dispatch, true /*saveRegs*/,
2111                              Rdisp, R3_bytecode,
2112                              noreg);
2113       __ ldr_u32(Rtemp, Address(Rcounters, be_offset));           // load backedge counter
2114       __ add(Rtemp, Rtemp, InvocationCounter::count_increment);   // increment counter
2115       __ str_32(Rtemp, Address(Rcounters, be_offset));            // store counter
2116 
2117       __ ldr_u32(Rcnt, Address(Rcounters, inv_offset));           // load invocation counter
2118       __ bic(Rcnt, Rcnt, ~InvocationCounter::count_mask_value);  // and the status bits
2119       __ add(Rcnt, Rcnt, Rtemp);                                 // add both counters
2120 
2121       if (ProfileInterpreter) {
2122         // Test to see if we should create a method data oop
2123         const Address profile_limit(Rcounters, in_bytes(MethodCounters::interpreter_profile_limit_offset()));
2124         __ ldr_s32(Rtemp, profile_limit);
2125         __ cmp_32(Rcnt, Rtemp);
2126         __ b(dispatch, lt);
2127 
2128         // if no method data exists, go to profile method
2129         __ test_method_data_pointer(R4_tmp, profile_method);
2130 
2131         if (UseOnStackReplacement) {
2132           // check for overflow against Rbumped_taken_count, which is the MDO taken count
2133           const Address backward_branch_limit(Rcounters, in_bytes(MethodCounters::interpreter_backward_branch_limit_offset()));
2134           __ ldr_s32(Rtemp, backward_branch_limit);
2135           __ cmp(Rbumped_taken_count, Rtemp);
2136           __ b(dispatch, lo);
2137 
2138           // When ProfileInterpreter is on, the backedge_count comes from the
2139           // MethodData*, which value does not get reset on the call to
2140           // frequency_counter_overflow().  To avoid excessive calls to the overflow
2141           // routine while the method is being compiled, add a second test to make
2142           // sure the overflow function is called only once every overflow_frequency.
2143           const int overflow_frequency = 1024;
2144 
2145           // was &#39;__ andrs(...,overflow_frequency-1)&#39;, testing if lowest 10 bits are 0
2146           assert(overflow_frequency == (1 &lt;&lt; 10),&quot;shift by 22 not correct for expected frequency&quot;);
2147           __ movs(Rbumped_taken_count, AsmOperand(Rbumped_taken_count, lsl, 22));
2148 
2149           __ b(backedge_counter_overflow, eq);
2150         }
2151       } else {
2152         if (UseOnStackReplacement) {
2153           // check for overflow against Rcnt, which is the sum of the counters
2154           const Address backward_branch_limit(Rcounters, in_bytes(MethodCounters::interpreter_backward_branch_limit_offset()));
2155           __ ldr_s32(Rtemp, backward_branch_limit);
2156           __ cmp_32(Rcnt, Rtemp);
2157           __ b(backedge_counter_overflow, hs);
2158 
2159         }
2160       }
2161     }
2162     __ bind(dispatch);
2163   }
2164 
2165   if (!UseOnStackReplacement) {
2166     __ bind(backedge_counter_overflow);
2167   }
2168 
2169   // continue with the bytecode @ target
<a name="7" id="anc7"></a><span class="line-modified">2170   __ dispatch_only(vtos);</span>
2171 
2172   if (UseLoopCounter) {
2173     if (ProfileInterpreter) {
2174       // Out-of-line code to allocate method data oop.
2175       __ bind(profile_method);
2176 
2177       __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));
2178       __ set_method_data_pointer_for_bcp();
2179       // reload next bytecode
2180       __ ldrb(R3_bytecode, Address(Rbcp));
2181       __ b(dispatch);
2182     }
2183 
2184     if (UseOnStackReplacement) {
2185       // invocation counter overflow
2186       __ bind(backedge_counter_overflow);
2187 
2188       __ sub(R1, Rbcp, Rdisp);                   // branch bcp
2189       call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::frequency_counter_overflow), R1);
2190 
2191       // R0: osr nmethod (osr ok) or NULL (osr not possible)
2192       const Register Rnmethod = R0;
2193 
2194       __ ldrb(R3_bytecode, Address(Rbcp));       // reload next bytecode
2195 
2196       __ cbz(Rnmethod, dispatch);                // test result, no osr if null
2197 
2198       // nmethod may have been invalidated (VM may block upon call_VM return)
2199       __ ldrb(R1_tmp, Address(Rnmethod, nmethod::state_offset()));
2200       __ cmp(R1_tmp, nmethod::in_use);
2201       __ b(dispatch, ne);
2202 
2203       // We have the address of an on stack replacement routine in Rnmethod,
2204       // We need to prepare to execute the OSR method. First we must
2205       // migrate the locals and monitors off of the stack.
2206 
2207       __ mov(Rtmp_save0, Rnmethod);                      // save the nmethod
2208 
2209       call_VM(noreg, CAST_FROM_FN_PTR(address, SharedRuntime::OSR_migration_begin));
2210 
2211       // R0 is OSR buffer
2212 
2213       __ ldr(R1_tmp, Address(Rtmp_save0, nmethod::osr_entry_point_offset()));
2214       __ ldr(Rtemp, Address(FP, frame::interpreter_frame_sender_sp_offset * wordSize));
2215 
2216       __ ldmia(FP, RegisterSet(FP) | RegisterSet(LR));
2217       __ bic(SP, Rtemp, StackAlignmentInBytes - 1);     // Remove frame and align stack
2218 
2219       __ jump(R1_tmp);
2220     }
2221   }
2222 }
2223 
2224 
2225 void TemplateTable::if_0cmp(Condition cc) {
2226   transition(itos, vtos);
2227   // assume branch is more often taken than not (loops use backward branches)
2228   Label not_taken;
2229   __ cmp_32(R0_tos, 0);
2230   __ b(not_taken, convNegCond(cc));
2231   branch(false, false);
2232   __ bind(not_taken);
2233   __ profile_not_taken_branch(R0_tmp);
2234 }
2235 
2236 
2237 void TemplateTable::if_icmp(Condition cc) {
2238   transition(itos, vtos);
2239   // assume branch is more often taken than not (loops use backward branches)
2240   Label not_taken;
2241   __ pop_i(R1_tmp);
2242   __ cmp_32(R1_tmp, R0_tos);
2243   __ b(not_taken, convNegCond(cc));
2244   branch(false, false);
2245   __ bind(not_taken);
2246   __ profile_not_taken_branch(R0_tmp);
2247 }
2248 
2249 
2250 void TemplateTable::if_nullcmp(Condition cc) {
2251   transition(atos, vtos);
2252   assert(cc == equal || cc == not_equal, &quot;invalid condition&quot;);
2253 
2254   // assume branch is more often taken than not (loops use backward branches)
2255   Label not_taken;
2256   if (cc == equal) {
2257     __ cbnz(R0_tos, not_taken);
2258   } else {
2259     __ cbz(R0_tos, not_taken);
2260   }
2261   branch(false, false);
2262   __ bind(not_taken);
2263   __ profile_not_taken_branch(R0_tmp);
2264 }
2265 
2266 
2267 void TemplateTable::if_acmp(Condition cc) {
2268   transition(atos, vtos);
2269   // assume branch is more often taken than not (loops use backward branches)
2270   Label not_taken;
2271   __ pop_ptr(R1_tmp);
2272   __ cmpoop(R1_tmp, R0_tos);
2273   __ b(not_taken, convNegCond(cc));
2274   branch(false, false);
2275   __ bind(not_taken);
2276   __ profile_not_taken_branch(R0_tmp);
2277 }
2278 
2279 
2280 void TemplateTable::ret() {
2281   transition(vtos, vtos);
2282   const Register Rlocal_index = R1_tmp;
2283   const Register Rret_bci = Rtmp_save0; // R4/R19
2284 
2285   locals_index(Rlocal_index);
2286   Address local = load_iaddress(Rlocal_index, Rtemp);
2287   __ ldr_s32(Rret_bci, local);          // get return bci, compute return bcp
2288   __ profile_ret(Rtmp_save1, Rret_bci);
2289   __ ldr(Rtemp, Address(Rmethod, Method::const_offset()));
2290   __ add(Rtemp, Rtemp, in_bytes(ConstMethod::codes_offset()));
2291   __ add(Rbcp, Rtemp, Rret_bci);
2292   __ dispatch_next(vtos);
2293 }
2294 
2295 
2296 void TemplateTable::wide_ret() {
2297   transition(vtos, vtos);
2298   const Register Rlocal_index = R1_tmp;
2299   const Register Rret_bci = Rtmp_save0; // R4/R19
2300 
2301   locals_index_wide(Rlocal_index);
2302   Address local = load_iaddress(Rlocal_index, Rtemp);
2303   __ ldr_s32(Rret_bci, local);               // get return bci, compute return bcp
2304   __ profile_ret(Rtmp_save1, Rret_bci);
2305   __ ldr(Rtemp, Address(Rmethod, Method::const_offset()));
2306   __ add(Rtemp, Rtemp, in_bytes(ConstMethod::codes_offset()));
2307   __ add(Rbcp, Rtemp, Rret_bci);
2308   __ dispatch_next(vtos);
2309 }
2310 
2311 
2312 void TemplateTable::tableswitch() {
2313   transition(itos, vtos);
2314 
2315   const Register Rindex  = R0_tos;
2316   const Register Rtemp2  = R1_tmp;
2317   const Register Rabcp   = R2_tmp;  // aligned bcp
2318   const Register Rlow    = R3_tmp;
2319   const Register Rhigh   = R4_tmp;
2320   const Register Roffset = R5_tmp;
2321 
2322   // align bcp
2323   __ add(Rtemp, Rbcp, 1 + (2*BytesPerInt-1));
2324   __ align_reg(Rabcp, Rtemp, BytesPerInt);
2325 
2326   // load lo &amp; hi
2327   __ ldmia(Rabcp, RegisterSet(Rlow) | RegisterSet(Rhigh), writeback);
2328   __ byteswap_u32(Rlow, Rtemp, Rtemp2);
2329   __ byteswap_u32(Rhigh, Rtemp, Rtemp2);
2330 
2331   // compare index with high bound
2332   __ cmp_32(Rhigh, Rindex);
2333 
2334 
2335   // if Rindex &lt;= Rhigh then calculate index in table (Rindex - Rlow)
2336   __ subs(Rindex, Rindex, Rlow, ge);
2337 
2338   // if Rindex &lt;= Rhigh and (Rindex - Rlow) &gt;= 0
2339   // (&quot;ge&quot; status accumulated from cmp and subs instructions) then load
2340   // offset from table, otherwise load offset for default case
2341 
2342   if(ProfileInterpreter) {
2343     Label default_case, continue_execution;
2344 
2345     __ b(default_case, lt);
2346     __ ldr(Roffset, Address(Rabcp, Rindex, lsl, LogBytesPerInt));
2347     __ profile_switch_case(Rabcp, Rindex, Rtemp2, R0_tmp);
2348     __ b(continue_execution);
2349 
2350     __ bind(default_case);
2351     __ profile_switch_default(R0_tmp);
2352     __ ldr(Roffset, Address(Rabcp, -3 * BytesPerInt));
2353 
2354     __ bind(continue_execution);
2355   } else {
2356     __ ldr(Roffset, Address(Rabcp, -3 * BytesPerInt), lt);
2357     __ ldr(Roffset, Address(Rabcp, Rindex, lsl, LogBytesPerInt), ge);
2358   }
2359 
2360   __ byteswap_u32(Roffset, Rtemp, Rtemp2);
2361 
2362   // load the next bytecode to R3_bytecode and advance Rbcp
2363   __ ldrb(R3_bytecode, Address(Rbcp, Roffset, lsl, 0, pre_indexed));
<a name="8" id="anc8"></a><span class="line-modified">2364   __ dispatch_only(vtos);</span>
2365 
2366 }
2367 
2368 
2369 void TemplateTable::lookupswitch() {
2370   transition(itos, itos);
2371   __ stop(&quot;lookupswitch bytecode should have been rewritten&quot;);
2372 }
2373 
2374 
2375 void TemplateTable::fast_linearswitch() {
2376   transition(itos, vtos);
2377   Label loop, found, default_case, continue_execution;
2378 
2379   const Register Rkey     = R0_tos;
2380   const Register Rabcp    = R2_tmp;  // aligned bcp
2381   const Register Rdefault = R3_tmp;
2382   const Register Rcount   = R4_tmp;
2383   const Register Roffset  = R5_tmp;
2384 
2385   // bswap Rkey, so we can avoid bswapping the table entries
2386   __ byteswap_u32(Rkey, R1_tmp, Rtemp);
2387 
2388   // align bcp
2389   __ add(Rtemp, Rbcp, 1 + (BytesPerInt-1));
2390   __ align_reg(Rabcp, Rtemp, BytesPerInt);
2391 
2392   // load default &amp; counter
2393   __ ldmia(Rabcp, RegisterSet(Rdefault) | RegisterSet(Rcount), writeback);
2394   __ byteswap_u32(Rcount, R1_tmp, Rtemp);
2395 
2396   __ cmp_32(Rcount, 0);
2397   __ ldr(Rtemp, Address(Rabcp, 2*BytesPerInt, post_indexed), ne);
2398   __ b(default_case, eq);
2399 
2400   // table search
2401   __ bind(loop);
2402   __ cmp_32(Rtemp, Rkey);
2403   __ b(found, eq);
2404   __ subs(Rcount, Rcount, 1);
2405   __ ldr(Rtemp, Address(Rabcp, 2*BytesPerInt, post_indexed), ne);
2406   __ b(loop, ne);
2407 
2408   // default case
2409   __ bind(default_case);
2410   __ profile_switch_default(R0_tmp);
2411   __ mov(Roffset, Rdefault);
2412   __ b(continue_execution);
2413 
2414   // entry found -&gt; get offset
2415   __ bind(found);
2416   // Rabcp is already incremented and points to the next entry
2417   __ ldr_s32(Roffset, Address(Rabcp, -BytesPerInt));
2418   if (ProfileInterpreter) {
2419     // Calculate index of the selected case.
2420     assert_different_registers(Roffset, Rcount, Rtemp, R0_tmp, R1_tmp, R2_tmp);
2421 
2422     // align bcp
2423     __ add(Rtemp, Rbcp, 1 + (BytesPerInt-1));
2424     __ align_reg(R2_tmp, Rtemp, BytesPerInt);
2425 
2426     // load number of cases
2427     __ ldr_u32(R2_tmp, Address(R2_tmp, BytesPerInt));
2428     __ byteswap_u32(R2_tmp, R1_tmp, Rtemp);
2429 
2430     // Selected index = &lt;number of cases&gt; - &lt;current loop count&gt;
2431     __ sub(R1_tmp, R2_tmp, Rcount);
2432     __ profile_switch_case(R0_tmp, R1_tmp, Rtemp, R1_tmp);
2433   }
2434 
2435   // continue execution
2436   __ bind(continue_execution);
2437   __ byteswap_u32(Roffset, R1_tmp, Rtemp);
2438 
2439   // load the next bytecode to R3_bytecode and advance Rbcp
2440   __ ldrb(R3_bytecode, Address(Rbcp, Roffset, lsl, 0, pre_indexed));
<a name="9" id="anc9"></a><span class="line-modified">2441   __ dispatch_only(vtos);</span>
2442 }
2443 
2444 
2445 void TemplateTable::fast_binaryswitch() {
2446   transition(itos, vtos);
2447   // Implementation using the following core algorithm:
2448   //
2449   // int binary_search(int key, LookupswitchPair* array, int n) {
2450   //   // Binary search according to &quot;Methodik des Programmierens&quot; by
2451   //   // Edsger W. Dijkstra and W.H.J. Feijen, Addison Wesley Germany 1985.
2452   //   int i = 0;
2453   //   int j = n;
2454   //   while (i+1 &lt; j) {
2455   //     // invariant P: 0 &lt;= i &lt; j &lt;= n and (a[i] &lt;= key &lt; a[j] or Q)
2456   //     // with      Q: for all i: 0 &lt;= i &lt; n: key &lt; a[i]
2457   //     // where a stands for the array and assuming that the (inexisting)
2458   //     // element a[n] is infinitely big.
2459   //     int h = (i + j) &gt;&gt; 1;
2460   //     // i &lt; h &lt; j
2461   //     if (key &lt; array[h].fast_match()) {
2462   //       j = h;
2463   //     } else {
2464   //       i = h;
2465   //     }
2466   //   }
2467   //   // R: a[i] &lt;= key &lt; a[i+1] or Q
2468   //   // (i.e., if key is within array, i is the correct index)
2469   //   return i;
2470   // }
2471 
2472   // register allocation
2473   const Register key    = R0_tos;                // already set (tosca)
2474   const Register array  = R1_tmp;
2475   const Register i      = R2_tmp;
2476   const Register j      = R3_tmp;
2477   const Register h      = R4_tmp;
2478   const Register val    = R5_tmp;
2479   const Register temp1  = Rtemp;
2480   const Register temp2  = LR_tmp;
2481   const Register offset = R3_tmp;
2482 
2483   // set &#39;array&#39; = aligned bcp + 2 ints
2484   __ add(temp1, Rbcp, 1 + (BytesPerInt-1) + 2*BytesPerInt);
2485   __ align_reg(array, temp1, BytesPerInt);
2486 
2487   // initialize i &amp; j
2488   __ mov(i, 0);                                  // i = 0;
2489   __ ldr_s32(j, Address(array, -BytesPerInt));   // j = length(array);
2490   // Convert j into native byteordering
2491   __ byteswap_u32(j, temp1, temp2);
2492 
2493   // and start
2494   Label entry;
2495   __ b(entry);
2496 
2497   // binary search loop
2498   { Label loop;
2499     __ bind(loop);
2500     // int h = (i + j) &gt;&gt; 1;
2501     __ add(h, i, j);                             // h = i + j;
2502     __ logical_shift_right(h, h, 1);             // h = (i + j) &gt;&gt; 1;
2503     // if (key &lt; array[h].fast_match()) {
2504     //   j = h;
2505     // } else {
2506     //   i = h;
2507     // }
2508     __ ldr_s32(val, Address(array, h, lsl, 1+LogBytesPerInt));
2509     // Convert array[h].match to native byte-ordering before compare
2510     __ byteswap_u32(val, temp1, temp2);
2511     __ cmp_32(key, val);
2512     __ mov(j, h, lt);   // j = h if (key &lt;  array[h].fast_match())
2513     __ mov(i, h, ge);   // i = h if (key &gt;= array[h].fast_match())
2514     // while (i+1 &lt; j)
2515     __ bind(entry);
2516     __ add(temp1, i, 1);                             // i+1
2517     __ cmp(temp1, j);                                // i+1 &lt; j
2518     __ b(loop, lt);
2519   }
2520 
2521   // end of binary search, result index is i (must check again!)
2522   Label default_case;
2523   // Convert array[i].match to native byte-ordering before compare
2524   __ ldr_s32(val, Address(array, i, lsl, 1+LogBytesPerInt));
2525   __ byteswap_u32(val, temp1, temp2);
2526   __ cmp_32(key, val);
2527   __ b(default_case, ne);
2528 
2529   // entry found
2530   __ add(temp1, array, AsmOperand(i, lsl, 1+LogBytesPerInt));
2531   __ ldr_s32(offset, Address(temp1, 1*BytesPerInt));
2532   __ profile_switch_case(R0, i, R1, i);
2533   __ byteswap_u32(offset, temp1, temp2);
2534   __ ldrb(R3_bytecode, Address(Rbcp, offset, lsl, 0, pre_indexed));
<a name="10" id="anc10"></a><span class="line-modified">2535   __ dispatch_only(vtos);</span>
2536 
2537   // default case
2538   __ bind(default_case);
2539   __ profile_switch_default(R0);
2540   __ ldr_s32(offset, Address(array, -2*BytesPerInt));
2541   __ byteswap_u32(offset, temp1, temp2);
2542   __ ldrb(R3_bytecode, Address(Rbcp, offset, lsl, 0, pre_indexed));
<a name="11" id="anc11"></a><span class="line-modified">2543   __ dispatch_only(vtos);</span>
2544 }
2545 
2546 
2547 void TemplateTable::_return(TosState state) {
2548   transition(state, state);
2549   assert(_desc-&gt;calls_vm(), &quot;inconsistent calls_vm information&quot;); // call in remove_activation
2550 
2551   if (_desc-&gt;bytecode() == Bytecodes::_return_register_finalizer) {
2552     Label skip_register_finalizer;
2553     assert(state == vtos, &quot;only valid state&quot;);
2554     __ ldr(R1, aaddress(0));
2555     __ load_klass(Rtemp, R1);
2556     __ ldr_u32(Rtemp, Address(Rtemp, Klass::access_flags_offset()));
2557     __ tbz(Rtemp, exact_log2(JVM_ACC_HAS_FINALIZER), skip_register_finalizer);
2558 
2559     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::register_finalizer), R1);
2560 
2561     __ bind(skip_register_finalizer);
2562   }
2563 
2564   // Narrow result if state is itos but result type is smaller.
2565   // Need to narrow in the return bytecode rather than in generate_return_entry
2566   // since compiled code callers expect the result to already be narrowed.
2567   if (state == itos) {
2568     __ narrow(R0_tos);
2569   }
2570   __ remove_activation(state, LR);
2571 
2572   __ interp_verify_oop(R0_tos, state, __FILE__, __LINE__);
2573 
2574   // According to interpreter calling conventions, result is returned in R0/R1,
2575   // so ftos (S0) and dtos (D0) are moved to R0/R1.
2576   // This conversion should be done after remove_activation, as it uses
2577   // push(state) &amp; pop(state) to preserve return value.
2578   __ convert_tos_to_retval(state);
2579 
2580   __ ret();
2581 
2582   __ nop(); // to avoid filling CPU pipeline with invalid instructions
2583   __ nop();
2584 }
2585 
2586 
2587 // ----------------------------------------------------------------------------
2588 // Volatile variables demand their effects be made known to all CPU&#39;s in
2589 // order.  Store buffers on most chips allow reads &amp; writes to reorder; the
2590 // JMM&#39;s ReadAfterWrite.java test fails in -Xint mode without some kind of
2591 // memory barrier (i.e., it&#39;s not sufficient that the interpreter does not
2592 // reorder volatile references, the hardware also must not reorder them).
2593 //
2594 // According to the new Java Memory Model (JMM):
2595 // (1) All volatiles are serialized wrt to each other.
2596 // ALSO reads &amp; writes act as aquire &amp; release, so:
2597 // (2) A read cannot let unrelated NON-volatile memory refs that happen after
2598 // the read float up to before the read.  It&#39;s OK for non-volatile memory refs
2599 // that happen before the volatile read to float down below it.
2600 // (3) Similar a volatile write cannot let unrelated NON-volatile memory refs
2601 // that happen BEFORE the write float down to after the write.  It&#39;s OK for
2602 // non-volatile memory refs that happen after the volatile write to float up
2603 // before it.
2604 //
2605 // We only put in barriers around volatile refs (they are expensive), not
2606 // _between_ memory refs (that would require us to track the flavor of the
2607 // previous memory refs).  Requirements (2) and (3) require some barriers
2608 // before volatile stores and after volatile loads.  These nearly cover
2609 // requirement (1) but miss the volatile-store-volatile-load case.  This final
2610 // case is placed after volatile-stores although it could just as well go
2611 // before volatile-loads.
2612 void TemplateTable::volatile_barrier(MacroAssembler::Membar_mask_bits order_constraint,
2613                                      Register tmp,
2614                                      bool preserve_flags,
2615                                      Register load_tgt) {
2616   __ membar(order_constraint, tmp, preserve_flags, load_tgt);
2617 }
2618 
2619 // Blows all volatile registers: R0-R3, Rtemp, LR.
2620 void TemplateTable::resolve_cache_and_index(int byte_no,
2621                                             Register Rcache,
2622                                             Register Rindex,
2623                                             size_t index_size) {
2624   assert_different_registers(Rcache, Rindex, Rtemp);
2625 
2626   Label resolved;
2627   Bytecodes::Code code = bytecode();
2628   switch (code) {
2629   case Bytecodes::_nofast_getfield: code = Bytecodes::_getfield; break;
2630   case Bytecodes::_nofast_putfield: code = Bytecodes::_putfield; break;
2631   default: break;
2632   }
2633 
2634   assert(byte_no == f1_byte || byte_no == f2_byte, &quot;byte_no out of range&quot;);
2635   __ get_cache_and_index_and_bytecode_at_bcp(Rcache, Rindex, Rtemp, byte_no, 1, index_size);
2636   __ cmp(Rtemp, code);  // have we resolved this bytecode?
2637   __ b(resolved, eq);
2638 
2639   // resolve first time through
2640   address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache);
2641   __ mov(R1, code);
2642   __ call_VM(noreg, entry, R1);
2643   // Update registers with resolved info
2644   __ get_cache_and_index_at_bcp(Rcache, Rindex, 1, index_size);
2645   __ bind(resolved);
2646 }
2647 
2648 
2649 // The Rcache and Rindex registers must be set before call
2650 void TemplateTable::load_field_cp_cache_entry(Register Rcache,
2651                                               Register Rindex,
2652                                               Register Roffset,
2653                                               Register Rflags,
2654                                               Register Robj,
2655                                               bool is_static = false) {
2656 
2657   assert_different_registers(Rcache, Rindex, Rtemp);
2658   assert_different_registers(Roffset, Rflags, Robj, Rtemp);
2659 
2660   ByteSize cp_base_offset = ConstantPoolCache::base_offset();
2661 
2662   __ add(Rtemp, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));
2663 
2664   // Field offset
2665   __ ldr(Roffset, Address(Rtemp,
2666            cp_base_offset + ConstantPoolCacheEntry::f2_offset()));
2667 
2668   // Flags
2669   __ ldr_u32(Rflags, Address(Rtemp,
2670            cp_base_offset + ConstantPoolCacheEntry::flags_offset()));
2671 
2672   if (is_static) {
2673     __ ldr(Robj, Address(Rtemp,
2674              cp_base_offset + ConstantPoolCacheEntry::f1_offset()));
2675     const int mirror_offset = in_bytes(Klass::java_mirror_offset());
2676     __ ldr(Robj, Address(Robj, mirror_offset));
2677     __ resolve_oop_handle(Robj);
2678   }
2679 }
2680 
2681 
2682 // Blows all volatile registers: R0-R3, Rtemp, LR.
2683 void TemplateTable::load_invoke_cp_cache_entry(int byte_no,
2684                                                Register method,
2685                                                Register itable_index,
2686                                                Register flags,
2687                                                bool is_invokevirtual,
2688                                                bool is_invokevfinal/*unused*/,
2689                                                bool is_invokedynamic) {
2690   // setup registers
2691   const Register cache = R2_tmp;
2692   const Register index = R3_tmp;
2693   const Register temp_reg = Rtemp;
2694   assert_different_registers(cache, index, temp_reg);
2695   assert_different_registers(method, itable_index, temp_reg);
2696 
2697   // determine constant pool cache field offsets
2698   assert(is_invokevirtual == (byte_no == f2_byte), &quot;is_invokevirtual flag redundant&quot;);
2699   const int method_offset = in_bytes(
2700     ConstantPoolCache::base_offset() +
2701       ((byte_no == f2_byte)
2702        ? ConstantPoolCacheEntry::f2_offset()
2703        : ConstantPoolCacheEntry::f1_offset()
2704       )
2705     );
2706   const int flags_offset = in_bytes(ConstantPoolCache::base_offset() +
2707                                     ConstantPoolCacheEntry::flags_offset());
2708   // access constant pool cache fields
2709   const int index_offset = in_bytes(ConstantPoolCache::base_offset() +
2710                                     ConstantPoolCacheEntry::f2_offset());
2711 
2712   size_t index_size = (is_invokedynamic ? sizeof(u4) : sizeof(u2));
2713   resolve_cache_and_index(byte_no, cache, index, index_size);
2714     __ add(temp_reg, cache, AsmOperand(index, lsl, LogBytesPerWord));
2715     __ ldr(method, Address(temp_reg, method_offset));
2716 
2717   if (itable_index != noreg) {
2718     __ ldr(itable_index, Address(temp_reg, index_offset));
2719   }
2720   __ ldr_u32(flags, Address(temp_reg, flags_offset));
2721 }
2722 
2723 
2724 // The registers cache and index expected to be set before call, and should not be Rtemp.
2725 // Blows volatile registers R0-R3, Rtemp, LR,
2726 // except cache and index registers which are preserved.
2727 void TemplateTable::jvmti_post_field_access(Register Rcache,
2728                                             Register Rindex,
2729                                             bool is_static,
2730                                             bool has_tos) {
2731   assert_different_registers(Rcache, Rindex, Rtemp);
2732 
2733   if (__ can_post_field_access()) {
2734     // Check to see if a field access watch has been set before we take
2735     // the time to call into the VM.
2736 
2737     Label Lcontinue;
2738 
2739     __ ldr_global_s32(Rtemp, (address)JvmtiExport::get_field_access_count_addr());
2740     __ cbz(Rtemp, Lcontinue);
2741 
2742     // cache entry pointer
2743     __ add(R2, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));
2744     __ add(R2, R2, in_bytes(ConstantPoolCache::base_offset()));
2745     if (is_static) {
2746       __ mov(R1, 0);        // NULL object reference
2747     } else {
2748       __ pop(atos);         // Get the object
2749       __ mov(R1, R0_tos);
2750       __ verify_oop(R1);
2751       __ push(atos);        // Restore stack state
2752     }
2753     // R1: object pointer or NULL
2754     // R2: cache entry pointer
2755     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_access),
2756                R1, R2);
2757     __ get_cache_and_index_at_bcp(Rcache, Rindex, 1);
2758 
2759     __ bind(Lcontinue);
2760   }
2761 }
2762 
2763 
2764 void TemplateTable::pop_and_check_object(Register r) {
2765   __ pop_ptr(r);
2766   __ null_check(r, Rtemp);  // for field access must check obj.
2767   __ verify_oop(r);
2768 }
2769 
2770 
2771 void TemplateTable::getfield_or_static(int byte_no, bool is_static, RewriteControl rc) {
2772   transition(vtos, vtos);
2773 
2774   const Register Roffset  = R2_tmp;
2775   const Register Robj     = R3_tmp;
2776   const Register Rcache   = R4_tmp;
2777   const Register Rflagsav = Rtmp_save0;  // R4/R19
2778   const Register Rindex   = R5_tmp;
2779   const Register Rflags   = R5_tmp;
2780 
2781   resolve_cache_and_index(byte_no, Rcache, Rindex, sizeof(u2));
2782   jvmti_post_field_access(Rcache, Rindex, is_static, false);
2783   load_field_cp_cache_entry(Rcache, Rindex, Roffset, Rflags, Robj, is_static);
2784 
2785   __ mov(Rflagsav, Rflags);
2786 
2787   if (!is_static) pop_and_check_object(Robj);
2788 
2789   Label Done, Lint, Ltable, shouldNotReachHere;
2790   Label Lbtos, Lztos, Lctos, Lstos, Litos, Lltos, Lftos, Ldtos, Latos;
2791 
2792   // compute type
2793   __ logical_shift_right(Rflags, Rflags, ConstantPoolCacheEntry::tos_state_shift);
2794   // Make sure we don&#39;t need to mask flags after the above shift
2795   ConstantPoolCacheEntry::verify_tos_state_shift();
2796 
2797   // There are actually two versions of implementation of getfield/getstatic:
2798   //
2799   // 1) Table switch using add(PC,...) instruction (fast_version)
2800   // 2) Table switch using ldr(PC,...) instruction
2801   //
2802   // First version requires fixed size of code block for each case and
2803   // can not be used in RewriteBytecodes and VerifyOops
2804   // modes.
2805 
2806   // Size of fixed size code block for fast_version
2807   const int log_max_block_size = 3;
2808   const int max_block_size = 1 &lt;&lt; log_max_block_size;
2809 
2810   // Decide if fast version is enabled
2811   bool fast_version = (is_static || !RewriteBytecodes) &amp;&amp; !VerifyOops;
2812 
2813   // On 32-bit ARM atos and itos cases can be merged only for fast version, because
2814   // atos requires additional processing in slow version.
2815   bool atos_merged_with_itos = fast_version;
2816 
2817   assert(number_of_states == 10, &quot;number of tos states should be equal to 9&quot;);
2818 
2819   __ cmp(Rflags, itos);
2820   if(atos_merged_with_itos) {
2821     __ cmp(Rflags, atos, ne);
2822   }
2823 
2824   // table switch by type
2825   if(fast_version) {
2826     __ add(PC, PC, AsmOperand(Rflags, lsl, log_max_block_size + Assembler::LogInstructionSize), ne);
2827   } else {
2828     __ ldr(PC, Address(PC, Rflags, lsl, LogBytesPerWord), ne);
2829   }
2830 
2831   // jump to itos/atos case
2832   __ b(Lint);
2833 
2834   // table with addresses for slow version
2835   if (fast_version) {
2836     // nothing to do
2837   } else  {
2838     __ bind(Ltable);
2839     __ emit_address(Lbtos);
2840     __ emit_address(Lztos);
2841     __ emit_address(Lctos);
2842     __ emit_address(Lstos);
2843     __ emit_address(Litos);
2844     __ emit_address(Lltos);
2845     __ emit_address(Lftos);
2846     __ emit_address(Ldtos);
2847     __ emit_address(Latos);
2848   }
2849 
2850 #ifdef ASSERT
2851   int seq = 0;
2852 #endif
2853   // btos
2854   {
2855     assert(btos == seq++, &quot;btos has unexpected value&quot;);
2856     FixedSizeCodeBlock btos_block(_masm, max_block_size, fast_version);
2857     __ bind(Lbtos);
2858     __ access_load_at(T_BYTE, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg);
2859     __ push(btos);
2860     // Rewrite bytecode to be faster
2861     if (!is_static &amp;&amp; rc == may_rewrite) {
2862       patch_bytecode(Bytecodes::_fast_bgetfield, R0_tmp, Rtemp);
2863     }
2864     __ b(Done);
2865   }
2866 
2867   // ztos (same as btos for getfield)
2868   {
2869     assert(ztos == seq++, &quot;btos has unexpected value&quot;);
2870     FixedSizeCodeBlock ztos_block(_masm, max_block_size, fast_version);
2871     __ bind(Lztos);
2872     __ access_load_at(T_BOOLEAN, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg);
2873     __ push(ztos);
2874     // Rewrite bytecode to be faster (use btos fast getfield)
2875     if (!is_static &amp;&amp; rc == may_rewrite) {
2876       patch_bytecode(Bytecodes::_fast_bgetfield, R0_tmp, Rtemp);
2877     }
2878     __ b(Done);
2879   }
2880 
2881   // ctos
2882   {
2883     assert(ctos == seq++, &quot;ctos has unexpected value&quot;);
2884     FixedSizeCodeBlock ctos_block(_masm, max_block_size, fast_version);
2885     __ bind(Lctos);
2886     __ access_load_at(T_CHAR, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg);
2887     __ push(ctos);
2888     if (!is_static &amp;&amp; rc == may_rewrite) {
2889       patch_bytecode(Bytecodes::_fast_cgetfield, R0_tmp, Rtemp);
2890     }
2891     __ b(Done);
2892   }
2893 
2894   // stos
2895   {
2896     assert(stos == seq++, &quot;stos has unexpected value&quot;);
2897     FixedSizeCodeBlock stos_block(_masm, max_block_size, fast_version);
2898     __ bind(Lstos);
2899     __ access_load_at(T_SHORT, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg);
2900     __ push(stos);
2901     if (!is_static &amp;&amp; rc == may_rewrite) {
2902       patch_bytecode(Bytecodes::_fast_sgetfield, R0_tmp, Rtemp);
2903     }
2904     __ b(Done);
2905   }
2906 
2907   // itos
2908   {
2909     assert(itos == seq++, &quot;itos has unexpected value&quot;);
2910     FixedSizeCodeBlock itos_block(_masm, max_block_size, fast_version);
2911     __ bind(Litos);
2912     __ b(shouldNotReachHere);
2913   }
2914 
2915   // ltos
2916   {
2917     assert(ltos == seq++, &quot;ltos has unexpected value&quot;);
2918     FixedSizeCodeBlock ltos_block(_masm, max_block_size, fast_version);
2919     __ bind(Lltos);
2920     __ access_load_at(T_LONG, IN_HEAP, Address(Robj, Roffset), noreg /* ltos */, noreg, noreg, noreg);
2921     __ push(ltos);
2922     if (!is_static &amp;&amp; rc == may_rewrite) {
2923       patch_bytecode(Bytecodes::_fast_lgetfield, R0_tmp, Rtemp);
2924     }
2925     __ b(Done);
2926   }
2927 
2928   // ftos
2929   {
2930     assert(ftos == seq++, &quot;ftos has unexpected value&quot;);
2931     FixedSizeCodeBlock ftos_block(_masm, max_block_size, fast_version);
2932     __ bind(Lftos);
2933     // floats and ints are placed on stack in same way, so
2934     // we can use push(itos) to transfer value without using VFP
2935     __ access_load_at(T_INT, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg);
2936     __ push(itos);
2937     if (!is_static &amp;&amp; rc == may_rewrite) {
2938       patch_bytecode(Bytecodes::_fast_fgetfield, R0_tmp, Rtemp);
2939     }
2940     __ b(Done);
2941   }
2942 
2943   // dtos
2944   {
2945     assert(dtos == seq++, &quot;dtos has unexpected value&quot;);
2946     FixedSizeCodeBlock dtos_block(_masm, max_block_size, fast_version);
2947     __ bind(Ldtos);
2948     // doubles and longs are placed on stack in the same way, so
2949     // we can use push(ltos) to transfer value without using VFP
2950     __ access_load_at(T_LONG, IN_HEAP, Address(Robj, Roffset), noreg /* ltos */, noreg, noreg, noreg);
2951     __ push(ltos);
2952     if (!is_static &amp;&amp; rc == may_rewrite) {
2953       patch_bytecode(Bytecodes::_fast_dgetfield, R0_tmp, Rtemp);
2954     }
2955     __ b(Done);
2956   }
2957 
2958   // atos
2959   {
2960     assert(atos == seq++, &quot;atos has unexpected value&quot;);
2961 
2962     // atos case for slow version on 32-bit ARM
2963     if(!atos_merged_with_itos) {
2964       __ bind(Latos);
2965       do_oop_load(_masm, R0_tos, Address(Robj, Roffset));
2966       __ push(atos);
2967       // Rewrite bytecode to be faster
2968       if (!is_static &amp;&amp; rc == may_rewrite) {
2969         patch_bytecode(Bytecodes::_fast_agetfield, R0_tmp, Rtemp);
2970       }
2971       __ b(Done);
2972     }
2973   }
2974 
2975   assert(vtos == seq++, &quot;vtos has unexpected value&quot;);
2976 
2977   __ bind(shouldNotReachHere);
2978   __ should_not_reach_here();
2979 
2980   // itos and atos cases are frequent so it makes sense to move them out of table switch
2981   // atos case can be merged with itos case (and thus moved out of table switch) on 32-bit ARM, fast version only
2982 
2983   __ bind(Lint);
2984   __ access_load_at(T_INT, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg);
2985   __ push(itos);
2986   // Rewrite bytecode to be faster
2987   if (!is_static &amp;&amp; rc == may_rewrite) {
2988     patch_bytecode(Bytecodes::_fast_igetfield, R0_tmp, Rtemp);
2989   }
2990 
2991   __ bind(Done);
2992 
2993   // Check for volatile field
2994   Label notVolatile;
2995   __ tbz(Rflagsav, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
2996 
2997   volatile_barrier(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), Rtemp);
2998 
2999   __ bind(notVolatile);
3000 }
3001 
3002 void TemplateTable::getfield(int byte_no) {
3003   getfield_or_static(byte_no, false);
3004 }
3005 
3006 void TemplateTable::nofast_getfield(int byte_no) {
3007   getfield_or_static(byte_no, false, may_not_rewrite);
3008 }
3009 
3010 void TemplateTable::getstatic(int byte_no) {
3011   getfield_or_static(byte_no, true);
3012 }
3013 
3014 
3015 // The registers cache and index expected to be set before call, and should not be R1 or Rtemp.
3016 // Blows volatile registers R0-R3, Rtemp, LR,
3017 // except cache and index registers which are preserved.
3018 void TemplateTable::jvmti_post_field_mod(Register Rcache, Register Rindex, bool is_static) {
3019   ByteSize cp_base_offset = ConstantPoolCache::base_offset();
3020   assert_different_registers(Rcache, Rindex, R1, Rtemp);
3021 
3022   if (__ can_post_field_modification()) {
3023     // Check to see if a field modification watch has been set before we take
3024     // the time to call into the VM.
3025     Label Lcontinue;
3026 
3027     __ ldr_global_s32(Rtemp, (address)JvmtiExport::get_field_modification_count_addr());
3028     __ cbz(Rtemp, Lcontinue);
3029 
3030     if (is_static) {
3031       // Life is simple.  Null out the object pointer.
3032       __ mov(R1, 0);
3033     } else {
3034       // Life is harder. The stack holds the value on top, followed by the object.
3035       // We don&#39;t know the size of the value, though; it could be one or two words
3036       // depending on its type. As a result, we must find the type to determine where
3037       // the object is.
3038 
3039       __ add(Rtemp, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));
3040       __ ldr_u32(Rtemp, Address(Rtemp, cp_base_offset + ConstantPoolCacheEntry::flags_offset()));
3041 
3042       __ logical_shift_right(Rtemp, Rtemp, ConstantPoolCacheEntry::tos_state_shift);
3043       // Make sure we don&#39;t need to mask Rtemp after the above shift
3044       ConstantPoolCacheEntry::verify_tos_state_shift();
3045 
3046       __ cmp(Rtemp, ltos);
3047       __ cond_cmp(Rtemp, dtos, ne);
3048       // two word value (ltos/dtos)
3049       __ ldr(R1, Address(SP, Interpreter::expr_offset_in_bytes(2)), eq);
3050 
3051       // one word value (not ltos, dtos)
3052       __ ldr(R1, Address(SP, Interpreter::expr_offset_in_bytes(1)), ne);
3053     }
3054 
3055     // cache entry pointer
3056     __ add(R2, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));
3057     __ add(R2, R2, in_bytes(cp_base_offset));
3058 
3059     // object (tos)
3060     __ mov(R3, Rstack_top);
3061 
3062     // R1: object pointer set up above (NULL if static)
3063     // R2: cache entry pointer
3064     // R3: value object on the stack
3065     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_modification),
3066                R1, R2, R3);
3067     __ get_cache_and_index_at_bcp(Rcache, Rindex, 1);
3068 
3069     __ bind(Lcontinue);
3070   }
3071 }
3072 
3073 
3074 void TemplateTable::putfield_or_static(int byte_no, bool is_static, RewriteControl rc) {
3075   transition(vtos, vtos);
3076 
3077   const Register Roffset  = R2_tmp;
3078   const Register Robj     = R3_tmp;
3079   const Register Rcache   = R4_tmp;
3080   const Register Rflagsav = Rtmp_save0;  // R4/R19
3081   const Register Rindex   = R5_tmp;
3082   const Register Rflags   = R5_tmp;
3083 
3084   resolve_cache_and_index(byte_no, Rcache, Rindex, sizeof(u2));
3085   jvmti_post_field_mod(Rcache, Rindex, is_static);
3086   load_field_cp_cache_entry(Rcache, Rindex, Roffset, Rflags, Robj, is_static);
3087 
3088   // Check for volatile field
3089   Label notVolatile;
3090   __ mov(Rflagsav, Rflags);
3091   __ tbz(Rflagsav, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3092 
3093   volatile_barrier(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);
3094 
3095   __ bind(notVolatile);
3096 
3097   Label Done, Lint, shouldNotReachHere;
3098   Label Ltable, Lbtos, Lztos, Lctos, Lstos, Litos, Lltos, Lftos, Ldtos, Latos;
3099 
3100   // compute type
3101   __ logical_shift_right(Rflags, Rflags, ConstantPoolCacheEntry::tos_state_shift);
3102   // Make sure we don&#39;t need to mask flags after the above shift
3103   ConstantPoolCacheEntry::verify_tos_state_shift();
3104 
3105   // There are actually two versions of implementation of putfield/putstatic:
3106   //
3107   // 32-bit ARM:
3108   // 1) Table switch using add(PC,...) instruction (fast_version)
3109   // 2) Table switch using ldr(PC,...) instruction
3110   //
3111   // First version requires fixed size of code block for each case and
3112   // can not be used in RewriteBytecodes and VerifyOops
3113   // modes.
3114 
3115   // Size of fixed size code block for fast_version (in instructions)
3116   const int log_max_block_size = 3;
3117   const int max_block_size = 1 &lt;&lt; log_max_block_size;
3118 
3119   // Decide if fast version is enabled
3120   bool fast_version = (is_static || !RewriteBytecodes) &amp;&amp; !VerifyOops;
3121 
3122   assert(number_of_states == 10, &quot;number of tos states should be equal to 9&quot;);
3123 
3124   // itos case is frequent and is moved outside table switch
3125   __ cmp(Rflags, itos);
3126 
3127   // table switch by type
3128   if (fast_version) {
3129     __ add(PC, PC, AsmOperand(Rflags, lsl, log_max_block_size + Assembler::LogInstructionSize), ne);
3130   } else  {
3131     __ ldr(PC, Address(PC, Rflags, lsl, LogBytesPerWord), ne);
3132   }
3133 
3134   // jump to itos case
3135   __ b(Lint);
3136 
3137   // table with addresses for slow version
3138   if (fast_version) {
3139     // nothing to do
3140   } else  {
3141     __ bind(Ltable);
3142     __ emit_address(Lbtos);
3143     __ emit_address(Lztos);
3144     __ emit_address(Lctos);
3145     __ emit_address(Lstos);
3146     __ emit_address(Litos);
3147     __ emit_address(Lltos);
3148     __ emit_address(Lftos);
3149     __ emit_address(Ldtos);
3150     __ emit_address(Latos);
3151   }
3152 
3153 #ifdef ASSERT
3154   int seq = 0;
3155 #endif
3156   // btos
3157   {
3158     assert(btos == seq++, &quot;btos has unexpected value&quot;);
3159     FixedSizeCodeBlock btos_block(_masm, max_block_size, fast_version);
3160     __ bind(Lbtos);
3161     __ pop(btos);
3162     if (!is_static) pop_and_check_object(Robj);
3163     __ access_store_at(T_BYTE, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg, false);
3164     if (!is_static &amp;&amp; rc == may_rewrite) {
3165       patch_bytecode(Bytecodes::_fast_bputfield, R0_tmp, Rtemp, true, byte_no);
3166     }
3167     __ b(Done);
3168   }
3169 
3170   // ztos
3171   {
3172     assert(ztos == seq++, &quot;ztos has unexpected value&quot;);
3173     FixedSizeCodeBlock ztos_block(_masm, max_block_size, fast_version);
3174     __ bind(Lztos);
3175     __ pop(ztos);
3176     if (!is_static) pop_and_check_object(Robj);
3177     __ access_store_at(T_BOOLEAN, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg, false);
3178     if (!is_static &amp;&amp; rc == may_rewrite) {
3179       patch_bytecode(Bytecodes::_fast_zputfield, R0_tmp, Rtemp, true, byte_no);
3180     }
3181     __ b(Done);
3182   }
3183 
3184   // ctos
3185   {
3186     assert(ctos == seq++, &quot;ctos has unexpected value&quot;);
3187     FixedSizeCodeBlock ctos_block(_masm, max_block_size, fast_version);
3188     __ bind(Lctos);
3189     __ pop(ctos);
3190     if (!is_static) pop_and_check_object(Robj);
3191     __ access_store_at(T_CHAR, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg, false);
3192     if (!is_static &amp;&amp; rc == may_rewrite) {
3193       patch_bytecode(Bytecodes::_fast_cputfield, R0_tmp, Rtemp, true, byte_no);
3194     }
3195     __ b(Done);
3196   }
3197 
3198   // stos
3199   {
3200     assert(stos == seq++, &quot;stos has unexpected value&quot;);
3201     FixedSizeCodeBlock stos_block(_masm, max_block_size, fast_version);
3202     __ bind(Lstos);
3203     __ pop(stos);
3204     if (!is_static) pop_and_check_object(Robj);
3205     __ access_store_at(T_SHORT, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg, false);
3206     if (!is_static &amp;&amp; rc == may_rewrite) {
3207       patch_bytecode(Bytecodes::_fast_sputfield, R0_tmp, Rtemp, true, byte_no);
3208     }
3209     __ b(Done);
3210   }
3211 
3212   // itos
3213   {
3214     assert(itos == seq++, &quot;itos has unexpected value&quot;);
3215     FixedSizeCodeBlock itos_block(_masm, max_block_size, fast_version);
3216     __ bind(Litos);
3217     __ b(shouldNotReachHere);
3218   }
3219 
3220   // ltos
3221   {
3222     assert(ltos == seq++, &quot;ltos has unexpected value&quot;);
3223     FixedSizeCodeBlock ltos_block(_masm, max_block_size, fast_version);
3224     __ bind(Lltos);
3225     __ pop(ltos);
3226     if (!is_static) pop_and_check_object(Robj);
3227     __ access_store_at(T_LONG, IN_HEAP, Address(Robj, Roffset), noreg /* ltos */, noreg, noreg, noreg, false);
3228     if (!is_static &amp;&amp; rc == may_rewrite) {
3229       patch_bytecode(Bytecodes::_fast_lputfield, R0_tmp, Rtemp, true, byte_no);
3230     }
3231     __ b(Done);
3232   }
3233 
3234   // ftos
3235   {
3236     assert(ftos == seq++, &quot;ftos has unexpected value&quot;);
3237     FixedSizeCodeBlock ftos_block(_masm, max_block_size, fast_version);
3238     __ bind(Lftos);
3239     // floats and ints are placed on stack in the same way, so
3240     // we can use pop(itos) to transfer value without using VFP
3241     __ pop(itos);
3242     if (!is_static) pop_and_check_object(Robj);
3243     __ access_store_at(T_INT, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg, false);
3244     if (!is_static &amp;&amp; rc == may_rewrite) {
3245       patch_bytecode(Bytecodes::_fast_fputfield, R0_tmp, Rtemp, true, byte_no);
3246     }
3247     __ b(Done);
3248   }
3249 
3250   // dtos
3251   {
3252     assert(dtos == seq++, &quot;dtos has unexpected value&quot;);
3253     FixedSizeCodeBlock dtos_block(_masm, max_block_size, fast_version);
3254     __ bind(Ldtos);
3255     // doubles and longs are placed on stack in the same way, so
3256     // we can use pop(ltos) to transfer value without using VFP
3257     __ pop(ltos);
3258     if (!is_static) pop_and_check_object(Robj);
3259     __ access_store_at(T_LONG, IN_HEAP, Address(Robj, Roffset), noreg /* ltos */, noreg, noreg, noreg, false);
3260     if (!is_static &amp;&amp; rc == may_rewrite) {
3261       patch_bytecode(Bytecodes::_fast_dputfield, R0_tmp, Rtemp, true, byte_no);
3262     }
3263     __ b(Done);
3264   }
3265 
3266   // atos
3267   {
3268     assert(atos == seq++, &quot;dtos has unexpected value&quot;);
3269     __ bind(Latos);
3270     __ pop(atos);
3271     if (!is_static) pop_and_check_object(Robj);
3272     // Store into the field
3273     do_oop_store(_masm, Address(Robj, Roffset), R0_tos, Rtemp, R1_tmp, R5_tmp, false);
3274     if (!is_static &amp;&amp; rc == may_rewrite) {
3275       patch_bytecode(Bytecodes::_fast_aputfield, R0_tmp, Rtemp, true, byte_no);
3276     }
3277     __ b(Done);
3278   }
3279 
3280   __ bind(shouldNotReachHere);
3281   __ should_not_reach_here();
3282 
3283   // itos case is frequent and is moved outside table switch
3284   __ bind(Lint);
3285   __ pop(itos);
3286   if (!is_static) pop_and_check_object(Robj);
3287   __ access_store_at(T_INT, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg, false);
3288   if (!is_static &amp;&amp; rc == may_rewrite) {
3289     patch_bytecode(Bytecodes::_fast_iputfield, R0_tmp, Rtemp, true, byte_no);
3290   }
3291 
3292   __ bind(Done);
3293 
3294   Label notVolatile2;
3295   if (is_static) {
3296     // Just check for volatile. Memory barrier for static final field
3297     // is handled by class initialization.
3298     __ tbz(Rflagsav, ConstantPoolCacheEntry::is_volatile_shift, notVolatile2);
3299     volatile_barrier(MacroAssembler::StoreLoad, Rtemp);
3300     __ bind(notVolatile2);
3301   } else {
3302     // Check for volatile field and final field
3303     Label skipMembar;
3304 
3305     __ tst(Rflagsav, 1 &lt;&lt; ConstantPoolCacheEntry::is_volatile_shift |
3306            1 &lt;&lt; ConstantPoolCacheEntry::is_final_shift);
3307     __ b(skipMembar, eq);
3308 
3309     __ tbz(Rflagsav, ConstantPoolCacheEntry::is_volatile_shift, notVolatile2);
3310 
3311     // StoreLoad barrier after volatile field write
3312     volatile_barrier(MacroAssembler::StoreLoad, Rtemp);
3313     __ b(skipMembar);
3314 
3315     // StoreStore barrier after final field write
3316     __ bind(notVolatile2);
3317     volatile_barrier(MacroAssembler::StoreStore, Rtemp);
3318 
3319     __ bind(skipMembar);
3320   }
3321 }
3322 
3323 void TemplateTable::putfield(int byte_no) {
3324   putfield_or_static(byte_no, false);
3325 }
3326 
3327 void TemplateTable::nofast_putfield(int byte_no) {
3328   putfield_or_static(byte_no, false, may_not_rewrite);
3329 }
3330 
3331 void TemplateTable::putstatic(int byte_no) {
3332   putfield_or_static(byte_no, true);
3333 }
3334 
3335 
3336 void TemplateTable::jvmti_post_fast_field_mod() {
3337   // This version of jvmti_post_fast_field_mod() is not used on ARM
3338   Unimplemented();
3339 }
3340 
3341 // Blows volatile registers R0-R3, Rtemp, LR,
3342 // but preserves tosca with the given state.
3343 void TemplateTable::jvmti_post_fast_field_mod(TosState state) {
3344   if (__ can_post_field_modification()) {
3345     // Check to see if a field modification watch has been set before we take
3346     // the time to call into the VM.
3347     Label done;
3348 
3349     __ ldr_global_s32(R2, (address)JvmtiExport::get_field_modification_count_addr());
3350     __ cbz(R2, done);
3351 
3352     __ pop_ptr(R3);               // copy the object pointer from tos
3353     __ verify_oop(R3);
3354     __ push_ptr(R3);              // put the object pointer back on tos
3355 
3356     __ push(state);               // save value on the stack
3357 
3358     // access constant pool cache entry
3359     __ get_cache_entry_pointer_at_bcp(R2, R1, 1);
3360 
3361     __ mov(R1, R3);
3362     assert(Interpreter::expr_offset_in_bytes(0) == 0, &quot;adjust this code&quot;);
3363     __ mov(R3, Rstack_top); // put tos addr into R3
3364 
3365     // R1: object pointer copied above
3366     // R2: cache entry pointer
3367     // R3: jvalue object on the stack
3368     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_modification), R1, R2, R3);
3369 
3370     __ pop(state);                // restore value
3371 
3372     __ bind(done);
3373   }
3374 }
3375 
3376 
3377 void TemplateTable::fast_storefield(TosState state) {
3378   transition(state, vtos);
3379 
3380   ByteSize base = ConstantPoolCache::base_offset();
3381 
3382   jvmti_post_fast_field_mod(state);
3383 
3384   const Register Rcache  = R2_tmp;
3385   const Register Rindex  = R3_tmp;
3386   const Register Roffset = R3_tmp;
3387   const Register Rflags  = Rtmp_save0; // R4/R19
3388   const Register Robj    = R5_tmp;
3389 
3390   // access constant pool cache
3391   __ get_cache_and_index_at_bcp(Rcache, Rindex, 1);
3392 
3393   __ add(Rcache, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));
3394 
3395   // load flags to test volatile
3396   __ ldr_u32(Rflags, Address(Rcache, base + ConstantPoolCacheEntry::flags_offset()));
3397 
3398   // replace index with field offset from cache entry
3399   __ ldr(Roffset, Address(Rcache, base + ConstantPoolCacheEntry::f2_offset()));
3400 
3401   // Check for volatile store
3402   Label notVolatile;
3403   __ tbz(Rflags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3404 
3405   volatile_barrier(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);
3406 
3407   __ bind(notVolatile);
3408 
3409   // Get object from stack
3410   pop_and_check_object(Robj);
3411 
3412   Address addr = Address(Robj, Roffset);
3413   // access field
3414   switch (bytecode()) {
3415     case Bytecodes::_fast_zputfield:
3416       __ access_store_at(T_BOOLEAN, IN_HEAP, addr, R0_tos, noreg, noreg, noreg, false);
3417       break;
3418     case Bytecodes::_fast_bputfield:
3419       __ access_store_at(T_BYTE, IN_HEAP, addr, R0_tos, noreg, noreg, noreg, false);
3420       break;
3421     case Bytecodes::_fast_sputfield:
3422       __ access_store_at(T_SHORT, IN_HEAP, addr, R0_tos, noreg, noreg, noreg, false);
3423       break;
3424     case Bytecodes::_fast_cputfield:
3425       __ access_store_at(T_CHAR, IN_HEAP, addr, R0_tos, noreg, noreg, noreg,false);
3426       break;
3427     case Bytecodes::_fast_iputfield:
3428       __ access_store_at(T_INT, IN_HEAP, addr, R0_tos, noreg, noreg, noreg, false);
3429       break;
3430     case Bytecodes::_fast_lputfield:
3431       __ access_store_at(T_LONG, IN_HEAP, addr, noreg, noreg, noreg, noreg, false);
3432       break;
3433     case Bytecodes::_fast_fputfield:
3434       __ access_store_at(T_FLOAT, IN_HEAP, addr, noreg, noreg, noreg, noreg, false);
3435       break;
3436     case Bytecodes::_fast_dputfield:
3437       __ access_store_at(T_DOUBLE, IN_HEAP, addr, noreg, noreg, noreg, noreg, false);
3438       break;
3439     case Bytecodes::_fast_aputfield:
3440       do_oop_store(_masm, addr, R0_tos, Rtemp, R1_tmp, R2_tmp, false);
3441       break;
3442 
3443     default:
3444       ShouldNotReachHere();
3445   }
3446 
3447   Label notVolatile2;
3448   Label skipMembar;
3449   __ tst(Rflags, 1 &lt;&lt; ConstantPoolCacheEntry::is_volatile_shift |
3450          1 &lt;&lt; ConstantPoolCacheEntry::is_final_shift);
3451   __ b(skipMembar, eq);
3452 
3453   __ tbz(Rflags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile2);
3454 
3455   // StoreLoad barrier after volatile field write
3456   volatile_barrier(MacroAssembler::StoreLoad, Rtemp);
3457   __ b(skipMembar);
3458 
3459   // StoreStore barrier after final field write
3460   __ bind(notVolatile2);
3461   volatile_barrier(MacroAssembler::StoreStore, Rtemp);
3462 
3463   __ bind(skipMembar);
3464 }
3465 
3466 void TemplateTable::fast_accessfield(TosState state) {
3467   transition(atos, state);
3468 
3469   // do the JVMTI work here to avoid disturbing the register state below
3470   if (__ can_post_field_access()) {
3471     // Check to see if a field access watch has been set before we take
3472     // the time to call into the VM.
3473     Label done;
3474     __ ldr_global_s32(R2, (address) JvmtiExport::get_field_access_count_addr());
3475     __ cbz(R2, done);
3476     // access constant pool cache entry
3477     __ get_cache_entry_pointer_at_bcp(R2, R1, 1);
3478     __ push_ptr(R0_tos);  // save object pointer before call_VM() clobbers it
3479     __ verify_oop(R0_tos);
3480     __ mov(R1, R0_tos);
3481     // R1: object pointer copied above
3482     // R2: cache entry pointer
3483     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_access), R1, R2);
3484     __ pop_ptr(R0_tos);   // restore object pointer
3485 
3486     __ bind(done);
3487   }
3488 
3489   const Register Robj    = R0_tos;
3490   const Register Rcache  = R2_tmp;
3491   const Register Rflags  = R2_tmp;
3492   const Register Rindex  = R3_tmp;
3493   const Register Roffset = R3_tmp;
3494 
3495   // access constant pool cache
3496   __ get_cache_and_index_at_bcp(Rcache, Rindex, 1);
3497   // replace index with field offset from cache entry
3498   __ add(Rtemp, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));
3499   __ ldr(Roffset, Address(Rtemp, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f2_offset()));
3500 
3501   // load flags to test volatile
3502   __ ldr_u32(Rflags, Address(Rtemp, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()));
3503 
3504   __ verify_oop(Robj);
3505   __ null_check(Robj, Rtemp);
3506 
3507   Address addr = Address(Robj, Roffset);
3508   // access field
3509   switch (bytecode()) {
3510     case Bytecodes::_fast_bgetfield:
3511       __ access_load_at(T_BYTE, IN_HEAP, addr, R0_tos, noreg, noreg, noreg);
3512       break;
3513     case Bytecodes::_fast_sgetfield:
3514       __ access_load_at(T_SHORT, IN_HEAP, addr, R0_tos, noreg, noreg, noreg);
3515       break;
3516     case Bytecodes::_fast_cgetfield:
3517       __ access_load_at(T_CHAR, IN_HEAP, addr, R0_tos, noreg, noreg, noreg);
3518       break;
3519     case Bytecodes::_fast_igetfield:
3520       __ access_load_at(T_INT, IN_HEAP, addr, R0_tos, noreg, noreg, noreg);
3521       break;
3522     case Bytecodes::_fast_lgetfield:
3523       __ access_load_at(T_LONG, IN_HEAP, addr, noreg, noreg, noreg, noreg);
3524       break;
3525     case Bytecodes::_fast_fgetfield:
3526       __ access_load_at(T_FLOAT, IN_HEAP, addr, noreg, noreg, noreg, noreg);
3527       break;
3528     case Bytecodes::_fast_dgetfield:
3529       __ access_load_at(T_DOUBLE, IN_HEAP, addr, noreg, noreg, noreg, noreg);
3530       break;
3531     case Bytecodes::_fast_agetfield:
3532       do_oop_load(_masm, R0_tos, addr);
3533       __ verify_oop(R0_tos);
3534       break;
3535     default:
3536       ShouldNotReachHere();
3537   }
3538 
3539   // Check for volatile load
3540   Label notVolatile;
3541   __ tbz(Rflags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3542 
3543   volatile_barrier(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), Rtemp);
3544 
3545   __ bind(notVolatile);
3546 }
3547 
3548 
3549 void TemplateTable::fast_xaccess(TosState state) {
3550   transition(vtos, state);
3551 
3552   const Register Robj = R1_tmp;
3553   const Register Rcache = R2_tmp;
3554   const Register Rindex = R3_tmp;
3555   const Register Roffset = R3_tmp;
3556   const Register Rflags = R4_tmp;
3557   Label done;
3558 
3559   // get receiver
3560   __ ldr(Robj, aaddress(0));
3561 
3562   // access constant pool cache
3563   __ get_cache_and_index_at_bcp(Rcache, Rindex, 2);
3564   __ add(Rtemp, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));
3565   __ ldr(Roffset, Address(Rtemp, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f2_offset()));
3566 
3567   // load flags to test volatile
3568   __ ldr_u32(Rflags, Address(Rtemp, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()));
3569 
3570   // make sure exception is reported in correct bcp range (getfield is next instruction)
3571   __ add(Rbcp, Rbcp, 1);
3572   __ null_check(Robj, Rtemp);
3573   __ sub(Rbcp, Rbcp, 1);
3574 
3575 
3576   if (state == itos) {
3577     __ access_load_at(T_INT, IN_HEAP, Address(Robj, Roffset), R0_tos, noreg, noreg, noreg);
3578   } else if (state == atos) {
3579     do_oop_load(_masm, R0_tos, Address(Robj, Roffset));
3580     __ verify_oop(R0_tos);
3581   } else if (state == ftos) {
3582 #ifdef __SOFTFP__
3583     __ ldr(R0_tos, Address(Robj, Roffset));
3584 #else
3585     __ access_load_at(T_FLOAT, IN_HEAP, Address(Robj, Roffset), noreg /* ftos */, noreg, noreg, noreg);
3586 #endif // __SOFTFP__
3587   } else {
3588     ShouldNotReachHere();
3589   }
3590 
3591   // Check for volatile load
3592   Label notVolatile;
3593   __ tbz(Rflags, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3594 
3595   volatile_barrier(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), Rtemp);
3596 
3597   __ bind(notVolatile);
3598 
3599   __ bind(done);
3600 }
3601 
3602 
3603 
3604 //----------------------------------------------------------------------------------------------------
3605 // Calls
3606 
3607 void TemplateTable::count_calls(Register method, Register temp) {
3608   // implemented elsewhere
3609   ShouldNotReachHere();
3610 }
3611 
3612 
3613 void TemplateTable::prepare_invoke(int byte_no,
3614                                    Register method,  // linked method (or i-klass)
3615                                    Register index,   // itable index, MethodType, etc.
3616                                    Register recv,    // if caller wants to see it
3617                                    Register flags    // if caller wants to test it
3618                                    ) {
3619   // determine flags
3620   const Bytecodes::Code code = bytecode();
3621   const bool is_invokeinterface  = code == Bytecodes::_invokeinterface;
3622   const bool is_invokedynamic    = code == Bytecodes::_invokedynamic;
3623   const bool is_invokehandle     = code == Bytecodes::_invokehandle;
3624   const bool is_invokevirtual    = code == Bytecodes::_invokevirtual;
3625   const bool is_invokespecial    = code == Bytecodes::_invokespecial;
3626   const bool load_receiver       = (recv != noreg);
3627   assert(load_receiver == (code != Bytecodes::_invokestatic &amp;&amp; code != Bytecodes::_invokedynamic), &quot;&quot;);
3628   assert(recv  == noreg || recv  == R2, &quot;&quot;);
3629   assert(flags == noreg || flags == R3, &quot;&quot;);
3630 
3631   // setup registers &amp; access constant pool cache
3632   if (recv  == noreg)  recv  = R2;
3633   if (flags == noreg)  flags = R3;
3634   const Register temp = Rtemp;
3635   const Register ret_type = R1_tmp;
3636   assert_different_registers(method, index, flags, recv, LR, ret_type, temp);
3637 
3638   // save &#39;interpreter return address&#39;
3639   __ save_bcp();
3640 
3641   load_invoke_cp_cache_entry(byte_no, method, index, flags, is_invokevirtual, false, is_invokedynamic);
3642 
3643   // maybe push extra argument
3644   if (is_invokedynamic || is_invokehandle) {
3645     Label L_no_push;
3646     __ tbz(flags, ConstantPoolCacheEntry::has_appendix_shift, L_no_push);
3647     __ mov(temp, index);
3648     __ load_resolved_reference_at_index(index, temp);
3649     __ verify_oop(index);
3650     __ push_ptr(index);  // push appendix (MethodType, CallSite, etc.)
3651     __ bind(L_no_push);
3652   }
3653 
3654   // load receiver if needed (after extra argument is pushed so parameter size is correct)
3655   if (load_receiver) {
3656     __ andr(temp, flags, (uintx)ConstantPoolCacheEntry::parameter_size_mask);  // get parameter size
3657     Address recv_addr = __ receiver_argument_address(Rstack_top, temp, recv);
3658     __ ldr(recv, recv_addr);
3659     __ verify_oop(recv);
3660   }
3661 
3662   // compute return type
3663   __ logical_shift_right(ret_type, flags, ConstantPoolCacheEntry::tos_state_shift);
3664   // Make sure we don&#39;t need to mask flags after the above shift
3665   ConstantPoolCacheEntry::verify_tos_state_shift();
3666   // load return address
3667   { const address table = (address) Interpreter::invoke_return_entry_table_for(code);
3668     __ mov_slow(temp, table);
3669     __ ldr(LR, Address::indexed_ptr(temp, ret_type));
3670   }
3671 }
3672 
3673 
3674 void TemplateTable::invokevirtual_helper(Register index,
3675                                          Register recv,
3676                                          Register flags) {
3677 
3678   const Register recv_klass = R2_tmp;
3679 
3680   assert_different_registers(index, recv, flags, Rtemp);
3681   assert_different_registers(index, recv_klass, R0_tmp, Rtemp);
3682 
3683   // Test for an invoke of a final method
3684   Label notFinal;
3685   __ tbz(flags, ConstantPoolCacheEntry::is_vfinal_shift, notFinal);
3686 
3687   assert(index == Rmethod, &quot;Method* must be Rmethod, for interpreter calling convention&quot;);
3688 
3689   // do the call - the index is actually the method to call
3690 
3691   // It&#39;s final, need a null check here!
3692   __ null_check(recv, Rtemp);
3693 
3694   // profile this call
3695   __ profile_final_call(R0_tmp);
3696 
3697   __ jump_from_interpreted(Rmethod);
3698 
3699   __ bind(notFinal);
3700 
3701   // get receiver klass
3702   __ null_check(recv, Rtemp, oopDesc::klass_offset_in_bytes());
3703   __ load_klass(recv_klass, recv);
3704 
3705   // profile this call
3706   __ profile_virtual_call(R0_tmp, recv_klass);
3707 
3708   // get target Method* &amp; entry point
3709   const int base = in_bytes(Klass::vtable_start_offset());
3710   assert(vtableEntry::size() == 1, &quot;adjust the scaling in the code below&quot;);
3711   __ add(Rtemp, recv_klass, AsmOperand(index, lsl, LogHeapWordSize));
3712   __ ldr(Rmethod, Address(Rtemp, base + vtableEntry::method_offset_in_bytes()));
3713   __ jump_from_interpreted(Rmethod);
3714 }
3715 
3716 void TemplateTable::invokevirtual(int byte_no) {
3717   transition(vtos, vtos);
3718   assert(byte_no == f2_byte, &quot;use this argument&quot;);
3719 
3720   const Register Rrecv  = R2_tmp;
3721   const Register Rflags = R3_tmp;
3722 
3723   prepare_invoke(byte_no, Rmethod, noreg, Rrecv, Rflags);
3724 
3725   // Rmethod: index
3726   // Rrecv:   receiver
3727   // Rflags:  flags
3728   // LR:      return address
3729 
3730   invokevirtual_helper(Rmethod, Rrecv, Rflags);
3731 }
3732 
3733 
3734 void TemplateTable::invokespecial(int byte_no) {
3735   transition(vtos, vtos);
3736   assert(byte_no == f1_byte, &quot;use this argument&quot;);
3737   const Register Rrecv  = R2_tmp;
3738   prepare_invoke(byte_no, Rmethod, noreg, Rrecv);
3739   __ verify_oop(Rrecv);
3740   __ null_check(Rrecv, Rtemp);
3741   // do the call
3742   __ profile_call(Rrecv);
3743   __ jump_from_interpreted(Rmethod);
3744 }
3745 
3746 
3747 void TemplateTable::invokestatic(int byte_no) {
3748   transition(vtos, vtos);
3749   assert(byte_no == f1_byte, &quot;use this argument&quot;);
3750   prepare_invoke(byte_no, Rmethod);
3751   // do the call
3752   __ profile_call(R2_tmp);
3753   __ jump_from_interpreted(Rmethod);
3754 }
3755 
3756 
3757 void TemplateTable::fast_invokevfinal(int byte_no) {
3758   transition(vtos, vtos);
3759   assert(byte_no == f2_byte, &quot;use this argument&quot;);
3760   __ stop(&quot;fast_invokevfinal is not used on ARM&quot;);
3761 }
3762 
3763 
3764 void TemplateTable::invokeinterface(int byte_no) {
3765   transition(vtos, vtos);
3766   assert(byte_no == f1_byte, &quot;use this argument&quot;);
3767 
3768   const Register Ritable = R1_tmp;
3769   const Register Rrecv   = R2_tmp;
3770   const Register Rinterf = R5_tmp;
3771   const Register Rindex  = R4_tmp;
3772   const Register Rflags  = R3_tmp;
3773   const Register Rklass  = R2_tmp; // Note! Same register with Rrecv
3774 
3775   prepare_invoke(byte_no, Rinterf, Rmethod, Rrecv, Rflags);
3776 
3777   // First check for Object case, then private interface method,
3778   // then regular interface method.
3779 
3780   // Special case of invokeinterface called for virtual method of
3781   // java.lang.Object.  See cpCache.cpp for details.
3782   Label notObjectMethod;
3783   __ tbz(Rflags, ConstantPoolCacheEntry::is_forced_virtual_shift, notObjectMethod);
3784   invokevirtual_helper(Rmethod, Rrecv, Rflags);
3785   __ bind(notObjectMethod);
3786 
3787   // Get receiver klass into Rklass - also a null check
3788   __ load_klass(Rklass, Rrecv);
3789 
3790   // Check for private method invocation - indicated by vfinal
3791   Label no_such_interface;
3792 
3793   Label notVFinal;
3794   __ tbz(Rflags, ConstantPoolCacheEntry::is_vfinal_shift, notVFinal);
3795 
3796   Label subtype;
3797   __ check_klass_subtype(Rklass, Rinterf, R1_tmp, R3_tmp, noreg, subtype);
3798   // If we get here the typecheck failed
3799   __ b(no_such_interface);
3800   __ bind(subtype);
3801 
3802   // do the call
3803   __ profile_final_call(R0_tmp);
3804   __ jump_from_interpreted(Rmethod);
3805 
3806   __ bind(notVFinal);
3807 
3808   // Receiver subtype check against REFC.
3809   __ lookup_interface_method(// inputs: rec. class, interface
3810                              Rklass, Rinterf, noreg,
3811                              // outputs:  scan temp. reg1, scan temp. reg2
3812                              noreg, Ritable, Rtemp,
3813                              no_such_interface);
3814 
3815   // profile this call
3816   __ profile_virtual_call(R0_tmp, Rklass);
3817 
3818   // Get declaring interface class from method
3819   __ ldr(Rtemp, Address(Rmethod, Method::const_offset()));
3820   __ ldr(Rtemp, Address(Rtemp, ConstMethod::constants_offset()));
3821   __ ldr(Rinterf, Address(Rtemp, ConstantPool::pool_holder_offset_in_bytes()));
3822 
3823   // Get itable index from method
3824   __ ldr_s32(Rtemp, Address(Rmethod, Method::itable_index_offset()));
3825   __ add(Rtemp, Rtemp, (-Method::itable_index_max)); // small negative constant is too large for an immediate on arm32
3826   __ neg(Rindex, Rtemp);
3827 
3828   __ lookup_interface_method(// inputs: rec. class, interface
3829                              Rklass, Rinterf, Rindex,
3830                              // outputs:  scan temp. reg1, scan temp. reg2
3831                              Rmethod, Ritable, Rtemp,
3832                              no_such_interface);
3833 
3834   // Rmethod: Method* to call
3835 
3836   // Check for abstract method error
3837   // Note: This should be done more efficiently via a throw_abstract_method_error
3838   //       interpreter entry point and a conditional jump to it in case of a null
3839   //       method.
3840   { Label L;
3841     __ cbnz(Rmethod, L);
3842     // throw exception
3843     // note: must restore interpreter registers to canonical
3844     //       state for exception handling to work correctly!
3845     __ restore_method();
3846     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_AbstractMethodError));
3847     // the call_VM checks for exception, so we should never return here.
3848     __ should_not_reach_here();
3849     __ bind(L);
3850   }
3851 
3852   // do the call
3853   __ jump_from_interpreted(Rmethod);
3854 
3855   // throw exception
3856   __ bind(no_such_interface);
3857   __ restore_method();
3858   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_IncompatibleClassChangeError));
3859   // the call_VM checks for exception, so we should never return here.
3860   __ should_not_reach_here();
3861 }
3862 
3863 void TemplateTable::invokehandle(int byte_no) {
3864   transition(vtos, vtos);
3865 
3866   const Register Rrecv  = R2_tmp;
3867   const Register Rmtype = R4_tmp;
3868   const Register R5_method = R5_tmp;  // can&#39;t reuse Rmethod!
3869 
3870   prepare_invoke(byte_no, R5_method, Rmtype, Rrecv);
3871   __ null_check(Rrecv, Rtemp);
3872 
3873   // Rmtype:  MethodType object (from cpool-&gt;resolved_references[f1], if necessary)
3874   // Rmethod: MH.invokeExact_MT method (from f2)
3875 
3876   // Note:  Rmtype is already pushed (if necessary) by prepare_invoke
3877 
3878   // do the call
3879   __ profile_final_call(R3_tmp);  // FIXME: profile the LambdaForm also
3880   __ mov(Rmethod, R5_method);
3881   __ jump_from_interpreted(Rmethod);
3882 }
3883 
3884 void TemplateTable::invokedynamic(int byte_no) {
3885   transition(vtos, vtos);
3886 
3887   const Register Rcallsite = R4_tmp;
3888   const Register R5_method = R5_tmp;  // can&#39;t reuse Rmethod!
3889 
3890   prepare_invoke(byte_no, R5_method, Rcallsite);
3891 
3892   // Rcallsite: CallSite object (from cpool-&gt;resolved_references[f1])
3893   // Rmethod:   MH.linkToCallSite method (from f2)
3894 
3895   // Note:  Rcallsite is already pushed by prepare_invoke
3896 
3897   if (ProfileInterpreter) {
3898     __ profile_call(R2_tmp);
3899   }
3900 
3901   // do the call
3902   __ mov(Rmethod, R5_method);
3903   __ jump_from_interpreted(Rmethod);
3904 }
3905 
3906 //----------------------------------------------------------------------------------------------------
3907 // Allocation
3908 
3909 void TemplateTable::_new() {
3910   transition(vtos, atos);
3911 
3912   const Register Robj   = R0_tos;
3913   const Register Rcpool = R1_tmp;
3914   const Register Rindex = R2_tmp;
3915   const Register Rtags  = R3_tmp;
3916   const Register Rsize  = R3_tmp;
3917 
3918   Register Rklass = R4_tmp;
3919   assert_different_registers(Rcpool, Rindex, Rtags, Rklass, Rtemp);
3920   assert_different_registers(Rcpool, Rindex, Rklass, Rsize);
3921 
3922   Label slow_case;
3923   Label done;
3924   Label initialize_header;
3925   Label initialize_object;  // including clearing the fields
3926 
3927   const bool allow_shared_alloc =
3928     Universe::heap()-&gt;supports_inline_contig_alloc();
3929 
3930   // Literals
3931   InlinedAddress Lheap_top_addr(allow_shared_alloc ? (address)Universe::heap()-&gt;top_addr() : NULL);
3932 
3933   __ get_unsigned_2_byte_index_at_bcp(Rindex, 1);
3934   __ get_cpool_and_tags(Rcpool, Rtags);
3935 
3936   // Make sure the class we&#39;re about to instantiate has been resolved.
3937   // This is done before loading InstanceKlass to be consistent with the order
3938   // how Constant Pool is updated (see ConstantPool::klass_at_put)
3939   const int tags_offset = Array&lt;u1&gt;::base_offset_in_bytes();
3940   __ add(Rtemp, Rtags, Rindex);
3941 
3942   __ ldrb(Rtemp, Address(Rtemp, tags_offset));
3943 
3944   // use Rklass as a scratch
3945   volatile_barrier(MacroAssembler::LoadLoad, Rklass);
3946 
3947   // get InstanceKlass
3948   __ cmp(Rtemp, JVM_CONSTANT_Class);
3949   __ b(slow_case, ne);
3950   __ load_resolved_klass_at_offset(Rcpool, Rindex, Rklass);
3951 
3952   // make sure klass is initialized &amp; doesn&#39;t have finalizer
3953   // make sure klass is fully initialized
3954   __ ldrb(Rtemp, Address(Rklass, InstanceKlass::init_state_offset()));
3955   __ cmp(Rtemp, InstanceKlass::fully_initialized);
3956   __ b(slow_case, ne);
3957 
3958   // get instance_size in InstanceKlass (scaled to a count of bytes)
3959   __ ldr_u32(Rsize, Address(Rklass, Klass::layout_helper_offset()));
3960 
3961   // test to see if it has a finalizer or is malformed in some way
3962   // Klass::_lh_instance_slow_path_bit is really a bit mask, not bit number
3963   __ tbnz(Rsize, exact_log2(Klass::_lh_instance_slow_path_bit), slow_case);
3964 
3965   // Allocate the instance:
3966   //  If TLAB is enabled:
3967   //    Try to allocate in the TLAB.
3968   //    If fails, go to the slow path.
3969   //  Else If inline contiguous allocations are enabled:
3970   //    Try to allocate in eden.
3971   //    If fails due to heap end, go to slow path.
3972   //
3973   //  If TLAB is enabled OR inline contiguous is enabled:
3974   //    Initialize the allocation.
3975   //    Exit.
3976   //
3977   //  Go to slow path.
3978   if (UseTLAB) {
3979     const Register Rtlab_top = R1_tmp;
3980     const Register Rtlab_end = R2_tmp;
3981     assert_different_registers(Robj, Rsize, Rklass, Rtlab_top, Rtlab_end);
3982 
3983     __ tlab_allocate(Robj, Rtlab_top, Rtlab_end, Rsize, slow_case);
3984     if (ZeroTLAB) {
3985       // the fields have been already cleared
3986       __ b(initialize_header);
3987     } else {
3988       // initialize both the header and fields
3989       __ b(initialize_object);
3990     }
3991   } else {
3992     // Allocation in the shared Eden, if allowed.
3993     if (allow_shared_alloc) {
3994       const Register Rheap_top_addr = R2_tmp;
3995       const Register Rheap_top = R5_tmp;
3996       const Register Rheap_end = Rtemp;
3997       assert_different_registers(Robj, Rklass, Rsize, Rheap_top_addr, Rheap_top, Rheap_end, LR);
3998 
3999       __ eden_allocate(Robj, Rheap_top, Rheap_top_addr, Rheap_end, Rsize, slow_case);
4000     }
4001   }
4002 
4003   if (UseTLAB || allow_shared_alloc) {
4004     const Register Rzero0 = R1_tmp;
4005     const Register Rzero1 = R2_tmp;
4006     const Register Rzero_end = R5_tmp;
4007     const Register Rzero_cur = Rtemp;
4008     assert_different_registers(Robj, Rsize, Rklass, Rzero0, Rzero1, Rzero_cur, Rzero_end);
4009 
4010     // The object is initialized before the header.  If the object size is
4011     // zero, go directly to the header initialization.
4012     __ bind(initialize_object);
4013     __ subs(Rsize, Rsize, sizeof(oopDesc));
4014     __ add(Rzero_cur, Robj, sizeof(oopDesc));
4015     __ b(initialize_header, eq);
4016 
4017 #ifdef ASSERT
4018     // make sure Rsize is a multiple of 8
4019     Label L;
4020     __ tst(Rsize, 0x07);
4021     __ b(L, eq);
4022     __ stop(&quot;object size is not multiple of 8 - adjust this code&quot;);
4023     __ bind(L);
4024 #endif
4025 
4026     __ mov(Rzero0, 0);
4027     __ mov(Rzero1, 0);
4028     __ add(Rzero_end, Rzero_cur, Rsize);
4029 
4030     // initialize remaining object fields: Rsize was a multiple of 8
4031     { Label loop;
4032       // loop is unrolled 2 times
4033       __ bind(loop);
4034       // #1
4035       __ stmia(Rzero_cur, RegisterSet(Rzero0) | RegisterSet(Rzero1), writeback);
4036       __ cmp(Rzero_cur, Rzero_end);
4037       // #2
4038       __ stmia(Rzero_cur, RegisterSet(Rzero0) | RegisterSet(Rzero1), writeback, ne);
4039       __ cmp(Rzero_cur, Rzero_end, ne);
4040       __ b(loop, ne);
4041     }
4042 
4043     // initialize object header only.
4044     __ bind(initialize_header);
4045     if (UseBiasedLocking) {
4046       __ ldr(Rtemp, Address(Rklass, Klass::prototype_header_offset()));
4047     } else {
<a name="12" id="anc12"></a><span class="line-modified">4048       __ mov_slow(Rtemp, (intptr_t)markOopDesc::prototype());</span>
4049     }
4050     // mark
4051     __ str(Rtemp, Address(Robj, oopDesc::mark_offset_in_bytes()));
4052 
4053     // klass
4054     __ store_klass(Rklass, Robj); // blows Rklass:
4055     Rklass = noreg;
4056 
4057     // Note: Disable DTrace runtime check for now to eliminate overhead on each allocation
4058     if (DTraceAllocProbes) {
4059       // Trigger dtrace event for fastpath
4060       Label Lcontinue;
4061 
4062       __ ldrb_global(Rtemp, (address)&amp;DTraceAllocProbes);
4063       __ cbz(Rtemp, Lcontinue);
4064 
4065       __ push(atos);
4066       __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), Robj);
4067       __ pop(atos);
4068 
4069       __ bind(Lcontinue);
4070     }
4071 
4072     __ b(done);
4073   } else {
4074     // jump over literals
4075     __ b(slow_case);
4076   }
4077 
4078   if (allow_shared_alloc) {
4079     __ bind_literal(Lheap_top_addr);
4080   }
4081 
4082   // slow case
4083   __ bind(slow_case);
4084   __ get_constant_pool(Rcpool);
4085   __ get_unsigned_2_byte_index_at_bcp(Rindex, 1);
4086   __ call_VM(Robj, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), Rcpool, Rindex);
4087 
4088   // continue
4089   __ bind(done);
4090 
4091   // StoreStore barrier required after complete initialization
4092   // (headers + content zeroing), before the object may escape.
4093   __ membar(MacroAssembler::StoreStore, R1_tmp);
4094 }
4095 
4096 
4097 void TemplateTable::newarray() {
4098   transition(itos, atos);
4099   __ ldrb(R1, at_bcp(1));
4100   __ mov(R2, R0_tos);
4101   call_VM(R0_tos, CAST_FROM_FN_PTR(address, InterpreterRuntime::newarray), R1, R2);
4102   // MacroAssembler::StoreStore useless (included in the runtime exit path)
4103 }
4104 
4105 
4106 void TemplateTable::anewarray() {
4107   transition(itos, atos);
4108   __ get_unsigned_2_byte_index_at_bcp(R2, 1);
4109   __ get_constant_pool(R1);
4110   __ mov(R3, R0_tos);
4111   call_VM(R0_tos, CAST_FROM_FN_PTR(address, InterpreterRuntime::anewarray), R1, R2, R3);
4112   // MacroAssembler::StoreStore useless (included in the runtime exit path)
4113 }
4114 
4115 
4116 void TemplateTable::arraylength() {
4117   transition(atos, itos);
4118   __ null_check(R0_tos, Rtemp, arrayOopDesc::length_offset_in_bytes());
4119   __ ldr_s32(R0_tos, Address(R0_tos, arrayOopDesc::length_offset_in_bytes()));
4120 }
4121 
4122 
4123 void TemplateTable::checkcast() {
4124   transition(atos, atos);
4125   Label done, is_null, quicked, resolved, throw_exception;
4126 
4127   const Register Robj = R0_tos;
4128   const Register Rcpool = R2_tmp;
4129   const Register Rtags = R3_tmp;
4130   const Register Rindex = R4_tmp;
4131   const Register Rsuper = R3_tmp;
4132   const Register Rsub   = R4_tmp;
4133   const Register Rsubtype_check_tmp1 = R1_tmp;
4134   const Register Rsubtype_check_tmp2 = LR_tmp;
4135 
4136   __ cbz(Robj, is_null);
4137 
4138   // Get cpool &amp; tags index
4139   __ get_cpool_and_tags(Rcpool, Rtags);
4140   __ get_unsigned_2_byte_index_at_bcp(Rindex, 1);
4141 
4142   // See if bytecode has already been quicked
4143   __ add(Rtemp, Rtags, Rindex);
4144   __ ldrb(Rtemp, Address(Rtemp, Array&lt;u1&gt;::base_offset_in_bytes()));
4145 
4146   __ cmp(Rtemp, JVM_CONSTANT_Class);
4147 
4148   volatile_barrier(MacroAssembler::LoadLoad, Rtemp, true);
4149 
4150   __ b(quicked, eq);
4151 
4152   __ push(atos);
4153   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::quicken_io_cc));
4154   // vm_result_2 has metadata result
4155   __ get_vm_result_2(Rsuper, Robj);
4156   __ pop_ptr(Robj);
4157   __ b(resolved);
4158 
4159   __ bind(throw_exception);
4160   // Come here on failure of subtype check
4161   __ profile_typecheck_failed(R1_tmp);
4162   __ mov(R2_ClassCastException_obj, Robj);             // convention with generate_ClassCastException_handler()
4163   __ b(Interpreter::_throw_ClassCastException_entry);
4164 
4165   // Get superklass in Rsuper and subklass in Rsub
4166   __ bind(quicked);
4167   __ load_resolved_klass_at_offset(Rcpool, Rindex, Rsuper);
4168 
4169   __ bind(resolved);
4170   __ load_klass(Rsub, Robj);
4171 
4172   // Generate subtype check. Blows both tmps and Rtemp.
4173   assert_different_registers(Robj, Rsub, Rsuper, Rsubtype_check_tmp1, Rsubtype_check_tmp2, Rtemp);
4174   __ gen_subtype_check(Rsub, Rsuper, throw_exception, Rsubtype_check_tmp1, Rsubtype_check_tmp2);
4175 
4176   // Come here on success
4177 
4178   // Collect counts on whether this check-cast sees NULLs a lot or not.
4179   if (ProfileInterpreter) {
4180     __ b(done);
4181     __ bind(is_null);
4182     __ profile_null_seen(R1_tmp);
4183   } else {
4184     __ bind(is_null);   // same as &#39;done&#39;
4185   }
4186   __ bind(done);
4187 }
4188 
4189 
4190 void TemplateTable::instanceof() {
4191   // result = 0: obj == NULL or  obj is not an instanceof the specified klass
4192   // result = 1: obj != NULL and obj is     an instanceof the specified klass
4193 
4194   transition(atos, itos);
4195   Label done, is_null, not_subtype, quicked, resolved;
4196 
4197   const Register Robj = R0_tos;
4198   const Register Rcpool = R2_tmp;
4199   const Register Rtags = R3_tmp;
4200   const Register Rindex = R4_tmp;
4201   const Register Rsuper = R3_tmp;
4202   const Register Rsub   = R4_tmp;
4203   const Register Rsubtype_check_tmp1 = R0_tmp;
4204   const Register Rsubtype_check_tmp2 = R1_tmp;
4205 
4206   __ cbz(Robj, is_null);
4207 
4208   __ load_klass(Rsub, Robj);
4209 
4210   // Get cpool &amp; tags index
4211   __ get_cpool_and_tags(Rcpool, Rtags);
4212   __ get_unsigned_2_byte_index_at_bcp(Rindex, 1);
4213 
4214   // See if bytecode has already been quicked
4215   __ add(Rtemp, Rtags, Rindex);
4216   __ ldrb(Rtemp, Address(Rtemp, Array&lt;u1&gt;::base_offset_in_bytes()));
4217   __ cmp(Rtemp, JVM_CONSTANT_Class);
4218 
4219   volatile_barrier(MacroAssembler::LoadLoad, Rtemp, true);
4220 
4221   __ b(quicked, eq);
4222 
4223   __ push(atos);
4224   call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::quicken_io_cc));
4225   // vm_result_2 has metadata result
4226   __ get_vm_result_2(Rsuper, Robj);
4227   __ pop_ptr(Robj);
4228   __ b(resolved);
4229 
4230   // Get superklass in Rsuper and subklass in Rsub
4231   __ bind(quicked);
4232   __ load_resolved_klass_at_offset(Rcpool, Rindex, Rsuper);
4233 
4234   __ bind(resolved);
4235   __ load_klass(Rsub, Robj);
4236 
4237   // Generate subtype check. Blows both tmps and Rtemp.
4238   __ gen_subtype_check(Rsub, Rsuper, not_subtype, Rsubtype_check_tmp1, Rsubtype_check_tmp2);
4239 
4240   // Come here on success
4241   __ mov(R0_tos, 1);
4242   __ b(done);
4243 
4244   __ bind(not_subtype);
4245   // Come here on failure
4246   __ profile_typecheck_failed(R1_tmp);
4247   __ mov(R0_tos, 0);
4248 
4249   // Collect counts on whether this test sees NULLs a lot or not.
4250   if (ProfileInterpreter) {
4251     __ b(done);
4252     __ bind(is_null);
4253     __ profile_null_seen(R1_tmp);
4254   } else {
4255     __ bind(is_null);   // same as &#39;done&#39;
4256   }
4257   __ bind(done);
4258 }
4259 
4260 
4261 //----------------------------------------------------------------------------------------------------
4262 // Breakpoints
4263 void TemplateTable::_breakpoint() {
4264 
4265   // Note: We get here even if we are single stepping..
4266   // jbug inists on setting breakpoints at every bytecode
4267   // even if we are in single step mode.
4268 
4269   transition(vtos, vtos);
4270 
4271   // get the unpatched byte code
4272   __ mov(R1, Rmethod);
4273   __ mov(R2, Rbcp);
4274   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::get_original_bytecode_at), R1, R2);
4275   __ mov(Rtmp_save0, R0);
4276 
4277   // post the breakpoint event
4278   __ mov(R1, Rmethod);
4279   __ mov(R2, Rbcp);
4280   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::_breakpoint), R1, R2);
4281 
4282   // complete the execution of original bytecode
4283   __ mov(R3_bytecode, Rtmp_save0);
4284   __ dispatch_only_normal(vtos);
4285 }
4286 
4287 
4288 //----------------------------------------------------------------------------------------------------
4289 // Exceptions
4290 
4291 void TemplateTable::athrow() {
4292   transition(atos, vtos);
4293   __ mov(Rexception_obj, R0_tos);
4294   __ null_check(Rexception_obj, Rtemp);
4295   __ b(Interpreter::throw_exception_entry());
4296 }
4297 
4298 
4299 //----------------------------------------------------------------------------------------------------
4300 // Synchronization
4301 //
4302 // Note: monitorenter &amp; exit are symmetric routines; which is reflected
4303 //       in the assembly code structure as well
4304 //
4305 // Stack layout:
4306 //
4307 // [expressions  ] &lt;--- Rstack_top        = expression stack top
4308 // ..
4309 // [expressions  ]
4310 // [monitor entry] &lt;--- monitor block top = expression stack bot
4311 // ..
4312 // [monitor entry]
4313 // [frame data   ] &lt;--- monitor block bot
4314 // ...
4315 // [saved FP     ] &lt;--- FP
4316 
4317 
4318 void TemplateTable::monitorenter() {
4319   transition(atos, vtos);
4320 
4321   const Register Robj = R0_tos;
4322   const Register Rentry = R1_tmp;
4323 
4324   // check for NULL object
4325   __ null_check(Robj, Rtemp);
4326 
4327   __ resolve(IS_NOT_NULL, Robj);
4328 
4329   const int entry_size = (frame::interpreter_frame_monitor_size() * wordSize);
4330   assert (entry_size % StackAlignmentInBytes == 0, &quot;keep stack alignment&quot;);
4331   Label allocate_monitor, allocated;
4332 
4333   // initialize entry pointer
4334   __ mov(Rentry, 0);                             // points to free slot or NULL
4335 
4336   // find a free slot in the monitor block (result in Rentry)
4337   { Label loop, exit;
4338     const Register Rcur = R2_tmp;
4339     const Register Rcur_obj = Rtemp;
4340     const Register Rbottom = R3_tmp;
4341     assert_different_registers(Robj, Rentry, Rcur, Rbottom, Rcur_obj);
4342 
4343     __ ldr(Rcur, Address(FP, frame::interpreter_frame_monitor_block_top_offset * wordSize));
4344                                  // points to current entry, starting with top-most entry
4345     __ sub(Rbottom, FP, -frame::interpreter_frame_monitor_block_bottom_offset * wordSize);
4346                                  // points to word before bottom of monitor block
4347 
4348     __ cmp(Rcur, Rbottom);                       // check if there are no monitors
4349     __ ldr(Rcur_obj, Address(Rcur, BasicObjectLock::obj_offset_in_bytes()), ne);
4350                                                  // prefetch monitor&#39;s object for the first iteration
4351     __ b(allocate_monitor, eq);                  // there are no monitors, skip searching
4352 
4353     __ bind(loop);
4354     __ cmp(Rcur_obj, 0);                         // check if current entry is used
4355     __ mov(Rentry, Rcur, eq);                    // if not used then remember entry
4356 
4357     __ cmp(Rcur_obj, Robj);                      // check if current entry is for same object
4358     __ b(exit, eq);                              // if same object then stop searching
4359 
4360     __ add(Rcur, Rcur, entry_size);              // otherwise advance to next entry
4361 
4362     __ cmp(Rcur, Rbottom);                       // check if bottom reached
4363     __ ldr(Rcur_obj, Address(Rcur, BasicObjectLock::obj_offset_in_bytes()), ne);
4364                                                  // prefetch monitor&#39;s object for the next iteration
4365     __ b(loop, ne);                              // if not at bottom then check this entry
4366     __ bind(exit);
4367   }
4368 
4369   __ cbnz(Rentry, allocated);                    // check if a slot has been found; if found, continue with that one
4370 
4371   __ bind(allocate_monitor);
4372 
4373   // allocate one if there&#39;s no free slot
4374   { Label loop;
4375     assert_different_registers(Robj, Rentry, R2_tmp, Rtemp);
4376 
4377     // 1. compute new pointers
4378 
4379 
4380     __ ldr(Rentry, Address(FP, frame::interpreter_frame_monitor_block_top_offset * wordSize));
4381                                                  // old monitor block top / expression stack bottom
4382 
4383     __ sub(Rstack_top, Rstack_top, entry_size);  // move expression stack top
4384     __ check_stack_top_on_expansion();
4385 
4386     __ sub(Rentry, Rentry, entry_size);          // move expression stack bottom
4387 
4388     __ mov(R2_tmp, Rstack_top);                  // set start value for copy loop
4389 
4390     __ str(Rentry, Address(FP, frame::interpreter_frame_monitor_block_top_offset * wordSize));
4391                                                  // set new monitor block top
4392 
4393     // 2. move expression stack contents
4394 
4395     __ cmp(R2_tmp, Rentry);                                 // check if expression stack is empty
4396     __ ldr(Rtemp, Address(R2_tmp, entry_size), ne);         // load expression stack word from old location
4397     __ b(allocated, eq);
4398 
4399     __ bind(loop);
4400     __ str(Rtemp, Address(R2_tmp, wordSize, post_indexed)); // store expression stack word at new location
4401                                                             // and advance to next word
4402     __ cmp(R2_tmp, Rentry);                                 // check if bottom reached
4403     __ ldr(Rtemp, Address(R2, entry_size), ne);             // load expression stack word from old location
4404     __ b(loop, ne);                                         // if not at bottom then copy next word
4405   }
4406 
4407   // call run-time routine
4408 
4409   // Rentry: points to monitor entry
4410   __ bind(allocated);
4411 
4412   // Increment bcp to point to the next bytecode, so exception handling for async. exceptions work correctly.
4413   // The object has already been poped from the stack, so the expression stack looks correct.
4414   __ add(Rbcp, Rbcp, 1);
4415 
4416   __ str(Robj, Address(Rentry, BasicObjectLock::obj_offset_in_bytes()));     // store object
4417   __ lock_object(Rentry);
4418 
4419   // check to make sure this monitor doesn&#39;t cause stack overflow after locking
4420   __ save_bcp();  // in case of exception
4421   __ arm_stack_overflow_check(0, Rtemp);
4422 
4423   // The bcp has already been incremented. Just need to dispatch to next instruction.
4424   __ dispatch_next(vtos);
4425 }
4426 
4427 
4428 void TemplateTable::monitorexit() {
4429   transition(atos, vtos);
4430 
4431   const Register Robj = R0_tos;
4432   const Register Rcur = R1_tmp;
4433   const Register Rbottom = R2_tmp;
4434   const Register Rcur_obj = Rtemp;
4435 
4436   // check for NULL object
4437   __ null_check(Robj, Rtemp);
4438 
4439   __ resolve(IS_NOT_NULL, Robj);
4440 
4441   const int entry_size = (frame::interpreter_frame_monitor_size() * wordSize);
4442   Label found, throw_exception;
4443 
4444   // find matching slot
4445   { Label loop;
4446     assert_different_registers(Robj, Rcur, Rbottom, Rcur_obj);
4447 
4448     __ ldr(Rcur, Address(FP, frame::interpreter_frame_monitor_block_top_offset * wordSize));
4449                                  // points to current entry, starting with top-most entry
4450     __ sub(Rbottom, FP, -frame::interpreter_frame_monitor_block_bottom_offset * wordSize);
4451                                  // points to word before bottom of monitor block
4452 
4453     __ cmp(Rcur, Rbottom);                       // check if bottom reached
4454     __ ldr(Rcur_obj, Address(Rcur, BasicObjectLock::obj_offset_in_bytes()), ne);
4455                                                  // prefetch monitor&#39;s object for the first iteration
4456     __ b(throw_exception, eq);                   // throw exception if there are now monitors
4457 
4458     __ bind(loop);
4459     // check if current entry is for same object
4460     __ cmp(Rcur_obj, Robj);
4461     __ b(found, eq);                             // if same object then stop searching
4462     __ add(Rcur, Rcur, entry_size);              // otherwise advance to next entry
4463     __ cmp(Rcur, Rbottom);                       // check if bottom reached
4464     __ ldr(Rcur_obj, Address(Rcur, BasicObjectLock::obj_offset_in_bytes()), ne);
4465     __ b (loop, ne);                             // if not at bottom then check this entry
4466   }
4467 
4468   // error handling. Unlocking was not block-structured
4469   __ bind(throw_exception);
4470   __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_illegal_monitor_state_exception));
4471   __ should_not_reach_here();
4472 
4473   // call run-time routine
4474   // Rcur: points to monitor entry
4475   __ bind(found);
4476   __ push_ptr(Robj);                             // make sure object is on stack (contract with oopMaps)
4477   __ unlock_object(Rcur);
4478   __ pop_ptr(Robj);                              // discard object
4479 }
4480 
4481 
4482 //----------------------------------------------------------------------------------------------------
4483 // Wide instructions
4484 
4485 void TemplateTable::wide() {
4486   transition(vtos, vtos);
4487   __ ldrb(R3_bytecode, at_bcp(1));
4488 
4489   InlinedAddress Ltable((address)Interpreter::_wentry_point);
4490   __ ldr_literal(Rtemp, Ltable);
4491   __ indirect_jump(Address::indexed_ptr(Rtemp, R3_bytecode), Rtemp);
4492 
4493   __ nop(); // to avoid filling CPU pipeline with invalid instructions
4494   __ nop();
4495   __ bind_literal(Ltable);
4496 }
4497 
4498 
4499 //----------------------------------------------------------------------------------------------------
4500 // Multi arrays
4501 
4502 void TemplateTable::multianewarray() {
4503   transition(vtos, atos);
4504   __ ldrb(Rtmp_save0, at_bcp(3));   // get number of dimensions
4505 
4506   // last dim is on top of stack; we want address of first one:
4507   // first_addr = last_addr + ndims * stackElementSize - 1*wordsize
4508   // the latter wordSize to point to the beginning of the array.
4509   __ add(Rtemp, Rstack_top, AsmOperand(Rtmp_save0, lsl, Interpreter::logStackElementSize));
4510   __ sub(R1, Rtemp, wordSize);
4511 
4512   call_VM(R0, CAST_FROM_FN_PTR(address, InterpreterRuntime::multianewarray), R1);
4513   __ add(Rstack_top, Rstack_top, AsmOperand(Rtmp_save0, lsl, Interpreter::logStackElementSize));
4514   // MacroAssembler::StoreStore useless (included in the runtime exit path)
4515 }
<a name="13" id="anc13"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="13" type="hidden" />
</body>
</html>