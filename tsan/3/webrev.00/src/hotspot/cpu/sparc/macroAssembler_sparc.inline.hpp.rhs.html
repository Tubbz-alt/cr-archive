<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/sparc/macroAssembler_sparc.inline.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef CPU_SPARC_MACROASSEMBLER_SPARC_INLINE_HPP
 26 #define CPU_SPARC_MACROASSEMBLER_SPARC_INLINE_HPP
 27 
 28 #include &quot;asm/assembler.inline.hpp&quot;
 29 #include &quot;asm/macroAssembler.hpp&quot;
 30 #include &quot;asm/codeBuffer.hpp&quot;
 31 #include &quot;code/codeCache.hpp&quot;
 32 
 33 inline bool Address::is_simm13(int offset) { return Assembler::is_simm13(disp() + offset); }
 34 
 35 
 36 inline int AddressLiteral::low10() const {
 37   return Assembler::low10(value());
 38 }
 39 
 40 
 41 inline void MacroAssembler::pd_patch_instruction(address branch, address target, const char* file, int line) {
 42   jint&amp; stub_inst = *(jint*) branch;
 43   stub_inst = patched_branch(target - branch, stub_inst, 0);
 44 }
 45 
 46 // Use the right loads/stores for the platform
 47 inline void MacroAssembler::ld_ptr( Register s1, Register s2, Register d ) {
 48   Assembler::ldx(s1, s2, d);
 49 }
 50 
 51 inline void MacroAssembler::ld_ptr( Register s1, int simm13a, Register d ) {
 52   Assembler::ldx(s1, simm13a, d);
 53 }
 54 
 55 #ifdef ASSERT
 56 // ByteSize is only a class when ASSERT is defined, otherwise it&#39;s an int.
 57 inline void MacroAssembler::ld_ptr( Register s1, ByteSize simm13a, Register d ) {
 58   ld_ptr(s1, in_bytes(simm13a), d);
 59 }
 60 #endif
 61 
 62 inline void MacroAssembler::ld_ptr( Register s1, RegisterOrConstant s2, Register d ) {
 63   ldx(s1, s2, d);
 64 }
 65 
 66 inline void MacroAssembler::ld_ptr(const Address&amp; a, Register d, int offset) {
 67   ldx(a, d, offset);
 68 }
 69 
 70 inline void MacroAssembler::st_ptr( Register d, Register s1, Register s2 ) {
 71   Assembler::stx(d, s1, s2);
 72 }
 73 
 74 inline void MacroAssembler::st_ptr( Register d, Register s1, int simm13a ) {
 75   Assembler::stx(d, s1, simm13a);
 76 }
 77 
 78 #ifdef ASSERT
 79 // ByteSize is only a class when ASSERT is defined, otherwise it&#39;s an int.
 80 inline void MacroAssembler::st_ptr( Register d, Register s1, ByteSize simm13a ) {
 81   st_ptr(d, s1, in_bytes(simm13a));
 82 }
 83 #endif
 84 
 85 inline void MacroAssembler::st_ptr( Register d, Register s1, RegisterOrConstant s2 ) {
 86   stx(d, s1, s2);
 87 }
 88 
 89 inline void MacroAssembler::st_ptr(Register d, const Address&amp; a, int offset) {
 90   stx(d, a, offset);
 91 }
 92 
 93 // Use the right loads/stores for the platform
 94 inline void MacroAssembler::ld_long( Register s1, Register s2, Register d ) {
 95   Assembler::ldx(s1, s2, d);
 96 }
 97 
 98 inline void MacroAssembler::ld_long( Register s1, int simm13a, Register d ) {
 99   Assembler::ldx(s1, simm13a, d);
100 }
101 
102 inline void MacroAssembler::ld_long( Register s1, RegisterOrConstant s2, Register d ) {
103   ldx(s1, s2, d);
104 }
105 
106 inline void MacroAssembler::ld_long(const Address&amp; a, Register d, int offset) {
107   ldx(a, d, offset);
108 }
109 
110 inline void MacroAssembler::st_long( Register d, Register s1, Register s2 ) {
111   Assembler::stx(d, s1, s2);
112 }
113 
114 inline void MacroAssembler::st_long( Register d, Register s1, int simm13a ) {
115   Assembler::stx(d, s1, simm13a);
116 }
117 
118 inline void MacroAssembler::st_long( Register d, Register s1, RegisterOrConstant s2 ) {
119   stx(d, s1, s2);
120 }
121 
122 inline void MacroAssembler::st_long( Register d, const Address&amp; a, int offset ) {
123   stx(d, a, offset);
124 }
125 
126 inline void MacroAssembler::stbool(Register d, const Address&amp; a) { stb(d, a); }
127 inline void MacroAssembler::ldbool(const Address&amp; a, Register d) { ldub(a, d); }
128 inline void MacroAssembler::movbool( bool boolconst, Register d) { mov( (int) boolconst, d); }
129 
130 
131 inline void MacroAssembler::signx( Register s, Register d ) { sra( s, G0, d); }
132 inline void MacroAssembler::signx( Register d )             { sra( d, G0, d); }
133 
134 inline void MacroAssembler::not1( Register s, Register d ) { xnor( s, G0, d ); }
135 inline void MacroAssembler::not1( Register d )             { xnor( d, G0, d ); }
136 
137 inline void MacroAssembler::neg( Register s, Register d ) { sub( G0, s, d ); }
138 inline void MacroAssembler::neg( Register d )             { sub( G0, d, d ); }
139 
140 inline void MacroAssembler::cas(  Register s1, Register s2, Register d) { casa( s1, s2, d, ASI_PRIMARY); }
141 inline void MacroAssembler::casx( Register s1, Register s2, Register d) { casxa(s1, s2, d, ASI_PRIMARY); }
142 
143 // Functions for isolating 64 bit atomic swaps for LP64
144 // cas_ptr will perform cas for 32 bit VM&#39;s and casx for 64 bit VM&#39;s
145 inline void MacroAssembler::cas_ptr(  Register s1, Register s2, Register d) {
146   casx( s1, s2, d );
147 }
148 
149 // Functions for isolating 64 bit shifts for LP64
150 
151 inline void MacroAssembler::sll_ptr( Register s1, Register s2, Register d ) {
152   Assembler::sllx(s1, s2, d);
153 }
154 
155 inline void MacroAssembler::sll_ptr( Register s1, int imm6a,   Register d ) {
156   Assembler::sllx(s1, imm6a, d);
157 }
158 
159 inline void MacroAssembler::srl_ptr( Register s1, Register s2, Register d ) {
160   Assembler::srlx(s1, s2, d);
161 }
162 
163 inline void MacroAssembler::srl_ptr( Register s1, int imm6a,   Register d ) {
164   Assembler::srlx(s1, imm6a, d);
165 }
166 
167 inline void MacroAssembler::sll_ptr( Register s1, RegisterOrConstant s2, Register d ) {
168   if (s2.is_register())  sll_ptr(s1, s2.as_register(), d);
169   else                   sll_ptr(s1, s2.as_constant(), d);
170 }
171 
172 inline void MacroAssembler::casl(  Register s1, Register s2, Register d) { casa( s1, s2, d, ASI_PRIMARY_LITTLE); }
173 inline void MacroAssembler::casxl( Register s1, Register s2, Register d) { casxa(s1, s2, d, ASI_PRIMARY_LITTLE); }
174 
175 inline void MacroAssembler::inc(   Register d,  int const13 ) { add(   d, const13, d); }
176 inline void MacroAssembler::inccc( Register d,  int const13 ) { addcc( d, const13, d); }
177 
178 inline void MacroAssembler::dec(   Register d,  int const13 ) { sub(   d, const13, d); }
179 inline void MacroAssembler::deccc( Register d,  int const13 ) { subcc( d, const13, d); }
180 
181 // Use the right branch for the platform
182 
183 inline void MacroAssembler::br( Condition c, bool a, Predict p, address d, relocInfo::relocType rt ) {
184   Assembler::bp(c, a, icc, p, d, rt);
185 }
186 
187 inline void MacroAssembler::br( Condition c, bool a, Predict p, Label&amp; L ) {
188   // See note[+] on &#39;avoid_pipeline_stall()&#39;, in &quot;assembler_sparc.inline.hpp&quot;.
189   avoid_pipeline_stall();
190   br(c, a, p, target(L));
191 }
192 
193 
194 // Branch that tests either xcc or icc depending on the
195 // architecture compiled (LP64 or not)
196 inline void MacroAssembler::brx( Condition c, bool a, Predict p, address d, relocInfo::relocType rt ) {
197     Assembler::bp(c, a, xcc, p, d, rt);
198 }
199 
200 inline void MacroAssembler::brx( Condition c, bool a, Predict p, Label&amp; L ) {
201   avoid_pipeline_stall();
202   brx(c, a, p, target(L));
203 }
204 
205 inline void MacroAssembler::ba( Label&amp; L ) {
206   br(always, false, pt, L);
207 }
208 
209 // Warning: V9 only functions
210 inline void MacroAssembler::bp( Condition c, bool a, CC cc, Predict p, address d, relocInfo::relocType rt ) {
211   Assembler::bp(c, a, cc, p, d, rt);
212 }
213 
214 inline void MacroAssembler::bp( Condition c, bool a, CC cc, Predict p, Label&amp; L ) {
215   Assembler::bp(c, a, cc, p, L);
216 }
217 
218 inline void MacroAssembler::fb( Condition c, bool a, Predict p, address d, relocInfo::relocType rt ) {
219   fbp(c, a, fcc0, p, d, rt);
220 }
221 
222 inline void MacroAssembler::fb( Condition c, bool a, Predict p, Label&amp; L ) {
223   avoid_pipeline_stall();
224   fb(c, a, p, target(L));
225 }
226 
227 inline void MacroAssembler::fbp( Condition c, bool a, CC cc, Predict p, address d, relocInfo::relocType rt ) {
228   Assembler::fbp(c, a, cc, p, d, rt);
229 }
230 
231 inline void MacroAssembler::fbp( Condition c, bool a, CC cc, Predict p, Label&amp; L ) {
232   Assembler::fbp(c, a, cc, p, L);
233 }
234 
235 inline void MacroAssembler::jmp( Register s1, Register s2 ) { jmpl( s1, s2, G0 ); }
236 inline void MacroAssembler::jmp( Register s1, int simm13a, RelocationHolder const&amp; rspec ) { jmpl( s1, simm13a, G0, rspec); }
237 
238 inline bool MacroAssembler::is_far_target(address d) {
239   if (ForceUnreachable) {
240     // References outside the code cache should be treated as far
241     return d &lt; CodeCache::low_bound() || d &gt; CodeCache::high_bound();
242   }
243   return !is_in_wdisp30_range(d, CodeCache::low_bound()) || !is_in_wdisp30_range(d, CodeCache::high_bound());
244 }
245 
246 // Call with a check to see if we need to deal with the added
247 // expense of relocation and if we overflow the displacement
248 // of the quick call instruction.
249 inline void MacroAssembler::call( address d, relocInfo::relocType rt ) {
250   MacroAssembler::call(d, Relocation::spec_simple(rt));
251 }
252 
253 inline void MacroAssembler::call( address d, RelocationHolder const&amp; rspec ) {
254   intptr_t disp;
255   // NULL is ok because it will be relocated later.
256   // Must change NULL to a reachable address in order to
257   // pass asserts here and in wdisp.
258   if ( d == NULL )
259     d = pc();
260 
261   // Is this address within range of the call instruction?
262   // If not, use the expensive instruction sequence
263   if (is_far_target(d)) {
264     relocate(rspec);
265     AddressLiteral dest(d);
266     jumpl_to(dest, O7, O7);
267   } else {
268     Assembler::call(d, rspec);
269   }
270 }
271 
272 inline void MacroAssembler::call( Label&amp; L, relocInfo::relocType rt ) {
273   avoid_pipeline_stall();
274   MacroAssembler::call(target(L), rt);
275 }
276 
277 
278 inline void MacroAssembler::callr( Register s1, Register s2 ) { jmpl( s1, s2, O7 ); }
279 inline void MacroAssembler::callr( Register s1, int simm13a, RelocationHolder const&amp; rspec ) { jmpl( s1, simm13a, O7, rspec); }
280 
<a name="1" id="anc1"></a>






281 inline void MacroAssembler::tst( Register s ) { orcc( G0, s, G0 ); }
282 
283 inline void MacroAssembler::ret( bool trace ) {
284   if (trace) {
285     mov(I7, O7); // traceable register
286     JMP(O7, 2 * BytesPerInstWord);
287   } else {
288     jmpl( I7, 2 * BytesPerInstWord, G0 );
289   }
290 }
291 
292 inline void MacroAssembler::retl( bool trace ) {
293   if (trace) {
294     JMP(O7, 2 * BytesPerInstWord);
295   } else {
296     jmpl( O7, 2 * BytesPerInstWord, G0 );
297   }
298 }
299 
300 
301 inline void MacroAssembler::cmp(  Register s1, Register s2 ) { subcc( s1, s2, G0 ); }
302 inline void MacroAssembler::cmp(  Register s1, int simm13a ) { subcc( s1, simm13a, G0 ); }
303 
304 // Note:  All MacroAssembler::set_foo functions are defined out-of-line.
305 
306 
307 // Loads the current PC of the following instruction as an immediate value in
308 // 2 instructions.  All PCs in the CodeCache are within 2 Gig of each other.
309 inline intptr_t MacroAssembler::load_pc_address( Register reg, int bytes_to_skip ) {
310   intptr_t thepc = (intptr_t)pc() + 2*BytesPerInstWord + bytes_to_skip;
311   Unimplemented();
312   return thepc;
313 }
314 
315 
316 inline void MacroAssembler::load_contents(const AddressLiteral&amp; addrlit, Register d, int offset) {
317   assert_not_delayed();
318   if (ForceUnreachable) {
319     patchable_sethi(addrlit, d);
320   } else {
321     sethi(addrlit, d);
322   }
323   ld(d, addrlit.low10() + offset, d);
324 }
325 
326 
327 inline void MacroAssembler::load_bool_contents(const AddressLiteral&amp; addrlit, Register d, int offset) {
328   assert_not_delayed();
329   if (ForceUnreachable) {
330     patchable_sethi(addrlit, d);
331   } else {
332     sethi(addrlit, d);
333   }
334   ldub(d, addrlit.low10() + offset, d);
335 }
336 
337 
338 inline void MacroAssembler::load_ptr_contents(const AddressLiteral&amp; addrlit, Register d, int offset) {
339   assert_not_delayed();
340   if (ForceUnreachable) {
341     patchable_sethi(addrlit, d);
342   } else {
343     sethi(addrlit, d);
344   }
345   ld_ptr(d, addrlit.low10() + offset, d);
346 }
347 
348 
349 inline void MacroAssembler::store_contents(Register s, const AddressLiteral&amp; addrlit, Register temp, int offset) {
350   assert_not_delayed();
351   if (ForceUnreachable) {
352     patchable_sethi(addrlit, temp);
353   } else {
354     sethi(addrlit, temp);
355   }
356   st(s, temp, addrlit.low10() + offset);
357 }
358 
359 
360 inline void MacroAssembler::store_ptr_contents(Register s, const AddressLiteral&amp; addrlit, Register temp, int offset) {
361   assert_not_delayed();
362   if (ForceUnreachable) {
363     patchable_sethi(addrlit, temp);
364   } else {
365     sethi(addrlit, temp);
366   }
367   st_ptr(s, temp, addrlit.low10() + offset);
368 }
369 
370 
371 // This code sequence is relocatable to any address, even on LP64.
372 inline void MacroAssembler::jumpl_to(const AddressLiteral&amp; addrlit, Register temp, Register d, int offset) {
373   assert_not_delayed();
374   // Force fixed length sethi because NativeJump and NativeFarCall don&#39;t handle
375   // variable length instruction streams.
376   patchable_sethi(addrlit, temp);
377   jmpl(temp, addrlit.low10() + offset, d);
378 }
379 
380 
381 inline void MacroAssembler::jump_to(const AddressLiteral&amp; addrlit, Register temp, int offset) {
382   jumpl_to(addrlit, temp, G0, offset);
383 }
384 
385 
386 inline void MacroAssembler::jump_indirect_to(Address&amp; a, Register temp,
387                                              int ld_offset, int jmp_offset) {
388   assert_not_delayed();
389   //sethi(al);                   // sethi is caller responsibility for this one
390   ld_ptr(a, temp, ld_offset);
391   jmp(temp, jmp_offset);
392 }
393 
394 
395 inline void MacroAssembler::set_metadata(Metadata* obj, Register d) {
396   set_metadata(allocate_metadata_address(obj), d);
397 }
398 
399 inline void MacroAssembler::set_metadata_constant(Metadata* obj, Register d) {
400   set_metadata(constant_metadata_address(obj), d);
401 }
402 
403 inline void MacroAssembler::set_metadata(const AddressLiteral&amp; obj_addr, Register d) {
404   assert(obj_addr.rspec().type() == relocInfo::metadata_type, &quot;must be a metadata reloc&quot;);
405   set(obj_addr, d);
406 }
407 
408 inline void MacroAssembler::set_oop(jobject obj, Register d) {
409   set_oop(allocate_oop_address(obj), d);
410 }
411 
412 
413 inline void MacroAssembler::set_oop_constant(jobject obj, Register d) {
414   set_oop(constant_oop_address(obj), d);
415 }
416 
417 
418 inline void MacroAssembler::set_oop(const AddressLiteral&amp; obj_addr, Register d) {
419   assert(obj_addr.rspec().type() == relocInfo::oop_type, &quot;must be an oop reloc&quot;);
420   set(obj_addr, d);
421 }
422 
423 
424 inline void MacroAssembler::load_argument( Argument&amp; a, Register  d ) {
425   if (a.is_register())
426     mov(a.as_register(), d);
427   else
428     ld (a.as_address(),  d);
429 }
430 
431 inline void MacroAssembler::store_argument( Register s, Argument&amp; a ) {
432   if (a.is_register())
433     mov(s, a.as_register());
434   else
435     st_ptr (s, a.as_address());         // ABI says everything is right justified.
436 }
437 
438 inline void MacroAssembler::store_ptr_argument( Register s, Argument&amp; a ) {
439   if (a.is_register())
440     mov(s, a.as_register());
441   else
442     st_ptr (s, a.as_address());
443 }
444 
445 
446 inline void MacroAssembler::store_float_argument( FloatRegister s, Argument&amp; a ) {
447   if (a.is_float_register())
448 // V9 ABI has F1, F3, F5 are used to pass instead of O0, O1, O2
449     fmov(FloatRegisterImpl::S, s, a.as_float_register() );
450   else
451     // Floats are stored in the high half of the stack entry
452     // The low half is undefined per the ABI.
453     stf(FloatRegisterImpl::S, s, a.as_address(), sizeof(jfloat));
454 }
455 
456 inline void MacroAssembler::store_double_argument( FloatRegister s, Argument&amp; a ) {
457   if (a.is_float_register())
458 // V9 ABI has D0, D2, D4 are used to pass instead of O0, O1, O2
459     fmov(FloatRegisterImpl::D, s, a.as_double_register() );
460   else
461     stf(FloatRegisterImpl::D, s, a.as_address());
462 }
463 
464 inline void MacroAssembler::store_long_argument( Register s, Argument&amp; a ) {
465   if (a.is_register())
466     mov(s, a.as_register());
467   else
468     stx(s, a.as_address());
469 }
470 
471 inline void MacroAssembler::round_to( Register r, int modulus ) {
472   assert_not_delayed();
473   inc( r, modulus - 1 );
474   and3( r, -modulus, r );
475 }
476 
477 inline void MacroAssembler::add(Register s1, int simm13a, Register d, relocInfo::relocType rtype) {
478   relocate(rtype);
479   add(s1, simm13a, d);
480 }
481 inline void MacroAssembler::add(Register s1, int simm13a, Register d, RelocationHolder const&amp; rspec) {
482   relocate(rspec);
483   add(s1, simm13a, d);
484 }
485 
486 // form effective addresses this way:
487 inline void MacroAssembler::add(const Address&amp; a, Register d, int offset) {
488   if (a.has_index())   add(a.base(), a.index(),         d);
489   else               { add(a.base(), a.disp() + offset, d, a.rspec(offset)); offset = 0; }
490   if (offset != 0)     add(d,        offset,            d);
491 }
492 inline void MacroAssembler::add(Register s1, RegisterOrConstant s2, Register d, int offset) {
493   if (s2.is_register())  add(s1, s2.as_register(),          d);
494   else                 { add(s1, s2.as_constant() + offset, d); offset = 0; }
495   if (offset != 0)       add(d,  offset,                    d);
496 }
497 
498 inline void MacroAssembler::andn(Register s1, RegisterOrConstant s2, Register d) {
499   if (s2.is_register())  andn(s1, s2.as_register(), d);
500   else                   andn(s1, s2.as_constant(), d);
501 }
502 
503 inline void MacroAssembler::btst( Register s1,  Register s2 ) { andcc( s1, s2, G0 ); }
504 inline void MacroAssembler::btst( int simm13a,  Register s )  { andcc( s,  simm13a, G0 ); }
505 
506 inline void MacroAssembler::bset( Register s1,  Register s2 ) { or3( s1, s2, s2 ); }
507 inline void MacroAssembler::bset( int simm13a,  Register s )  { or3( s,  simm13a, s ); }
508 
509 inline void MacroAssembler::bclr( Register s1,  Register s2 ) { andn( s1, s2, s2 ); }
510 inline void MacroAssembler::bclr( int simm13a,  Register s )  { andn( s,  simm13a, s ); }
511 
512 inline void MacroAssembler::btog( Register s1,  Register s2 ) { xor3( s1, s2, s2 ); }
513 inline void MacroAssembler::btog( int simm13a,  Register s )  { xor3( s,  simm13a, s ); }
514 
515 inline void MacroAssembler::clr( Register d ) { or3( G0, G0, d ); }
516 
517 inline void MacroAssembler::clrb( Register s1, Register s2) { stb( G0, s1, s2 ); }
518 inline void MacroAssembler::clrh( Register s1, Register s2) { sth( G0, s1, s2 ); }
519 inline void MacroAssembler::clr(  Register s1, Register s2) { stw( G0, s1, s2 ); }
520 inline void MacroAssembler::clrx( Register s1, Register s2) { stx( G0, s1, s2 ); }
521 
522 inline void MacroAssembler::clrb( Register s1, int simm13a) { stb( G0, s1, simm13a); }
523 inline void MacroAssembler::clrh( Register s1, int simm13a) { sth( G0, s1, simm13a); }
524 inline void MacroAssembler::clr(  Register s1, int simm13a) { stw( G0, s1, simm13a); }
525 inline void MacroAssembler::clrx( Register s1, int simm13a) { stx( G0, s1, simm13a); }
526 
527 inline void MacroAssembler::clruw( Register s, Register d ) { srl( s, G0, d); }
528 inline void MacroAssembler::clruwu( Register d ) { srl( d, G0, d); }
529 
530 // Make all 32 bit loads signed so 64 bit registers maintain proper sign
531 inline void MacroAssembler::ld(  Register s1, Register s2, Register d)      { ldsw( s1, s2, d); }
532 inline void MacroAssembler::ld(  Register s1, int simm13a, Register d)      { ldsw( s1, simm13a, d); }
533 
534 #ifdef ASSERT
535   // ByteSize is only a class when ASSERT is defined, otherwise it&#39;s an int.
536 inline void MacroAssembler::ld(Register s1, ByteSize simm13a, Register d) { ldsw( s1, in_bytes(simm13a), d); }
537 #endif
538 
539 inline void MacroAssembler::ld(  const Address&amp; a, Register d, int offset) {
540   if (a.has_index()) { assert(offset == 0, &quot;&quot;); ld(  a.base(), a.index(),         d); }
541   else               {                          ld(  a.base(), a.disp() + offset, d); }
542 }
543 
544 inline void MacroAssembler::ldsb(const Address&amp; a, Register d, int offset) {
545   if (a.has_index()) { assert(offset == 0, &quot;&quot;); ldsb(a.base(), a.index(),         d); }
546   else               {                          ldsb(a.base(), a.disp() + offset, d); }
547 }
548 inline void MacroAssembler::ldsh(const Address&amp; a, Register d, int offset) {
549   if (a.has_index()) { assert(offset == 0, &quot;&quot;); ldsh(a.base(), a.index(),         d); }
550   else               {                          ldsh(a.base(), a.disp() + offset, d); }
551 }
552 inline void MacroAssembler::ldsw(const Address&amp; a, Register d, int offset) {
553   if (a.has_index()) { assert(offset == 0, &quot;&quot;); ldsw(a.base(), a.index(),         d); }
554   else               {                          ldsw(a.base(), a.disp() + offset, d); }
555 }
556 inline void MacroAssembler::ldub(const Address&amp; a, Register d, int offset) {
557   if (a.has_index()) { assert(offset == 0, &quot;&quot;); ldub(a.base(), a.index(),         d); }
558   else               {                          ldub(a.base(), a.disp() + offset, d); }
559 }
560 inline void MacroAssembler::lduh(const Address&amp; a, Register d, int offset) {
561   if (a.has_index()) { assert(offset == 0, &quot;&quot;); lduh(a.base(), a.index(),         d); }
562   else               {                          lduh(a.base(), a.disp() + offset, d); }
563 }
564 inline void MacroAssembler::lduw(const Address&amp; a, Register d, int offset) {
565   if (a.has_index()) { assert(offset == 0, &quot;&quot;); lduw(a.base(), a.index(),         d); }
566   else               {                          lduw(a.base(), a.disp() + offset, d); }
567 }
568 inline void MacroAssembler::ldd( const Address&amp; a, Register d, int offset) {
569   if (a.has_index()) { assert(offset == 0, &quot;&quot;); ldd( a.base(), a.index(),         d); }
570   else               {                          ldd( a.base(), a.disp() + offset, d); }
571 }
572 inline void MacroAssembler::ldx( const Address&amp; a, Register d, int offset) {
573   if (a.has_index()) { assert(offset == 0, &quot;&quot;); ldx( a.base(), a.index(),         d); }
574   else               {                          ldx( a.base(), a.disp() + offset, d); }
575 }
576 
577 inline void MacroAssembler::ldub(Register s1, RegisterOrConstant s2, Register d) { ldub(Address(s1, s2), d); }
578 inline void MacroAssembler::ldsb(Register s1, RegisterOrConstant s2, Register d) { ldsb(Address(s1, s2), d); }
579 inline void MacroAssembler::lduh(Register s1, RegisterOrConstant s2, Register d) { lduh(Address(s1, s2), d); }
580 inline void MacroAssembler::ldsh(Register s1, RegisterOrConstant s2, Register d) { ldsh(Address(s1, s2), d); }
581 inline void MacroAssembler::lduw(Register s1, RegisterOrConstant s2, Register d) { lduw(Address(s1, s2), d); }
582 inline void MacroAssembler::ldsw(Register s1, RegisterOrConstant s2, Register d) { ldsw(Address(s1, s2), d); }
583 inline void MacroAssembler::ldx( Register s1, RegisterOrConstant s2, Register d) { ldx( Address(s1, s2), d); }
584 inline void MacroAssembler::ld(  Register s1, RegisterOrConstant s2, Register d) { ld(  Address(s1, s2), d); }
585 inline void MacroAssembler::ldd( Register s1, RegisterOrConstant s2, Register d) { ldd( Address(s1, s2), d); }
586 
587 inline void MacroAssembler::ldf(FloatRegisterImpl::Width w, Register s1, RegisterOrConstant s2, FloatRegister d) {
588   if (s2.is_register())  ldf(w, s1, s2.as_register(), d);
589   else                   ldf(w, s1, s2.as_constant(), d);
590 }
591 
592 inline void MacroAssembler::ldf(FloatRegisterImpl::Width w, const Address&amp; a, FloatRegister d, int offset) {
593   relocate(a.rspec(offset));
594   if (a.has_index()) {
595     assert(offset == 0, &quot;&quot;);
596     ldf(w, a.base(), a.index(), d);
597   } else {
598     ldf(w, a.base(), a.disp() + offset, d);
599   }
600 }
601 
602 inline void MacroAssembler::lduwl(Register s1, Register s2, Register d) { lduwa(s1, s2, ASI_PRIMARY_LITTLE, d); }
603 inline void MacroAssembler::ldswl(Register s1, Register s2, Register d) { ldswa(s1, s2, ASI_PRIMARY_LITTLE, d);}
604 inline void MacroAssembler::ldxl( Register s1, Register s2, Register d) { ldxa(s1, s2, ASI_PRIMARY_LITTLE, d); }
605 inline void MacroAssembler::ldfl(FloatRegisterImpl::Width w, Register s1, Register s2, FloatRegister d) { ldfa(w, s1, s2, ASI_PRIMARY_LITTLE, d); }
606 
607 // returns if membar generates anything, obviously this code should mirror
608 // membar below.
609 inline bool MacroAssembler::membar_has_effect( Membar_mask_bits const7a ) {
610   const Membar_mask_bits effective_mask =
611       Membar_mask_bits(const7a &amp; ~(LoadLoad | LoadStore | StoreStore));
612   return (effective_mask != 0);
613 }
614 
615 inline void MacroAssembler::membar( Membar_mask_bits const7a ) {
616   // Weakened for current Sparcs and TSO.  See the v9 manual, sections 8.4.3,
617   // 8.4.4.3, a.31 and a.50.
618   // Under TSO, setting bit 3, 2, or 0 is redundant, so the only value
619   // of the mmask subfield of const7a that does anything that isn&#39;t done
620   // implicitly is StoreLoad.
621   const Membar_mask_bits effective_mask =
622       Membar_mask_bits(const7a &amp; ~(LoadLoad | LoadStore | StoreStore));
623   if (effective_mask != 0) {
624     Assembler::membar(effective_mask);
625   }
626 }
627 
628 inline void MacroAssembler::mov(Register s, Register d) {
629   if (s != d) {
630     or3(G0, s, d);
631   } else {
632     assert_not_delayed();  // Put something useful in the delay slot!
633   }
634 }
635 
636 inline void MacroAssembler::mov_or_nop(Register s, Register d) {
637   if (s != d) {
638     or3(G0, s, d);
639   } else {
640     nop();
641   }
642 }
643 
644 inline void MacroAssembler::mov( int simm13a, Register d) { or3( G0, simm13a, d); }
645 
646 inline void MacroAssembler::prefetch(const Address&amp; a, PrefetchFcn f, int offset) {
647   relocate(a.rspec(offset));
648   assert(!a.has_index(), &quot;&quot;);
649   prefetch(a.base(), a.disp() + offset, f);
650 }
651 
652 inline void MacroAssembler::st(Register d, Register s1, Register s2)      { stw(d, s1, s2); }
653 inline void MacroAssembler::st(Register d, Register s1, int simm13a)      { stw(d, s1, simm13a); }
654 
655 #ifdef ASSERT
656 // ByteSize is only a class when ASSERT is defined, otherwise it&#39;s an int.
657 inline void MacroAssembler::st(Register d, Register s1, ByteSize simm13a) { stw(d, s1, in_bytes(simm13a)); }
658 #endif
659 
660 inline void MacroAssembler::st(Register d, const Address&amp; a, int offset) {
661   if (a.has_index()) { assert(offset == 0, &quot;&quot;); st( d, a.base(), a.index()        ); }
662   else               {                          st( d, a.base(), a.disp() + offset); }
663 }
664 
665 inline void MacroAssembler::stb(Register d, const Address&amp; a, int offset) {
666   if (a.has_index()) { assert(offset == 0, &quot;&quot;); stb(d, a.base(), a.index()        ); }
667   else               {                          stb(d, a.base(), a.disp() + offset); }
668 }
669 inline void MacroAssembler::sth(Register d, const Address&amp; a, int offset) {
670   if (a.has_index()) { assert(offset == 0, &quot;&quot;); sth(d, a.base(), a.index()        ); }
671   else               {                          sth(d, a.base(), a.disp() + offset); }
672 }
673 inline void MacroAssembler::stw(Register d, const Address&amp; a, int offset) {
674   if (a.has_index()) { assert(offset == 0, &quot;&quot;); stw(d, a.base(), a.index()        ); }
675   else               {                          stw(d, a.base(), a.disp() + offset); }
676 }
677 inline void MacroAssembler::std(Register d, const Address&amp; a, int offset) {
678   if (a.has_index()) { assert(offset == 0, &quot;&quot;); std(d, a.base(), a.index()        ); }
679   else               {                          std(d, a.base(), a.disp() + offset); }
680 }
681 inline void MacroAssembler::stx(Register d, const Address&amp; a, int offset) {
682   if (a.has_index()) { assert(offset == 0, &quot;&quot;); stx(d, a.base(), a.index()        ); }
683   else               {                          stx(d, a.base(), a.disp() + offset); }
684 }
685 
686 inline void MacroAssembler::stb(Register d, Register s1, RegisterOrConstant s2) { stb(d, Address(s1, s2)); }
687 inline void MacroAssembler::sth(Register d, Register s1, RegisterOrConstant s2) { sth(d, Address(s1, s2)); }
688 inline void MacroAssembler::stw(Register d, Register s1, RegisterOrConstant s2) { stw(d, Address(s1, s2)); }
689 inline void MacroAssembler::stx(Register d, Register s1, RegisterOrConstant s2) { stx(d, Address(s1, s2)); }
690 inline void MacroAssembler::std(Register d, Register s1, RegisterOrConstant s2) { std(d, Address(s1, s2)); }
691 inline void MacroAssembler::st( Register d, Register s1, RegisterOrConstant s2) { st( d, Address(s1, s2)); }
692 
693 inline void MacroAssembler::stf(FloatRegisterImpl::Width w, FloatRegister d, Register s1, RegisterOrConstant s2) {
694   if (s2.is_register())  stf(w, d, s1, s2.as_register());
695   else                   stf(w, d, s1, s2.as_constant());
696 }
697 
698 inline void MacroAssembler::stf(FloatRegisterImpl::Width w, FloatRegister d, const Address&amp; a, int offset) {
699   relocate(a.rspec(offset));
700   if (a.has_index()) { assert(offset == 0, &quot;&quot;); stf(w, d, a.base(), a.index()        ); }
701   else               {                          stf(w, d, a.base(), a.disp() + offset); }
702 }
703 
704 inline void MacroAssembler::sub(Register s1, RegisterOrConstant s2, Register d, int offset) {
705   if (s2.is_register())  sub(s1, s2.as_register(),          d);
706   else                 { sub(s1, s2.as_constant() + offset, d); offset = 0; }
707   if (offset != 0)       sub(d,  offset,                    d);
708 }
709 
710 inline void MacroAssembler::swap(const Address&amp; a, Register d, int offset) {
711   relocate(a.rspec(offset));
712   if (a.has_index()) { assert(offset == 0, &quot;&quot;); swap(a.base(), a.index(), d        ); }
713   else               {                          swap(a.base(), a.disp() + offset, d); }
714 }
715 #endif // CPU_SPARC_MACROASSEMBLER_SPARC_INLINE_HPP
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="2" type="hidden" />
</body>
</html>