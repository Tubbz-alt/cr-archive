<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/sparc/macroAssembler_sparc.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="jvmciCodeInstaller_sparc.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_sparc.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/sparc/macroAssembler_sparc.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;jvm.h&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;compiler/disassembler.hpp&quot;
  29 #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
  30 #include &quot;gc/shared/barrierSet.hpp&quot;
  31 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  32 #include &quot;interpreter/interpreter.hpp&quot;
  33 #include &quot;memory/resourceArea.hpp&quot;
  34 #include &quot;memory/universe.hpp&quot;
  35 #include &quot;oops/accessDecorators.hpp&quot;

  36 #include &quot;oops/klass.inline.hpp&quot;
  37 #include &quot;prims/methodHandles.hpp&quot;
  38 #include &quot;runtime/biasedLocking.hpp&quot;
  39 #include &quot;runtime/flags/flagSetting.hpp&quot;
  40 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  41 #include &quot;runtime/jniHandles.inline.hpp&quot;
  42 #include &quot;runtime/objectMonitor.hpp&quot;
  43 #include &quot;runtime/os.inline.hpp&quot;
  44 #include &quot;runtime/safepoint.hpp&quot;
  45 #include &quot;runtime/safepointMechanism.hpp&quot;
  46 #include &quot;runtime/sharedRuntime.hpp&quot;
  47 #include &quot;runtime/stubRoutines.hpp&quot;
  48 #include &quot;utilities/align.hpp&quot;
  49 #include &quot;utilities/macros.hpp&quot;

  50 #ifdef COMPILER2
  51 #include &quot;opto/intrinsicnode.hpp&quot;
  52 #endif
  53 
  54 #ifdef PRODUCT
  55 #define BLOCK_COMMENT(str) /* nothing */
  56 #define STOP(error) stop(error)
  57 #else
  58 #define BLOCK_COMMENT(str) block_comment(str)
  59 #define STOP(error) block_comment(error); stop(error)
  60 #endif
  61 
  62 // Convert the raw encoding form into the form expected by the
  63 // constructor for Address.
  64 Address Address::make_raw(int base, int index, int scale, int disp, relocInfo::relocType disp_reloc) {
  65   assert(scale == 0, &quot;not supported&quot;);
  66   RelocationHolder rspec;
  67   if (disp_reloc != relocInfo::none) {
  68     rspec = Relocation::spec_simple(disp_reloc);
  69   }
</pre>
<hr />
<pre>
 972 AddressLiteral MacroAssembler::allocate_metadata_address(Metadata* obj) {
 973   assert(oop_recorder() != NULL, &quot;this assembler needs a Recorder&quot;);
 974   int index = oop_recorder()-&gt;allocate_metadata_index(obj);
 975   RelocationHolder rspec = metadata_Relocation::spec(index);
 976   return AddressLiteral((address)obj, rspec);
 977 }
 978 
 979 AddressLiteral MacroAssembler::constant_metadata_address(Metadata* obj) {
 980   assert(oop_recorder() != NULL, &quot;this assembler needs a Recorder&quot;);
 981   int index = oop_recorder()-&gt;find_index(obj);
 982   RelocationHolder rspec = metadata_Relocation::spec(index);
 983   return AddressLiteral((address)obj, rspec);
 984 }
 985 
 986 
 987 AddressLiteral MacroAssembler::constant_oop_address(jobject obj) {
 988 #ifdef ASSERT
 989   {
 990     ThreadInVMfromUnknown tiv;
 991     assert(oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
<span class="line-modified"> 992     assert(Universe::heap()-&gt;is_in_reserved(JNIHandles::resolve(obj)), &quot;not an oop&quot;);</span>
 993   }
 994 #endif
 995   int oop_index = oop_recorder()-&gt;find_index(obj);
 996   return AddressLiteral(obj, oop_Relocation::spec(oop_index));
 997 }
 998 
 999 void  MacroAssembler::set_narrow_oop(jobject obj, Register d) {
1000   assert(oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
1001   int oop_index = oop_recorder()-&gt;find_index(obj);
1002   RelocationHolder rspec = oop_Relocation::spec(oop_index);
1003 
1004   assert_not_delayed();
1005   // Relocation with special format (see relocInfo_sparc.hpp).
1006   relocate(rspec, 1);
1007   // Assembler::sethi(0x3fffff, d);
1008   emit_int32( op(branch_op) | rd(d) | op2(sethi_op2) | hi22(0x3fffff) );
1009   // Don&#39;t add relocation for &#39;add&#39;. Do patching during &#39;sethi&#39; processing.
1010   add(d, 0x3ff, d);
1011 
1012 }
1013 
1014 void  MacroAssembler::set_narrow_klass(Klass* k, Register d) {
1015   assert(oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
1016   int klass_index = oop_recorder()-&gt;find_index(k);
1017   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
<span class="line-modified">1018   narrowOop encoded_k = Klass::encode_klass(k);</span>
1019 
1020   assert_not_delayed();
1021   // Relocation with special format (see relocInfo_sparc.hpp).
1022   relocate(rspec, 1);
1023   // Assembler::sethi(encoded_k, d);
1024   emit_int32( op(branch_op) | rd(d) | op2(sethi_op2) | hi22(encoded_k) );
1025   // Don&#39;t add relocation for &#39;add&#39;. Do patching during &#39;sethi&#39; processing.
1026   add(d, low10(encoded_k), d);
1027 
1028 }
1029 
1030 void MacroAssembler::align(int modulus) {
1031   while (offset() % modulus != 0) nop();
1032 }
1033 
1034 void RegistersForDebugging::print(outputStream* s) {
1035   FlagSetting fs(Debugging, true);
1036   int j;
1037   for (j = 0; j &lt; 8; ++j) {
1038     if (j != 6) { s-&gt;print(&quot;i%d = &quot;, j); os::print_location(s, i[j]); }
</pre>
<hr />
<pre>
1112   for (i = 0;  i &lt; 32; ++i) {
1113     a-&gt;stf(FloatRegisterImpl::S, as_FloatRegister(i), O0, f_offset(i));
1114   }
1115   for (i = 0; i &lt; 64; i += 2) {
1116     a-&gt;stf(FloatRegisterImpl::D, as_FloatRegister(i), O0, d_offset(i));
1117   }
1118 }
1119 
1120 void RegistersForDebugging::restore_registers(MacroAssembler* a, Register r) {
1121   for (int i = 1; i &lt; 8;  ++i) {
1122     a-&gt;ld_ptr(r, g_offset(i), as_gRegister(i));
1123   }
1124   for (int j = 0; j &lt; 32; ++j) {
1125     a-&gt;ldf(FloatRegisterImpl::S, O0, f_offset(j), as_FloatRegister(j));
1126   }
1127   for (int k = 0; k &lt; 64; k += 2) {
1128     a-&gt;ldf(FloatRegisterImpl::D, O0, d_offset(k), as_FloatRegister(k));
1129   }
1130 }
1131 
<span class="line-removed">1132 </span>
<span class="line-removed">1133 // pushes double TOS element of FPU stack on CPU stack; pops from FPU stack</span>
<span class="line-removed">1134 void MacroAssembler::push_fTOS() {</span>
<span class="line-removed">1135   // %%%%%% need to implement this</span>
<span class="line-removed">1136 }</span>
<span class="line-removed">1137 </span>
<span class="line-removed">1138 // pops double TOS element from CPU stack and pushes on FPU stack</span>
<span class="line-removed">1139 void MacroAssembler::pop_fTOS() {</span>
<span class="line-removed">1140   // %%%%%% need to implement this</span>
<span class="line-removed">1141 }</span>
<span class="line-removed">1142 </span>
<span class="line-removed">1143 void MacroAssembler::empty_FPU_stack() {</span>
<span class="line-removed">1144   // %%%%%% need to implement this</span>
<span class="line-removed">1145 }</span>
<span class="line-removed">1146 </span>
1147 void MacroAssembler::_verify_oop(Register reg, const char* msg, const char * file, int line) {
1148   // plausibility check for oops
1149   if (!VerifyOops) return;
1150 
1151   if (reg == G0)  return;       // always NULL, which is always an oop
1152 
1153   BLOCK_COMMENT(&quot;verify_oop {&quot;);
1154   char buffer[64];
1155 #ifdef COMPILER1
1156   if (CommentedAssembly) {
1157     snprintf(buffer, sizeof(buffer), &quot;verify_oop at %d&quot;, offset());
1158     block_comment(buffer);
1159   }
1160 #endif
1161 
1162   const char* real_msg = NULL;
1163   {
1164     ResourceMark rm;
1165     stringStream ss;
1166     ss.print(&quot;%s at offset %d (%s:%d)&quot;, msg, offset(), file, line);
</pre>
<hr />
<pre>
2434 
2435 void MacroAssembler::biased_locking_enter(Register obj_reg, Register mark_reg,
2436                                           Register temp_reg,
2437                                           Label&amp; done, Label* slow_case,
2438                                           BiasedLockingCounters* counters) {
2439   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
2440 
2441   if (PrintBiasedLockingStatistics) {
2442     assert_different_registers(obj_reg, mark_reg, temp_reg, O7);
2443     if (counters == NULL)
2444       counters = BiasedLocking::counters();
2445   }
2446 
2447   Label cas_label;
2448 
2449   // Biased locking
2450   // See whether the lock is currently biased toward our thread and
2451   // whether the epoch is still valid
2452   // Note that the runtime guarantees sufficient alignment of JavaThread
2453   // pointers to allow age to be placed into low bits
<span class="line-modified">2454   assert(markOopDesc::age_shift == markOopDesc::lock_bits + markOopDesc::biased_lock_bits, &quot;biased locking makes assumptions about bit layout&quot;);</span>
<span class="line-modified">2455   and3(mark_reg, markOopDesc::biased_lock_mask_in_place, temp_reg);</span>
<span class="line-modified">2456   cmp_and_brx_short(temp_reg, markOopDesc::biased_lock_pattern, Assembler::notEqual, Assembler::pn, cas_label);</span>
2457 
2458   load_klass(obj_reg, temp_reg);
2459   ld_ptr(Address(temp_reg, Klass::prototype_header_offset()), temp_reg);
2460   or3(G2_thread, temp_reg, temp_reg);
2461   xor3(mark_reg, temp_reg, temp_reg);
<span class="line-modified">2462   andcc(temp_reg, ~((int) markOopDesc::age_mask_in_place), temp_reg);</span>
2463   if (counters != NULL) {
2464     cond_inc(Assembler::equal, (address) counters-&gt;biased_lock_entry_count_addr(), mark_reg, temp_reg);
2465     // Reload mark_reg as we may need it later
2466     ld_ptr(Address(obj_reg, oopDesc::mark_offset_in_bytes()), mark_reg);
2467   }
2468   brx(Assembler::equal, true, Assembler::pt, done);
2469   delayed()-&gt;nop();
2470 
2471   Label try_revoke_bias;
2472   Label try_rebias;
2473   Address mark_addr = Address(obj_reg, oopDesc::mark_offset_in_bytes());
2474   assert(mark_addr.disp() == 0, &quot;cas must take a zero displacement&quot;);
2475 
2476   // At this point we know that the header has the bias pattern and
2477   // that we are not the bias owner in the current epoch. We need to
2478   // figure out more details about the state of the header in order to
2479   // know what operations can be legally performed on the object&#39;s
2480   // header.
2481 
2482   // If the low three bits in the xor result aren&#39;t clear, that means
2483   // the prototype header is no longer biased and we have to revoke
2484   // the bias on this object.
<span class="line-modified">2485   btst(markOopDesc::biased_lock_mask_in_place, temp_reg);</span>
2486   brx(Assembler::notZero, false, Assembler::pn, try_revoke_bias);
2487 
2488   // Biasing is still enabled for this data type. See whether the
2489   // epoch of the current bias is still valid, meaning that the epoch
2490   // bits of the mark word are equal to the epoch bits of the
2491   // prototype header. (Note that the prototype header&#39;s epoch bits
2492   // only change at a safepoint.) If not, attempt to rebias the object
2493   // toward the current thread. Note that we must be absolutely sure
2494   // that the current epoch is invalid in order to do this because
2495   // otherwise the manipulations it performs on the mark word are
2496   // illegal.
<span class="line-modified">2497   delayed()-&gt;btst(markOopDesc::epoch_mask_in_place, temp_reg);</span>
2498   brx(Assembler::notZero, false, Assembler::pn, try_rebias);
2499 
2500   // The epoch of the current bias is still valid but we know nothing
2501   // about the owner; it might be set or it might be clear. Try to
2502   // acquire the bias of the object using an atomic operation. If this
2503   // fails we will go in to the runtime to revoke the object&#39;s bias.
2504   // Note that we first construct the presumed unbiased header so we
2505   // don&#39;t accidentally blow away another thread&#39;s valid bias.
2506   delayed()-&gt;and3(mark_reg,
<span class="line-modified">2507                   markOopDesc::biased_lock_mask_in_place | markOopDesc::age_mask_in_place | markOopDesc::epoch_mask_in_place,</span>
2508                   mark_reg);
2509   or3(G2_thread, mark_reg, temp_reg);
2510   cas_ptr(mark_addr.base(), mark_reg, temp_reg);
2511   // If the biasing toward our thread failed, this means that
2512   // another thread succeeded in biasing it toward itself and we
2513   // need to revoke that bias. The revocation will occur in the
2514   // interpreter runtime in the slow case.
2515   cmp(mark_reg, temp_reg);
2516   if (counters != NULL) {
2517     cond_inc(Assembler::zero, (address) counters-&gt;anonymously_biased_lock_entry_count_addr(), mark_reg, temp_reg);
2518   }
2519   if (slow_case != NULL) {
2520     brx(Assembler::notEqual, true, Assembler::pn, *slow_case);
2521     delayed()-&gt;nop();
2522   }
2523   ba_short(done);
2524 
2525   bind(try_rebias);
2526   // At this point we know the epoch has expired, meaning that the
2527   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
</pre>
<hr />
<pre>
2568   // Fall through to the normal CAS-based lock, because no matter what
2569   // the result of the above CAS, some thread must have succeeded in
2570   // removing the bias bit from the object&#39;s header.
2571   if (counters != NULL) {
2572     cmp(mark_reg, temp_reg);
2573     cond_inc(Assembler::zero, (address) counters-&gt;revoked_lock_entry_count_addr(), mark_reg, temp_reg);
2574   }
2575 
2576   bind(cas_label);
2577 }
2578 
2579 void MacroAssembler::biased_locking_exit (Address mark_addr, Register temp_reg, Label&amp; done,
2580                                           bool allow_delay_slot_filling) {
2581   // Check for biased locking unlock case, which is a no-op
2582   // Note: we do not have to check the thread ID for two reasons.
2583   // First, the interpreter checks for IllegalMonitorStateException at
2584   // a higher level. Second, if the bias was revoked while we held the
2585   // lock, the object could not be rebiased toward another thread, so
2586   // the bias bit would be clear.
2587   ld_ptr(mark_addr, temp_reg);
<span class="line-modified">2588   and3(temp_reg, markOopDesc::biased_lock_mask_in_place, temp_reg);</span>
<span class="line-modified">2589   cmp(temp_reg, markOopDesc::biased_lock_pattern);</span>
2590   brx(Assembler::equal, allow_delay_slot_filling, Assembler::pt, done);
2591   delayed();
2592   if (!allow_delay_slot_filling) {
2593     nop();
2594   }
2595 }
2596 
2597 
2598 // compiler_lock_object() and compiler_unlock_object() are direct transliterations
2599 // of i486.ad fast_lock() and fast_unlock().  See those methods for detailed comments.
2600 // The code could be tightened up considerably.
2601 //
2602 // box-&gt;dhw disposition - post-conditions at DONE_LABEL.
2603 // -   Successful inflated lock:  box-&gt;dhw != 0.
2604 //     Any non-zero value suffices.
<span class="line-modified">2605 //     Consider G2_thread, rsp, boxReg, or markOopDesc::unused_mark()</span>
2606 // -   Successful Stack-lock: box-&gt;dhw == mark.
2607 //     box-&gt;dhw must contain the displaced mark word value
2608 // -   Failure -- icc.ZFlag == 0 and box-&gt;dhw is undefined.
<span class="line-modified">2609 //     The slow-path fast_enter() and slow_enter() operators</span>
<span class="line-modified">2610 //     are responsible for setting box-&gt;dhw = NonZero (typically markOopDesc::unused_mark()).</span>
2611 // -   Biased: box-&gt;dhw is undefined
2612 //
2613 // SPARC refworkload performance - specifically jetstream and scimark - are
2614 // extremely sensitive to the size of the code emitted by compiler_lock_object
2615 // and compiler_unlock_object.  Critically, the key factor is code size, not path
2616 // length.  (Simply experiments to pad CLO with unexecuted NOPs demonstrte the
2617 // effect).
2618 
2619 
2620 void MacroAssembler::compiler_lock_object(Register Roop, Register Rmark,
2621                                           Register Rbox, Register Rscratch,
2622                                           BiasedLockingCounters* counters,
2623                                           bool try_bias) {
2624    Address mark_addr(Roop, oopDesc::mark_offset_in_bytes());
2625 
2626    verify_oop(Roop);
2627    Label done ;
2628 
2629    if (counters != NULL) {
2630      inc_counter((address) counters-&gt;total_entry_count_addr(), Rmark, Rscratch);
</pre>
<hr />
<pre>
2640    ld_ptr(mark_addr, Rmark);           // fetch obj-&gt;mark
2641    // Triage: biased, stack-locked, neutral, inflated
2642 
2643    if (try_bias) {
2644      biased_locking_enter(Roop, Rmark, Rscratch, done, NULL, counters);
2645      // Invariant: if control reaches this point in the emitted stream
2646      // then Rmark has not been modified.
2647    }
2648    andcc(Rmark, 2, G0);
2649    brx(Assembler::notZero, false, Assembler::pn, IsInflated);
2650    delayed()-&gt;                         // Beware - dangling delay-slot
2651 
2652    // Try stack-lock acquisition.
2653    // Transiently install BUSY (0) encoding in the mark word.
2654    // if the CAS of 0 into the mark was successful then we execute:
2655    //   ST box-&gt;dhw  = mark   -- save fetched mark in on-stack basiclock box
2656    //   ST obj-&gt;mark = box    -- overwrite transient 0 value
2657    // This presumes TSO, of course.
2658 
2659    mov(0, Rscratch);
<span class="line-modified">2660    or3(Rmark, markOopDesc::unlocked_value, Rmark);</span>
2661    assert(mark_addr.disp() == 0, &quot;cas must take a zero displacement&quot;);
2662    cas_ptr(mark_addr.base(), Rmark, Rscratch);
2663 // prefetch (mark_addr, Assembler::severalWritesAndPossiblyReads);
2664    cmp(Rscratch, Rmark);
2665    brx(Assembler::notZero, false, Assembler::pn, Recursive);
2666    delayed()-&gt;st_ptr(Rmark, Rbox, BasicLock::displaced_header_offset_in_bytes());
2667    if (counters != NULL) {
2668      cond_inc(Assembler::equal, (address) counters-&gt;fast_path_entry_count_addr(), Rmark, Rscratch);
2669    }
2670    ba(done);
2671    delayed()-&gt;st_ptr(Rbox, mark_addr);
2672 
2673    bind(Recursive);
2674    // Stack-lock attempt failed - check for recursive stack-lock.
2675    // Tests show that we can remove the recursive case with no impact
2676    // on refworkload 0.83.  If we need to reduce the size of the code
2677    // emitted by compiler_lock_object() the recursive case is perfect
2678    // candidate.
2679    //
2680    // A more extreme idea is to always inflate on stack-lock recursion.
</pre>
<hr />
<pre>
2694      // Accounting needs the Rscratch register
2695      st_ptr(Rscratch, Rbox, BasicLock::displaced_header_offset_in_bytes());
2696      cond_inc(Assembler::equal, (address) counters-&gt;fast_path_entry_count_addr(), Rmark, Rscratch);
2697      ba_short(done);
2698    } else {
2699      ba(done);
2700      delayed()-&gt;st_ptr(Rscratch, Rbox, BasicLock::displaced_header_offset_in_bytes());
2701    }
2702 
2703    bind   (IsInflated);
2704 
2705    // Try to CAS m-&gt;owner from null to Self
2706    // Invariant: if we acquire the lock then _recursions should be 0.
2707    add(Rmark, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner), Rmark);
2708    mov(G2_thread, Rscratch);
2709    cas_ptr(Rmark, G0, Rscratch);
2710    andcc(Rscratch, Rscratch, G0);             // set ICCs for done: icc.zf iff success
2711    // set icc.zf : 1=success 0=failure
2712    // ST box-&gt;displaced_header = NonZero.
2713    // Any non-zero value suffices:
<span class="line-modified">2714    //    markOopDesc::unused_mark(), G2_thread, RBox, RScratch, rsp, etc.</span>
2715    st_ptr(Rbox, Rbox, BasicLock::displaced_header_offset_in_bytes());
2716    // Intentional fall-through into done
2717 
2718    bind   (done);
2719 }
2720 
2721 void MacroAssembler::compiler_unlock_object(Register Roop, Register Rmark,
2722                                             Register Rbox, Register Rscratch,
2723                                             bool try_bias) {
2724    Address mark_addr(Roop, oopDesc::mark_offset_in_bytes());
2725 
2726    Label done ;
2727 
2728    // Beware ... If the aggregate size of the code emitted by CLO and CUO is
2729    // is too large performance rolls abruptly off a cliff.
2730    // This could be related to inlining policies, code cache management, or
2731    // I$ effects.
2732    Label LStacked ;
2733 
2734    if (try_bias) {
</pre>
<hr />
<pre>
2808    // to O and O is stack-locked by T1.  The &quot;stomp&quot; race could cause
2809    // an assigned hashCode value to be lost.  We can avoid that condition
2810    // and provide the necessary hashCode stability invariants by ensuring
2811    // that hashCode generation is idempotent between copying GCs.
2812    // For example we could compute the hashCode of an object O as
2813    // O&#39;s heap address XOR some high quality RNG value that is refreshed
2814    // at GC-time.  The monitor scavenger would install the hashCode
2815    // found in any orphan monitors.  Again, the mechanism admits a
2816    // lost-update &quot;stomp&quot; WAW race but detects and recovers as needed.
2817    //
2818    // A prototype implementation showed excellent results, although
2819    // the scavenger and timeout code was rather involved.
2820 
2821    cas_ptr(mark_addr.base(), Rbox, Rscratch);
2822    cmp(Rbox, Rscratch);
2823    // Intentional fall through into done ...
2824 
2825    bind(done);
2826 }
2827 
<span class="line-removed">2828 </span>
<span class="line-removed">2829 </span>
<span class="line-removed">2830 void MacroAssembler::print_CPU_state() {</span>
<span class="line-removed">2831   // %%%%% need to implement this</span>
<span class="line-removed">2832 }</span>
<span class="line-removed">2833 </span>
<span class="line-removed">2834 void MacroAssembler::verify_FPU(int stack_depth, const char* s) {</span>
<span class="line-removed">2835   // %%%%% need to implement this</span>
<span class="line-removed">2836 }</span>
<span class="line-removed">2837 </span>
<span class="line-removed">2838 void MacroAssembler::push_IU_state() {</span>
<span class="line-removed">2839   // %%%%% need to implement this</span>
<span class="line-removed">2840 }</span>
<span class="line-removed">2841 </span>
<span class="line-removed">2842 </span>
<span class="line-removed">2843 void MacroAssembler::pop_IU_state() {</span>
<span class="line-removed">2844   // %%%%% need to implement this</span>
<span class="line-removed">2845 }</span>
<span class="line-removed">2846 </span>
<span class="line-removed">2847 </span>
<span class="line-removed">2848 void MacroAssembler::push_FPU_state() {</span>
<span class="line-removed">2849   // %%%%% need to implement this</span>
<span class="line-removed">2850 }</span>
<span class="line-removed">2851 </span>
<span class="line-removed">2852 </span>
<span class="line-removed">2853 void MacroAssembler::pop_FPU_state() {</span>
<span class="line-removed">2854   // %%%%% need to implement this</span>
<span class="line-removed">2855 }</span>
<span class="line-removed">2856 </span>
<span class="line-removed">2857 </span>
<span class="line-removed">2858 void MacroAssembler::push_CPU_state() {</span>
<span class="line-removed">2859   // %%%%% need to implement this</span>
<span class="line-removed">2860 }</span>
<span class="line-removed">2861 </span>
<span class="line-removed">2862 </span>
<span class="line-removed">2863 void MacroAssembler::pop_CPU_state() {</span>
<span class="line-removed">2864   // %%%%% need to implement this</span>
<span class="line-removed">2865 }</span>
<span class="line-removed">2866 </span>
<span class="line-removed">2867 </span>
<span class="line-removed">2868 </span>
2869 void MacroAssembler::verify_tlab() {
2870 #ifdef ASSERT
2871   if (UseTLAB &amp;&amp; VerifyOops) {
2872     Label next, next2, ok;
2873     Register t1 = L0;
2874     Register t2 = L1;
2875     Register t3 = L2;
2876 
2877     save_frame(0);
2878     ld_ptr(G2_thread, in_bytes(JavaThread::tlab_top_offset()), t1);
2879     ld_ptr(G2_thread, in_bytes(JavaThread::tlab_start_offset()), t2);
2880     or3(t1, t2, t3);
2881     cmp_and_br_short(t1, t2, Assembler::greaterEqual, Assembler::pn, next);
2882     STOP(&quot;assert(top &gt;= start)&quot;);
2883     should_not_reach_here();
2884 
2885     bind(next);
2886     ld_ptr(G2_thread, in_bytes(JavaThread::tlab_top_offset()), t1);
2887     ld_ptr(G2_thread, in_bytes(JavaThread::tlab_end_offset()), t2);
2888     or3(t3, t2, t3);
</pre>
<hr />
<pre>
3278 }
3279 
3280 void MacroAssembler::store_heap_oop(Register d, Register s1, int simm13a, Register tmp, DecoratorSet decorators) {
3281   access_store_at(T_OBJECT, IN_HEAP | decorators, d, Address(s1, simm13a), tmp);
3282 }
3283 
3284 void MacroAssembler::store_heap_oop(Register d, const Address&amp; a, int offset, Register tmp, DecoratorSet decorators) {
3285   if (a.has_index()) {
3286     assert(!a.has_disp(), &quot;not supported yet&quot;);
3287     assert(offset == 0, &quot;not supported yet&quot;);
3288     access_store_at(T_OBJECT, IN_HEAP | decorators, d, Address(a.base(), a.index()), tmp);
3289   } else {
3290     access_store_at(T_OBJECT, IN_HEAP | decorators, d, Address(a.base(), a.disp() + offset), tmp);
3291   }
3292 }
3293 
3294 
3295 void MacroAssembler::encode_heap_oop(Register src, Register dst) {
3296   assert (UseCompressedOops, &quot;must be compressed&quot;);
3297   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
<span class="line-modified">3298   assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
3299   verify_oop(src);
<span class="line-modified">3300   if (Universe::narrow_oop_base() == NULL) {</span>
3301     srlx(src, LogMinObjAlignmentInBytes, dst);
3302     return;
3303   }
3304   Label done;
3305   if (src == dst) {
3306     // optimize for frequent case src == dst
3307     bpr(rc_nz, true, Assembler::pt, src, done);
3308     delayed() -&gt; sub(src, G6_heapbase, dst); // annuled if not taken
3309     bind(done);
3310     srlx(src, LogMinObjAlignmentInBytes, dst);
3311   } else {
3312     bpr(rc_z, false, Assembler::pn, src, done);
3313     delayed() -&gt; mov(G0, dst);
3314     // could be moved before branch, and annulate delay,
3315     // but may add some unneeded work decoding null
3316     sub(src, G6_heapbase, dst);
3317     srlx(dst, LogMinObjAlignmentInBytes, dst);
3318     bind(done);
3319   }
3320 }
3321 
3322 
3323 void MacroAssembler::encode_heap_oop_not_null(Register r) {
3324   assert (UseCompressedOops, &quot;must be compressed&quot;);
3325   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
<span class="line-modified">3326   assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
3327   verify_oop(r);
<span class="line-modified">3328   if (Universe::narrow_oop_base() != NULL)</span>
3329     sub(r, G6_heapbase, r);
3330   srlx(r, LogMinObjAlignmentInBytes, r);
3331 }
3332 
3333 void MacroAssembler::encode_heap_oop_not_null(Register src, Register dst) {
3334   assert (UseCompressedOops, &quot;must be compressed&quot;);
3335   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
<span class="line-modified">3336   assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
3337   verify_oop(src);
<span class="line-modified">3338   if (Universe::narrow_oop_base() == NULL) {</span>
3339     srlx(src, LogMinObjAlignmentInBytes, dst);
3340   } else {
3341     sub(src, G6_heapbase, dst);
3342     srlx(dst, LogMinObjAlignmentInBytes, dst);
3343   }
3344 }
3345 
3346 // Same algorithm as oops.inline.hpp decode_heap_oop.
3347 void  MacroAssembler::decode_heap_oop(Register src, Register dst) {
3348   assert (UseCompressedOops, &quot;must be compressed&quot;);
3349   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
<span class="line-modified">3350   assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
3351   sllx(src, LogMinObjAlignmentInBytes, dst);
<span class="line-modified">3352   if (Universe::narrow_oop_base() != NULL) {</span>
3353     Label done;
3354     bpr(rc_nz, true, Assembler::pt, dst, done);
3355     delayed() -&gt; add(dst, G6_heapbase, dst); // annuled if not taken
3356     bind(done);
3357   }
3358   verify_oop(dst);
3359 }
3360 
3361 void  MacroAssembler::decode_heap_oop_not_null(Register r) {
3362   // Do not add assert code to this unless you change vtableStubs_sparc.cpp
3363   // pd_code_size_limit.
3364   // Also do not verify_oop as this is called by verify_oop.
3365   assert (UseCompressedOops, &quot;must be compressed&quot;);
3366   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
<span class="line-modified">3367   assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
3368   sllx(r, LogMinObjAlignmentInBytes, r);
<span class="line-modified">3369   if (Universe::narrow_oop_base() != NULL)</span>
3370     add(r, G6_heapbase, r);
3371 }
3372 
3373 void  MacroAssembler::decode_heap_oop_not_null(Register src, Register dst) {
3374   // Do not add assert code to this unless you change vtableStubs_sparc.cpp
3375   // pd_code_size_limit.
3376   // Also do not verify_oop as this is called by verify_oop.
3377   assert (UseCompressedOops, &quot;must be compressed&quot;);
<span class="line-modified">3378   assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
3379   sllx(src, LogMinObjAlignmentInBytes, dst);
<span class="line-modified">3380   if (Universe::narrow_oop_base() != NULL)</span>
3381     add(dst, G6_heapbase, dst);
3382 }
3383 
3384 void MacroAssembler::encode_klass_not_null(Register r) {
3385   assert (UseCompressedClassPointers, &quot;must be compressed&quot;);
<span class="line-modified">3386   if (Universe::narrow_klass_base() != NULL) {</span>
3387     assert(r != G6_heapbase, &quot;bad register choice&quot;);
<span class="line-modified">3388     set((intptr_t)Universe::narrow_klass_base(), G6_heapbase);</span>
3389     sub(r, G6_heapbase, r);
<span class="line-modified">3390     if (Universe::narrow_klass_shift() != 0) {</span>
<span class="line-modified">3391       assert (LogKlassAlignmentInBytes == Universe::narrow_klass_shift(), &quot;decode alg wrong&quot;);</span>
3392       srlx(r, LogKlassAlignmentInBytes, r);
3393     }
3394     reinit_heapbase();
3395   } else {
<span class="line-modified">3396     assert (LogKlassAlignmentInBytes == Universe::narrow_klass_shift() || Universe::narrow_klass_shift() == 0, &quot;decode alg wrong&quot;);</span>
<span class="line-modified">3397     srlx(r, Universe::narrow_klass_shift(), r);</span>
3398   }
3399 }
3400 
3401 void MacroAssembler::encode_klass_not_null(Register src, Register dst) {
3402   if (src == dst) {
3403     encode_klass_not_null(src);
3404   } else {
3405     assert (UseCompressedClassPointers, &quot;must be compressed&quot;);
<span class="line-modified">3406     if (Universe::narrow_klass_base() != NULL) {</span>
<span class="line-modified">3407       set((intptr_t)Universe::narrow_klass_base(), dst);</span>
3408       sub(src, dst, dst);
<span class="line-modified">3409       if (Universe::narrow_klass_shift() != 0) {</span>
3410         srlx(dst, LogKlassAlignmentInBytes, dst);
3411       }
3412     } else {
3413       // shift src into dst
<span class="line-modified">3414       assert (LogKlassAlignmentInBytes == Universe::narrow_klass_shift() || Universe::narrow_klass_shift() == 0, &quot;decode alg wrong&quot;);</span>
<span class="line-modified">3415       srlx(src, Universe::narrow_klass_shift(), dst);</span>
3416     }
3417   }
3418 }
3419 
3420 // Function instr_size_for_decode_klass_not_null() counts the instructions
3421 // generated by decode_klass_not_null() and reinit_heapbase().  Hence, if
3422 // the instructions they generate change, then this method needs to be updated.
3423 int MacroAssembler::instr_size_for_decode_klass_not_null() {
3424   assert (UseCompressedClassPointers, &quot;only for compressed klass ptrs&quot;);
3425   int num_instrs = 1;  // shift src,dst or add
<span class="line-modified">3426   if (Universe::narrow_klass_base() != NULL) {</span>
3427     // set + add + set
<span class="line-modified">3428     num_instrs += insts_for_internal_set((intptr_t)Universe::narrow_klass_base()) +</span>
<span class="line-modified">3429                   insts_for_internal_set((intptr_t)Universe::narrow_ptrs_base());</span>
<span class="line-modified">3430     if (Universe::narrow_klass_shift() != 0) {</span>
3431       num_instrs += 1;  // sllx
3432     }
3433   }
3434   return num_instrs * BytesPerInstWord;
3435 }
3436 
3437 // !!! If the instructions that get generated here change then function
3438 // instr_size_for_decode_klass_not_null() needs to get updated.
3439 void  MacroAssembler::decode_klass_not_null(Register r) {
3440   // Do not add assert code to this unless you change vtableStubs_sparc.cpp
3441   // pd_code_size_limit.
3442   assert (UseCompressedClassPointers, &quot;must be compressed&quot;);
<span class="line-modified">3443   if (Universe::narrow_klass_base() != NULL) {</span>
3444     assert(r != G6_heapbase, &quot;bad register choice&quot;);
<span class="line-modified">3445     set((intptr_t)Universe::narrow_klass_base(), G6_heapbase);</span>
<span class="line-modified">3446     if (Universe::narrow_klass_shift() != 0)</span>
3447       sllx(r, LogKlassAlignmentInBytes, r);
3448     add(r, G6_heapbase, r);
3449     reinit_heapbase();
3450   } else {
<span class="line-modified">3451     assert (LogKlassAlignmentInBytes == Universe::narrow_klass_shift() || Universe::narrow_klass_shift() == 0, &quot;decode alg wrong&quot;);</span>
<span class="line-modified">3452     sllx(r, Universe::narrow_klass_shift(), r);</span>
3453   }
3454 }
3455 
3456 void  MacroAssembler::decode_klass_not_null(Register src, Register dst) {
3457   if (src == dst) {
3458     decode_klass_not_null(src);
3459   } else {
3460     // Do not add assert code to this unless you change vtableStubs_sparc.cpp
3461     // pd_code_size_limit.
3462     assert (UseCompressedClassPointers, &quot;must be compressed&quot;);
<span class="line-modified">3463     if (Universe::narrow_klass_base() != NULL) {</span>
<span class="line-modified">3464       if (Universe::narrow_klass_shift() != 0) {</span>
3465         assert((src != G6_heapbase) &amp;&amp; (dst != G6_heapbase), &quot;bad register choice&quot;);
<span class="line-modified">3466         set((intptr_t)Universe::narrow_klass_base(), G6_heapbase);</span>
3467         sllx(src, LogKlassAlignmentInBytes, dst);
3468         add(dst, G6_heapbase, dst);
3469         reinit_heapbase();
3470       } else {
<span class="line-modified">3471         set((intptr_t)Universe::narrow_klass_base(), dst);</span>
3472         add(src, dst, dst);
3473       }
3474     } else {
3475       // shift/mov src into dst.
<span class="line-modified">3476       assert (LogKlassAlignmentInBytes == Universe::narrow_klass_shift() || Universe::narrow_klass_shift() == 0, &quot;decode alg wrong&quot;);</span>
<span class="line-modified">3477       sllx(src, Universe::narrow_klass_shift(), dst);</span>
3478     }
3479   }
3480 }
3481 
3482 void MacroAssembler::reinit_heapbase() {
3483   if (UseCompressedOops || UseCompressedClassPointers) {
3484     if (Universe::heap() != NULL) {
<span class="line-modified">3485       set((intptr_t)Universe::narrow_ptrs_base(), G6_heapbase);</span>
3486     } else {
<span class="line-modified">3487       AddressLiteral base(Universe::narrow_ptrs_base_addr());</span>
3488       load_ptr_contents(base, G6_heapbase);
3489     }
3490   }
3491 }
3492 
3493 #ifdef COMPILER2
3494 
3495 // Compress char[] to byte[] by compressing 16 bytes at once. Return 0 on failure.
3496 void MacroAssembler::string_compress_16(Register src, Register dst, Register cnt, Register result,
3497                                         Register tmp1, Register tmp2, Register tmp3, Register tmp4,
3498                                         FloatRegister ftmp1, FloatRegister ftmp2, FloatRegister ftmp3, Label&amp; Ldone) {
3499   Label Lloop, Lslow;
3500   assert(UseVIS &gt;= 3, &quot;VIS3 is required&quot;);
3501   assert_different_registers(src, dst, cnt, tmp1, tmp2, tmp3, tmp4, result);
3502   assert_different_registers(ftmp1, ftmp2, ftmp3);
3503 
3504   // Check if cnt &gt;= 8 (= 16 bytes)
3505   cmp(cnt, 8);
3506   br(Assembler::less, false, Assembler::pn, Lslow);
3507   delayed()-&gt;mov(cnt, result); // copy count
</pre>
</td>
<td>
<hr />
<pre>
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;jvm.h&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;compiler/disassembler.hpp&quot;
  29 #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
  30 #include &quot;gc/shared/barrierSet.hpp&quot;
  31 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  32 #include &quot;interpreter/interpreter.hpp&quot;
  33 #include &quot;memory/resourceArea.hpp&quot;
  34 #include &quot;memory/universe.hpp&quot;
  35 #include &quot;oops/accessDecorators.hpp&quot;
<span class="line-added">  36 #include &quot;oops/compressedOops.hpp&quot;</span>
  37 #include &quot;oops/klass.inline.hpp&quot;
  38 #include &quot;prims/methodHandles.hpp&quot;
  39 #include &quot;runtime/biasedLocking.hpp&quot;
  40 #include &quot;runtime/flags/flagSetting.hpp&quot;
  41 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  42 #include &quot;runtime/jniHandles.inline.hpp&quot;
  43 #include &quot;runtime/objectMonitor.hpp&quot;
  44 #include &quot;runtime/os.inline.hpp&quot;
  45 #include &quot;runtime/safepoint.hpp&quot;
  46 #include &quot;runtime/safepointMechanism.hpp&quot;
  47 #include &quot;runtime/sharedRuntime.hpp&quot;
  48 #include &quot;runtime/stubRoutines.hpp&quot;
  49 #include &quot;utilities/align.hpp&quot;
  50 #include &quot;utilities/macros.hpp&quot;
<span class="line-added">  51 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  52 #ifdef COMPILER2
  53 #include &quot;opto/intrinsicnode.hpp&quot;
  54 #endif
  55 
  56 #ifdef PRODUCT
  57 #define BLOCK_COMMENT(str) /* nothing */
  58 #define STOP(error) stop(error)
  59 #else
  60 #define BLOCK_COMMENT(str) block_comment(str)
  61 #define STOP(error) block_comment(error); stop(error)
  62 #endif
  63 
  64 // Convert the raw encoding form into the form expected by the
  65 // constructor for Address.
  66 Address Address::make_raw(int base, int index, int scale, int disp, relocInfo::relocType disp_reloc) {
  67   assert(scale == 0, &quot;not supported&quot;);
  68   RelocationHolder rspec;
  69   if (disp_reloc != relocInfo::none) {
  70     rspec = Relocation::spec_simple(disp_reloc);
  71   }
</pre>
<hr />
<pre>
 974 AddressLiteral MacroAssembler::allocate_metadata_address(Metadata* obj) {
 975   assert(oop_recorder() != NULL, &quot;this assembler needs a Recorder&quot;);
 976   int index = oop_recorder()-&gt;allocate_metadata_index(obj);
 977   RelocationHolder rspec = metadata_Relocation::spec(index);
 978   return AddressLiteral((address)obj, rspec);
 979 }
 980 
 981 AddressLiteral MacroAssembler::constant_metadata_address(Metadata* obj) {
 982   assert(oop_recorder() != NULL, &quot;this assembler needs a Recorder&quot;);
 983   int index = oop_recorder()-&gt;find_index(obj);
 984   RelocationHolder rspec = metadata_Relocation::spec(index);
 985   return AddressLiteral((address)obj, rspec);
 986 }
 987 
 988 
 989 AddressLiteral MacroAssembler::constant_oop_address(jobject obj) {
 990 #ifdef ASSERT
 991   {
 992     ThreadInVMfromUnknown tiv;
 993     assert(oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
<span class="line-modified"> 994     assert(Universe::heap()-&gt;is_in(JNIHandles::resolve(obj)), &quot;not an oop&quot;);</span>
 995   }
 996 #endif
 997   int oop_index = oop_recorder()-&gt;find_index(obj);
 998   return AddressLiteral(obj, oop_Relocation::spec(oop_index));
 999 }
1000 
1001 void  MacroAssembler::set_narrow_oop(jobject obj, Register d) {
1002   assert(oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
1003   int oop_index = oop_recorder()-&gt;find_index(obj);
1004   RelocationHolder rspec = oop_Relocation::spec(oop_index);
1005 
1006   assert_not_delayed();
1007   // Relocation with special format (see relocInfo_sparc.hpp).
1008   relocate(rspec, 1);
1009   // Assembler::sethi(0x3fffff, d);
1010   emit_int32( op(branch_op) | rd(d) | op2(sethi_op2) | hi22(0x3fffff) );
1011   // Don&#39;t add relocation for &#39;add&#39;. Do patching during &#39;sethi&#39; processing.
1012   add(d, 0x3ff, d);
1013 
1014 }
1015 
1016 void  MacroAssembler::set_narrow_klass(Klass* k, Register d) {
1017   assert(oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
1018   int klass_index = oop_recorder()-&gt;find_index(k);
1019   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
<span class="line-modified">1020   narrowOop encoded_k = CompressedKlassPointers::encode(k);</span>
1021 
1022   assert_not_delayed();
1023   // Relocation with special format (see relocInfo_sparc.hpp).
1024   relocate(rspec, 1);
1025   // Assembler::sethi(encoded_k, d);
1026   emit_int32( op(branch_op) | rd(d) | op2(sethi_op2) | hi22(encoded_k) );
1027   // Don&#39;t add relocation for &#39;add&#39;. Do patching during &#39;sethi&#39; processing.
1028   add(d, low10(encoded_k), d);
1029 
1030 }
1031 
1032 void MacroAssembler::align(int modulus) {
1033   while (offset() % modulus != 0) nop();
1034 }
1035 
1036 void RegistersForDebugging::print(outputStream* s) {
1037   FlagSetting fs(Debugging, true);
1038   int j;
1039   for (j = 0; j &lt; 8; ++j) {
1040     if (j != 6) { s-&gt;print(&quot;i%d = &quot;, j); os::print_location(s, i[j]); }
</pre>
<hr />
<pre>
1114   for (i = 0;  i &lt; 32; ++i) {
1115     a-&gt;stf(FloatRegisterImpl::S, as_FloatRegister(i), O0, f_offset(i));
1116   }
1117   for (i = 0; i &lt; 64; i += 2) {
1118     a-&gt;stf(FloatRegisterImpl::D, as_FloatRegister(i), O0, d_offset(i));
1119   }
1120 }
1121 
1122 void RegistersForDebugging::restore_registers(MacroAssembler* a, Register r) {
1123   for (int i = 1; i &lt; 8;  ++i) {
1124     a-&gt;ld_ptr(r, g_offset(i), as_gRegister(i));
1125   }
1126   for (int j = 0; j &lt; 32; ++j) {
1127     a-&gt;ldf(FloatRegisterImpl::S, O0, f_offset(j), as_FloatRegister(j));
1128   }
1129   for (int k = 0; k &lt; 64; k += 2) {
1130     a-&gt;ldf(FloatRegisterImpl::D, O0, d_offset(k), as_FloatRegister(k));
1131   }
1132 }
1133 















1134 void MacroAssembler::_verify_oop(Register reg, const char* msg, const char * file, int line) {
1135   // plausibility check for oops
1136   if (!VerifyOops) return;
1137 
1138   if (reg == G0)  return;       // always NULL, which is always an oop
1139 
1140   BLOCK_COMMENT(&quot;verify_oop {&quot;);
1141   char buffer[64];
1142 #ifdef COMPILER1
1143   if (CommentedAssembly) {
1144     snprintf(buffer, sizeof(buffer), &quot;verify_oop at %d&quot;, offset());
1145     block_comment(buffer);
1146   }
1147 #endif
1148 
1149   const char* real_msg = NULL;
1150   {
1151     ResourceMark rm;
1152     stringStream ss;
1153     ss.print(&quot;%s at offset %d (%s:%d)&quot;, msg, offset(), file, line);
</pre>
<hr />
<pre>
2421 
2422 void MacroAssembler::biased_locking_enter(Register obj_reg, Register mark_reg,
2423                                           Register temp_reg,
2424                                           Label&amp; done, Label* slow_case,
2425                                           BiasedLockingCounters* counters) {
2426   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
2427 
2428   if (PrintBiasedLockingStatistics) {
2429     assert_different_registers(obj_reg, mark_reg, temp_reg, O7);
2430     if (counters == NULL)
2431       counters = BiasedLocking::counters();
2432   }
2433 
2434   Label cas_label;
2435 
2436   // Biased locking
2437   // See whether the lock is currently biased toward our thread and
2438   // whether the epoch is still valid
2439   // Note that the runtime guarantees sufficient alignment of JavaThread
2440   // pointers to allow age to be placed into low bits
<span class="line-modified">2441   assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits, &quot;biased locking makes assumptions about bit layout&quot;);</span>
<span class="line-modified">2442   and3(mark_reg, markWord::biased_lock_mask_in_place, temp_reg);</span>
<span class="line-modified">2443   cmp_and_brx_short(temp_reg, markWord::biased_lock_pattern, Assembler::notEqual, Assembler::pn, cas_label);</span>
2444 
2445   load_klass(obj_reg, temp_reg);
2446   ld_ptr(Address(temp_reg, Klass::prototype_header_offset()), temp_reg);
2447   or3(G2_thread, temp_reg, temp_reg);
2448   xor3(mark_reg, temp_reg, temp_reg);
<span class="line-modified">2449   andcc(temp_reg, ~((int) markWord::age_mask_in_place), temp_reg);</span>
2450   if (counters != NULL) {
2451     cond_inc(Assembler::equal, (address) counters-&gt;biased_lock_entry_count_addr(), mark_reg, temp_reg);
2452     // Reload mark_reg as we may need it later
2453     ld_ptr(Address(obj_reg, oopDesc::mark_offset_in_bytes()), mark_reg);
2454   }
2455   brx(Assembler::equal, true, Assembler::pt, done);
2456   delayed()-&gt;nop();
2457 
2458   Label try_revoke_bias;
2459   Label try_rebias;
2460   Address mark_addr = Address(obj_reg, oopDesc::mark_offset_in_bytes());
2461   assert(mark_addr.disp() == 0, &quot;cas must take a zero displacement&quot;);
2462 
2463   // At this point we know that the header has the bias pattern and
2464   // that we are not the bias owner in the current epoch. We need to
2465   // figure out more details about the state of the header in order to
2466   // know what operations can be legally performed on the object&#39;s
2467   // header.
2468 
2469   // If the low three bits in the xor result aren&#39;t clear, that means
2470   // the prototype header is no longer biased and we have to revoke
2471   // the bias on this object.
<span class="line-modified">2472   btst(markWord::biased_lock_mask_in_place, temp_reg);</span>
2473   brx(Assembler::notZero, false, Assembler::pn, try_revoke_bias);
2474 
2475   // Biasing is still enabled for this data type. See whether the
2476   // epoch of the current bias is still valid, meaning that the epoch
2477   // bits of the mark word are equal to the epoch bits of the
2478   // prototype header. (Note that the prototype header&#39;s epoch bits
2479   // only change at a safepoint.) If not, attempt to rebias the object
2480   // toward the current thread. Note that we must be absolutely sure
2481   // that the current epoch is invalid in order to do this because
2482   // otherwise the manipulations it performs on the mark word are
2483   // illegal.
<span class="line-modified">2484   delayed()-&gt;btst(markWord::epoch_mask_in_place, temp_reg);</span>
2485   brx(Assembler::notZero, false, Assembler::pn, try_rebias);
2486 
2487   // The epoch of the current bias is still valid but we know nothing
2488   // about the owner; it might be set or it might be clear. Try to
2489   // acquire the bias of the object using an atomic operation. If this
2490   // fails we will go in to the runtime to revoke the object&#39;s bias.
2491   // Note that we first construct the presumed unbiased header so we
2492   // don&#39;t accidentally blow away another thread&#39;s valid bias.
2493   delayed()-&gt;and3(mark_reg,
<span class="line-modified">2494                   markWord::biased_lock_mask_in_place | markWord::age_mask_in_place | markWord::epoch_mask_in_place,</span>
2495                   mark_reg);
2496   or3(G2_thread, mark_reg, temp_reg);
2497   cas_ptr(mark_addr.base(), mark_reg, temp_reg);
2498   // If the biasing toward our thread failed, this means that
2499   // another thread succeeded in biasing it toward itself and we
2500   // need to revoke that bias. The revocation will occur in the
2501   // interpreter runtime in the slow case.
2502   cmp(mark_reg, temp_reg);
2503   if (counters != NULL) {
2504     cond_inc(Assembler::zero, (address) counters-&gt;anonymously_biased_lock_entry_count_addr(), mark_reg, temp_reg);
2505   }
2506   if (slow_case != NULL) {
2507     brx(Assembler::notEqual, true, Assembler::pn, *slow_case);
2508     delayed()-&gt;nop();
2509   }
2510   ba_short(done);
2511 
2512   bind(try_rebias);
2513   // At this point we know the epoch has expired, meaning that the
2514   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
</pre>
<hr />
<pre>
2555   // Fall through to the normal CAS-based lock, because no matter what
2556   // the result of the above CAS, some thread must have succeeded in
2557   // removing the bias bit from the object&#39;s header.
2558   if (counters != NULL) {
2559     cmp(mark_reg, temp_reg);
2560     cond_inc(Assembler::zero, (address) counters-&gt;revoked_lock_entry_count_addr(), mark_reg, temp_reg);
2561   }
2562 
2563   bind(cas_label);
2564 }
2565 
2566 void MacroAssembler::biased_locking_exit (Address mark_addr, Register temp_reg, Label&amp; done,
2567                                           bool allow_delay_slot_filling) {
2568   // Check for biased locking unlock case, which is a no-op
2569   // Note: we do not have to check the thread ID for two reasons.
2570   // First, the interpreter checks for IllegalMonitorStateException at
2571   // a higher level. Second, if the bias was revoked while we held the
2572   // lock, the object could not be rebiased toward another thread, so
2573   // the bias bit would be clear.
2574   ld_ptr(mark_addr, temp_reg);
<span class="line-modified">2575   and3(temp_reg, markWord::biased_lock_mask_in_place, temp_reg);</span>
<span class="line-modified">2576   cmp(temp_reg, markWord::biased_lock_pattern);</span>
2577   brx(Assembler::equal, allow_delay_slot_filling, Assembler::pt, done);
2578   delayed();
2579   if (!allow_delay_slot_filling) {
2580     nop();
2581   }
2582 }
2583 
2584 
2585 // compiler_lock_object() and compiler_unlock_object() are direct transliterations
2586 // of i486.ad fast_lock() and fast_unlock().  See those methods for detailed comments.
2587 // The code could be tightened up considerably.
2588 //
2589 // box-&gt;dhw disposition - post-conditions at DONE_LABEL.
2590 // -   Successful inflated lock:  box-&gt;dhw != 0.
2591 //     Any non-zero value suffices.
<span class="line-modified">2592 //     Consider G2_thread, rsp, boxReg, or markWord::unused_mark()</span>
2593 // -   Successful Stack-lock: box-&gt;dhw == mark.
2594 //     box-&gt;dhw must contain the displaced mark word value
2595 // -   Failure -- icc.ZFlag == 0 and box-&gt;dhw is undefined.
<span class="line-modified">2596 //     The slow-path enter() is responsible for setting</span>
<span class="line-modified">2597 //     box-&gt;dhw = NonZero (typically markWord::unused_mark()).</span>
2598 // -   Biased: box-&gt;dhw is undefined
2599 //
2600 // SPARC refworkload performance - specifically jetstream and scimark - are
2601 // extremely sensitive to the size of the code emitted by compiler_lock_object
2602 // and compiler_unlock_object.  Critically, the key factor is code size, not path
2603 // length.  (Simply experiments to pad CLO with unexecuted NOPs demonstrte the
2604 // effect).
2605 
2606 
2607 void MacroAssembler::compiler_lock_object(Register Roop, Register Rmark,
2608                                           Register Rbox, Register Rscratch,
2609                                           BiasedLockingCounters* counters,
2610                                           bool try_bias) {
2611    Address mark_addr(Roop, oopDesc::mark_offset_in_bytes());
2612 
2613    verify_oop(Roop);
2614    Label done ;
2615 
2616    if (counters != NULL) {
2617      inc_counter((address) counters-&gt;total_entry_count_addr(), Rmark, Rscratch);
</pre>
<hr />
<pre>
2627    ld_ptr(mark_addr, Rmark);           // fetch obj-&gt;mark
2628    // Triage: biased, stack-locked, neutral, inflated
2629 
2630    if (try_bias) {
2631      biased_locking_enter(Roop, Rmark, Rscratch, done, NULL, counters);
2632      // Invariant: if control reaches this point in the emitted stream
2633      // then Rmark has not been modified.
2634    }
2635    andcc(Rmark, 2, G0);
2636    brx(Assembler::notZero, false, Assembler::pn, IsInflated);
2637    delayed()-&gt;                         // Beware - dangling delay-slot
2638 
2639    // Try stack-lock acquisition.
2640    // Transiently install BUSY (0) encoding in the mark word.
2641    // if the CAS of 0 into the mark was successful then we execute:
2642    //   ST box-&gt;dhw  = mark   -- save fetched mark in on-stack basiclock box
2643    //   ST obj-&gt;mark = box    -- overwrite transient 0 value
2644    // This presumes TSO, of course.
2645 
2646    mov(0, Rscratch);
<span class="line-modified">2647    or3(Rmark, markWord::unlocked_value, Rmark);</span>
2648    assert(mark_addr.disp() == 0, &quot;cas must take a zero displacement&quot;);
2649    cas_ptr(mark_addr.base(), Rmark, Rscratch);
2650 // prefetch (mark_addr, Assembler::severalWritesAndPossiblyReads);
2651    cmp(Rscratch, Rmark);
2652    brx(Assembler::notZero, false, Assembler::pn, Recursive);
2653    delayed()-&gt;st_ptr(Rmark, Rbox, BasicLock::displaced_header_offset_in_bytes());
2654    if (counters != NULL) {
2655      cond_inc(Assembler::equal, (address) counters-&gt;fast_path_entry_count_addr(), Rmark, Rscratch);
2656    }
2657    ba(done);
2658    delayed()-&gt;st_ptr(Rbox, mark_addr);
2659 
2660    bind(Recursive);
2661    // Stack-lock attempt failed - check for recursive stack-lock.
2662    // Tests show that we can remove the recursive case with no impact
2663    // on refworkload 0.83.  If we need to reduce the size of the code
2664    // emitted by compiler_lock_object() the recursive case is perfect
2665    // candidate.
2666    //
2667    // A more extreme idea is to always inflate on stack-lock recursion.
</pre>
<hr />
<pre>
2681      // Accounting needs the Rscratch register
2682      st_ptr(Rscratch, Rbox, BasicLock::displaced_header_offset_in_bytes());
2683      cond_inc(Assembler::equal, (address) counters-&gt;fast_path_entry_count_addr(), Rmark, Rscratch);
2684      ba_short(done);
2685    } else {
2686      ba(done);
2687      delayed()-&gt;st_ptr(Rscratch, Rbox, BasicLock::displaced_header_offset_in_bytes());
2688    }
2689 
2690    bind   (IsInflated);
2691 
2692    // Try to CAS m-&gt;owner from null to Self
2693    // Invariant: if we acquire the lock then _recursions should be 0.
2694    add(Rmark, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner), Rmark);
2695    mov(G2_thread, Rscratch);
2696    cas_ptr(Rmark, G0, Rscratch);
2697    andcc(Rscratch, Rscratch, G0);             // set ICCs for done: icc.zf iff success
2698    // set icc.zf : 1=success 0=failure
2699    // ST box-&gt;displaced_header = NonZero.
2700    // Any non-zero value suffices:
<span class="line-modified">2701    //    markWord::unused_mark(), G2_thread, RBox, RScratch, rsp, etc.</span>
2702    st_ptr(Rbox, Rbox, BasicLock::displaced_header_offset_in_bytes());
2703    // Intentional fall-through into done
2704 
2705    bind   (done);
2706 }
2707 
2708 void MacroAssembler::compiler_unlock_object(Register Roop, Register Rmark,
2709                                             Register Rbox, Register Rscratch,
2710                                             bool try_bias) {
2711    Address mark_addr(Roop, oopDesc::mark_offset_in_bytes());
2712 
2713    Label done ;
2714 
2715    // Beware ... If the aggregate size of the code emitted by CLO and CUO is
2716    // is too large performance rolls abruptly off a cliff.
2717    // This could be related to inlining policies, code cache management, or
2718    // I$ effects.
2719    Label LStacked ;
2720 
2721    if (try_bias) {
</pre>
<hr />
<pre>
2795    // to O and O is stack-locked by T1.  The &quot;stomp&quot; race could cause
2796    // an assigned hashCode value to be lost.  We can avoid that condition
2797    // and provide the necessary hashCode stability invariants by ensuring
2798    // that hashCode generation is idempotent between copying GCs.
2799    // For example we could compute the hashCode of an object O as
2800    // O&#39;s heap address XOR some high quality RNG value that is refreshed
2801    // at GC-time.  The monitor scavenger would install the hashCode
2802    // found in any orphan monitors.  Again, the mechanism admits a
2803    // lost-update &quot;stomp&quot; WAW race but detects and recovers as needed.
2804    //
2805    // A prototype implementation showed excellent results, although
2806    // the scavenger and timeout code was rather involved.
2807 
2808    cas_ptr(mark_addr.base(), Rbox, Rscratch);
2809    cmp(Rbox, Rscratch);
2810    // Intentional fall through into done ...
2811 
2812    bind(done);
2813 }
2814 









































2815 void MacroAssembler::verify_tlab() {
2816 #ifdef ASSERT
2817   if (UseTLAB &amp;&amp; VerifyOops) {
2818     Label next, next2, ok;
2819     Register t1 = L0;
2820     Register t2 = L1;
2821     Register t3 = L2;
2822 
2823     save_frame(0);
2824     ld_ptr(G2_thread, in_bytes(JavaThread::tlab_top_offset()), t1);
2825     ld_ptr(G2_thread, in_bytes(JavaThread::tlab_start_offset()), t2);
2826     or3(t1, t2, t3);
2827     cmp_and_br_short(t1, t2, Assembler::greaterEqual, Assembler::pn, next);
2828     STOP(&quot;assert(top &gt;= start)&quot;);
2829     should_not_reach_here();
2830 
2831     bind(next);
2832     ld_ptr(G2_thread, in_bytes(JavaThread::tlab_top_offset()), t1);
2833     ld_ptr(G2_thread, in_bytes(JavaThread::tlab_end_offset()), t2);
2834     or3(t3, t2, t3);
</pre>
<hr />
<pre>
3224 }
3225 
3226 void MacroAssembler::store_heap_oop(Register d, Register s1, int simm13a, Register tmp, DecoratorSet decorators) {
3227   access_store_at(T_OBJECT, IN_HEAP | decorators, d, Address(s1, simm13a), tmp);
3228 }
3229 
3230 void MacroAssembler::store_heap_oop(Register d, const Address&amp; a, int offset, Register tmp, DecoratorSet decorators) {
3231   if (a.has_index()) {
3232     assert(!a.has_disp(), &quot;not supported yet&quot;);
3233     assert(offset == 0, &quot;not supported yet&quot;);
3234     access_store_at(T_OBJECT, IN_HEAP | decorators, d, Address(a.base(), a.index()), tmp);
3235   } else {
3236     access_store_at(T_OBJECT, IN_HEAP | decorators, d, Address(a.base(), a.disp() + offset), tmp);
3237   }
3238 }
3239 
3240 
3241 void MacroAssembler::encode_heap_oop(Register src, Register dst) {
3242   assert (UseCompressedOops, &quot;must be compressed&quot;);
3243   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
<span class="line-modified">3244   assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
3245   verify_oop(src);
<span class="line-modified">3246   if (CompressedOops::base() == NULL) {</span>
3247     srlx(src, LogMinObjAlignmentInBytes, dst);
3248     return;
3249   }
3250   Label done;
3251   if (src == dst) {
3252     // optimize for frequent case src == dst
3253     bpr(rc_nz, true, Assembler::pt, src, done);
3254     delayed() -&gt; sub(src, G6_heapbase, dst); // annuled if not taken
3255     bind(done);
3256     srlx(src, LogMinObjAlignmentInBytes, dst);
3257   } else {
3258     bpr(rc_z, false, Assembler::pn, src, done);
3259     delayed() -&gt; mov(G0, dst);
3260     // could be moved before branch, and annulate delay,
3261     // but may add some unneeded work decoding null
3262     sub(src, G6_heapbase, dst);
3263     srlx(dst, LogMinObjAlignmentInBytes, dst);
3264     bind(done);
3265   }
3266 }
3267 
3268 
3269 void MacroAssembler::encode_heap_oop_not_null(Register r) {
3270   assert (UseCompressedOops, &quot;must be compressed&quot;);
3271   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
<span class="line-modified">3272   assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
3273   verify_oop(r);
<span class="line-modified">3274   if (CompressedOops::base() != NULL)</span>
3275     sub(r, G6_heapbase, r);
3276   srlx(r, LogMinObjAlignmentInBytes, r);
3277 }
3278 
3279 void MacroAssembler::encode_heap_oop_not_null(Register src, Register dst) {
3280   assert (UseCompressedOops, &quot;must be compressed&quot;);
3281   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
<span class="line-modified">3282   assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
3283   verify_oop(src);
<span class="line-modified">3284   if (CompressedOops::base() == NULL) {</span>
3285     srlx(src, LogMinObjAlignmentInBytes, dst);
3286   } else {
3287     sub(src, G6_heapbase, dst);
3288     srlx(dst, LogMinObjAlignmentInBytes, dst);
3289   }
3290 }
3291 
3292 // Same algorithm as oops.inline.hpp decode_heap_oop.
3293 void  MacroAssembler::decode_heap_oop(Register src, Register dst) {
3294   assert (UseCompressedOops, &quot;must be compressed&quot;);
3295   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
<span class="line-modified">3296   assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
3297   sllx(src, LogMinObjAlignmentInBytes, dst);
<span class="line-modified">3298   if (CompressedOops::base() != NULL) {</span>
3299     Label done;
3300     bpr(rc_nz, true, Assembler::pt, dst, done);
3301     delayed() -&gt; add(dst, G6_heapbase, dst); // annuled if not taken
3302     bind(done);
3303   }
3304   verify_oop(dst);
3305 }
3306 
3307 void  MacroAssembler::decode_heap_oop_not_null(Register r) {
3308   // Do not add assert code to this unless you change vtableStubs_sparc.cpp
3309   // pd_code_size_limit.
3310   // Also do not verify_oop as this is called by verify_oop.
3311   assert (UseCompressedOops, &quot;must be compressed&quot;);
3312   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
<span class="line-modified">3313   assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
3314   sllx(r, LogMinObjAlignmentInBytes, r);
<span class="line-modified">3315   if (CompressedOops::base() != NULL)</span>
3316     add(r, G6_heapbase, r);
3317 }
3318 
3319 void  MacroAssembler::decode_heap_oop_not_null(Register src, Register dst) {
3320   // Do not add assert code to this unless you change vtableStubs_sparc.cpp
3321   // pd_code_size_limit.
3322   // Also do not verify_oop as this is called by verify_oop.
3323   assert (UseCompressedOops, &quot;must be compressed&quot;);
<span class="line-modified">3324   assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
3325   sllx(src, LogMinObjAlignmentInBytes, dst);
<span class="line-modified">3326   if (CompressedOops::base() != NULL)</span>
3327     add(dst, G6_heapbase, dst);
3328 }
3329 
3330 void MacroAssembler::encode_klass_not_null(Register r) {
3331   assert (UseCompressedClassPointers, &quot;must be compressed&quot;);
<span class="line-modified">3332   if (CompressedKlassPointers::base() != NULL) {</span>
3333     assert(r != G6_heapbase, &quot;bad register choice&quot;);
<span class="line-modified">3334     set((intptr_t)CompressedKlassPointers::base(), G6_heapbase);</span>
3335     sub(r, G6_heapbase, r);
<span class="line-modified">3336     if (CompressedKlassPointers::shift() != 0) {</span>
<span class="line-modified">3337       assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);</span>
3338       srlx(r, LogKlassAlignmentInBytes, r);
3339     }
3340     reinit_heapbase();
3341   } else {
<span class="line-modified">3342     assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift() || CompressedKlassPointers::shift() == 0, &quot;decode alg wrong&quot;);</span>
<span class="line-modified">3343     srlx(r, CompressedKlassPointers::shift(), r);</span>
3344   }
3345 }
3346 
3347 void MacroAssembler::encode_klass_not_null(Register src, Register dst) {
3348   if (src == dst) {
3349     encode_klass_not_null(src);
3350   } else {
3351     assert (UseCompressedClassPointers, &quot;must be compressed&quot;);
<span class="line-modified">3352     if (CompressedKlassPointers::base() != NULL) {</span>
<span class="line-modified">3353       set((intptr_t)CompressedKlassPointers::base(), dst);</span>
3354       sub(src, dst, dst);
<span class="line-modified">3355       if (CompressedKlassPointers::shift() != 0) {</span>
3356         srlx(dst, LogKlassAlignmentInBytes, dst);
3357       }
3358     } else {
3359       // shift src into dst
<span class="line-modified">3360       assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift() || CompressedKlassPointers::shift() == 0, &quot;decode alg wrong&quot;);</span>
<span class="line-modified">3361       srlx(src, CompressedKlassPointers::shift(), dst);</span>
3362     }
3363   }
3364 }
3365 
3366 // Function instr_size_for_decode_klass_not_null() counts the instructions
3367 // generated by decode_klass_not_null() and reinit_heapbase().  Hence, if
3368 // the instructions they generate change, then this method needs to be updated.
3369 int MacroAssembler::instr_size_for_decode_klass_not_null() {
3370   assert (UseCompressedClassPointers, &quot;only for compressed klass ptrs&quot;);
3371   int num_instrs = 1;  // shift src,dst or add
<span class="line-modified">3372   if (CompressedKlassPointers::base() != NULL) {</span>
3373     // set + add + set
<span class="line-modified">3374     num_instrs += insts_for_internal_set((intptr_t)CompressedKlassPointers::base()) +</span>
<span class="line-modified">3375                   insts_for_internal_set((intptr_t)CompressedOops::ptrs_base());</span>
<span class="line-modified">3376     if (CompressedKlassPointers::shift() != 0) {</span>
3377       num_instrs += 1;  // sllx
3378     }
3379   }
3380   return num_instrs * BytesPerInstWord;
3381 }
3382 
3383 // !!! If the instructions that get generated here change then function
3384 // instr_size_for_decode_klass_not_null() needs to get updated.
3385 void  MacroAssembler::decode_klass_not_null(Register r) {
3386   // Do not add assert code to this unless you change vtableStubs_sparc.cpp
3387   // pd_code_size_limit.
3388   assert (UseCompressedClassPointers, &quot;must be compressed&quot;);
<span class="line-modified">3389   if (CompressedKlassPointers::base() != NULL) {</span>
3390     assert(r != G6_heapbase, &quot;bad register choice&quot;);
<span class="line-modified">3391     set((intptr_t)CompressedKlassPointers::base(), G6_heapbase);</span>
<span class="line-modified">3392     if (CompressedKlassPointers::shift() != 0)</span>
3393       sllx(r, LogKlassAlignmentInBytes, r);
3394     add(r, G6_heapbase, r);
3395     reinit_heapbase();
3396   } else {
<span class="line-modified">3397     assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift() || CompressedKlassPointers::shift() == 0, &quot;decode alg wrong&quot;);</span>
<span class="line-modified">3398     sllx(r, CompressedKlassPointers::shift(), r);</span>
3399   }
3400 }
3401 
3402 void  MacroAssembler::decode_klass_not_null(Register src, Register dst) {
3403   if (src == dst) {
3404     decode_klass_not_null(src);
3405   } else {
3406     // Do not add assert code to this unless you change vtableStubs_sparc.cpp
3407     // pd_code_size_limit.
3408     assert (UseCompressedClassPointers, &quot;must be compressed&quot;);
<span class="line-modified">3409     if (CompressedKlassPointers::base() != NULL) {</span>
<span class="line-modified">3410       if (CompressedKlassPointers::shift() != 0) {</span>
3411         assert((src != G6_heapbase) &amp;&amp; (dst != G6_heapbase), &quot;bad register choice&quot;);
<span class="line-modified">3412         set((intptr_t)CompressedKlassPointers::base(), G6_heapbase);</span>
3413         sllx(src, LogKlassAlignmentInBytes, dst);
3414         add(dst, G6_heapbase, dst);
3415         reinit_heapbase();
3416       } else {
<span class="line-modified">3417         set((intptr_t)CompressedKlassPointers::base(), dst);</span>
3418         add(src, dst, dst);
3419       }
3420     } else {
3421       // shift/mov src into dst.
<span class="line-modified">3422       assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift() || CompressedKlassPointers::shift() == 0, &quot;decode alg wrong&quot;);</span>
<span class="line-modified">3423       sllx(src, CompressedKlassPointers::shift(), dst);</span>
3424     }
3425   }
3426 }
3427 
3428 void MacroAssembler::reinit_heapbase() {
3429   if (UseCompressedOops || UseCompressedClassPointers) {
3430     if (Universe::heap() != NULL) {
<span class="line-modified">3431       set((intptr_t)CompressedOops::ptrs_base(), G6_heapbase);</span>
3432     } else {
<span class="line-modified">3433       AddressLiteral base(CompressedOops::ptrs_base_addr());</span>
3434       load_ptr_contents(base, G6_heapbase);
3435     }
3436   }
3437 }
3438 
3439 #ifdef COMPILER2
3440 
3441 // Compress char[] to byte[] by compressing 16 bytes at once. Return 0 on failure.
3442 void MacroAssembler::string_compress_16(Register src, Register dst, Register cnt, Register result,
3443                                         Register tmp1, Register tmp2, Register tmp3, Register tmp4,
3444                                         FloatRegister ftmp1, FloatRegister ftmp2, FloatRegister ftmp3, Label&amp; Ldone) {
3445   Label Lloop, Lslow;
3446   assert(UseVIS &gt;= 3, &quot;VIS3 is required&quot;);
3447   assert_different_registers(src, dst, cnt, tmp1, tmp2, tmp3, tmp4, result);
3448   assert_different_registers(ftmp1, ftmp2, ftmp3);
3449 
3450   // Check if cnt &gt;= 8 (= 16 bytes)
3451   cmp(cnt, 8);
3452   br(Assembler::less, false, Assembler::pn, Lslow);
3453   delayed()-&gt;mov(cnt, result); // copy count
</pre>
</td>
</tr>
</table>
<center><a href="jvmciCodeInstaller_sparc.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_sparc.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>