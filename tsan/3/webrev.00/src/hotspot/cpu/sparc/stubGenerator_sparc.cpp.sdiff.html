<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/sparc/stubGenerator_sparc.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="sparc.ad.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="templateTable_sparc.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/sparc/stubGenerator_sparc.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 568     address start = __ pc();
 569 
 570     __ stop_subroutine();
 571 
 572     return start;
 573   }
 574 
 575   address generate_flush_callers_register_windows() {
 576     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;flush_callers_register_windows&quot;);
 577     address start = __ pc();
 578 
 579     __ flushw();
 580     __ retl(false);
 581     __ delayed()-&gt;add( FP, STACK_BIAS, O0 );
 582     // The returned value must be a stack pointer whose register save area
 583     // is flushed, and will stay flushed while the caller executes.
 584 
 585     return start;
 586   }
 587 
<span class="line-modified"> 588   // Support for jint Atomic::xchg(jint exchange_value, volatile jint* dest).</span>

 589   //
 590   // Arguments:
 591   //
 592   //      exchange_value: O0
 593   //      dest:           O1
 594   //
 595   // Results:
 596   //
 597   //     O0: the value previously stored in dest
 598   //
 599   address generate_atomic_xchg() {
 600     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_xchg&quot;);
 601     address start = __ pc();
 602 
 603     if (UseCASForSwap) {
 604       // Use CAS instead of swap, just in case the MP hardware
 605       // prefers to work with just one kind of synch. instruction.
 606       Label retry;
 607       __ BIND(retry);
 608       __ mov(O0, O3);       // scratch copy of exchange value
 609       __ ld(O1, 0, O2);     // observe the previous value
 610       // try to replace O2 with O3
 611       __ cas(O1, O2, O3);
 612       __ cmp_and_br_short(O2, O3, Assembler::notEqual, Assembler::pn, retry);
 613 
 614       __ retl(false);
 615       __ delayed()-&gt;mov(O2, O0);  // report previous value to caller
 616     } else {
 617       __ retl(false);
 618       __ delayed()-&gt;swap(O1, 0, O0);
 619     }
 620 
 621     return start;
 622   }
 623 
 624 
<span class="line-modified"> 625   // Support for jint Atomic::cmpxchg(jint exchange_value, volatile jint* dest, jint compare_value)</span>

 626   //
 627   // Arguments:
 628   //
 629   //      exchange_value: O0
 630   //      dest:           O1
 631   //      compare_value:  O2
 632   //
 633   // Results:
 634   //
 635   //     O0: the value previously stored in dest
 636   //
 637   address generate_atomic_cmpxchg() {
 638     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_cmpxchg&quot;);
 639     address start = __ pc();
 640 
 641     // cmpxchg(dest, compare_value, exchange_value)
 642     __ cas(O1, O2, O0);
 643     __ retl(false);
 644     __ delayed()-&gt;nop();
 645 
 646     return start;
 647   }
 648 
<span class="line-modified"> 649   // Support for jlong Atomic::cmpxchg(jlong exchange_value, volatile jlong *dest, jlong compare_value)</span>

 650   //
 651   // Arguments:
 652   //
 653   //      exchange_value: O1:O0
 654   //      dest:           O2
 655   //      compare_value:  O4:O3
 656   //
 657   // Results:
 658   //
 659   //     O1:O0: the value previously stored in dest
 660   //
 661   // Overwrites: G1,G2,G3
 662   //
 663   address generate_atomic_cmpxchg_long() {
 664     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_cmpxchg_long&quot;);
 665     address start = __ pc();
 666 
 667     __ sllx(O0, 32, O0);
 668     __ srl(O1, 0, O1);
 669     __ or3(O0,O1,O0);      // O0 holds 64-bit value from compare_value
 670     __ sllx(O3, 32, O3);
 671     __ srl(O4, 0, O4);
 672     __ or3(O3,O4,O3);     // O3 holds 64-bit value from exchange_value
 673     __ casx(O2, O3, O0);
 674     __ srl(O0, 0, O1);    // unpacked return value in O1:O0
 675     __ retl(false);
 676     __ delayed()-&gt;srlx(O0, 32, O0);
 677 
 678     return start;
 679   }
 680 
 681 
<span class="line-modified"> 682   // Support for jint Atomic::add(jint add_value, volatile jint* dest).</span>

 683   //
 684   // Arguments:
 685   //
 686   //      add_value: O0   (e.g., +1 or -1)
 687   //      dest:      O1
 688   //
 689   // Results:
 690   //
 691   //     O0: the new value stored in dest
 692   //
 693   // Overwrites: O3
 694   //
 695   address generate_atomic_add() {
 696     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_add&quot;);
 697     address start = __ pc();
 698     __ BIND(_atomic_add_stub);
 699 
 700     Label(retry);
 701     __ BIND(retry);
 702 
</pre>
<hr />
<pre>
1059 
1060       __ inccc(count, count_dec&gt;&gt;1 ); // + 8 bytes
1061       __ brx(Assembler::negative, true, Assembler::pn, L_copy_last_bytes);
1062       __ delayed()-&gt;inc(count, count_dec&gt;&gt;1); // restore &#39;count&#39;
1063 
1064       // copy 8 bytes, part of them already loaded in O3
1065       __ ldx(end_from, -8, O4);
1066       __ dec(end_to, 8);
1067       __ dec(end_from, 8);
1068       __ srlx(O3, right_shift, O3);
1069       __ sllx(O4, left_shift,  G3);
1070       __ bset(O3, G3);
1071       __ stx(G3, end_to, 0);
1072 
1073     __ BIND(L_copy_last_bytes);
1074       __ srl(left_shift, LogBitsPerByte, left_shift);    // misaligned bytes
1075       __ br(Assembler::always, false, Assembler::pt, L_copy_bytes);
1076       __ delayed()-&gt;add(end_from, left_shift, end_from); // restore address
1077   }
1078 











1079   //
1080   //  Generate stub for disjoint byte copy.  If &quot;aligned&quot; is true, the
1081   //  &quot;from&quot; and &quot;to&quot; addresses are assumed to be heapword aligned.
1082   //
1083   // Arguments for generated stub:
1084   //      from:  O0
1085   //      to:    O1
1086   //      count: O2 treated as signed
1087   //
1088   address generate_disjoint_byte_copy(bool aligned, address *entry, const char *name) {
1089     __ align(CodeEntryAlignment);
1090     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1091     address start = __ pc();
1092 
1093     Label L_skip_alignment, L_align;
1094     Label L_copy_byte, L_copy_byte_loop, L_exit;
1095 
1096     const Register from      = O0;   // source array address
1097     const Register to        = O1;   // destination array address
1098     const Register count     = O2;   // elements count
1099     const Register offset    = O5;   // offset from start of arrays
1100     // O3, O4, G3, G4 are used as temp registers
1101 
1102     assert_clean_int(count, O3);     // Make sure &#39;count&#39; is clean int.
1103 
1104     if (entry != NULL) {
1105       *entry = __ pc();
1106       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1107       BLOCK_COMMENT(&quot;Entry:&quot;);
1108     }
1109 
<span class="line-modified">1110     // for short arrays, just do single element copy</span>
<span class="line-modified">1111     __ cmp(count, 23); // 16 + 7</span>
<span class="line-modified">1112     __ brx(Assembler::less, false, Assembler::pn, L_copy_byte);</span>
<span class="line-removed">1113     __ delayed()-&gt;mov(G0, offset);</span>
1114 
<span class="line-modified">1115     if (aligned) {</span>
<span class="line-modified">1116       // &#39;aligned&#39; == true when it is known statically during compilation</span>
<span class="line-modified">1117       // of this arraycopy call site that both &#39;from&#39; and &#39;to&#39; addresses</span>
<span class="line-modified">1118       // are HeapWordSize aligned (see LibraryCallKit::basictype2arraycopy()).</span>
<span class="line-removed">1119       //</span>
<span class="line-removed">1120       // Aligned arrays have 4 bytes alignment in 32-bits VM</span>
<span class="line-removed">1121       // and 8 bytes - in 64-bits VM. So we do it only for 32-bits VM</span>
<span class="line-removed">1122       //</span>
<span class="line-removed">1123     } else {</span>
<span class="line-removed">1124       // copy bytes to align &#39;to&#39; on 8 byte boundary</span>
<span class="line-removed">1125       __ andcc(to, 7, G1); // misaligned bytes</span>
<span class="line-removed">1126       __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);</span>
<span class="line-removed">1127       __ delayed()-&gt;neg(G1);</span>
<span class="line-removed">1128       __ inc(G1, 8);       // bytes need to copy to next 8-bytes alignment</span>
<span class="line-removed">1129       __ sub(count, G1, count);</span>
<span class="line-removed">1130     __ BIND(L_align);</span>
<span class="line-removed">1131       __ ldub(from, 0, O3);</span>
<span class="line-removed">1132       __ deccc(G1);</span>
<span class="line-removed">1133       __ inc(from);</span>
<span class="line-removed">1134       __ stb(O3, to, 0);</span>
<span class="line-removed">1135       __ br(Assembler::notZero, false, Assembler::pt, L_align);</span>
<span class="line-removed">1136       __ delayed()-&gt;inc(to);</span>
<span class="line-removed">1137     __ BIND(L_skip_alignment);</span>
<span class="line-removed">1138     }</span>
<span class="line-removed">1139     if (!aligned) {</span>
<span class="line-removed">1140       // Copy with shift 16 bytes per iteration if arrays do not have</span>
<span class="line-removed">1141       // the same alignment mod 8, otherwise fall through to the next</span>
<span class="line-removed">1142       // code for aligned copy.</span>
<span class="line-removed">1143       // The compare above (count &gt;= 23) guarantes &#39;count&#39; &gt;= 16 bytes.</span>
<span class="line-removed">1144       // Also jump over aligned copy after the copy with shift completed.</span>
1145 
<span class="line-modified">1146       copy_16_bytes_forward_with_shift(from, to, count, 0, L_copy_byte);</span>
<span class="line-modified">1147     }</span>































1148 
<span class="line-modified">1149     // Both array are 8 bytes aligned, copy 16 bytes at a time</span>
1150       __ and3(count, 7, G4); // Save count
1151       __ srl(count, 3, count);
<span class="line-modified">1152      generate_disjoint_long_copy_core(aligned);</span>
1153       __ mov(G4, count);     // Restore count
1154 
<span class="line-modified">1155     // copy tailing bytes</span>
<span class="line-modified">1156     __ BIND(L_copy_byte);</span>
<span class="line-modified">1157       __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);</span>
<span class="line-modified">1158       __ align(OptoLoopAlignment);</span>
<span class="line-modified">1159     __ BIND(L_copy_byte_loop);</span>
<span class="line-modified">1160       __ ldub(from, offset, O3);</span>
<span class="line-modified">1161       __ deccc(count);</span>
<span class="line-modified">1162       __ stb(O3, to, offset);</span>
<span class="line-modified">1163       __ brx(Assembler::notZero, false, Assembler::pt, L_copy_byte_loop);</span>
<span class="line-modified">1164       __ delayed()-&gt;inc(offset);</span>

1165 
1166     __ BIND(L_exit);
1167       // O3, O4 are used as temp registers
1168       inc_counter_np(SharedRuntime::_jbyte_array_copy_ctr, O3, O4);
1169       __ retl();
1170       __ delayed()-&gt;mov(G0, O0); // return 0
1171     return start;
1172   }
1173 
1174   //
1175   //  Generate stub for conjoint byte copy.  If &quot;aligned&quot; is true, the
1176   //  &quot;from&quot; and &quot;to&quot; addresses are assumed to be heapword aligned.
1177   //
1178   // Arguments for generated stub:
1179   //      from:  O0
1180   //      to:    O1
1181   //      count: O2 treated as signed
1182   //
1183   address generate_conjoint_byte_copy(bool aligned, address nooverlap_target,
1184                                       address *entry, const char *name) {
</pre>
<hr />
<pre>
1190 
1191     Label L_skip_alignment, L_align, L_aligned_copy;
1192     Label L_copy_byte, L_copy_byte_loop, L_exit;
1193 
1194     const Register from      = O0;   // source array address
1195     const Register to        = O1;   // destination array address
1196     const Register count     = O2;   // elements count
1197     const Register end_from  = from; // source array end address
1198     const Register end_to    = to;   // destination array end address
1199 
1200     assert_clean_int(count, O3);     // Make sure &#39;count&#39; is clean int.
1201 
1202     if (entry != NULL) {
1203       *entry = __ pc();
1204       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1205       BLOCK_COMMENT(&quot;Entry:&quot;);
1206     }
1207 
1208     array_overlap_test(nooverlap_target, 0);
1209 
<span class="line-modified">1210     __ add(to, count, end_to);       // offset after last copied element</span>


1211 
<span class="line-modified">1212     // for short arrays, just do single element copy</span>
<span class="line-removed">1213     __ cmp(count, 23); // 16 + 7</span>
<span class="line-removed">1214     __ brx(Assembler::less, false, Assembler::pn, L_copy_byte);</span>
<span class="line-removed">1215     __ delayed()-&gt;add(from, count, end_from);</span>
1216 
<span class="line-modified">1217     {</span>
<span class="line-modified">1218       // Align end of arrays since they could be not aligned even</span>
<span class="line-modified">1219       // when arrays itself are aligned.</span>

1220 
<span class="line-modified">1221       // copy bytes to align &#39;end_to&#39; on 8 byte boundary</span>
<span class="line-modified">1222       __ andcc(end_to, 7, G1); // misaligned bytes</span>
<span class="line-modified">1223       __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);</span>
<span class="line-removed">1224       __ delayed()-&gt;nop();</span>
<span class="line-removed">1225       __ sub(count, G1, count);</span>
<span class="line-removed">1226     __ BIND(L_align);</span>
<span class="line-removed">1227       __ dec(end_from);</span>
<span class="line-removed">1228       __ dec(end_to);</span>
<span class="line-removed">1229       __ ldub(end_from, 0, O3);</span>
<span class="line-removed">1230       __ deccc(G1);</span>
<span class="line-removed">1231       __ brx(Assembler::notZero, false, Assembler::pt, L_align);</span>
<span class="line-removed">1232       __ delayed()-&gt;stb(O3, end_to, 0);</span>
<span class="line-removed">1233     __ BIND(L_skip_alignment);</span>
<span class="line-removed">1234     }</span>
<span class="line-removed">1235     if (aligned) {</span>
<span class="line-removed">1236       // Both arrays are aligned to 8-bytes in 64-bits VM.</span>
<span class="line-removed">1237       // The &#39;count&#39; is decremented in copy_16_bytes_backward_with_shift()</span>
<span class="line-removed">1238       // in unaligned case.</span>
<span class="line-removed">1239       __ dec(count, 16);</span>
<span class="line-removed">1240     } else {</span>
<span class="line-removed">1241       // Copy with shift 16 bytes per iteration if arrays do not have</span>
<span class="line-removed">1242       // the same alignment mod 8, otherwise jump to the next</span>
<span class="line-removed">1243       // code for aligned copy (and substracting 16 from &#39;count&#39; before jump).</span>
<span class="line-removed">1244       // The compare above (count &gt;= 11) guarantes &#39;count&#39; &gt;= 16 bytes.</span>
<span class="line-removed">1245       // Also jump over aligned copy after the copy with shift completed.</span>
1246 
<span class="line-modified">1247       copy_16_bytes_backward_with_shift(end_from, end_to, count, 16,</span>
<span class="line-modified">1248                                         L_aligned_copy, L_copy_byte);</span>



















































1249     }
<span class="line-removed">1250     // copy 4 elements (16 bytes) at a time</span>
<span class="line-removed">1251       __ align(OptoLoopAlignment);</span>
<span class="line-removed">1252     __ BIND(L_aligned_copy);</span>
<span class="line-removed">1253       __ dec(end_from, 16);</span>
<span class="line-removed">1254       __ ldx(end_from, 8, O3);</span>
<span class="line-removed">1255       __ ldx(end_from, 0, O4);</span>
<span class="line-removed">1256       __ dec(end_to, 16);</span>
<span class="line-removed">1257       __ deccc(count, 16);</span>
<span class="line-removed">1258       __ stx(O3, end_to, 8);</span>
<span class="line-removed">1259       __ brx(Assembler::greaterEqual, false, Assembler::pt, L_aligned_copy);</span>
<span class="line-removed">1260       __ delayed()-&gt;stx(O4, end_to, 0);</span>
<span class="line-removed">1261       __ inc(count, 16);</span>
<span class="line-removed">1262 </span>
<span class="line-removed">1263     // copy 1 element (2 bytes) at a time</span>
<span class="line-removed">1264     __ BIND(L_copy_byte);</span>
<span class="line-removed">1265       __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);</span>
<span class="line-removed">1266       __ align(OptoLoopAlignment);</span>
<span class="line-removed">1267     __ BIND(L_copy_byte_loop);</span>
<span class="line-removed">1268       __ dec(end_from);</span>
<span class="line-removed">1269       __ dec(end_to);</span>
<span class="line-removed">1270       __ ldub(end_from, 0, O4);</span>
<span class="line-removed">1271       __ deccc(count);</span>
<span class="line-removed">1272       __ brx(Assembler::greater, false, Assembler::pt, L_copy_byte_loop);</span>
<span class="line-removed">1273       __ delayed()-&gt;stb(O4, end_to, 0);</span>
1274 
1275     __ BIND(L_exit);
1276     // O3, O4 are used as temp registers
1277     inc_counter_np(SharedRuntime::_jbyte_array_copy_ctr, O3, O4);
1278     __ retl();
1279     __ delayed()-&gt;mov(G0, O0); // return 0
1280     return start;
1281   }
1282 
1283   //
1284   //  Generate stub for disjoint short copy.  If &quot;aligned&quot; is true, the
1285   //  &quot;from&quot; and &quot;to&quot; addresses are assumed to be heapword aligned.
1286   //
1287   // Arguments for generated stub:
1288   //      from:  O0
1289   //      to:    O1
1290   //      count: O2 treated as signed
1291   //
1292   address generate_disjoint_short_copy(bool aligned, address *entry, const char * name) {
1293     __ align(CodeEntryAlignment);
1294     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1295     address start = __ pc();
1296 
1297     Label L_skip_alignment, L_skip_alignment2;
1298     Label L_copy_2_bytes, L_copy_2_bytes_loop, L_exit;
1299 
1300     const Register from      = O0;   // source array address
1301     const Register to        = O1;   // destination array address
1302     const Register count     = O2;   // elements count
1303     const Register offset    = O5;   // offset from start of arrays
1304     // O3, O4, G3, G4 are used as temp registers
1305 
1306     assert_clean_int(count, O3);     // Make sure &#39;count&#39; is clean int.
1307 
1308     if (entry != NULL) {
1309       *entry = __ pc();
1310       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1311       BLOCK_COMMENT(&quot;Entry:&quot;);
1312     }
1313 
<span class="line-modified">1314     // for short arrays, just do single element copy</span>
<span class="line-modified">1315     __ cmp(count, 11); // 8 + 3  (22 bytes)</span>
<span class="line-modified">1316     __ brx(Assembler::less, false, Assembler::pn, L_copy_2_bytes);</span>
<span class="line-modified">1317     __ delayed()-&gt;mov(G0, offset);</span>



1318 
<span class="line-modified">1319     if (aligned) {</span>
<span class="line-modified">1320       // &#39;aligned&#39; == true when it is known statically during compilation</span>
<span class="line-modified">1321       // of this arraycopy call site that both &#39;from&#39; and &#39;to&#39; addresses</span>
<span class="line-modified">1322       // are HeapWordSize aligned (see LibraryCallKit::basictype2arraycopy()).</span>
<span class="line-modified">1323       //</span>
<span class="line-modified">1324       // Aligned arrays have 4 bytes alignment in 32-bits VM</span>
<span class="line-modified">1325       // and 8 bytes - in 64-bits VM.</span>
<span class="line-modified">1326       //</span>
<span class="line-modified">1327     } else {</span>
<span class="line-modified">1328       // copy 1 element if necessary to align &#39;to&#39; on an 4 bytes</span>
<span class="line-modified">1329       __ andcc(to, 3, G0);</span>
<span class="line-modified">1330       __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);</span>
<span class="line-modified">1331       __ delayed()-&gt;lduh(from, 0, O3);</span>
<span class="line-modified">1332       __ inc(from, 2);</span>
<span class="line-modified">1333       __ inc(to, 2);</span>
<span class="line-modified">1334       __ dec(count);</span>
<span class="line-modified">1335       __ sth(O3, to, -2);</span>
<span class="line-modified">1336     __ BIND(L_skip_alignment);</span>






















1337 
<span class="line-modified">1338       // copy 2 elements to align &#39;to&#39; on an 8 byte boundary</span>
<span class="line-modified">1339       __ andcc(to, 7, G0);</span>
<span class="line-modified">1340       __ br(Assembler::zero, false, Assembler::pn, L_skip_alignment2);</span>
<span class="line-modified">1341       __ delayed()-&gt;lduh(from, 0, O3);</span>
<span class="line-modified">1342       __ dec(count, 2);</span>
<span class="line-removed">1343       __ lduh(from, 2, O4);</span>
<span class="line-removed">1344       __ inc(from, 4);</span>
<span class="line-removed">1345       __ inc(to, 4);</span>
<span class="line-removed">1346       __ sth(O3, to, -4);</span>
<span class="line-removed">1347       __ sth(O4, to, -2);</span>
<span class="line-removed">1348     __ BIND(L_skip_alignment2);</span>
<span class="line-removed">1349     }</span>
<span class="line-removed">1350     if (!aligned) {</span>
<span class="line-removed">1351       // Copy with shift 16 bytes per iteration if arrays do not have</span>
<span class="line-removed">1352       // the same alignment mod 8, otherwise fall through to the next</span>
<span class="line-removed">1353       // code for aligned copy.</span>
<span class="line-removed">1354       // The compare above (count &gt;= 11) guarantes &#39;count&#39; &gt;= 16 bytes.</span>
<span class="line-removed">1355       // Also jump over aligned copy after the copy with shift completed.</span>
1356 
<span class="line-modified">1357       copy_16_bytes_forward_with_shift(from, to, count, 1, L_copy_2_bytes);</span>









1358     }
1359 
<span class="line-removed">1360     // Both array are 8 bytes aligned, copy 16 bytes at a time</span>
<span class="line-removed">1361       __ and3(count, 3, G4); // Save</span>
<span class="line-removed">1362       __ srl(count, 2, count);</span>
<span class="line-removed">1363      generate_disjoint_long_copy_core(aligned);</span>
<span class="line-removed">1364       __ mov(G4, count); // restore</span>
<span class="line-removed">1365 </span>
<span class="line-removed">1366     // copy 1 element at a time</span>
<span class="line-removed">1367     __ BIND(L_copy_2_bytes);</span>
<span class="line-removed">1368       __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);</span>
<span class="line-removed">1369       __ align(OptoLoopAlignment);</span>
<span class="line-removed">1370     __ BIND(L_copy_2_bytes_loop);</span>
<span class="line-removed">1371       __ lduh(from, offset, O3);</span>
<span class="line-removed">1372       __ deccc(count);</span>
<span class="line-removed">1373       __ sth(O3, to, offset);</span>
<span class="line-removed">1374       __ brx(Assembler::notZero, false, Assembler::pt, L_copy_2_bytes_loop);</span>
<span class="line-removed">1375       __ delayed()-&gt;inc(offset, 2);</span>
<span class="line-removed">1376 </span>
1377     __ BIND(L_exit);
1378       // O3, O4 are used as temp registers
1379       inc_counter_np(SharedRuntime::_jshort_array_copy_ctr, O3, O4);
1380       __ retl();
1381       __ delayed()-&gt;mov(G0, O0); // return 0
1382     return start;
1383   }
1384 
1385   //
1386   //  Generate stub for disjoint short fill.  If &quot;aligned&quot; is true, the
1387   //  &quot;to&quot; address is assumed to be heapword aligned.
1388   //
1389   // Arguments for generated stub:
1390   //      to:    O0
1391   //      value: O1
1392   //      count: O2 treated as signed
1393   //
1394   address generate_fill(BasicType t, bool aligned, const char* name) {
1395     __ align(CodeEntryAlignment);
1396     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
</pre>
<hr />
<pre>
1622     Label L_copy_2_bytes, L_copy_2_bytes_loop, L_exit;
1623 
1624     const Register from      = O0;   // source array address
1625     const Register to        = O1;   // destination array address
1626     const Register count     = O2;   // elements count
1627     const Register end_from  = from; // source array end address
1628     const Register end_to    = to;   // destination array end address
1629 
1630     const Register byte_count = O3;  // bytes count to copy
1631 
1632     assert_clean_int(count, O3);     // Make sure &#39;count&#39; is clean int.
1633 
1634     if (entry != NULL) {
1635       *entry = __ pc();
1636       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1637       BLOCK_COMMENT(&quot;Entry:&quot;);
1638     }
1639 
1640     array_overlap_test(nooverlap_target, 1);
1641 
<span class="line-removed">1642     __ sllx(count, LogBytesPerShort, byte_count);</span>
<span class="line-removed">1643     __ add(to, byte_count, end_to);  // offset after last copied element</span>
<span class="line-removed">1644 </span>
<span class="line-removed">1645     // for short arrays, just do single element copy</span>
<span class="line-removed">1646     __ cmp(count, 11); // 8 + 3  (22 bytes)</span>
<span class="line-removed">1647     __ brx(Assembler::less, false, Assembler::pn, L_copy_2_bytes);</span>
<span class="line-removed">1648     __ delayed()-&gt;add(from, byte_count, end_from);</span>
<span class="line-removed">1649 </span>
1650     {
<span class="line-modified">1651       // Align end of arrays since they could be not aligned even</span>
<span class="line-modified">1652       // when arrays itself are aligned.</span>
1653 
<span class="line-modified">1654       // copy 1 element if necessary to align &#39;end_to&#39; on an 4 bytes</span>
<span class="line-modified">1655       __ andcc(end_to, 3, G0);</span>
<span class="line-removed">1656       __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);</span>
<span class="line-removed">1657       __ delayed()-&gt;lduh(end_from, -2, O3);</span>
<span class="line-removed">1658       __ dec(end_from, 2);</span>
<span class="line-removed">1659       __ dec(end_to, 2);</span>
<span class="line-removed">1660       __ dec(count);</span>
<span class="line-removed">1661       __ sth(O3, end_to, 0);</span>
<span class="line-removed">1662     __ BIND(L_skip_alignment);</span>
1663 
<span class="line-modified">1664       // copy 2 elements to align &#39;end_to&#39; on an 8 byte boundary</span>
<span class="line-modified">1665       __ andcc(end_to, 7, G0);</span>
<span class="line-modified">1666       __ br(Assembler::zero, false, Assembler::pn, L_skip_alignment2);</span>
<span class="line-modified">1667       __ delayed()-&gt;lduh(end_from, -2, O3);</span>
<span class="line-modified">1668       __ dec(count, 2);</span>
<span class="line-modified">1669       __ lduh(end_from, -4, O4);</span>
<span class="line-modified">1670       __ dec(end_from, 4);</span>
<span class="line-modified">1671       __ dec(end_to, 4);</span>
<span class="line-modified">1672       __ sth(O3, end_to, 2);</span>
<span class="line-modified">1673       __ sth(O4, end_to, 0);</span>
<span class="line-modified">1674     __ BIND(L_skip_alignment2);</span>
<span class="line-modified">1675     }</span>
<span class="line-modified">1676     if (aligned) {</span>
<span class="line-modified">1677       // Both arrays are aligned to 8-bytes in 64-bits VM.</span>
<span class="line-modified">1678       // The &#39;count&#39; is decremented in copy_16_bytes_backward_with_shift()</span>
<span class="line-modified">1679       // in unaligned case.</span>
<span class="line-modified">1680       __ dec(count, 8);</span>
<span class="line-modified">1681     } else {</span>
<span class="line-modified">1682       // Copy with shift 16 bytes per iteration if arrays do not have</span>
<span class="line-modified">1683       // the same alignment mod 8, otherwise jump to the next</span>
<span class="line-modified">1684       // code for aligned copy (and substracting 8 from &#39;count&#39; before jump).</span>
<span class="line-modified">1685       // The compare above (count &gt;= 11) guarantes &#39;count&#39; &gt;= 16 bytes.</span>
<span class="line-modified">1686       // Also jump over aligned copy after the copy with shift completed.</span>



















1687 
<span class="line-modified">1688       copy_16_bytes_backward_with_shift(end_from, end_to, count, 8,</span>
1689                                         L_aligned_copy, L_copy_2_bytes);
























1690     }
<span class="line-removed">1691     // copy 4 elements (16 bytes) at a time</span>
<span class="line-removed">1692       __ align(OptoLoopAlignment);</span>
<span class="line-removed">1693     __ BIND(L_aligned_copy);</span>
<span class="line-removed">1694       __ dec(end_from, 16);</span>
<span class="line-removed">1695       __ ldx(end_from, 8, O3);</span>
<span class="line-removed">1696       __ ldx(end_from, 0, O4);</span>
<span class="line-removed">1697       __ dec(end_to, 16);</span>
<span class="line-removed">1698       __ deccc(count, 8);</span>
<span class="line-removed">1699       __ stx(O3, end_to, 8);</span>
<span class="line-removed">1700       __ brx(Assembler::greaterEqual, false, Assembler::pt, L_aligned_copy);</span>
<span class="line-removed">1701       __ delayed()-&gt;stx(O4, end_to, 0);</span>
<span class="line-removed">1702       __ inc(count, 8);</span>
<span class="line-removed">1703 </span>
<span class="line-removed">1704     // copy 1 element (2 bytes) at a time</span>
<span class="line-removed">1705     __ BIND(L_copy_2_bytes);</span>
<span class="line-removed">1706       __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);</span>
<span class="line-removed">1707     __ BIND(L_copy_2_bytes_loop);</span>
<span class="line-removed">1708       __ dec(end_from, 2);</span>
<span class="line-removed">1709       __ dec(end_to, 2);</span>
<span class="line-removed">1710       __ lduh(end_from, 0, O4);</span>
<span class="line-removed">1711       __ deccc(count);</span>
<span class="line-removed">1712       __ brx(Assembler::greater, false, Assembler::pt, L_copy_2_bytes_loop);</span>
<span class="line-removed">1713       __ delayed()-&gt;sth(O4, end_to, 0);</span>
<span class="line-removed">1714 </span>
1715     __ BIND(L_exit);
1716     // O3, O4 are used as temp registers
1717     inc_counter_np(SharedRuntime::_jshort_array_copy_ctr, O3, O4);
1718     __ retl();
1719     __ delayed()-&gt;mov(G0, O0); // return 0
1720     return start;
1721   }
1722 
1723   //
1724   // Helper methods for generate_disjoint_int_copy_core()
1725   //
1726   void copy_16_bytes_loop(Register from, Register to, Register count, int count_dec,
1727                           Label&amp; L_loop, bool use_prefetch, bool use_bis) {
1728 
1729     __ align(OptoLoopAlignment);
1730     __ BIND(L_loop);
1731     if (use_prefetch) {
1732       if (ArraycopySrcPrefetchDistance &gt; 0) {
1733         __ prefetch(from, ArraycopySrcPrefetchDistance, Assembler::severalReads);
1734       }
</pre>
<hr />
<pre>
1853   //  &quot;from&quot; and &quot;to&quot; addresses are assumed to be heapword aligned.
1854   //
1855   // Arguments for generated stub:
1856   //      from:  O0
1857   //      to:    O1
1858   //      count: O2 treated as signed
1859   //
1860   address generate_disjoint_int_copy(bool aligned, address *entry, const char *name) {
1861     __ align(CodeEntryAlignment);
1862     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1863     address start = __ pc();
1864 
1865     const Register count = O2;
1866     assert_clean_int(count, O3);     // Make sure &#39;count&#39; is clean int.
1867 
1868     if (entry != NULL) {
1869       *entry = __ pc();
1870       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1871       BLOCK_COMMENT(&quot;Entry:&quot;);
1872     }
<span class="line-modified">1873 </span>
<span class="line-modified">1874     generate_disjoint_int_copy_core(aligned);</span>
<span class="line-modified">1875 </span>


1876     // O3, O4 are used as temp registers
1877     inc_counter_np(SharedRuntime::_jint_array_copy_ctr, O3, O4);
1878     __ retl();
1879     __ delayed()-&gt;mov(G0, O0); // return 0
1880     return start;
1881   }
1882 
1883   //
1884   //  Generate core code for conjoint int copy (and oop copy on 32-bit).
1885   //  If &quot;aligned&quot; is true, the &quot;from&quot; and &quot;to&quot; addresses are assumed
1886   //  to be heapword aligned.
1887   //
1888   // Arguments:
1889   //      from:  O0
1890   //      to:    O1
1891   //      count: O2 treated as signed
1892   //
1893   void generate_conjoint_int_copy_core(bool aligned) {
1894     // Do reverse copy.
1895 
</pre>
<hr />
<pre>
1988   // Arguments for generated stub:
1989   //      from:  O0
1990   //      to:    O1
1991   //      count: O2 treated as signed
1992   //
1993   address generate_conjoint_int_copy(bool aligned, address nooverlap_target,
1994                                      address *entry, const char *name) {
1995     __ align(CodeEntryAlignment);
1996     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1997     address start = __ pc();
1998 
1999     assert_clean_int(O2, O3);     // Make sure &#39;count&#39; is clean int.
2000 
2001     if (entry != NULL) {
2002       *entry = __ pc();
2003       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
2004       BLOCK_COMMENT(&quot;Entry:&quot;);
2005     }
2006 
2007     array_overlap_test(nooverlap_target, 2);
<span class="line-modified">2008 </span>
<span class="line-modified">2009     generate_conjoint_int_copy_core(aligned);</span>
<span class="line-modified">2010 </span>


2011     // O3, O4 are used as temp registers
2012     inc_counter_np(SharedRuntime::_jint_array_copy_ctr, O3, O4);
2013     __ retl();
2014     __ delayed()-&gt;mov(G0, O0); // return 0
2015     return start;
2016   }
2017 
2018   //
2019   // Helper methods for generate_disjoint_long_copy_core()
2020   //
2021   void copy_64_bytes_loop(Register from, Register to, Register count, int count_dec,
2022                           Label&amp; L_loop, bool use_prefetch, bool use_bis) {
2023     __ align(OptoLoopAlignment);
2024     __ BIND(L_loop);
2025     for (int off = 0; off &lt; 64; off += 16) {
2026       if (use_prefetch &amp;&amp; (off &amp; 31) == 0) {
2027         if (ArraycopySrcPrefetchDistance &gt; 0) {
2028           __ prefetch(from, ArraycopySrcPrefetchDistance+off, Assembler::severalReads);
2029         }
2030         if (ArraycopyDstPrefetchDistance &gt; 0) {
</pre>
<hr />
<pre>
2139   //  assumption that both addresses are always 64-bit aligned.
2140   //
2141   // Arguments for generated stub:
2142   //      from:  O0
2143   //      to:    O1
2144   //      count: O2 treated as signed
2145   //
2146   address generate_disjoint_long_copy(bool aligned, address *entry, const char *name) {
2147     __ align(CodeEntryAlignment);
2148     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2149     address start = __ pc();
2150 
2151     assert_clean_int(O2, O3);     // Make sure &#39;count&#39; is clean int.
2152 
2153     if (entry != NULL) {
2154       *entry = __ pc();
2155       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
2156       BLOCK_COMMENT(&quot;Entry:&quot;);
2157     }
2158 
<span class="line-modified">2159     generate_disjoint_long_copy_core(aligned);</span>
<span class="line-modified">2160 </span>



2161     // O3, O4 are used as temp registers
2162     inc_counter_np(SharedRuntime::_jlong_array_copy_ctr, O3, O4);
2163     __ retl();
2164     __ delayed()-&gt;mov(G0, O0); // return 0
2165     return start;
2166   }
2167 
2168   //
2169   //  Generate core code for conjoint long copy (and oop copy on 64-bit).
2170   //  &quot;aligned&quot; is ignored, because we must make the stronger
2171   //  assumption that both addresses are always 64-bit aligned.
2172   //
2173   // Arguments:
2174   //      from:  O0
2175   //      to:    O1
2176   //      count: O2 treated as signed
2177   //
2178   void generate_conjoint_long_copy_core(bool aligned) {
2179     // Do reverse copy.
2180     Label L_copy_8_bytes, L_copy_16_bytes, L_exit;
</pre>
<hr />
<pre>
2215   //      to:    O1
2216   //      count: O2 treated as signed
2217   //
2218   address generate_conjoint_long_copy(bool aligned, address nooverlap_target,
2219                                       address *entry, const char *name) {
2220     __ align(CodeEntryAlignment);
2221     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2222     address start = __ pc();
2223 
2224     assert(aligned, &quot;Should always be aligned&quot;);
2225 
2226     assert_clean_int(O2, O3);     // Make sure &#39;count&#39; is clean int.
2227 
2228     if (entry != NULL) {
2229       *entry = __ pc();
2230       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
2231       BLOCK_COMMENT(&quot;Entry:&quot;);
2232     }
2233 
2234     array_overlap_test(nooverlap_target, 3);
<span class="line-modified">2235 </span>
<span class="line-modified">2236     generate_conjoint_long_copy_core(aligned);</span>
<span class="line-modified">2237 </span>


2238     // O3, O4 are used as temp registers
2239     inc_counter_np(SharedRuntime::_jlong_array_copy_ctr, O3, O4);
2240     __ retl();
2241     __ delayed()-&gt;mov(G0, O0); // return 0
2242     return start;
2243   }
2244 
2245   //  Generate stub for disjoint oop copy.  If &quot;aligned&quot; is true, the
2246   //  &quot;from&quot; and &quot;to&quot; addresses are assumed to be heapword aligned.
2247   //
2248   // Arguments for generated stub:
2249   //      from:  O0
2250   //      to:    O1
2251   //      count: O2 treated as signed
2252   //
2253   address generate_disjoint_oop_copy(bool aligned, address *entry, const char *name,
2254                                      bool dest_uninitialized = false) {
2255 
2256     const Register from  = O0;  // source array address
2257     const Register to    = O1;  // destination array address
</pre>
<hr />
<pre>
2912 
2913     Label Ldone;
2914     __ sllx(count, LogHeapWordSize, count); // to bytes count
2915     // Use BIS for zeroing
2916     __ bis_zeroing(to, count, temp, Ldone);
2917     __ bind(Ldone);
2918     __ retl();
2919     __ delayed()-&gt;nop();
2920     return start;
2921 }
2922 
2923   void generate_arraycopy_stubs() {
2924     address entry;
2925     address entry_jbyte_arraycopy;
2926     address entry_jshort_arraycopy;
2927     address entry_jint_arraycopy;
2928     address entry_oop_arraycopy;
2929     address entry_jlong_arraycopy;
2930     address entry_checkcast_arraycopy;
2931 



2932     //*** jbyte
2933     // Always need aligned and unaligned versions
2934     StubRoutines::_jbyte_disjoint_arraycopy         = generate_disjoint_byte_copy(false, &amp;entry,
2935                                                                                   &quot;jbyte_disjoint_arraycopy&quot;);
2936     StubRoutines::_jbyte_arraycopy                  = generate_conjoint_byte_copy(false, entry,
2937                                                                                   &amp;entry_jbyte_arraycopy,
2938                                                                                   &quot;jbyte_arraycopy&quot;);
2939     StubRoutines::_arrayof_jbyte_disjoint_arraycopy = generate_disjoint_byte_copy(true, &amp;entry,
2940                                                                                   &quot;arrayof_jbyte_disjoint_arraycopy&quot;);
2941     StubRoutines::_arrayof_jbyte_arraycopy          = generate_conjoint_byte_copy(true, entry, NULL,
2942                                                                                   &quot;arrayof_jbyte_arraycopy&quot;);
2943 
2944     //*** jshort
2945     // Always need aligned and unaligned versions
2946     StubRoutines::_jshort_disjoint_arraycopy         = generate_disjoint_short_copy(false, &amp;entry,
2947                                                                                     &quot;jshort_disjoint_arraycopy&quot;);
2948     StubRoutines::_jshort_arraycopy                  = generate_conjoint_short_copy(false, entry,
2949                                                                                     &amp;entry_jshort_arraycopy,
2950                                                                                     &quot;jshort_arraycopy&quot;);
2951     StubRoutines::_arrayof_jshort_disjoint_arraycopy = generate_disjoint_short_copy(true, &amp;entry,
</pre>
<hr />
<pre>
5804 
5805   void align(bool at_header = false) {
5806     // %%%%% move this constant somewhere else
5807     // UltraSPARC cache line size is 8 instructions:
5808     const unsigned int icache_line_size = 32;
5809     const unsigned int icache_half_line_size = 16;
5810 
5811     if (at_header) {
5812       while ((intptr_t)(__ pc()) % icache_line_size != 0) {
5813         __ emit_data(0, relocInfo::none);
5814       }
5815     } else {
5816       while ((intptr_t)(__ pc()) % icache_half_line_size != 0) {
5817         __ nop();
5818       }
5819     }
5820   }
5821 
5822 }; // end class declaration
5823 

5824 void StubGenerator_generate(CodeBuffer* code, bool all) {



5825   StubGenerator g(code, all);
5826 }
</pre>
</td>
<td>
<hr />
<pre>
 568     address start = __ pc();
 569 
 570     __ stop_subroutine();
 571 
 572     return start;
 573   }
 574 
 575   address generate_flush_callers_register_windows() {
 576     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;flush_callers_register_windows&quot;);
 577     address start = __ pc();
 578 
 579     __ flushw();
 580     __ retl(false);
 581     __ delayed()-&gt;add( FP, STACK_BIAS, O0 );
 582     // The returned value must be a stack pointer whose register save area
 583     // is flushed, and will stay flushed while the caller executes.
 584 
 585     return start;
 586   }
 587 
<span class="line-modified"> 588   // Implementation of jint atomic_xchg(jint exchange_value, volatile jint* dest)</span>
<span class="line-added"> 589   // used by Atomic::xchg(volatile jint* dest, jint exchange_value)</span>
 590   //
 591   // Arguments:
 592   //
 593   //      exchange_value: O0
 594   //      dest:           O1
 595   //
 596   // Results:
 597   //
 598   //     O0: the value previously stored in dest
 599   //
 600   address generate_atomic_xchg() {
 601     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_xchg&quot;);
 602     address start = __ pc();
 603 
 604     if (UseCASForSwap) {
 605       // Use CAS instead of swap, just in case the MP hardware
 606       // prefers to work with just one kind of synch. instruction.
 607       Label retry;
 608       __ BIND(retry);
 609       __ mov(O0, O3);       // scratch copy of exchange value
 610       __ ld(O1, 0, O2);     // observe the previous value
 611       // try to replace O2 with O3
 612       __ cas(O1, O2, O3);
 613       __ cmp_and_br_short(O2, O3, Assembler::notEqual, Assembler::pn, retry);
 614 
 615       __ retl(false);
 616       __ delayed()-&gt;mov(O2, O0);  // report previous value to caller
 617     } else {
 618       __ retl(false);
 619       __ delayed()-&gt;swap(O1, 0, O0);
 620     }
 621 
 622     return start;
 623   }
 624 
 625 
<span class="line-modified"> 626   // Implementation of jint atomic_cmpxchg(jint exchange_value, volatile jint* dest, jint compare_value)</span>
<span class="line-added"> 627   // used by Atomic::cmpxchg(volatile jint* dest, jint compare_value, jint exchange_value)</span>
 628   //
 629   // Arguments:
 630   //
 631   //      exchange_value: O0
 632   //      dest:           O1
 633   //      compare_value:  O2
 634   //
 635   // Results:
 636   //
 637   //     O0: the value previously stored in dest
 638   //
 639   address generate_atomic_cmpxchg() {
 640     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_cmpxchg&quot;);
 641     address start = __ pc();
 642 
 643     // cmpxchg(dest, compare_value, exchange_value)
 644     __ cas(O1, O2, O0);
 645     __ retl(false);
 646     __ delayed()-&gt;nop();
 647 
 648     return start;
 649   }
 650 
<span class="line-modified"> 651   // Implementation of jlong atomic_cmpxchg_long(jlong exchange_value, volatile jlong *dest, jlong compare_value)</span>
<span class="line-added"> 652   // used by Atomic::cmpxchg(volatile jlong *dest, jlong compare_value, jlong exchange_value)</span>
 653   //
 654   // Arguments:
 655   //
 656   //      exchange_value: O1:O0
 657   //      dest:           O2
 658   //      compare_value:  O4:O3
 659   //
 660   // Results:
 661   //
 662   //     O1:O0: the value previously stored in dest
 663   //
 664   // Overwrites: G1,G2,G3
 665   //
 666   address generate_atomic_cmpxchg_long() {
 667     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_cmpxchg_long&quot;);
 668     address start = __ pc();
 669 
 670     __ sllx(O0, 32, O0);
 671     __ srl(O1, 0, O1);
 672     __ or3(O0,O1,O0);      // O0 holds 64-bit value from compare_value
 673     __ sllx(O3, 32, O3);
 674     __ srl(O4, 0, O4);
 675     __ or3(O3,O4,O3);     // O3 holds 64-bit value from exchange_value
 676     __ casx(O2, O3, O0);
 677     __ srl(O0, 0, O1);    // unpacked return value in O1:O0
 678     __ retl(false);
 679     __ delayed()-&gt;srlx(O0, 32, O0);
 680 
 681     return start;
 682   }
 683 
 684 
<span class="line-modified"> 685   // Implementation of jint atomic_add(jint add_value, volatile jint* dest)</span>
<span class="line-added"> 686   // used by Atomic::add(volatile jint* dest, jint add_value)</span>
 687   //
 688   // Arguments:
 689   //
 690   //      add_value: O0   (e.g., +1 or -1)
 691   //      dest:      O1
 692   //
 693   // Results:
 694   //
 695   //     O0: the new value stored in dest
 696   //
 697   // Overwrites: O3
 698   //
 699   address generate_atomic_add() {
 700     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_add&quot;);
 701     address start = __ pc();
 702     __ BIND(_atomic_add_stub);
 703 
 704     Label(retry);
 705     __ BIND(retry);
 706 
</pre>
<hr />
<pre>
1063 
1064       __ inccc(count, count_dec&gt;&gt;1 ); // + 8 bytes
1065       __ brx(Assembler::negative, true, Assembler::pn, L_copy_last_bytes);
1066       __ delayed()-&gt;inc(count, count_dec&gt;&gt;1); // restore &#39;count&#39;
1067 
1068       // copy 8 bytes, part of them already loaded in O3
1069       __ ldx(end_from, -8, O4);
1070       __ dec(end_to, 8);
1071       __ dec(end_from, 8);
1072       __ srlx(O3, right_shift, O3);
1073       __ sllx(O4, left_shift,  G3);
1074       __ bset(O3, G3);
1075       __ stx(G3, end_to, 0);
1076 
1077     __ BIND(L_copy_last_bytes);
1078       __ srl(left_shift, LogBitsPerByte, left_shift);    // misaligned bytes
1079       __ br(Assembler::always, false, Assembler::pt, L_copy_bytes);
1080       __ delayed()-&gt;add(end_from, left_shift, end_from); // restore address
1081   }
1082 
<span class="line-added">1083   address generate_unsafecopy_common_error_exit() {</span>
<span class="line-added">1084     address start_pc = __ pc();</span>
<span class="line-added">1085     if (UseBlockCopy) {</span>
<span class="line-added">1086       __ wrasi(G0, Assembler::ASI_PRIMARY_NOFAULT);</span>
<span class="line-added">1087       __ membar(Assembler::StoreLoad);</span>
<span class="line-added">1088     }</span>
<span class="line-added">1089     __ retl();</span>
<span class="line-added">1090     __ delayed()-&gt;mov(G0, O0); // return 0</span>
<span class="line-added">1091     return start_pc;</span>
<span class="line-added">1092   }</span>
<span class="line-added">1093 </span>
1094   //
1095   //  Generate stub for disjoint byte copy.  If &quot;aligned&quot; is true, the
1096   //  &quot;from&quot; and &quot;to&quot; addresses are assumed to be heapword aligned.
1097   //
1098   // Arguments for generated stub:
1099   //      from:  O0
1100   //      to:    O1
1101   //      count: O2 treated as signed
1102   //
1103   address generate_disjoint_byte_copy(bool aligned, address *entry, const char *name) {
1104     __ align(CodeEntryAlignment);
1105     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1106     address start = __ pc();
1107 
1108     Label L_skip_alignment, L_align;
1109     Label L_copy_byte, L_copy_byte_loop, L_exit;
1110 
1111     const Register from      = O0;   // source array address
1112     const Register to        = O1;   // destination array address
1113     const Register count     = O2;   // elements count
1114     const Register offset    = O5;   // offset from start of arrays
1115     // O3, O4, G3, G4 are used as temp registers
1116 
1117     assert_clean_int(count, O3);     // Make sure &#39;count&#39; is clean int.
1118 
1119     if (entry != NULL) {
1120       *entry = __ pc();
1121       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1122       BLOCK_COMMENT(&quot;Entry:&quot;);
1123     }
1124 
<span class="line-modified">1125     {</span>
<span class="line-modified">1126       // UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit</span>
<span class="line-modified">1127       UnsafeCopyMemoryMark ucmm(this, !aligned, false);</span>

1128 
<span class="line-modified">1129       // for short arrays, just do single element copy</span>
<span class="line-modified">1130       __ cmp(count, 23); // 16 + 7</span>
<span class="line-modified">1131       __ brx(Assembler::less, false, Assembler::pn, L_copy_byte);</span>
<span class="line-modified">1132       __ delayed()-&gt;mov(G0, offset);</span>


























1133 
<span class="line-modified">1134       if (aligned) {</span>
<span class="line-modified">1135         // &#39;aligned&#39; == true when it is known statically during compilation</span>
<span class="line-added">1136         // of this arraycopy call site that both &#39;from&#39; and &#39;to&#39; addresses</span>
<span class="line-added">1137         // are HeapWordSize aligned (see LibraryCallKit::basictype2arraycopy()).</span>
<span class="line-added">1138         //</span>
<span class="line-added">1139         // Aligned arrays have 4 bytes alignment in 32-bits VM</span>
<span class="line-added">1140         // and 8 bytes - in 64-bits VM. So we do it only for 32-bits VM</span>
<span class="line-added">1141         //</span>
<span class="line-added">1142       } else {</span>
<span class="line-added">1143         // copy bytes to align &#39;to&#39; on 8 byte boundary</span>
<span class="line-added">1144         __ andcc(to, 7, G1); // misaligned bytes</span>
<span class="line-added">1145         __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);</span>
<span class="line-added">1146         __ delayed()-&gt;neg(G1);</span>
<span class="line-added">1147         __ inc(G1, 8);       // bytes need to copy to next 8-bytes alignment</span>
<span class="line-added">1148         __ sub(count, G1, count);</span>
<span class="line-added">1149       __ BIND(L_align);</span>
<span class="line-added">1150         __ ldub(from, 0, O3);</span>
<span class="line-added">1151         __ deccc(G1);</span>
<span class="line-added">1152         __ inc(from);</span>
<span class="line-added">1153         __ stb(O3, to, 0);</span>
<span class="line-added">1154         __ br(Assembler::notZero, false, Assembler::pt, L_align);</span>
<span class="line-added">1155         __ delayed()-&gt;inc(to);</span>
<span class="line-added">1156       __ BIND(L_skip_alignment);</span>
<span class="line-added">1157       }</span>
<span class="line-added">1158       if (!aligned) {</span>
<span class="line-added">1159         // Copy with shift 16 bytes per iteration if arrays do not have</span>
<span class="line-added">1160         // the same alignment mod 8, otherwise fall through to the next</span>
<span class="line-added">1161         // code for aligned copy.</span>
<span class="line-added">1162         // The compare above (count &gt;= 23) guarantes &#39;count&#39; &gt;= 16 bytes.</span>
<span class="line-added">1163         // Also jump over aligned copy after the copy with shift completed.</span>
<span class="line-added">1164 </span>
<span class="line-added">1165         copy_16_bytes_forward_with_shift(from, to, count, 0, L_copy_byte);</span>
<span class="line-added">1166       }</span>
1167 
<span class="line-modified">1168       // Both array are 8 bytes aligned, copy 16 bytes at a time</span>
1169       __ and3(count, 7, G4); // Save count
1170       __ srl(count, 3, count);
<span class="line-modified">1171       generate_disjoint_long_copy_core(aligned);</span>
1172       __ mov(G4, count);     // Restore count
1173 
<span class="line-modified">1174       // copy tailing bytes</span>
<span class="line-modified">1175       __ BIND(L_copy_byte);</span>
<span class="line-modified">1176         __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);</span>
<span class="line-modified">1177         __ align(OptoLoopAlignment);</span>
<span class="line-modified">1178       __ BIND(L_copy_byte_loop);</span>
<span class="line-modified">1179         __ ldub(from, offset, O3);</span>
<span class="line-modified">1180         __ deccc(count);</span>
<span class="line-modified">1181         __ stb(O3, to, offset);</span>
<span class="line-modified">1182         __ brx(Assembler::notZero, false, Assembler::pt, L_copy_byte_loop);</span>
<span class="line-modified">1183         __ delayed()-&gt;inc(offset);</span>
<span class="line-added">1184     }</span>
1185 
1186     __ BIND(L_exit);
1187       // O3, O4 are used as temp registers
1188       inc_counter_np(SharedRuntime::_jbyte_array_copy_ctr, O3, O4);
1189       __ retl();
1190       __ delayed()-&gt;mov(G0, O0); // return 0
1191     return start;
1192   }
1193 
1194   //
1195   //  Generate stub for conjoint byte copy.  If &quot;aligned&quot; is true, the
1196   //  &quot;from&quot; and &quot;to&quot; addresses are assumed to be heapword aligned.
1197   //
1198   // Arguments for generated stub:
1199   //      from:  O0
1200   //      to:    O1
1201   //      count: O2 treated as signed
1202   //
1203   address generate_conjoint_byte_copy(bool aligned, address nooverlap_target,
1204                                       address *entry, const char *name) {
</pre>
<hr />
<pre>
1210 
1211     Label L_skip_alignment, L_align, L_aligned_copy;
1212     Label L_copy_byte, L_copy_byte_loop, L_exit;
1213 
1214     const Register from      = O0;   // source array address
1215     const Register to        = O1;   // destination array address
1216     const Register count     = O2;   // elements count
1217     const Register end_from  = from; // source array end address
1218     const Register end_to    = to;   // destination array end address
1219 
1220     assert_clean_int(count, O3);     // Make sure &#39;count&#39; is clean int.
1221 
1222     if (entry != NULL) {
1223       *entry = __ pc();
1224       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1225       BLOCK_COMMENT(&quot;Entry:&quot;);
1226     }
1227 
1228     array_overlap_test(nooverlap_target, 0);
1229 
<span class="line-modified">1230     {</span>
<span class="line-added">1231       // UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit</span>
<span class="line-added">1232       UnsafeCopyMemoryMark ucmm(this, !aligned, false);</span>
1233 
<span class="line-modified">1234       __ add(to, count, end_to);       // offset after last copied element</span>



1235 
<span class="line-modified">1236       // for short arrays, just do single element copy</span>
<span class="line-modified">1237       __ cmp(count, 23); // 16 + 7</span>
<span class="line-modified">1238       __ brx(Assembler::less, false, Assembler::pn, L_copy_byte);</span>
<span class="line-added">1239       __ delayed()-&gt;add(from, count, end_from);</span>
1240 
<span class="line-modified">1241       {</span>
<span class="line-modified">1242         // Align end of arrays since they could be not aligned even</span>
<span class="line-modified">1243         // when arrays itself are aligned.</span>






















1244 
<span class="line-modified">1245         // copy bytes to align &#39;end_to&#39; on 8 byte boundary</span>
<span class="line-modified">1246         __ andcc(end_to, 7, G1); // misaligned bytes</span>
<span class="line-added">1247         __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);</span>
<span class="line-added">1248         __ delayed()-&gt;nop();</span>
<span class="line-added">1249         __ sub(count, G1, count);</span>
<span class="line-added">1250       __ BIND(L_align);</span>
<span class="line-added">1251         __ dec(end_from);</span>
<span class="line-added">1252         __ dec(end_to);</span>
<span class="line-added">1253         __ ldub(end_from, 0, O3);</span>
<span class="line-added">1254         __ deccc(G1);</span>
<span class="line-added">1255         __ brx(Assembler::notZero, false, Assembler::pt, L_align);</span>
<span class="line-added">1256         __ delayed()-&gt;stb(O3, end_to, 0);</span>
<span class="line-added">1257       __ BIND(L_skip_alignment);</span>
<span class="line-added">1258       }</span>
<span class="line-added">1259       if (aligned) {</span>
<span class="line-added">1260         // Both arrays are aligned to 8-bytes in 64-bits VM.</span>
<span class="line-added">1261         // The &#39;count&#39; is decremented in copy_16_bytes_backward_with_shift()</span>
<span class="line-added">1262         // in unaligned case.</span>
<span class="line-added">1263         __ dec(count, 16);</span>
<span class="line-added">1264       } else {</span>
<span class="line-added">1265         // Copy with shift 16 bytes per iteration if arrays do not have</span>
<span class="line-added">1266         // the same alignment mod 8, otherwise jump to the next</span>
<span class="line-added">1267         // code for aligned copy (and substracting 16 from &#39;count&#39; before jump).</span>
<span class="line-added">1268         // The compare above (count &gt;= 11) guarantes &#39;count&#39; &gt;= 16 bytes.</span>
<span class="line-added">1269         // Also jump over aligned copy after the copy with shift completed.</span>
<span class="line-added">1270 </span>
<span class="line-added">1271        copy_16_bytes_backward_with_shift(end_from, end_to, count, 16,</span>
<span class="line-added">1272                                           L_aligned_copy, L_copy_byte);</span>
<span class="line-added">1273       }</span>
<span class="line-added">1274       // copy 4 elements (16 bytes) at a time</span>
<span class="line-added">1275         __ align(OptoLoopAlignment);</span>
<span class="line-added">1276       __ BIND(L_aligned_copy);</span>
<span class="line-added">1277         __ dec(end_from, 16);</span>
<span class="line-added">1278         __ ldx(end_from, 8, O3);</span>
<span class="line-added">1279         __ ldx(end_from, 0, O4);</span>
<span class="line-added">1280         __ dec(end_to, 16);</span>
<span class="line-added">1281         __ deccc(count, 16);</span>
<span class="line-added">1282         __ stx(O3, end_to, 8);</span>
<span class="line-added">1283         __ brx(Assembler::greaterEqual, false, Assembler::pt, L_aligned_copy);</span>
<span class="line-added">1284         __ delayed()-&gt;stx(O4, end_to, 0);</span>
<span class="line-added">1285         __ inc(count, 16);</span>
<span class="line-added">1286 </span>
<span class="line-added">1287       // copy 1 element (2 bytes) at a time</span>
<span class="line-added">1288       __ BIND(L_copy_byte);</span>
<span class="line-added">1289         __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);</span>
<span class="line-added">1290         __ align(OptoLoopAlignment);</span>
<span class="line-added">1291       __ BIND(L_copy_byte_loop);</span>
<span class="line-added">1292         __ dec(end_from);</span>
<span class="line-added">1293         __ dec(end_to);</span>
<span class="line-added">1294         __ ldub(end_from, 0, O4);</span>
<span class="line-added">1295         __ deccc(count);</span>
<span class="line-added">1296         __ brx(Assembler::greater, false, Assembler::pt, L_copy_byte_loop);</span>
<span class="line-added">1297         __ delayed()-&gt;stb(O4, end_to, 0);</span>
1298     }
























1299 
1300     __ BIND(L_exit);
1301     // O3, O4 are used as temp registers
1302     inc_counter_np(SharedRuntime::_jbyte_array_copy_ctr, O3, O4);
1303     __ retl();
1304     __ delayed()-&gt;mov(G0, O0); // return 0
1305     return start;
1306   }
1307 
1308   //
1309   //  Generate stub for disjoint short copy.  If &quot;aligned&quot; is true, the
1310   //  &quot;from&quot; and &quot;to&quot; addresses are assumed to be heapword aligned.
1311   //
1312   // Arguments for generated stub:
1313   //      from:  O0
1314   //      to:    O1
1315   //      count: O2 treated as signed
1316   //
1317   address generate_disjoint_short_copy(bool aligned, address *entry, const char * name) {
1318     __ align(CodeEntryAlignment);
1319     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1320     address start = __ pc();
1321 
1322     Label L_skip_alignment, L_skip_alignment2;
1323     Label L_copy_2_bytes, L_copy_2_bytes_loop, L_exit;
1324 
1325     const Register from      = O0;   // source array address
1326     const Register to        = O1;   // destination array address
1327     const Register count     = O2;   // elements count
1328     const Register offset    = O5;   // offset from start of arrays
1329     // O3, O4, G3, G4 are used as temp registers
1330 
1331     assert_clean_int(count, O3);     // Make sure &#39;count&#39; is clean int.
1332 
1333     if (entry != NULL) {
1334       *entry = __ pc();
1335       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1336       BLOCK_COMMENT(&quot;Entry:&quot;);
1337     }
1338 
<span class="line-modified">1339     {</span>
<span class="line-modified">1340       // UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit</span>
<span class="line-modified">1341       UnsafeCopyMemoryMark ucmm(this, !aligned, false);</span>
<span class="line-modified">1342       // for short arrays, just do single element copy</span>
<span class="line-added">1343       __ cmp(count, 11); // 8 + 3  (22 bytes)</span>
<span class="line-added">1344       __ brx(Assembler::less, false, Assembler::pn, L_copy_2_bytes);</span>
<span class="line-added">1345       __ delayed()-&gt;mov(G0, offset);</span>
1346 
<span class="line-modified">1347       if (aligned) {</span>
<span class="line-modified">1348         // &#39;aligned&#39; == true when it is known statically during compilation</span>
<span class="line-modified">1349         // of this arraycopy call site that both &#39;from&#39; and &#39;to&#39; addresses</span>
<span class="line-modified">1350         // are HeapWordSize aligned (see LibraryCallKit::basictype2arraycopy()).</span>
<span class="line-modified">1351         //</span>
<span class="line-modified">1352         // Aligned arrays have 4 bytes alignment in 32-bits VM</span>
<span class="line-modified">1353         // and 8 bytes - in 64-bits VM.</span>
<span class="line-modified">1354         //</span>
<span class="line-modified">1355       } else {</span>
<span class="line-modified">1356         // copy 1 element if necessary to align &#39;to&#39; on an 4 bytes</span>
<span class="line-modified">1357         __ andcc(to, 3, G0);</span>
<span class="line-modified">1358         __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);</span>
<span class="line-modified">1359         __ delayed()-&gt;lduh(from, 0, O3);</span>
<span class="line-modified">1360         __ inc(from, 2);</span>
<span class="line-modified">1361         __ inc(to, 2);</span>
<span class="line-modified">1362         __ dec(count);</span>
<span class="line-modified">1363         __ sth(O3, to, -2);</span>
<span class="line-modified">1364       __ BIND(L_skip_alignment);</span>
<span class="line-added">1365 </span>
<span class="line-added">1366         // copy 2 elements to align &#39;to&#39; on an 8 byte boundary</span>
<span class="line-added">1367         __ andcc(to, 7, G0);</span>
<span class="line-added">1368         __ br(Assembler::zero, false, Assembler::pn, L_skip_alignment2);</span>
<span class="line-added">1369         __ delayed()-&gt;lduh(from, 0, O3);</span>
<span class="line-added">1370         __ dec(count, 2);</span>
<span class="line-added">1371         __ lduh(from, 2, O4);</span>
<span class="line-added">1372         __ inc(from, 4);</span>
<span class="line-added">1373         __ inc(to, 4);</span>
<span class="line-added">1374         __ sth(O3, to, -4);</span>
<span class="line-added">1375         __ sth(O4, to, -2);</span>
<span class="line-added">1376       __ BIND(L_skip_alignment2);</span>
<span class="line-added">1377       }</span>
<span class="line-added">1378       if (!aligned) {</span>
<span class="line-added">1379         // Copy with shift 16 bytes per iteration if arrays do not have</span>
<span class="line-added">1380         // the same alignment mod 8, otherwise fall through to the next</span>
<span class="line-added">1381         // code for aligned copy.</span>
<span class="line-added">1382         // The compare above (count &gt;= 11) guarantes &#39;count&#39; &gt;= 16 bytes.</span>
<span class="line-added">1383         // Also jump over aligned copy after the copy with shift completed.</span>
<span class="line-added">1384 </span>
<span class="line-added">1385         copy_16_bytes_forward_with_shift(from, to, count, 1, L_copy_2_bytes);</span>
<span class="line-added">1386       }</span>
1387 
<span class="line-modified">1388       // Both array are 8 bytes aligned, copy 16 bytes at a time</span>
<span class="line-modified">1389         __ and3(count, 3, G4); // Save</span>
<span class="line-modified">1390         __ srl(count, 2, count);</span>
<span class="line-modified">1391        generate_disjoint_long_copy_core(aligned);</span>
<span class="line-modified">1392         __ mov(G4, count); // restore</span>













1393 
<span class="line-modified">1394       // copy 1 element at a time</span>
<span class="line-added">1395       __ BIND(L_copy_2_bytes);</span>
<span class="line-added">1396         __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);</span>
<span class="line-added">1397         __ align(OptoLoopAlignment);</span>
<span class="line-added">1398       __ BIND(L_copy_2_bytes_loop);</span>
<span class="line-added">1399         __ lduh(from, offset, O3);</span>
<span class="line-added">1400         __ deccc(count);</span>
<span class="line-added">1401         __ sth(O3, to, offset);</span>
<span class="line-added">1402         __ brx(Assembler::notZero, false, Assembler::pt, L_copy_2_bytes_loop);</span>
<span class="line-added">1403         __ delayed()-&gt;inc(offset, 2);</span>
1404     }
1405 

















1406     __ BIND(L_exit);
1407       // O3, O4 are used as temp registers
1408       inc_counter_np(SharedRuntime::_jshort_array_copy_ctr, O3, O4);
1409       __ retl();
1410       __ delayed()-&gt;mov(G0, O0); // return 0
1411     return start;
1412   }
1413 
1414   //
1415   //  Generate stub for disjoint short fill.  If &quot;aligned&quot; is true, the
1416   //  &quot;to&quot; address is assumed to be heapword aligned.
1417   //
1418   // Arguments for generated stub:
1419   //      to:    O0
1420   //      value: O1
1421   //      count: O2 treated as signed
1422   //
1423   address generate_fill(BasicType t, bool aligned, const char* name) {
1424     __ align(CodeEntryAlignment);
1425     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
</pre>
<hr />
<pre>
1651     Label L_copy_2_bytes, L_copy_2_bytes_loop, L_exit;
1652 
1653     const Register from      = O0;   // source array address
1654     const Register to        = O1;   // destination array address
1655     const Register count     = O2;   // elements count
1656     const Register end_from  = from; // source array end address
1657     const Register end_to    = to;   // destination array end address
1658 
1659     const Register byte_count = O3;  // bytes count to copy
1660 
1661     assert_clean_int(count, O3);     // Make sure &#39;count&#39; is clean int.
1662 
1663     if (entry != NULL) {
1664       *entry = __ pc();
1665       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1666       BLOCK_COMMENT(&quot;Entry:&quot;);
1667     }
1668 
1669     array_overlap_test(nooverlap_target, 1);
1670 








1671     {
<span class="line-modified">1672       // UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit</span>
<span class="line-modified">1673       UnsafeCopyMemoryMark ucmm(this, !aligned, false);</span>
1674 
<span class="line-modified">1675       __ sllx(count, LogBytesPerShort, byte_count);</span>
<span class="line-modified">1676       __ add(to, byte_count, end_to);  // offset after last copied element</span>







1677 
<span class="line-modified">1678       // for short arrays, just do single element copy</span>
<span class="line-modified">1679       __ cmp(count, 11); // 8 + 3  (22 bytes)</span>
<span class="line-modified">1680       __ brx(Assembler::less, false, Assembler::pn, L_copy_2_bytes);</span>
<span class="line-modified">1681       __ delayed()-&gt;add(from, byte_count, end_from);</span>
<span class="line-modified">1682 </span>
<span class="line-modified">1683       {</span>
<span class="line-modified">1684         // Align end of arrays since they could be not aligned even</span>
<span class="line-modified">1685         // when arrays itself are aligned.</span>
<span class="line-modified">1686 </span>
<span class="line-modified">1687         // copy 1 element if necessary to align &#39;end_to&#39; on an 4 bytes</span>
<span class="line-modified">1688         __ andcc(end_to, 3, G0);</span>
<span class="line-modified">1689         __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);</span>
<span class="line-modified">1690         __ delayed()-&gt;lduh(end_from, -2, O3);</span>
<span class="line-modified">1691         __ dec(end_from, 2);</span>
<span class="line-modified">1692         __ dec(end_to, 2);</span>
<span class="line-modified">1693         __ dec(count);</span>
<span class="line-modified">1694         __ sth(O3, end_to, 0);</span>
<span class="line-modified">1695       __ BIND(L_skip_alignment);</span>
<span class="line-modified">1696 </span>
<span class="line-modified">1697         // copy 2 elements to align &#39;end_to&#39; on an 8 byte boundary</span>
<span class="line-modified">1698         __ andcc(end_to, 7, G0);</span>
<span class="line-modified">1699         __ br(Assembler::zero, false, Assembler::pn, L_skip_alignment2);</span>
<span class="line-modified">1700         __ delayed()-&gt;lduh(end_from, -2, O3);</span>
<span class="line-added">1701         __ dec(count, 2);</span>
<span class="line-added">1702         __ lduh(end_from, -4, O4);</span>
<span class="line-added">1703         __ dec(end_from, 4);</span>
<span class="line-added">1704         __ dec(end_to, 4);</span>
<span class="line-added">1705         __ sth(O3, end_to, 2);</span>
<span class="line-added">1706         __ sth(O4, end_to, 0);</span>
<span class="line-added">1707       __ BIND(L_skip_alignment2);</span>
<span class="line-added">1708       }</span>
<span class="line-added">1709       if (aligned) {</span>
<span class="line-added">1710         // Both arrays are aligned to 8-bytes in 64-bits VM.</span>
<span class="line-added">1711         // The &#39;count&#39; is decremented in copy_16_bytes_backward_with_shift()</span>
<span class="line-added">1712         // in unaligned case.</span>
<span class="line-added">1713         __ dec(count, 8);</span>
<span class="line-added">1714       } else {</span>
<span class="line-added">1715         // Copy with shift 16 bytes per iteration if arrays do not have</span>
<span class="line-added">1716         // the same alignment mod 8, otherwise jump to the next</span>
<span class="line-added">1717         // code for aligned copy (and substracting 8 from &#39;count&#39; before jump).</span>
<span class="line-added">1718         // The compare above (count &gt;= 11) guarantes &#39;count&#39; &gt;= 16 bytes.</span>
<span class="line-added">1719         // Also jump over aligned copy after the copy with shift completed.</span>
1720 
<span class="line-modified">1721         copy_16_bytes_backward_with_shift(end_from, end_to, count, 8,</span>
1722                                         L_aligned_copy, L_copy_2_bytes);
<span class="line-added">1723       }</span>
<span class="line-added">1724       // copy 4 elements (16 bytes) at a time</span>
<span class="line-added">1725         __ align(OptoLoopAlignment);</span>
<span class="line-added">1726       __ BIND(L_aligned_copy);</span>
<span class="line-added">1727         __ dec(end_from, 16);</span>
<span class="line-added">1728         __ ldx(end_from, 8, O3);</span>
<span class="line-added">1729         __ ldx(end_from, 0, O4);</span>
<span class="line-added">1730         __ dec(end_to, 16);</span>
<span class="line-added">1731         __ deccc(count, 8);</span>
<span class="line-added">1732         __ stx(O3, end_to, 8);</span>
<span class="line-added">1733         __ brx(Assembler::greaterEqual, false, Assembler::pt, L_aligned_copy);</span>
<span class="line-added">1734         __ delayed()-&gt;stx(O4, end_to, 0);</span>
<span class="line-added">1735         __ inc(count, 8);</span>
<span class="line-added">1736 </span>
<span class="line-added">1737       // copy 1 element (2 bytes) at a time</span>
<span class="line-added">1738       __ BIND(L_copy_2_bytes);</span>
<span class="line-added">1739         __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);</span>
<span class="line-added">1740       __ BIND(L_copy_2_bytes_loop);</span>
<span class="line-added">1741         __ dec(end_from, 2);</span>
<span class="line-added">1742         __ dec(end_to, 2);</span>
<span class="line-added">1743         __ lduh(end_from, 0, O4);</span>
<span class="line-added">1744         __ deccc(count);</span>
<span class="line-added">1745         __ brx(Assembler::greater, false, Assembler::pt, L_copy_2_bytes_loop);</span>
<span class="line-added">1746         __ delayed()-&gt;sth(O4, end_to, 0);</span>
1747     }
























1748     __ BIND(L_exit);
1749     // O3, O4 are used as temp registers
1750     inc_counter_np(SharedRuntime::_jshort_array_copy_ctr, O3, O4);
1751     __ retl();
1752     __ delayed()-&gt;mov(G0, O0); // return 0
1753     return start;
1754   }
1755 
1756   //
1757   // Helper methods for generate_disjoint_int_copy_core()
1758   //
1759   void copy_16_bytes_loop(Register from, Register to, Register count, int count_dec,
1760                           Label&amp; L_loop, bool use_prefetch, bool use_bis) {
1761 
1762     __ align(OptoLoopAlignment);
1763     __ BIND(L_loop);
1764     if (use_prefetch) {
1765       if (ArraycopySrcPrefetchDistance &gt; 0) {
1766         __ prefetch(from, ArraycopySrcPrefetchDistance, Assembler::severalReads);
1767       }
</pre>
<hr />
<pre>
1886   //  &quot;from&quot; and &quot;to&quot; addresses are assumed to be heapword aligned.
1887   //
1888   // Arguments for generated stub:
1889   //      from:  O0
1890   //      to:    O1
1891   //      count: O2 treated as signed
1892   //
1893   address generate_disjoint_int_copy(bool aligned, address *entry, const char *name) {
1894     __ align(CodeEntryAlignment);
1895     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1896     address start = __ pc();
1897 
1898     const Register count = O2;
1899     assert_clean_int(count, O3);     // Make sure &#39;count&#39; is clean int.
1900 
1901     if (entry != NULL) {
1902       *entry = __ pc();
1903       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1904       BLOCK_COMMENT(&quot;Entry:&quot;);
1905     }
<span class="line-modified">1906     {</span>
<span class="line-modified">1907       // UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit</span>
<span class="line-modified">1908       UnsafeCopyMemoryMark ucmm(this, !aligned, false);</span>
<span class="line-added">1909       generate_disjoint_int_copy_core(aligned);</span>
<span class="line-added">1910     }</span>
1911     // O3, O4 are used as temp registers
1912     inc_counter_np(SharedRuntime::_jint_array_copy_ctr, O3, O4);
1913     __ retl();
1914     __ delayed()-&gt;mov(G0, O0); // return 0
1915     return start;
1916   }
1917 
1918   //
1919   //  Generate core code for conjoint int copy (and oop copy on 32-bit).
1920   //  If &quot;aligned&quot; is true, the &quot;from&quot; and &quot;to&quot; addresses are assumed
1921   //  to be heapword aligned.
1922   //
1923   // Arguments:
1924   //      from:  O0
1925   //      to:    O1
1926   //      count: O2 treated as signed
1927   //
1928   void generate_conjoint_int_copy_core(bool aligned) {
1929     // Do reverse copy.
1930 
</pre>
<hr />
<pre>
2023   // Arguments for generated stub:
2024   //      from:  O0
2025   //      to:    O1
2026   //      count: O2 treated as signed
2027   //
2028   address generate_conjoint_int_copy(bool aligned, address nooverlap_target,
2029                                      address *entry, const char *name) {
2030     __ align(CodeEntryAlignment);
2031     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2032     address start = __ pc();
2033 
2034     assert_clean_int(O2, O3);     // Make sure &#39;count&#39; is clean int.
2035 
2036     if (entry != NULL) {
2037       *entry = __ pc();
2038       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
2039       BLOCK_COMMENT(&quot;Entry:&quot;);
2040     }
2041 
2042     array_overlap_test(nooverlap_target, 2);
<span class="line-modified">2043     {</span>
<span class="line-modified">2044       // UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit</span>
<span class="line-modified">2045       UnsafeCopyMemoryMark ucmm(this, !aligned, false);</span>
<span class="line-added">2046       generate_conjoint_int_copy_core(aligned);</span>
<span class="line-added">2047     }</span>
2048     // O3, O4 are used as temp registers
2049     inc_counter_np(SharedRuntime::_jint_array_copy_ctr, O3, O4);
2050     __ retl();
2051     __ delayed()-&gt;mov(G0, O0); // return 0
2052     return start;
2053   }
2054 
2055   //
2056   // Helper methods for generate_disjoint_long_copy_core()
2057   //
2058   void copy_64_bytes_loop(Register from, Register to, Register count, int count_dec,
2059                           Label&amp; L_loop, bool use_prefetch, bool use_bis) {
2060     __ align(OptoLoopAlignment);
2061     __ BIND(L_loop);
2062     for (int off = 0; off &lt; 64; off += 16) {
2063       if (use_prefetch &amp;&amp; (off &amp; 31) == 0) {
2064         if (ArraycopySrcPrefetchDistance &gt; 0) {
2065           __ prefetch(from, ArraycopySrcPrefetchDistance+off, Assembler::severalReads);
2066         }
2067         if (ArraycopyDstPrefetchDistance &gt; 0) {
</pre>
<hr />
<pre>
2176   //  assumption that both addresses are always 64-bit aligned.
2177   //
2178   // Arguments for generated stub:
2179   //      from:  O0
2180   //      to:    O1
2181   //      count: O2 treated as signed
2182   //
2183   address generate_disjoint_long_copy(bool aligned, address *entry, const char *name) {
2184     __ align(CodeEntryAlignment);
2185     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2186     address start = __ pc();
2187 
2188     assert_clean_int(O2, O3);     // Make sure &#39;count&#39; is clean int.
2189 
2190     if (entry != NULL) {
2191       *entry = __ pc();
2192       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
2193       BLOCK_COMMENT(&quot;Entry:&quot;);
2194     }
2195 
<span class="line-modified">2196     {</span>
<span class="line-modified">2197       // UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit</span>
<span class="line-added">2198       UnsafeCopyMemoryMark ucmm(this, true, false);</span>
<span class="line-added">2199       generate_disjoint_long_copy_core(aligned);</span>
<span class="line-added">2200     }</span>
2201     // O3, O4 are used as temp registers
2202     inc_counter_np(SharedRuntime::_jlong_array_copy_ctr, O3, O4);
2203     __ retl();
2204     __ delayed()-&gt;mov(G0, O0); // return 0
2205     return start;
2206   }
2207 
2208   //
2209   //  Generate core code for conjoint long copy (and oop copy on 64-bit).
2210   //  &quot;aligned&quot; is ignored, because we must make the stronger
2211   //  assumption that both addresses are always 64-bit aligned.
2212   //
2213   // Arguments:
2214   //      from:  O0
2215   //      to:    O1
2216   //      count: O2 treated as signed
2217   //
2218   void generate_conjoint_long_copy_core(bool aligned) {
2219     // Do reverse copy.
2220     Label L_copy_8_bytes, L_copy_16_bytes, L_exit;
</pre>
<hr />
<pre>
2255   //      to:    O1
2256   //      count: O2 treated as signed
2257   //
2258   address generate_conjoint_long_copy(bool aligned, address nooverlap_target,
2259                                       address *entry, const char *name) {
2260     __ align(CodeEntryAlignment);
2261     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2262     address start = __ pc();
2263 
2264     assert(aligned, &quot;Should always be aligned&quot;);
2265 
2266     assert_clean_int(O2, O3);     // Make sure &#39;count&#39; is clean int.
2267 
2268     if (entry != NULL) {
2269       *entry = __ pc();
2270       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
2271       BLOCK_COMMENT(&quot;Entry:&quot;);
2272     }
2273 
2274     array_overlap_test(nooverlap_target, 3);
<span class="line-modified">2275     {</span>
<span class="line-modified">2276       // UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit</span>
<span class="line-modified">2277       UnsafeCopyMemoryMark ucmm(this, true, false);</span>
<span class="line-added">2278       generate_conjoint_long_copy_core(aligned);</span>
<span class="line-added">2279     }</span>
2280     // O3, O4 are used as temp registers
2281     inc_counter_np(SharedRuntime::_jlong_array_copy_ctr, O3, O4);
2282     __ retl();
2283     __ delayed()-&gt;mov(G0, O0); // return 0
2284     return start;
2285   }
2286 
2287   //  Generate stub for disjoint oop copy.  If &quot;aligned&quot; is true, the
2288   //  &quot;from&quot; and &quot;to&quot; addresses are assumed to be heapword aligned.
2289   //
2290   // Arguments for generated stub:
2291   //      from:  O0
2292   //      to:    O1
2293   //      count: O2 treated as signed
2294   //
2295   address generate_disjoint_oop_copy(bool aligned, address *entry, const char *name,
2296                                      bool dest_uninitialized = false) {
2297 
2298     const Register from  = O0;  // source array address
2299     const Register to    = O1;  // destination array address
</pre>
<hr />
<pre>
2954 
2955     Label Ldone;
2956     __ sllx(count, LogHeapWordSize, count); // to bytes count
2957     // Use BIS for zeroing
2958     __ bis_zeroing(to, count, temp, Ldone);
2959     __ bind(Ldone);
2960     __ retl();
2961     __ delayed()-&gt;nop();
2962     return start;
2963 }
2964 
2965   void generate_arraycopy_stubs() {
2966     address entry;
2967     address entry_jbyte_arraycopy;
2968     address entry_jshort_arraycopy;
2969     address entry_jint_arraycopy;
2970     address entry_oop_arraycopy;
2971     address entry_jlong_arraycopy;
2972     address entry_checkcast_arraycopy;
2973 
<span class="line-added">2974     address ucm_common_error_exit       =  generate_unsafecopy_common_error_exit();</span>
<span class="line-added">2975     UnsafeCopyMemory::set_common_exit_stub_pc(ucm_common_error_exit);</span>
<span class="line-added">2976 </span>
2977     //*** jbyte
2978     // Always need aligned and unaligned versions
2979     StubRoutines::_jbyte_disjoint_arraycopy         = generate_disjoint_byte_copy(false, &amp;entry,
2980                                                                                   &quot;jbyte_disjoint_arraycopy&quot;);
2981     StubRoutines::_jbyte_arraycopy                  = generate_conjoint_byte_copy(false, entry,
2982                                                                                   &amp;entry_jbyte_arraycopy,
2983                                                                                   &quot;jbyte_arraycopy&quot;);
2984     StubRoutines::_arrayof_jbyte_disjoint_arraycopy = generate_disjoint_byte_copy(true, &amp;entry,
2985                                                                                   &quot;arrayof_jbyte_disjoint_arraycopy&quot;);
2986     StubRoutines::_arrayof_jbyte_arraycopy          = generate_conjoint_byte_copy(true, entry, NULL,
2987                                                                                   &quot;arrayof_jbyte_arraycopy&quot;);
2988 
2989     //*** jshort
2990     // Always need aligned and unaligned versions
2991     StubRoutines::_jshort_disjoint_arraycopy         = generate_disjoint_short_copy(false, &amp;entry,
2992                                                                                     &quot;jshort_disjoint_arraycopy&quot;);
2993     StubRoutines::_jshort_arraycopy                  = generate_conjoint_short_copy(false, entry,
2994                                                                                     &amp;entry_jshort_arraycopy,
2995                                                                                     &quot;jshort_arraycopy&quot;);
2996     StubRoutines::_arrayof_jshort_disjoint_arraycopy = generate_disjoint_short_copy(true, &amp;entry,
</pre>
<hr />
<pre>
5849 
5850   void align(bool at_header = false) {
5851     // %%%%% move this constant somewhere else
5852     // UltraSPARC cache line size is 8 instructions:
5853     const unsigned int icache_line_size = 32;
5854     const unsigned int icache_half_line_size = 16;
5855 
5856     if (at_header) {
5857       while ((intptr_t)(__ pc()) % icache_line_size != 0) {
5858         __ emit_data(0, relocInfo::none);
5859       }
5860     } else {
5861       while ((intptr_t)(__ pc()) % icache_half_line_size != 0) {
5862         __ nop();
5863       }
5864     }
5865   }
5866 
5867 }; // end class declaration
5868 
<span class="line-added">5869 #define UCM_TABLE_MAX_ENTRIES 8</span>
5870 void StubGenerator_generate(CodeBuffer* code, bool all) {
<span class="line-added">5871   if (UnsafeCopyMemory::_table == NULL) {</span>
<span class="line-added">5872     UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);</span>
<span class="line-added">5873   }</span>
5874   StubGenerator g(code, all);
5875 }
</pre>
</td>
</tr>
</table>
<center><a href="sparc.ad.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="templateTable_sparc.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>