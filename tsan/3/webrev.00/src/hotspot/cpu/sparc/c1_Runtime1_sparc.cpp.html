<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/cpu/sparc/c1_Runtime1_sparc.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 1999, 2018, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;asm/macroAssembler.inline.hpp&quot;
 27 #include &quot;c1/c1_Defs.hpp&quot;
 28 #include &quot;c1/c1_MacroAssembler.hpp&quot;
 29 #include &quot;c1/c1_Runtime1.hpp&quot;
 30 #include &quot;ci/ciUtilities.hpp&quot;
 31 #include &quot;gc/shared/cardTable.hpp&quot;
 32 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
 33 #include &quot;interpreter/interpreter.hpp&quot;
 34 #include &quot;memory/universe.hpp&quot;
 35 #include &quot;nativeInst_sparc.hpp&quot;
 36 #include &quot;oops/compiledICHolder.hpp&quot;
 37 #include &quot;oops/oop.inline.hpp&quot;
 38 #include &quot;prims/jvmtiExport.hpp&quot;
 39 #include &quot;runtime/sharedRuntime.hpp&quot;
 40 #include &quot;runtime/signature.hpp&quot;
 41 #include &quot;runtime/vframeArray.hpp&quot;
 42 #include &quot;utilities/macros.hpp&quot;
 43 #include &quot;utilities/align.hpp&quot;
 44 #include &quot;vmreg_sparc.inline.hpp&quot;
 45 
 46 // Implementation of StubAssembler
 47 
 48 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry_point, int number_of_arguments) {
 49   // for sparc changing the number of arguments doesn&#39;t change
 50   // anything about the frame size so we&#39;ll always lie and claim that
 51   // we are only passing 1 argument.
 52   set_num_rt_args(1);
 53 
 54   assert_not_delayed();
 55   // bang stack before going to runtime
 56   set(-os::vm_page_size() + STACK_BIAS, G3_scratch);
 57   st(G0, SP, G3_scratch);
 58 
 59   // debugging support
 60   assert(number_of_arguments &gt;= 0   , &quot;cannot have negative number of arguments&quot;);
 61 
 62   set_last_Java_frame(SP, noreg);
 63   if (VerifyThread)  mov(G2_thread, O0); // about to be smashed; pass early
 64   save_thread(L7_thread_cache);
 65   // do the call
 66   call(entry_point, relocInfo::runtime_call_type);
 67   if (!VerifyThread) {
 68     delayed()-&gt;mov(G2_thread, O0);  // pass thread as first argument
 69   } else {
 70     delayed()-&gt;nop();             // (thread already passed)
 71   }
 72   int call_offset = offset();  // offset of return address
 73   restore_thread(L7_thread_cache);
 74   reset_last_Java_frame();
 75 
 76   // check for pending exceptions
 77   { Label L;
 78     Address exception_addr(G2_thread, Thread::pending_exception_offset());
 79     ld_ptr(exception_addr, Gtemp);
 80     br_null_short(Gtemp, pt, L);
 81     Address vm_result_addr(G2_thread, JavaThread::vm_result_offset());
 82     st_ptr(G0, vm_result_addr);
 83     Address vm_result_addr_2(G2_thread, JavaThread::vm_result_2_offset());
 84     st_ptr(G0, vm_result_addr_2);
 85 
 86     if (frame_size() == no_frame_size) {
 87       // we use O7 linkage so that forward_exception_entry has the issuing PC
 88       call(StubRoutines::forward_exception_entry(), relocInfo::runtime_call_type);
 89       delayed()-&gt;restore();
 90     } else if (_stub_id == Runtime1::forward_exception_id) {
 91       should_not_reach_here();
 92     } else {
 93       AddressLiteral exc(Runtime1::entry_for(Runtime1::forward_exception_id));
 94       jump_to(exc, G4);
 95       delayed()-&gt;nop();
 96     }
 97     bind(L);
 98   }
 99 
100   // get oop result if there is one and reset the value in the thread
101   if (oop_result1-&gt;is_valid()) {                    // get oop result if there is one and reset it in the thread
102     get_vm_result  (oop_result1);
103   } else {
104     // be a little paranoid and clear the result
105     Address vm_result_addr(G2_thread, JavaThread::vm_result_offset());
106     st_ptr(G0, vm_result_addr);
107   }
108 
109   // get second result if there is one and reset the value in the thread
110   if (metadata_result-&gt;is_valid()) {
111     get_vm_result_2  (metadata_result);
112   } else {
113     // be a little paranoid and clear the result
114     Address vm_result_addr_2(G2_thread, JavaThread::vm_result_2_offset());
115     st_ptr(G0, vm_result_addr_2);
116   }
117 
118   return call_offset;
119 }
120 
121 
122 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1) {
123   // O0 is reserved for the thread
124   mov(arg1, O1);
125   return call_RT(oop_result1, metadata_result, entry, 1);
126 }
127 
128 
129 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1, Register arg2) {
130   // O0 is reserved for the thread
131   mov(arg1, O1);
132   mov(arg2, O2); assert(arg2 != O1, &quot;smashed argument&quot;);
133   return call_RT(oop_result1, metadata_result, entry, 2);
134 }
135 
136 
137 int StubAssembler::call_RT(Register oop_result1, Register metadata_result, address entry, Register arg1, Register arg2, Register arg3) {
138   // O0 is reserved for the thread
139   mov(arg1, O1);
140   mov(arg2, O2); assert(arg2 != O1,               &quot;smashed argument&quot;);
141   mov(arg3, O3); assert(arg3 != O1 &amp;&amp; arg3 != O2, &quot;smashed argument&quot;);
142   return call_RT(oop_result1, metadata_result, entry, 3);
143 }
144 
145 void StubAssembler::prologue(const char* name, bool must_gc_arguments) {
146   set_info(name, must_gc_arguments);
147 }
148 
149 void StubAssembler::epilogue() {
150   delayed()-&gt;restore();
151 }
152 
153 // Implementation of Runtime1
154 
155 
156 static int cpu_reg_save_offsets[FrameMap::nof_cpu_regs];
157 static int fpu_reg_save_offsets[FrameMap::nof_fpu_regs];
158 static int reg_save_size_in_words;
159 static int frame_size_in_bytes = -1;
160 
161 static OopMap* generate_oop_map(StubAssembler* sasm, bool save_fpu_registers) {
162   assert(frame_size_in_bytes == sasm-&gt;total_frame_size_in_bytes(reg_save_size_in_words),
163          &quot;mismatch in calculation&quot;);
164   sasm-&gt;set_frame_size(frame_size_in_bytes / BytesPerWord);
165   int frame_size_in_slots = frame_size_in_bytes / sizeof(jint);
166   OopMap* oop_map = new OopMap(frame_size_in_slots, 0);
167 
168   int i;
169   for (i = 0; i &lt; FrameMap::nof_cpu_regs; i++) {
170     Register r = as_Register(i);
171     if (r == G1 || r == G3 || r == G4 || r == G5) {
172       int sp_offset = cpu_reg_save_offsets[i];
173       oop_map-&gt;set_callee_saved(VMRegImpl::stack2reg(sp_offset),
174                                 r-&gt;as_VMReg());
175     }
176   }
177 
178   if (save_fpu_registers) {
179     for (i = 0; i &lt; FrameMap::nof_fpu_regs; i++) {
180       FloatRegister r = as_FloatRegister(i);
181       int sp_offset = fpu_reg_save_offsets[i];
182       oop_map-&gt;set_callee_saved(VMRegImpl::stack2reg(sp_offset),
183                                 r-&gt;as_VMReg());
184     }
185   }
186   return oop_map;
187 }
188 
189 #define __ this-&gt;
190 
191 void C1_MacroAssembler::save_live_registers_no_oop_map(bool save_fpu_registers) {
192   assert(frame_size_in_bytes == __ total_frame_size_in_bytes(reg_save_size_in_words),
193          &quot;mismatch in calculation&quot;);
194   __ save_frame_c1(frame_size_in_bytes);
195 
196   // Record volatile registers as callee-save values in an OopMap so their save locations will be
197   // propagated to the caller frame&#39;s RegisterMap during StackFrameStream construction (needed for
198   // deoptimization; see compiledVFrame::create_stack_value).  The caller&#39;s I, L and O registers
199   // are saved in register windows - I&#39;s and L&#39;s in the caller&#39;s frame and O&#39;s in the stub frame
200   // (as the stub&#39;s I&#39;s) when the runtime routine called by the stub creates its frame.
201   // OopMap frame sizes are in c2 stack slot sizes (sizeof(jint))
202 
203   int i;
204   for (i = 0; i &lt; FrameMap::nof_cpu_regs; i++) {
205     Register r = as_Register(i);
206     if (r == G1 || r == G3 || r == G4 || r == G5) {
207       int sp_offset = cpu_reg_save_offsets[i];
208       __ st_ptr(r, SP, (sp_offset * BytesPerWord) + STACK_BIAS);
209     }
210   }
211 
212   if (save_fpu_registers) {
213     for (i = 0; i &lt; FrameMap::nof_fpu_regs; i++) {
214       FloatRegister r = as_FloatRegister(i);
215       int sp_offset = fpu_reg_save_offsets[i];
216       __ stf(FloatRegisterImpl::S, r, SP, (sp_offset * BytesPerWord) + STACK_BIAS);
217     }
218   }
219 }
220 
221 void C1_MacroAssembler::restore_live_registers(bool restore_fpu_registers) {
222   for (int i = 0; i &lt; FrameMap::nof_cpu_regs; i++) {
223     Register r = as_Register(i);
224     if (r == G1 || r == G3 || r == G4 || r == G5) {
225       __ ld_ptr(SP, (cpu_reg_save_offsets[i] * BytesPerWord) + STACK_BIAS, r);
226     }
227   }
228 
229   if (restore_fpu_registers) {
230     for (int i = 0; i &lt; FrameMap::nof_fpu_regs; i++) {
231       FloatRegister r = as_FloatRegister(i);
232       __ ldf(FloatRegisterImpl::S, SP, (fpu_reg_save_offsets[i] * BytesPerWord) + STACK_BIAS, r);
233     }
234   }
235 }
236 
237 #undef __
238 #define __ sasm-&gt;
239 
240 static OopMap* save_live_registers(StubAssembler* sasm, bool save_fpu_registers = true) {
241   __ save_live_registers_no_oop_map(save_fpu_registers);
242   return generate_oop_map(sasm, save_fpu_registers);
243 }
244 
245 static void restore_live_registers(StubAssembler* sasm, bool restore_fpu_registers = true) {
246   __ restore_live_registers(restore_fpu_registers);
247 }
248 
249 
250 void Runtime1::initialize_pd() {
251   // compute word offsets from SP at which live (non-windowed) registers are captured by stub routines
252   //
253   // A stub routine will have a frame that is at least large enough to hold
254   // a register window save area (obviously) and the volatile g registers
255   // and floating registers. A user of save_live_registers can have a frame
256   // that has more scratch area in it (although typically they will use L-regs).
257   // in that case the frame will look like this (stack growing down)
258   //
259   // FP -&gt; |             |
260   //       | scratch mem |
261   //       |   &quot;      &quot;  |
262   //       --------------
263   //       | float regs  |
264   //       |   &quot;    &quot;    |
265   //       ---------------
266   //       | G regs      |
267   //       | &quot;  &quot;        |
268   //       ---------------
269   //       | abi reg.    |
270   //       | window save |
271   //       | area        |
272   // SP -&gt; ---------------
273   //
274   int i;
275   int sp_offset = align_up((int)frame::register_save_words, 2); //  start doubleword aligned
276 
277   // only G int registers are saved explicitly; others are found in register windows
278   for (i = 0; i &lt; FrameMap::nof_cpu_regs; i++) {
279     Register r = as_Register(i);
280     if (r == G1 || r == G3 || r == G4 || r == G5) {
281       cpu_reg_save_offsets[i] = sp_offset;
282       sp_offset++;
283     }
284   }
285 
286   // all float registers are saved explicitly
287   assert(FrameMap::nof_fpu_regs == 32, &quot;double registers not handled here&quot;);
288   for (i = 0; i &lt; FrameMap::nof_fpu_regs; i++) {
289     fpu_reg_save_offsets[i] = sp_offset;
290     sp_offset++;
291   }
292   reg_save_size_in_words = sp_offset - frame::memory_parameter_word_sp_offset;
293   // this should match assembler::total_frame_size_in_bytes, which
294   // isn&#39;t callable from this context.  It&#39;s checked by an assert when
295   // it&#39;s used though.
296   frame_size_in_bytes = align_up(sp_offset * wordSize, 8);
297 }
298 
299 
300 OopMapSet* Runtime1::generate_exception_throw(StubAssembler* sasm, address target, bool has_argument) {
301   // make a frame and preserve the caller&#39;s caller-save registers
302   OopMap* oop_map = save_live_registers(sasm);
303   int call_offset;
304   if (!has_argument) {
305     call_offset = __ call_RT(noreg, noreg, target);
306   } else {
307     call_offset = __ call_RT(noreg, noreg, target, G4, G5);
308   }
309   OopMapSet* oop_maps = new OopMapSet();
310   oop_maps-&gt;add_gc_map(call_offset, oop_map);
311 
312   __ should_not_reach_here();
313   return oop_maps;
314 }
315 
316 
317 OopMapSet* Runtime1::generate_stub_call(StubAssembler* sasm, Register result, address target,
318                                         Register arg1, Register arg2, Register arg3) {
319   // make a frame and preserve the caller&#39;s caller-save registers
320   OopMap* oop_map = save_live_registers(sasm);
321 
322   int call_offset;
323   if (arg1 == noreg) {
324     call_offset = __ call_RT(result, noreg, target);
325   } else if (arg2 == noreg) {
326     call_offset = __ call_RT(result, noreg, target, arg1);
327   } else if (arg3 == noreg) {
328     call_offset = __ call_RT(result, noreg, target, arg1, arg2);
329   } else {
330     call_offset = __ call_RT(result, noreg, target, arg1, arg2, arg3);
331   }
332   OopMapSet* oop_maps = NULL;
333 
334   oop_maps = new OopMapSet();
335   oop_maps-&gt;add_gc_map(call_offset, oop_map);
336   restore_live_registers(sasm);
337 
338   __ ret();
339   __ delayed()-&gt;restore();
340 
341   return oop_maps;
342 }
343 
344 
345 OopMapSet* Runtime1::generate_patching(StubAssembler* sasm, address target) {
346   // make a frame and preserve the caller&#39;s caller-save registers
347   OopMap* oop_map = save_live_registers(sasm);
348 
349   // call the runtime patching routine, returns non-zero if nmethod got deopted.
350   int call_offset = __ call_RT(noreg, noreg, target);
351   OopMapSet* oop_maps = new OopMapSet();
352   oop_maps-&gt;add_gc_map(call_offset, oop_map);
353 
354   // re-execute the patched instruction or, if the nmethod was deoptmized, return to the
355   // deoptimization handler entry that will cause re-execution of the current bytecode
356   DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
357   assert(deopt_blob != NULL, &quot;deoptimization blob must have been created&quot;);
358 
359   Label no_deopt;
360   __ br_null_short(O0, Assembler::pt, no_deopt);
361 
362   // return to the deoptimization handler entry for unpacking and rexecute
363   // if we simply returned the we&#39;d deopt as if any call we patched had just
364   // returned.
365 
366   restore_live_registers(sasm);
367 
368   AddressLiteral dest(deopt_blob-&gt;unpack_with_reexecution());
369   __ jump_to(dest, O0);
370   __ delayed()-&gt;restore();
371 
372   __ bind(no_deopt);
373   restore_live_registers(sasm);
374   __ ret();
375   __ delayed()-&gt;restore();
376 
377   return oop_maps;
378 }
379 
380 OopMapSet* Runtime1::generate_code_for(StubID id, StubAssembler* sasm) {
381 
382   OopMapSet* oop_maps = NULL;
383   // for better readability
384   const bool must_gc_arguments = true;
385   const bool dont_gc_arguments = false;
386 
387   // stub code &amp; info for the different stubs
388   switch (id) {
389     case forward_exception_id:
390       {
391         oop_maps = generate_handle_exception(id, sasm);
392       }
393       break;
394 
395     case new_instance_id:
396     case fast_new_instance_id:
397     case fast_new_instance_init_check_id:
398       {
399         Register G5_klass = G5; // Incoming
400         Register O0_obj   = O0; // Outgoing
401 
402         if (id == new_instance_id) {
403           __ set_info(&quot;new_instance&quot;, dont_gc_arguments);
404         } else if (id == fast_new_instance_id) {
405           __ set_info(&quot;fast new_instance&quot;, dont_gc_arguments);
406         } else {
407           assert(id == fast_new_instance_init_check_id, &quot;bad StubID&quot;);
408           __ set_info(&quot;fast new_instance init check&quot;, dont_gc_arguments);
409         }
410 
411         // If TLAB is disabled, see if there is support for inlining contiguous
412         // allocations.
413         // Otherwise, just go to the slow path.
414         if ((id == fast_new_instance_id || id == fast_new_instance_init_check_id) &amp;&amp;
415             !UseTLAB &amp;&amp; Universe::heap()-&gt;supports_inline_contig_alloc()) {
416           Label slow_path;
417           Register G1_obj_size = G1;
418           Register G3_t1 = G3;
419           Register G4_t2 = G4;
420           assert_different_registers(G5_klass, G1_obj_size, G3_t1, G4_t2);
421 
422           // Push a frame since we may do dtrace notification for the
423           // allocation which requires calling out and we don&#39;t want
424           // to stomp the real return address.
425           __ save_frame(0);
426 
427           if (id == fast_new_instance_init_check_id) {
428             // make sure the klass is initialized
429             __ ldub(G5_klass, in_bytes(InstanceKlass::init_state_offset()), G3_t1);
430             __ cmp(G3_t1, InstanceKlass::fully_initialized);
431             __ br(Assembler::notEqual, false, Assembler::pn, slow_path);
432             __ delayed()-&gt;nop();
433           }
434 #ifdef ASSERT
435           // assert object can be fast path allocated
436           {
437             Label ok, not_ok;
438           __ ld(G5_klass, in_bytes(Klass::layout_helper_offset()), G1_obj_size);
439           // make sure it&#39;s an instance (LH &gt; 0)
440           __ cmp_and_br_short(G1_obj_size, 0, Assembler::lessEqual, Assembler::pn, not_ok);
441           __ btst(Klass::_lh_instance_slow_path_bit, G1_obj_size);
442           __ br(Assembler::zero, false, Assembler::pn, ok);
443           __ delayed()-&gt;nop();
444           __ bind(not_ok);
445           __ stop(&quot;assert(can be fast path allocated)&quot;);
446           __ should_not_reach_here();
447           __ bind(ok);
448           }
449 #endif // ASSERT
450 
451           // If we got here then the TLAB allocation failed, so try allocating directly from eden.
452           // get the instance size
453           __ ld(G5_klass, in_bytes(Klass::layout_helper_offset()), G1_obj_size);
454           __ eden_allocate(O0_obj, G1_obj_size, 0, G3_t1, G4_t2, slow_path);
455           __ incr_allocated_bytes(G1_obj_size, G3_t1, G4_t2);
456 
457           __ initialize_object(O0_obj, G5_klass, G1_obj_size, 0, G3_t1, G4_t2, /* is_tlab_allocated */ false);
458           __ verify_oop(O0_obj);
459           __ mov(O0, I0);
460           __ ret();
461           __ delayed()-&gt;restore();
462 
463           __ bind(slow_path);
464 
465           // pop this frame so generate_stub_call can push it&#39;s own
466           __ restore();
467         }
468 
469         oop_maps = generate_stub_call(sasm, I0, CAST_FROM_FN_PTR(address, new_instance), G5_klass);
470         // I0-&gt;O0: new instance
471       }
472 
473       break;
474 
475     case counter_overflow_id:
476         // G4 contains bci, G5 contains method
477       oop_maps = generate_stub_call(sasm, noreg, CAST_FROM_FN_PTR(address, counter_overflow), G4, G5);
478       break;
479 
480     case new_type_array_id:
481     case new_object_array_id:
482       {
483         Register G5_klass = G5; // Incoming
484         Register G4_length = G4; // Incoming
485         Register O0_obj   = O0; // Outgoing
486 
487         Address klass_lh(G5_klass, Klass::layout_helper_offset());
488         assert(Klass::_lh_header_size_shift % BitsPerByte == 0, &quot;bytewise&quot;);
489         assert(Klass::_lh_header_size_mask == 0xFF, &quot;bytewise&quot;);
490         // Use this offset to pick out an individual byte of the layout_helper:
491         const int klass_lh_header_size_offset = ((BytesPerInt - 1)  // 3 - 2 selects byte {0,1,0,0}
492                                                  - Klass::_lh_header_size_shift / BitsPerByte);
493 
494         if (id == new_type_array_id) {
495           __ set_info(&quot;new_type_array&quot;, dont_gc_arguments);
496         } else {
497           __ set_info(&quot;new_object_array&quot;, dont_gc_arguments);
498         }
499 
500 #ifdef ASSERT
501         // assert object type is really an array of the proper kind
502         {
503           Label ok;
504           Register G3_t1 = G3;
505           __ ld(klass_lh, G3_t1);
506           __ sra(G3_t1, Klass::_lh_array_tag_shift, G3_t1);
507           int tag = ((id == new_type_array_id)
508                      ? Klass::_lh_array_tag_type_value
509                      : Klass::_lh_array_tag_obj_value);
510           __ cmp_and_brx_short(G3_t1, tag, Assembler::equal, Assembler::pt, ok);
511           __ stop(&quot;assert(is an array klass)&quot;);
512           __ should_not_reach_here();
513           __ bind(ok);
514         }
515 #endif // ASSERT
516 
517         if (id == new_type_array_id) {
518           oop_maps = generate_stub_call(sasm, I0, CAST_FROM_FN_PTR(address, new_type_array), G5_klass, G4_length);
519         } else {
520           oop_maps = generate_stub_call(sasm, I0, CAST_FROM_FN_PTR(address, new_object_array), G5_klass, G4_length);
521         }
522         // I0 -&gt; O0: new array
523       }
524       break;
525 
526     case new_multi_array_id:
527       { // O0: klass
528         // O1: rank
529         // O2: address of 1st dimension
530         __ set_info(&quot;new_multi_array&quot;, dont_gc_arguments);
531         oop_maps = generate_stub_call(sasm, I0, CAST_FROM_FN_PTR(address, new_multi_array), I0, I1, I2);
532         // I0 -&gt; O0: new multi array
533       }
534       break;
535 
536     case register_finalizer_id:
537       {
538         __ set_info(&quot;register_finalizer&quot;, dont_gc_arguments);
539 
540         // load the klass and check the has finalizer flag
541         Label register_finalizer;
542         Register t = O1;
543         __ load_klass(O0, t);
544         __ ld(t, in_bytes(Klass::access_flags_offset()), t);
545         __ set(JVM_ACC_HAS_FINALIZER, G3);
546         __ andcc(G3, t, G0);
547         __ br(Assembler::notZero, false, Assembler::pt, register_finalizer);
548         __ delayed()-&gt;nop();
549 
550         // do a leaf return
551         __ retl();
552         __ delayed()-&gt;nop();
553 
554         __ bind(register_finalizer);
555         OopMap* oop_map = save_live_registers(sasm);
556         int call_offset = __ call_RT(noreg, noreg,
557                                      CAST_FROM_FN_PTR(address, SharedRuntime::register_finalizer), I0);
558         oop_maps = new OopMapSet();
559         oop_maps-&gt;add_gc_map(call_offset, oop_map);
560 
561         // Now restore all the live registers
562         restore_live_registers(sasm);
563 
564         __ ret();
565         __ delayed()-&gt;restore();
566       }
567       break;
568 
569     case throw_range_check_failed_id:
570       { __ set_info(&quot;range_check_failed&quot;, dont_gc_arguments); // arguments will be discarded
571         // G4: index
572         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_range_check_exception), true);
573       }
574       break;
575 
576     case throw_index_exception_id:
577       { __ set_info(&quot;index_range_check_failed&quot;, dont_gc_arguments); // arguments will be discarded
578         // G4: index
579         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_index_exception), true);
580       }
581       break;
582 
583     case throw_div0_exception_id:
584       { __ set_info(&quot;throw_div0_exception&quot;, dont_gc_arguments);
585         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_div0_exception), false);
586       }
587       break;
588 
589     case throw_null_pointer_exception_id:
590       { __ set_info(&quot;throw_null_pointer_exception&quot;, dont_gc_arguments);
591         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_null_pointer_exception), false);
592       }
593       break;
594 
595     case handle_exception_id:
596       { __ set_info(&quot;handle_exception&quot;, dont_gc_arguments);
597         oop_maps = generate_handle_exception(id, sasm);
598       }
599       break;
600 
601     case handle_exception_from_callee_id:
602       { __ set_info(&quot;handle_exception_from_callee&quot;, dont_gc_arguments);
603         oop_maps = generate_handle_exception(id, sasm);
604       }
605       break;
606 
607     case unwind_exception_id:
608       {
609         // O0: exception
610         // I7: address of call to this method
611 
612         __ set_info(&quot;unwind_exception&quot;, dont_gc_arguments);
613         __ mov(Oexception, Oexception-&gt;after_save());
614         __ add(I7, frame::pc_return_offset, Oissuing_pc-&gt;after_save());
615 
616         __ call_VM_leaf(L7_thread_cache, CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address),
617                         G2_thread, Oissuing_pc-&gt;after_save());
618         __ verify_not_null_oop(Oexception-&gt;after_save());
619 
620         // Restore SP from L7 if the exception PC is a method handle call site.
621         __ mov(O0, G5);  // Save the target address.
622         __ lduw(Address(G2_thread, JavaThread::is_method_handle_return_offset()), L0);
623         __ tst(L0);  // Condition codes are preserved over the restore.
624         __ restore();
625 
626         __ jmp(G5, 0);
627         __ delayed()-&gt;movcc(Assembler::notZero, false, Assembler::icc, L7_mh_SP_save, SP);  // Restore SP if required.
628       }
629       break;
630 
631     case throw_array_store_exception_id:
632       {
633         __ set_info(&quot;throw_array_store_exception&quot;, dont_gc_arguments);
634         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_array_store_exception), true);
635       }
636       break;
637 
638     case throw_class_cast_exception_id:
639       {
640         // G4: object
641         __ set_info(&quot;throw_class_cast_exception&quot;, dont_gc_arguments);
642         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_class_cast_exception), true);
643       }
644       break;
645 
646     case throw_incompatible_class_change_error_id:
647       {
648         __ set_info(&quot;throw_incompatible_class_cast_exception&quot;, dont_gc_arguments);
649         oop_maps = generate_exception_throw(sasm, CAST_FROM_FN_PTR(address, throw_incompatible_class_change_error), false);
650       }
651       break;
652 
653     case slow_subtype_check_id:
654       { // Support for uint StubRoutine::partial_subtype_check( Klass sub, Klass super );
655         // Arguments :
656         //
657         //      ret  : G3
658         //      sub  : G3, argument, destroyed
659         //      super: G1, argument, not changed
660         //      raddr: O7, blown by call
661         Label miss;
662 
663         __ save_frame(0);               // Blow no registers!
664 
665         __ check_klass_subtype_slow_path(G3, G1, L0, L1, L2, L4, NULL, &amp;miss);
666 
667         __ mov(1, G3);
668         __ ret();                       // Result in G5 is &#39;true&#39;
669         __ delayed()-&gt;restore();        // free copy or add can go here
670 
671         __ bind(miss);
672         __ mov(0, G3);
673         __ ret();                       // Result in G5 is &#39;false&#39;
674         __ delayed()-&gt;restore();        // free copy or add can go here
675       }
676 
677     case monitorenter_nofpu_id:
678     case monitorenter_id:
679       { // G4: object
680         // G5: lock address
681         __ set_info(&quot;monitorenter&quot;, dont_gc_arguments);
682 
683         int save_fpu_registers = (id == monitorenter_id);
684         // make a frame and preserve the caller&#39;s caller-save registers
685         OopMap* oop_map = save_live_registers(sasm, save_fpu_registers);
686 
687         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, monitorenter), G4, G5);
688 
689         oop_maps = new OopMapSet();
690         oop_maps-&gt;add_gc_map(call_offset, oop_map);
691         restore_live_registers(sasm, save_fpu_registers);
692 
693         __ ret();
694         __ delayed()-&gt;restore();
695       }
696       break;
697 
698     case monitorexit_nofpu_id:
699     case monitorexit_id:
700       { // G4: lock address
701         // note: really a leaf routine but must setup last java sp
702         //       =&gt; use call_RT for now (speed can be improved by
703         //       doing last java sp setup manually)
704         __ set_info(&quot;monitorexit&quot;, dont_gc_arguments);
705 
706         int save_fpu_registers = (id == monitorexit_id);
707         // make a frame and preserve the caller&#39;s caller-save registers
708         OopMap* oop_map = save_live_registers(sasm, save_fpu_registers);
709 
710         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, monitorexit), G4);
711 
712         oop_maps = new OopMapSet();
713         oop_maps-&gt;add_gc_map(call_offset, oop_map);
714         restore_live_registers(sasm, save_fpu_registers);
715 
716         __ ret();
717         __ delayed()-&gt;restore();
718       }
719       break;
720 
721     case deoptimize_id:
722       {
723         __ set_info(&quot;deoptimize&quot;, dont_gc_arguments);
724         OopMap* oop_map = save_live_registers(sasm);
725         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, deoptimize), G4);
726         oop_maps = new OopMapSet();
727         oop_maps-&gt;add_gc_map(call_offset, oop_map);
728         restore_live_registers(sasm);
729         DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
730         assert(deopt_blob != NULL, &quot;deoptimization blob must have been created&quot;);
731         AddressLiteral dest(deopt_blob-&gt;unpack_with_reexecution());
732         __ jump_to(dest, O0);
733         __ delayed()-&gt;restore();
734       }
735       break;
736 
737     case access_field_patching_id:
738       { __ set_info(&quot;access_field_patching&quot;, dont_gc_arguments);
739         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, access_field_patching));
740       }
741       break;
742 
743     case load_klass_patching_id:
744       { __ set_info(&quot;load_klass_patching&quot;, dont_gc_arguments);
745         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_klass_patching));
746       }
747       break;
748 
749     case load_mirror_patching_id:
750       { __ set_info(&quot;load_mirror_patching&quot;, dont_gc_arguments);
751         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_mirror_patching));
752       }
753       break;
754 
755     case load_appendix_patching_id:
756       { __ set_info(&quot;load_appendix_patching&quot;, dont_gc_arguments);
757         oop_maps = generate_patching(sasm, CAST_FROM_FN_PTR(address, move_appendix_patching));
758       }
759       break;
760 
761     case dtrace_object_alloc_id:
762       { // O0: object
763         __ set_info(&quot;dtrace_object_alloc&quot;, dont_gc_arguments);
764         // we can&#39;t gc here so skip the oopmap but make sure that all
765         // the live registers get saved.
766         save_live_registers(sasm);
767 
768         __ save_thread(L7_thread_cache);
769         __ call(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc),
770                 relocInfo::runtime_call_type);
771         __ delayed()-&gt;mov(I0, O0);
772         __ restore_thread(L7_thread_cache);
773 
774         restore_live_registers(sasm);
775         __ ret();
776         __ delayed()-&gt;restore();
777       }
778       break;
779 
780     case predicate_failed_trap_id:
781       {
782         __ set_info(&quot;predicate_failed_trap&quot;, dont_gc_arguments);
783         OopMap* oop_map = save_live_registers(sasm);
784 
785         int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, predicate_failed_trap));
786 
787         oop_maps = new OopMapSet();
788         oop_maps-&gt;add_gc_map(call_offset, oop_map);
789 
790         DeoptimizationBlob* deopt_blob = SharedRuntime::deopt_blob();
791         assert(deopt_blob != NULL, &quot;deoptimization blob must have been created&quot;);
792         restore_live_registers(sasm);
793 
794         AddressLiteral dest(deopt_blob-&gt;unpack_with_reexecution());
795         __ jump_to(dest, O0);
796         __ delayed()-&gt;restore();
797       }
798       break;
799 
800     default:
801       { __ set_info(&quot;unimplemented entry&quot;, dont_gc_arguments);
802         __ save_frame(0);
803         __ set((int)id, O1);
804         __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, unimplemented_entry), O1);
805         __ should_not_reach_here();
806       }
807       break;
808   }
809   return oop_maps;
810 }
811 
812 
813 OopMapSet* Runtime1::generate_handle_exception(StubID id, StubAssembler* sasm) {
814   __ block_comment(&quot;generate_handle_exception&quot;);
815 
816   // Save registers, if required.
817   OopMapSet* oop_maps = new OopMapSet();
818   OopMap* oop_map = NULL;
819   switch (id) {
820   case forward_exception_id:
821     // We&#39;re handling an exception in the context of a compiled frame.
822     // The registers have been saved in the standard places.  Perform
823     // an exception lookup in the caller and dispatch to the handler
824     // if found.  Otherwise unwind and dispatch to the callers
825     // exception handler.
826      oop_map = generate_oop_map(sasm, true);
827 
828      // transfer the pending exception to the exception_oop
829      __ ld_ptr(G2_thread, in_bytes(JavaThread::pending_exception_offset()), Oexception);
830      __ ld_ptr(Oexception, 0, G0);
831      __ st_ptr(G0, G2_thread, in_bytes(JavaThread::pending_exception_offset()));
832      __ add(I7, frame::pc_return_offset, Oissuing_pc);
833     break;
834   case handle_exception_id:
835     // At this point all registers MAY be live.
836     oop_map = save_live_registers(sasm);
837     __ mov(Oexception-&gt;after_save(),  Oexception);
838     __ mov(Oissuing_pc-&gt;after_save(), Oissuing_pc);
839     break;
840   case handle_exception_from_callee_id:
841     // At this point all registers except exception oop (Oexception)
842     // and exception pc (Oissuing_pc) are dead.
843     oop_map = new OopMap(frame_size_in_bytes / sizeof(jint), 0);
844     sasm-&gt;set_frame_size(frame_size_in_bytes / BytesPerWord);
845     __ save_frame_c1(frame_size_in_bytes);
846     __ mov(Oexception-&gt;after_save(),  Oexception);
847     __ mov(Oissuing_pc-&gt;after_save(), Oissuing_pc);
848     break;
849   default:  ShouldNotReachHere();
850   }
851 
852   __ verify_not_null_oop(Oexception);
853 
854 #ifdef ASSERT
855   // check that fields in JavaThread for exception oop and issuing pc are
856   // empty before writing to them
857   Label oop_empty;
858   Register scratch = I7;  // We can use I7 here because it&#39;s overwritten later anyway.
859   __ ld_ptr(Address(G2_thread, JavaThread::exception_oop_offset()), scratch);
860   __ br_null(scratch, false, Assembler::pt, oop_empty);
861   __ delayed()-&gt;nop();
862   __ stop(&quot;exception oop already set&quot;);
863   __ bind(oop_empty);
864 
865   Label pc_empty;
866   __ ld_ptr(Address(G2_thread, JavaThread::exception_pc_offset()), scratch);
867   __ br_null(scratch, false, Assembler::pt, pc_empty);
868   __ delayed()-&gt;nop();
869   __ stop(&quot;exception pc already set&quot;);
870   __ bind(pc_empty);
871 #endif
872 
873   // save the exception and issuing pc in the thread
874   __ st_ptr(Oexception,  G2_thread, in_bytes(JavaThread::exception_oop_offset()));
875   __ st_ptr(Oissuing_pc, G2_thread, in_bytes(JavaThread::exception_pc_offset()));
876 
877   // use the throwing pc as the return address to lookup (has bci &amp; oop map)
878   __ mov(Oissuing_pc, I7);
879   __ sub(I7, frame::pc_return_offset, I7);
880   int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, exception_handler_for_pc));
881   oop_maps-&gt;add_gc_map(call_offset, oop_map);
882 
883   // Note: if nmethod has been deoptimized then regardless of
884   // whether it had a handler or not we will deoptimize
885   // by entering the deopt blob with a pending exception.
886 
887   // Restore the registers that were saved at the beginning, remove
888   // the frame and jump to the exception handler.
889   switch (id) {
890   case forward_exception_id:
891   case handle_exception_id:
892     restore_live_registers(sasm);
893     __ jmp(O0, 0);
894     __ delayed()-&gt;restore();
895     break;
896   case handle_exception_from_callee_id:
897     // Restore SP from L7 if the exception PC is a method handle call site.
898     __ mov(O0, G5);  // Save the target address.
899     __ lduw(Address(G2_thread, JavaThread::is_method_handle_return_offset()), L0);
900     __ tst(L0);  // Condition codes are preserved over the restore.
901     __ restore();
902 
903     __ jmp(G5, 0);  // jump to the exception handler
904     __ delayed()-&gt;movcc(Assembler::notZero, false, Assembler::icc, L7_mh_SP_save, SP);  // Restore SP if required.
905     break;
906   default:  ShouldNotReachHere();
907   }
908 
909   return oop_maps;
910 }
911 
912 
913 #undef __
914 
915 const char *Runtime1::pd_name_for_address(address entry) {
916   return &quot;&lt;unknown function&gt;&quot;;
917 }
    </pre>
  </body>
</html>