<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/sparc/macroAssembler_sparc.inline.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef CPU_SPARC_MACROASSEMBLER_SPARC_INLINE_HPP
 26 #define CPU_SPARC_MACROASSEMBLER_SPARC_INLINE_HPP
 27 
 28 #include &quot;asm/assembler.inline.hpp&quot;
 29 #include &quot;asm/macroAssembler.hpp&quot;
 30 #include &quot;asm/codeBuffer.hpp&quot;
 31 #include &quot;code/codeCache.hpp&quot;
 32 
 33 inline bool Address::is_simm13(int offset) { return Assembler::is_simm13(disp() + offset); }
 34 
 35 
 36 inline int AddressLiteral::low10() const {
 37   return Assembler::low10(value());
 38 }
 39 
 40 
 41 inline void MacroAssembler::pd_patch_instruction(address branch, address target, const char* file, int line) {
 42   jint&amp; stub_inst = *(jint*) branch;
 43   stub_inst = patched_branch(target - branch, stub_inst, 0);
 44 }
 45 
 46 // Use the right loads/stores for the platform
 47 inline void MacroAssembler::ld_ptr( Register s1, Register s2, Register d ) {
 48   Assembler::ldx(s1, s2, d);
 49 }
 50 
 51 inline void MacroAssembler::ld_ptr( Register s1, int simm13a, Register d ) {
 52   Assembler::ldx(s1, simm13a, d);
 53 }
 54 
 55 #ifdef ASSERT
 56 // ByteSize is only a class when ASSERT is defined, otherwise it&#39;s an int.
 57 inline void MacroAssembler::ld_ptr( Register s1, ByteSize simm13a, Register d ) {
 58   ld_ptr(s1, in_bytes(simm13a), d);
 59 }
 60 #endif
 61 
 62 inline void MacroAssembler::ld_ptr( Register s1, RegisterOrConstant s2, Register d ) {
 63   ldx(s1, s2, d);
 64 }
 65 
 66 inline void MacroAssembler::ld_ptr(const Address&amp; a, Register d, int offset) {
 67   ldx(a, d, offset);
 68 }
 69 
 70 inline void MacroAssembler::st_ptr( Register d, Register s1, Register s2 ) {
 71   Assembler::stx(d, s1, s2);
 72 }
 73 
 74 inline void MacroAssembler::st_ptr( Register d, Register s1, int simm13a ) {
 75   Assembler::stx(d, s1, simm13a);
 76 }
 77 
 78 #ifdef ASSERT
 79 // ByteSize is only a class when ASSERT is defined, otherwise it&#39;s an int.
 80 inline void MacroAssembler::st_ptr( Register d, Register s1, ByteSize simm13a ) {
 81   st_ptr(d, s1, in_bytes(simm13a));
 82 }
 83 #endif
 84 
 85 inline void MacroAssembler::st_ptr( Register d, Register s1, RegisterOrConstant s2 ) {
 86   stx(d, s1, s2);
 87 }
 88 
 89 inline void MacroAssembler::st_ptr(Register d, const Address&amp; a, int offset) {
 90   stx(d, a, offset);
 91 }
 92 
 93 // Use the right loads/stores for the platform
 94 inline void MacroAssembler::ld_long( Register s1, Register s2, Register d ) {
 95   Assembler::ldx(s1, s2, d);
 96 }
 97 
 98 inline void MacroAssembler::ld_long( Register s1, int simm13a, Register d ) {
 99   Assembler::ldx(s1, simm13a, d);
100 }
101 
102 inline void MacroAssembler::ld_long( Register s1, RegisterOrConstant s2, Register d ) {
103   ldx(s1, s2, d);
104 }
105 
106 inline void MacroAssembler::ld_long(const Address&amp; a, Register d, int offset) {
107   ldx(a, d, offset);
108 }
109 
110 inline void MacroAssembler::st_long( Register d, Register s1, Register s2 ) {
111   Assembler::stx(d, s1, s2);
112 }
113 
114 inline void MacroAssembler::st_long( Register d, Register s1, int simm13a ) {
115   Assembler::stx(d, s1, simm13a);
116 }
117 
118 inline void MacroAssembler::st_long( Register d, Register s1, RegisterOrConstant s2 ) {
119   stx(d, s1, s2);
120 }
121 
122 inline void MacroAssembler::st_long( Register d, const Address&amp; a, int offset ) {
123   stx(d, a, offset);
124 }
125 
126 inline void MacroAssembler::stbool(Register d, const Address&amp; a) { stb(d, a); }
127 inline void MacroAssembler::ldbool(const Address&amp; a, Register d) { ldub(a, d); }
128 inline void MacroAssembler::movbool( bool boolconst, Register d) { mov( (int) boolconst, d); }
129 
130 
131 inline void MacroAssembler::signx( Register s, Register d ) { sra( s, G0, d); }
132 inline void MacroAssembler::signx( Register d )             { sra( d, G0, d); }
133 
134 inline void MacroAssembler::not1( Register s, Register d ) { xnor( s, G0, d ); }
135 inline void MacroAssembler::not1( Register d )             { xnor( d, G0, d ); }
136 
137 inline void MacroAssembler::neg( Register s, Register d ) { sub( G0, s, d ); }
138 inline void MacroAssembler::neg( Register d )             { sub( G0, d, d ); }
139 
140 inline void MacroAssembler::cas(  Register s1, Register s2, Register d) { casa( s1, s2, d, ASI_PRIMARY); }
141 inline void MacroAssembler::casx( Register s1, Register s2, Register d) { casxa(s1, s2, d, ASI_PRIMARY); }
142 
143 // Functions for isolating 64 bit atomic swaps for LP64
144 // cas_ptr will perform cas for 32 bit VM&#39;s and casx for 64 bit VM&#39;s
145 inline void MacroAssembler::cas_ptr(  Register s1, Register s2, Register d) {
146   casx( s1, s2, d );
147 }
148 
149 // Functions for isolating 64 bit shifts for LP64
150 
151 inline void MacroAssembler::sll_ptr( Register s1, Register s2, Register d ) {
152   Assembler::sllx(s1, s2, d);
153 }
154 
155 inline void MacroAssembler::sll_ptr( Register s1, int imm6a,   Register d ) {
156   Assembler::sllx(s1, imm6a, d);
157 }
158 
159 inline void MacroAssembler::srl_ptr( Register s1, Register s2, Register d ) {
160   Assembler::srlx(s1, s2, d);
161 }
162 
163 inline void MacroAssembler::srl_ptr( Register s1, int imm6a,   Register d ) {
164   Assembler::srlx(s1, imm6a, d);
165 }
166 
167 inline void MacroAssembler::sll_ptr( Register s1, RegisterOrConstant s2, Register d ) {
168   if (s2.is_register())  sll_ptr(s1, s2.as_register(), d);
169   else                   sll_ptr(s1, s2.as_constant(), d);
170 }
171 
172 inline void MacroAssembler::casl(  Register s1, Register s2, Register d) { casa( s1, s2, d, ASI_PRIMARY_LITTLE); }
173 inline void MacroAssembler::casxl( Register s1, Register s2, Register d) { casxa(s1, s2, d, ASI_PRIMARY_LITTLE); }
174 
175 inline void MacroAssembler::inc(   Register d,  int const13 ) { add(   d, const13, d); }
176 inline void MacroAssembler::inccc( Register d,  int const13 ) { addcc( d, const13, d); }
177 
178 inline void MacroAssembler::dec(   Register d,  int const13 ) { sub(   d, const13, d); }
179 inline void MacroAssembler::deccc( Register d,  int const13 ) { subcc( d, const13, d); }
180 
181 // Use the right branch for the platform
182 
183 inline void MacroAssembler::br( Condition c, bool a, Predict p, address d, relocInfo::relocType rt ) {
184   Assembler::bp(c, a, icc, p, d, rt);
185 }
186 
187 inline void MacroAssembler::br( Condition c, bool a, Predict p, Label&amp; L ) {
188   // See note[+] on &#39;avoid_pipeline_stall()&#39;, in &quot;assembler_sparc.inline.hpp&quot;.
189   avoid_pipeline_stall();
190   br(c, a, p, target(L));
191 }
192 
193 
194 // Branch that tests either xcc or icc depending on the
195 // architecture compiled (LP64 or not)
196 inline void MacroAssembler::brx( Condition c, bool a, Predict p, address d, relocInfo::relocType rt ) {
197     Assembler::bp(c, a, xcc, p, d, rt);
198 }
199 
200 inline void MacroAssembler::brx( Condition c, bool a, Predict p, Label&amp; L ) {
201   avoid_pipeline_stall();
202   brx(c, a, p, target(L));
203 }
204 
205 inline void MacroAssembler::ba( Label&amp; L ) {
206   br(always, false, pt, L);
207 }
208 
209 // Warning: V9 only functions
210 inline void MacroAssembler::bp( Condition c, bool a, CC cc, Predict p, address d, relocInfo::relocType rt ) {
211   Assembler::bp(c, a, cc, p, d, rt);
212 }
213 
214 inline void MacroAssembler::bp( Condition c, bool a, CC cc, Predict p, Label&amp; L ) {
215   Assembler::bp(c, a, cc, p, L);
216 }
217 
218 inline void MacroAssembler::fb( Condition c, bool a, Predict p, address d, relocInfo::relocType rt ) {
219   fbp(c, a, fcc0, p, d, rt);
220 }
221 
222 inline void MacroAssembler::fb( Condition c, bool a, Predict p, Label&amp; L ) {
223   avoid_pipeline_stall();
224   fb(c, a, p, target(L));
225 }
226 
227 inline void MacroAssembler::fbp( Condition c, bool a, CC cc, Predict p, address d, relocInfo::relocType rt ) {
228   Assembler::fbp(c, a, cc, p, d, rt);
229 }
230 
231 inline void MacroAssembler::fbp( Condition c, bool a, CC cc, Predict p, Label&amp; L ) {
232   Assembler::fbp(c, a, cc, p, L);
233 }
234 
235 inline void MacroAssembler::jmp( Register s1, Register s2 ) { jmpl( s1, s2, G0 ); }
236 inline void MacroAssembler::jmp( Register s1, int simm13a, RelocationHolder const&amp; rspec ) { jmpl( s1, simm13a, G0, rspec); }
237 
238 inline bool MacroAssembler::is_far_target(address d) {
239   if (ForceUnreachable) {
240     // References outside the code cache should be treated as far
241     return d &lt; CodeCache::low_bound() || d &gt; CodeCache::high_bound();
242   }
243   return !is_in_wdisp30_range(d, CodeCache::low_bound()) || !is_in_wdisp30_range(d, CodeCache::high_bound());
244 }
245 
246 // Call with a check to see if we need to deal with the added
247 // expense of relocation and if we overflow the displacement
248 // of the quick call instruction.
249 inline void MacroAssembler::call( address d, relocInfo::relocType rt ) {
250   MacroAssembler::call(d, Relocation::spec_simple(rt));
251 }
252 
253 inline void MacroAssembler::call( address d, RelocationHolder const&amp; rspec ) {
254   intptr_t disp;
255   // NULL is ok because it will be relocated later.
256   // Must change NULL to a reachable address in order to
257   // pass asserts here and in wdisp.
258   if ( d == NULL )
259     d = pc();
260 
261   // Is this address within range of the call instruction?
262   // If not, use the expensive instruction sequence
263   if (is_far_target(d)) {
264     relocate(rspec);
265     AddressLiteral dest(d);
266     jumpl_to(dest, O7, O7);
267   } else {
268     Assembler::call(d, rspec);
269   }
270 }
271 
272 inline void MacroAssembler::call( Label&amp; L, relocInfo::relocType rt ) {
273   avoid_pipeline_stall();
274   MacroAssembler::call(target(L), rt);
275 }
276 
277 
278 inline void MacroAssembler::callr( Register s1, Register s2 ) { jmpl( s1, s2, O7 ); }
279 inline void MacroAssembler::callr( Register s1, int simm13a, RelocationHolder const&amp; rspec ) { jmpl( s1, simm13a, O7, rspec); }
280 
<a name="1" id="anc1"></a><span class="line-removed">281 // prefetch instruction</span>
<span class="line-removed">282 inline void MacroAssembler::iprefetch( address d, relocInfo::relocType rt ) {</span>
<span class="line-removed">283   Assembler::bp( never, true, xcc, pt, d, rt );</span>
<span class="line-removed">284     Assembler::bp( never, true, xcc, pt, d, rt );</span>
<span class="line-removed">285 }</span>
<span class="line-removed">286 inline void MacroAssembler::iprefetch( Label&amp; L) { iprefetch( target(L) ); }</span>
<span class="line-removed">287 </span>
288 inline void MacroAssembler::tst( Register s ) { orcc( G0, s, G0 ); }
289 
290 inline void MacroAssembler::ret( bool trace ) {
291   if (trace) {
292     mov(I7, O7); // traceable register
293     JMP(O7, 2 * BytesPerInstWord);
294   } else {
295     jmpl( I7, 2 * BytesPerInstWord, G0 );
296   }
297 }
298 
299 inline void MacroAssembler::retl( bool trace ) {
300   if (trace) {
301     JMP(O7, 2 * BytesPerInstWord);
302   } else {
303     jmpl( O7, 2 * BytesPerInstWord, G0 );
304   }
305 }
306 
307 
308 inline void MacroAssembler::cmp(  Register s1, Register s2 ) { subcc( s1, s2, G0 ); }
309 inline void MacroAssembler::cmp(  Register s1, int simm13a ) { subcc( s1, simm13a, G0 ); }
310 
311 // Note:  All MacroAssembler::set_foo functions are defined out-of-line.
312 
313 
314 // Loads the current PC of the following instruction as an immediate value in
315 // 2 instructions.  All PCs in the CodeCache are within 2 Gig of each other.
316 inline intptr_t MacroAssembler::load_pc_address( Register reg, int bytes_to_skip ) {
317   intptr_t thepc = (intptr_t)pc() + 2*BytesPerInstWord + bytes_to_skip;
318   Unimplemented();
319   return thepc;
320 }
321 
322 
323 inline void MacroAssembler::load_contents(const AddressLiteral&amp; addrlit, Register d, int offset) {
324   assert_not_delayed();
325   if (ForceUnreachable) {
326     patchable_sethi(addrlit, d);
327   } else {
328     sethi(addrlit, d);
329   }
330   ld(d, addrlit.low10() + offset, d);
331 }
332 
333 
334 inline void MacroAssembler::load_bool_contents(const AddressLiteral&amp; addrlit, Register d, int offset) {
335   assert_not_delayed();
336   if (ForceUnreachable) {
337     patchable_sethi(addrlit, d);
338   } else {
339     sethi(addrlit, d);
340   }
341   ldub(d, addrlit.low10() + offset, d);
342 }
343 
344 
345 inline void MacroAssembler::load_ptr_contents(const AddressLiteral&amp; addrlit, Register d, int offset) {
346   assert_not_delayed();
347   if (ForceUnreachable) {
348     patchable_sethi(addrlit, d);
349   } else {
350     sethi(addrlit, d);
351   }
352   ld_ptr(d, addrlit.low10() + offset, d);
353 }
354 
355 
356 inline void MacroAssembler::store_contents(Register s, const AddressLiteral&amp; addrlit, Register temp, int offset) {
357   assert_not_delayed();
358   if (ForceUnreachable) {
359     patchable_sethi(addrlit, temp);
360   } else {
361     sethi(addrlit, temp);
362   }
363   st(s, temp, addrlit.low10() + offset);
364 }
365 
366 
367 inline void MacroAssembler::store_ptr_contents(Register s, const AddressLiteral&amp; addrlit, Register temp, int offset) {
368   assert_not_delayed();
369   if (ForceUnreachable) {
370     patchable_sethi(addrlit, temp);
371   } else {
372     sethi(addrlit, temp);
373   }
374   st_ptr(s, temp, addrlit.low10() + offset);
375 }
376 
377 
378 // This code sequence is relocatable to any address, even on LP64.
379 inline void MacroAssembler::jumpl_to(const AddressLiteral&amp; addrlit, Register temp, Register d, int offset) {
380   assert_not_delayed();
381   // Force fixed length sethi because NativeJump and NativeFarCall don&#39;t handle
382   // variable length instruction streams.
383   patchable_sethi(addrlit, temp);
384   jmpl(temp, addrlit.low10() + offset, d);
385 }
386 
387 
388 inline void MacroAssembler::jump_to(const AddressLiteral&amp; addrlit, Register temp, int offset) {
389   jumpl_to(addrlit, temp, G0, offset);
390 }
391 
392 
393 inline void MacroAssembler::jump_indirect_to(Address&amp; a, Register temp,
394                                              int ld_offset, int jmp_offset) {
395   assert_not_delayed();
396   //sethi(al);                   // sethi is caller responsibility for this one
397   ld_ptr(a, temp, ld_offset);
398   jmp(temp, jmp_offset);
399 }
400 
401 
402 inline void MacroAssembler::set_metadata(Metadata* obj, Register d) {
403   set_metadata(allocate_metadata_address(obj), d);
404 }
405 
406 inline void MacroAssembler::set_metadata_constant(Metadata* obj, Register d) {
407   set_metadata(constant_metadata_address(obj), d);
408 }
409 
410 inline void MacroAssembler::set_metadata(const AddressLiteral&amp; obj_addr, Register d) {
411   assert(obj_addr.rspec().type() == relocInfo::metadata_type, &quot;must be a metadata reloc&quot;);
412   set(obj_addr, d);
413 }
414 
415 inline void MacroAssembler::set_oop(jobject obj, Register d) {
416   set_oop(allocate_oop_address(obj), d);
417 }
418 
419 
420 inline void MacroAssembler::set_oop_constant(jobject obj, Register d) {
421   set_oop(constant_oop_address(obj), d);
422 }
423 
424 
425 inline void MacroAssembler::set_oop(const AddressLiteral&amp; obj_addr, Register d) {
426   assert(obj_addr.rspec().type() == relocInfo::oop_type, &quot;must be an oop reloc&quot;);
427   set(obj_addr, d);
428 }
429 
430 
431 inline void MacroAssembler::load_argument( Argument&amp; a, Register  d ) {
432   if (a.is_register())
433     mov(a.as_register(), d);
434   else
435     ld (a.as_address(),  d);
436 }
437 
438 inline void MacroAssembler::store_argument( Register s, Argument&amp; a ) {
439   if (a.is_register())
440     mov(s, a.as_register());
441   else
442     st_ptr (s, a.as_address());         // ABI says everything is right justified.
443 }
444 
445 inline void MacroAssembler::store_ptr_argument( Register s, Argument&amp; a ) {
446   if (a.is_register())
447     mov(s, a.as_register());
448   else
449     st_ptr (s, a.as_address());
450 }
451 
452 
453 inline void MacroAssembler::store_float_argument( FloatRegister s, Argument&amp; a ) {
454   if (a.is_float_register())
455 // V9 ABI has F1, F3, F5 are used to pass instead of O0, O1, O2
456     fmov(FloatRegisterImpl::S, s, a.as_float_register() );
457   else
458     // Floats are stored in the high half of the stack entry
459     // The low half is undefined per the ABI.
460     stf(FloatRegisterImpl::S, s, a.as_address(), sizeof(jfloat));
461 }
462 
463 inline void MacroAssembler::store_double_argument( FloatRegister s, Argument&amp; a ) {
464   if (a.is_float_register())
465 // V9 ABI has D0, D2, D4 are used to pass instead of O0, O1, O2
466     fmov(FloatRegisterImpl::D, s, a.as_double_register() );
467   else
468     stf(FloatRegisterImpl::D, s, a.as_address());
469 }
470 
471 inline void MacroAssembler::store_long_argument( Register s, Argument&amp; a ) {
472   if (a.is_register())
473     mov(s, a.as_register());
474   else
475     stx(s, a.as_address());
476 }
477 
478 inline void MacroAssembler::round_to( Register r, int modulus ) {
479   assert_not_delayed();
480   inc( r, modulus - 1 );
481   and3( r, -modulus, r );
482 }
483 
484 inline void MacroAssembler::add(Register s1, int simm13a, Register d, relocInfo::relocType rtype) {
485   relocate(rtype);
486   add(s1, simm13a, d);
487 }
488 inline void MacroAssembler::add(Register s1, int simm13a, Register d, RelocationHolder const&amp; rspec) {
489   relocate(rspec);
490   add(s1, simm13a, d);
491 }
492 
493 // form effective addresses this way:
494 inline void MacroAssembler::add(const Address&amp; a, Register d, int offset) {
495   if (a.has_index())   add(a.base(), a.index(),         d);
496   else               { add(a.base(), a.disp() + offset, d, a.rspec(offset)); offset = 0; }
497   if (offset != 0)     add(d,        offset,            d);
498 }
499 inline void MacroAssembler::add(Register s1, RegisterOrConstant s2, Register d, int offset) {
500   if (s2.is_register())  add(s1, s2.as_register(),          d);
501   else                 { add(s1, s2.as_constant() + offset, d); offset = 0; }
502   if (offset != 0)       add(d,  offset,                    d);
503 }
504 
505 inline void MacroAssembler::andn(Register s1, RegisterOrConstant s2, Register d) {
506   if (s2.is_register())  andn(s1, s2.as_register(), d);
507   else                   andn(s1, s2.as_constant(), d);
508 }
509 
510 inline void MacroAssembler::btst( Register s1,  Register s2 ) { andcc( s1, s2, G0 ); }
511 inline void MacroAssembler::btst( int simm13a,  Register s )  { andcc( s,  simm13a, G0 ); }
512 
513 inline void MacroAssembler::bset( Register s1,  Register s2 ) { or3( s1, s2, s2 ); }
514 inline void MacroAssembler::bset( int simm13a,  Register s )  { or3( s,  simm13a, s ); }
515 
516 inline void MacroAssembler::bclr( Register s1,  Register s2 ) { andn( s1, s2, s2 ); }
517 inline void MacroAssembler::bclr( int simm13a,  Register s )  { andn( s,  simm13a, s ); }
518 
519 inline void MacroAssembler::btog( Register s1,  Register s2 ) { xor3( s1, s2, s2 ); }
520 inline void MacroAssembler::btog( int simm13a,  Register s )  { xor3( s,  simm13a, s ); }
521 
522 inline void MacroAssembler::clr( Register d ) { or3( G0, G0, d ); }
523 
524 inline void MacroAssembler::clrb( Register s1, Register s2) { stb( G0, s1, s2 ); }
525 inline void MacroAssembler::clrh( Register s1, Register s2) { sth( G0, s1, s2 ); }
526 inline void MacroAssembler::clr(  Register s1, Register s2) { stw( G0, s1, s2 ); }
527 inline void MacroAssembler::clrx( Register s1, Register s2) { stx( G0, s1, s2 ); }
528 
529 inline void MacroAssembler::clrb( Register s1, int simm13a) { stb( G0, s1, simm13a); }
530 inline void MacroAssembler::clrh( Register s1, int simm13a) { sth( G0, s1, simm13a); }
531 inline void MacroAssembler::clr(  Register s1, int simm13a) { stw( G0, s1, simm13a); }
532 inline void MacroAssembler::clrx( Register s1, int simm13a) { stx( G0, s1, simm13a); }
533 
534 inline void MacroAssembler::clruw( Register s, Register d ) { srl( s, G0, d); }
535 inline void MacroAssembler::clruwu( Register d ) { srl( d, G0, d); }
536 
537 // Make all 32 bit loads signed so 64 bit registers maintain proper sign
538 inline void MacroAssembler::ld(  Register s1, Register s2, Register d)      { ldsw( s1, s2, d); }
539 inline void MacroAssembler::ld(  Register s1, int simm13a, Register d)      { ldsw( s1, simm13a, d); }
540 
541 #ifdef ASSERT
542   // ByteSize is only a class when ASSERT is defined, otherwise it&#39;s an int.
543 inline void MacroAssembler::ld(Register s1, ByteSize simm13a, Register d) { ldsw( s1, in_bytes(simm13a), d); }
544 #endif
545 
546 inline void MacroAssembler::ld(  const Address&amp; a, Register d, int offset) {
547   if (a.has_index()) { assert(offset == 0, &quot;&quot;); ld(  a.base(), a.index(),         d); }
548   else               {                          ld(  a.base(), a.disp() + offset, d); }
549 }
550 
551 inline void MacroAssembler::ldsb(const Address&amp; a, Register d, int offset) {
552   if (a.has_index()) { assert(offset == 0, &quot;&quot;); ldsb(a.base(), a.index(),         d); }
553   else               {                          ldsb(a.base(), a.disp() + offset, d); }
554 }
555 inline void MacroAssembler::ldsh(const Address&amp; a, Register d, int offset) {
556   if (a.has_index()) { assert(offset == 0, &quot;&quot;); ldsh(a.base(), a.index(),         d); }
557   else               {                          ldsh(a.base(), a.disp() + offset, d); }
558 }
559 inline void MacroAssembler::ldsw(const Address&amp; a, Register d, int offset) {
560   if (a.has_index()) { assert(offset == 0, &quot;&quot;); ldsw(a.base(), a.index(),         d); }
561   else               {                          ldsw(a.base(), a.disp() + offset, d); }
562 }
563 inline void MacroAssembler::ldub(const Address&amp; a, Register d, int offset) {
564   if (a.has_index()) { assert(offset == 0, &quot;&quot;); ldub(a.base(), a.index(),         d); }
565   else               {                          ldub(a.base(), a.disp() + offset, d); }
566 }
567 inline void MacroAssembler::lduh(const Address&amp; a, Register d, int offset) {
568   if (a.has_index()) { assert(offset == 0, &quot;&quot;); lduh(a.base(), a.index(),         d); }
569   else               {                          lduh(a.base(), a.disp() + offset, d); }
570 }
571 inline void MacroAssembler::lduw(const Address&amp; a, Register d, int offset) {
572   if (a.has_index()) { assert(offset == 0, &quot;&quot;); lduw(a.base(), a.index(),         d); }
573   else               {                          lduw(a.base(), a.disp() + offset, d); }
574 }
575 inline void MacroAssembler::ldd( const Address&amp; a, Register d, int offset) {
576   if (a.has_index()) { assert(offset == 0, &quot;&quot;); ldd( a.base(), a.index(),         d); }
577   else               {                          ldd( a.base(), a.disp() + offset, d); }
578 }
579 inline void MacroAssembler::ldx( const Address&amp; a, Register d, int offset) {
580   if (a.has_index()) { assert(offset == 0, &quot;&quot;); ldx( a.base(), a.index(),         d); }
581   else               {                          ldx( a.base(), a.disp() + offset, d); }
582 }
583 
584 inline void MacroAssembler::ldub(Register s1, RegisterOrConstant s2, Register d) { ldub(Address(s1, s2), d); }
585 inline void MacroAssembler::ldsb(Register s1, RegisterOrConstant s2, Register d) { ldsb(Address(s1, s2), d); }
586 inline void MacroAssembler::lduh(Register s1, RegisterOrConstant s2, Register d) { lduh(Address(s1, s2), d); }
587 inline void MacroAssembler::ldsh(Register s1, RegisterOrConstant s2, Register d) { ldsh(Address(s1, s2), d); }
588 inline void MacroAssembler::lduw(Register s1, RegisterOrConstant s2, Register d) { lduw(Address(s1, s2), d); }
589 inline void MacroAssembler::ldsw(Register s1, RegisterOrConstant s2, Register d) { ldsw(Address(s1, s2), d); }
590 inline void MacroAssembler::ldx( Register s1, RegisterOrConstant s2, Register d) { ldx( Address(s1, s2), d); }
591 inline void MacroAssembler::ld(  Register s1, RegisterOrConstant s2, Register d) { ld(  Address(s1, s2), d); }
592 inline void MacroAssembler::ldd( Register s1, RegisterOrConstant s2, Register d) { ldd( Address(s1, s2), d); }
593 
594 inline void MacroAssembler::ldf(FloatRegisterImpl::Width w, Register s1, RegisterOrConstant s2, FloatRegister d) {
595   if (s2.is_register())  ldf(w, s1, s2.as_register(), d);
596   else                   ldf(w, s1, s2.as_constant(), d);
597 }
598 
599 inline void MacroAssembler::ldf(FloatRegisterImpl::Width w, const Address&amp; a, FloatRegister d, int offset) {
600   relocate(a.rspec(offset));
601   if (a.has_index()) {
602     assert(offset == 0, &quot;&quot;);
603     ldf(w, a.base(), a.index(), d);
604   } else {
605     ldf(w, a.base(), a.disp() + offset, d);
606   }
607 }
608 
609 inline void MacroAssembler::lduwl(Register s1, Register s2, Register d) { lduwa(s1, s2, ASI_PRIMARY_LITTLE, d); }
610 inline void MacroAssembler::ldswl(Register s1, Register s2, Register d) { ldswa(s1, s2, ASI_PRIMARY_LITTLE, d);}
611 inline void MacroAssembler::ldxl( Register s1, Register s2, Register d) { ldxa(s1, s2, ASI_PRIMARY_LITTLE, d); }
612 inline void MacroAssembler::ldfl(FloatRegisterImpl::Width w, Register s1, Register s2, FloatRegister d) { ldfa(w, s1, s2, ASI_PRIMARY_LITTLE, d); }
613 
614 // returns if membar generates anything, obviously this code should mirror
615 // membar below.
616 inline bool MacroAssembler::membar_has_effect( Membar_mask_bits const7a ) {
617   const Membar_mask_bits effective_mask =
618       Membar_mask_bits(const7a &amp; ~(LoadLoad | LoadStore | StoreStore));
619   return (effective_mask != 0);
620 }
621 
622 inline void MacroAssembler::membar( Membar_mask_bits const7a ) {
623   // Weakened for current Sparcs and TSO.  See the v9 manual, sections 8.4.3,
624   // 8.4.4.3, a.31 and a.50.
625   // Under TSO, setting bit 3, 2, or 0 is redundant, so the only value
626   // of the mmask subfield of const7a that does anything that isn&#39;t done
627   // implicitly is StoreLoad.
628   const Membar_mask_bits effective_mask =
629       Membar_mask_bits(const7a &amp; ~(LoadLoad | LoadStore | StoreStore));
630   if (effective_mask != 0) {
631     Assembler::membar(effective_mask);
632   }
633 }
634 
635 inline void MacroAssembler::mov(Register s, Register d) {
636   if (s != d) {
637     or3(G0, s, d);
638   } else {
639     assert_not_delayed();  // Put something useful in the delay slot!
640   }
641 }
642 
643 inline void MacroAssembler::mov_or_nop(Register s, Register d) {
644   if (s != d) {
645     or3(G0, s, d);
646   } else {
647     nop();
648   }
649 }
650 
651 inline void MacroAssembler::mov( int simm13a, Register d) { or3( G0, simm13a, d); }
652 
653 inline void MacroAssembler::prefetch(const Address&amp; a, PrefetchFcn f, int offset) {
654   relocate(a.rspec(offset));
655   assert(!a.has_index(), &quot;&quot;);
656   prefetch(a.base(), a.disp() + offset, f);
657 }
658 
659 inline void MacroAssembler::st(Register d, Register s1, Register s2)      { stw(d, s1, s2); }
660 inline void MacroAssembler::st(Register d, Register s1, int simm13a)      { stw(d, s1, simm13a); }
661 
662 #ifdef ASSERT
663 // ByteSize is only a class when ASSERT is defined, otherwise it&#39;s an int.
664 inline void MacroAssembler::st(Register d, Register s1, ByteSize simm13a) { stw(d, s1, in_bytes(simm13a)); }
665 #endif
666 
667 inline void MacroAssembler::st(Register d, const Address&amp; a, int offset) {
668   if (a.has_index()) { assert(offset == 0, &quot;&quot;); st( d, a.base(), a.index()        ); }
669   else               {                          st( d, a.base(), a.disp() + offset); }
670 }
671 
672 inline void MacroAssembler::stb(Register d, const Address&amp; a, int offset) {
673   if (a.has_index()) { assert(offset == 0, &quot;&quot;); stb(d, a.base(), a.index()        ); }
674   else               {                          stb(d, a.base(), a.disp() + offset); }
675 }
676 inline void MacroAssembler::sth(Register d, const Address&amp; a, int offset) {
677   if (a.has_index()) { assert(offset == 0, &quot;&quot;); sth(d, a.base(), a.index()        ); }
678   else               {                          sth(d, a.base(), a.disp() + offset); }
679 }
680 inline void MacroAssembler::stw(Register d, const Address&amp; a, int offset) {
681   if (a.has_index()) { assert(offset == 0, &quot;&quot;); stw(d, a.base(), a.index()        ); }
682   else               {                          stw(d, a.base(), a.disp() + offset); }
683 }
684 inline void MacroAssembler::std(Register d, const Address&amp; a, int offset) {
685   if (a.has_index()) { assert(offset == 0, &quot;&quot;); std(d, a.base(), a.index()        ); }
686   else               {                          std(d, a.base(), a.disp() + offset); }
687 }
688 inline void MacroAssembler::stx(Register d, const Address&amp; a, int offset) {
689   if (a.has_index()) { assert(offset == 0, &quot;&quot;); stx(d, a.base(), a.index()        ); }
690   else               {                          stx(d, a.base(), a.disp() + offset); }
691 }
692 
693 inline void MacroAssembler::stb(Register d, Register s1, RegisterOrConstant s2) { stb(d, Address(s1, s2)); }
694 inline void MacroAssembler::sth(Register d, Register s1, RegisterOrConstant s2) { sth(d, Address(s1, s2)); }
695 inline void MacroAssembler::stw(Register d, Register s1, RegisterOrConstant s2) { stw(d, Address(s1, s2)); }
696 inline void MacroAssembler::stx(Register d, Register s1, RegisterOrConstant s2) { stx(d, Address(s1, s2)); }
697 inline void MacroAssembler::std(Register d, Register s1, RegisterOrConstant s2) { std(d, Address(s1, s2)); }
698 inline void MacroAssembler::st( Register d, Register s1, RegisterOrConstant s2) { st( d, Address(s1, s2)); }
699 
700 inline void MacroAssembler::stf(FloatRegisterImpl::Width w, FloatRegister d, Register s1, RegisterOrConstant s2) {
701   if (s2.is_register())  stf(w, d, s1, s2.as_register());
702   else                   stf(w, d, s1, s2.as_constant());
703 }
704 
705 inline void MacroAssembler::stf(FloatRegisterImpl::Width w, FloatRegister d, const Address&amp; a, int offset) {
706   relocate(a.rspec(offset));
707   if (a.has_index()) { assert(offset == 0, &quot;&quot;); stf(w, d, a.base(), a.index()        ); }
708   else               {                          stf(w, d, a.base(), a.disp() + offset); }
709 }
710 
711 inline void MacroAssembler::sub(Register s1, RegisterOrConstant s2, Register d, int offset) {
712   if (s2.is_register())  sub(s1, s2.as_register(),          d);
713   else                 { sub(s1, s2.as_constant() + offset, d); offset = 0; }
714   if (offset != 0)       sub(d,  offset,                    d);
715 }
716 
717 inline void MacroAssembler::swap(const Address&amp; a, Register d, int offset) {
718   relocate(a.rspec(offset));
719   if (a.has_index()) { assert(offset == 0, &quot;&quot;); swap(a.base(), a.index(), d        ); }
720   else               {                          swap(a.base(), a.disp() + offset, d); }
721 }
722 #endif // CPU_SPARC_MACROASSEMBLER_SPARC_INLINE_HPP
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="2" type="hidden" />
</body>
</html>