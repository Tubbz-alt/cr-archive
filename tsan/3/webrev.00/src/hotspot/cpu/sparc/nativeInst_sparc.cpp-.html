<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/cpu/sparc/nativeInst_sparc.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;asm/macroAssembler.inline.hpp&quot;
 27 #include &quot;code/codeCache.hpp&quot;
 28 #include &quot;code/compiledIC.hpp&quot;
 29 #include &quot;memory/resourceArea.hpp&quot;
 30 #include &quot;nativeInst_sparc.hpp&quot;
 31 #include &quot;oops/oop.inline.hpp&quot;
 32 #include &quot;runtime/handles.hpp&quot;
 33 #include &quot;runtime/sharedRuntime.hpp&quot;
 34 #include &quot;runtime/stubRoutines.hpp&quot;
 35 #include &quot;utilities/ostream.hpp&quot;
 36 #ifdef COMPILER1
 37 #include &quot;c1/c1_Runtime1.hpp&quot;
 38 #endif
 39 
 40 void NativeInstruction::set_data64_sethi(address instaddr, intptr_t x) {
 41   ResourceMark rm;
 42   CodeBuffer buf(instaddr, 10 * BytesPerInstWord );
 43   MacroAssembler* _masm = new MacroAssembler(&amp;buf);
 44   Register destreg;
 45 
 46   destreg = inv_rd(*(unsigned int *)instaddr);
 47   // Generate a the new sequence
 48   _masm-&gt;patchable_sethi(x, destreg);
 49   ICache::invalidate_range(instaddr, 7 * BytesPerInstWord);
 50 }
 51 
 52 void NativeInstruction::verify_data64_sethi(address instaddr, intptr_t x) {
 53   ResourceMark rm;
 54   unsigned char buffer[10 * BytesPerInstWord];
 55   CodeBuffer buf(buffer, 10 * BytesPerInstWord);
 56   MacroAssembler masm(&amp;buf);
 57 
 58   Register destreg = inv_rd(*(unsigned int *)instaddr);
 59   // Generate the proper sequence into a temporary buffer and compare
 60   // it with the original sequence.
 61   masm.patchable_sethi(x, destreg);
 62   int len = buffer - masm.pc();
 63   for (int i = 0; i &lt; len; i++) {
 64     guarantee(instaddr[i] == buffer[i], &quot;instructions must match&quot;);
 65   }
 66 }
 67 
 68 void NativeInstruction::verify() {
 69   // make sure code pattern is actually an instruction address
 70   address addr = addr_at(0);
 71   if (addr == 0 || ((intptr_t)addr &amp; 3) != 0) {
 72     fatal(&quot;not an instruction address&quot;);
 73   }
 74 }
 75 
 76 void NativeInstruction::print() {
 77   tty-&gt;print_cr(INTPTR_FORMAT &quot;: 0x%x&quot;, p2i(addr_at(0)), long_at(0));
 78 }
 79 
 80 void NativeInstruction::set_long_at(int offset, int i) {
 81   address addr = addr_at(offset);
 82   *(int*)addr = i;
 83   ICache::invalidate_word(addr);
 84 }
 85 
 86 void NativeInstruction::set_jlong_at(int offset, jlong i) {
 87   address addr = addr_at(offset);
 88   *(jlong*)addr = i;
 89   // Don&#39;t need to invalidate 2 words here, because
 90   // the flush instruction operates on doublewords.
 91   ICache::invalidate_word(addr);
 92 }
 93 
 94 void NativeInstruction::set_addr_at(int offset, address x) {
 95   address addr = addr_at(offset);
 96   assert( ((intptr_t)addr &amp; (wordSize-1)) == 0, &quot;set_addr_at bad address alignment&quot;);
 97   *(uintptr_t*)addr = (uintptr_t)x;
 98   // Don&#39;t need to invalidate 2 words here in the 64-bit case,
 99   // because the flush instruction operates on doublewords.
100   ICache::invalidate_word(addr);
101   // The Intel code has this assertion for NativeCall::set_destination,
102   // NativeMovConstReg::set_data, NativeMovRegMem::set_offset,
103   // NativeJump::set_jump_destination, and NativePushImm32::set_data
104   //assert (Patching_lock-&gt;owned_by_self(), &quot;must hold lock to patch instruction&quot;)
105 }
106 
107 bool NativeInstruction::is_zero_test(Register &amp;reg) {
108   int x = long_at(0);
109   Assembler::op3s temp = (Assembler::op3s) (Assembler::sub_op3 | Assembler::cc_bit_op3);
110   if (is_op3(x, temp, Assembler::arith_op) &amp;&amp;
111       inv_immed(x) &amp;&amp; inv_rd(x) == G0) {
112       if (inv_rs1(x) == G0) {
113         reg = inv_rs2(x);
114         return true;
115       } else if (inv_rs2(x) == G0) {
116         reg = inv_rs1(x);
117         return true;
118       }
119   }
120   return false;
121 }
122 
123 bool NativeInstruction::is_load_store_with_small_offset(Register reg) {
124   int x = long_at(0);
125   if (is_op(x, Assembler::ldst_op) &amp;&amp;
126       inv_rs1(x) == reg &amp;&amp; inv_immed(x)) {
127     return true;
128   }
129   return false;
130 }
131 
132 void NativeCall::verify() {
133   NativeInstruction::verify();
134   // make sure code pattern is actually a call instruction
135   int x = long_at(0);
136   if (!is_op(x, Assembler::call_op)) {
137     fatal(&quot;not a call: 0x%x @ &quot; INTPTR_FORMAT, x, p2i(instruction_address()));
138   }
139 }
140 
141 void NativeCall::print() {
142   tty-&gt;print_cr(INTPTR_FORMAT &quot;: call &quot; INTPTR_FORMAT, p2i(instruction_address()), p2i(destination()));
143 }
144 
145 
146 // MT-safe patching of a call instruction (and following word).
147 // First patches the second word, and then atomicly replaces
148 // the first word with the first new instruction word.
149 // Other processors might briefly see the old first word
150 // followed by the new second word.  This is OK if the old
151 // second word is harmless, and the new second word may be
152 // harmlessly executed in the delay slot of the call.
153 void NativeCall::replace_mt_safe(address instr_addr, address code_buffer) {
154   assert(Patching_lock-&gt;is_locked() ||
155          SafepointSynchronize::is_at_safepoint(), &quot;concurrent code patching&quot;);
156    assert (instr_addr != NULL, &quot;illegal address for code patching&quot;);
157    NativeCall* n_call =  nativeCall_at (instr_addr); // checking that it is a call
158    assert(NativeCall::instruction_size == 8, &quot;wrong instruction size; must be 8&quot;);
159    int i0 = ((int*)code_buffer)[0];
160    int i1 = ((int*)code_buffer)[1];
161    int* contention_addr = (int*) n_call-&gt;addr_at(1*BytesPerInstWord);
162    assert(inv_op(*contention_addr) == Assembler::arith_op ||
163           *contention_addr == nop_instruction(),
164           &quot;must not interfere with original call&quot;);
165    // The set_long_at calls do the ICacheInvalidate so we just need to do them in reverse order
166    n_call-&gt;set_long_at(1*BytesPerInstWord, i1);
167    n_call-&gt;set_long_at(0*BytesPerInstWord, i0);
168    // NOTE:  It is possible that another thread T will execute
169    // only the second patched word.
170    // In other words, since the original instruction is this
171    //    call patching_stub; nop                   (NativeCall)
172    // and the new sequence from the buffer is this:
173    //    sethi %hi(K), %r; add %r, %lo(K), %r      (NativeMovConstReg)
174    // what T will execute is this:
175    //    call patching_stub; add %r, %lo(K), %r
176    // thereby putting garbage into %r before calling the patching stub.
177    // This is OK, because the patching stub ignores the value of %r.
178 
179    // Make sure the first-patched instruction, which may co-exist
180    // briefly with the call, will do something harmless.
181    assert(inv_op(*contention_addr) == Assembler::arith_op ||
182           *contention_addr == nop_instruction(),
183           &quot;must not interfere with original call&quot;);
184 }
185 
186 // Similar to replace_mt_safe, but just changes the destination.  The
187 // important thing is that free-running threads are able to execute this
188 // call instruction at all times.  Thus, the displacement field must be
189 // instruction-word-aligned.  This is always true on SPARC.
190 //
191 // Used in the runtime linkage of calls; see class CompiledIC.
192 void NativeCall::set_destination_mt_safe(address dest) {
193   assert((Patching_lock-&gt;is_locked() || SafepointSynchronize::is_at_safepoint()) ||
194          CompiledICLocker::is_safe(addr_at(0)),
195          &quot;concurrent code patching&quot;);
196   // set_destination uses set_long_at which does the ICache::invalidate
197   set_destination(dest);
198 }
199 
200 // Code for unit testing implementation of NativeCall class
201 void NativeCall::test() {
202 #ifdef ASSERT
203   ResourceMark rm;
204   CodeBuffer cb(&quot;test&quot;, 100, 100);
205   MacroAssembler* a = new MacroAssembler(&amp;cb);
206   NativeCall  *nc;
207   uint idx;
208   int offsets[] = {
209     0x0,
210     (int)0xfffffff0,
211     (int)0x7ffffff0,
212     (int)0x80000000,
213     0x20,
214     0x4000,
215   };
216 
217   VM_Version::allow_all();
218 
219   a-&gt;call( a-&gt;pc(), relocInfo::none );
220   a-&gt;delayed()-&gt;nop();
221   nc = nativeCall_at( cb.insts_begin() );
222   nc-&gt;print();
223 
224   nc = nativeCall_overwriting_at( nc-&gt;next_instruction_address() );
225   for (idx = 0; idx &lt; ARRAY_SIZE(offsets); idx++) {
226     nc-&gt;set_destination( cb.insts_begin() + offsets[idx] );
227     assert(nc-&gt;destination() == (cb.insts_begin() + offsets[idx]), &quot;check unit test&quot;);
228     nc-&gt;print();
229   }
230 
231   nc = nativeCall_before( cb.insts_begin() + 8 );
232   nc-&gt;print();
233 
234   VM_Version::revert();
235 #endif
236 }
237 // End code for unit testing implementation of NativeCall class
238 
239 //-------------------------------------------------------------------
240 
241 void NativeFarCall::set_destination(address dest) {
242   // Address materialized in the instruction stream, so nothing to do.
243   return;
244 #if 0 // What we&#39;d do if we really did want to change the destination
245   if (destination() == dest) {
246     return;
247   }
248   ResourceMark rm;
249   CodeBuffer buf(addr_at(0), instruction_size + 1);
250   MacroAssembler* _masm = new MacroAssembler(&amp;buf);
251   // Generate the new sequence
252   AddressLiteral(dest);
253   _masm-&gt;jumpl_to(dest, O7, O7);
254   ICache::invalidate_range(addr_at(0), instruction_size );
255 #endif
256 }
257 
258 void NativeFarCall::verify() {
259   // make sure code pattern is actually a jumpl_to instruction
260   assert((int)instruction_size == (int)NativeJump::instruction_size, &quot;same as jump_to&quot;);
261   assert((int)jmpl_offset == (int)NativeMovConstReg::add_offset, &quot;sethi size ok&quot;);
262   nativeJump_at(addr_at(0))-&gt;verify();
263 }
264 
265 bool NativeFarCall::is_call_at(address instr) {
266   return nativeInstruction_at(instr)-&gt;is_sethi();
267 }
268 
269 void NativeFarCall::print() {
270   tty-&gt;print_cr(INTPTR_FORMAT &quot;: call &quot; INTPTR_FORMAT, p2i(instruction_address()), p2i(destination()));
271 }
272 
273 bool NativeFarCall::destination_is_compiled_verified_entry_point() {
274   nmethod* callee = CodeCache::find_nmethod(destination());
275   if (callee == NULL) {
276     return false;
277   } else {
278     return destination() == callee-&gt;verified_entry_point();
279   }
280 }
281 
282 // MT-safe patching of a far call.
283 void NativeFarCall::replace_mt_safe(address instr_addr, address code_buffer) {
284   Unimplemented();
285 }
286 
287 // Code for unit testing implementation of NativeFarCall class
288 void NativeFarCall::test() {
289   Unimplemented();
290 }
291 // End code for unit testing implementation of NativeFarCall class
292 
293 //-------------------------------------------------------------------
294 
295 
296 void NativeMovConstReg::verify() {
297   NativeInstruction::verify();
298   // make sure code pattern is actually a &quot;set_metadata&quot; synthetic instruction
299   // see MacroAssembler::set_oop()
300   int i0 = long_at(sethi_offset);
301   int i1 = long_at(add_offset);
302 
303   // verify the pattern &quot;sethi %hi22(imm), reg ;  add reg, %lo10(imm), reg&quot;
304   Register rd = inv_rd(i0);
305   if (!is_op2(i0, Assembler::sethi_op2) &amp;&amp; rd != G0 ) {
306     fatal(&quot;not a set_metadata&quot;);
307   }
308 }
309 
310 
311 void NativeMovConstReg::print() {
312   tty-&gt;print_cr(INTPTR_FORMAT &quot;: mov reg, &quot; INTPTR_FORMAT, p2i(instruction_address()), data());
313 }
314 
315 
316 intptr_t NativeMovConstReg::data() const {
317   return data64(addr_at(sethi_offset), long_at(add_offset));
318 }
319 
320 
321 void NativeMovConstReg::set_data(intptr_t x) {
322   set_data64_sethi(addr_at(sethi_offset), x);
323   set_long_at(add_offset,   set_data32_simm13( long_at(add_offset),   x));
324 
325   // also store the value into an oop_Relocation cell, if any
326   CodeBlob* cb = CodeCache::find_blob(instruction_address());
327   nmethod*  nm = cb ? cb-&gt;as_nmethod_or_null() : NULL;
328   if (nm != NULL) {
329     RelocIterator iter(nm, instruction_address(), next_instruction_address());
330     oop* oop_addr = NULL;
331     Metadata** metadata_addr = NULL;
332     while (iter.next()) {
333       if (iter.type() == relocInfo::oop_type) {
334         oop_Relocation *r = iter.oop_reloc();
335         if (oop_addr == NULL) {
336           oop_addr = r-&gt;oop_addr();
337           *oop_addr = cast_to_oop(x);
338         } else {
339           assert(oop_addr == r-&gt;oop_addr(), &quot;must be only one set-oop here&quot;);
340         }
341       }
342       if (iter.type() == relocInfo::metadata_type) {
343         metadata_Relocation *r = iter.metadata_reloc();
344         if (metadata_addr == NULL) {
345           metadata_addr = r-&gt;metadata_addr();
346           *metadata_addr = (Metadata*)x;
347         } else {
348           assert(metadata_addr == r-&gt;metadata_addr(), &quot;must be only one set-metadata here&quot;);
349         }
350       }
351     }
352   }
353 }
354 
355 
356 // Code for unit testing implementation of NativeMovConstReg class
357 void NativeMovConstReg::test() {
358 #ifdef ASSERT
359   ResourceMark rm;
360   CodeBuffer cb(&quot;test&quot;, 100, 100);
361   MacroAssembler* a = new MacroAssembler(&amp;cb);
362   NativeMovConstReg* nm;
363   uint idx;
364   int offsets[] = {
365     0x0,
366     (int)0x7fffffff,
367     (int)0x80000000,
368     (int)0xffffffff,
369     0x20,
370     4096,
371     4097,
372   };
373 
374   VM_Version::allow_all();
375 
376   AddressLiteral al1(0xaaaabbbb, relocInfo::external_word_type);
377   a-&gt;sethi(al1, I3);
378   a-&gt;add(I3, al1.low10(), I3);
379   AddressLiteral al2(0xccccdddd, relocInfo::external_word_type);
380   a-&gt;sethi(al2, O2);
381   a-&gt;add(O2, al2.low10(), O2);
382 
383   nm = nativeMovConstReg_at( cb.insts_begin() );
384   nm-&gt;print();
385 
386   nm = nativeMovConstReg_at( nm-&gt;next_instruction_address() );
387   for (idx = 0; idx &lt; ARRAY_SIZE(offsets); idx++) {
388     nm-&gt;set_data( offsets[idx] );
389     assert(nm-&gt;data() == offsets[idx], &quot;check unit test&quot;);
390   }
391   nm-&gt;print();
392 
393   VM_Version::revert();
394 #endif
395 }
396 // End code for unit testing implementation of NativeMovConstReg class
397 
398 //-------------------------------------------------------------------
399 
400 void NativeMovConstReg32::verify() {
401   NativeInstruction::verify();
402   // make sure code pattern is actually a &quot;set_metadata&quot; synthetic instruction
403   // see MacroAssembler::set_oop()
404   int i0 = long_at(sethi_offset);
405   int i1 = long_at(add_offset);
406 
407   // verify the pattern &quot;sethi %hi22(imm), reg ;  add reg, %lo10(imm), reg&quot;
408   Register rd = inv_rd(i0);
409   if (!is_op2(i0, Assembler::sethi_op2) &amp;&amp; rd != G0 ) {
410     fatal(&quot;not a set_metadata&quot;);
411   }
412 }
413 
414 
415 void NativeMovConstReg32::print() {
416   tty-&gt;print_cr(INTPTR_FORMAT &quot;: mov reg, &quot; INTPTR_FORMAT, p2i(instruction_address()), data());
417 }
418 
419 
420 intptr_t NativeMovConstReg32::data() const {
421   return data32(long_at(sethi_offset), long_at(add_offset));
422 }
423 
424 
425 void NativeMovConstReg32::set_data(intptr_t x) {
426   set_long_at(sethi_offset, set_data32_sethi(  long_at(sethi_offset), x));
427   set_long_at(add_offset,   set_data32_simm13( long_at(add_offset),   x));
428 
429   // also store the value into an oop_Relocation cell, if any
430   CodeBlob* cb = CodeCache::find_blob(instruction_address());
431   nmethod*  nm = cb ? cb-&gt;as_nmethod_or_null() : NULL;
432   if (nm != NULL) {
433     RelocIterator iter(nm, instruction_address(), next_instruction_address());
434     oop* oop_addr = NULL;
435     Metadata** metadata_addr = NULL;
436     while (iter.next()) {
437       if (iter.type() == relocInfo::oop_type) {
438         oop_Relocation *r = iter.oop_reloc();
439         if (oop_addr == NULL) {
440           oop_addr = r-&gt;oop_addr();
441           *oop_addr = cast_to_oop(x);
442         } else {
443           assert(oop_addr == r-&gt;oop_addr(), &quot;must be only one set-oop here&quot;);
444         }
445       }
446       if (iter.type() == relocInfo::metadata_type) {
447         metadata_Relocation *r = iter.metadata_reloc();
448         if (metadata_addr == NULL) {
449           metadata_addr = r-&gt;metadata_addr();
450           *metadata_addr = (Metadata*)x;
451         } else {
452           assert(metadata_addr == r-&gt;metadata_addr(), &quot;must be only one set-metadata here&quot;);
453         }
454       }
455     }
456   }
457 }
458 
459 //-------------------------------------------------------------------
460 
461 void NativeMovConstRegPatching::verify() {
462   NativeInstruction::verify();
463   // Make sure code pattern is sethi/nop/add.
464   int i0 = long_at(sethi_offset);
465   int i1 = long_at(nop_offset);
466   int i2 = long_at(add_offset);
467   assert((int)nop_offset == (int)NativeMovConstReg::add_offset, &quot;sethi size ok&quot;);
468 
469   // Verify the pattern &quot;sethi %hi22(imm), reg; nop; add reg, %lo10(imm), reg&quot;
470   // The casual reader should note that on Sparc a nop is a special case if sethi
471   // in which the destination register is %g0.
472   Register rd0 = inv_rd(i0);
473   Register rd1 = inv_rd(i1);
474   if (!(is_op2(i0, Assembler::sethi_op2) &amp;&amp; rd0 != G0 &amp;&amp;
475         is_op2(i1, Assembler::sethi_op2) &amp;&amp; rd1 == G0 &amp;&amp;        // nop is a special case of sethi
476         is_op3(i2, Assembler::add_op3, Assembler::arith_op) &amp;&amp;
477         inv_immed(i2) &amp;&amp; (unsigned)get_simm13(i2) &lt; (1 &lt;&lt; 10) &amp;&amp;
478         rd0 == inv_rs1(i2) &amp;&amp; rd0 == inv_rd(i2))) {
479     fatal(&quot;not a set_metadata&quot;);
480   }
481 }
482 
483 
484 void NativeMovConstRegPatching::print() {
485   tty-&gt;print_cr(INTPTR_FORMAT &quot;: mov reg, 0x%x&quot;, p2i(instruction_address()), data());
486 }
487 
488 
489 int NativeMovConstRegPatching::data() const {
490   return data64(addr_at(sethi_offset), long_at(add_offset));
491 }
492 
493 
494 void NativeMovConstRegPatching::set_data(int x) {
495   set_data64_sethi(addr_at(sethi_offset), x);
496   set_long_at(add_offset, set_data32_simm13(long_at(add_offset), x));
497 
498   // also store the value into an oop_Relocation cell, if any
499   CodeBlob* cb = CodeCache::find_blob(instruction_address());
500   nmethod*  nm = cb ? cb-&gt;as_nmethod_or_null() : NULL;
501   if (nm != NULL) {
502     RelocIterator iter(nm, instruction_address(), next_instruction_address());
503     oop* oop_addr = NULL;
504     Metadata** metadata_addr = NULL;
505     while (iter.next()) {
506       if (iter.type() == relocInfo::oop_type) {
507         oop_Relocation *r = iter.oop_reloc();
508         if (oop_addr == NULL) {
509           oop_addr = r-&gt;oop_addr();
510           *oop_addr = cast_to_oop(x);
511         } else {
512           assert(oop_addr == r-&gt;oop_addr(), &quot;must be only one set-oop here&quot;);
513         }
514       }
515       if (iter.type() == relocInfo::metadata_type) {
516         metadata_Relocation *r = iter.metadata_reloc();
517         if (metadata_addr == NULL) {
518           metadata_addr = r-&gt;metadata_addr();
519           *metadata_addr = (Metadata*)x;
520         } else {
521           assert(metadata_addr == r-&gt;metadata_addr(), &quot;must be only one set-metadata here&quot;);
522         }
523       }
524     }
525   }
526 }
527 
528 
529 // Code for unit testing implementation of NativeMovConstRegPatching class
530 void NativeMovConstRegPatching::test() {
531 #ifdef ASSERT
532   ResourceMark rm;
533   CodeBuffer cb(&quot;test&quot;, 100, 100);
534   MacroAssembler* a = new MacroAssembler(&amp;cb);
535   NativeMovConstRegPatching* nm;
536   uint idx;
537   int offsets[] = {
538     0x0,
539     (int)0x7fffffff,
540     (int)0x80000000,
541     (int)0xffffffff,
542     0x20,
543     4096,
544     4097,
545   };
546 
547   VM_Version::allow_all();
548 
549   AddressLiteral al1(0xaaaabbbb, relocInfo::external_word_type);
550   a-&gt;sethi(al1, I3);
551   a-&gt;nop();
552   a-&gt;add(I3, al1.low10(), I3);
553   AddressLiteral al2(0xccccdddd, relocInfo::external_word_type);
554   a-&gt;sethi(al2, O2);
555   a-&gt;nop();
556   a-&gt;add(O2, al2.low10(), O2);
557 
558   nm = nativeMovConstRegPatching_at( cb.insts_begin() );
559   nm-&gt;print();
560 
561   nm = nativeMovConstRegPatching_at( nm-&gt;next_instruction_address() );
562   for (idx = 0; idx &lt; ARRAY_SIZE(offsets); idx++) {
563     nm-&gt;set_data( offsets[idx] );
564     assert(nm-&gt;data() == offsets[idx], &quot;check unit test&quot;);
565   }
566   nm-&gt;print();
567 
568   VM_Version::revert();
569 #endif // ASSERT
570 }
571 // End code for unit testing implementation of NativeMovConstRegPatching class
572 
573 
574 //-------------------------------------------------------------------
575 
576 
577 void NativeMovRegMem::copy_instruction_to(address new_instruction_address) {
578   Untested(&quot;copy_instruction_to&quot;);
579   int instruction_size = next_instruction_address() - instruction_address();
580   for (int i = 0; i &lt; instruction_size; i += BytesPerInstWord) {
581     *(int*)(new_instruction_address + i) = *(int*)(address(this) + i);
582   }
583 }
584 
585 
586 void NativeMovRegMem::verify() {
587   NativeInstruction::verify();
588   // make sure code pattern is actually a &quot;ld&quot; or &quot;st&quot; of some sort.
589   int i0 = long_at(0);
590   int op3 = inv_op3(i0);
591 
592   assert((int)add_offset == NativeMovConstReg::add_offset, &quot;sethi size ok&quot;);
593 
594   if (!(is_op(i0, Assembler::ldst_op) &amp;&amp;
595         inv_immed(i0) &amp;&amp;
596         0 != (op3 &lt; op3_ldst_int_limit
597          ? (1 &lt;&lt;  op3                      ) &amp; (op3_mask_ld  | op3_mask_st)
598          : (1 &lt;&lt; (op3 - op3_ldst_int_limit)) &amp; (op3_mask_ldf | op3_mask_stf))))
599   {
600     int i1 = long_at(ldst_offset);
601     Register rd = inv_rd(i0);
602 
603     op3 = inv_op3(i1);
604     if (!is_op(i1, Assembler::ldst_op) &amp;&amp; rd == inv_rs2(i1) &amp;&amp;
605          0 != (op3 &lt; op3_ldst_int_limit
606               ? (1 &lt;&lt;  op3                      ) &amp; (op3_mask_ld  | op3_mask_st)
607                : (1 &lt;&lt; (op3 - op3_ldst_int_limit)) &amp; (op3_mask_ldf | op3_mask_stf))) {
608       fatal(&quot;not a ld* or st* op&quot;);
609     }
610   }
611 }
612 
613 
614 void NativeMovRegMem::print() {
615   if (is_immediate()) {
616     // offset is a signed 13-bit immediate, so casting it to int will not lose significant bits
617     tty-&gt;print_cr(INTPTR_FORMAT &quot;: mov reg, [reg + %d]&quot;, p2i(instruction_address()), (int)offset());
618   } else {
619     tty-&gt;print_cr(INTPTR_FORMAT &quot;: mov reg, [reg + reg]&quot;, p2i(instruction_address()));
620   }
621 }
622 
623 
624 // Code for unit testing implementation of NativeMovRegMem class
625 void NativeMovRegMem::test() {
626 #ifdef ASSERT
627   ResourceMark rm;
628   CodeBuffer cb(&quot;test&quot;, 1000, 1000);
629   MacroAssembler* a = new MacroAssembler(&amp;cb);
630   NativeMovRegMem* nm;
631   uint idx = 0;
632   uint idx1;
633   int offsets[] = {
634     0x0,
635     (int)0xffffffff,
636     (int)0x7fffffff,
637     (int)0x80000000,
638     4096,
639     4097,
640     0x20,
641     0x4000,
642   };
643 
644   VM_Version::allow_all();
645 
646   AddressLiteral al1(0xffffffff, relocInfo::external_word_type);
647   AddressLiteral al2(0xaaaabbbb, relocInfo::external_word_type);
648   a-&gt;ldsw( G5, al1.low10(), G4 ); idx++;
649   a-&gt;sethi(al2, I3); a-&gt;add(I3, al2.low10(), I3);
650   a-&gt;ldsw( G5, I3, G4 ); idx++;
651   a-&gt;ldsb( G5, al1.low10(), G4 ); idx++;
652   a-&gt;sethi(al2, I3); a-&gt;add(I3, al2.low10(), I3);
653   a-&gt;ldsb( G5, I3, G4 ); idx++;
654   a-&gt;ldsh( G5, al1.low10(), G4 ); idx++;
655   a-&gt;sethi(al2, I3); a-&gt;add(I3, al2.low10(), I3);
656   a-&gt;ldsh( G5, I3, G4 ); idx++;
657   a-&gt;lduw( G5, al1.low10(), G4 ); idx++;
658   a-&gt;sethi(al2, I3); a-&gt;add(I3, al2.low10(), I3);
659   a-&gt;lduw( G5, I3, G4 ); idx++;
660   a-&gt;ldub( G5, al1.low10(), G4 ); idx++;
661   a-&gt;sethi(al2, I3); a-&gt;add(I3, al2.low10(), I3);
662   a-&gt;ldub( G5, I3, G4 ); idx++;
663   a-&gt;lduh( G5, al1.low10(), G4 ); idx++;
664   a-&gt;sethi(al2, I3); a-&gt;add(I3, al2.low10(), I3);
665   a-&gt;lduh( G5, I3, G4 ); idx++;
666   a-&gt;ldx( G5, al1.low10(), G4 ); idx++;
667   a-&gt;sethi(al2, I3); a-&gt;add(I3, al2.low10(), I3);
668   a-&gt;ldx( G5, I3, G4 ); idx++;
669   a-&gt;ldd( G5, al1.low10(), G4 ); idx++;
670   a-&gt;sethi(al2, I3); a-&gt;add(I3, al2.low10(), I3);
671   a-&gt;ldd( G5, I3, G4 ); idx++;
672   a-&gt;ldf( FloatRegisterImpl::D, O2, -1, F14 ); idx++;
673   a-&gt;sethi(al2, I3); a-&gt;add(I3, al2.low10(), I3);
674   a-&gt;ldf( FloatRegisterImpl::S, O0, I3, F15 ); idx++;
675 
676   a-&gt;stw( G5, G4, al1.low10() ); idx++;
677   a-&gt;sethi(al2, I3); a-&gt;add(I3, al2.low10(), I3);
678   a-&gt;stw( G5, G4, I3 ); idx++;
679   a-&gt;stb( G5, G4, al1.low10() ); idx++;
680   a-&gt;sethi(al2, I3); a-&gt;add(I3, al2.low10(), I3);
681   a-&gt;stb( G5, G4, I3 ); idx++;
682   a-&gt;sth( G5, G4, al1.low10() ); idx++;
683   a-&gt;sethi(al2, I3); a-&gt;add(I3, al2.low10(), I3);
684   a-&gt;sth( G5, G4, I3 ); idx++;
685   a-&gt;stx( G5, G4, al1.low10() ); idx++;
686   a-&gt;sethi(al2, I3); a-&gt;add(I3, al2.low10(), I3);
687   a-&gt;stx( G5, G4, I3 ); idx++;
688   a-&gt;std( G5, G4, al1.low10() ); idx++;
689   a-&gt;sethi(al2, I3); a-&gt;add(I3, al2.low10(), I3);
690   a-&gt;std( G5, G4, I3 ); idx++;
691   a-&gt;stf( FloatRegisterImpl::S, F18, O2, -1 ); idx++;
692   a-&gt;sethi(al2, I3); a-&gt;add(I3, al2.low10(), I3);
693   a-&gt;stf( FloatRegisterImpl::S, F15, O0, I3 ); idx++;
694 
695   nm = nativeMovRegMem_at( cb.insts_begin() );
696   nm-&gt;print();
697   nm-&gt;set_offset( low10(0) );
698   nm-&gt;print();
699   nm-&gt;add_offset_in_bytes( low10(0xbb) * wordSize );
700   nm-&gt;print();
701 
702   while (--idx) {
703     nm = nativeMovRegMem_at( nm-&gt;next_instruction_address() );
704     nm-&gt;print();
705     for (idx1 = 0; idx1 &lt; ARRAY_SIZE(offsets); idx1++) {
706       nm-&gt;set_offset( nm-&gt;is_immediate() ? low10(offsets[idx1]) : offsets[idx1] );
707       assert(nm-&gt;offset() == (nm-&gt;is_immediate() ? low10(offsets[idx1]) : offsets[idx1]),
708              &quot;check unit test&quot;);
709       nm-&gt;print();
710     }
711     nm-&gt;add_offset_in_bytes( low10(0xbb) * wordSize );
712     nm-&gt;print();
713   }
714 
715   VM_Version::revert();
716 #endif // ASSERT
717 }
718 
719 // End code for unit testing implementation of NativeMovRegMem class
720 
721 
722 //--------------------------------------------------------------------------------
723 
724 
725 void NativeJump::verify() {
726   NativeInstruction::verify();
727   int i0 = long_at(sethi_offset);
728   int i1 = long_at(jmpl_offset);
729   assert((int)jmpl_offset == (int)NativeMovConstReg::add_offset, &quot;sethi size ok&quot;);
730   // verify the pattern &quot;sethi %hi22(imm), treg ;  jmpl treg, %lo10(imm), lreg&quot;
731   Register rd = inv_rd(i0);
732   // In LP64, the jump instruction location varies for non relocatable
733   // jumps, for example is could be sethi, xor, jmp instead of the
734   // 7 instructions for sethi.  So let&#39;s check sethi only.
735   if (!is_op2(i0, Assembler::sethi_op2) &amp;&amp; rd != G0 ) {
736     fatal(&quot;not a jump_to instruction&quot;);
737   }
738 }
739 
740 
741 void NativeJump::print() {
742   tty-&gt;print_cr(INTPTR_FORMAT &quot;: jmpl reg, &quot; INTPTR_FORMAT, p2i(instruction_address()), p2i(jump_destination()));
743 }
744 
745 
746 // Code for unit testing implementation of NativeJump class
747 void NativeJump::test() {
748 #ifdef ASSERT
749   ResourceMark rm;
750   CodeBuffer cb(&quot;test&quot;, 100, 100);
751   MacroAssembler* a = new MacroAssembler(&amp;cb);
752   NativeJump* nj;
753   uint idx;
754   int offsets[] = {
755     0x0,
756     (int)0xffffffff,
757     (int)0x7fffffff,
758     (int)0x80000000,
759     4096,
760     4097,
761     0x20,
762     0x4000,
763   };
764 
765   VM_Version::allow_all();
766 
767   AddressLiteral al(0x7fffbbbb, relocInfo::external_word_type);
768   a-&gt;sethi(al, I3);
769   a-&gt;jmpl(I3, al.low10(), G0, RelocationHolder::none);
770   a-&gt;delayed()-&gt;nop();
771   a-&gt;sethi(al, I3);
772   a-&gt;jmpl(I3, al.low10(), L3, RelocationHolder::none);
773   a-&gt;delayed()-&gt;nop();
774 
775   nj = nativeJump_at( cb.insts_begin() );
776   nj-&gt;print();
777 
778   nj = nativeJump_at( nj-&gt;next_instruction_address() );
779   for (idx = 0; idx &lt; ARRAY_SIZE(offsets); idx++) {
780     nj-&gt;set_jump_destination( nj-&gt;instruction_address() + offsets[idx] );
781     assert(nj-&gt;jump_destination() == (nj-&gt;instruction_address() + offsets[idx]), &quot;check unit test&quot;);
782     nj-&gt;print();
783   }
784 
785   VM_Version::revert();
786 #endif // ASSERT
787 }
788 // End code for unit testing implementation of NativeJump class
789 
790 
791 void NativeJump::insert(address code_pos, address entry) {
792   Unimplemented();
793 }
794 
795 // MT safe inserting of a jump over an unknown instruction sequence (used by nmethod::makeZombie)
796 // The problem: jump_to &lt;dest&gt; is a 3-word instruction (including its delay slot).
797 // Atomic write can be only with 1 word.
798 void NativeJump::patch_verified_entry(address entry, address verified_entry, address dest) {
799   // Here&#39;s one way to do it:  Pre-allocate a three-word jump sequence somewhere
800   // in the header of the nmethod, within a short branch&#39;s span of the patch point.
801   // Set up the jump sequence using NativeJump::insert, and then use an annulled
802   // unconditional branch at the target site (an atomic 1-word update).
803   // Limitations:  You can only patch nmethods, with any given nmethod patched at
804   // most once, and the patch must be in the nmethod&#39;s header.
805   // It&#39;s messy, but you can ask the CodeCache for the nmethod containing the
806   // target address.
807 
808   // %%%%% For now, do something MT-stupid:
809   ResourceMark rm;
810   int code_size = 1 * BytesPerInstWord;
811   CodeBuffer cb(verified_entry, code_size + 1);
812   MacroAssembler* a = new MacroAssembler(&amp;cb);
813   a-&gt;ldsw(G0, 0, O7); // &quot;ld&quot; must agree with code in the signal handler
814   ICache::invalidate_range(verified_entry, code_size);
815 }
816 
817 
818 void NativeIllegalInstruction::insert(address code_pos) {
819   NativeIllegalInstruction* nii = (NativeIllegalInstruction*) nativeInstruction_at(code_pos);
820   nii-&gt;set_long_at(0, illegal_instruction());
821 }
822 
823 static int illegal_instruction_bits = 0;
824 
825 int NativeInstruction::illegal_instruction() {
826   if (illegal_instruction_bits == 0) {
827     ResourceMark rm;
828     char buf[40];
829     CodeBuffer cbuf((address)&amp;buf[0], 20);
830     MacroAssembler* a = new MacroAssembler(&amp;cbuf);
831     address ia = a-&gt;pc();
832     a-&gt;trap(ST_RESERVED_FOR_USER_0 + 1);
833     int bits = *(int*)ia;
834     assert(is_op3(bits, Assembler::trap_op3, Assembler::arith_op), &quot;bad instruction&quot;);
835     illegal_instruction_bits = bits;
836     assert(illegal_instruction_bits != 0, &quot;oops&quot;);
837   }
838   return illegal_instruction_bits;
839 }
840 
841 static int ic_miss_trap_bits = 0;
842 
843 bool NativeInstruction::is_ic_miss_trap() {
844   if (ic_miss_trap_bits == 0) {
845     ResourceMark rm;
846     char buf[40];
847     CodeBuffer cbuf((address)&amp;buf[0], 20);
848     MacroAssembler* a = new MacroAssembler(&amp;cbuf);
849     address ia = a-&gt;pc();
850     a-&gt;trap(Assembler::notEqual, Assembler::ptr_cc, G0, ST_RESERVED_FOR_USER_0 + 2);
851     int bits = *(int*)ia;
852     assert(is_op3(bits, Assembler::trap_op3, Assembler::arith_op), &quot;bad instruction&quot;);
853     ic_miss_trap_bits = bits;
854     assert(ic_miss_trap_bits != 0, &quot;oops&quot;);
855   }
856   return long_at(0) == ic_miss_trap_bits;
857 }
858 
859 
860 bool NativeInstruction::is_illegal() {
861   if (illegal_instruction_bits == 0) {
862     return false;
863   }
864   return long_at(0) == illegal_instruction_bits;
865 }
866 
867 
868 void NativeGeneralJump::verify() {
869   assert(((NativeInstruction *)this)-&gt;is_jump() ||
870          ((NativeInstruction *)this)-&gt;is_cond_jump(), &quot;not a general jump instruction&quot;);
871 }
872 
873 
874 void NativeGeneralJump::insert_unconditional(address code_pos, address entry) {
875   Assembler::Condition condition = Assembler::always;
876   int x = Assembler::op2(Assembler::br_op2) | Assembler::annul(false) |
877     Assembler::cond(condition) | Assembler::wdisp((intptr_t)entry, (intptr_t)code_pos, 22);
878   NativeGeneralJump* ni = (NativeGeneralJump*) nativeInstruction_at(code_pos);
879   ni-&gt;set_long_at(0, x);
880 }
881 
882 
883 // MT-safe patching of a jmp instruction (and following word).
884 // First patches the second word, and then atomicly replaces
885 // the first word with the first new instruction word.
886 // Other processors might briefly see the old first word
887 // followed by the new second word.  This is OK if the old
888 // second word is harmless, and the new second word may be
889 // harmlessly executed in the delay slot of the call.
890 void NativeGeneralJump::replace_mt_safe(address instr_addr, address code_buffer) {
891    assert(Patching_lock-&gt;is_locked() ||
892          SafepointSynchronize::is_at_safepoint(), &quot;concurrent code patching&quot;);
893    assert (instr_addr != NULL, &quot;illegal address for code patching&quot;);
894    NativeGeneralJump* h_jump =  nativeGeneralJump_at (instr_addr); // checking that it is a call
895    assert(NativeGeneralJump::instruction_size == 8, &quot;wrong instruction size; must be 8&quot;);
896    int i0 = ((int*)code_buffer)[0];
897    int i1 = ((int*)code_buffer)[1];
898    int* contention_addr = (int*) h_jump-&gt;addr_at(1*BytesPerInstWord);
899    assert(inv_op(*contention_addr) == Assembler::arith_op ||
900           *contention_addr == nop_instruction(),
901           &quot;must not interfere with original call&quot;);
902    // The set_long_at calls do the ICacheInvalidate so we just need to do them in reverse order
903    h_jump-&gt;set_long_at(1*BytesPerInstWord, i1);
904    h_jump-&gt;set_long_at(0*BytesPerInstWord, i0);
905    // NOTE:  It is possible that another thread T will execute
906    // only the second patched word.
907    // In other words, since the original instruction is this
908    //    jmp patching_stub; nop                    (NativeGeneralJump)
909    // and the new sequence from the buffer is this:
910    //    sethi %hi(K), %r; add %r, %lo(K), %r      (NativeMovConstReg)
911    // what T will execute is this:
912    //    jmp patching_stub; add %r, %lo(K), %r
913    // thereby putting garbage into %r before calling the patching stub.
914    // This is OK, because the patching stub ignores the value of %r.
915 
916    // Make sure the first-patched instruction, which may co-exist
917    // briefly with the call, will do something harmless.
918    assert(inv_op(*contention_addr) == Assembler::arith_op ||
919           *contention_addr == nop_instruction(),
920           &quot;must not interfere with original call&quot;);
921 }
    </pre>
  </body>
</html>