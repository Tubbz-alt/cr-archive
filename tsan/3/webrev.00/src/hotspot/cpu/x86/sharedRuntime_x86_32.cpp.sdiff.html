<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/sharedRuntime_x86_32.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="sharedRuntime_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="sharedRuntime_x86_64.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/sharedRuntime_x86_32.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2003, 2018, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;code/debugInfoRec.hpp&quot;
  29 #include &quot;code/icBuffer.hpp&quot;
  30 #include &quot;code/nativeInst.hpp&quot;
  31 #include &quot;code/vtableStubs.hpp&quot;
  32 #include &quot;gc/shared/gcLocker.hpp&quot;


  33 #include &quot;interpreter/interpreter.hpp&quot;
  34 #include &quot;logging/log.hpp&quot;
  35 #include &quot;memory/resourceArea.hpp&quot;
  36 #include &quot;oops/compiledICHolder.hpp&quot;

  37 #include &quot;runtime/safepointMechanism.hpp&quot;
  38 #include &quot;runtime/sharedRuntime.hpp&quot;
  39 #include &quot;runtime/vframeArray.hpp&quot;

  40 #include &quot;utilities/align.hpp&quot;
  41 #include &quot;vmreg_x86.inline.hpp&quot;
  42 #ifdef COMPILER1
  43 #include &quot;c1/c1_Runtime1.hpp&quot;
  44 #endif
  45 #ifdef COMPILER2
  46 #include &quot;opto/runtime.hpp&quot;
  47 #endif
<span class="line-removed">  48 #include &quot;vm_version_x86.hpp&quot;</span>
  49 
  50 #define __ masm-&gt;
  51 
  52 const int StackAlignmentInSlots = StackAlignmentInBytes / VMRegImpl::stack_slot_size;
  53 
  54 class RegisterSaver {
  55   // Capture info about frame layout
  56 #define DEF_XMM_OFFS(regnum) xmm ## regnum ## _off = xmm_off + (regnum)*16/BytesPerInt, xmm ## regnum ## H_off
  57   enum layout {
  58                 fpu_state_off = 0,
  59                 fpu_state_end = fpu_state_off+FPUStateSizeInWords,
  60                 st0_off, st0H_off,
  61                 st1_off, st1H_off,
  62                 st2_off, st2H_off,
  63                 st3_off, st3H_off,
  64                 st4_off, st4H_off,
  65                 st5_off, st5H_off,
  66                 st6_off, st6H_off,
  67                 st7_off, st7H_off,
  68                 xmm_off,
</pre>
<hr />
<pre>
 957 
 958   {
 959 
 960     Label missed;
 961     __ movptr(temp, Address(receiver, oopDesc::klass_offset_in_bytes()));
 962     __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));
 963     __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));
 964     __ jcc(Assembler::notEqual, missed);
 965     // Method might have been compiled since the call site was patched to
 966     // interpreted if that is the case treat it as a miss so we can get
 967     // the call site corrected.
 968     __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);
 969     __ jcc(Assembler::equal, skip_fixup);
 970 
 971     __ bind(missed);
 972     __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
 973   }
 974 
 975   address c2i_entry = __ pc();
 976 



 977   gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);
 978 
 979   __ flush();
 980   return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry);
 981 }
 982 
 983 int SharedRuntime::c_calling_convention(const BasicType *sig_bt,
 984                                          VMRegPair *regs,
 985                                          VMRegPair *regs2,
 986                                          int total_args_passed) {
 987   assert(regs2 == NULL, &quot;not needed on x86&quot;);
 988 // We return the amount of VMRegImpl stack slots we need to reserve for all
 989 // the arguments NOT counting out_preserve_stack_slots.
 990 
 991   uint    stack = 0;        // All arguments on stack
 992 
 993   for( int i = 0; i &lt; total_args_passed; i++) {
 994     // From the type and the argument number (count) compute the location
 995     switch( sig_bt[i] ) {
 996     case T_BOOLEAN:
</pre>
<hr />
<pre>
1286     } else if (in_regs[i].first()-&gt;is_XMMRegister()) {
1287       if (in_sig_bt[i] == T_FLOAT) {
1288         int slot = handle_index++ * VMRegImpl::slots_per_word + arg_save_area;
1289         int offset = slot * VMRegImpl::stack_slot_size;
1290         assert(handle_index &lt;= stack_slots, &quot;overflow&quot;);
1291         if (map != NULL) {
1292           __ movflt(Address(rsp, offset), in_regs[i].first()-&gt;as_XMMRegister());
1293         } else {
1294           __ movflt(in_regs[i].first()-&gt;as_XMMRegister(), Address(rsp, offset));
1295         }
1296       }
1297     } else if (in_regs[i].first()-&gt;is_stack()) {
1298       if (in_sig_bt[i] == T_ARRAY &amp;&amp; map != NULL) {
1299         int offset_in_older_frame = in_regs[i].first()-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
1300         map-&gt;set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));
1301       }
1302     }
1303   }
1304 }
1305 



























































































1306 // Check GCLocker::needs_gc and enter the runtime if it&#39;s true.  This
1307 // keeps a new JNI critical region from starting until a GC has been
1308 // forced.  Save down any oops in registers and describe them in an
1309 // OopMap.
1310 static void check_needs_gc_for_critical_native(MacroAssembler* masm,
1311                                                Register thread,
1312                                                int stack_slots,
1313                                                int total_c_args,
1314                                                int total_in_args,
1315                                                int arg_save_area,
1316                                                OopMapSet* oop_maps,
1317                                                VMRegPair* in_regs,
1318                                                BasicType* in_sig_bt) {
1319   __ block_comment(&quot;check GCLocker::needs_gc&quot;);
1320   Label cont;
1321   __ cmp8(ExternalAddress((address)GCLocker::needs_gc_address()), false);
1322   __ jcc(Assembler::equal, cont);
1323 
1324   // Save down any incoming oops and call into the runtime to halt for a GC
1325 
</pre>
<hr />
<pre>
1399   // load the length relative to the body.
1400   __ movl(tmp_reg, Address(tmp_reg, arrayOopDesc::length_offset_in_bytes() -
1401                            arrayOopDesc::base_offset_in_bytes(in_elem_type)));
1402   simple_move32(masm, tmp, length_arg);
1403   __ jmpb(done);
1404   __ bind(is_null);
1405   // Pass zeros
1406   __ xorptr(tmp_reg, tmp_reg);
1407   simple_move32(masm, tmp, body_arg);
1408   simple_move32(masm, tmp, length_arg);
1409   __ bind(done);
1410 }
1411 
1412 static void verify_oop_args(MacroAssembler* masm,
1413                             const methodHandle&amp; method,
1414                             const BasicType* sig_bt,
1415                             const VMRegPair* regs) {
1416   Register temp_reg = rbx;  // not part of any compiled calling seq
1417   if (VerifyOops) {
1418     for (int i = 0; i &lt; method-&gt;size_of_parameters(); i++) {
<span class="line-modified">1419       if (sig_bt[i] == T_OBJECT ||</span>
<span class="line-removed">1420           sig_bt[i] == T_ARRAY) {</span>
1421         VMReg r = regs[i].first();
1422         assert(r-&gt;is_valid(), &quot;bad oop arg&quot;);
1423         if (r-&gt;is_stack()) {
1424           __ movptr(temp_reg, Address(rsp, r-&gt;reg2stack() * VMRegImpl::stack_slot_size + wordSize));
1425           __ verify_oop(temp_reg);
1426         } else {
1427           __ verify_oop(r-&gt;as_Register());
1428         }
1429       }
1430     }
1431   }
1432 }
1433 
1434 static void gen_special_dispatch(MacroAssembler* masm,
1435                                  const methodHandle&amp; method,
1436                                  const BasicType* sig_bt,
1437                                  const VMRegPair* regs) {
1438   verify_oop_args(masm, method, sig_bt, regs);
1439   vmIntrinsics::ID iid = method-&gt;intrinsic_id();
1440 
</pre>
<hr />
<pre>
1507 // block and the check for pending exceptions it&#39;s impossible for them
1508 // to be thrown.
1509 //
1510 // They are roughly structured like this:
1511 //    if (GCLocker::needs_gc())
1512 //      SharedRuntime::block_for_jni_critical();
1513 //    tranistion to thread_in_native
1514 //    unpack arrray arguments and call native entry point
1515 //    check for safepoint in progress
1516 //    check if any thread suspend flags are set
1517 //      call into JVM and possible unlock the JNI critical
1518 //      if a GC was suppressed while in the critical native.
1519 //    transition back to thread_in_Java
1520 //    return to caller
1521 //
1522 nmethod* SharedRuntime::generate_native_wrapper(MacroAssembler* masm,
1523                                                 const methodHandle&amp; method,
1524                                                 int compile_id,
1525                                                 BasicType* in_sig_bt,
1526                                                 VMRegPair* in_regs,
<span class="line-modified">1527                                                 BasicType ret_type) {</span>

1528   if (method-&gt;is_method_handle_intrinsic()) {
1529     vmIntrinsics::ID iid = method-&gt;intrinsic_id();
1530     intptr_t start = (intptr_t)__ pc();
1531     int vep_offset = ((intptr_t)__ pc()) - start;
1532     gen_special_dispatch(masm,
1533                          method,
1534                          in_sig_bt,
1535                          in_regs);
1536     int frame_complete = ((intptr_t)__ pc()) - start;  // not complete, period
1537     __ flush();
1538     int stack_slots = SharedRuntime::out_preserve_stack_slots();  // no out slots at all, actually
1539     return nmethod::new_native_nmethod(method,
1540                                        compile_id,
1541                                        masm-&gt;code(),
1542                                        vep_offset,
1543                                        frame_complete,
1544                                        stack_slots / VMRegImpl::slots_per_word,
1545                                        in_ByteSize(-1),
1546                                        in_ByteSize(-1),
1547                                        (OopMapSet*)NULL);
1548   }
1549   bool is_critical_native = true;
<span class="line-modified">1550   address native_func = method-&gt;critical_native_function();</span>
1551   if (native_func == NULL) {
1552     native_func = method-&gt;native_function();
1553     is_critical_native = false;
1554   }
1555   assert(native_func != NULL, &quot;must have function&quot;);
1556 
1557   // An OopMap for lock (and class if static)
1558   OopMapSet *oop_maps = new OopMapSet();
1559 
1560   // We have received a description of where all the java arg are located
1561   // on entry to the wrapper. We need to convert these args to where
1562   // the jni function will expect them. To figure out where they go
1563   // we convert the java signature to a C signature by inserting
1564   // the hidden arguments as arg[0] and possibly arg[1] (static method)
1565 
1566   const int total_in_args = method-&gt;size_of_parameters();
1567   int total_c_args = total_in_args;
1568   if (!is_critical_native) {
1569     total_c_args += 1;
1570     if (method-&gt;is_static()) {
</pre>
<hr />
<pre>
1576         total_c_args++;
1577       }
1578     }
1579   }
1580 
1581   BasicType* out_sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_c_args);
1582   VMRegPair* out_regs   = NEW_RESOURCE_ARRAY(VMRegPair, total_c_args);
1583   BasicType* in_elem_bt = NULL;
1584 
1585   int argc = 0;
1586   if (!is_critical_native) {
1587     out_sig_bt[argc++] = T_ADDRESS;
1588     if (method-&gt;is_static()) {
1589       out_sig_bt[argc++] = T_OBJECT;
1590     }
1591 
1592     for (int i = 0; i &lt; total_in_args ; i++ ) {
1593       out_sig_bt[argc++] = in_sig_bt[i];
1594     }
1595   } else {
<span class="line-removed">1596     Thread* THREAD = Thread::current();</span>
1597     in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_in_args);
1598     SignatureStream ss(method-&gt;signature());
1599     for (int i = 0; i &lt; total_in_args ; i++ ) {
1600       if (in_sig_bt[i] == T_ARRAY) {
1601         // Arrays are passed as int, elem* pair
1602         out_sig_bt[argc++] = T_INT;
1603         out_sig_bt[argc++] = T_ADDRESS;
<span class="line-modified">1604         Symbol* atype = ss.as_symbol(CHECK_NULL);</span>
<span class="line-modified">1605         const char* at = atype-&gt;as_C_string();</span>
<span class="line-modified">1606         if (strlen(at) == 2) {</span>
<span class="line-removed">1607           assert(at[0] == &#39;[&#39;, &quot;must be&quot;);</span>
<span class="line-removed">1608           switch (at[1]) {</span>
<span class="line-removed">1609             case &#39;B&#39;: in_elem_bt[i]  = T_BYTE; break;</span>
<span class="line-removed">1610             case &#39;C&#39;: in_elem_bt[i]  = T_CHAR; break;</span>
<span class="line-removed">1611             case &#39;D&#39;: in_elem_bt[i]  = T_DOUBLE; break;</span>
<span class="line-removed">1612             case &#39;F&#39;: in_elem_bt[i]  = T_FLOAT; break;</span>
<span class="line-removed">1613             case &#39;I&#39;: in_elem_bt[i]  = T_INT; break;</span>
<span class="line-removed">1614             case &#39;J&#39;: in_elem_bt[i]  = T_LONG; break;</span>
<span class="line-removed">1615             case &#39;S&#39;: in_elem_bt[i]  = T_SHORT; break;</span>
<span class="line-removed">1616             case &#39;Z&#39;: in_elem_bt[i]  = T_BOOLEAN; break;</span>
<span class="line-removed">1617             default: ShouldNotReachHere();</span>
<span class="line-removed">1618           }</span>
<span class="line-removed">1619         }</span>
1620       } else {
1621         out_sig_bt[argc++] = in_sig_bt[i];
1622         in_elem_bt[i] = T_VOID;
1623       }
1624       if (in_sig_bt[i] != T_VOID) {
<span class="line-modified">1625         assert(in_sig_bt[i] == ss.type(), &quot;must match&quot;);</span>

1626         ss.next();
1627       }
1628     }
1629   }
1630 
1631   // Now figure out where the args must be stored and how much stack space
1632   // they require.
1633   int out_arg_slots;
1634   out_arg_slots = c_calling_convention(out_sig_bt, out_regs, NULL, total_c_args);
1635 
1636   // Compute framesize for the wrapper.  We need to handlize all oops in
1637   // registers a max of 2 on x86.
1638 
1639   // Calculate the total number of stack slots we will need.
1640 
1641   // First count the abi requirement plus all of the outgoing args
1642   int stack_slots = SharedRuntime::out_preserve_stack_slots() + out_arg_slots;
1643 
1644   // Now the space for the inbound oop handle area
1645   int total_save_slots = 2 * VMRegImpl::slots_per_word; // 2 arguments passed in registers
</pre>
<hr />
<pre>
1778 #endif // COMPILER1
1779 
1780   // The instruction at the verified entry point must be 5 bytes or longer
1781   // because it can be patched on the fly by make_non_entrant. The stack bang
1782   // instruction fits that requirement.
1783 
1784   // Generate stack overflow check
1785 
1786   if (UseStackBanging) {
1787     __ bang_stack_with_offset((int)JavaThread::stack_shadow_zone_size());
1788   } else {
1789     // need a 5 byte instruction to allow MT safe patching to non-entrant
1790     __ fat_nop();
1791   }
1792 
1793   // Generate a new frame for the wrapper.
1794   __ enter();
1795   // -2 because return address is already present and so is saved rbp
1796   __ subptr(rsp, stack_size - 2*wordSize);
1797 




1798   // Frame is now completed as far as size and linkage.
1799   int frame_complete = ((intptr_t)__ pc()) - start;
1800 
1801   if (UseRTMLocking) {
1802     // Abort RTM transaction before calling JNI
1803     // because critical section will be large and will be
1804     // aborted anyway. Also nmethod could be deoptimized.
1805     __ xabort(0);
1806   }
1807 
1808   // Calculate the difference between rsp and rbp,. We need to know it
1809   // after the native call because on windows Java Natives will pop
1810   // the arguments and it is painful to do rsp relative addressing
1811   // in a platform independent way. So after the call we switch to
1812   // rbp, relative addressing.
1813 
1814   int fp_adjustment = stack_size - 2*wordSize;
1815 
1816 #ifdef COMPILER2
1817   // C2 may leave the stack dirty if not in SSE2+ mode
1818   if (UseSSE &gt;= 2) {
1819     __ verify_FPU(0, &quot;c2i transition should have clean FPU stack&quot;);
1820   } else {
1821     __ empty_FPU_stack();
1822   }
1823 #endif /* COMPILER2 */
1824 
1825   // Compute the rbp, offset for any slots used after the jni call
1826 
1827   int lock_slot_rbp_offset = (lock_slot_offset*VMRegImpl::stack_slot_size) - fp_adjustment;
1828 
1829   // We use rdi as a thread pointer because it is callee save and
1830   // if we load it once it is usable thru the entire wrapper
1831   const Register thread = rdi;
1832 
<span class="line-modified">1833   // We use rsi as the oop handle for the receiver/klass</span>
<span class="line-modified">1834   // It is callee save so it survives the call to native</span>
1835 
<span class="line-modified">1836   const Register oop_handle_reg = rsi;</span>
1837 
<span class="line-modified">1838   __ get_thread(thread);</span>
1839 
<span class="line-modified">1840   if (is_critical_native) {</span>
1841     check_needs_gc_for_critical_native(masm, thread, stack_slots, total_c_args, total_in_args,
1842                                        oop_handle_offset, oop_maps, in_regs, in_sig_bt);
1843   }
1844 
1845   //
1846   // We immediately shuffle the arguments so that any vm call we have to
1847   // make from here on out (sync slow path, jvmti, etc.) we will have
1848   // captured the oops from our caller and have a valid oopMap for
1849   // them.
1850 
1851   // -----------------
1852   // The Grand Shuffle
1853   //
1854   // Natives require 1 or 2 extra arguments over the normal ones: the JNIEnv*
1855   // and, if static, the class mirror instead of a receiver.  This pretty much
1856   // guarantees that register layout will not match (and x86 doesn&#39;t use reg
1857   // parms though amd does).  Since the native abi doesn&#39;t use register args
1858   // and the java conventions does we don&#39;t have to worry about collisions.
1859   // All of our moved are reg-&gt;stack or stack-&gt;stack.
1860   // We ignore the extra arguments during the shuffle and handle them at the
1861   // last moment. The shuffle is described by the two calling convention
1862   // vectors we have in our possession. We simply walk the java vector to
1863   // get the source locations and the c vector to get the destinations.
1864 
1865   int c_arg = is_critical_native ? 0 : (method-&gt;is_static() ? 2 : 1 );
1866 
1867   // Record rsp-based slot for receiver on stack for non-static methods
1868   int receiver_offset = -1;
1869 
1870   // This is a trick. We double the stack slots so we can claim
1871   // the oops in the caller&#39;s frame. Since we are sure to have
1872   // more args than the caller doubling is enough to make
1873   // sure we can capture all the incoming oop args from the
1874   // caller.
1875   //
1876   OopMap* map = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
1877 





1878   // Mark location of rbp,
1879   // map-&gt;set_callee_saved(VMRegImpl::stack2reg( stack_slots - 2), stack_slots * 2, 0, rbp-&gt;as_VMReg());
1880 
1881   // We know that we only have args in at most two integer registers (rcx, rdx). So rax, rbx
1882   // Are free to temporaries if we have to do  stack to steck moves.
1883   // All inbound args are referenced based on rbp, and all outbound args via rsp.
1884 
1885   for (int i = 0; i &lt; total_in_args ; i++, c_arg++ ) {
1886     switch (in_sig_bt[i]) {
1887       case T_ARRAY:
1888         if (is_critical_native) {
<span class="line-modified">1889           unpack_array_argument(masm, in_regs[i], in_elem_bt[i], out_regs[c_arg + 1], out_regs[c_arg]);</span>





















1890           c_arg++;
1891           break;
1892         }
1893       case T_OBJECT:
1894         assert(!is_critical_native, &quot;no oop arguments&quot;);
1895         object_move(masm, map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],
1896                     ((i == 0) &amp;&amp; (!is_static)),
1897                     &amp;receiver_offset);
1898         break;
1899       case T_VOID:
1900         break;
1901 
1902       case T_FLOAT:
1903         float_move(masm, in_regs[i], out_regs[c_arg]);
1904           break;
1905 
1906       case T_DOUBLE:
1907         assert( i + 1 &lt; total_in_args &amp;&amp;
1908                 in_sig_bt[i + 1] == T_VOID &amp;&amp;
1909                 out_sig_bt[c_arg+1] == T_VOID, &quot;bad arg list&quot;);
</pre>
<hr />
<pre>
2062 
2063   // Unpack native results.
2064   switch (ret_type) {
2065   case T_BOOLEAN: __ c2bool(rax);            break;
2066   case T_CHAR   : __ andptr(rax, 0xFFFF);    break;
2067   case T_BYTE   : __ sign_extend_byte (rax); break;
2068   case T_SHORT  : __ sign_extend_short(rax); break;
2069   case T_INT    : /* nothing to do */        break;
2070   case T_DOUBLE :
2071   case T_FLOAT  :
2072     // Result is in st0 we&#39;ll save as needed
2073     break;
2074   case T_ARRAY:                 // Really a handle
2075   case T_OBJECT:                // Really a handle
2076       break; // can&#39;t de-handlize until after safepoint check
2077   case T_VOID: break;
2078   case T_LONG: break;
2079   default       : ShouldNotReachHere();
2080   }
2081 




















2082   // Switch thread to &quot;native transition&quot; state before reading the synchronization state.
2083   // This additional state is necessary because reading and testing the synchronization
2084   // state is not atomic w.r.t. GC, as this scenario demonstrates:
2085   //     Java thread A, in _thread_in_native state, loads _not_synchronized and is preempted.
2086   //     VM thread changes sync state to synchronizing and suspends threads for GC.
2087   //     Thread A is resumed to finish this native method, but doesn&#39;t block here since it
2088   //     didn&#39;t see any synchronization is progress, and escapes.
2089   __ movl(Address(thread, JavaThread::thread_state_offset()), _thread_in_native_trans);
2090 
2091   // Force this write out before the read below
2092   __ membar(Assembler::Membar_mask_bits(
2093             Assembler::LoadLoad | Assembler::LoadStore |
2094             Assembler::StoreLoad | Assembler::StoreStore));
2095 
2096   if (AlwaysRestoreFPU) {
2097     // Make sure the control word is correct.
2098     __ fldcw(ExternalAddress(StubRoutines::addr_fpu_cntrl_wrd_std()));
2099   }
2100 
2101   Label after_transition;
</pre>
<hr />
<pre>
2201     __ bind(done);
2202 
2203   }
2204 
2205   {
2206     SkipIfEqual skip_if(masm, &amp;DTraceMethodProbes, 0);
2207     // Tell dtrace about this method exit
2208     save_native_result(masm, ret_type, stack_slots);
2209     __ mov_metadata(rax, method());
2210     __ call_VM_leaf(
2211          CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit),
2212          thread, rax);
2213     restore_native_result(masm, ret_type, stack_slots);
2214   }
2215 
2216   // We can finally stop using that last_Java_frame we setup ages ago
2217 
2218   __ reset_last_Java_frame(thread, false);
2219 
2220   // Unbox oop result, e.g. JNIHandles::resolve value.
<span class="line-modified">2221   if (ret_type == T_OBJECT || ret_type == T_ARRAY) {</span>
2222     __ resolve_jobject(rax /* value */,
2223                        thread /* thread */,
2224                        rcx /* tmp */);
2225   }
2226 
2227   if (CheckJNICalls) {
2228     // clear_pending_jni_exception_check
2229     __ movptr(Address(thread, JavaThread::pending_jni_exception_check_fn_offset()), NULL_WORD);
2230   }
2231 
2232   if (!is_critical_native) {
2233     // reset handle block
2234     __ movptr(rcx, Address(thread, JavaThread::active_handles_offset()));
2235     __ movl(Address(rcx, JNIHandleBlock::top_offset_in_bytes()), NULL_WORD);
2236 
2237     // Any exception pending?
2238     __ cmpptr(Address(thread, in_bytes(Thread::pending_exception_offset())), (int32_t)NULL_WORD);
2239     __ jcc(Assembler::notEqual, exception_pending);
2240   }
2241 
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2003, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;code/debugInfoRec.hpp&quot;
  29 #include &quot;code/icBuffer.hpp&quot;
  30 #include &quot;code/nativeInst.hpp&quot;
  31 #include &quot;code/vtableStubs.hpp&quot;
  32 #include &quot;gc/shared/gcLocker.hpp&quot;
<span class="line-added">  33 #include &quot;gc/shared/barrierSet.hpp&quot;</span>
<span class="line-added">  34 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;</span>
  35 #include &quot;interpreter/interpreter.hpp&quot;
  36 #include &quot;logging/log.hpp&quot;
  37 #include &quot;memory/resourceArea.hpp&quot;
  38 #include &quot;oops/compiledICHolder.hpp&quot;
<span class="line-added">  39 #include &quot;oops/klass.inline.hpp&quot;</span>
  40 #include &quot;runtime/safepointMechanism.hpp&quot;
  41 #include &quot;runtime/sharedRuntime.hpp&quot;
  42 #include &quot;runtime/vframeArray.hpp&quot;
<span class="line-added">  43 #include &quot;runtime/vm_version.hpp&quot;</span>
  44 #include &quot;utilities/align.hpp&quot;
  45 #include &quot;vmreg_x86.inline.hpp&quot;
  46 #ifdef COMPILER1
  47 #include &quot;c1/c1_Runtime1.hpp&quot;
  48 #endif
  49 #ifdef COMPILER2
  50 #include &quot;opto/runtime.hpp&quot;
  51 #endif

  52 
  53 #define __ masm-&gt;
  54 
  55 const int StackAlignmentInSlots = StackAlignmentInBytes / VMRegImpl::stack_slot_size;
  56 
  57 class RegisterSaver {
  58   // Capture info about frame layout
  59 #define DEF_XMM_OFFS(regnum) xmm ## regnum ## _off = xmm_off + (regnum)*16/BytesPerInt, xmm ## regnum ## H_off
  60   enum layout {
  61                 fpu_state_off = 0,
  62                 fpu_state_end = fpu_state_off+FPUStateSizeInWords,
  63                 st0_off, st0H_off,
  64                 st1_off, st1H_off,
  65                 st2_off, st2H_off,
  66                 st3_off, st3H_off,
  67                 st4_off, st4H_off,
  68                 st5_off, st5H_off,
  69                 st6_off, st6H_off,
  70                 st7_off, st7H_off,
  71                 xmm_off,
</pre>
<hr />
<pre>
 960 
 961   {
 962 
 963     Label missed;
 964     __ movptr(temp, Address(receiver, oopDesc::klass_offset_in_bytes()));
 965     __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));
 966     __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));
 967     __ jcc(Assembler::notEqual, missed);
 968     // Method might have been compiled since the call site was patched to
 969     // interpreted if that is the case treat it as a miss so we can get
 970     // the call site corrected.
 971     __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);
 972     __ jcc(Assembler::equal, skip_fixup);
 973 
 974     __ bind(missed);
 975     __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
 976   }
 977 
 978   address c2i_entry = __ pc();
 979 
<span class="line-added"> 980   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();</span>
<span class="line-added"> 981   bs-&gt;c2i_entry_barrier(masm);</span>
<span class="line-added"> 982 </span>
 983   gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);
 984 
 985   __ flush();
 986   return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry);
 987 }
 988 
 989 int SharedRuntime::c_calling_convention(const BasicType *sig_bt,
 990                                          VMRegPair *regs,
 991                                          VMRegPair *regs2,
 992                                          int total_args_passed) {
 993   assert(regs2 == NULL, &quot;not needed on x86&quot;);
 994 // We return the amount of VMRegImpl stack slots we need to reserve for all
 995 // the arguments NOT counting out_preserve_stack_slots.
 996 
 997   uint    stack = 0;        // All arguments on stack
 998 
 999   for( int i = 0; i &lt; total_args_passed; i++) {
1000     // From the type and the argument number (count) compute the location
1001     switch( sig_bt[i] ) {
1002     case T_BOOLEAN:
</pre>
<hr />
<pre>
1292     } else if (in_regs[i].first()-&gt;is_XMMRegister()) {
1293       if (in_sig_bt[i] == T_FLOAT) {
1294         int slot = handle_index++ * VMRegImpl::slots_per_word + arg_save_area;
1295         int offset = slot * VMRegImpl::stack_slot_size;
1296         assert(handle_index &lt;= stack_slots, &quot;overflow&quot;);
1297         if (map != NULL) {
1298           __ movflt(Address(rsp, offset), in_regs[i].first()-&gt;as_XMMRegister());
1299         } else {
1300           __ movflt(in_regs[i].first()-&gt;as_XMMRegister(), Address(rsp, offset));
1301         }
1302       }
1303     } else if (in_regs[i].first()-&gt;is_stack()) {
1304       if (in_sig_bt[i] == T_ARRAY &amp;&amp; map != NULL) {
1305         int offset_in_older_frame = in_regs[i].first()-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
1306         map-&gt;set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));
1307       }
1308     }
1309   }
1310 }
1311 
<span class="line-added">1312 // Registers need to be saved for runtime call</span>
<span class="line-added">1313 static Register caller_saved_registers[] = {</span>
<span class="line-added">1314   rcx, rdx, rsi, rdi</span>
<span class="line-added">1315 };</span>
<span class="line-added">1316 </span>
<span class="line-added">1317 // Save caller saved registers except r1 and r2</span>
<span class="line-added">1318 static void save_registers_except(MacroAssembler* masm, Register r1, Register r2) {</span>
<span class="line-added">1319   int reg_len = (int)(sizeof(caller_saved_registers) / sizeof(Register));</span>
<span class="line-added">1320   for (int index = 0; index &lt; reg_len; index ++) {</span>
<span class="line-added">1321     Register this_reg = caller_saved_registers[index];</span>
<span class="line-added">1322     if (this_reg != r1 &amp;&amp; this_reg != r2) {</span>
<span class="line-added">1323       __ push(this_reg);</span>
<span class="line-added">1324     }</span>
<span class="line-added">1325   }</span>
<span class="line-added">1326 }</span>
<span class="line-added">1327 </span>
<span class="line-added">1328 // Restore caller saved registers except r1 and r2</span>
<span class="line-added">1329 static void restore_registers_except(MacroAssembler* masm, Register r1, Register r2) {</span>
<span class="line-added">1330   int reg_len = (int)(sizeof(caller_saved_registers) / sizeof(Register));</span>
<span class="line-added">1331   for (int index = reg_len - 1; index &gt;= 0; index --) {</span>
<span class="line-added">1332     Register this_reg = caller_saved_registers[index];</span>
<span class="line-added">1333     if (this_reg != r1 &amp;&amp; this_reg != r2) {</span>
<span class="line-added">1334       __ pop(this_reg);</span>
<span class="line-added">1335     }</span>
<span class="line-added">1336   }</span>
<span class="line-added">1337 }</span>
<span class="line-added">1338 </span>
<span class="line-added">1339 // Pin object, return pinned object or null in rax</span>
<span class="line-added">1340 static void gen_pin_object(MacroAssembler* masm,</span>
<span class="line-added">1341                            Register thread, VMRegPair reg) {</span>
<span class="line-added">1342   __ block_comment(&quot;gen_pin_object {&quot;);</span>
<span class="line-added">1343 </span>
<span class="line-added">1344   Label is_null;</span>
<span class="line-added">1345   Register tmp_reg = rax;</span>
<span class="line-added">1346   VMRegPair tmp(tmp_reg-&gt;as_VMReg());</span>
<span class="line-added">1347   if (reg.first()-&gt;is_stack()) {</span>
<span class="line-added">1348     // Load the arg up from the stack</span>
<span class="line-added">1349     simple_move32(masm, reg, tmp);</span>
<span class="line-added">1350     reg = tmp;</span>
<span class="line-added">1351   } else {</span>
<span class="line-added">1352     __ movl(tmp_reg, reg.first()-&gt;as_Register());</span>
<span class="line-added">1353   }</span>
<span class="line-added">1354   __ testptr(reg.first()-&gt;as_Register(), reg.first()-&gt;as_Register());</span>
<span class="line-added">1355   __ jccb(Assembler::equal, is_null);</span>
<span class="line-added">1356 </span>
<span class="line-added">1357   // Save registers that may be used by runtime call</span>
<span class="line-added">1358   Register arg = reg.first()-&gt;is_Register() ? reg.first()-&gt;as_Register() : noreg;</span>
<span class="line-added">1359   save_registers_except(masm, arg, thread);</span>
<span class="line-added">1360 </span>
<span class="line-added">1361   __ call_VM_leaf(</span>
<span class="line-added">1362     CAST_FROM_FN_PTR(address, SharedRuntime::pin_object),</span>
<span class="line-added">1363     thread, reg.first()-&gt;as_Register());</span>
<span class="line-added">1364 </span>
<span class="line-added">1365   // Restore saved registers</span>
<span class="line-added">1366   restore_registers_except(masm, arg, thread);</span>
<span class="line-added">1367 </span>
<span class="line-added">1368   __ bind(is_null);</span>
<span class="line-added">1369   __ block_comment(&quot;} gen_pin_object&quot;);</span>
<span class="line-added">1370 }</span>
<span class="line-added">1371 </span>
<span class="line-added">1372 // Unpin object</span>
<span class="line-added">1373 static void gen_unpin_object(MacroAssembler* masm,</span>
<span class="line-added">1374                              Register thread, VMRegPair reg) {</span>
<span class="line-added">1375   __ block_comment(&quot;gen_unpin_object {&quot;);</span>
<span class="line-added">1376   Label is_null;</span>
<span class="line-added">1377 </span>
<span class="line-added">1378   // temp register</span>
<span class="line-added">1379   __ push(rax);</span>
<span class="line-added">1380   Register tmp_reg = rax;</span>
<span class="line-added">1381   VMRegPair tmp(tmp_reg-&gt;as_VMReg());</span>
<span class="line-added">1382 </span>
<span class="line-added">1383   simple_move32(masm, reg, tmp);</span>
<span class="line-added">1384 </span>
<span class="line-added">1385   __ testptr(rax, rax);</span>
<span class="line-added">1386   __ jccb(Assembler::equal, is_null);</span>
<span class="line-added">1387 </span>
<span class="line-added">1388   // Save registers that may be used by runtime call</span>
<span class="line-added">1389   Register arg = reg.first()-&gt;is_Register() ? reg.first()-&gt;as_Register() : noreg;</span>
<span class="line-added">1390   save_registers_except(masm, arg, thread);</span>
<span class="line-added">1391 </span>
<span class="line-added">1392   __ call_VM_leaf(</span>
<span class="line-added">1393     CAST_FROM_FN_PTR(address, SharedRuntime::unpin_object),</span>
<span class="line-added">1394     thread, rax);</span>
<span class="line-added">1395 </span>
<span class="line-added">1396   // Restore saved registers</span>
<span class="line-added">1397   restore_registers_except(masm, arg, thread);</span>
<span class="line-added">1398   __ bind(is_null);</span>
<span class="line-added">1399   __ pop(rax);</span>
<span class="line-added">1400   __ block_comment(&quot;} gen_unpin_object&quot;);</span>
<span class="line-added">1401 }</span>
<span class="line-added">1402 </span>
1403 // Check GCLocker::needs_gc and enter the runtime if it&#39;s true.  This
1404 // keeps a new JNI critical region from starting until a GC has been
1405 // forced.  Save down any oops in registers and describe them in an
1406 // OopMap.
1407 static void check_needs_gc_for_critical_native(MacroAssembler* masm,
1408                                                Register thread,
1409                                                int stack_slots,
1410                                                int total_c_args,
1411                                                int total_in_args,
1412                                                int arg_save_area,
1413                                                OopMapSet* oop_maps,
1414                                                VMRegPair* in_regs,
1415                                                BasicType* in_sig_bt) {
1416   __ block_comment(&quot;check GCLocker::needs_gc&quot;);
1417   Label cont;
1418   __ cmp8(ExternalAddress((address)GCLocker::needs_gc_address()), false);
1419   __ jcc(Assembler::equal, cont);
1420 
1421   // Save down any incoming oops and call into the runtime to halt for a GC
1422 
</pre>
<hr />
<pre>
1496   // load the length relative to the body.
1497   __ movl(tmp_reg, Address(tmp_reg, arrayOopDesc::length_offset_in_bytes() -
1498                            arrayOopDesc::base_offset_in_bytes(in_elem_type)));
1499   simple_move32(masm, tmp, length_arg);
1500   __ jmpb(done);
1501   __ bind(is_null);
1502   // Pass zeros
1503   __ xorptr(tmp_reg, tmp_reg);
1504   simple_move32(masm, tmp, body_arg);
1505   simple_move32(masm, tmp, length_arg);
1506   __ bind(done);
1507 }
1508 
1509 static void verify_oop_args(MacroAssembler* masm,
1510                             const methodHandle&amp; method,
1511                             const BasicType* sig_bt,
1512                             const VMRegPair* regs) {
1513   Register temp_reg = rbx;  // not part of any compiled calling seq
1514   if (VerifyOops) {
1515     for (int i = 0; i &lt; method-&gt;size_of_parameters(); i++) {
<span class="line-modified">1516       if (is_reference_type(sig_bt[i])) {</span>

1517         VMReg r = regs[i].first();
1518         assert(r-&gt;is_valid(), &quot;bad oop arg&quot;);
1519         if (r-&gt;is_stack()) {
1520           __ movptr(temp_reg, Address(rsp, r-&gt;reg2stack() * VMRegImpl::stack_slot_size + wordSize));
1521           __ verify_oop(temp_reg);
1522         } else {
1523           __ verify_oop(r-&gt;as_Register());
1524         }
1525       }
1526     }
1527   }
1528 }
1529 
1530 static void gen_special_dispatch(MacroAssembler* masm,
1531                                  const methodHandle&amp; method,
1532                                  const BasicType* sig_bt,
1533                                  const VMRegPair* regs) {
1534   verify_oop_args(masm, method, sig_bt, regs);
1535   vmIntrinsics::ID iid = method-&gt;intrinsic_id();
1536 
</pre>
<hr />
<pre>
1603 // block and the check for pending exceptions it&#39;s impossible for them
1604 // to be thrown.
1605 //
1606 // They are roughly structured like this:
1607 //    if (GCLocker::needs_gc())
1608 //      SharedRuntime::block_for_jni_critical();
1609 //    tranistion to thread_in_native
1610 //    unpack arrray arguments and call native entry point
1611 //    check for safepoint in progress
1612 //    check if any thread suspend flags are set
1613 //      call into JVM and possible unlock the JNI critical
1614 //      if a GC was suppressed while in the critical native.
1615 //    transition back to thread_in_Java
1616 //    return to caller
1617 //
1618 nmethod* SharedRuntime::generate_native_wrapper(MacroAssembler* masm,
1619                                                 const methodHandle&amp; method,
1620                                                 int compile_id,
1621                                                 BasicType* in_sig_bt,
1622                                                 VMRegPair* in_regs,
<span class="line-modified">1623                                                 BasicType ret_type,</span>
<span class="line-added">1624                                                 address critical_entry) {</span>
1625   if (method-&gt;is_method_handle_intrinsic()) {
1626     vmIntrinsics::ID iid = method-&gt;intrinsic_id();
1627     intptr_t start = (intptr_t)__ pc();
1628     int vep_offset = ((intptr_t)__ pc()) - start;
1629     gen_special_dispatch(masm,
1630                          method,
1631                          in_sig_bt,
1632                          in_regs);
1633     int frame_complete = ((intptr_t)__ pc()) - start;  // not complete, period
1634     __ flush();
1635     int stack_slots = SharedRuntime::out_preserve_stack_slots();  // no out slots at all, actually
1636     return nmethod::new_native_nmethod(method,
1637                                        compile_id,
1638                                        masm-&gt;code(),
1639                                        vep_offset,
1640                                        frame_complete,
1641                                        stack_slots / VMRegImpl::slots_per_word,
1642                                        in_ByteSize(-1),
1643                                        in_ByteSize(-1),
1644                                        (OopMapSet*)NULL);
1645   }
1646   bool is_critical_native = true;
<span class="line-modified">1647   address native_func = critical_entry;</span>
1648   if (native_func == NULL) {
1649     native_func = method-&gt;native_function();
1650     is_critical_native = false;
1651   }
1652   assert(native_func != NULL, &quot;must have function&quot;);
1653 
1654   // An OopMap for lock (and class if static)
1655   OopMapSet *oop_maps = new OopMapSet();
1656 
1657   // We have received a description of where all the java arg are located
1658   // on entry to the wrapper. We need to convert these args to where
1659   // the jni function will expect them. To figure out where they go
1660   // we convert the java signature to a C signature by inserting
1661   // the hidden arguments as arg[0] and possibly arg[1] (static method)
1662 
1663   const int total_in_args = method-&gt;size_of_parameters();
1664   int total_c_args = total_in_args;
1665   if (!is_critical_native) {
1666     total_c_args += 1;
1667     if (method-&gt;is_static()) {
</pre>
<hr />
<pre>
1673         total_c_args++;
1674       }
1675     }
1676   }
1677 
1678   BasicType* out_sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_c_args);
1679   VMRegPair* out_regs   = NEW_RESOURCE_ARRAY(VMRegPair, total_c_args);
1680   BasicType* in_elem_bt = NULL;
1681 
1682   int argc = 0;
1683   if (!is_critical_native) {
1684     out_sig_bt[argc++] = T_ADDRESS;
1685     if (method-&gt;is_static()) {
1686       out_sig_bt[argc++] = T_OBJECT;
1687     }
1688 
1689     for (int i = 0; i &lt; total_in_args ; i++ ) {
1690       out_sig_bt[argc++] = in_sig_bt[i];
1691     }
1692   } else {

1693     in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_in_args);
1694     SignatureStream ss(method-&gt;signature());
1695     for (int i = 0; i &lt; total_in_args ; i++ ) {
1696       if (in_sig_bt[i] == T_ARRAY) {
1697         // Arrays are passed as int, elem* pair
1698         out_sig_bt[argc++] = T_INT;
1699         out_sig_bt[argc++] = T_ADDRESS;
<span class="line-modified">1700         ss.skip_array_prefix(1);  // skip one &#39;[&#39;</span>
<span class="line-modified">1701         assert(ss.is_primitive(), &quot;primitive type expected&quot;);</span>
<span class="line-modified">1702         in_elem_bt[i] = ss.type();</span>













1703       } else {
1704         out_sig_bt[argc++] = in_sig_bt[i];
1705         in_elem_bt[i] = T_VOID;
1706       }
1707       if (in_sig_bt[i] != T_VOID) {
<span class="line-modified">1708         assert(in_sig_bt[i] == ss.type() ||</span>
<span class="line-added">1709                in_sig_bt[i] == T_ARRAY, &quot;must match&quot;);</span>
1710         ss.next();
1711       }
1712     }
1713   }
1714 
1715   // Now figure out where the args must be stored and how much stack space
1716   // they require.
1717   int out_arg_slots;
1718   out_arg_slots = c_calling_convention(out_sig_bt, out_regs, NULL, total_c_args);
1719 
1720   // Compute framesize for the wrapper.  We need to handlize all oops in
1721   // registers a max of 2 on x86.
1722 
1723   // Calculate the total number of stack slots we will need.
1724 
1725   // First count the abi requirement plus all of the outgoing args
1726   int stack_slots = SharedRuntime::out_preserve_stack_slots() + out_arg_slots;
1727 
1728   // Now the space for the inbound oop handle area
1729   int total_save_slots = 2 * VMRegImpl::slots_per_word; // 2 arguments passed in registers
</pre>
<hr />
<pre>
1862 #endif // COMPILER1
1863 
1864   // The instruction at the verified entry point must be 5 bytes or longer
1865   // because it can be patched on the fly by make_non_entrant. The stack bang
1866   // instruction fits that requirement.
1867 
1868   // Generate stack overflow check
1869 
1870   if (UseStackBanging) {
1871     __ bang_stack_with_offset((int)JavaThread::stack_shadow_zone_size());
1872   } else {
1873     // need a 5 byte instruction to allow MT safe patching to non-entrant
1874     __ fat_nop();
1875   }
1876 
1877   // Generate a new frame for the wrapper.
1878   __ enter();
1879   // -2 because return address is already present and so is saved rbp
1880   __ subptr(rsp, stack_size - 2*wordSize);
1881 
<span class="line-added">1882 </span>
<span class="line-added">1883   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();</span>
<span class="line-added">1884   bs-&gt;nmethod_entry_barrier(masm);</span>
<span class="line-added">1885 </span>
1886   // Frame is now completed as far as size and linkage.
1887   int frame_complete = ((intptr_t)__ pc()) - start;
1888 
1889   if (UseRTMLocking) {
1890     // Abort RTM transaction before calling JNI
1891     // because critical section will be large and will be
1892     // aborted anyway. Also nmethod could be deoptimized.
1893     __ xabort(0);
1894   }
1895 
1896   // Calculate the difference between rsp and rbp,. We need to know it
1897   // after the native call because on windows Java Natives will pop
1898   // the arguments and it is painful to do rsp relative addressing
1899   // in a platform independent way. So after the call we switch to
1900   // rbp, relative addressing.
1901 
1902   int fp_adjustment = stack_size - 2*wordSize;
1903 
1904 #ifdef COMPILER2
1905   // C2 may leave the stack dirty if not in SSE2+ mode
1906   if (UseSSE &gt;= 2) {
1907     __ verify_FPU(0, &quot;c2i transition should have clean FPU stack&quot;);
1908   } else {
1909     __ empty_FPU_stack();
1910   }
1911 #endif /* COMPILER2 */
1912 
1913   // Compute the rbp, offset for any slots used after the jni call
1914 
1915   int lock_slot_rbp_offset = (lock_slot_offset*VMRegImpl::stack_slot_size) - fp_adjustment;
1916 
1917   // We use rdi as a thread pointer because it is callee save and
1918   // if we load it once it is usable thru the entire wrapper
1919   const Register thread = rdi;
1920 
<span class="line-modified">1921    // We use rsi as the oop handle for the receiver/klass</span>
<span class="line-modified">1922    // It is callee save so it survives the call to native</span>
1923 
<span class="line-modified">1924    const Register oop_handle_reg = rsi;</span>
1925 
<span class="line-modified">1926    __ get_thread(thread);</span>
1927 
<span class="line-modified">1928   if (is_critical_native &amp;&amp; !Universe::heap()-&gt;supports_object_pinning()) {</span>
1929     check_needs_gc_for_critical_native(masm, thread, stack_slots, total_c_args, total_in_args,
1930                                        oop_handle_offset, oop_maps, in_regs, in_sig_bt);
1931   }
1932 
1933   //
1934   // We immediately shuffle the arguments so that any vm call we have to
1935   // make from here on out (sync slow path, jvmti, etc.) we will have
1936   // captured the oops from our caller and have a valid oopMap for
1937   // them.
1938 
1939   // -----------------
1940   // The Grand Shuffle
1941   //
1942   // Natives require 1 or 2 extra arguments over the normal ones: the JNIEnv*
1943   // and, if static, the class mirror instead of a receiver.  This pretty much
1944   // guarantees that register layout will not match (and x86 doesn&#39;t use reg
1945   // parms though amd does).  Since the native abi doesn&#39;t use register args
1946   // and the java conventions does we don&#39;t have to worry about collisions.
1947   // All of our moved are reg-&gt;stack or stack-&gt;stack.
1948   // We ignore the extra arguments during the shuffle and handle them at the
1949   // last moment. The shuffle is described by the two calling convention
1950   // vectors we have in our possession. We simply walk the java vector to
1951   // get the source locations and the c vector to get the destinations.
1952 
1953   int c_arg = is_critical_native ? 0 : (method-&gt;is_static() ? 2 : 1 );
1954 
1955   // Record rsp-based slot for receiver on stack for non-static methods
1956   int receiver_offset = -1;
1957 
1958   // This is a trick. We double the stack slots so we can claim
1959   // the oops in the caller&#39;s frame. Since we are sure to have
1960   // more args than the caller doubling is enough to make
1961   // sure we can capture all the incoming oop args from the
1962   // caller.
1963   //
1964   OopMap* map = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
1965 
<span class="line-added">1966   // Inbound arguments that need to be pinned for critical natives</span>
<span class="line-added">1967   GrowableArray&lt;int&gt; pinned_args(total_in_args);</span>
<span class="line-added">1968   // Current stack slot for storing register based array argument</span>
<span class="line-added">1969   int pinned_slot = oop_handle_offset;</span>
<span class="line-added">1970 </span>
1971   // Mark location of rbp,
1972   // map-&gt;set_callee_saved(VMRegImpl::stack2reg( stack_slots - 2), stack_slots * 2, 0, rbp-&gt;as_VMReg());
1973 
1974   // We know that we only have args in at most two integer registers (rcx, rdx). So rax, rbx
1975   // Are free to temporaries if we have to do  stack to steck moves.
1976   // All inbound args are referenced based on rbp, and all outbound args via rsp.
1977 
1978   for (int i = 0; i &lt; total_in_args ; i++, c_arg++ ) {
1979     switch (in_sig_bt[i]) {
1980       case T_ARRAY:
1981         if (is_critical_native) {
<span class="line-modified">1982           VMRegPair in_arg = in_regs[i];</span>
<span class="line-added">1983           if (Universe::heap()-&gt;supports_object_pinning()) {</span>
<span class="line-added">1984             // gen_pin_object handles save and restore</span>
<span class="line-added">1985             // of any clobbered registers</span>
<span class="line-added">1986             gen_pin_object(masm, thread, in_arg);</span>
<span class="line-added">1987             pinned_args.append(i);</span>
<span class="line-added">1988 </span>
<span class="line-added">1989             // rax has pinned array</span>
<span class="line-added">1990             VMRegPair result_reg(rax-&gt;as_VMReg());</span>
<span class="line-added">1991             if (!in_arg.first()-&gt;is_stack()) {</span>
<span class="line-added">1992               assert(pinned_slot &lt;= stack_slots, &quot;overflow&quot;);</span>
<span class="line-added">1993               simple_move32(masm, result_reg, VMRegImpl::stack2reg(pinned_slot));</span>
<span class="line-added">1994               pinned_slot += VMRegImpl::slots_per_word;</span>
<span class="line-added">1995             } else {</span>
<span class="line-added">1996               // Write back pinned value, it will be used to unpin this argument</span>
<span class="line-added">1997               __ movptr(Address(rbp, reg2offset_in(in_arg.first())), result_reg.first()-&gt;as_Register());</span>
<span class="line-added">1998             }</span>
<span class="line-added">1999             // We have the array in register, use it</span>
<span class="line-added">2000             in_arg = result_reg;</span>
<span class="line-added">2001           }</span>
<span class="line-added">2002 </span>
<span class="line-added">2003           unpack_array_argument(masm, in_arg, in_elem_bt[i], out_regs[c_arg + 1], out_regs[c_arg]);</span>
2004           c_arg++;
2005           break;
2006         }
2007       case T_OBJECT:
2008         assert(!is_critical_native, &quot;no oop arguments&quot;);
2009         object_move(masm, map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],
2010                     ((i == 0) &amp;&amp; (!is_static)),
2011                     &amp;receiver_offset);
2012         break;
2013       case T_VOID:
2014         break;
2015 
2016       case T_FLOAT:
2017         float_move(masm, in_regs[i], out_regs[c_arg]);
2018           break;
2019 
2020       case T_DOUBLE:
2021         assert( i + 1 &lt; total_in_args &amp;&amp;
2022                 in_sig_bt[i + 1] == T_VOID &amp;&amp;
2023                 out_sig_bt[c_arg+1] == T_VOID, &quot;bad arg list&quot;);
</pre>
<hr />
<pre>
2176 
2177   // Unpack native results.
2178   switch (ret_type) {
2179   case T_BOOLEAN: __ c2bool(rax);            break;
2180   case T_CHAR   : __ andptr(rax, 0xFFFF);    break;
2181   case T_BYTE   : __ sign_extend_byte (rax); break;
2182   case T_SHORT  : __ sign_extend_short(rax); break;
2183   case T_INT    : /* nothing to do */        break;
2184   case T_DOUBLE :
2185   case T_FLOAT  :
2186     // Result is in st0 we&#39;ll save as needed
2187     break;
2188   case T_ARRAY:                 // Really a handle
2189   case T_OBJECT:                // Really a handle
2190       break; // can&#39;t de-handlize until after safepoint check
2191   case T_VOID: break;
2192   case T_LONG: break;
2193   default       : ShouldNotReachHere();
2194   }
2195 
<span class="line-added">2196   // unpin pinned arguments</span>
<span class="line-added">2197   pinned_slot = oop_handle_offset;</span>
<span class="line-added">2198   if (pinned_args.length() &gt; 0) {</span>
<span class="line-added">2199     // save return value that may be overwritten otherwise.</span>
<span class="line-added">2200     save_native_result(masm, ret_type, stack_slots);</span>
<span class="line-added">2201     for (int index = 0; index &lt; pinned_args.length(); index ++) {</span>
<span class="line-added">2202       int i = pinned_args.at(index);</span>
<span class="line-added">2203       assert(pinned_slot &lt;= stack_slots, &quot;overflow&quot;);</span>
<span class="line-added">2204       if (!in_regs[i].first()-&gt;is_stack()) {</span>
<span class="line-added">2205         int offset = pinned_slot * VMRegImpl::stack_slot_size;</span>
<span class="line-added">2206         __ movl(in_regs[i].first()-&gt;as_Register(), Address(rsp, offset));</span>
<span class="line-added">2207         pinned_slot += VMRegImpl::slots_per_word;</span>
<span class="line-added">2208       }</span>
<span class="line-added">2209       // gen_pin_object handles save and restore</span>
<span class="line-added">2210       // of any other clobbered registers</span>
<span class="line-added">2211       gen_unpin_object(masm, thread, in_regs[i]);</span>
<span class="line-added">2212     }</span>
<span class="line-added">2213     restore_native_result(masm, ret_type, stack_slots);</span>
<span class="line-added">2214   }</span>
<span class="line-added">2215 </span>
2216   // Switch thread to &quot;native transition&quot; state before reading the synchronization state.
2217   // This additional state is necessary because reading and testing the synchronization
2218   // state is not atomic w.r.t. GC, as this scenario demonstrates:
2219   //     Java thread A, in _thread_in_native state, loads _not_synchronized and is preempted.
2220   //     VM thread changes sync state to synchronizing and suspends threads for GC.
2221   //     Thread A is resumed to finish this native method, but doesn&#39;t block here since it
2222   //     didn&#39;t see any synchronization is progress, and escapes.
2223   __ movl(Address(thread, JavaThread::thread_state_offset()), _thread_in_native_trans);
2224 
2225   // Force this write out before the read below
2226   __ membar(Assembler::Membar_mask_bits(
2227             Assembler::LoadLoad | Assembler::LoadStore |
2228             Assembler::StoreLoad | Assembler::StoreStore));
2229 
2230   if (AlwaysRestoreFPU) {
2231     // Make sure the control word is correct.
2232     __ fldcw(ExternalAddress(StubRoutines::addr_fpu_cntrl_wrd_std()));
2233   }
2234 
2235   Label after_transition;
</pre>
<hr />
<pre>
2335     __ bind(done);
2336 
2337   }
2338 
2339   {
2340     SkipIfEqual skip_if(masm, &amp;DTraceMethodProbes, 0);
2341     // Tell dtrace about this method exit
2342     save_native_result(masm, ret_type, stack_slots);
2343     __ mov_metadata(rax, method());
2344     __ call_VM_leaf(
2345          CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit),
2346          thread, rax);
2347     restore_native_result(masm, ret_type, stack_slots);
2348   }
2349 
2350   // We can finally stop using that last_Java_frame we setup ages ago
2351 
2352   __ reset_last_Java_frame(thread, false);
2353 
2354   // Unbox oop result, e.g. JNIHandles::resolve value.
<span class="line-modified">2355   if (is_reference_type(ret_type)) {</span>
2356     __ resolve_jobject(rax /* value */,
2357                        thread /* thread */,
2358                        rcx /* tmp */);
2359   }
2360 
2361   if (CheckJNICalls) {
2362     // clear_pending_jni_exception_check
2363     __ movptr(Address(thread, JavaThread::pending_jni_exception_check_fn_offset()), NULL_WORD);
2364   }
2365 
2366   if (!is_critical_native) {
2367     // reset handle block
2368     __ movptr(rcx, Address(thread, JavaThread::active_handles_offset()));
2369     __ movl(Address(rcx, JNIHandleBlock::top_offset_in_bytes()), NULL_WORD);
2370 
2371     // Any exception pending?
2372     __ cmpptr(Address(thread, in_bytes(Thread::pending_exception_offset())), (int32_t)NULL_WORD);
2373     __ jcc(Assembler::notEqual, exception_pending);
2374   }
2375 
</pre>
</td>
</tr>
</table>
<center><a href="sharedRuntime_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="sharedRuntime_x86_64.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>