<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/cpu/x86/frame_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;interpreter/interpreter.hpp&quot;
 27 #include &quot;memory/resourceArea.hpp&quot;
 28 #include &quot;memory/universe.hpp&quot;
 29 #include &quot;oops/markWord.hpp&quot;
 30 #include &quot;oops/method.hpp&quot;
 31 #include &quot;oops/oop.inline.hpp&quot;
 32 #include &quot;prims/methodHandles.hpp&quot;
 33 #include &quot;runtime/frame.inline.hpp&quot;
 34 #include &quot;runtime/handles.inline.hpp&quot;
 35 #include &quot;runtime/javaCalls.hpp&quot;
 36 #include &quot;runtime/monitorChunk.hpp&quot;
 37 #include &quot;runtime/os.inline.hpp&quot;
 38 #include &quot;runtime/signature.hpp&quot;
 39 #include &quot;runtime/stubCodeGenerator.hpp&quot;
 40 #include &quot;runtime/stubRoutines.hpp&quot;
 41 #include &quot;vmreg_x86.inline.hpp&quot;
 42 #ifdef COMPILER1
 43 #include &quot;c1/c1_Runtime1.hpp&quot;
 44 #include &quot;runtime/vframeArray.hpp&quot;
 45 #endif
 46 
 47 #ifdef ASSERT
 48 void RegisterMap::check_location_valid() {
 49 }
 50 #endif
 51 
 52 // Profiling/safepoint support
 53 
 54 bool frame::safe_for_sender(JavaThread *thread) {
 55   address   sp = (address)_sp;
 56   address   fp = (address)_fp;
 57   address   unextended_sp = (address)_unextended_sp;
 58 
 59   // consider stack guards when trying to determine &quot;safe&quot; stack pointers
 60   // sp must be within the usable part of the stack (not in guards)
 61   if (!thread-&gt;is_in_usable_stack(sp)) {
 62     return false;
 63   }
 64 
 65   // unextended sp must be within the stack and above or equal sp
 66   bool unextended_sp_safe = (unextended_sp &lt; thread-&gt;stack_base()) &amp;&amp;
 67                             (unextended_sp &gt;= sp);
 68 
 69   if (!unextended_sp_safe) {
 70     return false;
 71   }
 72 
 73   // an fp must be within the stack and above (but not equal) sp
 74   // second evaluation on fp+ is added to handle situation where fp is -1
 75   bool fp_safe = (fp &lt; thread-&gt;stack_base() &amp;&amp; (fp &gt; sp) &amp;&amp; (((fp + (return_addr_offset * sizeof(void*))) &lt; thread-&gt;stack_base())));
 76 
 77   // We know sp/unextended_sp are safe only fp is questionable here
 78 
 79   // If the current frame is known to the code cache then we can attempt to
 80   // to construct the sender and do some validation of it. This goes a long way
 81   // toward eliminating issues when we get in frame construction code
 82 
 83   if (_cb != NULL ) {
 84 
 85     // First check if frame is complete and tester is reliable
 86     // Unfortunately we can only check frame complete for runtime stubs and nmethod
 87     // other generic buffer blobs are more problematic so we just assume they are
 88     // ok. adapter blobs never have a frame complete and are never ok.
 89 
 90     if (!_cb-&gt;is_frame_complete_at(_pc)) {
 91       if (_cb-&gt;is_compiled() || _cb-&gt;is_adapter_blob() || _cb-&gt;is_runtime_stub()) {
 92         return false;
 93       }
 94     }
 95 
 96     // Could just be some random pointer within the codeBlob
 97     if (!_cb-&gt;code_contains(_pc)) {
 98       return false;
 99     }
100 
101     // Entry frame checks
102     if (is_entry_frame()) {
103       // an entry frame must have a valid fp.
104       return fp_safe &amp;&amp; is_entry_frame_valid(thread);
105     }
106 
107     intptr_t* sender_sp = NULL;
108     intptr_t* sender_unextended_sp = NULL;
109     address   sender_pc = NULL;
110     intptr_t* saved_fp =  NULL;
111 
112     if (is_interpreted_frame()) {
113       // fp must be safe
114       if (!fp_safe) {
115         return false;
116       }
117 
118       sender_pc = (address) this-&gt;fp()[return_addr_offset];
119       // for interpreted frames, the value below is the sender &quot;raw&quot; sp,
120       // which can be different from the sender unextended sp (the sp seen
121       // by the sender) because of current frame local variables
122       sender_sp = (intptr_t*) addr_at(sender_sp_offset);
123       sender_unextended_sp = (intptr_t*) this-&gt;fp()[interpreter_frame_sender_sp_offset];
124       saved_fp = (intptr_t*) this-&gt;fp()[link_offset];
125 
126     } else {
127       // must be some sort of compiled/runtime frame
128       // fp does not have to be safe (although it could be check for c1?)
129 
130       // check for a valid frame_size, otherwise we are unlikely to get a valid sender_pc
131       if (_cb-&gt;frame_size() &lt;= 0) {
132         return false;
133       }
134 
135       sender_sp = _unextended_sp + _cb-&gt;frame_size();
136       // Is sender_sp safe?
137       if ((address)sender_sp &gt;= thread-&gt;stack_base()) {
138         return false;
139       }
140       sender_unextended_sp = sender_sp;
141       // On Intel the return_address is always the word on the stack
142       sender_pc = (address) *(sender_sp-1);
143       // Note: frame::sender_sp_offset is only valid for compiled frame
144       saved_fp = (intptr_t*) *(sender_sp - frame::sender_sp_offset);
145     }
146 
147 
148     // If the potential sender is the interpreter then we can do some more checking
149     if (Interpreter::contains(sender_pc)) {
150 
151       // ebp is always saved in a recognizable place in any code we generate. However
152       // only if the sender is interpreted/call_stub (c1 too?) are we certain that the saved ebp
153       // is really a frame pointer.
154 
155       bool saved_fp_safe = ((address)saved_fp &lt; thread-&gt;stack_base()) &amp;&amp; (saved_fp &gt; sender_sp);
156 
157       if (!saved_fp_safe) {
158         return false;
159       }
160 
161       // construct the potential sender
162 
163       frame sender(sender_sp, sender_unextended_sp, saved_fp, sender_pc);
164 
165       return sender.is_interpreted_frame_valid(thread);
166 
167     }
168 
169     // We must always be able to find a recognizable pc
170     CodeBlob* sender_blob = CodeCache::find_blob_unsafe(sender_pc);
171     if (sender_pc == NULL ||  sender_blob == NULL) {
172       return false;
173     }
174 
175     // Could be a zombie method
176     if (sender_blob-&gt;is_zombie() || sender_blob-&gt;is_unloaded()) {
177       return false;
178     }
179 
180     // Could just be some random pointer within the codeBlob
181     if (!sender_blob-&gt;code_contains(sender_pc)) {
182       return false;
183     }
184 
185     // We should never be able to see an adapter if the current frame is something from code cache
186     if (sender_blob-&gt;is_adapter_blob()) {
187       return false;
188     }
189 
190     // Could be the call_stub
191     if (StubRoutines::returns_to_call_stub(sender_pc)) {
192       bool saved_fp_safe = ((address)saved_fp &lt; thread-&gt;stack_base()) &amp;&amp; (saved_fp &gt; sender_sp);
193 
194       if (!saved_fp_safe) {
195         return false;
196       }
197 
198       // construct the potential sender
199 
200       frame sender(sender_sp, sender_unextended_sp, saved_fp, sender_pc);
201 
202       // Validate the JavaCallWrapper an entry frame must have
203       address jcw = (address)sender.entry_frame_call_wrapper();
204 
205       bool jcw_safe = (jcw &lt; thread-&gt;stack_base()) &amp;&amp; (jcw &gt; (address)sender.fp());
206 
207       return jcw_safe;
208     }
209 
210     CompiledMethod* nm = sender_blob-&gt;as_compiled_method_or_null();
211     if (nm != NULL) {
212         if (nm-&gt;is_deopt_mh_entry(sender_pc) || nm-&gt;is_deopt_entry(sender_pc) ||
213             nm-&gt;method()-&gt;is_method_handle_intrinsic()) {
214             return false;
215         }
216     }
217 
218     // If the frame size is 0 something (or less) is bad because every nmethod has a non-zero frame size
219     // because the return address counts against the callee&#39;s frame.
220 
221     if (sender_blob-&gt;frame_size() &lt;= 0) {
222       assert(!sender_blob-&gt;is_compiled(), &quot;should count return address at least&quot;);
223       return false;
224     }
225 
226     // We should never be able to see anything here except an nmethod. If something in the
227     // code cache (current frame) is called by an entity within the code cache that entity
228     // should not be anything but the call stub (already covered), the interpreter (already covered)
229     // or an nmethod.
230 
231     if (!sender_blob-&gt;is_compiled()) {
232         return false;
233     }
234 
235     // Could put some more validation for the potential non-interpreted sender
236     // frame we&#39;d create by calling sender if I could think of any. Wait for next crash in forte...
237 
238     // One idea is seeing if the sender_pc we have is one that we&#39;d expect to call to current cb
239 
240     // We&#39;ve validated the potential sender that would be created
241     return true;
242   }
243 
244   // Must be native-compiled frame. Since sender will try and use fp to find
245   // linkages it must be safe
246 
247   if (!fp_safe) {
248     return false;
249   }
250 
251   // Will the pc we fetch be non-zero (which we&#39;ll find at the oldest frame)
252 
253   if ( (address) this-&gt;fp()[return_addr_offset] == NULL) return false;
254 
255 
256   // could try and do some more potential verification of native frame if we could think of some...
257 
258   return true;
259 
260 }
261 
262 
263 void frame::patch_pc(Thread* thread, address pc) {
264   address* pc_addr = &amp;(((address*) sp())[-1]);
265   if (TracePcPatching) {
266     tty-&gt;print_cr(&quot;patch_pc at address &quot; INTPTR_FORMAT &quot; [&quot; INTPTR_FORMAT &quot; -&gt; &quot; INTPTR_FORMAT &quot;]&quot;,
267                   p2i(pc_addr), p2i(*pc_addr), p2i(pc));
268   }
269   // Either the return address is the original one or we are going to
270   // patch in the same address that&#39;s already there.
271   assert(_pc == *pc_addr || pc == *pc_addr, &quot;must be&quot;);
272   *pc_addr = pc;
273   _cb = CodeCache::find_blob(pc);
274   address original_pc = CompiledMethod::get_deopt_original_pc(this);
275   if (original_pc != NULL) {
276     assert(original_pc == _pc, &quot;expected original PC to be stored before patching&quot;);
277     _deopt_state = is_deoptimized;
278     // leave _pc as is
279   } else {
280     _deopt_state = not_deoptimized;
281     _pc = pc;
282   }
283 }
284 
285 bool frame::is_interpreted_frame() const  {
286   return Interpreter::contains(pc());
287 }
288 
289 int frame::frame_size(RegisterMap* map) const {
290   frame sender = this-&gt;sender(map);
291   return sender.sp() - sp();
292 }
293 
294 intptr_t* frame::entry_frame_argument_at(int offset) const {
295   // convert offset to index to deal with tsi
296   int index = (Interpreter::expr_offset_in_bytes(offset)/wordSize);
297   // Entry frame&#39;s arguments are always in relation to unextended_sp()
298   return &amp;unextended_sp()[index];
299 }
300 
301 // sender_sp
302 
303 intptr_t* frame::interpreter_frame_sender_sp() const {
304   assert(is_interpreted_frame(), &quot;interpreted frame expected&quot;);
305   return (intptr_t*) at(interpreter_frame_sender_sp_offset);
306 }
307 
308 void frame::set_interpreter_frame_sender_sp(intptr_t* sender_sp) {
309   assert(is_interpreted_frame(), &quot;interpreted frame expected&quot;);
310   ptr_at_put(interpreter_frame_sender_sp_offset, (intptr_t) sender_sp);
311 }
312 
313 
314 // monitor elements
315 
316 BasicObjectLock* frame::interpreter_frame_monitor_begin() const {
317   return (BasicObjectLock*) addr_at(interpreter_frame_monitor_block_bottom_offset);
318 }
319 
320 BasicObjectLock* frame::interpreter_frame_monitor_end() const {
321   BasicObjectLock* result = (BasicObjectLock*) *addr_at(interpreter_frame_monitor_block_top_offset);
322   // make sure the pointer points inside the frame
323   assert(sp() &lt;= (intptr_t*) result, &quot;monitor end should be above the stack pointer&quot;);
324   assert((intptr_t*) result &lt; fp(),  &quot;monitor end should be strictly below the frame pointer&quot;);
325   return result;
326 }
327 
328 void frame::interpreter_frame_set_monitor_end(BasicObjectLock* value) {
329   *((BasicObjectLock**)addr_at(interpreter_frame_monitor_block_top_offset)) = value;
330 }
331 
332 // Used by template based interpreter deoptimization
333 void frame::interpreter_frame_set_last_sp(intptr_t* sp) {
334     *((intptr_t**)addr_at(interpreter_frame_last_sp_offset)) = sp;
335 }
336 
337 frame frame::sender_for_entry_frame(RegisterMap* map) const {
338   assert(map != NULL, &quot;map must be set&quot;);
339   // Java frame called from C; skip all C frames and return top C
340   // frame of that chunk as the sender
341   JavaFrameAnchor* jfa = entry_frame_call_wrapper()-&gt;anchor();
342   assert(!entry_frame_is_first(), &quot;next Java fp must be non zero&quot;);
343   assert(jfa-&gt;last_Java_sp() &gt; sp(), &quot;must be above this frame on stack&quot;);
344   // Since we are walking the stack now this nested anchor is obviously walkable
345   // even if it wasn&#39;t when it was stacked.
346   if (!jfa-&gt;walkable()) {
347     // Capture _last_Java_pc (if needed) and mark anchor walkable.
348     jfa-&gt;capture_last_Java_pc();
349   }
350   map-&gt;clear();
351   assert(map-&gt;include_argument_oops(), &quot;should be set by clear&quot;);
352   vmassert(jfa-&gt;last_Java_pc() != NULL, &quot;not walkable&quot;);
353   frame fr(jfa-&gt;last_Java_sp(), jfa-&gt;last_Java_fp(), jfa-&gt;last_Java_pc());
354   return fr;
355 }
356 
357 //------------------------------------------------------------------------------
358 // frame::verify_deopt_original_pc
359 //
360 // Verifies the calculated original PC of a deoptimization PC for the
361 // given unextended SP.
362 #ifdef ASSERT
363 void frame::verify_deopt_original_pc(CompiledMethod* nm, intptr_t* unextended_sp) {
364   frame fr;
365 
366   // This is ugly but it&#39;s better than to change {get,set}_original_pc
367   // to take an SP value as argument.  And it&#39;s only a debugging
368   // method anyway.
369   fr._unextended_sp = unextended_sp;
370 
371   address original_pc = nm-&gt;get_original_pc(&amp;fr);
372   assert(nm-&gt;insts_contains_inclusive(original_pc),
373          &quot;original PC must be in the main code section of the the compiled method (or must be immediately following it)&quot;);
374 }
375 #endif
376 
377 //------------------------------------------------------------------------------
378 // frame::adjust_unextended_sp
379 #ifdef ASSERT
380 void frame::adjust_unextended_sp() {
381   // On x86, sites calling method handle intrinsics and lambda forms are treated
382   // as any other call site. Therefore, no special action is needed when we are
383   // returning to any of these call sites.
384 
385   if (_cb != NULL) {
386     CompiledMethod* sender_cm = _cb-&gt;as_compiled_method_or_null();
387     if (sender_cm != NULL) {
388       // If the sender PC is a deoptimization point, get the original PC.
389       if (sender_cm-&gt;is_deopt_entry(_pc) ||
390           sender_cm-&gt;is_deopt_mh_entry(_pc)) {
391         verify_deopt_original_pc(sender_cm, _unextended_sp);
392       }
393     }
394   }
395 }
396 #endif
397 
398 //------------------------------------------------------------------------------
399 // frame::update_map_with_saved_link
400 void frame::update_map_with_saved_link(RegisterMap* map, intptr_t** link_addr) {
401   // The interpreter and compiler(s) always save EBP/RBP in a known
402   // location on entry. We must record where that location is
403   // so this if EBP/RBP was live on callout from c2 we can find
404   // the saved copy no matter what it called.
405 
406   // Since the interpreter always saves EBP/RBP if we record where it is then
407   // we don&#39;t have to always save EBP/RBP on entry and exit to c2 compiled
408   // code, on entry will be enough.
409   map-&gt;set_location(rbp-&gt;as_VMReg(), (address) link_addr);
410 #ifdef AMD64
411   // this is weird &quot;H&quot; ought to be at a higher address however the
412   // oopMaps seems to have the &quot;H&quot; regs at the same address and the
413   // vanilla register.
414   // XXXX make this go away
415   if (true) {
416     map-&gt;set_location(rbp-&gt;as_VMReg()-&gt;next(), (address) link_addr);
417   }
418 #endif // AMD64
419 }
420 
421 
422 //------------------------------------------------------------------------------
423 // frame::sender_for_interpreter_frame
424 frame frame::sender_for_interpreter_frame(RegisterMap* map) const {
425   // SP is the raw SP from the sender after adapter or interpreter
426   // extension.
427   intptr_t* sender_sp = this-&gt;sender_sp();
428 
429   // This is the sp before any possible extension (adapter/locals).
430   intptr_t* unextended_sp = interpreter_frame_sender_sp();
431 
432 #if COMPILER2_OR_JVMCI
433   if (map-&gt;update_map()) {
434     update_map_with_saved_link(map, (intptr_t**) addr_at(link_offset));
435   }
436 #endif // COMPILER2_OR_JVMCI
437 
438   return frame(sender_sp, unextended_sp, link(), sender_pc());
439 }
440 
441 
442 //------------------------------------------------------------------------------
443 // frame::sender_for_compiled_frame
444 frame frame::sender_for_compiled_frame(RegisterMap* map) const {
445   assert(map != NULL, &quot;map must be set&quot;);
446 
447   // frame owned by optimizing compiler
448   assert(_cb-&gt;frame_size() &gt;= 0, &quot;must have non-zero frame size&quot;);
449   intptr_t* sender_sp = unextended_sp() + _cb-&gt;frame_size();
450   intptr_t* unextended_sp = sender_sp;
451 
452   // On Intel the return_address is always the word on the stack
453   address sender_pc = (address) *(sender_sp-1);
454 
455   // This is the saved value of EBP which may or may not really be an FP.
456   // It is only an FP if the sender is an interpreter frame (or C1?).
457   intptr_t** saved_fp_addr = (intptr_t**) (sender_sp - frame::sender_sp_offset);
458 
459   if (map-&gt;update_map()) {
460     // Tell GC to use argument oopmaps for some runtime stubs that need it.
461     // For C1, the runtime stub might not have oop maps, so set this flag
462     // outside of update_register_map.
463     map-&gt;set_include_argument_oops(_cb-&gt;caller_must_gc_arguments(map-&gt;thread()));
464     if (_cb-&gt;oop_maps() != NULL) {
465       OopMapSet::update_register_map(this, map);
466     }
467 
468     // Since the prolog does the save and restore of EBP there is no oopmap
469     // for it so we must fill in its location as if there was an oopmap entry
470     // since if our caller was compiled code there could be live jvm state in it.
471     update_map_with_saved_link(map, saved_fp_addr);
472   }
473 
474   assert(sender_sp != sp(), &quot;must have changed&quot;);
475   return frame(sender_sp, unextended_sp, *saved_fp_addr, sender_pc);
476 }
477 
478 
479 //------------------------------------------------------------------------------
480 // frame::sender
481 frame frame::sender(RegisterMap* map) const {
482   // Default is we done have to follow them. The sender_for_xxx will
483   // update it accordingly
484   map-&gt;set_include_argument_oops(false);
485 
486   if (is_entry_frame())       return sender_for_entry_frame(map);
487   if (is_interpreted_frame()) return sender_for_interpreter_frame(map);
488   assert(_cb == CodeCache::find_blob(pc()),&quot;Must be the same&quot;);
489 
490   if (_cb != NULL) {
491     return sender_for_compiled_frame(map);
492   }
493   // Must be native-compiled frame, i.e. the marshaling code for native
494   // methods that exists in the core system.
495   return frame(sender_sp(), link(), sender_pc());
496 }
497 
498 bool frame::is_interpreted_frame_valid(JavaThread* thread) const {
499   assert(is_interpreted_frame(), &quot;Not an interpreted frame&quot;);
500   // These are reasonable sanity checks
501   if (fp() == 0 || (intptr_t(fp()) &amp; (wordSize-1)) != 0) {
502     return false;
503   }
504   if (sp() == 0 || (intptr_t(sp()) &amp; (wordSize-1)) != 0) {
505     return false;
506   }
507   if (fp() + interpreter_frame_initial_sp_offset &lt; sp()) {
508     return false;
509   }
510   // These are hacks to keep us out of trouble.
511   // The problem with these is that they mask other problems
512   if (fp() &lt;= sp()) {        // this attempts to deal with unsigned comparison above
513     return false;
514   }
515 
516   // do some validation of frame elements
517   // first the method
518 
519   Method* m = *interpreter_frame_method_addr();
520 
521   // validate the method we&#39;d find in this potential sender
522   if (!Method::is_valid_method(m)) return false;
523 
524   // stack frames shouldn&#39;t be much larger than max_stack elements
525   // this test requires the use the unextended_sp which is the sp as seen by
526   // the current frame, and not sp which is the &quot;raw&quot; pc which could point
527   // further because of local variables of the callee method inserted after
528   // method arguments
529   if (fp() - unextended_sp() &gt; 1024 + m-&gt;max_stack()*Interpreter::stackElementSize) {
530     return false;
531   }
532 
533   // validate bci/bcp
534 
535   address bcp = interpreter_frame_bcp();
536   if (m-&gt;validate_bci_from_bcp(bcp) &lt; 0) {
537     return false;
538   }
539 
540   // validate ConstantPoolCache*
541   ConstantPoolCache* cp = *interpreter_frame_cache_addr();
542   if (MetaspaceObj::is_valid(cp) == false) return false;
543 
544   // validate locals
545 
546   address locals =  (address) *interpreter_frame_locals_addr();
547 
548   if (locals &gt;= thread-&gt;stack_base() || locals &lt; (address) fp()) return false;
549 
550   // We&#39;d have to be pretty unlucky to be mislead at this point
551   return true;
552 }
553 
554 BasicType frame::interpreter_frame_result(oop* oop_result, jvalue* value_result) {
555   assert(is_interpreted_frame(), &quot;interpreted frame expected&quot;);
556   Method* method = interpreter_frame_method();
557   BasicType type = method-&gt;result_type();
558 
559   intptr_t* tos_addr;
560   if (method-&gt;is_native()) {
561     // Prior to calling into the runtime to report the method_exit the possible
562     // return value is pushed to the native stack. If the result is a jfloat/jdouble
563     // then ST0 is saved before EAX/EDX. See the note in generate_native_result
564     tos_addr = (intptr_t*)sp();
565     if (type == T_FLOAT || type == T_DOUBLE) {
566     // QQQ seems like this code is equivalent on the two platforms
567 #ifdef AMD64
568       // This is times two because we do a push(ltos) after pushing XMM0
569       // and that takes two interpreter stack slots.
570       tos_addr += 2 * Interpreter::stackElementWords;
571 #else
572       tos_addr += 2;
573 #endif // AMD64
574     }
575   } else {
576     tos_addr = (intptr_t*)interpreter_frame_tos_address();
577   }
578 
579   switch (type) {
580     case T_OBJECT  :
581     case T_ARRAY   : {
582       oop obj;
583       if (method-&gt;is_native()) {
584         obj = cast_to_oop(at(interpreter_frame_oop_temp_offset));
585       } else {
586         oop* obj_p = (oop*)tos_addr;
587         obj = (obj_p == NULL) ? (oop)NULL : *obj_p;
588       }
589       assert(obj == NULL || Universe::heap()-&gt;is_in(obj), &quot;sanity check&quot;);
590       *oop_result = obj;
591       break;
592     }
593     case T_BOOLEAN : value_result-&gt;z = *(jboolean*)tos_addr; break;
594     case T_BYTE    : value_result-&gt;b = *(jbyte*)tos_addr; break;
595     case T_CHAR    : value_result-&gt;c = *(jchar*)tos_addr; break;
596     case T_SHORT   : value_result-&gt;s = *(jshort*)tos_addr; break;
597     case T_INT     : value_result-&gt;i = *(jint*)tos_addr; break;
598     case T_LONG    : value_result-&gt;j = *(jlong*)tos_addr; break;
599     case T_FLOAT   : {
600 #ifdef AMD64
601         value_result-&gt;f = *(jfloat*)tos_addr;
602 #else
603       if (method-&gt;is_native()) {
604         jdouble d = *(jdouble*)tos_addr;  // Result was in ST0 so need to convert to jfloat
605         value_result-&gt;f = (jfloat)d;
606       } else {
607         value_result-&gt;f = *(jfloat*)tos_addr;
608       }
609 #endif // AMD64
610       break;
611     }
612     case T_DOUBLE  : value_result-&gt;d = *(jdouble*)tos_addr; break;
613     case T_VOID    : /* Nothing to do */ break;
614     default        : ShouldNotReachHere();
615   }
616 
617   return type;
618 }
619 
620 
621 intptr_t* frame::interpreter_frame_tos_at(jint offset) const {
622   int index = (Interpreter::expr_offset_in_bytes(offset)/wordSize);
623   return &amp;interpreter_frame_tos_address()[index];
624 }
625 
626 #ifndef PRODUCT
627 
628 #define DESCRIBE_FP_OFFSET(name) \
629   values.describe(frame_no, fp() + frame::name##_offset, #name)
630 
631 void frame::describe_pd(FrameValues&amp; values, int frame_no) {
632   if (is_interpreted_frame()) {
633     DESCRIBE_FP_OFFSET(interpreter_frame_sender_sp);
634     DESCRIBE_FP_OFFSET(interpreter_frame_last_sp);
635     DESCRIBE_FP_OFFSET(interpreter_frame_method);
636     DESCRIBE_FP_OFFSET(interpreter_frame_mirror);
637     DESCRIBE_FP_OFFSET(interpreter_frame_mdp);
638     DESCRIBE_FP_OFFSET(interpreter_frame_cache);
639     DESCRIBE_FP_OFFSET(interpreter_frame_locals);
640     DESCRIBE_FP_OFFSET(interpreter_frame_bcp);
641     DESCRIBE_FP_OFFSET(interpreter_frame_initial_sp);
642 #ifdef AMD64
643   } else if (is_entry_frame()) {
644     // This could be more descriptive if we use the enum in
645     // stubGenerator to map to real names but it&#39;s most important to
646     // claim these frame slots so the error checking works.
647     for (int i = 0; i &lt; entry_frame_after_call_words; i++) {
648       values.describe(frame_no, fp() - i, err_msg(&quot;call_stub word fp - %d&quot;, i));
649     }
650 #endif // AMD64
651   }
652 }
653 #endif // !PRODUCT
654 
655 intptr_t *frame::initial_deoptimization_info() {
656   // used to reset the saved FP
657   return fp();
658 }
659 
660 intptr_t* frame::real_fp() const {
661   if (_cb != NULL) {
662     // use the frame size if valid
663     int size = _cb-&gt;frame_size();
664     if (size &gt; 0) {
665       return unextended_sp() + size;
666     }
667   }
668   // else rely on fp()
669   assert(! is_compiled_frame(), &quot;unknown compiled frame size&quot;);
670   return fp();
671 }
672 
673 #ifndef PRODUCT
674 // This is a generic constructor which is only used by pns() in debug.cpp.
675 frame::frame(void* sp, void* fp, void* pc) {
676   init((intptr_t*)sp, (intptr_t*)fp, (address)pc);
677 }
678 
679 void frame::pd_ps() {}
680 #endif
681 
682 void JavaFrameAnchor::make_walkable(JavaThread* thread) {
683   // last frame set?
684   if (last_Java_sp() == NULL) return;
685   // already walkable?
686   if (walkable()) return;
687   vmassert(Thread::current() == (Thread*)thread, &quot;not current thread&quot;);
688   vmassert(last_Java_sp() != NULL, &quot;not called from Java code?&quot;);
689   vmassert(last_Java_pc() == NULL, &quot;already walkable&quot;);
690   capture_last_Java_pc();
691   vmassert(walkable(), &quot;something went wrong&quot;);
692 }
693 
694 void JavaFrameAnchor::capture_last_Java_pc() {
695   vmassert(_last_Java_sp != NULL, &quot;no last frame set&quot;);
696   vmassert(_last_Java_pc == NULL, &quot;already walkable&quot;);
697   _last_Java_pc = (address)_last_Java_sp[-1];
698 }
    </pre>
  </body>
</html>