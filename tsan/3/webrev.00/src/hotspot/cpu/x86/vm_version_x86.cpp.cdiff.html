<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/cpu/x86/vm_version_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="vm_version_ext_x86.hpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="vm_version_x86.hpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/vm_version_x86.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 1,7 ***</span>
  /*
<span class="line-modified">!  * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
<span class="line-new-header">--- 1,7 ---</span>
  /*
<span class="line-modified">!  * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 30,12 ***</span>
  #include &quot;logging/logStream.hpp&quot;
  #include &quot;memory/resourceArea.hpp&quot;
  #include &quot;runtime/java.hpp&quot;
  #include &quot;runtime/os.hpp&quot;
  #include &quot;runtime/stubCodeGenerator.hpp&quot;
<span class="line-modified">! #include &quot;vm_version_x86.hpp&quot;</span>
  
  
  int VM_Version::_cpu;
  int VM_Version::_model;
  int VM_Version::_stepping;
  VM_Version::CpuidInfo VM_Version::_cpuid_info = { 0, };
<span class="line-new-header">--- 30,15 ---</span>
  #include &quot;logging/logStream.hpp&quot;
  #include &quot;memory/resourceArea.hpp&quot;
  #include &quot;runtime/java.hpp&quot;
  #include &quot;runtime/os.hpp&quot;
  #include &quot;runtime/stubCodeGenerator.hpp&quot;
<span class="line-modified">! #include &quot;runtime/vm_version.hpp&quot;</span>
<span class="line-added">+ #include &quot;utilities/powerOfTwo.hpp&quot;</span>
<span class="line-added">+ #include &quot;utilities/virtualizationSupport.hpp&quot;</span>
  
<span class="line-added">+ #include OS_HEADER_INLINE(os)</span>
  
  int VM_Version::_cpu;
  int VM_Version::_model;
  int VM_Version::_stepping;
  VM_Version::CpuidInfo VM_Version::_cpuid_info = { 0, };
</pre>
<hr />
<pre>
<span class="line-old-header">*** 363,26 ***</span>
      // Generate SEGV here (reference through NULL)
      // and check upper YMM/ZMM bits after it.
      //
      intx saved_useavx = UseAVX;
      intx saved_usesse = UseSSE;
<span class="line-removed">-     // check _cpuid_info.sef_cpuid7_ebx.bits.avx512f</span>
<span class="line-removed">-     __ lea(rsi, Address(rbp, in_bytes(VM_Version::sef_cpuid7_offset())));</span>
<span class="line-removed">-     __ movl(rax, 0x10000);</span>
<span class="line-removed">-     __ andl(rax, Address(rsi, 4)); // xcr0 bits sse | ymm</span>
<span class="line-removed">-     __ cmpl(rax, 0x10000);</span>
<span class="line-removed">-     __ jccb(Assembler::notEqual, legacy_setup); // jump if EVEX is not supported</span>
<span class="line-removed">-     // check _cpuid_info.xem_xcr0_eax.bits.opmask</span>
<span class="line-removed">-     // check _cpuid_info.xem_xcr0_eax.bits.zmm512</span>
<span class="line-removed">-     // check _cpuid_info.xem_xcr0_eax.bits.zmm32</span>
<span class="line-removed">-     __ movl(rax, 0xE0);</span>
<span class="line-removed">-     __ andl(rax, Address(rbp, in_bytes(VM_Version::xem_xcr0_offset()))); // xcr0 bits sse | ymm</span>
<span class="line-removed">-     __ cmpl(rax, 0xE0);</span>
<span class="line-removed">-     __ jccb(Assembler::notEqual, legacy_setup); // jump if EVEX is not supported</span>
  
      // If UseAVX is unitialized or is set by the user to include EVEX
      if (use_evex) {
        // EVEX setup: run in lowest evex mode
        VM_Version::set_evex_cpuFeatures(); // Enable temporary to pass asserts
        UseAVX = 3;
        UseSSE = 2;
  #ifdef _WINDOWS
<span class="line-new-header">--- 366,33 ---</span>
      // Generate SEGV here (reference through NULL)
      // and check upper YMM/ZMM bits after it.
      //
      intx saved_useavx = UseAVX;
      intx saved_usesse = UseSSE;
  
      // If UseAVX is unitialized or is set by the user to include EVEX
      if (use_evex) {
<span class="line-added">+       // check _cpuid_info.sef_cpuid7_ebx.bits.avx512f</span>
<span class="line-added">+       __ lea(rsi, Address(rbp, in_bytes(VM_Version::sef_cpuid7_offset())));</span>
<span class="line-added">+       __ movl(rax, 0x10000);</span>
<span class="line-added">+       __ andl(rax, Address(rsi, 4)); // xcr0 bits sse | ymm</span>
<span class="line-added">+       __ cmpl(rax, 0x10000);</span>
<span class="line-added">+       __ jccb(Assembler::notEqual, legacy_setup); // jump if EVEX is not supported</span>
<span class="line-added">+       // check _cpuid_info.xem_xcr0_eax.bits.opmask</span>
<span class="line-added">+       // check _cpuid_info.xem_xcr0_eax.bits.zmm512</span>
<span class="line-added">+       // check _cpuid_info.xem_xcr0_eax.bits.zmm32</span>
<span class="line-added">+       __ movl(rax, 0xE0);</span>
<span class="line-added">+       __ andl(rax, Address(rbp, in_bytes(VM_Version::xem_xcr0_offset()))); // xcr0 bits sse | ymm</span>
<span class="line-added">+       __ cmpl(rax, 0xE0);</span>
<span class="line-added">+       __ jccb(Assembler::notEqual, legacy_setup); // jump if EVEX is not supported</span>
<span class="line-added">+ </span>
<span class="line-added">+       if (FLAG_IS_DEFAULT(UseAVX)) {</span>
<span class="line-added">+         __ lea(rsi, Address(rbp, in_bytes(VM_Version::std_cpuid1_offset())));</span>
<span class="line-added">+         __ movl(rax, Address(rsi, 0));</span>
<span class="line-added">+         __ cmpl(rax, 0x50654);              // If it is Skylake</span>
<span class="line-added">+         __ jcc(Assembler::equal, legacy_setup);</span>
<span class="line-added">+       }</span>
        // EVEX setup: run in lowest evex mode
        VM_Version::set_evex_cpuFeatures(); // Enable temporary to pass asserts
        UseAVX = 3;
        UseSSE = 2;
  #ifdef _WINDOWS
</pre>
<hr />
<pre>
<span class="line-old-header">*** 447,26 ***</span>
      __ movl(rax, Address(rsi, 0));
  
      VM_Version::set_cpuinfo_cont_addr(__ pc());
      // Returns here after signal. Save xmm0 to check it later.
  
<span class="line-removed">-     // check _cpuid_info.sef_cpuid7_ebx.bits.avx512f</span>
<span class="line-removed">-     __ lea(rsi, Address(rbp, in_bytes(VM_Version::sef_cpuid7_offset())));</span>
<span class="line-removed">-     __ movl(rax, 0x10000);</span>
<span class="line-removed">-     __ andl(rax, Address(rsi, 4));</span>
<span class="line-removed">-     __ cmpl(rax, 0x10000);</span>
<span class="line-removed">-     __ jcc(Assembler::notEqual, legacy_save_restore);</span>
<span class="line-removed">-     // check _cpuid_info.xem_xcr0_eax.bits.opmask</span>
<span class="line-removed">-     // check _cpuid_info.xem_xcr0_eax.bits.zmm512</span>
<span class="line-removed">-     // check _cpuid_info.xem_xcr0_eax.bits.zmm32</span>
<span class="line-removed">-     __ movl(rax, 0xE0);</span>
<span class="line-removed">-     __ andl(rax, Address(rbp, in_bytes(VM_Version::xem_xcr0_offset()))); // xcr0 bits sse | ymm</span>
<span class="line-removed">-     __ cmpl(rax, 0xE0);</span>
<span class="line-removed">-     __ jcc(Assembler::notEqual, legacy_save_restore);</span>
<span class="line-removed">- </span>
      // If UseAVX is unitialized or is set by the user to include EVEX
      if (use_evex) {
        // EVEX check: run in lowest evex mode
        VM_Version::set_evex_cpuFeatures(); // Enable temporary to pass asserts
        UseAVX = 3;
        UseSSE = 2;
        __ lea(rsi, Address(rbp, in_bytes(VM_Version::zmm_save_offset())));
<span class="line-new-header">--- 457,32 ---</span>
      __ movl(rax, Address(rsi, 0));
  
      VM_Version::set_cpuinfo_cont_addr(__ pc());
      // Returns here after signal. Save xmm0 to check it later.
  
      // If UseAVX is unitialized or is set by the user to include EVEX
      if (use_evex) {
<span class="line-added">+       // check _cpuid_info.sef_cpuid7_ebx.bits.avx512f</span>
<span class="line-added">+       __ lea(rsi, Address(rbp, in_bytes(VM_Version::sef_cpuid7_offset())));</span>
<span class="line-added">+       __ movl(rax, 0x10000);</span>
<span class="line-added">+       __ andl(rax, Address(rsi, 4));</span>
<span class="line-added">+       __ cmpl(rax, 0x10000);</span>
<span class="line-added">+       __ jcc(Assembler::notEqual, legacy_save_restore);</span>
<span class="line-added">+       // check _cpuid_info.xem_xcr0_eax.bits.opmask</span>
<span class="line-added">+       // check _cpuid_info.xem_xcr0_eax.bits.zmm512</span>
<span class="line-added">+       // check _cpuid_info.xem_xcr0_eax.bits.zmm32</span>
<span class="line-added">+       __ movl(rax, 0xE0);</span>
<span class="line-added">+       __ andl(rax, Address(rbp, in_bytes(VM_Version::xem_xcr0_offset()))); // xcr0 bits sse | ymm</span>
<span class="line-added">+       __ cmpl(rax, 0xE0);</span>
<span class="line-added">+       __ jcc(Assembler::notEqual, legacy_save_restore);</span>
<span class="line-added">+ </span>
<span class="line-added">+       if (FLAG_IS_DEFAULT(UseAVX)) {</span>
<span class="line-added">+         __ lea(rsi, Address(rbp, in_bytes(VM_Version::std_cpuid1_offset())));</span>
<span class="line-added">+         __ movl(rax, Address(rsi, 0));</span>
<span class="line-added">+         __ cmpl(rax, 0x50654);              // If it is Skylake</span>
<span class="line-added">+         __ jcc(Assembler::equal, legacy_save_restore);</span>
<span class="line-added">+       }</span>
        // EVEX check: run in lowest evex mode
        VM_Version::set_evex_cpuFeatures(); // Enable temporary to pass asserts
        UseAVX = 3;
        UseSSE = 2;
        __ lea(rsi, Address(rbp, in_bytes(VM_Version::zmm_save_offset())));
</pre>
<hr />
<pre>
<span class="line-old-header">*** 605,10 ***</span>
<span class="line-new-header">--- 621,20 ---</span>
    guarantee(_cpuid_info.std_cpuid1_edx.bits.clflush != 0, &quot;clflush is not supported&quot;);
    // clflush_size is size in quadwords (8 bytes).
    guarantee(_cpuid_info.std_cpuid1_ebx.bits.clflush_size == 8, &quot;such clflush size is not supported&quot;);
  #endif
  
<span class="line-added">+ #ifdef _LP64</span>
<span class="line-added">+   // assigning this field effectively enables Unsafe.writebackMemory()</span>
<span class="line-added">+   // by initing UnsafeConstant.DATA_CACHE_LINE_FLUSH_SIZE to non-zero</span>
<span class="line-added">+   // that is only implemented on x86_64 and only if the OS plays ball</span>
<span class="line-added">+   if (os::supports_map_sync()) {</span>
<span class="line-added">+     // publish data cache line flush size to generic field, otherwise</span>
<span class="line-added">+     // let if default to zero thereby disabling writeback</span>
<span class="line-added">+     _data_cache_line_flush_size = _cpuid_info.std_cpuid1_ebx.bits.clflush_size * 8;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ #endif</span>
    // If the OS doesn&#39;t support SSE, we can&#39;t use this feature even if the HW does
    if (!os::supports_sse())
      _features &amp;= ~(CPU_SSE|CPU_SSE2|CPU_SSE3|CPU_SSSE3|CPU_SSE4A|CPU_SSE4_1|CPU_SSE4_2);
  
    if (UseSSE &lt; 4) {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 645,12 ***</span>
      } else {
        use_avx_limit = 0;
      }
    }
    if (FLAG_IS_DEFAULT(UseAVX)) {
<span class="line-modified">!     FLAG_SET_DEFAULT(UseAVX, use_avx_limit);</span>
<span class="line-modified">!   } else if (UseAVX &gt; use_avx_limit) {</span>
      warning(&quot;UseAVX=%d is not supported on this CPU, setting it to UseAVX=%d&quot;, (int) UseAVX, use_avx_limit);
      FLAG_SET_DEFAULT(UseAVX, use_avx_limit);
    } else if (UseAVX &lt; 0) {
      warning(&quot;UseAVX=%d is not valid, setting it to UseAVX=0&quot;, (int) UseAVX);
      FLAG_SET_DEFAULT(UseAVX, 0);
<span class="line-new-header">--- 671,18 ---</span>
      } else {
        use_avx_limit = 0;
      }
    }
    if (FLAG_IS_DEFAULT(UseAVX)) {
<span class="line-modified">!     // Don&#39;t use AVX-512 on older Skylakes unless explicitly requested.</span>
<span class="line-modified">!     if (use_avx_limit &gt; 2 &amp;&amp; is_intel_skylake() &amp;&amp; _stepping &lt; 5) {</span>
<span class="line-added">+       FLAG_SET_DEFAULT(UseAVX, 2);</span>
<span class="line-added">+     } else {</span>
<span class="line-added">+       FLAG_SET_DEFAULT(UseAVX, use_avx_limit);</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+   if (UseAVX &gt; use_avx_limit) {</span>
      warning(&quot;UseAVX=%d is not supported on this CPU, setting it to UseAVX=%d&quot;, (int) UseAVX, use_avx_limit);
      FLAG_SET_DEFAULT(UseAVX, use_avx_limit);
    } else if (UseAVX &lt; 0) {
      warning(&quot;UseAVX=%d is not valid, setting it to UseAVX=0&quot;, (int) UseAVX);
      FLAG_SET_DEFAULT(UseAVX, 0);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 661,12 ***</span>
      _features &amp;= ~CPU_AVX512DQ;
      _features &amp;= ~CPU_AVX512CD;
      _features &amp;= ~CPU_AVX512BW;
      _features &amp;= ~CPU_AVX512VL;
      _features &amp;= ~CPU_AVX512_VPOPCNTDQ;
<span class="line-modified">!     _features &amp;= ~CPU_VPCLMULQDQ;</span>
      _features &amp;= ~CPU_VAES;
    }
  
    if (UseAVX &lt; 2)
      _features &amp;= ~CPU_AVX2;
  
<span class="line-new-header">--- 693,14 ---</span>
      _features &amp;= ~CPU_AVX512DQ;
      _features &amp;= ~CPU_AVX512CD;
      _features &amp;= ~CPU_AVX512BW;
      _features &amp;= ~CPU_AVX512VL;
      _features &amp;= ~CPU_AVX512_VPOPCNTDQ;
<span class="line-modified">!     _features &amp;= ~CPU_AVX512_VPCLMULQDQ;</span>
      _features &amp;= ~CPU_VAES;
<span class="line-added">+     _features &amp;= ~CPU_VNNI;</span>
<span class="line-added">+     _features &amp;= ~CPU_VBMI2;</span>
    }
  
    if (UseAVX &lt; 2)
      _features &amp;= ~CPU_AVX2;
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 678,18 ***</span>
    if (logical_processors_per_package() == 1) {
      // HT processor could be installed on a system which doesn&#39;t support HT.
      _features &amp;= ~CPU_HT;
    }
  
<span class="line-modified">!   if( is_intel() ) { // Intel cpus specific settings</span>
      if (is_knights_family()) {
        _features &amp;= ~CPU_VZEROUPPER;
      }
    }
  
    char buf[256];
<span class="line-modified">!   jio_snprintf(buf, sizeof(buf), &quot;(%u cores per cpu, %u threads per core) family %d model %d stepping %d%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s&quot;,</span>
                 cores_per_cpu(), threads_per_core(),
                 cpu_family(), _model, _stepping,
                 (supports_cmov() ? &quot;, cmov&quot; : &quot;&quot;),
                 (supports_cmpxchg8() ? &quot;, cx8&quot; : &quot;&quot;),
                 (supports_fxsr() ? &quot;, fxsr&quot; : &quot;&quot;),
<span class="line-new-header">--- 712,18 ---</span>
    if (logical_processors_per_package() == 1) {
      // HT processor could be installed on a system which doesn&#39;t support HT.
      _features &amp;= ~CPU_HT;
    }
  
<span class="line-modified">!   if (is_intel()) { // Intel cpus specific settings</span>
      if (is_knights_family()) {
        _features &amp;= ~CPU_VZEROUPPER;
      }
    }
  
    char buf[256];
<span class="line-modified">!   jio_snprintf(buf, sizeof(buf), &quot;(%u cores per cpu, %u threads per core) family %d model %d stepping %d%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s&quot;,</span>
                 cores_per_cpu(), threads_per_core(),
                 cpu_family(), _model, _stepping,
                 (supports_cmov() ? &quot;, cmov&quot; : &quot;&quot;),
                 (supports_cmpxchg8() ? &quot;, cx8&quot; : &quot;&quot;),
                 (supports_fxsr() ? &quot;, fxsr&quot; : &quot;&quot;),
</pre>
<hr />
<pre>
<span class="line-old-header">*** 718,11 ***</span>
                 (supports_bmi1() ? &quot;, bmi1&quot; : &quot;&quot;),
                 (supports_bmi2() ? &quot;, bmi2&quot; : &quot;&quot;),
                 (supports_adx() ? &quot;, adx&quot; : &quot;&quot;),
                 (supports_evex() ? &quot;, evex&quot; : &quot;&quot;),
                 (supports_sha() ? &quot;, sha&quot; : &quot;&quot;),
<span class="line-modified">!                (supports_fma() ? &quot;, fma&quot; : &quot;&quot;));</span>
    _features_string = os::strdup(buf);
  
    // UseSSE is set to the smaller of what hardware supports and what
    // the command line requires.  I.e., you cannot set UseSSE to 2 on
    // older Pentiums which do not support it.
<span class="line-new-header">--- 752,14 ---</span>
                 (supports_bmi1() ? &quot;, bmi1&quot; : &quot;&quot;),
                 (supports_bmi2() ? &quot;, bmi2&quot; : &quot;&quot;),
                 (supports_adx() ? &quot;, adx&quot; : &quot;&quot;),
                 (supports_evex() ? &quot;, evex&quot; : &quot;&quot;),
                 (supports_sha() ? &quot;, sha&quot; : &quot;&quot;),
<span class="line-modified">!                (supports_fma() ? &quot;, fma&quot; : &quot;&quot;),</span>
<span class="line-added">+                (supports_vbmi2() ? &quot;, vbmi2&quot; : &quot;&quot;),</span>
<span class="line-added">+                (supports_vaes() ? &quot;, vaes&quot; : &quot;&quot;),</span>
<span class="line-added">+                (supports_vnni() ? &quot;, vnni&quot; : &quot;&quot;));</span>
    _features_string = os::strdup(buf);
  
    // UseSSE is set to the smaller of what hardware supports and what
    // the command line requires.  I.e., you cannot set UseSSE to 2 on
    // older Pentiums which do not support it.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 779,11 ***</span>
          if (UseAESCTRIntrinsics &amp;&amp; !FLAG_IS_DEFAULT(UseAESCTRIntrinsics)) {
            warning(&quot;AES-CTR intrinsics require UseAESIntrinsics flag to be enabled. Intrinsics will be disabled.&quot;);
            FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);
          }
        } else {
<span class="line-modified">!         if(supports_sse4_1()) {</span>
            if (FLAG_IS_DEFAULT(UseAESCTRIntrinsics)) {
              FLAG_SET_DEFAULT(UseAESCTRIntrinsics, true);
            }
          } else {
             // The AES-CTR intrinsic stubs require AES instruction support (of course)
<span class="line-new-header">--- 816,11 ---</span>
          if (UseAESCTRIntrinsics &amp;&amp; !FLAG_IS_DEFAULT(UseAESCTRIntrinsics)) {
            warning(&quot;AES-CTR intrinsics require UseAESIntrinsics flag to be enabled. Intrinsics will be disabled.&quot;);
            FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);
          }
        } else {
<span class="line-modified">!         if (supports_sse4_1()) {</span>
            if (FLAG_IS_DEFAULT(UseAESCTRIntrinsics)) {
              FLAG_SET_DEFAULT(UseAESCTRIntrinsics, true);
            }
          } else {
             // The AES-CTR intrinsic stubs require AES instruction support (of course)
</pre>
<hr />
<pre>
<span class="line-old-header">*** 899,15 ***</span>
    } else if (UseSHA256Intrinsics) {
      warning(&quot;Intrinsics for SHA-224 and SHA-256 crypto hash functions not available on this CPU.&quot;);
      FLAG_SET_DEFAULT(UseSHA256Intrinsics, false);
    }
  
    if (UseSHA &amp;&amp; supports_avx2() &amp;&amp; supports_bmi2()) {
      if (FLAG_IS_DEFAULT(UseSHA512Intrinsics)) {
        FLAG_SET_DEFAULT(UseSHA512Intrinsics, true);
      }
<span class="line-modified">!   } else if (UseSHA512Intrinsics) {</span>
      warning(&quot;Intrinsics for SHA-384 and SHA-512 crypto hash functions not available on this CPU.&quot;);
      FLAG_SET_DEFAULT(UseSHA512Intrinsics, false);
    }
  
    if (!(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics)) {
<span class="line-new-header">--- 936,19 ---</span>
    } else if (UseSHA256Intrinsics) {
      warning(&quot;Intrinsics for SHA-224 and SHA-256 crypto hash functions not available on this CPU.&quot;);
      FLAG_SET_DEFAULT(UseSHA256Intrinsics, false);
    }
  
<span class="line-added">+ #ifdef _LP64</span>
<span class="line-added">+   // These are only supported on 64-bit</span>
    if (UseSHA &amp;&amp; supports_avx2() &amp;&amp; supports_bmi2()) {
      if (FLAG_IS_DEFAULT(UseSHA512Intrinsics)) {
        FLAG_SET_DEFAULT(UseSHA512Intrinsics, true);
      }
<span class="line-modified">!   } else</span>
<span class="line-added">+ #endif</span>
<span class="line-added">+   if (UseSHA512Intrinsics) {</span>
      warning(&quot;Intrinsics for SHA-384 and SHA-512 crypto hash functions not available on this CPU.&quot;);
      FLAG_SET_DEFAULT(UseSHA512Intrinsics, false);
    }
  
    if (!(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics)) {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 995,11 ***</span>
      // 16 byte vectors (in XMM) are supported with SSE2+
      max_vector_size = 16;
    } else if (UseAVX == 1 || UseAVX == 2) {
      // 32 bytes vectors (in YMM) are only supported with AVX+
      max_vector_size = 32;
<span class="line-modified">!   } else if (UseAVX &gt; 2 ) {</span>
      // 64 bytes vectors (in ZMM) are only supported with AVX 3
      max_vector_size = 64;
    }
  
  #ifdef _LP64
<span class="line-new-header">--- 1036,11 ---</span>
      // 16 byte vectors (in XMM) are supported with SSE2+
      max_vector_size = 16;
    } else if (UseAVX == 1 || UseAVX == 2) {
      // 32 bytes vectors (in YMM) are only supported with AVX+
      max_vector_size = 32;
<span class="line-modified">!   } else if (UseAVX &gt; 2) {</span>
      // 64 bytes vectors (in ZMM) are only supported with AVX 3
      max_vector_size = 64;
    }
  
  #ifdef _LP64
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1041,10 ***</span>
<span class="line-new-header">--- 1082,17 ---</span>
        }
      }
    }
  #endif // COMPILER2 &amp;&amp; ASSERT
  
<span class="line-added">+   if (!FLAG_IS_DEFAULT(AVX3Threshold)) {</span>
<span class="line-added">+     if (!is_power_of_2(AVX3Threshold)) {</span>
<span class="line-added">+       warning(&quot;AVX3Threshold must be a power of 2&quot;);</span>
<span class="line-added">+       FLAG_SET_DEFAULT(AVX3Threshold, 4096);</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
  #ifdef _LP64
    if (FLAG_IS_DEFAULT(UseMultiplyToLenIntrinsic)) {
      UseMultiplyToLenIntrinsic = true;
    }
    if (FLAG_IS_DEFAULT(UseSquareToLenIntrinsic)) {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1159,42 ***</span>
      if (FLAG_IS_DEFAULT(AllocatePrefetchInstr) &amp;&amp; supports_3dnow_prefetch()) {
        FLAG_SET_DEFAULT(AllocatePrefetchInstr, 3);
      }
    }
  
<span class="line-modified">!   if( is_amd() ) { // AMD cpus specific settings</span>
<span class="line-modified">!     if( supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseAddressNop) ) {</span>
        // Use it on new AMD cpus starting from Opteron.
        UseAddressNop = true;
      }
<span class="line-modified">!     if( supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseNewLongLShift) ) {</span>
        // Use it on new AMD cpus starting from Opteron.
        UseNewLongLShift = true;
      }
<span class="line-modified">!     if( FLAG_IS_DEFAULT(UseXmmLoadAndClearUpper) ) {</span>
        if (supports_sse4a()) {
          UseXmmLoadAndClearUpper = true; // use movsd only on &#39;10h&#39; Opteron
        } else {
          UseXmmLoadAndClearUpper = false;
        }
      }
<span class="line-modified">!     if( FLAG_IS_DEFAULT(UseXmmRegToRegMoveAll) ) {</span>
<span class="line-modified">!       if( supports_sse4a() ) {</span>
          UseXmmRegToRegMoveAll = true; // use movaps, movapd only on &#39;10h&#39;
        } else {
          UseXmmRegToRegMoveAll = false;
        }
      }
<span class="line-modified">!     if( FLAG_IS_DEFAULT(UseXmmI2F) ) {</span>
<span class="line-modified">!       if( supports_sse4a() ) {</span>
          UseXmmI2F = true;
        } else {
          UseXmmI2F = false;
        }
      }
<span class="line-modified">!     if( FLAG_IS_DEFAULT(UseXmmI2D) ) {</span>
<span class="line-modified">!       if( supports_sse4a() ) {</span>
          UseXmmI2D = true;
        } else {
          UseXmmI2D = false;
        }
      }
<span class="line-new-header">--- 1207,42 ---</span>
      if (FLAG_IS_DEFAULT(AllocatePrefetchInstr) &amp;&amp; supports_3dnow_prefetch()) {
        FLAG_SET_DEFAULT(AllocatePrefetchInstr, 3);
      }
    }
  
<span class="line-modified">!   if (is_amd_family()) { // AMD cpus specific settings</span>
<span class="line-modified">!     if (supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseAddressNop)) {</span>
        // Use it on new AMD cpus starting from Opteron.
        UseAddressNop = true;
      }
<span class="line-modified">!     if (supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseNewLongLShift)) {</span>
        // Use it on new AMD cpus starting from Opteron.
        UseNewLongLShift = true;
      }
<span class="line-modified">!     if (FLAG_IS_DEFAULT(UseXmmLoadAndClearUpper)) {</span>
        if (supports_sse4a()) {
          UseXmmLoadAndClearUpper = true; // use movsd only on &#39;10h&#39; Opteron
        } else {
          UseXmmLoadAndClearUpper = false;
        }
      }
<span class="line-modified">!     if (FLAG_IS_DEFAULT(UseXmmRegToRegMoveAll)) {</span>
<span class="line-modified">!       if (supports_sse4a()) {</span>
          UseXmmRegToRegMoveAll = true; // use movaps, movapd only on &#39;10h&#39;
        } else {
          UseXmmRegToRegMoveAll = false;
        }
      }
<span class="line-modified">!     if (FLAG_IS_DEFAULT(UseXmmI2F)) {</span>
<span class="line-modified">!       if (supports_sse4a()) {</span>
          UseXmmI2F = true;
        } else {
          UseXmmI2F = false;
        }
      }
<span class="line-modified">!     if (FLAG_IS_DEFAULT(UseXmmI2D)) {</span>
<span class="line-modified">!       if (supports_sse4a()) {</span>
          UseXmmI2D = true;
        } else {
          UseXmmI2D = false;
        }
      }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1208,11 ***</span>
        }
        FLAG_SET_DEFAULT(UseSSE42Intrinsics, false);
      }
  
      // some defaults for AMD family 15h
<span class="line-modified">!     if ( cpu_family() == 0x15 ) {</span>
        // On family 15h processors default is no sw prefetch
        if (FLAG_IS_DEFAULT(AllocatePrefetchStyle)) {
          FLAG_SET_DEFAULT(AllocatePrefetchStyle, 0);
        }
        // Also, if some other prefetch style is specified, default instruction type is PREFETCHW
<span class="line-new-header">--- 1256,11 ---</span>
        }
        FLAG_SET_DEFAULT(UseSSE42Intrinsics, false);
      }
  
      // some defaults for AMD family 15h
<span class="line-modified">!     if (cpu_family() == 0x15) {</span>
        // On family 15h processors default is no sw prefetch
        if (FLAG_IS_DEFAULT(AllocatePrefetchStyle)) {
          FLAG_SET_DEFAULT(AllocatePrefetchStyle, 0);
        }
        // Also, if some other prefetch style is specified, default instruction type is PREFETCHW
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1233,12 ***</span>
        // Limit vectors size to 16 bytes on AMD cpus &lt; 17h.
        FLAG_SET_DEFAULT(MaxVectorSize, 16);
      }
  #endif // COMPILER2
  
<span class="line-modified">!     // Some defaults for AMD family 17h</span>
<span class="line-modified">!     if ( cpu_family() == 0x17 ) {</span>
        // On family 17h processors use XMM and UnalignedLoadStores for Array Copy
        if (supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseXMMForArrayCopy)) {
          FLAG_SET_DEFAULT(UseXMMForArrayCopy, true);
        }
        if (supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseUnalignedLoadStores)) {
<span class="line-new-header">--- 1281,12 ---</span>
        // Limit vectors size to 16 bytes on AMD cpus &lt; 17h.
        FLAG_SET_DEFAULT(MaxVectorSize, 16);
      }
  #endif // COMPILER2
  
<span class="line-modified">!     // Some defaults for AMD family 17h || Hygon family 18h</span>
<span class="line-modified">!     if (cpu_family() == 0x17 || cpu_family() == 0x18) {</span>
        // On family 17h processors use XMM and UnalignedLoadStores for Array Copy
        if (supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseXMMForArrayCopy)) {
          FLAG_SET_DEFAULT(UseXMMForArrayCopy, true);
        }
        if (supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseUnalignedLoadStores)) {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1250,33 ***</span>
        }
  #endif
      }
    }
  
<span class="line-modified">!   if( is_intel() ) { // Intel cpus specific settings</span>
<span class="line-modified">!     if( FLAG_IS_DEFAULT(UseStoreImmI16) ) {</span>
        UseStoreImmI16 = false; // don&#39;t use it on Intel cpus
      }
<span class="line-modified">!     if( cpu_family() == 6 || cpu_family() == 15 ) {</span>
<span class="line-modified">!       if( FLAG_IS_DEFAULT(UseAddressNop) ) {</span>
          // Use it on all Intel cpus starting from PentiumPro
          UseAddressNop = true;
        }
      }
<span class="line-modified">!     if( FLAG_IS_DEFAULT(UseXmmLoadAndClearUpper) ) {</span>
        UseXmmLoadAndClearUpper = true; // use movsd on all Intel cpus
      }
<span class="line-modified">!     if( FLAG_IS_DEFAULT(UseXmmRegToRegMoveAll) ) {</span>
<span class="line-modified">!       if( supports_sse3() ) {</span>
          UseXmmRegToRegMoveAll = true; // use movaps, movapd on new Intel cpus
        } else {
          UseXmmRegToRegMoveAll = false;
        }
      }
<span class="line-modified">!     if( cpu_family() == 6 &amp;&amp; supports_sse3() ) { // New Intel cpus</span>
  #ifdef COMPILER2
<span class="line-modified">!       if( FLAG_IS_DEFAULT(MaxLoopPad) ) {</span>
          // For new Intel cpus do the next optimization:
          // don&#39;t align the beginning of a loop if there are enough instructions
          // left (NumberOfLoopInstrToAlign defined in c2_globals.hpp)
          // in current fetch line (OptoLoopAlignment) or the padding
          // is big (&gt; MaxLoopPad).
<span class="line-new-header">--- 1298,33 ---</span>
        }
  #endif
      }
    }
  
<span class="line-modified">!   if (is_intel()) { // Intel cpus specific settings</span>
<span class="line-modified">!     if (FLAG_IS_DEFAULT(UseStoreImmI16)) {</span>
        UseStoreImmI16 = false; // don&#39;t use it on Intel cpus
      }
<span class="line-modified">!     if (cpu_family() == 6 || cpu_family() == 15) {</span>
<span class="line-modified">!       if (FLAG_IS_DEFAULT(UseAddressNop)) {</span>
          // Use it on all Intel cpus starting from PentiumPro
          UseAddressNop = true;
        }
      }
<span class="line-modified">!     if (FLAG_IS_DEFAULT(UseXmmLoadAndClearUpper)) {</span>
        UseXmmLoadAndClearUpper = true; // use movsd on all Intel cpus
      }
<span class="line-modified">!     if (FLAG_IS_DEFAULT(UseXmmRegToRegMoveAll)) {</span>
<span class="line-modified">!       if (supports_sse3()) {</span>
          UseXmmRegToRegMoveAll = true; // use movaps, movapd on new Intel cpus
        } else {
          UseXmmRegToRegMoveAll = false;
        }
      }
<span class="line-modified">!     if (cpu_family() == 6 &amp;&amp; supports_sse3()) { // New Intel cpus</span>
  #ifdef COMPILER2
<span class="line-modified">!       if (FLAG_IS_DEFAULT(MaxLoopPad)) {</span>
          // For new Intel cpus do the next optimization:
          // don&#39;t align the beginning of a loop if there are enough instructions
          // left (NumberOfLoopInstrToAlign defined in c2_globals.hpp)
          // in current fetch line (OptoLoopAlignment) or the padding
          // is big (&gt; MaxLoopPad).
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1318,11 ***</span>
        }
        if (FLAG_IS_DEFAULT(UseIncDec)) {
          FLAG_SET_DEFAULT(UseIncDec, false);
        }
      }
<span class="line-modified">!     if(FLAG_IS_DEFAULT(AllocatePrefetchInstr) &amp;&amp; supports_3dnow_prefetch()) {</span>
        FLAG_SET_DEFAULT(AllocatePrefetchInstr, 3);
      }
    }
  
  #ifdef _LP64
<span class="line-new-header">--- 1366,11 ---</span>
        }
        if (FLAG_IS_DEFAULT(UseIncDec)) {
          FLAG_SET_DEFAULT(UseIncDec, false);
        }
      }
<span class="line-modified">!     if (FLAG_IS_DEFAULT(AllocatePrefetchInstr) &amp;&amp; supports_3dnow_prefetch()) {</span>
        FLAG_SET_DEFAULT(AllocatePrefetchInstr, 3);
      }
    }
  
  #ifdef _LP64
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1567,10 ***</span>
<span class="line-new-header">--- 1615,70 ---</span>
      }
    }
  #endif // !PRODUCT
  }
  
<span class="line-added">+ void VM_Version::print_platform_virtualization_info(outputStream* st) {</span>
<span class="line-added">+   VirtualizationType vrt = VM_Version::get_detected_virtualization();</span>
<span class="line-added">+   if (vrt == XenHVM) {</span>
<span class="line-added">+     st-&gt;print_cr(&quot;Xen hardware-assisted virtualization detected&quot;);</span>
<span class="line-added">+   } else if (vrt == KVM) {</span>
<span class="line-added">+     st-&gt;print_cr(&quot;KVM virtualization detected&quot;);</span>
<span class="line-added">+   } else if (vrt == VMWare) {</span>
<span class="line-added">+     st-&gt;print_cr(&quot;VMWare virtualization detected&quot;);</span>
<span class="line-added">+     VirtualizationSupport::print_virtualization_info(st);</span>
<span class="line-added">+   } else if (vrt == HyperV) {</span>
<span class="line-added">+     st-&gt;print_cr(&quot;HyperV virtualization detected&quot;);</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ void VM_Version::check_virt_cpuid(uint32_t idx, uint32_t *regs) {</span>
<span class="line-added">+ // TODO support 32 bit</span>
<span class="line-added">+ #if defined(_LP64)</span>
<span class="line-added">+ #if defined(_MSC_VER)</span>
<span class="line-added">+   // Allocate space for the code</span>
<span class="line-added">+   const int code_size = 100;</span>
<span class="line-added">+   ResourceMark rm;</span>
<span class="line-added">+   CodeBuffer cb(&quot;detect_virt&quot;, code_size, 0);</span>
<span class="line-added">+   MacroAssembler* a = new MacroAssembler(&amp;cb);</span>
<span class="line-added">+   address code = a-&gt;pc();</span>
<span class="line-added">+   void (*test)(uint32_t idx, uint32_t *regs) = (void(*)(uint32_t idx, uint32_t *regs))code;</span>
<span class="line-added">+ </span>
<span class="line-added">+   a-&gt;movq(r9, rbx); // save nonvolatile register</span>
<span class="line-added">+ </span>
<span class="line-added">+   // next line would not work on 32-bit</span>
<span class="line-added">+   a-&gt;movq(rax, c_rarg0 /* rcx */);</span>
<span class="line-added">+   a-&gt;movq(r8, c_rarg1 /* rdx */);</span>
<span class="line-added">+   a-&gt;cpuid();</span>
<span class="line-added">+   a-&gt;movl(Address(r8,  0), rax);</span>
<span class="line-added">+   a-&gt;movl(Address(r8,  4), rbx);</span>
<span class="line-added">+   a-&gt;movl(Address(r8,  8), rcx);</span>
<span class="line-added">+   a-&gt;movl(Address(r8, 12), rdx);</span>
<span class="line-added">+ </span>
<span class="line-added">+   a-&gt;movq(rbx, r9); // restore nonvolatile register</span>
<span class="line-added">+   a-&gt;ret(0);</span>
<span class="line-added">+ </span>
<span class="line-added">+   uint32_t *code_end = (uint32_t *)a-&gt;pc();</span>
<span class="line-added">+   a-&gt;flush();</span>
<span class="line-added">+ </span>
<span class="line-added">+   // execute code</span>
<span class="line-added">+   (*test)(idx, regs);</span>
<span class="line-added">+ #elif defined(__GNUC__)</span>
<span class="line-added">+   __asm__ volatile (</span>
<span class="line-added">+      &quot;        cpuid;&quot;</span>
<span class="line-added">+      &quot;        mov %%eax,(%1);&quot;</span>
<span class="line-added">+      &quot;        mov %%ebx,4(%1);&quot;</span>
<span class="line-added">+      &quot;        mov %%ecx,8(%1);&quot;</span>
<span class="line-added">+      &quot;        mov %%edx,12(%1);&quot;</span>
<span class="line-added">+      : &quot;+a&quot; (idx)</span>
<span class="line-added">+      : &quot;S&quot; (regs)</span>
<span class="line-added">+      : &quot;ebx&quot;, &quot;ecx&quot;, &quot;edx&quot;, &quot;memory&quot; );</span>
<span class="line-added">+ #endif</span>
<span class="line-added">+ #endif</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ </span>
  bool VM_Version::use_biased_locking() {
  #if INCLUDE_RTM_OPT
    // RTM locking is most useful when there is high lock contention and
    // low data contention.  With high lock contention the lock is usually
    // inflated and biased locking is not suitable for that case.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1588,10 ***</span>
<span class="line-new-header">--- 1696,60 ---</span>
    }
  #endif
    return UseBiasedLocking;
  }
  
<span class="line-added">+ // On Xen, the cpuid instruction returns</span>
<span class="line-added">+ //  eax / registers[0]: Version of Xen</span>
<span class="line-added">+ //  ebx / registers[1]: chars &#39;XenV&#39;</span>
<span class="line-added">+ //  ecx / registers[2]: chars &#39;MMXe&#39;</span>
<span class="line-added">+ //  edx / registers[3]: chars &#39;nVMM&#39;</span>
<span class="line-added">+ //</span>
<span class="line-added">+ // On KVM / VMWare / MS Hyper-V, the cpuid instruction returns</span>
<span class="line-added">+ //  ebx / registers[1]: chars &#39;KVMK&#39; / &#39;VMwa&#39; / &#39;Micr&#39;</span>
<span class="line-added">+ //  ecx / registers[2]: chars &#39;VMKV&#39; / &#39;reVM&#39; / &#39;osof&#39;</span>
<span class="line-added">+ //  edx / registers[3]: chars &#39;M&#39;    / &#39;ware&#39; / &#39;t Hv&#39;</span>
<span class="line-added">+ //</span>
<span class="line-added">+ // more information :</span>
<span class="line-added">+ // https://kb.vmware.com/s/article/1009458</span>
<span class="line-added">+ //</span>
<span class="line-added">+ void VM_Version::check_virtualizations() {</span>
<span class="line-added">+ #if defined(_LP64)</span>
<span class="line-added">+   uint32_t registers[4];</span>
<span class="line-added">+   char signature[13];</span>
<span class="line-added">+   uint32_t base;</span>
<span class="line-added">+   signature[12] = &#39;\0&#39;;</span>
<span class="line-added">+   memset((void*)registers, 0, 4*sizeof(uint32_t));</span>
<span class="line-added">+ </span>
<span class="line-added">+   for (base = 0x40000000; base &lt; 0x40010000; base += 0x100) {</span>
<span class="line-added">+     check_virt_cpuid(base, registers);</span>
<span class="line-added">+ </span>
<span class="line-added">+     *(uint32_t *)(signature + 0) = registers[1];</span>
<span class="line-added">+     *(uint32_t *)(signature + 4) = registers[2];</span>
<span class="line-added">+     *(uint32_t *)(signature + 8) = registers[3];</span>
<span class="line-added">+ </span>
<span class="line-added">+     if (strncmp(&quot;VMwareVMware&quot;, signature, 12) == 0) {</span>
<span class="line-added">+       Abstract_VM_Version::_detected_virtualization = VMWare;</span>
<span class="line-added">+       // check for extended metrics from guestlib</span>
<span class="line-added">+       VirtualizationSupport::initialize();</span>
<span class="line-added">+     }</span>
<span class="line-added">+ </span>
<span class="line-added">+     if (strncmp(&quot;Microsoft Hv&quot;, signature, 12) == 0) {</span>
<span class="line-added">+       Abstract_VM_Version::_detected_virtualization = HyperV;</span>
<span class="line-added">+     }</span>
<span class="line-added">+ </span>
<span class="line-added">+     if (strncmp(&quot;KVMKVMKVM&quot;, signature, 9) == 0) {</span>
<span class="line-added">+       Abstract_VM_Version::_detected_virtualization = KVM;</span>
<span class="line-added">+     }</span>
<span class="line-added">+ </span>
<span class="line-added">+     if (strncmp(&quot;XenVMMXenVMM&quot;, signature, 12) == 0) {</span>
<span class="line-added">+       Abstract_VM_Version::_detected_virtualization = XenHVM;</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ #endif</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
  void VM_Version::initialize() {
    ResourceMark rm;
    // Making this stub must be FIRST use of assembler
  
    stub_blob = BufferBlob::create(&quot;get_cpu_info_stub&quot;, stub_size);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1602,6 ***</span>
<span class="line-new-header">--- 1760,9 ---</span>
    VM_Version_StubGenerator g(&amp;c);
    get_cpu_info_stub = CAST_TO_FN_PTR(get_cpu_info_stub_t,
                                       g.generate_get_cpu_info());
  
    get_processor_features();
<span class="line-added">+   if (cpu_family() &gt; 4) { // it supports CPUID</span>
<span class="line-added">+     check_virtualizations();</span>
<span class="line-added">+   }</span>
  }
</pre>
<center><a href="vm_version_ext_x86.hpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="vm_version_x86.hpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>