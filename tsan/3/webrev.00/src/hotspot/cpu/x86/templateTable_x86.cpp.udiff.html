<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Udiff src/hotspot/cpu/x86/templateTable_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="templateInterpreterGenerator_x86_64.cpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="vm_version_ext_x86.cpp.udiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/templateTable_x86.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-new-header">@@ -2843,37 +2843,53 @@</span>
    // Helper function to insert a is-volatile test and memory barrier
    __ membar(order_constraint);
  }
  
  void TemplateTable::resolve_cache_and_index(int byte_no,
<span class="udiff-line-modified-removed">-                                             Register Rcache,</span>
<span class="udiff-line-modified-added">+                                             Register cache,</span>
                                              Register index,
                                              size_t index_size) {
    const Register temp = rbx;
<span class="udiff-line-modified-removed">-   assert_different_registers(Rcache, index, temp);</span>
<span class="udiff-line-modified-added">+   assert_different_registers(cache, index, temp);</span>
  
<span class="udiff-line-added">+   Label L_clinit_barrier_slow;</span>
    Label resolved;
  
    Bytecodes::Code code = bytecode();
    switch (code) {
    case Bytecodes::_nofast_getfield: code = Bytecodes::_getfield; break;
    case Bytecodes::_nofast_putfield: code = Bytecodes::_putfield; break;
    default: break;
    }
  
    assert(byte_no == f1_byte || byte_no == f2_byte, &quot;byte_no out of range&quot;);
<span class="udiff-line-modified-removed">-   __ get_cache_and_index_and_bytecode_at_bcp(Rcache, index, temp, byte_no, 1, index_size);</span>
<span class="udiff-line-modified-added">+   __ get_cache_and_index_and_bytecode_at_bcp(cache, index, temp, byte_no, 1, index_size);</span>
    __ cmpl(temp, code);  // have we resolved this bytecode?
    __ jcc(Assembler::equal, resolved);
  
    // resolve first time through
<span class="udiff-line-added">+   // Class initialization barrier slow path lands here as well.</span>
<span class="udiff-line-added">+   __ bind(L_clinit_barrier_slow);</span>
    address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache);
    __ movl(temp, code);
    __ call_VM(noreg, entry, temp);
    // Update registers with resolved info
<span class="udiff-line-modified-removed">-   __ get_cache_and_index_at_bcp(Rcache, index, 1, index_size);</span>
<span class="udiff-line-modified-added">+   __ get_cache_and_index_at_bcp(cache, index, 1, index_size);</span>
<span class="udiff-line-added">+ </span>
    __ bind(resolved);
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   // Class initialization barrier for static methods</span>
<span class="udiff-line-added">+   if (VM_Version::supports_fast_class_init_checks() &amp;&amp; bytecode() == Bytecodes::_invokestatic) {</span>
<span class="udiff-line-added">+     const Register method = temp;</span>
<span class="udiff-line-added">+     const Register klass  = temp;</span>
<span class="udiff-line-added">+     const Register thread = LP64_ONLY(r15_thread) NOT_LP64(noreg);</span>
<span class="udiff-line-added">+     assert(thread != noreg, &quot;x86_32 not supported&quot;);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     __ load_resolved_method_at_index(byte_no, method, cache, index);</span>
<span class="udiff-line-added">+     __ load_method_holder(klass, method);</span>
<span class="udiff-line-added">+     __ clinit_barrier(klass, thread, NULL /*L_fast_path*/, &amp;L_clinit_barrier_slow);</span>
<span class="udiff-line-added">+   }</span>
  }
  
  // The cache and index registers must be set before call
  void TemplateTable::load_field_cp_cache_entry(Register obj,
                                                Register cache,
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2938,24 +2954,19 @@</span>
    assert_different_registers(method, cache, index);
    assert_different_registers(itable_index, flags);
    assert_different_registers(itable_index, cache, index);
    // determine constant pool cache field offsets
    assert(is_invokevirtual == (byte_no == f2_byte), &quot;is_invokevirtual flag redundant&quot;);
<span class="udiff-line-removed">-   const int method_offset = in_bytes(</span>
<span class="udiff-line-removed">-     ConstantPoolCache::base_offset() +</span>
<span class="udiff-line-removed">-       ((byte_no == f2_byte)</span>
<span class="udiff-line-removed">-        ? ConstantPoolCacheEntry::f2_offset()</span>
<span class="udiff-line-removed">-        : ConstantPoolCacheEntry::f1_offset()));</span>
    const int flags_offset = in_bytes(ConstantPoolCache::base_offset() +
                                      ConstantPoolCacheEntry::flags_offset());
    // access constant pool cache fields
    const int index_offset = in_bytes(ConstantPoolCache::base_offset() +
                                      ConstantPoolCacheEntry::f2_offset());
  
    size_t index_size = (is_invokedynamic ? sizeof(u4) : sizeof(u2));
    resolve_cache_and_index(byte_no, cache, index, index_size);
<span class="udiff-line-modified-removed">-     __ movptr(method, Address(cache, index, Address::times_ptr, method_offset));</span>
<span class="udiff-line-modified-added">+   __ load_resolved_method_at_index(byte_no, method, cache, index);</span>
  
    if (itable_index != noreg) {
      // pick up itable or appendix index from f2 also:
      __ movptr(itable_index, Address(cache, index, Address::times_ptr, index_offset));
    }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3153,11 +3164,12 @@</span>
    Label notDouble;
    __ cmpl(flags, dtos);
    __ jcc(Assembler::notEqual, notDouble);
  #endif
    // dtos
<span class="udiff-line-modified-removed">-   __ access_load_at(T_DOUBLE, IN_HEAP, noreg /* dtos */, field, noreg, noreg);</span>
<span class="udiff-line-modified-added">+   // MO_RELAXED: for the case of volatile field, in fact it adds no extra work for the underlying implementation</span>
<span class="udiff-line-added">+   __ access_load_at(T_DOUBLE, IN_HEAP | MO_RELAXED, noreg /* dtos */, field, noreg, noreg);</span>
    __ push(dtos);
    TSAN_RUNTIME_ONLY(tsan_observe_get_or_put(
        field, rdx, SharedRuntime::tsan_read8, dtos));
    // Rewrite bytecode to be faster
    if (!is_static &amp;&amp; rc == may_rewrite) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3438,11 +3450,12 @@</span>
    {
      __ pop(ltos);
      if (!is_static) pop_and_check_object(obj);
      TSAN_RUNTIME_ONLY(tsan_observe_get_or_put(
          field, rdx, SharedRuntime::tsan_write8, ltos));
<span class="udiff-line-modified-removed">-     __ access_store_at(T_LONG, IN_HEAP, field, noreg /* ltos*/, noreg, noreg);</span>
<span class="udiff-line-modified-added">+     // MO_RELAXED: generate atomic store for the case of volatile field (important for x86_32)</span>
<span class="udiff-line-added">+     __ access_store_at(T_LONG, IN_HEAP | MO_RELAXED, field, noreg /* ltos*/, noreg, noreg);</span>
  #ifdef _LP64
      if (!is_static &amp;&amp; rc == may_rewrite) {
        patch_bytecode(Bytecodes::_fast_lputfield, bc, rbx, true, byte_no);
      }
  #endif // _LP64
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3477,11 +3490,12 @@</span>
    {
      __ pop(dtos);
      if (!is_static) pop_and_check_object(obj);
      TSAN_RUNTIME_ONLY(tsan_observe_get_or_put(
          field, rdx, SharedRuntime::tsan_write8, dtos));
<span class="udiff-line-modified-removed">-     __ access_store_at(T_DOUBLE, IN_HEAP, field, noreg /* dtos */, noreg, noreg);</span>
<span class="udiff-line-modified-added">+     // MO_RELAXED: for the case of volatile field, in fact it adds no extra work for the underlying implementation</span>
<span class="udiff-line-added">+     __ access_store_at(T_DOUBLE, IN_HEAP | MO_RELAXED, field, noreg /* dtos */, noreg, noreg);</span>
      if (!is_static &amp;&amp; rc == may_rewrite) {
        patch_bytecode(Bytecodes::_fast_dputfield, bc, rbx, true, byte_no);
      }
    }
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -4046,13 +4060,11 @@</span>
    // profile this call
    __ restore_bcp(); // rbcp was destroyed by receiver type check
    __ profile_virtual_call(rdx, rbcp, rlocals);
  
    // Get declaring interface class from method, and itable index
<span class="udiff-line-modified-removed">-   __ movptr(rax, Address(rbx, Method::const_offset()));</span>
<span class="udiff-line-removed">-   __ movptr(rax, Address(rax, ConstMethod::constants_offset()));</span>
<span class="udiff-line-removed">-   __ movptr(rax, Address(rax, ConstantPool::pool_holder_offset_in_bytes()));</span>
<span class="udiff-line-modified-added">+   __ load_method_holder(rax, rbx);</span>
    __ movl(rbx, Address(rbx, Method::itable_index_offset()));
    __ subl(rbx, Method::itable_index_max);
    __ negl(rbx);
  
    // Preserve recvKlass for throw_AbstractMethodErrorVerbose.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -4187,11 +4199,11 @@</span>
    const int tags_offset = Array&lt;u1&gt;::base_offset_in_bytes();
    __ cmpb(Address(rax, rdx, Address::times_1, tags_offset), JVM_CONSTANT_Class);
    __ jcc(Assembler::notEqual, slow_case_no_pop);
  
    // get InstanceKlass
<span class="udiff-line-modified-removed">-   __ load_resolved_klass_at_index(rcx, rdx, rcx);</span>
<span class="udiff-line-modified-added">+   __ load_resolved_klass_at_index(rcx, rcx, rdx);</span>
    __ push(rcx);  // save the contexts of klass for initializing the header
  
    // make sure klass is initialized &amp; doesn&#39;t have finalizer
    // make sure klass is fully initialized
    __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -4283,11 +4295,11 @@</span>
        __ pop(rcx);   // get saved klass back in the register.
        __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));
        __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()), rbx);
      } else {
        __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()),
<span class="udiff-line-modified-removed">-                 (intptr_t)markOopDesc::prototype()); // header</span>
<span class="udiff-line-modified-added">+                 (intptr_t)markWord::prototype().value()); // header</span>
        __ pop(rcx);   // get saved klass back in the register.
      }
  #ifdef _LP64
      __ xorl(rsi, rsi); // use zero reg to clear memory (shorter code)
      __ store_klass_gap(rax, rsi);  // zero klass gap for compressed oops
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -4389,11 +4401,11 @@</span>
    __ jmpb(resolved);
  
    // Get superklass in rax and subklass in rbx
    __ bind(quicked);
    __ mov(rdx, rax); // Save object in rdx; rax needed for subtype check
<span class="udiff-line-modified-removed">-   __ load_resolved_klass_at_index(rcx, rbx, rax);</span>
<span class="udiff-line-modified-added">+   __ load_resolved_klass_at_index(rax, rcx, rbx);</span>
  
    __ bind(resolved);
    __ load_klass(rbx, rdx);
  
    // Generate subtype check.  Blows rcx, rdi.  Object in rdx.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -4455,11 +4467,11 @@</span>
    __ jmpb(resolved);
  
    // Get superklass in rax and subklass in rdx
    __ bind(quicked);
    __ load_klass(rdx, rax);
<span class="udiff-line-modified-removed">-   __ load_resolved_klass_at_index(rcx, rbx, rax);</span>
<span class="udiff-line-modified-added">+   __ load_resolved_klass_at_index(rax, rcx, rbx);</span>
  
    __ bind(resolved);
  
    // Generate subtype check.  Blows rcx, rdi
    // Superklass in rax.  Subklass in rdx.
</pre>
<center><a href="templateInterpreterGenerator_x86_64.cpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="vm_version_ext_x86.cpp.udiff.html" target="_top">next &gt;</a></center>  </body>
</html>