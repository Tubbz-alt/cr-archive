<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/cpu/x86/gc/shared/barrierSetNMethod_x86.cpp</title>
    <link rel="stylesheet" href="../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2018, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;code/codeCache.hpp&quot;
 27 #include &quot;code/nativeInst.hpp&quot;
 28 #include &quot;gc/shared/barrierSetNMethod.hpp&quot;
 29 #include &quot;logging/log.hpp&quot;
 30 #include &quot;memory/resourceArea.hpp&quot;
 31 #include &quot;runtime/sharedRuntime.hpp&quot;
 32 #include &quot;runtime/thread.hpp&quot;
 33 #include &quot;utilities/align.hpp&quot;
 34 #include &quot;utilities/debug.hpp&quot;
 35 
 36 class NativeNMethodCmpBarrier: public NativeInstruction {
 37 public:
 38   enum Intel_specific_constants {
 39     instruction_code        = 0x81,
 40     instruction_size        = 8,
 41     imm_offset              = 4,
 42     instruction_rex_prefix  = Assembler::REX | Assembler::REX_B,
 43     instruction_modrm       = 0x7f  // [r15 + offset]
 44   };
 45 
 46   address instruction_address() const { return addr_at(0); }
 47   address immediate_address() const { return addr_at(imm_offset); }
 48 
 49   jint get_immedate() const { return int_at(imm_offset); }
 50   void set_immediate(jint imm) { set_int_at(imm_offset, imm); }
 51   void verify() const;
 52 };
 53 
 54 void NativeNMethodCmpBarrier::verify() const {
 55   if (((uintptr_t) instruction_address()) &amp; 0x7) {
 56     fatal(&quot;Not properly aligned&quot;);
 57   }
 58 
 59   int prefix = ubyte_at(0);
 60   if (prefix != instruction_rex_prefix) {
 61     tty-&gt;print_cr(&quot;Addr: &quot; INTPTR_FORMAT &quot; Prefix: 0x%x&quot;, p2i(instruction_address()),
 62         prefix);
 63     fatal(&quot;not a cmp barrier&quot;);
 64   }
 65 
 66   int inst = ubyte_at(1);
 67   if (inst != instruction_code) {
 68     tty-&gt;print_cr(&quot;Addr: &quot; INTPTR_FORMAT &quot; Code: 0x%x&quot;, p2i(instruction_address()),
 69         inst);
 70     fatal(&quot;not a cmp barrier&quot;);
 71   }
 72 
 73   int modrm = ubyte_at(2);
 74   if (modrm != instruction_modrm) {
 75     tty-&gt;print_cr(&quot;Addr: &quot; INTPTR_FORMAT &quot; mod/rm: 0x%x&quot;, p2i(instruction_address()),
 76         modrm);
 77     fatal(&quot;not a cmp barrier&quot;);
 78   }
 79 }
 80 
 81 void BarrierSetNMethod::deoptimize(nmethod* nm, address* return_address_ptr) {
 82   /*
 83    * [ callers frame          ]
 84    * [ callers return address ] &lt;- callers rsp
 85    * [ callers rbp            ] &lt;- callers rbp
 86    * [ callers frame slots    ]
 87    * [ return_address         ] &lt;- return_address_ptr
 88    * [ cookie ]                 &lt;- used to write the new rsp (callers rsp)
 89    * [ stub rbp ]
 90    * [ stub stuff             ]
 91    */
 92 
 93   address* stub_rbp = return_address_ptr - 2;
 94   address* callers_rsp = return_address_ptr + nm-&gt;frame_size(); /* points to callers return_address now */
 95   address* callers_rbp = callers_rsp - 1; // 1 to move to the callers return address, 1 more to move to the rbp
 96   address* cookie = return_address_ptr - 1;
 97 
 98   LogTarget(Trace, nmethod, barrier) out;
 99   if (out.is_enabled()) {
100     Thread* thread = Thread::current();
101     assert(thread-&gt;is_Java_thread(), &quot;must be JavaThread&quot;);
102     JavaThread* jth = (JavaThread*) thread;
103     ResourceMark mark;
104     log_trace(nmethod, barrier)(&quot;deoptimize(nmethod: %p, return_addr: %p, osr: %d, thread: %p(%s), making rsp: %p) -&gt; %p&quot;,
105                                nm, (address *) return_address_ptr, nm-&gt;is_osr_method(), jth,
106                                jth-&gt;get_thread_name(), callers_rsp, nm-&gt;verified_entry_point());
107   }
108 
109   assert(nm-&gt;frame_size() &gt;= 3, &quot;invariant&quot;);
110   assert(*cookie == (address) -1, &quot;invariant&quot;);
111 
112   // Preserve caller rbp.
113   *stub_rbp = *callers_rbp;
114 
115   // At the cookie address put the callers rsp.
116   *cookie = (address) callers_rsp; // should point to the return address
117 
118   // In the slot that used to be the callers rbp we put the address that our stub needs to jump to at the end.
119   // Overwriting the caller rbp should be okay since our stub rbp has the same value.
120   address* jmp_addr_ptr = callers_rbp;
121   *jmp_addr_ptr = SharedRuntime::get_handle_wrong_method_stub();
122 }
123 
124 // This is the offset of the entry barrier from where the frame is completed.
125 // If any code changes between the end of the verified entry where the entry
126 // barrier resides, and the completion of the frame, then
127 // NativeNMethodCmpBarrier::verify() will immediately complain when it does
128 // not find the expected native instruction at this offset, which needs updating.
129 // Note that this offset is invariant of PreserveFramePointer.
130 static const int entry_barrier_offset = -19;
131 
132 static NativeNMethodCmpBarrier* native_nmethod_barrier(nmethod* nm) {
133   address barrier_address = nm-&gt;code_begin() + nm-&gt;frame_complete_offset() + entry_barrier_offset;
134   NativeNMethodCmpBarrier* barrier = reinterpret_cast&lt;NativeNMethodCmpBarrier*&gt;(barrier_address);
135   debug_only(barrier-&gt;verify());
136   return barrier;
137 }
138 
139 void BarrierSetNMethod::disarm(nmethod* nm) {
140   if (!supports_entry_barrier(nm)) {
141     return;
142   }
143 
144   NativeNMethodCmpBarrier* cmp = native_nmethod_barrier(nm);
145   cmp-&gt;set_immediate(disarmed_value());
146 }
147 
148 bool BarrierSetNMethod::is_armed(nmethod* nm) {
149   if (!supports_entry_barrier(nm)) {
150     return false;
151   }
152 
153   NativeNMethodCmpBarrier* cmp = native_nmethod_barrier(nm);
154   return (disarmed_value() != cmp-&gt;get_immedate());
155 }
    </pre>
  </body>
</html>