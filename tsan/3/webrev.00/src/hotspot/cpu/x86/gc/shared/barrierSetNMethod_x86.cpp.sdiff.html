<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/gc/shared/barrierSetNMethod_x86.cpp</title>
    <link rel="stylesheet" href="../../../../../../style.css" />
  </head>
<body>
<center><a href="barrierSetAssembler_x86.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../index.html" target="_top">index</a> <a href="modRefBarrierSetAssembler_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/gc/shared/barrierSetNMethod_x86.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2018, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;code/codeCache.hpp&quot;
 27 #include &quot;code/nativeInst.hpp&quot;
 28 #include &quot;gc/shared/barrierSetNMethod.hpp&quot;
 29 #include &quot;logging/log.hpp&quot;
 30 #include &quot;memory/resourceArea.hpp&quot;
 31 #include &quot;runtime/sharedRuntime.hpp&quot;
 32 #include &quot;runtime/thread.hpp&quot;
 33 #include &quot;utilities/align.hpp&quot;
 34 #include &quot;utilities/debug.hpp&quot;
 35 
 36 class NativeNMethodCmpBarrier: public NativeInstruction {
 37 public:

 38   enum Intel_specific_constants {
 39     instruction_code        = 0x81,
 40     instruction_size        = 8,
 41     imm_offset              = 4,
 42     instruction_rex_prefix  = Assembler::REX | Assembler::REX_B,
 43     instruction_modrm       = 0x7f  // [r15 + offset]
 44   };








 45 
 46   address instruction_address() const { return addr_at(0); }
 47   address immediate_address() const { return addr_at(imm_offset); }
 48 
 49   jint get_immedate() const { return int_at(imm_offset); }
 50   void set_immediate(jint imm) { set_int_at(imm_offset, imm); }
 51   void verify() const;
 52 };
 53 

 54 void NativeNMethodCmpBarrier::verify() const {
 55   if (((uintptr_t) instruction_address()) &amp; 0x7) {
 56     fatal(&quot;Not properly aligned&quot;);
 57   }
 58 
 59   int prefix = ubyte_at(0);
 60   if (prefix != instruction_rex_prefix) {
 61     tty-&gt;print_cr(&quot;Addr: &quot; INTPTR_FORMAT &quot; Prefix: 0x%x&quot;, p2i(instruction_address()),
 62         prefix);
 63     fatal(&quot;not a cmp barrier&quot;);
 64   }
 65 
 66   int inst = ubyte_at(1);
 67   if (inst != instruction_code) {
 68     tty-&gt;print_cr(&quot;Addr: &quot; INTPTR_FORMAT &quot; Code: 0x%x&quot;, p2i(instruction_address()),
 69         inst);
 70     fatal(&quot;not a cmp barrier&quot;);
 71   }
 72 
 73   int modrm = ubyte_at(2);
 74   if (modrm != instruction_modrm) {
 75     tty-&gt;print_cr(&quot;Addr: &quot; INTPTR_FORMAT &quot; mod/rm: 0x%x&quot;, p2i(instruction_address()),
 76         modrm);
 77     fatal(&quot;not a cmp barrier&quot;);
 78   }
 79 }





















 80 
 81 void BarrierSetNMethod::deoptimize(nmethod* nm, address* return_address_ptr) {
 82   /*
 83    * [ callers frame          ]
 84    * [ callers return address ] &lt;- callers rsp
 85    * [ callers rbp            ] &lt;- callers rbp
 86    * [ callers frame slots    ]
 87    * [ return_address         ] &lt;- return_address_ptr
 88    * [ cookie ]                 &lt;- used to write the new rsp (callers rsp)
 89    * [ stub rbp ]
 90    * [ stub stuff             ]
 91    */
 92 
 93   address* stub_rbp = return_address_ptr - 2;
 94   address* callers_rsp = return_address_ptr + nm-&gt;frame_size(); /* points to callers return_address now */
 95   address* callers_rbp = callers_rsp - 1; // 1 to move to the callers return address, 1 more to move to the rbp
 96   address* cookie = return_address_ptr - 1;
 97 
 98   LogTarget(Trace, nmethod, barrier) out;
 99   if (out.is_enabled()) {
</pre>
<hr />
<pre>
110   assert(*cookie == (address) -1, &quot;invariant&quot;);
111 
112   // Preserve caller rbp.
113   *stub_rbp = *callers_rbp;
114 
115   // At the cookie address put the callers rsp.
116   *cookie = (address) callers_rsp; // should point to the return address
117 
118   // In the slot that used to be the callers rbp we put the address that our stub needs to jump to at the end.
119   // Overwriting the caller rbp should be okay since our stub rbp has the same value.
120   address* jmp_addr_ptr = callers_rbp;
121   *jmp_addr_ptr = SharedRuntime::get_handle_wrong_method_stub();
122 }
123 
124 // This is the offset of the entry barrier from where the frame is completed.
125 // If any code changes between the end of the verified entry where the entry
126 // barrier resides, and the completion of the frame, then
127 // NativeNMethodCmpBarrier::verify() will immediately complain when it does
128 // not find the expected native instruction at this offset, which needs updating.
129 // Note that this offset is invariant of PreserveFramePointer.
<span class="line-modified">130 static const int entry_barrier_offset = -19;</span>
131 
132 static NativeNMethodCmpBarrier* native_nmethod_barrier(nmethod* nm) {
133   address barrier_address = nm-&gt;code_begin() + nm-&gt;frame_complete_offset() + entry_barrier_offset;
134   NativeNMethodCmpBarrier* barrier = reinterpret_cast&lt;NativeNMethodCmpBarrier*&gt;(barrier_address);
135   debug_only(barrier-&gt;verify());
136   return barrier;
137 }
138 
139 void BarrierSetNMethod::disarm(nmethod* nm) {
140   if (!supports_entry_barrier(nm)) {
141     return;
142   }
143 
144   NativeNMethodCmpBarrier* cmp = native_nmethod_barrier(nm);
145   cmp-&gt;set_immediate(disarmed_value());
146 }
147 
148 bool BarrierSetNMethod::is_armed(nmethod* nm) {
149   if (!supports_entry_barrier(nm)) {
150     return false;
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2018, 2019, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;code/codeCache.hpp&quot;
 27 #include &quot;code/nativeInst.hpp&quot;
 28 #include &quot;gc/shared/barrierSetNMethod.hpp&quot;
 29 #include &quot;logging/log.hpp&quot;
 30 #include &quot;memory/resourceArea.hpp&quot;
 31 #include &quot;runtime/sharedRuntime.hpp&quot;
 32 #include &quot;runtime/thread.hpp&quot;
 33 #include &quot;utilities/align.hpp&quot;
 34 #include &quot;utilities/debug.hpp&quot;
 35 
 36 class NativeNMethodCmpBarrier: public NativeInstruction {
 37 public:
<span class="line-added"> 38 #ifdef _LP64</span>
 39   enum Intel_specific_constants {
 40     instruction_code        = 0x81,
 41     instruction_size        = 8,
 42     imm_offset              = 4,
 43     instruction_rex_prefix  = Assembler::REX | Assembler::REX_B,
 44     instruction_modrm       = 0x7f  // [r15 + offset]
 45   };
<span class="line-added"> 46 #else</span>
<span class="line-added"> 47   enum Intel_specific_constants {</span>
<span class="line-added"> 48     instruction_code        = 0x81,</span>
<span class="line-added"> 49     instruction_size        = 7,</span>
<span class="line-added"> 50     imm_offset              = 2,</span>
<span class="line-added"> 51     instruction_modrm       = 0x3f  // [rdi]</span>
<span class="line-added"> 52   };</span>
<span class="line-added"> 53 #endif</span>
 54 
 55   address instruction_address() const { return addr_at(0); }
 56   address immediate_address() const { return addr_at(imm_offset); }
 57 
 58   jint get_immedate() const { return int_at(imm_offset); }
 59   void set_immediate(jint imm) { set_int_at(imm_offset, imm); }
 60   void verify() const;
 61 };
 62 
<span class="line-added"> 63 #ifdef _LP64</span>
 64 void NativeNMethodCmpBarrier::verify() const {
 65   if (((uintptr_t) instruction_address()) &amp; 0x7) {
 66     fatal(&quot;Not properly aligned&quot;);
 67   }
 68 
 69   int prefix = ubyte_at(0);
 70   if (prefix != instruction_rex_prefix) {
 71     tty-&gt;print_cr(&quot;Addr: &quot; INTPTR_FORMAT &quot; Prefix: 0x%x&quot;, p2i(instruction_address()),
 72         prefix);
 73     fatal(&quot;not a cmp barrier&quot;);
 74   }
 75 
 76   int inst = ubyte_at(1);
 77   if (inst != instruction_code) {
 78     tty-&gt;print_cr(&quot;Addr: &quot; INTPTR_FORMAT &quot; Code: 0x%x&quot;, p2i(instruction_address()),
 79         inst);
 80     fatal(&quot;not a cmp barrier&quot;);
 81   }
 82 
 83   int modrm = ubyte_at(2);
 84   if (modrm != instruction_modrm) {
 85     tty-&gt;print_cr(&quot;Addr: &quot; INTPTR_FORMAT &quot; mod/rm: 0x%x&quot;, p2i(instruction_address()),
 86         modrm);
 87     fatal(&quot;not a cmp barrier&quot;);
 88   }
 89 }
<span class="line-added"> 90 #else</span>
<span class="line-added"> 91 void NativeNMethodCmpBarrier::verify() const {</span>
<span class="line-added"> 92   if (((uintptr_t) instruction_address()) &amp; 0x3) {</span>
<span class="line-added"> 93     fatal(&quot;Not properly aligned&quot;);</span>
<span class="line-added"> 94   }</span>
<span class="line-added"> 95 </span>
<span class="line-added"> 96   int inst = ubyte_at(0);</span>
<span class="line-added"> 97   if (inst != instruction_code) {</span>
<span class="line-added"> 98     tty-&gt;print_cr(&quot;Addr: &quot; INTPTR_FORMAT &quot; Code: 0x%x&quot;, p2i(instruction_address()),</span>
<span class="line-added"> 99         inst);</span>
<span class="line-added">100     fatal(&quot;not a cmp barrier&quot;);</span>
<span class="line-added">101   }</span>
<span class="line-added">102 </span>
<span class="line-added">103   int modrm = ubyte_at(1);</span>
<span class="line-added">104   if (modrm != instruction_modrm) {</span>
<span class="line-added">105     tty-&gt;print_cr(&quot;Addr: &quot; INTPTR_FORMAT &quot; mod/rm: 0x%x&quot;, p2i(instruction_address()),</span>
<span class="line-added">106         modrm);</span>
<span class="line-added">107     fatal(&quot;not a cmp barrier&quot;);</span>
<span class="line-added">108   }</span>
<span class="line-added">109 }</span>
<span class="line-added">110 #endif // _LP64</span>
111 
112 void BarrierSetNMethod::deoptimize(nmethod* nm, address* return_address_ptr) {
113   /*
114    * [ callers frame          ]
115    * [ callers return address ] &lt;- callers rsp
116    * [ callers rbp            ] &lt;- callers rbp
117    * [ callers frame slots    ]
118    * [ return_address         ] &lt;- return_address_ptr
119    * [ cookie ]                 &lt;- used to write the new rsp (callers rsp)
120    * [ stub rbp ]
121    * [ stub stuff             ]
122    */
123 
124   address* stub_rbp = return_address_ptr - 2;
125   address* callers_rsp = return_address_ptr + nm-&gt;frame_size(); /* points to callers return_address now */
126   address* callers_rbp = callers_rsp - 1; // 1 to move to the callers return address, 1 more to move to the rbp
127   address* cookie = return_address_ptr - 1;
128 
129   LogTarget(Trace, nmethod, barrier) out;
130   if (out.is_enabled()) {
</pre>
<hr />
<pre>
141   assert(*cookie == (address) -1, &quot;invariant&quot;);
142 
143   // Preserve caller rbp.
144   *stub_rbp = *callers_rbp;
145 
146   // At the cookie address put the callers rsp.
147   *cookie = (address) callers_rsp; // should point to the return address
148 
149   // In the slot that used to be the callers rbp we put the address that our stub needs to jump to at the end.
150   // Overwriting the caller rbp should be okay since our stub rbp has the same value.
151   address* jmp_addr_ptr = callers_rbp;
152   *jmp_addr_ptr = SharedRuntime::get_handle_wrong_method_stub();
153 }
154 
155 // This is the offset of the entry barrier from where the frame is completed.
156 // If any code changes between the end of the verified entry where the entry
157 // barrier resides, and the completion of the frame, then
158 // NativeNMethodCmpBarrier::verify() will immediately complain when it does
159 // not find the expected native instruction at this offset, which needs updating.
160 // Note that this offset is invariant of PreserveFramePointer.
<span class="line-modified">161 static const int entry_barrier_offset = LP64_ONLY(-19) NOT_LP64(-18);</span>
162 
163 static NativeNMethodCmpBarrier* native_nmethod_barrier(nmethod* nm) {
164   address barrier_address = nm-&gt;code_begin() + nm-&gt;frame_complete_offset() + entry_barrier_offset;
165   NativeNMethodCmpBarrier* barrier = reinterpret_cast&lt;NativeNMethodCmpBarrier*&gt;(barrier_address);
166   debug_only(barrier-&gt;verify());
167   return barrier;
168 }
169 
170 void BarrierSetNMethod::disarm(nmethod* nm) {
171   if (!supports_entry_barrier(nm)) {
172     return;
173   }
174 
175   NativeNMethodCmpBarrier* cmp = native_nmethod_barrier(nm);
176   cmp-&gt;set_immediate(disarmed_value());
177 }
178 
179 bool BarrierSetNMethod::is_armed(nmethod* nm) {
180   if (!supports_entry_barrier(nm)) {
181     return false;
</pre>
</td>
</tr>
</table>
<center><a href="barrierSetAssembler_x86.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../index.html" target="_top">index</a> <a href="modRefBarrierSetAssembler_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>