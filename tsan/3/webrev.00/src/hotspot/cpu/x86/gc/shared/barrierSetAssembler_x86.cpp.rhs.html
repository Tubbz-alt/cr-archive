<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/x86/gc/shared/barrierSetAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
<a name="1" id="anc1"></a><span class="line-modified">  2  * Copyright (c) 2018, 2019, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/shared/barrierSet.hpp&quot;
 27 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
 28 #include &quot;gc/shared/barrierSetNMethod.hpp&quot;
 29 #include &quot;gc/shared/collectedHeap.hpp&quot;
 30 #include &quot;interpreter/interp_masm.hpp&quot;
<a name="2" id="anc2"></a><span class="line-added"> 31 #include &quot;memory/universe.hpp&quot;</span>
 32 #include &quot;runtime/jniHandles.hpp&quot;
<a name="3" id="anc3"></a><span class="line-added"> 33 #include &quot;runtime/sharedRuntime.hpp&quot;</span>
 34 #include &quot;runtime/thread.hpp&quot;
 35 
 36 #define __ masm-&gt;
 37 
 38 void BarrierSetAssembler::load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,
 39                                   Register dst, Address src, Register tmp1, Register tmp_thread) {
 40   bool in_heap = (decorators &amp; IN_HEAP) != 0;
 41   bool in_native = (decorators &amp; IN_NATIVE) != 0;
 42   bool is_not_null = (decorators &amp; IS_NOT_NULL) != 0;
 43   bool atomic = (decorators &amp; MO_RELAXED) != 0;
 44 
 45   switch (type) {
 46   case T_OBJECT:
 47   case T_ARRAY: {
 48     if (in_heap) {
 49 #ifdef _LP64
 50       if (UseCompressedOops) {
 51         __ movl(dst, src);
 52         if (is_not_null) {
 53           __ decode_heap_oop_not_null(dst);
 54         } else {
 55           __ decode_heap_oop(dst);
 56         }
 57       } else
 58 #endif
 59       {
 60         __ movptr(dst, src);
 61       }
 62     } else {
 63       assert(in_native, &quot;why else?&quot;);
 64       __ movptr(dst, src);
 65     }
 66     break;
 67   }
 68   case T_BOOLEAN: __ load_unsigned_byte(dst, src);  break;
 69   case T_BYTE:    __ load_signed_byte(dst, src);    break;
 70   case T_CHAR:    __ load_unsigned_short(dst, src); break;
 71   case T_SHORT:   __ load_signed_short(dst, src);   break;
 72   case T_INT:     __ movl  (dst, src);              break;
 73   case T_ADDRESS: __ movptr(dst, src);              break;
 74   case T_FLOAT:
 75     assert(dst == noreg, &quot;only to ftos&quot;);
 76     __ load_float(src);
 77     break;
 78   case T_DOUBLE:
 79     assert(dst == noreg, &quot;only to dtos&quot;);
 80     __ load_double(src);
 81     break;
 82   case T_LONG:
 83     assert(dst == noreg, &quot;only to ltos&quot;);
 84 #ifdef _LP64
 85     __ movq(rax, src);
 86 #else
 87     if (atomic) {
 88       __ fild_d(src);               // Must load atomically
 89       __ subptr(rsp,2*wordSize);    // Make space for store
 90       __ fistp_d(Address(rsp,0));
 91       __ pop(rax);
 92       __ pop(rdx);
 93     } else {
 94       __ movl(rax, src);
 95       __ movl(rdx, src.plus_disp(wordSize));
 96     }
 97 #endif
 98     break;
 99   default: Unimplemented();
100   }
101 }
102 
103 void BarrierSetAssembler::store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,
104                                    Address dst, Register val, Register tmp1, Register tmp2) {
105   bool in_heap = (decorators &amp; IN_HEAP) != 0;
106   bool in_native = (decorators &amp; IN_NATIVE) != 0;
107   bool is_not_null = (decorators &amp; IS_NOT_NULL) != 0;
108   bool atomic = (decorators &amp; MO_RELAXED) != 0;
109 
110   switch (type) {
111   case T_OBJECT:
112   case T_ARRAY: {
113     if (in_heap) {
114       if (val == noreg) {
115         assert(!is_not_null, &quot;inconsistent access&quot;);
116 #ifdef _LP64
117         if (UseCompressedOops) {
118           __ movl(dst, (int32_t)NULL_WORD);
119         } else {
120           __ movslq(dst, (int32_t)NULL_WORD);
121         }
122 #else
123         __ movl(dst, (int32_t)NULL_WORD);
124 #endif
125       } else {
126 #ifdef _LP64
127         if (UseCompressedOops) {
128           assert(!dst.uses(val), &quot;not enough registers&quot;);
129           if (is_not_null) {
130             __ encode_heap_oop_not_null(val);
131           } else {
132             __ encode_heap_oop(val);
133           }
134           __ movl(dst, val);
135         } else
136 #endif
137         {
138           __ movptr(dst, val);
139         }
140       }
141     } else {
142       assert(in_native, &quot;why else?&quot;);
143       assert(val != noreg, &quot;not supported&quot;);
144       __ movptr(dst, val);
145     }
146     break;
147   }
148   case T_BOOLEAN:
149     __ andl(val, 0x1);  // boolean is true if LSB is 1
150     __ movb(dst, val);
151     break;
152   case T_BYTE:
153     __ movb(dst, val);
154     break;
155   case T_SHORT:
156     __ movw(dst, val);
157     break;
158   case T_CHAR:
159     __ movw(dst, val);
160     break;
161   case T_INT:
162     __ movl(dst, val);
163     break;
164   case T_LONG:
165     assert(val == noreg, &quot;only tos&quot;);
166 #ifdef _LP64
167     __ movq(dst, rax);
168 #else
169     if (atomic) {
170       __ push(rdx);
171       __ push(rax);                 // Must update atomically with FIST
172       __ fild_d(Address(rsp,0));    // So load into FPU register
173       __ fistp_d(dst);              // and put into memory atomically
174       __ addptr(rsp, 2*wordSize);
175     } else {
176       __ movptr(dst, rax);
177       __ movptr(dst.plus_disp(wordSize), rdx);
178     }
179 #endif
180     break;
181   case T_FLOAT:
182     assert(val == noreg, &quot;only tos&quot;);
183     __ store_float(dst);
184     break;
185   case T_DOUBLE:
186     assert(val == noreg, &quot;only tos&quot;);
187     __ store_double(dst);
188     break;
189   case T_ADDRESS:
190     __ movptr(dst, val);
191     break;
192   default: Unimplemented();
193   }
194 }
195 
196 #ifndef _LP64
197 void BarrierSetAssembler::obj_equals(MacroAssembler* masm,
198                                      Address obj1, jobject obj2) {
199   __ cmpoop_raw(obj1, obj2);
200 }
201 
202 void BarrierSetAssembler::obj_equals(MacroAssembler* masm,
203                                      Register obj1, jobject obj2) {
204   __ cmpoop_raw(obj1, obj2);
205 }
206 #endif
207 void BarrierSetAssembler::obj_equals(MacroAssembler* masm,
208                                      Register obj1, Address obj2) {
209   __ cmpptr(obj1, obj2);
210 }
211 
212 void BarrierSetAssembler::obj_equals(MacroAssembler* masm,
213                                      Register obj1, Register obj2) {
214   __ cmpptr(obj1, obj2);
215 }
216 
217 void BarrierSetAssembler::try_resolve_jobject_in_native(MacroAssembler* masm, Register jni_env,
218                                                         Register obj, Register tmp, Label&amp; slowpath) {
219   __ clear_jweak_tag(obj);
220   __ movptr(obj, Address(obj, 0));
221 }
222 
223 void BarrierSetAssembler::tlab_allocate(MacroAssembler* masm,
224                                         Register thread, Register obj,
225                                         Register var_size_in_bytes,
226                                         int con_size_in_bytes,
227                                         Register t1,
228                                         Register t2,
229                                         Label&amp; slow_case) {
230   assert_different_registers(obj, t1, t2);
231   assert_different_registers(obj, var_size_in_bytes, t1);
232   Register end = t2;
233   if (!thread-&gt;is_valid()) {
234 #ifdef _LP64
235     thread = r15_thread;
236 #else
237     assert(t1-&gt;is_valid(), &quot;need temp reg&quot;);
238     thread = t1;
239     __ get_thread(thread);
240 #endif
241   }
242 
243   __ verify_tlab();
244 
245   __ movptr(obj, Address(thread, JavaThread::tlab_top_offset()));
246   if (var_size_in_bytes == noreg) {
247     __ lea(end, Address(obj, con_size_in_bytes));
248   } else {
249     __ lea(end, Address(obj, var_size_in_bytes, Address::times_1));
250   }
251   __ cmpptr(end, Address(thread, JavaThread::tlab_end_offset()));
252   __ jcc(Assembler::above, slow_case);
253 
254   // update the tlab top pointer
255   __ movptr(Address(thread, JavaThread::tlab_top_offset()), end);
256 
257   // recover var_size_in_bytes if necessary
258   if (var_size_in_bytes == end) {
259     __ subptr(var_size_in_bytes, obj);
260   }
261   __ verify_tlab();
262 }
263 
264 // Defines obj, preserves var_size_in_bytes
265 void BarrierSetAssembler::eden_allocate(MacroAssembler* masm,
266                                         Register thread, Register obj,
267                                         Register var_size_in_bytes,
268                                         int con_size_in_bytes,
269                                         Register t1,
270                                         Label&amp; slow_case) {
271   assert(obj == rax, &quot;obj must be in rax, for cmpxchg&quot;);
272   assert_different_registers(obj, var_size_in_bytes, t1);
273   if (!Universe::heap()-&gt;supports_inline_contig_alloc()) {
274     __ jmp(slow_case);
275   } else {
276     Register end = t1;
277     Label retry;
278     __ bind(retry);
279     ExternalAddress heap_top((address) Universe::heap()-&gt;top_addr());
280     __ movptr(obj, heap_top);
281     if (var_size_in_bytes == noreg) {
282       __ lea(end, Address(obj, con_size_in_bytes));
283     } else {
284       __ lea(end, Address(obj, var_size_in_bytes, Address::times_1));
285     }
286     // if end &lt; obj then we wrapped around =&gt; object too long =&gt; slow case
287     __ cmpptr(end, obj);
288     __ jcc(Assembler::below, slow_case);
289     __ cmpptr(end, ExternalAddress((address) Universe::heap()-&gt;end_addr()));
290     __ jcc(Assembler::above, slow_case);
291     // Compare obj with the top addr, and if still equal, store the new top addr in
292     // end at the address of the top addr pointer. Sets ZF if was equal, and clears
293     // it otherwise. Use lock prefix for atomicity on MPs.
294     __ locked_cmpxchgptr(end, heap_top);
295     __ jcc(Assembler::notEqual, retry);
296     incr_allocated_bytes(masm, thread, var_size_in_bytes, con_size_in_bytes, thread-&gt;is_valid() ? noreg : t1);
297   }
298 }
299 
300 void BarrierSetAssembler::incr_allocated_bytes(MacroAssembler* masm, Register thread,
301                                                Register var_size_in_bytes,
302                                                int con_size_in_bytes,
303                                                Register t1) {
304   if (!thread-&gt;is_valid()) {
305 #ifdef _LP64
306     thread = r15_thread;
307 #else
308     assert(t1-&gt;is_valid(), &quot;need temp reg&quot;);
309     thread = t1;
310     __ get_thread(thread);
311 #endif
312   }
313 
314 #ifdef _LP64
315   if (var_size_in_bytes-&gt;is_valid()) {
316     __ addq(Address(thread, in_bytes(JavaThread::allocated_bytes_offset())), var_size_in_bytes);
317   } else {
318     __ addq(Address(thread, in_bytes(JavaThread::allocated_bytes_offset())), con_size_in_bytes);
319   }
320 #else
321   if (var_size_in_bytes-&gt;is_valid()) {
322     __ addl(Address(thread, in_bytes(JavaThread::allocated_bytes_offset())), var_size_in_bytes);
323   } else {
324     __ addl(Address(thread, in_bytes(JavaThread::allocated_bytes_offset())), con_size_in_bytes);
325   }
326   __ adcl(Address(thread, in_bytes(JavaThread::allocated_bytes_offset())+4), 0);
327 #endif
328 }
329 
<a name="4" id="anc4"></a><span class="line-added">330 #ifdef _LP64</span>
331 void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm) {
332   BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();
333   if (bs_nm == NULL) {
334     return;
335   }
<a name="5" id="anc5"></a>


336   Label continuation;
<a name="6" id="anc6"></a><span class="line-modified">337   Register thread = r15_thread;</span>
338   Address disarmed_addr(thread, in_bytes(bs_nm-&gt;thread_disarmed_offset()));
339   __ align(8);
340   __ cmpl(disarmed_addr, 0);
341   __ jcc(Assembler::equal, continuation);
342   __ call(RuntimeAddress(StubRoutines::x86::method_entry_barrier()));
343   __ bind(continuation);
<a name="7" id="anc7"></a><span class="line-added">344 }</span>
<span class="line-added">345 #else</span>
<span class="line-added">346 void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm) {</span>
<span class="line-added">347   BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();</span>
<span class="line-added">348   if (bs_nm == NULL) {</span>
<span class="line-added">349     return;</span>
<span class="line-added">350   }</span>
<span class="line-added">351 </span>
<span class="line-added">352   Label continuation;</span>
<span class="line-added">353 </span>
<span class="line-added">354   Register tmp = rdi;</span>
<span class="line-added">355   __ push(tmp);</span>
<span class="line-added">356   __ movptr(tmp, (intptr_t)bs_nm-&gt;disarmed_value_address());</span>
<span class="line-added">357   Address disarmed_addr(tmp, 0);</span>
<span class="line-added">358   __ align(4);</span>
<span class="line-added">359   __ cmpl(disarmed_addr, 0);</span>
<span class="line-added">360   __ pop(tmp);</span>
<span class="line-added">361   __ jcc(Assembler::equal, continuation);</span>
<span class="line-added">362   __ call(RuntimeAddress(StubRoutines::x86::method_entry_barrier()));</span>
<span class="line-added">363   __ bind(continuation);</span>
<span class="line-added">364 }</span>
<span class="line-added">365 #endif</span>
<span class="line-added">366 </span>
<span class="line-added">367 void BarrierSetAssembler::c2i_entry_barrier(MacroAssembler* masm) {</span>
<span class="line-added">368   BarrierSetNMethod* bs = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();</span>
<span class="line-added">369   if (bs == NULL) {</span>
<span class="line-added">370     return;</span>
<span class="line-added">371   }</span>
<span class="line-added">372 </span>
<span class="line-added">373   Label bad_call;</span>
<span class="line-added">374   __ cmpptr(rbx, 0); // rbx contains the incoming method for c2i adapters.</span>
<span class="line-added">375   __ jcc(Assembler::equal, bad_call);</span>
<span class="line-added">376 </span>
<span class="line-added">377 #ifdef _LP64</span>
<span class="line-added">378   Register tmp1 = rscratch1;</span>
<span class="line-added">379   Register tmp2 = rscratch2;</span>
<span class="line-added">380 #else</span>
<span class="line-added">381   Register tmp1 = rax;</span>
<span class="line-added">382   Register tmp2 = rcx;</span>
<span class="line-added">383   __ push(tmp1);</span>
<span class="line-added">384   __ push(tmp2);</span>
<span class="line-added">385 #endif // _LP64</span>
<span class="line-added">386 </span>
<span class="line-added">387   // Pointer chase to the method holder to find out if the method is concurrently unloading.</span>
<span class="line-added">388   Label method_live;</span>
<span class="line-added">389   __ load_method_holder_cld(tmp1, rbx);</span>
<span class="line-added">390 </span>
<span class="line-added">391    // Is it a strong CLD?</span>
<span class="line-added">392   __ cmpl(Address(tmp1, ClassLoaderData::keep_alive_offset()), 0);</span>
<span class="line-added">393   __ jcc(Assembler::greater, method_live);</span>
<span class="line-added">394 </span>
<span class="line-added">395    // Is it a weak but alive CLD?</span>
<span class="line-added">396   __ movptr(tmp1, Address(tmp1, ClassLoaderData::holder_offset()));</span>
<span class="line-added">397   __ resolve_weak_handle(tmp1, tmp2);</span>
<span class="line-added">398   __ cmpptr(tmp1, 0);</span>
<span class="line-added">399   __ jcc(Assembler::notEqual, method_live);</span>
<span class="line-added">400 </span>
<span class="line-added">401 #ifndef _LP64</span>
<span class="line-added">402   __ pop(tmp2);</span>
<span class="line-added">403   __ pop(tmp1);</span>
<span class="line-added">404 #endif</span>
<span class="line-added">405 </span>
<span class="line-added">406   __ bind(bad_call);</span>
<span class="line-added">407   __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));</span>
<span class="line-added">408   __ bind(method_live);</span>
<span class="line-added">409 </span>
<span class="line-added">410 #ifndef _LP64</span>
<span class="line-added">411   __ pop(tmp2);</span>
<span class="line-added">412   __ pop(tmp1);</span>
413 #endif
414 }
<a name="8" id="anc8"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="8" type="hidden" />
</body>
</html>