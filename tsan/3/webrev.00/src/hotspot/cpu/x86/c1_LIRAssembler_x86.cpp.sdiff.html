<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/c1_LIRAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c1_FrameMap_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_LIRAssembler_x86.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/c1_LIRAssembler_x86.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;c1/c1_Compilation.hpp&quot;
  29 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  30 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  31 #include &quot;c1/c1_Runtime1.hpp&quot;
  32 #include &quot;c1/c1_ValueStack.hpp&quot;
  33 #include &quot;ci/ciArrayKlass.hpp&quot;
  34 #include &quot;ci/ciInstance.hpp&quot;
<span class="line-removed">  35 #include &quot;gc/shared/barrierSet.hpp&quot;</span>
<span class="line-removed">  36 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;</span>
  37 #include &quot;gc/shared/collectedHeap.hpp&quot;
  38 #include &quot;nativeInst_x86.hpp&quot;
  39 #include &quot;oops/objArrayKlass.hpp&quot;
  40 #include &quot;runtime/frame.inline.hpp&quot;
  41 #include &quot;runtime/safepointMechanism.hpp&quot;
  42 #include &quot;runtime/sharedRuntime.hpp&quot;

  43 #include &quot;vmreg_x86.inline.hpp&quot;
  44 
  45 
  46 // These masks are used to provide 128-bit aligned bitmasks to the XMM
  47 // instructions, to allow sign-masking or sign-bit flipping.  They allow
  48 // fast versions of NegF/NegD and AbsF/AbsD.
  49 
  50 // Note: &#39;double&#39; and &#39;long long&#39; have 32-bits alignment on x86.
  51 static jlong* double_quadword(jlong *adr, jlong lo, jlong hi) {
  52   // Use the expression (adr)&amp;(~0xF) to provide 128-bits aligned address
  53   // of 128-bits operands for SSE instructions.
  54   jlong *operand = (jlong*)(((intptr_t)adr) &amp; ((intptr_t)(~0xF)));
  55   // Store the value to a 128-bits operand.
  56   operand[0] = lo;
  57   operand[1] = hi;
  58   return operand;
  59 }
  60 
  61 // Buffer for 128-bits masks used by SSE instructions.
  62 static jlong fp_signmask_pool[(4+1)*2]; // 4*128bits(data) + 128bits(alignment)
</pre>
<hr />
<pre>
 143   address const_addr = __ float_constant(f);
 144   if (const_addr == NULL) {
 145     bailout(&quot;const section overflow&quot;);
 146     return __ code()-&gt;consts()-&gt;start();
 147   } else {
 148     return const_addr;
 149   }
 150 }
 151 
 152 
 153 address LIR_Assembler::double_constant(double d) {
 154   address const_addr = __ double_constant(d);
 155   if (const_addr == NULL) {
 156     bailout(&quot;const section overflow&quot;);
 157     return __ code()-&gt;consts()-&gt;start();
 158   } else {
 159     return const_addr;
 160   }
 161 }
 162 
<span class="line-modified"> 163 </span>
<span class="line-removed"> 164 void LIR_Assembler::set_24bit_FPU() {</span>
<span class="line-removed"> 165   __ fldcw(ExternalAddress(StubRoutines::addr_fpu_cntrl_wrd_24()));</span>
<span class="line-removed"> 166 }</span>
<span class="line-removed"> 167 </span>
<span class="line-removed"> 168 void LIR_Assembler::reset_FPU() {</span>
<span class="line-removed"> 169   __ fldcw(ExternalAddress(StubRoutines::addr_fpu_cntrl_wrd_std()));</span>
<span class="line-removed"> 170 }</span>
<span class="line-removed"> 171 </span>
 172 void LIR_Assembler::fpop() {
 173   __ fpop();
 174 }
 175 
 176 void LIR_Assembler::fxch(int i) {
 177   __ fxch(i);
 178 }
 179 
 180 void LIR_Assembler::fld(int i) {
 181   __ fld_s(i);
 182 }
 183 
 184 void LIR_Assembler::ffree(int i) {
 185   __ ffree(i);
 186 }

 187 
 188 void LIR_Assembler::breakpoint() {
 189   __ int3();
 190 }
 191 
 192 void LIR_Assembler::push(LIR_Opr opr) {
 193   if (opr-&gt;is_single_cpu()) {
 194     __ push_reg(opr-&gt;as_register());
 195   } else if (opr-&gt;is_double_cpu()) {
 196     NOT_LP64(__ push_reg(opr-&gt;as_register_hi()));
 197     __ push_reg(opr-&gt;as_register_lo());
 198   } else if (opr-&gt;is_stack()) {
 199     __ push_addr(frame_map()-&gt;address_for_slot(opr-&gt;single_stack_ix()));
 200   } else if (opr-&gt;is_constant()) {
 201     LIR_Const* const_opr = opr-&gt;as_constant_ptr();
 202     if (const_opr-&gt;type() == T_OBJECT) {
 203       __ push_oop(const_opr-&gt;as_jobject());
 204     } else if (const_opr-&gt;type() == T_INT) {
 205       __ push_jint(const_opr-&gt;as_jint());
 206     } else {
</pre>
<hr />
<pre>
 342 int LIR_Assembler::check_icache() {
 343   Register receiver = FrameMap::receiver_opr-&gt;as_register();
 344   Register ic_klass = IC_Klass;
 345   const int ic_cmp_size = LP64_ONLY(10) NOT_LP64(9);
 346   const bool do_post_padding = VerifyOops || UseCompressedClassPointers;
 347   if (!do_post_padding) {
 348     // insert some nops so that the verified entry point is aligned on CodeEntryAlignment
 349     __ align(CodeEntryAlignment, __ offset() + ic_cmp_size);
 350   }
 351   int offset = __ offset();
 352   __ inline_cache_check(receiver, IC_Klass);
 353   assert(__ offset() % CodeEntryAlignment == 0 || do_post_padding, &quot;alignment must be correct&quot;);
 354   if (do_post_padding) {
 355     // force alignment after the cache check.
 356     // It&#39;s been verified to be aligned if !VerifyOops
 357     __ align(CodeEntryAlignment);
 358   }
 359   return offset;
 360 }
 361 
















 362 
 363 void LIR_Assembler::jobject2reg_with_patching(Register reg, CodeEmitInfo* info) {
 364   jobject o = NULL;
 365   PatchingStub* patch = new PatchingStub(_masm, patching_id(info));
 366   __ movoop(reg, o);
 367   patching_epilog(patch, lir_patch_normal, reg, info);
 368 }
 369 
 370 void LIR_Assembler::klass2reg_with_patching(Register reg, CodeEmitInfo* info) {
 371   Metadata* o = NULL;
 372   PatchingStub* patch = new PatchingStub(_masm, PatchingStub::load_klass_id);
 373   __ mov_metadata(reg, o);
 374   patching_epilog(patch, lir_patch_normal, reg, info);
 375 }
 376 
 377 // This specifies the rsp decrement needed to build the frame
 378 int LIR_Assembler::initial_frame_size_in_bytes() const {
 379   // if rounding, must let FrameMap know!
 380 
 381   // The frame_map records size in slots (32bit word)
</pre>
<hr />
<pre>
 639     }
 640 
 641     case T_METADATA: {
 642       if (patch_code != lir_patch_none) {
 643         klass2reg_with_patching(dest-&gt;as_register(), info);
 644       } else {
 645         __ mov_metadata(dest-&gt;as_register(), c-&gt;as_metadata());
 646       }
 647       break;
 648     }
 649 
 650     case T_FLOAT: {
 651       if (dest-&gt;is_single_xmm()) {
 652         if (LP64_ONLY(UseAVX &lt;= 2 &amp;&amp;) c-&gt;is_zero_float()) {
 653           __ xorps(dest-&gt;as_xmm_float_reg(), dest-&gt;as_xmm_float_reg());
 654         } else {
 655           __ movflt(dest-&gt;as_xmm_float_reg(),
 656                    InternalAddress(float_constant(c-&gt;as_jfloat())));
 657         }
 658       } else {

 659         assert(dest-&gt;is_single_fpu(), &quot;must be&quot;);
 660         assert(dest-&gt;fpu_regnr() == 0, &quot;dest must be TOS&quot;);
 661         if (c-&gt;is_zero_float()) {
 662           __ fldz();
 663         } else if (c-&gt;is_one_float()) {
 664           __ fld1();
 665         } else {
 666           __ fld_s (InternalAddress(float_constant(c-&gt;as_jfloat())));
 667         }



 668       }
 669       break;
 670     }
 671 
 672     case T_DOUBLE: {
 673       if (dest-&gt;is_double_xmm()) {
 674         if (LP64_ONLY(UseAVX &lt;= 2 &amp;&amp;) c-&gt;is_zero_double()) {
 675           __ xorpd(dest-&gt;as_xmm_double_reg(), dest-&gt;as_xmm_double_reg());
 676         } else {
 677           __ movdbl(dest-&gt;as_xmm_double_reg(),
 678                     InternalAddress(double_constant(c-&gt;as_jdouble())));
 679         }
 680       } else {

 681         assert(dest-&gt;is_double_fpu(), &quot;must be&quot;);
 682         assert(dest-&gt;fpu_regnrLo() == 0, &quot;dest must be TOS&quot;);
 683         if (c-&gt;is_zero_double()) {
 684           __ fldz();
 685         } else if (c-&gt;is_one_double()) {
 686           __ fld1();
 687         } else {
 688           __ fld_d (InternalAddress(double_constant(c-&gt;as_jdouble())));
 689         }



 690       }
 691       break;
 692     }
 693 
 694     default:
 695       ShouldNotReachHere();
 696   }
 697 }
 698 
 699 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
 700   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 701   assert(dest-&gt;is_stack(), &quot;should not call otherwise&quot;);
 702   LIR_Const* c = src-&gt;as_constant_ptr();
 703 
 704   switch (c-&gt;type()) {
 705     case T_INT:  // fall through
 706     case T_FLOAT:
 707       __ movl(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jint_bits());
 708       break;
 709 
</pre>
<hr />
<pre>
 828   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 829   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 830 
 831   // move between cpu-registers
 832   if (dest-&gt;is_single_cpu()) {
 833 #ifdef _LP64
 834     if (src-&gt;type() == T_LONG) {
 835       // Can do LONG -&gt; OBJECT
 836       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
 837       return;
 838     }
 839 #endif
 840     assert(src-&gt;is_single_cpu(), &quot;must match&quot;);
 841     if (src-&gt;type() == T_OBJECT) {
 842       __ verify_oop(src-&gt;as_register());
 843     }
 844     move_regs(src-&gt;as_register(), dest-&gt;as_register());
 845 
 846   } else if (dest-&gt;is_double_cpu()) {
 847 #ifdef _LP64
<span class="line-modified"> 848     if (src-&gt;type() == T_OBJECT || src-&gt;type() == T_ARRAY) {</span>
 849       // Surprising to me but we can see move of a long to t_object
 850       __ verify_oop(src-&gt;as_register());
 851       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
 852       return;
 853     }
 854 #endif
 855     assert(src-&gt;is_double_cpu(), &quot;must match&quot;);
 856     Register f_lo = src-&gt;as_register_lo();
 857     Register f_hi = src-&gt;as_register_hi();
 858     Register t_lo = dest-&gt;as_register_lo();
 859     Register t_hi = dest-&gt;as_register_hi();
 860 #ifdef _LP64
 861     assert(f_hi == f_lo, &quot;must be same&quot;);
 862     assert(t_hi == t_lo, &quot;must be same&quot;);
 863     move_regs(f_lo, t_lo);
 864 #else
 865     assert(f_lo != f_hi &amp;&amp; t_lo != t_hi, &quot;invalid register allocation&quot;);
 866 
 867 
 868     if (f_lo == t_hi &amp;&amp; f_hi == t_lo) {
 869       swap_reg(f_lo, f_hi);
 870     } else if (f_hi == t_lo) {
 871       assert(f_lo != t_hi, &quot;overwriting register&quot;);
 872       move_regs(f_hi, t_hi);
 873       move_regs(f_lo, t_lo);
 874     } else {
 875       assert(f_hi != t_lo, &quot;overwriting register&quot;);
 876       move_regs(f_lo, t_lo);
 877       move_regs(f_hi, t_hi);
 878     }
 879 #endif // LP64
 880 

 881     // special moves from fpu-register to xmm-register
 882     // necessary for method results
 883   } else if (src-&gt;is_single_xmm() &amp;&amp; !dest-&gt;is_single_xmm()) {
 884     __ movflt(Address(rsp, 0), src-&gt;as_xmm_float_reg());
 885     __ fld_s(Address(rsp, 0));
 886   } else if (src-&gt;is_double_xmm() &amp;&amp; !dest-&gt;is_double_xmm()) {
 887     __ movdbl(Address(rsp, 0), src-&gt;as_xmm_double_reg());
 888     __ fld_d(Address(rsp, 0));
 889   } else if (dest-&gt;is_single_xmm() &amp;&amp; !src-&gt;is_single_xmm()) {
 890     __ fstp_s(Address(rsp, 0));
 891     __ movflt(dest-&gt;as_xmm_float_reg(), Address(rsp, 0));
 892   } else if (dest-&gt;is_double_xmm() &amp;&amp; !src-&gt;is_double_xmm()) {
 893     __ fstp_d(Address(rsp, 0));
 894     __ movdbl(dest-&gt;as_xmm_double_reg(), Address(rsp, 0));

 895 
 896     // move between xmm-registers
 897   } else if (dest-&gt;is_single_xmm()) {
 898     assert(src-&gt;is_single_xmm(), &quot;must match&quot;);
 899     __ movflt(dest-&gt;as_xmm_float_reg(), src-&gt;as_xmm_float_reg());
 900   } else if (dest-&gt;is_double_xmm()) {
 901     assert(src-&gt;is_double_xmm(), &quot;must match&quot;);
 902     __ movdbl(dest-&gt;as_xmm_double_reg(), src-&gt;as_xmm_double_reg());
 903 

 904     // move between fpu-registers (no instruction necessary because of fpu-stack)
 905   } else if (dest-&gt;is_single_fpu() || dest-&gt;is_double_fpu()) {
 906     assert(src-&gt;is_single_fpu() || src-&gt;is_double_fpu(), &quot;must match&quot;);
 907     assert(src-&gt;fpu() == dest-&gt;fpu(), &quot;currently should be nothing to do&quot;);


 908   } else {
 909     ShouldNotReachHere();
 910   }
 911 }
 912 
 913 void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {
 914   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 915   assert(dest-&gt;is_stack(), &quot;should not call otherwise&quot;);
 916 
 917   if (src-&gt;is_single_cpu()) {
 918     Address dst = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
<span class="line-modified"> 919     if (type == T_OBJECT || type == T_ARRAY) {</span>
 920       __ verify_oop(src-&gt;as_register());
 921       __ movptr (dst, src-&gt;as_register());
<span class="line-modified"> 922     } else if (type == T_METADATA) {</span>
 923       __ movptr (dst, src-&gt;as_register());
 924     } else {
 925       __ movl (dst, src-&gt;as_register());
 926     }
 927 
 928   } else if (src-&gt;is_double_cpu()) {
 929     Address dstLO = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), lo_word_offset_in_bytes);
 930     Address dstHI = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), hi_word_offset_in_bytes);
 931     __ movptr (dstLO, src-&gt;as_register_lo());
 932     NOT_LP64(__ movptr (dstHI, src-&gt;as_register_hi()));
 933 
 934   } else if (src-&gt;is_single_xmm()) {
 935     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 936     __ movflt(dst_addr, src-&gt;as_xmm_float_reg());
 937 
 938   } else if (src-&gt;is_double_xmm()) {
 939     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
 940     __ movdbl(dst_addr, src-&gt;as_xmm_double_reg());
 941 

 942   } else if (src-&gt;is_single_fpu()) {
 943     assert(src-&gt;fpu_regnr() == 0, &quot;argument must be on TOS&quot;);
 944     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 945     if (pop_fpu_stack)     __ fstp_s (dst_addr);
 946     else                   __ fst_s  (dst_addr);
 947 
 948   } else if (src-&gt;is_double_fpu()) {
 949     assert(src-&gt;fpu_regnrLo() == 0, &quot;argument must be on TOS&quot;);
 950     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
 951     if (pop_fpu_stack)     __ fstp_d (dst_addr);
 952     else                   __ fst_d  (dst_addr);

 953 
 954   } else {
 955     ShouldNotReachHere();
 956   }
 957 }
 958 
 959 
 960 void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide, bool /* unaligned */) {
 961   LIR_Address* to_addr = dest-&gt;as_address_ptr();
 962   PatchingStub* patch = NULL;
 963   Register compressed_src = rscratch1;
 964 
<span class="line-modified"> 965   if (type == T_ARRAY || type == T_OBJECT) {</span>
 966     __ verify_oop(src-&gt;as_register());
 967 #ifdef _LP64
 968     if (UseCompressedOops &amp;&amp; !wide) {
 969       __ movptr(compressed_src, src-&gt;as_register());
 970       __ encode_heap_oop(compressed_src);
 971       if (patch_code != lir_patch_none) {
 972         info-&gt;oop_map()-&gt;set_narrowoop(compressed_src-&gt;as_VMReg());
 973       }
 974     }
 975 #endif
 976   }
 977 
 978   if (patch_code != lir_patch_none) {
 979     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 980     Address toa = as_Address(to_addr);
 981     assert(toa.disp() != 0, &quot;must have&quot;);
 982   }
 983 
 984   int null_check_here = code_offset();
 985   switch (type) {
 986     case T_FLOAT: {




 987       if (src-&gt;is_single_xmm()) {
 988         __ movflt(as_Address(to_addr), src-&gt;as_xmm_float_reg());
 989       } else {
 990         assert(src-&gt;is_single_fpu(), &quot;must be&quot;);
 991         assert(src-&gt;fpu_regnr() == 0, &quot;argument must be on TOS&quot;);
 992         if (pop_fpu_stack)      __ fstp_s(as_Address(to_addr));
 993         else                    __ fst_s (as_Address(to_addr));
 994       }

 995       break;
 996     }
 997 
 998     case T_DOUBLE: {




 999       if (src-&gt;is_double_xmm()) {
1000         __ movdbl(as_Address(to_addr), src-&gt;as_xmm_double_reg());
1001       } else {
1002         assert(src-&gt;is_double_fpu(), &quot;must be&quot;);
1003         assert(src-&gt;fpu_regnrLo() == 0, &quot;argument must be on TOS&quot;);
1004         if (pop_fpu_stack)      __ fstp_d(as_Address(to_addr));
1005         else                    __ fst_d (as_Address(to_addr));
1006       }

1007       break;
1008     }
1009 
1010     case T_ARRAY:   // fall through
1011     case T_OBJECT:  // fall through
1012       if (UseCompressedOops &amp;&amp; !wide) {
1013         __ movl(as_Address(to_addr), compressed_src);
1014       } else {
1015         __ movptr(as_Address(to_addr), src-&gt;as_register());
1016       }
1017       break;
1018     case T_METADATA:
1019       // We get here to store a method pointer to the stack to pass to
1020       // a dtrace runtime call. This can&#39;t work on 64 bit with
1021       // compressed klass ptrs: T_METADATA can be a compressed klass
1022       // ptr or a 64 bit method pointer.
1023       LP64_ONLY(ShouldNotReachHere());
1024       __ movptr(as_Address(to_addr), src-&gt;as_register());
1025       break;
1026     case T_ADDRESS:
</pre>
<hr />
<pre>
1080       break;
1081 
1082     default:
1083       ShouldNotReachHere();
1084   }
1085   if (info != NULL) {
1086     add_debug_info_for_null_check(null_check_here, info);
1087   }
1088 
1089   if (patch_code != lir_patch_none) {
1090     patching_epilog(patch, patch_code, to_addr-&gt;base()-&gt;as_register(), info);
1091   }
1092 }
1093 
1094 
1095 void LIR_Assembler::stack2reg(LIR_Opr src, LIR_Opr dest, BasicType type) {
1096   assert(src-&gt;is_stack(), &quot;should not call otherwise&quot;);
1097   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
1098 
1099   if (dest-&gt;is_single_cpu()) {
<span class="line-modified">1100     if (type == T_ARRAY || type == T_OBJECT) {</span>
1101       __ movptr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
1102       __ verify_oop(dest-&gt;as_register());
<span class="line-modified">1103     } else if (type == T_METADATA) {</span>
1104       __ movptr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
1105     } else {
1106       __ movl(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
1107     }
1108 
1109   } else if (dest-&gt;is_double_cpu()) {
1110     Address src_addr_LO = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), lo_word_offset_in_bytes);
1111     Address src_addr_HI = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), hi_word_offset_in_bytes);
1112     __ movptr(dest-&gt;as_register_lo(), src_addr_LO);
1113     NOT_LP64(__ movptr(dest-&gt;as_register_hi(), src_addr_HI));
1114 
1115   } else if (dest-&gt;is_single_xmm()) {
1116     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
1117     __ movflt(dest-&gt;as_xmm_float_reg(), src_addr);
1118 
1119   } else if (dest-&gt;is_double_xmm()) {
1120     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
1121     __ movdbl(dest-&gt;as_xmm_double_reg(), src_addr);
1122 

1123   } else if (dest-&gt;is_single_fpu()) {
1124     assert(dest-&gt;fpu_regnr() == 0, &quot;dest must be TOS&quot;);
1125     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
1126     __ fld_s(src_addr);
1127 
1128   } else if (dest-&gt;is_double_fpu()) {
1129     assert(dest-&gt;fpu_regnrLo() == 0, &quot;dest must be TOS&quot;);
1130     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
1131     __ fld_d(src_addr);

1132 
1133   } else {
1134     ShouldNotReachHere();
1135   }
1136 }
1137 
1138 
1139 void LIR_Assembler::stack2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {
1140   if (src-&gt;is_single_stack()) {
<span class="line-modified">1141     if (type == T_OBJECT || type == T_ARRAY) {</span>
1142       __ pushptr(frame_map()-&gt;address_for_slot(src -&gt;single_stack_ix()));
1143       __ popptr (frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
1144     } else {
1145 #ifndef _LP64
1146       __ pushl(frame_map()-&gt;address_for_slot(src -&gt;single_stack_ix()));
1147       __ popl (frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
1148 #else
1149       //no pushl on 64bits
1150       __ movl(rscratch1, frame_map()-&gt;address_for_slot(src -&gt;single_stack_ix()));
1151       __ movl(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), rscratch1);
1152 #endif
1153     }
1154 
1155   } else if (src-&gt;is_double_stack()) {
1156 #ifdef _LP64
1157     __ pushptr(frame_map()-&gt;address_for_slot(src -&gt;double_stack_ix()));
1158     __ popptr (frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix()));
1159 #else
1160     __ pushl(frame_map()-&gt;address_for_slot(src -&gt;double_stack_ix(), 0));
1161     // push and pop the part at src + wordSize, adding wordSize for the previous push
</pre>
<hr />
<pre>
1195       }
1196       break;
1197    default:
1198      break;
1199   }
1200 
1201   PatchingStub* patch = NULL;
1202   if (patch_code != lir_patch_none) {
1203     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
1204     assert(from_addr.disp() != 0, &quot;must have&quot;);
1205   }
1206   if (info != NULL) {
1207     add_debug_info_for_null_check_here(info);
1208   }
1209 
1210   switch (type) {
1211     case T_FLOAT: {
1212       if (dest-&gt;is_single_xmm()) {
1213         __ movflt(dest-&gt;as_xmm_float_reg(), from_addr);
1214       } else {

1215         assert(dest-&gt;is_single_fpu(), &quot;must be&quot;);
1216         assert(dest-&gt;fpu_regnr() == 0, &quot;dest must be TOS&quot;);
1217         __ fld_s(from_addr);



1218       }
1219       break;
1220     }
1221 
1222     case T_DOUBLE: {
1223       if (dest-&gt;is_double_xmm()) {
1224         __ movdbl(dest-&gt;as_xmm_double_reg(), from_addr);
1225       } else {

1226         assert(dest-&gt;is_double_fpu(), &quot;must be&quot;);
1227         assert(dest-&gt;fpu_regnrLo() == 0, &quot;dest must be TOS&quot;);
1228         __ fld_d(from_addr);



1229       }
1230       break;
1231     }
1232 
1233     case T_OBJECT:  // fall through
1234     case T_ARRAY:   // fall through
1235       if (UseCompressedOops &amp;&amp; !wide) {
1236         __ movl(dest-&gt;as_register(), from_addr);
1237       } else {
1238         __ movptr(dest-&gt;as_register(), from_addr);
1239       }
1240       break;
1241 
1242     case T_ADDRESS:
1243       if (UseCompressedClassPointers &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1244         __ movl(dest-&gt;as_register(), from_addr);
1245       } else {
1246         __ movptr(dest-&gt;as_register(), from_addr);
1247       }
1248       break;
</pre>
<hr />
<pre>
1322     case T_SHORT: {
1323       Register dest_reg = dest-&gt;as_register();
1324       if (VM_Version::is_P6() || from_addr.uses(dest_reg)) {
1325         __ movswl(dest_reg, from_addr);
1326       } else {
1327         __ movw(dest_reg, from_addr);
1328         __ shll(dest_reg, 16);
1329         __ sarl(dest_reg, 16);
1330       }
1331       break;
1332     }
1333 
1334     default:
1335       ShouldNotReachHere();
1336   }
1337 
1338   if (patch != NULL) {
1339     patching_epilog(patch, patch_code, addr-&gt;base()-&gt;as_register(), info);
1340   }
1341 
<span class="line-modified">1342   if (type == T_ARRAY || type == T_OBJECT) {</span>
1343 #ifdef _LP64
1344     if (UseCompressedOops &amp;&amp; !wide) {
1345       __ decode_heap_oop(dest-&gt;as_register());
1346     }
1347 #endif
1348 
1349     // Load barrier has not yet been applied, so ZGC can&#39;t verify the oop here
1350     if (!UseZGC) {
1351       __ verify_oop(dest-&gt;as_register());
1352     }
1353   } else if (type == T_ADDRESS &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1354 #ifdef _LP64
1355     if (UseCompressedClassPointers) {
1356       __ decode_klass_not_null(dest-&gt;as_register());
1357     }
1358 #endif
1359   }
1360 }
1361 
1362 
</pre>
<hr />
<pre>
1464       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
1465 #endif
1466       break;
1467 
1468     case Bytecodes::_i2b:
1469       move_regs(src-&gt;as_register(), dest-&gt;as_register());
1470       __ sign_extend_byte(dest-&gt;as_register());
1471       break;
1472 
1473     case Bytecodes::_i2c:
1474       move_regs(src-&gt;as_register(), dest-&gt;as_register());
1475       __ andl(dest-&gt;as_register(), 0xFFFF);
1476       break;
1477 
1478     case Bytecodes::_i2s:
1479       move_regs(src-&gt;as_register(), dest-&gt;as_register());
1480       __ sign_extend_short(dest-&gt;as_register());
1481       break;
1482 
1483 









































1484     case Bytecodes::_f2d:
1485     case Bytecodes::_d2f:
1486       if (dest-&gt;is_single_xmm()) {
1487         __ cvtsd2ss(dest-&gt;as_xmm_float_reg(), src-&gt;as_xmm_double_reg());
1488       } else if (dest-&gt;is_double_xmm()) {
1489         __ cvtss2sd(dest-&gt;as_xmm_double_reg(), src-&gt;as_xmm_float_reg());
1490       } else {
1491         assert(src-&gt;fpu() == dest-&gt;fpu(), &quot;register must be equal&quot;);
1492         // do nothing (float result is rounded later through spilling)
1493       }
1494       break;
1495 
1496     case Bytecodes::_i2f:
1497     case Bytecodes::_i2d:
1498       if (dest-&gt;is_single_xmm()) {
1499         __ cvtsi2ssl(dest-&gt;as_xmm_float_reg(), src-&gt;as_register());
1500       } else if (dest-&gt;is_double_xmm()) {
1501         __ cvtsi2sdl(dest-&gt;as_xmm_double_reg(), src-&gt;as_register());
1502       } else {
1503         assert(dest-&gt;fpu() == 0, &quot;result must be on TOS&quot;);
1504         __ movl(Address(rsp, 0), src-&gt;as_register());
1505         __ fild_s(Address(rsp, 0));
1506       }
1507       break;
1508 










1509     case Bytecodes::_f2i:
1510     case Bytecodes::_d2i:
1511       if (src-&gt;is_single_xmm()) {
1512         __ cvttss2sil(dest-&gt;as_register(), src-&gt;as_xmm_float_reg());
1513       } else if (src-&gt;is_double_xmm()) {
1514         __ cvttsd2sil(dest-&gt;as_register(), src-&gt;as_xmm_double_reg());
1515       } else {
1516         assert(src-&gt;fpu() == 0, &quot;input must be on TOS&quot;);
1517         __ fldcw(ExternalAddress(StubRoutines::addr_fpu_cntrl_wrd_trunc()));
1518         __ fist_s(Address(rsp, 0));
1519         __ movl(dest-&gt;as_register(), Address(rsp, 0));
1520         __ fldcw(ExternalAddress(StubRoutines::addr_fpu_cntrl_wrd_std()));
1521       }
<span class="line-removed">1522 </span>
1523       // IA32 conversion instructions do not match JLS for overflow, underflow and NaN -&gt; fixup in stub
1524       assert(op-&gt;stub() != NULL, &quot;stub required&quot;);
1525       __ cmpl(dest-&gt;as_register(), 0x80000000);
1526       __ jcc(Assembler::equal, *op-&gt;stub()-&gt;entry());
1527       __ bind(*op-&gt;stub()-&gt;continuation());
1528       break;
1529 
<span class="line-removed">1530     case Bytecodes::_l2f:</span>
<span class="line-removed">1531     case Bytecodes::_l2d:</span>
<span class="line-removed">1532       assert(!dest-&gt;is_xmm_register(), &quot;result in xmm register not supported (no SSE instruction present)&quot;);</span>
<span class="line-removed">1533       assert(dest-&gt;fpu() == 0, &quot;result must be on TOS&quot;);</span>
<span class="line-removed">1534 </span>
<span class="line-removed">1535       __ movptr(Address(rsp, 0),            src-&gt;as_register_lo());</span>
<span class="line-removed">1536       NOT_LP64(__ movl(Address(rsp, BytesPerWord), src-&gt;as_register_hi()));</span>
<span class="line-removed">1537       __ fild_d(Address(rsp, 0));</span>
<span class="line-removed">1538       // float result is rounded later through spilling</span>
<span class="line-removed">1539       break;</span>
<span class="line-removed">1540 </span>
1541     case Bytecodes::_f2l:
1542     case Bytecodes::_d2l:
1543       assert(!src-&gt;is_xmm_register(), &quot;input in xmm register not supported (no SSE instruction present)&quot;);
1544       assert(src-&gt;fpu() == 0, &quot;input must be on TOS&quot;);
1545       assert(dest == FrameMap::long0_opr, &quot;runtime stub places result in these registers&quot;);
1546 
1547       // instruction sequence too long to inline it here
1548       {
1549         __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::fpu2long_stub_id)));
1550       }
1551       break;

1552 
1553     default: ShouldNotReachHere();
1554   }
1555 }
1556 
1557 void LIR_Assembler::emit_alloc_obj(LIR_OpAllocObj* op) {
1558   if (op-&gt;init_check()) {
1559     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
1560     __ cmpb(Address(op-&gt;klass()-&gt;as_register(),
1561                     InstanceKlass::init_state_offset()),
1562                     InstanceKlass::fully_initialized);
1563     __ jcc(Assembler::notEqual, *op-&gt;stub()-&gt;entry());
1564   }
1565   __ allocate_object(op-&gt;obj()-&gt;as_register(),
1566                      op-&gt;tmp1()-&gt;as_register(),
1567                      op-&gt;tmp2()-&gt;as_register(),
1568                      op-&gt;header_size(),
1569                      op-&gt;object_size(),
1570                      op-&gt;klass()-&gt;as_register(),
1571                      *op-&gt;stub()-&gt;entry());
1572   __ bind(*op-&gt;stub()-&gt;continuation());
1573 }
1574 
1575 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
1576   Register len =  op-&gt;len()-&gt;as_register();
1577   LP64_ONLY( __ movslq(len, len); )
1578 
1579   if (UseSlowPath ||
<span class="line-modified">1580       (!UseFastNewObjectArray &amp;&amp; (op-&gt;type() == T_OBJECT || op-&gt;type() == T_ARRAY)) ||</span>
<span class="line-modified">1581       (!UseFastNewTypeArray   &amp;&amp; (op-&gt;type() != T_OBJECT &amp;&amp; op-&gt;type() != T_ARRAY))) {</span>
1582     __ jmp(*op-&gt;stub()-&gt;entry());
1583   } else {
1584     Register tmp1 = op-&gt;tmp1()-&gt;as_register();
1585     Register tmp2 = op-&gt;tmp2()-&gt;as_register();
1586     Register tmp3 = op-&gt;tmp3()-&gt;as_register();
1587     if (len == tmp1) {
1588       tmp1 = tmp3;
1589     } else if (len == tmp2) {
1590       tmp2 = tmp3;
1591     } else if (len == tmp3) {
1592       // everything is ok
1593     } else {
1594       __ mov(tmp3, len);
1595     }
1596     __ allocate_array(op-&gt;obj()-&gt;as_register(),
1597                       len,
1598                       tmp1,
1599                       tmp2,
1600                       arrayOopDesc::header_size(op-&gt;type()),
1601                       array_element_size(op-&gt;type()),
</pre>
<hr />
<pre>
2191       Address raddr;
2192       if (right-&gt;is_double_stack()) {
2193         raddr = frame_map()-&gt;address_for_slot(right-&gt;double_stack_ix());
2194       } else if (right-&gt;is_constant()) {
2195         // hack for now
2196         raddr = __ as_Address(InternalAddress(double_constant(right-&gt;as_jdouble())));
2197       } else {
2198         ShouldNotReachHere();
2199       }
2200       switch (code) {
2201         case lir_add: __ addsd(lreg, raddr);  break;
2202         case lir_sub: __ subsd(lreg, raddr);  break;
2203         case lir_mul_strictfp: // fall through
2204         case lir_mul: __ mulsd(lreg, raddr);  break;
2205         case lir_div_strictfp: // fall through
2206         case lir_div: __ divsd(lreg, raddr);  break;
2207         default: ShouldNotReachHere();
2208       }
2209     }
2210 

2211   } else if (left-&gt;is_single_fpu()) {
2212     assert(dest-&gt;is_single_fpu(),  &quot;fpu stack allocation required&quot;);
2213 
2214     if (right-&gt;is_single_fpu()) {
2215       arith_fpu_implementation(code, left-&gt;fpu_regnr(), right-&gt;fpu_regnr(), dest-&gt;fpu_regnr(), pop_fpu_stack);
2216 
2217     } else {
2218       assert(left-&gt;fpu_regnr() == 0, &quot;left must be on TOS&quot;);
2219       assert(dest-&gt;fpu_regnr() == 0, &quot;dest must be on TOS&quot;);
2220 
2221       Address raddr;
2222       if (right-&gt;is_single_stack()) {
2223         raddr = frame_map()-&gt;address_for_slot(right-&gt;single_stack_ix());
2224       } else if (right-&gt;is_constant()) {
2225         address const_addr = float_constant(right-&gt;as_jfloat());
2226         assert(const_addr != NULL, &quot;incorrect float/double constant maintainance&quot;);
2227         // hack for now
2228         raddr = __ as_Address(InternalAddress(const_addr));
2229       } else {
2230         ShouldNotReachHere();
</pre>
<hr />
<pre>
2266       } else {
2267         ShouldNotReachHere();
2268       }
2269 
2270       switch (code) {
2271         case lir_add: __ fadd_d(raddr); break;
2272         case lir_sub: __ fsub_d(raddr); break;
2273         case lir_mul_strictfp: // fall through
2274         case lir_mul: __ fmul_d(raddr); break;
2275         case lir_div_strictfp: // fall through
2276         case lir_div: __ fdiv_d(raddr); break;
2277         default: ShouldNotReachHere();
2278       }
2279     }
2280 
2281     if (code == lir_mul_strictfp || code == lir_div_strictfp) {
2282       // Double values require special handling for strictfp mul/div on x86
2283       __ fld_x(ExternalAddress(StubRoutines::addr_fpu_subnormal_bias2()));
2284       __ fmulp(dest-&gt;fpu_regnrLo() + 1);
2285     }

2286 
2287   } else if (left-&gt;is_single_stack() || left-&gt;is_address()) {
2288     assert(left == dest, &quot;left and dest must be equal&quot;);
2289 
2290     Address laddr;
2291     if (left-&gt;is_single_stack()) {
2292       laddr = frame_map()-&gt;address_for_slot(left-&gt;single_stack_ix());
2293     } else if (left-&gt;is_address()) {
2294       laddr = as_Address(left-&gt;as_address_ptr());
2295     } else {
2296       ShouldNotReachHere();
2297     }
2298 
2299     if (right-&gt;is_single_cpu()) {
2300       Register rreg = right-&gt;as_register();
2301       switch (code) {
2302         case lir_add: __ addl(laddr, rreg); break;
2303         case lir_sub: __ subl(laddr, rreg); break;
2304         default:      ShouldNotReachHere();
2305       }
</pre>
<hr />
<pre>
2308       switch (code) {
2309         case lir_add: {
2310           __ incrementl(laddr, c);
2311           break;
2312         }
2313         case lir_sub: {
2314           __ decrementl(laddr, c);
2315           break;
2316         }
2317         default: ShouldNotReachHere();
2318       }
2319     } else {
2320       ShouldNotReachHere();
2321     }
2322 
2323   } else {
2324     ShouldNotReachHere();
2325   }
2326 }
2327 

2328 void LIR_Assembler::arith_fpu_implementation(LIR_Code code, int left_index, int right_index, int dest_index, bool pop_fpu_stack) {
2329   assert(pop_fpu_stack  || (left_index     == dest_index || right_index     == dest_index), &quot;invalid LIR&quot;);
2330   assert(!pop_fpu_stack || (left_index - 1 == dest_index || right_index - 1 == dest_index), &quot;invalid LIR&quot;);
2331   assert(left_index == 0 || right_index == 0, &quot;either must be on top of stack&quot;);
2332 
2333   bool left_is_tos = (left_index == 0);
2334   bool dest_is_tos = (dest_index == 0);
2335   int non_tos_index = (left_is_tos ? right_index : left_index);
2336 
2337   switch (code) {
2338     case lir_add:
2339       if (pop_fpu_stack)       __ faddp(non_tos_index);
2340       else if (dest_is_tos)    __ fadd (non_tos_index);
2341       else                     __ fadda(non_tos_index);
2342       break;
2343 
2344     case lir_sub:
2345       if (left_is_tos) {
2346         if (pop_fpu_stack)     __ fsubrp(non_tos_index);
2347         else if (dest_is_tos)  __ fsub  (non_tos_index);
</pre>
<hr />
<pre>
2365       if (left_is_tos) {
2366         if (pop_fpu_stack)     __ fdivrp(non_tos_index);
2367         else if (dest_is_tos)  __ fdiv  (non_tos_index);
2368         else                   __ fdivra(non_tos_index);
2369       } else {
2370         if (pop_fpu_stack)     __ fdivp (non_tos_index);
2371         else if (dest_is_tos)  __ fdivr (non_tos_index);
2372         else                   __ fdiva (non_tos_index);
2373       }
2374       break;
2375 
2376     case lir_rem:
2377       assert(left_is_tos &amp;&amp; dest_is_tos &amp;&amp; right_index == 1, &quot;must be guaranteed by FPU stack allocation&quot;);
2378       __ fremr(noreg);
2379       break;
2380 
2381     default:
2382       ShouldNotReachHere();
2383   }
2384 }

2385 
2386 
2387 void LIR_Assembler::intrinsic_op(LIR_Code code, LIR_Opr value, LIR_Opr tmp, LIR_Opr dest, LIR_Op* op) {
2388   if (value-&gt;is_double_xmm()) {
2389     switch(code) {
2390       case lir_abs :
2391         {
2392 #ifdef _LP64
2393           if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512vl()) {
2394             assert(tmp-&gt;is_valid(), &quot;need temporary&quot;);
2395             __ vpandn(dest-&gt;as_xmm_double_reg(), tmp-&gt;as_xmm_double_reg(), value-&gt;as_xmm_double_reg(), 2);
2396           } else
2397 #endif
2398           {
2399             if (dest-&gt;as_xmm_double_reg() != value-&gt;as_xmm_double_reg()) {
2400               __ movdbl(dest-&gt;as_xmm_double_reg(), value-&gt;as_xmm_double_reg());
2401             }
2402             assert(!tmp-&gt;is_valid(), &quot;do not need temporary&quot;);
2403             __ andpd(dest-&gt;as_xmm_double_reg(),
2404                      ExternalAddress((address)double_signmask_pool));
2405           }
2406         }
2407         break;
2408 
2409       case lir_sqrt: __ sqrtsd(dest-&gt;as_xmm_double_reg(), value-&gt;as_xmm_double_reg()); break;
2410       // all other intrinsics are not available in the SSE instruction set, so FPU is used
2411       default      : ShouldNotReachHere();
2412     }
2413 

2414   } else if (value-&gt;is_double_fpu()) {
2415     assert(value-&gt;fpu_regnrLo() == 0 &amp;&amp; dest-&gt;fpu_regnrLo() == 0, &quot;both must be on TOS&quot;);
2416     switch(code) {
2417       case lir_abs   : __ fabs() ; break;
2418       case lir_sqrt  : __ fsqrt(); break;
2419       default      : ShouldNotReachHere();
2420     }

2421   } else {
2422     Unimplemented();
2423   }
2424 }
2425 
2426 void LIR_Assembler::logic_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst) {
2427   // assert(left-&gt;destroys_register(), &quot;check&quot;);
2428   if (left-&gt;is_single_cpu()) {
2429     Register reg = left-&gt;as_register();
2430     if (right-&gt;is_constant()) {
2431       int val = right-&gt;as_constant_ptr()-&gt;as_jint();
2432       switch (code) {
2433         case lir_logic_and: __ andl (reg, val); break;
2434         case lir_logic_or:  __ orl  (reg, val); break;
2435         case lir_logic_xor: __ xorl (reg, val); break;
2436         default: ShouldNotReachHere();
2437       }
2438     } else if (right-&gt;is_stack()) {
2439       // added support for stack operands
2440       Address raddr = frame_map()-&gt;address_for_slot(right-&gt;single_stack_ix());
</pre>
<hr />
<pre>
2477       int r_hi = right-&gt;as_constant_ptr()-&gt;as_jint_hi();
2478       switch (code) {
2479         case lir_logic_and:
2480           __ andl(l_lo, r_lo);
2481           __ andl(l_hi, r_hi);
2482           break;
2483         case lir_logic_or:
2484           __ orl(l_lo, r_lo);
2485           __ orl(l_hi, r_hi);
2486           break;
2487         case lir_logic_xor:
2488           __ xorl(l_lo, r_lo);
2489           __ xorl(l_hi, r_hi);
2490           break;
2491         default: ShouldNotReachHere();
2492       }
2493 #endif // _LP64
2494     } else {
2495 #ifdef _LP64
2496       Register r_lo;
<span class="line-modified">2497       if (right-&gt;type() == T_OBJECT || right-&gt;type() == T_ARRAY) {</span>
2498         r_lo = right-&gt;as_register();
2499       } else {
2500         r_lo = right-&gt;as_register_lo();
2501       }
2502 #else
2503       Register r_lo = right-&gt;as_register_lo();
2504       Register r_hi = right-&gt;as_register_hi();
2505       assert(l_lo != r_hi, &quot;overwriting registers&quot;);
2506 #endif
2507       switch (code) {
2508         case lir_logic_and:
2509           __ andptr(l_lo, r_lo);
2510           NOT_LP64(__ andptr(l_hi, r_hi);)
2511           break;
2512         case lir_logic_or:
2513           __ orptr(l_lo, r_lo);
2514           NOT_LP64(__ orptr(l_hi, r_hi);)
2515           break;
2516         case lir_logic_xor:
2517           __ xorptr(l_lo, r_lo);
</pre>
<hr />
<pre>
2590     move_regs(lreg, rax);
2591 
2592     int idivl_offset = __ corrected_idivl(rreg);
2593     if (ImplicitDiv0Checks) {
2594       add_debug_info_for_div0(idivl_offset, info);
2595     }
2596     if (code == lir_irem) {
2597       move_regs(rdx, dreg); // result is in rdx
2598     } else {
2599       move_regs(rax, dreg);
2600     }
2601   }
2602 }
2603 
2604 
2605 void LIR_Assembler::comp_op(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Op2* op) {
2606   if (opr1-&gt;is_single_cpu()) {
2607     Register reg1 = opr1-&gt;as_register();
2608     if (opr2-&gt;is_single_cpu()) {
2609       // cpu register - cpu register
<span class="line-modified">2610       if (opr1-&gt;type() == T_OBJECT || opr1-&gt;type() == T_ARRAY) {</span>
2611         __ cmpoop(reg1, opr2-&gt;as_register());
2612       } else {
<span class="line-modified">2613         assert(opr2-&gt;type() != T_OBJECT &amp;&amp; opr2-&gt;type() != T_ARRAY, &quot;cmp int, oop?&quot;);</span>
2614         __ cmpl(reg1, opr2-&gt;as_register());
2615       }
2616     } else if (opr2-&gt;is_stack()) {
2617       // cpu register - stack
<span class="line-modified">2618       if (opr1-&gt;type() == T_OBJECT || opr1-&gt;type() == T_ARRAY) {</span>
2619         __ cmpoop(reg1, frame_map()-&gt;address_for_slot(opr2-&gt;single_stack_ix()));
2620       } else {
2621         __ cmpl(reg1, frame_map()-&gt;address_for_slot(opr2-&gt;single_stack_ix()));
2622       }
2623     } else if (opr2-&gt;is_constant()) {
2624       // cpu register - constant
2625       LIR_Const* c = opr2-&gt;as_constant_ptr();
2626       if (c-&gt;type() == T_INT) {
2627         __ cmpl(reg1, c-&gt;as_jint());
<span class="line-modified">2628       } else if (c-&gt;type() == T_OBJECT || c-&gt;type() == T_ARRAY) {</span>









2629         // In 64bit oops are single register
2630         jobject o = c-&gt;as_jobject();
2631         if (o == NULL) {
2632           __ cmpptr(reg1, (int32_t)NULL_WORD);
2633         } else {
2634           __ cmpoop(reg1, o);
2635         }
2636       } else {
2637         fatal(&quot;unexpected type: %s&quot;, basictype_to_str(c-&gt;type()));
2638       }
2639       // cpu register - address
2640     } else if (opr2-&gt;is_address()) {
2641       if (op-&gt;info() != NULL) {
2642         add_debug_info_for_null_check_here(op-&gt;info());
2643       }
2644       __ cmpl(reg1, as_Address(opr2-&gt;as_address_ptr()));
2645     } else {
2646       ShouldNotReachHere();
2647     }
2648 
</pre>
<hr />
<pre>
2700     XMMRegister reg1 = opr1-&gt;as_xmm_double_reg();
2701     if (opr2-&gt;is_double_xmm()) {
2702       // xmm register - xmm register
2703       __ ucomisd(reg1, opr2-&gt;as_xmm_double_reg());
2704     } else if (opr2-&gt;is_stack()) {
2705       // xmm register - stack
2706       __ ucomisd(reg1, frame_map()-&gt;address_for_slot(opr2-&gt;double_stack_ix()));
2707     } else if (opr2-&gt;is_constant()) {
2708       // xmm register - constant
2709       __ ucomisd(reg1, InternalAddress(double_constant(opr2-&gt;as_jdouble())));
2710     } else if (opr2-&gt;is_address()) {
2711       // xmm register - address
2712       if (op-&gt;info() != NULL) {
2713         add_debug_info_for_null_check_here(op-&gt;info());
2714       }
2715       __ ucomisd(reg1, as_Address(opr2-&gt;pointer()-&gt;as_address()));
2716     } else {
2717       ShouldNotReachHere();
2718     }
2719 

2720   } else if(opr1-&gt;is_single_fpu() || opr1-&gt;is_double_fpu()) {
2721     assert(opr1-&gt;is_fpu_register() &amp;&amp; opr1-&gt;fpu() == 0, &quot;currently left-hand side must be on TOS (relax this restriction)&quot;);
2722     assert(opr2-&gt;is_fpu_register(), &quot;both must be registers&quot;);
2723     __ fcmp(noreg, opr2-&gt;fpu(), op-&gt;fpu_pop_count() &gt; 0, op-&gt;fpu_pop_count() &gt; 1);

2724 
2725   } else if (opr1-&gt;is_address() &amp;&amp; opr2-&gt;is_constant()) {
2726     LIR_Const* c = opr2-&gt;as_constant_ptr();
2727 #ifdef _LP64
<span class="line-modified">2728     if (c-&gt;type() == T_OBJECT || c-&gt;type() == T_ARRAY) {</span>
2729       assert(condition == lir_cond_equal || condition == lir_cond_notEqual, &quot;need to reverse&quot;);
2730       __ movoop(rscratch1, c-&gt;as_jobject());
2731     }
2732 #endif // LP64
2733     if (op-&gt;info() != NULL) {
2734       add_debug_info_for_null_check_here(op-&gt;info());
2735     }
2736     // special case: address - constant
2737     LIR_Address* addr = opr1-&gt;as_address_ptr();
2738     if (c-&gt;type() == T_INT) {
2739       __ cmpl(as_Address(addr), c-&gt;as_jint());
<span class="line-modified">2740     } else if (c-&gt;type() == T_OBJECT || c-&gt;type() == T_ARRAY) {</span>
2741 #ifdef _LP64
2742       // %%% Make this explode if addr isn&#39;t reachable until we figure out a
2743       // better strategy by giving noreg as the temp for as_Address
2744       __ cmpoop(rscratch1, as_Address(addr, noreg));
2745 #else
2746       __ cmpoop(as_Address(addr), c-&gt;as_jobject());
2747 #endif // _LP64
2748     } else {
2749       ShouldNotReachHere();
2750     }
2751 
2752   } else {
2753     ShouldNotReachHere();
2754   }
2755 }
2756 
2757 void LIR_Assembler::comp_fl2i(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst, LIR_Op2* op) {
2758   if (code == lir_cmp_fd2i || code == lir_ucmp_fd2i) {
2759     if (left-&gt;is_single_xmm()) {
2760       assert(right-&gt;is_single_xmm(), &quot;must match&quot;);
2761       __ cmpss2int(left-&gt;as_xmm_float_reg(), right-&gt;as_xmm_float_reg(), dst-&gt;as_register(), code == lir_ucmp_fd2i);
2762     } else if (left-&gt;is_double_xmm()) {
2763       assert(right-&gt;is_double_xmm(), &quot;must match&quot;);
2764       __ cmpsd2int(left-&gt;as_xmm_double_reg(), right-&gt;as_xmm_double_reg(), dst-&gt;as_register(), code == lir_ucmp_fd2i);
2765 
2766     } else {



2767       assert(left-&gt;is_single_fpu() || left-&gt;is_double_fpu(), &quot;must be&quot;);
2768       assert(right-&gt;is_single_fpu() || right-&gt;is_double_fpu(), &quot;must match&quot;);
2769 
2770       assert(left-&gt;fpu() == 0, &quot;left must be on TOS&quot;);
2771       __ fcmp2int(dst-&gt;as_register(), code == lir_ucmp_fd2i, right-&gt;fpu(),
2772                   op-&gt;fpu_pop_count() &gt; 0, op-&gt;fpu_pop_count() &gt; 1);

2773     }
2774   } else {
2775     assert(code == lir_cmp_l2i, &quot;check&quot;);
2776 #ifdef _LP64
2777     Label done;
2778     Register dest = dst-&gt;as_register();
2779     __ cmpptr(left-&gt;as_register_lo(), right-&gt;as_register_lo());
2780     __ movl(dest, -1);
2781     __ jccb(Assembler::less, done);
2782     __ set_byte_if_not_zero(dest);
2783     __ movzbl(dest, dest);
2784     __ bind(done);
2785 #else
2786     __ lcmp2int(left-&gt;as_register_hi(),
2787                 left-&gt;as_register_lo(),
2788                 right-&gt;as_register_hi(),
2789                 right-&gt;as_register_lo());
2790     move_regs(left-&gt;as_register_hi(), dst-&gt;as_register());
2791 #endif // _LP64
2792   }
</pre>
<hr />
<pre>
3019 
3020 
3021 // This code replaces a call to arraycopy; no exception may
3022 // be thrown in this code, they must be thrown in the System.arraycopy
3023 // activation frame; we could save some checks if this would not be the case
3024 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
3025   ciArrayKlass* default_type = op-&gt;expected_type();
3026   Register src = op-&gt;src()-&gt;as_register();
3027   Register dst = op-&gt;dst()-&gt;as_register();
3028   Register src_pos = op-&gt;src_pos()-&gt;as_register();
3029   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
3030   Register length  = op-&gt;length()-&gt;as_register();
3031   Register tmp = op-&gt;tmp()-&gt;as_register();
3032 
3033   __ resolve(ACCESS_READ, src);
3034   __ resolve(ACCESS_WRITE, dst);
3035 
3036   CodeStub* stub = op-&gt;stub();
3037   int flags = op-&gt;flags();
3038   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
<span class="line-modified">3039   if (basic_type == T_ARRAY) basic_type = T_OBJECT;</span>
3040 
3041   // if we don&#39;t know anything, just go through the generic arraycopy
3042   if (default_type == NULL) {
3043     // save outgoing arguments on stack in case call to System.arraycopy is needed
3044     // HACK ALERT. This code used to push the parameters in a hardwired fashion
3045     // for interpreter calling conventions. Now we have to do it in new style conventions.
3046     // For the moment until C1 gets the new register allocator I just force all the
3047     // args to the right place (except the register args) and then on the back side
3048     // reload the register args properly if we go slow path. Yuck
3049 
3050     // These are proper for the calling convention
3051     store_parameter(length, 2);
3052     store_parameter(dst_pos, 1);
3053     store_parameter(dst, 0);
3054 
3055     // these are just temporary placements until we need to reload
3056     store_parameter(src_pos, 3);
3057     store_parameter(src, 4);
3058     NOT_LP64(assert(src == rcx &amp;&amp; src_pos == rdx, &quot;mismatch in calling convention&quot;);)
3059 
</pre>
<hr />
<pre>
3769       __ xorps(dest-&gt;as_xmm_float_reg(),
3770                ExternalAddress((address)float_signflip_pool));
3771     }
3772   } else if (dest-&gt;is_double_xmm()) {
3773 #ifdef _LP64
3774     if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512vl()) {
3775       assert(tmp-&gt;is_valid(), &quot;need temporary&quot;);
3776       assert_different_registers(left-&gt;as_xmm_double_reg(), tmp-&gt;as_xmm_double_reg());
3777       __ vpxor(dest-&gt;as_xmm_double_reg(), tmp-&gt;as_xmm_double_reg(), left-&gt;as_xmm_double_reg(), 2);
3778     }
3779     else
3780 #endif
3781     {
3782       assert(!tmp-&gt;is_valid(), &quot;do not need temporary&quot;);
3783       if (left-&gt;as_xmm_double_reg() != dest-&gt;as_xmm_double_reg()) {
3784         __ movdbl(dest-&gt;as_xmm_double_reg(), left-&gt;as_xmm_double_reg());
3785       }
3786       __ xorpd(dest-&gt;as_xmm_double_reg(),
3787                ExternalAddress((address)double_signflip_pool));
3788     }

3789   } else if (left-&gt;is_single_fpu() || left-&gt;is_double_fpu()) {
3790     assert(left-&gt;fpu() == 0, &quot;arg must be on TOS&quot;);
3791     assert(dest-&gt;fpu() == 0, &quot;dest must be TOS&quot;);
3792     __ fchs();

3793 
3794   } else {
3795     ShouldNotReachHere();
3796   }
3797 }
3798 
3799 
3800 void LIR_Assembler::leal(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
3801   assert(src-&gt;is_address(), &quot;must be an address&quot;);
3802   assert(dest-&gt;is_register(), &quot;must be a register&quot;);
3803 
3804   PatchingStub* patch = NULL;
3805   if (patch_code != lir_patch_none) {
3806     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
3807   }
3808 
3809   Register reg = dest-&gt;as_pointer_register();
3810   LIR_Address* addr = src-&gt;as_address_ptr();
3811   __ lea(reg, as_Address(addr));
3812 
</pre>
<hr />
<pre>
3842       __ psrlq(src-&gt;as_xmm_double_reg(), 32);
3843       __ movdl(dest-&gt;as_register_hi(), src-&gt;as_xmm_double_reg());
3844 #endif // _LP64
3845     } else if (dest-&gt;is_double_stack()) {
3846       __ movdbl(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix()), src-&gt;as_xmm_double_reg());
3847     } else if (dest-&gt;is_address()) {
3848       __ movdbl(as_Address(dest-&gt;as_address_ptr()), src-&gt;as_xmm_double_reg());
3849     } else {
3850       ShouldNotReachHere();
3851     }
3852 
3853   } else if (dest-&gt;is_double_xmm()) {
3854     if (src-&gt;is_double_stack()) {
3855       __ movdbl(dest-&gt;as_xmm_double_reg(), frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix()));
3856     } else if (src-&gt;is_address()) {
3857       __ movdbl(dest-&gt;as_xmm_double_reg(), as_Address(src-&gt;as_address_ptr()));
3858     } else {
3859       ShouldNotReachHere();
3860     }
3861 

3862   } else if (src-&gt;is_double_fpu()) {
3863     assert(src-&gt;fpu_regnrLo() == 0, &quot;must be TOS&quot;);
3864     if (dest-&gt;is_double_stack()) {
3865       __ fistp_d(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix()));
3866     } else if (dest-&gt;is_address()) {
3867       __ fistp_d(as_Address(dest-&gt;as_address_ptr()));
3868     } else {
3869       ShouldNotReachHere();
3870     }
3871 
3872   } else if (dest-&gt;is_double_fpu()) {
3873     assert(dest-&gt;fpu_regnrLo() == 0, &quot;must be TOS&quot;);
3874     if (src-&gt;is_double_stack()) {
3875       __ fild_d(frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix()));
3876     } else if (src-&gt;is_address()) {
3877       __ fild_d(as_Address(src-&gt;as_address_ptr()));
3878     } else {
3879       ShouldNotReachHere();
3880     }


3881   } else {
3882     ShouldNotReachHere();
3883   }
3884 }
3885 
3886 #ifdef ASSERT
3887 // emit run-time assertion
3888 void LIR_Assembler::emit_assert(LIR_OpAssert* op) {
3889   assert(op-&gt;code() == lir_assert, &quot;must be&quot;);
3890 
3891   if (op-&gt;in_opr1()-&gt;is_valid()) {
3892     assert(op-&gt;in_opr2()-&gt;is_valid(), &quot;both operands must be valid&quot;);
3893     comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
3894   } else {
3895     assert(op-&gt;in_opr2()-&gt;is_illegal(), &quot;both operands must be illegal&quot;);
3896     assert(op-&gt;condition() == lir_cond_always, &quot;no other conditions allowed&quot;);
3897   }
3898 
3899   Label ok;
3900   if (op-&gt;condition() != lir_cond_always) {
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;c1/c1_Compilation.hpp&quot;
  29 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  30 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  31 #include &quot;c1/c1_Runtime1.hpp&quot;
  32 #include &quot;c1/c1_ValueStack.hpp&quot;
  33 #include &quot;ci/ciArrayKlass.hpp&quot;
  34 #include &quot;ci/ciInstance.hpp&quot;


  35 #include &quot;gc/shared/collectedHeap.hpp&quot;
  36 #include &quot;nativeInst_x86.hpp&quot;
  37 #include &quot;oops/objArrayKlass.hpp&quot;
  38 #include &quot;runtime/frame.inline.hpp&quot;
  39 #include &quot;runtime/safepointMechanism.hpp&quot;
  40 #include &quot;runtime/sharedRuntime.hpp&quot;
<span class="line-added">  41 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  42 #include &quot;vmreg_x86.inline.hpp&quot;
  43 
  44 
  45 // These masks are used to provide 128-bit aligned bitmasks to the XMM
  46 // instructions, to allow sign-masking or sign-bit flipping.  They allow
  47 // fast versions of NegF/NegD and AbsF/AbsD.
  48 
  49 // Note: &#39;double&#39; and &#39;long long&#39; have 32-bits alignment on x86.
  50 static jlong* double_quadword(jlong *adr, jlong lo, jlong hi) {
  51   // Use the expression (adr)&amp;(~0xF) to provide 128-bits aligned address
  52   // of 128-bits operands for SSE instructions.
  53   jlong *operand = (jlong*)(((intptr_t)adr) &amp; ((intptr_t)(~0xF)));
  54   // Store the value to a 128-bits operand.
  55   operand[0] = lo;
  56   operand[1] = hi;
  57   return operand;
  58 }
  59 
  60 // Buffer for 128-bits masks used by SSE instructions.
  61 static jlong fp_signmask_pool[(4+1)*2]; // 4*128bits(data) + 128bits(alignment)
</pre>
<hr />
<pre>
 142   address const_addr = __ float_constant(f);
 143   if (const_addr == NULL) {
 144     bailout(&quot;const section overflow&quot;);
 145     return __ code()-&gt;consts()-&gt;start();
 146   } else {
 147     return const_addr;
 148   }
 149 }
 150 
 151 
 152 address LIR_Assembler::double_constant(double d) {
 153   address const_addr = __ double_constant(d);
 154   if (const_addr == NULL) {
 155     bailout(&quot;const section overflow&quot;);
 156     return __ code()-&gt;consts()-&gt;start();
 157   } else {
 158     return const_addr;
 159   }
 160 }
 161 
<span class="line-modified"> 162 #ifndef _LP64</span>








 163 void LIR_Assembler::fpop() {
 164   __ fpop();
 165 }
 166 
 167 void LIR_Assembler::fxch(int i) {
 168   __ fxch(i);
 169 }
 170 
 171 void LIR_Assembler::fld(int i) {
 172   __ fld_s(i);
 173 }
 174 
 175 void LIR_Assembler::ffree(int i) {
 176   __ ffree(i);
 177 }
<span class="line-added"> 178 #endif // !_LP64</span>
 179 
 180 void LIR_Assembler::breakpoint() {
 181   __ int3();
 182 }
 183 
 184 void LIR_Assembler::push(LIR_Opr opr) {
 185   if (opr-&gt;is_single_cpu()) {
 186     __ push_reg(opr-&gt;as_register());
 187   } else if (opr-&gt;is_double_cpu()) {
 188     NOT_LP64(__ push_reg(opr-&gt;as_register_hi()));
 189     __ push_reg(opr-&gt;as_register_lo());
 190   } else if (opr-&gt;is_stack()) {
 191     __ push_addr(frame_map()-&gt;address_for_slot(opr-&gt;single_stack_ix()));
 192   } else if (opr-&gt;is_constant()) {
 193     LIR_Const* const_opr = opr-&gt;as_constant_ptr();
 194     if (const_opr-&gt;type() == T_OBJECT) {
 195       __ push_oop(const_opr-&gt;as_jobject());
 196     } else if (const_opr-&gt;type() == T_INT) {
 197       __ push_jint(const_opr-&gt;as_jint());
 198     } else {
</pre>
<hr />
<pre>
 334 int LIR_Assembler::check_icache() {
 335   Register receiver = FrameMap::receiver_opr-&gt;as_register();
 336   Register ic_klass = IC_Klass;
 337   const int ic_cmp_size = LP64_ONLY(10) NOT_LP64(9);
 338   const bool do_post_padding = VerifyOops || UseCompressedClassPointers;
 339   if (!do_post_padding) {
 340     // insert some nops so that the verified entry point is aligned on CodeEntryAlignment
 341     __ align(CodeEntryAlignment, __ offset() + ic_cmp_size);
 342   }
 343   int offset = __ offset();
 344   __ inline_cache_check(receiver, IC_Klass);
 345   assert(__ offset() % CodeEntryAlignment == 0 || do_post_padding, &quot;alignment must be correct&quot;);
 346   if (do_post_padding) {
 347     // force alignment after the cache check.
 348     // It&#39;s been verified to be aligned if !VerifyOops
 349     __ align(CodeEntryAlignment);
 350   }
 351   return offset;
 352 }
 353 
<span class="line-added"> 354 void LIR_Assembler::clinit_barrier(ciMethod* method) {</span>
<span class="line-added"> 355   assert(VM_Version::supports_fast_class_init_checks(), &quot;sanity&quot;);</span>
<span class="line-added"> 356   assert(!method-&gt;holder()-&gt;is_not_initialized(), &quot;initialization should have been started&quot;);</span>
<span class="line-added"> 357 </span>
<span class="line-added"> 358   Label L_skip_barrier;</span>
<span class="line-added"> 359   Register klass = rscratch1;</span>
<span class="line-added"> 360   Register thread = LP64_ONLY( r15_thread ) NOT_LP64( noreg );</span>
<span class="line-added"> 361   assert(thread != noreg, &quot;x86_32 not implemented&quot;);</span>
<span class="line-added"> 362 </span>
<span class="line-added"> 363   __ mov_metadata(klass, method-&gt;holder()-&gt;constant_encoding());</span>
<span class="line-added"> 364   __ clinit_barrier(klass, thread, &amp;L_skip_barrier /*L_fast_path*/);</span>
<span class="line-added"> 365 </span>
<span class="line-added"> 366   __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));</span>
<span class="line-added"> 367 </span>
<span class="line-added"> 368   __ bind(L_skip_barrier);</span>
<span class="line-added"> 369 }</span>
 370 
 371 void LIR_Assembler::jobject2reg_with_patching(Register reg, CodeEmitInfo* info) {
 372   jobject o = NULL;
 373   PatchingStub* patch = new PatchingStub(_masm, patching_id(info));
 374   __ movoop(reg, o);
 375   patching_epilog(patch, lir_patch_normal, reg, info);
 376 }
 377 
 378 void LIR_Assembler::klass2reg_with_patching(Register reg, CodeEmitInfo* info) {
 379   Metadata* o = NULL;
 380   PatchingStub* patch = new PatchingStub(_masm, PatchingStub::load_klass_id);
 381   __ mov_metadata(reg, o);
 382   patching_epilog(patch, lir_patch_normal, reg, info);
 383 }
 384 
 385 // This specifies the rsp decrement needed to build the frame
 386 int LIR_Assembler::initial_frame_size_in_bytes() const {
 387   // if rounding, must let FrameMap know!
 388 
 389   // The frame_map records size in slots (32bit word)
</pre>
<hr />
<pre>
 647     }
 648 
 649     case T_METADATA: {
 650       if (patch_code != lir_patch_none) {
 651         klass2reg_with_patching(dest-&gt;as_register(), info);
 652       } else {
 653         __ mov_metadata(dest-&gt;as_register(), c-&gt;as_metadata());
 654       }
 655       break;
 656     }
 657 
 658     case T_FLOAT: {
 659       if (dest-&gt;is_single_xmm()) {
 660         if (LP64_ONLY(UseAVX &lt;= 2 &amp;&amp;) c-&gt;is_zero_float()) {
 661           __ xorps(dest-&gt;as_xmm_float_reg(), dest-&gt;as_xmm_float_reg());
 662         } else {
 663           __ movflt(dest-&gt;as_xmm_float_reg(),
 664                    InternalAddress(float_constant(c-&gt;as_jfloat())));
 665         }
 666       } else {
<span class="line-added"> 667 #ifndef _LP64</span>
 668         assert(dest-&gt;is_single_fpu(), &quot;must be&quot;);
 669         assert(dest-&gt;fpu_regnr() == 0, &quot;dest must be TOS&quot;);
 670         if (c-&gt;is_zero_float()) {
 671           __ fldz();
 672         } else if (c-&gt;is_one_float()) {
 673           __ fld1();
 674         } else {
 675           __ fld_s (InternalAddress(float_constant(c-&gt;as_jfloat())));
 676         }
<span class="line-added"> 677 #else</span>
<span class="line-added"> 678         ShouldNotReachHere();</span>
<span class="line-added"> 679 #endif // !_LP64</span>
 680       }
 681       break;
 682     }
 683 
 684     case T_DOUBLE: {
 685       if (dest-&gt;is_double_xmm()) {
 686         if (LP64_ONLY(UseAVX &lt;= 2 &amp;&amp;) c-&gt;is_zero_double()) {
 687           __ xorpd(dest-&gt;as_xmm_double_reg(), dest-&gt;as_xmm_double_reg());
 688         } else {
 689           __ movdbl(dest-&gt;as_xmm_double_reg(),
 690                     InternalAddress(double_constant(c-&gt;as_jdouble())));
 691         }
 692       } else {
<span class="line-added"> 693 #ifndef _LP64</span>
 694         assert(dest-&gt;is_double_fpu(), &quot;must be&quot;);
 695         assert(dest-&gt;fpu_regnrLo() == 0, &quot;dest must be TOS&quot;);
 696         if (c-&gt;is_zero_double()) {
 697           __ fldz();
 698         } else if (c-&gt;is_one_double()) {
 699           __ fld1();
 700         } else {
 701           __ fld_d (InternalAddress(double_constant(c-&gt;as_jdouble())));
 702         }
<span class="line-added"> 703 #else</span>
<span class="line-added"> 704         ShouldNotReachHere();</span>
<span class="line-added"> 705 #endif // !_LP64</span>
 706       }
 707       break;
 708     }
 709 
 710     default:
 711       ShouldNotReachHere();
 712   }
 713 }
 714 
 715 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
 716   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 717   assert(dest-&gt;is_stack(), &quot;should not call otherwise&quot;);
 718   LIR_Const* c = src-&gt;as_constant_ptr();
 719 
 720   switch (c-&gt;type()) {
 721     case T_INT:  // fall through
 722     case T_FLOAT:
 723       __ movl(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jint_bits());
 724       break;
 725 
</pre>
<hr />
<pre>
 844   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 845   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 846 
 847   // move between cpu-registers
 848   if (dest-&gt;is_single_cpu()) {
 849 #ifdef _LP64
 850     if (src-&gt;type() == T_LONG) {
 851       // Can do LONG -&gt; OBJECT
 852       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
 853       return;
 854     }
 855 #endif
 856     assert(src-&gt;is_single_cpu(), &quot;must match&quot;);
 857     if (src-&gt;type() == T_OBJECT) {
 858       __ verify_oop(src-&gt;as_register());
 859     }
 860     move_regs(src-&gt;as_register(), dest-&gt;as_register());
 861 
 862   } else if (dest-&gt;is_double_cpu()) {
 863 #ifdef _LP64
<span class="line-modified"> 864     if (is_reference_type(src-&gt;type())) {</span>
 865       // Surprising to me but we can see move of a long to t_object
 866       __ verify_oop(src-&gt;as_register());
 867       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
 868       return;
 869     }
 870 #endif
 871     assert(src-&gt;is_double_cpu(), &quot;must match&quot;);
 872     Register f_lo = src-&gt;as_register_lo();
 873     Register f_hi = src-&gt;as_register_hi();
 874     Register t_lo = dest-&gt;as_register_lo();
 875     Register t_hi = dest-&gt;as_register_hi();
 876 #ifdef _LP64
 877     assert(f_hi == f_lo, &quot;must be same&quot;);
 878     assert(t_hi == t_lo, &quot;must be same&quot;);
 879     move_regs(f_lo, t_lo);
 880 #else
 881     assert(f_lo != f_hi &amp;&amp; t_lo != t_hi, &quot;invalid register allocation&quot;);
 882 
 883 
 884     if (f_lo == t_hi &amp;&amp; f_hi == t_lo) {
 885       swap_reg(f_lo, f_hi);
 886     } else if (f_hi == t_lo) {
 887       assert(f_lo != t_hi, &quot;overwriting register&quot;);
 888       move_regs(f_hi, t_hi);
 889       move_regs(f_lo, t_lo);
 890     } else {
 891       assert(f_hi != t_lo, &quot;overwriting register&quot;);
 892       move_regs(f_lo, t_lo);
 893       move_regs(f_hi, t_hi);
 894     }
 895 #endif // LP64
 896 
<span class="line-added"> 897 #ifndef _LP64</span>
 898     // special moves from fpu-register to xmm-register
 899     // necessary for method results
 900   } else if (src-&gt;is_single_xmm() &amp;&amp; !dest-&gt;is_single_xmm()) {
 901     __ movflt(Address(rsp, 0), src-&gt;as_xmm_float_reg());
 902     __ fld_s(Address(rsp, 0));
 903   } else if (src-&gt;is_double_xmm() &amp;&amp; !dest-&gt;is_double_xmm()) {
 904     __ movdbl(Address(rsp, 0), src-&gt;as_xmm_double_reg());
 905     __ fld_d(Address(rsp, 0));
 906   } else if (dest-&gt;is_single_xmm() &amp;&amp; !src-&gt;is_single_xmm()) {
 907     __ fstp_s(Address(rsp, 0));
 908     __ movflt(dest-&gt;as_xmm_float_reg(), Address(rsp, 0));
 909   } else if (dest-&gt;is_double_xmm() &amp;&amp; !src-&gt;is_double_xmm()) {
 910     __ fstp_d(Address(rsp, 0));
 911     __ movdbl(dest-&gt;as_xmm_double_reg(), Address(rsp, 0));
<span class="line-added"> 912 #endif // !_LP64</span>
 913 
 914     // move between xmm-registers
 915   } else if (dest-&gt;is_single_xmm()) {
 916     assert(src-&gt;is_single_xmm(), &quot;must match&quot;);
 917     __ movflt(dest-&gt;as_xmm_float_reg(), src-&gt;as_xmm_float_reg());
 918   } else if (dest-&gt;is_double_xmm()) {
 919     assert(src-&gt;is_double_xmm(), &quot;must match&quot;);
 920     __ movdbl(dest-&gt;as_xmm_double_reg(), src-&gt;as_xmm_double_reg());
 921 
<span class="line-added"> 922 #ifndef _LP64</span>
 923     // move between fpu-registers (no instruction necessary because of fpu-stack)
 924   } else if (dest-&gt;is_single_fpu() || dest-&gt;is_double_fpu()) {
 925     assert(src-&gt;is_single_fpu() || src-&gt;is_double_fpu(), &quot;must match&quot;);
 926     assert(src-&gt;fpu() == dest-&gt;fpu(), &quot;currently should be nothing to do&quot;);
<span class="line-added"> 927 #endif // !_LP64</span>
<span class="line-added"> 928 </span>
 929   } else {
 930     ShouldNotReachHere();
 931   }
 932 }
 933 
 934 void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {
 935   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 936   assert(dest-&gt;is_stack(), &quot;should not call otherwise&quot;);
 937 
 938   if (src-&gt;is_single_cpu()) {
 939     Address dst = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
<span class="line-modified"> 940     if (is_reference_type(type)) {</span>
 941       __ verify_oop(src-&gt;as_register());
 942       __ movptr (dst, src-&gt;as_register());
<span class="line-modified"> 943     } else if (type == T_METADATA || type == T_ADDRESS) {</span>
 944       __ movptr (dst, src-&gt;as_register());
 945     } else {
 946       __ movl (dst, src-&gt;as_register());
 947     }
 948 
 949   } else if (src-&gt;is_double_cpu()) {
 950     Address dstLO = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), lo_word_offset_in_bytes);
 951     Address dstHI = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), hi_word_offset_in_bytes);
 952     __ movptr (dstLO, src-&gt;as_register_lo());
 953     NOT_LP64(__ movptr (dstHI, src-&gt;as_register_hi()));
 954 
 955   } else if (src-&gt;is_single_xmm()) {
 956     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 957     __ movflt(dst_addr, src-&gt;as_xmm_float_reg());
 958 
 959   } else if (src-&gt;is_double_xmm()) {
 960     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
 961     __ movdbl(dst_addr, src-&gt;as_xmm_double_reg());
 962 
<span class="line-added"> 963 #ifndef _LP64</span>
 964   } else if (src-&gt;is_single_fpu()) {
 965     assert(src-&gt;fpu_regnr() == 0, &quot;argument must be on TOS&quot;);
 966     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 967     if (pop_fpu_stack)     __ fstp_s (dst_addr);
 968     else                   __ fst_s  (dst_addr);
 969 
 970   } else if (src-&gt;is_double_fpu()) {
 971     assert(src-&gt;fpu_regnrLo() == 0, &quot;argument must be on TOS&quot;);
 972     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
 973     if (pop_fpu_stack)     __ fstp_d (dst_addr);
 974     else                   __ fst_d  (dst_addr);
<span class="line-added"> 975 #endif // !_LP64</span>
 976 
 977   } else {
 978     ShouldNotReachHere();
 979   }
 980 }
 981 
 982 
 983 void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide, bool /* unaligned */) {
 984   LIR_Address* to_addr = dest-&gt;as_address_ptr();
 985   PatchingStub* patch = NULL;
 986   Register compressed_src = rscratch1;
 987 
<span class="line-modified"> 988   if (is_reference_type(type)) {</span>
 989     __ verify_oop(src-&gt;as_register());
 990 #ifdef _LP64
 991     if (UseCompressedOops &amp;&amp; !wide) {
 992       __ movptr(compressed_src, src-&gt;as_register());
 993       __ encode_heap_oop(compressed_src);
 994       if (patch_code != lir_patch_none) {
 995         info-&gt;oop_map()-&gt;set_narrowoop(compressed_src-&gt;as_VMReg());
 996       }
 997     }
 998 #endif
 999   }
1000 
1001   if (patch_code != lir_patch_none) {
1002     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
1003     Address toa = as_Address(to_addr);
1004     assert(toa.disp() != 0, &quot;must have&quot;);
1005   }
1006 
1007   int null_check_here = code_offset();
1008   switch (type) {
1009     case T_FLOAT: {
<span class="line-added">1010 #ifdef _LP64</span>
<span class="line-added">1011       assert(src-&gt;is_single_xmm(), &quot;not a float&quot;);</span>
<span class="line-added">1012       __ movflt(as_Address(to_addr), src-&gt;as_xmm_float_reg());</span>
<span class="line-added">1013 #else</span>
1014       if (src-&gt;is_single_xmm()) {
1015         __ movflt(as_Address(to_addr), src-&gt;as_xmm_float_reg());
1016       } else {
1017         assert(src-&gt;is_single_fpu(), &quot;must be&quot;);
1018         assert(src-&gt;fpu_regnr() == 0, &quot;argument must be on TOS&quot;);
1019         if (pop_fpu_stack)      __ fstp_s(as_Address(to_addr));
1020         else                    __ fst_s (as_Address(to_addr));
1021       }
<span class="line-added">1022 #endif // _LP64</span>
1023       break;
1024     }
1025 
1026     case T_DOUBLE: {
<span class="line-added">1027 #ifdef _LP64</span>
<span class="line-added">1028       assert(src-&gt;is_double_xmm(), &quot;not a double&quot;);</span>
<span class="line-added">1029       __ movdbl(as_Address(to_addr), src-&gt;as_xmm_double_reg());</span>
<span class="line-added">1030 #else</span>
1031       if (src-&gt;is_double_xmm()) {
1032         __ movdbl(as_Address(to_addr), src-&gt;as_xmm_double_reg());
1033       } else {
1034         assert(src-&gt;is_double_fpu(), &quot;must be&quot;);
1035         assert(src-&gt;fpu_regnrLo() == 0, &quot;argument must be on TOS&quot;);
1036         if (pop_fpu_stack)      __ fstp_d(as_Address(to_addr));
1037         else                    __ fst_d (as_Address(to_addr));
1038       }
<span class="line-added">1039 #endif // _LP64</span>
1040       break;
1041     }
1042 
1043     case T_ARRAY:   // fall through
1044     case T_OBJECT:  // fall through
1045       if (UseCompressedOops &amp;&amp; !wide) {
1046         __ movl(as_Address(to_addr), compressed_src);
1047       } else {
1048         __ movptr(as_Address(to_addr), src-&gt;as_register());
1049       }
1050       break;
1051     case T_METADATA:
1052       // We get here to store a method pointer to the stack to pass to
1053       // a dtrace runtime call. This can&#39;t work on 64 bit with
1054       // compressed klass ptrs: T_METADATA can be a compressed klass
1055       // ptr or a 64 bit method pointer.
1056       LP64_ONLY(ShouldNotReachHere());
1057       __ movptr(as_Address(to_addr), src-&gt;as_register());
1058       break;
1059     case T_ADDRESS:
</pre>
<hr />
<pre>
1113       break;
1114 
1115     default:
1116       ShouldNotReachHere();
1117   }
1118   if (info != NULL) {
1119     add_debug_info_for_null_check(null_check_here, info);
1120   }
1121 
1122   if (patch_code != lir_patch_none) {
1123     patching_epilog(patch, patch_code, to_addr-&gt;base()-&gt;as_register(), info);
1124   }
1125 }
1126 
1127 
1128 void LIR_Assembler::stack2reg(LIR_Opr src, LIR_Opr dest, BasicType type) {
1129   assert(src-&gt;is_stack(), &quot;should not call otherwise&quot;);
1130   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
1131 
1132   if (dest-&gt;is_single_cpu()) {
<span class="line-modified">1133     if (is_reference_type(type)) {</span>
1134       __ movptr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
1135       __ verify_oop(dest-&gt;as_register());
<span class="line-modified">1136     } else if (type == T_METADATA || type == T_ADDRESS) {</span>
1137       __ movptr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
1138     } else {
1139       __ movl(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
1140     }
1141 
1142   } else if (dest-&gt;is_double_cpu()) {
1143     Address src_addr_LO = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), lo_word_offset_in_bytes);
1144     Address src_addr_HI = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), hi_word_offset_in_bytes);
1145     __ movptr(dest-&gt;as_register_lo(), src_addr_LO);
1146     NOT_LP64(__ movptr(dest-&gt;as_register_hi(), src_addr_HI));
1147 
1148   } else if (dest-&gt;is_single_xmm()) {
1149     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
1150     __ movflt(dest-&gt;as_xmm_float_reg(), src_addr);
1151 
1152   } else if (dest-&gt;is_double_xmm()) {
1153     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
1154     __ movdbl(dest-&gt;as_xmm_double_reg(), src_addr);
1155 
<span class="line-added">1156 #ifndef _LP64</span>
1157   } else if (dest-&gt;is_single_fpu()) {
1158     assert(dest-&gt;fpu_regnr() == 0, &quot;dest must be TOS&quot;);
1159     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
1160     __ fld_s(src_addr);
1161 
1162   } else if (dest-&gt;is_double_fpu()) {
1163     assert(dest-&gt;fpu_regnrLo() == 0, &quot;dest must be TOS&quot;);
1164     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
1165     __ fld_d(src_addr);
<span class="line-added">1166 #endif // _LP64</span>
1167 
1168   } else {
1169     ShouldNotReachHere();
1170   }
1171 }
1172 
1173 
1174 void LIR_Assembler::stack2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {
1175   if (src-&gt;is_single_stack()) {
<span class="line-modified">1176     if (is_reference_type(type)) {</span>
1177       __ pushptr(frame_map()-&gt;address_for_slot(src -&gt;single_stack_ix()));
1178       __ popptr (frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
1179     } else {
1180 #ifndef _LP64
1181       __ pushl(frame_map()-&gt;address_for_slot(src -&gt;single_stack_ix()));
1182       __ popl (frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
1183 #else
1184       //no pushl on 64bits
1185       __ movl(rscratch1, frame_map()-&gt;address_for_slot(src -&gt;single_stack_ix()));
1186       __ movl(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), rscratch1);
1187 #endif
1188     }
1189 
1190   } else if (src-&gt;is_double_stack()) {
1191 #ifdef _LP64
1192     __ pushptr(frame_map()-&gt;address_for_slot(src -&gt;double_stack_ix()));
1193     __ popptr (frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix()));
1194 #else
1195     __ pushl(frame_map()-&gt;address_for_slot(src -&gt;double_stack_ix(), 0));
1196     // push and pop the part at src + wordSize, adding wordSize for the previous push
</pre>
<hr />
<pre>
1230       }
1231       break;
1232    default:
1233      break;
1234   }
1235 
1236   PatchingStub* patch = NULL;
1237   if (patch_code != lir_patch_none) {
1238     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
1239     assert(from_addr.disp() != 0, &quot;must have&quot;);
1240   }
1241   if (info != NULL) {
1242     add_debug_info_for_null_check_here(info);
1243   }
1244 
1245   switch (type) {
1246     case T_FLOAT: {
1247       if (dest-&gt;is_single_xmm()) {
1248         __ movflt(dest-&gt;as_xmm_float_reg(), from_addr);
1249       } else {
<span class="line-added">1250 #ifndef _LP64</span>
1251         assert(dest-&gt;is_single_fpu(), &quot;must be&quot;);
1252         assert(dest-&gt;fpu_regnr() == 0, &quot;dest must be TOS&quot;);
1253         __ fld_s(from_addr);
<span class="line-added">1254 #else</span>
<span class="line-added">1255         ShouldNotReachHere();</span>
<span class="line-added">1256 #endif // !LP64</span>
1257       }
1258       break;
1259     }
1260 
1261     case T_DOUBLE: {
1262       if (dest-&gt;is_double_xmm()) {
1263         __ movdbl(dest-&gt;as_xmm_double_reg(), from_addr);
1264       } else {
<span class="line-added">1265 #ifndef _LP64</span>
1266         assert(dest-&gt;is_double_fpu(), &quot;must be&quot;);
1267         assert(dest-&gt;fpu_regnrLo() == 0, &quot;dest must be TOS&quot;);
1268         __ fld_d(from_addr);
<span class="line-added">1269 #else</span>
<span class="line-added">1270         ShouldNotReachHere();</span>
<span class="line-added">1271 #endif // !LP64</span>
1272       }
1273       break;
1274     }
1275 
1276     case T_OBJECT:  // fall through
1277     case T_ARRAY:   // fall through
1278       if (UseCompressedOops &amp;&amp; !wide) {
1279         __ movl(dest-&gt;as_register(), from_addr);
1280       } else {
1281         __ movptr(dest-&gt;as_register(), from_addr);
1282       }
1283       break;
1284 
1285     case T_ADDRESS:
1286       if (UseCompressedClassPointers &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1287         __ movl(dest-&gt;as_register(), from_addr);
1288       } else {
1289         __ movptr(dest-&gt;as_register(), from_addr);
1290       }
1291       break;
</pre>
<hr />
<pre>
1365     case T_SHORT: {
1366       Register dest_reg = dest-&gt;as_register();
1367       if (VM_Version::is_P6() || from_addr.uses(dest_reg)) {
1368         __ movswl(dest_reg, from_addr);
1369       } else {
1370         __ movw(dest_reg, from_addr);
1371         __ shll(dest_reg, 16);
1372         __ sarl(dest_reg, 16);
1373       }
1374       break;
1375     }
1376 
1377     default:
1378       ShouldNotReachHere();
1379   }
1380 
1381   if (patch != NULL) {
1382     patching_epilog(patch, patch_code, addr-&gt;base()-&gt;as_register(), info);
1383   }
1384 
<span class="line-modified">1385   if (is_reference_type(type)) {</span>
1386 #ifdef _LP64
1387     if (UseCompressedOops &amp;&amp; !wide) {
1388       __ decode_heap_oop(dest-&gt;as_register());
1389     }
1390 #endif
1391 
1392     // Load barrier has not yet been applied, so ZGC can&#39;t verify the oop here
1393     if (!UseZGC) {
1394       __ verify_oop(dest-&gt;as_register());
1395     }
1396   } else if (type == T_ADDRESS &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1397 #ifdef _LP64
1398     if (UseCompressedClassPointers) {
1399       __ decode_klass_not_null(dest-&gt;as_register());
1400     }
1401 #endif
1402   }
1403 }
1404 
1405 
</pre>
<hr />
<pre>
1507       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
1508 #endif
1509       break;
1510 
1511     case Bytecodes::_i2b:
1512       move_regs(src-&gt;as_register(), dest-&gt;as_register());
1513       __ sign_extend_byte(dest-&gt;as_register());
1514       break;
1515 
1516     case Bytecodes::_i2c:
1517       move_regs(src-&gt;as_register(), dest-&gt;as_register());
1518       __ andl(dest-&gt;as_register(), 0xFFFF);
1519       break;
1520 
1521     case Bytecodes::_i2s:
1522       move_regs(src-&gt;as_register(), dest-&gt;as_register());
1523       __ sign_extend_short(dest-&gt;as_register());
1524       break;
1525 
1526 
<span class="line-added">1527 #ifdef _LP64</span>
<span class="line-added">1528     case Bytecodes::_f2d:</span>
<span class="line-added">1529       __ cvtss2sd(dest-&gt;as_xmm_double_reg(), src-&gt;as_xmm_float_reg());</span>
<span class="line-added">1530       break;</span>
<span class="line-added">1531 </span>
<span class="line-added">1532     case Bytecodes::_d2f:</span>
<span class="line-added">1533       __ cvtsd2ss(dest-&gt;as_xmm_float_reg(), src-&gt;as_xmm_double_reg());</span>
<span class="line-added">1534       break;</span>
<span class="line-added">1535 </span>
<span class="line-added">1536     case Bytecodes::_i2f:</span>
<span class="line-added">1537       __ cvtsi2ssl(dest-&gt;as_xmm_float_reg(), src-&gt;as_register());</span>
<span class="line-added">1538       break;</span>
<span class="line-added">1539 </span>
<span class="line-added">1540     case Bytecodes::_i2d:</span>
<span class="line-added">1541       __ cvtsi2sdl(dest-&gt;as_xmm_double_reg(), src-&gt;as_register());</span>
<span class="line-added">1542       break;</span>
<span class="line-added">1543 </span>
<span class="line-added">1544     case Bytecodes::_l2f:</span>
<span class="line-added">1545       __ cvtsi2ssq(dest-&gt;as_xmm_float_reg(), src-&gt;as_register_lo());</span>
<span class="line-added">1546       break;</span>
<span class="line-added">1547 </span>
<span class="line-added">1548     case Bytecodes::_l2d:</span>
<span class="line-added">1549       __ cvtsi2sdq(dest-&gt;as_xmm_double_reg(), src-&gt;as_register_lo());</span>
<span class="line-added">1550       break;</span>
<span class="line-added">1551 </span>
<span class="line-added">1552     case Bytecodes::_f2i:</span>
<span class="line-added">1553       __ convert_f2i(dest-&gt;as_register(), src-&gt;as_xmm_float_reg());</span>
<span class="line-added">1554       break;</span>
<span class="line-added">1555 </span>
<span class="line-added">1556     case Bytecodes::_d2i:</span>
<span class="line-added">1557       __ convert_d2i(dest-&gt;as_register(), src-&gt;as_xmm_double_reg());</span>
<span class="line-added">1558       break;</span>
<span class="line-added">1559 </span>
<span class="line-added">1560     case Bytecodes::_f2l:</span>
<span class="line-added">1561       __ convert_f2l(dest-&gt;as_register_lo(), src-&gt;as_xmm_float_reg());</span>
<span class="line-added">1562       break;</span>
<span class="line-added">1563 </span>
<span class="line-added">1564     case Bytecodes::_d2l:</span>
<span class="line-added">1565       __ convert_d2l(dest-&gt;as_register_lo(), src-&gt;as_xmm_double_reg());</span>
<span class="line-added">1566       break;</span>
<span class="line-added">1567 #else</span>
1568     case Bytecodes::_f2d:
1569     case Bytecodes::_d2f:
1570       if (dest-&gt;is_single_xmm()) {
1571         __ cvtsd2ss(dest-&gt;as_xmm_float_reg(), src-&gt;as_xmm_double_reg());
1572       } else if (dest-&gt;is_double_xmm()) {
1573         __ cvtss2sd(dest-&gt;as_xmm_double_reg(), src-&gt;as_xmm_float_reg());
1574       } else {
1575         assert(src-&gt;fpu() == dest-&gt;fpu(), &quot;register must be equal&quot;);
1576         // do nothing (float result is rounded later through spilling)
1577       }
1578       break;
1579 
1580     case Bytecodes::_i2f:
1581     case Bytecodes::_i2d:
1582       if (dest-&gt;is_single_xmm()) {
1583         __ cvtsi2ssl(dest-&gt;as_xmm_float_reg(), src-&gt;as_register());
1584       } else if (dest-&gt;is_double_xmm()) {
1585         __ cvtsi2sdl(dest-&gt;as_xmm_double_reg(), src-&gt;as_register());
1586       } else {
1587         assert(dest-&gt;fpu() == 0, &quot;result must be on TOS&quot;);
1588         __ movl(Address(rsp, 0), src-&gt;as_register());
1589         __ fild_s(Address(rsp, 0));
1590       }
1591       break;
1592 
<span class="line-added">1593     case Bytecodes::_l2f:</span>
<span class="line-added">1594     case Bytecodes::_l2d:</span>
<span class="line-added">1595       assert(!dest-&gt;is_xmm_register(), &quot;result in xmm register not supported (no SSE instruction present)&quot;);</span>
<span class="line-added">1596       assert(dest-&gt;fpu() == 0, &quot;result must be on TOS&quot;);</span>
<span class="line-added">1597       __ movptr(Address(rsp, 0),          src-&gt;as_register_lo());</span>
<span class="line-added">1598       __ movl(Address(rsp, BytesPerWord), src-&gt;as_register_hi());</span>
<span class="line-added">1599       __ fild_d(Address(rsp, 0));</span>
<span class="line-added">1600       // float result is rounded later through spilling</span>
<span class="line-added">1601       break;</span>
<span class="line-added">1602 </span>
1603     case Bytecodes::_f2i:
1604     case Bytecodes::_d2i:
1605       if (src-&gt;is_single_xmm()) {
1606         __ cvttss2sil(dest-&gt;as_register(), src-&gt;as_xmm_float_reg());
1607       } else if (src-&gt;is_double_xmm()) {
1608         __ cvttsd2sil(dest-&gt;as_register(), src-&gt;as_xmm_double_reg());
1609       } else {
1610         assert(src-&gt;fpu() == 0, &quot;input must be on TOS&quot;);
1611         __ fldcw(ExternalAddress(StubRoutines::addr_fpu_cntrl_wrd_trunc()));
1612         __ fist_s(Address(rsp, 0));
1613         __ movl(dest-&gt;as_register(), Address(rsp, 0));
1614         __ fldcw(ExternalAddress(StubRoutines::addr_fpu_cntrl_wrd_std()));
1615       }

1616       // IA32 conversion instructions do not match JLS for overflow, underflow and NaN -&gt; fixup in stub
1617       assert(op-&gt;stub() != NULL, &quot;stub required&quot;);
1618       __ cmpl(dest-&gt;as_register(), 0x80000000);
1619       __ jcc(Assembler::equal, *op-&gt;stub()-&gt;entry());
1620       __ bind(*op-&gt;stub()-&gt;continuation());
1621       break;
1622 











1623     case Bytecodes::_f2l:
1624     case Bytecodes::_d2l:
1625       assert(!src-&gt;is_xmm_register(), &quot;input in xmm register not supported (no SSE instruction present)&quot;);
1626       assert(src-&gt;fpu() == 0, &quot;input must be on TOS&quot;);
1627       assert(dest == FrameMap::long0_opr, &quot;runtime stub places result in these registers&quot;);
1628 
1629       // instruction sequence too long to inline it here
1630       {
1631         __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::fpu2long_stub_id)));
1632       }
1633       break;
<span class="line-added">1634 #endif // _LP64</span>
1635 
1636     default: ShouldNotReachHere();
1637   }
1638 }
1639 
1640 void LIR_Assembler::emit_alloc_obj(LIR_OpAllocObj* op) {
1641   if (op-&gt;init_check()) {
1642     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
1643     __ cmpb(Address(op-&gt;klass()-&gt;as_register(),
1644                     InstanceKlass::init_state_offset()),
1645                     InstanceKlass::fully_initialized);
1646     __ jcc(Assembler::notEqual, *op-&gt;stub()-&gt;entry());
1647   }
1648   __ allocate_object(op-&gt;obj()-&gt;as_register(),
1649                      op-&gt;tmp1()-&gt;as_register(),
1650                      op-&gt;tmp2()-&gt;as_register(),
1651                      op-&gt;header_size(),
1652                      op-&gt;object_size(),
1653                      op-&gt;klass()-&gt;as_register(),
1654                      *op-&gt;stub()-&gt;entry());
1655   __ bind(*op-&gt;stub()-&gt;continuation());
1656 }
1657 
1658 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
1659   Register len =  op-&gt;len()-&gt;as_register();
1660   LP64_ONLY( __ movslq(len, len); )
1661 
1662   if (UseSlowPath ||
<span class="line-modified">1663       (!UseFastNewObjectArray &amp;&amp; is_reference_type(op-&gt;type())) ||</span>
<span class="line-modified">1664       (!UseFastNewTypeArray   &amp;&amp; !is_reference_type(op-&gt;type()))) {</span>
1665     __ jmp(*op-&gt;stub()-&gt;entry());
1666   } else {
1667     Register tmp1 = op-&gt;tmp1()-&gt;as_register();
1668     Register tmp2 = op-&gt;tmp2()-&gt;as_register();
1669     Register tmp3 = op-&gt;tmp3()-&gt;as_register();
1670     if (len == tmp1) {
1671       tmp1 = tmp3;
1672     } else if (len == tmp2) {
1673       tmp2 = tmp3;
1674     } else if (len == tmp3) {
1675       // everything is ok
1676     } else {
1677       __ mov(tmp3, len);
1678     }
1679     __ allocate_array(op-&gt;obj()-&gt;as_register(),
1680                       len,
1681                       tmp1,
1682                       tmp2,
1683                       arrayOopDesc::header_size(op-&gt;type()),
1684                       array_element_size(op-&gt;type()),
</pre>
<hr />
<pre>
2274       Address raddr;
2275       if (right-&gt;is_double_stack()) {
2276         raddr = frame_map()-&gt;address_for_slot(right-&gt;double_stack_ix());
2277       } else if (right-&gt;is_constant()) {
2278         // hack for now
2279         raddr = __ as_Address(InternalAddress(double_constant(right-&gt;as_jdouble())));
2280       } else {
2281         ShouldNotReachHere();
2282       }
2283       switch (code) {
2284         case lir_add: __ addsd(lreg, raddr);  break;
2285         case lir_sub: __ subsd(lreg, raddr);  break;
2286         case lir_mul_strictfp: // fall through
2287         case lir_mul: __ mulsd(lreg, raddr);  break;
2288         case lir_div_strictfp: // fall through
2289         case lir_div: __ divsd(lreg, raddr);  break;
2290         default: ShouldNotReachHere();
2291       }
2292     }
2293 
<span class="line-added">2294 #ifndef _LP64</span>
2295   } else if (left-&gt;is_single_fpu()) {
2296     assert(dest-&gt;is_single_fpu(),  &quot;fpu stack allocation required&quot;);
2297 
2298     if (right-&gt;is_single_fpu()) {
2299       arith_fpu_implementation(code, left-&gt;fpu_regnr(), right-&gt;fpu_regnr(), dest-&gt;fpu_regnr(), pop_fpu_stack);
2300 
2301     } else {
2302       assert(left-&gt;fpu_regnr() == 0, &quot;left must be on TOS&quot;);
2303       assert(dest-&gt;fpu_regnr() == 0, &quot;dest must be on TOS&quot;);
2304 
2305       Address raddr;
2306       if (right-&gt;is_single_stack()) {
2307         raddr = frame_map()-&gt;address_for_slot(right-&gt;single_stack_ix());
2308       } else if (right-&gt;is_constant()) {
2309         address const_addr = float_constant(right-&gt;as_jfloat());
2310         assert(const_addr != NULL, &quot;incorrect float/double constant maintainance&quot;);
2311         // hack for now
2312         raddr = __ as_Address(InternalAddress(const_addr));
2313       } else {
2314         ShouldNotReachHere();
</pre>
<hr />
<pre>
2350       } else {
2351         ShouldNotReachHere();
2352       }
2353 
2354       switch (code) {
2355         case lir_add: __ fadd_d(raddr); break;
2356         case lir_sub: __ fsub_d(raddr); break;
2357         case lir_mul_strictfp: // fall through
2358         case lir_mul: __ fmul_d(raddr); break;
2359         case lir_div_strictfp: // fall through
2360         case lir_div: __ fdiv_d(raddr); break;
2361         default: ShouldNotReachHere();
2362       }
2363     }
2364 
2365     if (code == lir_mul_strictfp || code == lir_div_strictfp) {
2366       // Double values require special handling for strictfp mul/div on x86
2367       __ fld_x(ExternalAddress(StubRoutines::addr_fpu_subnormal_bias2()));
2368       __ fmulp(dest-&gt;fpu_regnrLo() + 1);
2369     }
<span class="line-added">2370 #endif // !_LP64</span>
2371 
2372   } else if (left-&gt;is_single_stack() || left-&gt;is_address()) {
2373     assert(left == dest, &quot;left and dest must be equal&quot;);
2374 
2375     Address laddr;
2376     if (left-&gt;is_single_stack()) {
2377       laddr = frame_map()-&gt;address_for_slot(left-&gt;single_stack_ix());
2378     } else if (left-&gt;is_address()) {
2379       laddr = as_Address(left-&gt;as_address_ptr());
2380     } else {
2381       ShouldNotReachHere();
2382     }
2383 
2384     if (right-&gt;is_single_cpu()) {
2385       Register rreg = right-&gt;as_register();
2386       switch (code) {
2387         case lir_add: __ addl(laddr, rreg); break;
2388         case lir_sub: __ subl(laddr, rreg); break;
2389         default:      ShouldNotReachHere();
2390       }
</pre>
<hr />
<pre>
2393       switch (code) {
2394         case lir_add: {
2395           __ incrementl(laddr, c);
2396           break;
2397         }
2398         case lir_sub: {
2399           __ decrementl(laddr, c);
2400           break;
2401         }
2402         default: ShouldNotReachHere();
2403       }
2404     } else {
2405       ShouldNotReachHere();
2406     }
2407 
2408   } else {
2409     ShouldNotReachHere();
2410   }
2411 }
2412 
<span class="line-added">2413 #ifndef _LP64</span>
2414 void LIR_Assembler::arith_fpu_implementation(LIR_Code code, int left_index, int right_index, int dest_index, bool pop_fpu_stack) {
2415   assert(pop_fpu_stack  || (left_index     == dest_index || right_index     == dest_index), &quot;invalid LIR&quot;);
2416   assert(!pop_fpu_stack || (left_index - 1 == dest_index || right_index - 1 == dest_index), &quot;invalid LIR&quot;);
2417   assert(left_index == 0 || right_index == 0, &quot;either must be on top of stack&quot;);
2418 
2419   bool left_is_tos = (left_index == 0);
2420   bool dest_is_tos = (dest_index == 0);
2421   int non_tos_index = (left_is_tos ? right_index : left_index);
2422 
2423   switch (code) {
2424     case lir_add:
2425       if (pop_fpu_stack)       __ faddp(non_tos_index);
2426       else if (dest_is_tos)    __ fadd (non_tos_index);
2427       else                     __ fadda(non_tos_index);
2428       break;
2429 
2430     case lir_sub:
2431       if (left_is_tos) {
2432         if (pop_fpu_stack)     __ fsubrp(non_tos_index);
2433         else if (dest_is_tos)  __ fsub  (non_tos_index);
</pre>
<hr />
<pre>
2451       if (left_is_tos) {
2452         if (pop_fpu_stack)     __ fdivrp(non_tos_index);
2453         else if (dest_is_tos)  __ fdiv  (non_tos_index);
2454         else                   __ fdivra(non_tos_index);
2455       } else {
2456         if (pop_fpu_stack)     __ fdivp (non_tos_index);
2457         else if (dest_is_tos)  __ fdivr (non_tos_index);
2458         else                   __ fdiva (non_tos_index);
2459       }
2460       break;
2461 
2462     case lir_rem:
2463       assert(left_is_tos &amp;&amp; dest_is_tos &amp;&amp; right_index == 1, &quot;must be guaranteed by FPU stack allocation&quot;);
2464       __ fremr(noreg);
2465       break;
2466 
2467     default:
2468       ShouldNotReachHere();
2469   }
2470 }
<span class="line-added">2471 #endif // _LP64</span>
2472 
2473 
2474 void LIR_Assembler::intrinsic_op(LIR_Code code, LIR_Opr value, LIR_Opr tmp, LIR_Opr dest, LIR_Op* op) {
2475   if (value-&gt;is_double_xmm()) {
2476     switch(code) {
2477       case lir_abs :
2478         {
2479 #ifdef _LP64
2480           if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512vl()) {
2481             assert(tmp-&gt;is_valid(), &quot;need temporary&quot;);
2482             __ vpandn(dest-&gt;as_xmm_double_reg(), tmp-&gt;as_xmm_double_reg(), value-&gt;as_xmm_double_reg(), 2);
2483           } else
2484 #endif
2485           {
2486             if (dest-&gt;as_xmm_double_reg() != value-&gt;as_xmm_double_reg()) {
2487               __ movdbl(dest-&gt;as_xmm_double_reg(), value-&gt;as_xmm_double_reg());
2488             }
2489             assert(!tmp-&gt;is_valid(), &quot;do not need temporary&quot;);
2490             __ andpd(dest-&gt;as_xmm_double_reg(),
2491                      ExternalAddress((address)double_signmask_pool));
2492           }
2493         }
2494         break;
2495 
2496       case lir_sqrt: __ sqrtsd(dest-&gt;as_xmm_double_reg(), value-&gt;as_xmm_double_reg()); break;
2497       // all other intrinsics are not available in the SSE instruction set, so FPU is used
2498       default      : ShouldNotReachHere();
2499     }
2500 
<span class="line-added">2501 #ifndef _LP64</span>
2502   } else if (value-&gt;is_double_fpu()) {
2503     assert(value-&gt;fpu_regnrLo() == 0 &amp;&amp; dest-&gt;fpu_regnrLo() == 0, &quot;both must be on TOS&quot;);
2504     switch(code) {
2505       case lir_abs   : __ fabs() ; break;
2506       case lir_sqrt  : __ fsqrt(); break;
2507       default      : ShouldNotReachHere();
2508     }
<span class="line-added">2509 #endif // !_LP64</span>
2510   } else {
2511     Unimplemented();
2512   }
2513 }
2514 
2515 void LIR_Assembler::logic_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst) {
2516   // assert(left-&gt;destroys_register(), &quot;check&quot;);
2517   if (left-&gt;is_single_cpu()) {
2518     Register reg = left-&gt;as_register();
2519     if (right-&gt;is_constant()) {
2520       int val = right-&gt;as_constant_ptr()-&gt;as_jint();
2521       switch (code) {
2522         case lir_logic_and: __ andl (reg, val); break;
2523         case lir_logic_or:  __ orl  (reg, val); break;
2524         case lir_logic_xor: __ xorl (reg, val); break;
2525         default: ShouldNotReachHere();
2526       }
2527     } else if (right-&gt;is_stack()) {
2528       // added support for stack operands
2529       Address raddr = frame_map()-&gt;address_for_slot(right-&gt;single_stack_ix());
</pre>
<hr />
<pre>
2566       int r_hi = right-&gt;as_constant_ptr()-&gt;as_jint_hi();
2567       switch (code) {
2568         case lir_logic_and:
2569           __ andl(l_lo, r_lo);
2570           __ andl(l_hi, r_hi);
2571           break;
2572         case lir_logic_or:
2573           __ orl(l_lo, r_lo);
2574           __ orl(l_hi, r_hi);
2575           break;
2576         case lir_logic_xor:
2577           __ xorl(l_lo, r_lo);
2578           __ xorl(l_hi, r_hi);
2579           break;
2580         default: ShouldNotReachHere();
2581       }
2582 #endif // _LP64
2583     } else {
2584 #ifdef _LP64
2585       Register r_lo;
<span class="line-modified">2586       if (is_reference_type(right-&gt;type())) {</span>
2587         r_lo = right-&gt;as_register();
2588       } else {
2589         r_lo = right-&gt;as_register_lo();
2590       }
2591 #else
2592       Register r_lo = right-&gt;as_register_lo();
2593       Register r_hi = right-&gt;as_register_hi();
2594       assert(l_lo != r_hi, &quot;overwriting registers&quot;);
2595 #endif
2596       switch (code) {
2597         case lir_logic_and:
2598           __ andptr(l_lo, r_lo);
2599           NOT_LP64(__ andptr(l_hi, r_hi);)
2600           break;
2601         case lir_logic_or:
2602           __ orptr(l_lo, r_lo);
2603           NOT_LP64(__ orptr(l_hi, r_hi);)
2604           break;
2605         case lir_logic_xor:
2606           __ xorptr(l_lo, r_lo);
</pre>
<hr />
<pre>
2679     move_regs(lreg, rax);
2680 
2681     int idivl_offset = __ corrected_idivl(rreg);
2682     if (ImplicitDiv0Checks) {
2683       add_debug_info_for_div0(idivl_offset, info);
2684     }
2685     if (code == lir_irem) {
2686       move_regs(rdx, dreg); // result is in rdx
2687     } else {
2688       move_regs(rax, dreg);
2689     }
2690   }
2691 }
2692 
2693 
2694 void LIR_Assembler::comp_op(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Op2* op) {
2695   if (opr1-&gt;is_single_cpu()) {
2696     Register reg1 = opr1-&gt;as_register();
2697     if (opr2-&gt;is_single_cpu()) {
2698       // cpu register - cpu register
<span class="line-modified">2699       if (is_reference_type(opr1-&gt;type())) {</span>
2700         __ cmpoop(reg1, opr2-&gt;as_register());
2701       } else {
<span class="line-modified">2702         assert(!is_reference_type(opr2-&gt;type()), &quot;cmp int, oop?&quot;);</span>
2703         __ cmpl(reg1, opr2-&gt;as_register());
2704       }
2705     } else if (opr2-&gt;is_stack()) {
2706       // cpu register - stack
<span class="line-modified">2707       if (is_reference_type(opr1-&gt;type())) {</span>
2708         __ cmpoop(reg1, frame_map()-&gt;address_for_slot(opr2-&gt;single_stack_ix()));
2709       } else {
2710         __ cmpl(reg1, frame_map()-&gt;address_for_slot(opr2-&gt;single_stack_ix()));
2711       }
2712     } else if (opr2-&gt;is_constant()) {
2713       // cpu register - constant
2714       LIR_Const* c = opr2-&gt;as_constant_ptr();
2715       if (c-&gt;type() == T_INT) {
2716         __ cmpl(reg1, c-&gt;as_jint());
<span class="line-modified">2717       } else if (c-&gt;type() == T_METADATA) {</span>
<span class="line-added">2718         // All we need for now is a comparison with NULL for equality.</span>
<span class="line-added">2719         assert(condition == lir_cond_equal || condition == lir_cond_notEqual, &quot;oops&quot;);</span>
<span class="line-added">2720         Metadata* m = c-&gt;as_metadata();</span>
<span class="line-added">2721         if (m == NULL) {</span>
<span class="line-added">2722           __ cmpptr(reg1, (int32_t)0);</span>
<span class="line-added">2723         } else {</span>
<span class="line-added">2724           ShouldNotReachHere();</span>
<span class="line-added">2725         }</span>
<span class="line-added">2726       } else if (is_reference_type(c-&gt;type())) {</span>
2727         // In 64bit oops are single register
2728         jobject o = c-&gt;as_jobject();
2729         if (o == NULL) {
2730           __ cmpptr(reg1, (int32_t)NULL_WORD);
2731         } else {
2732           __ cmpoop(reg1, o);
2733         }
2734       } else {
2735         fatal(&quot;unexpected type: %s&quot;, basictype_to_str(c-&gt;type()));
2736       }
2737       // cpu register - address
2738     } else if (opr2-&gt;is_address()) {
2739       if (op-&gt;info() != NULL) {
2740         add_debug_info_for_null_check_here(op-&gt;info());
2741       }
2742       __ cmpl(reg1, as_Address(opr2-&gt;as_address_ptr()));
2743     } else {
2744       ShouldNotReachHere();
2745     }
2746 
</pre>
<hr />
<pre>
2798     XMMRegister reg1 = opr1-&gt;as_xmm_double_reg();
2799     if (opr2-&gt;is_double_xmm()) {
2800       // xmm register - xmm register
2801       __ ucomisd(reg1, opr2-&gt;as_xmm_double_reg());
2802     } else if (opr2-&gt;is_stack()) {
2803       // xmm register - stack
2804       __ ucomisd(reg1, frame_map()-&gt;address_for_slot(opr2-&gt;double_stack_ix()));
2805     } else if (opr2-&gt;is_constant()) {
2806       // xmm register - constant
2807       __ ucomisd(reg1, InternalAddress(double_constant(opr2-&gt;as_jdouble())));
2808     } else if (opr2-&gt;is_address()) {
2809       // xmm register - address
2810       if (op-&gt;info() != NULL) {
2811         add_debug_info_for_null_check_here(op-&gt;info());
2812       }
2813       __ ucomisd(reg1, as_Address(opr2-&gt;pointer()-&gt;as_address()));
2814     } else {
2815       ShouldNotReachHere();
2816     }
2817 
<span class="line-added">2818 #ifndef _LP64</span>
2819   } else if(opr1-&gt;is_single_fpu() || opr1-&gt;is_double_fpu()) {
2820     assert(opr1-&gt;is_fpu_register() &amp;&amp; opr1-&gt;fpu() == 0, &quot;currently left-hand side must be on TOS (relax this restriction)&quot;);
2821     assert(opr2-&gt;is_fpu_register(), &quot;both must be registers&quot;);
2822     __ fcmp(noreg, opr2-&gt;fpu(), op-&gt;fpu_pop_count() &gt; 0, op-&gt;fpu_pop_count() &gt; 1);
<span class="line-added">2823 #endif // LP64</span>
2824 
2825   } else if (opr1-&gt;is_address() &amp;&amp; opr2-&gt;is_constant()) {
2826     LIR_Const* c = opr2-&gt;as_constant_ptr();
2827 #ifdef _LP64
<span class="line-modified">2828     if (is_reference_type(c-&gt;type())) {</span>
2829       assert(condition == lir_cond_equal || condition == lir_cond_notEqual, &quot;need to reverse&quot;);
2830       __ movoop(rscratch1, c-&gt;as_jobject());
2831     }
2832 #endif // LP64
2833     if (op-&gt;info() != NULL) {
2834       add_debug_info_for_null_check_here(op-&gt;info());
2835     }
2836     // special case: address - constant
2837     LIR_Address* addr = opr1-&gt;as_address_ptr();
2838     if (c-&gt;type() == T_INT) {
2839       __ cmpl(as_Address(addr), c-&gt;as_jint());
<span class="line-modified">2840     } else if (is_reference_type(c-&gt;type())) {</span>
2841 #ifdef _LP64
2842       // %%% Make this explode if addr isn&#39;t reachable until we figure out a
2843       // better strategy by giving noreg as the temp for as_Address
2844       __ cmpoop(rscratch1, as_Address(addr, noreg));
2845 #else
2846       __ cmpoop(as_Address(addr), c-&gt;as_jobject());
2847 #endif // _LP64
2848     } else {
2849       ShouldNotReachHere();
2850     }
2851 
2852   } else {
2853     ShouldNotReachHere();
2854   }
2855 }
2856 
2857 void LIR_Assembler::comp_fl2i(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst, LIR_Op2* op) {
2858   if (code == lir_cmp_fd2i || code == lir_ucmp_fd2i) {
2859     if (left-&gt;is_single_xmm()) {
2860       assert(right-&gt;is_single_xmm(), &quot;must match&quot;);
2861       __ cmpss2int(left-&gt;as_xmm_float_reg(), right-&gt;as_xmm_float_reg(), dst-&gt;as_register(), code == lir_ucmp_fd2i);
2862     } else if (left-&gt;is_double_xmm()) {
2863       assert(right-&gt;is_double_xmm(), &quot;must match&quot;);
2864       __ cmpsd2int(left-&gt;as_xmm_double_reg(), right-&gt;as_xmm_double_reg(), dst-&gt;as_register(), code == lir_ucmp_fd2i);
2865 
2866     } else {
<span class="line-added">2867 #ifdef _LP64</span>
<span class="line-added">2868       ShouldNotReachHere();</span>
<span class="line-added">2869 #else</span>
2870       assert(left-&gt;is_single_fpu() || left-&gt;is_double_fpu(), &quot;must be&quot;);
2871       assert(right-&gt;is_single_fpu() || right-&gt;is_double_fpu(), &quot;must match&quot;);
2872 
2873       assert(left-&gt;fpu() == 0, &quot;left must be on TOS&quot;);
2874       __ fcmp2int(dst-&gt;as_register(), code == lir_ucmp_fd2i, right-&gt;fpu(),
2875                   op-&gt;fpu_pop_count() &gt; 0, op-&gt;fpu_pop_count() &gt; 1);
<span class="line-added">2876 #endif // LP64</span>
2877     }
2878   } else {
2879     assert(code == lir_cmp_l2i, &quot;check&quot;);
2880 #ifdef _LP64
2881     Label done;
2882     Register dest = dst-&gt;as_register();
2883     __ cmpptr(left-&gt;as_register_lo(), right-&gt;as_register_lo());
2884     __ movl(dest, -1);
2885     __ jccb(Assembler::less, done);
2886     __ set_byte_if_not_zero(dest);
2887     __ movzbl(dest, dest);
2888     __ bind(done);
2889 #else
2890     __ lcmp2int(left-&gt;as_register_hi(),
2891                 left-&gt;as_register_lo(),
2892                 right-&gt;as_register_hi(),
2893                 right-&gt;as_register_lo());
2894     move_regs(left-&gt;as_register_hi(), dst-&gt;as_register());
2895 #endif // _LP64
2896   }
</pre>
<hr />
<pre>
3123 
3124 
3125 // This code replaces a call to arraycopy; no exception may
3126 // be thrown in this code, they must be thrown in the System.arraycopy
3127 // activation frame; we could save some checks if this would not be the case
3128 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
3129   ciArrayKlass* default_type = op-&gt;expected_type();
3130   Register src = op-&gt;src()-&gt;as_register();
3131   Register dst = op-&gt;dst()-&gt;as_register();
3132   Register src_pos = op-&gt;src_pos()-&gt;as_register();
3133   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
3134   Register length  = op-&gt;length()-&gt;as_register();
3135   Register tmp = op-&gt;tmp()-&gt;as_register();
3136 
3137   __ resolve(ACCESS_READ, src);
3138   __ resolve(ACCESS_WRITE, dst);
3139 
3140   CodeStub* stub = op-&gt;stub();
3141   int flags = op-&gt;flags();
3142   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
<span class="line-modified">3143   if (is_reference_type(basic_type)) basic_type = T_OBJECT;</span>
3144 
3145   // if we don&#39;t know anything, just go through the generic arraycopy
3146   if (default_type == NULL) {
3147     // save outgoing arguments on stack in case call to System.arraycopy is needed
3148     // HACK ALERT. This code used to push the parameters in a hardwired fashion
3149     // for interpreter calling conventions. Now we have to do it in new style conventions.
3150     // For the moment until C1 gets the new register allocator I just force all the
3151     // args to the right place (except the register args) and then on the back side
3152     // reload the register args properly if we go slow path. Yuck
3153 
3154     // These are proper for the calling convention
3155     store_parameter(length, 2);
3156     store_parameter(dst_pos, 1);
3157     store_parameter(dst, 0);
3158 
3159     // these are just temporary placements until we need to reload
3160     store_parameter(src_pos, 3);
3161     store_parameter(src, 4);
3162     NOT_LP64(assert(src == rcx &amp;&amp; src_pos == rdx, &quot;mismatch in calling convention&quot;);)
3163 
</pre>
<hr />
<pre>
3873       __ xorps(dest-&gt;as_xmm_float_reg(),
3874                ExternalAddress((address)float_signflip_pool));
3875     }
3876   } else if (dest-&gt;is_double_xmm()) {
3877 #ifdef _LP64
3878     if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512vl()) {
3879       assert(tmp-&gt;is_valid(), &quot;need temporary&quot;);
3880       assert_different_registers(left-&gt;as_xmm_double_reg(), tmp-&gt;as_xmm_double_reg());
3881       __ vpxor(dest-&gt;as_xmm_double_reg(), tmp-&gt;as_xmm_double_reg(), left-&gt;as_xmm_double_reg(), 2);
3882     }
3883     else
3884 #endif
3885     {
3886       assert(!tmp-&gt;is_valid(), &quot;do not need temporary&quot;);
3887       if (left-&gt;as_xmm_double_reg() != dest-&gt;as_xmm_double_reg()) {
3888         __ movdbl(dest-&gt;as_xmm_double_reg(), left-&gt;as_xmm_double_reg());
3889       }
3890       __ xorpd(dest-&gt;as_xmm_double_reg(),
3891                ExternalAddress((address)double_signflip_pool));
3892     }
<span class="line-added">3893 #ifndef _LP64</span>
3894   } else if (left-&gt;is_single_fpu() || left-&gt;is_double_fpu()) {
3895     assert(left-&gt;fpu() == 0, &quot;arg must be on TOS&quot;);
3896     assert(dest-&gt;fpu() == 0, &quot;dest must be TOS&quot;);
3897     __ fchs();
<span class="line-added">3898 #endif // !_LP64</span>
3899 
3900   } else {
3901     ShouldNotReachHere();
3902   }
3903 }
3904 
3905 
3906 void LIR_Assembler::leal(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
3907   assert(src-&gt;is_address(), &quot;must be an address&quot;);
3908   assert(dest-&gt;is_register(), &quot;must be a register&quot;);
3909 
3910   PatchingStub* patch = NULL;
3911   if (patch_code != lir_patch_none) {
3912     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
3913   }
3914 
3915   Register reg = dest-&gt;as_pointer_register();
3916   LIR_Address* addr = src-&gt;as_address_ptr();
3917   __ lea(reg, as_Address(addr));
3918 
</pre>
<hr />
<pre>
3948       __ psrlq(src-&gt;as_xmm_double_reg(), 32);
3949       __ movdl(dest-&gt;as_register_hi(), src-&gt;as_xmm_double_reg());
3950 #endif // _LP64
3951     } else if (dest-&gt;is_double_stack()) {
3952       __ movdbl(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix()), src-&gt;as_xmm_double_reg());
3953     } else if (dest-&gt;is_address()) {
3954       __ movdbl(as_Address(dest-&gt;as_address_ptr()), src-&gt;as_xmm_double_reg());
3955     } else {
3956       ShouldNotReachHere();
3957     }
3958 
3959   } else if (dest-&gt;is_double_xmm()) {
3960     if (src-&gt;is_double_stack()) {
3961       __ movdbl(dest-&gt;as_xmm_double_reg(), frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix()));
3962     } else if (src-&gt;is_address()) {
3963       __ movdbl(dest-&gt;as_xmm_double_reg(), as_Address(src-&gt;as_address_ptr()));
3964     } else {
3965       ShouldNotReachHere();
3966     }
3967 
<span class="line-added">3968 #ifndef _LP64</span>
3969   } else if (src-&gt;is_double_fpu()) {
3970     assert(src-&gt;fpu_regnrLo() == 0, &quot;must be TOS&quot;);
3971     if (dest-&gt;is_double_stack()) {
3972       __ fistp_d(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix()));
3973     } else if (dest-&gt;is_address()) {
3974       __ fistp_d(as_Address(dest-&gt;as_address_ptr()));
3975     } else {
3976       ShouldNotReachHere();
3977     }
3978 
3979   } else if (dest-&gt;is_double_fpu()) {
3980     assert(dest-&gt;fpu_regnrLo() == 0, &quot;must be TOS&quot;);
3981     if (src-&gt;is_double_stack()) {
3982       __ fild_d(frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix()));
3983     } else if (src-&gt;is_address()) {
3984       __ fild_d(as_Address(src-&gt;as_address_ptr()));
3985     } else {
3986       ShouldNotReachHere();
3987     }
<span class="line-added">3988 #endif // !_LP64</span>
<span class="line-added">3989 </span>
3990   } else {
3991     ShouldNotReachHere();
3992   }
3993 }
3994 
3995 #ifdef ASSERT
3996 // emit run-time assertion
3997 void LIR_Assembler::emit_assert(LIR_OpAssert* op) {
3998   assert(op-&gt;code() == lir_assert, &quot;must be&quot;);
3999 
4000   if (op-&gt;in_opr1()-&gt;is_valid()) {
4001     assert(op-&gt;in_opr2()-&gt;is_valid(), &quot;both operands must be valid&quot;);
4002     comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
4003   } else {
4004     assert(op-&gt;in_opr2()-&gt;is_illegal(), &quot;both operands must be illegal&quot;);
4005     assert(op-&gt;condition() == lir_cond_always, &quot;no other conditions allowed&quot;);
4006   }
4007 
4008   Label ok;
4009   if (op-&gt;condition() != lir_cond_always) {
</pre>
</td>
</tr>
</table>
<center><a href="c1_FrameMap_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_LIRAssembler_x86.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>