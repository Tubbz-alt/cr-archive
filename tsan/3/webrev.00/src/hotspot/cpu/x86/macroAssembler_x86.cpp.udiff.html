<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Udiff src/hotspot/cpu/x86/macroAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="jvmciCodeInstaller_x86.cpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_x86.hpp.udiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/macroAssembler_x86.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-new-header">@@ -1,7 +1,7 @@</span>
  /*
<span class="udiff-line-modified-removed">-  * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.</span>
<span class="udiff-line-modified-added">+  * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -32,10 +32,11 @@</span>
  #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
  #include &quot;interpreter/interpreter.hpp&quot;
  #include &quot;memory/resourceArea.hpp&quot;
  #include &quot;memory/universe.hpp&quot;
  #include &quot;oops/accessDecorators.hpp&quot;
<span class="udiff-line-added">+ #include &quot;oops/compressedOops.inline.hpp&quot;</span>
  #include &quot;oops/klass.inline.hpp&quot;
  #include &quot;prims/methodHandles.hpp&quot;
  #include &quot;runtime/biasedLocking.hpp&quot;
  #include &quot;runtime/flags/flagSetting.hpp&quot;
  #include &quot;runtime/interfaceSupport.inline.hpp&quot;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -346,28 +347,17 @@</span>
    pop(rdx);
    pop(rdi);
    pop(rsi);
  }
  
<span class="udiff-line-removed">- void MacroAssembler::pop_fTOS() {</span>
<span class="udiff-line-removed">-   fld_d(Address(rsp, 0));</span>
<span class="udiff-line-removed">-   addl(rsp, 2 * wordSize);</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-removed">- </span>
  void MacroAssembler::push_callee_saved_registers() {
    push(rsi);
    push(rdi);
    push(rdx);
    push(rcx);
  }
  
<span class="udiff-line-removed">- void MacroAssembler::push_fTOS() {</span>
<span class="udiff-line-removed">-   subl(rsp, 2 * wordSize);</span>
<span class="udiff-line-removed">-   fstp_d(Address(rsp, 0));</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">- </span>
  void MacroAssembler::pushoop(jobject obj) {
    push_literal32((int32_t)obj, oop_Relocation::spec_for_immediate());
  }
  
  void MacroAssembler::pushklass(Metadata* obj) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -424,17 +414,12 @@</span>
      // This is the value of eip which points to where verify_oop will return.
      if (os::message_box(msg, &quot;Execution stopped, print registers?&quot;)) {
        print_state32(rdi, rsi, rbp, rsp, rbx, rdx, rcx, rax, eip);
        BREAKPOINT;
      }
<span class="udiff-line-removed">-   } else {</span>
<span class="udiff-line-removed">-     ttyLocker ttyl;</span>
<span class="udiff-line-removed">-     ::tty-&gt;print_cr(&quot;=============== DEBUG MESSAGE: %s ================\n&quot;, msg);</span>
    }
<span class="udiff-line-modified-removed">-   // Don&#39;t assert holding the ttyLock</span>
<span class="udiff-line-removed">-     assert(false, &quot;DEBUG MESSAGE: %s&quot;, msg);</span>
<span class="udiff-line-removed">-   ThreadStateTransition::transition(thread, _thread_in_vm, saved_state);</span>
<span class="udiff-line-modified-added">+   fatal(&quot;DEBUG MESSAGE: %s&quot;, msg);</span>
  }
  
  void MacroAssembler::print_state32(int rdi, int rsi, int rbp, int rsp, int rbx, int rdx, int rcx, int rax, int eip) {
    ttyLocker ttyl;
    FlagSetting fs(Debugging, true);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -826,15 +811,17 @@</span>
      masm-&gt;mov(c_rarg3, arg);
    }
  }
  
  void MacroAssembler::stop(const char* msg) {
<span class="udiff-line-modified-removed">-   address rip = pc();</span>
<span class="udiff-line-modified-removed">-   pusha(); // get regs on stack</span>
<span class="udiff-line-modified-added">+   if (ShowMessageBoxOnError) {</span>
<span class="udiff-line-modified-added">+     address rip = pc();</span>
<span class="udiff-line-added">+     pusha(); // get regs on stack</span>
<span class="udiff-line-added">+     lea(c_rarg1, InternalAddress(rip));</span>
<span class="udiff-line-added">+     movq(c_rarg2, rsp); // pass pointer to regs array</span>
<span class="udiff-line-added">+   }</span>
    lea(c_rarg0, ExternalAddress((address) msg));
<span class="udiff-line-removed">-   lea(c_rarg1, InternalAddress(rip));</span>
<span class="udiff-line-removed">-   movq(c_rarg2, rsp); // pass pointer to regs array</span>
    andq(rsp, -16); // align stack as required by ABI
    call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug64)));
    hlt();
  }
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -889,19 +876,13 @@</span>
      // XXX correct this offset for amd64
      // This is the value of eip which points to where verify_oop will return.
      if (os::message_box(msg, &quot;Execution stopped, print registers?&quot;)) {
        print_state64(pc, regs);
        BREAKPOINT;
<span class="udiff-line-removed">-       assert(false, &quot;start up GDB&quot;);</span>
      }
<span class="udiff-line-removed">-     ThreadStateTransition::transition(thread, _thread_in_vm, saved_state);</span>
<span class="udiff-line-removed">-   } else {</span>
<span class="udiff-line-removed">-     ttyLocker ttyl;</span>
<span class="udiff-line-removed">-     ::tty-&gt;print_cr(&quot;=============== DEBUG MESSAGE: %s ================\n&quot;,</span>
<span class="udiff-line-removed">-                     msg);</span>
<span class="udiff-line-removed">-     assert(false, &quot;DEBUG MESSAGE: %s&quot;, msg);</span>
    }
<span class="udiff-line-added">+   fatal(&quot;DEBUG MESSAGE: %s&quot;, msg);</span>
  }
  
  void MacroAssembler::print_state64(int64_t pc, int64_t regs[]) {
    ttyLocker ttyl;
    FlagSetting fs(Debugging, true);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1001,29 +982,29 @@</span>
    if (target % modulus != 0) {
      nop(modulus - (target % modulus));
    }
  }
  
<span class="udiff-line-modified-removed">- void MacroAssembler::andpd(XMMRegister dst, AddressLiteral src) {</span>
<span class="udiff-line-modified-added">+ void MacroAssembler::andpd(XMMRegister dst, AddressLiteral src, Register scratch_reg) {</span>
    // Used in sign-masking with aligned address.
    assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
    if (reachable(src)) {
      Assembler::andpd(dst, as_Address(src));
    } else {
<span class="udiff-line-modified-removed">-     lea(rscratch1, src);</span>
<span class="udiff-line-modified-removed">-     Assembler::andpd(dst, Address(rscratch1, 0));</span>
<span class="udiff-line-modified-added">+     lea(scratch_reg, src);</span>
<span class="udiff-line-modified-added">+     Assembler::andpd(dst, Address(scratch_reg, 0));</span>
    }
  }
  
<span class="udiff-line-modified-removed">- void MacroAssembler::andps(XMMRegister dst, AddressLiteral src) {</span>
<span class="udiff-line-modified-added">+ void MacroAssembler::andps(XMMRegister dst, AddressLiteral src, Register scratch_reg) {</span>
    // Used in sign-masking with aligned address.
    assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
    if (reachable(src)) {
      Assembler::andps(dst, as_Address(src));
    } else {
<span class="udiff-line-modified-removed">-     lea(rscratch1, src);</span>
<span class="udiff-line-modified-removed">-     Assembler::andps(dst, Address(rscratch1, 0));</span>
<span class="udiff-line-modified-added">+     lea(scratch_reg, src);</span>
<span class="udiff-line-modified-added">+     Assembler::andps(dst, Address(scratch_reg, 0));</span>
    }
  }
  
  void MacroAssembler::andptr(Register dst, int32_t imm32) {
    LP64_ONLY(andq(dst, imm32)) NOT_LP64(andl(dst, imm32));
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1112,11 +1093,11 @@</span>
                                           BiasedLockingCounters* counters) {
    assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
    assert(swap_reg == rax, &quot;swap_reg must be rax for cmpxchgq&quot;);
    assert(tmp_reg != noreg, &quot;tmp_reg must be supplied&quot;);
    assert_different_registers(lock_reg, obj_reg, swap_reg, tmp_reg);
<span class="udiff-line-modified-removed">-   assert(markOopDesc::age_shift == markOopDesc::lock_bits + markOopDesc::biased_lock_bits, &quot;biased locking makes assumptions about bit layout&quot;);</span>
<span class="udiff-line-modified-added">+   assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits, &quot;biased locking makes assumptions about bit layout&quot;);</span>
    Address mark_addr      (obj_reg, oopDesc::mark_offset_in_bytes());
    NOT_LP64( Address saved_mark_addr(lock_reg, 0); )
  
    if (PrintBiasedLockingStatistics &amp;&amp; counters == NULL) {
      counters = BiasedLocking::counters();
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1132,12 +1113,12 @@</span>
    if (!swap_reg_contains_mark) {
      null_check_offset = offset();
      movptr(swap_reg, mark_addr);
    }
    movptr(tmp_reg, swap_reg);
<span class="udiff-line-modified-removed">-   andptr(tmp_reg, markOopDesc::biased_lock_mask_in_place);</span>
<span class="udiff-line-modified-removed">-   cmpptr(tmp_reg, markOopDesc::biased_lock_pattern);</span>
<span class="udiff-line-modified-added">+   andptr(tmp_reg, markWord::biased_lock_mask_in_place);</span>
<span class="udiff-line-modified-added">+   cmpptr(tmp_reg, markWord::biased_lock_pattern);</span>
    jcc(Assembler::notEqual, cas_label);
    // The bias pattern is present in the object&#39;s header. Need to check
    // whether the bias owner and the epoch are both still current.
  #ifndef _LP64
    // Note that because there is no current thread register on x86_32 we
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1159,11 +1140,11 @@</span>
    xorptr(tmp_reg, swap_reg);
    get_thread(swap_reg);
    xorptr(swap_reg, tmp_reg);
    Register header_reg = swap_reg;
  #endif
<span class="udiff-line-modified-removed">-   andptr(header_reg, ~((int) markOopDesc::age_mask_in_place));</span>
<span class="udiff-line-modified-added">+   andptr(header_reg, ~((int) markWord::age_mask_in_place));</span>
    if (counters != NULL) {
      cond_inc32(Assembler::zero,
                 ExternalAddress((address) counters-&gt;biased_lock_entry_count_addr()));
    }
    jcc(Assembler::equal, done);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1178,11 +1159,11 @@</span>
    // header.
  
    // If the low three bits in the xor result aren&#39;t clear, that means
    // the prototype header is no longer biased and we have to revoke
    // the bias on this object.
<span class="udiff-line-modified-removed">-   testptr(header_reg, markOopDesc::biased_lock_mask_in_place);</span>
<span class="udiff-line-modified-added">+   testptr(header_reg, markWord::biased_lock_mask_in_place);</span>
    jccb(Assembler::notZero, try_revoke_bias);
  
    // Biasing is still enabled for this data type. See whether the
    // epoch of the current bias is still valid, meaning that the epoch
    // bits of the mark word are equal to the epoch bits of the
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1190,22 +1171,22 @@</span>
    // only change at a safepoint.) If not, attempt to rebias the object
    // toward the current thread. Note that we must be absolutely sure
    // that the current epoch is invalid in order to do this because
    // otherwise the manipulations it performs on the mark word are
    // illegal.
<span class="udiff-line-modified-removed">-   testptr(header_reg, markOopDesc::epoch_mask_in_place);</span>
<span class="udiff-line-modified-added">+   testptr(header_reg, markWord::epoch_mask_in_place);</span>
    jccb(Assembler::notZero, try_rebias);
  
    // The epoch of the current bias is still valid but we know nothing
    // about the owner; it might be set or it might be clear. Try to
    // acquire the bias of the object using an atomic operation. If this
    // fails we will go in to the runtime to revoke the object&#39;s bias.
    // Note that we first construct the presumed unbiased header so we
    // don&#39;t accidentally blow away another thread&#39;s valid bias.
    NOT_LP64( movptr(swap_reg, saved_mark_addr); )
    andptr(swap_reg,
<span class="udiff-line-modified-removed">-          markOopDesc::biased_lock_mask_in_place | markOopDesc::age_mask_in_place | markOopDesc::epoch_mask_in_place);</span>
<span class="udiff-line-modified-added">+          markWord::biased_lock_mask_in_place | markWord::age_mask_in_place | markWord::epoch_mask_in_place);</span>
  #ifdef _LP64
    movptr(tmp_reg, swap_reg);
    orptr(tmp_reg, r15_thread);
  #else
    get_thread(tmp_reg);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1295,12 +1276,12 @@</span>
    // First, the interpreter checks for IllegalMonitorStateException at
    // a higher level. Second, if the bias was revoked while we held the
    // lock, the object could not be rebiased toward another thread, so
    // the bias bit would be clear.
    movptr(temp_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
<span class="udiff-line-modified-removed">-   andptr(temp_reg, markOopDesc::biased_lock_mask_in_place);</span>
<span class="udiff-line-modified-removed">-   cmpptr(temp_reg, markOopDesc::biased_lock_pattern);</span>
<span class="udiff-line-modified-added">+   andptr(temp_reg, markWord::biased_lock_mask_in_place);</span>
<span class="udiff-line-modified-added">+   cmpptr(temp_reg, markWord::biased_lock_pattern);</span>
    jcc(Assembler::equal, done);
  }
  
  #ifdef COMPILER2
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1483,11 +1464,11 @@</span>
    if (RTMRetryCount &gt; 0) {
      movl(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort
      bind(L_rtm_retry);
    }
    movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));
<span class="udiff-line-modified-removed">-   testptr(tmpReg, markOopDesc::monitor_value);  // inflated vs stack-locked|neutral|biased</span>
<span class="udiff-line-modified-added">+   testptr(tmpReg, markWord::monitor_value);  // inflated vs stack-locked|neutral|biased</span>
    jcc(Assembler::notZero, IsInflated);
  
    if (PrintPreciseRTMLockingStatistics || profile_rtm) {
      Label L_noincrement;
      if (RTMTotalCountIncrRate &gt; 1) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1498,12 +1479,12 @@</span>
      atomic_incptr(ExternalAddress((address)stack_rtm_counters-&gt;total_count_addr()), scrReg);
      bind(L_noincrement);
    }
    xbegin(L_on_abort);
    movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));       // fetch markword
<span class="udiff-line-modified-removed">-   andptr(tmpReg, markOopDesc::biased_lock_mask_in_place); // look at 3 lock bits</span>
<span class="udiff-line-modified-removed">-   cmpptr(tmpReg, markOopDesc::unlocked_value);            // bits = 001 unlocked</span>
<span class="udiff-line-modified-added">+   andptr(tmpReg, markWord::biased_lock_mask_in_place); // look at 3 lock bits</span>
<span class="udiff-line-modified-added">+   cmpptr(tmpReg, markWord::unlocked_value);            // bits = 001 unlocked</span>
    jcc(Assembler::equal, DONE_LABEL);        // all done if unlocked
  
    Register abort_status_Reg = tmpReg; // status of abort is stored in RAX
    if (UseRTMXendForLockBusy) {
      xend();
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1525,11 +1506,11 @@</span>
  }
  
  // Use RTM for inflating locks
  // inputs: objReg (object to lock)
  //         boxReg (on-stack box address (displaced header location) - KILLED)
<span class="udiff-line-modified-removed">- //         tmpReg (ObjectMonitor address + markOopDesc::monitor_value)</span>
<span class="udiff-line-modified-added">+ //         tmpReg (ObjectMonitor address + markWord::monitor_value)</span>
  void MacroAssembler::rtm_inflated_locking(Register objReg, Register boxReg, Register tmpReg,
                                            Register scrReg, Register retry_on_busy_count_Reg,
                                            Register retry_on_abort_count_Reg,
                                            RTMLockingCounters* rtm_counters,
                                            Metadata* method_data, bool profile_rtm,
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1538,12 +1519,12 @@</span>
    assert(tmpReg == rax, &quot;&quot;);
    assert(scrReg == rdx, &quot;&quot;);
    Label L_rtm_retry, L_decrement_retry, L_on_abort;
    int owner_offset = OM_OFFSET_NO_MONITOR_VALUE_TAG(owner);
  
<span class="udiff-line-modified-removed">-   // Without cast to int32_t a movptr will destroy r10 which is typically obj</span>
<span class="udiff-line-modified-removed">-   movptr(Address(boxReg, 0), (int32_t)intptr_t(markOopDesc::unused_mark()));</span>
<span class="udiff-line-modified-added">+   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.</span>
<span class="udiff-line-modified-added">+   movptr(Address(boxReg, 0), (int32_t)intptr_t(markWord::unused_mark().value()));</span>
    movptr(boxReg, tmpReg); // Save ObjectMonitor address
  
    if (RTMRetryCount &gt; 0) {
      movl(retry_on_busy_count_Reg, RTMRetryCount);  // Retry on lock busy
      movl(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1608,24 +1589,24 @@</span>
    }
  }
  
  #endif //  INCLUDE_RTM_OPT
  
<span class="udiff-line-modified-removed">- // Fast_Lock and Fast_Unlock used by C2</span>
<span class="udiff-line-modified-added">+ // fast_lock and fast_unlock used by C2</span>
  
  // Because the transitions from emitted code to the runtime
  // monitorenter/exit helper stubs are so slow it&#39;s critical that
<span class="udiff-line-modified-removed">- // we inline both the stack-locking fast-path and the inflated fast path.</span>
<span class="udiff-line-modified-added">+ // we inline both the stack-locking fast path and the inflated fast path.</span>
  //
  // See also: cmpFastLock and cmpFastUnlock.
  //
  // What follows is a specialized inline transliteration of the code
<span class="udiff-line-modified-removed">- // in slow_enter() and slow_exit().  If we&#39;re concerned about I$ bloat</span>
<span class="udiff-line-modified-removed">- // another option would be to emit TrySlowEnter and TrySlowExit methods</span>
<span class="udiff-line-modified-added">+ // in enter() and exit(). If we&#39;re concerned about I$ bloat another</span>
<span class="udiff-line-modified-added">+ // option would be to emit TrySlowEnter and TrySlowExit methods</span>
  // at startup-time.  These methods would accept arguments as
  // (rax,=Obj, rbx=Self, rcx=box, rdx=Scratch) and return success-failure
<span class="udiff-line-modified-removed">- // indications in the icc.ZFlag.  Fast_Lock and Fast_Unlock would simply</span>
<span class="udiff-line-modified-added">+ // indications in the icc.ZFlag.  fast_lock and fast_unlock would simply</span>
  // marshal the arguments and emit calls to TrySlowEnter and TrySlowExit.
  // In practice, however, the # of lock sites is bounded and is usually small.
  // Besides the call overhead, TrySlowEnter and TrySlowExit might suffer
  // if the processor uses simple bimodal branch predictors keyed by EIP
  // Since the helper routines would be called from multiple synchronization
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1640,12 +1621,12 @@</span>
  // to (a) prevent compiler-JIT reordering of non-volatile accesses, and
  // (b) explicit barriers or fence operations.
  //
  // TODO:
  //
<span class="udiff-line-modified-removed">- // *  Arrange for C2 to pass &quot;Self&quot; into Fast_Lock and Fast_Unlock in one of the registers (scr).</span>
<span class="udiff-line-modified-removed">- //    This avoids manifesting the Self pointer in the Fast_Lock and Fast_Unlock terminals.</span>
<span class="udiff-line-modified-added">+ // *  Arrange for C2 to pass &quot;Self&quot; into fast_lock and fast_unlock in one of the registers (scr).</span>
<span class="udiff-line-modified-added">+ //    This avoids manifesting the Self pointer in the fast_lock and fast_unlock terminals.</span>
  //    Given TLAB allocation, Self is usually manifested in a register, so passing it into
  //    the lock operators would typically be faster than reifying Self.
  //
  // *  Ideally I&#39;d define the primitives as:
  //       fast_lock   (nax Obj, nax box, EAX tmp, nax scr) where box, tmp and scr are KILLED.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1667,18 +1648,18 @@</span>
  //    avoid the expensive JNI call to JVM_Notify() and JVM_NotifyAll().
  //
  // *  use jccb and jmpb instead of jcc and jmp to improve code density.
  //    But beware of excessive branch density on AMD Opterons.
  //
<span class="udiff-line-modified-removed">- // *  Both Fast_Lock and Fast_Unlock set the ICC.ZF to indicate success</span>
<span class="udiff-line-modified-removed">- //    or failure of the fast-path.  If the fast-path fails then we pass</span>
<span class="udiff-line-modified-removed">- //    control to the slow-path, typically in C.  In Fast_Lock and</span>
<span class="udiff-line-modified-removed">- //    Fast_Unlock we often branch to DONE_LABEL, just to find that C2</span>
<span class="udiff-line-modified-added">+ // *  Both fast_lock and fast_unlock set the ICC.ZF to indicate success</span>
<span class="udiff-line-modified-added">+ //    or failure of the fast path.  If the fast path fails then we pass</span>
<span class="udiff-line-modified-added">+ //    control to the slow path, typically in C.  In fast_lock and</span>
<span class="udiff-line-modified-added">+ //    fast_unlock we often branch to DONE_LABEL, just to find that C2</span>
  //    will emit a conditional branch immediately after the node.
  //    So we have branches to branches and lots of ICC.ZF games.
  //    Instead, it might be better to have C2 pass a &quot;FailureLabel&quot;
<span class="udiff-line-modified-removed">- //    into Fast_Lock and Fast_Unlock.  In the case of success, control</span>
<span class="udiff-line-modified-added">+ //    into fast_lock and fast_unlock.  In the case of success, control</span>
  //    will drop through the node.  ICC.ZF is undefined at exit.
  //    In the case of failure, the node will branch directly to the
  //    FailureLabel
  
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1745,15 +1726,15 @@</span>
                        DONE_LABEL, IsInflated);
    }
  #endif // INCLUDE_RTM_OPT
  
    movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));          // [FETCH]
<span class="udiff-line-modified-removed">-   testptr(tmpReg, markOopDesc::monitor_value); // inflated vs stack-locked|neutral|biased</span>
<span class="udiff-line-modified-added">+   testptr(tmpReg, markWord::monitor_value); // inflated vs stack-locked|neutral|biased</span>
    jccb(Assembler::notZero, IsInflated);
  
    // Attempt stack-locking ...
<span class="udiff-line-modified-removed">-   orptr (tmpReg, markOopDesc::unlocked_value);</span>
<span class="udiff-line-modified-added">+   orptr (tmpReg, markWord::unlocked_value);</span>
    movptr(Address(boxReg, 0), tmpReg);          // Anticipate successful CAS
    lock();
    cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      // Updates tmpReg
    if (counters != NULL) {
      cond_inc32(Assembler::equal,
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1773,11 +1754,11 @@</span>
                 ExternalAddress((address)counters-&gt;fast_path_entry_count_addr()));
    }
    jmp(DONE_LABEL);
  
    bind(IsInflated);
<span class="udiff-line-modified-removed">-   // The object is inflated. tmpReg contains pointer to ObjectMonitor* + markOopDesc::monitor_value</span>
<span class="udiff-line-modified-added">+   // The object is inflated. tmpReg contains pointer to ObjectMonitor* + markWord::monitor_value</span>
  
  #if INCLUDE_RTM_OPT
    // Use the same RTM locking code in 32- and 64-bit VM.
    if (use_rtm) {
      rtm_inflated_locking(objReg, boxReg, tmpReg, scrReg, cx1Reg, cx2Reg,
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1788,11 +1769,11 @@</span>
  #ifndef _LP64
    // The object is inflated.
  
    // boxReg refers to the on-stack BasicLock in the current frame.
    // We&#39;d like to write:
<span class="udiff-line-modified-removed">-   //   set box-&gt;_displaced_header = markOopDesc::unused_mark().  Any non-0 value suffices.</span>
<span class="udiff-line-modified-added">+   //   set box-&gt;_displaced_header = markWord::unused_mark().  Any non-0 value suffices.</span>
    // This is convenient but results a ST-before-CAS penalty.  The following CAS suffers
    // additional latency as we have another ST in the store buffer that must drain.
  
    // avoid ST-before-CAS
    // register juggle because we need tmpReg for cmpxchgptr below
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1819,27 +1800,26 @@</span>
    // update _owner from BasicLock to thread
    get_thread (scrReg);                    // beware: clobbers ICCs
    movptr(Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), scrReg);
    xorptr(boxReg, boxReg);                 // set icc.ZFlag = 1 to indicate success
  
<span class="udiff-line-modified-removed">-   // If the CAS fails we can either retry or pass control to the slow-path.</span>
<span class="udiff-line-modified-added">+   // If the CAS fails we can either retry or pass control to the slow path.</span>
    // We use the latter tactic.
    // Pass the CAS result in the icc.ZFlag into DONE_LABEL
    // If the CAS was successful ...
    //   Self has acquired the lock
    //   Invariant: m-&gt;_recursions should already be 0, so we don&#39;t need to explicitly set it.
    // Intentional fall-through into DONE_LABEL ...
  #else // _LP64
<span class="udiff-line-modified-removed">-   // It&#39;s inflated</span>
<span class="udiff-line-modified-added">+   // It&#39;s inflated and we use scrReg for ObjectMonitor* in this section.</span>
    movq(scrReg, tmpReg);
    xorq(tmpReg, tmpReg);
<span class="udiff-line-removed">- </span>
    lock();
    cmpxchgptr(r15_thread, Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
<span class="udiff-line-modified-removed">-   // Unconditionally set box-&gt;_displaced_header = markOopDesc::unused_mark().</span>
<span class="udiff-line-modified-removed">-   // Without cast to int32_t movptr will destroy r10 which is typically obj.</span>
<span class="udiff-line-modified-removed">-   movptr(Address(boxReg, 0), (int32_t)intptr_t(markOopDesc::unused_mark()));</span>
<span class="udiff-line-modified-added">+   // Unconditionally set box-&gt;_displaced_header = markWord::unused_mark().</span>
<span class="udiff-line-modified-added">+   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.</span>
<span class="udiff-line-modified-added">+   movptr(Address(boxReg, 0), (int32_t)intptr_t(markWord::unused_mark().value()));</span>
    // Intentional fall-through into DONE_LABEL ...
    // Propagate ICC.ZF from CAS above into DONE_LABEL.
  #endif // _LP64
  #if INCLUDE_RTM_OPT
    } // use_rtm()
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1850,22 +1830,22 @@</span>
    // most efficient &quot;long&quot; NOP encodings.
    // Unfortunately none of our alignment mechanisms suffice.
    bind(DONE_LABEL);
  
    // At DONE_LABEL the icc ZFlag is set as follows ...
<span class="udiff-line-modified-removed">-   // Fast_Unlock uses the same protocol.</span>
<span class="udiff-line-modified-added">+   // fast_unlock uses the same protocol.</span>
    // ZFlag == 1 -&gt; Success
<span class="udiff-line-modified-removed">-   // ZFlag == 0 -&gt; Failure - force control through the slow-path</span>
<span class="udiff-line-modified-added">+   // ZFlag == 0 -&gt; Failure - force control through the slow path</span>
  }
  
  // obj: object to unlock
  // box: box address (displaced header location), killed.  Must be EAX.
  // tmp: killed, cannot be obj nor box.
  //
  // Some commentary on balanced locking:
  //
<span class="udiff-line-modified-removed">- // Fast_Lock and Fast_Unlock are emitted only for provably balanced lock sites.</span>
<span class="udiff-line-modified-added">+ // fast_lock and fast_unlock are emitted only for provably balanced lock sites.</span>
  // Methods that don&#39;t have provably balanced locking are forced to run in the
  // interpreter - such methods won&#39;t be compiled to use fast_lock and fast_unlock.
  // The interpreter provides two properties:
  // I1:  At return-time the interpreter automatically and quietly unlocks any
  //      objects acquired the current activation (frame).  Recall that the
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1882,11 +1862,11 @@</span>
  // The only other source of unbalanced locking would be JNI.  The &quot;Java Native Interface:
  // Programmer&#39;s Guide and Specification&quot; claims that an object locked by jni_monitorenter
  // should not be unlocked by &quot;normal&quot; java-level locking and vice-versa.  The specification
  // doesn&#39;t specify what will occur if a program engages in such mixed-mode locking, however.
  // Arguably given that the spec legislates the JNI case as undefined our implementation
<span class="udiff-line-modified-removed">- // could reasonably *avoid* checking owner in Fast_Unlock().</span>
<span class="udiff-line-modified-added">+ // could reasonably *avoid* checking owner in fast_unlock().</span>
  // In the interest of performance we elide m-&gt;Owner==Self check in unlock.
  // A perfectly viable alternative is to elide the owner check except when
  // Xcheck:jni is enabled.
  
  void MacroAssembler::fast_unlock(Register objReg, Register boxReg, Register tmpReg, bool use_rtm) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1903,24 +1883,24 @@</span>
  
  #if INCLUDE_RTM_OPT
    if (UseRTMForStackLocks &amp;&amp; use_rtm) {
      assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
      Label L_regular_unlock;
<span class="udiff-line-modified-removed">-     movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));           // fetch markword</span>
<span class="udiff-line-modified-removed">-     andptr(tmpReg, markOopDesc::biased_lock_mask_in_place); // look at 3 lock bits</span>
<span class="udiff-line-modified-removed">-     cmpptr(tmpReg, markOopDesc::unlocked_value);            // bits = 001 unlocked</span>
<span class="udiff-line-modified-removed">-     jccb(Assembler::notEqual, L_regular_unlock);  // if !HLE RegularLock</span>
<span class="udiff-line-modified-removed">-     xend();                                       // otherwise end...</span>
<span class="udiff-line-modified-removed">-     jmp(DONE_LABEL);                              // ... and we&#39;re done</span>
<span class="udiff-line-modified-added">+     movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // fetch markword</span>
<span class="udiff-line-modified-added">+     andptr(tmpReg, markWord::biased_lock_mask_in_place);              // look at 3 lock bits</span>
<span class="udiff-line-modified-added">+     cmpptr(tmpReg, markWord::unlocked_value);                         // bits = 001 unlocked</span>
<span class="udiff-line-modified-added">+     jccb(Assembler::notEqual, L_regular_unlock);                      // if !HLE RegularLock</span>
<span class="udiff-line-modified-added">+     xend();                                                           // otherwise end...</span>
<span class="udiff-line-modified-added">+     jmp(DONE_LABEL);                                                  // ... and we&#39;re done</span>
      bind(L_regular_unlock);
    }
  #endif
  
<span class="udiff-line-modified-removed">-   cmpptr(Address(boxReg, 0), (int32_t)NULL_WORD); // Examine the displaced header</span>
<span class="udiff-line-modified-removed">-   jcc   (Assembler::zero, DONE_LABEL);            // 0 indicates recursive stack-lock</span>
<span class="udiff-line-modified-removed">-   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));             // Examine the object&#39;s markword</span>
<span class="udiff-line-modified-removed">-   testptr(tmpReg, markOopDesc::monitor_value);    // Inflated?</span>
<span class="udiff-line-modified-added">+   cmpptr(Address(boxReg, 0), (int32_t)NULL_WORD);                   // Examine the displaced header</span>
<span class="udiff-line-modified-added">+   jcc   (Assembler::zero, DONE_LABEL);                              // 0 indicates recursive stack-lock</span>
<span class="udiff-line-modified-added">+   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Examine the object&#39;s markword</span>
<span class="udiff-line-modified-added">+   testptr(tmpReg, markWord::monitor_value);                         // Inflated?</span>
    jccb  (Assembler::zero, Stacked);
  
    // It&#39;s inflated.
  #if INCLUDE_RTM_OPT
    if (use_rtm) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1947,11 +1927,11 @@</span>
    //
    // If there&#39;s no contention try a 1-0 exit.  That is, exit without
    // a costly MEMBAR or CAS.  See synchronizer.cpp for details on how
    // we detect and recover from the race that the 1-0 exit admits.
    //
<span class="udiff-line-modified-removed">-   // Conceptually Fast_Unlock() must execute a STST|LDST &quot;release&quot; barrier</span>
<span class="udiff-line-modified-added">+   // Conceptually fast_unlock() must execute a STST|LDST &quot;release&quot; barrier</span>
    // before it STs null into _owner, releasing the lock.  Updates
    // to data protected by the critical section must be visible before
    // we drop the lock (and thus before any other thread could acquire
    // the lock and observe the fields protected by the lock).
    // IA32&#39;s memory-model is SPO, so STs are ordered with respect to
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1996,25 +1976,27 @@</span>
    orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));
    jccb  (Assembler::notZero, DONE_LABEL);
    movptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));
    orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));
    jccb  (Assembler::notZero, CheckSucc);
<span class="udiff-line-added">+   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.</span>
    movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t)NULL_WORD);
    jmpb  (DONE_LABEL);
  
    // Try to avoid passing control into the slow_path ...
    Label LSuccess, LGoSlowPath ;
    bind  (CheckSucc);
  
    // The following optional optimization can be elided if necessary
<span class="udiff-line-modified-removed">-   // Effectively: if (succ == null) goto SlowPath</span>
<span class="udiff-line-modified-added">+   // Effectively: if (succ == null) goto slow path</span>
    // The code reduces the window for a race, however,
    // and thus benefits performance.
    cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), (int32_t)NULL_WORD);
    jccb  (Assembler::zero, LGoSlowPath);
  
    xorptr(boxReg, boxReg);
<span class="udiff-line-added">+   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.</span>
    movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t)NULL_WORD);
  
    // Memory barrier/fence
    // Dekker pivot point -- fulcrum : ST Owner; MEMBAR; LD Succ
    // Instead of MFENCE we use a dummy locked add of 0 to the top-of-stack.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2045,11 +2027,11 @@</span>
    cmpxchgptr(r15_thread, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
    // There&#39;s no successor so we tried to regrab the lock.
    // If that didn&#39;t work, then another thread grabbed the
    // lock so we&#39;re done (and exit was a success).
    jccb  (Assembler::notEqual, LSuccess);
<span class="udiff-line-modified-removed">-   // Intentional fall-through into slow-path</span>
<span class="udiff-line-modified-added">+   // Intentional fall-through into slow path</span>
  
    bind  (LGoSlowPath);
    orl   (boxReg, 1);                      // set ICC.ZF=0 to indicate failure
    jmpb  (DONE_LABEL);
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2740,20 +2722,19 @@</span>
      lea(rscratch1, src);
      Assembler::divss(dst, Address(rscratch1, 0));
    }
  }
  
<span class="udiff-line-modified-removed">- // !defined(COMPILER2) is because of stupid core builds</span>
<span class="udiff-line-removed">- #if !defined(_LP64) || defined(COMPILER1) || !defined(COMPILER2) || INCLUDE_JVMCI</span>
<span class="udiff-line-modified-added">+ #ifndef _LP64</span>
  void MacroAssembler::empty_FPU_stack() {
    if (VM_Version::supports_mmx()) {
      emms();
    } else {
      for (int i = 8; i-- &gt; 0; ) ffree(i);
    }
  }
<span class="udiff-line-modified-removed">- #endif // !LP64 || C1 || !C2 || INCLUDE_JVMCI</span>
<span class="udiff-line-modified-added">+ #endif // !LP64</span>
  
  
  void MacroAssembler::enter() {
    push(rbp);
    mov(rbp, rsp);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2770,10 +2751,11 @@</span>
      emit_int8(0x65); // gs:
      emit_int8((unsigned char)0x90);
    }
  }
  
<span class="udiff-line-added">+ #if !defined(_LP64)</span>
  void MacroAssembler::fcmp(Register tmp) {
    fcmp(tmp, 1, true, true);
  }
  
  void MacroAssembler::fcmp(Register tmp, int index, bool pop_left, bool pop_right) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2851,39 +2833,42 @@</span>
  
  void MacroAssembler::fldcw(AddressLiteral src) {
    Assembler::fldcw(as_Address(src));
  }
  
<span class="udiff-line-added">+ void MacroAssembler::fpop() {</span>
<span class="udiff-line-added">+   ffree();</span>
<span class="udiff-line-added">+   fincstp();</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::fremr(Register tmp) {</span>
<span class="udiff-line-added">+   save_rax(tmp);</span>
<span class="udiff-line-added">+   { Label L;</span>
<span class="udiff-line-added">+     bind(L);</span>
<span class="udiff-line-added">+     fprem();</span>
<span class="udiff-line-added">+     fwait(); fnstsw_ax();</span>
<span class="udiff-line-added">+     sahf();</span>
<span class="udiff-line-added">+     jcc(Assembler::parity, L);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+   restore_rax(tmp);</span>
<span class="udiff-line-added">+   // Result is in ST0.</span>
<span class="udiff-line-added">+   // Note: fxch &amp; fpop to get rid of ST1</span>
<span class="udiff-line-added">+   // (otherwise FPU stack could overflow eventually)</span>
<span class="udiff-line-added">+   fxch(1);</span>
<span class="udiff-line-added">+   fpop();</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ #endif // !LP64</span>
<span class="udiff-line-added">+ </span>
  void MacroAssembler::mulpd(XMMRegister dst, AddressLiteral src) {
    if (reachable(src)) {
      Assembler::mulpd(dst, as_Address(src));
    } else {
      lea(rscratch1, src);
      Assembler::mulpd(dst, Address(rscratch1, 0));
    }
  }
  
<span class="udiff-line-removed">- void MacroAssembler::increase_precision() {</span>
<span class="udiff-line-removed">-   subptr(rsp, BytesPerWord);</span>
<span class="udiff-line-removed">-   fnstcw(Address(rsp, 0));</span>
<span class="udiff-line-removed">-   movl(rax, Address(rsp, 0));</span>
<span class="udiff-line-removed">-   orl(rax, 0x300);</span>
<span class="udiff-line-removed">-   push(rax);</span>
<span class="udiff-line-removed">-   fldcw(Address(rsp, 0));</span>
<span class="udiff-line-removed">-   pop(rax);</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">- void MacroAssembler::restore_precision() {</span>
<span class="udiff-line-removed">-   fldcw(Address(rsp, 0));</span>
<span class="udiff-line-removed">-   addptr(rsp, BytesPerWord);</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">- void MacroAssembler::fpop() {</span>
<span class="udiff-line-removed">-   ffree();</span>
<span class="udiff-line-removed">-   fincstp();</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-removed">- </span>
  void MacroAssembler::load_float(Address src) {
    if (UseSSE &gt;= 1) {
      movflt(xmm0, src);
    } else {
      LP64_ONLY(ShouldNotReachHere());
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2916,32 +2901,10 @@</span>
      LP64_ONLY(ShouldNotReachHere());
      NOT_LP64(fstp_d(dst));
    }
  }
  
<span class="udiff-line-removed">- void MacroAssembler::fremr(Register tmp) {</span>
<span class="udiff-line-removed">-   save_rax(tmp);</span>
<span class="udiff-line-removed">-   { Label L;</span>
<span class="udiff-line-removed">-     bind(L);</span>
<span class="udiff-line-removed">-     fprem();</span>
<span class="udiff-line-removed">-     fwait(); fnstsw_ax();</span>
<span class="udiff-line-removed">- #ifdef _LP64</span>
<span class="udiff-line-removed">-     testl(rax, 0x400);</span>
<span class="udiff-line-removed">-     jcc(Assembler::notEqual, L);</span>
<span class="udiff-line-removed">- #else</span>
<span class="udiff-line-removed">-     sahf();</span>
<span class="udiff-line-removed">-     jcc(Assembler::parity, L);</span>
<span class="udiff-line-removed">- #endif // _LP64</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">-   restore_rax(tmp);</span>
<span class="udiff-line-removed">-   // Result is in ST0.</span>
<span class="udiff-line-removed">-   // Note: fxch &amp; fpop to get rid of ST1</span>
<span class="udiff-line-removed">-   // (otherwise FPU stack could overflow eventually)</span>
<span class="udiff-line-removed">-   fxch(1);</span>
<span class="udiff-line-removed">-   fpop();</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-removed">- </span>
  // dst = c = a * b + c
  void MacroAssembler::fmad(XMMRegister dst, XMMRegister a, XMMRegister b, XMMRegister c) {
    Assembler::vfmadd231sd(c, a, b);
    if (dst != c) {
      movdbl(dst, c);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3338,17 +3301,17 @@</span>
  void MacroAssembler::vmovdqu(XMMRegister dst, XMMRegister src) {
      assert(((dst-&gt;encoding() &lt; 16  &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
      Assembler::vmovdqu(dst, src);
  }
  
<span class="udiff-line-modified-removed">- void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src) {</span>
<span class="udiff-line-modified-added">+ void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg) {</span>
    if (reachable(src)) {
      vmovdqu(dst, as_Address(src));
    }
    else {
<span class="udiff-line-modified-removed">-     lea(rscratch1, src);</span>
<span class="udiff-line-modified-removed">-     vmovdqu(dst, Address(rscratch1, 0));</span>
<span class="udiff-line-modified-added">+     lea(scratch_reg, src);</span>
<span class="udiff-line-modified-added">+     vmovdqu(dst, Address(scratch_reg, 0));</span>
    }
  }
  
  void MacroAssembler::evmovdquq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {
    if (reachable(src)) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3669,10 +3632,19 @@</span>
      lea(rscratch1, src);
      Assembler::subsd(dst, Address(rscratch1, 0));
    }
  }
  
<span class="udiff-line-added">+ void MacroAssembler::roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register scratch_reg) {</span>
<span class="udiff-line-added">+   if (reachable(src)) {</span>
<span class="udiff-line-added">+     Assembler::roundsd(dst, as_Address(src), rmode);</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     lea(scratch_reg, src);</span>
<span class="udiff-line-added">+     Assembler::roundsd(dst, Address(scratch_reg, 0), rmode);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
  void MacroAssembler::subss(XMMRegister dst, AddressLiteral src) {
    if (reachable(src)) {
      Assembler::subss(dst, as_Address(src));
    } else {
      lea(rscratch1, src);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3696,18 +3668,18 @@</span>
      lea(rscratch1, src);
      Assembler::ucomiss(dst, Address(rscratch1, 0));
    }
  }
  
<span class="udiff-line-modified-removed">- void MacroAssembler::xorpd(XMMRegister dst, AddressLiteral src) {</span>
<span class="udiff-line-modified-added">+ void MacroAssembler::xorpd(XMMRegister dst, AddressLiteral src, Register scratch_reg) {</span>
    // Used in sign-bit flipping with aligned address.
    assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
    if (reachable(src)) {
      Assembler::xorpd(dst, as_Address(src));
    } else {
<span class="udiff-line-modified-removed">-     lea(rscratch1, src);</span>
<span class="udiff-line-modified-removed">-     Assembler::xorpd(dst, Address(rscratch1, 0));</span>
<span class="udiff-line-modified-added">+     lea(scratch_reg, src);</span>
<span class="udiff-line-modified-added">+     Assembler::xorpd(dst, Address(scratch_reg, 0));</span>
    }
  }
  
  void MacroAssembler::xorpd(XMMRegister dst, XMMRegister src) {
    if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512dq() &amp;&amp; (dst-&gt;encoding() == src-&gt;encoding())) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3724,18 +3696,18 @@</span>
    } else {
      Assembler::xorps(dst, src);
    }
  }
  
<span class="udiff-line-modified-removed">- void MacroAssembler::xorps(XMMRegister dst, AddressLiteral src) {</span>
<span class="udiff-line-modified-added">+ void MacroAssembler::xorps(XMMRegister dst, AddressLiteral src, Register scratch_reg) {</span>
    // Used in sign-bit flipping with aligned address.
    assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
    if (reachable(src)) {
      Assembler::xorps(dst, as_Address(src));
    } else {
<span class="udiff-line-modified-removed">-     lea(rscratch1, src);</span>
<span class="udiff-line-modified-removed">-     Assembler::xorps(dst, Address(rscratch1, 0));</span>
<span class="udiff-line-modified-added">+     lea(scratch_reg, src);</span>
<span class="udiff-line-modified-added">+     Assembler::xorps(dst, Address(scratch_reg, 0));</span>
    }
  }
  
  void MacroAssembler::pshufb(XMMRegister dst, AddressLiteral src) {
    // Used in sign-bit flipping with aligned address.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3767,10 +3739,20 @@</span>
      lea(rscratch1, src);
      vaddss(dst, nds, Address(rscratch1, 0));
    }
  }
  
<span class="udiff-line-added">+ void MacroAssembler::vpaddd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {</span>
<span class="udiff-line-added">+   assert(UseAVX &gt; 0, &quot;requires some form of AVX&quot;);</span>
<span class="udiff-line-added">+   if (reachable(src)) {</span>
<span class="udiff-line-added">+     Assembler::vpaddd(dst, nds, as_Address(src), vector_len);</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     lea(rscratch, src);</span>
<span class="udiff-line-added">+     Assembler::vpaddd(dst, nds, Address(rscratch, 0), vector_len);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
  void MacroAssembler::vabsss(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len) {
    assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
    vandps(dst, nds, negate_field, vector_len);
  }
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3797,16 +3779,16 @@</span>
  void MacroAssembler::vpaddw(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
    assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
    Assembler::vpaddw(dst, nds, src, vector_len);
  }
  
<span class="udiff-line-modified-removed">- void MacroAssembler::vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len) {</span>
<span class="udiff-line-modified-added">+ void MacroAssembler::vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {</span>
    if (reachable(src)) {
      Assembler::vpand(dst, nds, as_Address(src), vector_len);
    } else {
<span class="udiff-line-modified-removed">-     lea(rscratch1, src);</span>
<span class="udiff-line-modified-removed">-     Assembler::vpand(dst, nds, Address(rscratch1, 0), vector_len);</span>
<span class="udiff-line-modified-added">+     lea(scratch_reg, src);</span>
<span class="udiff-line-modified-added">+     Assembler::vpand(dst, nds, Address(scratch_reg, 0), vector_len);</span>
    }
  }
  
  void MacroAssembler::vpbroadcastw(XMMRegister dst, XMMRegister src, int vector_len) {
    assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3871,10 +3853,26 @@</span>
  void MacroAssembler::vpsraw(XMMRegister dst, XMMRegister nds, int shift, int vector_len) {
    assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
    Assembler::vpsraw(dst, nds, shift, vector_len);
  }
  
<span class="udiff-line-added">+ void MacroAssembler::evpsraq(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len) {</span>
<span class="udiff-line-added">+   assert(UseAVX &gt; 2,&quot;&quot;);</span>
<span class="udiff-line-added">+   if (!VM_Version::supports_avx512vl() &amp;&amp; vector_len &lt; 2) {</span>
<span class="udiff-line-added">+      vector_len = 2;</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+   Assembler::evpsraq(dst, nds, shift, vector_len);</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::evpsraq(XMMRegister dst, XMMRegister nds, int shift, int vector_len) {</span>
<span class="udiff-line-added">+   assert(UseAVX &gt; 2,&quot;&quot;);</span>
<span class="udiff-line-added">+   if (!VM_Version::supports_avx512vl() &amp;&amp; vector_len &lt; 2) {</span>
<span class="udiff-line-added">+      vector_len = 2;</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+   Assembler::evpsraq(dst, nds, shift, vector_len);</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
  void MacroAssembler::vpsrlw(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len) {
    assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; shift-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
    Assembler::vpsrlw(dst, nds, shift, vector_len);
  }
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3911,25 +3909,25 @@</span>
  void MacroAssembler::pshuflw(XMMRegister dst, XMMRegister src, int mode) {
    assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
    Assembler::pshuflw(dst, src, mode);
  }
  
<span class="udiff-line-modified-removed">- void MacroAssembler::vandpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len) {</span>
<span class="udiff-line-modified-added">+ void MacroAssembler::vandpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {</span>
    if (reachable(src)) {
      vandpd(dst, nds, as_Address(src), vector_len);
    } else {
<span class="udiff-line-modified-removed">-     lea(rscratch1, src);</span>
<span class="udiff-line-modified-removed">-     vandpd(dst, nds, Address(rscratch1, 0), vector_len);</span>
<span class="udiff-line-modified-added">+     lea(scratch_reg, src);</span>
<span class="udiff-line-modified-added">+     vandpd(dst, nds, Address(scratch_reg, 0), vector_len);</span>
    }
  }
  
<span class="udiff-line-modified-removed">- void MacroAssembler::vandps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len) {</span>
<span class="udiff-line-modified-added">+ void MacroAssembler::vandps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {</span>
    if (reachable(src)) {
      vandps(dst, nds, as_Address(src), vector_len);
    } else {
<span class="udiff-line-modified-removed">-     lea(rscratch1, src);</span>
<span class="udiff-line-modified-removed">-     vandps(dst, nds, Address(rscratch1, 0), vector_len);</span>
<span class="udiff-line-modified-added">+     lea(scratch_reg, src);</span>
<span class="udiff-line-modified-added">+     vandps(dst, nds, Address(scratch_reg, 0), vector_len);</span>
    }
  }
  
  void MacroAssembler::vdivsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
    if (reachable(src)) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3993,28 +3991,172 @@</span>
  void MacroAssembler::vnegatesd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
    assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
    vxorpd(dst, nds, src, Assembler::AVX_128bit);
  }
  
<span class="udiff-line-modified-removed">- void MacroAssembler::vxorpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len) {</span>
<span class="udiff-line-modified-added">+ void MacroAssembler::vxorpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {</span>
    if (reachable(src)) {
      vxorpd(dst, nds, as_Address(src), vector_len);
    } else {
<span class="udiff-line-modified-removed">-     lea(rscratch1, src);</span>
<span class="udiff-line-modified-removed">-     vxorpd(dst, nds, Address(rscratch1, 0), vector_len);</span>
<span class="udiff-line-modified-added">+     lea(scratch_reg, src);</span>
<span class="udiff-line-modified-added">+     vxorpd(dst, nds, Address(scratch_reg, 0), vector_len);</span>
    }
  }
  
<span class="udiff-line-modified-removed">- void MacroAssembler::vxorps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len) {</span>
<span class="udiff-line-modified-added">+ void MacroAssembler::vxorps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {</span>
    if (reachable(src)) {
      vxorps(dst, nds, as_Address(src), vector_len);
    } else {
<span class="udiff-line-modified-removed">-     lea(rscratch1, src);</span>
<span class="udiff-line-modified-removed">-     vxorps(dst, nds, Address(rscratch1, 0), vector_len);</span>
<span class="udiff-line-modified-added">+     lea(scratch_reg, src);</span>
<span class="udiff-line-modified-added">+     vxorps(dst, nds, Address(scratch_reg, 0), vector_len);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::vpxor(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {</span>
<span class="udiff-line-added">+   if (UseAVX &gt; 1 || (vector_len &lt; 1)) {</span>
<span class="udiff-line-added">+     if (reachable(src)) {</span>
<span class="udiff-line-added">+       Assembler::vpxor(dst, nds, as_Address(src), vector_len);</span>
<span class="udiff-line-added">+     } else {</span>
<span class="udiff-line-added">+       lea(scratch_reg, src);</span>
<span class="udiff-line-added">+       Assembler::vpxor(dst, nds, Address(scratch_reg, 0), vector_len);</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+   else {</span>
<span class="udiff-line-added">+     MacroAssembler::vxorpd(dst, nds, src, vector_len, scratch_reg);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ //-------------------------------------------------------------------------------------------</span>
<span class="udiff-line-added">+ #ifdef COMPILER2</span>
<span class="udiff-line-added">+ // Generic instructions support for use in .ad files C2 code generation</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, Register scr) {</span>
<span class="udiff-line-added">+   if (dst != src) {</span>
<span class="udiff-line-added">+     movdqu(dst, src);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+   if (opcode == Op_AbsVD) {</span>
<span class="udiff-line-added">+     andpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), scr);</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     assert((opcode == Op_NegVD),&quot;opcode should be Op_NegD&quot;);</span>
<span class="udiff-line-added">+     xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), scr);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {</span>
<span class="udiff-line-added">+   if (opcode == Op_AbsVD) {</span>
<span class="udiff-line-added">+     vandpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), vector_len, scr);</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     assert((opcode == Op_NegVD),&quot;opcode should be Op_NegD&quot;);</span>
<span class="udiff-line-added">+     vxorpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), vector_len, scr);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, Register scr) {</span>
<span class="udiff-line-added">+   if (dst != src) {</span>
<span class="udiff-line-added">+     movdqu(dst, src);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+   if (opcode == Op_AbsVF) {</span>
<span class="udiff-line-added">+     andps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), scr);</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     assert((opcode == Op_NegVF),&quot;opcode should be Op_NegF&quot;);</span>
<span class="udiff-line-added">+     xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), scr);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {</span>
<span class="udiff-line-added">+   if (opcode == Op_AbsVF) {</span>
<span class="udiff-line-added">+     vandps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), vector_len, scr);</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     assert((opcode == Op_NegVF),&quot;opcode should be Op_NegF&quot;);</span>
<span class="udiff-line-added">+     vxorps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), vector_len, scr);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::vextendbw(bool sign, XMMRegister dst, XMMRegister src) {</span>
<span class="udiff-line-added">+   if (sign) {</span>
<span class="udiff-line-added">+     pmovsxbw(dst, src);</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     pmovzxbw(dst, src);</span>
    }
  }
  
<span class="udiff-line-added">+ void MacroAssembler::vextendbw(bool sign, XMMRegister dst, XMMRegister src, int vector_len) {</span>
<span class="udiff-line-added">+   if (sign) {</span>
<span class="udiff-line-added">+     vpmovsxbw(dst, src, vector_len);</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     vpmovzxbw(dst, src, vector_len);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::vshiftd(int opcode, XMMRegister dst, XMMRegister src) {</span>
<span class="udiff-line-added">+   if (opcode == Op_RShiftVI) {</span>
<span class="udiff-line-added">+     psrad(dst, src);</span>
<span class="udiff-line-added">+   } else if (opcode == Op_LShiftVI) {</span>
<span class="udiff-line-added">+     pslld(dst, src);</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     assert((opcode == Op_URShiftVI),&quot;opcode should be Op_URShiftVI&quot;);</span>
<span class="udiff-line-added">+     psrld(dst, src);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::vshiftd(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {</span>
<span class="udiff-line-added">+   if (opcode == Op_RShiftVI) {</span>
<span class="udiff-line-added">+     vpsrad(dst, nds, src, vector_len);</span>
<span class="udiff-line-added">+   } else if (opcode == Op_LShiftVI) {</span>
<span class="udiff-line-added">+     vpslld(dst, nds, src, vector_len);</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     assert((opcode == Op_URShiftVI),&quot;opcode should be Op_URShiftVI&quot;);</span>
<span class="udiff-line-added">+     vpsrld(dst, nds, src, vector_len);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::vshiftw(int opcode, XMMRegister dst, XMMRegister src) {</span>
<span class="udiff-line-added">+   if ((opcode == Op_RShiftVS) || (opcode == Op_RShiftVB)) {</span>
<span class="udiff-line-added">+     psraw(dst, src);</span>
<span class="udiff-line-added">+   } else if ((opcode == Op_LShiftVS) || (opcode == Op_LShiftVB)) {</span>
<span class="udiff-line-added">+     psllw(dst, src);</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     assert(((opcode == Op_URShiftVS) || (opcode == Op_URShiftVB)),&quot;opcode should be one of Op_URShiftVS or Op_URShiftVB&quot;);</span>
<span class="udiff-line-added">+     psrlw(dst, src);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::vshiftw(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {</span>
<span class="udiff-line-added">+   if ((opcode == Op_RShiftVS) || (opcode == Op_RShiftVB)) {</span>
<span class="udiff-line-added">+     vpsraw(dst, nds, src, vector_len);</span>
<span class="udiff-line-added">+   } else if ((opcode == Op_LShiftVS) || (opcode == Op_LShiftVB)) {</span>
<span class="udiff-line-added">+     vpsllw(dst, nds, src, vector_len);</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     assert(((opcode == Op_URShiftVS) || (opcode == Op_URShiftVB)),&quot;opcode should be one of Op_URShiftVS or Op_URShiftVB&quot;);</span>
<span class="udiff-line-added">+     vpsrlw(dst, nds, src, vector_len);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::vshiftq(int opcode, XMMRegister dst, XMMRegister src) {</span>
<span class="udiff-line-added">+   if (opcode == Op_RShiftVL) {</span>
<span class="udiff-line-added">+     psrlq(dst, src);  // using srl to implement sra on pre-avs512 systems</span>
<span class="udiff-line-added">+   } else if (opcode == Op_LShiftVL) {</span>
<span class="udiff-line-added">+     psllq(dst, src);</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     assert((opcode == Op_URShiftVL),&quot;opcode should be Op_URShiftVL&quot;);</span>
<span class="udiff-line-added">+     psrlq(dst, src);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::vshiftq(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {</span>
<span class="udiff-line-added">+   if (opcode == Op_RShiftVL) {</span>
<span class="udiff-line-added">+     evpsraq(dst, nds, src, vector_len);</span>
<span class="udiff-line-added">+   } else if (opcode == Op_LShiftVL) {</span>
<span class="udiff-line-added">+     vpsllq(dst, nds, src, vector_len);</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     assert((opcode == Op_URShiftVL),&quot;opcode should be Op_URShiftVL&quot;);</span>
<span class="udiff-line-added">+     vpsrlq(dst, nds, src, vector_len);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ #endif</span>
<span class="udiff-line-added">+ //-------------------------------------------------------------------------------------------</span>
<span class="udiff-line-added">+ </span>
  void MacroAssembler::clear_jweak_tag(Register possibly_jweak) {
    const int32_t inverted_jweak_mask = ~static_cast&lt;int32_t&gt;(JNIHandles::weak_tag_mask);
    STATIC_ASSERT(inverted_jweak_mask == -2); // otherwise check this code
    // The inverted mask is sign-extended
    andptr(possibly_jweak, inverted_jweak_mask);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -4445,10 +4587,36 @@</span>
  #undef IS_A_TEMP
  
    bind(L_fallthrough);
  }
  
<span class="udiff-line-added">+ void MacroAssembler::clinit_barrier(Register klass, Register thread, Label* L_fast_path, Label* L_slow_path) {</span>
<span class="udiff-line-added">+   assert(L_fast_path != NULL || L_slow_path != NULL, &quot;at least one is required&quot;);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   Label L_fallthrough;</span>
<span class="udiff-line-added">+   if (L_fast_path == NULL) {</span>
<span class="udiff-line-added">+     L_fast_path = &amp;L_fallthrough;</span>
<span class="udiff-line-added">+   } else if (L_slow_path == NULL) {</span>
<span class="udiff-line-added">+     L_slow_path = &amp;L_fallthrough;</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   // Fast path check: class is fully initialized</span>
<span class="udiff-line-added">+   cmpb(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);</span>
<span class="udiff-line-added">+   jcc(Assembler::equal, *L_fast_path);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   // Fast path check: current thread is initializer thread</span>
<span class="udiff-line-added">+   cmpptr(thread, Address(klass, InstanceKlass::init_thread_offset()));</span>
<span class="udiff-line-added">+   if (L_slow_path == &amp;L_fallthrough) {</span>
<span class="udiff-line-added">+     jcc(Assembler::equal, *L_fast_path);</span>
<span class="udiff-line-added">+     bind(*L_slow_path);</span>
<span class="udiff-line-added">+   } else if (L_fast_path == &amp;L_fallthrough) {</span>
<span class="udiff-line-added">+     jcc(Assembler::notEqual, *L_slow_path);</span>
<span class="udiff-line-added">+     bind(*L_fast_path);</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     Unimplemented();</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
  
  void MacroAssembler::cmov32(Condition cc, Register dst, Address src) {
    if (VM_Version::supports_cmov()) {
      cmovl(cc, dst, src);
    } else {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -4898,10 +5066,11 @@</span>
    addptr(rsp, wordSize);       // discard argument
    pop_CPU_state();
  }
  
  
<span class="udiff-line-added">+ #ifndef _LP64</span>
  static bool _verify_FPU(int stack_depth, char* s, CPU_State* state) {
    static int counter = 0;
    FPU_State* fs = &amp;state-&gt;_fpu_state;
    counter++;
    // For leaf calls, only verify that the top few elements remain empty.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -4954,11 +5123,10 @@</span>
    }
    // everything is cool
    return true;
  }
  
<span class="udiff-line-removed">- </span>
  void MacroAssembler::verify_FPU(int stack_depth, const char* s) {
    if (!VerifyFPU) return;
    push_CPU_state();
    push(rsp);                // pass CPU state
    ExternalAddress msg((address) s);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -4974,10 +5142,11 @@</span>
      int3();                  // break if error condition
      bind(L);
    }
    pop_CPU_state();
  }
<span class="udiff-line-added">+ #endif // _LP64</span>
  
  void MacroAssembler::restore_cpu_control_state_after_jni() {
    // Either restore the MXCSR register after returning from the JNI Call
    // or verify that it wasn&#39;t changed (with -Xcheck:jni flag).
    if (VM_Version::supports_sse()) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -5018,20 +5187,46 @@</span>
    // OopHandle::resolve is an indirection like jobject.
    access_load_at(T_OBJECT, IN_NATIVE,
                   result, Address(result, 0), tmp, /*tmp_thread*/noreg);
  }
  
<span class="udiff-line-added">+ // ((WeakHandle)result).resolve();</span>
<span class="udiff-line-added">+ void MacroAssembler::resolve_weak_handle(Register rresult, Register rtmp) {</span>
<span class="udiff-line-added">+   assert_different_registers(rresult, rtmp);</span>
<span class="udiff-line-added">+   Label resolved;</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   // A null weak handle resolves to null.</span>
<span class="udiff-line-added">+   cmpptr(rresult, 0);</span>
<span class="udiff-line-added">+   jcc(Assembler::equal, resolved);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   // Only 64 bit platforms support GCs that require a tmp register</span>
<span class="udiff-line-added">+   // Only IN_HEAP loads require a thread_tmp register</span>
<span class="udiff-line-added">+   // WeakHandle::resolve is an indirection like jweak.</span>
<span class="udiff-line-added">+   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF,</span>
<span class="udiff-line-added">+                  rresult, Address(rresult, 0), rtmp, /*tmp_thread*/noreg);</span>
<span class="udiff-line-added">+   bind(resolved);</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
  void MacroAssembler::load_mirror(Register mirror, Register method, Register tmp) {
    // get mirror
    const int mirror_offset = in_bytes(Klass::java_mirror_offset());
<span class="udiff-line-modified-removed">-   movptr(mirror, Address(method, Method::const_offset()));</span>
<span class="udiff-line-removed">-   movptr(mirror, Address(mirror, ConstMethod::constants_offset()));</span>
<span class="udiff-line-removed">-   movptr(mirror, Address(mirror, ConstantPool::pool_holder_offset_in_bytes()));</span>
<span class="udiff-line-modified-added">+   load_method_holder(mirror, method);</span>
    movptr(mirror, Address(mirror, mirror_offset));
    resolve_oop_handle(mirror, tmp);
  }
  
<span class="udiff-line-added">+ void MacroAssembler::load_method_holder_cld(Register rresult, Register rmethod) {</span>
<span class="udiff-line-added">+   load_method_holder(rresult, rmethod);</span>
<span class="udiff-line-added">+   movptr(rresult, Address(rresult, InstanceKlass::class_loader_data_offset()));</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::load_method_holder(Register holder, Register method) {</span>
<span class="udiff-line-added">+   movptr(holder, Address(method, Method::const_offset()));                      // ConstMethod*</span>
<span class="udiff-line-added">+   movptr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*</span>
<span class="udiff-line-added">+   movptr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
  void MacroAssembler::load_klass(Register dst, Register src) {
  #ifdef _LP64
    if (UseCompressedClassPointers) {
      movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));
      decode_klass_not_null(dst);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -5122,11 +5317,11 @@</span>
    assert (UseCompressedOops, &quot;should be compressed&quot;);
    assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
    if (CheckCompressedOops) {
      Label ok;
      push(rscratch1); // cmpptr trashes rscratch1
<span class="udiff-line-modified-removed">-     cmpptr(r12_heapbase, ExternalAddress((address)Universe::narrow_ptrs_base_addr()));</span>
<span class="udiff-line-modified-added">+     cmpptr(r12_heapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));</span>
      jcc(Assembler::equal, ok);
      STOP(msg);
      bind(ok);
      pop(rscratch1);
    }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -5137,13 +5332,13 @@</span>
  void MacroAssembler::encode_heap_oop(Register r) {
  #ifdef ASSERT
    verify_heapbase(&quot;MacroAssembler::encode_heap_oop: heap base corrupted?&quot;);
  #endif
    verify_oop(r, &quot;broken oop in encode_heap_oop&quot;);
<span class="udiff-line-modified-removed">-   if (Universe::narrow_oop_base() == NULL) {</span>
<span class="udiff-line-modified-removed">-     if (Universe::narrow_oop_shift() != 0) {</span>
<span class="udiff-line-modified-removed">-       assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
<span class="udiff-line-modified-added">+   if (CompressedOops::base() == NULL) {</span>
<span class="udiff-line-modified-added">+     if (CompressedOops::shift() != 0) {</span>
<span class="udiff-line-modified-added">+       assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
        shrq(r, LogMinObjAlignmentInBytes);
      }
      return;
    }
    testq(r, r);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -5162,15 +5357,15 @@</span>
      STOP(&quot;null oop passed to encode_heap_oop_not_null&quot;);
      bind(ok);
    }
  #endif
    verify_oop(r, &quot;broken oop in encode_heap_oop_not_null&quot;);
<span class="udiff-line-modified-removed">-   if (Universe::narrow_oop_base() != NULL) {</span>
<span class="udiff-line-modified-added">+   if (CompressedOops::base() != NULL) {</span>
      subq(r, r12_heapbase);
    }
<span class="udiff-line-modified-removed">-   if (Universe::narrow_oop_shift() != 0) {</span>
<span class="udiff-line-modified-removed">-     assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
<span class="udiff-line-modified-added">+   if (CompressedOops::shift() != 0) {</span>
<span class="udiff-line-modified-added">+     assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
      shrq(r, LogMinObjAlignmentInBytes);
    }
  }
  
  void MacroAssembler::encode_heap_oop_not_null(Register dst, Register src) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -5186,26 +5381,26 @@</span>
  #endif
    verify_oop(src, &quot;broken oop in encode_heap_oop_not_null2&quot;);
    if (dst != src) {
      movq(dst, src);
    }
<span class="udiff-line-modified-removed">-   if (Universe::narrow_oop_base() != NULL) {</span>
<span class="udiff-line-modified-added">+   if (CompressedOops::base() != NULL) {</span>
      subq(dst, r12_heapbase);
    }
<span class="udiff-line-modified-removed">-   if (Universe::narrow_oop_shift() != 0) {</span>
<span class="udiff-line-modified-removed">-     assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
<span class="udiff-line-modified-added">+   if (CompressedOops::shift() != 0) {</span>
<span class="udiff-line-modified-added">+     assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
      shrq(dst, LogMinObjAlignmentInBytes);
    }
  }
  
  void  MacroAssembler::decode_heap_oop(Register r) {
  #ifdef ASSERT
    verify_heapbase(&quot;MacroAssembler::decode_heap_oop: heap base corrupted?&quot;);
  #endif
<span class="udiff-line-modified-removed">-   if (Universe::narrow_oop_base() == NULL) {</span>
<span class="udiff-line-modified-removed">-     if (Universe::narrow_oop_shift() != 0) {</span>
<span class="udiff-line-modified-removed">-       assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
<span class="udiff-line-modified-added">+   if (CompressedOops::base() == NULL) {</span>
<span class="udiff-line-modified-added">+     if (CompressedOops::shift() != 0) {</span>
<span class="udiff-line-modified-added">+       assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
        shlq(r, LogMinObjAlignmentInBytes);
      }
    } else {
      Label done;
      shlq(r, LogMinObjAlignmentInBytes);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -5221,78 +5416,78 @@</span>
    assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
    assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
    // Cannot assert, unverified entry point counts instructions (see .ad file)
    // vtableStubs also counts instructions in pd_code_size_limit.
    // Also do not verify_oop as this is called by verify_oop.
<span class="udiff-line-modified-removed">-   if (Universe::narrow_oop_shift() != 0) {</span>
<span class="udiff-line-modified-removed">-     assert(LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
<span class="udiff-line-modified-added">+   if (CompressedOops::shift() != 0) {</span>
<span class="udiff-line-modified-added">+     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
      shlq(r, LogMinObjAlignmentInBytes);
<span class="udiff-line-modified-removed">-     if (Universe::narrow_oop_base() != NULL) {</span>
<span class="udiff-line-modified-added">+     if (CompressedOops::base() != NULL) {</span>
        addq(r, r12_heapbase);
      }
    } else {
<span class="udiff-line-modified-removed">-     assert (Universe::narrow_oop_base() == NULL, &quot;sanity&quot;);</span>
<span class="udiff-line-modified-added">+     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);</span>
    }
  }
  
  void  MacroAssembler::decode_heap_oop_not_null(Register dst, Register src) {
    // Note: it will change flags
    assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
    assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
    // Cannot assert, unverified entry point counts instructions (see .ad file)
    // vtableStubs also counts instructions in pd_code_size_limit.
    // Also do not verify_oop as this is called by verify_oop.
<span class="udiff-line-modified-removed">-   if (Universe::narrow_oop_shift() != 0) {</span>
<span class="udiff-line-modified-removed">-     assert(LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
<span class="udiff-line-modified-added">+   if (CompressedOops::shift() != 0) {</span>
<span class="udiff-line-modified-added">+     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
      if (LogMinObjAlignmentInBytes == Address::times_8) {
        leaq(dst, Address(r12_heapbase, src, Address::times_8, 0));
      } else {
        if (dst != src) {
          movq(dst, src);
        }
        shlq(dst, LogMinObjAlignmentInBytes);
<span class="udiff-line-modified-removed">-       if (Universe::narrow_oop_base() != NULL) {</span>
<span class="udiff-line-modified-added">+       if (CompressedOops::base() != NULL) {</span>
          addq(dst, r12_heapbase);
        }
      }
    } else {
<span class="udiff-line-modified-removed">-     assert (Universe::narrow_oop_base() == NULL, &quot;sanity&quot;);</span>
<span class="udiff-line-modified-added">+     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);</span>
      if (dst != src) {
        movq(dst, src);
      }
    }
  }
  
  void MacroAssembler::encode_klass_not_null(Register r) {
<span class="udiff-line-modified-removed">-   if (Universe::narrow_klass_base() != NULL) {</span>
<span class="udiff-line-modified-added">+   if (CompressedKlassPointers::base() != NULL) {</span>
      // Use r12 as a scratch register in which to temporarily load the narrow_klass_base.
      assert(r != r12_heapbase, &quot;Encoding a klass in r12&quot;);
<span class="udiff-line-modified-removed">-     mov64(r12_heapbase, (int64_t)Universe::narrow_klass_base());</span>
<span class="udiff-line-modified-added">+     mov64(r12_heapbase, (int64_t)CompressedKlassPointers::base());</span>
      subq(r, r12_heapbase);
    }
<span class="udiff-line-modified-removed">-   if (Universe::narrow_klass_shift() != 0) {</span>
<span class="udiff-line-modified-removed">-     assert (LogKlassAlignmentInBytes == Universe::narrow_klass_shift(), &quot;decode alg wrong&quot;);</span>
<span class="udiff-line-modified-added">+   if (CompressedKlassPointers::shift() != 0) {</span>
<span class="udiff-line-modified-added">+     assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);</span>
      shrq(r, LogKlassAlignmentInBytes);
    }
<span class="udiff-line-modified-removed">-   if (Universe::narrow_klass_base() != NULL) {</span>
<span class="udiff-line-modified-added">+   if (CompressedKlassPointers::base() != NULL) {</span>
      reinit_heapbase();
    }
  }
  
  void MacroAssembler::encode_klass_not_null(Register dst, Register src) {
    if (dst == src) {
      encode_klass_not_null(src);
    } else {
<span class="udiff-line-modified-removed">-     if (Universe::narrow_klass_base() != NULL) {</span>
<span class="udiff-line-modified-removed">-       mov64(dst, (int64_t)Universe::narrow_klass_base());</span>
<span class="udiff-line-modified-added">+     if (CompressedKlassPointers::base() != NULL) {</span>
<span class="udiff-line-modified-added">+       mov64(dst, (int64_t)CompressedKlassPointers::base());</span>
        negq(dst);
        addq(dst, src);
      } else {
        movptr(dst, src);
      }
<span class="udiff-line-modified-removed">-     if (Universe::narrow_klass_shift() != 0) {</span>
<span class="udiff-line-modified-removed">-       assert (LogKlassAlignmentInBytes == Universe::narrow_klass_shift(), &quot;decode alg wrong&quot;);</span>
<span class="udiff-line-modified-added">+     if (CompressedKlassPointers::shift() != 0) {</span>
<span class="udiff-line-modified-added">+       assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);</span>
        shrq(dst, LogKlassAlignmentInBytes);
      }
    }
  }
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -5300,13 +5495,13 @@</span>
  // generated by decode_klass_not_null(register r) and reinit_heapbase(),
  // when (Universe::heap() != NULL).  Hence, if the instructions they
  // generate change, then this method needs to be updated.
  int MacroAssembler::instr_size_for_decode_klass_not_null() {
    assert (UseCompressedClassPointers, &quot;only for compressed klass ptrs&quot;);
<span class="udiff-line-modified-removed">-   if (Universe::narrow_klass_base() != NULL) {</span>
<span class="udiff-line-modified-added">+   if (CompressedKlassPointers::base() != NULL) {</span>
      // mov64 + addq + shlq? + mov64  (for reinit_heapbase()).
<span class="udiff-line-modified-removed">-     return (Universe::narrow_klass_shift() == 0 ? 20 : 24);</span>
<span class="udiff-line-modified-added">+     return (CompressedKlassPointers::shift() == 0 ? 20 : 24);</span>
    } else {
      // longest load decode klass function, mov64, leaq
      return 16;
    }
  }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -5318,17 +5513,17 @@</span>
    assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
    assert(r != r12_heapbase, &quot;Decoding a klass in r12&quot;);
    // Cannot assert, unverified entry point counts instructions (see .ad file)
    // vtableStubs also counts instructions in pd_code_size_limit.
    // Also do not verify_oop as this is called by verify_oop.
<span class="udiff-line-modified-removed">-   if (Universe::narrow_klass_shift() != 0) {</span>
<span class="udiff-line-modified-removed">-     assert(LogKlassAlignmentInBytes == Universe::narrow_klass_shift(), &quot;decode alg wrong&quot;);</span>
<span class="udiff-line-modified-added">+   if (CompressedKlassPointers::shift() != 0) {</span>
<span class="udiff-line-modified-added">+     assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);</span>
      shlq(r, LogKlassAlignmentInBytes);
    }
    // Use r12 as a scratch register in which to temporarily load the narrow_klass_base.
<span class="udiff-line-modified-removed">-   if (Universe::narrow_klass_base() != NULL) {</span>
<span class="udiff-line-modified-removed">-     mov64(r12_heapbase, (int64_t)Universe::narrow_klass_base());</span>
<span class="udiff-line-modified-added">+   if (CompressedKlassPointers::base() != NULL) {</span>
<span class="udiff-line-modified-added">+     mov64(r12_heapbase, (int64_t)CompressedKlassPointers::base());</span>
      addq(r, r12_heapbase);
      reinit_heapbase();
    }
  }
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -5339,13 +5534,13 @@</span>
      decode_klass_not_null(dst);
    } else {
      // Cannot assert, unverified entry point counts instructions (see .ad file)
      // vtableStubs also counts instructions in pd_code_size_limit.
      // Also do not verify_oop as this is called by verify_oop.
<span class="udiff-line-modified-removed">-     mov64(dst, (int64_t)Universe::narrow_klass_base());</span>
<span class="udiff-line-modified-removed">-     if (Universe::narrow_klass_shift() != 0) {</span>
<span class="udiff-line-modified-removed">-       assert(LogKlassAlignmentInBytes == Universe::narrow_klass_shift(), &quot;decode alg wrong&quot;);</span>
<span class="udiff-line-modified-added">+     mov64(dst, (int64_t)CompressedKlassPointers::base());</span>
<span class="udiff-line-modified-added">+     if (CompressedKlassPointers::shift() != 0) {</span>
<span class="udiff-line-modified-added">+       assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);</span>
        assert(LogKlassAlignmentInBytes == Address::times_8, &quot;klass not aligned on 64bits?&quot;);
        leaq(dst, Address(dst, src, Address::times_8, 0));
      } else {
        addq(dst, src);
      }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -5373,19 +5568,19 @@</span>
  void  MacroAssembler::set_narrow_klass(Register dst, Klass* k) {
    assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
    assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
    int klass_index = oop_recorder()-&gt;find_index(k);
    RelocationHolder rspec = metadata_Relocation::spec(klass_index);
<span class="udiff-line-modified-removed">-   mov_narrow_oop(dst, Klass::encode_klass(k), rspec);</span>
<span class="udiff-line-modified-added">+   mov_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);</span>
  }
  
  void  MacroAssembler::set_narrow_klass(Address dst, Klass* k) {
    assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
    assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
    int klass_index = oop_recorder()-&gt;find_index(k);
    RelocationHolder rspec = metadata_Relocation::spec(klass_index);
<span class="udiff-line-modified-removed">-   mov_narrow_oop(dst, Klass::encode_klass(k), rspec);</span>
<span class="udiff-line-modified-added">+   mov_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);</span>
  }
  
  void  MacroAssembler::cmp_narrow_oop(Register dst, jobject obj) {
    assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
    assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -5407,31 +5602,31 @@</span>
  void  MacroAssembler::cmp_narrow_klass(Register dst, Klass* k) {
    assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
    assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
    int klass_index = oop_recorder()-&gt;find_index(k);
    RelocationHolder rspec = metadata_Relocation::spec(klass_index);
<span class="udiff-line-modified-removed">-   Assembler::cmp_narrow_oop(dst, Klass::encode_klass(k), rspec);</span>
<span class="udiff-line-modified-added">+   Assembler::cmp_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);</span>
  }
  
  void  MacroAssembler::cmp_narrow_klass(Address dst, Klass* k) {
    assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
    assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
    int klass_index = oop_recorder()-&gt;find_index(k);
    RelocationHolder rspec = metadata_Relocation::spec(klass_index);
<span class="udiff-line-modified-removed">-   Assembler::cmp_narrow_oop(dst, Klass::encode_klass(k), rspec);</span>
<span class="udiff-line-modified-added">+   Assembler::cmp_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);</span>
  }
  
  void MacroAssembler::reinit_heapbase() {
    if (UseCompressedOops || UseCompressedClassPointers) {
      if (Universe::heap() != NULL) {
<span class="udiff-line-modified-removed">-       if (Universe::narrow_oop_base() == NULL) {</span>
<span class="udiff-line-modified-added">+       if (CompressedOops::base() == NULL) {</span>
          MacroAssembler::xorptr(r12_heapbase, r12_heapbase);
        } else {
<span class="udiff-line-modified-removed">-         mov64(r12_heapbase, (int64_t)Universe::narrow_ptrs_base());</span>
<span class="udiff-line-modified-added">+         mov64(r12_heapbase, (int64_t)CompressedOops::ptrs_base());</span>
        }
      } else {
<span class="udiff-line-modified-removed">-       movptr(r12_heapbase, ExternalAddress((address)Universe::narrow_ptrs_base_addr()));</span>
<span class="udiff-line-modified-added">+       movptr(r12_heapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));</span>
      }
    }
  }
  
  #endif // _LP64
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -6143,11 +6338,11 @@</span>
          FOUND_SEQ_CHAR, DONE_LABEL;
  
    movptr(result, str1);
    if (UseAVX &gt;= 2) {
      cmpl(cnt1, stride);
<span class="udiff-line-modified-removed">-     jcc(Assembler::less, SCAN_TO_CHAR_LOOP);</span>
<span class="udiff-line-modified-added">+     jcc(Assembler::less, SCAN_TO_CHAR);</span>
      cmpl(cnt1, 2*stride);
      jcc(Assembler::less, SCAN_TO_8_CHAR_INIT);
      movdl(vec1, ch);
      vpbroadcastw(vec1, vec1, Assembler::AVX_256bit);
      vpxor(vec2, vec2);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -6170,14 +6365,12 @@</span>
      pshufd(vec1, vec1, 0);
      pxor(vec2, vec2);
    }
    bind(SCAN_TO_8_CHAR);
    cmpl(cnt1, stride);
<span class="udiff-line-modified-removed">-   if (UseAVX &gt;= 2) {</span>
<span class="udiff-line-modified-removed">-     jcc(Assembler::less, SCAN_TO_CHAR);</span>
<span class="udiff-line-removed">-   } else {</span>
<span class="udiff-line-removed">-     jcc(Assembler::less, SCAN_TO_CHAR_LOOP);</span>
<span class="udiff-line-modified-added">+   jcc(Assembler::less, SCAN_TO_CHAR);</span>
<span class="udiff-line-modified-added">+   if (UseAVX &lt; 2) {</span>
      movdl(vec1, ch);
      pshuflw(vec1, vec1, 0x00);
      pshufd(vec1, vec1, 0);
      pxor(vec2, vec2);
    }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -6386,11 +6579,11 @@</span>
  
      //  In a loop, compare 16-chars (32-bytes) at once using (vpxor+vptest)
      bind(COMPARE_WIDE_VECTORS_LOOP);
  
  #ifdef _LP64
<span class="udiff-line-modified-removed">-     if (VM_Version::supports_avx512vlbw()) { // trying 64 bytes fast loop</span>
<span class="udiff-line-modified-added">+     if ((AVX3Threshold == 0) &amp;&amp; VM_Version::supports_avx512vlbw()) { // trying 64 bytes fast loop</span>
        cmpl(cnt2, stride2x2);
        jccb(Assembler::below, COMPARE_WIDE_VECTORS_LOOP_AVX2);
        testl(cnt2, stride2x2-1);   // cnt2 holds the vector count
        jccb(Assembler::notZero, COMPARE_WIDE_VECTORS_LOOP_AVX2);   // means we cannot subtract by 0x40
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -6646,11 +6839,11 @@</span>
  
    // len == 0
    testl(len, len);
    jcc(Assembler::zero, FALSE_LABEL);
  
<span class="udiff-line-modified-removed">-   if ((UseAVX &gt; 2) &amp;&amp; // AVX512</span>
<span class="udiff-line-modified-added">+   if ((AVX3Threshold == 0) &amp;&amp; (UseAVX &gt; 2) &amp;&amp; // AVX512</span>
      VM_Version::supports_avx512vlbw() &amp;&amp;
      VM_Version::supports_bmi2()) {
  
      Label test_64_loop, test_tail;
      Register tmp3_aliased = len;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -6719,11 +6912,11 @@</span>
  
      jmp(FALSE_LABEL);
    } else {
      movl(result, len); // copy
  
<span class="udiff-line-modified-removed">-     if (UseAVX == 2 &amp;&amp; UseSSE &gt;= 2) {</span>
<span class="udiff-line-modified-added">+     if (UseAVX &gt;= 2 &amp;&amp; UseSSE &gt;= 2) {</span>
        // With AVX2, use 32-byte vector compare
        Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
  
        // Compare 32-byte vectors
        andl(result, 0x0000001f);  //   tail count (in bytes)
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -6892,18 +7085,16 @@</span>
  
      lea(ary1, Address(ary1, limit, Address::times_1));
      lea(ary2, Address(ary2, limit, Address::times_1));
      negptr(limit);
  
<span class="udiff-line-removed">-     bind(COMPARE_WIDE_VECTORS);</span>
<span class="udiff-line-removed">- </span>
  #ifdef _LP64
<span class="udiff-line-modified-removed">-     if (VM_Version::supports_avx512vlbw()) { // trying 64 bytes fast loop</span>
<span class="udiff-line-modified-added">+     if ((AVX3Threshold == 0) &amp;&amp; VM_Version::supports_avx512vlbw()) { // trying 64 bytes fast loop</span>
        Label COMPARE_WIDE_VECTORS_LOOP_AVX2, COMPARE_WIDE_VECTORS_LOOP_AVX3;
  
        cmpl(limit, -64);
<span class="udiff-line-modified-removed">-       jccb(Assembler::greater, COMPARE_WIDE_VECTORS_LOOP_AVX2);</span>
<span class="udiff-line-modified-added">+       jcc(Assembler::greater, COMPARE_WIDE_VECTORS_LOOP_AVX2);</span>
  
        bind(COMPARE_WIDE_VECTORS_LOOP_AVX3); // the hottest loop
  
        evmovdquq(vec1, Address(ary1, limit, Address::times_1), Assembler::AVX_512bit);
        evpcmpeqb(k7, vec1, Address(ary2, limit, Address::times_1), Assembler::AVX_512bit);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -6932,11 +7123,11 @@</span>
  
        bind(COMPARE_WIDE_VECTORS_LOOP_AVX2);
  
      }//if (VM_Version::supports_avx512vlbw())
  #endif //_LP64
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-added">+     bind(COMPARE_WIDE_VECTORS);</span>
      vmovdqu(vec1, Address(ary1, limit, Address::times_1));
      vmovdqu(vec2, Address(ary2, limit, Address::times_1));
      vpxor(vec1, vec2);
  
      vptest(vec1, vec1);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -7158,36 +7349,37 @@</span>
      BIND(L_fill_32_bytes);
      {
        assert( UseSSE &gt;= 2, &quot;supported cpu only&quot; );
        Label L_fill_32_bytes_loop, L_check_fill_8_bytes, L_fill_8_bytes_loop, L_fill_8_bytes;
        movdl(xtmp, value);
<span class="udiff-line-modified-removed">-       if (UseAVX &gt; 2 &amp;&amp; UseUnalignedLoadStores) {</span>
<span class="udiff-line-modified-removed">-         // Fill 64-byte chunks</span>
<span class="udiff-line-modified-removed">-         Label L_fill_64_bytes_loop, L_check_fill_32_bytes;</span>
<span class="udiff-line-modified-removed">-         vpbroadcastd(xtmp, xtmp, Assembler::AVX_512bit);</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-         subl(count, 16 &lt;&lt; shift);</span>
<span class="udiff-line-modified-removed">-         jcc(Assembler::less, L_check_fill_32_bytes);</span>
<span class="udiff-line-modified-removed">-         align(16);</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-         BIND(L_fill_64_bytes_loop);</span>
<span class="udiff-line-modified-removed">-         evmovdqul(Address(to, 0), xtmp, Assembler::AVX_512bit);</span>
<span class="udiff-line-modified-removed">-         addptr(to, 64);</span>
<span class="udiff-line-modified-removed">-         subl(count, 16 &lt;&lt; shift);</span>
<span class="udiff-line-modified-removed">-         jcc(Assembler::greaterEqual, L_fill_64_bytes_loop);</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-         BIND(L_check_fill_32_bytes);</span>
<span class="udiff-line-modified-removed">-         addl(count, 8 &lt;&lt; shift);</span>
<span class="udiff-line-modified-removed">-         jccb(Assembler::less, L_check_fill_8_bytes);</span>
<span class="udiff-line-modified-removed">-         vmovdqu(Address(to, 0), xtmp);</span>
<span class="udiff-line-modified-removed">-         addptr(to, 32);</span>
<span class="udiff-line-modified-removed">-         subl(count, 8 &lt;&lt; shift);</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-         BIND(L_check_fill_8_bytes);</span>
<span class="udiff-line-modified-removed">-       } else if (UseAVX == 2 &amp;&amp; UseUnalignedLoadStores) {</span>
<span class="udiff-line-modified-added">+       if (UseAVX &gt;= 2 &amp;&amp; UseUnalignedLoadStores) {</span>
<span class="udiff-line-modified-added">+         Label L_check_fill_32_bytes;</span>
<span class="udiff-line-modified-added">+         if (UseAVX &gt; 2) {</span>
<span class="udiff-line-modified-added">+           // Fill 64-byte chunks</span>
<span class="udiff-line-modified-added">+           Label L_fill_64_bytes_loop_avx3, L_check_fill_64_bytes_avx2;</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+           // If number of bytes to fill &lt; AVX3Threshold, perform fill using AVX2</span>
<span class="udiff-line-modified-added">+           cmpl(count, AVX3Threshold);</span>
<span class="udiff-line-modified-added">+           jccb(Assembler::below, L_check_fill_64_bytes_avx2);</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+           vpbroadcastd(xtmp, xtmp, Assembler::AVX_512bit);</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+           subl(count, 16 &lt;&lt; shift);</span>
<span class="udiff-line-modified-added">+           jccb(Assembler::less, L_check_fill_32_bytes);</span>
<span class="udiff-line-modified-added">+           align(16);</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+           BIND(L_fill_64_bytes_loop_avx3);</span>
<span class="udiff-line-modified-added">+           evmovdqul(Address(to, 0), xtmp, Assembler::AVX_512bit);</span>
<span class="udiff-line-modified-added">+           addptr(to, 64);</span>
<span class="udiff-line-modified-added">+           subl(count, 16 &lt;&lt; shift);</span>
<span class="udiff-line-modified-added">+           jcc(Assembler::greaterEqual, L_fill_64_bytes_loop_avx3);</span>
<span class="udiff-line-modified-added">+           jmpb(L_check_fill_32_bytes);</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+           BIND(L_check_fill_64_bytes_avx2);</span>
<span class="udiff-line-added">+         }</span>
          // Fill 64-byte chunks
<span class="udiff-line-modified-removed">-         Label L_fill_64_bytes_loop, L_check_fill_32_bytes;</span>
<span class="udiff-line-modified-added">+         Label L_fill_64_bytes_loop;</span>
          vpbroadcastd(xtmp, xtmp, Assembler::AVX_256bit);
  
          subl(count, 16 &lt;&lt; shift);
          jcc(Assembler::less, L_check_fill_32_bytes);
          align(16);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -7897,16 +8089,17 @@</span>
    ShortBranchVerifier sbv(this);
  
    shlq(length);
    xorq(result, result);
  
<span class="udiff-line-modified-removed">-   if ((UseAVX &gt; 2) &amp;&amp;</span>
<span class="udiff-line-modified-added">+   if ((AVX3Threshold == 0) &amp;&amp; (UseAVX &gt; 2) &amp;&amp;</span>
        VM_Version::supports_avx512vlbw()) {
      Label VECTOR64_LOOP, VECTOR64_NOT_EQUAL, VECTOR32_TAIL;
  
      cmpq(length, 64);
      jcc(Assembler::less, VECTOR32_TAIL);
<span class="udiff-line-added">+ </span>
      movq(tmp1, length);
      andq(tmp1, 0x3F);      // tail count
      andq(length, ~(0x3F)); //vector count
  
      bind(VECTOR64_LOOP);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -8727,38 +8920,10 @@</span>
    BIND(L_aligned);
    movl(tmp, len); // save
    shrl(len, 4);
    jcc(Assembler::zero, L_tail_restore);
  
<span class="udiff-line-removed">-   // Fold total 512 bits of polynomial on each iteration</span>
<span class="udiff-line-removed">-   if (VM_Version::supports_vpclmulqdq()) {</span>
<span class="udiff-line-removed">-     Label Parallel_loop, L_No_Parallel;</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     cmpl(len, 8);</span>
<span class="udiff-line-removed">-     jccb(Assembler::less, L_No_Parallel);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 32));</span>
<span class="udiff-line-removed">-     evmovdquq(xmm1, Address(buf, 0), Assembler::AVX_512bit);</span>
<span class="udiff-line-removed">-     movdl(xmm5, crc);</span>
<span class="udiff-line-removed">-     evpxorq(xmm1, xmm1, xmm5, Assembler::AVX_512bit);</span>
<span class="udiff-line-removed">-     addptr(buf, 64);</span>
<span class="udiff-line-removed">-     subl(len, 7);</span>
<span class="udiff-line-removed">-     evshufi64x2(xmm0, xmm0, xmm0, 0x00, Assembler::AVX_512bit); //propagate the mask from 128 bits to 512 bits</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     BIND(Parallel_loop);</span>
<span class="udiff-line-removed">-     fold_128bit_crc32_avx512(xmm1, xmm0, xmm5, buf, 0);</span>
<span class="udiff-line-removed">-     addptr(buf, 64);</span>
<span class="udiff-line-removed">-     subl(len, 4);</span>
<span class="udiff-line-removed">-     jcc(Assembler::greater, Parallel_loop);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     vextracti64x2(xmm2, xmm1, 0x01);</span>
<span class="udiff-line-removed">-     vextracti64x2(xmm3, xmm1, 0x02);</span>
<span class="udiff-line-removed">-     vextracti64x2(xmm4, xmm1, 0x03);</span>
<span class="udiff-line-removed">-     jmp(L_fold_512b);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     BIND(L_No_Parallel);</span>
<span class="udiff-line-removed">-   }</span>
    // Fold crc into first bytes of vector
    movdqa(xmm1, Address(buf, 0));
    movdl(rax, xmm1);
    xorl(crc, rax);
    if (VM_Version::supports_sse4_1()) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -9359,11 +9524,11 @@</span>
    assert(len != result, &quot;&quot;);
  
    // save length for return
    push(len);
  
<span class="udiff-line-modified-removed">-   if ((UseAVX &gt; 2) &amp;&amp; // AVX512</span>
<span class="udiff-line-modified-added">+   if ((AVX3Threshold == 0) &amp;&amp; (UseAVX &gt; 2) &amp;&amp; // AVX512</span>
      VM_Version::supports_avx512vlbw() &amp;&amp;
      VM_Version::supports_bmi2()) {
  
      Label copy_32_loop, copy_loop_tail, below_threshold;
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -9551,21 +9716,21 @@</span>
  //       dst[dstOff++] = (char)(src[srcOff++] &amp; 0xff);
  //     }
  //   }
  void MacroAssembler::byte_array_inflate(Register src, Register dst, Register len,
    XMMRegister tmp1, Register tmp2) {
<span class="udiff-line-modified-removed">-   Label copy_chars_loop, done, below_threshold;</span>
<span class="udiff-line-modified-added">+   Label copy_chars_loop, done, below_threshold, avx3_threshold;</span>
    // rsi: src
    // rdi: dst
    // rdx: len
    // rcx: tmp2
  
    // rsi holds start addr of source byte[] to be inflated
    // rdi holds start addr of destination char[]
    // rdx holds length
    assert_different_registers(src, dst, len, tmp2);
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-added">+   movl(tmp2, len);</span>
    if ((UseAVX &gt; 2) &amp;&amp; // AVX512
      VM_Version::supports_avx512vlbw() &amp;&amp;
      VM_Version::supports_bmi2()) {
  
      Label copy_32_loop, copy_tail;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -9573,13 +9738,15 @@</span>
  
      // if length of the string is less than 16, handle it in an old fashioned way
      testl(len, -16);
      jcc(Assembler::zero, below_threshold);
  
<span class="udiff-line-added">+     testl(len, -1 * AVX3Threshold);</span>
<span class="udiff-line-added">+     jcc(Assembler::zero, avx3_threshold);</span>
<span class="udiff-line-added">+ </span>
      // In order to use only one arithmetic operation for the main loop we use
      // this pre-calculation
<span class="udiff-line-removed">-     movl(tmp2, len);</span>
      andl(tmp2, (32 - 1)); // tail count (in chars), 32 element wide loop
      andl(len, -32);     // vector count
      jccb(Assembler::zero, copy_tail);
  
      lea(src, Address(src, len, Address::times_1));
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -9606,16 +9773,15 @@</span>
      kmovdl(k2, tmp3_aliased);
      evpmovzxbw(tmp1, k2, Address(src, 0), Assembler::AVX_512bit);
      evmovdquw(Address(dst, 0), k2, tmp1, Assembler::AVX_512bit);
  
      jmp(done);
<span class="udiff-line-added">+     bind(avx3_threshold);</span>
    }
    if (UseSSE42Intrinsics) {
      Label copy_16_loop, copy_8_loop, copy_bytes, copy_new_tail, copy_tail;
  
<span class="udiff-line-removed">-     movl(tmp2, len);</span>
<span class="udiff-line-removed">- </span>
      if (UseAVX &gt; 1) {
        andl(tmp2, (16 - 1));
        andl(len, -16);
        jccb(Assembler::zero, copy_new_tail);
      } else {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -9636,17 +9802,11 @@</span>
        addptr(len, 16);
        jcc(Assembler::notZero, copy_16_loop);
  
        bind(below_threshold);
        bind(copy_new_tail);
<span class="udiff-line-modified-removed">-       if ((UseAVX &gt; 2) &amp;&amp;</span>
<span class="udiff-line-removed">-         VM_Version::supports_avx512vlbw() &amp;&amp;</span>
<span class="udiff-line-removed">-         VM_Version::supports_bmi2()) {</span>
<span class="udiff-line-removed">-         movl(tmp2, len);</span>
<span class="udiff-line-removed">-       } else {</span>
<span class="udiff-line-removed">-         movl(len, tmp2);</span>
<span class="udiff-line-removed">-       }</span>
<span class="udiff-line-modified-added">+       movl(len, tmp2);</span>
        andl(tmp2, 0x00000007);
        andl(len, 0xFFFFFFF8);
        jccb(Assembler::zero, copy_tail);
  
        pmovzxbw(tmp1, Address(src, 0));
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -9696,10 +9856,101 @@</span>
    jcc(Assembler::notZero, copy_chars_loop);
  
    bind(done);
  }
  
<span class="udiff-line-added">+ #ifdef _LP64</span>
<span class="udiff-line-added">+ void MacroAssembler::convert_f2i(Register dst, XMMRegister src) {</span>
<span class="udiff-line-added">+   Label done;</span>
<span class="udiff-line-added">+   cvttss2sil(dst, src);</span>
<span class="udiff-line-added">+   // Conversion instructions do not match JLS for overflow, underflow and NaN -&gt; fixup in stub</span>
<span class="udiff-line-added">+   cmpl(dst, 0x80000000); // float_sign_flip</span>
<span class="udiff-line-added">+   jccb(Assembler::notEqual, done);</span>
<span class="udiff-line-added">+   subptr(rsp, 8);</span>
<span class="udiff-line-added">+   movflt(Address(rsp, 0), src);</span>
<span class="udiff-line-added">+   call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::f2i_fixup())));</span>
<span class="udiff-line-added">+   pop(dst);</span>
<span class="udiff-line-added">+   bind(done);</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::convert_d2i(Register dst, XMMRegister src) {</span>
<span class="udiff-line-added">+   Label done;</span>
<span class="udiff-line-added">+   cvttsd2sil(dst, src);</span>
<span class="udiff-line-added">+   // Conversion instructions do not match JLS for overflow, underflow and NaN -&gt; fixup in stub</span>
<span class="udiff-line-added">+   cmpl(dst, 0x80000000); // float_sign_flip</span>
<span class="udiff-line-added">+   jccb(Assembler::notEqual, done);</span>
<span class="udiff-line-added">+   subptr(rsp, 8);</span>
<span class="udiff-line-added">+   movdbl(Address(rsp, 0), src);</span>
<span class="udiff-line-added">+   call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::d2i_fixup())));</span>
<span class="udiff-line-added">+   pop(dst);</span>
<span class="udiff-line-added">+   bind(done);</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::convert_f2l(Register dst, XMMRegister src) {</span>
<span class="udiff-line-added">+   Label done;</span>
<span class="udiff-line-added">+   cvttss2siq(dst, src);</span>
<span class="udiff-line-added">+   cmp64(dst, ExternalAddress((address) StubRoutines::x86::double_sign_flip()));</span>
<span class="udiff-line-added">+   jccb(Assembler::notEqual, done);</span>
<span class="udiff-line-added">+   subptr(rsp, 8);</span>
<span class="udiff-line-added">+   movflt(Address(rsp, 0), src);</span>
<span class="udiff-line-added">+   call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::f2l_fixup())));</span>
<span class="udiff-line-added">+   pop(dst);</span>
<span class="udiff-line-added">+   bind(done);</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::convert_d2l(Register dst, XMMRegister src) {</span>
<span class="udiff-line-added">+   Label done;</span>
<span class="udiff-line-added">+   cvttsd2siq(dst, src);</span>
<span class="udiff-line-added">+   cmp64(dst, ExternalAddress((address) StubRoutines::x86::double_sign_flip()));</span>
<span class="udiff-line-added">+   jccb(Assembler::notEqual, done);</span>
<span class="udiff-line-added">+   subptr(rsp, 8);</span>
<span class="udiff-line-added">+   movdbl(Address(rsp, 0), src);</span>
<span class="udiff-line-added">+   call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::d2l_fixup())));</span>
<span class="udiff-line-added">+   pop(dst);</span>
<span class="udiff-line-added">+   bind(done);</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::cache_wb(Address line)</span>
<span class="udiff-line-added">+ {</span>
<span class="udiff-line-added">+   // 64 bit cpus always support clflush</span>
<span class="udiff-line-added">+   assert(VM_Version::supports_clflush(), &quot;clflush should be available&quot;);</span>
<span class="udiff-line-added">+   bool optimized = VM_Version::supports_clflushopt();</span>
<span class="udiff-line-added">+   bool no_evict = VM_Version::supports_clwb();</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   // prefer clwb (writeback without evict) otherwise</span>
<span class="udiff-line-added">+   // prefer clflushopt (potentially parallel writeback with evict)</span>
<span class="udiff-line-added">+   // otherwise fallback on clflush (serial writeback with evict)</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   if (optimized) {</span>
<span class="udiff-line-added">+     if (no_evict) {</span>
<span class="udiff-line-added">+       clwb(line);</span>
<span class="udiff-line-added">+     } else {</span>
<span class="udiff-line-added">+       clflushopt(line);</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     // no need for fence when using CLFLUSH</span>
<span class="udiff-line-added">+     clflush(line);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void MacroAssembler::cache_wbsync(bool is_pre)</span>
<span class="udiff-line-added">+ {</span>
<span class="udiff-line-added">+   assert(VM_Version::supports_clflush(), &quot;clflush should be available&quot;);</span>
<span class="udiff-line-added">+   bool optimized = VM_Version::supports_clflushopt();</span>
<span class="udiff-line-added">+   bool no_evict = VM_Version::supports_clwb();</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   // pick the correct implementation</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   if (!is_pre &amp;&amp; (optimized || no_evict)) {</span>
<span class="udiff-line-added">+     // need an sfence for post flush when using clflushopt or clwb</span>
<span class="udiff-line-added">+     // otherwise no no need for any synchroniaztion</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     sfence();</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ #endif // _LP64</span>
<span class="udiff-line-added">+ </span>
  Assembler::Condition MacroAssembler::negate_condition(Assembler::Condition cond) {
    switch (cond) {
      // Note some conditions are synonyms for others
      case Assembler::zero:         return Assembler::notZero;
      case Assembler::notZero:      return Assembler::zero;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -9768,6 +10019,6 @@</span>
      mov(thread, rax);
      pop(rax);
    }
  }
  
<span class="udiff-line-modified-removed">- #endif</span>
<span class="udiff-line-modified-added">+ #endif // !WIN32 || _LP64</span>
</pre>
<center><a href="jvmciCodeInstaller_x86.cpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_x86.hpp.udiff.html" target="_top">next &gt;</a></center>  </body>
</html>