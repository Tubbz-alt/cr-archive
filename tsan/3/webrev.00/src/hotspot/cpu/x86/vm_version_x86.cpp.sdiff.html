<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/vm_version_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="vm_version_ext_x86.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="vm_version_x86.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/vm_version_x86.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;jvm.h&quot;
  27 #include &quot;asm/macroAssembler.hpp&quot;
  28 #include &quot;asm/macroAssembler.inline.hpp&quot;
  29 #include &quot;logging/log.hpp&quot;
  30 #include &quot;logging/logStream.hpp&quot;
  31 #include &quot;memory/resourceArea.hpp&quot;
  32 #include &quot;runtime/java.hpp&quot;
  33 #include &quot;runtime/os.hpp&quot;
  34 #include &quot;runtime/stubCodeGenerator.hpp&quot;
<span class="line-modified">  35 #include &quot;vm_version_x86.hpp&quot;</span>


  36 

  37 
  38 int VM_Version::_cpu;
  39 int VM_Version::_model;
  40 int VM_Version::_stepping;
  41 VM_Version::CpuidInfo VM_Version::_cpuid_info = { 0, };
  42 
  43 // Address of instruction which causes SEGV
  44 address VM_Version::_cpuinfo_segv_addr = 0;
  45 // Address of instruction after the one which causes SEGV
  46 address VM_Version::_cpuinfo_cont_addr = 0;
  47 
  48 static BufferBlob* stub_blob;
  49 static const int stub_size = 1100;
  50 
  51 extern &quot;C&quot; {
  52   typedef void (*get_cpu_info_stub_t)(void*);
  53 }
  54 static get_cpu_info_stub_t get_cpu_info_stub = NULL;
  55 
  56 
</pre>
<hr />
<pre>
 348     __ jccb(Assembler::notEqual, done); // jump if AVX is not supported
 349 
 350     __ movl(rax, 0x6);
 351     __ andl(rax, Address(rbp, in_bytes(VM_Version::xem_xcr0_offset()))); // xcr0 bits sse | ymm
 352     __ cmpl(rax, 0x6);
 353     __ jccb(Assembler::equal, start_simd_check); // return if AVX is not supported
 354 
 355     // we need to bridge farther than imm8, so we use this island as a thunk
 356     __ bind(done);
 357     __ jmp(wrapup);
 358 
 359     __ bind(start_simd_check);
 360     //
 361     // Some OSs have a bug when upper 128/256bits of YMM/ZMM
 362     // registers are not restored after a signal processing.
 363     // Generate SEGV here (reference through NULL)
 364     // and check upper YMM/ZMM bits after it.
 365     //
 366     intx saved_useavx = UseAVX;
 367     intx saved_usesse = UseSSE;
<span class="line-removed"> 368     // check _cpuid_info.sef_cpuid7_ebx.bits.avx512f</span>
<span class="line-removed"> 369     __ lea(rsi, Address(rbp, in_bytes(VM_Version::sef_cpuid7_offset())));</span>
<span class="line-removed"> 370     __ movl(rax, 0x10000);</span>
<span class="line-removed"> 371     __ andl(rax, Address(rsi, 4)); // xcr0 bits sse | ymm</span>
<span class="line-removed"> 372     __ cmpl(rax, 0x10000);</span>
<span class="line-removed"> 373     __ jccb(Assembler::notEqual, legacy_setup); // jump if EVEX is not supported</span>
<span class="line-removed"> 374     // check _cpuid_info.xem_xcr0_eax.bits.opmask</span>
<span class="line-removed"> 375     // check _cpuid_info.xem_xcr0_eax.bits.zmm512</span>
<span class="line-removed"> 376     // check _cpuid_info.xem_xcr0_eax.bits.zmm32</span>
<span class="line-removed"> 377     __ movl(rax, 0xE0);</span>
<span class="line-removed"> 378     __ andl(rax, Address(rbp, in_bytes(VM_Version::xem_xcr0_offset()))); // xcr0 bits sse | ymm</span>
<span class="line-removed"> 379     __ cmpl(rax, 0xE0);</span>
<span class="line-removed"> 380     __ jccb(Assembler::notEqual, legacy_setup); // jump if EVEX is not supported</span>
 381 
 382     // If UseAVX is unitialized or is set by the user to include EVEX
 383     if (use_evex) {




















 384       // EVEX setup: run in lowest evex mode
 385       VM_Version::set_evex_cpuFeatures(); // Enable temporary to pass asserts
 386       UseAVX = 3;
 387       UseSSE = 2;
 388 #ifdef _WINDOWS
 389       // xmm5-xmm15 are not preserved by caller on windows
 390       // https://msdn.microsoft.com/en-us/library/9z1stfyw.aspx
 391       __ subptr(rsp, 64);
 392       __ evmovdqul(Address(rsp, 0), xmm7, Assembler::AVX_512bit);
 393 #ifdef _LP64
 394       __ subptr(rsp, 64);
 395       __ evmovdqul(Address(rsp, 0), xmm8, Assembler::AVX_512bit);
 396       __ subptr(rsp, 64);
 397       __ evmovdqul(Address(rsp, 0), xmm31, Assembler::AVX_512bit);
 398 #endif // _LP64
 399 #endif // _WINDOWS
 400 
 401       // load value into all 64 bytes of zmm7 register
 402       __ movl(rcx, VM_Version::ymm_test_value());
 403       __ movdl(xmm0, rcx);
</pre>
<hr />
<pre>
 432 
 433     __ movdl(xmm0, rcx);
 434     __ pshufd(xmm0, xmm0, 0x00);
 435     __ vinsertf128_high(xmm0, xmm0);
 436     __ vmovdqu(xmm7, xmm0);
 437 #ifdef _LP64
 438     __ vmovdqu(xmm8, xmm0);
 439     __ vmovdqu(xmm15, xmm0);
 440 #endif
 441     VM_Version::clean_cpuFeatures();
 442 
 443     __ bind(save_restore_except);
 444     __ xorl(rsi, rsi);
 445     VM_Version::set_cpuinfo_segv_addr(__ pc());
 446     // Generate SEGV
 447     __ movl(rax, Address(rsi, 0));
 448 
 449     VM_Version::set_cpuinfo_cont_addr(__ pc());
 450     // Returns here after signal. Save xmm0 to check it later.
 451 
<span class="line-removed"> 452     // check _cpuid_info.sef_cpuid7_ebx.bits.avx512f</span>
<span class="line-removed"> 453     __ lea(rsi, Address(rbp, in_bytes(VM_Version::sef_cpuid7_offset())));</span>
<span class="line-removed"> 454     __ movl(rax, 0x10000);</span>
<span class="line-removed"> 455     __ andl(rax, Address(rsi, 4));</span>
<span class="line-removed"> 456     __ cmpl(rax, 0x10000);</span>
<span class="line-removed"> 457     __ jcc(Assembler::notEqual, legacy_save_restore);</span>
<span class="line-removed"> 458     // check _cpuid_info.xem_xcr0_eax.bits.opmask</span>
<span class="line-removed"> 459     // check _cpuid_info.xem_xcr0_eax.bits.zmm512</span>
<span class="line-removed"> 460     // check _cpuid_info.xem_xcr0_eax.bits.zmm32</span>
<span class="line-removed"> 461     __ movl(rax, 0xE0);</span>
<span class="line-removed"> 462     __ andl(rax, Address(rbp, in_bytes(VM_Version::xem_xcr0_offset()))); // xcr0 bits sse | ymm</span>
<span class="line-removed"> 463     __ cmpl(rax, 0xE0);</span>
<span class="line-removed"> 464     __ jcc(Assembler::notEqual, legacy_save_restore);</span>
<span class="line-removed"> 465 </span>
 466     // If UseAVX is unitialized or is set by the user to include EVEX
 467     if (use_evex) {




















 468       // EVEX check: run in lowest evex mode
 469       VM_Version::set_evex_cpuFeatures(); // Enable temporary to pass asserts
 470       UseAVX = 3;
 471       UseSSE = 2;
 472       __ lea(rsi, Address(rbp, in_bytes(VM_Version::zmm_save_offset())));
 473       __ evmovdqul(Address(rsi, 0), xmm0, Assembler::AVX_512bit);
 474       __ evmovdqul(Address(rsi, 64), xmm7, Assembler::AVX_512bit);
 475 #ifdef _LP64
 476       __ evmovdqul(Address(rsi, 128), xmm8, Assembler::AVX_512bit);
 477       __ evmovdqul(Address(rsi, 192), xmm31, Assembler::AVX_512bit);
 478 #endif
 479 
 480 #ifdef _WINDOWS
 481 #ifdef _LP64
 482       __ evmovdqul(xmm31, Address(rsp, 0), Assembler::AVX_512bit);
 483       __ addptr(rsp, 64);
 484       __ evmovdqul(xmm8, Address(rsp, 0), Assembler::AVX_512bit);
 485       __ addptr(rsp, 64);
 486 #endif // _LP64
 487       __ evmovdqul(xmm7, Address(rsp, 0), Assembler::AVX_512bit);
</pre>
<hr />
<pre>
 590     vm_exit_during_initialization(&quot;Unknown x64 processor: SSE2 not supported&quot;);
 591   }
 592   // in 64 bit the use of SSE2 is the minimum
 593   if (UseSSE &lt; 2) UseSSE = 2;
 594 #endif
 595 
 596 #ifdef AMD64
 597   // flush_icache_stub have to be generated first.
 598   // That is why Icache line size is hard coded in ICache class,
 599   // see icache_x86.hpp. It is also the reason why we can&#39;t use
 600   // clflush instruction in 32-bit VM since it could be running
 601   // on CPU which does not support it.
 602   //
 603   // The only thing we can do is to verify that flushed
 604   // ICache::line_size has correct value.
 605   guarantee(_cpuid_info.std_cpuid1_edx.bits.clflush != 0, &quot;clflush is not supported&quot;);
 606   // clflush_size is size in quadwords (8 bytes).
 607   guarantee(_cpuid_info.std_cpuid1_ebx.bits.clflush_size == 8, &quot;such clflush size is not supported&quot;);
 608 #endif
 609 










 610   // If the OS doesn&#39;t support SSE, we can&#39;t use this feature even if the HW does
 611   if (!os::supports_sse())
 612     _features &amp;= ~(CPU_SSE|CPU_SSE2|CPU_SSE3|CPU_SSSE3|CPU_SSE4A|CPU_SSE4_1|CPU_SSE4_2);
 613 
 614   if (UseSSE &lt; 4) {
 615     _features &amp;= ~CPU_SSE4_1;
 616     _features &amp;= ~CPU_SSE4_2;
 617   }
 618 
 619   if (UseSSE &lt; 3) {
 620     _features &amp;= ~CPU_SSE3;
 621     _features &amp;= ~CPU_SSSE3;
 622     _features &amp;= ~CPU_SSE4A;
 623   }
 624 
 625   if (UseSSE &lt; 2)
 626     _features &amp;= ~CPU_SSE2;
 627 
 628   if (UseSSE &lt; 1)
 629     _features &amp;= ~CPU_SSE;
 630 
 631   //since AVX instructions is slower than SSE in some ZX cpus, force USEAVX=0.
 632   if (is_zx() &amp;&amp; ((cpu_family() == 6) || (cpu_family() == 7))) {
 633     UseAVX = 0;
 634   }
 635 
 636   // first try initial setting and detect what we can support
 637   int use_avx_limit = 0;
 638   if (UseAVX &gt; 0) {
 639     if (UseAVX &gt; 2 &amp;&amp; supports_evex()) {
 640       use_avx_limit = 3;
 641     } else if (UseAVX &gt; 1 &amp;&amp; supports_avx2()) {
 642       use_avx_limit = 2;
 643     } else if (UseAVX &gt; 0 &amp;&amp; supports_avx()) {
 644       use_avx_limit = 1;
 645     } else {
 646       use_avx_limit = 0;
 647     }
 648   }
 649   if (FLAG_IS_DEFAULT(UseAVX)) {
<span class="line-modified"> 650     FLAG_SET_DEFAULT(UseAVX, use_avx_limit);</span>
<span class="line-modified"> 651   } else if (UseAVX &gt; use_avx_limit) {</span>






 652     warning(&quot;UseAVX=%d is not supported on this CPU, setting it to UseAVX=%d&quot;, (int) UseAVX, use_avx_limit);
 653     FLAG_SET_DEFAULT(UseAVX, use_avx_limit);
 654   } else if (UseAVX &lt; 0) {
 655     warning(&quot;UseAVX=%d is not valid, setting it to UseAVX=0&quot;, (int) UseAVX);
 656     FLAG_SET_DEFAULT(UseAVX, 0);
 657   }
 658 
 659   if (UseAVX &lt; 3) {
 660     _features &amp;= ~CPU_AVX512F;
 661     _features &amp;= ~CPU_AVX512DQ;
 662     _features &amp;= ~CPU_AVX512CD;
 663     _features &amp;= ~CPU_AVX512BW;
 664     _features &amp;= ~CPU_AVX512VL;
 665     _features &amp;= ~CPU_AVX512_VPOPCNTDQ;
<span class="line-modified"> 666     _features &amp;= ~CPU_VPCLMULQDQ;</span>
 667     _features &amp;= ~CPU_VAES;


 668   }
 669 
 670   if (UseAVX &lt; 2)
 671     _features &amp;= ~CPU_AVX2;
 672 
 673   if (UseAVX &lt; 1) {
 674     _features &amp;= ~CPU_AVX;
 675     _features &amp;= ~CPU_VZEROUPPER;
 676   }
 677 
 678   if (logical_processors_per_package() == 1) {
 679     // HT processor could be installed on a system which doesn&#39;t support HT.
 680     _features &amp;= ~CPU_HT;
 681   }
 682 
<span class="line-modified"> 683   if( is_intel() ) { // Intel cpus specific settings</span>
 684     if (is_knights_family()) {
 685       _features &amp;= ~CPU_VZEROUPPER;
 686     }
 687   }
 688 
 689   char buf[256];
<span class="line-modified"> 690   jio_snprintf(buf, sizeof(buf), &quot;(%u cores per cpu, %u threads per core) family %d model %d stepping %d%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s&quot;,</span>
 691                cores_per_cpu(), threads_per_core(),
 692                cpu_family(), _model, _stepping,
 693                (supports_cmov() ? &quot;, cmov&quot; : &quot;&quot;),
 694                (supports_cmpxchg8() ? &quot;, cx8&quot; : &quot;&quot;),
 695                (supports_fxsr() ? &quot;, fxsr&quot; : &quot;&quot;),
 696                (supports_mmx()  ? &quot;, mmx&quot;  : &quot;&quot;),
 697                (supports_sse()  ? &quot;, sse&quot;  : &quot;&quot;),
 698                (supports_sse2() ? &quot;, sse2&quot; : &quot;&quot;),
 699                (supports_sse3() ? &quot;, sse3&quot; : &quot;&quot;),
 700                (supports_ssse3()? &quot;, ssse3&quot;: &quot;&quot;),
 701                (supports_sse4_1() ? &quot;, sse4.1&quot; : &quot;&quot;),
 702                (supports_sse4_2() ? &quot;, sse4.2&quot; : &quot;&quot;),
 703                (supports_popcnt() ? &quot;, popcnt&quot; : &quot;&quot;),
 704                (supports_avx()    ? &quot;, avx&quot; : &quot;&quot;),
 705                (supports_avx2()   ? &quot;, avx2&quot; : &quot;&quot;),
 706                (supports_aes()    ? &quot;, aes&quot; : &quot;&quot;),
 707                (supports_clmul()  ? &quot;, clmul&quot; : &quot;&quot;),
 708                (supports_erms()   ? &quot;, erms&quot; : &quot;&quot;),
 709                (supports_rtm()    ? &quot;, rtm&quot; : &quot;&quot;),
 710                (supports_mmx_ext() ? &quot;, mmxext&quot; : &quot;&quot;),
 711                (supports_3dnow_prefetch() ? &quot;, 3dnowpref&quot; : &quot;&quot;),
 712                (supports_lzcnt()   ? &quot;, lzcnt&quot;: &quot;&quot;),
 713                (supports_sse4a()   ? &quot;, sse4a&quot;: &quot;&quot;),
 714                (supports_ht() ? &quot;, ht&quot;: &quot;&quot;),
 715                (supports_tsc() ? &quot;, tsc&quot;: &quot;&quot;),
 716                (supports_tscinv_bit() ? &quot;, tscinvbit&quot;: &quot;&quot;),
 717                (supports_tscinv() ? &quot;, tscinv&quot;: &quot;&quot;),
 718                (supports_bmi1() ? &quot;, bmi1&quot; : &quot;&quot;),
 719                (supports_bmi2() ? &quot;, bmi2&quot; : &quot;&quot;),
 720                (supports_adx() ? &quot;, adx&quot; : &quot;&quot;),
 721                (supports_evex() ? &quot;, evex&quot; : &quot;&quot;),
 722                (supports_sha() ? &quot;, sha&quot; : &quot;&quot;),
<span class="line-modified"> 723                (supports_fma() ? &quot;, fma&quot; : &quot;&quot;));</span>



 724   _features_string = os::strdup(buf);
 725 
 726   // UseSSE is set to the smaller of what hardware supports and what
 727   // the command line requires.  I.e., you cannot set UseSSE to 2 on
 728   // older Pentiums which do not support it.
 729   int use_sse_limit = 0;
 730   if (UseSSE &gt; 0) {
 731     if (UseSSE &gt; 3 &amp;&amp; supports_sse4_1()) {
 732       use_sse_limit = 4;
 733     } else if (UseSSE &gt; 2 &amp;&amp; supports_sse3()) {
 734       use_sse_limit = 3;
 735     } else if (UseSSE &gt; 1 &amp;&amp; supports_sse2()) {
 736       use_sse_limit = 2;
 737     } else if (UseSSE &gt; 0 &amp;&amp; supports_sse()) {
 738       use_sse_limit = 1;
 739     } else {
 740       use_sse_limit = 0;
 741     }
 742   }
 743   if (FLAG_IS_DEFAULT(UseSSE)) {
</pre>
<hr />
<pre>
 764       if (UseSSE &gt; 2) {
 765         if (FLAG_IS_DEFAULT(UseAESIntrinsics)) {
 766           FLAG_SET_DEFAULT(UseAESIntrinsics, true);
 767         }
 768       } else {
 769         // The AES intrinsic stubs require AES instruction support (of course)
 770         // but also require sse3 mode or higher for instructions it use.
 771         if (UseAESIntrinsics &amp;&amp; !FLAG_IS_DEFAULT(UseAESIntrinsics)) {
 772           warning(&quot;X86 AES intrinsics require SSE3 instructions or higher. Intrinsics will be disabled.&quot;);
 773         }
 774         FLAG_SET_DEFAULT(UseAESIntrinsics, false);
 775       }
 776 
 777       // --AES-CTR begins--
 778       if (!UseAESIntrinsics) {
 779         if (UseAESCTRIntrinsics &amp;&amp; !FLAG_IS_DEFAULT(UseAESCTRIntrinsics)) {
 780           warning(&quot;AES-CTR intrinsics require UseAESIntrinsics flag to be enabled. Intrinsics will be disabled.&quot;);
 781           FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);
 782         }
 783       } else {
<span class="line-modified"> 784         if(supports_sse4_1()) {</span>
 785           if (FLAG_IS_DEFAULT(UseAESCTRIntrinsics)) {
 786             FLAG_SET_DEFAULT(UseAESCTRIntrinsics, true);
 787           }
 788         } else {
 789            // The AES-CTR intrinsic stubs require AES instruction support (of course)
 790            // but also require sse4.1 mode or higher for instructions it use.
 791           if (UseAESCTRIntrinsics &amp;&amp; !FLAG_IS_DEFAULT(UseAESCTRIntrinsics)) {
 792              warning(&quot;X86 AES-CTR intrinsics require SSE4.1 instructions or higher. Intrinsics will be disabled.&quot;);
 793            }
 794            FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);
 795         }
 796       }
 797       // --AES-CTR ends--
 798     }
 799   } else if (UseAES || UseAESIntrinsics || UseAESCTRIntrinsics) {
 800     if (UseAES &amp;&amp; !FLAG_IS_DEFAULT(UseAES)) {
 801       warning(&quot;AES instructions are not available on this CPU&quot;);
 802       FLAG_SET_DEFAULT(UseAES, false);
 803     }
 804     if (UseAESIntrinsics &amp;&amp; !FLAG_IS_DEFAULT(UseAESIntrinsics)) {
</pre>
<hr />
<pre>
 884   }
 885 
 886   if (supports_sha() &amp;&amp; supports_sse4_1() &amp;&amp; UseSHA) {
 887     if (FLAG_IS_DEFAULT(UseSHA1Intrinsics)) {
 888       FLAG_SET_DEFAULT(UseSHA1Intrinsics, true);
 889     }
 890   } else if (UseSHA1Intrinsics) {
 891     warning(&quot;Intrinsics for SHA-1 crypto hash functions not available on this CPU.&quot;);
 892     FLAG_SET_DEFAULT(UseSHA1Intrinsics, false);
 893   }
 894 
 895   if (supports_sse4_1() &amp;&amp; UseSHA) {
 896     if (FLAG_IS_DEFAULT(UseSHA256Intrinsics)) {
 897       FLAG_SET_DEFAULT(UseSHA256Intrinsics, true);
 898     }
 899   } else if (UseSHA256Intrinsics) {
 900     warning(&quot;Intrinsics for SHA-224 and SHA-256 crypto hash functions not available on this CPU.&quot;);
 901     FLAG_SET_DEFAULT(UseSHA256Intrinsics, false);
 902   }
 903 


 904   if (UseSHA &amp;&amp; supports_avx2() &amp;&amp; supports_bmi2()) {
 905     if (FLAG_IS_DEFAULT(UseSHA512Intrinsics)) {
 906       FLAG_SET_DEFAULT(UseSHA512Intrinsics, true);
 907     }
<span class="line-modified"> 908   } else if (UseSHA512Intrinsics) {</span>


 909     warning(&quot;Intrinsics for SHA-384 and SHA-512 crypto hash functions not available on this CPU.&quot;);
 910     FLAG_SET_DEFAULT(UseSHA512Intrinsics, false);
 911   }
 912 
 913   if (!(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics)) {
 914     FLAG_SET_DEFAULT(UseSHA, false);
 915   }
 916 
 917   if (UseAdler32Intrinsics) {
 918     warning(&quot;Adler32Intrinsics not available on this CPU.&quot;);
 919     FLAG_SET_DEFAULT(UseAdler32Intrinsics, false);
 920   }
 921 
 922   if (!supports_rtm() &amp;&amp; UseRTMLocking) {
 923     // Can&#39;t continue because UseRTMLocking affects UseBiasedLocking flag
 924     // setting during arguments processing. See use_biased_locking().
 925     // VM_Version_init() is executed after UseBiasedLocking is used
 926     // in Thread::allocate().
 927     vm_exit_during_initialization(&quot;RTM instructions are not available on this CPU&quot;);
 928   }
</pre>
<hr />
<pre>
 980   if (UseFPUForSpilling) {
 981     if (UseSSE &lt; 2) {
 982       // Only supported with SSE2+
 983       FLAG_SET_DEFAULT(UseFPUForSpilling, false);
 984     }
 985   }
 986 #endif
 987 
 988 #if COMPILER2_OR_JVMCI
 989   int max_vector_size = 0;
 990   if (UseSSE &lt; 2) {
 991     // Vectors (in XMM) are only supported with SSE2+
 992     // SSE is always 2 on x64.
 993     max_vector_size = 0;
 994   } else if (UseAVX == 0 || !os_supports_avx_vectors()) {
 995     // 16 byte vectors (in XMM) are supported with SSE2+
 996     max_vector_size = 16;
 997   } else if (UseAVX == 1 || UseAVX == 2) {
 998     // 32 bytes vectors (in YMM) are only supported with AVX+
 999     max_vector_size = 32;
<span class="line-modified">1000   } else if (UseAVX &gt; 2 ) {</span>
1001     // 64 bytes vectors (in ZMM) are only supported with AVX 3
1002     max_vector_size = 64;
1003   }
1004 
1005 #ifdef _LP64
1006   int min_vector_size = 4; // We require MaxVectorSize to be at least 4 on 64bit
1007 #else
1008   int min_vector_size = 0;
1009 #endif
1010 
1011   if (!FLAG_IS_DEFAULT(MaxVectorSize)) {
1012     if (MaxVectorSize &lt; min_vector_size) {
1013       warning(&quot;MaxVectorSize must be at least %i on this platform&quot;, min_vector_size);
1014       FLAG_SET_DEFAULT(MaxVectorSize, min_vector_size);
1015     }
1016     if (MaxVectorSize &gt; max_vector_size) {
1017       warning(&quot;MaxVectorSize must be at most %i on this platform&quot;, max_vector_size);
1018       FLAG_SET_DEFAULT(MaxVectorSize, max_vector_size);
1019     }
1020     if (!is_power_of_2(MaxVectorSize)) {
</pre>
<hr />
<pre>
1026     FLAG_SET_DEFAULT(MaxVectorSize, max_vector_size);
1027   }
1028 
1029 #if defined(COMPILER2) &amp;&amp; defined(ASSERT)
1030   if (MaxVectorSize &gt; 0) {
1031     if (supports_avx() &amp;&amp; PrintMiscellaneous &amp;&amp; Verbose &amp;&amp; TraceNewVectors) {
1032       tty-&gt;print_cr(&quot;State of YMM registers after signal handle:&quot;);
1033       int nreg = 2 LP64_ONLY(+2);
1034       const char* ymm_name[4] = {&quot;0&quot;, &quot;7&quot;, &quot;8&quot;, &quot;15&quot;};
1035       for (int i = 0; i &lt; nreg; i++) {
1036         tty-&gt;print(&quot;YMM%s:&quot;, ymm_name[i]);
1037         for (int j = 7; j &gt;=0; j--) {
1038           tty-&gt;print(&quot; %x&quot;, _cpuid_info.ymm_save[i*8 + j]);
1039         }
1040         tty-&gt;cr();
1041       }
1042     }
1043   }
1044 #endif // COMPILER2 &amp;&amp; ASSERT
1045 







1046 #ifdef _LP64
1047   if (FLAG_IS_DEFAULT(UseMultiplyToLenIntrinsic)) {
1048     UseMultiplyToLenIntrinsic = true;
1049   }
1050   if (FLAG_IS_DEFAULT(UseSquareToLenIntrinsic)) {
1051     UseSquareToLenIntrinsic = true;
1052   }
1053   if (FLAG_IS_DEFAULT(UseMulAddIntrinsic)) {
1054     UseMulAddIntrinsic = true;
1055   }
1056   if (FLAG_IS_DEFAULT(UseMontgomeryMultiplyIntrinsic)) {
1057     UseMontgomeryMultiplyIntrinsic = true;
1058   }
1059   if (FLAG_IS_DEFAULT(UseMontgomerySquareIntrinsic)) {
1060     UseMontgomerySquareIntrinsic = true;
1061   }
1062 #else
1063   if (UseMultiplyToLenIntrinsic) {
1064     if (!FLAG_IS_DEFAULT(UseMultiplyToLenIntrinsic)) {
1065       warning(&quot;multiplyToLen intrinsic is not available in 32-bit VM&quot;);
</pre>
<hr />
<pre>
1144           UseUnalignedLoadStores = true; // use movdqu on newest ZX cpus
1145         }
1146       }
1147       if (supports_sse4_2()) {
1148         if (FLAG_IS_DEFAULT(UseSSE42Intrinsics)) {
1149           FLAG_SET_DEFAULT(UseSSE42Intrinsics, true);
1150         }
1151       } else {
1152         if (UseSSE42Intrinsics &amp;&amp; !FLAG_IS_DEFAULT(UseAESIntrinsics)) {
1153           warning(&quot;SSE4.2 intrinsics require SSE4.2 instructions or higher. Intrinsics will be disabled.&quot;);
1154         }
1155         FLAG_SET_DEFAULT(UseSSE42Intrinsics, false);
1156       }
1157     }
1158 
1159     if (FLAG_IS_DEFAULT(AllocatePrefetchInstr) &amp;&amp; supports_3dnow_prefetch()) {
1160       FLAG_SET_DEFAULT(AllocatePrefetchInstr, 3);
1161     }
1162   }
1163 
<span class="line-modified">1164   if( is_amd() ) { // AMD cpus specific settings</span>
<span class="line-modified">1165     if( supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseAddressNop) ) {</span>
1166       // Use it on new AMD cpus starting from Opteron.
1167       UseAddressNop = true;
1168     }
<span class="line-modified">1169     if( supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseNewLongLShift) ) {</span>
1170       // Use it on new AMD cpus starting from Opteron.
1171       UseNewLongLShift = true;
1172     }
<span class="line-modified">1173     if( FLAG_IS_DEFAULT(UseXmmLoadAndClearUpper) ) {</span>
1174       if (supports_sse4a()) {
1175         UseXmmLoadAndClearUpper = true; // use movsd only on &#39;10h&#39; Opteron
1176       } else {
1177         UseXmmLoadAndClearUpper = false;
1178       }
1179     }
<span class="line-modified">1180     if( FLAG_IS_DEFAULT(UseXmmRegToRegMoveAll) ) {</span>
<span class="line-modified">1181       if( supports_sse4a() ) {</span>
1182         UseXmmRegToRegMoveAll = true; // use movaps, movapd only on &#39;10h&#39;
1183       } else {
1184         UseXmmRegToRegMoveAll = false;
1185       }
1186     }
<span class="line-modified">1187     if( FLAG_IS_DEFAULT(UseXmmI2F) ) {</span>
<span class="line-modified">1188       if( supports_sse4a() ) {</span>
1189         UseXmmI2F = true;
1190       } else {
1191         UseXmmI2F = false;
1192       }
1193     }
<span class="line-modified">1194     if( FLAG_IS_DEFAULT(UseXmmI2D) ) {</span>
<span class="line-modified">1195       if( supports_sse4a() ) {</span>
1196         UseXmmI2D = true;
1197       } else {
1198         UseXmmI2D = false;
1199       }
1200     }
1201     if (supports_sse4_2()) {
1202       if (FLAG_IS_DEFAULT(UseSSE42Intrinsics)) {
1203         FLAG_SET_DEFAULT(UseSSE42Intrinsics, true);
1204       }
1205     } else {
1206       if (UseSSE42Intrinsics &amp;&amp; !FLAG_IS_DEFAULT(UseAESIntrinsics)) {
1207         warning(&quot;SSE4.2 intrinsics require SSE4.2 instructions or higher. Intrinsics will be disabled.&quot;);
1208       }
1209       FLAG_SET_DEFAULT(UseSSE42Intrinsics, false);
1210     }
1211 
1212     // some defaults for AMD family 15h
<span class="line-modified">1213     if ( cpu_family() == 0x15 ) {</span>
1214       // On family 15h processors default is no sw prefetch
1215       if (FLAG_IS_DEFAULT(AllocatePrefetchStyle)) {
1216         FLAG_SET_DEFAULT(AllocatePrefetchStyle, 0);
1217       }
1218       // Also, if some other prefetch style is specified, default instruction type is PREFETCHW
1219       if (FLAG_IS_DEFAULT(AllocatePrefetchInstr)) {
1220         FLAG_SET_DEFAULT(AllocatePrefetchInstr, 3);
1221       }
1222       // On family 15h processors use XMM and UnalignedLoadStores for Array Copy
1223       if (supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseXMMForArrayCopy)) {
1224         FLAG_SET_DEFAULT(UseXMMForArrayCopy, true);
1225       }
1226       if (supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseUnalignedLoadStores)) {
1227         FLAG_SET_DEFAULT(UseUnalignedLoadStores, true);
1228       }
1229     }
1230 
1231 #ifdef COMPILER2
1232     if (cpu_family() &lt; 0x17 &amp;&amp; MaxVectorSize &gt; 16) {
1233       // Limit vectors size to 16 bytes on AMD cpus &lt; 17h.
1234       FLAG_SET_DEFAULT(MaxVectorSize, 16);
1235     }
1236 #endif // COMPILER2
1237 
<span class="line-modified">1238     // Some defaults for AMD family 17h</span>
<span class="line-modified">1239     if ( cpu_family() == 0x17 ) {</span>
1240       // On family 17h processors use XMM and UnalignedLoadStores for Array Copy
1241       if (supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseXMMForArrayCopy)) {
1242         FLAG_SET_DEFAULT(UseXMMForArrayCopy, true);
1243       }
1244       if (supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseUnalignedLoadStores)) {
1245         FLAG_SET_DEFAULT(UseUnalignedLoadStores, true);
1246       }
1247 #ifdef COMPILER2
1248       if (supports_sse4_2() &amp;&amp; FLAG_IS_DEFAULT(UseFPUForSpilling)) {
1249         FLAG_SET_DEFAULT(UseFPUForSpilling, true);
1250       }
1251 #endif
1252     }
1253   }
1254 
<span class="line-modified">1255   if( is_intel() ) { // Intel cpus specific settings</span>
<span class="line-modified">1256     if( FLAG_IS_DEFAULT(UseStoreImmI16) ) {</span>
1257       UseStoreImmI16 = false; // don&#39;t use it on Intel cpus
1258     }
<span class="line-modified">1259     if( cpu_family() == 6 || cpu_family() == 15 ) {</span>
<span class="line-modified">1260       if( FLAG_IS_DEFAULT(UseAddressNop) ) {</span>
1261         // Use it on all Intel cpus starting from PentiumPro
1262         UseAddressNop = true;
1263       }
1264     }
<span class="line-modified">1265     if( FLAG_IS_DEFAULT(UseXmmLoadAndClearUpper) ) {</span>
1266       UseXmmLoadAndClearUpper = true; // use movsd on all Intel cpus
1267     }
<span class="line-modified">1268     if( FLAG_IS_DEFAULT(UseXmmRegToRegMoveAll) ) {</span>
<span class="line-modified">1269       if( supports_sse3() ) {</span>
1270         UseXmmRegToRegMoveAll = true; // use movaps, movapd on new Intel cpus
1271       } else {
1272         UseXmmRegToRegMoveAll = false;
1273       }
1274     }
<span class="line-modified">1275     if( cpu_family() == 6 &amp;&amp; supports_sse3() ) { // New Intel cpus</span>
1276 #ifdef COMPILER2
<span class="line-modified">1277       if( FLAG_IS_DEFAULT(MaxLoopPad) ) {</span>
1278         // For new Intel cpus do the next optimization:
1279         // don&#39;t align the beginning of a loop if there are enough instructions
1280         // left (NumberOfLoopInstrToAlign defined in c2_globals.hpp)
1281         // in current fetch line (OptoLoopAlignment) or the padding
1282         // is big (&gt; MaxLoopPad).
1283         // Set MaxLoopPad to 11 for new Intel cpus to reduce number of
1284         // generated NOP instructions. 11 is the largest size of one
1285         // address NOP instruction &#39;0F 1F&#39; (see Assembler::nop(i)).
1286         MaxLoopPad = 11;
1287       }
1288 #endif // COMPILER2
1289       if (FLAG_IS_DEFAULT(UseXMMForArrayCopy)) {
1290         UseXMMForArrayCopy = true; // use SSE2 movq on new Intel cpus
1291       }
1292       if ((supports_sse4_2() &amp;&amp; supports_ht()) || supports_avx()) { // Newest Intel cpus
1293         if (FLAG_IS_DEFAULT(UseUnalignedLoadStores)) {
1294           UseUnalignedLoadStores = true; // use movdqu on newest Intel cpus
1295         }
1296       }
1297       if (supports_sse4_2()) {
</pre>
<hr />
<pre>
1303           warning(&quot;SSE4.2 intrinsics require SSE4.2 instructions or higher. Intrinsics will be disabled.&quot;);
1304         }
1305         FLAG_SET_DEFAULT(UseSSE42Intrinsics, false);
1306       }
1307     }
1308     if (is_atom_family() || is_knights_family()) {
1309 #ifdef COMPILER2
1310       if (FLAG_IS_DEFAULT(OptoScheduling)) {
1311         OptoScheduling = true;
1312       }
1313 #endif
1314       if (supports_sse4_2()) { // Silvermont
1315         if (FLAG_IS_DEFAULT(UseUnalignedLoadStores)) {
1316           UseUnalignedLoadStores = true; // use movdqu on newest Intel cpus
1317         }
1318       }
1319       if (FLAG_IS_DEFAULT(UseIncDec)) {
1320         FLAG_SET_DEFAULT(UseIncDec, false);
1321       }
1322     }
<span class="line-modified">1323     if(FLAG_IS_DEFAULT(AllocatePrefetchInstr) &amp;&amp; supports_3dnow_prefetch()) {</span>
1324       FLAG_SET_DEFAULT(AllocatePrefetchInstr, 3);
1325     }
1326   }
1327 
1328 #ifdef _LP64
1329   if (UseSSE42Intrinsics) {
1330     if (FLAG_IS_DEFAULT(UseVectorizedMismatchIntrinsic)) {
1331       UseVectorizedMismatchIntrinsic = true;
1332     }
1333   } else if (UseVectorizedMismatchIntrinsic) {
1334     if (!FLAG_IS_DEFAULT(UseVectorizedMismatchIntrinsic))
1335       warning(&quot;vectorizedMismatch intrinsics are not available on this CPU&quot;);
1336     FLAG_SET_DEFAULT(UseVectorizedMismatchIntrinsic, false);
1337   }
1338 #else
1339   if (UseVectorizedMismatchIntrinsic) {
1340     if (!FLAG_IS_DEFAULT(UseVectorizedMismatchIntrinsic)) {
1341       warning(&quot;vectorizedMismatch intrinsic is not available in 32-bit VM&quot;);
1342     }
1343     FLAG_SET_DEFAULT(UseVectorizedMismatchIntrinsic, false);
</pre>
<hr />
<pre>
1552         log-&gt;print_cr(&quot; at distance %d, one line of %d bytes&quot;, (int) AllocatePrefetchDistance, (int) AllocatePrefetchStepSize);
1553       }
1554     }
1555 
1556     if (PrefetchCopyIntervalInBytes &gt; 0) {
1557       log-&gt;print_cr(&quot;PrefetchCopyIntervalInBytes %d&quot;, (int) PrefetchCopyIntervalInBytes);
1558     }
1559     if (PrefetchScanIntervalInBytes &gt; 0) {
1560       log-&gt;print_cr(&quot;PrefetchScanIntervalInBytes %d&quot;, (int) PrefetchScanIntervalInBytes);
1561     }
1562     if (PrefetchFieldsAhead &gt; 0) {
1563       log-&gt;print_cr(&quot;PrefetchFieldsAhead %d&quot;, (int) PrefetchFieldsAhead);
1564     }
1565     if (ContendedPaddingWidth &gt; 0) {
1566       log-&gt;print_cr(&quot;ContendedPaddingWidth %d&quot;, (int) ContendedPaddingWidth);
1567     }
1568   }
1569 #endif // !PRODUCT
1570 }
1571 




























































1572 bool VM_Version::use_biased_locking() {
1573 #if INCLUDE_RTM_OPT
1574   // RTM locking is most useful when there is high lock contention and
1575   // low data contention.  With high lock contention the lock is usually
1576   // inflated and biased locking is not suitable for that case.
1577   // RTM locking code requires that biased locking is off.
1578   // Note: we can&#39;t switch off UseBiasedLocking in get_processor_features()
1579   // because it is used by Thread::allocate() which is called before
1580   // VM_Version::initialize().
1581   if (UseRTMLocking &amp;&amp; UseBiasedLocking) {
1582     if (FLAG_IS_DEFAULT(UseBiasedLocking)) {
1583       FLAG_SET_DEFAULT(UseBiasedLocking, false);
1584     } else {
1585       warning(&quot;Biased locking is not supported with RTM locking; ignoring UseBiasedLocking flag.&quot; );
1586       UseBiasedLocking = false;
1587     }
1588   }
1589 #endif
1590   return UseBiasedLocking;
1591 }
1592 


















































1593 void VM_Version::initialize() {
1594   ResourceMark rm;
1595   // Making this stub must be FIRST use of assembler
1596 
1597   stub_blob = BufferBlob::create(&quot;get_cpu_info_stub&quot;, stub_size);
1598   if (stub_blob == NULL) {
1599     vm_exit_during_initialization(&quot;Unable to allocate get_cpu_info_stub&quot;);
1600   }
1601   CodeBuffer c(stub_blob);
1602   VM_Version_StubGenerator g(&amp;c);
1603   get_cpu_info_stub = CAST_TO_FN_PTR(get_cpu_info_stub_t,
1604                                      g.generate_get_cpu_info());
1605 
1606   get_processor_features();



1607 }
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;jvm.h&quot;
  27 #include &quot;asm/macroAssembler.hpp&quot;
  28 #include &quot;asm/macroAssembler.inline.hpp&quot;
  29 #include &quot;logging/log.hpp&quot;
  30 #include &quot;logging/logStream.hpp&quot;
  31 #include &quot;memory/resourceArea.hpp&quot;
  32 #include &quot;runtime/java.hpp&quot;
  33 #include &quot;runtime/os.hpp&quot;
  34 #include &quot;runtime/stubCodeGenerator.hpp&quot;
<span class="line-modified">  35 #include &quot;runtime/vm_version.hpp&quot;</span>
<span class="line-added">  36 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
<span class="line-added">  37 #include &quot;utilities/virtualizationSupport.hpp&quot;</span>
  38 
<span class="line-added">  39 #include OS_HEADER_INLINE(os)</span>
  40 
  41 int VM_Version::_cpu;
  42 int VM_Version::_model;
  43 int VM_Version::_stepping;
  44 VM_Version::CpuidInfo VM_Version::_cpuid_info = { 0, };
  45 
  46 // Address of instruction which causes SEGV
  47 address VM_Version::_cpuinfo_segv_addr = 0;
  48 // Address of instruction after the one which causes SEGV
  49 address VM_Version::_cpuinfo_cont_addr = 0;
  50 
  51 static BufferBlob* stub_blob;
  52 static const int stub_size = 1100;
  53 
  54 extern &quot;C&quot; {
  55   typedef void (*get_cpu_info_stub_t)(void*);
  56 }
  57 static get_cpu_info_stub_t get_cpu_info_stub = NULL;
  58 
  59 
</pre>
<hr />
<pre>
 351     __ jccb(Assembler::notEqual, done); // jump if AVX is not supported
 352 
 353     __ movl(rax, 0x6);
 354     __ andl(rax, Address(rbp, in_bytes(VM_Version::xem_xcr0_offset()))); // xcr0 bits sse | ymm
 355     __ cmpl(rax, 0x6);
 356     __ jccb(Assembler::equal, start_simd_check); // return if AVX is not supported
 357 
 358     // we need to bridge farther than imm8, so we use this island as a thunk
 359     __ bind(done);
 360     __ jmp(wrapup);
 361 
 362     __ bind(start_simd_check);
 363     //
 364     // Some OSs have a bug when upper 128/256bits of YMM/ZMM
 365     // registers are not restored after a signal processing.
 366     // Generate SEGV here (reference through NULL)
 367     // and check upper YMM/ZMM bits after it.
 368     //
 369     intx saved_useavx = UseAVX;
 370     intx saved_usesse = UseSSE;













 371 
 372     // If UseAVX is unitialized or is set by the user to include EVEX
 373     if (use_evex) {
<span class="line-added"> 374       // check _cpuid_info.sef_cpuid7_ebx.bits.avx512f</span>
<span class="line-added"> 375       __ lea(rsi, Address(rbp, in_bytes(VM_Version::sef_cpuid7_offset())));</span>
<span class="line-added"> 376       __ movl(rax, 0x10000);</span>
<span class="line-added"> 377       __ andl(rax, Address(rsi, 4)); // xcr0 bits sse | ymm</span>
<span class="line-added"> 378       __ cmpl(rax, 0x10000);</span>
<span class="line-added"> 379       __ jccb(Assembler::notEqual, legacy_setup); // jump if EVEX is not supported</span>
<span class="line-added"> 380       // check _cpuid_info.xem_xcr0_eax.bits.opmask</span>
<span class="line-added"> 381       // check _cpuid_info.xem_xcr0_eax.bits.zmm512</span>
<span class="line-added"> 382       // check _cpuid_info.xem_xcr0_eax.bits.zmm32</span>
<span class="line-added"> 383       __ movl(rax, 0xE0);</span>
<span class="line-added"> 384       __ andl(rax, Address(rbp, in_bytes(VM_Version::xem_xcr0_offset()))); // xcr0 bits sse | ymm</span>
<span class="line-added"> 385       __ cmpl(rax, 0xE0);</span>
<span class="line-added"> 386       __ jccb(Assembler::notEqual, legacy_setup); // jump if EVEX is not supported</span>
<span class="line-added"> 387 </span>
<span class="line-added"> 388       if (FLAG_IS_DEFAULT(UseAVX)) {</span>
<span class="line-added"> 389         __ lea(rsi, Address(rbp, in_bytes(VM_Version::std_cpuid1_offset())));</span>
<span class="line-added"> 390         __ movl(rax, Address(rsi, 0));</span>
<span class="line-added"> 391         __ cmpl(rax, 0x50654);              // If it is Skylake</span>
<span class="line-added"> 392         __ jcc(Assembler::equal, legacy_setup);</span>
<span class="line-added"> 393       }</span>
 394       // EVEX setup: run in lowest evex mode
 395       VM_Version::set_evex_cpuFeatures(); // Enable temporary to pass asserts
 396       UseAVX = 3;
 397       UseSSE = 2;
 398 #ifdef _WINDOWS
 399       // xmm5-xmm15 are not preserved by caller on windows
 400       // https://msdn.microsoft.com/en-us/library/9z1stfyw.aspx
 401       __ subptr(rsp, 64);
 402       __ evmovdqul(Address(rsp, 0), xmm7, Assembler::AVX_512bit);
 403 #ifdef _LP64
 404       __ subptr(rsp, 64);
 405       __ evmovdqul(Address(rsp, 0), xmm8, Assembler::AVX_512bit);
 406       __ subptr(rsp, 64);
 407       __ evmovdqul(Address(rsp, 0), xmm31, Assembler::AVX_512bit);
 408 #endif // _LP64
 409 #endif // _WINDOWS
 410 
 411       // load value into all 64 bytes of zmm7 register
 412       __ movl(rcx, VM_Version::ymm_test_value());
 413       __ movdl(xmm0, rcx);
</pre>
<hr />
<pre>
 442 
 443     __ movdl(xmm0, rcx);
 444     __ pshufd(xmm0, xmm0, 0x00);
 445     __ vinsertf128_high(xmm0, xmm0);
 446     __ vmovdqu(xmm7, xmm0);
 447 #ifdef _LP64
 448     __ vmovdqu(xmm8, xmm0);
 449     __ vmovdqu(xmm15, xmm0);
 450 #endif
 451     VM_Version::clean_cpuFeatures();
 452 
 453     __ bind(save_restore_except);
 454     __ xorl(rsi, rsi);
 455     VM_Version::set_cpuinfo_segv_addr(__ pc());
 456     // Generate SEGV
 457     __ movl(rax, Address(rsi, 0));
 458 
 459     VM_Version::set_cpuinfo_cont_addr(__ pc());
 460     // Returns here after signal. Save xmm0 to check it later.
 461 














 462     // If UseAVX is unitialized or is set by the user to include EVEX
 463     if (use_evex) {
<span class="line-added"> 464       // check _cpuid_info.sef_cpuid7_ebx.bits.avx512f</span>
<span class="line-added"> 465       __ lea(rsi, Address(rbp, in_bytes(VM_Version::sef_cpuid7_offset())));</span>
<span class="line-added"> 466       __ movl(rax, 0x10000);</span>
<span class="line-added"> 467       __ andl(rax, Address(rsi, 4));</span>
<span class="line-added"> 468       __ cmpl(rax, 0x10000);</span>
<span class="line-added"> 469       __ jcc(Assembler::notEqual, legacy_save_restore);</span>
<span class="line-added"> 470       // check _cpuid_info.xem_xcr0_eax.bits.opmask</span>
<span class="line-added"> 471       // check _cpuid_info.xem_xcr0_eax.bits.zmm512</span>
<span class="line-added"> 472       // check _cpuid_info.xem_xcr0_eax.bits.zmm32</span>
<span class="line-added"> 473       __ movl(rax, 0xE0);</span>
<span class="line-added"> 474       __ andl(rax, Address(rbp, in_bytes(VM_Version::xem_xcr0_offset()))); // xcr0 bits sse | ymm</span>
<span class="line-added"> 475       __ cmpl(rax, 0xE0);</span>
<span class="line-added"> 476       __ jcc(Assembler::notEqual, legacy_save_restore);</span>
<span class="line-added"> 477 </span>
<span class="line-added"> 478       if (FLAG_IS_DEFAULT(UseAVX)) {</span>
<span class="line-added"> 479         __ lea(rsi, Address(rbp, in_bytes(VM_Version::std_cpuid1_offset())));</span>
<span class="line-added"> 480         __ movl(rax, Address(rsi, 0));</span>
<span class="line-added"> 481         __ cmpl(rax, 0x50654);              // If it is Skylake</span>
<span class="line-added"> 482         __ jcc(Assembler::equal, legacy_save_restore);</span>
<span class="line-added"> 483       }</span>
 484       // EVEX check: run in lowest evex mode
 485       VM_Version::set_evex_cpuFeatures(); // Enable temporary to pass asserts
 486       UseAVX = 3;
 487       UseSSE = 2;
 488       __ lea(rsi, Address(rbp, in_bytes(VM_Version::zmm_save_offset())));
 489       __ evmovdqul(Address(rsi, 0), xmm0, Assembler::AVX_512bit);
 490       __ evmovdqul(Address(rsi, 64), xmm7, Assembler::AVX_512bit);
 491 #ifdef _LP64
 492       __ evmovdqul(Address(rsi, 128), xmm8, Assembler::AVX_512bit);
 493       __ evmovdqul(Address(rsi, 192), xmm31, Assembler::AVX_512bit);
 494 #endif
 495 
 496 #ifdef _WINDOWS
 497 #ifdef _LP64
 498       __ evmovdqul(xmm31, Address(rsp, 0), Assembler::AVX_512bit);
 499       __ addptr(rsp, 64);
 500       __ evmovdqul(xmm8, Address(rsp, 0), Assembler::AVX_512bit);
 501       __ addptr(rsp, 64);
 502 #endif // _LP64
 503       __ evmovdqul(xmm7, Address(rsp, 0), Assembler::AVX_512bit);
</pre>
<hr />
<pre>
 606     vm_exit_during_initialization(&quot;Unknown x64 processor: SSE2 not supported&quot;);
 607   }
 608   // in 64 bit the use of SSE2 is the minimum
 609   if (UseSSE &lt; 2) UseSSE = 2;
 610 #endif
 611 
 612 #ifdef AMD64
 613   // flush_icache_stub have to be generated first.
 614   // That is why Icache line size is hard coded in ICache class,
 615   // see icache_x86.hpp. It is also the reason why we can&#39;t use
 616   // clflush instruction in 32-bit VM since it could be running
 617   // on CPU which does not support it.
 618   //
 619   // The only thing we can do is to verify that flushed
 620   // ICache::line_size has correct value.
 621   guarantee(_cpuid_info.std_cpuid1_edx.bits.clflush != 0, &quot;clflush is not supported&quot;);
 622   // clflush_size is size in quadwords (8 bytes).
 623   guarantee(_cpuid_info.std_cpuid1_ebx.bits.clflush_size == 8, &quot;such clflush size is not supported&quot;);
 624 #endif
 625 
<span class="line-added"> 626 #ifdef _LP64</span>
<span class="line-added"> 627   // assigning this field effectively enables Unsafe.writebackMemory()</span>
<span class="line-added"> 628   // by initing UnsafeConstant.DATA_CACHE_LINE_FLUSH_SIZE to non-zero</span>
<span class="line-added"> 629   // that is only implemented on x86_64 and only if the OS plays ball</span>
<span class="line-added"> 630   if (os::supports_map_sync()) {</span>
<span class="line-added"> 631     // publish data cache line flush size to generic field, otherwise</span>
<span class="line-added"> 632     // let if default to zero thereby disabling writeback</span>
<span class="line-added"> 633     _data_cache_line_flush_size = _cpuid_info.std_cpuid1_ebx.bits.clflush_size * 8;</span>
<span class="line-added"> 634   }</span>
<span class="line-added"> 635 #endif</span>
 636   // If the OS doesn&#39;t support SSE, we can&#39;t use this feature even if the HW does
 637   if (!os::supports_sse())
 638     _features &amp;= ~(CPU_SSE|CPU_SSE2|CPU_SSE3|CPU_SSSE3|CPU_SSE4A|CPU_SSE4_1|CPU_SSE4_2);
 639 
 640   if (UseSSE &lt; 4) {
 641     _features &amp;= ~CPU_SSE4_1;
 642     _features &amp;= ~CPU_SSE4_2;
 643   }
 644 
 645   if (UseSSE &lt; 3) {
 646     _features &amp;= ~CPU_SSE3;
 647     _features &amp;= ~CPU_SSSE3;
 648     _features &amp;= ~CPU_SSE4A;
 649   }
 650 
 651   if (UseSSE &lt; 2)
 652     _features &amp;= ~CPU_SSE2;
 653 
 654   if (UseSSE &lt; 1)
 655     _features &amp;= ~CPU_SSE;
 656 
 657   //since AVX instructions is slower than SSE in some ZX cpus, force USEAVX=0.
 658   if (is_zx() &amp;&amp; ((cpu_family() == 6) || (cpu_family() == 7))) {
 659     UseAVX = 0;
 660   }
 661 
 662   // first try initial setting and detect what we can support
 663   int use_avx_limit = 0;
 664   if (UseAVX &gt; 0) {
 665     if (UseAVX &gt; 2 &amp;&amp; supports_evex()) {
 666       use_avx_limit = 3;
 667     } else if (UseAVX &gt; 1 &amp;&amp; supports_avx2()) {
 668       use_avx_limit = 2;
 669     } else if (UseAVX &gt; 0 &amp;&amp; supports_avx()) {
 670       use_avx_limit = 1;
 671     } else {
 672       use_avx_limit = 0;
 673     }
 674   }
 675   if (FLAG_IS_DEFAULT(UseAVX)) {
<span class="line-modified"> 676     // Don&#39;t use AVX-512 on older Skylakes unless explicitly requested.</span>
<span class="line-modified"> 677     if (use_avx_limit &gt; 2 &amp;&amp; is_intel_skylake() &amp;&amp; _stepping &lt; 5) {</span>
<span class="line-added"> 678       FLAG_SET_DEFAULT(UseAVX, 2);</span>
<span class="line-added"> 679     } else {</span>
<span class="line-added"> 680       FLAG_SET_DEFAULT(UseAVX, use_avx_limit);</span>
<span class="line-added"> 681     }</span>
<span class="line-added"> 682   }</span>
<span class="line-added"> 683   if (UseAVX &gt; use_avx_limit) {</span>
 684     warning(&quot;UseAVX=%d is not supported on this CPU, setting it to UseAVX=%d&quot;, (int) UseAVX, use_avx_limit);
 685     FLAG_SET_DEFAULT(UseAVX, use_avx_limit);
 686   } else if (UseAVX &lt; 0) {
 687     warning(&quot;UseAVX=%d is not valid, setting it to UseAVX=0&quot;, (int) UseAVX);
 688     FLAG_SET_DEFAULT(UseAVX, 0);
 689   }
 690 
 691   if (UseAVX &lt; 3) {
 692     _features &amp;= ~CPU_AVX512F;
 693     _features &amp;= ~CPU_AVX512DQ;
 694     _features &amp;= ~CPU_AVX512CD;
 695     _features &amp;= ~CPU_AVX512BW;
 696     _features &amp;= ~CPU_AVX512VL;
 697     _features &amp;= ~CPU_AVX512_VPOPCNTDQ;
<span class="line-modified"> 698     _features &amp;= ~CPU_AVX512_VPCLMULQDQ;</span>
 699     _features &amp;= ~CPU_VAES;
<span class="line-added"> 700     _features &amp;= ~CPU_VNNI;</span>
<span class="line-added"> 701     _features &amp;= ~CPU_VBMI2;</span>
 702   }
 703 
 704   if (UseAVX &lt; 2)
 705     _features &amp;= ~CPU_AVX2;
 706 
 707   if (UseAVX &lt; 1) {
 708     _features &amp;= ~CPU_AVX;
 709     _features &amp;= ~CPU_VZEROUPPER;
 710   }
 711 
 712   if (logical_processors_per_package() == 1) {
 713     // HT processor could be installed on a system which doesn&#39;t support HT.
 714     _features &amp;= ~CPU_HT;
 715   }
 716 
<span class="line-modified"> 717   if (is_intel()) { // Intel cpus specific settings</span>
 718     if (is_knights_family()) {
 719       _features &amp;= ~CPU_VZEROUPPER;
 720     }
 721   }
 722 
 723   char buf[256];
<span class="line-modified"> 724   jio_snprintf(buf, sizeof(buf), &quot;(%u cores per cpu, %u threads per core) family %d model %d stepping %d%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s%s&quot;,</span>
 725                cores_per_cpu(), threads_per_core(),
 726                cpu_family(), _model, _stepping,
 727                (supports_cmov() ? &quot;, cmov&quot; : &quot;&quot;),
 728                (supports_cmpxchg8() ? &quot;, cx8&quot; : &quot;&quot;),
 729                (supports_fxsr() ? &quot;, fxsr&quot; : &quot;&quot;),
 730                (supports_mmx()  ? &quot;, mmx&quot;  : &quot;&quot;),
 731                (supports_sse()  ? &quot;, sse&quot;  : &quot;&quot;),
 732                (supports_sse2() ? &quot;, sse2&quot; : &quot;&quot;),
 733                (supports_sse3() ? &quot;, sse3&quot; : &quot;&quot;),
 734                (supports_ssse3()? &quot;, ssse3&quot;: &quot;&quot;),
 735                (supports_sse4_1() ? &quot;, sse4.1&quot; : &quot;&quot;),
 736                (supports_sse4_2() ? &quot;, sse4.2&quot; : &quot;&quot;),
 737                (supports_popcnt() ? &quot;, popcnt&quot; : &quot;&quot;),
 738                (supports_avx()    ? &quot;, avx&quot; : &quot;&quot;),
 739                (supports_avx2()   ? &quot;, avx2&quot; : &quot;&quot;),
 740                (supports_aes()    ? &quot;, aes&quot; : &quot;&quot;),
 741                (supports_clmul()  ? &quot;, clmul&quot; : &quot;&quot;),
 742                (supports_erms()   ? &quot;, erms&quot; : &quot;&quot;),
 743                (supports_rtm()    ? &quot;, rtm&quot; : &quot;&quot;),
 744                (supports_mmx_ext() ? &quot;, mmxext&quot; : &quot;&quot;),
 745                (supports_3dnow_prefetch() ? &quot;, 3dnowpref&quot; : &quot;&quot;),
 746                (supports_lzcnt()   ? &quot;, lzcnt&quot;: &quot;&quot;),
 747                (supports_sse4a()   ? &quot;, sse4a&quot;: &quot;&quot;),
 748                (supports_ht() ? &quot;, ht&quot;: &quot;&quot;),
 749                (supports_tsc() ? &quot;, tsc&quot;: &quot;&quot;),
 750                (supports_tscinv_bit() ? &quot;, tscinvbit&quot;: &quot;&quot;),
 751                (supports_tscinv() ? &quot;, tscinv&quot;: &quot;&quot;),
 752                (supports_bmi1() ? &quot;, bmi1&quot; : &quot;&quot;),
 753                (supports_bmi2() ? &quot;, bmi2&quot; : &quot;&quot;),
 754                (supports_adx() ? &quot;, adx&quot; : &quot;&quot;),
 755                (supports_evex() ? &quot;, evex&quot; : &quot;&quot;),
 756                (supports_sha() ? &quot;, sha&quot; : &quot;&quot;),
<span class="line-modified"> 757                (supports_fma() ? &quot;, fma&quot; : &quot;&quot;),</span>
<span class="line-added"> 758                (supports_vbmi2() ? &quot;, vbmi2&quot; : &quot;&quot;),</span>
<span class="line-added"> 759                (supports_vaes() ? &quot;, vaes&quot; : &quot;&quot;),</span>
<span class="line-added"> 760                (supports_vnni() ? &quot;, vnni&quot; : &quot;&quot;));</span>
 761   _features_string = os::strdup(buf);
 762 
 763   // UseSSE is set to the smaller of what hardware supports and what
 764   // the command line requires.  I.e., you cannot set UseSSE to 2 on
 765   // older Pentiums which do not support it.
 766   int use_sse_limit = 0;
 767   if (UseSSE &gt; 0) {
 768     if (UseSSE &gt; 3 &amp;&amp; supports_sse4_1()) {
 769       use_sse_limit = 4;
 770     } else if (UseSSE &gt; 2 &amp;&amp; supports_sse3()) {
 771       use_sse_limit = 3;
 772     } else if (UseSSE &gt; 1 &amp;&amp; supports_sse2()) {
 773       use_sse_limit = 2;
 774     } else if (UseSSE &gt; 0 &amp;&amp; supports_sse()) {
 775       use_sse_limit = 1;
 776     } else {
 777       use_sse_limit = 0;
 778     }
 779   }
 780   if (FLAG_IS_DEFAULT(UseSSE)) {
</pre>
<hr />
<pre>
 801       if (UseSSE &gt; 2) {
 802         if (FLAG_IS_DEFAULT(UseAESIntrinsics)) {
 803           FLAG_SET_DEFAULT(UseAESIntrinsics, true);
 804         }
 805       } else {
 806         // The AES intrinsic stubs require AES instruction support (of course)
 807         // but also require sse3 mode or higher for instructions it use.
 808         if (UseAESIntrinsics &amp;&amp; !FLAG_IS_DEFAULT(UseAESIntrinsics)) {
 809           warning(&quot;X86 AES intrinsics require SSE3 instructions or higher. Intrinsics will be disabled.&quot;);
 810         }
 811         FLAG_SET_DEFAULT(UseAESIntrinsics, false);
 812       }
 813 
 814       // --AES-CTR begins--
 815       if (!UseAESIntrinsics) {
 816         if (UseAESCTRIntrinsics &amp;&amp; !FLAG_IS_DEFAULT(UseAESCTRIntrinsics)) {
 817           warning(&quot;AES-CTR intrinsics require UseAESIntrinsics flag to be enabled. Intrinsics will be disabled.&quot;);
 818           FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);
 819         }
 820       } else {
<span class="line-modified"> 821         if (supports_sse4_1()) {</span>
 822           if (FLAG_IS_DEFAULT(UseAESCTRIntrinsics)) {
 823             FLAG_SET_DEFAULT(UseAESCTRIntrinsics, true);
 824           }
 825         } else {
 826            // The AES-CTR intrinsic stubs require AES instruction support (of course)
 827            // but also require sse4.1 mode or higher for instructions it use.
 828           if (UseAESCTRIntrinsics &amp;&amp; !FLAG_IS_DEFAULT(UseAESCTRIntrinsics)) {
 829              warning(&quot;X86 AES-CTR intrinsics require SSE4.1 instructions or higher. Intrinsics will be disabled.&quot;);
 830            }
 831            FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);
 832         }
 833       }
 834       // --AES-CTR ends--
 835     }
 836   } else if (UseAES || UseAESIntrinsics || UseAESCTRIntrinsics) {
 837     if (UseAES &amp;&amp; !FLAG_IS_DEFAULT(UseAES)) {
 838       warning(&quot;AES instructions are not available on this CPU&quot;);
 839       FLAG_SET_DEFAULT(UseAES, false);
 840     }
 841     if (UseAESIntrinsics &amp;&amp; !FLAG_IS_DEFAULT(UseAESIntrinsics)) {
</pre>
<hr />
<pre>
 921   }
 922 
 923   if (supports_sha() &amp;&amp; supports_sse4_1() &amp;&amp; UseSHA) {
 924     if (FLAG_IS_DEFAULT(UseSHA1Intrinsics)) {
 925       FLAG_SET_DEFAULT(UseSHA1Intrinsics, true);
 926     }
 927   } else if (UseSHA1Intrinsics) {
 928     warning(&quot;Intrinsics for SHA-1 crypto hash functions not available on this CPU.&quot;);
 929     FLAG_SET_DEFAULT(UseSHA1Intrinsics, false);
 930   }
 931 
 932   if (supports_sse4_1() &amp;&amp; UseSHA) {
 933     if (FLAG_IS_DEFAULT(UseSHA256Intrinsics)) {
 934       FLAG_SET_DEFAULT(UseSHA256Intrinsics, true);
 935     }
 936   } else if (UseSHA256Intrinsics) {
 937     warning(&quot;Intrinsics for SHA-224 and SHA-256 crypto hash functions not available on this CPU.&quot;);
 938     FLAG_SET_DEFAULT(UseSHA256Intrinsics, false);
 939   }
 940 
<span class="line-added"> 941 #ifdef _LP64</span>
<span class="line-added"> 942   // These are only supported on 64-bit</span>
 943   if (UseSHA &amp;&amp; supports_avx2() &amp;&amp; supports_bmi2()) {
 944     if (FLAG_IS_DEFAULT(UseSHA512Intrinsics)) {
 945       FLAG_SET_DEFAULT(UseSHA512Intrinsics, true);
 946     }
<span class="line-modified"> 947   } else</span>
<span class="line-added"> 948 #endif</span>
<span class="line-added"> 949   if (UseSHA512Intrinsics) {</span>
 950     warning(&quot;Intrinsics for SHA-384 and SHA-512 crypto hash functions not available on this CPU.&quot;);
 951     FLAG_SET_DEFAULT(UseSHA512Intrinsics, false);
 952   }
 953 
 954   if (!(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics)) {
 955     FLAG_SET_DEFAULT(UseSHA, false);
 956   }
 957 
 958   if (UseAdler32Intrinsics) {
 959     warning(&quot;Adler32Intrinsics not available on this CPU.&quot;);
 960     FLAG_SET_DEFAULT(UseAdler32Intrinsics, false);
 961   }
 962 
 963   if (!supports_rtm() &amp;&amp; UseRTMLocking) {
 964     // Can&#39;t continue because UseRTMLocking affects UseBiasedLocking flag
 965     // setting during arguments processing. See use_biased_locking().
 966     // VM_Version_init() is executed after UseBiasedLocking is used
 967     // in Thread::allocate().
 968     vm_exit_during_initialization(&quot;RTM instructions are not available on this CPU&quot;);
 969   }
</pre>
<hr />
<pre>
1021   if (UseFPUForSpilling) {
1022     if (UseSSE &lt; 2) {
1023       // Only supported with SSE2+
1024       FLAG_SET_DEFAULT(UseFPUForSpilling, false);
1025     }
1026   }
1027 #endif
1028 
1029 #if COMPILER2_OR_JVMCI
1030   int max_vector_size = 0;
1031   if (UseSSE &lt; 2) {
1032     // Vectors (in XMM) are only supported with SSE2+
1033     // SSE is always 2 on x64.
1034     max_vector_size = 0;
1035   } else if (UseAVX == 0 || !os_supports_avx_vectors()) {
1036     // 16 byte vectors (in XMM) are supported with SSE2+
1037     max_vector_size = 16;
1038   } else if (UseAVX == 1 || UseAVX == 2) {
1039     // 32 bytes vectors (in YMM) are only supported with AVX+
1040     max_vector_size = 32;
<span class="line-modified">1041   } else if (UseAVX &gt; 2) {</span>
1042     // 64 bytes vectors (in ZMM) are only supported with AVX 3
1043     max_vector_size = 64;
1044   }
1045 
1046 #ifdef _LP64
1047   int min_vector_size = 4; // We require MaxVectorSize to be at least 4 on 64bit
1048 #else
1049   int min_vector_size = 0;
1050 #endif
1051 
1052   if (!FLAG_IS_DEFAULT(MaxVectorSize)) {
1053     if (MaxVectorSize &lt; min_vector_size) {
1054       warning(&quot;MaxVectorSize must be at least %i on this platform&quot;, min_vector_size);
1055       FLAG_SET_DEFAULT(MaxVectorSize, min_vector_size);
1056     }
1057     if (MaxVectorSize &gt; max_vector_size) {
1058       warning(&quot;MaxVectorSize must be at most %i on this platform&quot;, max_vector_size);
1059       FLAG_SET_DEFAULT(MaxVectorSize, max_vector_size);
1060     }
1061     if (!is_power_of_2(MaxVectorSize)) {
</pre>
<hr />
<pre>
1067     FLAG_SET_DEFAULT(MaxVectorSize, max_vector_size);
1068   }
1069 
1070 #if defined(COMPILER2) &amp;&amp; defined(ASSERT)
1071   if (MaxVectorSize &gt; 0) {
1072     if (supports_avx() &amp;&amp; PrintMiscellaneous &amp;&amp; Verbose &amp;&amp; TraceNewVectors) {
1073       tty-&gt;print_cr(&quot;State of YMM registers after signal handle:&quot;);
1074       int nreg = 2 LP64_ONLY(+2);
1075       const char* ymm_name[4] = {&quot;0&quot;, &quot;7&quot;, &quot;8&quot;, &quot;15&quot;};
1076       for (int i = 0; i &lt; nreg; i++) {
1077         tty-&gt;print(&quot;YMM%s:&quot;, ymm_name[i]);
1078         for (int j = 7; j &gt;=0; j--) {
1079           tty-&gt;print(&quot; %x&quot;, _cpuid_info.ymm_save[i*8 + j]);
1080         }
1081         tty-&gt;cr();
1082       }
1083     }
1084   }
1085 #endif // COMPILER2 &amp;&amp; ASSERT
1086 
<span class="line-added">1087   if (!FLAG_IS_DEFAULT(AVX3Threshold)) {</span>
<span class="line-added">1088     if (!is_power_of_2(AVX3Threshold)) {</span>
<span class="line-added">1089       warning(&quot;AVX3Threshold must be a power of 2&quot;);</span>
<span class="line-added">1090       FLAG_SET_DEFAULT(AVX3Threshold, 4096);</span>
<span class="line-added">1091     }</span>
<span class="line-added">1092   }</span>
<span class="line-added">1093 </span>
1094 #ifdef _LP64
1095   if (FLAG_IS_DEFAULT(UseMultiplyToLenIntrinsic)) {
1096     UseMultiplyToLenIntrinsic = true;
1097   }
1098   if (FLAG_IS_DEFAULT(UseSquareToLenIntrinsic)) {
1099     UseSquareToLenIntrinsic = true;
1100   }
1101   if (FLAG_IS_DEFAULT(UseMulAddIntrinsic)) {
1102     UseMulAddIntrinsic = true;
1103   }
1104   if (FLAG_IS_DEFAULT(UseMontgomeryMultiplyIntrinsic)) {
1105     UseMontgomeryMultiplyIntrinsic = true;
1106   }
1107   if (FLAG_IS_DEFAULT(UseMontgomerySquareIntrinsic)) {
1108     UseMontgomerySquareIntrinsic = true;
1109   }
1110 #else
1111   if (UseMultiplyToLenIntrinsic) {
1112     if (!FLAG_IS_DEFAULT(UseMultiplyToLenIntrinsic)) {
1113       warning(&quot;multiplyToLen intrinsic is not available in 32-bit VM&quot;);
</pre>
<hr />
<pre>
1192           UseUnalignedLoadStores = true; // use movdqu on newest ZX cpus
1193         }
1194       }
1195       if (supports_sse4_2()) {
1196         if (FLAG_IS_DEFAULT(UseSSE42Intrinsics)) {
1197           FLAG_SET_DEFAULT(UseSSE42Intrinsics, true);
1198         }
1199       } else {
1200         if (UseSSE42Intrinsics &amp;&amp; !FLAG_IS_DEFAULT(UseAESIntrinsics)) {
1201           warning(&quot;SSE4.2 intrinsics require SSE4.2 instructions or higher. Intrinsics will be disabled.&quot;);
1202         }
1203         FLAG_SET_DEFAULT(UseSSE42Intrinsics, false);
1204       }
1205     }
1206 
1207     if (FLAG_IS_DEFAULT(AllocatePrefetchInstr) &amp;&amp; supports_3dnow_prefetch()) {
1208       FLAG_SET_DEFAULT(AllocatePrefetchInstr, 3);
1209     }
1210   }
1211 
<span class="line-modified">1212   if (is_amd_family()) { // AMD cpus specific settings</span>
<span class="line-modified">1213     if (supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseAddressNop)) {</span>
1214       // Use it on new AMD cpus starting from Opteron.
1215       UseAddressNop = true;
1216     }
<span class="line-modified">1217     if (supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseNewLongLShift)) {</span>
1218       // Use it on new AMD cpus starting from Opteron.
1219       UseNewLongLShift = true;
1220     }
<span class="line-modified">1221     if (FLAG_IS_DEFAULT(UseXmmLoadAndClearUpper)) {</span>
1222       if (supports_sse4a()) {
1223         UseXmmLoadAndClearUpper = true; // use movsd only on &#39;10h&#39; Opteron
1224       } else {
1225         UseXmmLoadAndClearUpper = false;
1226       }
1227     }
<span class="line-modified">1228     if (FLAG_IS_DEFAULT(UseXmmRegToRegMoveAll)) {</span>
<span class="line-modified">1229       if (supports_sse4a()) {</span>
1230         UseXmmRegToRegMoveAll = true; // use movaps, movapd only on &#39;10h&#39;
1231       } else {
1232         UseXmmRegToRegMoveAll = false;
1233       }
1234     }
<span class="line-modified">1235     if (FLAG_IS_DEFAULT(UseXmmI2F)) {</span>
<span class="line-modified">1236       if (supports_sse4a()) {</span>
1237         UseXmmI2F = true;
1238       } else {
1239         UseXmmI2F = false;
1240       }
1241     }
<span class="line-modified">1242     if (FLAG_IS_DEFAULT(UseXmmI2D)) {</span>
<span class="line-modified">1243       if (supports_sse4a()) {</span>
1244         UseXmmI2D = true;
1245       } else {
1246         UseXmmI2D = false;
1247       }
1248     }
1249     if (supports_sse4_2()) {
1250       if (FLAG_IS_DEFAULT(UseSSE42Intrinsics)) {
1251         FLAG_SET_DEFAULT(UseSSE42Intrinsics, true);
1252       }
1253     } else {
1254       if (UseSSE42Intrinsics &amp;&amp; !FLAG_IS_DEFAULT(UseAESIntrinsics)) {
1255         warning(&quot;SSE4.2 intrinsics require SSE4.2 instructions or higher. Intrinsics will be disabled.&quot;);
1256       }
1257       FLAG_SET_DEFAULT(UseSSE42Intrinsics, false);
1258     }
1259 
1260     // some defaults for AMD family 15h
<span class="line-modified">1261     if (cpu_family() == 0x15) {</span>
1262       // On family 15h processors default is no sw prefetch
1263       if (FLAG_IS_DEFAULT(AllocatePrefetchStyle)) {
1264         FLAG_SET_DEFAULT(AllocatePrefetchStyle, 0);
1265       }
1266       // Also, if some other prefetch style is specified, default instruction type is PREFETCHW
1267       if (FLAG_IS_DEFAULT(AllocatePrefetchInstr)) {
1268         FLAG_SET_DEFAULT(AllocatePrefetchInstr, 3);
1269       }
1270       // On family 15h processors use XMM and UnalignedLoadStores for Array Copy
1271       if (supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseXMMForArrayCopy)) {
1272         FLAG_SET_DEFAULT(UseXMMForArrayCopy, true);
1273       }
1274       if (supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseUnalignedLoadStores)) {
1275         FLAG_SET_DEFAULT(UseUnalignedLoadStores, true);
1276       }
1277     }
1278 
1279 #ifdef COMPILER2
1280     if (cpu_family() &lt; 0x17 &amp;&amp; MaxVectorSize &gt; 16) {
1281       // Limit vectors size to 16 bytes on AMD cpus &lt; 17h.
1282       FLAG_SET_DEFAULT(MaxVectorSize, 16);
1283     }
1284 #endif // COMPILER2
1285 
<span class="line-modified">1286     // Some defaults for AMD family 17h || Hygon family 18h</span>
<span class="line-modified">1287     if (cpu_family() == 0x17 || cpu_family() == 0x18) {</span>
1288       // On family 17h processors use XMM and UnalignedLoadStores for Array Copy
1289       if (supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseXMMForArrayCopy)) {
1290         FLAG_SET_DEFAULT(UseXMMForArrayCopy, true);
1291       }
1292       if (supports_sse2() &amp;&amp; FLAG_IS_DEFAULT(UseUnalignedLoadStores)) {
1293         FLAG_SET_DEFAULT(UseUnalignedLoadStores, true);
1294       }
1295 #ifdef COMPILER2
1296       if (supports_sse4_2() &amp;&amp; FLAG_IS_DEFAULT(UseFPUForSpilling)) {
1297         FLAG_SET_DEFAULT(UseFPUForSpilling, true);
1298       }
1299 #endif
1300     }
1301   }
1302 
<span class="line-modified">1303   if (is_intel()) { // Intel cpus specific settings</span>
<span class="line-modified">1304     if (FLAG_IS_DEFAULT(UseStoreImmI16)) {</span>
1305       UseStoreImmI16 = false; // don&#39;t use it on Intel cpus
1306     }
<span class="line-modified">1307     if (cpu_family() == 6 || cpu_family() == 15) {</span>
<span class="line-modified">1308       if (FLAG_IS_DEFAULT(UseAddressNop)) {</span>
1309         // Use it on all Intel cpus starting from PentiumPro
1310         UseAddressNop = true;
1311       }
1312     }
<span class="line-modified">1313     if (FLAG_IS_DEFAULT(UseXmmLoadAndClearUpper)) {</span>
1314       UseXmmLoadAndClearUpper = true; // use movsd on all Intel cpus
1315     }
<span class="line-modified">1316     if (FLAG_IS_DEFAULT(UseXmmRegToRegMoveAll)) {</span>
<span class="line-modified">1317       if (supports_sse3()) {</span>
1318         UseXmmRegToRegMoveAll = true; // use movaps, movapd on new Intel cpus
1319       } else {
1320         UseXmmRegToRegMoveAll = false;
1321       }
1322     }
<span class="line-modified">1323     if (cpu_family() == 6 &amp;&amp; supports_sse3()) { // New Intel cpus</span>
1324 #ifdef COMPILER2
<span class="line-modified">1325       if (FLAG_IS_DEFAULT(MaxLoopPad)) {</span>
1326         // For new Intel cpus do the next optimization:
1327         // don&#39;t align the beginning of a loop if there are enough instructions
1328         // left (NumberOfLoopInstrToAlign defined in c2_globals.hpp)
1329         // in current fetch line (OptoLoopAlignment) or the padding
1330         // is big (&gt; MaxLoopPad).
1331         // Set MaxLoopPad to 11 for new Intel cpus to reduce number of
1332         // generated NOP instructions. 11 is the largest size of one
1333         // address NOP instruction &#39;0F 1F&#39; (see Assembler::nop(i)).
1334         MaxLoopPad = 11;
1335       }
1336 #endif // COMPILER2
1337       if (FLAG_IS_DEFAULT(UseXMMForArrayCopy)) {
1338         UseXMMForArrayCopy = true; // use SSE2 movq on new Intel cpus
1339       }
1340       if ((supports_sse4_2() &amp;&amp; supports_ht()) || supports_avx()) { // Newest Intel cpus
1341         if (FLAG_IS_DEFAULT(UseUnalignedLoadStores)) {
1342           UseUnalignedLoadStores = true; // use movdqu on newest Intel cpus
1343         }
1344       }
1345       if (supports_sse4_2()) {
</pre>
<hr />
<pre>
1351           warning(&quot;SSE4.2 intrinsics require SSE4.2 instructions or higher. Intrinsics will be disabled.&quot;);
1352         }
1353         FLAG_SET_DEFAULT(UseSSE42Intrinsics, false);
1354       }
1355     }
1356     if (is_atom_family() || is_knights_family()) {
1357 #ifdef COMPILER2
1358       if (FLAG_IS_DEFAULT(OptoScheduling)) {
1359         OptoScheduling = true;
1360       }
1361 #endif
1362       if (supports_sse4_2()) { // Silvermont
1363         if (FLAG_IS_DEFAULT(UseUnalignedLoadStores)) {
1364           UseUnalignedLoadStores = true; // use movdqu on newest Intel cpus
1365         }
1366       }
1367       if (FLAG_IS_DEFAULT(UseIncDec)) {
1368         FLAG_SET_DEFAULT(UseIncDec, false);
1369       }
1370     }
<span class="line-modified">1371     if (FLAG_IS_DEFAULT(AllocatePrefetchInstr) &amp;&amp; supports_3dnow_prefetch()) {</span>
1372       FLAG_SET_DEFAULT(AllocatePrefetchInstr, 3);
1373     }
1374   }
1375 
1376 #ifdef _LP64
1377   if (UseSSE42Intrinsics) {
1378     if (FLAG_IS_DEFAULT(UseVectorizedMismatchIntrinsic)) {
1379       UseVectorizedMismatchIntrinsic = true;
1380     }
1381   } else if (UseVectorizedMismatchIntrinsic) {
1382     if (!FLAG_IS_DEFAULT(UseVectorizedMismatchIntrinsic))
1383       warning(&quot;vectorizedMismatch intrinsics are not available on this CPU&quot;);
1384     FLAG_SET_DEFAULT(UseVectorizedMismatchIntrinsic, false);
1385   }
1386 #else
1387   if (UseVectorizedMismatchIntrinsic) {
1388     if (!FLAG_IS_DEFAULT(UseVectorizedMismatchIntrinsic)) {
1389       warning(&quot;vectorizedMismatch intrinsic is not available in 32-bit VM&quot;);
1390     }
1391     FLAG_SET_DEFAULT(UseVectorizedMismatchIntrinsic, false);
</pre>
<hr />
<pre>
1600         log-&gt;print_cr(&quot; at distance %d, one line of %d bytes&quot;, (int) AllocatePrefetchDistance, (int) AllocatePrefetchStepSize);
1601       }
1602     }
1603 
1604     if (PrefetchCopyIntervalInBytes &gt; 0) {
1605       log-&gt;print_cr(&quot;PrefetchCopyIntervalInBytes %d&quot;, (int) PrefetchCopyIntervalInBytes);
1606     }
1607     if (PrefetchScanIntervalInBytes &gt; 0) {
1608       log-&gt;print_cr(&quot;PrefetchScanIntervalInBytes %d&quot;, (int) PrefetchScanIntervalInBytes);
1609     }
1610     if (PrefetchFieldsAhead &gt; 0) {
1611       log-&gt;print_cr(&quot;PrefetchFieldsAhead %d&quot;, (int) PrefetchFieldsAhead);
1612     }
1613     if (ContendedPaddingWidth &gt; 0) {
1614       log-&gt;print_cr(&quot;ContendedPaddingWidth %d&quot;, (int) ContendedPaddingWidth);
1615     }
1616   }
1617 #endif // !PRODUCT
1618 }
1619 
<span class="line-added">1620 void VM_Version::print_platform_virtualization_info(outputStream* st) {</span>
<span class="line-added">1621   VirtualizationType vrt = VM_Version::get_detected_virtualization();</span>
<span class="line-added">1622   if (vrt == XenHVM) {</span>
<span class="line-added">1623     st-&gt;print_cr(&quot;Xen hardware-assisted virtualization detected&quot;);</span>
<span class="line-added">1624   } else if (vrt == KVM) {</span>
<span class="line-added">1625     st-&gt;print_cr(&quot;KVM virtualization detected&quot;);</span>
<span class="line-added">1626   } else if (vrt == VMWare) {</span>
<span class="line-added">1627     st-&gt;print_cr(&quot;VMWare virtualization detected&quot;);</span>
<span class="line-added">1628     VirtualizationSupport::print_virtualization_info(st);</span>
<span class="line-added">1629   } else if (vrt == HyperV) {</span>
<span class="line-added">1630     st-&gt;print_cr(&quot;HyperV virtualization detected&quot;);</span>
<span class="line-added">1631   }</span>
<span class="line-added">1632 }</span>
<span class="line-added">1633 </span>
<span class="line-added">1634 void VM_Version::check_virt_cpuid(uint32_t idx, uint32_t *regs) {</span>
<span class="line-added">1635 // TODO support 32 bit</span>
<span class="line-added">1636 #if defined(_LP64)</span>
<span class="line-added">1637 #if defined(_MSC_VER)</span>
<span class="line-added">1638   // Allocate space for the code</span>
<span class="line-added">1639   const int code_size = 100;</span>
<span class="line-added">1640   ResourceMark rm;</span>
<span class="line-added">1641   CodeBuffer cb(&quot;detect_virt&quot;, code_size, 0);</span>
<span class="line-added">1642   MacroAssembler* a = new MacroAssembler(&amp;cb);</span>
<span class="line-added">1643   address code = a-&gt;pc();</span>
<span class="line-added">1644   void (*test)(uint32_t idx, uint32_t *regs) = (void(*)(uint32_t idx, uint32_t *regs))code;</span>
<span class="line-added">1645 </span>
<span class="line-added">1646   a-&gt;movq(r9, rbx); // save nonvolatile register</span>
<span class="line-added">1647 </span>
<span class="line-added">1648   // next line would not work on 32-bit</span>
<span class="line-added">1649   a-&gt;movq(rax, c_rarg0 /* rcx */);</span>
<span class="line-added">1650   a-&gt;movq(r8, c_rarg1 /* rdx */);</span>
<span class="line-added">1651   a-&gt;cpuid();</span>
<span class="line-added">1652   a-&gt;movl(Address(r8,  0), rax);</span>
<span class="line-added">1653   a-&gt;movl(Address(r8,  4), rbx);</span>
<span class="line-added">1654   a-&gt;movl(Address(r8,  8), rcx);</span>
<span class="line-added">1655   a-&gt;movl(Address(r8, 12), rdx);</span>
<span class="line-added">1656 </span>
<span class="line-added">1657   a-&gt;movq(rbx, r9); // restore nonvolatile register</span>
<span class="line-added">1658   a-&gt;ret(0);</span>
<span class="line-added">1659 </span>
<span class="line-added">1660   uint32_t *code_end = (uint32_t *)a-&gt;pc();</span>
<span class="line-added">1661   a-&gt;flush();</span>
<span class="line-added">1662 </span>
<span class="line-added">1663   // execute code</span>
<span class="line-added">1664   (*test)(idx, regs);</span>
<span class="line-added">1665 #elif defined(__GNUC__)</span>
<span class="line-added">1666   __asm__ volatile (</span>
<span class="line-added">1667      &quot;        cpuid;&quot;</span>
<span class="line-added">1668      &quot;        mov %%eax,(%1);&quot;</span>
<span class="line-added">1669      &quot;        mov %%ebx,4(%1);&quot;</span>
<span class="line-added">1670      &quot;        mov %%ecx,8(%1);&quot;</span>
<span class="line-added">1671      &quot;        mov %%edx,12(%1);&quot;</span>
<span class="line-added">1672      : &quot;+a&quot; (idx)</span>
<span class="line-added">1673      : &quot;S&quot; (regs)</span>
<span class="line-added">1674      : &quot;ebx&quot;, &quot;ecx&quot;, &quot;edx&quot;, &quot;memory&quot; );</span>
<span class="line-added">1675 #endif</span>
<span class="line-added">1676 #endif</span>
<span class="line-added">1677 }</span>
<span class="line-added">1678 </span>
<span class="line-added">1679 </span>
1680 bool VM_Version::use_biased_locking() {
1681 #if INCLUDE_RTM_OPT
1682   // RTM locking is most useful when there is high lock contention and
1683   // low data contention.  With high lock contention the lock is usually
1684   // inflated and biased locking is not suitable for that case.
1685   // RTM locking code requires that biased locking is off.
1686   // Note: we can&#39;t switch off UseBiasedLocking in get_processor_features()
1687   // because it is used by Thread::allocate() which is called before
1688   // VM_Version::initialize().
1689   if (UseRTMLocking &amp;&amp; UseBiasedLocking) {
1690     if (FLAG_IS_DEFAULT(UseBiasedLocking)) {
1691       FLAG_SET_DEFAULT(UseBiasedLocking, false);
1692     } else {
1693       warning(&quot;Biased locking is not supported with RTM locking; ignoring UseBiasedLocking flag.&quot; );
1694       UseBiasedLocking = false;
1695     }
1696   }
1697 #endif
1698   return UseBiasedLocking;
1699 }
1700 
<span class="line-added">1701 // On Xen, the cpuid instruction returns</span>
<span class="line-added">1702 //  eax / registers[0]: Version of Xen</span>
<span class="line-added">1703 //  ebx / registers[1]: chars &#39;XenV&#39;</span>
<span class="line-added">1704 //  ecx / registers[2]: chars &#39;MMXe&#39;</span>
<span class="line-added">1705 //  edx / registers[3]: chars &#39;nVMM&#39;</span>
<span class="line-added">1706 //</span>
<span class="line-added">1707 // On KVM / VMWare / MS Hyper-V, the cpuid instruction returns</span>
<span class="line-added">1708 //  ebx / registers[1]: chars &#39;KVMK&#39; / &#39;VMwa&#39; / &#39;Micr&#39;</span>
<span class="line-added">1709 //  ecx / registers[2]: chars &#39;VMKV&#39; / &#39;reVM&#39; / &#39;osof&#39;</span>
<span class="line-added">1710 //  edx / registers[3]: chars &#39;M&#39;    / &#39;ware&#39; / &#39;t Hv&#39;</span>
<span class="line-added">1711 //</span>
<span class="line-added">1712 // more information :</span>
<span class="line-added">1713 // https://kb.vmware.com/s/article/1009458</span>
<span class="line-added">1714 //</span>
<span class="line-added">1715 void VM_Version::check_virtualizations() {</span>
<span class="line-added">1716 #if defined(_LP64)</span>
<span class="line-added">1717   uint32_t registers[4];</span>
<span class="line-added">1718   char signature[13];</span>
<span class="line-added">1719   uint32_t base;</span>
<span class="line-added">1720   signature[12] = &#39;\0&#39;;</span>
<span class="line-added">1721   memset((void*)registers, 0, 4*sizeof(uint32_t));</span>
<span class="line-added">1722 </span>
<span class="line-added">1723   for (base = 0x40000000; base &lt; 0x40010000; base += 0x100) {</span>
<span class="line-added">1724     check_virt_cpuid(base, registers);</span>
<span class="line-added">1725 </span>
<span class="line-added">1726     *(uint32_t *)(signature + 0) = registers[1];</span>
<span class="line-added">1727     *(uint32_t *)(signature + 4) = registers[2];</span>
<span class="line-added">1728     *(uint32_t *)(signature + 8) = registers[3];</span>
<span class="line-added">1729 </span>
<span class="line-added">1730     if (strncmp(&quot;VMwareVMware&quot;, signature, 12) == 0) {</span>
<span class="line-added">1731       Abstract_VM_Version::_detected_virtualization = VMWare;</span>
<span class="line-added">1732       // check for extended metrics from guestlib</span>
<span class="line-added">1733       VirtualizationSupport::initialize();</span>
<span class="line-added">1734     }</span>
<span class="line-added">1735 </span>
<span class="line-added">1736     if (strncmp(&quot;Microsoft Hv&quot;, signature, 12) == 0) {</span>
<span class="line-added">1737       Abstract_VM_Version::_detected_virtualization = HyperV;</span>
<span class="line-added">1738     }</span>
<span class="line-added">1739 </span>
<span class="line-added">1740     if (strncmp(&quot;KVMKVMKVM&quot;, signature, 9) == 0) {</span>
<span class="line-added">1741       Abstract_VM_Version::_detected_virtualization = KVM;</span>
<span class="line-added">1742     }</span>
<span class="line-added">1743 </span>
<span class="line-added">1744     if (strncmp(&quot;XenVMMXenVMM&quot;, signature, 12) == 0) {</span>
<span class="line-added">1745       Abstract_VM_Version::_detected_virtualization = XenHVM;</span>
<span class="line-added">1746     }</span>
<span class="line-added">1747   }</span>
<span class="line-added">1748 #endif</span>
<span class="line-added">1749 }</span>
<span class="line-added">1750 </span>
1751 void VM_Version::initialize() {
1752   ResourceMark rm;
1753   // Making this stub must be FIRST use of assembler
1754 
1755   stub_blob = BufferBlob::create(&quot;get_cpu_info_stub&quot;, stub_size);
1756   if (stub_blob == NULL) {
1757     vm_exit_during_initialization(&quot;Unable to allocate get_cpu_info_stub&quot;);
1758   }
1759   CodeBuffer c(stub_blob);
1760   VM_Version_StubGenerator g(&amp;c);
1761   get_cpu_info_stub = CAST_TO_FN_PTR(get_cpu_info_stub_t,
1762                                      g.generate_get_cpu_info());
1763 
1764   get_processor_features();
<span class="line-added">1765   if (cpu_family() &gt; 4) { // it supports CPUID</span>
<span class="line-added">1766     check_virtualizations();</span>
<span class="line-added">1767   }</span>
1768 }
</pre>
</td>
</tr>
</table>
<center><a href="vm_version_ext_x86.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="vm_version_x86.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>