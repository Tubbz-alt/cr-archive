<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/macroAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="jvmciCodeInstaller_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_x86.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/macroAssembler_x86.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
    1 /*
<span class="line-modified">    2  * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.</span>
    3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
    4  *
    5  * This code is free software; you can redistribute it and/or modify it
    6  * under the terms of the GNU General Public License version 2 only, as
    7  * published by the Free Software Foundation.
    8  *
    9  * This code is distributed in the hope that it will be useful, but WITHOUT
   10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
   11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
   12  * version 2 for more details (a copy is included in the LICENSE file that
   13  * accompanied this code).
   14  *
   15  * You should have received a copy of the GNU General Public License version
   16  * 2 along with this work; if not, write to the Free Software Foundation,
   17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
   18  *
   19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
   20  * or visit www.oracle.com if you need additional information or have any
   21  * questions.
   22  *
   23  */
   24 
   25 #include &quot;precompiled.hpp&quot;
   26 #include &quot;jvm.h&quot;
   27 #include &quot;asm/assembler.hpp&quot;
   28 #include &quot;asm/assembler.inline.hpp&quot;
   29 #include &quot;compiler/disassembler.hpp&quot;
   30 #include &quot;gc/shared/barrierSet.hpp&quot;
   31 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
   32 #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
   33 #include &quot;interpreter/interpreter.hpp&quot;
   34 #include &quot;memory/resourceArea.hpp&quot;
   35 #include &quot;memory/universe.hpp&quot;
   36 #include &quot;oops/accessDecorators.hpp&quot;

   37 #include &quot;oops/klass.inline.hpp&quot;
   38 #include &quot;prims/methodHandles.hpp&quot;
   39 #include &quot;runtime/biasedLocking.hpp&quot;
   40 #include &quot;runtime/flags/flagSetting.hpp&quot;
   41 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
   42 #include &quot;runtime/objectMonitor.hpp&quot;
   43 #include &quot;runtime/os.hpp&quot;
   44 #include &quot;runtime/safepoint.hpp&quot;
   45 #include &quot;runtime/safepointMechanism.hpp&quot;
   46 #include &quot;runtime/sharedRuntime.hpp&quot;
   47 #include &quot;runtime/stubRoutines.hpp&quot;
   48 #include &quot;runtime/thread.hpp&quot;
   49 #include &quot;utilities/macros.hpp&quot;
   50 #include &quot;crc32c.h&quot;
   51 #ifdef COMPILER2
   52 #include &quot;opto/intrinsicnode.hpp&quot;
   53 #endif
   54 
   55 #ifdef PRODUCT
   56 #define BLOCK_COMMENT(str) /* nothing */
</pre>
<hr />
<pre>
  331   movl(as_Address(dst), src);
  332 }
  333 
  334 void MacroAssembler::movptr(Register dst, ArrayAddress src) {
  335   movl(dst, as_Address(src));
  336 }
  337 
  338 // src should NEVER be a real pointer. Use AddressLiteral for true pointers
  339 void MacroAssembler::movptr(Address dst, intptr_t src) {
  340   movl(dst, src);
  341 }
  342 
  343 
  344 void MacroAssembler::pop_callee_saved_registers() {
  345   pop(rcx);
  346   pop(rdx);
  347   pop(rdi);
  348   pop(rsi);
  349 }
  350 
<span class="line-removed">  351 void MacroAssembler::pop_fTOS() {</span>
<span class="line-removed">  352   fld_d(Address(rsp, 0));</span>
<span class="line-removed">  353   addl(rsp, 2 * wordSize);</span>
<span class="line-removed">  354 }</span>
<span class="line-removed">  355 </span>
  356 void MacroAssembler::push_callee_saved_registers() {
  357   push(rsi);
  358   push(rdi);
  359   push(rdx);
  360   push(rcx);
  361 }
  362 
<span class="line-removed">  363 void MacroAssembler::push_fTOS() {</span>
<span class="line-removed">  364   subl(rsp, 2 * wordSize);</span>
<span class="line-removed">  365   fstp_d(Address(rsp, 0));</span>
<span class="line-removed">  366 }</span>
<span class="line-removed">  367 </span>
<span class="line-removed">  368 </span>
  369 void MacroAssembler::pushoop(jobject obj) {
  370   push_literal32((int32_t)obj, oop_Relocation::spec_for_immediate());
  371 }
  372 
  373 void MacroAssembler::pushklass(Metadata* obj) {
  374   push_literal32((int32_t)obj, metadata_Relocation::spec_for_immediate());
  375 }
  376 
  377 void MacroAssembler::pushptr(AddressLiteral src) {
  378   if (src.is_lval()) {
  379     push_literal32((int32_t)src.target(), src.rspec());
  380   } else {
  381     pushl(as_Address(src));
  382   }
  383 }
  384 
  385 void MacroAssembler::set_word_if_not_zero(Register dst) {
  386   xorl(dst, dst);
  387   set_byte_if_not_zero(dst);
  388 }
</pre>
<hr />
<pre>
  409 
  410 void MacroAssembler::debug32(int rdi, int rsi, int rbp, int rsp, int rbx, int rdx, int rcx, int rax, int eip, char* msg) {
  411   // In order to get locks to work, we need to fake a in_VM state
  412   JavaThread* thread = JavaThread::current();
  413   JavaThreadState saved_state = thread-&gt;thread_state();
  414   thread-&gt;set_thread_state(_thread_in_vm);
  415   if (ShowMessageBoxOnError) {
  416     JavaThread* thread = JavaThread::current();
  417     JavaThreadState saved_state = thread-&gt;thread_state();
  418     thread-&gt;set_thread_state(_thread_in_vm);
  419     if (CountBytecodes || TraceBytecodes || StopInterpreterAt) {
  420       ttyLocker ttyl;
  421       BytecodeCounter::print();
  422     }
  423     // To see where a verify_oop failed, get $ebx+40/X for this frame.
  424     // This is the value of eip which points to where verify_oop will return.
  425     if (os::message_box(msg, &quot;Execution stopped, print registers?&quot;)) {
  426       print_state32(rdi, rsi, rbp, rsp, rbx, rdx, rcx, rax, eip);
  427       BREAKPOINT;
  428     }
<span class="line-removed">  429   } else {</span>
<span class="line-removed">  430     ttyLocker ttyl;</span>
<span class="line-removed">  431     ::tty-&gt;print_cr(&quot;=============== DEBUG MESSAGE: %s ================\n&quot;, msg);</span>
  432   }
<span class="line-modified">  433   // Don&#39;t assert holding the ttyLock</span>
<span class="line-removed">  434     assert(false, &quot;DEBUG MESSAGE: %s&quot;, msg);</span>
<span class="line-removed">  435   ThreadStateTransition::transition(thread, _thread_in_vm, saved_state);</span>
  436 }
  437 
  438 void MacroAssembler::print_state32(int rdi, int rsi, int rbp, int rsp, int rbx, int rdx, int rcx, int rax, int eip) {
  439   ttyLocker ttyl;
  440   FlagSetting fs(Debugging, true);
  441   tty-&gt;print_cr(&quot;eip = 0x%08x&quot;, eip);
  442 #ifndef PRODUCT
  443   if ((WizardMode || Verbose) &amp;&amp; PrintMiscellaneous) {
  444     tty-&gt;cr();
  445     findpc(eip);
  446     tty-&gt;cr();
  447   }
  448 #endif
  449 #define PRINT_REG(rax) \
  450   { tty-&gt;print(&quot;%s = &quot;, #rax); os::print_location(tty, rax); }
  451   PRINT_REG(rax);
  452   PRINT_REG(rbx);
  453   PRINT_REG(rcx);
  454   PRINT_REG(rdx);
  455   PRINT_REG(rdi);
</pre>
<hr />
<pre>
  811 
  812 static void pass_arg1(MacroAssembler* masm, Register arg) {
  813   if (c_rarg1 != arg ) {
  814     masm-&gt;mov(c_rarg1, arg);
  815   }
  816 }
  817 
  818 static void pass_arg2(MacroAssembler* masm, Register arg) {
  819   if (c_rarg2 != arg ) {
  820     masm-&gt;mov(c_rarg2, arg);
  821   }
  822 }
  823 
  824 static void pass_arg3(MacroAssembler* masm, Register arg) {
  825   if (c_rarg3 != arg ) {
  826     masm-&gt;mov(c_rarg3, arg);
  827   }
  828 }
  829 
  830 void MacroAssembler::stop(const char* msg) {
<span class="line-modified">  831   address rip = pc();</span>
<span class="line-modified">  832   pusha(); // get regs on stack</span>




  833   lea(c_rarg0, ExternalAddress((address) msg));
<span class="line-removed">  834   lea(c_rarg1, InternalAddress(rip));</span>
<span class="line-removed">  835   movq(c_rarg2, rsp); // pass pointer to regs array</span>
  836   andq(rsp, -16); // align stack as required by ABI
  837   call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug64)));
  838   hlt();
  839 }
  840 
  841 void MacroAssembler::warn(const char* msg) {
  842   push(rbp);
  843   movq(rbp, rsp);
  844   andq(rsp, -16);     // align stack as required by push_CPU_state and call
  845   push_CPU_state();   // keeps alignment at 16 bytes
  846   lea(c_rarg0, ExternalAddress((address) msg));
  847   lea(rax, ExternalAddress(CAST_FROM_FN_PTR(address, warning)));
  848   call(rax);
  849   pop_CPU_state();
  850   mov(rsp, rbp);
  851   pop(rbp);
  852 }
  853 
  854 void MacroAssembler::print_state() {
  855   address rip = pc();
</pre>
<hr />
<pre>
  874 #endif
  875 
  876 void MacroAssembler::debug64(char* msg, int64_t pc, int64_t regs[]) {
  877   // In order to get locks to work, we need to fake a in_VM state
  878   if (ShowMessageBoxOnError) {
  879     JavaThread* thread = JavaThread::current();
  880     JavaThreadState saved_state = thread-&gt;thread_state();
  881     thread-&gt;set_thread_state(_thread_in_vm);
  882 #ifndef PRODUCT
  883     if (CountBytecodes || TraceBytecodes || StopInterpreterAt) {
  884       ttyLocker ttyl;
  885       BytecodeCounter::print();
  886     }
  887 #endif
  888     // To see where a verify_oop failed, get $ebx+40/X for this frame.
  889     // XXX correct this offset for amd64
  890     // This is the value of eip which points to where verify_oop will return.
  891     if (os::message_box(msg, &quot;Execution stopped, print registers?&quot;)) {
  892       print_state64(pc, regs);
  893       BREAKPOINT;
<span class="line-removed">  894       assert(false, &quot;start up GDB&quot;);</span>
  895     }
<span class="line-removed">  896     ThreadStateTransition::transition(thread, _thread_in_vm, saved_state);</span>
<span class="line-removed">  897   } else {</span>
<span class="line-removed">  898     ttyLocker ttyl;</span>
<span class="line-removed">  899     ::tty-&gt;print_cr(&quot;=============== DEBUG MESSAGE: %s ================\n&quot;,</span>
<span class="line-removed">  900                     msg);</span>
<span class="line-removed">  901     assert(false, &quot;DEBUG MESSAGE: %s&quot;, msg);</span>
  902   }

  903 }
  904 
  905 void MacroAssembler::print_state64(int64_t pc, int64_t regs[]) {
  906   ttyLocker ttyl;
  907   FlagSetting fs(Debugging, true);
  908   tty-&gt;print_cr(&quot;rip = 0x%016lx&quot;, (intptr_t)pc);
  909 #ifndef PRODUCT
  910   tty-&gt;cr();
  911   findpc(pc);
  912   tty-&gt;cr();
  913 #endif
  914 #define PRINT_REG(rax, value) \
  915   { tty-&gt;print(&quot;%s = &quot;, #rax); os::print_location(tty, value); }
  916   PRINT_REG(rax, regs[15]);
  917   PRINT_REG(rbx, regs[12]);
  918   PRINT_REG(rcx, regs[14]);
  919   PRINT_REG(rdx, regs[13]);
  920   PRINT_REG(rdi, regs[8]);
  921   PRINT_REG(rsi, regs[9]);
  922   PRINT_REG(rbp, regs[10]);
</pre>
<hr />
<pre>
  986 
  987 void MacroAssembler::addpd(XMMRegister dst, AddressLiteral src) {
  988   if (reachable(src)) {
  989     Assembler::addpd(dst, as_Address(src));
  990   } else {
  991     lea(rscratch1, src);
  992     Assembler::addpd(dst, Address(rscratch1, 0));
  993   }
  994 }
  995 
  996 void MacroAssembler::align(int modulus) {
  997   align(modulus, offset());
  998 }
  999 
 1000 void MacroAssembler::align(int modulus, int target) {
 1001   if (target % modulus != 0) {
 1002     nop(modulus - (target % modulus));
 1003   }
 1004 }
 1005 
<span class="line-modified"> 1006 void MacroAssembler::andpd(XMMRegister dst, AddressLiteral src) {</span>
 1007   // Used in sign-masking with aligned address.
 1008   assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
 1009   if (reachable(src)) {
 1010     Assembler::andpd(dst, as_Address(src));
 1011   } else {
<span class="line-modified"> 1012     lea(rscratch1, src);</span>
<span class="line-modified"> 1013     Assembler::andpd(dst, Address(rscratch1, 0));</span>
 1014   }
 1015 }
 1016 
<span class="line-modified"> 1017 void MacroAssembler::andps(XMMRegister dst, AddressLiteral src) {</span>
 1018   // Used in sign-masking with aligned address.
 1019   assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
 1020   if (reachable(src)) {
 1021     Assembler::andps(dst, as_Address(src));
 1022   } else {
<span class="line-modified"> 1023     lea(rscratch1, src);</span>
<span class="line-modified"> 1024     Assembler::andps(dst, Address(rscratch1, 0));</span>
 1025   }
 1026 }
 1027 
 1028 void MacroAssembler::andptr(Register dst, int32_t imm32) {
 1029   LP64_ONLY(andq(dst, imm32)) NOT_LP64(andl(dst, imm32));
 1030 }
 1031 
 1032 void MacroAssembler::atomic_incl(Address counter_addr) {
 1033   lock();
 1034   incrementl(counter_addr);
 1035 }
 1036 
 1037 void MacroAssembler::atomic_incl(AddressLiteral counter_addr, Register scr) {
 1038   if (reachable(counter_addr)) {
 1039     atomic_incl(as_Address(counter_addr));
 1040   } else {
 1041     lea(scr, counter_addr);
 1042     atomic_incl(Address(scr, 0));
 1043   }
 1044 }
</pre>
<hr />
<pre>
 1097 
 1098     call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), thread);
 1099     jump(RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()));
 1100     should_not_reach_here();
 1101 
 1102     bind(no_reserved_zone_enabling);
 1103 }
 1104 
 1105 int MacroAssembler::biased_locking_enter(Register lock_reg,
 1106                                          Register obj_reg,
 1107                                          Register swap_reg,
 1108                                          Register tmp_reg,
 1109                                          bool swap_reg_contains_mark,
 1110                                          Label&amp; done,
 1111                                          Label* slow_case,
 1112                                          BiasedLockingCounters* counters) {
 1113   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 1114   assert(swap_reg == rax, &quot;swap_reg must be rax for cmpxchgq&quot;);
 1115   assert(tmp_reg != noreg, &quot;tmp_reg must be supplied&quot;);
 1116   assert_different_registers(lock_reg, obj_reg, swap_reg, tmp_reg);
<span class="line-modified"> 1117   assert(markOopDesc::age_shift == markOopDesc::lock_bits + markOopDesc::biased_lock_bits, &quot;biased locking makes assumptions about bit layout&quot;);</span>
 1118   Address mark_addr      (obj_reg, oopDesc::mark_offset_in_bytes());
 1119   NOT_LP64( Address saved_mark_addr(lock_reg, 0); )
 1120 
 1121   if (PrintBiasedLockingStatistics &amp;&amp; counters == NULL) {
 1122     counters = BiasedLocking::counters();
 1123   }
 1124   // Biased locking
 1125   // See whether the lock is currently biased toward our thread and
 1126   // whether the epoch is still valid
 1127   // Note that the runtime guarantees sufficient alignment of JavaThread
 1128   // pointers to allow age to be placed into low bits
 1129   // First check to see whether biasing is even enabled for this object
 1130   Label cas_label;
 1131   int null_check_offset = -1;
 1132   if (!swap_reg_contains_mark) {
 1133     null_check_offset = offset();
 1134     movptr(swap_reg, mark_addr);
 1135   }
 1136   movptr(tmp_reg, swap_reg);
<span class="line-modified"> 1137   andptr(tmp_reg, markOopDesc::biased_lock_mask_in_place);</span>
<span class="line-modified"> 1138   cmpptr(tmp_reg, markOopDesc::biased_lock_pattern);</span>
 1139   jcc(Assembler::notEqual, cas_label);
 1140   // The bias pattern is present in the object&#39;s header. Need to check
 1141   // whether the bias owner and the epoch are both still current.
 1142 #ifndef _LP64
 1143   // Note that because there is no current thread register on x86_32 we
 1144   // need to store off the mark word we read out of the object to
 1145   // avoid reloading it and needing to recheck invariants below. This
 1146   // store is unfortunate but it makes the overall code shorter and
 1147   // simpler.
 1148   movptr(saved_mark_addr, swap_reg);
 1149 #endif
 1150   if (swap_reg_contains_mark) {
 1151     null_check_offset = offset();
 1152   }
 1153   load_prototype_header(tmp_reg, obj_reg);
 1154 #ifdef _LP64
 1155   orptr(tmp_reg, r15_thread);
 1156   xorptr(tmp_reg, swap_reg);
 1157   Register header_reg = tmp_reg;
 1158 #else
 1159   xorptr(tmp_reg, swap_reg);
 1160   get_thread(swap_reg);
 1161   xorptr(swap_reg, tmp_reg);
 1162   Register header_reg = swap_reg;
 1163 #endif
<span class="line-modified"> 1164   andptr(header_reg, ~((int) markOopDesc::age_mask_in_place));</span>
 1165   if (counters != NULL) {
 1166     cond_inc32(Assembler::zero,
 1167                ExternalAddress((address) counters-&gt;biased_lock_entry_count_addr()));
 1168   }
 1169   jcc(Assembler::equal, done);
 1170 
 1171   Label try_revoke_bias;
 1172   Label try_rebias;
 1173 
 1174   // At this point we know that the header has the bias pattern and
 1175   // that we are not the bias owner in the current epoch. We need to
 1176   // figure out more details about the state of the header in order to
 1177   // know what operations can be legally performed on the object&#39;s
 1178   // header.
 1179 
 1180   // If the low three bits in the xor result aren&#39;t clear, that means
 1181   // the prototype header is no longer biased and we have to revoke
 1182   // the bias on this object.
<span class="line-modified"> 1183   testptr(header_reg, markOopDesc::biased_lock_mask_in_place);</span>
 1184   jccb(Assembler::notZero, try_revoke_bias);
 1185 
 1186   // Biasing is still enabled for this data type. See whether the
 1187   // epoch of the current bias is still valid, meaning that the epoch
 1188   // bits of the mark word are equal to the epoch bits of the
 1189   // prototype header. (Note that the prototype header&#39;s epoch bits
 1190   // only change at a safepoint.) If not, attempt to rebias the object
 1191   // toward the current thread. Note that we must be absolutely sure
 1192   // that the current epoch is invalid in order to do this because
 1193   // otherwise the manipulations it performs on the mark word are
 1194   // illegal.
<span class="line-modified"> 1195   testptr(header_reg, markOopDesc::epoch_mask_in_place);</span>
 1196   jccb(Assembler::notZero, try_rebias);
 1197 
 1198   // The epoch of the current bias is still valid but we know nothing
 1199   // about the owner; it might be set or it might be clear. Try to
 1200   // acquire the bias of the object using an atomic operation. If this
 1201   // fails we will go in to the runtime to revoke the object&#39;s bias.
 1202   // Note that we first construct the presumed unbiased header so we
 1203   // don&#39;t accidentally blow away another thread&#39;s valid bias.
 1204   NOT_LP64( movptr(swap_reg, saved_mark_addr); )
 1205   andptr(swap_reg,
<span class="line-modified"> 1206          markOopDesc::biased_lock_mask_in_place | markOopDesc::age_mask_in_place | markOopDesc::epoch_mask_in_place);</span>
 1207 #ifdef _LP64
 1208   movptr(tmp_reg, swap_reg);
 1209   orptr(tmp_reg, r15_thread);
 1210 #else
 1211   get_thread(tmp_reg);
 1212   orptr(tmp_reg, swap_reg);
 1213 #endif
 1214   lock();
 1215   cmpxchgptr(tmp_reg, mark_addr); // compare tmp_reg and swap_reg
 1216   // If the biasing toward our thread failed, this means that
 1217   // another thread succeeded in biasing it toward itself and we
 1218   // need to revoke that bias. The revocation will occur in the
 1219   // interpreter runtime in the slow case.
 1220   if (counters != NULL) {
 1221     cond_inc32(Assembler::zero,
 1222                ExternalAddress((address) counters-&gt;anonymously_biased_lock_entry_count_addr()));
 1223   }
 1224   if (slow_case != NULL) {
 1225     jcc(Assembler::notZero, *slow_case);
 1226   }
</pre>
<hr />
<pre>
 1280   if (counters != NULL) {
 1281     cond_inc32(Assembler::zero,
 1282                ExternalAddress((address) counters-&gt;revoked_lock_entry_count_addr()));
 1283   }
 1284 
 1285   bind(cas_label);
 1286 
 1287   return null_check_offset;
 1288 }
 1289 
 1290 void MacroAssembler::biased_locking_exit(Register obj_reg, Register temp_reg, Label&amp; done) {
 1291   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 1292 
 1293   // Check for biased locking unlock case, which is a no-op
 1294   // Note: we do not have to check the thread ID for two reasons.
 1295   // First, the interpreter checks for IllegalMonitorStateException at
 1296   // a higher level. Second, if the bias was revoked while we held the
 1297   // lock, the object could not be rebiased toward another thread, so
 1298   // the bias bit would be clear.
 1299   movptr(temp_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
<span class="line-modified"> 1300   andptr(temp_reg, markOopDesc::biased_lock_mask_in_place);</span>
<span class="line-modified"> 1301   cmpptr(temp_reg, markOopDesc::biased_lock_pattern);</span>
 1302   jcc(Assembler::equal, done);
 1303 }
 1304 
 1305 #ifdef COMPILER2
 1306 
 1307 #if INCLUDE_RTM_OPT
 1308 
 1309 // Update rtm_counters based on abort status
 1310 // input: abort_status
 1311 //        rtm_counters (RTMLockingCounters*)
 1312 // flags are killed
 1313 void MacroAssembler::rtm_counters_update(Register abort_status, Register rtm_counters) {
 1314 
 1315   atomic_incptr(Address(rtm_counters, RTMLockingCounters::abort_count_offset()));
 1316   if (PrintPreciseRTMLockingStatistics) {
 1317     for (int i = 0; i &lt; RTMLockingCounters::ABORT_STATUS_LIMIT; i++) {
 1318       Label check_abort;
 1319       testl(abort_status, (1&lt;&lt;i));
 1320       jccb(Assembler::equal, check_abort);
 1321       atomic_incptr(Address(rtm_counters, RTMLockingCounters::abortX_count_offset() + (i * sizeof(uintx))));
</pre>
<hr />
<pre>
 1468 }
 1469 
 1470 // Use RTM for normal stack locks
 1471 // Input: objReg (object to lock)
 1472 void MacroAssembler::rtm_stack_locking(Register objReg, Register tmpReg, Register scrReg,
 1473                                        Register retry_on_abort_count_Reg,
 1474                                        RTMLockingCounters* stack_rtm_counters,
 1475                                        Metadata* method_data, bool profile_rtm,
 1476                                        Label&amp; DONE_LABEL, Label&amp; IsInflated) {
 1477   assert(UseRTMForStackLocks, &quot;why call this otherwise?&quot;);
 1478   assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
 1479   assert(tmpReg == rax, &quot;&quot;);
 1480   assert(scrReg == rdx, &quot;&quot;);
 1481   Label L_rtm_retry, L_decrement_retry, L_on_abort;
 1482 
 1483   if (RTMRetryCount &gt; 0) {
 1484     movl(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort
 1485     bind(L_rtm_retry);
 1486   }
 1487   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));
<span class="line-modified"> 1488   testptr(tmpReg, markOopDesc::monitor_value);  // inflated vs stack-locked|neutral|biased</span>
 1489   jcc(Assembler::notZero, IsInflated);
 1490 
 1491   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 1492     Label L_noincrement;
 1493     if (RTMTotalCountIncrRate &gt; 1) {
 1494       // tmpReg, scrReg and flags are killed
 1495       branch_on_random_using_rdtsc(tmpReg, scrReg, RTMTotalCountIncrRate, L_noincrement);
 1496     }
 1497     assert(stack_rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 1498     atomic_incptr(ExternalAddress((address)stack_rtm_counters-&gt;total_count_addr()), scrReg);
 1499     bind(L_noincrement);
 1500   }
 1501   xbegin(L_on_abort);
 1502   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));       // fetch markword
<span class="line-modified"> 1503   andptr(tmpReg, markOopDesc::biased_lock_mask_in_place); // look at 3 lock bits</span>
<span class="line-modified"> 1504   cmpptr(tmpReg, markOopDesc::unlocked_value);            // bits = 001 unlocked</span>
 1505   jcc(Assembler::equal, DONE_LABEL);        // all done if unlocked
 1506 
 1507   Register abort_status_Reg = tmpReg; // status of abort is stored in RAX
 1508   if (UseRTMXendForLockBusy) {
 1509     xend();
 1510     movptr(abort_status_Reg, 0x2);   // Set the abort status to 2 (so we can retry)
 1511     jmp(L_decrement_retry);
 1512   }
 1513   else {
 1514     xabort(0);
 1515   }
 1516   bind(L_on_abort);
 1517   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 1518     rtm_profiling(abort_status_Reg, scrReg, stack_rtm_counters, method_data, profile_rtm);
 1519   }
 1520   bind(L_decrement_retry);
 1521   if (RTMRetryCount &gt; 0) {
 1522     // retry on lock abort if abort status is &#39;can retry&#39; (0x2) or &#39;memory conflict&#39; (0x4)
 1523     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry);
 1524   }
 1525 }
 1526 
 1527 // Use RTM for inflating locks
 1528 // inputs: objReg (object to lock)
 1529 //         boxReg (on-stack box address (displaced header location) - KILLED)
<span class="line-modified"> 1530 //         tmpReg (ObjectMonitor address + markOopDesc::monitor_value)</span>
 1531 void MacroAssembler::rtm_inflated_locking(Register objReg, Register boxReg, Register tmpReg,
 1532                                           Register scrReg, Register retry_on_busy_count_Reg,
 1533                                           Register retry_on_abort_count_Reg,
 1534                                           RTMLockingCounters* rtm_counters,
 1535                                           Metadata* method_data, bool profile_rtm,
 1536                                           Label&amp; DONE_LABEL) {
 1537   assert(UseRTMLocking, &quot;why call this otherwise?&quot;);
 1538   assert(tmpReg == rax, &quot;&quot;);
 1539   assert(scrReg == rdx, &quot;&quot;);
 1540   Label L_rtm_retry, L_decrement_retry, L_on_abort;
 1541   int owner_offset = OM_OFFSET_NO_MONITOR_VALUE_TAG(owner);
 1542 
<span class="line-modified"> 1543   // Without cast to int32_t a movptr will destroy r10 which is typically obj</span>
<span class="line-modified"> 1544   movptr(Address(boxReg, 0), (int32_t)intptr_t(markOopDesc::unused_mark()));</span>
 1545   movptr(boxReg, tmpReg); // Save ObjectMonitor address
 1546 
 1547   if (RTMRetryCount &gt; 0) {
 1548     movl(retry_on_busy_count_Reg, RTMRetryCount);  // Retry on lock busy
 1549     movl(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort
 1550     bind(L_rtm_retry);
 1551   }
 1552   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 1553     Label L_noincrement;
 1554     if (RTMTotalCountIncrRate &gt; 1) {
 1555       // tmpReg, scrReg and flags are killed
 1556       branch_on_random_using_rdtsc(tmpReg, scrReg, RTMTotalCountIncrRate, L_noincrement);
 1557     }
 1558     assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 1559     atomic_incptr(ExternalAddress((address)rtm_counters-&gt;total_count_addr()), scrReg);
 1560     bind(L_noincrement);
 1561   }
 1562   xbegin(L_on_abort);
 1563   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));
 1564   movptr(tmpReg, Address(tmpReg, owner_offset));
</pre>
<hr />
<pre>
 1593   get_thread(scrReg);
 1594   Register threadReg = scrReg;
 1595 #endif
 1596   lock();
 1597   cmpxchgptr(threadReg, Address(boxReg, owner_offset)); // Updates tmpReg
 1598 
 1599   if (RTMRetryCount &gt; 0) {
 1600     // success done else retry
 1601     jccb(Assembler::equal, DONE_LABEL) ;
 1602     bind(L_decrement_retry);
 1603     // Spin and retry if lock is busy.
 1604     rtm_retry_lock_on_busy(retry_on_busy_count_Reg, boxReg, tmpReg, scrReg, L_rtm_retry);
 1605   }
 1606   else {
 1607     bind(L_decrement_retry);
 1608   }
 1609 }
 1610 
 1611 #endif //  INCLUDE_RTM_OPT
 1612 
<span class="line-modified"> 1613 // Fast_Lock and Fast_Unlock used by C2</span>
 1614 
 1615 // Because the transitions from emitted code to the runtime
 1616 // monitorenter/exit helper stubs are so slow it&#39;s critical that
<span class="line-modified"> 1617 // we inline both the stack-locking fast-path and the inflated fast path.</span>
 1618 //
 1619 // See also: cmpFastLock and cmpFastUnlock.
 1620 //
 1621 // What follows is a specialized inline transliteration of the code
<span class="line-modified"> 1622 // in slow_enter() and slow_exit().  If we&#39;re concerned about I$ bloat</span>
<span class="line-modified"> 1623 // another option would be to emit TrySlowEnter and TrySlowExit methods</span>
 1624 // at startup-time.  These methods would accept arguments as
 1625 // (rax,=Obj, rbx=Self, rcx=box, rdx=Scratch) and return success-failure
<span class="line-modified"> 1626 // indications in the icc.ZFlag.  Fast_Lock and Fast_Unlock would simply</span>
 1627 // marshal the arguments and emit calls to TrySlowEnter and TrySlowExit.
 1628 // In practice, however, the # of lock sites is bounded and is usually small.
 1629 // Besides the call overhead, TrySlowEnter and TrySlowExit might suffer
 1630 // if the processor uses simple bimodal branch predictors keyed by EIP
 1631 // Since the helper routines would be called from multiple synchronization
 1632 // sites.
 1633 //
 1634 // An even better approach would be write &quot;MonitorEnter()&quot; and &quot;MonitorExit()&quot;
 1635 // in java - using j.u.c and unsafe - and just bind the lock and unlock sites
 1636 // to those specialized methods.  That&#39;d give us a mostly platform-independent
 1637 // implementation that the JITs could optimize and inline at their pleasure.
 1638 // Done correctly, the only time we&#39;d need to cross to native could would be
 1639 // to park() or unpark() threads.  We&#39;d also need a few more unsafe operators
 1640 // to (a) prevent compiler-JIT reordering of non-volatile accesses, and
 1641 // (b) explicit barriers or fence operations.
 1642 //
 1643 // TODO:
 1644 //
<span class="line-modified"> 1645 // *  Arrange for C2 to pass &quot;Self&quot; into Fast_Lock and Fast_Unlock in one of the registers (scr).</span>
<span class="line-modified"> 1646 //    This avoids manifesting the Self pointer in the Fast_Lock and Fast_Unlock terminals.</span>
 1647 //    Given TLAB allocation, Self is usually manifested in a register, so passing it into
 1648 //    the lock operators would typically be faster than reifying Self.
 1649 //
 1650 // *  Ideally I&#39;d define the primitives as:
 1651 //       fast_lock   (nax Obj, nax box, EAX tmp, nax scr) where box, tmp and scr are KILLED.
 1652 //       fast_unlock (nax Obj, EAX box, nax tmp) where box and tmp are KILLED
 1653 //    Unfortunately ADLC bugs prevent us from expressing the ideal form.
 1654 //    Instead, we&#39;re stuck with a rather awkward and brittle register assignments below.
 1655 //    Furthermore the register assignments are overconstrained, possibly resulting in
 1656 //    sub-optimal code near the synchronization site.
 1657 //
 1658 // *  Eliminate the sp-proximity tests and just use &quot;== Self&quot; tests instead.
 1659 //    Alternately, use a better sp-proximity test.
 1660 //
 1661 // *  Currently ObjectMonitor._Owner can hold either an sp value or a (THREAD *) value.
 1662 //    Either one is sufficient to uniquely identify a thread.
 1663 //    TODO: eliminate use of sp in _owner and use get_thread(tr) instead.
 1664 //
 1665 // *  Intrinsify notify() and notifyAll() for the common cases where the
 1666 //    object is locked by the calling thread but the waitlist is empty.
 1667 //    avoid the expensive JNI call to JVM_Notify() and JVM_NotifyAll().
 1668 //
 1669 // *  use jccb and jmpb instead of jcc and jmp to improve code density.
 1670 //    But beware of excessive branch density on AMD Opterons.
 1671 //
<span class="line-modified"> 1672 // *  Both Fast_Lock and Fast_Unlock set the ICC.ZF to indicate success</span>
<span class="line-modified"> 1673 //    or failure of the fast-path.  If the fast-path fails then we pass</span>
<span class="line-modified"> 1674 //    control to the slow-path, typically in C.  In Fast_Lock and</span>
<span class="line-modified"> 1675 //    Fast_Unlock we often branch to DONE_LABEL, just to find that C2</span>
 1676 //    will emit a conditional branch immediately after the node.
 1677 //    So we have branches to branches and lots of ICC.ZF games.
 1678 //    Instead, it might be better to have C2 pass a &quot;FailureLabel&quot;
<span class="line-modified"> 1679 //    into Fast_Lock and Fast_Unlock.  In the case of success, control</span>
 1680 //    will drop through the node.  ICC.ZF is undefined at exit.
 1681 //    In the case of failure, the node will branch directly to the
 1682 //    FailureLabel
 1683 
 1684 
 1685 // obj: object to lock
 1686 // box: on-stack box address (displaced header location) - KILLED
 1687 // rax,: tmp -- KILLED
 1688 // scr: tmp -- KILLED
 1689 void MacroAssembler::fast_lock(Register objReg, Register boxReg, Register tmpReg,
 1690                                Register scrReg, Register cx1Reg, Register cx2Reg,
 1691                                BiasedLockingCounters* counters,
 1692                                RTMLockingCounters* rtm_counters,
 1693                                RTMLockingCounters* stack_rtm_counters,
 1694                                Metadata* method_data,
 1695                                bool use_rtm, bool profile_rtm) {
 1696   // Ensure the register assignments are disjoint
 1697   assert(tmpReg == rax, &quot;&quot;);
 1698 
 1699   if (use_rtm) {
</pre>
<hr />
<pre>
 1730 
 1731   // it&#39;s stack-locked, biased or neutral
 1732   // TODO: optimize away redundant LDs of obj-&gt;mark and improve the markword triage
 1733   // order to reduce the number of conditional branches in the most common cases.
 1734   // Beware -- there&#39;s a subtle invariant that fetch of the markword
 1735   // at [FETCH], below, will never observe a biased encoding (*101b).
 1736   // If this invariant is not held we risk exclusion (safety) failure.
 1737   if (UseBiasedLocking &amp;&amp; !UseOptoBiasInlining) {
 1738     biased_locking_enter(boxReg, objReg, tmpReg, scrReg, false, DONE_LABEL, NULL, counters);
 1739   }
 1740 
 1741 #if INCLUDE_RTM_OPT
 1742   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
 1743     rtm_stack_locking(objReg, tmpReg, scrReg, cx2Reg,
 1744                       stack_rtm_counters, method_data, profile_rtm,
 1745                       DONE_LABEL, IsInflated);
 1746   }
 1747 #endif // INCLUDE_RTM_OPT
 1748 
 1749   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));          // [FETCH]
<span class="line-modified"> 1750   testptr(tmpReg, markOopDesc::monitor_value); // inflated vs stack-locked|neutral|biased</span>
 1751   jccb(Assembler::notZero, IsInflated);
 1752 
 1753   // Attempt stack-locking ...
<span class="line-modified"> 1754   orptr (tmpReg, markOopDesc::unlocked_value);</span>
 1755   movptr(Address(boxReg, 0), tmpReg);          // Anticipate successful CAS
 1756   lock();
 1757   cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      // Updates tmpReg
 1758   if (counters != NULL) {
 1759     cond_inc32(Assembler::equal,
 1760                ExternalAddress((address)counters-&gt;fast_path_entry_count_addr()));
 1761   }
 1762   jcc(Assembler::equal, DONE_LABEL);           // Success
 1763 
 1764   // Recursive locking.
 1765   // The object is stack-locked: markword contains stack pointer to BasicLock.
 1766   // Locked by current thread if difference with current SP is less than one page.
 1767   subptr(tmpReg, rsp);
 1768   // Next instruction set ZFlag == 1 (Success) if difference is less then one page.
 1769   andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - os::vm_page_size())) );
 1770   movptr(Address(boxReg, 0), tmpReg);
 1771   if (counters != NULL) {
 1772     cond_inc32(Assembler::equal,
 1773                ExternalAddress((address)counters-&gt;fast_path_entry_count_addr()));
 1774   }
 1775   jmp(DONE_LABEL);
 1776 
 1777   bind(IsInflated);
<span class="line-modified"> 1778   // The object is inflated. tmpReg contains pointer to ObjectMonitor* + markOopDesc::monitor_value</span>
 1779 
 1780 #if INCLUDE_RTM_OPT
 1781   // Use the same RTM locking code in 32- and 64-bit VM.
 1782   if (use_rtm) {
 1783     rtm_inflated_locking(objReg, boxReg, tmpReg, scrReg, cx1Reg, cx2Reg,
 1784                          rtm_counters, method_data, profile_rtm, DONE_LABEL);
 1785   } else {
 1786 #endif // INCLUDE_RTM_OPT
 1787 
 1788 #ifndef _LP64
 1789   // The object is inflated.
 1790 
 1791   // boxReg refers to the on-stack BasicLock in the current frame.
 1792   // We&#39;d like to write:
<span class="line-modified"> 1793   //   set box-&gt;_displaced_header = markOopDesc::unused_mark().  Any non-0 value suffices.</span>
 1794   // This is convenient but results a ST-before-CAS penalty.  The following CAS suffers
 1795   // additional latency as we have another ST in the store buffer that must drain.
 1796 
 1797   // avoid ST-before-CAS
 1798   // register juggle because we need tmpReg for cmpxchgptr below
 1799   movptr(scrReg, boxReg);
 1800   movptr(boxReg, tmpReg);                   // consider: LEA box, [tmp-2]
 1801 
 1802   // Optimistic form: consider XORL tmpReg,tmpReg
 1803   movptr(tmpReg, NULL_WORD);
 1804 
 1805   // Appears unlocked - try to swing _owner from null to non-null.
 1806   // Ideally, I&#39;d manifest &quot;Self&quot; with get_thread and then attempt
 1807   // to CAS the register containing Self into m-&gt;Owner.
 1808   // But we don&#39;t have enough registers, so instead we can either try to CAS
 1809   // rsp or the address of the box (in scr) into &amp;m-&gt;owner.  If the CAS succeeds
 1810   // we later store &quot;Self&quot; into m-&gt;Owner.  Transiently storing a stack address
 1811   // (rsp or the address of the box) into  m-&gt;owner is harmless.
 1812   // Invariant: tmpReg == 0.  tmpReg is EAX which is the implicit cmpxchg comparand.
 1813   lock();
 1814   cmpxchgptr(scrReg, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
 1815   movptr(Address(scrReg, 0), 3);          // box-&gt;_displaced_header = 3
 1816   // If we weren&#39;t able to swing _owner from NULL to the BasicLock
 1817   // then take the slow path.
 1818   jccb  (Assembler::notZero, DONE_LABEL);
 1819   // update _owner from BasicLock to thread
 1820   get_thread (scrReg);                    // beware: clobbers ICCs
 1821   movptr(Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), scrReg);
 1822   xorptr(boxReg, boxReg);                 // set icc.ZFlag = 1 to indicate success
 1823 
<span class="line-modified"> 1824   // If the CAS fails we can either retry or pass control to the slow-path.</span>
 1825   // We use the latter tactic.
 1826   // Pass the CAS result in the icc.ZFlag into DONE_LABEL
 1827   // If the CAS was successful ...
 1828   //   Self has acquired the lock
 1829   //   Invariant: m-&gt;_recursions should already be 0, so we don&#39;t need to explicitly set it.
 1830   // Intentional fall-through into DONE_LABEL ...
 1831 #else // _LP64
<span class="line-modified"> 1832   // It&#39;s inflated</span>
 1833   movq(scrReg, tmpReg);
 1834   xorq(tmpReg, tmpReg);
<span class="line-removed"> 1835 </span>
 1836   lock();
 1837   cmpxchgptr(r15_thread, Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
<span class="line-modified"> 1838   // Unconditionally set box-&gt;_displaced_header = markOopDesc::unused_mark().</span>
<span class="line-modified"> 1839   // Without cast to int32_t movptr will destroy r10 which is typically obj.</span>
<span class="line-modified"> 1840   movptr(Address(boxReg, 0), (int32_t)intptr_t(markOopDesc::unused_mark()));</span>
 1841   // Intentional fall-through into DONE_LABEL ...
 1842   // Propagate ICC.ZF from CAS above into DONE_LABEL.
 1843 #endif // _LP64
 1844 #if INCLUDE_RTM_OPT
 1845   } // use_rtm()
 1846 #endif
 1847   // DONE_LABEL is a hot target - we&#39;d really like to place it at the
 1848   // start of cache line by padding with NOPs.
 1849   // See the AMD and Intel software optimization manuals for the
 1850   // most efficient &quot;long&quot; NOP encodings.
 1851   // Unfortunately none of our alignment mechanisms suffice.
 1852   bind(DONE_LABEL);
 1853 
 1854   // At DONE_LABEL the icc ZFlag is set as follows ...
<span class="line-modified"> 1855   // Fast_Unlock uses the same protocol.</span>
 1856   // ZFlag == 1 -&gt; Success
<span class="line-modified"> 1857   // ZFlag == 0 -&gt; Failure - force control through the slow-path</span>
 1858 }
 1859 
 1860 // obj: object to unlock
 1861 // box: box address (displaced header location), killed.  Must be EAX.
 1862 // tmp: killed, cannot be obj nor box.
 1863 //
 1864 // Some commentary on balanced locking:
 1865 //
<span class="line-modified"> 1866 // Fast_Lock and Fast_Unlock are emitted only for provably balanced lock sites.</span>
 1867 // Methods that don&#39;t have provably balanced locking are forced to run in the
 1868 // interpreter - such methods won&#39;t be compiled to use fast_lock and fast_unlock.
 1869 // The interpreter provides two properties:
 1870 // I1:  At return-time the interpreter automatically and quietly unlocks any
 1871 //      objects acquired the current activation (frame).  Recall that the
 1872 //      interpreter maintains an on-stack list of locks currently held by
 1873 //      a frame.
 1874 // I2:  If a method attempts to unlock an object that is not held by the
 1875 //      the frame the interpreter throws IMSX.
 1876 //
 1877 // Lets say A(), which has provably balanced locking, acquires O and then calls B().
 1878 // B() doesn&#39;t have provably balanced locking so it runs in the interpreter.
 1879 // Control returns to A() and A() unlocks O.  By I1 and I2, above, we know that O
 1880 // is still locked by A().
 1881 //
 1882 // The only other source of unbalanced locking would be JNI.  The &quot;Java Native Interface:
 1883 // Programmer&#39;s Guide and Specification&quot; claims that an object locked by jni_monitorenter
 1884 // should not be unlocked by &quot;normal&quot; java-level locking and vice-versa.  The specification
 1885 // doesn&#39;t specify what will occur if a program engages in such mixed-mode locking, however.
 1886 // Arguably given that the spec legislates the JNI case as undefined our implementation
<span class="line-modified"> 1887 // could reasonably *avoid* checking owner in Fast_Unlock().</span>
 1888 // In the interest of performance we elide m-&gt;Owner==Self check in unlock.
 1889 // A perfectly viable alternative is to elide the owner check except when
 1890 // Xcheck:jni is enabled.
 1891 
 1892 void MacroAssembler::fast_unlock(Register objReg, Register boxReg, Register tmpReg, bool use_rtm) {
 1893   assert(boxReg == rax, &quot;&quot;);
 1894   assert_different_registers(objReg, boxReg, tmpReg);
 1895 
 1896   Label DONE_LABEL, Stacked, CheckSucc;
 1897 
 1898   // Critically, the biased locking test must have precedence over
 1899   // and appear before the (box-&gt;dhw == 0) recursive stack-lock test.
 1900   if (UseBiasedLocking &amp;&amp; !UseOptoBiasInlining) {
 1901     biased_locking_exit(objReg, tmpReg, DONE_LABEL);
 1902   }
 1903 
 1904 #if INCLUDE_RTM_OPT
 1905   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
 1906     assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
 1907     Label L_regular_unlock;
<span class="line-modified"> 1908     movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));           // fetch markword</span>
<span class="line-modified"> 1909     andptr(tmpReg, markOopDesc::biased_lock_mask_in_place); // look at 3 lock bits</span>
<span class="line-modified"> 1910     cmpptr(tmpReg, markOopDesc::unlocked_value);            // bits = 001 unlocked</span>
<span class="line-modified"> 1911     jccb(Assembler::notEqual, L_regular_unlock);  // if !HLE RegularLock</span>
<span class="line-modified"> 1912     xend();                                       // otherwise end...</span>
<span class="line-modified"> 1913     jmp(DONE_LABEL);                              // ... and we&#39;re done</span>
 1914     bind(L_regular_unlock);
 1915   }
 1916 #endif
 1917 
<span class="line-modified"> 1918   cmpptr(Address(boxReg, 0), (int32_t)NULL_WORD); // Examine the displaced header</span>
<span class="line-modified"> 1919   jcc   (Assembler::zero, DONE_LABEL);            // 0 indicates recursive stack-lock</span>
<span class="line-modified"> 1920   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));             // Examine the object&#39;s markword</span>
<span class="line-modified"> 1921   testptr(tmpReg, markOopDesc::monitor_value);    // Inflated?</span>
 1922   jccb  (Assembler::zero, Stacked);
 1923 
 1924   // It&#39;s inflated.
 1925 #if INCLUDE_RTM_OPT
 1926   if (use_rtm) {
 1927     Label L_regular_inflated_unlock;
 1928     int owner_offset = OM_OFFSET_NO_MONITOR_VALUE_TAG(owner);
 1929     movptr(boxReg, Address(tmpReg, owner_offset));
 1930     testptr(boxReg, boxReg);
 1931     jccb(Assembler::notZero, L_regular_inflated_unlock);
 1932     xend();
 1933     jmpb(DONE_LABEL);
 1934     bind(L_regular_inflated_unlock);
 1935   }
 1936 #endif
 1937 
 1938   // Despite our balanced locking property we still check that m-&gt;_owner == Self
 1939   // as java routines or native JNI code called by this thread might
 1940   // have released the lock.
 1941   // Refer to the comments in synchronizer.cpp for how we might encode extra
 1942   // state in _succ so we can avoid fetching EntryList|cxq.
 1943   //
 1944   // I&#39;d like to add more cases in fast_lock() and fast_unlock() --
 1945   // such as recursive enter and exit -- but we have to be wary of
 1946   // I$ bloat, T$ effects and BP$ effects.
 1947   //
 1948   // If there&#39;s no contention try a 1-0 exit.  That is, exit without
 1949   // a costly MEMBAR or CAS.  See synchronizer.cpp for details on how
 1950   // we detect and recover from the race that the 1-0 exit admits.
 1951   //
<span class="line-modified"> 1952   // Conceptually Fast_Unlock() must execute a STST|LDST &quot;release&quot; barrier</span>
 1953   // before it STs null into _owner, releasing the lock.  Updates
 1954   // to data protected by the critical section must be visible before
 1955   // we drop the lock (and thus before any other thread could acquire
 1956   // the lock and observe the fields protected by the lock).
 1957   // IA32&#39;s memory-model is SPO, so STs are ordered with respect to
 1958   // each other and there&#39;s no need for an explicit barrier (fence).
 1959   // See also http://gee.cs.oswego.edu/dl/jmm/cookbook.html.
 1960 #ifndef _LP64
 1961   get_thread (boxReg);
 1962 
 1963   // Note that we could employ various encoding schemes to reduce
 1964   // the number of loads below (currently 4) to just 2 or 3.
 1965   // Refer to the comments in synchronizer.cpp.
 1966   // In practice the chain of fetches doesn&#39;t seem to impact performance, however.
 1967   xorptr(boxReg, boxReg);
 1968   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));
 1969   jccb  (Assembler::notZero, DONE_LABEL);
 1970   movptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));
 1971   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));
 1972   jccb  (Assembler::notZero, CheckSucc);
</pre>
<hr />
<pre>
 1981   // and be assured we observe the same value as above.
 1982   movptr(tmpReg, Address(boxReg, 0));
 1983   lock();
 1984   cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Uses RAX which is box
 1985   // Intention fall-thru into DONE_LABEL
 1986 
 1987   // DONE_LABEL is a hot target - we&#39;d really like to place it at the
 1988   // start of cache line by padding with NOPs.
 1989   // See the AMD and Intel software optimization manuals for the
 1990   // most efficient &quot;long&quot; NOP encodings.
 1991   // Unfortunately none of our alignment mechanisms suffice.
 1992   bind (CheckSucc);
 1993 #else // _LP64
 1994   // It&#39;s inflated
 1995   xorptr(boxReg, boxReg);
 1996   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));
 1997   jccb  (Assembler::notZero, DONE_LABEL);
 1998   movptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));
 1999   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));
 2000   jccb  (Assembler::notZero, CheckSucc);

 2001   movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t)NULL_WORD);
 2002   jmpb  (DONE_LABEL);
 2003 
 2004   // Try to avoid passing control into the slow_path ...
 2005   Label LSuccess, LGoSlowPath ;
 2006   bind  (CheckSucc);
 2007 
 2008   // The following optional optimization can be elided if necessary
<span class="line-modified"> 2009   // Effectively: if (succ == null) goto SlowPath</span>
 2010   // The code reduces the window for a race, however,
 2011   // and thus benefits performance.
 2012   cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), (int32_t)NULL_WORD);
 2013   jccb  (Assembler::zero, LGoSlowPath);
 2014 
 2015   xorptr(boxReg, boxReg);

 2016   movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t)NULL_WORD);
 2017 
 2018   // Memory barrier/fence
 2019   // Dekker pivot point -- fulcrum : ST Owner; MEMBAR; LD Succ
 2020   // Instead of MFENCE we use a dummy locked add of 0 to the top-of-stack.
 2021   // This is faster on Nehalem and AMD Shanghai/Barcelona.
 2022   // See https://blogs.oracle.com/dave/entry/instruction_selection_for_volatile_fences
 2023   // We might also restructure (ST Owner=0;barrier;LD _Succ) to
 2024   // (mov box,0; xchgq box, &amp;m-&gt;Owner; LD _succ) .
 2025   lock(); addl(Address(rsp, 0), 0);
 2026 
 2027   cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), (int32_t)NULL_WORD);
 2028   jccb  (Assembler::notZero, LSuccess);
 2029 
 2030   // Rare inopportune interleaving - race.
 2031   // The successor vanished in the small window above.
 2032   // The lock is contended -- (cxq|EntryList) != null -- and there&#39;s no apparent successor.
 2033   // We need to ensure progress and succession.
 2034   // Try to reacquire the lock.
 2035   // If that fails then the new owner is responsible for succession and this
 2036   // thread needs to take no further action and can exit via the fast path (success).
 2037   // If the re-acquire succeeds then pass control into the slow path.
 2038   // As implemented, this latter mode is horrible because we generated more
 2039   // coherence traffic on the lock *and* artifically extended the critical section
 2040   // length while by virtue of passing control into the slow path.
 2041 
 2042   // box is really RAX -- the following CMPXCHG depends on that binding
 2043   // cmpxchg R,[M] is equivalent to rax = CAS(M,rax,R)
 2044   lock();
 2045   cmpxchgptr(r15_thread, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
 2046   // There&#39;s no successor so we tried to regrab the lock.
 2047   // If that didn&#39;t work, then another thread grabbed the
 2048   // lock so we&#39;re done (and exit was a success).
 2049   jccb  (Assembler::notEqual, LSuccess);
<span class="line-modified"> 2050   // Intentional fall-through into slow-path</span>
 2051 
 2052   bind  (LGoSlowPath);
 2053   orl   (boxReg, 1);                      // set ICC.ZF=0 to indicate failure
 2054   jmpb  (DONE_LABEL);
 2055 
 2056   bind  (LSuccess);
 2057   testl (boxReg, 0);                      // set ICC.ZF=1 to indicate success
 2058   jmpb  (DONE_LABEL);
 2059 
 2060   bind  (Stacked);
 2061   movptr(tmpReg, Address (boxReg, 0));      // re-fetch
 2062   lock();
 2063   cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Uses RAX which is box
 2064 
 2065 #endif
 2066   bind(DONE_LABEL);
 2067 }
 2068 #endif // COMPILER2
 2069 
 2070 void MacroAssembler::c2bool(Register x) {
</pre>
<hr />
<pre>
 2725 }
 2726 
 2727 void MacroAssembler::divsd(XMMRegister dst, AddressLiteral src) {
 2728   if (reachable(src)) {
 2729     Assembler::divsd(dst, as_Address(src));
 2730   } else {
 2731     lea(rscratch1, src);
 2732     Assembler::divsd(dst, Address(rscratch1, 0));
 2733   }
 2734 }
 2735 
 2736 void MacroAssembler::divss(XMMRegister dst, AddressLiteral src) {
 2737   if (reachable(src)) {
 2738     Assembler::divss(dst, as_Address(src));
 2739   } else {
 2740     lea(rscratch1, src);
 2741     Assembler::divss(dst, Address(rscratch1, 0));
 2742   }
 2743 }
 2744 
<span class="line-modified"> 2745 // !defined(COMPILER2) is because of stupid core builds</span>
<span class="line-removed"> 2746 #if !defined(_LP64) || defined(COMPILER1) || !defined(COMPILER2) || INCLUDE_JVMCI</span>
 2747 void MacroAssembler::empty_FPU_stack() {
 2748   if (VM_Version::supports_mmx()) {
 2749     emms();
 2750   } else {
 2751     for (int i = 8; i-- &gt; 0; ) ffree(i);
 2752   }
 2753 }
<span class="line-modified"> 2754 #endif // !LP64 || C1 || !C2 || INCLUDE_JVMCI</span>
 2755 
 2756 
 2757 void MacroAssembler::enter() {
 2758   push(rbp);
 2759   mov(rbp, rsp);
 2760 }
 2761 
 2762 // A 5 byte nop that is safe for patching (see patch_verified_entry)
 2763 void MacroAssembler::fat_nop() {
 2764   if (UseAddressNop) {
 2765     addr_nop_5();
 2766   } else {
 2767     emit_int8(0x26); // es:
 2768     emit_int8(0x2e); // cs:
 2769     emit_int8(0x64); // fs:
 2770     emit_int8(0x65); // gs:
 2771     emit_int8((unsigned char)0x90);
 2772   }
 2773 }
 2774 

 2775 void MacroAssembler::fcmp(Register tmp) {
 2776   fcmp(tmp, 1, true, true);
 2777 }
 2778 
 2779 void MacroAssembler::fcmp(Register tmp, int index, bool pop_left, bool pop_right) {
 2780   assert(!pop_right || pop_left, &quot;usage error&quot;);
 2781   if (VM_Version::supports_cmov()) {
 2782     assert(tmp == noreg, &quot;unneeded temp&quot;);
 2783     if (pop_left) {
 2784       fucomip(index);
 2785     } else {
 2786       fucomi(index);
 2787     }
 2788     if (pop_right) {
 2789       fpop();
 2790     }
 2791   } else {
 2792     assert(tmp != noreg, &quot;need temp&quot;);
 2793     if (pop_left) {
 2794       if (pop_right) {
</pre>
<hr />
<pre>
 2836   }
 2837   bind(L);
 2838 }
 2839 
 2840 void MacroAssembler::fld_d(AddressLiteral src) {
 2841   fld_d(as_Address(src));
 2842 }
 2843 
 2844 void MacroAssembler::fld_s(AddressLiteral src) {
 2845   fld_s(as_Address(src));
 2846 }
 2847 
 2848 void MacroAssembler::fld_x(AddressLiteral src) {
 2849   Assembler::fld_x(as_Address(src));
 2850 }
 2851 
 2852 void MacroAssembler::fldcw(AddressLiteral src) {
 2853   Assembler::fldcw(as_Address(src));
 2854 }
 2855 























 2856 void MacroAssembler::mulpd(XMMRegister dst, AddressLiteral src) {
 2857   if (reachable(src)) {
 2858     Assembler::mulpd(dst, as_Address(src));
 2859   } else {
 2860     lea(rscratch1, src);
 2861     Assembler::mulpd(dst, Address(rscratch1, 0));
 2862   }
 2863 }
 2864 
<span class="line-removed"> 2865 void MacroAssembler::increase_precision() {</span>
<span class="line-removed"> 2866   subptr(rsp, BytesPerWord);</span>
<span class="line-removed"> 2867   fnstcw(Address(rsp, 0));</span>
<span class="line-removed"> 2868   movl(rax, Address(rsp, 0));</span>
<span class="line-removed"> 2869   orl(rax, 0x300);</span>
<span class="line-removed"> 2870   push(rax);</span>
<span class="line-removed"> 2871   fldcw(Address(rsp, 0));</span>
<span class="line-removed"> 2872   pop(rax);</span>
<span class="line-removed"> 2873 }</span>
<span class="line-removed"> 2874 </span>
<span class="line-removed"> 2875 void MacroAssembler::restore_precision() {</span>
<span class="line-removed"> 2876   fldcw(Address(rsp, 0));</span>
<span class="line-removed"> 2877   addptr(rsp, BytesPerWord);</span>
<span class="line-removed"> 2878 }</span>
<span class="line-removed"> 2879 </span>
<span class="line-removed"> 2880 void MacroAssembler::fpop() {</span>
<span class="line-removed"> 2881   ffree();</span>
<span class="line-removed"> 2882   fincstp();</span>
<span class="line-removed"> 2883 }</span>
<span class="line-removed"> 2884 </span>
 2885 void MacroAssembler::load_float(Address src) {
 2886   if (UseSSE &gt;= 1) {
 2887     movflt(xmm0, src);
 2888   } else {
 2889     LP64_ONLY(ShouldNotReachHere());
 2890     NOT_LP64(fld_s(src));
 2891   }
 2892 }
 2893 
 2894 void MacroAssembler::store_float(Address dst) {
 2895   if (UseSSE &gt;= 1) {
 2896     movflt(dst, xmm0);
 2897   } else {
 2898     LP64_ONLY(ShouldNotReachHere());
 2899     NOT_LP64(fstp_s(dst));
 2900   }
 2901 }
 2902 
 2903 void MacroAssembler::load_double(Address src) {
 2904   if (UseSSE &gt;= 2) {
 2905     movdbl(xmm0, src);
 2906   } else {
 2907     LP64_ONLY(ShouldNotReachHere());
 2908     NOT_LP64(fld_d(src));
 2909   }
 2910 }
 2911 
 2912 void MacroAssembler::store_double(Address dst) {
 2913   if (UseSSE &gt;= 2) {
 2914     movdbl(dst, xmm0);
 2915   } else {
 2916     LP64_ONLY(ShouldNotReachHere());
 2917     NOT_LP64(fstp_d(dst));
 2918   }
 2919 }
 2920 
<span class="line-removed"> 2921 void MacroAssembler::fremr(Register tmp) {</span>
<span class="line-removed"> 2922   save_rax(tmp);</span>
<span class="line-removed"> 2923   { Label L;</span>
<span class="line-removed"> 2924     bind(L);</span>
<span class="line-removed"> 2925     fprem();</span>
<span class="line-removed"> 2926     fwait(); fnstsw_ax();</span>
<span class="line-removed"> 2927 #ifdef _LP64</span>
<span class="line-removed"> 2928     testl(rax, 0x400);</span>
<span class="line-removed"> 2929     jcc(Assembler::notEqual, L);</span>
<span class="line-removed"> 2930 #else</span>
<span class="line-removed"> 2931     sahf();</span>
<span class="line-removed"> 2932     jcc(Assembler::parity, L);</span>
<span class="line-removed"> 2933 #endif // _LP64</span>
<span class="line-removed"> 2934   }</span>
<span class="line-removed"> 2935   restore_rax(tmp);</span>
<span class="line-removed"> 2936   // Result is in ST0.</span>
<span class="line-removed"> 2937   // Note: fxch &amp; fpop to get rid of ST1</span>
<span class="line-removed"> 2938   // (otherwise FPU stack could overflow eventually)</span>
<span class="line-removed"> 2939   fxch(1);</span>
<span class="line-removed"> 2940   fpop();</span>
<span class="line-removed"> 2941 }</span>
<span class="line-removed"> 2942 </span>
 2943 // dst = c = a * b + c
 2944 void MacroAssembler::fmad(XMMRegister dst, XMMRegister a, XMMRegister b, XMMRegister c) {
 2945   Assembler::vfmadd231sd(c, a, b);
 2946   if (dst != c) {
 2947     movdbl(dst, c);
 2948   }
 2949 }
 2950 
 2951 // dst = c = a * b + c
 2952 void MacroAssembler::fmaf(XMMRegister dst, XMMRegister a, XMMRegister b, XMMRegister c) {
 2953   Assembler::vfmadd231ss(c, a, b);
 2954   if (dst != c) {
 2955     movflt(dst, c);
 2956   }
 2957 }
 2958 
 2959 // dst = c = a * b + c
 2960 void MacroAssembler::vfmad(XMMRegister dst, XMMRegister a, XMMRegister b, XMMRegister c, int vector_len) {
 2961   Assembler::vfmadd231pd(c, a, b, vector_len);
 2962   if (dst != c) {
</pre>
<hr />
<pre>
 3323     lea(scratchReg, src);
 3324     movdqu(dst, Address(scratchReg, 0));
 3325   }
 3326 }
 3327 
 3328 void MacroAssembler::vmovdqu(Address dst, XMMRegister src) {
 3329     assert(((src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3330     Assembler::vmovdqu(dst, src);
 3331 }
 3332 
 3333 void MacroAssembler::vmovdqu(XMMRegister dst, Address src) {
 3334     assert(((dst-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3335     Assembler::vmovdqu(dst, src);
 3336 }
 3337 
 3338 void MacroAssembler::vmovdqu(XMMRegister dst, XMMRegister src) {
 3339     assert(((dst-&gt;encoding() &lt; 16  &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3340     Assembler::vmovdqu(dst, src);
 3341 }
 3342 
<span class="line-modified"> 3343 void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src) {</span>
 3344   if (reachable(src)) {
 3345     vmovdqu(dst, as_Address(src));
 3346   }
 3347   else {
<span class="line-modified"> 3348     lea(rscratch1, src);</span>
<span class="line-modified"> 3349     vmovdqu(dst, Address(rscratch1, 0));</span>
 3350   }
 3351 }
 3352 
 3353 void MacroAssembler::evmovdquq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {
 3354   if (reachable(src)) {
 3355     Assembler::evmovdquq(dst, as_Address(src), vector_len);
 3356   } else {
 3357     lea(rscratch, src);
 3358     Assembler::evmovdquq(dst, Address(rscratch, 0), vector_len);
 3359   }
 3360 }
 3361 
 3362 void MacroAssembler::movdqa(XMMRegister dst, AddressLiteral src) {
 3363   if (reachable(src)) {
 3364     Assembler::movdqa(dst, as_Address(src));
 3365   } else {
 3366     lea(rscratch1, src);
 3367     Assembler::movdqa(dst, Address(rscratch1, 0));
 3368   }
 3369 }
</pre>
<hr />
<pre>
 3654 }
 3655 
 3656 void MacroAssembler::sqrtss(XMMRegister dst, AddressLiteral src) {
 3657   if (reachable(src)) {
 3658     Assembler::sqrtss(dst, as_Address(src));
 3659   } else {
 3660     lea(rscratch1, src);
 3661     Assembler::sqrtss(dst, Address(rscratch1, 0));
 3662   }
 3663 }
 3664 
 3665 void MacroAssembler::subsd(XMMRegister dst, AddressLiteral src) {
 3666   if (reachable(src)) {
 3667     Assembler::subsd(dst, as_Address(src));
 3668   } else {
 3669     lea(rscratch1, src);
 3670     Assembler::subsd(dst, Address(rscratch1, 0));
 3671   }
 3672 }
 3673 









 3674 void MacroAssembler::subss(XMMRegister dst, AddressLiteral src) {
 3675   if (reachable(src)) {
 3676     Assembler::subss(dst, as_Address(src));
 3677   } else {
 3678     lea(rscratch1, src);
 3679     Assembler::subss(dst, Address(rscratch1, 0));
 3680   }
 3681 }
 3682 
 3683 void MacroAssembler::ucomisd(XMMRegister dst, AddressLiteral src) {
 3684   if (reachable(src)) {
 3685     Assembler::ucomisd(dst, as_Address(src));
 3686   } else {
 3687     lea(rscratch1, src);
 3688     Assembler::ucomisd(dst, Address(rscratch1, 0));
 3689   }
 3690 }
 3691 
 3692 void MacroAssembler::ucomiss(XMMRegister dst, AddressLiteral src) {
 3693   if (reachable(src)) {
 3694     Assembler::ucomiss(dst, as_Address(src));
 3695   } else {
 3696     lea(rscratch1, src);
 3697     Assembler::ucomiss(dst, Address(rscratch1, 0));
 3698   }
 3699 }
 3700 
<span class="line-modified"> 3701 void MacroAssembler::xorpd(XMMRegister dst, AddressLiteral src) {</span>
 3702   // Used in sign-bit flipping with aligned address.
 3703   assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
 3704   if (reachable(src)) {
 3705     Assembler::xorpd(dst, as_Address(src));
 3706   } else {
<span class="line-modified"> 3707     lea(rscratch1, src);</span>
<span class="line-modified"> 3708     Assembler::xorpd(dst, Address(rscratch1, 0));</span>
 3709   }
 3710 }
 3711 
 3712 void MacroAssembler::xorpd(XMMRegister dst, XMMRegister src) {
 3713   if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512dq() &amp;&amp; (dst-&gt;encoding() == src-&gt;encoding())) {
 3714     Assembler::vpxor(dst, dst, src, Assembler::AVX_512bit);
 3715   }
 3716   else {
 3717     Assembler::xorpd(dst, src);
 3718   }
 3719 }
 3720 
 3721 void MacroAssembler::xorps(XMMRegister dst, XMMRegister src) {
 3722   if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512dq() &amp;&amp; (dst-&gt;encoding() == src-&gt;encoding())) {
 3723     Assembler::vpxor(dst, dst, src, Assembler::AVX_512bit);
 3724   } else {
 3725     Assembler::xorps(dst, src);
 3726   }
 3727 }
 3728 
<span class="line-modified"> 3729 void MacroAssembler::xorps(XMMRegister dst, AddressLiteral src) {</span>
 3730   // Used in sign-bit flipping with aligned address.
 3731   assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
 3732   if (reachable(src)) {
 3733     Assembler::xorps(dst, as_Address(src));
 3734   } else {
<span class="line-modified"> 3735     lea(rscratch1, src);</span>
<span class="line-modified"> 3736     Assembler::xorps(dst, Address(rscratch1, 0));</span>
 3737   }
 3738 }
 3739 
 3740 void MacroAssembler::pshufb(XMMRegister dst, AddressLiteral src) {
 3741   // Used in sign-bit flipping with aligned address.
 3742   bool aligned_adr = (((intptr_t)src.target() &amp; 15) == 0);
 3743   assert((UseAVX &gt; 0) || aligned_adr, &quot;SSE mode requires address alignment 16 bytes&quot;);
 3744   if (reachable(src)) {
 3745     Assembler::pshufb(dst, as_Address(src));
 3746   } else {
 3747     lea(rscratch1, src);
 3748     Assembler::pshufb(dst, Address(rscratch1, 0));
 3749   }
 3750 }
 3751 
 3752 // AVX 3-operands instructions
 3753 
 3754 void MacroAssembler::vaddsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3755   if (reachable(src)) {
 3756     vaddsd(dst, nds, as_Address(src));
 3757   } else {
 3758     lea(rscratch1, src);
 3759     vaddsd(dst, nds, Address(rscratch1, 0));
 3760   }
 3761 }
 3762 
 3763 void MacroAssembler::vaddss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3764   if (reachable(src)) {
 3765     vaddss(dst, nds, as_Address(src));
 3766   } else {
 3767     lea(rscratch1, src);
 3768     vaddss(dst, nds, Address(rscratch1, 0));
 3769   }
 3770 }
 3771 










 3772 void MacroAssembler::vabsss(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len) {
 3773   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
 3774   vandps(dst, nds, negate_field, vector_len);
 3775 }
 3776 
 3777 void MacroAssembler::vabssd(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len) {
 3778   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
 3779   vandpd(dst, nds, negate_field, vector_len);
 3780 }
 3781 
 3782 void MacroAssembler::vpaddb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3783   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3784   Assembler::vpaddb(dst, nds, src, vector_len);
 3785 }
 3786 
 3787 void MacroAssembler::vpaddb(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
 3788   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3789   Assembler::vpaddb(dst, nds, src, vector_len);
 3790 }
 3791 
 3792 void MacroAssembler::vpaddw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3793   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3794   Assembler::vpaddw(dst, nds, src, vector_len);
 3795 }
 3796 
 3797 void MacroAssembler::vpaddw(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
 3798   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3799   Assembler::vpaddw(dst, nds, src, vector_len);
 3800 }
 3801 
<span class="line-modified"> 3802 void MacroAssembler::vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len) {</span>
 3803   if (reachable(src)) {
 3804     Assembler::vpand(dst, nds, as_Address(src), vector_len);
 3805   } else {
<span class="line-modified"> 3806     lea(rscratch1, src);</span>
<span class="line-modified"> 3807     Assembler::vpand(dst, nds, Address(rscratch1, 0), vector_len);</span>
 3808   }
 3809 }
 3810 
 3811 void MacroAssembler::vpbroadcastw(XMMRegister dst, XMMRegister src, int vector_len) {
 3812   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3813   Assembler::vpbroadcastw(dst, src, vector_len);
 3814 }
 3815 
 3816 void MacroAssembler::vpcmpeqb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3817   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3818   Assembler::vpcmpeqb(dst, nds, src, vector_len);
 3819 }
 3820 
 3821 void MacroAssembler::vpcmpeqw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3822   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3823   Assembler::vpcmpeqw(dst, nds, src, vector_len);
 3824 }
 3825 
 3826 void MacroAssembler::vpmovzxbw(XMMRegister dst, Address src, int vector_len) {
 3827   assert(((dst-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
</pre>
<hr />
<pre>
 3856 void MacroAssembler::vpsubw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3857   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3858   Assembler::vpsubw(dst, nds, src, vector_len);
 3859 }
 3860 
 3861 void MacroAssembler::vpsubw(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
 3862   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3863   Assembler::vpsubw(dst, nds, src, vector_len);
 3864 }
 3865 
 3866 void MacroAssembler::vpsraw(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len) {
 3867   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; shift-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3868   Assembler::vpsraw(dst, nds, shift, vector_len);
 3869 }
 3870 
 3871 void MacroAssembler::vpsraw(XMMRegister dst, XMMRegister nds, int shift, int vector_len) {
 3872   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3873   Assembler::vpsraw(dst, nds, shift, vector_len);
 3874 }
 3875 
















 3876 void MacroAssembler::vpsrlw(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len) {
 3877   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; shift-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3878   Assembler::vpsrlw(dst, nds, shift, vector_len);
 3879 }
 3880 
 3881 void MacroAssembler::vpsrlw(XMMRegister dst, XMMRegister nds, int shift, int vector_len) {
 3882   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3883   Assembler::vpsrlw(dst, nds, shift, vector_len);
 3884 }
 3885 
 3886 void MacroAssembler::vpsllw(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len) {
 3887   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; shift-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3888   Assembler::vpsllw(dst, nds, shift, vector_len);
 3889 }
 3890 
 3891 void MacroAssembler::vpsllw(XMMRegister dst, XMMRegister nds, int shift, int vector_len) {
 3892   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3893   Assembler::vpsllw(dst, nds, shift, vector_len);
 3894 }
 3895 
 3896 void MacroAssembler::vptest(XMMRegister dst, XMMRegister src) {
 3897   assert((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16),&quot;XMM register should be 0-15&quot;);
 3898   Assembler::vptest(dst, src);
 3899 }
 3900 
 3901 void MacroAssembler::punpcklbw(XMMRegister dst, XMMRegister src) {
 3902   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3903   Assembler::punpcklbw(dst, src);
 3904 }
 3905 
 3906 void MacroAssembler::pshufd(XMMRegister dst, Address src, int mode) {
 3907   assert(((dst-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3908   Assembler::pshufd(dst, src, mode);
 3909 }
 3910 
 3911 void MacroAssembler::pshuflw(XMMRegister dst, XMMRegister src, int mode) {
 3912   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3913   Assembler::pshuflw(dst, src, mode);
 3914 }
 3915 
<span class="line-modified"> 3916 void MacroAssembler::vandpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len) {</span>
 3917   if (reachable(src)) {
 3918     vandpd(dst, nds, as_Address(src), vector_len);
 3919   } else {
<span class="line-modified"> 3920     lea(rscratch1, src);</span>
<span class="line-modified"> 3921     vandpd(dst, nds, Address(rscratch1, 0), vector_len);</span>
 3922   }
 3923 }
 3924 
<span class="line-modified"> 3925 void MacroAssembler::vandps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len) {</span>
 3926   if (reachable(src)) {
 3927     vandps(dst, nds, as_Address(src), vector_len);
 3928   } else {
<span class="line-modified"> 3929     lea(rscratch1, src);</span>
<span class="line-modified"> 3930     vandps(dst, nds, Address(rscratch1, 0), vector_len);</span>
 3931   }
 3932 }
 3933 
 3934 void MacroAssembler::vdivsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3935   if (reachable(src)) {
 3936     vdivsd(dst, nds, as_Address(src));
 3937   } else {
 3938     lea(rscratch1, src);
 3939     vdivsd(dst, nds, Address(rscratch1, 0));
 3940   }
 3941 }
 3942 
 3943 void MacroAssembler::vdivss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3944   if (reachable(src)) {
 3945     vdivss(dst, nds, as_Address(src));
 3946   } else {
 3947     lea(rscratch1, src);
 3948     vdivss(dst, nds, Address(rscratch1, 0));
 3949   }
 3950 }
</pre>
<hr />
<pre>
 3978 
 3979 void MacroAssembler::vsubss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3980   if (reachable(src)) {
 3981     vsubss(dst, nds, as_Address(src));
 3982   } else {
 3983     lea(rscratch1, src);
 3984     vsubss(dst, nds, Address(rscratch1, 0));
 3985   }
 3986 }
 3987 
 3988 void MacroAssembler::vnegatess(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3989   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
 3990   vxorps(dst, nds, src, Assembler::AVX_128bit);
 3991 }
 3992 
 3993 void MacroAssembler::vnegatesd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3994   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
 3995   vxorpd(dst, nds, src, Assembler::AVX_128bit);
 3996 }
 3997 
<span class="line-modified"> 3998 void MacroAssembler::vxorpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len) {</span>
 3999   if (reachable(src)) {
 4000     vxorpd(dst, nds, as_Address(src), vector_len);
 4001   } else {
<span class="line-modified"> 4002     lea(rscratch1, src);</span>
<span class="line-modified"> 4003     vxorpd(dst, nds, Address(rscratch1, 0), vector_len);</span>
 4004   }
 4005 }
 4006 
<span class="line-modified"> 4007 void MacroAssembler::vxorps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len) {</span>
 4008   if (reachable(src)) {
 4009     vxorps(dst, nds, as_Address(src), vector_len);
 4010   } else {
<span class="line-modified"> 4011     lea(rscratch1, src);</span>
<span class="line-modified"> 4012     vxorps(dst, nds, Address(rscratch1, 0), vector_len);</span>




































































 4013   }
 4014 }
 4015 












































































 4016 void MacroAssembler::clear_jweak_tag(Register possibly_jweak) {
 4017   const int32_t inverted_jweak_mask = ~static_cast&lt;int32_t&gt;(JNIHandles::weak_tag_mask);
 4018   STATIC_ASSERT(inverted_jweak_mask == -2); // otherwise check this code
 4019   // The inverted mask is sign-extended
 4020   andptr(possibly_jweak, inverted_jweak_mask);
 4021 }
 4022 
 4023 void MacroAssembler::resolve_jobject(Register value,
 4024                                      Register thread,
 4025                                      Register tmp) {
 4026   assert_different_registers(value, thread, tmp);
 4027   Label done, not_weak;
 4028   testptr(value, value);
 4029   jcc(Assembler::zero, done);                // Use NULL as-is.
 4030   testptr(value, JNIHandles::weak_tag_mask); // Test for jweak tag.
 4031   jcc(Assembler::zero, not_weak);
 4032   // Resolve jweak.
 4033   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF,
 4034                  value, Address(value, -JNIHandles::weak_tag_value), tmp, thread);
 4035   verify_oop(value);
</pre>
<hr />
<pre>
 4430     assert(!pushed_rdi, &quot;rdi must be left non-NULL&quot;);
 4431     // Also, the condition codes are properly set Z/NZ on succeed/failure.
 4432   }
 4433 
 4434   if (L_failure == &amp;L_fallthrough)
 4435         jccb(Assembler::notEqual, *L_failure);
 4436   else  jcc(Assembler::notEqual, *L_failure);
 4437 
 4438   // Success.  Cache the super we found and proceed in triumph.
 4439   movptr(super_cache_addr, super_klass);
 4440 
 4441   if (L_success != &amp;L_fallthrough) {
 4442     jmp(*L_success);
 4443   }
 4444 
 4445 #undef IS_A_TEMP
 4446 
 4447   bind(L_fallthrough);
 4448 }
 4449 


























 4450 
 4451 void MacroAssembler::cmov32(Condition cc, Register dst, Address src) {
 4452   if (VM_Version::supports_cmov()) {
 4453     cmovl(cc, dst, src);
 4454   } else {
 4455     Label L;
 4456     jccb(negate_condition(cc), L);
 4457     movl(dst, src);
 4458     bind(L);
 4459   }
 4460 }
 4461 
 4462 void MacroAssembler::cmov32(Condition cc, Register dst, Register src) {
 4463   if (VM_Version::supports_cmov()) {
 4464     cmovl(cc, dst, src);
 4465   } else {
 4466     Label L;
 4467     jccb(negate_condition(cc), L);
 4468     movl(dst, src);
 4469     bind(L);
</pre>
<hr />
<pre>
 4883     printf(&quot;--------------------------------------------------\n&quot;);
 4884   }
 4885 
 4886 };
 4887 
 4888 
 4889 static void _print_CPU_state(CPU_State* state) {
 4890   state-&gt;print();
 4891 };
 4892 
 4893 
 4894 void MacroAssembler::print_CPU_state() {
 4895   push_CPU_state();
 4896   push(rsp);                // pass CPU state
 4897   call(RuntimeAddress(CAST_FROM_FN_PTR(address, _print_CPU_state)));
 4898   addptr(rsp, wordSize);       // discard argument
 4899   pop_CPU_state();
 4900 }
 4901 
 4902 

 4903 static bool _verify_FPU(int stack_depth, char* s, CPU_State* state) {
 4904   static int counter = 0;
 4905   FPU_State* fs = &amp;state-&gt;_fpu_state;
 4906   counter++;
 4907   // For leaf calls, only verify that the top few elements remain empty.
 4908   // We only need 1 empty at the top for C2 code.
 4909   if( stack_depth &lt; 0 ) {
 4910     if( fs-&gt;tag_for_st(7) != 3 ) {
 4911       printf(&quot;FPR7 not empty\n&quot;);
 4912       state-&gt;print();
 4913       assert(false, &quot;error&quot;);
 4914       return false;
 4915     }
 4916     return true;                // All other stack states do not matter
 4917   }
 4918 
 4919   assert((fs-&gt;_control_word._value &amp; 0xffff) == StubRoutines::_fpu_cntrl_wrd_std,
 4920          &quot;bad FPU control word&quot;);
 4921 
 4922   // compute stack depth
</pre>
<hr />
<pre>
 4939       // too many elements on the stack
 4940       printf(&quot;%s: &lt;= %d stack elements expected but found %d\n&quot;, s, -stack_depth, d);
 4941       state-&gt;print();
 4942       assert(false, &quot;error&quot;);
 4943       return false;
 4944     }
 4945   } else {
 4946     // expected stack depth is stack_depth
 4947     if (d != stack_depth) {
 4948       // wrong stack depth
 4949       printf(&quot;%s: %d stack elements expected but found %d\n&quot;, s, stack_depth, d);
 4950       state-&gt;print();
 4951       assert(false, &quot;error&quot;);
 4952       return false;
 4953     }
 4954   }
 4955   // everything is cool
 4956   return true;
 4957 }
 4958 
<span class="line-removed"> 4959 </span>
 4960 void MacroAssembler::verify_FPU(int stack_depth, const char* s) {
 4961   if (!VerifyFPU) return;
 4962   push_CPU_state();
 4963   push(rsp);                // pass CPU state
 4964   ExternalAddress msg((address) s);
 4965   // pass message string s
 4966   pushptr(msg.addr());
 4967   push(stack_depth);        // pass stack depth
 4968   call(RuntimeAddress(CAST_FROM_FN_PTR(address, _verify_FPU)));
 4969   addptr(rsp, 3 * wordSize);   // discard arguments
 4970   // check for error
 4971   { Label L;
 4972     testl(rax, rax);
 4973     jcc(Assembler::notZero, L);
 4974     int3();                  // break if error condition
 4975     bind(L);
 4976   }
 4977   pop_CPU_state();
 4978 }

 4979 
 4980 void MacroAssembler::restore_cpu_control_state_after_jni() {
 4981   // Either restore the MXCSR register after returning from the JNI Call
 4982   // or verify that it wasn&#39;t changed (with -Xcheck:jni flag).
 4983   if (VM_Version::supports_sse()) {
 4984     if (RestoreMXCSROnJNICalls) {
 4985       ldmxcsr(ExternalAddress(StubRoutines::addr_mxcsr_std()));
 4986     } else if (CheckJNICalls) {
 4987       call(RuntimeAddress(StubRoutines::x86::verify_mxcsr_entry()));
 4988     }
 4989   }
 4990   // Clear upper bits of YMM registers to avoid SSE &lt;-&gt; AVX transition penalty.
 4991   vzeroupper();
 4992   // Reset k1 to 0xffff.
 4993 
 4994 #ifdef COMPILER2
 4995   if (PostLoopMultiversioning &amp;&amp; VM_Version::supports_evex()) {
 4996     push(rcx);
 4997     movl(rcx, 0xffff);
 4998     kmovwl(k1, rcx);
</pre>
<hr />
<pre>
 5003 #ifndef _LP64
 5004   // Either restore the x87 floating pointer control word after returning
 5005   // from the JNI call or verify that it wasn&#39;t changed.
 5006   if (CheckJNICalls) {
 5007     call(RuntimeAddress(StubRoutines::x86::verify_fpu_cntrl_wrd_entry()));
 5008   }
 5009 #endif // _LP64
 5010 }
 5011 
 5012 // ((OopHandle)result).resolve();
 5013 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
 5014   assert_different_registers(result, tmp);
 5015 
 5016   // Only 64 bit platforms support GCs that require a tmp register
 5017   // Only IN_HEAP loads require a thread_tmp register
 5018   // OopHandle::resolve is an indirection like jobject.
 5019   access_load_at(T_OBJECT, IN_NATIVE,
 5020                  result, Address(result, 0), tmp, /*tmp_thread*/noreg);
 5021 }
 5022 

















 5023 void MacroAssembler::load_mirror(Register mirror, Register method, Register tmp) {
 5024   // get mirror
 5025   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
<span class="line-modified"> 5026   movptr(mirror, Address(method, Method::const_offset()));</span>
<span class="line-removed"> 5027   movptr(mirror, Address(mirror, ConstMethod::constants_offset()));</span>
<span class="line-removed"> 5028   movptr(mirror, Address(mirror, ConstantPool::pool_holder_offset_in_bytes()));</span>
 5029   movptr(mirror, Address(mirror, mirror_offset));
 5030   resolve_oop_handle(mirror, tmp);
 5031 }
 5032 











 5033 void MacroAssembler::load_klass(Register dst, Register src) {
 5034 #ifdef _LP64
 5035   if (UseCompressedClassPointers) {
 5036     movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));
 5037     decode_klass_not_null(dst);
 5038   } else
 5039 #endif
 5040     movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
 5041 }
 5042 
 5043 void MacroAssembler::load_prototype_header(Register dst, Register src) {
 5044   load_klass(dst, src);
 5045   movptr(dst, Address(dst, Klass::prototype_header_offset()));
 5046 }
 5047 
 5048 void MacroAssembler::store_klass(Register dst, Register src) {
 5049 #ifdef _LP64
 5050   if (UseCompressedClassPointers) {
 5051     encode_klass_not_null(src);
 5052     movl(Address(dst, oopDesc::klass_offset_in_bytes()), src);
</pre>
<hr />
<pre>
 5107 // Used for storing NULLs.
 5108 void MacroAssembler::store_heap_oop_null(Address dst) {
 5109   access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);
 5110 }
 5111 
 5112 #ifdef _LP64
 5113 void MacroAssembler::store_klass_gap(Register dst, Register src) {
 5114   if (UseCompressedClassPointers) {
 5115     // Store to klass gap in destination
 5116     movl(Address(dst, oopDesc::klass_gap_offset_in_bytes()), src);
 5117   }
 5118 }
 5119 
 5120 #ifdef ASSERT
 5121 void MacroAssembler::verify_heapbase(const char* msg) {
 5122   assert (UseCompressedOops, &quot;should be compressed&quot;);
 5123   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5124   if (CheckCompressedOops) {
 5125     Label ok;
 5126     push(rscratch1); // cmpptr trashes rscratch1
<span class="line-modified"> 5127     cmpptr(r12_heapbase, ExternalAddress((address)Universe::narrow_ptrs_base_addr()));</span>
 5128     jcc(Assembler::equal, ok);
 5129     STOP(msg);
 5130     bind(ok);
 5131     pop(rscratch1);
 5132   }
 5133 }
 5134 #endif
 5135 
 5136 // Algorithm must match oop.inline.hpp encode_heap_oop.
 5137 void MacroAssembler::encode_heap_oop(Register r) {
 5138 #ifdef ASSERT
 5139   verify_heapbase(&quot;MacroAssembler::encode_heap_oop: heap base corrupted?&quot;);
 5140 #endif
 5141   verify_oop(r, &quot;broken oop in encode_heap_oop&quot;);
<span class="line-modified"> 5142   if (Universe::narrow_oop_base() == NULL) {</span>
<span class="line-modified"> 5143     if (Universe::narrow_oop_shift() != 0) {</span>
<span class="line-modified"> 5144       assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
 5145       shrq(r, LogMinObjAlignmentInBytes);
 5146     }
 5147     return;
 5148   }
 5149   testq(r, r);
 5150   cmovq(Assembler::equal, r, r12_heapbase);
 5151   subq(r, r12_heapbase);
 5152   shrq(r, LogMinObjAlignmentInBytes);
 5153 }
 5154 
 5155 void MacroAssembler::encode_heap_oop_not_null(Register r) {
 5156 #ifdef ASSERT
 5157   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null: heap base corrupted?&quot;);
 5158   if (CheckCompressedOops) {
 5159     Label ok;
 5160     testq(r, r);
 5161     jcc(Assembler::notEqual, ok);
 5162     STOP(&quot;null oop passed to encode_heap_oop_not_null&quot;);
 5163     bind(ok);
 5164   }
 5165 #endif
 5166   verify_oop(r, &quot;broken oop in encode_heap_oop_not_null&quot;);
<span class="line-modified"> 5167   if (Universe::narrow_oop_base() != NULL) {</span>
 5168     subq(r, r12_heapbase);
 5169   }
<span class="line-modified"> 5170   if (Universe::narrow_oop_shift() != 0) {</span>
<span class="line-modified"> 5171     assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
 5172     shrq(r, LogMinObjAlignmentInBytes);
 5173   }
 5174 }
 5175 
 5176 void MacroAssembler::encode_heap_oop_not_null(Register dst, Register src) {
 5177 #ifdef ASSERT
 5178   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null2: heap base corrupted?&quot;);
 5179   if (CheckCompressedOops) {
 5180     Label ok;
 5181     testq(src, src);
 5182     jcc(Assembler::notEqual, ok);
 5183     STOP(&quot;null oop passed to encode_heap_oop_not_null2&quot;);
 5184     bind(ok);
 5185   }
 5186 #endif
 5187   verify_oop(src, &quot;broken oop in encode_heap_oop_not_null2&quot;);
 5188   if (dst != src) {
 5189     movq(dst, src);
 5190   }
<span class="line-modified"> 5191   if (Universe::narrow_oop_base() != NULL) {</span>
 5192     subq(dst, r12_heapbase);
 5193   }
<span class="line-modified"> 5194   if (Universe::narrow_oop_shift() != 0) {</span>
<span class="line-modified"> 5195     assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
 5196     shrq(dst, LogMinObjAlignmentInBytes);
 5197   }
 5198 }
 5199 
 5200 void  MacroAssembler::decode_heap_oop(Register r) {
 5201 #ifdef ASSERT
 5202   verify_heapbase(&quot;MacroAssembler::decode_heap_oop: heap base corrupted?&quot;);
 5203 #endif
<span class="line-modified"> 5204   if (Universe::narrow_oop_base() == NULL) {</span>
<span class="line-modified"> 5205     if (Universe::narrow_oop_shift() != 0) {</span>
<span class="line-modified"> 5206       assert (LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
 5207       shlq(r, LogMinObjAlignmentInBytes);
 5208     }
 5209   } else {
 5210     Label done;
 5211     shlq(r, LogMinObjAlignmentInBytes);
 5212     jccb(Assembler::equal, done);
 5213     addq(r, r12_heapbase);
 5214     bind(done);
 5215   }
 5216   verify_oop(r, &quot;broken oop in decode_heap_oop&quot;);
 5217 }
 5218 
 5219 void  MacroAssembler::decode_heap_oop_not_null(Register r) {
 5220   // Note: it will change flags
 5221   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5222   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5223   // Cannot assert, unverified entry point counts instructions (see .ad file)
 5224   // vtableStubs also counts instructions in pd_code_size_limit.
 5225   // Also do not verify_oop as this is called by verify_oop.
<span class="line-modified"> 5226   if (Universe::narrow_oop_shift() != 0) {</span>
<span class="line-modified"> 5227     assert(LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
 5228     shlq(r, LogMinObjAlignmentInBytes);
<span class="line-modified"> 5229     if (Universe::narrow_oop_base() != NULL) {</span>
 5230       addq(r, r12_heapbase);
 5231     }
 5232   } else {
<span class="line-modified"> 5233     assert (Universe::narrow_oop_base() == NULL, &quot;sanity&quot;);</span>
 5234   }
 5235 }
 5236 
 5237 void  MacroAssembler::decode_heap_oop_not_null(Register dst, Register src) {
 5238   // Note: it will change flags
 5239   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5240   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5241   // Cannot assert, unverified entry point counts instructions (see .ad file)
 5242   // vtableStubs also counts instructions in pd_code_size_limit.
 5243   // Also do not verify_oop as this is called by verify_oop.
<span class="line-modified"> 5244   if (Universe::narrow_oop_shift() != 0) {</span>
<span class="line-modified"> 5245     assert(LogMinObjAlignmentInBytes == Universe::narrow_oop_shift(), &quot;decode alg wrong&quot;);</span>
 5246     if (LogMinObjAlignmentInBytes == Address::times_8) {
 5247       leaq(dst, Address(r12_heapbase, src, Address::times_8, 0));
 5248     } else {
 5249       if (dst != src) {
 5250         movq(dst, src);
 5251       }
 5252       shlq(dst, LogMinObjAlignmentInBytes);
<span class="line-modified"> 5253       if (Universe::narrow_oop_base() != NULL) {</span>
 5254         addq(dst, r12_heapbase);
 5255       }
 5256     }
 5257   } else {
<span class="line-modified"> 5258     assert (Universe::narrow_oop_base() == NULL, &quot;sanity&quot;);</span>
 5259     if (dst != src) {
 5260       movq(dst, src);
 5261     }
 5262   }
 5263 }
 5264 
 5265 void MacroAssembler::encode_klass_not_null(Register r) {
<span class="line-modified"> 5266   if (Universe::narrow_klass_base() != NULL) {</span>
 5267     // Use r12 as a scratch register in which to temporarily load the narrow_klass_base.
 5268     assert(r != r12_heapbase, &quot;Encoding a klass in r12&quot;);
<span class="line-modified"> 5269     mov64(r12_heapbase, (int64_t)Universe::narrow_klass_base());</span>
 5270     subq(r, r12_heapbase);
 5271   }
<span class="line-modified"> 5272   if (Universe::narrow_klass_shift() != 0) {</span>
<span class="line-modified"> 5273     assert (LogKlassAlignmentInBytes == Universe::narrow_klass_shift(), &quot;decode alg wrong&quot;);</span>
 5274     shrq(r, LogKlassAlignmentInBytes);
 5275   }
<span class="line-modified"> 5276   if (Universe::narrow_klass_base() != NULL) {</span>
 5277     reinit_heapbase();
 5278   }
 5279 }
 5280 
 5281 void MacroAssembler::encode_klass_not_null(Register dst, Register src) {
 5282   if (dst == src) {
 5283     encode_klass_not_null(src);
 5284   } else {
<span class="line-modified"> 5285     if (Universe::narrow_klass_base() != NULL) {</span>
<span class="line-modified"> 5286       mov64(dst, (int64_t)Universe::narrow_klass_base());</span>
 5287       negq(dst);
 5288       addq(dst, src);
 5289     } else {
 5290       movptr(dst, src);
 5291     }
<span class="line-modified"> 5292     if (Universe::narrow_klass_shift() != 0) {</span>
<span class="line-modified"> 5293       assert (LogKlassAlignmentInBytes == Universe::narrow_klass_shift(), &quot;decode alg wrong&quot;);</span>
 5294       shrq(dst, LogKlassAlignmentInBytes);
 5295     }
 5296   }
 5297 }
 5298 
 5299 // Function instr_size_for_decode_klass_not_null() counts the instructions
 5300 // generated by decode_klass_not_null(register r) and reinit_heapbase(),
 5301 // when (Universe::heap() != NULL).  Hence, if the instructions they
 5302 // generate change, then this method needs to be updated.
 5303 int MacroAssembler::instr_size_for_decode_klass_not_null() {
 5304   assert (UseCompressedClassPointers, &quot;only for compressed klass ptrs&quot;);
<span class="line-modified"> 5305   if (Universe::narrow_klass_base() != NULL) {</span>
 5306     // mov64 + addq + shlq? + mov64  (for reinit_heapbase()).
<span class="line-modified"> 5307     return (Universe::narrow_klass_shift() == 0 ? 20 : 24);</span>
 5308   } else {
 5309     // longest load decode klass function, mov64, leaq
 5310     return 16;
 5311   }
 5312 }
 5313 
 5314 // !!! If the instructions that get generated here change then function
 5315 // instr_size_for_decode_klass_not_null() needs to get updated.
 5316 void  MacroAssembler::decode_klass_not_null(Register r) {
 5317   // Note: it will change flags
 5318   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5319   assert(r != r12_heapbase, &quot;Decoding a klass in r12&quot;);
 5320   // Cannot assert, unverified entry point counts instructions (see .ad file)
 5321   // vtableStubs also counts instructions in pd_code_size_limit.
 5322   // Also do not verify_oop as this is called by verify_oop.
<span class="line-modified"> 5323   if (Universe::narrow_klass_shift() != 0) {</span>
<span class="line-modified"> 5324     assert(LogKlassAlignmentInBytes == Universe::narrow_klass_shift(), &quot;decode alg wrong&quot;);</span>
 5325     shlq(r, LogKlassAlignmentInBytes);
 5326   }
 5327   // Use r12 as a scratch register in which to temporarily load the narrow_klass_base.
<span class="line-modified"> 5328   if (Universe::narrow_klass_base() != NULL) {</span>
<span class="line-modified"> 5329     mov64(r12_heapbase, (int64_t)Universe::narrow_klass_base());</span>
 5330     addq(r, r12_heapbase);
 5331     reinit_heapbase();
 5332   }
 5333 }
 5334 
 5335 void  MacroAssembler::decode_klass_not_null(Register dst, Register src) {
 5336   // Note: it will change flags
 5337   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5338   if (dst == src) {
 5339     decode_klass_not_null(dst);
 5340   } else {
 5341     // Cannot assert, unverified entry point counts instructions (see .ad file)
 5342     // vtableStubs also counts instructions in pd_code_size_limit.
 5343     // Also do not verify_oop as this is called by verify_oop.
<span class="line-modified"> 5344     mov64(dst, (int64_t)Universe::narrow_klass_base());</span>
<span class="line-modified"> 5345     if (Universe::narrow_klass_shift() != 0) {</span>
<span class="line-modified"> 5346       assert(LogKlassAlignmentInBytes == Universe::narrow_klass_shift(), &quot;decode alg wrong&quot;);</span>
 5347       assert(LogKlassAlignmentInBytes == Address::times_8, &quot;klass not aligned on 64bits?&quot;);
 5348       leaq(dst, Address(dst, src, Address::times_8, 0));
 5349     } else {
 5350       addq(dst, src);
 5351     }
 5352   }
 5353 }
 5354 
 5355 void  MacroAssembler::set_narrow_oop(Register dst, jobject obj) {
 5356   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5357   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5358   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5359   int oop_index = oop_recorder()-&gt;find_index(obj);
 5360   RelocationHolder rspec = oop_Relocation::spec(oop_index);
 5361   mov_narrow_oop(dst, oop_index, rspec);
 5362 }
 5363 
 5364 void  MacroAssembler::set_narrow_oop(Address dst, jobject obj) {
 5365   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5366   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5367   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5368   int oop_index = oop_recorder()-&gt;find_index(obj);
 5369   RelocationHolder rspec = oop_Relocation::spec(oop_index);
 5370   mov_narrow_oop(dst, oop_index, rspec);
 5371 }
 5372 
 5373 void  MacroAssembler::set_narrow_klass(Register dst, Klass* k) {
 5374   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5375   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5376   int klass_index = oop_recorder()-&gt;find_index(k);
 5377   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
<span class="line-modified"> 5378   mov_narrow_oop(dst, Klass::encode_klass(k), rspec);</span>
 5379 }
 5380 
 5381 void  MacroAssembler::set_narrow_klass(Address dst, Klass* k) {
 5382   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5383   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5384   int klass_index = oop_recorder()-&gt;find_index(k);
 5385   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
<span class="line-modified"> 5386   mov_narrow_oop(dst, Klass::encode_klass(k), rspec);</span>
 5387 }
 5388 
 5389 void  MacroAssembler::cmp_narrow_oop(Register dst, jobject obj) {
 5390   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5391   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5392   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5393   int oop_index = oop_recorder()-&gt;find_index(obj);
 5394   RelocationHolder rspec = oop_Relocation::spec(oop_index);
 5395   Assembler::cmp_narrow_oop(dst, oop_index, rspec);
 5396 }
 5397 
 5398 void  MacroAssembler::cmp_narrow_oop(Address dst, jobject obj) {
 5399   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5400   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5401   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5402   int oop_index = oop_recorder()-&gt;find_index(obj);
 5403   RelocationHolder rspec = oop_Relocation::spec(oop_index);
 5404   Assembler::cmp_narrow_oop(dst, oop_index, rspec);
 5405 }
 5406 
 5407 void  MacroAssembler::cmp_narrow_klass(Register dst, Klass* k) {
 5408   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5409   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5410   int klass_index = oop_recorder()-&gt;find_index(k);
 5411   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
<span class="line-modified"> 5412   Assembler::cmp_narrow_oop(dst, Klass::encode_klass(k), rspec);</span>
 5413 }
 5414 
 5415 void  MacroAssembler::cmp_narrow_klass(Address dst, Klass* k) {
 5416   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5417   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5418   int klass_index = oop_recorder()-&gt;find_index(k);
 5419   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
<span class="line-modified"> 5420   Assembler::cmp_narrow_oop(dst, Klass::encode_klass(k), rspec);</span>
 5421 }
 5422 
 5423 void MacroAssembler::reinit_heapbase() {
 5424   if (UseCompressedOops || UseCompressedClassPointers) {
 5425     if (Universe::heap() != NULL) {
<span class="line-modified"> 5426       if (Universe::narrow_oop_base() == NULL) {</span>
 5427         MacroAssembler::xorptr(r12_heapbase, r12_heapbase);
 5428       } else {
<span class="line-modified"> 5429         mov64(r12_heapbase, (int64_t)Universe::narrow_ptrs_base());</span>
 5430       }
 5431     } else {
<span class="line-modified"> 5432       movptr(r12_heapbase, ExternalAddress((address)Universe::narrow_ptrs_base_addr()));</span>
 5433     }
 5434   }
 5435 }
 5436 
 5437 #endif // _LP64
 5438 
 5439 // C2 compiled method&#39;s prolog code.
 5440 void MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {
 5441 
 5442   // WARNING: Initial instruction MUST be 5 bytes or longer so that
 5443   // NativeJump::patch_verified_entry will be able to patch out the entry
 5444   // code safely. The push to verify stack depth is ok at 5 bytes,
 5445   // the frame allocation can be either 3 or 6 bytes. So if we don&#39;t do
 5446   // stack bang then we must use the 6 byte frame allocation even if
 5447   // we have no frame. :-(
 5448   assert(stack_bang_size &gt;= framesize || stack_bang_size &lt;= 0, &quot;stack bang size incorrect&quot;);
 5449 
 5450   assert((framesize &amp; (StackAlignmentInBytes-1)) == 0, &quot;frame size not aligned&quot;);
 5451   // Remove word for return addr
 5452   framesize -= wordSize;
</pre>
<hr />
<pre>
 6128   bind(CLEANUP);
 6129   pop(rsp); // restore SP
 6130 
 6131 } // string_indexof
 6132 
 6133 void MacroAssembler::string_indexof_char(Register str1, Register cnt1, Register ch, Register result,
 6134                                          XMMRegister vec1, XMMRegister vec2, XMMRegister vec3, Register tmp) {
 6135   ShortBranchVerifier sbv(this);
 6136   assert(UseSSE42Intrinsics, &quot;SSE4.2 intrinsics are required&quot;);
 6137 
 6138   int stride = 8;
 6139 
 6140   Label FOUND_CHAR, SCAN_TO_CHAR, SCAN_TO_CHAR_LOOP,
 6141         SCAN_TO_8_CHAR, SCAN_TO_8_CHAR_LOOP, SCAN_TO_16_CHAR_LOOP,
 6142         RET_NOT_FOUND, SCAN_TO_8_CHAR_INIT,
 6143         FOUND_SEQ_CHAR, DONE_LABEL;
 6144 
 6145   movptr(result, str1);
 6146   if (UseAVX &gt;= 2) {
 6147     cmpl(cnt1, stride);
<span class="line-modified"> 6148     jcc(Assembler::less, SCAN_TO_CHAR_LOOP);</span>
 6149     cmpl(cnt1, 2*stride);
 6150     jcc(Assembler::less, SCAN_TO_8_CHAR_INIT);
 6151     movdl(vec1, ch);
 6152     vpbroadcastw(vec1, vec1, Assembler::AVX_256bit);
 6153     vpxor(vec2, vec2);
 6154     movl(tmp, cnt1);
 6155     andl(tmp, 0xFFFFFFF0);  //vector count (in chars)
 6156     andl(cnt1,0x0000000F);  //tail count (in chars)
 6157 
 6158     bind(SCAN_TO_16_CHAR_LOOP);
 6159     vmovdqu(vec3, Address(result, 0));
 6160     vpcmpeqw(vec3, vec3, vec1, 1);
 6161     vptest(vec2, vec3);
 6162     jcc(Assembler::carryClear, FOUND_CHAR);
 6163     addptr(result, 32);
 6164     subl(tmp, 2*stride);
 6165     jcc(Assembler::notZero, SCAN_TO_16_CHAR_LOOP);
 6166     jmp(SCAN_TO_8_CHAR);
 6167     bind(SCAN_TO_8_CHAR_INIT);
 6168     movdl(vec1, ch);
 6169     pshuflw(vec1, vec1, 0x00);
 6170     pshufd(vec1, vec1, 0);
 6171     pxor(vec2, vec2);
 6172   }
 6173   bind(SCAN_TO_8_CHAR);
 6174   cmpl(cnt1, stride);
<span class="line-modified"> 6175   if (UseAVX &gt;= 2) {</span>
<span class="line-modified"> 6176     jcc(Assembler::less, SCAN_TO_CHAR);</span>
<span class="line-removed"> 6177   } else {</span>
<span class="line-removed"> 6178     jcc(Assembler::less, SCAN_TO_CHAR_LOOP);</span>
 6179     movdl(vec1, ch);
 6180     pshuflw(vec1, vec1, 0x00);
 6181     pshufd(vec1, vec1, 0);
 6182     pxor(vec2, vec2);
 6183   }
 6184   movl(tmp, cnt1);
 6185   andl(tmp, 0xFFFFFFF8);  //vector count (in chars)
 6186   andl(cnt1,0x00000007);  //tail count (in chars)
 6187 
 6188   bind(SCAN_TO_8_CHAR_LOOP);
 6189   movdqu(vec3, Address(result, 0));
 6190   pcmpeqw(vec3, vec1);
 6191   ptest(vec2, vec3);
 6192   jcc(Assembler::carryClear, FOUND_CHAR);
 6193   addptr(result, 16);
 6194   subl(tmp, stride);
 6195   jcc(Assembler::notZero, SCAN_TO_8_CHAR_LOOP);
 6196   bind(SCAN_TO_CHAR);
 6197   testl(cnt1, cnt1);
 6198   jcc(Assembler::zero, RET_NOT_FOUND);
</pre>
<hr />
<pre>
 6371     jmp(POP_LABEL);
 6372 
 6373     // Setup the registers to start vector comparison loop
 6374     bind(COMPARE_WIDE_VECTORS);
 6375     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 6376       lea(str1, Address(str1, result, scale));
 6377       lea(str2, Address(str2, result, scale));
 6378     } else {
 6379       lea(str1, Address(str1, result, scale1));
 6380       lea(str2, Address(str2, result, scale2));
 6381     }
 6382     subl(result, stride2);
 6383     subl(cnt2, stride2);
 6384     jcc(Assembler::zero, COMPARE_WIDE_TAIL);
 6385     negptr(result);
 6386 
 6387     //  In a loop, compare 16-chars (32-bytes) at once using (vpxor+vptest)
 6388     bind(COMPARE_WIDE_VECTORS_LOOP);
 6389 
 6390 #ifdef _LP64
<span class="line-modified"> 6391     if (VM_Version::supports_avx512vlbw()) { // trying 64 bytes fast loop</span>
 6392       cmpl(cnt2, stride2x2);
 6393       jccb(Assembler::below, COMPARE_WIDE_VECTORS_LOOP_AVX2);
 6394       testl(cnt2, stride2x2-1);   // cnt2 holds the vector count
 6395       jccb(Assembler::notZero, COMPARE_WIDE_VECTORS_LOOP_AVX2);   // means we cannot subtract by 0x40
 6396 
 6397       bind(COMPARE_WIDE_VECTORS_LOOP_AVX3); // the hottest loop
 6398       if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 6399         evmovdquq(vec1, Address(str1, result, scale), Assembler::AVX_512bit);
 6400         evpcmpeqb(k7, vec1, Address(str2, result, scale), Assembler::AVX_512bit); // k7 == 11..11, if operands equal, otherwise k7 has some 0
 6401       } else {
 6402         vpmovzxbw(vec1, Address(str1, result, scale1), Assembler::AVX_512bit);
 6403         evpcmpeqb(k7, vec1, Address(str2, result, scale2), Assembler::AVX_512bit); // k7 == 11..11, if operands equal, otherwise k7 has some 0
 6404       }
 6405       kortestql(k7, k7);
 6406       jcc(Assembler::aboveEqual, COMPARE_WIDE_VECTORS_LOOP_FAILED);     // miscompare
 6407       addptr(result, stride2x2);  // update since we already compared at this addr
 6408       subl(cnt2, stride2x2);      // and sub the size too
 6409       jccb(Assembler::notZero, COMPARE_WIDE_VECTORS_LOOP_AVX3);
 6410 
 6411       vpxor(vec1, vec1);
</pre>
<hr />
<pre>
 6631 //         return true;
 6632 //       }
 6633 //     }
 6634 //     return false;
 6635 //   }
 6636 void MacroAssembler::has_negatives(Register ary1, Register len,
 6637   Register result, Register tmp1,
 6638   XMMRegister vec1, XMMRegister vec2) {
 6639   // rsi: byte array
 6640   // rcx: len
 6641   // rax: result
 6642   ShortBranchVerifier sbv(this);
 6643   assert_different_registers(ary1, len, result, tmp1);
 6644   assert_different_registers(vec1, vec2);
 6645   Label TRUE_LABEL, FALSE_LABEL, DONE, COMPARE_CHAR, COMPARE_VECTORS, COMPARE_BYTE;
 6646 
 6647   // len == 0
 6648   testl(len, len);
 6649   jcc(Assembler::zero, FALSE_LABEL);
 6650 
<span class="line-modified"> 6651   if ((UseAVX &gt; 2) &amp;&amp; // AVX512</span>
 6652     VM_Version::supports_avx512vlbw() &amp;&amp;
 6653     VM_Version::supports_bmi2()) {
 6654 
 6655     Label test_64_loop, test_tail;
 6656     Register tmp3_aliased = len;
 6657 
 6658     movl(tmp1, len);
 6659     vpxor(vec2, vec2, vec2, Assembler::AVX_512bit);
 6660 
 6661     andl(tmp1, 64 - 1);   // tail count (in chars) 0x3F
 6662     andl(len, ~(64 - 1));    // vector count (in chars)
 6663     jccb(Assembler::zero, test_tail);
 6664 
 6665     lea(ary1, Address(ary1, len, Address::times_1));
 6666     negptr(len);
 6667 
 6668     bind(test_64_loop);
 6669     // Check whether our 64 elements of size byte contain negatives
 6670     evpcmpgtb(k2, vec2, Address(ary1, len, Address::times_1), Assembler::AVX_512bit);
 6671     kortestql(k2, k2);
</pre>
<hr />
<pre>
 6704     emit_int64(0x2726252423222120);
 6705     emit_int64(0x2F2E2D2C2B2A2928);
 6706     emit_int64(0x3736353433323130);
 6707     emit_int64(0x3F3E3D3C3B3A3938);
 6708 
 6709     bind(k_init);
 6710     lea(len, InternalAddress(tmp));
 6711     // create mask to test for negative byte inside a vector
 6712     evpbroadcastb(vec1, tmp1, Assembler::AVX_512bit);
 6713     evpcmpgtb(k3, vec1, Address(len, 0), Assembler::AVX_512bit);
 6714 
 6715 #endif
 6716     evpcmpgtb(k2, k3, vec2, Address(ary1, 0), Assembler::AVX_512bit);
 6717     ktestq(k2, k3);
 6718     jcc(Assembler::notZero, TRUE_LABEL);
 6719 
 6720     jmp(FALSE_LABEL);
 6721   } else {
 6722     movl(result, len); // copy
 6723 
<span class="line-modified"> 6724     if (UseAVX == 2 &amp;&amp; UseSSE &gt;= 2) {</span>
 6725       // With AVX2, use 32-byte vector compare
 6726       Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
 6727 
 6728       // Compare 32-byte vectors
 6729       andl(result, 0x0000001f);  //   tail count (in bytes)
 6730       andl(len, 0xffffffe0);   // vector count (in bytes)
 6731       jccb(Assembler::zero, COMPARE_TAIL);
 6732 
 6733       lea(ary1, Address(ary1, len, Address::times_1));
 6734       negptr(len);
 6735 
 6736       movl(tmp1, 0x80808080);   // create mask to test for Unicode chars in vector
 6737       movdl(vec2, tmp1);
 6738       vpbroadcastd(vec2, vec2, Assembler::AVX_256bit);
 6739 
 6740       bind(COMPARE_WIDE_VECTORS);
 6741       vmovdqu(vec1, Address(ary1, len, Address::times_1));
 6742       vptest(vec1, vec2);
 6743       jccb(Assembler::notZero, TRUE_LABEL);
 6744       addptr(len, 32);
</pre>
<hr />
<pre>
 6877 
 6878   if (is_array_equ &amp;&amp; is_char) {
 6879     // arrays_equals when used for char[].
 6880     shll(limit, 1);      // byte count != 0
 6881   }
 6882   movl(result, limit); // copy
 6883 
 6884   if (UseAVX &gt;= 2) {
 6885     // With AVX2, use 32-byte vector compare
 6886     Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
 6887 
 6888     // Compare 32-byte vectors
 6889     andl(result, 0x0000001f);  //   tail count (in bytes)
 6890     andl(limit, 0xffffffe0);   // vector count (in bytes)
 6891     jcc(Assembler::zero, COMPARE_TAIL);
 6892 
 6893     lea(ary1, Address(ary1, limit, Address::times_1));
 6894     lea(ary2, Address(ary2, limit, Address::times_1));
 6895     negptr(limit);
 6896 
<span class="line-removed"> 6897     bind(COMPARE_WIDE_VECTORS);</span>
<span class="line-removed"> 6898 </span>
 6899 #ifdef _LP64
<span class="line-modified"> 6900     if (VM_Version::supports_avx512vlbw()) { // trying 64 bytes fast loop</span>
 6901       Label COMPARE_WIDE_VECTORS_LOOP_AVX2, COMPARE_WIDE_VECTORS_LOOP_AVX3;
 6902 
 6903       cmpl(limit, -64);
<span class="line-modified"> 6904       jccb(Assembler::greater, COMPARE_WIDE_VECTORS_LOOP_AVX2);</span>
 6905 
 6906       bind(COMPARE_WIDE_VECTORS_LOOP_AVX3); // the hottest loop
 6907 
 6908       evmovdquq(vec1, Address(ary1, limit, Address::times_1), Assembler::AVX_512bit);
 6909       evpcmpeqb(k7, vec1, Address(ary2, limit, Address::times_1), Assembler::AVX_512bit);
 6910       kortestql(k7, k7);
 6911       jcc(Assembler::aboveEqual, FALSE_LABEL);     // miscompare
 6912       addptr(limit, 64);  // update since we already compared at this addr
 6913       cmpl(limit, -64);
 6914       jccb(Assembler::lessEqual, COMPARE_WIDE_VECTORS_LOOP_AVX3);
 6915 
 6916       // At this point we may still need to compare -limit+result bytes.
 6917       // We could execute the next two instruction and just continue via non-wide path:
 6918       //  cmpl(limit, 0);
 6919       //  jcc(Assembler::equal, COMPARE_TAIL);  // true
 6920       // But since we stopped at the points ary{1,2}+limit which are
 6921       // not farther than 64 bytes from the ends of arrays ary{1,2}+result
 6922       // (|limit| &lt;= 32 and result &lt; 32),
 6923       // we may just compare the last 64 bytes.
 6924       //
 6925       addptr(result, -64);   // it is safe, bc we just came from this area
 6926       evmovdquq(vec1, Address(ary1, result, Address::times_1), Assembler::AVX_512bit);
 6927       evpcmpeqb(k7, vec1, Address(ary2, result, Address::times_1), Assembler::AVX_512bit);
 6928       kortestql(k7, k7);
 6929       jcc(Assembler::aboveEqual, FALSE_LABEL);     // miscompare
 6930 
 6931       jmp(TRUE_LABEL);
 6932 
 6933       bind(COMPARE_WIDE_VECTORS_LOOP_AVX2);
 6934 
 6935     }//if (VM_Version::supports_avx512vlbw())
 6936 #endif //_LP64
<span class="line-modified"> 6937 </span>
 6938     vmovdqu(vec1, Address(ary1, limit, Address::times_1));
 6939     vmovdqu(vec2, Address(ary2, limit, Address::times_1));
 6940     vpxor(vec1, vec2);
 6941 
 6942     vptest(vec1, vec1);
 6943     jcc(Assembler::notZero, FALSE_LABEL);
 6944     addptr(limit, 32);
 6945     jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
 6946 
 6947     testl(result, result);
 6948     jcc(Assembler::zero, TRUE_LABEL);
 6949 
 6950     vmovdqu(vec1, Address(ary1, result, Address::times_1, -32));
 6951     vmovdqu(vec2, Address(ary2, result, Address::times_1, -32));
 6952     vpxor(vec1, vec2);
 6953 
 6954     vptest(vec1, vec1);
 6955     jccb(Assembler::notZero, FALSE_LABEL);
 6956     jmpb(TRUE_LABEL);
 6957 
</pre>
<hr />
<pre>
 7143     addptr(to, 8);
 7144     BIND(L_fill_8_bytes);
 7145     subl(count, 1 &lt;&lt; (shift + 1));
 7146     jcc(Assembler::greaterEqual, L_fill_8_bytes_loop);
 7147     // fall through to fill 4 bytes
 7148   } else {
 7149     Label L_fill_32_bytes;
 7150     if (!UseUnalignedLoadStores) {
 7151       // align to 8 bytes, we know we are 4 byte aligned to start
 7152       testptr(to, 4);
 7153       jccb(Assembler::zero, L_fill_32_bytes);
 7154       movl(Address(to, 0), value);
 7155       addptr(to, 4);
 7156       subl(count, 1&lt;&lt;shift);
 7157     }
 7158     BIND(L_fill_32_bytes);
 7159     {
 7160       assert( UseSSE &gt;= 2, &quot;supported cpu only&quot; );
 7161       Label L_fill_32_bytes_loop, L_check_fill_8_bytes, L_fill_8_bytes_loop, L_fill_8_bytes;
 7162       movdl(xtmp, value);
<span class="line-modified"> 7163       if (UseAVX &gt; 2 &amp;&amp; UseUnalignedLoadStores) {</span>
<span class="line-modified"> 7164         // Fill 64-byte chunks</span>
<span class="line-modified"> 7165         Label L_fill_64_bytes_loop, L_check_fill_32_bytes;</span>
<span class="line-modified"> 7166         vpbroadcastd(xtmp, xtmp, Assembler::AVX_512bit);</span>
<span class="line-modified"> 7167 </span>
<span class="line-modified"> 7168         subl(count, 16 &lt;&lt; shift);</span>
<span class="line-modified"> 7169         jcc(Assembler::less, L_check_fill_32_bytes);</span>
<span class="line-modified"> 7170         align(16);</span>
<span class="line-modified"> 7171 </span>
<span class="line-modified"> 7172         BIND(L_fill_64_bytes_loop);</span>
<span class="line-modified"> 7173         evmovdqul(Address(to, 0), xtmp, Assembler::AVX_512bit);</span>
<span class="line-modified"> 7174         addptr(to, 64);</span>
<span class="line-modified"> 7175         subl(count, 16 &lt;&lt; shift);</span>
<span class="line-modified"> 7176         jcc(Assembler::greaterEqual, L_fill_64_bytes_loop);</span>
<span class="line-modified"> 7177 </span>
<span class="line-modified"> 7178         BIND(L_check_fill_32_bytes);</span>
<span class="line-modified"> 7179         addl(count, 8 &lt;&lt; shift);</span>
<span class="line-modified"> 7180         jccb(Assembler::less, L_check_fill_8_bytes);</span>
<span class="line-modified"> 7181         vmovdqu(Address(to, 0), xtmp);</span>
<span class="line-modified"> 7182         addptr(to, 32);</span>
<span class="line-modified"> 7183         subl(count, 8 &lt;&lt; shift);</span>
<span class="line-modified"> 7184 </span>
<span class="line-modified"> 7185         BIND(L_check_fill_8_bytes);</span>
<span class="line-modified"> 7186       } else if (UseAVX == 2 &amp;&amp; UseUnalignedLoadStores) {</span>

 7187         // Fill 64-byte chunks
<span class="line-modified"> 7188         Label L_fill_64_bytes_loop, L_check_fill_32_bytes;</span>
 7189         vpbroadcastd(xtmp, xtmp, Assembler::AVX_256bit);
 7190 
 7191         subl(count, 16 &lt;&lt; shift);
 7192         jcc(Assembler::less, L_check_fill_32_bytes);
 7193         align(16);
 7194 
 7195         BIND(L_fill_64_bytes_loop);
 7196         vmovdqu(Address(to, 0), xtmp);
 7197         vmovdqu(Address(to, 32), xtmp);
 7198         addptr(to, 64);
 7199         subl(count, 16 &lt;&lt; shift);
 7200         jcc(Assembler::greaterEqual, L_fill_64_bytes_loop);
 7201 
 7202         BIND(L_check_fill_32_bytes);
 7203         addl(count, 8 &lt;&lt; shift);
 7204         jccb(Assembler::less, L_check_fill_8_bytes);
 7205         vmovdqu(Address(to, 0), xtmp);
 7206         addptr(to, 32);
 7207         subl(count, 8 &lt;&lt; shift);
 7208 
</pre>
<hr />
<pre>
 7882   pop(tmp3);
 7883   pop(tmp2);
 7884   pop(tmp1);
 7885 }
 7886 
 7887 void MacroAssembler::vectorized_mismatch(Register obja, Register objb, Register length, Register log2_array_indxscale,
 7888   Register result, Register tmp1, Register tmp2, XMMRegister rymm0, XMMRegister rymm1, XMMRegister rymm2){
 7889   assert(UseSSE42Intrinsics, &quot;SSE4.2 must be enabled.&quot;);
 7890   Label VECTOR16_LOOP, VECTOR8_LOOP, VECTOR4_LOOP;
 7891   Label VECTOR8_TAIL, VECTOR4_TAIL;
 7892   Label VECTOR32_NOT_EQUAL, VECTOR16_NOT_EQUAL, VECTOR8_NOT_EQUAL, VECTOR4_NOT_EQUAL;
 7893   Label SAME_TILL_END, DONE;
 7894   Label BYTES_LOOP, BYTES_TAIL, BYTES_NOT_EQUAL;
 7895 
 7896   //scale is in rcx in both Win64 and Unix
 7897   ShortBranchVerifier sbv(this);
 7898 
 7899   shlq(length);
 7900   xorq(result, result);
 7901 
<span class="line-modified"> 7902   if ((UseAVX &gt; 2) &amp;&amp;</span>
 7903       VM_Version::supports_avx512vlbw()) {
 7904     Label VECTOR64_LOOP, VECTOR64_NOT_EQUAL, VECTOR32_TAIL;
 7905 
 7906     cmpq(length, 64);
 7907     jcc(Assembler::less, VECTOR32_TAIL);

 7908     movq(tmp1, length);
 7909     andq(tmp1, 0x3F);      // tail count
 7910     andq(length, ~(0x3F)); //vector count
 7911 
 7912     bind(VECTOR64_LOOP);
 7913     // AVX512 code to compare 64 byte vectors.
 7914     evmovdqub(rymm0, Address(obja, result), Assembler::AVX_512bit);
 7915     evpcmpeqb(k7, rymm0, Address(objb, result), Assembler::AVX_512bit);
 7916     kortestql(k7, k7);
 7917     jcc(Assembler::aboveEqual, VECTOR64_NOT_EQUAL);     // mismatch
 7918     addq(result, 64);
 7919     subq(length, 64);
 7920     jccb(Assembler::notZero, VECTOR64_LOOP);
 7921 
 7922     //bind(VECTOR64_TAIL);
 7923     testq(tmp1, tmp1);
 7924     jcc(Assembler::zero, SAME_TILL_END);
 7925 
 7926     //bind(VECTOR64_TAIL);
 7927     // AVX512 code to compare upto 63 byte vectors.
</pre>
<hr />
<pre>
 8712   // Align buffer to 16 bytes
 8713   movl(tmp, buf);
 8714   andl(tmp, 0xF);
 8715   jccb(Assembler::zero, L_aligned);
 8716   subl(tmp,  16);
 8717   addl(len, tmp);
 8718 
 8719   align(4);
 8720   BIND(L_align_loop);
 8721   movsbl(rax, Address(buf, 0)); // load byte with sign extension
 8722   update_byte_crc32(crc, rax, table);
 8723   increment(buf);
 8724   incrementl(tmp);
 8725   jccb(Assembler::less, L_align_loop);
 8726 
 8727   BIND(L_aligned);
 8728   movl(tmp, len); // save
 8729   shrl(len, 4);
 8730   jcc(Assembler::zero, L_tail_restore);
 8731 
<span class="line-removed"> 8732   // Fold total 512 bits of polynomial on each iteration</span>
<span class="line-removed"> 8733   if (VM_Version::supports_vpclmulqdq()) {</span>
<span class="line-removed"> 8734     Label Parallel_loop, L_No_Parallel;</span>
<span class="line-removed"> 8735 </span>
<span class="line-removed"> 8736     cmpl(len, 8);</span>
<span class="line-removed"> 8737     jccb(Assembler::less, L_No_Parallel);</span>
<span class="line-removed"> 8738 </span>
<span class="line-removed"> 8739     movdqu(xmm0, ExternalAddress(StubRoutines::x86::crc_by128_masks_addr() + 32));</span>
<span class="line-removed"> 8740     evmovdquq(xmm1, Address(buf, 0), Assembler::AVX_512bit);</span>
<span class="line-removed"> 8741     movdl(xmm5, crc);</span>
<span class="line-removed"> 8742     evpxorq(xmm1, xmm1, xmm5, Assembler::AVX_512bit);</span>
<span class="line-removed"> 8743     addptr(buf, 64);</span>
<span class="line-removed"> 8744     subl(len, 7);</span>
<span class="line-removed"> 8745     evshufi64x2(xmm0, xmm0, xmm0, 0x00, Assembler::AVX_512bit); //propagate the mask from 128 bits to 512 bits</span>
<span class="line-removed"> 8746 </span>
<span class="line-removed"> 8747     BIND(Parallel_loop);</span>
<span class="line-removed"> 8748     fold_128bit_crc32_avx512(xmm1, xmm0, xmm5, buf, 0);</span>
<span class="line-removed"> 8749     addptr(buf, 64);</span>
<span class="line-removed"> 8750     subl(len, 4);</span>
<span class="line-removed"> 8751     jcc(Assembler::greater, Parallel_loop);</span>
<span class="line-removed"> 8752 </span>
<span class="line-removed"> 8753     vextracti64x2(xmm2, xmm1, 0x01);</span>
<span class="line-removed"> 8754     vextracti64x2(xmm3, xmm1, 0x02);</span>
<span class="line-removed"> 8755     vextracti64x2(xmm4, xmm1, 0x03);</span>
<span class="line-removed"> 8756     jmp(L_fold_512b);</span>
<span class="line-removed"> 8757 </span>
<span class="line-removed"> 8758     BIND(L_No_Parallel);</span>
<span class="line-removed"> 8759   }</span>
 8760   // Fold crc into first bytes of vector
 8761   movdqa(xmm1, Address(buf, 0));
 8762   movdl(rax, xmm1);
 8763   xorl(crc, rax);
 8764   if (VM_Version::supports_sse4_1()) {
 8765     pinsrd(xmm1, crc, 0);
 8766   } else {
 8767     pinsrw(xmm1, crc, 0);
 8768     shrl(crc, 16);
 8769     pinsrw(xmm1, crc, 1);
 8770   }
 8771   addptr(buf, 16);
 8772   subl(len, 4); // len &gt; 0
 8773   jcc(Assembler::less, L_fold_tail);
 8774 
 8775   movdqa(xmm2, Address(buf,  0));
 8776   movdqa(xmm3, Address(buf, 16));
 8777   movdqa(xmm4, Address(buf, 32));
 8778   addptr(buf, 48);
 8779   subl(len, 3);
</pre>
<hr />
<pre>
 9344   XMMRegister tmp1Reg, XMMRegister tmp2Reg,
 9345   XMMRegister tmp3Reg, XMMRegister tmp4Reg,
 9346   Register tmp5, Register result) {
 9347   Label copy_chars_loop, return_length, return_zero, done;
 9348 
 9349   // rsi: src
 9350   // rdi: dst
 9351   // rdx: len
 9352   // rcx: tmp5
 9353   // rax: result
 9354 
 9355   // rsi holds start addr of source char[] to be compressed
 9356   // rdi holds start addr of destination byte[]
 9357   // rdx holds length
 9358 
 9359   assert(len != result, &quot;&quot;);
 9360 
 9361   // save length for return
 9362   push(len);
 9363 
<span class="line-modified"> 9364   if ((UseAVX &gt; 2) &amp;&amp; // AVX512</span>
 9365     VM_Version::supports_avx512vlbw() &amp;&amp;
 9366     VM_Version::supports_bmi2()) {
 9367 
 9368     Label copy_32_loop, copy_loop_tail, below_threshold;
 9369 
 9370     // alignment
 9371     Label post_alignment;
 9372 
 9373     // if length of the string is less than 16, handle it in an old fashioned way
 9374     testl(len, -32);
 9375     jcc(Assembler::zero, below_threshold);
 9376 
 9377     // First check whether a character is compressable ( &lt;= 0xFF).
 9378     // Create mask to test for Unicode chars inside zmm vector
 9379     movl(result, 0x00FF);
 9380     evpbroadcastw(tmp2Reg, result, Assembler::AVX_512bit);
 9381 
 9382     testl(len, -64);
 9383     jcc(Assembler::zero, post_alignment);
 9384 
</pre>
<hr />
<pre>
 9536   jmpb(done);
 9537 
 9538   // if compression failed, return 0
 9539   bind(return_zero);
 9540   xorl(result, result);
 9541   addptr(rsp, wordSize);
 9542 
 9543   bind(done);
 9544 }
 9545 
 9546 // Inflate byte[] array to char[].
 9547 //   ..\jdk\src\java.base\share\classes\java\lang\StringLatin1.java
 9548 //   @HotSpotIntrinsicCandidate
 9549 //   private static void inflate(byte[] src, int srcOff, char[] dst, int dstOff, int len) {
 9550 //     for (int i = 0; i &lt; len; i++) {
 9551 //       dst[dstOff++] = (char)(src[srcOff++] &amp; 0xff);
 9552 //     }
 9553 //   }
 9554 void MacroAssembler::byte_array_inflate(Register src, Register dst, Register len,
 9555   XMMRegister tmp1, Register tmp2) {
<span class="line-modified"> 9556   Label copy_chars_loop, done, below_threshold;</span>
 9557   // rsi: src
 9558   // rdi: dst
 9559   // rdx: len
 9560   // rcx: tmp2
 9561 
 9562   // rsi holds start addr of source byte[] to be inflated
 9563   // rdi holds start addr of destination char[]
 9564   // rdx holds length
 9565   assert_different_registers(src, dst, len, tmp2);
<span class="line-modified"> 9566 </span>
 9567   if ((UseAVX &gt; 2) &amp;&amp; // AVX512
 9568     VM_Version::supports_avx512vlbw() &amp;&amp;
 9569     VM_Version::supports_bmi2()) {
 9570 
 9571     Label copy_32_loop, copy_tail;
 9572     Register tmp3_aliased = len;
 9573 
 9574     // if length of the string is less than 16, handle it in an old fashioned way
 9575     testl(len, -16);
 9576     jcc(Assembler::zero, below_threshold);
 9577 



 9578     // In order to use only one arithmetic operation for the main loop we use
 9579     // this pre-calculation
<span class="line-removed"> 9580     movl(tmp2, len);</span>
 9581     andl(tmp2, (32 - 1)); // tail count (in chars), 32 element wide loop
 9582     andl(len, -32);     // vector count
 9583     jccb(Assembler::zero, copy_tail);
 9584 
 9585     lea(src, Address(src, len, Address::times_1));
 9586     lea(dst, Address(dst, len, Address::times_2));
 9587     negptr(len);
 9588 
 9589 
 9590     // inflate 32 chars per iter
 9591     bind(copy_32_loop);
 9592     vpmovzxbw(tmp1, Address(src, len, Address::times_1), Assembler::AVX_512bit);
 9593     evmovdquw(Address(dst, len, Address::times_2), tmp1, Assembler::AVX_512bit);
 9594     addptr(len, 32);
 9595     jcc(Assembler::notZero, copy_32_loop);
 9596 
 9597     bind(copy_tail);
 9598     // bail out when there is nothing to be done
 9599     testl(tmp2, -1); // we don&#39;t destroy the contents of tmp2 here
 9600     jcc(Assembler::zero, done);
 9601 
 9602     // ~(~0 &lt;&lt; length), where length is the # of remaining elements to process
 9603     movl(tmp3_aliased, -1);
 9604     shlxl(tmp3_aliased, tmp3_aliased, tmp2);
 9605     notl(tmp3_aliased);
 9606     kmovdl(k2, tmp3_aliased);
 9607     evpmovzxbw(tmp1, k2, Address(src, 0), Assembler::AVX_512bit);
 9608     evmovdquw(Address(dst, 0), k2, tmp1, Assembler::AVX_512bit);
 9609 
 9610     jmp(done);

 9611   }
 9612   if (UseSSE42Intrinsics) {
 9613     Label copy_16_loop, copy_8_loop, copy_bytes, copy_new_tail, copy_tail;
 9614 
<span class="line-removed"> 9615     movl(tmp2, len);</span>
<span class="line-removed"> 9616 </span>
 9617     if (UseAVX &gt; 1) {
 9618       andl(tmp2, (16 - 1));
 9619       andl(len, -16);
 9620       jccb(Assembler::zero, copy_new_tail);
 9621     } else {
 9622       andl(tmp2, 0x00000007);   // tail count (in chars)
 9623       andl(len, 0xfffffff8);    // vector count (in chars)
 9624       jccb(Assembler::zero, copy_tail);
 9625     }
 9626 
 9627     // vectored inflation
 9628     lea(src, Address(src, len, Address::times_1));
 9629     lea(dst, Address(dst, len, Address::times_2));
 9630     negptr(len);
 9631 
 9632     if (UseAVX &gt; 1) {
 9633       bind(copy_16_loop);
 9634       vpmovzxbw(tmp1, Address(src, len, Address::times_1), Assembler::AVX_256bit);
 9635       vmovdqu(Address(dst, len, Address::times_2), tmp1);
 9636       addptr(len, 16);
 9637       jcc(Assembler::notZero, copy_16_loop);
 9638 
 9639       bind(below_threshold);
 9640       bind(copy_new_tail);
<span class="line-modified"> 9641       if ((UseAVX &gt; 2) &amp;&amp;</span>
<span class="line-removed"> 9642         VM_Version::supports_avx512vlbw() &amp;&amp;</span>
<span class="line-removed"> 9643         VM_Version::supports_bmi2()) {</span>
<span class="line-removed"> 9644         movl(tmp2, len);</span>
<span class="line-removed"> 9645       } else {</span>
<span class="line-removed"> 9646         movl(len, tmp2);</span>
<span class="line-removed"> 9647       }</span>
 9648       andl(tmp2, 0x00000007);
 9649       andl(len, 0xFFFFFFF8);
 9650       jccb(Assembler::zero, copy_tail);
 9651 
 9652       pmovzxbw(tmp1, Address(src, 0));
 9653       movdqu(Address(dst, 0), tmp1);
 9654       addptr(src, 8);
 9655       addptr(dst, 2 * 8);
 9656 
 9657       jmp(copy_tail, true);
 9658     }
 9659 
 9660     // inflate 8 chars per iter
 9661     bind(copy_8_loop);
 9662     pmovzxbw(tmp1, Address(src, len, Address::times_1));  // unpack to 8 words
 9663     movdqu(Address(dst, len, Address::times_2), tmp1);
 9664     addptr(len, 8);
 9665     jcc(Assembler::notZero, copy_8_loop);
 9666 
 9667     bind(copy_tail);
</pre>
<hr />
<pre>
 9681   } else {
 9682     bind(below_threshold);
 9683   }
 9684 
 9685   testl(len, len);
 9686   jccb(Assembler::zero, done);
 9687   lea(src, Address(src, len, Address::times_1));
 9688   lea(dst, Address(dst, len, Address::times_2));
 9689   negptr(len);
 9690 
 9691   // inflate 1 char per iter
 9692   bind(copy_chars_loop);
 9693   load_unsigned_byte(tmp2, Address(src, len, Address::times_1));  // load byte char
 9694   movw(Address(dst, len, Address::times_2), tmp2);  // inflate byte char to word
 9695   increment(len);
 9696   jcc(Assembler::notZero, copy_chars_loop);
 9697 
 9698   bind(done);
 9699 }
 9700 



























































































 9701 Assembler::Condition MacroAssembler::negate_condition(Assembler::Condition cond) {
 9702   switch (cond) {
 9703     // Note some conditions are synonyms for others
 9704     case Assembler::zero:         return Assembler::notZero;
 9705     case Assembler::notZero:      return Assembler::zero;
 9706     case Assembler::less:         return Assembler::greaterEqual;
 9707     case Assembler::lessEqual:    return Assembler::greater;
 9708     case Assembler::greater:      return Assembler::lessEqual;
 9709     case Assembler::greaterEqual: return Assembler::less;
 9710     case Assembler::below:        return Assembler::aboveEqual;
 9711     case Assembler::belowEqual:   return Assembler::above;
 9712     case Assembler::above:        return Assembler::belowEqual;
 9713     case Assembler::aboveEqual:   return Assembler::below;
 9714     case Assembler::overflow:     return Assembler::noOverflow;
 9715     case Assembler::noOverflow:   return Assembler::overflow;
 9716     case Assembler::negative:     return Assembler::positive;
 9717     case Assembler::positive:     return Assembler::negative;
 9718     case Assembler::parity:       return Assembler::noParity;
 9719     case Assembler::noParity:     return Assembler::parity;
 9720   }
</pre>
<hr />
<pre>
 9753 #endif
 9754 
 9755   MacroAssembler::call_VM_leaf_base(CAST_FROM_FN_PTR(address, Thread::current), 0);
 9756 
 9757 #ifdef _LP64
 9758   pop(r11);
 9759   pop(r10);
 9760   pop(r9);
 9761   pop(r8);
 9762 #endif
 9763   pop(rcx);
 9764   pop(rdx);
 9765   LP64_ONLY(pop(rsi);)
 9766   LP64_ONLY(pop(rdi);)
 9767   if (thread != rax) {
 9768     mov(thread, rax);
 9769     pop(rax);
 9770   }
 9771 }
 9772 
<span class="line-modified"> 9773 #endif</span>
</pre>
</td>
<td>
<hr />
<pre>
    1 /*
<span class="line-modified">    2  * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.</span>
    3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
    4  *
    5  * This code is free software; you can redistribute it and/or modify it
    6  * under the terms of the GNU General Public License version 2 only, as
    7  * published by the Free Software Foundation.
    8  *
    9  * This code is distributed in the hope that it will be useful, but WITHOUT
   10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
   11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
   12  * version 2 for more details (a copy is included in the LICENSE file that
   13  * accompanied this code).
   14  *
   15  * You should have received a copy of the GNU General Public License version
   16  * 2 along with this work; if not, write to the Free Software Foundation,
   17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
   18  *
   19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
   20  * or visit www.oracle.com if you need additional information or have any
   21  * questions.
   22  *
   23  */
   24 
   25 #include &quot;precompiled.hpp&quot;
   26 #include &quot;jvm.h&quot;
   27 #include &quot;asm/assembler.hpp&quot;
   28 #include &quot;asm/assembler.inline.hpp&quot;
   29 #include &quot;compiler/disassembler.hpp&quot;
   30 #include &quot;gc/shared/barrierSet.hpp&quot;
   31 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
   32 #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
   33 #include &quot;interpreter/interpreter.hpp&quot;
   34 #include &quot;memory/resourceArea.hpp&quot;
   35 #include &quot;memory/universe.hpp&quot;
   36 #include &quot;oops/accessDecorators.hpp&quot;
<span class="line-added">   37 #include &quot;oops/compressedOops.inline.hpp&quot;</span>
   38 #include &quot;oops/klass.inline.hpp&quot;
   39 #include &quot;prims/methodHandles.hpp&quot;
   40 #include &quot;runtime/biasedLocking.hpp&quot;
   41 #include &quot;runtime/flags/flagSetting.hpp&quot;
   42 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
   43 #include &quot;runtime/objectMonitor.hpp&quot;
   44 #include &quot;runtime/os.hpp&quot;
   45 #include &quot;runtime/safepoint.hpp&quot;
   46 #include &quot;runtime/safepointMechanism.hpp&quot;
   47 #include &quot;runtime/sharedRuntime.hpp&quot;
   48 #include &quot;runtime/stubRoutines.hpp&quot;
   49 #include &quot;runtime/thread.hpp&quot;
   50 #include &quot;utilities/macros.hpp&quot;
   51 #include &quot;crc32c.h&quot;
   52 #ifdef COMPILER2
   53 #include &quot;opto/intrinsicnode.hpp&quot;
   54 #endif
   55 
   56 #ifdef PRODUCT
   57 #define BLOCK_COMMENT(str) /* nothing */
</pre>
<hr />
<pre>
  332   movl(as_Address(dst), src);
  333 }
  334 
  335 void MacroAssembler::movptr(Register dst, ArrayAddress src) {
  336   movl(dst, as_Address(src));
  337 }
  338 
  339 // src should NEVER be a real pointer. Use AddressLiteral for true pointers
  340 void MacroAssembler::movptr(Address dst, intptr_t src) {
  341   movl(dst, src);
  342 }
  343 
  344 
  345 void MacroAssembler::pop_callee_saved_registers() {
  346   pop(rcx);
  347   pop(rdx);
  348   pop(rdi);
  349   pop(rsi);
  350 }
  351 





  352 void MacroAssembler::push_callee_saved_registers() {
  353   push(rsi);
  354   push(rdi);
  355   push(rdx);
  356   push(rcx);
  357 }
  358 






  359 void MacroAssembler::pushoop(jobject obj) {
  360   push_literal32((int32_t)obj, oop_Relocation::spec_for_immediate());
  361 }
  362 
  363 void MacroAssembler::pushklass(Metadata* obj) {
  364   push_literal32((int32_t)obj, metadata_Relocation::spec_for_immediate());
  365 }
  366 
  367 void MacroAssembler::pushptr(AddressLiteral src) {
  368   if (src.is_lval()) {
  369     push_literal32((int32_t)src.target(), src.rspec());
  370   } else {
  371     pushl(as_Address(src));
  372   }
  373 }
  374 
  375 void MacroAssembler::set_word_if_not_zero(Register dst) {
  376   xorl(dst, dst);
  377   set_byte_if_not_zero(dst);
  378 }
</pre>
<hr />
<pre>
  399 
  400 void MacroAssembler::debug32(int rdi, int rsi, int rbp, int rsp, int rbx, int rdx, int rcx, int rax, int eip, char* msg) {
  401   // In order to get locks to work, we need to fake a in_VM state
  402   JavaThread* thread = JavaThread::current();
  403   JavaThreadState saved_state = thread-&gt;thread_state();
  404   thread-&gt;set_thread_state(_thread_in_vm);
  405   if (ShowMessageBoxOnError) {
  406     JavaThread* thread = JavaThread::current();
  407     JavaThreadState saved_state = thread-&gt;thread_state();
  408     thread-&gt;set_thread_state(_thread_in_vm);
  409     if (CountBytecodes || TraceBytecodes || StopInterpreterAt) {
  410       ttyLocker ttyl;
  411       BytecodeCounter::print();
  412     }
  413     // To see where a verify_oop failed, get $ebx+40/X for this frame.
  414     // This is the value of eip which points to where verify_oop will return.
  415     if (os::message_box(msg, &quot;Execution stopped, print registers?&quot;)) {
  416       print_state32(rdi, rsi, rbp, rsp, rbx, rdx, rcx, rax, eip);
  417       BREAKPOINT;
  418     }



  419   }
<span class="line-modified">  420   fatal(&quot;DEBUG MESSAGE: %s&quot;, msg);</span>


  421 }
  422 
  423 void MacroAssembler::print_state32(int rdi, int rsi, int rbp, int rsp, int rbx, int rdx, int rcx, int rax, int eip) {
  424   ttyLocker ttyl;
  425   FlagSetting fs(Debugging, true);
  426   tty-&gt;print_cr(&quot;eip = 0x%08x&quot;, eip);
  427 #ifndef PRODUCT
  428   if ((WizardMode || Verbose) &amp;&amp; PrintMiscellaneous) {
  429     tty-&gt;cr();
  430     findpc(eip);
  431     tty-&gt;cr();
  432   }
  433 #endif
  434 #define PRINT_REG(rax) \
  435   { tty-&gt;print(&quot;%s = &quot;, #rax); os::print_location(tty, rax); }
  436   PRINT_REG(rax);
  437   PRINT_REG(rbx);
  438   PRINT_REG(rcx);
  439   PRINT_REG(rdx);
  440   PRINT_REG(rdi);
</pre>
<hr />
<pre>
  796 
  797 static void pass_arg1(MacroAssembler* masm, Register arg) {
  798   if (c_rarg1 != arg ) {
  799     masm-&gt;mov(c_rarg1, arg);
  800   }
  801 }
  802 
  803 static void pass_arg2(MacroAssembler* masm, Register arg) {
  804   if (c_rarg2 != arg ) {
  805     masm-&gt;mov(c_rarg2, arg);
  806   }
  807 }
  808 
  809 static void pass_arg3(MacroAssembler* masm, Register arg) {
  810   if (c_rarg3 != arg ) {
  811     masm-&gt;mov(c_rarg3, arg);
  812   }
  813 }
  814 
  815 void MacroAssembler::stop(const char* msg) {
<span class="line-modified">  816   if (ShowMessageBoxOnError) {</span>
<span class="line-modified">  817     address rip = pc();</span>
<span class="line-added">  818     pusha(); // get regs on stack</span>
<span class="line-added">  819     lea(c_rarg1, InternalAddress(rip));</span>
<span class="line-added">  820     movq(c_rarg2, rsp); // pass pointer to regs array</span>
<span class="line-added">  821   }</span>
  822   lea(c_rarg0, ExternalAddress((address) msg));


  823   andq(rsp, -16); // align stack as required by ABI
  824   call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug64)));
  825   hlt();
  826 }
  827 
  828 void MacroAssembler::warn(const char* msg) {
  829   push(rbp);
  830   movq(rbp, rsp);
  831   andq(rsp, -16);     // align stack as required by push_CPU_state and call
  832   push_CPU_state();   // keeps alignment at 16 bytes
  833   lea(c_rarg0, ExternalAddress((address) msg));
  834   lea(rax, ExternalAddress(CAST_FROM_FN_PTR(address, warning)));
  835   call(rax);
  836   pop_CPU_state();
  837   mov(rsp, rbp);
  838   pop(rbp);
  839 }
  840 
  841 void MacroAssembler::print_state() {
  842   address rip = pc();
</pre>
<hr />
<pre>
  861 #endif
  862 
  863 void MacroAssembler::debug64(char* msg, int64_t pc, int64_t regs[]) {
  864   // In order to get locks to work, we need to fake a in_VM state
  865   if (ShowMessageBoxOnError) {
  866     JavaThread* thread = JavaThread::current();
  867     JavaThreadState saved_state = thread-&gt;thread_state();
  868     thread-&gt;set_thread_state(_thread_in_vm);
  869 #ifndef PRODUCT
  870     if (CountBytecodes || TraceBytecodes || StopInterpreterAt) {
  871       ttyLocker ttyl;
  872       BytecodeCounter::print();
  873     }
  874 #endif
  875     // To see where a verify_oop failed, get $ebx+40/X for this frame.
  876     // XXX correct this offset for amd64
  877     // This is the value of eip which points to where verify_oop will return.
  878     if (os::message_box(msg, &quot;Execution stopped, print registers?&quot;)) {
  879       print_state64(pc, regs);
  880       BREAKPOINT;

  881     }






  882   }
<span class="line-added">  883   fatal(&quot;DEBUG MESSAGE: %s&quot;, msg);</span>
  884 }
  885 
  886 void MacroAssembler::print_state64(int64_t pc, int64_t regs[]) {
  887   ttyLocker ttyl;
  888   FlagSetting fs(Debugging, true);
  889   tty-&gt;print_cr(&quot;rip = 0x%016lx&quot;, (intptr_t)pc);
  890 #ifndef PRODUCT
  891   tty-&gt;cr();
  892   findpc(pc);
  893   tty-&gt;cr();
  894 #endif
  895 #define PRINT_REG(rax, value) \
  896   { tty-&gt;print(&quot;%s = &quot;, #rax); os::print_location(tty, value); }
  897   PRINT_REG(rax, regs[15]);
  898   PRINT_REG(rbx, regs[12]);
  899   PRINT_REG(rcx, regs[14]);
  900   PRINT_REG(rdx, regs[13]);
  901   PRINT_REG(rdi, regs[8]);
  902   PRINT_REG(rsi, regs[9]);
  903   PRINT_REG(rbp, regs[10]);
</pre>
<hr />
<pre>
  967 
  968 void MacroAssembler::addpd(XMMRegister dst, AddressLiteral src) {
  969   if (reachable(src)) {
  970     Assembler::addpd(dst, as_Address(src));
  971   } else {
  972     lea(rscratch1, src);
  973     Assembler::addpd(dst, Address(rscratch1, 0));
  974   }
  975 }
  976 
  977 void MacroAssembler::align(int modulus) {
  978   align(modulus, offset());
  979 }
  980 
  981 void MacroAssembler::align(int modulus, int target) {
  982   if (target % modulus != 0) {
  983     nop(modulus - (target % modulus));
  984   }
  985 }
  986 
<span class="line-modified">  987 void MacroAssembler::andpd(XMMRegister dst, AddressLiteral src, Register scratch_reg) {</span>
  988   // Used in sign-masking with aligned address.
  989   assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
  990   if (reachable(src)) {
  991     Assembler::andpd(dst, as_Address(src));
  992   } else {
<span class="line-modified">  993     lea(scratch_reg, src);</span>
<span class="line-modified">  994     Assembler::andpd(dst, Address(scratch_reg, 0));</span>
  995   }
  996 }
  997 
<span class="line-modified">  998 void MacroAssembler::andps(XMMRegister dst, AddressLiteral src, Register scratch_reg) {</span>
  999   // Used in sign-masking with aligned address.
 1000   assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
 1001   if (reachable(src)) {
 1002     Assembler::andps(dst, as_Address(src));
 1003   } else {
<span class="line-modified"> 1004     lea(scratch_reg, src);</span>
<span class="line-modified"> 1005     Assembler::andps(dst, Address(scratch_reg, 0));</span>
 1006   }
 1007 }
 1008 
 1009 void MacroAssembler::andptr(Register dst, int32_t imm32) {
 1010   LP64_ONLY(andq(dst, imm32)) NOT_LP64(andl(dst, imm32));
 1011 }
 1012 
 1013 void MacroAssembler::atomic_incl(Address counter_addr) {
 1014   lock();
 1015   incrementl(counter_addr);
 1016 }
 1017 
 1018 void MacroAssembler::atomic_incl(AddressLiteral counter_addr, Register scr) {
 1019   if (reachable(counter_addr)) {
 1020     atomic_incl(as_Address(counter_addr));
 1021   } else {
 1022     lea(scr, counter_addr);
 1023     atomic_incl(Address(scr, 0));
 1024   }
 1025 }
</pre>
<hr />
<pre>
 1078 
 1079     call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), thread);
 1080     jump(RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()));
 1081     should_not_reach_here();
 1082 
 1083     bind(no_reserved_zone_enabling);
 1084 }
 1085 
 1086 int MacroAssembler::biased_locking_enter(Register lock_reg,
 1087                                          Register obj_reg,
 1088                                          Register swap_reg,
 1089                                          Register tmp_reg,
 1090                                          bool swap_reg_contains_mark,
 1091                                          Label&amp; done,
 1092                                          Label* slow_case,
 1093                                          BiasedLockingCounters* counters) {
 1094   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 1095   assert(swap_reg == rax, &quot;swap_reg must be rax for cmpxchgq&quot;);
 1096   assert(tmp_reg != noreg, &quot;tmp_reg must be supplied&quot;);
 1097   assert_different_registers(lock_reg, obj_reg, swap_reg, tmp_reg);
<span class="line-modified"> 1098   assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits, &quot;biased locking makes assumptions about bit layout&quot;);</span>
 1099   Address mark_addr      (obj_reg, oopDesc::mark_offset_in_bytes());
 1100   NOT_LP64( Address saved_mark_addr(lock_reg, 0); )
 1101 
 1102   if (PrintBiasedLockingStatistics &amp;&amp; counters == NULL) {
 1103     counters = BiasedLocking::counters();
 1104   }
 1105   // Biased locking
 1106   // See whether the lock is currently biased toward our thread and
 1107   // whether the epoch is still valid
 1108   // Note that the runtime guarantees sufficient alignment of JavaThread
 1109   // pointers to allow age to be placed into low bits
 1110   // First check to see whether biasing is even enabled for this object
 1111   Label cas_label;
 1112   int null_check_offset = -1;
 1113   if (!swap_reg_contains_mark) {
 1114     null_check_offset = offset();
 1115     movptr(swap_reg, mark_addr);
 1116   }
 1117   movptr(tmp_reg, swap_reg);
<span class="line-modified"> 1118   andptr(tmp_reg, markWord::biased_lock_mask_in_place);</span>
<span class="line-modified"> 1119   cmpptr(tmp_reg, markWord::biased_lock_pattern);</span>
 1120   jcc(Assembler::notEqual, cas_label);
 1121   // The bias pattern is present in the object&#39;s header. Need to check
 1122   // whether the bias owner and the epoch are both still current.
 1123 #ifndef _LP64
 1124   // Note that because there is no current thread register on x86_32 we
 1125   // need to store off the mark word we read out of the object to
 1126   // avoid reloading it and needing to recheck invariants below. This
 1127   // store is unfortunate but it makes the overall code shorter and
 1128   // simpler.
 1129   movptr(saved_mark_addr, swap_reg);
 1130 #endif
 1131   if (swap_reg_contains_mark) {
 1132     null_check_offset = offset();
 1133   }
 1134   load_prototype_header(tmp_reg, obj_reg);
 1135 #ifdef _LP64
 1136   orptr(tmp_reg, r15_thread);
 1137   xorptr(tmp_reg, swap_reg);
 1138   Register header_reg = tmp_reg;
 1139 #else
 1140   xorptr(tmp_reg, swap_reg);
 1141   get_thread(swap_reg);
 1142   xorptr(swap_reg, tmp_reg);
 1143   Register header_reg = swap_reg;
 1144 #endif
<span class="line-modified"> 1145   andptr(header_reg, ~((int) markWord::age_mask_in_place));</span>
 1146   if (counters != NULL) {
 1147     cond_inc32(Assembler::zero,
 1148                ExternalAddress((address) counters-&gt;biased_lock_entry_count_addr()));
 1149   }
 1150   jcc(Assembler::equal, done);
 1151 
 1152   Label try_revoke_bias;
 1153   Label try_rebias;
 1154 
 1155   // At this point we know that the header has the bias pattern and
 1156   // that we are not the bias owner in the current epoch. We need to
 1157   // figure out more details about the state of the header in order to
 1158   // know what operations can be legally performed on the object&#39;s
 1159   // header.
 1160 
 1161   // If the low three bits in the xor result aren&#39;t clear, that means
 1162   // the prototype header is no longer biased and we have to revoke
 1163   // the bias on this object.
<span class="line-modified"> 1164   testptr(header_reg, markWord::biased_lock_mask_in_place);</span>
 1165   jccb(Assembler::notZero, try_revoke_bias);
 1166 
 1167   // Biasing is still enabled for this data type. See whether the
 1168   // epoch of the current bias is still valid, meaning that the epoch
 1169   // bits of the mark word are equal to the epoch bits of the
 1170   // prototype header. (Note that the prototype header&#39;s epoch bits
 1171   // only change at a safepoint.) If not, attempt to rebias the object
 1172   // toward the current thread. Note that we must be absolutely sure
 1173   // that the current epoch is invalid in order to do this because
 1174   // otherwise the manipulations it performs on the mark word are
 1175   // illegal.
<span class="line-modified"> 1176   testptr(header_reg, markWord::epoch_mask_in_place);</span>
 1177   jccb(Assembler::notZero, try_rebias);
 1178 
 1179   // The epoch of the current bias is still valid but we know nothing
 1180   // about the owner; it might be set or it might be clear. Try to
 1181   // acquire the bias of the object using an atomic operation. If this
 1182   // fails we will go in to the runtime to revoke the object&#39;s bias.
 1183   // Note that we first construct the presumed unbiased header so we
 1184   // don&#39;t accidentally blow away another thread&#39;s valid bias.
 1185   NOT_LP64( movptr(swap_reg, saved_mark_addr); )
 1186   andptr(swap_reg,
<span class="line-modified"> 1187          markWord::biased_lock_mask_in_place | markWord::age_mask_in_place | markWord::epoch_mask_in_place);</span>
 1188 #ifdef _LP64
 1189   movptr(tmp_reg, swap_reg);
 1190   orptr(tmp_reg, r15_thread);
 1191 #else
 1192   get_thread(tmp_reg);
 1193   orptr(tmp_reg, swap_reg);
 1194 #endif
 1195   lock();
 1196   cmpxchgptr(tmp_reg, mark_addr); // compare tmp_reg and swap_reg
 1197   // If the biasing toward our thread failed, this means that
 1198   // another thread succeeded in biasing it toward itself and we
 1199   // need to revoke that bias. The revocation will occur in the
 1200   // interpreter runtime in the slow case.
 1201   if (counters != NULL) {
 1202     cond_inc32(Assembler::zero,
 1203                ExternalAddress((address) counters-&gt;anonymously_biased_lock_entry_count_addr()));
 1204   }
 1205   if (slow_case != NULL) {
 1206     jcc(Assembler::notZero, *slow_case);
 1207   }
</pre>
<hr />
<pre>
 1261   if (counters != NULL) {
 1262     cond_inc32(Assembler::zero,
 1263                ExternalAddress((address) counters-&gt;revoked_lock_entry_count_addr()));
 1264   }
 1265 
 1266   bind(cas_label);
 1267 
 1268   return null_check_offset;
 1269 }
 1270 
 1271 void MacroAssembler::biased_locking_exit(Register obj_reg, Register temp_reg, Label&amp; done) {
 1272   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 1273 
 1274   // Check for biased locking unlock case, which is a no-op
 1275   // Note: we do not have to check the thread ID for two reasons.
 1276   // First, the interpreter checks for IllegalMonitorStateException at
 1277   // a higher level. Second, if the bias was revoked while we held the
 1278   // lock, the object could not be rebiased toward another thread, so
 1279   // the bias bit would be clear.
 1280   movptr(temp_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
<span class="line-modified"> 1281   andptr(temp_reg, markWord::biased_lock_mask_in_place);</span>
<span class="line-modified"> 1282   cmpptr(temp_reg, markWord::biased_lock_pattern);</span>
 1283   jcc(Assembler::equal, done);
 1284 }
 1285 
 1286 #ifdef COMPILER2
 1287 
 1288 #if INCLUDE_RTM_OPT
 1289 
 1290 // Update rtm_counters based on abort status
 1291 // input: abort_status
 1292 //        rtm_counters (RTMLockingCounters*)
 1293 // flags are killed
 1294 void MacroAssembler::rtm_counters_update(Register abort_status, Register rtm_counters) {
 1295 
 1296   atomic_incptr(Address(rtm_counters, RTMLockingCounters::abort_count_offset()));
 1297   if (PrintPreciseRTMLockingStatistics) {
 1298     for (int i = 0; i &lt; RTMLockingCounters::ABORT_STATUS_LIMIT; i++) {
 1299       Label check_abort;
 1300       testl(abort_status, (1&lt;&lt;i));
 1301       jccb(Assembler::equal, check_abort);
 1302       atomic_incptr(Address(rtm_counters, RTMLockingCounters::abortX_count_offset() + (i * sizeof(uintx))));
</pre>
<hr />
<pre>
 1449 }
 1450 
 1451 // Use RTM for normal stack locks
 1452 // Input: objReg (object to lock)
 1453 void MacroAssembler::rtm_stack_locking(Register objReg, Register tmpReg, Register scrReg,
 1454                                        Register retry_on_abort_count_Reg,
 1455                                        RTMLockingCounters* stack_rtm_counters,
 1456                                        Metadata* method_data, bool profile_rtm,
 1457                                        Label&amp; DONE_LABEL, Label&amp; IsInflated) {
 1458   assert(UseRTMForStackLocks, &quot;why call this otherwise?&quot;);
 1459   assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
 1460   assert(tmpReg == rax, &quot;&quot;);
 1461   assert(scrReg == rdx, &quot;&quot;);
 1462   Label L_rtm_retry, L_decrement_retry, L_on_abort;
 1463 
 1464   if (RTMRetryCount &gt; 0) {
 1465     movl(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort
 1466     bind(L_rtm_retry);
 1467   }
 1468   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));
<span class="line-modified"> 1469   testptr(tmpReg, markWord::monitor_value);  // inflated vs stack-locked|neutral|biased</span>
 1470   jcc(Assembler::notZero, IsInflated);
 1471 
 1472   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 1473     Label L_noincrement;
 1474     if (RTMTotalCountIncrRate &gt; 1) {
 1475       // tmpReg, scrReg and flags are killed
 1476       branch_on_random_using_rdtsc(tmpReg, scrReg, RTMTotalCountIncrRate, L_noincrement);
 1477     }
 1478     assert(stack_rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 1479     atomic_incptr(ExternalAddress((address)stack_rtm_counters-&gt;total_count_addr()), scrReg);
 1480     bind(L_noincrement);
 1481   }
 1482   xbegin(L_on_abort);
 1483   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));       // fetch markword
<span class="line-modified"> 1484   andptr(tmpReg, markWord::biased_lock_mask_in_place); // look at 3 lock bits</span>
<span class="line-modified"> 1485   cmpptr(tmpReg, markWord::unlocked_value);            // bits = 001 unlocked</span>
 1486   jcc(Assembler::equal, DONE_LABEL);        // all done if unlocked
 1487 
 1488   Register abort_status_Reg = tmpReg; // status of abort is stored in RAX
 1489   if (UseRTMXendForLockBusy) {
 1490     xend();
 1491     movptr(abort_status_Reg, 0x2);   // Set the abort status to 2 (so we can retry)
 1492     jmp(L_decrement_retry);
 1493   }
 1494   else {
 1495     xabort(0);
 1496   }
 1497   bind(L_on_abort);
 1498   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 1499     rtm_profiling(abort_status_Reg, scrReg, stack_rtm_counters, method_data, profile_rtm);
 1500   }
 1501   bind(L_decrement_retry);
 1502   if (RTMRetryCount &gt; 0) {
 1503     // retry on lock abort if abort status is &#39;can retry&#39; (0x2) or &#39;memory conflict&#39; (0x4)
 1504     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry);
 1505   }
 1506 }
 1507 
 1508 // Use RTM for inflating locks
 1509 // inputs: objReg (object to lock)
 1510 //         boxReg (on-stack box address (displaced header location) - KILLED)
<span class="line-modified"> 1511 //         tmpReg (ObjectMonitor address + markWord::monitor_value)</span>
 1512 void MacroAssembler::rtm_inflated_locking(Register objReg, Register boxReg, Register tmpReg,
 1513                                           Register scrReg, Register retry_on_busy_count_Reg,
 1514                                           Register retry_on_abort_count_Reg,
 1515                                           RTMLockingCounters* rtm_counters,
 1516                                           Metadata* method_data, bool profile_rtm,
 1517                                           Label&amp; DONE_LABEL) {
 1518   assert(UseRTMLocking, &quot;why call this otherwise?&quot;);
 1519   assert(tmpReg == rax, &quot;&quot;);
 1520   assert(scrReg == rdx, &quot;&quot;);
 1521   Label L_rtm_retry, L_decrement_retry, L_on_abort;
 1522   int owner_offset = OM_OFFSET_NO_MONITOR_VALUE_TAG(owner);
 1523 
<span class="line-modified"> 1524   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.</span>
<span class="line-modified"> 1525   movptr(Address(boxReg, 0), (int32_t)intptr_t(markWord::unused_mark().value()));</span>
 1526   movptr(boxReg, tmpReg); // Save ObjectMonitor address
 1527 
 1528   if (RTMRetryCount &gt; 0) {
 1529     movl(retry_on_busy_count_Reg, RTMRetryCount);  // Retry on lock busy
 1530     movl(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort
 1531     bind(L_rtm_retry);
 1532   }
 1533   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
 1534     Label L_noincrement;
 1535     if (RTMTotalCountIncrRate &gt; 1) {
 1536       // tmpReg, scrReg and flags are killed
 1537       branch_on_random_using_rdtsc(tmpReg, scrReg, RTMTotalCountIncrRate, L_noincrement);
 1538     }
 1539     assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
 1540     atomic_incptr(ExternalAddress((address)rtm_counters-&gt;total_count_addr()), scrReg);
 1541     bind(L_noincrement);
 1542   }
 1543   xbegin(L_on_abort);
 1544   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));
 1545   movptr(tmpReg, Address(tmpReg, owner_offset));
</pre>
<hr />
<pre>
 1574   get_thread(scrReg);
 1575   Register threadReg = scrReg;
 1576 #endif
 1577   lock();
 1578   cmpxchgptr(threadReg, Address(boxReg, owner_offset)); // Updates tmpReg
 1579 
 1580   if (RTMRetryCount &gt; 0) {
 1581     // success done else retry
 1582     jccb(Assembler::equal, DONE_LABEL) ;
 1583     bind(L_decrement_retry);
 1584     // Spin and retry if lock is busy.
 1585     rtm_retry_lock_on_busy(retry_on_busy_count_Reg, boxReg, tmpReg, scrReg, L_rtm_retry);
 1586   }
 1587   else {
 1588     bind(L_decrement_retry);
 1589   }
 1590 }
 1591 
 1592 #endif //  INCLUDE_RTM_OPT
 1593 
<span class="line-modified"> 1594 // fast_lock and fast_unlock used by C2</span>
 1595 
 1596 // Because the transitions from emitted code to the runtime
 1597 // monitorenter/exit helper stubs are so slow it&#39;s critical that
<span class="line-modified"> 1598 // we inline both the stack-locking fast path and the inflated fast path.</span>
 1599 //
 1600 // See also: cmpFastLock and cmpFastUnlock.
 1601 //
 1602 // What follows is a specialized inline transliteration of the code
<span class="line-modified"> 1603 // in enter() and exit(). If we&#39;re concerned about I$ bloat another</span>
<span class="line-modified"> 1604 // option would be to emit TrySlowEnter and TrySlowExit methods</span>
 1605 // at startup-time.  These methods would accept arguments as
 1606 // (rax,=Obj, rbx=Self, rcx=box, rdx=Scratch) and return success-failure
<span class="line-modified"> 1607 // indications in the icc.ZFlag.  fast_lock and fast_unlock would simply</span>
 1608 // marshal the arguments and emit calls to TrySlowEnter and TrySlowExit.
 1609 // In practice, however, the # of lock sites is bounded and is usually small.
 1610 // Besides the call overhead, TrySlowEnter and TrySlowExit might suffer
 1611 // if the processor uses simple bimodal branch predictors keyed by EIP
 1612 // Since the helper routines would be called from multiple synchronization
 1613 // sites.
 1614 //
 1615 // An even better approach would be write &quot;MonitorEnter()&quot; and &quot;MonitorExit()&quot;
 1616 // in java - using j.u.c and unsafe - and just bind the lock and unlock sites
 1617 // to those specialized methods.  That&#39;d give us a mostly platform-independent
 1618 // implementation that the JITs could optimize and inline at their pleasure.
 1619 // Done correctly, the only time we&#39;d need to cross to native could would be
 1620 // to park() or unpark() threads.  We&#39;d also need a few more unsafe operators
 1621 // to (a) prevent compiler-JIT reordering of non-volatile accesses, and
 1622 // (b) explicit barriers or fence operations.
 1623 //
 1624 // TODO:
 1625 //
<span class="line-modified"> 1626 // *  Arrange for C2 to pass &quot;Self&quot; into fast_lock and fast_unlock in one of the registers (scr).</span>
<span class="line-modified"> 1627 //    This avoids manifesting the Self pointer in the fast_lock and fast_unlock terminals.</span>
 1628 //    Given TLAB allocation, Self is usually manifested in a register, so passing it into
 1629 //    the lock operators would typically be faster than reifying Self.
 1630 //
 1631 // *  Ideally I&#39;d define the primitives as:
 1632 //       fast_lock   (nax Obj, nax box, EAX tmp, nax scr) where box, tmp and scr are KILLED.
 1633 //       fast_unlock (nax Obj, EAX box, nax tmp) where box and tmp are KILLED
 1634 //    Unfortunately ADLC bugs prevent us from expressing the ideal form.
 1635 //    Instead, we&#39;re stuck with a rather awkward and brittle register assignments below.
 1636 //    Furthermore the register assignments are overconstrained, possibly resulting in
 1637 //    sub-optimal code near the synchronization site.
 1638 //
 1639 // *  Eliminate the sp-proximity tests and just use &quot;== Self&quot; tests instead.
 1640 //    Alternately, use a better sp-proximity test.
 1641 //
 1642 // *  Currently ObjectMonitor._Owner can hold either an sp value or a (THREAD *) value.
 1643 //    Either one is sufficient to uniquely identify a thread.
 1644 //    TODO: eliminate use of sp in _owner and use get_thread(tr) instead.
 1645 //
 1646 // *  Intrinsify notify() and notifyAll() for the common cases where the
 1647 //    object is locked by the calling thread but the waitlist is empty.
 1648 //    avoid the expensive JNI call to JVM_Notify() and JVM_NotifyAll().
 1649 //
 1650 // *  use jccb and jmpb instead of jcc and jmp to improve code density.
 1651 //    But beware of excessive branch density on AMD Opterons.
 1652 //
<span class="line-modified"> 1653 // *  Both fast_lock and fast_unlock set the ICC.ZF to indicate success</span>
<span class="line-modified"> 1654 //    or failure of the fast path.  If the fast path fails then we pass</span>
<span class="line-modified"> 1655 //    control to the slow path, typically in C.  In fast_lock and</span>
<span class="line-modified"> 1656 //    fast_unlock we often branch to DONE_LABEL, just to find that C2</span>
 1657 //    will emit a conditional branch immediately after the node.
 1658 //    So we have branches to branches and lots of ICC.ZF games.
 1659 //    Instead, it might be better to have C2 pass a &quot;FailureLabel&quot;
<span class="line-modified"> 1660 //    into fast_lock and fast_unlock.  In the case of success, control</span>
 1661 //    will drop through the node.  ICC.ZF is undefined at exit.
 1662 //    In the case of failure, the node will branch directly to the
 1663 //    FailureLabel
 1664 
 1665 
 1666 // obj: object to lock
 1667 // box: on-stack box address (displaced header location) - KILLED
 1668 // rax,: tmp -- KILLED
 1669 // scr: tmp -- KILLED
 1670 void MacroAssembler::fast_lock(Register objReg, Register boxReg, Register tmpReg,
 1671                                Register scrReg, Register cx1Reg, Register cx2Reg,
 1672                                BiasedLockingCounters* counters,
 1673                                RTMLockingCounters* rtm_counters,
 1674                                RTMLockingCounters* stack_rtm_counters,
 1675                                Metadata* method_data,
 1676                                bool use_rtm, bool profile_rtm) {
 1677   // Ensure the register assignments are disjoint
 1678   assert(tmpReg == rax, &quot;&quot;);
 1679 
 1680   if (use_rtm) {
</pre>
<hr />
<pre>
 1711 
 1712   // it&#39;s stack-locked, biased or neutral
 1713   // TODO: optimize away redundant LDs of obj-&gt;mark and improve the markword triage
 1714   // order to reduce the number of conditional branches in the most common cases.
 1715   // Beware -- there&#39;s a subtle invariant that fetch of the markword
 1716   // at [FETCH], below, will never observe a biased encoding (*101b).
 1717   // If this invariant is not held we risk exclusion (safety) failure.
 1718   if (UseBiasedLocking &amp;&amp; !UseOptoBiasInlining) {
 1719     biased_locking_enter(boxReg, objReg, tmpReg, scrReg, false, DONE_LABEL, NULL, counters);
 1720   }
 1721 
 1722 #if INCLUDE_RTM_OPT
 1723   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
 1724     rtm_stack_locking(objReg, tmpReg, scrReg, cx2Reg,
 1725                       stack_rtm_counters, method_data, profile_rtm,
 1726                       DONE_LABEL, IsInflated);
 1727   }
 1728 #endif // INCLUDE_RTM_OPT
 1729 
 1730   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));          // [FETCH]
<span class="line-modified"> 1731   testptr(tmpReg, markWord::monitor_value); // inflated vs stack-locked|neutral|biased</span>
 1732   jccb(Assembler::notZero, IsInflated);
 1733 
 1734   // Attempt stack-locking ...
<span class="line-modified"> 1735   orptr (tmpReg, markWord::unlocked_value);</span>
 1736   movptr(Address(boxReg, 0), tmpReg);          // Anticipate successful CAS
 1737   lock();
 1738   cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      // Updates tmpReg
 1739   if (counters != NULL) {
 1740     cond_inc32(Assembler::equal,
 1741                ExternalAddress((address)counters-&gt;fast_path_entry_count_addr()));
 1742   }
 1743   jcc(Assembler::equal, DONE_LABEL);           // Success
 1744 
 1745   // Recursive locking.
 1746   // The object is stack-locked: markword contains stack pointer to BasicLock.
 1747   // Locked by current thread if difference with current SP is less than one page.
 1748   subptr(tmpReg, rsp);
 1749   // Next instruction set ZFlag == 1 (Success) if difference is less then one page.
 1750   andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - os::vm_page_size())) );
 1751   movptr(Address(boxReg, 0), tmpReg);
 1752   if (counters != NULL) {
 1753     cond_inc32(Assembler::equal,
 1754                ExternalAddress((address)counters-&gt;fast_path_entry_count_addr()));
 1755   }
 1756   jmp(DONE_LABEL);
 1757 
 1758   bind(IsInflated);
<span class="line-modified"> 1759   // The object is inflated. tmpReg contains pointer to ObjectMonitor* + markWord::monitor_value</span>
 1760 
 1761 #if INCLUDE_RTM_OPT
 1762   // Use the same RTM locking code in 32- and 64-bit VM.
 1763   if (use_rtm) {
 1764     rtm_inflated_locking(objReg, boxReg, tmpReg, scrReg, cx1Reg, cx2Reg,
 1765                          rtm_counters, method_data, profile_rtm, DONE_LABEL);
 1766   } else {
 1767 #endif // INCLUDE_RTM_OPT
 1768 
 1769 #ifndef _LP64
 1770   // The object is inflated.
 1771 
 1772   // boxReg refers to the on-stack BasicLock in the current frame.
 1773   // We&#39;d like to write:
<span class="line-modified"> 1774   //   set box-&gt;_displaced_header = markWord::unused_mark().  Any non-0 value suffices.</span>
 1775   // This is convenient but results a ST-before-CAS penalty.  The following CAS suffers
 1776   // additional latency as we have another ST in the store buffer that must drain.
 1777 
 1778   // avoid ST-before-CAS
 1779   // register juggle because we need tmpReg for cmpxchgptr below
 1780   movptr(scrReg, boxReg);
 1781   movptr(boxReg, tmpReg);                   // consider: LEA box, [tmp-2]
 1782 
 1783   // Optimistic form: consider XORL tmpReg,tmpReg
 1784   movptr(tmpReg, NULL_WORD);
 1785 
 1786   // Appears unlocked - try to swing _owner from null to non-null.
 1787   // Ideally, I&#39;d manifest &quot;Self&quot; with get_thread and then attempt
 1788   // to CAS the register containing Self into m-&gt;Owner.
 1789   // But we don&#39;t have enough registers, so instead we can either try to CAS
 1790   // rsp or the address of the box (in scr) into &amp;m-&gt;owner.  If the CAS succeeds
 1791   // we later store &quot;Self&quot; into m-&gt;Owner.  Transiently storing a stack address
 1792   // (rsp or the address of the box) into  m-&gt;owner is harmless.
 1793   // Invariant: tmpReg == 0.  tmpReg is EAX which is the implicit cmpxchg comparand.
 1794   lock();
 1795   cmpxchgptr(scrReg, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
 1796   movptr(Address(scrReg, 0), 3);          // box-&gt;_displaced_header = 3
 1797   // If we weren&#39;t able to swing _owner from NULL to the BasicLock
 1798   // then take the slow path.
 1799   jccb  (Assembler::notZero, DONE_LABEL);
 1800   // update _owner from BasicLock to thread
 1801   get_thread (scrReg);                    // beware: clobbers ICCs
 1802   movptr(Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), scrReg);
 1803   xorptr(boxReg, boxReg);                 // set icc.ZFlag = 1 to indicate success
 1804 
<span class="line-modified"> 1805   // If the CAS fails we can either retry or pass control to the slow path.</span>
 1806   // We use the latter tactic.
 1807   // Pass the CAS result in the icc.ZFlag into DONE_LABEL
 1808   // If the CAS was successful ...
 1809   //   Self has acquired the lock
 1810   //   Invariant: m-&gt;_recursions should already be 0, so we don&#39;t need to explicitly set it.
 1811   // Intentional fall-through into DONE_LABEL ...
 1812 #else // _LP64
<span class="line-modified"> 1813   // It&#39;s inflated and we use scrReg for ObjectMonitor* in this section.</span>
 1814   movq(scrReg, tmpReg);
 1815   xorq(tmpReg, tmpReg);

 1816   lock();
 1817   cmpxchgptr(r15_thread, Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
<span class="line-modified"> 1818   // Unconditionally set box-&gt;_displaced_header = markWord::unused_mark().</span>
<span class="line-modified"> 1819   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.</span>
<span class="line-modified"> 1820   movptr(Address(boxReg, 0), (int32_t)intptr_t(markWord::unused_mark().value()));</span>
 1821   // Intentional fall-through into DONE_LABEL ...
 1822   // Propagate ICC.ZF from CAS above into DONE_LABEL.
 1823 #endif // _LP64
 1824 #if INCLUDE_RTM_OPT
 1825   } // use_rtm()
 1826 #endif
 1827   // DONE_LABEL is a hot target - we&#39;d really like to place it at the
 1828   // start of cache line by padding with NOPs.
 1829   // See the AMD and Intel software optimization manuals for the
 1830   // most efficient &quot;long&quot; NOP encodings.
 1831   // Unfortunately none of our alignment mechanisms suffice.
 1832   bind(DONE_LABEL);
 1833 
 1834   // At DONE_LABEL the icc ZFlag is set as follows ...
<span class="line-modified"> 1835   // fast_unlock uses the same protocol.</span>
 1836   // ZFlag == 1 -&gt; Success
<span class="line-modified"> 1837   // ZFlag == 0 -&gt; Failure - force control through the slow path</span>
 1838 }
 1839 
 1840 // obj: object to unlock
 1841 // box: box address (displaced header location), killed.  Must be EAX.
 1842 // tmp: killed, cannot be obj nor box.
 1843 //
 1844 // Some commentary on balanced locking:
 1845 //
<span class="line-modified"> 1846 // fast_lock and fast_unlock are emitted only for provably balanced lock sites.</span>
 1847 // Methods that don&#39;t have provably balanced locking are forced to run in the
 1848 // interpreter - such methods won&#39;t be compiled to use fast_lock and fast_unlock.
 1849 // The interpreter provides two properties:
 1850 // I1:  At return-time the interpreter automatically and quietly unlocks any
 1851 //      objects acquired the current activation (frame).  Recall that the
 1852 //      interpreter maintains an on-stack list of locks currently held by
 1853 //      a frame.
 1854 // I2:  If a method attempts to unlock an object that is not held by the
 1855 //      the frame the interpreter throws IMSX.
 1856 //
 1857 // Lets say A(), which has provably balanced locking, acquires O and then calls B().
 1858 // B() doesn&#39;t have provably balanced locking so it runs in the interpreter.
 1859 // Control returns to A() and A() unlocks O.  By I1 and I2, above, we know that O
 1860 // is still locked by A().
 1861 //
 1862 // The only other source of unbalanced locking would be JNI.  The &quot;Java Native Interface:
 1863 // Programmer&#39;s Guide and Specification&quot; claims that an object locked by jni_monitorenter
 1864 // should not be unlocked by &quot;normal&quot; java-level locking and vice-versa.  The specification
 1865 // doesn&#39;t specify what will occur if a program engages in such mixed-mode locking, however.
 1866 // Arguably given that the spec legislates the JNI case as undefined our implementation
<span class="line-modified"> 1867 // could reasonably *avoid* checking owner in fast_unlock().</span>
 1868 // In the interest of performance we elide m-&gt;Owner==Self check in unlock.
 1869 // A perfectly viable alternative is to elide the owner check except when
 1870 // Xcheck:jni is enabled.
 1871 
 1872 void MacroAssembler::fast_unlock(Register objReg, Register boxReg, Register tmpReg, bool use_rtm) {
 1873   assert(boxReg == rax, &quot;&quot;);
 1874   assert_different_registers(objReg, boxReg, tmpReg);
 1875 
 1876   Label DONE_LABEL, Stacked, CheckSucc;
 1877 
 1878   // Critically, the biased locking test must have precedence over
 1879   // and appear before the (box-&gt;dhw == 0) recursive stack-lock test.
 1880   if (UseBiasedLocking &amp;&amp; !UseOptoBiasInlining) {
 1881     biased_locking_exit(objReg, tmpReg, DONE_LABEL);
 1882   }
 1883 
 1884 #if INCLUDE_RTM_OPT
 1885   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
 1886     assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
 1887     Label L_regular_unlock;
<span class="line-modified"> 1888     movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // fetch markword</span>
<span class="line-modified"> 1889     andptr(tmpReg, markWord::biased_lock_mask_in_place);              // look at 3 lock bits</span>
<span class="line-modified"> 1890     cmpptr(tmpReg, markWord::unlocked_value);                         // bits = 001 unlocked</span>
<span class="line-modified"> 1891     jccb(Assembler::notEqual, L_regular_unlock);                      // if !HLE RegularLock</span>
<span class="line-modified"> 1892     xend();                                                           // otherwise end...</span>
<span class="line-modified"> 1893     jmp(DONE_LABEL);                                                  // ... and we&#39;re done</span>
 1894     bind(L_regular_unlock);
 1895   }
 1896 #endif
 1897 
<span class="line-modified"> 1898   cmpptr(Address(boxReg, 0), (int32_t)NULL_WORD);                   // Examine the displaced header</span>
<span class="line-modified"> 1899   jcc   (Assembler::zero, DONE_LABEL);                              // 0 indicates recursive stack-lock</span>
<span class="line-modified"> 1900   movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Examine the object&#39;s markword</span>
<span class="line-modified"> 1901   testptr(tmpReg, markWord::monitor_value);                         // Inflated?</span>
 1902   jccb  (Assembler::zero, Stacked);
 1903 
 1904   // It&#39;s inflated.
 1905 #if INCLUDE_RTM_OPT
 1906   if (use_rtm) {
 1907     Label L_regular_inflated_unlock;
 1908     int owner_offset = OM_OFFSET_NO_MONITOR_VALUE_TAG(owner);
 1909     movptr(boxReg, Address(tmpReg, owner_offset));
 1910     testptr(boxReg, boxReg);
 1911     jccb(Assembler::notZero, L_regular_inflated_unlock);
 1912     xend();
 1913     jmpb(DONE_LABEL);
 1914     bind(L_regular_inflated_unlock);
 1915   }
 1916 #endif
 1917 
 1918   // Despite our balanced locking property we still check that m-&gt;_owner == Self
 1919   // as java routines or native JNI code called by this thread might
 1920   // have released the lock.
 1921   // Refer to the comments in synchronizer.cpp for how we might encode extra
 1922   // state in _succ so we can avoid fetching EntryList|cxq.
 1923   //
 1924   // I&#39;d like to add more cases in fast_lock() and fast_unlock() --
 1925   // such as recursive enter and exit -- but we have to be wary of
 1926   // I$ bloat, T$ effects and BP$ effects.
 1927   //
 1928   // If there&#39;s no contention try a 1-0 exit.  That is, exit without
 1929   // a costly MEMBAR or CAS.  See synchronizer.cpp for details on how
 1930   // we detect and recover from the race that the 1-0 exit admits.
 1931   //
<span class="line-modified"> 1932   // Conceptually fast_unlock() must execute a STST|LDST &quot;release&quot; barrier</span>
 1933   // before it STs null into _owner, releasing the lock.  Updates
 1934   // to data protected by the critical section must be visible before
 1935   // we drop the lock (and thus before any other thread could acquire
 1936   // the lock and observe the fields protected by the lock).
 1937   // IA32&#39;s memory-model is SPO, so STs are ordered with respect to
 1938   // each other and there&#39;s no need for an explicit barrier (fence).
 1939   // See also http://gee.cs.oswego.edu/dl/jmm/cookbook.html.
 1940 #ifndef _LP64
 1941   get_thread (boxReg);
 1942 
 1943   // Note that we could employ various encoding schemes to reduce
 1944   // the number of loads below (currently 4) to just 2 or 3.
 1945   // Refer to the comments in synchronizer.cpp.
 1946   // In practice the chain of fetches doesn&#39;t seem to impact performance, however.
 1947   xorptr(boxReg, boxReg);
 1948   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));
 1949   jccb  (Assembler::notZero, DONE_LABEL);
 1950   movptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));
 1951   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));
 1952   jccb  (Assembler::notZero, CheckSucc);
</pre>
<hr />
<pre>
 1961   // and be assured we observe the same value as above.
 1962   movptr(tmpReg, Address(boxReg, 0));
 1963   lock();
 1964   cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Uses RAX which is box
 1965   // Intention fall-thru into DONE_LABEL
 1966 
 1967   // DONE_LABEL is a hot target - we&#39;d really like to place it at the
 1968   // start of cache line by padding with NOPs.
 1969   // See the AMD and Intel software optimization manuals for the
 1970   // most efficient &quot;long&quot; NOP encodings.
 1971   // Unfortunately none of our alignment mechanisms suffice.
 1972   bind (CheckSucc);
 1973 #else // _LP64
 1974   // It&#39;s inflated
 1975   xorptr(boxReg, boxReg);
 1976   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));
 1977   jccb  (Assembler::notZero, DONE_LABEL);
 1978   movptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));
 1979   orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));
 1980   jccb  (Assembler::notZero, CheckSucc);
<span class="line-added"> 1981   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.</span>
 1982   movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t)NULL_WORD);
 1983   jmpb  (DONE_LABEL);
 1984 
 1985   // Try to avoid passing control into the slow_path ...
 1986   Label LSuccess, LGoSlowPath ;
 1987   bind  (CheckSucc);
 1988 
 1989   // The following optional optimization can be elided if necessary
<span class="line-modified"> 1990   // Effectively: if (succ == null) goto slow path</span>
 1991   // The code reduces the window for a race, however,
 1992   // and thus benefits performance.
 1993   cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), (int32_t)NULL_WORD);
 1994   jccb  (Assembler::zero, LGoSlowPath);
 1995 
 1996   xorptr(boxReg, boxReg);
<span class="line-added"> 1997   // Without cast to int32_t this style of movptr will destroy r10 which is typically obj.</span>
 1998   movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t)NULL_WORD);
 1999 
 2000   // Memory barrier/fence
 2001   // Dekker pivot point -- fulcrum : ST Owner; MEMBAR; LD Succ
 2002   // Instead of MFENCE we use a dummy locked add of 0 to the top-of-stack.
 2003   // This is faster on Nehalem and AMD Shanghai/Barcelona.
 2004   // See https://blogs.oracle.com/dave/entry/instruction_selection_for_volatile_fences
 2005   // We might also restructure (ST Owner=0;barrier;LD _Succ) to
 2006   // (mov box,0; xchgq box, &amp;m-&gt;Owner; LD _succ) .
 2007   lock(); addl(Address(rsp, 0), 0);
 2008 
 2009   cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), (int32_t)NULL_WORD);
 2010   jccb  (Assembler::notZero, LSuccess);
 2011 
 2012   // Rare inopportune interleaving - race.
 2013   // The successor vanished in the small window above.
 2014   // The lock is contended -- (cxq|EntryList) != null -- and there&#39;s no apparent successor.
 2015   // We need to ensure progress and succession.
 2016   // Try to reacquire the lock.
 2017   // If that fails then the new owner is responsible for succession and this
 2018   // thread needs to take no further action and can exit via the fast path (success).
 2019   // If the re-acquire succeeds then pass control into the slow path.
 2020   // As implemented, this latter mode is horrible because we generated more
 2021   // coherence traffic on the lock *and* artifically extended the critical section
 2022   // length while by virtue of passing control into the slow path.
 2023 
 2024   // box is really RAX -- the following CMPXCHG depends on that binding
 2025   // cmpxchg R,[M] is equivalent to rax = CAS(M,rax,R)
 2026   lock();
 2027   cmpxchgptr(r15_thread, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));
 2028   // There&#39;s no successor so we tried to regrab the lock.
 2029   // If that didn&#39;t work, then another thread grabbed the
 2030   // lock so we&#39;re done (and exit was a success).
 2031   jccb  (Assembler::notEqual, LSuccess);
<span class="line-modified"> 2032   // Intentional fall-through into slow path</span>
 2033 
 2034   bind  (LGoSlowPath);
 2035   orl   (boxReg, 1);                      // set ICC.ZF=0 to indicate failure
 2036   jmpb  (DONE_LABEL);
 2037 
 2038   bind  (LSuccess);
 2039   testl (boxReg, 0);                      // set ICC.ZF=1 to indicate success
 2040   jmpb  (DONE_LABEL);
 2041 
 2042   bind  (Stacked);
 2043   movptr(tmpReg, Address (boxReg, 0));      // re-fetch
 2044   lock();
 2045   cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); // Uses RAX which is box
 2046 
 2047 #endif
 2048   bind(DONE_LABEL);
 2049 }
 2050 #endif // COMPILER2
 2051 
 2052 void MacroAssembler::c2bool(Register x) {
</pre>
<hr />
<pre>
 2707 }
 2708 
 2709 void MacroAssembler::divsd(XMMRegister dst, AddressLiteral src) {
 2710   if (reachable(src)) {
 2711     Assembler::divsd(dst, as_Address(src));
 2712   } else {
 2713     lea(rscratch1, src);
 2714     Assembler::divsd(dst, Address(rscratch1, 0));
 2715   }
 2716 }
 2717 
 2718 void MacroAssembler::divss(XMMRegister dst, AddressLiteral src) {
 2719   if (reachable(src)) {
 2720     Assembler::divss(dst, as_Address(src));
 2721   } else {
 2722     lea(rscratch1, src);
 2723     Assembler::divss(dst, Address(rscratch1, 0));
 2724   }
 2725 }
 2726 
<span class="line-modified"> 2727 #ifndef _LP64</span>

 2728 void MacroAssembler::empty_FPU_stack() {
 2729   if (VM_Version::supports_mmx()) {
 2730     emms();
 2731   } else {
 2732     for (int i = 8; i-- &gt; 0; ) ffree(i);
 2733   }
 2734 }
<span class="line-modified"> 2735 #endif // !LP64</span>
 2736 
 2737 
 2738 void MacroAssembler::enter() {
 2739   push(rbp);
 2740   mov(rbp, rsp);
 2741 }
 2742 
 2743 // A 5 byte nop that is safe for patching (see patch_verified_entry)
 2744 void MacroAssembler::fat_nop() {
 2745   if (UseAddressNop) {
 2746     addr_nop_5();
 2747   } else {
 2748     emit_int8(0x26); // es:
 2749     emit_int8(0x2e); // cs:
 2750     emit_int8(0x64); // fs:
 2751     emit_int8(0x65); // gs:
 2752     emit_int8((unsigned char)0x90);
 2753   }
 2754 }
 2755 
<span class="line-added"> 2756 #if !defined(_LP64)</span>
 2757 void MacroAssembler::fcmp(Register tmp) {
 2758   fcmp(tmp, 1, true, true);
 2759 }
 2760 
 2761 void MacroAssembler::fcmp(Register tmp, int index, bool pop_left, bool pop_right) {
 2762   assert(!pop_right || pop_left, &quot;usage error&quot;);
 2763   if (VM_Version::supports_cmov()) {
 2764     assert(tmp == noreg, &quot;unneeded temp&quot;);
 2765     if (pop_left) {
 2766       fucomip(index);
 2767     } else {
 2768       fucomi(index);
 2769     }
 2770     if (pop_right) {
 2771       fpop();
 2772     }
 2773   } else {
 2774     assert(tmp != noreg, &quot;need temp&quot;);
 2775     if (pop_left) {
 2776       if (pop_right) {
</pre>
<hr />
<pre>
 2818   }
 2819   bind(L);
 2820 }
 2821 
 2822 void MacroAssembler::fld_d(AddressLiteral src) {
 2823   fld_d(as_Address(src));
 2824 }
 2825 
 2826 void MacroAssembler::fld_s(AddressLiteral src) {
 2827   fld_s(as_Address(src));
 2828 }
 2829 
 2830 void MacroAssembler::fld_x(AddressLiteral src) {
 2831   Assembler::fld_x(as_Address(src));
 2832 }
 2833 
 2834 void MacroAssembler::fldcw(AddressLiteral src) {
 2835   Assembler::fldcw(as_Address(src));
 2836 }
 2837 
<span class="line-added"> 2838 void MacroAssembler::fpop() {</span>
<span class="line-added"> 2839   ffree();</span>
<span class="line-added"> 2840   fincstp();</span>
<span class="line-added"> 2841 }</span>
<span class="line-added"> 2842 </span>
<span class="line-added"> 2843 void MacroAssembler::fremr(Register tmp) {</span>
<span class="line-added"> 2844   save_rax(tmp);</span>
<span class="line-added"> 2845   { Label L;</span>
<span class="line-added"> 2846     bind(L);</span>
<span class="line-added"> 2847     fprem();</span>
<span class="line-added"> 2848     fwait(); fnstsw_ax();</span>
<span class="line-added"> 2849     sahf();</span>
<span class="line-added"> 2850     jcc(Assembler::parity, L);</span>
<span class="line-added"> 2851   }</span>
<span class="line-added"> 2852   restore_rax(tmp);</span>
<span class="line-added"> 2853   // Result is in ST0.</span>
<span class="line-added"> 2854   // Note: fxch &amp; fpop to get rid of ST1</span>
<span class="line-added"> 2855   // (otherwise FPU stack could overflow eventually)</span>
<span class="line-added"> 2856   fxch(1);</span>
<span class="line-added"> 2857   fpop();</span>
<span class="line-added"> 2858 }</span>
<span class="line-added"> 2859 #endif // !LP64</span>
<span class="line-added"> 2860 </span>
 2861 void MacroAssembler::mulpd(XMMRegister dst, AddressLiteral src) {
 2862   if (reachable(src)) {
 2863     Assembler::mulpd(dst, as_Address(src));
 2864   } else {
 2865     lea(rscratch1, src);
 2866     Assembler::mulpd(dst, Address(rscratch1, 0));
 2867   }
 2868 }
 2869 




















 2870 void MacroAssembler::load_float(Address src) {
 2871   if (UseSSE &gt;= 1) {
 2872     movflt(xmm0, src);
 2873   } else {
 2874     LP64_ONLY(ShouldNotReachHere());
 2875     NOT_LP64(fld_s(src));
 2876   }
 2877 }
 2878 
 2879 void MacroAssembler::store_float(Address dst) {
 2880   if (UseSSE &gt;= 1) {
 2881     movflt(dst, xmm0);
 2882   } else {
 2883     LP64_ONLY(ShouldNotReachHere());
 2884     NOT_LP64(fstp_s(dst));
 2885   }
 2886 }
 2887 
 2888 void MacroAssembler::load_double(Address src) {
 2889   if (UseSSE &gt;= 2) {
 2890     movdbl(xmm0, src);
 2891   } else {
 2892     LP64_ONLY(ShouldNotReachHere());
 2893     NOT_LP64(fld_d(src));
 2894   }
 2895 }
 2896 
 2897 void MacroAssembler::store_double(Address dst) {
 2898   if (UseSSE &gt;= 2) {
 2899     movdbl(dst, xmm0);
 2900   } else {
 2901     LP64_ONLY(ShouldNotReachHere());
 2902     NOT_LP64(fstp_d(dst));
 2903   }
 2904 }
 2905 






















 2906 // dst = c = a * b + c
 2907 void MacroAssembler::fmad(XMMRegister dst, XMMRegister a, XMMRegister b, XMMRegister c) {
 2908   Assembler::vfmadd231sd(c, a, b);
 2909   if (dst != c) {
 2910     movdbl(dst, c);
 2911   }
 2912 }
 2913 
 2914 // dst = c = a * b + c
 2915 void MacroAssembler::fmaf(XMMRegister dst, XMMRegister a, XMMRegister b, XMMRegister c) {
 2916   Assembler::vfmadd231ss(c, a, b);
 2917   if (dst != c) {
 2918     movflt(dst, c);
 2919   }
 2920 }
 2921 
 2922 // dst = c = a * b + c
 2923 void MacroAssembler::vfmad(XMMRegister dst, XMMRegister a, XMMRegister b, XMMRegister c, int vector_len) {
 2924   Assembler::vfmadd231pd(c, a, b, vector_len);
 2925   if (dst != c) {
</pre>
<hr />
<pre>
 3286     lea(scratchReg, src);
 3287     movdqu(dst, Address(scratchReg, 0));
 3288   }
 3289 }
 3290 
 3291 void MacroAssembler::vmovdqu(Address dst, XMMRegister src) {
 3292     assert(((src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3293     Assembler::vmovdqu(dst, src);
 3294 }
 3295 
 3296 void MacroAssembler::vmovdqu(XMMRegister dst, Address src) {
 3297     assert(((dst-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3298     Assembler::vmovdqu(dst, src);
 3299 }
 3300 
 3301 void MacroAssembler::vmovdqu(XMMRegister dst, XMMRegister src) {
 3302     assert(((dst-&gt;encoding() &lt; 16  &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3303     Assembler::vmovdqu(dst, src);
 3304 }
 3305 
<span class="line-modified"> 3306 void MacroAssembler::vmovdqu(XMMRegister dst, AddressLiteral src, Register scratch_reg) {</span>
 3307   if (reachable(src)) {
 3308     vmovdqu(dst, as_Address(src));
 3309   }
 3310   else {
<span class="line-modified"> 3311     lea(scratch_reg, src);</span>
<span class="line-modified"> 3312     vmovdqu(dst, Address(scratch_reg, 0));</span>
 3313   }
 3314 }
 3315 
 3316 void MacroAssembler::evmovdquq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {
 3317   if (reachable(src)) {
 3318     Assembler::evmovdquq(dst, as_Address(src), vector_len);
 3319   } else {
 3320     lea(rscratch, src);
 3321     Assembler::evmovdquq(dst, Address(rscratch, 0), vector_len);
 3322   }
 3323 }
 3324 
 3325 void MacroAssembler::movdqa(XMMRegister dst, AddressLiteral src) {
 3326   if (reachable(src)) {
 3327     Assembler::movdqa(dst, as_Address(src));
 3328   } else {
 3329     lea(rscratch1, src);
 3330     Assembler::movdqa(dst, Address(rscratch1, 0));
 3331   }
 3332 }
</pre>
<hr />
<pre>
 3617 }
 3618 
 3619 void MacroAssembler::sqrtss(XMMRegister dst, AddressLiteral src) {
 3620   if (reachable(src)) {
 3621     Assembler::sqrtss(dst, as_Address(src));
 3622   } else {
 3623     lea(rscratch1, src);
 3624     Assembler::sqrtss(dst, Address(rscratch1, 0));
 3625   }
 3626 }
 3627 
 3628 void MacroAssembler::subsd(XMMRegister dst, AddressLiteral src) {
 3629   if (reachable(src)) {
 3630     Assembler::subsd(dst, as_Address(src));
 3631   } else {
 3632     lea(rscratch1, src);
 3633     Assembler::subsd(dst, Address(rscratch1, 0));
 3634   }
 3635 }
 3636 
<span class="line-added"> 3637 void MacroAssembler::roundsd(XMMRegister dst, AddressLiteral src, int32_t rmode, Register scratch_reg) {</span>
<span class="line-added"> 3638   if (reachable(src)) {</span>
<span class="line-added"> 3639     Assembler::roundsd(dst, as_Address(src), rmode);</span>
<span class="line-added"> 3640   } else {</span>
<span class="line-added"> 3641     lea(scratch_reg, src);</span>
<span class="line-added"> 3642     Assembler::roundsd(dst, Address(scratch_reg, 0), rmode);</span>
<span class="line-added"> 3643   }</span>
<span class="line-added"> 3644 }</span>
<span class="line-added"> 3645 </span>
 3646 void MacroAssembler::subss(XMMRegister dst, AddressLiteral src) {
 3647   if (reachable(src)) {
 3648     Assembler::subss(dst, as_Address(src));
 3649   } else {
 3650     lea(rscratch1, src);
 3651     Assembler::subss(dst, Address(rscratch1, 0));
 3652   }
 3653 }
 3654 
 3655 void MacroAssembler::ucomisd(XMMRegister dst, AddressLiteral src) {
 3656   if (reachable(src)) {
 3657     Assembler::ucomisd(dst, as_Address(src));
 3658   } else {
 3659     lea(rscratch1, src);
 3660     Assembler::ucomisd(dst, Address(rscratch1, 0));
 3661   }
 3662 }
 3663 
 3664 void MacroAssembler::ucomiss(XMMRegister dst, AddressLiteral src) {
 3665   if (reachable(src)) {
 3666     Assembler::ucomiss(dst, as_Address(src));
 3667   } else {
 3668     lea(rscratch1, src);
 3669     Assembler::ucomiss(dst, Address(rscratch1, 0));
 3670   }
 3671 }
 3672 
<span class="line-modified"> 3673 void MacroAssembler::xorpd(XMMRegister dst, AddressLiteral src, Register scratch_reg) {</span>
 3674   // Used in sign-bit flipping with aligned address.
 3675   assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
 3676   if (reachable(src)) {
 3677     Assembler::xorpd(dst, as_Address(src));
 3678   } else {
<span class="line-modified"> 3679     lea(scratch_reg, src);</span>
<span class="line-modified"> 3680     Assembler::xorpd(dst, Address(scratch_reg, 0));</span>
 3681   }
 3682 }
 3683 
 3684 void MacroAssembler::xorpd(XMMRegister dst, XMMRegister src) {
 3685   if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512dq() &amp;&amp; (dst-&gt;encoding() == src-&gt;encoding())) {
 3686     Assembler::vpxor(dst, dst, src, Assembler::AVX_512bit);
 3687   }
 3688   else {
 3689     Assembler::xorpd(dst, src);
 3690   }
 3691 }
 3692 
 3693 void MacroAssembler::xorps(XMMRegister dst, XMMRegister src) {
 3694   if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512dq() &amp;&amp; (dst-&gt;encoding() == src-&gt;encoding())) {
 3695     Assembler::vpxor(dst, dst, src, Assembler::AVX_512bit);
 3696   } else {
 3697     Assembler::xorps(dst, src);
 3698   }
 3699 }
 3700 
<span class="line-modified"> 3701 void MacroAssembler::xorps(XMMRegister dst, AddressLiteral src, Register scratch_reg) {</span>
 3702   // Used in sign-bit flipping with aligned address.
 3703   assert((UseAVX &gt; 0) || (((intptr_t)src.target() &amp; 15) == 0), &quot;SSE mode requires address alignment 16 bytes&quot;);
 3704   if (reachable(src)) {
 3705     Assembler::xorps(dst, as_Address(src));
 3706   } else {
<span class="line-modified"> 3707     lea(scratch_reg, src);</span>
<span class="line-modified"> 3708     Assembler::xorps(dst, Address(scratch_reg, 0));</span>
 3709   }
 3710 }
 3711 
 3712 void MacroAssembler::pshufb(XMMRegister dst, AddressLiteral src) {
 3713   // Used in sign-bit flipping with aligned address.
 3714   bool aligned_adr = (((intptr_t)src.target() &amp; 15) == 0);
 3715   assert((UseAVX &gt; 0) || aligned_adr, &quot;SSE mode requires address alignment 16 bytes&quot;);
 3716   if (reachable(src)) {
 3717     Assembler::pshufb(dst, as_Address(src));
 3718   } else {
 3719     lea(rscratch1, src);
 3720     Assembler::pshufb(dst, Address(rscratch1, 0));
 3721   }
 3722 }
 3723 
 3724 // AVX 3-operands instructions
 3725 
 3726 void MacroAssembler::vaddsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3727   if (reachable(src)) {
 3728     vaddsd(dst, nds, as_Address(src));
 3729   } else {
 3730     lea(rscratch1, src);
 3731     vaddsd(dst, nds, Address(rscratch1, 0));
 3732   }
 3733 }
 3734 
 3735 void MacroAssembler::vaddss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3736   if (reachable(src)) {
 3737     vaddss(dst, nds, as_Address(src));
 3738   } else {
 3739     lea(rscratch1, src);
 3740     vaddss(dst, nds, Address(rscratch1, 0));
 3741   }
 3742 }
 3743 
<span class="line-added"> 3744 void MacroAssembler::vpaddd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {</span>
<span class="line-added"> 3745   assert(UseAVX &gt; 0, &quot;requires some form of AVX&quot;);</span>
<span class="line-added"> 3746   if (reachable(src)) {</span>
<span class="line-added"> 3747     Assembler::vpaddd(dst, nds, as_Address(src), vector_len);</span>
<span class="line-added"> 3748   } else {</span>
<span class="line-added"> 3749     lea(rscratch, src);</span>
<span class="line-added"> 3750     Assembler::vpaddd(dst, nds, Address(rscratch, 0), vector_len);</span>
<span class="line-added"> 3751   }</span>
<span class="line-added"> 3752 }</span>
<span class="line-added"> 3753 </span>
 3754 void MacroAssembler::vabsss(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len) {
 3755   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
 3756   vandps(dst, nds, negate_field, vector_len);
 3757 }
 3758 
 3759 void MacroAssembler::vabssd(XMMRegister dst, XMMRegister nds, XMMRegister src, AddressLiteral negate_field, int vector_len) {
 3760   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
 3761   vandpd(dst, nds, negate_field, vector_len);
 3762 }
 3763 
 3764 void MacroAssembler::vpaddb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3765   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3766   Assembler::vpaddb(dst, nds, src, vector_len);
 3767 }
 3768 
 3769 void MacroAssembler::vpaddb(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
 3770   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3771   Assembler::vpaddb(dst, nds, src, vector_len);
 3772 }
 3773 
 3774 void MacroAssembler::vpaddw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3775   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3776   Assembler::vpaddw(dst, nds, src, vector_len);
 3777 }
 3778 
 3779 void MacroAssembler::vpaddw(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
 3780   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3781   Assembler::vpaddw(dst, nds, src, vector_len);
 3782 }
 3783 
<span class="line-modified"> 3784 void MacroAssembler::vpand(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {</span>
 3785   if (reachable(src)) {
 3786     Assembler::vpand(dst, nds, as_Address(src), vector_len);
 3787   } else {
<span class="line-modified"> 3788     lea(scratch_reg, src);</span>
<span class="line-modified"> 3789     Assembler::vpand(dst, nds, Address(scratch_reg, 0), vector_len);</span>
 3790   }
 3791 }
 3792 
 3793 void MacroAssembler::vpbroadcastw(XMMRegister dst, XMMRegister src, int vector_len) {
 3794   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3795   Assembler::vpbroadcastw(dst, src, vector_len);
 3796 }
 3797 
 3798 void MacroAssembler::vpcmpeqb(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3799   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3800   Assembler::vpcmpeqb(dst, nds, src, vector_len);
 3801 }
 3802 
 3803 void MacroAssembler::vpcmpeqw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3804   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3805   Assembler::vpcmpeqw(dst, nds, src, vector_len);
 3806 }
 3807 
 3808 void MacroAssembler::vpmovzxbw(XMMRegister dst, Address src, int vector_len) {
 3809   assert(((dst-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
</pre>
<hr />
<pre>
 3838 void MacroAssembler::vpsubw(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {
 3839   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3840   Assembler::vpsubw(dst, nds, src, vector_len);
 3841 }
 3842 
 3843 void MacroAssembler::vpsubw(XMMRegister dst, XMMRegister nds, Address src, int vector_len) {
 3844   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3845   Assembler::vpsubw(dst, nds, src, vector_len);
 3846 }
 3847 
 3848 void MacroAssembler::vpsraw(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len) {
 3849   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; shift-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3850   Assembler::vpsraw(dst, nds, shift, vector_len);
 3851 }
 3852 
 3853 void MacroAssembler::vpsraw(XMMRegister dst, XMMRegister nds, int shift, int vector_len) {
 3854   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3855   Assembler::vpsraw(dst, nds, shift, vector_len);
 3856 }
 3857 
<span class="line-added"> 3858 void MacroAssembler::evpsraq(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len) {</span>
<span class="line-added"> 3859   assert(UseAVX &gt; 2,&quot;&quot;);</span>
<span class="line-added"> 3860   if (!VM_Version::supports_avx512vl() &amp;&amp; vector_len &lt; 2) {</span>
<span class="line-added"> 3861      vector_len = 2;</span>
<span class="line-added"> 3862   }</span>
<span class="line-added"> 3863   Assembler::evpsraq(dst, nds, shift, vector_len);</span>
<span class="line-added"> 3864 }</span>
<span class="line-added"> 3865 </span>
<span class="line-added"> 3866 void MacroAssembler::evpsraq(XMMRegister dst, XMMRegister nds, int shift, int vector_len) {</span>
<span class="line-added"> 3867   assert(UseAVX &gt; 2,&quot;&quot;);</span>
<span class="line-added"> 3868   if (!VM_Version::supports_avx512vl() &amp;&amp; vector_len &lt; 2) {</span>
<span class="line-added"> 3869      vector_len = 2;</span>
<span class="line-added"> 3870   }</span>
<span class="line-added"> 3871   Assembler::evpsraq(dst, nds, shift, vector_len);</span>
<span class="line-added"> 3872 }</span>
<span class="line-added"> 3873 </span>
 3874 void MacroAssembler::vpsrlw(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len) {
 3875   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; shift-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3876   Assembler::vpsrlw(dst, nds, shift, vector_len);
 3877 }
 3878 
 3879 void MacroAssembler::vpsrlw(XMMRegister dst, XMMRegister nds, int shift, int vector_len) {
 3880   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3881   Assembler::vpsrlw(dst, nds, shift, vector_len);
 3882 }
 3883 
 3884 void MacroAssembler::vpsllw(XMMRegister dst, XMMRegister nds, XMMRegister shift, int vector_len) {
 3885   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; shift-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3886   Assembler::vpsllw(dst, nds, shift, vector_len);
 3887 }
 3888 
 3889 void MacroAssembler::vpsllw(XMMRegister dst, XMMRegister nds, int shift, int vector_len) {
 3890   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3891   Assembler::vpsllw(dst, nds, shift, vector_len);
 3892 }
 3893 
 3894 void MacroAssembler::vptest(XMMRegister dst, XMMRegister src) {
 3895   assert((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16),&quot;XMM register should be 0-15&quot;);
 3896   Assembler::vptest(dst, src);
 3897 }
 3898 
 3899 void MacroAssembler::punpcklbw(XMMRegister dst, XMMRegister src) {
 3900   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3901   Assembler::punpcklbw(dst, src);
 3902 }
 3903 
 3904 void MacroAssembler::pshufd(XMMRegister dst, Address src, int mode) {
 3905   assert(((dst-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vl()),&quot;XMM register should be 0-15&quot;);
 3906   Assembler::pshufd(dst, src, mode);
 3907 }
 3908 
 3909 void MacroAssembler::pshuflw(XMMRegister dst, XMMRegister src, int mode) {
 3910   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; src-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vlbw()),&quot;XMM register should be 0-15&quot;);
 3911   Assembler::pshuflw(dst, src, mode);
 3912 }
 3913 
<span class="line-modified"> 3914 void MacroAssembler::vandpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {</span>
 3915   if (reachable(src)) {
 3916     vandpd(dst, nds, as_Address(src), vector_len);
 3917   } else {
<span class="line-modified"> 3918     lea(scratch_reg, src);</span>
<span class="line-modified"> 3919     vandpd(dst, nds, Address(scratch_reg, 0), vector_len);</span>
 3920   }
 3921 }
 3922 
<span class="line-modified"> 3923 void MacroAssembler::vandps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {</span>
 3924   if (reachable(src)) {
 3925     vandps(dst, nds, as_Address(src), vector_len);
 3926   } else {
<span class="line-modified"> 3927     lea(scratch_reg, src);</span>
<span class="line-modified"> 3928     vandps(dst, nds, Address(scratch_reg, 0), vector_len);</span>
 3929   }
 3930 }
 3931 
 3932 void MacroAssembler::vdivsd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3933   if (reachable(src)) {
 3934     vdivsd(dst, nds, as_Address(src));
 3935   } else {
 3936     lea(rscratch1, src);
 3937     vdivsd(dst, nds, Address(rscratch1, 0));
 3938   }
 3939 }
 3940 
 3941 void MacroAssembler::vdivss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3942   if (reachable(src)) {
 3943     vdivss(dst, nds, as_Address(src));
 3944   } else {
 3945     lea(rscratch1, src);
 3946     vdivss(dst, nds, Address(rscratch1, 0));
 3947   }
 3948 }
</pre>
<hr />
<pre>
 3976 
 3977 void MacroAssembler::vsubss(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3978   if (reachable(src)) {
 3979     vsubss(dst, nds, as_Address(src));
 3980   } else {
 3981     lea(rscratch1, src);
 3982     vsubss(dst, nds, Address(rscratch1, 0));
 3983   }
 3984 }
 3985 
 3986 void MacroAssembler::vnegatess(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3987   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
 3988   vxorps(dst, nds, src, Assembler::AVX_128bit);
 3989 }
 3990 
 3991 void MacroAssembler::vnegatesd(XMMRegister dst, XMMRegister nds, AddressLiteral src) {
 3992   assert(((dst-&gt;encoding() &lt; 16 &amp;&amp; nds-&gt;encoding() &lt; 16) || VM_Version::supports_avx512vldq()),&quot;XMM register should be 0-15&quot;);
 3993   vxorpd(dst, nds, src, Assembler::AVX_128bit);
 3994 }
 3995 
<span class="line-modified"> 3996 void MacroAssembler::vxorpd(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {</span>
 3997   if (reachable(src)) {
 3998     vxorpd(dst, nds, as_Address(src), vector_len);
 3999   } else {
<span class="line-modified"> 4000     lea(scratch_reg, src);</span>
<span class="line-modified"> 4001     vxorpd(dst, nds, Address(scratch_reg, 0), vector_len);</span>
 4002   }
 4003 }
 4004 
<span class="line-modified"> 4005 void MacroAssembler::vxorps(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {</span>
 4006   if (reachable(src)) {
 4007     vxorps(dst, nds, as_Address(src), vector_len);
 4008   } else {
<span class="line-modified"> 4009     lea(scratch_reg, src);</span>
<span class="line-modified"> 4010     vxorps(dst, nds, Address(scratch_reg, 0), vector_len);</span>
<span class="line-added"> 4011   }</span>
<span class="line-added"> 4012 }</span>
<span class="line-added"> 4013 </span>
<span class="line-added"> 4014 void MacroAssembler::vpxor(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register scratch_reg) {</span>
<span class="line-added"> 4015   if (UseAVX &gt; 1 || (vector_len &lt; 1)) {</span>
<span class="line-added"> 4016     if (reachable(src)) {</span>
<span class="line-added"> 4017       Assembler::vpxor(dst, nds, as_Address(src), vector_len);</span>
<span class="line-added"> 4018     } else {</span>
<span class="line-added"> 4019       lea(scratch_reg, src);</span>
<span class="line-added"> 4020       Assembler::vpxor(dst, nds, Address(scratch_reg, 0), vector_len);</span>
<span class="line-added"> 4021     }</span>
<span class="line-added"> 4022   }</span>
<span class="line-added"> 4023   else {</span>
<span class="line-added"> 4024     MacroAssembler::vxorpd(dst, nds, src, vector_len, scratch_reg);</span>
<span class="line-added"> 4025   }</span>
<span class="line-added"> 4026 }</span>
<span class="line-added"> 4027 </span>
<span class="line-added"> 4028 //-------------------------------------------------------------------------------------------</span>
<span class="line-added"> 4029 #ifdef COMPILER2</span>
<span class="line-added"> 4030 // Generic instructions support for use in .ad files C2 code generation</span>
<span class="line-added"> 4031 </span>
<span class="line-added"> 4032 void MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, Register scr) {</span>
<span class="line-added"> 4033   if (dst != src) {</span>
<span class="line-added"> 4034     movdqu(dst, src);</span>
<span class="line-added"> 4035   }</span>
<span class="line-added"> 4036   if (opcode == Op_AbsVD) {</span>
<span class="line-added"> 4037     andpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), scr);</span>
<span class="line-added"> 4038   } else {</span>
<span class="line-added"> 4039     assert((opcode == Op_NegVD),&quot;opcode should be Op_NegD&quot;);</span>
<span class="line-added"> 4040     xorpd(dst, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), scr);</span>
<span class="line-added"> 4041   }</span>
<span class="line-added"> 4042 }</span>
<span class="line-added"> 4043 </span>
<span class="line-added"> 4044 void MacroAssembler::vabsnegd(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {</span>
<span class="line-added"> 4045   if (opcode == Op_AbsVD) {</span>
<span class="line-added"> 4046     vandpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_mask()), vector_len, scr);</span>
<span class="line-added"> 4047   } else {</span>
<span class="line-added"> 4048     assert((opcode == Op_NegVD),&quot;opcode should be Op_NegD&quot;);</span>
<span class="line-added"> 4049     vxorpd(dst, src, ExternalAddress(StubRoutines::x86::vector_double_sign_flip()), vector_len, scr);</span>
<span class="line-added"> 4050   }</span>
<span class="line-added"> 4051 }</span>
<span class="line-added"> 4052 </span>
<span class="line-added"> 4053 void MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, Register scr) {</span>
<span class="line-added"> 4054   if (dst != src) {</span>
<span class="line-added"> 4055     movdqu(dst, src);</span>
<span class="line-added"> 4056   }</span>
<span class="line-added"> 4057   if (opcode == Op_AbsVF) {</span>
<span class="line-added"> 4058     andps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), scr);</span>
<span class="line-added"> 4059   } else {</span>
<span class="line-added"> 4060     assert((opcode == Op_NegVF),&quot;opcode should be Op_NegF&quot;);</span>
<span class="line-added"> 4061     xorps(dst, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), scr);</span>
<span class="line-added"> 4062   }</span>
<span class="line-added"> 4063 }</span>
<span class="line-added"> 4064 </span>
<span class="line-added"> 4065 void MacroAssembler::vabsnegf(int opcode, XMMRegister dst, XMMRegister src, int vector_len, Register scr) {</span>
<span class="line-added"> 4066   if (opcode == Op_AbsVF) {</span>
<span class="line-added"> 4067     vandps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_mask()), vector_len, scr);</span>
<span class="line-added"> 4068   } else {</span>
<span class="line-added"> 4069     assert((opcode == Op_NegVF),&quot;opcode should be Op_NegF&quot;);</span>
<span class="line-added"> 4070     vxorps(dst, src, ExternalAddress(StubRoutines::x86::vector_float_sign_flip()), vector_len, scr);</span>
<span class="line-added"> 4071   }</span>
<span class="line-added"> 4072 }</span>
<span class="line-added"> 4073 </span>
<span class="line-added"> 4074 void MacroAssembler::vextendbw(bool sign, XMMRegister dst, XMMRegister src) {</span>
<span class="line-added"> 4075   if (sign) {</span>
<span class="line-added"> 4076     pmovsxbw(dst, src);</span>
<span class="line-added"> 4077   } else {</span>
<span class="line-added"> 4078     pmovzxbw(dst, src);</span>
 4079   }
 4080 }
 4081 
<span class="line-added"> 4082 void MacroAssembler::vextendbw(bool sign, XMMRegister dst, XMMRegister src, int vector_len) {</span>
<span class="line-added"> 4083   if (sign) {</span>
<span class="line-added"> 4084     vpmovsxbw(dst, src, vector_len);</span>
<span class="line-added"> 4085   } else {</span>
<span class="line-added"> 4086     vpmovzxbw(dst, src, vector_len);</span>
<span class="line-added"> 4087   }</span>
<span class="line-added"> 4088 }</span>
<span class="line-added"> 4089 </span>
<span class="line-added"> 4090 void MacroAssembler::vshiftd(int opcode, XMMRegister dst, XMMRegister src) {</span>
<span class="line-added"> 4091   if (opcode == Op_RShiftVI) {</span>
<span class="line-added"> 4092     psrad(dst, src);</span>
<span class="line-added"> 4093   } else if (opcode == Op_LShiftVI) {</span>
<span class="line-added"> 4094     pslld(dst, src);</span>
<span class="line-added"> 4095   } else {</span>
<span class="line-added"> 4096     assert((opcode == Op_URShiftVI),&quot;opcode should be Op_URShiftVI&quot;);</span>
<span class="line-added"> 4097     psrld(dst, src);</span>
<span class="line-added"> 4098   }</span>
<span class="line-added"> 4099 }</span>
<span class="line-added"> 4100 </span>
<span class="line-added"> 4101 void MacroAssembler::vshiftd(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {</span>
<span class="line-added"> 4102   if (opcode == Op_RShiftVI) {</span>
<span class="line-added"> 4103     vpsrad(dst, nds, src, vector_len);</span>
<span class="line-added"> 4104   } else if (opcode == Op_LShiftVI) {</span>
<span class="line-added"> 4105     vpslld(dst, nds, src, vector_len);</span>
<span class="line-added"> 4106   } else {</span>
<span class="line-added"> 4107     assert((opcode == Op_URShiftVI),&quot;opcode should be Op_URShiftVI&quot;);</span>
<span class="line-added"> 4108     vpsrld(dst, nds, src, vector_len);</span>
<span class="line-added"> 4109   }</span>
<span class="line-added"> 4110 }</span>
<span class="line-added"> 4111 </span>
<span class="line-added"> 4112 void MacroAssembler::vshiftw(int opcode, XMMRegister dst, XMMRegister src) {</span>
<span class="line-added"> 4113   if ((opcode == Op_RShiftVS) || (opcode == Op_RShiftVB)) {</span>
<span class="line-added"> 4114     psraw(dst, src);</span>
<span class="line-added"> 4115   } else if ((opcode == Op_LShiftVS) || (opcode == Op_LShiftVB)) {</span>
<span class="line-added"> 4116     psllw(dst, src);</span>
<span class="line-added"> 4117   } else {</span>
<span class="line-added"> 4118     assert(((opcode == Op_URShiftVS) || (opcode == Op_URShiftVB)),&quot;opcode should be one of Op_URShiftVS or Op_URShiftVB&quot;);</span>
<span class="line-added"> 4119     psrlw(dst, src);</span>
<span class="line-added"> 4120   }</span>
<span class="line-added"> 4121 }</span>
<span class="line-added"> 4122 </span>
<span class="line-added"> 4123 void MacroAssembler::vshiftw(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {</span>
<span class="line-added"> 4124   if ((opcode == Op_RShiftVS) || (opcode == Op_RShiftVB)) {</span>
<span class="line-added"> 4125     vpsraw(dst, nds, src, vector_len);</span>
<span class="line-added"> 4126   } else if ((opcode == Op_LShiftVS) || (opcode == Op_LShiftVB)) {</span>
<span class="line-added"> 4127     vpsllw(dst, nds, src, vector_len);</span>
<span class="line-added"> 4128   } else {</span>
<span class="line-added"> 4129     assert(((opcode == Op_URShiftVS) || (opcode == Op_URShiftVB)),&quot;opcode should be one of Op_URShiftVS or Op_URShiftVB&quot;);</span>
<span class="line-added"> 4130     vpsrlw(dst, nds, src, vector_len);</span>
<span class="line-added"> 4131   }</span>
<span class="line-added"> 4132 }</span>
<span class="line-added"> 4133 </span>
<span class="line-added"> 4134 void MacroAssembler::vshiftq(int opcode, XMMRegister dst, XMMRegister src) {</span>
<span class="line-added"> 4135   if (opcode == Op_RShiftVL) {</span>
<span class="line-added"> 4136     psrlq(dst, src);  // using srl to implement sra on pre-avs512 systems</span>
<span class="line-added"> 4137   } else if (opcode == Op_LShiftVL) {</span>
<span class="line-added"> 4138     psllq(dst, src);</span>
<span class="line-added"> 4139   } else {</span>
<span class="line-added"> 4140     assert((opcode == Op_URShiftVL),&quot;opcode should be Op_URShiftVL&quot;);</span>
<span class="line-added"> 4141     psrlq(dst, src);</span>
<span class="line-added"> 4142   }</span>
<span class="line-added"> 4143 }</span>
<span class="line-added"> 4144 </span>
<span class="line-added"> 4145 void MacroAssembler::vshiftq(int opcode, XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {</span>
<span class="line-added"> 4146   if (opcode == Op_RShiftVL) {</span>
<span class="line-added"> 4147     evpsraq(dst, nds, src, vector_len);</span>
<span class="line-added"> 4148   } else if (opcode == Op_LShiftVL) {</span>
<span class="line-added"> 4149     vpsllq(dst, nds, src, vector_len);</span>
<span class="line-added"> 4150   } else {</span>
<span class="line-added"> 4151     assert((opcode == Op_URShiftVL),&quot;opcode should be Op_URShiftVL&quot;);</span>
<span class="line-added"> 4152     vpsrlq(dst, nds, src, vector_len);</span>
<span class="line-added"> 4153   }</span>
<span class="line-added"> 4154 }</span>
<span class="line-added"> 4155 #endif</span>
<span class="line-added"> 4156 //-------------------------------------------------------------------------------------------</span>
<span class="line-added"> 4157 </span>
 4158 void MacroAssembler::clear_jweak_tag(Register possibly_jweak) {
 4159   const int32_t inverted_jweak_mask = ~static_cast&lt;int32_t&gt;(JNIHandles::weak_tag_mask);
 4160   STATIC_ASSERT(inverted_jweak_mask == -2); // otherwise check this code
 4161   // The inverted mask is sign-extended
 4162   andptr(possibly_jweak, inverted_jweak_mask);
 4163 }
 4164 
 4165 void MacroAssembler::resolve_jobject(Register value,
 4166                                      Register thread,
 4167                                      Register tmp) {
 4168   assert_different_registers(value, thread, tmp);
 4169   Label done, not_weak;
 4170   testptr(value, value);
 4171   jcc(Assembler::zero, done);                // Use NULL as-is.
 4172   testptr(value, JNIHandles::weak_tag_mask); // Test for jweak tag.
 4173   jcc(Assembler::zero, not_weak);
 4174   // Resolve jweak.
 4175   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF,
 4176                  value, Address(value, -JNIHandles::weak_tag_value), tmp, thread);
 4177   verify_oop(value);
</pre>
<hr />
<pre>
 4572     assert(!pushed_rdi, &quot;rdi must be left non-NULL&quot;);
 4573     // Also, the condition codes are properly set Z/NZ on succeed/failure.
 4574   }
 4575 
 4576   if (L_failure == &amp;L_fallthrough)
 4577         jccb(Assembler::notEqual, *L_failure);
 4578   else  jcc(Assembler::notEqual, *L_failure);
 4579 
 4580   // Success.  Cache the super we found and proceed in triumph.
 4581   movptr(super_cache_addr, super_klass);
 4582 
 4583   if (L_success != &amp;L_fallthrough) {
 4584     jmp(*L_success);
 4585   }
 4586 
 4587 #undef IS_A_TEMP
 4588 
 4589   bind(L_fallthrough);
 4590 }
 4591 
<span class="line-added"> 4592 void MacroAssembler::clinit_barrier(Register klass, Register thread, Label* L_fast_path, Label* L_slow_path) {</span>
<span class="line-added"> 4593   assert(L_fast_path != NULL || L_slow_path != NULL, &quot;at least one is required&quot;);</span>
<span class="line-added"> 4594 </span>
<span class="line-added"> 4595   Label L_fallthrough;</span>
<span class="line-added"> 4596   if (L_fast_path == NULL) {</span>
<span class="line-added"> 4597     L_fast_path = &amp;L_fallthrough;</span>
<span class="line-added"> 4598   } else if (L_slow_path == NULL) {</span>
<span class="line-added"> 4599     L_slow_path = &amp;L_fallthrough;</span>
<span class="line-added"> 4600   }</span>
<span class="line-added"> 4601 </span>
<span class="line-added"> 4602   // Fast path check: class is fully initialized</span>
<span class="line-added"> 4603   cmpb(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);</span>
<span class="line-added"> 4604   jcc(Assembler::equal, *L_fast_path);</span>
<span class="line-added"> 4605 </span>
<span class="line-added"> 4606   // Fast path check: current thread is initializer thread</span>
<span class="line-added"> 4607   cmpptr(thread, Address(klass, InstanceKlass::init_thread_offset()));</span>
<span class="line-added"> 4608   if (L_slow_path == &amp;L_fallthrough) {</span>
<span class="line-added"> 4609     jcc(Assembler::equal, *L_fast_path);</span>
<span class="line-added"> 4610     bind(*L_slow_path);</span>
<span class="line-added"> 4611   } else if (L_fast_path == &amp;L_fallthrough) {</span>
<span class="line-added"> 4612     jcc(Assembler::notEqual, *L_slow_path);</span>
<span class="line-added"> 4613     bind(*L_fast_path);</span>
<span class="line-added"> 4614   } else {</span>
<span class="line-added"> 4615     Unimplemented();</span>
<span class="line-added"> 4616   }</span>
<span class="line-added"> 4617 }</span>
 4618 
 4619 void MacroAssembler::cmov32(Condition cc, Register dst, Address src) {
 4620   if (VM_Version::supports_cmov()) {
 4621     cmovl(cc, dst, src);
 4622   } else {
 4623     Label L;
 4624     jccb(negate_condition(cc), L);
 4625     movl(dst, src);
 4626     bind(L);
 4627   }
 4628 }
 4629 
 4630 void MacroAssembler::cmov32(Condition cc, Register dst, Register src) {
 4631   if (VM_Version::supports_cmov()) {
 4632     cmovl(cc, dst, src);
 4633   } else {
 4634     Label L;
 4635     jccb(negate_condition(cc), L);
 4636     movl(dst, src);
 4637     bind(L);
</pre>
<hr />
<pre>
 5051     printf(&quot;--------------------------------------------------\n&quot;);
 5052   }
 5053 
 5054 };
 5055 
 5056 
 5057 static void _print_CPU_state(CPU_State* state) {
 5058   state-&gt;print();
 5059 };
 5060 
 5061 
 5062 void MacroAssembler::print_CPU_state() {
 5063   push_CPU_state();
 5064   push(rsp);                // pass CPU state
 5065   call(RuntimeAddress(CAST_FROM_FN_PTR(address, _print_CPU_state)));
 5066   addptr(rsp, wordSize);       // discard argument
 5067   pop_CPU_state();
 5068 }
 5069 
 5070 
<span class="line-added"> 5071 #ifndef _LP64</span>
 5072 static bool _verify_FPU(int stack_depth, char* s, CPU_State* state) {
 5073   static int counter = 0;
 5074   FPU_State* fs = &amp;state-&gt;_fpu_state;
 5075   counter++;
 5076   // For leaf calls, only verify that the top few elements remain empty.
 5077   // We only need 1 empty at the top for C2 code.
 5078   if( stack_depth &lt; 0 ) {
 5079     if( fs-&gt;tag_for_st(7) != 3 ) {
 5080       printf(&quot;FPR7 not empty\n&quot;);
 5081       state-&gt;print();
 5082       assert(false, &quot;error&quot;);
 5083       return false;
 5084     }
 5085     return true;                // All other stack states do not matter
 5086   }
 5087 
 5088   assert((fs-&gt;_control_word._value &amp; 0xffff) == StubRoutines::_fpu_cntrl_wrd_std,
 5089          &quot;bad FPU control word&quot;);
 5090 
 5091   // compute stack depth
</pre>
<hr />
<pre>
 5108       // too many elements on the stack
 5109       printf(&quot;%s: &lt;= %d stack elements expected but found %d\n&quot;, s, -stack_depth, d);
 5110       state-&gt;print();
 5111       assert(false, &quot;error&quot;);
 5112       return false;
 5113     }
 5114   } else {
 5115     // expected stack depth is stack_depth
 5116     if (d != stack_depth) {
 5117       // wrong stack depth
 5118       printf(&quot;%s: %d stack elements expected but found %d\n&quot;, s, stack_depth, d);
 5119       state-&gt;print();
 5120       assert(false, &quot;error&quot;);
 5121       return false;
 5122     }
 5123   }
 5124   // everything is cool
 5125   return true;
 5126 }
 5127 

 5128 void MacroAssembler::verify_FPU(int stack_depth, const char* s) {
 5129   if (!VerifyFPU) return;
 5130   push_CPU_state();
 5131   push(rsp);                // pass CPU state
 5132   ExternalAddress msg((address) s);
 5133   // pass message string s
 5134   pushptr(msg.addr());
 5135   push(stack_depth);        // pass stack depth
 5136   call(RuntimeAddress(CAST_FROM_FN_PTR(address, _verify_FPU)));
 5137   addptr(rsp, 3 * wordSize);   // discard arguments
 5138   // check for error
 5139   { Label L;
 5140     testl(rax, rax);
 5141     jcc(Assembler::notZero, L);
 5142     int3();                  // break if error condition
 5143     bind(L);
 5144   }
 5145   pop_CPU_state();
 5146 }
<span class="line-added"> 5147 #endif // _LP64</span>
 5148 
 5149 void MacroAssembler::restore_cpu_control_state_after_jni() {
 5150   // Either restore the MXCSR register after returning from the JNI Call
 5151   // or verify that it wasn&#39;t changed (with -Xcheck:jni flag).
 5152   if (VM_Version::supports_sse()) {
 5153     if (RestoreMXCSROnJNICalls) {
 5154       ldmxcsr(ExternalAddress(StubRoutines::addr_mxcsr_std()));
 5155     } else if (CheckJNICalls) {
 5156       call(RuntimeAddress(StubRoutines::x86::verify_mxcsr_entry()));
 5157     }
 5158   }
 5159   // Clear upper bits of YMM registers to avoid SSE &lt;-&gt; AVX transition penalty.
 5160   vzeroupper();
 5161   // Reset k1 to 0xffff.
 5162 
 5163 #ifdef COMPILER2
 5164   if (PostLoopMultiversioning &amp;&amp; VM_Version::supports_evex()) {
 5165     push(rcx);
 5166     movl(rcx, 0xffff);
 5167     kmovwl(k1, rcx);
</pre>
<hr />
<pre>
 5172 #ifndef _LP64
 5173   // Either restore the x87 floating pointer control word after returning
 5174   // from the JNI call or verify that it wasn&#39;t changed.
 5175   if (CheckJNICalls) {
 5176     call(RuntimeAddress(StubRoutines::x86::verify_fpu_cntrl_wrd_entry()));
 5177   }
 5178 #endif // _LP64
 5179 }
 5180 
 5181 // ((OopHandle)result).resolve();
 5182 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
 5183   assert_different_registers(result, tmp);
 5184 
 5185   // Only 64 bit platforms support GCs that require a tmp register
 5186   // Only IN_HEAP loads require a thread_tmp register
 5187   // OopHandle::resolve is an indirection like jobject.
 5188   access_load_at(T_OBJECT, IN_NATIVE,
 5189                  result, Address(result, 0), tmp, /*tmp_thread*/noreg);
 5190 }
 5191 
<span class="line-added"> 5192 // ((WeakHandle)result).resolve();</span>
<span class="line-added"> 5193 void MacroAssembler::resolve_weak_handle(Register rresult, Register rtmp) {</span>
<span class="line-added"> 5194   assert_different_registers(rresult, rtmp);</span>
<span class="line-added"> 5195   Label resolved;</span>
<span class="line-added"> 5196 </span>
<span class="line-added"> 5197   // A null weak handle resolves to null.</span>
<span class="line-added"> 5198   cmpptr(rresult, 0);</span>
<span class="line-added"> 5199   jcc(Assembler::equal, resolved);</span>
<span class="line-added"> 5200 </span>
<span class="line-added"> 5201   // Only 64 bit platforms support GCs that require a tmp register</span>
<span class="line-added"> 5202   // Only IN_HEAP loads require a thread_tmp register</span>
<span class="line-added"> 5203   // WeakHandle::resolve is an indirection like jweak.</span>
<span class="line-added"> 5204   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF,</span>
<span class="line-added"> 5205                  rresult, Address(rresult, 0), rtmp, /*tmp_thread*/noreg);</span>
<span class="line-added"> 5206   bind(resolved);</span>
<span class="line-added"> 5207 }</span>
<span class="line-added"> 5208 </span>
 5209 void MacroAssembler::load_mirror(Register mirror, Register method, Register tmp) {
 5210   // get mirror
 5211   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
<span class="line-modified"> 5212   load_method_holder(mirror, method);</span>


 5213   movptr(mirror, Address(mirror, mirror_offset));
 5214   resolve_oop_handle(mirror, tmp);
 5215 }
 5216 
<span class="line-added"> 5217 void MacroAssembler::load_method_holder_cld(Register rresult, Register rmethod) {</span>
<span class="line-added"> 5218   load_method_holder(rresult, rmethod);</span>
<span class="line-added"> 5219   movptr(rresult, Address(rresult, InstanceKlass::class_loader_data_offset()));</span>
<span class="line-added"> 5220 }</span>
<span class="line-added"> 5221 </span>
<span class="line-added"> 5222 void MacroAssembler::load_method_holder(Register holder, Register method) {</span>
<span class="line-added"> 5223   movptr(holder, Address(method, Method::const_offset()));                      // ConstMethod*</span>
<span class="line-added"> 5224   movptr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*</span>
<span class="line-added"> 5225   movptr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*</span>
<span class="line-added"> 5226 }</span>
<span class="line-added"> 5227 </span>
 5228 void MacroAssembler::load_klass(Register dst, Register src) {
 5229 #ifdef _LP64
 5230   if (UseCompressedClassPointers) {
 5231     movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));
 5232     decode_klass_not_null(dst);
 5233   } else
 5234 #endif
 5235     movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
 5236 }
 5237 
 5238 void MacroAssembler::load_prototype_header(Register dst, Register src) {
 5239   load_klass(dst, src);
 5240   movptr(dst, Address(dst, Klass::prototype_header_offset()));
 5241 }
 5242 
 5243 void MacroAssembler::store_klass(Register dst, Register src) {
 5244 #ifdef _LP64
 5245   if (UseCompressedClassPointers) {
 5246     encode_klass_not_null(src);
 5247     movl(Address(dst, oopDesc::klass_offset_in_bytes()), src);
</pre>
<hr />
<pre>
 5302 // Used for storing NULLs.
 5303 void MacroAssembler::store_heap_oop_null(Address dst) {
 5304   access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);
 5305 }
 5306 
 5307 #ifdef _LP64
 5308 void MacroAssembler::store_klass_gap(Register dst, Register src) {
 5309   if (UseCompressedClassPointers) {
 5310     // Store to klass gap in destination
 5311     movl(Address(dst, oopDesc::klass_gap_offset_in_bytes()), src);
 5312   }
 5313 }
 5314 
 5315 #ifdef ASSERT
 5316 void MacroAssembler::verify_heapbase(const char* msg) {
 5317   assert (UseCompressedOops, &quot;should be compressed&quot;);
 5318   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5319   if (CheckCompressedOops) {
 5320     Label ok;
 5321     push(rscratch1); // cmpptr trashes rscratch1
<span class="line-modified"> 5322     cmpptr(r12_heapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));</span>
 5323     jcc(Assembler::equal, ok);
 5324     STOP(msg);
 5325     bind(ok);
 5326     pop(rscratch1);
 5327   }
 5328 }
 5329 #endif
 5330 
 5331 // Algorithm must match oop.inline.hpp encode_heap_oop.
 5332 void MacroAssembler::encode_heap_oop(Register r) {
 5333 #ifdef ASSERT
 5334   verify_heapbase(&quot;MacroAssembler::encode_heap_oop: heap base corrupted?&quot;);
 5335 #endif
 5336   verify_oop(r, &quot;broken oop in encode_heap_oop&quot;);
<span class="line-modified"> 5337   if (CompressedOops::base() == NULL) {</span>
<span class="line-modified"> 5338     if (CompressedOops::shift() != 0) {</span>
<span class="line-modified"> 5339       assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
 5340       shrq(r, LogMinObjAlignmentInBytes);
 5341     }
 5342     return;
 5343   }
 5344   testq(r, r);
 5345   cmovq(Assembler::equal, r, r12_heapbase);
 5346   subq(r, r12_heapbase);
 5347   shrq(r, LogMinObjAlignmentInBytes);
 5348 }
 5349 
 5350 void MacroAssembler::encode_heap_oop_not_null(Register r) {
 5351 #ifdef ASSERT
 5352   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null: heap base corrupted?&quot;);
 5353   if (CheckCompressedOops) {
 5354     Label ok;
 5355     testq(r, r);
 5356     jcc(Assembler::notEqual, ok);
 5357     STOP(&quot;null oop passed to encode_heap_oop_not_null&quot;);
 5358     bind(ok);
 5359   }
 5360 #endif
 5361   verify_oop(r, &quot;broken oop in encode_heap_oop_not_null&quot;);
<span class="line-modified"> 5362   if (CompressedOops::base() != NULL) {</span>
 5363     subq(r, r12_heapbase);
 5364   }
<span class="line-modified"> 5365   if (CompressedOops::shift() != 0) {</span>
<span class="line-modified"> 5366     assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
 5367     shrq(r, LogMinObjAlignmentInBytes);
 5368   }
 5369 }
 5370 
 5371 void MacroAssembler::encode_heap_oop_not_null(Register dst, Register src) {
 5372 #ifdef ASSERT
 5373   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null2: heap base corrupted?&quot;);
 5374   if (CheckCompressedOops) {
 5375     Label ok;
 5376     testq(src, src);
 5377     jcc(Assembler::notEqual, ok);
 5378     STOP(&quot;null oop passed to encode_heap_oop_not_null2&quot;);
 5379     bind(ok);
 5380   }
 5381 #endif
 5382   verify_oop(src, &quot;broken oop in encode_heap_oop_not_null2&quot;);
 5383   if (dst != src) {
 5384     movq(dst, src);
 5385   }
<span class="line-modified"> 5386   if (CompressedOops::base() != NULL) {</span>
 5387     subq(dst, r12_heapbase);
 5388   }
<span class="line-modified"> 5389   if (CompressedOops::shift() != 0) {</span>
<span class="line-modified"> 5390     assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
 5391     shrq(dst, LogMinObjAlignmentInBytes);
 5392   }
 5393 }
 5394 
 5395 void  MacroAssembler::decode_heap_oop(Register r) {
 5396 #ifdef ASSERT
 5397   verify_heapbase(&quot;MacroAssembler::decode_heap_oop: heap base corrupted?&quot;);
 5398 #endif
<span class="line-modified"> 5399   if (CompressedOops::base() == NULL) {</span>
<span class="line-modified"> 5400     if (CompressedOops::shift() != 0) {</span>
<span class="line-modified"> 5401       assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
 5402       shlq(r, LogMinObjAlignmentInBytes);
 5403     }
 5404   } else {
 5405     Label done;
 5406     shlq(r, LogMinObjAlignmentInBytes);
 5407     jccb(Assembler::equal, done);
 5408     addq(r, r12_heapbase);
 5409     bind(done);
 5410   }
 5411   verify_oop(r, &quot;broken oop in decode_heap_oop&quot;);
 5412 }
 5413 
 5414 void  MacroAssembler::decode_heap_oop_not_null(Register r) {
 5415   // Note: it will change flags
 5416   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5417   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5418   // Cannot assert, unverified entry point counts instructions (see .ad file)
 5419   // vtableStubs also counts instructions in pd_code_size_limit.
 5420   // Also do not verify_oop as this is called by verify_oop.
<span class="line-modified"> 5421   if (CompressedOops::shift() != 0) {</span>
<span class="line-modified"> 5422     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
 5423     shlq(r, LogMinObjAlignmentInBytes);
<span class="line-modified"> 5424     if (CompressedOops::base() != NULL) {</span>
 5425       addq(r, r12_heapbase);
 5426     }
 5427   } else {
<span class="line-modified"> 5428     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);</span>
 5429   }
 5430 }
 5431 
 5432 void  MacroAssembler::decode_heap_oop_not_null(Register dst, Register src) {
 5433   // Note: it will change flags
 5434   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5435   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5436   // Cannot assert, unverified entry point counts instructions (see .ad file)
 5437   // vtableStubs also counts instructions in pd_code_size_limit.
 5438   // Also do not verify_oop as this is called by verify_oop.
<span class="line-modified"> 5439   if (CompressedOops::shift() != 0) {</span>
<span class="line-modified"> 5440     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);</span>
 5441     if (LogMinObjAlignmentInBytes == Address::times_8) {
 5442       leaq(dst, Address(r12_heapbase, src, Address::times_8, 0));
 5443     } else {
 5444       if (dst != src) {
 5445         movq(dst, src);
 5446       }
 5447       shlq(dst, LogMinObjAlignmentInBytes);
<span class="line-modified"> 5448       if (CompressedOops::base() != NULL) {</span>
 5449         addq(dst, r12_heapbase);
 5450       }
 5451     }
 5452   } else {
<span class="line-modified"> 5453     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);</span>
 5454     if (dst != src) {
 5455       movq(dst, src);
 5456     }
 5457   }
 5458 }
 5459 
 5460 void MacroAssembler::encode_klass_not_null(Register r) {
<span class="line-modified"> 5461   if (CompressedKlassPointers::base() != NULL) {</span>
 5462     // Use r12 as a scratch register in which to temporarily load the narrow_klass_base.
 5463     assert(r != r12_heapbase, &quot;Encoding a klass in r12&quot;);
<span class="line-modified"> 5464     mov64(r12_heapbase, (int64_t)CompressedKlassPointers::base());</span>
 5465     subq(r, r12_heapbase);
 5466   }
<span class="line-modified"> 5467   if (CompressedKlassPointers::shift() != 0) {</span>
<span class="line-modified"> 5468     assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);</span>
 5469     shrq(r, LogKlassAlignmentInBytes);
 5470   }
<span class="line-modified"> 5471   if (CompressedKlassPointers::base() != NULL) {</span>
 5472     reinit_heapbase();
 5473   }
 5474 }
 5475 
 5476 void MacroAssembler::encode_klass_not_null(Register dst, Register src) {
 5477   if (dst == src) {
 5478     encode_klass_not_null(src);
 5479   } else {
<span class="line-modified"> 5480     if (CompressedKlassPointers::base() != NULL) {</span>
<span class="line-modified"> 5481       mov64(dst, (int64_t)CompressedKlassPointers::base());</span>
 5482       negq(dst);
 5483       addq(dst, src);
 5484     } else {
 5485       movptr(dst, src);
 5486     }
<span class="line-modified"> 5487     if (CompressedKlassPointers::shift() != 0) {</span>
<span class="line-modified"> 5488       assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);</span>
 5489       shrq(dst, LogKlassAlignmentInBytes);
 5490     }
 5491   }
 5492 }
 5493 
 5494 // Function instr_size_for_decode_klass_not_null() counts the instructions
 5495 // generated by decode_klass_not_null(register r) and reinit_heapbase(),
 5496 // when (Universe::heap() != NULL).  Hence, if the instructions they
 5497 // generate change, then this method needs to be updated.
 5498 int MacroAssembler::instr_size_for_decode_klass_not_null() {
 5499   assert (UseCompressedClassPointers, &quot;only for compressed klass ptrs&quot;);
<span class="line-modified"> 5500   if (CompressedKlassPointers::base() != NULL) {</span>
 5501     // mov64 + addq + shlq? + mov64  (for reinit_heapbase()).
<span class="line-modified"> 5502     return (CompressedKlassPointers::shift() == 0 ? 20 : 24);</span>
 5503   } else {
 5504     // longest load decode klass function, mov64, leaq
 5505     return 16;
 5506   }
 5507 }
 5508 
 5509 // !!! If the instructions that get generated here change then function
 5510 // instr_size_for_decode_klass_not_null() needs to get updated.
 5511 void  MacroAssembler::decode_klass_not_null(Register r) {
 5512   // Note: it will change flags
 5513   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5514   assert(r != r12_heapbase, &quot;Decoding a klass in r12&quot;);
 5515   // Cannot assert, unverified entry point counts instructions (see .ad file)
 5516   // vtableStubs also counts instructions in pd_code_size_limit.
 5517   // Also do not verify_oop as this is called by verify_oop.
<span class="line-modified"> 5518   if (CompressedKlassPointers::shift() != 0) {</span>
<span class="line-modified"> 5519     assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);</span>
 5520     shlq(r, LogKlassAlignmentInBytes);
 5521   }
 5522   // Use r12 as a scratch register in which to temporarily load the narrow_klass_base.
<span class="line-modified"> 5523   if (CompressedKlassPointers::base() != NULL) {</span>
<span class="line-modified"> 5524     mov64(r12_heapbase, (int64_t)CompressedKlassPointers::base());</span>
 5525     addq(r, r12_heapbase);
 5526     reinit_heapbase();
 5527   }
 5528 }
 5529 
 5530 void  MacroAssembler::decode_klass_not_null(Register dst, Register src) {
 5531   // Note: it will change flags
 5532   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5533   if (dst == src) {
 5534     decode_klass_not_null(dst);
 5535   } else {
 5536     // Cannot assert, unverified entry point counts instructions (see .ad file)
 5537     // vtableStubs also counts instructions in pd_code_size_limit.
 5538     // Also do not verify_oop as this is called by verify_oop.
<span class="line-modified"> 5539     mov64(dst, (int64_t)CompressedKlassPointers::base());</span>
<span class="line-modified"> 5540     if (CompressedKlassPointers::shift() != 0) {</span>
<span class="line-modified"> 5541       assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);</span>
 5542       assert(LogKlassAlignmentInBytes == Address::times_8, &quot;klass not aligned on 64bits?&quot;);
 5543       leaq(dst, Address(dst, src, Address::times_8, 0));
 5544     } else {
 5545       addq(dst, src);
 5546     }
 5547   }
 5548 }
 5549 
 5550 void  MacroAssembler::set_narrow_oop(Register dst, jobject obj) {
 5551   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5552   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5553   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5554   int oop_index = oop_recorder()-&gt;find_index(obj);
 5555   RelocationHolder rspec = oop_Relocation::spec(oop_index);
 5556   mov_narrow_oop(dst, oop_index, rspec);
 5557 }
 5558 
 5559 void  MacroAssembler::set_narrow_oop(Address dst, jobject obj) {
 5560   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5561   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5562   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5563   int oop_index = oop_recorder()-&gt;find_index(obj);
 5564   RelocationHolder rspec = oop_Relocation::spec(oop_index);
 5565   mov_narrow_oop(dst, oop_index, rspec);
 5566 }
 5567 
 5568 void  MacroAssembler::set_narrow_klass(Register dst, Klass* k) {
 5569   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5570   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5571   int klass_index = oop_recorder()-&gt;find_index(k);
 5572   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
<span class="line-modified"> 5573   mov_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);</span>
 5574 }
 5575 
 5576 void  MacroAssembler::set_narrow_klass(Address dst, Klass* k) {
 5577   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5578   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5579   int klass_index = oop_recorder()-&gt;find_index(k);
 5580   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
<span class="line-modified"> 5581   mov_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);</span>
 5582 }
 5583 
 5584 void  MacroAssembler::cmp_narrow_oop(Register dst, jobject obj) {
 5585   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5586   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5587   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5588   int oop_index = oop_recorder()-&gt;find_index(obj);
 5589   RelocationHolder rspec = oop_Relocation::spec(oop_index);
 5590   Assembler::cmp_narrow_oop(dst, oop_index, rspec);
 5591 }
 5592 
 5593 void  MacroAssembler::cmp_narrow_oop(Address dst, jobject obj) {
 5594   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
 5595   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
 5596   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5597   int oop_index = oop_recorder()-&gt;find_index(obj);
 5598   RelocationHolder rspec = oop_Relocation::spec(oop_index);
 5599   Assembler::cmp_narrow_oop(dst, oop_index, rspec);
 5600 }
 5601 
 5602 void  MacroAssembler::cmp_narrow_klass(Register dst, Klass* k) {
 5603   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5604   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5605   int klass_index = oop_recorder()-&gt;find_index(k);
 5606   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
<span class="line-modified"> 5607   Assembler::cmp_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);</span>
 5608 }
 5609 
 5610 void  MacroAssembler::cmp_narrow_klass(Address dst, Klass* k) {
 5611   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
 5612   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 5613   int klass_index = oop_recorder()-&gt;find_index(k);
 5614   RelocationHolder rspec = metadata_Relocation::spec(klass_index);
<span class="line-modified"> 5615   Assembler::cmp_narrow_oop(dst, CompressedKlassPointers::encode(k), rspec);</span>
 5616 }
 5617 
 5618 void MacroAssembler::reinit_heapbase() {
 5619   if (UseCompressedOops || UseCompressedClassPointers) {
 5620     if (Universe::heap() != NULL) {
<span class="line-modified"> 5621       if (CompressedOops::base() == NULL) {</span>
 5622         MacroAssembler::xorptr(r12_heapbase, r12_heapbase);
 5623       } else {
<span class="line-modified"> 5624         mov64(r12_heapbase, (int64_t)CompressedOops::ptrs_base());</span>
 5625       }
 5626     } else {
<span class="line-modified"> 5627       movptr(r12_heapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));</span>
 5628     }
 5629   }
 5630 }
 5631 
 5632 #endif // _LP64
 5633 
 5634 // C2 compiled method&#39;s prolog code.
 5635 void MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {
 5636 
 5637   // WARNING: Initial instruction MUST be 5 bytes or longer so that
 5638   // NativeJump::patch_verified_entry will be able to patch out the entry
 5639   // code safely. The push to verify stack depth is ok at 5 bytes,
 5640   // the frame allocation can be either 3 or 6 bytes. So if we don&#39;t do
 5641   // stack bang then we must use the 6 byte frame allocation even if
 5642   // we have no frame. :-(
 5643   assert(stack_bang_size &gt;= framesize || stack_bang_size &lt;= 0, &quot;stack bang size incorrect&quot;);
 5644 
 5645   assert((framesize &amp; (StackAlignmentInBytes-1)) == 0, &quot;frame size not aligned&quot;);
 5646   // Remove word for return addr
 5647   framesize -= wordSize;
</pre>
<hr />
<pre>
 6323   bind(CLEANUP);
 6324   pop(rsp); // restore SP
 6325 
 6326 } // string_indexof
 6327 
 6328 void MacroAssembler::string_indexof_char(Register str1, Register cnt1, Register ch, Register result,
 6329                                          XMMRegister vec1, XMMRegister vec2, XMMRegister vec3, Register tmp) {
 6330   ShortBranchVerifier sbv(this);
 6331   assert(UseSSE42Intrinsics, &quot;SSE4.2 intrinsics are required&quot;);
 6332 
 6333   int stride = 8;
 6334 
 6335   Label FOUND_CHAR, SCAN_TO_CHAR, SCAN_TO_CHAR_LOOP,
 6336         SCAN_TO_8_CHAR, SCAN_TO_8_CHAR_LOOP, SCAN_TO_16_CHAR_LOOP,
 6337         RET_NOT_FOUND, SCAN_TO_8_CHAR_INIT,
 6338         FOUND_SEQ_CHAR, DONE_LABEL;
 6339 
 6340   movptr(result, str1);
 6341   if (UseAVX &gt;= 2) {
 6342     cmpl(cnt1, stride);
<span class="line-modified"> 6343     jcc(Assembler::less, SCAN_TO_CHAR);</span>
 6344     cmpl(cnt1, 2*stride);
 6345     jcc(Assembler::less, SCAN_TO_8_CHAR_INIT);
 6346     movdl(vec1, ch);
 6347     vpbroadcastw(vec1, vec1, Assembler::AVX_256bit);
 6348     vpxor(vec2, vec2);
 6349     movl(tmp, cnt1);
 6350     andl(tmp, 0xFFFFFFF0);  //vector count (in chars)
 6351     andl(cnt1,0x0000000F);  //tail count (in chars)
 6352 
 6353     bind(SCAN_TO_16_CHAR_LOOP);
 6354     vmovdqu(vec3, Address(result, 0));
 6355     vpcmpeqw(vec3, vec3, vec1, 1);
 6356     vptest(vec2, vec3);
 6357     jcc(Assembler::carryClear, FOUND_CHAR);
 6358     addptr(result, 32);
 6359     subl(tmp, 2*stride);
 6360     jcc(Assembler::notZero, SCAN_TO_16_CHAR_LOOP);
 6361     jmp(SCAN_TO_8_CHAR);
 6362     bind(SCAN_TO_8_CHAR_INIT);
 6363     movdl(vec1, ch);
 6364     pshuflw(vec1, vec1, 0x00);
 6365     pshufd(vec1, vec1, 0);
 6366     pxor(vec2, vec2);
 6367   }
 6368   bind(SCAN_TO_8_CHAR);
 6369   cmpl(cnt1, stride);
<span class="line-modified"> 6370   jcc(Assembler::less, SCAN_TO_CHAR);</span>
<span class="line-modified"> 6371   if (UseAVX &lt; 2) {</span>


 6372     movdl(vec1, ch);
 6373     pshuflw(vec1, vec1, 0x00);
 6374     pshufd(vec1, vec1, 0);
 6375     pxor(vec2, vec2);
 6376   }
 6377   movl(tmp, cnt1);
 6378   andl(tmp, 0xFFFFFFF8);  //vector count (in chars)
 6379   andl(cnt1,0x00000007);  //tail count (in chars)
 6380 
 6381   bind(SCAN_TO_8_CHAR_LOOP);
 6382   movdqu(vec3, Address(result, 0));
 6383   pcmpeqw(vec3, vec1);
 6384   ptest(vec2, vec3);
 6385   jcc(Assembler::carryClear, FOUND_CHAR);
 6386   addptr(result, 16);
 6387   subl(tmp, stride);
 6388   jcc(Assembler::notZero, SCAN_TO_8_CHAR_LOOP);
 6389   bind(SCAN_TO_CHAR);
 6390   testl(cnt1, cnt1);
 6391   jcc(Assembler::zero, RET_NOT_FOUND);
</pre>
<hr />
<pre>
 6564     jmp(POP_LABEL);
 6565 
 6566     // Setup the registers to start vector comparison loop
 6567     bind(COMPARE_WIDE_VECTORS);
 6568     if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 6569       lea(str1, Address(str1, result, scale));
 6570       lea(str2, Address(str2, result, scale));
 6571     } else {
 6572       lea(str1, Address(str1, result, scale1));
 6573       lea(str2, Address(str2, result, scale2));
 6574     }
 6575     subl(result, stride2);
 6576     subl(cnt2, stride2);
 6577     jcc(Assembler::zero, COMPARE_WIDE_TAIL);
 6578     negptr(result);
 6579 
 6580     //  In a loop, compare 16-chars (32-bytes) at once using (vpxor+vptest)
 6581     bind(COMPARE_WIDE_VECTORS_LOOP);
 6582 
 6583 #ifdef _LP64
<span class="line-modified"> 6584     if ((AVX3Threshold == 0) &amp;&amp; VM_Version::supports_avx512vlbw()) { // trying 64 bytes fast loop</span>
 6585       cmpl(cnt2, stride2x2);
 6586       jccb(Assembler::below, COMPARE_WIDE_VECTORS_LOOP_AVX2);
 6587       testl(cnt2, stride2x2-1);   // cnt2 holds the vector count
 6588       jccb(Assembler::notZero, COMPARE_WIDE_VECTORS_LOOP_AVX2);   // means we cannot subtract by 0x40
 6589 
 6590       bind(COMPARE_WIDE_VECTORS_LOOP_AVX3); // the hottest loop
 6591       if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
 6592         evmovdquq(vec1, Address(str1, result, scale), Assembler::AVX_512bit);
 6593         evpcmpeqb(k7, vec1, Address(str2, result, scale), Assembler::AVX_512bit); // k7 == 11..11, if operands equal, otherwise k7 has some 0
 6594       } else {
 6595         vpmovzxbw(vec1, Address(str1, result, scale1), Assembler::AVX_512bit);
 6596         evpcmpeqb(k7, vec1, Address(str2, result, scale2), Assembler::AVX_512bit); // k7 == 11..11, if operands equal, otherwise k7 has some 0
 6597       }
 6598       kortestql(k7, k7);
 6599       jcc(Assembler::aboveEqual, COMPARE_WIDE_VECTORS_LOOP_FAILED);     // miscompare
 6600       addptr(result, stride2x2);  // update since we already compared at this addr
 6601       subl(cnt2, stride2x2);      // and sub the size too
 6602       jccb(Assembler::notZero, COMPARE_WIDE_VECTORS_LOOP_AVX3);
 6603 
 6604       vpxor(vec1, vec1);
</pre>
<hr />
<pre>
 6824 //         return true;
 6825 //       }
 6826 //     }
 6827 //     return false;
 6828 //   }
 6829 void MacroAssembler::has_negatives(Register ary1, Register len,
 6830   Register result, Register tmp1,
 6831   XMMRegister vec1, XMMRegister vec2) {
 6832   // rsi: byte array
 6833   // rcx: len
 6834   // rax: result
 6835   ShortBranchVerifier sbv(this);
 6836   assert_different_registers(ary1, len, result, tmp1);
 6837   assert_different_registers(vec1, vec2);
 6838   Label TRUE_LABEL, FALSE_LABEL, DONE, COMPARE_CHAR, COMPARE_VECTORS, COMPARE_BYTE;
 6839 
 6840   // len == 0
 6841   testl(len, len);
 6842   jcc(Assembler::zero, FALSE_LABEL);
 6843 
<span class="line-modified"> 6844   if ((AVX3Threshold == 0) &amp;&amp; (UseAVX &gt; 2) &amp;&amp; // AVX512</span>
 6845     VM_Version::supports_avx512vlbw() &amp;&amp;
 6846     VM_Version::supports_bmi2()) {
 6847 
 6848     Label test_64_loop, test_tail;
 6849     Register tmp3_aliased = len;
 6850 
 6851     movl(tmp1, len);
 6852     vpxor(vec2, vec2, vec2, Assembler::AVX_512bit);
 6853 
 6854     andl(tmp1, 64 - 1);   // tail count (in chars) 0x3F
 6855     andl(len, ~(64 - 1));    // vector count (in chars)
 6856     jccb(Assembler::zero, test_tail);
 6857 
 6858     lea(ary1, Address(ary1, len, Address::times_1));
 6859     negptr(len);
 6860 
 6861     bind(test_64_loop);
 6862     // Check whether our 64 elements of size byte contain negatives
 6863     evpcmpgtb(k2, vec2, Address(ary1, len, Address::times_1), Assembler::AVX_512bit);
 6864     kortestql(k2, k2);
</pre>
<hr />
<pre>
 6897     emit_int64(0x2726252423222120);
 6898     emit_int64(0x2F2E2D2C2B2A2928);
 6899     emit_int64(0x3736353433323130);
 6900     emit_int64(0x3F3E3D3C3B3A3938);
 6901 
 6902     bind(k_init);
 6903     lea(len, InternalAddress(tmp));
 6904     // create mask to test for negative byte inside a vector
 6905     evpbroadcastb(vec1, tmp1, Assembler::AVX_512bit);
 6906     evpcmpgtb(k3, vec1, Address(len, 0), Assembler::AVX_512bit);
 6907 
 6908 #endif
 6909     evpcmpgtb(k2, k3, vec2, Address(ary1, 0), Assembler::AVX_512bit);
 6910     ktestq(k2, k3);
 6911     jcc(Assembler::notZero, TRUE_LABEL);
 6912 
 6913     jmp(FALSE_LABEL);
 6914   } else {
 6915     movl(result, len); // copy
 6916 
<span class="line-modified"> 6917     if (UseAVX &gt;= 2 &amp;&amp; UseSSE &gt;= 2) {</span>
 6918       // With AVX2, use 32-byte vector compare
 6919       Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
 6920 
 6921       // Compare 32-byte vectors
 6922       andl(result, 0x0000001f);  //   tail count (in bytes)
 6923       andl(len, 0xffffffe0);   // vector count (in bytes)
 6924       jccb(Assembler::zero, COMPARE_TAIL);
 6925 
 6926       lea(ary1, Address(ary1, len, Address::times_1));
 6927       negptr(len);
 6928 
 6929       movl(tmp1, 0x80808080);   // create mask to test for Unicode chars in vector
 6930       movdl(vec2, tmp1);
 6931       vpbroadcastd(vec2, vec2, Assembler::AVX_256bit);
 6932 
 6933       bind(COMPARE_WIDE_VECTORS);
 6934       vmovdqu(vec1, Address(ary1, len, Address::times_1));
 6935       vptest(vec1, vec2);
 6936       jccb(Assembler::notZero, TRUE_LABEL);
 6937       addptr(len, 32);
</pre>
<hr />
<pre>
 7070 
 7071   if (is_array_equ &amp;&amp; is_char) {
 7072     // arrays_equals when used for char[].
 7073     shll(limit, 1);      // byte count != 0
 7074   }
 7075   movl(result, limit); // copy
 7076 
 7077   if (UseAVX &gt;= 2) {
 7078     // With AVX2, use 32-byte vector compare
 7079     Label COMPARE_WIDE_VECTORS, COMPARE_TAIL;
 7080 
 7081     // Compare 32-byte vectors
 7082     andl(result, 0x0000001f);  //   tail count (in bytes)
 7083     andl(limit, 0xffffffe0);   // vector count (in bytes)
 7084     jcc(Assembler::zero, COMPARE_TAIL);
 7085 
 7086     lea(ary1, Address(ary1, limit, Address::times_1));
 7087     lea(ary2, Address(ary2, limit, Address::times_1));
 7088     negptr(limit);
 7089 


 7090 #ifdef _LP64
<span class="line-modified"> 7091     if ((AVX3Threshold == 0) &amp;&amp; VM_Version::supports_avx512vlbw()) { // trying 64 bytes fast loop</span>
 7092       Label COMPARE_WIDE_VECTORS_LOOP_AVX2, COMPARE_WIDE_VECTORS_LOOP_AVX3;
 7093 
 7094       cmpl(limit, -64);
<span class="line-modified"> 7095       jcc(Assembler::greater, COMPARE_WIDE_VECTORS_LOOP_AVX2);</span>
 7096 
 7097       bind(COMPARE_WIDE_VECTORS_LOOP_AVX3); // the hottest loop
 7098 
 7099       evmovdquq(vec1, Address(ary1, limit, Address::times_1), Assembler::AVX_512bit);
 7100       evpcmpeqb(k7, vec1, Address(ary2, limit, Address::times_1), Assembler::AVX_512bit);
 7101       kortestql(k7, k7);
 7102       jcc(Assembler::aboveEqual, FALSE_LABEL);     // miscompare
 7103       addptr(limit, 64);  // update since we already compared at this addr
 7104       cmpl(limit, -64);
 7105       jccb(Assembler::lessEqual, COMPARE_WIDE_VECTORS_LOOP_AVX3);
 7106 
 7107       // At this point we may still need to compare -limit+result bytes.
 7108       // We could execute the next two instruction and just continue via non-wide path:
 7109       //  cmpl(limit, 0);
 7110       //  jcc(Assembler::equal, COMPARE_TAIL);  // true
 7111       // But since we stopped at the points ary{1,2}+limit which are
 7112       // not farther than 64 bytes from the ends of arrays ary{1,2}+result
 7113       // (|limit| &lt;= 32 and result &lt; 32),
 7114       // we may just compare the last 64 bytes.
 7115       //
 7116       addptr(result, -64);   // it is safe, bc we just came from this area
 7117       evmovdquq(vec1, Address(ary1, result, Address::times_1), Assembler::AVX_512bit);
 7118       evpcmpeqb(k7, vec1, Address(ary2, result, Address::times_1), Assembler::AVX_512bit);
 7119       kortestql(k7, k7);
 7120       jcc(Assembler::aboveEqual, FALSE_LABEL);     // miscompare
 7121 
 7122       jmp(TRUE_LABEL);
 7123 
 7124       bind(COMPARE_WIDE_VECTORS_LOOP_AVX2);
 7125 
 7126     }//if (VM_Version::supports_avx512vlbw())
 7127 #endif //_LP64
<span class="line-modified"> 7128     bind(COMPARE_WIDE_VECTORS);</span>
 7129     vmovdqu(vec1, Address(ary1, limit, Address::times_1));
 7130     vmovdqu(vec2, Address(ary2, limit, Address::times_1));
 7131     vpxor(vec1, vec2);
 7132 
 7133     vptest(vec1, vec1);
 7134     jcc(Assembler::notZero, FALSE_LABEL);
 7135     addptr(limit, 32);
 7136     jcc(Assembler::notZero, COMPARE_WIDE_VECTORS);
 7137 
 7138     testl(result, result);
 7139     jcc(Assembler::zero, TRUE_LABEL);
 7140 
 7141     vmovdqu(vec1, Address(ary1, result, Address::times_1, -32));
 7142     vmovdqu(vec2, Address(ary2, result, Address::times_1, -32));
 7143     vpxor(vec1, vec2);
 7144 
 7145     vptest(vec1, vec1);
 7146     jccb(Assembler::notZero, FALSE_LABEL);
 7147     jmpb(TRUE_LABEL);
 7148 
</pre>
<hr />
<pre>
 7334     addptr(to, 8);
 7335     BIND(L_fill_8_bytes);
 7336     subl(count, 1 &lt;&lt; (shift + 1));
 7337     jcc(Assembler::greaterEqual, L_fill_8_bytes_loop);
 7338     // fall through to fill 4 bytes
 7339   } else {
 7340     Label L_fill_32_bytes;
 7341     if (!UseUnalignedLoadStores) {
 7342       // align to 8 bytes, we know we are 4 byte aligned to start
 7343       testptr(to, 4);
 7344       jccb(Assembler::zero, L_fill_32_bytes);
 7345       movl(Address(to, 0), value);
 7346       addptr(to, 4);
 7347       subl(count, 1&lt;&lt;shift);
 7348     }
 7349     BIND(L_fill_32_bytes);
 7350     {
 7351       assert( UseSSE &gt;= 2, &quot;supported cpu only&quot; );
 7352       Label L_fill_32_bytes_loop, L_check_fill_8_bytes, L_fill_8_bytes_loop, L_fill_8_bytes;
 7353       movdl(xtmp, value);
<span class="line-modified"> 7354       if (UseAVX &gt;= 2 &amp;&amp; UseUnalignedLoadStores) {</span>
<span class="line-modified"> 7355         Label L_check_fill_32_bytes;</span>
<span class="line-modified"> 7356         if (UseAVX &gt; 2) {</span>
<span class="line-modified"> 7357           // Fill 64-byte chunks</span>
<span class="line-modified"> 7358           Label L_fill_64_bytes_loop_avx3, L_check_fill_64_bytes_avx2;</span>
<span class="line-modified"> 7359 </span>
<span class="line-modified"> 7360           // If number of bytes to fill &lt; AVX3Threshold, perform fill using AVX2</span>
<span class="line-modified"> 7361           cmpl(count, AVX3Threshold);</span>
<span class="line-modified"> 7362           jccb(Assembler::below, L_check_fill_64_bytes_avx2);</span>
<span class="line-modified"> 7363 </span>
<span class="line-modified"> 7364           vpbroadcastd(xtmp, xtmp, Assembler::AVX_512bit);</span>
<span class="line-modified"> 7365 </span>
<span class="line-modified"> 7366           subl(count, 16 &lt;&lt; shift);</span>
<span class="line-modified"> 7367           jccb(Assembler::less, L_check_fill_32_bytes);</span>
<span class="line-modified"> 7368           align(16);</span>
<span class="line-modified"> 7369 </span>
<span class="line-modified"> 7370           BIND(L_fill_64_bytes_loop_avx3);</span>
<span class="line-modified"> 7371           evmovdqul(Address(to, 0), xtmp, Assembler::AVX_512bit);</span>
<span class="line-modified"> 7372           addptr(to, 64);</span>
<span class="line-modified"> 7373           subl(count, 16 &lt;&lt; shift);</span>
<span class="line-modified"> 7374           jcc(Assembler::greaterEqual, L_fill_64_bytes_loop_avx3);</span>
<span class="line-modified"> 7375           jmpb(L_check_fill_32_bytes);</span>
<span class="line-modified"> 7376 </span>
<span class="line-modified"> 7377           BIND(L_check_fill_64_bytes_avx2);</span>
<span class="line-added"> 7378         }</span>
 7379         // Fill 64-byte chunks
<span class="line-modified"> 7380         Label L_fill_64_bytes_loop;</span>
 7381         vpbroadcastd(xtmp, xtmp, Assembler::AVX_256bit);
 7382 
 7383         subl(count, 16 &lt;&lt; shift);
 7384         jcc(Assembler::less, L_check_fill_32_bytes);
 7385         align(16);
 7386 
 7387         BIND(L_fill_64_bytes_loop);
 7388         vmovdqu(Address(to, 0), xtmp);
 7389         vmovdqu(Address(to, 32), xtmp);
 7390         addptr(to, 64);
 7391         subl(count, 16 &lt;&lt; shift);
 7392         jcc(Assembler::greaterEqual, L_fill_64_bytes_loop);
 7393 
 7394         BIND(L_check_fill_32_bytes);
 7395         addl(count, 8 &lt;&lt; shift);
 7396         jccb(Assembler::less, L_check_fill_8_bytes);
 7397         vmovdqu(Address(to, 0), xtmp);
 7398         addptr(to, 32);
 7399         subl(count, 8 &lt;&lt; shift);
 7400 
</pre>
<hr />
<pre>
 8074   pop(tmp3);
 8075   pop(tmp2);
 8076   pop(tmp1);
 8077 }
 8078 
 8079 void MacroAssembler::vectorized_mismatch(Register obja, Register objb, Register length, Register log2_array_indxscale,
 8080   Register result, Register tmp1, Register tmp2, XMMRegister rymm0, XMMRegister rymm1, XMMRegister rymm2){
 8081   assert(UseSSE42Intrinsics, &quot;SSE4.2 must be enabled.&quot;);
 8082   Label VECTOR16_LOOP, VECTOR8_LOOP, VECTOR4_LOOP;
 8083   Label VECTOR8_TAIL, VECTOR4_TAIL;
 8084   Label VECTOR32_NOT_EQUAL, VECTOR16_NOT_EQUAL, VECTOR8_NOT_EQUAL, VECTOR4_NOT_EQUAL;
 8085   Label SAME_TILL_END, DONE;
 8086   Label BYTES_LOOP, BYTES_TAIL, BYTES_NOT_EQUAL;
 8087 
 8088   //scale is in rcx in both Win64 and Unix
 8089   ShortBranchVerifier sbv(this);
 8090 
 8091   shlq(length);
 8092   xorq(result, result);
 8093 
<span class="line-modified"> 8094   if ((AVX3Threshold == 0) &amp;&amp; (UseAVX &gt; 2) &amp;&amp;</span>
 8095       VM_Version::supports_avx512vlbw()) {
 8096     Label VECTOR64_LOOP, VECTOR64_NOT_EQUAL, VECTOR32_TAIL;
 8097 
 8098     cmpq(length, 64);
 8099     jcc(Assembler::less, VECTOR32_TAIL);
<span class="line-added"> 8100 </span>
 8101     movq(tmp1, length);
 8102     andq(tmp1, 0x3F);      // tail count
 8103     andq(length, ~(0x3F)); //vector count
 8104 
 8105     bind(VECTOR64_LOOP);
 8106     // AVX512 code to compare 64 byte vectors.
 8107     evmovdqub(rymm0, Address(obja, result), Assembler::AVX_512bit);
 8108     evpcmpeqb(k7, rymm0, Address(objb, result), Assembler::AVX_512bit);
 8109     kortestql(k7, k7);
 8110     jcc(Assembler::aboveEqual, VECTOR64_NOT_EQUAL);     // mismatch
 8111     addq(result, 64);
 8112     subq(length, 64);
 8113     jccb(Assembler::notZero, VECTOR64_LOOP);
 8114 
 8115     //bind(VECTOR64_TAIL);
 8116     testq(tmp1, tmp1);
 8117     jcc(Assembler::zero, SAME_TILL_END);
 8118 
 8119     //bind(VECTOR64_TAIL);
 8120     // AVX512 code to compare upto 63 byte vectors.
</pre>
<hr />
<pre>
 8905   // Align buffer to 16 bytes
 8906   movl(tmp, buf);
 8907   andl(tmp, 0xF);
 8908   jccb(Assembler::zero, L_aligned);
 8909   subl(tmp,  16);
 8910   addl(len, tmp);
 8911 
 8912   align(4);
 8913   BIND(L_align_loop);
 8914   movsbl(rax, Address(buf, 0)); // load byte with sign extension
 8915   update_byte_crc32(crc, rax, table);
 8916   increment(buf);
 8917   incrementl(tmp);
 8918   jccb(Assembler::less, L_align_loop);
 8919 
 8920   BIND(L_aligned);
 8921   movl(tmp, len); // save
 8922   shrl(len, 4);
 8923   jcc(Assembler::zero, L_tail_restore);
 8924 




























 8925   // Fold crc into first bytes of vector
 8926   movdqa(xmm1, Address(buf, 0));
 8927   movdl(rax, xmm1);
 8928   xorl(crc, rax);
 8929   if (VM_Version::supports_sse4_1()) {
 8930     pinsrd(xmm1, crc, 0);
 8931   } else {
 8932     pinsrw(xmm1, crc, 0);
 8933     shrl(crc, 16);
 8934     pinsrw(xmm1, crc, 1);
 8935   }
 8936   addptr(buf, 16);
 8937   subl(len, 4); // len &gt; 0
 8938   jcc(Assembler::less, L_fold_tail);
 8939 
 8940   movdqa(xmm2, Address(buf,  0));
 8941   movdqa(xmm3, Address(buf, 16));
 8942   movdqa(xmm4, Address(buf, 32));
 8943   addptr(buf, 48);
 8944   subl(len, 3);
</pre>
<hr />
<pre>
 9509   XMMRegister tmp1Reg, XMMRegister tmp2Reg,
 9510   XMMRegister tmp3Reg, XMMRegister tmp4Reg,
 9511   Register tmp5, Register result) {
 9512   Label copy_chars_loop, return_length, return_zero, done;
 9513 
 9514   // rsi: src
 9515   // rdi: dst
 9516   // rdx: len
 9517   // rcx: tmp5
 9518   // rax: result
 9519 
 9520   // rsi holds start addr of source char[] to be compressed
 9521   // rdi holds start addr of destination byte[]
 9522   // rdx holds length
 9523 
 9524   assert(len != result, &quot;&quot;);
 9525 
 9526   // save length for return
 9527   push(len);
 9528 
<span class="line-modified"> 9529   if ((AVX3Threshold == 0) &amp;&amp; (UseAVX &gt; 2) &amp;&amp; // AVX512</span>
 9530     VM_Version::supports_avx512vlbw() &amp;&amp;
 9531     VM_Version::supports_bmi2()) {
 9532 
 9533     Label copy_32_loop, copy_loop_tail, below_threshold;
 9534 
 9535     // alignment
 9536     Label post_alignment;
 9537 
 9538     // if length of the string is less than 16, handle it in an old fashioned way
 9539     testl(len, -32);
 9540     jcc(Assembler::zero, below_threshold);
 9541 
 9542     // First check whether a character is compressable ( &lt;= 0xFF).
 9543     // Create mask to test for Unicode chars inside zmm vector
 9544     movl(result, 0x00FF);
 9545     evpbroadcastw(tmp2Reg, result, Assembler::AVX_512bit);
 9546 
 9547     testl(len, -64);
 9548     jcc(Assembler::zero, post_alignment);
 9549 
</pre>
<hr />
<pre>
 9701   jmpb(done);
 9702 
 9703   // if compression failed, return 0
 9704   bind(return_zero);
 9705   xorl(result, result);
 9706   addptr(rsp, wordSize);
 9707 
 9708   bind(done);
 9709 }
 9710 
 9711 // Inflate byte[] array to char[].
 9712 //   ..\jdk\src\java.base\share\classes\java\lang\StringLatin1.java
 9713 //   @HotSpotIntrinsicCandidate
 9714 //   private static void inflate(byte[] src, int srcOff, char[] dst, int dstOff, int len) {
 9715 //     for (int i = 0; i &lt; len; i++) {
 9716 //       dst[dstOff++] = (char)(src[srcOff++] &amp; 0xff);
 9717 //     }
 9718 //   }
 9719 void MacroAssembler::byte_array_inflate(Register src, Register dst, Register len,
 9720   XMMRegister tmp1, Register tmp2) {
<span class="line-modified"> 9721   Label copy_chars_loop, done, below_threshold, avx3_threshold;</span>
 9722   // rsi: src
 9723   // rdi: dst
 9724   // rdx: len
 9725   // rcx: tmp2
 9726 
 9727   // rsi holds start addr of source byte[] to be inflated
 9728   // rdi holds start addr of destination char[]
 9729   // rdx holds length
 9730   assert_different_registers(src, dst, len, tmp2);
<span class="line-modified"> 9731   movl(tmp2, len);</span>
 9732   if ((UseAVX &gt; 2) &amp;&amp; // AVX512
 9733     VM_Version::supports_avx512vlbw() &amp;&amp;
 9734     VM_Version::supports_bmi2()) {
 9735 
 9736     Label copy_32_loop, copy_tail;
 9737     Register tmp3_aliased = len;
 9738 
 9739     // if length of the string is less than 16, handle it in an old fashioned way
 9740     testl(len, -16);
 9741     jcc(Assembler::zero, below_threshold);
 9742 
<span class="line-added"> 9743     testl(len, -1 * AVX3Threshold);</span>
<span class="line-added"> 9744     jcc(Assembler::zero, avx3_threshold);</span>
<span class="line-added"> 9745 </span>
 9746     // In order to use only one arithmetic operation for the main loop we use
 9747     // this pre-calculation

 9748     andl(tmp2, (32 - 1)); // tail count (in chars), 32 element wide loop
 9749     andl(len, -32);     // vector count
 9750     jccb(Assembler::zero, copy_tail);
 9751 
 9752     lea(src, Address(src, len, Address::times_1));
 9753     lea(dst, Address(dst, len, Address::times_2));
 9754     negptr(len);
 9755 
 9756 
 9757     // inflate 32 chars per iter
 9758     bind(copy_32_loop);
 9759     vpmovzxbw(tmp1, Address(src, len, Address::times_1), Assembler::AVX_512bit);
 9760     evmovdquw(Address(dst, len, Address::times_2), tmp1, Assembler::AVX_512bit);
 9761     addptr(len, 32);
 9762     jcc(Assembler::notZero, copy_32_loop);
 9763 
 9764     bind(copy_tail);
 9765     // bail out when there is nothing to be done
 9766     testl(tmp2, -1); // we don&#39;t destroy the contents of tmp2 here
 9767     jcc(Assembler::zero, done);
 9768 
 9769     // ~(~0 &lt;&lt; length), where length is the # of remaining elements to process
 9770     movl(tmp3_aliased, -1);
 9771     shlxl(tmp3_aliased, tmp3_aliased, tmp2);
 9772     notl(tmp3_aliased);
 9773     kmovdl(k2, tmp3_aliased);
 9774     evpmovzxbw(tmp1, k2, Address(src, 0), Assembler::AVX_512bit);
 9775     evmovdquw(Address(dst, 0), k2, tmp1, Assembler::AVX_512bit);
 9776 
 9777     jmp(done);
<span class="line-added"> 9778     bind(avx3_threshold);</span>
 9779   }
 9780   if (UseSSE42Intrinsics) {
 9781     Label copy_16_loop, copy_8_loop, copy_bytes, copy_new_tail, copy_tail;
 9782 


 9783     if (UseAVX &gt; 1) {
 9784       andl(tmp2, (16 - 1));
 9785       andl(len, -16);
 9786       jccb(Assembler::zero, copy_new_tail);
 9787     } else {
 9788       andl(tmp2, 0x00000007);   // tail count (in chars)
 9789       andl(len, 0xfffffff8);    // vector count (in chars)
 9790       jccb(Assembler::zero, copy_tail);
 9791     }
 9792 
 9793     // vectored inflation
 9794     lea(src, Address(src, len, Address::times_1));
 9795     lea(dst, Address(dst, len, Address::times_2));
 9796     negptr(len);
 9797 
 9798     if (UseAVX &gt; 1) {
 9799       bind(copy_16_loop);
 9800       vpmovzxbw(tmp1, Address(src, len, Address::times_1), Assembler::AVX_256bit);
 9801       vmovdqu(Address(dst, len, Address::times_2), tmp1);
 9802       addptr(len, 16);
 9803       jcc(Assembler::notZero, copy_16_loop);
 9804 
 9805       bind(below_threshold);
 9806       bind(copy_new_tail);
<span class="line-modified"> 9807       movl(len, tmp2);</span>






 9808       andl(tmp2, 0x00000007);
 9809       andl(len, 0xFFFFFFF8);
 9810       jccb(Assembler::zero, copy_tail);
 9811 
 9812       pmovzxbw(tmp1, Address(src, 0));
 9813       movdqu(Address(dst, 0), tmp1);
 9814       addptr(src, 8);
 9815       addptr(dst, 2 * 8);
 9816 
 9817       jmp(copy_tail, true);
 9818     }
 9819 
 9820     // inflate 8 chars per iter
 9821     bind(copy_8_loop);
 9822     pmovzxbw(tmp1, Address(src, len, Address::times_1));  // unpack to 8 words
 9823     movdqu(Address(dst, len, Address::times_2), tmp1);
 9824     addptr(len, 8);
 9825     jcc(Assembler::notZero, copy_8_loop);
 9826 
 9827     bind(copy_tail);
</pre>
<hr />
<pre>
 9841   } else {
 9842     bind(below_threshold);
 9843   }
 9844 
 9845   testl(len, len);
 9846   jccb(Assembler::zero, done);
 9847   lea(src, Address(src, len, Address::times_1));
 9848   lea(dst, Address(dst, len, Address::times_2));
 9849   negptr(len);
 9850 
 9851   // inflate 1 char per iter
 9852   bind(copy_chars_loop);
 9853   load_unsigned_byte(tmp2, Address(src, len, Address::times_1));  // load byte char
 9854   movw(Address(dst, len, Address::times_2), tmp2);  // inflate byte char to word
 9855   increment(len);
 9856   jcc(Assembler::notZero, copy_chars_loop);
 9857 
 9858   bind(done);
 9859 }
 9860 
<span class="line-added"> 9861 #ifdef _LP64</span>
<span class="line-added"> 9862 void MacroAssembler::convert_f2i(Register dst, XMMRegister src) {</span>
<span class="line-added"> 9863   Label done;</span>
<span class="line-added"> 9864   cvttss2sil(dst, src);</span>
<span class="line-added"> 9865   // Conversion instructions do not match JLS for overflow, underflow and NaN -&gt; fixup in stub</span>
<span class="line-added"> 9866   cmpl(dst, 0x80000000); // float_sign_flip</span>
<span class="line-added"> 9867   jccb(Assembler::notEqual, done);</span>
<span class="line-added"> 9868   subptr(rsp, 8);</span>
<span class="line-added"> 9869   movflt(Address(rsp, 0), src);</span>
<span class="line-added"> 9870   call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::f2i_fixup())));</span>
<span class="line-added"> 9871   pop(dst);</span>
<span class="line-added"> 9872   bind(done);</span>
<span class="line-added"> 9873 }</span>
<span class="line-added"> 9874 </span>
<span class="line-added"> 9875 void MacroAssembler::convert_d2i(Register dst, XMMRegister src) {</span>
<span class="line-added"> 9876   Label done;</span>
<span class="line-added"> 9877   cvttsd2sil(dst, src);</span>
<span class="line-added"> 9878   // Conversion instructions do not match JLS for overflow, underflow and NaN -&gt; fixup in stub</span>
<span class="line-added"> 9879   cmpl(dst, 0x80000000); // float_sign_flip</span>
<span class="line-added"> 9880   jccb(Assembler::notEqual, done);</span>
<span class="line-added"> 9881   subptr(rsp, 8);</span>
<span class="line-added"> 9882   movdbl(Address(rsp, 0), src);</span>
<span class="line-added"> 9883   call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::d2i_fixup())));</span>
<span class="line-added"> 9884   pop(dst);</span>
<span class="line-added"> 9885   bind(done);</span>
<span class="line-added"> 9886 }</span>
<span class="line-added"> 9887 </span>
<span class="line-added"> 9888 void MacroAssembler::convert_f2l(Register dst, XMMRegister src) {</span>
<span class="line-added"> 9889   Label done;</span>
<span class="line-added"> 9890   cvttss2siq(dst, src);</span>
<span class="line-added"> 9891   cmp64(dst, ExternalAddress((address) StubRoutines::x86::double_sign_flip()));</span>
<span class="line-added"> 9892   jccb(Assembler::notEqual, done);</span>
<span class="line-added"> 9893   subptr(rsp, 8);</span>
<span class="line-added"> 9894   movflt(Address(rsp, 0), src);</span>
<span class="line-added"> 9895   call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::f2l_fixup())));</span>
<span class="line-added"> 9896   pop(dst);</span>
<span class="line-added"> 9897   bind(done);</span>
<span class="line-added"> 9898 }</span>
<span class="line-added"> 9899 </span>
<span class="line-added"> 9900 void MacroAssembler::convert_d2l(Register dst, XMMRegister src) {</span>
<span class="line-added"> 9901   Label done;</span>
<span class="line-added"> 9902   cvttsd2siq(dst, src);</span>
<span class="line-added"> 9903   cmp64(dst, ExternalAddress((address) StubRoutines::x86::double_sign_flip()));</span>
<span class="line-added"> 9904   jccb(Assembler::notEqual, done);</span>
<span class="line-added"> 9905   subptr(rsp, 8);</span>
<span class="line-added"> 9906   movdbl(Address(rsp, 0), src);</span>
<span class="line-added"> 9907   call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::d2l_fixup())));</span>
<span class="line-added"> 9908   pop(dst);</span>
<span class="line-added"> 9909   bind(done);</span>
<span class="line-added"> 9910 }</span>
<span class="line-added"> 9911 </span>
<span class="line-added"> 9912 void MacroAssembler::cache_wb(Address line)</span>
<span class="line-added"> 9913 {</span>
<span class="line-added"> 9914   // 64 bit cpus always support clflush</span>
<span class="line-added"> 9915   assert(VM_Version::supports_clflush(), &quot;clflush should be available&quot;);</span>
<span class="line-added"> 9916   bool optimized = VM_Version::supports_clflushopt();</span>
<span class="line-added"> 9917   bool no_evict = VM_Version::supports_clwb();</span>
<span class="line-added"> 9918 </span>
<span class="line-added"> 9919   // prefer clwb (writeback without evict) otherwise</span>
<span class="line-added"> 9920   // prefer clflushopt (potentially parallel writeback with evict)</span>
<span class="line-added"> 9921   // otherwise fallback on clflush (serial writeback with evict)</span>
<span class="line-added"> 9922 </span>
<span class="line-added"> 9923   if (optimized) {</span>
<span class="line-added"> 9924     if (no_evict) {</span>
<span class="line-added"> 9925       clwb(line);</span>
<span class="line-added"> 9926     } else {</span>
<span class="line-added"> 9927       clflushopt(line);</span>
<span class="line-added"> 9928     }</span>
<span class="line-added"> 9929   } else {</span>
<span class="line-added"> 9930     // no need for fence when using CLFLUSH</span>
<span class="line-added"> 9931     clflush(line);</span>
<span class="line-added"> 9932   }</span>
<span class="line-added"> 9933 }</span>
<span class="line-added"> 9934 </span>
<span class="line-added"> 9935 void MacroAssembler::cache_wbsync(bool is_pre)</span>
<span class="line-added"> 9936 {</span>
<span class="line-added"> 9937   assert(VM_Version::supports_clflush(), &quot;clflush should be available&quot;);</span>
<span class="line-added"> 9938   bool optimized = VM_Version::supports_clflushopt();</span>
<span class="line-added"> 9939   bool no_evict = VM_Version::supports_clwb();</span>
<span class="line-added"> 9940 </span>
<span class="line-added"> 9941   // pick the correct implementation</span>
<span class="line-added"> 9942 </span>
<span class="line-added"> 9943   if (!is_pre &amp;&amp; (optimized || no_evict)) {</span>
<span class="line-added"> 9944     // need an sfence for post flush when using clflushopt or clwb</span>
<span class="line-added"> 9945     // otherwise no no need for any synchroniaztion</span>
<span class="line-added"> 9946 </span>
<span class="line-added"> 9947     sfence();</span>
<span class="line-added"> 9948   }</span>
<span class="line-added"> 9949 }</span>
<span class="line-added"> 9950 #endif // _LP64</span>
<span class="line-added"> 9951 </span>
 9952 Assembler::Condition MacroAssembler::negate_condition(Assembler::Condition cond) {
 9953   switch (cond) {
 9954     // Note some conditions are synonyms for others
 9955     case Assembler::zero:         return Assembler::notZero;
 9956     case Assembler::notZero:      return Assembler::zero;
 9957     case Assembler::less:         return Assembler::greaterEqual;
 9958     case Assembler::lessEqual:    return Assembler::greater;
 9959     case Assembler::greater:      return Assembler::lessEqual;
 9960     case Assembler::greaterEqual: return Assembler::less;
 9961     case Assembler::below:        return Assembler::aboveEqual;
 9962     case Assembler::belowEqual:   return Assembler::above;
 9963     case Assembler::above:        return Assembler::belowEqual;
 9964     case Assembler::aboveEqual:   return Assembler::below;
 9965     case Assembler::overflow:     return Assembler::noOverflow;
 9966     case Assembler::noOverflow:   return Assembler::overflow;
 9967     case Assembler::negative:     return Assembler::positive;
 9968     case Assembler::positive:     return Assembler::negative;
 9969     case Assembler::parity:       return Assembler::noParity;
 9970     case Assembler::noParity:     return Assembler::parity;
 9971   }
</pre>
<hr />
<pre>
10004 #endif
10005 
10006   MacroAssembler::call_VM_leaf_base(CAST_FROM_FN_PTR(address, Thread::current), 0);
10007 
10008 #ifdef _LP64
10009   pop(r11);
10010   pop(r10);
10011   pop(r9);
10012   pop(r8);
10013 #endif
10014   pop(rcx);
10015   pop(rdx);
10016   LP64_ONLY(pop(rsi);)
10017   LP64_ONLY(pop(rdi);)
10018   if (thread != rax) {
10019     mov(thread, rax);
10020     pop(rax);
10021   }
10022 }
10023 
<span class="line-modified">10024 #endif // !WIN32 || _LP64</span>
</pre>
</td>
</tr>
</table>
<center><a href="jvmciCodeInstaller_x86.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_x86.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>