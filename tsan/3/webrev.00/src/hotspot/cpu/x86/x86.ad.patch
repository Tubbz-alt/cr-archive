diff a/src/hotspot/cpu/x86/x86.ad b/src/hotspot/cpu/x86/x86.ad
--- a/src/hotspot/cpu/x86/x86.ad
+++ b/src/hotspot/cpu/x86/x86.ad
@@ -1092,145 +1092,13 @@
                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p
 #endif
                       );
 
-reg_class_dynamic vectorz_reg(vectorz_reg_evex, vectorz_reg_legacy, %{ VM_Version::supports_evex() %} );
+reg_class_dynamic vectorz_reg   (vectorz_reg_evex, vectorz_reg_legacy, %{ VM_Version::supports_evex() %} );
 reg_class_dynamic vectorz_reg_vl(vectorz_reg_evex, vectorz_reg_legacy, %{ VM_Version::supports_evex() && VM_Version::supports_avx512vl() %} );
 
-reg_class xmm0_reg(XMM0, XMM0b, XMM0c, XMM0d);
-reg_class ymm0_reg(XMM0, XMM0b, XMM0c, XMM0d, XMM0e, XMM0f, XMM0g, XMM0h);
-reg_class zmm0_reg(XMM0, XMM0b, XMM0c, XMM0d, XMM0e, XMM0f, XMM0g, XMM0h, XMM0i, XMM0j, XMM0k, XMM0l, XMM0m, XMM0n, XMM0o, XMM0p);
-
-reg_class xmm1_reg(XMM1, XMM1b, XMM1c, XMM1d);
-reg_class ymm1_reg(XMM1, XMM1b, XMM1c, XMM1d, XMM1e, XMM1f, XMM1g, XMM1h);
-reg_class zmm1_reg(XMM1, XMM1b, XMM1c, XMM1d, XMM1e, XMM1f, XMM1g, XMM1h, XMM1i, XMM1j, XMM1k, XMM1l, XMM1m, XMM1n, XMM1o, XMM1p);
-
-reg_class xmm2_reg(XMM2, XMM2b, XMM2c, XMM2d);
-reg_class ymm2_reg(XMM2, XMM2b, XMM2c, XMM2d, XMM2e, XMM2f, XMM2g, XMM2h);
-reg_class zmm2_reg(XMM2, XMM2b, XMM2c, XMM2d, XMM2e, XMM2f, XMM2g, XMM2h, XMM2i, XMM2j, XMM2k, XMM2l, XMM2m, XMM2n, XMM2o, XMM2p);
-
-reg_class xmm3_reg(XMM3, XMM3b, XMM3c, XMM3d);
-reg_class ymm3_reg(XMM3, XMM3b, XMM3c, XMM3d, XMM3e, XMM3f, XMM3g, XMM3h);
-reg_class zmm3_reg(XMM3, XMM3b, XMM3c, XMM3d, XMM3e, XMM3f, XMM3g, XMM3h, XMM3i, XMM3j, XMM3k, XMM3l, XMM3m, XMM3n, XMM3o, XMM3p);
-
-reg_class xmm4_reg(XMM4, XMM4b, XMM4c, XMM4d);
-reg_class ymm4_reg(XMM4, XMM4b, XMM4c, XMM4d, XMM4e, XMM4f, XMM4g, XMM4h);
-reg_class zmm4_reg(XMM4, XMM4b, XMM4c, XMM4d, XMM4e, XMM4f, XMM4g, XMM4h, XMM4i, XMM4j, XMM4k, XMM4l, XMM4m, XMM4n, XMM4o, XMM4p);
-
-reg_class xmm5_reg(XMM5, XMM5b, XMM5c, XMM5d);
-reg_class ymm5_reg(XMM5, XMM5b, XMM5c, XMM5d, XMM5e, XMM5f, XMM5g, XMM5h);
-reg_class zmm5_reg(XMM5, XMM5b, XMM5c, XMM5d, XMM5e, XMM5f, XMM5g, XMM5h, XMM5i, XMM5j, XMM5k, XMM5l, XMM5m, XMM5n, XMM5o, XMM5p);
-
-reg_class xmm6_reg(XMM6, XMM6b, XMM6c, XMM6d);
-reg_class ymm6_reg(XMM6, XMM6b, XMM6c, XMM6d, XMM6e, XMM6f, XMM6g, XMM6h);
-reg_class zmm6_reg(XMM6, XMM6b, XMM6c, XMM6d, XMM6e, XMM6f, XMM6g, XMM6h, XMM6i, XMM6j, XMM6k, XMM6l, XMM6m, XMM6n, XMM6o, XMM6p);
-
-reg_class xmm7_reg(XMM7, XMM7b, XMM7c, XMM7d);
-reg_class ymm7_reg(XMM7, XMM7b, XMM7c, XMM7d, XMM7e, XMM7f, XMM7g, XMM7h);
-reg_class zmm7_reg(XMM7, XMM7b, XMM7c, XMM7d, XMM7e, XMM7f, XMM7g, XMM7h, XMM7i, XMM7j, XMM7k, XMM7l, XMM7m, XMM7n, XMM7o, XMM7p);
-
-#ifdef _LP64
-
-reg_class xmm8_reg(XMM8, XMM8b, XMM8c, XMM8d);
-reg_class ymm8_reg(XMM8, XMM8b, XMM8c, XMM8d, XMM8e, XMM8f, XMM8g, XMM8h);
-reg_class zmm8_reg(XMM8, XMM8b, XMM8c, XMM8d, XMM8e, XMM8f, XMM8g, XMM8h, XMM8i, XMM8j, XMM8k, XMM8l, XMM8m, XMM8n, XMM8o, XMM8p);
-
-reg_class xmm9_reg(XMM9, XMM9b, XMM9c, XMM9d);
-reg_class ymm9_reg(XMM9, XMM9b, XMM9c, XMM9d, XMM9e, XMM9f, XMM9g, XMM9h);
-reg_class zmm9_reg(XMM9, XMM9b, XMM9c, XMM9d, XMM9e, XMM9f, XMM9g, XMM9h, XMM9i, XMM9j, XMM9k, XMM9l, XMM9m, XMM9n, XMM9o, XMM9p);
-
-reg_class xmm10_reg(XMM10, XMM10b, XMM10c, XMM10d);
-reg_class ymm10_reg(XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h);
-reg_class zmm10_reg(XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p);
-
-reg_class xmm11_reg(XMM11, XMM11b, XMM11c, XMM11d);
-reg_class ymm11_reg(XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h);
-reg_class zmm11_reg(XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p);
-
-reg_class xmm12_reg(XMM12, XMM12b, XMM12c, XMM12d);
-reg_class ymm12_reg(XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h);
-reg_class zmm12_reg(XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p);
-
-reg_class xmm13_reg(XMM13, XMM13b, XMM13c, XMM13d);
-reg_class ymm13_reg(XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h);
-reg_class zmm13_reg(XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p);
-
-reg_class xmm14_reg(XMM14, XMM14b, XMM14c, XMM14d);
-reg_class ymm14_reg(XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h);
-reg_class zmm14_reg(XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p);
-
-reg_class xmm15_reg(XMM15, XMM15b, XMM15c, XMM15d);
-reg_class ymm15_reg(XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h);
-reg_class zmm15_reg(XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p);
-
-reg_class xmm16_reg(XMM16, XMM16b, XMM16c, XMM16d);
-reg_class ymm16_reg(XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h);
-reg_class zmm16_reg(XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p);
-
-reg_class xmm17_reg(XMM17, XMM17b, XMM17c, XMM17d);
-reg_class ymm17_reg(XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h);
-reg_class zmm17_reg(XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h, XMM17i, XMM17j, XMM17k, XMM17l, XMM17m, XMM17n, XMM17o, XMM17p);
-
-reg_class xmm18_reg(XMM18, XMM18b, XMM18c, XMM18d);
-reg_class ymm18_reg(XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h);
-reg_class zmm18_reg(XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h, XMM18i, XMM18j, XMM18k, XMM18l, XMM18m, XMM18n, XMM18o, XMM18p);
-
-reg_class xmm19_reg(XMM19, XMM19b, XMM19c, XMM19d);
-reg_class ymm19_reg(XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h);
-reg_class zmm19_reg(XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h, XMM19i, XMM19j, XMM19k, XMM19l, XMM19m, XMM19n, XMM19o, XMM19p);
-
-reg_class xmm20_reg(XMM20, XMM20b, XMM20c, XMM20d);
-reg_class ymm20_reg(XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h);
-reg_class zmm20_reg(XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h, XMM20i, XMM20j, XMM20k, XMM20l, XMM20m, XMM20n, XMM20o, XMM20p);
-
-reg_class xmm21_reg(XMM21, XMM21b, XMM21c, XMM21d);
-reg_class ymm21_reg(XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h);
-reg_class zmm21_reg(XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h, XMM21i, XMM21j, XMM21k, XMM21l, XMM21m, XMM21n, XMM21o, XMM21p);
-
-reg_class xmm22_reg(XMM22, XMM22b, XMM22c, XMM22d);
-reg_class ymm22_reg(XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h);
-reg_class zmm22_reg(XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h, XMM22i, XMM22j, XMM22k, XMM22l, XMM22m, XMM22n, XMM22o, XMM22p);
-
-reg_class xmm23_reg(XMM23, XMM23b, XMM23c, XMM23d);
-reg_class ymm23_reg(XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h);
-reg_class zmm23_reg(XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h, XMM23i, XMM23j, XMM23k, XMM23l, XMM23m, XMM23n, XMM23o, XMM23p);
-
-reg_class xmm24_reg(XMM24, XMM24b, XMM24c, XMM24d);
-reg_class ymm24_reg(XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h);
-reg_class zmm24_reg(XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h, XMM24i, XMM24j, XMM24k, XMM24l, XMM24m, XMM24n, XMM24o, XMM24p);
-
-reg_class xmm25_reg(XMM25, XMM25b, XMM25c, XMM25d);
-reg_class ymm25_reg(XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h);
-reg_class zmm25_reg(XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h, XMM25i, XMM25j, XMM25k, XMM25l, XMM25m, XMM25n, XMM25o, XMM25p);
-
-reg_class xmm26_reg(XMM26, XMM26b, XMM26c, XMM26d);
-reg_class ymm26_reg(XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h);
-reg_class zmm26_reg(XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h, XMM26i, XMM26j, XMM26k, XMM26l, XMM26m, XMM26n, XMM26o, XMM26p);
-
-reg_class xmm27_reg(XMM27, XMM27b, XMM27c, XMM27d);
-reg_class ymm27_reg(XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h);
-reg_class zmm27_reg(XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h, XMM27i, XMM27j, XMM27k, XMM27l, XMM27m, XMM27n, XMM27o, XMM27p);
-
-reg_class xmm28_reg(XMM28, XMM28b, XMM28c, XMM28d);
-reg_class ymm28_reg(XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h);
-reg_class zmm28_reg(XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h, XMM28i, XMM28j, XMM28k, XMM28l, XMM28m, XMM28n, XMM28o, XMM28p);
-
-reg_class xmm29_reg(XMM29, XMM29b, XMM29c, XMM29d);
-reg_class ymm29_reg(XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h);
-reg_class zmm29_reg(XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h, XMM29i, XMM29j, XMM29k, XMM29l, XMM29m, XMM29n, XMM29o, XMM29p);
-
-reg_class xmm30_reg(XMM30, XMM30b, XMM30c, XMM30d);
-reg_class ymm30_reg(XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h);
-reg_class zmm30_reg(XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h, XMM30i, XMM30j, XMM30k, XMM30l, XMM30m, XMM30n, XMM30o, XMM30p);
-
-reg_class xmm31_reg(XMM31, XMM31b, XMM31c, XMM31d);
-reg_class ymm31_reg(XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h);
-reg_class zmm31_reg(XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p);
-
-#endif
-
 %}
 
 
 //----------SOURCE BLOCK-------------------------------------------------------
 // This is a block of C++ code which provides values, functions, and
@@ -1370,136 +1238,259 @@
   static address float_signmask()  { return (address)float_signmask_pool; }
   static address float_signflip()  { return (address)float_signflip_pool; }
   static address double_signmask() { return (address)double_signmask_pool; }
   static address double_signflip() { return (address)double_signflip_pool; }
 #endif
+  static address vector_short_to_byte_mask() { return StubRoutines::x86::vector_short_to_byte_mask(); }
+  static address vector_byte_perm_mask() { return StubRoutines::x86::vector_byte_perm_mask(); }
+  static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }
 
-
+//=============================================================================
 const bool Matcher::match_rule_supported(int opcode) {
-  if (!has_match_rule(opcode))
-    return false;
-
-  bool ret_value = true;
+  if (!has_match_rule(opcode)) {
+    return false; // no match rule present
+  }
   switch (opcode) {
+    case Op_AbsVL:
+      if (UseAVX < 3) {
+        return false;
+      }
+      break;
     case Op_PopCountI:
     case Op_PopCountL:
-      if (!UsePopCountInstruction)
-        ret_value = false;
+      if (!UsePopCountInstruction) {
+        return false;
+      }
       break;
     case Op_PopCountVI:
-      if (!UsePopCountInstruction || !VM_Version::supports_vpopcntdq())
-        ret_value = false;
+      if (!UsePopCountInstruction || !VM_Version::supports_vpopcntdq()) {
+        return false;
+      }
       break;
     case Op_MulVI:
-      if ((UseSSE < 4) && (UseAVX < 1)) // only with SSE4_1 or AVX
-        ret_value = false;
+      if ((UseSSE < 4) && (UseAVX < 1)) { // only with SSE4_1 or AVX
+        return false;
+      }
       break;
     case Op_MulVL:
     case Op_MulReductionVL:
-      if (VM_Version::supports_avx512dq() == false)
-        ret_value = false;
+      if (VM_Version::supports_avx512dq() == false) {
+        return false;
+      }
       break;
     case Op_AddReductionVL:
-      if (UseAVX < 3) // only EVEX : vector connectivity becomes an issue here
-        ret_value = false;
+      if (UseAVX < 3) { // only EVEX : vector connectivity becomes an issue here
+        return false;
+      }
       break;
+    case Op_AbsVB:
+    case Op_AbsVS:
+    case Op_AbsVI:
     case Op_AddReductionVI:
-      if (UseSSE < 3) // requires at least SSE3
-        ret_value = false;
+      if (UseSSE < 3 || !VM_Version::supports_ssse3()) { // requires at least SSSE3
+        return false;
+      }
       break;
     case Op_MulReductionVI:
-      if (UseSSE < 4) // requires at least SSE4
-        ret_value = false;
+      if (UseSSE < 4) { // requires at least SSE4
+        return false;
+      }
       break;
     case Op_AddReductionVF:
     case Op_AddReductionVD:
     case Op_MulReductionVF:
     case Op_MulReductionVD:
-      if (UseSSE < 1) // requires at least SSE
-        ret_value = false;
+      if (UseSSE < 1) { // requires at least SSE
+        return false;
+      }
       break;
     case Op_SqrtVD:
     case Op_SqrtVF:
-      if (UseAVX < 1) // enabled for AVX only
-        ret_value = false;
+      if (UseAVX < 1) { // enabled for AVX only
+        return false;
+      }
       break;
     case Op_CompareAndSwapL:
 #ifdef _LP64
     case Op_CompareAndSwapP:
 #endif
-      if (!VM_Version::supports_cx8())
-        ret_value = false;
+      if (!VM_Version::supports_cx8()) {
+        return false;
+      }
       break;
     case Op_CMoveVF:
     case Op_CMoveVD:
-      if (UseAVX < 1 || UseAVX > 2)
-        ret_value = false;
+      if (UseAVX < 1 || UseAVX > 2) {
+        return false;
+      }
       break;
     case Op_StrIndexOf:
-      if (!UseSSE42Intrinsics)
-        ret_value = false;
+      if (!UseSSE42Intrinsics) {
+        return false;
+      }
       break;
     case Op_StrIndexOfChar:
-      if (!UseSSE42Intrinsics)
-        ret_value = false;
+      if (!UseSSE42Intrinsics) {
+        return false;
+      }
       break;
     case Op_OnSpinWait:
-      if (VM_Version::supports_on_spin_wait() == false)
-        ret_value = false;
+      if (VM_Version::supports_on_spin_wait() == false) {
+        return false;
+      }
       break;
     case Op_MulAddVS2VI:
-      if (UseSSE < 2)
-        ret_value = false;
+    case Op_RShiftVL:
+    case Op_AbsVD:
+    case Op_NegVD:
+      if (UseSSE < 2) {
+        return false;
+      }
+      break;
+    case Op_MulVB:
+    case Op_LShiftVB:
+    case Op_RShiftVB:
+    case Op_URShiftVB:
+      if (UseSSE < 4) {
+        return false;
+      }
       break;
 #ifdef _LP64
     case Op_MaxD:
     case Op_MaxF:
     case Op_MinD:
     case Op_MinF:
-      if (UseAVX < 1) // enabled for AVX only
-        ret_value = false;
+      if (UseAVX < 1) { // enabled for AVX only
+        return false;
+      }
       break;
 #endif
+    case Op_CacheWB:
+    case Op_CacheWBPreSync:
+    case Op_CacheWBPostSync:
+      if (!VM_Version::supports_data_cache_line_flush()) {
+        return false;
+      }
+      break;
+    case Op_RoundDoubleMode:
+      if (UseSSE < 4) {
+        return false;
+      }
+      break;
+    case Op_RoundDoubleModeV:
+      if (VM_Version::supports_avx() == false) {
+        return false; // 128bit vroundpd is not available
+      }
+      break;
   }
+  return true;  // Match rules are supported by default.
+}
 
-  return ret_value;  // Per default match rules are supported.
+//------------------------------------------------------------------------
+
+// Identify extra cases that we might want to provide match rules for vector nodes and
+// other intrinsics guarded with vector length (vlen) and element type (bt).
+const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {
+  if (!match_rule_supported(opcode)) {
+    return false;
+  }
+  // Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):
+  //   * SSE2 supports 128bit vectors for all types;
+  //   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;
+  //   * AVX2 supports 256bit vectors for all types;
+  //   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;
+  //   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.
+  // There's also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).
+  // And MaxVectorSize is taken into account as well.
+  if (!vector_size_supported(bt, vlen)) {
+    return false;
+  }
+  // Special cases which require vector length follow:
+  //   * implementation limitations
+  //   * some 512bit vector operations on FLOAT and DOUBLE types require AVX512DQ
+  //   * 128bit vroundpd instruction is present only in AVX1
+  switch (opcode) {
+    case Op_AbsVF:
+    case Op_NegVF:
+      if ((vlen == 16) && (VM_Version::supports_avx512dq() == false)) {
+        return false; // 512bit vandps and vxorps are not available
+      }
+      break;
+    case Op_AbsVD:
+    case Op_NegVD:
+      if ((vlen == 8) && (VM_Version::supports_avx512dq() == false)) {
+        return false; // 512bit vandpd and vxorpd are not available
+      }
+      break;
+    case Op_CMoveVF:
+      if (vlen != 8) {
+        return false; // implementation limitation (only vcmov8F_reg is present)
+      }
+      break;
+    case Op_CMoveVD:
+      if (vlen != 4) {
+        return false; // implementation limitation (only vcmov4D_reg is present)
+      }
+      break;
+  }
+  return true;  // Per default match rules are supported.
 }
 
-const bool Matcher::match_rule_supported_vector(int opcode, int vlen) {
-  // identify extra cases that we might want to provide match rules for
-  // e.g. Op_ vector nodes and other intrinsics while guarding with vlen
-  bool ret_value = match_rule_supported(opcode);
-  if (ret_value) {
-    switch (opcode) {
-      case Op_AddVB:
-      case Op_SubVB:
-        if ((vlen == 64) && (VM_Version::supports_avx512bw() == false))
-          ret_value = false;
-        break;
-      case Op_URShiftVS:
-      case Op_RShiftVS:
-      case Op_LShiftVS:
-      case Op_MulVS:
-      case Op_AddVS:
-      case Op_SubVS:
-        if ((vlen == 32) && (VM_Version::supports_avx512bw() == false))
-          ret_value = false;
-        break;
-      case Op_CMoveVF:
-        if (vlen != 8)
-          ret_value  = false;
-        break;
-      case Op_CMoveVD:
-        if (vlen != 4)
-          ret_value  = false;
-        break;
+// x86 supports generic vector operands: vec and legVec.
+const bool Matcher::supports_generic_vector_operands = true;
+
+MachOper* Matcher::specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp) {
+  assert(Matcher::is_generic_vector(generic_opnd), "not generic");
+  bool legacy = (generic_opnd->opcode() == LEGVEC);
+  if (!VM_Version::supports_avx512vlbwdq() && // KNL
+      is_temp && !legacy && (ideal_reg == Op_VecZ)) {
+    // Conservatively specialize 512bit vec TEMP operands to legVecZ (zmm0-15) on KNL.
+    return new legVecZOper();
+  }
+  if (legacy) {
+    switch (ideal_reg) {
+      case Op_VecS: return new legVecSOper();
+      case Op_VecD: return new legVecDOper();
+      case Op_VecX: return new legVecXOper();
+      case Op_VecY: return new legVecYOper();
+      case Op_VecZ: return new legVecZOper();
+    }
+  } else {
+    switch (ideal_reg) {
+      case Op_VecS: return new vecSOper();
+      case Op_VecD: return new vecDOper();
+      case Op_VecX: return new vecXOper();
+      case Op_VecY: return new vecYOper();
+      case Op_VecZ: return new vecZOper();
     }
   }
+  ShouldNotReachHere();
+  return NULL;
+}
+
+bool Matcher::is_generic_reg2reg_move(MachNode* m) {
+  switch (m->rule()) {
+    case MoveVec2Leg_rule:
+    case MoveLeg2Vec_rule:
+      return true;
+    default:
+      return false;
+  }
+}
 
-  return ret_value;  // Per default match rules are supported.
+bool Matcher::is_generic_vector(MachOper* opnd) {
+  switch (opnd->opcode()) {
+    case VEC:
+    case LEGVEC:
+      return true;
+    default:
+      return false;
+  }
 }
 
+//------------------------------------------------------------------------
+
 const bool Matcher::has_predicated_vectors(void) {
   bool ret_value = false;
   if (UseAVX > 2) {
     ret_value = VM_Version::supports_avx512vl();
   }
@@ -1675,10 +1666,41 @@
 }
 
 void Compile::reshape_address(AddPNode* addp) {
 }
 
+static inline uint vector_length(const MachNode* n) {
+  const TypeVect* vt = n->bottom_type()->is_vect();
+  return vt->length();
+}
+
+static inline uint vector_length_in_bytes(const MachNode* n) {
+  const TypeVect* vt = n->bottom_type()->is_vect();
+  return vt->length_in_bytes();
+}
+
+static inline uint vector_length_in_bytes(const MachNode* use, MachOper* opnd) {
+  uint def_idx = use->operand_index(opnd);
+  Node* def = use->in(def_idx);
+  return def->bottom_type()->is_vect()->length_in_bytes();
+}
+
+static inline Assembler::AvxVectorLen vector_length_encoding(const MachNode* n) {
+  switch(vector_length_in_bytes(n)) {
+    case  4: // fall-through
+    case  8: // fall-through
+    case 16: return Assembler::AVX_128bit;
+    case 32: return Assembler::AVX_256bit;
+    case 64: return Assembler::AVX_512bit;
+
+    default: {
+      ShouldNotReachHere();
+      return Assembler::AVX_NoVec;
+    }
+  }
+}
+
 // Helper methods for MachSpillCopyNode::implementation().
 static int vec_mov_helper(CodeBuffer *cbuf, bool do_size, int src_lo, int dst_lo,
                           int src_hi, int dst_hi, uint ireg, outputStream* st) {
   // In 64-bit VM size calculation is very complex. Emitting instructions
   // into scratch buffer is used to get size in 64-bit VM.
@@ -1746,12 +1768,12 @@
   }
   // VEX_2bytes prefix is used if UseAVX > 0, and it takes the same 2 bytes as SIMD prefix.
   return (UseAVX > 2) ? 6 : 4;
 }
 
-static int vec_spill_helper(CodeBuffer *cbuf, bool do_size, bool is_load,
-                            int stack_offset, int reg, uint ireg, outputStream* st) {
+int vec_spill_helper(CodeBuffer *cbuf, bool do_size, bool is_load,
+                     int stack_offset, int reg, uint ireg, outputStream* st) {
   // In 64-bit VM size calculation is very complex. Emitting instructions
   // into scratch buffer is used to get size in 64-bit VM.
   LP64_ONLY( assert(!do_size, "this method calculates size only for 32-bit VM"); )
   if (cbuf) {
     MacroAssembler _masm(cbuf);
@@ -2004,20 +2026,126 @@
 //----------OPERANDS-----------------------------------------------------------
 // Operand definitions must precede instruction definitions for correct parsing
 // in the ADLC because operands constitute user defined types which are used in
 // instruction definitions.
 
+// Vectors
+
+// Dummy generic vector class. Should be used for all vector operands.
+// Replaced with vec[SDXYZ] during post-selection pass.
+operand vec() %{
+  constraint(ALLOC_IN_RC(dynamic));
+  match(VecX);
+  match(VecY);
+  match(VecZ);
+  match(VecS);
+  match(VecD);
+
+  format %{ %}
+  interface(REG_INTER);
+%}
+
+// Dummy generic legacy vector class. Should be used for all legacy vector operands.
+// Replaced with legVec[SDXYZ] during post-selection cleanup.
+// Note: legacy register class is used to avoid extra (unneeded in 32-bit VM)
+// runtime code generation via reg_class_dynamic.
+operand legVec() %{
+  constraint(ALLOC_IN_RC(dynamic));
+  match(VecX);
+  match(VecY);
+  match(VecZ);
+  match(VecS);
+  match(VecD);
+
+  format %{ %}
+  interface(REG_INTER);
+%}
+
+// Replaces vec during post-selection cleanup. See above.
+operand vecS() %{
+  constraint(ALLOC_IN_RC(vectors_reg_vlbwdq));
+  match(VecS);
+
+  format %{ %}
+  interface(REG_INTER);
+%}
+
+// Replaces legVec during post-selection cleanup. See above.
+operand legVecS() %{
+  constraint(ALLOC_IN_RC(vectors_reg_legacy));
+  match(VecS);
+
+  format %{ %}
+  interface(REG_INTER);
+%}
+
+// Replaces vec during post-selection cleanup. See above.
+operand vecD() %{
+  constraint(ALLOC_IN_RC(vectord_reg_vlbwdq));
+  match(VecD);
+
+  format %{ %}
+  interface(REG_INTER);
+%}
+
+// Replaces legVec during post-selection cleanup. See above.
+operand legVecD() %{
+  constraint(ALLOC_IN_RC(vectord_reg_legacy));
+  match(VecD);
+
+  format %{ %}
+  interface(REG_INTER);
+%}
+
+// Replaces vec during post-selection cleanup. See above.
+operand vecX() %{
+  constraint(ALLOC_IN_RC(vectorx_reg_vlbwdq));
+  match(VecX);
+
+  format %{ %}
+  interface(REG_INTER);
+%}
+
+// Replaces legVec during post-selection cleanup. See above.
+operand legVecX() %{
+  constraint(ALLOC_IN_RC(vectorx_reg_legacy));
+  match(VecX);
+
+  format %{ %}
+  interface(REG_INTER);
+%}
+
+// Replaces vec during post-selection cleanup. See above.
+operand vecY() %{
+  constraint(ALLOC_IN_RC(vectory_reg_vlbwdq));
+  match(VecY);
+
+  format %{ %}
+  interface(REG_INTER);
+%}
+
+// Replaces legVec during post-selection cleanup. See above.
+operand legVecY() %{
+  constraint(ALLOC_IN_RC(vectory_reg_legacy));
+  match(VecY);
+
+  format %{ %}
+  interface(REG_INTER);
+%}
+
+// Replaces vec during post-selection cleanup. See above.
 operand vecZ() %{
   constraint(ALLOC_IN_RC(vectorz_reg));
   match(VecZ);
 
   format %{ %}
   interface(REG_INTER);
 %}
 
+// Replaces legVec during post-selection cleanup. See above.
 operand legVecZ() %{
-  constraint(ALLOC_IN_RC(vectorz_reg_vl));
+  constraint(ALLOC_IN_RC(vectorz_reg_legacy));
   match(VecZ);
 
   format %{ %}
   interface(REG_INTER);
 %}
@@ -2051,11 +2179,11 @@
 
 instruct ShouldNotReachHere() %{
   match(Halt);
   format %{ "ud2\t# ShouldNotReachHere" %}
   ins_encode %{
-    __ ud2();
+    __ stop(_halt_reason);
   %}
   ins_pipe(pipe_slow);
 %}
 
 // =================================EVEX special===============================
@@ -2808,10 +2936,93 @@
     __ sqrtsd($dst$$XMMRegister, $constantaddress($con));
   %}
   ins_pipe(pipe_slow);
 %}
 
+
+#ifdef _LP64
+instruct roundD_reg(legRegD dst, legRegD src, immU8 rmode) %{
+  match(Set dst (RoundDoubleMode src rmode));
+  format %{ "roundsd $dst,$src" %}
+  ins_cost(150);
+  ins_encode %{
+    assert(UseSSE >= 4, "required");
+    __ roundsd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant);
+  %}
+  ins_pipe(pipe_slow);
+%}
+
+instruct roundD_mem(legRegD dst, memory src, immU8 rmode) %{
+  match(Set dst (RoundDoubleMode (LoadD src) rmode));
+  format %{ "roundsd $dst,$src" %}
+  ins_cost(150);
+  ins_encode %{
+    assert(UseSSE >= 4, "required");
+    __ roundsd($dst$$XMMRegister, $src$$Address, $rmode$$constant);
+  %}
+  ins_pipe(pipe_slow);
+%}
+
+instruct roundD_imm(legRegD dst, immD con, immU8 rmode, rRegI scratch_reg) %{
+  match(Set dst (RoundDoubleMode con rmode));
+  effect(TEMP scratch_reg);
+  format %{ "roundsd $dst,[$constantaddress]\t# load from constant table: double=$con" %}
+  ins_cost(150);
+  ins_encode %{
+    assert(UseSSE >= 4, "required");
+    __ roundsd($dst$$XMMRegister, $constantaddress($con), $rmode$$constant, $scratch_reg$$Register);
+  %}
+  ins_pipe(pipe_slow);
+%}
+
+instruct vroundD_reg(legVec dst, legVec src, immU8 rmode) %{
+  predicate(n->as_Vector()->length() < 8);
+  match(Set dst (RoundDoubleModeV src rmode));
+  format %{ "vroundpd $dst,$src,$rmode\t! round packedD" %}
+  ins_encode %{
+    assert(UseAVX > 0, "required");
+    int vector_len = vector_length_encoding(this);
+    __ vroundpd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant, vector_len);
+  %}
+  ins_pipe( pipe_slow );
+%}
+
+instruct vround8D_reg(vec dst, vec src, immU8 rmode) %{
+  predicate(n->as_Vector()->length() == 8);
+  match(Set dst (RoundDoubleModeV src rmode));
+  format %{ "vrndscalepd $dst,$src,$rmode\t! round packed8D" %}
+  ins_encode %{
+    assert(UseAVX > 2, "required");
+    __ vrndscalepd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant, Assembler::AVX_512bit);
+  %}
+  ins_pipe( pipe_slow );
+%}
+
+instruct vroundD_mem(legVec dst, memory mem, immU8 rmode) %{
+  predicate(n->as_Vector()->length() < 8);
+  match(Set dst (RoundDoubleModeV (LoadVector mem) rmode));
+  format %{ "vroundpd $dst, $mem, $rmode\t! round packedD" %}
+  ins_encode %{
+    assert(UseAVX > 0, "required");
+    int vector_len = vector_length_encoding(this);
+    __ vroundpd($dst$$XMMRegister, $mem$$Address, $rmode$$constant, vector_len);
+  %}
+  ins_pipe( pipe_slow );
+%}
+
+instruct vround8D_mem(vec dst, memory mem, immU8 rmode) %{
+  predicate(n->as_Vector()->length() == 8);
+  match(Set dst (RoundDoubleModeV (LoadVector mem) rmode));
+  format %{ "vrndscalepd $dst,$mem,$rmode\t! round packed8D" %}
+  ins_encode %{
+    assert(UseAVX > 2, "required");
+    __ vrndscalepd($dst$$XMMRegister, $mem$$Address, $rmode$$constant, Assembler::AVX_512bit);
+  %}
+  ins_pipe( pipe_slow );
+%}
+#endif // _LP64
+
 instruct onspinwait() %{
   match(OnSpinWait);
   ins_cost(200);
 
   format %{
@@ -2848,7183 +3059,2753 @@
   ins_pipe( pipe_slow );
 %}
 
 // ====================VECTOR INSTRUCTIONS=====================================
 
-
-// Load vectors (4 bytes long)
-instruct loadV4(vecS dst, memory mem) %{
-  predicate(n->as_LoadVector()->memory_size() == 4);
-  match(Set dst (LoadVector mem));
-  ins_cost(125);
-  format %{ "movd    $dst,$mem\t! load vector (4 bytes)" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $mem$$Address);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Load vectors (4 bytes long)
-instruct MoveVecS2Leg(legVecS dst, vecS src) %{
+// Dummy reg-to-reg vector moves. Removed during post-selection cleanup.
+instruct MoveVec2Leg(legVec dst, vec src) %{
   match(Set dst src);
-  format %{ "movss $dst,$src\t! load vector (4 bytes)" %}
+  format %{ "" %}
   ins_encode %{
-    __ movflt($dst$$XMMRegister, $src$$XMMRegister);
+    ShouldNotReachHere();
   %}
   ins_pipe( fpu_reg_reg );
 %}
 
-// Load vectors (4 bytes long)
-instruct MoveLeg2VecS(vecS dst, legVecS src) %{
+instruct MoveLeg2Vec(vec dst, legVec src) %{
   match(Set dst src);
-  format %{ "movss $dst,$src\t! load vector (4 bytes)" %}
+  format %{ "" %}
   ins_encode %{
-    __ movflt($dst$$XMMRegister, $src$$XMMRegister);
+    ShouldNotReachHere();
   %}
   ins_pipe( fpu_reg_reg );
 %}
 
-// Load vectors (8 bytes long)
-instruct loadV8(vecD dst, memory mem) %{
-  predicate(n->as_LoadVector()->memory_size() == 8);
+// ============================================================================
+
+// Load vectors
+instruct loadV(vec dst, memory mem) %{
   match(Set dst (LoadVector mem));
   ins_cost(125);
-  format %{ "movq    $dst,$mem\t! load vector (8 bytes)" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $mem$$Address);
+  format %{ "load_vector $dst,$mem" %}
+  ins_encode %{
+    switch (vector_length_in_bytes(this)) {
+      case  4: __ movdl    ($dst$$XMMRegister, $mem$$Address); break;
+      case  8: __ movq     ($dst$$XMMRegister, $mem$$Address); break;
+      case 16: __ movdqu   ($dst$$XMMRegister, $mem$$Address); break;
+      case 32: __ vmovdqu  ($dst$$XMMRegister, $mem$$Address); break;
+      case 64: __ evmovdqul($dst$$XMMRegister, $mem$$Address, Assembler::AVX_512bit); break;
+      default: ShouldNotReachHere();
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-// Load vectors (8 bytes long)
-instruct MoveVecD2Leg(legVecD dst, vecD src) %{
-  match(Set dst src);
-  format %{ "movsd $dst,$src\t! load vector (8 bytes)" %}
-  ins_encode %{
-    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
+// Store vectors generic operand pattern.
+instruct storeV(memory mem, vec src) %{
+  match(Set mem (StoreVector mem src));
+  ins_cost(145);
+  format %{ "store_vector $mem,$src\n\t" %}
+  ins_encode %{
+    switch (vector_length_in_bytes(this, $src)) {
+      case  4: __ movdl    ($mem$$Address, $src$$XMMRegister); break;
+      case  8: __ movq     ($mem$$Address, $src$$XMMRegister); break;
+      case 16: __ movdqu   ($mem$$Address, $src$$XMMRegister); break;
+      case 32: __ vmovdqu  ($mem$$Address, $src$$XMMRegister); break;
+      case 64: __ evmovdqul($mem$$Address, $src$$XMMRegister, Assembler::AVX_512bit); break;
+      default: ShouldNotReachHere();
+    }
   %}
-  ins_pipe( fpu_reg_reg );
+  ins_pipe( pipe_slow );
 %}
 
-// Load vectors (8 bytes long)
-instruct MoveLeg2VecD(vecD dst, legVecD src) %{
-  match(Set dst src);
-  format %{ "movsd $dst,$src\t! load vector (8 bytes)" %}
+// ====================REPLICATE=======================================
+
+// Replicate byte scalar to be vector
+instruct ReplB_reg(vec dst, rRegI src) %{
+  predicate((n->as_Vector()->length() <= 32) ||
+            (n->as_Vector()->length() == 64 && VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions
+  match(Set dst (ReplicateB src));
+  format %{ "replicateB $dst,$src" %}
   ins_encode %{
-    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
+    uint vlen = vector_length(this);
+    if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for <512bit operands
+      assert(VM_Version::supports_avx512bw(), "required");
+      int vlen_enc = vector_length_encoding(this);
+      __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);
+    } else {
+      __ movdl($dst$$XMMRegister, $src$$Register);
+      __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
+      __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
+      if (vlen >= 16) {
+        __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+        if (vlen >= 32) {
+          assert(vlen == 32, "sanity"); // vlen == 64 && !AVX512BW is covered by ReplB_reg_leg
+          __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+        }
+      }
+    }
   %}
-  ins_pipe( fpu_reg_reg );
+  ins_pipe( pipe_slow );
 %}
 
-// Load vectors (16 bytes long)
-instruct loadV16(vecX dst, memory mem) %{
-  predicate(n->as_LoadVector()->memory_size() == 16);
-  match(Set dst (LoadVector mem));
-  ins_cost(125);
-  format %{ "movdqu  $dst,$mem\t! load vector (16 bytes)" %}
+instruct ReplB_reg_leg(legVec dst, rRegI src) %{
+  predicate(n->as_Vector()->length() == 64 && !VM_Version::supports_avx512bw()); // AVX512BW for 512bit byte instructions
+  match(Set dst (ReplicateB src));
+  format %{ "replicateB $dst,$src" %}
   ins_encode %{
-    __ movdqu($dst$$XMMRegister, $mem$$Address);
+    assert(UseAVX > 2, "required");
+    __ movdl($dst$$XMMRegister, $src$$Register);
+    __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
+    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
+    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
   %}
   ins_pipe( pipe_slow );
 %}
 
-// Load vectors (16 bytes long)
-instruct MoveVecX2Leg(legVecX dst, vecX src) %{
-  match(Set dst src);
-  format %{ "movdqu $dst,$src\t! load vector (16 bytes)" %}
+instruct ReplB_mem(vec dst, memory mem) %{
+  predicate((n->as_Vector()->length() <= 32 && VM_Version::supports_avx512vlbw()) || // AVX512VL for <512bit operands
+            (n->as_Vector()->length() == 64 && VM_Version::supports_avx512bw()));    // AVX512BW for 512bit byte instructions
+  match(Set dst (ReplicateB (LoadB mem)));
+  format %{ "replicateB $dst,$mem" %}
   ins_encode %{
-    if (UseAVX > 2 && !VM_Version::supports_avx512vl()) {
-      int vector_len = 2;
-      __ evmovdquq($dst$$XMMRegister, $src$$XMMRegister, vector_len);
-    } else {
-      __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
-    }
+    assert(UseAVX > 2, "required");
+    int vector_len = vector_length_encoding(this);
+    __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
   %}
-  ins_pipe( fpu_reg_reg );
+  ins_pipe( pipe_slow );
 %}
 
-// Load vectors (16 bytes long)
-instruct MoveLeg2VecX(vecX dst, legVecX src) %{
-  match(Set dst src);
-  format %{ "movdqu $dst,$src\t! load vector (16 bytes)" %}
+instruct ReplB_imm(vec dst, immI con) %{
+  predicate((n->as_Vector()->length() <= 32) ||
+            (n->as_Vector()->length() == 64 && VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions
+  match(Set dst (ReplicateB con));
+  format %{ "replicateB $dst,$con" %}
   ins_encode %{
-    if (UseAVX > 2 && !VM_Version::supports_avx512vl()) {
-      int vector_len = 2;
-      __ evmovdquq($dst$$XMMRegister, $src$$XMMRegister, vector_len);
+    uint vlen = vector_length(this);
+    InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 1));
+    if (vlen == 4) {
+      __ movdl($dst$$XMMRegister, const_addr);
     } else {
-      __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
+      __ movq($dst$$XMMRegister, const_addr);
+      if (vlen >= 16) {
+        if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for <512bit operands
+          int vlen_enc = vector_length_encoding(this);
+          __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
+        } else {
+          __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+          if (vlen >= 32) {
+             assert(vlen == 32, "sanity");// vlen == 64 && !AVX512BW is covered by ReplB_imm_leg
+            __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+          }
+        }
+      }
     }
   %}
-  ins_pipe( fpu_reg_reg );
+  ins_pipe( pipe_slow );
 %}
 
-// Load vectors (32 bytes long)
-instruct loadV32(vecY dst, memory mem) %{
-  predicate(n->as_LoadVector()->memory_size() == 32);
-  match(Set dst (LoadVector mem));
-  ins_cost(125);
-  format %{ "vmovdqu $dst,$mem\t! load vector (32 bytes)" %}
+instruct ReplB_imm_leg(legVec dst, immI con) %{
+  predicate(n->as_Vector()->length() == 64 && !VM_Version::supports_avx512bw());
+  match(Set dst (ReplicateB con));
+  format %{ "replicateB $dst,$con" %}
   ins_encode %{
-    __ vmovdqu($dst$$XMMRegister, $mem$$Address);
+    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 1)));
+    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
   %}
   ins_pipe( pipe_slow );
 %}
 
-// Load vectors (32 bytes long)
-instruct MoveVecY2Leg(legVecY dst, vecY src) %{
-  match(Set dst src);
-  format %{ "vmovdqu $dst,$src\t! load vector (32 bytes)" %}
+// Replicate byte scalar zero to be vector
+instruct ReplB_zero(vec dst, immI0 zero) %{
+  match(Set dst (ReplicateB zero));
+  format %{ "replicateB $dst,$zero" %}
   ins_encode %{
-    if (UseAVX > 2 && !VM_Version::supports_avx512vl()) {
-      int vector_len = 2;
-      __ evmovdquq($dst$$XMMRegister, $src$$XMMRegister, vector_len);
+    uint vlen = vector_length(this);
+    if (vlen <= 16) {
+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
     } else {
-      __ vmovdqu($dst$$XMMRegister, $src$$XMMRegister);
+      // Use vpxor since AVX512F does not have 512bit vxorpd (requires AVX512DQ).
+      int vlen_enc = vector_length_encoding(this);
+      __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
     }
   %}
   ins_pipe( fpu_reg_reg );
 %}
 
-// Load vectors (32 bytes long)
-instruct MoveLeg2VecY(vecY dst, legVecY src) %{
-  match(Set dst src);
-  format %{ "vmovdqu $dst,$src\t! load vector (32 bytes)" %}
+// ====================ReplicateS=======================================
+
+instruct ReplS_reg(vec dst, rRegI src) %{
+  predicate((n->as_Vector()->length() <= 16) ||
+            (n->as_Vector()->length() == 32 && VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts
+  match(Set dst (ReplicateS src));
+  format %{ "replicateS $dst,$src" %}
   ins_encode %{
-    if (UseAVX > 2 && !VM_Version::supports_avx512vl()) {
-      int vector_len = 2;
-      __ evmovdquq($dst$$XMMRegister, $src$$XMMRegister, vector_len);
+    uint vlen = vector_length(this);
+    if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for <512bit operands
+      assert(VM_Version::supports_avx512bw(), "required");
+      int vlen_enc = vector_length_encoding(this);
+      __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);
     } else {
-      __ vmovdqu($dst$$XMMRegister, $src$$XMMRegister);
+      __ movdl($dst$$XMMRegister, $src$$Register);
+      __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
+      if (vlen >= 8) {
+        __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+        if (vlen >= 16) {
+          assert(vlen == 16, "sanity"); // vlen == 32 && !AVX512BW is covered by ReplS_reg_leg
+          __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+        }
+      }
     }
   %}
-  ins_pipe( fpu_reg_reg );
+  ins_pipe( pipe_slow );
 %}
 
-// Load vectors (64 bytes long)
-instruct loadV64_dword(vecZ dst, memory mem) %{
-  predicate(n->as_LoadVector()->memory_size() == 64 && n->as_LoadVector()->element_size() <= 4);
-  match(Set dst (LoadVector mem));
-  ins_cost(125);
-  format %{ "vmovdqul $dst k0,$mem\t! load vector (64 bytes)" %}
+instruct ReplS_reg_leg(legVec dst, rRegI src) %{
+  predicate(n->as_Vector()->length() == 32 && !VM_Version::supports_avx512bw());
+  match(Set dst (ReplicateS src));
+  format %{ "replicateS $dst,$src" %}
   ins_encode %{
-    int vector_len = 2;
-    __ evmovdqul($dst$$XMMRegister, $mem$$Address, vector_len);
+    __ movdl($dst$$XMMRegister, $src$$Register);
+    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
+    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
   %}
   ins_pipe( pipe_slow );
 %}
 
-// Load vectors (64 bytes long)
-instruct loadV64_qword(vecZ dst, memory mem) %{
-  predicate(n->as_LoadVector()->memory_size() == 64 && n->as_LoadVector()->element_size() > 4);
-  match(Set dst (LoadVector mem));
-  ins_cost(125);
-  format %{ "vmovdquq $dst k0,$mem\t! load vector (64 bytes)" %}
+instruct ReplS_mem(vec dst, memory mem) %{
+  predicate((n->as_Vector()->length() >= 4  &&
+             n->as_Vector()->length() <= 16 && VM_Version::supports_avx()) ||
+            (n->as_Vector()->length() == 32 && VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts
+  match(Set dst (ReplicateS (LoadS mem)));
+  format %{ "replicateS $dst,$mem" %}
   ins_encode %{
-    int vector_len = 2;
-    __ evmovdquq($dst$$XMMRegister, $mem$$Address, vector_len);
+    uint vlen = vector_length(this);
+    if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for <512bit operands
+      assert(VM_Version::supports_avx512bw(), "required");
+      int vlen_enc = vector_length_encoding(this);
+      __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);
+    } else {
+      __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);
+      if (vlen >= 8) {
+        __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+        if (vlen >= 16) {
+          assert(vlen == 16, "sanity"); // vlen == 32 && !AVX512BW is covered by ReplS_mem_leg
+          __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+        }
+      }
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct MoveVecZ2Leg(legVecZ dst, vecZ  src) %{
-  match(Set dst src);
-  format %{ "vmovdquq $dst k0,$src\t! Move vector (64 bytes)" %}
+instruct ReplS_mem_leg(legVec dst, memory mem) %{
+  predicate(n->as_Vector()->length() == 32 && !VM_Version::supports_avx512bw());
+  match(Set dst (ReplicateS (LoadS mem)));
+  format %{ "replicateS $dst,$mem" %}
   ins_encode %{
-    int vector_len = 2;
-    __ evmovdquq($dst$$XMMRegister, $src$$XMMRegister, vector_len);
+    __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);
+    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
   %}
-  ins_pipe( fpu_reg_reg );
+  ins_pipe( pipe_slow );
 %}
 
-instruct MoveLeg2VecZ(vecZ dst, legVecZ  src) %{
-  match(Set dst src);
-  format %{ "vmovdquq $dst k0,$src\t! Move vector (64 bytes)" %}
+instruct ReplS_imm(vec dst, immI con) %{
+  predicate((n->as_Vector()->length() <= 16) ||
+            (n->as_Vector()->length() == 32 && VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts
+  match(Set dst (ReplicateS con));
+  format %{ "replicateS $dst,$con" %}
   ins_encode %{
-    int vector_len = 2;
-    __ evmovdquq($dst$$XMMRegister, $src$$XMMRegister, vector_len);
+    uint vlen = vector_length(this);
+    InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 2));
+    if (vlen == 2) {
+      __ movdl($dst$$XMMRegister, constaddr);
+    } else {
+      __ movq($dst$$XMMRegister, constaddr);
+      if (vlen == 32 || VM_Version::supports_avx512vlbw() ) { // AVX512VL for <512bit operands
+        assert(VM_Version::supports_avx512bw(), "required");
+        int vlen_enc = vector_length_encoding(this);
+        __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
+      } else {
+        __ movq($dst$$XMMRegister, constaddr);
+        __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+        if (vlen >= 16) {
+          assert(vlen == 16, "sanity"); // vlen == 32 && !AVX512BW is covered by ReplS_imm_leg
+          __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+        }
+      }
+    }
   %}
   ins_pipe( fpu_reg_reg );
 %}
 
-// Store vectors
-instruct storeV4(memory mem, vecS src) %{
-  predicate(n->as_StoreVector()->memory_size() == 4);
-  match(Set mem (StoreVector mem src));
-  ins_cost(145);
-  format %{ "movd    $mem,$src\t! store vector (4 bytes)" %}
+instruct ReplS_imm_leg(legVec dst, immI con) %{
+  predicate(n->as_Vector()->length() == 32 && !VM_Version::supports_avx512bw());
+  match(Set dst (ReplicateS con));
+  format %{ "replicateS $dst,$con" %}
   ins_encode %{
-    __ movdl($mem$$Address, $src$$XMMRegister);
+    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 2)));
+    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct storeV8(memory mem, vecD src) %{
-  predicate(n->as_StoreVector()->memory_size() == 8);
-  match(Set mem (StoreVector mem src));
-  ins_cost(145);
-  format %{ "movq    $mem,$src\t! store vector (8 bytes)" %}
+instruct ReplS_zero(vec dst, immI0 zero) %{
+  match(Set dst (ReplicateS zero));
+  format %{ "replicateS $dst,$zero" %}
   ins_encode %{
-    __ movq($mem$$Address, $src$$XMMRegister);
+    uint vlen = vector_length(this);
+    if (vlen <= 8) {
+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
+    } else {
+      int vlen_enc = vector_length_encoding(this);
+      __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
+    }
   %}
-  ins_pipe( pipe_slow );
+  ins_pipe( fpu_reg_reg );
 %}
 
-instruct storeV16(memory mem, vecX src) %{
-  predicate(n->as_StoreVector()->memory_size() == 16);
-  match(Set mem (StoreVector mem src));
-  ins_cost(145);
-  format %{ "movdqu  $mem,$src\t! store vector (16 bytes)" %}
+// ====================ReplicateI=======================================
+
+instruct ReplI_reg(vec dst, rRegI src) %{
+  match(Set dst (ReplicateI src));
+  format %{ "replicateI $dst,$src" %}
   ins_encode %{
-    __ movdqu($mem$$Address, $src$$XMMRegister);
+    uint vlen = vector_length(this);
+    if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+      int vlen_enc = vector_length_encoding(this);
+      __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vlen_enc);
+    } else {
+      __ movdl($dst$$XMMRegister, $src$$Register);
+      __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
+      if (vlen >= 8) {
+        assert(vlen == 8, "sanity");
+        __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+      }
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct storeV32(memory mem, vecY src) %{
-  predicate(n->as_StoreVector()->memory_size() == 32);
-  match(Set mem (StoreVector mem src));
-  ins_cost(145);
-  format %{ "vmovdqu $mem,$src\t! store vector (32 bytes)" %}
-  ins_encode %{
-    __ vmovdqu($mem$$Address, $src$$XMMRegister);
+instruct ReplI_mem(vec dst, memory mem) %{
+  predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source
+  match(Set dst (ReplicateI (LoadI mem)));
+  format %{ "replicateI $dst,$mem" %}
+  ins_encode %{
+    uint vlen = vector_length(this);
+    if (vlen <= 4) {
+      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
+    } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+      int vector_len = vector_length_encoding(this);
+      __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);
+    } else {
+      assert(vlen == 8, "sanity");
+      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
+      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct storeV64_dword(memory mem, vecZ src) %{
-  predicate(n->as_StoreVector()->memory_size() == 64 && n->as_StoreVector()->element_size() <= 4);
-  match(Set mem (StoreVector mem src));
-  ins_cost(145);
-  format %{ "vmovdqul $mem k0,$src\t! store vector (64 bytes)" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ evmovdqul($mem$$Address, $src$$XMMRegister, vector_len);
+instruct ReplI_imm(vec dst, immI con) %{
+  match(Set dst (ReplicateI con));
+  format %{ "replicateI $dst,$con" %}
+  ins_encode %{
+    uint vlen = vector_length(this);
+    InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 4));
+    if (vlen == 2) {
+      __ movq($dst$$XMMRegister, constaddr);
+    } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+      int vector_len = vector_length_encoding(this);
+      __ movq($dst$$XMMRegister, constaddr);
+      __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
+    } else {
+      __ movq($dst$$XMMRegister, constaddr);
+      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+      if (vlen >= 8) {
+        assert(vlen == 8, "sanity");
+        __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+      }
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct storeV64_qword(memory mem, vecZ src) %{
-  predicate(n->as_StoreVector()->memory_size() == 64 && n->as_StoreVector()->element_size() > 4);
-  match(Set mem (StoreVector mem src));
-  ins_cost(145);
-  format %{ "vmovdquq $mem k0,$src\t! store vector (64 bytes)" %}
+// Replicate integer (4 byte) scalar zero to be vector
+instruct ReplI_zero(vec dst, immI0 zero) %{
+  match(Set dst (ReplicateI zero));
+  format %{ "replicateI $dst,$zero" %}
   ins_encode %{
-    int vector_len = 2;
-    __ evmovdquq($mem$$Address, $src$$XMMRegister, vector_len);
+    uint vlen = vector_length(this);
+    if (vlen <= 4) {
+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
+    } else {
+      int vlen_enc = vector_length_encoding(this);
+      __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
+    }
   %}
-  ins_pipe( pipe_slow );
+  ins_pipe( fpu_reg_reg );
 %}
 
-// ====================LEGACY REPLICATE=======================================
+// ====================ReplicateL=======================================
 
-instruct Repl4B_mem(vecS dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 0 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB (LoadB mem)));
-  format %{ "punpcklbw $dst,$mem\n\t"
-            "pshuflw $dst,$dst,0x00\t! replicate4B" %}
-  ins_encode %{
-    __ punpcklbw($dst$$XMMRegister, $mem$$Address);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
+#ifdef _LP64
+// Replicate long (8 byte) scalar to be vector
+instruct ReplL_reg(vec dst, rRegL src) %{
+  match(Set dst (ReplicateL src));
+  format %{ "replicateL $dst,$src" %}
+  ins_encode %{
+    uint vlen = vector_length(this);
+    if (vlen == 2) {
+      __ movdq($dst$$XMMRegister, $src$$Register);
+      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+    } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+      int vlen_enc = vector_length_encoding(this);
+      __ evpbroadcastq($dst$$XMMRegister, $src$$Register, vlen_enc);
+    } else {
+      assert(vlen == 4, "sanity");
+      __ movdq($dst$$XMMRegister, $src$$Register);
+      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
-
-instruct Repl8B_mem(vecD dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 0 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB (LoadB mem)));
-  format %{ "punpcklbw $dst,$mem\n\t"
-            "pshuflw $dst,$dst,0x00\t! replicate8B" %}
-  ins_encode %{
-    __ punpcklbw($dst$$XMMRegister, $mem$$Address);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
+#else // _LP64
+// Replicate long (8 byte) scalar to be vector
+instruct ReplL_reg(vec dst, eRegL src, vec tmp) %{
+  predicate(n->as_Vector()->length() <= 4);
+  match(Set dst (ReplicateL src));
+  effect(TEMP dst, USE src, TEMP tmp);
+  format %{ "replicateL $dst,$src" %}
+  ins_encode %{
+    uint vlen = vector_length(this);
+    if (vlen == 2) {
+      __ movdl($dst$$XMMRegister, $src$$Register);
+      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
+      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
+      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+    } else if (VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+      int vector_len = Assembler::AVX_256bit;
+      __ movdl($dst$$XMMRegister, $src$$Register);
+      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
+      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
+      __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
+    } else {
+      __ movdl($dst$$XMMRegister, $src$$Register);
+      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
+      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
+      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct Repl16B(vecX dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 16 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB src));
-  format %{ "movd    $dst,$src\n\t"
-            "punpcklbw $dst,$dst\n\t"
-            "pshuflw $dst,$dst,0x00\n\t"
-            "punpcklqdq $dst,$dst\t! replicate16B" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+instruct ReplL_reg_leg(legVec dst, eRegL src, legVec tmp) %{
+  predicate(n->as_Vector()->length() == 8);
+  match(Set dst (ReplicateL src));
+  effect(TEMP dst, USE src, TEMP tmp);
+  format %{ "replicateL $dst,$src" %}
+  ins_encode %{
+    if (VM_Version::supports_avx512vl()) {
+      __ movdl($dst$$XMMRegister, $src$$Register);
+      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
+      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
+      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+      __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
+    } else {
+      int vector_len = Assembler::AVX_512bit;
+      __ movdl($dst$$XMMRegister, $src$$Register);
+      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
+      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
+      __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
+#endif // _LP64
 
-instruct Repl16B_mem(vecX dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 16 && UseAVX > 0 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB (LoadB mem)));
-  format %{ "punpcklbw $dst,$mem\n\t"
-            "pshuflw $dst,$dst,0x00\n\t"
-            "punpcklqdq $dst,$dst\t! replicate16B" %}
-  ins_encode %{
-    __ punpcklbw($dst$$XMMRegister, $mem$$Address);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+instruct ReplL_mem(vec dst, memory mem) %{
+  match(Set dst (ReplicateL (LoadL mem)));
+  format %{ "replicateL $dst,$mem" %}
+  ins_encode %{
+    uint vlen = vector_length(this);
+    if (vlen == 2) {
+      __ movq($dst$$XMMRegister, $mem$$Address);
+      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+    } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+      int vlen_enc = vector_length_encoding(this);
+      __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);
+    } else {
+      assert(vlen == 4, "sanity");
+      __ movq($dst$$XMMRegister, $mem$$Address);
+      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct Repl32B(vecY dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 32 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB src));
-  format %{ "movd    $dst,$src\n\t"
-            "punpcklbw $dst,$dst\n\t"
-            "pshuflw $dst,$dst,0x00\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t! replicate32B" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+// Replicate long (8 byte) scalar immediate to be vector by loading from const table.
+instruct ReplL_imm(vec dst, immL con) %{
+  match(Set dst (ReplicateL con));
+  format %{ "replicateL $dst,$con" %}
+  ins_encode %{
+    uint vlen = vector_length(this);
+    InternalAddress const_addr = $constantaddress($con);
+    if (vlen == 2) {
+      __ movq($dst$$XMMRegister, const_addr);
+      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+    } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+      int vlen_enc = vector_length_encoding(this);
+      __ movq($dst$$XMMRegister, const_addr);
+      __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
+    } else {
+      assert(vlen == 4, "sanity");
+      __ movq($dst$$XMMRegister, const_addr);
+      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct Repl32B_mem(vecY dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 32 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB (LoadB mem)));
-  format %{ "punpcklbw $dst,$mem\n\t"
-            "pshuflw $dst,$dst,0x00\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t! replicate32B" %}
+instruct ReplL_zero(vec dst, immL0 zero) %{
+  match(Set dst (ReplicateL zero));
+  format %{ "replicateL $dst,$zero" %}
   ins_encode %{
-    __ punpcklbw($dst$$XMMRegister, $mem$$Address);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+    int vlen = vector_length(this);
+    if (vlen == 2) {
+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
+    } else {
+      int vlen_enc = vector_length_encoding(this);
+      __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
+    }
   %}
-  ins_pipe( pipe_slow );
+  ins_pipe( fpu_reg_reg );
 %}
 
-instruct Repl64B(legVecZ dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 64 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB src));
-  format %{ "movd    $dst,$src\n\t"
-            "punpcklbw $dst,$dst\n\t"
-            "pshuflw $dst,$dst,0x00\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t"
-            "vinserti64x4 $dst,$dst,$dst,0x1\t! replicate64B" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
+// ====================ReplicateF=======================================
 
-instruct Repl64B_mem(legVecZ dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 64 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB (LoadB mem)));
-  format %{ "punpcklbw $dst,$mem\n\t"
-            "pshuflw $dst,$dst,0x00\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t"
-            "vinserti64x4 $dst,$dst,$dst,0x1\t! replicate64B" %}
-  ins_encode %{
-    __ punpcklbw($dst$$XMMRegister, $mem$$Address);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
+instruct ReplF_reg(vec dst, vlRegF src) %{
+  match(Set dst (ReplicateF src));
+  format %{ "replicateF $dst,$src" %}
+  ins_encode %{
+    uint vlen = vector_length(this);
+    if (vlen <= 4) {
+      __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
+    } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+      int vector_len = vector_length_encoding(this);
+      __ vbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len);
+    } else {
+      assert(vlen == 8, "sanity");
+      __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
+      __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct Repl16B_imm(vecX dst, immI con) %{
-  predicate(n->as_Vector()->length() == 16 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB con));
-  format %{ "movq    $dst,[$constantaddress]\n\t"
-            "punpcklqdq $dst,$dst\t! replicate16B($con)" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 1)));
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+instruct ReplF_mem(vec dst, memory mem) %{
+  predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source
+  match(Set dst (ReplicateF (LoadF mem)));
+  format %{ "replicateF $dst,$mem" %}
+  ins_encode %{
+    uint vlen = vector_length(this);
+    if (vlen <= 4) {
+      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
+    } else if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+      int vector_len = vector_length_encoding(this);
+      __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);
+    } else {
+      assert(vlen == 8, "sanity");
+      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
+      __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct Repl32B_imm(vecY dst, immI con) %{
-  predicate(n->as_Vector()->length() == 32 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB con));
-  format %{ "movq    $dst,[$constantaddress]\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t! lreplicate32B($con)" %}
+instruct ReplF_zero(vec dst, immF0 zero) %{
+  match(Set dst (ReplicateF zero));
+  format %{ "replicateF $dst,$zero" %}
   ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 1)));
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+    uint vlen = vector_length(this);
+    if (vlen <= 4) {
+      __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
+    } else {
+      int vlen_enc = vector_length_encoding(this);
+      __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
+    }
   %}
-  ins_pipe( pipe_slow );
+  ins_pipe( fpu_reg_reg );
 %}
 
-instruct Repl64B_imm(legVecZ dst, immI con) %{
-  predicate(n->as_Vector()->length() == 64 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB con));
-  format %{ "movq    $dst,[$constantaddress]\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t"
-            "vinserti64x4 $dst,$dst,$dst,0x1\t! replicate64B($con)" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 1)));
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
+// ====================ReplicateD=======================================
 
-instruct Repl4S(vecD dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 4 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS src));
-  format %{ "movd    $dst,$src\n\t"
-            "pshuflw $dst,$dst,0x00\t! replicate4S" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
+// Replicate double (8 bytes) scalar to be vector
+instruct ReplD_reg(vec dst, vlRegD src) %{
+  match(Set dst (ReplicateD src));
+  format %{ "replicateD $dst,$src" %}
+  ins_encode %{
+    uint vlen = vector_length(this);
+    if (vlen == 2) {
+      __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
+    } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+      int vector_len = vector_length_encoding(this);
+      __ vbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
+    } else {
+      assert(vlen == 4, "sanity");
+      __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
+      __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct Repl4S_mem(vecD dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 0 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS (LoadS mem)));
-  format %{ "pshuflw $dst,$mem,0x00\t! replicate4S" %}
-  ins_encode %{
-    __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);
+instruct ReplD_mem(vec dst, memory mem) %{
+  predicate(VM_Version::supports_avx()); // use VEX-encoded pshufd to relax 16-byte alignment restriction on the source
+  match(Set dst (ReplicateD (LoadD mem)));
+  format %{ "replicateD $dst,$mem" %}
+  ins_encode %{
+    uint vlen = vector_length(this);
+    if (vlen == 2) {
+      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);
+    } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for <512bit operands
+      int vector_len = vector_length_encoding(this);
+      __ vbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);
+    } else {
+      assert(vlen == 4, "sanity");
+      __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);
+      __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct Repl8S(vecX dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 8 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS src));
-  format %{ "movd    $dst,$src\n\t"
-            "pshuflw $dst,$dst,0x00\n\t"
-            "punpcklqdq $dst,$dst\t! replicate8S" %}
+instruct ReplD_zero(vec dst, immD0 zero) %{
+  match(Set dst (ReplicateD zero));
+  format %{ "replicateD $dst,$zero" %}
   ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+    uint vlen = vector_length(this);
+    if (vlen == 2) {
+      __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);
+    } else {
+      int vlen_enc = vector_length_encoding(this);
+      __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
+    }
   %}
-  ins_pipe( pipe_slow );
+  ins_pipe( fpu_reg_reg );
 %}
 
-instruct Repl8S_mem(vecX dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 0 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS (LoadS mem)));
-  format %{ "pshuflw $dst,$mem,0x00\n\t"
-            "punpcklqdq $dst,$dst\t! replicate8S" %}
-  ins_encode %{
-    __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+// ====================REDUCTION ARITHMETIC=======================================
+
+// =======================AddReductionVI==========================================
+
+instruct vadd2I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 2); // vector_length(src2) == 2
+  match(Set dst (AddReductionVI src1 src2));
+  effect(TEMP tmp, TEMP tmp2);
+  format %{ "vector_add2I_reduction $dst,$src1,$src2" %}
+  ins_encode %{
+    if (UseAVX > 2) {
+      int vector_len = Assembler::AVX_128bit;
+      __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
+      __ vpaddd($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+      __ movdl($tmp2$$XMMRegister, $src1$$Register);
+      __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+      __ movdl($dst$$Register, $tmp2$$XMMRegister);
+    } else if (VM_Version::supports_avxonly()) {
+      int vector_len = Assembler::AVX_128bit;
+      __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);
+      __ movdl($tmp2$$XMMRegister, $src1$$Register);
+      __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, vector_len);
+      __ movdl($dst$$Register, $tmp2$$XMMRegister);
+    } else {
+      assert(UseSSE > 2, "required");
+      __ movdqu($tmp2$$XMMRegister, $src2$$XMMRegister);
+      __ phaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister);
+      __ movdl($tmp$$XMMRegister, $src1$$Register);
+      __ paddd($tmp$$XMMRegister, $tmp2$$XMMRegister);
+      __ movdl($dst$$Register, $tmp$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct Repl8S_imm(vecX dst, immI con) %{
-  predicate(n->as_Vector()->length() == 8 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS con));
-  format %{ "movq    $dst,[$constantaddress]\n\t"
-            "punpcklqdq $dst,$dst\t! replicate8S($con)" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 2)));
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
+instruct vadd4I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 4); // vector_length(src2) == 4
+  match(Set dst (AddReductionVI src1 src2));
+  effect(TEMP tmp, TEMP tmp2);
+  format %{ "vector_add4I_reduction $dst,$src1,$src2" %}
+  ins_encode %{
+    if (UseAVX > 2) {
+      int vector_len = Assembler::AVX_128bit;
+      __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
+      __ vpaddd($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+      __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
+      __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+      __ movdl($tmp2$$XMMRegister, $src1$$Register);
+      __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+      __ movdl($dst$$Register, $tmp2$$XMMRegister);
+    } else if (VM_Version::supports_avxonly()) {
+      int vector_len = Assembler::AVX_128bit;
+      __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);
+      __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp$$XMMRegister, vector_len);
+      __ movdl($tmp2$$XMMRegister, $src1$$Register);
+      __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, vector_len);
+      __ movdl($dst$$Register, $tmp2$$XMMRegister);
+    } else {
+      assert(UseSSE > 2, "required");
+      __ movdqu($tmp$$XMMRegister, $src2$$XMMRegister);
+      __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);
+      __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);
+      __ movdl($tmp2$$XMMRegister, $src1$$Register);
+      __ paddd($tmp2$$XMMRegister, $tmp$$XMMRegister);
+      __ movdl($dst$$Register, $tmp2$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct Repl16S(vecY dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 16 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS src));
-  format %{ "movd    $dst,$src\n\t"
-            "pshuflw $dst,$dst,0x00\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t! replicate16S" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
+instruct vadd8I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8); // vector_length(src2) == 8
+  match(Set dst (AddReductionVI src1 src2));
+  effect(TEMP tmp, TEMP tmp2);
+  format %{ "vector_add8I_reduction $dst,$src1,$src2" %}
+  ins_encode %{
+    if (UseAVX > 2) {
+      int vector_len = Assembler::AVX_128bit;
+      __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
+      __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, vector_len);
+      __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
+      __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+      __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
+      __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+      __ movdl($tmp2$$XMMRegister, $src1$$Register);
+      __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+      __ movdl($dst$$Register, $tmp2$$XMMRegister);
+    } else {
+      assert(UseAVX > 0, "");
+      int vector_len = Assembler::AVX_256bit;
+      __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);
+      __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+      __ vextracti128_high($tmp2$$XMMRegister, $tmp$$XMMRegister);
+      __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
+      __ movdl($tmp2$$XMMRegister, $src1$$Register);
+      __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
+      __ movdl($dst$$Register, $tmp2$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct Repl16S_mem(vecY dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 16 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS (LoadS mem)));
-  format %{ "pshuflw $dst,$mem,0x00\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t! replicate16S" %}
-  ins_encode %{
-    __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16S_imm(vecY dst, immI con) %{
-  predicate(n->as_Vector()->length() == 16 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS con));
-  format %{ "movq    $dst,[$constantaddress]\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t! replicate16S($con)" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 2)));
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl32S(legVecZ dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 32 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS src));
-  format %{ "movd    $dst,$src\n\t"
-            "pshuflw $dst,$dst,0x00\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t"
-            "vinserti64x4 $dst,$dst,$dst,0x1\t! replicate32S" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl32S_mem(legVecZ dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 32 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS (LoadS mem)));
-  format %{ "pshuflw $dst,$mem,0x00\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t"
-            "vinserti64x4 $dst,$dst,$dst,0x1\t! replicate32S" %}
-  ins_encode %{
-    __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl32S_imm(legVecZ dst, immI con) %{
-  predicate(n->as_Vector()->length() == 32 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS con));
-  format %{ "movq    $dst,[$constantaddress]\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t"
-            "vinserti64x4 $dst,$dst,$dst,0x1\t! replicate32S($con)" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 2)));
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl4I(vecX dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 4 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateI src));
-  format %{ "movd    $dst,$src\n\t"
-            "pshufd  $dst,$dst,0x00\t! replicate4I" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl4I_mem(vecX dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 0 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateI (LoadI mem)));
-  format %{ "pshufd  $dst,$mem,0x00\t! replicate4I" %}
-  ins_encode %{
-    __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8I(vecY dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 8 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateI src));
-  format %{ "movd    $dst,$src\n\t"
-            "pshufd  $dst,$dst,0x00\n\t"
-            "vinserti128_high $dst,$dst\t! replicate8I" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8I_mem(vecY dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 8 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateI (LoadI mem)));
-  format %{ "pshufd  $dst,$mem,0x00\n\t"
-            "vinserti128_high $dst,$dst\t! replicate8I" %}
-  ins_encode %{
-    __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16I(legVecZ dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 16 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateI src));
-  format %{ "movd    $dst,$src\n\t"
-            "pshufd  $dst,$dst,0x00\n\t"
-            "vinserti128_high $dst,$dst\t"
-            "vinserti64x4 $dst,$dst,$dst,0x1\t! replicate16I" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16I_mem(legVecZ dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 16 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateI (LoadI mem)));
-  format %{ "pshufd  $dst,$mem,0x00\n\t"
-            "vinserti128_high $dst,$dst\t"
-            "vinserti64x4 $dst,$dst,$dst,0x1\t! replicate16I" %}
-  ins_encode %{
-    __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl4I_imm(vecX dst, immI con) %{
-  predicate(n->as_Vector()->length() == 4 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateI con));
-  format %{ "movq    $dst,[$constantaddress]\t! replicate4I($con)\n\t"
-            "punpcklqdq $dst,$dst" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 4)));
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8I_imm(vecY dst, immI con) %{
-  predicate(n->as_Vector()->length() == 8 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateI con));
-  format %{ "movq    $dst,[$constantaddress]\t! replicate8I($con)\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 4)));
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16I_imm(legVecZ dst, immI con) %{
-  predicate(n->as_Vector()->length() == 16 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateI con));
-  format %{ "movq    $dst,[$constantaddress]\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst"
-            "vinserti64x4 $dst,$dst,$dst,0x1\t! replicate16I($con)" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 4)));
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Long could be loaded into xmm register directly from memory.
-instruct Repl2L_mem(vecX dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 2 && !VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateL (LoadL mem)));
-  format %{ "movq    $dst,$mem\n\t"
-            "punpcklqdq $dst,$dst\t! replicate2L" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $mem$$Address);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Replicate long (8 byte) scalar to be vector
-#ifdef _LP64
-instruct Repl4L(vecY dst, rRegL src) %{
-  predicate(n->as_Vector()->length() == 4 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateL src));
-  format %{ "movdq   $dst,$src\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t! replicate4L" %}
-  ins_encode %{
-    __ movdq($dst$$XMMRegister, $src$$Register);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8L(legVecZ dst, rRegL src) %{
-  predicate(n->as_Vector()->length() == 8 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateL src));
-  format %{ "movdq   $dst,$src\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t"
-            "vinserti64x4 $dst,$dst,$dst,0x1\t! replicate8L" %}
-  ins_encode %{
-    __ movdq($dst$$XMMRegister, $src$$Register);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-#else // _LP64
-instruct Repl4L(vecY dst, eRegL src, vecY tmp) %{
-  predicate(n->as_Vector()->length() == 4 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateL src));
-  effect(TEMP dst, USE src, TEMP tmp);
-  format %{ "movdl   $dst,$src.lo\n\t"
-            "movdl   $tmp,$src.hi\n\t"
-            "punpckldq $dst,$tmp\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t! replicate4L" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
-    __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8L(legVecZ dst, eRegL src, legVecZ tmp) %{
-  predicate(n->as_Vector()->length() == 8 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateL src));
-  effect(TEMP dst, USE src, TEMP tmp);
-  format %{ "movdl   $dst,$src.lo\n\t"
-            "movdl   $tmp,$src.hi\n\t"
-            "punpckldq $dst,$tmp\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t"
-            "vinserti64x4 $dst,$dst,$dst,0x1\t! replicate8L" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
-    __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-#endif // _LP64
-
-instruct Repl4L_imm(vecY dst, immL con) %{
-  predicate(n->as_Vector()->length() == 4 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateL con));
-  format %{ "movq    $dst,[$constantaddress]\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t! replicate4L($con)" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress($con));
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8L_imm(legVecZ dst, immL con) %{
-  predicate(n->as_Vector()->length() == 8 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateL con));
-  format %{ "movq    $dst,[$constantaddress]\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t"
-            "vinserti64x4 $dst,$dst,$dst,0x1\t! replicate8L($con)" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress($con));
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl4L_mem(vecY dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 4 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateL (LoadL mem)));
-  format %{ "movq    $dst,$mem\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t! replicate4L" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $mem$$Address);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8L_mem(legVecZ dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 8 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateL (LoadL mem)));
-  format %{ "movq    $dst,$mem\n\t"
-            "punpcklqdq $dst,$dst\n\t"
-            "vinserti128_high $dst,$dst\t"
-            "vinserti64x4 $dst,$dst,$dst,0x1\t! replicate8L" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $mem$$Address);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl2F_mem(vecD dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 2 && UseAVX > 0 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateF (LoadF mem)));
-  format %{ "pshufd  $dst,$mem,0x00\t! replicate2F" %}
-  ins_encode %{
-    __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl4F_mem(vecX dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 0 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateF (LoadF mem)));
-  format %{ "pshufd  $dst,$mem,0x00\t! replicate4F" %}
-  ins_encode %{
-    __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8F(vecY dst, vlRegF src) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 0 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateF src));
-  format %{ "pshufd  $dst,$src,0x00\n\t"
-            "vinsertf128_high $dst,$dst\t! replicate8F" %}
-  ins_encode %{
-    __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
-    __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8F_mem(vecY dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 8 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateF (LoadF mem)));
-  format %{ "pshufd  $dst,$mem,0x00\n\t"
-            "vinsertf128_high $dst,$dst\t! replicate8F" %}
-  ins_encode %{
-    __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
-    __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16F(legVecZ dst, vlRegF src) %{
-  predicate(n->as_Vector()->length() == 16 && UseAVX > 0 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateF src));
-  format %{ "pshufd  $dst,$src,0x00\n\t"
-            "vinsertf128_high $dst,$dst\t"
-            "vinserti64x4 $dst,$dst,$dst,0x1\t! replicate16F" %}
-  ins_encode %{
-    __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
-    __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16F_mem(legVecZ dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 16 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateF (LoadF mem)));
-  format %{ "pshufd  $dst,$mem,0x00\n\t"
-            "vinsertf128_high $dst,$dst\t"
-            "vinserti64x4 $dst,$dst,$dst,0x1\t! replicate16F" %}
-  ins_encode %{
-    __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
-    __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl2F_zero(vecD dst, immF0 zero) %{
-  predicate(n->as_Vector()->length() == 2 && UseAVX < 3);
-  match(Set dst (ReplicateF zero));
-  format %{ "xorps   $dst,$dst\t! replicate2F zero" %}
-  ins_encode %{
-    __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl4F_zero(vecX dst, immF0 zero) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX < 3);
-  match(Set dst (ReplicateF zero));
-  format %{ "xorps   $dst,$dst\t! replicate4F zero" %}
-  ins_encode %{
-    __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl8F_zero(vecY dst, immF0 zero) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX < 3);
-  match(Set dst (ReplicateF zero));
-  format %{ "vxorps  $dst,$dst,$dst\t! replicate8F zero" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vxorps($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl2D_mem(vecX dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 2 && UseAVX > 0 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateD (LoadD mem)));
-  format %{ "pshufd  $dst,$mem,0x44\t! replicate2D" %}
-  ins_encode %{
-    __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl4D(vecY dst, vlRegD src) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 0 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateD src));
-  format %{ "pshufd  $dst,$src,0x44\n\t"
-            "vinsertf128_high $dst,$dst\t! replicate4D" %}
-  ins_encode %{
-    __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
-    __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl4D_mem(vecY dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 4 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateD (LoadD mem)));
-  format %{ "pshufd  $dst,$mem,0x44\n\t"
-            "vinsertf128_high $dst,$dst\t! replicate4D" %}
-  ins_encode %{
-    __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);
-    __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8D(legVecZ dst, vlRegD src) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 0 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateD src));
-  format %{ "pshufd  $dst,$src,0x44\n\t"
-            "vinsertf128_high $dst,$dst\t"
-            "vinserti64x4 $dst,$dst,$dst,0x1\t! replicate8D" %}
-  ins_encode %{
-    __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
-    __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8D_mem(legVecZ dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 8 && !VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateD (LoadD mem)));
-  format %{ "pshufd  $dst,$mem,0x44\n\t"
-            "vinsertf128_high $dst,$dst\t"
-            "vinserti64x4 $dst,$dst,$dst,0x1\t! replicate8D" %}
-  ins_encode %{
-    __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);
-    __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
-    __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Replicate double (8 byte) scalar zero to be vector
-instruct Repl2D_zero(vecX dst, immD0 zero) %{
-  predicate(n->as_Vector()->length() == 2 && UseAVX < 3);
-  match(Set dst (ReplicateD zero));
-  format %{ "xorpd   $dst,$dst\t! replicate2D zero" %}
-  ins_encode %{
-    __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl4D_zero(vecY dst, immD0 zero) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX < 3);
-  match(Set dst (ReplicateD zero));
-  format %{ "vxorpd  $dst,$dst,$dst,vect256\t! replicate4D zero" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vxorpd($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-// ====================GENERIC REPLICATE==========================================
-
-// Replicate byte scalar to be vector
-instruct Repl4B(vecS dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 4);
-  match(Set dst (ReplicateB src));
-  format %{ "movd    $dst,$src\n\t"
-            "punpcklbw $dst,$dst\n\t"
-            "pshuflw $dst,$dst,0x00\t! replicate4B" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8B(vecD dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 8);
-  match(Set dst (ReplicateB src));
-  format %{ "movd    $dst,$src\n\t"
-            "punpcklbw $dst,$dst\n\t"
-            "pshuflw $dst,$dst,0x00\t! replicate8B" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Replicate byte scalar immediate to be vector by loading from const table.
-instruct Repl4B_imm(vecS dst, immI con) %{
-  predicate(n->as_Vector()->length() == 4);
-  match(Set dst (ReplicateB con));
-  format %{ "movdl   $dst,[$constantaddress]\t! replicate4B($con)" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $constantaddress(replicate4_imm($con$$constant, 1)));
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8B_imm(vecD dst, immI con) %{
-  predicate(n->as_Vector()->length() == 8);
-  match(Set dst (ReplicateB con));
-  format %{ "movq    $dst,[$constantaddress]\t! replicate8B($con)" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 1)));
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Replicate byte scalar zero to be vector
-instruct Repl4B_zero(vecS dst, immI0 zero) %{
-  predicate(n->as_Vector()->length() == 4);
-  match(Set dst (ReplicateB zero));
-  format %{ "pxor    $dst,$dst\t! replicate4B zero" %}
-  ins_encode %{
-    __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl8B_zero(vecD dst, immI0 zero) %{
-  predicate(n->as_Vector()->length() == 8);
-  match(Set dst (ReplicateB zero));
-  format %{ "pxor    $dst,$dst\t! replicate8B zero" %}
-  ins_encode %{
-    __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl16B_zero(vecX dst, immI0 zero) %{
-  predicate(n->as_Vector()->length() == 16);
-  match(Set dst (ReplicateB zero));
-  format %{ "pxor    $dst,$dst\t! replicate16B zero" %}
-  ins_encode %{
-    __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl32B_zero(vecY dst, immI0 zero) %{
-  predicate(n->as_Vector()->length() == 32);
-  match(Set dst (ReplicateB zero));
-  format %{ "vpxor   $dst,$dst,$dst\t! replicate32B zero" %}
-  ins_encode %{
-    // Use vxorpd since AVX does not have vpxor for 256-bit (AVX2 will have it).
-    int vector_len = 1;
-    __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-// Replicate char/short (2 byte) scalar to be vector
-instruct Repl2S(vecS dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 2);
-  match(Set dst (ReplicateS src));
-  format %{ "movd    $dst,$src\n\t"
-            "pshuflw $dst,$dst,0x00\t! replicate2S" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-// Replicate char/short (2 byte) scalar immediate to be vector by loading from const table.
-instruct Repl2S_imm(vecS dst, immI con) %{
-  predicate(n->as_Vector()->length() == 2);
-  match(Set dst (ReplicateS con));
-  format %{ "movdl   $dst,[$constantaddress]\t! replicate2S($con)" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $constantaddress(replicate4_imm($con$$constant, 2)));
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl4S_imm(vecD dst, immI con) %{
-  predicate(n->as_Vector()->length() == 4);
-  match(Set dst (ReplicateS con));
-  format %{ "movq    $dst,[$constantaddress]\t! replicate4S($con)" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 2)));
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-// Replicate char/short (2 byte) scalar zero to be vector
-instruct Repl2S_zero(vecS dst, immI0 zero) %{
-  predicate(n->as_Vector()->length() == 2);
-  match(Set dst (ReplicateS zero));
-  format %{ "pxor    $dst,$dst\t! replicate2S zero" %}
-  ins_encode %{
-    __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl4S_zero(vecD dst, immI0 zero) %{
-  predicate(n->as_Vector()->length() == 4);
-  match(Set dst (ReplicateS zero));
-  format %{ "pxor    $dst,$dst\t! replicate4S zero" %}
-  ins_encode %{
-    __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl8S_zero(vecX dst, immI0 zero) %{
-  predicate(n->as_Vector()->length() == 8);
-  match(Set dst (ReplicateS zero));
-  format %{ "pxor    $dst,$dst\t! replicate8S zero" %}
-  ins_encode %{
-    __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl16S_zero(vecY dst, immI0 zero) %{
-  predicate(n->as_Vector()->length() == 16);
-  match(Set dst (ReplicateS zero));
-  format %{ "vpxor   $dst,$dst,$dst\t! replicate16S zero" %}
-  ins_encode %{
-    // Use vxorpd since AVX does not have vpxor for 256-bit (AVX2 will have it).
-    int vector_len = 1;
-    __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-// Replicate integer (4 byte) scalar to be vector
-instruct Repl2I(vecD dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 2);
-  match(Set dst (ReplicateI src));
-  format %{ "movd    $dst,$src\n\t"
-            "pshufd  $dst,$dst,0x00\t! replicate2I" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-// Integer could be loaded into xmm register directly from memory.
-instruct Repl2I_mem(vecD dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 2);
-  match(Set dst (ReplicateI (LoadI mem)));
-  format %{ "movd    $dst,$mem\n\t"
-            "pshufd  $dst,$dst,0x00\t! replicate2I" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $mem$$Address);
-    __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-// Replicate integer (4 byte) scalar immediate to be vector by loading from const table.
-instruct Repl2I_imm(vecD dst, immI con) %{
-  predicate(n->as_Vector()->length() == 2);
-  match(Set dst (ReplicateI con));
-  format %{ "movq    $dst,[$constantaddress]\t! replicate2I($con)" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 4)));
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-// Replicate integer (4 byte) scalar zero to be vector
-instruct Repl2I_zero(vecD dst, immI0 zero) %{
-  predicate(n->as_Vector()->length() == 2);
-  match(Set dst (ReplicateI zero));
-  format %{ "pxor    $dst,$dst\t! replicate2I" %}
-  ins_encode %{
-    __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl4I_zero(vecX dst, immI0 zero) %{
-  predicate(n->as_Vector()->length() == 4);
-  match(Set dst (ReplicateI zero));
-  format %{ "pxor    $dst,$dst\t! replicate4I zero)" %}
-  ins_encode %{
-    __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl8I_zero(vecY dst, immI0 zero) %{
-  predicate(n->as_Vector()->length() == 8);
-  match(Set dst (ReplicateI zero));
-  format %{ "vpxor   $dst,$dst,$dst\t! replicate8I zero" %}
-  ins_encode %{
-    // Use vxorpd since AVX does not have vpxor for 256-bit (AVX2 will have it).
-    int vector_len = 1;
-    __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-// Replicate long (8 byte) scalar to be vector
-#ifdef _LP64
-instruct Repl2L(vecX dst, rRegL src) %{
-  predicate(n->as_Vector()->length() == 2);
-  match(Set dst (ReplicateL src));
-  format %{ "movdq   $dst,$src\n\t"
-            "punpcklqdq $dst,$dst\t! replicate2L" %}
-  ins_encode %{
-    __ movdq($dst$$XMMRegister, $src$$Register);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-#else // _LP64
-instruct Repl2L(vecX dst, eRegL src, vecX tmp) %{
-  predicate(n->as_Vector()->length() == 2);
-  match(Set dst (ReplicateL src));
-  effect(TEMP dst, USE src, TEMP tmp);
-  format %{ "movdl   $dst,$src.lo\n\t"
-            "movdl   $tmp,$src.hi\n\t"
-            "punpckldq $dst,$tmp\n\t"
-            "punpcklqdq $dst,$dst\t! replicate2L"%}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
-    __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-#endif // _LP64
-
-// Replicate long (8 byte) scalar immediate to be vector by loading from const table.
-instruct Repl2L_imm(vecX dst, immL con) %{
-  predicate(n->as_Vector()->length() == 2);
-  match(Set dst (ReplicateL con));
-  format %{ "movq    $dst,[$constantaddress]\n\t"
-            "punpcklqdq $dst,$dst\t! replicate2L($con)" %}
-  ins_encode %{
-    __ movq($dst$$XMMRegister, $constantaddress($con));
-    __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Replicate long (8 byte) scalar zero to be vector
-instruct Repl2L_zero(vecX dst, immL0 zero) %{
-  predicate(n->as_Vector()->length() == 2);
-  match(Set dst (ReplicateL zero));
-  format %{ "pxor    $dst,$dst\t! replicate2L zero" %}
-  ins_encode %{
-    __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl4L_zero(vecY dst, immL0 zero) %{
-  predicate(n->as_Vector()->length() == 4);
-  match(Set dst (ReplicateL zero));
-  format %{ "vpxor   $dst,$dst,$dst\t! replicate4L zero" %}
-  ins_encode %{
-    // Use vxorpd since AVX does not have vpxor for 256-bit (AVX2 will have it).
-    int vector_len = 1;
-    __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-// Replicate float (4 byte) scalar to be vector
-instruct Repl2F(vecD dst, vlRegF src) %{
-  predicate(n->as_Vector()->length() == 2);
-  match(Set dst (ReplicateF src));
-  format %{ "pshufd  $dst,$dst,0x00\t! replicate2F" %}
-  ins_encode %{
-    __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl4F(vecX dst, vlRegF src) %{
-  predicate(n->as_Vector()->length() == 4);
-  match(Set dst (ReplicateF src));
-  format %{ "pshufd  $dst,$dst,0x00\t! replicate4F" %}
-  ins_encode %{
-    __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Replicate double (8 bytes) scalar to be vector
-instruct Repl2D(vecX dst, vlRegD src) %{
-  predicate(n->as_Vector()->length() == 2);
-  match(Set dst (ReplicateD src));
-  format %{ "pshufd  $dst,$src,0x44\t! replicate2D" %}
-  ins_encode %{
-    __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// ====================EVEX REPLICATE=============================================
-
-instruct Repl4B_mem_evex(vecS dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 2 && VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB (LoadB mem)));
-  format %{ "vpbroadcastb  $dst,$mem\t! replicate4B" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8B_mem_evex(vecD dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2 && VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB (LoadB mem)));
-  format %{ "vpbroadcastb  $dst,$mem\t! replicate8B" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16B_evex(vecX dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 16 && UseAVX > 2 && VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB src));
-  format %{ "evpbroadcastb $dst,$src\t! replicate16B" %}
-  ins_encode %{
-   int vector_len = 0;
-    __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16B_mem_evex(vecX dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 16 && UseAVX > 2 && VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB (LoadB mem)));
-  format %{ "vpbroadcastb  $dst,$mem\t! replicate16B" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl32B_evex(vecY dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 32 && UseAVX > 2 && VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB src));
-  format %{ "evpbroadcastb $dst,$src\t! replicate32B" %}
-  ins_encode %{
-   int vector_len = 1;
-    __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl32B_mem_evex(vecY dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 32 && UseAVX > 2 && VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB (LoadB mem)));
-  format %{ "vpbroadcastb  $dst,$mem\t! replicate32B" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl64B_evex(vecZ dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 64 && UseAVX > 2 && VM_Version::supports_avx512bw());
-  match(Set dst (ReplicateB src));
-  format %{ "evpbroadcastb $dst,$src\t! upper replicate64B" %}
-  ins_encode %{
-   int vector_len = 2;
-    __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl64B_mem_evex(vecZ dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 64 && UseAVX > 2 && VM_Version::supports_avx512bw());
-  match(Set dst (ReplicateB (LoadB mem)));
-  format %{ "vpbroadcastb  $dst,$mem\t! replicate64B" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16B_imm_evex(vecX dst, immI con) %{
-  predicate(n->as_Vector()->length() == 16 && UseAVX > 2 && VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB con));
-  format %{ "movq    $dst,[$constantaddress]\n\t"
-            "vpbroadcastb $dst,$dst\t! replicate16B" %}
-  ins_encode %{
-   int vector_len = 0;
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 1)));
-    __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl32B_imm_evex(vecY dst, immI con) %{
-  predicate(n->as_Vector()->length() == 32 && UseAVX > 2 && VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateB con));
-  format %{ "movq    $dst,[$constantaddress]\n\t"
-            "vpbroadcastb $dst,$dst\t! replicate32B" %}
-  ins_encode %{
-   int vector_len = 1;
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 1)));
-    __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl64B_imm_evex(vecZ dst, immI con) %{
-  predicate(n->as_Vector()->length() == 64 && UseAVX > 2 && VM_Version::supports_avx512bw());
-  match(Set dst (ReplicateB con));
-  format %{ "movq    $dst,[$constantaddress]\n\t"
-            "vpbroadcastb $dst,$dst\t! upper replicate64B" %}
-  ins_encode %{
-   int vector_len = 2;
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 1)));
-    __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl64B_zero_evex(vecZ dst, immI0 zero) %{
-  predicate(n->as_Vector()->length() == 64 && UseAVX > 2);
-  match(Set dst (ReplicateB zero));
-  format %{ "vpxor   $dst k0,$dst,$dst\t! replicate64B zero" %}
-  ins_encode %{
-    // Use vxorpd since AVX does not have vpxor for 512-bit (EVEX will have it).
-    int vector_len = 2;
-    __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl4S_evex(vecD dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 2 && VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS src));
-  format %{ "evpbroadcastw $dst,$src\t! replicate4S" %}
-  ins_encode %{
-   int vector_len = 0;
-    __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl4S_mem_evex(vecD dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 2 && VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS (LoadS mem)));
-  format %{ "vpbroadcastw  $dst,$mem\t! replicate4S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8S_evex(vecX dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2 && VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS src));
-  format %{ "evpbroadcastw $dst,$src\t! replicate8S" %}
-  ins_encode %{
-   int vector_len = 0;
-    __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8S_mem_evex(vecX dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2 && VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS (LoadS mem)));
-  format %{ "vpbroadcastw  $dst,$mem\t! replicate8S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16S_evex(vecY dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 16 && UseAVX > 2 && VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS src));
-  format %{ "evpbroadcastw $dst,$src\t! replicate16S" %}
-  ins_encode %{
-   int vector_len = 1;
-    __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16S_mem_evex(vecY dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 16 && UseAVX > 2 && VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS (LoadS mem)));
-  format %{ "vpbroadcastw  $dst,$mem\t! replicate16S" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl32S_evex(vecZ dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 32 && UseAVX > 2 && VM_Version::supports_avx512bw());
-  match(Set dst (ReplicateS src));
-  format %{ "evpbroadcastw $dst,$src\t! replicate32S" %}
-  ins_encode %{
-   int vector_len = 2;
-    __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl32S_mem_evex(vecZ dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 32 && UseAVX > 2 && VM_Version::supports_avx512bw());
-  match(Set dst (ReplicateS (LoadS mem)));
-  format %{ "vpbroadcastw  $dst,$mem\t! replicate32S" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8S_imm_evex(vecX dst, immI con) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2 && VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS con));
-  format %{ "movq    $dst,[$constantaddress]\n\t"
-            "vpbroadcastw $dst,$dst\t! replicate8S" %}
-  ins_encode %{
-   int vector_len = 0;
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 2)));
-    __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16S_imm_evex(vecY dst, immI con) %{
-  predicate(n->as_Vector()->length() == 16 && UseAVX > 2 && VM_Version::supports_avx512vlbw());
-  match(Set dst (ReplicateS con));
-  format %{ "movq    $dst,[$constantaddress]\n\t"
-            "vpbroadcastw $dst,$dst\t! replicate16S" %}
-  ins_encode %{
-   int vector_len = 1;
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 2)));
-    __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl32S_imm_evex(vecZ dst, immI con) %{
-  predicate(n->as_Vector()->length() == 32 && UseAVX > 2 && VM_Version::supports_avx512bw());
-  match(Set dst (ReplicateS con));
-  format %{ "movq    $dst,[$constantaddress]\n\t"
-            "vpbroadcastw $dst,$dst\t! replicate32S" %}
-  ins_encode %{
-   int vector_len = 2;
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 2)));
-    __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl32S_zero_evex(vecZ dst, immI0 zero) %{
-  predicate(n->as_Vector()->length() == 32 && UseAVX > 2);
-  match(Set dst (ReplicateS zero));
-  format %{ "vpxor   $dst k0,$dst,$dst\t! replicate32S zero" %}
-  ins_encode %{
-    // Use vxorpd since AVX does not have vpxor for 512-bit (EVEX will have it).
-    int vector_len = 2;
-    __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl4I_evex(vecX dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 2 && VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateI src));
-  format %{ "evpbroadcastd  $dst,$src\t! replicate4I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl4I_mem_evex(vecX dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 2 && VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateI (LoadI mem)));
-  format %{ "vpbroadcastd  $dst,$mem\t! replicate4I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8I_evex(vecY dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2 && VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateI src));
-  format %{ "evpbroadcastd  $dst,$src\t! replicate8I" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8I_mem_evex(vecY dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2 && VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateI (LoadI mem)));
-  format %{ "vpbroadcastd  $dst,$mem\t! replicate8I" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16I_evex(vecZ dst, rRegI src) %{
-  predicate(n->as_Vector()->length() == 16 && UseAVX > 2);
-  match(Set dst (ReplicateI src));
-  format %{ "evpbroadcastd  $dst,$src\t! replicate16I" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16I_mem_evex(vecZ dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 16 && UseAVX > 2);
-  match(Set dst (ReplicateI (LoadI mem)));
-  format %{ "vpbroadcastd  $dst,$mem\t! replicate16I" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl4I_imm_evex(vecX dst, immI con) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 2 && VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateI con));
-  format %{ "movq    $dst,[$constantaddress]\t! replicate8I($con)\n\t"
-            "vpbroadcastd  $dst,$dst\t! replicate4I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 4)));
-    __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8I_imm_evex(vecY dst, immI con) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2 && VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateI con));
-  format %{ "movq    $dst,[$constantaddress]\t! replicate8I($con)\n\t"
-            "vpbroadcastd  $dst,$dst\t! replicate8I" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 4)));
-    __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16I_imm_evex(vecZ dst, immI con) %{
-  predicate(n->as_Vector()->length() == 16 && UseAVX > 2);
-  match(Set dst (ReplicateI con));
-  format %{ "movq    $dst,[$constantaddress]\t! replicate16I($con)\n\t"
-            "vpbroadcastd  $dst,$dst\t! replicate16I" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 4)));
-    __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16I_zero_evex(vecZ dst, immI0 zero) %{
-  predicate(n->as_Vector()->length() == 16 && UseAVX > 2);
-  match(Set dst (ReplicateI zero));
-  format %{ "vpxor   $dst k0,$dst,$dst\t! replicate16I zero" %}
-  ins_encode %{
-    // Use vxorpd since AVX does not have vpxor for 512-bit (AVX2 will have it).
-    int vector_len = 2;
-    __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-// Replicate long (8 byte) scalar to be vector
-#ifdef _LP64
-instruct Repl4L_evex(vecY dst, rRegL src) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 2 && VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateL src));
-  format %{ "evpbroadcastq  $dst,$src\t! replicate4L" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ evpbroadcastq($dst$$XMMRegister, $src$$Register, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8L_evex(vecZ dst, rRegL src) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2);
-  match(Set dst (ReplicateL src));
-  format %{ "evpbroadcastq  $dst,$src\t! replicate8L" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ evpbroadcastq($dst$$XMMRegister, $src$$Register, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-#else // _LP64
-instruct Repl4L_evex(vecY dst, eRegL src, regD tmp) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 2 && VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateL src));
-  effect(TEMP dst, USE src, TEMP tmp);
-  format %{ "movdl   $dst,$src.lo\n\t"
-            "movdl   $tmp,$src.hi\n\t"
-            "punpckldq $dst,$tmp\n\t"
-            "vpbroadcastq  $dst,$dst\t! replicate4L" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
-    __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8L_evex(legVecZ dst, eRegL src, legVecZ tmp) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2);
-  match(Set dst (ReplicateL src));
-  effect(TEMP dst, USE src, TEMP tmp);
-  format %{ "movdl   $dst,$src.lo\n\t"
-            "movdl   $tmp,$src.hi\n\t"
-            "punpckldq $dst,$tmp\n\t"
-            "vpbroadcastq  $dst,$dst\t! replicate8L" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ movdl($dst$$XMMRegister, $src$$Register);
-    __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
-    __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-#endif // _LP64
-
-instruct Repl4L_imm_evex(vecY dst, immL con) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 2 && VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateL con));
-  format %{ "movq    $dst,[$constantaddress]\n\t"
-            "vpbroadcastq  $dst,$dst\t! replicate4L" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ movq($dst$$XMMRegister, $constantaddress($con));
-    __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8L_imm_evex(vecZ dst, immL con) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2);
-  match(Set dst (ReplicateL con));
-  format %{ "movq    $dst,[$constantaddress]\n\t"
-            "vpbroadcastq  $dst,$dst\t! replicate8L" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ movq($dst$$XMMRegister, $constantaddress($con));
-    __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl2L_mem_evex(vecX dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 2 && UseAVX > 2 && VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateL (LoadL mem)));
-  format %{ "vpbroadcastd  $dst,$mem\t! replicate2L" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl4L_mem_evex(vecY dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 2 && VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateL (LoadL mem)));
-  format %{ "vpbroadcastd  $dst,$mem\t! replicate4L" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8L_mem_evex(vecZ dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2);
-  match(Set dst (ReplicateL (LoadL mem)));
-  format %{ "vpbroadcastd  $dst,$mem\t! replicate8L" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8L_zero_evex(vecZ dst, immL0 zero) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2);
-  match(Set dst (ReplicateL zero));
-  format %{ "vpxor   $dst k0,$dst,$dst\t! replicate8L zero" %}
-  ins_encode %{
-    // Use vxorpd since AVX does not have vpxor for 512-bit (EVEX will have it).
-    int vector_len = 2;
-    __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl8F_evex(vecY dst, regF src) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2 && VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateF src));
-  format %{ "vpbroadcastss $dst,$src\t! replicate8F" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8F_mem_evex(vecY dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2 && VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateF (LoadF mem)));
-  format %{ "vbroadcastss  $dst,$mem\t! replicate8F" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16F_evex(vecZ dst, regF src) %{
-  predicate(n->as_Vector()->length() == 16 && UseAVX > 2);
-  match(Set dst (ReplicateF src));
-  format %{ "vpbroadcastss $dst,$src\t! replicate16F" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl16F_mem_evex(vecZ dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 16 && UseAVX > 2);
-  match(Set dst (ReplicateF (LoadF mem)));
-  format %{ "vbroadcastss  $dst,$mem\t! replicate16F" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl2F_zero_evex(vecD dst, immF0 zero) %{
-  predicate(n->as_Vector()->length() == 2 && UseAVX > 2);
-  match(Set dst (ReplicateF zero));
-  format %{ "vpxor  $dst k0,$dst,$dst\t! replicate2F zero" %}
-  ins_encode %{
-    // Use vpxor in place of vxorps since EVEX has a constriant on dq for vxorps: this is a 512-bit operation
-    int vector_len = 2;
-    __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl4F_zero_evex(vecX dst, immF0 zero) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 2);
-  match(Set dst (ReplicateF zero));
-  format %{ "vpxor  $dst k0,$dst,$dst\t! replicate4F zero" %}
-  ins_encode %{
-    // Use vpxor in place of vxorps since EVEX has a constriant on dq for vxorps: this is a 512-bit operation
-    int vector_len = 2;
-    __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl8F_zero_evex(vecY dst, immF0 zero) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2);
-  match(Set dst (ReplicateF zero));
-  format %{ "vpxor  $dst k0,$dst,$dst\t! replicate8F zero" %}
-  ins_encode %{
-    // Use vpxor in place of vxorps since EVEX has a constriant on dq for vxorps: this is a 512-bit operation
-    int vector_len = 2;
-    __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl16F_zero_evex(vecZ dst, immF0 zero) %{
-  predicate(n->as_Vector()->length() == 16 && UseAVX > 2);
-  match(Set dst (ReplicateF zero));
-  format %{ "vpxor  $dst k0,$dst,$dst\t! replicate16F zero" %}
-  ins_encode %{
-    // Use vpxor in place of vxorps since EVEX has a constriant on dq for vxorps: this is a 512-bit operation
-    int vector_len = 2;
-    __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl4D_evex(vecY dst, regD src) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 2 && VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateD src));
-  format %{ "vpbroadcastsd $dst,$src\t! replicate4D" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl4D_mem_evex(vecY dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 2 && VM_Version::supports_avx512vl());
-  match(Set dst (ReplicateD (LoadD mem)));
-  format %{ "vbroadcastsd  $dst,$mem\t! replicate4D" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8D_evex(vecZ dst, regD src) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2);
-  match(Set dst (ReplicateD src));
-  format %{ "vpbroadcastsd $dst,$src\t! replicate8D" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl8D_mem_evex(vecZ dst, memory mem) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2);
-  match(Set dst (ReplicateD (LoadD mem)));
-  format %{ "vbroadcastsd  $dst,$mem\t! replicate8D" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct Repl2D_zero_evex(vecX dst, immD0 zero) %{
-  predicate(n->as_Vector()->length() == 2 && UseAVX > 2);
-  match(Set dst (ReplicateD zero));
-  format %{ "vpxor  $dst k0,$dst,$dst\t! replicate2D zero" %}
-  ins_encode %{
-    // Use vpxor in place of vxorpd since EVEX has a constriant on dq for vxorpd: this is a 512-bit operation
-    int vector_len = 2;
-    __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl4D_zero_evex(vecY dst, immD0 zero) %{
-  predicate(n->as_Vector()->length() == 4 && UseAVX > 2);
-  match(Set dst (ReplicateD zero));
-  format %{ "vpxor  $dst k0,$dst,$dst\t! replicate4D zero" %}
-  ins_encode %{
-    // Use vpxor in place of vxorpd since EVEX has a constriant on dq for vxorpd: this is a 512-bit operation
-    int vector_len = 2;
-    __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-instruct Repl8D_zero_evex(vecZ dst, immD0 zero) %{
-  predicate(n->as_Vector()->length() == 8 && UseAVX > 2);
-  match(Set dst (ReplicateD zero));
-  format %{ "vpxor  $dst k0,$dst,$dst,vect512\t! replicate8D zero" %}
-  ins_encode %{
-    // Use vpxor in place of vxorpd since EVEX has a constriant on dq for vxorpd: this is a 512-bit operation
-    int vector_len = 2;
-    __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( fpu_reg_reg );
-%}
-
-// ====================REDUCTION ARITHMETIC=======================================
-
-instruct rsadd2I_reduction_reg(rRegI dst, rRegI src1, vecD src2, vecD tmp, vecD tmp2) %{
-  predicate(UseSSE > 2 && UseAVX == 0);
-  match(Set dst (AddReductionVI src1 src2));
-  effect(TEMP tmp2, TEMP tmp);
-  format %{ "movdqu  $tmp2,$src2\n\t"
-            "phaddd  $tmp2,$tmp2\n\t"
-            "movd    $tmp,$src1\n\t"
-            "paddd   $tmp,$tmp2\n\t"
-            "movd    $dst,$tmp\t! add reduction2I" %}
-  ins_encode %{
-    __ movdqu($tmp2$$XMMRegister, $src2$$XMMRegister);
-    __ phaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister);
-    __ movdl($tmp$$XMMRegister, $src1$$Register);
-    __ paddd($tmp$$XMMRegister, $tmp2$$XMMRegister);
-    __ movdl($dst$$Register, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvadd2I_reduction_reg(rRegI dst, rRegI src1, vecD src2, vecD tmp, vecD tmp2) %{
-  predicate(VM_Version::supports_avxonly());
-  match(Set dst (AddReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vphaddd  $tmp,$src2,$src2\n\t"
-            "movd     $tmp2,$src1\n\t"
-            "vpaddd   $tmp2,$tmp2,$tmp\n\t"
-            "movd     $dst,$tmp2\t! add reduction2I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);
-    __ movdl($tmp2$$XMMRegister, $src1$$Register);
-    __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, vector_len);
-    __ movdl($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvadd2I_reduction_reg_evex(rRegI dst, rRegI src1, vecD src2, vecD tmp, vecD tmp2) %{
-  predicate(UseAVX > 2);
-  match(Set dst (AddReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "pshufd  $tmp2,$src2,0x1\n\t"
-            "vpaddd  $tmp,$src2,$tmp2\n\t"
-            "movd    $tmp2,$src1\n\t"
-            "vpaddd  $tmp2,$tmp,$tmp2\n\t"
-            "movd    $dst,$tmp2\t! add reduction2I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
-    __ vpaddd($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ movdl($tmp2$$XMMRegister, $src1$$Register);
-    __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ movdl($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rsadd4I_reduction_reg(rRegI dst, rRegI src1, vecX src2, vecX tmp, vecX tmp2) %{
-  predicate(UseSSE > 2 && UseAVX == 0);
-  match(Set dst (AddReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "movdqu  $tmp,$src2\n\t"
-            "phaddd  $tmp,$tmp\n\t"
-            "phaddd  $tmp,$tmp\n\t"
-            "movd    $tmp2,$src1\n\t"
-            "paddd   $tmp2,$tmp\n\t"
-            "movd    $dst,$tmp2\t! add reduction4I" %}
-  ins_encode %{
-    __ movdqu($tmp$$XMMRegister, $src2$$XMMRegister);
-    __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);
-    __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);
-    __ movdl($tmp2$$XMMRegister, $src1$$Register);
-    __ paddd($tmp2$$XMMRegister, $tmp$$XMMRegister);
-    __ movdl($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvadd4I_reduction_reg(rRegI dst, rRegI src1, vecX src2, vecX tmp, vecX tmp2) %{
-  predicate(VM_Version::supports_avxonly());
-  match(Set dst (AddReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vphaddd  $tmp,$src2,$src2\n\t"
-            "vphaddd  $tmp,$tmp,$tmp\n\t"
-            "movd     $tmp2,$src1\n\t"
-            "vpaddd   $tmp2,$tmp2,$tmp\n\t"
-            "movd     $dst,$tmp2\t! add reduction4I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);
-    __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp$$XMMRegister, vector_len);
-    __ movdl($tmp2$$XMMRegister, $src1$$Register);
-    __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, vector_len);
-    __ movdl($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvadd4I_reduction_reg_evex(rRegI dst, rRegI src1, vecX src2, vecX tmp, vecX tmp2) %{
-  predicate(UseAVX > 2);
-  match(Set dst (AddReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "pshufd  $tmp2,$src2,0xE\n\t"
-            "vpaddd  $tmp,$src2,$tmp2\n\t"
-            "pshufd  $tmp2,$tmp,0x1\n\t"
-            "vpaddd  $tmp,$tmp,$tmp2\n\t"
-            "movd    $tmp2,$src1\n\t"
-            "vpaddd  $tmp2,$tmp,$tmp2\n\t"
-            "movd    $dst,$tmp2\t! add reduction4I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ vpaddd($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
-    __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ movdl($tmp2$$XMMRegister, $src1$$Register);
-    __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ movdl($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvadd8I_reduction_reg(rRegI dst, rRegI src1, vecY src2, vecY tmp, vecY tmp2) %{
-  predicate(VM_Version::supports_avxonly());
-  match(Set dst (AddReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vphaddd  $tmp,$src2,$src2\n\t"
-            "vphaddd  $tmp,$tmp,$tmp2\n\t"
-            "vextracti128_high  $tmp2,$tmp\n\t"
-            "vpaddd   $tmp,$tmp,$tmp2\n\t"
-            "movd     $tmp2,$src1\n\t"
-            "vpaddd   $tmp2,$tmp2,$tmp\n\t"
-            "movd     $dst,$tmp2\t! add reduction8I" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);
-    __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ vextracti128_high($tmp2$$XMMRegister, $tmp$$XMMRegister);
-    __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ movdl($tmp2$$XMMRegister, $src1$$Register);
-    __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ movdl($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvadd8I_reduction_reg_evex(rRegI dst, rRegI src1, vecY src2, vecY tmp, vecY tmp2) %{
-  predicate(UseAVX > 2);
-  match(Set dst (AddReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vextracti128_high  $tmp,$src2\n\t"
-            "vpaddd  $tmp,$tmp,$src2\n\t"
-            "pshufd  $tmp2,$tmp,0xE\n\t"
-            "vpaddd  $tmp,$tmp,$tmp2\n\t"
-            "pshufd  $tmp2,$tmp,0x1\n\t"
-            "vpaddd  $tmp,$tmp,$tmp2\n\t"
-            "movd    $tmp2,$src1\n\t"
-            "vpaddd  $tmp2,$tmp,$tmp2\n\t"
-            "movd    $dst,$tmp2\t! add reduction8I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
-    __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, vector_len);
-    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
-    __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
-    __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ movdl($tmp2$$XMMRegister, $src1$$Register);
-    __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ movdl($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvadd16I_reduction_reg_evex(rRegI dst, rRegI src1, legVecZ src2, legVecZ tmp, legVecZ tmp2, legVecZ tmp3) %{
-  predicate(UseAVX > 2);
-  match(Set dst (AddReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2, TEMP tmp3);
-  format %{ "vextracti64x4_high  $tmp3,$src2\n\t"
-            "vpaddd  $tmp3,$tmp3,$src2\n\t"
-            "vextracti128_high  $tmp,$tmp3\n\t"
-            "vpaddd  $tmp,$tmp,$tmp3\n\t"
-            "pshufd  $tmp2,$tmp,0xE\n\t"
-            "vpaddd  $tmp,$tmp,$tmp2\n\t"
-            "pshufd  $tmp2,$tmp,0x1\n\t"
-            "vpaddd  $tmp,$tmp,$tmp2\n\t"
-            "movd    $tmp2,$src1\n\t"
-            "vpaddd  $tmp2,$tmp,$tmp2\n\t"
-            "movd    $dst,$tmp2\t! mul reduction16I" %}
-  ins_encode %{
-    __ vextracti64x4_high($tmp3$$XMMRegister, $src2$$XMMRegister);
-    __ vpaddd($tmp3$$XMMRegister, $tmp3$$XMMRegister, $src2$$XMMRegister, 1);
-    __ vextracti128_high($tmp$$XMMRegister, $tmp3$$XMMRegister);
-    __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp3$$XMMRegister, 0);
-    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
-    __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
-    __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ movdl($tmp2$$XMMRegister, $src1$$Register);
-    __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ movdl($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-#ifdef _LP64
-instruct rvadd2L_reduction_reg(rRegL dst, rRegL src1, vecX src2, vecX tmp, vecX tmp2) %{
-  predicate(UseAVX > 2);
-  match(Set dst (AddReductionVL src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "pshufd  $tmp2,$src2,0xE\n\t"
-            "vpaddq  $tmp,$src2,$tmp2\n\t"
-            "movdq   $tmp2,$src1\n\t"
-            "vpaddq  $tmp2,$tmp,$tmp2\n\t"
-            "movdq   $dst,$tmp2\t! add reduction2L" %}
-  ins_encode %{
-    __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ vpaddq($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ movdq($tmp2$$XMMRegister, $src1$$Register);
-    __ vpaddq($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ movdq($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvadd4L_reduction_reg(rRegL dst, rRegL src1, vecY src2, vecY tmp, vecY tmp2) %{
-  predicate(UseAVX > 2);
-  match(Set dst (AddReductionVL src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vextracti128_high  $tmp,$src2\n\t"
-            "vpaddq  $tmp2,$tmp,$src2\n\t"
-            "pshufd  $tmp,$tmp2,0xE\n\t"
-            "vpaddq  $tmp2,$tmp2,$tmp\n\t"
-            "movdq   $tmp,$src1\n\t"
-            "vpaddq  $tmp2,$tmp2,$tmp\n\t"
-            "movdq   $dst,$tmp2\t! add reduction4L" %}
-  ins_encode %{
-    __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
-    __ vpaddq($tmp2$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, 0);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ movdq($tmp$$XMMRegister, $src1$$Register);
-    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ movdq($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvadd8L_reduction_reg(rRegL dst, rRegL src1, legVecZ src2, legVecZ tmp, legVecZ tmp2) %{
-  predicate(UseAVX > 2);
-  match(Set dst (AddReductionVL src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vextracti64x4_high  $tmp2,$src2\n\t"
-            "vpaddq  $tmp2,$tmp2,$src2\n\t"
-            "vextracti128_high  $tmp,$tmp2\n\t"
-            "vpaddq  $tmp2,$tmp2,$tmp\n\t"
-            "pshufd  $tmp,$tmp2,0xE\n\t"
-            "vpaddq  $tmp2,$tmp2,$tmp\n\t"
-            "movdq   $tmp,$src1\n\t"
-            "vpaddq  $tmp2,$tmp2,$tmp\n\t"
-            "movdq   $dst,$tmp2\t! add reduction8L" %}
-  ins_encode %{
-    __ vextracti64x4_high($tmp2$$XMMRegister, $src2$$XMMRegister);
-    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $src2$$XMMRegister, 1);
-    __ vextracti128_high($tmp$$XMMRegister, $tmp2$$XMMRegister);
-    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ movdq($tmp$$XMMRegister, $src1$$Register);
-    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ movdq($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-#endif
-
-instruct rsadd2F_reduction_reg(regF dst, vecD src2, vecD tmp) %{
-  predicate(UseSSE >= 1 && UseAVX == 0);
-  match(Set dst (AddReductionVF dst src2));
-  effect(TEMP dst, TEMP tmp);
-  format %{ "addss   $dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0x01\n\t"
-            "addss   $dst,$tmp\t! add reduction2F" %}
-  ins_encode %{
-    __ addss($dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-    __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvadd2F_reduction_reg(regF dst, vecD src2, vecD tmp) %{
-  predicate(UseAVX > 0);
-  match(Set dst (AddReductionVF dst src2));
-  effect(TEMP dst, TEMP tmp);
-  format %{ "vaddss  $dst,$dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0x01\n\t"
-            "vaddss  $dst,$dst,$tmp\t! add reduction2F" %}
-  ins_encode %{
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rsadd4F_reduction_reg(regF dst, vecX src2, vecX tmp) %{
-  predicate(UseSSE >= 1 && UseAVX == 0);
-  match(Set dst (AddReductionVF dst src2));
-  effect(TEMP dst, TEMP tmp);
-  format %{ "addss   $dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0x01\n\t"
-            "addss   $dst,$tmp\n\t"
-            "pshufd  $tmp,$src2,0x02\n\t"
-            "addss   $dst,$tmp\n\t"
-            "pshufd  $tmp,$src2,0x03\n\t"
-            "addss   $dst,$tmp\t! add reduction4F" %}
-  ins_encode %{
-    __ addss($dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-    __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
-    __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
-    __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvadd4F_reduction_reg(regF dst, vecX src2, vecX tmp) %{
-  predicate(UseAVX > 0);
-  match(Set dst (AddReductionVF dst src2));
-  effect(TEMP tmp, TEMP dst);
-  format %{ "vaddss  $dst,dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0x01\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$src2,0x02\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$src2,0x03\n\t"
-            "vaddss  $dst,$dst,$tmp\t! add reduction4F" %}
-  ins_encode %{
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct radd8F_reduction_reg(regF dst, vecY src2, vecY tmp, vecY tmp2) %{
-  predicate(UseAVX > 0);
-  match(Set dst (AddReductionVF dst src2));
-  effect(TEMP tmp, TEMP dst, TEMP tmp2);
-  format %{ "vaddss  $dst,$dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0x01\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$src2,0x02\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$src2,0x03\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "vextractf128_high  $tmp2,$src2\n\t"
-            "vaddss  $dst,$dst,$tmp2\n\t"
-            "pshufd  $tmp,$tmp2,0x01\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$tmp2,0x02\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$tmp2,0x03\n\t"
-            "vaddss  $dst,$dst,$tmp\t! add reduction8F" %}
-  ins_encode %{
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct radd16F_reduction_reg(regF dst, legVecZ src2, legVecZ tmp, legVecZ tmp2) %{
-  predicate(UseAVX > 2);
-  match(Set dst (AddReductionVF dst src2));
-  effect(TEMP tmp, TEMP dst, TEMP tmp2);
-  format %{ "vaddss  $dst,$dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0x01\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$src2,0x02\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$src2,0x03\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "vextractf32x4  $tmp2,$src2,0x1\n\t"
-            "vaddss  $dst,$dst,$tmp2\n\t"
-            "pshufd  $tmp,$tmp2,0x01\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$tmp2,0x02\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$tmp2,0x03\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "vextractf32x4  $tmp2,$src2,0x2\n\t"
-            "vaddss  $dst,$dst,$tmp2\n\t"
-            "pshufd  $tmp,$tmp2,0x01\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$tmp2,0x02\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$tmp2,0x03\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "vextractf32x4  $tmp2,$src2,0x3\n\t"
-            "vaddss  $dst,$dst,$tmp2\n\t"
-            "pshufd  $tmp,$tmp2,0x01\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$tmp2,0x02\n\t"
-            "vaddss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$tmp2,0x03\n\t"
-            "vaddss  $dst,$dst,$tmp\t! add reduction16F" %}
-  ins_encode %{
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
-    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rsadd2D_reduction_reg(regD dst, vecX src2, vecX tmp) %{
-  predicate(UseSSE >= 1 && UseAVX == 0);
-  match(Set dst (AddReductionVD dst src2));
-  effect(TEMP tmp, TEMP dst);
-  format %{ "addsd   $dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0xE\n\t"
-            "addsd   $dst,$tmp\t! add reduction2D" %}
-  ins_encode %{
-    __ addsd($dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ addsd($dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvadd2D_reduction_reg(regD dst, vecX src2, vecX tmp) %{
-  predicate(UseAVX > 0);
-  match(Set dst (AddReductionVD dst src2));
-  effect(TEMP tmp, TEMP dst);
-  format %{ "vaddsd  $dst,$dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0xE\n\t"
-            "vaddsd  $dst,$dst,$tmp\t! add reduction2D" %}
-  ins_encode %{
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvadd4D_reduction_reg(regD dst, vecY src2, vecX tmp, vecX tmp2) %{
-  predicate(UseAVX > 0);
-  match(Set dst (AddReductionVD dst src2));
-  effect(TEMP tmp, TEMP dst, TEMP tmp2);
-  format %{ "vaddsd  $dst,$dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0xE\n\t"
-            "vaddsd  $dst,$dst,$tmp\n\t"
-            "vextractf128  $tmp2,$src2,0x1\n\t"
-            "vaddsd  $dst,$dst,$tmp2\n\t"
-            "pshufd  $tmp,$tmp2,0xE\n\t"
-            "vaddsd  $dst,$dst,$tmp\t! add reduction4D" %}
-  ins_encode %{
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf128($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvadd8D_reduction_reg(regD dst, legVecZ src2, legVecZ tmp, legVecZ tmp2) %{
-  predicate(UseAVX > 2);
-  match(Set dst (AddReductionVD dst src2));
-  effect(TEMP tmp, TEMP dst, TEMP tmp2);
-  format %{ "vaddsd  $dst,$dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0xE\n\t"
-            "vaddsd  $dst,$dst,$tmp\n\t"
-            "vextractf32x4  $tmp2,$src2,0x1\n\t"
-            "vaddsd  $dst,$dst,$tmp2\n\t"
-            "pshufd  $tmp,$tmp2,0xE\n\t"
-            "vaddsd  $dst,$dst,$tmp\n\t"
-            "vextractf32x4  $tmp2,$src2,0x2\n\t"
-            "vaddsd  $dst,$dst,$tmp2\n\t"
-            "pshufd  $tmp,$tmp2,0xE\n\t"
-            "vaddsd  $dst,$dst,$tmp\n\t"
-            "vextractf32x4  $tmp2,$src2,0x3\n\t"
-            "vaddsd  $dst,$dst,$tmp2\n\t"
-            "pshufd  $tmp,$tmp2,0xE\n\t"
-            "vaddsd  $dst,$dst,$tmp\t! add reduction8D" %}
-  ins_encode %{
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rsmul2I_reduction_reg(rRegI dst, rRegI src1, vecD src2, vecD tmp, vecD tmp2) %{
-  predicate(UseSSE > 3 && UseAVX == 0);
-  match(Set dst (MulReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "pshufd  $tmp2,$src2,0x1\n\t"
-            "pmulld  $tmp2,$src2\n\t"
-            "movd    $tmp,$src1\n\t"
-            "pmulld  $tmp2,$tmp\n\t"
-            "movd    $dst,$tmp2\t! mul reduction2I" %}
-  ins_encode %{
-    __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
-    __ pmulld($tmp2$$XMMRegister, $src2$$XMMRegister);
-    __ movdl($tmp$$XMMRegister, $src1$$Register);
-    __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);
-    __ movdl($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvmul2I_reduction_reg(rRegI dst, rRegI src1, vecD src2, vecD tmp, vecD tmp2) %{
-  predicate(UseAVX > 0);
-  match(Set dst (MulReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "pshufd   $tmp2,$src2,0x1\n\t"
-            "vpmulld  $tmp,$src2,$tmp2\n\t"
-            "movd     $tmp2,$src1\n\t"
-            "vpmulld  $tmp2,$tmp,$tmp2\n\t"
-            "movd     $dst,$tmp2\t! mul reduction2I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
-    __ vpmulld($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ movdl($tmp2$$XMMRegister, $src1$$Register);
-    __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ movdl($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rsmul4I_reduction_reg(rRegI dst, rRegI src1, vecX src2, vecX tmp, vecX tmp2) %{
-  predicate(UseSSE > 3 && UseAVX == 0);
-  match(Set dst (MulReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "pshufd  $tmp2,$src2,0xE\n\t"
-            "pmulld  $tmp2,$src2\n\t"
-            "pshufd  $tmp,$tmp2,0x1\n\t"
-            "pmulld  $tmp2,$tmp\n\t"
-            "movd    $tmp,$src1\n\t"
-            "pmulld  $tmp2,$tmp\n\t"
-            "movd    $dst,$tmp2\t! mul reduction4I" %}
-  ins_encode %{
-    __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ pmulld($tmp2$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x1);
-    __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);
-    __ movdl($tmp$$XMMRegister, $src1$$Register);
-    __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);
-    __ movdl($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvmul4I_reduction_reg(rRegI dst, rRegI src1, vecX src2, vecX tmp, vecX tmp2) %{
-  predicate(UseAVX > 0);
-  match(Set dst (MulReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "pshufd   $tmp2,$src2,0xE\n\t"
-            "vpmulld  $tmp,$src2,$tmp2\n\t"
-            "pshufd   $tmp2,$tmp,0x1\n\t"
-            "vpmulld  $tmp,$tmp,$tmp2\n\t"
-            "movd     $tmp2,$src1\n\t"
-            "vpmulld  $tmp2,$tmp,$tmp2\n\t"
-            "movd     $dst,$tmp2\t! mul reduction4I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ vpmulld($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
-    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ movdl($tmp2$$XMMRegister, $src1$$Register);
-    __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ movdl($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvmul8I_reduction_reg(rRegI dst, rRegI src1, vecY src2, vecY tmp, vecY tmp2) %{
-  predicate(UseAVX > 1);
-  match(Set dst (MulReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vextracti128_high  $tmp,$src2\n\t"
-            "vpmulld  $tmp,$tmp,$src2\n\t"
-            "pshufd   $tmp2,$tmp,0xE\n\t"
-            "vpmulld  $tmp,$tmp,$tmp2\n\t"
-            "pshufd   $tmp2,$tmp,0x1\n\t"
-            "vpmulld  $tmp,$tmp,$tmp2\n\t"
-            "movd     $tmp2,$src1\n\t"
-            "vpmulld  $tmp2,$tmp,$tmp2\n\t"
-            "movd     $dst,$tmp2\t! mul reduction8I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
-    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, vector_len);
-    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
-    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
-    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ movdl($tmp2$$XMMRegister, $src1$$Register);
-    __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
-    __ movdl($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvmul16I_reduction_reg(rRegI dst, rRegI src1, legVecZ src2, legVecZ tmp, legVecZ tmp2, legVecZ tmp3) %{
-  predicate(UseAVX > 2);
-  match(Set dst (MulReductionVI src1 src2));
-  effect(TEMP tmp, TEMP tmp2, TEMP tmp3);
-  format %{ "vextracti64x4_high  $tmp3,$src2\n\t"
-            "vpmulld  $tmp3,$tmp3,$src2\n\t"
-            "vextracti128_high  $tmp,$tmp3\n\t"
-            "vpmulld  $tmp,$tmp,$src2\n\t"
-            "pshufd   $tmp2,$tmp,0xE\n\t"
-            "vpmulld  $tmp,$tmp,$tmp2\n\t"
-            "pshufd   $tmp2,$tmp,0x1\n\t"
-            "vpmulld  $tmp,$tmp,$tmp2\n\t"
-            "movd     $tmp2,$src1\n\t"
-            "vpmulld  $tmp2,$tmp,$tmp2\n\t"
-            "movd     $dst,$tmp2\t! mul reduction16I" %}
-  ins_encode %{
-    __ vextracti64x4_high($tmp3$$XMMRegister, $src2$$XMMRegister);
-    __ vpmulld($tmp3$$XMMRegister, $tmp3$$XMMRegister, $src2$$XMMRegister, 1);
-    __ vextracti128_high($tmp$$XMMRegister, $tmp3$$XMMRegister);
-    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp3$$XMMRegister, 0);
-    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
-    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
-    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ movdl($tmp2$$XMMRegister, $src1$$Register);
-    __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ movdl($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-#ifdef _LP64
-instruct rvmul2L_reduction_reg(rRegL dst, rRegL src1, vecX src2, vecX tmp, vecX tmp2) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512dq());
-  match(Set dst (MulReductionVL src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "pshufd   $tmp2,$src2,0xE\n\t"
-            "vpmullq  $tmp,$src2,$tmp2\n\t"
-            "movdq    $tmp2,$src1\n\t"
-            "vpmullq  $tmp2,$tmp,$tmp2\n\t"
-            "movdq    $dst,$tmp2\t! mul reduction2L" %}
-  ins_encode %{
-    __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ vpmullq($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ movdq($tmp2$$XMMRegister, $src1$$Register);
-    __ vpmullq($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
-    __ movdq($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvmul4L_reduction_reg(rRegL dst, rRegL src1, vecY src2, vecY tmp, vecY tmp2) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512dq());
-  match(Set dst (MulReductionVL src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vextracti128_high  $tmp,$src2\n\t"
-            "vpmullq  $tmp2,$tmp,$src2\n\t"
-            "pshufd   $tmp,$tmp2,0xE\n\t"
-            "vpmullq  $tmp2,$tmp2,$tmp\n\t"
-            "movdq    $tmp,$src1\n\t"
-            "vpmullq  $tmp2,$tmp2,$tmp\n\t"
-            "movdq    $dst,$tmp2\t! mul reduction4L" %}
-  ins_encode %{
-    __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
-    __ vpmullq($tmp2$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, 0);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ movdq($tmp$$XMMRegister, $src1$$Register);
-    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ movdq($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvmul8L_reduction_reg(rRegL dst, rRegL src1, legVecZ src2, legVecZ tmp, legVecZ tmp2) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512dq());
-  match(Set dst (MulReductionVL src1 src2));
-  effect(TEMP tmp, TEMP tmp2);
-  format %{ "vextracti64x4_high  $tmp2,$src2\n\t"
-            "vpmullq  $tmp2,$tmp2,$src2\n\t"
-            "vextracti128_high  $tmp,$tmp2\n\t"
-            "vpmullq  $tmp2,$tmp2,$tmp\n\t"
-            "pshufd   $tmp,$tmp2,0xE\n\t"
-            "vpmullq  $tmp2,$tmp2,$tmp\n\t"
-            "movdq    $tmp,$src1\n\t"
-            "vpmullq  $tmp2,$tmp2,$tmp\n\t"
-            "movdq    $dst,$tmp2\t! mul reduction8L" %}
-  ins_encode %{
-    __ vextracti64x4_high($tmp2$$XMMRegister, $src2$$XMMRegister);
-    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $src2$$XMMRegister, 1);
-    __ vextracti128_high($tmp$$XMMRegister, $tmp2$$XMMRegister);
-    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ movdq($tmp$$XMMRegister, $src1$$Register);
-    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
-    __ movdq($dst$$Register, $tmp2$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-#endif
-
-instruct rsmul2F_reduction(regF dst, vecD src2, vecD tmp) %{
-  predicate(UseSSE >= 1 && UseAVX == 0);
-  match(Set dst (MulReductionVF dst src2));
-  effect(TEMP dst, TEMP tmp);
-  format %{ "mulss   $dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0x01\n\t"
-            "mulss   $dst,$tmp\t! mul reduction2F" %}
-  ins_encode %{
-    __ mulss($dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-    __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvmul2F_reduction_reg(regF dst, vecD src2, vecD tmp) %{
-  predicate(UseAVX > 0);
-  match(Set dst (MulReductionVF dst src2));
-  effect(TEMP tmp, TEMP dst);
-  format %{ "vmulss  $dst,$dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0x01\n\t"
-            "vmulss  $dst,$dst,$tmp\t! mul reduction2F" %}
-  ins_encode %{
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rsmul4F_reduction_reg(regF dst, vecX src2, vecX tmp) %{
-  predicate(UseSSE >= 1 && UseAVX == 0);
-  match(Set dst (MulReductionVF dst src2));
-  effect(TEMP dst, TEMP tmp);
-  format %{ "mulss   $dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0x01\n\t"
-            "mulss   $dst,$tmp\n\t"
-            "pshufd  $tmp,$src2,0x02\n\t"
-            "mulss   $dst,$tmp\n\t"
-            "pshufd  $tmp,$src2,0x03\n\t"
-            "mulss   $dst,$tmp\t! mul reduction4F" %}
-  ins_encode %{
-    __ mulss($dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-    __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
-    __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
-    __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvmul4F_reduction_reg(regF dst, vecX src2, vecX tmp) %{
-  predicate(UseAVX > 0);
-  match(Set dst (MulReductionVF dst src2));
-  effect(TEMP tmp, TEMP dst);
-  format %{ "vmulss  $dst,$dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0x01\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$src2,0x02\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$src2,0x03\n\t"
-            "vmulss  $dst,$dst,$tmp\t! mul reduction4F" %}
-  ins_encode %{
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvmul8F_reduction_reg(regF dst, vecY src2, vecY tmp, vecY tmp2) %{
-  predicate(UseAVX > 0);
-  match(Set dst (MulReductionVF dst src2));
-  effect(TEMP tmp, TEMP dst, TEMP tmp2);
-  format %{ "vmulss  $dst,$dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0x01\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$src2,0x02\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$src2,0x03\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "vextractf128_high  $tmp2,$src2\n\t"
-            "vmulss  $dst,$dst,$tmp2\n\t"
-            "pshufd  $tmp,$tmp2,0x01\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$tmp2,0x02\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$tmp2,0x03\n\t"
-            "vmulss  $dst,$dst,$tmp\t! mul reduction8F" %}
-  ins_encode %{
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvmul16F_reduction_reg(regF dst, legVecZ src2, legVecZ tmp, legVecZ tmp2) %{
-  predicate(UseAVX > 2);
-  match(Set dst (MulReductionVF dst src2));
-  effect(TEMP tmp, TEMP dst, TEMP tmp2);
-  format %{ "vmulss  $dst,$dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0x01\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$src2,0x02\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$src2,0x03\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "vextractf32x4  $tmp2,$src2,0x1\n\t"
-            "vmulss  $dst,$dst,$tmp2\n\t"
-            "pshufd  $tmp,$tmp2,0x01\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$tmp2,0x02\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$tmp2,0x03\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "vextractf32x4  $tmp2,$src2,0x2\n\t"
-            "vmulss  $dst,$dst,$tmp2\n\t"
-            "pshufd  $tmp,$tmp2,0x01\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$tmp2,0x02\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$tmp2,0x03\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "vextractf32x4  $tmp2,$src2,0x3\n\t"
-            "vmulss  $dst,$dst,$tmp2\n\t"
-            "pshufd  $tmp,$tmp2,0x01\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$tmp2,0x02\n\t"
-            "vmulss  $dst,$dst,$tmp\n\t"
-            "pshufd  $tmp,$tmp2,0x03\n\t"
-            "vmulss  $dst,$dst,$tmp\t! mul reduction16F" %}
-  ins_encode %{
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
-    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rsmul2D_reduction_reg(regD dst, vecX src2, vecX tmp) %{
-  predicate(UseSSE >= 1 && UseAVX == 0);
-  match(Set dst (MulReductionVD dst src2));
-  effect(TEMP dst, TEMP tmp);
-  format %{ "mulsd   $dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0xE\n\t"
-            "mulsd   $dst,$tmp\t! mul reduction2D" %}
-  ins_encode %{
-    __ mulsd($dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ mulsd($dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvmul2D_reduction_reg(regD dst, vecX src2, vecX tmp) %{
-  predicate(UseAVX > 0);
-  match(Set dst (MulReductionVD dst src2));
-  effect(TEMP tmp, TEMP dst);
-  format %{ "vmulsd  $dst,$dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0xE\n\t"
-            "vmulsd  $dst,$dst,$tmp\t! mul reduction2D" %}
-  ins_encode %{
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvmul4D_reduction_reg(regD dst, vecY src2, vecY tmp, vecY tmp2) %{
-  predicate(UseAVX > 0);
-  match(Set dst (MulReductionVD dst src2));
-  effect(TEMP tmp, TEMP dst, TEMP tmp2);
-  format %{ "vmulsd  $dst,$dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0xE\n\t"
-            "vmulsd  $dst,$dst,$tmp\n\t"
-            "vextractf128_high  $tmp2,$src2\n\t"
-            "vmulsd  $dst,$dst,$tmp2\n\t"
-            "pshufd  $tmp,$tmp2,0xE\n\t"
-            "vmulsd  $dst,$dst,$tmp\t! mul reduction4D" %}
-  ins_encode %{
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct rvmul8D_reduction_reg(regD dst, legVecZ src2, legVecZ tmp, legVecZ tmp2) %{
-  predicate(UseAVX > 2);
-  match(Set dst (MulReductionVD dst src2));
-  effect(TEMP tmp, TEMP dst, TEMP tmp2);
-  format %{ "vmulsd  $dst,$dst,$src2\n\t"
-            "pshufd  $tmp,$src2,0xE\n\t"
-            "vmulsd  $dst,$dst,$tmp\n\t"
-            "vextractf32x4  $tmp2,$src2,0x1\n\t"
-            "vmulsd  $dst,$dst,$tmp2\n\t"
-            "pshufd  $tmp,$src2,0xE\n\t"
-            "vmulsd  $dst,$dst,$tmp\n\t"
-            "vextractf32x4  $tmp2,$src2,0x2\n\t"
-            "vmulsd  $dst,$dst,$tmp2\n\t"
-            "pshufd  $tmp,$tmp2,0xE\n\t"
-            "vmulsd  $dst,$dst,$tmp\n\t"
-            "vextractf32x4  $tmp2,$src2,0x3\n\t"
-            "vmulsd  $dst,$dst,$tmp2\n\t"
-            "pshufd  $tmp,$tmp2,0xE\n\t"
-            "vmulsd  $dst,$dst,$tmp\t! mul reduction8D" %}
-  ins_encode %{
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
-    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
-    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// ====================VECTOR ARITHMETIC=======================================
-
-// --------------------------------- ADD --------------------------------------
-
-// Bytes vector add
-instruct vadd4B(vecS dst, vecS src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (AddVB dst src));
-  format %{ "paddb   $dst,$src\t! add packed4B" %}
-  ins_encode %{
-    __ paddb($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4B_reg(vecS dst, vecS src1, vecS src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (AddVB src1 src2));
-  format %{ "vpaddb  $dst,$src1,$src2\t! add packed4B" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-
-instruct vadd4B_mem(vecS dst, vecS src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (AddVB src (LoadVector mem)));
-  format %{ "vpaddb  $dst,$src,$mem\t! add packed4B" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd8B(vecD dst, vecD src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 8);
-  match(Set dst (AddVB dst src));
-  format %{ "paddb   $dst,$src\t! add packed8B" %}
-  ins_encode %{
-    __ paddb($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd8B_reg(vecD dst, vecD src1, vecD src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (AddVB src1 src2));
-  format %{ "vpaddb  $dst,$src1,$src2\t! add packed8B" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-
-instruct vadd8B_mem(vecD dst, vecD src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (AddVB src (LoadVector mem)));
-  format %{ "vpaddb  $dst,$src,$mem\t! add packed8B" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd16B(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 16);
-  match(Set dst (AddVB dst src));
-  format %{ "paddb   $dst,$src\t! add packed16B" %}
-  ins_encode %{
-    __ paddb($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd16B_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0  && n->as_Vector()->length() == 16);
-  match(Set dst (AddVB src1 src2));
-  format %{ "vpaddb  $dst,$src1,$src2\t! add packed16B" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd16B_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 16);
-  match(Set dst (AddVB src (LoadVector mem)));
-  format %{ "vpaddb  $dst,$src,$mem\t! add packed16B" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd32B_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 32);
-  match(Set dst (AddVB src1 src2));
-  format %{ "vpaddb  $dst,$src1,$src2\t! add packed32B" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpaddb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd32B_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 32);
-  match(Set dst (AddVB src (LoadVector mem)));
-  format %{ "vpaddb  $dst,$src,$mem\t! add packed32B" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpaddb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd64B_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512bw() && n->as_Vector()->length() == 64);
-  match(Set dst (AddVB src1 src2));
-  format %{ "vpaddb  $dst,$src1,$src2\t! add packed64B" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpaddb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd64B_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512bw() && n->as_Vector()->length() == 64);
-  match(Set dst (AddVB src (LoadVector mem)));
-  format %{ "vpaddb  $dst,$src,$mem\t! add packed64B" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpaddb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Shorts/Chars vector add
-instruct vadd2S(vecS dst, vecS src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (AddVS dst src));
-  format %{ "paddw   $dst,$src\t! add packed2S" %}
-  ins_encode %{
-    __ paddw($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd2S_reg(vecS dst, vecS src1, vecS src2) %{
-  predicate(UseAVX > 0  && n->as_Vector()->length() == 2);
-  match(Set dst (AddVS src1 src2));
-  format %{ "vpaddw  $dst,$src1,$src2\t! add packed2S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd2S_mem(vecS dst, vecS src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (AddVS src (LoadVector mem)));
-  format %{ "vpaddw  $dst,$src,$mem\t! add packed2S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4S(vecD dst, vecD src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (AddVS dst src));
-  format %{ "paddw   $dst,$src\t! add packed4S" %}
-  ins_encode %{
-    __ paddw($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4S_reg(vecD dst, vecD src1, vecD src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (AddVS src1 src2));
-  format %{ "vpaddw  $dst,$src1,$src2\t! add packed4S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4S_mem(vecD dst, vecD src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (AddVS src (LoadVector mem)));
-  format %{ "vpaddw  $dst,$src,$mem\t! add packed4S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd8S(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 8);
-  match(Set dst (AddVS dst src));
-  format %{ "paddw   $dst,$src\t! add packed8S" %}
-  ins_encode %{
-    __ paddw($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd8S_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (AddVS src1 src2));
-  format %{ "vpaddw  $dst,$src1,$src2\t! add packed8S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd8S_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (AddVS src (LoadVector mem)));
-  format %{ "vpaddw  $dst,$src,$mem\t! add packed8S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd16S_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 16);
-  match(Set dst (AddVS src1 src2));
-  format %{ "vpaddw  $dst,$src1,$src2\t! add packed16S" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpaddw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd16S_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 16);
-  match(Set dst (AddVS src (LoadVector mem)));
-  format %{ "vpaddw  $dst,$src,$mem\t! add packed16S" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpaddw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd32S_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512bw() && n->as_Vector()->length() == 32);
-  match(Set dst (AddVS src1 src2));
-  format %{ "vpaddw  $dst,$src1,$src2\t! add packed32S" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpaddw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd32S_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512bw() && n->as_Vector()->length() == 32);
-  match(Set dst (AddVS src (LoadVector mem)));
-  format %{ "vpaddw  $dst,$src,$mem\t! add packed32S" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpaddw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Integers vector add
-instruct vadd2I(vecD dst, vecD src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (AddVI dst src));
-  format %{ "paddd   $dst,$src\t! add packed2I" %}
-  ins_encode %{
-    __ paddd($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd2I_reg(vecD dst, vecD src1, vecD src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (AddVI src1 src2));
-  format %{ "vpaddd  $dst,$src1,$src2\t! add packed2I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd2I_mem(vecD dst, vecD src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (AddVI src (LoadVector mem)));
-  format %{ "vpaddd  $dst,$src,$mem\t! add packed2I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4I(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (AddVI dst src));
-  format %{ "paddd   $dst,$src\t! add packed4I" %}
-  ins_encode %{
-    __ paddd($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4I_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (AddVI src1 src2));
-  format %{ "vpaddd  $dst,$src1,$src2\t! add packed4I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4I_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (AddVI src (LoadVector mem)));
-  format %{ "vpaddd  $dst,$src,$mem\t! add packed4I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd8I_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 8);
-  match(Set dst (AddVI src1 src2));
-  format %{ "vpaddd  $dst,$src1,$src2\t! add packed8I" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpaddd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd8I_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 8);
-  match(Set dst (AddVI src (LoadVector mem)));
-  format %{ "vpaddd  $dst,$src,$mem\t! add packed8I" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpaddd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd16I_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (AddVI src1 src2));
-  format %{ "vpaddd  $dst,$src1,$src2\t! add packed16I" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpaddd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd16I_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (AddVI src (LoadVector mem)));
-  format %{ "vpaddd  $dst,$src,$mem\t! add packed16I" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpaddd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Longs vector add
-instruct vadd2L(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (AddVL dst src));
-  format %{ "paddq   $dst,$src\t! add packed2L" %}
-  ins_encode %{
-    __ paddq($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd2L_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (AddVL src1 src2));
-  format %{ "vpaddq  $dst,$src1,$src2\t! add packed2L" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd2L_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (AddVL src (LoadVector mem)));
-  format %{ "vpaddq  $dst,$src,$mem\t! add packed2L" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpaddq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4L_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 4);
-  match(Set dst (AddVL src1 src2));
-  format %{ "vpaddq  $dst,$src1,$src2\t! add packed4L" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpaddq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4L_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 4);
-  match(Set dst (AddVL src (LoadVector mem)));
-  format %{ "vpaddq  $dst,$src,$mem\t! add packed4L" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpaddq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd8L_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (AddVL src1 src2));
-  format %{ "vpaddq  $dst,$src1,$src2\t! add packed8L" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpaddq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd8L_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (AddVL src (LoadVector mem)));
-  format %{ "vpaddq  $dst,$src,$mem\t! add packed8L" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpaddq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Floats vector add
-instruct vadd2F(vecD dst, vecD src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (AddVF dst src));
-  format %{ "addps   $dst,$src\t! add packed2F" %}
-  ins_encode %{
-    __ addps($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd2F_reg(vecD dst, vecD src1, vecD src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (AddVF src1 src2));
-  format %{ "vaddps  $dst,$src1,$src2\t! add packed2F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vaddps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd2F_mem(vecD dst, vecD src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (AddVF src (LoadVector mem)));
-  format %{ "vaddps  $dst,$src,$mem\t! add packed2F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vaddps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4F(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (AddVF dst src));
-  format %{ "addps   $dst,$src\t! add packed4F" %}
-  ins_encode %{
-    __ addps($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4F_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (AddVF src1 src2));
-  format %{ "vaddps  $dst,$src1,$src2\t! add packed4F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vaddps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4F_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (AddVF src (LoadVector mem)));
-  format %{ "vaddps  $dst,$src,$mem\t! add packed4F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vaddps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd8F_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (AddVF src1 src2));
-  format %{ "vaddps  $dst,$src1,$src2\t! add packed8F" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vaddps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd8F_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (AddVF src (LoadVector mem)));
-  format %{ "vaddps  $dst,$src,$mem\t! add packed8F" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vaddps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd16F_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (AddVF src1 src2));
-  format %{ "vaddps  $dst,$src1,$src2\t! add packed16F" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vaddps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd16F_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (AddVF src (LoadVector mem)));
-  format %{ "vaddps  $dst,$src,$mem\t! add packed16F" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vaddps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Doubles vector add
-instruct vadd2D(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (AddVD dst src));
-  format %{ "addpd   $dst,$src\t! add packed2D" %}
-  ins_encode %{
-    __ addpd($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd2D_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (AddVD src1 src2));
-  format %{ "vaddpd  $dst,$src1,$src2\t! add packed2D" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vaddpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd2D_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (AddVD src (LoadVector mem)));
-  format %{ "vaddpd  $dst,$src,$mem\t! add packed2D" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vaddpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4D_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (AddVD src1 src2));
-  format %{ "vaddpd  $dst,$src1,$src2\t! add packed4D" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vaddpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd4D_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (AddVD src (LoadVector mem)));
-  format %{ "vaddpd  $dst,$src,$mem\t! add packed4D" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vaddpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd8D_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (AddVD src1 src2));
-  format %{ "vaddpd  $dst,$src1,$src2\t! add packed8D" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vaddpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vadd8D_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (AddVD src (LoadVector mem)));
-  format %{ "vaddpd  $dst,$src,$mem\t! add packed8D" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vaddpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// --------------------------------- SUB --------------------------------------
-
-// Bytes vector sub
-instruct vsub4B(vecS dst, vecS src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SubVB dst src));
-  format %{ "psubb   $dst,$src\t! sub packed4B" %}
-  ins_encode %{
-    __ psubb($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub4B_reg(vecS dst, vecS src1, vecS src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SubVB src1 src2));
-  format %{ "vpsubb  $dst,$src1,$src2\t! sub packed4B" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub4B_mem(vecS dst, vecS src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SubVB src (LoadVector mem)));
-  format %{ "vpsubb  $dst,$src,$mem\t! sub packed4B" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub8B(vecD dst, vecD src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 8);
-  match(Set dst (SubVB dst src));
-  format %{ "psubb   $dst,$src\t! sub packed8B" %}
-  ins_encode %{
-    __ psubb($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub8B_reg(vecD dst, vecD src1, vecD src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (SubVB src1 src2));
-  format %{ "vpsubb  $dst,$src1,$src2\t! sub packed8B" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub8B_mem(vecD dst, vecD src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (SubVB src (LoadVector mem)));
-  format %{ "vpsubb  $dst,$src,$mem\t! sub packed8B" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub16B(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 16);
-  match(Set dst (SubVB dst src));
-  format %{ "psubb   $dst,$src\t! sub packed16B" %}
-  ins_encode %{
-    __ psubb($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub16B_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 16);
-  match(Set dst (SubVB src1 src2));
-  format %{ "vpsubb  $dst,$src1,$src2\t! sub packed16B" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub16B_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 16);
-  match(Set dst (SubVB src (LoadVector mem)));
-  format %{ "vpsubb  $dst,$src,$mem\t! sub packed16B" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub32B_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 32);
-  match(Set dst (SubVB src1 src2));
-  format %{ "vpsubb  $dst,$src1,$src2\t! sub packed32B" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpsubb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub32B_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 32);
-  match(Set dst (SubVB src (LoadVector mem)));
-  format %{ "vpsubb  $dst,$src,$mem\t! sub packed32B" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpsubb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub64B_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512bw() && n->as_Vector()->length() == 64);
-  match(Set dst (SubVB src1 src2));
-  format %{ "vpsubb  $dst,$src1,$src2\t! sub packed64B" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpsubb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub64B_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512bw() && n->as_Vector()->length() == 64);
-  match(Set dst (SubVB src (LoadVector mem)));
-  format %{ "vpsubb  $dst,$src,$mem\t! sub packed64B" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpsubb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Shorts/Chars vector sub
-instruct vsub2S(vecS dst, vecS src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SubVS dst src));
-  format %{ "psubw   $dst,$src\t! sub packed2S" %}
-  ins_encode %{
-    __ psubw($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub2S_reg(vecS dst, vecS src1, vecS src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SubVS src1 src2));
-  format %{ "vpsubw  $dst,$src1,$src2\t! sub packed2S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub2S_mem(vecS dst, vecS src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SubVS src (LoadVector mem)));
-  format %{ "vpsubw  $dst,$src,$mem\t! sub packed2S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub4S(vecD dst, vecD src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SubVS dst src));
-  format %{ "psubw   $dst,$src\t! sub packed4S" %}
-  ins_encode %{
-    __ psubw($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub4S_reg(vecD dst, vecD src1, vecD src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SubVS src1 src2));
-  format %{ "vpsubw  $dst,$src1,$src2\t! sub packed4S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub4S_mem(vecD dst, vecD src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SubVS src (LoadVector mem)));
-  format %{ "vpsubw  $dst,$src,$mem\t! sub packed4S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub8S(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 8);
-  match(Set dst (SubVS dst src));
-  format %{ "psubw   $dst,$src\t! sub packed8S" %}
-  ins_encode %{
-    __ psubw($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub8S_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (SubVS src1 src2));
-  format %{ "vpsubw  $dst,$src1,$src2\t! sub packed8S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub8S_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (SubVS src (LoadVector mem)));
-  format %{ "vpsubw  $dst,$src,$mem\t! sub packed8S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub16S_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 16);
-  match(Set dst (SubVS src1 src2));
-  format %{ "vpsubw  $dst,$src1,$src2\t! sub packed16S" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpsubw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub16S_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 16);
-  match(Set dst (SubVS src (LoadVector mem)));
-  format %{ "vpsubw  $dst,$src,$mem\t! sub packed16S" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpsubw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub32S_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512bw() && n->as_Vector()->length() == 32);
-  match(Set dst (SubVS src1 src2));
-  format %{ "vpsubw  $dst,$src1,$src2\t! sub packed32S" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpsubw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub32S_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512bw() && n->as_Vector()->length() == 32);
-  match(Set dst (SubVS src (LoadVector mem)));
-  format %{ "vpsubw  $dst,$src,$mem\t! sub packed32S" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpsubw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Integers vector sub
-instruct vsub2I(vecD dst, vecD src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SubVI dst src));
-  format %{ "psubd   $dst,$src\t! sub packed2I" %}
-  ins_encode %{
-    __ psubd($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub2I_reg(vecD dst, vecD src1, vecD src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SubVI src1 src2));
-  format %{ "vpsubd  $dst,$src1,$src2\t! sub packed2I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub2I_mem(vecD dst, vecD src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SubVI src (LoadVector mem)));
-  format %{ "vpsubd  $dst,$src,$mem\t! sub packed2I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub4I(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SubVI dst src));
-  format %{ "psubd   $dst,$src\t! sub packed4I" %}
-  ins_encode %{
-    __ psubd($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub4I_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SubVI src1 src2));
-  format %{ "vpsubd  $dst,$src1,$src2\t! sub packed4I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub4I_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SubVI src (LoadVector mem)));
-  format %{ "vpsubd  $dst,$src,$mem\t! sub packed4I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub8I_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 8);
-  match(Set dst (SubVI src1 src2));
-  format %{ "vpsubd  $dst,$src1,$src2\t! sub packed8I" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpsubd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub8I_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 8);
-  match(Set dst (SubVI src (LoadVector mem)));
-  format %{ "vpsubd  $dst,$src,$mem\t! sub packed8I" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpsubd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub16I_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (SubVI src1 src2));
-  format %{ "vpsubd  $dst,$src1,$src2\t! sub packed16I" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpsubd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub16I_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (SubVI src (LoadVector mem)));
-  format %{ "vpsubd  $dst,$src,$mem\t! sub packed16I" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpsubd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Longs vector sub
-instruct vsub2L(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SubVL dst src));
-  format %{ "psubq   $dst,$src\t! sub packed2L" %}
-  ins_encode %{
-    __ psubq($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub2L_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SubVL src1 src2));
-  format %{ "vpsubq  $dst,$src1,$src2\t! sub packed2L" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub2L_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SubVL src (LoadVector mem)));
-  format %{ "vpsubq  $dst,$src,$mem\t! sub packed2L" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsubq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub4L_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 4);
-  match(Set dst (SubVL src1 src2));
-  format %{ "vpsubq  $dst,$src1,$src2\t! sub packed4L" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpsubq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub4L_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 4);
-  match(Set dst (SubVL src (LoadVector mem)));
-  format %{ "vpsubq  $dst,$src,$mem\t! sub packed4L" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpsubq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub8L_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (SubVL src1 src2));
-  format %{ "vpsubq  $dst,$src1,$src2\t! sub packed8L" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpsubq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub8L_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (SubVL src (LoadVector mem)));
-  format %{ "vpsubq  $dst,$src,$mem\t! sub packed8L" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpsubq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Floats vector sub
-instruct vsub2F(vecD dst, vecD src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SubVF dst src));
-  format %{ "subps   $dst,$src\t! sub packed2F" %}
-  ins_encode %{
-    __ subps($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub2F_reg(vecD dst, vecD src1, vecD src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SubVF src1 src2));
-  format %{ "vsubps  $dst,$src1,$src2\t! sub packed2F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vsubps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub2F_mem(vecD dst, vecD src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SubVF src (LoadVector mem)));
-  format %{ "vsubps  $dst,$src,$mem\t! sub packed2F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vsubps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub4F(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SubVF dst src));
-  format %{ "subps   $dst,$src\t! sub packed4F" %}
-  ins_encode %{
-    __ subps($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub4F_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SubVF src1 src2));
-  format %{ "vsubps  $dst,$src1,$src2\t! sub packed4F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vsubps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub4F_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SubVF src (LoadVector mem)));
-  format %{ "vsubps  $dst,$src,$mem\t! sub packed4F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vsubps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub8F_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (SubVF src1 src2));
-  format %{ "vsubps  $dst,$src1,$src2\t! sub packed8F" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vsubps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub8F_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (SubVF src (LoadVector mem)));
-  format %{ "vsubps  $dst,$src,$mem\t! sub packed8F" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vsubps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub16F_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (SubVF src1 src2));
-  format %{ "vsubps  $dst,$src1,$src2\t! sub packed16F" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vsubps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub16F_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (SubVF src (LoadVector mem)));
-  format %{ "vsubps  $dst,$src,$mem\t! sub packed16F" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vsubps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Doubles vector sub
-instruct vsub2D(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SubVD dst src));
-  format %{ "subpd   $dst,$src\t! sub packed2D" %}
-  ins_encode %{
-    __ subpd($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub2D_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SubVD src1 src2));
-  format %{ "vsubpd  $dst,$src1,$src2\t! sub packed2D" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vsubpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub2D_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SubVD src (LoadVector mem)));
-  format %{ "vsubpd  $dst,$src,$mem\t! sub packed2D" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vsubpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub4D_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SubVD src1 src2));
-  format %{ "vsubpd  $dst,$src1,$src2\t! sub packed4D" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vsubpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub4D_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SubVD src (LoadVector mem)));
-  format %{ "vsubpd  $dst,$src,$mem\t! sub packed4D" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vsubpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub8D_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (SubVD src1 src2));
-  format %{ "vsubpd  $dst,$src1,$src2\t! sub packed8D" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vsubpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsub8D_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (SubVD src (LoadVector mem)));
-  format %{ "vsubpd  $dst,$src,$mem\t! sub packed8D" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vsubpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// --------------------------------- MUL --------------------------------------
-
-// Shorts/Chars vector mul
-instruct vmul2S(vecS dst, vecS src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (MulVS dst src));
-  format %{ "pmullw $dst,$src\t! mul packed2S" %}
-  ins_encode %{
-    __ pmullw($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul2S_reg(vecS dst, vecS src1, vecS src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (MulVS src1 src2));
-  format %{ "vpmullw $dst,$src1,$src2\t! mul packed2S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpmullw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul2S_mem(vecS dst, vecS src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (MulVS src (LoadVector mem)));
-  format %{ "vpmullw $dst,$src,$mem\t! mul packed2S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpmullw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul4S(vecD dst, vecD src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (MulVS dst src));
-  format %{ "pmullw  $dst,$src\t! mul packed4S" %}
-  ins_encode %{
-    __ pmullw($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul4S_reg(vecD dst, vecD src1, vecD src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (MulVS src1 src2));
-  format %{ "vpmullw $dst,$src1,$src2\t! mul packed4S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpmullw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul4S_mem(vecD dst, vecD src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (MulVS src (LoadVector mem)));
-  format %{ "vpmullw $dst,$src,$mem\t! mul packed4S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpmullw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul8S(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 8);
-  match(Set dst (MulVS dst src));
-  format %{ "pmullw  $dst,$src\t! mul packed8S" %}
-  ins_encode %{
-    __ pmullw($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul8S_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (MulVS src1 src2));
-  format %{ "vpmullw $dst,$src1,$src2\t! mul packed8S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpmullw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul8S_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (MulVS src (LoadVector mem)));
-  format %{ "vpmullw $dst,$src,$mem\t! mul packed8S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpmullw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul16S_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 16);
-  match(Set dst (MulVS src1 src2));
-  format %{ "vpmullw $dst,$src1,$src2\t! mul packed16S" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpmullw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul16S_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 16);
-  match(Set dst (MulVS src (LoadVector mem)));
-  format %{ "vpmullw $dst,$src,$mem\t! mul packed16S" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpmullw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul32S_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512bw() && n->as_Vector()->length() == 32);
-  match(Set dst (MulVS src1 src2));
-  format %{ "vpmullw $dst,$src1,$src2\t! mul packed32S" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpmullw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul32S_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512bw() && n->as_Vector()->length() == 32);
-  match(Set dst (MulVS src (LoadVector mem)));
-  format %{ "vpmullw $dst,$src,$mem\t! mul packed32S" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpmullw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Integers vector mul (sse4_1)
-instruct vmul2I(vecD dst, vecD src) %{
-  predicate(UseSSE > 3 && n->as_Vector()->length() == 2);
-  match(Set dst (MulVI dst src));
-  format %{ "pmulld  $dst,$src\t! mul packed2I" %}
-  ins_encode %{
-    __ pmulld($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul2I_reg(vecD dst, vecD src1, vecD src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (MulVI src1 src2));
-  format %{ "vpmulld $dst,$src1,$src2\t! mul packed2I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpmulld($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul2I_mem(vecD dst, vecD src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (MulVI src (LoadVector mem)));
-  format %{ "vpmulld $dst,$src,$mem\t! mul packed2I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpmulld($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul4I(vecX dst, vecX src) %{
-  predicate(UseSSE > 3 && n->as_Vector()->length() == 4);
-  match(Set dst (MulVI dst src));
-  format %{ "pmulld  $dst,$src\t! mul packed4I" %}
-  ins_encode %{
-    __ pmulld($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul4I_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (MulVI src1 src2));
-  format %{ "vpmulld $dst,$src1,$src2\t! mul packed4I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpmulld($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul4I_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (MulVI src (LoadVector mem)));
-  format %{ "vpmulld $dst,$src,$mem\t! mul packed4I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpmulld($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul2L_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 2 && VM_Version::supports_avx512dq());
-  match(Set dst (MulVL src1 src2));
-  format %{ "vpmullq $dst,$src1,$src2\t! mul packed2L" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpmullq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul2L_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 2 && VM_Version::supports_avx512dq());
-  match(Set dst (MulVL src (LoadVector mem)));
-  format %{ "vpmullq $dst,$src,$mem\t! mul packed2L" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpmullq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul4L_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 4 && VM_Version::supports_avx512dq());
-  match(Set dst (MulVL src1 src2));
-  format %{ "vpmullq $dst,$src1,$src2\t! mul packed4L" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpmullq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul4L_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 4 && VM_Version::supports_avx512dq());
-  match(Set dst (MulVL src (LoadVector mem)));
-  format %{ "vpmullq $dst,$src,$mem\t! mul packed4L" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpmullq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul8L_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8 && VM_Version::supports_avx512dq());
-  match(Set dst (MulVL src1 src2));
-  format %{ "vpmullq $dst,$src1,$src2\t! mul packed8L" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpmullq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul8L_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8 && VM_Version::supports_avx512dq());
-  match(Set dst (MulVL src (LoadVector mem)));
-  format %{ "vpmullq $dst,$src,$mem\t! mul packed8L" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpmullq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul8I_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 8);
-  match(Set dst (MulVI src1 src2));
-  format %{ "vpmulld $dst,$src1,$src2\t! mul packed8I" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpmulld($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul8I_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 8);
-  match(Set dst (MulVI src (LoadVector mem)));
-  format %{ "vpmulld $dst,$src,$mem\t! mul packed8I" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpmulld($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul16I_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (MulVI src1 src2));
-  format %{ "vpmulld $dst,$src1,$src2\t! mul packed16I" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpmulld($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul16I_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (MulVI src (LoadVector mem)));
-  format %{ "vpmulld $dst,$src,$mem\t! mul packed16I" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpmulld($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Floats vector mul
-instruct vmul2F(vecD dst, vecD src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (MulVF dst src));
-  format %{ "mulps   $dst,$src\t! mul packed2F" %}
-  ins_encode %{
-    __ mulps($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul2F_reg(vecD dst, vecD src1, vecD src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (MulVF src1 src2));
-  format %{ "vmulps  $dst,$src1,$src2\t! mul packed2F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vmulps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul2F_mem(vecD dst, vecD src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (MulVF src (LoadVector mem)));
-  format %{ "vmulps  $dst,$src,$mem\t! mul packed2F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vmulps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul4F(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (MulVF dst src));
-  format %{ "mulps   $dst,$src\t! mul packed4F" %}
-  ins_encode %{
-    __ mulps($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul4F_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (MulVF src1 src2));
-  format %{ "vmulps  $dst,$src1,$src2\t! mul packed4F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vmulps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul4F_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (MulVF src (LoadVector mem)));
-  format %{ "vmulps  $dst,$src,$mem\t! mul packed4F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vmulps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul8F_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (MulVF src1 src2));
-  format %{ "vmulps  $dst,$src1,$src2\t! mul packed8F" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vmulps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul8F_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (MulVF src (LoadVector mem)));
-  format %{ "vmulps  $dst,$src,$mem\t! mul packed8F" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vmulps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul16F_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (MulVF src1 src2));
-  format %{ "vmulps  $dst,$src1,$src2\t! mul packed16F" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vmulps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul16F_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (MulVF src (LoadVector mem)));
-  format %{ "vmulps  $dst,$src,$mem\t! mul packed16F" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vmulps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Doubles vector mul
-instruct vmul2D(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (MulVD dst src));
-  format %{ "mulpd   $dst,$src\t! mul packed2D" %}
-  ins_encode %{
-    __ mulpd($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul2D_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (MulVD src1 src2));
-  format %{ "vmulpd  $dst,$src1,$src2\t! mul packed2D" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vmulpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul2D_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (MulVD src (LoadVector mem)));
-  format %{ "vmulpd  $dst,$src,$mem\t! mul packed2D" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vmulpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul4D_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (MulVD src1 src2));
-  format %{ "vmulpd  $dst,$src1,$src2\t! mul packed4D" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vmulpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul4D_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (MulVD src (LoadVector mem)));
-  format %{ "vmulpd  $dst,$src,$mem\t! mul packed4D" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vmulpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul8D_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (MulVD src1 src2));
-  format %{ "vmulpd  $dst k0,$src1,$src2\t! mul packed8D" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vmulpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmul8D_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (MulVD src (LoadVector mem)));
-  format %{ "vmulpd  $dst k0,$src,$mem\t! mul packed8D" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vmulpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vcmov8F_reg(legVecY dst, legVecY src1, legVecY src2, immI8 cop, cmpOp_vcmppd copnd) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (CMoveVF (Binary copnd cop) (Binary src1 src2)));
-  effect(TEMP dst, USE src1, USE src2);
-  format %{ "cmpps.$copnd  $dst, $src1, $src2  ! vcmovevf, cond=$cop\n\t"
-            "blendvps $dst,$src1,$src2,$dst ! vcmovevf\n\t"
-         %}
-  ins_encode %{
-    int vector_len = 1;
-    int cond = (Assembler::Condition)($copnd$$cmpcode);
-    __ cmpps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, cond, vector_len);
-    __ blendvps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vcmov4D_reg(legVecY dst, legVecY src1, legVecY src2, immI8 cop, cmpOp_vcmppd copnd) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (CMoveVD (Binary copnd cop) (Binary src1 src2)));
-  effect(TEMP dst, USE src1, USE src2);
-  format %{ "cmppd.$copnd  $dst, $src1, $src2  ! vcmovevd, cond=$cop\n\t"
-            "blendvpd $dst,$src1,$src2,$dst ! vcmovevd\n\t"
-         %}
-  ins_encode %{
-    int vector_len = 1;
-    int cond = (Assembler::Condition)($copnd$$cmpcode);
-    __ cmppd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, cond, vector_len);
-    __ blendvpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $dst$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// --------------------------------- DIV --------------------------------------
-
-// Floats vector div
-instruct vdiv2F(vecD dst, vecD src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (DivVF dst src));
-  format %{ "divps   $dst,$src\t! div packed2F" %}
-  ins_encode %{
-    __ divps($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vdiv2F_reg(vecD dst, vecD src1, vecD src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (DivVF src1 src2));
-  format %{ "vdivps  $dst,$src1,$src2\t! div packed2F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vdivps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vdiv2F_mem(vecD dst, vecD src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (DivVF src (LoadVector mem)));
-  format %{ "vdivps  $dst,$src,$mem\t! div packed2F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vdivps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vdiv4F(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (DivVF dst src));
-  format %{ "divps   $dst,$src\t! div packed4F" %}
-  ins_encode %{
-    __ divps($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vdiv4F_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (DivVF src1 src2));
-  format %{ "vdivps  $dst,$src1,$src2\t! div packed4F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vdivps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vdiv4F_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (DivVF src (LoadVector mem)));
-  format %{ "vdivps  $dst,$src,$mem\t! div packed4F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vdivps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vdiv8F_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (DivVF src1 src2));
-  format %{ "vdivps  $dst,$src1,$src2\t! div packed8F" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vdivps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vdiv8F_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (DivVF src (LoadVector mem)));
-  format %{ "vdivps  $dst,$src,$mem\t! div packed8F" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vdivps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vdiv16F_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 16);
-  match(Set dst (DivVF src1 src2));
-  format %{ "vdivps  $dst,$src1,$src2\t! div packed16F" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vdivps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vdiv16F_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 16);
-  match(Set dst (DivVF src (LoadVector mem)));
-  format %{ "vdivps  $dst,$src,$mem\t! div packed16F" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vdivps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Doubles vector div
-instruct vdiv2D(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (DivVD dst src));
-  format %{ "divpd   $dst,$src\t! div packed2D" %}
-  ins_encode %{
-    __ divpd($dst$$XMMRegister, $src$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vdiv2D_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (DivVD src1 src2));
-  format %{ "vdivpd  $dst,$src1,$src2\t! div packed2D" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vdivpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vdiv2D_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (DivVD src (LoadVector mem)));
-  format %{ "vdivpd  $dst,$src,$mem\t! div packed2D" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vdivpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vdiv4D_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (DivVD src1 src2));
-  format %{ "vdivpd  $dst,$src1,$src2\t! div packed4D" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vdivpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vdiv4D_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (DivVD src (LoadVector mem)));
-  format %{ "vdivpd  $dst,$src,$mem\t! div packed4D" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vdivpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vdiv8D_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (DivVD src1 src2));
-  format %{ "vdivpd  $dst,$src1,$src2\t! div packed8D" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vdivpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vdiv8D_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (DivVD src (LoadVector mem)));
-  format %{ "vdivpd  $dst,$src,$mem\t! div packed8D" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vdivpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// ------------------------------ Shift ---------------------------------------
-
-// Left and right shift count vectors are the same on x86
-// (only lowest bits of xmm reg are used for count).
-instruct vshiftcnt(vecS dst, rRegI cnt) %{
-  match(Set dst (LShiftCntV cnt));
-  match(Set dst (RShiftCntV cnt));
-  format %{ "movd    $dst,$cnt\t! load shift count" %}
-  ins_encode %{
-    __ movdl($dst$$XMMRegister, $cnt$$Register);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// --------------------------------- Sqrt --------------------------------------
-
-// Floating point vector sqrt
-instruct vsqrt2D_reg(vecX dst, vecX src) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SqrtVD src));
-  format %{ "vsqrtpd  $dst,$src\t! sqrt packed2D" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vsqrtpd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsqrt2D_mem(vecX dst, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SqrtVD (LoadVector mem)));
-  format %{ "vsqrtpd  $dst,$mem\t! sqrt packed2D" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vsqrtpd($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsqrt4D_reg(vecY dst, vecY src) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SqrtVD src));
-  format %{ "vsqrtpd  $dst,$src\t! sqrt packed4D" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vsqrtpd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsqrt4D_mem(vecY dst, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SqrtVD (LoadVector mem)));
-  format %{ "vsqrtpd  $dst,$mem\t! sqrt packed4D" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vsqrtpd($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsqrt8D_reg(vecZ dst, vecZ src) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (SqrtVD src));
-  format %{ "vsqrtpd  $dst,$src\t! sqrt packed8D" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vsqrtpd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsqrt8D_mem(vecZ dst, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (SqrtVD (LoadVector mem)));
-  format %{ "vsqrtpd  $dst,$mem\t! sqrt packed8D" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vsqrtpd($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsqrt2F_reg(vecD dst, vecD src) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SqrtVF src));
-  format %{ "vsqrtps  $dst,$src\t! sqrt packed2F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vsqrtps($dst$$XMMRegister, $src$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsqrt2F_mem(vecD dst, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (SqrtVF (LoadVector mem)));
-  format %{ "vsqrtps  $dst,$mem\t! sqrt packed2F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vsqrtps($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsqrt4F_reg(vecX dst, vecX src) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SqrtVF src));
-  format %{ "vsqrtps  $dst,$src\t! sqrt packed4F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vsqrtps($dst$$XMMRegister, $src$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsqrt4F_mem(vecX dst, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (SqrtVF (LoadVector mem)));
-  format %{ "vsqrtps  $dst,$mem\t! sqrt packed4F" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vsqrtps($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsqrt8F_reg(vecY dst, vecY src) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (SqrtVF src));
-  format %{ "vsqrtps  $dst,$src\t! sqrt packed8F" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vsqrtps($dst$$XMMRegister, $src$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsqrt8F_mem(vecY dst, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (SqrtVF (LoadVector mem)));
-  format %{ "vsqrtps  $dst,$mem\t! sqrt packed8F" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vsqrtps($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsqrt16F_reg(vecZ dst, vecZ src) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (SqrtVF src));
-  format %{ "vsqrtps  $dst,$src\t! sqrt packed16F" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vsqrtps($dst$$XMMRegister, $src$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsqrt16F_mem(vecZ dst, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (SqrtVF (LoadVector mem)));
-  format %{ "vsqrtps  $dst,$mem\t! sqrt packed16F" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vsqrtps($dst$$XMMRegister, $mem$$Address, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// ------------------------------ LeftShift -----------------------------------
-
-// Shorts/Chars vector left shift
-instruct vsll2S(vecS dst, vecS shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (LShiftVS dst shift));
-  format %{ "psllw   $dst,$shift\t! left shift packed2S" %}
-  ins_encode %{
-    __ psllw($dst$$XMMRegister, $shift$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll2S_imm(vecS dst, immI8 shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (LShiftVS dst shift));
-  format %{ "psllw   $dst,$shift\t! left shift packed2S" %}
-  ins_encode %{
-    __ psllw($dst$$XMMRegister, (int)$shift$$constant);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll2S_reg(vecS dst, vecS src, vecS shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (LShiftVS src shift));
-  format %{ "vpsllw  $dst,$src,$shift\t! left shift packed2S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsllw($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll2S_reg_imm(vecS dst, vecS src, immI8 shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (LShiftVS src shift));
-  format %{ "vpsllw  $dst,$src,$shift\t! left shift packed2S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsllw($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll4S(vecD dst, vecS shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (LShiftVS dst shift));
-  format %{ "psllw   $dst,$shift\t! left shift packed4S" %}
-  ins_encode %{
-    __ psllw($dst$$XMMRegister, $shift$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll4S_imm(vecD dst, immI8 shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (LShiftVS dst shift));
-  format %{ "psllw   $dst,$shift\t! left shift packed4S" %}
-  ins_encode %{
-    __ psllw($dst$$XMMRegister, (int)$shift$$constant);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll4S_reg(vecD dst, vecD src, vecS shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (LShiftVS src shift));
-  format %{ "vpsllw  $dst,$src,$shift\t! left shift packed4S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsllw($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll4S_reg_imm(vecD dst, vecD src, immI8 shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (LShiftVS src shift));
-  format %{ "vpsllw  $dst,$src,$shift\t! left shift packed4S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsllw($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll8S(vecX dst, vecS shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 8);
-  match(Set dst (LShiftVS dst shift));
-  format %{ "psllw   $dst,$shift\t! left shift packed8S" %}
-  ins_encode %{
-    __ psllw($dst$$XMMRegister, $shift$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll8S_imm(vecX dst, immI8 shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 8);
-  match(Set dst (LShiftVS dst shift));
-  format %{ "psllw   $dst,$shift\t! left shift packed8S" %}
-  ins_encode %{
-    __ psllw($dst$$XMMRegister, (int)$shift$$constant);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll8S_reg(vecX dst, vecX src, vecS shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (LShiftVS src shift));
-  format %{ "vpsllw  $dst,$src,$shift\t! left shift packed8S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsllw($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll8S_reg_imm(vecX dst, vecX src, immI8 shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (LShiftVS src shift));
-  format %{ "vpsllw  $dst,$src,$shift\t! left shift packed8S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsllw($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll16S_reg(vecY dst, vecY src, vecS shift) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 16);
-  match(Set dst (LShiftVS src shift));
-  format %{ "vpsllw  $dst,$src,$shift\t! left shift packed16S" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpsllw($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll16S_reg_imm(vecY dst, vecY src, immI8 shift) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 16);
-  match(Set dst (LShiftVS src shift));
-  format %{ "vpsllw  $dst,$src,$shift\t! left shift packed16S" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpsllw($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll32S_reg(vecZ dst, vecZ src, vecS shift) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512bw() && n->as_Vector()->length() == 32);
-  match(Set dst (LShiftVS src shift));
-  format %{ "vpsllw  $dst,$src,$shift\t! left shift packed32S" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpsllw($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll32S_reg_imm(vecZ dst, vecZ src, immI8 shift) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512bw() && n->as_Vector()->length() == 32);
-  match(Set dst (LShiftVS src shift));
-  format %{ "vpsllw  $dst,$src,$shift\t! left shift packed32S" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpsllw($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-// Integers vector left shift
-instruct vsll2I(vecD dst, vecS shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (LShiftVI dst shift));
-  format %{ "pslld   $dst,$shift\t! left shift packed2I" %}
-  ins_encode %{
-    __ pslld($dst$$XMMRegister, $shift$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll2I_imm(vecD dst, immI8 shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (LShiftVI dst shift));
-  format %{ "pslld   $dst,$shift\t! left shift packed2I" %}
-  ins_encode %{
-    __ pslld($dst$$XMMRegister, (int)$shift$$constant);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll2I_reg(vecD dst, vecD src, vecS shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (LShiftVI src shift));
-  format %{ "vpslld  $dst,$src,$shift\t! left shift packed2I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpslld($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll2I_reg_imm(vecD dst, vecD src, immI8 shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (LShiftVI src shift));
-  format %{ "vpslld  $dst,$src,$shift\t! left shift packed2I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpslld($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll4I(vecX dst, vecS shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (LShiftVI dst shift));
-  format %{ "pslld   $dst,$shift\t! left shift packed4I" %}
-  ins_encode %{
-    __ pslld($dst$$XMMRegister, $shift$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll4I_imm(vecX dst, immI8 shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (LShiftVI dst shift));
-  format %{ "pslld   $dst,$shift\t! left shift packed4I" %}
-  ins_encode %{
-    __ pslld($dst$$XMMRegister, (int)$shift$$constant);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll4I_reg(vecX dst, vecX src, vecS shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (LShiftVI src shift));
-  format %{ "vpslld  $dst,$src,$shift\t! left shift packed4I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpslld($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll4I_reg_imm(vecX dst, vecX src, immI8 shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (LShiftVI src shift));
-  format %{ "vpslld  $dst,$src,$shift\t! left shift packed4I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpslld($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll8I_reg(vecY dst, vecY src, vecS shift) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 8);
-  match(Set dst (LShiftVI src shift));
-  format %{ "vpslld  $dst,$src,$shift\t! left shift packed8I" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpslld($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vsll8I_reg_imm(vecY dst, vecY src, immI8 shift) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 8);
-  match(Set dst (LShiftVI src shift));
-  format %{ "vpslld  $dst,$src,$shift\t! left shift packed8I" %}
+instruct vadd16I_reduction_reg(rRegI dst, rRegI src1, legVec src2, legVec tmp, legVec tmp2, legVec tmp3) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 16); // vector_length(src2) == 16
+  match(Set dst (AddReductionVI src1 src2));
+  effect(TEMP tmp, TEMP tmp2, TEMP tmp3);
+  format %{ "vector_add16I_reduction $dst,$src1,$src2" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vpslld($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    __ vextracti64x4_high($tmp3$$XMMRegister, $src2$$XMMRegister);
+    __ vpaddd($tmp3$$XMMRegister, $tmp3$$XMMRegister, $src2$$XMMRegister, 1);
+    __ vextracti128_high($tmp$$XMMRegister, $tmp3$$XMMRegister);
+    __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp3$$XMMRegister, 0);
+    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
+    __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
+    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
+    __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
+    __ movdl($tmp2$$XMMRegister, $src1$$Register);
+    __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
+    __ movdl($dst$$Register, $tmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsll16I_reg(vecZ dst, vecZ src, vecS shift) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (LShiftVI src shift));
-  format %{ "vpslld  $dst,$src,$shift\t! left shift packed16I" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpslld($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
+// =======================AddReductionVL==========================================
 
-instruct vsll16I_reg_imm(vecZ dst, vecZ src, immI8 shift) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (LShiftVI src shift));
-  format %{ "vpslld  $dst,$src,$shift\t! left shift packed16I" %}
+#ifdef _LP64
+instruct vadd2L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 2); // vector_length(src2) == 2
+  match(Set dst (AddReductionVL src1 src2));
+  effect(TEMP tmp, TEMP tmp2);
+  format %{ "vector_add2L_reduction $dst,$src1,$src2" %}
   ins_encode %{
-    int vector_len = 2;
-    __ vpslld($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    assert(UseAVX > 2, "required");
+    __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
+    __ vpaddq($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, 0);
+    __ movdq($tmp2$$XMMRegister, $src1$$Register);
+    __ vpaddq($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
+    __ movdq($dst$$Register, $tmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-// Longs vector left shift
-instruct vsll2L(vecX dst, vecS shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (LShiftVL dst shift));
-  format %{ "psllq   $dst,$shift\t! left shift packed2L" %}
+instruct vadd4L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 4); // vector_length(src2) == 4
+  match(Set dst (AddReductionVL src1 src2));
+  effect(TEMP tmp, TEMP tmp2);
+  format %{ "vector_add4L_reduction $dst,$src1,$src2" %}
   ins_encode %{
-    __ psllq($dst$$XMMRegister, $shift$$XMMRegister);
+    assert(UseAVX > 2, "required");
+    __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
+    __ vpaddq($tmp2$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, 0);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
+    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
+    __ movdq($tmp$$XMMRegister, $src1$$Register);
+    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
+    __ movdq($dst$$Register, $tmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsll2L_imm(vecX dst, immI8 shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (LShiftVL dst shift));
-  format %{ "psllq   $dst,$shift\t! left shift packed2L" %}
+instruct vadd8L_reduction_reg(rRegL dst, rRegL src1, legVec src2, legVec tmp, legVec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8); // vector_length(src2) == 8
+  match(Set dst (AddReductionVL src1 src2));
+  effect(TEMP tmp, TEMP tmp2);
+  format %{ "vector_addL_reduction $dst,$src1,$src2" %}
   ins_encode %{
-    __ psllq($dst$$XMMRegister, (int)$shift$$constant);
+    assert(UseAVX > 2, "required");
+    __ vextracti64x4_high($tmp2$$XMMRegister, $src2$$XMMRegister);
+    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $src2$$XMMRegister, 1);
+    __ vextracti128_high($tmp$$XMMRegister, $tmp2$$XMMRegister);
+    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
+    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
+    __ movdq($tmp$$XMMRegister, $src1$$Register);
+    __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
+    __ movdq($dst$$Register, $tmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
+#endif // _LP64
 
-instruct vsll2L_reg(vecX dst, vecX src, vecS shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (LShiftVL src shift));
-  format %{ "vpsllq  $dst,$src,$shift\t! left shift packed2L" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsllq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
+// =======================AddReductionVF==========================================
 
-instruct vsll2L_reg_imm(vecX dst, vecX src, immI8 shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (LShiftVL src shift));
-  format %{ "vpsllq  $dst,$src,$shift\t! left shift packed2L" %}
+instruct vadd2F_reduction_reg(regF dst, vec src2, vec tmp) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 2); // vector_length(src2) == 2
+  match(Set dst (AddReductionVF dst src2));
+  effect(TEMP dst, TEMP tmp);
+  format %{ "vector_add2F_reduction $dst,$dst,$src2" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsllq($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    if (UseAVX > 0) {
+      __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
+      __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    } else {
+      assert(UseSSE > 0, "required");
+      __ addss($dst$$XMMRegister, $src2$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
+      __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsll4L_reg(vecY dst, vecY src, vecS shift) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 4);
-  match(Set dst (LShiftVL src shift));
-  format %{ "vpsllq  $dst,$src,$shift\t! left shift packed4L" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpsllq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+instruct vadd4F_reduction_reg(regF dst, vec src2, vec tmp) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 4); // vector_length(src2) == 4
+  match(Set dst (AddReductionVF dst src2));
+  effect(TEMP dst, TEMP tmp);
+  format %{ "vector_add4F_reduction $dst,$dst,$src2" %}
+  ins_encode %{
+    if (UseAVX > 0) {
+      __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
+      __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
+      __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
+      __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    } else {
+      assert(UseSSE > 0, "required");
+      __ addss($dst$$XMMRegister, $src2$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
+      __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
+      __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
+      __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsll4L_reg_imm(vecY dst, vecY src, immI8 shift) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 4);
-  match(Set dst (LShiftVL src shift));
-  format %{ "vpsllq  $dst,$src,$shift\t! left shift packed4L" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpsllq($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
 
-instruct vsll8L_reg(vecZ dst, vecZ src, vecS shift) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (LShiftVL src shift));
-  format %{ "vpsllq  $dst,$src,$shift\t! left shift packed8L" %}
+instruct vadd8F_reduction_reg(regF dst, vec src2, vec tmp, vec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8); // vector_length(src2) == 8
+  match(Set dst (AddReductionVF dst src2));
+  effect(TEMP tmp, TEMP dst, TEMP tmp2);
+  format %{ "vector_add8F_reduction $dst,$dst,$src2" %}
   ins_encode %{
-    int vector_len = 2;
-    __ vpsllq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    assert(UseAVX > 0, "required");
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsll8L_reg_imm(vecZ dst, vecZ src, immI8 shift) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (LShiftVL src shift));
-  format %{ "vpsllq  $dst,$src,$shift\t! left shift packed8L" %}
+instruct vadd16F_reduction_reg(regF dst, legVec src2, legVec tmp, legVec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 16); // vector_length(src2) == 16
+  match(Set dst (AddReductionVF dst src2));
+  effect(TEMP tmp, TEMP dst, TEMP tmp2);
+  format %{ "vector_add16F_reduction $dst,$dst,$src2" %}
   ins_encode %{
-    int vector_len = 2;
-    __ vpsllq($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    assert(UseAVX > 2, "required");
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
+    __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-// ----------------------- LogicalRightShift -----------------------------------
-
-// Shorts vector logical right shift produces incorrect Java result
-// for negative data because java code convert short value into int with
-// sign extension before a shift. But char vectors are fine since chars are
-// unsigned values.
+// =======================AddReductionVD==========================================
 
-instruct vsrl2S(vecS dst, vecS shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (URShiftVS dst shift));
-  format %{ "psrlw   $dst,$shift\t! logical right shift packed2S" %}
+instruct vadd2D_reduction_reg(regD dst, vec src2, vec tmp) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 2); // vector_length(src2) == 2
+  match(Set dst (AddReductionVD dst src2));
+  effect(TEMP tmp, TEMP dst);
+  format %{ "vector_add2D_reduction  $dst,$src2" %}
   ins_encode %{
-    __ psrlw($dst$$XMMRegister, $shift$$XMMRegister);
+    if (UseAVX > 0) {
+      __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
+      __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    } else {
+      assert(UseSSE > 0, "required");
+      __ addsd($dst$$XMMRegister, $src2$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
+      __ addsd($dst$$XMMRegister, $tmp$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl2S_imm(vecS dst, immI8 shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (URShiftVS dst shift));
-  format %{ "psrlw   $dst,$shift\t! logical right shift packed2S" %}
+instruct vadd4D_reduction_reg(regD dst, vec src2, vec tmp, vec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 4); // vector_length(src2) == 4
+  match(Set dst (AddReductionVD dst src2));
+  effect(TEMP tmp, TEMP dst, TEMP tmp2);
+  format %{ "vector_add4D_reduction $dst,$dst,$src2" %}
   ins_encode %{
-    __ psrlw($dst$$XMMRegister, (int)$shift$$constant);
+    assert(UseAVX > 0, "required");
+    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
+    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ vextractf128($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
+    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
+    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl2S_reg(vecS dst, vecS src, vecS shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (URShiftVS src shift));
-  format %{ "vpsrlw  $dst,$src,$shift\t! logical right shift packed2S" %}
+instruct vadd8D_reduction_reg(regD dst, legVec src2, legVec tmp, legVec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8); // vector_length(src2) == 8
+  match(Set dst (AddReductionVD dst src2));
+  effect(TEMP tmp, TEMP dst, TEMP tmp2);
+  format %{ "vector_add8D_reduction $dst,$dst,$src2" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsrlw($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    assert(UseAVX > 2, "required");
+    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
+    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
+    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
+    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
+    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
+    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
+    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
+    __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl2S_reg_imm(vecS dst, vecS src, immI8 shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (URShiftVS src shift));
-  format %{ "vpsrlw  $dst,$src,$shift\t! logical right shift packed2S" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpsrlw($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
+// =======================MulReductionVI==========================================
 
-instruct vsrl4S(vecD dst, vecS shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (URShiftVS dst shift));
-  format %{ "psrlw   $dst,$shift\t! logical right shift packed4S" %}
-  ins_encode %{
-    __ psrlw($dst$$XMMRegister, $shift$$XMMRegister);
+instruct vmul2I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 2); // vector_length(src2) == 2
+  match(Set dst (MulReductionVI src1 src2));
+  effect(TEMP tmp, TEMP tmp2);
+  format %{ "vector_mul2I_reduction $dst,$src1,$src2" %}
+  ins_encode %{
+    if (UseAVX > 0) {
+      int vector_len = Assembler::AVX_128bit;
+      __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
+      __ vpmulld($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+      __ movdl($tmp2$$XMMRegister, $src1$$Register);
+      __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+      __ movdl($dst$$Register, $tmp2$$XMMRegister);
+    } else {
+      assert(UseSSE > 3, "required");
+      __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
+      __ pmulld($tmp2$$XMMRegister, $src2$$XMMRegister);
+      __ movdl($tmp$$XMMRegister, $src1$$Register);
+      __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);
+      __ movdl($dst$$Register, $tmp2$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl4S_imm(vecD dst, immI8 shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (URShiftVS dst shift));
-  format %{ "psrlw   $dst,$shift\t! logical right shift packed4S" %}
-  ins_encode %{
-    __ psrlw($dst$$XMMRegister, (int)$shift$$constant);
+instruct vmul4I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 4); // vector_length(src2) == 4
+  match(Set dst (MulReductionVI src1 src2));
+  effect(TEMP tmp, TEMP tmp2);
+  format %{ "vector_mul4I_reduction $dst,$src1,$src2" %}
+  ins_encode %{
+    if (UseAVX > 0) {
+      int vector_len = Assembler::AVX_128bit;
+      __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
+      __ vpmulld($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+      __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
+      __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+      __ movdl($tmp2$$XMMRegister, $src1$$Register);
+      __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+      __ movdl($dst$$Register, $tmp2$$XMMRegister);
+    } else {
+      assert(UseSSE > 3, "required");
+      __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
+      __ pmulld($tmp2$$XMMRegister, $src2$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x1);
+      __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);
+      __ movdl($tmp$$XMMRegister, $src1$$Register);
+      __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);
+      __ movdl($dst$$Register, $tmp2$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl4S_reg(vecD dst, vecD src, vecS shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (URShiftVS src shift));
-  format %{ "vpsrlw  $dst,$src,$shift\t! logical right shift packed4S" %}
+instruct vmul8I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8); // vector_length(src2) == 8
+  match(Set dst (MulReductionVI src1 src2));
+  effect(TEMP tmp, TEMP tmp2);
+  format %{ "vector_mul8I_reduction $dst,$src1,$src2" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsrlw($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    assert(UseAVX > 1, "required");
+    int vector_len = Assembler::AVX_128bit;
+    __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
+    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, vector_len);
+    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
+    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
+    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+    __ movdl($tmp2$$XMMRegister, $src1$$Register);
+    __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+    __ movdl($dst$$Register, $tmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl4S_reg_imm(vecD dst, vecD src, immI8 shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (URShiftVS src shift));
-  format %{ "vpsrlw  $dst,$src,$shift\t! logical right shift packed4S" %}
+instruct vmul16I_reduction_reg(rRegI dst, rRegI src1, legVec src2, legVec tmp, legVec tmp2, legVec tmp3) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 16); // vector_length(src2) == 16
+  match(Set dst (MulReductionVI src1 src2));
+  effect(TEMP tmp, TEMP tmp2, TEMP tmp3);
+  format %{ "vector_mul16I_reduction $dst,$src1,$src2" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsrlw($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    assert(UseAVX > 2, "required");
+    __ vextracti64x4_high($tmp3$$XMMRegister, $src2$$XMMRegister);
+    __ vpmulld($tmp3$$XMMRegister, $tmp3$$XMMRegister, $src2$$XMMRegister, 1);
+    __ vextracti128_high($tmp$$XMMRegister, $tmp3$$XMMRegister);
+    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp3$$XMMRegister, 0);
+    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
+    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
+    __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
+    __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
+    __ movdl($tmp2$$XMMRegister, $src1$$Register);
+    __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
+    __ movdl($dst$$Register, $tmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl8S(vecX dst, vecS shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 8);
-  match(Set dst (URShiftVS dst shift));
-  format %{ "psrlw   $dst,$shift\t! logical right shift packed8S" %}
+// =======================MulReductionVL==========================================
+
+#ifdef _LP64
+instruct vmul2L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 2); // vector_length(src2) == 2
+  match(Set dst (MulReductionVL src1 src2));
+  effect(TEMP tmp, TEMP tmp2);
+  format %{ "vector_mul2L_reduction $dst,$src1,$src2" %}
   ins_encode %{
-    __ psrlw($dst$$XMMRegister, $shift$$XMMRegister);
+    assert(VM_Version::supports_avx512dq(), "required");
+    __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
+    __ vpmullq($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, 0);
+    __ movdq($tmp2$$XMMRegister, $src1$$Register);
+    __ vpmullq($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
+    __ movdq($dst$$Register, $tmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl8S_imm(vecX dst, immI8 shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 8);
-  match(Set dst (URShiftVS dst shift));
-  format %{ "psrlw   $dst,$shift\t! logical right shift packed8S" %}
+instruct vmul4L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 4); // vector_length(src2) == 4
+  match(Set dst (MulReductionVL src1 src2));
+  effect(TEMP tmp, TEMP tmp2);
+  format %{ "vector_mul4L_reduction $dst,$src1,$src2" %}
   ins_encode %{
-    __ psrlw($dst$$XMMRegister, (int)$shift$$constant);
+    assert(VM_Version::supports_avx512dq(), "required");
+    __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
+    __ vpmullq($tmp2$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, 0);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
+    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
+    __ movdq($tmp$$XMMRegister, $src1$$Register);
+    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
+    __ movdq($dst$$Register, $tmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl8S_reg(vecX dst, vecX src, vecS shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (URShiftVS src shift));
-  format %{ "vpsrlw  $dst,$src,$shift\t! logical right shift packed8S" %}
+instruct vmul8L_reduction_reg(rRegL dst, rRegL src1, legVec src2, legVec tmp, legVec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8); // vector_length(src2) == 8
+  match(Set dst (MulReductionVL src1 src2));
+  effect(TEMP tmp, TEMP tmp2);
+  format %{ "vector_mul8L_reduction $dst,$src1,$src2" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsrlw($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    assert(VM_Version::supports_avx512dq(), "required");
+    __ vextracti64x4_high($tmp2$$XMMRegister, $src2$$XMMRegister);
+    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $src2$$XMMRegister, 1);
+    __ vextracti128_high($tmp$$XMMRegister, $tmp2$$XMMRegister);
+    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
+    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
+    __ movdq($tmp$$XMMRegister, $src1$$Register);
+    __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
+    __ movdq($dst$$Register, $tmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
+#endif
 
-instruct vsrl8S_reg_imm(vecX dst, vecX src, immI8 shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (URShiftVS src shift));
-  format %{ "vpsrlw  $dst,$src,$shift\t! logical right shift packed8S" %}
+// =======================MulReductionVF==========================================
+
+instruct vmul2F_reduction_reg(regF dst, vec src2, vec tmp) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 2); // vector_length(src2) == 2
+  match(Set dst (MulReductionVF dst src2));
+  effect(TEMP dst, TEMP tmp);
+  format %{ "vector_mul2F_reduction $dst,$dst,$src2" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsrlw($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    if (UseAVX > 0) {
+      __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
+      __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    } else {
+      assert(UseSSE > 0, "required");
+      __ mulss($dst$$XMMRegister, $src2$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
+      __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl16S_reg(vecY dst, vecY src, vecS shift) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 16);
-  match(Set dst (URShiftVS src shift));
-  format %{ "vpsrlw  $dst,$src,$shift\t! logical right shift packed16S" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpsrlw($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+instruct vmul4F_reduction_reg(regF dst, vec src2, vec tmp) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 4); // vector_length(src2) == 4
+  match(Set dst (MulReductionVF dst src2));
+  effect(TEMP dst, TEMP tmp);
+  format %{ "vector_mul4F_reduction $dst,$dst,$src2" %}
+  ins_encode %{
+    if (UseAVX > 0) {
+      __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
+      __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
+      __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
+      __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    } else {
+      assert(UseSSE > 0, "required");
+      __ mulss($dst$$XMMRegister, $src2$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
+      __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
+      __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
+      __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl16S_reg_imm(vecY dst, vecY src, immI8 shift) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 16);
-  match(Set dst (URShiftVS src shift));
-  format %{ "vpsrlw  $dst,$src,$shift\t! logical right shift packed16S" %}
+instruct vmul8F_reduction_reg(regF dst, vec src2, vec tmp, vec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8); // vector_length(src2) == 8
+  match(Set dst (MulReductionVF dst src2));
+  effect(TEMP tmp, TEMP dst, TEMP tmp2);
+  format %{ "vector_mul8F_reduction $dst,$dst,$src2" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vpsrlw($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    assert(UseAVX > 0, "required");
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl32S_reg(vecZ dst, vecZ src, vecS shift) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512bw() && n->as_Vector()->length() == 32);
-  match(Set dst (URShiftVS src shift));
-  format %{ "vpsrlw  $dst,$src,$shift\t! logical right shift packed32S" %}
+instruct vmul16F_reduction_reg(regF dst, legVec src2, legVec tmp, legVec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 16); // vector_length(src2) == 16
+  match(Set dst (MulReductionVF dst src2));
+  effect(TEMP tmp, TEMP dst, TEMP tmp2);
+  format %{ "vector_mul16F_reduction $dst,$dst,$src2" %}
   ins_encode %{
-    int vector_len = 2;
-    __ vpsrlw($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    assert(UseAVX > 2, "required");
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
+    __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl32S_reg_imm(vecZ dst, vecZ src, immI8 shift) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512bw() && n->as_Vector()->length() == 32);
-  match(Set dst (URShiftVS src shift));
-  format %{ "vpsrlw  $dst,$src,$shift\t! logical right shift packed32S" %}
+// =======================MulReductionVD==========================================
+
+instruct vmul2D_reduction_reg(regD dst, vec src2, vec tmp) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 2); // vector_length(src2) == 2
+  match(Set dst (MulReductionVD dst src2));
+  effect(TEMP dst, TEMP tmp);
+  format %{ "vector_mul2D_reduction $dst,$dst,$src2" %}
   ins_encode %{
-    int vector_len = 2;
-    __ vpsrlw($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    if (UseAVX > 0) {
+      __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
+      __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    } else {
+      assert(UseSSE > 0, "required");
+      __ mulsd($dst$$XMMRegister, $src2$$XMMRegister);
+      __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
+      __ mulsd($dst$$XMMRegister, $tmp$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-// Integers vector logical right shift
-instruct vsrl2I(vecD dst, vecS shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (URShiftVI dst shift));
-  format %{ "psrld   $dst,$shift\t! logical right shift packed2I" %}
+
+instruct vmul4D_reduction_reg(regD dst, vec src2, vec tmp, vec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 4); // vector_length(src2) == 2
+  match(Set dst (MulReductionVD dst src2));
+  effect(TEMP tmp, TEMP dst, TEMP tmp2);
+  format %{ "vector_mul4D_reduction  $dst,$dst,$src2" %}
   ins_encode %{
-    __ psrld($dst$$XMMRegister, $shift$$XMMRegister);
+    assert(UseAVX > 0, "required");
+    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
+    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);
+    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
+    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl2I_imm(vecD dst, immI8 shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (URShiftVI dst shift));
-  format %{ "psrld   $dst,$shift\t! logical right shift packed2I" %}
+instruct vmul8D_reduction_reg(regD dst, legVec src2, legVec tmp, legVec tmp2) %{
+  predicate(n->in(2)->bottom_type()->is_vect()->length() == 8); // vector_length(src2) == 2
+  match(Set dst (MulReductionVD dst src2));
+  effect(TEMP tmp, TEMP dst, TEMP tmp2);
+  format %{ "vector_mul8D_reduction $dst,$dst,$src2" %}
   ins_encode %{
-    __ psrld($dst$$XMMRegister, (int)$shift$$constant);
+    assert(UseAVX > 0, "required");
+    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
+    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
+    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
+    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
+    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
+    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
+    __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
+    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
+    __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
+    __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl2I_reg(vecD dst, vecD src, vecS shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (URShiftVI src shift));
-  format %{ "vpsrld  $dst,$src,$shift\t! logical right shift packed2I" %}
+// ====================VECTOR ARITHMETIC=======================================
+
+// --------------------------------- ADD --------------------------------------
+
+// Bytes vector add
+instruct vaddB(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (AddVB dst src));
+  format %{ "paddb   $dst,$src\t! add packedB" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsrld($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    __ paddb($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl2I_reg_imm(vecD dst, vecD src, immI8 shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (URShiftVI src shift));
-  format %{ "vpsrld  $dst,$src,$shift\t! logical right shift packed2I" %}
+instruct vaddB_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (AddVB src1 src2));
+  format %{ "vpaddb  $dst,$src1,$src2\t! add packedB" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsrld($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vpaddb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl4I(vecX dst, vecS shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (URShiftVI dst shift));
-  format %{ "psrld   $dst,$shift\t! logical right shift packed4I" %}
+instruct vaddB_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (AddVB src (LoadVector mem)));
+  format %{ "vpaddb  $dst,$src,$mem\t! add packedB" %}
   ins_encode %{
-    __ psrld($dst$$XMMRegister, $shift$$XMMRegister);
+    int vector_len = vector_length_encoding(this);
+    __ vpaddb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl4I_imm(vecX dst, immI8 shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (URShiftVI dst shift));
-  format %{ "psrld   $dst,$shift\t! logical right shift packed4I" %}
+// Shorts/Chars vector add
+instruct vaddS(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (AddVS dst src));
+  format %{ "paddw   $dst,$src\t! add packedS" %}
   ins_encode %{
-    __ psrld($dst$$XMMRegister, (int)$shift$$constant);
+    __ paddw($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl4I_reg(vecX dst, vecX src, vecS shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (URShiftVI src shift));
-  format %{ "vpsrld  $dst,$src,$shift\t! logical right shift packed4I" %}
+instruct vaddS_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (AddVS src1 src2));
+  format %{ "vpaddw  $dst,$src1,$src2\t! add packedS" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsrld($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vpaddw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl4I_reg_imm(vecX dst, vecX src, immI8 shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (URShiftVI src shift));
-  format %{ "vpsrld  $dst,$src,$shift\t! logical right shift packed4I" %}
+instruct vaddS_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (AddVS src (LoadVector mem)));
+  format %{ "vpaddw  $dst,$src,$mem\t! add packedS" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsrld($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vpaddw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl8I_reg(vecY dst, vecY src, vecS shift) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 8);
-  match(Set dst (URShiftVI src shift));
-  format %{ "vpsrld  $dst,$src,$shift\t! logical right shift packed8I" %}
+// Integers vector add
+instruct vaddI(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (AddVI dst src));
+  format %{ "paddd   $dst,$src\t! add packedI" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vpsrld($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    __ paddd($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl8I_reg_imm(vecY dst, vecY src, immI8 shift) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 8);
-  match(Set dst (URShiftVI src shift));
-  format %{ "vpsrld  $dst,$src,$shift\t! logical right shift packed8I" %}
+instruct vaddI_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (AddVI src1 src2));
+  format %{ "vpaddd  $dst,$src1,$src2\t! add packedI" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vpsrld($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vpaddd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl16I_reg(vecZ dst, vecZ src, vecS shift) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (URShiftVI src shift));
-  format %{ "vpsrld  $dst,$src,$shift\t! logical right shift packed16I" %}
+
+instruct vaddI_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (AddVI src (LoadVector mem)));
+  format %{ "vpaddd  $dst,$src,$mem\t! add packedI" %}
   ins_encode %{
-    int vector_len = 2;
-    __ vpsrld($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vpaddd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl16I_reg_imm(vecZ dst, vecZ src, immI8 shift) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (URShiftVI src shift));
-  format %{ "vpsrld  $dst,$src,$shift\t! logical right shift packed16I" %}
+// Longs vector add
+instruct vaddL(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (AddVL dst src));
+  format %{ "paddq   $dst,$src\t! add packedL" %}
   ins_encode %{
-    int vector_len = 2;
-    __ vpsrld($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    __ paddq($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-// Longs vector logical right shift
-instruct vsrl2L(vecX dst, vecS shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (URShiftVL dst shift));
-  format %{ "psrlq   $dst,$shift\t! logical right shift packed2L" %}
+instruct vaddL_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (AddVL src1 src2));
+  format %{ "vpaddq  $dst,$src1,$src2\t! add packedL" %}
   ins_encode %{
-    __ psrlq($dst$$XMMRegister, $shift$$XMMRegister);
+    int vector_len = vector_length_encoding(this);
+    __ vpaddq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl2L_imm(vecX dst, immI8 shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (URShiftVL dst shift));
-  format %{ "psrlq   $dst,$shift\t! logical right shift packed2L" %}
+instruct vaddL_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (AddVL src (LoadVector mem)));
+  format %{ "vpaddq  $dst,$src,$mem\t! add packedL" %}
   ins_encode %{
-    __ psrlq($dst$$XMMRegister, (int)$shift$$constant);
+    int vector_len = vector_length_encoding(this);
+    __ vpaddq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl2L_reg(vecX dst, vecX src, vecS shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (URShiftVL src shift));
-  format %{ "vpsrlq  $dst,$src,$shift\t! logical right shift packed2L" %}
+// Floats vector add
+instruct vaddF(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (AddVF dst src));
+  format %{ "addps   $dst,$src\t! add packedF" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsrlq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    __ addps($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl2L_reg_imm(vecX dst, vecX src, immI8 shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (URShiftVL src shift));
-  format %{ "vpsrlq  $dst,$src,$shift\t! logical right shift packed2L" %}
+instruct vaddF_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (AddVF src1 src2));
+  format %{ "vaddps  $dst,$src1,$src2\t! add packedF" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsrlq($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vaddps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl4L_reg(vecY dst, vecY src, vecS shift) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 4);
-  match(Set dst (URShiftVL src shift));
-  format %{ "vpsrlq  $dst,$src,$shift\t! logical right shift packed4L" %}
+instruct vaddF_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (AddVF src (LoadVector mem)));
+  format %{ "vaddps  $dst,$src,$mem\t! add packedF" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vpsrlq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vaddps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl4L_reg_imm(vecY dst, vecY src, immI8 shift) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 4);
-  match(Set dst (URShiftVL src shift));
-  format %{ "vpsrlq  $dst,$src,$shift\t! logical right shift packed4L" %}
+// Doubles vector add
+instruct vaddD(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (AddVD dst src));
+  format %{ "addpd   $dst,$src\t! add packedD" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vpsrlq($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    __ addpd($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl8L_reg(vecZ dst, vecZ src, vecS shift) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (URShiftVL src shift));
-  format %{ "vpsrlq  $dst,$src,$shift\t! logical right shift packed8L" %}
+instruct vaddD_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (AddVD src1 src2));
+  format %{ "vaddpd  $dst,$src1,$src2\t! add packedD" %}
   ins_encode %{
-    int vector_len = 2;
-    __ vpsrlq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vaddpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsrl8L_reg_imm(vecZ dst, vecZ src, immI8 shift) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 8);
-  match(Set dst (URShiftVL src shift));
-  format %{ "vpsrlq  $dst,$src,$shift\t! logical right shift packed8L" %}
+instruct vaddD_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (AddVD src (LoadVector mem)));
+  format %{ "vaddpd  $dst,$src,$mem\t! add packedD" %}
   ins_encode %{
-    int vector_len = 2;
-    __ vpsrlq($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vaddpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-// ------------------- ArithmeticRightShift -----------------------------------
+// --------------------------------- SUB --------------------------------------
 
-// Shorts/Chars vector arithmetic right shift
-instruct vsra2S(vecS dst, vecS shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (RShiftVS dst shift));
-  format %{ "psraw   $dst,$shift\t! arithmetic right shift packed2S" %}
+// Bytes vector sub
+instruct vsubB(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (SubVB dst src));
+  format %{ "psubb   $dst,$src\t! sub packedB" %}
   ins_encode %{
-    __ psraw($dst$$XMMRegister, $shift$$XMMRegister);
+    __ psubb($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra2S_imm(vecS dst, immI8 shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (RShiftVS dst shift));
-  format %{ "psraw   $dst,$shift\t! arithmetic right shift packed2S" %}
+instruct vsubB_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (SubVB src1 src2));
+  format %{ "vpsubb  $dst,$src1,$src2\t! sub packedB" %}
   ins_encode %{
-    __ psraw($dst$$XMMRegister, (int)$shift$$constant);
+    int vector_len = vector_length_encoding(this);
+    __ vpsubb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra2S_reg(vecS dst, vecS src, vecS shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (RShiftVS src shift));
-  format %{ "vpsraw  $dst,$src,$shift\t! arithmetic right shift packed2S" %}
+instruct vsubB_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (SubVB src (LoadVector mem)));
+  format %{ "vpsubb  $dst,$src,$mem\t! sub packedB" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsraw($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vpsubb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra2S_reg_imm(vecS dst, vecS src, immI8 shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (RShiftVS src shift));
-  format %{ "vpsraw  $dst,$src,$shift\t! arithmetic right shift packed2S" %}
+// Shorts/Chars vector sub
+instruct vsubS(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (SubVS dst src));
+  format %{ "psubw   $dst,$src\t! sub packedS" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsraw($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    __ psubw($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra4S(vecD dst, vecS shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (RShiftVS dst shift));
-  format %{ "psraw   $dst,$shift\t! arithmetic right shift packed4S" %}
+
+instruct vsubS_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (SubVS src1 src2));
+  format %{ "vpsubw  $dst,$src1,$src2\t! sub packedS" %}
   ins_encode %{
-    __ psraw($dst$$XMMRegister, $shift$$XMMRegister);
+    int vector_len = vector_length_encoding(this);
+    __ vpsubw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra4S_imm(vecD dst, immI8 shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (RShiftVS dst shift));
-  format %{ "psraw   $dst,$shift\t! arithmetic right shift packed4S" %}
+instruct vsubS_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (SubVS src (LoadVector mem)));
+  format %{ "vpsubw  $dst,$src,$mem\t! sub packedS" %}
   ins_encode %{
-    __ psraw($dst$$XMMRegister, (int)$shift$$constant);
+    int vector_len = vector_length_encoding(this);
+    __ vpsubw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra4S_reg(vecD dst, vecD src, vecS shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (RShiftVS src shift));
-  format %{ "vpsraw  $dst,$src,$shift\t! arithmetic right shift packed4S" %}
+// Integers vector sub
+instruct vsubI(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (SubVI dst src));
+  format %{ "psubd   $dst,$src\t! sub packedI" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsraw($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    __ psubd($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra4S_reg_imm(vecD dst, vecD src, immI8 shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (RShiftVS src shift));
-  format %{ "vpsraw  $dst,$src,$shift\t! arithmetic right shift packed4S" %}
+instruct vsubI_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (SubVI src1 src2));
+  format %{ "vpsubd  $dst,$src1,$src2\t! sub packedI" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsraw($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vpsubd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra8S(vecX dst, vecS shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 8);
-  match(Set dst (RShiftVS dst shift));
-  format %{ "psraw   $dst,$shift\t! arithmetic right shift packed8S" %}
+instruct vsubI_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (SubVI src (LoadVector mem)));
+  format %{ "vpsubd  $dst,$src,$mem\t! sub packedI" %}
   ins_encode %{
-    __ psraw($dst$$XMMRegister, $shift$$XMMRegister);
+    int vector_len = vector_length_encoding(this);
+    __ vpsubd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra8S_imm(vecX dst, immI8 shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 8);
-  match(Set dst (RShiftVS dst shift));
-  format %{ "psraw   $dst,$shift\t! arithmetic right shift packed8S" %}
+// Longs vector sub
+instruct vsubL(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (SubVL dst src));
+  format %{ "psubq   $dst,$src\t! sub packedL" %}
   ins_encode %{
-    __ psraw($dst$$XMMRegister, (int)$shift$$constant);
+    __ psubq($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra8S_reg(vecX dst, vecX src, vecS shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (RShiftVS src shift));
-  format %{ "vpsraw  $dst,$src,$shift\t! arithmetic right shift packed8S" %}
+instruct vsubL_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (SubVL src1 src2));
+  format %{ "vpsubq  $dst,$src1,$src2\t! sub packedL" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsraw($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vpsubq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra8S_reg_imm(vecX dst, vecX src, immI8 shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
-  match(Set dst (RShiftVS src shift));
-  format %{ "vpsraw  $dst,$src,$shift\t! arithmetic right shift packed8S" %}
+
+instruct vsubL_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (SubVL src (LoadVector mem)));
+  format %{ "vpsubq  $dst,$src,$mem\t! sub packedL" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsraw($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vpsubq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra16S_reg(vecY dst, vecY src, vecS shift) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 16);
-  match(Set dst (RShiftVS src shift));
-  format %{ "vpsraw  $dst,$src,$shift\t! arithmetic right shift packed16S" %}
+// Floats vector sub
+instruct vsubF(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (SubVF dst src));
+  format %{ "subps   $dst,$src\t! sub packedF" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vpsraw($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    __ subps($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra16S_reg_imm(vecY dst, vecY src, immI8 shift) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 16);
-  match(Set dst (RShiftVS src shift));
-  format %{ "vpsraw  $dst,$src,$shift\t! arithmetic right shift packed16S" %}
+instruct vsubF_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (SubVF src1 src2));
+  format %{ "vsubps  $dst,$src1,$src2\t! sub packedF" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vpsraw($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vsubps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra32S_reg(vecZ dst, vecZ src, vecS shift) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512bw() && n->as_Vector()->length() == 32);
-  match(Set dst (RShiftVS src shift));
-  format %{ "vpsraw  $dst,$src,$shift\t! arithmetic right shift packed32S" %}
+instruct vsubF_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (SubVF src (LoadVector mem)));
+  format %{ "vsubps  $dst,$src,$mem\t! sub packedF" %}
   ins_encode %{
-    int vector_len = 2;
-    __ vpsraw($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vsubps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra32S_reg_imm(vecZ dst, vecZ src, immI8 shift) %{
-  predicate(UseAVX > 2 && VM_Version::supports_avx512bw() && n->as_Vector()->length() == 32);
-  match(Set dst (RShiftVS src shift));
-  format %{ "vpsraw  $dst,$src,$shift\t! arithmetic right shift packed32S" %}
+// Doubles vector sub
+instruct vsubD(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (SubVD dst src));
+  format %{ "subpd   $dst,$src\t! sub packedD" %}
   ins_encode %{
-    int vector_len = 2;
-    __ vpsraw($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    __ subpd($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-// Integers vector arithmetic right shift
-instruct vsra2I(vecD dst, vecS shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (RShiftVI dst shift));
-  format %{ "psrad   $dst,$shift\t! arithmetic right shift packed2I" %}
+instruct vsubD_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (SubVD src1 src2));
+  format %{ "vsubpd  $dst,$src1,$src2\t! sub packedD" %}
   ins_encode %{
-    __ psrad($dst$$XMMRegister, $shift$$XMMRegister);
+    int vector_len = vector_length_encoding(this);
+    __ vsubpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra2I_imm(vecD dst, immI8 shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (RShiftVI dst shift));
-  format %{ "psrad   $dst,$shift\t! arithmetic right shift packed2I" %}
+instruct vsubD_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (SubVD src (LoadVector mem)));
+  format %{ "vsubpd  $dst,$src,$mem\t! sub packedD" %}
   ins_encode %{
-    __ psrad($dst$$XMMRegister, (int)$shift$$constant);
+    int vector_len = vector_length_encoding(this);
+    __ vsubpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra2I_reg(vecD dst, vecD src, vecS shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (RShiftVI src shift));
-  format %{ "vpsrad  $dst,$src,$shift\t! arithmetic right shift packed2I" %}
+// --------------------------------- MUL --------------------------------------
+
+// Byte vector mul
+instruct mulB_reg(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{
+  predicate(n->as_Vector()->length() == 4 ||
+            n->as_Vector()->length() == 8);
+  match(Set dst (MulVB src1 src2));
+  effect(TEMP dst, TEMP tmp, TEMP scratch);
+  format %{"vector_mulB $dst,$src1,$src2" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsrad($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    assert(UseSSE > 3, "required");
+    __ pmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister);
+    __ pmovsxbw($dst$$XMMRegister, $src2$$XMMRegister);
+    __ pmullw($tmp$$XMMRegister, $dst$$XMMRegister);
+    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
+    __ pand($dst$$XMMRegister, $tmp$$XMMRegister);
+    __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra2I_reg_imm(vecD dst, vecD src, immI8 shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (RShiftVI src shift));
-  format %{ "vpsrad  $dst,$src,$shift\t! arithmetic right shift packed2I" %}
+instruct mul16B_reg(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
+  predicate(n->as_Vector()->length() == 16 && UseAVX <= 1);
+  match(Set dst (MulVB src1 src2));
+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
+  format %{"vector_mulB $dst,$src1,$src2" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsrad($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    assert(UseSSE > 3, "required");
+    __ pmovsxbw($tmp1$$XMMRegister, $src1$$XMMRegister);
+    __ pmovsxbw($tmp2$$XMMRegister, $src2$$XMMRegister);
+    __ pmullw($tmp1$$XMMRegister, $tmp2$$XMMRegister);
+    __ pshufd($tmp2$$XMMRegister, $src1$$XMMRegister, 0xEE);
+    __ pshufd($dst$$XMMRegister, $src2$$XMMRegister, 0xEE);
+    __ pmovsxbw($tmp2$$XMMRegister, $tmp2$$XMMRegister);
+    __ pmovsxbw($dst$$XMMRegister, $dst$$XMMRegister);
+    __ pmullw($tmp2$$XMMRegister, $dst$$XMMRegister);
+    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
+    __ pand($tmp2$$XMMRegister, $dst$$XMMRegister);
+    __ pand($dst$$XMMRegister, $tmp1$$XMMRegister);
+    __ packuswb($dst$$XMMRegister, $tmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra4I(vecX dst, vecS shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (RShiftVI dst shift));
-  format %{ "psrad   $dst,$shift\t! arithmetic right shift packed4I" %}
+instruct vmul16B_reg_avx(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{
+  predicate(n->as_Vector()->length() == 16 && UseAVX > 1);
+  match(Set dst (MulVB src1 src2));
+  effect(TEMP dst, TEMP tmp, TEMP scratch);
+  format %{"vector_mulB $dst,$src1,$src2" %}
   ins_encode %{
-    __ psrad($dst$$XMMRegister, $shift$$XMMRegister);
+  int vector_len = Assembler::AVX_256bit;
+    __ vpmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister, vector_len);
+    __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
+    __ vpmullw($tmp$$XMMRegister, $tmp$$XMMRegister, $dst$$XMMRegister, vector_len);
+    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
+    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
+    __ vextracti128_high($tmp$$XMMRegister, $dst$$XMMRegister);
+    __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, 0);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra4I_imm(vecX dst, immI8 shift) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length() == 4);
-  match(Set dst (RShiftVI dst shift));
-  format %{ "psrad   $dst,$shift\t! arithmetic right shift packed4I" %}
-  ins_encode %{
-    __ psrad($dst$$XMMRegister, (int)$shift$$constant);
+instruct vmul32B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
+  predicate(n->as_Vector()->length() == 32);
+  match(Set dst (MulVB src1 src2));
+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
+  format %{"vector_mulB $dst,$src1,$src2" %}
+  ins_encode %{
+    assert(UseAVX > 1, "required");
+    int vector_len = Assembler::AVX_256bit;
+    __ vextracti128_high($tmp1$$XMMRegister, $src1$$XMMRegister);
+    __ vextracti128_high($dst$$XMMRegister, $src2$$XMMRegister);
+    __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
+    __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
+    __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
+    __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vector_len);
+    __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
+    __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
+    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
+    __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
+    __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
+    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+    __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp1$$XMMRegister, vector_len);
+    __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 0xD8, vector_len);
+  %}
+  ins_pipe( pipe_slow );
+%}
+
+instruct vmul64B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
+  predicate(n->as_Vector()->length() == 64);
+  match(Set dst (MulVB src1 src2));
+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
+  format %{"vector_mulB $dst,$src1,$src2\n\t" %}
+  ins_encode %{
+    assert(UseAVX > 2, "required");
+    int vector_len = Assembler::AVX_512bit;
+    __ vextracti64x4_high($tmp1$$XMMRegister, $src1$$XMMRegister);
+    __ vextracti64x4_high($dst$$XMMRegister, $src2$$XMMRegister);
+    __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
+    __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
+    __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
+    __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vector_len);
+    __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
+    __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
+    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
+    __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
+    __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
+    __ vpand($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
+    __ vpackuswb($dst$$XMMRegister, $tmp1$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+    __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vector_len, $scratch$$Register);
+    __ vpermq($dst$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra4I_reg(vecX dst, vecX src, vecS shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (RShiftVI src shift));
-  format %{ "vpsrad  $dst,$src,$shift\t! arithmetic right shift packed4I" %}
+// Shorts/Chars vector mul
+instruct vmulS(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (MulVS dst src));
+  format %{ "pmullw $dst,$src\t! mul packedS" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsrad($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    __ pmullw($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra4I_reg_imm(vecX dst, vecX src, immI8 shift) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (RShiftVI src shift));
-  format %{ "vpsrad  $dst,$src,$shift\t! arithmetic right shift packed4I" %}
+instruct vmulS_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (MulVS src1 src2));
+  format %{ "vpmullw $dst,$src1,$src2\t! mul packedS" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpsrad($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vpmullw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra8I_reg(vecY dst, vecY src, vecS shift) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 8);
-  match(Set dst (RShiftVI src shift));
-  format %{ "vpsrad  $dst,$src,$shift\t! arithmetic right shift packed8I" %}
+instruct vmulS_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (MulVS src (LoadVector mem)));
+  format %{ "vpmullw $dst,$src,$mem\t! mul packedS" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vpsrad($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vpmullw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra8I_reg_imm(vecY dst, vecY src, immI8 shift) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 8);
-  match(Set dst (RShiftVI src shift));
-  format %{ "vpsrad  $dst,$src,$shift\t! arithmetic right shift packed8I" %}
+// Integers vector mul
+instruct vmulI(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (MulVI dst src));
+  format %{ "pmulld  $dst,$src\t! mul packedI" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vpsrad($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    assert(UseSSE > 3, "required");
+    __ pmulld($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra16I_reg(vecZ dst, vecZ src, vecS shift) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (RShiftVI src shift));
-  format %{ "vpsrad  $dst,$src,$shift\t! arithmetic right shift packed16I" %}
+instruct vmulI_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (MulVI src1 src2));
+  format %{ "vpmulld $dst,$src1,$src2\t! mul packedI" %}
   ins_encode %{
-    int vector_len = 2;
-    __ vpsrad($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vpmulld($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vsra16I_reg_imm(vecZ dst, vecZ src, immI8 shift) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (RShiftVI src shift));
-  format %{ "vpsrad  $dst,$src,$shift\t! arithmetic right shift packed16I" %}
+instruct vmulI_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (MulVI src (LoadVector mem)));
+  format %{ "vpmulld $dst,$src,$mem\t! mul packedI" %}
   ins_encode %{
-    int vector_len = 2;
-    __ vpsrad($dst$$XMMRegister, $src$$XMMRegister, (int)$shift$$constant, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vpmulld($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-// There are no longs vector arithmetic right shift instructions.
-
-
-// --------------------------------- AND --------------------------------------
-
-instruct vand4B(vecS dst, vecS src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length_in_bytes() == 4);
-  match(Set dst (AndV dst src));
-  format %{ "pand    $dst,$src\t! and vectors (4 bytes)" %}
+// Longs vector mul
+instruct vmulL_reg(vec dst, vec src1, vec src2) %{
+  match(Set dst (MulVL src1 src2));
+  format %{ "vpmullq $dst,$src1,$src2\t! mul packedL" %}
   ins_encode %{
-    __ pand($dst$$XMMRegister, $src$$XMMRegister);
+    assert(UseAVX > 2, "required");
+    int vector_len = vector_length_encoding(this);
+    __ vpmullq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vand4B_reg(vecS dst, vecS src1, vecS src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 4);
-  match(Set dst (AndV src1 src2));
-  format %{ "vpand   $dst,$src1,$src2\t! and vectors (4 bytes)" %}
+instruct vmulL_mem(vec dst, vec src, memory mem) %{
+  match(Set dst (MulVL src (LoadVector mem)));
+  format %{ "vpmullq $dst,$src,$mem\t! mul packedL" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpand($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
+    assert(UseAVX > 2, "required");
+    int vector_len = vector_length_encoding(this);
+    __ vpmullq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vand4B_mem(vecS dst, vecS src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 4);
-  match(Set dst (AndV src (LoadVector mem)));
-  format %{ "vpand   $dst,$src,$mem\t! and vectors (4 bytes)" %}
+// Floats vector mul
+instruct vmulF(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (MulVF dst src));
+  format %{ "mulps   $dst,$src\t! mul packedF" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpand($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
+    __ mulps($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vand8B(vecD dst, vecD src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length_in_bytes() == 8);
-  match(Set dst (AndV dst src));
-  format %{ "pand    $dst,$src\t! and vectors (8 bytes)" %}
+instruct vmulF_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (MulVF src1 src2));
+  format %{ "vmulps  $dst,$src1,$src2\t! mul packedF" %}
   ins_encode %{
-    __ pand($dst$$XMMRegister, $src$$XMMRegister);
+    int vector_len = vector_length_encoding(this);
+    __ vmulps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vand8B_reg(vecD dst, vecD src1, vecD src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 8);
-  match(Set dst (AndV src1 src2));
-  format %{ "vpand   $dst,$src1,$src2\t! and vectors (8 bytes)" %}
+instruct vmulF_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (MulVF src (LoadVector mem)));
+  format %{ "vmulps  $dst,$src,$mem\t! mul packedF" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpand($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vmulps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vand8B_mem(vecD dst, vecD src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 8);
-  match(Set dst (AndV src (LoadVector mem)));
-  format %{ "vpand   $dst,$src,$mem\t! and vectors (8 bytes)" %}
+// Doubles vector mul
+instruct vmulD(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (MulVD dst src));
+  format %{ "mulpd   $dst,$src\t! mul packedD" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpand($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
+    __ mulpd($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vand16B(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length_in_bytes() == 16);
-  match(Set dst (AndV dst src));
-  format %{ "pand    $dst,$src\t! and vectors (16 bytes)" %}
+instruct vmulD_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (MulVD src1 src2));
+  format %{ "vmulpd  $dst,$src1,$src2\t! mul packedD" %}
   ins_encode %{
-    __ pand($dst$$XMMRegister, $src$$XMMRegister);
+    int vector_len = vector_length_encoding(this);
+    __ vmulpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vand16B_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 16);
-  match(Set dst (AndV src1 src2));
-  format %{ "vpand   $dst,$src1,$src2\t! and vectors (16 bytes)" %}
+instruct vmulD_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (MulVD src (LoadVector mem)));
+  format %{ "vmulpd  $dst,$src,$mem\t! mul packedD" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpand($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vmulpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vand16B_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 16);
-  match(Set dst (AndV src (LoadVector mem)));
-  format %{ "vpand   $dst,$src,$mem\t! and vectors (16 bytes)" %}
+instruct vcmov8F_reg(legVec dst, legVec src1, legVec src2, immI8 cop, cmpOp_vcmppd copnd) %{
+  predicate(UseAVX > 0 && n->as_Vector()->length() == 8);
+  match(Set dst (CMoveVF (Binary copnd cop) (Binary src1 src2)));
+  effect(TEMP dst, USE src1, USE src2);
+  format %{ "cmpps.$copnd  $dst, $src1, $src2  ! vcmovevf, cond=$cop\n\t"
+            "blendvps $dst,$src1,$src2,$dst ! vcmovevf\n\t"
+         %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpand($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
+    int vector_len = 1;
+    int cond = (Assembler::Condition)($copnd$$cmpcode);
+    __ cmpps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, cond, vector_len);
+    __ blendvps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $dst$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vand32B_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length_in_bytes() == 32);
-  match(Set dst (AndV src1 src2));
-  format %{ "vpand   $dst,$src1,$src2\t! and vectors (32 bytes)" %}
+instruct vcmov4D_reg(legVec dst, legVec src1, legVec src2, immI8 cop, cmpOp_vcmppd copnd) %{
+  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
+  match(Set dst (CMoveVD (Binary copnd cop) (Binary src1 src2)));
+  effect(TEMP dst, USE src1, USE src2);
+  format %{ "cmppd.$copnd  $dst, $src1, $src2  ! vcmovevd, cond=$cop\n\t"
+            "blendvpd $dst,$src1,$src2,$dst ! vcmovevd\n\t"
+         %}
   ins_encode %{
     int vector_len = 1;
-    __ vpand($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
+    int cond = (Assembler::Condition)($copnd$$cmpcode);
+    __ cmppd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, cond, vector_len);
+    __ blendvpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $dst$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vand32B_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length_in_bytes() == 32);
-  match(Set dst (AndV src (LoadVector mem)));
-  format %{ "vpand   $dst,$src,$mem\t! and vectors (32 bytes)" %}
+// --------------------------------- DIV --------------------------------------
+
+// Floats vector div
+instruct vdivF(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (DivVF dst src));
+  format %{ "divps   $dst,$src\t! div packedF" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vpand($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
+    __ divps($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vand64B_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length_in_bytes() == 64);
-  match(Set dst (AndV src1 src2));
-  format %{ "vpand   $dst,$src1,$src2\t! and vectors (64 bytes)" %}
+instruct vdivF_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (DivVF src1 src2));
+  format %{ "vdivps  $dst,$src1,$src2\t! div packedF" %}
   ins_encode %{
-    int vector_len = 2;
-    __ vpand($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vdivps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vand64B_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length_in_bytes() == 64);
-  match(Set dst (AndV src (LoadVector mem)));
-  format %{ "vpand   $dst,$src,$mem\t! and vectors (64 bytes)" %}
+instruct vdivF_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (DivVF src (LoadVector mem)));
+  format %{ "vdivps  $dst,$src,$mem\t! div packedF" %}
   ins_encode %{
-    int vector_len = 2;
-    __ vpand($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vdivps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-// --------------------------------- OR ---------------------------------------
-
-instruct vor4B(vecS dst, vecS src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length_in_bytes() == 4);
-  match(Set dst (OrV dst src));
-  format %{ "por     $dst,$src\t! or vectors (4 bytes)" %}
+// Doubles vector div
+instruct vdivD(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (DivVD dst src));
+  format %{ "divpd   $dst,$src\t! div packedD" %}
   ins_encode %{
-    __ por($dst$$XMMRegister, $src$$XMMRegister);
+    __ divpd($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vor4B_reg(vecS dst, vecS src1, vecS src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 4);
-  match(Set dst (OrV src1 src2));
-  format %{ "vpor    $dst,$src1,$src2\t! or vectors (4 bytes)" %}
+instruct vdivD_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (DivVD src1 src2));
+  format %{ "vdivpd  $dst,$src1,$src2\t! div packedD" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vdivpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vor4B_mem(vecS dst, vecS src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 4);
-  match(Set dst (OrV src (LoadVector mem)));
-  format %{ "vpor    $dst,$src,$mem\t! or vectors (4 bytes)" %}
+instruct vdivD_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (DivVD src (LoadVector mem)));
+  format %{ "vdivpd  $dst,$src,$mem\t! div packedD" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vdivpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vor8B(vecD dst, vecD src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length_in_bytes() == 8);
-  match(Set dst (OrV dst src));
-  format %{ "por     $dst,$src\t! or vectors (8 bytes)" %}
+// --------------------------------- Sqrt --------------------------------------
+
+instruct vsqrtF_reg(vec dst, vec src) %{
+  match(Set dst (SqrtVF src));
+  format %{ "vsqrtps  $dst,$src\t! sqrt packedF" %}
   ins_encode %{
-    __ por($dst$$XMMRegister, $src$$XMMRegister);
+    assert(UseAVX > 0, "required");
+    int vector_len = vector_length_encoding(this);
+    __ vsqrtps($dst$$XMMRegister, $src$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vor8B_reg(vecD dst, vecD src1, vecD src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 8);
-  match(Set dst (OrV src1 src2));
-  format %{ "vpor    $dst,$src1,$src2\t! or vectors (8 bytes)" %}
+instruct vsqrtF_mem(vec dst, memory mem) %{
+  match(Set dst (SqrtVF (LoadVector mem)));
+  format %{ "vsqrtps  $dst,$mem\t! sqrt packedF" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
+    assert(UseAVX > 0, "required");
+    int vector_len = vector_length_encoding(this);
+    __ vsqrtps($dst$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vor8B_mem(vecD dst, vecD src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 4);
-  match(Set dst (OrV src (LoadVector mem)));
-  format %{ "vpor    $dst,$src,$mem\t! or vectors (8 bytes)" %}
+// Floating point vector sqrt
+instruct vsqrtD_reg(vec dst, vec src) %{
+  match(Set dst (SqrtVD src));
+  format %{ "vsqrtpd  $dst,$src\t! sqrt packedD" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
+    assert(UseAVX > 0, "required");
+    int vector_len = vector_length_encoding(this);
+    __ vsqrtpd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vor16B(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length_in_bytes() == 16);
-  match(Set dst (OrV dst src));
-  format %{ "por     $dst,$src\t! or vectors (16 bytes)" %}
+instruct vsqrtD_mem(vec dst, memory mem) %{
+  match(Set dst (SqrtVD (LoadVector mem)));
+  format %{ "vsqrtpd  $dst,$mem\t! sqrt packedD" %}
   ins_encode %{
-    __ por($dst$$XMMRegister, $src$$XMMRegister);
+    assert(UseAVX > 0, "required");
+    int vector_len = vector_length_encoding(this);
+    __ vsqrtpd($dst$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vor16B_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 16);
-  match(Set dst (OrV src1 src2));
-  format %{ "vpor    $dst,$src1,$src2\t! or vectors (16 bytes)" %}
+// ------------------------------ Shift ---------------------------------------
+
+// Left and right shift count vectors are the same on x86
+// (only lowest bits of xmm reg are used for count).
+instruct vshiftcnt(vec dst, rRegI cnt) %{
+  match(Set dst (LShiftCntV cnt));
+  match(Set dst (RShiftCntV cnt));
+  format %{ "movdl    $dst,$cnt\t! load shift count" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
+    __ movdl($dst$$XMMRegister, $cnt$$Register);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vor16B_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 16);
-  match(Set dst (OrV src (LoadVector mem)));
-  format %{ "vpor    $dst,$src,$mem\t! or vectors (16 bytes)" %}
+// Byte vector shift
+instruct vshiftB(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
+  predicate(n->as_Vector()->length() <= 8);
+  match(Set dst (LShiftVB src shift));
+  match(Set dst (RShiftVB src shift));
+  match(Set dst (URShiftVB src shift));
+  effect(TEMP dst, USE src, USE shift, TEMP tmp, TEMP scratch);
+  format %{"vector_byte_shift $dst,$src,$shift" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
+    assert(UseSSE > 3, "required");
+    int opcode = this->ideal_Opcode();
+    __ vextendbw(opcode, $tmp$$XMMRegister, $src$$XMMRegister);
+    __ vshiftw(opcode, $tmp$$XMMRegister, $shift$$XMMRegister);
+    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
+    __ pand($dst$$XMMRegister, $tmp$$XMMRegister);
+    __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vor32B_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length_in_bytes() == 32);
-  match(Set dst (OrV src1 src2));
-  format %{ "vpor    $dst,$src1,$src2\t! or vectors (32 bytes)" %}
+instruct vshift16B(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{
+  predicate(n->as_Vector()->length() == 16 && UseAVX <= 1);
+  match(Set dst (LShiftVB src shift));
+  match(Set dst (RShiftVB src shift));
+  match(Set dst (URShiftVB src shift));
+  effect(TEMP dst, USE src, USE shift, TEMP tmp1, TEMP tmp2, TEMP scratch);
+  format %{"vector_byte_shift $dst,$src,$shift" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vpor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
+    assert(UseSSE > 3, "required");
+    int opcode = this->ideal_Opcode();
+
+    __ vextendbw(opcode, $tmp1$$XMMRegister, $src$$XMMRegister);
+    __ vshiftw(opcode, $tmp1$$XMMRegister, $shift$$XMMRegister);
+    __ pshufd($tmp2$$XMMRegister, $src$$XMMRegister, 0xE);
+    __ vextendbw(opcode, $tmp2$$XMMRegister, $tmp2$$XMMRegister);
+    __ vshiftw(opcode, $tmp2$$XMMRegister, $shift$$XMMRegister);
+    __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
+    __ pand($tmp2$$XMMRegister, $dst$$XMMRegister);
+    __ pand($dst$$XMMRegister, $tmp1$$XMMRegister);
+    __ packuswb($dst$$XMMRegister, $tmp2$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vor32B_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length_in_bytes() == 32);
-  match(Set dst (OrV src (LoadVector mem)));
-  format %{ "vpor    $dst,$src,$mem\t! or vectors (32 bytes)" %}
+instruct vshift16B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
+  predicate(n->as_Vector()->length() == 16 && UseAVX > 1);
+  match(Set dst (LShiftVB src shift));
+  match(Set dst (RShiftVB src shift));
+  match(Set dst (URShiftVB src shift));
+  effect(TEMP dst, TEMP tmp, TEMP scratch);
+  format %{"vector_byte_shift $dst,$src,$shift" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vpor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
+    int opcode = this->ideal_Opcode();
+    int vector_len = Assembler::AVX_256bit;
+    __ vextendbw(opcode, $tmp$$XMMRegister, $src$$XMMRegister, vector_len);
+    __ vshiftw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
+    __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
+    __ vextracti128_high($dst$$XMMRegister, $tmp$$XMMRegister);
+    __ vpackuswb($dst$$XMMRegister, $tmp$$XMMRegister, $dst$$XMMRegister, 0);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vor64B_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length_in_bytes() == 64);
-  match(Set dst (OrV src1 src2));
-  format %{ "vpor    $dst,$src1,$src2\t! or vectors (64 bytes)" %}
-  ins_encode %{
-    int vector_len = 2;
-    __ vpor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
+instruct vshift32B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
+  predicate(n->as_Vector()->length() == 32);
+  match(Set dst (LShiftVB src shift));
+  match(Set dst (RShiftVB src shift));
+  match(Set dst (URShiftVB src shift));
+  effect(TEMP dst, TEMP tmp, TEMP scratch);
+  format %{"vector_byte_shift $dst,$src,$shift" %}
+  ins_encode %{
+    assert(UseAVX > 1, "required");
+    int opcode = this->ideal_Opcode();
+    int vector_len = Assembler::AVX_256bit;
+    __ vextracti128_high($tmp$$XMMRegister, $src$$XMMRegister);
+    __ vextendbw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, vector_len);
+    __ vextendbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, vector_len);
+    __ vshiftw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
+    __ vshiftw(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $shift$$XMMRegister, vector_len);
+    __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
+    __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
+    __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
+    __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 0xD8, vector_len);
+  %}
+  ins_pipe( pipe_slow );
+%}
+
+instruct vshift64B_avx(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{
+  predicate(n->as_Vector()->length() == 64);
+  match(Set dst (LShiftVB src shift));
+  match(Set dst (RShiftVB src shift));
+  match(Set dst (URShiftVB src shift));
+  effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
+  format %{"vector_byte_shift $dst,$src,$shift" %}
+  ins_encode %{
+    assert(UseAVX > 2, "required");
+    int opcode = this->ideal_Opcode();
+    int vector_len = Assembler::AVX_512bit;
+    __ vextracti64x4($tmp1$$XMMRegister, $src$$XMMRegister, 1);
+    __ vextendbw(opcode, $tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
+    __ vextendbw(opcode, $tmp2$$XMMRegister, $src$$XMMRegister, vector_len);
+    __ vshiftw(opcode, $tmp1$$XMMRegister, $tmp1$$XMMRegister, $shift$$XMMRegister, vector_len);
+    __ vshiftw(opcode, $tmp2$$XMMRegister, $tmp2$$XMMRegister, $shift$$XMMRegister, vector_len);
+    __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
+    __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
+    __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
+    __ vpand($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
+    __ vpackuswb($dst$$XMMRegister, $tmp1$$XMMRegister, $tmp2$$XMMRegister, vector_len);
+    __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vector_len, $scratch$$Register);
+    __ vpermq($dst$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vor64B_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length_in_bytes() == 64);
-  match(Set dst (OrV src (LoadVector mem)));
-  format %{ "vpor    $dst,$src,$mem\t! or vectors (64 bytes)" %}
+// Shorts vector logical right shift produces incorrect Java result
+// for negative data because java code convert short value into int with
+// sign extension before a shift. But char vectors are fine since chars are
+// unsigned values.
+// Shorts/Chars vector left shift
+instruct vshiftS(vec dst, vec src, vec shift) %{
+  match(Set dst (LShiftVS src shift));
+  match(Set dst (RShiftVS src shift));
+  match(Set dst (URShiftVS src shift));
+  effect(TEMP dst, USE src, USE shift);
+  format %{ "vshiftw  $dst,$src,$shift\t! shift packedS" %}
   ins_encode %{
-    int vector_len = 2;
-    __ vpor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
+    int opcode = this->ideal_Opcode();
+    if (UseAVX > 0) {
+      int vlen_enc = vector_length_encoding(this);
+      __ vshiftw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc);
+    } else {
+      int vlen = vector_length(this);
+      if (vlen == 2) {
+        __ movflt($dst$$XMMRegister, $src$$XMMRegister);
+        __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
+      } else if (vlen == 4) {
+        __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
+        __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
+      } else {
+        assert (vlen == 8, "sanity");
+        __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
+        __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
+      }
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-// --------------------------------- XOR --------------------------------------
-
-instruct vxor4B(vecS dst, vecS src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length_in_bytes() == 4);
-  match(Set dst (XorV dst src));
-  format %{ "pxor    $dst,$src\t! xor vectors (4 bytes)" %}
+// Integers vector left shift
+instruct vshiftI(vec dst, vec src, vec shift) %{
+  match(Set dst (LShiftVI src shift));
+  match(Set dst (RShiftVI src shift));
+  match(Set dst (URShiftVI src shift));
+  effect(TEMP dst, USE src, USE shift);
+  format %{ "vshiftd  $dst,$src,$shift\t! shift packedI" %}
   ins_encode %{
-    __ pxor($dst$$XMMRegister, $src$$XMMRegister);
+    int opcode = this->ideal_Opcode();
+    if (UseAVX > 0) {
+      int vector_len = vector_length_encoding(this);
+      __ vshiftd(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    } else {
+      int vlen = vector_length(this);
+      if (vlen == 2) {
+        __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
+        __ vshiftd(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
+      } else {
+        assert(vlen == 4, "sanity");
+        __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
+        __ vshiftd(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
+      }
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vxor4B_reg(vecS dst, vecS src1, vecS src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 4);
-  match(Set dst (XorV src1 src2));
-  format %{ "vpxor   $dst,$src1,$src2\t! xor vectors (4 bytes)" %}
+// Longs vector shift
+instruct vshiftL(vec dst, vec src, vec shift) %{
+  match(Set dst (LShiftVL src shift));
+  match(Set dst (URShiftVL src shift));
+  effect(TEMP dst, USE src, USE shift);
+  format %{ "vshiftq  $dst,$src,$shift\t! shift packedL" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpxor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
+    int opcode = this->ideal_Opcode();
+    if (UseAVX > 0) {
+      int vector_len = vector_length_encoding(this);
+      __ vshiftq(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+    } else {
+      assert(vector_length(this) == 2, "");
+      __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
+      __ vshiftq(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vxor4B_mem(vecS dst, vecS src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 4);
-  match(Set dst (XorV src (LoadVector mem)));
-  format %{ "vpxor   $dst,$src,$mem\t! xor vectors (4 bytes)" %}
+// -------------------ArithmeticRightShift -----------------------------------
+// Long vector arithmetic right shift
+instruct vshiftL_arith_reg(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
+  predicate(UseAVX <= 2);
+  match(Set dst (RShiftVL src shift));
+  effect(TEMP dst, TEMP tmp, TEMP scratch);
+  format %{ "vshiftq $dst,$src,$shift" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpxor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
+    uint vlen = vector_length(this);
+    if (vlen == 2) {
+      assert(UseSSE >= 2, "required");
+      __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
+      __ psrlq($dst$$XMMRegister, $shift$$XMMRegister);
+      __ movdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);
+      __ psrlq($tmp$$XMMRegister, $shift$$XMMRegister);
+      __ pxor($dst$$XMMRegister, $tmp$$XMMRegister);
+      __ psubq($dst$$XMMRegister, $tmp$$XMMRegister);
+    } else {
+      assert(vlen == 4, "sanity");
+      assert(UseAVX > 1, "required");
+      int vector_len = Assembler::AVX_256bit;
+      __ vpsrlq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
+      __ vmovdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);
+      __ vpsrlq($tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
+      __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
+      __ vpsubq($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vxor8B(vecD dst, vecD src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length_in_bytes() == 8);
-  match(Set dst (XorV dst src));
-  format %{ "pxor    $dst,$src\t! xor vectors (8 bytes)" %}
+instruct vshiftL_arith_reg_evex(vec dst, vec src, vec shift) %{
+  predicate(UseAVX > 2);
+  match(Set dst (RShiftVL src shift));
+  format %{ "vshiftq $dst,$src,$shift" %}
   ins_encode %{
-    __ pxor($dst$$XMMRegister, $src$$XMMRegister);
+    int vector_len = vector_length_encoding(this);
+    __ evpsraq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vxor8B_reg(vecD dst, vecD src1, vecD src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 8);
-  match(Set dst (XorV src1 src2));
-  format %{ "vpxor   $dst,$src1,$src2\t! xor vectors (8 bytes)" %}
+// --------------------------------- AND --------------------------------------
+
+instruct vand(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (AndV dst src));
+  format %{ "pand    $dst,$src\t! and vectors" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpxor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
+    __ pand($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vxor8B_mem(vecD dst, vecD src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 8);
-  match(Set dst (XorV src (LoadVector mem)));
-  format %{ "vpxor   $dst,$src,$mem\t! xor vectors (8 bytes)" %}
+instruct vand_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (AndV src1 src2));
+  format %{ "vpand   $dst,$src1,$src2\t! and vectors" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpxor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vpand($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vxor16B(vecX dst, vecX src) %{
-  predicate(UseAVX == 0 && n->as_Vector()->length_in_bytes() == 16);
-  match(Set dst (XorV dst src));
-  format %{ "pxor    $dst,$src\t! xor vectors (16 bytes)" %}
+instruct vand_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (AndV src (LoadVector mem)));
+  format %{ "vpand   $dst,$src,$mem\t! and vectors" %}
   ins_encode %{
-    __ pxor($dst$$XMMRegister, $src$$XMMRegister);
+    int vector_len = vector_length_encoding(this);
+    __ vpand($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vxor16B_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 16);
-  match(Set dst (XorV src1 src2));
-  format %{ "vpxor   $dst,$src1,$src2\t! xor vectors (16 bytes)" %}
+// --------------------------------- OR ---------------------------------------
+
+instruct vor(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (OrV dst src));
+  format %{ "por     $dst,$src\t! or vectors" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpxor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
+    __ por($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vxor16B_mem(vecX dst, vecX src, memory mem) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length_in_bytes() == 16);
-  match(Set dst (XorV src (LoadVector mem)));
-  format %{ "vpxor   $dst,$src,$mem\t! xor vectors (16 bytes)" %}
+instruct vor_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
+  match(Set dst (OrV src1 src2));
+  format %{ "vpor    $dst,$src1,$src2\t! or vectors" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vpxor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vpor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vxor32B_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length_in_bytes() == 32);
-  match(Set dst (XorV src1 src2));
-  format %{ "vpxor   $dst,$src1,$src2\t! xor vectors (32 bytes)" %}
+instruct vor_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
+  match(Set dst (OrV src (LoadVector mem)));
+  format %{ "vpor    $dst,$src,$mem\t! or vectors" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vpxor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
+    int vector_len = vector_length_encoding(this);
+    __ vpor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vxor32B_mem(vecY dst, vecY src, memory mem) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length_in_bytes() == 32);
-  match(Set dst (XorV src (LoadVector mem)));
-  format %{ "vpxor   $dst,$src,$mem\t! xor vectors (32 bytes)" %}
+// --------------------------------- XOR --------------------------------------
+
+instruct vxor(vec dst, vec src) %{
+  predicate(UseAVX == 0);
+  match(Set dst (XorV dst src));
+  format %{ "pxor    $dst,$src\t! xor vectors" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vpxor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
+    __ pxor($dst$$XMMRegister, $src$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vxor64B_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length_in_bytes() == 64);
+instruct vxor_reg(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
   match(Set dst (XorV src1 src2));
-  format %{ "vpxor   $dst,$src1,$src2\t! xor vectors (64 bytes)" %}
+  format %{ "vpxor   $dst,$src1,$src2\t! xor vectors" %}
   ins_encode %{
-    int vector_len = 2;
+    int vector_len = vector_length_encoding(this);
     __ vpxor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vxor64B_mem(vecZ dst, vecZ src, memory mem) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length_in_bytes() == 64);
+instruct vxor_mem(vec dst, vec src, memory mem) %{
+  predicate(UseAVX > 0);
   match(Set dst (XorV src (LoadVector mem)));
-  format %{ "vpxor   $dst,$src,$mem\t! xor vectors (64 bytes)" %}
+  format %{ "vpxor   $dst,$src,$mem\t! xor vectors" %}
   ins_encode %{
-    int vector_len = 2;
+    int vector_len = vector_length_encoding(this);
     __ vpxor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-// --------------------------------- FMA --------------------------------------
-
-// a * b + c
-instruct vfma2D_reg(vecX a, vecX b, vecX c) %{
-  predicate(UseFMA && n->as_Vector()->length() == 2);
-  match(Set c (FmaVD  c (Binary a b)));
-  format %{ "fmapd $a,$b,$c\t# $c = $a * $b + $c fma packed2D" %}
-  ins_cost(150);
+// --------------------------------- ABS --------------------------------------
+// a = |a|
+instruct vabsB_reg(vec dst, vec src) %{
+  match(Set dst (AbsVB  src));
+  format %{ "vabsb $dst,$src\t# $dst = |$src| abs packedB" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vector_len);
+    uint vlen = vector_length(this);
+    if (vlen <= 16) {
+      __ pabsb($dst$$XMMRegister, $src$$XMMRegister);
+    } else {
+      int vlen_enc = vector_length_encoding(this);
+      __ vpabsb($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-// a * b + c
-instruct vfma2D_mem(vecX a, memory b, vecX c) %{
-  predicate(UseFMA && n->as_Vector()->length() == 2);
-  match(Set c (FmaVD  c (Binary a (LoadVector b))));
-  format %{ "fmapd $a,$b,$c\t# $c = $a * $b + $c fma packed2D" %}
-  ins_cost(150);
+instruct vabsS_reg(vec dst, vec src) %{
+  match(Set dst (AbsVS  src));
+  format %{ "vabsw $dst,$src\t# $dst = |$src| abs packedS" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vector_len);
+    uint vlen = vector_length(this);
+    if (vlen <= 8) {
+      __ pabsw($dst$$XMMRegister, $src$$XMMRegister);
+    } else {
+      int vlen_enc = vector_length_encoding(this);
+      __ vpabsw($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-
-// a * b + c
-instruct vfma4D_reg(vecY a, vecY b, vecY c) %{
-  predicate(UseFMA && n->as_Vector()->length() == 4);
-  match(Set c (FmaVD  c (Binary a b)));
-  format %{ "fmapd $a,$b,$c\t# $c = $a * $b + $c fma packed4D" %}
-  ins_cost(150);
+instruct vabsI_reg(vec dst, vec src) %{
+  match(Set dst (AbsVI  src));
+  format %{ "pabsd $dst,$src\t# $dst = |$src| abs packedI" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vector_len);
+    uint vlen = vector_length(this);
+    if (vlen <= 4) {
+      __ pabsd($dst$$XMMRegister, $src$$XMMRegister);
+    } else {
+      int vlen_enc = vector_length_encoding(this);
+      __ vpabsd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-// a * b + c
-instruct vfma4D_mem(vecY a, memory b, vecY c) %{
-  predicate(UseFMA && n->as_Vector()->length() == 4);
-  match(Set c (FmaVD  c (Binary a (LoadVector b))));
-  format %{ "fmapd $a,$b,$c\t# $c = $a * $b + $c fma packed4D" %}
-  ins_cost(150);
+instruct vabsL_reg(vec dst, vec src) %{
+  match(Set dst (AbsVL  src));
+  format %{ "evpabsq $dst,$src\t# $dst = |$src| abs packedL" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vector_len);
+    assert(UseAVX > 2, "required");
+    int vector_len = vector_length_encoding(this);
+    __ evpabsq($dst$$XMMRegister, $src$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-// a * b + c
-instruct vfma8D_reg(vecZ a, vecZ b, vecZ c) %{
-  predicate(UseFMA && n->as_Vector()->length() == 8);
-  match(Set c (FmaVD  c (Binary a b)));
-  format %{ "fmapd $a,$b,$c\t# $c = $a * $b + $c fma packed8D" %}
-  ins_cost(150);
-  ins_encode %{
-    int vector_len = 2;
-    __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
+// --------------------------------- ABSNEG --------------------------------------
 
-// a * b + c
-instruct vfma8D_mem(vecZ a, memory b, vecZ c) %{
-  predicate(UseFMA && n->as_Vector()->length() == 8);
-  match(Set c (FmaVD  c (Binary a (LoadVector b))));
-  format %{ "fmapd $a,$b,$c\t# $c = $a * $b + $c fma packed8D" %}
+instruct vabsnegF(vec dst, vec src, rRegI scratch) %{
+  predicate(n->as_Vector()->length() != 4); // handled by 1-operand instruction vabsneg4F
+  match(Set dst (AbsVF src));
+  match(Set dst (NegVF src));
+  effect(TEMP scratch);
+  format %{ "vabsnegf $dst,$src,[mask]\t# absneg packedF" %}
   ins_cost(150);
   ins_encode %{
-    int vector_len = 2;
-    __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vector_len);
+    int opcode = this->ideal_Opcode();
+    int vlen = vector_length(this);
+    if (vlen == 2) {
+      __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);
+    } else {
+      assert(vlen == 8 || vlen == 16, "required");
+      int vlen_enc = vector_length_encoding(this);
+      __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
-// a * b + c
-instruct vfma4F_reg(vecX a, vecX b, vecX c) %{
-  predicate(UseFMA && n->as_Vector()->length() == 4);
-  match(Set c (FmaVF  c (Binary a b)));
-  format %{ "fmaps $a,$b,$c\t# $c = $a * $b + $c fma packed4F" %}
+instruct vabsneg4F(vec dst, rRegI scratch) %{
+  predicate(n->as_Vector()->length() == 4);
+  match(Set dst (AbsVF dst));
+  match(Set dst (NegVF dst));
+  effect(TEMP scratch);
+  format %{ "vabsnegf $dst,[mask]\t# absneg packed4F" %}
   ins_cost(150);
   ins_encode %{
-    int vector_len = 0;
-    __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vector_len);
+    int opcode = this->ideal_Opcode();
+    __ vabsnegf(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $scratch$$Register);
   %}
   ins_pipe( pipe_slow );
 %}
 
-// a * b + c
-instruct vfma4F_mem(vecX a, memory b, vecX c) %{
-  predicate(UseFMA && n->as_Vector()->length() == 4);
-  match(Set c (FmaVF  c (Binary a (LoadVector b))));
-  format %{ "fmaps $a,$b,$c\t# $c = $a * $b + $c fma packed4F" %}
-  ins_cost(150);
+instruct vabsnegD(vec dst, vec src, rRegI scratch) %{
+  match(Set dst (AbsVD  src));
+  match(Set dst (NegVD  src));
+  effect(TEMP scratch);
+  format %{ "vabsnegd $dst,$src,[mask]\t# absneg packedD" %}
   ins_encode %{
-    int vector_len = 0;
-    __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vector_len);
+    int opcode = this->ideal_Opcode();
+    uint vlen = vector_length(this);
+    if (vlen == 2) {
+      assert(UseSSE >= 2, "required");
+      __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);
+    } else {
+      int vlen_enc = vector_length_encoding(this);
+      __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);
+    }
   %}
   ins_pipe( pipe_slow );
 %}
 
+// --------------------------------- FMA --------------------------------------
 // a * b + c
-instruct vfma8F_reg(vecY a, vecY b, vecY c) %{
-  predicate(UseFMA && n->as_Vector()->length() == 8);
+
+instruct vfmaF_reg(vec a, vec b, vec c) %{
   match(Set c (FmaVF  c (Binary a b)));
-  format %{ "fmaps $a,$b,$c\t# $c = $a * $b + $c fma packed8F" %}
+  format %{ "fmaps $a,$b,$c\t# $c = $a * $b + $c fma packedF" %}
   ins_cost(150);
   ins_encode %{
-    int vector_len = 1;
+    assert(UseFMA, "not enabled");
+    int vector_len = vector_length_encoding(this);
     __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-// a * b + c
-instruct vfma8F_mem(vecY a, memory b, vecY c) %{
-  predicate(UseFMA && n->as_Vector()->length() == 8);
+instruct vfmaF_mem(vec a, memory b, vec c) %{
   match(Set c (FmaVF  c (Binary a (LoadVector b))));
-  format %{ "fmaps $a,$b,$c\t# $c = $a * $b + $c fma packed8F" %}
+  format %{ "fmaps $a,$b,$c\t# $c = $a * $b + $c fma packedF" %}
   ins_cost(150);
   ins_encode %{
-    int vector_len = 1;
+    assert(UseFMA, "not enabled");
+    int vector_len = vector_length_encoding(this);
     __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-// a * b + c
-instruct vfma16F_reg(vecZ a, vecZ b, vecZ c) %{
-  predicate(UseFMA && n->as_Vector()->length() == 16);
-  match(Set c (FmaVF  c (Binary a b)));
-  format %{ "fmaps $a,$b,$c\t# $c = $a * $b + $c fma packed16F" %}
+instruct vfmaD_reg(vec a, vec b, vec c) %{
+  match(Set c (FmaVD  c (Binary a b)));
+  format %{ "fmapd $a,$b,$c\t# $c = $a * $b + $c fma packedD" %}
   ins_cost(150);
   ins_encode %{
-    int vector_len = 2;
-    __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vector_len);
+    assert(UseFMA, "not enabled");
+    int vector_len = vector_length_encoding(this);
+    __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
-// a * b + c
-instruct vfma16F_mem(vecZ a, memory b, vecZ c) %{
-  predicate(UseFMA && n->as_Vector()->length() == 16);
-  match(Set c (FmaVF  c (Binary a (LoadVector b))));
-  format %{ "fmaps $a,$b,$c\t# $c = $a * $b + $c fma packed16F" %}
+instruct vfmaD_mem(vec a, memory b, vec c) %{
+  match(Set c (FmaVD  c (Binary a (LoadVector b))));
+  format %{ "fmapd $a,$b,$c\t# $c = $a * $b + $c fma packedD" %}
   ins_cost(150);
   ins_encode %{
-    int vector_len = 2;
-    __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vector_len);
+    assert(UseFMA, "not enabled");
+    int vector_len = vector_length_encoding(this);
+    __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
 // --------------------------------- Vector Multiply Add --------------------------------------
 
-instruct smuladd4S2I_reg(vecD dst, vecD src1) %{
-  predicate(UseSSE >= 2 && UseAVX == 0 && n->as_Vector()->length() == 2);
-  match(Set dst (MulAddVS2VI dst src1));
-  format %{ "pmaddwd $dst,$dst,$src1\t! muladd packed4Sto2I" %}
-  ins_encode %{
-    __ pmaddwd($dst$$XMMRegister, $src1$$XMMRegister);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmuladd4S2I_reg(vecD dst, vecD src1, vecD src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 2);
-  match(Set dst (MulAddVS2VI src1 src2));
-  format %{ "vpmaddwd $dst,$src1,$src2\t! muladd packed4Sto2I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct smuladd8S4I_reg(vecX dst, vecX src1) %{
-  predicate(UseSSE >= 2 && UseAVX == 0 && n->as_Vector()->length() == 4);
+instruct vmuladdS2I_reg_sse(vec dst, vec src1) %{
+  predicate(UseAVX == 0);
   match(Set dst (MulAddVS2VI dst src1));
-  format %{ "pmaddwd $dst,$dst,$src1\t! muladd packed8Sto4I" %}
+  format %{ "pmaddwd $dst,$dst,$src1\t! muladd packedStoI" %}
   ins_encode %{
     __ pmaddwd($dst$$XMMRegister, $src1$$XMMRegister);
   %}
   ins_pipe( pipe_slow );
 %}
 
-instruct vmuladd8S4I_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(UseAVX > 0 && n->as_Vector()->length() == 4);
-  match(Set dst (MulAddVS2VI src1 src2));
-  format %{ "vpmaddwd $dst,$src1,$src2\t! muladd packed8Sto4I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmuladd16S8I_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(UseAVX > 1 && n->as_Vector()->length() == 8);
-  match(Set dst (MulAddVS2VI src1 src2));
-  format %{ "vpmaddwd $dst,$src1,$src2\t! muladd packed16Sto8I" %}
-  ins_encode %{
-    int vector_len = 1;
-    __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vmuladd32S16I_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(UseAVX > 2 && n->as_Vector()->length() == 16);
+instruct vmuladdS2I_reg_avx(vec dst, vec src1, vec src2) %{
+  predicate(UseAVX > 0);
   match(Set dst (MulAddVS2VI src1 src2));
-  format %{ "vpmaddwd $dst,$src1,$src2\t! muladd packed32Sto16I" %}
+  format %{ "vpmaddwd $dst,$src1,$src2\t! muladd packedStoI" %}
   ins_encode %{
-    int vector_len = 2;
+    int vector_len = vector_length_encoding(this);
     __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
 
 // --------------------------------- Vector Multiply Add Add ----------------------------------
 
-instruct vmuladdadd4S2I_reg(vecD dst, vecD src1, vecD src2) %{
-  predicate(VM_Version::supports_vnni() && UseAVX > 2 && n->as_Vector()->length() == 2);
-  match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
-  format %{ "evpdpwssd $dst,$src1,$src2\t! muladdadd packed4Sto2I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-  ins_cost(10);
-%}
-
-instruct vmuladdadd8S4I_reg(vecX dst, vecX src1, vecX src2) %{
-  predicate(VM_Version::supports_vnni() && UseAVX > 2 && n->as_Vector()->length() == 4);
-  match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
-  format %{ "evpdpwssd $dst,$src1,$src2\t! muladdadd packed8Sto4I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-  ins_cost(10);
-%}
-
-instruct vmuladdadd16S8I_reg(vecY dst, vecY src1, vecY src2) %{
-  predicate(VM_Version::supports_vnni() && UseAVX > 2 && n->as_Vector()->length() == 8);
+instruct vmuladdaddS2I_reg(vec dst, vec src1, vec src2) %{
+  predicate(VM_Version::supports_vnni());
   match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
-  format %{ "evpdpwssd $dst,$src1,$src2\t! muladdadd packed16Sto8I" %}
+  format %{ "evpdpwssd $dst,$src1,$src2\t! muladdadd packedStoI" %}
   ins_encode %{
-    int vector_len = 1;
-    __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-  ins_cost(10);
-%}
-
-instruct vmuladdadd32S16I_reg(vecZ dst, vecZ src1, vecZ src2) %{
-  predicate(VM_Version::supports_vnni() && UseAVX > 2 && n->as_Vector()->length() == 16);
-  match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
-  format %{ "evpdpwssd $dst,$src1,$src2\t! muladdadd packed32Sto16I" %}
-  ins_encode %{
-    int vector_len = 2;
+    assert(UseAVX > 2, "required");
+    int vector_len = vector_length_encoding(this);
     __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
   ins_cost(10);
 %}
 
 // --------------------------------- PopCount --------------------------------------
 
-instruct vpopcount2I(vecD dst, vecD src) %{
-  predicate(VM_Version::supports_vpopcntdq() && UsePopCountInstruction && n->as_Vector()->length() == 2);
-  match(Set dst (PopCountVI src));
-  format %{ "vpopcntd  $dst,$src\t! vector popcount packed2I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpopcntd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vpopcount4I(vecX dst, vecX src) %{
-  predicate(VM_Version::supports_vpopcntdq() && UsePopCountInstruction && n->as_Vector()->length() == 4);
-  match(Set dst (PopCountVI src));
-  format %{ "vpopcntd  $dst,$src\t! vector popcount packed4I" %}
-  ins_encode %{
-    int vector_len = 0;
-    __ vpopcntd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
-
-instruct vpopcount8I(vecY dst, vecY src) %{
-  predicate(VM_Version::supports_vpopcntdq() && UsePopCountInstruction && n->as_Vector()->length() == 8);
+instruct vpopcountI(vec dst, vec src) %{
   match(Set dst (PopCountVI src));
-  format %{ "vpopcntd  $dst,$src\t! vector popcount packed8I" %}
+  format %{ "vpopcntd  $dst,$src\t! vector popcount packedI" %}
   ins_encode %{
-    int vector_len = 1;
-    __ vpopcntd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
-  %}
-  ins_pipe( pipe_slow );
-%}
+    assert(UsePopCountInstruction, "not enabled");
 
-instruct vpopcount16I(vecZ dst, vecZ src) %{
-  predicate(VM_Version::supports_vpopcntdq() && UsePopCountInstruction && n->as_Vector()->length() == 16);
-  match(Set dst (PopCountVI src));
-  format %{ "vpopcntd  $dst,$src\t! vector popcount packed16I" %}
-  ins_encode %{
-    int vector_len = 2;
+    int vector_len = vector_length_encoding(this);
     __ vpopcntd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
   %}
   ins_pipe( pipe_slow );
 %}
