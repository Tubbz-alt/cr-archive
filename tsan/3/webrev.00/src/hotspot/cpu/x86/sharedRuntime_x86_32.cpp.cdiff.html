<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/cpu/x86/sharedRuntime_x86_32.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="sharedRuntime_x86.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="sharedRuntime_x86_64.cpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/sharedRuntime_x86_32.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 1,7 ***</span>
  /*
<span class="line-modified">!  * Copyright (c) 2003, 2018, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
<span class="line-new-header">--- 1,7 ---</span>
  /*
<span class="line-modified">!  * Copyright (c) 2003, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 28,26 ***</span>
  #include &quot;code/debugInfoRec.hpp&quot;
  #include &quot;code/icBuffer.hpp&quot;
  #include &quot;code/nativeInst.hpp&quot;
  #include &quot;code/vtableStubs.hpp&quot;
  #include &quot;gc/shared/gcLocker.hpp&quot;
  #include &quot;interpreter/interpreter.hpp&quot;
  #include &quot;logging/log.hpp&quot;
  #include &quot;memory/resourceArea.hpp&quot;
  #include &quot;oops/compiledICHolder.hpp&quot;
  #include &quot;runtime/safepointMechanism.hpp&quot;
  #include &quot;runtime/sharedRuntime.hpp&quot;
  #include &quot;runtime/vframeArray.hpp&quot;
  #include &quot;utilities/align.hpp&quot;
  #include &quot;vmreg_x86.inline.hpp&quot;
  #ifdef COMPILER1
  #include &quot;c1/c1_Runtime1.hpp&quot;
  #endif
  #ifdef COMPILER2
  #include &quot;opto/runtime.hpp&quot;
  #endif
<span class="line-removed">- #include &quot;vm_version_x86.hpp&quot;</span>
  
  #define __ masm-&gt;
  
  const int StackAlignmentInSlots = StackAlignmentInBytes / VMRegImpl::stack_slot_size;
  
<span class="line-new-header">--- 28,29 ---</span>
  #include &quot;code/debugInfoRec.hpp&quot;
  #include &quot;code/icBuffer.hpp&quot;
  #include &quot;code/nativeInst.hpp&quot;
  #include &quot;code/vtableStubs.hpp&quot;
  #include &quot;gc/shared/gcLocker.hpp&quot;
<span class="line-added">+ #include &quot;gc/shared/barrierSet.hpp&quot;</span>
<span class="line-added">+ #include &quot;gc/shared/barrierSetAssembler.hpp&quot;</span>
  #include &quot;interpreter/interpreter.hpp&quot;
  #include &quot;logging/log.hpp&quot;
  #include &quot;memory/resourceArea.hpp&quot;
  #include &quot;oops/compiledICHolder.hpp&quot;
<span class="line-added">+ #include &quot;oops/klass.inline.hpp&quot;</span>
  #include &quot;runtime/safepointMechanism.hpp&quot;
  #include &quot;runtime/sharedRuntime.hpp&quot;
  #include &quot;runtime/vframeArray.hpp&quot;
<span class="line-added">+ #include &quot;runtime/vm_version.hpp&quot;</span>
  #include &quot;utilities/align.hpp&quot;
  #include &quot;vmreg_x86.inline.hpp&quot;
  #ifdef COMPILER1
  #include &quot;c1/c1_Runtime1.hpp&quot;
  #endif
  #ifdef COMPILER2
  #include &quot;opto/runtime.hpp&quot;
  #endif
  
  #define __ masm-&gt;
  
  const int StackAlignmentInSlots = StackAlignmentInBytes / VMRegImpl::stack_slot_size;
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 972,10 ***</span>
<span class="line-new-header">--- 975,13 ---</span>
      __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
    }
  
    address c2i_entry = __ pc();
  
<span class="line-added">+   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();</span>
<span class="line-added">+   bs-&gt;c2i_entry_barrier(masm);</span>
<span class="line-added">+ </span>
    gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);
  
    __ flush();
    return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry);
  }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1301,10 ***</span>
<span class="line-new-header">--- 1307,101 ---</span>
        }
      }
    }
  }
  
<span class="line-added">+ // Registers need to be saved for runtime call</span>
<span class="line-added">+ static Register caller_saved_registers[] = {</span>
<span class="line-added">+   rcx, rdx, rsi, rdi</span>
<span class="line-added">+ };</span>
<span class="line-added">+ </span>
<span class="line-added">+ // Save caller saved registers except r1 and r2</span>
<span class="line-added">+ static void save_registers_except(MacroAssembler* masm, Register r1, Register r2) {</span>
<span class="line-added">+   int reg_len = (int)(sizeof(caller_saved_registers) / sizeof(Register));</span>
<span class="line-added">+   for (int index = 0; index &lt; reg_len; index ++) {</span>
<span class="line-added">+     Register this_reg = caller_saved_registers[index];</span>
<span class="line-added">+     if (this_reg != r1 &amp;&amp; this_reg != r2) {</span>
<span class="line-added">+       __ push(this_reg);</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ // Restore caller saved registers except r1 and r2</span>
<span class="line-added">+ static void restore_registers_except(MacroAssembler* masm, Register r1, Register r2) {</span>
<span class="line-added">+   int reg_len = (int)(sizeof(caller_saved_registers) / sizeof(Register));</span>
<span class="line-added">+   for (int index = reg_len - 1; index &gt;= 0; index --) {</span>
<span class="line-added">+     Register this_reg = caller_saved_registers[index];</span>
<span class="line-added">+     if (this_reg != r1 &amp;&amp; this_reg != r2) {</span>
<span class="line-added">+       __ pop(this_reg);</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ // Pin object, return pinned object or null in rax</span>
<span class="line-added">+ static void gen_pin_object(MacroAssembler* masm,</span>
<span class="line-added">+                            Register thread, VMRegPair reg) {</span>
<span class="line-added">+   __ block_comment(&quot;gen_pin_object {&quot;);</span>
<span class="line-added">+ </span>
<span class="line-added">+   Label is_null;</span>
<span class="line-added">+   Register tmp_reg = rax;</span>
<span class="line-added">+   VMRegPair tmp(tmp_reg-&gt;as_VMReg());</span>
<span class="line-added">+   if (reg.first()-&gt;is_stack()) {</span>
<span class="line-added">+     // Load the arg up from the stack</span>
<span class="line-added">+     simple_move32(masm, reg, tmp);</span>
<span class="line-added">+     reg = tmp;</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     __ movl(tmp_reg, reg.first()-&gt;as_Register());</span>
<span class="line-added">+   }</span>
<span class="line-added">+   __ testptr(reg.first()-&gt;as_Register(), reg.first()-&gt;as_Register());</span>
<span class="line-added">+   __ jccb(Assembler::equal, is_null);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Save registers that may be used by runtime call</span>
<span class="line-added">+   Register arg = reg.first()-&gt;is_Register() ? reg.first()-&gt;as_Register() : noreg;</span>
<span class="line-added">+   save_registers_except(masm, arg, thread);</span>
<span class="line-added">+ </span>
<span class="line-added">+   __ call_VM_leaf(</span>
<span class="line-added">+     CAST_FROM_FN_PTR(address, SharedRuntime::pin_object),</span>
<span class="line-added">+     thread, reg.first()-&gt;as_Register());</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Restore saved registers</span>
<span class="line-added">+   restore_registers_except(masm, arg, thread);</span>
<span class="line-added">+ </span>
<span class="line-added">+   __ bind(is_null);</span>
<span class="line-added">+   __ block_comment(&quot;} gen_pin_object&quot;);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ // Unpin object</span>
<span class="line-added">+ static void gen_unpin_object(MacroAssembler* masm,</span>
<span class="line-added">+                              Register thread, VMRegPair reg) {</span>
<span class="line-added">+   __ block_comment(&quot;gen_unpin_object {&quot;);</span>
<span class="line-added">+   Label is_null;</span>
<span class="line-added">+ </span>
<span class="line-added">+   // temp register</span>
<span class="line-added">+   __ push(rax);</span>
<span class="line-added">+   Register tmp_reg = rax;</span>
<span class="line-added">+   VMRegPair tmp(tmp_reg-&gt;as_VMReg());</span>
<span class="line-added">+ </span>
<span class="line-added">+   simple_move32(masm, reg, tmp);</span>
<span class="line-added">+ </span>
<span class="line-added">+   __ testptr(rax, rax);</span>
<span class="line-added">+   __ jccb(Assembler::equal, is_null);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Save registers that may be used by runtime call</span>
<span class="line-added">+   Register arg = reg.first()-&gt;is_Register() ? reg.first()-&gt;as_Register() : noreg;</span>
<span class="line-added">+   save_registers_except(masm, arg, thread);</span>
<span class="line-added">+ </span>
<span class="line-added">+   __ call_VM_leaf(</span>
<span class="line-added">+     CAST_FROM_FN_PTR(address, SharedRuntime::unpin_object),</span>
<span class="line-added">+     thread, rax);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Restore saved registers</span>
<span class="line-added">+   restore_registers_except(masm, arg, thread);</span>
<span class="line-added">+   __ bind(is_null);</span>
<span class="line-added">+   __ pop(rax);</span>
<span class="line-added">+   __ block_comment(&quot;} gen_unpin_object&quot;);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
  // Check GCLocker::needs_gc and enter the runtime if it&#39;s true.  This
  // keeps a new JNI critical region from starting until a GC has been
  // forced.  Save down any oops in registers and describe them in an
  // OopMap.
  static void check_needs_gc_for_critical_native(MacroAssembler* masm,
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1414,12 ***</span>
                              const BasicType* sig_bt,
                              const VMRegPair* regs) {
    Register temp_reg = rbx;  // not part of any compiled calling seq
    if (VerifyOops) {
      for (int i = 0; i &lt; method-&gt;size_of_parameters(); i++) {
<span class="line-modified">!       if (sig_bt[i] == T_OBJECT ||</span>
<span class="line-removed">-           sig_bt[i] == T_ARRAY) {</span>
          VMReg r = regs[i].first();
          assert(r-&gt;is_valid(), &quot;bad oop arg&quot;);
          if (r-&gt;is_stack()) {
            __ movptr(temp_reg, Address(rsp, r-&gt;reg2stack() * VMRegImpl::stack_slot_size + wordSize));
            __ verify_oop(temp_reg);
<span class="line-new-header">--- 1511,11 ---</span>
                              const BasicType* sig_bt,
                              const VMRegPair* regs) {
    Register temp_reg = rbx;  // not part of any compiled calling seq
    if (VerifyOops) {
      for (int i = 0; i &lt; method-&gt;size_of_parameters(); i++) {
<span class="line-modified">!       if (is_reference_type(sig_bt[i])) {</span>
          VMReg r = regs[i].first();
          assert(r-&gt;is_valid(), &quot;bad oop arg&quot;);
          if (r-&gt;is_stack()) {
            __ movptr(temp_reg, Address(rsp, r-&gt;reg2stack() * VMRegImpl::stack_slot_size + wordSize));
            __ verify_oop(temp_reg);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1522,11 ***</span>
  nmethod* SharedRuntime::generate_native_wrapper(MacroAssembler* masm,
                                                  const methodHandle&amp; method,
                                                  int compile_id,
                                                  BasicType* in_sig_bt,
                                                  VMRegPair* in_regs,
<span class="line-modified">!                                                 BasicType ret_type) {</span>
    if (method-&gt;is_method_handle_intrinsic()) {
      vmIntrinsics::ID iid = method-&gt;intrinsic_id();
      intptr_t start = (intptr_t)__ pc();
      int vep_offset = ((intptr_t)__ pc()) - start;
      gen_special_dispatch(masm,
<span class="line-new-header">--- 1618,12 ---</span>
  nmethod* SharedRuntime::generate_native_wrapper(MacroAssembler* masm,
                                                  const methodHandle&amp; method,
                                                  int compile_id,
                                                  BasicType* in_sig_bt,
                                                  VMRegPair* in_regs,
<span class="line-modified">!                                                 BasicType ret_type,</span>
<span class="line-added">+                                                 address critical_entry) {</span>
    if (method-&gt;is_method_handle_intrinsic()) {
      vmIntrinsics::ID iid = method-&gt;intrinsic_id();
      intptr_t start = (intptr_t)__ pc();
      int vep_offset = ((intptr_t)__ pc()) - start;
      gen_special_dispatch(masm,
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1545,11 ***</span>
                                         in_ByteSize(-1),
                                         in_ByteSize(-1),
                                         (OopMapSet*)NULL);
    }
    bool is_critical_native = true;
<span class="line-modified">!   address native_func = method-&gt;critical_native_function();</span>
    if (native_func == NULL) {
      native_func = method-&gt;native_function();
      is_critical_native = false;
    }
    assert(native_func != NULL, &quot;must have function&quot;);
<span class="line-new-header">--- 1642,11 ---</span>
                                         in_ByteSize(-1),
                                         in_ByteSize(-1),
                                         (OopMapSet*)NULL);
    }
    bool is_critical_native = true;
<span class="line-modified">!   address native_func = critical_entry;</span>
    if (native_func == NULL) {
      native_func = method-&gt;native_function();
      is_critical_native = false;
    }
    assert(native_func != NULL, &quot;must have function&quot;);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1591,40 ***</span>
  
      for (int i = 0; i &lt; total_in_args ; i++ ) {
        out_sig_bt[argc++] = in_sig_bt[i];
      }
    } else {
<span class="line-removed">-     Thread* THREAD = Thread::current();</span>
      in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_in_args);
      SignatureStream ss(method-&gt;signature());
      for (int i = 0; i &lt; total_in_args ; i++ ) {
        if (in_sig_bt[i] == T_ARRAY) {
          // Arrays are passed as int, elem* pair
          out_sig_bt[argc++] = T_INT;
          out_sig_bt[argc++] = T_ADDRESS;
<span class="line-modified">!         Symbol* atype = ss.as_symbol(CHECK_NULL);</span>
<span class="line-modified">!         const char* at = atype-&gt;as_C_string();</span>
<span class="line-modified">!         if (strlen(at) == 2) {</span>
<span class="line-removed">-           assert(at[0] == &#39;[&#39;, &quot;must be&quot;);</span>
<span class="line-removed">-           switch (at[1]) {</span>
<span class="line-removed">-             case &#39;B&#39;: in_elem_bt[i]  = T_BYTE; break;</span>
<span class="line-removed">-             case &#39;C&#39;: in_elem_bt[i]  = T_CHAR; break;</span>
<span class="line-removed">-             case &#39;D&#39;: in_elem_bt[i]  = T_DOUBLE; break;</span>
<span class="line-removed">-             case &#39;F&#39;: in_elem_bt[i]  = T_FLOAT; break;</span>
<span class="line-removed">-             case &#39;I&#39;: in_elem_bt[i]  = T_INT; break;</span>
<span class="line-removed">-             case &#39;J&#39;: in_elem_bt[i]  = T_LONG; break;</span>
<span class="line-removed">-             case &#39;S&#39;: in_elem_bt[i]  = T_SHORT; break;</span>
<span class="line-removed">-             case &#39;Z&#39;: in_elem_bt[i]  = T_BOOLEAN; break;</span>
<span class="line-removed">-             default: ShouldNotReachHere();</span>
<span class="line-removed">-           }</span>
<span class="line-removed">-         }</span>
        } else {
          out_sig_bt[argc++] = in_sig_bt[i];
          in_elem_bt[i] = T_VOID;
        }
        if (in_sig_bt[i] != T_VOID) {
<span class="line-modified">!         assert(in_sig_bt[i] == ss.type(), &quot;must match&quot;);</span>
          ss.next();
        }
      }
    }
  
<span class="line-new-header">--- 1688,27 ---</span>
  
      for (int i = 0; i &lt; total_in_args ; i++ ) {
        out_sig_bt[argc++] = in_sig_bt[i];
      }
    } else {
      in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_in_args);
      SignatureStream ss(method-&gt;signature());
      for (int i = 0; i &lt; total_in_args ; i++ ) {
        if (in_sig_bt[i] == T_ARRAY) {
          // Arrays are passed as int, elem* pair
          out_sig_bt[argc++] = T_INT;
          out_sig_bt[argc++] = T_ADDRESS;
<span class="line-modified">!         ss.skip_array_prefix(1);  // skip one &#39;[&#39;</span>
<span class="line-modified">!         assert(ss.is_primitive(), &quot;primitive type expected&quot;);</span>
<span class="line-modified">!         in_elem_bt[i] = ss.type();</span>
        } else {
          out_sig_bt[argc++] = in_sig_bt[i];
          in_elem_bt[i] = T_VOID;
        }
        if (in_sig_bt[i] != T_VOID) {
<span class="line-modified">!         assert(in_sig_bt[i] == ss.type() ||</span>
<span class="line-added">+                in_sig_bt[i] == T_ARRAY, &quot;must match&quot;);</span>
          ss.next();
        }
      }
    }
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1793,10 ***</span>
<span class="line-new-header">--- 1877,14 ---</span>
    // Generate a new frame for the wrapper.
    __ enter();
    // -2 because return address is already present and so is saved rbp
    __ subptr(rsp, stack_size - 2*wordSize);
  
<span class="line-added">+ </span>
<span class="line-added">+   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();</span>
<span class="line-added">+   bs-&gt;nmethod_entry_barrier(masm);</span>
<span class="line-added">+ </span>
    // Frame is now completed as far as size and linkage.
    int frame_complete = ((intptr_t)__ pc()) - start;
  
    if (UseRTMLocking) {
      // Abort RTM transaction before calling JNI
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1828,18 ***</span>
  
    // We use rdi as a thread pointer because it is callee save and
    // if we load it once it is usable thru the entire wrapper
    const Register thread = rdi;
  
<span class="line-modified">!   // We use rsi as the oop handle for the receiver/klass</span>
<span class="line-modified">!   // It is callee save so it survives the call to native</span>
  
<span class="line-modified">!   const Register oop_handle_reg = rsi;</span>
  
<span class="line-modified">!   __ get_thread(thread);</span>
  
<span class="line-modified">!   if (is_critical_native) {</span>
      check_needs_gc_for_critical_native(masm, thread, stack_slots, total_c_args, total_in_args,
                                         oop_handle_offset, oop_maps, in_regs, in_sig_bt);
    }
  
    //
<span class="line-new-header">--- 1916,18 ---</span>
  
    // We use rdi as a thread pointer because it is callee save and
    // if we load it once it is usable thru the entire wrapper
    const Register thread = rdi;
  
<span class="line-modified">!    // We use rsi as the oop handle for the receiver/klass</span>
<span class="line-modified">!    // It is callee save so it survives the call to native</span>
  
<span class="line-modified">!    const Register oop_handle_reg = rsi;</span>
  
<span class="line-modified">!    __ get_thread(thread);</span>
  
<span class="line-modified">!   if (is_critical_native &amp;&amp; !Universe::heap()-&gt;supports_object_pinning()) {</span>
      check_needs_gc_for_critical_native(masm, thread, stack_slots, total_c_args, total_in_args,
                                         oop_handle_offset, oop_maps, in_regs, in_sig_bt);
    }
  
    //
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1873,10 ***</span>
<span class="line-new-header">--- 1961,15 ---</span>
    // sure we can capture all the incoming oop args from the
    // caller.
    //
    OopMap* map = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
  
<span class="line-added">+   // Inbound arguments that need to be pinned for critical natives</span>
<span class="line-added">+   GrowableArray&lt;int&gt; pinned_args(total_in_args);</span>
<span class="line-added">+   // Current stack slot for storing register based array argument</span>
<span class="line-added">+   int pinned_slot = oop_handle_offset;</span>
<span class="line-added">+ </span>
    // Mark location of rbp,
    // map-&gt;set_callee_saved(VMRegImpl::stack2reg( stack_slots - 2), stack_slots * 2, 0, rbp-&gt;as_VMReg());
  
    // We know that we only have args in at most two integer registers (rcx, rdx). So rax, rbx
    // Are free to temporaries if we have to do  stack to steck moves.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1884,11 ***</span>
  
    for (int i = 0; i &lt; total_in_args ; i++, c_arg++ ) {
      switch (in_sig_bt[i]) {
        case T_ARRAY:
          if (is_critical_native) {
<span class="line-modified">!           unpack_array_argument(masm, in_regs[i], in_elem_bt[i], out_regs[c_arg + 1], out_regs[c_arg]);</span>
            c_arg++;
            break;
          }
        case T_OBJECT:
          assert(!is_critical_native, &quot;no oop arguments&quot;);
<span class="line-new-header">--- 1977,32 ---</span>
  
    for (int i = 0; i &lt; total_in_args ; i++, c_arg++ ) {
      switch (in_sig_bt[i]) {
        case T_ARRAY:
          if (is_critical_native) {
<span class="line-modified">!           VMRegPair in_arg = in_regs[i];</span>
<span class="line-added">+           if (Universe::heap()-&gt;supports_object_pinning()) {</span>
<span class="line-added">+             // gen_pin_object handles save and restore</span>
<span class="line-added">+             // of any clobbered registers</span>
<span class="line-added">+             gen_pin_object(masm, thread, in_arg);</span>
<span class="line-added">+             pinned_args.append(i);</span>
<span class="line-added">+ </span>
<span class="line-added">+             // rax has pinned array</span>
<span class="line-added">+             VMRegPair result_reg(rax-&gt;as_VMReg());</span>
<span class="line-added">+             if (!in_arg.first()-&gt;is_stack()) {</span>
<span class="line-added">+               assert(pinned_slot &lt;= stack_slots, &quot;overflow&quot;);</span>
<span class="line-added">+               simple_move32(masm, result_reg, VMRegImpl::stack2reg(pinned_slot));</span>
<span class="line-added">+               pinned_slot += VMRegImpl::slots_per_word;</span>
<span class="line-added">+             } else {</span>
<span class="line-added">+               // Write back pinned value, it will be used to unpin this argument</span>
<span class="line-added">+               __ movptr(Address(rbp, reg2offset_in(in_arg.first())), result_reg.first()-&gt;as_Register());</span>
<span class="line-added">+             }</span>
<span class="line-added">+             // We have the array in register, use it</span>
<span class="line-added">+             in_arg = result_reg;</span>
<span class="line-added">+           }</span>
<span class="line-added">+ </span>
<span class="line-added">+           unpack_array_argument(masm, in_arg, in_elem_bt[i], out_regs[c_arg + 1], out_regs[c_arg]);</span>
            c_arg++;
            break;
          }
        case T_OBJECT:
          assert(!is_critical_native, &quot;no oop arguments&quot;);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2077,10 ***</span>
<span class="line-new-header">--- 2191,30 ---</span>
    case T_VOID: break;
    case T_LONG: break;
    default       : ShouldNotReachHere();
    }
  
<span class="line-added">+   // unpin pinned arguments</span>
<span class="line-added">+   pinned_slot = oop_handle_offset;</span>
<span class="line-added">+   if (pinned_args.length() &gt; 0) {</span>
<span class="line-added">+     // save return value that may be overwritten otherwise.</span>
<span class="line-added">+     save_native_result(masm, ret_type, stack_slots);</span>
<span class="line-added">+     for (int index = 0; index &lt; pinned_args.length(); index ++) {</span>
<span class="line-added">+       int i = pinned_args.at(index);</span>
<span class="line-added">+       assert(pinned_slot &lt;= stack_slots, &quot;overflow&quot;);</span>
<span class="line-added">+       if (!in_regs[i].first()-&gt;is_stack()) {</span>
<span class="line-added">+         int offset = pinned_slot * VMRegImpl::stack_slot_size;</span>
<span class="line-added">+         __ movl(in_regs[i].first()-&gt;as_Register(), Address(rsp, offset));</span>
<span class="line-added">+         pinned_slot += VMRegImpl::slots_per_word;</span>
<span class="line-added">+       }</span>
<span class="line-added">+       // gen_pin_object handles save and restore</span>
<span class="line-added">+       // of any other clobbered registers</span>
<span class="line-added">+       gen_unpin_object(masm, thread, in_regs[i]);</span>
<span class="line-added">+     }</span>
<span class="line-added">+     restore_native_result(masm, ret_type, stack_slots);</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
    // Switch thread to &quot;native transition&quot; state before reading the synchronization state.
    // This additional state is necessary because reading and testing the synchronization
    // state is not atomic w.r.t. GC, as this scenario demonstrates:
    //     Java thread A, in _thread_in_native state, loads _not_synchronized and is preempted.
    //     VM thread changes sync state to synchronizing and suspends threads for GC.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2216,11 ***</span>
    // We can finally stop using that last_Java_frame we setup ages ago
  
    __ reset_last_Java_frame(thread, false);
  
    // Unbox oop result, e.g. JNIHandles::resolve value.
<span class="line-modified">!   if (ret_type == T_OBJECT || ret_type == T_ARRAY) {</span>
      __ resolve_jobject(rax /* value */,
                         thread /* thread */,
                         rcx /* tmp */);
    }
  
<span class="line-new-header">--- 2350,11 ---</span>
    // We can finally stop using that last_Java_frame we setup ages ago
  
    __ reset_last_Java_frame(thread, false);
  
    // Unbox oop result, e.g. JNIHandles::resolve value.
<span class="line-modified">!   if (is_reference_type(ret_type)) {</span>
      __ resolve_jobject(rax /* value */,
                         thread /* thread */,
                         rcx /* tmp */);
    }
  
</pre>
<center><a href="sharedRuntime_x86.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="sharedRuntime_x86_64.cpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>