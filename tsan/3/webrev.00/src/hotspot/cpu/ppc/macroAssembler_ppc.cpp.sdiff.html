<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/ppc/macroAssembler_ppc.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="jniFastGetField_ppc.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_ppc.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/ppc/macroAssembler_ppc.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;compiler/disassembler.hpp&quot;
  29 #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
  30 #include &quot;gc/shared/barrierSet.hpp&quot;
  31 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  32 #include &quot;interpreter/interpreter.hpp&quot;
  33 #include &quot;memory/resourceArea.hpp&quot;
  34 #include &quot;nativeInst_ppc.hpp&quot;

  35 #include &quot;prims/methodHandles.hpp&quot;
  36 #include &quot;runtime/biasedLocking.hpp&quot;
  37 #include &quot;runtime/icache.hpp&quot;
  38 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  39 #include &quot;runtime/objectMonitor.hpp&quot;
  40 #include &quot;runtime/os.hpp&quot;
  41 #include &quot;runtime/safepoint.hpp&quot;
  42 #include &quot;runtime/safepointMechanism.hpp&quot;
  43 #include &quot;runtime/sharedRuntime.hpp&quot;
  44 #include &quot;runtime/stubRoutines.hpp&quot;
  45 #include &quot;utilities/macros.hpp&quot;

  46 #ifdef COMPILER2
  47 #include &quot;opto/intrinsicnode.hpp&quot;
  48 #endif
  49 
  50 #ifdef PRODUCT
  51 #define BLOCK_COMMENT(str) // nothing
  52 #else
  53 #define BLOCK_COMMENT(str) block_comment(str)
  54 #endif
  55 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  56 
  57 #ifdef ASSERT
  58 // On RISC, there&#39;s no benefit to verifying instruction boundaries.
  59 bool AbstractAssembler::pd_check_instruction_mark() { return false; }
  60 #endif
  61 
  62 void MacroAssembler::ld_largeoffset_unchecked(Register d, int si31, Register a, int emit_filler_nop) {
  63   assert(Assembler::is_simm(si31, 31) &amp;&amp; si31 &gt;= 0, &quot;si31 out of range&quot;);
  64   if (Assembler::is_simm(si31, 16)) {
  65     ld(d, si31, a);
</pre>
<hr />
<pre>
1994   std(super_klass, target_offset, sub_klass); // save result to cache
1995   if (result_reg != noreg) { li(result_reg, 0); } // load zero result (indicates a hit)
1996   if (L_success != NULL) { b(*L_success); }
1997   else if (result_reg == noreg) { blr(); } // return with CR0.eq if neither label nor result reg provided
1998 
1999   bind(fallthru);
2000 }
2001 
2002 // Try fast path, then go to slow one if not successful
2003 void MacroAssembler::check_klass_subtype(Register sub_klass,
2004                          Register super_klass,
2005                          Register temp1_reg,
2006                          Register temp2_reg,
2007                          Label&amp; L_success) {
2008   Label L_failure;
2009   check_klass_subtype_fast_path(sub_klass, super_klass, temp1_reg, temp2_reg, &amp;L_success, &amp;L_failure);
2010   check_klass_subtype_slow_path(sub_klass, super_klass, temp1_reg, temp2_reg, &amp;L_success);
2011   bind(L_failure); // Fallthru if not successful.
2012 }
2013 
<span class="line-modified">2014 void MacroAssembler::check_method_handle_type(Register mtype_reg, Register mh_reg,</span>
<span class="line-modified">2015                                               Register temp_reg,</span>
<span class="line-modified">2016                                               Label&amp; wrong_method_type) {</span>
<span class="line-modified">2017   assert_different_registers(mtype_reg, mh_reg, temp_reg);</span>
<span class="line-modified">2018   // Compare method type against that of the receiver.</span>
<span class="line-modified">2019   load_heap_oop(temp_reg, delayed_value(java_lang_invoke_MethodHandle::type_offset_in_bytes, temp_reg), mh_reg,</span>
<span class="line-modified">2020                 noreg, noreg, false, IS_NOT_NULL);</span>
<span class="line-modified">2021   cmpd(CCR0, temp_reg, mtype_reg);</span>
<span class="line-modified">2022   bne(CCR0, wrong_method_type);</span>


















2023 }
2024 
2025 RegisterOrConstant MacroAssembler::argument_offset(RegisterOrConstant arg_slot,
2026                                                    Register temp_reg,
2027                                                    int extra_slot_offset) {
2028   // cf. TemplateTable::prepare_invoke(), if (load_receiver).
2029   int stackElementSize = Interpreter::stackElementSize;
2030   int offset = extra_slot_offset * stackElementSize;
2031   if (arg_slot.is_constant()) {
2032     offset += arg_slot.as_constant() * stackElementSize;
2033     return offset;
2034   } else {
2035     assert(temp_reg != noreg, &quot;must specify&quot;);
2036     sldi(temp_reg, arg_slot.as_register(), exact_log2(stackElementSize));
2037     if (offset != 0)
2038       addi(temp_reg, temp_reg, offset);
2039     return temp_reg;
2040   }
2041 }
2042 
2043 // Supports temp2_reg = R0.
2044 void MacroAssembler::biased_locking_enter(ConditionRegister cr_reg, Register obj_reg,
2045                                           Register mark_reg, Register temp_reg,
2046                                           Register temp2_reg, Label&amp; done, Label* slow_case) {
2047   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
2048 
2049 #ifdef ASSERT
2050   assert_different_registers(obj_reg, mark_reg, temp_reg, temp2_reg);
2051 #endif
2052 
2053   Label cas_label;
2054 
2055   // Branch to done if fast path fails and no slow_case provided.
2056   Label *slow_case_int = (slow_case != NULL) ? slow_case : &amp;done;
2057 
2058   // Biased locking
2059   // See whether the lock is currently biased toward our thread and
2060   // whether the epoch is still valid
2061   // Note that the runtime guarantees sufficient alignment of JavaThread
2062   // pointers to allow age to be placed into low bits
<span class="line-modified">2063   assert(markOopDesc::age_shift == markOopDesc::lock_bits + markOopDesc::biased_lock_bits,</span>
2064          &quot;biased locking makes assumptions about bit layout&quot;);
2065 
2066   if (PrintBiasedLockingStatistics) {
2067     load_const(temp2_reg, (address) BiasedLocking::total_entry_count_addr(), temp_reg);
2068     lwzx(temp_reg, temp2_reg);
2069     addi(temp_reg, temp_reg, 1);
2070     stwx(temp_reg, temp2_reg);
2071   }
2072 
<span class="line-modified">2073   andi(temp_reg, mark_reg, markOopDesc::biased_lock_mask_in_place);</span>
<span class="line-modified">2074   cmpwi(cr_reg, temp_reg, markOopDesc::biased_lock_pattern);</span>
2075   bne(cr_reg, cas_label);
2076 
2077   load_klass(temp_reg, obj_reg);
2078 
<span class="line-modified">2079   load_const_optimized(temp2_reg, ~((int) markOopDesc::age_mask_in_place));</span>
2080   ld(temp_reg, in_bytes(Klass::prototype_header_offset()), temp_reg);
2081   orr(temp_reg, R16_thread, temp_reg);
2082   xorr(temp_reg, mark_reg, temp_reg);
2083   andr(temp_reg, temp_reg, temp2_reg);
2084   cmpdi(cr_reg, temp_reg, 0);
2085   if (PrintBiasedLockingStatistics) {
2086     Label l;
2087     bne(cr_reg, l);
2088     load_const(temp2_reg, (address) BiasedLocking::biased_lock_entry_count_addr());
2089     lwzx(mark_reg, temp2_reg);
2090     addi(mark_reg, mark_reg, 1);
2091     stwx(mark_reg, temp2_reg);
2092     // restore mark_reg
2093     ld(mark_reg, oopDesc::mark_offset_in_bytes(), obj_reg);
2094     bind(l);
2095   }
2096   beq(cr_reg, done);
2097 
2098   Label try_revoke_bias;
2099   Label try_rebias;
2100 
2101   // At this point we know that the header has the bias pattern and
2102   // that we are not the bias owner in the current epoch. We need to
2103   // figure out more details about the state of the header in order to
2104   // know what operations can be legally performed on the object&#39;s
2105   // header.
2106 
2107   // If the low three bits in the xor result aren&#39;t clear, that means
2108   // the prototype header is no longer biased and we have to revoke
2109   // the bias on this object.
<span class="line-modified">2110   andi(temp2_reg, temp_reg, markOopDesc::biased_lock_mask_in_place);</span>
2111   cmpwi(cr_reg, temp2_reg, 0);
2112   bne(cr_reg, try_revoke_bias);
2113 
2114   // Biasing is still enabled for this data type. See whether the
2115   // epoch of the current bias is still valid, meaning that the epoch
2116   // bits of the mark word are equal to the epoch bits of the
2117   // prototype header. (Note that the prototype header&#39;s epoch bits
2118   // only change at a safepoint.) If not, attempt to rebias the object
2119   // toward the current thread. Note that we must be absolutely sure
2120   // that the current epoch is invalid in order to do this because
2121   // otherwise the manipulations it performs on the mark word are
2122   // illegal.
2123 
<span class="line-modified">2124   int shift_amount = 64 - markOopDesc::epoch_shift;</span>
2125   // rotate epoch bits to right (little) end and set other bits to 0
2126   // [ big part | epoch | little part ] -&gt; [ 0..0 | epoch ]
<span class="line-modified">2127   rldicl_(temp2_reg, temp_reg, shift_amount, 64 - markOopDesc::epoch_bits);</span>
2128   // branch if epoch bits are != 0, i.e. they differ, because the epoch has been incremented
2129   bne(CCR0, try_rebias);
2130 
2131   // The epoch of the current bias is still valid but we know nothing
2132   // about the owner; it might be set or it might be clear. Try to
2133   // acquire the bias of the object using an atomic operation. If this
2134   // fails we will go in to the runtime to revoke the object&#39;s bias.
2135   // Note that we first construct the presumed unbiased header so we
2136   // don&#39;t accidentally blow away another thread&#39;s valid bias.
<span class="line-modified">2137   andi(mark_reg, mark_reg, (markOopDesc::biased_lock_mask_in_place |</span>
<span class="line-modified">2138                                 markOopDesc::age_mask_in_place |</span>
<span class="line-modified">2139                                 markOopDesc::epoch_mask_in_place));</span>
2140   orr(temp_reg, R16_thread, mark_reg);
2141 
2142   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
2143 
2144   // CmpxchgX sets cr_reg to cmpX(temp2_reg, mark_reg).
2145   cmpxchgd(/*flag=*/cr_reg, /*current_value=*/temp2_reg,
2146            /*compare_value=*/mark_reg, /*exchange_value=*/temp_reg,
2147            /*where=*/obj_reg,
2148            MacroAssembler::MemBarAcq,
2149            MacroAssembler::cmpxchgx_hint_acquire_lock(),
2150            noreg, slow_case_int); // bail out if failed
2151 
2152   // If the biasing toward our thread failed, this means that
2153   // another thread succeeded in biasing it toward itself and we
2154   // need to revoke that bias. The revocation will occur in the
2155   // interpreter runtime in the slow case.
2156   if (PrintBiasedLockingStatistics) {
2157     load_const(temp2_reg, (address) BiasedLocking::anonymously_biased_lock_entry_count_addr(), temp_reg);
2158     lwzx(temp_reg, temp2_reg);
2159     addi(temp_reg, temp_reg, 1);
2160     stwx(temp_reg, temp2_reg);
2161   }
2162   b(done);
2163 
2164   bind(try_rebias);
2165   // At this point we know the epoch has expired, meaning that the
2166   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
2167   // circumstances _only_, we are allowed to use the current header&#39;s
2168   // value as the comparison value when doing the cas to acquire the
2169   // bias in the current epoch. In other words, we allow transfer of
2170   // the bias from one thread to another directly in this situation.
2171   load_klass(temp_reg, obj_reg);
<span class="line-modified">2172   andi(temp2_reg, mark_reg, markOopDesc::age_mask_in_place);</span>
2173   orr(temp2_reg, R16_thread, temp2_reg);
2174   ld(temp_reg, in_bytes(Klass::prototype_header_offset()), temp_reg);
2175   orr(temp_reg, temp2_reg, temp_reg);
2176 
2177   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
2178 
2179   cmpxchgd(/*flag=*/cr_reg, /*current_value=*/temp2_reg,
2180                  /*compare_value=*/mark_reg, /*exchange_value=*/temp_reg,
2181                  /*where=*/obj_reg,
2182                  MacroAssembler::MemBarAcq,
2183                  MacroAssembler::cmpxchgx_hint_acquire_lock(),
2184                  noreg, slow_case_int); // bail out if failed
2185 
2186   // If the biasing toward our thread failed, this means that
2187   // another thread succeeded in biasing it toward itself and we
2188   // need to revoke that bias. The revocation will occur in the
2189   // interpreter runtime in the slow case.
2190   if (PrintBiasedLockingStatistics) {
2191     load_const(temp2_reg, (address) BiasedLocking::rebiased_lock_entry_count_addr(), temp_reg);
2192     lwzx(temp_reg, temp2_reg);
2193     addi(temp_reg, temp_reg, 1);
2194     stwx(temp_reg, temp2_reg);
2195   }
2196   b(done);
2197 
2198   bind(try_revoke_bias);
2199   // The prototype mark in the klass doesn&#39;t have the bias bit set any
2200   // more, indicating that objects of this data type are not supposed
2201   // to be biased any more. We are going to try to reset the mark of
2202   // this object to the prototype value and fall through to the
2203   // CAS-based locking scheme. Note that if our CAS fails, it means
2204   // that another thread raced us for the privilege of revoking the
2205   // bias of this particular object, so it&#39;s okay to continue in the
2206   // normal locking code.
2207   load_klass(temp_reg, obj_reg);
2208   ld(temp_reg, in_bytes(Klass::prototype_header_offset()), temp_reg);
<span class="line-modified">2209   andi(temp2_reg, mark_reg, markOopDesc::age_mask_in_place);</span>
2210   orr(temp_reg, temp_reg, temp2_reg);
2211 
2212   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
2213 
2214   // CmpxchgX sets cr_reg to cmpX(temp2_reg, mark_reg).
2215   cmpxchgd(/*flag=*/cr_reg, /*current_value=*/temp2_reg,
2216                  /*compare_value=*/mark_reg, /*exchange_value=*/temp_reg,
2217                  /*where=*/obj_reg,
2218                  MacroAssembler::MemBarAcq,
2219                  MacroAssembler::cmpxchgx_hint_acquire_lock());
2220 
<span class="line-modified">2221   // reload markOop in mark_reg before continuing with lightweight locking</span>
2222   ld(mark_reg, oopDesc::mark_offset_in_bytes(), obj_reg);
2223 
2224   // Fall through to the normal CAS-based lock, because no matter what
2225   // the result of the above CAS, some thread must have succeeded in
2226   // removing the bias bit from the object&#39;s header.
2227   if (PrintBiasedLockingStatistics) {
2228     Label l;
2229     bne(cr_reg, l);
2230     load_const(temp2_reg, (address) BiasedLocking::revoked_lock_entry_count_addr(), temp_reg);
2231     lwzx(temp_reg, temp2_reg);
2232     addi(temp_reg, temp_reg, 1);
2233     stwx(temp_reg, temp2_reg);
2234     bind(l);
2235   }
2236 
2237   bind(cas_label);
2238 }
2239 
2240 void MacroAssembler::biased_locking_exit (ConditionRegister cr_reg, Register mark_addr, Register temp_reg, Label&amp; done) {
2241   // Check for biased locking unlock case, which is a no-op
2242   // Note: we do not have to check the thread ID for two reasons.
2243   // First, the interpreter checks for IllegalMonitorStateException at
2244   // a higher level. Second, if the bias was revoked while we held the
2245   // lock, the object could not be rebiased toward another thread, so
2246   // the bias bit would be clear.
2247 
2248   ld(temp_reg, 0, mark_addr);
<span class="line-modified">2249   andi(temp_reg, temp_reg, markOopDesc::biased_lock_mask_in_place);</span>
2250 
<span class="line-modified">2251   cmpwi(cr_reg, temp_reg, markOopDesc::biased_lock_pattern);</span>
2252   beq(cr_reg, done);
2253 }
2254 
2255 // allocation (for C1)
2256 void MacroAssembler::eden_allocate(
2257   Register obj,                      // result: pointer to object after successful allocation
2258   Register var_size_in_bytes,        // object size in bytes if unknown at compile time; invalid otherwise
2259   int      con_size_in_bytes,        // object size in bytes if   known at compile time
2260   Register t1,                       // temp register
2261   Register t2,                       // temp register
2262   Label&amp;   slow_case                 // continuation point if fast allocation fails
2263 ) {
2264   b(slow_case);
2265 }
2266 
2267 void MacroAssembler::tlab_allocate(
2268   Register obj,                      // result: pointer to object after successful allocation
2269   Register var_size_in_bytes,        // object size in bytes if unknown at compile time; invalid otherwise
2270   int      con_size_in_bytes,        // object size in bytes if   known at compile time
2271   Register t1,                       // temp register
</pre>
<hr />
<pre>
2652 
2653   bind(doneRetry);
2654 }
2655 
2656 // Use RTM for normal stack locks.
2657 // Input: objReg (object to lock)
2658 void MacroAssembler::rtm_stack_locking(ConditionRegister flag,
2659                                        Register obj, Register mark_word, Register tmp,
2660                                        Register retry_on_abort_count_Reg,
2661                                        RTMLockingCounters* stack_rtm_counters,
2662                                        Metadata* method_data, bool profile_rtm,
2663                                        Label&amp; DONE_LABEL, Label&amp; IsInflated) {
2664   assert(UseRTMForStackLocks, &quot;why call this otherwise?&quot;);
2665   assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
2666   Label L_rtm_retry, L_decrement_retry, L_on_abort;
2667 
2668   if (RTMRetryCount &gt; 0) {
2669     load_const_optimized(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort
2670     bind(L_rtm_retry);
2671   }
<span class="line-modified">2672   andi_(R0, mark_word, markOopDesc::monitor_value);  // inflated vs stack-locked|neutral|biased</span>
2673   bne(CCR0, IsInflated);
2674 
2675   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
2676     Label L_noincrement;
2677     if (RTMTotalCountIncrRate &gt; 1) {
2678       branch_on_random_using_tb(tmp, RTMTotalCountIncrRate, L_noincrement);
2679     }
2680     assert(stack_rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
2681     load_const_optimized(tmp, (address)stack_rtm_counters-&gt;total_count_addr(), R0);
2682     //atomic_inc_ptr(tmp, /*temp, will be reloaded*/mark_word); We don&#39;t increment atomically
2683     ldx(mark_word, tmp);
2684     addi(mark_word, mark_word, 1);
2685     stdx(mark_word, tmp);
2686     bind(L_noincrement);
2687   }
2688   tbegin_();
2689   beq(CCR0, L_on_abort);
<span class="line-modified">2690   ld(mark_word, oopDesc::mark_offset_in_bytes(), obj);         // Reload in transaction, conflicts need to be tracked.</span>
<span class="line-modified">2691   andi(R0, mark_word, markOopDesc::biased_lock_mask_in_place); // look at 3 lock bits</span>
<span class="line-modified">2692   cmpwi(flag, R0, markOopDesc::unlocked_value);                // bits = 001 unlocked</span>
<span class="line-modified">2693   beq(flag, DONE_LABEL);                                       // all done if unlocked</span>
2694 
2695   if (UseRTMXendForLockBusy) {
2696     tend_();
2697     b(L_decrement_retry);
2698   } else {
2699     tabort_();
2700   }
2701   bind(L_on_abort);
2702   const Register abort_status_Reg = tmp;
2703   mftexasr(abort_status_Reg);
2704   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
2705     rtm_profiling(abort_status_Reg, /*temp*/mark_word, stack_rtm_counters, method_data, profile_rtm);
2706   }
2707   ld(mark_word, oopDesc::mark_offset_in_bytes(), obj); // reload
2708   if (RTMRetryCount &gt; 0) {
2709     // Retry on lock abort if abort status is not permanent.
2710     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry, &amp;L_decrement_retry);
2711   } else {
2712     bind(L_decrement_retry);
2713   }
2714 }
2715 
2716 // Use RTM for inflating locks
2717 // inputs: obj       (object to lock)
2718 //         mark_word (current header - KILLED)
2719 //         boxReg    (on-stack box address (displaced header location) - KILLED)
2720 void MacroAssembler::rtm_inflated_locking(ConditionRegister flag,
2721                                           Register obj, Register mark_word, Register boxReg,
2722                                           Register retry_on_busy_count_Reg, Register retry_on_abort_count_Reg,
2723                                           RTMLockingCounters* rtm_counters,
2724                                           Metadata* method_data, bool profile_rtm,
2725                                           Label&amp; DONE_LABEL) {
2726   assert(UseRTMLocking, &quot;why call this otherwise?&quot;);
2727   Label L_rtm_retry, L_decrement_retry, L_on_abort;
2728   // Clean monitor_value bit to get valid pointer.
<span class="line-modified">2729   int owner_offset = ObjectMonitor::owner_offset_in_bytes() - markOopDesc::monitor_value;</span>
2730 
<span class="line-modified">2731   // Store non-null, using boxReg instead of (intptr_t)markOopDesc::unused_mark().</span>
2732   std(boxReg, BasicLock::displaced_header_offset_in_bytes(), boxReg);
2733   const Register tmpReg = boxReg;
2734   const Register owner_addr_Reg = mark_word;
2735   addi(owner_addr_Reg, mark_word, owner_offset);
2736 
2737   if (RTMRetryCount &gt; 0) {
2738     load_const_optimized(retry_on_busy_count_Reg, RTMRetryCount);  // Retry on lock busy.
2739     load_const_optimized(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort.
2740     bind(L_rtm_retry);
2741   }
2742   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
2743     Label L_noincrement;
2744     if (RTMTotalCountIncrRate &gt; 1) {
2745       branch_on_random_using_tb(R0, RTMTotalCountIncrRate, L_noincrement);
2746     }
2747     assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
2748     load_const(R0, (address)rtm_counters-&gt;total_count_addr(), tmpReg);
2749     //atomic_inc_ptr(R0, tmpReg); We don&#39;t increment atomically
2750     ldx(tmpReg, R0);
2751     addi(tmpReg, tmpReg, 1);
</pre>
<hr />
<pre>
2756   beq(CCR0, L_on_abort);
2757   // We don&#39;t reload mark word. Will only be reset at safepoint.
2758   ld(R0, 0, owner_addr_Reg); // Load in transaction, conflicts need to be tracked.
2759   cmpdi(flag, R0, 0);
2760   beq(flag, DONE_LABEL);
2761 
2762   if (UseRTMXendForLockBusy) {
2763     tend_();
2764     b(L_decrement_retry);
2765   } else {
2766     tabort_();
2767   }
2768   bind(L_on_abort);
2769   const Register abort_status_Reg = tmpReg;
2770   mftexasr(abort_status_Reg);
2771   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
2772     rtm_profiling(abort_status_Reg, /*temp*/ owner_addr_Reg, rtm_counters, method_data, profile_rtm);
2773     // Restore owner_addr_Reg
2774     ld(mark_word, oopDesc::mark_offset_in_bytes(), obj);
2775 #ifdef ASSERT
<span class="line-modified">2776     andi_(R0, mark_word, markOopDesc::monitor_value);</span>
2777     asm_assert_ne(&quot;must be inflated&quot;, 0xa754); // Deflating only allowed at safepoint.
2778 #endif
2779     addi(owner_addr_Reg, mark_word, owner_offset);
2780   }
2781   if (RTMRetryCount &gt; 0) {
2782     // Retry on lock abort if abort status is not permanent.
2783     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry);
2784   }
2785 
2786   // Appears unlocked - try to swing _owner from null to non-null.
2787   cmpxchgd(flag, /*current val*/ R0, (intptr_t)0, /*new val*/ R16_thread, owner_addr_Reg,
2788            MacroAssembler::MemBarRel | MacroAssembler::MemBarAcq,
2789            MacroAssembler::cmpxchgx_hint_acquire_lock(), noreg, &amp;L_decrement_retry, true);
2790 
2791   if (RTMRetryCount &gt; 0) {
2792     // success done else retry
2793     b(DONE_LABEL);
2794     bind(L_decrement_retry);
2795     // Spin and retry if lock is busy.
2796     rtm_retry_lock_on_busy(retry_on_busy_count_Reg, owner_addr_Reg, L_rtm_retry);
</pre>
<hr />
<pre>
2798     bind(L_decrement_retry);
2799   }
2800 }
2801 
2802 #endif //  INCLUDE_RTM_OPT
2803 
2804 // &quot;The box&quot; is the space on the stack where we copy the object mark.
2805 void MacroAssembler::compiler_fast_lock_object(ConditionRegister flag, Register oop, Register box,
2806                                                Register temp, Register displaced_header, Register current_header,
2807                                                bool try_bias,
2808                                                RTMLockingCounters* rtm_counters,
2809                                                RTMLockingCounters* stack_rtm_counters,
2810                                                Metadata* method_data,
2811                                                bool use_rtm, bool profile_rtm) {
2812   assert_different_registers(oop, box, temp, displaced_header, current_header);
2813   assert(flag != CCR0, &quot;bad condition register&quot;);
2814   Label cont;
2815   Label object_has_monitor;
2816   Label cas_failed;
2817 
<span class="line-modified">2818   // Load markOop from object into displaced_header.</span>
2819   ld(displaced_header, oopDesc::mark_offset_in_bytes(), oop);
2820 
2821 
2822   if (try_bias) {
2823     biased_locking_enter(flag, oop, displaced_header, temp, current_header, cont);
2824   }
2825 
2826 #if INCLUDE_RTM_OPT
2827   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
2828     rtm_stack_locking(flag, oop, displaced_header, temp, /*temp*/ current_header,
2829                       stack_rtm_counters, method_data, profile_rtm,
2830                       cont, object_has_monitor);
2831   }
2832 #endif // INCLUDE_RTM_OPT
2833 
2834   // Handle existing monitor.
2835   // The object has an existing monitor iff (mark &amp; monitor_value) != 0.
<span class="line-modified">2836   andi_(temp, displaced_header, markOopDesc::monitor_value);</span>
2837   bne(CCR0, object_has_monitor);
2838 
<span class="line-modified">2839   // Set displaced_header to be (markOop of object | UNLOCK_VALUE).</span>
<span class="line-modified">2840   ori(displaced_header, displaced_header, markOopDesc::unlocked_value);</span>
2841 
2842   // Load Compare Value application register.
2843 
2844   // Initialize the box. (Must happen before we update the object mark!)
2845   std(displaced_header, BasicLock::displaced_header_offset_in_bytes(), box);
2846 
2847   // Must fence, otherwise, preceding store(s) may float below cmpxchg.
<span class="line-modified">2848   // Compare object markOop with mark and if equal exchange scratch1 with object markOop.</span>
2849   cmpxchgd(/*flag=*/flag,
2850            /*current_value=*/current_header,
2851            /*compare_value=*/displaced_header,
2852            /*exchange_value=*/box,
2853            /*where=*/oop,
2854            MacroAssembler::MemBarRel | MacroAssembler::MemBarAcq,
2855            MacroAssembler::cmpxchgx_hint_acquire_lock(),
2856            noreg,
2857            &amp;cas_failed,
2858            /*check without membar and ldarx first*/true);
2859   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
2860 
2861   // If the compare-and-exchange succeeded, then we found an unlocked
2862   // object and we have now locked it.
2863   b(cont);
2864 
2865   bind(cas_failed);
2866   // We did not see an unlocked object so try the fast recursive case.
2867 
<span class="line-modified">2868   // Check if the owner is self by comparing the value in the markOop of object</span>
2869   // (current_header) with the stack pointer.
2870   sub(current_header, current_header, R1_SP);
<span class="line-modified">2871   load_const_optimized(temp, ~(os::vm_page_size()-1) | markOopDesc::lock_mask_in_place);</span>
2872 
2873   and_(R0/*==0?*/, current_header, temp);
2874   // If condition is true we are cont and hence we can store 0 as the
2875   // displaced header in the box, which indicates that it is a recursive lock.
2876   mcrf(flag,CCR0);
2877   std(R0/*==0, perhaps*/, BasicLock::displaced_header_offset_in_bytes(), box);
2878 
2879   // Handle existing monitor.
2880   b(cont);
2881 
2882   bind(object_has_monitor);
2883   // The object&#39;s monitor m is unlocked iff m-&gt;owner == NULL,
2884   // otherwise m-&gt;owner may contain a thread or a stack address.
2885 
2886 #if INCLUDE_RTM_OPT
2887   // Use the same RTM locking code in 32- and 64-bit VM.
2888   if (use_rtm) {
2889     rtm_inflated_locking(flag, oop, displaced_header, box, temp, /*temp*/ current_header,
2890                          rtm_counters, method_data, profile_rtm, cont);
2891   } else {
2892 #endif // INCLUDE_RTM_OPT
2893 
2894   // Try to CAS m-&gt;owner from NULL to current thread.
<span class="line-modified">2895   addi(temp, displaced_header, ObjectMonitor::owner_offset_in_bytes()-markOopDesc::monitor_value);</span>
2896   cmpxchgd(/*flag=*/flag,
2897            /*current_value=*/current_header,
2898            /*compare_value=*/(intptr_t)0,
2899            /*exchange_value=*/R16_thread,
2900            /*where=*/temp,
2901            MacroAssembler::MemBarRel | MacroAssembler::MemBarAcq,
2902            MacroAssembler::cmpxchgx_hint_acquire_lock());
2903 
2904   // Store a non-null value into the box.
2905   std(box, BasicLock::displaced_header_offset_in_bytes(), box);
2906 
2907 # ifdef ASSERT
2908   bne(flag, cont);
2909   // We have acquired the monitor, check some invariants.
2910   addi(/*monitor=*/temp, temp, -ObjectMonitor::owner_offset_in_bytes());
2911   // Invariant 1: _recursions should be 0.
2912   //assert(ObjectMonitor::recursions_size_in_bytes() == 8, &quot;unexpected size&quot;);
2913   asm_assert_mem8_is_zero(ObjectMonitor::recursions_offset_in_bytes(), temp,
2914                             &quot;monitor-&gt;_recursions should be 0&quot;, -1);
2915 # endif
</pre>
<hr />
<pre>
2922   // flag == EQ indicates success
2923   // flag == NE indicates failure
2924 }
2925 
2926 void MacroAssembler::compiler_fast_unlock_object(ConditionRegister flag, Register oop, Register box,
2927                                                  Register temp, Register displaced_header, Register current_header,
2928                                                  bool try_bias, bool use_rtm) {
2929   assert_different_registers(oop, box, temp, displaced_header, current_header);
2930   assert(flag != CCR0, &quot;bad condition register&quot;);
2931   Label cont;
2932   Label object_has_monitor;
2933 
2934   if (try_bias) {
2935     biased_locking_exit(flag, oop, current_header, cont);
2936   }
2937 
2938 #if INCLUDE_RTM_OPT
2939   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
2940     assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
2941     Label L_regular_unlock;
<span class="line-modified">2942     ld(current_header, oopDesc::mark_offset_in_bytes(), oop);         // fetch markword</span>
<span class="line-modified">2943     andi(R0, current_header, markOopDesc::biased_lock_mask_in_place); // look at 3 lock bits</span>
<span class="line-modified">2944     cmpwi(flag, R0, markOopDesc::unlocked_value);                     // bits = 001 unlocked</span>
<span class="line-modified">2945     bne(flag, L_regular_unlock);                                      // else RegularLock</span>
<span class="line-modified">2946     tend_();                                                          // otherwise end...</span>
<span class="line-modified">2947     b(cont);                                                          // ... and we&#39;re done</span>
2948     bind(L_regular_unlock);
2949   }
2950 #endif
2951 
2952   // Find the lock address and load the displaced header from the stack.
2953   ld(displaced_header, BasicLock::displaced_header_offset_in_bytes(), box);
2954 
2955   // If the displaced header is 0, we have a recursive unlock.
2956   cmpdi(flag, displaced_header, 0);
2957   beq(flag, cont);
2958 
2959   // Handle existing monitor.
2960   // The object has an existing monitor iff (mark &amp; monitor_value) != 0.
2961   RTM_OPT_ONLY( if (!(UseRTMForStackLocks &amp;&amp; use_rtm)) ) // skip load if already done
2962   ld(current_header, oopDesc::mark_offset_in_bytes(), oop);
<span class="line-modified">2963   andi_(R0, current_header, markOopDesc::monitor_value);</span>
2964   bne(CCR0, object_has_monitor);
2965 
2966   // Check if it is still a light weight lock, this is is true if we see
<span class="line-modified">2967   // the stack address of the basicLock in the markOop of the object.</span>
2968   // Cmpxchg sets flag to cmpd(current_header, box).
2969   cmpxchgd(/*flag=*/flag,
2970            /*current_value=*/current_header,
2971            /*compare_value=*/box,
2972            /*exchange_value=*/displaced_header,
2973            /*where=*/oop,
2974            MacroAssembler::MemBarRel,
2975            MacroAssembler::cmpxchgx_hint_release_lock(),
2976            noreg,
2977            &amp;cont);
2978 
2979   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
2980 
2981   // Handle existing monitor.
2982   b(cont);
2983 
2984   bind(object_has_monitor);
<span class="line-modified">2985   addi(current_header, current_header, -markOopDesc::monitor_value); // monitor</span>

2986   ld(temp,             ObjectMonitor::owner_offset_in_bytes(), current_header);
2987 
2988     // It&#39;s inflated.
2989 #if INCLUDE_RTM_OPT
2990   if (use_rtm) {
2991     Label L_regular_inflated_unlock;
2992     // Clean monitor_value bit to get valid pointer
2993     cmpdi(flag, temp, 0);
2994     bne(flag, L_regular_inflated_unlock);
2995     tend_();
2996     b(cont);
2997     bind(L_regular_inflated_unlock);
2998   }
2999 #endif
3000 
3001   ld(displaced_header, ObjectMonitor::recursions_offset_in_bytes(), current_header);
3002   xorr(temp, R16_thread, temp);      // Will be 0 if we are the owner.
3003   orr(temp, temp, displaced_header); // Will be 0 if there are 0 recursions.
3004   cmpdi(flag, temp, 0);
3005   bne(flag, cont);
</pre>
<hr />
<pre>
3083   load_const_optimized(tmp1, entry);
3084 
3085   set_last_Java_frame(/*sp=*/sp, /*pc=*/tmp1);
3086 }
3087 
3088 void MacroAssembler::get_vm_result(Register oop_result) {
3089   // Read:
3090   //   R16_thread
3091   //   R16_thread-&gt;in_bytes(JavaThread::vm_result_offset())
3092   //
3093   // Updated:
3094   //   oop_result
3095   //   R16_thread-&gt;in_bytes(JavaThread::vm_result_offset())
3096 
3097   verify_thread();
3098 
3099   ld(oop_result, in_bytes(JavaThread::vm_result_offset()), R16_thread);
3100   li(R0, 0);
3101   std(R0, in_bytes(JavaThread::vm_result_offset()), R16_thread);
3102 
<span class="line-modified">3103   verify_oop(oop_result);</span>
3104 }
3105 
3106 void MacroAssembler::get_vm_result_2(Register metadata_result) {
3107   // Read:
3108   //   R16_thread
3109   //   R16_thread-&gt;in_bytes(JavaThread::vm_result_2_offset())
3110   //
3111   // Updated:
3112   //   metadata_result
3113   //   R16_thread-&gt;in_bytes(JavaThread::vm_result_2_offset())
3114 
3115   ld(metadata_result, in_bytes(JavaThread::vm_result_2_offset()), R16_thread);
3116   li(R0, 0);
3117   std(R0, in_bytes(JavaThread::vm_result_2_offset()), R16_thread);
3118 }
3119 
3120 Register MacroAssembler::encode_klass_not_null(Register dst, Register src) {
3121   Register current = (src != noreg) ? src : dst; // Klass is in dst if no src provided.
<span class="line-modified">3122   if (Universe::narrow_klass_base() != 0) {</span>
3123     // Use dst as temp if it is free.
<span class="line-modified">3124     sub_const_optimized(dst, current, Universe::narrow_klass_base(), R0);</span>
3125     current = dst;
3126   }
<span class="line-modified">3127   if (Universe::narrow_klass_shift() != 0) {</span>
<span class="line-modified">3128     srdi(dst, current, Universe::narrow_klass_shift());</span>
3129     current = dst;
3130   }
3131   return current;
3132 }
3133 
3134 void MacroAssembler::store_klass(Register dst_oop, Register klass, Register ck) {
3135   if (UseCompressedClassPointers) {
3136     Register compressedKlass = encode_klass_not_null(ck, klass);
3137     stw(compressedKlass, oopDesc::klass_offset_in_bytes(), dst_oop);
3138   } else {
3139     std(klass, oopDesc::klass_offset_in_bytes(), dst_oop);
3140   }
3141 }
3142 
3143 void MacroAssembler::store_klass_gap(Register dst_oop, Register val) {
3144   if (UseCompressedClassPointers) {
3145     if (val == noreg) {
3146       val = R0;
3147       li(val, 0);
3148     }
3149     stw(val, oopDesc::klass_gap_offset_in_bytes(), dst_oop); // klass gap if compressed
3150   }
3151 }
3152 
3153 int MacroAssembler::instr_size_for_decode_klass_not_null() {
3154   if (!UseCompressedClassPointers) return 0;
3155   int num_instrs = 1;  // shift or move
<span class="line-modified">3156   if (Universe::narrow_klass_base() != 0) num_instrs = 7;  // shift + load const + add</span>
3157   return num_instrs * BytesPerInstWord;
3158 }
3159 
3160 void MacroAssembler::decode_klass_not_null(Register dst, Register src) {
3161   assert(dst != R0, &quot;Dst reg may not be R0, as R0 is used here.&quot;);
3162   if (src == noreg) src = dst;
3163   Register shifted_src = src;
<span class="line-modified">3164   if (Universe::narrow_klass_shift() != 0 ||</span>
<span class="line-modified">3165       Universe::narrow_klass_base() == 0 &amp;&amp; src != dst) {  // Move required.</span>
3166     shifted_src = dst;
<span class="line-modified">3167     sldi(shifted_src, src, Universe::narrow_klass_shift());</span>
3168   }
<span class="line-modified">3169   if (Universe::narrow_klass_base() != 0) {</span>
<span class="line-modified">3170     add_const_optimized(dst, shifted_src, Universe::narrow_klass_base(), R0);</span>
3171   }
3172 }
3173 
3174 void MacroAssembler::load_klass(Register dst, Register src) {
3175   if (UseCompressedClassPointers) {
3176     lwz(dst, oopDesc::klass_offset_in_bytes(), src);
3177     // Attention: no null check here!
3178     decode_klass_not_null(dst, dst);
3179   } else {
3180     ld(dst, oopDesc::klass_offset_in_bytes(), src);
3181   }
3182 }
3183 
3184 // ((OopHandle)result).resolve();
3185 void MacroAssembler::resolve_oop_handle(Register result) {
3186   // OopHandle::resolve is an indirection.
3187   ld(result, 0, result);
3188 }
3189 
3190 void MacroAssembler::load_mirror_from_const_method(Register mirror, Register const_method) {
3191   ld(mirror, in_bytes(ConstMethod::constants_offset()), const_method);
3192   ld(mirror, ConstantPool::pool_holder_offset_in_bytes(), mirror);
3193   ld(mirror, in_bytes(Klass::java_mirror_offset()), mirror);
3194   resolve_oop_handle(mirror);
3195 }
3196 






3197 // Clear Array
3198 // For very short arrays. tmp == R0 is allowed.
3199 void MacroAssembler::clear_memory_unrolled(Register base_ptr, int cnt_dwords, Register tmp, int offset) {
3200   if (cnt_dwords &gt; 0) { li(tmp, 0); }
3201   for (int i = 0; i &lt; cnt_dwords; ++i) { std(tmp, offset + i * 8, base_ptr); }
3202 }
3203 
3204 // Version for constant short array length. Kills base_ptr. tmp == R0 is allowed.
3205 void MacroAssembler::clear_memory_constlen(Register base_ptr, int cnt_dwords, Register tmp) {
3206   if (cnt_dwords &lt; 8) {
3207     clear_memory_unrolled(base_ptr, cnt_dwords, tmp);
3208     return;
3209   }
3210 
3211   Label loop;
3212   const long loopcnt   = cnt_dwords &gt;&gt; 1,
3213              remainder = cnt_dwords &amp; 1;
3214 
3215   li(tmp, loopcnt);
3216   mtctr(tmp);
</pre>
<hr />
<pre>
4874       lwz(R0, mem_offset, mem_base);
4875       cmpwi(CCR0, R0, 0);
4876       break;
4877     case 8:
4878       ld(R0, mem_offset, mem_base);
4879       cmpdi(CCR0, R0, 0);
4880       break;
4881     default:
4882       ShouldNotReachHere();
4883   }
4884   asm_assert(check_equal, msg, id);
4885 #endif // ASSERT
4886 }
4887 
4888 void MacroAssembler::verify_thread() {
4889   if (VerifyThread) {
4890     unimplemented(&quot;&#39;VerifyThread&#39; currently not implemented on PPC&quot;);
4891   }
4892 }
4893 







4894 // READ: oop. KILL: R0. Volatile floats perhaps.
4895 void MacroAssembler::verify_oop(Register oop, const char* msg) {
4896   if (!VerifyOops) {
4897     return;
4898   }
4899 
4900   address/* FunctionDescriptor** */fd = StubRoutines::verify_oop_subroutine_entry_address();
4901   const Register tmp = R11; // Will be preserved.
4902   const int nbytes_save = MacroAssembler::num_volatile_regs * 8;



4903   save_volatile_gprs(R1_SP, -nbytes_save); // except R0
4904 
4905   mr_if_needed(R4_ARG2, oop);
4906   save_LR_CR(tmp); // save in old frame
4907   push_frame_reg_args(nbytes_save, tmp);
4908   // load FunctionDescriptor** / entry_address *
4909   load_const_optimized(tmp, fd, R0);
4910   // load FunctionDescriptor* / entry_address
4911   ld(tmp, 0, tmp);
4912   load_const_optimized(R3_ARG1, (address)msg, R0);
4913   // Call destination for its side effect.
4914   call_c(tmp);
4915 
4916   pop_frame();
4917   restore_LR_CR(tmp);
4918   restore_volatile_gprs(R1_SP, -nbytes_save); // except R0


4919 }
4920 
4921 void MacroAssembler::verify_oop_addr(RegisterOrConstant offs, Register base, const char* msg) {
4922   if (!VerifyOops) {
4923     return;
4924   }
4925 
4926   address/* FunctionDescriptor** */fd = StubRoutines::verify_oop_subroutine_entry_address();
4927   const Register tmp = R11; // Will be preserved.
4928   const int nbytes_save = MacroAssembler::num_volatile_regs * 8;
4929   save_volatile_gprs(R1_SP, -nbytes_save); // except R0
4930 
4931   ld(R4_ARG2, offs, base);
4932   save_LR_CR(tmp); // save in old frame
4933   push_frame_reg_args(nbytes_save, tmp);
4934   // load FunctionDescriptor** / entry_address *
4935   load_const_optimized(tmp, fd, R0);
4936   // load FunctionDescriptor* / entry_address
4937   ld(tmp, 0, tmp);
4938   load_const_optimized(R3_ARG1, (address)msg, R0);
</pre>
<hr />
<pre>
5008 }
5009 
5010 #endif // !PRODUCT
5011 
5012 void SkipIfEqualZero::skip_to_label_if_equal_zero(MacroAssembler* masm, Register temp,
5013                                                   const bool* flag_addr, Label&amp; label) {
5014   int simm16_offset = masm-&gt;load_const_optimized(temp, (address)flag_addr, R0, true);
5015   assert(sizeof(bool) == 1, &quot;PowerPC ABI&quot;);
5016   masm-&gt;lbz(temp, simm16_offset, temp);
5017   masm-&gt;cmpwi(CCR0, temp, 0);
5018   masm-&gt;beq(CCR0, label);
5019 }
5020 
5021 SkipIfEqualZero::SkipIfEqualZero(MacroAssembler* masm, Register temp, const bool* flag_addr) : _masm(masm), _label() {
5022   skip_to_label_if_equal_zero(masm, temp, flag_addr, _label);
5023 }
5024 
5025 SkipIfEqualZero::~SkipIfEqualZero() {
5026   _masm-&gt;bind(_label);
5027 }



















</pre>
</td>
<td>
<hr />
<pre>
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;compiler/disassembler.hpp&quot;
  29 #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
  30 #include &quot;gc/shared/barrierSet.hpp&quot;
  31 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  32 #include &quot;interpreter/interpreter.hpp&quot;
  33 #include &quot;memory/resourceArea.hpp&quot;
  34 #include &quot;nativeInst_ppc.hpp&quot;
<span class="line-added">  35 #include &quot;oops/klass.inline.hpp&quot;</span>
  36 #include &quot;prims/methodHandles.hpp&quot;
  37 #include &quot;runtime/biasedLocking.hpp&quot;
  38 #include &quot;runtime/icache.hpp&quot;
  39 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  40 #include &quot;runtime/objectMonitor.hpp&quot;
  41 #include &quot;runtime/os.hpp&quot;
  42 #include &quot;runtime/safepoint.hpp&quot;
  43 #include &quot;runtime/safepointMechanism.hpp&quot;
  44 #include &quot;runtime/sharedRuntime.hpp&quot;
  45 #include &quot;runtime/stubRoutines.hpp&quot;
  46 #include &quot;utilities/macros.hpp&quot;
<span class="line-added">  47 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  48 #ifdef COMPILER2
  49 #include &quot;opto/intrinsicnode.hpp&quot;
  50 #endif
  51 
  52 #ifdef PRODUCT
  53 #define BLOCK_COMMENT(str) // nothing
  54 #else
  55 #define BLOCK_COMMENT(str) block_comment(str)
  56 #endif
  57 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  58 
  59 #ifdef ASSERT
  60 // On RISC, there&#39;s no benefit to verifying instruction boundaries.
  61 bool AbstractAssembler::pd_check_instruction_mark() { return false; }
  62 #endif
  63 
  64 void MacroAssembler::ld_largeoffset_unchecked(Register d, int si31, Register a, int emit_filler_nop) {
  65   assert(Assembler::is_simm(si31, 31) &amp;&amp; si31 &gt;= 0, &quot;si31 out of range&quot;);
  66   if (Assembler::is_simm(si31, 16)) {
  67     ld(d, si31, a);
</pre>
<hr />
<pre>
1996   std(super_klass, target_offset, sub_klass); // save result to cache
1997   if (result_reg != noreg) { li(result_reg, 0); } // load zero result (indicates a hit)
1998   if (L_success != NULL) { b(*L_success); }
1999   else if (result_reg == noreg) { blr(); } // return with CR0.eq if neither label nor result reg provided
2000 
2001   bind(fallthru);
2002 }
2003 
2004 // Try fast path, then go to slow one if not successful
2005 void MacroAssembler::check_klass_subtype(Register sub_klass,
2006                          Register super_klass,
2007                          Register temp1_reg,
2008                          Register temp2_reg,
2009                          Label&amp; L_success) {
2010   Label L_failure;
2011   check_klass_subtype_fast_path(sub_klass, super_klass, temp1_reg, temp2_reg, &amp;L_success, &amp;L_failure);
2012   check_klass_subtype_slow_path(sub_klass, super_klass, temp1_reg, temp2_reg, &amp;L_success);
2013   bind(L_failure); // Fallthru if not successful.
2014 }
2015 
<span class="line-modified">2016 void MacroAssembler::clinit_barrier(Register klass, Register thread, Label* L_fast_path, Label* L_slow_path) {</span>
<span class="line-modified">2017   assert(L_fast_path != NULL || L_slow_path != NULL, &quot;at least one is required&quot;);</span>
<span class="line-modified">2018 </span>
<span class="line-modified">2019   Label L_fallthrough;</span>
<span class="line-modified">2020   if (L_fast_path == NULL) {</span>
<span class="line-modified">2021     L_fast_path = &amp;L_fallthrough;</span>
<span class="line-modified">2022   } else if (L_slow_path == NULL) {</span>
<span class="line-modified">2023     L_slow_path = &amp;L_fallthrough;</span>
<span class="line-modified">2024   }</span>
<span class="line-added">2025 </span>
<span class="line-added">2026   // Fast path check: class is fully initialized</span>
<span class="line-added">2027   lbz(R0, in_bytes(InstanceKlass::init_state_offset()), klass);</span>
<span class="line-added">2028   cmpwi(CCR0, R0, InstanceKlass::fully_initialized);</span>
<span class="line-added">2029   beq(CCR0, *L_fast_path);</span>
<span class="line-added">2030 </span>
<span class="line-added">2031   // Fast path check: current thread is initializer thread</span>
<span class="line-added">2032   ld(R0, in_bytes(InstanceKlass::init_thread_offset()), klass);</span>
<span class="line-added">2033   cmpd(CCR0, thread, R0);</span>
<span class="line-added">2034   if (L_slow_path == &amp;L_fallthrough) {</span>
<span class="line-added">2035     beq(CCR0, *L_fast_path);</span>
<span class="line-added">2036   } else if (L_fast_path == &amp;L_fallthrough) {</span>
<span class="line-added">2037     bne(CCR0, *L_slow_path);</span>
<span class="line-added">2038   } else {</span>
<span class="line-added">2039     Unimplemented();</span>
<span class="line-added">2040   }</span>
<span class="line-added">2041 </span>
<span class="line-added">2042   bind(L_fallthrough);</span>
2043 }
2044 
2045 RegisterOrConstant MacroAssembler::argument_offset(RegisterOrConstant arg_slot,
2046                                                    Register temp_reg,
2047                                                    int extra_slot_offset) {
2048   // cf. TemplateTable::prepare_invoke(), if (load_receiver).
2049   int stackElementSize = Interpreter::stackElementSize;
2050   int offset = extra_slot_offset * stackElementSize;
2051   if (arg_slot.is_constant()) {
2052     offset += arg_slot.as_constant() * stackElementSize;
2053     return offset;
2054   } else {
2055     assert(temp_reg != noreg, &quot;must specify&quot;);
2056     sldi(temp_reg, arg_slot.as_register(), exact_log2(stackElementSize));
2057     if (offset != 0)
2058       addi(temp_reg, temp_reg, offset);
2059     return temp_reg;
2060   }
2061 }
2062 
2063 // Supports temp2_reg = R0.
2064 void MacroAssembler::biased_locking_enter(ConditionRegister cr_reg, Register obj_reg,
2065                                           Register mark_reg, Register temp_reg,
2066                                           Register temp2_reg, Label&amp; done, Label* slow_case) {
2067   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
2068 
2069 #ifdef ASSERT
2070   assert_different_registers(obj_reg, mark_reg, temp_reg, temp2_reg);
2071 #endif
2072 
2073   Label cas_label;
2074 
2075   // Branch to done if fast path fails and no slow_case provided.
2076   Label *slow_case_int = (slow_case != NULL) ? slow_case : &amp;done;
2077 
2078   // Biased locking
2079   // See whether the lock is currently biased toward our thread and
2080   // whether the epoch is still valid
2081   // Note that the runtime guarantees sufficient alignment of JavaThread
2082   // pointers to allow age to be placed into low bits
<span class="line-modified">2083   assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits,</span>
2084          &quot;biased locking makes assumptions about bit layout&quot;);
2085 
2086   if (PrintBiasedLockingStatistics) {
2087     load_const(temp2_reg, (address) BiasedLocking::total_entry_count_addr(), temp_reg);
2088     lwzx(temp_reg, temp2_reg);
2089     addi(temp_reg, temp_reg, 1);
2090     stwx(temp_reg, temp2_reg);
2091   }
2092 
<span class="line-modified">2093   andi(temp_reg, mark_reg, markWord::biased_lock_mask_in_place);</span>
<span class="line-modified">2094   cmpwi(cr_reg, temp_reg, markWord::biased_lock_pattern);</span>
2095   bne(cr_reg, cas_label);
2096 
2097   load_klass(temp_reg, obj_reg);
2098 
<span class="line-modified">2099   load_const_optimized(temp2_reg, ~((int) markWord::age_mask_in_place));</span>
2100   ld(temp_reg, in_bytes(Klass::prototype_header_offset()), temp_reg);
2101   orr(temp_reg, R16_thread, temp_reg);
2102   xorr(temp_reg, mark_reg, temp_reg);
2103   andr(temp_reg, temp_reg, temp2_reg);
2104   cmpdi(cr_reg, temp_reg, 0);
2105   if (PrintBiasedLockingStatistics) {
2106     Label l;
2107     bne(cr_reg, l);
2108     load_const(temp2_reg, (address) BiasedLocking::biased_lock_entry_count_addr());
2109     lwzx(mark_reg, temp2_reg);
2110     addi(mark_reg, mark_reg, 1);
2111     stwx(mark_reg, temp2_reg);
2112     // restore mark_reg
2113     ld(mark_reg, oopDesc::mark_offset_in_bytes(), obj_reg);
2114     bind(l);
2115   }
2116   beq(cr_reg, done);
2117 
2118   Label try_revoke_bias;
2119   Label try_rebias;
2120 
2121   // At this point we know that the header has the bias pattern and
2122   // that we are not the bias owner in the current epoch. We need to
2123   // figure out more details about the state of the header in order to
2124   // know what operations can be legally performed on the object&#39;s
2125   // header.
2126 
2127   // If the low three bits in the xor result aren&#39;t clear, that means
2128   // the prototype header is no longer biased and we have to revoke
2129   // the bias on this object.
<span class="line-modified">2130   andi(temp2_reg, temp_reg, markWord::biased_lock_mask_in_place);</span>
2131   cmpwi(cr_reg, temp2_reg, 0);
2132   bne(cr_reg, try_revoke_bias);
2133 
2134   // Biasing is still enabled for this data type. See whether the
2135   // epoch of the current bias is still valid, meaning that the epoch
2136   // bits of the mark word are equal to the epoch bits of the
2137   // prototype header. (Note that the prototype header&#39;s epoch bits
2138   // only change at a safepoint.) If not, attempt to rebias the object
2139   // toward the current thread. Note that we must be absolutely sure
2140   // that the current epoch is invalid in order to do this because
2141   // otherwise the manipulations it performs on the mark word are
2142   // illegal.
2143 
<span class="line-modified">2144   int shift_amount = 64 - markWord::epoch_shift;</span>
2145   // rotate epoch bits to right (little) end and set other bits to 0
2146   // [ big part | epoch | little part ] -&gt; [ 0..0 | epoch ]
<span class="line-modified">2147   rldicl_(temp2_reg, temp_reg, shift_amount, 64 - markWord::epoch_bits);</span>
2148   // branch if epoch bits are != 0, i.e. they differ, because the epoch has been incremented
2149   bne(CCR0, try_rebias);
2150 
2151   // The epoch of the current bias is still valid but we know nothing
2152   // about the owner; it might be set or it might be clear. Try to
2153   // acquire the bias of the object using an atomic operation. If this
2154   // fails we will go in to the runtime to revoke the object&#39;s bias.
2155   // Note that we first construct the presumed unbiased header so we
2156   // don&#39;t accidentally blow away another thread&#39;s valid bias.
<span class="line-modified">2157   andi(mark_reg, mark_reg, (markWord::biased_lock_mask_in_place |</span>
<span class="line-modified">2158                                 markWord::age_mask_in_place |</span>
<span class="line-modified">2159                                 markWord::epoch_mask_in_place));</span>
2160   orr(temp_reg, R16_thread, mark_reg);
2161 
2162   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
2163 
2164   // CmpxchgX sets cr_reg to cmpX(temp2_reg, mark_reg).
2165   cmpxchgd(/*flag=*/cr_reg, /*current_value=*/temp2_reg,
2166            /*compare_value=*/mark_reg, /*exchange_value=*/temp_reg,
2167            /*where=*/obj_reg,
2168            MacroAssembler::MemBarAcq,
2169            MacroAssembler::cmpxchgx_hint_acquire_lock(),
2170            noreg, slow_case_int); // bail out if failed
2171 
2172   // If the biasing toward our thread failed, this means that
2173   // another thread succeeded in biasing it toward itself and we
2174   // need to revoke that bias. The revocation will occur in the
2175   // interpreter runtime in the slow case.
2176   if (PrintBiasedLockingStatistics) {
2177     load_const(temp2_reg, (address) BiasedLocking::anonymously_biased_lock_entry_count_addr(), temp_reg);
2178     lwzx(temp_reg, temp2_reg);
2179     addi(temp_reg, temp_reg, 1);
2180     stwx(temp_reg, temp2_reg);
2181   }
2182   b(done);
2183 
2184   bind(try_rebias);
2185   // At this point we know the epoch has expired, meaning that the
2186   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
2187   // circumstances _only_, we are allowed to use the current header&#39;s
2188   // value as the comparison value when doing the cas to acquire the
2189   // bias in the current epoch. In other words, we allow transfer of
2190   // the bias from one thread to another directly in this situation.
2191   load_klass(temp_reg, obj_reg);
<span class="line-modified">2192   andi(temp2_reg, mark_reg, markWord::age_mask_in_place);</span>
2193   orr(temp2_reg, R16_thread, temp2_reg);
2194   ld(temp_reg, in_bytes(Klass::prototype_header_offset()), temp_reg);
2195   orr(temp_reg, temp2_reg, temp_reg);
2196 
2197   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
2198 
2199   cmpxchgd(/*flag=*/cr_reg, /*current_value=*/temp2_reg,
2200                  /*compare_value=*/mark_reg, /*exchange_value=*/temp_reg,
2201                  /*where=*/obj_reg,
2202                  MacroAssembler::MemBarAcq,
2203                  MacroAssembler::cmpxchgx_hint_acquire_lock(),
2204                  noreg, slow_case_int); // bail out if failed
2205 
2206   // If the biasing toward our thread failed, this means that
2207   // another thread succeeded in biasing it toward itself and we
2208   // need to revoke that bias. The revocation will occur in the
2209   // interpreter runtime in the slow case.
2210   if (PrintBiasedLockingStatistics) {
2211     load_const(temp2_reg, (address) BiasedLocking::rebiased_lock_entry_count_addr(), temp_reg);
2212     lwzx(temp_reg, temp2_reg);
2213     addi(temp_reg, temp_reg, 1);
2214     stwx(temp_reg, temp2_reg);
2215   }
2216   b(done);
2217 
2218   bind(try_revoke_bias);
2219   // The prototype mark in the klass doesn&#39;t have the bias bit set any
2220   // more, indicating that objects of this data type are not supposed
2221   // to be biased any more. We are going to try to reset the mark of
2222   // this object to the prototype value and fall through to the
2223   // CAS-based locking scheme. Note that if our CAS fails, it means
2224   // that another thread raced us for the privilege of revoking the
2225   // bias of this particular object, so it&#39;s okay to continue in the
2226   // normal locking code.
2227   load_klass(temp_reg, obj_reg);
2228   ld(temp_reg, in_bytes(Klass::prototype_header_offset()), temp_reg);
<span class="line-modified">2229   andi(temp2_reg, mark_reg, markWord::age_mask_in_place);</span>
2230   orr(temp_reg, temp_reg, temp2_reg);
2231 
2232   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
2233 
2234   // CmpxchgX sets cr_reg to cmpX(temp2_reg, mark_reg).
2235   cmpxchgd(/*flag=*/cr_reg, /*current_value=*/temp2_reg,
2236                  /*compare_value=*/mark_reg, /*exchange_value=*/temp_reg,
2237                  /*where=*/obj_reg,
2238                  MacroAssembler::MemBarAcq,
2239                  MacroAssembler::cmpxchgx_hint_acquire_lock());
2240 
<span class="line-modified">2241   // reload markWord in mark_reg before continuing with lightweight locking</span>
2242   ld(mark_reg, oopDesc::mark_offset_in_bytes(), obj_reg);
2243 
2244   // Fall through to the normal CAS-based lock, because no matter what
2245   // the result of the above CAS, some thread must have succeeded in
2246   // removing the bias bit from the object&#39;s header.
2247   if (PrintBiasedLockingStatistics) {
2248     Label l;
2249     bne(cr_reg, l);
2250     load_const(temp2_reg, (address) BiasedLocking::revoked_lock_entry_count_addr(), temp_reg);
2251     lwzx(temp_reg, temp2_reg);
2252     addi(temp_reg, temp_reg, 1);
2253     stwx(temp_reg, temp2_reg);
2254     bind(l);
2255   }
2256 
2257   bind(cas_label);
2258 }
2259 
2260 void MacroAssembler::biased_locking_exit (ConditionRegister cr_reg, Register mark_addr, Register temp_reg, Label&amp; done) {
2261   // Check for biased locking unlock case, which is a no-op
2262   // Note: we do not have to check the thread ID for two reasons.
2263   // First, the interpreter checks for IllegalMonitorStateException at
2264   // a higher level. Second, if the bias was revoked while we held the
2265   // lock, the object could not be rebiased toward another thread, so
2266   // the bias bit would be clear.
2267 
2268   ld(temp_reg, 0, mark_addr);
<span class="line-modified">2269   andi(temp_reg, temp_reg, markWord::biased_lock_mask_in_place);</span>
2270 
<span class="line-modified">2271   cmpwi(cr_reg, temp_reg, markWord::biased_lock_pattern);</span>
2272   beq(cr_reg, done);
2273 }
2274 
2275 // allocation (for C1)
2276 void MacroAssembler::eden_allocate(
2277   Register obj,                      // result: pointer to object after successful allocation
2278   Register var_size_in_bytes,        // object size in bytes if unknown at compile time; invalid otherwise
2279   int      con_size_in_bytes,        // object size in bytes if   known at compile time
2280   Register t1,                       // temp register
2281   Register t2,                       // temp register
2282   Label&amp;   slow_case                 // continuation point if fast allocation fails
2283 ) {
2284   b(slow_case);
2285 }
2286 
2287 void MacroAssembler::tlab_allocate(
2288   Register obj,                      // result: pointer to object after successful allocation
2289   Register var_size_in_bytes,        // object size in bytes if unknown at compile time; invalid otherwise
2290   int      con_size_in_bytes,        // object size in bytes if   known at compile time
2291   Register t1,                       // temp register
</pre>
<hr />
<pre>
2672 
2673   bind(doneRetry);
2674 }
2675 
2676 // Use RTM for normal stack locks.
2677 // Input: objReg (object to lock)
2678 void MacroAssembler::rtm_stack_locking(ConditionRegister flag,
2679                                        Register obj, Register mark_word, Register tmp,
2680                                        Register retry_on_abort_count_Reg,
2681                                        RTMLockingCounters* stack_rtm_counters,
2682                                        Metadata* method_data, bool profile_rtm,
2683                                        Label&amp; DONE_LABEL, Label&amp; IsInflated) {
2684   assert(UseRTMForStackLocks, &quot;why call this otherwise?&quot;);
2685   assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
2686   Label L_rtm_retry, L_decrement_retry, L_on_abort;
2687 
2688   if (RTMRetryCount &gt; 0) {
2689     load_const_optimized(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort
2690     bind(L_rtm_retry);
2691   }
<span class="line-modified">2692   andi_(R0, mark_word, markWord::monitor_value);  // inflated vs stack-locked|neutral|biased</span>
2693   bne(CCR0, IsInflated);
2694 
2695   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
2696     Label L_noincrement;
2697     if (RTMTotalCountIncrRate &gt; 1) {
2698       branch_on_random_using_tb(tmp, RTMTotalCountIncrRate, L_noincrement);
2699     }
2700     assert(stack_rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
2701     load_const_optimized(tmp, (address)stack_rtm_counters-&gt;total_count_addr(), R0);
2702     //atomic_inc_ptr(tmp, /*temp, will be reloaded*/mark_word); We don&#39;t increment atomically
2703     ldx(mark_word, tmp);
2704     addi(mark_word, mark_word, 1);
2705     stdx(mark_word, tmp);
2706     bind(L_noincrement);
2707   }
2708   tbegin_();
2709   beq(CCR0, L_on_abort);
<span class="line-modified">2710   ld(mark_word, oopDesc::mark_offset_in_bytes(), obj);      // Reload in transaction, conflicts need to be tracked.</span>
<span class="line-modified">2711   andi(R0, mark_word, markWord::biased_lock_mask_in_place); // look at 3 lock bits</span>
<span class="line-modified">2712   cmpwi(flag, R0, markWord::unlocked_value);                // bits = 001 unlocked</span>
<span class="line-modified">2713   beq(flag, DONE_LABEL);                                    // all done if unlocked</span>
2714 
2715   if (UseRTMXendForLockBusy) {
2716     tend_();
2717     b(L_decrement_retry);
2718   } else {
2719     tabort_();
2720   }
2721   bind(L_on_abort);
2722   const Register abort_status_Reg = tmp;
2723   mftexasr(abort_status_Reg);
2724   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
2725     rtm_profiling(abort_status_Reg, /*temp*/mark_word, stack_rtm_counters, method_data, profile_rtm);
2726   }
2727   ld(mark_word, oopDesc::mark_offset_in_bytes(), obj); // reload
2728   if (RTMRetryCount &gt; 0) {
2729     // Retry on lock abort if abort status is not permanent.
2730     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry, &amp;L_decrement_retry);
2731   } else {
2732     bind(L_decrement_retry);
2733   }
2734 }
2735 
2736 // Use RTM for inflating locks
2737 // inputs: obj       (object to lock)
2738 //         mark_word (current header - KILLED)
2739 //         boxReg    (on-stack box address (displaced header location) - KILLED)
2740 void MacroAssembler::rtm_inflated_locking(ConditionRegister flag,
2741                                           Register obj, Register mark_word, Register boxReg,
2742                                           Register retry_on_busy_count_Reg, Register retry_on_abort_count_Reg,
2743                                           RTMLockingCounters* rtm_counters,
2744                                           Metadata* method_data, bool profile_rtm,
2745                                           Label&amp; DONE_LABEL) {
2746   assert(UseRTMLocking, &quot;why call this otherwise?&quot;);
2747   Label L_rtm_retry, L_decrement_retry, L_on_abort;
2748   // Clean monitor_value bit to get valid pointer.
<span class="line-modified">2749   int owner_offset = ObjectMonitor::owner_offset_in_bytes() - markWord::monitor_value;</span>
2750 
<span class="line-modified">2751   // Store non-null, using boxReg instead of (intptr_t)markWord::unused_mark().</span>
2752   std(boxReg, BasicLock::displaced_header_offset_in_bytes(), boxReg);
2753   const Register tmpReg = boxReg;
2754   const Register owner_addr_Reg = mark_word;
2755   addi(owner_addr_Reg, mark_word, owner_offset);
2756 
2757   if (RTMRetryCount &gt; 0) {
2758     load_const_optimized(retry_on_busy_count_Reg, RTMRetryCount);  // Retry on lock busy.
2759     load_const_optimized(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort.
2760     bind(L_rtm_retry);
2761   }
2762   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
2763     Label L_noincrement;
2764     if (RTMTotalCountIncrRate &gt; 1) {
2765       branch_on_random_using_tb(R0, RTMTotalCountIncrRate, L_noincrement);
2766     }
2767     assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
2768     load_const(R0, (address)rtm_counters-&gt;total_count_addr(), tmpReg);
2769     //atomic_inc_ptr(R0, tmpReg); We don&#39;t increment atomically
2770     ldx(tmpReg, R0);
2771     addi(tmpReg, tmpReg, 1);
</pre>
<hr />
<pre>
2776   beq(CCR0, L_on_abort);
2777   // We don&#39;t reload mark word. Will only be reset at safepoint.
2778   ld(R0, 0, owner_addr_Reg); // Load in transaction, conflicts need to be tracked.
2779   cmpdi(flag, R0, 0);
2780   beq(flag, DONE_LABEL);
2781 
2782   if (UseRTMXendForLockBusy) {
2783     tend_();
2784     b(L_decrement_retry);
2785   } else {
2786     tabort_();
2787   }
2788   bind(L_on_abort);
2789   const Register abort_status_Reg = tmpReg;
2790   mftexasr(abort_status_Reg);
2791   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
2792     rtm_profiling(abort_status_Reg, /*temp*/ owner_addr_Reg, rtm_counters, method_data, profile_rtm);
2793     // Restore owner_addr_Reg
2794     ld(mark_word, oopDesc::mark_offset_in_bytes(), obj);
2795 #ifdef ASSERT
<span class="line-modified">2796     andi_(R0, mark_word, markWord::monitor_value);</span>
2797     asm_assert_ne(&quot;must be inflated&quot;, 0xa754); // Deflating only allowed at safepoint.
2798 #endif
2799     addi(owner_addr_Reg, mark_word, owner_offset);
2800   }
2801   if (RTMRetryCount &gt; 0) {
2802     // Retry on lock abort if abort status is not permanent.
2803     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry);
2804   }
2805 
2806   // Appears unlocked - try to swing _owner from null to non-null.
2807   cmpxchgd(flag, /*current val*/ R0, (intptr_t)0, /*new val*/ R16_thread, owner_addr_Reg,
2808            MacroAssembler::MemBarRel | MacroAssembler::MemBarAcq,
2809            MacroAssembler::cmpxchgx_hint_acquire_lock(), noreg, &amp;L_decrement_retry, true);
2810 
2811   if (RTMRetryCount &gt; 0) {
2812     // success done else retry
2813     b(DONE_LABEL);
2814     bind(L_decrement_retry);
2815     // Spin and retry if lock is busy.
2816     rtm_retry_lock_on_busy(retry_on_busy_count_Reg, owner_addr_Reg, L_rtm_retry);
</pre>
<hr />
<pre>
2818     bind(L_decrement_retry);
2819   }
2820 }
2821 
2822 #endif //  INCLUDE_RTM_OPT
2823 
2824 // &quot;The box&quot; is the space on the stack where we copy the object mark.
2825 void MacroAssembler::compiler_fast_lock_object(ConditionRegister flag, Register oop, Register box,
2826                                                Register temp, Register displaced_header, Register current_header,
2827                                                bool try_bias,
2828                                                RTMLockingCounters* rtm_counters,
2829                                                RTMLockingCounters* stack_rtm_counters,
2830                                                Metadata* method_data,
2831                                                bool use_rtm, bool profile_rtm) {
2832   assert_different_registers(oop, box, temp, displaced_header, current_header);
2833   assert(flag != CCR0, &quot;bad condition register&quot;);
2834   Label cont;
2835   Label object_has_monitor;
2836   Label cas_failed;
2837 
<span class="line-modified">2838   // Load markWord from object into displaced_header.</span>
2839   ld(displaced_header, oopDesc::mark_offset_in_bytes(), oop);
2840 
2841 
2842   if (try_bias) {
2843     biased_locking_enter(flag, oop, displaced_header, temp, current_header, cont);
2844   }
2845 
2846 #if INCLUDE_RTM_OPT
2847   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
2848     rtm_stack_locking(flag, oop, displaced_header, temp, /*temp*/ current_header,
2849                       stack_rtm_counters, method_data, profile_rtm,
2850                       cont, object_has_monitor);
2851   }
2852 #endif // INCLUDE_RTM_OPT
2853 
2854   // Handle existing monitor.
2855   // The object has an existing monitor iff (mark &amp; monitor_value) != 0.
<span class="line-modified">2856   andi_(temp, displaced_header, markWord::monitor_value);</span>
2857   bne(CCR0, object_has_monitor);
2858 
<span class="line-modified">2859   // Set displaced_header to be (markWord of object | UNLOCK_VALUE).</span>
<span class="line-modified">2860   ori(displaced_header, displaced_header, markWord::unlocked_value);</span>
2861 
2862   // Load Compare Value application register.
2863 
2864   // Initialize the box. (Must happen before we update the object mark!)
2865   std(displaced_header, BasicLock::displaced_header_offset_in_bytes(), box);
2866 
2867   // Must fence, otherwise, preceding store(s) may float below cmpxchg.
<span class="line-modified">2868   // Compare object markWord with mark and if equal exchange scratch1 with object markWord.</span>
2869   cmpxchgd(/*flag=*/flag,
2870            /*current_value=*/current_header,
2871            /*compare_value=*/displaced_header,
2872            /*exchange_value=*/box,
2873            /*where=*/oop,
2874            MacroAssembler::MemBarRel | MacroAssembler::MemBarAcq,
2875            MacroAssembler::cmpxchgx_hint_acquire_lock(),
2876            noreg,
2877            &amp;cas_failed,
2878            /*check without membar and ldarx first*/true);
2879   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
2880 
2881   // If the compare-and-exchange succeeded, then we found an unlocked
2882   // object and we have now locked it.
2883   b(cont);
2884 
2885   bind(cas_failed);
2886   // We did not see an unlocked object so try the fast recursive case.
2887 
<span class="line-modified">2888   // Check if the owner is self by comparing the value in the markWord of object</span>
2889   // (current_header) with the stack pointer.
2890   sub(current_header, current_header, R1_SP);
<span class="line-modified">2891   load_const_optimized(temp, ~(os::vm_page_size()-1) | markWord::lock_mask_in_place);</span>
2892 
2893   and_(R0/*==0?*/, current_header, temp);
2894   // If condition is true we are cont and hence we can store 0 as the
2895   // displaced header in the box, which indicates that it is a recursive lock.
2896   mcrf(flag,CCR0);
2897   std(R0/*==0, perhaps*/, BasicLock::displaced_header_offset_in_bytes(), box);
2898 
2899   // Handle existing monitor.
2900   b(cont);
2901 
2902   bind(object_has_monitor);
2903   // The object&#39;s monitor m is unlocked iff m-&gt;owner == NULL,
2904   // otherwise m-&gt;owner may contain a thread or a stack address.
2905 
2906 #if INCLUDE_RTM_OPT
2907   // Use the same RTM locking code in 32- and 64-bit VM.
2908   if (use_rtm) {
2909     rtm_inflated_locking(flag, oop, displaced_header, box, temp, /*temp*/ current_header,
2910                          rtm_counters, method_data, profile_rtm, cont);
2911   } else {
2912 #endif // INCLUDE_RTM_OPT
2913 
2914   // Try to CAS m-&gt;owner from NULL to current thread.
<span class="line-modified">2915   addi(temp, displaced_header, ObjectMonitor::owner_offset_in_bytes()-markWord::monitor_value);</span>
2916   cmpxchgd(/*flag=*/flag,
2917            /*current_value=*/current_header,
2918            /*compare_value=*/(intptr_t)0,
2919            /*exchange_value=*/R16_thread,
2920            /*where=*/temp,
2921            MacroAssembler::MemBarRel | MacroAssembler::MemBarAcq,
2922            MacroAssembler::cmpxchgx_hint_acquire_lock());
2923 
2924   // Store a non-null value into the box.
2925   std(box, BasicLock::displaced_header_offset_in_bytes(), box);
2926 
2927 # ifdef ASSERT
2928   bne(flag, cont);
2929   // We have acquired the monitor, check some invariants.
2930   addi(/*monitor=*/temp, temp, -ObjectMonitor::owner_offset_in_bytes());
2931   // Invariant 1: _recursions should be 0.
2932   //assert(ObjectMonitor::recursions_size_in_bytes() == 8, &quot;unexpected size&quot;);
2933   asm_assert_mem8_is_zero(ObjectMonitor::recursions_offset_in_bytes(), temp,
2934                             &quot;monitor-&gt;_recursions should be 0&quot;, -1);
2935 # endif
</pre>
<hr />
<pre>
2942   // flag == EQ indicates success
2943   // flag == NE indicates failure
2944 }
2945 
2946 void MacroAssembler::compiler_fast_unlock_object(ConditionRegister flag, Register oop, Register box,
2947                                                  Register temp, Register displaced_header, Register current_header,
2948                                                  bool try_bias, bool use_rtm) {
2949   assert_different_registers(oop, box, temp, displaced_header, current_header);
2950   assert(flag != CCR0, &quot;bad condition register&quot;);
2951   Label cont;
2952   Label object_has_monitor;
2953 
2954   if (try_bias) {
2955     biased_locking_exit(flag, oop, current_header, cont);
2956   }
2957 
2958 #if INCLUDE_RTM_OPT
2959   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
2960     assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
2961     Label L_regular_unlock;
<span class="line-modified">2962     ld(current_header, oopDesc::mark_offset_in_bytes(), oop);      // fetch markword</span>
<span class="line-modified">2963     andi(R0, current_header, markWord::biased_lock_mask_in_place); // look at 3 lock bits</span>
<span class="line-modified">2964     cmpwi(flag, R0, markWord::unlocked_value);                     // bits = 001 unlocked</span>
<span class="line-modified">2965     bne(flag, L_regular_unlock);                                   // else RegularLock</span>
<span class="line-modified">2966     tend_();                                                       // otherwise end...</span>
<span class="line-modified">2967     b(cont);                                                       // ... and we&#39;re done</span>
2968     bind(L_regular_unlock);
2969   }
2970 #endif
2971 
2972   // Find the lock address and load the displaced header from the stack.
2973   ld(displaced_header, BasicLock::displaced_header_offset_in_bytes(), box);
2974 
2975   // If the displaced header is 0, we have a recursive unlock.
2976   cmpdi(flag, displaced_header, 0);
2977   beq(flag, cont);
2978 
2979   // Handle existing monitor.
2980   // The object has an existing monitor iff (mark &amp; monitor_value) != 0.
2981   RTM_OPT_ONLY( if (!(UseRTMForStackLocks &amp;&amp; use_rtm)) ) // skip load if already done
2982   ld(current_header, oopDesc::mark_offset_in_bytes(), oop);
<span class="line-modified">2983   andi_(R0, current_header, markWord::monitor_value);</span>
2984   bne(CCR0, object_has_monitor);
2985 
2986   // Check if it is still a light weight lock, this is is true if we see
<span class="line-modified">2987   // the stack address of the basicLock in the markWord of the object.</span>
2988   // Cmpxchg sets flag to cmpd(current_header, box).
2989   cmpxchgd(/*flag=*/flag,
2990            /*current_value=*/current_header,
2991            /*compare_value=*/box,
2992            /*exchange_value=*/displaced_header,
2993            /*where=*/oop,
2994            MacroAssembler::MemBarRel,
2995            MacroAssembler::cmpxchgx_hint_release_lock(),
2996            noreg,
2997            &amp;cont);
2998 
2999   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
3000 
3001   // Handle existing monitor.
3002   b(cont);
3003 
3004   bind(object_has_monitor);
<span class="line-modified">3005   STATIC_ASSERT(markWord::monitor_value &lt;= INT_MAX);</span>
<span class="line-added">3006   addi(current_header, current_header, -(int)markWord::monitor_value); // monitor</span>
3007   ld(temp,             ObjectMonitor::owner_offset_in_bytes(), current_header);
3008 
3009     // It&#39;s inflated.
3010 #if INCLUDE_RTM_OPT
3011   if (use_rtm) {
3012     Label L_regular_inflated_unlock;
3013     // Clean monitor_value bit to get valid pointer
3014     cmpdi(flag, temp, 0);
3015     bne(flag, L_regular_inflated_unlock);
3016     tend_();
3017     b(cont);
3018     bind(L_regular_inflated_unlock);
3019   }
3020 #endif
3021 
3022   ld(displaced_header, ObjectMonitor::recursions_offset_in_bytes(), current_header);
3023   xorr(temp, R16_thread, temp);      // Will be 0 if we are the owner.
3024   orr(temp, temp, displaced_header); // Will be 0 if there are 0 recursions.
3025   cmpdi(flag, temp, 0);
3026   bne(flag, cont);
</pre>
<hr />
<pre>
3104   load_const_optimized(tmp1, entry);
3105 
3106   set_last_Java_frame(/*sp=*/sp, /*pc=*/tmp1);
3107 }
3108 
3109 void MacroAssembler::get_vm_result(Register oop_result) {
3110   // Read:
3111   //   R16_thread
3112   //   R16_thread-&gt;in_bytes(JavaThread::vm_result_offset())
3113   //
3114   // Updated:
3115   //   oop_result
3116   //   R16_thread-&gt;in_bytes(JavaThread::vm_result_offset())
3117 
3118   verify_thread();
3119 
3120   ld(oop_result, in_bytes(JavaThread::vm_result_offset()), R16_thread);
3121   li(R0, 0);
3122   std(R0, in_bytes(JavaThread::vm_result_offset()), R16_thread);
3123 
<span class="line-modified">3124   verify_oop(oop_result, FILE_AND_LINE);</span>
3125 }
3126 
3127 void MacroAssembler::get_vm_result_2(Register metadata_result) {
3128   // Read:
3129   //   R16_thread
3130   //   R16_thread-&gt;in_bytes(JavaThread::vm_result_2_offset())
3131   //
3132   // Updated:
3133   //   metadata_result
3134   //   R16_thread-&gt;in_bytes(JavaThread::vm_result_2_offset())
3135 
3136   ld(metadata_result, in_bytes(JavaThread::vm_result_2_offset()), R16_thread);
3137   li(R0, 0);
3138   std(R0, in_bytes(JavaThread::vm_result_2_offset()), R16_thread);
3139 }
3140 
3141 Register MacroAssembler::encode_klass_not_null(Register dst, Register src) {
3142   Register current = (src != noreg) ? src : dst; // Klass is in dst if no src provided.
<span class="line-modified">3143   if (CompressedKlassPointers::base() != 0) {</span>
3144     // Use dst as temp if it is free.
<span class="line-modified">3145     sub_const_optimized(dst, current, CompressedKlassPointers::base(), R0);</span>
3146     current = dst;
3147   }
<span class="line-modified">3148   if (CompressedKlassPointers::shift() != 0) {</span>
<span class="line-modified">3149     srdi(dst, current, CompressedKlassPointers::shift());</span>
3150     current = dst;
3151   }
3152   return current;
3153 }
3154 
3155 void MacroAssembler::store_klass(Register dst_oop, Register klass, Register ck) {
3156   if (UseCompressedClassPointers) {
3157     Register compressedKlass = encode_klass_not_null(ck, klass);
3158     stw(compressedKlass, oopDesc::klass_offset_in_bytes(), dst_oop);
3159   } else {
3160     std(klass, oopDesc::klass_offset_in_bytes(), dst_oop);
3161   }
3162 }
3163 
3164 void MacroAssembler::store_klass_gap(Register dst_oop, Register val) {
3165   if (UseCompressedClassPointers) {
3166     if (val == noreg) {
3167       val = R0;
3168       li(val, 0);
3169     }
3170     stw(val, oopDesc::klass_gap_offset_in_bytes(), dst_oop); // klass gap if compressed
3171   }
3172 }
3173 
3174 int MacroAssembler::instr_size_for_decode_klass_not_null() {
3175   if (!UseCompressedClassPointers) return 0;
3176   int num_instrs = 1;  // shift or move
<span class="line-modified">3177   if (CompressedKlassPointers::base() != 0) num_instrs = 7;  // shift + load const + add</span>
3178   return num_instrs * BytesPerInstWord;
3179 }
3180 
3181 void MacroAssembler::decode_klass_not_null(Register dst, Register src) {
3182   assert(dst != R0, &quot;Dst reg may not be R0, as R0 is used here.&quot;);
3183   if (src == noreg) src = dst;
3184   Register shifted_src = src;
<span class="line-modified">3185   if (CompressedKlassPointers::shift() != 0 ||</span>
<span class="line-modified">3186       CompressedKlassPointers::base() == 0 &amp;&amp; src != dst) {  // Move required.</span>
3187     shifted_src = dst;
<span class="line-modified">3188     sldi(shifted_src, src, CompressedKlassPointers::shift());</span>
3189   }
<span class="line-modified">3190   if (CompressedKlassPointers::base() != 0) {</span>
<span class="line-modified">3191     add_const_optimized(dst, shifted_src, CompressedKlassPointers::base(), R0);</span>
3192   }
3193 }
3194 
3195 void MacroAssembler::load_klass(Register dst, Register src) {
3196   if (UseCompressedClassPointers) {
3197     lwz(dst, oopDesc::klass_offset_in_bytes(), src);
3198     // Attention: no null check here!
3199     decode_klass_not_null(dst, dst);
3200   } else {
3201     ld(dst, oopDesc::klass_offset_in_bytes(), src);
3202   }
3203 }
3204 
3205 // ((OopHandle)result).resolve();
3206 void MacroAssembler::resolve_oop_handle(Register result) {
3207   // OopHandle::resolve is an indirection.
3208   ld(result, 0, result);
3209 }
3210 
3211 void MacroAssembler::load_mirror_from_const_method(Register mirror, Register const_method) {
3212   ld(mirror, in_bytes(ConstMethod::constants_offset()), const_method);
3213   ld(mirror, ConstantPool::pool_holder_offset_in_bytes(), mirror);
3214   ld(mirror, in_bytes(Klass::java_mirror_offset()), mirror);
3215   resolve_oop_handle(mirror);
3216 }
3217 
<span class="line-added">3218 void MacroAssembler::load_method_holder(Register holder, Register method) {</span>
<span class="line-added">3219   ld(holder, in_bytes(Method::const_offset()), method);</span>
<span class="line-added">3220   ld(holder, in_bytes(ConstMethod::constants_offset()), holder);</span>
<span class="line-added">3221   ld(holder, ConstantPool::pool_holder_offset_in_bytes(), holder);</span>
<span class="line-added">3222 }</span>
<span class="line-added">3223 </span>
3224 // Clear Array
3225 // For very short arrays. tmp == R0 is allowed.
3226 void MacroAssembler::clear_memory_unrolled(Register base_ptr, int cnt_dwords, Register tmp, int offset) {
3227   if (cnt_dwords &gt; 0) { li(tmp, 0); }
3228   for (int i = 0; i &lt; cnt_dwords; ++i) { std(tmp, offset + i * 8, base_ptr); }
3229 }
3230 
3231 // Version for constant short array length. Kills base_ptr. tmp == R0 is allowed.
3232 void MacroAssembler::clear_memory_constlen(Register base_ptr, int cnt_dwords, Register tmp) {
3233   if (cnt_dwords &lt; 8) {
3234     clear_memory_unrolled(base_ptr, cnt_dwords, tmp);
3235     return;
3236   }
3237 
3238   Label loop;
3239   const long loopcnt   = cnt_dwords &gt;&gt; 1,
3240              remainder = cnt_dwords &amp; 1;
3241 
3242   li(tmp, loopcnt);
3243   mtctr(tmp);
</pre>
<hr />
<pre>
4901       lwz(R0, mem_offset, mem_base);
4902       cmpwi(CCR0, R0, 0);
4903       break;
4904     case 8:
4905       ld(R0, mem_offset, mem_base);
4906       cmpdi(CCR0, R0, 0);
4907       break;
4908     default:
4909       ShouldNotReachHere();
4910   }
4911   asm_assert(check_equal, msg, id);
4912 #endif // ASSERT
4913 }
4914 
4915 void MacroAssembler::verify_thread() {
4916   if (VerifyThread) {
4917     unimplemented(&quot;&#39;VerifyThread&#39; currently not implemented on PPC&quot;);
4918   }
4919 }
4920 
<span class="line-added">4921 void MacroAssembler::verify_coop(Register coop, const char* msg) {</span>
<span class="line-added">4922   if (!VerifyOops) { return; }</span>
<span class="line-added">4923   if (UseCompressedOops) { decode_heap_oop(coop); }</span>
<span class="line-added">4924   verify_oop(coop, msg);</span>
<span class="line-added">4925   if (UseCompressedOops) { encode_heap_oop(coop, coop); }</span>
<span class="line-added">4926 }</span>
<span class="line-added">4927 </span>
4928 // READ: oop. KILL: R0. Volatile floats perhaps.
4929 void MacroAssembler::verify_oop(Register oop, const char* msg) {
4930   if (!VerifyOops) {
4931     return;
4932   }
4933 
4934   address/* FunctionDescriptor** */fd = StubRoutines::verify_oop_subroutine_entry_address();
4935   const Register tmp = R11; // Will be preserved.
4936   const int nbytes_save = MacroAssembler::num_volatile_regs * 8;
<span class="line-added">4937 </span>
<span class="line-added">4938   BLOCK_COMMENT(&quot;verify_oop {&quot;);</span>
<span class="line-added">4939 </span>
4940   save_volatile_gprs(R1_SP, -nbytes_save); // except R0
4941 
4942   mr_if_needed(R4_ARG2, oop);
4943   save_LR_CR(tmp); // save in old frame
4944   push_frame_reg_args(nbytes_save, tmp);
4945   // load FunctionDescriptor** / entry_address *
4946   load_const_optimized(tmp, fd, R0);
4947   // load FunctionDescriptor* / entry_address
4948   ld(tmp, 0, tmp);
4949   load_const_optimized(R3_ARG1, (address)msg, R0);
4950   // Call destination for its side effect.
4951   call_c(tmp);
4952 
4953   pop_frame();
4954   restore_LR_CR(tmp);
4955   restore_volatile_gprs(R1_SP, -nbytes_save); // except R0
<span class="line-added">4956 </span>
<span class="line-added">4957   BLOCK_COMMENT(&quot;} verify_oop&quot;);</span>
4958 }
4959 
4960 void MacroAssembler::verify_oop_addr(RegisterOrConstant offs, Register base, const char* msg) {
4961   if (!VerifyOops) {
4962     return;
4963   }
4964 
4965   address/* FunctionDescriptor** */fd = StubRoutines::verify_oop_subroutine_entry_address();
4966   const Register tmp = R11; // Will be preserved.
4967   const int nbytes_save = MacroAssembler::num_volatile_regs * 8;
4968   save_volatile_gprs(R1_SP, -nbytes_save); // except R0
4969 
4970   ld(R4_ARG2, offs, base);
4971   save_LR_CR(tmp); // save in old frame
4972   push_frame_reg_args(nbytes_save, tmp);
4973   // load FunctionDescriptor** / entry_address *
4974   load_const_optimized(tmp, fd, R0);
4975   // load FunctionDescriptor* / entry_address
4976   ld(tmp, 0, tmp);
4977   load_const_optimized(R3_ARG1, (address)msg, R0);
</pre>
<hr />
<pre>
5047 }
5048 
5049 #endif // !PRODUCT
5050 
5051 void SkipIfEqualZero::skip_to_label_if_equal_zero(MacroAssembler* masm, Register temp,
5052                                                   const bool* flag_addr, Label&amp; label) {
5053   int simm16_offset = masm-&gt;load_const_optimized(temp, (address)flag_addr, R0, true);
5054   assert(sizeof(bool) == 1, &quot;PowerPC ABI&quot;);
5055   masm-&gt;lbz(temp, simm16_offset, temp);
5056   masm-&gt;cmpwi(CCR0, temp, 0);
5057   masm-&gt;beq(CCR0, label);
5058 }
5059 
5060 SkipIfEqualZero::SkipIfEqualZero(MacroAssembler* masm, Register temp, const bool* flag_addr) : _masm(masm), _label() {
5061   skip_to_label_if_equal_zero(masm, temp, flag_addr, _label);
5062 }
5063 
5064 SkipIfEqualZero::~SkipIfEqualZero() {
5065   _masm-&gt;bind(_label);
5066 }
<span class="line-added">5067 </span>
<span class="line-added">5068 void MacroAssembler::cache_wb(Address line) {</span>
<span class="line-added">5069   assert(line.index() == noreg, &quot;index should be noreg&quot;);</span>
<span class="line-added">5070   assert(line.disp() == 0, &quot;displacement should be 0&quot;);</span>
<span class="line-added">5071   assert(VM_Version::supports_data_cache_line_flush(), &quot;CPU or OS does not support flush to persistent memory&quot;);</span>
<span class="line-added">5072   // Data Cache Store, not really a flush, so it works like a sync of cache</span>
<span class="line-added">5073   // line and persistent mem, i.e. copying the cache line to persistent whilst</span>
<span class="line-added">5074   // not invalidating the cache line.</span>
<span class="line-added">5075   dcbst(line.base());</span>
<span class="line-added">5076 }</span>
<span class="line-added">5077 </span>
<span class="line-added">5078 void MacroAssembler::cache_wbsync(bool is_presync) {</span>
<span class="line-added">5079   assert(VM_Version::supports_data_cache_line_flush(), &quot;CPU or OS does not support sync related to persistent memory&quot;);</span>
<span class="line-added">5080   // We only need a post sync barrier. Post means _after_ a cache line flush or</span>
<span class="line-added">5081   // store instruction, pre means a barrier emitted before such a instructions.</span>
<span class="line-added">5082   if (!is_presync) {</span>
<span class="line-added">5083     fence();</span>
<span class="line-added">5084   }</span>
<span class="line-added">5085 }</span>
</pre>
</td>
</tr>
</table>
<center><a href="jniFastGetField_ppc.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_ppc.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>