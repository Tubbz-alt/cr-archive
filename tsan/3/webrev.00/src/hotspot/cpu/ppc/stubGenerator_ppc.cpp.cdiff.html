<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/cpu/ppc/stubGenerator_ppc.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="sharedRuntime_ppc.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="templateInterpreterGenerator_ppc.cpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/ppc/stubGenerator_ppc.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 39,10 ***</span>
<span class="line-new-header">--- 39,11 ---</span>
  #include &quot;runtime/sharedRuntime.hpp&quot;
  #include &quot;runtime/stubCodeGenerator.hpp&quot;
  #include &quot;runtime/stubRoutines.hpp&quot;
  #include &quot;runtime/thread.inline.hpp&quot;
  #include &quot;utilities/align.hpp&quot;
<span class="line-added">+ #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  
  // Declaration and definition of StubGenerator (no .hpp file).
  // For a more detailed description of the stub routine structure
  // see the comment in stubRoutines.hpp.
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 438,11 ***</span>
    //
    address generate_forward_exception() {
      StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;forward_exception&quot;);
      address start = __ pc();
  
<span class="line-removed">- #if !defined(PRODUCT)</span>
      if (VerifyOops) {
        // Get pending exception oop.
        __ ld(R3_ARG1,
                  in_bytes(Thread::pending_exception_offset()),
                  R16_thread);
<span class="line-new-header">--- 439,10 ---</span>
</pre>
<hr />
<pre>
<span class="line-old-header">*** 454,11 ***</span>
          __ stop(&quot;StubRoutines::forward exception: no pending exception (1)&quot;);
          __ bind(L);
        }
        __ verify_oop(R3_ARG1, &quot;StubRoutines::forward exception: not an oop&quot;);
      }
<span class="line-removed">- #endif</span>
  
      // Save LR/CR and copy exception pc (LR) into R4_ARG2.
      __ save_LR_CR(R4_ARG2);
      __ push_frame_reg_args(0, R0);
      // Find exception handler.
<span class="line-new-header">--- 454,10 ---</span>
</pre>
<hr />
<pre>
<span class="line-old-header">*** 700,13 ***</span>
    }
  
  #if !defined(PRODUCT)
    // Wrapper which calls oopDesc::is_oop_or_null()
    // Only called by MacroAssembler::verify_oop
<span class="line-modified">!   static void verify_oop_helper(const char* message, oop o) {</span>
      if (!oopDesc::is_oop_or_null(o)) {
<span class="line-modified">!       fatal(&quot;%s&quot;, message);</span>
      }
      ++ StubRoutines::_verify_oop_count;
    }
  #endif
  
<span class="line-new-header">--- 699,13 ---</span>
    }
  
  #if !defined(PRODUCT)
    // Wrapper which calls oopDesc::is_oop_or_null()
    // Only called by MacroAssembler::verify_oop
<span class="line-modified">!   static void verify_oop_helper(const char* message, oopDesc* o) {</span>
      if (!oopDesc::is_oop_or_null(o)) {
<span class="line-modified">!       fatal(&quot;%s. oop: &quot; PTR_FORMAT, message, p2i(o));</span>
      }
      ++ StubRoutines::_verify_oop_count;
    }
  #endif
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 723,11 ***</span>
  #endif
  
      return start;
    }
  
<span class="line-removed">- </span>
    // -XX:+OptimizeFill : convert fill/copy loops into intrinsic
    //
    // The code is implemented(ported from sparc) as we believe it benefits JVM98, however
    // tracing(-XX:+TraceOptimizeFill) shows the intrinsic replacement doesn&#39;t happen at all!
    //
<span class="line-new-header">--- 722,10 ---</span>
</pre>
<hr />
<pre>
<span class="line-old-header">*** 950,10 ***</span>
<span class="line-new-header">--- 948,24 ---</span>
      __ bc(Assembler::bcondCRbiIs1, Assembler::bi0(CCR0, Assembler::less), no_overlap_target);
  
      // need to copy backwards
    }
  
<span class="line-added">+   // This is common errorexit stub for UnsafeCopyMemory.</span>
<span class="line-added">+   address generate_unsafecopy_common_error_exit() {</span>
<span class="line-added">+     address start_pc = __ pc();</span>
<span class="line-added">+     Register tmp1 = R6_ARG4;</span>
<span class="line-added">+     // probably copy stub would have changed value reset it.</span>
<span class="line-added">+     if (VM_Version::has_mfdscr()) {</span>
<span class="line-added">+       __ load_const_optimized(tmp1, VM_Version::_dscr_val);</span>
<span class="line-added">+       __ mtdscr(tmp1);</span>
<span class="line-added">+     }</span>
<span class="line-added">+     __ li(R3_RET, 0); // return 0</span>
<span class="line-added">+     __ blr();</span>
<span class="line-added">+     return start_pc;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
    // The guideline in the implementations of generate_disjoint_xxx_copy
    // (xxx=byte,short,int,long,oop) is to copy as many elements as possible with
    // single instructions, but to avoid alignment interrupts (see subsequent
    // comment). Furthermore, we try to minimize misaligned access, even
    // though they cause no alignment interrupt.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 987,154 ***</span>
  
      VectorSRegister tmp_vsr1  = VSR1;
      VectorSRegister tmp_vsr2  = VSR2;
  
      Label l_1, l_2, l_3, l_4, l_5, l_6, l_7, l_8, l_9, l_10;
  
<span class="line-modified">!     // Don&#39;t try anything fancy if arrays don&#39;t have many elements.</span>
<span class="line-modified">!     __ li(tmp3, 0);</span>
<span class="line-modified">!     __ cmpwi(CCR0, R5_ARG3, 17);</span>
<span class="line-modified">!     __ ble(CCR0, l_6); // copy 4 at a time</span>
  
<span class="line-modified">!     if (!aligned) {</span>
<span class="line-modified">!       __ xorr(tmp1, R3_ARG1, R4_ARG2);</span>
<span class="line-modified">!       __ andi_(tmp1, tmp1, 3);</span>
<span class="line-modified">!       __ bne(CCR0, l_6); // If arrays don&#39;t have the same alignment mod 4, do 4 element copy.</span>
<span class="line-removed">- </span>
<span class="line-removed">-       // Copy elements if necessary to align to 4 bytes.</span>
<span class="line-removed">-       __ neg(tmp1, R3_ARG1); // Compute distance to alignment boundary.</span>
<span class="line-removed">-       __ andi_(tmp1, tmp1, 3);</span>
<span class="line-removed">-       __ beq(CCR0, l_2);</span>
<span class="line-removed">- </span>
<span class="line-removed">-       __ subf(R5_ARG3, tmp1, R5_ARG3);</span>
<span class="line-removed">-       __ bind(l_9);</span>
<span class="line-removed">-       __ lbz(tmp2, 0, R3_ARG1);</span>
<span class="line-removed">-       __ addic_(tmp1, tmp1, -1);</span>
<span class="line-removed">-       __ stb(tmp2, 0, R4_ARG2);</span>
<span class="line-removed">-       __ addi(R3_ARG1, R3_ARG1, 1);</span>
<span class="line-removed">-       __ addi(R4_ARG2, R4_ARG2, 1);</span>
<span class="line-removed">-       __ bne(CCR0, l_9);</span>
  
<span class="line-modified">!       __ bind(l_2);</span>
<span class="line-modified">!     }</span>
  
<span class="line-modified">!     // copy 8 elements at a time</span>
<span class="line-modified">!     __ xorr(tmp2, R3_ARG1, R4_ARG2); // skip if src &amp; dest have differing alignment mod 8</span>
<span class="line-modified">!     __ andi_(tmp1, tmp2, 7);</span>
<span class="line-modified">!     __ bne(CCR0, l_7); // not same alignment -&gt; to or from is aligned -&gt; copy 8</span>
  
<span class="line-modified">!     // copy a 2-element word if necessary to align to 8 bytes</span>
<span class="line-modified">!     __ andi_(R0, R3_ARG1, 7);</span>
<span class="line-modified">!     __ beq(CCR0, l_7);</span>
  
<span class="line-modified">!     __ lwzx(tmp2, R3_ARG1, tmp3);</span>
<span class="line-modified">!     __ addi(R5_ARG3, R5_ARG3, -4);</span>
<span class="line-modified">!     __ stwx(tmp2, R4_ARG2, tmp3);</span>
<span class="line-removed">-     { // FasterArrayCopy</span>
<span class="line-removed">-       __ addi(R3_ARG1, R3_ARG1, 4);</span>
<span class="line-removed">-       __ addi(R4_ARG2, R4_ARG2, 4);</span>
<span class="line-removed">-     }</span>
<span class="line-removed">-     __ bind(l_7);</span>
  
<span class="line-modified">!     { // FasterArrayCopy</span>
<span class="line-modified">!       __ cmpwi(CCR0, R5_ARG3, 31);</span>
<span class="line-modified">!       __ ble(CCR0, l_6); // copy 2 at a time if less than 32 elements remain</span>
  
<span class="line-modified">!       __ srdi(tmp1, R5_ARG3, 5);</span>
<span class="line-modified">!       __ andi_(R5_ARG3, R5_ARG3, 31);</span>
<span class="line-modified">!       __ mtctr(tmp1);</span>
  
<span class="line-modified">!      if (!VM_Version::has_vsx()) {</span>
  
<span class="line-modified">!       __ bind(l_8);</span>
<span class="line-removed">-       // Use unrolled version for mass copying (copy 32 elements a time)</span>
<span class="line-removed">-       // Load feeding store gets zero latency on Power6, however not on Power5.</span>
<span class="line-removed">-       // Therefore, the following sequence is made for the good of both.</span>
<span class="line-removed">-       __ ld(tmp1, 0, R3_ARG1);</span>
<span class="line-removed">-       __ ld(tmp2, 8, R3_ARG1);</span>
<span class="line-removed">-       __ ld(tmp3, 16, R3_ARG1);</span>
<span class="line-removed">-       __ ld(tmp4, 24, R3_ARG1);</span>
<span class="line-removed">-       __ std(tmp1, 0, R4_ARG2);</span>
<span class="line-removed">-       __ std(tmp2, 8, R4_ARG2);</span>
<span class="line-removed">-       __ std(tmp3, 16, R4_ARG2);</span>
<span class="line-removed">-       __ std(tmp4, 24, R4_ARG2);</span>
<span class="line-removed">-       __ addi(R3_ARG1, R3_ARG1, 32);</span>
<span class="line-removed">-       __ addi(R4_ARG2, R4_ARG2, 32);</span>
<span class="line-removed">-       __ bdnz(l_8);</span>
  
<span class="line-modified">!     } else { // Processor supports VSX, so use it to mass copy.</span>
  
<span class="line-modified">!       // Prefetch the data into the L2 cache.</span>
<span class="line-removed">-       __ dcbt(R3_ARG1, 0);</span>
  
<span class="line-modified">!       // If supported set DSCR pre-fetch to deepest.</span>
<span class="line-modified">!       if (VM_Version::has_mfdscr()) {</span>
<span class="line-removed">-         __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);</span>
<span class="line-removed">-         __ mtdscr(tmp2);</span>
<span class="line-removed">-       }</span>
  
<span class="line-modified">!       __ li(tmp1, 16);</span>
  
<span class="line-modified">!       // Backbranch target aligned to 32-byte. Not 16-byte align as</span>
<span class="line-removed">-       // loop contains &lt; 8 instructions that fit inside a single</span>
<span class="line-removed">-       // i-cache sector.</span>
<span class="line-removed">-       __ align(32);</span>
  
<span class="line-modified">!       __ bind(l_10);</span>
<span class="line-modified">!       // Use loop with VSX load/store instructions to</span>
<span class="line-modified">!       // copy 32 elements a time.</span>
<span class="line-modified">!       __ lxvd2x(tmp_vsr1, R3_ARG1);        // Load src</span>
<span class="line-removed">-       __ stxvd2x(tmp_vsr1, R4_ARG2);       // Store to dst</span>
<span class="line-removed">-       __ lxvd2x(tmp_vsr2, tmp1, R3_ARG1);  // Load src + 16</span>
<span class="line-removed">-       __ stxvd2x(tmp_vsr2, tmp1, R4_ARG2); // Store to dst + 16</span>
<span class="line-removed">-       __ addi(R3_ARG1, R3_ARG1, 32);       // Update src+=32</span>
<span class="line-removed">-       __ addi(R4_ARG2, R4_ARG2, 32);       // Update dsc+=32</span>
<span class="line-removed">-       __ bdnz(l_10);                       // Dec CTR and loop if not zero.</span>
  
<span class="line-modified">!       // Restore DSCR pre-fetch value.</span>
<span class="line-modified">!       if (VM_Version::has_mfdscr()) {</span>
<span class="line-modified">!         __ load_const_optimized(tmp2, VM_Version::_dscr_val);</span>
<span class="line-modified">!         __ mtdscr(tmp2);</span>
<span class="line-modified">!       }</span>
  
<span class="line-modified">!     } // VSX</span>
<span class="line-modified">!    } // FasterArrayCopy</span>
  
<span class="line-modified">!     __ bind(l_6);</span>
  
<span class="line-modified">!     // copy 4 elements at a time</span>
<span class="line-removed">-     __ cmpwi(CCR0, R5_ARG3, 4);</span>
<span class="line-removed">-     __ blt(CCR0, l_1);</span>
<span class="line-removed">-     __ srdi(tmp1, R5_ARG3, 2);</span>
<span class="line-removed">-     __ mtctr(tmp1); // is &gt; 0</span>
<span class="line-removed">-     __ andi_(R5_ARG3, R5_ARG3, 3);</span>
  
<span class="line-modified">!     { // FasterArrayCopy</span>
<span class="line-modified">!       __ addi(R3_ARG1, R3_ARG1, -4);</span>
<span class="line-modified">!       __ addi(R4_ARG2, R4_ARG2, -4);</span>
<span class="line-modified">!       __ bind(l_3);</span>
<span class="line-modified">!       __ lwzu(tmp2, 4, R3_ARG1);</span>
<span class="line-modified">!       __ stwu(tmp2, 4, R4_ARG2);</span>
<span class="line-removed">-       __ bdnz(l_3);</span>
<span class="line-removed">-       __ addi(R3_ARG1, R3_ARG1, 4);</span>
<span class="line-removed">-       __ addi(R4_ARG2, R4_ARG2, 4);</span>
<span class="line-removed">-     }</span>
  
<span class="line-modified">!     // do single element copy</span>
<span class="line-modified">!     __ bind(l_1);</span>
<span class="line-modified">!     __ cmpwi(CCR0, R5_ARG3, 0);</span>
<span class="line-modified">!     __ beq(CCR0, l_4);</span>
  
<span class="line-modified">!     { // FasterArrayCopy</span>
<span class="line-modified">!       __ mtctr(R5_ARG3);</span>
<span class="line-modified">!       __ addi(R3_ARG1, R3_ARG1, -1);</span>
<span class="line-modified">!       __ addi(R4_ARG2, R4_ARG2, -1);</span>
  
<span class="line-modified">!       __ bind(l_5);</span>
<span class="line-modified">!       __ lbzu(tmp2, 1, R3_ARG1);</span>
<span class="line-modified">!       __ stbu(tmp2, 1, R4_ARG2);</span>
<span class="line-modified">!       __ bdnz(l_5);</span>
      }
  
      __ bind(l_4);
      __ li(R3_RET, 0); // return 0
      __ blr();
<span class="line-new-header">--- 999,158 ---</span>
  
      VectorSRegister tmp_vsr1  = VSR1;
      VectorSRegister tmp_vsr2  = VSR2;
  
      Label l_1, l_2, l_3, l_4, l_5, l_6, l_7, l_8, l_9, l_10;
<span class="line-added">+     {</span>
<span class="line-added">+       // UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit</span>
<span class="line-added">+       UnsafeCopyMemoryMark ucmm(this, !aligned, false);</span>
  
<span class="line-modified">!       // Don&#39;t try anything fancy if arrays don&#39;t have many elements.</span>
<span class="line-modified">!       __ li(tmp3, 0);</span>
<span class="line-modified">!       __ cmpwi(CCR0, R5_ARG3, 17);</span>
<span class="line-modified">!       __ ble(CCR0, l_6); // copy 4 at a time</span>
  
<span class="line-modified">!       if (!aligned) {</span>
<span class="line-modified">!         __ xorr(tmp1, R3_ARG1, R4_ARG2);</span>
<span class="line-modified">!         __ andi_(tmp1, tmp1, 3);</span>
<span class="line-modified">!         __ bne(CCR0, l_6); // If arrays don&#39;t have the same alignment mod 4, do 4 element copy.</span>
  
<span class="line-modified">!         // Copy elements if necessary to align to 4 bytes.</span>
<span class="line-modified">!         __ neg(tmp1, R3_ARG1); // Compute distance to alignment boundary.</span>
<span class="line-added">+         __ andi_(tmp1, tmp1, 3);</span>
<span class="line-added">+         __ beq(CCR0, l_2);</span>
  
<span class="line-modified">!         __ subf(R5_ARG3, tmp1, R5_ARG3);</span>
<span class="line-modified">!         __ bind(l_9);</span>
<span class="line-modified">!         __ lbz(tmp2, 0, R3_ARG1);</span>
<span class="line-modified">!         __ addic_(tmp1, tmp1, -1);</span>
<span class="line-added">+         __ stb(tmp2, 0, R4_ARG2);</span>
<span class="line-added">+         __ addi(R3_ARG1, R3_ARG1, 1);</span>
<span class="line-added">+         __ addi(R4_ARG2, R4_ARG2, 1);</span>
<span class="line-added">+         __ bne(CCR0, l_9);</span>
<span class="line-added">+ </span>
<span class="line-added">+         __ bind(l_2);</span>
<span class="line-added">+       }</span>
  
<span class="line-modified">!       // copy 8 elements at a time</span>
<span class="line-modified">!       __ xorr(tmp2, R3_ARG1, R4_ARG2); // skip if src &amp; dest have differing alignment mod 8</span>
<span class="line-modified">!       __ andi_(tmp1, tmp2, 7);</span>
<span class="line-added">+       __ bne(CCR0, l_7); // not same alignment -&gt; to or from is aligned -&gt; copy 8</span>
  
<span class="line-modified">!       // copy a 2-element word if necessary to align to 8 bytes</span>
<span class="line-modified">!       __ andi_(R0, R3_ARG1, 7);</span>
<span class="line-modified">!       __ beq(CCR0, l_7);</span>
  
<span class="line-modified">!       __ lwzx(tmp2, R3_ARG1, tmp3);</span>
<span class="line-modified">!       __ addi(R5_ARG3, R5_ARG3, -4);</span>
<span class="line-modified">!       __ stwx(tmp2, R4_ARG2, tmp3);</span>
<span class="line-added">+       { // FasterArrayCopy</span>
<span class="line-added">+         __ addi(R3_ARG1, R3_ARG1, 4);</span>
<span class="line-added">+         __ addi(R4_ARG2, R4_ARG2, 4);</span>
<span class="line-added">+       }</span>
<span class="line-added">+       __ bind(l_7);</span>
  
<span class="line-modified">!       { // FasterArrayCopy</span>
<span class="line-modified">!         __ cmpwi(CCR0, R5_ARG3, 31);</span>
<span class="line-modified">!         __ ble(CCR0, l_6); // copy 2 at a time if less than 32 elements remain</span>
  
<span class="line-modified">!         __ srdi(tmp1, R5_ARG3, 5);</span>
<span class="line-added">+         __ andi_(R5_ARG3, R5_ARG3, 31);</span>
<span class="line-added">+         __ mtctr(tmp1);</span>
  
<span class="line-modified">!        if (!VM_Version::has_vsx()) {</span>
  
<span class="line-modified">!         __ bind(l_8);</span>
<span class="line-added">+         // Use unrolled version for mass copying (copy 32 elements a time)</span>
<span class="line-added">+         // Load feeding store gets zero latency on Power6, however not on Power5.</span>
<span class="line-added">+         // Therefore, the following sequence is made for the good of both.</span>
<span class="line-added">+         __ ld(tmp1, 0, R3_ARG1);</span>
<span class="line-added">+         __ ld(tmp2, 8, R3_ARG1);</span>
<span class="line-added">+         __ ld(tmp3, 16, R3_ARG1);</span>
<span class="line-added">+         __ ld(tmp4, 24, R3_ARG1);</span>
<span class="line-added">+         __ std(tmp1, 0, R4_ARG2);</span>
<span class="line-added">+         __ std(tmp2, 8, R4_ARG2);</span>
<span class="line-added">+         __ std(tmp3, 16, R4_ARG2);</span>
<span class="line-added">+         __ std(tmp4, 24, R4_ARG2);</span>
<span class="line-added">+         __ addi(R3_ARG1, R3_ARG1, 32);</span>
<span class="line-added">+         __ addi(R4_ARG2, R4_ARG2, 32);</span>
<span class="line-added">+         __ bdnz(l_8);</span>
  
<span class="line-modified">!       } else { // Processor supports VSX, so use it to mass copy.</span>
  
<span class="line-modified">!         // Prefetch the data into the L2 cache.</span>
<span class="line-modified">!         __ dcbt(R3_ARG1, 0);</span>
  
<span class="line-modified">!         // If supported set DSCR pre-fetch to deepest.</span>
<span class="line-added">+         if (VM_Version::has_mfdscr()) {</span>
<span class="line-added">+           __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);</span>
<span class="line-added">+           __ mtdscr(tmp2);</span>
<span class="line-added">+         }</span>
  
<span class="line-modified">!         __ li(tmp1, 16);</span>
  
<span class="line-modified">!         // Backbranch target aligned to 32-byte. Not 16-byte align as</span>
<span class="line-modified">!         // loop contains &lt; 8 instructions that fit inside a single</span>
<span class="line-modified">!         // i-cache sector.</span>
<span class="line-modified">!         __ align(32);</span>
  
<span class="line-modified">!         __ bind(l_10);</span>
<span class="line-modified">!         // Use loop with VSX load/store instructions to</span>
<span class="line-modified">!         // copy 32 elements a time.</span>
<span class="line-modified">!         __ lxvd2x(tmp_vsr1, R3_ARG1);        // Load src</span>
<span class="line-modified">!         __ stxvd2x(tmp_vsr1, R4_ARG2);       // Store to dst</span>
<span class="line-added">+         __ lxvd2x(tmp_vsr2, tmp1, R3_ARG1);  // Load src + 16</span>
<span class="line-added">+         __ stxvd2x(tmp_vsr2, tmp1, R4_ARG2); // Store to dst + 16</span>
<span class="line-added">+         __ addi(R3_ARG1, R3_ARG1, 32);       // Update src+=32</span>
<span class="line-added">+         __ addi(R4_ARG2, R4_ARG2, 32);       // Update dsc+=32</span>
<span class="line-added">+         __ bdnz(l_10);                       // Dec CTR and loop if not zero.</span>
  
<span class="line-modified">!         // Restore DSCR pre-fetch value.</span>
<span class="line-modified">!         if (VM_Version::has_mfdscr()) {</span>
<span class="line-added">+           __ load_const_optimized(tmp2, VM_Version::_dscr_val);</span>
<span class="line-added">+           __ mtdscr(tmp2);</span>
<span class="line-added">+         }</span>
  
<span class="line-modified">!       } // VSX</span>
<span class="line-added">+      } // FasterArrayCopy</span>
  
<span class="line-modified">!       __ bind(l_6);</span>
  
<span class="line-modified">!       // copy 4 elements at a time</span>
<span class="line-modified">!       __ cmpwi(CCR0, R5_ARG3, 4);</span>
<span class="line-modified">!       __ blt(CCR0, l_1);</span>
<span class="line-modified">!       __ srdi(tmp1, R5_ARG3, 2);</span>
<span class="line-modified">!       __ mtctr(tmp1); // is &gt; 0</span>
<span class="line-modified">!       __ andi_(R5_ARG3, R5_ARG3, 3);</span>
  
<span class="line-modified">!       { // FasterArrayCopy</span>
<span class="line-modified">!         __ addi(R3_ARG1, R3_ARG1, -4);</span>
<span class="line-modified">!         __ addi(R4_ARG2, R4_ARG2, -4);</span>
<span class="line-modified">!         __ bind(l_3);</span>
<span class="line-added">+         __ lwzu(tmp2, 4, R3_ARG1);</span>
<span class="line-added">+         __ stwu(tmp2, 4, R4_ARG2);</span>
<span class="line-added">+         __ bdnz(l_3);</span>
<span class="line-added">+         __ addi(R3_ARG1, R3_ARG1, 4);</span>
<span class="line-added">+         __ addi(R4_ARG2, R4_ARG2, 4);</span>
<span class="line-added">+       }</span>
  
<span class="line-modified">!       // do single element copy</span>
<span class="line-modified">!       __ bind(l_1);</span>
<span class="line-modified">!       __ cmpwi(CCR0, R5_ARG3, 0);</span>
<span class="line-modified">!       __ beq(CCR0, l_4);</span>
  
<span class="line-modified">!       { // FasterArrayCopy</span>
<span class="line-modified">!         __ mtctr(R5_ARG3);</span>
<span class="line-modified">!         __ addi(R3_ARG1, R3_ARG1, -1);</span>
<span class="line-modified">!         __ addi(R4_ARG2, R4_ARG2, -1);</span>
<span class="line-added">+ </span>
<span class="line-added">+         __ bind(l_5);</span>
<span class="line-added">+         __ lbzu(tmp2, 1, R3_ARG1);</span>
<span class="line-added">+         __ stbu(tmp2, 1, R4_ARG2);</span>
<span class="line-added">+         __ bdnz(l_5);</span>
<span class="line-added">+       }</span>
      }
  
      __ bind(l_4);
      __ li(R3_RET, 0); // return 0
      __ blr();
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1165,19 ***</span>
  
      array_overlap_test(nooverlap_target, 0);
      // Do reverse copy. We assume the case of actual overlap is rare enough
      // that we don&#39;t have to optimize it.
      Label l_1, l_2;
<span class="line-modified">! </span>
<span class="line-modified">!     __ b(l_2);</span>
<span class="line-modified">!     __ bind(l_1);</span>
<span class="line-modified">!     __ stbx(tmp1, R4_ARG2, R5_ARG3);</span>
<span class="line-modified">!     __ bind(l_2);</span>
<span class="line-modified">!     __ addic_(R5_ARG3, R5_ARG3, -1);</span>
<span class="line-modified">!     __ lbzx(tmp1, R3_ARG1, R5_ARG3);</span>
<span class="line-modified">!     __ bge(CCR0, l_1);</span>
<span class="line-modified">! </span>
      __ li(R3_RET, 0); // return 0
      __ blr();
  
      return start;
    }
<span class="line-new-header">--- 1181,21 ---</span>
  
      array_overlap_test(nooverlap_target, 0);
      // Do reverse copy. We assume the case of actual overlap is rare enough
      // that we don&#39;t have to optimize it.
      Label l_1, l_2;
<span class="line-modified">!     {</span>
<span class="line-modified">!       // UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit</span>
<span class="line-modified">!       UnsafeCopyMemoryMark ucmm(this, !aligned, false);</span>
<span class="line-modified">!       __ b(l_2);</span>
<span class="line-modified">!       __ bind(l_1);</span>
<span class="line-modified">!       __ stbx(tmp1, R4_ARG2, R5_ARG3);</span>
<span class="line-modified">!       __ bind(l_2);</span>
<span class="line-modified">!       __ addic_(R5_ARG3, R5_ARG3, -1);</span>
<span class="line-modified">!       __ lbzx(tmp1, R3_ARG1, R5_ARG3);</span>
<span class="line-added">+       __ bge(CCR0, l_1);</span>
<span class="line-added">+     }</span>
      __ li(R3_RET, 0); // return 0
      __ blr();
  
      return start;
    }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1250,159 ***</span>
  
      address start = __ function_entry();
      assert_positive_int(R5_ARG3);
  
      Label l_1, l_2, l_3, l_4, l_5, l_6, l_7, l_8, l_9;
  
<span class="line-modified">!     // don&#39;t try anything fancy if arrays don&#39;t have many elements</span>
<span class="line-modified">!     __ li(tmp3, 0);</span>
<span class="line-modified">!     __ cmpwi(CCR0, R5_ARG3, 9);</span>
<span class="line-modified">!     __ ble(CCR0, l_6); // copy 2 at a time</span>
<span class="line-removed">- </span>
<span class="line-removed">-     if (!aligned) {</span>
<span class="line-removed">-       __ xorr(tmp1, R3_ARG1, R4_ARG2);</span>
<span class="line-removed">-       __ andi_(tmp1, tmp1, 3);</span>
<span class="line-removed">-       __ bne(CCR0, l_6); // if arrays don&#39;t have the same alignment mod 4, do 2 element copy</span>
  
<span class="line-modified">!       // At this point it is guaranteed that both, from and to have the same alignment mod 4.</span>
  
<span class="line-modified">!       // Copy 1 element if necessary to align to 4 bytes.</span>
<span class="line-modified">!       __ andi_(tmp1, R3_ARG1, 3);</span>
<span class="line-modified">!       __ beq(CCR0, l_2);</span>
  
<span class="line-modified">!       __ lhz(tmp2, 0, R3_ARG1);</span>
<span class="line-modified">!       __ addi(R3_ARG1, R3_ARG1, 2);</span>
<span class="line-modified">!       __ sth(tmp2, 0, R4_ARG2);</span>
<span class="line-modified">!       __ addi(R4_ARG2, R4_ARG2, 2);</span>
<span class="line-modified">!       __ addi(R5_ARG3, R5_ARG3, -1);</span>
<span class="line-modified">!       __ bind(l_2);</span>
  
<span class="line-modified">!       // At this point the positions of both, from and to, are at least 4 byte aligned.</span>
  
<span class="line-modified">!       // Copy 4 elements at a time.</span>
<span class="line-modified">!       // Align to 8 bytes, but only if both, from and to, have same alignment mod 8.</span>
<span class="line-modified">!       __ xorr(tmp2, R3_ARG1, R4_ARG2);</span>
<span class="line-modified">!       __ andi_(tmp1, tmp2, 7);</span>
<span class="line-modified">!       __ bne(CCR0, l_7); // not same alignment mod 8 -&gt; copy 4, either from or to will be unaligned</span>
  
<span class="line-modified">!       // Copy a 2-element word if necessary to align to 8 bytes.</span>
<span class="line-modified">!       __ andi_(R0, R3_ARG1, 7);</span>
<span class="line-modified">!       __ beq(CCR0, l_7);</span>
  
<span class="line-modified">!       __ lwzx(tmp2, R3_ARG1, tmp3);</span>
<span class="line-modified">!       __ addi(R5_ARG3, R5_ARG3, -2);</span>
<span class="line-modified">!       __ stwx(tmp2, R4_ARG2, tmp3);</span>
<span class="line-modified">!       { // FasterArrayCopy</span>
<span class="line-modified">!         __ addi(R3_ARG1, R3_ARG1, 4);</span>
<span class="line-modified">!         __ addi(R4_ARG2, R4_ARG2, 4);</span>
        }
<span class="line-removed">-     }</span>
<span class="line-removed">- </span>
<span class="line-removed">-     __ bind(l_7);</span>
<span class="line-removed">- </span>
<span class="line-removed">-     // Copy 4 elements at a time; either the loads or the stores can</span>
<span class="line-removed">-     // be unaligned if aligned == false.</span>
<span class="line-removed">- </span>
<span class="line-removed">-     { // FasterArrayCopy</span>
<span class="line-removed">-       __ cmpwi(CCR0, R5_ARG3, 15);</span>
<span class="line-removed">-       __ ble(CCR0, l_6); // copy 2 at a time if less than 16 elements remain</span>
  
<span class="line-modified">!       __ srdi(tmp1, R5_ARG3, 4);</span>
<span class="line-removed">-       __ andi_(R5_ARG3, R5_ARG3, 15);</span>
<span class="line-removed">-       __ mtctr(tmp1);</span>
<span class="line-removed">- </span>
<span class="line-removed">-       if (!VM_Version::has_vsx()) {</span>
<span class="line-removed">- </span>
<span class="line-removed">-         __ bind(l_8);</span>
<span class="line-removed">-         // Use unrolled version for mass copying (copy 16 elements a time).</span>
<span class="line-removed">-         // Load feeding store gets zero latency on Power6, however not on Power5.</span>
<span class="line-removed">-         // Therefore, the following sequence is made for the good of both.</span>
<span class="line-removed">-         __ ld(tmp1, 0, R3_ARG1);</span>
<span class="line-removed">-         __ ld(tmp2, 8, R3_ARG1);</span>
<span class="line-removed">-         __ ld(tmp3, 16, R3_ARG1);</span>
<span class="line-removed">-         __ ld(tmp4, 24, R3_ARG1);</span>
<span class="line-removed">-         __ std(tmp1, 0, R4_ARG2);</span>
<span class="line-removed">-         __ std(tmp2, 8, R4_ARG2);</span>
<span class="line-removed">-         __ std(tmp3, 16, R4_ARG2);</span>
<span class="line-removed">-         __ std(tmp4, 24, R4_ARG2);</span>
<span class="line-removed">-         __ addi(R3_ARG1, R3_ARG1, 32);</span>
<span class="line-removed">-         __ addi(R4_ARG2, R4_ARG2, 32);</span>
<span class="line-removed">-         __ bdnz(l_8);</span>
  
<span class="line-modified">!       } else { // Processor supports VSX, so use it to mass copy.</span>
  
<span class="line-modified">!         // Prefetch src data into L2 cache.</span>
<span class="line-modified">!         __ dcbt(R3_ARG1, 0);</span>
  
<span class="line-removed">-         // If supported set DSCR pre-fetch to deepest.</span>
<span class="line-removed">-         if (VM_Version::has_mfdscr()) {</span>
<span class="line-removed">-           __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);</span>
<span class="line-removed">-           __ mtdscr(tmp2);</span>
          }
<span class="line-modified">!         __ li(tmp1, 16);</span>
  
<span class="line-modified">!         // Backbranch target aligned to 32-byte. It&#39;s not aligned 16-byte</span>
<span class="line-modified">!         // as loop contains &lt; 8 instructions that fit inside a single</span>
<span class="line-modified">!         // i-cache sector.</span>
<span class="line-modified">!         __ align(32);</span>
  
<span class="line-modified">!         __ bind(l_9);</span>
<span class="line-modified">!         // Use loop with VSX load/store instructions to</span>
<span class="line-modified">!         // copy 16 elements a time.</span>
<span class="line-removed">-         __ lxvd2x(tmp_vsr1, R3_ARG1);        // Load from src.</span>
<span class="line-removed">-         __ stxvd2x(tmp_vsr1, R4_ARG2);       // Store to dst.</span>
<span class="line-removed">-         __ lxvd2x(tmp_vsr2, R3_ARG1, tmp1);  // Load from src + 16.</span>
<span class="line-removed">-         __ stxvd2x(tmp_vsr2, R4_ARG2, tmp1); // Store to dst + 16.</span>
<span class="line-removed">-         __ addi(R3_ARG1, R3_ARG1, 32);       // Update src+=32.</span>
<span class="line-removed">-         __ addi(R4_ARG2, R4_ARG2, 32);       // Update dsc+=32.</span>
<span class="line-removed">-         __ bdnz(l_9);                        // Dec CTR and loop if not zero.</span>
  
<span class="line-modified">!         // Restore DSCR pre-fetch value.</span>
<span class="line-modified">!         if (VM_Version::has_mfdscr()) {</span>
<span class="line-modified">!           __ load_const_optimized(tmp2, VM_Version::_dscr_val);</span>
<span class="line-modified">!           __ mtdscr(tmp2);</span>
<span class="line-removed">-         }</span>
  
        }
<span class="line-removed">-     } // FasterArrayCopy</span>
<span class="line-removed">-     __ bind(l_6);</span>
  
<span class="line-modified">!     // copy 2 elements at a time</span>
<span class="line-modified">!     { // FasterArrayCopy</span>
<span class="line-modified">!       __ cmpwi(CCR0, R5_ARG3, 2);</span>
<span class="line-modified">!       __ blt(CCR0, l_1);</span>
<span class="line-removed">-       __ srdi(tmp1, R5_ARG3, 1);</span>
<span class="line-removed">-       __ andi_(R5_ARG3, R5_ARG3, 1);</span>
<span class="line-removed">- </span>
<span class="line-removed">-       __ addi(R3_ARG1, R3_ARG1, -4);</span>
<span class="line-removed">-       __ addi(R4_ARG2, R4_ARG2, -4);</span>
<span class="line-removed">-       __ mtctr(tmp1);</span>
<span class="line-removed">- </span>
<span class="line-removed">-       __ bind(l_3);</span>
<span class="line-removed">-       __ lwzu(tmp2, 4, R3_ARG1);</span>
<span class="line-removed">-       __ stwu(tmp2, 4, R4_ARG2);</span>
<span class="line-removed">-       __ bdnz(l_3);</span>
  
<span class="line-modified">!       __ addi(R3_ARG1, R3_ARG1, 4);</span>
<span class="line-modified">!       __ addi(R4_ARG2, R4_ARG2, 4);</span>
      }
  
<span class="line-removed">-     // do single element copy</span>
<span class="line-removed">-     __ bind(l_1);</span>
<span class="line-removed">-     __ cmpwi(CCR0, R5_ARG3, 0);</span>
<span class="line-removed">-     __ beq(CCR0, l_4);</span>
<span class="line-removed">- </span>
<span class="line-removed">-     { // FasterArrayCopy</span>
<span class="line-removed">-       __ mtctr(R5_ARG3);</span>
<span class="line-removed">-       __ addi(R3_ARG1, R3_ARG1, -2);</span>
<span class="line-removed">-       __ addi(R4_ARG2, R4_ARG2, -2);</span>
<span class="line-removed">- </span>
<span class="line-removed">-       __ bind(l_5);</span>
<span class="line-removed">-       __ lhzu(tmp2, 2, R3_ARG1);</span>
<span class="line-removed">-       __ sthu(tmp2, 2, R4_ARG2);</span>
<span class="line-removed">-       __ bdnz(l_5);</span>
<span class="line-removed">-     }</span>
      __ bind(l_4);
      __ li(R3_RET, 0); // return 0
      __ blr();
  
      return start;
<span class="line-new-header">--- 1268,163 ---</span>
  
      address start = __ function_entry();
      assert_positive_int(R5_ARG3);
  
      Label l_1, l_2, l_3, l_4, l_5, l_6, l_7, l_8, l_9;
<span class="line-added">+     {</span>
<span class="line-added">+       // UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit</span>
<span class="line-added">+       UnsafeCopyMemoryMark ucmm(this, !aligned, false);</span>
<span class="line-added">+       // don&#39;t try anything fancy if arrays don&#39;t have many elements</span>
<span class="line-added">+       __ li(tmp3, 0);</span>
<span class="line-added">+       __ cmpwi(CCR0, R5_ARG3, 9);</span>
<span class="line-added">+       __ ble(CCR0, l_6); // copy 2 at a time</span>
  
<span class="line-modified">!       if (!aligned) {</span>
<span class="line-modified">!         __ xorr(tmp1, R3_ARG1, R4_ARG2);</span>
<span class="line-modified">!         __ andi_(tmp1, tmp1, 3);</span>
<span class="line-modified">!         __ bne(CCR0, l_6); // if arrays don&#39;t have the same alignment mod 4, do 2 element copy</span>
  
<span class="line-modified">!         // At this point it is guaranteed that both, from and to have the same alignment mod 4.</span>
  
<span class="line-modified">!         // Copy 1 element if necessary to align to 4 bytes.</span>
<span class="line-modified">!         __ andi_(tmp1, R3_ARG1, 3);</span>
<span class="line-modified">!         __ beq(CCR0, l_2);</span>
  
<span class="line-modified">!         __ lhz(tmp2, 0, R3_ARG1);</span>
<span class="line-modified">!         __ addi(R3_ARG1, R3_ARG1, 2);</span>
<span class="line-modified">!         __ sth(tmp2, 0, R4_ARG2);</span>
<span class="line-modified">!         __ addi(R4_ARG2, R4_ARG2, 2);</span>
<span class="line-modified">!         __ addi(R5_ARG3, R5_ARG3, -1);</span>
<span class="line-modified">!         __ bind(l_2);</span>
  
<span class="line-modified">!         // At this point the positions of both, from and to, are at least 4 byte aligned.</span>
  
<span class="line-modified">!         // Copy 4 elements at a time.</span>
<span class="line-modified">!         // Align to 8 bytes, but only if both, from and to, have same alignment mod 8.</span>
<span class="line-modified">!         __ xorr(tmp2, R3_ARG1, R4_ARG2);</span>
<span class="line-modified">!         __ andi_(tmp1, tmp2, 7);</span>
<span class="line-modified">!         __ bne(CCR0, l_7); // not same alignment mod 8 -&gt; copy 4, either from or to will be unaligned</span>
  
<span class="line-modified">!         // Copy a 2-element word if necessary to align to 8 bytes.</span>
<span class="line-modified">!         __ andi_(R0, R3_ARG1, 7);</span>
<span class="line-modified">!         __ beq(CCR0, l_7);</span>
  
<span class="line-modified">!         __ lwzx(tmp2, R3_ARG1, tmp3);</span>
<span class="line-modified">!         __ addi(R5_ARG3, R5_ARG3, -2);</span>
<span class="line-modified">!         __ stwx(tmp2, R4_ARG2, tmp3);</span>
<span class="line-modified">!         { // FasterArrayCopy</span>
<span class="line-modified">!           __ addi(R3_ARG1, R3_ARG1, 4);</span>
<span class="line-modified">!           __ addi(R4_ARG2, R4_ARG2, 4);</span>
<span class="line-added">+         }</span>
        }
  
<span class="line-modified">!       __ bind(l_7);</span>
  
<span class="line-modified">!       // Copy 4 elements at a time; either the loads or the stores can</span>
<span class="line-added">+       // be unaligned if aligned == false.</span>
  
<span class="line-modified">!       { // FasterArrayCopy</span>
<span class="line-modified">!         __ cmpwi(CCR0, R5_ARG3, 15);</span>
<span class="line-added">+         __ ble(CCR0, l_6); // copy 2 at a time if less than 16 elements remain</span>
<span class="line-added">+ </span>
<span class="line-added">+         __ srdi(tmp1, R5_ARG3, 4);</span>
<span class="line-added">+         __ andi_(R5_ARG3, R5_ARG3, 15);</span>
<span class="line-added">+         __ mtctr(tmp1);</span>
<span class="line-added">+ </span>
<span class="line-added">+         if (!VM_Version::has_vsx()) {</span>
<span class="line-added">+ </span>
<span class="line-added">+           __ bind(l_8);</span>
<span class="line-added">+           // Use unrolled version for mass copying (copy 16 elements a time).</span>
<span class="line-added">+           // Load feeding store gets zero latency on Power6, however not on Power5.</span>
<span class="line-added">+           // Therefore, the following sequence is made for the good of both.</span>
<span class="line-added">+           __ ld(tmp1, 0, R3_ARG1);</span>
<span class="line-added">+           __ ld(tmp2, 8, R3_ARG1);</span>
<span class="line-added">+           __ ld(tmp3, 16, R3_ARG1);</span>
<span class="line-added">+           __ ld(tmp4, 24, R3_ARG1);</span>
<span class="line-added">+           __ std(tmp1, 0, R4_ARG2);</span>
<span class="line-added">+           __ std(tmp2, 8, R4_ARG2);</span>
<span class="line-added">+           __ std(tmp3, 16, R4_ARG2);</span>
<span class="line-added">+           __ std(tmp4, 24, R4_ARG2);</span>
<span class="line-added">+           __ addi(R3_ARG1, R3_ARG1, 32);</span>
<span class="line-added">+           __ addi(R4_ARG2, R4_ARG2, 32);</span>
<span class="line-added">+           __ bdnz(l_8);</span>
<span class="line-added">+ </span>
<span class="line-added">+         } else { // Processor supports VSX, so use it to mass copy.</span>
<span class="line-added">+ </span>
<span class="line-added">+           // Prefetch src data into L2 cache.</span>
<span class="line-added">+           __ dcbt(R3_ARG1, 0);</span>
<span class="line-added">+ </span>
<span class="line-added">+           // If supported set DSCR pre-fetch to deepest.</span>
<span class="line-added">+           if (VM_Version::has_mfdscr()) {</span>
<span class="line-added">+             __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);</span>
<span class="line-added">+             __ mtdscr(tmp2);</span>
<span class="line-added">+           }</span>
<span class="line-added">+           __ li(tmp1, 16);</span>
<span class="line-added">+ </span>
<span class="line-added">+           // Backbranch target aligned to 32-byte. It&#39;s not aligned 16-byte</span>
<span class="line-added">+           // as loop contains &lt; 8 instructions that fit inside a single</span>
<span class="line-added">+           // i-cache sector.</span>
<span class="line-added">+           __ align(32);</span>
<span class="line-added">+ </span>
<span class="line-added">+           __ bind(l_9);</span>
<span class="line-added">+           // Use loop with VSX load/store instructions to</span>
<span class="line-added">+           // copy 16 elements a time.</span>
<span class="line-added">+           __ lxvd2x(tmp_vsr1, R3_ARG1);        // Load from src.</span>
<span class="line-added">+           __ stxvd2x(tmp_vsr1, R4_ARG2);       // Store to dst.</span>
<span class="line-added">+           __ lxvd2x(tmp_vsr2, R3_ARG1, tmp1);  // Load from src + 16.</span>
<span class="line-added">+           __ stxvd2x(tmp_vsr2, R4_ARG2, tmp1); // Store to dst + 16.</span>
<span class="line-added">+           __ addi(R3_ARG1, R3_ARG1, 32);       // Update src+=32.</span>
<span class="line-added">+           __ addi(R4_ARG2, R4_ARG2, 32);       // Update dsc+=32.</span>
<span class="line-added">+           __ bdnz(l_9);                        // Dec CTR and loop if not zero.</span>
<span class="line-added">+ </span>
<span class="line-added">+           // Restore DSCR pre-fetch value.</span>
<span class="line-added">+           if (VM_Version::has_mfdscr()) {</span>
<span class="line-added">+             __ load_const_optimized(tmp2, VM_Version::_dscr_val);</span>
<span class="line-added">+             __ mtdscr(tmp2);</span>
<span class="line-added">+           }</span>
  
          }
<span class="line-modified">!       } // FasterArrayCopy</span>
<span class="line-added">+       __ bind(l_6);</span>
  
<span class="line-modified">!       // copy 2 elements at a time</span>
<span class="line-modified">!       { // FasterArrayCopy</span>
<span class="line-modified">!         __ cmpwi(CCR0, R5_ARG3, 2);</span>
<span class="line-modified">!         __ blt(CCR0, l_1);</span>
<span class="line-added">+         __ srdi(tmp1, R5_ARG3, 1);</span>
<span class="line-added">+         __ andi_(R5_ARG3, R5_ARG3, 1);</span>
  
<span class="line-modified">!         __ addi(R3_ARG1, R3_ARG1, -4);</span>
<span class="line-modified">!         __ addi(R4_ARG2, R4_ARG2, -4);</span>
<span class="line-modified">!         __ mtctr(tmp1);</span>
  
<span class="line-modified">!         __ bind(l_3);</span>
<span class="line-modified">!         __ lwzu(tmp2, 4, R3_ARG1);</span>
<span class="line-modified">!         __ stwu(tmp2, 4, R4_ARG2);</span>
<span class="line-modified">!         __ bdnz(l_3);</span>
  
<span class="line-added">+         __ addi(R3_ARG1, R3_ARG1, 4);</span>
<span class="line-added">+         __ addi(R4_ARG2, R4_ARG2, 4);</span>
        }
  
<span class="line-modified">!       // do single element copy</span>
<span class="line-modified">!       __ bind(l_1);</span>
<span class="line-modified">!       __ cmpwi(CCR0, R5_ARG3, 0);</span>
<span class="line-modified">!       __ beq(CCR0, l_4);</span>
  
<span class="line-modified">!       { // FasterArrayCopy</span>
<span class="line-modified">!         __ mtctr(R5_ARG3);</span>
<span class="line-added">+         __ addi(R3_ARG1, R3_ARG1, -2);</span>
<span class="line-added">+         __ addi(R4_ARG2, R4_ARG2, -2);</span>
<span class="line-added">+ </span>
<span class="line-added">+         __ bind(l_5);</span>
<span class="line-added">+         __ lhzu(tmp2, 2, R3_ARG1);</span>
<span class="line-added">+         __ sthu(tmp2, 2, R4_ARG2);</span>
<span class="line-added">+         __ bdnz(l_5);</span>
<span class="line-added">+       }</span>
      }
  
      __ bind(l_4);
      __ li(R3_RET, 0); // return 0
      __ blr();
  
      return start;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1430,19 ***</span>
        STUB_ENTRY(jshort_disjoint_arraycopy);
  
      array_overlap_test(nooverlap_target, 1);
  
      Label l_1, l_2;
<span class="line-modified">!     __ sldi(tmp1, R5_ARG3, 1);</span>
<span class="line-modified">!     __ b(l_2);</span>
<span class="line-modified">!     __ bind(l_1);</span>
<span class="line-modified">!     __ sthx(tmp2, R4_ARG2, tmp1);</span>
<span class="line-modified">!     __ bind(l_2);</span>
<span class="line-modified">!     __ addic_(tmp1, tmp1, -2);</span>
<span class="line-modified">!     __ lhzx(tmp2, R3_ARG1, tmp1);</span>
<span class="line-modified">!     __ bge(CCR0, l_1);</span>
<span class="line-modified">! </span>
      __ li(R3_RET, 0); // return 0
      __ blr();
  
      return start;
    }
<span class="line-new-header">--- 1452,22 ---</span>
        STUB_ENTRY(jshort_disjoint_arraycopy);
  
      array_overlap_test(nooverlap_target, 1);
  
      Label l_1, l_2;
<span class="line-modified">!     {</span>
<span class="line-modified">!       // UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit</span>
<span class="line-modified">!       UnsafeCopyMemoryMark ucmm(this, !aligned, false);</span>
<span class="line-modified">!       __ sldi(tmp1, R5_ARG3, 1);</span>
<span class="line-modified">!       __ b(l_2);</span>
<span class="line-modified">!       __ bind(l_1);</span>
<span class="line-modified">!       __ sthx(tmp2, R4_ARG2, tmp1);</span>
<span class="line-modified">!       __ bind(l_2);</span>
<span class="line-modified">!       __ addic_(tmp1, tmp1, -2);</span>
<span class="line-added">+       __ lhzx(tmp2, R3_ARG1, tmp1);</span>
<span class="line-added">+       __ bge(CCR0, l_1);</span>
<span class="line-added">+     }</span>
      __ li(R3_RET, 0); // return 0
      __ blr();
  
      return start;
    }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1586,11 ***</span>
    //
    address generate_disjoint_int_copy(bool aligned, const char * name) {
      StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
      address start = __ function_entry();
      assert_positive_int(R5_ARG3);
<span class="line-modified">!     generate_disjoint_int_copy_core(aligned);</span>
      __ li(R3_RET, 0); // return 0
      __ blr();
      return start;
    }
  
<span class="line-new-header">--- 1611,15 ---</span>
    //
    address generate_disjoint_int_copy(bool aligned, const char * name) {
      StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
      address start = __ function_entry();
      assert_positive_int(R5_ARG3);
<span class="line-modified">!     {</span>
<span class="line-added">+       // UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit</span>
<span class="line-added">+       UnsafeCopyMemoryMark ucmm(this, !aligned, false);</span>
<span class="line-added">+       generate_disjoint_int_copy_core(aligned);</span>
<span class="line-added">+     }</span>
      __ li(R3_RET, 0); // return 0
      __ blr();
      return start;
    }
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1734,12 ***</span>
      address nooverlap_target = aligned ?
        STUB_ENTRY(arrayof_jint_disjoint_arraycopy) :
        STUB_ENTRY(jint_disjoint_arraycopy);
  
      array_overlap_test(nooverlap_target, 2);
<span class="line-modified">! </span>
<span class="line-modified">!     generate_conjoint_int_copy_core(aligned);</span>
  
      __ li(R3_RET, 0); // return 0
      __ blr();
  
      return start;
<span class="line-new-header">--- 1763,15 ---</span>
      address nooverlap_target = aligned ?
        STUB_ENTRY(arrayof_jint_disjoint_arraycopy) :
        STUB_ENTRY(jint_disjoint_arraycopy);
  
      array_overlap_test(nooverlap_target, 2);
<span class="line-modified">!     {</span>
<span class="line-modified">!       // UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit</span>
<span class="line-added">+       UnsafeCopyMemoryMark ucmm(this, !aligned, false);</span>
<span class="line-added">+       generate_conjoint_int_copy_core(aligned);</span>
<span class="line-added">+     }</span>
  
      __ li(R3_RET, 0); // return 0
      __ blr();
  
      return start;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1857,15 ***</span>
    //
    address generate_disjoint_long_copy(bool aligned, const char * name) {
      StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
      address start = __ function_entry();
      assert_positive_int(R5_ARG3);
<span class="line-modified">!     generate_disjoint_long_copy_core(aligned);</span>
      __ li(R3_RET, 0); // return 0
      __ blr();
  
<span class="line-modified">!     return start;</span>
    }
  
    // Generate core code for conjoint long copy (and oop copy on
    // 64-bit).  If &quot;aligned&quot; is true, the &quot;from&quot; and &quot;to&quot; addresses
    // are assumed to be heapword aligned.
<span class="line-new-header">--- 1889,19 ---</span>
    //
    address generate_disjoint_long_copy(bool aligned, const char * name) {
      StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
      address start = __ function_entry();
      assert_positive_int(R5_ARG3);
<span class="line-modified">!     {</span>
<span class="line-added">+       // UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit</span>
<span class="line-added">+       UnsafeCopyMemoryMark ucmm(this, !aligned, false);</span>
<span class="line-added">+       generate_disjoint_long_copy_core(aligned);</span>
<span class="line-added">+     }</span>
      __ li(R3_RET, 0); // return 0
      __ blr();
  
<span class="line-modified">!   return start;</span>
    }
  
    // Generate core code for conjoint long copy (and oop copy on
    // 64-bit).  If &quot;aligned&quot; is true, the &quot;from&quot; and &quot;to&quot; addresses
    // are assumed to be heapword aligned.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1984,12 ***</span>
      address nooverlap_target = aligned ?
        STUB_ENTRY(arrayof_jlong_disjoint_arraycopy) :
        STUB_ENTRY(jlong_disjoint_arraycopy);
  
      array_overlap_test(nooverlap_target, 3);
<span class="line-modified">!     generate_conjoint_long_copy_core(aligned);</span>
<span class="line-modified">! </span>
      __ li(R3_RET, 0); // return 0
      __ blr();
  
      return start;
    }
<span class="line-new-header">--- 2020,15 ---</span>
      address nooverlap_target = aligned ?
        STUB_ENTRY(arrayof_jlong_disjoint_arraycopy) :
        STUB_ENTRY(jlong_disjoint_arraycopy);
  
      array_overlap_test(nooverlap_target, 3);
<span class="line-modified">!     {</span>
<span class="line-modified">!       // UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit</span>
<span class="line-added">+       UnsafeCopyMemoryMark ucmm(this, !aligned, false);</span>
<span class="line-added">+       generate_conjoint_long_copy_core(aligned);</span>
<span class="line-added">+     }</span>
      __ li(R3_RET, 0); // return 0
      __ blr();
  
      return start;
    }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2986,30 ***</span>
      assert(UseSHA, &quot;need SHA instructions&quot;);
      StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
      address start = __ function_entry();
  
      __ sha256 (multi_block);
<span class="line-removed">- </span>
      __ blr();
      return start;
    }
  
    address generate_sha512_implCompress(bool multi_block, const char *name) {
      assert(UseSHA, &quot;need SHA instructions&quot;);
      StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
      address start = __ function_entry();
  
      __ sha512 (multi_block);
  
      __ blr();
      return start;
    }
  
    void generate_arraycopy_stubs() {
      // Note: the disjoint stubs must be generated first, some of
      // the conjoint stubs use them.
  
      // non-aligned disjoint versions
      StubRoutines::_jbyte_disjoint_arraycopy       = generate_disjoint_byte_copy(false, &quot;jbyte_disjoint_arraycopy&quot;);
      StubRoutines::_jshort_disjoint_arraycopy      = generate_disjoint_short_copy(false, &quot;jshort_disjoint_arraycopy&quot;);
      StubRoutines::_jint_disjoint_arraycopy        = generate_disjoint_int_copy(false, &quot;jint_disjoint_arraycopy&quot;);
      StubRoutines::_jlong_disjoint_arraycopy       = generate_disjoint_long_copy(false, &quot;jlong_disjoint_arraycopy&quot;);
<span class="line-new-header">--- 3025,61 ---</span>
      assert(UseSHA, &quot;need SHA instructions&quot;);
      StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
      address start = __ function_entry();
  
      __ sha256 (multi_block);
      __ blr();
<span class="line-added">+ </span>
      return start;
    }
  
    address generate_sha512_implCompress(bool multi_block, const char *name) {
      assert(UseSHA, &quot;need SHA instructions&quot;);
      StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
      address start = __ function_entry();
  
      __ sha512 (multi_block);
<span class="line-added">+     __ blr();</span>
<span class="line-added">+ </span>
<span class="line-added">+     return start;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   address generate_data_cache_writeback() {</span>
<span class="line-added">+     const Register cacheline = R3_ARG1;</span>
<span class="line-added">+     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_data_cache_writeback&quot;);</span>
<span class="line-added">+     address start = __ pc();</span>
<span class="line-added">+ </span>
<span class="line-added">+     __ cache_wb(Address(cacheline));</span>
<span class="line-added">+     __ blr();</span>
<span class="line-added">+ </span>
<span class="line-added">+     return start;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   address generate_data_cache_writeback_sync() {</span>
<span class="line-added">+     const Register is_presync = R3_ARG1;</span>
<span class="line-added">+     Register temp = R4;</span>
<span class="line-added">+     Label SKIP;</span>
  
<span class="line-added">+     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_data_cache_writeback_sync&quot;);</span>
<span class="line-added">+     address start = __ pc();</span>
<span class="line-added">+ </span>
<span class="line-added">+     __ andi_(temp, is_presync, 1);</span>
<span class="line-added">+     __ bne(CCR0, SKIP);</span>
<span class="line-added">+     __ cache_wbsync(false); // post sync =&gt; emit &#39;sync&#39;</span>
<span class="line-added">+     __ bind(SKIP);          // pre sync =&gt; emit nothing</span>
      __ blr();
<span class="line-added">+ </span>
      return start;
    }
  
    void generate_arraycopy_stubs() {
      // Note: the disjoint stubs must be generated first, some of
      // the conjoint stubs use them.
  
<span class="line-added">+     address ucm_common_error_exit       =  generate_unsafecopy_common_error_exit();</span>
<span class="line-added">+     UnsafeCopyMemory::set_common_exit_stub_pc(ucm_common_error_exit);</span>
<span class="line-added">+ </span>
      // non-aligned disjoint versions
      StubRoutines::_jbyte_disjoint_arraycopy       = generate_disjoint_byte_copy(false, &quot;jbyte_disjoint_arraycopy&quot;);
      StubRoutines::_jshort_disjoint_arraycopy      = generate_disjoint_short_copy(false, &quot;jshort_disjoint_arraycopy&quot;);
      StubRoutines::_jint_disjoint_arraycopy        = generate_disjoint_int_copy(false, &quot;jint_disjoint_arraycopy&quot;);
      StubRoutines::_jlong_disjoint_arraycopy       = generate_disjoint_long_copy(false, &quot;jlong_disjoint_arraycopy&quot;);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 3057,18 ***</span>
<span class="line-new-header">--- 3127,20 ---</span>
                                                               STUB_ENTRY(oop_disjoint_arraycopy),
                                                               STUB_ENTRY(jlong_arraycopy),
                                                               STUB_ENTRY(checkcast_arraycopy));
  
      // fill routines
<span class="line-added">+ #ifdef COMPILER2</span>
      if (OptimizeFill) {
        StubRoutines::_jbyte_fill          = generate_fill(T_BYTE,  false, &quot;jbyte_fill&quot;);
        StubRoutines::_jshort_fill         = generate_fill(T_SHORT, false, &quot;jshort_fill&quot;);
        StubRoutines::_jint_fill           = generate_fill(T_INT,   false, &quot;jint_fill&quot;);
        StubRoutines::_arrayof_jbyte_fill  = generate_fill(T_BYTE,  true, &quot;arrayof_jbyte_fill&quot;);
        StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, &quot;arrayof_jshort_fill&quot;);
        StubRoutines::_arrayof_jint_fill   = generate_fill(T_INT,   true, &quot;arrayof_jint_fill&quot;);
      }
<span class="line-added">+ #endif</span>
    }
  
    // Safefetch stubs.
    void generate_safefetch(const char* name, int size, address* entry, address* fault_pc, address* continuation_pc) {
      // safefetch signatures:
</pre>
<hr />
<pre>
<span class="line-old-header">*** 3533,12 ***</span>
  
  #ifdef COMPILER2
      if (UseMultiplyToLenIntrinsic) {
        StubRoutines::_multiplyToLen = generate_multiplyToLen();
      }
<span class="line-removed">- #endif</span>
<span class="line-removed">- </span>
      if (UseSquareToLenIntrinsic) {
        StubRoutines::_squareToLen = generate_squareToLen();
      }
      if (UseMulAddIntrinsic) {
        StubRoutines::_mulAdd = generate_mulAdd();
<span class="line-new-header">--- 3605,10 ---</span>
</pre>
<hr />
<pre>
<span class="line-old-header">*** 3549,10 ***</span>
<span class="line-new-header">--- 3619,17 ---</span>
      }
      if (UseMontgomerySquareIntrinsic) {
        StubRoutines::_montgomerySquare
          = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_square);
      }
<span class="line-added">+ #endif</span>
<span class="line-added">+ </span>
<span class="line-added">+     // data cache line writeback</span>
<span class="line-added">+     if (VM_Version::supports_data_cache_line_flush()) {</span>
<span class="line-added">+       StubRoutines::_data_cache_writeback = generate_data_cache_writeback();</span>
<span class="line-added">+       StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();</span>
<span class="line-added">+     }</span>
  
      if (UseAESIntrinsics) {
        StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();
        StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();
      }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 3577,8 ***</span>
<span class="line-new-header">--- 3654,12 ---</span>
        generate_initial();
      }
    }
  };
  
<span class="line-added">+ #define UCM_TABLE_MAX_ENTRIES 8</span>
  void StubGenerator_generate(CodeBuffer* code, bool all) {
<span class="line-added">+   if (UnsafeCopyMemory::_table == NULL) {</span>
<span class="line-added">+     UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);</span>
<span class="line-added">+   }</span>
    StubGenerator g(code, all);
  }
</pre>
<center><a href="sharedRuntime_ppc.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="templateInterpreterGenerator_ppc.cpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>