<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/ppc/sharedRuntime_ppc.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="relocInfo_ppc.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stubGenerator_ppc.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/ppc/sharedRuntime_ppc.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.</span>
<span class="line-modified">   3  * Copyright (c) 2012, 2018 SAP SE. All rights reserved.</span>
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;code/debugInfoRec.hpp&quot;
  29 #include &quot;code/icBuffer.hpp&quot;
  30 #include &quot;code/vtableStubs.hpp&quot;
  31 #include &quot;frame_ppc.hpp&quot;
  32 #include &quot;gc/shared/gcLocker.hpp&quot;
  33 #include &quot;interpreter/interpreter.hpp&quot;
  34 #include &quot;interpreter/interp_masm.hpp&quot;
  35 #include &quot;memory/resourceArea.hpp&quot;
  36 #include &quot;oops/compiledICHolder.hpp&quot;

  37 #include &quot;runtime/safepointMechanism.hpp&quot;
  38 #include &quot;runtime/sharedRuntime.hpp&quot;
  39 #include &quot;runtime/vframeArray.hpp&quot;
  40 #include &quot;utilities/align.hpp&quot;
  41 #include &quot;vmreg_ppc.inline.hpp&quot;
  42 #ifdef COMPILER1
  43 #include &quot;c1/c1_Runtime1.hpp&quot;
  44 #endif
  45 #ifdef COMPILER2
  46 #include &quot;opto/ad.hpp&quot;
  47 #include &quot;opto/runtime.hpp&quot;
  48 #endif
  49 
  50 #include &lt;alloca.h&gt;
  51 
  52 #define __ masm-&gt;
  53 
  54 #ifdef PRODUCT
  55 #define BLOCK_COMMENT(str) // nothing
  56 #else
</pre>
<hr />
<pre>
 553 }
 554 
 555 // Is vector&#39;s size (in bytes) bigger than a size saved by default?
 556 bool SharedRuntime::is_wide_vector(int size) {
 557   // Note, MaxVectorSize == 8/16 on PPC64.
 558   assert(size &lt;= (SuperwordUseVSX ? 16 : 8), &quot;%d bytes vectors are not supported&quot;, size);
 559   return size &gt; 8;
 560 }
 561 
 562 size_t SharedRuntime::trampoline_size() {
 563   return Assembler::load_const_size + 8;
 564 }
 565 
 566 void SharedRuntime::generate_trampoline(MacroAssembler *masm, address destination) {
 567   Register Rtemp = R12;
 568   __ load_const(Rtemp, destination);
 569   __ mtctr(Rtemp);
 570   __ bctr();
 571 }
 572 
<span class="line-removed"> 573 #ifdef COMPILER2</span>
 574 static int reg2slot(VMReg r) {
 575   return r-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
 576 }
 577 
 578 static int reg2offset(VMReg r) {
 579   return (r-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;
 580 }
<span class="line-removed"> 581 #endif</span>
 582 
 583 // ---------------------------------------------------------------------------
 584 // Read the array of BasicTypes from a signature, and compute where the
 585 // arguments should go. Values in the VMRegPair regs array refer to 4-byte
 586 // quantities. Values less than VMRegImpl::stack0 are registers, those above
 587 // refer to 4-byte stack slots. All stack slots are based off of the stack pointer
 588 // as framesizes are fixed.
 589 // VMRegImpl::stack0 refers to the first slot 0(sp).
 590 // and VMRegImpl::stack0+1 refers to the memory word 4-bytes higher. Register
 591 // up to RegisterImpl::number_of_registers) are the 64-bit
 592 // integer registers.
 593 
 594 // Note: the INPUTS in sig_bt are in units of Java argument words, which are
 595 // either 32-bit or 64-bit depending on the build. The OUTPUTS are in 32-bit
 596 // units regardless of build. Of course for i486 there is no 64 bit build
 597 
 598 // The Java calling convention is a &quot;shifted&quot; version of the C ABI.
 599 // By skipping the first C ABI register we can call non-static jni methods
 600 // with small numbers of arguments without having to shuffle the arguments
 601 // at all. Since we control the java ABI we ought to at least get some
</pre>
<hr />
<pre>
1125     if (r_1-&gt;is_FloatRegister()) {
1126       if (!r_2-&gt;is_valid()) {
1127         __ lfs(r_1-&gt;as_FloatRegister(), ld_offset, ld_ptr);
1128         ld_offset-=wordSize;
1129       } else {
1130         // Skip the unused interpreter slot.
1131         __ lfd(r_1-&gt;as_FloatRegister(), ld_offset-wordSize, ld_ptr);
1132         ld_offset-=2*wordSize;
1133       }
1134     } else {
1135       Register r;
1136       if (r_1-&gt;is_stack()) {
1137         // Must do a memory to memory move thru &quot;value&quot;.
1138         r = value_regs[value_regs_index];
1139         value_regs_index = (value_regs_index + 1) % num_value_regs;
1140       } else {
1141         r = r_1-&gt;as_Register();
1142       }
1143       if (!r_2-&gt;is_valid()) {
1144         // Not sure we need to do this but it shouldn&#39;t hurt.
<span class="line-modified">1145         if (sig_bt[i] == T_OBJECT || sig_bt[i] == T_ADDRESS || sig_bt[i] == T_ARRAY) {</span>
1146           __ ld(r, ld_offset, ld_ptr);
1147           ld_offset-=wordSize;
1148         } else {
1149           __ lwz(r, ld_offset, ld_ptr);
1150           ld_offset-=wordSize;
1151         }
1152       } else {
1153         // In 64bit, longs are given 2 64-bit slots in the interpreter, but the
1154         // data is passed in only 1 slot.
1155         if (sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {
1156           ld_offset-=wordSize;
1157         }
1158         __ ld(r, ld_offset, ld_ptr);
1159         ld_offset-=wordSize;
1160       }
1161 
1162       if (r_1-&gt;is_stack()) {
1163         // Now store value where the compiler expects it
1164         int st_off = (r_1-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots())*VMRegImpl::stack_slot_size;
1165 
</pre>
<hr />
<pre>
1257                      relocInfo::runtime_call_type);
1258     __ BIND(valid);
1259   }
1260 
1261   // Argument is valid and klass is as expected, continue.
1262 
1263   // Extract method from inline cache, verified entry point needs it.
1264   __ ld(R19_method, CompiledICHolder::holder_metadata_offset(), ic);
1265   assert(R19_method == ic, &quot;the inline cache register is dead here&quot;);
1266 
1267   __ ld(code, method_(code));
1268   __ cmpdi(CCR0, code, 0);
1269   __ ld(ientry, method_(interpreter_entry)); // preloaded
1270   __ beq_predict_taken(CCR0, call_interpreter);
1271 
1272   // Branch to ic_miss_stub.
1273   __ b64_patchable((address)SharedRuntime::get_ic_miss_stub(), relocInfo::runtime_call_type);
1274 
1275   // entry: c2i
1276 
<span class="line-modified">1277   c2i_entry = gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, call_interpreter, ientry);</span>



















1278 
<span class="line-modified">1279   return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry);</span>






1280 }
1281 
<span class="line-removed">1282 #ifdef COMPILER2</span>
1283 // An oop arg. Must pass a handle not the oop itself.
1284 static void object_move(MacroAssembler* masm,
1285                         int frame_size_in_slots,
1286                         OopMap* oop_map, int oop_handle_offset,
1287                         bool is_receiver, int* receiver_offset,
1288                         VMRegPair src, VMRegPair dst,
1289                         Register r_caller_sp, Register r_temp_1, Register r_temp_2) {
1290   assert(!is_receiver || (is_receiver &amp;&amp; (*receiver_offset == -1)),
1291          &quot;receiver has already been moved&quot;);
1292 
1293   // We must pass a handle. First figure out the location we use as a handle.
1294 
1295   if (src.first()-&gt;is_stack()) {
1296     // stack to stack or reg
1297 
1298     const Register r_handle = dst.first()-&gt;is_stack() ? r_temp_1 : dst.first()-&gt;as_Register();
1299     Label skip;
1300     const int oop_slot_in_callers_frame = reg2slot(src.first());
1301 
1302     guarantee(!is_receiver, &quot;expecting receiver in register&quot;);
</pre>
<hr />
<pre>
1697     reg = tmp;
1698   }
1699   __ li(tmp2_reg, 0); // Pass zeros if Array=null.
1700   if (tmp_reg != reg.first()-&gt;as_Register()) __ li(tmp_reg, 0);
1701   __ cmpdi(CCR0, reg.first()-&gt;as_Register(), 0);
1702   __ beq(CCR0, set_out_args);
1703   __ lwa(tmp2_reg, arrayOopDesc::length_offset_in_bytes(), reg.first()-&gt;as_Register());
1704   __ addi(tmp_reg, reg.first()-&gt;as_Register(), arrayOopDesc::base_offset_in_bytes(in_elem_type));
1705   __ bind(set_out_args);
1706   move_ptr(masm, tmp, body_arg, r_caller_sp, /*unused*/ R0);
1707   move_ptr(masm, tmp2, length_arg, r_caller_sp, /*unused*/ R0); // Same as move32_64 on PPC64.
1708 }
1709 
1710 static void verify_oop_args(MacroAssembler* masm,
1711                             const methodHandle&amp; method,
1712                             const BasicType* sig_bt,
1713                             const VMRegPair* regs) {
1714   Register temp_reg = R19_method;  // not part of any compiled calling seq
1715   if (VerifyOops) {
1716     for (int i = 0; i &lt; method-&gt;size_of_parameters(); i++) {
<span class="line-modified">1717       if (sig_bt[i] == T_OBJECT ||</span>
<span class="line-removed">1718           sig_bt[i] == T_ARRAY) {</span>
1719         VMReg r = regs[i].first();
1720         assert(r-&gt;is_valid(), &quot;bad oop arg&quot;);
1721         if (r-&gt;is_stack()) {
1722           __ ld(temp_reg, reg2offset(r), R1_SP);
<span class="line-modified">1723           __ verify_oop(temp_reg);</span>
1724         } else {
<span class="line-modified">1725           __ verify_oop(r-&gt;as_Register());</span>
1726         }
1727       }
1728     }
1729   }
1730 }
1731 
1732 static void gen_special_dispatch(MacroAssembler* masm,
1733                                  const methodHandle&amp; method,
1734                                  const BasicType* sig_bt,
1735                                  const VMRegPair* regs) {
1736   verify_oop_args(masm, method, sig_bt, regs);
1737   vmIntrinsics::ID iid = method-&gt;intrinsic_id();
1738 
1739   // Now write the args into the outgoing interpreter space
1740   bool     has_receiver   = false;
1741   Register receiver_reg   = noreg;
1742   int      member_arg_pos = -1;
1743   Register member_reg     = noreg;
1744   int      ref_kind       = MethodHandles::signature_polymorphic_intrinsic_ref_kind(iid);
1745   if (ref_kind != 0) {
</pre>
<hr />
<pre>
1771     VMReg r = regs[0].first();
1772     assert(r-&gt;is_valid(), &quot;bad receiver arg&quot;);
1773     if (r-&gt;is_stack()) {
1774       // Porting note:  This assumes that compiled calling conventions always
1775       // pass the receiver oop in a register.  If this is not true on some
1776       // platform, pick a temp and load the receiver from stack.
1777       fatal(&quot;receiver always in a register&quot;);
1778       receiver_reg = R11_scratch1;  // TODO (hs24): is R11_scratch1 really free at this point?
1779       __ ld(receiver_reg, reg2offset(r), R1_SP);
1780     } else {
1781       // no data motion is needed
1782       receiver_reg = r-&gt;as_Register();
1783     }
1784   }
1785 
1786   // Figure out which address we are really jumping to:
1787   MethodHandles::generate_method_handle_dispatch(masm, iid,
1788                                                  receiver_reg, member_reg, /*for_compiler_entry:*/ true);
1789 }
1790 
<span class="line-removed">1791 #endif // COMPILER2</span>
<span class="line-removed">1792 </span>
1793 // ---------------------------------------------------------------------------
1794 // Generate a native wrapper for a given method. The method takes arguments
1795 // in the Java compiled code convention, marshals them to the native
1796 // convention (handlizes oops, etc), transitions to native, makes the call,
1797 // returns to java state (possibly blocking), unhandlizes any result and
1798 // returns.
1799 //
1800 // Critical native functions are a shorthand for the use of
1801 // GetPrimtiveArrayCritical and disallow the use of any other JNI
1802 // functions.  The wrapper is expected to unpack the arguments before
1803 // passing them to the callee and perform checks before and after the
1804 // native call to ensure that they GCLocker
1805 // lock_critical/unlock_critical semantics are followed.  Some other
1806 // parts of JNI setup are skipped like the tear down of the JNI handle
1807 // block and the check for pending exceptions it&#39;s impossible for them
1808 // to be thrown.
1809 //
1810 // They are roughly structured like this:
1811 //   if (GCLocker::needs_gc())
1812 //     SharedRuntime::block_for_jni_critical();
1813 //   tranistion to thread_in_native
1814 //   unpack arrray arguments and call native entry point
1815 //   check for safepoint in progress
1816 //   check if any thread suspend flags are set
1817 //     call into JVM and possible unlock the JNI critical
1818 //     if a GC was suppressed while in the critical native.
1819 //   transition back to thread_in_Java
1820 //   return to caller
1821 //
1822 nmethod *SharedRuntime::generate_native_wrapper(MacroAssembler *masm,
1823                                                 const methodHandle&amp; method,
1824                                                 int compile_id,
1825                                                 BasicType *in_sig_bt,
1826                                                 VMRegPair *in_regs,
<span class="line-modified">1827                                                 BasicType ret_type) {</span>
<span class="line-modified">1828 #ifdef COMPILER2</span>
1829   if (method-&gt;is_method_handle_intrinsic()) {
1830     vmIntrinsics::ID iid = method-&gt;intrinsic_id();
1831     intptr_t start = (intptr_t)__ pc();
1832     int vep_offset = ((intptr_t)__ pc()) - start;
1833     gen_special_dispatch(masm,
1834                          method,
1835                          in_sig_bt,
1836                          in_regs);
1837     int frame_complete = ((intptr_t)__ pc()) - start;  // not complete, period
1838     __ flush();
1839     int stack_slots = SharedRuntime::out_preserve_stack_slots();  // no out slots at all, actually
1840     return nmethod::new_native_nmethod(method,
1841                                        compile_id,
1842                                        masm-&gt;code(),
1843                                        vep_offset,
1844                                        frame_complete,
1845                                        stack_slots / VMRegImpl::slots_per_word,
1846                                        in_ByteSize(-1),
1847                                        in_ByteSize(-1),
1848                                        (OopMapSet*)NULL);
1849   }
1850 
1851   bool is_critical_native = true;
<span class="line-modified">1852   address native_func = method-&gt;critical_native_function();</span>
1853   if (native_func == NULL) {
1854     native_func = method-&gt;native_function();
1855     is_critical_native = false;
1856   }
1857   assert(native_func != NULL, &quot;must have function&quot;);
1858 
1859   // First, create signature for outgoing C call
1860   // --------------------------------------------------------------------------
1861 
1862   int total_in_args = method-&gt;size_of_parameters();
1863   // We have received a description of where all the java args are located
1864   // on entry to the wrapper. We need to convert these args to where
1865   // the jni function will expect them. To figure out where they go
1866   // we convert the java signature to a C signature by inserting
1867   // the hidden arguments as arg[0] and possibly arg[1] (static method)
1868 
1869   // Calculate the total number of C arguments and create arrays for the
1870   // signature and the outgoing registers.
1871   // On ppc64, we have two arrays for the outgoing registers, because
1872   // some floating-point arguments must be passed in registers _and_
</pre>
<hr />
<pre>
1891   VMRegPair *out_regs2  = NEW_RESOURCE_ARRAY(VMRegPair, total_c_args);
1892   BasicType* in_elem_bt = NULL;
1893 
1894   // Create the signature for the C call:
1895   //   1) add the JNIEnv*
1896   //   2) add the class if the method is static
1897   //   3) copy the rest of the incoming signature (shifted by the number of
1898   //      hidden arguments).
1899 
1900   int argc = 0;
1901   if (!is_critical_native) {
1902     out_sig_bt[argc++] = T_ADDRESS;
1903     if (method-&gt;is_static()) {
1904       out_sig_bt[argc++] = T_OBJECT;
1905     }
1906 
1907     for (int i = 0; i &lt; total_in_args ; i++ ) {
1908       out_sig_bt[argc++] = in_sig_bt[i];
1909     }
1910   } else {
<span class="line-removed">1911     Thread* THREAD = Thread::current();</span>
1912     in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_c_args);
1913     SignatureStream ss(method-&gt;signature());
1914     int o = 0;
1915     for (int i = 0; i &lt; total_in_args ; i++, o++) {
1916       if (in_sig_bt[i] == T_ARRAY) {
1917         // Arrays are passed as int, elem* pair
<span class="line-modified">1918         Symbol* atype = ss.as_symbol(CHECK_NULL);</span>
<span class="line-modified">1919         const char* at = atype-&gt;as_C_string();</span>
<span class="line-modified">1920         if (strlen(at) == 2) {</span>
<span class="line-removed">1921           assert(at[0] == &#39;[&#39;, &quot;must be&quot;);</span>
<span class="line-removed">1922           switch (at[1]) {</span>
<span class="line-removed">1923             case &#39;B&#39;: in_elem_bt[o] = T_BYTE; break;</span>
<span class="line-removed">1924             case &#39;C&#39;: in_elem_bt[o] = T_CHAR; break;</span>
<span class="line-removed">1925             case &#39;D&#39;: in_elem_bt[o] = T_DOUBLE; break;</span>
<span class="line-removed">1926             case &#39;F&#39;: in_elem_bt[o] = T_FLOAT; break;</span>
<span class="line-removed">1927             case &#39;I&#39;: in_elem_bt[o] = T_INT; break;</span>
<span class="line-removed">1928             case &#39;J&#39;: in_elem_bt[o] = T_LONG; break;</span>
<span class="line-removed">1929             case &#39;S&#39;: in_elem_bt[o] = T_SHORT; break;</span>
<span class="line-removed">1930             case &#39;Z&#39;: in_elem_bt[o] = T_BOOLEAN; break;</span>
<span class="line-removed">1931             default: ShouldNotReachHere();</span>
<span class="line-removed">1932           }</span>
<span class="line-removed">1933         }</span>
1934       } else {
1935         in_elem_bt[o] = T_VOID;
1936       }
1937       if (in_sig_bt[i] != T_VOID) {
<span class="line-modified">1938         assert(in_sig_bt[i] == ss.type(), &quot;must match&quot;);</span>

1939         ss.next();
1940       }
1941     }
1942 
1943     for (int i = 0; i &lt; total_in_args ; i++ ) {
1944       if (in_sig_bt[i] == T_ARRAY) {
1945         // Arrays are passed as int, elem* pair.
1946         out_sig_bt[argc++] = T_INT;
1947         out_sig_bt[argc++] = T_ADDRESS;
1948       } else {
1949         out_sig_bt[argc++] = in_sig_bt[i];
1950       }
1951     }
1952   }
1953 
1954 
1955   // Compute the wrapper&#39;s frame size.
1956   // --------------------------------------------------------------------------
1957 
1958   // Now figure out where the args must be stored and how much stack space
</pre>
<hr />
<pre>
2066   Register r_temp_3     = R24;
2067   Register r_temp_4     = R25;
2068   Register r_temp_5     = R26;
2069   Register r_temp_6     = R27;
2070   Register r_return_pc  = R28;
2071 
2072   Register r_carg1_jnienv        = noreg;
2073   Register r_carg2_classorobject = noreg;
2074   if (!is_critical_native) {
2075     r_carg1_jnienv        = out_regs[0].first()-&gt;as_Register();
2076     r_carg2_classorobject = out_regs[1].first()-&gt;as_Register();
2077   }
2078 
2079 
2080   // Generate the Unverified Entry Point (UEP).
2081   // --------------------------------------------------------------------------
2082   assert(start_pc == (intptr_t)__ pc(), &quot;uep must be at start&quot;);
2083 
2084   // Check ic: object class == cached class?
2085   if (!method_is_static) {
<span class="line-modified">2086   Register ic = as_Register(Matcher::inline_cache_reg_encode());</span>
2087   Register receiver_klass = r_temp_1;
2088 
2089   __ cmpdi(CCR0, R3_ARG1, 0);
2090   __ beq(CCR0, ic_miss);
<span class="line-modified">2091   __ verify_oop(R3_ARG1);</span>
2092   __ load_klass(receiver_klass, R3_ARG1);
2093 
2094   __ cmpd(CCR0, receiver_klass, ic);
2095   __ bne(CCR0, ic_miss);
2096   }
2097 
2098 
2099   // Generate the Verified Entry Point (VEP).
2100   // --------------------------------------------------------------------------
2101   vep_start_pc = (intptr_t)__ pc();
2102 
2103   if (UseRTMLocking) {
2104     // Abort RTM transaction before calling JNI
2105     // because critical section can be large and
2106     // abort anyway. Also nmethod can be deoptimized.
2107     __ tabort_();
2108   }
2109 















2110   __ save_LR_CR(r_temp_1);
2111   __ generate_stack_overflow_check(frame_size_in_bytes); // Check before creating frame.
2112   __ mr(r_callers_sp, R1_SP);                            // Remember frame pointer.
2113   __ push_frame(frame_size_in_bytes, r_temp_1);          // Push the c2n adapter&#39;s frame.
2114   frame_done_pc = (intptr_t)__ pc();
2115 
2116   __ verify_thread();
2117 
2118   // Native nmethod wrappers never take possesion of the oop arguments.
2119   // So the caller will gc the arguments.
2120   // The only thing we need an oopMap for is if the call is static.
2121   //
2122   // An OopMap for lock (and class if static), and one for the VM call itself.
2123   OopMapSet *oop_maps = new OopMapSet();
2124   OopMap    *oop_map  = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
2125 
2126   if (is_critical_native) {
2127     check_needs_gc_for_critical_native(masm, stack_slots, total_in_args, oop_handle_slot_offset,
2128                                        oop_maps, in_regs, in_sig_bt, r_temp_1);
2129   }
</pre>
<hr />
<pre>
2545     // exception in pending_exception and not in a register. Kind of clumsy,
2546     // since all folks who branch to forward_exception must have tested
2547     // pending_exception first and hence have it in a register already.
2548     __ std(r_exception, thread_(pending_exception));
2549 
2550     __ bind(done);
2551   }
2552 
2553 # if 0
2554   // DTrace method exit
2555 # endif
2556 
2557   // Clear &quot;last Java frame&quot; SP and PC.
2558   // --------------------------------------------------------------------------
2559 
2560   __ reset_last_Java_frame();
2561 
2562   // Unbox oop result, e.g. JNIHandles::resolve value.
2563   // --------------------------------------------------------------------------
2564 
<span class="line-modified">2565   if (ret_type == T_OBJECT || ret_type == T_ARRAY) {</span>
2566     __ resolve_jobject(R3_RET, r_temp_1, r_temp_2, /* needs_frame */ false);
2567   }
2568 
2569   if (CheckJNICalls) {
2570     // clear_pending_jni_exception_check
2571     __ load_const_optimized(R0, 0L);
2572     __ st_ptr(R0, JavaThread::pending_jni_exception_check_fn_offset(), R16_thread);
2573   }
2574 
2575   // Reset handle block.
2576   // --------------------------------------------------------------------------
2577   if (!is_critical_native) {
2578   __ ld(r_temp_1, thread_(active_handles));
2579   // TODO: PPC port assert(4 == JNIHandleBlock::top_size_in_bytes(), &quot;unexpected field size&quot;);
2580   __ li(r_temp_2, 0);
2581   __ stw(r_temp_2, JNIHandleBlock::top_offset_in_bytes(), r_temp_1);
2582 
2583 
2584   // Check for pending exceptions.
2585   // --------------------------------------------------------------------------
2586   __ ld(r_temp_2, thread_(pending_exception));
2587   __ cmpdi(CCR0, r_temp_2, 0);
2588   __ bne(CCR0, handle_pending_exception);
2589   }
2590 
2591   // Return
2592   // --------------------------------------------------------------------------
2593 
2594   __ pop_frame();
2595   __ restore_LR_CR(R11);
2596   __ blr();
2597 
2598 
2599   // Handler for pending exceptions (out-of-line).
2600   // --------------------------------------------------------------------------
<span class="line-removed">2601 </span>
2602   // Since this is a native call, we know the proper exception handler
2603   // is the empty function. We just pop this frame and then jump to
2604   // forward_exception_entry.
2605   if (!is_critical_native) {
<span class="line-removed">2606   __ align(InteriorEntryAlignment);</span>
2607   __ bind(handle_pending_exception);
2608 
2609   __ pop_frame();
2610   __ restore_LR_CR(R11);
2611   __ b64_patchable((address)StubRoutines::forward_exception_entry(),
2612                        relocInfo::runtime_call_type);
2613   }
2614 
2615   // Handler for a cache miss (out-of-line).
2616   // --------------------------------------------------------------------------
2617 
2618   if (!method_is_static) {
<span class="line-removed">2619   __ align(InteriorEntryAlignment);</span>
2620   __ bind(ic_miss);
2621 
2622   __ b64_patchable((address)SharedRuntime::get_ic_miss_stub(),
2623                        relocInfo::runtime_call_type);
2624   }
2625 
2626   // Done.
2627   // --------------------------------------------------------------------------
2628 
2629   __ flush();
2630 
2631   nmethod *nm = nmethod::new_native_nmethod(method,
2632                                             compile_id,
2633                                             masm-&gt;code(),
2634                                             vep_start_pc-start_pc,
2635                                             frame_done_pc-start_pc,
2636                                             stack_slots / VMRegImpl::slots_per_word,
2637                                             (method_is_static ? in_ByteSize(klass_offset) : in_ByteSize(receiver_offset)),
2638                                             in_ByteSize(lock_offset),
2639                                             oop_maps);
2640 
2641   if (is_critical_native) {
2642     nm-&gt;set_lazy_critical_native(true);
2643   }
2644 
2645   return nm;
<span class="line-removed">2646 #else</span>
<span class="line-removed">2647   ShouldNotReachHere();</span>
<span class="line-removed">2648   return NULL;</span>
<span class="line-removed">2649 #endif // COMPILER2</span>
2650 }
2651 
2652 // This function returns the adjust size (in number of words) to a c2i adapter
2653 // activation for use during deoptimization.
2654 int Deoptimization::last_frame_adjust(int callee_parameters, int callee_locals) {
2655   return align_up((callee_locals - callee_parameters) * Interpreter::stackElementWords, frame::alignment_in_bytes);
2656 }
2657 
2658 uint SharedRuntime::out_preserve_stack_slots() {
2659 #if defined(COMPILER1) || defined(COMPILER2)
2660   return frame::jit_out_preserve_size / VMRegImpl::stack_slot_size;
2661 #else
2662   return 0;
2663 #endif
2664 }
2665 
2666 #if defined(COMPILER1) || defined(COMPILER2)
2667 // Frame generation for deopt and uncommon trap blobs.
2668 static void push_skeleton_frame(MacroAssembler* masm, bool deopt,
2669                                 /* Read */
2670                                 Register unroll_block_reg,
2671                                 /* Update */
2672                                 Register frame_sizes_reg,
2673                                 Register number_of_frames_reg,
2674                                 Register pcs_reg,
2675                                 /* Invalidate */
2676                                 Register frame_size_reg,
2677                                 Register pc_reg) {
2678 
2679   __ ld(pc_reg, 0, pcs_reg);
2680   __ ld(frame_size_reg, 0, frame_sizes_reg);
2681   __ std(pc_reg, _abi(lr), R1_SP);
2682   __ push_frame(frame_size_reg, R0/*tmp*/);
<span class="line-removed">2683 #ifdef ASSERT</span>
<span class="line-removed">2684   __ load_const_optimized(pc_reg, 0x5afe);</span>
<span class="line-removed">2685   __ std(pc_reg, _ijava_state_neg(ijava_reserved), R1_SP);</span>
<span class="line-removed">2686 #endif</span>
2687   __ std(R1_SP, _ijava_state_neg(sender_sp), R1_SP);
2688   __ addi(number_of_frames_reg, number_of_frames_reg, -1);
2689   __ addi(frame_sizes_reg, frame_sizes_reg, wordSize);
2690   __ addi(pcs_reg, pcs_reg, wordSize);
2691 }
2692 
2693 // Loop through the UnrollBlock info and create new frames.
2694 static void push_skeleton_frames(MacroAssembler* masm, bool deopt,
2695                                  /* read */
2696                                  Register unroll_block_reg,
2697                                  /* invalidate */
2698                                  Register frame_sizes_reg,
2699                                  Register number_of_frames_reg,
2700                                  Register pcs_reg,
2701                                  Register frame_size_reg,
2702                                  Register pc_reg) {
2703   Label loop;
2704 
2705  // _number_of_frames is of type int (deoptimization.hpp)
2706   __ lwa(number_of_frames_reg,
</pre>
<hr />
<pre>
2739              Deoptimization::UnrollBlock::caller_adjustment_offset_in_bytes(),
2740              unroll_block_reg);
2741   __ neg(R11_scratch1, R11_scratch1);
2742 
2743   // R11_scratch1 contains size of locals for frame resizing.
2744   // R12_scratch2 contains top frame&#39;s lr.
2745 
2746   // Resize frame by complete frame size prevents TOC from being
2747   // overwritten by locals. A more stack space saving way would be
2748   // to copy the TOC to its location in the new abi.
2749   __ addi(R11_scratch1, R11_scratch1, - frame::parent_ijava_frame_abi_size);
2750 
2751   // now, resize the frame
2752   __ resize_frame(R11_scratch1, pc_reg/*tmp*/);
2753 
2754   // In the case where we have resized a c2i frame above, the optional
2755   // alignment below the locals has size 32 (why?).
2756   __ std(R12_scratch2, _abi(lr), R1_SP);
2757 
2758   // Initialize initial_caller_sp.
<span class="line-removed">2759 #ifdef ASSERT</span>
<span class="line-removed">2760  __ load_const_optimized(pc_reg, 0x5afe);</span>
<span class="line-removed">2761  __ std(pc_reg, _ijava_state_neg(ijava_reserved), R1_SP);</span>
<span class="line-removed">2762 #endif</span>
2763  __ std(frame_size_reg, _ijava_state_neg(sender_sp), R1_SP);
2764 
2765 #ifdef ASSERT
2766   // Make sure that there is at least one entry in the array.
2767   __ cmpdi(CCR0, number_of_frames_reg, 0);
2768   __ asm_assert_ne(&quot;array_size must be &gt; 0&quot;, 0x205);
2769 #endif
2770 
2771   // Now push the new interpreter frames.
2772   //
2773   __ bind(loop);
2774   // Allocate a new frame, fill in the pc.
2775   push_skeleton_frame(masm, deopt,
2776                       unroll_block_reg,
2777                       frame_sizes_reg,
2778                       number_of_frames_reg,
2779                       pcs_reg,
2780                       frame_size_reg,
2781                       pc_reg);
2782   __ cmpdi(CCR0, number_of_frames_reg, 0);
</pre>
<hr />
<pre>
2814   // --------------------------------------------------------------------------
2815   // Prolog for non exception case!
2816 
2817   // We have been called from the deopt handler of the deoptee.
2818   //
2819   // deoptee:
2820   //                      ...
2821   //                      call X
2822   //                      ...
2823   //  deopt_handler:      call_deopt_stub
2824   //  cur. return pc  --&gt; ...
2825   //
2826   // So currently SR_LR points behind the call in the deopt handler.
2827   // We adjust it such that it points to the start of the deopt handler.
2828   // The return_pc has been stored in the frame of the deoptee and
2829   // will replace the address of the deopt_handler in the call
2830   // to Deoptimization::fetch_unroll_info below.
2831   // We can&#39;t grab a free register here, because all registers may
2832   // contain live values, so let the RegisterSaver do the adjustment
2833   // of the return pc.
<span class="line-modified">2834   const int return_pc_adjustment_no_exception = -HandlerImpl::size_deopt_handler();</span>
2835 
2836   // Push the &quot;unpack frame&quot;
2837   // Save everything in sight.
2838   map = RegisterSaver::push_frame_reg_args_and_save_live_registers(masm,
2839                                                                    &amp;first_frame_size_in_bytes,
2840                                                                    /*generate_oop_map=*/ true,
2841                                                                    return_pc_adjustment_no_exception,
2842                                                                    RegisterSaver::return_pc_is_lr);
2843   assert(map != NULL, &quot;OopMap must have been created&quot;);
2844 
2845   __ li(exec_mode_reg, Deoptimization::Unpack_deopt);
2846   // Save exec mode for unpack_frames.
2847   __ b(exec_mode_initialized);
2848 
2849   // --------------------------------------------------------------------------
2850   // Prolog for exception case
2851 
2852   // An exception is pending.
2853   // We have been called with a return (interpreter) or a jump (exception blob).
2854   //
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.</span>
<span class="line-modified">   3  * Copyright (c) 2012, 2019 SAP SE. All rights reserved.</span>
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;code/debugInfoRec.hpp&quot;
  29 #include &quot;code/icBuffer.hpp&quot;
  30 #include &quot;code/vtableStubs.hpp&quot;
  31 #include &quot;frame_ppc.hpp&quot;
  32 #include &quot;gc/shared/gcLocker.hpp&quot;
  33 #include &quot;interpreter/interpreter.hpp&quot;
  34 #include &quot;interpreter/interp_masm.hpp&quot;
  35 #include &quot;memory/resourceArea.hpp&quot;
  36 #include &quot;oops/compiledICHolder.hpp&quot;
<span class="line-added">  37 #include &quot;oops/klass.inline.hpp&quot;</span>
  38 #include &quot;runtime/safepointMechanism.hpp&quot;
  39 #include &quot;runtime/sharedRuntime.hpp&quot;
  40 #include &quot;runtime/vframeArray.hpp&quot;
  41 #include &quot;utilities/align.hpp&quot;
  42 #include &quot;vmreg_ppc.inline.hpp&quot;
  43 #ifdef COMPILER1
  44 #include &quot;c1/c1_Runtime1.hpp&quot;
  45 #endif
  46 #ifdef COMPILER2
  47 #include &quot;opto/ad.hpp&quot;
  48 #include &quot;opto/runtime.hpp&quot;
  49 #endif
  50 
  51 #include &lt;alloca.h&gt;
  52 
  53 #define __ masm-&gt;
  54 
  55 #ifdef PRODUCT
  56 #define BLOCK_COMMENT(str) // nothing
  57 #else
</pre>
<hr />
<pre>
 554 }
 555 
 556 // Is vector&#39;s size (in bytes) bigger than a size saved by default?
 557 bool SharedRuntime::is_wide_vector(int size) {
 558   // Note, MaxVectorSize == 8/16 on PPC64.
 559   assert(size &lt;= (SuperwordUseVSX ? 16 : 8), &quot;%d bytes vectors are not supported&quot;, size);
 560   return size &gt; 8;
 561 }
 562 
 563 size_t SharedRuntime::trampoline_size() {
 564   return Assembler::load_const_size + 8;
 565 }
 566 
 567 void SharedRuntime::generate_trampoline(MacroAssembler *masm, address destination) {
 568   Register Rtemp = R12;
 569   __ load_const(Rtemp, destination);
 570   __ mtctr(Rtemp);
 571   __ bctr();
 572 }
 573 

 574 static int reg2slot(VMReg r) {
 575   return r-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
 576 }
 577 
 578 static int reg2offset(VMReg r) {
 579   return (r-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;
 580 }

 581 
 582 // ---------------------------------------------------------------------------
 583 // Read the array of BasicTypes from a signature, and compute where the
 584 // arguments should go. Values in the VMRegPair regs array refer to 4-byte
 585 // quantities. Values less than VMRegImpl::stack0 are registers, those above
 586 // refer to 4-byte stack slots. All stack slots are based off of the stack pointer
 587 // as framesizes are fixed.
 588 // VMRegImpl::stack0 refers to the first slot 0(sp).
 589 // and VMRegImpl::stack0+1 refers to the memory word 4-bytes higher. Register
 590 // up to RegisterImpl::number_of_registers) are the 64-bit
 591 // integer registers.
 592 
 593 // Note: the INPUTS in sig_bt are in units of Java argument words, which are
 594 // either 32-bit or 64-bit depending on the build. The OUTPUTS are in 32-bit
 595 // units regardless of build. Of course for i486 there is no 64 bit build
 596 
 597 // The Java calling convention is a &quot;shifted&quot; version of the C ABI.
 598 // By skipping the first C ABI register we can call non-static jni methods
 599 // with small numbers of arguments without having to shuffle the arguments
 600 // at all. Since we control the java ABI we ought to at least get some
</pre>
<hr />
<pre>
1124     if (r_1-&gt;is_FloatRegister()) {
1125       if (!r_2-&gt;is_valid()) {
1126         __ lfs(r_1-&gt;as_FloatRegister(), ld_offset, ld_ptr);
1127         ld_offset-=wordSize;
1128       } else {
1129         // Skip the unused interpreter slot.
1130         __ lfd(r_1-&gt;as_FloatRegister(), ld_offset-wordSize, ld_ptr);
1131         ld_offset-=2*wordSize;
1132       }
1133     } else {
1134       Register r;
1135       if (r_1-&gt;is_stack()) {
1136         // Must do a memory to memory move thru &quot;value&quot;.
1137         r = value_regs[value_regs_index];
1138         value_regs_index = (value_regs_index + 1) % num_value_regs;
1139       } else {
1140         r = r_1-&gt;as_Register();
1141       }
1142       if (!r_2-&gt;is_valid()) {
1143         // Not sure we need to do this but it shouldn&#39;t hurt.
<span class="line-modified">1144         if (is_reference_type(sig_bt[i]) || sig_bt[i] == T_ADDRESS) {</span>
1145           __ ld(r, ld_offset, ld_ptr);
1146           ld_offset-=wordSize;
1147         } else {
1148           __ lwz(r, ld_offset, ld_ptr);
1149           ld_offset-=wordSize;
1150         }
1151       } else {
1152         // In 64bit, longs are given 2 64-bit slots in the interpreter, but the
1153         // data is passed in only 1 slot.
1154         if (sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {
1155           ld_offset-=wordSize;
1156         }
1157         __ ld(r, ld_offset, ld_ptr);
1158         ld_offset-=wordSize;
1159       }
1160 
1161       if (r_1-&gt;is_stack()) {
1162         // Now store value where the compiler expects it
1163         int st_off = (r_1-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots())*VMRegImpl::stack_slot_size;
1164 
</pre>
<hr />
<pre>
1256                      relocInfo::runtime_call_type);
1257     __ BIND(valid);
1258   }
1259 
1260   // Argument is valid and klass is as expected, continue.
1261 
1262   // Extract method from inline cache, verified entry point needs it.
1263   __ ld(R19_method, CompiledICHolder::holder_metadata_offset(), ic);
1264   assert(R19_method == ic, &quot;the inline cache register is dead here&quot;);
1265 
1266   __ ld(code, method_(code));
1267   __ cmpdi(CCR0, code, 0);
1268   __ ld(ientry, method_(interpreter_entry)); // preloaded
1269   __ beq_predict_taken(CCR0, call_interpreter);
1270 
1271   // Branch to ic_miss_stub.
1272   __ b64_patchable((address)SharedRuntime::get_ic_miss_stub(), relocInfo::runtime_call_type);
1273 
1274   // entry: c2i
1275 
<span class="line-modified">1276   c2i_entry = __ pc();</span>
<span class="line-added">1277 </span>
<span class="line-added">1278   // Class initialization barrier for static methods</span>
<span class="line-added">1279   address c2i_no_clinit_check_entry = NULL;</span>
<span class="line-added">1280   if (VM_Version::supports_fast_class_init_checks()) {</span>
<span class="line-added">1281     Label L_skip_barrier;</span>
<span class="line-added">1282 </span>
<span class="line-added">1283     { // Bypass the barrier for non-static methods</span>
<span class="line-added">1284       __ lwz(R0, in_bytes(Method::access_flags_offset()), R19_method);</span>
<span class="line-added">1285       __ andi_(R0, R0, JVM_ACC_STATIC);</span>
<span class="line-added">1286       __ beq(CCR0, L_skip_barrier); // non-static</span>
<span class="line-added">1287     }</span>
<span class="line-added">1288 </span>
<span class="line-added">1289     Register klass = R11_scratch1;</span>
<span class="line-added">1290     __ load_method_holder(klass, R19_method);</span>
<span class="line-added">1291     __ clinit_barrier(klass, R16_thread, &amp;L_skip_barrier /*L_fast_path*/);</span>
<span class="line-added">1292 </span>
<span class="line-added">1293     __ load_const_optimized(klass, SharedRuntime::get_handle_wrong_method_stub(), R0);</span>
<span class="line-added">1294     __ mtctr(klass);</span>
<span class="line-added">1295     __ bctr();</span>
1296 
<span class="line-modified">1297     __ bind(L_skip_barrier);</span>
<span class="line-added">1298     c2i_no_clinit_check_entry = __ pc();</span>
<span class="line-added">1299   }</span>
<span class="line-added">1300 </span>
<span class="line-added">1301   gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, call_interpreter, ientry);</span>
<span class="line-added">1302 </span>
<span class="line-added">1303   return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);</span>
1304 }
1305 

1306 // An oop arg. Must pass a handle not the oop itself.
1307 static void object_move(MacroAssembler* masm,
1308                         int frame_size_in_slots,
1309                         OopMap* oop_map, int oop_handle_offset,
1310                         bool is_receiver, int* receiver_offset,
1311                         VMRegPair src, VMRegPair dst,
1312                         Register r_caller_sp, Register r_temp_1, Register r_temp_2) {
1313   assert(!is_receiver || (is_receiver &amp;&amp; (*receiver_offset == -1)),
1314          &quot;receiver has already been moved&quot;);
1315 
1316   // We must pass a handle. First figure out the location we use as a handle.
1317 
1318   if (src.first()-&gt;is_stack()) {
1319     // stack to stack or reg
1320 
1321     const Register r_handle = dst.first()-&gt;is_stack() ? r_temp_1 : dst.first()-&gt;as_Register();
1322     Label skip;
1323     const int oop_slot_in_callers_frame = reg2slot(src.first());
1324 
1325     guarantee(!is_receiver, &quot;expecting receiver in register&quot;);
</pre>
<hr />
<pre>
1720     reg = tmp;
1721   }
1722   __ li(tmp2_reg, 0); // Pass zeros if Array=null.
1723   if (tmp_reg != reg.first()-&gt;as_Register()) __ li(tmp_reg, 0);
1724   __ cmpdi(CCR0, reg.first()-&gt;as_Register(), 0);
1725   __ beq(CCR0, set_out_args);
1726   __ lwa(tmp2_reg, arrayOopDesc::length_offset_in_bytes(), reg.first()-&gt;as_Register());
1727   __ addi(tmp_reg, reg.first()-&gt;as_Register(), arrayOopDesc::base_offset_in_bytes(in_elem_type));
1728   __ bind(set_out_args);
1729   move_ptr(masm, tmp, body_arg, r_caller_sp, /*unused*/ R0);
1730   move_ptr(masm, tmp2, length_arg, r_caller_sp, /*unused*/ R0); // Same as move32_64 on PPC64.
1731 }
1732 
1733 static void verify_oop_args(MacroAssembler* masm,
1734                             const methodHandle&amp; method,
1735                             const BasicType* sig_bt,
1736                             const VMRegPair* regs) {
1737   Register temp_reg = R19_method;  // not part of any compiled calling seq
1738   if (VerifyOops) {
1739     for (int i = 0; i &lt; method-&gt;size_of_parameters(); i++) {
<span class="line-modified">1740       if (is_reference_type(sig_bt[i])) {</span>

1741         VMReg r = regs[i].first();
1742         assert(r-&gt;is_valid(), &quot;bad oop arg&quot;);
1743         if (r-&gt;is_stack()) {
1744           __ ld(temp_reg, reg2offset(r), R1_SP);
<span class="line-modified">1745           __ verify_oop(temp_reg, FILE_AND_LINE);</span>
1746         } else {
<span class="line-modified">1747           __ verify_oop(r-&gt;as_Register(), FILE_AND_LINE);</span>
1748         }
1749       }
1750     }
1751   }
1752 }
1753 
1754 static void gen_special_dispatch(MacroAssembler* masm,
1755                                  const methodHandle&amp; method,
1756                                  const BasicType* sig_bt,
1757                                  const VMRegPair* regs) {
1758   verify_oop_args(masm, method, sig_bt, regs);
1759   vmIntrinsics::ID iid = method-&gt;intrinsic_id();
1760 
1761   // Now write the args into the outgoing interpreter space
1762   bool     has_receiver   = false;
1763   Register receiver_reg   = noreg;
1764   int      member_arg_pos = -1;
1765   Register member_reg     = noreg;
1766   int      ref_kind       = MethodHandles::signature_polymorphic_intrinsic_ref_kind(iid);
1767   if (ref_kind != 0) {
</pre>
<hr />
<pre>
1793     VMReg r = regs[0].first();
1794     assert(r-&gt;is_valid(), &quot;bad receiver arg&quot;);
1795     if (r-&gt;is_stack()) {
1796       // Porting note:  This assumes that compiled calling conventions always
1797       // pass the receiver oop in a register.  If this is not true on some
1798       // platform, pick a temp and load the receiver from stack.
1799       fatal(&quot;receiver always in a register&quot;);
1800       receiver_reg = R11_scratch1;  // TODO (hs24): is R11_scratch1 really free at this point?
1801       __ ld(receiver_reg, reg2offset(r), R1_SP);
1802     } else {
1803       // no data motion is needed
1804       receiver_reg = r-&gt;as_Register();
1805     }
1806   }
1807 
1808   // Figure out which address we are really jumping to:
1809   MethodHandles::generate_method_handle_dispatch(masm, iid,
1810                                                  receiver_reg, member_reg, /*for_compiler_entry:*/ true);
1811 }
1812 


1813 // ---------------------------------------------------------------------------
1814 // Generate a native wrapper for a given method. The method takes arguments
1815 // in the Java compiled code convention, marshals them to the native
1816 // convention (handlizes oops, etc), transitions to native, makes the call,
1817 // returns to java state (possibly blocking), unhandlizes any result and
1818 // returns.
1819 //
1820 // Critical native functions are a shorthand for the use of
1821 // GetPrimtiveArrayCritical and disallow the use of any other JNI
1822 // functions.  The wrapper is expected to unpack the arguments before
1823 // passing them to the callee and perform checks before and after the
1824 // native call to ensure that they GCLocker
1825 // lock_critical/unlock_critical semantics are followed.  Some other
1826 // parts of JNI setup are skipped like the tear down of the JNI handle
1827 // block and the check for pending exceptions it&#39;s impossible for them
1828 // to be thrown.
1829 //
1830 // They are roughly structured like this:
1831 //   if (GCLocker::needs_gc())
1832 //     SharedRuntime::block_for_jni_critical();
1833 //   tranistion to thread_in_native
1834 //   unpack arrray arguments and call native entry point
1835 //   check for safepoint in progress
1836 //   check if any thread suspend flags are set
1837 //     call into JVM and possible unlock the JNI critical
1838 //     if a GC was suppressed while in the critical native.
1839 //   transition back to thread_in_Java
1840 //   return to caller
1841 //
1842 nmethod *SharedRuntime::generate_native_wrapper(MacroAssembler *masm,
1843                                                 const methodHandle&amp; method,
1844                                                 int compile_id,
1845                                                 BasicType *in_sig_bt,
1846                                                 VMRegPair *in_regs,
<span class="line-modified">1847                                                 BasicType ret_type,</span>
<span class="line-modified">1848                                                 address critical_entry) {</span>
1849   if (method-&gt;is_method_handle_intrinsic()) {
1850     vmIntrinsics::ID iid = method-&gt;intrinsic_id();
1851     intptr_t start = (intptr_t)__ pc();
1852     int vep_offset = ((intptr_t)__ pc()) - start;
1853     gen_special_dispatch(masm,
1854                          method,
1855                          in_sig_bt,
1856                          in_regs);
1857     int frame_complete = ((intptr_t)__ pc()) - start;  // not complete, period
1858     __ flush();
1859     int stack_slots = SharedRuntime::out_preserve_stack_slots();  // no out slots at all, actually
1860     return nmethod::new_native_nmethod(method,
1861                                        compile_id,
1862                                        masm-&gt;code(),
1863                                        vep_offset,
1864                                        frame_complete,
1865                                        stack_slots / VMRegImpl::slots_per_word,
1866                                        in_ByteSize(-1),
1867                                        in_ByteSize(-1),
1868                                        (OopMapSet*)NULL);
1869   }
1870 
1871   bool is_critical_native = true;
<span class="line-modified">1872   address native_func = critical_entry;</span>
1873   if (native_func == NULL) {
1874     native_func = method-&gt;native_function();
1875     is_critical_native = false;
1876   }
1877   assert(native_func != NULL, &quot;must have function&quot;);
1878 
1879   // First, create signature for outgoing C call
1880   // --------------------------------------------------------------------------
1881 
1882   int total_in_args = method-&gt;size_of_parameters();
1883   // We have received a description of where all the java args are located
1884   // on entry to the wrapper. We need to convert these args to where
1885   // the jni function will expect them. To figure out where they go
1886   // we convert the java signature to a C signature by inserting
1887   // the hidden arguments as arg[0] and possibly arg[1] (static method)
1888 
1889   // Calculate the total number of C arguments and create arrays for the
1890   // signature and the outgoing registers.
1891   // On ppc64, we have two arrays for the outgoing registers, because
1892   // some floating-point arguments must be passed in registers _and_
</pre>
<hr />
<pre>
1911   VMRegPair *out_regs2  = NEW_RESOURCE_ARRAY(VMRegPair, total_c_args);
1912   BasicType* in_elem_bt = NULL;
1913 
1914   // Create the signature for the C call:
1915   //   1) add the JNIEnv*
1916   //   2) add the class if the method is static
1917   //   3) copy the rest of the incoming signature (shifted by the number of
1918   //      hidden arguments).
1919 
1920   int argc = 0;
1921   if (!is_critical_native) {
1922     out_sig_bt[argc++] = T_ADDRESS;
1923     if (method-&gt;is_static()) {
1924       out_sig_bt[argc++] = T_OBJECT;
1925     }
1926 
1927     for (int i = 0; i &lt; total_in_args ; i++ ) {
1928       out_sig_bt[argc++] = in_sig_bt[i];
1929     }
1930   } else {

1931     in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_c_args);
1932     SignatureStream ss(method-&gt;signature());
1933     int o = 0;
1934     for (int i = 0; i &lt; total_in_args ; i++, o++) {
1935       if (in_sig_bt[i] == T_ARRAY) {
1936         // Arrays are passed as int, elem* pair
<span class="line-modified">1937         ss.skip_array_prefix(1);  // skip one &#39;[&#39;</span>
<span class="line-modified">1938         assert(ss.is_primitive(), &quot;primitive type expected&quot;);</span>
<span class="line-modified">1939         in_elem_bt[o] = ss.type();</span>













1940       } else {
1941         in_elem_bt[o] = T_VOID;
1942       }
1943       if (in_sig_bt[i] != T_VOID) {
<span class="line-modified">1944         assert(in_sig_bt[i] == ss.type() ||</span>
<span class="line-added">1945                in_sig_bt[i] == T_ARRAY, &quot;must match&quot;);</span>
1946         ss.next();
1947       }
1948     }
1949 
1950     for (int i = 0; i &lt; total_in_args ; i++ ) {
1951       if (in_sig_bt[i] == T_ARRAY) {
1952         // Arrays are passed as int, elem* pair.
1953         out_sig_bt[argc++] = T_INT;
1954         out_sig_bt[argc++] = T_ADDRESS;
1955       } else {
1956         out_sig_bt[argc++] = in_sig_bt[i];
1957       }
1958     }
1959   }
1960 
1961 
1962   // Compute the wrapper&#39;s frame size.
1963   // --------------------------------------------------------------------------
1964 
1965   // Now figure out where the args must be stored and how much stack space
</pre>
<hr />
<pre>
2073   Register r_temp_3     = R24;
2074   Register r_temp_4     = R25;
2075   Register r_temp_5     = R26;
2076   Register r_temp_6     = R27;
2077   Register r_return_pc  = R28;
2078 
2079   Register r_carg1_jnienv        = noreg;
2080   Register r_carg2_classorobject = noreg;
2081   if (!is_critical_native) {
2082     r_carg1_jnienv        = out_regs[0].first()-&gt;as_Register();
2083     r_carg2_classorobject = out_regs[1].first()-&gt;as_Register();
2084   }
2085 
2086 
2087   // Generate the Unverified Entry Point (UEP).
2088   // --------------------------------------------------------------------------
2089   assert(start_pc == (intptr_t)__ pc(), &quot;uep must be at start&quot;);
2090 
2091   // Check ic: object class == cached class?
2092   if (!method_is_static) {
<span class="line-modified">2093   Register ic = R19_inline_cache_reg;</span>
2094   Register receiver_klass = r_temp_1;
2095 
2096   __ cmpdi(CCR0, R3_ARG1, 0);
2097   __ beq(CCR0, ic_miss);
<span class="line-modified">2098   __ verify_oop(R3_ARG1, FILE_AND_LINE);</span>
2099   __ load_klass(receiver_klass, R3_ARG1);
2100 
2101   __ cmpd(CCR0, receiver_klass, ic);
2102   __ bne(CCR0, ic_miss);
2103   }
2104 
2105 
2106   // Generate the Verified Entry Point (VEP).
2107   // --------------------------------------------------------------------------
2108   vep_start_pc = (intptr_t)__ pc();
2109 
2110   if (UseRTMLocking) {
2111     // Abort RTM transaction before calling JNI
2112     // because critical section can be large and
2113     // abort anyway. Also nmethod can be deoptimized.
2114     __ tabort_();
2115   }
2116 
<span class="line-added">2117   if (VM_Version::supports_fast_class_init_checks() &amp;&amp; method-&gt;needs_clinit_barrier()) {</span>
<span class="line-added">2118     Label L_skip_barrier;</span>
<span class="line-added">2119     Register klass = r_temp_1;</span>
<span class="line-added">2120     // Notify OOP recorder (don&#39;t need the relocation)</span>
<span class="line-added">2121     AddressLiteral md = __ constant_metadata_address(method-&gt;method_holder());</span>
<span class="line-added">2122     __ load_const_optimized(klass, md.value(), R0);</span>
<span class="line-added">2123     __ clinit_barrier(klass, R16_thread, &amp;L_skip_barrier /*L_fast_path*/);</span>
<span class="line-added">2124 </span>
<span class="line-added">2125     __ load_const_optimized(klass, SharedRuntime::get_handle_wrong_method_stub(), R0);</span>
<span class="line-added">2126     __ mtctr(klass);</span>
<span class="line-added">2127     __ bctr();</span>
<span class="line-added">2128 </span>
<span class="line-added">2129     __ bind(L_skip_barrier);</span>
<span class="line-added">2130   }</span>
<span class="line-added">2131 </span>
2132   __ save_LR_CR(r_temp_1);
2133   __ generate_stack_overflow_check(frame_size_in_bytes); // Check before creating frame.
2134   __ mr(r_callers_sp, R1_SP);                            // Remember frame pointer.
2135   __ push_frame(frame_size_in_bytes, r_temp_1);          // Push the c2n adapter&#39;s frame.
2136   frame_done_pc = (intptr_t)__ pc();
2137 
2138   __ verify_thread();
2139 
2140   // Native nmethod wrappers never take possesion of the oop arguments.
2141   // So the caller will gc the arguments.
2142   // The only thing we need an oopMap for is if the call is static.
2143   //
2144   // An OopMap for lock (and class if static), and one for the VM call itself.
2145   OopMapSet *oop_maps = new OopMapSet();
2146   OopMap    *oop_map  = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
2147 
2148   if (is_critical_native) {
2149     check_needs_gc_for_critical_native(masm, stack_slots, total_in_args, oop_handle_slot_offset,
2150                                        oop_maps, in_regs, in_sig_bt, r_temp_1);
2151   }
</pre>
<hr />
<pre>
2567     // exception in pending_exception and not in a register. Kind of clumsy,
2568     // since all folks who branch to forward_exception must have tested
2569     // pending_exception first and hence have it in a register already.
2570     __ std(r_exception, thread_(pending_exception));
2571 
2572     __ bind(done);
2573   }
2574 
2575 # if 0
2576   // DTrace method exit
2577 # endif
2578 
2579   // Clear &quot;last Java frame&quot; SP and PC.
2580   // --------------------------------------------------------------------------
2581 
2582   __ reset_last_Java_frame();
2583 
2584   // Unbox oop result, e.g. JNIHandles::resolve value.
2585   // --------------------------------------------------------------------------
2586 
<span class="line-modified">2587   if (is_reference_type(ret_type)) {</span>
2588     __ resolve_jobject(R3_RET, r_temp_1, r_temp_2, /* needs_frame */ false);
2589   }
2590 
2591   if (CheckJNICalls) {
2592     // clear_pending_jni_exception_check
2593     __ load_const_optimized(R0, 0L);
2594     __ st_ptr(R0, JavaThread::pending_jni_exception_check_fn_offset(), R16_thread);
2595   }
2596 
2597   // Reset handle block.
2598   // --------------------------------------------------------------------------
2599   if (!is_critical_native) {
2600   __ ld(r_temp_1, thread_(active_handles));
2601   // TODO: PPC port assert(4 == JNIHandleBlock::top_size_in_bytes(), &quot;unexpected field size&quot;);
2602   __ li(r_temp_2, 0);
2603   __ stw(r_temp_2, JNIHandleBlock::top_offset_in_bytes(), r_temp_1);
2604 
2605 
2606   // Check for pending exceptions.
2607   // --------------------------------------------------------------------------
2608   __ ld(r_temp_2, thread_(pending_exception));
2609   __ cmpdi(CCR0, r_temp_2, 0);
2610   __ bne(CCR0, handle_pending_exception);
2611   }
2612 
2613   // Return
2614   // --------------------------------------------------------------------------
2615 
2616   __ pop_frame();
2617   __ restore_LR_CR(R11);
2618   __ blr();
2619 
2620 
2621   // Handler for pending exceptions (out-of-line).
2622   // --------------------------------------------------------------------------

2623   // Since this is a native call, we know the proper exception handler
2624   // is the empty function. We just pop this frame and then jump to
2625   // forward_exception_entry.
2626   if (!is_critical_native) {

2627   __ bind(handle_pending_exception);
2628 
2629   __ pop_frame();
2630   __ restore_LR_CR(R11);
2631   __ b64_patchable((address)StubRoutines::forward_exception_entry(),
2632                        relocInfo::runtime_call_type);
2633   }
2634 
2635   // Handler for a cache miss (out-of-line).
2636   // --------------------------------------------------------------------------
2637 
2638   if (!method_is_static) {

2639   __ bind(ic_miss);
2640 
2641   __ b64_patchable((address)SharedRuntime::get_ic_miss_stub(),
2642                        relocInfo::runtime_call_type);
2643   }
2644 
2645   // Done.
2646   // --------------------------------------------------------------------------
2647 
2648   __ flush();
2649 
2650   nmethod *nm = nmethod::new_native_nmethod(method,
2651                                             compile_id,
2652                                             masm-&gt;code(),
2653                                             vep_start_pc-start_pc,
2654                                             frame_done_pc-start_pc,
2655                                             stack_slots / VMRegImpl::slots_per_word,
2656                                             (method_is_static ? in_ByteSize(klass_offset) : in_ByteSize(receiver_offset)),
2657                                             in_ByteSize(lock_offset),
2658                                             oop_maps);
2659 
2660   if (is_critical_native) {
2661     nm-&gt;set_lazy_critical_native(true);
2662   }
2663 
2664   return nm;




2665 }
2666 
2667 // This function returns the adjust size (in number of words) to a c2i adapter
2668 // activation for use during deoptimization.
2669 int Deoptimization::last_frame_adjust(int callee_parameters, int callee_locals) {
2670   return align_up((callee_locals - callee_parameters) * Interpreter::stackElementWords, frame::alignment_in_bytes);
2671 }
2672 
2673 uint SharedRuntime::out_preserve_stack_slots() {
2674 #if defined(COMPILER1) || defined(COMPILER2)
2675   return frame::jit_out_preserve_size / VMRegImpl::stack_slot_size;
2676 #else
2677   return 0;
2678 #endif
2679 }
2680 
2681 #if defined(COMPILER1) || defined(COMPILER2)
2682 // Frame generation for deopt and uncommon trap blobs.
2683 static void push_skeleton_frame(MacroAssembler* masm, bool deopt,
2684                                 /* Read */
2685                                 Register unroll_block_reg,
2686                                 /* Update */
2687                                 Register frame_sizes_reg,
2688                                 Register number_of_frames_reg,
2689                                 Register pcs_reg,
2690                                 /* Invalidate */
2691                                 Register frame_size_reg,
2692                                 Register pc_reg) {
2693 
2694   __ ld(pc_reg, 0, pcs_reg);
2695   __ ld(frame_size_reg, 0, frame_sizes_reg);
2696   __ std(pc_reg, _abi(lr), R1_SP);
2697   __ push_frame(frame_size_reg, R0/*tmp*/);




2698   __ std(R1_SP, _ijava_state_neg(sender_sp), R1_SP);
2699   __ addi(number_of_frames_reg, number_of_frames_reg, -1);
2700   __ addi(frame_sizes_reg, frame_sizes_reg, wordSize);
2701   __ addi(pcs_reg, pcs_reg, wordSize);
2702 }
2703 
2704 // Loop through the UnrollBlock info and create new frames.
2705 static void push_skeleton_frames(MacroAssembler* masm, bool deopt,
2706                                  /* read */
2707                                  Register unroll_block_reg,
2708                                  /* invalidate */
2709                                  Register frame_sizes_reg,
2710                                  Register number_of_frames_reg,
2711                                  Register pcs_reg,
2712                                  Register frame_size_reg,
2713                                  Register pc_reg) {
2714   Label loop;
2715 
2716  // _number_of_frames is of type int (deoptimization.hpp)
2717   __ lwa(number_of_frames_reg,
</pre>
<hr />
<pre>
2750              Deoptimization::UnrollBlock::caller_adjustment_offset_in_bytes(),
2751              unroll_block_reg);
2752   __ neg(R11_scratch1, R11_scratch1);
2753 
2754   // R11_scratch1 contains size of locals for frame resizing.
2755   // R12_scratch2 contains top frame&#39;s lr.
2756 
2757   // Resize frame by complete frame size prevents TOC from being
2758   // overwritten by locals. A more stack space saving way would be
2759   // to copy the TOC to its location in the new abi.
2760   __ addi(R11_scratch1, R11_scratch1, - frame::parent_ijava_frame_abi_size);
2761 
2762   // now, resize the frame
2763   __ resize_frame(R11_scratch1, pc_reg/*tmp*/);
2764 
2765   // In the case where we have resized a c2i frame above, the optional
2766   // alignment below the locals has size 32 (why?).
2767   __ std(R12_scratch2, _abi(lr), R1_SP);
2768 
2769   // Initialize initial_caller_sp.




2770  __ std(frame_size_reg, _ijava_state_neg(sender_sp), R1_SP);
2771 
2772 #ifdef ASSERT
2773   // Make sure that there is at least one entry in the array.
2774   __ cmpdi(CCR0, number_of_frames_reg, 0);
2775   __ asm_assert_ne(&quot;array_size must be &gt; 0&quot;, 0x205);
2776 #endif
2777 
2778   // Now push the new interpreter frames.
2779   //
2780   __ bind(loop);
2781   // Allocate a new frame, fill in the pc.
2782   push_skeleton_frame(masm, deopt,
2783                       unroll_block_reg,
2784                       frame_sizes_reg,
2785                       number_of_frames_reg,
2786                       pcs_reg,
2787                       frame_size_reg,
2788                       pc_reg);
2789   __ cmpdi(CCR0, number_of_frames_reg, 0);
</pre>
<hr />
<pre>
2821   // --------------------------------------------------------------------------
2822   // Prolog for non exception case!
2823 
2824   // We have been called from the deopt handler of the deoptee.
2825   //
2826   // deoptee:
2827   //                      ...
2828   //                      call X
2829   //                      ...
2830   //  deopt_handler:      call_deopt_stub
2831   //  cur. return pc  --&gt; ...
2832   //
2833   // So currently SR_LR points behind the call in the deopt handler.
2834   // We adjust it such that it points to the start of the deopt handler.
2835   // The return_pc has been stored in the frame of the deoptee and
2836   // will replace the address of the deopt_handler in the call
2837   // to Deoptimization::fetch_unroll_info below.
2838   // We can&#39;t grab a free register here, because all registers may
2839   // contain live values, so let the RegisterSaver do the adjustment
2840   // of the return pc.
<span class="line-modified">2841   const int return_pc_adjustment_no_exception = -MacroAssembler::bl64_patchable_size;</span>
2842 
2843   // Push the &quot;unpack frame&quot;
2844   // Save everything in sight.
2845   map = RegisterSaver::push_frame_reg_args_and_save_live_registers(masm,
2846                                                                    &amp;first_frame_size_in_bytes,
2847                                                                    /*generate_oop_map=*/ true,
2848                                                                    return_pc_adjustment_no_exception,
2849                                                                    RegisterSaver::return_pc_is_lr);
2850   assert(map != NULL, &quot;OopMap must have been created&quot;);
2851 
2852   __ li(exec_mode_reg, Deoptimization::Unpack_deopt);
2853   // Save exec mode for unpack_frames.
2854   __ b(exec_mode_initialized);
2855 
2856   // --------------------------------------------------------------------------
2857   // Prolog for exception case
2858 
2859   // An exception is pending.
2860   // We have been called with a return (interpreter) or a jump (exception blob).
2861   //
</pre>
</td>
</tr>
</table>
<center><a href="relocInfo_ppc.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stubGenerator_ppc.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>