diff a/src/hotspot/share/gc/epsilon/epsilonHeap.cpp b/src/hotspot/share/gc/epsilon/epsilonHeap.cpp
--- a/src/hotspot/share/gc/epsilon/epsilonHeap.cpp
+++ b/src/hotspot/share/gc/epsilon/epsilonHeap.cpp
@@ -1,7 +1,8 @@
 /*
  * Copyright (c) 2017, 2018, Red Hat, Inc. All rights reserved.
+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
  *
@@ -23,27 +24,32 @@
 
 #include "precompiled.hpp"
 #include "gc/epsilon/epsilonHeap.hpp"
 #include "gc/epsilon/epsilonMemoryPool.hpp"
 #include "gc/epsilon/epsilonThreadLocalData.hpp"
+#include "gc/shared/gcArguments.hpp"
+#include "gc/shared/locationPrinter.inline.hpp"
 #include "memory/allocation.hpp"
 #include "memory/allocation.inline.hpp"
 #include "memory/resourceArea.hpp"
+#include "memory/universe.hpp"
+#include "runtime/atomic.hpp"
+#include "runtime/globals.hpp"
 
 jint EpsilonHeap::initialize() {
-  size_t align = _policy->heap_alignment();
-  size_t init_byte_size = align_up(_policy->initial_heap_byte_size(), align);
-  size_t max_byte_size  = align_up(_policy->max_heap_byte_size(), align);
+  size_t align = HeapAlignment;
+  size_t init_byte_size = align_up(InitialHeapSize, align);
+  size_t max_byte_size  = align_up(MaxHeapSize, align);
 
   // Initialize backing storage
-  ReservedSpace heap_rs = Universe::reserve_heap(max_byte_size, align);
+  ReservedHeapSpace heap_rs = Universe::reserve_heap(max_byte_size, align);
   _virtual_space.initialize(heap_rs, init_byte_size);
 
   MemRegion committed_region((HeapWord*)_virtual_space.low(),          (HeapWord*)_virtual_space.high());
   MemRegion  reserved_region((HeapWord*)_virtual_space.low_boundary(), (HeapWord*)_virtual_space.high_boundary());
 
-  initialize_reserved_region(reserved_region.start(), reserved_region.end());
+  initialize_reserved_region(heap_rs);
 
   _space = new ContiguousSpace();
   _space->initialize(committed_region, /* clear_space = */ true, /* mangle_space = */ true);
 
   // Precompute hot fields
@@ -104,12 +110,12 @@
   return memory_pools;
 }
 
 size_t EpsilonHeap::unsafe_max_tlab_alloc(Thread* thr) const {
   // Return max allocatable TLAB size, and let allocation path figure out
-  // the actual TLAB allocation size.
-  return _max_tlab_size;
+  // the actual allocation size. Note: result should be in bytes.
+  return _max_tlab_size * HeapWordSize;
 }
 
 EpsilonHeap* EpsilonHeap::heap() {
   CollectedHeap* heap = Universe::heap();
   assert(heap != NULL, "Uninitialized access to EpsilonHeap::heap()");
@@ -122,11 +128,11 @@
 
   HeapWord* res = _space->par_allocate(size);
 
   while (res == NULL) {
     // Allocation failed, attempt expansion, and retry:
-    MutexLockerEx ml(Heap_lock);
+    MutexLocker ml(Heap_lock);
 
     size_t space_left = max_capacity() - capacity();
     size_t want_space = MAX2(size, EpsilonMinHeapExpand);
 
     if (want_space < space_left) {
@@ -150,19 +156,19 @@
   size_t used = _space->used();
 
   // Allocation successful, update counters
   {
     size_t last = _last_counter_update;
-    if ((used - last >= _step_counter_update) && Atomic::cmpxchg(used, &_last_counter_update, last) == last) {
+    if ((used - last >= _step_counter_update) && Atomic::cmpxchg(&_last_counter_update, last, used) == last) {
       _monitoring_support->update_counters();
     }
   }
 
   // ...and print the occupancy line, if needed
   {
     size_t last = _last_heap_print;
-    if ((used - last >= _step_heap_print) && Atomic::cmpxchg(used, &_last_heap_print, last) == last) {
+    if ((used - last >= _step_heap_print) && Atomic::cmpxchg(&_last_heap_print, last, used) == last) {
       print_heap_info(used);
       print_metaspace_info();
     }
   }
 
@@ -206,11 +212,11 @@
       size = (size_t) (ergo_tlab * EpsilonTLABElasticity);
     }
   }
 
   // Always honor boundaries
-  size = MAX2(min_size, MIN2(_max_tlab_size, size));
+  size = clamp(size, min_size, _max_tlab_size);
 
   // Always honor alignment
   size = align_up(size, MinObjAlignment);
 
   // Check that adjustments did not break local and global invariants
@@ -284,12 +290,12 @@
 
 void EpsilonHeap::do_full_collection(bool clear_all_soft_refs) {
   collect(gc_cause());
 }
 
-void EpsilonHeap::safe_object_iterate(ObjectClosure *cl) {
-  _space->safe_object_iterate(cl);
+void EpsilonHeap::object_iterate(ObjectClosure *cl) {
+  _space->object_iterate(cl);
 }
 
 void EpsilonHeap::print_on(outputStream *st) const {
   st->print_cr("Epsilon Heap");
 
@@ -300,10 +306,14 @@
   _space->print_on(st);
 
   MetaspaceUtils::print_on(st);
 }
 
+bool EpsilonHeap::print_location(outputStream* st, void* addr) const {
+  return BlockLocationPrinter<EpsilonHeap>::print_location(st, addr);
+}
+
 void EpsilonHeap::print_tracing_info() const {
   print_heap_info(used());
   print_metaspace_info();
 }
 
