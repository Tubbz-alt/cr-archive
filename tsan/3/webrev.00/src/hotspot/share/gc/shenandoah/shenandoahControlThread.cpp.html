<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/share/gc/shenandoah/shenandoahControlThread.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2013, 2019, Red Hat, Inc. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 
 27 #include &quot;gc/shenandoah/shenandoahConcurrentMark.inline.hpp&quot;
 28 #include &quot;gc/shenandoah/shenandoahCollectorPolicy.hpp&quot;
 29 #include &quot;gc/shenandoah/shenandoahFreeSet.hpp&quot;
 30 #include &quot;gc/shenandoah/shenandoahPhaseTimings.hpp&quot;
 31 #include &quot;gc/shenandoah/shenandoahHeap.inline.hpp&quot;
 32 #include &quot;gc/shenandoah/shenandoahHeuristics.hpp&quot;
 33 #include &quot;gc/shenandoah/shenandoahMonitoringSupport.hpp&quot;
 34 #include &quot;gc/shenandoah/shenandoahControlThread.hpp&quot;
 35 #include &quot;gc/shenandoah/shenandoahTraversalGC.hpp&quot;
 36 #include &quot;gc/shenandoah/shenandoahUtils.hpp&quot;
 37 #include &quot;gc/shenandoah/shenandoahVMOperations.hpp&quot;
 38 #include &quot;gc/shenandoah/shenandoahWorkerPolicy.hpp&quot;
 39 #include &quot;memory/iterator.hpp&quot;
 40 #include &quot;memory/universe.hpp&quot;
 41 #include &quot;runtime/atomic.hpp&quot;
 42 
 43 ShenandoahControlThread::ShenandoahControlThread() :
 44   ConcurrentGCThread(),
 45   _alloc_failure_waiters_lock(Mutex::leaf, &quot;ShenandoahAllocFailureGC_lock&quot;, true, Monitor::_safepoint_check_always),
 46   _gc_waiters_lock(Mutex::leaf, &quot;ShenandoahRequestedGC_lock&quot;, true, Monitor::_safepoint_check_always),
 47   _periodic_task(this),
 48   _requested_gc_cause(GCCause::_no_cause_specified),
 49   _degen_point(ShenandoahHeap::_degenerated_outside_cycle),
 50   _allocs_seen(0) {
 51 
 52   create_and_start(ShenandoahCriticalControlThreadPriority ? CriticalPriority : NearMaxPriority);
 53   _periodic_task.enroll();
 54   _periodic_satb_flush_task.enroll();
 55 }
 56 
 57 ShenandoahControlThread::~ShenandoahControlThread() {
 58   // This is here so that super is called.
 59 }
 60 
 61 void ShenandoahPeriodicTask::task() {
 62   _thread-&gt;handle_force_counters_update();
 63   _thread-&gt;handle_counters_update();
 64 }
 65 
 66 void ShenandoahPeriodicSATBFlushTask::task() {
 67   ShenandoahHeap::heap()-&gt;force_satb_flush_all_threads();
 68 }
 69 
 70 void ShenandoahControlThread::run_service() {
 71   ShenandoahHeap* heap = ShenandoahHeap::heap();
 72 
 73   GCMode default_mode = heap-&gt;is_traversal_mode() ?
 74                            concurrent_traversal : concurrent_normal;
 75   GCCause::Cause default_cause = heap-&gt;is_traversal_mode() ?
 76                            GCCause::_shenandoah_traversal_gc : GCCause::_shenandoah_concurrent_gc;
 77   int sleep = ShenandoahControlIntervalMin;
 78 
 79   double last_shrink_time = os::elapsedTime();
 80   double last_sleep_adjust_time = os::elapsedTime();
 81 
 82   // Shrink period avoids constantly polling regions for shrinking.
 83   // Having a period 10x lower than the delay would mean we hit the
 84   // shrinking with lag of less than 1/10-th of true delay.
 85   // ShenandoahUncommitDelay is in msecs, but shrink_period is in seconds.
 86   double shrink_period = (double)ShenandoahUncommitDelay / 1000 / 10;
 87 
 88   ShenandoahCollectorPolicy* policy = heap-&gt;shenandoah_policy();
 89   ShenandoahHeuristics* heuristics = heap-&gt;heuristics();
 90   while (!in_graceful_shutdown() &amp;&amp; !should_terminate()) {
 91     // Figure out if we have pending requests.
 92     bool alloc_failure_pending = _alloc_failure_gc.is_set();
 93     bool explicit_gc_requested = _gc_requested.is_set() &amp;&amp;  is_explicit_gc(_requested_gc_cause);
 94     bool implicit_gc_requested = _gc_requested.is_set() &amp;&amp; !is_explicit_gc(_requested_gc_cause);
 95 
 96     // This control loop iteration have seen this much allocations.
 97     size_t allocs_seen = Atomic::xchg(&amp;_allocs_seen, (size_t)0);
 98 
 99     // Choose which GC mode to run in. The block below should select a single mode.
100     GCMode mode = none;
101     GCCause::Cause cause = GCCause::_last_gc_cause;
102     ShenandoahHeap::ShenandoahDegenPoint degen_point = ShenandoahHeap::_degenerated_unset;
103 
104     if (alloc_failure_pending) {
105       // Allocation failure takes precedence: we have to deal with it first thing
106       log_info(gc)(&quot;Trigger: Handle Allocation Failure&quot;);
107 
108       cause = GCCause::_allocation_failure;
109 
110       // Consume the degen point, and seed it with default value
111       degen_point = _degen_point;
112       _degen_point = ShenandoahHeap::_degenerated_outside_cycle;
113 
114       if (ShenandoahDegeneratedGC &amp;&amp; heuristics-&gt;should_degenerate_cycle()) {
115         heuristics-&gt;record_allocation_failure_gc();
116         policy-&gt;record_alloc_failure_to_degenerated(degen_point);
117         mode = stw_degenerated;
118       } else {
119         heuristics-&gt;record_allocation_failure_gc();
120         policy-&gt;record_alloc_failure_to_full();
121         mode = stw_full;
122       }
123 
124     } else if (explicit_gc_requested) {
125       cause = _requested_gc_cause;
126       log_info(gc)(&quot;Trigger: Explicit GC request (%s)&quot;, GCCause::to_string(cause));
127 
128       heuristics-&gt;record_requested_gc();
129 
130       if (ExplicitGCInvokesConcurrent) {
131         policy-&gt;record_explicit_to_concurrent();
132         mode = default_mode;
133         // Unload and clean up everything
134         heap-&gt;set_process_references(heuristics-&gt;can_process_references());
135         heap-&gt;set_unload_classes(heuristics-&gt;can_unload_classes());
136       } else {
137         policy-&gt;record_explicit_to_full();
138         mode = stw_full;
139       }
140     } else if (implicit_gc_requested) {
141       cause = _requested_gc_cause;
142       log_info(gc)(&quot;Trigger: Implicit GC request (%s)&quot;, GCCause::to_string(cause));
143 
144       heuristics-&gt;record_requested_gc();
145 
146       if (ShenandoahImplicitGCInvokesConcurrent) {
147         policy-&gt;record_implicit_to_concurrent();
148         mode = default_mode;
149 
150         // Unload and clean up everything
151         heap-&gt;set_process_references(heuristics-&gt;can_process_references());
152         heap-&gt;set_unload_classes(heuristics-&gt;can_unload_classes());
153       } else {
154         policy-&gt;record_implicit_to_full();
155         mode = stw_full;
156       }
157     } else {
158       // Potential normal cycle: ask heuristics if it wants to act
159       if (heuristics-&gt;should_start_gc()) {
160         mode = default_mode;
161         cause = default_cause;
162       }
163 
164       // Ask policy if this cycle wants to process references or unload classes
165       heap-&gt;set_process_references(heuristics-&gt;should_process_references());
166       heap-&gt;set_unload_classes(heuristics-&gt;should_unload_classes());
167     }
168 
169     // Blow all soft references on this cycle, if handling allocation failure,
170     // or we are requested to do so unconditionally.
171     if (alloc_failure_pending || ShenandoahAlwaysClearSoftRefs) {
172       heap-&gt;soft_ref_policy()-&gt;set_should_clear_all_soft_refs(true);
173     }
174 
175     bool gc_requested = (mode != none);
176     assert (!gc_requested || cause != GCCause::_last_gc_cause, &quot;GC cause should be set&quot;);
177 
178     if (gc_requested) {
179       heap-&gt;reset_bytes_allocated_since_gc_start();
180 
181       // If GC was requested, we are sampling the counters even without actual triggers
182       // from allocation machinery. This captures GC phases more accurately.
183       set_forced_counters_update(true);
184 
185       // If GC was requested, we better dump freeset data for performance debugging
186       {
187         ShenandoahHeapLocker locker(heap-&gt;lock());
188         heap-&gt;free_set()-&gt;log_status();
189       }
190     }
191 
192     switch (mode) {
193       case none:
194         break;
195       case concurrent_traversal:
196         service_concurrent_traversal_cycle(cause);
197         break;
198       case concurrent_normal:
199         service_concurrent_normal_cycle(cause);
200         break;
201       case stw_degenerated:
202         service_stw_degenerated_cycle(cause, degen_point);
203         break;
204       case stw_full:
205         service_stw_full_cycle(cause);
206         break;
207       default:
208         ShouldNotReachHere();
209     }
210 
211     if (gc_requested) {
212       // If this was the requested GC cycle, notify waiters about it
213       if (explicit_gc_requested || implicit_gc_requested) {
214         notify_gc_waiters();
215       }
216 
217       // If this was the allocation failure GC cycle, notify waiters about it
218       if (alloc_failure_pending) {
219         notify_alloc_failure_waiters();
220       }
221 
222       // Report current free set state at the end of cycle, whether
223       // it is a normal completion, or the abort.
224       {
225         ShenandoahHeapLocker locker(heap-&gt;lock());
226         heap-&gt;free_set()-&gt;log_status();
227 
228         // Notify Universe about new heap usage. This has implications for
229         // global soft refs policy, and we better report it every time heap
230         // usage goes down.
231         Universe::update_heap_info_at_gc();
232       }
233 
234       // Disable forced counters update, and update counters one more time
235       // to capture the state at the end of GC session.
236       handle_force_counters_update();
237       set_forced_counters_update(false);
238 
239       // Retract forceful part of soft refs policy
240       heap-&gt;soft_ref_policy()-&gt;set_should_clear_all_soft_refs(false);
241 
242       // Clear metaspace oom flag, if current cycle unloaded classes
243       if (heap-&gt;unload_classes()) {
244         heuristics-&gt;clear_metaspace_oom();
245       }
246 
247       // GC is over, we are at idle now
248       if (ShenandoahPacing) {
249         heap-&gt;pacer()-&gt;setup_for_idle();
250       }
251     } else {
252       // Allow allocators to know we have seen this much regions
253       if (ShenandoahPacing &amp;&amp; (allocs_seen &gt; 0)) {
254         heap-&gt;pacer()-&gt;report_alloc(allocs_seen);
255       }
256     }
257 
258     double current = os::elapsedTime();
259 
260     if (ShenandoahUncommit &amp;&amp; (explicit_gc_requested || (current - last_shrink_time &gt; shrink_period))) {
261       // Try to uncommit enough stale regions. Explicit GC tries to uncommit everything.
262       // Regular paths uncommit only occasionally.
263       double shrink_before = explicit_gc_requested ?
264                              current :
265                              current - (ShenandoahUncommitDelay / 1000.0);
266       service_uncommit(shrink_before);
267       last_shrink_time = current;
268     }
269 
270     // Wait before performing the next action. If allocation happened during this wait,
271     // we exit sooner, to let heuristics re-evaluate new conditions. If we are at idle,
272     // back off exponentially.
273     if (_heap_changed.try_unset()) {
274       sleep = ShenandoahControlIntervalMin;
275     } else if ((current - last_sleep_adjust_time) * 1000 &gt; ShenandoahControlIntervalAdjustPeriod){
276       sleep = MIN2&lt;int&gt;(ShenandoahControlIntervalMax, MAX2(1, sleep * 2));
277       last_sleep_adjust_time = current;
278     }
279     os::naked_short_sleep(sleep);
280   }
281 
282   // Wait for the actual stop(), can&#39;t leave run_service() earlier.
283   while (!should_terminate()) {
284     os::naked_short_sleep(ShenandoahControlIntervalMin);
285   }
286 }
287 
288 void ShenandoahControlThread::service_concurrent_traversal_cycle(GCCause::Cause cause) {
289   GCIdMark gc_id_mark;
290   ShenandoahGCSession session(cause);
291 
292   ShenandoahHeap* heap = ShenandoahHeap::heap();
293   TraceCollectorStats tcs(heap-&gt;monitoring_support()-&gt;concurrent_collection_counters());
294 
295   // Reset for upcoming cycle
296   heap-&gt;entry_reset();
297 
298   heap-&gt;vmop_entry_init_traversal();
299 
300   if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_traversal)) return;
301 
302   heap-&gt;entry_traversal();
303   if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_traversal)) return;
304 
305   heap-&gt;vmop_entry_final_traversal();
306 
307   heap-&gt;entry_cleanup();
308 
309   heap-&gt;heuristics()-&gt;record_success_concurrent();
310   heap-&gt;shenandoah_policy()-&gt;record_success_concurrent();
311 }
312 
313 void ShenandoahControlThread::service_concurrent_normal_cycle(GCCause::Cause cause) {
314   // Normal cycle goes via all concurrent phases. If allocation failure (af) happens during
315   // any of the concurrent phases, it first degrades to Degenerated GC and completes GC there.
316   // If second allocation failure happens during Degenerated GC cycle (for example, when GC
317   // tries to evac something and no memory is available), cycle degrades to Full GC.
318   //
319   // There are also two shortcuts through the normal cycle: a) immediate garbage shortcut, when
320   // heuristics says there are no regions to compact, and all the collection comes from immediately
321   // reclaimable regions; b) coalesced UR shortcut, when heuristics decides to coalesce UR with the
322   // mark from the next cycle.
323   //
324   // ................................................................................................
325   //
326   //                                    (immediate garbage shortcut)                Concurrent GC
327   //                             /-------------------------------------------\
328   //                             |                       (coalesced UR)      v
329   //                             |                  /-----------------------&gt;o
330   //                             |                  |                        |
331   //                             |                  |                        v
332   // [START] ----&gt; Conc Mark ----o----&gt; Conc Evac --o--&gt; Conc Update-Refs ---o----&gt; [END]
333   //                   |                    |                 |              ^
334   //                   | (af)               | (af)            | (af)         |
335   // ..................|....................|.................|..............|.......................
336   //                   |                    |                 |              |
337   //                   |                    |                 |              |      Degenerated GC
338   //                   v                    v                 v              |
339   //               STW Mark ----------&gt; STW Evac ----&gt; STW Update-Refs -----&gt;o
340   //                   |                    |                 |              ^
341   //                   | (af)               | (af)            | (af)         |
342   // ..................|....................|.................|..............|.......................
343   //                   |                    |                 |              |
344   //                   |                    v                 |              |      Full GC
345   //                   \-------------------&gt;o&lt;----------------/              |
346   //                                        |                                |
347   //                                        v                                |
348   //                                      Full GC  --------------------------/
349   //
350   ShenandoahHeap* heap = ShenandoahHeap::heap();
351 
352   if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_outside_cycle)) return;
353 
354   GCIdMark gc_id_mark;
355   ShenandoahGCSession session(cause);
356 
357   TraceCollectorStats tcs(heap-&gt;monitoring_support()-&gt;concurrent_collection_counters());
358 
359   // Reset for upcoming marking
360   heap-&gt;entry_reset();
361 
362   // Start initial mark under STW
363   heap-&gt;vmop_entry_init_mark();
364 
365   // Continue concurrent mark
366   heap-&gt;entry_mark();
367   if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_mark)) return;
368 
369   // If not cancelled, can try to concurrently pre-clean
370   heap-&gt;entry_preclean();
371 
372   // Complete marking under STW, and start evacuation
373   heap-&gt;vmop_entry_final_mark();
374 
375   // Evacuate concurrent roots
376   heap-&gt;entry_roots();
377 
378   // Final mark might have reclaimed some immediate garbage, kick cleanup to reclaim
379   // the space. This would be the last action if there is nothing to evacuate.
380   heap-&gt;entry_cleanup();
381 
382   {
383     ShenandoahHeapLocker locker(heap-&gt;lock());
384     heap-&gt;free_set()-&gt;log_status();
385   }
386 
387   // Continue the cycle with evacuation and optional update-refs.
388   // This may be skipped if there is nothing to evacuate.
389   // If so, evac_in_progress would be unset by collection set preparation code.
390   if (heap-&gt;is_evacuation_in_progress()) {
391     // Concurrently evacuate
392     heap-&gt;entry_evac();
393     if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_evac)) return;
394 
395     // Perform update-refs phase, if required. This phase can be skipped if heuristics
396     // decides to piggy-back the update-refs on the next marking cycle. On either path,
397     // we need to turn off evacuation: either in init-update-refs, or in final-evac.
398     if (heap-&gt;heuristics()-&gt;should_start_update_refs()) {
399       heap-&gt;vmop_entry_init_updaterefs();
400       heap-&gt;entry_updaterefs();
401       if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_updaterefs)) return;
402 
403       heap-&gt;vmop_entry_final_updaterefs();
404 
405       // Update references freed up collection set, kick the cleanup to reclaim the space.
406       heap-&gt;entry_cleanup();
407 
408     } else {
409       heap-&gt;vmop_entry_final_evac();
410     }
411   }
412 
413   // Cycle is complete
414   heap-&gt;heuristics()-&gt;record_success_concurrent();
415   heap-&gt;shenandoah_policy()-&gt;record_success_concurrent();
416 }
417 
418 bool ShenandoahControlThread::check_cancellation_or_degen(ShenandoahHeap::ShenandoahDegenPoint point) {
419   ShenandoahHeap* heap = ShenandoahHeap::heap();
420   if (heap-&gt;cancelled_gc()) {
421     assert (is_alloc_failure_gc() || in_graceful_shutdown(), &quot;Cancel GC either for alloc failure GC, or gracefully exiting&quot;);
422     if (!in_graceful_shutdown()) {
423       assert (_degen_point == ShenandoahHeap::_degenerated_outside_cycle,
424               &quot;Should not be set yet: %s&quot;, ShenandoahHeap::degen_point_to_string(_degen_point));
425       _degen_point = point;
426     }
427     return true;
428   }
429   return false;
430 }
431 
432 void ShenandoahControlThread::stop_service() {
433   // Nothing to do here.
434 }
435 
436 void ShenandoahControlThread::service_stw_full_cycle(GCCause::Cause cause) {
437   GCIdMark gc_id_mark;
438   ShenandoahGCSession session(cause);
439 
440   ShenandoahHeap* heap = ShenandoahHeap::heap();
441   heap-&gt;vmop_entry_full(cause);
442 
443   heap-&gt;heuristics()-&gt;record_success_full();
444   heap-&gt;shenandoah_policy()-&gt;record_success_full();
445 }
446 
447 void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahHeap::ShenandoahDegenPoint point) {
448   assert (point != ShenandoahHeap::_degenerated_unset, &quot;Degenerated point should be set&quot;);
449 
450   GCIdMark gc_id_mark;
451   ShenandoahGCSession session(cause);
452 
453   ShenandoahHeap* heap = ShenandoahHeap::heap();
454   heap-&gt;vmop_degenerated(point);
455 
456   heap-&gt;heuristics()-&gt;record_success_degenerated();
457   heap-&gt;shenandoah_policy()-&gt;record_success_degenerated();
458 }
459 
460 void ShenandoahControlThread::service_uncommit(double shrink_before) {
461   ShenandoahHeap* heap = ShenandoahHeap::heap();
462 
463   // Determine if there is work to do. This avoids taking heap lock if there is
464   // no work available, avoids spamming logs with superfluous logging messages,
465   // and minimises the amount of work while locks are taken.
466 
467   if (heap-&gt;committed() &lt;= heap-&gt;min_capacity()) return;
468 
469   bool has_work = false;
470   for (size_t i = 0; i &lt; heap-&gt;num_regions(); i++) {
471     ShenandoahHeapRegion *r = heap-&gt;get_region(i);
472     if (r-&gt;is_empty_committed() &amp;&amp; (r-&gt;empty_time() &lt; shrink_before)) {
473       has_work = true;
474       break;
475     }
476   }
477 
478   if (has_work) {
479     heap-&gt;entry_uncommit(shrink_before);
480   }
481 }
482 
483 bool ShenandoahControlThread::is_explicit_gc(GCCause::Cause cause) const {
484   return GCCause::is_user_requested_gc(cause) ||
485          GCCause::is_serviceability_requested_gc(cause);
486 }
487 
488 void ShenandoahControlThread::request_gc(GCCause::Cause cause) {
489   assert(GCCause::is_user_requested_gc(cause) ||
490          GCCause::is_serviceability_requested_gc(cause) ||
491          cause == GCCause::_metadata_GC_clear_soft_refs ||
492          cause == GCCause::_full_gc_alot ||
493          cause == GCCause::_wb_full_gc ||
494          cause == GCCause::_scavenge_alot,
495          &quot;only requested GCs here&quot;);
496 
497   if (is_explicit_gc(cause)) {
498     if (!DisableExplicitGC) {
499       handle_requested_gc(cause);
500     }
501   } else {
502     handle_requested_gc(cause);
503   }
504 }
505 
506 void ShenandoahControlThread::handle_requested_gc(GCCause::Cause cause) {
507   _requested_gc_cause = cause;
508   _gc_requested.set();
509   MonitorLocker ml(&amp;_gc_waiters_lock);
510   while (_gc_requested.is_set()) {
511     ml.wait();
512   }
513 }
514 
515 void ShenandoahControlThread::handle_alloc_failure(size_t words) {
516   ShenandoahHeap* heap = ShenandoahHeap::heap();
517 
518   assert(current()-&gt;is_Java_thread(), &quot;expect Java thread here&quot;);
519 
520   if (try_set_alloc_failure_gc()) {
521     // Only report the first allocation failure
522     log_info(gc)(&quot;Failed to allocate &quot; SIZE_FORMAT &quot;%s&quot;,
523                  byte_size_in_proper_unit(words * HeapWordSize), proper_unit_for_byte_size(words * HeapWordSize));
524 
525     // Now that alloc failure GC is scheduled, we can abort everything else
526     heap-&gt;cancel_gc(GCCause::_allocation_failure);
527   }
528 
529   MonitorLocker ml(&amp;_alloc_failure_waiters_lock);
530   while (is_alloc_failure_gc()) {
531     ml.wait();
532   }
533 }
534 
535 void ShenandoahControlThread::handle_alloc_failure_evac(size_t words) {
536   ShenandoahHeap* heap = ShenandoahHeap::heap();
537 
538   if (try_set_alloc_failure_gc()) {
539     // Only report the first allocation failure
540     log_info(gc)(&quot;Failed to allocate &quot; SIZE_FORMAT &quot;%s for evacuation&quot;,
541                  byte_size_in_proper_unit(words * HeapWordSize), proper_unit_for_byte_size(words * HeapWordSize));
542   }
543 
544   // Forcefully report allocation failure
545   heap-&gt;cancel_gc(GCCause::_shenandoah_allocation_failure_evac);
546 }
547 
548 void ShenandoahControlThread::notify_alloc_failure_waiters() {
549   _alloc_failure_gc.unset();
550   MonitorLocker ml(&amp;_alloc_failure_waiters_lock);
551   ml.notify_all();
552 }
553 
554 bool ShenandoahControlThread::try_set_alloc_failure_gc() {
555   return _alloc_failure_gc.try_set();
556 }
557 
558 bool ShenandoahControlThread::is_alloc_failure_gc() {
559   return _alloc_failure_gc.is_set();
560 }
561 
562 void ShenandoahControlThread::notify_gc_waiters() {
563   _gc_requested.unset();
564   MonitorLocker ml(&amp;_gc_waiters_lock);
565   ml.notify_all();
566 }
567 
568 void ShenandoahControlThread::handle_counters_update() {
569   if (_do_counters_update.is_set()) {
570     _do_counters_update.unset();
571     ShenandoahHeap::heap()-&gt;monitoring_support()-&gt;update_counters();
572   }
573 }
574 
575 void ShenandoahControlThread::handle_force_counters_update() {
576   if (_force_counters_update.is_set()) {
577     _do_counters_update.unset(); // reset these too, we do update now!
578     ShenandoahHeap::heap()-&gt;monitoring_support()-&gt;update_counters();
579   }
580 }
581 
582 void ShenandoahControlThread::notify_heap_changed() {
583   // This is called from allocation path, and thus should be fast.
584 
585   // Update monitoring counters when we took a new region. This amortizes the
586   // update costs on slow path.
587   if (_do_counters_update.is_unset()) {
588     _do_counters_update.set();
589   }
590   // Notify that something had changed.
591   if (_heap_changed.is_unset()) {
592     _heap_changed.set();
593   }
594 }
595 
596 void ShenandoahControlThread::pacing_notify_alloc(size_t words) {
597   assert(ShenandoahPacing, &quot;should only call when pacing is enabled&quot;);
598   Atomic::add(&amp;_allocs_seen, words);
599 }
600 
601 void ShenandoahControlThread::set_forced_counters_update(bool value) {
602   _force_counters_update.set_cond(value);
603 }
604 
605 void ShenandoahControlThread::print() const {
606   print_on(tty);
607 }
608 
609 void ShenandoahControlThread::print_on(outputStream* st) const {
610   st-&gt;print(&quot;Shenandoah Concurrent Thread&quot;);
611   Thread::print_on(st);
612   st-&gt;cr();
613 }
614 
615 void ShenandoahControlThread::start() {
616   create_and_start();
617 }
618 
619 void ShenandoahControlThread::prepare_for_graceful_shutdown() {
620   _graceful_shutdown.set();
621 }
622 
623 bool ShenandoahControlThread::in_graceful_shutdown() {
624   return _graceful_shutdown.is_set();
625 }
    </pre>
  </body>
</html>