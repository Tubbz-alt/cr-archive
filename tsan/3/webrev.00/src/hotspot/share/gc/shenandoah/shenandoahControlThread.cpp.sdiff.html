<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/shenandoah/shenandoahControlThread.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="shenandoahConcurrentMark.inline.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="shenandoahControlThread.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/shenandoah/shenandoahControlThread.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2013, 2018, Red Hat, Inc. All rights reserved.</span>

  3  *
  4  * This code is free software; you can redistribute it and/or modify it
  5  * under the terms of the GNU General Public License version 2 only, as
  6  * published by the Free Software Foundation.
  7  *
  8  * This code is distributed in the hope that it will be useful, but WITHOUT
  9  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 10  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 11  * version 2 for more details (a copy is included in the LICENSE file that
 12  * accompanied this code).
 13  *
 14  * You should have received a copy of the GNU General Public License version
 15  * 2 along with this work; if not, write to the Free Software Foundation,
 16  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 17  *
 18  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 19  * or visit www.oracle.com if you need additional information or have any
 20  * questions.
 21  *
 22  */
 23 
 24 #include &quot;precompiled.hpp&quot;
 25 
 26 #include &quot;gc/shenandoah/shenandoahConcurrentMark.inline.hpp&quot;
 27 #include &quot;gc/shenandoah/shenandoahCollectorPolicy.hpp&quot;
 28 #include &quot;gc/shenandoah/shenandoahFreeSet.hpp&quot;
 29 #include &quot;gc/shenandoah/shenandoahPhaseTimings.hpp&quot;
 30 #include &quot;gc/shenandoah/shenandoahHeap.inline.hpp&quot;
 31 #include &quot;gc/shenandoah/shenandoahHeuristics.hpp&quot;
 32 #include &quot;gc/shenandoah/shenandoahMonitoringSupport.hpp&quot;
 33 #include &quot;gc/shenandoah/shenandoahControlThread.hpp&quot;
 34 #include &quot;gc/shenandoah/shenandoahTraversalGC.hpp&quot;
 35 #include &quot;gc/shenandoah/shenandoahUtils.hpp&quot;
 36 #include &quot;gc/shenandoah/shenandoahVMOperations.hpp&quot;
 37 #include &quot;gc/shenandoah/shenandoahWorkerPolicy.hpp&quot;
 38 #include &quot;memory/iterator.hpp&quot;
 39 #include &quot;memory/universe.hpp&quot;

 40 
 41 ShenandoahControlThread::ShenandoahControlThread() :
 42   ConcurrentGCThread(),
 43   _alloc_failure_waiters_lock(Mutex::leaf, &quot;ShenandoahAllocFailureGC_lock&quot;, true, Monitor::_safepoint_check_always),
 44   _gc_waiters_lock(Mutex::leaf, &quot;ShenandoahRequestedGC_lock&quot;, true, Monitor::_safepoint_check_always),
 45   _periodic_task(this),
 46   _requested_gc_cause(GCCause::_no_cause_specified),
 47   _degen_point(ShenandoahHeap::_degenerated_outside_cycle),
 48   _allocs_seen(0) {
 49 
 50   create_and_start(ShenandoahCriticalControlThreadPriority ? CriticalPriority : NearMaxPriority);
 51   _periodic_task.enroll();
 52   _periodic_satb_flush_task.enroll();
 53 }
 54 
 55 ShenandoahControlThread::~ShenandoahControlThread() {
 56   // This is here so that super is called.
 57 }
 58 
 59 void ShenandoahPeriodicTask::task() {
 60   _thread-&gt;handle_force_counters_update();
 61   _thread-&gt;handle_counters_update();
 62 }
 63 
 64 void ShenandoahPeriodicSATBFlushTask::task() {
 65   ShenandoahHeap::heap()-&gt;force_satb_flush_all_threads();
 66 }
 67 
 68 void ShenandoahControlThread::run_service() {
 69   ShenandoahHeap* heap = ShenandoahHeap::heap();
 70 




 71   int sleep = ShenandoahControlIntervalMin;
 72 
 73   double last_shrink_time = os::elapsedTime();
 74   double last_sleep_adjust_time = os::elapsedTime();
 75 
 76   // Shrink period avoids constantly polling regions for shrinking.
 77   // Having a period 10x lower than the delay would mean we hit the
 78   // shrinking with lag of less than 1/10-th of true delay.
 79   // ShenandoahUncommitDelay is in msecs, but shrink_period is in seconds.
 80   double shrink_period = (double)ShenandoahUncommitDelay / 1000 / 10;
 81 
 82   ShenandoahCollectorPolicy* policy = heap-&gt;shenandoah_policy();
 83   ShenandoahHeuristics* heuristics = heap-&gt;heuristics();
 84   while (!in_graceful_shutdown() &amp;&amp; !should_terminate()) {
 85     // Figure out if we have pending requests.
 86     bool alloc_failure_pending = _alloc_failure_gc.is_set();
 87     bool explicit_gc_requested = _gc_requested.is_set() &amp;&amp;  is_explicit_gc(_requested_gc_cause);
 88     bool implicit_gc_requested = _gc_requested.is_set() &amp;&amp; !is_explicit_gc(_requested_gc_cause);
 89 
 90     // This control loop iteration have seen this much allocations.
<span class="line-modified"> 91     size_t allocs_seen = Atomic::xchg&lt;size_t&gt;(0, &amp;_allocs_seen);</span>
 92 
 93     // Choose which GC mode to run in. The block below should select a single mode.
 94     GCMode mode = none;
 95     GCCause::Cause cause = GCCause::_last_gc_cause;
 96     ShenandoahHeap::ShenandoahDegenPoint degen_point = ShenandoahHeap::_degenerated_unset;
 97 
 98     if (alloc_failure_pending) {
 99       // Allocation failure takes precedence: we have to deal with it first thing
100       log_info(gc)(&quot;Trigger: Handle Allocation Failure&quot;);
101 
102       cause = GCCause::_allocation_failure;
103 
104       // Consume the degen point, and seed it with default value
105       degen_point = _degen_point;
106       _degen_point = ShenandoahHeap::_degenerated_outside_cycle;
107 
108       if (ShenandoahDegeneratedGC &amp;&amp; heuristics-&gt;should_degenerate_cycle()) {
109         heuristics-&gt;record_allocation_failure_gc();
110         policy-&gt;record_alloc_failure_to_degenerated(degen_point);
111         mode = stw_degenerated;
112       } else {
113         heuristics-&gt;record_allocation_failure_gc();
114         policy-&gt;record_alloc_failure_to_full();
115         mode = stw_full;
116       }
117 
118     } else if (explicit_gc_requested) {
119       cause = _requested_gc_cause;
120       log_info(gc)(&quot;Trigger: Explicit GC request (%s)&quot;, GCCause::to_string(cause));
121 
122       heuristics-&gt;record_requested_gc();
123 
124       if (ExplicitGCInvokesConcurrent) {
125         policy-&gt;record_explicit_to_concurrent();
<span class="line-modified">126         if (heuristics-&gt;can_do_traversal_gc()) {</span>
<span class="line-removed">127           mode = concurrent_traversal;</span>
<span class="line-removed">128         } else {</span>
<span class="line-removed">129           mode = concurrent_normal;</span>
<span class="line-removed">130         }</span>
131         // Unload and clean up everything
132         heap-&gt;set_process_references(heuristics-&gt;can_process_references());
133         heap-&gt;set_unload_classes(heuristics-&gt;can_unload_classes());
134       } else {
135         policy-&gt;record_explicit_to_full();
136         mode = stw_full;
137       }
138     } else if (implicit_gc_requested) {
139       cause = _requested_gc_cause;
140       log_info(gc)(&quot;Trigger: Implicit GC request (%s)&quot;, GCCause::to_string(cause));
141 
142       heuristics-&gt;record_requested_gc();
143 
144       if (ShenandoahImplicitGCInvokesConcurrent) {
145         policy-&gt;record_implicit_to_concurrent();
<span class="line-modified">146         if (heuristics-&gt;can_do_traversal_gc()) {</span>
<span class="line-removed">147           mode = concurrent_traversal;</span>
<span class="line-removed">148         } else {</span>
<span class="line-removed">149           mode = concurrent_normal;</span>
<span class="line-removed">150         }</span>
151 
152         // Unload and clean up everything
153         heap-&gt;set_process_references(heuristics-&gt;can_process_references());
154         heap-&gt;set_unload_classes(heuristics-&gt;can_unload_classes());
155       } else {
156         policy-&gt;record_implicit_to_full();
157         mode = stw_full;
158       }
159     } else {
160       // Potential normal cycle: ask heuristics if it wants to act
<span class="line-modified">161       if (heuristics-&gt;should_start_traversal_gc()) {</span>
<span class="line-modified">162         mode = concurrent_traversal;</span>
<span class="line-modified">163         cause = GCCause::_shenandoah_traversal_gc;</span>
<span class="line-removed">164       } else if (heuristics-&gt;should_start_normal_gc()) {</span>
<span class="line-removed">165         mode = concurrent_normal;</span>
<span class="line-removed">166         cause = GCCause::_shenandoah_concurrent_gc;</span>
167       }
168 
169       // Ask policy if this cycle wants to process references or unload classes
170       heap-&gt;set_process_references(heuristics-&gt;should_process_references());
171       heap-&gt;set_unload_classes(heuristics-&gt;should_unload_classes());
172     }
173 
174     // Blow all soft references on this cycle, if handling allocation failure,
175     // or we are requested to do so unconditionally.
176     if (alloc_failure_pending || ShenandoahAlwaysClearSoftRefs) {
177       heap-&gt;soft_ref_policy()-&gt;set_should_clear_all_soft_refs(true);
178     }
179 
180     bool gc_requested = (mode != none);
181     assert (!gc_requested || cause != GCCause::_last_gc_cause, &quot;GC cause should be set&quot;);
182 
183     if (gc_requested) {
184       heap-&gt;reset_bytes_allocated_since_gc_start();
185 
186       // If GC was requested, we are sampling the counters even without actual triggers
</pre>
<hr />
<pre>
360   ShenandoahGCSession session(cause);
361 
362   TraceCollectorStats tcs(heap-&gt;monitoring_support()-&gt;concurrent_collection_counters());
363 
364   // Reset for upcoming marking
365   heap-&gt;entry_reset();
366 
367   // Start initial mark under STW
368   heap-&gt;vmop_entry_init_mark();
369 
370   // Continue concurrent mark
371   heap-&gt;entry_mark();
372   if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_mark)) return;
373 
374   // If not cancelled, can try to concurrently pre-clean
375   heap-&gt;entry_preclean();
376 
377   // Complete marking under STW, and start evacuation
378   heap-&gt;vmop_entry_final_mark();
379 



380   // Final mark might have reclaimed some immediate garbage, kick cleanup to reclaim
381   // the space. This would be the last action if there is nothing to evacuate.
382   heap-&gt;entry_cleanup();
383 
384   {
385     ShenandoahHeapLocker locker(heap-&gt;lock());
386     heap-&gt;free_set()-&gt;log_status();
387   }
388 
389   // Continue the cycle with evacuation and optional update-refs.
390   // This may be skipped if there is nothing to evacuate.
391   // If so, evac_in_progress would be unset by collection set preparation code.
392   if (heap-&gt;is_evacuation_in_progress()) {
393     // Concurrently evacuate
394     heap-&gt;entry_evac();
395     if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_evac)) return;
396 
397     // Perform update-refs phase, if required. This phase can be skipped if heuristics
398     // decides to piggy-back the update-refs on the next marking cycle. On either path,
399     // we need to turn off evacuation: either in init-update-refs, or in final-evac.
</pre>
<hr />
<pre>
445   heap-&gt;heuristics()-&gt;record_success_full();
446   heap-&gt;shenandoah_policy()-&gt;record_success_full();
447 }
448 
449 void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahHeap::ShenandoahDegenPoint point) {
450   assert (point != ShenandoahHeap::_degenerated_unset, &quot;Degenerated point should be set&quot;);
451 
452   GCIdMark gc_id_mark;
453   ShenandoahGCSession session(cause);
454 
455   ShenandoahHeap* heap = ShenandoahHeap::heap();
456   heap-&gt;vmop_degenerated(point);
457 
458   heap-&gt;heuristics()-&gt;record_success_degenerated();
459   heap-&gt;shenandoah_policy()-&gt;record_success_degenerated();
460 }
461 
462 void ShenandoahControlThread::service_uncommit(double shrink_before) {
463   ShenandoahHeap* heap = ShenandoahHeap::heap();
464 
<span class="line-modified">465   // Scan through the heap and determine if there is work to do. This avoids taking</span>
<span class="line-modified">466   // heap lock if there is no work available, avoids spamming logs with superfluous</span>
<span class="line-modified">467   // logging messages, and minimises the amount of work while locks are taken.</span>


468 
469   bool has_work = false;
470   for (size_t i = 0; i &lt; heap-&gt;num_regions(); i++) {
471     ShenandoahHeapRegion *r = heap-&gt;get_region(i);
472     if (r-&gt;is_empty_committed() &amp;&amp; (r-&gt;empty_time() &lt; shrink_before)) {
473       has_work = true;
474       break;
475     }
476   }
477 
478   if (has_work) {
479     heap-&gt;entry_uncommit(shrink_before);
480   }
481 }
482 
483 bool ShenandoahControlThread::is_explicit_gc(GCCause::Cause cause) const {
484   return GCCause::is_user_requested_gc(cause) ||
485          GCCause::is_serviceability_requested_gc(cause);
486 }
487 
</pre>
<hr />
<pre>
489   assert(GCCause::is_user_requested_gc(cause) ||
490          GCCause::is_serviceability_requested_gc(cause) ||
491          cause == GCCause::_metadata_GC_clear_soft_refs ||
492          cause == GCCause::_full_gc_alot ||
493          cause == GCCause::_wb_full_gc ||
494          cause == GCCause::_scavenge_alot,
495          &quot;only requested GCs here&quot;);
496 
497   if (is_explicit_gc(cause)) {
498     if (!DisableExplicitGC) {
499       handle_requested_gc(cause);
500     }
501   } else {
502     handle_requested_gc(cause);
503   }
504 }
505 
506 void ShenandoahControlThread::handle_requested_gc(GCCause::Cause cause) {
507   _requested_gc_cause = cause;
508   _gc_requested.set();
<span class="line-modified">509   MonitorLockerEx ml(&amp;_gc_waiters_lock);</span>
510   while (_gc_requested.is_set()) {
511     ml.wait();
512   }
513 }
514 
515 void ShenandoahControlThread::handle_alloc_failure(size_t words) {
516   ShenandoahHeap* heap = ShenandoahHeap::heap();
517 
518   assert(current()-&gt;is_Java_thread(), &quot;expect Java thread here&quot;);
519 
520   if (try_set_alloc_failure_gc()) {
521     // Only report the first allocation failure
522     log_info(gc)(&quot;Failed to allocate &quot; SIZE_FORMAT &quot;%s&quot;,
523                  byte_size_in_proper_unit(words * HeapWordSize), proper_unit_for_byte_size(words * HeapWordSize));
524 
525     // Now that alloc failure GC is scheduled, we can abort everything else
526     heap-&gt;cancel_gc(GCCause::_allocation_failure);
527   }
528 
<span class="line-modified">529   MonitorLockerEx ml(&amp;_alloc_failure_waiters_lock);</span>
530   while (is_alloc_failure_gc()) {
531     ml.wait();
532   }
533 }
534 
535 void ShenandoahControlThread::handle_alloc_failure_evac(size_t words) {
536   ShenandoahHeap* heap = ShenandoahHeap::heap();
537 
538   if (try_set_alloc_failure_gc()) {
539     // Only report the first allocation failure
540     log_info(gc)(&quot;Failed to allocate &quot; SIZE_FORMAT &quot;%s for evacuation&quot;,
541                  byte_size_in_proper_unit(words * HeapWordSize), proper_unit_for_byte_size(words * HeapWordSize));
542   }
543 
544   // Forcefully report allocation failure
545   heap-&gt;cancel_gc(GCCause::_shenandoah_allocation_failure_evac);
546 }
547 
548 void ShenandoahControlThread::notify_alloc_failure_waiters() {
549   _alloc_failure_gc.unset();
<span class="line-modified">550   MonitorLockerEx ml(&amp;_alloc_failure_waiters_lock);</span>
551   ml.notify_all();
552 }
553 
554 bool ShenandoahControlThread::try_set_alloc_failure_gc() {
555   return _alloc_failure_gc.try_set();
556 }
557 
558 bool ShenandoahControlThread::is_alloc_failure_gc() {
559   return _alloc_failure_gc.is_set();
560 }
561 
562 void ShenandoahControlThread::notify_gc_waiters() {
563   _gc_requested.unset();
<span class="line-modified">564   MonitorLockerEx ml(&amp;_gc_waiters_lock);</span>
565   ml.notify_all();
566 }
567 
568 void ShenandoahControlThread::handle_counters_update() {
569   if (_do_counters_update.is_set()) {
570     _do_counters_update.unset();
571     ShenandoahHeap::heap()-&gt;monitoring_support()-&gt;update_counters();
572   }
573 }
574 
575 void ShenandoahControlThread::handle_force_counters_update() {
576   if (_force_counters_update.is_set()) {
577     _do_counters_update.unset(); // reset these too, we do update now!
578     ShenandoahHeap::heap()-&gt;monitoring_support()-&gt;update_counters();
579   }
580 }
581 
582 void ShenandoahControlThread::notify_heap_changed() {
583   // This is called from allocation path, and thus should be fast.
584 
585   // Update monitoring counters when we took a new region. This amortizes the
586   // update costs on slow path.
587   if (_do_counters_update.is_unset()) {
588     _do_counters_update.set();
589   }
590   // Notify that something had changed.
591   if (_heap_changed.is_unset()) {
592     _heap_changed.set();
593   }
594 }
595 
596 void ShenandoahControlThread::pacing_notify_alloc(size_t words) {
597   assert(ShenandoahPacing, &quot;should only call when pacing is enabled&quot;);
<span class="line-modified">598   Atomic::add(words, &amp;_allocs_seen);</span>
599 }
600 
601 void ShenandoahControlThread::set_forced_counters_update(bool value) {
602   _force_counters_update.set_cond(value);
603 }
604 
605 void ShenandoahControlThread::print() const {
606   print_on(tty);
607 }
608 
609 void ShenandoahControlThread::print_on(outputStream* st) const {
610   st-&gt;print(&quot;Shenandoah Concurrent Thread&quot;);
611   Thread::print_on(st);
612   st-&gt;cr();
613 }
614 
615 void ShenandoahControlThread::start() {
616   create_and_start();
617 }
618 
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2013, 2019, Red Hat, Inc. All rights reserved.</span>
<span class="line-added">  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.</span>
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 
 27 #include &quot;gc/shenandoah/shenandoahConcurrentMark.inline.hpp&quot;
 28 #include &quot;gc/shenandoah/shenandoahCollectorPolicy.hpp&quot;
 29 #include &quot;gc/shenandoah/shenandoahFreeSet.hpp&quot;
 30 #include &quot;gc/shenandoah/shenandoahPhaseTimings.hpp&quot;
 31 #include &quot;gc/shenandoah/shenandoahHeap.inline.hpp&quot;
 32 #include &quot;gc/shenandoah/shenandoahHeuristics.hpp&quot;
 33 #include &quot;gc/shenandoah/shenandoahMonitoringSupport.hpp&quot;
 34 #include &quot;gc/shenandoah/shenandoahControlThread.hpp&quot;
 35 #include &quot;gc/shenandoah/shenandoahTraversalGC.hpp&quot;
 36 #include &quot;gc/shenandoah/shenandoahUtils.hpp&quot;
 37 #include &quot;gc/shenandoah/shenandoahVMOperations.hpp&quot;
 38 #include &quot;gc/shenandoah/shenandoahWorkerPolicy.hpp&quot;
 39 #include &quot;memory/iterator.hpp&quot;
 40 #include &quot;memory/universe.hpp&quot;
<span class="line-added"> 41 #include &quot;runtime/atomic.hpp&quot;</span>
 42 
 43 ShenandoahControlThread::ShenandoahControlThread() :
 44   ConcurrentGCThread(),
 45   _alloc_failure_waiters_lock(Mutex::leaf, &quot;ShenandoahAllocFailureGC_lock&quot;, true, Monitor::_safepoint_check_always),
 46   _gc_waiters_lock(Mutex::leaf, &quot;ShenandoahRequestedGC_lock&quot;, true, Monitor::_safepoint_check_always),
 47   _periodic_task(this),
 48   _requested_gc_cause(GCCause::_no_cause_specified),
 49   _degen_point(ShenandoahHeap::_degenerated_outside_cycle),
 50   _allocs_seen(0) {
 51 
 52   create_and_start(ShenandoahCriticalControlThreadPriority ? CriticalPriority : NearMaxPriority);
 53   _periodic_task.enroll();
 54   _periodic_satb_flush_task.enroll();
 55 }
 56 
 57 ShenandoahControlThread::~ShenandoahControlThread() {
 58   // This is here so that super is called.
 59 }
 60 
 61 void ShenandoahPeriodicTask::task() {
 62   _thread-&gt;handle_force_counters_update();
 63   _thread-&gt;handle_counters_update();
 64 }
 65 
 66 void ShenandoahPeriodicSATBFlushTask::task() {
 67   ShenandoahHeap::heap()-&gt;force_satb_flush_all_threads();
 68 }
 69 
 70 void ShenandoahControlThread::run_service() {
 71   ShenandoahHeap* heap = ShenandoahHeap::heap();
 72 
<span class="line-added"> 73   GCMode default_mode = heap-&gt;is_traversal_mode() ?</span>
<span class="line-added"> 74                            concurrent_traversal : concurrent_normal;</span>
<span class="line-added"> 75   GCCause::Cause default_cause = heap-&gt;is_traversal_mode() ?</span>
<span class="line-added"> 76                            GCCause::_shenandoah_traversal_gc : GCCause::_shenandoah_concurrent_gc;</span>
 77   int sleep = ShenandoahControlIntervalMin;
 78 
 79   double last_shrink_time = os::elapsedTime();
 80   double last_sleep_adjust_time = os::elapsedTime();
 81 
 82   // Shrink period avoids constantly polling regions for shrinking.
 83   // Having a period 10x lower than the delay would mean we hit the
 84   // shrinking with lag of less than 1/10-th of true delay.
 85   // ShenandoahUncommitDelay is in msecs, but shrink_period is in seconds.
 86   double shrink_period = (double)ShenandoahUncommitDelay / 1000 / 10;
 87 
 88   ShenandoahCollectorPolicy* policy = heap-&gt;shenandoah_policy();
 89   ShenandoahHeuristics* heuristics = heap-&gt;heuristics();
 90   while (!in_graceful_shutdown() &amp;&amp; !should_terminate()) {
 91     // Figure out if we have pending requests.
 92     bool alloc_failure_pending = _alloc_failure_gc.is_set();
 93     bool explicit_gc_requested = _gc_requested.is_set() &amp;&amp;  is_explicit_gc(_requested_gc_cause);
 94     bool implicit_gc_requested = _gc_requested.is_set() &amp;&amp; !is_explicit_gc(_requested_gc_cause);
 95 
 96     // This control loop iteration have seen this much allocations.
<span class="line-modified"> 97     size_t allocs_seen = Atomic::xchg(&amp;_allocs_seen, (size_t)0);</span>
 98 
 99     // Choose which GC mode to run in. The block below should select a single mode.
100     GCMode mode = none;
101     GCCause::Cause cause = GCCause::_last_gc_cause;
102     ShenandoahHeap::ShenandoahDegenPoint degen_point = ShenandoahHeap::_degenerated_unset;
103 
104     if (alloc_failure_pending) {
105       // Allocation failure takes precedence: we have to deal with it first thing
106       log_info(gc)(&quot;Trigger: Handle Allocation Failure&quot;);
107 
108       cause = GCCause::_allocation_failure;
109 
110       // Consume the degen point, and seed it with default value
111       degen_point = _degen_point;
112       _degen_point = ShenandoahHeap::_degenerated_outside_cycle;
113 
114       if (ShenandoahDegeneratedGC &amp;&amp; heuristics-&gt;should_degenerate_cycle()) {
115         heuristics-&gt;record_allocation_failure_gc();
116         policy-&gt;record_alloc_failure_to_degenerated(degen_point);
117         mode = stw_degenerated;
118       } else {
119         heuristics-&gt;record_allocation_failure_gc();
120         policy-&gt;record_alloc_failure_to_full();
121         mode = stw_full;
122       }
123 
124     } else if (explicit_gc_requested) {
125       cause = _requested_gc_cause;
126       log_info(gc)(&quot;Trigger: Explicit GC request (%s)&quot;, GCCause::to_string(cause));
127 
128       heuristics-&gt;record_requested_gc();
129 
130       if (ExplicitGCInvokesConcurrent) {
131         policy-&gt;record_explicit_to_concurrent();
<span class="line-modified">132         mode = default_mode;</span>




133         // Unload and clean up everything
134         heap-&gt;set_process_references(heuristics-&gt;can_process_references());
135         heap-&gt;set_unload_classes(heuristics-&gt;can_unload_classes());
136       } else {
137         policy-&gt;record_explicit_to_full();
138         mode = stw_full;
139       }
140     } else if (implicit_gc_requested) {
141       cause = _requested_gc_cause;
142       log_info(gc)(&quot;Trigger: Implicit GC request (%s)&quot;, GCCause::to_string(cause));
143 
144       heuristics-&gt;record_requested_gc();
145 
146       if (ShenandoahImplicitGCInvokesConcurrent) {
147         policy-&gt;record_implicit_to_concurrent();
<span class="line-modified">148         mode = default_mode;</span>




149 
150         // Unload and clean up everything
151         heap-&gt;set_process_references(heuristics-&gt;can_process_references());
152         heap-&gt;set_unload_classes(heuristics-&gt;can_unload_classes());
153       } else {
154         policy-&gt;record_implicit_to_full();
155         mode = stw_full;
156       }
157     } else {
158       // Potential normal cycle: ask heuristics if it wants to act
<span class="line-modified">159       if (heuristics-&gt;should_start_gc()) {</span>
<span class="line-modified">160         mode = default_mode;</span>
<span class="line-modified">161         cause = default_cause;</span>



162       }
163 
164       // Ask policy if this cycle wants to process references or unload classes
165       heap-&gt;set_process_references(heuristics-&gt;should_process_references());
166       heap-&gt;set_unload_classes(heuristics-&gt;should_unload_classes());
167     }
168 
169     // Blow all soft references on this cycle, if handling allocation failure,
170     // or we are requested to do so unconditionally.
171     if (alloc_failure_pending || ShenandoahAlwaysClearSoftRefs) {
172       heap-&gt;soft_ref_policy()-&gt;set_should_clear_all_soft_refs(true);
173     }
174 
175     bool gc_requested = (mode != none);
176     assert (!gc_requested || cause != GCCause::_last_gc_cause, &quot;GC cause should be set&quot;);
177 
178     if (gc_requested) {
179       heap-&gt;reset_bytes_allocated_since_gc_start();
180 
181       // If GC was requested, we are sampling the counters even without actual triggers
</pre>
<hr />
<pre>
355   ShenandoahGCSession session(cause);
356 
357   TraceCollectorStats tcs(heap-&gt;monitoring_support()-&gt;concurrent_collection_counters());
358 
359   // Reset for upcoming marking
360   heap-&gt;entry_reset();
361 
362   // Start initial mark under STW
363   heap-&gt;vmop_entry_init_mark();
364 
365   // Continue concurrent mark
366   heap-&gt;entry_mark();
367   if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_mark)) return;
368 
369   // If not cancelled, can try to concurrently pre-clean
370   heap-&gt;entry_preclean();
371 
372   // Complete marking under STW, and start evacuation
373   heap-&gt;vmop_entry_final_mark();
374 
<span class="line-added">375   // Evacuate concurrent roots</span>
<span class="line-added">376   heap-&gt;entry_roots();</span>
<span class="line-added">377 </span>
378   // Final mark might have reclaimed some immediate garbage, kick cleanup to reclaim
379   // the space. This would be the last action if there is nothing to evacuate.
380   heap-&gt;entry_cleanup();
381 
382   {
383     ShenandoahHeapLocker locker(heap-&gt;lock());
384     heap-&gt;free_set()-&gt;log_status();
385   }
386 
387   // Continue the cycle with evacuation and optional update-refs.
388   // This may be skipped if there is nothing to evacuate.
389   // If so, evac_in_progress would be unset by collection set preparation code.
390   if (heap-&gt;is_evacuation_in_progress()) {
391     // Concurrently evacuate
392     heap-&gt;entry_evac();
393     if (check_cancellation_or_degen(ShenandoahHeap::_degenerated_evac)) return;
394 
395     // Perform update-refs phase, if required. This phase can be skipped if heuristics
396     // decides to piggy-back the update-refs on the next marking cycle. On either path,
397     // we need to turn off evacuation: either in init-update-refs, or in final-evac.
</pre>
<hr />
<pre>
443   heap-&gt;heuristics()-&gt;record_success_full();
444   heap-&gt;shenandoah_policy()-&gt;record_success_full();
445 }
446 
447 void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahHeap::ShenandoahDegenPoint point) {
448   assert (point != ShenandoahHeap::_degenerated_unset, &quot;Degenerated point should be set&quot;);
449 
450   GCIdMark gc_id_mark;
451   ShenandoahGCSession session(cause);
452 
453   ShenandoahHeap* heap = ShenandoahHeap::heap();
454   heap-&gt;vmop_degenerated(point);
455 
456   heap-&gt;heuristics()-&gt;record_success_degenerated();
457   heap-&gt;shenandoah_policy()-&gt;record_success_degenerated();
458 }
459 
460 void ShenandoahControlThread::service_uncommit(double shrink_before) {
461   ShenandoahHeap* heap = ShenandoahHeap::heap();
462 
<span class="line-modified">463   // Determine if there is work to do. This avoids taking heap lock if there is</span>
<span class="line-modified">464   // no work available, avoids spamming logs with superfluous logging messages,</span>
<span class="line-modified">465   // and minimises the amount of work while locks are taken.</span>
<span class="line-added">466 </span>
<span class="line-added">467   if (heap-&gt;committed() &lt;= heap-&gt;min_capacity()) return;</span>
468 
469   bool has_work = false;
470   for (size_t i = 0; i &lt; heap-&gt;num_regions(); i++) {
471     ShenandoahHeapRegion *r = heap-&gt;get_region(i);
472     if (r-&gt;is_empty_committed() &amp;&amp; (r-&gt;empty_time() &lt; shrink_before)) {
473       has_work = true;
474       break;
475     }
476   }
477 
478   if (has_work) {
479     heap-&gt;entry_uncommit(shrink_before);
480   }
481 }
482 
483 bool ShenandoahControlThread::is_explicit_gc(GCCause::Cause cause) const {
484   return GCCause::is_user_requested_gc(cause) ||
485          GCCause::is_serviceability_requested_gc(cause);
486 }
487 
</pre>
<hr />
<pre>
489   assert(GCCause::is_user_requested_gc(cause) ||
490          GCCause::is_serviceability_requested_gc(cause) ||
491          cause == GCCause::_metadata_GC_clear_soft_refs ||
492          cause == GCCause::_full_gc_alot ||
493          cause == GCCause::_wb_full_gc ||
494          cause == GCCause::_scavenge_alot,
495          &quot;only requested GCs here&quot;);
496 
497   if (is_explicit_gc(cause)) {
498     if (!DisableExplicitGC) {
499       handle_requested_gc(cause);
500     }
501   } else {
502     handle_requested_gc(cause);
503   }
504 }
505 
506 void ShenandoahControlThread::handle_requested_gc(GCCause::Cause cause) {
507   _requested_gc_cause = cause;
508   _gc_requested.set();
<span class="line-modified">509   MonitorLocker ml(&amp;_gc_waiters_lock);</span>
510   while (_gc_requested.is_set()) {
511     ml.wait();
512   }
513 }
514 
515 void ShenandoahControlThread::handle_alloc_failure(size_t words) {
516   ShenandoahHeap* heap = ShenandoahHeap::heap();
517 
518   assert(current()-&gt;is_Java_thread(), &quot;expect Java thread here&quot;);
519 
520   if (try_set_alloc_failure_gc()) {
521     // Only report the first allocation failure
522     log_info(gc)(&quot;Failed to allocate &quot; SIZE_FORMAT &quot;%s&quot;,
523                  byte_size_in_proper_unit(words * HeapWordSize), proper_unit_for_byte_size(words * HeapWordSize));
524 
525     // Now that alloc failure GC is scheduled, we can abort everything else
526     heap-&gt;cancel_gc(GCCause::_allocation_failure);
527   }
528 
<span class="line-modified">529   MonitorLocker ml(&amp;_alloc_failure_waiters_lock);</span>
530   while (is_alloc_failure_gc()) {
531     ml.wait();
532   }
533 }
534 
535 void ShenandoahControlThread::handle_alloc_failure_evac(size_t words) {
536   ShenandoahHeap* heap = ShenandoahHeap::heap();
537 
538   if (try_set_alloc_failure_gc()) {
539     // Only report the first allocation failure
540     log_info(gc)(&quot;Failed to allocate &quot; SIZE_FORMAT &quot;%s for evacuation&quot;,
541                  byte_size_in_proper_unit(words * HeapWordSize), proper_unit_for_byte_size(words * HeapWordSize));
542   }
543 
544   // Forcefully report allocation failure
545   heap-&gt;cancel_gc(GCCause::_shenandoah_allocation_failure_evac);
546 }
547 
548 void ShenandoahControlThread::notify_alloc_failure_waiters() {
549   _alloc_failure_gc.unset();
<span class="line-modified">550   MonitorLocker ml(&amp;_alloc_failure_waiters_lock);</span>
551   ml.notify_all();
552 }
553 
554 bool ShenandoahControlThread::try_set_alloc_failure_gc() {
555   return _alloc_failure_gc.try_set();
556 }
557 
558 bool ShenandoahControlThread::is_alloc_failure_gc() {
559   return _alloc_failure_gc.is_set();
560 }
561 
562 void ShenandoahControlThread::notify_gc_waiters() {
563   _gc_requested.unset();
<span class="line-modified">564   MonitorLocker ml(&amp;_gc_waiters_lock);</span>
565   ml.notify_all();
566 }
567 
568 void ShenandoahControlThread::handle_counters_update() {
569   if (_do_counters_update.is_set()) {
570     _do_counters_update.unset();
571     ShenandoahHeap::heap()-&gt;monitoring_support()-&gt;update_counters();
572   }
573 }
574 
575 void ShenandoahControlThread::handle_force_counters_update() {
576   if (_force_counters_update.is_set()) {
577     _do_counters_update.unset(); // reset these too, we do update now!
578     ShenandoahHeap::heap()-&gt;monitoring_support()-&gt;update_counters();
579   }
580 }
581 
582 void ShenandoahControlThread::notify_heap_changed() {
583   // This is called from allocation path, and thus should be fast.
584 
585   // Update monitoring counters when we took a new region. This amortizes the
586   // update costs on slow path.
587   if (_do_counters_update.is_unset()) {
588     _do_counters_update.set();
589   }
590   // Notify that something had changed.
591   if (_heap_changed.is_unset()) {
592     _heap_changed.set();
593   }
594 }
595 
596 void ShenandoahControlThread::pacing_notify_alloc(size_t words) {
597   assert(ShenandoahPacing, &quot;should only call when pacing is enabled&quot;);
<span class="line-modified">598   Atomic::add(&amp;_allocs_seen, words);</span>
599 }
600 
601 void ShenandoahControlThread::set_forced_counters_update(bool value) {
602   _force_counters_update.set_cond(value);
603 }
604 
605 void ShenandoahControlThread::print() const {
606   print_on(tty);
607 }
608 
609 void ShenandoahControlThread::print_on(outputStream* st) const {
610   st-&gt;print(&quot;Shenandoah Concurrent Thread&quot;);
611   Thread::print_on(st);
612   st-&gt;cr();
613 }
614 
615 void ShenandoahControlThread::start() {
616   create_and_start();
617 }
618 
</pre>
</td>
</tr>
</table>
<center><a href="shenandoahConcurrentMark.inline.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="shenandoahControlThread.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>