<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/share/gc/g1/heapRegion.inline.hpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="heapRegion.hpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="heapRegionManager.cpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/heapRegion.inline.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 26,99 ***</span>
  #define SHARE_GC_G1_HEAPREGION_INLINE_HPP
  
  #include &quot;gc/g1/g1BlockOffsetTable.inline.hpp&quot;
  #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
  #include &quot;gc/g1/g1ConcurrentMarkBitMap.inline.hpp&quot;
  #include &quot;gc/g1/heapRegion.hpp&quot;
<span class="line-removed">- #include &quot;gc/shared/space.hpp&quot;</span>
  #include &quot;oops/oop.inline.hpp&quot;
  #include &quot;runtime/atomic.hpp&quot;
  #include &quot;runtime/prefetch.inline.hpp&quot;
  #include &quot;utilities/align.hpp&quot;
  
<span class="line-modified">! inline HeapWord* G1ContiguousSpace::allocate_impl(size_t min_word_size,</span>
<span class="line-modified">!                                                   size_t desired_word_size,</span>
<span class="line-modified">!                                                   size_t* actual_size) {</span>
    HeapWord* obj = top();
    size_t available = pointer_delta(end(), obj);
    size_t want_to_allocate = MIN2(available, desired_word_size);
    if (want_to_allocate &gt;= min_word_size) {
      HeapWord* new_top = obj + want_to_allocate;
      set_top(new_top);
<span class="line-modified">!     assert(is_aligned(obj) &amp;&amp; is_aligned(new_top), &quot;checking alignment&quot;);</span>
      *actual_size = want_to_allocate;
      return obj;
    } else {
      return NULL;
    }
  }
  
<span class="line-modified">! inline HeapWord* G1ContiguousSpace::par_allocate_impl(size_t min_word_size,</span>
<span class="line-modified">!                                                       size_t desired_word_size,</span>
<span class="line-modified">!                                                       size_t* actual_size) {</span>
    do {
      HeapWord* obj = top();
      size_t available = pointer_delta(end(), obj);
      size_t want_to_allocate = MIN2(available, desired_word_size);
      if (want_to_allocate &gt;= min_word_size) {
        HeapWord* new_top = obj + want_to_allocate;
<span class="line-modified">!       HeapWord* result = Atomic::cmpxchg(new_top, top_addr(), obj);</span>
        // result can be one of two:
        //  the old top value: the exchange succeeded
        //  otherwise: the new value of the top is returned.
        if (result == obj) {
<span class="line-modified">!         assert(is_aligned(obj) &amp;&amp; is_aligned(new_top), &quot;checking alignment&quot;);</span>
          *actual_size = want_to_allocate;
          return obj;
        }
      } else {
        return NULL;
      }
    } while (true);
  }
  
<span class="line-modified">! inline HeapWord* G1ContiguousSpace::allocate(size_t min_word_size,</span>
<span class="line-modified">!                                              size_t desired_word_size,</span>
<span class="line-modified">!                                              size_t* actual_size) {</span>
    HeapWord* res = allocate_impl(min_word_size, desired_word_size, actual_size);
    if (res != NULL) {
      _bot_part.alloc_block(res, *actual_size);
    }
    return res;
  }
  
<span class="line-modified">! inline HeapWord* G1ContiguousSpace::allocate(size_t word_size) {</span>
    size_t temp;
    return allocate(word_size, word_size, &amp;temp);
  }
  
<span class="line-modified">! inline HeapWord* G1ContiguousSpace::par_allocate(size_t word_size) {</span>
    size_t temp;
    return par_allocate(word_size, word_size, &amp;temp);
  }
  
  // Because of the requirement of keeping &quot;_offsets&quot; up to date with the
  // allocations, we sequentialize these with a lock.  Therefore, best if
  // this is used for larger LAB allocations only.
<span class="line-modified">! inline HeapWord* G1ContiguousSpace::par_allocate(size_t min_word_size,</span>
<span class="line-modified">!                                                  size_t desired_word_size,</span>
<span class="line-modified">!                                                  size_t* actual_size) {</span>
    MutexLocker x(&amp;_par_alloc_lock);
    return allocate(min_word_size, desired_word_size, actual_size);
  }
  
<span class="line-modified">! inline HeapWord* G1ContiguousSpace::block_start(const void* p) {</span>
    return _bot_part.block_start(p);
  }
  
<span class="line-modified">! inline HeapWord*</span>
<span class="line-removed">- G1ContiguousSpace::block_start_const(const void* p) const {</span>
    return _bot_part.block_start_const(p);
  }
  
  inline bool HeapRegion::is_obj_dead_with_size(const oop obj, const G1CMBitMap* const prev_bitmap, size_t* size) const {
<span class="line-modified">!   HeapWord* addr = (HeapWord*) obj;</span>
  
    assert(addr &lt; top(), &quot;must be&quot;);
    assert(!is_closed_archive(),
           &quot;Closed archive regions should not have references into other regions&quot;);
    assert(!is_humongous(), &quot;Humongous objects not handled here&quot;);
<span class="line-new-header">--- 26,99 ---</span>
  #define SHARE_GC_G1_HEAPREGION_INLINE_HPP
  
  #include &quot;gc/g1/g1BlockOffsetTable.inline.hpp&quot;
  #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
  #include &quot;gc/g1/g1ConcurrentMarkBitMap.inline.hpp&quot;
<span class="line-added">+ #include &quot;gc/g1/g1Predictions.hpp&quot;</span>
  #include &quot;gc/g1/heapRegion.hpp&quot;
  #include &quot;oops/oop.inline.hpp&quot;
  #include &quot;runtime/atomic.hpp&quot;
  #include &quot;runtime/prefetch.inline.hpp&quot;
  #include &quot;utilities/align.hpp&quot;
<span class="line-added">+ #include &quot;utilities/globalDefinitions.hpp&quot;</span>
  
<span class="line-modified">! inline HeapWord* HeapRegion::allocate_impl(size_t min_word_size,</span>
<span class="line-modified">!                                            size_t desired_word_size,</span>
<span class="line-modified">!                                            size_t* actual_size) {</span>
    HeapWord* obj = top();
    size_t available = pointer_delta(end(), obj);
    size_t want_to_allocate = MIN2(available, desired_word_size);
    if (want_to_allocate &gt;= min_word_size) {
      HeapWord* new_top = obj + want_to_allocate;
      set_top(new_top);
<span class="line-modified">!     assert(is_object_aligned(obj) &amp;&amp; is_object_aligned(new_top), &quot;checking alignment&quot;);</span>
      *actual_size = want_to_allocate;
      return obj;
    } else {
      return NULL;
    }
  }
  
<span class="line-modified">! inline HeapWord* HeapRegion::par_allocate_impl(size_t min_word_size,</span>
<span class="line-modified">!                                                size_t desired_word_size,</span>
<span class="line-modified">!                                                size_t* actual_size) {</span>
    do {
      HeapWord* obj = top();
      size_t available = pointer_delta(end(), obj);
      size_t want_to_allocate = MIN2(available, desired_word_size);
      if (want_to_allocate &gt;= min_word_size) {
        HeapWord* new_top = obj + want_to_allocate;
<span class="line-modified">!       HeapWord* result = Atomic::cmpxchg(&amp;_top, obj, new_top);</span>
        // result can be one of two:
        //  the old top value: the exchange succeeded
        //  otherwise: the new value of the top is returned.
        if (result == obj) {
<span class="line-modified">!         assert(is_object_aligned(obj) &amp;&amp; is_object_aligned(new_top), &quot;checking alignment&quot;);</span>
          *actual_size = want_to_allocate;
          return obj;
        }
      } else {
        return NULL;
      }
    } while (true);
  }
  
<span class="line-modified">! inline HeapWord* HeapRegion::allocate(size_t min_word_size,</span>
<span class="line-modified">!                                       size_t desired_word_size,</span>
<span class="line-modified">!                                       size_t* actual_size) {</span>
    HeapWord* res = allocate_impl(min_word_size, desired_word_size, actual_size);
    if (res != NULL) {
      _bot_part.alloc_block(res, *actual_size);
    }
    return res;
  }
  
<span class="line-modified">! inline HeapWord* HeapRegion::allocate(size_t word_size) {</span>
    size_t temp;
    return allocate(word_size, word_size, &amp;temp);
  }
  
<span class="line-modified">! inline HeapWord* HeapRegion::par_allocate(size_t word_size) {</span>
    size_t temp;
    return par_allocate(word_size, word_size, &amp;temp);
  }
  
  // Because of the requirement of keeping &quot;_offsets&quot; up to date with the
  // allocations, we sequentialize these with a lock.  Therefore, best if
  // this is used for larger LAB allocations only.
<span class="line-modified">! inline HeapWord* HeapRegion::par_allocate(size_t min_word_size,</span>
<span class="line-modified">!                                           size_t desired_word_size,</span>
<span class="line-modified">!                                           size_t* actual_size) {</span>
    MutexLocker x(&amp;_par_alloc_lock);
    return allocate(min_word_size, desired_word_size, actual_size);
  }
  
<span class="line-modified">! inline HeapWord* HeapRegion::block_start(const void* p) {</span>
    return _bot_part.block_start(p);
  }
  
<span class="line-modified">! inline HeapWord* HeapRegion::block_start_const(const void* p) const {</span>
    return _bot_part.block_start_const(p);
  }
  
  inline bool HeapRegion::is_obj_dead_with_size(const oop obj, const G1CMBitMap* const prev_bitmap, size_t* size) const {
<span class="line-modified">!   HeapWord* addr = cast_from_oop&lt;HeapWord*&gt;(obj);</span>
  
    assert(addr &lt; top(), &quot;must be&quot;);
    assert(!is_closed_archive(),
           &quot;Closed archive regions should not have references into other regions&quot;);
    assert(!is_humongous(), &quot;Humongous objects not handled here&quot;);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 132,12 ***</span>
      *size = obj-&gt;size();
    }
    return obj_is_dead;
  }
  
<span class="line-modified">! inline bool</span>
<span class="line-removed">- HeapRegion::block_is_obj(const HeapWord* p) const {</span>
    G1CollectedHeap* g1h = G1CollectedHeap::heap();
  
    if (!this-&gt;is_in(p)) {
      assert(is_continues_humongous(), &quot;This case can only happen for humongous regions&quot;);
      return (p == humongous_start_region()-&gt;bottom());
<span class="line-new-header">--- 132,11 ---</span>
      *size = obj-&gt;size();
    }
    return obj_is_dead;
  }
  
<span class="line-modified">! inline bool HeapRegion::block_is_obj(const HeapWord* p) const {</span>
    G1CollectedHeap* g1h = G1CollectedHeap::heap();
  
    if (!this-&gt;is_in(p)) {
      assert(is_continues_humongous(), &quot;This case can only happen for humongous regions&quot;);
      return (p == humongous_start_region()-&gt;bottom());
</pre>
<hr />
<pre>
<span class="line-old-header">*** 164,11 ***</span>
  }
  
  inline bool HeapRegion::is_obj_dead(const oop obj, const G1CMBitMap* const prev_bitmap) const {
    assert(is_in_reserved(obj), &quot;Object &quot; PTR_FORMAT &quot; must be in region&quot;, p2i(obj));
    return !obj_allocated_since_prev_marking(obj) &amp;&amp;
<span class="line-modified">!          !prev_bitmap-&gt;is_marked((HeapWord*)obj) &amp;&amp;</span>
           !is_open_archive();
  }
  
  inline size_t HeapRegion::block_size(const HeapWord *addr) const {
    if (addr == top()) {
<span class="line-new-header">--- 163,11 ---</span>
  }
  
  inline bool HeapRegion::is_obj_dead(const oop obj, const G1CMBitMap* const prev_bitmap) const {
    assert(is_in_reserved(obj), &quot;Object &quot; PTR_FORMAT &quot; must be in region&quot;, p2i(obj));
    return !obj_allocated_since_prev_marking(obj) &amp;&amp;
<span class="line-modified">!          !prev_bitmap-&gt;is_marked(obj) &amp;&amp;</span>
           !is_open_archive();
  }
  
  inline size_t HeapRegion::block_size(const HeapWord *addr) const {
    if (addr == top()) {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 183,11 ***</span>
  }
  
  inline void HeapRegion::complete_compaction() {
    // Reset space and bot after compaction is complete if needed.
    reset_after_compaction();
<span class="line-modified">!   if (used_region().is_empty()) {</span>
      reset_bot();
    }
  
    // After a compaction the mark bitmap is invalid, so we must
    // treat all objects as being inside the unmarked area.
<span class="line-new-header">--- 182,11 ---</span>
  }
  
  inline void HeapRegion::complete_compaction() {
    // Reset space and bot after compaction is complete if needed.
    reset_after_compaction();
<span class="line-modified">!   if (is_empty()) {</span>
      reset_bot();
    }
  
    // After a compaction the mark bitmap is invalid, so we must
    // treat all objects as being inside the unmarked area.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 200,11 ***</span>
    }
  }
  
  template&lt;typename ApplyToMarkedClosure&gt;
  inline void HeapRegion::apply_to_marked_objects(G1CMBitMap* bitmap, ApplyToMarkedClosure* closure) {
<span class="line-modified">!   HeapWord* limit = scan_limit();</span>
    HeapWord* next_addr = bottom();
  
    while (next_addr &lt; limit) {
      Prefetch::write(next_addr, PrefetchScanIntervalInBytes);
      // This explicit is_marked check is a way to avoid
<span class="line-new-header">--- 199,11 ---</span>
    }
  }
  
  template&lt;typename ApplyToMarkedClosure&gt;
  inline void HeapRegion::apply_to_marked_objects(G1CMBitMap* bitmap, ApplyToMarkedClosure* closure) {
<span class="line-modified">!   HeapWord* limit = top();</span>
    HeapWord* next_addr = bottom();
  
    while (next_addr &lt; limit) {
      Prefetch::write(next_addr, PrefetchScanIntervalInBytes);
      // This explicit is_marked check is a way to avoid
</pre>
<hr />
<pre>
<span class="line-old-header">*** 255,13 ***</span>
  inline bool HeapRegion::in_collection_set() const {
    return G1CollectedHeap::heap()-&gt;is_in_cset(this);
  }
  
  template &lt;class Closure, bool is_gc_active&gt;
<span class="line-modified">! bool HeapRegion::do_oops_on_card_in_humongous(MemRegion mr,</span>
<span class="line-modified">!                                               Closure* cl,</span>
<span class="line-modified">!                                               G1CollectedHeap* g1h) {</span>
    assert(is_humongous(), &quot;precondition&quot;);
    HeapRegion* sr = humongous_start_region();
    oop obj = oop(sr-&gt;bottom());
  
    // If concurrent and klass_or_null is NULL, then space has been
<span class="line-new-header">--- 254,13 ---</span>
  inline bool HeapRegion::in_collection_set() const {
    return G1CollectedHeap::heap()-&gt;is_in_cset(this);
  }
  
  template &lt;class Closure, bool is_gc_active&gt;
<span class="line-modified">! HeapWord* HeapRegion::do_oops_on_memregion_in_humongous(MemRegion mr,</span>
<span class="line-modified">!                                                         Closure* cl,</span>
<span class="line-modified">!                                                         G1CollectedHeap* g1h) {</span>
    assert(is_humongous(), &quot;precondition&quot;);
    HeapRegion* sr = humongous_start_region();
    oop obj = oop(sr-&gt;bottom());
  
    // If concurrent and klass_or_null is NULL, then space has been
</pre>
<hr />
<pre>
<span class="line-old-header">*** 269,45 ***</span>
    // the klass.  That can only happen if the card is stale.  However,
    // we&#39;ve already set the card clean, so we must return failure,
    // since the allocating thread could have performed a write to the
    // card that might be missed otherwise.
    if (!is_gc_active &amp;&amp; (obj-&gt;klass_or_null_acquire() == NULL)) {
<span class="line-modified">!     return false;</span>
    }
  
    // We have a well-formed humongous object at the start of sr.
    // Only filler objects follow a humongous object in the containing
    // regions, and we can ignore those.  So only process the one
    // humongous object.
<span class="line-modified">!   if (!g1h-&gt;is_obj_dead(obj, sr)) {</span>
<span class="line-modified">!     if (obj-&gt;is_objArray() || (sr-&gt;bottom() &lt; mr.start())) {</span>
<span class="line-modified">!       // objArrays are always marked precisely, so limit processing</span>
<span class="line-modified">!       // with mr.  Non-objArrays might be precisely marked, and since</span>
<span class="line-modified">!       // it&#39;s humongous it&#39;s worthwhile avoiding full processing.</span>
<span class="line-modified">!       // However, the card could be stale and only cover filler</span>
<span class="line-modified">!       // objects.  That should be rare, so not worth checking for;</span>
<span class="line-modified">!       // instead let it fall out from the bounded iteration.</span>
<span class="line-modified">!       obj-&gt;oop_iterate(cl, mr);</span>
<span class="line-modified">!     } else {</span>
<span class="line-modified">!       // If obj is not an objArray and mr contains the start of the</span>
<span class="line-modified">!       // obj, then this could be an imprecise mark, and we need to</span>
<span class="line-modified">!       // process the entire object.</span>
<span class="line-modified">!       obj-&gt;oop_iterate(cl);</span>
<span class="line-modified">!     }</span>
    }
<span class="line-removed">-   return true;</span>
  }
  
  template &lt;bool is_gc_active, class Closure&gt;
<span class="line-modified">! bool HeapRegion::oops_on_card_seq_iterate_careful(MemRegion mr,</span>
<span class="line-modified">!                                                   Closure* cl) {</span>
    assert(MemRegion(bottom(), end()).contains(mr), &quot;Card region not in heap region&quot;);
    G1CollectedHeap* g1h = G1CollectedHeap::heap();
  
    // Special handling for humongous regions.
    if (is_humongous()) {
<span class="line-modified">!     return do_oops_on_card_in_humongous&lt;Closure, is_gc_active&gt;(mr, cl, g1h);</span>
    }
    assert(is_old() || is_archive(), &quot;Wrongly trying to iterate over region %u type %s&quot;, _hrm_index, get_type_str());
  
    // Because mr has been trimmed to what&#39;s been allocated in this
    // region, the parts of the heap that are examined here are always
<span class="line-new-header">--- 268,52 ---</span>
    // the klass.  That can only happen if the card is stale.  However,
    // we&#39;ve already set the card clean, so we must return failure,
    // since the allocating thread could have performed a write to the
    // card that might be missed otherwise.
    if (!is_gc_active &amp;&amp; (obj-&gt;klass_or_null_acquire() == NULL)) {
<span class="line-modified">!     return NULL;</span>
    }
  
    // We have a well-formed humongous object at the start of sr.
    // Only filler objects follow a humongous object in the containing
    // regions, and we can ignore those.  So only process the one
    // humongous object.
<span class="line-modified">!   if (g1h-&gt;is_obj_dead(obj, sr)) {</span>
<span class="line-modified">!     // The object is dead. There can be no other object in this region, so return</span>
<span class="line-modified">!     // the end of that region.</span>
<span class="line-modified">!     return end();</span>
<span class="line-modified">!   }</span>
<span class="line-modified">!   if (obj-&gt;is_objArray() || (sr-&gt;bottom() &lt; mr.start())) {</span>
<span class="line-modified">!     // objArrays are always marked precisely, so limit processing</span>
<span class="line-modified">!     // with mr.  Non-objArrays might be precisely marked, and since</span>
<span class="line-modified">!     // it&#39;s humongous it&#39;s worthwhile avoiding full processing.</span>
<span class="line-modified">!     // However, the card could be stale and only cover filler</span>
<span class="line-modified">!     // objects.  That should be rare, so not worth checking for;</span>
<span class="line-modified">!     // instead let it fall out from the bounded iteration.</span>
<span class="line-modified">!     obj-&gt;oop_iterate(cl, mr);</span>
<span class="line-modified">!     return mr.end();</span>
<span class="line-modified">!   } else {</span>
<span class="line-added">+     // If obj is not an objArray and mr contains the start of the</span>
<span class="line-added">+     // obj, then this could be an imprecise mark, and we need to</span>
<span class="line-added">+     // process the entire object.</span>
<span class="line-added">+     int size = obj-&gt;oop_iterate_size(cl);</span>
<span class="line-added">+     // We have scanned to the end of the object, but since there can be no objects</span>
<span class="line-added">+     // after this humongous object in the region, we can return the end of the</span>
<span class="line-added">+     // region if it is greater.</span>
<span class="line-added">+     return MAX2(cast_from_oop&lt;HeapWord*&gt;(obj) + size, mr.end());</span>
    }
  }
  
  template &lt;bool is_gc_active, class Closure&gt;
<span class="line-modified">! HeapWord* HeapRegion::oops_on_memregion_seq_iterate_careful(MemRegion mr,</span>
<span class="line-modified">!                                                             Closure* cl) {</span>
    assert(MemRegion(bottom(), end()).contains(mr), &quot;Card region not in heap region&quot;);
    G1CollectedHeap* g1h = G1CollectedHeap::heap();
  
    // Special handling for humongous regions.
    if (is_humongous()) {
<span class="line-modified">!     return do_oops_on_memregion_in_humongous&lt;Closure, is_gc_active&gt;(mr, cl, g1h);</span>
    }
    assert(is_old() || is_archive(), &quot;Wrongly trying to iterate over region %u type %s&quot;, _hrm_index, get_type_str());
  
    // Because mr has been trimmed to what&#39;s been allocated in this
    // region, the parts of the heap that are examined here are always
</pre>
<hr />
<pre>
<span class="line-old-header">*** 332,34 ***</span>
             &quot;start: &quot; PTR_FORMAT &quot;, next: &quot; PTR_FORMAT, p2i(start), p2i(next));
    }
  #endif
  
    const G1CMBitMap* const bitmap = g1h-&gt;concurrent_mark()-&gt;prev_mark_bitmap();
<span class="line-modified">!   do {</span>
      oop obj = oop(cur);
      assert(oopDesc::is_oop(obj, true), &quot;Not an oop at &quot; PTR_FORMAT, p2i(cur));
      assert(obj-&gt;klass_or_null() != NULL,
             &quot;Unparsable heap at &quot; PTR_FORMAT, p2i(cur));
  
      size_t size;
      bool is_dead = is_obj_dead_with_size(obj, bitmap, &amp;size);
  
      cur += size;
      if (!is_dead) {
        // Process live object&#39;s references.
  
        // Non-objArrays are usually marked imprecise at the object
        // start, in which case we need to iterate over them in full.
        // objArrays are precisely marked, but can still be iterated
        // over in full if completely covered.
<span class="line-modified">!       if (!obj-&gt;is_objArray() || (((HeapWord*)obj) &gt;= start &amp;&amp; cur &lt;= end)) {</span>
          obj-&gt;oop_iterate(cl);
        } else {
          obj-&gt;oop_iterate(cl, mr);
        }
      }
<span class="line-modified">!   } while (cur &lt; end);</span>
  
<span class="line-modified">!   return true;</span>
  }
  
  #endif // SHARE_GC_G1_HEAPREGION_INLINE_HPP
<span class="line-new-header">--- 338,84 ---</span>
             &quot;start: &quot; PTR_FORMAT &quot;, next: &quot; PTR_FORMAT, p2i(start), p2i(next));
    }
  #endif
  
    const G1CMBitMap* const bitmap = g1h-&gt;concurrent_mark()-&gt;prev_mark_bitmap();
<span class="line-modified">!   while (true) {</span>
      oop obj = oop(cur);
      assert(oopDesc::is_oop(obj, true), &quot;Not an oop at &quot; PTR_FORMAT, p2i(cur));
      assert(obj-&gt;klass_or_null() != NULL,
             &quot;Unparsable heap at &quot; PTR_FORMAT, p2i(cur));
  
      size_t size;
      bool is_dead = is_obj_dead_with_size(obj, bitmap, &amp;size);
<span class="line-added">+     bool is_precise = false;</span>
  
      cur += size;
      if (!is_dead) {
        // Process live object&#39;s references.
  
        // Non-objArrays are usually marked imprecise at the object
        // start, in which case we need to iterate over them in full.
        // objArrays are precisely marked, but can still be iterated
        // over in full if completely covered.
<span class="line-modified">!       if (!obj-&gt;is_objArray() || (cast_from_oop&lt;HeapWord*&gt;(obj) &gt;= start &amp;&amp; cur &lt;= end)) {</span>
          obj-&gt;oop_iterate(cl);
        } else {
          obj-&gt;oop_iterate(cl, mr);
<span class="line-added">+         is_precise = true;</span>
        }
      }
<span class="line-modified">!     if (cur &gt;= end) {</span>
<span class="line-added">+       return is_precise ? end : cur;</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ inline int HeapRegion::age_in_surv_rate_group() const {</span>
<span class="line-added">+   assert(has_surv_rate_group(), &quot;pre-condition&quot;);</span>
<span class="line-added">+   assert(has_valid_age_in_surv_rate(), &quot;pre-condition&quot;);</span>
<span class="line-added">+   return _surv_rate_group-&gt;age_in_group(_age_index);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ inline bool HeapRegion::has_valid_age_in_surv_rate() const {</span>
<span class="line-added">+   return G1SurvRateGroup::is_valid_age_index(_age_index);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ inline bool HeapRegion::has_surv_rate_group() const {</span>
<span class="line-added">+   return _surv_rate_group != NULL;</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ inline double HeapRegion::surv_rate_prediction(G1Predictions const&amp; predictor) const {</span>
<span class="line-added">+   assert(has_surv_rate_group(), &quot;pre-condition&quot;);</span>
<span class="line-added">+   return _surv_rate_group-&gt;surv_rate_pred(predictor, age_in_surv_rate_group());</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ inline void HeapRegion::install_surv_rate_group(G1SurvRateGroup* surv_rate_group) {</span>
<span class="line-added">+   assert(surv_rate_group != NULL, &quot;pre-condition&quot;);</span>
<span class="line-added">+   assert(!has_surv_rate_group(), &quot;pre-condition&quot;);</span>
<span class="line-added">+   assert(is_young(), &quot;pre-condition&quot;);</span>
<span class="line-added">+ </span>
<span class="line-added">+   _surv_rate_group = surv_rate_group;</span>
<span class="line-added">+   _age_index = surv_rate_group-&gt;next_age_index();</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ inline void HeapRegion::uninstall_surv_rate_group() {</span>
<span class="line-added">+   if (has_surv_rate_group()) {</span>
<span class="line-added">+     assert(has_valid_age_in_surv_rate(), &quot;pre-condition&quot;);</span>
<span class="line-added">+     assert(is_young(), &quot;pre-condition&quot;);</span>
<span class="line-added">+ </span>
<span class="line-added">+     _surv_rate_group = NULL;</span>
<span class="line-added">+     _age_index = G1SurvRateGroup::InvalidAgeIndex;</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     assert(!has_valid_age_in_surv_rate(), &quot;pre-condition&quot;);</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
  
<span class="line-modified">! inline void HeapRegion::record_surv_words_in_group(size_t words_survived) {</span>
<span class="line-added">+   assert(has_surv_rate_group(), &quot;pre-condition&quot;);</span>
<span class="line-added">+   assert(has_valid_age_in_surv_rate(), &quot;pre-condition&quot;);</span>
<span class="line-added">+   int age_in_group = age_in_surv_rate_group();</span>
<span class="line-added">+   _surv_rate_group-&gt;record_surviving_words(age_in_group, words_survived);</span>
  }
  
  #endif // SHARE_GC_G1_HEAPREGION_INLINE_HPP
</pre>
<center><a href="heapRegion.hpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="heapRegionManager.cpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>