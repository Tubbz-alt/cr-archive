<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/share/gc/g1/g1HotCardCache.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2013, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/g1/g1CardTableEntryClosure.hpp&quot;
 27 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
 28 #include &quot;gc/g1/g1DirtyCardQueue.hpp&quot;
 29 #include &quot;gc/g1/g1HotCardCache.hpp&quot;
 30 #include &quot;runtime/atomic.hpp&quot;
 31 
 32 G1HotCardCache::G1HotCardCache(G1CollectedHeap *g1h):
 33   _g1h(g1h), _use_cache(false), _card_counts(g1h),
 34   _hot_cache(NULL), _hot_cache_size(0), _hot_cache_par_chunk_size(0),
 35   _hot_cache_idx(0), _hot_cache_par_claimed_idx(0), _cache_wrapped_around(false)
 36 {}
 37 
 38 void G1HotCardCache::initialize(G1RegionToSpaceMapper* card_counts_storage) {
 39   if (default_use_cache()) {
 40     _use_cache = true;
 41 
 42     _hot_cache_size = (size_t)1 &lt;&lt; G1ConcRSLogCacheSize;
 43     _hot_cache = ArrayAllocator&lt;CardValue*&gt;::allocate(_hot_cache_size, mtGC);
 44 
 45     reset_hot_cache_internal();
 46 
 47     // For refining the cards in the hot cache in parallel
 48     _hot_cache_par_chunk_size = ClaimChunkSize;
 49     _hot_cache_par_claimed_idx = 0;
 50 
 51     _cache_wrapped_around = false;
 52 
 53     _card_counts.initialize(card_counts_storage);
 54   }
 55 }
 56 
 57 G1HotCardCache::~G1HotCardCache() {
 58   if (default_use_cache()) {
 59     assert(_hot_cache != NULL, &quot;Logic&quot;);
 60     ArrayAllocator&lt;CardValue*&gt;::free(_hot_cache, _hot_cache_size);
 61     _hot_cache = NULL;
 62   }
 63 }
 64 
 65 CardTable::CardValue* G1HotCardCache::insert(CardValue* card_ptr) {
 66   uint count = _card_counts.add_card_count(card_ptr);
 67   if (!_card_counts.is_hot(count)) {
 68     // The card is not hot so do not store it in the cache;
 69     // return it for immediate refining.
 70     return card_ptr;
 71   }
 72   // Otherwise, the card is hot.
 73   size_t index = Atomic::fetch_and_add(&amp;_hot_cache_idx, 1u);
 74   if (index == _hot_cache_size) {
 75     // Can use relaxed store because all racing threads are writing the same
 76     // value and there aren&#39;t any concurrent readers.
 77     Atomic::store(&amp;_cache_wrapped_around, true);
 78   }
 79   size_t masked_index = index &amp; (_hot_cache_size - 1);
 80   CardValue* current_ptr = _hot_cache[masked_index];
 81 
 82   // Try to store the new card pointer into the cache. Compare-and-swap to guard
 83   // against the unlikely event of a race resulting in another card pointer to
 84   // have already been written to the cache. In this case we will return
 85   // card_ptr in favor of the other option, which would be starting over. This
 86   // should be OK since card_ptr will likely be the older card already when/if
 87   // this ever happens.
 88   CardValue* previous_ptr = Atomic::cmpxchg(&amp;_hot_cache[masked_index],
 89                                             current_ptr,
 90                                             card_ptr);
 91   return (previous_ptr == current_ptr) ? previous_ptr : card_ptr;
 92 }
 93 
 94 void G1HotCardCache::drain(G1CardTableEntryClosure* cl, uint worker_id) {
 95   assert(default_use_cache(), &quot;Drain only necessary if we use the hot card cache.&quot;);
 96 
 97   assert(_hot_cache != NULL, &quot;Logic&quot;);
 98   assert(!use_cache(), &quot;cache should be disabled&quot;);
 99 
100   while (_hot_cache_par_claimed_idx &lt; _hot_cache_size) {
101     size_t end_idx = Atomic::add(&amp;_hot_cache_par_claimed_idx,
102                                  _hot_cache_par_chunk_size);
103     size_t start_idx = end_idx - _hot_cache_par_chunk_size;
104     // The current worker has successfully claimed the chunk [start_idx..end_idx)
105     end_idx = MIN2(end_idx, _hot_cache_size);
106     for (size_t i = start_idx; i &lt; end_idx; i++) {
107       CardValue* card_ptr = _hot_cache[i];
108       if (card_ptr != NULL) {
109         cl-&gt;do_card_ptr(card_ptr, worker_id);
110       } else {
111         break;
112       }
113     }
114   }
115 
116   // The existing entries in the hot card cache, which were just refined
117   // above, are discarded prior to re-enabling the cache near the end of the GC.
118 }
119 
120 void G1HotCardCache::reset_card_counts(HeapRegion* hr) {
121   _card_counts.clear_region(hr);
122 }
    </pre>
  </body>
</html>