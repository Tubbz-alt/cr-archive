<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/share/gc/g1/g1EvacFailure.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2012, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
 27 #include &quot;gc/g1/g1CollectorState.hpp&quot;
 28 #include &quot;gc/g1/g1ConcurrentMark.inline.hpp&quot;
 29 #include &quot;gc/g1/g1DirtyCardQueue.hpp&quot;
 30 #include &quot;gc/g1/g1EvacFailure.hpp&quot;
 31 #include &quot;gc/g1/g1HeapVerifier.hpp&quot;
 32 #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
 33 #include &quot;gc/g1/g1_globals.hpp&quot;
 34 #include &quot;gc/g1/heapRegion.hpp&quot;
 35 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
 36 #include &quot;gc/shared/preservedMarks.inline.hpp&quot;
 37 #include &quot;oops/access.inline.hpp&quot;
 38 #include &quot;oops/compressedOops.inline.hpp&quot;
 39 #include &quot;oops/oop.inline.hpp&quot;
 40 
 41 class UpdateRSetDeferred : public BasicOopIterateClosure {
 42 private:
 43   G1CollectedHeap* _g1h;
 44   G1DirtyCardQueue* _dcq;
 45   G1CardTable*    _ct;
 46 
 47 public:
 48   UpdateRSetDeferred(G1DirtyCardQueue* dcq) :
 49     _g1h(G1CollectedHeap::heap()), _dcq(dcq), _ct(_g1h-&gt;card_table()) {}
 50 
 51   virtual void do_oop(narrowOop* p) { do_oop_work(p); }
 52   virtual void do_oop(      oop* p) { do_oop_work(p); }
 53   template &lt;class T&gt; void do_oop_work(T* p) {
 54     assert(_g1h-&gt;heap_region_containing(p)-&gt;is_in_reserved(p), &quot;paranoia&quot;);
 55     assert(!_g1h-&gt;heap_region_containing(p)-&gt;is_survivor(), &quot;Unexpected evac failure in survivor region&quot;);
 56 
 57     T const o = RawAccess&lt;&gt;::oop_load(p);
 58     if (CompressedOops::is_null(o)) {
 59       return;
 60     }
 61 
 62     if (HeapRegion::is_in_same_region(p, CompressedOops::decode(o))) {
 63       return;
 64     }
 65     size_t card_index = _ct-&gt;index_for(p);
 66     if (_ct-&gt;mark_card_deferred(card_index)) {
 67       _dcq-&gt;enqueue(_ct-&gt;byte_for_index(card_index));
 68     }
 69   }
 70 };
 71 
 72 class RemoveSelfForwardPtrObjClosure: public ObjectClosure {
 73   G1CollectedHeap* _g1h;
 74   G1ConcurrentMark* _cm;
 75   HeapRegion* _hr;
 76   size_t _marked_bytes;
 77   UpdateRSetDeferred* _update_rset_cl;
 78   bool _during_initial_mark;
 79   uint _worker_id;
 80   HeapWord* _last_forwarded_object_end;
 81 
 82 public:
 83   RemoveSelfForwardPtrObjClosure(HeapRegion* hr,
 84                                  UpdateRSetDeferred* update_rset_cl,
 85                                  bool during_initial_mark,
 86                                  uint worker_id) :
 87     _g1h(G1CollectedHeap::heap()),
 88     _cm(_g1h-&gt;concurrent_mark()),
 89     _hr(hr),
 90     _marked_bytes(0),
 91     _update_rset_cl(update_rset_cl),
 92     _during_initial_mark(during_initial_mark),
 93     _worker_id(worker_id),
 94     _last_forwarded_object_end(hr-&gt;bottom()) { }
 95 
 96   size_t marked_bytes() { return _marked_bytes; }
 97 
 98   // Iterate over the live objects in the region to find self-forwarded objects
 99   // that need to be kept live. We need to update the remembered sets of these
100   // objects. Further update the BOT and marks.
101   // We can coalesce and overwrite the remaining heap contents with dummy objects
102   // as they have either been dead or evacuated (which are unreferenced now, i.e.
103   // dead too) already.
104   void do_object(oop obj) {
105     HeapWord* obj_addr = (HeapWord*) obj;
106     assert(_hr-&gt;is_in(obj_addr), &quot;sanity&quot;);
107 
108     if (obj-&gt;is_forwarded() &amp;&amp; obj-&gt;forwardee() == obj) {
109       // The object failed to move.
110 
111       zap_dead_objects(_last_forwarded_object_end, obj_addr);
112       // We consider all objects that we find self-forwarded to be
113       // live. What we&#39;ll do is that we&#39;ll update the prev marking
114       // info so that they are all under PTAMS and explicitly marked.
115       if (!_cm-&gt;is_marked_in_prev_bitmap(obj)) {
116         _cm-&gt;mark_in_prev_bitmap(obj);
117       }
118       if (_during_initial_mark) {
119         // For the next marking info we&#39;ll only mark the
120         // self-forwarded objects explicitly if we are during
121         // initial-mark (since, normally, we only mark objects pointed
122         // to by roots if we succeed in copying them). By marking all
123         // self-forwarded objects we ensure that we mark any that are
124         // still pointed to be roots. During concurrent marking, and
125         // after initial-mark, we don&#39;t need to mark any objects
126         // explicitly and all objects in the CSet are considered
127         // (implicitly) live. So, we won&#39;t mark them explicitly and
128         // we&#39;ll leave them over NTAMS.
129         _cm-&gt;mark_in_next_bitmap(_worker_id, _hr, obj);
130       }
131       size_t obj_size = obj-&gt;size();
132 
133       _marked_bytes += (obj_size * HeapWordSize);
134       PreservedMarks::init_forwarded_mark(obj);
135 
136       // While we were processing RSet buffers during the collection,
137       // we actually didn&#39;t scan any cards on the collection set,
138       // since we didn&#39;t want to update remembered sets with entries
139       // that point into the collection set, given that live objects
140       // from the collection set are about to move and such entries
141       // will be stale very soon.
142       // This change also dealt with a reliability issue which
143       // involved scanning a card in the collection set and coming
144       // across an array that was being chunked and looking malformed.
145       // The problem is that, if evacuation fails, we might have
146       // remembered set entries missing given that we skipped cards on
147       // the collection set. So, we&#39;ll recreate such entries now.
148       obj-&gt;oop_iterate(_update_rset_cl);
149 
150       HeapWord* obj_end = obj_addr + obj_size;
151       _last_forwarded_object_end = obj_end;
152       _hr-&gt;cross_threshold(obj_addr, obj_end);
153     }
154   }
155 
156   // Fill the memory area from start to end with filler objects, and update the BOT
157   // and the mark bitmap accordingly.
158   void zap_dead_objects(HeapWord* start, HeapWord* end) {
159     if (start == end) {
160       return;
161     }
162 
163     size_t gap_size = pointer_delta(end, start);
164     MemRegion mr(start, gap_size);
165     if (gap_size &gt;= CollectedHeap::min_fill_size()) {
166       CollectedHeap::fill_with_objects(start, gap_size);
167 
168       HeapWord* end_first_obj = start + ((oop)start)-&gt;size();
169       _hr-&gt;cross_threshold(start, end_first_obj);
170       // Fill_with_objects() may have created multiple (i.e. two)
171       // objects, as the max_fill_size() is half a region.
172       // After updating the BOT for the first object, also update the
173       // BOT for the second object to make the BOT complete.
174       if (end_first_obj != end) {
175         _hr-&gt;cross_threshold(end_first_obj, end);
176 #ifdef ASSERT
177         size_t size_second_obj = ((oop)end_first_obj)-&gt;size();
178         HeapWord* end_of_second_obj = end_first_obj + size_second_obj;
179         assert(end == end_of_second_obj,
180                &quot;More than two objects were used to fill the area from &quot; PTR_FORMAT &quot; to &quot; PTR_FORMAT &quot;, &quot;
181                &quot;second objects size &quot; SIZE_FORMAT &quot; ends at &quot; PTR_FORMAT,
182                p2i(start), p2i(end), size_second_obj, p2i(end_of_second_obj));
183 #endif
184       }
185     }
186     _cm-&gt;clear_range_in_prev_bitmap(mr);
187   }
188 
189   void zap_remainder() {
190     zap_dead_objects(_last_forwarded_object_end, _hr-&gt;top());
191   }
192 };
193 
194 class RemoveSelfForwardPtrHRClosure: public HeapRegionClosure {
195   G1CollectedHeap* _g1h;
196   uint _worker_id;
197   HeapRegionClaimer* _hrclaimer;
198 
199   G1DirtyCardQueue _dcq;
200   UpdateRSetDeferred _update_rset_cl;
201 
202 public:
203   RemoveSelfForwardPtrHRClosure(uint worker_id,
204                                 HeapRegionClaimer* hrclaimer) :
205     _g1h(G1CollectedHeap::heap()),
206     _worker_id(worker_id),
207     _hrclaimer(hrclaimer),
208     _dcq(&amp;_g1h-&gt;dirty_card_queue_set()),
209     _update_rset_cl(&amp;_dcq){
210   }
211 
212   size_t remove_self_forward_ptr_by_walking_hr(HeapRegion* hr,
213                                                bool during_initial_mark) {
214     RemoveSelfForwardPtrObjClosure rspc(hr,
215                                         &amp;_update_rset_cl,
216                                         during_initial_mark,
217                                         _worker_id);
218     hr-&gt;object_iterate(&amp;rspc);
219     // Need to zap the remainder area of the processed region.
220     rspc.zap_remainder();
221 
222     return rspc.marked_bytes();
223   }
224 
225   bool do_heap_region(HeapRegion *hr) {
226     assert(!hr-&gt;is_pinned(), &quot;Unexpected pinned region at index %u&quot;, hr-&gt;hrm_index());
227     assert(hr-&gt;in_collection_set(), &quot;bad CS&quot;);
228 
229     if (_hrclaimer-&gt;claim_region(hr-&gt;hrm_index())) {
230       if (hr-&gt;evacuation_failed()) {
231         bool during_initial_mark = _g1h-&gt;collector_state()-&gt;in_initial_mark_gc();
232         bool during_conc_mark = _g1h-&gt;collector_state()-&gt;mark_or_rebuild_in_progress();
233 
234         hr-&gt;note_self_forwarding_removal_start(during_initial_mark,
235                                                during_conc_mark);
236         _g1h-&gt;verifier()-&gt;check_bitmaps(&quot;Self-Forwarding Ptr Removal&quot;, hr);
237 
238         hr-&gt;reset_bot();
239 
240         size_t live_bytes = remove_self_forward_ptr_by_walking_hr(hr, during_initial_mark);
241 
242         hr-&gt;rem_set()-&gt;clean_strong_code_roots(hr);
243         hr-&gt;rem_set()-&gt;clear_locked(true);
244 
245         hr-&gt;note_self_forwarding_removal_end(live_bytes);
246       }
247     }
248     return false;
249   }
250 };
251 
252 G1ParRemoveSelfForwardPtrsTask::G1ParRemoveSelfForwardPtrsTask() :
253   AbstractGangTask(&quot;G1 Remove Self-forwarding Pointers&quot;),
254   _g1h(G1CollectedHeap::heap()),
255   _hrclaimer(_g1h-&gt;workers()-&gt;active_workers()) { }
256 
257 void G1ParRemoveSelfForwardPtrsTask::work(uint worker_id) {
258   RemoveSelfForwardPtrHRClosure rsfp_cl(worker_id, &amp;_hrclaimer);
259 
260   _g1h-&gt;collection_set_iterate_from(&amp;rsfp_cl, worker_id);
261 }
    </pre>
  </body>
</html>