<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1OopClosures.inline.hpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1OopClosures.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1OopStarChunkedList.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1OopClosures.inline.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_GC_G1_G1OOPCLOSURES_INLINE_HPP
 26 #define SHARE_GC_G1_G1OOPCLOSURES_INLINE_HPP
 27 
 28 #include &quot;gc/g1/g1CollectedHeap.hpp&quot;
 29 #include &quot;gc/g1/g1ConcurrentMark.inline.hpp&quot;
 30 #include &quot;gc/g1/g1OopClosures.hpp&quot;
 31 #include &quot;gc/g1/g1ParScanThreadState.inline.hpp&quot;
 32 #include &quot;gc/g1/g1RemSet.hpp&quot;
 33 #include &quot;gc/g1/heapRegion.inline.hpp&quot;
 34 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
 35 #include &quot;memory/iterator.inline.hpp&quot;
 36 #include &quot;oops/access.inline.hpp&quot;
 37 #include &quot;oops/compressedOops.inline.hpp&quot;
 38 #include &quot;oops/oopsHierarchy.hpp&quot;
 39 #include &quot;oops/oop.inline.hpp&quot;
 40 #include &quot;runtime/prefetch.inline.hpp&quot;

 41 
 42 template &lt;class T&gt;
 43 inline void G1ScanClosureBase::prefetch_and_push(T* p, const oop obj) {
 44   // We&#39;re not going to even bother checking whether the object is
 45   // already forwarded or not, as this usually causes an immediate
 46   // stall. We&#39;ll try to prefetch the object (for write, given that
 47   // we might need to install the forwarding reference) and we&#39;ll
 48   // get back to it when pop it from the queue
 49   Prefetch::write(obj-&gt;mark_addr_raw(), 0);
 50   Prefetch::read(obj-&gt;mark_addr_raw(), (HeapWordSize*2));
 51 
 52   // slightly paranoid test; I&#39;m trying to catch potential
 53   // problems before we go into push_on_queue to know where the
 54   // problem is coming from
 55   assert((obj == RawAccess&lt;&gt;::oop_load(p)) ||
 56          (obj-&gt;is_forwarded() &amp;&amp;
 57          obj-&gt;forwardee() == RawAccess&lt;&gt;::oop_load(p)),
 58          &quot;p should still be pointing to obj or to its forwardee&quot;);
 59 
 60   _par_scan_state-&gt;push_on_queue(p);
 61 }
 62 
 63 template &lt;class T&gt;
<span class="line-modified"> 64 inline void G1ScanClosureBase::handle_non_cset_obj_common(InCSetState const state, T* p, oop const obj) {</span>
<span class="line-modified"> 65   if (state.is_humongous()) {</span>
 66     _g1h-&gt;set_humongous_is_live(obj);
<span class="line-modified"> 67   } else if (state.is_optional()) {</span>
 68     _par_scan_state-&gt;remember_reference_into_optional_region(p);
 69   }
 70 }
 71 
 72 inline void G1ScanClosureBase::trim_queue_partially() {
 73   _par_scan_state-&gt;trim_queue_partially();
 74 }
 75 
 76 template &lt;class T&gt;
 77 inline void G1ScanEvacuatedObjClosure::do_oop_work(T* p) {
 78   T heap_oop = RawAccess&lt;&gt;::oop_load(p);
 79 
 80   if (CompressedOops::is_null(heap_oop)) {
 81     return;
 82   }
 83   oop obj = CompressedOops::decode_not_null(heap_oop);
<span class="line-modified"> 84   const InCSetState state = _g1h-&gt;in_cset_state(obj);</span>
<span class="line-modified"> 85   if (state.is_in_cset()) {</span>
 86     prefetch_and_push(p, obj);
 87   } else if (!HeapRegion::is_in_same_region(p, obj)) {
<span class="line-modified"> 88     handle_non_cset_obj_common(state, p, obj);</span>
 89     assert(_scanning_in_young != Uninitialized, &quot;Scan location has not been initialized.&quot;);
 90     if (_scanning_in_young == True) {
 91       return;
 92     }
<span class="line-modified"> 93     _par_scan_state-&gt;enqueue_card_if_tracked(p, obj);</span>
 94   }
 95 }
 96 
 97 template &lt;class T&gt;
 98 inline void G1CMOopClosure::do_oop_work(T* p) {
 99   _task-&gt;deal_with_reference(p);
100 }
101 
102 template &lt;class T&gt;
103 inline void G1RootRegionScanClosure::do_oop_work(T* p) {
104   T heap_oop = RawAccess&lt;MO_VOLATILE&gt;::oop_load(p);
105   if (CompressedOops::is_null(heap_oop)) {
106     return;
107   }
108   oop obj = CompressedOops::decode_not_null(heap_oop);
109   _cm-&gt;mark_in_next_bitmap(_worker_id, obj);
110 }
111 
112 template &lt;class T&gt;
113 inline static void check_obj_during_refinement(T* p, oop const obj) {
114 #ifdef ASSERT
115   G1CollectedHeap* g1h = G1CollectedHeap::heap();
116   // can&#39;t do because of races
117   // assert(oopDesc::is_oop_or_null(obj), &quot;expected an oop&quot;);
<span class="line-modified">118   assert(check_obj_alignment(obj), &quot;not oop aligned&quot;);</span>
<span class="line-modified">119   assert(g1h-&gt;is_in_reserved(obj), &quot;must be in heap&quot;);</span>
120 
121   HeapRegion* from = g1h-&gt;heap_region_containing(p);
122 
123   assert(from != NULL, &quot;from region must be non-NULL&quot;);
124   assert(from-&gt;is_in_reserved(p) ||
125          (from-&gt;is_humongous() &amp;&amp;
126           g1h-&gt;heap_region_containing(p)-&gt;is_humongous() &amp;&amp;
127           from-&gt;humongous_start_region() == g1h-&gt;heap_region_containing(p)-&gt;humongous_start_region()),
128          &quot;p &quot; PTR_FORMAT &quot; is not in the same region %u or part of the correct humongous object starting at region %u.&quot;,
129          p2i(p), from-&gt;hrm_index(), from-&gt;humongous_start_region()-&gt;hrm_index());
130 #endif // ASSERT
131 }
132 
133 template &lt;class T&gt;
134 inline void G1ConcurrentRefineOopClosure::do_oop_work(T* p) {
135   T o = RawAccess&lt;MO_VOLATILE&gt;::oop_load(p);
136   if (CompressedOops::is_null(o)) {
137     return;
138   }
139   oop obj = CompressedOops::decode_not_null(o);
140 
141   check_obj_during_refinement(p, obj);
142 
143   if (HeapRegion::is_in_same_region(p, obj)) {
144     // Normally this closure should only be called with cross-region references.
145     // But since Java threads are manipulating the references concurrently and we
146     // reload the values things may have changed.
147     // Also this check lets slip through references from a humongous continues region
148     // to its humongous start region, as they are in different regions, and adds a
149     // remembered set entry. This is benign (apart from memory usage), as we never
150     // try to either evacuate or eager reclaim humonguous arrays of j.l.O.
151     return;
152   }
153 
154   HeapRegionRemSet* to_rem_set = _g1h-&gt;heap_region_containing(obj)-&gt;rem_set();
155 
156   assert(to_rem_set != NULL, &quot;Need per-region &#39;into&#39; remsets.&quot;);
157   if (to_rem_set-&gt;is_tracked()) {
<span class="line-modified">158     to_rem_set-&gt;add_reference(p, _worker_i);</span>
159   }
160 }
161 
162 template &lt;class T&gt;
<span class="line-modified">163 inline void G1ScanObjsDuringUpdateRSClosure::do_oop_work(T* p) {</span>
164   T o = RawAccess&lt;&gt;::oop_load(p);
165   if (CompressedOops::is_null(o)) {
166     return;
167   }
168   oop obj = CompressedOops::decode_not_null(o);
169 
170   check_obj_during_refinement(p, obj);
171 
<span class="line-modified">172   assert(!_g1h-&gt;is_in_cset((HeapWord*)p), &quot;Oop originates from &quot; PTR_FORMAT &quot; (region: %u) which is in the collection set.&quot;, p2i(p), _g1h-&gt;addr_to_region((HeapWord*)p));</span>
<span class="line-modified">173   const InCSetState state = _g1h-&gt;in_cset_state(obj);</span>
<span class="line-modified">174   if (state.is_in_cset()) {</span>



175     // Since the source is always from outside the collection set, here we implicitly know
176     // that this is a cross-region reference too.
177     prefetch_and_push(p, obj);
178   } else if (!HeapRegion::is_in_same_region(p, obj)) {
<span class="line-modified">179     handle_non_cset_obj_common(state, p, obj);</span>
<span class="line-modified">180     _par_scan_state-&gt;enqueue_card_if_tracked(p, obj);</span>
181   }
182 }
183 
184 template &lt;class T&gt;
<span class="line-modified">185 inline void G1ScanObjsDuringScanRSClosure::do_oop_work(T* p) {</span>
<span class="line-modified">186   T heap_oop = RawAccess&lt;&gt;::oop_load(p);</span>
<span class="line-modified">187   if (CompressedOops::is_null(heap_oop)) {</span>



188     return;
189   }
<span class="line-removed">190   oop obj = CompressedOops::decode_not_null(heap_oop);</span>
<span class="line-removed">191 </span>
<span class="line-removed">192   const InCSetState state = _g1h-&gt;in_cset_state(obj);</span>
<span class="line-removed">193   if (state.is_in_cset()) {</span>
<span class="line-removed">194     prefetch_and_push(p, obj);</span>
<span class="line-removed">195   } else if (!HeapRegion::is_in_same_region(p, obj)) {</span>
<span class="line-removed">196     handle_non_cset_obj_common(state, p, obj);</span>
<span class="line-removed">197   }</span>
<span class="line-removed">198 }</span>
<span class="line-removed">199 </span>
<span class="line-removed">200 template &lt;class T&gt;</span>
<span class="line-removed">201 inline void G1ScanRSForOptionalClosure::do_oop_work(T* p) {</span>
202   _scan_cl-&gt;do_oop_work(p);
203   _scan_cl-&gt;trim_queue_partially();
204 }
205 
206 void G1ParCopyHelper::do_cld_barrier(oop new_obj) {
207   if (_g1h-&gt;heap_region_containing(new_obj)-&gt;is_young()) {
208     _scanned_cld-&gt;record_modified_oops();
209   }
210 }
211 
212 void G1ParCopyHelper::mark_object(oop obj) {
213   assert(!_g1h-&gt;heap_region_containing(obj)-&gt;in_collection_set(), &quot;should not mark objects in the CSet&quot;);
214 
215   // We know that the object is not moving so it&#39;s safe to read its size.
216   _cm-&gt;mark_in_next_bitmap(_worker_id, obj);
217 }
218 
219 void G1ParCopyHelper::trim_queue_partially() {
220   _par_scan_state-&gt;trim_queue_partially();
221 }
222 
223 template &lt;G1Barrier barrier, G1Mark do_mark_object&gt;
224 template &lt;class T&gt;
225 void G1ParCopyClosure&lt;barrier, do_mark_object&gt;::do_oop_work(T* p) {
226   T heap_oop = RawAccess&lt;&gt;::oop_load(p);
227 
228   if (CompressedOops::is_null(heap_oop)) {
229     return;
230   }
231 
232   oop obj = CompressedOops::decode_not_null(heap_oop);
233 
234   assert(_worker_id == _par_scan_state-&gt;worker_id(), &quot;sanity&quot;);
235 
<span class="line-modified">236   const InCSetState state = _g1h-&gt;in_cset_state(obj);</span>
237   if (state.is_in_cset()) {
238     oop forwardee;
<span class="line-modified">239     markOop m = obj-&gt;mark_raw();</span>
<span class="line-modified">240     if (m-&gt;is_marked()) {</span>
<span class="line-modified">241       forwardee = (oop) m-&gt;decode_pointer();</span>
242     } else {
243       forwardee = _par_scan_state-&gt;copy_to_survivor_space(state, obj, m);
244     }
245     assert(forwardee != NULL, &quot;forwardee should not be NULL&quot;);
246     RawAccess&lt;IS_NOT_NULL&gt;::oop_store(p, forwardee);
247 
248     if (barrier == G1BarrierCLD) {
249       do_cld_barrier(forwardee);
250     }
251   } else {
252     if (state.is_humongous()) {
253       _g1h-&gt;set_humongous_is_live(obj);
<span class="line-modified">254     } else if (state.is_optional()) {</span>
255       _par_scan_state-&gt;remember_root_into_optional_region(p);
256     }
257 
258     // The object is not in collection set. If we&#39;re a root scanning
259     // closure during an initial mark pause then attempt to mark the object.
260     if (do_mark_object == G1MarkFromRoot) {
261       mark_object(obj);
262     }
263   }
264   trim_queue_partially();
265 }
266 
267 template &lt;class T&gt; void G1RebuildRemSetClosure::do_oop_work(T* p) {
268   oop const obj = RawAccess&lt;MO_VOLATILE&gt;::oop_load(p);
269   if (obj == NULL) {
270     return;
271   }
272 
273   if (HeapRegion::is_in_same_region(p, obj)) {
274     return;
</pre>
</td>
<td>
<hr />
<pre>
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_GC_G1_G1OOPCLOSURES_INLINE_HPP
 26 #define SHARE_GC_G1_G1OOPCLOSURES_INLINE_HPP
 27 
 28 #include &quot;gc/g1/g1CollectedHeap.hpp&quot;
 29 #include &quot;gc/g1/g1ConcurrentMark.inline.hpp&quot;
 30 #include &quot;gc/g1/g1OopClosures.hpp&quot;
 31 #include &quot;gc/g1/g1ParScanThreadState.inline.hpp&quot;
 32 #include &quot;gc/g1/g1RemSet.hpp&quot;
 33 #include &quot;gc/g1/heapRegion.inline.hpp&quot;
 34 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
 35 #include &quot;memory/iterator.inline.hpp&quot;
 36 #include &quot;oops/access.inline.hpp&quot;
 37 #include &quot;oops/compressedOops.inline.hpp&quot;
 38 #include &quot;oops/oopsHierarchy.hpp&quot;
 39 #include &quot;oops/oop.inline.hpp&quot;
 40 #include &quot;runtime/prefetch.inline.hpp&quot;
<span class="line-added"> 41 #include &quot;utilities/align.hpp&quot;</span>
 42 
 43 template &lt;class T&gt;
 44 inline void G1ScanClosureBase::prefetch_and_push(T* p, const oop obj) {
 45   // We&#39;re not going to even bother checking whether the object is
 46   // already forwarded or not, as this usually causes an immediate
 47   // stall. We&#39;ll try to prefetch the object (for write, given that
 48   // we might need to install the forwarding reference) and we&#39;ll
 49   // get back to it when pop it from the queue
 50   Prefetch::write(obj-&gt;mark_addr_raw(), 0);
 51   Prefetch::read(obj-&gt;mark_addr_raw(), (HeapWordSize*2));
 52 
 53   // slightly paranoid test; I&#39;m trying to catch potential
 54   // problems before we go into push_on_queue to know where the
 55   // problem is coming from
 56   assert((obj == RawAccess&lt;&gt;::oop_load(p)) ||
 57          (obj-&gt;is_forwarded() &amp;&amp;
 58          obj-&gt;forwardee() == RawAccess&lt;&gt;::oop_load(p)),
 59          &quot;p should still be pointing to obj or to its forwardee&quot;);
 60 
 61   _par_scan_state-&gt;push_on_queue(p);
 62 }
 63 
 64 template &lt;class T&gt;
<span class="line-modified"> 65 inline void G1ScanClosureBase::handle_non_cset_obj_common(G1HeapRegionAttr const region_attr, T* p, oop const obj) {</span>
<span class="line-modified"> 66   if (region_attr.is_humongous()) {</span>
 67     _g1h-&gt;set_humongous_is_live(obj);
<span class="line-modified"> 68   } else if (region_attr.is_optional()) {</span>
 69     _par_scan_state-&gt;remember_reference_into_optional_region(p);
 70   }
 71 }
 72 
 73 inline void G1ScanClosureBase::trim_queue_partially() {
 74   _par_scan_state-&gt;trim_queue_partially();
 75 }
 76 
 77 template &lt;class T&gt;
 78 inline void G1ScanEvacuatedObjClosure::do_oop_work(T* p) {
 79   T heap_oop = RawAccess&lt;&gt;::oop_load(p);
 80 
 81   if (CompressedOops::is_null(heap_oop)) {
 82     return;
 83   }
 84   oop obj = CompressedOops::decode_not_null(heap_oop);
<span class="line-modified"> 85   const G1HeapRegionAttr region_attr = _g1h-&gt;region_attr(obj);</span>
<span class="line-modified"> 86   if (region_attr.is_in_cset()) {</span>
 87     prefetch_and_push(p, obj);
 88   } else if (!HeapRegion::is_in_same_region(p, obj)) {
<span class="line-modified"> 89     handle_non_cset_obj_common(region_attr, p, obj);</span>
 90     assert(_scanning_in_young != Uninitialized, &quot;Scan location has not been initialized.&quot;);
 91     if (_scanning_in_young == True) {
 92       return;
 93     }
<span class="line-modified"> 94     _par_scan_state-&gt;enqueue_card_if_tracked(region_attr, p, obj);</span>
 95   }
 96 }
 97 
 98 template &lt;class T&gt;
 99 inline void G1CMOopClosure::do_oop_work(T* p) {
100   _task-&gt;deal_with_reference(p);
101 }
102 
103 template &lt;class T&gt;
104 inline void G1RootRegionScanClosure::do_oop_work(T* p) {
105   T heap_oop = RawAccess&lt;MO_VOLATILE&gt;::oop_load(p);
106   if (CompressedOops::is_null(heap_oop)) {
107     return;
108   }
109   oop obj = CompressedOops::decode_not_null(heap_oop);
110   _cm-&gt;mark_in_next_bitmap(_worker_id, obj);
111 }
112 
113 template &lt;class T&gt;
114 inline static void check_obj_during_refinement(T* p, oop const obj) {
115 #ifdef ASSERT
116   G1CollectedHeap* g1h = G1CollectedHeap::heap();
117   // can&#39;t do because of races
118   // assert(oopDesc::is_oop_or_null(obj), &quot;expected an oop&quot;);
<span class="line-modified">119   assert(is_object_aligned(obj), &quot;oop must be aligned&quot;);</span>
<span class="line-modified">120   assert(g1h-&gt;is_in_reserved(obj), &quot;oop must be in reserved&quot;);</span>
121 
122   HeapRegion* from = g1h-&gt;heap_region_containing(p);
123 
124   assert(from != NULL, &quot;from region must be non-NULL&quot;);
125   assert(from-&gt;is_in_reserved(p) ||
126          (from-&gt;is_humongous() &amp;&amp;
127           g1h-&gt;heap_region_containing(p)-&gt;is_humongous() &amp;&amp;
128           from-&gt;humongous_start_region() == g1h-&gt;heap_region_containing(p)-&gt;humongous_start_region()),
129          &quot;p &quot; PTR_FORMAT &quot; is not in the same region %u or part of the correct humongous object starting at region %u.&quot;,
130          p2i(p), from-&gt;hrm_index(), from-&gt;humongous_start_region()-&gt;hrm_index());
131 #endif // ASSERT
132 }
133 
134 template &lt;class T&gt;
135 inline void G1ConcurrentRefineOopClosure::do_oop_work(T* p) {
136   T o = RawAccess&lt;MO_VOLATILE&gt;::oop_load(p);
137   if (CompressedOops::is_null(o)) {
138     return;
139   }
140   oop obj = CompressedOops::decode_not_null(o);
141 
142   check_obj_during_refinement(p, obj);
143 
144   if (HeapRegion::is_in_same_region(p, obj)) {
145     // Normally this closure should only be called with cross-region references.
146     // But since Java threads are manipulating the references concurrently and we
147     // reload the values things may have changed.
148     // Also this check lets slip through references from a humongous continues region
149     // to its humongous start region, as they are in different regions, and adds a
150     // remembered set entry. This is benign (apart from memory usage), as we never
151     // try to either evacuate or eager reclaim humonguous arrays of j.l.O.
152     return;
153   }
154 
155   HeapRegionRemSet* to_rem_set = _g1h-&gt;heap_region_containing(obj)-&gt;rem_set();
156 
157   assert(to_rem_set != NULL, &quot;Need per-region &#39;into&#39; remsets.&quot;);
158   if (to_rem_set-&gt;is_tracked()) {
<span class="line-modified">159     to_rem_set-&gt;add_reference(p, _worker_id);</span>
160   }
161 }
162 
163 template &lt;class T&gt;
<span class="line-modified">164 inline void G1ScanCardClosure::do_oop_work(T* p) {</span>
165   T o = RawAccess&lt;&gt;::oop_load(p);
166   if (CompressedOops::is_null(o)) {
167     return;
168   }
169   oop obj = CompressedOops::decode_not_null(o);
170 
171   check_obj_during_refinement(p, obj);
172 
<span class="line-modified">173   assert(!_g1h-&gt;is_in_cset((HeapWord*)p),</span>
<span class="line-modified">174          &quot;Oop originates from &quot; PTR_FORMAT &quot; (region: %u) which is in the collection set.&quot;,</span>
<span class="line-modified">175          p2i(p), _g1h-&gt;addr_to_region((HeapWord*)p));</span>
<span class="line-added">176 </span>
<span class="line-added">177   const G1HeapRegionAttr region_attr = _g1h-&gt;region_attr(obj);</span>
<span class="line-added">178   if (region_attr.is_in_cset()) {</span>
179     // Since the source is always from outside the collection set, here we implicitly know
180     // that this is a cross-region reference too.
181     prefetch_and_push(p, obj);
182   } else if (!HeapRegion::is_in_same_region(p, obj)) {
<span class="line-modified">183     handle_non_cset_obj_common(region_attr, p, obj);</span>
<span class="line-modified">184     _par_scan_state-&gt;enqueue_card_if_tracked(region_attr, p, obj);</span>
185   }
186 }
187 
188 template &lt;class T&gt;
<span class="line-modified">189 inline void G1ScanRSForOptionalClosure::do_oop_work(T* p) {</span>
<span class="line-modified">190   const G1HeapRegionAttr region_attr = _g1h-&gt;region_attr(p);</span>
<span class="line-modified">191   // Entries in the optional collection set may start to originate from the collection</span>
<span class="line-added">192   // set after one or more increments. In this case, previously optional regions</span>
<span class="line-added">193   // became actual collection set regions. Filter them out here.</span>
<span class="line-added">194   if (region_attr.is_in_cset()) {</span>
195     return;
196   }












197   _scan_cl-&gt;do_oop_work(p);
198   _scan_cl-&gt;trim_queue_partially();
199 }
200 
201 void G1ParCopyHelper::do_cld_barrier(oop new_obj) {
202   if (_g1h-&gt;heap_region_containing(new_obj)-&gt;is_young()) {
203     _scanned_cld-&gt;record_modified_oops();
204   }
205 }
206 
207 void G1ParCopyHelper::mark_object(oop obj) {
208   assert(!_g1h-&gt;heap_region_containing(obj)-&gt;in_collection_set(), &quot;should not mark objects in the CSet&quot;);
209 
210   // We know that the object is not moving so it&#39;s safe to read its size.
211   _cm-&gt;mark_in_next_bitmap(_worker_id, obj);
212 }
213 
214 void G1ParCopyHelper::trim_queue_partially() {
215   _par_scan_state-&gt;trim_queue_partially();
216 }
217 
218 template &lt;G1Barrier barrier, G1Mark do_mark_object&gt;
219 template &lt;class T&gt;
220 void G1ParCopyClosure&lt;barrier, do_mark_object&gt;::do_oop_work(T* p) {
221   T heap_oop = RawAccess&lt;&gt;::oop_load(p);
222 
223   if (CompressedOops::is_null(heap_oop)) {
224     return;
225   }
226 
227   oop obj = CompressedOops::decode_not_null(heap_oop);
228 
229   assert(_worker_id == _par_scan_state-&gt;worker_id(), &quot;sanity&quot;);
230 
<span class="line-modified">231   const G1HeapRegionAttr state = _g1h-&gt;region_attr(obj);</span>
232   if (state.is_in_cset()) {
233     oop forwardee;
<span class="line-modified">234     markWord m = obj-&gt;mark_raw();</span>
<span class="line-modified">235     if (m.is_marked()) {</span>
<span class="line-modified">236       forwardee = (oop) m.decode_pointer();</span>
237     } else {
238       forwardee = _par_scan_state-&gt;copy_to_survivor_space(state, obj, m);
239     }
240     assert(forwardee != NULL, &quot;forwardee should not be NULL&quot;);
241     RawAccess&lt;IS_NOT_NULL&gt;::oop_store(p, forwardee);
242 
243     if (barrier == G1BarrierCLD) {
244       do_cld_barrier(forwardee);
245     }
246   } else {
247     if (state.is_humongous()) {
248       _g1h-&gt;set_humongous_is_live(obj);
<span class="line-modified">249     } else if ((barrier != G1BarrierNoOptRoots) &amp;&amp; state.is_optional()) {</span>
250       _par_scan_state-&gt;remember_root_into_optional_region(p);
251     }
252 
253     // The object is not in collection set. If we&#39;re a root scanning
254     // closure during an initial mark pause then attempt to mark the object.
255     if (do_mark_object == G1MarkFromRoot) {
256       mark_object(obj);
257     }
258   }
259   trim_queue_partially();
260 }
261 
262 template &lt;class T&gt; void G1RebuildRemSetClosure::do_oop_work(T* p) {
263   oop const obj = RawAccess&lt;MO_VOLATILE&gt;::oop_load(p);
264   if (obj == NULL) {
265     return;
266   }
267 
268   if (HeapRegion::is_in_same_region(p, obj)) {
269     return;
</pre>
</td>
</tr>
</table>
<center><a href="g1OopClosures.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1OopStarChunkedList.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>