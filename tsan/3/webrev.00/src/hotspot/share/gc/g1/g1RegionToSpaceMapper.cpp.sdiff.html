<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1RegionToSpaceMapper.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1RegionMarkStatsCache.inline.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1RegionToSpaceMapper.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1RegionToSpaceMapper.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2001, 2018, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/g1/g1BiasedArray.hpp&quot;

 27 #include &quot;gc/g1/g1RegionToSpaceMapper.hpp&quot;
 28 #include &quot;logging/log.hpp&quot;
 29 #include &quot;memory/allocation.inline.hpp&quot;
 30 #include &quot;memory/virtualspace.hpp&quot;
 31 #include &quot;runtime/java.hpp&quot;
 32 #include &quot;runtime/os.inline.hpp&quot;
 33 #include &quot;services/memTracker.hpp&quot;
 34 #include &quot;utilities/align.hpp&quot;
 35 #include &quot;utilities/bitMap.inline.hpp&quot;
 36 #include &quot;utilities/formatBuffer.hpp&quot;

 37 
 38 G1RegionToSpaceMapper::G1RegionToSpaceMapper(ReservedSpace rs,
 39                                              size_t used_size,
 40                                              size_t page_size,
 41                                              size_t region_granularity,
 42                                              size_t commit_factor,
 43                                              MemoryType type) :
 44   _listener(NULL),
 45   _storage(rs, used_size, page_size),
 46   _region_granularity(region_granularity),
<span class="line-modified"> 47   _commit_map(rs.size() * commit_factor / region_granularity, mtGC) {</span>

 48   guarantee(is_power_of_2(page_size), &quot;must be&quot;);
 49   guarantee(is_power_of_2(region_granularity), &quot;must be&quot;);
 50 
 51   MemTracker::record_virtual_memory_type((address)rs.base(), type);
 52 }
 53 
 54 // G1RegionToSpaceMapper implementation where the region granularity is larger than
 55 // or the same as the commit granularity.
 56 // Basically, the space corresponding to one region region spans several OS pages.
 57 class G1RegionsLargerThanCommitSizeMapper : public G1RegionToSpaceMapper {
 58  private:
 59   size_t _pages_per_region;
 60 
 61  public:
 62   G1RegionsLargerThanCommitSizeMapper(ReservedSpace rs,
 63                                       size_t actual_size,
 64                                       size_t page_size,
 65                                       size_t alloc_granularity,
 66                                       size_t commit_factor,
 67                                       MemoryType type) :
 68     G1RegionToSpaceMapper(rs, actual_size, page_size, alloc_granularity, commit_factor, type),
 69     _pages_per_region(alloc_granularity / (page_size * commit_factor)) {
 70 
 71     guarantee(alloc_granularity &gt;= page_size, &quot;allocation granularity smaller than commit granularity&quot;);
 72   }
 73 
 74   virtual void commit_regions(uint start_idx, size_t num_regions, WorkGang* pretouch_gang) {
<span class="line-modified"> 75     size_t const start_page = (size_t)start_idx * _pages_per_region;</span>
<span class="line-modified"> 76     bool zero_filled = _storage.commit(start_page, num_regions * _pages_per_region);</span>








 77     if (AlwaysPreTouch) {
<span class="line-modified"> 78       _storage.pretouch(start_page, num_regions * _pages_per_region, pretouch_gang);</span>
 79     }
 80     _commit_map.set_range(start_idx, start_idx + num_regions);
 81     fire_on_commit(start_idx, num_regions, zero_filled);
 82   }
 83 
 84   virtual void uncommit_regions(uint start_idx, size_t num_regions) {
 85     _storage.uncommit((size_t)start_idx * _pages_per_region, num_regions * _pages_per_region);
 86     _commit_map.clear_range(start_idx, start_idx + num_regions);
 87   }
 88 };
 89 
 90 // G1RegionToSpaceMapper implementation where the region granularity is smaller
 91 // than the commit granularity.
 92 // Basically, the contents of one OS page span several regions.
 93 class G1RegionsSmallerThanCommitSizeMapper : public G1RegionToSpaceMapper {
 94  private:
 95   class CommitRefcountArray : public G1BiasedMappedArray&lt;uint&gt; {
 96    protected:
 97      virtual uint default_value() const { return 0; }
 98   };
</pre>
<hr />
<pre>
109   G1RegionsSmallerThanCommitSizeMapper(ReservedSpace rs,
110                                        size_t actual_size,
111                                        size_t page_size,
112                                        size_t alloc_granularity,
113                                        size_t commit_factor,
114                                        MemoryType type) :
115     G1RegionToSpaceMapper(rs, actual_size, page_size, alloc_granularity, commit_factor, type),
116     _regions_per_page((page_size * commit_factor) / alloc_granularity), _refcounts() {
117 
118     guarantee((page_size * commit_factor) &gt;= alloc_granularity, &quot;allocation granularity smaller than commit granularity&quot;);
119     _refcounts.initialize((HeapWord*)rs.base(), (HeapWord*)(rs.base() + align_up(rs.size(), page_size)), page_size);
120   }
121 
122   virtual void commit_regions(uint start_idx, size_t num_regions, WorkGang* pretouch_gang) {
123     size_t const NoPage = ~(size_t)0;
124 
125     size_t first_committed = NoPage;
126     size_t num_committed = 0;
127 
128     bool all_zero_filled = true;

129 
<span class="line-modified">130     for (uint i = start_idx; i &lt; start_idx + num_regions; i++) {</span>
<span class="line-modified">131       assert(!_commit_map.at(i), &quot;Trying to commit storage at region %u that is already committed&quot;, i);</span>
<span class="line-modified">132       size_t idx = region_idx_to_page_idx(i);</span>
<span class="line-modified">133       uint old_refcount = _refcounts.get_by_index(idx);</span>
134 
135       bool zero_filled = false;
136       if (old_refcount == 0) {
137         if (first_committed == NoPage) {
<span class="line-modified">138           first_committed = idx;</span>
139           num_committed = 1;
140         } else {
141           num_committed++;
142         }
<span class="line-modified">143         zero_filled = _storage.commit(idx, 1);</span>





144       }
145       all_zero_filled &amp;= zero_filled;
146 
<span class="line-modified">147       _refcounts.set_by_index(idx, old_refcount + 1);</span>
<span class="line-modified">148       _commit_map.set_bit(i);</span>
149     }
150     if (AlwaysPreTouch &amp;&amp; num_committed &gt; 0) {
151       _storage.pretouch(first_committed, num_committed, pretouch_gang);
152     }
153     fire_on_commit(start_idx, num_regions, all_zero_filled);
154   }
155 
156   virtual void uncommit_regions(uint start_idx, size_t num_regions) {
157     for (uint i = start_idx; i &lt; start_idx + num_regions; i++) {
158       assert(_commit_map.at(i), &quot;Trying to uncommit storage at region %u that is not committed&quot;, i);
159       size_t idx = region_idx_to_page_idx(i);
160       uint old_refcount = _refcounts.get_by_index(idx);
161       assert(old_refcount &gt; 0, &quot;must be&quot;);
162       if (old_refcount == 1) {
163         _storage.uncommit(idx, 1);
164       }
165       _refcounts.set_by_index(idx, old_refcount - 1);
166       _commit_map.clear_bit(i);
167     }
168   }
</pre>
<hr />
<pre>
188     if (ret != NULL) {
189       os::unmap_memory(rs.base(), rs.size());
190     }
191     log_error(gc, init)(&quot;Error in mapping Old Gen to given AllocateOldGenAt = %s&quot;, AllocateOldGenAt);
192     os::close(_backing_fd);
193     return false;
194   }
195 
196   os::close(_backing_fd);
197   return true;
198 }
199 
200 G1RegionToHeteroSpaceMapper::G1RegionToHeteroSpaceMapper(ReservedSpace rs,
201                                                          size_t actual_size,
202                                                          size_t page_size,
203                                                          size_t alloc_granularity,
204                                                          size_t commit_factor,
205                                                          MemoryType type) :
206   G1RegionToSpaceMapper(rs, actual_size, page_size, alloc_granularity, commit_factor, type),
207   _rs(rs),

208   _num_committed_dram(0),
209   _num_committed_nvdimm(0),

210   _page_size(page_size),
211   _commit_factor(commit_factor),
212   _type(type) {
213   assert(actual_size == 2 * MaxHeapSize, &quot;For 2-way heterogenuous heap, reserved space is two times MaxHeapSize&quot;);
214 }
215 
216 bool G1RegionToHeteroSpaceMapper::initialize() {
217   // Since we need to re-map the reserved space - &#39;Xmx&#39; to nv-dimm and &#39;Xmx&#39; to dram, we need to release the reserved memory first.
218   // Because on some OSes (e.g. Windows) you cannot do a file mapping on memory reserved with regular mapping.
219   os::release_memory(_rs.base(), _rs.size());
220   // First half of size Xmx is for nv-dimm.
221   ReservedSpace rs_nvdimm = _rs.first_part(MaxHeapSize);
222   assert(rs_nvdimm.base() == _rs.base(), &quot;We should get the same base address&quot;);
223 
224   // Second half of reserved memory is mapped to dram.
225   ReservedSpace rs_dram = _rs.last_part(MaxHeapSize);
226 
227   assert(rs_dram.size() == rs_nvdimm.size() &amp;&amp; rs_nvdimm.size() == MaxHeapSize, &quot;They all should be same&quot;);
228 
229   // Reserve dram memory
</pre>
<hr />
<pre>
231   if (base != rs_dram.base()) {
232     if (base != NULL) {
233       os::release_memory(base, rs_dram.size());
234     }
235     log_error(gc, init)(&quot;Error in re-mapping memory on dram during G1 heterogenous memory initialization&quot;);
236     return false;
237   }
238 
239   // We reserve and commit this entire space to NV-DIMM.
240   if (!map_nvdimm_space(rs_nvdimm)) {
241     log_error(gc, init)(&quot;Error in re-mapping memory to nv-dimm during G1 heterogenous memory initialization&quot;);
242     return false;
243   }
244 
245   if (_region_granularity &gt;= (_page_size * _commit_factor)) {
246     _dram_mapper = new G1RegionsLargerThanCommitSizeMapper(rs_dram, rs_dram.size(), _page_size, _region_granularity, _commit_factor, _type);
247   } else {
248     _dram_mapper = new G1RegionsSmallerThanCommitSizeMapper(rs_dram, rs_dram.size(), _page_size, _region_granularity, _commit_factor, _type);
249   }
250 
<span class="line-removed">251   _start_index_of_nvdimm = 0;</span>
252   _start_index_of_dram = (uint)(rs_nvdimm.size() / _region_granularity);
253   return true;
254 }
255 
256 void G1RegionToHeteroSpaceMapper::commit_regions(uint start_idx, size_t num_regions, WorkGang* pretouch_gang) {
257   uint end_idx = (start_idx + (uint)num_regions - 1);
258 
259   uint num_dram = end_idx &gt;= _start_index_of_dram ? MIN2((end_idx - _start_index_of_dram + 1), (uint)num_regions) : 0;
260   uint num_nvdimm = (uint)num_regions - num_dram;
261 
262   if (num_nvdimm &gt; 0) {
263     // We do not need to commit nv-dimm regions, since they are committed in the beginning.
264     _num_committed_nvdimm += num_nvdimm;
265   }
266   if (num_dram &gt; 0) {
267     _dram_mapper-&gt;commit_regions(start_idx &gt; _start_index_of_dram ? (start_idx - _start_index_of_dram) : 0, num_dram, pretouch_gang);
268     _num_committed_dram += num_dram;
269   }
270 }
271 
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2001, 2019, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/g1/g1BiasedArray.hpp&quot;
<span class="line-added"> 27 #include &quot;gc/g1/g1NUMA.hpp&quot;</span>
 28 #include &quot;gc/g1/g1RegionToSpaceMapper.hpp&quot;
 29 #include &quot;logging/log.hpp&quot;
 30 #include &quot;memory/allocation.inline.hpp&quot;
 31 #include &quot;memory/virtualspace.hpp&quot;
 32 #include &quot;runtime/java.hpp&quot;
 33 #include &quot;runtime/os.inline.hpp&quot;
 34 #include &quot;services/memTracker.hpp&quot;
 35 #include &quot;utilities/align.hpp&quot;
 36 #include &quot;utilities/bitMap.inline.hpp&quot;
 37 #include &quot;utilities/formatBuffer.hpp&quot;
<span class="line-added"> 38 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
 39 
 40 G1RegionToSpaceMapper::G1RegionToSpaceMapper(ReservedSpace rs,
 41                                              size_t used_size,
 42                                              size_t page_size,
 43                                              size_t region_granularity,
 44                                              size_t commit_factor,
 45                                              MemoryType type) :
 46   _listener(NULL),
 47   _storage(rs, used_size, page_size),
 48   _region_granularity(region_granularity),
<span class="line-modified"> 49   _commit_map(rs.size() * commit_factor / region_granularity, mtGC),</span>
<span class="line-added"> 50   _memory_type(type) {</span>
 51   guarantee(is_power_of_2(page_size), &quot;must be&quot;);
 52   guarantee(is_power_of_2(region_granularity), &quot;must be&quot;);
 53 
 54   MemTracker::record_virtual_memory_type((address)rs.base(), type);
 55 }
 56 
 57 // G1RegionToSpaceMapper implementation where the region granularity is larger than
 58 // or the same as the commit granularity.
 59 // Basically, the space corresponding to one region region spans several OS pages.
 60 class G1RegionsLargerThanCommitSizeMapper : public G1RegionToSpaceMapper {
 61  private:
 62   size_t _pages_per_region;
 63 
 64  public:
 65   G1RegionsLargerThanCommitSizeMapper(ReservedSpace rs,
 66                                       size_t actual_size,
 67                                       size_t page_size,
 68                                       size_t alloc_granularity,
 69                                       size_t commit_factor,
 70                                       MemoryType type) :
 71     G1RegionToSpaceMapper(rs, actual_size, page_size, alloc_granularity, commit_factor, type),
 72     _pages_per_region(alloc_granularity / (page_size * commit_factor)) {
 73 
 74     guarantee(alloc_granularity &gt;= page_size, &quot;allocation granularity smaller than commit granularity&quot;);
 75   }
 76 
 77   virtual void commit_regions(uint start_idx, size_t num_regions, WorkGang* pretouch_gang) {
<span class="line-modified"> 78     const size_t start_page = (size_t)start_idx * _pages_per_region;</span>
<span class="line-modified"> 79     const size_t size_in_pages = num_regions * _pages_per_region;</span>
<span class="line-added"> 80     bool zero_filled = _storage.commit(start_page, size_in_pages);</span>
<span class="line-added"> 81     if (_memory_type == mtJavaHeap) {</span>
<span class="line-added"> 82       for (uint region_index = start_idx; region_index &lt; start_idx + num_regions; region_index++ ) {</span>
<span class="line-added"> 83         void* address = _storage.page_start(region_index * _pages_per_region);</span>
<span class="line-added"> 84         size_t size_in_bytes = _storage.page_size() * _pages_per_region;</span>
<span class="line-added"> 85         G1NUMA::numa()-&gt;request_memory_on_node(address, size_in_bytes, region_index);</span>
<span class="line-added"> 86       }</span>
<span class="line-added"> 87     }</span>
 88     if (AlwaysPreTouch) {
<span class="line-modified"> 89       _storage.pretouch(start_page, size_in_pages, pretouch_gang);</span>
 90     }
 91     _commit_map.set_range(start_idx, start_idx + num_regions);
 92     fire_on_commit(start_idx, num_regions, zero_filled);
 93   }
 94 
 95   virtual void uncommit_regions(uint start_idx, size_t num_regions) {
 96     _storage.uncommit((size_t)start_idx * _pages_per_region, num_regions * _pages_per_region);
 97     _commit_map.clear_range(start_idx, start_idx + num_regions);
 98   }
 99 };
100 
101 // G1RegionToSpaceMapper implementation where the region granularity is smaller
102 // than the commit granularity.
103 // Basically, the contents of one OS page span several regions.
104 class G1RegionsSmallerThanCommitSizeMapper : public G1RegionToSpaceMapper {
105  private:
106   class CommitRefcountArray : public G1BiasedMappedArray&lt;uint&gt; {
107    protected:
108      virtual uint default_value() const { return 0; }
109   };
</pre>
<hr />
<pre>
120   G1RegionsSmallerThanCommitSizeMapper(ReservedSpace rs,
121                                        size_t actual_size,
122                                        size_t page_size,
123                                        size_t alloc_granularity,
124                                        size_t commit_factor,
125                                        MemoryType type) :
126     G1RegionToSpaceMapper(rs, actual_size, page_size, alloc_granularity, commit_factor, type),
127     _regions_per_page((page_size * commit_factor) / alloc_granularity), _refcounts() {
128 
129     guarantee((page_size * commit_factor) &gt;= alloc_granularity, &quot;allocation granularity smaller than commit granularity&quot;);
130     _refcounts.initialize((HeapWord*)rs.base(), (HeapWord*)(rs.base() + align_up(rs.size(), page_size)), page_size);
131   }
132 
133   virtual void commit_regions(uint start_idx, size_t num_regions, WorkGang* pretouch_gang) {
134     size_t const NoPage = ~(size_t)0;
135 
136     size_t first_committed = NoPage;
137     size_t num_committed = 0;
138 
139     bool all_zero_filled = true;
<span class="line-added">140     G1NUMA* numa = G1NUMA::numa();</span>
141 
<span class="line-modified">142     for (uint region_idx = start_idx; region_idx &lt; start_idx + num_regions; region_idx++) {</span>
<span class="line-modified">143       assert(!_commit_map.at(region_idx), &quot;Trying to commit storage at region %u that is already committed&quot;, region_idx);</span>
<span class="line-modified">144       size_t page_idx = region_idx_to_page_idx(region_idx);</span>
<span class="line-modified">145       uint old_refcount = _refcounts.get_by_index(page_idx);</span>
146 
147       bool zero_filled = false;
148       if (old_refcount == 0) {
149         if (first_committed == NoPage) {
<span class="line-modified">150           first_committed = page_idx;</span>
151           num_committed = 1;
152         } else {
153           num_committed++;
154         }
<span class="line-modified">155         zero_filled = _storage.commit(page_idx, 1);</span>
<span class="line-added">156         if (_memory_type == mtJavaHeap) {</span>
<span class="line-added">157           void* address = _storage.page_start(page_idx);</span>
<span class="line-added">158           size_t size_in_bytes = _storage.page_size();</span>
<span class="line-added">159           numa-&gt;request_memory_on_node(address, size_in_bytes, region_idx);</span>
<span class="line-added">160         }</span>
161       }
162       all_zero_filled &amp;= zero_filled;
163 
<span class="line-modified">164       _refcounts.set_by_index(page_idx, old_refcount + 1);</span>
<span class="line-modified">165       _commit_map.set_bit(region_idx);</span>
166     }
167     if (AlwaysPreTouch &amp;&amp; num_committed &gt; 0) {
168       _storage.pretouch(first_committed, num_committed, pretouch_gang);
169     }
170     fire_on_commit(start_idx, num_regions, all_zero_filled);
171   }
172 
173   virtual void uncommit_regions(uint start_idx, size_t num_regions) {
174     for (uint i = start_idx; i &lt; start_idx + num_regions; i++) {
175       assert(_commit_map.at(i), &quot;Trying to uncommit storage at region %u that is not committed&quot;, i);
176       size_t idx = region_idx_to_page_idx(i);
177       uint old_refcount = _refcounts.get_by_index(idx);
178       assert(old_refcount &gt; 0, &quot;must be&quot;);
179       if (old_refcount == 1) {
180         _storage.uncommit(idx, 1);
181       }
182       _refcounts.set_by_index(idx, old_refcount - 1);
183       _commit_map.clear_bit(i);
184     }
185   }
</pre>
<hr />
<pre>
205     if (ret != NULL) {
206       os::unmap_memory(rs.base(), rs.size());
207     }
208     log_error(gc, init)(&quot;Error in mapping Old Gen to given AllocateOldGenAt = %s&quot;, AllocateOldGenAt);
209     os::close(_backing_fd);
210     return false;
211   }
212 
213   os::close(_backing_fd);
214   return true;
215 }
216 
217 G1RegionToHeteroSpaceMapper::G1RegionToHeteroSpaceMapper(ReservedSpace rs,
218                                                          size_t actual_size,
219                                                          size_t page_size,
220                                                          size_t alloc_granularity,
221                                                          size_t commit_factor,
222                                                          MemoryType type) :
223   G1RegionToSpaceMapper(rs, actual_size, page_size, alloc_granularity, commit_factor, type),
224   _rs(rs),
<span class="line-added">225   _dram_mapper(NULL),</span>
226   _num_committed_dram(0),
227   _num_committed_nvdimm(0),
<span class="line-added">228   _start_index_of_dram(0),</span>
229   _page_size(page_size),
230   _commit_factor(commit_factor),
231   _type(type) {
232   assert(actual_size == 2 * MaxHeapSize, &quot;For 2-way heterogenuous heap, reserved space is two times MaxHeapSize&quot;);
233 }
234 
235 bool G1RegionToHeteroSpaceMapper::initialize() {
236   // Since we need to re-map the reserved space - &#39;Xmx&#39; to nv-dimm and &#39;Xmx&#39; to dram, we need to release the reserved memory first.
237   // Because on some OSes (e.g. Windows) you cannot do a file mapping on memory reserved with regular mapping.
238   os::release_memory(_rs.base(), _rs.size());
239   // First half of size Xmx is for nv-dimm.
240   ReservedSpace rs_nvdimm = _rs.first_part(MaxHeapSize);
241   assert(rs_nvdimm.base() == _rs.base(), &quot;We should get the same base address&quot;);
242 
243   // Second half of reserved memory is mapped to dram.
244   ReservedSpace rs_dram = _rs.last_part(MaxHeapSize);
245 
246   assert(rs_dram.size() == rs_nvdimm.size() &amp;&amp; rs_nvdimm.size() == MaxHeapSize, &quot;They all should be same&quot;);
247 
248   // Reserve dram memory
</pre>
<hr />
<pre>
250   if (base != rs_dram.base()) {
251     if (base != NULL) {
252       os::release_memory(base, rs_dram.size());
253     }
254     log_error(gc, init)(&quot;Error in re-mapping memory on dram during G1 heterogenous memory initialization&quot;);
255     return false;
256   }
257 
258   // We reserve and commit this entire space to NV-DIMM.
259   if (!map_nvdimm_space(rs_nvdimm)) {
260     log_error(gc, init)(&quot;Error in re-mapping memory to nv-dimm during G1 heterogenous memory initialization&quot;);
261     return false;
262   }
263 
264   if (_region_granularity &gt;= (_page_size * _commit_factor)) {
265     _dram_mapper = new G1RegionsLargerThanCommitSizeMapper(rs_dram, rs_dram.size(), _page_size, _region_granularity, _commit_factor, _type);
266   } else {
267     _dram_mapper = new G1RegionsSmallerThanCommitSizeMapper(rs_dram, rs_dram.size(), _page_size, _region_granularity, _commit_factor, _type);
268   }
269 

270   _start_index_of_dram = (uint)(rs_nvdimm.size() / _region_granularity);
271   return true;
272 }
273 
274 void G1RegionToHeteroSpaceMapper::commit_regions(uint start_idx, size_t num_regions, WorkGang* pretouch_gang) {
275   uint end_idx = (start_idx + (uint)num_regions - 1);
276 
277   uint num_dram = end_idx &gt;= _start_index_of_dram ? MIN2((end_idx - _start_index_of_dram + 1), (uint)num_regions) : 0;
278   uint num_nvdimm = (uint)num_regions - num_dram;
279 
280   if (num_nvdimm &gt; 0) {
281     // We do not need to commit nv-dimm regions, since they are committed in the beginning.
282     _num_committed_nvdimm += num_nvdimm;
283   }
284   if (num_dram &gt; 0) {
285     _dram_mapper-&gt;commit_regions(start_idx &gt; _start_index_of_dram ? (start_idx - _start_index_of_dram) : 0, num_dram, pretouch_gang);
286     _num_committed_dram += num_dram;
287   }
288 }
289 
</pre>
</td>
</tr>
</table>
<center><a href="g1RegionMarkStatsCache.inline.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1RegionToSpaceMapper.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>