<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/c2/g1BarrierSetC2.cpp</title>
    <link rel="stylesheet" href="../../../../../../style.css" />
  </head>
<body>
<center><a href="../../epsilon/epsilon_globals.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../index.html" target="_top">index</a> <a href="../g1AllocRegion.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/c2/g1BarrierSetC2.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2018, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
</pre>
<hr />
<pre>
281  * G1 similar to any GC with a Young Generation requires a way to keep track of
282  * references from Old Generation to Young Generation to make sure all live
283  * objects are found. G1 also requires to keep track of object references
284  * between different regions to enable evacuation of old regions, which is done
285  * as part of mixed collections. References are tracked in remembered sets and
286  * is continuously updated as reference are written to with the help of the
287  * post-barrier.
288  *
289  * To reduce the number of updates to the remembered set the post-barrier
290  * filters updates to fields in objects located in the Young Generation,
291  * the same region as the reference, when the NULL is being written or
292  * if the card is already marked as dirty by an earlier write.
293  *
294  * Under certain circumstances it is possible to avoid generating the
295  * post-barrier completely if it is possible during compile time to prove
296  * the object is newly allocated and that no safepoint exists between the
297  * allocation and the store.
298  *
299  * In the case of slow allocation the allocation code must handle the barrier
300  * as part of the allocation in the case the allocated object is not located
<span class="line-modified">301  * in the nursery, this would happen for humongous objects. This is similar to</span>
<span class="line-removed">302  * how CMS is required to handle this case, see the comments for the method</span>
<span class="line-removed">303  * CollectedHeap::new_deferred_store_barrier and OptoRuntime::new_deferred_store_barrier.</span>
<span class="line-removed">304  * A deferred card mark is required for these objects and handled in the above</span>
<span class="line-removed">305  * mentioned methods.</span>
306  *
307  * Returns true if the post barrier can be removed
308  */
309 bool G1BarrierSetC2::g1_can_remove_post_barrier(GraphKit* kit,
310                                                 PhaseTransform* phase, Node* store,
311                                                 Node* adr) const {
312   intptr_t      offset = 0;
313   Node*         base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
314   AllocateNode* alloc  = AllocateNode::Ideal_allocation(base, phase);
315 
316   if (offset == Type::OffsetBot) {
317     return false; // cannot unalias unless there are precise offsets
318   }
319 
320   if (alloc == NULL) {
321      return false; // No allocation found
322   }
323 
324   // Start search from Store node
325   Node* mem = store-&gt;in(MemNode::Control);
</pre>
<hr />
<pre>
400     return;
401   }
402 
403   if (use_ReduceInitialCardMarks()
404       &amp;&amp; g1_can_remove_post_barrier(kit, &amp;kit-&gt;gvn(), oop_store, adr)) {
405     return;
406   }
407 
408   if (!use_precise) {
409     // All card marks for a (non-array) instance are in one place:
410     adr = obj;
411   }
412   // (Else it&#39;s an array (or unknown), and we want more precise card marks.)
413   assert(adr != NULL, &quot;&quot;);
414 
415   IdealKit ideal(kit, true);
416 
417   Node* tls = __ thread(); // ThreadLocalStorage
418 
419   Node* no_base = __ top();
<span class="line-modified">420   float unlikely  = PROB_UNLIKELY(0.999);</span>

421   Node* young_card = __ ConI((jint)G1CardTable::g1_young_card_val());
422   Node* dirty_card = __ ConI((jint)G1CardTable::dirty_card_val());
423   Node* zeroX = __ ConX(0);
424 
425   const TypeFunc *tf = write_ref_field_post_entry_Type();
426 
427   // Offsets into the thread
428   const int index_offset  = in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset());
429   const int buffer_offset = in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset());
430 
431   // Pointers into the thread
432 
433   Node* buffer_adr = __ AddP(no_base, tls, __ ConX(buffer_offset));
434   Node* index_adr =  __ AddP(no_base, tls, __ ConX(index_offset));
435 
436   // Now some values
437   // Use ctrl to avoid hoisting these values past a safepoint, which could
438   // potentially reset these fields in the JavaThread.
439   Node* index  = __ load(__ ctrl(), index_adr, TypeX_X, TypeX_X-&gt;basic_type(), Compile::AliasIdxRaw);
440   Node* buffer = __ load(__ ctrl(), buffer_adr, TypeRawPtr::NOTNULL, T_ADDRESS, Compile::AliasIdxRaw);
</pre>
<hr />
<pre>
443   // Must use ctrl to prevent &quot;integerized oop&quot; existing across safepoint
444   Node* cast =  __ CastPX(__ ctrl(), adr);
445 
446   // Divide pointer by card size
447   Node* card_offset = __ URShiftX( cast, __ ConI(CardTable::card_shift) );
448 
449   // Combine card table base and card offset
450   Node* card_adr = __ AddP(no_base, byte_map_base_node(kit), card_offset );
451 
452   // If we know the value being stored does it cross regions?
453 
454   if (val != NULL) {
455     // Does the store cause us to cross regions?
456 
457     // Should be able to do an unsigned compare of region_size instead of
458     // and extra shift. Do we have an unsigned compare??
459     // Node* region_size = __ ConI(1 &lt;&lt; HeapRegion::LogOfHRGrainBytes);
460     Node* xor_res =  __ URShiftX ( __ XorX( cast,  __ CastPX(__ ctrl(), val)), __ ConI(HeapRegion::LogOfHRGrainBytes));
461 
462     // if (xor_res == 0) same region so skip
<span class="line-modified">463     __ if_then(xor_res, BoolTest::ne, zeroX); {</span>
464 
465       // No barrier if we are storing a NULL
<span class="line-modified">466       __ if_then(val, BoolTest::ne, kit-&gt;null(), unlikely); {</span>
467 
468         // Ok must mark the card if not already dirty
469 
470         // load the original value of the card
471         Node* card_val = __ load(__ ctrl(), card_adr, TypeInt::INT, T_BYTE, Compile::AliasIdxRaw);
472 
<span class="line-modified">473         __ if_then(card_val, BoolTest::ne, young_card); {</span>
474           kit-&gt;sync_kit(ideal);
475           kit-&gt;insert_mem_bar(Op_MemBarVolatile, oop_store);
476           __ sync_kit(kit);
477 
478           Node* card_val_reload = __ load(__ ctrl(), card_adr, TypeInt::INT, T_BYTE, Compile::AliasIdxRaw);
479           __ if_then(card_val_reload, BoolTest::ne, dirty_card); {
480             g1_mark_card(kit, ideal, card_adr, oop_store, alias_idx, index, index_adr, buffer, tf);
481           } __ end_if();
482         } __ end_if();
483       } __ end_if();
484     } __ end_if();
485   } else {
486     // The Object.clone() intrinsic uses this path if !ReduceInitialCardMarks.
487     // We don&#39;t need a barrier here if the destination is a newly allocated object
488     // in Eden. Otherwise, GC verification breaks because we assume that cards in Eden
489     // are set to &#39;g1_young_gen&#39; (see G1CardTable::verify_g1_young_region()).
490     assert(!use_ReduceInitialCardMarks(), &quot;can only happen with card marking&quot;);
491     Node* card_val = __ load(__ ctrl(), card_adr, TypeInt::INT, T_BYTE, Compile::AliasIdxRaw);
492     __ if_then(card_val, BoolTest::ne, young_card); {
493       g1_mark_card(kit, ideal, card_adr, oop_store, alias_idx, index, index_adr, buffer, tf);
</pre>
<hr />
<pre>
583           // across safepoint since GC can change its value.
584           kit-&gt;insert_mem_bar(Op_MemBarCPUOrder);
585         }
586         // Update IdealKit from graphKit.
587         __ sync_kit(kit);
588 
589       } __ end_if(); // _ref_type != ref_none
590   } __ end_if(); // offset == referent_offset
591 
592   // Final sync IdealKit and GraphKit.
593   kit-&gt;final_sync(ideal);
594 }
595 
596 #undef __
597 
598 Node* G1BarrierSetC2::load_at_resolved(C2Access&amp; access, const Type* val_type) const {
599   DecoratorSet decorators = access.decorators();
600   Node* adr = access.addr().node();
601   Node* obj = access.base();
602 

603   bool mismatched = (decorators &amp; C2_MISMATCHED) != 0;
604   bool unknown = (decorators &amp; ON_UNKNOWN_OOP_REF) != 0;
605   bool in_heap = (decorators &amp; IN_HEAP) != 0;

606   bool on_weak = (decorators &amp; ON_WEAK_OOP_REF) != 0;
607   bool is_unordered = (decorators &amp; MO_UNORDERED) != 0;
<span class="line-modified">608   bool need_cpu_mem_bar = !is_unordered || mismatched || !in_heap;</span>

609 
610   Node* top = Compile::current()-&gt;top();
611   Node* offset = adr-&gt;is_AddP() ? adr-&gt;in(AddPNode::Offset) : top;
612   Node* load = CardTableBarrierSetC2::load_at_resolved(access, val_type);
613 
614   // If we are reading the value of the referent field of a Reference
615   // object (either by using Unsafe directly or through reflection)
616   // then, if G1 is enabled, we need to record the referent in an
617   // SATB log buffer using the pre-barrier mechanism.
618   // Also we need to add memory barrier to prevent commoning reads
619   // from this field across safepoint since GC can change its value.
620   bool need_read_barrier = in_heap &amp;&amp; (on_weak ||
621                                        (unknown &amp;&amp; offset != top &amp;&amp; obj != top));
622 
623   if (!access.is_oop() || !need_read_barrier) {
624     return load;
625   }
626 
627   assert(access.is_parse_access(), &quot;entry not supported at optimization time&quot;);
628   C2ParseAccess&amp; parse_access = static_cast&lt;C2ParseAccess&amp;&gt;(access);
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2018, 2019, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
</pre>
<hr />
<pre>
281  * G1 similar to any GC with a Young Generation requires a way to keep track of
282  * references from Old Generation to Young Generation to make sure all live
283  * objects are found. G1 also requires to keep track of object references
284  * between different regions to enable evacuation of old regions, which is done
285  * as part of mixed collections. References are tracked in remembered sets and
286  * is continuously updated as reference are written to with the help of the
287  * post-barrier.
288  *
289  * To reduce the number of updates to the remembered set the post-barrier
290  * filters updates to fields in objects located in the Young Generation,
291  * the same region as the reference, when the NULL is being written or
292  * if the card is already marked as dirty by an earlier write.
293  *
294  * Under certain circumstances it is possible to avoid generating the
295  * post-barrier completely if it is possible during compile time to prove
296  * the object is newly allocated and that no safepoint exists between the
297  * allocation and the store.
298  *
299  * In the case of slow allocation the allocation code must handle the barrier
300  * as part of the allocation in the case the allocated object is not located
<span class="line-modified">301  * in the nursery; this would happen for humongous objects.</span>




302  *
303  * Returns true if the post barrier can be removed
304  */
305 bool G1BarrierSetC2::g1_can_remove_post_barrier(GraphKit* kit,
306                                                 PhaseTransform* phase, Node* store,
307                                                 Node* adr) const {
308   intptr_t      offset = 0;
309   Node*         base   = AddPNode::Ideal_base_and_offset(adr, phase, offset);
310   AllocateNode* alloc  = AllocateNode::Ideal_allocation(base, phase);
311 
312   if (offset == Type::OffsetBot) {
313     return false; // cannot unalias unless there are precise offsets
314   }
315 
316   if (alloc == NULL) {
317      return false; // No allocation found
318   }
319 
320   // Start search from Store node
321   Node* mem = store-&gt;in(MemNode::Control);
</pre>
<hr />
<pre>
396     return;
397   }
398 
399   if (use_ReduceInitialCardMarks()
400       &amp;&amp; g1_can_remove_post_barrier(kit, &amp;kit-&gt;gvn(), oop_store, adr)) {
401     return;
402   }
403 
404   if (!use_precise) {
405     // All card marks for a (non-array) instance are in one place:
406     adr = obj;
407   }
408   // (Else it&#39;s an array (or unknown), and we want more precise card marks.)
409   assert(adr != NULL, &quot;&quot;);
410 
411   IdealKit ideal(kit, true);
412 
413   Node* tls = __ thread(); // ThreadLocalStorage
414 
415   Node* no_base = __ top();
<span class="line-modified">416   float likely = PROB_LIKELY_MAG(3);</span>
<span class="line-added">417   float unlikely = PROB_UNLIKELY_MAG(3);</span>
418   Node* young_card = __ ConI((jint)G1CardTable::g1_young_card_val());
419   Node* dirty_card = __ ConI((jint)G1CardTable::dirty_card_val());
420   Node* zeroX = __ ConX(0);
421 
422   const TypeFunc *tf = write_ref_field_post_entry_Type();
423 
424   // Offsets into the thread
425   const int index_offset  = in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset());
426   const int buffer_offset = in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset());
427 
428   // Pointers into the thread
429 
430   Node* buffer_adr = __ AddP(no_base, tls, __ ConX(buffer_offset));
431   Node* index_adr =  __ AddP(no_base, tls, __ ConX(index_offset));
432 
433   // Now some values
434   // Use ctrl to avoid hoisting these values past a safepoint, which could
435   // potentially reset these fields in the JavaThread.
436   Node* index  = __ load(__ ctrl(), index_adr, TypeX_X, TypeX_X-&gt;basic_type(), Compile::AliasIdxRaw);
437   Node* buffer = __ load(__ ctrl(), buffer_adr, TypeRawPtr::NOTNULL, T_ADDRESS, Compile::AliasIdxRaw);
</pre>
<hr />
<pre>
440   // Must use ctrl to prevent &quot;integerized oop&quot; existing across safepoint
441   Node* cast =  __ CastPX(__ ctrl(), adr);
442 
443   // Divide pointer by card size
444   Node* card_offset = __ URShiftX( cast, __ ConI(CardTable::card_shift) );
445 
446   // Combine card table base and card offset
447   Node* card_adr = __ AddP(no_base, byte_map_base_node(kit), card_offset );
448 
449   // If we know the value being stored does it cross regions?
450 
451   if (val != NULL) {
452     // Does the store cause us to cross regions?
453 
454     // Should be able to do an unsigned compare of region_size instead of
455     // and extra shift. Do we have an unsigned compare??
456     // Node* region_size = __ ConI(1 &lt;&lt; HeapRegion::LogOfHRGrainBytes);
457     Node* xor_res =  __ URShiftX ( __ XorX( cast,  __ CastPX(__ ctrl(), val)), __ ConI(HeapRegion::LogOfHRGrainBytes));
458 
459     // if (xor_res == 0) same region so skip
<span class="line-modified">460     __ if_then(xor_res, BoolTest::ne, zeroX, likely); {</span>
461 
462       // No barrier if we are storing a NULL
<span class="line-modified">463       __ if_then(val, BoolTest::ne, kit-&gt;null(), likely); {</span>
464 
465         // Ok must mark the card if not already dirty
466 
467         // load the original value of the card
468         Node* card_val = __ load(__ ctrl(), card_adr, TypeInt::INT, T_BYTE, Compile::AliasIdxRaw);
469 
<span class="line-modified">470         __ if_then(card_val, BoolTest::ne, young_card, unlikely); {</span>
471           kit-&gt;sync_kit(ideal);
472           kit-&gt;insert_mem_bar(Op_MemBarVolatile, oop_store);
473           __ sync_kit(kit);
474 
475           Node* card_val_reload = __ load(__ ctrl(), card_adr, TypeInt::INT, T_BYTE, Compile::AliasIdxRaw);
476           __ if_then(card_val_reload, BoolTest::ne, dirty_card); {
477             g1_mark_card(kit, ideal, card_adr, oop_store, alias_idx, index, index_adr, buffer, tf);
478           } __ end_if();
479         } __ end_if();
480       } __ end_if();
481     } __ end_if();
482   } else {
483     // The Object.clone() intrinsic uses this path if !ReduceInitialCardMarks.
484     // We don&#39;t need a barrier here if the destination is a newly allocated object
485     // in Eden. Otherwise, GC verification breaks because we assume that cards in Eden
486     // are set to &#39;g1_young_gen&#39; (see G1CardTable::verify_g1_young_region()).
487     assert(!use_ReduceInitialCardMarks(), &quot;can only happen with card marking&quot;);
488     Node* card_val = __ load(__ ctrl(), card_adr, TypeInt::INT, T_BYTE, Compile::AliasIdxRaw);
489     __ if_then(card_val, BoolTest::ne, young_card); {
490       g1_mark_card(kit, ideal, card_adr, oop_store, alias_idx, index, index_adr, buffer, tf);
</pre>
<hr />
<pre>
580           // across safepoint since GC can change its value.
581           kit-&gt;insert_mem_bar(Op_MemBarCPUOrder);
582         }
583         // Update IdealKit from graphKit.
584         __ sync_kit(kit);
585 
586       } __ end_if(); // _ref_type != ref_none
587   } __ end_if(); // offset == referent_offset
588 
589   // Final sync IdealKit and GraphKit.
590   kit-&gt;final_sync(ideal);
591 }
592 
593 #undef __
594 
595 Node* G1BarrierSetC2::load_at_resolved(C2Access&amp; access, const Type* val_type) const {
596   DecoratorSet decorators = access.decorators();
597   Node* adr = access.addr().node();
598   Node* obj = access.base();
599 
<span class="line-added">600   bool anonymous = (decorators &amp; C2_UNSAFE_ACCESS) != 0;</span>
601   bool mismatched = (decorators &amp; C2_MISMATCHED) != 0;
602   bool unknown = (decorators &amp; ON_UNKNOWN_OOP_REF) != 0;
603   bool in_heap = (decorators &amp; IN_HEAP) != 0;
<span class="line-added">604   bool in_native = (decorators &amp; IN_NATIVE) != 0;</span>
605   bool on_weak = (decorators &amp; ON_WEAK_OOP_REF) != 0;
606   bool is_unordered = (decorators &amp; MO_UNORDERED) != 0;
<span class="line-modified">607   bool is_mixed = !in_heap &amp;&amp; !in_native;</span>
<span class="line-added">608   bool need_cpu_mem_bar = !is_unordered || mismatched || is_mixed;</span>
609 
610   Node* top = Compile::current()-&gt;top();
611   Node* offset = adr-&gt;is_AddP() ? adr-&gt;in(AddPNode::Offset) : top;
612   Node* load = CardTableBarrierSetC2::load_at_resolved(access, val_type);
613 
614   // If we are reading the value of the referent field of a Reference
615   // object (either by using Unsafe directly or through reflection)
616   // then, if G1 is enabled, we need to record the referent in an
617   // SATB log buffer using the pre-barrier mechanism.
618   // Also we need to add memory barrier to prevent commoning reads
619   // from this field across safepoint since GC can change its value.
620   bool need_read_barrier = in_heap &amp;&amp; (on_weak ||
621                                        (unknown &amp;&amp; offset != top &amp;&amp; obj != top));
622 
623   if (!access.is_oop() || !need_read_barrier) {
624     return load;
625   }
626 
627   assert(access.is_parse_access(), &quot;entry not supported at optimization time&quot;);
628   C2ParseAccess&amp; parse_access = static_cast&lt;C2ParseAccess&amp;&gt;(access);
</pre>
</td>
</tr>
</table>
<center><a href="../../epsilon/epsilon_globals.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../index.html" target="_top">index</a> <a href="../g1AllocRegion.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>