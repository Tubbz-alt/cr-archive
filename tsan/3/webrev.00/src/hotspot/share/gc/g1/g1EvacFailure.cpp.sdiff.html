<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1EvacFailure.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1EdenRegions.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1EvacFailure.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1EvacFailure.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
 27 #include &quot;gc/g1/g1CollectorState.hpp&quot;
 28 #include &quot;gc/g1/g1ConcurrentMark.inline.hpp&quot;
<span class="line-removed"> 29 #include &quot;gc/g1/g1DirtyCardQueue.hpp&quot;</span>
 30 #include &quot;gc/g1/g1EvacFailure.hpp&quot;
 31 #include &quot;gc/g1/g1HeapVerifier.hpp&quot;
 32 #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
<span class="line-modified"> 33 #include &quot;gc/g1/g1_globals.hpp&quot;</span>
 34 #include &quot;gc/g1/heapRegion.hpp&quot;
 35 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
 36 #include &quot;gc/shared/preservedMarks.inline.hpp&quot;
 37 #include &quot;oops/access.inline.hpp&quot;
 38 #include &quot;oops/compressedOops.inline.hpp&quot;
 39 #include &quot;oops/oop.inline.hpp&quot;
 40 
<span class="line-modified"> 41 class UpdateRSetDeferred : public BasicOopIterateClosure {</span>
 42 private:
 43   G1CollectedHeap* _g1h;
<span class="line-modified"> 44   G1DirtyCardQueue* _dcq;</span>
 45   G1CardTable*    _ct;
 46 




 47 public:
<span class="line-modified"> 48   UpdateRSetDeferred(G1DirtyCardQueue* dcq) :</span>
<span class="line-modified"> 49     _g1h(G1CollectedHeap::heap()), _dcq(dcq), _ct(_g1h-&gt;card_table()) {}</span>
 50 
 51   virtual void do_oop(narrowOop* p) { do_oop_work(p); }
 52   virtual void do_oop(      oop* p) { do_oop_work(p); }
 53   template &lt;class T&gt; void do_oop_work(T* p) {
 54     assert(_g1h-&gt;heap_region_containing(p)-&gt;is_in_reserved(p), &quot;paranoia&quot;);
 55     assert(!_g1h-&gt;heap_region_containing(p)-&gt;is_survivor(), &quot;Unexpected evac failure in survivor region&quot;);
 56 
 57     T const o = RawAccess&lt;&gt;::oop_load(p);
 58     if (CompressedOops::is_null(o)) {
 59       return;
 60     }
 61 
 62     if (HeapRegion::is_in_same_region(p, CompressedOops::decode(o))) {
 63       return;
 64     }
 65     size_t card_index = _ct-&gt;index_for(p);
<span class="line-modified"> 66     if (_ct-&gt;mark_card_deferred(card_index)) {</span>
<span class="line-modified"> 67       _dcq-&gt;enqueue(_ct-&gt;byte_for_index(card_index));</span>

 68     }
 69   }
 70 };
 71 
 72 class RemoveSelfForwardPtrObjClosure: public ObjectClosure {
 73   G1CollectedHeap* _g1h;
 74   G1ConcurrentMark* _cm;
 75   HeapRegion* _hr;
 76   size_t _marked_bytes;
<span class="line-modified"> 77   UpdateRSetDeferred* _update_rset_cl;</span>
 78   bool _during_initial_mark;
 79   uint _worker_id;
 80   HeapWord* _last_forwarded_object_end;
 81 
 82 public:
 83   RemoveSelfForwardPtrObjClosure(HeapRegion* hr,
<span class="line-modified"> 84                                  UpdateRSetDeferred* update_rset_cl,</span>
 85                                  bool during_initial_mark,
 86                                  uint worker_id) :
 87     _g1h(G1CollectedHeap::heap()),
 88     _cm(_g1h-&gt;concurrent_mark()),
 89     _hr(hr),
 90     _marked_bytes(0),
<span class="line-modified"> 91     _update_rset_cl(update_rset_cl),</span>
 92     _during_initial_mark(during_initial_mark),
 93     _worker_id(worker_id),
 94     _last_forwarded_object_end(hr-&gt;bottom()) { }
 95 
 96   size_t marked_bytes() { return _marked_bytes; }
 97 
 98   // Iterate over the live objects in the region to find self-forwarded objects
 99   // that need to be kept live. We need to update the remembered sets of these
100   // objects. Further update the BOT and marks.
101   // We can coalesce and overwrite the remaining heap contents with dummy objects
102   // as they have either been dead or evacuated (which are unreferenced now, i.e.
103   // dead too) already.
104   void do_object(oop obj) {
<span class="line-modified">105     HeapWord* obj_addr = (HeapWord*) obj;</span>
106     assert(_hr-&gt;is_in(obj_addr), &quot;sanity&quot;);
107 
108     if (obj-&gt;is_forwarded() &amp;&amp; obj-&gt;forwardee() == obj) {
109       // The object failed to move.
110 
111       zap_dead_objects(_last_forwarded_object_end, obj_addr);
112       // We consider all objects that we find self-forwarded to be
113       // live. What we&#39;ll do is that we&#39;ll update the prev marking
114       // info so that they are all under PTAMS and explicitly marked.
115       if (!_cm-&gt;is_marked_in_prev_bitmap(obj)) {
116         _cm-&gt;mark_in_prev_bitmap(obj);
117       }
118       if (_during_initial_mark) {
119         // For the next marking info we&#39;ll only mark the
120         // self-forwarded objects explicitly if we are during
121         // initial-mark (since, normally, we only mark objects pointed
122         // to by roots if we succeed in copying them). By marking all
123         // self-forwarded objects we ensure that we mark any that are
124         // still pointed to be roots. During concurrent marking, and
125         // after initial-mark, we don&#39;t need to mark any objects
</pre>
<hr />
<pre>
128         // we&#39;ll leave them over NTAMS.
129         _cm-&gt;mark_in_next_bitmap(_worker_id, _hr, obj);
130       }
131       size_t obj_size = obj-&gt;size();
132 
133       _marked_bytes += (obj_size * HeapWordSize);
134       PreservedMarks::init_forwarded_mark(obj);
135 
136       // While we were processing RSet buffers during the collection,
137       // we actually didn&#39;t scan any cards on the collection set,
138       // since we didn&#39;t want to update remembered sets with entries
139       // that point into the collection set, given that live objects
140       // from the collection set are about to move and such entries
141       // will be stale very soon.
142       // This change also dealt with a reliability issue which
143       // involved scanning a card in the collection set and coming
144       // across an array that was being chunked and looking malformed.
145       // The problem is that, if evacuation fails, we might have
146       // remembered set entries missing given that we skipped cards on
147       // the collection set. So, we&#39;ll recreate such entries now.
<span class="line-modified">148       obj-&gt;oop_iterate(_update_rset_cl);</span>
149 
150       HeapWord* obj_end = obj_addr + obj_size;
151       _last_forwarded_object_end = obj_end;
152       _hr-&gt;cross_threshold(obj_addr, obj_end);
153     }
154   }
155 
156   // Fill the memory area from start to end with filler objects, and update the BOT
157   // and the mark bitmap accordingly.
158   void zap_dead_objects(HeapWord* start, HeapWord* end) {
159     if (start == end) {
160       return;
161     }
162 
163     size_t gap_size = pointer_delta(end, start);
164     MemRegion mr(start, gap_size);
165     if (gap_size &gt;= CollectedHeap::min_fill_size()) {
166       CollectedHeap::fill_with_objects(start, gap_size);
167 
168       HeapWord* end_first_obj = start + ((oop)start)-&gt;size();
</pre>
<hr />
<pre>
177         size_t size_second_obj = ((oop)end_first_obj)-&gt;size();
178         HeapWord* end_of_second_obj = end_first_obj + size_second_obj;
179         assert(end == end_of_second_obj,
180                &quot;More than two objects were used to fill the area from &quot; PTR_FORMAT &quot; to &quot; PTR_FORMAT &quot;, &quot;
181                &quot;second objects size &quot; SIZE_FORMAT &quot; ends at &quot; PTR_FORMAT,
182                p2i(start), p2i(end), size_second_obj, p2i(end_of_second_obj));
183 #endif
184       }
185     }
186     _cm-&gt;clear_range_in_prev_bitmap(mr);
187   }
188 
189   void zap_remainder() {
190     zap_dead_objects(_last_forwarded_object_end, _hr-&gt;top());
191   }
192 };
193 
194 class RemoveSelfForwardPtrHRClosure: public HeapRegionClosure {
195   G1CollectedHeap* _g1h;
196   uint _worker_id;
<span class="line-removed">197   HeapRegionClaimer* _hrclaimer;</span>
198 
<span class="line-modified">199   G1DirtyCardQueue _dcq;</span>
<span class="line-modified">200   UpdateRSetDeferred _update_rset_cl;</span>
201 
202 public:
<span class="line-modified">203   RemoveSelfForwardPtrHRClosure(uint worker_id,</span>
<span class="line-removed">204                                 HeapRegionClaimer* hrclaimer) :</span>
205     _g1h(G1CollectedHeap::heap()),
206     _worker_id(worker_id),
<span class="line-modified">207     _hrclaimer(hrclaimer),</span>
<span class="line-modified">208     _dcq(&amp;_g1h-&gt;dirty_card_queue_set()),</span>
<span class="line-removed">209     _update_rset_cl(&amp;_dcq){</span>
210   }
211 
212   size_t remove_self_forward_ptr_by_walking_hr(HeapRegion* hr,
213                                                bool during_initial_mark) {
214     RemoveSelfForwardPtrObjClosure rspc(hr,
<span class="line-modified">215                                         &amp;_update_rset_cl,</span>
216                                         during_initial_mark,
217                                         _worker_id);
218     hr-&gt;object_iterate(&amp;rspc);
219     // Need to zap the remainder area of the processed region.
220     rspc.zap_remainder();
221 
222     return rspc.marked_bytes();
223   }
224 
225   bool do_heap_region(HeapRegion *hr) {
226     assert(!hr-&gt;is_pinned(), &quot;Unexpected pinned region at index %u&quot;, hr-&gt;hrm_index());
227     assert(hr-&gt;in_collection_set(), &quot;bad CS&quot;);
228 
<span class="line-modified">229     if (_hrclaimer-&gt;claim_region(hr-&gt;hrm_index())) {</span>
<span class="line-modified">230       if (hr-&gt;evacuation_failed()) {</span>
<span class="line-modified">231         bool during_initial_mark = _g1h-&gt;collector_state()-&gt;in_initial_mark_gc();</span>
<span class="line-modified">232         bool during_conc_mark = _g1h-&gt;collector_state()-&gt;mark_or_rebuild_in_progress();</span>

233 
<span class="line-modified">234         hr-&gt;note_self_forwarding_removal_start(during_initial_mark,</span>
235                                                during_conc_mark);
<span class="line-modified">236         _g1h-&gt;verifier()-&gt;check_bitmaps(&quot;Self-Forwarding Ptr Removal&quot;, hr);</span>
237 
<span class="line-modified">238         hr-&gt;reset_bot();</span>
239 
<span class="line-modified">240         size_t live_bytes = remove_self_forward_ptr_by_walking_hr(hr, during_initial_mark);</span>
241 
<span class="line-modified">242         hr-&gt;rem_set()-&gt;clean_strong_code_roots(hr);</span>
<span class="line-modified">243         hr-&gt;rem_set()-&gt;clear_locked(true);</span>
244 
<span class="line-modified">245         hr-&gt;note_self_forwarding_removal_end(live_bytes);</span>
<span class="line-removed">246       }</span>
247     }
248     return false;
249   }
250 };
251 
<span class="line-modified">252 G1ParRemoveSelfForwardPtrsTask::G1ParRemoveSelfForwardPtrsTask() :</span>
253   AbstractGangTask(&quot;G1 Remove Self-forwarding Pointers&quot;),
254   _g1h(G1CollectedHeap::heap()),

255   _hrclaimer(_g1h-&gt;workers()-&gt;active_workers()) { }
256 
257 void G1ParRemoveSelfForwardPtrsTask::work(uint worker_id) {
<span class="line-modified">258   RemoveSelfForwardPtrHRClosure rsfp_cl(worker_id, &amp;_hrclaimer);</span>
259 
<span class="line-modified">260   _g1h-&gt;collection_set_iterate_from(&amp;rsfp_cl, worker_id);</span>
261 }
</pre>
</td>
<td>
<hr />
<pre>
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
 27 #include &quot;gc/g1/g1CollectorState.hpp&quot;
 28 #include &quot;gc/g1/g1ConcurrentMark.inline.hpp&quot;

 29 #include &quot;gc/g1/g1EvacFailure.hpp&quot;
 30 #include &quot;gc/g1/g1HeapVerifier.hpp&quot;
 31 #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
<span class="line-modified"> 32 #include &quot;gc/g1/g1RedirtyCardsQueue.hpp&quot;</span>
 33 #include &quot;gc/g1/heapRegion.hpp&quot;
 34 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
 35 #include &quot;gc/shared/preservedMarks.inline.hpp&quot;
 36 #include &quot;oops/access.inline.hpp&quot;
 37 #include &quot;oops/compressedOops.inline.hpp&quot;
 38 #include &quot;oops/oop.inline.hpp&quot;
 39 
<span class="line-modified"> 40 class UpdateLogBuffersDeferred : public BasicOopIterateClosure {</span>
 41 private:
 42   G1CollectedHeap* _g1h;
<span class="line-modified"> 43   G1RedirtyCardsQueue* _rdcq;</span>
 44   G1CardTable*    _ct;
 45 
<span class="line-added"> 46   // Remember the last enqueued card to avoid enqueuing the same card over and over;</span>
<span class="line-added"> 47   // since we only ever handle a card once, this is sufficient.</span>
<span class="line-added"> 48   size_t _last_enqueued_card;</span>
<span class="line-added"> 49 </span>
 50 public:
<span class="line-modified"> 51   UpdateLogBuffersDeferred(G1RedirtyCardsQueue* rdcq) :</span>
<span class="line-modified"> 52     _g1h(G1CollectedHeap::heap()), _rdcq(rdcq), _ct(_g1h-&gt;card_table()), _last_enqueued_card(SIZE_MAX) {}</span>
 53 
 54   virtual void do_oop(narrowOop* p) { do_oop_work(p); }
 55   virtual void do_oop(      oop* p) { do_oop_work(p); }
 56   template &lt;class T&gt; void do_oop_work(T* p) {
 57     assert(_g1h-&gt;heap_region_containing(p)-&gt;is_in_reserved(p), &quot;paranoia&quot;);
 58     assert(!_g1h-&gt;heap_region_containing(p)-&gt;is_survivor(), &quot;Unexpected evac failure in survivor region&quot;);
 59 
 60     T const o = RawAccess&lt;&gt;::oop_load(p);
 61     if (CompressedOops::is_null(o)) {
 62       return;
 63     }
 64 
 65     if (HeapRegion::is_in_same_region(p, CompressedOops::decode(o))) {
 66       return;
 67     }
 68     size_t card_index = _ct-&gt;index_for(p);
<span class="line-modified"> 69     if (card_index != _last_enqueued_card) {</span>
<span class="line-modified"> 70       _rdcq-&gt;enqueue(_ct-&gt;byte_for_index(card_index));</span>
<span class="line-added"> 71       _last_enqueued_card = card_index;</span>
 72     }
 73   }
 74 };
 75 
 76 class RemoveSelfForwardPtrObjClosure: public ObjectClosure {
 77   G1CollectedHeap* _g1h;
 78   G1ConcurrentMark* _cm;
 79   HeapRegion* _hr;
 80   size_t _marked_bytes;
<span class="line-modified"> 81   UpdateLogBuffersDeferred* _log_buffer_cl;</span>
 82   bool _during_initial_mark;
 83   uint _worker_id;
 84   HeapWord* _last_forwarded_object_end;
 85 
 86 public:
 87   RemoveSelfForwardPtrObjClosure(HeapRegion* hr,
<span class="line-modified"> 88                                  UpdateLogBuffersDeferred* log_buffer_cl,</span>
 89                                  bool during_initial_mark,
 90                                  uint worker_id) :
 91     _g1h(G1CollectedHeap::heap()),
 92     _cm(_g1h-&gt;concurrent_mark()),
 93     _hr(hr),
 94     _marked_bytes(0),
<span class="line-modified"> 95     _log_buffer_cl(log_buffer_cl),</span>
 96     _during_initial_mark(during_initial_mark),
 97     _worker_id(worker_id),
 98     _last_forwarded_object_end(hr-&gt;bottom()) { }
 99 
100   size_t marked_bytes() { return _marked_bytes; }
101 
102   // Iterate over the live objects in the region to find self-forwarded objects
103   // that need to be kept live. We need to update the remembered sets of these
104   // objects. Further update the BOT and marks.
105   // We can coalesce and overwrite the remaining heap contents with dummy objects
106   // as they have either been dead or evacuated (which are unreferenced now, i.e.
107   // dead too) already.
108   void do_object(oop obj) {
<span class="line-modified">109     HeapWord* obj_addr = cast_from_oop&lt;HeapWord*&gt;(obj);</span>
110     assert(_hr-&gt;is_in(obj_addr), &quot;sanity&quot;);
111 
112     if (obj-&gt;is_forwarded() &amp;&amp; obj-&gt;forwardee() == obj) {
113       // The object failed to move.
114 
115       zap_dead_objects(_last_forwarded_object_end, obj_addr);
116       // We consider all objects that we find self-forwarded to be
117       // live. What we&#39;ll do is that we&#39;ll update the prev marking
118       // info so that they are all under PTAMS and explicitly marked.
119       if (!_cm-&gt;is_marked_in_prev_bitmap(obj)) {
120         _cm-&gt;mark_in_prev_bitmap(obj);
121       }
122       if (_during_initial_mark) {
123         // For the next marking info we&#39;ll only mark the
124         // self-forwarded objects explicitly if we are during
125         // initial-mark (since, normally, we only mark objects pointed
126         // to by roots if we succeed in copying them). By marking all
127         // self-forwarded objects we ensure that we mark any that are
128         // still pointed to be roots. During concurrent marking, and
129         // after initial-mark, we don&#39;t need to mark any objects
</pre>
<hr />
<pre>
132         // we&#39;ll leave them over NTAMS.
133         _cm-&gt;mark_in_next_bitmap(_worker_id, _hr, obj);
134       }
135       size_t obj_size = obj-&gt;size();
136 
137       _marked_bytes += (obj_size * HeapWordSize);
138       PreservedMarks::init_forwarded_mark(obj);
139 
140       // While we were processing RSet buffers during the collection,
141       // we actually didn&#39;t scan any cards on the collection set,
142       // since we didn&#39;t want to update remembered sets with entries
143       // that point into the collection set, given that live objects
144       // from the collection set are about to move and such entries
145       // will be stale very soon.
146       // This change also dealt with a reliability issue which
147       // involved scanning a card in the collection set and coming
148       // across an array that was being chunked and looking malformed.
149       // The problem is that, if evacuation fails, we might have
150       // remembered set entries missing given that we skipped cards on
151       // the collection set. So, we&#39;ll recreate such entries now.
<span class="line-modified">152       obj-&gt;oop_iterate(_log_buffer_cl);</span>
153 
154       HeapWord* obj_end = obj_addr + obj_size;
155       _last_forwarded_object_end = obj_end;
156       _hr-&gt;cross_threshold(obj_addr, obj_end);
157     }
158   }
159 
160   // Fill the memory area from start to end with filler objects, and update the BOT
161   // and the mark bitmap accordingly.
162   void zap_dead_objects(HeapWord* start, HeapWord* end) {
163     if (start == end) {
164       return;
165     }
166 
167     size_t gap_size = pointer_delta(end, start);
168     MemRegion mr(start, gap_size);
169     if (gap_size &gt;= CollectedHeap::min_fill_size()) {
170       CollectedHeap::fill_with_objects(start, gap_size);
171 
172       HeapWord* end_first_obj = start + ((oop)start)-&gt;size();
</pre>
<hr />
<pre>
181         size_t size_second_obj = ((oop)end_first_obj)-&gt;size();
182         HeapWord* end_of_second_obj = end_first_obj + size_second_obj;
183         assert(end == end_of_second_obj,
184                &quot;More than two objects were used to fill the area from &quot; PTR_FORMAT &quot; to &quot; PTR_FORMAT &quot;, &quot;
185                &quot;second objects size &quot; SIZE_FORMAT &quot; ends at &quot; PTR_FORMAT,
186                p2i(start), p2i(end), size_second_obj, p2i(end_of_second_obj));
187 #endif
188       }
189     }
190     _cm-&gt;clear_range_in_prev_bitmap(mr);
191   }
192 
193   void zap_remainder() {
194     zap_dead_objects(_last_forwarded_object_end, _hr-&gt;top());
195   }
196 };
197 
198 class RemoveSelfForwardPtrHRClosure: public HeapRegionClosure {
199   G1CollectedHeap* _g1h;
200   uint _worker_id;

201 
<span class="line-modified">202   G1RedirtyCardsQueue _rdcq;</span>
<span class="line-modified">203   UpdateLogBuffersDeferred _log_buffer_cl;</span>
204 
205 public:
<span class="line-modified">206   RemoveSelfForwardPtrHRClosure(G1RedirtyCardsQueueSet* rdcqs, uint worker_id) :</span>

207     _g1h(G1CollectedHeap::heap()),
208     _worker_id(worker_id),
<span class="line-modified">209     _rdcq(rdcqs),</span>
<span class="line-modified">210     _log_buffer_cl(&amp;_rdcq) {</span>

211   }
212 
213   size_t remove_self_forward_ptr_by_walking_hr(HeapRegion* hr,
214                                                bool during_initial_mark) {
215     RemoveSelfForwardPtrObjClosure rspc(hr,
<span class="line-modified">216                                         &amp;_log_buffer_cl,</span>
217                                         during_initial_mark,
218                                         _worker_id);
219     hr-&gt;object_iterate(&amp;rspc);
220     // Need to zap the remainder area of the processed region.
221     rspc.zap_remainder();
222 
223     return rspc.marked_bytes();
224   }
225 
226   bool do_heap_region(HeapRegion *hr) {
227     assert(!hr-&gt;is_pinned(), &quot;Unexpected pinned region at index %u&quot;, hr-&gt;hrm_index());
228     assert(hr-&gt;in_collection_set(), &quot;bad CS&quot;);
229 
<span class="line-modified">230     if (hr-&gt;evacuation_failed()) {</span>
<span class="line-modified">231       hr-&gt;clear_index_in_opt_cset();</span>
<span class="line-modified">232 </span>
<span class="line-modified">233       bool during_initial_mark = _g1h-&gt;collector_state()-&gt;in_initial_mark_gc();</span>
<span class="line-added">234       bool during_conc_mark = _g1h-&gt;collector_state()-&gt;mark_or_rebuild_in_progress();</span>
235 
<span class="line-modified">236       hr-&gt;note_self_forwarding_removal_start(during_initial_mark,</span>
237                                                during_conc_mark);
<span class="line-modified">238       _g1h-&gt;verifier()-&gt;check_bitmaps(&quot;Self-Forwarding Ptr Removal&quot;, hr);</span>
239 
<span class="line-modified">240       hr-&gt;reset_bot();</span>
241 
<span class="line-modified">242       size_t live_bytes = remove_self_forward_ptr_by_walking_hr(hr, during_initial_mark);</span>
243 
<span class="line-modified">244       hr-&gt;rem_set()-&gt;clean_strong_code_roots(hr);</span>
<span class="line-modified">245       hr-&gt;rem_set()-&gt;clear_locked(true);</span>
246 
<span class="line-modified">247       hr-&gt;note_self_forwarding_removal_end(live_bytes);</span>

248     }
249     return false;
250   }
251 };
252 
<span class="line-modified">253 G1ParRemoveSelfForwardPtrsTask::G1ParRemoveSelfForwardPtrsTask(G1RedirtyCardsQueueSet* rdcqs) :</span>
254   AbstractGangTask(&quot;G1 Remove Self-forwarding Pointers&quot;),
255   _g1h(G1CollectedHeap::heap()),
<span class="line-added">256   _rdcqs(rdcqs),</span>
257   _hrclaimer(_g1h-&gt;workers()-&gt;active_workers()) { }
258 
259 void G1ParRemoveSelfForwardPtrsTask::work(uint worker_id) {
<span class="line-modified">260   RemoveSelfForwardPtrHRClosure rsfp_cl(_rdcqs, worker_id);</span>
261 
<span class="line-modified">262   _g1h-&gt;collection_set_iterate_increment_from(&amp;rsfp_cl, &amp;_hrclaimer, worker_id);</span>
263 }
</pre>
</td>
</tr>
</table>
<center><a href="g1EdenRegions.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1EvacFailure.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>