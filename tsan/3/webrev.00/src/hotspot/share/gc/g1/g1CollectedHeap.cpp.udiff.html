<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Udiff src/hotspot/share/gc/g1/g1CollectedHeap.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1CodeCacheRemSet.cpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.udiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1CollectedHeap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-new-header">@@ -1,7 +1,7 @@</span>
  /*
<span class="udiff-line-modified-removed">-  * Copyright (c) 2001, 2019, Oracle and/or its affiliates. All rights reserved.</span>
<span class="udiff-line-modified-added">+  * Copyright (c) 2001, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -27,14 +27,15 @@</span>
  #include &quot;classfile/metadataOnStackMark.hpp&quot;
  #include &quot;classfile/stringTable.hpp&quot;
  #include &quot;code/codeCache.hpp&quot;
  #include &quot;code/icBuffer.hpp&quot;
  #include &quot;gc/g1/g1Allocator.inline.hpp&quot;
<span class="udiff-line-added">+ #include &quot;gc/g1/g1Arguments.hpp&quot;</span>
  #include &quot;gc/g1/g1BarrierSet.hpp&quot;
<span class="udiff-line-added">+ #include &quot;gc/g1/g1CardTableEntryClosure.hpp&quot;</span>
  #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
  #include &quot;gc/g1/g1CollectionSet.hpp&quot;
<span class="udiff-line-removed">- #include &quot;gc/g1/g1CollectorPolicy.hpp&quot;</span>
  #include &quot;gc/g1/g1CollectorState.hpp&quot;
  #include &quot;gc/g1/g1ConcurrentRefine.hpp&quot;
  #include &quot;gc/g1/g1ConcurrentRefineThread.hpp&quot;
  #include &quot;gc/g1/g1ConcurrentMarkThread.inline.hpp&quot;
  #include &quot;gc/g1/g1DirtyCardQueue.hpp&quot;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -45,19 +46,22 @@</span>
  #include &quot;gc/g1/g1HeapTransition.hpp&quot;
  #include &quot;gc/g1/g1HeapVerifier.hpp&quot;
  #include &quot;gc/g1/g1HotCardCache.hpp&quot;
  #include &quot;gc/g1/g1MemoryPool.hpp&quot;
  #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
<span class="udiff-line-added">+ #include &quot;gc/g1/g1ParallelCleaning.hpp&quot;</span>
  #include &quot;gc/g1/g1ParScanThreadState.inline.hpp&quot;
  #include &quot;gc/g1/g1Policy.hpp&quot;
<span class="udiff-line-added">+ #include &quot;gc/g1/g1RedirtyCardsQueue.hpp&quot;</span>
  #include &quot;gc/g1/g1RegionToSpaceMapper.hpp&quot;
  #include &quot;gc/g1/g1RemSet.hpp&quot;
  #include &quot;gc/g1/g1RootClosures.hpp&quot;
  #include &quot;gc/g1/g1RootProcessor.hpp&quot;
  #include &quot;gc/g1/g1SATBMarkQueueSet.hpp&quot;
  #include &quot;gc/g1/g1StringDedup.hpp&quot;
  #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
<span class="udiff-line-added">+ #include &quot;gc/g1/g1Trace.hpp&quot;</span>
  #include &quot;gc/g1/g1YCTypes.hpp&quot;
  #include &quot;gc/g1/g1YoungRemSetSamplingThread.hpp&quot;
  #include &quot;gc/g1/g1VMOperations.hpp&quot;
  #include &quot;gc/g1/heapRegion.inline.hpp&quot;
  #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -65,26 +69,27 @@</span>
  #include &quot;gc/shared/gcBehaviours.hpp&quot;
  #include &quot;gc/shared/gcHeapSummary.hpp&quot;
  #include &quot;gc/shared/gcId.hpp&quot;
  #include &quot;gc/shared/gcLocker.hpp&quot;
  #include &quot;gc/shared/gcTimer.hpp&quot;
<span class="udiff-line-removed">- #include &quot;gc/shared/gcTrace.hpp&quot;</span>
  #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  #include &quot;gc/shared/generationSpec.hpp&quot;
  #include &quot;gc/shared/isGCActiveMark.hpp&quot;
<span class="udiff-line-added">+ #include &quot;gc/shared/locationPrinter.inline.hpp&quot;</span>
  #include &quot;gc/shared/oopStorageParState.hpp&quot;
<span class="udiff-line-removed">- #include &quot;gc/shared/parallelCleaning.hpp&quot;</span>
  #include &quot;gc/shared/preservedMarks.inline.hpp&quot;
  #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
  #include &quot;gc/shared/referenceProcessor.inline.hpp&quot;
<span class="udiff-line-added">+ #include &quot;gc/shared/taskTerminator.hpp&quot;</span>
  #include &quot;gc/shared/taskqueue.inline.hpp&quot;
  #include &quot;gc/shared/weakProcessor.inline.hpp&quot;
  #include &quot;gc/shared/workerPolicy.hpp&quot;
  #include &quot;logging/log.hpp&quot;
  #include &quot;memory/allocation.hpp&quot;
  #include &quot;memory/iterator.hpp&quot;
  #include &quot;memory/resourceArea.hpp&quot;
<span class="udiff-line-added">+ #include &quot;memory/universe.hpp&quot;</span>
  #include &quot;oops/access.inline.hpp&quot;
  #include &quot;oops/compressedOops.inline.hpp&quot;
  #include &quot;oops/oop.inline.hpp&quot;
  #include &quot;runtime/atomic.hpp&quot;
  #include &quot;runtime/flags/flagSetting.hpp&quot;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -92,10 +97,11 @@</span>
  #include &quot;runtime/init.hpp&quot;
  #include &quot;runtime/orderAccess.hpp&quot;
  #include &quot;runtime/threadSMR.hpp&quot;
  #include &quot;runtime/vmThread.hpp&quot;
  #include &quot;utilities/align.hpp&quot;
<span class="udiff-line-added">+ #include &quot;utilities/bitMap.inline.hpp&quot;</span>
  #include &quot;utilities/globalDefinitions.hpp&quot;
  #include &quot;utilities/stack.inline.hpp&quot;
  
  size_t G1CollectedHeap::_humongous_object_threshold_in_words = 0;
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -126,20 +132,18 @@</span>
  
   public:
    RedirtyLoggedCardTableEntryClosure(G1CollectedHeap* g1h) : G1CardTableEntryClosure(),
      _num_dirtied(0), _g1h(g1h), _g1_ct(g1h-&gt;card_table()) { }
  
<span class="udiff-line-modified-removed">-   bool do_card_ptr(CardValue* card_ptr, uint worker_i) {</span>
<span class="udiff-line-modified-added">+   void do_card_ptr(CardValue* card_ptr, uint worker_id) {</span>
      HeapRegion* hr = region_for_card(card_ptr);
  
      // Should only dirty cards in regions that won&#39;t be freed.
      if (!will_become_free(hr)) {
        *card_ptr = G1CardTable::dirty_card_val();
        _num_dirtied++;
      }
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     return true;</span>
    }
  
    size_t num_dirtied()   const { return _num_dirtied; }
  };
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -152,24 +156,32 @@</span>
    // The from card cache is not the memory that is actually committed. So we cannot
    // take advantage of the zero_filled parameter.
    reset_from_card_cache(start_idx, num_regions);
  }
  
<span class="udiff-line-added">+ Tickspan G1CollectedHeap::run_task(AbstractGangTask* task) {</span>
<span class="udiff-line-added">+   Ticks start = Ticks::now();</span>
<span class="udiff-line-added">+   workers()-&gt;run_task(task, workers()-&gt;active_workers());</span>
<span class="udiff-line-added">+   return Ticks::now() - start;</span>
<span class="udiff-line-added">+ }</span>
  
  HeapRegion* G1CollectedHeap::new_heap_region(uint hrs_index,
                                               MemRegion mr) {
    return new HeapRegion(hrs_index, bot(), mr);
  }
  
  // Private methods.
  
<span class="udiff-line-modified-removed">- HeapRegion* G1CollectedHeap::new_region(size_t word_size, HeapRegionType type, bool do_expand) {</span>
<span class="udiff-line-modified-added">+ HeapRegion* G1CollectedHeap::new_region(size_t word_size,</span>
<span class="udiff-line-added">+                                         HeapRegionType type,</span>
<span class="udiff-line-added">+                                         bool do_expand,</span>
<span class="udiff-line-added">+                                         uint node_index) {</span>
    assert(!is_humongous(word_size) || word_size &lt;= HeapRegion::GrainWords,
           &quot;the only time we use this to allocate a humongous region is &quot;
           &quot;when we are allocating a single humongous region&quot;);
  
<span class="udiff-line-modified-removed">-   HeapRegion* res = _hrm-&gt;allocate_free_region(type);</span>
<span class="udiff-line-modified-added">+   HeapRegion* res = _hrm-&gt;allocate_free_region(type, node_index);</span>
  
    if (res == NULL &amp;&amp; do_expand &amp;&amp; _expand_heap_after_alloc_failure) {
      // Currently, only attempts to allocate GC alloc regions set
      // do_expand to true. So, we should only reach here during a
      // safepoint. If this assumption changes we might have to
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -177,16 +189,19 @@</span>
      assert(SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
  
      log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (region allocation request failed). Allocation request: &quot; SIZE_FORMAT &quot;B&quot;,
                                word_size * HeapWordSize);
  
<span class="udiff-line-modified-removed">-     if (expand(word_size * HeapWordSize)) {</span>
<span class="udiff-line-modified-removed">-       // Given that expand() succeeded in expanding the heap, and we</span>
<span class="udiff-line-modified-added">+     assert(word_size * HeapWordSize &lt; HeapRegion::GrainBytes,</span>
<span class="udiff-line-modified-added">+            &quot;This kind of expansion should never be more than one region. Size: &quot; SIZE_FORMAT,</span>
<span class="udiff-line-added">+            word_size * HeapWordSize);</span>
<span class="udiff-line-added">+     if (expand_single_region(node_index)) {</span>
<span class="udiff-line-added">+       // Given that expand_single_region() succeeded in expanding the heap, and we</span>
        // always expand the heap by an amount aligned to the heap
        // region size, the free list should in theory not be empty.
        // In either case allocate_free_region() will check for NULL.
<span class="udiff-line-modified-removed">-       res = _hrm-&gt;allocate_free_region(type);</span>
<span class="udiff-line-modified-added">+       res = _hrm-&gt;allocate_free_region(type, node_index);</span>
      } else {
        _expand_heap_after_alloc_failure = false;
      }
    }
    return res;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -429,11 +444,11 @@</span>
    for (uint try_count = 1, gclocker_retry_count = 0; /* we&#39;ll return */; try_count += 1) {
      bool should_try_gc;
      uint gc_count_before;
  
      {
<span class="udiff-line-modified-removed">-       MutexLockerEx x(Heap_lock);</span>
<span class="udiff-line-modified-added">+       MutexLocker x(Heap_lock);</span>
        result = _allocator-&gt;attempt_allocation_locked(word_size);
        if (result != NULL) {
          return result;
        }
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -568,11 +583,11 @@</span>
                                              size_t count,
                                              bool open) {
    assert(!is_init_completed(), &quot;Expect to be called at JVM init time&quot;);
    assert(ranges != NULL, &quot;MemRegion array NULL&quot;);
    assert(count != 0, &quot;No MemRegions provided&quot;);
<span class="udiff-line-modified-removed">-   MutexLockerEx x(Heap_lock);</span>
<span class="udiff-line-modified-added">+   MutexLocker x(Heap_lock);</span>
  
    MemRegion reserved = _hrm-&gt;reserved();
    HeapWord* prev_last_addr = NULL;
    HeapRegion* prev_last_region = NULL;
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -655,12 +670,10 @@</span>
        } else {
          top = last_address + 1;
          next_region = NULL;
        }
        curr_region-&gt;set_top(top);
<span class="udiff-line-removed">-       curr_region-&gt;set_first_dead(top);</span>
<span class="udiff-line-removed">-       curr_region-&gt;set_end_of_live(top);</span>
        curr_region = next_region;
      }
  
      // Notify mark-sweep of the archive
      G1ArchiveAllocator::set_range_archive(curr_range, open);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -678,11 +691,11 @@</span>
  
    // For each MemRegion, create filler objects, if needed, in the G1 regions
    // that contain the address range. The address range actually within the
    // MemRegion will not be modified. That is assumed to have been initialized
    // elsewhere, probably via an mmap of archived heap data.
<span class="udiff-line-modified-removed">-   MutexLockerEx x(Heap_lock);</span>
<span class="udiff-line-modified-added">+   MutexLocker x(Heap_lock);</span>
    for (size_t i = 0; i &lt; count; i++) {
      HeapWord* start_address = ranges[i].start();
      HeapWord* last_address = ranges[i].last();
  
      assert(reserved.contains(start_address) &amp;&amp; reserved.contains(last_address),
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -752,11 +765,11 @@</span>
    }
  
    return result;
  }
  
<span class="udiff-line-modified-removed">- void G1CollectedHeap::dealloc_archive_regions(MemRegion* ranges, size_t count, bool is_open) {</span>
<span class="udiff-line-modified-added">+ void G1CollectedHeap::dealloc_archive_regions(MemRegion* ranges, size_t count) {</span>
    assert(!is_init_completed(), &quot;Expect to be called at JVM init time&quot;);
    assert(ranges != NULL, &quot;MemRegion array NULL&quot;);
    assert(count != 0, &quot;No MemRegions provided&quot;);
    MemRegion reserved = _hrm-&gt;reserved();
    HeapWord* prev_last_addr = NULL;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -764,11 +777,11 @@</span>
    size_t size_used = 0;
    size_t uncommitted_regions = 0;
  
    // For each Memregion, free the G1 regions that constitute it, and
    // notify mark-sweep that the range is no longer to be considered &#39;archive.&#39;
<span class="udiff-line-modified-removed">-   MutexLockerEx x(Heap_lock);</span>
<span class="udiff-line-modified-added">+   MutexLocker x(Heap_lock);</span>
    for (size_t i = 0; i &lt; count; i++) {
      HeapWord* start_address = ranges[i].start();
      HeapWord* last_address = ranges[i].last();
  
      assert(reserved.contains(start_address) &amp;&amp; reserved.contains(last_address),
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -814,11 +827,11 @@</span>
        _hrm-&gt;shrink_at(curr_index, 1);
        uncommitted_regions++;
      }
  
      // Notify mark-sweep that this is no longer an archive range.
<span class="udiff-line-modified-removed">-     G1ArchiveAllocator::clear_range_archive(ranges[i], is_open);</span>
<span class="udiff-line-modified-added">+     G1ArchiveAllocator::clear_range_archive(ranges[i]);</span>
    }
  
    if (uncommitted_regions != 0) {
      log_debug(gc, ergo, heap)(&quot;Attempt heap shrinking (uncommitted archive regions). Total size: &quot; SIZE_FORMAT &quot;B&quot;,
                                HeapRegion::GrainWords * HeapWordSize * uncommitted_regions);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -875,11 +888,11 @@</span>
      bool should_try_gc;
      uint gc_count_before;
  
  
      {
<span class="udiff-line-modified-removed">-       MutexLockerEx x(Heap_lock);</span>
<span class="udiff-line-modified-added">+       MutexLocker x(Heap_lock);</span>
  
        // Given that humongous objects are not allocated in young
        // regions, we&#39;ll first try to do the allocation without doing a
        // collection hoping that there&#39;s enough space in the heap.
        result = humongous_obj_allocate(word_size);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1013,11 +1026,11 @@</span>
    concurrent_mark()-&gt;concurrent_cycle_abort();
  }
  
  void G1CollectedHeap::prepare_heap_for_full_collection() {
    // Make sure we&#39;ll choose a new allocation region afterwards.
<span class="udiff-line-modified-removed">-   _allocator-&gt;release_mutator_alloc_region();</span>
<span class="udiff-line-modified-added">+   _allocator-&gt;release_mutator_alloc_regions();</span>
    _allocator-&gt;abandon_gc_alloc_regions();
  
    // We may have added regions to the current incremental collection
    // set between the last GC or pause and now. We need to clear the
    // incremental collection set and then start rebuilding it afresh
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1029,11 +1042,11 @@</span>
    hrm()-&gt;prepare_for_full_collection_start();
  }
  
  void G1CollectedHeap::verify_before_full_collection(bool explicit_gc) {
    assert(!GCCause::is_user_requested_gc(gc_cause()) || explicit_gc, &quot;invariant&quot;);
<span class="udiff-line-modified-removed">-   assert(used() == recalculate_used(), &quot;Should be equal&quot;);</span>
<span class="udiff-line-modified-added">+   assert_used_and_recalculate_used_equal(this);</span>
    _verifier-&gt;verify_region_sets_optional();
    _verifier-&gt;verify_before_gc(G1HeapVerifier::G1VerifyFull);
    _verifier-&gt;check_bitmaps(&quot;Full GC Start&quot;);
  }
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1057,11 +1070,11 @@</span>
    purge_code_root_memory();
  
    // Start a new incremental collection set for the next pause
    start_new_collection_set();
  
<span class="udiff-line-modified-removed">-   _allocator-&gt;init_mutator_alloc_region();</span>
<span class="udiff-line-modified-added">+   _allocator-&gt;init_mutator_alloc_regions();</span>
  
    // Post collection state updates.
    MetaspaceGC::compute_new_size();
  }
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1070,11 +1083,12 @@</span>
      _hot_card_cache-&gt;reset_hot_cache();
    }
  
    // Discard all remembered set updates.
    G1BarrierSet::dirty_card_queue_set().abandon_logs();
<span class="udiff-line-modified-removed">-   assert(dirty_card_queue_set().completed_buffers_num() == 0, &quot;DCQS should be empty&quot;);</span>
<span class="udiff-line-modified-added">+   assert(G1BarrierSet::dirty_card_queue_set().num_cards() == 0,</span>
<span class="udiff-line-added">+          &quot;DCQS should be empty&quot;);</span>
  }
  
  void G1CollectedHeap::verify_after_full_collection() {
    _hrm-&gt;verify_optional();
    _verifier-&gt;verify_region_sets_optional();
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1085,11 +1099,11 @@</span>
    // objects marked during a full GC against the previous bitmap.
    // But we need to clear it before calling check_bitmaps below since
    // the full GC has compacted objects and updated TAMS but not updated
    // the prev bitmap.
    if (G1VerifyBitmaps) {
<span class="udiff-line-modified-removed">-     GCTraceTime(Debug, gc)(&quot;Clear Prev Bitmap for Verification&quot;);</span>
<span class="udiff-line-modified-added">+     GCTraceTime(Debug, gc) tm(&quot;Clear Prev Bitmap for Verification&quot;);</span>
      _cm-&gt;clear_prev_bitmap(workers());
    }
    // This call implicitly verifies that the next bitmap is clear after Full GC.
    _verifier-&gt;check_bitmaps(&quot;Full GC End&quot;);
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1116,13 +1130,10 @@</span>
    // the compaction events.
    print_hrm_post_compaction();
    heap_transition-&gt;print();
    print_heap_after_gc();
    print_heap_regions();
<span class="udiff-line-removed">- #ifdef TRACESPINNING</span>
<span class="udiff-line-removed">-   ParallelTaskTerminator::print_termination_counts();</span>
<span class="udiff-line-removed">- #endif</span>
  }
  
  bool G1CollectedHeap::do_full_collection(bool explicit_gc,
                                           bool clear_all_soft_refs) {
    assert_at_safepoint_on_vm_thread();
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1170,22 +1181,19 @@</span>
    const double minimum_free_percentage = (double) MinHeapFreeRatio / 100.0;
    const double maximum_used_percentage = 1.0 - minimum_free_percentage;
    const double maximum_free_percentage = (double) MaxHeapFreeRatio / 100.0;
    const double minimum_used_percentage = 1.0 - maximum_free_percentage;
  
<span class="udiff-line-removed">-   const size_t min_heap_size = collector_policy()-&gt;min_heap_byte_size();</span>
<span class="udiff-line-removed">-   const size_t max_heap_size = collector_policy()-&gt;max_heap_byte_size();</span>
<span class="udiff-line-removed">- </span>
    // We have to be careful here as these two calculations can overflow
    // 32-bit size_t&#39;s.
    double used_after_gc_d = (double) used_after_gc;
    double minimum_desired_capacity_d = used_after_gc_d / maximum_used_percentage;
    double maximum_desired_capacity_d = used_after_gc_d / minimum_used_percentage;
  
    // Let&#39;s make sure that they are both under the max heap size, which
    // by default will make them fit into a size_t.
<span class="udiff-line-modified-removed">-   double desired_capacity_upper_bound = (double) max_heap_size;</span>
<span class="udiff-line-modified-added">+   double desired_capacity_upper_bound = (double) MaxHeapSize;</span>
    minimum_desired_capacity_d = MIN2(minimum_desired_capacity_d,
                                      desired_capacity_upper_bound);
    maximum_desired_capacity_d = MIN2(maximum_desired_capacity_d,
                                      desired_capacity_upper_bound);
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1201,15 +1209,15 @@</span>
           minimum_desired_capacity, maximum_desired_capacity);
  
    // Should not be greater than the heap max size. No need to adjust
    // it with respect to the heap min size as it&#39;s a lower bound (i.e.,
    // we&#39;ll try to make the capacity larger than it, not smaller).
<span class="udiff-line-modified-removed">-   minimum_desired_capacity = MIN2(minimum_desired_capacity, max_heap_size);</span>
<span class="udiff-line-modified-added">+   minimum_desired_capacity = MIN2(minimum_desired_capacity, MaxHeapSize);</span>
    // Should not be less than the heap min size. No need to adjust it
    // with respect to the heap max size as it&#39;s an upper bound (i.e.,
    // we&#39;ll try to make the capacity smaller than it, not greater).
<span class="udiff-line-modified-removed">-   maximum_desired_capacity =  MAX2(maximum_desired_capacity, min_heap_size);</span>
<span class="udiff-line-modified-added">+   maximum_desired_capacity =  MAX2(maximum_desired_capacity, MinHeapSize);</span>
  
    if (capacity_after_gc &lt; minimum_desired_capacity) {
      // Don&#39;t expand unless it&#39;s significant
      size_t expand_bytes = minimum_desired_capacity - capacity_after_gc;
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1376,21 +1384,33 @@</span>
      }
    }
    return regions_to_expand &gt; 0;
  }
  
<span class="udiff-line-added">+ bool G1CollectedHeap::expand_single_region(uint node_index) {</span>
<span class="udiff-line-added">+   uint expanded_by = _hrm-&gt;expand_on_preferred_node(node_index);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   if (expanded_by == 0) {</span>
<span class="udiff-line-added">+     assert(is_maximal_no_gc(), &quot;Should be no regions left, available: %u&quot;, _hrm-&gt;available());</span>
<span class="udiff-line-added">+     log_debug(gc, ergo, heap)(&quot;Did not expand the heap (heap already fully expanded)&quot;);</span>
<span class="udiff-line-added">+     return false;</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   policy()-&gt;record_new_heap_size(num_regions());</span>
<span class="udiff-line-added">+   return true;</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
  void G1CollectedHeap::shrink_helper(size_t shrink_bytes) {
    size_t aligned_shrink_bytes =
      ReservedSpace::page_align_size_down(shrink_bytes);
    aligned_shrink_bytes = align_down(aligned_shrink_bytes,
                                           HeapRegion::GrainBytes);
    uint num_regions_to_remove = (uint)(shrink_bytes / HeapRegion::GrainBytes);
  
    uint num_regions_removed = _hrm-&gt;shrink_by(num_regions_to_remove);
    size_t shrunk_bytes = num_regions_removed * HeapRegion::GrainBytes;
  
<span class="udiff-line-removed">- </span>
    log_debug(gc, ergo, heap)(&quot;Shrink the heap. requested shrinking amount: &quot; SIZE_FORMAT &quot;B aligned shrinking amount: &quot; SIZE_FORMAT &quot;B attempted shrinking amount: &quot; SIZE_FORMAT &quot;B&quot;,
                              shrink_bytes, aligned_shrink_bytes, shrunk_bytes);
    if (num_regions_removed &gt; 0) {
      policy()-&gt;record_new_heap_size(num_regions());
    } else {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1477,26 +1497,27 @@</span>
    }
    bool is_correct_type(HeapRegion* hr) { return hr-&gt;is_humongous(); }
    const char* get_description() { return &quot;Humongous Regions&quot;; }
  };
  
<span class="udiff-line-modified-removed">- G1CollectedHeap::G1CollectedHeap(G1CollectorPolicy* collector_policy) :</span>
<span class="udiff-line-modified-added">+ G1CollectedHeap::G1CollectedHeap() :</span>
    CollectedHeap(),
    _young_gen_sampling_thread(NULL),
    _workers(NULL),
<span class="udiff-line-removed">-   _collector_policy(collector_policy),</span>
    _card_table(NULL),
    _soft_ref_policy(),
    _old_set(&quot;Old Region Set&quot;, new OldRegionSetChecker()),
    _archive_set(&quot;Archive Region Set&quot;, new ArchiveRegionSetChecker()),
    _humongous_set(&quot;Humongous Region Set&quot;, new HumongousRegionSetChecker()),
    _bot(NULL),
    _listener(),
<span class="udiff-line-added">+   _numa(G1NUMA::create()),</span>
    _hrm(NULL),
    _allocator(NULL),
    _verifier(NULL),
    _summary_bytes_used(0),
<span class="udiff-line-added">+   _bytes_used_during_gc(0),</span>
    _archive_allocator(NULL),
    _survivor_evac_stats(&quot;Young&quot;, YoungPLABSize, PLABWeight),
    _old_evac_stats(&quot;Old&quot;, OldPLABSize, PLABWeight),
    _expand_heap_after_alloc_failure(true),
    _g1mm(NULL),
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1508,16 +1529,15 @@</span>
    _old_marking_cycles_completed(0),
    _eden(),
    _survivor(),
    _gc_timer_stw(new (ResourceObj::C_HEAP, mtGC) STWGCTimer()),
    _gc_tracer_stw(new (ResourceObj::C_HEAP, mtGC) G1NewTracer()),
<span class="udiff-line-modified-removed">-   _policy(G1Policy::create_policy(collector_policy, _gc_timer_stw)),</span>
<span class="udiff-line-modified-added">+   _policy(G1Policy::create_policy(_gc_timer_stw)),</span>
    _heap_sizing_policy(NULL),
    _collection_set(this, _policy),
    _hot_card_cache(NULL),
    _rem_set(NULL),
<span class="udiff-line-removed">-   _dirty_card_queue_set(false),</span>
    _cm(NULL),
    _cm_thread(NULL),
    _cr(NULL),
    _task_queues(NULL),
    _evacuation_failed(false),
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1532,11 +1552,11 @@</span>
    _is_alive_closure_stw(this),
    _is_subject_to_discovery_stw(this),
    _ref_processor_cm(NULL),
    _is_alive_closure_cm(this),
    _is_subject_to_discovery_cm(this),
<span class="udiff-line-modified-removed">-   _in_cset_fast_test() {</span>
<span class="udiff-line-modified-added">+   _region_attr() {</span>
  
    _verifier = new G1HeapVerifier(this);
  
    _allocator = new G1Allocator(this);
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1560,10 +1580,11 @@</span>
      ::new (&amp;_evacuation_failed_info_array[i]) EvacuationFailedInfo();
    }
  
    // Initialize the G1EvacuationFailureALot counters and flags.
    NOT_PRODUCT(reset_evacuation_should_fail();)
<span class="udiff-line-added">+   _gc_tracer_stw-&gt;initialize();</span>
  
    guarantee(_task_queues != NULL, &quot;task_queues allocation failure.&quot;);
  }
  
  static size_t actual_reserved_page_size(ReservedSpace rs) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1573,11 +1594,14 @@</span>
      // 1. OS supports committing large page memory.
      // 2. OS doesn&#39;t support committing large page memory so ReservedSpace manages it.
      //    And ReservedSpace calls it &#39;special&#39;. If we failed to set &#39;special&#39;,
      //    we reserved memory without large page.
      if (os::can_commit_large_page_memory() || rs.special()) {
<span class="udiff-line-modified-removed">-       page_size = rs.alignment();</span>
<span class="udiff-line-modified-added">+       // An alignment at ReservedSpace comes from preferred page size or</span>
<span class="udiff-line-added">+       // heap alignment, and if the alignment came from heap alignment, it could be</span>
<span class="udiff-line-added">+       // larger than large pages size. So need to cap with the large page size.</span>
<span class="udiff-line-added">+       page_size = MIN2(rs.alignment(), os::large_page_size());</span>
      }
    }
  
    return page_size;
  }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1621,11 +1645,10 @@</span>
    }
    return JNI_OK;
  }
  
  jint G1CollectedHeap::initialize() {
<span class="udiff-line-removed">-   os::enable_vtime();</span>
  
    // Necessary to satisfy locking discipline assertions.
  
    MutexLocker x(Heap_lock);
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1634,18 +1657,17 @@</span>
    // system which believe this to be true (e.g. oop-&gt;object_size in some
    // cases incorrectly returns the size in wordSize units rather than
    // HeapWordSize).
    guarantee(HeapWordSize == wordSize, &quot;HeapWordSize must equal wordSize&quot;);
  
<span class="udiff-line-modified-removed">-   size_t init_byte_size = collector_policy()-&gt;initial_heap_byte_size();</span>
<span class="udiff-line-modified-removed">-   size_t max_byte_size = _collector_policy-&gt;heap_reserved_size_bytes();</span>
<span class="udiff-line-removed">-   size_t heap_alignment = collector_policy()-&gt;heap_alignment();</span>
<span class="udiff-line-modified-added">+   size_t init_byte_size = InitialHeapSize;</span>
<span class="udiff-line-modified-added">+   size_t reserved_byte_size = G1Arguments::heap_reserved_size_bytes();</span>
  
    // Ensure that the sizes are properly aligned.
    Universe::check_alignment(init_byte_size, HeapRegion::GrainBytes, &quot;g1 heap&quot;);
<span class="udiff-line-modified-removed">-   Universe::check_alignment(max_byte_size, HeapRegion::GrainBytes, &quot;g1 heap&quot;);</span>
<span class="udiff-line-modified-removed">-   Universe::check_alignment(max_byte_size, heap_alignment, &quot;g1 heap&quot;);</span>
<span class="udiff-line-modified-added">+   Universe::check_alignment(reserved_byte_size, HeapRegion::GrainBytes, &quot;g1 heap&quot;);</span>
<span class="udiff-line-modified-added">+   Universe::check_alignment(reserved_byte_size, HeapAlignment, &quot;g1 heap&quot;);</span>
  
    // Reserve the maximum.
  
    // When compressed oops are enabled, the preferred heap base
    // is calculated by subtracting the requested size from the
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1656,46 +1678,35 @@</span>
    // base of the reserved heap may end up differing from the
    // address that was requested (i.e. the preferred heap base).
    // If this happens then we could end up using a non-optimal
    // compressed oops mode.
  
<span class="udiff-line-modified-removed">-   ReservedSpace heap_rs = Universe::reserve_heap(max_byte_size,</span>
<span class="udiff-line-modified-removed">-                                                  heap_alignment);</span>
<span class="udiff-line-modified-added">+   ReservedHeapSpace heap_rs = Universe::reserve_heap(reserved_byte_size,</span>
<span class="udiff-line-modified-added">+                                                      HeapAlignment);</span>
  
<span class="udiff-line-modified-removed">-   initialize_reserved_region((HeapWord*)heap_rs.base(), (HeapWord*)(heap_rs.base() + heap_rs.size()));</span>
<span class="udiff-line-modified-added">+   initialize_reserved_region(heap_rs);</span>
  
    // Create the barrier set for the entire reserved region.
<span class="udiff-line-modified-removed">-   G1CardTable* ct = new G1CardTable(reserved_region());</span>
<span class="udiff-line-modified-added">+   G1CardTable* ct = new G1CardTable(heap_rs.region());</span>
    ct-&gt;initialize();
    G1BarrierSet* bs = new G1BarrierSet(ct);
    bs-&gt;initialize();
    assert(bs-&gt;is_a(BarrierSet::G1BarrierSet), &quot;sanity&quot;);
    BarrierSet::set_barrier_set(bs);
    _card_table = ct;
  
<span class="udiff-line-modified-removed">-   G1BarrierSet::satb_mark_queue_set().initialize(this,</span>
<span class="udiff-line-modified-removed">-                                                  SATB_Q_CBL_mon,</span>
<span class="udiff-line-modified-removed">-                                                  &amp;bs-&gt;satb_mark_queue_buffer_allocator(),</span>
<span class="udiff-line-modified-removed">-                                                  G1SATBProcessCompletedThreshold,</span>
<span class="udiff-line-modified-removed">-                                                  G1SATBBufferEnqueueingThresholdPercent);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   // process_completed_buffers_threshold and max_completed_buffers are updated</span>
<span class="udiff-line-removed">-   // later, based on the concurrent refinement object.</span>
<span class="udiff-line-removed">-   G1BarrierSet::dirty_card_queue_set().initialize(DirtyCardQ_CBL_mon,</span>
<span class="udiff-line-removed">-                                                   &amp;bs-&gt;dirty_card_queue_buffer_allocator(),</span>
<span class="udiff-line-removed">-                                                   Shared_DirtyCardQ_lock,</span>
<span class="udiff-line-removed">-                                                   true); // init_free_ids</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   dirty_card_queue_set().initialize(DirtyCardQ_CBL_mon,</span>
<span class="udiff-line-removed">-                                     &amp;bs-&gt;dirty_card_queue_buffer_allocator(),</span>
<span class="udiff-line-removed">-                                     Shared_DirtyCardQ_lock);</span>
<span class="udiff-line-modified-added">+   {</span>
<span class="udiff-line-modified-added">+     G1SATBMarkQueueSet&amp; satbqs = bs-&gt;satb_mark_queue_set();</span>
<span class="udiff-line-modified-added">+     satbqs.set_process_completed_buffers_threshold(G1SATBProcessCompletedThreshold);</span>
<span class="udiff-line-modified-added">+     satbqs.set_buffer_enqueue_threshold_percentage(G1SATBBufferEnqueueingThresholdPercent);</span>
<span class="udiff-line-modified-added">+   }</span>
  
    // Create the hot card cache.
    _hot_card_cache = new G1HotCardCache(this);
  
    // Carve out the G1 part of the heap.
<span class="udiff-line-modified-removed">-   ReservedSpace g1_rs = heap_rs.first_part(max_byte_size);</span>
<span class="udiff-line-modified-added">+   ReservedSpace g1_rs = heap_rs.first_part(reserved_byte_size);</span>
    size_t page_size = actual_reserved_page_size(heap_rs);
    G1RegionToSpaceMapper* heap_storage =
      G1RegionToSpaceMapper::create_heap_mapper(g1_rs,
                                                g1_rs.size(),
                                                page_size,
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1706,12 +1717,12 @@</span>
      vm_shutdown_during_initialization(&quot;Could not initialize G1 heap&quot;);
      return JNI_ERR;
    }
  
    os::trace_page_sizes(&quot;Heap&quot;,
<span class="udiff-line-modified-removed">-                        collector_policy()-&gt;min_heap_byte_size(),</span>
<span class="udiff-line-modified-removed">-                        max_byte_size,</span>
<span class="udiff-line-modified-added">+                        MinHeapSize,</span>
<span class="udiff-line-modified-added">+                        reserved_byte_size,</span>
                         page_size,
                         heap_rs.base(),
                         heap_rs.size());
    heap_storage-&gt;set_mapping_changed_listener(&amp;_listener);
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1735,14 +1746,15 @@</span>
    G1RegionToSpaceMapper* prev_bitmap_storage =
      create_aux_memory_mapper(&quot;Prev Bitmap&quot;, bitmap_size, G1CMBitMap::heap_map_factor());
    G1RegionToSpaceMapper* next_bitmap_storage =
      create_aux_memory_mapper(&quot;Next Bitmap&quot;, bitmap_size, G1CMBitMap::heap_map_factor());
  
<span class="udiff-line-modified-removed">-   _hrm = HeapRegionManager::create_manager(this, _collector_policy);</span>
<span class="udiff-line-modified-added">+   _hrm = HeapRegionManager::create_manager(this);</span>
  
    _hrm-&gt;initialize(heap_storage, prev_bitmap_storage, next_bitmap_storage, bot_storage, cardtable_storage, card_counts_storage);
    _card_table-&gt;initialize(cardtable_storage);
<span class="udiff-line-added">+ </span>
    // Do later initialization work for concurrent refinement.
    _hot_card_cache-&gt;initialize(card_counts_storage);
  
    // 6843694 - ensure that the maximum region index can fit
    // in the remembered set structures.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1768,11 +1780,11 @@</span>
    {
      HeapWord* start = _hrm-&gt;reserved().start();
      HeapWord* end = _hrm-&gt;reserved().end();
      size_t granularity = HeapRegion::GrainBytes;
  
<span class="udiff-line-modified-removed">-     _in_cset_fast_test.initialize(start, end, granularity);</span>
<span class="udiff-line-modified-added">+     _region_attr.initialize(start, end, granularity);</span>
      _humongous_reclaim_candidates.initialize(start, end, granularity);
    }
  
    _workers = new WorkGang(&quot;GC Thread&quot;, ParallelGCThreads,
                            true /* are_GC_task_threads */,
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1780,15 +1792,17 @@</span>
    if (_workers == NULL) {
      return JNI_ENOMEM;
    }
    _workers-&gt;initialize_workers();
  
<span class="udiff-line-added">+   _numa-&gt;set_region_info(HeapRegion::GrainBytes, page_size);</span>
<span class="udiff-line-added">+ </span>
    // Create the G1ConcurrentMark data structure and thread.
    // (Must do this late, so that &quot;max_regions&quot; is defined.)
    _cm = new G1ConcurrentMark(this, prev_bitmap_storage, next_bitmap_storage);
<span class="udiff-line-modified-removed">-   if (_cm == NULL || !_cm-&gt;completed_initialization()) {</span>
<span class="udiff-line-modified-removed">-     vm_shutdown_during_initialization(&quot;Could not create/initialize G1ConcurrentMark&quot;);</span>
<span class="udiff-line-modified-added">+   if (!_cm-&gt;completed_initialization()) {</span>
<span class="udiff-line-modified-added">+     vm_shutdown_during_initialization(&quot;Could not initialize G1ConcurrentMark&quot;);</span>
      return JNI_ENOMEM;
    }
    _cm_thread = _cm-&gt;cm_thread();
  
    // Now expand into the initial heap size.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1810,12 +1824,12 @@</span>
      return ecode;
    }
  
    {
      G1DirtyCardQueueSet&amp; dcqs = G1BarrierSet::dirty_card_queue_set();
<span class="udiff-line-modified-removed">-     dcqs.set_process_completed_buffers_threshold(concurrent_refine()-&gt;yellow_zone());</span>
<span class="udiff-line-modified-removed">-     dcqs.set_max_completed_buffers(concurrent_refine()-&gt;red_zone());</span>
<span class="udiff-line-modified-added">+     dcqs.set_process_cards_threshold(concurrent_refine()-&gt;yellow_zone());</span>
<span class="udiff-line-modified-added">+     dcqs.set_max_cards(concurrent_refine()-&gt;red_zone());</span>
    }
  
    // Here we allocate the dummy HeapRegion that is required by the
    // G1AllocRegion class.
    HeapRegion* dummy_region = _hrm-&gt;get_dummy_region();
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1827,11 +1841,11 @@</span>
    dummy_region-&gt;set_eden();
    // Make sure it&#39;s full.
    dummy_region-&gt;set_top(dummy_region-&gt;end());
    G1AllocRegion::setup(this, dummy_region);
  
<span class="udiff-line-modified-removed">-   _allocator-&gt;init_mutator_alloc_region();</span>
<span class="udiff-line-modified-added">+   _allocator-&gt;init_mutator_alloc_regions();</span>
  
    // Do create of the monitoring and management support so that
    // values in the heap have been properly initialized.
    _g1mm = new G1MonitoringSupport(this);
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1862,14 +1876,10 @@</span>
  
  void G1CollectedHeap::safepoint_synchronize_end() {
    SuspendibleThreadSet::desynchronize();
  }
  
<span class="udiff-line-removed">- size_t G1CollectedHeap::conservative_max_heap_alignment() {</span>
<span class="udiff-line-removed">-   return HeapRegion::max_region_size();</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-removed">- </span>
  void G1CollectedHeap::post_initialize() {
    CollectedHeap::post_initialize();
    ref_processing_init();
  }
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1932,14 +1942,10 @@</span>
                             true,                                 // Reference discovery is atomic
                             &amp;_is_alive_closure_stw,               // is alive closure
                             true);                                // allow changes to number of processing threads
  }
  
<span class="udiff-line-removed">- CollectorPolicy* G1CollectedHeap::collector_policy() const {</span>
<span class="udiff-line-removed">-   return _collector_policy;</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-removed">- </span>
  SoftRefPolicy* G1CollectedHeap::soft_ref_policy() {
    return &amp;_soft_ref_policy;
  }
  
  size_t G1CollectedHeap::capacity() const {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1948,22 +1954,12 @@</span>
  
  size_t G1CollectedHeap::unused_committed_regions_in_bytes() const {
    return _hrm-&gt;total_free_bytes();
  }
  
<span class="udiff-line-modified-removed">- void G1CollectedHeap::iterate_hcc_closure(G1CardTableEntryClosure* cl, uint worker_i) {</span>
<span class="udiff-line-modified-removed">-   _hot_card_cache-&gt;drain(cl, worker_i);</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">- void G1CollectedHeap::iterate_dirty_card_closure(G1CardTableEntryClosure* cl, uint worker_i) {</span>
<span class="udiff-line-removed">-   G1DirtyCardQueueSet&amp; dcqs = G1BarrierSet::dirty_card_queue_set();</span>
<span class="udiff-line-removed">-   size_t n_completed_buffers = 0;</span>
<span class="udiff-line-removed">-   while (dcqs.apply_closure_during_gc(cl, worker_i)) {</span>
<span class="udiff-line-removed">-     n_completed_buffers++;</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">-   assert(dcqs.completed_buffers_num() == 0, &quot;Completed buffers exist!&quot;);</span>
<span class="udiff-line-removed">-   phase_times()-&gt;record_thread_work_item(G1GCPhaseTimes::UpdateRS, worker_i, n_completed_buffers, G1GCPhaseTimes::UpdateRSProcessedBuffers);</span>
<span class="udiff-line-modified-added">+ void G1CollectedHeap::iterate_hcc_closure(G1CardTableEntryClosure* cl, uint worker_id) {</span>
<span class="udiff-line-modified-added">+   _hot_card_cache-&gt;drain(cl, worker_id);</span>
  }
  
  // Computes the sum of the storage used by the various regions.
  size_t G1CollectedHeap::used() const {
    size_t result = _summary_bytes_used + _allocator-&gt;used_in_alloc_regions();
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2003,19 +1999,18 @@</span>
    }
  }
  
  bool G1CollectedHeap::should_do_concurrent_full_gc(GCCause::Cause cause) {
    switch (cause) {
<span class="udiff-line-removed">-     case GCCause::_gc_locker:               return GCLockerInvokesConcurrent;</span>
      case GCCause::_g1_humongous_allocation: return true;
      case GCCause::_g1_periodic_collection:  return G1PeriodicGCInvokesConcurrent;
      default:                                return is_user_requested_concurrent_full_gc(cause);
    }
  }
  
  bool G1CollectedHeap::should_upgrade_to_full_gc(GCCause::Cause cause) {
<span class="udiff-line-modified-removed">-   if(policy()-&gt;force_upgrade_to_full()) {</span>
<span class="udiff-line-modified-added">+   if (policy()-&gt;force_upgrade_to_full()) {</span>
      return true;
    } else if (should_do_concurrent_full_gc(_gc_cause)) {
      return false;
    } else if (has_regions_left_for_allocation()) {
      return false;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2058,11 +2053,11 @@</span>
  
    _old_marking_cycles_started++;
  }
  
  void G1CollectedHeap::increment_old_marking_cycles_completed(bool concurrent) {
<span class="udiff-line-modified-removed">-   MonitorLockerEx x(FullGCCount_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="udiff-line-modified-added">+   MonitorLocker ml(G1OldGCCount_lock, Mutex::_no_safepoint_check_flag);</span>
  
    // We assume that if concurrent == true, then the caller is a
    // concurrent thread that was joined the Suspendible Thread
    // Set. If there&#39;s ever a cheap way to check this, we should add an
    // assert here.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2098,89 +2093,221 @@</span>
    // incorrectly see that a marking cycle is still in progress.
    if (concurrent) {
      _cm_thread-&gt;set_idle();
    }
  
<span class="udiff-line-modified-removed">-   // This notify_all() will ensure that a thread that called</span>
<span class="udiff-line-modified-removed">-   // System.gc() with (with ExplicitGCInvokesConcurrent set or not)</span>
<span class="udiff-line-modified-removed">-   // and it&#39;s waiting for a full GC to finish will be woken up. It is</span>
<span class="udiff-line-removed">-   // waiting in VM_G1CollectForAllocation::doit_epilogue().</span>
<span class="udiff-line-removed">-   FullGCCount_lock-&gt;notify_all();</span>
<span class="udiff-line-modified-added">+   // Notify threads waiting in System.gc() (with ExplicitGCInvokesConcurrent)</span>
<span class="udiff-line-modified-added">+   // for a full GC to finish that their wait is over.</span>
<span class="udiff-line-modified-added">+   ml.notify_all();</span>
  }
  
  void G1CollectedHeap::collect(GCCause::Cause cause) {
<span class="udiff-line-modified-removed">-   try_collect(cause, true);</span>
<span class="udiff-line-modified-removed">- }</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">- bool G1CollectedHeap::try_collect(GCCause::Cause cause, bool retry_on_gc_failure) {</span>
<span class="udiff-line-modified-added">+   try_collect(cause);</span>
<span class="udiff-line-modified-added">+ }</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+ // Return true if (x &lt; y) with allowance for wraparound.</span>
<span class="udiff-line-added">+ static bool gc_counter_less_than(uint x, uint y) {</span>
<span class="udiff-line-added">+   return (x - y) &gt; (UINT_MAX/2);</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ // LOG_COLLECT_CONCURRENTLY(cause, msg, args...)</span>
<span class="udiff-line-added">+ // Macro so msg printing is format-checked.</span>
<span class="udiff-line-added">+ #define LOG_COLLECT_CONCURRENTLY(cause, ...)                            \</span>
<span class="udiff-line-added">+   do {                                                                  \</span>
<span class="udiff-line-added">+     LogTarget(Trace, gc) LOG_COLLECT_CONCURRENTLY_lt;                   \</span>
<span class="udiff-line-added">+     if (LOG_COLLECT_CONCURRENTLY_lt.is_enabled()) {                     \</span>
<span class="udiff-line-added">+       ResourceMark rm; /* For thread name. */                           \</span>
<span class="udiff-line-added">+       LogStream LOG_COLLECT_CONCURRENTLY_s(&amp;LOG_COLLECT_CONCURRENTLY_lt); \</span>
<span class="udiff-line-added">+       LOG_COLLECT_CONCURRENTLY_s.print(&quot;%s: Try Collect Concurrently (%s): &quot;, \</span>
<span class="udiff-line-added">+                                        Thread::current()-&gt;name(),       \</span>
<span class="udiff-line-added">+                                        GCCause::to_string(cause));      \</span>
<span class="udiff-line-added">+       LOG_COLLECT_CONCURRENTLY_s.print(__VA_ARGS__);                    \</span>
<span class="udiff-line-added">+     }                                                                   \</span>
<span class="udiff-line-added">+   } while (0)</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ #define LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, result) \</span>
<span class="udiff-line-added">+   LOG_COLLECT_CONCURRENTLY(cause, &quot;complete %s&quot;, BOOL_TO_STR(result))</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ bool G1CollectedHeap::try_collect_concurrently(GCCause::Cause cause,</span>
<span class="udiff-line-added">+                                                uint gc_counter,</span>
<span class="udiff-line-added">+                                                uint old_marking_started_before) {</span>
    assert_heap_not_locked();
<span class="udiff-line-added">+   assert(should_do_concurrent_full_gc(cause),</span>
<span class="udiff-line-added">+          &quot;Non-concurrent cause %s&quot;, GCCause::to_string(cause));</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   for (uint i = 1; true; ++i) {</span>
<span class="udiff-line-added">+     // Try to schedule an initial-mark evacuation pause that will</span>
<span class="udiff-line-added">+     // start a concurrent cycle.</span>
<span class="udiff-line-added">+     LOG_COLLECT_CONCURRENTLY(cause, &quot;attempt %u&quot;, i);</span>
<span class="udiff-line-added">+     VM_G1TryInitiateConcMark op(gc_counter,</span>
<span class="udiff-line-added">+                                 cause,</span>
<span class="udiff-line-added">+                                 policy()-&gt;max_pause_time_ms());</span>
<span class="udiff-line-added">+     VMThread::execute(&amp;op);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     // Request is trivially finished.</span>
<span class="udiff-line-added">+     if (cause == GCCause::_g1_periodic_collection) {</span>
<span class="udiff-line-added">+       LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, op.gc_succeeded());</span>
<span class="udiff-line-added">+       return op.gc_succeeded();</span>
<span class="udiff-line-added">+     }</span>
  
<span class="udiff-line-modified-removed">-   bool gc_succeeded;</span>
<span class="udiff-line-modified-removed">-   bool should_retry_gc;</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-   do {</span>
<span class="udiff-line-modified-removed">-     should_retry_gc = false;</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-removed">-     uint gc_count_before;</span>
<span class="udiff-line-removed">-     uint old_marking_count_before;</span>
<span class="udiff-line-removed">-     uint full_gc_count_before;</span>
<span class="udiff-line-modified-added">+     // If VMOp skipped initiating concurrent marking cycle because</span>
<span class="udiff-line-modified-added">+     // we&#39;re terminating, then we&#39;re done.</span>
<span class="udiff-line-modified-added">+     if (op.terminating()) {</span>
<span class="udiff-line-modified-added">+       LOG_COLLECT_CONCURRENTLY(cause, &quot;skipped: terminating&quot;);</span>
<span class="udiff-line-modified-added">+       return false;</span>
<span class="udiff-line-modified-added">+     }</span>
  
<span class="udiff-line-added">+     // Lock to get consistent set of values.</span>
<span class="udiff-line-added">+     uint old_marking_started_after;</span>
<span class="udiff-line-added">+     uint old_marking_completed_after;</span>
      {
        MutexLocker ml(Heap_lock);
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-       // Read the GC count while holding the Heap_lock</span>
<span class="udiff-line-modified-removed">-       gc_count_before = total_collections();</span>
<span class="udiff-line-modified-removed">-       full_gc_count_before = total_full_collections();</span>
<span class="udiff-line-modified-removed">-       old_marking_count_before = _old_marking_cycles_started;</span>
<span class="udiff-line-modified-added">+       // Update gc_counter for retrying VMOp if needed. Captured here to be</span>
<span class="udiff-line-modified-added">+       // consistent with the values we use below for termination tests.  If</span>
<span class="udiff-line-modified-added">+       // a retry is needed after a possible wait, and another collection</span>
<span class="udiff-line-modified-added">+       // occurs in the meantime, it will cause our retry to be skipped and</span>
<span class="udiff-line-modified-added">+       // we&#39;ll recheck for termination with updated conditions from that</span>
<span class="udiff-line-added">+       // more recent collection.  That&#39;s what we want, rather than having</span>
<span class="udiff-line-added">+       // our retry possibly perform an unnecessary collection.</span>
<span class="udiff-line-added">+       gc_counter = total_collections();</span>
<span class="udiff-line-added">+       old_marking_started_after = _old_marking_cycles_started;</span>
<span class="udiff-line-added">+       old_marking_completed_after = _old_marking_cycles_completed;</span>
      }
  
<span class="udiff-line-modified-removed">-     if (should_do_concurrent_full_gc(cause)) {</span>
<span class="udiff-line-modified-removed">-       // Schedule an initial-mark evacuation pause that will start a</span>
<span class="udiff-line-modified-removed">-       // concurrent cycle. We&#39;re setting word_size to 0 which means that</span>
<span class="udiff-line-modified-removed">-       // we are not requesting a post-GC allocation.</span>
<span class="udiff-line-modified-removed">-       VM_G1CollectForAllocation op(0,     /* word_size */</span>
<span class="udiff-line-modified-removed">-                                    gc_count_before,</span>
<span class="udiff-line-modified-removed">-                                    cause,</span>
<span class="udiff-line-modified-removed">-                                    true,  /* should_initiate_conc_mark */</span>
<span class="udiff-line-modified-removed">-                                    policy()-&gt;max_pause_time_ms());</span>
<span class="udiff-line-modified-removed">-       VMThread::execute(&amp;op);</span>
<span class="udiff-line-modified-removed">-       gc_succeeded = op.gc_succeeded();</span>
<span class="udiff-line-modified-removed">-       if (!gc_succeeded &amp;&amp; retry_on_gc_failure) {</span>
<span class="udiff-line-modified-removed">-         if (old_marking_count_before == _old_marking_cycles_started) {</span>
<span class="udiff-line-modified-removed">-           should_retry_gc = op.should_retry_gc();</span>
<span class="udiff-line-modified-removed">-         } else {</span>
<span class="udiff-line-modified-removed">-           // A Full GC happened while we were trying to schedule the</span>
<span class="udiff-line-modified-removed">-           // concurrent cycle. No point in starting a new cycle given</span>
<span class="udiff-line-modified-removed">-           // that the whole heap was collected anyway.</span>
<span class="udiff-line-modified-removed">-         }</span>
<span class="udiff-line-modified-added">+     if (!GCCause::is_user_requested_gc(cause)) {</span>
<span class="udiff-line-modified-added">+       // For an &quot;automatic&quot; (not user-requested) collection, we just need to</span>
<span class="udiff-line-modified-added">+       // ensure that progress is made.</span>
<span class="udiff-line-modified-added">+       //</span>
<span class="udiff-line-modified-added">+       // Request is finished if any of</span>
<span class="udiff-line-modified-added">+       // (1) the VMOp successfully performed a GC,</span>
<span class="udiff-line-modified-added">+       // (2) a concurrent cycle was already in progress,</span>
<span class="udiff-line-modified-added">+       // (3) a new cycle was started (by this thread or some other), or</span>
<span class="udiff-line-modified-added">+       // (4) a Full GC was performed.</span>
<span class="udiff-line-modified-added">+       // Cases (3) and (4) are detected together by a change to</span>
<span class="udiff-line-modified-added">+       // _old_marking_cycles_started.</span>
<span class="udiff-line-modified-added">+       //</span>
<span class="udiff-line-modified-added">+       // Note that (1) does not imply (3).  If we&#39;re still in the mixed</span>
<span class="udiff-line-modified-added">+       // phase of an earlier concurrent collection, the request to make the</span>
<span class="udiff-line-modified-added">+       // collection an initial-mark won&#39;t be honored.  If we don&#39;t check for</span>
<span class="udiff-line-modified-added">+       // both conditions we&#39;ll spin doing back-to-back collections.</span>
<span class="udiff-line-modified-added">+       if (op.gc_succeeded() ||</span>
<span class="udiff-line-modified-added">+           op.cycle_already_in_progress() ||</span>
<span class="udiff-line-modified-added">+           (old_marking_started_before != old_marking_started_after)) {</span>
<span class="udiff-line-added">+         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);</span>
<span class="udiff-line-added">+         return true;</span>
<span class="udiff-line-added">+       }</span>
<span class="udiff-line-added">+     } else {                    // User-requested GC.</span>
<span class="udiff-line-added">+       // For a user-requested collection, we want to ensure that a complete</span>
<span class="udiff-line-added">+       // full collection has been performed before returning, but without</span>
<span class="udiff-line-added">+       // waiting for more than needed.</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+       // For user-requested GCs (unlike non-UR), a successful VMOp implies a</span>
<span class="udiff-line-added">+       // new cycle was started.  That&#39;s good, because it&#39;s not clear what we</span>
<span class="udiff-line-added">+       // should do otherwise.  Trying again just does back to back GCs.</span>
<span class="udiff-line-added">+       // Can&#39;t wait for someone else to start a cycle.  And returning fails</span>
<span class="udiff-line-added">+       // to meet the goal of ensuring a full collection was performed.</span>
<span class="udiff-line-added">+       assert(!op.gc_succeeded() ||</span>
<span class="udiff-line-added">+              (old_marking_started_before != old_marking_started_after),</span>
<span class="udiff-line-added">+              &quot;invariant: succeeded %s, started before %u, started after %u&quot;,</span>
<span class="udiff-line-added">+              BOOL_TO_STR(op.gc_succeeded()),</span>
<span class="udiff-line-added">+              old_marking_started_before, old_marking_started_after);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+       // Request is finished if a full collection (concurrent or stw)</span>
<span class="udiff-line-added">+       // was started after this request and has completed, e.g.</span>
<span class="udiff-line-added">+       // started_before &lt; completed_after.</span>
<span class="udiff-line-added">+       if (gc_counter_less_than(old_marking_started_before,</span>
<span class="udiff-line-added">+                                old_marking_completed_after)) {</span>
<span class="udiff-line-added">+         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);</span>
<span class="udiff-line-added">+         return true;</span>
<span class="udiff-line-added">+       }</span>
  
<span class="udiff-line-modified-removed">-         if (should_retry_gc &amp;&amp; GCLocker::is_active_and_needs_gc()) {</span>
<span class="udiff-line-modified-removed">-           GCLocker::stall_until_clear();</span>
<span class="udiff-line-modified-added">+       if (old_marking_started_after != old_marking_completed_after) {</span>
<span class="udiff-line-modified-added">+         // If there is an in-progress cycle (possibly started by us), then</span>
<span class="udiff-line-added">+         // wait for that cycle to complete, e.g.</span>
<span class="udiff-line-added">+         // while completed_now &lt; started_after.</span>
<span class="udiff-line-added">+         LOG_COLLECT_CONCURRENTLY(cause, &quot;wait&quot;);</span>
<span class="udiff-line-added">+         MonitorLocker ml(G1OldGCCount_lock);</span>
<span class="udiff-line-added">+         while (gc_counter_less_than(_old_marking_cycles_completed,</span>
<span class="udiff-line-added">+                                     old_marking_started_after)) {</span>
<span class="udiff-line-added">+           ml.wait();</span>
<span class="udiff-line-added">+         }</span>
<span class="udiff-line-added">+         // Request is finished if the collection we just waited for was</span>
<span class="udiff-line-added">+         // started after this request.</span>
<span class="udiff-line-added">+         if (old_marking_started_before != old_marking_started_after) {</span>
<span class="udiff-line-added">+           LOG_COLLECT_CONCURRENTLY(cause, &quot;complete after wait&quot;);</span>
<span class="udiff-line-added">+           return true;</span>
          }
        }
<span class="udiff-line-modified-removed">-     } else {</span>
<span class="udiff-line-modified-removed">-       if (cause == GCCause::_gc_locker || cause == GCCause::_wb_young_gc</span>
<span class="udiff-line-modified-removed">-           DEBUG_ONLY(|| cause == GCCause::_scavenge_alot)) {</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-         // Schedule a standard evacuation pause. We&#39;re setting word_size</span>
<span class="udiff-line-modified-removed">-         // to 0 which means that we are not requesting a post-GC allocation.</span>
<span class="udiff-line-modified-removed">-         VM_G1CollectForAllocation op(0,     /* word_size */</span>
<span class="udiff-line-modified-removed">-                                      gc_count_before,</span>
<span class="udiff-line-modified-removed">-                                      cause,</span>
<span class="udiff-line-modified-removed">-                                      false, /* should_initiate_conc_mark */</span>
<span class="udiff-line-modified-removed">-                                      policy()-&gt;max_pause_time_ms());</span>
<span class="udiff-line-modified-removed">-         VMThread::execute(&amp;op);</span>
<span class="udiff-line-modified-removed">-         gc_succeeded = op.gc_succeeded();</span>
<span class="udiff-line-removed">-       } else {</span>
<span class="udiff-line-removed">-         // Schedule a Full GC.</span>
<span class="udiff-line-removed">-         VM_G1CollectFull op(gc_count_before, full_gc_count_before, cause);</span>
<span class="udiff-line-removed">-         VMThread::execute(&amp;op);</span>
<span class="udiff-line-removed">-         gc_succeeded = op.gc_succeeded();</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+       // If VMOp was successful then it started a new cycle that the above</span>
<span class="udiff-line-modified-added">+       // wait &amp;etc should have recognized as finishing this request.  This</span>
<span class="udiff-line-modified-added">+       // differs from a non-user-request, where gc_succeeded does not imply</span>
<span class="udiff-line-modified-added">+       // a new cycle was started.</span>
<span class="udiff-line-modified-added">+       assert(!op.gc_succeeded(), &quot;invariant&quot;);</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+       // If VMOp failed because a cycle was already in progress, it is now</span>
<span class="udiff-line-modified-added">+       // complete.  But it didn&#39;t finish this user-requested GC, so try</span>
<span class="udiff-line-modified-added">+       // again.</span>
<span class="udiff-line-modified-added">+       if (op.cycle_already_in_progress()) {</span>
<span class="udiff-line-modified-added">+         LOG_COLLECT_CONCURRENTLY(cause, &quot;retry after in-progress&quot;);</span>
<span class="udiff-line-modified-added">+         continue;</span>
        }
      }
<span class="udiff-line-modified-removed">-   } while (should_retry_gc);</span>
<span class="udiff-line-modified-removed">-   return gc_succeeded;</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+     // Collection failed and should be retried.</span>
<span class="udiff-line-added">+     assert(op.transient_failure(), &quot;invariant&quot;);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     // If GCLocker is active, wait until clear before retrying.</span>
<span class="udiff-line-added">+     if (GCLocker::is_active_and_needs_gc()) {</span>
<span class="udiff-line-added">+       LOG_COLLECT_CONCURRENTLY(cause, &quot;gc-locker stall&quot;);</span>
<span class="udiff-line-added">+       GCLocker::stall_until_clear();</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     LOG_COLLECT_CONCURRENTLY(cause, &quot;retry&quot;);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ bool G1CollectedHeap::try_collect(GCCause::Cause cause) {</span>
<span class="udiff-line-added">+   assert_heap_not_locked();</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   // Lock to get consistent set of values.</span>
<span class="udiff-line-added">+   uint gc_count_before;</span>
<span class="udiff-line-added">+   uint full_gc_count_before;</span>
<span class="udiff-line-added">+   uint old_marking_started_before;</span>
<span class="udiff-line-added">+   {</span>
<span class="udiff-line-added">+     MutexLocker ml(Heap_lock);</span>
<span class="udiff-line-added">+     gc_count_before = total_collections();</span>
<span class="udiff-line-added">+     full_gc_count_before = total_full_collections();</span>
<span class="udiff-line-added">+     old_marking_started_before = _old_marking_cycles_started;</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   if (should_do_concurrent_full_gc(cause)) {</span>
<span class="udiff-line-added">+     return try_collect_concurrently(cause,</span>
<span class="udiff-line-added">+                                     gc_count_before,</span>
<span class="udiff-line-added">+                                     old_marking_started_before);</span>
<span class="udiff-line-added">+   } else if (GCLocker::should_discard(cause, gc_count_before)) {</span>
<span class="udiff-line-added">+     // Indicate failure to be consistent with VMOp failure due to</span>
<span class="udiff-line-added">+     // another collection slipping in after our gc_count but before</span>
<span class="udiff-line-added">+     // our request is processed.</span>
<span class="udiff-line-added">+     return false;</span>
<span class="udiff-line-added">+   } else if (cause == GCCause::_gc_locker || cause == GCCause::_wb_young_gc</span>
<span class="udiff-line-added">+              DEBUG_ONLY(|| cause == GCCause::_scavenge_alot)) {</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     // Schedule a standard evacuation pause. We&#39;re setting word_size</span>
<span class="udiff-line-added">+     // to 0 which means that we are not requesting a post-GC allocation.</span>
<span class="udiff-line-added">+     VM_G1CollectForAllocation op(0,     /* word_size */</span>
<span class="udiff-line-added">+                                  gc_count_before,</span>
<span class="udiff-line-added">+                                  cause,</span>
<span class="udiff-line-added">+                                  policy()-&gt;max_pause_time_ms());</span>
<span class="udiff-line-added">+     VMThread::execute(&amp;op);</span>
<span class="udiff-line-added">+     return op.gc_succeeded();</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     // Schedule a Full GC.</span>
<span class="udiff-line-added">+     VM_G1CollectFull op(gc_count_before, full_gc_count_before, cause);</span>
<span class="udiff-line-added">+     VMThread::execute(&amp;op);</span>
<span class="udiff-line-added">+     return op.gc_succeeded();</span>
<span class="udiff-line-added">+   }</span>
  }
  
  bool G1CollectedHeap::is_in(const void* p) const {
    if (_hrm-&gt;reserved().contains(p)) {
      // Given that we know that p is in the reserved space,
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2224,10 +2351,14 @@</span>
  void G1CollectedHeap::object_iterate(ObjectClosure* cl) {
    IterateObjectClosureRegionClosure blk(cl);
    heap_region_iterate(&amp;blk);
  }
  
<span class="udiff-line-added">+ void G1CollectedHeap::keep_alive(oop obj) {</span>
<span class="udiff-line-added">+   G1BarrierSet::enqueue(obj);</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
  void G1CollectedHeap::heap_region_iterate(HeapRegionClosure* cl) const {
    _hrm-&gt;iterate(cl);
  }
  
  void G1CollectedHeap::heap_region_par_iterate_from_worker_offset(HeapRegionClosure* cl,
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2239,16 +2370,20 @@</span>
  void G1CollectedHeap::heap_region_par_iterate_from_start(HeapRegionClosure* cl,
                                                           HeapRegionClaimer *hrclaimer) const {
    _hrm-&gt;par_iterate(cl, hrclaimer, 0);
  }
  
<span class="udiff-line-modified-removed">- void G1CollectedHeap::collection_set_iterate(HeapRegionClosure* cl) {</span>
<span class="udiff-line-modified-added">+ void G1CollectedHeap::collection_set_iterate_all(HeapRegionClosure* cl) {</span>
    _collection_set.iterate(cl);
  }
  
<span class="udiff-line-modified-removed">- void G1CollectedHeap::collection_set_iterate_from(HeapRegionClosure *cl, uint worker_id) {</span>
<span class="udiff-line-modified-removed">-   _collection_set.iterate_from(cl, worker_id, workers()-&gt;active_workers());</span>
<span class="udiff-line-modified-added">+ void G1CollectedHeap::collection_set_par_iterate_all(HeapRegionClosure* cl, HeapRegionClaimer* hr_claimer, uint worker_id) {</span>
<span class="udiff-line-modified-added">+   _collection_set.par_iterate(cl, hr_claimer, worker_id, workers()-&gt;active_workers());</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void G1CollectedHeap::collection_set_iterate_increment_from(HeapRegionClosure *cl, HeapRegionClaimer* hr_claimer, uint worker_id) {</span>
<span class="udiff-line-added">+   _collection_set.iterate_incremental_part_from(cl, hr_claimer, worker_id, workers()-&gt;active_workers());</span>
  }
  
  HeapWord* G1CollectedHeap::block_start(const void* addr) const {
    HeapRegion* hr = heap_region_containing(addr);
    return hr-&gt;block_start(addr);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2325,11 +2460,11 @@</span>
  bool G1CollectedHeap::request_concurrent_phase(const char* phase) {
    return _cm_thread-&gt;request_concurrent_phase(phase);
  }
  
  bool G1CollectedHeap::is_heterogeneous_heap() const {
<span class="udiff-line-modified-removed">-   return _collector_policy-&gt;is_heterogeneous_heap();</span>
<span class="udiff-line-modified-added">+   return G1Arguments::is_heterogeneous_heap();</span>
  }
  
  class PrintRegionClosure: public HeapRegionClosure {
    outputStream* _st;
  public:
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2385,17 +2520,27 @@</span>
              (size_t) young_regions * HeapRegion::GrainBytes / K);
    uint survivor_regions = survivor_regions_count();
    st-&gt;print(&quot;%u survivors (&quot; SIZE_FORMAT &quot;K)&quot;, survivor_regions,
              (size_t) survivor_regions * HeapRegion::GrainBytes / K);
    st-&gt;cr();
<span class="udiff-line-added">+   if (_numa-&gt;is_enabled()) {</span>
<span class="udiff-line-added">+     uint num_nodes = _numa-&gt;num_active_nodes();</span>
<span class="udiff-line-added">+     st-&gt;print(&quot;  remaining free region(s) on each NUMA node: &quot;);</span>
<span class="udiff-line-added">+     const int* node_ids = _numa-&gt;node_ids();</span>
<span class="udiff-line-added">+     for (uint node_index = 0; node_index &lt; num_nodes; node_index++) {</span>
<span class="udiff-line-added">+       st-&gt;print(&quot;%d=%u &quot;, node_ids[node_index], _hrm-&gt;num_free_regions(node_index));</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+     st-&gt;cr();</span>
<span class="udiff-line-added">+   }</span>
    MetaspaceUtils::print_on(st);
  }
  
  void G1CollectedHeap::print_regions_on(outputStream* st) const {
    st-&gt;print_cr(&quot;Heap Regions: E=young(eden), S=young(survivor), O=old, &quot;
                 &quot;HS=humongous(starts), HC=humongous(continues), &quot;
<span class="udiff-line-modified-removed">-                &quot;CS=collection set, F=free, A=archive, &quot;</span>
<span class="udiff-line-modified-added">+                &quot;CS=collection set, F=free, &quot;</span>
<span class="udiff-line-added">+                &quot;OA=open archive, CA=closed archive, &quot;</span>
                 &quot;TAMS=top-at-mark-start (previous, next)&quot;);
    PrintRegionClosure blk(st);
    heap_region_iterate(&amp;blk);
  }
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2481,23 +2626,27 @@</span>
    }
  };
  
  void G1CollectedHeap::print_cset_rsets() {
    PrintRSetsClosure cl(&quot;Printing CSet RSets&quot;);
<span class="udiff-line-modified-removed">-   collection_set_iterate(&amp;cl);</span>
<span class="udiff-line-modified-added">+   collection_set_iterate_all(&amp;cl);</span>
  }
  
  void G1CollectedHeap::print_all_rsets() {
    PrintRSetsClosure cl(&quot;Printing All RSets&quot;);;
    heap_region_iterate(&amp;cl);
  }
  #endif // PRODUCT
  
<span class="udiff-line-added">+ bool G1CollectedHeap::print_location(outputStream* st, void* addr) const {</span>
<span class="udiff-line-added">+   return BlockLocationPrinter&lt;G1CollectedHeap&gt;::print_location(st, addr);</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
  G1HeapSummary G1CollectedHeap::create_g1_heap_summary() {
  
<span class="udiff-line-modified-removed">-   size_t eden_used_bytes = heap()-&gt;eden_regions_count() * HeapRegion::GrainBytes;</span>
<span class="udiff-line-modified-removed">-   size_t survivor_used_bytes = heap()-&gt;survivor_regions_count() * HeapRegion::GrainBytes;</span>
<span class="udiff-line-modified-added">+   size_t eden_used_bytes = _eden.used_bytes();</span>
<span class="udiff-line-modified-added">+   size_t survivor_used_bytes = _survivor.used_bytes();</span>
    size_t heap_used = Heap_lock-&gt;owned_by_self() ? used() : used_unlocked();
  
    size_t eden_capacity_bytes =
      (policy()-&gt;young_list_target_length() * HeapRegion::GrainBytes) - survivor_used_bytes;
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2527,19 +2676,18 @@</span>
    assert(heap-&gt;kind() == CollectedHeap::G1, &quot;Invalid name&quot;);
    return (G1CollectedHeap*)heap;
  }
  
  void G1CollectedHeap::gc_prologue(bool full) {
<span class="udiff-line-removed">-   // always_do_update_barrier = false;</span>
    assert(InlineCacheBuffer::is_empty(), &quot;should have cleaned up ICBuffer&quot;);
  
    // This summary needs to be printed before incrementing total collections.
    rem_set()-&gt;print_periodic_summary_info(&quot;Before GC RS summary&quot;, total_collections());
  
    // Update common counters.
    increment_total_collections(full /* full gc */);
<span class="udiff-line-modified-removed">-   if (full) {</span>
<span class="udiff-line-modified-added">+   if (full || collector_state()-&gt;in_initial_mark_gc()) {</span>
      increment_old_marking_cycles_started();
    }
  
    // Fill TLAB&#39;s and such
    double start = os::elapsedTime();
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2561,31 +2709,43 @@</span>
    // I&#39;m ignoring the &quot;fill_newgen()&quot; call if &quot;alloc_event_enabled&quot;
    // is set.
  #if COMPILER2_OR_JVMCI
    assert(DerivedPointerTable::is_empty(), &quot;derived pointer present&quot;);
  #endif
<span class="udiff-line-removed">-   // always_do_update_barrier = true;</span>
  
    double start = os::elapsedTime();
    resize_all_tlabs();
    phase_times()-&gt;record_resize_tlab_time_ms((os::elapsedTime() - start) * 1000.0);
  
    MemoryService::track_memory_usage();
    // We have just completed a GC. Update the soft reference
    // policy with the new heap occupancy
    Universe::update_heap_info_at_gc();
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   // Print NUMA statistics.</span>
<span class="udiff-line-added">+   _numa-&gt;print_statistics();</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void G1CollectedHeap::verify_numa_regions(const char* desc) {</span>
<span class="udiff-line-added">+   LogTarget(Trace, gc, heap, verify) lt;</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   if (lt.is_enabled()) {</span>
<span class="udiff-line-added">+     LogStream ls(lt);</span>
<span class="udiff-line-added">+     // Iterate all heap regions to print matching between preferred numa id and actual numa id.</span>
<span class="udiff-line-added">+     G1NodeIndexCheckClosure cl(desc, _numa, &amp;ls);</span>
<span class="udiff-line-added">+     heap_region_iterate(&amp;cl);</span>
<span class="udiff-line-added">+   }</span>
  }
  
  HeapWord* G1CollectedHeap::do_collection_pause(size_t word_size,
                                                 uint gc_count_before,
                                                 bool* succeeded,
                                                 GCCause::Cause gc_cause) {
    assert_heap_not_locked_and_not_at_safepoint();
    VM_G1CollectForAllocation op(word_size,
                                 gc_count_before,
                                 gc_cause,
<span class="udiff-line-removed">-                                false, /* should_initiate_conc_mark */</span>
                                 policy()-&gt;max_pause_time_ms());
    VMThread::execute(&amp;op);
  
    HeapWord* result = op.result();
    bool ret_succeeded = op.prologue_succeeded() &amp;&amp; op.gc_succeeded();
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2596,11 +2756,11 @@</span>
    assert_heap_not_locked();
    return result;
  }
  
  void G1CollectedHeap::do_concurrent_mark() {
<span class="udiff-line-modified-removed">-   MutexLockerEx x(CGC_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="udiff-line-modified-added">+   MutexLocker x(CGC_lock, Mutex::_no_safepoint_check_flag);</span>
    if (!_cm_thread-&gt;in_progress()) {
      _cm_thread-&gt;set_started();
      CGC_lock-&gt;notify();
    }
  }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2614,14 +2774,11 @@</span>
      }
    } count_from_threads;
    Threads::threads_do(&amp;count_from_threads);
  
    G1DirtyCardQueueSet&amp; dcqs = G1BarrierSet::dirty_card_queue_set();
<span class="udiff-line-modified-removed">-   size_t buffer_size = dcqs.buffer_size();</span>
<span class="udiff-line-removed">-   size_t buffer_num = dcqs.completed_buffers_num();</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   return buffer_size * buffer_num + count_from_threads._cards;</span>
<span class="udiff-line-modified-added">+   return dcqs.num_cards() + count_from_threads._cards;</span>
  }
  
  bool G1CollectedHeap::is_potential_eager_reclaim_candidate(HeapRegion* r) const {
    // We don&#39;t nominate objects with many remembered set entries, on
    // the assumption that such objects are likely still live.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2630,161 +2787,26 @@</span>
    return G1EagerReclaimHumongousObjectsWithStaleRefs ?
           rem_set-&gt;occupancy_less_or_equal_than(G1RSetSparseRegionEntries) :
           G1EagerReclaimHumongousObjects &amp;&amp; rem_set-&gt;is_empty();
  }
  
<span class="udiff-line-modified-removed">- class RegisterHumongousWithInCSetFastTestClosure : public HeapRegionClosure {</span>
<span class="udiff-line-modified-removed">-  private:</span>
<span class="udiff-line-modified-removed">-   size_t _total_humongous;</span>
<span class="udiff-line-modified-removed">-   size_t _candidate_humongous;</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-   G1DirtyCardQueue _dcq;</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-   bool humongous_region_is_candidate(G1CollectedHeap* g1h, HeapRegion* region) const {</span>
<span class="udiff-line-modified-removed">-     assert(region-&gt;is_starts_humongous(), &quot;Must start a humongous object&quot;);</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-removed">-     oop obj = oop(region-&gt;bottom());</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     // Dead objects cannot be eager reclaim candidates. Due to class</span>
<span class="udiff-line-removed">-     // unloading it is unsafe to query their classes so we return early.</span>
<span class="udiff-line-removed">-     if (g1h-&gt;is_obj_dead(obj, region)) {</span>
<span class="udiff-line-removed">-       return false;</span>
<span class="udiff-line-removed">-     }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     // If we do not have a complete remembered set for the region, then we can</span>
<span class="udiff-line-removed">-     // not be sure that we have all references to it.</span>
<span class="udiff-line-removed">-     if (!region-&gt;rem_set()-&gt;is_complete()) {</span>
<span class="udiff-line-removed">-       return false;</span>
<span class="udiff-line-removed">-     }</span>
<span class="udiff-line-removed">-     // Candidate selection must satisfy the following constraints</span>
<span class="udiff-line-removed">-     // while concurrent marking is in progress:</span>
<span class="udiff-line-removed">-     //</span>
<span class="udiff-line-removed">-     // * In order to maintain SATB invariants, an object must not be</span>
<span class="udiff-line-removed">-     // reclaimed if it was allocated before the start of marking and</span>
<span class="udiff-line-removed">-     // has not had its references scanned.  Such an object must have</span>
<span class="udiff-line-removed">-     // its references (including type metadata) scanned to ensure no</span>
<span class="udiff-line-removed">-     // live objects are missed by the marking process.  Objects</span>
<span class="udiff-line-removed">-     // allocated after the start of concurrent marking don&#39;t need to</span>
<span class="udiff-line-removed">-     // be scanned.</span>
<span class="udiff-line-removed">-     //</span>
<span class="udiff-line-removed">-     // * An object must not be reclaimed if it is on the concurrent</span>
<span class="udiff-line-removed">-     // mark stack.  Objects allocated after the start of concurrent</span>
<span class="udiff-line-removed">-     // marking are never pushed on the mark stack.</span>
<span class="udiff-line-removed">-     //</span>
<span class="udiff-line-removed">-     // Nominating only objects allocated after the start of concurrent</span>
<span class="udiff-line-removed">-     // marking is sufficient to meet both constraints.  This may miss</span>
<span class="udiff-line-removed">-     // some objects that satisfy the constraints, but the marking data</span>
<span class="udiff-line-removed">-     // structures don&#39;t support efficiently performing the needed</span>
<span class="udiff-line-removed">-     // additional tests or scrubbing of the mark stack.</span>
<span class="udiff-line-removed">-     //</span>
<span class="udiff-line-removed">-     // However, we presently only nominate is_typeArray() objects.</span>
<span class="udiff-line-removed">-     // A humongous object containing references induces remembered</span>
<span class="udiff-line-removed">-     // set entries on other regions.  In order to reclaim such an</span>
<span class="udiff-line-removed">-     // object, those remembered sets would need to be cleaned up.</span>
<span class="udiff-line-removed">-     //</span>
<span class="udiff-line-removed">-     // We also treat is_typeArray() objects specially, allowing them</span>
<span class="udiff-line-removed">-     // to be reclaimed even if allocated before the start of</span>
<span class="udiff-line-removed">-     // concurrent mark.  For this we rely on mark stack insertion to</span>
<span class="udiff-line-removed">-     // exclude is_typeArray() objects, preventing reclaiming an object</span>
<span class="udiff-line-removed">-     // that is in the mark stack.  We also rely on the metadata for</span>
<span class="udiff-line-removed">-     // such objects to be built-in and so ensured to be kept live.</span>
<span class="udiff-line-removed">-     // Frequent allocation and drop of large binary blobs is an</span>
<span class="udiff-line-removed">-     // important use case for eager reclaim, and this special handling</span>
<span class="udiff-line-removed">-     // may reduce needed headroom.</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     return obj-&gt;is_typeArray() &amp;&amp;</span>
<span class="udiff-line-removed">-            g1h-&gt;is_potential_eager_reclaim_candidate(region);</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-  public:</span>
<span class="udiff-line-removed">-   RegisterHumongousWithInCSetFastTestClosure()</span>
<span class="udiff-line-removed">-   : _total_humongous(0),</span>
<span class="udiff-line-removed">-     _candidate_humongous(0),</span>
<span class="udiff-line-removed">-     _dcq(&amp;G1BarrierSet::dirty_card_queue_set()) {</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   virtual bool do_heap_region(HeapRegion* r) {</span>
<span class="udiff-line-removed">-     if (!r-&gt;is_starts_humongous()) {</span>
<span class="udiff-line-modified-added">+ #ifndef PRODUCT</span>
<span class="udiff-line-modified-added">+ void G1CollectedHeap::verify_region_attr_remset_update() {</span>
<span class="udiff-line-modified-added">+   class VerifyRegionAttrRemSet : public HeapRegionClosure {</span>
<span class="udiff-line-modified-added">+   public:</span>
<span class="udiff-line-modified-added">+     virtual bool do_heap_region(HeapRegion* r) {</span>
<span class="udiff-line-modified-added">+       G1CollectedHeap* g1h = G1CollectedHeap::heap();</span>
<span class="udiff-line-modified-added">+       bool const needs_remset_update = g1h-&gt;region_attr(r-&gt;bottom()).needs_remset_update();</span>
<span class="udiff-line-modified-added">+       assert(r-&gt;rem_set()-&gt;is_tracked() == needs_remset_update,</span>
<span class="udiff-line-modified-added">+              &quot;Region %u remset tracking status (%s) different to region attribute (%s)&quot;,</span>
<span class="udiff-line-modified-added">+              r-&gt;hrm_index(), BOOL_TO_STR(r-&gt;rem_set()-&gt;is_tracked()), BOOL_TO_STR(needs_remset_update));</span>
        return false;
      }
<span class="udiff-line-modified-removed">-     G1CollectedHeap* g1h = G1CollectedHeap::heap();</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     bool is_candidate = humongous_region_is_candidate(g1h, r);</span>
<span class="udiff-line-removed">-     uint rindex = r-&gt;hrm_index();</span>
<span class="udiff-line-removed">-     g1h-&gt;set_humongous_reclaim_candidate(rindex, is_candidate);</span>
<span class="udiff-line-removed">-     if (is_candidate) {</span>
<span class="udiff-line-removed">-       _candidate_humongous++;</span>
<span class="udiff-line-removed">-       g1h-&gt;register_humongous_region_with_cset(rindex);</span>
<span class="udiff-line-removed">-       // Is_candidate already filters out humongous object with large remembered sets.</span>
<span class="udiff-line-removed">-       // If we have a humongous object with a few remembered sets, we simply flush these</span>
<span class="udiff-line-removed">-       // remembered set entries into the DCQS. That will result in automatic</span>
<span class="udiff-line-removed">-       // re-evaluation of their remembered set entries during the following evacuation</span>
<span class="udiff-line-removed">-       // phase.</span>
<span class="udiff-line-removed">-       if (!r-&gt;rem_set()-&gt;is_empty()) {</span>
<span class="udiff-line-removed">-         guarantee(r-&gt;rem_set()-&gt;occupancy_less_or_equal_than(G1RSetSparseRegionEntries),</span>
<span class="udiff-line-removed">-                   &quot;Found a not-small remembered set here. This is inconsistent with previous assumptions.&quot;);</span>
<span class="udiff-line-removed">-         G1CardTable* ct = g1h-&gt;card_table();</span>
<span class="udiff-line-removed">-         HeapRegionRemSetIterator hrrs(r-&gt;rem_set());</span>
<span class="udiff-line-removed">-         size_t card_index;</span>
<span class="udiff-line-removed">-         while (hrrs.has_next(card_index)) {</span>
<span class="udiff-line-removed">-           CardTable::CardValue* card_ptr = ct-&gt;byte_for_index(card_index);</span>
<span class="udiff-line-removed">-           // The remembered set might contain references to already freed</span>
<span class="udiff-line-removed">-           // regions. Filter out such entries to avoid failing card table</span>
<span class="udiff-line-removed">-           // verification.</span>
<span class="udiff-line-removed">-           if (g1h-&gt;is_in_closed_subset(ct-&gt;addr_for(card_ptr))) {</span>
<span class="udiff-line-removed">-             if (*card_ptr != G1CardTable::dirty_card_val()) {</span>
<span class="udiff-line-removed">-               *card_ptr = G1CardTable::dirty_card_val();</span>
<span class="udiff-line-removed">-               _dcq.enqueue(card_ptr);</span>
<span class="udiff-line-removed">-             }</span>
<span class="udiff-line-removed">-           }</span>
<span class="udiff-line-removed">-         }</span>
<span class="udiff-line-removed">-         assert(hrrs.n_yielded() == r-&gt;rem_set()-&gt;occupied(),</span>
<span class="udiff-line-removed">-                &quot;Remembered set hash maps out of sync, cur: &quot; SIZE_FORMAT &quot; entries, next: &quot; SIZE_FORMAT &quot; entries&quot;,</span>
<span class="udiff-line-removed">-                hrrs.n_yielded(), r-&gt;rem_set()-&gt;occupied());</span>
<span class="udiff-line-removed">-         // We should only clear the card based remembered set here as we will not</span>
<span class="udiff-line-removed">-         // implicitly rebuild anything else during eager reclaim. Note that at the moment</span>
<span class="udiff-line-removed">-         // (and probably never) we do not enter this path if there are other kind of</span>
<span class="udiff-line-removed">-         // remembered sets for this region.</span>
<span class="udiff-line-removed">-         r-&gt;rem_set()-&gt;clear_locked(true /* only_cardset */);</span>
<span class="udiff-line-removed">-         // Clear_locked() above sets the state to Empty. However we want to continue</span>
<span class="udiff-line-removed">-         // collecting remembered set entries for humongous regions that were not</span>
<span class="udiff-line-removed">-         // reclaimed.</span>
<span class="udiff-line-removed">-         r-&gt;rem_set()-&gt;set_state_complete();</span>
<span class="udiff-line-removed">-       }</span>
<span class="udiff-line-removed">-       assert(r-&gt;rem_set()-&gt;is_empty(), &quot;At this point any humongous candidate remembered set must be empty.&quot;);</span>
<span class="udiff-line-removed">-     }</span>
<span class="udiff-line-removed">-     _total_humongous++;</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     return false;</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   size_t total_humongous() const { return _total_humongous; }</span>
<span class="udiff-line-removed">-   size_t candidate_humongous() const { return _candidate_humongous; }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   void flush_rem_set_entries() { _dcq.flush(); }</span>
<span class="udiff-line-removed">- };</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">- void G1CollectedHeap::register_humongous_regions_with_cset() {</span>
<span class="udiff-line-removed">-   if (!G1EagerReclaimHumongousObjects) {</span>
<span class="udiff-line-removed">-     phase_times()-&gt;record_fast_reclaim_humongous_stats(0.0, 0, 0);</span>
<span class="udiff-line-removed">-     return;</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">-   double time = os::elapsed_counter();</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   // Collect reclaim candidate information and register candidates with cset.</span>
<span class="udiff-line-removed">-   RegisterHumongousWithInCSetFastTestClosure cl;</span>
<span class="udiff-line-modified-added">+   } cl;</span>
    heap_region_iterate(&amp;cl);
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   time = ((double)(os::elapsed_counter() - time) / os::elapsed_frequency()) * 1000.0;</span>
<span class="udiff-line-removed">-   phase_times()-&gt;record_fast_reclaim_humongous_stats(time,</span>
<span class="udiff-line-removed">-                                                      cl.total_humongous(),</span>
<span class="udiff-line-removed">-                                                      cl.candidate_humongous());</span>
<span class="udiff-line-removed">-   _has_humongous_reclaim_candidates = cl.candidate_humongous() &gt; 0;</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   // Finally flush all remembered set entries to re-check into the global DCQS.</span>
<span class="udiff-line-removed">-   cl.flush_rem_set_entries();</span>
  }
<span class="udiff-line-added">+ #endif</span>
  
  class VerifyRegionRemSetClosure : public HeapRegionClosure {
    public:
      bool do_heap_region(HeapRegion* hr) {
        if (!hr-&gt;is_archive() &amp;&amp; !hr-&gt;is_continues_humongous()) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2861,37 +2883,131 @@</span>
      return false;
    }
  };
  
  void G1CollectedHeap::start_new_collection_set() {
<span class="udiff-line-added">+   double start = os::elapsedTime();</span>
<span class="udiff-line-added">+ </span>
    collection_set()-&gt;start_incremental_building();
  
<span class="udiff-line-modified-removed">-   clear_cset_fast_test();</span>
<span class="udiff-line-modified-added">+   clear_region_attr();</span>
  
    guarantee(_eden.length() == 0, &quot;eden should have been cleared&quot;);
    policy()-&gt;transfer_survivors_to_cset(survivor());
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   // We redo the verification but now wrt to the new CSet which</span>
<span class="udiff-line-added">+   // has just got initialized after the previous CSet was freed.</span>
<span class="udiff-line-added">+   _cm-&gt;verify_no_collection_set_oops();</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   phase_times()-&gt;record_start_new_cset_time_ms((os::elapsedTime() - start) * 1000.0);</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void G1CollectedHeap::calculate_collection_set(G1EvacuationInfo&amp; evacuation_info, double target_pause_time_ms) {</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   _collection_set.finalize_initial_collection_set(target_pause_time_ms, &amp;_survivor);</span>
<span class="udiff-line-added">+   evacuation_info.set_collectionset_regions(collection_set()-&gt;region_length() +</span>
<span class="udiff-line-added">+                                             collection_set()-&gt;optional_region_length());</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   _cm-&gt;verify_no_collection_set_oops();</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   if (_hr_printer.is_active()) {</span>
<span class="udiff-line-added">+     G1PrintCollectionSetClosure cl(&amp;_hr_printer);</span>
<span class="udiff-line-added">+     _collection_set.iterate(&amp;cl);</span>
<span class="udiff-line-added">+     _collection_set.iterate_optional(&amp;cl);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ G1HeapVerifier::G1VerifyType G1CollectedHeap::young_collection_verify_type() const {</span>
<span class="udiff-line-added">+   if (collector_state()-&gt;in_initial_mark_gc()) {</span>
<span class="udiff-line-added">+     return G1HeapVerifier::G1VerifyConcurrentStart;</span>
<span class="udiff-line-added">+   } else if (collector_state()-&gt;in_young_only_phase()) {</span>
<span class="udiff-line-added">+     return G1HeapVerifier::G1VerifyYoungNormal;</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     return G1HeapVerifier::G1VerifyMixed;</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void G1CollectedHeap::verify_before_young_collection(G1HeapVerifier::G1VerifyType type) {</span>
<span class="udiff-line-added">+   if (VerifyRememberedSets) {</span>
<span class="udiff-line-added">+     log_info(gc, verify)(&quot;[Verifying RemSets before GC]&quot;);</span>
<span class="udiff-line-added">+     VerifyRegionRemSetClosure v_cl;</span>
<span class="udiff-line-added">+     heap_region_iterate(&amp;v_cl);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+   _verifier-&gt;verify_before_gc(type);</span>
<span class="udiff-line-added">+   _verifier-&gt;check_bitmaps(&quot;GC Start&quot;);</span>
<span class="udiff-line-added">+   verify_numa_regions(&quot;GC Start&quot;);</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void G1CollectedHeap::verify_after_young_collection(G1HeapVerifier::G1VerifyType type) {</span>
<span class="udiff-line-added">+   if (VerifyRememberedSets) {</span>
<span class="udiff-line-added">+     log_info(gc, verify)(&quot;[Verifying RemSets after GC]&quot;);</span>
<span class="udiff-line-added">+     VerifyRegionRemSetClosure v_cl;</span>
<span class="udiff-line-added">+     heap_region_iterate(&amp;v_cl);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+   _verifier-&gt;verify_after_gc(type);</span>
<span class="udiff-line-added">+   _verifier-&gt;check_bitmaps(&quot;GC End&quot;);</span>
<span class="udiff-line-added">+   verify_numa_regions(&quot;GC End&quot;);</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void G1CollectedHeap::expand_heap_after_young_collection(){</span>
<span class="udiff-line-added">+   size_t expand_bytes = _heap_sizing_policy-&gt;expansion_amount();</span>
<span class="udiff-line-added">+   if (expand_bytes &gt; 0) {</span>
<span class="udiff-line-added">+     // No need for an ergo logging here,</span>
<span class="udiff-line-added">+     // expansion_amount() does this when it returns a value &gt; 0.</span>
<span class="udiff-line-added">+     double expand_ms;</span>
<span class="udiff-line-added">+     if (!expand(expand_bytes, _workers, &amp;expand_ms)) {</span>
<span class="udiff-line-added">+       // We failed to expand the heap. Cannot do anything about it.</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+     phase_times()-&gt;record_expand_heap_time(expand_ms);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ const char* G1CollectedHeap::young_gc_name() const {</span>
<span class="udiff-line-added">+   if (collector_state()-&gt;in_initial_mark_gc()) {</span>
<span class="udiff-line-added">+     return &quot;Pause Young (Concurrent Start)&quot;;</span>
<span class="udiff-line-added">+   } else if (collector_state()-&gt;in_young_only_phase()) {</span>
<span class="udiff-line-added">+     if (collector_state()-&gt;in_young_gc_before_mixed()) {</span>
<span class="udiff-line-added">+       return &quot;Pause Young (Prepare Mixed)&quot;;</span>
<span class="udiff-line-added">+     } else {</span>
<span class="udiff-line-added">+       return &quot;Pause Young (Normal)&quot;;</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     return &quot;Pause Young (Mixed)&quot;;</span>
<span class="udiff-line-added">+   }</span>
  }
  
<span class="udiff-line-modified-removed">- bool</span>
<span class="udiff-line-removed">- G1CollectedHeap::do_collection_pause_at_safepoint(double target_pause_time_ms) {</span>
<span class="udiff-line-modified-added">+ bool G1CollectedHeap::do_collection_pause_at_safepoint(double target_pause_time_ms) {</span>
    assert_at_safepoint_on_vm_thread();
    guarantee(!is_gc_active(), &quot;collection is not reentrant&quot;);
  
    if (GCLocker::check_active_before_gc()) {
      return false;
    }
  
<span class="udiff-line-modified-removed">-   _gc_timer_stw-&gt;register_gc_start();</span>
<span class="udiff-line-modified-added">+   do_collection_pause_at_safepoint_helper(target_pause_time_ms);</span>
<span class="udiff-line-added">+   if (should_upgrade_to_full_gc(gc_cause())) {</span>
<span class="udiff-line-added">+     log_info(gc, ergo)(&quot;Attempting maximally compacting collection&quot;);</span>
<span class="udiff-line-added">+     bool result = do_full_collection(false /* explicit gc */,</span>
<span class="udiff-line-added">+                                      true /* clear_all_soft_refs */);</span>
<span class="udiff-line-added">+     // do_full_collection only fails if blocked by GC locker, but</span>
<span class="udiff-line-added">+     // we&#39;ve already checked for that above.</span>
<span class="udiff-line-added">+     assert(result, &quot;invariant&quot;);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+   return true;</span>
<span class="udiff-line-added">+ }</span>
  
<span class="udiff-line-added">+ void G1CollectedHeap::do_collection_pause_at_safepoint_helper(double target_pause_time_ms) {</span>
    GCIdMark gc_id_mark;
<span class="udiff-line-removed">-   _gc_tracer_stw-&gt;report_gc_start(gc_cause(), _gc_timer_stw-&gt;gc_start());</span>
  
    SvcGCMarker sgcm(SvcGCMarker::MINOR);
    ResourceMark rm;
  
    policy()-&gt;note_gc_start();
  
<span class="udiff-line-added">+   _gc_timer_stw-&gt;register_gc_start();</span>
<span class="udiff-line-added">+   _gc_tracer_stw-&gt;report_gc_start(gc_cause(), _gc_timer_stw-&gt;gc_start());</span>
<span class="udiff-line-added">+ </span>
    wait_for_root_region_scanning();
  
    print_heap_before_gc();
    print_heap_regions();
    trace_heap_before_gc(_gc_tracer_stw);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2907,52 +3023,31 @@</span>
      policy()-&gt;decide_on_conc_mark_initiation();
    }
  
    // We do not allow initial-mark to be piggy-backed on a mixed GC.
    assert(!collector_state()-&gt;in_initial_mark_gc() ||
<span class="udiff-line-modified-removed">-           collector_state()-&gt;in_young_only_phase(), &quot;sanity&quot;);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-modified-added">+          collector_state()-&gt;in_young_only_phase(), &quot;sanity&quot;);</span>
    // We also do not allow mixed GCs during marking.
    assert(!collector_state()-&gt;mark_or_rebuild_in_progress() || collector_state()-&gt;in_young_only_phase(), &quot;sanity&quot;);
  
    // Record whether this pause is an initial mark. When the current
    // thread has completed its logging output and it&#39;s safe to signal
    // the CM thread, the flag&#39;s value in the policy has been reset.
    bool should_start_conc_mark = collector_state()-&gt;in_initial_mark_gc();
<span class="udiff-line-added">+   if (should_start_conc_mark) {</span>
<span class="udiff-line-added">+     _cm-&gt;gc_tracer_cm()-&gt;set_gc_cause(gc_cause());</span>
<span class="udiff-line-added">+   }</span>
  
    // Inner scope for scope based logging, timers, and stats collection
    {
      G1EvacuationInfo evacuation_info;
  
<span class="udiff-line-removed">-     if (collector_state()-&gt;in_initial_mark_gc()) {</span>
<span class="udiff-line-removed">-       // We are about to start a marking cycle, so we increment the</span>
<span class="udiff-line-removed">-       // full collection counter.</span>
<span class="udiff-line-removed">-       increment_old_marking_cycles_started();</span>
<span class="udiff-line-removed">-       _cm-&gt;gc_tracer_cm()-&gt;set_gc_cause(gc_cause());</span>
<span class="udiff-line-removed">-     }</span>
<span class="udiff-line-removed">- </span>
      _gc_tracer_stw-&gt;report_yc_type(collector_state()-&gt;yc_type());
  
      GCTraceCPUTime tcpu;
  
<span class="udiff-line-modified-removed">-     G1HeapVerifier::G1VerifyType verify_type;</span>
<span class="udiff-line-removed">-     FormatBuffer&lt;&gt; gc_string(&quot;Pause Young &quot;);</span>
<span class="udiff-line-removed">-     if (collector_state()-&gt;in_initial_mark_gc()) {</span>
<span class="udiff-line-removed">-       gc_string.append(&quot;(Concurrent Start)&quot;);</span>
<span class="udiff-line-removed">-       verify_type = G1HeapVerifier::G1VerifyConcurrentStart;</span>
<span class="udiff-line-removed">-     } else if (collector_state()-&gt;in_young_only_phase()) {</span>
<span class="udiff-line-removed">-       if (collector_state()-&gt;in_young_gc_before_mixed()) {</span>
<span class="udiff-line-removed">-         gc_string.append(&quot;(Prepare Mixed)&quot;);</span>
<span class="udiff-line-removed">-       } else {</span>
<span class="udiff-line-removed">-         gc_string.append(&quot;(Normal)&quot;);</span>
<span class="udiff-line-removed">-       }</span>
<span class="udiff-line-removed">-       verify_type = G1HeapVerifier::G1VerifyYoungNormal;</span>
<span class="udiff-line-removed">-     } else {</span>
<span class="udiff-line-removed">-       gc_string.append(&quot;(Mixed)&quot;);</span>
<span class="udiff-line-removed">-       verify_type = G1HeapVerifier::G1VerifyMixed;</span>
<span class="udiff-line-removed">-     }</span>
<span class="udiff-line-removed">-     GCTraceTime(Info, gc) tm(gc_string, NULL, gc_cause(), true);</span>
<span class="udiff-line-modified-added">+     GCTraceTime(Info, gc) tm(young_gc_name(), NULL, gc_cause(), true);</span>
  
      uint active_workers = WorkerPolicy::calc_active_workers(workers()-&gt;total_workers(),
                                                              workers()-&gt;active_workers(),
                                                              Threads::number_of_non_daemon_threads());
      active_workers = workers()-&gt;update_active_workers(active_workers);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2961,134 +3056,65 @@</span>
      G1MonitoringScope ms(g1mm(),
                           false /* full_gc */,
                           collector_state()-&gt;yc_type() == Mixed /* all_memory_pools_affected */);
  
      G1HeapTransition heap_transition(this);
<span class="udiff-line-removed">-     size_t heap_used_bytes_before_gc = used();</span>
  
<span class="udiff-line-modified-removed">-     // Don&#39;t dynamically change the number of GC threads this early.  A value of</span>
<span class="udiff-line-removed">-     // 0 is used to indicate serial work.  When parallel work is done,</span>
<span class="udiff-line-removed">-     // it will be set.</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     { // Call to jvmpi::post_class_unload_events must occur outside of active GC</span>
<span class="udiff-line-modified-added">+     {</span>
        IsGCActiveMark x;
  
        gc_prologue(false);
  
<span class="udiff-line-modified-removed">-       if (VerifyRememberedSets) {</span>
<span class="udiff-line-modified-removed">-         log_info(gc, verify)(&quot;[Verifying RemSets before GC]&quot;);</span>
<span class="udiff-line-removed">-         VerifyRegionRemSetClosure v_cl;</span>
<span class="udiff-line-removed">-         heap_region_iterate(&amp;v_cl);</span>
<span class="udiff-line-removed">-       }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       _verifier-&gt;verify_before_gc(verify_type);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       _verifier-&gt;check_bitmaps(&quot;GC Start&quot;);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">- #if COMPILER2_OR_JVMCI</span>
<span class="udiff-line-removed">-       DerivedPointerTable::clear();</span>
<span class="udiff-line-removed">- #endif</span>
<span class="udiff-line-modified-added">+       G1HeapVerifier::G1VerifyType verify_type = young_collection_verify_type();</span>
<span class="udiff-line-modified-added">+       verify_before_young_collection(verify_type);</span>
  
<span class="udiff-line-modified-removed">-       // Please see comment in g1CollectedHeap.hpp and</span>
<span class="udiff-line-modified-removed">-       // G1CollectedHeap::ref_processing_init() to see how</span>
<span class="udiff-line-modified-removed">-       // reference processing currently works in G1.</span>
<span class="udiff-line-modified-added">+       {</span>
<span class="udiff-line-modified-added">+         // The elapsed time induced by the start time below deliberately elides</span>
<span class="udiff-line-modified-added">+         // the possible verification above.</span>
<span class="udiff-line-added">+         double sample_start_time_sec = os::elapsedTime();</span>
  
<span class="udiff-line-modified-removed">-       // Enable discovery in the STW reference processor</span>
<span class="udiff-line-modified-removed">-       _ref_processor_stw-&gt;enable_discovery();</span>
<span class="udiff-line-modified-added">+         // Please see comment in g1CollectedHeap.hpp and</span>
<span class="udiff-line-modified-added">+         // G1CollectedHeap::ref_processing_init() to see how</span>
<span class="udiff-line-added">+         // reference processing currently works in G1.</span>
<span class="udiff-line-added">+         _ref_processor_stw-&gt;enable_discovery();</span>
  
<span class="udiff-line-removed">-       {</span>
          // We want to temporarily turn off discovery by the
          // CM ref processor, if necessary, and turn it back on
          // on again later if we do. Using a scoped
          // NoRefDiscovery object will do this.
          NoRefDiscovery no_cm_discovery(_ref_processor_cm);
  
<span class="udiff-line-removed">-         // Forget the current alloc region (we might even choose it to be part</span>
<span class="udiff-line-removed">-         // of the collection set!).</span>
<span class="udiff-line-removed">-         _allocator-&gt;release_mutator_alloc_region();</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         // This timing is only used by the ergonomics to handle our pause target.</span>
<span class="udiff-line-removed">-         // It is unclear why this should not include the full pause. We will</span>
<span class="udiff-line-removed">-         // investigate this in CR 7178365.</span>
<span class="udiff-line-removed">-         //</span>
<span class="udiff-line-removed">-         // Preserving the old comment here if that helps the investigation:</span>
<span class="udiff-line-removed">-         //</span>
<span class="udiff-line-removed">-         // The elapsed time induced by the start time below deliberately elides</span>
<span class="udiff-line-removed">-         // the possible verification above.</span>
<span class="udiff-line-removed">-         double sample_start_time_sec = os::elapsedTime();</span>
<span class="udiff-line-removed">- </span>
          policy()-&gt;record_collection_pause_start(sample_start_time_sec);
  
<span class="udiff-line-modified-removed">-         if (collector_state()-&gt;in_initial_mark_gc()) {</span>
<span class="udiff-line-modified-removed">-           concurrent_mark()-&gt;pre_initial_mark();</span>
<span class="udiff-line-modified-removed">-         }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         policy()-&gt;finalize_collection_set(target_pause_time_ms, &amp;_survivor);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         evacuation_info.set_collectionset_regions(collection_set()-&gt;region_length());</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         register_humongous_regions_with_cset();</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         assert(_verifier-&gt;check_cset_fast_test(), &quot;Inconsistency in the InCSetState table.&quot;);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         // We call this after finalize_cset() to</span>
<span class="udiff-line-removed">-         // ensure that the CSet has been finalized.</span>
<span class="udiff-line-removed">-         _cm-&gt;verify_no_cset_oops();</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         if (_hr_printer.is_active()) {</span>
<span class="udiff-line-removed">-           G1PrintCollectionSetClosure cl(&amp;_hr_printer);</span>
<span class="udiff-line-removed">-           _collection_set.iterate(&amp;cl);</span>
<span class="udiff-line-removed">-         }</span>
<span class="udiff-line-modified-added">+         // Forget the current allocation region (we might even choose it to be part</span>
<span class="udiff-line-modified-added">+         // of the collection set!).</span>
<span class="udiff-line-modified-added">+         _allocator-&gt;release_mutator_alloc_regions();</span>
  
<span class="udiff-line-modified-removed">-         // Initialize the GC alloc regions.</span>
<span class="udiff-line-removed">-         _allocator-&gt;init_gc_alloc_regions(evacuation_info);</span>
<span class="udiff-line-modified-added">+         calculate_collection_set(evacuation_info, target_pause_time_ms);</span>
  
<span class="udiff-line-added">+         G1RedirtyCardsQueueSet rdcqs(G1BarrierSet::dirty_card_queue_set().allocator());</span>
          G1ParScanThreadStateSet per_thread_states(this,
<span class="udiff-line-added">+                                                   &amp;rdcqs,</span>
                                                    workers()-&gt;active_workers(),
                                                    collection_set()-&gt;young_region_length(),
                                                    collection_set()-&gt;optional_region_length());
<span class="udiff-line-modified-removed">-         pre_evacuate_collection_set();</span>
<span class="udiff-line-modified-added">+         pre_evacuate_collection_set(evacuation_info, &amp;per_thread_states);</span>
  
          // Actually do the work...
<span class="udiff-line-modified-removed">-         evacuate_collection_set(&amp;per_thread_states);</span>
<span class="udiff-line-removed">-         evacuate_optional_collection_set(&amp;per_thread_states);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         post_evacuate_collection_set(evacuation_info, &amp;per_thread_states);</span>
<span class="udiff-line-modified-added">+         evacuate_initial_collection_set(&amp;per_thread_states);</span>
  
<span class="udiff-line-modified-removed">-         const size_t* surviving_young_words = per_thread_states.surviving_young_words();</span>
<span class="udiff-line-modified-removed">-         free_collection_set(&amp;_collection_set, evacuation_info, surviving_young_words);</span>
<span class="udiff-line-modified-added">+         if (_collection_set.optional_region_length() != 0) {</span>
<span class="udiff-line-modified-added">+           evacuate_optional_collection_set(&amp;per_thread_states);</span>
<span class="udiff-line-added">+         }</span>
<span class="udiff-line-added">+         post_evacuate_collection_set(evacuation_info, &amp;rdcqs, &amp;per_thread_states);</span>
  
<span class="udiff-line-modified-removed">-         eagerly_reclaim_humongous_regions();</span>
<span class="udiff-line-modified-added">+         start_new_collection_set();</span>
  
<span class="udiff-line-removed">-         record_obj_copy_mem_stats();</span>
          _survivor_evac_stats.adjust_desired_plab_sz();
          _old_evac_stats.adjust_desired_plab_sz();
  
<span class="udiff-line-modified-removed">-         double start = os::elapsedTime();</span>
<span class="udiff-line-removed">-         start_new_collection_set();</span>
<span class="udiff-line-removed">-         phase_times()-&gt;record_start_new_cset_time_ms((os::elapsedTime() - start) * 1000.0);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         if (evacuation_failed()) {</span>
<span class="udiff-line-removed">-           double recalculate_used_start = os::elapsedTime();</span>
<span class="udiff-line-removed">-           set_used(recalculate_used());</span>
<span class="udiff-line-removed">-           phase_times()-&gt;record_evac_fail_recalc_used_time((os::elapsedTime() - recalculate_used_start) * 1000.0);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-           if (_archive_allocator != NULL) {</span>
<span class="udiff-line-removed">-             _archive_allocator-&gt;clear_used();</span>
<span class="udiff-line-removed">-           }</span>
<span class="udiff-line-removed">-           for (uint i = 0; i &lt; ParallelGCThreads; i++) {</span>
<span class="udiff-line-removed">-             if (_evacuation_failed_info_array[i].has_failed()) {</span>
<span class="udiff-line-removed">-               _gc_tracer_stw-&gt;report_evacuation_failed(_evacuation_failed_info_array[i]);</span>
<span class="udiff-line-removed">-             }</span>
<span class="udiff-line-removed">-           }</span>
<span class="udiff-line-removed">-         } else {</span>
<span class="udiff-line-removed">-           // The &quot;used&quot; of the the collection set have already been subtracted</span>
<span class="udiff-line-removed">-           // when they were freed.  Add in the bytes evacuated.</span>
<span class="udiff-line-removed">-           increase_used(policy()-&gt;bytes_copied_during_gc());</span>
<span class="udiff-line-removed">-         }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         if (collector_state()-&gt;in_initial_mark_gc()) {</span>
<span class="udiff-line-modified-added">+         if (should_start_conc_mark) {</span>
            // We have to do this before we notify the CM threads that
            // they can start working to make sure that all the
            // appropriate initialization is done on the CM object.
            concurrent_mark()-&gt;post_initial_mark();
            // Note that we don&#39;t actually trigger the CM thread at
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3096,59 +3122,20 @@</span>
            // the current thread has completed its logging output.
          }
  
          allocate_dummy_regions();
  
<span class="udiff-line-modified-removed">-         _allocator-&gt;init_mutator_alloc_region();</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         {</span>
<span class="udiff-line-removed">-           size_t expand_bytes = _heap_sizing_policy-&gt;expansion_amount();</span>
<span class="udiff-line-removed">-           if (expand_bytes &gt; 0) {</span>
<span class="udiff-line-removed">-             size_t bytes_before = capacity();</span>
<span class="udiff-line-removed">-             // No need for an ergo logging here,</span>
<span class="udiff-line-removed">-             // expansion_amount() does this when it returns a value &gt; 0.</span>
<span class="udiff-line-removed">-             double expand_ms;</span>
<span class="udiff-line-removed">-             if (!expand(expand_bytes, _workers, &amp;expand_ms)) {</span>
<span class="udiff-line-removed">-               // We failed to expand the heap. Cannot do anything about it.</span>
<span class="udiff-line-removed">-             }</span>
<span class="udiff-line-removed">-             phase_times()-&gt;record_expand_heap_time(expand_ms);</span>
<span class="udiff-line-removed">-           }</span>
<span class="udiff-line-removed">-         }</span>
<span class="udiff-line-modified-added">+         _allocator-&gt;init_mutator_alloc_regions();</span>
  
<span class="udiff-line-modified-removed">-         // We redo the verification but now wrt to the new CSet which</span>
<span class="udiff-line-removed">-         // has just got initialized after the previous CSet was freed.</span>
<span class="udiff-line-removed">-         _cm-&gt;verify_no_cset_oops();</span>
<span class="udiff-line-modified-added">+         expand_heap_after_young_collection();</span>
  
<span class="udiff-line-removed">-         // This timing is only used by the ergonomics to handle our pause target.</span>
<span class="udiff-line-removed">-         // It is unclear why this should not include the full pause. We will</span>
<span class="udiff-line-removed">-         // investigate this in CR 7178365.</span>
          double sample_end_time_sec = os::elapsedTime();
          double pause_time_ms = (sample_end_time_sec - sample_start_time_sec) * MILLIUNITS;
<span class="udiff-line-modified-removed">-         size_t total_cards_scanned = phase_times()-&gt;sum_thread_work_items(G1GCPhaseTimes::ScanRS, G1GCPhaseTimes::ScanRSScannedCards);</span>
<span class="udiff-line-removed">-         policy()-&gt;record_collection_pause_end(pause_time_ms, total_cards_scanned, heap_used_bytes_before_gc);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         evacuation_info.set_collectionset_used_before(collection_set()-&gt;bytes_used_before());</span>
<span class="udiff-line-removed">-         evacuation_info.set_bytes_copied(policy()-&gt;bytes_copied_during_gc());</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         if (VerifyRememberedSets) {</span>
<span class="udiff-line-removed">-           log_info(gc, verify)(&quot;[Verifying RemSets after GC]&quot;);</span>
<span class="udiff-line-removed">-           VerifyRegionRemSetClosure v_cl;</span>
<span class="udiff-line-removed">-           heap_region_iterate(&amp;v_cl);</span>
<span class="udiff-line-removed">-         }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         _verifier-&gt;verify_after_gc(verify_type);</span>
<span class="udiff-line-removed">-         _verifier-&gt;check_bitmaps(&quot;GC End&quot;);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         assert(!_ref_processor_stw-&gt;discovery_enabled(), &quot;Postcondition&quot;);</span>
<span class="udiff-line-removed">-         _ref_processor_stw-&gt;verify_no_references_recorded();</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         // CM reference discovery will be re-enabled if necessary.</span>
<span class="udiff-line-modified-added">+         policy()-&gt;record_collection_pause_end(pause_time_ms);</span>
        }
  
<span class="udiff-line-modified-removed">- #ifdef TRACESPINNING</span>
<span class="udiff-line-removed">-       ParallelTaskTerminator::print_termination_counts();</span>
<span class="udiff-line-removed">- #endif</span>
<span class="udiff-line-modified-added">+       verify_after_young_collection(verify_type);</span>
  
        gc_epilogue(false);
      }
  
      // Print the remainder of the GC log output.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3157,15 +3144,10 @@</span>
      }
  
      policy()-&gt;print_phases();
      heap_transition.print();
  
<span class="udiff-line-removed">-     // It is not yet to safe to tell the concurrent mark to</span>
<span class="udiff-line-removed">-     // start as we have some optional output below. We don&#39;t want the</span>
<span class="udiff-line-removed">-     // output from the concurrent mark thread interfering with this</span>
<span class="udiff-line-removed">-     // logging output either.</span>
<span class="udiff-line-removed">- </span>
      _hrm-&gt;verify_optional();
      _verifier-&gt;verify_region_sets_optional();
  
      TASKQUEUE_STATS_ONLY(print_taskqueue_stats());
      TASKQUEUE_STATS_ONLY(reset_taskqueue_stats());
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3188,39 +3170,34 @@</span>
    // It should now be safe to tell the concurrent mark thread to start
    // without its logging output interfering with the logging output
    // that came from the pause.
  
    if (should_start_conc_mark) {
<span class="udiff-line-modified-removed">-     // CAUTION: after the doConcurrentMark() call below,</span>
<span class="udiff-line-modified-removed">-     // the concurrent marking thread(s) could be running</span>
<span class="udiff-line-modified-removed">-     // concurrently with us. Make sure that anything after</span>
<span class="udiff-line-modified-removed">-     // this point does not assume that we are the only GC thread</span>
<span class="udiff-line-modified-removed">-     // running. Note: of course, the actual marking work will</span>
<span class="udiff-line-removed">-     // not start until the safepoint itself is released in</span>
<span class="udiff-line-removed">-     // SuspendibleThreadSet::desynchronize().</span>
<span class="udiff-line-modified-added">+     // CAUTION: after the doConcurrentMark() call below, the concurrent marking</span>
<span class="udiff-line-modified-added">+     // thread(s) could be running concurrently with us. Make sure that anything</span>
<span class="udiff-line-modified-added">+     // after this point does not assume that we are the only GC thread running.</span>
<span class="udiff-line-modified-added">+     // Note: of course, the actual marking work will not start until the safepoint</span>
<span class="udiff-line-modified-added">+     // itself is released in SuspendibleThreadSet::desynchronize().</span>
      do_concurrent_mark();
    }
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   return true;</span>
  }
  
<span class="udiff-line-modified-removed">- void G1CollectedHeap::remove_self_forwarding_pointers() {</span>
<span class="udiff-line-modified-removed">-   G1ParRemoveSelfForwardPtrsTask rsfp_task;</span>
<span class="udiff-line-modified-added">+ void G1CollectedHeap::remove_self_forwarding_pointers(G1RedirtyCardsQueueSet* rdcqs) {</span>
<span class="udiff-line-modified-added">+   G1ParRemoveSelfForwardPtrsTask rsfp_task(rdcqs);</span>
    workers()-&gt;run_task(&amp;rsfp_task);
  }
  
<span class="udiff-line-modified-removed">- void G1CollectedHeap::restore_after_evac_failure() {</span>
<span class="udiff-line-modified-added">+ void G1CollectedHeap::restore_after_evac_failure(G1RedirtyCardsQueueSet* rdcqs) {</span>
    double remove_self_forwards_start = os::elapsedTime();
  
<span class="udiff-line-modified-removed">-   remove_self_forwarding_pointers();</span>
<span class="udiff-line-modified-removed">-   SharedRestorePreservedMarksTaskExecutor task_executor(workers());</span>
<span class="udiff-line-removed">-   _preserved_marks_set.restore(&amp;task_executor);</span>
<span class="udiff-line-modified-added">+   remove_self_forwarding_pointers(rdcqs);</span>
<span class="udiff-line-modified-added">+   _preserved_marks_set.restore(workers());</span>
  
    phase_times()-&gt;record_evac_fail_remove_self_forwards((os::elapsedTime() - remove_self_forwards_start) * 1000.0);
  }
  
<span class="udiff-line-modified-removed">- void G1CollectedHeap::preserve_mark_during_evac_failure(uint worker_id, oop obj, markOop m) {</span>
<span class="udiff-line-modified-added">+ void G1CollectedHeap::preserve_mark_during_evac_failure(uint worker_id, oop obj, markWord m) {</span>
    if (!_evacuation_failed) {
      _evacuation_failed = true;
    }
  
    _evacuation_failed_info_array[worker_id].register_copy_failure(obj-&gt;size());
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3247,94 +3224,14 @@</span>
      pss-&gt;steal_and_trim_queue(queues());
      event.commit(GCId::current(), pss-&gt;worker_id(), G1GCPhaseTimes::phase_name(_phase));
    } while (!offer_termination());
  }
  
<span class="udiff-line-removed">- class G1ParTask : public AbstractGangTask {</span>
<span class="udiff-line-removed">- protected:</span>
<span class="udiff-line-removed">-   G1CollectedHeap*         _g1h;</span>
<span class="udiff-line-removed">-   G1ParScanThreadStateSet* _pss;</span>
<span class="udiff-line-removed">-   RefToScanQueueSet*       _queues;</span>
<span class="udiff-line-removed">-   G1RootProcessor*         _root_processor;</span>
<span class="udiff-line-removed">-   TaskTerminator           _terminator;</span>
<span class="udiff-line-removed">-   uint                     _n_workers;</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">- public:</span>
<span class="udiff-line-removed">-   G1ParTask(G1CollectedHeap* g1h, G1ParScanThreadStateSet* per_thread_states, RefToScanQueueSet *task_queues, G1RootProcessor* root_processor, uint n_workers)</span>
<span class="udiff-line-removed">-     : AbstractGangTask(&quot;G1 collection&quot;),</span>
<span class="udiff-line-removed">-       _g1h(g1h),</span>
<span class="udiff-line-removed">-       _pss(per_thread_states),</span>
<span class="udiff-line-removed">-       _queues(task_queues),</span>
<span class="udiff-line-removed">-       _root_processor(root_processor),</span>
<span class="udiff-line-removed">-       _terminator(n_workers, _queues),</span>
<span class="udiff-line-removed">-       _n_workers(n_workers)</span>
<span class="udiff-line-removed">-   {}</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   void work(uint worker_id) {</span>
<span class="udiff-line-removed">-     if (worker_id &gt;= _n_workers) return;  // no work needed this round</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     double start_sec = os::elapsedTime();</span>
<span class="udiff-line-removed">-     _g1h-&gt;phase_times()-&gt;record_time_secs(G1GCPhaseTimes::GCWorkerStart, worker_id, start_sec);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     {</span>
<span class="udiff-line-removed">-       ResourceMark rm;</span>
<span class="udiff-line-removed">-       HandleMark   hm;</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       ReferenceProcessor*             rp = _g1h-&gt;ref_processor_stw();</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       G1ParScanThreadState*           pss = _pss-&gt;state_for_worker(worker_id);</span>
<span class="udiff-line-removed">-       pss-&gt;set_ref_discoverer(rp);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       double start_strong_roots_sec = os::elapsedTime();</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       _root_processor-&gt;evacuate_roots(pss, worker_id);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       _g1h-&gt;rem_set()-&gt;oops_into_collection_set_do(pss, worker_id);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       double strong_roots_sec = os::elapsedTime() - start_strong_roots_sec;</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       double term_sec = 0.0;</span>
<span class="udiff-line-removed">-       size_t evac_term_attempts = 0;</span>
<span class="udiff-line-removed">-       {</span>
<span class="udiff-line-removed">-         double start = os::elapsedTime();</span>
<span class="udiff-line-removed">-         G1ParEvacuateFollowersClosure evac(_g1h, pss, _queues, _terminator.terminator(), G1GCPhaseTimes::ObjCopy);</span>
<span class="udiff-line-removed">-         evac.do_void();</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         evac_term_attempts = evac.term_attempts();</span>
<span class="udiff-line-removed">-         term_sec = evac.term_time();</span>
<span class="udiff-line-removed">-         double elapsed_sec = os::elapsedTime() - start;</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         G1GCPhaseTimes* p = _g1h-&gt;phase_times();</span>
<span class="udiff-line-removed">-         p-&gt;add_time_secs(G1GCPhaseTimes::ObjCopy, worker_id, elapsed_sec - term_sec);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         p-&gt;record_or_add_thread_work_item(G1GCPhaseTimes::ObjCopy,</span>
<span class="udiff-line-removed">-                                           worker_id,</span>
<span class="udiff-line-removed">-                                           pss-&gt;lab_waste_words() * HeapWordSize,</span>
<span class="udiff-line-removed">-                                           G1GCPhaseTimes::ObjCopyLABWaste);</span>
<span class="udiff-line-removed">-         p-&gt;record_or_add_thread_work_item(G1GCPhaseTimes::ObjCopy,</span>
<span class="udiff-line-removed">-                                           worker_id,</span>
<span class="udiff-line-removed">-                                           pss-&gt;lab_undo_waste_words() * HeapWordSize,</span>
<span class="udiff-line-removed">-                                           G1GCPhaseTimes::ObjCopyLABUndoWaste);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         p-&gt;record_time_secs(G1GCPhaseTimes::Termination, worker_id, term_sec);</span>
<span class="udiff-line-removed">-         p-&gt;record_thread_work_item(G1GCPhaseTimes::Termination, worker_id, evac_term_attempts);</span>
<span class="udiff-line-removed">-       }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       assert(pss-&gt;queue_is_empty(), &quot;should be empty&quot;);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       // Close the inner scope so that the ResourceMark and HandleMark</span>
<span class="udiff-line-removed">-       // destructors are executed here and are included as part of the</span>
<span class="udiff-line-removed">-       // &quot;GC Worker Time&quot;.</span>
<span class="udiff-line-removed">-     }</span>
<span class="udiff-line-removed">-     _g1h-&gt;phase_times()-&gt;record_time_secs(G1GCPhaseTimes::GCWorkerEnd, worker_id, os::elapsedTime());</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">- };</span>
<span class="udiff-line-removed">- </span>
  void G1CollectedHeap::complete_cleaning(BoolObjectClosure* is_alive,
                                          bool class_unloading_occurred) {
    uint num_workers = workers()-&gt;active_workers();
<span class="udiff-line-modified-removed">-   ParallelCleaningTask unlink_task(is_alive, num_workers, class_unloading_occurred, false);</span>
<span class="udiff-line-modified-added">+   G1ParallelCleaningTask unlink_task(is_alive, num_workers, class_unloading_occurred, false);</span>
    workers()-&gt;run_task(&amp;unlink_task);
  }
  
  // Clean string dedup data structures.
  // Ideally we would prefer to use a StringDedupCleaningTask here, but we want to
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3381,37 +3278,51 @@</span>
    workers()-&gt;run_task(&amp;cl);
  }
  
  class G1RedirtyLoggedCardsTask : public AbstractGangTask {
   private:
<span class="udiff-line-modified-removed">-   G1DirtyCardQueueSet* _queue;</span>
<span class="udiff-line-modified-added">+   G1RedirtyCardsQueueSet* _qset;</span>
    G1CollectedHeap* _g1h;
<span class="udiff-line-added">+   BufferNode* volatile _nodes;</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   void par_apply(RedirtyLoggedCardTableEntryClosure* cl, uint worker_id) {</span>
<span class="udiff-line-added">+     size_t buffer_size = _qset-&gt;buffer_size();</span>
<span class="udiff-line-added">+     BufferNode* next = Atomic::load(&amp;_nodes);</span>
<span class="udiff-line-added">+     while (next != NULL) {</span>
<span class="udiff-line-added">+       BufferNode* node = next;</span>
<span class="udiff-line-added">+       next = Atomic::cmpxchg(&amp;_nodes, node, node-&gt;next());</span>
<span class="udiff-line-added">+       if (next == node) {</span>
<span class="udiff-line-added">+         cl-&gt;apply_to_buffer(node, buffer_size, worker_id);</span>
<span class="udiff-line-added">+         next = node-&gt;next();</span>
<span class="udiff-line-added">+       }</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ </span>
   public:
<span class="udiff-line-modified-removed">-   G1RedirtyLoggedCardsTask(G1DirtyCardQueueSet* queue, G1CollectedHeap* g1h) : AbstractGangTask(&quot;Redirty Cards&quot;),</span>
<span class="udiff-line-modified-removed">-     _queue(queue), _g1h(g1h) { }</span>
<span class="udiff-line-modified-added">+   G1RedirtyLoggedCardsTask(G1RedirtyCardsQueueSet* qset, G1CollectedHeap* g1h) :</span>
<span class="udiff-line-modified-added">+     AbstractGangTask(&quot;Redirty Cards&quot;),</span>
<span class="udiff-line-added">+     _qset(qset), _g1h(g1h), _nodes(qset-&gt;all_completed_buffers()) { }</span>
  
    virtual void work(uint worker_id) {
      G1GCPhaseTimes* p = _g1h-&gt;phase_times();
      G1GCParPhaseTimesTracker x(p, G1GCPhaseTimes::RedirtyCards, worker_id);
  
      RedirtyLoggedCardTableEntryClosure cl(_g1h);
<span class="udiff-line-modified-removed">-     _queue-&gt;par_apply_closure_to_all_completed_buffers(&amp;cl);</span>
<span class="udiff-line-modified-added">+     par_apply(&amp;cl, worker_id);</span>
  
      p-&gt;record_thread_work_item(G1GCPhaseTimes::RedirtyCards, worker_id, cl.num_dirtied());
    }
  };
  
<span class="udiff-line-modified-removed">- void G1CollectedHeap::redirty_logged_cards() {</span>
<span class="udiff-line-modified-added">+ void G1CollectedHeap::redirty_logged_cards(G1RedirtyCardsQueueSet* rdcqs) {</span>
    double redirty_logged_cards_start = os::elapsedTime();
  
<span class="udiff-line-modified-removed">-   G1RedirtyLoggedCardsTask redirty_task(&amp;dirty_card_queue_set(), this);</span>
<span class="udiff-line-removed">-   dirty_card_queue_set().reset_for_par_iteration();</span>
<span class="udiff-line-modified-added">+   G1RedirtyLoggedCardsTask redirty_task(rdcqs, this);</span>
    workers()-&gt;run_task(&amp;redirty_task);
  
    G1DirtyCardQueueSet&amp; dcq = G1BarrierSet::dirty_card_queue_set();
<span class="udiff-line-modified-removed">-   dcq.merge_bufferlists(&amp;dirty_card_queue_set());</span>
<span class="udiff-line-removed">-   assert(dirty_card_queue_set().completed_buffers_num() == 0, &quot;All should be consumed&quot;);</span>
<span class="udiff-line-modified-added">+   dcq.merge_bufferlists(rdcqs);</span>
  
    phase_times()-&gt;record_redirty_logged_cards_time_ms((os::elapsedTime() - redirty_logged_cards_start) * 1000.0);
  }
  
  // Weak Reference Processing support
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3439,21 +3350,21 @@</span>
    void do_oop(narrowOop* p) { guarantee(false, &quot;Not needed&quot;); }
    void do_oop(oop* p) {
      oop obj = *p;
      assert(obj != NULL, &quot;the caller should have filtered out NULL values&quot;);
  
<span class="udiff-line-modified-removed">-     const InCSetState cset_state =_g1h-&gt;in_cset_state(obj);</span>
<span class="udiff-line-modified-removed">-     if (!cset_state.is_in_cset_or_humongous()) {</span>
<span class="udiff-line-modified-added">+     const G1HeapRegionAttr region_attr =_g1h-&gt;region_attr(obj);</span>
<span class="udiff-line-modified-added">+     if (!region_attr.is_in_cset_or_humongous()) {</span>
        return;
      }
<span class="udiff-line-modified-removed">-     if (cset_state.is_in_cset()) {</span>
<span class="udiff-line-modified-added">+     if (region_attr.is_in_cset()) {</span>
        assert( obj-&gt;is_forwarded(), &quot;invariant&quot; );
        *p = obj-&gt;forwardee();
      } else {
        assert(!obj-&gt;is_forwarded(), &quot;invariant&quot; );
<span class="udiff-line-modified-removed">-       assert(cset_state.is_humongous(),</span>
<span class="udiff-line-modified-removed">-              &quot;Only allowed InCSet state is IsHumongous, but is %d&quot;, cset_state.value());</span>
<span class="udiff-line-modified-added">+       assert(region_attr.is_humongous(),</span>
<span class="udiff-line-modified-added">+              &quot;Only allowed G1HeapRegionAttr state is IsHumongous, but is %d&quot;, region_attr.type());</span>
       _g1h-&gt;set_humongous_is_live(obj);
      }
    }
  };
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3555,18 +3466,18 @@</span>
    typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
    ProcessTask&amp;     _proc_task;
    G1CollectedHeap* _g1h;
    G1ParScanThreadStateSet* _pss;
    RefToScanQueueSet* _task_queues;
<span class="udiff-line-modified-removed">-   ParallelTaskTerminator* _terminator;</span>
<span class="udiff-line-modified-added">+   TaskTerminator* _terminator;</span>
  
  public:
    G1STWRefProcTaskProxy(ProcessTask&amp; proc_task,
                          G1CollectedHeap* g1h,
                          G1ParScanThreadStateSet* per_thread_states,
                          RefToScanQueueSet *task_queues,
<span class="udiff-line-modified-removed">-                         ParallelTaskTerminator* terminator) :</span>
<span class="udiff-line-modified-added">+                         TaskTerminator* terminator) :</span>
      AbstractGangTask(&quot;Process reference objects in parallel&quot;),
      _proc_task(proc_task),
      _g1h(g1h),
      _pss(per_thread_states),
      _task_queues(task_queues),
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3607,11 +3518,11 @@</span>
  
    assert(_workers-&gt;active_workers() &gt;= ergo_workers,
           &quot;Ergonomically chosen workers (%u) should be less than or equal to active workers (%u)&quot;,
           ergo_workers, _workers-&gt;active_workers());
    TaskTerminator terminator(ergo_workers, _queues);
<span class="udiff-line-modified-removed">-   G1STWRefProcTaskProxy proc_task_proxy(proc_task, _g1h, _pss, _queues, terminator.terminator());</span>
<span class="udiff-line-modified-added">+   G1STWRefProcTaskProxy proc_task_proxy(proc_task, _g1h, _pss, _queues, &amp;terminator);</span>
  
    _workers-&gt;run_task(&amp;proc_task_proxy, ergo_workers);
  }
  
  // End of weak reference support closures
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3675,10 +3586,11 @@</span>
    // We have completed copying any necessary live referent objects.
    assert(pss-&gt;queue_is_empty(), &quot;both queue and overflow should be empty&quot;);
  
    make_pending_list_reachable();
  
<span class="udiff-line-added">+   assert(!rp-&gt;discovery_enabled(), &quot;Postcondition&quot;);</span>
    rp-&gt;verify_no_references_recorded();
  
    double ref_proc_time = os::elapsedTime() - ref_proc_start;
    phase_times()-&gt;record_ref_proc_time(ref_proc_time * 1000.0);
  }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3692,211 +3604,407 @@</span>
      }
    }
  }
  
  void G1CollectedHeap::merge_per_thread_state_info(G1ParScanThreadStateSet* per_thread_states) {
<span class="udiff-line-modified-removed">-   double merge_pss_time_start = os::elapsedTime();</span>
<span class="udiff-line-modified-added">+   Ticks start = Ticks::now();</span>
    per_thread_states-&gt;flush();
<span class="udiff-line-modified-removed">-   phase_times()-&gt;record_merge_pss_time_ms((os::elapsedTime() - merge_pss_time_start) * 1000.0);</span>
<span class="udiff-line-modified-added">+   phase_times()-&gt;record_or_add_time_secs(G1GCPhaseTimes::MergePSS, 0 /* worker_id */, (Ticks::now() - start).seconds());</span>
  }
  
<span class="udiff-line-modified-removed">- void G1CollectedHeap::pre_evacuate_collection_set() {</span>
<span class="udiff-line-modified-added">+ class G1PrepareEvacuationTask : public AbstractGangTask {</span>
<span class="udiff-line-added">+   class G1PrepareRegionsClosure : public HeapRegionClosure {</span>
<span class="udiff-line-added">+     G1CollectedHeap* _g1h;</span>
<span class="udiff-line-added">+     G1PrepareEvacuationTask* _parent_task;</span>
<span class="udiff-line-added">+     size_t _worker_humongous_total;</span>
<span class="udiff-line-added">+     size_t _worker_humongous_candidates;</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     bool humongous_region_is_candidate(HeapRegion* region) const {</span>
<span class="udiff-line-added">+       assert(region-&gt;is_starts_humongous(), &quot;Must start a humongous object&quot;);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+       oop obj = oop(region-&gt;bottom());</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+       // Dead objects cannot be eager reclaim candidates. Due to class</span>
<span class="udiff-line-added">+       // unloading it is unsafe to query their classes so we return early.</span>
<span class="udiff-line-added">+       if (_g1h-&gt;is_obj_dead(obj, region)) {</span>
<span class="udiff-line-added">+         return false;</span>
<span class="udiff-line-added">+       }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+       // If we do not have a complete remembered set for the region, then we can</span>
<span class="udiff-line-added">+       // not be sure that we have all references to it.</span>
<span class="udiff-line-added">+       if (!region-&gt;rem_set()-&gt;is_complete()) {</span>
<span class="udiff-line-added">+         return false;</span>
<span class="udiff-line-added">+       }</span>
<span class="udiff-line-added">+       // Candidate selection must satisfy the following constraints</span>
<span class="udiff-line-added">+       // while concurrent marking is in progress:</span>
<span class="udiff-line-added">+       //</span>
<span class="udiff-line-added">+       // * In order to maintain SATB invariants, an object must not be</span>
<span class="udiff-line-added">+       // reclaimed if it was allocated before the start of marking and</span>
<span class="udiff-line-added">+       // has not had its references scanned.  Such an object must have</span>
<span class="udiff-line-added">+       // its references (including type metadata) scanned to ensure no</span>
<span class="udiff-line-added">+       // live objects are missed by the marking process.  Objects</span>
<span class="udiff-line-added">+       // allocated after the start of concurrent marking don&#39;t need to</span>
<span class="udiff-line-added">+       // be scanned.</span>
<span class="udiff-line-added">+       //</span>
<span class="udiff-line-added">+       // * An object must not be reclaimed if it is on the concurrent</span>
<span class="udiff-line-added">+       // mark stack.  Objects allocated after the start of concurrent</span>
<span class="udiff-line-added">+       // marking are never pushed on the mark stack.</span>
<span class="udiff-line-added">+       //</span>
<span class="udiff-line-added">+       // Nominating only objects allocated after the start of concurrent</span>
<span class="udiff-line-added">+       // marking is sufficient to meet both constraints.  This may miss</span>
<span class="udiff-line-added">+       // some objects that satisfy the constraints, but the marking data</span>
<span class="udiff-line-added">+       // structures don&#39;t support efficiently performing the needed</span>
<span class="udiff-line-added">+       // additional tests or scrubbing of the mark stack.</span>
<span class="udiff-line-added">+       //</span>
<span class="udiff-line-added">+       // However, we presently only nominate is_typeArray() objects.</span>
<span class="udiff-line-added">+       // A humongous object containing references induces remembered</span>
<span class="udiff-line-added">+       // set entries on other regions.  In order to reclaim such an</span>
<span class="udiff-line-added">+       // object, those remembered sets would need to be cleaned up.</span>
<span class="udiff-line-added">+       //</span>
<span class="udiff-line-added">+       // We also treat is_typeArray() objects specially, allowing them</span>
<span class="udiff-line-added">+       // to be reclaimed even if allocated before the start of</span>
<span class="udiff-line-added">+       // concurrent mark.  For this we rely on mark stack insertion to</span>
<span class="udiff-line-added">+       // exclude is_typeArray() objects, preventing reclaiming an object</span>
<span class="udiff-line-added">+       // that is in the mark stack.  We also rely on the metadata for</span>
<span class="udiff-line-added">+       // such objects to be built-in and so ensured to be kept live.</span>
<span class="udiff-line-added">+       // Frequent allocation and drop of large binary blobs is an</span>
<span class="udiff-line-added">+       // important use case for eager reclaim, and this special handling</span>
<span class="udiff-line-added">+       // may reduce needed headroom.</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+       return obj-&gt;is_typeArray() &amp;&amp;</span>
<span class="udiff-line-added">+              _g1h-&gt;is_potential_eager_reclaim_candidate(region);</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   public:</span>
<span class="udiff-line-added">+     G1PrepareRegionsClosure(G1CollectedHeap* g1h, G1PrepareEvacuationTask* parent_task) :</span>
<span class="udiff-line-added">+       _g1h(g1h),</span>
<span class="udiff-line-added">+       _parent_task(parent_task),</span>
<span class="udiff-line-added">+       _worker_humongous_total(0),</span>
<span class="udiff-line-added">+       _worker_humongous_candidates(0) { }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     ~G1PrepareRegionsClosure() {</span>
<span class="udiff-line-added">+       _parent_task-&gt;add_humongous_candidates(_worker_humongous_candidates);</span>
<span class="udiff-line-added">+       _parent_task-&gt;add_humongous_total(_worker_humongous_total);</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     virtual bool do_heap_region(HeapRegion* hr) {</span>
<span class="udiff-line-added">+       // First prepare the region for scanning</span>
<span class="udiff-line-added">+       _g1h-&gt;rem_set()-&gt;prepare_region_for_scan(hr);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+       // Now check if region is a humongous candidate</span>
<span class="udiff-line-added">+       if (!hr-&gt;is_starts_humongous()) {</span>
<span class="udiff-line-added">+         _g1h-&gt;register_region_with_region_attr(hr);</span>
<span class="udiff-line-added">+         return false;</span>
<span class="udiff-line-added">+       }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+       uint index = hr-&gt;hrm_index();</span>
<span class="udiff-line-added">+       if (humongous_region_is_candidate(hr)) {</span>
<span class="udiff-line-added">+         _g1h-&gt;set_humongous_reclaim_candidate(index, true);</span>
<span class="udiff-line-added">+         _g1h-&gt;register_humongous_region_with_region_attr(index);</span>
<span class="udiff-line-added">+         _worker_humongous_candidates++;</span>
<span class="udiff-line-added">+         // We will later handle the remembered sets of these regions.</span>
<span class="udiff-line-added">+       } else {</span>
<span class="udiff-line-added">+         _g1h-&gt;set_humongous_reclaim_candidate(index, false);</span>
<span class="udiff-line-added">+         _g1h-&gt;register_region_with_region_attr(hr);</span>
<span class="udiff-line-added">+       }</span>
<span class="udiff-line-added">+       _worker_humongous_total++;</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+       return false;</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+   };</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   G1CollectedHeap* _g1h;</span>
<span class="udiff-line-added">+   HeapRegionClaimer _claimer;</span>
<span class="udiff-line-added">+   volatile size_t _humongous_total;</span>
<span class="udiff-line-added">+   volatile size_t _humongous_candidates;</span>
<span class="udiff-line-added">+ public:</span>
<span class="udiff-line-added">+   G1PrepareEvacuationTask(G1CollectedHeap* g1h) :</span>
<span class="udiff-line-added">+     AbstractGangTask(&quot;Prepare Evacuation&quot;),</span>
<span class="udiff-line-added">+     _g1h(g1h),</span>
<span class="udiff-line-added">+     _claimer(_g1h-&gt;workers()-&gt;active_workers()),</span>
<span class="udiff-line-added">+     _humongous_total(0),</span>
<span class="udiff-line-added">+     _humongous_candidates(0) { }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   ~G1PrepareEvacuationTask() {</span>
<span class="udiff-line-added">+     _g1h-&gt;set_has_humongous_reclaim_candidate(_humongous_candidates &gt; 0);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   void work(uint worker_id) {</span>
<span class="udiff-line-added">+     G1PrepareRegionsClosure cl(_g1h, this);</span>
<span class="udiff-line-added">+     _g1h-&gt;heap_region_par_iterate_from_worker_offset(&amp;cl, &amp;_claimer, worker_id);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   void add_humongous_candidates(size_t candidates) {</span>
<span class="udiff-line-added">+     Atomic::add(&amp;_humongous_candidates, candidates);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   void add_humongous_total(size_t total) {</span>
<span class="udiff-line-added">+     Atomic::add(&amp;_humongous_total, total);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   size_t humongous_candidates() {</span>
<span class="udiff-line-added">+     return _humongous_candidates;</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   size_t humongous_total() {</span>
<span class="udiff-line-added">+     return _humongous_total;</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ };</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void G1CollectedHeap::pre_evacuate_collection_set(G1EvacuationInfo&amp; evacuation_info, G1ParScanThreadStateSet* per_thread_states) {</span>
<span class="udiff-line-added">+   _bytes_used_during_gc = 0;</span>
<span class="udiff-line-added">+ </span>
    _expand_heap_after_alloc_failure = true;
    _evacuation_failed = false;
  
    // Disable the hot card cache.
    _hot_card_cache-&gt;reset_hot_cache_claimed_index();
    _hot_card_cache-&gt;set_use_cache(false);
  
<span class="udiff-line-modified-removed">-   rem_set()-&gt;prepare_for_oops_into_collection_set_do();</span>
<span class="udiff-line-modified-added">+   // Initialize the GC alloc regions.</span>
<span class="udiff-line-added">+   _allocator-&gt;init_gc_alloc_regions(evacuation_info);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   {</span>
<span class="udiff-line-added">+     Ticks start = Ticks::now();</span>
<span class="udiff-line-added">+     rem_set()-&gt;prepare_for_scan_heap_roots();</span>
<span class="udiff-line-added">+     phase_times()-&gt;record_prepare_heap_roots_time_ms((Ticks::now() - start).seconds() * 1000.0);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   {</span>
<span class="udiff-line-added">+     G1PrepareEvacuationTask g1_prep_task(this);</span>
<span class="udiff-line-added">+     Tickspan task_time = run_task(&amp;g1_prep_task);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     phase_times()-&gt;record_register_regions(task_time.seconds() * 1000.0,</span>
<span class="udiff-line-added">+                                            g1_prep_task.humongous_total(),</span>
<span class="udiff-line-added">+                                            g1_prep_task.humongous_candidates());</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   assert(_verifier-&gt;check_region_attr_table(), &quot;Inconsistency in the region attributes table.&quot;);</span>
    _preserved_marks_set.assert_empty();
  
<span class="udiff-line-added">+ #if COMPILER2_OR_JVMCI</span>
<span class="udiff-line-added">+   DerivedPointerTable::clear();</span>
<span class="udiff-line-added">+ #endif</span>
<span class="udiff-line-added">+ </span>
    // InitialMark needs claim bits to keep track of the marked-through CLDs.
    if (collector_state()-&gt;in_initial_mark_gc()) {
<span class="udiff-line-added">+     concurrent_mark()-&gt;pre_initial_mark();</span>
<span class="udiff-line-added">+ </span>
      double start_clear_claimed_marks = os::elapsedTime();
  
      ClassLoaderDataGraph::clear_claimed_marks();
  
      double recorded_clear_claimed_marks_time_ms = (os::elapsedTime() - start_clear_claimed_marks) * 1000.0;
      phase_times()-&gt;record_clear_claimed_marks_time_ms(recorded_clear_claimed_marks_time_ms);
    }
<span class="udiff-line-removed">- }</span>
  
<span class="udiff-line-removed">- void G1CollectedHeap::evacuate_collection_set(G1ParScanThreadStateSet* per_thread_states) {</span>
    // Should G1EvacuationFailureALot be in effect for this GC?
    NOT_PRODUCT(set_evacuation_failure_alot_for_current_gc();)
<span class="udiff-line-added">+ }</span>
  
<span class="udiff-line-modified-removed">-   assert(dirty_card_queue_set().completed_buffers_num() == 0, &quot;Should be empty&quot;);</span>
<span class="udiff-line-modified-added">+ class G1EvacuateRegionsBaseTask : public AbstractGangTask {</span>
<span class="udiff-line-added">+ protected:</span>
<span class="udiff-line-added">+   G1CollectedHeap* _g1h;</span>
<span class="udiff-line-added">+   G1ParScanThreadStateSet* _per_thread_states;</span>
<span class="udiff-line-added">+   RefToScanQueueSet* _task_queues;</span>
<span class="udiff-line-added">+   TaskTerminator _terminator;</span>
<span class="udiff-line-added">+   uint _num_workers;</span>
  
<span class="udiff-line-modified-removed">-   double start_par_time_sec = os::elapsedTime();</span>
<span class="udiff-line-modified-removed">-   double end_par_time_sec;</span>
<span class="udiff-line-modified-added">+   void evacuate_live_objects(G1ParScanThreadState* pss,</span>
<span class="udiff-line-modified-added">+                              uint worker_id,</span>
<span class="udiff-line-added">+                              G1GCPhaseTimes::GCParPhases objcopy_phase,</span>
<span class="udiff-line-added">+                              G1GCPhaseTimes::GCParPhases termination_phase) {</span>
<span class="udiff-line-added">+     G1GCPhaseTimes* p = _g1h-&gt;phase_times();</span>
  
<span class="udiff-line-modified-removed">-   {</span>
<span class="udiff-line-modified-removed">-     const uint n_workers = workers()-&gt;active_workers();</span>
<span class="udiff-line-modified-removed">-     G1RootProcessor root_processor(this, n_workers);</span>
<span class="udiff-line-removed">-     G1ParTask g1_par_task(this, per_thread_states, _task_queues, &amp;root_processor, n_workers);</span>
<span class="udiff-line-modified-added">+     Ticks start = Ticks::now();</span>
<span class="udiff-line-modified-added">+     G1ParEvacuateFollowersClosure cl(_g1h, pss, _task_queues, &amp;_terminator, objcopy_phase);</span>
<span class="udiff-line-modified-added">+     cl.do_void();</span>
  
<span class="udiff-line-modified-removed">-     workers()-&gt;run_task(&amp;g1_par_task);</span>
<span class="udiff-line-modified-removed">-     end_par_time_sec = os::elapsedTime();</span>
<span class="udiff-line-modified-added">+     assert(pss-&gt;queue_is_empty(), &quot;should be empty&quot;);</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-added">+     Tickspan evac_time = (Ticks::now() - start);</span>
<span class="udiff-line-added">+     p-&gt;record_or_add_time_secs(objcopy_phase, worker_id, evac_time.seconds() - cl.term_time());</span>
  
<span class="udiff-line-modified-removed">-     // Closing the inner scope will execute the destructor</span>
<span class="udiff-line-modified-removed">-     // for the G1RootProcessor object. We record the current</span>
<span class="udiff-line-modified-removed">-     // elapsed time before closing the scope so that time</span>
<span class="udiff-line-modified-removed">-     // taken for the destructor is NOT included in the</span>
<span class="udiff-line-modified-removed">-     // reported parallel time.</span>
<span class="udiff-line-modified-added">+     if (termination_phase == G1GCPhaseTimes::Termination) {</span>
<span class="udiff-line-modified-added">+       p-&gt;record_time_secs(termination_phase, worker_id, cl.term_time());</span>
<span class="udiff-line-modified-added">+       p-&gt;record_thread_work_item(termination_phase, worker_id, cl.term_attempts());</span>
<span class="udiff-line-modified-added">+     } else {</span>
<span class="udiff-line-modified-added">+       p-&gt;record_or_add_time_secs(termination_phase, worker_id, cl.term_time());</span>
<span class="udiff-line-added">+       p-&gt;record_or_add_thread_work_item(termination_phase, worker_id, cl.term_attempts());</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+     assert(pss-&gt;trim_ticks().seconds() == 0.0, &quot;Unexpected partial trimming during evacuation&quot;);</span>
    }
  
<span class="udiff-line-modified-removed">-   double par_time_ms = (end_par_time_sec - start_par_time_sec) * 1000.0;</span>
<span class="udiff-line-removed">-   phase_times()-&gt;record_par_time(par_time_ms);</span>
<span class="udiff-line-modified-added">+   virtual void start_work(uint worker_id) { }</span>
  
<span class="udiff-line-modified-removed">-   double code_root_fixup_time_ms =</span>
<span class="udiff-line-removed">-         (os::elapsedTime() - end_par_time_sec) * 1000.0;</span>
<span class="udiff-line-removed">-   phase_times()-&gt;record_code_root_fixup_time(code_root_fixup_time_ms);</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-modified-added">+   virtual void end_work(uint worker_id) { }</span>
  
<span class="udiff-line-modified-removed">- class G1EvacuateOptionalRegionTask : public AbstractGangTask {</span>
<span class="udiff-line-removed">-   G1CollectedHeap* _g1h;</span>
<span class="udiff-line-removed">-   G1ParScanThreadStateSet* _per_thread_states;</span>
<span class="udiff-line-removed">-   G1OptionalCSet* _optional;</span>
<span class="udiff-line-removed">-   RefToScanQueueSet* _queues;</span>
<span class="udiff-line-removed">-   ParallelTaskTerminator _terminator;</span>
<span class="udiff-line-modified-added">+   virtual void scan_roots(G1ParScanThreadState* pss, uint worker_id) = 0;</span>
  
<span class="udiff-line-modified-removed">-   Tickspan trim_ticks(G1ParScanThreadState* pss) {</span>
<span class="udiff-line-removed">-     Tickspan copy_time = pss-&gt;trim_ticks();</span>
<span class="udiff-line-removed">-     pss-&gt;reset_trim_ticks();</span>
<span class="udiff-line-removed">-     return copy_time;</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-modified-added">+   virtual void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) = 0;</span>
  
<span class="udiff-line-modified-removed">-   void scan_roots(G1ParScanThreadState* pss, uint worker_id) {</span>
<span class="udiff-line-modified-removed">-     G1EvacuationRootClosures* root_cls = pss-&gt;closures();</span>
<span class="udiff-line-modified-removed">-     G1ScanObjsDuringScanRSClosure obj_cl(_g1h, pss);</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-     size_t scanned = 0;</span>
<span class="udiff-line-modified-removed">-     size_t claimed = 0;</span>
<span class="udiff-line-modified-removed">-     size_t skipped = 0;</span>
<span class="udiff-line-modified-removed">-     size_t used_memory = 0;</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-     Ticks    start = Ticks::now();</span>
<span class="udiff-line-modified-removed">-     Tickspan copy_time;</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-     for (uint i = _optional-&gt;current_index(); i &lt; _optional-&gt;current_limit(); i++) {</span>
<span class="udiff-line-modified-removed">-       HeapRegion* hr = _optional-&gt;region_at(i);</span>
<span class="udiff-line-modified-removed">-       G1ScanRSForOptionalClosure scan_opt_cl(&amp;obj_cl);</span>
<span class="udiff-line-modified-removed">-       pss-&gt;oops_into_optional_region(hr)-&gt;oops_do(&amp;scan_opt_cl, root_cls-&gt;raw_strong_oops());</span>
<span class="udiff-line-modified-removed">-       copy_time += trim_ticks(pss);</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-       G1ScanRSForRegionClosure scan_rs_cl(_g1h-&gt;rem_set()-&gt;scan_state(), &amp;obj_cl, pss, G1GCPhaseTimes::OptScanRS, worker_id);</span>
<span class="udiff-line-modified-removed">-       scan_rs_cl.do_heap_region(hr);</span>
<span class="udiff-line-modified-removed">-       copy_time += trim_ticks(pss);</span>
<span class="udiff-line-modified-removed">-       scanned += scan_rs_cl.cards_scanned();</span>
<span class="udiff-line-removed">-       claimed += scan_rs_cl.cards_claimed();</span>
<span class="udiff-line-removed">-       skipped += scan_rs_cl.cards_skipped();</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       // Chunk lists for this region is no longer needed.</span>
<span class="udiff-line-removed">-       used_memory += pss-&gt;oops_into_optional_region(hr)-&gt;used_memory();</span>
<span class="udiff-line-modified-added">+ public:</span>
<span class="udiff-line-modified-added">+   G1EvacuateRegionsBaseTask(const char* name, G1ParScanThreadStateSet* per_thread_states, RefToScanQueueSet* task_queues, uint num_workers) :</span>
<span class="udiff-line-modified-added">+     AbstractGangTask(name),</span>
<span class="udiff-line-modified-added">+     _g1h(G1CollectedHeap::heap()),</span>
<span class="udiff-line-modified-added">+     _per_thread_states(per_thread_states),</span>
<span class="udiff-line-modified-added">+     _task_queues(task_queues),</span>
<span class="udiff-line-modified-added">+     _terminator(num_workers, _task_queues),</span>
<span class="udiff-line-modified-added">+     _num_workers(num_workers)</span>
<span class="udiff-line-modified-added">+   { }</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+   void work(uint worker_id) {</span>
<span class="udiff-line-modified-added">+     start_work(worker_id);</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+     {</span>
<span class="udiff-line-modified-added">+       ResourceMark rm;</span>
<span class="udiff-line-modified-added">+       HandleMark   hm;</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+       G1ParScanThreadState* pss = _per_thread_states-&gt;state_for_worker(worker_id);</span>
<span class="udiff-line-modified-added">+       pss-&gt;set_ref_discoverer(_g1h-&gt;ref_processor_stw());</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+       scan_roots(pss, worker_id);</span>
<span class="udiff-line-modified-added">+       evacuate_live_objects(pss, worker_id);</span>
      }
  
<span class="udiff-line-modified-removed">-     Tickspan scan_time = (Ticks::now() - start) - copy_time;</span>
<span class="udiff-line-modified-removed">-     G1GCPhaseTimes* p = _g1h-&gt;phase_times();</span>
<span class="udiff-line-modified-removed">-     p-&gt;record_or_add_time_secs(G1GCPhaseTimes::OptScanRS, worker_id, scan_time.seconds());</span>
<span class="udiff-line-removed">-     p-&gt;record_or_add_time_secs(G1GCPhaseTimes::OptObjCopy, worker_id, copy_time.seconds());</span>
<span class="udiff-line-modified-added">+     end_work(worker_id);</span>
<span class="udiff-line-modified-added">+   }</span>
<span class="udiff-line-modified-added">+ };</span>
  
<span class="udiff-line-modified-removed">-     p-&gt;record_or_add_thread_work_item(G1GCPhaseTimes::OptScanRS, worker_id, scanned, G1GCPhaseTimes::OptCSetScannedCards);</span>
<span class="udiff-line-modified-removed">-     p-&gt;record_or_add_thread_work_item(G1GCPhaseTimes::OptScanRS, worker_id, claimed, G1GCPhaseTimes::OptCSetClaimedCards);</span>
<span class="udiff-line-modified-removed">-     p-&gt;record_or_add_thread_work_item(G1GCPhaseTimes::OptScanRS, worker_id, skipped, G1GCPhaseTimes::OptCSetSkippedCards);</span>
<span class="udiff-line-modified-removed">-     p-&gt;record_or_add_thread_work_item(G1GCPhaseTimes::OptScanRS, worker_id, used_memory, G1GCPhaseTimes::OptCSetUsedMemory);</span>
<span class="udiff-line-modified-added">+ class G1EvacuateRegionsTask : public G1EvacuateRegionsBaseTask {</span>
<span class="udiff-line-modified-added">+   G1RootProcessor* _root_processor;</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+   void scan_roots(G1ParScanThreadState* pss, uint worker_id) {</span>
<span class="udiff-line-added">+     _root_processor-&gt;evacuate_roots(pss, worker_id);</span>
<span class="udiff-line-added">+     _g1h-&gt;rem_set()-&gt;scan_heap_roots(pss, worker_id, G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::ObjCopy);</span>
<span class="udiff-line-added">+     _g1h-&gt;rem_set()-&gt;scan_collection_set_regions(pss, worker_id, G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::CodeRoots, G1GCPhaseTimes::ObjCopy);</span>
    }
  
    void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) {
<span class="udiff-line-modified-removed">-     Ticks start = Ticks::now();</span>
<span class="udiff-line-modified-removed">-     G1ParEvacuateFollowersClosure cl(_g1h, pss, _queues, &amp;_terminator, G1GCPhaseTimes::OptObjCopy);</span>
<span class="udiff-line-removed">-     cl.do_void();</span>
<span class="udiff-line-modified-added">+     G1EvacuateRegionsBaseTask::evacuate_live_objects(pss, worker_id, G1GCPhaseTimes::ObjCopy, G1GCPhaseTimes::Termination);</span>
<span class="udiff-line-modified-added">+   }</span>
  
<span class="udiff-line-modified-removed">-     Tickspan evac_time = (Ticks::now() - start);</span>
<span class="udiff-line-modified-removed">-     G1GCPhaseTimes* p = _g1h-&gt;phase_times();</span>
<span class="udiff-line-removed">-     p-&gt;record_or_add_time_secs(G1GCPhaseTimes::OptObjCopy, worker_id, evac_time.seconds());</span>
<span class="udiff-line-removed">-     assert(pss-&gt;trim_ticks().seconds() == 0.0, &quot;Unexpected partial trimming done during optional evacuation&quot;);</span>
<span class="udiff-line-modified-added">+   void start_work(uint worker_id) {</span>
<span class="udiff-line-modified-added">+     _g1h-&gt;phase_times()-&gt;record_time_secs(G1GCPhaseTimes::GCWorkerStart, worker_id, Ticks::now().seconds());</span>
    }
  
<span class="udiff-line-modified-removed">-  public:</span>
<span class="udiff-line-modified-removed">-   G1EvacuateOptionalRegionTask(G1CollectedHeap* g1h,</span>
<span class="udiff-line-removed">-                                G1ParScanThreadStateSet* per_thread_states,</span>
<span class="udiff-line-removed">-                                G1OptionalCSet* cset,</span>
<span class="udiff-line-removed">-                                RefToScanQueueSet* queues,</span>
<span class="udiff-line-removed">-                                uint n_workers) :</span>
<span class="udiff-line-removed">-     AbstractGangTask(&quot;G1 Evacuation Optional Region Task&quot;),</span>
<span class="udiff-line-removed">-     _g1h(g1h),</span>
<span class="udiff-line-removed">-     _per_thread_states(per_thread_states),</span>
<span class="udiff-line-removed">-     _optional(cset),</span>
<span class="udiff-line-removed">-     _queues(queues),</span>
<span class="udiff-line-removed">-     _terminator(n_workers, _queues) {</span>
<span class="udiff-line-modified-added">+   void end_work(uint worker_id) {</span>
<span class="udiff-line-modified-added">+     _g1h-&gt;phase_times()-&gt;record_time_secs(G1GCPhaseTimes::GCWorkerEnd, worker_id, Ticks::now().seconds());</span>
    }
  
<span class="udiff-line-modified-removed">-   void work(uint worker_id) {</span>
<span class="udiff-line-modified-removed">-     ResourceMark rm;</span>
<span class="udiff-line-modified-removed">-     HandleMark  hm;</span>
<span class="udiff-line-modified-added">+ public:</span>
<span class="udiff-line-modified-added">+   G1EvacuateRegionsTask(G1CollectedHeap* g1h,</span>
<span class="udiff-line-modified-added">+                         G1ParScanThreadStateSet* per_thread_states,</span>
<span class="udiff-line-added">+                         RefToScanQueueSet* task_queues,</span>
<span class="udiff-line-added">+                         G1RootProcessor* root_processor,</span>
<span class="udiff-line-added">+                         uint num_workers) :</span>
<span class="udiff-line-added">+     G1EvacuateRegionsBaseTask(&quot;G1 Evacuate Regions&quot;, per_thread_states, task_queues, num_workers),</span>
<span class="udiff-line-added">+     _root_processor(root_processor)</span>
<span class="udiff-line-added">+   { }</span>
<span class="udiff-line-added">+ };</span>
  
<span class="udiff-line-modified-removed">-     G1ParScanThreadState* pss = _per_thread_states-&gt;state_for_worker(worker_id);</span>
<span class="udiff-line-modified-removed">-     pss-&gt;set_ref_discoverer(_g1h-&gt;ref_processor_stw());</span>
<span class="udiff-line-modified-added">+ void G1CollectedHeap::evacuate_initial_collection_set(G1ParScanThreadStateSet* per_thread_states) {</span>
<span class="udiff-line-modified-added">+   G1GCPhaseTimes* p = phase_times();</span>
  
<span class="udiff-line-modified-removed">-     scan_roots(pss, worker_id);</span>
<span class="udiff-line-modified-removed">-     evacuate_live_objects(pss, worker_id);</span>
<span class="udiff-line-modified-added">+   {</span>
<span class="udiff-line-modified-added">+     Ticks start = Ticks::now();</span>
<span class="udiff-line-added">+     rem_set()-&gt;merge_heap_roots(true /* initial_evacuation */);</span>
<span class="udiff-line-added">+     p-&gt;record_merge_heap_roots_time((Ticks::now() - start).seconds() * 1000.0);</span>
    }
<span class="udiff-line-removed">- };</span>
  
<span class="udiff-line-modified-removed">- void G1CollectedHeap::evacuate_optional_regions(G1ParScanThreadStateSet* per_thread_states, G1OptionalCSet* ocset) {</span>
<span class="udiff-line-modified-removed">-   class G1MarkScope : public MarkScope {};</span>
<span class="udiff-line-modified-removed">-   G1MarkScope code_mark_scope;</span>
<span class="udiff-line-modified-added">+   Tickspan task_time;</span>
<span class="udiff-line-modified-added">+   const uint num_workers = workers()-&gt;active_workers();</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-added">+   Ticks start_processing = Ticks::now();</span>
<span class="udiff-line-added">+   {</span>
<span class="udiff-line-added">+     G1RootProcessor root_processor(this, num_workers);</span>
<span class="udiff-line-added">+     G1EvacuateRegionsTask g1_par_task(this, per_thread_states, _task_queues, &amp;root_processor, num_workers);</span>
<span class="udiff-line-added">+     task_time = run_task(&amp;g1_par_task);</span>
<span class="udiff-line-added">+     // Closing the inner scope will execute the destructor for the G1RootProcessor object.</span>
<span class="udiff-line-added">+     // To extract its code root fixup time we measure total time of this scope and</span>
<span class="udiff-line-added">+     // subtract from the time the WorkGang task took.</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+   Tickspan total_processing = Ticks::now() - start_processing;</span>
  
<span class="udiff-line-modified-removed">-   G1EvacuateOptionalRegionTask task(this, per_thread_states, ocset, _task_queues, workers()-&gt;active_workers());</span>
<span class="udiff-line-modified-removed">-   workers()-&gt;run_task(&amp;task);</span>
<span class="udiff-line-modified-added">+   p-&gt;record_initial_evac_time(task_time.seconds() * 1000.0);</span>
<span class="udiff-line-modified-added">+   p-&gt;record_or_add_code_root_fixup_time((total_processing - task_time).seconds() * 1000.0);</span>
  }
  
<span class="udiff-line-modified-removed">- void G1CollectedHeap::evacuate_optional_collection_set(G1ParScanThreadStateSet* per_thread_states) {</span>
<span class="udiff-line-modified-removed">-   G1OptionalCSet optional_cset(&amp;_collection_set, per_thread_states);</span>
<span class="udiff-line-modified-removed">-   if (optional_cset.is_empty()) {</span>
<span class="udiff-line-modified-removed">-     return;</span>
<span class="udiff-line-modified-added">+ class G1EvacuateOptionalRegionsTask : public G1EvacuateRegionsBaseTask {</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+   void scan_roots(G1ParScanThreadState* pss, uint worker_id) {</span>
<span class="udiff-line-modified-added">+     _g1h-&gt;rem_set()-&gt;scan_heap_roots(pss, worker_id, G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::OptObjCopy);</span>
<span class="udiff-line-added">+     _g1h-&gt;rem_set()-&gt;scan_collection_set_regions(pss, worker_id, G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::OptCodeRoots, G1GCPhaseTimes::OptObjCopy);</span>
    }
  
<span class="udiff-line-modified-removed">-   if (evacuation_failed()) {</span>
<span class="udiff-line-modified-removed">-     return;</span>
<span class="udiff-line-modified-added">+   void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) {</span>
<span class="udiff-line-modified-added">+     G1EvacuateRegionsBaseTask::evacuate_live_objects(pss, worker_id, G1GCPhaseTimes::OptObjCopy, G1GCPhaseTimes::OptTermination);</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ public:</span>
<span class="udiff-line-added">+   G1EvacuateOptionalRegionsTask(G1ParScanThreadStateSet* per_thread_states,</span>
<span class="udiff-line-added">+                                 RefToScanQueueSet* queues,</span>
<span class="udiff-line-added">+                                 uint num_workers) :</span>
<span class="udiff-line-added">+     G1EvacuateRegionsBaseTask(&quot;G1 Evacuate Optional Regions&quot;, per_thread_states, queues, num_workers) {</span>
<span class="udiff-line-added">+   }</span>
<span class="udiff-line-added">+ };</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void G1CollectedHeap::evacuate_next_optional_regions(G1ParScanThreadStateSet* per_thread_states) {</span>
<span class="udiff-line-added">+   class G1MarkScope : public MarkScope { };</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   Tickspan task_time;</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   Ticks start_processing = Ticks::now();</span>
<span class="udiff-line-added">+   {</span>
<span class="udiff-line-added">+     G1MarkScope code_mark_scope;</span>
<span class="udiff-line-added">+     G1EvacuateOptionalRegionsTask task(per_thread_states, _task_queues, workers()-&gt;active_workers());</span>
<span class="udiff-line-added">+     task_time = run_task(&amp;task);</span>
<span class="udiff-line-added">+     // See comment in evacuate_collection_set() for the reason of the scope.</span>
    }
<span class="udiff-line-added">+   Tickspan total_processing = Ticks::now() - start_processing;</span>
  
<span class="udiff-line-added">+   G1GCPhaseTimes* p = phase_times();</span>
<span class="udiff-line-added">+   p-&gt;record_or_add_code_root_fixup_time((total_processing - task_time).seconds() * 1000.0);</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ void G1CollectedHeap::evacuate_optional_collection_set(G1ParScanThreadStateSet* per_thread_states) {</span>
    const double gc_start_time_ms = phase_times()-&gt;cur_collection_start_sec() * 1000.0;
  
<span class="udiff-line-modified-removed">-   double start_time_sec = os::elapsedTime();</span>
<span class="udiff-line-modified-added">+   while (!evacuation_failed() &amp;&amp; _collection_set.optional_region_length() &gt; 0) {</span>
  
<span class="udiff-line-removed">-   do {</span>
      double time_used_ms = os::elapsedTime() * 1000.0 - gc_start_time_ms;
      double time_left_ms = MaxGCPauseMillis - time_used_ms;
  
<span class="udiff-line-modified-removed">-     if (time_left_ms &lt; 0) {</span>
<span class="udiff-line-modified-removed">-       log_trace(gc, ergo, cset)(&quot;Skipping %u optional regions, pause time exceeded %.3fms&quot;, optional_cset.size(), time_used_ms);</span>
<span class="udiff-line-modified-added">+     if (time_left_ms &lt; 0 ||</span>
<span class="udiff-line-modified-added">+         !_collection_set.finalize_optional_for_evacuation(time_left_ms * policy()-&gt;optional_evacuation_fraction())) {</span>
<span class="udiff-line-added">+       log_trace(gc, ergo, cset)(&quot;Skipping evacuation of %u optional regions, no more regions can be evacuated in %.3fms&quot;,</span>
<span class="udiff-line-added">+                                 _collection_set.optional_region_length(), time_left_ms);</span>
        break;
      }
  
<span class="udiff-line-modified-removed">-     optional_cset.prepare_evacuation(time_left_ms * _policy-&gt;optional_evacuation_fraction());</span>
<span class="udiff-line-modified-removed">-     if (optional_cset.prepare_failed()) {</span>
<span class="udiff-line-modified-removed">-       log_trace(gc, ergo, cset)(&quot;Skipping %u optional regions, no regions can be evacuated in %.3fms&quot;, optional_cset.size(), time_left_ms);</span>
<span class="udiff-line-modified-removed">-       break;</span>
<span class="udiff-line-modified-added">+     {</span>
<span class="udiff-line-modified-added">+       Ticks start = Ticks::now();</span>
<span class="udiff-line-modified-added">+       rem_set()-&gt;merge_heap_roots(false /* initial_evacuation */);</span>
<span class="udiff-line-modified-added">+       phase_times()-&gt;record_or_add_optional_merge_heap_roots_time((Ticks::now() - start).seconds() * 1000.0);</span>
      }
  
<span class="udiff-line-modified-removed">-     evacuate_optional_regions(per_thread_states, &amp;optional_cset);</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-     optional_cset.complete_evacuation();</span>
<span class="udiff-line-modified-removed">-     if (optional_cset.evacuation_failed()) {</span>
<span class="udiff-line-removed">-       break;</span>
<span class="udiff-line-modified-added">+     {</span>
<span class="udiff-line-modified-added">+       Ticks start = Ticks::now();</span>
<span class="udiff-line-modified-added">+       evacuate_next_optional_regions(per_thread_states);</span>
<span class="udiff-line-modified-added">+       phase_times()-&gt;record_or_add_optional_evac_time((Ticks::now() - start).seconds() * 1000.0);</span>
      }
<span class="udiff-line-modified-removed">-   } while (!optional_cset.is_empty());</span>
<span class="udiff-line-modified-added">+   }</span>
  
<span class="udiff-line-modified-removed">-   phase_times()-&gt;record_optional_evacuation((os::elapsedTime() - start_time_sec) * 1000.0);</span>
<span class="udiff-line-modified-added">+   _collection_set.abandon_optional_collection_set(per_thread_states);</span>
  }
  
<span class="udiff-line-modified-removed">- void G1CollectedHeap::post_evacuate_collection_set(G1EvacuationInfo&amp; evacuation_info, G1ParScanThreadStateSet* per_thread_states) {</span>
<span class="udiff-line-modified-removed">-   // Also cleans the card table from temporary duplicate detection information used</span>
<span class="udiff-line-modified-removed">-   // during UpdateRS/ScanRS.</span>
<span class="udiff-line-modified-removed">-   rem_set()-&gt;cleanup_after_oops_into_collection_set_do();</span>
<span class="udiff-line-modified-added">+ void G1CollectedHeap::post_evacuate_collection_set(G1EvacuationInfo&amp; evacuation_info,</span>
<span class="udiff-line-modified-added">+                                                    G1RedirtyCardsQueueSet* rdcqs,</span>
<span class="udiff-line-modified-added">+                                                    G1ParScanThreadStateSet* per_thread_states) {</span>
<span class="udiff-line-modified-added">+   G1GCPhaseTimes* p = phase_times();</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   rem_set()-&gt;cleanup_after_scan_heap_roots();</span>
  
    // Process any discovered reference objects - we have
    // to do this _before_ we retire the GC alloc regions
    // as we may have to copy some &#39;reachable&#39; referent
    // objects (and their reachable sub-graphs) that were
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3904,46 +4012,70 @@</span>
    process_discovered_references(per_thread_states);
  
    G1STWIsAliveClosure is_alive(this);
    G1KeepAliveClosure keep_alive(this);
  
<span class="udiff-line-modified-removed">-   WeakProcessor::weak_oops_do(workers(), &amp;is_alive, &amp;keep_alive,</span>
<span class="udiff-line-removed">-                               phase_times()-&gt;weak_phase_times());</span>
<span class="udiff-line-modified-added">+   WeakProcessor::weak_oops_do(workers(), &amp;is_alive, &amp;keep_alive, p-&gt;weak_phase_times());</span>
  
    if (G1StringDedup::is_enabled()) {
      double string_dedup_time_ms = os::elapsedTime();
  
<span class="udiff-line-modified-removed">-     string_dedup_cleaning(&amp;is_alive, &amp;keep_alive, phase_times());</span>
<span class="udiff-line-modified-added">+     string_dedup_cleaning(&amp;is_alive, &amp;keep_alive, p);</span>
  
      double string_cleanup_time_ms = (os::elapsedTime() - string_dedup_time_ms) * 1000.0;
<span class="udiff-line-modified-removed">-     phase_times()-&gt;record_string_deduplication_time(string_cleanup_time_ms);</span>
<span class="udiff-line-modified-added">+     p-&gt;record_string_deduplication_time(string_cleanup_time_ms);</span>
    }
  
<span class="udiff-line-added">+   _allocator-&gt;release_gc_alloc_regions(evacuation_info);</span>
<span class="udiff-line-added">+ </span>
    if (evacuation_failed()) {
<span class="udiff-line-modified-removed">-     restore_after_evac_failure();</span>
<span class="udiff-line-modified-added">+     restore_after_evac_failure(rdcqs);</span>
  
      // Reset the G1EvacuationFailureALot counters and flags
<span class="udiff-line-removed">-     // Note: the values are reset only when an actual</span>
<span class="udiff-line-removed">-     // evacuation failure occurs.</span>
      NOT_PRODUCT(reset_evacuation_should_fail();)
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     double recalculate_used_start = os::elapsedTime();</span>
<span class="udiff-line-added">+     set_used(recalculate_used());</span>
<span class="udiff-line-added">+     p-&gt;record_evac_fail_recalc_used_time((os::elapsedTime() - recalculate_used_start) * 1000.0);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     if (_archive_allocator != NULL) {</span>
<span class="udiff-line-added">+       _archive_allocator-&gt;clear_used();</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+     for (uint i = 0; i &lt; ParallelGCThreads; i++) {</span>
<span class="udiff-line-added">+       if (_evacuation_failed_info_array[i].has_failed()) {</span>
<span class="udiff-line-added">+         _gc_tracer_stw-&gt;report_evacuation_failed(_evacuation_failed_info_array[i]);</span>
<span class="udiff-line-added">+       }</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     // The &quot;used&quot; of the the collection set have already been subtracted</span>
<span class="udiff-line-added">+     // when they were freed.  Add in the bytes used.</span>
<span class="udiff-line-added">+     increase_used(_bytes_used_during_gc);</span>
    }
  
    _preserved_marks_set.assert_empty();
  
<span class="udiff-line-removed">-   _allocator-&gt;release_gc_alloc_regions(evacuation_info);</span>
<span class="udiff-line-removed">- </span>
    merge_per_thread_state_info(per_thread_states);
  
    // Reset and re-enable the hot card cache.
    // Note the counts for the cards in the regions in the
    // collection set are reset when the collection set is freed.
    _hot_card_cache-&gt;reset_hot_cache();
    _hot_card_cache-&gt;set_use_cache(true);
  
    purge_code_root_memory();
  
<span class="udiff-line-modified-removed">-   redirty_logged_cards();</span>
<span class="udiff-line-modified-added">+   redirty_logged_cards(rdcqs);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   free_collection_set(&amp;_collection_set, evacuation_info, per_thread_states-&gt;surviving_young_words());</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   eagerly_reclaim_humongous_regions();</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   record_obj_copy_mem_stats();</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   evacuation_info.set_collectionset_used_before(collection_set()-&gt;bytes_used_before());</span>
<span class="udiff-line-added">+   evacuation_info.set_bytes_used(_bytes_used_during_gc);</span>
<span class="udiff-line-added">+ </span>
  #if COMPILER2_OR_JVMCI
    double start = os::elapsedTime();
    DerivedPointerTable::update_pointers();
    phase_times()-&gt;record_derived_pointer_table_update_time((os::elapsedTime() - start) * 1000.0);
  #endif
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -3955,343 +4087,341 @@</span>
  
    _gc_tracer_stw-&gt;report_evacuation_statistics(create_g1_evac_summary(&amp;_survivor_evac_stats),
                                                 create_g1_evac_summary(&amp;_old_evac_stats));
  }
  
<span class="udiff-line-modified-removed">- void G1CollectedHeap::free_region(HeapRegion* hr,</span>
<span class="udiff-line-removed">-                                   FreeRegionList* free_list,</span>
<span class="udiff-line-removed">-                                   bool skip_remset,</span>
<span class="udiff-line-removed">-                                   bool skip_hot_card_cache,</span>
<span class="udiff-line-removed">-                                   bool locked) {</span>
<span class="udiff-line-modified-added">+ void G1CollectedHeap::free_region(HeapRegion* hr, FreeRegionList* free_list) {</span>
    assert(!hr-&gt;is_free(), &quot;the region should not be free&quot;);
    assert(!hr-&gt;is_empty(), &quot;the region should not be empty&quot;);
    assert(_hrm-&gt;is_available(hr-&gt;hrm_index()), &quot;region should be committed&quot;);
<span class="udiff-line-removed">-   assert(free_list != NULL, &quot;pre-condition&quot;);</span>
  
    if (G1VerifyBitmaps) {
      MemRegion mr(hr-&gt;bottom(), hr-&gt;end());
      concurrent_mark()-&gt;clear_range_in_prev_bitmap(mr);
    }
  
    // Clear the card counts for this region.
    // Note: we only need to do this if the region is not young
    // (since we don&#39;t refine cards in young regions).
<span class="udiff-line-modified-removed">-   if (!skip_hot_card_cache &amp;&amp; !hr-&gt;is_young()) {</span>
<span class="udiff-line-modified-added">+   if (!hr-&gt;is_young()) {</span>
      _hot_card_cache-&gt;reset_card_counts(hr);
    }
<span class="udiff-line-modified-removed">-   hr-&gt;hr_clear(skip_remset, true /* clear_space */, locked /* locked */);</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-added">+   // Reset region metadata to allow reuse.</span>
<span class="udiff-line-added">+   hr-&gt;hr_clear(true /* clear_space */);</span>
    _policy-&gt;remset_tracker()-&gt;update_at_free(hr);
<span class="udiff-line-modified-removed">-   free_list-&gt;add_ordered(hr);</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-added">+   if (free_list != NULL) {</span>
<span class="udiff-line-added">+     free_list-&gt;add_ordered(hr);</span>
<span class="udiff-line-added">+   }</span>
  }
  
  void G1CollectedHeap::free_humongous_region(HeapRegion* hr,
                                              FreeRegionList* free_list) {
    assert(hr-&gt;is_humongous(), &quot;this is only for humongous regions&quot;);
    assert(free_list != NULL, &quot;pre-condition&quot;);
    hr-&gt;clear_humongous();
<span class="udiff-line-modified-removed">-   free_region(hr, free_list, false /* skip_remset */, false /* skip_hcc */, true /* locked */);</span>
<span class="udiff-line-modified-added">+   free_region(hr, free_list);</span>
  }
  
  void G1CollectedHeap::remove_from_old_sets(const uint old_regions_removed,
                                             const uint humongous_regions_removed) {
    if (old_regions_removed &gt; 0 || humongous_regions_removed &gt; 0) {
<span class="udiff-line-modified-removed">-     MutexLockerEx x(OldSets_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="udiff-line-modified-added">+     MutexLocker x(OldSets_lock, Mutex::_no_safepoint_check_flag);</span>
      _old_set.bulk_remove(old_regions_removed);
      _humongous_set.bulk_remove(humongous_regions_removed);
    }
  
  }
  
  void G1CollectedHeap::prepend_to_freelist(FreeRegionList* list) {
    assert(list != NULL, &quot;list can&#39;t be null&quot;);
    if (!list-&gt;is_empty()) {
<span class="udiff-line-modified-removed">-     MutexLockerEx x(FreeList_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="udiff-line-modified-added">+     MutexLocker x(FreeList_lock, Mutex::_no_safepoint_check_flag);</span>
      _hrm-&gt;insert_list_into_free_list(list);
    }
  }
  
  void G1CollectedHeap::decrement_summary_bytes(size_t bytes) {
    decrease_used(bytes);
  }
  
  class G1FreeCollectionSetTask : public AbstractGangTask {
<span class="udiff-line-modified-removed">- private:</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-   // Closure applied to all regions in the collection set to do work that needs to</span>
<span class="udiff-line-modified-removed">-   // be done serially in a single thread.</span>
<span class="udiff-line-modified-removed">-   class G1SerialFreeCollectionSetClosure : public HeapRegionClosure {</span>
<span class="udiff-line-modified-removed">-   private:</span>
<span class="udiff-line-modified-removed">-     G1EvacuationInfo* _evacuation_info;</span>
<span class="udiff-line-modified-removed">-     const size_t* _surviving_young_words;</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-removed">-     // Bytes used in successfully evacuated regions before the evacuation.</span>
<span class="udiff-line-removed">-     size_t _before_used_bytes;</span>
<span class="udiff-line-removed">-     // Bytes used in unsucessfully evacuated regions before the evacuation</span>
<span class="udiff-line-removed">-     size_t _after_used_bytes;</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     size_t _bytes_allocated_in_old_since_last_gc;</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     size_t _failure_used_words;</span>
<span class="udiff-line-removed">-     size_t _failure_waste_words;</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     FreeRegionList _local_free_list;</span>
<span class="udiff-line-modified-added">+   // Helper class to keep statistics for the collection set freeing</span>
<span class="udiff-line-modified-added">+   class FreeCSetStats {</span>
<span class="udiff-line-modified-added">+     size_t _before_used_bytes;   // Usage in regions successfully evacutate</span>
<span class="udiff-line-modified-added">+     size_t _after_used_bytes;    // Usage in regions failing evacuation</span>
<span class="udiff-line-modified-added">+     size_t _bytes_allocated_in_old_since_last_gc; // Size of young regions turned into old</span>
<span class="udiff-line-modified-added">+     size_t _failure_used_words;  // Live size in failed regions</span>
<span class="udiff-line-modified-added">+     size_t _failure_waste_words; // Wasted size in failed regions</span>
<span class="udiff-line-modified-added">+     size_t _rs_length;           // Remembered set size</span>
<span class="udiff-line-modified-added">+     uint _regions_freed;         // Number of regions freed</span>
    public:
<span class="udiff-line-modified-removed">-     G1SerialFreeCollectionSetClosure(G1EvacuationInfo* evacuation_info, const size_t* surviving_young_words) :</span>
<span class="udiff-line-modified-removed">-       HeapRegionClosure(),</span>
<span class="udiff-line-modified-removed">-       _evacuation_info(evacuation_info),</span>
<span class="udiff-line-modified-removed">-       _surviving_young_words(surviving_young_words),</span>
<span class="udiff-line-modified-removed">-       _before_used_bytes(0),</span>
<span class="udiff-line-modified-removed">-       _after_used_bytes(0),</span>
<span class="udiff-line-modified-removed">-       _bytes_allocated_in_old_since_last_gc(0),</span>
<span class="udiff-line-modified-removed">-       _failure_used_words(0),</span>
<span class="udiff-line-modified-removed">-       _failure_waste_words(0),</span>
<span class="udiff-line-modified-removed">-       _local_free_list(&quot;Local Region List for CSet Freeing&quot;) {</span>
<span class="udiff-line-modified-added">+     FreeCSetStats() :</span>
<span class="udiff-line-modified-added">+         _before_used_bytes(0),</span>
<span class="udiff-line-modified-added">+         _after_used_bytes(0),</span>
<span class="udiff-line-modified-added">+         _bytes_allocated_in_old_since_last_gc(0),</span>
<span class="udiff-line-modified-added">+         _failure_used_words(0),</span>
<span class="udiff-line-modified-added">+         _failure_waste_words(0),</span>
<span class="udiff-line-modified-added">+         _rs_length(0),</span>
<span class="udiff-line-modified-added">+         _regions_freed(0) { }</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+     void merge_stats(FreeCSetStats* other) {</span>
<span class="udiff-line-added">+       assert(other != NULL, &quot;invariant&quot;);</span>
<span class="udiff-line-added">+       _before_used_bytes += other-&gt;_before_used_bytes;</span>
<span class="udiff-line-added">+       _after_used_bytes += other-&gt;_after_used_bytes;</span>
<span class="udiff-line-added">+       _bytes_allocated_in_old_since_last_gc += other-&gt;_bytes_allocated_in_old_since_last_gc;</span>
<span class="udiff-line-added">+       _failure_used_words += other-&gt;_failure_used_words;</span>
<span class="udiff-line-added">+       _failure_waste_words += other-&gt;_failure_waste_words;</span>
<span class="udiff-line-added">+       _rs_length += other-&gt;_rs_length;</span>
<span class="udiff-line-added">+       _regions_freed += other-&gt;_regions_freed;</span>
      }
  
<span class="udiff-line-modified-removed">-     virtual bool do_heap_region(HeapRegion* r) {</span>
<span class="udiff-line-modified-removed">-       G1CollectedHeap* g1h = G1CollectedHeap::heap();</span>
<span class="udiff-line-modified-added">+     void report(G1CollectedHeap* g1h, G1EvacuationInfo* evacuation_info) {</span>
<span class="udiff-line-modified-added">+       evacuation_info-&gt;set_regions_freed(_regions_freed);</span>
<span class="udiff-line-added">+       evacuation_info-&gt;increment_collectionset_used_after(_after_used_bytes);</span>
  
<span class="udiff-line-modified-removed">-       assert(r-&gt;in_collection_set(), &quot;Region %u should be in collection set.&quot;, r-&gt;hrm_index());</span>
<span class="udiff-line-modified-removed">-       g1h-&gt;clear_in_cset(r);</span>
<span class="udiff-line-modified-added">+       g1h-&gt;decrement_summary_bytes(_before_used_bytes);</span>
<span class="udiff-line-modified-added">+       g1h-&gt;alloc_buffer_stats(G1HeapRegionAttr::Old)-&gt;add_failure_used_and_waste(_failure_used_words, _failure_waste_words);</span>
  
<span class="udiff-line-modified-removed">-       if (r-&gt;is_young()) {</span>
<span class="udiff-line-modified-removed">-         assert(r-&gt;young_index_in_cset() != -1 &amp;&amp; (uint)r-&gt;young_index_in_cset() &lt; g1h-&gt;collection_set()-&gt;young_region_length(),</span>
<span class="udiff-line-modified-removed">-                &quot;Young index %d is wrong for region %u of type %s with %u young regions&quot;,</span>
<span class="udiff-line-modified-removed">-                r-&gt;young_index_in_cset(),</span>
<span class="udiff-line-modified-removed">-                r-&gt;hrm_index(),</span>
<span class="udiff-line-removed">-                r-&gt;get_type_str(),</span>
<span class="udiff-line-removed">-                g1h-&gt;collection_set()-&gt;young_region_length());</span>
<span class="udiff-line-removed">-         size_t words_survived = _surviving_young_words[r-&gt;young_index_in_cset()];</span>
<span class="udiff-line-removed">-         r-&gt;record_surv_words_in_group(words_survived);</span>
<span class="udiff-line-removed">-       }</span>
<span class="udiff-line-modified-added">+       G1Policy *policy = g1h-&gt;policy();</span>
<span class="udiff-line-modified-added">+       policy-&gt;add_bytes_allocated_in_old_since_last_gc(_bytes_allocated_in_old_since_last_gc);</span>
<span class="udiff-line-modified-added">+       policy-&gt;record_rs_length(_rs_length);</span>
<span class="udiff-line-modified-added">+       policy-&gt;cset_regions_freed();</span>
<span class="udiff-line-modified-added">+     }</span>
  
<span class="udiff-line-modified-removed">-       if (!r-&gt;evacuation_failed()) {</span>
<span class="udiff-line-modified-removed">-         assert(r-&gt;not_empty(), &quot;Region %u is an empty region in the collection set.&quot;, r-&gt;hrm_index());</span>
<span class="udiff-line-modified-removed">-         _before_used_bytes += r-&gt;used();</span>
<span class="udiff-line-modified-removed">-         g1h-&gt;free_region(r,</span>
<span class="udiff-line-modified-removed">-                          &amp;_local_free_list,</span>
<span class="udiff-line-modified-removed">-                          true, /* skip_remset */</span>
<span class="udiff-line-modified-removed">-                          true, /* skip_hot_card_cache */</span>
<span class="udiff-line-modified-removed">-                          true  /* locked */);</span>
<span class="udiff-line-modified-removed">-       } else {</span>
<span class="udiff-line-modified-removed">-         r-&gt;uninstall_surv_rate_group();</span>
<span class="udiff-line-modified-removed">-         r-&gt;set_young_index_in_cset(-1);</span>
<span class="udiff-line-modified-removed">-         r-&gt;set_evacuation_failed(false);</span>
<span class="udiff-line-modified-removed">-         // When moving a young gen region to old gen, we &quot;allocate&quot; that whole region</span>
<span class="udiff-line-removed">-         // there. This is in addition to any already evacuated objects. Notify the</span>
<span class="udiff-line-removed">-         // policy about that.</span>
<span class="udiff-line-removed">-         // Old gen regions do not cause an additional allocation: both the objects</span>
<span class="udiff-line-removed">-         // still in the region and the ones already moved are accounted for elsewhere.</span>
<span class="udiff-line-removed">-         if (r-&gt;is_young()) {</span>
<span class="udiff-line-removed">-           _bytes_allocated_in_old_since_last_gc += HeapRegion::GrainBytes;</span>
<span class="udiff-line-removed">-         }</span>
<span class="udiff-line-removed">-         // The region is now considered to be old.</span>
<span class="udiff-line-removed">-         r-&gt;set_old();</span>
<span class="udiff-line-removed">-         // Do some allocation statistics accounting. Regions that failed evacuation</span>
<span class="udiff-line-removed">-         // are always made old, so there is no need to update anything in the young</span>
<span class="udiff-line-removed">-         // gen statistics, but we need to update old gen statistics.</span>
<span class="udiff-line-removed">-         size_t used_words = r-&gt;marked_bytes() / HeapWordSize;</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         _failure_used_words += used_words;</span>
<span class="udiff-line-removed">-         _failure_waste_words += HeapRegion::GrainWords - used_words;</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         g1h-&gt;old_set_add(r);</span>
<span class="udiff-line-removed">-         _after_used_bytes += r-&gt;used();</span>
<span class="udiff-line-modified-added">+     void account_failed_region(HeapRegion* r) {</span>
<span class="udiff-line-modified-added">+       size_t used_words = r-&gt;marked_bytes() / HeapWordSize;</span>
<span class="udiff-line-modified-added">+       _failure_used_words += used_words;</span>
<span class="udiff-line-modified-added">+       _failure_waste_words += HeapRegion::GrainWords - used_words;</span>
<span class="udiff-line-modified-added">+       _after_used_bytes += r-&gt;used();</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+       // When moving a young gen region to old gen, we &quot;allocate&quot; that whole</span>
<span class="udiff-line-modified-added">+       // region there. This is in addition to any already evacuated objects.</span>
<span class="udiff-line-modified-added">+       // Notify the policy about that. Old gen regions do not cause an</span>
<span class="udiff-line-modified-added">+       // additional allocation: both the objects still in the region and the</span>
<span class="udiff-line-modified-added">+       // ones already moved are accounted for elsewhere.</span>
<span class="udiff-line-modified-added">+       if (r-&gt;is_young()) {</span>
<span class="udiff-line-modified-added">+         _bytes_allocated_in_old_since_last_gc += HeapRegion::GrainBytes;</span>
        }
<span class="udiff-line-removed">-       return false;</span>
      }
  
<span class="udiff-line-modified-removed">-     void complete_work() {</span>
<span class="udiff-line-modified-removed">-       G1CollectedHeap* g1h = G1CollectedHeap::heap();</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-       _evacuation_info-&gt;set_regions_freed(_local_free_list.length());</span>
<span class="udiff-line-removed">-       _evacuation_info-&gt;increment_collectionset_used_after(_after_used_bytes);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       g1h-&gt;prepend_to_freelist(&amp;_local_free_list);</span>
<span class="udiff-line-removed">-       g1h-&gt;decrement_summary_bytes(_before_used_bytes);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       G1Policy* policy = g1h-&gt;policy();</span>
<span class="udiff-line-removed">-       policy-&gt;add_bytes_allocated_in_old_since_last_gc(_bytes_allocated_in_old_since_last_gc);</span>
<span class="udiff-line-modified-added">+     void account_evacuated_region(HeapRegion* r) {</span>
<span class="udiff-line-modified-added">+       _before_used_bytes += r-&gt;used();</span>
<span class="udiff-line-modified-added">+       _regions_freed += 1;</span>
<span class="udiff-line-modified-added">+     }</span>
  
<span class="udiff-line-modified-removed">-       g1h-&gt;alloc_buffer_stats(InCSetState::Old)-&gt;add_failure_used_and_waste(_failure_used_words, _failure_waste_words);</span>
<span class="udiff-line-modified-added">+     void account_rs_length(HeapRegion* r) {</span>
<span class="udiff-line-added">+       _rs_length += r-&gt;rem_set()-&gt;occupied();</span>
      }
    };
  
<span class="udiff-line-modified-removed">-   G1CollectionSet* _collection_set;</span>
<span class="udiff-line-modified-removed">-   G1SerialFreeCollectionSetClosure _cl;</span>
<span class="udiff-line-modified-removed">-   const size_t* _surviving_young_words;</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-   size_t _rs_lengths;</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-   volatile jint _serial_work_claim;</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-   struct WorkItem {</span>
<span class="udiff-line-modified-removed">-     uint region_idx;</span>
<span class="udiff-line-modified-removed">-     bool is_young;</span>
<span class="udiff-line-modified-removed">-     bool evacuation_failed;</span>
<span class="udiff-line-modified-added">+   // Closure applied to all regions in the collection set.</span>
<span class="udiff-line-modified-added">+   class FreeCSetClosure : public HeapRegionClosure {</span>
<span class="udiff-line-modified-added">+     // Helper to send JFR events for regions.</span>
<span class="udiff-line-modified-added">+     class JFREventForRegion {</span>
<span class="udiff-line-modified-added">+       EventGCPhaseParallel _event;</span>
<span class="udiff-line-modified-added">+     public:</span>
<span class="udiff-line-modified-added">+       JFREventForRegion(HeapRegion* region, uint worker_id) : _event() {</span>
<span class="udiff-line-modified-added">+         _event.set_gcId(GCId::current());</span>
<span class="udiff-line-modified-added">+         _event.set_gcWorkerId(worker_id);</span>
<span class="udiff-line-modified-added">+         if (region-&gt;is_young()) {</span>
<span class="udiff-line-modified-added">+           _event.set_name(G1GCPhaseTimes::phase_name(G1GCPhaseTimes::YoungFreeCSet));</span>
<span class="udiff-line-modified-added">+         } else {</span>
<span class="udiff-line-added">+           _event.set_name(G1GCPhaseTimes::phase_name(G1GCPhaseTimes::NonYoungFreeCSet));</span>
<span class="udiff-line-added">+         }</span>
<span class="udiff-line-added">+       }</span>
  
<span class="udiff-line-modified-removed">-     WorkItem(HeapRegion* r) {</span>
<span class="udiff-line-modified-removed">-       region_idx = r-&gt;hrm_index();</span>
<span class="udiff-line-modified-removed">-       is_young = r-&gt;is_young();</span>
<span class="udiff-line-modified-removed">-       evacuation_failed = r-&gt;evacuation_failed();</span>
<span class="udiff-line-modified-added">+       ~JFREventForRegion() {</span>
<span class="udiff-line-modified-added">+         _event.commit();</span>
<span class="udiff-line-modified-added">+       }</span>
<span class="udiff-line-modified-added">+     };</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     // Helper to do timing for region work.</span>
<span class="udiff-line-added">+     class TimerForRegion {</span>
<span class="udiff-line-added">+       Tickspan&amp; _time;</span>
<span class="udiff-line-added">+       Ticks     _start_time;</span>
<span class="udiff-line-added">+     public:</span>
<span class="udiff-line-added">+       TimerForRegion(Tickspan&amp; time) : _time(time), _start_time(Ticks::now()) { }</span>
<span class="udiff-line-added">+       ~TimerForRegion() {</span>
<span class="udiff-line-added">+         _time += Ticks::now() - _start_time;</span>
<span class="udiff-line-added">+       }</span>
<span class="udiff-line-added">+     };</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     // FreeCSetClosure members</span>
<span class="udiff-line-added">+     G1CollectedHeap* _g1h;</span>
<span class="udiff-line-added">+     const size_t*    _surviving_young_words;</span>
<span class="udiff-line-added">+     uint             _worker_id;</span>
<span class="udiff-line-added">+     Tickspan         _young_time;</span>
<span class="udiff-line-added">+     Tickspan         _non_young_time;</span>
<span class="udiff-line-added">+     FreeCSetStats*   _stats;</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     void assert_in_cset(HeapRegion* r) {</span>
<span class="udiff-line-added">+       assert(r-&gt;young_index_in_cset() != 0 &amp;&amp;</span>
<span class="udiff-line-added">+              (uint)r-&gt;young_index_in_cset() &lt;= _g1h-&gt;collection_set()-&gt;young_region_length(),</span>
<span class="udiff-line-added">+              &quot;Young index %u is wrong for region %u of type %s with %u young regions&quot;,</span>
<span class="udiff-line-added">+              r-&gt;young_index_in_cset(), r-&gt;hrm_index(), r-&gt;get_type_str(), _g1h-&gt;collection_set()-&gt;young_region_length());</span>
      }
<span class="udiff-line-removed">-   };</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-   volatile size_t _parallel_work_claim;</span>
<span class="udiff-line-removed">-   size_t _num_work_items;</span>
<span class="udiff-line-removed">-   WorkItem* _work_items;</span>
  
<span class="udiff-line-modified-removed">-   void do_serial_work() {</span>
<span class="udiff-line-modified-removed">-     // Need to grab the lock to be allowed to modify the old region list.</span>
<span class="udiff-line-modified-removed">-     MutexLockerEx x(OldSets_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="udiff-line-removed">-     _collection_set-&gt;iterate(&amp;_cl);</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-modified-added">+     void handle_evacuated_region(HeapRegion* r) {</span>
<span class="udiff-line-modified-added">+       assert(!r-&gt;is_empty(), &quot;Region %u is an empty region in the collection set.&quot;, r-&gt;hrm_index());</span>
<span class="udiff-line-modified-added">+       stats()-&gt;account_evacuated_region(r);</span>
  
<span class="udiff-line-modified-removed">-   void do_parallel_work_for_region(uint region_idx, bool is_young, bool evacuation_failed) {</span>
<span class="udiff-line-modified-removed">-     G1CollectedHeap* g1h = G1CollectedHeap::heap();</span>
<span class="udiff-line-modified-added">+       // Free the region and and its remembered set.</span>
<span class="udiff-line-modified-added">+       _g1h-&gt;free_region(r, NULL);</span>
<span class="udiff-line-added">+     }</span>
  
<span class="udiff-line-modified-removed">-     HeapRegion* r = g1h-&gt;region_at(region_idx);</span>
<span class="udiff-line-modified-removed">-     assert(!g1h-&gt;is_on_master_free_list(r), &quot;sanity&quot;);</span>
<span class="udiff-line-modified-added">+     void handle_failed_region(HeapRegion* r) {</span>
<span class="udiff-line-modified-added">+       // Do some allocation statistics accounting. Regions that failed evacuation</span>
<span class="udiff-line-added">+       // are always made old, so there is no need to update anything in the young</span>
<span class="udiff-line-added">+       // gen statistics, but we need to update old gen statistics.</span>
<span class="udiff-line-added">+       stats()-&gt;account_failed_region(r);</span>
  
<span class="udiff-line-modified-removed">-     Atomic::add(r-&gt;rem_set()-&gt;occupied_locked(), &amp;_rs_lengths);</span>
<span class="udiff-line-modified-added">+       // Update the region state due to the failed evacuation.</span>
<span class="udiff-line-added">+       r-&gt;handle_evacuation_failure();</span>
  
<span class="udiff-line-modified-removed">-     if (!is_young) {</span>
<span class="udiff-line-modified-removed">-       g1h-&gt;_hot_card_cache-&gt;reset_card_counts(r);</span>
<span class="udiff-line-modified-added">+       // Add region to old set, need to hold lock.</span>
<span class="udiff-line-modified-added">+       MutexLocker x(OldSets_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="udiff-line-added">+       _g1h-&gt;old_set_add(r);</span>
      }
  
<span class="udiff-line-modified-removed">-     if (!evacuation_failed) {</span>
<span class="udiff-line-modified-removed">-       r-&gt;rem_set()-&gt;clear_locked();</span>
<span class="udiff-line-modified-added">+     Tickspan&amp; timer_for_region(HeapRegion* r) {</span>
<span class="udiff-line-modified-added">+       return r-&gt;is_young() ? _young_time : _non_young_time;</span>
      }
<span class="udiff-line-removed">-   }</span>
  
<span class="udiff-line-modified-removed">-   class G1PrepareFreeCollectionSetClosure : public HeapRegionClosure {</span>
<span class="udiff-line-modified-removed">-   private:</span>
<span class="udiff-line-modified-removed">-     size_t _cur_idx;</span>
<span class="udiff-line-removed">-     WorkItem* _work_items;</span>
<span class="udiff-line-modified-added">+     FreeCSetStats* stats() {</span>
<span class="udiff-line-modified-added">+       return _stats;</span>
<span class="udiff-line-modified-added">+     }</span>
    public:
<span class="udiff-line-modified-removed">-     G1PrepareFreeCollectionSetClosure(WorkItem* work_items) : HeapRegionClosure(), _cur_idx(0), _work_items(work_items) { }</span>
<span class="udiff-line-modified-added">+     FreeCSetClosure(const size_t* surviving_young_words,</span>
<span class="udiff-line-added">+                     uint worker_id,</span>
<span class="udiff-line-added">+                     FreeCSetStats* stats) :</span>
<span class="udiff-line-added">+         HeapRegionClosure(),</span>
<span class="udiff-line-added">+         _g1h(G1CollectedHeap::heap()),</span>
<span class="udiff-line-added">+         _surviving_young_words(surviving_young_words),</span>
<span class="udiff-line-added">+         _worker_id(worker_id),</span>
<span class="udiff-line-added">+         _young_time(),</span>
<span class="udiff-line-added">+         _non_young_time(),</span>
<span class="udiff-line-added">+         _stats(stats) { }</span>
  
      virtual bool do_heap_region(HeapRegion* r) {
<span class="udiff-line-modified-removed">-       _work_items[_cur_idx++] = WorkItem(r);</span>
<span class="udiff-line-modified-added">+       assert(r-&gt;in_collection_set(), &quot;Invariant: %u missing from CSet&quot;, r-&gt;hrm_index());</span>
<span class="udiff-line-added">+       JFREventForRegion event(r, _worker_id);</span>
<span class="udiff-line-added">+       TimerForRegion timer(timer_for_region(r));</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+       _g1h-&gt;clear_region_attr(r);</span>
<span class="udiff-line-added">+       stats()-&gt;account_rs_length(r);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+       if (r-&gt;is_young()) {</span>
<span class="udiff-line-added">+         assert_in_cset(r);</span>
<span class="udiff-line-added">+         r-&gt;record_surv_words_in_group(_surviving_young_words[r-&gt;young_index_in_cset()]);</span>
<span class="udiff-line-added">+       }</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+       if (r-&gt;evacuation_failed()) {</span>
<span class="udiff-line-added">+         handle_failed_region(r);</span>
<span class="udiff-line-added">+       } else {</span>
<span class="udiff-line-added">+         handle_evacuated_region(r);</span>
<span class="udiff-line-added">+       }</span>
<span class="udiff-line-added">+       assert(!_g1h-&gt;is_on_master_free_list(r), &quot;sanity&quot;);</span>
<span class="udiff-line-added">+ </span>
        return false;
      }
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+     void report_timing(Tickspan parallel_time) {</span>
<span class="udiff-line-added">+       G1GCPhaseTimes* pt = _g1h-&gt;phase_times();</span>
<span class="udiff-line-added">+       pt-&gt;record_time_secs(G1GCPhaseTimes::ParFreeCSet, _worker_id, parallel_time.seconds());</span>
<span class="udiff-line-added">+       if (_young_time.value() &gt; 0) {</span>
<span class="udiff-line-added">+         pt-&gt;record_time_secs(G1GCPhaseTimes::YoungFreeCSet, _worker_id, _young_time.seconds());</span>
<span class="udiff-line-added">+       }</span>
<span class="udiff-line-added">+       if (_non_young_time.value() &gt; 0) {</span>
<span class="udiff-line-added">+         pt-&gt;record_time_secs(G1GCPhaseTimes::NonYoungFreeCSet, _worker_id, _non_young_time.seconds());</span>
<span class="udiff-line-added">+       }</span>
<span class="udiff-line-added">+     }</span>
    };
  
<span class="udiff-line-modified-removed">-   void prepare_work() {</span>
<span class="udiff-line-modified-removed">-     G1PrepareFreeCollectionSetClosure cl(_work_items);</span>
<span class="udiff-line-modified-removed">-     _collection_set-&gt;iterate(&amp;cl);</span>
<span class="udiff-line-modified-removed">-   }</span>
<span class="udiff-line-modified-added">+   // G1FreeCollectionSetTask members</span>
<span class="udiff-line-modified-added">+   G1CollectedHeap*  _g1h;</span>
<span class="udiff-line-modified-added">+   G1EvacuationInfo* _evacuation_info;</span>
<span class="udiff-line-modified-added">+   FreeCSetStats*    _worker_stats;</span>
<span class="udiff-line-added">+   HeapRegionClaimer _claimer;</span>
<span class="udiff-line-added">+   const size_t*     _surviving_young_words;</span>
<span class="udiff-line-added">+   uint              _active_workers;</span>
  
<span class="udiff-line-modified-removed">-   void complete_work() {</span>
<span class="udiff-line-modified-removed">-     _cl.complete_work();</span>
<span class="udiff-line-modified-added">+   FreeCSetStats* worker_stats(uint worker) {</span>
<span class="udiff-line-modified-added">+     return &amp;_worker_stats[worker];</span>
<span class="udiff-line-added">+   }</span>
  
<span class="udiff-line-modified-removed">-     G1Policy* policy = G1CollectedHeap::heap()-&gt;policy();</span>
<span class="udiff-line-modified-removed">-     policy-&gt;record_max_rs_lengths(_rs_lengths);</span>
<span class="udiff-line-modified-removed">-     policy-&gt;cset_regions_freed();</span>
<span class="udiff-line-modified-added">+   void report_statistics() {</span>
<span class="udiff-line-modified-added">+     // Merge the accounting</span>
<span class="udiff-line-modified-added">+     FreeCSetStats total_stats;</span>
<span class="udiff-line-added">+     for (uint worker = 0; worker &lt; _active_workers; worker++) {</span>
<span class="udiff-line-added">+       total_stats.merge_stats(worker_stats(worker));</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+     total_stats.report(_g1h, _evacuation_info);</span>
    }
<span class="udiff-line-added">+ </span>
  public:
<span class="udiff-line-modified-removed">-   G1FreeCollectionSetTask(G1CollectionSet* collection_set, G1EvacuationInfo* evacuation_info, const size_t* surviving_young_words) :</span>
<span class="udiff-line-modified-removed">-     AbstractGangTask(&quot;G1 Free Collection Set&quot;),</span>
<span class="udiff-line-modified-removed">-     _collection_set(collection_set),</span>
<span class="udiff-line-modified-removed">-     _cl(evacuation_info, surviving_young_words),</span>
<span class="udiff-line-modified-removed">-     _surviving_young_words(surviving_young_words),</span>
<span class="udiff-line-modified-removed">-     _rs_lengths(0),</span>
<span class="udiff-line-modified-removed">-     _serial_work_claim(0),</span>
<span class="udiff-line-modified-removed">-     _parallel_work_claim(0),</span>
<span class="udiff-line-modified-removed">-     _num_work_items(collection_set-&gt;region_length()),</span>
<span class="udiff-line-modified-removed">-     _work_items(NEW_C_HEAP_ARRAY(WorkItem, _num_work_items, mtGC)) {</span>
<span class="udiff-line-modified-removed">-     prepare_work();</span>
<span class="udiff-line-modified-added">+   G1FreeCollectionSetTask(G1EvacuationInfo* evacuation_info, const size_t* surviving_young_words, uint active_workers) :</span>
<span class="udiff-line-modified-added">+       AbstractGangTask(&quot;G1 Free Collection Set&quot;),</span>
<span class="udiff-line-modified-added">+       _g1h(G1CollectedHeap::heap()),</span>
<span class="udiff-line-modified-added">+       _evacuation_info(evacuation_info),</span>
<span class="udiff-line-modified-added">+       _worker_stats(NEW_C_HEAP_ARRAY(FreeCSetStats, active_workers, mtGC)),</span>
<span class="udiff-line-modified-added">+       _claimer(active_workers),</span>
<span class="udiff-line-modified-added">+       _surviving_young_words(surviving_young_words),</span>
<span class="udiff-line-modified-added">+       _active_workers(active_workers) {</span>
<span class="udiff-line-modified-added">+     for (uint worker = 0; worker &lt; active_workers; worker++) {</span>
<span class="udiff-line-modified-added">+       ::new (&amp;_worker_stats[worker]) FreeCSetStats();</span>
<span class="udiff-line-modified-added">+     }</span>
    }
  
    ~G1FreeCollectionSetTask() {
<span class="udiff-line-modified-removed">-     complete_work();</span>
<span class="udiff-line-modified-removed">-     FREE_C_HEAP_ARRAY(WorkItem, _work_items);</span>
<span class="udiff-line-modified-added">+     Ticks serial_time = Ticks::now();</span>
<span class="udiff-line-modified-added">+     report_statistics();</span>
<span class="udiff-line-added">+     for (uint worker = 0; worker &lt; _active_workers; worker++) {</span>
<span class="udiff-line-added">+       _worker_stats[worker].~FreeCSetStats();</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+     FREE_C_HEAP_ARRAY(FreeCSetStats, _worker_stats);</span>
<span class="udiff-line-added">+     _g1h-&gt;phase_times()-&gt;record_serial_free_cset_time_ms((Ticks::now() - serial_time).seconds() * 1000.0);</span>
    }
  
<span class="udiff-line-removed">-   // Chunk size for work distribution. The chosen value has been determined experimentally</span>
<span class="udiff-line-removed">-   // to be a good tradeoff between overhead and achievable parallelism.</span>
<span class="udiff-line-removed">-   static uint chunk_size() { return 32; }</span>
<span class="udiff-line-removed">- </span>
    virtual void work(uint worker_id) {
<span class="udiff-line-modified-removed">-     G1GCPhaseTimes* timer = G1CollectedHeap::heap()-&gt;phase_times();</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">-     // Claim serial work.</span>
<span class="udiff-line-modified-removed">-     if (_serial_work_claim == 0) {</span>
<span class="udiff-line-removed">-       jint value = Atomic::add(1, &amp;_serial_work_claim) - 1;</span>
<span class="udiff-line-removed">-       if (value == 0) {</span>
<span class="udiff-line-removed">-         double serial_time = os::elapsedTime();</span>
<span class="udiff-line-removed">-         do_serial_work();</span>
<span class="udiff-line-removed">-         timer-&gt;record_serial_free_cset_time_ms((os::elapsedTime() - serial_time) * 1000.0);</span>
<span class="udiff-line-removed">-       }</span>
<span class="udiff-line-removed">-     }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     // Start parallel work.</span>
<span class="udiff-line-removed">-     double young_time = 0.0;</span>
<span class="udiff-line-removed">-     bool has_young_time = false;</span>
<span class="udiff-line-removed">-     double non_young_time = 0.0;</span>
<span class="udiff-line-removed">-     bool has_non_young_time = false;</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-     while (true) {</span>
<span class="udiff-line-removed">-       size_t end = Atomic::add(chunk_size(), &amp;_parallel_work_claim);</span>
<span class="udiff-line-removed">-       size_t cur = end - chunk_size();</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       if (cur &gt;= _num_work_items) {</span>
<span class="udiff-line-removed">-         break;</span>
<span class="udiff-line-removed">-       }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       EventGCPhaseParallel event;</span>
<span class="udiff-line-removed">-       double start_time = os::elapsedTime();</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       end = MIN2(end, _num_work_items);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-       for (; cur &lt; end; cur++) {</span>
<span class="udiff-line-removed">-         bool is_young = _work_items[cur].is_young;</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         do_parallel_work_for_region(_work_items[cur].region_idx, is_young, _work_items[cur].evacuation_failed);</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">-         double end_time = os::elapsedTime();</span>
<span class="udiff-line-removed">-         double time_taken = end_time - start_time;</span>
<span class="udiff-line-removed">-         if (is_young) {</span>
<span class="udiff-line-removed">-           young_time += time_taken;</span>
<span class="udiff-line-removed">-           has_young_time = true;</span>
<span class="udiff-line-removed">-           event.commit(GCId::current(), worker_id, G1GCPhaseTimes::phase_name(G1GCPhaseTimes::YoungFreeCSet));</span>
<span class="udiff-line-removed">-         } else {</span>
<span class="udiff-line-removed">-           non_young_time += time_taken;</span>
<span class="udiff-line-removed">-           has_non_young_time = true;</span>
<span class="udiff-line-removed">-           event.commit(GCId::current(), worker_id, G1GCPhaseTimes::phase_name(G1GCPhaseTimes::NonYoungFreeCSet));</span>
<span class="udiff-line-removed">-         }</span>
<span class="udiff-line-removed">-         start_time = end_time;</span>
<span class="udiff-line-removed">-       }</span>
<span class="udiff-line-removed">-     }</span>
<span class="udiff-line-modified-added">+     EventGCPhaseParallel event;</span>
<span class="udiff-line-modified-added">+     Ticks start = Ticks::now();</span>
<span class="udiff-line-modified-added">+     FreeCSetClosure cl(_surviving_young_words, worker_id, worker_stats(worker_id));</span>
<span class="udiff-line-modified-added">+     _g1h-&gt;collection_set_par_iterate_all(&amp;cl, &amp;_claimer, worker_id);</span>
  
<span class="udiff-line-modified-removed">-     if (has_young_time) {</span>
<span class="udiff-line-modified-removed">-       timer-&gt;record_time_secs(G1GCPhaseTimes::YoungFreeCSet, worker_id, young_time);</span>
<span class="udiff-line-modified-removed">-     }</span>
<span class="udiff-line-removed">-     if (has_non_young_time) {</span>
<span class="udiff-line-removed">-       timer-&gt;record_time_secs(G1GCPhaseTimes::NonYoungFreeCSet, worker_id, non_young_time);</span>
<span class="udiff-line-removed">-     }</span>
<span class="udiff-line-modified-added">+     // Report the total parallel time along with some more detailed metrics.</span>
<span class="udiff-line-modified-added">+     cl.report_timing(Ticks::now() - start);</span>
<span class="udiff-line-modified-added">+     event.commit(GCId::current(), worker_id, G1GCPhaseTimes::phase_name(G1GCPhaseTimes::ParFreeCSet));</span>
    }
  };
  
  void G1CollectedHeap::free_collection_set(G1CollectionSet* collection_set, G1EvacuationInfo&amp; evacuation_info, const size_t* surviving_young_words) {
    _eden.clear();
  
<span class="udiff-line-modified-removed">-   double free_cset_start_time = os::elapsedTime();</span>
<span class="udiff-line-modified-added">+   // The free collections set is split up in two tasks, the first</span>
<span class="udiff-line-added">+   // frees the collection set and records what regions are free,</span>
<span class="udiff-line-added">+   // and the second one rebuilds the free list. This proved to be</span>
<span class="udiff-line-added">+   // more efficient than adding a sorted list to another.</span>
  
<span class="udiff-line-added">+   Ticks free_cset_start_time = Ticks::now();</span>
    {
<span class="udiff-line-modified-removed">-     uint const num_chunks = MAX2(_collection_set.region_length() / G1FreeCollectionSetTask::chunk_size(), 1U);</span>
<span class="udiff-line-modified-removed">-     uint const num_workers = MIN2(workers()-&gt;active_workers(), num_chunks);</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-removed">-     G1FreeCollectionSetTask cl(collection_set, &amp;evacuation_info, surviving_young_words);</span>
<span class="udiff-line-modified-added">+     uint const num_cs_regions = _collection_set.region_length();</span>
<span class="udiff-line-modified-added">+     uint const num_workers = clamp(num_cs_regions, 1u, workers()-&gt;active_workers());</span>
<span class="udiff-line-modified-added">+     G1FreeCollectionSetTask cl(&amp;evacuation_info, surviving_young_words, num_workers);</span>
  
<span class="udiff-line-modified-removed">-     log_debug(gc, ergo)(&quot;Running %s using %u workers for collection set length %u&quot;,</span>
<span class="udiff-line-modified-removed">-                         cl.name(),</span>
<span class="udiff-line-removed">-                         num_workers,</span>
<span class="udiff-line-removed">-                         _collection_set.region_length());</span>
<span class="udiff-line-modified-added">+     log_debug(gc, ergo)(&quot;Running %s using %u workers for collection set length %u (%u)&quot;,</span>
<span class="udiff-line-modified-added">+                         cl.name(), num_workers, num_cs_regions, num_regions());</span>
      workers()-&gt;run_task(&amp;cl, num_workers);
    }
<span class="udiff-line-modified-removed">-   phase_times()-&gt;record_total_free_cset_time_ms((os::elapsedTime() - free_cset_start_time) * 1000.0);</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-added">+   Ticks free_cset_end_time = Ticks::now();</span>
<span class="udiff-line-added">+   phase_times()-&gt;record_total_free_cset_time_ms((free_cset_end_time - free_cset_start_time).seconds() * 1000.0);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   // Now rebuild the free region list.</span>
<span class="udiff-line-added">+   hrm()-&gt;rebuild_free_list(workers());</span>
<span class="udiff-line-added">+   phase_times()-&gt;record_total_rebuild_freelist_time_ms((Ticks::now() - free_cset_end_time).seconds() * 1000.0);</span>
  
    collection_set-&gt;clear();
  }
  
  class G1FreeHumongousRegionClosure : public HeapRegionClosure {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -4445,19 +4575,19 @@</span>
  
  class G1AbandonCollectionSetClosure : public HeapRegionClosure {
  public:
    virtual bool do_heap_region(HeapRegion* r) {
      assert(r-&gt;in_collection_set(), &quot;Region %u must have been in collection set&quot;, r-&gt;hrm_index());
<span class="udiff-line-modified-removed">-     G1CollectedHeap::heap()-&gt;clear_in_cset(r);</span>
<span class="udiff-line-modified-removed">-     r-&gt;set_young_index_in_cset(-1);</span>
<span class="udiff-line-modified-added">+     G1CollectedHeap::heap()-&gt;clear_region_attr(r);</span>
<span class="udiff-line-modified-added">+     r-&gt;clear_young_index_in_cset();</span>
      return false;
    }
  };
  
  void G1CollectedHeap::abandon_collection_set(G1CollectionSet* collection_set) {
    G1AbandonCollectionSetClosure cl;
<span class="udiff-line-modified-removed">-   collection_set-&gt;iterate(&amp;cl);</span>
<span class="udiff-line-modified-added">+   collection_set_iterate_all(&amp;cl);</span>
  
    collection_set-&gt;clear();
    collection_set-&gt;stop_incremental_building();
  }
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -4620,30 +4750,25 @@</span>
      set_used(cl.total_used());
      if (_archive_allocator != NULL) {
        _archive_allocator-&gt;clear_used();
      }
    }
<span class="udiff-line-modified-removed">-   assert(used() == recalculate_used(),</span>
<span class="udiff-line-removed">-          &quot;inconsistent used(), value: &quot; SIZE_FORMAT &quot; recalculated: &quot; SIZE_FORMAT,</span>
<span class="udiff-line-removed">-          used(), recalculate_used());</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">- bool G1CollectedHeap::is_in_closed_subset(const void* p) const {</span>
<span class="udiff-line-removed">-   HeapRegion* hr = heap_region_containing(p);</span>
<span class="udiff-line-removed">-   return hr-&gt;is_in(p);</span>
<span class="udiff-line-modified-added">+   assert_used_and_recalculate_used_equal(this);</span>
  }
  
  // Methods for the mutator alloc region
  
  HeapRegion* G1CollectedHeap::new_mutator_alloc_region(size_t word_size,
<span class="udiff-line-modified-removed">-                                                       bool force) {</span>
<span class="udiff-line-modified-added">+                                                       bool force,</span>
<span class="udiff-line-added">+                                                       uint node_index) {</span>
    assert_heap_locked_or_at_safepoint(true /* should_be_vm_thread */);
    bool should_allocate = policy()-&gt;should_allocate_mutator_region();
    if (force || should_allocate) {
      HeapRegion* new_alloc_region = new_region(word_size,
                                                HeapRegionType::Eden,
<span class="udiff-line-modified-removed">-                                               false /* do_expand */);</span>
<span class="udiff-line-modified-added">+                                               false /* do_expand */,</span>
<span class="udiff-line-added">+                                               node_index);</span>
      if (new_alloc_region != NULL) {
        set_region_short_lived_locked(new_alloc_region);
        _hr_printer.alloc(new_alloc_region, !should_allocate);
        _verifier-&gt;check_bitmaps(&quot;Mutator Region Allocation&quot;, new_alloc_region);
        _policy-&gt;remset_tracker()-&gt;update_at_allocate(new_alloc_region);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -4658,28 +4783,30 @@</span>
    assert_heap_locked_or_at_safepoint(true /* should_be_vm_thread */);
    assert(alloc_region-&gt;is_eden(), &quot;all mutator alloc regions should be eden&quot;);
  
    collection_set()-&gt;add_eden_region(alloc_region);
    increase_used(allocated_bytes);
<span class="udiff-line-added">+   _eden.add_used_bytes(allocated_bytes);</span>
    _hr_printer.retire(alloc_region);
<span class="udiff-line-added">+ </span>
    // We update the eden sizes here, when the region is retired,
    // instead of when it&#39;s allocated, since this is the point that its
    // used space has been recorded in _summary_bytes_used.
    g1mm()-&gt;update_eden_size();
  }
  
  // Methods for the GC alloc regions
  
<span class="udiff-line-modified-removed">- bool G1CollectedHeap::has_more_regions(InCSetState dest) {</span>
<span class="udiff-line-modified-added">+ bool G1CollectedHeap::has_more_regions(G1HeapRegionAttr dest) {</span>
    if (dest.is_old()) {
      return true;
    } else {
      return survivor_regions_count() &lt; policy()-&gt;max_survivor_regions();
    }
  }
  
<span class="udiff-line-modified-removed">- HeapRegion* G1CollectedHeap::new_gc_alloc_region(size_t word_size, InCSetState dest) {</span>
<span class="udiff-line-modified-added">+ HeapRegion* G1CollectedHeap::new_gc_alloc_region(size_t word_size, G1HeapRegionAttr dest, uint node_index) {</span>
    assert(FreeList_lock-&gt;owned_by_self(), &quot;pre-condition&quot;);
  
    if (!has_more_regions(dest)) {
      return NULL;
    }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -4691,11 +4818,12 @@</span>
      type = HeapRegionType::Old;
    }
  
    HeapRegion* new_alloc_region = new_region(word_size,
                                              type,
<span class="udiff-line-modified-removed">-                                             true /* do_expand */);</span>
<span class="udiff-line-modified-added">+                                             true /* do_expand */,</span>
<span class="udiff-line-added">+                                             node_index);</span>
  
    if (new_alloc_region != NULL) {
      if (type.is_survivor()) {
        new_alloc_region-&gt;set_survivor();
        _survivor.add(new_alloc_region);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -4703,27 +4831,31 @@</span>
      } else {
        new_alloc_region-&gt;set_old();
        _verifier-&gt;check_bitmaps(&quot;Old Region Allocation&quot;, new_alloc_region);
      }
      _policy-&gt;remset_tracker()-&gt;update_at_allocate(new_alloc_region);
<span class="udiff-line-added">+     register_region_with_region_attr(new_alloc_region);</span>
      _hr_printer.alloc(new_alloc_region);
      return new_alloc_region;
    }
    return NULL;
  }
  
  void G1CollectedHeap::retire_gc_alloc_region(HeapRegion* alloc_region,
                                               size_t allocated_bytes,
<span class="udiff-line-modified-removed">-                                              InCSetState dest) {</span>
<span class="udiff-line-modified-removed">-   policy()-&gt;record_bytes_copied_during_gc(allocated_bytes);</span>
<span class="udiff-line-modified-added">+                                              G1HeapRegionAttr dest) {</span>
<span class="udiff-line-modified-added">+   _bytes_used_during_gc += allocated_bytes;</span>
    if (dest.is_old()) {
      old_set_add(alloc_region);
<span class="udiff-line-added">+   } else {</span>
<span class="udiff-line-added">+     assert(dest.is_young(), &quot;Retiring alloc region should be young (%d)&quot;, dest.type());</span>
<span class="udiff-line-added">+     _survivor.add_used_bytes(allocated_bytes);</span>
    }
  
    bool const during_im = collector_state()-&gt;in_initial_mark_gc();
    if (during_im &amp;&amp; allocated_bytes &gt; 0) {
<span class="udiff-line-modified-removed">-     _cm-&gt;root_regions()-&gt;add(alloc_region);</span>
<span class="udiff-line-modified-added">+     _cm-&gt;root_regions()-&gt;add(alloc_region-&gt;next_top_at_mark_start(), alloc_region-&gt;top());</span>
    }
    _hr_printer.retire(alloc_region);
  }
  
  HeapRegion* G1CollectedHeap::alloc_highest_free_region() {
</pre>
<center><a href="g1CodeCacheRemSet.cpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.udiff.html" target="_top">next &gt;</a></center>  </body>
</html>