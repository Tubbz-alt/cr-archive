<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/share/gc/g1/g1CollectedHeap.hpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1CollectedHeap.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.inline.hpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1CollectedHeap.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 1,7 ***</span>
  /*
<span class="line-modified">!  * Copyright (c) 2001, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
<span class="line-new-header">--- 1,7 ---</span>
  /*
<span class="line-modified">!  * Copyright (c) 2001, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 29,21 ***</span>
  #include &quot;gc/g1/g1BiasedArray.hpp&quot;
  #include &quot;gc/g1/g1CardTable.hpp&quot;
  #include &quot;gc/g1/g1CollectionSet.hpp&quot;
  #include &quot;gc/g1/g1CollectorState.hpp&quot;
  #include &quot;gc/g1/g1ConcurrentMark.hpp&quot;
<span class="line-removed">- #include &quot;gc/g1/g1DirtyCardQueue.hpp&quot;</span>
  #include &quot;gc/g1/g1EdenRegions.hpp&quot;
  #include &quot;gc/g1/g1EvacFailure.hpp&quot;
  #include &quot;gc/g1/g1EvacStats.hpp&quot;
  #include &quot;gc/g1/g1EvacuationInfo.hpp&quot;
  #include &quot;gc/g1/g1GCPhaseTimes.hpp&quot;
  #include &quot;gc/g1/g1HeapTransition.hpp&quot;
  #include &quot;gc/g1/g1HeapVerifier.hpp&quot;
  #include &quot;gc/g1/g1HRPrinter.hpp&quot;
<span class="line-modified">! #include &quot;gc/g1/g1InCSetState.hpp&quot;</span>
  #include &quot;gc/g1/g1MonitoringSupport.hpp&quot;
  #include &quot;gc/g1/g1SurvivorRegions.hpp&quot;
  #include &quot;gc/g1/g1YCTypes.hpp&quot;
  #include &quot;gc/g1/heapRegionManager.hpp&quot;
  #include &quot;gc/g1/heapRegionSet.hpp&quot;
  #include &quot;gc/g1/heterogeneousHeapRegionManager.hpp&quot;
<span class="line-new-header">--- 29,22 ---</span>
  #include &quot;gc/g1/g1BiasedArray.hpp&quot;
  #include &quot;gc/g1/g1CardTable.hpp&quot;
  #include &quot;gc/g1/g1CollectionSet.hpp&quot;
  #include &quot;gc/g1/g1CollectorState.hpp&quot;
  #include &quot;gc/g1/g1ConcurrentMark.hpp&quot;
  #include &quot;gc/g1/g1EdenRegions.hpp&quot;
  #include &quot;gc/g1/g1EvacFailure.hpp&quot;
  #include &quot;gc/g1/g1EvacStats.hpp&quot;
  #include &quot;gc/g1/g1EvacuationInfo.hpp&quot;
  #include &quot;gc/g1/g1GCPhaseTimes.hpp&quot;
  #include &quot;gc/g1/g1HeapTransition.hpp&quot;
  #include &quot;gc/g1/g1HeapVerifier.hpp&quot;
  #include &quot;gc/g1/g1HRPrinter.hpp&quot;
<span class="line-modified">! #include &quot;gc/g1/g1HeapRegionAttr.hpp&quot;</span>
  #include &quot;gc/g1/g1MonitoringSupport.hpp&quot;
<span class="line-added">+ #include &quot;gc/g1/g1NUMA.hpp&quot;</span>
<span class="line-added">+ #include &quot;gc/g1/g1RedirtyCardsQueue.hpp&quot;</span>
  #include &quot;gc/g1/g1SurvivorRegions.hpp&quot;
  #include &quot;gc/g1/g1YCTypes.hpp&quot;
  #include &quot;gc/g1/heapRegionManager.hpp&quot;
  #include &quot;gc/g1/heapRegionSet.hpp&quot;
  #include &quot;gc/g1/heterogeneousHeapRegionManager.hpp&quot;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 71,17 ***</span>
  class MemoryManager;
  class ObjectClosure;
  class SpaceClosure;
  class CompactibleSpaceClosure;
  class Space;
  class G1CollectionSet;
<span class="line-removed">- class G1CollectorPolicy;</span>
  class G1Policy;
  class G1HotCardCache;
  class G1RemSet;
  class G1YoungRemSetSamplingThread;
<span class="line-removed">- class HeapRegionRemSetIterator;</span>
  class G1ConcurrentMark;
  class G1ConcurrentMarkThread;
  class G1ConcurrentRefine;
  class GenerationCounters;
  class STWGCTimer;
<span class="line-new-header">--- 72,16 ---</span>
  class MemoryManager;
  class ObjectClosure;
  class SpaceClosure;
  class CompactibleSpaceClosure;
  class Space;
<span class="line-added">+ class G1CardTableEntryClosure;</span>
  class G1CollectionSet;
  class G1Policy;
  class G1HotCardCache;
  class G1RemSet;
  class G1YoungRemSetSamplingThread;
  class G1ConcurrentMark;
  class G1ConcurrentMarkThread;
  class G1ConcurrentRefine;
  class GenerationCounters;
  class STWGCTimer;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 128,38 ***</span>
   public:
    virtual void on_commit(uint start_idx, size_t num_regions, bool zero_filled);
  };
  
  class G1CollectedHeap : public CollectedHeap {
<span class="line-removed">-   friend class G1FreeCollectionSetTask;</span>
    friend class VM_CollectForMetadataAllocation;
    friend class VM_G1CollectForAllocation;
    friend class VM_G1CollectFull;
    friend class VMStructs;
    friend class MutatorAllocRegion;
    friend class G1FullCollector;
    friend class G1GCAllocRegion;
    friend class G1HeapVerifier;
  
    // Closures used in implementation.
    friend class G1ParScanThreadState;
    friend class G1ParScanThreadStateSet;
<span class="line-modified">!   friend class G1ParTask;</span>
    friend class G1PLABAllocator;
<span class="line-removed">-   friend class G1PrepareCompactClosure;</span>
  
    // Other related classes.
    friend class HeapRegionClaimer;
  
    // Testing classes.
<span class="line-modified">!   friend class G1CheckCSetFastTableClosure;</span>
  
  private:
    G1YoungRemSetSamplingThread* _young_gen_sampling_thread;
  
    WorkGang* _workers;
<span class="line-removed">-   G1CollectorPolicy* _collector_policy;</span>
    G1CardTable* _card_table;
  
    SoftRefPolicy      _soft_ref_policy;
  
    static size_t _humongous_object_threshold_in_words;
<span class="line-new-header">--- 128,36 ---</span>
   public:
    virtual void on_commit(uint start_idx, size_t num_regions, bool zero_filled);
  };
  
  class G1CollectedHeap : public CollectedHeap {
    friend class VM_CollectForMetadataAllocation;
    friend class VM_G1CollectForAllocation;
    friend class VM_G1CollectFull;
<span class="line-added">+   friend class VM_G1TryInitiateConcMark;</span>
    friend class VMStructs;
    friend class MutatorAllocRegion;
    friend class G1FullCollector;
    friend class G1GCAllocRegion;
    friend class G1HeapVerifier;
  
    // Closures used in implementation.
    friend class G1ParScanThreadState;
    friend class G1ParScanThreadStateSet;
<span class="line-modified">!   friend class G1EvacuateRegionsTask;</span>
    friend class G1PLABAllocator;
  
    // Other related classes.
    friend class HeapRegionClaimer;
  
    // Testing classes.
<span class="line-modified">!   friend class G1CheckRegionAttrTableClosure;</span>
  
  private:
    G1YoungRemSetSamplingThread* _young_gen_sampling_thread;
  
    WorkGang* _workers;
    G1CardTable* _card_table;
  
    SoftRefPolicy      _soft_ref_policy;
  
    static size_t _humongous_object_threshold_in_words;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 193,10 ***</span>
<span class="line-new-header">--- 191,13 ---</span>
    void rebuild_region_sets(bool free_list_only);
  
    // Callback for region mapping changed events.
    G1RegionMappingChangedListener _listener;
  
<span class="line-added">+   // Handle G1 NUMA support.</span>
<span class="line-added">+   G1NUMA* _numa;</span>
<span class="line-added">+ </span>
    // The sequence of all heap regions in the heap.
    HeapRegionManager* _hrm;
  
    // Manages all allocations with regions except humongous object allocations.
    G1Allocator* _allocator;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 204,17 ***</span>
    // Manages all heap verification.
    G1HeapVerifier* _verifier;
  
    // Outside of GC pauses, the number of bytes used in all regions other
    // than the current allocation region(s).
<span class="line-modified">!   size_t _summary_bytes_used;</span>
  
    void increase_used(size_t bytes);
    void decrease_used(size_t bytes);
  
    void set_used(size_t bytes);
  
    // Class that handles archive allocation ranges.
    G1ArchiveAllocator* _archive_allocator;
  
    // GC allocation statistics policy for survivors.
    G1EvacStats _survivor_evac_stats;
<span class="line-new-header">--- 205,21 ---</span>
    // Manages all heap verification.
    G1HeapVerifier* _verifier;
  
    // Outside of GC pauses, the number of bytes used in all regions other
    // than the current allocation region(s).
<span class="line-modified">!   volatile size_t _summary_bytes_used;</span>
  
    void increase_used(size_t bytes);
    void decrease_used(size_t bytes);
  
    void set_used(size_t bytes);
  
<span class="line-added">+   // Number of bytes used in all regions during GC. Typically changed when</span>
<span class="line-added">+   // retiring a GC alloc region.</span>
<span class="line-added">+   size_t _bytes_used_during_gc;</span>
<span class="line-added">+ </span>
    // Class that handles archive allocation ranges.
    G1ArchiveAllocator* _archive_allocator;
  
    // GC allocation statistics policy for survivors.
    G1EvacStats _survivor_evac_stats;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 257,20 ***</span>
    // If not, we can skip a few steps.
    bool _has_humongous_reclaim_candidates;
  
    G1HRPrinter _hr_printer;
  
<span class="line-modified">!   // It decides whether an explicit GC should start a concurrent cycle</span>
<span class="line-modified">!   // instead of doing a STW GC. Currently, a concurrent cycle is</span>
<span class="line-modified">!   // explicitly started if:</span>
<span class="line-modified">!   // (a) cause == _gc_locker and +GCLockerInvokesConcurrent, or</span>
<span class="line-modified">!   // (b) cause == _g1_humongous_allocation</span>
<span class="line-modified">!   // (c) cause == _java_lang_system_gc and +ExplicitGCInvokesConcurrent.</span>
<span class="line-modified">!   // (d) cause == _dcmd_gc_run and +ExplicitGCInvokesConcurrent.</span>
<span class="line-removed">-   // (e) cause == _wb_conc_mark</span>
    bool should_do_concurrent_full_gc(GCCause::Cause cause);
  
    // Return true if should upgrade to full gc after an incremental one.
    bool should_upgrade_to_full_gc(GCCause::Cause cause);
  
    // indicates whether we are in young or mixed GC mode
    G1CollectorState _collector_state;
<span class="line-new-header">--- 262,25 ---</span>
    // If not, we can skip a few steps.
    bool _has_humongous_reclaim_candidates;
  
    G1HRPrinter _hr_printer;
  
<span class="line-modified">!   // Return true if an explicit GC should start a concurrent cycle instead</span>
<span class="line-modified">!   // of doing a STW full GC. A concurrent cycle should be started if:</span>
<span class="line-modified">!   // (a) cause == _g1_humongous_allocation,</span>
<span class="line-modified">!   // (b) cause == _java_lang_system_gc and +ExplicitGCInvokesConcurrent,</span>
<span class="line-modified">!   // (c) cause == _dcmd_gc_run and +ExplicitGCInvokesConcurrent,</span>
<span class="line-modified">!   // (d) cause == _wb_conc_mark,</span>
<span class="line-modified">!   // (e) cause == _g1_periodic_collection and +G1PeriodicGCInvokesConcurrent.</span>
    bool should_do_concurrent_full_gc(GCCause::Cause cause);
  
<span class="line-added">+   // Attempt to start a concurrent cycle with the indicated cause.</span>
<span class="line-added">+   // precondition: should_do_concurrent_full_gc(cause)</span>
<span class="line-added">+   bool try_collect_concurrently(GCCause::Cause cause,</span>
<span class="line-added">+                                 uint gc_counter,</span>
<span class="line-added">+                                 uint old_marking_started_before);</span>
<span class="line-added">+ </span>
    // Return true if should upgrade to full gc after an incremental one.
    bool should_upgrade_to_full_gc(GCCause::Cause cause);
  
    // indicates whether we are in young or mixed GC mode
    G1CollectorState _collector_state;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 355,10 ***</span>
<span class="line-new-header">--- 365,25 ---</span>
      assert_at_safepoint();                                                    \
      assert(Thread::current_or_null() != NULL, &quot;no current thread&quot;);           \
      assert(Thread::current()-&gt;is_VM_thread(), &quot;current thread is not VM thread&quot;); \
    } while (0)
  
<span class="line-added">+ #ifdef ASSERT</span>
<span class="line-added">+ #define assert_used_and_recalculate_used_equal(g1h)                           \</span>
<span class="line-added">+   do {                                                                        \</span>
<span class="line-added">+     size_t cur_used_bytes = g1h-&gt;used();                                      \</span>
<span class="line-added">+     size_t recal_used_bytes = g1h-&gt;recalculate_used();                        \</span>
<span class="line-added">+     assert(cur_used_bytes == recal_used_bytes, &quot;Used(&quot; SIZE_FORMAT &quot;) is not&quot; \</span>
<span class="line-added">+            &quot; same as recalculated used(&quot; SIZE_FORMAT &quot;).&quot;,                    \</span>
<span class="line-added">+            cur_used_bytes, recal_used_bytes);                                 \</span>
<span class="line-added">+   } while (0)</span>
<span class="line-added">+ #else</span>
<span class="line-added">+ #define assert_used_and_recalculate_used_equal(g1h) do {} while(0)</span>
<span class="line-added">+ #endif</span>
<span class="line-added">+ </span>
<span class="line-added">+   const char* young_gc_name() const;</span>
<span class="line-added">+ </span>
    // The young region list.
    G1EdenRegions _eden;
    G1SurvivorRegions _survivor;
  
    STWGCTimer* _gc_timer_stw;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 374,11 ***</span>
    // Try to allocate a single non-humongous HeapRegion sufficient for
    // an allocation of the given word_size. If do_expand is true,
    // attempt to expand the heap if necessary to satisfy the allocation
    // request. &#39;type&#39; takes the type of region to be allocated. (Use constants
    // Old, Eden, Humongous, Survivor defined in HeapRegionType.)
<span class="line-modified">!   HeapRegion* new_region(size_t word_size, HeapRegionType type, bool do_expand);</span>
  
    // Initialize a contiguous set of free regions of length num_regions
    // and starting at index first so that they appear as a single
    // humongous region.
    HeapWord* humongous_obj_allocate_initialize_regions(uint first,
<span class="line-new-header">--- 399,14 ---</span>
    // Try to allocate a single non-humongous HeapRegion sufficient for
    // an allocation of the given word_size. If do_expand is true,
    // attempt to expand the heap if necessary to satisfy the allocation
    // request. &#39;type&#39; takes the type of region to be allocated. (Use constants
    // Old, Eden, Humongous, Survivor defined in HeapRegionType.)
<span class="line-modified">!   HeapRegion* new_region(size_t word_size,</span>
<span class="line-added">+                          HeapRegionType type,</span>
<span class="line-added">+                          bool do_expand,</span>
<span class="line-added">+                          uint node_index = G1NUMA::AnyNodeIndex);</span>
  
    // Initialize a contiguous set of free regions of length num_regions
    // and starting at index first so that they appear as a single
    // humongous region.
    HeapWord* humongous_obj_allocate_initialize_regions(uint first,
</pre>
<hr />
<pre>
<span class="line-old-header">*** 449,19 ***</span>
                                              bool expect_null_mutator_alloc_region);
  
    // These methods are the &quot;callbacks&quot; from the G1AllocRegion class.
  
    // For mutator alloc regions.
<span class="line-modified">!   HeapRegion* new_mutator_alloc_region(size_t word_size, bool force);</span>
    void retire_mutator_alloc_region(HeapRegion* alloc_region,
                                     size_t allocated_bytes);
  
    // For GC alloc regions.
<span class="line-modified">!   bool has_more_regions(InCSetState dest);</span>
<span class="line-modified">!   HeapRegion* new_gc_alloc_region(size_t word_size, InCSetState dest);</span>
    void retire_gc_alloc_region(HeapRegion* alloc_region,
<span class="line-modified">!                               size_t allocated_bytes, InCSetState dest);</span>
  
    // - if explicit_gc is true, the GC is for a System.gc() etc,
    //   otherwise it&#39;s for a failed allocation.
    // - if clear_all_soft_refs is true, all soft references should be
    //   cleared during the GC.
<span class="line-new-header">--- 477,19 ---</span>
                                              bool expect_null_mutator_alloc_region);
  
    // These methods are the &quot;callbacks&quot; from the G1AllocRegion class.
  
    // For mutator alloc regions.
<span class="line-modified">!   HeapRegion* new_mutator_alloc_region(size_t word_size, bool force, uint node_index);</span>
    void retire_mutator_alloc_region(HeapRegion* alloc_region,
                                     size_t allocated_bytes);
  
    // For GC alloc regions.
<span class="line-modified">!   bool has_more_regions(G1HeapRegionAttr dest);</span>
<span class="line-modified">!   HeapRegion* new_gc_alloc_region(size_t word_size, G1HeapRegionAttr dest, uint node_index);</span>
    void retire_gc_alloc_region(HeapRegion* alloc_region,
<span class="line-modified">!                               size_t allocated_bytes, G1HeapRegionAttr dest);</span>
  
    // - if explicit_gc is true, the GC is for a System.gc() etc,
    //   otherwise it&#39;s for a failed allocation.
    // - if clear_all_soft_refs is true, all soft references should be
    //   cleared during the GC.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 510,15 ***</span>
<span class="line-new-header">--- 538,22 ---</span>
    void make_pending_list_reachable();
  
    // Merges the information gathered on a per-thread basis for all worker threads
    // during GC into global variables.
    void merge_per_thread_state_info(G1ParScanThreadStateSet* per_thread_states);
<span class="line-added">+ </span>
<span class="line-added">+   void verify_numa_regions(const char* desc);</span>
<span class="line-added">+ </span>
  public:
    G1YoungRemSetSamplingThread* sampling_thread() const { return _young_gen_sampling_thread; }
  
    WorkGang* workers() const { return _workers; }
  
<span class="line-added">+   // Runs the given AbstractGangTask with the current active workers, returning the</span>
<span class="line-added">+   // total time taken.</span>
<span class="line-added">+   Tickspan run_task(AbstractGangTask* task);</span>
<span class="line-added">+ </span>
    G1Allocator* allocator() {
      return _allocator;
    }
  
    G1HeapVerifier* verifier() {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 530,21 ***</span>
      return _g1mm;
    }
  
    void resize_heap_if_necessary();
  
    // Expand the garbage-first heap by at least the given size (in bytes!).
    // Returns true if the heap was expanded by the requested amount;
    // false otherwise.
    // (Rounds up to a HeapRegion boundary.)
    bool expand(size_t expand_bytes, WorkGang* pretouch_workers = NULL, double* expand_time_ms = NULL);
  
    // Returns the PLAB statistics for a given destination.
<span class="line-modified">!   inline G1EvacStats* alloc_buffer_stats(InCSetState dest);</span>
  
    // Determines PLAB size for a given destination.
<span class="line-modified">!   inline size_t desired_plab_sz(InCSetState dest);</span>
  
    // Do anything common to GC&#39;s.
    void gc_prologue(bool full);
    void gc_epilogue(bool full);
  
<span class="line-new-header">--- 565,24 ---</span>
      return _g1mm;
    }
  
    void resize_heap_if_necessary();
  
<span class="line-added">+   G1NUMA* numa() const { return _numa; }</span>
<span class="line-added">+ </span>
    // Expand the garbage-first heap by at least the given size (in bytes!).
    // Returns true if the heap was expanded by the requested amount;
    // false otherwise.
    // (Rounds up to a HeapRegion boundary.)
    bool expand(size_t expand_bytes, WorkGang* pretouch_workers = NULL, double* expand_time_ms = NULL);
<span class="line-added">+   bool expand_single_region(uint node_index);</span>
  
    // Returns the PLAB statistics for a given destination.
<span class="line-modified">!   inline G1EvacStats* alloc_buffer_stats(G1HeapRegionAttr dest);</span>
  
    // Determines PLAB size for a given destination.
<span class="line-modified">!   inline size_t desired_plab_sz(G1HeapRegionAttr dest);</span>
  
    // Do anything common to GC&#39;s.
    void gc_prologue(bool full);
    void gc_epilogue(bool full);
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 553,39 ***</span>
  
    // Modify the reclaim candidate set and test for presence.
    // These are only valid for starts_humongous regions.
    inline void set_humongous_reclaim_candidate(uint region, bool value);
    inline bool is_humongous_reclaim_candidate(uint region);
  
    // Remove from the reclaim candidate set.  Also remove from the
    // collection set so that later encounters avoid the slow path.
    inline void set_humongous_is_live(oop obj);
  
    // Register the given region to be part of the collection set.
<span class="line-modified">!   inline void register_humongous_region_with_cset(uint index);</span>
<span class="line-modified">!   // Register regions with humongous objects (actually on the start region) in</span>
<span class="line-removed">-   // the in_cset_fast_test table.</span>
<span class="line-removed">-   void register_humongous_regions_with_cset();</span>
    // We register a region with the fast &quot;in collection set&quot; test. We
    // simply set to true the array slot corresponding to this region.
<span class="line-modified">!   void register_young_region_with_cset(HeapRegion* r) {</span>
<span class="line-modified">!     _in_cset_fast_test.set_in_young(r-&gt;hrm_index());</span>
<span class="line-removed">-   }</span>
<span class="line-removed">-   void register_old_region_with_cset(HeapRegion* r) {</span>
<span class="line-removed">-     _in_cset_fast_test.set_in_old(r-&gt;hrm_index());</span>
<span class="line-removed">-   }</span>
<span class="line-removed">-   void register_optional_region_with_cset(HeapRegion* r) {</span>
<span class="line-removed">-     _in_cset_fast_test.set_optional(r-&gt;hrm_index());</span>
    }
<span class="line-modified">!   void clear_in_cset(const HeapRegion* hr) {</span>
<span class="line-modified">!     _in_cset_fast_test.clear(hr);</span>
    }
  
<span class="line-modified">!   void clear_cset_fast_test() {</span>
<span class="line-modified">!     _in_cset_fast_test.clear();</span>
    }
  
    bool is_user_requested_concurrent_full_gc(GCCause::Cause cause);
  
    // This is called at the start of either a concurrent cycle or a Full
    // GC to update the number of old marking cycles started.
    void increment_old_marking_cycles_started();
<span class="line-new-header">--- 591,40 ---</span>
  
    // Modify the reclaim candidate set and test for presence.
    // These are only valid for starts_humongous regions.
    inline void set_humongous_reclaim_candidate(uint region, bool value);
    inline bool is_humongous_reclaim_candidate(uint region);
<span class="line-added">+   inline void set_has_humongous_reclaim_candidate(bool value);</span>
  
    // Remove from the reclaim candidate set.  Also remove from the
    // collection set so that later encounters avoid the slow path.
    inline void set_humongous_is_live(oop obj);
  
    // Register the given region to be part of the collection set.
<span class="line-modified">!   inline void register_humongous_region_with_region_attr(uint index);</span>
<span class="line-modified">! </span>
    // We register a region with the fast &quot;in collection set&quot; test. We
    // simply set to true the array slot corresponding to this region.
<span class="line-modified">!   void register_young_region_with_region_attr(HeapRegion* r) {</span>
<span class="line-modified">!     _region_attr.set_in_young(r-&gt;hrm_index());</span>
    }
<span class="line-modified">!   inline void register_region_with_region_attr(HeapRegion* r);</span>
<span class="line-modified">!   inline void register_old_region_with_region_attr(HeapRegion* r);</span>
<span class="line-added">+   inline void register_optional_region_with_region_attr(HeapRegion* r);</span>
<span class="line-added">+ </span>
<span class="line-added">+   void clear_region_attr(const HeapRegion* hr) {</span>
<span class="line-added">+     _region_attr.clear(hr);</span>
    }
  
<span class="line-modified">!   void clear_region_attr() {</span>
<span class="line-modified">!     _region_attr.clear();</span>
    }
  
<span class="line-added">+   // Verify that the G1RegionAttr remset tracking corresponds to actual remset tracking</span>
<span class="line-added">+   // for all regions.</span>
<span class="line-added">+   void verify_region_attr_remset_update() PRODUCT_RETURN;</span>
<span class="line-added">+ </span>
    bool is_user_requested_concurrent_full_gc(GCCause::Cause cause);
  
    // This is called at the start of either a concurrent cycle or a Full
    // GC to update the number of old marking cycles started.
    void increment_old_marking_cycles_started();
</pre>
<hr />
<pre>
<span class="line-old-header">*** 599,11 ***</span>
    // tighter consistency checking in the method. If concurrent is
    // false, the caller is the inner caller in the nesting (i.e., the
    // Full GC). If concurrent is true, the caller is the outer caller
    // in this nesting (i.e., the concurrent cycle). Further nesting is
    // not currently supported. The end of this call also notifies
<span class="line-modified">!   // the FullGCCount_lock in case a Java thread is waiting for a full</span>
    // GC to happen (e.g., it called System.gc() with
    // +ExplicitGCInvokesConcurrent).
    void increment_old_marking_cycles_completed(bool concurrent);
  
    uint old_marking_cycles_completed() {
<span class="line-new-header">--- 638,11 ---</span>
    // tighter consistency checking in the method. If concurrent is
    // false, the caller is the inner caller in the nesting (i.e., the
    // Full GC). If concurrent is true, the caller is the outer caller
    // in this nesting (i.e., the concurrent cycle). Further nesting is
    // not currently supported. The end of this call also notifies
<span class="line-modified">!   // the G1OldGCCount_lock in case a Java thread is waiting for a full</span>
    // GC to happen (e.g., it called System.gc() with
    // +ExplicitGCInvokesConcurrent).
    void increment_old_marking_cycles_completed(bool concurrent);
  
    uint old_marking_cycles_completed() {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 617,24 ***</span>
  
    // Allocate the highest free region in the reserved heap. This will commit
    // regions as necessary.
    HeapRegion* alloc_highest_free_region();
  
<span class="line-modified">!   // Frees a non-humongous region by initializing its contents and</span>
<span class="line-modified">!   // adding it to the free list that&#39;s passed as a parameter (this is</span>
<span class="line-modified">!   // usually a local list which will be appended to the master free</span>
<span class="line-modified">!   // list later). The used bytes of freed regions are accumulated in</span>
<span class="line-modified">!   // pre_used. If skip_remset is true, the region&#39;s RSet will not be freed</span>
<span class="line-modified">!   // up. If skip_hot_card_cache is true, the region&#39;s hot card cache will not</span>
<span class="line-modified">!   // be freed up. The assumption is that this will be done later.</span>
<span class="line-removed">-   // The locked parameter indicates if the caller has already taken</span>
<span class="line-removed">-   // care of proper synchronization. This may allow some optimizations.</span>
<span class="line-removed">-   void free_region(HeapRegion* hr,</span>
<span class="line-removed">-                    FreeRegionList* free_list,</span>
<span class="line-removed">-                    bool skip_remset,</span>
<span class="line-removed">-                    bool skip_hot_card_cache = false,</span>
<span class="line-removed">-                    bool locked = false);</span>
  
    // It dirties the cards that cover the block so that the post
    // write barrier never queues anything when updating objects on this
    // block. It is assumed (and in fact we assert) that the block
    // belongs to a young region.
<span class="line-new-header">--- 656,17 ---</span>
  
    // Allocate the highest free region in the reserved heap. This will commit
    // regions as necessary.
    HeapRegion* alloc_highest_free_region();
  
<span class="line-modified">!   // Frees a region by resetting its metadata and adding it to the free list</span>
<span class="line-modified">!   // passed as a parameter (this is usually a local list which will be appended</span>
<span class="line-modified">!   // to the master free list later or NULL if free list management is handled</span>
<span class="line-modified">!   // in another way).</span>
<span class="line-modified">!   // Callers must ensure they are the only one calling free on the given region</span>
<span class="line-modified">!   // at the same time.</span>
<span class="line-modified">!   void free_region(HeapRegion* hr, FreeRegionList* free_list);</span>
  
    // It dirties the cards that cover the block so that the post
    // write barrier never queues anything when updating objects on this
    // block. It is assumed (and in fact we assert) that the block
    // belongs to a young region.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 689,11 ***</span>
  
    // For each of the specified MemRegions, uncommit the containing G1 regions
    // which had been allocated by alloc_archive_regions. This should be called
    // rather than fill_archive_regions at JVM init time if the archive file
    // mapping failed, with the same non-overlapping and sorted MemRegion array.
<span class="line-modified">!   void dealloc_archive_regions(MemRegion* range, size_t count, bool is_open);</span>
  
    oop materialize_archived_object(oop obj);
  
  private:
  
<span class="line-new-header">--- 721,11 ---</span>
  
    // For each of the specified MemRegions, uncommit the containing G1 regions
    // which had been allocated by alloc_archive_regions. This should be called
    // rather than fill_archive_regions at JVM init time if the archive file
    // mapping failed, with the same non-overlapping and sorted MemRegion array.
<span class="line-modified">!   void dealloc_archive_regions(MemRegion* range, size_t count);</span>
  
    oop materialize_archived_object(oop obj);
  
  private:
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 723,36 ***</span>
                                  bool*          succeeded,
                                  GCCause::Cause gc_cause);
  
    void wait_for_root_region_scanning();
  
<span class="line-modified">!   // The guts of the incremental collection pause, executed by the vm</span>
<span class="line-modified">!   // thread. It returns false if it is unable to do the collection due</span>
<span class="line-modified">!   // to the GC locker being active, true otherwise</span>
    bool do_collection_pause_at_safepoint(double target_pause_time_ms);
  
<span class="line-modified">!   // Actually do the work of evacuating the collection set.</span>
<span class="line-modified">!   void evacuate_collection_set(G1ParScanThreadStateSet* per_thread_states);</span>
    void evacuate_optional_collection_set(G1ParScanThreadStateSet* per_thread_states);
<span class="line-modified">!   void evacuate_optional_regions(G1ParScanThreadStateSet* per_thread_states, G1OptionalCSet* ocset);</span>
  
<span class="line-modified">!   void pre_evacuate_collection_set();</span>
<span class="line-modified">!   void post_evacuate_collection_set(G1EvacuationInfo&amp; evacuation_info, G1ParScanThreadStateSet* pss);</span>
  
    // Update object copying statistics.
    void record_obj_copy_mem_stats();
  
    // The hot card cache for remembered set insertion optimization.
    G1HotCardCache* _hot_card_cache;
  
    // The g1 remembered set of the heap.
    G1RemSet* _rem_set;
  
<span class="line-removed">-   // A set of cards that cover the objects for which the Rsets should be updated</span>
<span class="line-removed">-   // concurrently after the collection.</span>
<span class="line-removed">-   G1DirtyCardQueueSet _dirty_card_queue_set;</span>
<span class="line-removed">- </span>
    // After a collection pause, convert the regions in the collection set into free
    // regions.
    void free_collection_set(G1CollectionSet* collection_set, G1EvacuationInfo&amp; evacuation_info, const size_t* surviving_young_words);
  
    // Abandon the current collection set without recording policy
<span class="line-new-header">--- 755,51 ---</span>
                                  bool*          succeeded,
                                  GCCause::Cause gc_cause);
  
    void wait_for_root_region_scanning();
  
<span class="line-modified">!   // Perform an incremental collection at a safepoint, possibly</span>
<span class="line-modified">!   // followed by a by-policy upgrade to a full collection.  Returns</span>
<span class="line-modified">!   // false if unable to do the collection due to the GC locker being</span>
<span class="line-added">+   // active, true otherwise.</span>
<span class="line-added">+   // precondition: at safepoint on VM thread</span>
<span class="line-added">+   // precondition: !is_gc_active()</span>
    bool do_collection_pause_at_safepoint(double target_pause_time_ms);
  
<span class="line-modified">!   // Helper for do_collection_pause_at_safepoint, containing the guts</span>
<span class="line-modified">!   // of the incremental collection pause, executed by the vm thread.</span>
<span class="line-added">+   void do_collection_pause_at_safepoint_helper(double target_pause_time_ms);</span>
<span class="line-added">+ </span>
<span class="line-added">+   G1HeapVerifier::G1VerifyType young_collection_verify_type() const;</span>
<span class="line-added">+   void verify_before_young_collection(G1HeapVerifier::G1VerifyType type);</span>
<span class="line-added">+   void verify_after_young_collection(G1HeapVerifier::G1VerifyType type);</span>
<span class="line-added">+ </span>
<span class="line-added">+   void calculate_collection_set(G1EvacuationInfo&amp; evacuation_info, double target_pause_time_ms);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Actually do the work of evacuating the parts of the collection set.</span>
<span class="line-added">+   void evacuate_initial_collection_set(G1ParScanThreadStateSet* per_thread_states);</span>
    void evacuate_optional_collection_set(G1ParScanThreadStateSet* per_thread_states);
<span class="line-modified">! private:</span>
<span class="line-added">+   // Evacuate the next set of optional regions.</span>
<span class="line-added">+   void evacuate_next_optional_regions(G1ParScanThreadStateSet* per_thread_states);</span>
  
<span class="line-modified">! public:</span>
<span class="line-modified">!   void pre_evacuate_collection_set(G1EvacuationInfo&amp; evacuation_info, G1ParScanThreadStateSet* pss);</span>
<span class="line-added">+   void post_evacuate_collection_set(G1EvacuationInfo&amp; evacuation_info,</span>
<span class="line-added">+                                     G1RedirtyCardsQueueSet* rdcqs,</span>
<span class="line-added">+                                     G1ParScanThreadStateSet* pss);</span>
  
<span class="line-added">+   void expand_heap_after_young_collection();</span>
    // Update object copying statistics.
    void record_obj_copy_mem_stats();
  
    // The hot card cache for remembered set insertion optimization.
    G1HotCardCache* _hot_card_cache;
  
    // The g1 remembered set of the heap.
    G1RemSet* _rem_set;
  
    // After a collection pause, convert the regions in the collection set into free
    // regions.
    void free_collection_set(G1CollectionSet* collection_set, G1EvacuationInfo&amp; evacuation_info, const size_t* surviving_young_words);
  
    // Abandon the current collection set without recording policy
</pre>
<hr />
<pre>
<span class="line-old-header">*** 774,21 ***</span>
  
    EvacuationFailedInfo* _evacuation_failed_info_array;
  
    // Failed evacuations cause some logical from-space objects to have
    // forwarding pointers to themselves.  Reset them.
<span class="line-modified">!   void remove_self_forwarding_pointers();</span>
  
    // Restore the objects in the regions in the collection set after an
    // evacuation failure.
<span class="line-modified">!   void restore_after_evac_failure();</span>
  
    PreservedMarksSet _preserved_marks_set;
  
    // Preserve the mark of &quot;obj&quot;, if necessary, in preparation for its mark
    // word being overwritten with a self-forwarding-pointer.
<span class="line-modified">!   void preserve_mark_during_evac_failure(uint worker_id, oop obj, markOop m);</span>
  
  #ifndef PRODUCT
    // Support for forcing evacuation failures. Analogous to
    // PromotionFailureALot for the other collectors.
  
<span class="line-new-header">--- 821,21 ---</span>
  
    EvacuationFailedInfo* _evacuation_failed_info_array;
  
    // Failed evacuations cause some logical from-space objects to have
    // forwarding pointers to themselves.  Reset them.
<span class="line-modified">!   void remove_self_forwarding_pointers(G1RedirtyCardsQueueSet* rdcqs);</span>
  
    // Restore the objects in the regions in the collection set after an
    // evacuation failure.
<span class="line-modified">!   void restore_after_evac_failure(G1RedirtyCardsQueueSet* rdcqs);</span>
  
    PreservedMarksSet _preserved_marks_set;
  
    // Preserve the mark of &quot;obj&quot;, if necessary, in preparation for its mark
    // word being overwritten with a self-forwarding-pointer.
<span class="line-modified">!   void preserve_mark_during_evac_failure(uint worker_id, oop obj, markWord m);</span>
  
  #ifndef PRODUCT
    // Support for forcing evacuation failures. Analogous to
    // PromotionFailureALot for the other collectors.
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 906,17 ***</span>
  
    RefToScanQueue *task_queue(uint i) const;
  
    uint num_task_queues() const;
  
<span class="line-modified">!   // A set of cards where updates happened during the GC</span>
<span class="line-removed">-   G1DirtyCardQueueSet&amp; dirty_card_queue_set() { return _dirty_card_queue_set; }</span>
<span class="line-removed">- </span>
<span class="line-removed">-   // Create a G1CollectedHeap with the specified policy.</span>
    // Must call the initialize method afterwards.
    // May not return if something goes wrong.
<span class="line-modified">!   G1CollectedHeap(G1CollectorPolicy* policy);</span>
  
  private:
    jint initialize_concurrent_refinement();
    jint initialize_young_gen_sampling_thread();
  public:
<span class="line-new-header">--- 953,14 ---</span>
  
    RefToScanQueue *task_queue(uint i) const;
  
    uint num_task_queues() const;
  
<span class="line-modified">!   // Create a G1CollectedHeap.</span>
    // Must call the initialize method afterwards.
    // May not return if something goes wrong.
<span class="line-modified">!   G1CollectedHeap();</span>
  
  private:
    jint initialize_concurrent_refinement();
    jint initialize_young_gen_sampling_thread();
  public:
</pre>
<hr />
<pre>
<span class="line-old-header">*** 927,13 ***</span>
  
    virtual void stop();
    virtual void safepoint_synchronize_begin();
    virtual void safepoint_synchronize_end();
  
<span class="line-removed">-   // Return the (conservative) maximum heap alignment for any G1 heap</span>
<span class="line-removed">-   static size_t conservative_max_heap_alignment();</span>
<span class="line-removed">- </span>
    // Does operations required after initialization has been done.
    void post_initialize();
  
    // Initialize weak reference processing.
    void ref_processing_init();
<span class="line-new-header">--- 971,10 ---</span>
</pre>
<hr />
<pre>
<span class="line-old-header">*** 959,12 ***</span>
    HeapRegionManager* hrm() const { return _hrm; }
  
    const G1CollectionSet* collection_set() const { return &amp;_collection_set; }
    G1CollectionSet* collection_set() { return &amp;_collection_set; }
  
<span class="line-removed">-   virtual CollectorPolicy* collector_policy() const;</span>
<span class="line-removed">- </span>
    virtual SoftRefPolicy* soft_ref_policy();
  
    virtual void initialize_serviceability();
    virtual MemoryUsage memory_usage();
    virtual GrowableArray&lt;GCMemoryManager*&gt; memory_managers();
<span class="line-new-header">--- 1000,10 ---</span>
</pre>
<hr />
<pre>
<span class="line-old-header">*** 972,14 ***</span>
  
    // Try to minimize the remembered set.
    void scrub_rem_set();
  
    // Apply the given closure on all cards in the Hot Card Cache, emptying it.
<span class="line-modified">!   void iterate_hcc_closure(G1CardTableEntryClosure* cl, uint worker_i);</span>
<span class="line-removed">- </span>
<span class="line-removed">-   // Apply the given closure on all cards in the Dirty Card Queue Set, emptying it.</span>
<span class="line-removed">-   void iterate_dirty_card_closure(G1CardTableEntryClosure* cl, uint worker_i);</span>
  
    // The shared block offset table array.
    G1BlockOffsetTable* bot() const { return _bot; }
  
    // Reference Processing accessors
<span class="line-new-header">--- 1011,11 ---</span>
  
    // Try to minimize the remembered set.
    void scrub_rem_set();
  
    // Apply the given closure on all cards in the Hot Card Cache, emptying it.
<span class="line-modified">!   void iterate_hcc_closure(G1CardTableEntryClosure* cl, uint worker_id);</span>
  
    // The shared block offset table array.
    G1BlockOffsetTable* bot() const { return _bot; }
  
    // Reference Processing accessors
</pre>
<hr />
<pre>
<span class="line-old-header">*** 991,10 ***</span>
<span class="line-new-header">--- 1027,11 ---</span>
  
    // The Concurrent Marking reference processor...
    ReferenceProcessor* ref_processor_cm() const { return _ref_processor_cm; }
  
    size_t unused_committed_regions_in_bytes() const;
<span class="line-added">+ </span>
    virtual size_t capacity() const;
    virtual size_t used() const;
    // This should be called when we&#39;re not holding the heap lock. The
    // result might be a bit inaccurate.
    size_t used_unlocked() const;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1059,14 ***</span>
    // Perform a collection of the heap; intended for use in implementing
    // &quot;System.gc&quot;.  This probably implies as full a collection as the
    // &quot;CollectedHeap&quot; supports.
    virtual void collect(GCCause::Cause cause);
  
<span class="line-modified">!   // Perform a collection of the heap with the given cause; if the VM operation</span>
<span class="line-removed">-   // fails to execute for any reason, retry only if retry_on_gc_failure is set.</span>
    // Returns whether this collection actually executed.
<span class="line-modified">!   bool try_collect(GCCause::Cause cause, bool retry_on_gc_failure);</span>
  
    // True iff an evacuation has failed in the most-recent collection.
    bool evacuation_failed() { return _evacuation_failed; }
  
    void remove_from_old_sets(const uint old_regions_removed, const uint humongous_regions_removed);
<span class="line-new-header">--- 1096,13 ---</span>
    // Perform a collection of the heap; intended for use in implementing
    // &quot;System.gc&quot;.  This probably implies as full a collection as the
    // &quot;CollectedHeap&quot; supports.
    virtual void collect(GCCause::Cause cause);
  
<span class="line-modified">!   // Perform a collection of the heap with the given cause.</span>
    // Returns whether this collection actually executed.
<span class="line-modified">!   bool try_collect(GCCause::Cause cause);</span>
  
    // True iff an evacuation has failed in the most-recent collection.
    bool evacuation_failed() { return _evacuation_failed; }
  
    void remove_from_old_sets(const uint old_regions_removed, const uint humongous_regions_removed);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1090,15 ***</span>
  
   private:
    // This array is used for a quick test on whether a reference points into
    // the collection set or not. Each of the array&#39;s elements denotes whether the
    // corresponding region is in the collection set or not.
<span class="line-modified">!   G1InCSetStateFastTestBiasedMappedArray _in_cset_fast_test;</span>
  
   public:
  
<span class="line-modified">!   inline InCSetState in_cset_state(const oop obj);</span>
  
    // Return &quot;TRUE&quot; iff the given object address is in the reserved
    // region of g1.
    bool is_in_g1_reserved(const void* p) const {
      return _hrm-&gt;reserved().contains(p);
<span class="line-new-header">--- 1126,16 ---</span>
  
   private:
    // This array is used for a quick test on whether a reference points into
    // the collection set or not. Each of the array&#39;s elements denotes whether the
    // corresponding region is in the collection set or not.
<span class="line-modified">!   G1HeapRegionAttrBiasedMappedArray _region_attr;</span>
  
   public:
  
<span class="line-modified">!   inline G1HeapRegionAttr region_attr(const void* obj) const;</span>
<span class="line-added">+   inline G1HeapRegionAttr region_attr(uint idx) const;</span>
  
    // Return &quot;TRUE&quot; iff the given object address is in the reserved
    // region of g1.
    bool is_in_g1_reserved(const void* p) const {
      return _hrm-&gt;reserved().contains(p);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1108,26 ***</span>
    // reserved for the heap
    MemRegion g1_reserved() const {
      return _hrm-&gt;reserved();
    }
  
<span class="line-modified">!   virtual bool is_in_closed_subset(const void* p) const;</span>
  
<span class="line-modified">!   G1HotCardCache* g1_hot_card_cache() const { return _hot_card_cache; }</span>
  
    G1CardTable* card_table() const {
      return _card_table;
    }
  
    // Iteration functions.
  
    // Iterate over all objects, calling &quot;cl.do_object&quot; on each.
    virtual void object_iterate(ObjectClosure* cl);
  
<span class="line-modified">!   virtual void safe_object_iterate(ObjectClosure* cl) {</span>
<span class="line-modified">!     object_iterate(cl);</span>
<span class="line-removed">-   }</span>
  
    // Iterate over heap regions, in address order, terminating the
    // iteration early if the &quot;do_heap_region&quot; method returns &quot;true&quot;.
    void heap_region_iterate(HeapRegionClosure* blk) const;
  
<span class="line-new-header">--- 1145,35 ---</span>
    // reserved for the heap
    MemRegion g1_reserved() const {
      return _hrm-&gt;reserved();
    }
  
<span class="line-modified">!   MemRegion reserved_region() const {</span>
<span class="line-added">+     return _reserved;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   HeapWord* base() const {</span>
<span class="line-added">+     return _reserved.start();</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   bool is_in_reserved(const void* addr) const {</span>
<span class="line-added">+     return _reserved.contains(addr);</span>
<span class="line-added">+   }</span>
  
<span class="line-modified">!   G1HotCardCache* hot_card_cache() const { return _hot_card_cache; }</span>
  
    G1CardTable* card_table() const {
      return _card_table;
    }
  
    // Iteration functions.
  
    // Iterate over all objects, calling &quot;cl.do_object&quot; on each.
    virtual void object_iterate(ObjectClosure* cl);
  
<span class="line-modified">!   // Keep alive an object that was loaded with AS_NO_KEEPALIVE.</span>
<span class="line-modified">!   virtual void keep_alive(oop obj);</span>
  
    // Iterate over heap regions, in address order, terminating the
    // iteration early if the &quot;do_heap_region&quot; method returns &quot;true&quot;.
    void heap_region_iterate(HeapRegionClosure* blk) const;
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1156,18 ***</span>
                                                    uint worker_id) const;
  
    void heap_region_par_iterate_from_start(HeapRegionClosure* cl,
                                            HeapRegionClaimer* hrclaimer) const;
  
<span class="line-modified">!   // Iterate over the regions (if any) in the current collection set.</span>
<span class="line-modified">!   void collection_set_iterate(HeapRegionClosure* blk);</span>
<span class="line-modified">! </span>
<span class="line-modified">!   // Iterate over the regions (if any) in the current collection set. Starts the</span>
<span class="line-modified">!   // iteration over the entire collection set so that the start regions of a given</span>
<span class="line-modified">!   // worker id over the set active_workers are evenly spread across the set of</span>
<span class="line-modified">!   // collection set regions.</span>
<span class="line-modified">!   void collection_set_iterate_from(HeapRegionClosure *blk, uint worker_id);</span>
  
    // Returns the HeapRegion that contains addr. addr must not be NULL.
    template &lt;class T&gt;
    inline HeapRegion* heap_region_containing(const T addr) const;
  
<span class="line-new-header">--- 1202,28 ---</span>
                                                    uint worker_id) const;
  
    void heap_region_par_iterate_from_start(HeapRegionClosure* cl,
                                            HeapRegionClaimer* hrclaimer) const;
  
<span class="line-modified">!   // Iterate over all regions in the collection set in parallel.</span>
<span class="line-modified">!   void collection_set_par_iterate_all(HeapRegionClosure* cl,</span>
<span class="line-modified">!                                       HeapRegionClaimer* hr_claimer,</span>
<span class="line-modified">!                                       uint worker_id);</span>
<span class="line-modified">! </span>
<span class="line-modified">!   // Iterate over all regions currently in the current collection set.</span>
<span class="line-modified">!   void collection_set_iterate_all(HeapRegionClosure* blk);</span>
<span class="line-modified">! </span>
<span class="line-added">+   // Iterate over the regions in the current increment of the collection set.</span>
<span class="line-added">+   // Starts the iteration so that the start regions of a given worker id over the</span>
<span class="line-added">+   // set active_workers are evenly spread across the set of collection set regions</span>
<span class="line-added">+   // to be iterated.</span>
<span class="line-added">+   // The variant with the HeapRegionClaimer guarantees that the closure will be</span>
<span class="line-added">+   // applied to a particular region exactly once.</span>
<span class="line-added">+   void collection_set_iterate_increment_from(HeapRegionClosure *blk, uint worker_id) {</span>
<span class="line-added">+     collection_set_iterate_increment_from(blk, NULL, worker_id);</span>
<span class="line-added">+   }</span>
<span class="line-added">+   void collection_set_iterate_increment_from(HeapRegionClosure *blk, HeapRegionClaimer* hr_claimer, uint worker_id);</span>
  
    // Returns the HeapRegion that contains addr. addr must not be NULL.
    template &lt;class T&gt;
    inline HeapRegion* heap_region_containing(const T addr) const;
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1187,15 ***</span>
  
    // Returns the address of the start of the &quot;block&quot; that contains the
    // address &quot;addr&quot;.  We say &quot;blocks&quot; instead of &quot;object&quot; since some heaps
    // may not pack objects densely; a chunk may either be an object or a
    // non-object.
<span class="line-modified">!   virtual HeapWord* block_start(const void* addr) const;</span>
  
    // Requires &quot;addr&quot; to be the start of a block, and returns &quot;TRUE&quot; iff
    // the block is an object.
<span class="line-modified">!   virtual bool block_is_obj(const HeapWord* addr) const;</span>
  
    // Section on thread-local allocation buffers (TLABs)
    // See CollectedHeap for semantics.
  
    bool supports_tlab_allocation() const;
<span class="line-new-header">--- 1243,15 ---</span>
  
    // Returns the address of the start of the &quot;block&quot; that contains the
    // address &quot;addr&quot;.  We say &quot;blocks&quot; instead of &quot;object&quot; since some heaps
    // may not pack objects densely; a chunk may either be an object or a
    // non-object.
<span class="line-modified">!   HeapWord* block_start(const void* addr) const;</span>
  
    // Requires &quot;addr&quot; to be the start of a block, and returns &quot;TRUE&quot; iff
    // the block is an object.
<span class="line-modified">!   bool block_is_obj(const HeapWord* addr) const;</span>
  
    // Section on thread-local allocation buffers (TLABs)
    // See CollectedHeap for semantics.
  
    bool supports_tlab_allocation() const;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1242,11 ***</span>
<span class="line-new-header">--- 1298,15 ---</span>
    // add appropriate methods for any other surv rate groups
  
    const G1SurvivorRegions* survivor() const { return &amp;_survivor; }
  
    uint eden_regions_count() const { return _eden.length(); }
<span class="line-added">+   uint eden_regions_count(uint node_index) const { return _eden.regions_on_node(node_index); }</span>
    uint survivor_regions_count() const { return _survivor.length(); }
<span class="line-added">+   uint survivor_regions_count(uint node_index) const { return _survivor.regions_on_node(node_index); }</span>
<span class="line-added">+   size_t eden_regions_used_bytes() const { return _eden.used_bytes(); }</span>
<span class="line-added">+   size_t survivor_regions_used_bytes() const { return _survivor.used_bytes(); }</span>
    uint young_regions_count() const { return _eden.length() + _survivor.length(); }
    uint old_regions_count() const { return _old_set.length(); }
    uint archive_regions_count() const { return _archive_set.length(); }
    uint humongous_regions_count() const { return _humongous_set.length(); }
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1311,10 ***</span>
<span class="line-new-header">--- 1371,16 ---</span>
    virtual void register_nmethod(nmethod* nm);
  
    // Unregister the given nmethod from the G1 heap.
    virtual void unregister_nmethod(nmethod* nm);
  
<span class="line-added">+   // No nmethod flushing needed.</span>
<span class="line-added">+   virtual void flush_nmethod(nmethod* nm) {}</span>
<span class="line-added">+ </span>
<span class="line-added">+   // No nmethod verification implemented.</span>
<span class="line-added">+   virtual void verify_nmethod(nmethod* nm) {}</span>
<span class="line-added">+ </span>
    // Free up superfluous code root memory.
    void purge_code_root_memory();
  
    // Rebuild the strong code root lists for each region
    // after a full GC.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1327,11 ***</span>
  
    // Performs cleaning of data structures after class unloading.
    void complete_cleaning(BoolObjectClosure* is_alive, bool class_unloading_occurred);
  
    // Redirty logged cards in the refinement queue.
<span class="line-modified">!   void redirty_logged_cards();</span>
    // Verification
  
    // Deduplicate the string
    virtual void deduplicate_string(oop str);
  
<span class="line-new-header">--- 1393,12 ---</span>
  
    // Performs cleaning of data structures after class unloading.
    void complete_cleaning(BoolObjectClosure* is_alive, bool class_unloading_occurred);
  
    // Redirty logged cards in the refinement queue.
<span class="line-modified">!   void redirty_logged_cards(G1RedirtyCardsQueueSet* rdcqs);</span>
<span class="line-added">+ </span>
    // Verification
  
    // Deduplicate the string
    virtual void deduplicate_string(oop str);
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1395,37 ***</span>
  
    // The following two methods are helpful for debugging RSet issues.
    void print_cset_rsets() PRODUCT_RETURN;
    void print_all_rsets() PRODUCT_RETURN;
  
    size_t pending_card_num();
  };
  
  class G1ParEvacuateFollowersClosure : public VoidClosure {
  private:
    double _start_term;
    double _term_time;
    size_t _term_attempts;
  
    void start_term_time() { _term_attempts++; _start_term = os::elapsedTime(); }
<span class="line-modified">!   void end_term_time() { _term_time += os::elapsedTime() - _start_term; }</span>
  protected:
    G1CollectedHeap*              _g1h;
    G1ParScanThreadState*         _par_scan_state;
    RefToScanQueueSet*            _queues;
<span class="line-modified">!   ParallelTaskTerminator*       _terminator;</span>
    G1GCPhaseTimes::GCParPhases   _phase;
  
    G1ParScanThreadState*   par_scan_state() { return _par_scan_state; }
    RefToScanQueueSet*      queues()         { return _queues; }
<span class="line-modified">!   ParallelTaskTerminator* terminator()     { return _terminator; }</span>
  
  public:
    G1ParEvacuateFollowersClosure(G1CollectedHeap* g1h,
                                  G1ParScanThreadState* par_scan_state,
                                  RefToScanQueueSet* queues,
<span class="line-modified">!                                 ParallelTaskTerminator* terminator,</span>
                                  G1GCPhaseTimes::GCParPhases phase)
      : _start_term(0.0), _term_time(0.0), _term_attempts(0),
        _g1h(g1h), _par_scan_state(par_scan_state),
        _queues(queues), _terminator(terminator), _phase(phase) {}
  
<span class="line-new-header">--- 1462,40 ---</span>
  
    // The following two methods are helpful for debugging RSet issues.
    void print_cset_rsets() PRODUCT_RETURN;
    void print_all_rsets() PRODUCT_RETURN;
  
<span class="line-added">+   // Used to print information about locations in the hs_err file.</span>
<span class="line-added">+   virtual bool print_location(outputStream* st, void* addr) const;</span>
<span class="line-added">+ </span>
    size_t pending_card_num();
  };
  
  class G1ParEvacuateFollowersClosure : public VoidClosure {
  private:
    double _start_term;
    double _term_time;
    size_t _term_attempts;
  
    void start_term_time() { _term_attempts++; _start_term = os::elapsedTime(); }
<span class="line-modified">!   void end_term_time() { _term_time += (os::elapsedTime() - _start_term); }</span>
  protected:
    G1CollectedHeap*              _g1h;
    G1ParScanThreadState*         _par_scan_state;
    RefToScanQueueSet*            _queues;
<span class="line-modified">!   TaskTerminator*               _terminator;</span>
    G1GCPhaseTimes::GCParPhases   _phase;
  
    G1ParScanThreadState*   par_scan_state() { return _par_scan_state; }
    RefToScanQueueSet*      queues()         { return _queues; }
<span class="line-modified">!   TaskTerminator*         terminator()     { return _terminator; }</span>
  
  public:
    G1ParEvacuateFollowersClosure(G1CollectedHeap* g1h,
                                  G1ParScanThreadState* par_scan_state,
                                  RefToScanQueueSet* queues,
<span class="line-modified">!                                 TaskTerminator* terminator,</span>
                                  G1GCPhaseTimes::GCParPhases phase)
      : _start_term(0.0), _term_time(0.0), _term_attempts(0),
        _g1h(g1h), _par_scan_state(par_scan_state),
        _queues(queues), _terminator(terminator), _phase(phase) {}
  
</pre>
<center><a href="g1CollectedHeap.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.inline.hpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>