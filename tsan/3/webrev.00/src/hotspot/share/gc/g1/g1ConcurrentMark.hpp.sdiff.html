<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1ConcurrentMark.hpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1ConcurrentMark.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1ConcurrentMark.inline.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1ConcurrentMark.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2001, 2019, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_GC_G1_G1CONCURRENTMARK_HPP
 26 #define SHARE_GC_G1_G1CONCURRENTMARK_HPP
 27 
 28 #include &quot;gc/g1/g1ConcurrentMarkBitMap.hpp&quot;
 29 #include &quot;gc/g1/g1ConcurrentMarkObjArrayProcessor.hpp&quot;
 30 #include &quot;gc/g1/g1HeapVerifier.hpp&quot;
 31 #include &quot;gc/g1/g1RegionMarkStatsCache.hpp&quot;
 32 #include &quot;gc/g1/heapRegionSet.hpp&quot;

 33 #include &quot;gc/shared/taskqueue.hpp&quot;


 34 #include &quot;memory/allocation.hpp&quot;
 35 #include &quot;utilities/compilerWarnings.hpp&quot;
 36 
 37 class ConcurrentGCTimer;
 38 class G1ConcurrentMarkThread;
 39 class G1CollectedHeap;
 40 class G1CMOopClosure;
 41 class G1CMTask;
 42 class G1ConcurrentMark;
 43 class G1OldTracer;
 44 class G1RegionToSpaceMapper;
 45 class G1SurvivorRegions;

 46 
 47 PRAGMA_DIAG_PUSH
 48 // warning C4522: multiple assignment operators specified
 49 PRAGMA_DISABLE_MSVC_WARNING(4522)
 50 
 51 // This is a container class for either an oop or a continuation address for
 52 // mark stack entries. Both are pushed onto the mark stack.
 53 class G1TaskQueueEntry {
 54 private:
 55   void* _holder;
 56 
 57   static const uintptr_t ArraySliceBit = 1;
 58 
 59   G1TaskQueueEntry(oop obj) : _holder(obj) {
 60     assert(_holder != NULL, &quot;Not allowed to set NULL task queue element&quot;);
 61   }
 62   G1TaskQueueEntry(HeapWord* addr) : _holder((void*)((uintptr_t)addr | ArraySliceBit)) { }
 63 public:
 64   G1TaskQueueEntry(const G1TaskQueueEntry&amp; other) { _holder = other._holder; }
 65   G1TaskQueueEntry() : _holder(NULL) { }
</pre>
<hr />
<pre>
204   // Return whether the chunk list is empty. Racy due to unsynchronized access to
205   // _chunk_list.
206   bool is_empty() const { return _chunk_list == NULL; }
207 
208   size_t capacity() const  { return _chunk_capacity; }
209 
210   // Expand the stack, typically in response to an overflow condition
211   void expand();
212 
213   // Return the approximate number of oops on this mark stack. Racy due to
214   // unsynchronized access to _chunks_in_chunk_list.
215   size_t size() const { return _chunks_in_chunk_list * EntriesPerChunk; }
216 
217   void set_empty();
218 
219   // Apply Fn to every oop on the mark stack. The mark stack must not
220   // be modified while iterating.
221   template&lt;typename Fn&gt; void iterate(Fn fn) const PRODUCT_RETURN;
222 };
223 
<span class="line-modified">224 // Root Regions are regions that contain objects from nTAMS to top. These are roots</span>
<span class="line-modified">225 // for marking, i.e. their referenced objects must be kept alive to maintain the</span>
226 // SATB invariant.
<span class="line-modified">227 // We could scan and mark them through during the initial-mark pause, but for</span>

228 // pause time reasons we move this work to the concurrent phase.
229 // We need to complete this procedure before the next GC because it might determine
230 // that some of these &quot;root objects&quot; are dead, potentially dropping some required
231 // references.
<span class="line-modified">232 // Root regions comprise of the complete contents of survivor regions, and any</span>
<span class="line-modified">233 // objects copied into old gen during GC.</span>
<span class="line-modified">234 class G1CMRootRegions {</span>
<span class="line-modified">235   HeapRegion** _root_regions;</span>

236   size_t const _max_regions;
237 
238   volatile size_t _num_root_regions; // Actual number of root regions.
239 
240   volatile size_t _claimed_root_regions; // Number of root regions currently claimed.
241 
242   volatile bool _scan_in_progress;
243   volatile bool _should_abort;
244 
245   void notify_scan_done();
246 
247 public:
<span class="line-modified">248   G1CMRootRegions(uint const max_regions);</span>
<span class="line-modified">249   ~G1CMRootRegions();</span>
250 
251   // Reset the data structure to allow addition of new root regions.
252   void reset();
253 
<span class="line-modified">254   void add(HeapRegion* hr);</span>
255 
256   // Reset the claiming / scanning of the root regions.
257   void prepare_for_scan();
258 
259   // Forces get_next() to return NULL so that the iteration aborts early.
260   void abort() { _should_abort = true; }
261 
262   // Return true if the CM thread are actively scanning root regions,
263   // false otherwise.
264   bool scan_in_progress() { return _scan_in_progress; }
265 
<span class="line-modified">266   // Claim the next root region to scan atomically, or return NULL if</span>
267   // all have been claimed.
<span class="line-modified">268   HeapRegion* claim_next();</span>
269 
270   // The number of root regions to scan.
271   uint num_root_regions() const;
272 
273   void cancel_scan();
274 
275   // Flag that we&#39;re done with root region scanning and notify anyone
276   // who&#39;s waiting on it. If aborted is false, assume that all regions
277   // have been claimed.
278   void scan_finished();
279 
280   // If CM threads are still scanning root regions, wait until they
281   // are done. Return true if we had to wait, false otherwise.
282   bool wait_until_scan_finished();
283 };
284 
285 // This class manages data structures and methods for doing liveness analysis in
286 // G1&#39;s concurrent cycle.
287 class G1ConcurrentMark : public CHeapObj&lt;mtGC&gt; {
288   friend class G1ConcurrentMarkThread;
</pre>
<hr />
<pre>
292   friend class G1CMDrainMarkingStackClosure;
293   friend class G1CMBitMapClosure;
294   friend class G1CMConcurrentMarkingTask;
295   friend class G1CMRemarkTask;
296   friend class G1CMTask;
297 
298   G1ConcurrentMarkThread* _cm_thread;     // The thread doing the work
299   G1CollectedHeap*        _g1h;           // The heap
300   bool                    _completed_initialization; // Set to true when initialization is complete
301 
302   // Concurrent marking support structures
303   G1CMBitMap              _mark_bitmap_1;
304   G1CMBitMap              _mark_bitmap_2;
305   G1CMBitMap*             _prev_mark_bitmap; // Completed mark bitmap
306   G1CMBitMap*             _next_mark_bitmap; // Under-construction mark bitmap
307 
308   // Heap bounds
309   MemRegion const         _heap;
310 
311   // Root region tracking and claiming
<span class="line-modified">312   G1CMRootRegions         _root_regions;</span>
313 
314   // For grey objects
315   G1CMMarkStack           _global_mark_stack; // Grey objects behind global finger
316   HeapWord* volatile      _finger;            // The global finger, region aligned,
317                                               // always pointing to the end of the
318                                               // last claimed region
319 
320   uint                    _worker_id_offset;
321   uint                    _max_num_tasks;    // Maximum number of marking tasks
322   uint                    _num_active_tasks; // Number of tasks currently active
323   G1CMTask**              _tasks;            // Task queue array (max_worker_id length)
324 
325   G1CMTaskQueueSet*       _task_queues; // Task queue set
326   TaskTerminator          _terminator;  // For termination
327 
328   // Two sync barriers that are used to synchronize tasks when an
329   // overflow occurs. The algorithm is the following. All tasks enter
330   // the first one to ensure that they have all stopped manipulating
331   // the global data structures. After they exit it, they re-initialize
332   // their data structures and task 0 re-initializes the global data
</pre>
<hr />
<pre>
392   void reset();
393 
394   // Resets all the marking data structures. Called when we have to restart
395   // marking or when marking completes (via set_non_marking_state below).
396   void reset_marking_for_restart();
397 
398   // We do this after we&#39;re done with marking so that the marking data
399   // structures are initialized to a sensible and predictable state.
400   void reset_at_marking_complete();
401 
402   // Called to indicate how many threads are currently active.
403   void set_concurrency(uint active_tasks);
404 
405   // Should be called to indicate which phase we&#39;re in (concurrent
406   // mark or remark) and how many threads are currently active.
407   void set_concurrency_and_phase(uint active_tasks, bool concurrent);
408 
409   // Prints all gathered CM-related statistics
410   void print_stats();
411 
<span class="line-modified">412   HeapWord*               finger()           { return _finger;   }</span>
<span class="line-modified">413   bool                    concurrent()       { return _concurrent; }</span>
<span class="line-modified">414   uint                    active_tasks()     { return _num_active_tasks; }</span>
<span class="line-modified">415   ParallelTaskTerminator* terminator() const { return _terminator.terminator(); }</span>
416 
417   // Claims the next available region to be scanned by a marking
418   // task/thread. It might return NULL if the next region is empty or
419   // we have run out of regions. In the latter case, out_of_regions()
420   // determines whether we&#39;ve really run out of regions or the task
421   // should call claim_region() again. This might seem a bit
422   // awkward. Originally, the code was written so that claim_region()
423   // either successfully returned with a non-empty region or there
424   // were no more regions to be claimed. The problem with this was
425   // that, in certain circumstances, it iterated over large chunks of
426   // the heap finding only empty regions and, while it was working, it
427   // was preventing the calling task to call its regular clock
428   // method. So, this way, each task will spend very little time in
429   // claim_region() and is allowed to call the regular clock method
430   // frequently.
431   HeapRegion* claim_region(uint worker_id);
432 
433   // Determines whether we&#39;ve run out of regions to scan. Note that
434   // the finger can point past the heap end in case the heap was expanded
435   // to satisfy an allocation without doing a GC. This is fine, because all
</pre>
<hr />
<pre>
483   void clear_statistics_in_region(uint region_idx);
484   // Notification for eagerly reclaimed regions to clean up.
485   void humongous_object_eagerly_reclaimed(HeapRegion* r);
486   // Manipulation of the global mark stack.
487   // The push and pop operations are used by tasks for transfers
488   // between task-local queues and the global mark stack.
489   bool mark_stack_push(G1TaskQueueEntry* arr) {
490     if (!_global_mark_stack.par_push_chunk(arr)) {
491       set_has_overflown();
492       return false;
493     }
494     return true;
495   }
496   bool mark_stack_pop(G1TaskQueueEntry* arr) {
497     return _global_mark_stack.par_pop_chunk(arr);
498   }
499   size_t mark_stack_size() const                { return _global_mark_stack.size(); }
500   size_t partial_mark_stack_size_target() const { return _global_mark_stack.capacity() / 3; }
501   bool mark_stack_empty() const                 { return _global_mark_stack.is_empty(); }
502 
<span class="line-modified">503   G1CMRootRegions* root_regions() { return &amp;_root_regions; }</span>
504 
505   void concurrent_cycle_start();
506   // Abandon current marking iteration due to a Full GC.
507   void concurrent_cycle_abort();
508   void concurrent_cycle_end();
509 
510   void update_accum_task_vtime(int i, double vtime) {
511     _accum_task_vtime[i] += vtime;
512   }
513 
514   double all_task_accum_vtime() {
515     double ret = 0.0;
516     for (uint i = 0; i &lt; _max_num_tasks; ++i)
517       ret += _accum_task_vtime[i];
518     return ret;
519   }
520 
521   // Attempts to steal an object from the task queues of other tasks
522   bool try_stealing(uint worker_id, G1TaskQueueEntry&amp; task_entry);
523 
</pre>
<hr />
<pre>
536 
537   // Moves all per-task cached data into global state.
538   void flush_all_task_caches();
539   // Prepare internal data structures for the next mark cycle. This includes clearing
540   // the next mark bitmap and some internal data structures. This method is intended
541   // to be called concurrently to the mutator. It will yield to safepoint requests.
542   void cleanup_for_next_mark();
543 
544   // Clear the previous marking bitmap during safepoint.
545   void clear_prev_bitmap(WorkGang* workers);
546 
547   // These two methods do the work that needs to be done at the start and end of the
548   // initial mark pause.
549   void pre_initial_mark();
550   void post_initial_mark();
551 
552   // Scan all the root regions and mark everything reachable from
553   // them.
554   void scan_root_regions();
555 
<span class="line-modified">556   // Scan a single root region from nTAMS to top and mark everything reachable from it.</span>
<span class="line-modified">557   void scan_root_region(HeapRegion* hr, uint worker_id);</span>
558 
559   // Do concurrent phase of marking, to a tentative transitive closure.
560   void mark_from_roots();
561 
562   // Do concurrent preclean work.
563   void preclean();
564 
565   void remark();
566 
567   void cleanup();
568   // Mark in the previous bitmap. Caution: the prev bitmap is usually read-only, so use
569   // this carefully.
570   inline void mark_in_prev_bitmap(oop p);
571 
572   // Clears marks for all objects in the given range, for the prev or
573   // next bitmaps.  Caution: the previous bitmap is usually
574   // read-only, so use this carefully!
575   void clear_range_in_prev_bitmap(MemRegion mr);
576 
577   inline bool is_marked_in_prev_bitmap(oop p) const;
578 
579   // Verify that there are no collection set oops on the stacks (taskqueues /
580   // global mark stack) and fingers (global / per-task).
581   // If marking is not in progress, it&#39;s a no-op.
<span class="line-modified">582   void verify_no_cset_oops() PRODUCT_RETURN;</span>
583 
584   inline bool do_yield_check();
585 
586   bool has_aborted()      { return _has_aborted; }
587 
588   void print_summary_info();
589 
590   void print_worker_threads_on(outputStream* st) const;
591   void threads_do(ThreadClosure* tc) const;
592 
593   void print_on_error(outputStream* st) const;
594 
595   // Mark the given object on the next bitmap if it is below nTAMS.
596   inline bool mark_in_next_bitmap(uint worker_id, HeapRegion* const hr, oop const obj);
597   inline bool mark_in_next_bitmap(uint worker_id, oop const obj);
598 
599   inline bool is_marked_in_next_bitmap(oop p) const;
600 
601   // Returns true if initialization was successfully completed.
602   bool completed_initialization() const {
</pre>
<hr />
<pre>
680   size_t                      _real_refs_reached_limit;
681 
682   // If true, then the task has aborted for some reason
683   bool                        _has_aborted;
684   // Set when the task aborts because it has met its time quota
685   bool                        _has_timed_out;
686   // True when we&#39;re draining SATB buffers; this avoids the task
687   // aborting due to SATB buffers being available (as we&#39;re already
688   // dealing with them)
689   bool                        _draining_satb_buffers;
690 
691   // Number sequence of past step times
692   NumberSeq                   _step_times_ms;
693   // Elapsed time of this task
694   double                      _elapsed_time_ms;
695   // Termination time of this task
696   double                      _termination_time_ms;
697   // When this task got into the termination protocol
698   double                      _termination_start_time_ms;
699 
<span class="line-modified">700   TruncatedSeq                _marking_step_diffs_ms;</span>
701 
702   // Updates the local fields after this task has claimed
703   // a new region to scan
704   void setup_for_region(HeapRegion* hr);
705   // Makes the limit of the region up-to-date
706   void update_region_limit();
707 
708   // Called when either the words scanned or the refs visited limit
709   // has been reached
710   void reached_limit();
711   // Recalculates the words scanned and refs visited limits
712   void recalculate_limits();
713   // Decreases the words scanned and refs visited limits when we reach
714   // an expensive operation
715   void decrease_limits();
716   // Checks whether the words scanned or refs visited reached their
717   // respective limit and calls reached_limit() if they have
718   void check_limits() {
719     if (_words_scanned &gt;= _words_scanned_limit ||
720         _refs_reached &gt;= _refs_reached_limit) {
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2001, 2020, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_GC_G1_G1CONCURRENTMARK_HPP
 26 #define SHARE_GC_G1_G1CONCURRENTMARK_HPP
 27 
 28 #include &quot;gc/g1/g1ConcurrentMarkBitMap.hpp&quot;
 29 #include &quot;gc/g1/g1ConcurrentMarkObjArrayProcessor.hpp&quot;
 30 #include &quot;gc/g1/g1HeapVerifier.hpp&quot;
 31 #include &quot;gc/g1/g1RegionMarkStatsCache.hpp&quot;
 32 #include &quot;gc/g1/heapRegionSet.hpp&quot;
<span class="line-added"> 33 #include &quot;gc/shared/taskTerminator.hpp&quot;</span>
 34 #include &quot;gc/shared/taskqueue.hpp&quot;
<span class="line-added"> 35 #include &quot;gc/shared/verifyOption.hpp&quot;</span>
<span class="line-added"> 36 #include &quot;gc/shared/workgroup.hpp&quot;</span>
 37 #include &quot;memory/allocation.hpp&quot;
 38 #include &quot;utilities/compilerWarnings.hpp&quot;
 39 
 40 class ConcurrentGCTimer;
 41 class G1ConcurrentMarkThread;
 42 class G1CollectedHeap;
 43 class G1CMOopClosure;
 44 class G1CMTask;
 45 class G1ConcurrentMark;
 46 class G1OldTracer;
 47 class G1RegionToSpaceMapper;
 48 class G1SurvivorRegions;
<span class="line-added"> 49 class ThreadClosure;</span>
 50 
 51 PRAGMA_DIAG_PUSH
 52 // warning C4522: multiple assignment operators specified
 53 PRAGMA_DISABLE_MSVC_WARNING(4522)
 54 
 55 // This is a container class for either an oop or a continuation address for
 56 // mark stack entries. Both are pushed onto the mark stack.
 57 class G1TaskQueueEntry {
 58 private:
 59   void* _holder;
 60 
 61   static const uintptr_t ArraySliceBit = 1;
 62 
 63   G1TaskQueueEntry(oop obj) : _holder(obj) {
 64     assert(_holder != NULL, &quot;Not allowed to set NULL task queue element&quot;);
 65   }
 66   G1TaskQueueEntry(HeapWord* addr) : _holder((void*)((uintptr_t)addr | ArraySliceBit)) { }
 67 public:
 68   G1TaskQueueEntry(const G1TaskQueueEntry&amp; other) { _holder = other._holder; }
 69   G1TaskQueueEntry() : _holder(NULL) { }
</pre>
<hr />
<pre>
208   // Return whether the chunk list is empty. Racy due to unsynchronized access to
209   // _chunk_list.
210   bool is_empty() const { return _chunk_list == NULL; }
211 
212   size_t capacity() const  { return _chunk_capacity; }
213 
214   // Expand the stack, typically in response to an overflow condition
215   void expand();
216 
217   // Return the approximate number of oops on this mark stack. Racy due to
218   // unsynchronized access to _chunks_in_chunk_list.
219   size_t size() const { return _chunks_in_chunk_list * EntriesPerChunk; }
220 
221   void set_empty();
222 
223   // Apply Fn to every oop on the mark stack. The mark stack must not
224   // be modified while iterating.
225   template&lt;typename Fn&gt; void iterate(Fn fn) const PRODUCT_RETURN;
226 };
227 
<span class="line-modified">228 // Root MemRegions are memory areas that contain objects which references are</span>
<span class="line-modified">229 // roots wrt to the marking. They must be scanned before marking to maintain the</span>
230 // SATB invariant.
<span class="line-modified">231 // Typically they contain the areas from nTAMS to top of the regions.</span>
<span class="line-added">232 // We could scan and mark through these objects during the initial-mark pause, but for</span>
233 // pause time reasons we move this work to the concurrent phase.
234 // We need to complete this procedure before the next GC because it might determine
235 // that some of these &quot;root objects&quot; are dead, potentially dropping some required
236 // references.
<span class="line-modified">237 // Root MemRegions comprise of the contents of survivor regions at the end</span>
<span class="line-modified">238 // of the GC, and any objects copied into the old gen during GC.</span>
<span class="line-modified">239 class G1CMRootMemRegions {</span>
<span class="line-modified">240   // The set of root MemRegions.</span>
<span class="line-added">241   MemRegion*   _root_regions;</span>
242   size_t const _max_regions;
243 
244   volatile size_t _num_root_regions; // Actual number of root regions.
245 
246   volatile size_t _claimed_root_regions; // Number of root regions currently claimed.
247 
248   volatile bool _scan_in_progress;
249   volatile bool _should_abort;
250 
251   void notify_scan_done();
252 
253 public:
<span class="line-modified">254   G1CMRootMemRegions(uint const max_regions);</span>
<span class="line-modified">255   ~G1CMRootMemRegions();</span>
256 
257   // Reset the data structure to allow addition of new root regions.
258   void reset();
259 
<span class="line-modified">260   void add(HeapWord* start, HeapWord* end);</span>
261 
262   // Reset the claiming / scanning of the root regions.
263   void prepare_for_scan();
264 
265   // Forces get_next() to return NULL so that the iteration aborts early.
266   void abort() { _should_abort = true; }
267 
268   // Return true if the CM thread are actively scanning root regions,
269   // false otherwise.
270   bool scan_in_progress() { return _scan_in_progress; }
271 
<span class="line-modified">272   // Claim the next root MemRegion to scan atomically, or return NULL if</span>
273   // all have been claimed.
<span class="line-modified">274   const MemRegion* claim_next();</span>
275 
276   // The number of root regions to scan.
277   uint num_root_regions() const;
278 
279   void cancel_scan();
280 
281   // Flag that we&#39;re done with root region scanning and notify anyone
282   // who&#39;s waiting on it. If aborted is false, assume that all regions
283   // have been claimed.
284   void scan_finished();
285 
286   // If CM threads are still scanning root regions, wait until they
287   // are done. Return true if we had to wait, false otherwise.
288   bool wait_until_scan_finished();
289 };
290 
291 // This class manages data structures and methods for doing liveness analysis in
292 // G1&#39;s concurrent cycle.
293 class G1ConcurrentMark : public CHeapObj&lt;mtGC&gt; {
294   friend class G1ConcurrentMarkThread;
</pre>
<hr />
<pre>
298   friend class G1CMDrainMarkingStackClosure;
299   friend class G1CMBitMapClosure;
300   friend class G1CMConcurrentMarkingTask;
301   friend class G1CMRemarkTask;
302   friend class G1CMTask;
303 
304   G1ConcurrentMarkThread* _cm_thread;     // The thread doing the work
305   G1CollectedHeap*        _g1h;           // The heap
306   bool                    _completed_initialization; // Set to true when initialization is complete
307 
308   // Concurrent marking support structures
309   G1CMBitMap              _mark_bitmap_1;
310   G1CMBitMap              _mark_bitmap_2;
311   G1CMBitMap*             _prev_mark_bitmap; // Completed mark bitmap
312   G1CMBitMap*             _next_mark_bitmap; // Under-construction mark bitmap
313 
314   // Heap bounds
315   MemRegion const         _heap;
316 
317   // Root region tracking and claiming
<span class="line-modified">318   G1CMRootMemRegions         _root_regions;</span>
319 
320   // For grey objects
321   G1CMMarkStack           _global_mark_stack; // Grey objects behind global finger
322   HeapWord* volatile      _finger;            // The global finger, region aligned,
323                                               // always pointing to the end of the
324                                               // last claimed region
325 
326   uint                    _worker_id_offset;
327   uint                    _max_num_tasks;    // Maximum number of marking tasks
328   uint                    _num_active_tasks; // Number of tasks currently active
329   G1CMTask**              _tasks;            // Task queue array (max_worker_id length)
330 
331   G1CMTaskQueueSet*       _task_queues; // Task queue set
332   TaskTerminator          _terminator;  // For termination
333 
334   // Two sync barriers that are used to synchronize tasks when an
335   // overflow occurs. The algorithm is the following. All tasks enter
336   // the first one to ensure that they have all stopped manipulating
337   // the global data structures. After they exit it, they re-initialize
338   // their data structures and task 0 re-initializes the global data
</pre>
<hr />
<pre>
398   void reset();
399 
400   // Resets all the marking data structures. Called when we have to restart
401   // marking or when marking completes (via set_non_marking_state below).
402   void reset_marking_for_restart();
403 
404   // We do this after we&#39;re done with marking so that the marking data
405   // structures are initialized to a sensible and predictable state.
406   void reset_at_marking_complete();
407 
408   // Called to indicate how many threads are currently active.
409   void set_concurrency(uint active_tasks);
410 
411   // Should be called to indicate which phase we&#39;re in (concurrent
412   // mark or remark) and how many threads are currently active.
413   void set_concurrency_and_phase(uint active_tasks, bool concurrent);
414 
415   // Prints all gathered CM-related statistics
416   void print_stats();
417 
<span class="line-modified">418   HeapWord*           finger()       { return _finger;   }</span>
<span class="line-modified">419   bool                concurrent()   { return _concurrent; }</span>
<span class="line-modified">420   uint                active_tasks() { return _num_active_tasks; }</span>
<span class="line-modified">421   TaskTerminator*     terminator()   { return &amp;_terminator; }</span>
422 
423   // Claims the next available region to be scanned by a marking
424   // task/thread. It might return NULL if the next region is empty or
425   // we have run out of regions. In the latter case, out_of_regions()
426   // determines whether we&#39;ve really run out of regions or the task
427   // should call claim_region() again. This might seem a bit
428   // awkward. Originally, the code was written so that claim_region()
429   // either successfully returned with a non-empty region or there
430   // were no more regions to be claimed. The problem with this was
431   // that, in certain circumstances, it iterated over large chunks of
432   // the heap finding only empty regions and, while it was working, it
433   // was preventing the calling task to call its regular clock
434   // method. So, this way, each task will spend very little time in
435   // claim_region() and is allowed to call the regular clock method
436   // frequently.
437   HeapRegion* claim_region(uint worker_id);
438 
439   // Determines whether we&#39;ve run out of regions to scan. Note that
440   // the finger can point past the heap end in case the heap was expanded
441   // to satisfy an allocation without doing a GC. This is fine, because all
</pre>
<hr />
<pre>
489   void clear_statistics_in_region(uint region_idx);
490   // Notification for eagerly reclaimed regions to clean up.
491   void humongous_object_eagerly_reclaimed(HeapRegion* r);
492   // Manipulation of the global mark stack.
493   // The push and pop operations are used by tasks for transfers
494   // between task-local queues and the global mark stack.
495   bool mark_stack_push(G1TaskQueueEntry* arr) {
496     if (!_global_mark_stack.par_push_chunk(arr)) {
497       set_has_overflown();
498       return false;
499     }
500     return true;
501   }
502   bool mark_stack_pop(G1TaskQueueEntry* arr) {
503     return _global_mark_stack.par_pop_chunk(arr);
504   }
505   size_t mark_stack_size() const                { return _global_mark_stack.size(); }
506   size_t partial_mark_stack_size_target() const { return _global_mark_stack.capacity() / 3; }
507   bool mark_stack_empty() const                 { return _global_mark_stack.is_empty(); }
508 
<span class="line-modified">509   G1CMRootMemRegions* root_regions() { return &amp;_root_regions; }</span>
510 
511   void concurrent_cycle_start();
512   // Abandon current marking iteration due to a Full GC.
513   void concurrent_cycle_abort();
514   void concurrent_cycle_end();
515 
516   void update_accum_task_vtime(int i, double vtime) {
517     _accum_task_vtime[i] += vtime;
518   }
519 
520   double all_task_accum_vtime() {
521     double ret = 0.0;
522     for (uint i = 0; i &lt; _max_num_tasks; ++i)
523       ret += _accum_task_vtime[i];
524     return ret;
525   }
526 
527   // Attempts to steal an object from the task queues of other tasks
528   bool try_stealing(uint worker_id, G1TaskQueueEntry&amp; task_entry);
529 
</pre>
<hr />
<pre>
542 
543   // Moves all per-task cached data into global state.
544   void flush_all_task_caches();
545   // Prepare internal data structures for the next mark cycle. This includes clearing
546   // the next mark bitmap and some internal data structures. This method is intended
547   // to be called concurrently to the mutator. It will yield to safepoint requests.
548   void cleanup_for_next_mark();
549 
550   // Clear the previous marking bitmap during safepoint.
551   void clear_prev_bitmap(WorkGang* workers);
552 
553   // These two methods do the work that needs to be done at the start and end of the
554   // initial mark pause.
555   void pre_initial_mark();
556   void post_initial_mark();
557 
558   // Scan all the root regions and mark everything reachable from
559   // them.
560   void scan_root_regions();
561 
<span class="line-modified">562   // Scan a single root MemRegion to mark everything reachable from it.</span>
<span class="line-modified">563   void scan_root_region(const MemRegion* region, uint worker_id);</span>
564 
565   // Do concurrent phase of marking, to a tentative transitive closure.
566   void mark_from_roots();
567 
568   // Do concurrent preclean work.
569   void preclean();
570 
571   void remark();
572 
573   void cleanup();
574   // Mark in the previous bitmap. Caution: the prev bitmap is usually read-only, so use
575   // this carefully.
576   inline void mark_in_prev_bitmap(oop p);
577 
578   // Clears marks for all objects in the given range, for the prev or
579   // next bitmaps.  Caution: the previous bitmap is usually
580   // read-only, so use this carefully!
581   void clear_range_in_prev_bitmap(MemRegion mr);
582 
583   inline bool is_marked_in_prev_bitmap(oop p) const;
584 
585   // Verify that there are no collection set oops on the stacks (taskqueues /
586   // global mark stack) and fingers (global / per-task).
587   // If marking is not in progress, it&#39;s a no-op.
<span class="line-modified">588   void verify_no_collection_set_oops() PRODUCT_RETURN;</span>
589 
590   inline bool do_yield_check();
591 
592   bool has_aborted()      { return _has_aborted; }
593 
594   void print_summary_info();
595 
596   void print_worker_threads_on(outputStream* st) const;
597   void threads_do(ThreadClosure* tc) const;
598 
599   void print_on_error(outputStream* st) const;
600 
601   // Mark the given object on the next bitmap if it is below nTAMS.
602   inline bool mark_in_next_bitmap(uint worker_id, HeapRegion* const hr, oop const obj);
603   inline bool mark_in_next_bitmap(uint worker_id, oop const obj);
604 
605   inline bool is_marked_in_next_bitmap(oop p) const;
606 
607   // Returns true if initialization was successfully completed.
608   bool completed_initialization() const {
</pre>
<hr />
<pre>
686   size_t                      _real_refs_reached_limit;
687 
688   // If true, then the task has aborted for some reason
689   bool                        _has_aborted;
690   // Set when the task aborts because it has met its time quota
691   bool                        _has_timed_out;
692   // True when we&#39;re draining SATB buffers; this avoids the task
693   // aborting due to SATB buffers being available (as we&#39;re already
694   // dealing with them)
695   bool                        _draining_satb_buffers;
696 
697   // Number sequence of past step times
698   NumberSeq                   _step_times_ms;
699   // Elapsed time of this task
700   double                      _elapsed_time_ms;
701   // Termination time of this task
702   double                      _termination_time_ms;
703   // When this task got into the termination protocol
704   double                      _termination_start_time_ms;
705 
<span class="line-modified">706   TruncatedSeq                _marking_step_diff_ms;</span>
707 
708   // Updates the local fields after this task has claimed
709   // a new region to scan
710   void setup_for_region(HeapRegion* hr);
711   // Makes the limit of the region up-to-date
712   void update_region_limit();
713 
714   // Called when either the words scanned or the refs visited limit
715   // has been reached
716   void reached_limit();
717   // Recalculates the words scanned and refs visited limits
718   void recalculate_limits();
719   // Decreases the words scanned and refs visited limits when we reach
720   // an expensive operation
721   void decrease_limits();
722   // Checks whether the words scanned or refs visited reached their
723   // respective limit and calls reached_limit() if they have
724   void check_limits() {
725     if (_words_scanned &gt;= _words_scanned_limit ||
726         _refs_reached &gt;= _refs_reached_limit) {
</pre>
</td>
</tr>
</table>
<center><a href="g1ConcurrentMark.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1ConcurrentMark.inline.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>