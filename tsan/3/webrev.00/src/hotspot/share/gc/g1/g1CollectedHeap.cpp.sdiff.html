<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1CollectedHeap.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1CodeCacheRemSet.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1CollectedHeap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2001, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;classfile/classLoaderDataGraph.hpp&quot;
  27 #include &quot;classfile/metadataOnStackMark.hpp&quot;
  28 #include &quot;classfile/stringTable.hpp&quot;
  29 #include &quot;code/codeCache.hpp&quot;
  30 #include &quot;code/icBuffer.hpp&quot;
  31 #include &quot;gc/g1/g1Allocator.inline.hpp&quot;

  32 #include &quot;gc/g1/g1BarrierSet.hpp&quot;

  33 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
  34 #include &quot;gc/g1/g1CollectionSet.hpp&quot;
<span class="line-removed">  35 #include &quot;gc/g1/g1CollectorPolicy.hpp&quot;</span>
  36 #include &quot;gc/g1/g1CollectorState.hpp&quot;
  37 #include &quot;gc/g1/g1ConcurrentRefine.hpp&quot;
  38 #include &quot;gc/g1/g1ConcurrentRefineThread.hpp&quot;
  39 #include &quot;gc/g1/g1ConcurrentMarkThread.inline.hpp&quot;
  40 #include &quot;gc/g1/g1DirtyCardQueue.hpp&quot;
  41 #include &quot;gc/g1/g1EvacStats.inline.hpp&quot;
  42 #include &quot;gc/g1/g1FullCollector.hpp&quot;
  43 #include &quot;gc/g1/g1GCPhaseTimes.hpp&quot;
  44 #include &quot;gc/g1/g1HeapSizingPolicy.hpp&quot;
  45 #include &quot;gc/g1/g1HeapTransition.hpp&quot;
  46 #include &quot;gc/g1/g1HeapVerifier.hpp&quot;
  47 #include &quot;gc/g1/g1HotCardCache.hpp&quot;
  48 #include &quot;gc/g1/g1MemoryPool.hpp&quot;
  49 #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;

  50 #include &quot;gc/g1/g1ParScanThreadState.inline.hpp&quot;
  51 #include &quot;gc/g1/g1Policy.hpp&quot;

  52 #include &quot;gc/g1/g1RegionToSpaceMapper.hpp&quot;
  53 #include &quot;gc/g1/g1RemSet.hpp&quot;
  54 #include &quot;gc/g1/g1RootClosures.hpp&quot;
  55 #include &quot;gc/g1/g1RootProcessor.hpp&quot;
  56 #include &quot;gc/g1/g1SATBMarkQueueSet.hpp&quot;
  57 #include &quot;gc/g1/g1StringDedup.hpp&quot;
  58 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;

  59 #include &quot;gc/g1/g1YCTypes.hpp&quot;
  60 #include &quot;gc/g1/g1YoungRemSetSamplingThread.hpp&quot;
  61 #include &quot;gc/g1/g1VMOperations.hpp&quot;
  62 #include &quot;gc/g1/heapRegion.inline.hpp&quot;
  63 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
  64 #include &quot;gc/g1/heapRegionSet.inline.hpp&quot;
  65 #include &quot;gc/shared/gcBehaviours.hpp&quot;
  66 #include &quot;gc/shared/gcHeapSummary.hpp&quot;
  67 #include &quot;gc/shared/gcId.hpp&quot;
  68 #include &quot;gc/shared/gcLocker.hpp&quot;
  69 #include &quot;gc/shared/gcTimer.hpp&quot;
<span class="line-removed">  70 #include &quot;gc/shared/gcTrace.hpp&quot;</span>
  71 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  72 #include &quot;gc/shared/generationSpec.hpp&quot;
  73 #include &quot;gc/shared/isGCActiveMark.hpp&quot;

  74 #include &quot;gc/shared/oopStorageParState.hpp&quot;
<span class="line-removed">  75 #include &quot;gc/shared/parallelCleaning.hpp&quot;</span>
  76 #include &quot;gc/shared/preservedMarks.inline.hpp&quot;
  77 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
  78 #include &quot;gc/shared/referenceProcessor.inline.hpp&quot;

  79 #include &quot;gc/shared/taskqueue.inline.hpp&quot;
  80 #include &quot;gc/shared/weakProcessor.inline.hpp&quot;
  81 #include &quot;gc/shared/workerPolicy.hpp&quot;
  82 #include &quot;logging/log.hpp&quot;
  83 #include &quot;memory/allocation.hpp&quot;
  84 #include &quot;memory/iterator.hpp&quot;
  85 #include &quot;memory/resourceArea.hpp&quot;

  86 #include &quot;oops/access.inline.hpp&quot;
  87 #include &quot;oops/compressedOops.inline.hpp&quot;
  88 #include &quot;oops/oop.inline.hpp&quot;
  89 #include &quot;runtime/atomic.hpp&quot;
  90 #include &quot;runtime/flags/flagSetting.hpp&quot;
  91 #include &quot;runtime/handles.inline.hpp&quot;
  92 #include &quot;runtime/init.hpp&quot;
  93 #include &quot;runtime/orderAccess.hpp&quot;
  94 #include &quot;runtime/threadSMR.hpp&quot;
  95 #include &quot;runtime/vmThread.hpp&quot;
  96 #include &quot;utilities/align.hpp&quot;

  97 #include &quot;utilities/globalDefinitions.hpp&quot;
  98 #include &quot;utilities/stack.inline.hpp&quot;
  99 
 100 size_t G1CollectedHeap::_humongous_object_threshold_in_words = 0;
 101 
 102 // INVARIANTS/NOTES
 103 //
 104 // All allocation activity covered by the G1CollectedHeap interface is
 105 // serialized by acquiring the HeapLock.  This happens in mem_allocate
 106 // and allocate_new_tlab, which are the &quot;entry&quot; points to the
 107 // allocation code from the rest of the JVM.  (Note that this does not
 108 // apply to TLAB allocation, which is not part of this interface: it
 109 // is done by clients of this interface.)
 110 
 111 class RedirtyLoggedCardTableEntryClosure : public G1CardTableEntryClosure {
 112  private:
 113   size_t _num_dirtied;
 114   G1CollectedHeap* _g1h;
 115   G1CardTable* _g1_ct;
 116 
 117   HeapRegion* region_for_card(CardValue* card_ptr) const {
 118     return _g1h-&gt;heap_region_containing(_g1_ct-&gt;addr_for(card_ptr));
 119   }
 120 
 121   bool will_become_free(HeapRegion* hr) const {
 122     // A region will be freed by free_collection_set if the region is in the
 123     // collection set and has not had an evacuation failure.
 124     return _g1h-&gt;is_in_cset(hr) &amp;&amp; !hr-&gt;evacuation_failed();
 125   }
 126 
 127  public:
 128   RedirtyLoggedCardTableEntryClosure(G1CollectedHeap* g1h) : G1CardTableEntryClosure(),
 129     _num_dirtied(0), _g1h(g1h), _g1_ct(g1h-&gt;card_table()) { }
 130 
<span class="line-modified"> 131   bool do_card_ptr(CardValue* card_ptr, uint worker_i) {</span>
 132     HeapRegion* hr = region_for_card(card_ptr);
 133 
 134     // Should only dirty cards in regions that won&#39;t be freed.
 135     if (!will_become_free(hr)) {
 136       *card_ptr = G1CardTable::dirty_card_val();
 137       _num_dirtied++;
 138     }
<span class="line-removed"> 139 </span>
<span class="line-removed"> 140     return true;</span>
 141   }
 142 
 143   size_t num_dirtied()   const { return _num_dirtied; }
 144 };
 145 
 146 
 147 void G1RegionMappingChangedListener::reset_from_card_cache(uint start_idx, size_t num_regions) {
 148   HeapRegionRemSet::invalidate_from_card_cache(start_idx, num_regions);
 149 }
 150 
 151 void G1RegionMappingChangedListener::on_commit(uint start_idx, size_t num_regions, bool zero_filled) {
 152   // The from card cache is not the memory that is actually committed. So we cannot
 153   // take advantage of the zero_filled parameter.
 154   reset_from_card_cache(start_idx, num_regions);
 155 }
 156 





 157 
 158 HeapRegion* G1CollectedHeap::new_heap_region(uint hrs_index,
 159                                              MemRegion mr) {
 160   return new HeapRegion(hrs_index, bot(), mr);
 161 }
 162 
 163 // Private methods.
 164 
<span class="line-modified"> 165 HeapRegion* G1CollectedHeap::new_region(size_t word_size, HeapRegionType type, bool do_expand) {</span>



 166   assert(!is_humongous(word_size) || word_size &lt;= HeapRegion::GrainWords,
 167          &quot;the only time we use this to allocate a humongous region is &quot;
 168          &quot;when we are allocating a single humongous region&quot;);
 169 
<span class="line-modified"> 170   HeapRegion* res = _hrm-&gt;allocate_free_region(type);</span>
 171 
 172   if (res == NULL &amp;&amp; do_expand &amp;&amp; _expand_heap_after_alloc_failure) {
 173     // Currently, only attempts to allocate GC alloc regions set
 174     // do_expand to true. So, we should only reach here during a
 175     // safepoint. If this assumption changes we might have to
 176     // reconsider the use of _expand_heap_after_alloc_failure.
 177     assert(SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
 178 
 179     log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (region allocation request failed). Allocation request: &quot; SIZE_FORMAT &quot;B&quot;,
 180                               word_size * HeapWordSize);
 181 
<span class="line-modified"> 182     if (expand(word_size * HeapWordSize)) {</span>
<span class="line-modified"> 183       // Given that expand() succeeded in expanding the heap, and we</span>



 184       // always expand the heap by an amount aligned to the heap
 185       // region size, the free list should in theory not be empty.
 186       // In either case allocate_free_region() will check for NULL.
<span class="line-modified"> 187       res = _hrm-&gt;allocate_free_region(type);</span>
 188     } else {
 189       _expand_heap_after_alloc_failure = false;
 190     }
 191   }
 192   return res;
 193 }
 194 
 195 HeapWord*
 196 G1CollectedHeap::humongous_obj_allocate_initialize_regions(uint first,
 197                                                            uint num_regions,
 198                                                            size_t word_size) {
 199   assert(first != G1_NO_HRM_INDEX, &quot;pre-condition&quot;);
 200   assert(is_humongous(word_size), &quot;word_size should be humongous&quot;);
 201   assert(num_regions * HeapRegion::GrainWords &gt;= word_size, &quot;pre-condition&quot;);
 202 
 203   // Index of last region in the series.
 204   uint last = first + num_regions - 1;
 205 
 206   // We need to initialize the region(s) we just discovered. This is
 207   // a bit tricky given that it can happen concurrently with
</pre>
<hr />
<pre>
 414 
 415   // Make sure you read the note in attempt_allocation_humongous().
 416 
 417   assert_heap_not_locked_and_not_at_safepoint();
 418   assert(!is_humongous(word_size), &quot;attempt_allocation_slow() should not &quot;
 419          &quot;be called for humongous allocation requests&quot;);
 420 
 421   // We should only get here after the first-level allocation attempt
 422   // (attempt_allocation()) failed to allocate.
 423 
 424   // We will loop until a) we manage to successfully perform the
 425   // allocation or b) we successfully schedule a collection which
 426   // fails to perform the allocation. b) is the only case when we&#39;ll
 427   // return NULL.
 428   HeapWord* result = NULL;
 429   for (uint try_count = 1, gclocker_retry_count = 0; /* we&#39;ll return */; try_count += 1) {
 430     bool should_try_gc;
 431     uint gc_count_before;
 432 
 433     {
<span class="line-modified"> 434       MutexLockerEx x(Heap_lock);</span>
 435       result = _allocator-&gt;attempt_allocation_locked(word_size);
 436       if (result != NULL) {
 437         return result;
 438       }
 439 
 440       // If the GCLocker is active and we are bound for a GC, try expanding young gen.
 441       // This is different to when only GCLocker::needs_gc() is set: try to avoid
 442       // waiting because the GCLocker is active to not wait too long.
 443       if (GCLocker::is_active_and_needs_gc() &amp;&amp; policy()-&gt;can_expand_young_list()) {
 444         // No need for an ergo message here, can_expand_young_list() does this when
 445         // it returns true.
 446         result = _allocator-&gt;attempt_allocation_force(word_size);
 447         if (result != NULL) {
 448           return result;
 449         }
 450       }
 451       // Only try a GC if the GCLocker does not signal the need for a GC. Wait until
 452       // the GCLocker initiated GC has been performed and then retry. This includes
 453       // the case when the GC Locker is not active but has not been performed.
 454       should_try_gc = !GCLocker::needs_gc();
</pre>
<hr />
<pre>
 553 }
 554 
 555 bool G1CollectedHeap::check_archive_addresses(MemRegion* ranges, size_t count) {
 556   assert(ranges != NULL, &quot;MemRegion array NULL&quot;);
 557   assert(count != 0, &quot;No MemRegions provided&quot;);
 558   MemRegion reserved = _hrm-&gt;reserved();
 559   for (size_t i = 0; i &lt; count; i++) {
 560     if (!reserved.contains(ranges[i].start()) || !reserved.contains(ranges[i].last())) {
 561       return false;
 562     }
 563   }
 564   return true;
 565 }
 566 
 567 bool G1CollectedHeap::alloc_archive_regions(MemRegion* ranges,
 568                                             size_t count,
 569                                             bool open) {
 570   assert(!is_init_completed(), &quot;Expect to be called at JVM init time&quot;);
 571   assert(ranges != NULL, &quot;MemRegion array NULL&quot;);
 572   assert(count != 0, &quot;No MemRegions provided&quot;);
<span class="line-modified"> 573   MutexLockerEx x(Heap_lock);</span>
 574 
 575   MemRegion reserved = _hrm-&gt;reserved();
 576   HeapWord* prev_last_addr = NULL;
 577   HeapRegion* prev_last_region = NULL;
 578 
 579   // Temporarily disable pretouching of heap pages. This interface is used
 580   // when mmap&#39;ing archived heap data in, so pre-touching is wasted.
 581   FlagSetting fs(AlwaysPreTouch, false);
 582 
 583   // Enable archive object checking used by G1MarkSweep. We have to let it know
 584   // about each archive range, so that objects in those ranges aren&#39;t marked.
 585   G1ArchiveAllocator::enable_archive_object_check();
 586 
 587   // For each specified MemRegion range, allocate the corresponding G1
 588   // regions and mark them as archive regions. We expect the ranges
 589   // in ascending starting address order, without overlap.
 590   for (size_t i = 0; i &lt; count; i++) {
 591     MemRegion curr_range = ranges[i];
 592     HeapWord* start_address = curr_range.start();
 593     size_t word_size = curr_range.word_size();
</pre>
<hr />
<pre>
 640     while (curr_region != NULL) {
 641       assert(curr_region-&gt;is_empty() &amp;&amp; !curr_region-&gt;is_pinned(),
 642              &quot;Region already in use (index %u)&quot;, curr_region-&gt;hrm_index());
 643       if (open) {
 644         curr_region-&gt;set_open_archive();
 645       } else {
 646         curr_region-&gt;set_closed_archive();
 647       }
 648       _hr_printer.alloc(curr_region);
 649       _archive_set.add(curr_region);
 650       HeapWord* top;
 651       HeapRegion* next_region;
 652       if (curr_region != last_region) {
 653         top = curr_region-&gt;end();
 654         next_region = _hrm-&gt;next_region_in_heap(curr_region);
 655       } else {
 656         top = last_address + 1;
 657         next_region = NULL;
 658       }
 659       curr_region-&gt;set_top(top);
<span class="line-removed"> 660       curr_region-&gt;set_first_dead(top);</span>
<span class="line-removed"> 661       curr_region-&gt;set_end_of_live(top);</span>
 662       curr_region = next_region;
 663     }
 664 
 665     // Notify mark-sweep of the archive
 666     G1ArchiveAllocator::set_range_archive(curr_range, open);
 667   }
 668   return true;
 669 }
 670 
 671 void G1CollectedHeap::fill_archive_regions(MemRegion* ranges, size_t count) {
 672   assert(!is_init_completed(), &quot;Expect to be called at JVM init time&quot;);
 673   assert(ranges != NULL, &quot;MemRegion array NULL&quot;);
 674   assert(count != 0, &quot;No MemRegions provided&quot;);
 675   MemRegion reserved = _hrm-&gt;reserved();
 676   HeapWord *prev_last_addr = NULL;
 677   HeapRegion* prev_last_region = NULL;
 678 
 679   // For each MemRegion, create filler objects, if needed, in the G1 regions
 680   // that contain the address range. The address range actually within the
 681   // MemRegion will not be modified. That is assumed to have been initialized
 682   // elsewhere, probably via an mmap of archived heap data.
<span class="line-modified"> 683   MutexLockerEx x(Heap_lock);</span>
 684   for (size_t i = 0; i &lt; count; i++) {
 685     HeapWord* start_address = ranges[i].start();
 686     HeapWord* last_address = ranges[i].last();
 687 
 688     assert(reserved.contains(start_address) &amp;&amp; reserved.contains(last_address),
 689            &quot;MemRegion outside of heap [&quot; PTR_FORMAT &quot;, &quot; PTR_FORMAT &quot;]&quot;,
 690            p2i(start_address), p2i(last_address));
 691     assert(start_address &gt; prev_last_addr,
 692            &quot;Ranges not in ascending order: &quot; PTR_FORMAT &quot; &lt;= &quot; PTR_FORMAT ,
 693            p2i(start_address), p2i(prev_last_addr));
 694 
 695     HeapRegion* start_region = _hrm-&gt;addr_to_region(start_address);
 696     HeapRegion* last_region = _hrm-&gt;addr_to_region(last_address);
 697     HeapWord* bottom_address = start_region-&gt;bottom();
 698 
 699     // Check for a range beginning in the same region in which the
 700     // previous one ended.
 701     if (start_region == prev_last_region) {
 702       bottom_address = prev_last_addr + 1;
 703     }
</pre>
<hr />
<pre>
 737          &quot;be called for humongous allocation requests&quot;);
 738 
 739   HeapWord* result = _allocator-&gt;attempt_allocation(min_word_size, desired_word_size, actual_word_size);
 740 
 741   if (result == NULL) {
 742     *actual_word_size = desired_word_size;
 743     result = attempt_allocation_slow(desired_word_size);
 744   }
 745 
 746   assert_heap_not_locked();
 747   if (result != NULL) {
 748     assert(*actual_word_size != 0, &quot;Actual size must have been set here&quot;);
 749     dirty_young_block(result, *actual_word_size);
 750   } else {
 751     *actual_word_size = 0;
 752   }
 753 
 754   return result;
 755 }
 756 
<span class="line-modified"> 757 void G1CollectedHeap::dealloc_archive_regions(MemRegion* ranges, size_t count, bool is_open) {</span>
 758   assert(!is_init_completed(), &quot;Expect to be called at JVM init time&quot;);
 759   assert(ranges != NULL, &quot;MemRegion array NULL&quot;);
 760   assert(count != 0, &quot;No MemRegions provided&quot;);
 761   MemRegion reserved = _hrm-&gt;reserved();
 762   HeapWord* prev_last_addr = NULL;
 763   HeapRegion* prev_last_region = NULL;
 764   size_t size_used = 0;
 765   size_t uncommitted_regions = 0;
 766 
 767   // For each Memregion, free the G1 regions that constitute it, and
 768   // notify mark-sweep that the range is no longer to be considered &#39;archive.&#39;
<span class="line-modified"> 769   MutexLockerEx x(Heap_lock);</span>
 770   for (size_t i = 0; i &lt; count; i++) {
 771     HeapWord* start_address = ranges[i].start();
 772     HeapWord* last_address = ranges[i].last();
 773 
 774     assert(reserved.contains(start_address) &amp;&amp; reserved.contains(last_address),
 775            &quot;MemRegion outside of heap [&quot; PTR_FORMAT &quot;, &quot; PTR_FORMAT &quot;]&quot;,
 776            p2i(start_address), p2i(last_address));
 777     assert(start_address &gt; prev_last_addr,
 778            &quot;Ranges not in ascending order: &quot; PTR_FORMAT &quot; &lt;= &quot; PTR_FORMAT ,
 779            p2i(start_address), p2i(prev_last_addr));
 780     size_used += ranges[i].byte_size();
 781     prev_last_addr = last_address;
 782 
 783     HeapRegion* start_region = _hrm-&gt;addr_to_region(start_address);
 784     HeapRegion* last_region = _hrm-&gt;addr_to_region(last_address);
 785 
 786     // Check for ranges that start in the same G1 region in which the previous
 787     // range ended, and adjust the start address so we don&#39;t try to free
 788     // the same region again. If the current range is entirely within that
 789     // region, skip it.
</pre>
<hr />
<pre>
 799     // After verifying that each region was marked as an archive region by
 800     // alloc_archive_regions, set it free and empty and uncommit it.
 801     HeapRegion* curr_region = start_region;
 802     while (curr_region != NULL) {
 803       guarantee(curr_region-&gt;is_archive(),
 804                 &quot;Expected archive region at index %u&quot;, curr_region-&gt;hrm_index());
 805       uint curr_index = curr_region-&gt;hrm_index();
 806       _archive_set.remove(curr_region);
 807       curr_region-&gt;set_free();
 808       curr_region-&gt;set_top(curr_region-&gt;bottom());
 809       if (curr_region != last_region) {
 810         curr_region = _hrm-&gt;next_region_in_heap(curr_region);
 811       } else {
 812         curr_region = NULL;
 813       }
 814       _hrm-&gt;shrink_at(curr_index, 1);
 815       uncommitted_regions++;
 816     }
 817 
 818     // Notify mark-sweep that this is no longer an archive range.
<span class="line-modified"> 819     G1ArchiveAllocator::clear_range_archive(ranges[i], is_open);</span>
 820   }
 821 
 822   if (uncommitted_regions != 0) {
 823     log_debug(gc, ergo, heap)(&quot;Attempt heap shrinking (uncommitted archive regions). Total size: &quot; SIZE_FORMAT &quot;B&quot;,
 824                               HeapRegion::GrainWords * HeapWordSize * uncommitted_regions);
 825   }
 826   decrease_used(size_used);
 827 }
 828 
 829 oop G1CollectedHeap::materialize_archived_object(oop obj) {
 830   assert(obj != NULL, &quot;archived obj is NULL&quot;);
 831   assert(G1ArchiveAllocator::is_archived_object(obj), &quot;must be archived object&quot;);
 832 
 833   // Loading an archived object makes it strongly reachable. If it is
 834   // loaded during concurrent marking, it must be enqueued to the SATB
 835   // queue, shading the previously white object gray.
 836   G1BarrierSet::enqueue(obj);
 837 
 838   return obj;
 839 }
</pre>
<hr />
<pre>
 860   // need to start a marking cycle at each humongous object allocation. We do
 861   // the check before we do the actual allocation. The reason for doing it
 862   // before the allocation is that we avoid having to keep track of the newly
 863   // allocated memory while we do a GC.
 864   if (policy()-&gt;need_to_start_conc_mark(&quot;concurrent humongous allocation&quot;,
 865                                            word_size)) {
 866     collect(GCCause::_g1_humongous_allocation);
 867   }
 868 
 869   // We will loop until a) we manage to successfully perform the
 870   // allocation or b) we successfully schedule a collection which
 871   // fails to perform the allocation. b) is the only case when we&#39;ll
 872   // return NULL.
 873   HeapWord* result = NULL;
 874   for (uint try_count = 1, gclocker_retry_count = 0; /* we&#39;ll return */; try_count += 1) {
 875     bool should_try_gc;
 876     uint gc_count_before;
 877 
 878 
 879     {
<span class="line-modified"> 880       MutexLockerEx x(Heap_lock);</span>
 881 
 882       // Given that humongous objects are not allocated in young
 883       // regions, we&#39;ll first try to do the allocation without doing a
 884       // collection hoping that there&#39;s enough space in the heap.
 885       result = humongous_obj_allocate(word_size);
 886       if (result != NULL) {
 887         size_t size_in_regions = humongous_obj_size_in_regions(word_size);
 888         policy()-&gt;add_bytes_allocated_in_old_since_last_gc(size_in_regions * HeapRegion::GrainBytes);
 889         return result;
 890       }
 891 
 892       // Only try a GC if the GCLocker does not signal the need for a GC. Wait until
 893       // the GCLocker initiated GC has been performed and then retry. This includes
 894       // the case when the GC Locker is not active but has not been performed.
 895       should_try_gc = !GCLocker::needs_gc();
 896       // Read the GC count while still holding the Heap_lock.
 897       gc_count_before = total_collections();
 898     }
 899 
 900     if (should_try_gc) {
</pre>
<hr />
<pre>
 998   // scanning the root regions we might trip them over as we&#39;ll
 999   // be moving objects / updating references. So let&#39;s wait until
1000   // they are done. By telling them to abort, they should complete
1001   // early.
1002   _cm-&gt;root_regions()-&gt;abort();
1003   _cm-&gt;root_regions()-&gt;wait_until_scan_finished();
1004 
1005   // Disable discovery and empty the discovered lists
1006   // for the CM ref processor.
1007   _ref_processor_cm-&gt;disable_discovery();
1008   _ref_processor_cm-&gt;abandon_partial_discovery();
1009   _ref_processor_cm-&gt;verify_no_references_recorded();
1010 
1011   // Abandon current iterations of concurrent marking and concurrent
1012   // refinement, if any are in progress.
1013   concurrent_mark()-&gt;concurrent_cycle_abort();
1014 }
1015 
1016 void G1CollectedHeap::prepare_heap_for_full_collection() {
1017   // Make sure we&#39;ll choose a new allocation region afterwards.
<span class="line-modified">1018   _allocator-&gt;release_mutator_alloc_region();</span>
1019   _allocator-&gt;abandon_gc_alloc_regions();
1020 
1021   // We may have added regions to the current incremental collection
1022   // set between the last GC or pause and now. We need to clear the
1023   // incremental collection set and then start rebuilding it afresh
1024   // after this full GC.
1025   abandon_collection_set(collection_set());
1026 
1027   tear_down_region_sets(false /* free_list_only */);
1028 
1029   hrm()-&gt;prepare_for_full_collection_start();
1030 }
1031 
1032 void G1CollectedHeap::verify_before_full_collection(bool explicit_gc) {
1033   assert(!GCCause::is_user_requested_gc(gc_cause()) || explicit_gc, &quot;invariant&quot;);
<span class="line-modified">1034   assert(used() == recalculate_used(), &quot;Should be equal&quot;);</span>
1035   _verifier-&gt;verify_region_sets_optional();
1036   _verifier-&gt;verify_before_gc(G1HeapVerifier::G1VerifyFull);
1037   _verifier-&gt;check_bitmaps(&quot;Full GC Start&quot;);
1038 }
1039 
1040 void G1CollectedHeap::prepare_heap_for_mutators() {
1041   hrm()-&gt;prepare_for_full_collection_end();
1042 
1043   // Delete metaspaces for unloaded class loaders and clean up loader_data graph
1044   ClassLoaderDataGraph::purge();
1045   MetaspaceUtils::verify_metrics();
1046 
1047   // Prepare heap for normal collections.
1048   assert(num_free_regions() == 0, &quot;we should not have added any free regions&quot;);
1049   rebuild_region_sets(false /* free_list_only */);
1050   abort_refinement();
1051   resize_heap_if_necessary();
1052 
1053   // Rebuild the strong code root lists for each region
1054   rebuild_strong_code_roots();
1055 
1056   // Purge code root memory
1057   purge_code_root_memory();
1058 
1059   // Start a new incremental collection set for the next pause
1060   start_new_collection_set();
1061 
<span class="line-modified">1062   _allocator-&gt;init_mutator_alloc_region();</span>
1063 
1064   // Post collection state updates.
1065   MetaspaceGC::compute_new_size();
1066 }
1067 
1068 void G1CollectedHeap::abort_refinement() {
1069   if (_hot_card_cache-&gt;use_cache()) {
1070     _hot_card_cache-&gt;reset_hot_cache();
1071   }
1072 
1073   // Discard all remembered set updates.
1074   G1BarrierSet::dirty_card_queue_set().abandon_logs();
<span class="line-modified">1075   assert(dirty_card_queue_set().completed_buffers_num() == 0, &quot;DCQS should be empty&quot;);</span>

1076 }
1077 
1078 void G1CollectedHeap::verify_after_full_collection() {
1079   _hrm-&gt;verify_optional();
1080   _verifier-&gt;verify_region_sets_optional();
1081   _verifier-&gt;verify_after_gc(G1HeapVerifier::G1VerifyFull);
1082   // Clear the previous marking bitmap, if needed for bitmap verification.
1083   // Note we cannot do this when we clear the next marking bitmap in
1084   // G1ConcurrentMark::abort() above since VerifyDuringGC verifies the
1085   // objects marked during a full GC against the previous bitmap.
1086   // But we need to clear it before calling check_bitmaps below since
1087   // the full GC has compacted objects and updated TAMS but not updated
1088   // the prev bitmap.
1089   if (G1VerifyBitmaps) {
<span class="line-modified">1090     GCTraceTime(Debug, gc)(&quot;Clear Prev Bitmap for Verification&quot;);</span>
1091     _cm-&gt;clear_prev_bitmap(workers());
1092   }
1093   // This call implicitly verifies that the next bitmap is clear after Full GC.
1094   _verifier-&gt;check_bitmaps(&quot;Full GC End&quot;);
1095 
1096   // At this point there should be no regions in the
1097   // entire heap tagged as young.
1098   assert(check_young_list_empty(), &quot;young list should be empty at this point&quot;);
1099 
1100   // Note: since we&#39;ve just done a full GC, concurrent
1101   // marking is no longer active. Therefore we need not
1102   // re-enable reference discovery for the CM ref processor.
1103   // That will be done at the start of the next marking cycle.
1104   // We also know that the STW processor should no longer
1105   // discover any new references.
1106   assert(!_ref_processor_stw-&gt;discovery_enabled(), &quot;Postcondition&quot;);
1107   assert(!_ref_processor_cm-&gt;discovery_enabled(), &quot;Postcondition&quot;);
1108   _ref_processor_stw-&gt;verify_no_references_recorded();
1109   _ref_processor_cm-&gt;verify_no_references_recorded();
1110 }
1111 
1112 void G1CollectedHeap::print_heap_after_full_collection(G1HeapTransition* heap_transition) {
1113   // Post collection logging.
1114   // We should do this after we potentially resize the heap so
1115   // that all the COMMIT / UNCOMMIT events are generated before
1116   // the compaction events.
1117   print_hrm_post_compaction();
1118   heap_transition-&gt;print();
1119   print_heap_after_gc();
1120   print_heap_regions();
<span class="line-removed">1121 #ifdef TRACESPINNING</span>
<span class="line-removed">1122   ParallelTaskTerminator::print_termination_counts();</span>
<span class="line-removed">1123 #endif</span>
1124 }
1125 
1126 bool G1CollectedHeap::do_full_collection(bool explicit_gc,
1127                                          bool clear_all_soft_refs) {
1128   assert_at_safepoint_on_vm_thread();
1129 
1130   if (GCLocker::check_active_before_gc()) {
1131     // Full GC was not completed.
1132     return false;
1133   }
1134 
1135   const bool do_clear_all_soft_refs = clear_all_soft_refs ||
1136       soft_ref_policy()-&gt;should_clear_all_soft_refs();
1137 
1138   G1FullCollector collector(this, explicit_gc, do_clear_all_soft_refs);
1139   GCTraceTime(Info, gc) tm(&quot;Pause Full&quot;, NULL, gc_cause(), true);
1140 
1141   collector.prepare_collection();
1142   collector.collect();
1143   collector.complete_collection();
</pre>
<hr />
<pre>
1155 }
1156 
1157 void G1CollectedHeap::resize_heap_if_necessary() {
1158   assert_at_safepoint_on_vm_thread();
1159 
1160   // Capacity, free and used after the GC counted as full regions to
1161   // include the waste in the following calculations.
1162   const size_t capacity_after_gc = capacity();
1163   const size_t used_after_gc = capacity_after_gc - unused_committed_regions_in_bytes();
1164 
1165   // This is enforced in arguments.cpp.
1166   assert(MinHeapFreeRatio &lt;= MaxHeapFreeRatio,
1167          &quot;otherwise the code below doesn&#39;t make sense&quot;);
1168 
1169   // We don&#39;t have floating point command-line arguments
1170   const double minimum_free_percentage = (double) MinHeapFreeRatio / 100.0;
1171   const double maximum_used_percentage = 1.0 - minimum_free_percentage;
1172   const double maximum_free_percentage = (double) MaxHeapFreeRatio / 100.0;
1173   const double minimum_used_percentage = 1.0 - maximum_free_percentage;
1174 
<span class="line-removed">1175   const size_t min_heap_size = collector_policy()-&gt;min_heap_byte_size();</span>
<span class="line-removed">1176   const size_t max_heap_size = collector_policy()-&gt;max_heap_byte_size();</span>
<span class="line-removed">1177 </span>
1178   // We have to be careful here as these two calculations can overflow
1179   // 32-bit size_t&#39;s.
1180   double used_after_gc_d = (double) used_after_gc;
1181   double minimum_desired_capacity_d = used_after_gc_d / maximum_used_percentage;
1182   double maximum_desired_capacity_d = used_after_gc_d / minimum_used_percentage;
1183 
1184   // Let&#39;s make sure that they are both under the max heap size, which
1185   // by default will make them fit into a size_t.
<span class="line-modified">1186   double desired_capacity_upper_bound = (double) max_heap_size;</span>
1187   minimum_desired_capacity_d = MIN2(minimum_desired_capacity_d,
1188                                     desired_capacity_upper_bound);
1189   maximum_desired_capacity_d = MIN2(maximum_desired_capacity_d,
1190                                     desired_capacity_upper_bound);
1191 
1192   // We can now safely turn them into size_t&#39;s.
1193   size_t minimum_desired_capacity = (size_t) minimum_desired_capacity_d;
1194   size_t maximum_desired_capacity = (size_t) maximum_desired_capacity_d;
1195 
1196   // This assert only makes sense here, before we adjust them
1197   // with respect to the min and max heap size.
1198   assert(minimum_desired_capacity &lt;= maximum_desired_capacity,
1199          &quot;minimum_desired_capacity = &quot; SIZE_FORMAT &quot;, &quot;
1200          &quot;maximum_desired_capacity = &quot; SIZE_FORMAT,
1201          minimum_desired_capacity, maximum_desired_capacity);
1202 
1203   // Should not be greater than the heap max size. No need to adjust
1204   // it with respect to the heap min size as it&#39;s a lower bound (i.e.,
1205   // we&#39;ll try to make the capacity larger than it, not smaller).
<span class="line-modified">1206   minimum_desired_capacity = MIN2(minimum_desired_capacity, max_heap_size);</span>
1207   // Should not be less than the heap min size. No need to adjust it
1208   // with respect to the heap max size as it&#39;s an upper bound (i.e.,
1209   // we&#39;ll try to make the capacity smaller than it, not greater).
<span class="line-modified">1210   maximum_desired_capacity =  MAX2(maximum_desired_capacity, min_heap_size);</span>
1211 
1212   if (capacity_after_gc &lt; minimum_desired_capacity) {
1213     // Don&#39;t expand unless it&#39;s significant
1214     size_t expand_bytes = minimum_desired_capacity - capacity_after_gc;
1215 
1216     log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (capacity lower than min desired capacity). &quot;
1217                               &quot;Capacity: &quot; SIZE_FORMAT &quot;B occupancy: &quot; SIZE_FORMAT &quot;B live: &quot; SIZE_FORMAT &quot;B &quot;
1218                               &quot;min_desired_capacity: &quot; SIZE_FORMAT &quot;B (&quot; UINTX_FORMAT &quot; %%)&quot;,
1219                               capacity_after_gc, used_after_gc, used(), minimum_desired_capacity, MinHeapFreeRatio);
1220 
1221     expand(expand_bytes, _workers);
1222 
1223     // No expansion, now see if we want to shrink
1224   } else if (capacity_after_gc &gt; maximum_desired_capacity) {
1225     // Capacity too large, compute shrinking size
1226     size_t shrink_bytes = capacity_after_gc - maximum_desired_capacity;
1227 
1228     log_debug(gc, ergo, heap)(&quot;Attempt heap shrinking (capacity higher than max desired capacity). &quot;
1229                               &quot;Capacity: &quot; SIZE_FORMAT &quot;B occupancy: &quot; SIZE_FORMAT &quot;B live: &quot; SIZE_FORMAT &quot;B &quot;
1230                               &quot;maximum_desired_capacity: &quot; SIZE_FORMAT &quot;B (&quot; UINTX_FORMAT &quot; %%)&quot;,
</pre>
<hr />
<pre>
1361   }
1362 
1363   if (expanded_by &gt; 0) {
1364     size_t actual_expand_bytes = expanded_by * HeapRegion::GrainBytes;
1365     assert(actual_expand_bytes &lt;= aligned_expand_bytes, &quot;post-condition&quot;);
1366     policy()-&gt;record_new_heap_size(num_regions());
1367   } else {
1368     log_debug(gc, ergo, heap)(&quot;Did not expand the heap (heap expansion operation failed)&quot;);
1369 
1370     // The expansion of the virtual storage space was unsuccessful.
1371     // Let&#39;s see if it was because we ran out of swap.
1372     if (G1ExitOnExpansionFailure &amp;&amp;
1373         _hrm-&gt;available() &gt;= regions_to_expand) {
1374       // We had head room...
1375       vm_exit_out_of_memory(aligned_expand_bytes, OOM_MMAP_ERROR, &quot;G1 heap expansion&quot;);
1376     }
1377   }
1378   return regions_to_expand &gt; 0;
1379 }
1380 













1381 void G1CollectedHeap::shrink_helper(size_t shrink_bytes) {
1382   size_t aligned_shrink_bytes =
1383     ReservedSpace::page_align_size_down(shrink_bytes);
1384   aligned_shrink_bytes = align_down(aligned_shrink_bytes,
1385                                          HeapRegion::GrainBytes);
1386   uint num_regions_to_remove = (uint)(shrink_bytes / HeapRegion::GrainBytes);
1387 
1388   uint num_regions_removed = _hrm-&gt;shrink_by(num_regions_to_remove);
1389   size_t shrunk_bytes = num_regions_removed * HeapRegion::GrainBytes;
1390 
<span class="line-removed">1391 </span>
1392   log_debug(gc, ergo, heap)(&quot;Shrink the heap. requested shrinking amount: &quot; SIZE_FORMAT &quot;B aligned shrinking amount: &quot; SIZE_FORMAT &quot;B attempted shrinking amount: &quot; SIZE_FORMAT &quot;B&quot;,
1393                             shrink_bytes, aligned_shrink_bytes, shrunk_bytes);
1394   if (num_regions_removed &gt; 0) {
1395     policy()-&gt;record_new_heap_size(num_regions());
1396   } else {
1397     log_debug(gc, ergo, heap)(&quot;Did not expand the heap (heap shrinking operation failed)&quot;);
1398   }
1399 }
1400 
1401 void G1CollectedHeap::shrink(size_t shrink_bytes) {
1402   _verifier-&gt;verify_region_sets_optional();
1403 
1404   // We should only reach here at the end of a Full GC or during Remark which
1405   // means we should not not be holding to any GC alloc regions. The method
1406   // below will make sure of that and do any remaining clean up.
1407   _allocator-&gt;abandon_gc_alloc_regions();
1408 
1409   // Instead of tearing down / rebuilding the free lists here, we
1410   // could instead use the remove_all_pending() method on free_list to
1411   // remove only the ones that we need to remove.
</pre>
<hr />
<pre>
1462     // (a) If we&#39;re at a safepoint, operations on the master humongous
1463     // set should be invoked by either the VM thread (which will
1464     // serialize them) or by the GC workers while holding the
1465     // OldSets_lock.
1466     // (b) If we&#39;re not at a safepoint, operations on the master
1467     // humongous set should be invoked while holding the Heap_lock.
1468 
1469     if (SafepointSynchronize::is_at_safepoint()) {
1470       guarantee(Thread::current()-&gt;is_VM_thread() ||
1471                 OldSets_lock-&gt;owned_by_self(),
1472                 &quot;master humongous set MT safety protocol at a safepoint&quot;);
1473     } else {
1474       guarantee(Heap_lock-&gt;owned_by_self(),
1475                 &quot;master humongous set MT safety protocol outside a safepoint&quot;);
1476     }
1477   }
1478   bool is_correct_type(HeapRegion* hr) { return hr-&gt;is_humongous(); }
1479   const char* get_description() { return &quot;Humongous Regions&quot;; }
1480 };
1481 
<span class="line-modified">1482 G1CollectedHeap::G1CollectedHeap(G1CollectorPolicy* collector_policy) :</span>
1483   CollectedHeap(),
1484   _young_gen_sampling_thread(NULL),
1485   _workers(NULL),
<span class="line-removed">1486   _collector_policy(collector_policy),</span>
1487   _card_table(NULL),
1488   _soft_ref_policy(),
1489   _old_set(&quot;Old Region Set&quot;, new OldRegionSetChecker()),
1490   _archive_set(&quot;Archive Region Set&quot;, new ArchiveRegionSetChecker()),
1491   _humongous_set(&quot;Humongous Region Set&quot;, new HumongousRegionSetChecker()),
1492   _bot(NULL),
1493   _listener(),

1494   _hrm(NULL),
1495   _allocator(NULL),
1496   _verifier(NULL),
1497   _summary_bytes_used(0),

1498   _archive_allocator(NULL),
1499   _survivor_evac_stats(&quot;Young&quot;, YoungPLABSize, PLABWeight),
1500   _old_evac_stats(&quot;Old&quot;, OldPLABSize, PLABWeight),
1501   _expand_heap_after_alloc_failure(true),
1502   _g1mm(NULL),
1503   _humongous_reclaim_candidates(),
1504   _has_humongous_reclaim_candidates(false),
1505   _hr_printer(),
1506   _collector_state(),
1507   _old_marking_cycles_started(0),
1508   _old_marking_cycles_completed(0),
1509   _eden(),
1510   _survivor(),
1511   _gc_timer_stw(new (ResourceObj::C_HEAP, mtGC) STWGCTimer()),
1512   _gc_tracer_stw(new (ResourceObj::C_HEAP, mtGC) G1NewTracer()),
<span class="line-modified">1513   _policy(G1Policy::create_policy(collector_policy, _gc_timer_stw)),</span>
1514   _heap_sizing_policy(NULL),
1515   _collection_set(this, _policy),
1516   _hot_card_cache(NULL),
1517   _rem_set(NULL),
<span class="line-removed">1518   _dirty_card_queue_set(false),</span>
1519   _cm(NULL),
1520   _cm_thread(NULL),
1521   _cr(NULL),
1522   _task_queues(NULL),
1523   _evacuation_failed(false),
1524   _evacuation_failed_info_array(NULL),
1525   _preserved_marks_set(true /* in_c_heap */),
1526 #ifndef PRODUCT
1527   _evacuation_failure_alot_for_current_gc(false),
1528   _evacuation_failure_alot_gc_number(0),
1529   _evacuation_failure_alot_count(0),
1530 #endif
1531   _ref_processor_stw(NULL),
1532   _is_alive_closure_stw(this),
1533   _is_subject_to_discovery_stw(this),
1534   _ref_processor_cm(NULL),
1535   _is_alive_closure_cm(this),
1536   _is_subject_to_discovery_cm(this),
<span class="line-modified">1537   _in_cset_fast_test() {</span>
1538 
1539   _verifier = new G1HeapVerifier(this);
1540 
1541   _allocator = new G1Allocator(this);
1542 
1543   _heap_sizing_policy = G1HeapSizingPolicy::create(this, _policy-&gt;analytics());
1544 
1545   _humongous_object_threshold_in_words = humongous_threshold_for(HeapRegion::GrainWords);
1546 
1547   // Override the default _filler_array_max_size so that no humongous filler
1548   // objects are created.
1549   _filler_array_max_size = _humongous_object_threshold_in_words;
1550 
1551   uint n_queues = ParallelGCThreads;
1552   _task_queues = new RefToScanQueueSet(n_queues);
1553 
1554   _evacuation_failed_info_array = NEW_C_HEAP_ARRAY(EvacuationFailedInfo, n_queues, mtGC);
1555 
1556   for (uint i = 0; i &lt; n_queues; i++) {
1557     RefToScanQueue* q = new RefToScanQueue();
1558     q-&gt;initialize();
1559     _task_queues-&gt;register_queue(i, q);
1560     ::new (&amp;_evacuation_failed_info_array[i]) EvacuationFailedInfo();
1561   }
1562 
1563   // Initialize the G1EvacuationFailureALot counters and flags.
1564   NOT_PRODUCT(reset_evacuation_should_fail();)

1565 
1566   guarantee(_task_queues != NULL, &quot;task_queues allocation failure.&quot;);
1567 }
1568 
1569 static size_t actual_reserved_page_size(ReservedSpace rs) {
1570   size_t page_size = os::vm_page_size();
1571   if (UseLargePages) {
1572     // There are two ways to manage large page memory.
1573     // 1. OS supports committing large page memory.
1574     // 2. OS doesn&#39;t support committing large page memory so ReservedSpace manages it.
1575     //    And ReservedSpace calls it &#39;special&#39;. If we failed to set &#39;special&#39;,
1576     //    we reserved memory without large page.
1577     if (os::can_commit_large_page_memory() || rs.special()) {
<span class="line-modified">1578       page_size = rs.alignment();</span>



1579     }
1580   }
1581 
1582   return page_size;
1583 }
1584 
1585 G1RegionToSpaceMapper* G1CollectedHeap::create_aux_memory_mapper(const char* description,
1586                                                                  size_t size,
1587                                                                  size_t translation_factor) {
1588   size_t preferred_page_size = os::page_size_for_region_unaligned(size, 1);
1589   // Allocate a new reserved space, preferring to use large pages.
1590   ReservedSpace rs(size, preferred_page_size);
1591   size_t page_size = actual_reserved_page_size(rs);
1592   G1RegionToSpaceMapper* result  =
1593     G1RegionToSpaceMapper::create_mapper(rs,
1594                                          size,
1595                                          page_size,
1596                                          HeapRegion::GrainBytes,
1597                                          translation_factor,
1598                                          mtGC);
</pre>
<hr />
<pre>
1606 
1607   return result;
1608 }
1609 
1610 jint G1CollectedHeap::initialize_concurrent_refinement() {
1611   jint ecode = JNI_OK;
1612   _cr = G1ConcurrentRefine::create(&amp;ecode);
1613   return ecode;
1614 }
1615 
1616 jint G1CollectedHeap::initialize_young_gen_sampling_thread() {
1617   _young_gen_sampling_thread = new G1YoungRemSetSamplingThread();
1618   if (_young_gen_sampling_thread-&gt;osthread() == NULL) {
1619     vm_shutdown_during_initialization(&quot;Could not create G1YoungRemSetSamplingThread&quot;);
1620     return JNI_ENOMEM;
1621   }
1622   return JNI_OK;
1623 }
1624 
1625 jint G1CollectedHeap::initialize() {
<span class="line-removed">1626   os::enable_vtime();</span>
1627 
1628   // Necessary to satisfy locking discipline assertions.
1629 
1630   MutexLocker x(Heap_lock);
1631 
1632   // While there are no constraints in the GC code that HeapWordSize
1633   // be any particular value, there are multiple other areas in the
1634   // system which believe this to be true (e.g. oop-&gt;object_size in some
1635   // cases incorrectly returns the size in wordSize units rather than
1636   // HeapWordSize).
1637   guarantee(HeapWordSize == wordSize, &quot;HeapWordSize must equal wordSize&quot;);
1638 
<span class="line-modified">1639   size_t init_byte_size = collector_policy()-&gt;initial_heap_byte_size();</span>
<span class="line-modified">1640   size_t max_byte_size = _collector_policy-&gt;heap_reserved_size_bytes();</span>
<span class="line-removed">1641   size_t heap_alignment = collector_policy()-&gt;heap_alignment();</span>
1642 
1643   // Ensure that the sizes are properly aligned.
1644   Universe::check_alignment(init_byte_size, HeapRegion::GrainBytes, &quot;g1 heap&quot;);
<span class="line-modified">1645   Universe::check_alignment(max_byte_size, HeapRegion::GrainBytes, &quot;g1 heap&quot;);</span>
<span class="line-modified">1646   Universe::check_alignment(max_byte_size, heap_alignment, &quot;g1 heap&quot;);</span>
1647 
1648   // Reserve the maximum.
1649 
1650   // When compressed oops are enabled, the preferred heap base
1651   // is calculated by subtracting the requested size from the
1652   // 32Gb boundary and using the result as the base address for
1653   // heap reservation. If the requested size is not aligned to
1654   // HeapRegion::GrainBytes (i.e. the alignment that is passed
1655   // into the ReservedHeapSpace constructor) then the actual
1656   // base of the reserved heap may end up differing from the
1657   // address that was requested (i.e. the preferred heap base).
1658   // If this happens then we could end up using a non-optimal
1659   // compressed oops mode.
1660 
<span class="line-modified">1661   ReservedSpace heap_rs = Universe::reserve_heap(max_byte_size,</span>
<span class="line-modified">1662                                                  heap_alignment);</span>
1663 
<span class="line-modified">1664   initialize_reserved_region((HeapWord*)heap_rs.base(), (HeapWord*)(heap_rs.base() + heap_rs.size()));</span>
1665 
1666   // Create the barrier set for the entire reserved region.
<span class="line-modified">1667   G1CardTable* ct = new G1CardTable(reserved_region());</span>
1668   ct-&gt;initialize();
1669   G1BarrierSet* bs = new G1BarrierSet(ct);
1670   bs-&gt;initialize();
1671   assert(bs-&gt;is_a(BarrierSet::G1BarrierSet), &quot;sanity&quot;);
1672   BarrierSet::set_barrier_set(bs);
1673   _card_table = ct;
1674 
<span class="line-modified">1675   G1BarrierSet::satb_mark_queue_set().initialize(this,</span>
<span class="line-modified">1676                                                  SATB_Q_CBL_mon,</span>
<span class="line-modified">1677                                                  &amp;bs-&gt;satb_mark_queue_buffer_allocator(),</span>
<span class="line-modified">1678                                                  G1SATBProcessCompletedThreshold,</span>
<span class="line-modified">1679                                                  G1SATBBufferEnqueueingThresholdPercent);</span>
<span class="line-removed">1680 </span>
<span class="line-removed">1681   // process_completed_buffers_threshold and max_completed_buffers are updated</span>
<span class="line-removed">1682   // later, based on the concurrent refinement object.</span>
<span class="line-removed">1683   G1BarrierSet::dirty_card_queue_set().initialize(DirtyCardQ_CBL_mon,</span>
<span class="line-removed">1684                                                   &amp;bs-&gt;dirty_card_queue_buffer_allocator(),</span>
<span class="line-removed">1685                                                   Shared_DirtyCardQ_lock,</span>
<span class="line-removed">1686                                                   true); // init_free_ids</span>
<span class="line-removed">1687 </span>
<span class="line-removed">1688   dirty_card_queue_set().initialize(DirtyCardQ_CBL_mon,</span>
<span class="line-removed">1689                                     &amp;bs-&gt;dirty_card_queue_buffer_allocator(),</span>
<span class="line-removed">1690                                     Shared_DirtyCardQ_lock);</span>
1691 
1692   // Create the hot card cache.
1693   _hot_card_cache = new G1HotCardCache(this);
1694 
1695   // Carve out the G1 part of the heap.
<span class="line-modified">1696   ReservedSpace g1_rs = heap_rs.first_part(max_byte_size);</span>
1697   size_t page_size = actual_reserved_page_size(heap_rs);
1698   G1RegionToSpaceMapper* heap_storage =
1699     G1RegionToSpaceMapper::create_heap_mapper(g1_rs,
1700                                               g1_rs.size(),
1701                                               page_size,
1702                                               HeapRegion::GrainBytes,
1703                                               1,
1704                                               mtJavaHeap);
1705   if(heap_storage == NULL) {
1706     vm_shutdown_during_initialization(&quot;Could not initialize G1 heap&quot;);
1707     return JNI_ERR;
1708   }
1709 
1710   os::trace_page_sizes(&quot;Heap&quot;,
<span class="line-modified">1711                        collector_policy()-&gt;min_heap_byte_size(),</span>
<span class="line-modified">1712                        max_byte_size,</span>
1713                        page_size,
1714                        heap_rs.base(),
1715                        heap_rs.size());
1716   heap_storage-&gt;set_mapping_changed_listener(&amp;_listener);
1717 
1718   // Create storage for the BOT, card table, card counts table (hot card cache) and the bitmaps.
1719   G1RegionToSpaceMapper* bot_storage =
1720     create_aux_memory_mapper(&quot;Block Offset Table&quot;,
1721                              G1BlockOffsetTable::compute_size(g1_rs.size() / HeapWordSize),
1722                              G1BlockOffsetTable::heap_map_factor());
1723 
1724   G1RegionToSpaceMapper* cardtable_storage =
1725     create_aux_memory_mapper(&quot;Card Table&quot;,
1726                              G1CardTable::compute_size(g1_rs.size() / HeapWordSize),
1727                              G1CardTable::heap_map_factor());
1728 
1729   G1RegionToSpaceMapper* card_counts_storage =
1730     create_aux_memory_mapper(&quot;Card Counts Table&quot;,
1731                              G1CardCounts::compute_size(g1_rs.size() / HeapWordSize),
1732                              G1CardCounts::heap_map_factor());
1733 
1734   size_t bitmap_size = G1CMBitMap::compute_size(g1_rs.size());
1735   G1RegionToSpaceMapper* prev_bitmap_storage =
1736     create_aux_memory_mapper(&quot;Prev Bitmap&quot;, bitmap_size, G1CMBitMap::heap_map_factor());
1737   G1RegionToSpaceMapper* next_bitmap_storage =
1738     create_aux_memory_mapper(&quot;Next Bitmap&quot;, bitmap_size, G1CMBitMap::heap_map_factor());
1739 
<span class="line-modified">1740   _hrm = HeapRegionManager::create_manager(this, _collector_policy);</span>
1741 
1742   _hrm-&gt;initialize(heap_storage, prev_bitmap_storage, next_bitmap_storage, bot_storage, cardtable_storage, card_counts_storage);
1743   _card_table-&gt;initialize(cardtable_storage);

1744   // Do later initialization work for concurrent refinement.
1745   _hot_card_cache-&gt;initialize(card_counts_storage);
1746 
1747   // 6843694 - ensure that the maximum region index can fit
1748   // in the remembered set structures.
1749   const uint max_region_idx = (1U &lt;&lt; (sizeof(RegionIdx_t)*BitsPerByte-1)) - 1;
1750   guarantee((max_regions() - 1) &lt;= max_region_idx, &quot;too many regions&quot;);
1751 
1752   // The G1FromCardCache reserves card with value 0 as &quot;invalid&quot;, so the heap must not
1753   // start within the first card.
1754   guarantee(g1_rs.base() &gt;= (char*)G1CardTable::card_size, &quot;Java heap must not start within the first card.&quot;);
1755   // Also create a G1 rem set.
1756   _rem_set = new G1RemSet(this, _card_table, _hot_card_cache);
1757   _rem_set-&gt;initialize(max_reserved_capacity(), max_regions());
1758 
1759   size_t max_cards_per_region = ((size_t)1 &lt;&lt; (sizeof(CardIdx_t)*BitsPerByte-1)) - 1;
1760   guarantee(HeapRegion::CardsPerRegion &gt; 0, &quot;make sure it&#39;s initialized&quot;);
1761   guarantee(HeapRegion::CardsPerRegion &lt; max_cards_per_region,
1762             &quot;too many cards per region&quot;);
1763 
1764   FreeRegionList::set_unrealistically_long_length(max_expandable_regions() + 1);
1765 
1766   _bot = new G1BlockOffsetTable(reserved_region(), bot_storage);
1767 
1768   {
1769     HeapWord* start = _hrm-&gt;reserved().start();
1770     HeapWord* end = _hrm-&gt;reserved().end();
1771     size_t granularity = HeapRegion::GrainBytes;
1772 
<span class="line-modified">1773     _in_cset_fast_test.initialize(start, end, granularity);</span>
1774     _humongous_reclaim_candidates.initialize(start, end, granularity);
1775   }
1776 
1777   _workers = new WorkGang(&quot;GC Thread&quot;, ParallelGCThreads,
1778                           true /* are_GC_task_threads */,
1779                           false /* are_ConcurrentGC_threads */);
1780   if (_workers == NULL) {
1781     return JNI_ENOMEM;
1782   }
1783   _workers-&gt;initialize_workers();
1784 


1785   // Create the G1ConcurrentMark data structure and thread.
1786   // (Must do this late, so that &quot;max_regions&quot; is defined.)
1787   _cm = new G1ConcurrentMark(this, prev_bitmap_storage, next_bitmap_storage);
<span class="line-modified">1788   if (_cm == NULL || !_cm-&gt;completed_initialization()) {</span>
<span class="line-modified">1789     vm_shutdown_during_initialization(&quot;Could not create/initialize G1ConcurrentMark&quot;);</span>
1790     return JNI_ENOMEM;
1791   }
1792   _cm_thread = _cm-&gt;cm_thread();
1793 
1794   // Now expand into the initial heap size.
1795   if (!expand(init_byte_size, _workers)) {
1796     vm_shutdown_during_initialization(&quot;Failed to allocate initial heap.&quot;);
1797     return JNI_ENOMEM;
1798   }
1799 
1800   // Perform any initialization actions delegated to the policy.
1801   policy()-&gt;init(this, &amp;_collection_set);
1802 
1803   jint ecode = initialize_concurrent_refinement();
1804   if (ecode != JNI_OK) {
1805     return ecode;
1806   }
1807 
1808   ecode = initialize_young_gen_sampling_thread();
1809   if (ecode != JNI_OK) {
1810     return ecode;
1811   }
1812 
1813   {
1814     G1DirtyCardQueueSet&amp; dcqs = G1BarrierSet::dirty_card_queue_set();
<span class="line-modified">1815     dcqs.set_process_completed_buffers_threshold(concurrent_refine()-&gt;yellow_zone());</span>
<span class="line-modified">1816     dcqs.set_max_completed_buffers(concurrent_refine()-&gt;red_zone());</span>
1817   }
1818 
1819   // Here we allocate the dummy HeapRegion that is required by the
1820   // G1AllocRegion class.
1821   HeapRegion* dummy_region = _hrm-&gt;get_dummy_region();
1822 
1823   // We&#39;ll re-use the same region whether the alloc region will
1824   // require BOT updates or not and, if it doesn&#39;t, then a non-young
1825   // region will complain that it cannot support allocations without
1826   // BOT updates. So we&#39;ll tag the dummy region as eden to avoid that.
1827   dummy_region-&gt;set_eden();
1828   // Make sure it&#39;s full.
1829   dummy_region-&gt;set_top(dummy_region-&gt;end());
1830   G1AllocRegion::setup(this, dummy_region);
1831 
<span class="line-modified">1832   _allocator-&gt;init_mutator_alloc_region();</span>
1833 
1834   // Do create of the monitoring and management support so that
1835   // values in the heap have been properly initialized.
1836   _g1mm = new G1MonitoringSupport(this);
1837 
1838   G1StringDedup::initialize();
1839 
1840   _preserved_marks_set.init(ParallelGCThreads);
1841 
1842   _collection_set.initialize(max_regions());
1843 
1844   return JNI_OK;
1845 }
1846 
1847 void G1CollectedHeap::stop() {
1848   // Stop all concurrent threads. We do this to make sure these threads
1849   // do not continue to execute and access resources (e.g. logging)
1850   // that are destroyed during shutdown.
1851   _cr-&gt;stop();
1852   _young_gen_sampling_thread-&gt;stop();
1853   _cm_thread-&gt;stop();
1854   if (G1StringDedup::is_enabled()) {
1855     G1StringDedup::stop();
1856   }
1857 }
1858 
1859 void G1CollectedHeap::safepoint_synchronize_begin() {
1860   SuspendibleThreadSet::synchronize();
1861 }
1862 
1863 void G1CollectedHeap::safepoint_synchronize_end() {
1864   SuspendibleThreadSet::desynchronize();
1865 }
1866 
<span class="line-removed">1867 size_t G1CollectedHeap::conservative_max_heap_alignment() {</span>
<span class="line-removed">1868   return HeapRegion::max_region_size();</span>
<span class="line-removed">1869 }</span>
<span class="line-removed">1870 </span>
1871 void G1CollectedHeap::post_initialize() {
1872   CollectedHeap::post_initialize();
1873   ref_processing_init();
1874 }
1875 
1876 void G1CollectedHeap::ref_processing_init() {
1877   // Reference processing in G1 currently works as follows:
1878   //
1879   // * There are two reference processor instances. One is
1880   //   used to record and process discovered references
1881   //   during concurrent marking; the other is used to
1882   //   record and process references during STW pauses
1883   //   (both full and incremental).
1884   // * Both ref processors need to &#39;span&#39; the entire heap as
1885   //   the regions in the collection set may be dotted around.
1886   //
1887   // * For the concurrent marking ref processor:
1888   //   * Reference discovery is enabled at initial marking.
1889   //   * Reference discovery is disabled and the discovered
1890   //     references processed etc during remarking.
</pre>
<hr />
<pre>
1917                            mt_processing,                                  // mt processing
1918                            ParallelGCThreads,                              // degree of mt processing
1919                            (ParallelGCThreads &gt; 1) || (ConcGCThreads &gt; 1), // mt discovery
1920                            MAX2(ParallelGCThreads, ConcGCThreads),         // degree of mt discovery
1921                            false,                                          // Reference discovery is not atomic
1922                            &amp;_is_alive_closure_cm,                          // is alive closure
1923                            true);                                          // allow changes to number of processing threads
1924 
1925   // STW ref processor
1926   _ref_processor_stw =
1927     new ReferenceProcessor(&amp;_is_subject_to_discovery_stw,
1928                            mt_processing,                        // mt processing
1929                            ParallelGCThreads,                    // degree of mt processing
1930                            (ParallelGCThreads &gt; 1),              // mt discovery
1931                            ParallelGCThreads,                    // degree of mt discovery
1932                            true,                                 // Reference discovery is atomic
1933                            &amp;_is_alive_closure_stw,               // is alive closure
1934                            true);                                // allow changes to number of processing threads
1935 }
1936 
<span class="line-removed">1937 CollectorPolicy* G1CollectedHeap::collector_policy() const {</span>
<span class="line-removed">1938   return _collector_policy;</span>
<span class="line-removed">1939 }</span>
<span class="line-removed">1940 </span>
1941 SoftRefPolicy* G1CollectedHeap::soft_ref_policy() {
1942   return &amp;_soft_ref_policy;
1943 }
1944 
1945 size_t G1CollectedHeap::capacity() const {
1946   return _hrm-&gt;length() * HeapRegion::GrainBytes;
1947 }
1948 
1949 size_t G1CollectedHeap::unused_committed_regions_in_bytes() const {
1950   return _hrm-&gt;total_free_bytes();
1951 }
1952 
<span class="line-modified">1953 void G1CollectedHeap::iterate_hcc_closure(G1CardTableEntryClosure* cl, uint worker_i) {</span>
<span class="line-modified">1954   _hot_card_cache-&gt;drain(cl, worker_i);</span>
<span class="line-removed">1955 }</span>
<span class="line-removed">1956 </span>
<span class="line-removed">1957 void G1CollectedHeap::iterate_dirty_card_closure(G1CardTableEntryClosure* cl, uint worker_i) {</span>
<span class="line-removed">1958   G1DirtyCardQueueSet&amp; dcqs = G1BarrierSet::dirty_card_queue_set();</span>
<span class="line-removed">1959   size_t n_completed_buffers = 0;</span>
<span class="line-removed">1960   while (dcqs.apply_closure_during_gc(cl, worker_i)) {</span>
<span class="line-removed">1961     n_completed_buffers++;</span>
<span class="line-removed">1962   }</span>
<span class="line-removed">1963   assert(dcqs.completed_buffers_num() == 0, &quot;Completed buffers exist!&quot;);</span>
<span class="line-removed">1964   phase_times()-&gt;record_thread_work_item(G1GCPhaseTimes::UpdateRS, worker_i, n_completed_buffers, G1GCPhaseTimes::UpdateRSProcessedBuffers);</span>
1965 }
1966 
1967 // Computes the sum of the storage used by the various regions.
1968 size_t G1CollectedHeap::used() const {
1969   size_t result = _summary_bytes_used + _allocator-&gt;used_in_alloc_regions();
1970   if (_archive_allocator != NULL) {
1971     result += _archive_allocator-&gt;used();
1972   }
1973   return result;
1974 }
1975 
1976 size_t G1CollectedHeap::used_unlocked() const {
1977   return _summary_bytes_used;
1978 }
1979 
1980 class SumUsedClosure: public HeapRegionClosure {
1981   size_t _used;
1982 public:
1983   SumUsedClosure() : _used(0) {}
1984   bool do_heap_region(HeapRegion* r) {
</pre>
<hr />
<pre>
1988   size_t result() { return _used; }
1989 };
1990 
1991 size_t G1CollectedHeap::recalculate_used() const {
1992   SumUsedClosure blk;
1993   heap_region_iterate(&amp;blk);
1994   return blk.result();
1995 }
1996 
1997 bool  G1CollectedHeap::is_user_requested_concurrent_full_gc(GCCause::Cause cause) {
1998   switch (cause) {
1999     case GCCause::_java_lang_system_gc:                 return ExplicitGCInvokesConcurrent;
2000     case GCCause::_dcmd_gc_run:                         return ExplicitGCInvokesConcurrent;
2001     case GCCause::_wb_conc_mark:                        return true;
2002     default :                                           return false;
2003   }
2004 }
2005 
2006 bool G1CollectedHeap::should_do_concurrent_full_gc(GCCause::Cause cause) {
2007   switch (cause) {
<span class="line-removed">2008     case GCCause::_gc_locker:               return GCLockerInvokesConcurrent;</span>
2009     case GCCause::_g1_humongous_allocation: return true;
2010     case GCCause::_g1_periodic_collection:  return G1PeriodicGCInvokesConcurrent;
2011     default:                                return is_user_requested_concurrent_full_gc(cause);
2012   }
2013 }
2014 
2015 bool G1CollectedHeap::should_upgrade_to_full_gc(GCCause::Cause cause) {
<span class="line-modified">2016   if(policy()-&gt;force_upgrade_to_full()) {</span>
2017     return true;
2018   } else if (should_do_concurrent_full_gc(_gc_cause)) {
2019     return false;
2020   } else if (has_regions_left_for_allocation()) {
2021     return false;
2022   } else {
2023     return true;
2024   }
2025 }
2026 
2027 #ifndef PRODUCT
2028 void G1CollectedHeap::allocate_dummy_regions() {
2029   // Let&#39;s fill up most of the region
2030   size_t word_size = HeapRegion::GrainWords - 1024;
2031   // And as a result the region we&#39;ll allocate will be humongous.
2032   guarantee(is_humongous(word_size), &quot;sanity&quot;);
2033 
2034   // _filler_array_max_size is set to humongous object threshold
2035   // but temporarily change it to use CollectedHeap::fill_with_object().
2036   SizeTFlagSetting fs(_filler_array_max_size, word_size);
</pre>
<hr />
<pre>
2043       CollectedHeap::fill_with_object(mr);
2044     } else {
2045       // If we can&#39;t allocate once, we probably cannot allocate
2046       // again. Let&#39;s get out of the loop.
2047       break;
2048     }
2049   }
2050 }
2051 #endif // !PRODUCT
2052 
2053 void G1CollectedHeap::increment_old_marking_cycles_started() {
2054   assert(_old_marking_cycles_started == _old_marking_cycles_completed ||
2055          _old_marking_cycles_started == _old_marking_cycles_completed + 1,
2056          &quot;Wrong marking cycle count (started: %d, completed: %d)&quot;,
2057          _old_marking_cycles_started, _old_marking_cycles_completed);
2058 
2059   _old_marking_cycles_started++;
2060 }
2061 
2062 void G1CollectedHeap::increment_old_marking_cycles_completed(bool concurrent) {
<span class="line-modified">2063   MonitorLockerEx x(FullGCCount_lock, Mutex::_no_safepoint_check_flag);</span>
2064 
2065   // We assume that if concurrent == true, then the caller is a
2066   // concurrent thread that was joined the Suspendible Thread
2067   // Set. If there&#39;s ever a cheap way to check this, we should add an
2068   // assert here.
2069 
2070   // Given that this method is called at the end of a Full GC or of a
2071   // concurrent cycle, and those can be nested (i.e., a Full GC can
2072   // interrupt a concurrent cycle), the number of full collections
2073   // completed should be either one (in the case where there was no
2074   // nesting) or two (when a Full GC interrupted a concurrent cycle)
2075   // behind the number of full collections started.
2076 
2077   // This is the case for the inner caller, i.e. a Full GC.
2078   assert(concurrent ||
2079          (_old_marking_cycles_started == _old_marking_cycles_completed + 1) ||
2080          (_old_marking_cycles_started == _old_marking_cycles_completed + 2),
2081          &quot;for inner caller (Full GC): _old_marking_cycles_started = %u &quot;
2082          &quot;is inconsistent with _old_marking_cycles_completed = %u&quot;,
2083          _old_marking_cycles_started, _old_marking_cycles_completed);
2084 
2085   // This is the case for the outer caller, i.e. the concurrent cycle.
2086   assert(!concurrent ||
2087          (_old_marking_cycles_started == _old_marking_cycles_completed + 1),
2088          &quot;for outer caller (concurrent cycle): &quot;
2089          &quot;_old_marking_cycles_started = %u &quot;
2090          &quot;is inconsistent with _old_marking_cycles_completed = %u&quot;,
2091          _old_marking_cycles_started, _old_marking_cycles_completed);
2092 
2093   _old_marking_cycles_completed += 1;
2094 
2095   // We need to clear the &quot;in_progress&quot; flag in the CM thread before
2096   // we wake up any waiters (especially when ExplicitInvokesConcurrent
2097   // is set) so that if a waiter requests another System.gc() it doesn&#39;t
2098   // incorrectly see that a marking cycle is still in progress.
2099   if (concurrent) {
2100     _cm_thread-&gt;set_idle();
2101   }
2102 
<span class="line-modified">2103   // This notify_all() will ensure that a thread that called</span>
<span class="line-modified">2104   // System.gc() with (with ExplicitGCInvokesConcurrent set or not)</span>
<span class="line-modified">2105   // and it&#39;s waiting for a full GC to finish will be woken up. It is</span>
<span class="line-removed">2106   // waiting in VM_G1CollectForAllocation::doit_epilogue().</span>
<span class="line-removed">2107   FullGCCount_lock-&gt;notify_all();</span>
2108 }
2109 
2110 void G1CollectedHeap::collect(GCCause::Cause cause) {
<span class="line-modified">2111   try_collect(cause, true);</span>
<span class="line-modified">2112 }</span>
<span class="line-modified">2113 </span>
<span class="line-modified">2114 bool G1CollectedHeap::try_collect(GCCause::Cause cause, bool retry_on_gc_failure) {</span>

























2115   assert_heap_not_locked();

















2116 
<span class="line-modified">2117   bool gc_succeeded;</span>
<span class="line-modified">2118   bool should_retry_gc;</span>
<span class="line-modified">2119 </span>
<span class="line-modified">2120   do {</span>
<span class="line-modified">2121     should_retry_gc = false;</span>
<span class="line-modified">2122 </span>
<span class="line-removed">2123     uint gc_count_before;</span>
<span class="line-removed">2124     uint old_marking_count_before;</span>
<span class="line-removed">2125     uint full_gc_count_before;</span>
2126 



2127     {
2128       MutexLocker ml(Heap_lock);
<span class="line-modified">2129 </span>
<span class="line-modified">2130       // Read the GC count while holding the Heap_lock</span>
<span class="line-modified">2131       gc_count_before = total_collections();</span>
<span class="line-modified">2132       full_gc_count_before = total_full_collections();</span>
<span class="line-modified">2133       old_marking_count_before = _old_marking_cycles_started;</span>





2134     }
2135 
<span class="line-modified">2136     if (should_do_concurrent_full_gc(cause)) {</span>
<span class="line-modified">2137       // Schedule an initial-mark evacuation pause that will start a</span>
<span class="line-modified">2138       // concurrent cycle. We&#39;re setting word_size to 0 which means that</span>
<span class="line-modified">2139       // we are not requesting a post-GC allocation.</span>
<span class="line-modified">2140       VM_G1CollectForAllocation op(0,     /* word_size */</span>
<span class="line-modified">2141                                    gc_count_before,</span>
<span class="line-modified">2142                                    cause,</span>
<span class="line-modified">2143                                    true,  /* should_initiate_conc_mark */</span>
<span class="line-modified">2144                                    policy()-&gt;max_pause_time_ms());</span>
<span class="line-modified">2145       VMThread::execute(&amp;op);</span>
<span class="line-modified">2146       gc_succeeded = op.gc_succeeded();</span>
<span class="line-modified">2147       if (!gc_succeeded &amp;&amp; retry_on_gc_failure) {</span>
<span class="line-modified">2148         if (old_marking_count_before == _old_marking_cycles_started) {</span>
<span class="line-modified">2149           should_retry_gc = op.should_retry_gc();</span>
<span class="line-modified">2150         } else {</span>
<span class="line-modified">2151           // A Full GC happened while we were trying to schedule the</span>
<span class="line-modified">2152           // concurrent cycle. No point in starting a new cycle given</span>
<span class="line-modified">2153           // that the whole heap was collected anyway.</span>
<span class="line-modified">2154         }</span>



























2155 
<span class="line-modified">2156         if (should_retry_gc &amp;&amp; GCLocker::is_active_and_needs_gc()) {</span>
<span class="line-modified">2157           GCLocker::stall_until_clear();</span>













2158         }
2159       }
<span class="line-modified">2160     } else {</span>
<span class="line-modified">2161       if (cause == GCCause::_gc_locker || cause == GCCause::_wb_young_gc</span>
<span class="line-modified">2162           DEBUG_ONLY(|| cause == GCCause::_scavenge_alot)) {</span>
<span class="line-modified">2163 </span>
<span class="line-modified">2164         // Schedule a standard evacuation pause. We&#39;re setting word_size</span>
<span class="line-modified">2165         // to 0 which means that we are not requesting a post-GC allocation.</span>
<span class="line-modified">2166         VM_G1CollectForAllocation op(0,     /* word_size */</span>
<span class="line-modified">2167                                      gc_count_before,</span>
<span class="line-modified">2168                                      cause,</span>
<span class="line-modified">2169                                      false, /* should_initiate_conc_mark */</span>
<span class="line-modified">2170                                      policy()-&gt;max_pause_time_ms());</span>
<span class="line-modified">2171         VMThread::execute(&amp;op);</span>
<span class="line-modified">2172         gc_succeeded = op.gc_succeeded();</span>
<span class="line-removed">2173       } else {</span>
<span class="line-removed">2174         // Schedule a Full GC.</span>
<span class="line-removed">2175         VM_G1CollectFull op(gc_count_before, full_gc_count_before, cause);</span>
<span class="line-removed">2176         VMThread::execute(&amp;op);</span>
<span class="line-removed">2177         gc_succeeded = op.gc_succeeded();</span>
2178       }
2179     }
<span class="line-modified">2180   } while (should_retry_gc);</span>
<span class="line-modified">2181   return gc_succeeded;</span>




















































2182 }
2183 
2184 bool G1CollectedHeap::is_in(const void* p) const {
2185   if (_hrm-&gt;reserved().contains(p)) {
2186     // Given that we know that p is in the reserved space,
2187     // heap_region_containing() should successfully
2188     // return the containing region.
2189     HeapRegion* hr = heap_region_containing(p);
2190     return hr-&gt;is_in(p);
2191   } else {
2192     return false;
2193   }
2194 }
2195 
2196 #ifdef ASSERT
2197 bool G1CollectedHeap::is_in_exact(const void* p) const {
2198   bool contains = reserved_region().contains(p);
2199   bool available = _hrm-&gt;is_available(addr_to_region((HeapWord*)p));
2200   if (contains &amp;&amp; available) {
2201     return true;
</pre>
<hr />
<pre>
2209 
2210 // Iterates an ObjectClosure over all objects within a HeapRegion.
2211 
2212 class IterateObjectClosureRegionClosure: public HeapRegionClosure {
2213   ObjectClosure* _cl;
2214 public:
2215   IterateObjectClosureRegionClosure(ObjectClosure* cl) : _cl(cl) {}
2216   bool do_heap_region(HeapRegion* r) {
2217     if (!r-&gt;is_continues_humongous()) {
2218       r-&gt;object_iterate(_cl);
2219     }
2220     return false;
2221   }
2222 };
2223 
2224 void G1CollectedHeap::object_iterate(ObjectClosure* cl) {
2225   IterateObjectClosureRegionClosure blk(cl);
2226   heap_region_iterate(&amp;blk);
2227 }
2228 




2229 void G1CollectedHeap::heap_region_iterate(HeapRegionClosure* cl) const {
2230   _hrm-&gt;iterate(cl);
2231 }
2232 
2233 void G1CollectedHeap::heap_region_par_iterate_from_worker_offset(HeapRegionClosure* cl,
2234                                                                  HeapRegionClaimer *hrclaimer,
2235                                                                  uint worker_id) const {
2236   _hrm-&gt;par_iterate(cl, hrclaimer, hrclaimer-&gt;offset_for_worker(worker_id));
2237 }
2238 
2239 void G1CollectedHeap::heap_region_par_iterate_from_start(HeapRegionClosure* cl,
2240                                                          HeapRegionClaimer *hrclaimer) const {
2241   _hrm-&gt;par_iterate(cl, hrclaimer, 0);
2242 }
2243 
<span class="line-modified">2244 void G1CollectedHeap::collection_set_iterate(HeapRegionClosure* cl) {</span>
2245   _collection_set.iterate(cl);
2246 }
2247 
<span class="line-modified">2248 void G1CollectedHeap::collection_set_iterate_from(HeapRegionClosure *cl, uint worker_id) {</span>
<span class="line-modified">2249   _collection_set.iterate_from(cl, worker_id, workers()-&gt;active_workers());</span>




2250 }
2251 
2252 HeapWord* G1CollectedHeap::block_start(const void* addr) const {
2253   HeapRegion* hr = heap_region_containing(addr);
2254   return hr-&gt;block_start(addr);
2255 }
2256 
2257 bool G1CollectedHeap::block_is_obj(const HeapWord* addr) const {
2258   HeapRegion* hr = heap_region_containing(addr);
2259   return hr-&gt;block_is_obj(addr);
2260 }
2261 
2262 bool G1CollectedHeap::supports_tlab_allocation() const {
2263   return true;
2264 }
2265 
2266 size_t G1CollectedHeap::tlab_capacity(Thread* ignored) const {
2267   return (_policy-&gt;young_list_target_length() - _survivor.length()) * HeapRegion::GrainBytes;
2268 }
2269 
</pre>
<hr />
<pre>
2310   }
2311 }
2312 
2313 void G1CollectedHeap::prepare_for_verify() {
2314   _verifier-&gt;prepare_for_verify();
2315 }
2316 
2317 void G1CollectedHeap::verify(VerifyOption vo) {
2318   _verifier-&gt;verify(vo);
2319 }
2320 
2321 bool G1CollectedHeap::supports_concurrent_phase_control() const {
2322   return true;
2323 }
2324 
2325 bool G1CollectedHeap::request_concurrent_phase(const char* phase) {
2326   return _cm_thread-&gt;request_concurrent_phase(phase);
2327 }
2328 
2329 bool G1CollectedHeap::is_heterogeneous_heap() const {
<span class="line-modified">2330   return _collector_policy-&gt;is_heterogeneous_heap();</span>
2331 }
2332 
2333 class PrintRegionClosure: public HeapRegionClosure {
2334   outputStream* _st;
2335 public:
2336   PrintRegionClosure(outputStream* st) : _st(st) {}
2337   bool do_heap_region(HeapRegion* r) {
2338     r-&gt;print_on(_st);
2339     return false;
2340   }
2341 };
2342 
2343 bool G1CollectedHeap::is_obj_dead_cond(const oop obj,
2344                                        const HeapRegion* hr,
2345                                        const VerifyOption vo) const {
2346   switch (vo) {
2347   case VerifyOption_G1UsePrevMarking: return is_obj_dead(obj, hr);
2348   case VerifyOption_G1UseNextMarking: return is_obj_ill(obj, hr);
2349   case VerifyOption_G1UseFullMarking: return is_obj_dead_full(obj, hr);
2350   default:                            ShouldNotReachHere();
</pre>
<hr />
<pre>
2370     print_regions_on(&amp;ls);
2371   }
2372 }
2373 
2374 void G1CollectedHeap::print_on(outputStream* st) const {
2375   st-&gt;print(&quot; %-20s&quot;, &quot;garbage-first heap&quot;);
2376   st-&gt;print(&quot; total &quot; SIZE_FORMAT &quot;K, used &quot; SIZE_FORMAT &quot;K&quot;,
2377             capacity()/K, used_unlocked()/K);
2378   st-&gt;print(&quot; [&quot; PTR_FORMAT &quot;, &quot; PTR_FORMAT &quot;)&quot;,
2379             p2i(_hrm-&gt;reserved().start()),
2380             p2i(_hrm-&gt;reserved().end()));
2381   st-&gt;cr();
2382   st-&gt;print(&quot;  region size &quot; SIZE_FORMAT &quot;K, &quot;, HeapRegion::GrainBytes / K);
2383   uint young_regions = young_regions_count();
2384   st-&gt;print(&quot;%u young (&quot; SIZE_FORMAT &quot;K), &quot;, young_regions,
2385             (size_t) young_regions * HeapRegion::GrainBytes / K);
2386   uint survivor_regions = survivor_regions_count();
2387   st-&gt;print(&quot;%u survivors (&quot; SIZE_FORMAT &quot;K)&quot;, survivor_regions,
2388             (size_t) survivor_regions * HeapRegion::GrainBytes / K);
2389   st-&gt;cr();









2390   MetaspaceUtils::print_on(st);
2391 }
2392 
2393 void G1CollectedHeap::print_regions_on(outputStream* st) const {
2394   st-&gt;print_cr(&quot;Heap Regions: E=young(eden), S=young(survivor), O=old, &quot;
2395                &quot;HS=humongous(starts), HC=humongous(continues), &quot;
<span class="line-modified">2396                &quot;CS=collection set, F=free, A=archive, &quot;</span>

2397                &quot;TAMS=top-at-mark-start (previous, next)&quot;);
2398   PrintRegionClosure blk(st);
2399   heap_region_iterate(&amp;blk);
2400 }
2401 
2402 void G1CollectedHeap::print_extended_on(outputStream* st) const {
2403   print_on(st);
2404 
2405   // Print the per-region information.
2406   print_regions_on(st);
2407 }
2408 
2409 void G1CollectedHeap::print_on_error(outputStream* st) const {
2410   this-&gt;CollectedHeap::print_on_error(st);
2411 
2412   if (_cm != NULL) {
2413     st-&gt;cr();
2414     _cm-&gt;print_on_error(st);
2415   }
2416 }
</pre>
<hr />
<pre>
2466     tty-&gt;print_cr(&quot;----------&quot;);
2467     return false;
2468   }
2469 
2470   PrintRSetsClosure(const char* msg) : _msg(msg), _occupied_sum(0) {
2471     tty-&gt;cr();
2472     tty-&gt;print_cr(&quot;========================================&quot;);
2473     tty-&gt;print_cr(&quot;%s&quot;, msg);
2474     tty-&gt;cr();
2475   }
2476 
2477   ~PrintRSetsClosure() {
2478     tty-&gt;print_cr(&quot;Occupied Sum: &quot; SIZE_FORMAT, _occupied_sum);
2479     tty-&gt;print_cr(&quot;========================================&quot;);
2480     tty-&gt;cr();
2481   }
2482 };
2483 
2484 void G1CollectedHeap::print_cset_rsets() {
2485   PrintRSetsClosure cl(&quot;Printing CSet RSets&quot;);
<span class="line-modified">2486   collection_set_iterate(&amp;cl);</span>
2487 }
2488 
2489 void G1CollectedHeap::print_all_rsets() {
2490   PrintRSetsClosure cl(&quot;Printing All RSets&quot;);;
2491   heap_region_iterate(&amp;cl);
2492 }
2493 #endif // PRODUCT
2494 




2495 G1HeapSummary G1CollectedHeap::create_g1_heap_summary() {
2496 
<span class="line-modified">2497   size_t eden_used_bytes = heap()-&gt;eden_regions_count() * HeapRegion::GrainBytes;</span>
<span class="line-modified">2498   size_t survivor_used_bytes = heap()-&gt;survivor_regions_count() * HeapRegion::GrainBytes;</span>
2499   size_t heap_used = Heap_lock-&gt;owned_by_self() ? used() : used_unlocked();
2500 
2501   size_t eden_capacity_bytes =
2502     (policy()-&gt;young_list_target_length() * HeapRegion::GrainBytes) - survivor_used_bytes;
2503 
2504   VirtualSpaceSummary heap_summary = create_heap_space_summary();
2505   return G1HeapSummary(heap_summary, heap_used, eden_used_bytes,
2506                        eden_capacity_bytes, survivor_used_bytes, num_regions());
2507 }
2508 
2509 G1EvacSummary G1CollectedHeap::create_g1_evac_summary(G1EvacStats* stats) {
2510   return G1EvacSummary(stats-&gt;allocated(), stats-&gt;wasted(), stats-&gt;undo_wasted(),
2511                        stats-&gt;unused(), stats-&gt;used(), stats-&gt;region_end_waste(),
2512                        stats-&gt;regions_filled(), stats-&gt;direct_allocated(),
2513                        stats-&gt;failure_used(), stats-&gt;failure_waste());
2514 }
2515 
2516 void G1CollectedHeap::trace_heap(GCWhen::Type when, const GCTracer* gc_tracer) {
2517   const G1HeapSummary&amp; heap_summary = create_g1_heap_summary();
2518   gc_tracer-&gt;report_gc_heap_summary(when, heap_summary);
2519 
2520   const MetaspaceSummary&amp; metaspace_summary = create_metaspace_summary();
2521   gc_tracer-&gt;report_metaspace_summary(when, metaspace_summary);
2522 }
2523 
2524 G1CollectedHeap* G1CollectedHeap::heap() {
2525   CollectedHeap* heap = Universe::heap();
2526   assert(heap != NULL, &quot;Uninitialized access to G1CollectedHeap::heap()&quot;);
2527   assert(heap-&gt;kind() == CollectedHeap::G1, &quot;Invalid name&quot;);
2528   return (G1CollectedHeap*)heap;
2529 }
2530 
2531 void G1CollectedHeap::gc_prologue(bool full) {
<span class="line-removed">2532   // always_do_update_barrier = false;</span>
2533   assert(InlineCacheBuffer::is_empty(), &quot;should have cleaned up ICBuffer&quot;);
2534 
2535   // This summary needs to be printed before incrementing total collections.
2536   rem_set()-&gt;print_periodic_summary_info(&quot;Before GC RS summary&quot;, total_collections());
2537 
2538   // Update common counters.
2539   increment_total_collections(full /* full gc */);
<span class="line-modified">2540   if (full) {</span>
2541     increment_old_marking_cycles_started();
2542   }
2543 
2544   // Fill TLAB&#39;s and such
2545   double start = os::elapsedTime();
2546   ensure_parsability(true);
2547   phase_times()-&gt;record_prepare_tlab_time_ms((os::elapsedTime() - start) * 1000.0);
2548 }
2549 
2550 void G1CollectedHeap::gc_epilogue(bool full) {
2551   // Update common counters.
2552   if (full) {
2553     // Update the number of full collections that have been completed.
2554     increment_old_marking_cycles_completed(false /* concurrent */);
2555   }
2556 
2557   // We are at the end of the GC. Total collections has already been increased.
2558   rem_set()-&gt;print_periodic_summary_info(&quot;After GC RS summary&quot;, total_collections() - 1);
2559 
2560   // FIXME: what is this about?
2561   // I&#39;m ignoring the &quot;fill_newgen()&quot; call if &quot;alloc_event_enabled&quot;
2562   // is set.
2563 #if COMPILER2_OR_JVMCI
2564   assert(DerivedPointerTable::is_empty(), &quot;derived pointer present&quot;);
2565 #endif
<span class="line-removed">2566   // always_do_update_barrier = true;</span>
2567 
2568   double start = os::elapsedTime();
2569   resize_all_tlabs();
2570   phase_times()-&gt;record_resize_tlab_time_ms((os::elapsedTime() - start) * 1000.0);
2571 
2572   MemoryService::track_memory_usage();
2573   // We have just completed a GC. Update the soft reference
2574   // policy with the new heap occupancy
2575   Universe::update_heap_info_at_gc();














2576 }
2577 
2578 HeapWord* G1CollectedHeap::do_collection_pause(size_t word_size,
2579                                                uint gc_count_before,
2580                                                bool* succeeded,
2581                                                GCCause::Cause gc_cause) {
2582   assert_heap_not_locked_and_not_at_safepoint();
2583   VM_G1CollectForAllocation op(word_size,
2584                                gc_count_before,
2585                                gc_cause,
<span class="line-removed">2586                                false, /* should_initiate_conc_mark */</span>
2587                                policy()-&gt;max_pause_time_ms());
2588   VMThread::execute(&amp;op);
2589 
2590   HeapWord* result = op.result();
2591   bool ret_succeeded = op.prologue_succeeded() &amp;&amp; op.gc_succeeded();
2592   assert(result == NULL || ret_succeeded,
2593          &quot;the result should be NULL if the VM did not succeed&quot;);
2594   *succeeded = ret_succeeded;
2595 
2596   assert_heap_not_locked();
2597   return result;
2598 }
2599 
2600 void G1CollectedHeap::do_concurrent_mark() {
<span class="line-modified">2601   MutexLockerEx x(CGC_lock, Mutex::_no_safepoint_check_flag);</span>
2602   if (!_cm_thread-&gt;in_progress()) {
2603     _cm_thread-&gt;set_started();
2604     CGC_lock-&gt;notify();
2605   }
2606 }
2607 
2608 size_t G1CollectedHeap::pending_card_num() {
2609   struct CountCardsClosure : public ThreadClosure {
2610     size_t _cards;
2611     CountCardsClosure() : _cards(0) {}
2612     virtual void do_thread(Thread* t) {
2613       _cards += G1ThreadLocalData::dirty_card_queue(t).size();
2614     }
2615   } count_from_threads;
2616   Threads::threads_do(&amp;count_from_threads);
2617 
2618   G1DirtyCardQueueSet&amp; dcqs = G1BarrierSet::dirty_card_queue_set();
<span class="line-modified">2619   size_t buffer_size = dcqs.buffer_size();</span>
<span class="line-removed">2620   size_t buffer_num = dcqs.completed_buffers_num();</span>
<span class="line-removed">2621 </span>
<span class="line-removed">2622   return buffer_size * buffer_num + count_from_threads._cards;</span>
2623 }
2624 
2625 bool G1CollectedHeap::is_potential_eager_reclaim_candidate(HeapRegion* r) const {
2626   // We don&#39;t nominate objects with many remembered set entries, on
2627   // the assumption that such objects are likely still live.
2628   HeapRegionRemSet* rem_set = r-&gt;rem_set();
2629 
2630   return G1EagerReclaimHumongousObjectsWithStaleRefs ?
2631          rem_set-&gt;occupancy_less_or_equal_than(G1RSetSparseRegionEntries) :
2632          G1EagerReclaimHumongousObjects &amp;&amp; rem_set-&gt;is_empty();
2633 }
2634 
<span class="line-modified">2635 class RegisterHumongousWithInCSetFastTestClosure : public HeapRegionClosure {</span>
<span class="line-modified">2636  private:</span>
<span class="line-modified">2637   size_t _total_humongous;</span>
<span class="line-modified">2638   size_t _candidate_humongous;</span>
<span class="line-modified">2639 </span>
<span class="line-modified">2640   G1DirtyCardQueue _dcq;</span>
<span class="line-modified">2641 </span>
<span class="line-modified">2642   bool humongous_region_is_candidate(G1CollectedHeap* g1h, HeapRegion* region) const {</span>
<span class="line-modified">2643     assert(region-&gt;is_starts_humongous(), &quot;Must start a humongous object&quot;);</span>
<span class="line-modified">2644 </span>
<span class="line-removed">2645     oop obj = oop(region-&gt;bottom());</span>
<span class="line-removed">2646 </span>
<span class="line-removed">2647     // Dead objects cannot be eager reclaim candidates. Due to class</span>
<span class="line-removed">2648     // unloading it is unsafe to query their classes so we return early.</span>
<span class="line-removed">2649     if (g1h-&gt;is_obj_dead(obj, region)) {</span>
<span class="line-removed">2650       return false;</span>
<span class="line-removed">2651     }</span>
<span class="line-removed">2652 </span>
<span class="line-removed">2653     // If we do not have a complete remembered set for the region, then we can</span>
<span class="line-removed">2654     // not be sure that we have all references to it.</span>
<span class="line-removed">2655     if (!region-&gt;rem_set()-&gt;is_complete()) {</span>
<span class="line-removed">2656       return false;</span>
<span class="line-removed">2657     }</span>
<span class="line-removed">2658     // Candidate selection must satisfy the following constraints</span>
<span class="line-removed">2659     // while concurrent marking is in progress:</span>
<span class="line-removed">2660     //</span>
<span class="line-removed">2661     // * In order to maintain SATB invariants, an object must not be</span>
<span class="line-removed">2662     // reclaimed if it was allocated before the start of marking and</span>
<span class="line-removed">2663     // has not had its references scanned.  Such an object must have</span>
<span class="line-removed">2664     // its references (including type metadata) scanned to ensure no</span>
<span class="line-removed">2665     // live objects are missed by the marking process.  Objects</span>
<span class="line-removed">2666     // allocated after the start of concurrent marking don&#39;t need to</span>
<span class="line-removed">2667     // be scanned.</span>
<span class="line-removed">2668     //</span>
<span class="line-removed">2669     // * An object must not be reclaimed if it is on the concurrent</span>
<span class="line-removed">2670     // mark stack.  Objects allocated after the start of concurrent</span>
<span class="line-removed">2671     // marking are never pushed on the mark stack.</span>
<span class="line-removed">2672     //</span>
<span class="line-removed">2673     // Nominating only objects allocated after the start of concurrent</span>
<span class="line-removed">2674     // marking is sufficient to meet both constraints.  This may miss</span>
<span class="line-removed">2675     // some objects that satisfy the constraints, but the marking data</span>
<span class="line-removed">2676     // structures don&#39;t support efficiently performing the needed</span>
<span class="line-removed">2677     // additional tests or scrubbing of the mark stack.</span>
<span class="line-removed">2678     //</span>
<span class="line-removed">2679     // However, we presently only nominate is_typeArray() objects.</span>
<span class="line-removed">2680     // A humongous object containing references induces remembered</span>
<span class="line-removed">2681     // set entries on other regions.  In order to reclaim such an</span>
<span class="line-removed">2682     // object, those remembered sets would need to be cleaned up.</span>
<span class="line-removed">2683     //</span>
<span class="line-removed">2684     // We also treat is_typeArray() objects specially, allowing them</span>
<span class="line-removed">2685     // to be reclaimed even if allocated before the start of</span>
<span class="line-removed">2686     // concurrent mark.  For this we rely on mark stack insertion to</span>
<span class="line-removed">2687     // exclude is_typeArray() objects, preventing reclaiming an object</span>
<span class="line-removed">2688     // that is in the mark stack.  We also rely on the metadata for</span>
<span class="line-removed">2689     // such objects to be built-in and so ensured to be kept live.</span>
<span class="line-removed">2690     // Frequent allocation and drop of large binary blobs is an</span>
<span class="line-removed">2691     // important use case for eager reclaim, and this special handling</span>
<span class="line-removed">2692     // may reduce needed headroom.</span>
<span class="line-removed">2693 </span>
<span class="line-removed">2694     return obj-&gt;is_typeArray() &amp;&amp;</span>
<span class="line-removed">2695            g1h-&gt;is_potential_eager_reclaim_candidate(region);</span>
<span class="line-removed">2696   }</span>
<span class="line-removed">2697 </span>
<span class="line-removed">2698  public:</span>
<span class="line-removed">2699   RegisterHumongousWithInCSetFastTestClosure()</span>
<span class="line-removed">2700   : _total_humongous(0),</span>
<span class="line-removed">2701     _candidate_humongous(0),</span>
<span class="line-removed">2702     _dcq(&amp;G1BarrierSet::dirty_card_queue_set()) {</span>
<span class="line-removed">2703   }</span>
<span class="line-removed">2704 </span>
<span class="line-removed">2705   virtual bool do_heap_region(HeapRegion* r) {</span>
<span class="line-removed">2706     if (!r-&gt;is_starts_humongous()) {</span>
2707       return false;
2708     }
<span class="line-modified">2709     G1CollectedHeap* g1h = G1CollectedHeap::heap();</span>
<span class="line-removed">2710 </span>
<span class="line-removed">2711     bool is_candidate = humongous_region_is_candidate(g1h, r);</span>
<span class="line-removed">2712     uint rindex = r-&gt;hrm_index();</span>
<span class="line-removed">2713     g1h-&gt;set_humongous_reclaim_candidate(rindex, is_candidate);</span>
<span class="line-removed">2714     if (is_candidate) {</span>
<span class="line-removed">2715       _candidate_humongous++;</span>
<span class="line-removed">2716       g1h-&gt;register_humongous_region_with_cset(rindex);</span>
<span class="line-removed">2717       // Is_candidate already filters out humongous object with large remembered sets.</span>
<span class="line-removed">2718       // If we have a humongous object with a few remembered sets, we simply flush these</span>
<span class="line-removed">2719       // remembered set entries into the DCQS. That will result in automatic</span>
<span class="line-removed">2720       // re-evaluation of their remembered set entries during the following evacuation</span>
<span class="line-removed">2721       // phase.</span>
<span class="line-removed">2722       if (!r-&gt;rem_set()-&gt;is_empty()) {</span>
<span class="line-removed">2723         guarantee(r-&gt;rem_set()-&gt;occupancy_less_or_equal_than(G1RSetSparseRegionEntries),</span>
<span class="line-removed">2724                   &quot;Found a not-small remembered set here. This is inconsistent with previous assumptions.&quot;);</span>
<span class="line-removed">2725         G1CardTable* ct = g1h-&gt;card_table();</span>
<span class="line-removed">2726         HeapRegionRemSetIterator hrrs(r-&gt;rem_set());</span>
<span class="line-removed">2727         size_t card_index;</span>
<span class="line-removed">2728         while (hrrs.has_next(card_index)) {</span>
<span class="line-removed">2729           CardTable::CardValue* card_ptr = ct-&gt;byte_for_index(card_index);</span>
<span class="line-removed">2730           // The remembered set might contain references to already freed</span>
<span class="line-removed">2731           // regions. Filter out such entries to avoid failing card table</span>
<span class="line-removed">2732           // verification.</span>
<span class="line-removed">2733           if (g1h-&gt;is_in_closed_subset(ct-&gt;addr_for(card_ptr))) {</span>
<span class="line-removed">2734             if (*card_ptr != G1CardTable::dirty_card_val()) {</span>
<span class="line-removed">2735               *card_ptr = G1CardTable::dirty_card_val();</span>
<span class="line-removed">2736               _dcq.enqueue(card_ptr);</span>
<span class="line-removed">2737             }</span>
<span class="line-removed">2738           }</span>
<span class="line-removed">2739         }</span>
<span class="line-removed">2740         assert(hrrs.n_yielded() == r-&gt;rem_set()-&gt;occupied(),</span>
<span class="line-removed">2741                &quot;Remembered set hash maps out of sync, cur: &quot; SIZE_FORMAT &quot; entries, next: &quot; SIZE_FORMAT &quot; entries&quot;,</span>
<span class="line-removed">2742                hrrs.n_yielded(), r-&gt;rem_set()-&gt;occupied());</span>
<span class="line-removed">2743         // We should only clear the card based remembered set here as we will not</span>
<span class="line-removed">2744         // implicitly rebuild anything else during eager reclaim. Note that at the moment</span>
<span class="line-removed">2745         // (and probably never) we do not enter this path if there are other kind of</span>
<span class="line-removed">2746         // remembered sets for this region.</span>
<span class="line-removed">2747         r-&gt;rem_set()-&gt;clear_locked(true /* only_cardset */);</span>
<span class="line-removed">2748         // Clear_locked() above sets the state to Empty. However we want to continue</span>
<span class="line-removed">2749         // collecting remembered set entries for humongous regions that were not</span>
<span class="line-removed">2750         // reclaimed.</span>
<span class="line-removed">2751         r-&gt;rem_set()-&gt;set_state_complete();</span>
<span class="line-removed">2752       }</span>
<span class="line-removed">2753       assert(r-&gt;rem_set()-&gt;is_empty(), &quot;At this point any humongous candidate remembered set must be empty.&quot;);</span>
<span class="line-removed">2754     }</span>
<span class="line-removed">2755     _total_humongous++;</span>
<span class="line-removed">2756 </span>
<span class="line-removed">2757     return false;</span>
<span class="line-removed">2758   }</span>
<span class="line-removed">2759 </span>
<span class="line-removed">2760   size_t total_humongous() const { return _total_humongous; }</span>
<span class="line-removed">2761   size_t candidate_humongous() const { return _candidate_humongous; }</span>
<span class="line-removed">2762 </span>
<span class="line-removed">2763   void flush_rem_set_entries() { _dcq.flush(); }</span>
<span class="line-removed">2764 };</span>
<span class="line-removed">2765 </span>
<span class="line-removed">2766 void G1CollectedHeap::register_humongous_regions_with_cset() {</span>
<span class="line-removed">2767   if (!G1EagerReclaimHumongousObjects) {</span>
<span class="line-removed">2768     phase_times()-&gt;record_fast_reclaim_humongous_stats(0.0, 0, 0);</span>
<span class="line-removed">2769     return;</span>
<span class="line-removed">2770   }</span>
<span class="line-removed">2771   double time = os::elapsed_counter();</span>
<span class="line-removed">2772 </span>
<span class="line-removed">2773   // Collect reclaim candidate information and register candidates with cset.</span>
<span class="line-removed">2774   RegisterHumongousWithInCSetFastTestClosure cl;</span>
2775   heap_region_iterate(&amp;cl);
<span class="line-removed">2776 </span>
<span class="line-removed">2777   time = ((double)(os::elapsed_counter() - time) / os::elapsed_frequency()) * 1000.0;</span>
<span class="line-removed">2778   phase_times()-&gt;record_fast_reclaim_humongous_stats(time,</span>
<span class="line-removed">2779                                                      cl.total_humongous(),</span>
<span class="line-removed">2780                                                      cl.candidate_humongous());</span>
<span class="line-removed">2781   _has_humongous_reclaim_candidates = cl.candidate_humongous() &gt; 0;</span>
<span class="line-removed">2782 </span>
<span class="line-removed">2783   // Finally flush all remembered set entries to re-check into the global DCQS.</span>
<span class="line-removed">2784   cl.flush_rem_set_entries();</span>
2785 }

2786 
2787 class VerifyRegionRemSetClosure : public HeapRegionClosure {
2788   public:
2789     bool do_heap_region(HeapRegion* hr) {
2790       if (!hr-&gt;is_archive() &amp;&amp; !hr-&gt;is_continues_humongous()) {
2791         hr-&gt;verify_rem_set();
2792       }
2793       return false;
2794     }
2795 };
2796 
2797 uint G1CollectedHeap::num_task_queues() const {
2798   return _task_queues-&gt;size();
2799 }
2800 
2801 #if TASKQUEUE_STATS
2802 void G1CollectedHeap::print_taskqueue_stats_hdr(outputStream* const st) {
2803   st-&gt;print_raw_cr(&quot;GC Task Stats&quot;);
2804   st-&gt;print_raw(&quot;thr &quot;); TaskQueueStats::print_header(1, st); st-&gt;cr();
2805   st-&gt;print_raw(&quot;--- &quot;); TaskQueueStats::print_header(2, st); st-&gt;cr();
</pre>
<hr />
<pre>
2846   if (waited) {
2847     double scan_wait_end = os::elapsedTime();
2848     wait_time_ms = (scan_wait_end - scan_wait_start) * 1000.0;
2849   }
2850   phase_times()-&gt;record_root_region_scan_wait_time(wait_time_ms);
2851 }
2852 
2853 class G1PrintCollectionSetClosure : public HeapRegionClosure {
2854 private:
2855   G1HRPrinter* _hr_printer;
2856 public:
2857   G1PrintCollectionSetClosure(G1HRPrinter* hr_printer) : HeapRegionClosure(), _hr_printer(hr_printer) { }
2858 
2859   virtual bool do_heap_region(HeapRegion* r) {
2860     _hr_printer-&gt;cset(r);
2861     return false;
2862   }
2863 };
2864 
2865 void G1CollectedHeap::start_new_collection_set() {


2866   collection_set()-&gt;start_incremental_building();
2867 
<span class="line-modified">2868   clear_cset_fast_test();</span>
2869 
2870   guarantee(_eden.length() == 0, &quot;eden should have been cleared&quot;);
2871   policy()-&gt;transfer_survivors_to_cset(survivor());
















































































2872 }
2873 
<span class="line-modified">2874 bool</span>
<span class="line-removed">2875 G1CollectedHeap::do_collection_pause_at_safepoint(double target_pause_time_ms) {</span>
2876   assert_at_safepoint_on_vm_thread();
2877   guarantee(!is_gc_active(), &quot;collection is not reentrant&quot;);
2878 
2879   if (GCLocker::check_active_before_gc()) {
2880     return false;
2881   }
2882 
<span class="line-modified">2883   _gc_timer_stw-&gt;register_gc_start();</span>










2884 

2885   GCIdMark gc_id_mark;
<span class="line-removed">2886   _gc_tracer_stw-&gt;report_gc_start(gc_cause(), _gc_timer_stw-&gt;gc_start());</span>
2887 
2888   SvcGCMarker sgcm(SvcGCMarker::MINOR);
2889   ResourceMark rm;
2890 
2891   policy()-&gt;note_gc_start();
2892 



2893   wait_for_root_region_scanning();
2894 
2895   print_heap_before_gc();
2896   print_heap_regions();
2897   trace_heap_before_gc(_gc_tracer_stw);
2898 
2899   _verifier-&gt;verify_region_sets_optional();
2900   _verifier-&gt;verify_dirty_young_regions();
2901 
2902   // We should not be doing initial mark unless the conc mark thread is running
2903   if (!_cm_thread-&gt;should_terminate()) {
2904     // This call will decide whether this pause is an initial-mark
2905     // pause. If it is, in_initial_mark_gc() will return true
2906     // for the duration of this pause.
2907     policy()-&gt;decide_on_conc_mark_initiation();
2908   }
2909 
2910   // We do not allow initial-mark to be piggy-backed on a mixed GC.
2911   assert(!collector_state()-&gt;in_initial_mark_gc() ||
<span class="line-modified">2912           collector_state()-&gt;in_young_only_phase(), &quot;sanity&quot;);</span>
<span class="line-removed">2913 </span>
2914   // We also do not allow mixed GCs during marking.
2915   assert(!collector_state()-&gt;mark_or_rebuild_in_progress() || collector_state()-&gt;in_young_only_phase(), &quot;sanity&quot;);
2916 
2917   // Record whether this pause is an initial mark. When the current
2918   // thread has completed its logging output and it&#39;s safe to signal
2919   // the CM thread, the flag&#39;s value in the policy has been reset.
2920   bool should_start_conc_mark = collector_state()-&gt;in_initial_mark_gc();



2921 
2922   // Inner scope for scope based logging, timers, and stats collection
2923   {
2924     G1EvacuationInfo evacuation_info;
2925 
<span class="line-removed">2926     if (collector_state()-&gt;in_initial_mark_gc()) {</span>
<span class="line-removed">2927       // We are about to start a marking cycle, so we increment the</span>
<span class="line-removed">2928       // full collection counter.</span>
<span class="line-removed">2929       increment_old_marking_cycles_started();</span>
<span class="line-removed">2930       _cm-&gt;gc_tracer_cm()-&gt;set_gc_cause(gc_cause());</span>
<span class="line-removed">2931     }</span>
<span class="line-removed">2932 </span>
2933     _gc_tracer_stw-&gt;report_yc_type(collector_state()-&gt;yc_type());
2934 
2935     GCTraceCPUTime tcpu;
2936 
<span class="line-modified">2937     G1HeapVerifier::G1VerifyType verify_type;</span>
<span class="line-removed">2938     FormatBuffer&lt;&gt; gc_string(&quot;Pause Young &quot;);</span>
<span class="line-removed">2939     if (collector_state()-&gt;in_initial_mark_gc()) {</span>
<span class="line-removed">2940       gc_string.append(&quot;(Concurrent Start)&quot;);</span>
<span class="line-removed">2941       verify_type = G1HeapVerifier::G1VerifyConcurrentStart;</span>
<span class="line-removed">2942     } else if (collector_state()-&gt;in_young_only_phase()) {</span>
<span class="line-removed">2943       if (collector_state()-&gt;in_young_gc_before_mixed()) {</span>
<span class="line-removed">2944         gc_string.append(&quot;(Prepare Mixed)&quot;);</span>
<span class="line-removed">2945       } else {</span>
<span class="line-removed">2946         gc_string.append(&quot;(Normal)&quot;);</span>
<span class="line-removed">2947       }</span>
<span class="line-removed">2948       verify_type = G1HeapVerifier::G1VerifyYoungNormal;</span>
<span class="line-removed">2949     } else {</span>
<span class="line-removed">2950       gc_string.append(&quot;(Mixed)&quot;);</span>
<span class="line-removed">2951       verify_type = G1HeapVerifier::G1VerifyMixed;</span>
<span class="line-removed">2952     }</span>
<span class="line-removed">2953     GCTraceTime(Info, gc) tm(gc_string, NULL, gc_cause(), true);</span>
2954 
2955     uint active_workers = WorkerPolicy::calc_active_workers(workers()-&gt;total_workers(),
2956                                                             workers()-&gt;active_workers(),
2957                                                             Threads::number_of_non_daemon_threads());
2958     active_workers = workers()-&gt;update_active_workers(active_workers);
2959     log_info(gc,task)(&quot;Using %u workers of %u for evacuation&quot;, active_workers, workers()-&gt;total_workers());
2960 
2961     G1MonitoringScope ms(g1mm(),
2962                          false /* full_gc */,
2963                          collector_state()-&gt;yc_type() == Mixed /* all_memory_pools_affected */);
2964 
2965     G1HeapTransition heap_transition(this);
<span class="line-removed">2966     size_t heap_used_bytes_before_gc = used();</span>
2967 
<span class="line-modified">2968     // Don&#39;t dynamically change the number of GC threads this early.  A value of</span>
<span class="line-removed">2969     // 0 is used to indicate serial work.  When parallel work is done,</span>
<span class="line-removed">2970     // it will be set.</span>
<span class="line-removed">2971 </span>
<span class="line-removed">2972     { // Call to jvmpi::post_class_unload_events must occur outside of active GC</span>
2973       IsGCActiveMark x;
2974 
2975       gc_prologue(false);
2976 
<span class="line-modified">2977       if (VerifyRememberedSets) {</span>
<span class="line-modified">2978         log_info(gc, verify)(&quot;[Verifying RemSets before GC]&quot;);</span>
<span class="line-removed">2979         VerifyRegionRemSetClosure v_cl;</span>
<span class="line-removed">2980         heap_region_iterate(&amp;v_cl);</span>
<span class="line-removed">2981       }</span>
<span class="line-removed">2982 </span>
<span class="line-removed">2983       _verifier-&gt;verify_before_gc(verify_type);</span>
<span class="line-removed">2984 </span>
<span class="line-removed">2985       _verifier-&gt;check_bitmaps(&quot;GC Start&quot;);</span>
<span class="line-removed">2986 </span>
<span class="line-removed">2987 #if COMPILER2_OR_JVMCI</span>
<span class="line-removed">2988       DerivedPointerTable::clear();</span>
<span class="line-removed">2989 #endif</span>
2990 
<span class="line-modified">2991       // Please see comment in g1CollectedHeap.hpp and</span>
<span class="line-modified">2992       // G1CollectedHeap::ref_processing_init() to see how</span>
<span class="line-modified">2993       // reference processing currently works in G1.</span>

2994 
<span class="line-modified">2995       // Enable discovery in the STW reference processor</span>
<span class="line-modified">2996       _ref_processor_stw-&gt;enable_discovery();</span>


2997 
<span class="line-removed">2998       {</span>
2999         // We want to temporarily turn off discovery by the
3000         // CM ref processor, if necessary, and turn it back on
3001         // on again later if we do. Using a scoped
3002         // NoRefDiscovery object will do this.
3003         NoRefDiscovery no_cm_discovery(_ref_processor_cm);
3004 
<span class="line-removed">3005         // Forget the current alloc region (we might even choose it to be part</span>
<span class="line-removed">3006         // of the collection set!).</span>
<span class="line-removed">3007         _allocator-&gt;release_mutator_alloc_region();</span>
<span class="line-removed">3008 </span>
<span class="line-removed">3009         // This timing is only used by the ergonomics to handle our pause target.</span>
<span class="line-removed">3010         // It is unclear why this should not include the full pause. We will</span>
<span class="line-removed">3011         // investigate this in CR 7178365.</span>
<span class="line-removed">3012         //</span>
<span class="line-removed">3013         // Preserving the old comment here if that helps the investigation:</span>
<span class="line-removed">3014         //</span>
<span class="line-removed">3015         // The elapsed time induced by the start time below deliberately elides</span>
<span class="line-removed">3016         // the possible verification above.</span>
<span class="line-removed">3017         double sample_start_time_sec = os::elapsedTime();</span>
<span class="line-removed">3018 </span>
3019         policy()-&gt;record_collection_pause_start(sample_start_time_sec);
3020 
<span class="line-modified">3021         if (collector_state()-&gt;in_initial_mark_gc()) {</span>
<span class="line-modified">3022           concurrent_mark()-&gt;pre_initial_mark();</span>
<span class="line-modified">3023         }</span>
<span class="line-removed">3024 </span>
<span class="line-removed">3025         policy()-&gt;finalize_collection_set(target_pause_time_ms, &amp;_survivor);</span>
<span class="line-removed">3026 </span>
<span class="line-removed">3027         evacuation_info.set_collectionset_regions(collection_set()-&gt;region_length());</span>
<span class="line-removed">3028 </span>
<span class="line-removed">3029         register_humongous_regions_with_cset();</span>
<span class="line-removed">3030 </span>
<span class="line-removed">3031         assert(_verifier-&gt;check_cset_fast_test(), &quot;Inconsistency in the InCSetState table.&quot;);</span>
<span class="line-removed">3032 </span>
<span class="line-removed">3033         // We call this after finalize_cset() to</span>
<span class="line-removed">3034         // ensure that the CSet has been finalized.</span>
<span class="line-removed">3035         _cm-&gt;verify_no_cset_oops();</span>
<span class="line-removed">3036 </span>
<span class="line-removed">3037         if (_hr_printer.is_active()) {</span>
<span class="line-removed">3038           G1PrintCollectionSetClosure cl(&amp;_hr_printer);</span>
<span class="line-removed">3039           _collection_set.iterate(&amp;cl);</span>
<span class="line-removed">3040         }</span>
3041 
<span class="line-modified">3042         // Initialize the GC alloc regions.</span>
<span class="line-removed">3043         _allocator-&gt;init_gc_alloc_regions(evacuation_info);</span>
3044 

3045         G1ParScanThreadStateSet per_thread_states(this,

3046                                                   workers()-&gt;active_workers(),
3047                                                   collection_set()-&gt;young_region_length(),
3048                                                   collection_set()-&gt;optional_region_length());
<span class="line-modified">3049         pre_evacuate_collection_set();</span>
3050 
3051         // Actually do the work...
<span class="line-modified">3052         evacuate_collection_set(&amp;per_thread_states);</span>
<span class="line-removed">3053         evacuate_optional_collection_set(&amp;per_thread_states);</span>
<span class="line-removed">3054 </span>
<span class="line-removed">3055         post_evacuate_collection_set(evacuation_info, &amp;per_thread_states);</span>
3056 
<span class="line-modified">3057         const size_t* surviving_young_words = per_thread_states.surviving_young_words();</span>
<span class="line-modified">3058         free_collection_set(&amp;_collection_set, evacuation_info, surviving_young_words);</span>


3059 
<span class="line-modified">3060         eagerly_reclaim_humongous_regions();</span>
3061 
<span class="line-removed">3062         record_obj_copy_mem_stats();</span>
3063         _survivor_evac_stats.adjust_desired_plab_sz();
3064         _old_evac_stats.adjust_desired_plab_sz();
3065 
<span class="line-modified">3066         double start = os::elapsedTime();</span>
<span class="line-removed">3067         start_new_collection_set();</span>
<span class="line-removed">3068         phase_times()-&gt;record_start_new_cset_time_ms((os::elapsedTime() - start) * 1000.0);</span>
<span class="line-removed">3069 </span>
<span class="line-removed">3070         if (evacuation_failed()) {</span>
<span class="line-removed">3071           double recalculate_used_start = os::elapsedTime();</span>
<span class="line-removed">3072           set_used(recalculate_used());</span>
<span class="line-removed">3073           phase_times()-&gt;record_evac_fail_recalc_used_time((os::elapsedTime() - recalculate_used_start) * 1000.0);</span>
<span class="line-removed">3074 </span>
<span class="line-removed">3075           if (_archive_allocator != NULL) {</span>
<span class="line-removed">3076             _archive_allocator-&gt;clear_used();</span>
<span class="line-removed">3077           }</span>
<span class="line-removed">3078           for (uint i = 0; i &lt; ParallelGCThreads; i++) {</span>
<span class="line-removed">3079             if (_evacuation_failed_info_array[i].has_failed()) {</span>
<span class="line-removed">3080               _gc_tracer_stw-&gt;report_evacuation_failed(_evacuation_failed_info_array[i]);</span>
<span class="line-removed">3081             }</span>
<span class="line-removed">3082           }</span>
<span class="line-removed">3083         } else {</span>
<span class="line-removed">3084           // The &quot;used&quot; of the the collection set have already been subtracted</span>
<span class="line-removed">3085           // when they were freed.  Add in the bytes evacuated.</span>
<span class="line-removed">3086           increase_used(policy()-&gt;bytes_copied_during_gc());</span>
<span class="line-removed">3087         }</span>
<span class="line-removed">3088 </span>
<span class="line-removed">3089         if (collector_state()-&gt;in_initial_mark_gc()) {</span>
3090           // We have to do this before we notify the CM threads that
3091           // they can start working to make sure that all the
3092           // appropriate initialization is done on the CM object.
3093           concurrent_mark()-&gt;post_initial_mark();
3094           // Note that we don&#39;t actually trigger the CM thread at
3095           // this point. We do that later when we&#39;re sure that
3096           // the current thread has completed its logging output.
3097         }
3098 
3099         allocate_dummy_regions();
3100 
<span class="line-modified">3101         _allocator-&gt;init_mutator_alloc_region();</span>
<span class="line-removed">3102 </span>
<span class="line-removed">3103         {</span>
<span class="line-removed">3104           size_t expand_bytes = _heap_sizing_policy-&gt;expansion_amount();</span>
<span class="line-removed">3105           if (expand_bytes &gt; 0) {</span>
<span class="line-removed">3106             size_t bytes_before = capacity();</span>
<span class="line-removed">3107             // No need for an ergo logging here,</span>
<span class="line-removed">3108             // expansion_amount() does this when it returns a value &gt; 0.</span>
<span class="line-removed">3109             double expand_ms;</span>
<span class="line-removed">3110             if (!expand(expand_bytes, _workers, &amp;expand_ms)) {</span>
<span class="line-removed">3111               // We failed to expand the heap. Cannot do anything about it.</span>
<span class="line-removed">3112             }</span>
<span class="line-removed">3113             phase_times()-&gt;record_expand_heap_time(expand_ms);</span>
<span class="line-removed">3114           }</span>
<span class="line-removed">3115         }</span>
3116 
<span class="line-modified">3117         // We redo the verification but now wrt to the new CSet which</span>
<span class="line-removed">3118         // has just got initialized after the previous CSet was freed.</span>
<span class="line-removed">3119         _cm-&gt;verify_no_cset_oops();</span>
3120 
<span class="line-removed">3121         // This timing is only used by the ergonomics to handle our pause target.</span>
<span class="line-removed">3122         // It is unclear why this should not include the full pause. We will</span>
<span class="line-removed">3123         // investigate this in CR 7178365.</span>
3124         double sample_end_time_sec = os::elapsedTime();
3125         double pause_time_ms = (sample_end_time_sec - sample_start_time_sec) * MILLIUNITS;
<span class="line-modified">3126         size_t total_cards_scanned = phase_times()-&gt;sum_thread_work_items(G1GCPhaseTimes::ScanRS, G1GCPhaseTimes::ScanRSScannedCards);</span>
<span class="line-removed">3127         policy()-&gt;record_collection_pause_end(pause_time_ms, total_cards_scanned, heap_used_bytes_before_gc);</span>
<span class="line-removed">3128 </span>
<span class="line-removed">3129         evacuation_info.set_collectionset_used_before(collection_set()-&gt;bytes_used_before());</span>
<span class="line-removed">3130         evacuation_info.set_bytes_copied(policy()-&gt;bytes_copied_during_gc());</span>
<span class="line-removed">3131 </span>
<span class="line-removed">3132         if (VerifyRememberedSets) {</span>
<span class="line-removed">3133           log_info(gc, verify)(&quot;[Verifying RemSets after GC]&quot;);</span>
<span class="line-removed">3134           VerifyRegionRemSetClosure v_cl;</span>
<span class="line-removed">3135           heap_region_iterate(&amp;v_cl);</span>
<span class="line-removed">3136         }</span>
<span class="line-removed">3137 </span>
<span class="line-removed">3138         _verifier-&gt;verify_after_gc(verify_type);</span>
<span class="line-removed">3139         _verifier-&gt;check_bitmaps(&quot;GC End&quot;);</span>
<span class="line-removed">3140 </span>
<span class="line-removed">3141         assert(!_ref_processor_stw-&gt;discovery_enabled(), &quot;Postcondition&quot;);</span>
<span class="line-removed">3142         _ref_processor_stw-&gt;verify_no_references_recorded();</span>
<span class="line-removed">3143 </span>
<span class="line-removed">3144         // CM reference discovery will be re-enabled if necessary.</span>
3145       }
3146 
<span class="line-modified">3147 #ifdef TRACESPINNING</span>
<span class="line-removed">3148       ParallelTaskTerminator::print_termination_counts();</span>
<span class="line-removed">3149 #endif</span>
3150 
3151       gc_epilogue(false);
3152     }
3153 
3154     // Print the remainder of the GC log output.
3155     if (evacuation_failed()) {
3156       log_info(gc)(&quot;To-space exhausted&quot;);
3157     }
3158 
3159     policy()-&gt;print_phases();
3160     heap_transition.print();
3161 
<span class="line-removed">3162     // It is not yet to safe to tell the concurrent mark to</span>
<span class="line-removed">3163     // start as we have some optional output below. We don&#39;t want the</span>
<span class="line-removed">3164     // output from the concurrent mark thread interfering with this</span>
<span class="line-removed">3165     // logging output either.</span>
<span class="line-removed">3166 </span>
3167     _hrm-&gt;verify_optional();
3168     _verifier-&gt;verify_region_sets_optional();
3169 
3170     TASKQUEUE_STATS_ONLY(print_taskqueue_stats());
3171     TASKQUEUE_STATS_ONLY(reset_taskqueue_stats());
3172 
3173     print_heap_after_gc();
3174     print_heap_regions();
3175     trace_heap_after_gc(_gc_tracer_stw);
3176 
3177     // We must call G1MonitoringSupport::update_sizes() in the same scoping level
3178     // as an active TraceMemoryManagerStats object (i.e. before the destructor for the
3179     // TraceMemoryManagerStats is called) so that the G1 memory pools are updated
3180     // before any GC notifications are raised.
3181     g1mm()-&gt;update_sizes();
3182 
3183     _gc_tracer_stw-&gt;report_evacuation_info(&amp;evacuation_info);
3184     _gc_tracer_stw-&gt;report_tenuring_threshold(_policy-&gt;tenuring_threshold());
3185     _gc_timer_stw-&gt;register_gc_end();
3186     _gc_tracer_stw-&gt;report_gc_end(_gc_timer_stw-&gt;gc_end(), _gc_timer_stw-&gt;time_partitions());
3187   }
3188   // It should now be safe to tell the concurrent mark thread to start
3189   // without its logging output interfering with the logging output
3190   // that came from the pause.
3191 
3192   if (should_start_conc_mark) {
<span class="line-modified">3193     // CAUTION: after the doConcurrentMark() call below,</span>
<span class="line-modified">3194     // the concurrent marking thread(s) could be running</span>
<span class="line-modified">3195     // concurrently with us. Make sure that anything after</span>
<span class="line-modified">3196     // this point does not assume that we are the only GC thread</span>
<span class="line-modified">3197     // running. Note: of course, the actual marking work will</span>
<span class="line-removed">3198     // not start until the safepoint itself is released in</span>
<span class="line-removed">3199     // SuspendibleThreadSet::desynchronize().</span>
3200     do_concurrent_mark();
3201   }
<span class="line-removed">3202 </span>
<span class="line-removed">3203   return true;</span>
3204 }
3205 
<span class="line-modified">3206 void G1CollectedHeap::remove_self_forwarding_pointers() {</span>
<span class="line-modified">3207   G1ParRemoveSelfForwardPtrsTask rsfp_task;</span>
3208   workers()-&gt;run_task(&amp;rsfp_task);
3209 }
3210 
<span class="line-modified">3211 void G1CollectedHeap::restore_after_evac_failure() {</span>
3212   double remove_self_forwards_start = os::elapsedTime();
3213 
<span class="line-modified">3214   remove_self_forwarding_pointers();</span>
<span class="line-modified">3215   SharedRestorePreservedMarksTaskExecutor task_executor(workers());</span>
<span class="line-removed">3216   _preserved_marks_set.restore(&amp;task_executor);</span>
3217 
3218   phase_times()-&gt;record_evac_fail_remove_self_forwards((os::elapsedTime() - remove_self_forwards_start) * 1000.0);
3219 }
3220 
<span class="line-modified">3221 void G1CollectedHeap::preserve_mark_during_evac_failure(uint worker_id, oop obj, markOop m) {</span>
3222   if (!_evacuation_failed) {
3223     _evacuation_failed = true;
3224   }
3225 
3226   _evacuation_failed_info_array[worker_id].register_copy_failure(obj-&gt;size());
3227   _preserved_marks_set.get(worker_id)-&gt;push_if_necessary(obj, m);
3228 }
3229 
3230 bool G1ParEvacuateFollowersClosure::offer_termination() {
3231   EventGCPhaseParallel event;
3232   G1ParScanThreadState* const pss = par_scan_state();
3233   start_term_time();
3234   const bool res = terminator()-&gt;offer_termination();
3235   end_term_time();
3236   event.commit(GCId::current(), pss-&gt;worker_id(), G1GCPhaseTimes::phase_name(G1GCPhaseTimes::Termination));
3237   return res;
3238 }
3239 
3240 void G1ParEvacuateFollowersClosure::do_void() {
3241   EventGCPhaseParallel event;
3242   G1ParScanThreadState* const pss = par_scan_state();
3243   pss-&gt;trim_queue();
3244   event.commit(GCId::current(), pss-&gt;worker_id(), G1GCPhaseTimes::phase_name(_phase));
3245   do {
3246     EventGCPhaseParallel event;
3247     pss-&gt;steal_and_trim_queue(queues());
3248     event.commit(GCId::current(), pss-&gt;worker_id(), G1GCPhaseTimes::phase_name(_phase));
3249   } while (!offer_termination());
3250 }
3251 
<span class="line-removed">3252 class G1ParTask : public AbstractGangTask {</span>
<span class="line-removed">3253 protected:</span>
<span class="line-removed">3254   G1CollectedHeap*         _g1h;</span>
<span class="line-removed">3255   G1ParScanThreadStateSet* _pss;</span>
<span class="line-removed">3256   RefToScanQueueSet*       _queues;</span>
<span class="line-removed">3257   G1RootProcessor*         _root_processor;</span>
<span class="line-removed">3258   TaskTerminator           _terminator;</span>
<span class="line-removed">3259   uint                     _n_workers;</span>
<span class="line-removed">3260 </span>
<span class="line-removed">3261 public:</span>
<span class="line-removed">3262   G1ParTask(G1CollectedHeap* g1h, G1ParScanThreadStateSet* per_thread_states, RefToScanQueueSet *task_queues, G1RootProcessor* root_processor, uint n_workers)</span>
<span class="line-removed">3263     : AbstractGangTask(&quot;G1 collection&quot;),</span>
<span class="line-removed">3264       _g1h(g1h),</span>
<span class="line-removed">3265       _pss(per_thread_states),</span>
<span class="line-removed">3266       _queues(task_queues),</span>
<span class="line-removed">3267       _root_processor(root_processor),</span>
<span class="line-removed">3268       _terminator(n_workers, _queues),</span>
<span class="line-removed">3269       _n_workers(n_workers)</span>
<span class="line-removed">3270   {}</span>
<span class="line-removed">3271 </span>
<span class="line-removed">3272   void work(uint worker_id) {</span>
<span class="line-removed">3273     if (worker_id &gt;= _n_workers) return;  // no work needed this round</span>
<span class="line-removed">3274 </span>
<span class="line-removed">3275     double start_sec = os::elapsedTime();</span>
<span class="line-removed">3276     _g1h-&gt;phase_times()-&gt;record_time_secs(G1GCPhaseTimes::GCWorkerStart, worker_id, start_sec);</span>
<span class="line-removed">3277 </span>
<span class="line-removed">3278     {</span>
<span class="line-removed">3279       ResourceMark rm;</span>
<span class="line-removed">3280       HandleMark   hm;</span>
<span class="line-removed">3281 </span>
<span class="line-removed">3282       ReferenceProcessor*             rp = _g1h-&gt;ref_processor_stw();</span>
<span class="line-removed">3283 </span>
<span class="line-removed">3284       G1ParScanThreadState*           pss = _pss-&gt;state_for_worker(worker_id);</span>
<span class="line-removed">3285       pss-&gt;set_ref_discoverer(rp);</span>
<span class="line-removed">3286 </span>
<span class="line-removed">3287       double start_strong_roots_sec = os::elapsedTime();</span>
<span class="line-removed">3288 </span>
<span class="line-removed">3289       _root_processor-&gt;evacuate_roots(pss, worker_id);</span>
<span class="line-removed">3290 </span>
<span class="line-removed">3291       _g1h-&gt;rem_set()-&gt;oops_into_collection_set_do(pss, worker_id);</span>
<span class="line-removed">3292 </span>
<span class="line-removed">3293       double strong_roots_sec = os::elapsedTime() - start_strong_roots_sec;</span>
<span class="line-removed">3294 </span>
<span class="line-removed">3295       double term_sec = 0.0;</span>
<span class="line-removed">3296       size_t evac_term_attempts = 0;</span>
<span class="line-removed">3297       {</span>
<span class="line-removed">3298         double start = os::elapsedTime();</span>
<span class="line-removed">3299         G1ParEvacuateFollowersClosure evac(_g1h, pss, _queues, _terminator.terminator(), G1GCPhaseTimes::ObjCopy);</span>
<span class="line-removed">3300         evac.do_void();</span>
<span class="line-removed">3301 </span>
<span class="line-removed">3302         evac_term_attempts = evac.term_attempts();</span>
<span class="line-removed">3303         term_sec = evac.term_time();</span>
<span class="line-removed">3304         double elapsed_sec = os::elapsedTime() - start;</span>
<span class="line-removed">3305 </span>
<span class="line-removed">3306         G1GCPhaseTimes* p = _g1h-&gt;phase_times();</span>
<span class="line-removed">3307         p-&gt;add_time_secs(G1GCPhaseTimes::ObjCopy, worker_id, elapsed_sec - term_sec);</span>
<span class="line-removed">3308 </span>
<span class="line-removed">3309         p-&gt;record_or_add_thread_work_item(G1GCPhaseTimes::ObjCopy,</span>
<span class="line-removed">3310                                           worker_id,</span>
<span class="line-removed">3311                                           pss-&gt;lab_waste_words() * HeapWordSize,</span>
<span class="line-removed">3312                                           G1GCPhaseTimes::ObjCopyLABWaste);</span>
<span class="line-removed">3313         p-&gt;record_or_add_thread_work_item(G1GCPhaseTimes::ObjCopy,</span>
<span class="line-removed">3314                                           worker_id,</span>
<span class="line-removed">3315                                           pss-&gt;lab_undo_waste_words() * HeapWordSize,</span>
<span class="line-removed">3316                                           G1GCPhaseTimes::ObjCopyLABUndoWaste);</span>
<span class="line-removed">3317 </span>
<span class="line-removed">3318         p-&gt;record_time_secs(G1GCPhaseTimes::Termination, worker_id, term_sec);</span>
<span class="line-removed">3319         p-&gt;record_thread_work_item(G1GCPhaseTimes::Termination, worker_id, evac_term_attempts);</span>
<span class="line-removed">3320       }</span>
<span class="line-removed">3321 </span>
<span class="line-removed">3322       assert(pss-&gt;queue_is_empty(), &quot;should be empty&quot;);</span>
<span class="line-removed">3323 </span>
<span class="line-removed">3324       // Close the inner scope so that the ResourceMark and HandleMark</span>
<span class="line-removed">3325       // destructors are executed here and are included as part of the</span>
<span class="line-removed">3326       // &quot;GC Worker Time&quot;.</span>
<span class="line-removed">3327     }</span>
<span class="line-removed">3328     _g1h-&gt;phase_times()-&gt;record_time_secs(G1GCPhaseTimes::GCWorkerEnd, worker_id, os::elapsedTime());</span>
<span class="line-removed">3329   }</span>
<span class="line-removed">3330 };</span>
<span class="line-removed">3331 </span>
3332 void G1CollectedHeap::complete_cleaning(BoolObjectClosure* is_alive,
3333                                         bool class_unloading_occurred) {
3334   uint num_workers = workers()-&gt;active_workers();
<span class="line-modified">3335   ParallelCleaningTask unlink_task(is_alive, num_workers, class_unloading_occurred, false);</span>
3336   workers()-&gt;run_task(&amp;unlink_task);
3337 }
3338 
3339 // Clean string dedup data structures.
3340 // Ideally we would prefer to use a StringDedupCleaningTask here, but we want to
3341 // record the durations of the phases. Hence the almost-copy.
3342 class G1StringDedupCleaningTask : public AbstractGangTask {
3343   BoolObjectClosure* _is_alive;
3344   OopClosure* _keep_alive;
3345   G1GCPhaseTimes* _phase_times;
3346 
3347 public:
3348   G1StringDedupCleaningTask(BoolObjectClosure* is_alive,
3349                             OopClosure* keep_alive,
3350                             G1GCPhaseTimes* phase_times) :
3351     AbstractGangTask(&quot;Partial Cleaning Task&quot;),
3352     _is_alive(is_alive),
3353     _keep_alive(keep_alive),
3354     _phase_times(phase_times)
3355   {
</pre>
<hr />
<pre>
3366     {
3367       G1GCParPhaseTimesTracker x(_phase_times, G1GCPhaseTimes::StringDedupQueueFixup, worker_id);
3368       StringDedupQueue::unlink_or_oops_do(&amp;cl);
3369     }
3370     {
3371       G1GCParPhaseTimesTracker x(_phase_times, G1GCPhaseTimes::StringDedupTableFixup, worker_id);
3372       StringDedupTable::unlink_or_oops_do(&amp;cl, worker_id);
3373     }
3374   }
3375 };
3376 
3377 void G1CollectedHeap::string_dedup_cleaning(BoolObjectClosure* is_alive,
3378                                             OopClosure* keep_alive,
3379                                             G1GCPhaseTimes* phase_times) {
3380   G1StringDedupCleaningTask cl(is_alive, keep_alive, phase_times);
3381   workers()-&gt;run_task(&amp;cl);
3382 }
3383 
3384 class G1RedirtyLoggedCardsTask : public AbstractGangTask {
3385  private:
<span class="line-modified">3386   G1DirtyCardQueueSet* _queue;</span>
3387   G1CollectedHeap* _g1h;















3388  public:
<span class="line-modified">3389   G1RedirtyLoggedCardsTask(G1DirtyCardQueueSet* queue, G1CollectedHeap* g1h) : AbstractGangTask(&quot;Redirty Cards&quot;),</span>
<span class="line-modified">3390     _queue(queue), _g1h(g1h) { }</span>

3391 
3392   virtual void work(uint worker_id) {
3393     G1GCPhaseTimes* p = _g1h-&gt;phase_times();
3394     G1GCParPhaseTimesTracker x(p, G1GCPhaseTimes::RedirtyCards, worker_id);
3395 
3396     RedirtyLoggedCardTableEntryClosure cl(_g1h);
<span class="line-modified">3397     _queue-&gt;par_apply_closure_to_all_completed_buffers(&amp;cl);</span>
3398 
3399     p-&gt;record_thread_work_item(G1GCPhaseTimes::RedirtyCards, worker_id, cl.num_dirtied());
3400   }
3401 };
3402 
<span class="line-modified">3403 void G1CollectedHeap::redirty_logged_cards() {</span>
3404   double redirty_logged_cards_start = os::elapsedTime();
3405 
<span class="line-modified">3406   G1RedirtyLoggedCardsTask redirty_task(&amp;dirty_card_queue_set(), this);</span>
<span class="line-removed">3407   dirty_card_queue_set().reset_for_par_iteration();</span>
3408   workers()-&gt;run_task(&amp;redirty_task);
3409 
3410   G1DirtyCardQueueSet&amp; dcq = G1BarrierSet::dirty_card_queue_set();
<span class="line-modified">3411   dcq.merge_bufferlists(&amp;dirty_card_queue_set());</span>
<span class="line-removed">3412   assert(dirty_card_queue_set().completed_buffers_num() == 0, &quot;All should be consumed&quot;);</span>
3413 
3414   phase_times()-&gt;record_redirty_logged_cards_time_ms((os::elapsedTime() - redirty_logged_cards_start) * 1000.0);
3415 }
3416 
3417 // Weak Reference Processing support
3418 
3419 bool G1STWIsAliveClosure::do_object_b(oop p) {
3420   // An object is reachable if it is outside the collection set,
3421   // or is inside and copied.
3422   return !_g1h-&gt;is_in_cset(p) || p-&gt;is_forwarded();
3423 }
3424 
3425 bool G1STWSubjectToDiscoveryClosure::do_object_b(oop obj) {
3426   assert(obj != NULL, &quot;must not be NULL&quot;);
3427   assert(_g1h-&gt;is_in_reserved(obj), &quot;Trying to discover obj &quot; PTR_FORMAT &quot; not in heap&quot;, p2i(obj));
3428   // The areas the CM and STW ref processor manage must be disjoint. The is_in_cset() below
3429   // may falsely indicate that this is not the case here: however the collection set only
3430   // contains old regions when concurrent mark is not running.
3431   return _g1h-&gt;is_in_cset(obj) || _g1h-&gt;heap_region_containing(obj)-&gt;is_survivor();
3432 }
3433 
3434 // Non Copying Keep Alive closure
3435 class G1KeepAliveClosure: public OopClosure {
3436   G1CollectedHeap*_g1h;
3437 public:
3438   G1KeepAliveClosure(G1CollectedHeap* g1h) :_g1h(g1h) {}
3439   void do_oop(narrowOop* p) { guarantee(false, &quot;Not needed&quot;); }
3440   void do_oop(oop* p) {
3441     oop obj = *p;
3442     assert(obj != NULL, &quot;the caller should have filtered out NULL values&quot;);
3443 
<span class="line-modified">3444     const InCSetState cset_state =_g1h-&gt;in_cset_state(obj);</span>
<span class="line-modified">3445     if (!cset_state.is_in_cset_or_humongous()) {</span>
3446       return;
3447     }
<span class="line-modified">3448     if (cset_state.is_in_cset()) {</span>
3449       assert( obj-&gt;is_forwarded(), &quot;invariant&quot; );
3450       *p = obj-&gt;forwardee();
3451     } else {
3452       assert(!obj-&gt;is_forwarded(), &quot;invariant&quot; );
<span class="line-modified">3453       assert(cset_state.is_humongous(),</span>
<span class="line-modified">3454              &quot;Only allowed InCSet state is IsHumongous, but is %d&quot;, cset_state.value());</span>
3455      _g1h-&gt;set_humongous_is_live(obj);
3456     }
3457   }
3458 };
3459 
3460 // Copying Keep Alive closure - can be called from both
3461 // serial and parallel code as long as different worker
3462 // threads utilize different G1ParScanThreadState instances
3463 // and different queues.
3464 
3465 class G1CopyingKeepAliveClosure: public OopClosure {
3466   G1CollectedHeap*         _g1h;
3467   G1ParScanThreadState*    _par_scan_state;
3468 
3469 public:
3470   G1CopyingKeepAliveClosure(G1CollectedHeap* g1h,
3471                             G1ParScanThreadState* pss):
3472     _g1h(g1h),
3473     _par_scan_state(pss)
3474   {}
</pre>
<hr />
<pre>
3540     _g1h(g1h),
3541     _pss(per_thread_states),
3542     _queues(task_queues),
3543     _workers(workers)
3544   {
3545     g1h-&gt;ref_processor_stw()-&gt;set_active_mt_degree(workers-&gt;active_workers());
3546   }
3547 
3548   // Executes the given task using concurrent marking worker threads.
3549   virtual void execute(ProcessTask&amp; task, uint ergo_workers);
3550 };
3551 
3552 // Gang task for possibly parallel reference processing
3553 
3554 class G1STWRefProcTaskProxy: public AbstractGangTask {
3555   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
3556   ProcessTask&amp;     _proc_task;
3557   G1CollectedHeap* _g1h;
3558   G1ParScanThreadStateSet* _pss;
3559   RefToScanQueueSet* _task_queues;
<span class="line-modified">3560   ParallelTaskTerminator* _terminator;</span>
3561 
3562 public:
3563   G1STWRefProcTaskProxy(ProcessTask&amp; proc_task,
3564                         G1CollectedHeap* g1h,
3565                         G1ParScanThreadStateSet* per_thread_states,
3566                         RefToScanQueueSet *task_queues,
<span class="line-modified">3567                         ParallelTaskTerminator* terminator) :</span>
3568     AbstractGangTask(&quot;Process reference objects in parallel&quot;),
3569     _proc_task(proc_task),
3570     _g1h(g1h),
3571     _pss(per_thread_states),
3572     _task_queues(task_queues),
3573     _terminator(terminator)
3574   {}
3575 
3576   virtual void work(uint worker_id) {
3577     // The reference processing task executed by a single worker.
3578     ResourceMark rm;
3579     HandleMark   hm;
3580 
3581     G1STWIsAliveClosure is_alive(_g1h);
3582 
3583     G1ParScanThreadState* pss = _pss-&gt;state_for_worker(worker_id);
3584     pss-&gt;set_ref_discoverer(NULL);
3585 
3586     // Keep alive closure.
3587     G1CopyingKeepAliveClosure keep_alive(_g1h, pss);
</pre>
<hr />
<pre>
3592     // Call the reference processing task&#39;s work routine.
3593     _proc_task.work(worker_id, is_alive, keep_alive, drain_queue);
3594 
3595     // Note we cannot assert that the refs array is empty here as not all
3596     // of the processing tasks (specifically phase2 - pp2_work) execute
3597     // the complete_gc closure (which ordinarily would drain the queue) so
3598     // the queue may not be empty.
3599   }
3600 };
3601 
3602 // Driver routine for parallel reference processing.
3603 // Creates an instance of the ref processing gang
3604 // task and has the worker threads execute it.
3605 void G1STWRefProcTaskExecutor::execute(ProcessTask&amp; proc_task, uint ergo_workers) {
3606   assert(_workers != NULL, &quot;Need parallel worker threads.&quot;);
3607 
3608   assert(_workers-&gt;active_workers() &gt;= ergo_workers,
3609          &quot;Ergonomically chosen workers (%u) should be less than or equal to active workers (%u)&quot;,
3610          ergo_workers, _workers-&gt;active_workers());
3611   TaskTerminator terminator(ergo_workers, _queues);
<span class="line-modified">3612   G1STWRefProcTaskProxy proc_task_proxy(proc_task, _g1h, _pss, _queues, terminator.terminator());</span>
3613 
3614   _workers-&gt;run_task(&amp;proc_task_proxy, ergo_workers);
3615 }
3616 
3617 // End of weak reference support closures
3618 
3619 void G1CollectedHeap::process_discovered_references(G1ParScanThreadStateSet* per_thread_states) {
3620   double ref_proc_start = os::elapsedTime();
3621 
3622   ReferenceProcessor* rp = _ref_processor_stw;
3623   assert(rp-&gt;discovery_enabled(), &quot;should have been enabled&quot;);
3624 
3625   // Closure to test whether a referent is alive.
3626   G1STWIsAliveClosure is_alive(this);
3627 
3628   // Even when parallel reference processing is enabled, the processing
3629   // of JNI refs is serial and performed serially by the current thread
3630   // rather than by a worker. The following PSS will be used for processing
3631   // JNI refs.
3632 
</pre>
<hr />
<pre>
3660     // Parallel reference processing
3661     assert(no_of_gc_workers &lt;= rp-&gt;max_num_queues(),
3662            &quot;Mismatch between the number of GC workers %u and the maximum number of Reference process queues %u&quot;,
3663            no_of_gc_workers,  rp-&gt;max_num_queues());
3664 
3665     G1STWRefProcTaskExecutor par_task_executor(this, per_thread_states, workers(), _task_queues);
3666     stats = rp-&gt;process_discovered_references(&amp;is_alive,
3667                                               &amp;keep_alive,
3668                                               &amp;drain_queue,
3669                                               &amp;par_task_executor,
3670                                               pt);
3671   }
3672 
3673   _gc_tracer_stw-&gt;report_gc_reference_stats(stats);
3674 
3675   // We have completed copying any necessary live referent objects.
3676   assert(pss-&gt;queue_is_empty(), &quot;both queue and overflow should be empty&quot;);
3677 
3678   make_pending_list_reachable();
3679 

3680   rp-&gt;verify_no_references_recorded();
3681 
3682   double ref_proc_time = os::elapsedTime() - ref_proc_start;
3683   phase_times()-&gt;record_ref_proc_time(ref_proc_time * 1000.0);
3684 }
3685 
3686 void G1CollectedHeap::make_pending_list_reachable() {
3687   if (collector_state()-&gt;in_initial_mark_gc()) {
3688     oop pll_head = Universe::reference_pending_list();
3689     if (pll_head != NULL) {
3690       // Any valid worker id is fine here as we are in the VM thread and single-threaded.
3691       _cm-&gt;mark_in_next_bitmap(0 /* worker_id */, pll_head);
3692     }
3693   }
3694 }
3695 
3696 void G1CollectedHeap::merge_per_thread_state_info(G1ParScanThreadStateSet* per_thread_states) {
<span class="line-modified">3697   double merge_pss_time_start = os::elapsedTime();</span>
3698   per_thread_states-&gt;flush();
<span class="line-modified">3699   phase_times()-&gt;record_merge_pss_time_ms((os::elapsedTime() - merge_pss_time_start) * 1000.0);</span>
3700 }
3701 
<span class="line-modified">3702 void G1CollectedHeap::pre_evacuate_collection_set() {</span>













































































































































3703   _expand_heap_after_alloc_failure = true;
3704   _evacuation_failed = false;
3705 
3706   // Disable the hot card cache.
3707   _hot_card_cache-&gt;reset_hot_cache_claimed_index();
3708   _hot_card_cache-&gt;set_use_cache(false);
3709 
<span class="line-modified">3710   rem_set()-&gt;prepare_for_oops_into_collection_set_do();</span>


















3711   _preserved_marks_set.assert_empty();
3712 




3713   // InitialMark needs claim bits to keep track of the marked-through CLDs.
3714   if (collector_state()-&gt;in_initial_mark_gc()) {


3715     double start_clear_claimed_marks = os::elapsedTime();
3716 
3717     ClassLoaderDataGraph::clear_claimed_marks();
3718 
3719     double recorded_clear_claimed_marks_time_ms = (os::elapsedTime() - start_clear_claimed_marks) * 1000.0;
3720     phase_times()-&gt;record_clear_claimed_marks_time_ms(recorded_clear_claimed_marks_time_ms);
3721   }
<span class="line-removed">3722 }</span>
3723 
<span class="line-removed">3724 void G1CollectedHeap::evacuate_collection_set(G1ParScanThreadStateSet* per_thread_states) {</span>
3725   // Should G1EvacuationFailureALot be in effect for this GC?
3726   NOT_PRODUCT(set_evacuation_failure_alot_for_current_gc();)

3727 
<span class="line-modified">3728   assert(dirty_card_queue_set().completed_buffers_num() == 0, &quot;Should be empty&quot;);</span>






3729 
<span class="line-modified">3730   double start_par_time_sec = os::elapsedTime();</span>
<span class="line-modified">3731   double end_par_time_sec;</span>



3732 
<span class="line-modified">3733   {</span>
<span class="line-modified">3734     const uint n_workers = workers()-&gt;active_workers();</span>
<span class="line-modified">3735     G1RootProcessor root_processor(this, n_workers);</span>
<span class="line-removed">3736     G1ParTask g1_par_task(this, per_thread_states, _task_queues, &amp;root_processor, n_workers);</span>
3737 
<span class="line-modified">3738     workers()-&gt;run_task(&amp;g1_par_task);</span>
<span class="line-modified">3739     end_par_time_sec = os::elapsedTime();</span>


3740 
<span class="line-modified">3741     // Closing the inner scope will execute the destructor</span>
<span class="line-modified">3742     // for the G1RootProcessor object. We record the current</span>
<span class="line-modified">3743     // elapsed time before closing the scope so that time</span>
<span class="line-modified">3744     // taken for the destructor is NOT included in the</span>
<span class="line-modified">3745     // reported parallel time.</span>



3746   }
3747 
<span class="line-modified">3748   double par_time_ms = (end_par_time_sec - start_par_time_sec) * 1000.0;</span>
<span class="line-removed">3749   phase_times()-&gt;record_par_time(par_time_ms);</span>
3750 
<span class="line-modified">3751   double code_root_fixup_time_ms =</span>
<span class="line-removed">3752         (os::elapsedTime() - end_par_time_sec) * 1000.0;</span>
<span class="line-removed">3753   phase_times()-&gt;record_code_root_fixup_time(code_root_fixup_time_ms);</span>
<span class="line-removed">3754 }</span>
3755 
<span class="line-modified">3756 class G1EvacuateOptionalRegionTask : public AbstractGangTask {</span>
<span class="line-removed">3757   G1CollectedHeap* _g1h;</span>
<span class="line-removed">3758   G1ParScanThreadStateSet* _per_thread_states;</span>
<span class="line-removed">3759   G1OptionalCSet* _optional;</span>
<span class="line-removed">3760   RefToScanQueueSet* _queues;</span>
<span class="line-removed">3761   ParallelTaskTerminator _terminator;</span>
3762 
<span class="line-modified">3763   Tickspan trim_ticks(G1ParScanThreadState* pss) {</span>
<span class="line-removed">3764     Tickspan copy_time = pss-&gt;trim_ticks();</span>
<span class="line-removed">3765     pss-&gt;reset_trim_ticks();</span>
<span class="line-removed">3766     return copy_time;</span>
<span class="line-removed">3767   }</span>
3768 
<span class="line-modified">3769   void scan_roots(G1ParScanThreadState* pss, uint worker_id) {</span>
<span class="line-modified">3770     G1EvacuationRootClosures* root_cls = pss-&gt;closures();</span>
<span class="line-modified">3771     G1ScanObjsDuringScanRSClosure obj_cl(_g1h, pss);</span>
<span class="line-modified">3772 </span>
<span class="line-modified">3773     size_t scanned = 0;</span>
<span class="line-modified">3774     size_t claimed = 0;</span>
<span class="line-modified">3775     size_t skipped = 0;</span>
<span class="line-modified">3776     size_t used_memory = 0;</span>
<span class="line-modified">3777 </span>
<span class="line-modified">3778     Ticks    start = Ticks::now();</span>
<span class="line-modified">3779     Tickspan copy_time;</span>
<span class="line-modified">3780 </span>
<span class="line-modified">3781     for (uint i = _optional-&gt;current_index(); i &lt; _optional-&gt;current_limit(); i++) {</span>
<span class="line-modified">3782       HeapRegion* hr = _optional-&gt;region_at(i);</span>
<span class="line-modified">3783       G1ScanRSForOptionalClosure scan_opt_cl(&amp;obj_cl);</span>
<span class="line-modified">3784       pss-&gt;oops_into_optional_region(hr)-&gt;oops_do(&amp;scan_opt_cl, root_cls-&gt;raw_strong_oops());</span>
<span class="line-modified">3785       copy_time += trim_ticks(pss);</span>
<span class="line-modified">3786 </span>
<span class="line-modified">3787       G1ScanRSForRegionClosure scan_rs_cl(_g1h-&gt;rem_set()-&gt;scan_state(), &amp;obj_cl, pss, G1GCPhaseTimes::OptScanRS, worker_id);</span>
<span class="line-modified">3788       scan_rs_cl.do_heap_region(hr);</span>
<span class="line-modified">3789       copy_time += trim_ticks(pss);</span>
<span class="line-modified">3790       scanned += scan_rs_cl.cards_scanned();</span>
<span class="line-removed">3791       claimed += scan_rs_cl.cards_claimed();</span>
<span class="line-removed">3792       skipped += scan_rs_cl.cards_skipped();</span>
<span class="line-removed">3793 </span>
<span class="line-removed">3794       // Chunk lists for this region is no longer needed.</span>
<span class="line-removed">3795       used_memory += pss-&gt;oops_into_optional_region(hr)-&gt;used_memory();</span>
3796     }
3797 
<span class="line-modified">3798     Tickspan scan_time = (Ticks::now() - start) - copy_time;</span>
<span class="line-modified">3799     G1GCPhaseTimes* p = _g1h-&gt;phase_times();</span>
<span class="line-modified">3800     p-&gt;record_or_add_time_secs(G1GCPhaseTimes::OptScanRS, worker_id, scan_time.seconds());</span>
<span class="line-removed">3801     p-&gt;record_or_add_time_secs(G1GCPhaseTimes::OptObjCopy, worker_id, copy_time.seconds());</span>
3802 
<span class="line-modified">3803     p-&gt;record_or_add_thread_work_item(G1GCPhaseTimes::OptScanRS, worker_id, scanned, G1GCPhaseTimes::OptCSetScannedCards);</span>
<span class="line-modified">3804     p-&gt;record_or_add_thread_work_item(G1GCPhaseTimes::OptScanRS, worker_id, claimed, G1GCPhaseTimes::OptCSetClaimedCards);</span>
<span class="line-modified">3805     p-&gt;record_or_add_thread_work_item(G1GCPhaseTimes::OptScanRS, worker_id, skipped, G1GCPhaseTimes::OptCSetSkippedCards);</span>
<span class="line-modified">3806     p-&gt;record_or_add_thread_work_item(G1GCPhaseTimes::OptScanRS, worker_id, used_memory, G1GCPhaseTimes::OptCSetUsedMemory);</span>



3807   }
3808 
3809   void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) {
<span class="line-modified">3810     Ticks start = Ticks::now();</span>
<span class="line-modified">3811     G1ParEvacuateFollowersClosure cl(_g1h, pss, _queues, &amp;_terminator, G1GCPhaseTimes::OptObjCopy);</span>
<span class="line-removed">3812     cl.do_void();</span>
3813 
<span class="line-modified">3814     Tickspan evac_time = (Ticks::now() - start);</span>
<span class="line-modified">3815     G1GCPhaseTimes* p = _g1h-&gt;phase_times();</span>
<span class="line-removed">3816     p-&gt;record_or_add_time_secs(G1GCPhaseTimes::OptObjCopy, worker_id, evac_time.seconds());</span>
<span class="line-removed">3817     assert(pss-&gt;trim_ticks().seconds() == 0.0, &quot;Unexpected partial trimming done during optional evacuation&quot;);</span>
3818   }
3819 
<span class="line-modified">3820  public:</span>
<span class="line-modified">3821   G1EvacuateOptionalRegionTask(G1CollectedHeap* g1h,</span>
<span class="line-removed">3822                                G1ParScanThreadStateSet* per_thread_states,</span>
<span class="line-removed">3823                                G1OptionalCSet* cset,</span>
<span class="line-removed">3824                                RefToScanQueueSet* queues,</span>
<span class="line-removed">3825                                uint n_workers) :</span>
<span class="line-removed">3826     AbstractGangTask(&quot;G1 Evacuation Optional Region Task&quot;),</span>
<span class="line-removed">3827     _g1h(g1h),</span>
<span class="line-removed">3828     _per_thread_states(per_thread_states),</span>
<span class="line-removed">3829     _optional(cset),</span>
<span class="line-removed">3830     _queues(queues),</span>
<span class="line-removed">3831     _terminator(n_workers, _queues) {</span>
3832   }
3833 
<span class="line-modified">3834   void work(uint worker_id) {</span>
<span class="line-modified">3835     ResourceMark rm;</span>
<span class="line-modified">3836     HandleMark  hm;</span>







3837 
<span class="line-modified">3838     G1ParScanThreadState* pss = _per_thread_states-&gt;state_for_worker(worker_id);</span>
<span class="line-modified">3839     pss-&gt;set_ref_discoverer(_g1h-&gt;ref_processor_stw());</span>
3840 
<span class="line-modified">3841     scan_roots(pss, worker_id);</span>
<span class="line-modified">3842     evacuate_live_objects(pss, worker_id);</span>


3843   }
<span class="line-removed">3844 };</span>
3845 
<span class="line-modified">3846 void G1CollectedHeap::evacuate_optional_regions(G1ParScanThreadStateSet* per_thread_states, G1OptionalCSet* ocset) {</span>
<span class="line-modified">3847   class G1MarkScope : public MarkScope {};</span>
<span class="line-modified">3848   G1MarkScope code_mark_scope;</span>










3849 
<span class="line-modified">3850   G1EvacuateOptionalRegionTask task(this, per_thread_states, ocset, _task_queues, workers()-&gt;active_workers());</span>
<span class="line-modified">3851   workers()-&gt;run_task(&amp;task);</span>
3852 }
3853 
<span class="line-modified">3854 void G1CollectedHeap::evacuate_optional_collection_set(G1ParScanThreadStateSet* per_thread_states) {</span>
<span class="line-modified">3855   G1OptionalCSet optional_cset(&amp;_collection_set, per_thread_states);</span>
<span class="line-modified">3856   if (optional_cset.is_empty()) {</span>
<span class="line-modified">3857     return;</span>

3858   }
3859 
<span class="line-modified">3860   if (evacuation_failed()) {</span>
<span class="line-modified">3861     return;</span>





















3862   }

3863 





3864   const double gc_start_time_ms = phase_times()-&gt;cur_collection_start_sec() * 1000.0;
3865 
<span class="line-modified">3866   double start_time_sec = os::elapsedTime();</span>
3867 
<span class="line-removed">3868   do {</span>
3869     double time_used_ms = os::elapsedTime() * 1000.0 - gc_start_time_ms;
3870     double time_left_ms = MaxGCPauseMillis - time_used_ms;
3871 
<span class="line-modified">3872     if (time_left_ms &lt; 0) {</span>
<span class="line-modified">3873       log_trace(gc, ergo, cset)(&quot;Skipping %u optional regions, pause time exceeded %.3fms&quot;, optional_cset.size(), time_used_ms);</span>


3874       break;
3875     }
3876 
<span class="line-modified">3877     optional_cset.prepare_evacuation(time_left_ms * _policy-&gt;optional_evacuation_fraction());</span>
<span class="line-modified">3878     if (optional_cset.prepare_failed()) {</span>
<span class="line-modified">3879       log_trace(gc, ergo, cset)(&quot;Skipping %u optional regions, no regions can be evacuated in %.3fms&quot;, optional_cset.size(), time_left_ms);</span>
<span class="line-modified">3880       break;</span>
3881     }
3882 
<span class="line-modified">3883     evacuate_optional_regions(per_thread_states, &amp;optional_cset);</span>
<span class="line-modified">3884 </span>
<span class="line-modified">3885     optional_cset.complete_evacuation();</span>
<span class="line-modified">3886     if (optional_cset.evacuation_failed()) {</span>
<span class="line-removed">3887       break;</span>
3888     }
<span class="line-modified">3889   } while (!optional_cset.is_empty());</span>
3890 
<span class="line-modified">3891   phase_times()-&gt;record_optional_evacuation((os::elapsedTime() - start_time_sec) * 1000.0);</span>
3892 }
3893 
<span class="line-modified">3894 void G1CollectedHeap::post_evacuate_collection_set(G1EvacuationInfo&amp; evacuation_info, G1ParScanThreadStateSet* per_thread_states) {</span>
<span class="line-modified">3895   // Also cleans the card table from temporary duplicate detection information used</span>
<span class="line-modified">3896   // during UpdateRS/ScanRS.</span>
<span class="line-modified">3897   rem_set()-&gt;cleanup_after_oops_into_collection_set_do();</span>


3898 
3899   // Process any discovered reference objects - we have
3900   // to do this _before_ we retire the GC alloc regions
3901   // as we may have to copy some &#39;reachable&#39; referent
3902   // objects (and their reachable sub-graphs) that were
3903   // not copied during the pause.
3904   process_discovered_references(per_thread_states);
3905 
3906   G1STWIsAliveClosure is_alive(this);
3907   G1KeepAliveClosure keep_alive(this);
3908 
<span class="line-modified">3909   WeakProcessor::weak_oops_do(workers(), &amp;is_alive, &amp;keep_alive,</span>
<span class="line-removed">3910                               phase_times()-&gt;weak_phase_times());</span>
3911 
3912   if (G1StringDedup::is_enabled()) {
3913     double string_dedup_time_ms = os::elapsedTime();
3914 
<span class="line-modified">3915     string_dedup_cleaning(&amp;is_alive, &amp;keep_alive, phase_times());</span>
3916 
3917     double string_cleanup_time_ms = (os::elapsedTime() - string_dedup_time_ms) * 1000.0;
<span class="line-modified">3918     phase_times()-&gt;record_string_deduplication_time(string_cleanup_time_ms);</span>
3919   }
3920 


3921   if (evacuation_failed()) {
<span class="line-modified">3922     restore_after_evac_failure();</span>
3923 
3924     // Reset the G1EvacuationFailureALot counters and flags
<span class="line-removed">3925     // Note: the values are reset only when an actual</span>
<span class="line-removed">3926     // evacuation failure occurs.</span>
3927     NOT_PRODUCT(reset_evacuation_should_fail();)

















3928   }
3929 
3930   _preserved_marks_set.assert_empty();
3931 
<span class="line-removed">3932   _allocator-&gt;release_gc_alloc_regions(evacuation_info);</span>
<span class="line-removed">3933 </span>
3934   merge_per_thread_state_info(per_thread_states);
3935 
3936   // Reset and re-enable the hot card cache.
3937   // Note the counts for the cards in the regions in the
3938   // collection set are reset when the collection set is freed.
3939   _hot_card_cache-&gt;reset_hot_cache();
3940   _hot_card_cache-&gt;set_use_cache(true);
3941 
3942   purge_code_root_memory();
3943 
<span class="line-modified">3944   redirty_logged_cards();</span>










3945 #if COMPILER2_OR_JVMCI
3946   double start = os::elapsedTime();
3947   DerivedPointerTable::update_pointers();
3948   phase_times()-&gt;record_derived_pointer_table_update_time((os::elapsedTime() - start) * 1000.0);
3949 #endif
3950   policy()-&gt;print_age_table();
3951 }
3952 
3953 void G1CollectedHeap::record_obj_copy_mem_stats() {
3954   policy()-&gt;add_bytes_allocated_in_old_since_last_gc(_old_evac_stats.allocated() * HeapWordSize);
3955 
3956   _gc_tracer_stw-&gt;report_evacuation_statistics(create_g1_evac_summary(&amp;_survivor_evac_stats),
3957                                                create_g1_evac_summary(&amp;_old_evac_stats));
3958 }
3959 
<span class="line-modified">3960 void G1CollectedHeap::free_region(HeapRegion* hr,</span>
<span class="line-removed">3961                                   FreeRegionList* free_list,</span>
<span class="line-removed">3962                                   bool skip_remset,</span>
<span class="line-removed">3963                                   bool skip_hot_card_cache,</span>
<span class="line-removed">3964                                   bool locked) {</span>
3965   assert(!hr-&gt;is_free(), &quot;the region should not be free&quot;);
3966   assert(!hr-&gt;is_empty(), &quot;the region should not be empty&quot;);
3967   assert(_hrm-&gt;is_available(hr-&gt;hrm_index()), &quot;region should be committed&quot;);
<span class="line-removed">3968   assert(free_list != NULL, &quot;pre-condition&quot;);</span>
3969 
3970   if (G1VerifyBitmaps) {
3971     MemRegion mr(hr-&gt;bottom(), hr-&gt;end());
3972     concurrent_mark()-&gt;clear_range_in_prev_bitmap(mr);
3973   }
3974 
3975   // Clear the card counts for this region.
3976   // Note: we only need to do this if the region is not young
3977   // (since we don&#39;t refine cards in young regions).
<span class="line-modified">3978   if (!skip_hot_card_cache &amp;&amp; !hr-&gt;is_young()) {</span>
3979     _hot_card_cache-&gt;reset_card_counts(hr);
3980   }
<span class="line-modified">3981   hr-&gt;hr_clear(skip_remset, true /* clear_space */, locked /* locked */);</span>


3982   _policy-&gt;remset_tracker()-&gt;update_at_free(hr);
<span class="line-modified">3983   free_list-&gt;add_ordered(hr);</span>



3984 }
3985 
3986 void G1CollectedHeap::free_humongous_region(HeapRegion* hr,
3987                                             FreeRegionList* free_list) {
3988   assert(hr-&gt;is_humongous(), &quot;this is only for humongous regions&quot;);
3989   assert(free_list != NULL, &quot;pre-condition&quot;);
3990   hr-&gt;clear_humongous();
<span class="line-modified">3991   free_region(hr, free_list, false /* skip_remset */, false /* skip_hcc */, true /* locked */);</span>
3992 }
3993 
3994 void G1CollectedHeap::remove_from_old_sets(const uint old_regions_removed,
3995                                            const uint humongous_regions_removed) {
3996   if (old_regions_removed &gt; 0 || humongous_regions_removed &gt; 0) {
<span class="line-modified">3997     MutexLockerEx x(OldSets_lock, Mutex::_no_safepoint_check_flag);</span>
3998     _old_set.bulk_remove(old_regions_removed);
3999     _humongous_set.bulk_remove(humongous_regions_removed);
4000   }
4001 
4002 }
4003 
4004 void G1CollectedHeap::prepend_to_freelist(FreeRegionList* list) {
4005   assert(list != NULL, &quot;list can&#39;t be null&quot;);
4006   if (!list-&gt;is_empty()) {
<span class="line-modified">4007     MutexLockerEx x(FreeList_lock, Mutex::_no_safepoint_check_flag);</span>
4008     _hrm-&gt;insert_list_into_free_list(list);
4009   }
4010 }
4011 
4012 void G1CollectedHeap::decrement_summary_bytes(size_t bytes) {
4013   decrease_used(bytes);
4014 }
4015 
4016 class G1FreeCollectionSetTask : public AbstractGangTask {
<span class="line-modified">4017 private:</span>
<span class="line-modified">4018 </span>
<span class="line-modified">4019   // Closure applied to all regions in the collection set to do work that needs to</span>
<span class="line-modified">4020   // be done serially in a single thread.</span>
<span class="line-modified">4021   class G1SerialFreeCollectionSetClosure : public HeapRegionClosure {</span>
<span class="line-modified">4022   private:</span>
<span class="line-modified">4023     G1EvacuationInfo* _evacuation_info;</span>
<span class="line-modified">4024     const size_t* _surviving_young_words;</span>
<span class="line-modified">4025 </span>
<span class="line-removed">4026     // Bytes used in successfully evacuated regions before the evacuation.</span>
<span class="line-removed">4027     size_t _before_used_bytes;</span>
<span class="line-removed">4028     // Bytes used in unsucessfully evacuated regions before the evacuation</span>
<span class="line-removed">4029     size_t _after_used_bytes;</span>
<span class="line-removed">4030 </span>
<span class="line-removed">4031     size_t _bytes_allocated_in_old_since_last_gc;</span>
<span class="line-removed">4032 </span>
<span class="line-removed">4033     size_t _failure_used_words;</span>
<span class="line-removed">4034     size_t _failure_waste_words;</span>
<span class="line-removed">4035 </span>
<span class="line-removed">4036     FreeRegionList _local_free_list;</span>
4037   public:
<span class="line-modified">4038     G1SerialFreeCollectionSetClosure(G1EvacuationInfo* evacuation_info, const size_t* surviving_young_words) :</span>
<span class="line-modified">4039       HeapRegionClosure(),</span>
<span class="line-modified">4040       _evacuation_info(evacuation_info),</span>
<span class="line-modified">4041       _surviving_young_words(surviving_young_words),</span>
<span class="line-modified">4042       _before_used_bytes(0),</span>
<span class="line-modified">4043       _after_used_bytes(0),</span>
<span class="line-modified">4044       _bytes_allocated_in_old_since_last_gc(0),</span>
<span class="line-modified">4045       _failure_used_words(0),</span>
<span class="line-modified">4046       _failure_waste_words(0),</span>
<span class="line-modified">4047       _local_free_list(&quot;Local Region List for CSet Freeing&quot;) {</span>








4048     }
4049 
<span class="line-modified">4050     virtual bool do_heap_region(HeapRegion* r) {</span>
<span class="line-modified">4051       G1CollectedHeap* g1h = G1CollectedHeap::heap();</span>

4052 
<span class="line-modified">4053       assert(r-&gt;in_collection_set(), &quot;Region %u should be in collection set.&quot;, r-&gt;hrm_index());</span>
<span class="line-modified">4054       g1h-&gt;clear_in_cset(r);</span>
4055 
<span class="line-modified">4056       if (r-&gt;is_young()) {</span>
<span class="line-modified">4057         assert(r-&gt;young_index_in_cset() != -1 &amp;&amp; (uint)r-&gt;young_index_in_cset() &lt; g1h-&gt;collection_set()-&gt;young_region_length(),</span>
<span class="line-modified">4058                &quot;Young index %d is wrong for region %u of type %s with %u young regions&quot;,</span>
<span class="line-modified">4059                r-&gt;young_index_in_cset(),</span>
<span class="line-modified">4060                r-&gt;hrm_index(),</span>
<span class="line-removed">4061                r-&gt;get_type_str(),</span>
<span class="line-removed">4062                g1h-&gt;collection_set()-&gt;young_region_length());</span>
<span class="line-removed">4063         size_t words_survived = _surviving_young_words[r-&gt;young_index_in_cset()];</span>
<span class="line-removed">4064         r-&gt;record_surv_words_in_group(words_survived);</span>
<span class="line-removed">4065       }</span>
4066 
<span class="line-modified">4067       if (!r-&gt;evacuation_failed()) {</span>
<span class="line-modified">4068         assert(r-&gt;not_empty(), &quot;Region %u is an empty region in the collection set.&quot;, r-&gt;hrm_index());</span>
<span class="line-modified">4069         _before_used_bytes += r-&gt;used();</span>
<span class="line-modified">4070         g1h-&gt;free_region(r,</span>
<span class="line-modified">4071                          &amp;_local_free_list,</span>
<span class="line-modified">4072                          true, /* skip_remset */</span>
<span class="line-modified">4073                          true, /* skip_hot_card_cache */</span>
<span class="line-modified">4074                          true  /* locked */);</span>
<span class="line-modified">4075       } else {</span>
<span class="line-modified">4076         r-&gt;uninstall_surv_rate_group();</span>
<span class="line-modified">4077         r-&gt;set_young_index_in_cset(-1);</span>
<span class="line-modified">4078         r-&gt;set_evacuation_failed(false);</span>
<span class="line-modified">4079         // When moving a young gen region to old gen, we &quot;allocate&quot; that whole region</span>
<span class="line-removed">4080         // there. This is in addition to any already evacuated objects. Notify the</span>
<span class="line-removed">4081         // policy about that.</span>
<span class="line-removed">4082         // Old gen regions do not cause an additional allocation: both the objects</span>
<span class="line-removed">4083         // still in the region and the ones already moved are accounted for elsewhere.</span>
<span class="line-removed">4084         if (r-&gt;is_young()) {</span>
<span class="line-removed">4085           _bytes_allocated_in_old_since_last_gc += HeapRegion::GrainBytes;</span>
<span class="line-removed">4086         }</span>
<span class="line-removed">4087         // The region is now considered to be old.</span>
<span class="line-removed">4088         r-&gt;set_old();</span>
<span class="line-removed">4089         // Do some allocation statistics accounting. Regions that failed evacuation</span>
<span class="line-removed">4090         // are always made old, so there is no need to update anything in the young</span>
<span class="line-removed">4091         // gen statistics, but we need to update old gen statistics.</span>
<span class="line-removed">4092         size_t used_words = r-&gt;marked_bytes() / HeapWordSize;</span>
<span class="line-removed">4093 </span>
<span class="line-removed">4094         _failure_used_words += used_words;</span>
<span class="line-removed">4095         _failure_waste_words += HeapRegion::GrainWords - used_words;</span>
<span class="line-removed">4096 </span>
<span class="line-removed">4097         g1h-&gt;old_set_add(r);</span>
<span class="line-removed">4098         _after_used_bytes += r-&gt;used();</span>
4099       }
<span class="line-removed">4100       return false;</span>
4101     }
4102 
<span class="line-modified">4103     void complete_work() {</span>
<span class="line-modified">4104       G1CollectedHeap* g1h = G1CollectedHeap::heap();</span>
<span class="line-modified">4105 </span>
<span class="line-modified">4106       _evacuation_info-&gt;set_regions_freed(_local_free_list.length());</span>
<span class="line-removed">4107       _evacuation_info-&gt;increment_collectionset_used_after(_after_used_bytes);</span>
<span class="line-removed">4108 </span>
<span class="line-removed">4109       g1h-&gt;prepend_to_freelist(&amp;_local_free_list);</span>
<span class="line-removed">4110       g1h-&gt;decrement_summary_bytes(_before_used_bytes);</span>
<span class="line-removed">4111 </span>
<span class="line-removed">4112       G1Policy* policy = g1h-&gt;policy();</span>
<span class="line-removed">4113       policy-&gt;add_bytes_allocated_in_old_since_last_gc(_bytes_allocated_in_old_since_last_gc);</span>
4114 
<span class="line-modified">4115       g1h-&gt;alloc_buffer_stats(InCSetState::Old)-&gt;add_failure_used_and_waste(_failure_used_words, _failure_waste_words);</span>

4116     }
4117   };
4118 
<span class="line-modified">4119   G1CollectionSet* _collection_set;</span>
<span class="line-modified">4120   G1SerialFreeCollectionSetClosure _cl;</span>
<span class="line-modified">4121   const size_t* _surviving_young_words;</span>
<span class="line-modified">4122 </span>
<span class="line-modified">4123   size_t _rs_lengths;</span>
<span class="line-modified">4124 </span>
<span class="line-modified">4125   volatile jint _serial_work_claim;</span>
<span class="line-modified">4126 </span>
<span class="line-modified">4127   struct WorkItem {</span>
<span class="line-modified">4128     uint region_idx;</span>
<span class="line-modified">4129     bool is_young;</span>
<span class="line-modified">4130     bool evacuation_failed;</span>



4131 
<span class="line-modified">4132     WorkItem(HeapRegion* r) {</span>
<span class="line-modified">4133       region_idx = r-&gt;hrm_index();</span>
<span class="line-modified">4134       is_young = r-&gt;is_young();</span>
<span class="line-modified">4135       evacuation_failed = r-&gt;evacuation_failed();</span>

























4136     }
<span class="line-removed">4137   };</span>
<span class="line-removed">4138 </span>
<span class="line-removed">4139   volatile size_t _parallel_work_claim;</span>
<span class="line-removed">4140   size_t _num_work_items;</span>
<span class="line-removed">4141   WorkItem* _work_items;</span>
4142 
<span class="line-modified">4143   void do_serial_work() {</span>
<span class="line-modified">4144     // Need to grab the lock to be allowed to modify the old region list.</span>
<span class="line-modified">4145     MutexLockerEx x(OldSets_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="line-removed">4146     _collection_set-&gt;iterate(&amp;_cl);</span>
<span class="line-removed">4147   }</span>
4148 
<span class="line-modified">4149   void do_parallel_work_for_region(uint region_idx, bool is_young, bool evacuation_failed) {</span>
<span class="line-modified">4150     G1CollectedHeap* g1h = G1CollectedHeap::heap();</span>

4151 
<span class="line-modified">4152     HeapRegion* r = g1h-&gt;region_at(region_idx);</span>
<span class="line-modified">4153     assert(!g1h-&gt;is_on_master_free_list(r), &quot;sanity&quot;);</span>



4154 
<span class="line-modified">4155     Atomic::add(r-&gt;rem_set()-&gt;occupied_locked(), &amp;_rs_lengths);</span>

4156 
<span class="line-modified">4157     if (!is_young) {</span>
<span class="line-modified">4158       g1h-&gt;_hot_card_cache-&gt;reset_card_counts(r);</span>

4159     }
4160 
<span class="line-modified">4161     if (!evacuation_failed) {</span>
<span class="line-modified">4162       r-&gt;rem_set()-&gt;clear_locked();</span>
4163     }
<span class="line-removed">4164   }</span>
4165 
<span class="line-modified">4166   class G1PrepareFreeCollectionSetClosure : public HeapRegionClosure {</span>
<span class="line-modified">4167   private:</span>
<span class="line-modified">4168     size_t _cur_idx;</span>
<span class="line-removed">4169     WorkItem* _work_items;</span>
4170   public:
<span class="line-modified">4171     G1PrepareFreeCollectionSetClosure(WorkItem* work_items) : HeapRegionClosure(), _cur_idx(0), _work_items(work_items) { }</span>









4172 
4173     virtual bool do_heap_region(HeapRegion* r) {
<span class="line-modified">4174       _work_items[_cur_idx++] = WorkItem(r);</span>


















4175       return false;
4176     }











4177   };
4178 
<span class="line-modified">4179   void prepare_work() {</span>
<span class="line-modified">4180     G1PrepareFreeCollectionSetClosure cl(_work_items);</span>
<span class="line-modified">4181     _collection_set-&gt;iterate(&amp;cl);</span>
<span class="line-modified">4182   }</span>



4183 
<span class="line-modified">4184   void complete_work() {</span>
<span class="line-modified">4185     _cl.complete_work();</span>

4186 
<span class="line-modified">4187     G1Policy* policy = G1CollectedHeap::heap()-&gt;policy();</span>
<span class="line-modified">4188     policy-&gt;record_max_rs_lengths(_rs_lengths);</span>
<span class="line-modified">4189     policy-&gt;cset_regions_freed();</span>




4190   }

4191 public:
<span class="line-modified">4192   G1FreeCollectionSetTask(G1CollectionSet* collection_set, G1EvacuationInfo* evacuation_info, const size_t* surviving_young_words) :</span>
<span class="line-modified">4193     AbstractGangTask(&quot;G1 Free Collection Set&quot;),</span>
<span class="line-modified">4194     _collection_set(collection_set),</span>
<span class="line-modified">4195     _cl(evacuation_info, surviving_young_words),</span>
<span class="line-modified">4196     _surviving_young_words(surviving_young_words),</span>
<span class="line-modified">4197     _rs_lengths(0),</span>
<span class="line-modified">4198     _serial_work_claim(0),</span>
<span class="line-modified">4199     _parallel_work_claim(0),</span>
<span class="line-modified">4200     _num_work_items(collection_set-&gt;region_length()),</span>
<span class="line-modified">4201     _work_items(NEW_C_HEAP_ARRAY(WorkItem, _num_work_items, mtGC)) {</span>
<span class="line-modified">4202     prepare_work();</span>
4203   }
4204 
4205   ~G1FreeCollectionSetTask() {
<span class="line-modified">4206     complete_work();</span>
<span class="line-modified">4207     FREE_C_HEAP_ARRAY(WorkItem, _work_items);</span>





4208   }
4209 
<span class="line-removed">4210   // Chunk size for work distribution. The chosen value has been determined experimentally</span>
<span class="line-removed">4211   // to be a good tradeoff between overhead and achievable parallelism.</span>
<span class="line-removed">4212   static uint chunk_size() { return 32; }</span>
<span class="line-removed">4213 </span>
4214   virtual void work(uint worker_id) {
<span class="line-modified">4215     G1GCPhaseTimes* timer = G1CollectedHeap::heap()-&gt;phase_times();</span>
<span class="line-modified">4216 </span>
<span class="line-modified">4217     // Claim serial work.</span>
<span class="line-modified">4218     if (_serial_work_claim == 0) {</span>
<span class="line-removed">4219       jint value = Atomic::add(1, &amp;_serial_work_claim) - 1;</span>
<span class="line-removed">4220       if (value == 0) {</span>
<span class="line-removed">4221         double serial_time = os::elapsedTime();</span>
<span class="line-removed">4222         do_serial_work();</span>
<span class="line-removed">4223         timer-&gt;record_serial_free_cset_time_ms((os::elapsedTime() - serial_time) * 1000.0);</span>
<span class="line-removed">4224       }</span>
<span class="line-removed">4225     }</span>
<span class="line-removed">4226 </span>
<span class="line-removed">4227     // Start parallel work.</span>
<span class="line-removed">4228     double young_time = 0.0;</span>
<span class="line-removed">4229     bool has_young_time = false;</span>
<span class="line-removed">4230     double non_young_time = 0.0;</span>
<span class="line-removed">4231     bool has_non_young_time = false;</span>
<span class="line-removed">4232 </span>
<span class="line-removed">4233     while (true) {</span>
<span class="line-removed">4234       size_t end = Atomic::add(chunk_size(), &amp;_parallel_work_claim);</span>
<span class="line-removed">4235       size_t cur = end - chunk_size();</span>
<span class="line-removed">4236 </span>
<span class="line-removed">4237       if (cur &gt;= _num_work_items) {</span>
<span class="line-removed">4238         break;</span>
<span class="line-removed">4239       }</span>
<span class="line-removed">4240 </span>
<span class="line-removed">4241       EventGCPhaseParallel event;</span>
<span class="line-removed">4242       double start_time = os::elapsedTime();</span>
<span class="line-removed">4243 </span>
<span class="line-removed">4244       end = MIN2(end, _num_work_items);</span>
<span class="line-removed">4245 </span>
<span class="line-removed">4246       for (; cur &lt; end; cur++) {</span>
<span class="line-removed">4247         bool is_young = _work_items[cur].is_young;</span>
<span class="line-removed">4248 </span>
<span class="line-removed">4249         do_parallel_work_for_region(_work_items[cur].region_idx, is_young, _work_items[cur].evacuation_failed);</span>
<span class="line-removed">4250 </span>
<span class="line-removed">4251         double end_time = os::elapsedTime();</span>
<span class="line-removed">4252         double time_taken = end_time - start_time;</span>
<span class="line-removed">4253         if (is_young) {</span>
<span class="line-removed">4254           young_time += time_taken;</span>
<span class="line-removed">4255           has_young_time = true;</span>
<span class="line-removed">4256           event.commit(GCId::current(), worker_id, G1GCPhaseTimes::phase_name(G1GCPhaseTimes::YoungFreeCSet));</span>
<span class="line-removed">4257         } else {</span>
<span class="line-removed">4258           non_young_time += time_taken;</span>
<span class="line-removed">4259           has_non_young_time = true;</span>
<span class="line-removed">4260           event.commit(GCId::current(), worker_id, G1GCPhaseTimes::phase_name(G1GCPhaseTimes::NonYoungFreeCSet));</span>
<span class="line-removed">4261         }</span>
<span class="line-removed">4262         start_time = end_time;</span>
<span class="line-removed">4263       }</span>
<span class="line-removed">4264     }</span>
4265 
<span class="line-modified">4266     if (has_young_time) {</span>
<span class="line-modified">4267       timer-&gt;record_time_secs(G1GCPhaseTimes::YoungFreeCSet, worker_id, young_time);</span>
<span class="line-modified">4268     }</span>
<span class="line-removed">4269     if (has_non_young_time) {</span>
<span class="line-removed">4270       timer-&gt;record_time_secs(G1GCPhaseTimes::NonYoungFreeCSet, worker_id, non_young_time);</span>
<span class="line-removed">4271     }</span>
4272   }
4273 };
4274 
4275 void G1CollectedHeap::free_collection_set(G1CollectionSet* collection_set, G1EvacuationInfo&amp; evacuation_info, const size_t* surviving_young_words) {
4276   _eden.clear();
4277 
<span class="line-modified">4278   double free_cset_start_time = os::elapsedTime();</span>



4279 

4280   {
<span class="line-modified">4281     uint const num_chunks = MAX2(_collection_set.region_length() / G1FreeCollectionSetTask::chunk_size(), 1U);</span>
<span class="line-modified">4282     uint const num_workers = MIN2(workers()-&gt;active_workers(), num_chunks);</span>
<span class="line-modified">4283 </span>
<span class="line-removed">4284     G1FreeCollectionSetTask cl(collection_set, &amp;evacuation_info, surviving_young_words);</span>
4285 
<span class="line-modified">4286     log_debug(gc, ergo)(&quot;Running %s using %u workers for collection set length %u&quot;,</span>
<span class="line-modified">4287                         cl.name(),</span>
<span class="line-removed">4288                         num_workers,</span>
<span class="line-removed">4289                         _collection_set.region_length());</span>
4290     workers()-&gt;run_task(&amp;cl, num_workers);
4291   }
<span class="line-modified">4292   phase_times()-&gt;record_total_free_cset_time_ms((os::elapsedTime() - free_cset_start_time) * 1000.0);</span>






4293 
4294   collection_set-&gt;clear();
4295 }
4296 
4297 class G1FreeHumongousRegionClosure : public HeapRegionClosure {
4298  private:
4299   FreeRegionList* _free_region_list;
4300   HeapRegionSet* _proxy_set;
4301   uint _humongous_objects_reclaimed;
4302   uint _humongous_regions_reclaimed;
4303   size_t _freed_bytes;
4304  public:
4305 
4306   G1FreeHumongousRegionClosure(FreeRegionList* free_region_list) :
4307     _free_region_list(free_region_list), _proxy_set(NULL), _humongous_objects_reclaimed(0), _humongous_regions_reclaimed(0), _freed_bytes(0) {
4308   }
4309 
4310   virtual bool do_heap_region(HeapRegion* r) {
4311     if (!r-&gt;is_starts_humongous()) {
4312       return false;
</pre>
<hr />
<pre>
4430   G1HRPrinter* hrp = hr_printer();
4431   if (hrp-&gt;is_active()) {
4432     FreeRegionListIterator iter(&amp;local_cleanup_list);
4433     while (iter.more_available()) {
4434       HeapRegion* hr = iter.get_next();
4435       hrp-&gt;cleanup(hr);
4436     }
4437   }
4438 
4439   prepend_to_freelist(&amp;local_cleanup_list);
4440   decrement_summary_bytes(cl.bytes_freed());
4441 
4442   phase_times()-&gt;record_fast_reclaim_humongous_time_ms((os::elapsedTime() - start_time) * 1000.0,
4443                                                        cl.humongous_objects_reclaimed());
4444 }
4445 
4446 class G1AbandonCollectionSetClosure : public HeapRegionClosure {
4447 public:
4448   virtual bool do_heap_region(HeapRegion* r) {
4449     assert(r-&gt;in_collection_set(), &quot;Region %u must have been in collection set&quot;, r-&gt;hrm_index());
<span class="line-modified">4450     G1CollectedHeap::heap()-&gt;clear_in_cset(r);</span>
<span class="line-modified">4451     r-&gt;set_young_index_in_cset(-1);</span>
4452     return false;
4453   }
4454 };
4455 
4456 void G1CollectedHeap::abandon_collection_set(G1CollectionSet* collection_set) {
4457   G1AbandonCollectionSetClosure cl;
<span class="line-modified">4458   collection_set-&gt;iterate(&amp;cl);</span>
4459 
4460   collection_set-&gt;clear();
4461   collection_set-&gt;stop_incremental_building();
4462 }
4463 
4464 bool G1CollectedHeap::is_old_gc_alloc_region(HeapRegion* hr) {
4465   return _allocator-&gt;is_retained_old_region(hr);
4466 }
4467 
4468 void G1CollectedHeap::set_region_short_lived_locked(HeapRegion* hr) {
4469   _eden.add(hr);
4470   _policy-&gt;set_region_eden(hr);
4471 }
4472 
4473 #ifdef ASSERT
4474 
4475 class NoYoungRegionsClosure: public HeapRegionClosure {
4476 private:
4477   bool _success;
4478 public:
</pre>
<hr />
<pre>
4605   }
4606 };
4607 
4608 void G1CollectedHeap::rebuild_region_sets(bool free_list_only) {
4609   assert_at_safepoint_on_vm_thread();
4610 
4611   if (!free_list_only) {
4612     _eden.clear();
4613     _survivor.clear();
4614   }
4615 
4616   RebuildRegionSetsClosure cl(free_list_only, &amp;_old_set, _hrm);
4617   heap_region_iterate(&amp;cl);
4618 
4619   if (!free_list_only) {
4620     set_used(cl.total_used());
4621     if (_archive_allocator != NULL) {
4622       _archive_allocator-&gt;clear_used();
4623     }
4624   }
<span class="line-modified">4625   assert(used() == recalculate_used(),</span>
<span class="line-removed">4626          &quot;inconsistent used(), value: &quot; SIZE_FORMAT &quot; recalculated: &quot; SIZE_FORMAT,</span>
<span class="line-removed">4627          used(), recalculate_used());</span>
<span class="line-removed">4628 }</span>
<span class="line-removed">4629 </span>
<span class="line-removed">4630 bool G1CollectedHeap::is_in_closed_subset(const void* p) const {</span>
<span class="line-removed">4631   HeapRegion* hr = heap_region_containing(p);</span>
<span class="line-removed">4632   return hr-&gt;is_in(p);</span>
4633 }
4634 
4635 // Methods for the mutator alloc region
4636 
4637 HeapRegion* G1CollectedHeap::new_mutator_alloc_region(size_t word_size,
<span class="line-modified">4638                                                       bool force) {</span>

4639   assert_heap_locked_or_at_safepoint(true /* should_be_vm_thread */);
4640   bool should_allocate = policy()-&gt;should_allocate_mutator_region();
4641   if (force || should_allocate) {
4642     HeapRegion* new_alloc_region = new_region(word_size,
4643                                               HeapRegionType::Eden,
<span class="line-modified">4644                                               false /* do_expand */);</span>

4645     if (new_alloc_region != NULL) {
4646       set_region_short_lived_locked(new_alloc_region);
4647       _hr_printer.alloc(new_alloc_region, !should_allocate);
4648       _verifier-&gt;check_bitmaps(&quot;Mutator Region Allocation&quot;, new_alloc_region);
4649       _policy-&gt;remset_tracker()-&gt;update_at_allocate(new_alloc_region);
4650       return new_alloc_region;
4651     }
4652   }
4653   return NULL;
4654 }
4655 
4656 void G1CollectedHeap::retire_mutator_alloc_region(HeapRegion* alloc_region,
4657                                                   size_t allocated_bytes) {
4658   assert_heap_locked_or_at_safepoint(true /* should_be_vm_thread */);
4659   assert(alloc_region-&gt;is_eden(), &quot;all mutator alloc regions should be eden&quot;);
4660 
4661   collection_set()-&gt;add_eden_region(alloc_region);
4662   increase_used(allocated_bytes);

4663   _hr_printer.retire(alloc_region);

4664   // We update the eden sizes here, when the region is retired,
4665   // instead of when it&#39;s allocated, since this is the point that its
4666   // used space has been recorded in _summary_bytes_used.
4667   g1mm()-&gt;update_eden_size();
4668 }
4669 
4670 // Methods for the GC alloc regions
4671 
<span class="line-modified">4672 bool G1CollectedHeap::has_more_regions(InCSetState dest) {</span>
4673   if (dest.is_old()) {
4674     return true;
4675   } else {
4676     return survivor_regions_count() &lt; policy()-&gt;max_survivor_regions();
4677   }
4678 }
4679 
<span class="line-modified">4680 HeapRegion* G1CollectedHeap::new_gc_alloc_region(size_t word_size, InCSetState dest) {</span>
4681   assert(FreeList_lock-&gt;owned_by_self(), &quot;pre-condition&quot;);
4682 
4683   if (!has_more_regions(dest)) {
4684     return NULL;
4685   }
4686 
4687   HeapRegionType type;
4688   if (dest.is_young()) {
4689     type = HeapRegionType::Survivor;
4690   } else {
4691     type = HeapRegionType::Old;
4692   }
4693 
4694   HeapRegion* new_alloc_region = new_region(word_size,
4695                                             type,
<span class="line-modified">4696                                             true /* do_expand */);</span>

4697 
4698   if (new_alloc_region != NULL) {
4699     if (type.is_survivor()) {
4700       new_alloc_region-&gt;set_survivor();
4701       _survivor.add(new_alloc_region);
4702       _verifier-&gt;check_bitmaps(&quot;Survivor Region Allocation&quot;, new_alloc_region);
4703     } else {
4704       new_alloc_region-&gt;set_old();
4705       _verifier-&gt;check_bitmaps(&quot;Old Region Allocation&quot;, new_alloc_region);
4706     }
4707     _policy-&gt;remset_tracker()-&gt;update_at_allocate(new_alloc_region);

4708     _hr_printer.alloc(new_alloc_region);
4709     return new_alloc_region;
4710   }
4711   return NULL;
4712 }
4713 
4714 void G1CollectedHeap::retire_gc_alloc_region(HeapRegion* alloc_region,
4715                                              size_t allocated_bytes,
<span class="line-modified">4716                                              InCSetState dest) {</span>
<span class="line-modified">4717   policy()-&gt;record_bytes_copied_during_gc(allocated_bytes);</span>
4718   if (dest.is_old()) {
4719     old_set_add(alloc_region);



4720   }
4721 
4722   bool const during_im = collector_state()-&gt;in_initial_mark_gc();
4723   if (during_im &amp;&amp; allocated_bytes &gt; 0) {
<span class="line-modified">4724     _cm-&gt;root_regions()-&gt;add(alloc_region);</span>
4725   }
4726   _hr_printer.retire(alloc_region);
4727 }
4728 
4729 HeapRegion* G1CollectedHeap::alloc_highest_free_region() {
4730   bool expanded = false;
4731   uint index = _hrm-&gt;find_highest_free(&amp;expanded);
4732 
4733   if (index != G1_NO_HRM_INDEX) {
4734     if (expanded) {
4735       log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (requested address range outside heap bounds). region size: &quot; SIZE_FORMAT &quot;B&quot;,
4736                                 HeapRegion::GrainWords * HeapWordSize);
4737     }
4738     _hrm-&gt;allocate_free_regions_starting_at(index, 1);
4739     return region_at(index);
4740   }
4741   return NULL;
4742 }
4743 
4744 // Optimized nmethod scanning
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2001, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;classfile/classLoaderDataGraph.hpp&quot;
  27 #include &quot;classfile/metadataOnStackMark.hpp&quot;
  28 #include &quot;classfile/stringTable.hpp&quot;
  29 #include &quot;code/codeCache.hpp&quot;
  30 #include &quot;code/icBuffer.hpp&quot;
  31 #include &quot;gc/g1/g1Allocator.inline.hpp&quot;
<span class="line-added">  32 #include &quot;gc/g1/g1Arguments.hpp&quot;</span>
  33 #include &quot;gc/g1/g1BarrierSet.hpp&quot;
<span class="line-added">  34 #include &quot;gc/g1/g1CardTableEntryClosure.hpp&quot;</span>
  35 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
  36 #include &quot;gc/g1/g1CollectionSet.hpp&quot;

  37 #include &quot;gc/g1/g1CollectorState.hpp&quot;
  38 #include &quot;gc/g1/g1ConcurrentRefine.hpp&quot;
  39 #include &quot;gc/g1/g1ConcurrentRefineThread.hpp&quot;
  40 #include &quot;gc/g1/g1ConcurrentMarkThread.inline.hpp&quot;
  41 #include &quot;gc/g1/g1DirtyCardQueue.hpp&quot;
  42 #include &quot;gc/g1/g1EvacStats.inline.hpp&quot;
  43 #include &quot;gc/g1/g1FullCollector.hpp&quot;
  44 #include &quot;gc/g1/g1GCPhaseTimes.hpp&quot;
  45 #include &quot;gc/g1/g1HeapSizingPolicy.hpp&quot;
  46 #include &quot;gc/g1/g1HeapTransition.hpp&quot;
  47 #include &quot;gc/g1/g1HeapVerifier.hpp&quot;
  48 #include &quot;gc/g1/g1HotCardCache.hpp&quot;
  49 #include &quot;gc/g1/g1MemoryPool.hpp&quot;
  50 #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
<span class="line-added">  51 #include &quot;gc/g1/g1ParallelCleaning.hpp&quot;</span>
  52 #include &quot;gc/g1/g1ParScanThreadState.inline.hpp&quot;
  53 #include &quot;gc/g1/g1Policy.hpp&quot;
<span class="line-added">  54 #include &quot;gc/g1/g1RedirtyCardsQueue.hpp&quot;</span>
  55 #include &quot;gc/g1/g1RegionToSpaceMapper.hpp&quot;
  56 #include &quot;gc/g1/g1RemSet.hpp&quot;
  57 #include &quot;gc/g1/g1RootClosures.hpp&quot;
  58 #include &quot;gc/g1/g1RootProcessor.hpp&quot;
  59 #include &quot;gc/g1/g1SATBMarkQueueSet.hpp&quot;
  60 #include &quot;gc/g1/g1StringDedup.hpp&quot;
  61 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
<span class="line-added">  62 #include &quot;gc/g1/g1Trace.hpp&quot;</span>
  63 #include &quot;gc/g1/g1YCTypes.hpp&quot;
  64 #include &quot;gc/g1/g1YoungRemSetSamplingThread.hpp&quot;
  65 #include &quot;gc/g1/g1VMOperations.hpp&quot;
  66 #include &quot;gc/g1/heapRegion.inline.hpp&quot;
  67 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
  68 #include &quot;gc/g1/heapRegionSet.inline.hpp&quot;
  69 #include &quot;gc/shared/gcBehaviours.hpp&quot;
  70 #include &quot;gc/shared/gcHeapSummary.hpp&quot;
  71 #include &quot;gc/shared/gcId.hpp&quot;
  72 #include &quot;gc/shared/gcLocker.hpp&quot;
  73 #include &quot;gc/shared/gcTimer.hpp&quot;

  74 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  75 #include &quot;gc/shared/generationSpec.hpp&quot;
  76 #include &quot;gc/shared/isGCActiveMark.hpp&quot;
<span class="line-added">  77 #include &quot;gc/shared/locationPrinter.inline.hpp&quot;</span>
  78 #include &quot;gc/shared/oopStorageParState.hpp&quot;

  79 #include &quot;gc/shared/preservedMarks.inline.hpp&quot;
  80 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
  81 #include &quot;gc/shared/referenceProcessor.inline.hpp&quot;
<span class="line-added">  82 #include &quot;gc/shared/taskTerminator.hpp&quot;</span>
  83 #include &quot;gc/shared/taskqueue.inline.hpp&quot;
  84 #include &quot;gc/shared/weakProcessor.inline.hpp&quot;
  85 #include &quot;gc/shared/workerPolicy.hpp&quot;
  86 #include &quot;logging/log.hpp&quot;
  87 #include &quot;memory/allocation.hpp&quot;
  88 #include &quot;memory/iterator.hpp&quot;
  89 #include &quot;memory/resourceArea.hpp&quot;
<span class="line-added">  90 #include &quot;memory/universe.hpp&quot;</span>
  91 #include &quot;oops/access.inline.hpp&quot;
  92 #include &quot;oops/compressedOops.inline.hpp&quot;
  93 #include &quot;oops/oop.inline.hpp&quot;
  94 #include &quot;runtime/atomic.hpp&quot;
  95 #include &quot;runtime/flags/flagSetting.hpp&quot;
  96 #include &quot;runtime/handles.inline.hpp&quot;
  97 #include &quot;runtime/init.hpp&quot;
  98 #include &quot;runtime/orderAccess.hpp&quot;
  99 #include &quot;runtime/threadSMR.hpp&quot;
 100 #include &quot;runtime/vmThread.hpp&quot;
 101 #include &quot;utilities/align.hpp&quot;
<span class="line-added"> 102 #include &quot;utilities/bitMap.inline.hpp&quot;</span>
 103 #include &quot;utilities/globalDefinitions.hpp&quot;
 104 #include &quot;utilities/stack.inline.hpp&quot;
 105 
 106 size_t G1CollectedHeap::_humongous_object_threshold_in_words = 0;
 107 
 108 // INVARIANTS/NOTES
 109 //
 110 // All allocation activity covered by the G1CollectedHeap interface is
 111 // serialized by acquiring the HeapLock.  This happens in mem_allocate
 112 // and allocate_new_tlab, which are the &quot;entry&quot; points to the
 113 // allocation code from the rest of the JVM.  (Note that this does not
 114 // apply to TLAB allocation, which is not part of this interface: it
 115 // is done by clients of this interface.)
 116 
 117 class RedirtyLoggedCardTableEntryClosure : public G1CardTableEntryClosure {
 118  private:
 119   size_t _num_dirtied;
 120   G1CollectedHeap* _g1h;
 121   G1CardTable* _g1_ct;
 122 
 123   HeapRegion* region_for_card(CardValue* card_ptr) const {
 124     return _g1h-&gt;heap_region_containing(_g1_ct-&gt;addr_for(card_ptr));
 125   }
 126 
 127   bool will_become_free(HeapRegion* hr) const {
 128     // A region will be freed by free_collection_set if the region is in the
 129     // collection set and has not had an evacuation failure.
 130     return _g1h-&gt;is_in_cset(hr) &amp;&amp; !hr-&gt;evacuation_failed();
 131   }
 132 
 133  public:
 134   RedirtyLoggedCardTableEntryClosure(G1CollectedHeap* g1h) : G1CardTableEntryClosure(),
 135     _num_dirtied(0), _g1h(g1h), _g1_ct(g1h-&gt;card_table()) { }
 136 
<span class="line-modified"> 137   void do_card_ptr(CardValue* card_ptr, uint worker_id) {</span>
 138     HeapRegion* hr = region_for_card(card_ptr);
 139 
 140     // Should only dirty cards in regions that won&#39;t be freed.
 141     if (!will_become_free(hr)) {
 142       *card_ptr = G1CardTable::dirty_card_val();
 143       _num_dirtied++;
 144     }


 145   }
 146 
 147   size_t num_dirtied()   const { return _num_dirtied; }
 148 };
 149 
 150 
 151 void G1RegionMappingChangedListener::reset_from_card_cache(uint start_idx, size_t num_regions) {
 152   HeapRegionRemSet::invalidate_from_card_cache(start_idx, num_regions);
 153 }
 154 
 155 void G1RegionMappingChangedListener::on_commit(uint start_idx, size_t num_regions, bool zero_filled) {
 156   // The from card cache is not the memory that is actually committed. So we cannot
 157   // take advantage of the zero_filled parameter.
 158   reset_from_card_cache(start_idx, num_regions);
 159 }
 160 
<span class="line-added"> 161 Tickspan G1CollectedHeap::run_task(AbstractGangTask* task) {</span>
<span class="line-added"> 162   Ticks start = Ticks::now();</span>
<span class="line-added"> 163   workers()-&gt;run_task(task, workers()-&gt;active_workers());</span>
<span class="line-added"> 164   return Ticks::now() - start;</span>
<span class="line-added"> 165 }</span>
 166 
 167 HeapRegion* G1CollectedHeap::new_heap_region(uint hrs_index,
 168                                              MemRegion mr) {
 169   return new HeapRegion(hrs_index, bot(), mr);
 170 }
 171 
 172 // Private methods.
 173 
<span class="line-modified"> 174 HeapRegion* G1CollectedHeap::new_region(size_t word_size,</span>
<span class="line-added"> 175                                         HeapRegionType type,</span>
<span class="line-added"> 176                                         bool do_expand,</span>
<span class="line-added"> 177                                         uint node_index) {</span>
 178   assert(!is_humongous(word_size) || word_size &lt;= HeapRegion::GrainWords,
 179          &quot;the only time we use this to allocate a humongous region is &quot;
 180          &quot;when we are allocating a single humongous region&quot;);
 181 
<span class="line-modified"> 182   HeapRegion* res = _hrm-&gt;allocate_free_region(type, node_index);</span>
 183 
 184   if (res == NULL &amp;&amp; do_expand &amp;&amp; _expand_heap_after_alloc_failure) {
 185     // Currently, only attempts to allocate GC alloc regions set
 186     // do_expand to true. So, we should only reach here during a
 187     // safepoint. If this assumption changes we might have to
 188     // reconsider the use of _expand_heap_after_alloc_failure.
 189     assert(SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
 190 
 191     log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (region allocation request failed). Allocation request: &quot; SIZE_FORMAT &quot;B&quot;,
 192                               word_size * HeapWordSize);
 193 
<span class="line-modified"> 194     assert(word_size * HeapWordSize &lt; HeapRegion::GrainBytes,</span>
<span class="line-modified"> 195            &quot;This kind of expansion should never be more than one region. Size: &quot; SIZE_FORMAT,</span>
<span class="line-added"> 196            word_size * HeapWordSize);</span>
<span class="line-added"> 197     if (expand_single_region(node_index)) {</span>
<span class="line-added"> 198       // Given that expand_single_region() succeeded in expanding the heap, and we</span>
 199       // always expand the heap by an amount aligned to the heap
 200       // region size, the free list should in theory not be empty.
 201       // In either case allocate_free_region() will check for NULL.
<span class="line-modified"> 202       res = _hrm-&gt;allocate_free_region(type, node_index);</span>
 203     } else {
 204       _expand_heap_after_alloc_failure = false;
 205     }
 206   }
 207   return res;
 208 }
 209 
 210 HeapWord*
 211 G1CollectedHeap::humongous_obj_allocate_initialize_regions(uint first,
 212                                                            uint num_regions,
 213                                                            size_t word_size) {
 214   assert(first != G1_NO_HRM_INDEX, &quot;pre-condition&quot;);
 215   assert(is_humongous(word_size), &quot;word_size should be humongous&quot;);
 216   assert(num_regions * HeapRegion::GrainWords &gt;= word_size, &quot;pre-condition&quot;);
 217 
 218   // Index of last region in the series.
 219   uint last = first + num_regions - 1;
 220 
 221   // We need to initialize the region(s) we just discovered. This is
 222   // a bit tricky given that it can happen concurrently with
</pre>
<hr />
<pre>
 429 
 430   // Make sure you read the note in attempt_allocation_humongous().
 431 
 432   assert_heap_not_locked_and_not_at_safepoint();
 433   assert(!is_humongous(word_size), &quot;attempt_allocation_slow() should not &quot;
 434          &quot;be called for humongous allocation requests&quot;);
 435 
 436   // We should only get here after the first-level allocation attempt
 437   // (attempt_allocation()) failed to allocate.
 438 
 439   // We will loop until a) we manage to successfully perform the
 440   // allocation or b) we successfully schedule a collection which
 441   // fails to perform the allocation. b) is the only case when we&#39;ll
 442   // return NULL.
 443   HeapWord* result = NULL;
 444   for (uint try_count = 1, gclocker_retry_count = 0; /* we&#39;ll return */; try_count += 1) {
 445     bool should_try_gc;
 446     uint gc_count_before;
 447 
 448     {
<span class="line-modified"> 449       MutexLocker x(Heap_lock);</span>
 450       result = _allocator-&gt;attempt_allocation_locked(word_size);
 451       if (result != NULL) {
 452         return result;
 453       }
 454 
 455       // If the GCLocker is active and we are bound for a GC, try expanding young gen.
 456       // This is different to when only GCLocker::needs_gc() is set: try to avoid
 457       // waiting because the GCLocker is active to not wait too long.
 458       if (GCLocker::is_active_and_needs_gc() &amp;&amp; policy()-&gt;can_expand_young_list()) {
 459         // No need for an ergo message here, can_expand_young_list() does this when
 460         // it returns true.
 461         result = _allocator-&gt;attempt_allocation_force(word_size);
 462         if (result != NULL) {
 463           return result;
 464         }
 465       }
 466       // Only try a GC if the GCLocker does not signal the need for a GC. Wait until
 467       // the GCLocker initiated GC has been performed and then retry. This includes
 468       // the case when the GC Locker is not active but has not been performed.
 469       should_try_gc = !GCLocker::needs_gc();
</pre>
<hr />
<pre>
 568 }
 569 
 570 bool G1CollectedHeap::check_archive_addresses(MemRegion* ranges, size_t count) {
 571   assert(ranges != NULL, &quot;MemRegion array NULL&quot;);
 572   assert(count != 0, &quot;No MemRegions provided&quot;);
 573   MemRegion reserved = _hrm-&gt;reserved();
 574   for (size_t i = 0; i &lt; count; i++) {
 575     if (!reserved.contains(ranges[i].start()) || !reserved.contains(ranges[i].last())) {
 576       return false;
 577     }
 578   }
 579   return true;
 580 }
 581 
 582 bool G1CollectedHeap::alloc_archive_regions(MemRegion* ranges,
 583                                             size_t count,
 584                                             bool open) {
 585   assert(!is_init_completed(), &quot;Expect to be called at JVM init time&quot;);
 586   assert(ranges != NULL, &quot;MemRegion array NULL&quot;);
 587   assert(count != 0, &quot;No MemRegions provided&quot;);
<span class="line-modified"> 588   MutexLocker x(Heap_lock);</span>
 589 
 590   MemRegion reserved = _hrm-&gt;reserved();
 591   HeapWord* prev_last_addr = NULL;
 592   HeapRegion* prev_last_region = NULL;
 593 
 594   // Temporarily disable pretouching of heap pages. This interface is used
 595   // when mmap&#39;ing archived heap data in, so pre-touching is wasted.
 596   FlagSetting fs(AlwaysPreTouch, false);
 597 
 598   // Enable archive object checking used by G1MarkSweep. We have to let it know
 599   // about each archive range, so that objects in those ranges aren&#39;t marked.
 600   G1ArchiveAllocator::enable_archive_object_check();
 601 
 602   // For each specified MemRegion range, allocate the corresponding G1
 603   // regions and mark them as archive regions. We expect the ranges
 604   // in ascending starting address order, without overlap.
 605   for (size_t i = 0; i &lt; count; i++) {
 606     MemRegion curr_range = ranges[i];
 607     HeapWord* start_address = curr_range.start();
 608     size_t word_size = curr_range.word_size();
</pre>
<hr />
<pre>
 655     while (curr_region != NULL) {
 656       assert(curr_region-&gt;is_empty() &amp;&amp; !curr_region-&gt;is_pinned(),
 657              &quot;Region already in use (index %u)&quot;, curr_region-&gt;hrm_index());
 658       if (open) {
 659         curr_region-&gt;set_open_archive();
 660       } else {
 661         curr_region-&gt;set_closed_archive();
 662       }
 663       _hr_printer.alloc(curr_region);
 664       _archive_set.add(curr_region);
 665       HeapWord* top;
 666       HeapRegion* next_region;
 667       if (curr_region != last_region) {
 668         top = curr_region-&gt;end();
 669         next_region = _hrm-&gt;next_region_in_heap(curr_region);
 670       } else {
 671         top = last_address + 1;
 672         next_region = NULL;
 673       }
 674       curr_region-&gt;set_top(top);


 675       curr_region = next_region;
 676     }
 677 
 678     // Notify mark-sweep of the archive
 679     G1ArchiveAllocator::set_range_archive(curr_range, open);
 680   }
 681   return true;
 682 }
 683 
 684 void G1CollectedHeap::fill_archive_regions(MemRegion* ranges, size_t count) {
 685   assert(!is_init_completed(), &quot;Expect to be called at JVM init time&quot;);
 686   assert(ranges != NULL, &quot;MemRegion array NULL&quot;);
 687   assert(count != 0, &quot;No MemRegions provided&quot;);
 688   MemRegion reserved = _hrm-&gt;reserved();
 689   HeapWord *prev_last_addr = NULL;
 690   HeapRegion* prev_last_region = NULL;
 691 
 692   // For each MemRegion, create filler objects, if needed, in the G1 regions
 693   // that contain the address range. The address range actually within the
 694   // MemRegion will not be modified. That is assumed to have been initialized
 695   // elsewhere, probably via an mmap of archived heap data.
<span class="line-modified"> 696   MutexLocker x(Heap_lock);</span>
 697   for (size_t i = 0; i &lt; count; i++) {
 698     HeapWord* start_address = ranges[i].start();
 699     HeapWord* last_address = ranges[i].last();
 700 
 701     assert(reserved.contains(start_address) &amp;&amp; reserved.contains(last_address),
 702            &quot;MemRegion outside of heap [&quot; PTR_FORMAT &quot;, &quot; PTR_FORMAT &quot;]&quot;,
 703            p2i(start_address), p2i(last_address));
 704     assert(start_address &gt; prev_last_addr,
 705            &quot;Ranges not in ascending order: &quot; PTR_FORMAT &quot; &lt;= &quot; PTR_FORMAT ,
 706            p2i(start_address), p2i(prev_last_addr));
 707 
 708     HeapRegion* start_region = _hrm-&gt;addr_to_region(start_address);
 709     HeapRegion* last_region = _hrm-&gt;addr_to_region(last_address);
 710     HeapWord* bottom_address = start_region-&gt;bottom();
 711 
 712     // Check for a range beginning in the same region in which the
 713     // previous one ended.
 714     if (start_region == prev_last_region) {
 715       bottom_address = prev_last_addr + 1;
 716     }
</pre>
<hr />
<pre>
 750          &quot;be called for humongous allocation requests&quot;);
 751 
 752   HeapWord* result = _allocator-&gt;attempt_allocation(min_word_size, desired_word_size, actual_word_size);
 753 
 754   if (result == NULL) {
 755     *actual_word_size = desired_word_size;
 756     result = attempt_allocation_slow(desired_word_size);
 757   }
 758 
 759   assert_heap_not_locked();
 760   if (result != NULL) {
 761     assert(*actual_word_size != 0, &quot;Actual size must have been set here&quot;);
 762     dirty_young_block(result, *actual_word_size);
 763   } else {
 764     *actual_word_size = 0;
 765   }
 766 
 767   return result;
 768 }
 769 
<span class="line-modified"> 770 void G1CollectedHeap::dealloc_archive_regions(MemRegion* ranges, size_t count) {</span>
 771   assert(!is_init_completed(), &quot;Expect to be called at JVM init time&quot;);
 772   assert(ranges != NULL, &quot;MemRegion array NULL&quot;);
 773   assert(count != 0, &quot;No MemRegions provided&quot;);
 774   MemRegion reserved = _hrm-&gt;reserved();
 775   HeapWord* prev_last_addr = NULL;
 776   HeapRegion* prev_last_region = NULL;
 777   size_t size_used = 0;
 778   size_t uncommitted_regions = 0;
 779 
 780   // For each Memregion, free the G1 regions that constitute it, and
 781   // notify mark-sweep that the range is no longer to be considered &#39;archive.&#39;
<span class="line-modified"> 782   MutexLocker x(Heap_lock);</span>
 783   for (size_t i = 0; i &lt; count; i++) {
 784     HeapWord* start_address = ranges[i].start();
 785     HeapWord* last_address = ranges[i].last();
 786 
 787     assert(reserved.contains(start_address) &amp;&amp; reserved.contains(last_address),
 788            &quot;MemRegion outside of heap [&quot; PTR_FORMAT &quot;, &quot; PTR_FORMAT &quot;]&quot;,
 789            p2i(start_address), p2i(last_address));
 790     assert(start_address &gt; prev_last_addr,
 791            &quot;Ranges not in ascending order: &quot; PTR_FORMAT &quot; &lt;= &quot; PTR_FORMAT ,
 792            p2i(start_address), p2i(prev_last_addr));
 793     size_used += ranges[i].byte_size();
 794     prev_last_addr = last_address;
 795 
 796     HeapRegion* start_region = _hrm-&gt;addr_to_region(start_address);
 797     HeapRegion* last_region = _hrm-&gt;addr_to_region(last_address);
 798 
 799     // Check for ranges that start in the same G1 region in which the previous
 800     // range ended, and adjust the start address so we don&#39;t try to free
 801     // the same region again. If the current range is entirely within that
 802     // region, skip it.
</pre>
<hr />
<pre>
 812     // After verifying that each region was marked as an archive region by
 813     // alloc_archive_regions, set it free and empty and uncommit it.
 814     HeapRegion* curr_region = start_region;
 815     while (curr_region != NULL) {
 816       guarantee(curr_region-&gt;is_archive(),
 817                 &quot;Expected archive region at index %u&quot;, curr_region-&gt;hrm_index());
 818       uint curr_index = curr_region-&gt;hrm_index();
 819       _archive_set.remove(curr_region);
 820       curr_region-&gt;set_free();
 821       curr_region-&gt;set_top(curr_region-&gt;bottom());
 822       if (curr_region != last_region) {
 823         curr_region = _hrm-&gt;next_region_in_heap(curr_region);
 824       } else {
 825         curr_region = NULL;
 826       }
 827       _hrm-&gt;shrink_at(curr_index, 1);
 828       uncommitted_regions++;
 829     }
 830 
 831     // Notify mark-sweep that this is no longer an archive range.
<span class="line-modified"> 832     G1ArchiveAllocator::clear_range_archive(ranges[i]);</span>
 833   }
 834 
 835   if (uncommitted_regions != 0) {
 836     log_debug(gc, ergo, heap)(&quot;Attempt heap shrinking (uncommitted archive regions). Total size: &quot; SIZE_FORMAT &quot;B&quot;,
 837                               HeapRegion::GrainWords * HeapWordSize * uncommitted_regions);
 838   }
 839   decrease_used(size_used);
 840 }
 841 
 842 oop G1CollectedHeap::materialize_archived_object(oop obj) {
 843   assert(obj != NULL, &quot;archived obj is NULL&quot;);
 844   assert(G1ArchiveAllocator::is_archived_object(obj), &quot;must be archived object&quot;);
 845 
 846   // Loading an archived object makes it strongly reachable. If it is
 847   // loaded during concurrent marking, it must be enqueued to the SATB
 848   // queue, shading the previously white object gray.
 849   G1BarrierSet::enqueue(obj);
 850 
 851   return obj;
 852 }
</pre>
<hr />
<pre>
 873   // need to start a marking cycle at each humongous object allocation. We do
 874   // the check before we do the actual allocation. The reason for doing it
 875   // before the allocation is that we avoid having to keep track of the newly
 876   // allocated memory while we do a GC.
 877   if (policy()-&gt;need_to_start_conc_mark(&quot;concurrent humongous allocation&quot;,
 878                                            word_size)) {
 879     collect(GCCause::_g1_humongous_allocation);
 880   }
 881 
 882   // We will loop until a) we manage to successfully perform the
 883   // allocation or b) we successfully schedule a collection which
 884   // fails to perform the allocation. b) is the only case when we&#39;ll
 885   // return NULL.
 886   HeapWord* result = NULL;
 887   for (uint try_count = 1, gclocker_retry_count = 0; /* we&#39;ll return */; try_count += 1) {
 888     bool should_try_gc;
 889     uint gc_count_before;
 890 
 891 
 892     {
<span class="line-modified"> 893       MutexLocker x(Heap_lock);</span>
 894 
 895       // Given that humongous objects are not allocated in young
 896       // regions, we&#39;ll first try to do the allocation without doing a
 897       // collection hoping that there&#39;s enough space in the heap.
 898       result = humongous_obj_allocate(word_size);
 899       if (result != NULL) {
 900         size_t size_in_regions = humongous_obj_size_in_regions(word_size);
 901         policy()-&gt;add_bytes_allocated_in_old_since_last_gc(size_in_regions * HeapRegion::GrainBytes);
 902         return result;
 903       }
 904 
 905       // Only try a GC if the GCLocker does not signal the need for a GC. Wait until
 906       // the GCLocker initiated GC has been performed and then retry. This includes
 907       // the case when the GC Locker is not active but has not been performed.
 908       should_try_gc = !GCLocker::needs_gc();
 909       // Read the GC count while still holding the Heap_lock.
 910       gc_count_before = total_collections();
 911     }
 912 
 913     if (should_try_gc) {
</pre>
<hr />
<pre>
1011   // scanning the root regions we might trip them over as we&#39;ll
1012   // be moving objects / updating references. So let&#39;s wait until
1013   // they are done. By telling them to abort, they should complete
1014   // early.
1015   _cm-&gt;root_regions()-&gt;abort();
1016   _cm-&gt;root_regions()-&gt;wait_until_scan_finished();
1017 
1018   // Disable discovery and empty the discovered lists
1019   // for the CM ref processor.
1020   _ref_processor_cm-&gt;disable_discovery();
1021   _ref_processor_cm-&gt;abandon_partial_discovery();
1022   _ref_processor_cm-&gt;verify_no_references_recorded();
1023 
1024   // Abandon current iterations of concurrent marking and concurrent
1025   // refinement, if any are in progress.
1026   concurrent_mark()-&gt;concurrent_cycle_abort();
1027 }
1028 
1029 void G1CollectedHeap::prepare_heap_for_full_collection() {
1030   // Make sure we&#39;ll choose a new allocation region afterwards.
<span class="line-modified">1031   _allocator-&gt;release_mutator_alloc_regions();</span>
1032   _allocator-&gt;abandon_gc_alloc_regions();
1033 
1034   // We may have added regions to the current incremental collection
1035   // set between the last GC or pause and now. We need to clear the
1036   // incremental collection set and then start rebuilding it afresh
1037   // after this full GC.
1038   abandon_collection_set(collection_set());
1039 
1040   tear_down_region_sets(false /* free_list_only */);
1041 
1042   hrm()-&gt;prepare_for_full_collection_start();
1043 }
1044 
1045 void G1CollectedHeap::verify_before_full_collection(bool explicit_gc) {
1046   assert(!GCCause::is_user_requested_gc(gc_cause()) || explicit_gc, &quot;invariant&quot;);
<span class="line-modified">1047   assert_used_and_recalculate_used_equal(this);</span>
1048   _verifier-&gt;verify_region_sets_optional();
1049   _verifier-&gt;verify_before_gc(G1HeapVerifier::G1VerifyFull);
1050   _verifier-&gt;check_bitmaps(&quot;Full GC Start&quot;);
1051 }
1052 
1053 void G1CollectedHeap::prepare_heap_for_mutators() {
1054   hrm()-&gt;prepare_for_full_collection_end();
1055 
1056   // Delete metaspaces for unloaded class loaders and clean up loader_data graph
1057   ClassLoaderDataGraph::purge();
1058   MetaspaceUtils::verify_metrics();
1059 
1060   // Prepare heap for normal collections.
1061   assert(num_free_regions() == 0, &quot;we should not have added any free regions&quot;);
1062   rebuild_region_sets(false /* free_list_only */);
1063   abort_refinement();
1064   resize_heap_if_necessary();
1065 
1066   // Rebuild the strong code root lists for each region
1067   rebuild_strong_code_roots();
1068 
1069   // Purge code root memory
1070   purge_code_root_memory();
1071 
1072   // Start a new incremental collection set for the next pause
1073   start_new_collection_set();
1074 
<span class="line-modified">1075   _allocator-&gt;init_mutator_alloc_regions();</span>
1076 
1077   // Post collection state updates.
1078   MetaspaceGC::compute_new_size();
1079 }
1080 
1081 void G1CollectedHeap::abort_refinement() {
1082   if (_hot_card_cache-&gt;use_cache()) {
1083     _hot_card_cache-&gt;reset_hot_cache();
1084   }
1085 
1086   // Discard all remembered set updates.
1087   G1BarrierSet::dirty_card_queue_set().abandon_logs();
<span class="line-modified">1088   assert(G1BarrierSet::dirty_card_queue_set().num_cards() == 0,</span>
<span class="line-added">1089          &quot;DCQS should be empty&quot;);</span>
1090 }
1091 
1092 void G1CollectedHeap::verify_after_full_collection() {
1093   _hrm-&gt;verify_optional();
1094   _verifier-&gt;verify_region_sets_optional();
1095   _verifier-&gt;verify_after_gc(G1HeapVerifier::G1VerifyFull);
1096   // Clear the previous marking bitmap, if needed for bitmap verification.
1097   // Note we cannot do this when we clear the next marking bitmap in
1098   // G1ConcurrentMark::abort() above since VerifyDuringGC verifies the
1099   // objects marked during a full GC against the previous bitmap.
1100   // But we need to clear it before calling check_bitmaps below since
1101   // the full GC has compacted objects and updated TAMS but not updated
1102   // the prev bitmap.
1103   if (G1VerifyBitmaps) {
<span class="line-modified">1104     GCTraceTime(Debug, gc) tm(&quot;Clear Prev Bitmap for Verification&quot;);</span>
1105     _cm-&gt;clear_prev_bitmap(workers());
1106   }
1107   // This call implicitly verifies that the next bitmap is clear after Full GC.
1108   _verifier-&gt;check_bitmaps(&quot;Full GC End&quot;);
1109 
1110   // At this point there should be no regions in the
1111   // entire heap tagged as young.
1112   assert(check_young_list_empty(), &quot;young list should be empty at this point&quot;);
1113 
1114   // Note: since we&#39;ve just done a full GC, concurrent
1115   // marking is no longer active. Therefore we need not
1116   // re-enable reference discovery for the CM ref processor.
1117   // That will be done at the start of the next marking cycle.
1118   // We also know that the STW processor should no longer
1119   // discover any new references.
1120   assert(!_ref_processor_stw-&gt;discovery_enabled(), &quot;Postcondition&quot;);
1121   assert(!_ref_processor_cm-&gt;discovery_enabled(), &quot;Postcondition&quot;);
1122   _ref_processor_stw-&gt;verify_no_references_recorded();
1123   _ref_processor_cm-&gt;verify_no_references_recorded();
1124 }
1125 
1126 void G1CollectedHeap::print_heap_after_full_collection(G1HeapTransition* heap_transition) {
1127   // Post collection logging.
1128   // We should do this after we potentially resize the heap so
1129   // that all the COMMIT / UNCOMMIT events are generated before
1130   // the compaction events.
1131   print_hrm_post_compaction();
1132   heap_transition-&gt;print();
1133   print_heap_after_gc();
1134   print_heap_regions();



1135 }
1136 
1137 bool G1CollectedHeap::do_full_collection(bool explicit_gc,
1138                                          bool clear_all_soft_refs) {
1139   assert_at_safepoint_on_vm_thread();
1140 
1141   if (GCLocker::check_active_before_gc()) {
1142     // Full GC was not completed.
1143     return false;
1144   }
1145 
1146   const bool do_clear_all_soft_refs = clear_all_soft_refs ||
1147       soft_ref_policy()-&gt;should_clear_all_soft_refs();
1148 
1149   G1FullCollector collector(this, explicit_gc, do_clear_all_soft_refs);
1150   GCTraceTime(Info, gc) tm(&quot;Pause Full&quot;, NULL, gc_cause(), true);
1151 
1152   collector.prepare_collection();
1153   collector.collect();
1154   collector.complete_collection();
</pre>
<hr />
<pre>
1166 }
1167 
1168 void G1CollectedHeap::resize_heap_if_necessary() {
1169   assert_at_safepoint_on_vm_thread();
1170 
1171   // Capacity, free and used after the GC counted as full regions to
1172   // include the waste in the following calculations.
1173   const size_t capacity_after_gc = capacity();
1174   const size_t used_after_gc = capacity_after_gc - unused_committed_regions_in_bytes();
1175 
1176   // This is enforced in arguments.cpp.
1177   assert(MinHeapFreeRatio &lt;= MaxHeapFreeRatio,
1178          &quot;otherwise the code below doesn&#39;t make sense&quot;);
1179 
1180   // We don&#39;t have floating point command-line arguments
1181   const double minimum_free_percentage = (double) MinHeapFreeRatio / 100.0;
1182   const double maximum_used_percentage = 1.0 - minimum_free_percentage;
1183   const double maximum_free_percentage = (double) MaxHeapFreeRatio / 100.0;
1184   const double minimum_used_percentage = 1.0 - maximum_free_percentage;
1185 



1186   // We have to be careful here as these two calculations can overflow
1187   // 32-bit size_t&#39;s.
1188   double used_after_gc_d = (double) used_after_gc;
1189   double minimum_desired_capacity_d = used_after_gc_d / maximum_used_percentage;
1190   double maximum_desired_capacity_d = used_after_gc_d / minimum_used_percentage;
1191 
1192   // Let&#39;s make sure that they are both under the max heap size, which
1193   // by default will make them fit into a size_t.
<span class="line-modified">1194   double desired_capacity_upper_bound = (double) MaxHeapSize;</span>
1195   minimum_desired_capacity_d = MIN2(minimum_desired_capacity_d,
1196                                     desired_capacity_upper_bound);
1197   maximum_desired_capacity_d = MIN2(maximum_desired_capacity_d,
1198                                     desired_capacity_upper_bound);
1199 
1200   // We can now safely turn them into size_t&#39;s.
1201   size_t minimum_desired_capacity = (size_t) minimum_desired_capacity_d;
1202   size_t maximum_desired_capacity = (size_t) maximum_desired_capacity_d;
1203 
1204   // This assert only makes sense here, before we adjust them
1205   // with respect to the min and max heap size.
1206   assert(minimum_desired_capacity &lt;= maximum_desired_capacity,
1207          &quot;minimum_desired_capacity = &quot; SIZE_FORMAT &quot;, &quot;
1208          &quot;maximum_desired_capacity = &quot; SIZE_FORMAT,
1209          minimum_desired_capacity, maximum_desired_capacity);
1210 
1211   // Should not be greater than the heap max size. No need to adjust
1212   // it with respect to the heap min size as it&#39;s a lower bound (i.e.,
1213   // we&#39;ll try to make the capacity larger than it, not smaller).
<span class="line-modified">1214   minimum_desired_capacity = MIN2(minimum_desired_capacity, MaxHeapSize);</span>
1215   // Should not be less than the heap min size. No need to adjust it
1216   // with respect to the heap max size as it&#39;s an upper bound (i.e.,
1217   // we&#39;ll try to make the capacity smaller than it, not greater).
<span class="line-modified">1218   maximum_desired_capacity =  MAX2(maximum_desired_capacity, MinHeapSize);</span>
1219 
1220   if (capacity_after_gc &lt; minimum_desired_capacity) {
1221     // Don&#39;t expand unless it&#39;s significant
1222     size_t expand_bytes = minimum_desired_capacity - capacity_after_gc;
1223 
1224     log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (capacity lower than min desired capacity). &quot;
1225                               &quot;Capacity: &quot; SIZE_FORMAT &quot;B occupancy: &quot; SIZE_FORMAT &quot;B live: &quot; SIZE_FORMAT &quot;B &quot;
1226                               &quot;min_desired_capacity: &quot; SIZE_FORMAT &quot;B (&quot; UINTX_FORMAT &quot; %%)&quot;,
1227                               capacity_after_gc, used_after_gc, used(), minimum_desired_capacity, MinHeapFreeRatio);
1228 
1229     expand(expand_bytes, _workers);
1230 
1231     // No expansion, now see if we want to shrink
1232   } else if (capacity_after_gc &gt; maximum_desired_capacity) {
1233     // Capacity too large, compute shrinking size
1234     size_t shrink_bytes = capacity_after_gc - maximum_desired_capacity;
1235 
1236     log_debug(gc, ergo, heap)(&quot;Attempt heap shrinking (capacity higher than max desired capacity). &quot;
1237                               &quot;Capacity: &quot; SIZE_FORMAT &quot;B occupancy: &quot; SIZE_FORMAT &quot;B live: &quot; SIZE_FORMAT &quot;B &quot;
1238                               &quot;maximum_desired_capacity: &quot; SIZE_FORMAT &quot;B (&quot; UINTX_FORMAT &quot; %%)&quot;,
</pre>
<hr />
<pre>
1369   }
1370 
1371   if (expanded_by &gt; 0) {
1372     size_t actual_expand_bytes = expanded_by * HeapRegion::GrainBytes;
1373     assert(actual_expand_bytes &lt;= aligned_expand_bytes, &quot;post-condition&quot;);
1374     policy()-&gt;record_new_heap_size(num_regions());
1375   } else {
1376     log_debug(gc, ergo, heap)(&quot;Did not expand the heap (heap expansion operation failed)&quot;);
1377 
1378     // The expansion of the virtual storage space was unsuccessful.
1379     // Let&#39;s see if it was because we ran out of swap.
1380     if (G1ExitOnExpansionFailure &amp;&amp;
1381         _hrm-&gt;available() &gt;= regions_to_expand) {
1382       // We had head room...
1383       vm_exit_out_of_memory(aligned_expand_bytes, OOM_MMAP_ERROR, &quot;G1 heap expansion&quot;);
1384     }
1385   }
1386   return regions_to_expand &gt; 0;
1387 }
1388 
<span class="line-added">1389 bool G1CollectedHeap::expand_single_region(uint node_index) {</span>
<span class="line-added">1390   uint expanded_by = _hrm-&gt;expand_on_preferred_node(node_index);</span>
<span class="line-added">1391 </span>
<span class="line-added">1392   if (expanded_by == 0) {</span>
<span class="line-added">1393     assert(is_maximal_no_gc(), &quot;Should be no regions left, available: %u&quot;, _hrm-&gt;available());</span>
<span class="line-added">1394     log_debug(gc, ergo, heap)(&quot;Did not expand the heap (heap already fully expanded)&quot;);</span>
<span class="line-added">1395     return false;</span>
<span class="line-added">1396   }</span>
<span class="line-added">1397 </span>
<span class="line-added">1398   policy()-&gt;record_new_heap_size(num_regions());</span>
<span class="line-added">1399   return true;</span>
<span class="line-added">1400 }</span>
<span class="line-added">1401 </span>
1402 void G1CollectedHeap::shrink_helper(size_t shrink_bytes) {
1403   size_t aligned_shrink_bytes =
1404     ReservedSpace::page_align_size_down(shrink_bytes);
1405   aligned_shrink_bytes = align_down(aligned_shrink_bytes,
1406                                          HeapRegion::GrainBytes);
1407   uint num_regions_to_remove = (uint)(shrink_bytes / HeapRegion::GrainBytes);
1408 
1409   uint num_regions_removed = _hrm-&gt;shrink_by(num_regions_to_remove);
1410   size_t shrunk_bytes = num_regions_removed * HeapRegion::GrainBytes;
1411 

1412   log_debug(gc, ergo, heap)(&quot;Shrink the heap. requested shrinking amount: &quot; SIZE_FORMAT &quot;B aligned shrinking amount: &quot; SIZE_FORMAT &quot;B attempted shrinking amount: &quot; SIZE_FORMAT &quot;B&quot;,
1413                             shrink_bytes, aligned_shrink_bytes, shrunk_bytes);
1414   if (num_regions_removed &gt; 0) {
1415     policy()-&gt;record_new_heap_size(num_regions());
1416   } else {
1417     log_debug(gc, ergo, heap)(&quot;Did not expand the heap (heap shrinking operation failed)&quot;);
1418   }
1419 }
1420 
1421 void G1CollectedHeap::shrink(size_t shrink_bytes) {
1422   _verifier-&gt;verify_region_sets_optional();
1423 
1424   // We should only reach here at the end of a Full GC or during Remark which
1425   // means we should not not be holding to any GC alloc regions. The method
1426   // below will make sure of that and do any remaining clean up.
1427   _allocator-&gt;abandon_gc_alloc_regions();
1428 
1429   // Instead of tearing down / rebuilding the free lists here, we
1430   // could instead use the remove_all_pending() method on free_list to
1431   // remove only the ones that we need to remove.
</pre>
<hr />
<pre>
1482     // (a) If we&#39;re at a safepoint, operations on the master humongous
1483     // set should be invoked by either the VM thread (which will
1484     // serialize them) or by the GC workers while holding the
1485     // OldSets_lock.
1486     // (b) If we&#39;re not at a safepoint, operations on the master
1487     // humongous set should be invoked while holding the Heap_lock.
1488 
1489     if (SafepointSynchronize::is_at_safepoint()) {
1490       guarantee(Thread::current()-&gt;is_VM_thread() ||
1491                 OldSets_lock-&gt;owned_by_self(),
1492                 &quot;master humongous set MT safety protocol at a safepoint&quot;);
1493     } else {
1494       guarantee(Heap_lock-&gt;owned_by_self(),
1495                 &quot;master humongous set MT safety protocol outside a safepoint&quot;);
1496     }
1497   }
1498   bool is_correct_type(HeapRegion* hr) { return hr-&gt;is_humongous(); }
1499   const char* get_description() { return &quot;Humongous Regions&quot;; }
1500 };
1501 
<span class="line-modified">1502 G1CollectedHeap::G1CollectedHeap() :</span>
1503   CollectedHeap(),
1504   _young_gen_sampling_thread(NULL),
1505   _workers(NULL),

1506   _card_table(NULL),
1507   _soft_ref_policy(),
1508   _old_set(&quot;Old Region Set&quot;, new OldRegionSetChecker()),
1509   _archive_set(&quot;Archive Region Set&quot;, new ArchiveRegionSetChecker()),
1510   _humongous_set(&quot;Humongous Region Set&quot;, new HumongousRegionSetChecker()),
1511   _bot(NULL),
1512   _listener(),
<span class="line-added">1513   _numa(G1NUMA::create()),</span>
1514   _hrm(NULL),
1515   _allocator(NULL),
1516   _verifier(NULL),
1517   _summary_bytes_used(0),
<span class="line-added">1518   _bytes_used_during_gc(0),</span>
1519   _archive_allocator(NULL),
1520   _survivor_evac_stats(&quot;Young&quot;, YoungPLABSize, PLABWeight),
1521   _old_evac_stats(&quot;Old&quot;, OldPLABSize, PLABWeight),
1522   _expand_heap_after_alloc_failure(true),
1523   _g1mm(NULL),
1524   _humongous_reclaim_candidates(),
1525   _has_humongous_reclaim_candidates(false),
1526   _hr_printer(),
1527   _collector_state(),
1528   _old_marking_cycles_started(0),
1529   _old_marking_cycles_completed(0),
1530   _eden(),
1531   _survivor(),
1532   _gc_timer_stw(new (ResourceObj::C_HEAP, mtGC) STWGCTimer()),
1533   _gc_tracer_stw(new (ResourceObj::C_HEAP, mtGC) G1NewTracer()),
<span class="line-modified">1534   _policy(G1Policy::create_policy(_gc_timer_stw)),</span>
1535   _heap_sizing_policy(NULL),
1536   _collection_set(this, _policy),
1537   _hot_card_cache(NULL),
1538   _rem_set(NULL),

1539   _cm(NULL),
1540   _cm_thread(NULL),
1541   _cr(NULL),
1542   _task_queues(NULL),
1543   _evacuation_failed(false),
1544   _evacuation_failed_info_array(NULL),
1545   _preserved_marks_set(true /* in_c_heap */),
1546 #ifndef PRODUCT
1547   _evacuation_failure_alot_for_current_gc(false),
1548   _evacuation_failure_alot_gc_number(0),
1549   _evacuation_failure_alot_count(0),
1550 #endif
1551   _ref_processor_stw(NULL),
1552   _is_alive_closure_stw(this),
1553   _is_subject_to_discovery_stw(this),
1554   _ref_processor_cm(NULL),
1555   _is_alive_closure_cm(this),
1556   _is_subject_to_discovery_cm(this),
<span class="line-modified">1557   _region_attr() {</span>
1558 
1559   _verifier = new G1HeapVerifier(this);
1560 
1561   _allocator = new G1Allocator(this);
1562 
1563   _heap_sizing_policy = G1HeapSizingPolicy::create(this, _policy-&gt;analytics());
1564 
1565   _humongous_object_threshold_in_words = humongous_threshold_for(HeapRegion::GrainWords);
1566 
1567   // Override the default _filler_array_max_size so that no humongous filler
1568   // objects are created.
1569   _filler_array_max_size = _humongous_object_threshold_in_words;
1570 
1571   uint n_queues = ParallelGCThreads;
1572   _task_queues = new RefToScanQueueSet(n_queues);
1573 
1574   _evacuation_failed_info_array = NEW_C_HEAP_ARRAY(EvacuationFailedInfo, n_queues, mtGC);
1575 
1576   for (uint i = 0; i &lt; n_queues; i++) {
1577     RefToScanQueue* q = new RefToScanQueue();
1578     q-&gt;initialize();
1579     _task_queues-&gt;register_queue(i, q);
1580     ::new (&amp;_evacuation_failed_info_array[i]) EvacuationFailedInfo();
1581   }
1582 
1583   // Initialize the G1EvacuationFailureALot counters and flags.
1584   NOT_PRODUCT(reset_evacuation_should_fail();)
<span class="line-added">1585   _gc_tracer_stw-&gt;initialize();</span>
1586 
1587   guarantee(_task_queues != NULL, &quot;task_queues allocation failure.&quot;);
1588 }
1589 
1590 static size_t actual_reserved_page_size(ReservedSpace rs) {
1591   size_t page_size = os::vm_page_size();
1592   if (UseLargePages) {
1593     // There are two ways to manage large page memory.
1594     // 1. OS supports committing large page memory.
1595     // 2. OS doesn&#39;t support committing large page memory so ReservedSpace manages it.
1596     //    And ReservedSpace calls it &#39;special&#39;. If we failed to set &#39;special&#39;,
1597     //    we reserved memory without large page.
1598     if (os::can_commit_large_page_memory() || rs.special()) {
<span class="line-modified">1599       // An alignment at ReservedSpace comes from preferred page size or</span>
<span class="line-added">1600       // heap alignment, and if the alignment came from heap alignment, it could be</span>
<span class="line-added">1601       // larger than large pages size. So need to cap with the large page size.</span>
<span class="line-added">1602       page_size = MIN2(rs.alignment(), os::large_page_size());</span>
1603     }
1604   }
1605 
1606   return page_size;
1607 }
1608 
1609 G1RegionToSpaceMapper* G1CollectedHeap::create_aux_memory_mapper(const char* description,
1610                                                                  size_t size,
1611                                                                  size_t translation_factor) {
1612   size_t preferred_page_size = os::page_size_for_region_unaligned(size, 1);
1613   // Allocate a new reserved space, preferring to use large pages.
1614   ReservedSpace rs(size, preferred_page_size);
1615   size_t page_size = actual_reserved_page_size(rs);
1616   G1RegionToSpaceMapper* result  =
1617     G1RegionToSpaceMapper::create_mapper(rs,
1618                                          size,
1619                                          page_size,
1620                                          HeapRegion::GrainBytes,
1621                                          translation_factor,
1622                                          mtGC);
</pre>
<hr />
<pre>
1630 
1631   return result;
1632 }
1633 
1634 jint G1CollectedHeap::initialize_concurrent_refinement() {
1635   jint ecode = JNI_OK;
1636   _cr = G1ConcurrentRefine::create(&amp;ecode);
1637   return ecode;
1638 }
1639 
1640 jint G1CollectedHeap::initialize_young_gen_sampling_thread() {
1641   _young_gen_sampling_thread = new G1YoungRemSetSamplingThread();
1642   if (_young_gen_sampling_thread-&gt;osthread() == NULL) {
1643     vm_shutdown_during_initialization(&quot;Could not create G1YoungRemSetSamplingThread&quot;);
1644     return JNI_ENOMEM;
1645   }
1646   return JNI_OK;
1647 }
1648 
1649 jint G1CollectedHeap::initialize() {

1650 
1651   // Necessary to satisfy locking discipline assertions.
1652 
1653   MutexLocker x(Heap_lock);
1654 
1655   // While there are no constraints in the GC code that HeapWordSize
1656   // be any particular value, there are multiple other areas in the
1657   // system which believe this to be true (e.g. oop-&gt;object_size in some
1658   // cases incorrectly returns the size in wordSize units rather than
1659   // HeapWordSize).
1660   guarantee(HeapWordSize == wordSize, &quot;HeapWordSize must equal wordSize&quot;);
1661 
<span class="line-modified">1662   size_t init_byte_size = InitialHeapSize;</span>
<span class="line-modified">1663   size_t reserved_byte_size = G1Arguments::heap_reserved_size_bytes();</span>

1664 
1665   // Ensure that the sizes are properly aligned.
1666   Universe::check_alignment(init_byte_size, HeapRegion::GrainBytes, &quot;g1 heap&quot;);
<span class="line-modified">1667   Universe::check_alignment(reserved_byte_size, HeapRegion::GrainBytes, &quot;g1 heap&quot;);</span>
<span class="line-modified">1668   Universe::check_alignment(reserved_byte_size, HeapAlignment, &quot;g1 heap&quot;);</span>
1669 
1670   // Reserve the maximum.
1671 
1672   // When compressed oops are enabled, the preferred heap base
1673   // is calculated by subtracting the requested size from the
1674   // 32Gb boundary and using the result as the base address for
1675   // heap reservation. If the requested size is not aligned to
1676   // HeapRegion::GrainBytes (i.e. the alignment that is passed
1677   // into the ReservedHeapSpace constructor) then the actual
1678   // base of the reserved heap may end up differing from the
1679   // address that was requested (i.e. the preferred heap base).
1680   // If this happens then we could end up using a non-optimal
1681   // compressed oops mode.
1682 
<span class="line-modified">1683   ReservedHeapSpace heap_rs = Universe::reserve_heap(reserved_byte_size,</span>
<span class="line-modified">1684                                                      HeapAlignment);</span>
1685 
<span class="line-modified">1686   initialize_reserved_region(heap_rs);</span>
1687 
1688   // Create the barrier set for the entire reserved region.
<span class="line-modified">1689   G1CardTable* ct = new G1CardTable(heap_rs.region());</span>
1690   ct-&gt;initialize();
1691   G1BarrierSet* bs = new G1BarrierSet(ct);
1692   bs-&gt;initialize();
1693   assert(bs-&gt;is_a(BarrierSet::G1BarrierSet), &quot;sanity&quot;);
1694   BarrierSet::set_barrier_set(bs);
1695   _card_table = ct;
1696 
<span class="line-modified">1697   {</span>
<span class="line-modified">1698     G1SATBMarkQueueSet&amp; satbqs = bs-&gt;satb_mark_queue_set();</span>
<span class="line-modified">1699     satbqs.set_process_completed_buffers_threshold(G1SATBProcessCompletedThreshold);</span>
<span class="line-modified">1700     satbqs.set_buffer_enqueue_threshold_percentage(G1SATBBufferEnqueueingThresholdPercent);</span>
<span class="line-modified">1701   }</span>











1702 
1703   // Create the hot card cache.
1704   _hot_card_cache = new G1HotCardCache(this);
1705 
1706   // Carve out the G1 part of the heap.
<span class="line-modified">1707   ReservedSpace g1_rs = heap_rs.first_part(reserved_byte_size);</span>
1708   size_t page_size = actual_reserved_page_size(heap_rs);
1709   G1RegionToSpaceMapper* heap_storage =
1710     G1RegionToSpaceMapper::create_heap_mapper(g1_rs,
1711                                               g1_rs.size(),
1712                                               page_size,
1713                                               HeapRegion::GrainBytes,
1714                                               1,
1715                                               mtJavaHeap);
1716   if(heap_storage == NULL) {
1717     vm_shutdown_during_initialization(&quot;Could not initialize G1 heap&quot;);
1718     return JNI_ERR;
1719   }
1720 
1721   os::trace_page_sizes(&quot;Heap&quot;,
<span class="line-modified">1722                        MinHeapSize,</span>
<span class="line-modified">1723                        reserved_byte_size,</span>
1724                        page_size,
1725                        heap_rs.base(),
1726                        heap_rs.size());
1727   heap_storage-&gt;set_mapping_changed_listener(&amp;_listener);
1728 
1729   // Create storage for the BOT, card table, card counts table (hot card cache) and the bitmaps.
1730   G1RegionToSpaceMapper* bot_storage =
1731     create_aux_memory_mapper(&quot;Block Offset Table&quot;,
1732                              G1BlockOffsetTable::compute_size(g1_rs.size() / HeapWordSize),
1733                              G1BlockOffsetTable::heap_map_factor());
1734 
1735   G1RegionToSpaceMapper* cardtable_storage =
1736     create_aux_memory_mapper(&quot;Card Table&quot;,
1737                              G1CardTable::compute_size(g1_rs.size() / HeapWordSize),
1738                              G1CardTable::heap_map_factor());
1739 
1740   G1RegionToSpaceMapper* card_counts_storage =
1741     create_aux_memory_mapper(&quot;Card Counts Table&quot;,
1742                              G1CardCounts::compute_size(g1_rs.size() / HeapWordSize),
1743                              G1CardCounts::heap_map_factor());
1744 
1745   size_t bitmap_size = G1CMBitMap::compute_size(g1_rs.size());
1746   G1RegionToSpaceMapper* prev_bitmap_storage =
1747     create_aux_memory_mapper(&quot;Prev Bitmap&quot;, bitmap_size, G1CMBitMap::heap_map_factor());
1748   G1RegionToSpaceMapper* next_bitmap_storage =
1749     create_aux_memory_mapper(&quot;Next Bitmap&quot;, bitmap_size, G1CMBitMap::heap_map_factor());
1750 
<span class="line-modified">1751   _hrm = HeapRegionManager::create_manager(this);</span>
1752 
1753   _hrm-&gt;initialize(heap_storage, prev_bitmap_storage, next_bitmap_storage, bot_storage, cardtable_storage, card_counts_storage);
1754   _card_table-&gt;initialize(cardtable_storage);
<span class="line-added">1755 </span>
1756   // Do later initialization work for concurrent refinement.
1757   _hot_card_cache-&gt;initialize(card_counts_storage);
1758 
1759   // 6843694 - ensure that the maximum region index can fit
1760   // in the remembered set structures.
1761   const uint max_region_idx = (1U &lt;&lt; (sizeof(RegionIdx_t)*BitsPerByte-1)) - 1;
1762   guarantee((max_regions() - 1) &lt;= max_region_idx, &quot;too many regions&quot;);
1763 
1764   // The G1FromCardCache reserves card with value 0 as &quot;invalid&quot;, so the heap must not
1765   // start within the first card.
1766   guarantee(g1_rs.base() &gt;= (char*)G1CardTable::card_size, &quot;Java heap must not start within the first card.&quot;);
1767   // Also create a G1 rem set.
1768   _rem_set = new G1RemSet(this, _card_table, _hot_card_cache);
1769   _rem_set-&gt;initialize(max_reserved_capacity(), max_regions());
1770 
1771   size_t max_cards_per_region = ((size_t)1 &lt;&lt; (sizeof(CardIdx_t)*BitsPerByte-1)) - 1;
1772   guarantee(HeapRegion::CardsPerRegion &gt; 0, &quot;make sure it&#39;s initialized&quot;);
1773   guarantee(HeapRegion::CardsPerRegion &lt; max_cards_per_region,
1774             &quot;too many cards per region&quot;);
1775 
1776   FreeRegionList::set_unrealistically_long_length(max_expandable_regions() + 1);
1777 
1778   _bot = new G1BlockOffsetTable(reserved_region(), bot_storage);
1779 
1780   {
1781     HeapWord* start = _hrm-&gt;reserved().start();
1782     HeapWord* end = _hrm-&gt;reserved().end();
1783     size_t granularity = HeapRegion::GrainBytes;
1784 
<span class="line-modified">1785     _region_attr.initialize(start, end, granularity);</span>
1786     _humongous_reclaim_candidates.initialize(start, end, granularity);
1787   }
1788 
1789   _workers = new WorkGang(&quot;GC Thread&quot;, ParallelGCThreads,
1790                           true /* are_GC_task_threads */,
1791                           false /* are_ConcurrentGC_threads */);
1792   if (_workers == NULL) {
1793     return JNI_ENOMEM;
1794   }
1795   _workers-&gt;initialize_workers();
1796 
<span class="line-added">1797   _numa-&gt;set_region_info(HeapRegion::GrainBytes, page_size);</span>
<span class="line-added">1798 </span>
1799   // Create the G1ConcurrentMark data structure and thread.
1800   // (Must do this late, so that &quot;max_regions&quot; is defined.)
1801   _cm = new G1ConcurrentMark(this, prev_bitmap_storage, next_bitmap_storage);
<span class="line-modified">1802   if (!_cm-&gt;completed_initialization()) {</span>
<span class="line-modified">1803     vm_shutdown_during_initialization(&quot;Could not initialize G1ConcurrentMark&quot;);</span>
1804     return JNI_ENOMEM;
1805   }
1806   _cm_thread = _cm-&gt;cm_thread();
1807 
1808   // Now expand into the initial heap size.
1809   if (!expand(init_byte_size, _workers)) {
1810     vm_shutdown_during_initialization(&quot;Failed to allocate initial heap.&quot;);
1811     return JNI_ENOMEM;
1812   }
1813 
1814   // Perform any initialization actions delegated to the policy.
1815   policy()-&gt;init(this, &amp;_collection_set);
1816 
1817   jint ecode = initialize_concurrent_refinement();
1818   if (ecode != JNI_OK) {
1819     return ecode;
1820   }
1821 
1822   ecode = initialize_young_gen_sampling_thread();
1823   if (ecode != JNI_OK) {
1824     return ecode;
1825   }
1826 
1827   {
1828     G1DirtyCardQueueSet&amp; dcqs = G1BarrierSet::dirty_card_queue_set();
<span class="line-modified">1829     dcqs.set_process_cards_threshold(concurrent_refine()-&gt;yellow_zone());</span>
<span class="line-modified">1830     dcqs.set_max_cards(concurrent_refine()-&gt;red_zone());</span>
1831   }
1832 
1833   // Here we allocate the dummy HeapRegion that is required by the
1834   // G1AllocRegion class.
1835   HeapRegion* dummy_region = _hrm-&gt;get_dummy_region();
1836 
1837   // We&#39;ll re-use the same region whether the alloc region will
1838   // require BOT updates or not and, if it doesn&#39;t, then a non-young
1839   // region will complain that it cannot support allocations without
1840   // BOT updates. So we&#39;ll tag the dummy region as eden to avoid that.
1841   dummy_region-&gt;set_eden();
1842   // Make sure it&#39;s full.
1843   dummy_region-&gt;set_top(dummy_region-&gt;end());
1844   G1AllocRegion::setup(this, dummy_region);
1845 
<span class="line-modified">1846   _allocator-&gt;init_mutator_alloc_regions();</span>
1847 
1848   // Do create of the monitoring and management support so that
1849   // values in the heap have been properly initialized.
1850   _g1mm = new G1MonitoringSupport(this);
1851 
1852   G1StringDedup::initialize();
1853 
1854   _preserved_marks_set.init(ParallelGCThreads);
1855 
1856   _collection_set.initialize(max_regions());
1857 
1858   return JNI_OK;
1859 }
1860 
1861 void G1CollectedHeap::stop() {
1862   // Stop all concurrent threads. We do this to make sure these threads
1863   // do not continue to execute and access resources (e.g. logging)
1864   // that are destroyed during shutdown.
1865   _cr-&gt;stop();
1866   _young_gen_sampling_thread-&gt;stop();
1867   _cm_thread-&gt;stop();
1868   if (G1StringDedup::is_enabled()) {
1869     G1StringDedup::stop();
1870   }
1871 }
1872 
1873 void G1CollectedHeap::safepoint_synchronize_begin() {
1874   SuspendibleThreadSet::synchronize();
1875 }
1876 
1877 void G1CollectedHeap::safepoint_synchronize_end() {
1878   SuspendibleThreadSet::desynchronize();
1879 }
1880 




1881 void G1CollectedHeap::post_initialize() {
1882   CollectedHeap::post_initialize();
1883   ref_processing_init();
1884 }
1885 
1886 void G1CollectedHeap::ref_processing_init() {
1887   // Reference processing in G1 currently works as follows:
1888   //
1889   // * There are two reference processor instances. One is
1890   //   used to record and process discovered references
1891   //   during concurrent marking; the other is used to
1892   //   record and process references during STW pauses
1893   //   (both full and incremental).
1894   // * Both ref processors need to &#39;span&#39; the entire heap as
1895   //   the regions in the collection set may be dotted around.
1896   //
1897   // * For the concurrent marking ref processor:
1898   //   * Reference discovery is enabled at initial marking.
1899   //   * Reference discovery is disabled and the discovered
1900   //     references processed etc during remarking.
</pre>
<hr />
<pre>
1927                            mt_processing,                                  // mt processing
1928                            ParallelGCThreads,                              // degree of mt processing
1929                            (ParallelGCThreads &gt; 1) || (ConcGCThreads &gt; 1), // mt discovery
1930                            MAX2(ParallelGCThreads, ConcGCThreads),         // degree of mt discovery
1931                            false,                                          // Reference discovery is not atomic
1932                            &amp;_is_alive_closure_cm,                          // is alive closure
1933                            true);                                          // allow changes to number of processing threads
1934 
1935   // STW ref processor
1936   _ref_processor_stw =
1937     new ReferenceProcessor(&amp;_is_subject_to_discovery_stw,
1938                            mt_processing,                        // mt processing
1939                            ParallelGCThreads,                    // degree of mt processing
1940                            (ParallelGCThreads &gt; 1),              // mt discovery
1941                            ParallelGCThreads,                    // degree of mt discovery
1942                            true,                                 // Reference discovery is atomic
1943                            &amp;_is_alive_closure_stw,               // is alive closure
1944                            true);                                // allow changes to number of processing threads
1945 }
1946 




1947 SoftRefPolicy* G1CollectedHeap::soft_ref_policy() {
1948   return &amp;_soft_ref_policy;
1949 }
1950 
1951 size_t G1CollectedHeap::capacity() const {
1952   return _hrm-&gt;length() * HeapRegion::GrainBytes;
1953 }
1954 
1955 size_t G1CollectedHeap::unused_committed_regions_in_bytes() const {
1956   return _hrm-&gt;total_free_bytes();
1957 }
1958 
<span class="line-modified">1959 void G1CollectedHeap::iterate_hcc_closure(G1CardTableEntryClosure* cl, uint worker_id) {</span>
<span class="line-modified">1960   _hot_card_cache-&gt;drain(cl, worker_id);</span>










1961 }
1962 
1963 // Computes the sum of the storage used by the various regions.
1964 size_t G1CollectedHeap::used() const {
1965   size_t result = _summary_bytes_used + _allocator-&gt;used_in_alloc_regions();
1966   if (_archive_allocator != NULL) {
1967     result += _archive_allocator-&gt;used();
1968   }
1969   return result;
1970 }
1971 
1972 size_t G1CollectedHeap::used_unlocked() const {
1973   return _summary_bytes_used;
1974 }
1975 
1976 class SumUsedClosure: public HeapRegionClosure {
1977   size_t _used;
1978 public:
1979   SumUsedClosure() : _used(0) {}
1980   bool do_heap_region(HeapRegion* r) {
</pre>
<hr />
<pre>
1984   size_t result() { return _used; }
1985 };
1986 
1987 size_t G1CollectedHeap::recalculate_used() const {
1988   SumUsedClosure blk;
1989   heap_region_iterate(&amp;blk);
1990   return blk.result();
1991 }
1992 
1993 bool  G1CollectedHeap::is_user_requested_concurrent_full_gc(GCCause::Cause cause) {
1994   switch (cause) {
1995     case GCCause::_java_lang_system_gc:                 return ExplicitGCInvokesConcurrent;
1996     case GCCause::_dcmd_gc_run:                         return ExplicitGCInvokesConcurrent;
1997     case GCCause::_wb_conc_mark:                        return true;
1998     default :                                           return false;
1999   }
2000 }
2001 
2002 bool G1CollectedHeap::should_do_concurrent_full_gc(GCCause::Cause cause) {
2003   switch (cause) {

2004     case GCCause::_g1_humongous_allocation: return true;
2005     case GCCause::_g1_periodic_collection:  return G1PeriodicGCInvokesConcurrent;
2006     default:                                return is_user_requested_concurrent_full_gc(cause);
2007   }
2008 }
2009 
2010 bool G1CollectedHeap::should_upgrade_to_full_gc(GCCause::Cause cause) {
<span class="line-modified">2011   if (policy()-&gt;force_upgrade_to_full()) {</span>
2012     return true;
2013   } else if (should_do_concurrent_full_gc(_gc_cause)) {
2014     return false;
2015   } else if (has_regions_left_for_allocation()) {
2016     return false;
2017   } else {
2018     return true;
2019   }
2020 }
2021 
2022 #ifndef PRODUCT
2023 void G1CollectedHeap::allocate_dummy_regions() {
2024   // Let&#39;s fill up most of the region
2025   size_t word_size = HeapRegion::GrainWords - 1024;
2026   // And as a result the region we&#39;ll allocate will be humongous.
2027   guarantee(is_humongous(word_size), &quot;sanity&quot;);
2028 
2029   // _filler_array_max_size is set to humongous object threshold
2030   // but temporarily change it to use CollectedHeap::fill_with_object().
2031   SizeTFlagSetting fs(_filler_array_max_size, word_size);
</pre>
<hr />
<pre>
2038       CollectedHeap::fill_with_object(mr);
2039     } else {
2040       // If we can&#39;t allocate once, we probably cannot allocate
2041       // again. Let&#39;s get out of the loop.
2042       break;
2043     }
2044   }
2045 }
2046 #endif // !PRODUCT
2047 
2048 void G1CollectedHeap::increment_old_marking_cycles_started() {
2049   assert(_old_marking_cycles_started == _old_marking_cycles_completed ||
2050          _old_marking_cycles_started == _old_marking_cycles_completed + 1,
2051          &quot;Wrong marking cycle count (started: %d, completed: %d)&quot;,
2052          _old_marking_cycles_started, _old_marking_cycles_completed);
2053 
2054   _old_marking_cycles_started++;
2055 }
2056 
2057 void G1CollectedHeap::increment_old_marking_cycles_completed(bool concurrent) {
<span class="line-modified">2058   MonitorLocker ml(G1OldGCCount_lock, Mutex::_no_safepoint_check_flag);</span>
2059 
2060   // We assume that if concurrent == true, then the caller is a
2061   // concurrent thread that was joined the Suspendible Thread
2062   // Set. If there&#39;s ever a cheap way to check this, we should add an
2063   // assert here.
2064 
2065   // Given that this method is called at the end of a Full GC or of a
2066   // concurrent cycle, and those can be nested (i.e., a Full GC can
2067   // interrupt a concurrent cycle), the number of full collections
2068   // completed should be either one (in the case where there was no
2069   // nesting) or two (when a Full GC interrupted a concurrent cycle)
2070   // behind the number of full collections started.
2071 
2072   // This is the case for the inner caller, i.e. a Full GC.
2073   assert(concurrent ||
2074          (_old_marking_cycles_started == _old_marking_cycles_completed + 1) ||
2075          (_old_marking_cycles_started == _old_marking_cycles_completed + 2),
2076          &quot;for inner caller (Full GC): _old_marking_cycles_started = %u &quot;
2077          &quot;is inconsistent with _old_marking_cycles_completed = %u&quot;,
2078          _old_marking_cycles_started, _old_marking_cycles_completed);
2079 
2080   // This is the case for the outer caller, i.e. the concurrent cycle.
2081   assert(!concurrent ||
2082          (_old_marking_cycles_started == _old_marking_cycles_completed + 1),
2083          &quot;for outer caller (concurrent cycle): &quot;
2084          &quot;_old_marking_cycles_started = %u &quot;
2085          &quot;is inconsistent with _old_marking_cycles_completed = %u&quot;,
2086          _old_marking_cycles_started, _old_marking_cycles_completed);
2087 
2088   _old_marking_cycles_completed += 1;
2089 
2090   // We need to clear the &quot;in_progress&quot; flag in the CM thread before
2091   // we wake up any waiters (especially when ExplicitInvokesConcurrent
2092   // is set) so that if a waiter requests another System.gc() it doesn&#39;t
2093   // incorrectly see that a marking cycle is still in progress.
2094   if (concurrent) {
2095     _cm_thread-&gt;set_idle();
2096   }
2097 
<span class="line-modified">2098   // Notify threads waiting in System.gc() (with ExplicitGCInvokesConcurrent)</span>
<span class="line-modified">2099   // for a full GC to finish that their wait is over.</span>
<span class="line-modified">2100   ml.notify_all();</span>


2101 }
2102 
2103 void G1CollectedHeap::collect(GCCause::Cause cause) {
<span class="line-modified">2104   try_collect(cause);</span>
<span class="line-modified">2105 }</span>
<span class="line-modified">2106 </span>
<span class="line-modified">2107 // Return true if (x &lt; y) with allowance for wraparound.</span>
<span class="line-added">2108 static bool gc_counter_less_than(uint x, uint y) {</span>
<span class="line-added">2109   return (x - y) &gt; (UINT_MAX/2);</span>
<span class="line-added">2110 }</span>
<span class="line-added">2111 </span>
<span class="line-added">2112 // LOG_COLLECT_CONCURRENTLY(cause, msg, args...)</span>
<span class="line-added">2113 // Macro so msg printing is format-checked.</span>
<span class="line-added">2114 #define LOG_COLLECT_CONCURRENTLY(cause, ...)                            \</span>
<span class="line-added">2115   do {                                                                  \</span>
<span class="line-added">2116     LogTarget(Trace, gc) LOG_COLLECT_CONCURRENTLY_lt;                   \</span>
<span class="line-added">2117     if (LOG_COLLECT_CONCURRENTLY_lt.is_enabled()) {                     \</span>
<span class="line-added">2118       ResourceMark rm; /* For thread name. */                           \</span>
<span class="line-added">2119       LogStream LOG_COLLECT_CONCURRENTLY_s(&amp;LOG_COLLECT_CONCURRENTLY_lt); \</span>
<span class="line-added">2120       LOG_COLLECT_CONCURRENTLY_s.print(&quot;%s: Try Collect Concurrently (%s): &quot;, \</span>
<span class="line-added">2121                                        Thread::current()-&gt;name(),       \</span>
<span class="line-added">2122                                        GCCause::to_string(cause));      \</span>
<span class="line-added">2123       LOG_COLLECT_CONCURRENTLY_s.print(__VA_ARGS__);                    \</span>
<span class="line-added">2124     }                                                                   \</span>
<span class="line-added">2125   } while (0)</span>
<span class="line-added">2126 </span>
<span class="line-added">2127 #define LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, result) \</span>
<span class="line-added">2128   LOG_COLLECT_CONCURRENTLY(cause, &quot;complete %s&quot;, BOOL_TO_STR(result))</span>
<span class="line-added">2129 </span>
<span class="line-added">2130 bool G1CollectedHeap::try_collect_concurrently(GCCause::Cause cause,</span>
<span class="line-added">2131                                                uint gc_counter,</span>
<span class="line-added">2132                                                uint old_marking_started_before) {</span>
2133   assert_heap_not_locked();
<span class="line-added">2134   assert(should_do_concurrent_full_gc(cause),</span>
<span class="line-added">2135          &quot;Non-concurrent cause %s&quot;, GCCause::to_string(cause));</span>
<span class="line-added">2136 </span>
<span class="line-added">2137   for (uint i = 1; true; ++i) {</span>
<span class="line-added">2138     // Try to schedule an initial-mark evacuation pause that will</span>
<span class="line-added">2139     // start a concurrent cycle.</span>
<span class="line-added">2140     LOG_COLLECT_CONCURRENTLY(cause, &quot;attempt %u&quot;, i);</span>
<span class="line-added">2141     VM_G1TryInitiateConcMark op(gc_counter,</span>
<span class="line-added">2142                                 cause,</span>
<span class="line-added">2143                                 policy()-&gt;max_pause_time_ms());</span>
<span class="line-added">2144     VMThread::execute(&amp;op);</span>
<span class="line-added">2145 </span>
<span class="line-added">2146     // Request is trivially finished.</span>
<span class="line-added">2147     if (cause == GCCause::_g1_periodic_collection) {</span>
<span class="line-added">2148       LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, op.gc_succeeded());</span>
<span class="line-added">2149       return op.gc_succeeded();</span>
<span class="line-added">2150     }</span>
2151 
<span class="line-modified">2152     // If VMOp skipped initiating concurrent marking cycle because</span>
<span class="line-modified">2153     // we&#39;re terminating, then we&#39;re done.</span>
<span class="line-modified">2154     if (op.terminating()) {</span>
<span class="line-modified">2155       LOG_COLLECT_CONCURRENTLY(cause, &quot;skipped: terminating&quot;);</span>
<span class="line-modified">2156       return false;</span>
<span class="line-modified">2157     }</span>



2158 
<span class="line-added">2159     // Lock to get consistent set of values.</span>
<span class="line-added">2160     uint old_marking_started_after;</span>
<span class="line-added">2161     uint old_marking_completed_after;</span>
2162     {
2163       MutexLocker ml(Heap_lock);
<span class="line-modified">2164       // Update gc_counter for retrying VMOp if needed. Captured here to be</span>
<span class="line-modified">2165       // consistent with the values we use below for termination tests.  If</span>
<span class="line-modified">2166       // a retry is needed after a possible wait, and another collection</span>
<span class="line-modified">2167       // occurs in the meantime, it will cause our retry to be skipped and</span>
<span class="line-modified">2168       // we&#39;ll recheck for termination with updated conditions from that</span>
<span class="line-added">2169       // more recent collection.  That&#39;s what we want, rather than having</span>
<span class="line-added">2170       // our retry possibly perform an unnecessary collection.</span>
<span class="line-added">2171       gc_counter = total_collections();</span>
<span class="line-added">2172       old_marking_started_after = _old_marking_cycles_started;</span>
<span class="line-added">2173       old_marking_completed_after = _old_marking_cycles_completed;</span>
2174     }
2175 
<span class="line-modified">2176     if (!GCCause::is_user_requested_gc(cause)) {</span>
<span class="line-modified">2177       // For an &quot;automatic&quot; (not user-requested) collection, we just need to</span>
<span class="line-modified">2178       // ensure that progress is made.</span>
<span class="line-modified">2179       //</span>
<span class="line-modified">2180       // Request is finished if any of</span>
<span class="line-modified">2181       // (1) the VMOp successfully performed a GC,</span>
<span class="line-modified">2182       // (2) a concurrent cycle was already in progress,</span>
<span class="line-modified">2183       // (3) a new cycle was started (by this thread or some other), or</span>
<span class="line-modified">2184       // (4) a Full GC was performed.</span>
<span class="line-modified">2185       // Cases (3) and (4) are detected together by a change to</span>
<span class="line-modified">2186       // _old_marking_cycles_started.</span>
<span class="line-modified">2187       //</span>
<span class="line-modified">2188       // Note that (1) does not imply (3).  If we&#39;re still in the mixed</span>
<span class="line-modified">2189       // phase of an earlier concurrent collection, the request to make the</span>
<span class="line-modified">2190       // collection an initial-mark won&#39;t be honored.  If we don&#39;t check for</span>
<span class="line-modified">2191       // both conditions we&#39;ll spin doing back-to-back collections.</span>
<span class="line-modified">2192       if (op.gc_succeeded() ||</span>
<span class="line-modified">2193           op.cycle_already_in_progress() ||</span>
<span class="line-modified">2194           (old_marking_started_before != old_marking_started_after)) {</span>
<span class="line-added">2195         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);</span>
<span class="line-added">2196         return true;</span>
<span class="line-added">2197       }</span>
<span class="line-added">2198     } else {                    // User-requested GC.</span>
<span class="line-added">2199       // For a user-requested collection, we want to ensure that a complete</span>
<span class="line-added">2200       // full collection has been performed before returning, but without</span>
<span class="line-added">2201       // waiting for more than needed.</span>
<span class="line-added">2202 </span>
<span class="line-added">2203       // For user-requested GCs (unlike non-UR), a successful VMOp implies a</span>
<span class="line-added">2204       // new cycle was started.  That&#39;s good, because it&#39;s not clear what we</span>
<span class="line-added">2205       // should do otherwise.  Trying again just does back to back GCs.</span>
<span class="line-added">2206       // Can&#39;t wait for someone else to start a cycle.  And returning fails</span>
<span class="line-added">2207       // to meet the goal of ensuring a full collection was performed.</span>
<span class="line-added">2208       assert(!op.gc_succeeded() ||</span>
<span class="line-added">2209              (old_marking_started_before != old_marking_started_after),</span>
<span class="line-added">2210              &quot;invariant: succeeded %s, started before %u, started after %u&quot;,</span>
<span class="line-added">2211              BOOL_TO_STR(op.gc_succeeded()),</span>
<span class="line-added">2212              old_marking_started_before, old_marking_started_after);</span>
<span class="line-added">2213 </span>
<span class="line-added">2214       // Request is finished if a full collection (concurrent or stw)</span>
<span class="line-added">2215       // was started after this request and has completed, e.g.</span>
<span class="line-added">2216       // started_before &lt; completed_after.</span>
<span class="line-added">2217       if (gc_counter_less_than(old_marking_started_before,</span>
<span class="line-added">2218                                old_marking_completed_after)) {</span>
<span class="line-added">2219         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);</span>
<span class="line-added">2220         return true;</span>
<span class="line-added">2221       }</span>
2222 
<span class="line-modified">2223       if (old_marking_started_after != old_marking_completed_after) {</span>
<span class="line-modified">2224         // If there is an in-progress cycle (possibly started by us), then</span>
<span class="line-added">2225         // wait for that cycle to complete, e.g.</span>
<span class="line-added">2226         // while completed_now &lt; started_after.</span>
<span class="line-added">2227         LOG_COLLECT_CONCURRENTLY(cause, &quot;wait&quot;);</span>
<span class="line-added">2228         MonitorLocker ml(G1OldGCCount_lock);</span>
<span class="line-added">2229         while (gc_counter_less_than(_old_marking_cycles_completed,</span>
<span class="line-added">2230                                     old_marking_started_after)) {</span>
<span class="line-added">2231           ml.wait();</span>
<span class="line-added">2232         }</span>
<span class="line-added">2233         // Request is finished if the collection we just waited for was</span>
<span class="line-added">2234         // started after this request.</span>
<span class="line-added">2235         if (old_marking_started_before != old_marking_started_after) {</span>
<span class="line-added">2236           LOG_COLLECT_CONCURRENTLY(cause, &quot;complete after wait&quot;);</span>
<span class="line-added">2237           return true;</span>
2238         }
2239       }
<span class="line-modified">2240 </span>
<span class="line-modified">2241       // If VMOp was successful then it started a new cycle that the above</span>
<span class="line-modified">2242       // wait &amp;etc should have recognized as finishing this request.  This</span>
<span class="line-modified">2243       // differs from a non-user-request, where gc_succeeded does not imply</span>
<span class="line-modified">2244       // a new cycle was started.</span>
<span class="line-modified">2245       assert(!op.gc_succeeded(), &quot;invariant&quot;);</span>
<span class="line-modified">2246 </span>
<span class="line-modified">2247       // If VMOp failed because a cycle was already in progress, it is now</span>
<span class="line-modified">2248       // complete.  But it didn&#39;t finish this user-requested GC, so try</span>
<span class="line-modified">2249       // again.</span>
<span class="line-modified">2250       if (op.cycle_already_in_progress()) {</span>
<span class="line-modified">2251         LOG_COLLECT_CONCURRENTLY(cause, &quot;retry after in-progress&quot;);</span>
<span class="line-modified">2252         continue;</span>





2253       }
2254     }
<span class="line-modified">2255 </span>
<span class="line-modified">2256     // Collection failed and should be retried.</span>
<span class="line-added">2257     assert(op.transient_failure(), &quot;invariant&quot;);</span>
<span class="line-added">2258 </span>
<span class="line-added">2259     // If GCLocker is active, wait until clear before retrying.</span>
<span class="line-added">2260     if (GCLocker::is_active_and_needs_gc()) {</span>
<span class="line-added">2261       LOG_COLLECT_CONCURRENTLY(cause, &quot;gc-locker stall&quot;);</span>
<span class="line-added">2262       GCLocker::stall_until_clear();</span>
<span class="line-added">2263     }</span>
<span class="line-added">2264 </span>
<span class="line-added">2265     LOG_COLLECT_CONCURRENTLY(cause, &quot;retry&quot;);</span>
<span class="line-added">2266   }</span>
<span class="line-added">2267 }</span>
<span class="line-added">2268 </span>
<span class="line-added">2269 bool G1CollectedHeap::try_collect(GCCause::Cause cause) {</span>
<span class="line-added">2270   assert_heap_not_locked();</span>
<span class="line-added">2271 </span>
<span class="line-added">2272   // Lock to get consistent set of values.</span>
<span class="line-added">2273   uint gc_count_before;</span>
<span class="line-added">2274   uint full_gc_count_before;</span>
<span class="line-added">2275   uint old_marking_started_before;</span>
<span class="line-added">2276   {</span>
<span class="line-added">2277     MutexLocker ml(Heap_lock);</span>
<span class="line-added">2278     gc_count_before = total_collections();</span>
<span class="line-added">2279     full_gc_count_before = total_full_collections();</span>
<span class="line-added">2280     old_marking_started_before = _old_marking_cycles_started;</span>
<span class="line-added">2281   }</span>
<span class="line-added">2282 </span>
<span class="line-added">2283   if (should_do_concurrent_full_gc(cause)) {</span>
<span class="line-added">2284     return try_collect_concurrently(cause,</span>
<span class="line-added">2285                                     gc_count_before,</span>
<span class="line-added">2286                                     old_marking_started_before);</span>
<span class="line-added">2287   } else if (GCLocker::should_discard(cause, gc_count_before)) {</span>
<span class="line-added">2288     // Indicate failure to be consistent with VMOp failure due to</span>
<span class="line-added">2289     // another collection slipping in after our gc_count but before</span>
<span class="line-added">2290     // our request is processed.</span>
<span class="line-added">2291     return false;</span>
<span class="line-added">2292   } else if (cause == GCCause::_gc_locker || cause == GCCause::_wb_young_gc</span>
<span class="line-added">2293              DEBUG_ONLY(|| cause == GCCause::_scavenge_alot)) {</span>
<span class="line-added">2294 </span>
<span class="line-added">2295     // Schedule a standard evacuation pause. We&#39;re setting word_size</span>
<span class="line-added">2296     // to 0 which means that we are not requesting a post-GC allocation.</span>
<span class="line-added">2297     VM_G1CollectForAllocation op(0,     /* word_size */</span>
<span class="line-added">2298                                  gc_count_before,</span>
<span class="line-added">2299                                  cause,</span>
<span class="line-added">2300                                  policy()-&gt;max_pause_time_ms());</span>
<span class="line-added">2301     VMThread::execute(&amp;op);</span>
<span class="line-added">2302     return op.gc_succeeded();</span>
<span class="line-added">2303   } else {</span>
<span class="line-added">2304     // Schedule a Full GC.</span>
<span class="line-added">2305     VM_G1CollectFull op(gc_count_before, full_gc_count_before, cause);</span>
<span class="line-added">2306     VMThread::execute(&amp;op);</span>
<span class="line-added">2307     return op.gc_succeeded();</span>
<span class="line-added">2308   }</span>
2309 }
2310 
2311 bool G1CollectedHeap::is_in(const void* p) const {
2312   if (_hrm-&gt;reserved().contains(p)) {
2313     // Given that we know that p is in the reserved space,
2314     // heap_region_containing() should successfully
2315     // return the containing region.
2316     HeapRegion* hr = heap_region_containing(p);
2317     return hr-&gt;is_in(p);
2318   } else {
2319     return false;
2320   }
2321 }
2322 
2323 #ifdef ASSERT
2324 bool G1CollectedHeap::is_in_exact(const void* p) const {
2325   bool contains = reserved_region().contains(p);
2326   bool available = _hrm-&gt;is_available(addr_to_region((HeapWord*)p));
2327   if (contains &amp;&amp; available) {
2328     return true;
</pre>
<hr />
<pre>
2336 
2337 // Iterates an ObjectClosure over all objects within a HeapRegion.
2338 
2339 class IterateObjectClosureRegionClosure: public HeapRegionClosure {
2340   ObjectClosure* _cl;
2341 public:
2342   IterateObjectClosureRegionClosure(ObjectClosure* cl) : _cl(cl) {}
2343   bool do_heap_region(HeapRegion* r) {
2344     if (!r-&gt;is_continues_humongous()) {
2345       r-&gt;object_iterate(_cl);
2346     }
2347     return false;
2348   }
2349 };
2350 
2351 void G1CollectedHeap::object_iterate(ObjectClosure* cl) {
2352   IterateObjectClosureRegionClosure blk(cl);
2353   heap_region_iterate(&amp;blk);
2354 }
2355 
<span class="line-added">2356 void G1CollectedHeap::keep_alive(oop obj) {</span>
<span class="line-added">2357   G1BarrierSet::enqueue(obj);</span>
<span class="line-added">2358 }</span>
<span class="line-added">2359 </span>
2360 void G1CollectedHeap::heap_region_iterate(HeapRegionClosure* cl) const {
2361   _hrm-&gt;iterate(cl);
2362 }
2363 
2364 void G1CollectedHeap::heap_region_par_iterate_from_worker_offset(HeapRegionClosure* cl,
2365                                                                  HeapRegionClaimer *hrclaimer,
2366                                                                  uint worker_id) const {
2367   _hrm-&gt;par_iterate(cl, hrclaimer, hrclaimer-&gt;offset_for_worker(worker_id));
2368 }
2369 
2370 void G1CollectedHeap::heap_region_par_iterate_from_start(HeapRegionClosure* cl,
2371                                                          HeapRegionClaimer *hrclaimer) const {
2372   _hrm-&gt;par_iterate(cl, hrclaimer, 0);
2373 }
2374 
<span class="line-modified">2375 void G1CollectedHeap::collection_set_iterate_all(HeapRegionClosure* cl) {</span>
2376   _collection_set.iterate(cl);
2377 }
2378 
<span class="line-modified">2379 void G1CollectedHeap::collection_set_par_iterate_all(HeapRegionClosure* cl, HeapRegionClaimer* hr_claimer, uint worker_id) {</span>
<span class="line-modified">2380   _collection_set.par_iterate(cl, hr_claimer, worker_id, workers()-&gt;active_workers());</span>
<span class="line-added">2381 }</span>
<span class="line-added">2382 </span>
<span class="line-added">2383 void G1CollectedHeap::collection_set_iterate_increment_from(HeapRegionClosure *cl, HeapRegionClaimer* hr_claimer, uint worker_id) {</span>
<span class="line-added">2384   _collection_set.iterate_incremental_part_from(cl, hr_claimer, worker_id, workers()-&gt;active_workers());</span>
2385 }
2386 
2387 HeapWord* G1CollectedHeap::block_start(const void* addr) const {
2388   HeapRegion* hr = heap_region_containing(addr);
2389   return hr-&gt;block_start(addr);
2390 }
2391 
2392 bool G1CollectedHeap::block_is_obj(const HeapWord* addr) const {
2393   HeapRegion* hr = heap_region_containing(addr);
2394   return hr-&gt;block_is_obj(addr);
2395 }
2396 
2397 bool G1CollectedHeap::supports_tlab_allocation() const {
2398   return true;
2399 }
2400 
2401 size_t G1CollectedHeap::tlab_capacity(Thread* ignored) const {
2402   return (_policy-&gt;young_list_target_length() - _survivor.length()) * HeapRegion::GrainBytes;
2403 }
2404 
</pre>
<hr />
<pre>
2445   }
2446 }
2447 
2448 void G1CollectedHeap::prepare_for_verify() {
2449   _verifier-&gt;prepare_for_verify();
2450 }
2451 
2452 void G1CollectedHeap::verify(VerifyOption vo) {
2453   _verifier-&gt;verify(vo);
2454 }
2455 
2456 bool G1CollectedHeap::supports_concurrent_phase_control() const {
2457   return true;
2458 }
2459 
2460 bool G1CollectedHeap::request_concurrent_phase(const char* phase) {
2461   return _cm_thread-&gt;request_concurrent_phase(phase);
2462 }
2463 
2464 bool G1CollectedHeap::is_heterogeneous_heap() const {
<span class="line-modified">2465   return G1Arguments::is_heterogeneous_heap();</span>
2466 }
2467 
2468 class PrintRegionClosure: public HeapRegionClosure {
2469   outputStream* _st;
2470 public:
2471   PrintRegionClosure(outputStream* st) : _st(st) {}
2472   bool do_heap_region(HeapRegion* r) {
2473     r-&gt;print_on(_st);
2474     return false;
2475   }
2476 };
2477 
2478 bool G1CollectedHeap::is_obj_dead_cond(const oop obj,
2479                                        const HeapRegion* hr,
2480                                        const VerifyOption vo) const {
2481   switch (vo) {
2482   case VerifyOption_G1UsePrevMarking: return is_obj_dead(obj, hr);
2483   case VerifyOption_G1UseNextMarking: return is_obj_ill(obj, hr);
2484   case VerifyOption_G1UseFullMarking: return is_obj_dead_full(obj, hr);
2485   default:                            ShouldNotReachHere();
</pre>
<hr />
<pre>
2505     print_regions_on(&amp;ls);
2506   }
2507 }
2508 
2509 void G1CollectedHeap::print_on(outputStream* st) const {
2510   st-&gt;print(&quot; %-20s&quot;, &quot;garbage-first heap&quot;);
2511   st-&gt;print(&quot; total &quot; SIZE_FORMAT &quot;K, used &quot; SIZE_FORMAT &quot;K&quot;,
2512             capacity()/K, used_unlocked()/K);
2513   st-&gt;print(&quot; [&quot; PTR_FORMAT &quot;, &quot; PTR_FORMAT &quot;)&quot;,
2514             p2i(_hrm-&gt;reserved().start()),
2515             p2i(_hrm-&gt;reserved().end()));
2516   st-&gt;cr();
2517   st-&gt;print(&quot;  region size &quot; SIZE_FORMAT &quot;K, &quot;, HeapRegion::GrainBytes / K);
2518   uint young_regions = young_regions_count();
2519   st-&gt;print(&quot;%u young (&quot; SIZE_FORMAT &quot;K), &quot;, young_regions,
2520             (size_t) young_regions * HeapRegion::GrainBytes / K);
2521   uint survivor_regions = survivor_regions_count();
2522   st-&gt;print(&quot;%u survivors (&quot; SIZE_FORMAT &quot;K)&quot;, survivor_regions,
2523             (size_t) survivor_regions * HeapRegion::GrainBytes / K);
2524   st-&gt;cr();
<span class="line-added">2525   if (_numa-&gt;is_enabled()) {</span>
<span class="line-added">2526     uint num_nodes = _numa-&gt;num_active_nodes();</span>
<span class="line-added">2527     st-&gt;print(&quot;  remaining free region(s) on each NUMA node: &quot;);</span>
<span class="line-added">2528     const int* node_ids = _numa-&gt;node_ids();</span>
<span class="line-added">2529     for (uint node_index = 0; node_index &lt; num_nodes; node_index++) {</span>
<span class="line-added">2530       st-&gt;print(&quot;%d=%u &quot;, node_ids[node_index], _hrm-&gt;num_free_regions(node_index));</span>
<span class="line-added">2531     }</span>
<span class="line-added">2532     st-&gt;cr();</span>
<span class="line-added">2533   }</span>
2534   MetaspaceUtils::print_on(st);
2535 }
2536 
2537 void G1CollectedHeap::print_regions_on(outputStream* st) const {
2538   st-&gt;print_cr(&quot;Heap Regions: E=young(eden), S=young(survivor), O=old, &quot;
2539                &quot;HS=humongous(starts), HC=humongous(continues), &quot;
<span class="line-modified">2540                &quot;CS=collection set, F=free, &quot;</span>
<span class="line-added">2541                &quot;OA=open archive, CA=closed archive, &quot;</span>
2542                &quot;TAMS=top-at-mark-start (previous, next)&quot;);
2543   PrintRegionClosure blk(st);
2544   heap_region_iterate(&amp;blk);
2545 }
2546 
2547 void G1CollectedHeap::print_extended_on(outputStream* st) const {
2548   print_on(st);
2549 
2550   // Print the per-region information.
2551   print_regions_on(st);
2552 }
2553 
2554 void G1CollectedHeap::print_on_error(outputStream* st) const {
2555   this-&gt;CollectedHeap::print_on_error(st);
2556 
2557   if (_cm != NULL) {
2558     st-&gt;cr();
2559     _cm-&gt;print_on_error(st);
2560   }
2561 }
</pre>
<hr />
<pre>
2611     tty-&gt;print_cr(&quot;----------&quot;);
2612     return false;
2613   }
2614 
2615   PrintRSetsClosure(const char* msg) : _msg(msg), _occupied_sum(0) {
2616     tty-&gt;cr();
2617     tty-&gt;print_cr(&quot;========================================&quot;);
2618     tty-&gt;print_cr(&quot;%s&quot;, msg);
2619     tty-&gt;cr();
2620   }
2621 
2622   ~PrintRSetsClosure() {
2623     tty-&gt;print_cr(&quot;Occupied Sum: &quot; SIZE_FORMAT, _occupied_sum);
2624     tty-&gt;print_cr(&quot;========================================&quot;);
2625     tty-&gt;cr();
2626   }
2627 };
2628 
2629 void G1CollectedHeap::print_cset_rsets() {
2630   PrintRSetsClosure cl(&quot;Printing CSet RSets&quot;);
<span class="line-modified">2631   collection_set_iterate_all(&amp;cl);</span>
2632 }
2633 
2634 void G1CollectedHeap::print_all_rsets() {
2635   PrintRSetsClosure cl(&quot;Printing All RSets&quot;);;
2636   heap_region_iterate(&amp;cl);
2637 }
2638 #endif // PRODUCT
2639 
<span class="line-added">2640 bool G1CollectedHeap::print_location(outputStream* st, void* addr) const {</span>
<span class="line-added">2641   return BlockLocationPrinter&lt;G1CollectedHeap&gt;::print_location(st, addr);</span>
<span class="line-added">2642 }</span>
<span class="line-added">2643 </span>
2644 G1HeapSummary G1CollectedHeap::create_g1_heap_summary() {
2645 
<span class="line-modified">2646   size_t eden_used_bytes = _eden.used_bytes();</span>
<span class="line-modified">2647   size_t survivor_used_bytes = _survivor.used_bytes();</span>
2648   size_t heap_used = Heap_lock-&gt;owned_by_self() ? used() : used_unlocked();
2649 
2650   size_t eden_capacity_bytes =
2651     (policy()-&gt;young_list_target_length() * HeapRegion::GrainBytes) - survivor_used_bytes;
2652 
2653   VirtualSpaceSummary heap_summary = create_heap_space_summary();
2654   return G1HeapSummary(heap_summary, heap_used, eden_used_bytes,
2655                        eden_capacity_bytes, survivor_used_bytes, num_regions());
2656 }
2657 
2658 G1EvacSummary G1CollectedHeap::create_g1_evac_summary(G1EvacStats* stats) {
2659   return G1EvacSummary(stats-&gt;allocated(), stats-&gt;wasted(), stats-&gt;undo_wasted(),
2660                        stats-&gt;unused(), stats-&gt;used(), stats-&gt;region_end_waste(),
2661                        stats-&gt;regions_filled(), stats-&gt;direct_allocated(),
2662                        stats-&gt;failure_used(), stats-&gt;failure_waste());
2663 }
2664 
2665 void G1CollectedHeap::trace_heap(GCWhen::Type when, const GCTracer* gc_tracer) {
2666   const G1HeapSummary&amp; heap_summary = create_g1_heap_summary();
2667   gc_tracer-&gt;report_gc_heap_summary(when, heap_summary);
2668 
2669   const MetaspaceSummary&amp; metaspace_summary = create_metaspace_summary();
2670   gc_tracer-&gt;report_metaspace_summary(when, metaspace_summary);
2671 }
2672 
2673 G1CollectedHeap* G1CollectedHeap::heap() {
2674   CollectedHeap* heap = Universe::heap();
2675   assert(heap != NULL, &quot;Uninitialized access to G1CollectedHeap::heap()&quot;);
2676   assert(heap-&gt;kind() == CollectedHeap::G1, &quot;Invalid name&quot;);
2677   return (G1CollectedHeap*)heap;
2678 }
2679 
2680 void G1CollectedHeap::gc_prologue(bool full) {

2681   assert(InlineCacheBuffer::is_empty(), &quot;should have cleaned up ICBuffer&quot;);
2682 
2683   // This summary needs to be printed before incrementing total collections.
2684   rem_set()-&gt;print_periodic_summary_info(&quot;Before GC RS summary&quot;, total_collections());
2685 
2686   // Update common counters.
2687   increment_total_collections(full /* full gc */);
<span class="line-modified">2688   if (full || collector_state()-&gt;in_initial_mark_gc()) {</span>
2689     increment_old_marking_cycles_started();
2690   }
2691 
2692   // Fill TLAB&#39;s and such
2693   double start = os::elapsedTime();
2694   ensure_parsability(true);
2695   phase_times()-&gt;record_prepare_tlab_time_ms((os::elapsedTime() - start) * 1000.0);
2696 }
2697 
2698 void G1CollectedHeap::gc_epilogue(bool full) {
2699   // Update common counters.
2700   if (full) {
2701     // Update the number of full collections that have been completed.
2702     increment_old_marking_cycles_completed(false /* concurrent */);
2703   }
2704 
2705   // We are at the end of the GC. Total collections has already been increased.
2706   rem_set()-&gt;print_periodic_summary_info(&quot;After GC RS summary&quot;, total_collections() - 1);
2707 
2708   // FIXME: what is this about?
2709   // I&#39;m ignoring the &quot;fill_newgen()&quot; call if &quot;alloc_event_enabled&quot;
2710   // is set.
2711 #if COMPILER2_OR_JVMCI
2712   assert(DerivedPointerTable::is_empty(), &quot;derived pointer present&quot;);
2713 #endif

2714 
2715   double start = os::elapsedTime();
2716   resize_all_tlabs();
2717   phase_times()-&gt;record_resize_tlab_time_ms((os::elapsedTime() - start) * 1000.0);
2718 
2719   MemoryService::track_memory_usage();
2720   // We have just completed a GC. Update the soft reference
2721   // policy with the new heap occupancy
2722   Universe::update_heap_info_at_gc();
<span class="line-added">2723 </span>
<span class="line-added">2724   // Print NUMA statistics.</span>
<span class="line-added">2725   _numa-&gt;print_statistics();</span>
<span class="line-added">2726 }</span>
<span class="line-added">2727 </span>
<span class="line-added">2728 void G1CollectedHeap::verify_numa_regions(const char* desc) {</span>
<span class="line-added">2729   LogTarget(Trace, gc, heap, verify) lt;</span>
<span class="line-added">2730 </span>
<span class="line-added">2731   if (lt.is_enabled()) {</span>
<span class="line-added">2732     LogStream ls(lt);</span>
<span class="line-added">2733     // Iterate all heap regions to print matching between preferred numa id and actual numa id.</span>
<span class="line-added">2734     G1NodeIndexCheckClosure cl(desc, _numa, &amp;ls);</span>
<span class="line-added">2735     heap_region_iterate(&amp;cl);</span>
<span class="line-added">2736   }</span>
2737 }
2738 
2739 HeapWord* G1CollectedHeap::do_collection_pause(size_t word_size,
2740                                                uint gc_count_before,
2741                                                bool* succeeded,
2742                                                GCCause::Cause gc_cause) {
2743   assert_heap_not_locked_and_not_at_safepoint();
2744   VM_G1CollectForAllocation op(word_size,
2745                                gc_count_before,
2746                                gc_cause,

2747                                policy()-&gt;max_pause_time_ms());
2748   VMThread::execute(&amp;op);
2749 
2750   HeapWord* result = op.result();
2751   bool ret_succeeded = op.prologue_succeeded() &amp;&amp; op.gc_succeeded();
2752   assert(result == NULL || ret_succeeded,
2753          &quot;the result should be NULL if the VM did not succeed&quot;);
2754   *succeeded = ret_succeeded;
2755 
2756   assert_heap_not_locked();
2757   return result;
2758 }
2759 
2760 void G1CollectedHeap::do_concurrent_mark() {
<span class="line-modified">2761   MutexLocker x(CGC_lock, Mutex::_no_safepoint_check_flag);</span>
2762   if (!_cm_thread-&gt;in_progress()) {
2763     _cm_thread-&gt;set_started();
2764     CGC_lock-&gt;notify();
2765   }
2766 }
2767 
2768 size_t G1CollectedHeap::pending_card_num() {
2769   struct CountCardsClosure : public ThreadClosure {
2770     size_t _cards;
2771     CountCardsClosure() : _cards(0) {}
2772     virtual void do_thread(Thread* t) {
2773       _cards += G1ThreadLocalData::dirty_card_queue(t).size();
2774     }
2775   } count_from_threads;
2776   Threads::threads_do(&amp;count_from_threads);
2777 
2778   G1DirtyCardQueueSet&amp; dcqs = G1BarrierSet::dirty_card_queue_set();
<span class="line-modified">2779   return dcqs.num_cards() + count_from_threads._cards;</span>



2780 }
2781 
2782 bool G1CollectedHeap::is_potential_eager_reclaim_candidate(HeapRegion* r) const {
2783   // We don&#39;t nominate objects with many remembered set entries, on
2784   // the assumption that such objects are likely still live.
2785   HeapRegionRemSet* rem_set = r-&gt;rem_set();
2786 
2787   return G1EagerReclaimHumongousObjectsWithStaleRefs ?
2788          rem_set-&gt;occupancy_less_or_equal_than(G1RSetSparseRegionEntries) :
2789          G1EagerReclaimHumongousObjects &amp;&amp; rem_set-&gt;is_empty();
2790 }
2791 
<span class="line-modified">2792 #ifndef PRODUCT</span>
<span class="line-modified">2793 void G1CollectedHeap::verify_region_attr_remset_update() {</span>
<span class="line-modified">2794   class VerifyRegionAttrRemSet : public HeapRegionClosure {</span>
<span class="line-modified">2795   public:</span>
<span class="line-modified">2796     virtual bool do_heap_region(HeapRegion* r) {</span>
<span class="line-modified">2797       G1CollectedHeap* g1h = G1CollectedHeap::heap();</span>
<span class="line-modified">2798       bool const needs_remset_update = g1h-&gt;region_attr(r-&gt;bottom()).needs_remset_update();</span>
<span class="line-modified">2799       assert(r-&gt;rem_set()-&gt;is_tracked() == needs_remset_update,</span>
<span class="line-modified">2800              &quot;Region %u remset tracking status (%s) different to region attribute (%s)&quot;,</span>
<span class="line-modified">2801              r-&gt;hrm_index(), BOOL_TO_STR(r-&gt;rem_set()-&gt;is_tracked()), BOOL_TO_STR(needs_remset_update));</span>






























































2802       return false;
2803     }
<span class="line-modified">2804   } cl;</span>

































































2805   heap_region_iterate(&amp;cl);









2806 }
<span class="line-added">2807 #endif</span>
2808 
2809 class VerifyRegionRemSetClosure : public HeapRegionClosure {
2810   public:
2811     bool do_heap_region(HeapRegion* hr) {
2812       if (!hr-&gt;is_archive() &amp;&amp; !hr-&gt;is_continues_humongous()) {
2813         hr-&gt;verify_rem_set();
2814       }
2815       return false;
2816     }
2817 };
2818 
2819 uint G1CollectedHeap::num_task_queues() const {
2820   return _task_queues-&gt;size();
2821 }
2822 
2823 #if TASKQUEUE_STATS
2824 void G1CollectedHeap::print_taskqueue_stats_hdr(outputStream* const st) {
2825   st-&gt;print_raw_cr(&quot;GC Task Stats&quot;);
2826   st-&gt;print_raw(&quot;thr &quot;); TaskQueueStats::print_header(1, st); st-&gt;cr();
2827   st-&gt;print_raw(&quot;--- &quot;); TaskQueueStats::print_header(2, st); st-&gt;cr();
</pre>
<hr />
<pre>
2868   if (waited) {
2869     double scan_wait_end = os::elapsedTime();
2870     wait_time_ms = (scan_wait_end - scan_wait_start) * 1000.0;
2871   }
2872   phase_times()-&gt;record_root_region_scan_wait_time(wait_time_ms);
2873 }
2874 
2875 class G1PrintCollectionSetClosure : public HeapRegionClosure {
2876 private:
2877   G1HRPrinter* _hr_printer;
2878 public:
2879   G1PrintCollectionSetClosure(G1HRPrinter* hr_printer) : HeapRegionClosure(), _hr_printer(hr_printer) { }
2880 
2881   virtual bool do_heap_region(HeapRegion* r) {
2882     _hr_printer-&gt;cset(r);
2883     return false;
2884   }
2885 };
2886 
2887 void G1CollectedHeap::start_new_collection_set() {
<span class="line-added">2888   double start = os::elapsedTime();</span>
<span class="line-added">2889 </span>
2890   collection_set()-&gt;start_incremental_building();
2891 
<span class="line-modified">2892   clear_region_attr();</span>
2893 
2894   guarantee(_eden.length() == 0, &quot;eden should have been cleared&quot;);
2895   policy()-&gt;transfer_survivors_to_cset(survivor());
<span class="line-added">2896 </span>
<span class="line-added">2897   // We redo the verification but now wrt to the new CSet which</span>
<span class="line-added">2898   // has just got initialized after the previous CSet was freed.</span>
<span class="line-added">2899   _cm-&gt;verify_no_collection_set_oops();</span>
<span class="line-added">2900 </span>
<span class="line-added">2901   phase_times()-&gt;record_start_new_cset_time_ms((os::elapsedTime() - start) * 1000.0);</span>
<span class="line-added">2902 }</span>
<span class="line-added">2903 </span>
<span class="line-added">2904 void G1CollectedHeap::calculate_collection_set(G1EvacuationInfo&amp; evacuation_info, double target_pause_time_ms) {</span>
<span class="line-added">2905 </span>
<span class="line-added">2906   _collection_set.finalize_initial_collection_set(target_pause_time_ms, &amp;_survivor);</span>
<span class="line-added">2907   evacuation_info.set_collectionset_regions(collection_set()-&gt;region_length() +</span>
<span class="line-added">2908                                             collection_set()-&gt;optional_region_length());</span>
<span class="line-added">2909 </span>
<span class="line-added">2910   _cm-&gt;verify_no_collection_set_oops();</span>
<span class="line-added">2911 </span>
<span class="line-added">2912   if (_hr_printer.is_active()) {</span>
<span class="line-added">2913     G1PrintCollectionSetClosure cl(&amp;_hr_printer);</span>
<span class="line-added">2914     _collection_set.iterate(&amp;cl);</span>
<span class="line-added">2915     _collection_set.iterate_optional(&amp;cl);</span>
<span class="line-added">2916   }</span>
<span class="line-added">2917 }</span>
<span class="line-added">2918 </span>
<span class="line-added">2919 G1HeapVerifier::G1VerifyType G1CollectedHeap::young_collection_verify_type() const {</span>
<span class="line-added">2920   if (collector_state()-&gt;in_initial_mark_gc()) {</span>
<span class="line-added">2921     return G1HeapVerifier::G1VerifyConcurrentStart;</span>
<span class="line-added">2922   } else if (collector_state()-&gt;in_young_only_phase()) {</span>
<span class="line-added">2923     return G1HeapVerifier::G1VerifyYoungNormal;</span>
<span class="line-added">2924   } else {</span>
<span class="line-added">2925     return G1HeapVerifier::G1VerifyMixed;</span>
<span class="line-added">2926   }</span>
<span class="line-added">2927 }</span>
<span class="line-added">2928 </span>
<span class="line-added">2929 void G1CollectedHeap::verify_before_young_collection(G1HeapVerifier::G1VerifyType type) {</span>
<span class="line-added">2930   if (VerifyRememberedSets) {</span>
<span class="line-added">2931     log_info(gc, verify)(&quot;[Verifying RemSets before GC]&quot;);</span>
<span class="line-added">2932     VerifyRegionRemSetClosure v_cl;</span>
<span class="line-added">2933     heap_region_iterate(&amp;v_cl);</span>
<span class="line-added">2934   }</span>
<span class="line-added">2935   _verifier-&gt;verify_before_gc(type);</span>
<span class="line-added">2936   _verifier-&gt;check_bitmaps(&quot;GC Start&quot;);</span>
<span class="line-added">2937   verify_numa_regions(&quot;GC Start&quot;);</span>
<span class="line-added">2938 }</span>
<span class="line-added">2939 </span>
<span class="line-added">2940 void G1CollectedHeap::verify_after_young_collection(G1HeapVerifier::G1VerifyType type) {</span>
<span class="line-added">2941   if (VerifyRememberedSets) {</span>
<span class="line-added">2942     log_info(gc, verify)(&quot;[Verifying RemSets after GC]&quot;);</span>
<span class="line-added">2943     VerifyRegionRemSetClosure v_cl;</span>
<span class="line-added">2944     heap_region_iterate(&amp;v_cl);</span>
<span class="line-added">2945   }</span>
<span class="line-added">2946   _verifier-&gt;verify_after_gc(type);</span>
<span class="line-added">2947   _verifier-&gt;check_bitmaps(&quot;GC End&quot;);</span>
<span class="line-added">2948   verify_numa_regions(&quot;GC End&quot;);</span>
<span class="line-added">2949 }</span>
<span class="line-added">2950 </span>
<span class="line-added">2951 void G1CollectedHeap::expand_heap_after_young_collection(){</span>
<span class="line-added">2952   size_t expand_bytes = _heap_sizing_policy-&gt;expansion_amount();</span>
<span class="line-added">2953   if (expand_bytes &gt; 0) {</span>
<span class="line-added">2954     // No need for an ergo logging here,</span>
<span class="line-added">2955     // expansion_amount() does this when it returns a value &gt; 0.</span>
<span class="line-added">2956     double expand_ms;</span>
<span class="line-added">2957     if (!expand(expand_bytes, _workers, &amp;expand_ms)) {</span>
<span class="line-added">2958       // We failed to expand the heap. Cannot do anything about it.</span>
<span class="line-added">2959     }</span>
<span class="line-added">2960     phase_times()-&gt;record_expand_heap_time(expand_ms);</span>
<span class="line-added">2961   }</span>
<span class="line-added">2962 }</span>
<span class="line-added">2963 </span>
<span class="line-added">2964 const char* G1CollectedHeap::young_gc_name() const {</span>
<span class="line-added">2965   if (collector_state()-&gt;in_initial_mark_gc()) {</span>
<span class="line-added">2966     return &quot;Pause Young (Concurrent Start)&quot;;</span>
<span class="line-added">2967   } else if (collector_state()-&gt;in_young_only_phase()) {</span>
<span class="line-added">2968     if (collector_state()-&gt;in_young_gc_before_mixed()) {</span>
<span class="line-added">2969       return &quot;Pause Young (Prepare Mixed)&quot;;</span>
<span class="line-added">2970     } else {</span>
<span class="line-added">2971       return &quot;Pause Young (Normal)&quot;;</span>
<span class="line-added">2972     }</span>
<span class="line-added">2973   } else {</span>
<span class="line-added">2974     return &quot;Pause Young (Mixed)&quot;;</span>
<span class="line-added">2975   }</span>
2976 }
2977 
<span class="line-modified">2978 bool G1CollectedHeap::do_collection_pause_at_safepoint(double target_pause_time_ms) {</span>

2979   assert_at_safepoint_on_vm_thread();
2980   guarantee(!is_gc_active(), &quot;collection is not reentrant&quot;);
2981 
2982   if (GCLocker::check_active_before_gc()) {
2983     return false;
2984   }
2985 
<span class="line-modified">2986   do_collection_pause_at_safepoint_helper(target_pause_time_ms);</span>
<span class="line-added">2987   if (should_upgrade_to_full_gc(gc_cause())) {</span>
<span class="line-added">2988     log_info(gc, ergo)(&quot;Attempting maximally compacting collection&quot;);</span>
<span class="line-added">2989     bool result = do_full_collection(false /* explicit gc */,</span>
<span class="line-added">2990                                      true /* clear_all_soft_refs */);</span>
<span class="line-added">2991     // do_full_collection only fails if blocked by GC locker, but</span>
<span class="line-added">2992     // we&#39;ve already checked for that above.</span>
<span class="line-added">2993     assert(result, &quot;invariant&quot;);</span>
<span class="line-added">2994   }</span>
<span class="line-added">2995   return true;</span>
<span class="line-added">2996 }</span>
2997 
<span class="line-added">2998 void G1CollectedHeap::do_collection_pause_at_safepoint_helper(double target_pause_time_ms) {</span>
2999   GCIdMark gc_id_mark;

3000 
3001   SvcGCMarker sgcm(SvcGCMarker::MINOR);
3002   ResourceMark rm;
3003 
3004   policy()-&gt;note_gc_start();
3005 
<span class="line-added">3006   _gc_timer_stw-&gt;register_gc_start();</span>
<span class="line-added">3007   _gc_tracer_stw-&gt;report_gc_start(gc_cause(), _gc_timer_stw-&gt;gc_start());</span>
<span class="line-added">3008 </span>
3009   wait_for_root_region_scanning();
3010 
3011   print_heap_before_gc();
3012   print_heap_regions();
3013   trace_heap_before_gc(_gc_tracer_stw);
3014 
3015   _verifier-&gt;verify_region_sets_optional();
3016   _verifier-&gt;verify_dirty_young_regions();
3017 
3018   // We should not be doing initial mark unless the conc mark thread is running
3019   if (!_cm_thread-&gt;should_terminate()) {
3020     // This call will decide whether this pause is an initial-mark
3021     // pause. If it is, in_initial_mark_gc() will return true
3022     // for the duration of this pause.
3023     policy()-&gt;decide_on_conc_mark_initiation();
3024   }
3025 
3026   // We do not allow initial-mark to be piggy-backed on a mixed GC.
3027   assert(!collector_state()-&gt;in_initial_mark_gc() ||
<span class="line-modified">3028          collector_state()-&gt;in_young_only_phase(), &quot;sanity&quot;);</span>

3029   // We also do not allow mixed GCs during marking.
3030   assert(!collector_state()-&gt;mark_or_rebuild_in_progress() || collector_state()-&gt;in_young_only_phase(), &quot;sanity&quot;);
3031 
3032   // Record whether this pause is an initial mark. When the current
3033   // thread has completed its logging output and it&#39;s safe to signal
3034   // the CM thread, the flag&#39;s value in the policy has been reset.
3035   bool should_start_conc_mark = collector_state()-&gt;in_initial_mark_gc();
<span class="line-added">3036   if (should_start_conc_mark) {</span>
<span class="line-added">3037     _cm-&gt;gc_tracer_cm()-&gt;set_gc_cause(gc_cause());</span>
<span class="line-added">3038   }</span>
3039 
3040   // Inner scope for scope based logging, timers, and stats collection
3041   {
3042     G1EvacuationInfo evacuation_info;
3043 







3044     _gc_tracer_stw-&gt;report_yc_type(collector_state()-&gt;yc_type());
3045 
3046     GCTraceCPUTime tcpu;
3047 
<span class="line-modified">3048     GCTraceTime(Info, gc) tm(young_gc_name(), NULL, gc_cause(), true);</span>
















3049 
3050     uint active_workers = WorkerPolicy::calc_active_workers(workers()-&gt;total_workers(),
3051                                                             workers()-&gt;active_workers(),
3052                                                             Threads::number_of_non_daemon_threads());
3053     active_workers = workers()-&gt;update_active_workers(active_workers);
3054     log_info(gc,task)(&quot;Using %u workers of %u for evacuation&quot;, active_workers, workers()-&gt;total_workers());
3055 
3056     G1MonitoringScope ms(g1mm(),
3057                          false /* full_gc */,
3058                          collector_state()-&gt;yc_type() == Mixed /* all_memory_pools_affected */);
3059 
3060     G1HeapTransition heap_transition(this);

3061 
<span class="line-modified">3062     {</span>




3063       IsGCActiveMark x;
3064 
3065       gc_prologue(false);
3066 
<span class="line-modified">3067       G1HeapVerifier::G1VerifyType verify_type = young_collection_verify_type();</span>
<span class="line-modified">3068       verify_before_young_collection(verify_type);</span>











3069 
<span class="line-modified">3070       {</span>
<span class="line-modified">3071         // The elapsed time induced by the start time below deliberately elides</span>
<span class="line-modified">3072         // the possible verification above.</span>
<span class="line-added">3073         double sample_start_time_sec = os::elapsedTime();</span>
3074 
<span class="line-modified">3075         // Please see comment in g1CollectedHeap.hpp and</span>
<span class="line-modified">3076         // G1CollectedHeap::ref_processing_init() to see how</span>
<span class="line-added">3077         // reference processing currently works in G1.</span>
<span class="line-added">3078         _ref_processor_stw-&gt;enable_discovery();</span>
3079 

3080         // We want to temporarily turn off discovery by the
3081         // CM ref processor, if necessary, and turn it back on
3082         // on again later if we do. Using a scoped
3083         // NoRefDiscovery object will do this.
3084         NoRefDiscovery no_cm_discovery(_ref_processor_cm);
3085 














3086         policy()-&gt;record_collection_pause_start(sample_start_time_sec);
3087 
<span class="line-modified">3088         // Forget the current allocation region (we might even choose it to be part</span>
<span class="line-modified">3089         // of the collection set!).</span>
<span class="line-modified">3090         _allocator-&gt;release_mutator_alloc_regions();</span>

















3091 
<span class="line-modified">3092         calculate_collection_set(evacuation_info, target_pause_time_ms);</span>

3093 
<span class="line-added">3094         G1RedirtyCardsQueueSet rdcqs(G1BarrierSet::dirty_card_queue_set().allocator());</span>
3095         G1ParScanThreadStateSet per_thread_states(this,
<span class="line-added">3096                                                   &amp;rdcqs,</span>
3097                                                   workers()-&gt;active_workers(),
3098                                                   collection_set()-&gt;young_region_length(),
3099                                                   collection_set()-&gt;optional_region_length());
<span class="line-modified">3100         pre_evacuate_collection_set(evacuation_info, &amp;per_thread_states);</span>
3101 
3102         // Actually do the work...
<span class="line-modified">3103         evacuate_initial_collection_set(&amp;per_thread_states);</span>



3104 
<span class="line-modified">3105         if (_collection_set.optional_region_length() != 0) {</span>
<span class="line-modified">3106           evacuate_optional_collection_set(&amp;per_thread_states);</span>
<span class="line-added">3107         }</span>
<span class="line-added">3108         post_evacuate_collection_set(evacuation_info, &amp;rdcqs, &amp;per_thread_states);</span>
3109 
<span class="line-modified">3110         start_new_collection_set();</span>
3111 

3112         _survivor_evac_stats.adjust_desired_plab_sz();
3113         _old_evac_stats.adjust_desired_plab_sz();
3114 
<span class="line-modified">3115         if (should_start_conc_mark) {</span>























3116           // We have to do this before we notify the CM threads that
3117           // they can start working to make sure that all the
3118           // appropriate initialization is done on the CM object.
3119           concurrent_mark()-&gt;post_initial_mark();
3120           // Note that we don&#39;t actually trigger the CM thread at
3121           // this point. We do that later when we&#39;re sure that
3122           // the current thread has completed its logging output.
3123         }
3124 
3125         allocate_dummy_regions();
3126 
<span class="line-modified">3127         _allocator-&gt;init_mutator_alloc_regions();</span>














3128 
<span class="line-modified">3129         expand_heap_after_young_collection();</span>


3130 



3131         double sample_end_time_sec = os::elapsedTime();
3132         double pause_time_ms = (sample_end_time_sec - sample_start_time_sec) * MILLIUNITS;
<span class="line-modified">3133         policy()-&gt;record_collection_pause_end(pause_time_ms);</span>


















3134       }
3135 
<span class="line-modified">3136       verify_after_young_collection(verify_type);</span>


3137 
3138       gc_epilogue(false);
3139     }
3140 
3141     // Print the remainder of the GC log output.
3142     if (evacuation_failed()) {
3143       log_info(gc)(&quot;To-space exhausted&quot;);
3144     }
3145 
3146     policy()-&gt;print_phases();
3147     heap_transition.print();
3148 





3149     _hrm-&gt;verify_optional();
3150     _verifier-&gt;verify_region_sets_optional();
3151 
3152     TASKQUEUE_STATS_ONLY(print_taskqueue_stats());
3153     TASKQUEUE_STATS_ONLY(reset_taskqueue_stats());
3154 
3155     print_heap_after_gc();
3156     print_heap_regions();
3157     trace_heap_after_gc(_gc_tracer_stw);
3158 
3159     // We must call G1MonitoringSupport::update_sizes() in the same scoping level
3160     // as an active TraceMemoryManagerStats object (i.e. before the destructor for the
3161     // TraceMemoryManagerStats is called) so that the G1 memory pools are updated
3162     // before any GC notifications are raised.
3163     g1mm()-&gt;update_sizes();
3164 
3165     _gc_tracer_stw-&gt;report_evacuation_info(&amp;evacuation_info);
3166     _gc_tracer_stw-&gt;report_tenuring_threshold(_policy-&gt;tenuring_threshold());
3167     _gc_timer_stw-&gt;register_gc_end();
3168     _gc_tracer_stw-&gt;report_gc_end(_gc_timer_stw-&gt;gc_end(), _gc_timer_stw-&gt;time_partitions());
3169   }
3170   // It should now be safe to tell the concurrent mark thread to start
3171   // without its logging output interfering with the logging output
3172   // that came from the pause.
3173 
3174   if (should_start_conc_mark) {
<span class="line-modified">3175     // CAUTION: after the doConcurrentMark() call below, the concurrent marking</span>
<span class="line-modified">3176     // thread(s) could be running concurrently with us. Make sure that anything</span>
<span class="line-modified">3177     // after this point does not assume that we are the only GC thread running.</span>
<span class="line-modified">3178     // Note: of course, the actual marking work will not start until the safepoint</span>
<span class="line-modified">3179     // itself is released in SuspendibleThreadSet::desynchronize().</span>


3180     do_concurrent_mark();
3181   }


3182 }
3183 
<span class="line-modified">3184 void G1CollectedHeap::remove_self_forwarding_pointers(G1RedirtyCardsQueueSet* rdcqs) {</span>
<span class="line-modified">3185   G1ParRemoveSelfForwardPtrsTask rsfp_task(rdcqs);</span>
3186   workers()-&gt;run_task(&amp;rsfp_task);
3187 }
3188 
<span class="line-modified">3189 void G1CollectedHeap::restore_after_evac_failure(G1RedirtyCardsQueueSet* rdcqs) {</span>
3190   double remove_self_forwards_start = os::elapsedTime();
3191 
<span class="line-modified">3192   remove_self_forwarding_pointers(rdcqs);</span>
<span class="line-modified">3193   _preserved_marks_set.restore(workers());</span>

3194 
3195   phase_times()-&gt;record_evac_fail_remove_self_forwards((os::elapsedTime() - remove_self_forwards_start) * 1000.0);
3196 }
3197 
<span class="line-modified">3198 void G1CollectedHeap::preserve_mark_during_evac_failure(uint worker_id, oop obj, markWord m) {</span>
3199   if (!_evacuation_failed) {
3200     _evacuation_failed = true;
3201   }
3202 
3203   _evacuation_failed_info_array[worker_id].register_copy_failure(obj-&gt;size());
3204   _preserved_marks_set.get(worker_id)-&gt;push_if_necessary(obj, m);
3205 }
3206 
3207 bool G1ParEvacuateFollowersClosure::offer_termination() {
3208   EventGCPhaseParallel event;
3209   G1ParScanThreadState* const pss = par_scan_state();
3210   start_term_time();
3211   const bool res = terminator()-&gt;offer_termination();
3212   end_term_time();
3213   event.commit(GCId::current(), pss-&gt;worker_id(), G1GCPhaseTimes::phase_name(G1GCPhaseTimes::Termination));
3214   return res;
3215 }
3216 
3217 void G1ParEvacuateFollowersClosure::do_void() {
3218   EventGCPhaseParallel event;
3219   G1ParScanThreadState* const pss = par_scan_state();
3220   pss-&gt;trim_queue();
3221   event.commit(GCId::current(), pss-&gt;worker_id(), G1GCPhaseTimes::phase_name(_phase));
3222   do {
3223     EventGCPhaseParallel event;
3224     pss-&gt;steal_and_trim_queue(queues());
3225     event.commit(GCId::current(), pss-&gt;worker_id(), G1GCPhaseTimes::phase_name(_phase));
3226   } while (!offer_termination());
3227 }
3228 
















































































3229 void G1CollectedHeap::complete_cleaning(BoolObjectClosure* is_alive,
3230                                         bool class_unloading_occurred) {
3231   uint num_workers = workers()-&gt;active_workers();
<span class="line-modified">3232   G1ParallelCleaningTask unlink_task(is_alive, num_workers, class_unloading_occurred, false);</span>
3233   workers()-&gt;run_task(&amp;unlink_task);
3234 }
3235 
3236 // Clean string dedup data structures.
3237 // Ideally we would prefer to use a StringDedupCleaningTask here, but we want to
3238 // record the durations of the phases. Hence the almost-copy.
3239 class G1StringDedupCleaningTask : public AbstractGangTask {
3240   BoolObjectClosure* _is_alive;
3241   OopClosure* _keep_alive;
3242   G1GCPhaseTimes* _phase_times;
3243 
3244 public:
3245   G1StringDedupCleaningTask(BoolObjectClosure* is_alive,
3246                             OopClosure* keep_alive,
3247                             G1GCPhaseTimes* phase_times) :
3248     AbstractGangTask(&quot;Partial Cleaning Task&quot;),
3249     _is_alive(is_alive),
3250     _keep_alive(keep_alive),
3251     _phase_times(phase_times)
3252   {
</pre>
<hr />
<pre>
3263     {
3264       G1GCParPhaseTimesTracker x(_phase_times, G1GCPhaseTimes::StringDedupQueueFixup, worker_id);
3265       StringDedupQueue::unlink_or_oops_do(&amp;cl);
3266     }
3267     {
3268       G1GCParPhaseTimesTracker x(_phase_times, G1GCPhaseTimes::StringDedupTableFixup, worker_id);
3269       StringDedupTable::unlink_or_oops_do(&amp;cl, worker_id);
3270     }
3271   }
3272 };
3273 
3274 void G1CollectedHeap::string_dedup_cleaning(BoolObjectClosure* is_alive,
3275                                             OopClosure* keep_alive,
3276                                             G1GCPhaseTimes* phase_times) {
3277   G1StringDedupCleaningTask cl(is_alive, keep_alive, phase_times);
3278   workers()-&gt;run_task(&amp;cl);
3279 }
3280 
3281 class G1RedirtyLoggedCardsTask : public AbstractGangTask {
3282  private:
<span class="line-modified">3283   G1RedirtyCardsQueueSet* _qset;</span>
3284   G1CollectedHeap* _g1h;
<span class="line-added">3285   BufferNode* volatile _nodes;</span>
<span class="line-added">3286 </span>
<span class="line-added">3287   void par_apply(RedirtyLoggedCardTableEntryClosure* cl, uint worker_id) {</span>
<span class="line-added">3288     size_t buffer_size = _qset-&gt;buffer_size();</span>
<span class="line-added">3289     BufferNode* next = Atomic::load(&amp;_nodes);</span>
<span class="line-added">3290     while (next != NULL) {</span>
<span class="line-added">3291       BufferNode* node = next;</span>
<span class="line-added">3292       next = Atomic::cmpxchg(&amp;_nodes, node, node-&gt;next());</span>
<span class="line-added">3293       if (next == node) {</span>
<span class="line-added">3294         cl-&gt;apply_to_buffer(node, buffer_size, worker_id);</span>
<span class="line-added">3295         next = node-&gt;next();</span>
<span class="line-added">3296       }</span>
<span class="line-added">3297     }</span>
<span class="line-added">3298   }</span>
<span class="line-added">3299 </span>
3300  public:
<span class="line-modified">3301   G1RedirtyLoggedCardsTask(G1RedirtyCardsQueueSet* qset, G1CollectedHeap* g1h) :</span>
<span class="line-modified">3302     AbstractGangTask(&quot;Redirty Cards&quot;),</span>
<span class="line-added">3303     _qset(qset), _g1h(g1h), _nodes(qset-&gt;all_completed_buffers()) { }</span>
3304 
3305   virtual void work(uint worker_id) {
3306     G1GCPhaseTimes* p = _g1h-&gt;phase_times();
3307     G1GCParPhaseTimesTracker x(p, G1GCPhaseTimes::RedirtyCards, worker_id);
3308 
3309     RedirtyLoggedCardTableEntryClosure cl(_g1h);
<span class="line-modified">3310     par_apply(&amp;cl, worker_id);</span>
3311 
3312     p-&gt;record_thread_work_item(G1GCPhaseTimes::RedirtyCards, worker_id, cl.num_dirtied());
3313   }
3314 };
3315 
<span class="line-modified">3316 void G1CollectedHeap::redirty_logged_cards(G1RedirtyCardsQueueSet* rdcqs) {</span>
3317   double redirty_logged_cards_start = os::elapsedTime();
3318 
<span class="line-modified">3319   G1RedirtyLoggedCardsTask redirty_task(rdcqs, this);</span>

3320   workers()-&gt;run_task(&amp;redirty_task);
3321 
3322   G1DirtyCardQueueSet&amp; dcq = G1BarrierSet::dirty_card_queue_set();
<span class="line-modified">3323   dcq.merge_bufferlists(rdcqs);</span>

3324 
3325   phase_times()-&gt;record_redirty_logged_cards_time_ms((os::elapsedTime() - redirty_logged_cards_start) * 1000.0);
3326 }
3327 
3328 // Weak Reference Processing support
3329 
3330 bool G1STWIsAliveClosure::do_object_b(oop p) {
3331   // An object is reachable if it is outside the collection set,
3332   // or is inside and copied.
3333   return !_g1h-&gt;is_in_cset(p) || p-&gt;is_forwarded();
3334 }
3335 
3336 bool G1STWSubjectToDiscoveryClosure::do_object_b(oop obj) {
3337   assert(obj != NULL, &quot;must not be NULL&quot;);
3338   assert(_g1h-&gt;is_in_reserved(obj), &quot;Trying to discover obj &quot; PTR_FORMAT &quot; not in heap&quot;, p2i(obj));
3339   // The areas the CM and STW ref processor manage must be disjoint. The is_in_cset() below
3340   // may falsely indicate that this is not the case here: however the collection set only
3341   // contains old regions when concurrent mark is not running.
3342   return _g1h-&gt;is_in_cset(obj) || _g1h-&gt;heap_region_containing(obj)-&gt;is_survivor();
3343 }
3344 
3345 // Non Copying Keep Alive closure
3346 class G1KeepAliveClosure: public OopClosure {
3347   G1CollectedHeap*_g1h;
3348 public:
3349   G1KeepAliveClosure(G1CollectedHeap* g1h) :_g1h(g1h) {}
3350   void do_oop(narrowOop* p) { guarantee(false, &quot;Not needed&quot;); }
3351   void do_oop(oop* p) {
3352     oop obj = *p;
3353     assert(obj != NULL, &quot;the caller should have filtered out NULL values&quot;);
3354 
<span class="line-modified">3355     const G1HeapRegionAttr region_attr =_g1h-&gt;region_attr(obj);</span>
<span class="line-modified">3356     if (!region_attr.is_in_cset_or_humongous()) {</span>
3357       return;
3358     }
<span class="line-modified">3359     if (region_attr.is_in_cset()) {</span>
3360       assert( obj-&gt;is_forwarded(), &quot;invariant&quot; );
3361       *p = obj-&gt;forwardee();
3362     } else {
3363       assert(!obj-&gt;is_forwarded(), &quot;invariant&quot; );
<span class="line-modified">3364       assert(region_attr.is_humongous(),</span>
<span class="line-modified">3365              &quot;Only allowed G1HeapRegionAttr state is IsHumongous, but is %d&quot;, region_attr.type());</span>
3366      _g1h-&gt;set_humongous_is_live(obj);
3367     }
3368   }
3369 };
3370 
3371 // Copying Keep Alive closure - can be called from both
3372 // serial and parallel code as long as different worker
3373 // threads utilize different G1ParScanThreadState instances
3374 // and different queues.
3375 
3376 class G1CopyingKeepAliveClosure: public OopClosure {
3377   G1CollectedHeap*         _g1h;
3378   G1ParScanThreadState*    _par_scan_state;
3379 
3380 public:
3381   G1CopyingKeepAliveClosure(G1CollectedHeap* g1h,
3382                             G1ParScanThreadState* pss):
3383     _g1h(g1h),
3384     _par_scan_state(pss)
3385   {}
</pre>
<hr />
<pre>
3451     _g1h(g1h),
3452     _pss(per_thread_states),
3453     _queues(task_queues),
3454     _workers(workers)
3455   {
3456     g1h-&gt;ref_processor_stw()-&gt;set_active_mt_degree(workers-&gt;active_workers());
3457   }
3458 
3459   // Executes the given task using concurrent marking worker threads.
3460   virtual void execute(ProcessTask&amp; task, uint ergo_workers);
3461 };
3462 
3463 // Gang task for possibly parallel reference processing
3464 
3465 class G1STWRefProcTaskProxy: public AbstractGangTask {
3466   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
3467   ProcessTask&amp;     _proc_task;
3468   G1CollectedHeap* _g1h;
3469   G1ParScanThreadStateSet* _pss;
3470   RefToScanQueueSet* _task_queues;
<span class="line-modified">3471   TaskTerminator* _terminator;</span>
3472 
3473 public:
3474   G1STWRefProcTaskProxy(ProcessTask&amp; proc_task,
3475                         G1CollectedHeap* g1h,
3476                         G1ParScanThreadStateSet* per_thread_states,
3477                         RefToScanQueueSet *task_queues,
<span class="line-modified">3478                         TaskTerminator* terminator) :</span>
3479     AbstractGangTask(&quot;Process reference objects in parallel&quot;),
3480     _proc_task(proc_task),
3481     _g1h(g1h),
3482     _pss(per_thread_states),
3483     _task_queues(task_queues),
3484     _terminator(terminator)
3485   {}
3486 
3487   virtual void work(uint worker_id) {
3488     // The reference processing task executed by a single worker.
3489     ResourceMark rm;
3490     HandleMark   hm;
3491 
3492     G1STWIsAliveClosure is_alive(_g1h);
3493 
3494     G1ParScanThreadState* pss = _pss-&gt;state_for_worker(worker_id);
3495     pss-&gt;set_ref_discoverer(NULL);
3496 
3497     // Keep alive closure.
3498     G1CopyingKeepAliveClosure keep_alive(_g1h, pss);
</pre>
<hr />
<pre>
3503     // Call the reference processing task&#39;s work routine.
3504     _proc_task.work(worker_id, is_alive, keep_alive, drain_queue);
3505 
3506     // Note we cannot assert that the refs array is empty here as not all
3507     // of the processing tasks (specifically phase2 - pp2_work) execute
3508     // the complete_gc closure (which ordinarily would drain the queue) so
3509     // the queue may not be empty.
3510   }
3511 };
3512 
3513 // Driver routine for parallel reference processing.
3514 // Creates an instance of the ref processing gang
3515 // task and has the worker threads execute it.
3516 void G1STWRefProcTaskExecutor::execute(ProcessTask&amp; proc_task, uint ergo_workers) {
3517   assert(_workers != NULL, &quot;Need parallel worker threads.&quot;);
3518 
3519   assert(_workers-&gt;active_workers() &gt;= ergo_workers,
3520          &quot;Ergonomically chosen workers (%u) should be less than or equal to active workers (%u)&quot;,
3521          ergo_workers, _workers-&gt;active_workers());
3522   TaskTerminator terminator(ergo_workers, _queues);
<span class="line-modified">3523   G1STWRefProcTaskProxy proc_task_proxy(proc_task, _g1h, _pss, _queues, &amp;terminator);</span>
3524 
3525   _workers-&gt;run_task(&amp;proc_task_proxy, ergo_workers);
3526 }
3527 
3528 // End of weak reference support closures
3529 
3530 void G1CollectedHeap::process_discovered_references(G1ParScanThreadStateSet* per_thread_states) {
3531   double ref_proc_start = os::elapsedTime();
3532 
3533   ReferenceProcessor* rp = _ref_processor_stw;
3534   assert(rp-&gt;discovery_enabled(), &quot;should have been enabled&quot;);
3535 
3536   // Closure to test whether a referent is alive.
3537   G1STWIsAliveClosure is_alive(this);
3538 
3539   // Even when parallel reference processing is enabled, the processing
3540   // of JNI refs is serial and performed serially by the current thread
3541   // rather than by a worker. The following PSS will be used for processing
3542   // JNI refs.
3543 
</pre>
<hr />
<pre>
3571     // Parallel reference processing
3572     assert(no_of_gc_workers &lt;= rp-&gt;max_num_queues(),
3573            &quot;Mismatch between the number of GC workers %u and the maximum number of Reference process queues %u&quot;,
3574            no_of_gc_workers,  rp-&gt;max_num_queues());
3575 
3576     G1STWRefProcTaskExecutor par_task_executor(this, per_thread_states, workers(), _task_queues);
3577     stats = rp-&gt;process_discovered_references(&amp;is_alive,
3578                                               &amp;keep_alive,
3579                                               &amp;drain_queue,
3580                                               &amp;par_task_executor,
3581                                               pt);
3582   }
3583 
3584   _gc_tracer_stw-&gt;report_gc_reference_stats(stats);
3585 
3586   // We have completed copying any necessary live referent objects.
3587   assert(pss-&gt;queue_is_empty(), &quot;both queue and overflow should be empty&quot;);
3588 
3589   make_pending_list_reachable();
3590 
<span class="line-added">3591   assert(!rp-&gt;discovery_enabled(), &quot;Postcondition&quot;);</span>
3592   rp-&gt;verify_no_references_recorded();
3593 
3594   double ref_proc_time = os::elapsedTime() - ref_proc_start;
3595   phase_times()-&gt;record_ref_proc_time(ref_proc_time * 1000.0);
3596 }
3597 
3598 void G1CollectedHeap::make_pending_list_reachable() {
3599   if (collector_state()-&gt;in_initial_mark_gc()) {
3600     oop pll_head = Universe::reference_pending_list();
3601     if (pll_head != NULL) {
3602       // Any valid worker id is fine here as we are in the VM thread and single-threaded.
3603       _cm-&gt;mark_in_next_bitmap(0 /* worker_id */, pll_head);
3604     }
3605   }
3606 }
3607 
3608 void G1CollectedHeap::merge_per_thread_state_info(G1ParScanThreadStateSet* per_thread_states) {
<span class="line-modified">3609   Ticks start = Ticks::now();</span>
3610   per_thread_states-&gt;flush();
<span class="line-modified">3611   phase_times()-&gt;record_or_add_time_secs(G1GCPhaseTimes::MergePSS, 0 /* worker_id */, (Ticks::now() - start).seconds());</span>
3612 }
3613 
<span class="line-modified">3614 class G1PrepareEvacuationTask : public AbstractGangTask {</span>
<span class="line-added">3615   class G1PrepareRegionsClosure : public HeapRegionClosure {</span>
<span class="line-added">3616     G1CollectedHeap* _g1h;</span>
<span class="line-added">3617     G1PrepareEvacuationTask* _parent_task;</span>
<span class="line-added">3618     size_t _worker_humongous_total;</span>
<span class="line-added">3619     size_t _worker_humongous_candidates;</span>
<span class="line-added">3620 </span>
<span class="line-added">3621     bool humongous_region_is_candidate(HeapRegion* region) const {</span>
<span class="line-added">3622       assert(region-&gt;is_starts_humongous(), &quot;Must start a humongous object&quot;);</span>
<span class="line-added">3623 </span>
<span class="line-added">3624       oop obj = oop(region-&gt;bottom());</span>
<span class="line-added">3625 </span>
<span class="line-added">3626       // Dead objects cannot be eager reclaim candidates. Due to class</span>
<span class="line-added">3627       // unloading it is unsafe to query their classes so we return early.</span>
<span class="line-added">3628       if (_g1h-&gt;is_obj_dead(obj, region)) {</span>
<span class="line-added">3629         return false;</span>
<span class="line-added">3630       }</span>
<span class="line-added">3631 </span>
<span class="line-added">3632       // If we do not have a complete remembered set for the region, then we can</span>
<span class="line-added">3633       // not be sure that we have all references to it.</span>
<span class="line-added">3634       if (!region-&gt;rem_set()-&gt;is_complete()) {</span>
<span class="line-added">3635         return false;</span>
<span class="line-added">3636       }</span>
<span class="line-added">3637       // Candidate selection must satisfy the following constraints</span>
<span class="line-added">3638       // while concurrent marking is in progress:</span>
<span class="line-added">3639       //</span>
<span class="line-added">3640       // * In order to maintain SATB invariants, an object must not be</span>
<span class="line-added">3641       // reclaimed if it was allocated before the start of marking and</span>
<span class="line-added">3642       // has not had its references scanned.  Such an object must have</span>
<span class="line-added">3643       // its references (including type metadata) scanned to ensure no</span>
<span class="line-added">3644       // live objects are missed by the marking process.  Objects</span>
<span class="line-added">3645       // allocated after the start of concurrent marking don&#39;t need to</span>
<span class="line-added">3646       // be scanned.</span>
<span class="line-added">3647       //</span>
<span class="line-added">3648       // * An object must not be reclaimed if it is on the concurrent</span>
<span class="line-added">3649       // mark stack.  Objects allocated after the start of concurrent</span>
<span class="line-added">3650       // marking are never pushed on the mark stack.</span>
<span class="line-added">3651       //</span>
<span class="line-added">3652       // Nominating only objects allocated after the start of concurrent</span>
<span class="line-added">3653       // marking is sufficient to meet both constraints.  This may miss</span>
<span class="line-added">3654       // some objects that satisfy the constraints, but the marking data</span>
<span class="line-added">3655       // structures don&#39;t support efficiently performing the needed</span>
<span class="line-added">3656       // additional tests or scrubbing of the mark stack.</span>
<span class="line-added">3657       //</span>
<span class="line-added">3658       // However, we presently only nominate is_typeArray() objects.</span>
<span class="line-added">3659       // A humongous object containing references induces remembered</span>
<span class="line-added">3660       // set entries on other regions.  In order to reclaim such an</span>
<span class="line-added">3661       // object, those remembered sets would need to be cleaned up.</span>
<span class="line-added">3662       //</span>
<span class="line-added">3663       // We also treat is_typeArray() objects specially, allowing them</span>
<span class="line-added">3664       // to be reclaimed even if allocated before the start of</span>
<span class="line-added">3665       // concurrent mark.  For this we rely on mark stack insertion to</span>
<span class="line-added">3666       // exclude is_typeArray() objects, preventing reclaiming an object</span>
<span class="line-added">3667       // that is in the mark stack.  We also rely on the metadata for</span>
<span class="line-added">3668       // such objects to be built-in and so ensured to be kept live.</span>
<span class="line-added">3669       // Frequent allocation and drop of large binary blobs is an</span>
<span class="line-added">3670       // important use case for eager reclaim, and this special handling</span>
<span class="line-added">3671       // may reduce needed headroom.</span>
<span class="line-added">3672 </span>
<span class="line-added">3673       return obj-&gt;is_typeArray() &amp;&amp;</span>
<span class="line-added">3674              _g1h-&gt;is_potential_eager_reclaim_candidate(region);</span>
<span class="line-added">3675     }</span>
<span class="line-added">3676 </span>
<span class="line-added">3677   public:</span>
<span class="line-added">3678     G1PrepareRegionsClosure(G1CollectedHeap* g1h, G1PrepareEvacuationTask* parent_task) :</span>
<span class="line-added">3679       _g1h(g1h),</span>
<span class="line-added">3680       _parent_task(parent_task),</span>
<span class="line-added">3681       _worker_humongous_total(0),</span>
<span class="line-added">3682       _worker_humongous_candidates(0) { }</span>
<span class="line-added">3683 </span>
<span class="line-added">3684     ~G1PrepareRegionsClosure() {</span>
<span class="line-added">3685       _parent_task-&gt;add_humongous_candidates(_worker_humongous_candidates);</span>
<span class="line-added">3686       _parent_task-&gt;add_humongous_total(_worker_humongous_total);</span>
<span class="line-added">3687     }</span>
<span class="line-added">3688 </span>
<span class="line-added">3689     virtual bool do_heap_region(HeapRegion* hr) {</span>
<span class="line-added">3690       // First prepare the region for scanning</span>
<span class="line-added">3691       _g1h-&gt;rem_set()-&gt;prepare_region_for_scan(hr);</span>
<span class="line-added">3692 </span>
<span class="line-added">3693       // Now check if region is a humongous candidate</span>
<span class="line-added">3694       if (!hr-&gt;is_starts_humongous()) {</span>
<span class="line-added">3695         _g1h-&gt;register_region_with_region_attr(hr);</span>
<span class="line-added">3696         return false;</span>
<span class="line-added">3697       }</span>
<span class="line-added">3698 </span>
<span class="line-added">3699       uint index = hr-&gt;hrm_index();</span>
<span class="line-added">3700       if (humongous_region_is_candidate(hr)) {</span>
<span class="line-added">3701         _g1h-&gt;set_humongous_reclaim_candidate(index, true);</span>
<span class="line-added">3702         _g1h-&gt;register_humongous_region_with_region_attr(index);</span>
<span class="line-added">3703         _worker_humongous_candidates++;</span>
<span class="line-added">3704         // We will later handle the remembered sets of these regions.</span>
<span class="line-added">3705       } else {</span>
<span class="line-added">3706         _g1h-&gt;set_humongous_reclaim_candidate(index, false);</span>
<span class="line-added">3707         _g1h-&gt;register_region_with_region_attr(hr);</span>
<span class="line-added">3708       }</span>
<span class="line-added">3709       _worker_humongous_total++;</span>
<span class="line-added">3710 </span>
<span class="line-added">3711       return false;</span>
<span class="line-added">3712     }</span>
<span class="line-added">3713   };</span>
<span class="line-added">3714 </span>
<span class="line-added">3715   G1CollectedHeap* _g1h;</span>
<span class="line-added">3716   HeapRegionClaimer _claimer;</span>
<span class="line-added">3717   volatile size_t _humongous_total;</span>
<span class="line-added">3718   volatile size_t _humongous_candidates;</span>
<span class="line-added">3719 public:</span>
<span class="line-added">3720   G1PrepareEvacuationTask(G1CollectedHeap* g1h) :</span>
<span class="line-added">3721     AbstractGangTask(&quot;Prepare Evacuation&quot;),</span>
<span class="line-added">3722     _g1h(g1h),</span>
<span class="line-added">3723     _claimer(_g1h-&gt;workers()-&gt;active_workers()),</span>
<span class="line-added">3724     _humongous_total(0),</span>
<span class="line-added">3725     _humongous_candidates(0) { }</span>
<span class="line-added">3726 </span>
<span class="line-added">3727   ~G1PrepareEvacuationTask() {</span>
<span class="line-added">3728     _g1h-&gt;set_has_humongous_reclaim_candidate(_humongous_candidates &gt; 0);</span>
<span class="line-added">3729   }</span>
<span class="line-added">3730 </span>
<span class="line-added">3731   void work(uint worker_id) {</span>
<span class="line-added">3732     G1PrepareRegionsClosure cl(_g1h, this);</span>
<span class="line-added">3733     _g1h-&gt;heap_region_par_iterate_from_worker_offset(&amp;cl, &amp;_claimer, worker_id);</span>
<span class="line-added">3734   }</span>
<span class="line-added">3735 </span>
<span class="line-added">3736   void add_humongous_candidates(size_t candidates) {</span>
<span class="line-added">3737     Atomic::add(&amp;_humongous_candidates, candidates);</span>
<span class="line-added">3738   }</span>
<span class="line-added">3739 </span>
<span class="line-added">3740   void add_humongous_total(size_t total) {</span>
<span class="line-added">3741     Atomic::add(&amp;_humongous_total, total);</span>
<span class="line-added">3742   }</span>
<span class="line-added">3743 </span>
<span class="line-added">3744   size_t humongous_candidates() {</span>
<span class="line-added">3745     return _humongous_candidates;</span>
<span class="line-added">3746   }</span>
<span class="line-added">3747 </span>
<span class="line-added">3748   size_t humongous_total() {</span>
<span class="line-added">3749     return _humongous_total;</span>
<span class="line-added">3750   }</span>
<span class="line-added">3751 };</span>
<span class="line-added">3752 </span>
<span class="line-added">3753 void G1CollectedHeap::pre_evacuate_collection_set(G1EvacuationInfo&amp; evacuation_info, G1ParScanThreadStateSet* per_thread_states) {</span>
<span class="line-added">3754   _bytes_used_during_gc = 0;</span>
<span class="line-added">3755 </span>
3756   _expand_heap_after_alloc_failure = true;
3757   _evacuation_failed = false;
3758 
3759   // Disable the hot card cache.
3760   _hot_card_cache-&gt;reset_hot_cache_claimed_index();
3761   _hot_card_cache-&gt;set_use_cache(false);
3762 
<span class="line-modified">3763   // Initialize the GC alloc regions.</span>
<span class="line-added">3764   _allocator-&gt;init_gc_alloc_regions(evacuation_info);</span>
<span class="line-added">3765 </span>
<span class="line-added">3766   {</span>
<span class="line-added">3767     Ticks start = Ticks::now();</span>
<span class="line-added">3768     rem_set()-&gt;prepare_for_scan_heap_roots();</span>
<span class="line-added">3769     phase_times()-&gt;record_prepare_heap_roots_time_ms((Ticks::now() - start).seconds() * 1000.0);</span>
<span class="line-added">3770   }</span>
<span class="line-added">3771 </span>
<span class="line-added">3772   {</span>
<span class="line-added">3773     G1PrepareEvacuationTask g1_prep_task(this);</span>
<span class="line-added">3774     Tickspan task_time = run_task(&amp;g1_prep_task);</span>
<span class="line-added">3775 </span>
<span class="line-added">3776     phase_times()-&gt;record_register_regions(task_time.seconds() * 1000.0,</span>
<span class="line-added">3777                                            g1_prep_task.humongous_total(),</span>
<span class="line-added">3778                                            g1_prep_task.humongous_candidates());</span>
<span class="line-added">3779   }</span>
<span class="line-added">3780 </span>
<span class="line-added">3781   assert(_verifier-&gt;check_region_attr_table(), &quot;Inconsistency in the region attributes table.&quot;);</span>
3782   _preserved_marks_set.assert_empty();
3783 
<span class="line-added">3784 #if COMPILER2_OR_JVMCI</span>
<span class="line-added">3785   DerivedPointerTable::clear();</span>
<span class="line-added">3786 #endif</span>
<span class="line-added">3787 </span>
3788   // InitialMark needs claim bits to keep track of the marked-through CLDs.
3789   if (collector_state()-&gt;in_initial_mark_gc()) {
<span class="line-added">3790     concurrent_mark()-&gt;pre_initial_mark();</span>
<span class="line-added">3791 </span>
3792     double start_clear_claimed_marks = os::elapsedTime();
3793 
3794     ClassLoaderDataGraph::clear_claimed_marks();
3795 
3796     double recorded_clear_claimed_marks_time_ms = (os::elapsedTime() - start_clear_claimed_marks) * 1000.0;
3797     phase_times()-&gt;record_clear_claimed_marks_time_ms(recorded_clear_claimed_marks_time_ms);
3798   }

3799 

3800   // Should G1EvacuationFailureALot be in effect for this GC?
3801   NOT_PRODUCT(set_evacuation_failure_alot_for_current_gc();)
<span class="line-added">3802 }</span>
3803 
<span class="line-modified">3804 class G1EvacuateRegionsBaseTask : public AbstractGangTask {</span>
<span class="line-added">3805 protected:</span>
<span class="line-added">3806   G1CollectedHeap* _g1h;</span>
<span class="line-added">3807   G1ParScanThreadStateSet* _per_thread_states;</span>
<span class="line-added">3808   RefToScanQueueSet* _task_queues;</span>
<span class="line-added">3809   TaskTerminator _terminator;</span>
<span class="line-added">3810   uint _num_workers;</span>
3811 
<span class="line-modified">3812   void evacuate_live_objects(G1ParScanThreadState* pss,</span>
<span class="line-modified">3813                              uint worker_id,</span>
<span class="line-added">3814                              G1GCPhaseTimes::GCParPhases objcopy_phase,</span>
<span class="line-added">3815                              G1GCPhaseTimes::GCParPhases termination_phase) {</span>
<span class="line-added">3816     G1GCPhaseTimes* p = _g1h-&gt;phase_times();</span>
3817 
<span class="line-modified">3818     Ticks start = Ticks::now();</span>
<span class="line-modified">3819     G1ParEvacuateFollowersClosure cl(_g1h, pss, _task_queues, &amp;_terminator, objcopy_phase);</span>
<span class="line-modified">3820     cl.do_void();</span>

3821 
<span class="line-modified">3822     assert(pss-&gt;queue_is_empty(), &quot;should be empty&quot;);</span>
<span class="line-modified">3823 </span>
<span class="line-added">3824     Tickspan evac_time = (Ticks::now() - start);</span>
<span class="line-added">3825     p-&gt;record_or_add_time_secs(objcopy_phase, worker_id, evac_time.seconds() - cl.term_time());</span>
3826 
<span class="line-modified">3827     if (termination_phase == G1GCPhaseTimes::Termination) {</span>
<span class="line-modified">3828       p-&gt;record_time_secs(termination_phase, worker_id, cl.term_time());</span>
<span class="line-modified">3829       p-&gt;record_thread_work_item(termination_phase, worker_id, cl.term_attempts());</span>
<span class="line-modified">3830     } else {</span>
<span class="line-modified">3831       p-&gt;record_or_add_time_secs(termination_phase, worker_id, cl.term_time());</span>
<span class="line-added">3832       p-&gt;record_or_add_thread_work_item(termination_phase, worker_id, cl.term_attempts());</span>
<span class="line-added">3833     }</span>
<span class="line-added">3834     assert(pss-&gt;trim_ticks().seconds() == 0.0, &quot;Unexpected partial trimming during evacuation&quot;);</span>
3835   }
3836 
<span class="line-modified">3837   virtual void start_work(uint worker_id) { }</span>

3838 
<span class="line-modified">3839   virtual void end_work(uint worker_id) { }</span>



3840 
<span class="line-modified">3841   virtual void scan_roots(G1ParScanThreadState* pss, uint worker_id) = 0;</span>





3842 
<span class="line-modified">3843   virtual void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) = 0;</span>




3844 
<span class="line-modified">3845 public:</span>
<span class="line-modified">3846   G1EvacuateRegionsBaseTask(const char* name, G1ParScanThreadStateSet* per_thread_states, RefToScanQueueSet* task_queues, uint num_workers) :</span>
<span class="line-modified">3847     AbstractGangTask(name),</span>
<span class="line-modified">3848     _g1h(G1CollectedHeap::heap()),</span>
<span class="line-modified">3849     _per_thread_states(per_thread_states),</span>
<span class="line-modified">3850     _task_queues(task_queues),</span>
<span class="line-modified">3851     _terminator(num_workers, _task_queues),</span>
<span class="line-modified">3852     _num_workers(num_workers)</span>
<span class="line-modified">3853   { }</span>
<span class="line-modified">3854 </span>
<span class="line-modified">3855   void work(uint worker_id) {</span>
<span class="line-modified">3856     start_work(worker_id);</span>
<span class="line-modified">3857 </span>
<span class="line-modified">3858     {</span>
<span class="line-modified">3859       ResourceMark rm;</span>
<span class="line-modified">3860       HandleMark   hm;</span>
<span class="line-modified">3861 </span>
<span class="line-modified">3862       G1ParScanThreadState* pss = _per_thread_states-&gt;state_for_worker(worker_id);</span>
<span class="line-modified">3863       pss-&gt;set_ref_discoverer(_g1h-&gt;ref_processor_stw());</span>
<span class="line-modified">3864 </span>
<span class="line-modified">3865       scan_roots(pss, worker_id);</span>
<span class="line-modified">3866       evacuate_live_objects(pss, worker_id);</span>





3867     }
3868 
<span class="line-modified">3869     end_work(worker_id);</span>
<span class="line-modified">3870   }</span>
<span class="line-modified">3871 };</span>

3872 
<span class="line-modified">3873 class G1EvacuateRegionsTask : public G1EvacuateRegionsBaseTask {</span>
<span class="line-modified">3874   G1RootProcessor* _root_processor;</span>
<span class="line-modified">3875 </span>
<span class="line-modified">3876   void scan_roots(G1ParScanThreadState* pss, uint worker_id) {</span>
<span class="line-added">3877     _root_processor-&gt;evacuate_roots(pss, worker_id);</span>
<span class="line-added">3878     _g1h-&gt;rem_set()-&gt;scan_heap_roots(pss, worker_id, G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::ObjCopy);</span>
<span class="line-added">3879     _g1h-&gt;rem_set()-&gt;scan_collection_set_regions(pss, worker_id, G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::CodeRoots, G1GCPhaseTimes::ObjCopy);</span>
3880   }
3881 
3882   void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) {
<span class="line-modified">3883     G1EvacuateRegionsBaseTask::evacuate_live_objects(pss, worker_id, G1GCPhaseTimes::ObjCopy, G1GCPhaseTimes::Termination);</span>
<span class="line-modified">3884   }</span>

3885 
<span class="line-modified">3886   void start_work(uint worker_id) {</span>
<span class="line-modified">3887     _g1h-&gt;phase_times()-&gt;record_time_secs(G1GCPhaseTimes::GCWorkerStart, worker_id, Ticks::now().seconds());</span>


3888   }
3889 
<span class="line-modified">3890   void end_work(uint worker_id) {</span>
<span class="line-modified">3891     _g1h-&gt;phase_times()-&gt;record_time_secs(G1GCPhaseTimes::GCWorkerEnd, worker_id, Ticks::now().seconds());</span>










3892   }
3893 
<span class="line-modified">3894 public:</span>
<span class="line-modified">3895   G1EvacuateRegionsTask(G1CollectedHeap* g1h,</span>
<span class="line-modified">3896                         G1ParScanThreadStateSet* per_thread_states,</span>
<span class="line-added">3897                         RefToScanQueueSet* task_queues,</span>
<span class="line-added">3898                         G1RootProcessor* root_processor,</span>
<span class="line-added">3899                         uint num_workers) :</span>
<span class="line-added">3900     G1EvacuateRegionsBaseTask(&quot;G1 Evacuate Regions&quot;, per_thread_states, task_queues, num_workers),</span>
<span class="line-added">3901     _root_processor(root_processor)</span>
<span class="line-added">3902   { }</span>
<span class="line-added">3903 };</span>
3904 
<span class="line-modified">3905 void G1CollectedHeap::evacuate_initial_collection_set(G1ParScanThreadStateSet* per_thread_states) {</span>
<span class="line-modified">3906   G1GCPhaseTimes* p = phase_times();</span>
3907 
<span class="line-modified">3908   {</span>
<span class="line-modified">3909     Ticks start = Ticks::now();</span>
<span class="line-added">3910     rem_set()-&gt;merge_heap_roots(true /* initial_evacuation */);</span>
<span class="line-added">3911     p-&gt;record_merge_heap_roots_time((Ticks::now() - start).seconds() * 1000.0);</span>
3912   }

3913 
<span class="line-modified">3914   Tickspan task_time;</span>
<span class="line-modified">3915   const uint num_workers = workers()-&gt;active_workers();</span>
<span class="line-modified">3916 </span>
<span class="line-added">3917   Ticks start_processing = Ticks::now();</span>
<span class="line-added">3918   {</span>
<span class="line-added">3919     G1RootProcessor root_processor(this, num_workers);</span>
<span class="line-added">3920     G1EvacuateRegionsTask g1_par_task(this, per_thread_states, _task_queues, &amp;root_processor, num_workers);</span>
<span class="line-added">3921     task_time = run_task(&amp;g1_par_task);</span>
<span class="line-added">3922     // Closing the inner scope will execute the destructor for the G1RootProcessor object.</span>
<span class="line-added">3923     // To extract its code root fixup time we measure total time of this scope and</span>
<span class="line-added">3924     // subtract from the time the WorkGang task took.</span>
<span class="line-added">3925   }</span>
<span class="line-added">3926   Tickspan total_processing = Ticks::now() - start_processing;</span>
3927 
<span class="line-modified">3928   p-&gt;record_initial_evac_time(task_time.seconds() * 1000.0);</span>
<span class="line-modified">3929   p-&gt;record_or_add_code_root_fixup_time((total_processing - task_time).seconds() * 1000.0);</span>
3930 }
3931 
<span class="line-modified">3932 class G1EvacuateOptionalRegionsTask : public G1EvacuateRegionsBaseTask {</span>
<span class="line-modified">3933 </span>
<span class="line-modified">3934   void scan_roots(G1ParScanThreadState* pss, uint worker_id) {</span>
<span class="line-modified">3935     _g1h-&gt;rem_set()-&gt;scan_heap_roots(pss, worker_id, G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::OptObjCopy);</span>
<span class="line-added">3936     _g1h-&gt;rem_set()-&gt;scan_collection_set_regions(pss, worker_id, G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::OptCodeRoots, G1GCPhaseTimes::OptObjCopy);</span>
3937   }
3938 
<span class="line-modified">3939   void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) {</span>
<span class="line-modified">3940     G1EvacuateRegionsBaseTask::evacuate_live_objects(pss, worker_id, G1GCPhaseTimes::OptObjCopy, G1GCPhaseTimes::OptTermination);</span>
<span class="line-added">3941   }</span>
<span class="line-added">3942 </span>
<span class="line-added">3943 public:</span>
<span class="line-added">3944   G1EvacuateOptionalRegionsTask(G1ParScanThreadStateSet* per_thread_states,</span>
<span class="line-added">3945                                 RefToScanQueueSet* queues,</span>
<span class="line-added">3946                                 uint num_workers) :</span>
<span class="line-added">3947     G1EvacuateRegionsBaseTask(&quot;G1 Evacuate Optional Regions&quot;, per_thread_states, queues, num_workers) {</span>
<span class="line-added">3948   }</span>
<span class="line-added">3949 };</span>
<span class="line-added">3950 </span>
<span class="line-added">3951 void G1CollectedHeap::evacuate_next_optional_regions(G1ParScanThreadStateSet* per_thread_states) {</span>
<span class="line-added">3952   class G1MarkScope : public MarkScope { };</span>
<span class="line-added">3953 </span>
<span class="line-added">3954   Tickspan task_time;</span>
<span class="line-added">3955 </span>
<span class="line-added">3956   Ticks start_processing = Ticks::now();</span>
<span class="line-added">3957   {</span>
<span class="line-added">3958     G1MarkScope code_mark_scope;</span>
<span class="line-added">3959     G1EvacuateOptionalRegionsTask task(per_thread_states, _task_queues, workers()-&gt;active_workers());</span>
<span class="line-added">3960     task_time = run_task(&amp;task);</span>
<span class="line-added">3961     // See comment in evacuate_collection_set() for the reason of the scope.</span>
3962   }
<span class="line-added">3963   Tickspan total_processing = Ticks::now() - start_processing;</span>
3964 
<span class="line-added">3965   G1GCPhaseTimes* p = phase_times();</span>
<span class="line-added">3966   p-&gt;record_or_add_code_root_fixup_time((total_processing - task_time).seconds() * 1000.0);</span>
<span class="line-added">3967 }</span>
<span class="line-added">3968 </span>
<span class="line-added">3969 void G1CollectedHeap::evacuate_optional_collection_set(G1ParScanThreadStateSet* per_thread_states) {</span>
3970   const double gc_start_time_ms = phase_times()-&gt;cur_collection_start_sec() * 1000.0;
3971 
<span class="line-modified">3972   while (!evacuation_failed() &amp;&amp; _collection_set.optional_region_length() &gt; 0) {</span>
3973 

3974     double time_used_ms = os::elapsedTime() * 1000.0 - gc_start_time_ms;
3975     double time_left_ms = MaxGCPauseMillis - time_used_ms;
3976 
<span class="line-modified">3977     if (time_left_ms &lt; 0 ||</span>
<span class="line-modified">3978         !_collection_set.finalize_optional_for_evacuation(time_left_ms * policy()-&gt;optional_evacuation_fraction())) {</span>
<span class="line-added">3979       log_trace(gc, ergo, cset)(&quot;Skipping evacuation of %u optional regions, no more regions can be evacuated in %.3fms&quot;,</span>
<span class="line-added">3980                                 _collection_set.optional_region_length(), time_left_ms);</span>
3981       break;
3982     }
3983 
<span class="line-modified">3984     {</span>
<span class="line-modified">3985       Ticks start = Ticks::now();</span>
<span class="line-modified">3986       rem_set()-&gt;merge_heap_roots(false /* initial_evacuation */);</span>
<span class="line-modified">3987       phase_times()-&gt;record_or_add_optional_merge_heap_roots_time((Ticks::now() - start).seconds() * 1000.0);</span>
3988     }
3989 
<span class="line-modified">3990     {</span>
<span class="line-modified">3991       Ticks start = Ticks::now();</span>
<span class="line-modified">3992       evacuate_next_optional_regions(per_thread_states);</span>
<span class="line-modified">3993       phase_times()-&gt;record_or_add_optional_evac_time((Ticks::now() - start).seconds() * 1000.0);</span>

3994     }
<span class="line-modified">3995   }</span>
3996 
<span class="line-modified">3997   _collection_set.abandon_optional_collection_set(per_thread_states);</span>
3998 }
3999 
<span class="line-modified">4000 void G1CollectedHeap::post_evacuate_collection_set(G1EvacuationInfo&amp; evacuation_info,</span>
<span class="line-modified">4001                                                    G1RedirtyCardsQueueSet* rdcqs,</span>
<span class="line-modified">4002                                                    G1ParScanThreadStateSet* per_thread_states) {</span>
<span class="line-modified">4003   G1GCPhaseTimes* p = phase_times();</span>
<span class="line-added">4004 </span>
<span class="line-added">4005   rem_set()-&gt;cleanup_after_scan_heap_roots();</span>
4006 
4007   // Process any discovered reference objects - we have
4008   // to do this _before_ we retire the GC alloc regions
4009   // as we may have to copy some &#39;reachable&#39; referent
4010   // objects (and their reachable sub-graphs) that were
4011   // not copied during the pause.
4012   process_discovered_references(per_thread_states);
4013 
4014   G1STWIsAliveClosure is_alive(this);
4015   G1KeepAliveClosure keep_alive(this);
4016 
<span class="line-modified">4017   WeakProcessor::weak_oops_do(workers(), &amp;is_alive, &amp;keep_alive, p-&gt;weak_phase_times());</span>

4018 
4019   if (G1StringDedup::is_enabled()) {
4020     double string_dedup_time_ms = os::elapsedTime();
4021 
<span class="line-modified">4022     string_dedup_cleaning(&amp;is_alive, &amp;keep_alive, p);</span>
4023 
4024     double string_cleanup_time_ms = (os::elapsedTime() - string_dedup_time_ms) * 1000.0;
<span class="line-modified">4025     p-&gt;record_string_deduplication_time(string_cleanup_time_ms);</span>
4026   }
4027 
<span class="line-added">4028   _allocator-&gt;release_gc_alloc_regions(evacuation_info);</span>
<span class="line-added">4029 </span>
4030   if (evacuation_failed()) {
<span class="line-modified">4031     restore_after_evac_failure(rdcqs);</span>
4032 
4033     // Reset the G1EvacuationFailureALot counters and flags


4034     NOT_PRODUCT(reset_evacuation_should_fail();)
<span class="line-added">4035 </span>
<span class="line-added">4036     double recalculate_used_start = os::elapsedTime();</span>
<span class="line-added">4037     set_used(recalculate_used());</span>
<span class="line-added">4038     p-&gt;record_evac_fail_recalc_used_time((os::elapsedTime() - recalculate_used_start) * 1000.0);</span>
<span class="line-added">4039 </span>
<span class="line-added">4040     if (_archive_allocator != NULL) {</span>
<span class="line-added">4041       _archive_allocator-&gt;clear_used();</span>
<span class="line-added">4042     }</span>
<span class="line-added">4043     for (uint i = 0; i &lt; ParallelGCThreads; i++) {</span>
<span class="line-added">4044       if (_evacuation_failed_info_array[i].has_failed()) {</span>
<span class="line-added">4045         _gc_tracer_stw-&gt;report_evacuation_failed(_evacuation_failed_info_array[i]);</span>
<span class="line-added">4046       }</span>
<span class="line-added">4047     }</span>
<span class="line-added">4048   } else {</span>
<span class="line-added">4049     // The &quot;used&quot; of the the collection set have already been subtracted</span>
<span class="line-added">4050     // when they were freed.  Add in the bytes used.</span>
<span class="line-added">4051     increase_used(_bytes_used_during_gc);</span>
4052   }
4053 
4054   _preserved_marks_set.assert_empty();
4055 


4056   merge_per_thread_state_info(per_thread_states);
4057 
4058   // Reset and re-enable the hot card cache.
4059   // Note the counts for the cards in the regions in the
4060   // collection set are reset when the collection set is freed.
4061   _hot_card_cache-&gt;reset_hot_cache();
4062   _hot_card_cache-&gt;set_use_cache(true);
4063 
4064   purge_code_root_memory();
4065 
<span class="line-modified">4066   redirty_logged_cards(rdcqs);</span>
<span class="line-added">4067 </span>
<span class="line-added">4068   free_collection_set(&amp;_collection_set, evacuation_info, per_thread_states-&gt;surviving_young_words());</span>
<span class="line-added">4069 </span>
<span class="line-added">4070   eagerly_reclaim_humongous_regions();</span>
<span class="line-added">4071 </span>
<span class="line-added">4072   record_obj_copy_mem_stats();</span>
<span class="line-added">4073 </span>
<span class="line-added">4074   evacuation_info.set_collectionset_used_before(collection_set()-&gt;bytes_used_before());</span>
<span class="line-added">4075   evacuation_info.set_bytes_used(_bytes_used_during_gc);</span>
<span class="line-added">4076 </span>
4077 #if COMPILER2_OR_JVMCI
4078   double start = os::elapsedTime();
4079   DerivedPointerTable::update_pointers();
4080   phase_times()-&gt;record_derived_pointer_table_update_time((os::elapsedTime() - start) * 1000.0);
4081 #endif
4082   policy()-&gt;print_age_table();
4083 }
4084 
4085 void G1CollectedHeap::record_obj_copy_mem_stats() {
4086   policy()-&gt;add_bytes_allocated_in_old_since_last_gc(_old_evac_stats.allocated() * HeapWordSize);
4087 
4088   _gc_tracer_stw-&gt;report_evacuation_statistics(create_g1_evac_summary(&amp;_survivor_evac_stats),
4089                                                create_g1_evac_summary(&amp;_old_evac_stats));
4090 }
4091 
<span class="line-modified">4092 void G1CollectedHeap::free_region(HeapRegion* hr, FreeRegionList* free_list) {</span>




4093   assert(!hr-&gt;is_free(), &quot;the region should not be free&quot;);
4094   assert(!hr-&gt;is_empty(), &quot;the region should not be empty&quot;);
4095   assert(_hrm-&gt;is_available(hr-&gt;hrm_index()), &quot;region should be committed&quot;);

4096 
4097   if (G1VerifyBitmaps) {
4098     MemRegion mr(hr-&gt;bottom(), hr-&gt;end());
4099     concurrent_mark()-&gt;clear_range_in_prev_bitmap(mr);
4100   }
4101 
4102   // Clear the card counts for this region.
4103   // Note: we only need to do this if the region is not young
4104   // (since we don&#39;t refine cards in young regions).
<span class="line-modified">4105   if (!hr-&gt;is_young()) {</span>
4106     _hot_card_cache-&gt;reset_card_counts(hr);
4107   }
<span class="line-modified">4108 </span>
<span class="line-added">4109   // Reset region metadata to allow reuse.</span>
<span class="line-added">4110   hr-&gt;hr_clear(true /* clear_space */);</span>
4111   _policy-&gt;remset_tracker()-&gt;update_at_free(hr);
<span class="line-modified">4112 </span>
<span class="line-added">4113   if (free_list != NULL) {</span>
<span class="line-added">4114     free_list-&gt;add_ordered(hr);</span>
<span class="line-added">4115   }</span>
4116 }
4117 
4118 void G1CollectedHeap::free_humongous_region(HeapRegion* hr,
4119                                             FreeRegionList* free_list) {
4120   assert(hr-&gt;is_humongous(), &quot;this is only for humongous regions&quot;);
4121   assert(free_list != NULL, &quot;pre-condition&quot;);
4122   hr-&gt;clear_humongous();
<span class="line-modified">4123   free_region(hr, free_list);</span>
4124 }
4125 
4126 void G1CollectedHeap::remove_from_old_sets(const uint old_regions_removed,
4127                                            const uint humongous_regions_removed) {
4128   if (old_regions_removed &gt; 0 || humongous_regions_removed &gt; 0) {
<span class="line-modified">4129     MutexLocker x(OldSets_lock, Mutex::_no_safepoint_check_flag);</span>
4130     _old_set.bulk_remove(old_regions_removed);
4131     _humongous_set.bulk_remove(humongous_regions_removed);
4132   }
4133 
4134 }
4135 
4136 void G1CollectedHeap::prepend_to_freelist(FreeRegionList* list) {
4137   assert(list != NULL, &quot;list can&#39;t be null&quot;);
4138   if (!list-&gt;is_empty()) {
<span class="line-modified">4139     MutexLocker x(FreeList_lock, Mutex::_no_safepoint_check_flag);</span>
4140     _hrm-&gt;insert_list_into_free_list(list);
4141   }
4142 }
4143 
4144 void G1CollectedHeap::decrement_summary_bytes(size_t bytes) {
4145   decrease_used(bytes);
4146 }
4147 
4148 class G1FreeCollectionSetTask : public AbstractGangTask {
<span class="line-modified">4149   // Helper class to keep statistics for the collection set freeing</span>
<span class="line-modified">4150   class FreeCSetStats {</span>
<span class="line-modified">4151     size_t _before_used_bytes;   // Usage in regions successfully evacutate</span>
<span class="line-modified">4152     size_t _after_used_bytes;    // Usage in regions failing evacuation</span>
<span class="line-modified">4153     size_t _bytes_allocated_in_old_since_last_gc; // Size of young regions turned into old</span>
<span class="line-modified">4154     size_t _failure_used_words;  // Live size in failed regions</span>
<span class="line-modified">4155     size_t _failure_waste_words; // Wasted size in failed regions</span>
<span class="line-modified">4156     size_t _rs_length;           // Remembered set size</span>
<span class="line-modified">4157     uint _regions_freed;         // Number of regions freed</span>











4158   public:
<span class="line-modified">4159     FreeCSetStats() :</span>
<span class="line-modified">4160         _before_used_bytes(0),</span>
<span class="line-modified">4161         _after_used_bytes(0),</span>
<span class="line-modified">4162         _bytes_allocated_in_old_since_last_gc(0),</span>
<span class="line-modified">4163         _failure_used_words(0),</span>
<span class="line-modified">4164         _failure_waste_words(0),</span>
<span class="line-modified">4165         _rs_length(0),</span>
<span class="line-modified">4166         _regions_freed(0) { }</span>
<span class="line-modified">4167 </span>
<span class="line-modified">4168     void merge_stats(FreeCSetStats* other) {</span>
<span class="line-added">4169       assert(other != NULL, &quot;invariant&quot;);</span>
<span class="line-added">4170       _before_used_bytes += other-&gt;_before_used_bytes;</span>
<span class="line-added">4171       _after_used_bytes += other-&gt;_after_used_bytes;</span>
<span class="line-added">4172       _bytes_allocated_in_old_since_last_gc += other-&gt;_bytes_allocated_in_old_since_last_gc;</span>
<span class="line-added">4173       _failure_used_words += other-&gt;_failure_used_words;</span>
<span class="line-added">4174       _failure_waste_words += other-&gt;_failure_waste_words;</span>
<span class="line-added">4175       _rs_length += other-&gt;_rs_length;</span>
<span class="line-added">4176       _regions_freed += other-&gt;_regions_freed;</span>
4177     }
4178 
<span class="line-modified">4179     void report(G1CollectedHeap* g1h, G1EvacuationInfo* evacuation_info) {</span>
<span class="line-modified">4180       evacuation_info-&gt;set_regions_freed(_regions_freed);</span>
<span class="line-added">4181       evacuation_info-&gt;increment_collectionset_used_after(_after_used_bytes);</span>
4182 
<span class="line-modified">4183       g1h-&gt;decrement_summary_bytes(_before_used_bytes);</span>
<span class="line-modified">4184       g1h-&gt;alloc_buffer_stats(G1HeapRegionAttr::Old)-&gt;add_failure_used_and_waste(_failure_used_words, _failure_waste_words);</span>
4185 
<span class="line-modified">4186       G1Policy *policy = g1h-&gt;policy();</span>
<span class="line-modified">4187       policy-&gt;add_bytes_allocated_in_old_since_last_gc(_bytes_allocated_in_old_since_last_gc);</span>
<span class="line-modified">4188       policy-&gt;record_rs_length(_rs_length);</span>
<span class="line-modified">4189       policy-&gt;cset_regions_freed();</span>
<span class="line-modified">4190     }</span>





4191 
<span class="line-modified">4192     void account_failed_region(HeapRegion* r) {</span>
<span class="line-modified">4193       size_t used_words = r-&gt;marked_bytes() / HeapWordSize;</span>
<span class="line-modified">4194       _failure_used_words += used_words;</span>
<span class="line-modified">4195       _failure_waste_words += HeapRegion::GrainWords - used_words;</span>
<span class="line-modified">4196       _after_used_bytes += r-&gt;used();</span>
<span class="line-modified">4197 </span>
<span class="line-modified">4198       // When moving a young gen region to old gen, we &quot;allocate&quot; that whole</span>
<span class="line-modified">4199       // region there. This is in addition to any already evacuated objects.</span>
<span class="line-modified">4200       // Notify the policy about that. Old gen regions do not cause an</span>
<span class="line-modified">4201       // additional allocation: both the objects still in the region and the</span>
<span class="line-modified">4202       // ones already moved are accounted for elsewhere.</span>
<span class="line-modified">4203       if (r-&gt;is_young()) {</span>
<span class="line-modified">4204         _bytes_allocated_in_old_since_last_gc += HeapRegion::GrainBytes;</span>



















4205       }

4206     }
4207 
<span class="line-modified">4208     void account_evacuated_region(HeapRegion* r) {</span>
<span class="line-modified">4209       _before_used_bytes += r-&gt;used();</span>
<span class="line-modified">4210       _regions_freed += 1;</span>
<span class="line-modified">4211     }</span>







4212 
<span class="line-modified">4213     void account_rs_length(HeapRegion* r) {</span>
<span class="line-added">4214       _rs_length += r-&gt;rem_set()-&gt;occupied();</span>
4215     }
4216   };
4217 
<span class="line-modified">4218   // Closure applied to all regions in the collection set.</span>
<span class="line-modified">4219   class FreeCSetClosure : public HeapRegionClosure {</span>
<span class="line-modified">4220     // Helper to send JFR events for regions.</span>
<span class="line-modified">4221     class JFREventForRegion {</span>
<span class="line-modified">4222       EventGCPhaseParallel _event;</span>
<span class="line-modified">4223     public:</span>
<span class="line-modified">4224       JFREventForRegion(HeapRegion* region, uint worker_id) : _event() {</span>
<span class="line-modified">4225         _event.set_gcId(GCId::current());</span>
<span class="line-modified">4226         _event.set_gcWorkerId(worker_id);</span>
<span class="line-modified">4227         if (region-&gt;is_young()) {</span>
<span class="line-modified">4228           _event.set_name(G1GCPhaseTimes::phase_name(G1GCPhaseTimes::YoungFreeCSet));</span>
<span class="line-modified">4229         } else {</span>
<span class="line-added">4230           _event.set_name(G1GCPhaseTimes::phase_name(G1GCPhaseTimes::NonYoungFreeCSet));</span>
<span class="line-added">4231         }</span>
<span class="line-added">4232       }</span>
4233 
<span class="line-modified">4234       ~JFREventForRegion() {</span>
<span class="line-modified">4235         _event.commit();</span>
<span class="line-modified">4236       }</span>
<span class="line-modified">4237     };</span>
<span class="line-added">4238 </span>
<span class="line-added">4239     // Helper to do timing for region work.</span>
<span class="line-added">4240     class TimerForRegion {</span>
<span class="line-added">4241       Tickspan&amp; _time;</span>
<span class="line-added">4242       Ticks     _start_time;</span>
<span class="line-added">4243     public:</span>
<span class="line-added">4244       TimerForRegion(Tickspan&amp; time) : _time(time), _start_time(Ticks::now()) { }</span>
<span class="line-added">4245       ~TimerForRegion() {</span>
<span class="line-added">4246         _time += Ticks::now() - _start_time;</span>
<span class="line-added">4247       }</span>
<span class="line-added">4248     };</span>
<span class="line-added">4249 </span>
<span class="line-added">4250     // FreeCSetClosure members</span>
<span class="line-added">4251     G1CollectedHeap* _g1h;</span>
<span class="line-added">4252     const size_t*    _surviving_young_words;</span>
<span class="line-added">4253     uint             _worker_id;</span>
<span class="line-added">4254     Tickspan         _young_time;</span>
<span class="line-added">4255     Tickspan         _non_young_time;</span>
<span class="line-added">4256     FreeCSetStats*   _stats;</span>
<span class="line-added">4257 </span>
<span class="line-added">4258     void assert_in_cset(HeapRegion* r) {</span>
<span class="line-added">4259       assert(r-&gt;young_index_in_cset() != 0 &amp;&amp;</span>
<span class="line-added">4260              (uint)r-&gt;young_index_in_cset() &lt;= _g1h-&gt;collection_set()-&gt;young_region_length(),</span>
<span class="line-added">4261              &quot;Young index %u is wrong for region %u of type %s with %u young regions&quot;,</span>
<span class="line-added">4262              r-&gt;young_index_in_cset(), r-&gt;hrm_index(), r-&gt;get_type_str(), _g1h-&gt;collection_set()-&gt;young_region_length());</span>
4263     }





4264 
<span class="line-modified">4265     void handle_evacuated_region(HeapRegion* r) {</span>
<span class="line-modified">4266       assert(!r-&gt;is_empty(), &quot;Region %u is an empty region in the collection set.&quot;, r-&gt;hrm_index());</span>
<span class="line-modified">4267       stats()-&gt;account_evacuated_region(r);</span>


4268 
<span class="line-modified">4269       // Free the region and and its remembered set.</span>
<span class="line-modified">4270       _g1h-&gt;free_region(r, NULL);</span>
<span class="line-added">4271     }</span>
4272 
<span class="line-modified">4273     void handle_failed_region(HeapRegion* r) {</span>
<span class="line-modified">4274       // Do some allocation statistics accounting. Regions that failed evacuation</span>
<span class="line-added">4275       // are always made old, so there is no need to update anything in the young</span>
<span class="line-added">4276       // gen statistics, but we need to update old gen statistics.</span>
<span class="line-added">4277       stats()-&gt;account_failed_region(r);</span>
4278 
<span class="line-modified">4279       // Update the region state due to the failed evacuation.</span>
<span class="line-added">4280       r-&gt;handle_evacuation_failure();</span>
4281 
<span class="line-modified">4282       // Add region to old set, need to hold lock.</span>
<span class="line-modified">4283       MutexLocker x(OldSets_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="line-added">4284       _g1h-&gt;old_set_add(r);</span>
4285     }
4286 
<span class="line-modified">4287     Tickspan&amp; timer_for_region(HeapRegion* r) {</span>
<span class="line-modified">4288       return r-&gt;is_young() ? _young_time : _non_young_time;</span>
4289     }

4290 
<span class="line-modified">4291     FreeCSetStats* stats() {</span>
<span class="line-modified">4292       return _stats;</span>
<span class="line-modified">4293     }</span>

4294   public:
<span class="line-modified">4295     FreeCSetClosure(const size_t* surviving_young_words,</span>
<span class="line-added">4296                     uint worker_id,</span>
<span class="line-added">4297                     FreeCSetStats* stats) :</span>
<span class="line-added">4298         HeapRegionClosure(),</span>
<span class="line-added">4299         _g1h(G1CollectedHeap::heap()),</span>
<span class="line-added">4300         _surviving_young_words(surviving_young_words),</span>
<span class="line-added">4301         _worker_id(worker_id),</span>
<span class="line-added">4302         _young_time(),</span>
<span class="line-added">4303         _non_young_time(),</span>
<span class="line-added">4304         _stats(stats) { }</span>
4305 
4306     virtual bool do_heap_region(HeapRegion* r) {
<span class="line-modified">4307       assert(r-&gt;in_collection_set(), &quot;Invariant: %u missing from CSet&quot;, r-&gt;hrm_index());</span>
<span class="line-added">4308       JFREventForRegion event(r, _worker_id);</span>
<span class="line-added">4309       TimerForRegion timer(timer_for_region(r));</span>
<span class="line-added">4310 </span>
<span class="line-added">4311       _g1h-&gt;clear_region_attr(r);</span>
<span class="line-added">4312       stats()-&gt;account_rs_length(r);</span>
<span class="line-added">4313 </span>
<span class="line-added">4314       if (r-&gt;is_young()) {</span>
<span class="line-added">4315         assert_in_cset(r);</span>
<span class="line-added">4316         r-&gt;record_surv_words_in_group(_surviving_young_words[r-&gt;young_index_in_cset()]);</span>
<span class="line-added">4317       }</span>
<span class="line-added">4318 </span>
<span class="line-added">4319       if (r-&gt;evacuation_failed()) {</span>
<span class="line-added">4320         handle_failed_region(r);</span>
<span class="line-added">4321       } else {</span>
<span class="line-added">4322         handle_evacuated_region(r);</span>
<span class="line-added">4323       }</span>
<span class="line-added">4324       assert(!_g1h-&gt;is_on_master_free_list(r), &quot;sanity&quot;);</span>
<span class="line-added">4325 </span>
4326       return false;
4327     }
<span class="line-added">4328 </span>
<span class="line-added">4329     void report_timing(Tickspan parallel_time) {</span>
<span class="line-added">4330       G1GCPhaseTimes* pt = _g1h-&gt;phase_times();</span>
<span class="line-added">4331       pt-&gt;record_time_secs(G1GCPhaseTimes::ParFreeCSet, _worker_id, parallel_time.seconds());</span>
<span class="line-added">4332       if (_young_time.value() &gt; 0) {</span>
<span class="line-added">4333         pt-&gt;record_time_secs(G1GCPhaseTimes::YoungFreeCSet, _worker_id, _young_time.seconds());</span>
<span class="line-added">4334       }</span>
<span class="line-added">4335       if (_non_young_time.value() &gt; 0) {</span>
<span class="line-added">4336         pt-&gt;record_time_secs(G1GCPhaseTimes::NonYoungFreeCSet, _worker_id, _non_young_time.seconds());</span>
<span class="line-added">4337       }</span>
<span class="line-added">4338     }</span>
4339   };
4340 
<span class="line-modified">4341   // G1FreeCollectionSetTask members</span>
<span class="line-modified">4342   G1CollectedHeap*  _g1h;</span>
<span class="line-modified">4343   G1EvacuationInfo* _evacuation_info;</span>
<span class="line-modified">4344   FreeCSetStats*    _worker_stats;</span>
<span class="line-added">4345   HeapRegionClaimer _claimer;</span>
<span class="line-added">4346   const size_t*     _surviving_young_words;</span>
<span class="line-added">4347   uint              _active_workers;</span>
4348 
<span class="line-modified">4349   FreeCSetStats* worker_stats(uint worker) {</span>
<span class="line-modified">4350     return &amp;_worker_stats[worker];</span>
<span class="line-added">4351   }</span>
4352 
<span class="line-modified">4353   void report_statistics() {</span>
<span class="line-modified">4354     // Merge the accounting</span>
<span class="line-modified">4355     FreeCSetStats total_stats;</span>
<span class="line-added">4356     for (uint worker = 0; worker &lt; _active_workers; worker++) {</span>
<span class="line-added">4357       total_stats.merge_stats(worker_stats(worker));</span>
<span class="line-added">4358     }</span>
<span class="line-added">4359     total_stats.report(_g1h, _evacuation_info);</span>
4360   }
<span class="line-added">4361 </span>
4362 public:
<span class="line-modified">4363   G1FreeCollectionSetTask(G1EvacuationInfo* evacuation_info, const size_t* surviving_young_words, uint active_workers) :</span>
<span class="line-modified">4364       AbstractGangTask(&quot;G1 Free Collection Set&quot;),</span>
<span class="line-modified">4365       _g1h(G1CollectedHeap::heap()),</span>
<span class="line-modified">4366       _evacuation_info(evacuation_info),</span>
<span class="line-modified">4367       _worker_stats(NEW_C_HEAP_ARRAY(FreeCSetStats, active_workers, mtGC)),</span>
<span class="line-modified">4368       _claimer(active_workers),</span>
<span class="line-modified">4369       _surviving_young_words(surviving_young_words),</span>
<span class="line-modified">4370       _active_workers(active_workers) {</span>
<span class="line-modified">4371     for (uint worker = 0; worker &lt; active_workers; worker++) {</span>
<span class="line-modified">4372       ::new (&amp;_worker_stats[worker]) FreeCSetStats();</span>
<span class="line-modified">4373     }</span>
4374   }
4375 
4376   ~G1FreeCollectionSetTask() {
<span class="line-modified">4377     Ticks serial_time = Ticks::now();</span>
<span class="line-modified">4378     report_statistics();</span>
<span class="line-added">4379     for (uint worker = 0; worker &lt; _active_workers; worker++) {</span>
<span class="line-added">4380       _worker_stats[worker].~FreeCSetStats();</span>
<span class="line-added">4381     }</span>
<span class="line-added">4382     FREE_C_HEAP_ARRAY(FreeCSetStats, _worker_stats);</span>
<span class="line-added">4383     _g1h-&gt;phase_times()-&gt;record_serial_free_cset_time_ms((Ticks::now() - serial_time).seconds() * 1000.0);</span>
4384   }
4385 




4386   virtual void work(uint worker_id) {
<span class="line-modified">4387     EventGCPhaseParallel event;</span>
<span class="line-modified">4388     Ticks start = Ticks::now();</span>
<span class="line-modified">4389     FreeCSetClosure cl(_surviving_young_words, worker_id, worker_stats(worker_id));</span>
<span class="line-modified">4390     _g1h-&gt;collection_set_par_iterate_all(&amp;cl, &amp;_claimer, worker_id);</span>














































4391 
<span class="line-modified">4392     // Report the total parallel time along with some more detailed metrics.</span>
<span class="line-modified">4393     cl.report_timing(Ticks::now() - start);</span>
<span class="line-modified">4394     event.commit(GCId::current(), worker_id, G1GCPhaseTimes::phase_name(G1GCPhaseTimes::ParFreeCSet));</span>



4395   }
4396 };
4397 
4398 void G1CollectedHeap::free_collection_set(G1CollectionSet* collection_set, G1EvacuationInfo&amp; evacuation_info, const size_t* surviving_young_words) {
4399   _eden.clear();
4400 
<span class="line-modified">4401   // The free collections set is split up in two tasks, the first</span>
<span class="line-added">4402   // frees the collection set and records what regions are free,</span>
<span class="line-added">4403   // and the second one rebuilds the free list. This proved to be</span>
<span class="line-added">4404   // more efficient than adding a sorted list to another.</span>
4405 
<span class="line-added">4406   Ticks free_cset_start_time = Ticks::now();</span>
4407   {
<span class="line-modified">4408     uint const num_cs_regions = _collection_set.region_length();</span>
<span class="line-modified">4409     uint const num_workers = clamp(num_cs_regions, 1u, workers()-&gt;active_workers());</span>
<span class="line-modified">4410     G1FreeCollectionSetTask cl(&amp;evacuation_info, surviving_young_words, num_workers);</span>

4411 
<span class="line-modified">4412     log_debug(gc, ergo)(&quot;Running %s using %u workers for collection set length %u (%u)&quot;,</span>
<span class="line-modified">4413                         cl.name(), num_workers, num_cs_regions, num_regions());</span>


4414     workers()-&gt;run_task(&amp;cl, num_workers);
4415   }
<span class="line-modified">4416 </span>
<span class="line-added">4417   Ticks free_cset_end_time = Ticks::now();</span>
<span class="line-added">4418   phase_times()-&gt;record_total_free_cset_time_ms((free_cset_end_time - free_cset_start_time).seconds() * 1000.0);</span>
<span class="line-added">4419 </span>
<span class="line-added">4420   // Now rebuild the free region list.</span>
<span class="line-added">4421   hrm()-&gt;rebuild_free_list(workers());</span>
<span class="line-added">4422   phase_times()-&gt;record_total_rebuild_freelist_time_ms((Ticks::now() - free_cset_end_time).seconds() * 1000.0);</span>
4423 
4424   collection_set-&gt;clear();
4425 }
4426 
4427 class G1FreeHumongousRegionClosure : public HeapRegionClosure {
4428  private:
4429   FreeRegionList* _free_region_list;
4430   HeapRegionSet* _proxy_set;
4431   uint _humongous_objects_reclaimed;
4432   uint _humongous_regions_reclaimed;
4433   size_t _freed_bytes;
4434  public:
4435 
4436   G1FreeHumongousRegionClosure(FreeRegionList* free_region_list) :
4437     _free_region_list(free_region_list), _proxy_set(NULL), _humongous_objects_reclaimed(0), _humongous_regions_reclaimed(0), _freed_bytes(0) {
4438   }
4439 
4440   virtual bool do_heap_region(HeapRegion* r) {
4441     if (!r-&gt;is_starts_humongous()) {
4442       return false;
</pre>
<hr />
<pre>
4560   G1HRPrinter* hrp = hr_printer();
4561   if (hrp-&gt;is_active()) {
4562     FreeRegionListIterator iter(&amp;local_cleanup_list);
4563     while (iter.more_available()) {
4564       HeapRegion* hr = iter.get_next();
4565       hrp-&gt;cleanup(hr);
4566     }
4567   }
4568 
4569   prepend_to_freelist(&amp;local_cleanup_list);
4570   decrement_summary_bytes(cl.bytes_freed());
4571 
4572   phase_times()-&gt;record_fast_reclaim_humongous_time_ms((os::elapsedTime() - start_time) * 1000.0,
4573                                                        cl.humongous_objects_reclaimed());
4574 }
4575 
4576 class G1AbandonCollectionSetClosure : public HeapRegionClosure {
4577 public:
4578   virtual bool do_heap_region(HeapRegion* r) {
4579     assert(r-&gt;in_collection_set(), &quot;Region %u must have been in collection set&quot;, r-&gt;hrm_index());
<span class="line-modified">4580     G1CollectedHeap::heap()-&gt;clear_region_attr(r);</span>
<span class="line-modified">4581     r-&gt;clear_young_index_in_cset();</span>
4582     return false;
4583   }
4584 };
4585 
4586 void G1CollectedHeap::abandon_collection_set(G1CollectionSet* collection_set) {
4587   G1AbandonCollectionSetClosure cl;
<span class="line-modified">4588   collection_set_iterate_all(&amp;cl);</span>
4589 
4590   collection_set-&gt;clear();
4591   collection_set-&gt;stop_incremental_building();
4592 }
4593 
4594 bool G1CollectedHeap::is_old_gc_alloc_region(HeapRegion* hr) {
4595   return _allocator-&gt;is_retained_old_region(hr);
4596 }
4597 
4598 void G1CollectedHeap::set_region_short_lived_locked(HeapRegion* hr) {
4599   _eden.add(hr);
4600   _policy-&gt;set_region_eden(hr);
4601 }
4602 
4603 #ifdef ASSERT
4604 
4605 class NoYoungRegionsClosure: public HeapRegionClosure {
4606 private:
4607   bool _success;
4608 public:
</pre>
<hr />
<pre>
4735   }
4736 };
4737 
4738 void G1CollectedHeap::rebuild_region_sets(bool free_list_only) {
4739   assert_at_safepoint_on_vm_thread();
4740 
4741   if (!free_list_only) {
4742     _eden.clear();
4743     _survivor.clear();
4744   }
4745 
4746   RebuildRegionSetsClosure cl(free_list_only, &amp;_old_set, _hrm);
4747   heap_region_iterate(&amp;cl);
4748 
4749   if (!free_list_only) {
4750     set_used(cl.total_used());
4751     if (_archive_allocator != NULL) {
4752       _archive_allocator-&gt;clear_used();
4753     }
4754   }
<span class="line-modified">4755   assert_used_and_recalculate_used_equal(this);</span>







4756 }
4757 
4758 // Methods for the mutator alloc region
4759 
4760 HeapRegion* G1CollectedHeap::new_mutator_alloc_region(size_t word_size,
<span class="line-modified">4761                                                       bool force,</span>
<span class="line-added">4762                                                       uint node_index) {</span>
4763   assert_heap_locked_or_at_safepoint(true /* should_be_vm_thread */);
4764   bool should_allocate = policy()-&gt;should_allocate_mutator_region();
4765   if (force || should_allocate) {
4766     HeapRegion* new_alloc_region = new_region(word_size,
4767                                               HeapRegionType::Eden,
<span class="line-modified">4768                                               false /* do_expand */,</span>
<span class="line-added">4769                                               node_index);</span>
4770     if (new_alloc_region != NULL) {
4771       set_region_short_lived_locked(new_alloc_region);
4772       _hr_printer.alloc(new_alloc_region, !should_allocate);
4773       _verifier-&gt;check_bitmaps(&quot;Mutator Region Allocation&quot;, new_alloc_region);
4774       _policy-&gt;remset_tracker()-&gt;update_at_allocate(new_alloc_region);
4775       return new_alloc_region;
4776     }
4777   }
4778   return NULL;
4779 }
4780 
4781 void G1CollectedHeap::retire_mutator_alloc_region(HeapRegion* alloc_region,
4782                                                   size_t allocated_bytes) {
4783   assert_heap_locked_or_at_safepoint(true /* should_be_vm_thread */);
4784   assert(alloc_region-&gt;is_eden(), &quot;all mutator alloc regions should be eden&quot;);
4785 
4786   collection_set()-&gt;add_eden_region(alloc_region);
4787   increase_used(allocated_bytes);
<span class="line-added">4788   _eden.add_used_bytes(allocated_bytes);</span>
4789   _hr_printer.retire(alloc_region);
<span class="line-added">4790 </span>
4791   // We update the eden sizes here, when the region is retired,
4792   // instead of when it&#39;s allocated, since this is the point that its
4793   // used space has been recorded in _summary_bytes_used.
4794   g1mm()-&gt;update_eden_size();
4795 }
4796 
4797 // Methods for the GC alloc regions
4798 
<span class="line-modified">4799 bool G1CollectedHeap::has_more_regions(G1HeapRegionAttr dest) {</span>
4800   if (dest.is_old()) {
4801     return true;
4802   } else {
4803     return survivor_regions_count() &lt; policy()-&gt;max_survivor_regions();
4804   }
4805 }
4806 
<span class="line-modified">4807 HeapRegion* G1CollectedHeap::new_gc_alloc_region(size_t word_size, G1HeapRegionAttr dest, uint node_index) {</span>
4808   assert(FreeList_lock-&gt;owned_by_self(), &quot;pre-condition&quot;);
4809 
4810   if (!has_more_regions(dest)) {
4811     return NULL;
4812   }
4813 
4814   HeapRegionType type;
4815   if (dest.is_young()) {
4816     type = HeapRegionType::Survivor;
4817   } else {
4818     type = HeapRegionType::Old;
4819   }
4820 
4821   HeapRegion* new_alloc_region = new_region(word_size,
4822                                             type,
<span class="line-modified">4823                                             true /* do_expand */,</span>
<span class="line-added">4824                                             node_index);</span>
4825 
4826   if (new_alloc_region != NULL) {
4827     if (type.is_survivor()) {
4828       new_alloc_region-&gt;set_survivor();
4829       _survivor.add(new_alloc_region);
4830       _verifier-&gt;check_bitmaps(&quot;Survivor Region Allocation&quot;, new_alloc_region);
4831     } else {
4832       new_alloc_region-&gt;set_old();
4833       _verifier-&gt;check_bitmaps(&quot;Old Region Allocation&quot;, new_alloc_region);
4834     }
4835     _policy-&gt;remset_tracker()-&gt;update_at_allocate(new_alloc_region);
<span class="line-added">4836     register_region_with_region_attr(new_alloc_region);</span>
4837     _hr_printer.alloc(new_alloc_region);
4838     return new_alloc_region;
4839   }
4840   return NULL;
4841 }
4842 
4843 void G1CollectedHeap::retire_gc_alloc_region(HeapRegion* alloc_region,
4844                                              size_t allocated_bytes,
<span class="line-modified">4845                                              G1HeapRegionAttr dest) {</span>
<span class="line-modified">4846   _bytes_used_during_gc += allocated_bytes;</span>
4847   if (dest.is_old()) {
4848     old_set_add(alloc_region);
<span class="line-added">4849   } else {</span>
<span class="line-added">4850     assert(dest.is_young(), &quot;Retiring alloc region should be young (%d)&quot;, dest.type());</span>
<span class="line-added">4851     _survivor.add_used_bytes(allocated_bytes);</span>
4852   }
4853 
4854   bool const during_im = collector_state()-&gt;in_initial_mark_gc();
4855   if (during_im &amp;&amp; allocated_bytes &gt; 0) {
<span class="line-modified">4856     _cm-&gt;root_regions()-&gt;add(alloc_region-&gt;next_top_at_mark_start(), alloc_region-&gt;top());</span>
4857   }
4858   _hr_printer.retire(alloc_region);
4859 }
4860 
4861 HeapRegion* G1CollectedHeap::alloc_highest_free_region() {
4862   bool expanded = false;
4863   uint index = _hrm-&gt;find_highest_free(&amp;expanded);
4864 
4865   if (index != G1_NO_HRM_INDEX) {
4866     if (expanded) {
4867       log_debug(gc, ergo, heap)(&quot;Attempt heap expansion (requested address range outside heap bounds). region size: &quot; SIZE_FORMAT &quot;B&quot;,
4868                                 HeapRegion::GrainWords * HeapWordSize);
4869     }
4870     _hrm-&gt;allocate_free_regions_starting_at(index, 1);
4871     return region_at(index);
4872   }
4873   return NULL;
4874 }
4875 
4876 // Optimized nmethod scanning
</pre>
</td>
</tr>
</table>
<center><a href="g1CodeCacheRemSet.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>