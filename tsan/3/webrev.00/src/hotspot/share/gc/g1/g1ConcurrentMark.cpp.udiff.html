<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Udiff src/hotspot/share/gc/g1/g1ConcurrentMark.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1CollectionSetChooser.cpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1ConcurrentMark.hpp.udiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1ConcurrentMark.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-new-header">@@ -1,7 +1,7 @@</span>
  /*
<span class="udiff-line-modified-removed">-  * Copyright (c) 2001, 2019, Oracle and/or its affiliates. All rights reserved.</span>
<span class="udiff-line-modified-added">+  * Copyright (c) 2001, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -35,34 +35,38 @@</span>
  #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
  #include &quot;gc/g1/g1Policy.hpp&quot;
  #include &quot;gc/g1/g1RegionMarkStatsCache.inline.hpp&quot;
  #include &quot;gc/g1/g1StringDedup.hpp&quot;
  #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
<span class="udiff-line-added">+ #include &quot;gc/g1/g1Trace.hpp&quot;</span>
  #include &quot;gc/g1/heapRegion.inline.hpp&quot;
  #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
  #include &quot;gc/g1/heapRegionSet.inline.hpp&quot;
  #include &quot;gc/shared/gcId.hpp&quot;
  #include &quot;gc/shared/gcTimer.hpp&quot;
<span class="udiff-line-removed">- #include &quot;gc/shared/gcTrace.hpp&quot;</span>
  #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  #include &quot;gc/shared/gcVMOperations.hpp&quot;
  #include &quot;gc/shared/genOopClosures.inline.hpp&quot;
  #include &quot;gc/shared/referencePolicy.hpp&quot;
  #include &quot;gc/shared/strongRootsScope.hpp&quot;
  #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
<span class="udiff-line-added">+ #include &quot;gc/shared/taskTerminator.hpp&quot;</span>
  #include &quot;gc/shared/taskqueue.inline.hpp&quot;
  #include &quot;gc/shared/weakProcessor.inline.hpp&quot;
  #include &quot;gc/shared/workerPolicy.hpp&quot;
  #include &quot;include/jvm.h&quot;
  #include &quot;logging/log.hpp&quot;
  #include &quot;memory/allocation.hpp&quot;
<span class="udiff-line-added">+ #include &quot;memory/iterator.hpp&quot;</span>
  #include &quot;memory/resourceArea.hpp&quot;
<span class="udiff-line-added">+ #include &quot;memory/universe.hpp&quot;</span>
  #include &quot;oops/access.inline.hpp&quot;
  #include &quot;oops/oop.inline.hpp&quot;
  #include &quot;runtime/atomic.hpp&quot;
  #include &quot;runtime/handles.inline.hpp&quot;
  #include &quot;runtime/java.hpp&quot;
<span class="udiff-line-added">+ #include &quot;runtime/orderAccess.hpp&quot;</span>
  #include &quot;runtime/prefetch.inline.hpp&quot;
  #include &quot;services/memTracker.hpp&quot;
  #include &quot;utilities/align.hpp&quot;
  #include &quot;utilities/growableArray.hpp&quot;
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -164,17 +168,17 @@</span>
    elem-&gt;next = *list;
    *list = elem;
  }
  
  void G1CMMarkStack::add_chunk_to_chunk_list(TaskQueueEntryChunk* elem) {
<span class="udiff-line-modified-removed">-   MutexLockerEx x(MarkStackChunkList_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="udiff-line-modified-added">+   MutexLocker x(MarkStackChunkList_lock, Mutex::_no_safepoint_check_flag);</span>
    add_chunk_to_list(&amp;_chunk_list, elem);
    _chunks_in_chunk_list++;
  }
  
  void G1CMMarkStack::add_chunk_to_free_list(TaskQueueEntryChunk* elem) {
<span class="udiff-line-modified-removed">-   MutexLockerEx x(MarkStackFreeList_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="udiff-line-modified-added">+   MutexLocker x(MarkStackFreeList_lock, Mutex::_no_safepoint_check_flag);</span>
    add_chunk_to_list(&amp;_free_list, elem);
  }
  
  G1CMMarkStack::TaskQueueEntryChunk* G1CMMarkStack::remove_chunk_from_list(TaskQueueEntryChunk* volatile* list) {
    TaskQueueEntryChunk* result = *list;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -183,20 +187,20 @@</span>
    }
    return result;
  }
  
  G1CMMarkStack::TaskQueueEntryChunk* G1CMMarkStack::remove_chunk_from_chunk_list() {
<span class="udiff-line-modified-removed">-   MutexLockerEx x(MarkStackChunkList_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="udiff-line-modified-added">+   MutexLocker x(MarkStackChunkList_lock, Mutex::_no_safepoint_check_flag);</span>
    TaskQueueEntryChunk* result = remove_chunk_from_list(&amp;_chunk_list);
    if (result != NULL) {
      _chunks_in_chunk_list--;
    }
    return result;
  }
  
  G1CMMarkStack::TaskQueueEntryChunk* G1CMMarkStack::remove_chunk_from_free_list() {
<span class="udiff-line-modified-removed">-   MutexLockerEx x(MarkStackFreeList_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="udiff-line-modified-added">+   MutexLocker x(MarkStackFreeList_lock, Mutex::_no_safepoint_check_flag);</span>
    return remove_chunk_from_list(&amp;_free_list);
  }
  
  G1CMMarkStack::TaskQueueEntryChunk* G1CMMarkStack::allocate_new_chunk() {
    // This dirty read of _hwm is okay because we only ever increase the _hwm in parallel code.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -204,11 +208,11 @@</span>
    // wraparound of _hwm.
    if (_hwm &gt;= _chunk_capacity) {
      return NULL;
    }
  
<span class="udiff-line-modified-removed">-   size_t cur_idx = Atomic::add(1u, &amp;_hwm) - 1;</span>
<span class="udiff-line-modified-added">+   size_t cur_idx = Atomic::fetch_and_add(&amp;_hwm, 1u);</span>
    if (cur_idx &gt;= _chunk_capacity) {
      return NULL;
    }
  
    TaskQueueEntryChunk* result = ::new (&amp;_base[cur_idx]) TaskQueueEntryChunk;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -254,75 +258,78 @@</span>
    _hwm = 0;
    _chunk_list = NULL;
    _free_list = NULL;
  }
  
<span class="udiff-line-modified-removed">- G1CMRootRegions::G1CMRootRegions(uint const max_regions) :</span>
<span class="udiff-line-modified-removed">-   _root_regions(NEW_C_HEAP_ARRAY(HeapRegion*, max_regions, mtGC)),</span>
<span class="udiff-line-modified-removed">-   _max_regions(max_regions),</span>
<span class="udiff-line-modified-removed">-   _num_root_regions(0),</span>
<span class="udiff-line-modified-removed">-   _claimed_root_regions(0),</span>
<span class="udiff-line-modified-removed">-   _scan_in_progress(false),</span>
<span class="udiff-line-modified-removed">-   _should_abort(false) { }</span>
<span class="udiff-line-modified-added">+ G1CMRootMemRegions::G1CMRootMemRegions(uint const max_regions) :</span>
<span class="udiff-line-modified-added">+     _root_regions(MemRegion::create_array(max_regions, mtGC)),</span>
<span class="udiff-line-modified-added">+     _max_regions(max_regions),</span>
<span class="udiff-line-modified-added">+     _num_root_regions(0),</span>
<span class="udiff-line-modified-added">+     _claimed_root_regions(0),</span>
<span class="udiff-line-modified-added">+     _scan_in_progress(false),</span>
<span class="udiff-line-modified-added">+     _should_abort(false) { }</span>
  
<span class="udiff-line-modified-removed">- G1CMRootRegions::~G1CMRootRegions() {</span>
<span class="udiff-line-modified-removed">-   FREE_C_HEAP_ARRAY(HeapRegion*, _max_regions);</span>
<span class="udiff-line-modified-added">+ G1CMRootMemRegions::~G1CMRootMemRegions() {</span>
<span class="udiff-line-modified-added">+   FREE_C_HEAP_ARRAY(MemRegion, _root_regions);</span>
  }
  
<span class="udiff-line-modified-removed">- void G1CMRootRegions::reset() {</span>
<span class="udiff-line-modified-added">+ void G1CMRootMemRegions::reset() {</span>
    _num_root_regions = 0;
  }
  
<span class="udiff-line-modified-removed">- void G1CMRootRegions::add(HeapRegion* hr) {</span>
<span class="udiff-line-modified-added">+ void G1CMRootMemRegions::add(HeapWord* start, HeapWord* end) {</span>
    assert_at_safepoint();
<span class="udiff-line-modified-removed">-   size_t idx = Atomic::add((size_t)1, &amp;_num_root_regions) - 1;</span>
<span class="udiff-line-modified-removed">-   assert(idx &lt; _max_regions, &quot;Trying to add more root regions than there is space &quot; SIZE_FORMAT, _max_regions);</span>
<span class="udiff-line-modified-removed">-   _root_regions[idx] = hr;</span>
<span class="udiff-line-modified-added">+   size_t idx = Atomic::fetch_and_add(&amp;_num_root_regions, 1u);</span>
<span class="udiff-line-modified-added">+   assert(idx &lt; _max_regions, &quot;Trying to add more root MemRegions than there is space &quot; SIZE_FORMAT, _max_regions);</span>
<span class="udiff-line-modified-added">+   assert(start != NULL &amp;&amp; end != NULL &amp;&amp; start &lt;= end, &quot;Start (&quot; PTR_FORMAT &quot;) should be less or equal to &quot;</span>
<span class="udiff-line-added">+          &quot;end (&quot; PTR_FORMAT &quot;)&quot;, p2i(start), p2i(end));</span>
<span class="udiff-line-added">+   _root_regions[idx].set_start(start);</span>
<span class="udiff-line-added">+   _root_regions[idx].set_end(end);</span>
  }
  
<span class="udiff-line-modified-removed">- void G1CMRootRegions::prepare_for_scan() {</span>
<span class="udiff-line-modified-added">+ void G1CMRootMemRegions::prepare_for_scan() {</span>
    assert(!scan_in_progress(), &quot;pre-condition&quot;);
  
    _scan_in_progress = _num_root_regions &gt; 0;
  
    _claimed_root_regions = 0;
    _should_abort = false;
  }
  
<span class="udiff-line-modified-removed">- HeapRegion* G1CMRootRegions::claim_next() {</span>
<span class="udiff-line-modified-added">+ const MemRegion* G1CMRootMemRegions::claim_next() {</span>
    if (_should_abort) {
      // If someone has set the should_abort flag, we return NULL to
      // force the caller to bail out of their loop.
      return NULL;
    }
  
    if (_claimed_root_regions &gt;= _num_root_regions) {
      return NULL;
    }
  
<span class="udiff-line-modified-removed">-   size_t claimed_index = Atomic::add((size_t)1, &amp;_claimed_root_regions) - 1;</span>
<span class="udiff-line-modified-added">+   size_t claimed_index = Atomic::fetch_and_add(&amp;_claimed_root_regions, 1u);</span>
    if (claimed_index &lt; _num_root_regions) {
<span class="udiff-line-modified-removed">-     return _root_regions[claimed_index];</span>
<span class="udiff-line-modified-added">+     return &amp;_root_regions[claimed_index];</span>
    }
    return NULL;
  }
  
<span class="udiff-line-modified-removed">- uint G1CMRootRegions::num_root_regions() const {</span>
<span class="udiff-line-modified-added">+ uint G1CMRootMemRegions::num_root_regions() const {</span>
    return (uint)_num_root_regions;
  }
  
<span class="udiff-line-modified-removed">- void G1CMRootRegions::notify_scan_done() {</span>
<span class="udiff-line-modified-removed">-   MutexLockerEx x(RootRegionScan_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="udiff-line-modified-added">+ void G1CMRootMemRegions::notify_scan_done() {</span>
<span class="udiff-line-modified-added">+   MutexLocker x(RootRegionScan_lock, Mutex::_no_safepoint_check_flag);</span>
    _scan_in_progress = false;
    RootRegionScan_lock-&gt;notify_all();
  }
  
<span class="udiff-line-modified-removed">- void G1CMRootRegions::cancel_scan() {</span>
<span class="udiff-line-modified-added">+ void G1CMRootMemRegions::cancel_scan() {</span>
    notify_scan_done();
  }
  
<span class="udiff-line-modified-removed">- void G1CMRootRegions::scan_finished() {</span>
<span class="udiff-line-modified-added">+ void G1CMRootMemRegions::scan_finished() {</span>
    assert(scan_in_progress(), &quot;pre-condition&quot;);
  
    if (!_should_abort) {
      assert(_claimed_root_regions &gt;= num_root_regions(),
             &quot;we should have claimed all root regions, claimed &quot; SIZE_FORMAT &quot;, length = %u&quot;,
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -330,19 +337,19 @@</span>
    }
  
    notify_scan_done();
  }
  
<span class="udiff-line-modified-removed">- bool G1CMRootRegions::wait_until_scan_finished() {</span>
<span class="udiff-line-modified-added">+ bool G1CMRootMemRegions::wait_until_scan_finished() {</span>
    if (!scan_in_progress()) {
      return false;
    }
  
    {
<span class="udiff-line-modified-removed">-     MutexLockerEx x(RootRegionScan_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="udiff-line-modified-added">+     MonitorLocker ml(RootRegionScan_lock, Mutex::_no_safepoint_check_flag);</span>
      while (scan_in_progress()) {
<span class="udiff-line-modified-removed">-       RootRegionScan_lock-&gt;wait(Mutex::_no_safepoint_check_flag);</span>
<span class="udiff-line-modified-added">+       ml.wait();</span>
      }
    }
    return true;
  }
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -422,11 +429,11 @@</span>
  
    if (FLAG_IS_DEFAULT(ConcGCThreads) || ConcGCThreads == 0) {
      // Calculate the number of concurrent worker threads by scaling
      // the number of parallel GC threads.
      uint marking_thread_num = scale_concurrent_worker_threads(ParallelGCThreads);
<span class="udiff-line-modified-removed">-     FLAG_SET_ERGO(uint, ConcGCThreads, marking_thread_num);</span>
<span class="udiff-line-modified-added">+     FLAG_SET_ERGO(ConcGCThreads, marking_thread_num);</span>
    }
  
    assert(ConcGCThreads &gt; 0, &quot;ConcGCThreads have been set.&quot;);
    if (ConcGCThreads &gt; ParallelGCThreads) {
      log_warning(gc)(&quot;More ConcGCThreads (%u) than ParallelGCThreads (%u).&quot;,
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -453,11 +460,11 @@</span>
        log_warning(gc)(&quot;Invalid value calculated for MarkStackSize (&quot; SIZE_FORMAT &quot;): &quot;
                        &quot;must be between 1 and &quot; SIZE_FORMAT,
                        mark_stack_size, MarkStackSizeMax);
        return;
      }
<span class="udiff-line-modified-removed">-     FLAG_SET_ERGO(size_t, MarkStackSize, mark_stack_size);</span>
<span class="udiff-line-modified-added">+     FLAG_SET_ERGO(MarkStackSize, mark_stack_size);</span>
    } else {
      // Verify MarkStackSize is in range.
      if (FLAG_IS_CMDLINE(MarkStackSize)) {
        if (FLAG_IS_DEFAULT(MarkStackSizeMax)) {
          if (!(MarkStackSize &gt;= 1 &amp;&amp; MarkStackSize &lt;= MarkStackSizeMax)) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -587,11 +594,11 @@</span>
    assert(active_tasks &lt;= _max_num_tasks, &quot;we should not have more&quot;);
  
    _num_active_tasks = active_tasks;
    // Need to update the three data structures below according to the
    // number of active threads for this phase.
<span class="udiff-line-modified-removed">-   _terminator.terminator()-&gt;reset_for_reuse((int) active_tasks);</span>
<span class="udiff-line-modified-added">+   _terminator.reset_for_reuse(active_tasks);</span>
    _first_overflow_barrier_sync.set_n_workers((int) active_tasks);
    _second_overflow_barrier_sync.set_n_workers((int) active_tasks);
  }
  
  void G1ConcurrentMark::set_concurrency_and_phase(uint active_tasks, bool concurrent) {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -732,11 +739,13 @@</span>
      return false;
    }
  };
  
  void G1ConcurrentMark::pre_initial_mark() {
<span class="udiff-line-modified-removed">-   // Initialize marking structures. This has to be done in a STW phase.</span>
<span class="udiff-line-modified-added">+   assert_at_safepoint_on_vm_thread();</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   // Reset marking state.</span>
    reset();
  
    // For each region note start of marking.
    NoteStartOfMarkHRClosure startcl;
    _g1h-&gt;heap_region_iterate(&amp;startcl);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -870,18 +879,25 @@</span>
           &quot;Calculated number of marking workers must be larger than zero and at most the maximum %u, but is %u&quot;,
           _max_concurrent_workers, result);
    return result;
  }
  
<span class="udiff-line-modified-removed">- void G1ConcurrentMark::scan_root_region(HeapRegion* hr, uint worker_id) {</span>
<span class="udiff-line-modified-removed">-   assert(hr-&gt;is_old() || (hr-&gt;is_survivor() &amp;&amp; hr-&gt;next_top_at_mark_start() == hr-&gt;bottom()),</span>
<span class="udiff-line-modified-removed">-          &quot;Root regions must be old or survivor but region %u is %s&quot;, hr-&gt;hrm_index(), hr-&gt;get_type_str());</span>
<span class="udiff-line-modified-added">+ void G1ConcurrentMark::scan_root_region(const MemRegion* region, uint worker_id) {</span>
<span class="udiff-line-modified-added">+ #ifdef ASSERT</span>
<span class="udiff-line-modified-added">+   HeapWord* last = region-&gt;last();</span>
<span class="udiff-line-added">+   HeapRegion* hr = _g1h-&gt;heap_region_containing(last);</span>
<span class="udiff-line-added">+   assert(hr-&gt;is_old() || hr-&gt;next_top_at_mark_start() == hr-&gt;bottom(),</span>
<span class="udiff-line-added">+          &quot;Root regions must be old or survivor/eden but region %u is %s&quot;, hr-&gt;hrm_index(), hr-&gt;get_type_str());</span>
<span class="udiff-line-added">+   assert(hr-&gt;next_top_at_mark_start() == region-&gt;start(),</span>
<span class="udiff-line-added">+          &quot;MemRegion start should be equal to nTAMS&quot;);</span>
<span class="udiff-line-added">+ #endif</span>
<span class="udiff-line-added">+ </span>
    G1RootRegionScanClosure cl(_g1h, this, worker_id);
  
    const uintx interval = PrefetchScanIntervalInBytes;
<span class="udiff-line-modified-removed">-   HeapWord* curr = hr-&gt;next_top_at_mark_start();</span>
<span class="udiff-line-modified-removed">-   const HeapWord* end = hr-&gt;top();</span>
<span class="udiff-line-modified-added">+   HeapWord* curr = region-&gt;start();</span>
<span class="udiff-line-modified-added">+   const HeapWord* end = region-&gt;end();</span>
    while (curr &lt; end) {
      Prefetch::read(curr, interval);
      oop obj = oop(curr);
      int size = obj-&gt;oop_iterate_size(&amp;cl);
      assert(size == obj-&gt;size(), &quot;sanity&quot;);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -897,15 +913,15 @@</span>
  
    void work(uint worker_id) {
      assert(Thread::current()-&gt;is_ConcurrentGC_thread(),
             &quot;this should only be done by a conc GC thread&quot;);
  
<span class="udiff-line-modified-removed">-     G1CMRootRegions* root_regions = _cm-&gt;root_regions();</span>
<span class="udiff-line-modified-removed">-     HeapRegion* hr = root_regions-&gt;claim_next();</span>
<span class="udiff-line-modified-removed">-     while (hr != NULL) {</span>
<span class="udiff-line-modified-removed">-       _cm-&gt;scan_root_region(hr, worker_id);</span>
<span class="udiff-line-modified-removed">-       hr = root_regions-&gt;claim_next();</span>
<span class="udiff-line-modified-added">+     G1CMRootMemRegions* root_regions = _cm-&gt;root_regions();</span>
<span class="udiff-line-modified-added">+     const MemRegion* region = root_regions-&gt;claim_next();</span>
<span class="udiff-line-modified-added">+     while (region != NULL) {</span>
<span class="udiff-line-modified-added">+       _cm-&gt;scan_root_region(region, worker_id);</span>
<span class="udiff-line-modified-added">+       region = root_regions-&gt;claim_next();</span>
      }
    }
  };
  
  void G1ConcurrentMark::scan_root_regions() {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1101,11 +1117,11 @@</span>
      _g1h(g1h), _cm(cm), _hrclaimer(num_workers), _total_selected_for_rebuild(0), _cl(&quot;Post-Marking&quot;) { }
  
    virtual void work(uint worker_id) {
      G1UpdateRemSetTrackingBeforeRebuild update_cl(_g1h, _cm, &amp;_cl);
      _g1h-&gt;heap_region_par_iterate_from_worker_offset(&amp;update_cl, &amp;_hrclaimer, worker_id);
<span class="udiff-line-modified-removed">-     Atomic::add(update_cl.num_selected_for_rebuild(), &amp;_total_selected_for_rebuild);</span>
<span class="udiff-line-modified-added">+     Atomic::add(&amp;_total_selected_for_rebuild, update_cl.num_selected_for_rebuild());</span>
    }
  
    uint total_selected_for_rebuild() const { return _total_selected_for_rebuild; }
  
    // Number of regions for which roughly one thread should be spawned for this work.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1252,11 +1268,11 @@</span>
          if (hr-&gt;is_humongous()) {
            _humongous_regions_removed++;
            _g1h-&gt;free_humongous_region(hr, _local_cleanup_list);
          } else {
            _old_regions_removed++;
<span class="udiff-line-modified-removed">-           _g1h-&gt;free_region(hr, _local_cleanup_list, false /* skip_remset */, false /* skip_hcc */, true /* locked */);</span>
<span class="udiff-line-modified-added">+           _g1h-&gt;free_region(hr, _local_cleanup_list);</span>
          }
          hr-&gt;clear_cardtable();
          _g1h-&gt;concurrent_mark()-&gt;clear_statistics_in_region(hr-&gt;hrm_index());
          log_trace(gc)(&quot;Reclaimed empty region %u (%s) bot &quot; PTR_FORMAT, hr-&gt;hrm_index(), hr-&gt;get_short_type_str(), p2i(hr-&gt;bottom()));
        }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1284,11 +1300,11 @@</span>
      assert(cl.is_complete(), &quot;Shouldn&#39;t have aborted!&quot;);
  
      // Now update the old/humongous region sets
      _g1h-&gt;remove_from_old_sets(cl.old_regions_removed(), cl.humongous_regions_removed());
      {
<span class="udiff-line-modified-removed">-       MutexLockerEx x(ParGCRareEvent_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="udiff-line-modified-added">+       MutexLocker x(ParGCRareEvent_lock, Mutex::_no_safepoint_check_flag);</span>
        _g1h-&gt;decrement_summary_bytes(cl.freed_bytes());
  
        _cleanup_list-&gt;add_ordered(&amp;local_cleanup_list);
        assert(local_cleanup_list.is_empty(), &quot;post-condition&quot;);
      }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1591,11 +1607,11 @@</span>
      // is not multi-threaded we use the current (VMThread) thread,
      // otherwise we use the work gang from the G1CollectedHeap and
      // we utilize all the worker threads we can.
      bool processing_is_mt = rp-&gt;processing_is_mt();
      uint active_workers = (processing_is_mt ? _g1h-&gt;workers()-&gt;active_workers() : 1U);
<span class="udiff-line-modified-removed">-     active_workers = MAX2(MIN2(active_workers, _max_num_tasks), 1U);</span>
<span class="udiff-line-modified-added">+     active_workers = clamp(active_workers, 1u, _max_num_tasks);</span>
  
      // Parallel processing task executor.
      G1CMRefProcTaskExecutor par_task_executor(_g1h, this,
                                                _g1h-&gt;workers(), active_workers);
      AbstractRefProcTaskExecutor* executor = (processing_is_mt ? &amp;par_task_executor : NULL);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1706,13 +1722,12 @@</span>
    G1CollectedHeap* _g1h;
  public:
    G1ObjectCountIsAliveClosure(G1CollectedHeap* g1h) : _g1h(g1h) { }
  
    bool do_object_b(oop obj) {
<span class="udiff-line-modified-removed">-     HeapWord* addr = (HeapWord*)obj;</span>
<span class="udiff-line-modified-removed">-     return addr != NULL &amp;&amp;</span>
<span class="udiff-line-removed">-            (!_g1h-&gt;is_in_g1_reserved(addr) || !_g1h-&gt;is_obj_dead(obj));</span>
<span class="udiff-line-modified-added">+     return obj != NULL &amp;&amp;</span>
<span class="udiff-line-modified-added">+            (!_g1h-&gt;is_in_g1_reserved(obj) || !_g1h-&gt;is_obj_dead(obj));</span>
    }
  };
  
  void G1ConcurrentMark::report_object_count(bool mark_completed) {
    // Depending on the completion of the marking liveness needs to be determined
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1762,21 +1777,21 @@</span>
  
  class G1RemarkThreadsClosure : public ThreadClosure {
    G1CMSATBBufferClosure _cm_satb_cl;
    G1CMOopClosure _cm_cl;
    MarkingCodeBlobClosure _code_cl;
<span class="udiff-line-modified-removed">-   int _thread_parity;</span>
<span class="udiff-line-modified-added">+   uintx _claim_token;</span>
  
   public:
    G1RemarkThreadsClosure(G1CollectedHeap* g1h, G1CMTask* task) :
      _cm_satb_cl(task, g1h),
      _cm_cl(g1h, task),
      _code_cl(&amp;_cm_cl, !CodeBlobToOopClosure::FixRelocations),
<span class="udiff-line-modified-removed">-     _thread_parity(Threads::thread_claim_parity()) {}</span>
<span class="udiff-line-modified-added">+     _claim_token(Threads::thread_claim_token()) {}</span>
  
    void do_thread(Thread* thread) {
<span class="udiff-line-modified-removed">-     if (thread-&gt;claim_oops_do(true, _thread_parity)) {</span>
<span class="udiff-line-modified-added">+     if (thread-&gt;claim_threads_do(true, _claim_token)) {</span>
        SATBMarkQueue&amp; queue = G1ThreadLocalData::satb_mark_queue(thread);
        queue.apply_closure_and_empty(&amp;_cm_satb_cl);
        if (thread-&gt;is_Java_thread()) {
          // In theory it should not be neccessary to explicitly walk the nmethods to find roots for concurrent marking
          // however the liveness of oops reachable from nmethods have very complex lifecycles:
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1886,11 +1901,11 @@</span>
      // Above heap_region_containing may return NULL as we always scan claim
      // until the end of the heap. In this case, just jump to the next region.
      HeapWord* end = curr_region != NULL ? curr_region-&gt;end() : finger + HeapRegion::GrainWords;
  
      // Is the gap between reading the finger and doing the CAS too long?
<span class="udiff-line-modified-removed">-     HeapWord* res = Atomic::cmpxchg(end, &amp;_finger, finger);</span>
<span class="udiff-line-modified-added">+     HeapWord* res = Atomic::cmpxchg(&amp;_finger, finger, end);</span>
      if (res == finger &amp;&amp; curr_region != NULL) {
        // we succeeded
        HeapWord*   bottom        = curr_region-&gt;bottom();
        HeapWord*   limit         = curr_region-&gt;next_top_at_mark_start();
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1936,17 +1951,18 @@</span>
        return;
      }
      guarantee(oopDesc::is_oop(task_entry.obj()),
                &quot;Non-oop &quot; PTR_FORMAT &quot;, phase: %s, info: %d&quot;,
                p2i(task_entry.obj()), _phase, _info);
<span class="udiff-line-modified-removed">-     guarantee(!_g1h-&gt;is_in_cset(task_entry.obj()),</span>
<span class="udiff-line-modified-removed">-               &quot;obj: &quot; PTR_FORMAT &quot; in CSet, phase: %s, info: %d&quot;,</span>
<span class="udiff-line-modified-removed">-               p2i(task_entry.obj()), _phase, _info);</span>
<span class="udiff-line-modified-added">+     HeapRegion* r = _g1h-&gt;heap_region_containing(task_entry.obj());</span>
<span class="udiff-line-modified-added">+     guarantee(!(r-&gt;in_collection_set() || r-&gt;has_index_in_opt_cset()),</span>
<span class="udiff-line-modified-added">+               &quot;obj &quot; PTR_FORMAT &quot; from %s (%d) in region %u in (optional) collection set&quot;,</span>
<span class="udiff-line-added">+               p2i(task_entry.obj()), _phase, _info, r-&gt;hrm_index());</span>
    }
  };
  
<span class="udiff-line-modified-removed">- void G1ConcurrentMark::verify_no_cset_oops() {</span>
<span class="udiff-line-modified-added">+ void G1ConcurrentMark::verify_no_collection_set_oops() {</span>
    assert(SafepointSynchronize::is_at_safepoint(), &quot;should be at a safepoint&quot;);
    if (!_g1h-&gt;collector_state()-&gt;mark_or_rebuild_in_progress()) {
      return;
    }
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1975,15 +1991,15 @@</span>
    for (uint i = 0; i &lt; _num_concurrent_workers; ++i) {
      G1CMTask* task = _tasks[i];
      HeapWord* task_finger = task-&gt;finger();
      if (task_finger != NULL &amp;&amp; task_finger &lt; _heap.end()) {
        // See above note on the global finger verification.
<span class="udiff-line-modified-removed">-       HeapRegion* task_hr = _g1h-&gt;heap_region_containing(task_finger);</span>
<span class="udiff-line-modified-removed">-       guarantee(task_hr == NULL || task_finger == task_hr-&gt;bottom() ||</span>
<span class="udiff-line-modified-removed">-                 !task_hr-&gt;in_collection_set(),</span>
<span class="udiff-line-modified-added">+       HeapRegion* r = _g1h-&gt;heap_region_containing(task_finger);</span>
<span class="udiff-line-modified-added">+       guarantee(r == NULL || task_finger == r-&gt;bottom() ||</span>
<span class="udiff-line-modified-added">+                 !r-&gt;in_collection_set() || !r-&gt;has_index_in_opt_cset(),</span>
                  &quot;task finger: &quot; PTR_FORMAT &quot; region: &quot; HR_FORMAT,
<span class="udiff-line-modified-removed">-                 p2i(task_finger), HR_FORMAT_PARAMS(task_hr));</span>
<span class="udiff-line-modified-added">+                 p2i(task_finger), HR_FORMAT_PARAMS(r));</span>
      }
    }
  }
  #endif // PRODUCT
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2398,15 +2414,16 @@</span>
    while (!has_aborted() &amp;&amp;
           satb_mq_set.apply_closure_to_completed_buffer(&amp;satb_cl)) {
      abort_marking_if_regular_check_fail();
    }
  
<span class="udiff-line-modified-removed">-   _draining_satb_buffers = false;</span>
<span class="udiff-line-modified-added">+   // Can&#39;t assert qset is empty here, even if not aborted.  If concurrent,</span>
<span class="udiff-line-added">+   // some other thread might be adding to the queue.  If not concurrent,</span>
<span class="udiff-line-added">+   // some other thread might have won the race for the last buffer, but</span>
<span class="udiff-line-added">+   // has not yet decremented the count.</span>
  
<span class="udiff-line-modified-removed">-   assert(has_aborted() ||</span>
<span class="udiff-line-removed">-          _cm-&gt;concurrent() ||</span>
<span class="udiff-line-removed">-          satb_mq_set.completed_buffers_num() == 0, &quot;invariant&quot;);</span>
<span class="udiff-line-modified-added">+   _draining_satb_buffers = false;</span>
  
    // again, this was a potentially expensive operation, decrease the
    // limits to get the regular clock call early
    decrease_limits();
  }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2565,11 +2582,12 @@</span>
    // steal work from the other G1CMTasks. It only makes sense to
    // enable stealing when the termination protocol is enabled
    // and do_marking_step() is not being called serially.
    bool do_stealing = do_termination &amp;&amp; !is_serial;
  
<span class="udiff-line-modified-removed">-   double diff_prediction_ms = _g1h-&gt;policy()-&gt;predictor().get_new_prediction(&amp;_marking_step_diffs_ms);</span>
<span class="udiff-line-modified-added">+   G1Predictions const&amp; predictor = _g1h-&gt;policy()-&gt;predictor();</span>
<span class="udiff-line-added">+   double diff_prediction_ms = predictor.predict_zero_bounded(&amp;_marking_step_diff_ms);</span>
    _time_target_ms = time_target_ms - diff_prediction_ms;
  
    // set up the variables that are used in the work-based scheme to
    // call the regular clock method
    _words_scanned = 0;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2807,11 +2825,11 @@</span>
      if (_has_timed_out) {
        double diff_ms = elapsed_time_ms - _time_target_ms;
        // Keep statistics of how well we did with respect to hitting
        // our target only if we actually timed out (if we aborted for
        // other reasons, then the results might get skewed).
<span class="udiff-line-modified-removed">-       _marking_step_diffs_ms.add(diff_ms);</span>
<span class="udiff-line-modified-added">+       _marking_step_diff_ms.add(diff_ms);</span>
      }
  
      if (_cm-&gt;has_overflown()) {
        // This is the interesting one. We aborted because a global
        // overflow was raised. This means we have to restart the
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2890,15 +2908,15 @@</span>
    _draining_satb_buffers(false),
    _step_times_ms(),
    _elapsed_time_ms(0.0),
    _termination_time_ms(0.0),
    _termination_start_time_ms(0.0),
<span class="udiff-line-modified-removed">-   _marking_step_diffs_ms()</span>
<span class="udiff-line-modified-added">+   _marking_step_diff_ms()</span>
  {
    guarantee(task_queue != NULL, &quot;invariant&quot;);
  
<span class="udiff-line-modified-removed">-   _marking_step_diffs_ms.add(0.5);</span>
<span class="udiff-line-modified-added">+   _marking_step_diff_ms.add(0.5);</span>
  }
  
  // These are formatting macros that are used below to ensure
  // consistent formatting. The *_H_* versions are used to format the
  // header for a particular value and they should be kept consistent
</pre>
<center><a href="g1CollectionSetChooser.cpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1ConcurrentMark.hpp.udiff.html" target="_top">next &gt;</a></center>  </body>
</html>