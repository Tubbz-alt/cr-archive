<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/share/gc/g1/heapRegion.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2001, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;code/nmethod.hpp&quot;
 27 #include &quot;gc/g1/g1BlockOffsetTable.inline.hpp&quot;
 28 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
 29 #include &quot;gc/g1/g1CollectionSet.hpp&quot;
 30 #include &quot;gc/g1/g1HeapRegionTraceType.hpp&quot;
 31 #include &quot;gc/g1/g1NUMA.hpp&quot;
 32 #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
 33 #include &quot;gc/g1/heapRegion.inline.hpp&quot;
 34 #include &quot;gc/g1/heapRegionBounds.inline.hpp&quot;
 35 #include &quot;gc/g1/heapRegionManager.inline.hpp&quot;
 36 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
 37 #include &quot;gc/g1/heapRegionTracer.hpp&quot;
 38 #include &quot;gc/shared/genOopClosures.inline.hpp&quot;
 39 #include &quot;logging/log.hpp&quot;
 40 #include &quot;logging/logStream.hpp&quot;
 41 #include &quot;memory/iterator.inline.hpp&quot;
 42 #include &quot;memory/resourceArea.hpp&quot;
 43 #include &quot;oops/access.inline.hpp&quot;
 44 #include &quot;oops/compressedOops.inline.hpp&quot;
 45 #include &quot;oops/oop.inline.hpp&quot;
 46 
 47 int    HeapRegion::LogOfHRGrainBytes = 0;
 48 int    HeapRegion::LogOfHRGrainWords = 0;
 49 int    HeapRegion::LogCardsPerRegion = 0;
 50 size_t HeapRegion::GrainBytes        = 0;
 51 size_t HeapRegion::GrainWords        = 0;
 52 size_t HeapRegion::CardsPerRegion    = 0;
 53 
 54 size_t HeapRegion::max_region_size() {
 55   return HeapRegionBounds::max_size();
 56 }
 57 
 58 size_t HeapRegion::min_region_size_in_words() {
 59   return HeapRegionBounds::min_size() &gt;&gt; LogHeapWordSize;
 60 }
 61 
 62 void HeapRegion::setup_heap_region_size(size_t initial_heap_size, size_t max_heap_size) {
 63   size_t region_size = G1HeapRegionSize;
 64   if (FLAG_IS_DEFAULT(G1HeapRegionSize)) {
 65     size_t average_heap_size = (initial_heap_size + max_heap_size) / 2;
 66     region_size = MAX2(average_heap_size / HeapRegionBounds::target_number(),
 67                        HeapRegionBounds::min_size());
 68   }
 69 
 70   int region_size_log = log2_long((jlong) region_size);
 71   // Recalculate the region size to make sure it&#39;s a power of
 72   // 2. This means that region_size is the largest power of 2 that&#39;s
 73   // &lt;= what we&#39;ve calculated so far.
 74   region_size = ((size_t)1 &lt;&lt; region_size_log);
 75 
 76   // Now make sure that we don&#39;t go over or under our limits.
 77   if (region_size &lt; HeapRegionBounds::min_size()) {
 78     region_size = HeapRegionBounds::min_size();
 79   } else if (region_size &gt; HeapRegionBounds::max_size()) {
 80     region_size = HeapRegionBounds::max_size();
 81   }
 82 
 83   // And recalculate the log.
 84   region_size_log = log2_long((jlong) region_size);
 85 
 86   // Now, set up the globals.
 87   guarantee(LogOfHRGrainBytes == 0, &quot;we should only set it once&quot;);
 88   LogOfHRGrainBytes = region_size_log;
 89 
 90   guarantee(LogOfHRGrainWords == 0, &quot;we should only set it once&quot;);
 91   LogOfHRGrainWords = LogOfHRGrainBytes - LogHeapWordSize;
 92 
 93   guarantee(GrainBytes == 0, &quot;we should only set it once&quot;);
 94   // The cast to int is safe, given that we&#39;ve bounded region_size by
 95   // MIN_REGION_SIZE and MAX_REGION_SIZE.
 96   GrainBytes = region_size;
 97   log_info(gc, heap)(&quot;Heap region size: &quot; SIZE_FORMAT &quot;M&quot;, GrainBytes / M);
 98 
 99   guarantee(GrainWords == 0, &quot;we should only set it once&quot;);
100   GrainWords = GrainBytes &gt;&gt; LogHeapWordSize;
101   guarantee((size_t) 1 &lt;&lt; LogOfHRGrainWords == GrainWords, &quot;sanity&quot;);
102 
103   guarantee(CardsPerRegion == 0, &quot;we should only set it once&quot;);
104   CardsPerRegion = GrainBytes &gt;&gt; G1CardTable::card_shift;
105 
106   LogCardsPerRegion = log2_long((jlong) CardsPerRegion);
107 
108   if (G1HeapRegionSize != GrainBytes) {
109     FLAG_SET_ERGO(G1HeapRegionSize, GrainBytes);
110   }
111 }
112 
113 void HeapRegion::handle_evacuation_failure() {
114   uninstall_surv_rate_group();
115   clear_young_index_in_cset();
116   set_evacuation_failed(false);
117   set_old();
118 }
119 
120 void HeapRegion::unlink_from_list() {
121   set_next(NULL);
122   set_prev(NULL);
123   set_containing_set(NULL);
124 }
125 
126 void HeapRegion::hr_clear(bool clear_space) {
127   assert(_humongous_start_region == NULL,
128          &quot;we should have already filtered out humongous regions&quot;);
129   assert(!in_collection_set(),
130          &quot;Should not clear heap region %u in the collection set&quot;, hrm_index());
131 
132   clear_young_index_in_cset();
133   clear_index_in_opt_cset();
134   uninstall_surv_rate_group();
135   set_free();
136   reset_pre_dummy_top();
137 
138   rem_set()-&gt;clear_locked();
139 
140   zero_marked_bytes();
141 
142   init_top_at_mark_start();
143   if (clear_space) clear(SpaceDecorator::Mangle);
144 
145   _evacuation_failed = false;
146   _gc_efficiency = 0.0;
147 }
148 
149 void HeapRegion::clear_cardtable() {
150   G1CardTable* ct = G1CollectedHeap::heap()-&gt;card_table();
151   ct-&gt;clear(MemRegion(bottom(), end()));
152 }
153 
154 void HeapRegion::calc_gc_efficiency() {
155   // GC efficiency is the ratio of how much space would be
156   // reclaimed over how long we predict it would take to reclaim it.
157   G1Policy* policy = G1CollectedHeap::heap()-&gt;policy();
158 
159   // Retrieve a prediction of the elapsed time for this region for
160   // a mixed gc because the region will only be evacuated during a
161   // mixed gc.
162   double region_elapsed_time_ms = policy-&gt;predict_region_total_time_ms(this, false /* for_young_gc */);
163   _gc_efficiency = (double) reclaimable_bytes() / region_elapsed_time_ms;
164 }
165 
166 void HeapRegion::set_free() {
167   report_region_type_change(G1HeapRegionTraceType::Free);
168   _type.set_free();
169 }
170 
171 void HeapRegion::set_eden() {
172   report_region_type_change(G1HeapRegionTraceType::Eden);
173   _type.set_eden();
174 }
175 
176 void HeapRegion::set_eden_pre_gc() {
177   report_region_type_change(G1HeapRegionTraceType::Eden);
178   _type.set_eden_pre_gc();
179 }
180 
181 void HeapRegion::set_survivor() {
182   report_region_type_change(G1HeapRegionTraceType::Survivor);
183   _type.set_survivor();
184 }
185 
186 void HeapRegion::move_to_old() {
187   if (_type.relabel_as_old()) {
188     report_region_type_change(G1HeapRegionTraceType::Old);
189   }
190 }
191 
192 void HeapRegion::set_old() {
193   report_region_type_change(G1HeapRegionTraceType::Old);
194   _type.set_old();
195 }
196 
197 void HeapRegion::set_open_archive() {
198   report_region_type_change(G1HeapRegionTraceType::OpenArchive);
199   _type.set_open_archive();
200 }
201 
202 void HeapRegion::set_closed_archive() {
203   report_region_type_change(G1HeapRegionTraceType::ClosedArchive);
204   _type.set_closed_archive();
205 }
206 
207 void HeapRegion::set_starts_humongous(HeapWord* obj_top, size_t fill_size) {
208   assert(!is_humongous(), &quot;sanity / pre-condition&quot;);
209   assert(top() == bottom(), &quot;should be empty&quot;);
210 
211   report_region_type_change(G1HeapRegionTraceType::StartsHumongous);
212   _type.set_starts_humongous();
213   _humongous_start_region = this;
214 
215   _bot_part.set_for_starts_humongous(obj_top, fill_size);
216 }
217 
218 void HeapRegion::set_continues_humongous(HeapRegion* first_hr) {
219   assert(!is_humongous(), &quot;sanity / pre-condition&quot;);
220   assert(top() == bottom(), &quot;should be empty&quot;);
221   assert(first_hr-&gt;is_starts_humongous(), &quot;pre-condition&quot;);
222 
223   report_region_type_change(G1HeapRegionTraceType::ContinuesHumongous);
224   _type.set_continues_humongous();
225   _humongous_start_region = first_hr;
226 
227   _bot_part.set_object_can_span(true);
228 }
229 
230 void HeapRegion::clear_humongous() {
231   assert(is_humongous(), &quot;pre-condition&quot;);
232 
233   assert(capacity() == HeapRegion::GrainBytes, &quot;pre-condition&quot;);
234   _humongous_start_region = NULL;
235 
236   _bot_part.set_object_can_span(false);
237 }
238 
239 HeapRegion::HeapRegion(uint hrm_index,
240                        G1BlockOffsetTable* bot,
241                        MemRegion mr) :
242   _bottom(mr.start()),
243   _end(mr.end()),
244   _top(NULL),
245   _compaction_top(NULL),
246   _bot_part(bot, this),
247   _par_alloc_lock(Mutex::leaf, &quot;HeapRegion par alloc lock&quot;, true),
248   _pre_dummy_top(NULL),
249   _rem_set(NULL),
250   _hrm_index(hrm_index),
251   _type(),
252   _humongous_start_region(NULL),
253   _evacuation_failed(false),
254   _index_in_opt_cset(InvalidCSetIndex),
255   _next(NULL), _prev(NULL),
256 #ifdef ASSERT
257   _containing_set(NULL),
258 #endif
259   _prev_top_at_mark_start(NULL), _next_top_at_mark_start(NULL),
260   _prev_marked_bytes(0), _next_marked_bytes(0),
261   _young_index_in_cset(-1),
262   _surv_rate_group(NULL), _age_index(G1SurvRateGroup::InvalidAgeIndex), _gc_efficiency(0.0),
263   _node_index(G1NUMA::UnknownNodeIndex)
264 {
265   assert(Universe::on_page_boundary(mr.start()) &amp;&amp; Universe::on_page_boundary(mr.end()),
266          &quot;invalid space boundaries&quot;);
267 
268   _rem_set = new HeapRegionRemSet(bot, this);
269   initialize();
270 }
271 
272 void HeapRegion::initialize(bool clear_space, bool mangle_space) {
273   assert(_rem_set-&gt;is_empty(), &quot;Remembered set must be empty&quot;);
274 
275   if (clear_space) {
276     clear(mangle_space);
277   }
278 
279   set_top(bottom());
280   set_compaction_top(bottom());
281   reset_bot();
282 
283   hr_clear(false /*clear_space*/);
284 }
285 
286 void HeapRegion::report_region_type_change(G1HeapRegionTraceType::Type to) {
287   HeapRegionTracer::send_region_type_change(_hrm_index,
288                                             get_trace_type(),
289                                             to,
290                                             (uintptr_t)bottom(),
291                                             used());
292 }
293 
294 void HeapRegion::note_self_forwarding_removal_start(bool during_initial_mark,
295                                                     bool during_conc_mark) {
296   // We always recreate the prev marking info and we&#39;ll explicitly
297   // mark all objects we find to be self-forwarded on the prev
298   // bitmap. So all objects need to be below PTAMS.
299   _prev_marked_bytes = 0;
300 
301   if (during_initial_mark) {
302     // During initial-mark, we&#39;ll also explicitly mark all objects
303     // we find to be self-forwarded on the next bitmap. So all
304     // objects need to be below NTAMS.
305     _next_top_at_mark_start = top();
306     _next_marked_bytes = 0;
307   } else if (during_conc_mark) {
308     // During concurrent mark, all objects in the CSet (including
309     // the ones we find to be self-forwarded) are implicitly live.
310     // So all objects need to be above NTAMS.
311     _next_top_at_mark_start = bottom();
312     _next_marked_bytes = 0;
313   }
314 }
315 
316 void HeapRegion::note_self_forwarding_removal_end(size_t marked_bytes) {
317   assert(marked_bytes &lt;= used(),
318          &quot;marked: &quot; SIZE_FORMAT &quot; used: &quot; SIZE_FORMAT, marked_bytes, used());
319   _prev_top_at_mark_start = top();
320   _prev_marked_bytes = marked_bytes;
321 }
322 
323 // Code roots support
324 
325 void HeapRegion::add_strong_code_root(nmethod* nm) {
326   HeapRegionRemSet* hrrs = rem_set();
327   hrrs-&gt;add_strong_code_root(nm);
328 }
329 
330 void HeapRegion::add_strong_code_root_locked(nmethod* nm) {
331   assert_locked_or_safepoint(CodeCache_lock);
332   HeapRegionRemSet* hrrs = rem_set();
333   hrrs-&gt;add_strong_code_root_locked(nm);
334 }
335 
336 void HeapRegion::remove_strong_code_root(nmethod* nm) {
337   HeapRegionRemSet* hrrs = rem_set();
338   hrrs-&gt;remove_strong_code_root(nm);
339 }
340 
341 void HeapRegion::strong_code_roots_do(CodeBlobClosure* blk) const {
342   HeapRegionRemSet* hrrs = rem_set();
343   hrrs-&gt;strong_code_roots_do(blk);
344 }
345 
346 class VerifyStrongCodeRootOopClosure: public OopClosure {
347   const HeapRegion* _hr;
348   bool _failures;
349   bool _has_oops_in_region;
350 
351   template &lt;class T&gt; void do_oop_work(T* p) {
352     T heap_oop = RawAccess&lt;&gt;::oop_load(p);
353     if (!CompressedOops::is_null(heap_oop)) {
354       oop obj = CompressedOops::decode_not_null(heap_oop);
355 
356       // Note: not all the oops embedded in the nmethod are in the
357       // current region. We only look at those which are.
358       if (_hr-&gt;is_in(obj)) {
359         // Object is in the region. Check that its less than top
360         if (_hr-&gt;top() &lt;= cast_from_oop&lt;HeapWord*&gt;(obj)) {
361           // Object is above top
362           log_error(gc, verify)(&quot;Object &quot; PTR_FORMAT &quot; in region &quot; HR_FORMAT &quot; is above top &quot;,
363                                 p2i(obj), HR_FORMAT_PARAMS(_hr));
364           _failures = true;
365           return;
366         }
367         // Nmethod has at least one oop in the current region
368         _has_oops_in_region = true;
369       }
370     }
371   }
372 
373 public:
374   VerifyStrongCodeRootOopClosure(const HeapRegion* hr):
375     _hr(hr), _failures(false), _has_oops_in_region(false) {}
376 
377   void do_oop(narrowOop* p) { do_oop_work(p); }
378   void do_oop(oop* p)       { do_oop_work(p); }
379 
380   bool failures()           { return _failures; }
381   bool has_oops_in_region() { return _has_oops_in_region; }
382 };
383 
384 class VerifyStrongCodeRootCodeBlobClosure: public CodeBlobClosure {
385   const HeapRegion* _hr;
386   bool _failures;
387 public:
388   VerifyStrongCodeRootCodeBlobClosure(const HeapRegion* hr) :
389     _hr(hr), _failures(false) {}
390 
391   void do_code_blob(CodeBlob* cb) {
392     nmethod* nm = (cb == NULL) ? NULL : cb-&gt;as_compiled_method()-&gt;as_nmethod_or_null();
393     if (nm != NULL) {
394       // Verify that the nemthod is live
395       if (!nm-&gt;is_alive()) {
396         log_error(gc, verify)(&quot;region [&quot; PTR_FORMAT &quot;,&quot; PTR_FORMAT &quot;] has dead nmethod &quot; PTR_FORMAT &quot; in its strong code roots&quot;,
397                               p2i(_hr-&gt;bottom()), p2i(_hr-&gt;end()), p2i(nm));
398         _failures = true;
399       } else {
400         VerifyStrongCodeRootOopClosure oop_cl(_hr);
401         nm-&gt;oops_do(&amp;oop_cl);
402         if (!oop_cl.has_oops_in_region()) {
403           log_error(gc, verify)(&quot;region [&quot; PTR_FORMAT &quot;,&quot; PTR_FORMAT &quot;] has nmethod &quot; PTR_FORMAT &quot; in its strong code roots with no pointers into region&quot;,
404                                 p2i(_hr-&gt;bottom()), p2i(_hr-&gt;end()), p2i(nm));
405           _failures = true;
406         } else if (oop_cl.failures()) {
407           log_error(gc, verify)(&quot;region [&quot; PTR_FORMAT &quot;,&quot; PTR_FORMAT &quot;] has other failures for nmethod &quot; PTR_FORMAT,
408                                 p2i(_hr-&gt;bottom()), p2i(_hr-&gt;end()), p2i(nm));
409           _failures = true;
410         }
411       }
412     }
413   }
414 
415   bool failures()       { return _failures; }
416 };
417 
418 void HeapRegion::verify_strong_code_roots(VerifyOption vo, bool* failures) const {
419   if (!G1VerifyHeapRegionCodeRoots) {
420     // We&#39;re not verifying code roots.
421     return;
422   }
423   if (vo == VerifyOption_G1UseFullMarking) {
424     // Marking verification during a full GC is performed after class
425     // unloading, code cache unloading, etc so the strong code roots
426     // attached to each heap region are in an inconsistent state. They won&#39;t
427     // be consistent until the strong code roots are rebuilt after the
428     // actual GC. Skip verifying the strong code roots in this particular
429     // time.
430     assert(VerifyDuringGC, &quot;only way to get here&quot;);
431     return;
432   }
433 
434   HeapRegionRemSet* hrrs = rem_set();
435   size_t strong_code_roots_length = hrrs-&gt;strong_code_roots_list_length();
436 
437   // if this region is empty then there should be no entries
438   // on its strong code root list
439   if (is_empty()) {
440     if (strong_code_roots_length &gt; 0) {
441       log_error(gc, verify)(&quot;region &quot; HR_FORMAT &quot; is empty but has &quot; SIZE_FORMAT &quot; code root entries&quot;,
442                             HR_FORMAT_PARAMS(this), strong_code_roots_length);
443       *failures = true;
444     }
445     return;
446   }
447 
448   if (is_continues_humongous()) {
449     if (strong_code_roots_length &gt; 0) {
450       log_error(gc, verify)(&quot;region &quot; HR_FORMAT &quot; is a continuation of a humongous region but has &quot; SIZE_FORMAT &quot; code root entries&quot;,
451                             HR_FORMAT_PARAMS(this), strong_code_roots_length);
452       *failures = true;
453     }
454     return;
455   }
456 
457   VerifyStrongCodeRootCodeBlobClosure cb_cl(this);
458   strong_code_roots_do(&amp;cb_cl);
459 
460   if (cb_cl.failures()) {
461     *failures = true;
462   }
463 }
464 
465 void HeapRegion::print() const { print_on(tty); }
466 
467 void HeapRegion::print_on(outputStream* st) const {
468   st-&gt;print(&quot;|%4u&quot;, this-&gt;_hrm_index);
469   st-&gt;print(&quot;|&quot; PTR_FORMAT &quot;, &quot; PTR_FORMAT &quot;, &quot; PTR_FORMAT,
470             p2i(bottom()), p2i(top()), p2i(end()));
471   st-&gt;print(&quot;|%3d%%&quot;, (int) ((double) used() * 100 / capacity()));
472   st-&gt;print(&quot;|%2s&quot;, get_short_type_str());
473   if (in_collection_set()) {
474     st-&gt;print(&quot;|CS&quot;);
475   } else {
476     st-&gt;print(&quot;|  &quot;);
477   }
478   st-&gt;print(&quot;|TAMS &quot; PTR_FORMAT &quot;, &quot; PTR_FORMAT &quot;| %s &quot;,
479                p2i(prev_top_at_mark_start()), p2i(next_top_at_mark_start()), rem_set()-&gt;get_state_str());
480   if (UseNUMA) {
481     G1NUMA* numa = G1NUMA::numa();
482     if (node_index() &lt; numa-&gt;num_active_nodes()) {
483       st-&gt;print(&quot;|%d&quot;, numa-&gt;numa_id(node_index()));
484     } else {
485       st-&gt;print(&quot;|-&quot;);
486     }
487   }
488   st-&gt;print_cr(&quot;&quot;);
489 }
490 
491 class G1VerificationClosure : public BasicOopIterateClosure {
492 protected:
493   G1CollectedHeap* _g1h;
494   G1CardTable *_ct;
495   oop _containing_obj;
496   bool _failures;
497   int _n_failures;
498   VerifyOption _vo;
499 public:
500   // _vo == UsePrevMarking -&gt; use &quot;prev&quot; marking information,
501   // _vo == UseNextMarking -&gt; use &quot;next&quot; marking information,
502   // _vo == UseFullMarking -&gt; use &quot;next&quot; marking bitmap but no TAMS.
503   G1VerificationClosure(G1CollectedHeap* g1h, VerifyOption vo) :
504     _g1h(g1h), _ct(g1h-&gt;card_table()),
505     _containing_obj(NULL), _failures(false), _n_failures(0), _vo(vo) {
506   }
507 
508   void set_containing_obj(oop obj) {
509     _containing_obj = obj;
510   }
511 
512   bool failures() { return _failures; }
513   int n_failures() { return _n_failures; }
514 
515   void print_object(outputStream* out, oop obj) {
516 #ifdef PRODUCT
517     Klass* k = obj-&gt;klass();
518     const char* class_name = k-&gt;external_name();
519     out-&gt;print_cr(&quot;class name %s&quot;, class_name);
520 #else // PRODUCT
521     obj-&gt;print_on(out);
522 #endif // PRODUCT
523   }
524 
525   // This closure provides its own oop verification code.
526   debug_only(virtual bool should_verify_oops() { return false; })
527 };
528 
529 class VerifyLiveClosure : public G1VerificationClosure {
530 public:
531   VerifyLiveClosure(G1CollectedHeap* g1h, VerifyOption vo) : G1VerificationClosure(g1h, vo) {}
532   virtual void do_oop(narrowOop* p) { do_oop_work(p); }
533   virtual void do_oop(oop* p) { do_oop_work(p); }
534 
535   template &lt;class T&gt;
536   void do_oop_work(T* p) {
537     assert(_containing_obj != NULL, &quot;Precondition&quot;);
538     assert(!_g1h-&gt;is_obj_dead_cond(_containing_obj, _vo),
539       &quot;Precondition&quot;);
540     verify_liveness(p);
541   }
542 
543   template &lt;class T&gt;
544   void verify_liveness(T* p) {
545     T heap_oop = RawAccess&lt;&gt;::oop_load(p);
546     Log(gc, verify) log;
547     if (!CompressedOops::is_null(heap_oop)) {
548       oop obj = CompressedOops::decode_not_null(heap_oop);
549       bool failed = false;
550       if (!_g1h-&gt;is_in(obj) || _g1h-&gt;is_obj_dead_cond(obj, _vo)) {
551         MutexLocker x(ParGCRareEvent_lock,
552           Mutex::_no_safepoint_check_flag);
553 
554         if (!_failures) {
555           log.error(&quot;----------&quot;);
556         }
557         ResourceMark rm;
558         if (!_g1h-&gt;is_in(obj)) {
559           HeapRegion* from = _g1h-&gt;heap_region_containing((HeapWord*)p);
560           log.error(&quot;Field &quot; PTR_FORMAT &quot; of live obj &quot; PTR_FORMAT &quot; in region &quot; HR_FORMAT,
561                     p2i(p), p2i(_containing_obj), HR_FORMAT_PARAMS(from));
562           LogStream ls(log.error());
563           print_object(&amp;ls, _containing_obj);
564           HeapRegion* const to = _g1h-&gt;heap_region_containing(obj);
565           log.error(&quot;points to obj &quot; PTR_FORMAT &quot; in region &quot; HR_FORMAT &quot; remset %s&quot;,
566                     p2i(obj), HR_FORMAT_PARAMS(to), to-&gt;rem_set()-&gt;get_state_str());
567         } else {
568           HeapRegion* from = _g1h-&gt;heap_region_containing((HeapWord*)p);
569           HeapRegion* to = _g1h-&gt;heap_region_containing(obj);
570           log.error(&quot;Field &quot; PTR_FORMAT &quot; of live obj &quot; PTR_FORMAT &quot; in region &quot; HR_FORMAT,
571                     p2i(p), p2i(_containing_obj), HR_FORMAT_PARAMS(from));
572           LogStream ls(log.error());
573           print_object(&amp;ls, _containing_obj);
574           log.error(&quot;points to dead obj &quot; PTR_FORMAT &quot; in region &quot; HR_FORMAT,
575                     p2i(obj), HR_FORMAT_PARAMS(to));
576           print_object(&amp;ls, obj);
577         }
578         log.error(&quot;----------&quot;);
579         _failures = true;
580         failed = true;
581         _n_failures++;
582       }
583     }
584   }
585 };
586 
587 class VerifyRemSetClosure : public G1VerificationClosure {
588 public:
589   VerifyRemSetClosure(G1CollectedHeap* g1h, VerifyOption vo) : G1VerificationClosure(g1h, vo) {}
590   virtual void do_oop(narrowOop* p) { do_oop_work(p); }
591   virtual void do_oop(oop* p) { do_oop_work(p); }
592 
593   template &lt;class T&gt;
594   void do_oop_work(T* p) {
595     assert(_containing_obj != NULL, &quot;Precondition&quot;);
596     assert(!_g1h-&gt;is_obj_dead_cond(_containing_obj, _vo),
597       &quot;Precondition&quot;);
598     verify_remembered_set(p);
599   }
600 
601   template &lt;class T&gt;
602   void verify_remembered_set(T* p) {
603     T heap_oop = RawAccess&lt;&gt;::oop_load(p);
604     Log(gc, verify) log;
605     if (!CompressedOops::is_null(heap_oop)) {
606       oop obj = CompressedOops::decode_not_null(heap_oop);
607       HeapRegion* from = _g1h-&gt;heap_region_containing((HeapWord*)p);
608       HeapRegion* to = _g1h-&gt;heap_region_containing(obj);
609       if (from != NULL &amp;&amp; to != NULL &amp;&amp;
610         from != to &amp;&amp;
611         !to-&gt;is_pinned() &amp;&amp;
612         to-&gt;rem_set()-&gt;is_complete()) {
613         jbyte cv_obj = *_ct-&gt;byte_for_const(_containing_obj);
614         jbyte cv_field = *_ct-&gt;byte_for_const(p);
615         const jbyte dirty = G1CardTable::dirty_card_val();
616 
617         bool is_bad = !(from-&gt;is_young()
618           || to-&gt;rem_set()-&gt;contains_reference(p)
619           || (_containing_obj-&gt;is_objArray() ?
620                 cv_field == dirty :
621                 cv_obj == dirty || cv_field == dirty));
622         if (is_bad) {
623           MutexLocker x(ParGCRareEvent_lock,
624             Mutex::_no_safepoint_check_flag);
625 
626           if (!_failures) {
627             log.error(&quot;----------&quot;);
628           }
629           log.error(&quot;Missing rem set entry:&quot;);
630           log.error(&quot;Field &quot; PTR_FORMAT &quot; of obj &quot; PTR_FORMAT &quot; in region &quot; HR_FORMAT,
631                     p2i(p), p2i(_containing_obj), HR_FORMAT_PARAMS(from));
632           ResourceMark rm;
633           LogStream ls(log.error());
634           _containing_obj-&gt;print_on(&amp;ls);
635           log.error(&quot;points to obj &quot; PTR_FORMAT &quot; in region &quot; HR_FORMAT &quot; remset %s&quot;,
636                     p2i(obj), HR_FORMAT_PARAMS(to), to-&gt;rem_set()-&gt;get_state_str());
637           if (oopDesc::is_oop(obj)) {
638             obj-&gt;print_on(&amp;ls);
639           }
640           log.error(&quot;Obj head CTE = %d, field CTE = %d.&quot;, cv_obj, cv_field);
641           log.error(&quot;----------&quot;);
642           _failures = true;
643           _n_failures++;
644         }
645       }
646     }
647   }
648 };
649 
650 // Closure that applies the given two closures in sequence.
651 class G1Mux2Closure : public BasicOopIterateClosure {
652   OopClosure* _c1;
653   OopClosure* _c2;
654 public:
655   G1Mux2Closure(OopClosure *c1, OopClosure *c2) { _c1 = c1; _c2 = c2; }
656   template &lt;class T&gt; inline void do_oop_work(T* p) {
657     // Apply first closure; then apply the second.
658     _c1-&gt;do_oop(p);
659     _c2-&gt;do_oop(p);
660   }
661   virtual inline void do_oop(oop* p) { do_oop_work(p); }
662   virtual inline void do_oop(narrowOop* p) { do_oop_work(p); }
663 
664   // This closure provides its own oop verification code.
665   debug_only(virtual bool should_verify_oops() { return false; })
666 };
667 
668 void HeapRegion::verify(VerifyOption vo,
669                         bool* failures) const {
670   G1CollectedHeap* g1h = G1CollectedHeap::heap();
671   *failures = false;
672   HeapWord* p = bottom();
673   HeapWord* prev_p = NULL;
674   VerifyLiveClosure vl_cl(g1h, vo);
675   VerifyRemSetClosure vr_cl(g1h, vo);
676   bool is_region_humongous = is_humongous();
677   size_t object_num = 0;
678   while (p &lt; top()) {
679     oop obj = oop(p);
680     size_t obj_size = block_size(p);
681     object_num += 1;
682 
683     if (!g1h-&gt;is_obj_dead_cond(obj, this, vo)) {
684       if (oopDesc::is_oop(obj)) {
685         Klass* klass = obj-&gt;klass();
686         bool is_metaspace_object = Metaspace::contains(klass);
687         if (!is_metaspace_object) {
688           log_error(gc, verify)(&quot;klass &quot; PTR_FORMAT &quot; of object &quot; PTR_FORMAT &quot; &quot;
689                                 &quot;not metadata&quot;, p2i(klass), p2i(obj));
690           *failures = true;
691           return;
692         } else if (!klass-&gt;is_klass()) {
693           log_error(gc, verify)(&quot;klass &quot; PTR_FORMAT &quot; of object &quot; PTR_FORMAT &quot; &quot;
694                                 &quot;not a klass&quot;, p2i(klass), p2i(obj));
695           *failures = true;
696           return;
697         } else {
698           vl_cl.set_containing_obj(obj);
699           if (!g1h-&gt;collector_state()-&gt;in_full_gc() || G1VerifyRSetsDuringFullGC) {
700             // verify liveness and rem_set
701             vr_cl.set_containing_obj(obj);
702             G1Mux2Closure mux(&amp;vl_cl, &amp;vr_cl);
703             obj-&gt;oop_iterate(&amp;mux);
704 
705             if (vr_cl.failures()) {
706               *failures = true;
707             }
708             if (G1MaxVerifyFailures &gt;= 0 &amp;&amp;
709               vr_cl.n_failures() &gt;= G1MaxVerifyFailures) {
710               return;
711             }
712           } else {
713             // verify only liveness
714             obj-&gt;oop_iterate(&amp;vl_cl);
715           }
716           if (vl_cl.failures()) {
717             *failures = true;
718           }
719           if (G1MaxVerifyFailures &gt;= 0 &amp;&amp;
720               vl_cl.n_failures() &gt;= G1MaxVerifyFailures) {
721             return;
722           }
723         }
724       } else {
725         log_error(gc, verify)(PTR_FORMAT &quot; not an oop&quot;, p2i(obj));
726         *failures = true;
727         return;
728       }
729     }
730     prev_p = p;
731     p += obj_size;
732   }
733 
734   if (!is_young() &amp;&amp; !is_empty()) {
735     _bot_part.verify();
736   }
737 
738   if (is_region_humongous) {
739     oop obj = oop(this-&gt;humongous_start_region()-&gt;bottom());
740     if (cast_from_oop&lt;HeapWord*&gt;(obj) &gt; bottom() || cast_from_oop&lt;HeapWord*&gt;(obj) + obj-&gt;size() &lt; bottom()) {
741       log_error(gc, verify)(&quot;this humongous region is not part of its&#39; humongous object &quot; PTR_FORMAT, p2i(obj));
742       *failures = true;
743       return;
744     }
745   }
746 
747   if (!is_region_humongous &amp;&amp; p != top()) {
748     log_error(gc, verify)(&quot;end of last object &quot; PTR_FORMAT &quot; &quot;
749                           &quot;does not match top &quot; PTR_FORMAT, p2i(p), p2i(top()));
750     *failures = true;
751     return;
752   }
753 
754   HeapWord* the_end = end();
755   // Do some extra BOT consistency checking for addresses in the
756   // range [top, end). BOT look-ups in this range should yield
757   // top. No point in doing that if top == end (there&#39;s nothing there).
758   if (p &lt; the_end) {
759     // Look up top
760     HeapWord* addr_1 = p;
761     HeapWord* b_start_1 = block_start_const(addr_1);
762     if (b_start_1 != p) {
763       log_error(gc, verify)(&quot;BOT look up for top: &quot; PTR_FORMAT &quot; &quot;
764                             &quot; yielded &quot; PTR_FORMAT &quot;, expecting &quot; PTR_FORMAT,
765                             p2i(addr_1), p2i(b_start_1), p2i(p));
766       *failures = true;
767       return;
768     }
769 
770     // Look up top + 1
771     HeapWord* addr_2 = p + 1;
772     if (addr_2 &lt; the_end) {
773       HeapWord* b_start_2 = block_start_const(addr_2);
774       if (b_start_2 != p) {
775         log_error(gc, verify)(&quot;BOT look up for top + 1: &quot; PTR_FORMAT &quot; &quot;
776                               &quot; yielded &quot; PTR_FORMAT &quot;, expecting &quot; PTR_FORMAT,
777                               p2i(addr_2), p2i(b_start_2), p2i(p));
778         *failures = true;
779         return;
780       }
781     }
782 
783     // Look up an address between top and end
784     size_t diff = pointer_delta(the_end, p) / 2;
785     HeapWord* addr_3 = p + diff;
786     if (addr_3 &lt; the_end) {
787       HeapWord* b_start_3 = block_start_const(addr_3);
788       if (b_start_3 != p) {
789         log_error(gc, verify)(&quot;BOT look up for top + diff: &quot; PTR_FORMAT &quot; &quot;
790                               &quot; yielded &quot; PTR_FORMAT &quot;, expecting &quot; PTR_FORMAT,
791                               p2i(addr_3), p2i(b_start_3), p2i(p));
792         *failures = true;
793         return;
794       }
795     }
796 
797     // Look up end - 1
798     HeapWord* addr_4 = the_end - 1;
799     HeapWord* b_start_4 = block_start_const(addr_4);
800     if (b_start_4 != p) {
801       log_error(gc, verify)(&quot;BOT look up for end - 1: &quot; PTR_FORMAT &quot; &quot;
802                             &quot; yielded &quot; PTR_FORMAT &quot;, expecting &quot; PTR_FORMAT,
803                             p2i(addr_4), p2i(b_start_4), p2i(p));
804       *failures = true;
805       return;
806     }
807   }
808 
809   verify_strong_code_roots(vo, failures);
810 }
811 
812 void HeapRegion::verify() const {
813   bool dummy = false;
814   verify(VerifyOption_G1UsePrevMarking, /* failures */ &amp;dummy);
815 }
816 
817 void HeapRegion::verify_rem_set(VerifyOption vo, bool* failures) const {
818   G1CollectedHeap* g1h = G1CollectedHeap::heap();
819   *failures = false;
820   HeapWord* p = bottom();
821   HeapWord* prev_p = NULL;
822   VerifyRemSetClosure vr_cl(g1h, vo);
823   while (p &lt; top()) {
824     oop obj = oop(p);
825     size_t obj_size = block_size(p);
826 
827     if (!g1h-&gt;is_obj_dead_cond(obj, this, vo)) {
828       if (oopDesc::is_oop(obj)) {
829         vr_cl.set_containing_obj(obj);
830         obj-&gt;oop_iterate(&amp;vr_cl);
831 
832         if (vr_cl.failures()) {
833           *failures = true;
834         }
835         if (G1MaxVerifyFailures &gt;= 0 &amp;&amp;
836           vr_cl.n_failures() &gt;= G1MaxVerifyFailures) {
837           return;
838         }
839       } else {
840         log_error(gc, verify)(PTR_FORMAT &quot; not an oop&quot;, p2i(obj));
841         *failures = true;
842         return;
843       }
844     }
845 
846     prev_p = p;
847     p += obj_size;
848   }
849 }
850 
851 void HeapRegion::verify_rem_set() const {
852   bool failures = false;
853   verify_rem_set(VerifyOption_G1UsePrevMarking, &amp;failures);
854   guarantee(!failures, &quot;HeapRegion RemSet verification failed&quot;);
855 }
856 
857 void HeapRegion::clear(bool mangle_space) {
858   set_top(bottom());
859   set_compaction_top(bottom());
860 
861   if (ZapUnusedHeapArea &amp;&amp; mangle_space) {
862     mangle_unused_area();
863   }
864   reset_bot();
865 }
866 
867 #ifndef PRODUCT
868 void HeapRegion::mangle_unused_area() {
869   SpaceMangler::mangle_region(MemRegion(top(), end()));
870 }
871 #endif
872 
873 HeapWord* HeapRegion::initialize_threshold() {
874   return _bot_part.initialize_threshold();
875 }
876 
877 HeapWord* HeapRegion::cross_threshold(HeapWord* start, HeapWord* end) {
878   _bot_part.alloc_block(start, end);
879   return _bot_part.threshold();
880 }
881 
882 void HeapRegion::object_iterate(ObjectClosure* blk) {
883   HeapWord* p = bottom();
884   while (p &lt; top()) {
885     if (block_is_obj(p)) {
886       blk-&gt;do_object(oop(p));
887     }
888     p += block_size(p);
889   }
890 }
    </pre>
  </body>
</html>