diff a/src/hotspot/share/gc/g1/heapRegion.inline.hpp b/src/hotspot/share/gc/g1/heapRegion.inline.hpp
--- a/src/hotspot/share/gc/g1/heapRegion.inline.hpp
+++ b/src/hotspot/share/gc/g1/heapRegion.inline.hpp
@@ -26,99 +26,99 @@
 #define SHARE_GC_G1_HEAPREGION_INLINE_HPP
 
 #include "gc/g1/g1BlockOffsetTable.inline.hpp"
 #include "gc/g1/g1CollectedHeap.inline.hpp"
 #include "gc/g1/g1ConcurrentMarkBitMap.inline.hpp"
+#include "gc/g1/g1Predictions.hpp"
 #include "gc/g1/heapRegion.hpp"
-#include "gc/shared/space.hpp"
 #include "oops/oop.inline.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/prefetch.inline.hpp"
 #include "utilities/align.hpp"
+#include "utilities/globalDefinitions.hpp"
 
-inline HeapWord* G1ContiguousSpace::allocate_impl(size_t min_word_size,
-                                                  size_t desired_word_size,
-                                                  size_t* actual_size) {
+inline HeapWord* HeapRegion::allocate_impl(size_t min_word_size,
+                                           size_t desired_word_size,
+                                           size_t* actual_size) {
   HeapWord* obj = top();
   size_t available = pointer_delta(end(), obj);
   size_t want_to_allocate = MIN2(available, desired_word_size);
   if (want_to_allocate >= min_word_size) {
     HeapWord* new_top = obj + want_to_allocate;
     set_top(new_top);
-    assert(is_aligned(obj) && is_aligned(new_top), "checking alignment");
+    assert(is_object_aligned(obj) && is_object_aligned(new_top), "checking alignment");
     *actual_size = want_to_allocate;
     return obj;
   } else {
     return NULL;
   }
 }
 
-inline HeapWord* G1ContiguousSpace::par_allocate_impl(size_t min_word_size,
-                                                      size_t desired_word_size,
-                                                      size_t* actual_size) {
+inline HeapWord* HeapRegion::par_allocate_impl(size_t min_word_size,
+                                               size_t desired_word_size,
+                                               size_t* actual_size) {
   do {
     HeapWord* obj = top();
     size_t available = pointer_delta(end(), obj);
     size_t want_to_allocate = MIN2(available, desired_word_size);
     if (want_to_allocate >= min_word_size) {
       HeapWord* new_top = obj + want_to_allocate;
-      HeapWord* result = Atomic::cmpxchg(new_top, top_addr(), obj);
+      HeapWord* result = Atomic::cmpxchg(&_top, obj, new_top);
       // result can be one of two:
       //  the old top value: the exchange succeeded
       //  otherwise: the new value of the top is returned.
       if (result == obj) {
-        assert(is_aligned(obj) && is_aligned(new_top), "checking alignment");
+        assert(is_object_aligned(obj) && is_object_aligned(new_top), "checking alignment");
         *actual_size = want_to_allocate;
         return obj;
       }
     } else {
       return NULL;
     }
   } while (true);
 }
 
-inline HeapWord* G1ContiguousSpace::allocate(size_t min_word_size,
-                                             size_t desired_word_size,
-                                             size_t* actual_size) {
+inline HeapWord* HeapRegion::allocate(size_t min_word_size,
+                                      size_t desired_word_size,
+                                      size_t* actual_size) {
   HeapWord* res = allocate_impl(min_word_size, desired_word_size, actual_size);
   if (res != NULL) {
     _bot_part.alloc_block(res, *actual_size);
   }
   return res;
 }
 
-inline HeapWord* G1ContiguousSpace::allocate(size_t word_size) {
+inline HeapWord* HeapRegion::allocate(size_t word_size) {
   size_t temp;
   return allocate(word_size, word_size, &temp);
 }
 
-inline HeapWord* G1ContiguousSpace::par_allocate(size_t word_size) {
+inline HeapWord* HeapRegion::par_allocate(size_t word_size) {
   size_t temp;
   return par_allocate(word_size, word_size, &temp);
 }
 
 // Because of the requirement of keeping "_offsets" up to date with the
 // allocations, we sequentialize these with a lock.  Therefore, best if
 // this is used for larger LAB allocations only.
-inline HeapWord* G1ContiguousSpace::par_allocate(size_t min_word_size,
-                                                 size_t desired_word_size,
-                                                 size_t* actual_size) {
+inline HeapWord* HeapRegion::par_allocate(size_t min_word_size,
+                                          size_t desired_word_size,
+                                          size_t* actual_size) {
   MutexLocker x(&_par_alloc_lock);
   return allocate(min_word_size, desired_word_size, actual_size);
 }
 
-inline HeapWord* G1ContiguousSpace::block_start(const void* p) {
+inline HeapWord* HeapRegion::block_start(const void* p) {
   return _bot_part.block_start(p);
 }
 
-inline HeapWord*
-G1ContiguousSpace::block_start_const(const void* p) const {
+inline HeapWord* HeapRegion::block_start_const(const void* p) const {
   return _bot_part.block_start_const(p);
 }
 
 inline bool HeapRegion::is_obj_dead_with_size(const oop obj, const G1CMBitMap* const prev_bitmap, size_t* size) const {
-  HeapWord* addr = (HeapWord*) obj;
+  HeapWord* addr = cast_from_oop<HeapWord*>(obj);
 
   assert(addr < top(), "must be");
   assert(!is_closed_archive(),
          "Closed archive regions should not have references into other regions");
   assert(!is_humongous(), "Humongous objects not handled here");
@@ -132,12 +132,11 @@
     *size = obj->size();
   }
   return obj_is_dead;
 }
 
-inline bool
-HeapRegion::block_is_obj(const HeapWord* p) const {
+inline bool HeapRegion::block_is_obj(const HeapWord* p) const {
   G1CollectedHeap* g1h = G1CollectedHeap::heap();
 
   if (!this->is_in(p)) {
     assert(is_continues_humongous(), "This case can only happen for humongous regions");
     return (p == humongous_start_region()->bottom());
@@ -164,11 +163,11 @@
 }
 
 inline bool HeapRegion::is_obj_dead(const oop obj, const G1CMBitMap* const prev_bitmap) const {
   assert(is_in_reserved(obj), "Object " PTR_FORMAT " must be in region", p2i(obj));
   return !obj_allocated_since_prev_marking(obj) &&
-         !prev_bitmap->is_marked((HeapWord*)obj) &&
+         !prev_bitmap->is_marked(obj) &&
          !is_open_archive();
 }
 
 inline size_t HeapRegion::block_size(const HeapWord *addr) const {
   if (addr == top()) {
@@ -183,11 +182,11 @@
 }
 
 inline void HeapRegion::complete_compaction() {
   // Reset space and bot after compaction is complete if needed.
   reset_after_compaction();
-  if (used_region().is_empty()) {
+  if (is_empty()) {
     reset_bot();
   }
 
   // After a compaction the mark bitmap is invalid, so we must
   // treat all objects as being inside the unmarked area.
@@ -200,11 +199,11 @@
   }
 }
 
 template<typename ApplyToMarkedClosure>
 inline void HeapRegion::apply_to_marked_objects(G1CMBitMap* bitmap, ApplyToMarkedClosure* closure) {
-  HeapWord* limit = scan_limit();
+  HeapWord* limit = top();
   HeapWord* next_addr = bottom();
 
   while (next_addr < limit) {
     Prefetch::write(next_addr, PrefetchScanIntervalInBytes);
     // This explicit is_marked check is a way to avoid
@@ -255,13 +254,13 @@
 inline bool HeapRegion::in_collection_set() const {
   return G1CollectedHeap::heap()->is_in_cset(this);
 }
 
 template <class Closure, bool is_gc_active>
-bool HeapRegion::do_oops_on_card_in_humongous(MemRegion mr,
-                                              Closure* cl,
-                                              G1CollectedHeap* g1h) {
+HeapWord* HeapRegion::do_oops_on_memregion_in_humongous(MemRegion mr,
+                                                        Closure* cl,
+                                                        G1CollectedHeap* g1h) {
   assert(is_humongous(), "precondition");
   HeapRegion* sr = humongous_start_region();
   oop obj = oop(sr->bottom());
 
   // If concurrent and klass_or_null is NULL, then space has been
@@ -269,45 +268,52 @@
   // the klass.  That can only happen if the card is stale.  However,
   // we've already set the card clean, so we must return failure,
   // since the allocating thread could have performed a write to the
   // card that might be missed otherwise.
   if (!is_gc_active && (obj->klass_or_null_acquire() == NULL)) {
-    return false;
+    return NULL;
   }
 
   // We have a well-formed humongous object at the start of sr.
   // Only filler objects follow a humongous object in the containing
   // regions, and we can ignore those.  So only process the one
   // humongous object.
-  if (!g1h->is_obj_dead(obj, sr)) {
-    if (obj->is_objArray() || (sr->bottom() < mr.start())) {
-      // objArrays are always marked precisely, so limit processing
-      // with mr.  Non-objArrays might be precisely marked, and since
-      // it's humongous it's worthwhile avoiding full processing.
-      // However, the card could be stale and only cover filler
-      // objects.  That should be rare, so not worth checking for;
-      // instead let it fall out from the bounded iteration.
-      obj->oop_iterate(cl, mr);
-    } else {
-      // If obj is not an objArray and mr contains the start of the
-      // obj, then this could be an imprecise mark, and we need to
-      // process the entire object.
-      obj->oop_iterate(cl);
-    }
+  if (g1h->is_obj_dead(obj, sr)) {
+    // The object is dead. There can be no other object in this region, so return
+    // the end of that region.
+    return end();
+  }
+  if (obj->is_objArray() || (sr->bottom() < mr.start())) {
+    // objArrays are always marked precisely, so limit processing
+    // with mr.  Non-objArrays might be precisely marked, and since
+    // it's humongous it's worthwhile avoiding full processing.
+    // However, the card could be stale and only cover filler
+    // objects.  That should be rare, so not worth checking for;
+    // instead let it fall out from the bounded iteration.
+    obj->oop_iterate(cl, mr);
+    return mr.end();
+  } else {
+    // If obj is not an objArray and mr contains the start of the
+    // obj, then this could be an imprecise mark, and we need to
+    // process the entire object.
+    int size = obj->oop_iterate_size(cl);
+    // We have scanned to the end of the object, but since there can be no objects
+    // after this humongous object in the region, we can return the end of the
+    // region if it is greater.
+    return MAX2(cast_from_oop<HeapWord*>(obj) + size, mr.end());
   }
-  return true;
 }
 
 template <bool is_gc_active, class Closure>
-bool HeapRegion::oops_on_card_seq_iterate_careful(MemRegion mr,
-                                                  Closure* cl) {
+HeapWord* HeapRegion::oops_on_memregion_seq_iterate_careful(MemRegion mr,
+                                                            Closure* cl) {
   assert(MemRegion(bottom(), end()).contains(mr), "Card region not in heap region");
   G1CollectedHeap* g1h = G1CollectedHeap::heap();
 
   // Special handling for humongous regions.
   if (is_humongous()) {
-    return do_oops_on_card_in_humongous<Closure, is_gc_active>(mr, cl, g1h);
+    return do_oops_on_memregion_in_humongous<Closure, is_gc_active>(mr, cl, g1h);
   }
   assert(is_old() || is_archive(), "Wrongly trying to iterate over region %u type %s", _hrm_index, get_type_str());
 
   // Because mr has been trimmed to what's been allocated in this
   // region, the parts of the heap that are examined here are always
@@ -332,34 +338,84 @@
            "start: " PTR_FORMAT ", next: " PTR_FORMAT, p2i(start), p2i(next));
   }
 #endif
 
   const G1CMBitMap* const bitmap = g1h->concurrent_mark()->prev_mark_bitmap();
-  do {
+  while (true) {
     oop obj = oop(cur);
     assert(oopDesc::is_oop(obj, true), "Not an oop at " PTR_FORMAT, p2i(cur));
     assert(obj->klass_or_null() != NULL,
            "Unparsable heap at " PTR_FORMAT, p2i(cur));
 
     size_t size;
     bool is_dead = is_obj_dead_with_size(obj, bitmap, &size);
+    bool is_precise = false;
 
     cur += size;
     if (!is_dead) {
       // Process live object's references.
 
       // Non-objArrays are usually marked imprecise at the object
       // start, in which case we need to iterate over them in full.
       // objArrays are precisely marked, but can still be iterated
       // over in full if completely covered.
-      if (!obj->is_objArray() || (((HeapWord*)obj) >= start && cur <= end)) {
+      if (!obj->is_objArray() || (cast_from_oop<HeapWord*>(obj) >= start && cur <= end)) {
         obj->oop_iterate(cl);
       } else {
         obj->oop_iterate(cl, mr);
+        is_precise = true;
       }
     }
-  } while (cur < end);
+    if (cur >= end) {
+      return is_precise ? end : cur;
+    }
+  }
+}
+
+inline int HeapRegion::age_in_surv_rate_group() const {
+  assert(has_surv_rate_group(), "pre-condition");
+  assert(has_valid_age_in_surv_rate(), "pre-condition");
+  return _surv_rate_group->age_in_group(_age_index);
+}
+
+inline bool HeapRegion::has_valid_age_in_surv_rate() const {
+  return G1SurvRateGroup::is_valid_age_index(_age_index);
+}
+
+inline bool HeapRegion::has_surv_rate_group() const {
+  return _surv_rate_group != NULL;
+}
+
+inline double HeapRegion::surv_rate_prediction(G1Predictions const& predictor) const {
+  assert(has_surv_rate_group(), "pre-condition");
+  return _surv_rate_group->surv_rate_pred(predictor, age_in_surv_rate_group());
+}
+
+inline void HeapRegion::install_surv_rate_group(G1SurvRateGroup* surv_rate_group) {
+  assert(surv_rate_group != NULL, "pre-condition");
+  assert(!has_surv_rate_group(), "pre-condition");
+  assert(is_young(), "pre-condition");
+
+  _surv_rate_group = surv_rate_group;
+  _age_index = surv_rate_group->next_age_index();
+}
+
+inline void HeapRegion::uninstall_surv_rate_group() {
+  if (has_surv_rate_group()) {
+    assert(has_valid_age_in_surv_rate(), "pre-condition");
+    assert(is_young(), "pre-condition");
+
+    _surv_rate_group = NULL;
+    _age_index = G1SurvRateGroup::InvalidAgeIndex;
+  } else {
+    assert(!has_valid_age_in_surv_rate(), "pre-condition");
+  }
+}
 
-  return true;
+inline void HeapRegion::record_surv_words_in_group(size_t words_survived) {
+  assert(has_surv_rate_group(), "pre-condition");
+  assert(has_valid_age_in_surv_rate(), "pre-condition");
+  int age_in_group = age_in_surv_rate_group();
+  _surv_rate_group->record_surviving_words(age_in_group, words_survived);
 }
 
 #endif // SHARE_GC_G1_HEAPREGION_INLINE_HPP
