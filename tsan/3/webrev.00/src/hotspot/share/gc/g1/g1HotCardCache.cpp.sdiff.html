<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1HotCardCache.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1HeterogeneousHeapYoungGenSizer.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1HotCardCache.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1HotCardCache.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;

 26 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
 27 #include &quot;gc/g1/g1DirtyCardQueue.hpp&quot;
 28 #include &quot;gc/g1/g1HotCardCache.hpp&quot;
 29 #include &quot;runtime/atomic.hpp&quot;
 30 
 31 G1HotCardCache::G1HotCardCache(G1CollectedHeap *g1h):
 32   _g1h(g1h), _use_cache(false), _card_counts(g1h),
 33   _hot_cache(NULL), _hot_cache_size(0), _hot_cache_par_chunk_size(0),
<span class="line-modified"> 34   _hot_cache_idx(0), _hot_cache_par_claimed_idx(0)</span>
 35 {}
 36 
 37 void G1HotCardCache::initialize(G1RegionToSpaceMapper* card_counts_storage) {
 38   if (default_use_cache()) {
 39     _use_cache = true;
 40 
 41     _hot_cache_size = (size_t)1 &lt;&lt; G1ConcRSLogCacheSize;
 42     _hot_cache = ArrayAllocator&lt;CardValue*&gt;::allocate(_hot_cache_size, mtGC);
 43 
 44     reset_hot_cache_internal();
 45 
 46     // For refining the cards in the hot cache in parallel
 47     _hot_cache_par_chunk_size = ClaimChunkSize;
 48     _hot_cache_par_claimed_idx = 0;
 49 


 50     _card_counts.initialize(card_counts_storage);
 51   }
 52 }
 53 
 54 G1HotCardCache::~G1HotCardCache() {
 55   if (default_use_cache()) {
 56     assert(_hot_cache != NULL, &quot;Logic&quot;);
 57     ArrayAllocator&lt;CardValue*&gt;::free(_hot_cache, _hot_cache_size);
 58     _hot_cache = NULL;
 59   }
 60 }
 61 
 62 CardTable::CardValue* G1HotCardCache::insert(CardValue* card_ptr) {
 63   uint count = _card_counts.add_card_count(card_ptr);
 64   if (!_card_counts.is_hot(count)) {
 65     // The card is not hot so do not store it in the cache;
 66     // return it for immediate refining.
 67     return card_ptr;
 68   }
 69   // Otherwise, the card is hot.
<span class="line-modified"> 70   size_t index = Atomic::add(1u, &amp;_hot_cache_idx) - 1;</span>





 71   size_t masked_index = index &amp; (_hot_cache_size - 1);
 72   CardValue* current_ptr = _hot_cache[masked_index];
 73 
 74   // Try to store the new card pointer into the cache. Compare-and-swap to guard
 75   // against the unlikely event of a race resulting in another card pointer to
 76   // have already been written to the cache. In this case we will return
 77   // card_ptr in favor of the other option, which would be starting over. This
 78   // should be OK since card_ptr will likely be the older card already when/if
 79   // this ever happens.
<span class="line-modified"> 80   CardValue* previous_ptr = Atomic::cmpxchg(card_ptr,</span>
<span class="line-modified"> 81                                             &amp;_hot_cache[masked_index],</span>
<span class="line-modified"> 82                                             current_ptr);</span>
 83   return (previous_ptr == current_ptr) ? previous_ptr : card_ptr;
 84 }
 85 
<span class="line-modified"> 86 void G1HotCardCache::drain(G1CardTableEntryClosure* cl, uint worker_i) {</span>
 87   assert(default_use_cache(), &quot;Drain only necessary if we use the hot card cache.&quot;);
 88 
 89   assert(_hot_cache != NULL, &quot;Logic&quot;);
 90   assert(!use_cache(), &quot;cache should be disabled&quot;);
 91 
 92   while (_hot_cache_par_claimed_idx &lt; _hot_cache_size) {
<span class="line-modified"> 93     size_t end_idx = Atomic::add(_hot_cache_par_chunk_size,</span>
<span class="line-modified"> 94                                  &amp;_hot_cache_par_claimed_idx);</span>
 95     size_t start_idx = end_idx - _hot_cache_par_chunk_size;
 96     // The current worker has successfully claimed the chunk [start_idx..end_idx)
 97     end_idx = MIN2(end_idx, _hot_cache_size);
 98     for (size_t i = start_idx; i &lt; end_idx; i++) {
 99       CardValue* card_ptr = _hot_cache[i];
100       if (card_ptr != NULL) {
<span class="line-modified">101         bool result = cl-&gt;do_card_ptr(card_ptr, worker_i);</span>
<span class="line-removed">102         assert(result, &quot;Closure should always return true&quot;);</span>
103       } else {
104         break;
105       }
106     }
107   }
108 
109   // The existing entries in the hot card cache, which were just refined
110   // above, are discarded prior to re-enabling the cache near the end of the GC.
111 }
112 
113 void G1HotCardCache::reset_card_counts(HeapRegion* hr) {
114   _card_counts.clear_region(hr);
115 }
</pre>
</td>
<td>
<hr />
<pre>
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
<span class="line-added"> 26 #include &quot;gc/g1/g1CardTableEntryClosure.hpp&quot;</span>
 27 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
 28 #include &quot;gc/g1/g1DirtyCardQueue.hpp&quot;
 29 #include &quot;gc/g1/g1HotCardCache.hpp&quot;
 30 #include &quot;runtime/atomic.hpp&quot;
 31 
 32 G1HotCardCache::G1HotCardCache(G1CollectedHeap *g1h):
 33   _g1h(g1h), _use_cache(false), _card_counts(g1h),
 34   _hot_cache(NULL), _hot_cache_size(0), _hot_cache_par_chunk_size(0),
<span class="line-modified"> 35   _hot_cache_idx(0), _hot_cache_par_claimed_idx(0), _cache_wrapped_around(false)</span>
 36 {}
 37 
 38 void G1HotCardCache::initialize(G1RegionToSpaceMapper* card_counts_storage) {
 39   if (default_use_cache()) {
 40     _use_cache = true;
 41 
 42     _hot_cache_size = (size_t)1 &lt;&lt; G1ConcRSLogCacheSize;
 43     _hot_cache = ArrayAllocator&lt;CardValue*&gt;::allocate(_hot_cache_size, mtGC);
 44 
 45     reset_hot_cache_internal();
 46 
 47     // For refining the cards in the hot cache in parallel
 48     _hot_cache_par_chunk_size = ClaimChunkSize;
 49     _hot_cache_par_claimed_idx = 0;
 50 
<span class="line-added"> 51     _cache_wrapped_around = false;</span>
<span class="line-added"> 52 </span>
 53     _card_counts.initialize(card_counts_storage);
 54   }
 55 }
 56 
 57 G1HotCardCache::~G1HotCardCache() {
 58   if (default_use_cache()) {
 59     assert(_hot_cache != NULL, &quot;Logic&quot;);
 60     ArrayAllocator&lt;CardValue*&gt;::free(_hot_cache, _hot_cache_size);
 61     _hot_cache = NULL;
 62   }
 63 }
 64 
 65 CardTable::CardValue* G1HotCardCache::insert(CardValue* card_ptr) {
 66   uint count = _card_counts.add_card_count(card_ptr);
 67   if (!_card_counts.is_hot(count)) {
 68     // The card is not hot so do not store it in the cache;
 69     // return it for immediate refining.
 70     return card_ptr;
 71   }
 72   // Otherwise, the card is hot.
<span class="line-modified"> 73   size_t index = Atomic::fetch_and_add(&amp;_hot_cache_idx, 1u);</span>
<span class="line-added"> 74   if (index == _hot_cache_size) {</span>
<span class="line-added"> 75     // Can use relaxed store because all racing threads are writing the same</span>
<span class="line-added"> 76     // value and there aren&#39;t any concurrent readers.</span>
<span class="line-added"> 77     Atomic::store(&amp;_cache_wrapped_around, true);</span>
<span class="line-added"> 78   }</span>
 79   size_t masked_index = index &amp; (_hot_cache_size - 1);
 80   CardValue* current_ptr = _hot_cache[masked_index];
 81 
 82   // Try to store the new card pointer into the cache. Compare-and-swap to guard
 83   // against the unlikely event of a race resulting in another card pointer to
 84   // have already been written to the cache. In this case we will return
 85   // card_ptr in favor of the other option, which would be starting over. This
 86   // should be OK since card_ptr will likely be the older card already when/if
 87   // this ever happens.
<span class="line-modified"> 88   CardValue* previous_ptr = Atomic::cmpxchg(&amp;_hot_cache[masked_index],</span>
<span class="line-modified"> 89                                             current_ptr,</span>
<span class="line-modified"> 90                                             card_ptr);</span>
 91   return (previous_ptr == current_ptr) ? previous_ptr : card_ptr;
 92 }
 93 
<span class="line-modified"> 94 void G1HotCardCache::drain(G1CardTableEntryClosure* cl, uint worker_id) {</span>
 95   assert(default_use_cache(), &quot;Drain only necessary if we use the hot card cache.&quot;);
 96 
 97   assert(_hot_cache != NULL, &quot;Logic&quot;);
 98   assert(!use_cache(), &quot;cache should be disabled&quot;);
 99 
100   while (_hot_cache_par_claimed_idx &lt; _hot_cache_size) {
<span class="line-modified">101     size_t end_idx = Atomic::add(&amp;_hot_cache_par_claimed_idx,</span>
<span class="line-modified">102                                  _hot_cache_par_chunk_size);</span>
103     size_t start_idx = end_idx - _hot_cache_par_chunk_size;
104     // The current worker has successfully claimed the chunk [start_idx..end_idx)
105     end_idx = MIN2(end_idx, _hot_cache_size);
106     for (size_t i = start_idx; i &lt; end_idx; i++) {
107       CardValue* card_ptr = _hot_cache[i];
108       if (card_ptr != NULL) {
<span class="line-modified">109         cl-&gt;do_card_ptr(card_ptr, worker_id);</span>

110       } else {
111         break;
112       }
113     }
114   }
115 
116   // The existing entries in the hot card cache, which were just refined
117   // above, are discarded prior to re-enabling the cache near the end of the GC.
118 }
119 
120 void G1HotCardCache::reset_card_counts(HeapRegion* hr) {
121   _card_counts.clear_region(hr);
122 }
</pre>
</td>
</tr>
</table>
<center><a href="g1HeterogeneousHeapYoungGenSizer.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1HotCardCache.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>