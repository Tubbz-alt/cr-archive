<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/share/gc/g1/g1BlockOffsetTable.hpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2001, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_GC_G1_G1BLOCKOFFSETTABLE_HPP
 26 #define SHARE_GC_G1_G1BLOCKOFFSETTABLE_HPP
 27 
 28 #include &quot;gc/g1/g1RegionToSpaceMapper.hpp&quot;
 29 #include &quot;gc/shared/blockOffsetTable.hpp&quot;
 30 #include &quot;memory/memRegion.hpp&quot;
 31 #include &quot;memory/virtualspace.hpp&quot;
 32 #include &quot;utilities/globalDefinitions.hpp&quot;
 33 
 34 // Forward declarations
 35 class G1BlockOffsetTable;
 36 class G1ContiguousSpace;
 37 
 38 // This implementation of &quot;G1BlockOffsetTable&quot; divides the covered region
 39 // into &quot;N&quot;-word subregions (where &quot;N&quot; = 2^&quot;LogN&quot;.  An array with an entry
 40 // for each such subregion indicates how far back one must go to find the
 41 // start of the chunk that includes the first word of the subregion.
 42 //
 43 // Each G1BlockOffsetTablePart is owned by a G1ContiguousSpace.
 44 
 45 class G1BlockOffsetTable: public CHeapObj&lt;mtGC&gt; {
 46   friend class G1BlockOffsetTablePart;
 47   friend class VMStructs;
 48 
 49 private:
 50   // The reserved region covered by the table.
 51   MemRegion _reserved;
 52 
 53   // Array for keeping offsets for retrieving object start fast given an
 54   // address.
 55   u_char* _offset_array;          // byte array keeping backwards offsets
 56 
 57   void check_offset(size_t offset, const char* msg) const {
 58     assert(offset &lt;= BOTConstants::N_words,
 59            &quot;%s - offset: &quot; SIZE_FORMAT &quot;, N_words: %u&quot;,
 60            msg, offset, BOTConstants::N_words);
 61   }
 62 
 63   // Bounds checking accessors:
 64   // For performance these have to devolve to array accesses in product builds.
 65   inline u_char offset_array(size_t index) const;
 66 
 67   void set_offset_array_raw(size_t index, u_char offset) {
 68     _offset_array[index] = offset;
 69   }
 70 
 71   inline void set_offset_array(size_t index, u_char offset);
 72 
 73   inline void set_offset_array(size_t index, HeapWord* high, HeapWord* low);
 74 
 75   inline void set_offset_array(size_t left, size_t right, u_char offset);
 76 
 77   bool is_card_boundary(HeapWord* p) const;
 78 
 79   void check_index(size_t index, const char* msg) const NOT_DEBUG_RETURN;
 80 
 81 public:
 82 
 83   // Return the number of slots needed for an offset array
 84   // that covers mem_region_words words.
 85   static size_t compute_size(size_t mem_region_words) {
 86     size_t number_of_slots = (mem_region_words / BOTConstants::N_words);
 87     return ReservedSpace::allocation_align_size_up(number_of_slots);
 88   }
 89 
 90   // Returns how many bytes of the heap a single byte of the BOT corresponds to.
 91   static size_t heap_map_factor() {
 92     return BOTConstants::N_bytes;
 93   }
 94 
 95   // Initialize the Block Offset Table to cover the memory region passed
 96   // in the heap parameter.
 97   G1BlockOffsetTable(MemRegion heap, G1RegionToSpaceMapper* storage);
 98 
 99   // Return the appropriate index into &quot;_offset_array&quot; for &quot;p&quot;.
100   inline size_t index_for(const void* p) const;
101   inline size_t index_for_raw(const void* p) const;
102 
103   // Return the address indicating the start of the region corresponding to
104   // &quot;index&quot; in &quot;_offset_array&quot;.
105   inline HeapWord* address_for_index(size_t index) const;
106   // Variant of address_for_index that does not check the index for validity.
107   inline HeapWord* address_for_index_raw(size_t index) const {
108     return _reserved.start() + (index &lt;&lt; BOTConstants::LogN_words);
109   }
110 };
111 
112 class G1BlockOffsetTablePart {
113   friend class G1BlockOffsetTable;
114   friend class VMStructs;
115 private:
116   // allocation boundary at which offset array must be updated
117   HeapWord* _next_offset_threshold;
118   size_t    _next_offset_index;      // index corresponding to that boundary
119 
120   // Indicates if an object can span into this G1BlockOffsetTablePart.
121   debug_only(bool _object_can_span;)
122 
123   // This is the global BlockOffsetTable.
124   G1BlockOffsetTable* _bot;
125 
126   // The space that owns this subregion.
127   G1ContiguousSpace* _space;
128 
129   // Sets the entries
130   // corresponding to the cards starting at &quot;start&quot; and ending at &quot;end&quot;
131   // to point back to the card before &quot;start&quot;: the interval [start, end)
132   // is right-open.
133   void set_remainder_to_point_to_start(HeapWord* start, HeapWord* end);
134   // Same as above, except that the args here are a card _index_ interval
135   // that is closed: [start_index, end_index]
136   void set_remainder_to_point_to_start_incl(size_t start, size_t end);
137 
138   // Zero out the entry for _bottom (offset will be zero). Does not check for availability of the
139   // memory first.
140   void zero_bottom_entry_raw();
141   // Variant of initialize_threshold that does not check for availability of the
142   // memory first.
143   HeapWord* initialize_threshold_raw();
144 
145   inline size_t block_size(const HeapWord* p) const;
146 
147   // Returns the address of a block whose start is at most &quot;addr&quot;.
148   // If &quot;has_max_index&quot; is true, &quot;assumes &quot;max_index&quot; is the last valid one
149   // in the array.
150   inline HeapWord* block_at_or_preceding(const void* addr,
151                                          bool has_max_index,
152                                          size_t max_index) const;
153 
154   // &quot;q&quot; is a block boundary that is &lt;= &quot;addr&quot;; &quot;n&quot; is the address of the
155   // next block (or the end of the space.)  Return the address of the
156   // beginning of the block that contains &quot;addr&quot;.  Does so without side
157   // effects (see, e.g., spec of  block_start.)
158   inline HeapWord* forward_to_block_containing_addr_const(HeapWord* q, HeapWord* n,
159                                                           const void* addr) const;
160 
161   // &quot;q&quot; is a block boundary that is &lt;= &quot;addr&quot;; return the address of the
162   // beginning of the block that contains &quot;addr&quot;.  May have side effects
163   // on &quot;this&quot;, by updating imprecise entries.
164   inline HeapWord* forward_to_block_containing_addr(HeapWord* q,
165                                                     const void* addr);
166 
167   // &quot;q&quot; is a block boundary that is &lt;= &quot;addr&quot;; &quot;n&quot; is the address of the
168   // next block (or the end of the space.)  Return the address of the
169   // beginning of the block that contains &quot;addr&quot;.  May have side effects
170   // on &quot;this&quot;, by updating imprecise entries.
171   HeapWord* forward_to_block_containing_addr_slow(HeapWord* q,
172                                                   HeapWord* n,
173                                                   const void* addr);
174 
175   // Requires that &quot;*threshold_&quot; be the first array entry boundary at or
176   // above &quot;blk_start&quot;, and that &quot;*index_&quot; be the corresponding array
177   // index.  If the block starts at or crosses &quot;*threshold_&quot;, records
178   // &quot;blk_start&quot; as the appropriate block start for the array index
179   // starting at &quot;*threshold_&quot;, and for any other indices crossed by the
180   // block.  Updates &quot;*threshold_&quot; and &quot;*index_&quot; to correspond to the first
181   // index after the block end.
182   void alloc_block_work(HeapWord** threshold_, size_t* index_,
183                         HeapWord* blk_start, HeapWord* blk_end);
184 
185   void check_all_cards(size_t left_card, size_t right_card) const;
186 
187 public:
188   //  The elements of the array are initialized to zero.
189   G1BlockOffsetTablePart(G1BlockOffsetTable* array, G1ContiguousSpace* gsp);
190 
191   void verify() const;
192 
193   // Returns the address of the start of the block containing &quot;addr&quot;, or
194   // else &quot;null&quot; if it is covered by no block.  (May have side effects,
195   // namely updating of shared array entries that &quot;point&quot; too far
196   // backwards.  This can occur, for example, when lab allocation is used
197   // in a space covered by the table.)
198   inline HeapWord* block_start(const void* addr);
199   // Same as above, but does not have any of the possible side effects
200   // discussed above.
201   inline HeapWord* block_start_const(const void* addr) const;
202 
203   // Initialize the threshold to reflect the first boundary after the
204   // bottom of the covered region.
205   HeapWord* initialize_threshold();
206 
207   void reset_bot() {
208     zero_bottom_entry_raw();
209     initialize_threshold_raw();
210   }
211 
212   // Return the next threshold, the point at which the table should be
213   // updated.
214   HeapWord* threshold() const { return _next_offset_threshold; }
215 
216   // These must be guaranteed to work properly (i.e., do nothing)
217   // when &quot;blk_start&quot; (&quot;blk&quot; for second version) is &quot;NULL&quot;.  In this
218   // implementation, that&#39;s true because NULL is represented as 0, and thus
219   // never exceeds the &quot;_next_offset_threshold&quot;.
220   void alloc_block(HeapWord* blk_start, HeapWord* blk_end) {
221     if (blk_end &gt; _next_offset_threshold) {
222       alloc_block_work(&amp;_next_offset_threshold, &amp;_next_offset_index, blk_start, blk_end);
223     }
224   }
225   void alloc_block(HeapWord* blk, size_t size) {
226     alloc_block(blk, blk+size);
227   }
228 
229   void set_for_starts_humongous(HeapWord* obj_top, size_t fill_size);
230   void set_object_can_span(bool can_span) NOT_DEBUG_RETURN;
231 
232   void print_on(outputStream* out) PRODUCT_RETURN;
233 };
234 
235 #endif // SHARE_GC_G1_G1BLOCKOFFSETTABLE_HPP
    </pre>
  </body>
</html>