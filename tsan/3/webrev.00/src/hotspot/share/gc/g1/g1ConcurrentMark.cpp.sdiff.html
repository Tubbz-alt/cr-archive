<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1ConcurrentMark.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1CollectionSetChooser.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1ConcurrentMark.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1ConcurrentMark.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2001, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;classfile/classLoaderDataGraph.hpp&quot;
  27 #include &quot;code/codeCache.hpp&quot;
  28 #include &quot;gc/g1/g1BarrierSet.hpp&quot;
  29 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
  30 #include &quot;gc/g1/g1CollectorState.hpp&quot;
  31 #include &quot;gc/g1/g1ConcurrentMark.inline.hpp&quot;
  32 #include &quot;gc/g1/g1ConcurrentMarkThread.inline.hpp&quot;
  33 #include &quot;gc/g1/g1DirtyCardQueue.hpp&quot;
  34 #include &quot;gc/g1/g1HeapVerifier.hpp&quot;
  35 #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
  36 #include &quot;gc/g1/g1Policy.hpp&quot;
  37 #include &quot;gc/g1/g1RegionMarkStatsCache.inline.hpp&quot;
  38 #include &quot;gc/g1/g1StringDedup.hpp&quot;
  39 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;

  40 #include &quot;gc/g1/heapRegion.inline.hpp&quot;
  41 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
  42 #include &quot;gc/g1/heapRegionSet.inline.hpp&quot;
  43 #include &quot;gc/shared/gcId.hpp&quot;
  44 #include &quot;gc/shared/gcTimer.hpp&quot;
<span class="line-removed">  45 #include &quot;gc/shared/gcTrace.hpp&quot;</span>
  46 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  47 #include &quot;gc/shared/gcVMOperations.hpp&quot;
  48 #include &quot;gc/shared/genOopClosures.inline.hpp&quot;
  49 #include &quot;gc/shared/referencePolicy.hpp&quot;
  50 #include &quot;gc/shared/strongRootsScope.hpp&quot;
  51 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;

  52 #include &quot;gc/shared/taskqueue.inline.hpp&quot;
  53 #include &quot;gc/shared/weakProcessor.inline.hpp&quot;
  54 #include &quot;gc/shared/workerPolicy.hpp&quot;
  55 #include &quot;include/jvm.h&quot;
  56 #include &quot;logging/log.hpp&quot;
  57 #include &quot;memory/allocation.hpp&quot;

  58 #include &quot;memory/resourceArea.hpp&quot;

  59 #include &quot;oops/access.inline.hpp&quot;
  60 #include &quot;oops/oop.inline.hpp&quot;
  61 #include &quot;runtime/atomic.hpp&quot;
  62 #include &quot;runtime/handles.inline.hpp&quot;
  63 #include &quot;runtime/java.hpp&quot;

  64 #include &quot;runtime/prefetch.inline.hpp&quot;
  65 #include &quot;services/memTracker.hpp&quot;
  66 #include &quot;utilities/align.hpp&quot;
  67 #include &quot;utilities/growableArray.hpp&quot;
  68 
  69 bool G1CMBitMapClosure::do_addr(HeapWord* const addr) {
  70   assert(addr &lt; _cm-&gt;finger(), &quot;invariant&quot;);
  71   assert(addr &gt;= _task-&gt;finger(), &quot;invariant&quot;);
  72 
  73   // We move that task&#39;s local finger along.
  74   _task-&gt;move_finger_to(addr);
  75 
  76   _task-&gt;scan_task_entry(G1TaskQueueEntry::from_oop(oop(addr)));
  77   // we only partially drain the local queue and global stack
  78   _task-&gt;drain_local_queue(true);
  79   _task-&gt;drain_global_stack(true);
  80 
  81   // if the has_aborted flag has been raised, we need to bail out of
  82   // the iteration
  83   return !_task-&gt;has_aborted();
</pre>
<hr />
<pre>
 149     log_debug(gc)(&quot;Expanded mark stack capacity from &quot; SIZE_FORMAT &quot; to &quot; SIZE_FORMAT &quot; chunks&quot;,
 150                   old_capacity, new_capacity);
 151   } else {
 152     log_warning(gc)(&quot;Failed to expand mark stack capacity from &quot; SIZE_FORMAT &quot; to &quot; SIZE_FORMAT &quot; chunks&quot;,
 153                     old_capacity, new_capacity);
 154   }
 155 }
 156 
 157 G1CMMarkStack::~G1CMMarkStack() {
 158   if (_base != NULL) {
 159     MmapArrayAllocator&lt;TaskQueueEntryChunk&gt;::free(_base, _chunk_capacity);
 160   }
 161 }
 162 
 163 void G1CMMarkStack::add_chunk_to_list(TaskQueueEntryChunk* volatile* list, TaskQueueEntryChunk* elem) {
 164   elem-&gt;next = *list;
 165   *list = elem;
 166 }
 167 
 168 void G1CMMarkStack::add_chunk_to_chunk_list(TaskQueueEntryChunk* elem) {
<span class="line-modified"> 169   MutexLockerEx x(MarkStackChunkList_lock, Mutex::_no_safepoint_check_flag);</span>
 170   add_chunk_to_list(&amp;_chunk_list, elem);
 171   _chunks_in_chunk_list++;
 172 }
 173 
 174 void G1CMMarkStack::add_chunk_to_free_list(TaskQueueEntryChunk* elem) {
<span class="line-modified"> 175   MutexLockerEx x(MarkStackFreeList_lock, Mutex::_no_safepoint_check_flag);</span>
 176   add_chunk_to_list(&amp;_free_list, elem);
 177 }
 178 
 179 G1CMMarkStack::TaskQueueEntryChunk* G1CMMarkStack::remove_chunk_from_list(TaskQueueEntryChunk* volatile* list) {
 180   TaskQueueEntryChunk* result = *list;
 181   if (result != NULL) {
 182     *list = (*list)-&gt;next;
 183   }
 184   return result;
 185 }
 186 
 187 G1CMMarkStack::TaskQueueEntryChunk* G1CMMarkStack::remove_chunk_from_chunk_list() {
<span class="line-modified"> 188   MutexLockerEx x(MarkStackChunkList_lock, Mutex::_no_safepoint_check_flag);</span>
 189   TaskQueueEntryChunk* result = remove_chunk_from_list(&amp;_chunk_list);
 190   if (result != NULL) {
 191     _chunks_in_chunk_list--;
 192   }
 193   return result;
 194 }
 195 
 196 G1CMMarkStack::TaskQueueEntryChunk* G1CMMarkStack::remove_chunk_from_free_list() {
<span class="line-modified"> 197   MutexLockerEx x(MarkStackFreeList_lock, Mutex::_no_safepoint_check_flag);</span>
 198   return remove_chunk_from_list(&amp;_free_list);
 199 }
 200 
 201 G1CMMarkStack::TaskQueueEntryChunk* G1CMMarkStack::allocate_new_chunk() {
 202   // This dirty read of _hwm is okay because we only ever increase the _hwm in parallel code.
 203   // Further this limits _hwm to a value of _chunk_capacity + #threads, avoiding
 204   // wraparound of _hwm.
 205   if (_hwm &gt;= _chunk_capacity) {
 206     return NULL;
 207   }
 208 
<span class="line-modified"> 209   size_t cur_idx = Atomic::add(1u, &amp;_hwm) - 1;</span>
 210   if (cur_idx &gt;= _chunk_capacity) {
 211     return NULL;
 212   }
 213 
 214   TaskQueueEntryChunk* result = ::new (&amp;_base[cur_idx]) TaskQueueEntryChunk;
 215   result-&gt;next = NULL;
 216   return result;
 217 }
 218 
 219 bool G1CMMarkStack::par_push_chunk(G1TaskQueueEntry* ptr_arr) {
 220   // Get a new chunk.
 221   TaskQueueEntryChunk* new_chunk = remove_chunk_from_free_list();
 222 
 223   if (new_chunk == NULL) {
 224     // Did not get a chunk from the free list. Allocate from backing memory.
 225     new_chunk = allocate_new_chunk();
 226 
 227     if (new_chunk == NULL) {
 228       return false;
 229     }
</pre>
<hr />
<pre>
 239 bool G1CMMarkStack::par_pop_chunk(G1TaskQueueEntry* ptr_arr) {
 240   TaskQueueEntryChunk* cur = remove_chunk_from_chunk_list();
 241 
 242   if (cur == NULL) {
 243     return false;
 244   }
 245 
 246   Copy::conjoint_memory_atomic(cur-&gt;data, ptr_arr, EntriesPerChunk * sizeof(G1TaskQueueEntry));
 247 
 248   add_chunk_to_free_list(cur);
 249   return true;
 250 }
 251 
 252 void G1CMMarkStack::set_empty() {
 253   _chunks_in_chunk_list = 0;
 254   _hwm = 0;
 255   _chunk_list = NULL;
 256   _free_list = NULL;
 257 }
 258 
<span class="line-modified"> 259 G1CMRootRegions::G1CMRootRegions(uint const max_regions) :</span>
<span class="line-modified"> 260   _root_regions(NEW_C_HEAP_ARRAY(HeapRegion*, max_regions, mtGC)),</span>
<span class="line-modified"> 261   _max_regions(max_regions),</span>
<span class="line-modified"> 262   _num_root_regions(0),</span>
<span class="line-modified"> 263   _claimed_root_regions(0),</span>
<span class="line-modified"> 264   _scan_in_progress(false),</span>
<span class="line-modified"> 265   _should_abort(false) { }</span>
 266 
<span class="line-modified"> 267 G1CMRootRegions::~G1CMRootRegions() {</span>
<span class="line-modified"> 268   FREE_C_HEAP_ARRAY(HeapRegion*, _max_regions);</span>
 269 }
 270 
<span class="line-modified"> 271 void G1CMRootRegions::reset() {</span>
 272   _num_root_regions = 0;
 273 }
 274 
<span class="line-modified"> 275 void G1CMRootRegions::add(HeapRegion* hr) {</span>
 276   assert_at_safepoint();
<span class="line-modified"> 277   size_t idx = Atomic::add((size_t)1, &amp;_num_root_regions) - 1;</span>
<span class="line-modified"> 278   assert(idx &lt; _max_regions, &quot;Trying to add more root regions than there is space &quot; SIZE_FORMAT, _max_regions);</span>
<span class="line-modified"> 279   _root_regions[idx] = hr;</span>



 280 }
 281 
<span class="line-modified"> 282 void G1CMRootRegions::prepare_for_scan() {</span>
 283   assert(!scan_in_progress(), &quot;pre-condition&quot;);
 284 
 285   _scan_in_progress = _num_root_regions &gt; 0;
 286 
 287   _claimed_root_regions = 0;
 288   _should_abort = false;
 289 }
 290 
<span class="line-modified"> 291 HeapRegion* G1CMRootRegions::claim_next() {</span>
 292   if (_should_abort) {
 293     // If someone has set the should_abort flag, we return NULL to
 294     // force the caller to bail out of their loop.
 295     return NULL;
 296   }
 297 
 298   if (_claimed_root_regions &gt;= _num_root_regions) {
 299     return NULL;
 300   }
 301 
<span class="line-modified"> 302   size_t claimed_index = Atomic::add((size_t)1, &amp;_claimed_root_regions) - 1;</span>
 303   if (claimed_index &lt; _num_root_regions) {
<span class="line-modified"> 304     return _root_regions[claimed_index];</span>
 305   }
 306   return NULL;
 307 }
 308 
<span class="line-modified"> 309 uint G1CMRootRegions::num_root_regions() const {</span>
 310   return (uint)_num_root_regions;
 311 }
 312 
<span class="line-modified"> 313 void G1CMRootRegions::notify_scan_done() {</span>
<span class="line-modified"> 314   MutexLockerEx x(RootRegionScan_lock, Mutex::_no_safepoint_check_flag);</span>
 315   _scan_in_progress = false;
 316   RootRegionScan_lock-&gt;notify_all();
 317 }
 318 
<span class="line-modified"> 319 void G1CMRootRegions::cancel_scan() {</span>
 320   notify_scan_done();
 321 }
 322 
<span class="line-modified"> 323 void G1CMRootRegions::scan_finished() {</span>
 324   assert(scan_in_progress(), &quot;pre-condition&quot;);
 325 
 326   if (!_should_abort) {
 327     assert(_claimed_root_regions &gt;= num_root_regions(),
 328            &quot;we should have claimed all root regions, claimed &quot; SIZE_FORMAT &quot;, length = %u&quot;,
 329            _claimed_root_regions, num_root_regions());
 330   }
 331 
 332   notify_scan_done();
 333 }
 334 
<span class="line-modified"> 335 bool G1CMRootRegions::wait_until_scan_finished() {</span>
 336   if (!scan_in_progress()) {
 337     return false;
 338   }
 339 
 340   {
<span class="line-modified"> 341     MutexLockerEx x(RootRegionScan_lock, Mutex::_no_safepoint_check_flag);</span>
 342     while (scan_in_progress()) {
<span class="line-modified"> 343       RootRegionScan_lock-&gt;wait(Mutex::_no_safepoint_check_flag);</span>
 344     }
 345   }
 346   return true;
 347 }
 348 
 349 // Returns the maximum number of workers to be used in a concurrent
 350 // phase based on the number of GC workers being used in a STW
 351 // phase.
 352 static uint scale_concurrent_worker_threads(uint num_gc_workers) {
 353   return MAX2((num_gc_workers + 2) / 4, 1U);
 354 }
 355 
 356 G1ConcurrentMark::G1ConcurrentMark(G1CollectedHeap* g1h,
 357                                    G1RegionToSpaceMapper* prev_bitmap_storage,
 358                                    G1RegionToSpaceMapper* next_bitmap_storage) :
 359   // _cm_thread set inside the constructor
 360   _g1h(g1h),
 361   _completed_initialization(false),
 362 
 363   _mark_bitmap_1(),
</pre>
<hr />
<pre>
 407   _max_concurrent_workers(0),
 408 
 409   _region_mark_stats(NEW_C_HEAP_ARRAY(G1RegionMarkStats, _g1h-&gt;max_regions(), mtGC)),
 410   _top_at_rebuild_starts(NEW_C_HEAP_ARRAY(HeapWord*, _g1h-&gt;max_regions(), mtGC))
 411 {
 412   _mark_bitmap_1.initialize(g1h-&gt;reserved_region(), prev_bitmap_storage);
 413   _mark_bitmap_2.initialize(g1h-&gt;reserved_region(), next_bitmap_storage);
 414 
 415   // Create &amp; start ConcurrentMark thread.
 416   _cm_thread = new G1ConcurrentMarkThread(this);
 417   if (_cm_thread-&gt;osthread() == NULL) {
 418     vm_shutdown_during_initialization(&quot;Could not create ConcurrentMarkThread&quot;);
 419   }
 420 
 421   assert(CGC_lock != NULL, &quot;CGC_lock must be initialized&quot;);
 422 
 423   if (FLAG_IS_DEFAULT(ConcGCThreads) || ConcGCThreads == 0) {
 424     // Calculate the number of concurrent worker threads by scaling
 425     // the number of parallel GC threads.
 426     uint marking_thread_num = scale_concurrent_worker_threads(ParallelGCThreads);
<span class="line-modified"> 427     FLAG_SET_ERGO(uint, ConcGCThreads, marking_thread_num);</span>
 428   }
 429 
 430   assert(ConcGCThreads &gt; 0, &quot;ConcGCThreads have been set.&quot;);
 431   if (ConcGCThreads &gt; ParallelGCThreads) {
 432     log_warning(gc)(&quot;More ConcGCThreads (%u) than ParallelGCThreads (%u).&quot;,
 433                     ConcGCThreads, ParallelGCThreads);
 434     return;
 435   }
 436 
 437   log_debug(gc)(&quot;ConcGCThreads: %u offset %u&quot;, ConcGCThreads, _worker_id_offset);
 438   log_debug(gc)(&quot;ParallelGCThreads: %u&quot;, ParallelGCThreads);
 439 
 440   _num_concurrent_workers = ConcGCThreads;
 441   _max_concurrent_workers = _num_concurrent_workers;
 442 
 443   _concurrent_workers = new WorkGang(&quot;G1 Conc&quot;, _max_concurrent_workers, false, true);
 444   _concurrent_workers-&gt;initialize_workers();
 445 
 446   if (FLAG_IS_DEFAULT(MarkStackSize)) {
 447     size_t mark_stack_size =
 448       MIN2(MarkStackSizeMax,
 449           MAX2(MarkStackSize, (size_t) (_max_concurrent_workers * TASKQUEUE_SIZE)));
 450     // Verify that the calculated value for MarkStackSize is in range.
 451     // It would be nice to use the private utility routine from Arguments.
 452     if (!(mark_stack_size &gt;= 1 &amp;&amp; mark_stack_size &lt;= MarkStackSizeMax)) {
 453       log_warning(gc)(&quot;Invalid value calculated for MarkStackSize (&quot; SIZE_FORMAT &quot;): &quot;
 454                       &quot;must be between 1 and &quot; SIZE_FORMAT,
 455                       mark_stack_size, MarkStackSizeMax);
 456       return;
 457     }
<span class="line-modified"> 458     FLAG_SET_ERGO(size_t, MarkStackSize, mark_stack_size);</span>
 459   } else {
 460     // Verify MarkStackSize is in range.
 461     if (FLAG_IS_CMDLINE(MarkStackSize)) {
 462       if (FLAG_IS_DEFAULT(MarkStackSizeMax)) {
 463         if (!(MarkStackSize &gt;= 1 &amp;&amp; MarkStackSize &lt;= MarkStackSizeMax)) {
 464           log_warning(gc)(&quot;Invalid value specified for MarkStackSize (&quot; SIZE_FORMAT &quot;): &quot;
 465                           &quot;must be between 1 and &quot; SIZE_FORMAT,
 466                           MarkStackSize, MarkStackSizeMax);
 467           return;
 468         }
 469       } else if (FLAG_IS_CMDLINE(MarkStackSizeMax)) {
 470         if (!(MarkStackSize &gt;= 1 &amp;&amp; MarkStackSize &lt;= MarkStackSizeMax)) {
 471           log_warning(gc)(&quot;Invalid value specified for MarkStackSize (&quot; SIZE_FORMAT &quot;)&quot;
 472                           &quot; or for MarkStackSizeMax (&quot; SIZE_FORMAT &quot;)&quot;,
 473                           MarkStackSize, MarkStackSizeMax);
 474           return;
 475         }
 476       }
 477     }
 478   }
</pre>
<hr />
<pre>
 572     for (uint i = 0; i &lt; max_regions; i++) {
 573       _region_mark_stats[i].clear_during_overflow();
 574     }
 575   }
 576 
 577   clear_has_overflown();
 578   _finger = _heap.start();
 579 
 580   for (uint i = 0; i &lt; _max_num_tasks; ++i) {
 581     G1CMTaskQueue* queue = _task_queues-&gt;queue(i);
 582     queue-&gt;set_empty();
 583   }
 584 }
 585 
 586 void G1ConcurrentMark::set_concurrency(uint active_tasks) {
 587   assert(active_tasks &lt;= _max_num_tasks, &quot;we should not have more&quot;);
 588 
 589   _num_active_tasks = active_tasks;
 590   // Need to update the three data structures below according to the
 591   // number of active threads for this phase.
<span class="line-modified"> 592   _terminator.terminator()-&gt;reset_for_reuse((int) active_tasks);</span>
 593   _first_overflow_barrier_sync.set_n_workers((int) active_tasks);
 594   _second_overflow_barrier_sync.set_n_workers((int) active_tasks);
 595 }
 596 
 597 void G1ConcurrentMark::set_concurrency_and_phase(uint active_tasks, bool concurrent) {
 598   set_concurrency(active_tasks);
 599 
 600   _concurrent = concurrent;
 601 
 602   if (!concurrent) {
 603     // At this point we should be in a STW phase, and completed marking.
 604     assert_at_safepoint_on_vm_thread();
 605     assert(out_of_regions(),
 606            &quot;only way to get here: _finger: &quot; PTR_FORMAT &quot;, _heap_end: &quot; PTR_FORMAT,
 607            p2i(_finger), p2i(_heap.end()));
 608   }
 609 }
 610 
 611 void G1ConcurrentMark::reset_at_marking_complete() {
 612   // We set the global marking state to some default values when we&#39;re
</pre>
<hr />
<pre>
 717 
 718   // Repeat the asserts from above.
 719   guarantee(cm_thread()-&gt;during_cycle(), &quot;invariant&quot;);
 720   guarantee(!_g1h-&gt;collector_state()-&gt;mark_or_rebuild_in_progress(), &quot;invariant&quot;);
 721 }
 722 
 723 void G1ConcurrentMark::clear_prev_bitmap(WorkGang* workers) {
 724   assert_at_safepoint_on_vm_thread();
 725   clear_bitmap(_prev_mark_bitmap, workers, false);
 726 }
 727 
 728 class NoteStartOfMarkHRClosure : public HeapRegionClosure {
 729 public:
 730   bool do_heap_region(HeapRegion* r) {
 731     r-&gt;note_start_of_marking();
 732     return false;
 733   }
 734 };
 735 
 736 void G1ConcurrentMark::pre_initial_mark() {
<span class="line-modified"> 737   // Initialize marking structures. This has to be done in a STW phase.</span>


 738   reset();
 739 
 740   // For each region note start of marking.
 741   NoteStartOfMarkHRClosure startcl;
 742   _g1h-&gt;heap_region_iterate(&amp;startcl);
 743 
 744   _root_regions.reset();
 745 }
 746 
 747 
 748 void G1ConcurrentMark::post_initial_mark() {
 749   // Start Concurrent Marking weak-reference discovery.
 750   ReferenceProcessor* rp = _g1h-&gt;ref_processor_cm();
 751   // enable (&quot;weak&quot;) refs discovery
 752   rp-&gt;enable_discovery();
 753   rp-&gt;setup_policy(false); // snapshot the soft ref policy to be used in this cycle
 754 
 755   SATBMarkQueueSet&amp; satb_mq_set = G1BarrierSet::satb_mark_queue_set();
 756   // This is the start of  the marking cycle, we&#39;re expected all
 757   // threads to have SATB queues with active set to false.
</pre>
<hr />
<pre>
 855   uint result = 0;
 856   if (!UseDynamicNumberOfGCThreads ||
 857       (!FLAG_IS_DEFAULT(ConcGCThreads) &amp;&amp;
 858        !ForceDynamicNumberOfGCThreads)) {
 859     result = _max_concurrent_workers;
 860   } else {
 861     result =
 862       WorkerPolicy::calc_default_active_workers(_max_concurrent_workers,
 863                                                 1, /* Minimum workers */
 864                                                 _num_concurrent_workers,
 865                                                 Threads::number_of_non_daemon_threads());
 866     // Don&#39;t scale the result down by scale_concurrent_workers() because
 867     // that scaling has already gone into &quot;_max_concurrent_workers&quot;.
 868   }
 869   assert(result &gt; 0 &amp;&amp; result &lt;= _max_concurrent_workers,
 870          &quot;Calculated number of marking workers must be larger than zero and at most the maximum %u, but is %u&quot;,
 871          _max_concurrent_workers, result);
 872   return result;
 873 }
 874 
<span class="line-modified"> 875 void G1ConcurrentMark::scan_root_region(HeapRegion* hr, uint worker_id) {</span>
<span class="line-modified"> 876   assert(hr-&gt;is_old() || (hr-&gt;is_survivor() &amp;&amp; hr-&gt;next_top_at_mark_start() == hr-&gt;bottom()),</span>
<span class="line-modified"> 877          &quot;Root regions must be old or survivor but region %u is %s&quot;, hr-&gt;hrm_index(), hr-&gt;get_type_str());</span>







 878   G1RootRegionScanClosure cl(_g1h, this, worker_id);
 879 
 880   const uintx interval = PrefetchScanIntervalInBytes;
<span class="line-modified"> 881   HeapWord* curr = hr-&gt;next_top_at_mark_start();</span>
<span class="line-modified"> 882   const HeapWord* end = hr-&gt;top();</span>
 883   while (curr &lt; end) {
 884     Prefetch::read(curr, interval);
 885     oop obj = oop(curr);
 886     int size = obj-&gt;oop_iterate_size(&amp;cl);
 887     assert(size == obj-&gt;size(), &quot;sanity&quot;);
 888     curr += size;
 889   }
 890 }
 891 
 892 class G1CMRootRegionScanTask : public AbstractGangTask {
 893   G1ConcurrentMark* _cm;
 894 public:
 895   G1CMRootRegionScanTask(G1ConcurrentMark* cm) :
 896     AbstractGangTask(&quot;G1 Root Region Scan&quot;), _cm(cm) { }
 897 
 898   void work(uint worker_id) {
 899     assert(Thread::current()-&gt;is_ConcurrentGC_thread(),
 900            &quot;this should only be done by a conc GC thread&quot;);
 901 
<span class="line-modified"> 902     G1CMRootRegions* root_regions = _cm-&gt;root_regions();</span>
<span class="line-modified"> 903     HeapRegion* hr = root_regions-&gt;claim_next();</span>
<span class="line-modified"> 904     while (hr != NULL) {</span>
<span class="line-modified"> 905       _cm-&gt;scan_root_region(hr, worker_id);</span>
<span class="line-modified"> 906       hr = root_regions-&gt;claim_next();</span>
 907     }
 908   }
 909 };
 910 
 911 void G1ConcurrentMark::scan_root_regions() {
 912   // scan_in_progress() will have been set to true only if there was
 913   // at least one root region to scan. So, if it&#39;s false, we
 914   // should not attempt to do any further work.
 915   if (root_regions()-&gt;scan_in_progress()) {
 916     assert(!has_aborted(), &quot;Aborting before root region scanning is finished not supported.&quot;);
 917 
 918     _num_concurrent_workers = MIN2(calc_active_marking_workers(),
 919                                    // We distribute work on a per-region basis, so starting
 920                                    // more threads than that is useless.
 921                                    root_regions()-&gt;num_root_regions());
 922     assert(_num_concurrent_workers &lt;= _max_concurrent_workers,
 923            &quot;Maximum number of marking threads exceeded&quot;);
 924 
 925     G1CMRootRegionScanTask task(this);
 926     log_debug(gc, ergo)(&quot;Running %s using %u workers for %u work units.&quot;,
</pre>
<hr />
<pre>
1086       _g1h(g1h), _cm(cm), _cl(cl), _num_regions_selected_for_rebuild(0) { }
1087 
1088     virtual bool do_heap_region(HeapRegion* r) {
1089       update_remset_before_rebuild(r);
1090       update_marked_bytes(r);
1091 
1092       return false;
1093     }
1094 
1095     uint num_selected_for_rebuild() const { return _num_regions_selected_for_rebuild; }
1096   };
1097 
1098 public:
1099   G1UpdateRemSetTrackingBeforeRebuildTask(G1CollectedHeap* g1h, G1ConcurrentMark* cm, uint num_workers) :
1100     AbstractGangTask(&quot;G1 Update RemSet Tracking Before Rebuild&quot;),
1101     _g1h(g1h), _cm(cm), _hrclaimer(num_workers), _total_selected_for_rebuild(0), _cl(&quot;Post-Marking&quot;) { }
1102 
1103   virtual void work(uint worker_id) {
1104     G1UpdateRemSetTrackingBeforeRebuild update_cl(_g1h, _cm, &amp;_cl);
1105     _g1h-&gt;heap_region_par_iterate_from_worker_offset(&amp;update_cl, &amp;_hrclaimer, worker_id);
<span class="line-modified">1106     Atomic::add(update_cl.num_selected_for_rebuild(), &amp;_total_selected_for_rebuild);</span>
1107   }
1108 
1109   uint total_selected_for_rebuild() const { return _total_selected_for_rebuild; }
1110 
1111   // Number of regions for which roughly one thread should be spawned for this work.
1112   static const uint RegionsPerThread = 384;
1113 };
1114 
1115 class G1UpdateRemSetTrackingAfterRebuild : public HeapRegionClosure {
1116   G1CollectedHeap* _g1h;
1117 public:
1118   G1UpdateRemSetTrackingAfterRebuild(G1CollectedHeap* g1h) : _g1h(g1h) { }
1119 
1120   virtual bool do_heap_region(HeapRegion* r) {
1121     _g1h-&gt;policy()-&gt;remset_tracker()-&gt;update_after_rebuild(r);
1122     return false;
1123   }
1124 };
1125 
1126 void G1ConcurrentMark::remark() {
</pre>
<hr />
<pre>
1237                                  FreeRegionList* local_cleanup_list) :
1238       _g1h(g1h),
1239       _freed_bytes(0),
1240       _local_cleanup_list(local_cleanup_list),
1241       _old_regions_removed(0),
1242       _humongous_regions_removed(0) { }
1243 
1244     size_t freed_bytes() { return _freed_bytes; }
1245     const uint old_regions_removed() { return _old_regions_removed; }
1246     const uint humongous_regions_removed() { return _humongous_regions_removed; }
1247 
1248     bool do_heap_region(HeapRegion *hr) {
1249       if (hr-&gt;used() &gt; 0 &amp;&amp; hr-&gt;max_live_bytes() == 0 &amp;&amp; !hr-&gt;is_young() &amp;&amp; !hr-&gt;is_archive()) {
1250         _freed_bytes += hr-&gt;used();
1251         hr-&gt;set_containing_set(NULL);
1252         if (hr-&gt;is_humongous()) {
1253           _humongous_regions_removed++;
1254           _g1h-&gt;free_humongous_region(hr, _local_cleanup_list);
1255         } else {
1256           _old_regions_removed++;
<span class="line-modified">1257           _g1h-&gt;free_region(hr, _local_cleanup_list, false /* skip_remset */, false /* skip_hcc */, true /* locked */);</span>
1258         }
1259         hr-&gt;clear_cardtable();
1260         _g1h-&gt;concurrent_mark()-&gt;clear_statistics_in_region(hr-&gt;hrm_index());
1261         log_trace(gc)(&quot;Reclaimed empty region %u (%s) bot &quot; PTR_FORMAT, hr-&gt;hrm_index(), hr-&gt;get_short_type_str(), p2i(hr-&gt;bottom()));
1262       }
1263 
1264       return false;
1265     }
1266   };
1267 
1268   G1CollectedHeap* _g1h;
1269   FreeRegionList* _cleanup_list;
1270   HeapRegionClaimer _hrclaimer;
1271 
1272 public:
1273   G1ReclaimEmptyRegionsTask(G1CollectedHeap* g1h, FreeRegionList* cleanup_list, uint n_workers) :
1274     AbstractGangTask(&quot;G1 Cleanup&quot;),
1275     _g1h(g1h),
1276     _cleanup_list(cleanup_list),
1277     _hrclaimer(n_workers) {
1278   }
1279 
1280   void work(uint worker_id) {
1281     FreeRegionList local_cleanup_list(&quot;Local Cleanup List&quot;);
1282     G1ReclaimEmptyRegionsClosure cl(_g1h, &amp;local_cleanup_list);
1283     _g1h-&gt;heap_region_par_iterate_from_worker_offset(&amp;cl, &amp;_hrclaimer, worker_id);
1284     assert(cl.is_complete(), &quot;Shouldn&#39;t have aborted!&quot;);
1285 
1286     // Now update the old/humongous region sets
1287     _g1h-&gt;remove_from_old_sets(cl.old_regions_removed(), cl.humongous_regions_removed());
1288     {
<span class="line-modified">1289       MutexLockerEx x(ParGCRareEvent_lock, Mutex::_no_safepoint_check_flag);</span>
1290       _g1h-&gt;decrement_summary_bytes(cl.freed_bytes());
1291 
1292       _cleanup_list-&gt;add_ordered(&amp;local_cleanup_list);
1293       assert(local_cleanup_list.is_empty(), &quot;post-condition&quot;);
1294     }
1295   }
1296 };
1297 
1298 void G1ConcurrentMark::reclaim_empty_regions() {
1299   WorkGang* workers = _g1h-&gt;workers();
1300   FreeRegionList empty_regions_list(&quot;Empty Regions After Mark List&quot;);
1301 
1302   G1ReclaimEmptyRegionsTask cl(_g1h, &amp;empty_regions_list, workers-&gt;active_workers());
1303   workers-&gt;run_task(&amp;cl);
1304 
1305   if (!empty_regions_list.is_empty()) {
1306     log_debug(gc)(&quot;Reclaimed %u empty regions&quot;, empty_regions_list.length());
1307     // Now print the empty regions list.
1308     G1HRPrinter* hrp = _g1h-&gt;hr_printer();
1309     if (hrp-&gt;is_active()) {
</pre>
<hr />
<pre>
1576     // JNI references during parallel reference processing.
1577     //
1578     // These closures do not need to synchronize with the worker
1579     // threads involved in parallel reference processing as these
1580     // instances are executed serially by the current thread (e.g.
1581     // reference processing is not multi-threaded and is thus
1582     // performed by the current thread instead of a gang worker).
1583     //
1584     // The gang tasks involved in parallel reference processing create
1585     // their own instances of these closures, which do their own
1586     // synchronization among themselves.
1587     G1CMKeepAliveAndDrainClosure g1_keep_alive(this, task(0), true /* is_serial */);
1588     G1CMDrainMarkingStackClosure g1_drain_mark_stack(this, task(0), true /* is_serial */);
1589 
1590     // We need at least one active thread. If reference processing
1591     // is not multi-threaded we use the current (VMThread) thread,
1592     // otherwise we use the work gang from the G1CollectedHeap and
1593     // we utilize all the worker threads we can.
1594     bool processing_is_mt = rp-&gt;processing_is_mt();
1595     uint active_workers = (processing_is_mt ? _g1h-&gt;workers()-&gt;active_workers() : 1U);
<span class="line-modified">1596     active_workers = MAX2(MIN2(active_workers, _max_num_tasks), 1U);</span>
1597 
1598     // Parallel processing task executor.
1599     G1CMRefProcTaskExecutor par_task_executor(_g1h, this,
1600                                               _g1h-&gt;workers(), active_workers);
1601     AbstractRefProcTaskExecutor* executor = (processing_is_mt ? &amp;par_task_executor : NULL);
1602 
1603     // Set the concurrency level. The phase was already set prior to
1604     // executing the remark task.
1605     set_concurrency(active_workers);
1606 
1607     // Set the degree of MT processing here.  If the discovery was done MT,
1608     // the number of threads involved during discovery could differ from
1609     // the number of active workers.  This is OK as long as the discovered
1610     // Reference lists are balanced (see balance_all_queues() and balance_queues()).
1611     rp-&gt;set_active_mt_degree(active_workers);
1612 
1613     ReferenceProcessorPhaseTimes pt(_gc_timer_cm, rp-&gt;max_num_queues());
1614 
1615     // Process the weak references.
1616     const ReferenceProcessorStats&amp; stats =
</pre>
<hr />
<pre>
1691   G1PrecleanYieldClosure yield_cl(this);
1692 
1693   ReferenceProcessor* rp = _g1h-&gt;ref_processor_cm();
1694   // Precleaning is single threaded. Temporarily disable MT discovery.
1695   ReferenceProcessorMTDiscoveryMutator rp_mut_discovery(rp, false);
1696   rp-&gt;preclean_discovered_references(rp-&gt;is_alive_non_header(),
1697                                      &amp;keep_alive,
1698                                      &amp;drain_mark_stack,
1699                                      &amp;yield_cl,
1700                                      _gc_timer_cm);
1701 }
1702 
1703 // When sampling object counts, we already swapped the mark bitmaps, so we need to use
1704 // the prev bitmap determining liveness.
1705 class G1ObjectCountIsAliveClosure: public BoolObjectClosure {
1706   G1CollectedHeap* _g1h;
1707 public:
1708   G1ObjectCountIsAliveClosure(G1CollectedHeap* g1h) : _g1h(g1h) { }
1709 
1710   bool do_object_b(oop obj) {
<span class="line-modified">1711     HeapWord* addr = (HeapWord*)obj;</span>
<span class="line-modified">1712     return addr != NULL &amp;&amp;</span>
<span class="line-removed">1713            (!_g1h-&gt;is_in_g1_reserved(addr) || !_g1h-&gt;is_obj_dead(obj));</span>
1714   }
1715 };
1716 
1717 void G1ConcurrentMark::report_object_count(bool mark_completed) {
1718   // Depending on the completion of the marking liveness needs to be determined
1719   // using either the next or prev bitmap.
1720   if (mark_completed) {
1721     G1ObjectCountIsAliveClosure is_alive(_g1h);
1722     _gc_tracer_cm-&gt;report_object_count_after_gc(&amp;is_alive);
1723   } else {
1724     G1CMIsAliveClosure is_alive(_g1h);
1725     _gc_tracer_cm-&gt;report_object_count_after_gc(&amp;is_alive);
1726   }
1727 }
1728 
1729 
1730 void G1ConcurrentMark::swap_mark_bitmaps() {
1731   G1CMBitMap* temp = _prev_mark_bitmap;
1732   _prev_mark_bitmap = _next_mark_bitmap;
1733   _next_mark_bitmap = temp;
</pre>
<hr />
<pre>
1747     _task-&gt;increment_refs_reached();
1748     oop const obj = static_cast&lt;oop&gt;(entry);
1749     _task-&gt;make_reference_grey(obj);
1750   }
1751 
1752 public:
1753   G1CMSATBBufferClosure(G1CMTask* task, G1CollectedHeap* g1h)
1754     : _task(task), _g1h(g1h) { }
1755 
1756   virtual void do_buffer(void** buffer, size_t size) {
1757     for (size_t i = 0; i &lt; size; ++i) {
1758       do_entry(buffer[i]);
1759     }
1760   }
1761 };
1762 
1763 class G1RemarkThreadsClosure : public ThreadClosure {
1764   G1CMSATBBufferClosure _cm_satb_cl;
1765   G1CMOopClosure _cm_cl;
1766   MarkingCodeBlobClosure _code_cl;
<span class="line-modified">1767   int _thread_parity;</span>
1768 
1769  public:
1770   G1RemarkThreadsClosure(G1CollectedHeap* g1h, G1CMTask* task) :
1771     _cm_satb_cl(task, g1h),
1772     _cm_cl(g1h, task),
1773     _code_cl(&amp;_cm_cl, !CodeBlobToOopClosure::FixRelocations),
<span class="line-modified">1774     _thread_parity(Threads::thread_claim_parity()) {}</span>
1775 
1776   void do_thread(Thread* thread) {
<span class="line-modified">1777     if (thread-&gt;claim_oops_do(true, _thread_parity)) {</span>
1778       SATBMarkQueue&amp; queue = G1ThreadLocalData::satb_mark_queue(thread);
1779       queue.apply_closure_and_empty(&amp;_cm_satb_cl);
1780       if (thread-&gt;is_Java_thread()) {
1781         // In theory it should not be neccessary to explicitly walk the nmethods to find roots for concurrent marking
1782         // however the liveness of oops reachable from nmethods have very complex lifecycles:
1783         // * Alive if on the stack of an executing method
1784         // * Weakly reachable otherwise
1785         // Some objects reachable from nmethods, such as the class loader (or klass_holder) of the receiver should be
1786         // live by the SATB invariant but other oops recorded in nmethods may behave differently.
1787         JavaThread* jt = (JavaThread*)thread;
1788         jt-&gt;nmethods_do(&amp;_code_cl);
1789       }
1790     }
1791   }
1792 };
1793 
1794 class G1CMRemarkTask : public AbstractGangTask {
1795   G1ConcurrentMark* _cm;
1796 public:
1797   void work(uint worker_id) {
</pre>
<hr />
<pre>
1871 void G1ConcurrentMark::clear_range_in_prev_bitmap(MemRegion mr) {
1872   _prev_mark_bitmap-&gt;clear_range(mr);
1873 }
1874 
1875 HeapRegion*
1876 G1ConcurrentMark::claim_region(uint worker_id) {
1877   // &quot;checkpoint&quot; the finger
1878   HeapWord* finger = _finger;
1879 
1880   while (finger &lt; _heap.end()) {
1881     assert(_g1h-&gt;is_in_g1_reserved(finger), &quot;invariant&quot;);
1882 
1883     HeapRegion* curr_region = _g1h-&gt;heap_region_containing(finger);
1884     // Make sure that the reads below do not float before loading curr_region.
1885     OrderAccess::loadload();
1886     // Above heap_region_containing may return NULL as we always scan claim
1887     // until the end of the heap. In this case, just jump to the next region.
1888     HeapWord* end = curr_region != NULL ? curr_region-&gt;end() : finger + HeapRegion::GrainWords;
1889 
1890     // Is the gap between reading the finger and doing the CAS too long?
<span class="line-modified">1891     HeapWord* res = Atomic::cmpxchg(end, &amp;_finger, finger);</span>
1892     if (res == finger &amp;&amp; curr_region != NULL) {
1893       // we succeeded
1894       HeapWord*   bottom        = curr_region-&gt;bottom();
1895       HeapWord*   limit         = curr_region-&gt;next_top_at_mark_start();
1896 
1897       // notice that _finger == end cannot be guaranteed here since,
1898       // someone else might have moved the finger even further
1899       assert(_finger &gt;= end, &quot;the finger should have moved forward&quot;);
1900 
1901       if (limit &gt; bottom) {
1902         return curr_region;
1903       } else {
1904         assert(limit == bottom,
1905                &quot;the region limit should be at bottom&quot;);
1906         // we return NULL and the caller should try calling
1907         // claim_region() again.
1908         return NULL;
1909       }
1910     } else {
1911       assert(_finger &gt; finger, &quot;the finger should have moved forward&quot;);
</pre>
<hr />
<pre>
1921 class VerifyNoCSetOops {
1922   G1CollectedHeap* _g1h;
1923   const char* _phase;
1924   int _info;
1925 
1926 public:
1927   VerifyNoCSetOops(const char* phase, int info = -1) :
1928     _g1h(G1CollectedHeap::heap()),
1929     _phase(phase),
1930     _info(info)
1931   { }
1932 
1933   void operator()(G1TaskQueueEntry task_entry) const {
1934     if (task_entry.is_array_slice()) {
1935       guarantee(_g1h-&gt;is_in_reserved(task_entry.slice()), &quot;Slice &quot; PTR_FORMAT &quot; must be in heap.&quot;, p2i(task_entry.slice()));
1936       return;
1937     }
1938     guarantee(oopDesc::is_oop(task_entry.obj()),
1939               &quot;Non-oop &quot; PTR_FORMAT &quot;, phase: %s, info: %d&quot;,
1940               p2i(task_entry.obj()), _phase, _info);
<span class="line-modified">1941     guarantee(!_g1h-&gt;is_in_cset(task_entry.obj()),</span>
<span class="line-modified">1942               &quot;obj: &quot; PTR_FORMAT &quot; in CSet, phase: %s, info: %d&quot;,</span>
<span class="line-modified">1943               p2i(task_entry.obj()), _phase, _info);</span>

1944   }
1945 };
1946 
<span class="line-modified">1947 void G1ConcurrentMark::verify_no_cset_oops() {</span>
1948   assert(SafepointSynchronize::is_at_safepoint(), &quot;should be at a safepoint&quot;);
1949   if (!_g1h-&gt;collector_state()-&gt;mark_or_rebuild_in_progress()) {
1950     return;
1951   }
1952 
1953   // Verify entries on the global mark stack
1954   _global_mark_stack.iterate(VerifyNoCSetOops(&quot;Stack&quot;));
1955 
1956   // Verify entries on the task queues
1957   for (uint i = 0; i &lt; _max_num_tasks; ++i) {
1958     G1CMTaskQueue* queue = _task_queues-&gt;queue(i);
1959     queue-&gt;iterate(VerifyNoCSetOops(&quot;Queue&quot;, i));
1960   }
1961 
1962   // Verify the global finger
1963   HeapWord* global_finger = finger();
1964   if (global_finger != NULL &amp;&amp; global_finger &lt; _heap.end()) {
1965     // Since we always iterate over all regions, we might get a NULL HeapRegion
1966     // here.
1967     HeapRegion* global_hr = _g1h-&gt;heap_region_containing(global_finger);
1968     guarantee(global_hr == NULL || global_finger == global_hr-&gt;bottom(),
1969               &quot;global finger: &quot; PTR_FORMAT &quot; region: &quot; HR_FORMAT,
1970               p2i(global_finger), HR_FORMAT_PARAMS(global_hr));
1971   }
1972 
1973   // Verify the task fingers
1974   assert(_num_concurrent_workers &lt;= _max_num_tasks, &quot;sanity&quot;);
1975   for (uint i = 0; i &lt; _num_concurrent_workers; ++i) {
1976     G1CMTask* task = _tasks[i];
1977     HeapWord* task_finger = task-&gt;finger();
1978     if (task_finger != NULL &amp;&amp; task_finger &lt; _heap.end()) {
1979       // See above note on the global finger verification.
<span class="line-modified">1980       HeapRegion* task_hr = _g1h-&gt;heap_region_containing(task_finger);</span>
<span class="line-modified">1981       guarantee(task_hr == NULL || task_finger == task_hr-&gt;bottom() ||</span>
<span class="line-modified">1982                 !task_hr-&gt;in_collection_set(),</span>
1983                 &quot;task finger: &quot; PTR_FORMAT &quot; region: &quot; HR_FORMAT,
<span class="line-modified">1984                 p2i(task_finger), HR_FORMAT_PARAMS(task_hr));</span>
1985     }
1986   }
1987 }
1988 #endif // PRODUCT
1989 
1990 void G1ConcurrentMark::rebuild_rem_set_concurrently() {
1991   _g1h-&gt;rem_set()-&gt;rebuild_rem_set(this, _concurrent_workers, _worker_id_offset);
1992 }
1993 
1994 void G1ConcurrentMark::print_stats() {
1995   if (!log_is_enabled(Debug, gc, stats)) {
1996     return;
1997   }
1998   log_debug(gc, stats)(&quot;---------------------------------------------------------------------&quot;);
1999   for (size_t i = 0; i &lt; _num_active_tasks; ++i) {
2000     _tasks[i]-&gt;print_stats();
2001     log_debug(gc, stats)(&quot;---------------------------------------------------------------------&quot;);
2002   }
2003 }
2004 
</pre>
<hr />
<pre>
2383   if (has_aborted()) {
2384     return;
2385   }
2386 
2387   // We set this so that the regular clock knows that we&#39;re in the
2388   // middle of draining buffers and doesn&#39;t set the abort flag when it
2389   // notices that SATB buffers are available for draining. It&#39;d be
2390   // very counter productive if it did that. :-)
2391   _draining_satb_buffers = true;
2392 
2393   G1CMSATBBufferClosure satb_cl(this, _g1h);
2394   SATBMarkQueueSet&amp; satb_mq_set = G1BarrierSet::satb_mark_queue_set();
2395 
2396   // This keeps claiming and applying the closure to completed buffers
2397   // until we run out of buffers or we need to abort.
2398   while (!has_aborted() &amp;&amp;
2399          satb_mq_set.apply_closure_to_completed_buffer(&amp;satb_cl)) {
2400     abort_marking_if_regular_check_fail();
2401   }
2402 
<span class="line-modified">2403   _draining_satb_buffers = false;</span>



2404 
<span class="line-modified">2405   assert(has_aborted() ||</span>
<span class="line-removed">2406          _cm-&gt;concurrent() ||</span>
<span class="line-removed">2407          satb_mq_set.completed_buffers_num() == 0, &quot;invariant&quot;);</span>
2408 
2409   // again, this was a potentially expensive operation, decrease the
2410   // limits to get the regular clock call early
2411   decrease_limits();
2412 }
2413 
2414 void G1CMTask::clear_mark_stats_cache(uint region_idx) {
2415   _mark_stats_cache.reset(region_idx);
2416 }
2417 
2418 Pair&lt;size_t, size_t&gt; G1CMTask::flush_mark_stats_cache() {
2419   return _mark_stats_cache.evict_all();
2420 }
2421 
2422 void G1CMTask::print_stats() {
2423   log_debug(gc, stats)(&quot;Marking Stats, task = %u, calls = %u&quot;, _worker_id, _calls);
2424   log_debug(gc, stats)(&quot;  Elapsed time = %1.2lfms, Termination time = %1.2lfms&quot;,
2425                        _elapsed_time_ms, _termination_time_ms);
2426   log_debug(gc, stats)(&quot;  Step Times (cum): num = %d, avg = %1.2lfms, sd = %1.2lfms max = %1.2lfms, total = %1.2lfms&quot;,
2427                        _step_times_ms.num(),
</pre>
<hr />
<pre>
2550     The value of is_serial must be false when do_marking_step is
2551     being called by any of the worker threads in a work gang.
2552     Examples include the concurrent marking code (CMMarkingTask),
2553     the MT remark code, and the MT reference processing closures.
2554 
2555  *****************************************************************************/
2556 
2557 void G1CMTask::do_marking_step(double time_target_ms,
2558                                bool do_termination,
2559                                bool is_serial) {
2560   assert(time_target_ms &gt;= 1.0, &quot;minimum granularity is 1ms&quot;);
2561 
2562   _start_time_ms = os::elapsedVTime() * 1000.0;
2563 
2564   // If do_stealing is true then do_marking_step will attempt to
2565   // steal work from the other G1CMTasks. It only makes sense to
2566   // enable stealing when the termination protocol is enabled
2567   // and do_marking_step() is not being called serially.
2568   bool do_stealing = do_termination &amp;&amp; !is_serial;
2569 
<span class="line-modified">2570   double diff_prediction_ms = _g1h-&gt;policy()-&gt;predictor().get_new_prediction(&amp;_marking_step_diffs_ms);</span>

2571   _time_target_ms = time_target_ms - diff_prediction_ms;
2572 
2573   // set up the variables that are used in the work-based scheme to
2574   // call the regular clock method
2575   _words_scanned = 0;
2576   _refs_reached  = 0;
2577   recalculate_limits();
2578 
2579   // clear all flags
2580   clear_has_aborted();
2581   _has_timed_out = false;
2582   _draining_satb_buffers = false;
2583 
2584   ++_calls;
2585 
2586   // Set up the bitmap and oop closures. Anything that uses them is
2587   // eventually called from this method, so it is OK to allocate these
2588   // statically.
2589   G1CMBitMapClosure bitmap_closure(this, _cm);
2590   G1CMOopClosure cm_oop_closure(_g1h, this);
</pre>
<hr />
<pre>
2792       set_has_aborted();
2793     }
2794   }
2795 
2796   // Mainly for debugging purposes to make sure that a pointer to the
2797   // closure which was statically allocated in this frame doesn&#39;t
2798   // escape it by accident.
2799   set_cm_oop_closure(NULL);
2800   double end_time_ms = os::elapsedVTime() * 1000.0;
2801   double elapsed_time_ms = end_time_ms - _start_time_ms;
2802   // Update the step history.
2803   _step_times_ms.add(elapsed_time_ms);
2804 
2805   if (has_aborted()) {
2806     // The task was aborted for some reason.
2807     if (_has_timed_out) {
2808       double diff_ms = elapsed_time_ms - _time_target_ms;
2809       // Keep statistics of how well we did with respect to hitting
2810       // our target only if we actually timed out (if we aborted for
2811       // other reasons, then the results might get skewed).
<span class="line-modified">2812       _marking_step_diffs_ms.add(diff_ms);</span>
2813     }
2814 
2815     if (_cm-&gt;has_overflown()) {
2816       // This is the interesting one. We aborted because a global
2817       // overflow was raised. This means we have to restart the
2818       // marking phase and start iterating over regions. However, in
2819       // order to do this we have to make sure that all tasks stop
2820       // what they are doing and re-initialize in a safe manner. We
2821       // will achieve this with the use of two barrier sync points.
2822 
2823       if (!is_serial) {
2824         // We only need to enter the sync barrier if being called
2825         // from a parallel context
2826         _cm-&gt;enter_first_sync_barrier(_worker_id);
2827 
2828         // When we exit this sync barrier we know that all tasks have
2829         // stopped doing marking work. So, it&#39;s now safe to
2830         // re-initialize our data structures.
2831       }
2832 
</pre>
<hr />
<pre>
2875   _calls(0),
2876   _time_target_ms(0.0),
2877   _start_time_ms(0.0),
2878   _cm_oop_closure(NULL),
2879   _curr_region(NULL),
2880   _finger(NULL),
2881   _region_limit(NULL),
2882   _words_scanned(0),
2883   _words_scanned_limit(0),
2884   _real_words_scanned_limit(0),
2885   _refs_reached(0),
2886   _refs_reached_limit(0),
2887   _real_refs_reached_limit(0),
2888   _has_aborted(false),
2889   _has_timed_out(false),
2890   _draining_satb_buffers(false),
2891   _step_times_ms(),
2892   _elapsed_time_ms(0.0),
2893   _termination_time_ms(0.0),
2894   _termination_start_time_ms(0.0),
<span class="line-modified">2895   _marking_step_diffs_ms()</span>
2896 {
2897   guarantee(task_queue != NULL, &quot;invariant&quot;);
2898 
<span class="line-modified">2899   _marking_step_diffs_ms.add(0.5);</span>
2900 }
2901 
2902 // These are formatting macros that are used below to ensure
2903 // consistent formatting. The *_H_* versions are used to format the
2904 // header for a particular value and they should be kept consistent
2905 // with the corresponding macro. Also note that most of the macros add
2906 // the necessary white space (as a prefix) which makes them a bit
2907 // easier to compose.
2908 
2909 // All the output lines are prefixed with this string to be able to
2910 // identify them easily in a large log file.
2911 #define G1PPRL_LINE_PREFIX            &quot;###&quot;
2912 
2913 #define G1PPRL_ADDR_BASE_FORMAT    &quot; &quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT
2914 #ifdef _LP64
2915 #define G1PPRL_ADDR_BASE_H_FORMAT  &quot; %37s&quot;
2916 #else // _LP64
2917 #define G1PPRL_ADDR_BASE_H_FORMAT  &quot; %21s&quot;
2918 #endif // _LP64
2919 
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2001, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;classfile/classLoaderDataGraph.hpp&quot;
  27 #include &quot;code/codeCache.hpp&quot;
  28 #include &quot;gc/g1/g1BarrierSet.hpp&quot;
  29 #include &quot;gc/g1/g1CollectedHeap.inline.hpp&quot;
  30 #include &quot;gc/g1/g1CollectorState.hpp&quot;
  31 #include &quot;gc/g1/g1ConcurrentMark.inline.hpp&quot;
  32 #include &quot;gc/g1/g1ConcurrentMarkThread.inline.hpp&quot;
  33 #include &quot;gc/g1/g1DirtyCardQueue.hpp&quot;
  34 #include &quot;gc/g1/g1HeapVerifier.hpp&quot;
  35 #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
  36 #include &quot;gc/g1/g1Policy.hpp&quot;
  37 #include &quot;gc/g1/g1RegionMarkStatsCache.inline.hpp&quot;
  38 #include &quot;gc/g1/g1StringDedup.hpp&quot;
  39 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
<span class="line-added">  40 #include &quot;gc/g1/g1Trace.hpp&quot;</span>
  41 #include &quot;gc/g1/heapRegion.inline.hpp&quot;
  42 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
  43 #include &quot;gc/g1/heapRegionSet.inline.hpp&quot;
  44 #include &quot;gc/shared/gcId.hpp&quot;
  45 #include &quot;gc/shared/gcTimer.hpp&quot;

  46 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  47 #include &quot;gc/shared/gcVMOperations.hpp&quot;
  48 #include &quot;gc/shared/genOopClosures.inline.hpp&quot;
  49 #include &quot;gc/shared/referencePolicy.hpp&quot;
  50 #include &quot;gc/shared/strongRootsScope.hpp&quot;
  51 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
<span class="line-added">  52 #include &quot;gc/shared/taskTerminator.hpp&quot;</span>
  53 #include &quot;gc/shared/taskqueue.inline.hpp&quot;
  54 #include &quot;gc/shared/weakProcessor.inline.hpp&quot;
  55 #include &quot;gc/shared/workerPolicy.hpp&quot;
  56 #include &quot;include/jvm.h&quot;
  57 #include &quot;logging/log.hpp&quot;
  58 #include &quot;memory/allocation.hpp&quot;
<span class="line-added">  59 #include &quot;memory/iterator.hpp&quot;</span>
  60 #include &quot;memory/resourceArea.hpp&quot;
<span class="line-added">  61 #include &quot;memory/universe.hpp&quot;</span>
  62 #include &quot;oops/access.inline.hpp&quot;
  63 #include &quot;oops/oop.inline.hpp&quot;
  64 #include &quot;runtime/atomic.hpp&quot;
  65 #include &quot;runtime/handles.inline.hpp&quot;
  66 #include &quot;runtime/java.hpp&quot;
<span class="line-added">  67 #include &quot;runtime/orderAccess.hpp&quot;</span>
  68 #include &quot;runtime/prefetch.inline.hpp&quot;
  69 #include &quot;services/memTracker.hpp&quot;
  70 #include &quot;utilities/align.hpp&quot;
  71 #include &quot;utilities/growableArray.hpp&quot;
  72 
  73 bool G1CMBitMapClosure::do_addr(HeapWord* const addr) {
  74   assert(addr &lt; _cm-&gt;finger(), &quot;invariant&quot;);
  75   assert(addr &gt;= _task-&gt;finger(), &quot;invariant&quot;);
  76 
  77   // We move that task&#39;s local finger along.
  78   _task-&gt;move_finger_to(addr);
  79 
  80   _task-&gt;scan_task_entry(G1TaskQueueEntry::from_oop(oop(addr)));
  81   // we only partially drain the local queue and global stack
  82   _task-&gt;drain_local_queue(true);
  83   _task-&gt;drain_global_stack(true);
  84 
  85   // if the has_aborted flag has been raised, we need to bail out of
  86   // the iteration
  87   return !_task-&gt;has_aborted();
</pre>
<hr />
<pre>
 153     log_debug(gc)(&quot;Expanded mark stack capacity from &quot; SIZE_FORMAT &quot; to &quot; SIZE_FORMAT &quot; chunks&quot;,
 154                   old_capacity, new_capacity);
 155   } else {
 156     log_warning(gc)(&quot;Failed to expand mark stack capacity from &quot; SIZE_FORMAT &quot; to &quot; SIZE_FORMAT &quot; chunks&quot;,
 157                     old_capacity, new_capacity);
 158   }
 159 }
 160 
 161 G1CMMarkStack::~G1CMMarkStack() {
 162   if (_base != NULL) {
 163     MmapArrayAllocator&lt;TaskQueueEntryChunk&gt;::free(_base, _chunk_capacity);
 164   }
 165 }
 166 
 167 void G1CMMarkStack::add_chunk_to_list(TaskQueueEntryChunk* volatile* list, TaskQueueEntryChunk* elem) {
 168   elem-&gt;next = *list;
 169   *list = elem;
 170 }
 171 
 172 void G1CMMarkStack::add_chunk_to_chunk_list(TaskQueueEntryChunk* elem) {
<span class="line-modified"> 173   MutexLocker x(MarkStackChunkList_lock, Mutex::_no_safepoint_check_flag);</span>
 174   add_chunk_to_list(&amp;_chunk_list, elem);
 175   _chunks_in_chunk_list++;
 176 }
 177 
 178 void G1CMMarkStack::add_chunk_to_free_list(TaskQueueEntryChunk* elem) {
<span class="line-modified"> 179   MutexLocker x(MarkStackFreeList_lock, Mutex::_no_safepoint_check_flag);</span>
 180   add_chunk_to_list(&amp;_free_list, elem);
 181 }
 182 
 183 G1CMMarkStack::TaskQueueEntryChunk* G1CMMarkStack::remove_chunk_from_list(TaskQueueEntryChunk* volatile* list) {
 184   TaskQueueEntryChunk* result = *list;
 185   if (result != NULL) {
 186     *list = (*list)-&gt;next;
 187   }
 188   return result;
 189 }
 190 
 191 G1CMMarkStack::TaskQueueEntryChunk* G1CMMarkStack::remove_chunk_from_chunk_list() {
<span class="line-modified"> 192   MutexLocker x(MarkStackChunkList_lock, Mutex::_no_safepoint_check_flag);</span>
 193   TaskQueueEntryChunk* result = remove_chunk_from_list(&amp;_chunk_list);
 194   if (result != NULL) {
 195     _chunks_in_chunk_list--;
 196   }
 197   return result;
 198 }
 199 
 200 G1CMMarkStack::TaskQueueEntryChunk* G1CMMarkStack::remove_chunk_from_free_list() {
<span class="line-modified"> 201   MutexLocker x(MarkStackFreeList_lock, Mutex::_no_safepoint_check_flag);</span>
 202   return remove_chunk_from_list(&amp;_free_list);
 203 }
 204 
 205 G1CMMarkStack::TaskQueueEntryChunk* G1CMMarkStack::allocate_new_chunk() {
 206   // This dirty read of _hwm is okay because we only ever increase the _hwm in parallel code.
 207   // Further this limits _hwm to a value of _chunk_capacity + #threads, avoiding
 208   // wraparound of _hwm.
 209   if (_hwm &gt;= _chunk_capacity) {
 210     return NULL;
 211   }
 212 
<span class="line-modified"> 213   size_t cur_idx = Atomic::fetch_and_add(&amp;_hwm, 1u);</span>
 214   if (cur_idx &gt;= _chunk_capacity) {
 215     return NULL;
 216   }
 217 
 218   TaskQueueEntryChunk* result = ::new (&amp;_base[cur_idx]) TaskQueueEntryChunk;
 219   result-&gt;next = NULL;
 220   return result;
 221 }
 222 
 223 bool G1CMMarkStack::par_push_chunk(G1TaskQueueEntry* ptr_arr) {
 224   // Get a new chunk.
 225   TaskQueueEntryChunk* new_chunk = remove_chunk_from_free_list();
 226 
 227   if (new_chunk == NULL) {
 228     // Did not get a chunk from the free list. Allocate from backing memory.
 229     new_chunk = allocate_new_chunk();
 230 
 231     if (new_chunk == NULL) {
 232       return false;
 233     }
</pre>
<hr />
<pre>
 243 bool G1CMMarkStack::par_pop_chunk(G1TaskQueueEntry* ptr_arr) {
 244   TaskQueueEntryChunk* cur = remove_chunk_from_chunk_list();
 245 
 246   if (cur == NULL) {
 247     return false;
 248   }
 249 
 250   Copy::conjoint_memory_atomic(cur-&gt;data, ptr_arr, EntriesPerChunk * sizeof(G1TaskQueueEntry));
 251 
 252   add_chunk_to_free_list(cur);
 253   return true;
 254 }
 255 
 256 void G1CMMarkStack::set_empty() {
 257   _chunks_in_chunk_list = 0;
 258   _hwm = 0;
 259   _chunk_list = NULL;
 260   _free_list = NULL;
 261 }
 262 
<span class="line-modified"> 263 G1CMRootMemRegions::G1CMRootMemRegions(uint const max_regions) :</span>
<span class="line-modified"> 264     _root_regions(MemRegion::create_array(max_regions, mtGC)),</span>
<span class="line-modified"> 265     _max_regions(max_regions),</span>
<span class="line-modified"> 266     _num_root_regions(0),</span>
<span class="line-modified"> 267     _claimed_root_regions(0),</span>
<span class="line-modified"> 268     _scan_in_progress(false),</span>
<span class="line-modified"> 269     _should_abort(false) { }</span>
 270 
<span class="line-modified"> 271 G1CMRootMemRegions::~G1CMRootMemRegions() {</span>
<span class="line-modified"> 272   FREE_C_HEAP_ARRAY(MemRegion, _root_regions);</span>
 273 }
 274 
<span class="line-modified"> 275 void G1CMRootMemRegions::reset() {</span>
 276   _num_root_regions = 0;
 277 }
 278 
<span class="line-modified"> 279 void G1CMRootMemRegions::add(HeapWord* start, HeapWord* end) {</span>
 280   assert_at_safepoint();
<span class="line-modified"> 281   size_t idx = Atomic::fetch_and_add(&amp;_num_root_regions, 1u);</span>
<span class="line-modified"> 282   assert(idx &lt; _max_regions, &quot;Trying to add more root MemRegions than there is space &quot; SIZE_FORMAT, _max_regions);</span>
<span class="line-modified"> 283   assert(start != NULL &amp;&amp; end != NULL &amp;&amp; start &lt;= end, &quot;Start (&quot; PTR_FORMAT &quot;) should be less or equal to &quot;</span>
<span class="line-added"> 284          &quot;end (&quot; PTR_FORMAT &quot;)&quot;, p2i(start), p2i(end));</span>
<span class="line-added"> 285   _root_regions[idx].set_start(start);</span>
<span class="line-added"> 286   _root_regions[idx].set_end(end);</span>
 287 }
 288 
<span class="line-modified"> 289 void G1CMRootMemRegions::prepare_for_scan() {</span>
 290   assert(!scan_in_progress(), &quot;pre-condition&quot;);
 291 
 292   _scan_in_progress = _num_root_regions &gt; 0;
 293 
 294   _claimed_root_regions = 0;
 295   _should_abort = false;
 296 }
 297 
<span class="line-modified"> 298 const MemRegion* G1CMRootMemRegions::claim_next() {</span>
 299   if (_should_abort) {
 300     // If someone has set the should_abort flag, we return NULL to
 301     // force the caller to bail out of their loop.
 302     return NULL;
 303   }
 304 
 305   if (_claimed_root_regions &gt;= _num_root_regions) {
 306     return NULL;
 307   }
 308 
<span class="line-modified"> 309   size_t claimed_index = Atomic::fetch_and_add(&amp;_claimed_root_regions, 1u);</span>
 310   if (claimed_index &lt; _num_root_regions) {
<span class="line-modified"> 311     return &amp;_root_regions[claimed_index];</span>
 312   }
 313   return NULL;
 314 }
 315 
<span class="line-modified"> 316 uint G1CMRootMemRegions::num_root_regions() const {</span>
 317   return (uint)_num_root_regions;
 318 }
 319 
<span class="line-modified"> 320 void G1CMRootMemRegions::notify_scan_done() {</span>
<span class="line-modified"> 321   MutexLocker x(RootRegionScan_lock, Mutex::_no_safepoint_check_flag);</span>
 322   _scan_in_progress = false;
 323   RootRegionScan_lock-&gt;notify_all();
 324 }
 325 
<span class="line-modified"> 326 void G1CMRootMemRegions::cancel_scan() {</span>
 327   notify_scan_done();
 328 }
 329 
<span class="line-modified"> 330 void G1CMRootMemRegions::scan_finished() {</span>
 331   assert(scan_in_progress(), &quot;pre-condition&quot;);
 332 
 333   if (!_should_abort) {
 334     assert(_claimed_root_regions &gt;= num_root_regions(),
 335            &quot;we should have claimed all root regions, claimed &quot; SIZE_FORMAT &quot;, length = %u&quot;,
 336            _claimed_root_regions, num_root_regions());
 337   }
 338 
 339   notify_scan_done();
 340 }
 341 
<span class="line-modified"> 342 bool G1CMRootMemRegions::wait_until_scan_finished() {</span>
 343   if (!scan_in_progress()) {
 344     return false;
 345   }
 346 
 347   {
<span class="line-modified"> 348     MonitorLocker ml(RootRegionScan_lock, Mutex::_no_safepoint_check_flag);</span>
 349     while (scan_in_progress()) {
<span class="line-modified"> 350       ml.wait();</span>
 351     }
 352   }
 353   return true;
 354 }
 355 
 356 // Returns the maximum number of workers to be used in a concurrent
 357 // phase based on the number of GC workers being used in a STW
 358 // phase.
 359 static uint scale_concurrent_worker_threads(uint num_gc_workers) {
 360   return MAX2((num_gc_workers + 2) / 4, 1U);
 361 }
 362 
 363 G1ConcurrentMark::G1ConcurrentMark(G1CollectedHeap* g1h,
 364                                    G1RegionToSpaceMapper* prev_bitmap_storage,
 365                                    G1RegionToSpaceMapper* next_bitmap_storage) :
 366   // _cm_thread set inside the constructor
 367   _g1h(g1h),
 368   _completed_initialization(false),
 369 
 370   _mark_bitmap_1(),
</pre>
<hr />
<pre>
 414   _max_concurrent_workers(0),
 415 
 416   _region_mark_stats(NEW_C_HEAP_ARRAY(G1RegionMarkStats, _g1h-&gt;max_regions(), mtGC)),
 417   _top_at_rebuild_starts(NEW_C_HEAP_ARRAY(HeapWord*, _g1h-&gt;max_regions(), mtGC))
 418 {
 419   _mark_bitmap_1.initialize(g1h-&gt;reserved_region(), prev_bitmap_storage);
 420   _mark_bitmap_2.initialize(g1h-&gt;reserved_region(), next_bitmap_storage);
 421 
 422   // Create &amp; start ConcurrentMark thread.
 423   _cm_thread = new G1ConcurrentMarkThread(this);
 424   if (_cm_thread-&gt;osthread() == NULL) {
 425     vm_shutdown_during_initialization(&quot;Could not create ConcurrentMarkThread&quot;);
 426   }
 427 
 428   assert(CGC_lock != NULL, &quot;CGC_lock must be initialized&quot;);
 429 
 430   if (FLAG_IS_DEFAULT(ConcGCThreads) || ConcGCThreads == 0) {
 431     // Calculate the number of concurrent worker threads by scaling
 432     // the number of parallel GC threads.
 433     uint marking_thread_num = scale_concurrent_worker_threads(ParallelGCThreads);
<span class="line-modified"> 434     FLAG_SET_ERGO(ConcGCThreads, marking_thread_num);</span>
 435   }
 436 
 437   assert(ConcGCThreads &gt; 0, &quot;ConcGCThreads have been set.&quot;);
 438   if (ConcGCThreads &gt; ParallelGCThreads) {
 439     log_warning(gc)(&quot;More ConcGCThreads (%u) than ParallelGCThreads (%u).&quot;,
 440                     ConcGCThreads, ParallelGCThreads);
 441     return;
 442   }
 443 
 444   log_debug(gc)(&quot;ConcGCThreads: %u offset %u&quot;, ConcGCThreads, _worker_id_offset);
 445   log_debug(gc)(&quot;ParallelGCThreads: %u&quot;, ParallelGCThreads);
 446 
 447   _num_concurrent_workers = ConcGCThreads;
 448   _max_concurrent_workers = _num_concurrent_workers;
 449 
 450   _concurrent_workers = new WorkGang(&quot;G1 Conc&quot;, _max_concurrent_workers, false, true);
 451   _concurrent_workers-&gt;initialize_workers();
 452 
 453   if (FLAG_IS_DEFAULT(MarkStackSize)) {
 454     size_t mark_stack_size =
 455       MIN2(MarkStackSizeMax,
 456           MAX2(MarkStackSize, (size_t) (_max_concurrent_workers * TASKQUEUE_SIZE)));
 457     // Verify that the calculated value for MarkStackSize is in range.
 458     // It would be nice to use the private utility routine from Arguments.
 459     if (!(mark_stack_size &gt;= 1 &amp;&amp; mark_stack_size &lt;= MarkStackSizeMax)) {
 460       log_warning(gc)(&quot;Invalid value calculated for MarkStackSize (&quot; SIZE_FORMAT &quot;): &quot;
 461                       &quot;must be between 1 and &quot; SIZE_FORMAT,
 462                       mark_stack_size, MarkStackSizeMax);
 463       return;
 464     }
<span class="line-modified"> 465     FLAG_SET_ERGO(MarkStackSize, mark_stack_size);</span>
 466   } else {
 467     // Verify MarkStackSize is in range.
 468     if (FLAG_IS_CMDLINE(MarkStackSize)) {
 469       if (FLAG_IS_DEFAULT(MarkStackSizeMax)) {
 470         if (!(MarkStackSize &gt;= 1 &amp;&amp; MarkStackSize &lt;= MarkStackSizeMax)) {
 471           log_warning(gc)(&quot;Invalid value specified for MarkStackSize (&quot; SIZE_FORMAT &quot;): &quot;
 472                           &quot;must be between 1 and &quot; SIZE_FORMAT,
 473                           MarkStackSize, MarkStackSizeMax);
 474           return;
 475         }
 476       } else if (FLAG_IS_CMDLINE(MarkStackSizeMax)) {
 477         if (!(MarkStackSize &gt;= 1 &amp;&amp; MarkStackSize &lt;= MarkStackSizeMax)) {
 478           log_warning(gc)(&quot;Invalid value specified for MarkStackSize (&quot; SIZE_FORMAT &quot;)&quot;
 479                           &quot; or for MarkStackSizeMax (&quot; SIZE_FORMAT &quot;)&quot;,
 480                           MarkStackSize, MarkStackSizeMax);
 481           return;
 482         }
 483       }
 484     }
 485   }
</pre>
<hr />
<pre>
 579     for (uint i = 0; i &lt; max_regions; i++) {
 580       _region_mark_stats[i].clear_during_overflow();
 581     }
 582   }
 583 
 584   clear_has_overflown();
 585   _finger = _heap.start();
 586 
 587   for (uint i = 0; i &lt; _max_num_tasks; ++i) {
 588     G1CMTaskQueue* queue = _task_queues-&gt;queue(i);
 589     queue-&gt;set_empty();
 590   }
 591 }
 592 
 593 void G1ConcurrentMark::set_concurrency(uint active_tasks) {
 594   assert(active_tasks &lt;= _max_num_tasks, &quot;we should not have more&quot;);
 595 
 596   _num_active_tasks = active_tasks;
 597   // Need to update the three data structures below according to the
 598   // number of active threads for this phase.
<span class="line-modified"> 599   _terminator.reset_for_reuse(active_tasks);</span>
 600   _first_overflow_barrier_sync.set_n_workers((int) active_tasks);
 601   _second_overflow_barrier_sync.set_n_workers((int) active_tasks);
 602 }
 603 
 604 void G1ConcurrentMark::set_concurrency_and_phase(uint active_tasks, bool concurrent) {
 605   set_concurrency(active_tasks);
 606 
 607   _concurrent = concurrent;
 608 
 609   if (!concurrent) {
 610     // At this point we should be in a STW phase, and completed marking.
 611     assert_at_safepoint_on_vm_thread();
 612     assert(out_of_regions(),
 613            &quot;only way to get here: _finger: &quot; PTR_FORMAT &quot;, _heap_end: &quot; PTR_FORMAT,
 614            p2i(_finger), p2i(_heap.end()));
 615   }
 616 }
 617 
 618 void G1ConcurrentMark::reset_at_marking_complete() {
 619   // We set the global marking state to some default values when we&#39;re
</pre>
<hr />
<pre>
 724 
 725   // Repeat the asserts from above.
 726   guarantee(cm_thread()-&gt;during_cycle(), &quot;invariant&quot;);
 727   guarantee(!_g1h-&gt;collector_state()-&gt;mark_or_rebuild_in_progress(), &quot;invariant&quot;);
 728 }
 729 
 730 void G1ConcurrentMark::clear_prev_bitmap(WorkGang* workers) {
 731   assert_at_safepoint_on_vm_thread();
 732   clear_bitmap(_prev_mark_bitmap, workers, false);
 733 }
 734 
 735 class NoteStartOfMarkHRClosure : public HeapRegionClosure {
 736 public:
 737   bool do_heap_region(HeapRegion* r) {
 738     r-&gt;note_start_of_marking();
 739     return false;
 740   }
 741 };
 742 
 743 void G1ConcurrentMark::pre_initial_mark() {
<span class="line-modified"> 744   assert_at_safepoint_on_vm_thread();</span>
<span class="line-added"> 745 </span>
<span class="line-added"> 746   // Reset marking state.</span>
 747   reset();
 748 
 749   // For each region note start of marking.
 750   NoteStartOfMarkHRClosure startcl;
 751   _g1h-&gt;heap_region_iterate(&amp;startcl);
 752 
 753   _root_regions.reset();
 754 }
 755 
 756 
 757 void G1ConcurrentMark::post_initial_mark() {
 758   // Start Concurrent Marking weak-reference discovery.
 759   ReferenceProcessor* rp = _g1h-&gt;ref_processor_cm();
 760   // enable (&quot;weak&quot;) refs discovery
 761   rp-&gt;enable_discovery();
 762   rp-&gt;setup_policy(false); // snapshot the soft ref policy to be used in this cycle
 763 
 764   SATBMarkQueueSet&amp; satb_mq_set = G1BarrierSet::satb_mark_queue_set();
 765   // This is the start of  the marking cycle, we&#39;re expected all
 766   // threads to have SATB queues with active set to false.
</pre>
<hr />
<pre>
 864   uint result = 0;
 865   if (!UseDynamicNumberOfGCThreads ||
 866       (!FLAG_IS_DEFAULT(ConcGCThreads) &amp;&amp;
 867        !ForceDynamicNumberOfGCThreads)) {
 868     result = _max_concurrent_workers;
 869   } else {
 870     result =
 871       WorkerPolicy::calc_default_active_workers(_max_concurrent_workers,
 872                                                 1, /* Minimum workers */
 873                                                 _num_concurrent_workers,
 874                                                 Threads::number_of_non_daemon_threads());
 875     // Don&#39;t scale the result down by scale_concurrent_workers() because
 876     // that scaling has already gone into &quot;_max_concurrent_workers&quot;.
 877   }
 878   assert(result &gt; 0 &amp;&amp; result &lt;= _max_concurrent_workers,
 879          &quot;Calculated number of marking workers must be larger than zero and at most the maximum %u, but is %u&quot;,
 880          _max_concurrent_workers, result);
 881   return result;
 882 }
 883 
<span class="line-modified"> 884 void G1ConcurrentMark::scan_root_region(const MemRegion* region, uint worker_id) {</span>
<span class="line-modified"> 885 #ifdef ASSERT</span>
<span class="line-modified"> 886   HeapWord* last = region-&gt;last();</span>
<span class="line-added"> 887   HeapRegion* hr = _g1h-&gt;heap_region_containing(last);</span>
<span class="line-added"> 888   assert(hr-&gt;is_old() || hr-&gt;next_top_at_mark_start() == hr-&gt;bottom(),</span>
<span class="line-added"> 889          &quot;Root regions must be old or survivor/eden but region %u is %s&quot;, hr-&gt;hrm_index(), hr-&gt;get_type_str());</span>
<span class="line-added"> 890   assert(hr-&gt;next_top_at_mark_start() == region-&gt;start(),</span>
<span class="line-added"> 891          &quot;MemRegion start should be equal to nTAMS&quot;);</span>
<span class="line-added"> 892 #endif</span>
<span class="line-added"> 893 </span>
 894   G1RootRegionScanClosure cl(_g1h, this, worker_id);
 895 
 896   const uintx interval = PrefetchScanIntervalInBytes;
<span class="line-modified"> 897   HeapWord* curr = region-&gt;start();</span>
<span class="line-modified"> 898   const HeapWord* end = region-&gt;end();</span>
 899   while (curr &lt; end) {
 900     Prefetch::read(curr, interval);
 901     oop obj = oop(curr);
 902     int size = obj-&gt;oop_iterate_size(&amp;cl);
 903     assert(size == obj-&gt;size(), &quot;sanity&quot;);
 904     curr += size;
 905   }
 906 }
 907 
 908 class G1CMRootRegionScanTask : public AbstractGangTask {
 909   G1ConcurrentMark* _cm;
 910 public:
 911   G1CMRootRegionScanTask(G1ConcurrentMark* cm) :
 912     AbstractGangTask(&quot;G1 Root Region Scan&quot;), _cm(cm) { }
 913 
 914   void work(uint worker_id) {
 915     assert(Thread::current()-&gt;is_ConcurrentGC_thread(),
 916            &quot;this should only be done by a conc GC thread&quot;);
 917 
<span class="line-modified"> 918     G1CMRootMemRegions* root_regions = _cm-&gt;root_regions();</span>
<span class="line-modified"> 919     const MemRegion* region = root_regions-&gt;claim_next();</span>
<span class="line-modified"> 920     while (region != NULL) {</span>
<span class="line-modified"> 921       _cm-&gt;scan_root_region(region, worker_id);</span>
<span class="line-modified"> 922       region = root_regions-&gt;claim_next();</span>
 923     }
 924   }
 925 };
 926 
 927 void G1ConcurrentMark::scan_root_regions() {
 928   // scan_in_progress() will have been set to true only if there was
 929   // at least one root region to scan. So, if it&#39;s false, we
 930   // should not attempt to do any further work.
 931   if (root_regions()-&gt;scan_in_progress()) {
 932     assert(!has_aborted(), &quot;Aborting before root region scanning is finished not supported.&quot;);
 933 
 934     _num_concurrent_workers = MIN2(calc_active_marking_workers(),
 935                                    // We distribute work on a per-region basis, so starting
 936                                    // more threads than that is useless.
 937                                    root_regions()-&gt;num_root_regions());
 938     assert(_num_concurrent_workers &lt;= _max_concurrent_workers,
 939            &quot;Maximum number of marking threads exceeded&quot;);
 940 
 941     G1CMRootRegionScanTask task(this);
 942     log_debug(gc, ergo)(&quot;Running %s using %u workers for %u work units.&quot;,
</pre>
<hr />
<pre>
1102       _g1h(g1h), _cm(cm), _cl(cl), _num_regions_selected_for_rebuild(0) { }
1103 
1104     virtual bool do_heap_region(HeapRegion* r) {
1105       update_remset_before_rebuild(r);
1106       update_marked_bytes(r);
1107 
1108       return false;
1109     }
1110 
1111     uint num_selected_for_rebuild() const { return _num_regions_selected_for_rebuild; }
1112   };
1113 
1114 public:
1115   G1UpdateRemSetTrackingBeforeRebuildTask(G1CollectedHeap* g1h, G1ConcurrentMark* cm, uint num_workers) :
1116     AbstractGangTask(&quot;G1 Update RemSet Tracking Before Rebuild&quot;),
1117     _g1h(g1h), _cm(cm), _hrclaimer(num_workers), _total_selected_for_rebuild(0), _cl(&quot;Post-Marking&quot;) { }
1118 
1119   virtual void work(uint worker_id) {
1120     G1UpdateRemSetTrackingBeforeRebuild update_cl(_g1h, _cm, &amp;_cl);
1121     _g1h-&gt;heap_region_par_iterate_from_worker_offset(&amp;update_cl, &amp;_hrclaimer, worker_id);
<span class="line-modified">1122     Atomic::add(&amp;_total_selected_for_rebuild, update_cl.num_selected_for_rebuild());</span>
1123   }
1124 
1125   uint total_selected_for_rebuild() const { return _total_selected_for_rebuild; }
1126 
1127   // Number of regions for which roughly one thread should be spawned for this work.
1128   static const uint RegionsPerThread = 384;
1129 };
1130 
1131 class G1UpdateRemSetTrackingAfterRebuild : public HeapRegionClosure {
1132   G1CollectedHeap* _g1h;
1133 public:
1134   G1UpdateRemSetTrackingAfterRebuild(G1CollectedHeap* g1h) : _g1h(g1h) { }
1135 
1136   virtual bool do_heap_region(HeapRegion* r) {
1137     _g1h-&gt;policy()-&gt;remset_tracker()-&gt;update_after_rebuild(r);
1138     return false;
1139   }
1140 };
1141 
1142 void G1ConcurrentMark::remark() {
</pre>
<hr />
<pre>
1253                                  FreeRegionList* local_cleanup_list) :
1254       _g1h(g1h),
1255       _freed_bytes(0),
1256       _local_cleanup_list(local_cleanup_list),
1257       _old_regions_removed(0),
1258       _humongous_regions_removed(0) { }
1259 
1260     size_t freed_bytes() { return _freed_bytes; }
1261     const uint old_regions_removed() { return _old_regions_removed; }
1262     const uint humongous_regions_removed() { return _humongous_regions_removed; }
1263 
1264     bool do_heap_region(HeapRegion *hr) {
1265       if (hr-&gt;used() &gt; 0 &amp;&amp; hr-&gt;max_live_bytes() == 0 &amp;&amp; !hr-&gt;is_young() &amp;&amp; !hr-&gt;is_archive()) {
1266         _freed_bytes += hr-&gt;used();
1267         hr-&gt;set_containing_set(NULL);
1268         if (hr-&gt;is_humongous()) {
1269           _humongous_regions_removed++;
1270           _g1h-&gt;free_humongous_region(hr, _local_cleanup_list);
1271         } else {
1272           _old_regions_removed++;
<span class="line-modified">1273           _g1h-&gt;free_region(hr, _local_cleanup_list);</span>
1274         }
1275         hr-&gt;clear_cardtable();
1276         _g1h-&gt;concurrent_mark()-&gt;clear_statistics_in_region(hr-&gt;hrm_index());
1277         log_trace(gc)(&quot;Reclaimed empty region %u (%s) bot &quot; PTR_FORMAT, hr-&gt;hrm_index(), hr-&gt;get_short_type_str(), p2i(hr-&gt;bottom()));
1278       }
1279 
1280       return false;
1281     }
1282   };
1283 
1284   G1CollectedHeap* _g1h;
1285   FreeRegionList* _cleanup_list;
1286   HeapRegionClaimer _hrclaimer;
1287 
1288 public:
1289   G1ReclaimEmptyRegionsTask(G1CollectedHeap* g1h, FreeRegionList* cleanup_list, uint n_workers) :
1290     AbstractGangTask(&quot;G1 Cleanup&quot;),
1291     _g1h(g1h),
1292     _cleanup_list(cleanup_list),
1293     _hrclaimer(n_workers) {
1294   }
1295 
1296   void work(uint worker_id) {
1297     FreeRegionList local_cleanup_list(&quot;Local Cleanup List&quot;);
1298     G1ReclaimEmptyRegionsClosure cl(_g1h, &amp;local_cleanup_list);
1299     _g1h-&gt;heap_region_par_iterate_from_worker_offset(&amp;cl, &amp;_hrclaimer, worker_id);
1300     assert(cl.is_complete(), &quot;Shouldn&#39;t have aborted!&quot;);
1301 
1302     // Now update the old/humongous region sets
1303     _g1h-&gt;remove_from_old_sets(cl.old_regions_removed(), cl.humongous_regions_removed());
1304     {
<span class="line-modified">1305       MutexLocker x(ParGCRareEvent_lock, Mutex::_no_safepoint_check_flag);</span>
1306       _g1h-&gt;decrement_summary_bytes(cl.freed_bytes());
1307 
1308       _cleanup_list-&gt;add_ordered(&amp;local_cleanup_list);
1309       assert(local_cleanup_list.is_empty(), &quot;post-condition&quot;);
1310     }
1311   }
1312 };
1313 
1314 void G1ConcurrentMark::reclaim_empty_regions() {
1315   WorkGang* workers = _g1h-&gt;workers();
1316   FreeRegionList empty_regions_list(&quot;Empty Regions After Mark List&quot;);
1317 
1318   G1ReclaimEmptyRegionsTask cl(_g1h, &amp;empty_regions_list, workers-&gt;active_workers());
1319   workers-&gt;run_task(&amp;cl);
1320 
1321   if (!empty_regions_list.is_empty()) {
1322     log_debug(gc)(&quot;Reclaimed %u empty regions&quot;, empty_regions_list.length());
1323     // Now print the empty regions list.
1324     G1HRPrinter* hrp = _g1h-&gt;hr_printer();
1325     if (hrp-&gt;is_active()) {
</pre>
<hr />
<pre>
1592     // JNI references during parallel reference processing.
1593     //
1594     // These closures do not need to synchronize with the worker
1595     // threads involved in parallel reference processing as these
1596     // instances are executed serially by the current thread (e.g.
1597     // reference processing is not multi-threaded and is thus
1598     // performed by the current thread instead of a gang worker).
1599     //
1600     // The gang tasks involved in parallel reference processing create
1601     // their own instances of these closures, which do their own
1602     // synchronization among themselves.
1603     G1CMKeepAliveAndDrainClosure g1_keep_alive(this, task(0), true /* is_serial */);
1604     G1CMDrainMarkingStackClosure g1_drain_mark_stack(this, task(0), true /* is_serial */);
1605 
1606     // We need at least one active thread. If reference processing
1607     // is not multi-threaded we use the current (VMThread) thread,
1608     // otherwise we use the work gang from the G1CollectedHeap and
1609     // we utilize all the worker threads we can.
1610     bool processing_is_mt = rp-&gt;processing_is_mt();
1611     uint active_workers = (processing_is_mt ? _g1h-&gt;workers()-&gt;active_workers() : 1U);
<span class="line-modified">1612     active_workers = clamp(active_workers, 1u, _max_num_tasks);</span>
1613 
1614     // Parallel processing task executor.
1615     G1CMRefProcTaskExecutor par_task_executor(_g1h, this,
1616                                               _g1h-&gt;workers(), active_workers);
1617     AbstractRefProcTaskExecutor* executor = (processing_is_mt ? &amp;par_task_executor : NULL);
1618 
1619     // Set the concurrency level. The phase was already set prior to
1620     // executing the remark task.
1621     set_concurrency(active_workers);
1622 
1623     // Set the degree of MT processing here.  If the discovery was done MT,
1624     // the number of threads involved during discovery could differ from
1625     // the number of active workers.  This is OK as long as the discovered
1626     // Reference lists are balanced (see balance_all_queues() and balance_queues()).
1627     rp-&gt;set_active_mt_degree(active_workers);
1628 
1629     ReferenceProcessorPhaseTimes pt(_gc_timer_cm, rp-&gt;max_num_queues());
1630 
1631     // Process the weak references.
1632     const ReferenceProcessorStats&amp; stats =
</pre>
<hr />
<pre>
1707   G1PrecleanYieldClosure yield_cl(this);
1708 
1709   ReferenceProcessor* rp = _g1h-&gt;ref_processor_cm();
1710   // Precleaning is single threaded. Temporarily disable MT discovery.
1711   ReferenceProcessorMTDiscoveryMutator rp_mut_discovery(rp, false);
1712   rp-&gt;preclean_discovered_references(rp-&gt;is_alive_non_header(),
1713                                      &amp;keep_alive,
1714                                      &amp;drain_mark_stack,
1715                                      &amp;yield_cl,
1716                                      _gc_timer_cm);
1717 }
1718 
1719 // When sampling object counts, we already swapped the mark bitmaps, so we need to use
1720 // the prev bitmap determining liveness.
1721 class G1ObjectCountIsAliveClosure: public BoolObjectClosure {
1722   G1CollectedHeap* _g1h;
1723 public:
1724   G1ObjectCountIsAliveClosure(G1CollectedHeap* g1h) : _g1h(g1h) { }
1725 
1726   bool do_object_b(oop obj) {
<span class="line-modified">1727     return obj != NULL &amp;&amp;</span>
<span class="line-modified">1728            (!_g1h-&gt;is_in_g1_reserved(obj) || !_g1h-&gt;is_obj_dead(obj));</span>

1729   }
1730 };
1731 
1732 void G1ConcurrentMark::report_object_count(bool mark_completed) {
1733   // Depending on the completion of the marking liveness needs to be determined
1734   // using either the next or prev bitmap.
1735   if (mark_completed) {
1736     G1ObjectCountIsAliveClosure is_alive(_g1h);
1737     _gc_tracer_cm-&gt;report_object_count_after_gc(&amp;is_alive);
1738   } else {
1739     G1CMIsAliveClosure is_alive(_g1h);
1740     _gc_tracer_cm-&gt;report_object_count_after_gc(&amp;is_alive);
1741   }
1742 }
1743 
1744 
1745 void G1ConcurrentMark::swap_mark_bitmaps() {
1746   G1CMBitMap* temp = _prev_mark_bitmap;
1747   _prev_mark_bitmap = _next_mark_bitmap;
1748   _next_mark_bitmap = temp;
</pre>
<hr />
<pre>
1762     _task-&gt;increment_refs_reached();
1763     oop const obj = static_cast&lt;oop&gt;(entry);
1764     _task-&gt;make_reference_grey(obj);
1765   }
1766 
1767 public:
1768   G1CMSATBBufferClosure(G1CMTask* task, G1CollectedHeap* g1h)
1769     : _task(task), _g1h(g1h) { }
1770 
1771   virtual void do_buffer(void** buffer, size_t size) {
1772     for (size_t i = 0; i &lt; size; ++i) {
1773       do_entry(buffer[i]);
1774     }
1775   }
1776 };
1777 
1778 class G1RemarkThreadsClosure : public ThreadClosure {
1779   G1CMSATBBufferClosure _cm_satb_cl;
1780   G1CMOopClosure _cm_cl;
1781   MarkingCodeBlobClosure _code_cl;
<span class="line-modified">1782   uintx _claim_token;</span>
1783 
1784  public:
1785   G1RemarkThreadsClosure(G1CollectedHeap* g1h, G1CMTask* task) :
1786     _cm_satb_cl(task, g1h),
1787     _cm_cl(g1h, task),
1788     _code_cl(&amp;_cm_cl, !CodeBlobToOopClosure::FixRelocations),
<span class="line-modified">1789     _claim_token(Threads::thread_claim_token()) {}</span>
1790 
1791   void do_thread(Thread* thread) {
<span class="line-modified">1792     if (thread-&gt;claim_threads_do(true, _claim_token)) {</span>
1793       SATBMarkQueue&amp; queue = G1ThreadLocalData::satb_mark_queue(thread);
1794       queue.apply_closure_and_empty(&amp;_cm_satb_cl);
1795       if (thread-&gt;is_Java_thread()) {
1796         // In theory it should not be neccessary to explicitly walk the nmethods to find roots for concurrent marking
1797         // however the liveness of oops reachable from nmethods have very complex lifecycles:
1798         // * Alive if on the stack of an executing method
1799         // * Weakly reachable otherwise
1800         // Some objects reachable from nmethods, such as the class loader (or klass_holder) of the receiver should be
1801         // live by the SATB invariant but other oops recorded in nmethods may behave differently.
1802         JavaThread* jt = (JavaThread*)thread;
1803         jt-&gt;nmethods_do(&amp;_code_cl);
1804       }
1805     }
1806   }
1807 };
1808 
1809 class G1CMRemarkTask : public AbstractGangTask {
1810   G1ConcurrentMark* _cm;
1811 public:
1812   void work(uint worker_id) {
</pre>
<hr />
<pre>
1886 void G1ConcurrentMark::clear_range_in_prev_bitmap(MemRegion mr) {
1887   _prev_mark_bitmap-&gt;clear_range(mr);
1888 }
1889 
1890 HeapRegion*
1891 G1ConcurrentMark::claim_region(uint worker_id) {
1892   // &quot;checkpoint&quot; the finger
1893   HeapWord* finger = _finger;
1894 
1895   while (finger &lt; _heap.end()) {
1896     assert(_g1h-&gt;is_in_g1_reserved(finger), &quot;invariant&quot;);
1897 
1898     HeapRegion* curr_region = _g1h-&gt;heap_region_containing(finger);
1899     // Make sure that the reads below do not float before loading curr_region.
1900     OrderAccess::loadload();
1901     // Above heap_region_containing may return NULL as we always scan claim
1902     // until the end of the heap. In this case, just jump to the next region.
1903     HeapWord* end = curr_region != NULL ? curr_region-&gt;end() : finger + HeapRegion::GrainWords;
1904 
1905     // Is the gap between reading the finger and doing the CAS too long?
<span class="line-modified">1906     HeapWord* res = Atomic::cmpxchg(&amp;_finger, finger, end);</span>
1907     if (res == finger &amp;&amp; curr_region != NULL) {
1908       // we succeeded
1909       HeapWord*   bottom        = curr_region-&gt;bottom();
1910       HeapWord*   limit         = curr_region-&gt;next_top_at_mark_start();
1911 
1912       // notice that _finger == end cannot be guaranteed here since,
1913       // someone else might have moved the finger even further
1914       assert(_finger &gt;= end, &quot;the finger should have moved forward&quot;);
1915 
1916       if (limit &gt; bottom) {
1917         return curr_region;
1918       } else {
1919         assert(limit == bottom,
1920                &quot;the region limit should be at bottom&quot;);
1921         // we return NULL and the caller should try calling
1922         // claim_region() again.
1923         return NULL;
1924       }
1925     } else {
1926       assert(_finger &gt; finger, &quot;the finger should have moved forward&quot;);
</pre>
<hr />
<pre>
1936 class VerifyNoCSetOops {
1937   G1CollectedHeap* _g1h;
1938   const char* _phase;
1939   int _info;
1940 
1941 public:
1942   VerifyNoCSetOops(const char* phase, int info = -1) :
1943     _g1h(G1CollectedHeap::heap()),
1944     _phase(phase),
1945     _info(info)
1946   { }
1947 
1948   void operator()(G1TaskQueueEntry task_entry) const {
1949     if (task_entry.is_array_slice()) {
1950       guarantee(_g1h-&gt;is_in_reserved(task_entry.slice()), &quot;Slice &quot; PTR_FORMAT &quot; must be in heap.&quot;, p2i(task_entry.slice()));
1951       return;
1952     }
1953     guarantee(oopDesc::is_oop(task_entry.obj()),
1954               &quot;Non-oop &quot; PTR_FORMAT &quot;, phase: %s, info: %d&quot;,
1955               p2i(task_entry.obj()), _phase, _info);
<span class="line-modified">1956     HeapRegion* r = _g1h-&gt;heap_region_containing(task_entry.obj());</span>
<span class="line-modified">1957     guarantee(!(r-&gt;in_collection_set() || r-&gt;has_index_in_opt_cset()),</span>
<span class="line-modified">1958               &quot;obj &quot; PTR_FORMAT &quot; from %s (%d) in region %u in (optional) collection set&quot;,</span>
<span class="line-added">1959               p2i(task_entry.obj()), _phase, _info, r-&gt;hrm_index());</span>
1960   }
1961 };
1962 
<span class="line-modified">1963 void G1ConcurrentMark::verify_no_collection_set_oops() {</span>
1964   assert(SafepointSynchronize::is_at_safepoint(), &quot;should be at a safepoint&quot;);
1965   if (!_g1h-&gt;collector_state()-&gt;mark_or_rebuild_in_progress()) {
1966     return;
1967   }
1968 
1969   // Verify entries on the global mark stack
1970   _global_mark_stack.iterate(VerifyNoCSetOops(&quot;Stack&quot;));
1971 
1972   // Verify entries on the task queues
1973   for (uint i = 0; i &lt; _max_num_tasks; ++i) {
1974     G1CMTaskQueue* queue = _task_queues-&gt;queue(i);
1975     queue-&gt;iterate(VerifyNoCSetOops(&quot;Queue&quot;, i));
1976   }
1977 
1978   // Verify the global finger
1979   HeapWord* global_finger = finger();
1980   if (global_finger != NULL &amp;&amp; global_finger &lt; _heap.end()) {
1981     // Since we always iterate over all regions, we might get a NULL HeapRegion
1982     // here.
1983     HeapRegion* global_hr = _g1h-&gt;heap_region_containing(global_finger);
1984     guarantee(global_hr == NULL || global_finger == global_hr-&gt;bottom(),
1985               &quot;global finger: &quot; PTR_FORMAT &quot; region: &quot; HR_FORMAT,
1986               p2i(global_finger), HR_FORMAT_PARAMS(global_hr));
1987   }
1988 
1989   // Verify the task fingers
1990   assert(_num_concurrent_workers &lt;= _max_num_tasks, &quot;sanity&quot;);
1991   for (uint i = 0; i &lt; _num_concurrent_workers; ++i) {
1992     G1CMTask* task = _tasks[i];
1993     HeapWord* task_finger = task-&gt;finger();
1994     if (task_finger != NULL &amp;&amp; task_finger &lt; _heap.end()) {
1995       // See above note on the global finger verification.
<span class="line-modified">1996       HeapRegion* r = _g1h-&gt;heap_region_containing(task_finger);</span>
<span class="line-modified">1997       guarantee(r == NULL || task_finger == r-&gt;bottom() ||</span>
<span class="line-modified">1998                 !r-&gt;in_collection_set() || !r-&gt;has_index_in_opt_cset(),</span>
1999                 &quot;task finger: &quot; PTR_FORMAT &quot; region: &quot; HR_FORMAT,
<span class="line-modified">2000                 p2i(task_finger), HR_FORMAT_PARAMS(r));</span>
2001     }
2002   }
2003 }
2004 #endif // PRODUCT
2005 
2006 void G1ConcurrentMark::rebuild_rem_set_concurrently() {
2007   _g1h-&gt;rem_set()-&gt;rebuild_rem_set(this, _concurrent_workers, _worker_id_offset);
2008 }
2009 
2010 void G1ConcurrentMark::print_stats() {
2011   if (!log_is_enabled(Debug, gc, stats)) {
2012     return;
2013   }
2014   log_debug(gc, stats)(&quot;---------------------------------------------------------------------&quot;);
2015   for (size_t i = 0; i &lt; _num_active_tasks; ++i) {
2016     _tasks[i]-&gt;print_stats();
2017     log_debug(gc, stats)(&quot;---------------------------------------------------------------------&quot;);
2018   }
2019 }
2020 
</pre>
<hr />
<pre>
2399   if (has_aborted()) {
2400     return;
2401   }
2402 
2403   // We set this so that the regular clock knows that we&#39;re in the
2404   // middle of draining buffers and doesn&#39;t set the abort flag when it
2405   // notices that SATB buffers are available for draining. It&#39;d be
2406   // very counter productive if it did that. :-)
2407   _draining_satb_buffers = true;
2408 
2409   G1CMSATBBufferClosure satb_cl(this, _g1h);
2410   SATBMarkQueueSet&amp; satb_mq_set = G1BarrierSet::satb_mark_queue_set();
2411 
2412   // This keeps claiming and applying the closure to completed buffers
2413   // until we run out of buffers or we need to abort.
2414   while (!has_aborted() &amp;&amp;
2415          satb_mq_set.apply_closure_to_completed_buffer(&amp;satb_cl)) {
2416     abort_marking_if_regular_check_fail();
2417   }
2418 
<span class="line-modified">2419   // Can&#39;t assert qset is empty here, even if not aborted.  If concurrent,</span>
<span class="line-added">2420   // some other thread might be adding to the queue.  If not concurrent,</span>
<span class="line-added">2421   // some other thread might have won the race for the last buffer, but</span>
<span class="line-added">2422   // has not yet decremented the count.</span>
2423 
<span class="line-modified">2424   _draining_satb_buffers = false;</span>


2425 
2426   // again, this was a potentially expensive operation, decrease the
2427   // limits to get the regular clock call early
2428   decrease_limits();
2429 }
2430 
2431 void G1CMTask::clear_mark_stats_cache(uint region_idx) {
2432   _mark_stats_cache.reset(region_idx);
2433 }
2434 
2435 Pair&lt;size_t, size_t&gt; G1CMTask::flush_mark_stats_cache() {
2436   return _mark_stats_cache.evict_all();
2437 }
2438 
2439 void G1CMTask::print_stats() {
2440   log_debug(gc, stats)(&quot;Marking Stats, task = %u, calls = %u&quot;, _worker_id, _calls);
2441   log_debug(gc, stats)(&quot;  Elapsed time = %1.2lfms, Termination time = %1.2lfms&quot;,
2442                        _elapsed_time_ms, _termination_time_ms);
2443   log_debug(gc, stats)(&quot;  Step Times (cum): num = %d, avg = %1.2lfms, sd = %1.2lfms max = %1.2lfms, total = %1.2lfms&quot;,
2444                        _step_times_ms.num(),
</pre>
<hr />
<pre>
2567     The value of is_serial must be false when do_marking_step is
2568     being called by any of the worker threads in a work gang.
2569     Examples include the concurrent marking code (CMMarkingTask),
2570     the MT remark code, and the MT reference processing closures.
2571 
2572  *****************************************************************************/
2573 
2574 void G1CMTask::do_marking_step(double time_target_ms,
2575                                bool do_termination,
2576                                bool is_serial) {
2577   assert(time_target_ms &gt;= 1.0, &quot;minimum granularity is 1ms&quot;);
2578 
2579   _start_time_ms = os::elapsedVTime() * 1000.0;
2580 
2581   // If do_stealing is true then do_marking_step will attempt to
2582   // steal work from the other G1CMTasks. It only makes sense to
2583   // enable stealing when the termination protocol is enabled
2584   // and do_marking_step() is not being called serially.
2585   bool do_stealing = do_termination &amp;&amp; !is_serial;
2586 
<span class="line-modified">2587   G1Predictions const&amp; predictor = _g1h-&gt;policy()-&gt;predictor();</span>
<span class="line-added">2588   double diff_prediction_ms = predictor.predict_zero_bounded(&amp;_marking_step_diff_ms);</span>
2589   _time_target_ms = time_target_ms - diff_prediction_ms;
2590 
2591   // set up the variables that are used in the work-based scheme to
2592   // call the regular clock method
2593   _words_scanned = 0;
2594   _refs_reached  = 0;
2595   recalculate_limits();
2596 
2597   // clear all flags
2598   clear_has_aborted();
2599   _has_timed_out = false;
2600   _draining_satb_buffers = false;
2601 
2602   ++_calls;
2603 
2604   // Set up the bitmap and oop closures. Anything that uses them is
2605   // eventually called from this method, so it is OK to allocate these
2606   // statically.
2607   G1CMBitMapClosure bitmap_closure(this, _cm);
2608   G1CMOopClosure cm_oop_closure(_g1h, this);
</pre>
<hr />
<pre>
2810       set_has_aborted();
2811     }
2812   }
2813 
2814   // Mainly for debugging purposes to make sure that a pointer to the
2815   // closure which was statically allocated in this frame doesn&#39;t
2816   // escape it by accident.
2817   set_cm_oop_closure(NULL);
2818   double end_time_ms = os::elapsedVTime() * 1000.0;
2819   double elapsed_time_ms = end_time_ms - _start_time_ms;
2820   // Update the step history.
2821   _step_times_ms.add(elapsed_time_ms);
2822 
2823   if (has_aborted()) {
2824     // The task was aborted for some reason.
2825     if (_has_timed_out) {
2826       double diff_ms = elapsed_time_ms - _time_target_ms;
2827       // Keep statistics of how well we did with respect to hitting
2828       // our target only if we actually timed out (if we aborted for
2829       // other reasons, then the results might get skewed).
<span class="line-modified">2830       _marking_step_diff_ms.add(diff_ms);</span>
2831     }
2832 
2833     if (_cm-&gt;has_overflown()) {
2834       // This is the interesting one. We aborted because a global
2835       // overflow was raised. This means we have to restart the
2836       // marking phase and start iterating over regions. However, in
2837       // order to do this we have to make sure that all tasks stop
2838       // what they are doing and re-initialize in a safe manner. We
2839       // will achieve this with the use of two barrier sync points.
2840 
2841       if (!is_serial) {
2842         // We only need to enter the sync barrier if being called
2843         // from a parallel context
2844         _cm-&gt;enter_first_sync_barrier(_worker_id);
2845 
2846         // When we exit this sync barrier we know that all tasks have
2847         // stopped doing marking work. So, it&#39;s now safe to
2848         // re-initialize our data structures.
2849       }
2850 
</pre>
<hr />
<pre>
2893   _calls(0),
2894   _time_target_ms(0.0),
2895   _start_time_ms(0.0),
2896   _cm_oop_closure(NULL),
2897   _curr_region(NULL),
2898   _finger(NULL),
2899   _region_limit(NULL),
2900   _words_scanned(0),
2901   _words_scanned_limit(0),
2902   _real_words_scanned_limit(0),
2903   _refs_reached(0),
2904   _refs_reached_limit(0),
2905   _real_refs_reached_limit(0),
2906   _has_aborted(false),
2907   _has_timed_out(false),
2908   _draining_satb_buffers(false),
2909   _step_times_ms(),
2910   _elapsed_time_ms(0.0),
2911   _termination_time_ms(0.0),
2912   _termination_start_time_ms(0.0),
<span class="line-modified">2913   _marking_step_diff_ms()</span>
2914 {
2915   guarantee(task_queue != NULL, &quot;invariant&quot;);
2916 
<span class="line-modified">2917   _marking_step_diff_ms.add(0.5);</span>
2918 }
2919 
2920 // These are formatting macros that are used below to ensure
2921 // consistent formatting. The *_H_* versions are used to format the
2922 // header for a particular value and they should be kept consistent
2923 // with the corresponding macro. Also note that most of the macros add
2924 // the necessary white space (as a prefix) which makes them a bit
2925 // easier to compose.
2926 
2927 // All the output lines are prefixed with this string to be able to
2928 // identify them easily in a large log file.
2929 #define G1PPRL_LINE_PREFIX            &quot;###&quot;
2930 
2931 #define G1PPRL_ADDR_BASE_FORMAT    &quot; &quot; PTR_FORMAT &quot;-&quot; PTR_FORMAT
2932 #ifdef _LP64
2933 #define G1PPRL_ADDR_BASE_H_FORMAT  &quot; %37s&quot;
2934 #else // _LP64
2935 #define G1PPRL_ADDR_BASE_H_FORMAT  &quot; %21s&quot;
2936 #endif // _LP64
2937 
</pre>
</td>
</tr>
</table>
<center><a href="g1CollectionSetChooser.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1ConcurrentMark.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>