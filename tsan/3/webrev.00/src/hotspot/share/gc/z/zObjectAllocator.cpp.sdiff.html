<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/z/zObjectAllocator.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="zNUMA.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="zObjectAllocator.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/z/zObjectAllocator.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 #include &quot;precompiled.hpp&quot;
 25 #include &quot;gc/z/zCollectedHeap.hpp&quot;
 26 #include &quot;gc/z/zGlobals.hpp&quot;
 27 #include &quot;gc/z/zHeap.inline.hpp&quot;

 28 #include &quot;gc/z/zObjectAllocator.hpp&quot;
 29 #include &quot;gc/z/zPage.inline.hpp&quot;
 30 #include &quot;gc/z/zStat.hpp&quot;
<span class="line-modified"> 31 #include &quot;gc/z/zThread.hpp&quot;</span>
 32 #include &quot;gc/z/zUtils.inline.hpp&quot;

 33 #include &quot;logging/log.hpp&quot;
 34 #include &quot;runtime/atomic.hpp&quot;
 35 #include &quot;runtime/safepoint.hpp&quot;
 36 #include &quot;runtime/thread.hpp&quot;
 37 #include &quot;runtime/threadSMR.hpp&quot;
 38 #include &quot;utilities/align.hpp&quot;
 39 #include &quot;utilities/debug.hpp&quot;
 40 
 41 static const ZStatCounter ZCounterUndoObjectAllocationSucceeded(&quot;Memory&quot;, &quot;Undo Object Allocation Succeeded&quot;, ZStatUnitOpsPerSecond);
 42 static const ZStatCounter ZCounterUndoObjectAllocationFailed(&quot;Memory&quot;, &quot;Undo Object Allocation Failed&quot;, ZStatUnitOpsPerSecond);
 43 
<span class="line-modified"> 44 ZObjectAllocator::ZObjectAllocator(uint nworkers) :</span>
<span class="line-modified"> 45     _nworkers(nworkers),</span>
 46     _used(0),

 47     _shared_medium_page(NULL),
 48     _shared_small_page(NULL),
 49     _worker_small_page(NULL) {}
 50 








 51 ZPage* ZObjectAllocator::alloc_page(uint8_t type, size_t size, ZAllocationFlags flags) {
 52   ZPage* const page = ZHeap::heap()-&gt;alloc_page(type, size, flags);
 53   if (page != NULL) {
 54     // Increment used bytes
<span class="line-modified"> 55     Atomic::add(size, _used.addr());</span>
 56   }
 57 
 58   return page;
 59 }
 60 







 61 uintptr_t ZObjectAllocator::alloc_object_in_shared_page(ZPage** shared_page,
 62                                                         uint8_t page_type,
 63                                                         size_t page_size,
 64                                                         size_t size,
 65                                                         ZAllocationFlags flags) {
 66   uintptr_t addr = 0;
<span class="line-modified"> 67   ZPage* page = *shared_page;</span>
 68 
 69   if (page != NULL) {
 70     addr = page-&gt;alloc_object_atomic(size);
 71   }
 72 
 73   if (addr == 0) {
 74     // Allocate new page
 75     ZPage* const new_page = alloc_page(page_type, page_size, flags);
 76     if (new_page != NULL) {
 77       // Allocate object before installing the new page
 78       addr = new_page-&gt;alloc_object(size);
 79 
 80     retry:
 81       // Install new page
<span class="line-modified"> 82       ZPage* const prev_page = Atomic::cmpxchg(new_page, shared_page, page);</span>
 83       if (prev_page != page) {
 84         if (prev_page == NULL) {
 85           // Previous page was retired, retry installing the new page
 86           page = prev_page;
 87           goto retry;
 88         }
 89 
 90         // Another page already installed, try allocation there first
 91         const uintptr_t prev_addr = prev_page-&gt;alloc_object_atomic(size);
 92         if (prev_addr == 0) {
 93           // Allocation failed, retry installing the new page
 94           page = prev_page;
 95           goto retry;
 96         }
 97 
 98         // Allocation succeeded in already installed page
 99         addr = prev_addr;
100 
101         // Undo new page allocation
<span class="line-modified">102         ZHeap::heap()-&gt;undo_alloc_page(new_page);</span>
103       }
104     }
105   }
106 
107   return addr;
108 }
109 
110 uintptr_t ZObjectAllocator::alloc_large_object(size_t size, ZAllocationFlags flags) {
111   assert(ZThread::is_java(), &quot;Should be a Java thread&quot;);
112 
113   uintptr_t addr = 0;
114 
115   // Allocate new large page
116   const size_t page_size = align_up(size, ZGranuleSize);
117   ZPage* const page = alloc_page(ZPageTypeLarge, page_size, flags);
118   if (page != NULL) {
119     // Allocate the object
120     addr = page-&gt;alloc_object(size);
121   }
122 
123   return addr;
124 }
125 
126 uintptr_t ZObjectAllocator::alloc_medium_object(size_t size, ZAllocationFlags flags) {
127   return alloc_object_in_shared_page(_shared_medium_page.addr(), ZPageTypeMedium, ZPageSizeMedium, size, flags);
128 }
129 
130 uintptr_t ZObjectAllocator::alloc_small_object_from_nonworker(size_t size, ZAllocationFlags flags) {
131   assert(ZThread::is_java() || ZThread::is_vm() || ZThread::is_runtime_worker(),
132          &quot;Should be a Java, VM or Runtime worker thread&quot;);
133 
134   // Non-worker small page allocation can never use the reserve
135   flags.set_no_reserve();
136 
<span class="line-modified">137   return alloc_object_in_shared_page(_shared_small_page.addr(), ZPageTypeSmall, ZPageSizeSmall, size, flags);</span>
138 }
139 
140 uintptr_t ZObjectAllocator::alloc_small_object_from_worker(size_t size, ZAllocationFlags flags) {
141   assert(ZThread::is_worker(), &quot;Should be a worker thread&quot;);
142 
143   ZPage* page = _worker_small_page.get();
144   uintptr_t addr = 0;
145 
146   if (page != NULL) {
147     addr = page-&gt;alloc_object(size);
148   }
149 
150   if (addr == 0) {
151     // Allocate new page
152     page = alloc_page(ZPageTypeSmall, ZPageSizeSmall, flags);
153     if (page != NULL) {
154       addr = page-&gt;alloc_object(size);
155     }
156     _worker_small_page.set(page);
157   }
</pre>
<hr />
<pre>
169 
170 uintptr_t ZObjectAllocator::alloc_object(size_t size, ZAllocationFlags flags) {
171   if (size &lt;= ZObjectSizeLimitSmall) {
172     // Small
173     return alloc_small_object(size, flags);
174   } else if (size &lt;= ZObjectSizeLimitMedium) {
175     // Medium
176     return alloc_medium_object(size, flags);
177   } else {
178     // Large
179     return alloc_large_object(size, flags);
180   }
181 }
182 
183 uintptr_t ZObjectAllocator::alloc_object(size_t size) {
184   assert(ZThread::is_java(), &quot;Must be a Java thread&quot;);
185 
186   ZAllocationFlags flags;
187   flags.set_no_reserve();
188 
<span class="line-removed">189   if (!ZStallOnOutOfMemory) {</span>
<span class="line-removed">190     flags.set_non_blocking();</span>
<span class="line-removed">191   }</span>
<span class="line-removed">192 </span>
193   return alloc_object(size, flags);
194 }
195 
196 uintptr_t ZObjectAllocator::alloc_object_for_relocation(size_t size) {
197   assert(ZThread::is_java() || ZThread::is_vm() || ZThread::is_worker() || ZThread::is_runtime_worker(),
198          &quot;Unknown thread&quot;);
199 
200   ZAllocationFlags flags;
201   flags.set_relocation();
202   flags.set_non_blocking();
203 
204   if (ZThread::is_worker()) {
205     flags.set_worker_thread();
206   }
207 
208   return alloc_object(size, flags);
209 }
210 
211 bool ZObjectAllocator::undo_alloc_large_object(ZPage* page) {
212   assert(page-&gt;type() == ZPageTypeLarge, &quot;Invalid page type&quot;);
213 
214   // Undo page allocation
<span class="line-modified">215   ZHeap::heap()-&gt;undo_alloc_page(page);</span>
216   return true;
217 }
218 
219 bool ZObjectAllocator::undo_alloc_medium_object(ZPage* page, uintptr_t addr, size_t size) {
220   assert(page-&gt;type() == ZPageTypeMedium, &quot;Invalid page type&quot;);
221 
222   // Try atomic undo on shared page
223   return page-&gt;undo_alloc_object_atomic(addr, size);
224 }
225 
226 bool ZObjectAllocator::undo_alloc_small_object_from_nonworker(ZPage* page, uintptr_t addr, size_t size) {
227   assert(page-&gt;type() == ZPageTypeSmall, &quot;Invalid page type&quot;);
228 
229   // Try atomic undo on shared page
230   return page-&gt;undo_alloc_object_atomic(addr, size);
231 }
232 
233 bool ZObjectAllocator::undo_alloc_small_object_from_worker(ZPage* page, uintptr_t addr, size_t size) {
234   assert(page-&gt;type() == ZPageTypeSmall, &quot;Invalid page type&quot;);
235   assert(page == _worker_small_page.get(), &quot;Invalid page&quot;);
</pre>
<hr />
<pre>
255     return undo_alloc_small_object(page, addr, size);
256   } else if (type == ZPageTypeMedium) {
257     return undo_alloc_medium_object(page, addr, size);
258   } else {
259     return undo_alloc_large_object(page);
260   }
261 }
262 
263 void ZObjectAllocator::undo_alloc_object_for_relocation(ZPage* page, uintptr_t addr, size_t size) {
264   if (undo_alloc_object(page, addr, size)) {
265     ZStatInc(ZCounterUndoObjectAllocationSucceeded);
266   } else {
267     ZStatInc(ZCounterUndoObjectAllocationFailed);
268     log_trace(gc)(&quot;Failed to undo object allocation: &quot; PTR_FORMAT &quot;, Size: &quot; SIZE_FORMAT &quot;, Thread: &quot; PTR_FORMAT &quot; (%s)&quot;,
269                   addr, size, ZThread::id(), ZThread::name());
270   }
271 }
272 
273 size_t ZObjectAllocator::used() const {
274   size_t total_used = 0;

275 
<span class="line-modified">276   ZPerCPUConstIterator&lt;size_t&gt; iter(&amp;_used);</span>
<span class="line-modified">277   for (const size_t* cpu_used; iter.next(&amp;cpu_used);) {</span>
278     total_used += *cpu_used;
279   }
280 
<span class="line-modified">281   return total_used;</span>





282 }
283 
284 size_t ZObjectAllocator::remaining() const {
285   assert(ZThread::is_java(), &quot;Should be a Java thread&quot;);
286 
<span class="line-modified">287   ZPage* page = _shared_small_page.get();</span>
288   if (page != NULL) {
289     return page-&gt;remaining();
290   }
291 
292   return 0;
293 }
294 
295 void ZObjectAllocator::retire_pages() {
296   assert(SafepointSynchronize::is_at_safepoint(), &quot;Should be at safepoint&quot;);
297 
<span class="line-modified">298   // Reset used</span>
299   _used.set_all(0);

300 
301   // Reset allocation pages
302   _shared_medium_page.set(NULL);
303   _shared_small_page.set_all(NULL);
304   _worker_small_page.set_all(NULL);
305 }
</pre>
</td>
<td>
<hr />
<pre>
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 #include &quot;precompiled.hpp&quot;
 25 #include &quot;gc/z/zCollectedHeap.hpp&quot;
 26 #include &quot;gc/z/zGlobals.hpp&quot;
 27 #include &quot;gc/z/zHeap.inline.hpp&quot;
<span class="line-added"> 28 #include &quot;gc/z/zHeuristics.hpp&quot;</span>
 29 #include &quot;gc/z/zObjectAllocator.hpp&quot;
 30 #include &quot;gc/z/zPage.inline.hpp&quot;
 31 #include &quot;gc/z/zStat.hpp&quot;
<span class="line-modified"> 32 #include &quot;gc/z/zThread.inline.hpp&quot;</span>
 33 #include &quot;gc/z/zUtils.inline.hpp&quot;
<span class="line-added"> 34 #include &quot;gc/z/zValue.inline.hpp&quot;</span>
 35 #include &quot;logging/log.hpp&quot;
 36 #include &quot;runtime/atomic.hpp&quot;
 37 #include &quot;runtime/safepoint.hpp&quot;
 38 #include &quot;runtime/thread.hpp&quot;
 39 #include &quot;runtime/threadSMR.hpp&quot;
 40 #include &quot;utilities/align.hpp&quot;
 41 #include &quot;utilities/debug.hpp&quot;
 42 
 43 static const ZStatCounter ZCounterUndoObjectAllocationSucceeded(&quot;Memory&quot;, &quot;Undo Object Allocation Succeeded&quot;, ZStatUnitOpsPerSecond);
 44 static const ZStatCounter ZCounterUndoObjectAllocationFailed(&quot;Memory&quot;, &quot;Undo Object Allocation Failed&quot;, ZStatUnitOpsPerSecond);
 45 
<span class="line-modified"> 46 ZObjectAllocator::ZObjectAllocator() :</span>
<span class="line-modified"> 47     _use_per_cpu_shared_small_pages(ZHeuristics::use_per_cpu_shared_small_pages()),</span>
 48     _used(0),
<span class="line-added"> 49     _undone(0),</span>
 50     _shared_medium_page(NULL),
 51     _shared_small_page(NULL),
 52     _worker_small_page(NULL) {}
 53 
<span class="line-added"> 54 ZPage** ZObjectAllocator::shared_small_page_addr() {</span>
<span class="line-added"> 55   return _use_per_cpu_shared_small_pages ? _shared_small_page.addr() : _shared_small_page.addr(0);</span>
<span class="line-added"> 56 }</span>
<span class="line-added"> 57 </span>
<span class="line-added"> 58 ZPage* const* ZObjectAllocator::shared_small_page_addr() const {</span>
<span class="line-added"> 59   return _use_per_cpu_shared_small_pages ? _shared_small_page.addr() : _shared_small_page.addr(0);</span>
<span class="line-added"> 60 }</span>
<span class="line-added"> 61 </span>
 62 ZPage* ZObjectAllocator::alloc_page(uint8_t type, size_t size, ZAllocationFlags flags) {
 63   ZPage* const page = ZHeap::heap()-&gt;alloc_page(type, size, flags);
 64   if (page != NULL) {
 65     // Increment used bytes
<span class="line-modified"> 66     Atomic::add(_used.addr(), size);</span>
 67   }
 68 
 69   return page;
 70 }
 71 
<span class="line-added"> 72 void ZObjectAllocator::undo_alloc_page(ZPage* page) {</span>
<span class="line-added"> 73   // Increment undone bytes</span>
<span class="line-added"> 74   Atomic::add(_undone.addr(), page-&gt;size());</span>
<span class="line-added"> 75 </span>
<span class="line-added"> 76   ZHeap::heap()-&gt;undo_alloc_page(page);</span>
<span class="line-added"> 77 }</span>
<span class="line-added"> 78 </span>
 79 uintptr_t ZObjectAllocator::alloc_object_in_shared_page(ZPage** shared_page,
 80                                                         uint8_t page_type,
 81                                                         size_t page_size,
 82                                                         size_t size,
 83                                                         ZAllocationFlags flags) {
 84   uintptr_t addr = 0;
<span class="line-modified"> 85   ZPage* page = Atomic::load_acquire(shared_page);</span>
 86 
 87   if (page != NULL) {
 88     addr = page-&gt;alloc_object_atomic(size);
 89   }
 90 
 91   if (addr == 0) {
 92     // Allocate new page
 93     ZPage* const new_page = alloc_page(page_type, page_size, flags);
 94     if (new_page != NULL) {
 95       // Allocate object before installing the new page
 96       addr = new_page-&gt;alloc_object(size);
 97 
 98     retry:
 99       // Install new page
<span class="line-modified">100       ZPage* const prev_page = Atomic::cmpxchg(shared_page, page, new_page);</span>
101       if (prev_page != page) {
102         if (prev_page == NULL) {
103           // Previous page was retired, retry installing the new page
104           page = prev_page;
105           goto retry;
106         }
107 
108         // Another page already installed, try allocation there first
109         const uintptr_t prev_addr = prev_page-&gt;alloc_object_atomic(size);
110         if (prev_addr == 0) {
111           // Allocation failed, retry installing the new page
112           page = prev_page;
113           goto retry;
114         }
115 
116         // Allocation succeeded in already installed page
117         addr = prev_addr;
118 
119         // Undo new page allocation
<span class="line-modified">120         undo_alloc_page(new_page);</span>
121       }
122     }
123   }
124 
125   return addr;
126 }
127 
128 uintptr_t ZObjectAllocator::alloc_large_object(size_t size, ZAllocationFlags flags) {
129   assert(ZThread::is_java(), &quot;Should be a Java thread&quot;);
130 
131   uintptr_t addr = 0;
132 
133   // Allocate new large page
134   const size_t page_size = align_up(size, ZGranuleSize);
135   ZPage* const page = alloc_page(ZPageTypeLarge, page_size, flags);
136   if (page != NULL) {
137     // Allocate the object
138     addr = page-&gt;alloc_object(size);
139   }
140 
141   return addr;
142 }
143 
144 uintptr_t ZObjectAllocator::alloc_medium_object(size_t size, ZAllocationFlags flags) {
145   return alloc_object_in_shared_page(_shared_medium_page.addr(), ZPageTypeMedium, ZPageSizeMedium, size, flags);
146 }
147 
148 uintptr_t ZObjectAllocator::alloc_small_object_from_nonworker(size_t size, ZAllocationFlags flags) {
149   assert(ZThread::is_java() || ZThread::is_vm() || ZThread::is_runtime_worker(),
150          &quot;Should be a Java, VM or Runtime worker thread&quot;);
151 
152   // Non-worker small page allocation can never use the reserve
153   flags.set_no_reserve();
154 
<span class="line-modified">155   return alloc_object_in_shared_page(shared_small_page_addr(), ZPageTypeSmall, ZPageSizeSmall, size, flags);</span>
156 }
157 
158 uintptr_t ZObjectAllocator::alloc_small_object_from_worker(size_t size, ZAllocationFlags flags) {
159   assert(ZThread::is_worker(), &quot;Should be a worker thread&quot;);
160 
161   ZPage* page = _worker_small_page.get();
162   uintptr_t addr = 0;
163 
164   if (page != NULL) {
165     addr = page-&gt;alloc_object(size);
166   }
167 
168   if (addr == 0) {
169     // Allocate new page
170     page = alloc_page(ZPageTypeSmall, ZPageSizeSmall, flags);
171     if (page != NULL) {
172       addr = page-&gt;alloc_object(size);
173     }
174     _worker_small_page.set(page);
175   }
</pre>
<hr />
<pre>
187 
188 uintptr_t ZObjectAllocator::alloc_object(size_t size, ZAllocationFlags flags) {
189   if (size &lt;= ZObjectSizeLimitSmall) {
190     // Small
191     return alloc_small_object(size, flags);
192   } else if (size &lt;= ZObjectSizeLimitMedium) {
193     // Medium
194     return alloc_medium_object(size, flags);
195   } else {
196     // Large
197     return alloc_large_object(size, flags);
198   }
199 }
200 
201 uintptr_t ZObjectAllocator::alloc_object(size_t size) {
202   assert(ZThread::is_java(), &quot;Must be a Java thread&quot;);
203 
204   ZAllocationFlags flags;
205   flags.set_no_reserve();
206 




207   return alloc_object(size, flags);
208 }
209 
210 uintptr_t ZObjectAllocator::alloc_object_for_relocation(size_t size) {
211   assert(ZThread::is_java() || ZThread::is_vm() || ZThread::is_worker() || ZThread::is_runtime_worker(),
212          &quot;Unknown thread&quot;);
213 
214   ZAllocationFlags flags;
215   flags.set_relocation();
216   flags.set_non_blocking();
217 
218   if (ZThread::is_worker()) {
219     flags.set_worker_thread();
220   }
221 
222   return alloc_object(size, flags);
223 }
224 
225 bool ZObjectAllocator::undo_alloc_large_object(ZPage* page) {
226   assert(page-&gt;type() == ZPageTypeLarge, &quot;Invalid page type&quot;);
227 
228   // Undo page allocation
<span class="line-modified">229   undo_alloc_page(page);</span>
230   return true;
231 }
232 
233 bool ZObjectAllocator::undo_alloc_medium_object(ZPage* page, uintptr_t addr, size_t size) {
234   assert(page-&gt;type() == ZPageTypeMedium, &quot;Invalid page type&quot;);
235 
236   // Try atomic undo on shared page
237   return page-&gt;undo_alloc_object_atomic(addr, size);
238 }
239 
240 bool ZObjectAllocator::undo_alloc_small_object_from_nonworker(ZPage* page, uintptr_t addr, size_t size) {
241   assert(page-&gt;type() == ZPageTypeSmall, &quot;Invalid page type&quot;);
242 
243   // Try atomic undo on shared page
244   return page-&gt;undo_alloc_object_atomic(addr, size);
245 }
246 
247 bool ZObjectAllocator::undo_alloc_small_object_from_worker(ZPage* page, uintptr_t addr, size_t size) {
248   assert(page-&gt;type() == ZPageTypeSmall, &quot;Invalid page type&quot;);
249   assert(page == _worker_small_page.get(), &quot;Invalid page&quot;);
</pre>
<hr />
<pre>
269     return undo_alloc_small_object(page, addr, size);
270   } else if (type == ZPageTypeMedium) {
271     return undo_alloc_medium_object(page, addr, size);
272   } else {
273     return undo_alloc_large_object(page);
274   }
275 }
276 
277 void ZObjectAllocator::undo_alloc_object_for_relocation(ZPage* page, uintptr_t addr, size_t size) {
278   if (undo_alloc_object(page, addr, size)) {
279     ZStatInc(ZCounterUndoObjectAllocationSucceeded);
280   } else {
281     ZStatInc(ZCounterUndoObjectAllocationFailed);
282     log_trace(gc)(&quot;Failed to undo object allocation: &quot; PTR_FORMAT &quot;, Size: &quot; SIZE_FORMAT &quot;, Thread: &quot; PTR_FORMAT &quot; (%s)&quot;,
283                   addr, size, ZThread::id(), ZThread::name());
284   }
285 }
286 
287 size_t ZObjectAllocator::used() const {
288   size_t total_used = 0;
<span class="line-added">289   size_t total_undone = 0;</span>
290 
<span class="line-modified">291   ZPerCPUConstIterator&lt;size_t&gt; iter_used(&amp;_used);</span>
<span class="line-modified">292   for (const size_t* cpu_used; iter_used.next(&amp;cpu_used);) {</span>
293     total_used += *cpu_used;
294   }
295 
<span class="line-modified">296   ZPerCPUConstIterator&lt;size_t&gt; iter_undone(&amp;_undone);</span>
<span class="line-added">297   for (const size_t* cpu_undone; iter_undone.next(&amp;cpu_undone);) {</span>
<span class="line-added">298     total_undone += *cpu_undone;</span>
<span class="line-added">299   }</span>
<span class="line-added">300 </span>
<span class="line-added">301   return total_used - total_undone;</span>
302 }
303 
304 size_t ZObjectAllocator::remaining() const {
305   assert(ZThread::is_java(), &quot;Should be a Java thread&quot;);
306 
<span class="line-modified">307   const ZPage* const page = Atomic::load_acquire(shared_small_page_addr());</span>
308   if (page != NULL) {
309     return page-&gt;remaining();
310   }
311 
312   return 0;
313 }
314 
315 void ZObjectAllocator::retire_pages() {
316   assert(SafepointSynchronize::is_at_safepoint(), &quot;Should be at safepoint&quot;);
317 
<span class="line-modified">318   // Reset used and undone bytes</span>
319   _used.set_all(0);
<span class="line-added">320   _undone.set_all(0);</span>
321 
322   // Reset allocation pages
323   _shared_medium_page.set(NULL);
324   _shared_small_page.set_all(NULL);
325   _worker_small_page.set_all(NULL);
326 }
</pre>
</td>
</tr>
</table>
<center><a href="zNUMA.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="zObjectAllocator.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>