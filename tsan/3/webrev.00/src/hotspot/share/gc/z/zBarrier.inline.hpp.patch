diff a/src/hotspot/share/gc/z/zBarrier.inline.hpp b/src/hotspot/share/gc/z/zBarrier.inline.hpp
--- a/src/hotspot/share/gc/z/zBarrier.inline.hpp
+++ b/src/hotspot/share/gc/z/zBarrier.inline.hpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 2015, 2017, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2015, 2019, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -22,82 +22,162 @@
  */
 
 #ifndef SHARE_GC_Z_ZBARRIER_INLINE_HPP
 #define SHARE_GC_Z_ZBARRIER_INLINE_HPP
 
+#include "classfile/javaClasses.hpp"
 #include "gc/z/zAddress.inline.hpp"
 #include "gc/z/zBarrier.hpp"
 #include "gc/z/zOop.inline.hpp"
 #include "gc/z/zResurrection.inline.hpp"
+#include "oops/oop.hpp"
 #include "runtime/atomic.hpp"
 
+// A self heal must always "upgrade" the address metadata bits in
+// accordance with the metadata bits state machine, which has the
+// valid state transitions as described below (where N is the GC
+// cycle).
+//
+// Note the subtleness of overlapping GC cycles. Specifically that
+// oops are colored Remapped(N) starting at relocation N and ending
+// at marking N + 1.
+//
+//              +--- Mark Start
+//              | +--- Mark End
+//              | | +--- Relocate Start
+//              | | | +--- Relocate End
+//              | | | |
+// Marked       |---N---|--N+1--|--N+2--|----
+// Finalizable  |---N---|--N+1--|--N+2--|----
+// Remapped     ----|---N---|--N+1--|--N+2--|
+//
+// VALID STATE TRANSITIONS
+//
+//   Marked(N)           -> Remapped(N)
+//                       -> Marked(N + 1)
+//                       -> Finalizable(N + 1)
+//
+//   Finalizable(N)      -> Marked(N)
+//                       -> Remapped(N)
+//                       -> Marked(N + 1)
+//                       -> Finalizable(N + 1)
+//
+//   Remapped(N)         -> Marked(N + 1)
+//                       -> Finalizable(N + 1)
+//
+// PHASE VIEW
+//
+// ZPhaseMark
+//   Load & Mark
+//     Marked(N)         <- Marked(N - 1)
+//                       <- Finalizable(N - 1)
+//                       <- Remapped(N - 1)
+//                       <- Finalizable(N)
+//
+//   Mark(Finalizable)
+//     Finalizable(N)    <- Marked(N - 1)
+//                       <- Finalizable(N - 1)
+//                       <- Remapped(N - 1)
+//
+//   Load(AS_NO_KEEPALIVE)
+//     Remapped(N - 1)   <- Marked(N - 1)
+//                       <- Finalizable(N - 1)
+//
+// ZPhaseMarkCompleted (Resurrection blocked)
+//   Load & Load(ON_WEAK/PHANTOM_OOP_REF | AS_NO_KEEPALIVE) & KeepAlive
+//     Marked(N)         <- Marked(N - 1)
+//                       <- Finalizable(N - 1)
+//                       <- Remapped(N - 1)
+//                       <- Finalizable(N)
+//
+//   Load(ON_STRONG_OOP_REF | AS_NO_KEEPALIVE)
+//     Remapped(N - 1)   <- Marked(N - 1)
+//                       <- Finalizable(N - 1)
+//
+// ZPhaseMarkCompleted (Resurrection unblocked)
+//   Load
+//     Marked(N)         <- Finalizable(N)
+//
+// ZPhaseRelocate
+//   Load & Load(AS_NO_KEEPALIVE)
+//     Remapped(N)       <- Marked(N)
+//                       <- Finalizable(N)
+
+template <ZBarrierFastPath fast_path>
+inline void ZBarrier::self_heal(volatile oop* p, uintptr_t addr, uintptr_t heal_addr) {
+  if (heal_addr == 0) {
+    // Never heal with null since it interacts badly with reference processing.
+    // A mutator clearing an oop would be similar to calling Reference.clear(),
+    // which would make the reference non-discoverable or silently dropped
+    // by the reference processor.
+    return;
+  }
+
+  assert(!fast_path(addr), "Invalid self heal");
+  assert(fast_path(heal_addr), "Invalid self heal");
+
+  for (;;) {
+    // Heal
+    const uintptr_t prev_addr = Atomic::cmpxchg((volatile uintptr_t*)p, addr, heal_addr);
+    if (prev_addr == addr) {
+      // Success
+      return;
+    }
+
+    if (fast_path(prev_addr)) {
+      // Must not self heal
+      return;
+    }
+
+    // The oop location was healed by another barrier, but still needs upgrading.
+    // Re-apply healing to make sure the oop is not left with weaker (remapped or
+    // finalizable) metadata bits than what this barrier tried to apply.
+    assert(ZAddress::offset(prev_addr) == ZAddress::offset(heal_addr), "Invalid offset");
+    addr = prev_addr;
+  }
+}
+
 template <ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path>
 inline oop ZBarrier::barrier(volatile oop* p, oop o) {
-  uintptr_t addr = ZOop::to_address(o);
+  const uintptr_t addr = ZOop::to_address(o);
 
-retry:
   // Fast path
   if (fast_path(addr)) {
-    return ZOop::to_oop(addr);
+    return ZOop::from_address(addr);
   }
 
   // Slow path
   const uintptr_t good_addr = slow_path(addr);
 
-  // Self heal, but only if the address was actually updated by the slow path,
-  // which might not be the case, e.g. when marking through an already good oop.
-  if (p != NULL && good_addr != addr) {
-    const uintptr_t prev_addr = Atomic::cmpxchg(good_addr, (volatile uintptr_t*)p, addr);
-    if (prev_addr != addr) {
-      // Some other thread overwrote the oop. If this oop was updated by a
-      // weak barrier the new oop might not be good, in which case we need
-      // to re-apply this barrier.
-      addr = prev_addr;
-      goto retry;
-    }
+  if (p != NULL) {
+    self_heal<fast_path>(p, addr, good_addr);
   }
 
-  return ZOop::to_oop(good_addr);
+  return ZOop::from_address(good_addr);
 }
 
 template <ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path>
 inline oop ZBarrier::weak_barrier(volatile oop* p, oop o) {
   const uintptr_t addr = ZOop::to_address(o);
 
   // Fast path
   if (fast_path(addr)) {
     // Return the good address instead of the weak good address
     // to ensure that the currently active heap view is used.
-    return ZOop::to_oop(ZAddress::good_or_null(addr));
+    return ZOop::from_address(ZAddress::good_or_null(addr));
   }
 
   // Slow path
-  uintptr_t good_addr = slow_path(addr);
-
-  // Self heal unless the address returned from the slow path is null,
-  // in which case resurrection was blocked and we must let the reference
-  // processor clear the oop. Mutators are not allowed to clear oops in
-  // these cases, since that would be similar to calling Reference.clear(),
-  // which would make the reference non-discoverable or silently dropped
-  // by the reference processor.
-  if (p != NULL && good_addr != 0) {
-    // The slow path returns a good/marked address, but we never mark oops
-    // in a weak load barrier so we always self heal with the remapped address.
-    const uintptr_t weak_good_addr = ZAddress::remapped(good_addr);
-    const uintptr_t prev_addr = Atomic::cmpxchg(weak_good_addr, (volatile uintptr_t*)p, addr);
-    if (prev_addr != addr) {
-      // Some other thread overwrote the oop. The new
-      // oop is guaranteed to be weak good or null.
-      assert(ZAddress::is_weak_good_or_null(prev_addr), "Bad weak overwrite");
-
-      // Return the good address instead of the weak good address
-      // to ensure that the currently active heap view is used.
-      good_addr = ZAddress::good_or_null(prev_addr);
-    }
+  const uintptr_t good_addr = slow_path(addr);
+
+  if (p != NULL) {
+    // The slow path returns a good/marked address or null, but we never mark
+    // oops in a weak load barrier so we always heal with the remapped address.
+    self_heal<fast_path>(p, addr, ZAddress::remapped_or_null(good_addr));
   }
 
-  return ZOop::to_oop(good_addr);
+  return ZOop::from_address(good_addr);
 }
 
 template <ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path>
 inline void ZBarrier::root_barrier(oop* p, oop o) {
   const uintptr_t addr = ZOop::to_address(o);
@@ -115,42 +195,31 @@
   // which ensures we are never racing with mutators modifying roots while
   // we are healing them. It's also safe in case multiple GC threads try
   // to heal the same root if it is aligned, since they would always heal
   // the root in the same way and it does not matter in which order it
   // happens. For misaligned oops, there needs to be mutual exclusion.
-  *p = ZOop::to_oop(good_addr);
-}
-
-inline bool ZBarrier::is_null_fast_path(uintptr_t addr) {
-  return ZAddress::is_null(addr);
+  *p = ZOop::from_address(good_addr);
 }
 
 inline bool ZBarrier::is_good_or_null_fast_path(uintptr_t addr) {
   return ZAddress::is_good_or_null(addr);
 }
 
 inline bool ZBarrier::is_weak_good_or_null_fast_path(uintptr_t addr) {
   return ZAddress::is_weak_good_or_null(addr);
 }
 
-inline bool ZBarrier::is_resurrection_blocked(volatile oop* p, oop* o) {
-  const bool is_blocked = ZResurrection::is_blocked();
+inline bool ZBarrier::is_marked_or_null_fast_path(uintptr_t addr) {
+  return ZAddress::is_marked_or_null(addr);
+}
 
-  // Reload oop after checking the resurrection blocked state. This is
-  // done to prevent a race where we first load an oop, which is logically
-  // null but not yet cleared, then this oop is cleared by the reference
-  // processor and resurrection is unblocked. At this point the mutator
-  // would see the unblocked state and pass this invalid oop through the
-  // normal barrier path, which would incorrectly try to mark this oop.
-  if (p != NULL) {
-    // First assign to reloaded_o to avoid compiler warning about
-    // implicit dereference of volatile oop.
-    const oop reloaded_o = *p;
-    *o = reloaded_o;
-  }
+inline bool ZBarrier::during_mark() {
+  return ZGlobalPhase == ZPhaseMark;
+}
 
-  return is_blocked;
+inline bool ZBarrier::during_relocate() {
+  return ZGlobalPhase == ZPhaseRelocate;
 }
 
 //
 // Load barrier
 //
@@ -171,21 +240,35 @@
   for (volatile const oop* const end = p + length; p < end; p++) {
     load_barrier_on_oop_field(p);
   }
 }
 
+// ON_WEAK barriers should only ever be applied to j.l.r.Reference.referents.
+inline void verify_on_weak(volatile oop* referent_addr) {
+#ifdef ASSERT
+  if (referent_addr != NULL) {
+    uintptr_t base = (uintptr_t)referent_addr - java_lang_ref_Reference::referent_offset;
+    oop obj = cast_to_oop(base);
+    assert(oopDesc::is_oop(obj), "Verification failed for: ref " PTR_FORMAT " obj: " PTR_FORMAT, (uintptr_t)referent_addr, base);
+    assert(java_lang_ref_Reference::is_referent_field(obj, java_lang_ref_Reference::referent_offset), "Sanity");
+  }
+#endif
+}
+
 inline oop ZBarrier::load_barrier_on_weak_oop_field_preloaded(volatile oop* p, oop o) {
-  if (is_resurrection_blocked(p, &o)) {
-    return weak_barrier<is_good_or_null_fast_path, weak_load_barrier_on_weak_oop_slow_path>(p, o);
+  verify_on_weak(p);
+
+  if (ZResurrection::is_blocked()) {
+    return barrier<is_good_or_null_fast_path, weak_load_barrier_on_weak_oop_slow_path>(p, o);
   }
 
   return load_barrier_on_oop_field_preloaded(p, o);
 }
 
 inline oop ZBarrier::load_barrier_on_phantom_oop_field_preloaded(volatile oop* p, oop o) {
-  if (is_resurrection_blocked(p, &o)) {
-    return weak_barrier<is_good_or_null_fast_path, weak_load_barrier_on_phantom_oop_slow_path>(p, o);
+  if (ZResurrection::is_blocked()) {
+    return barrier<is_good_or_null_fast_path, weak_load_barrier_on_phantom_oop_slow_path>(p, o);
   }
 
   return load_barrier_on_oop_field_preloaded(p, o);
 }
 
@@ -215,12 +298,14 @@
   const oop o = *p;
   return weak_load_barrier_on_weak_oop_field_preloaded(p, o);
 }
 
 inline oop ZBarrier::weak_load_barrier_on_weak_oop_field_preloaded(volatile oop* p, oop o) {
-  if (is_resurrection_blocked(p, &o)) {
-    return weak_barrier<is_good_or_null_fast_path, weak_load_barrier_on_weak_oop_slow_path>(p, o);
+  verify_on_weak(p);
+
+  if (ZResurrection::is_blocked()) {
+    return barrier<is_good_or_null_fast_path, weak_load_barrier_on_weak_oop_slow_path>(p, o);
   }
 
   return weak_load_barrier_on_oop_field_preloaded(p, o);
 }
 
@@ -232,12 +317,12 @@
   const oop o = *p;
   return weak_load_barrier_on_phantom_oop_field_preloaded(p, o);
 }
 
 inline oop ZBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(volatile oop* p, oop o) {
-  if (is_resurrection_blocked(p, &o)) {
-    return weak_barrier<is_good_or_null_fast_path, weak_load_barrier_on_phantom_oop_slow_path>(p, o);
+  if (ZResurrection::is_blocked()) {
+    return barrier<is_good_or_null_fast_path, weak_load_barrier_on_phantom_oop_slow_path>(p, o);
   }
 
   return weak_load_barrier_on_oop_field_preloaded(p, o);
 }
 
@@ -280,22 +365,36 @@
   assert(ZResurrection::is_blocked(), "Invalid phase");
   const oop o = *p;
   root_barrier<is_good_or_null_fast_path, keep_alive_barrier_on_phantom_oop_slow_path>(p, o);
 }
 
+inline void ZBarrier::keep_alive_barrier_on_oop(oop o) {
+  const uintptr_t addr = ZOop::to_address(o);
+  assert(ZAddress::is_good(addr), "Invalid address");
+
+  if (during_mark()) {
+    mark_barrier_on_oop_slow_path(addr);
+  }
+}
+
 //
 // Mark barrier
 //
 inline void ZBarrier::mark_barrier_on_oop_field(volatile oop* p, bool finalizable) {
-  // The fast path only checks for null since the GC worker
-  // threads doing marking wants to mark through good oops.
   const oop o = *p;
 
   if (finalizable) {
-    barrier<is_null_fast_path, mark_barrier_on_finalizable_oop_slow_path>(p, o);
+    barrier<is_marked_or_null_fast_path, mark_barrier_on_finalizable_oop_slow_path>(p, o);
   } else {
-    barrier<is_null_fast_path, mark_barrier_on_oop_slow_path>(p, o);
+    const uintptr_t addr = ZOop::to_address(o);
+    if (ZAddress::is_good(addr)) {
+      // Mark through good oop
+      mark_barrier_on_oop_slow_path(addr);
+    } else {
+      // Mark through bad oop
+      barrier<is_good_or_null_fast_path, mark_barrier_on_oop_slow_path>(p, o);
+    }
   }
 }
 
 inline void ZBarrier::mark_barrier_on_oop_array(volatile oop* p, size_t length, bool finalizable) {
   for (volatile const oop* const end = p + length; p < end; p++) {
@@ -306,10 +405,15 @@
 inline void ZBarrier::mark_barrier_on_root_oop_field(oop* p) {
   const oop o = *p;
   root_barrier<is_good_or_null_fast_path, mark_barrier_on_root_oop_slow_path>(p, o);
 }
 
+inline void ZBarrier::mark_barrier_on_invisible_root_oop_field(oop* p) {
+  const oop o = *p;
+  root_barrier<is_good_or_null_fast_path, mark_barrier_on_invisible_root_oop_slow_path>(p, o);
+}
+
 //
 // Relocate barrier
 //
 inline void ZBarrier::relocate_barrier_on_root_oop_field(oop* p) {
   const oop o = *p;
