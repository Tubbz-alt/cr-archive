diff a/src/hotspot/share/gc/z/zMarkStack.inline.hpp b/src/hotspot/share/gc/z/zMarkStack.inline.hpp
--- a/src/hotspot/share/gc/z/zMarkStack.inline.hpp
+++ b/src/hotspot/share/gc/z/zMarkStack.inline.hpp
@@ -112,18 +112,18 @@
   decode_versioned_pointer(vstack, &stack, &version);
   return stack == NULL;
 }
 
 template <typename T>
-inline void ZStackList<T>::push_atomic(T* stack) {
+inline void ZStackList<T>::push(T* stack) {
   T* vstack = _head;
   uint32_t version = 0;
 
   for (;;) {
     decode_versioned_pointer(vstack, stack->next_addr(), &version);
     T* const new_vstack = encode_versioned_pointer(stack, version + 1);
-    T* const prev_vstack = Atomic::cmpxchg(new_vstack, &_head, vstack);
+    T* const prev_vstack = Atomic::cmpxchg(&_head, vstack, new_vstack);
     if (prev_vstack == vstack) {
       // Success
       break;
     }
 
@@ -131,11 +131,11 @@
     vstack = prev_vstack;
   }
 }
 
 template <typename T>
-inline T* ZStackList<T>::pop_atomic() {
+inline T* ZStackList<T>::pop() {
   T* vstack = _head;
   T* stack = NULL;
   uint32_t version = 0;
 
   for (;;) {
@@ -143,11 +143,11 @@
     if (stack == NULL) {
       return NULL;
     }
 
     T* const new_vstack = encode_versioned_pointer(stack->next(), version + 1);
-    T* const prev_vstack = Atomic::cmpxchg(new_vstack, &_head, vstack);
+    T* const prev_vstack = Atomic::cmpxchg(&_head, vstack, new_vstack);
     if (prev_vstack == vstack) {
       // Success
       return stack;
     }
 
@@ -166,24 +166,24 @@
   // workers to work on, while the overflowed list is used by GC workers
   // to publish stacks that overflowed. The intention here is to avoid
   // contention between mutators and GC workers as much as possible, while
   // still allowing GC workers to help out and steal work from each other.
   if (publish) {
-    _published.push_atomic(stack);
+    _published.push(stack);
   } else {
-    _overflowed.push_atomic(stack);
+    _overflowed.push(stack);
   }
 }
 
 inline ZMarkStack* ZMarkStripe::steal_stack() {
   // Steal overflowed stacks first, then published stacks
-  ZMarkStack* const stack = _overflowed.pop_atomic();
+  ZMarkStack* const stack = _overflowed.pop();
   if (stack != NULL) {
     return stack;
   }
 
-  return _published.pop_atomic();
+  return _published.pop();
 }
 
 inline size_t ZMarkStripeSet::nstripes() const {
   return _nstripes;
 }
