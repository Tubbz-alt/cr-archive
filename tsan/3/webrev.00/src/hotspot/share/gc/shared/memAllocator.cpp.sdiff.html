<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/shared/memAllocator.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="markBitMap.inline.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="memAllocator.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/shared/memAllocator.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2018, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
</pre>
<hr />
<pre>
126     report_java_out_of_memory(message);
127 
128     if (JvmtiExport::should_post_resource_exhausted()) {
129       JvmtiExport::post_resource_exhausted(
130         JVMTI_RESOURCE_EXHAUSTED_OOM_ERROR | JVMTI_RESOURCE_EXHAUSTED_JAVA_HEAP,
131         message);
132     }
133     oop exception = _overhead_limit_exceeded ?
134         Universe::out_of_memory_error_gc_overhead_limit() :
135         Universe::out_of_memory_error_java_heap();
136     THROW_OOP_(exception, true);
137   } else {
138     THROW_OOP_(Universe::out_of_memory_error_retry(), true);
139   }
140 }
141 
142 void MemAllocator::Allocation::verify_before() {
143   // Clear unhandled oops for memory allocation.  Memory allocation might
144   // not take out a lock if from tlab, so clear here.
145   Thread* THREAD = _thread;
<span class="line-removed">146   CHECK_UNHANDLED_OOPS_ONLY(THREAD-&gt;clear_unhandled_oops();)</span>
147   assert(!HAS_PENDING_EXCEPTION, &quot;Should not allocate with exception pending&quot;);
148   debug_only(check_for_valid_allocation_state());
149   assert(!Universe::heap()-&gt;is_gc_active(), &quot;Allocation during gc not allowed&quot;);
150 }
151 
152 void MemAllocator::Allocation::verify_after() {
153   NOT_PRODUCT(check_for_bad_heap_word_value();)
154 }
155 
156 void MemAllocator::Allocation::check_for_bad_heap_word_value() const {
157   MemRegion obj_range = _allocator.obj_memory_range(obj());
158   HeapWord* addr = obj_range.start();
159   size_t size = obj_range.word_size();
160   if (CheckMemoryInitialization &amp;&amp; ZapUnusedHeapArea) {
161     for (size_t slot = 0; slot &lt; size; slot += 1) {
162       assert((*(intptr_t*) (addr + slot)) != ((intptr_t) badHeapWordVal),
163              &quot;Found badHeapWordValue in post-allocation check&quot;);
164     }
165   }
166 }
167 
168 #ifdef ASSERT
169 void MemAllocator::Allocation::check_for_valid_allocation_state() const {
170   // How to choose between a pending exception and a potential
171   // OutOfMemoryError?  Don&#39;t allow pending exceptions.
172   // This is a VM policy failure, so how do we exhaustively test it?
173   assert(!_thread-&gt;has_pending_exception(),
174          &quot;shouldn&#39;t be allocating with pending exception&quot;);
<span class="line-modified">175   if (StrictSafepointChecks) {</span>
<span class="line-modified">176     assert(_thread-&gt;allow_allocation(),</span>
<span class="line-removed">177            &quot;Allocation done by thread for which allocation is blocked &quot;</span>
<span class="line-removed">178            &quot;by No_Allocation_Verifier!&quot;);</span>
<span class="line-removed">179     // Allocation of an oop can always invoke a safepoint,</span>
<span class="line-removed">180     // hence, the true argument</span>
<span class="line-removed">181     _thread-&gt;check_for_valid_safepoint_state(true);</span>
<span class="line-removed">182   }</span>
183 }
184 #endif
185 
186 void MemAllocator::Allocation::notify_allocation_jvmti_sampler() {
187   // support for JVMTI VMObjectAlloc event (no-op if not enabled)
188   JvmtiExport::vm_object_alloc_event_collector(obj());
189 
190   if (!JvmtiExport::should_post_sampled_object_alloc()) {
191     // Sampling disabled
192     return;
193   }
194 
195   if (!_allocated_outside_tlab &amp;&amp; _allocated_tlab_size == 0 &amp;&amp; !_tlab_end_reset_for_sample) {
196     // Sample if it&#39;s a non-TLAB allocation, or a TLAB allocation that either refills the TLAB
197     // or expands it due to taking a sampler induced slow path.
198     return;
199   }
200 
<span class="line-modified">201   if (JvmtiExport::should_post_sampled_object_alloc()) {</span>
<span class="line-modified">202     // If we want to be sampling, protect the allocated object with a Handle</span>
<span class="line-modified">203     // before doing the callback. The callback is done in the destructor of</span>
<span class="line-modified">204     // the JvmtiSampledObjectAllocEventCollector.</span>


205     PreserveObj obj_h(_thread, _obj_ptr);
206     JvmtiSampledObjectAllocEventCollector collector;
207     size_t size_in_bytes = _allocator._word_size * HeapWordSize;
208     ThreadLocalAllocBuffer&amp; tlab = _thread-&gt;tlab();
<span class="line-modified">209     size_t bytes_since_last = _allocated_outside_tlab ? 0 : tlab.bytes_since_last_sample_point();</span>




210     _thread-&gt;heap_sampler().check_for_sampling(obj_h(), size_in_bytes, bytes_since_last);
211   }
212 
213   if (_tlab_end_reset_for_sample || _allocated_tlab_size != 0) {
<span class="line-modified">214     _thread-&gt;tlab().set_sample_end();</span>

215   }
216 }
217 
218 void MemAllocator::Allocation::notify_allocation_low_memory_detector() {
219   // support low memory notifications (no-op if not enabled)
220   LowMemoryDetector::detect_low_memory_for_collected_pools();
221 }
222 
223 void MemAllocator::Allocation::notify_allocation_jfr_sampler() {
<span class="line-modified">224   HeapWord* mem = (HeapWord*)obj();</span>
225   size_t size_in_bytes = _allocator._word_size * HeapWordSize;
226 
227   if (_allocated_outside_tlab) {
<span class="line-modified">228     AllocTracer::send_allocation_outside_tlab(_allocator._klass, mem, size_in_bytes, _thread);</span>
229   } else if (_allocated_tlab_size != 0) {
230     // TLAB was refilled
<span class="line-modified">231     AllocTracer::send_allocation_in_new_tlab(_allocator._klass, mem, _allocated_tlab_size * HeapWordSize,</span>
232                                              size_in_bytes, _thread);
233   }
234 }
235 
236 void MemAllocator::Allocation::notify_allocation_dtrace_sampler() {
237   if (DTraceAllocProbes) {
238     // support for Dtrace object alloc event (no-op most of the time)
<span class="line-modified">239     Klass* klass = _allocator._klass;</span>
240     size_t word_size = _allocator._word_size;
241     if (klass != NULL &amp;&amp; klass-&gt;name() != NULL) {
242       SharedRuntime::dtrace_object_alloc(obj(), (int)word_size);
243     }
244   }
245 }
246 
247 void MemAllocator::Allocation::notify_allocation() {
248   notify_allocation_low_memory_detector();
249   notify_allocation_jfr_sampler();
250   notify_allocation_dtrace_sampler();
251   notify_allocation_jvmti_sampler();
252   TSAN_RUNTIME_ONLY(
253       SharedRuntime::tsan_track_obj_with_size(obj(), (int)_allocator._word_size);
254   );
255 }
256 
257 HeapWord* MemAllocator::allocate_outside_tlab(Allocation&amp; allocation) const {
258   allocation._allocated_outside_tlab = true;
<span class="line-modified">259   HeapWord* mem = _heap-&gt;mem_allocate(_word_size, &amp;allocation._overhead_limit_exceeded);</span>
260   if (mem == NULL) {
261     return mem;
262   }
263 
<span class="line-modified">264   NOT_PRODUCT(_heap-&gt;check_for_non_bad_heap_word_value(mem, _word_size));</span>
265   size_t size_in_bytes = _word_size * HeapWordSize;
266   _thread-&gt;incr_allocated_bytes(size_in_bytes);
267 
268   return mem;
269 }
270 
271 HeapWord* MemAllocator::allocate_inside_tlab(Allocation&amp; allocation) const {
272   assert(UseTLAB, &quot;should use UseTLAB&quot;);
273 
274   // Try allocating from an existing TLAB.
275   HeapWord* mem = _thread-&gt;tlab().allocate(_word_size);
276   if (mem != NULL) {
277     return mem;
278   }
279 
280   // Try refilling the TLAB and allocating the object in it.
281   return allocate_inside_tlab_slow(allocation);
282 }
283 
284 HeapWord* MemAllocator::allocate_inside_tlab_slow(Allocation&amp; allocation) const {
285   HeapWord* mem = NULL;
286   ThreadLocalAllocBuffer&amp; tlab = _thread-&gt;tlab();
287 
288   if (JvmtiExport::should_post_sampled_object_alloc()) {
<span class="line-removed">289     // Try to allocate the sampled object from TLAB, it is possible a sample</span>
<span class="line-removed">290     // point was put and the TLAB still has space.</span>
291     tlab.set_back_allocation_end();
292     mem = tlab.allocate(_word_size);





293     if (mem != NULL) {
<span class="line-removed">294       allocation._tlab_end_reset_for_sample = true;</span>
295       return mem;
296     }
297   }
298 
299   // Retain tlab and allocate object in shared space if
300   // the amount free in the tlab is too large to discard.
301   if (tlab.free() &gt; tlab.refill_waste_limit()) {
302     tlab.record_slow_allocation(_word_size);
303     return NULL;
304   }
305 
306   // Discard tlab and allocate a new one.
307   // To minimize fragmentation, the last TLAB may be smaller than the rest.
308   size_t new_tlab_size = tlab.compute_size(_word_size);
309 
310   tlab.retire_before_allocation();
311 
312   if (new_tlab_size == 0) {
313     return NULL;
314   }
315 
316   // Allocate a new TLAB requesting new_tlab_size. Any size
317   // between minimal and new_tlab_size is accepted.
318   size_t min_tlab_size = ThreadLocalAllocBuffer::compute_min_size(_word_size);
<span class="line-modified">319   mem = _heap-&gt;allocate_new_tlab(min_tlab_size, new_tlab_size, &amp;allocation._allocated_tlab_size);</span>
320   if (mem == NULL) {
321     assert(allocation._allocated_tlab_size == 0,
322            &quot;Allocation failed, but actual size was updated. min: &quot; SIZE_FORMAT
323            &quot;, desired: &quot; SIZE_FORMAT &quot;, actual: &quot; SIZE_FORMAT,
324            min_tlab_size, new_tlab_size, allocation._allocated_tlab_size);
325     return NULL;
326   }
327   assert(allocation._allocated_tlab_size != 0, &quot;Allocation succeeded but actual size not updated. mem at: &quot;
328          PTR_FORMAT &quot; min: &quot; SIZE_FORMAT &quot;, desired: &quot; SIZE_FORMAT,
329          p2i(mem), min_tlab_size, new_tlab_size);
330 
331   if (ZeroTLAB) {
332     // ..and clear it.
333     Copy::zero_to_words(mem, allocation._allocated_tlab_size);
334   } else {
335     // ...and zap just allocated object.
336 #ifdef ASSERT
337     // Skip mangling the space corresponding to the object header to
338     // ensure that the returned space is not considered parsable by
339     // any concurrent GC thread.
</pre>
<hr />
<pre>
347 }
348 
349 HeapWord* MemAllocator::mem_allocate(Allocation&amp; allocation) const {
350   if (UseTLAB) {
351     HeapWord* result = allocate_inside_tlab(allocation);
352     if (result != NULL) {
353       return result;
354     }
355   }
356 
357   return allocate_outside_tlab(allocation);
358 }
359 
360 oop MemAllocator::allocate() const {
361   oop obj = NULL;
362   {
363     Allocation allocation(*this, &amp;obj);
364     HeapWord* mem = mem_allocate(allocation);
365     if (mem != NULL) {
366       obj = initialize(mem);




367     }
368   }
369   return obj;
370 }
371 
372 void MemAllocator::mem_clear(HeapWord* mem) const {
373   assert(mem != NULL, &quot;cannot initialize NULL object&quot;);
374   const size_t hs = oopDesc::header_size();
375   assert(_word_size &gt;= hs, &quot;unexpected object size&quot;);
376   oopDesc::set_klass_gap(mem, 0);
377   Copy::fill_to_aligned_words(mem + hs, _word_size - hs);
378 }
379 
380 oop MemAllocator::finish(HeapWord* mem) const {
381   assert(mem != NULL, &quot;NULL object pointer&quot;);
382   if (UseBiasedLocking) {
383     oopDesc::set_mark_raw(mem, _klass-&gt;prototype_header());
384   } else {
385     // May be bootstrapping
<span class="line-modified">386     oopDesc::set_mark_raw(mem, markOopDesc::prototype());</span>
387   }
388   // Need a release store to ensure array/class length, mark word, and
389   // object zeroing are visible before setting the klass non-NULL, for
390   // concurrent collectors.
391   oopDesc::release_set_klass(mem, _klass);
392   return oop(mem);
393 }
394 
395 oop ObjAllocator::initialize(HeapWord* mem) const {
396   mem_clear(mem);
397   return finish(mem);
398 }
399 
400 MemRegion ObjArrayAllocator::obj_memory_range(oop obj) const {
401   if (_do_zero) {
402     return MemAllocator::obj_memory_range(obj);
403   }
404   ArrayKlass* array_klass = ArrayKlass::cast(_klass);
405   const size_t hs = arrayOopDesc::header_size(array_klass-&gt;element_type());
<span class="line-modified">406   return MemRegion(((HeapWord*)obj) + hs, _word_size - hs);</span>
407 }
408 
409 oop ObjArrayAllocator::initialize(HeapWord* mem) const {
410   // Set array length before setting the _klass field because a
411   // non-NULL klass field indicates that the object is parsable by
412   // concurrent GC.
413   assert(_length &gt;= 0, &quot;length should be non-negative&quot;);
414   if (_do_zero) {
415     mem_clear(mem);
416   }
417   arrayOopDesc::set_length(mem, _length);
418   return finish(mem);
419 }
420 
421 oop ClassAllocator::initialize(HeapWord* mem) const {
422   // Set oop_size field before setting the _klass field because a
423   // non-NULL _klass field indicates that the object is parsable by
424   // concurrent GC.
425   assert(_word_size &gt; 0, &quot;oop_size must be positive.&quot;);
426   mem_clear(mem);
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2018, 2019, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
</pre>
<hr />
<pre>
126     report_java_out_of_memory(message);
127 
128     if (JvmtiExport::should_post_resource_exhausted()) {
129       JvmtiExport::post_resource_exhausted(
130         JVMTI_RESOURCE_EXHAUSTED_OOM_ERROR | JVMTI_RESOURCE_EXHAUSTED_JAVA_HEAP,
131         message);
132     }
133     oop exception = _overhead_limit_exceeded ?
134         Universe::out_of_memory_error_gc_overhead_limit() :
135         Universe::out_of_memory_error_java_heap();
136     THROW_OOP_(exception, true);
137   } else {
138     THROW_OOP_(Universe::out_of_memory_error_retry(), true);
139   }
140 }
141 
142 void MemAllocator::Allocation::verify_before() {
143   // Clear unhandled oops for memory allocation.  Memory allocation might
144   // not take out a lock if from tlab, so clear here.
145   Thread* THREAD = _thread;

146   assert(!HAS_PENDING_EXCEPTION, &quot;Should not allocate with exception pending&quot;);
147   debug_only(check_for_valid_allocation_state());
148   assert(!Universe::heap()-&gt;is_gc_active(), &quot;Allocation during gc not allowed&quot;);
149 }
150 
151 void MemAllocator::Allocation::verify_after() {
152   NOT_PRODUCT(check_for_bad_heap_word_value();)
153 }
154 
155 void MemAllocator::Allocation::check_for_bad_heap_word_value() const {
156   MemRegion obj_range = _allocator.obj_memory_range(obj());
157   HeapWord* addr = obj_range.start();
158   size_t size = obj_range.word_size();
159   if (CheckMemoryInitialization &amp;&amp; ZapUnusedHeapArea) {
160     for (size_t slot = 0; slot &lt; size; slot += 1) {
161       assert((*(intptr_t*) (addr + slot)) != ((intptr_t) badHeapWordVal),
162              &quot;Found badHeapWordValue in post-allocation check&quot;);
163     }
164   }
165 }
166 
167 #ifdef ASSERT
168 void MemAllocator::Allocation::check_for_valid_allocation_state() const {
169   // How to choose between a pending exception and a potential
170   // OutOfMemoryError?  Don&#39;t allow pending exceptions.
171   // This is a VM policy failure, so how do we exhaustively test it?
172   assert(!_thread-&gt;has_pending_exception(),
173          &quot;shouldn&#39;t be allocating with pending exception&quot;);
<span class="line-modified">174   // Allocation of an oop can always invoke a safepoint.</span>
<span class="line-modified">175   _thread-&gt;check_for_valid_safepoint_state();</span>






176 }
177 #endif
178 
179 void MemAllocator::Allocation::notify_allocation_jvmti_sampler() {
180   // support for JVMTI VMObjectAlloc event (no-op if not enabled)
181   JvmtiExport::vm_object_alloc_event_collector(obj());
182 
183   if (!JvmtiExport::should_post_sampled_object_alloc()) {
184     // Sampling disabled
185     return;
186   }
187 
188   if (!_allocated_outside_tlab &amp;&amp; _allocated_tlab_size == 0 &amp;&amp; !_tlab_end_reset_for_sample) {
189     // Sample if it&#39;s a non-TLAB allocation, or a TLAB allocation that either refills the TLAB
190     // or expands it due to taking a sampler induced slow path.
191     return;
192   }
193 
<span class="line-modified">194   // If we want to be sampling, protect the allocated object with a Handle</span>
<span class="line-modified">195   // before doing the callback. The callback is done in the destructor of</span>
<span class="line-modified">196   // the JvmtiSampledObjectAllocEventCollector.</span>
<span class="line-modified">197   size_t bytes_since_last = 0;</span>
<span class="line-added">198 </span>
<span class="line-added">199   {</span>
200     PreserveObj obj_h(_thread, _obj_ptr);
201     JvmtiSampledObjectAllocEventCollector collector;
202     size_t size_in_bytes = _allocator._word_size * HeapWordSize;
203     ThreadLocalAllocBuffer&amp; tlab = _thread-&gt;tlab();
<span class="line-modified">204 </span>
<span class="line-added">205     if (!_allocated_outside_tlab) {</span>
<span class="line-added">206       bytes_since_last = tlab.bytes_since_last_sample_point();</span>
<span class="line-added">207     }</span>
<span class="line-added">208 </span>
209     _thread-&gt;heap_sampler().check_for_sampling(obj_h(), size_in_bytes, bytes_since_last);
210   }
211 
212   if (_tlab_end_reset_for_sample || _allocated_tlab_size != 0) {
<span class="line-modified">213     // Tell tlab to forget bytes_since_last if we passed it to the heap sampler.</span>
<span class="line-added">214     _thread-&gt;tlab().set_sample_end(bytes_since_last != 0);</span>
215   }
216 }
217 
218 void MemAllocator::Allocation::notify_allocation_low_memory_detector() {
219   // support low memory notifications (no-op if not enabled)
220   LowMemoryDetector::detect_low_memory_for_collected_pools();
221 }
222 
223 void MemAllocator::Allocation::notify_allocation_jfr_sampler() {
<span class="line-modified">224   HeapWord* mem = cast_from_oop&lt;HeapWord*&gt;(obj());</span>
225   size_t size_in_bytes = _allocator._word_size * HeapWordSize;
226 
227   if (_allocated_outside_tlab) {
<span class="line-modified">228     AllocTracer::send_allocation_outside_tlab(obj()-&gt;klass(), mem, size_in_bytes, _thread);</span>
229   } else if (_allocated_tlab_size != 0) {
230     // TLAB was refilled
<span class="line-modified">231     AllocTracer::send_allocation_in_new_tlab(obj()-&gt;klass(), mem, _allocated_tlab_size * HeapWordSize,</span>
232                                              size_in_bytes, _thread);
233   }
234 }
235 
236 void MemAllocator::Allocation::notify_allocation_dtrace_sampler() {
237   if (DTraceAllocProbes) {
238     // support for Dtrace object alloc event (no-op most of the time)
<span class="line-modified">239     Klass* klass = obj()-&gt;klass();</span>
240     size_t word_size = _allocator._word_size;
241     if (klass != NULL &amp;&amp; klass-&gt;name() != NULL) {
242       SharedRuntime::dtrace_object_alloc(obj(), (int)word_size);
243     }
244   }
245 }
246 
247 void MemAllocator::Allocation::notify_allocation() {
248   notify_allocation_low_memory_detector();
249   notify_allocation_jfr_sampler();
250   notify_allocation_dtrace_sampler();
251   notify_allocation_jvmti_sampler();
252   TSAN_RUNTIME_ONLY(
253       SharedRuntime::tsan_track_obj_with_size(obj(), (int)_allocator._word_size);
254   );
255 }
256 
257 HeapWord* MemAllocator::allocate_outside_tlab(Allocation&amp; allocation) const {
258   allocation._allocated_outside_tlab = true;
<span class="line-modified">259   HeapWord* mem = Universe::heap()-&gt;mem_allocate(_word_size, &amp;allocation._overhead_limit_exceeded);</span>
260   if (mem == NULL) {
261     return mem;
262   }
263 
<span class="line-modified">264   NOT_PRODUCT(Universe::heap()-&gt;check_for_non_bad_heap_word_value(mem, _word_size));</span>
265   size_t size_in_bytes = _word_size * HeapWordSize;
266   _thread-&gt;incr_allocated_bytes(size_in_bytes);
267 
268   return mem;
269 }
270 
271 HeapWord* MemAllocator::allocate_inside_tlab(Allocation&amp; allocation) const {
272   assert(UseTLAB, &quot;should use UseTLAB&quot;);
273 
274   // Try allocating from an existing TLAB.
275   HeapWord* mem = _thread-&gt;tlab().allocate(_word_size);
276   if (mem != NULL) {
277     return mem;
278   }
279 
280   // Try refilling the TLAB and allocating the object in it.
281   return allocate_inside_tlab_slow(allocation);
282 }
283 
284 HeapWord* MemAllocator::allocate_inside_tlab_slow(Allocation&amp; allocation) const {
285   HeapWord* mem = NULL;
286   ThreadLocalAllocBuffer&amp; tlab = _thread-&gt;tlab();
287 
288   if (JvmtiExport::should_post_sampled_object_alloc()) {


289     tlab.set_back_allocation_end();
290     mem = tlab.allocate(_word_size);
<span class="line-added">291 </span>
<span class="line-added">292     // We set back the allocation sample point to try to allocate this, reset it</span>
<span class="line-added">293     // when done.</span>
<span class="line-added">294     allocation._tlab_end_reset_for_sample = true;</span>
<span class="line-added">295 </span>
296     if (mem != NULL) {

297       return mem;
298     }
299   }
300 
301   // Retain tlab and allocate object in shared space if
302   // the amount free in the tlab is too large to discard.
303   if (tlab.free() &gt; tlab.refill_waste_limit()) {
304     tlab.record_slow_allocation(_word_size);
305     return NULL;
306   }
307 
308   // Discard tlab and allocate a new one.
309   // To minimize fragmentation, the last TLAB may be smaller than the rest.
310   size_t new_tlab_size = tlab.compute_size(_word_size);
311 
312   tlab.retire_before_allocation();
313 
314   if (new_tlab_size == 0) {
315     return NULL;
316   }
317 
318   // Allocate a new TLAB requesting new_tlab_size. Any size
319   // between minimal and new_tlab_size is accepted.
320   size_t min_tlab_size = ThreadLocalAllocBuffer::compute_min_size(_word_size);
<span class="line-modified">321   mem = Universe::heap()-&gt;allocate_new_tlab(min_tlab_size, new_tlab_size, &amp;allocation._allocated_tlab_size);</span>
322   if (mem == NULL) {
323     assert(allocation._allocated_tlab_size == 0,
324            &quot;Allocation failed, but actual size was updated. min: &quot; SIZE_FORMAT
325            &quot;, desired: &quot; SIZE_FORMAT &quot;, actual: &quot; SIZE_FORMAT,
326            min_tlab_size, new_tlab_size, allocation._allocated_tlab_size);
327     return NULL;
328   }
329   assert(allocation._allocated_tlab_size != 0, &quot;Allocation succeeded but actual size not updated. mem at: &quot;
330          PTR_FORMAT &quot; min: &quot; SIZE_FORMAT &quot;, desired: &quot; SIZE_FORMAT,
331          p2i(mem), min_tlab_size, new_tlab_size);
332 
333   if (ZeroTLAB) {
334     // ..and clear it.
335     Copy::zero_to_words(mem, allocation._allocated_tlab_size);
336   } else {
337     // ...and zap just allocated object.
338 #ifdef ASSERT
339     // Skip mangling the space corresponding to the object header to
340     // ensure that the returned space is not considered parsable by
341     // any concurrent GC thread.
</pre>
<hr />
<pre>
349 }
350 
351 HeapWord* MemAllocator::mem_allocate(Allocation&amp; allocation) const {
352   if (UseTLAB) {
353     HeapWord* result = allocate_inside_tlab(allocation);
354     if (result != NULL) {
355       return result;
356     }
357   }
358 
359   return allocate_outside_tlab(allocation);
360 }
361 
362 oop MemAllocator::allocate() const {
363   oop obj = NULL;
364   {
365     Allocation allocation(*this, &amp;obj);
366     HeapWord* mem = mem_allocate(allocation);
367     if (mem != NULL) {
368       obj = initialize(mem);
<span class="line-added">369     } else {</span>
<span class="line-added">370       // The unhandled oop detector will poison local variable obj,</span>
<span class="line-added">371       // so reset it to NULL if mem is NULL.</span>
<span class="line-added">372       obj = NULL;</span>
373     }
374   }
375   return obj;
376 }
377 
378 void MemAllocator::mem_clear(HeapWord* mem) const {
379   assert(mem != NULL, &quot;cannot initialize NULL object&quot;);
380   const size_t hs = oopDesc::header_size();
381   assert(_word_size &gt;= hs, &quot;unexpected object size&quot;);
382   oopDesc::set_klass_gap(mem, 0);
383   Copy::fill_to_aligned_words(mem + hs, _word_size - hs);
384 }
385 
386 oop MemAllocator::finish(HeapWord* mem) const {
387   assert(mem != NULL, &quot;NULL object pointer&quot;);
388   if (UseBiasedLocking) {
389     oopDesc::set_mark_raw(mem, _klass-&gt;prototype_header());
390   } else {
391     // May be bootstrapping
<span class="line-modified">392     oopDesc::set_mark_raw(mem, markWord::prototype());</span>
393   }
394   // Need a release store to ensure array/class length, mark word, and
395   // object zeroing are visible before setting the klass non-NULL, for
396   // concurrent collectors.
397   oopDesc::release_set_klass(mem, _klass);
398   return oop(mem);
399 }
400 
401 oop ObjAllocator::initialize(HeapWord* mem) const {
402   mem_clear(mem);
403   return finish(mem);
404 }
405 
406 MemRegion ObjArrayAllocator::obj_memory_range(oop obj) const {
407   if (_do_zero) {
408     return MemAllocator::obj_memory_range(obj);
409   }
410   ArrayKlass* array_klass = ArrayKlass::cast(_klass);
411   const size_t hs = arrayOopDesc::header_size(array_klass-&gt;element_type());
<span class="line-modified">412   return MemRegion(cast_from_oop&lt;HeapWord*&gt;(obj) + hs, _word_size - hs);</span>
413 }
414 
415 oop ObjArrayAllocator::initialize(HeapWord* mem) const {
416   // Set array length before setting the _klass field because a
417   // non-NULL klass field indicates that the object is parsable by
418   // concurrent GC.
419   assert(_length &gt;= 0, &quot;length should be non-negative&quot;);
420   if (_do_zero) {
421     mem_clear(mem);
422   }
423   arrayOopDesc::set_length(mem, _length);
424   return finish(mem);
425 }
426 
427 oop ClassAllocator::initialize(HeapWord* mem) const {
428   // Set oop_size field before setting the _klass field because a
429   // non-NULL _klass field indicates that the object is parsable by
430   // concurrent GC.
431   assert(_word_size &gt; 0, &quot;oop_size must be positive.&quot;);
432   mem_clear(mem);
</pre>
</td>
</tr>
</table>
<center><a href="markBitMap.inline.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="memAllocator.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>