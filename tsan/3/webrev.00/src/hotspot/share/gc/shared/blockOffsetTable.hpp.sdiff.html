<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/shared/blockOffsetTable.hpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="blockOffsetTable.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="blockOffsetTable.inline.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/shared/blockOffsetTable.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 24 
 25 #ifndef SHARE_GC_SHARED_BLOCKOFFSETTABLE_HPP
 26 #define SHARE_GC_SHARED_BLOCKOFFSETTABLE_HPP
 27 
 28 #include &quot;gc/shared/memset_with_concurrent_readers.hpp&quot;
 29 #include &quot;memory/allocation.hpp&quot;
 30 #include &quot;memory/memRegion.hpp&quot;
 31 #include &quot;memory/virtualspace.hpp&quot;
 32 #include &quot;runtime/globals.hpp&quot;
 33 #include &quot;utilities/globalDefinitions.hpp&quot;
 34 #include &quot;utilities/macros.hpp&quot;
 35 
 36 // The CollectedHeap type requires subtypes to implement a method
 37 // &quot;block_start&quot;.  For some subtypes, notably generational
 38 // systems using card-table-based write barriers, the efficiency of this
 39 // operation may be important.  Implementations of the &quot;BlockOffsetArray&quot;
 40 // class may be useful in providing such efficient implementations.
 41 //
 42 // BlockOffsetTable (abstract)
 43 //   - BlockOffsetArray (abstract)
<span class="line-removed"> 44 //     - BlockOffsetArrayNonContigSpace</span>
 45 //     - BlockOffsetArrayContigSpace
 46 //
 47 
 48 class ContiguousSpace;
 49 
 50 class BOTConstants : public AllStatic {
 51 public:
 52   static const uint LogN = 9;
 53   static const uint LogN_words = LogN - LogHeapWordSize;
 54   static const uint N_bytes = 1 &lt;&lt; LogN;
 55   static const uint N_words = 1 &lt;&lt; LogN_words;
 56   // entries &quot;e&quot; of at least N_words mean &quot;go back by Base^(e-N_words).&quot;
 57   // All entries are less than &quot;N_words + N_powers&quot;.
 58   static const uint LogBase = 4;
 59   static const uint Base = (1 &lt;&lt; LogBase);
 60   static const uint N_powers = 14;
 61 
 62   static size_t power_to_cards_back(uint i) {
 63     return (size_t)1 &lt;&lt; (LogBase * i);
 64   }
</pre>
<hr />
<pre>
138   friend class VMStructs;
139 
140  private:
141   bool _init_to_zero;
142 
143   // The reserved region covered by the shared array.
144   MemRegion _reserved;
145 
146   // End of the current committed region.
147   HeapWord* _end;
148 
149   // Array for keeping offsets for retrieving object start fast given an
150   // address.
151   VirtualSpace _vs;
152   u_char* _offset_array;          // byte array keeping backwards offsets
153 
154   void fill_range(size_t start, size_t num_cards, u_char offset) {
155     void* start_ptr = &amp;_offset_array[start];
156     // If collector is concurrent, special handling may be needed.
157     G1GC_ONLY(assert(!UseG1GC, &quot;Shouldn&#39;t be here when using G1&quot;);)
<span class="line-removed">158 #if INCLUDE_CMSGC</span>
<span class="line-removed">159     if (UseConcMarkSweepGC) {</span>
<span class="line-removed">160       memset_with_concurrent_readers(start_ptr, offset, num_cards);</span>
<span class="line-removed">161       return;</span>
<span class="line-removed">162     }</span>
<span class="line-removed">163 #endif // INCLUDE_CMSGC</span>
164     memset(start_ptr, offset, num_cards);
165   }
166 
167  protected:
168   // Bounds checking accessors:
169   // For performance these have to devolve to array accesses in product builds.
170   u_char offset_array(size_t index) const {
171     assert(index &lt; _vs.committed_size(), &quot;index out of range&quot;);
172     return _offset_array[index];
173   }
174   // An assertion-checking helper method for the set_offset_array() methods below.
175   void check_reducing_assertion(bool reducing);
176 
177   void set_offset_array(size_t index, u_char offset, bool reducing = false) {
178     check_reducing_assertion(reducing);
179     assert(index &lt; _vs.committed_size(), &quot;index out of range&quot;);
180     assert(!reducing || _offset_array[index] &gt;= offset, &quot;Not reducing&quot;);
181     _offset_array[index] = offset;
182   }
183 
</pre>
<hr />
<pre>
371 
372   // If true, initialize array slots with no allocated blocks to zero.
373   // Otherwise, make them point back to the front.
374   bool init_to_zero() { return _init_to_zero; }
375   // Corresponding setter
376   void set_init_to_zero(bool val) {
377     _init_to_zero = val;
378     assert(_array != NULL, &quot;_array should be non-NULL&quot;);
379     _array-&gt;set_init_to_zero(val);
380   }
381 
382   // Debugging
383   // Return the index of the last entry in the &quot;active&quot; region.
384   virtual size_t last_active_index() const = 0;
385   // Verify the block offset table
386   void verify() const;
387   void check_all_cards(size_t left_card, size_t right_card) const;
388 };
389 
390 ////////////////////////////////////////////////////////////////////////////
<span class="line-removed">391 // A subtype of BlockOffsetArray that takes advantage of the fact</span>
<span class="line-removed">392 // that its underlying space is a NonContiguousSpace, so that some</span>
<span class="line-removed">393 // specialized interfaces can be made available for spaces that</span>
<span class="line-removed">394 // manipulate the table.</span>
<span class="line-removed">395 ////////////////////////////////////////////////////////////////////////////</span>
<span class="line-removed">396 class BlockOffsetArrayNonContigSpace: public BlockOffsetArray {</span>
<span class="line-removed">397   friend class VMStructs;</span>
<span class="line-removed">398  private:</span>
<span class="line-removed">399   // The portion [_unallocated_block, _sp.end()) of the space that</span>
<span class="line-removed">400   // is a single block known not to contain any objects.</span>
<span class="line-removed">401   // NOTE: See BlockOffsetArrayUseUnallocatedBlock flag.</span>
<span class="line-removed">402   HeapWord* _unallocated_block;</span>
<span class="line-removed">403 </span>
<span class="line-removed">404  public:</span>
<span class="line-removed">405   BlockOffsetArrayNonContigSpace(BlockOffsetSharedArray* array, MemRegion mr):</span>
<span class="line-removed">406     BlockOffsetArray(array, mr, false),</span>
<span class="line-removed">407     _unallocated_block(_bottom) { }</span>
<span class="line-removed">408 </span>
<span class="line-removed">409   // Accessor</span>
<span class="line-removed">410   HeapWord* unallocated_block() const {</span>
<span class="line-removed">411     assert(BlockOffsetArrayUseUnallocatedBlock,</span>
<span class="line-removed">412            &quot;_unallocated_block is not being maintained&quot;);</span>
<span class="line-removed">413     return _unallocated_block;</span>
<span class="line-removed">414   }</span>
<span class="line-removed">415 </span>
<span class="line-removed">416   void set_unallocated_block(HeapWord* block) {</span>
<span class="line-removed">417     assert(BlockOffsetArrayUseUnallocatedBlock,</span>
<span class="line-removed">418            &quot;_unallocated_block is not being maintained&quot;);</span>
<span class="line-removed">419     assert(block &gt;= _bottom &amp;&amp; block &lt;= _end, &quot;out of range&quot;);</span>
<span class="line-removed">420     _unallocated_block = block;</span>
<span class="line-removed">421   }</span>
<span class="line-removed">422 </span>
<span class="line-removed">423   // These methods expect to be called with [blk_start, blk_end)</span>
<span class="line-removed">424   // representing a block of memory in the heap.</span>
<span class="line-removed">425   void alloc_block(HeapWord* blk_start, HeapWord* blk_end);</span>
<span class="line-removed">426   void alloc_block(HeapWord* blk, size_t size) {</span>
<span class="line-removed">427     alloc_block(blk, blk + size);</span>
<span class="line-removed">428   }</span>
<span class="line-removed">429 </span>
<span class="line-removed">430   // The following methods are useful and optimized for a</span>
<span class="line-removed">431   // non-contiguous space.</span>
<span class="line-removed">432 </span>
<span class="line-removed">433   // Given a block [blk_start, blk_start + full_blk_size), and</span>
<span class="line-removed">434   // a left_blk_size &lt; full_blk_size, adjust the BOT to show two</span>
<span class="line-removed">435   // blocks [blk_start, blk_start + left_blk_size) and</span>
<span class="line-removed">436   // [blk_start + left_blk_size, blk_start + full_blk_size).</span>
<span class="line-removed">437   // It is assumed (and verified in the non-product VM) that the</span>
<span class="line-removed">438   // BOT was correct for the original block.</span>
<span class="line-removed">439   void split_block(HeapWord* blk_start, size_t full_blk_size,</span>
<span class="line-removed">440                            size_t left_blk_size);</span>
<span class="line-removed">441 </span>
<span class="line-removed">442   // Adjust BOT to show that it has a block in the range</span>
<span class="line-removed">443   // [blk_start, blk_start + size). Only the first card</span>
<span class="line-removed">444   // of BOT is touched. It is assumed (and verified in the</span>
<span class="line-removed">445   // non-product VM) that the remaining cards of the block</span>
<span class="line-removed">446   // are correct.</span>
<span class="line-removed">447   void mark_block(HeapWord* blk_start, HeapWord* blk_end, bool reducing = false);</span>
<span class="line-removed">448   void mark_block(HeapWord* blk, size_t size, bool reducing = false) {</span>
<span class="line-removed">449     mark_block(blk, blk + size, reducing);</span>
<span class="line-removed">450   }</span>
<span class="line-removed">451 </span>
<span class="line-removed">452   // Adjust _unallocated_block to indicate that a particular</span>
<span class="line-removed">453   // block has been newly allocated or freed. It is assumed (and</span>
<span class="line-removed">454   // verified in the non-product VM) that the BOT is correct for</span>
<span class="line-removed">455   // the given block.</span>
<span class="line-removed">456   void allocated(HeapWord* blk_start, HeapWord* blk_end, bool reducing = false) {</span>
<span class="line-removed">457     // Verify that the BOT shows [blk, blk + blk_size) to be one block.</span>
<span class="line-removed">458     verify_single_block(blk_start, blk_end);</span>
<span class="line-removed">459     if (BlockOffsetArrayUseUnallocatedBlock) {</span>
<span class="line-removed">460       _unallocated_block = MAX2(_unallocated_block, blk_end);</span>
<span class="line-removed">461     }</span>
<span class="line-removed">462   }</span>
<span class="line-removed">463 </span>
<span class="line-removed">464   void allocated(HeapWord* blk, size_t size, bool reducing = false) {</span>
<span class="line-removed">465     allocated(blk, blk + size, reducing);</span>
<span class="line-removed">466   }</span>
<span class="line-removed">467 </span>
<span class="line-removed">468   void freed(HeapWord* blk_start, HeapWord* blk_end);</span>
<span class="line-removed">469   void freed(HeapWord* blk, size_t size);</span>
<span class="line-removed">470 </span>
<span class="line-removed">471   HeapWord* block_start_unsafe(const void* addr) const;</span>
<span class="line-removed">472 </span>
<span class="line-removed">473   // Requires &quot;addr&quot; to be the start of a card and returns the</span>
<span class="line-removed">474   // start of the block that contains the given address.</span>
<span class="line-removed">475   HeapWord* block_start_careful(const void* addr) const;</span>
<span class="line-removed">476 </span>
<span class="line-removed">477   // Verification &amp; debugging: ensure that the offset table reflects</span>
<span class="line-removed">478   // the fact that the block [blk_start, blk_end) or [blk, blk + size)</span>
<span class="line-removed">479   // is a single block of storage. NOTE: can&#39;t const this because of</span>
<span class="line-removed">480   // call to non-const do_block_internal() below.</span>
<span class="line-removed">481   void verify_single_block(HeapWord* blk_start, HeapWord* blk_end)</span>
<span class="line-removed">482     PRODUCT_RETURN;</span>
<span class="line-removed">483   void verify_single_block(HeapWord* blk, size_t size) PRODUCT_RETURN;</span>
<span class="line-removed">484 </span>
<span class="line-removed">485   // Verify that the given block is before _unallocated_block</span>
<span class="line-removed">486   void verify_not_unallocated(HeapWord* blk_start, HeapWord* blk_end)</span>
<span class="line-removed">487     const PRODUCT_RETURN;</span>
<span class="line-removed">488   void verify_not_unallocated(HeapWord* blk, size_t size)</span>
<span class="line-removed">489     const PRODUCT_RETURN;</span>
<span class="line-removed">490 </span>
<span class="line-removed">491   // Debugging support</span>
<span class="line-removed">492   virtual size_t last_active_index() const;</span>
<span class="line-removed">493 };</span>
<span class="line-removed">494 </span>
<span class="line-removed">495 ////////////////////////////////////////////////////////////////////////////</span>
496 // A subtype of BlockOffsetArray that takes advantage of the fact
497 // that its underlying space is a ContiguousSpace, so that its &quot;active&quot;
498 // region can be more efficiently tracked (than for a non-contiguous space).
499 ////////////////////////////////////////////////////////////////////////////
500 class BlockOffsetArrayContigSpace: public BlockOffsetArray {
501   friend class VMStructs;
502  private:
503   // allocation boundary at which offset array must be updated
504   HeapWord* _next_offset_threshold;
505   size_t    _next_offset_index;      // index corresponding to that boundary
506 
507   // Work function when allocation start crosses threshold.
508   void alloc_block_work(HeapWord* blk_start, HeapWord* blk_end);
509 
510  public:
511   BlockOffsetArrayContigSpace(BlockOffsetSharedArray* array, MemRegion mr):
512     BlockOffsetArray(array, mr, true) {
513     _next_offset_threshold = NULL;
514     _next_offset_index = 0;
515   }
</pre>
</td>
<td>
<hr />
<pre>
 24 
 25 #ifndef SHARE_GC_SHARED_BLOCKOFFSETTABLE_HPP
 26 #define SHARE_GC_SHARED_BLOCKOFFSETTABLE_HPP
 27 
 28 #include &quot;gc/shared/memset_with_concurrent_readers.hpp&quot;
 29 #include &quot;memory/allocation.hpp&quot;
 30 #include &quot;memory/memRegion.hpp&quot;
 31 #include &quot;memory/virtualspace.hpp&quot;
 32 #include &quot;runtime/globals.hpp&quot;
 33 #include &quot;utilities/globalDefinitions.hpp&quot;
 34 #include &quot;utilities/macros.hpp&quot;
 35 
 36 // The CollectedHeap type requires subtypes to implement a method
 37 // &quot;block_start&quot;.  For some subtypes, notably generational
 38 // systems using card-table-based write barriers, the efficiency of this
 39 // operation may be important.  Implementations of the &quot;BlockOffsetArray&quot;
 40 // class may be useful in providing such efficient implementations.
 41 //
 42 // BlockOffsetTable (abstract)
 43 //   - BlockOffsetArray (abstract)

 44 //     - BlockOffsetArrayContigSpace
 45 //
 46 
 47 class ContiguousSpace;
 48 
 49 class BOTConstants : public AllStatic {
 50 public:
 51   static const uint LogN = 9;
 52   static const uint LogN_words = LogN - LogHeapWordSize;
 53   static const uint N_bytes = 1 &lt;&lt; LogN;
 54   static const uint N_words = 1 &lt;&lt; LogN_words;
 55   // entries &quot;e&quot; of at least N_words mean &quot;go back by Base^(e-N_words).&quot;
 56   // All entries are less than &quot;N_words + N_powers&quot;.
 57   static const uint LogBase = 4;
 58   static const uint Base = (1 &lt;&lt; LogBase);
 59   static const uint N_powers = 14;
 60 
 61   static size_t power_to_cards_back(uint i) {
 62     return (size_t)1 &lt;&lt; (LogBase * i);
 63   }
</pre>
<hr />
<pre>
137   friend class VMStructs;
138 
139  private:
140   bool _init_to_zero;
141 
142   // The reserved region covered by the shared array.
143   MemRegion _reserved;
144 
145   // End of the current committed region.
146   HeapWord* _end;
147 
148   // Array for keeping offsets for retrieving object start fast given an
149   // address.
150   VirtualSpace _vs;
151   u_char* _offset_array;          // byte array keeping backwards offsets
152 
153   void fill_range(size_t start, size_t num_cards, u_char offset) {
154     void* start_ptr = &amp;_offset_array[start];
155     // If collector is concurrent, special handling may be needed.
156     G1GC_ONLY(assert(!UseG1GC, &quot;Shouldn&#39;t be here when using G1&quot;);)






157     memset(start_ptr, offset, num_cards);
158   }
159 
160  protected:
161   // Bounds checking accessors:
162   // For performance these have to devolve to array accesses in product builds.
163   u_char offset_array(size_t index) const {
164     assert(index &lt; _vs.committed_size(), &quot;index out of range&quot;);
165     return _offset_array[index];
166   }
167   // An assertion-checking helper method for the set_offset_array() methods below.
168   void check_reducing_assertion(bool reducing);
169 
170   void set_offset_array(size_t index, u_char offset, bool reducing = false) {
171     check_reducing_assertion(reducing);
172     assert(index &lt; _vs.committed_size(), &quot;index out of range&quot;);
173     assert(!reducing || _offset_array[index] &gt;= offset, &quot;Not reducing&quot;);
174     _offset_array[index] = offset;
175   }
176 
</pre>
<hr />
<pre>
364 
365   // If true, initialize array slots with no allocated blocks to zero.
366   // Otherwise, make them point back to the front.
367   bool init_to_zero() { return _init_to_zero; }
368   // Corresponding setter
369   void set_init_to_zero(bool val) {
370     _init_to_zero = val;
371     assert(_array != NULL, &quot;_array should be non-NULL&quot;);
372     _array-&gt;set_init_to_zero(val);
373   }
374 
375   // Debugging
376   // Return the index of the last entry in the &quot;active&quot; region.
377   virtual size_t last_active_index() const = 0;
378   // Verify the block offset table
379   void verify() const;
380   void check_all_cards(size_t left_card, size_t right_card) const;
381 };
382 
383 ////////////////////////////////////////////////////////////////////////////









































































































384 // A subtype of BlockOffsetArray that takes advantage of the fact
385 // that its underlying space is a ContiguousSpace, so that its &quot;active&quot;
386 // region can be more efficiently tracked (than for a non-contiguous space).
387 ////////////////////////////////////////////////////////////////////////////
388 class BlockOffsetArrayContigSpace: public BlockOffsetArray {
389   friend class VMStructs;
390  private:
391   // allocation boundary at which offset array must be updated
392   HeapWord* _next_offset_threshold;
393   size_t    _next_offset_index;      // index corresponding to that boundary
394 
395   // Work function when allocation start crosses threshold.
396   void alloc_block_work(HeapWord* blk_start, HeapWord* blk_end);
397 
398  public:
399   BlockOffsetArrayContigSpace(BlockOffsetSharedArray* array, MemRegion mr):
400     BlockOffsetArray(array, mr, true) {
401     _next_offset_threshold = NULL;
402     _next_offset_index = 0;
403   }
</pre>
</td>
</tr>
</table>
<center><a href="blockOffsetTable.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="blockOffsetTable.inline.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>