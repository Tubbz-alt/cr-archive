<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/share/gc/shared/oopStorage.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="modRefBarrierSet.inline.hpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="oopStorage.hpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/shared/oopStorage.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 1,7 ***</span>
  /*
<span class="line-modified">!  * Copyright (c) 2018, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
<span class="line-new-header">--- 1,7 ---</span>
  /*
<span class="line-modified">!  * Copyright (c) 2018, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 33,19 ***</span>
<span class="line-new-header">--- 33,21 ---</span>
  #include &quot;runtime/handles.inline.hpp&quot;
  #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  #include &quot;runtime/mutex.hpp&quot;
  #include &quot;runtime/mutexLocker.hpp&quot;
  #include &quot;runtime/orderAccess.hpp&quot;
<span class="line-added">+ #include &quot;runtime/os.hpp&quot;</span>
  #include &quot;runtime/safepoint.hpp&quot;
  #include &quot;runtime/stubRoutines.hpp&quot;
  #include &quot;runtime/thread.hpp&quot;
  #include &quot;utilities/align.hpp&quot;
  #include &quot;utilities/count_trailing_zeros.hpp&quot;
  #include &quot;utilities/debug.hpp&quot;
  #include &quot;utilities/globalDefinitions.hpp&quot;
  #include &quot;utilities/macros.hpp&quot;
  #include &quot;utilities/ostream.hpp&quot;
<span class="line-added">+ #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  
  OopStorage::AllocationListEntry::AllocationListEntry() : _prev(NULL), _next(NULL) {}
  
  OopStorage::AllocationListEntry::~AllocationListEntry() {
    assert(_prev == NULL, &quot;deleting attached block&quot;);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 137,20 ***</span>
  size_t OopStorage::ActiveArray::block_count() const {
    return _block_count;
  }
  
  size_t OopStorage::ActiveArray::block_count_acquire() const {
<span class="line-modified">!   return OrderAccess::load_acquire(&amp;_block_count);</span>
  }
  
  void OopStorage::ActiveArray::increment_refcount() const {
<span class="line-modified">!   int new_value = Atomic::add(1, &amp;_refcount);</span>
    assert(new_value &gt;= 1, &quot;negative refcount %d&quot;, new_value - 1);
  }
  
  bool OopStorage::ActiveArray::decrement_refcount() const {
<span class="line-modified">!   int new_value = Atomic::sub(1, &amp;_refcount);</span>
    assert(new_value &gt;= 0, &quot;negative refcount %d&quot;, new_value);
    return new_value == 0;
  }
  
  bool OopStorage::ActiveArray::push(Block* block) {
<span class="line-new-header">--- 139,20 ---</span>
  size_t OopStorage::ActiveArray::block_count() const {
    return _block_count;
  }
  
  size_t OopStorage::ActiveArray::block_count_acquire() const {
<span class="line-modified">!   return Atomic::load_acquire(&amp;_block_count);</span>
  }
  
  void OopStorage::ActiveArray::increment_refcount() const {
<span class="line-modified">!   int new_value = Atomic::add(&amp;_refcount, 1);</span>
    assert(new_value &gt;= 1, &quot;negative refcount %d&quot;, new_value - 1);
  }
  
  bool OopStorage::ActiveArray::decrement_refcount() const {
<span class="line-modified">!   int new_value = Atomic::sub(&amp;_refcount, 1);</span>
    assert(new_value &gt;= 0, &quot;negative refcount %d&quot;, new_value);
    return new_value == 0;
  }
  
  bool OopStorage::ActiveArray::push(Block* block) {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 158,11 ***</span>
    if (index &lt; _size) {
      block-&gt;set_active_index(index);
      *block_ptr(index) = block;
      // Use a release_store to ensure all the setup is complete before
      // making the block visible.
<span class="line-modified">!     OrderAccess::release_store(&amp;_block_count, index + 1);</span>
      return true;
    } else {
      return false;
    }
  }
<span class="line-new-header">--- 160,11 ---</span>
    if (index &lt; _size) {
      block-&gt;set_active_index(index);
      *block_ptr(index) = block;
      // Use a release_store to ensure all the setup is complete before
      // making the block visible.
<span class="line-modified">!     Atomic::release_store(&amp;_block_count, index + 1);</span>
      return true;
    } else {
      return false;
    }
  }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 205,11 ***</span>
  const unsigned block_alignment = sizeof(oop) * section_size;
  
  OopStorage::Block::Block(const OopStorage* owner, void* memory) :
    _data(),
    _allocated_bitmask(0),
<span class="line-modified">!   _owner(owner),</span>
    _memory(memory),
    _active_index(0),
    _allocation_list_entry(),
    _deferred_updates_next(NULL),
    _release_refcount(0)
<span class="line-new-header">--- 207,11 ---</span>
  const unsigned block_alignment = sizeof(oop) * section_size;
  
  OopStorage::Block::Block(const OopStorage* owner, void* memory) :
    _data(),
    _allocated_bitmask(0),
<span class="line-modified">!   _owner_address(reinterpret_cast&lt;intptr_t&gt;(owner)),</span>
    _memory(memory),
    _active_index(0),
    _allocation_list_entry(),
    _deferred_updates_next(NULL),
    _release_refcount(0)
</pre>
<hr />
<pre>
<span class="line-old-header">*** 225,11 ***</span>
    assert(_release_refcount == 0, &quot;deleting block while releasing&quot;);
    assert(_deferred_updates_next == NULL, &quot;deleting block with deferred update&quot;);
    // Clear fields used by block_for_ptr and entry validation, which
    // might help catch bugs.  Volatile to prevent dead-store elimination.
    const_cast&lt;uintx volatile&amp;&gt;(_allocated_bitmask) = 0;
<span class="line-modified">!   const_cast&lt;OopStorage* volatile&amp;&gt;(_owner) = NULL;</span>
  }
  
  size_t OopStorage::Block::allocation_size() {
    // _data must be first member, so aligning Block aligns _data.
    STATIC_ASSERT(_data_pos == 0);
<span class="line-new-header">--- 227,11 ---</span>
    assert(_release_refcount == 0, &quot;deleting block while releasing&quot;);
    assert(_deferred_updates_next == NULL, &quot;deleting block with deferred update&quot;);
    // Clear fields used by block_for_ptr and entry validation, which
    // might help catch bugs.  Volatile to prevent dead-store elimination.
    const_cast&lt;uintx volatile&amp;&gt;(_allocated_bitmask) = 0;
<span class="line-modified">!   const_cast&lt;intptr_t volatile&amp;&gt;(_owner_address) = 0;</span>
  }
  
  size_t OopStorage::Block::allocation_size() {
    // _data must be first member, so aligning Block aligns _data.
    STATIC_ASSERT(_data_pos == 0);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 261,12 ***</span>
  // For interaction with release(), these must follow the empty check,
  // and the order of these checks is important.
  bool OopStorage::Block::is_safe_to_delete() const {
    assert(is_empty(), &quot;precondition&quot;);
    OrderAccess::loadload();
<span class="line-modified">!   return (OrderAccess::load_acquire(&amp;_release_refcount) == 0) &amp;&amp;</span>
<span class="line-modified">!          (OrderAccess::load_acquire(&amp;_deferred_updates_next) == NULL);</span>
  }
  
  OopStorage::Block* OopStorage::Block::deferred_updates_next() const {
    return _deferred_updates_next;
  }
<span class="line-new-header">--- 263,12 ---</span>
  // For interaction with release(), these must follow the empty check,
  // and the order of these checks is important.
  bool OopStorage::Block::is_safe_to_delete() const {
    assert(is_empty(), &quot;precondition&quot;);
    OrderAccess::loadload();
<span class="line-modified">!   return (Atomic::load_acquire(&amp;_release_refcount) == 0) &amp;&amp;</span>
<span class="line-modified">!          (Atomic::load_acquire(&amp;_deferred_updates_next) == NULL);</span>
  }
  
  OopStorage::Block* OopStorage::Block::deferred_updates_next() const {
    return _deferred_updates_next;
  }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 304,11 ***</span>
    uintx allocated = allocated_bitmask();
    while (true) {
      assert(!is_full_bitmask(allocated), &quot;attempt to allocate from full block&quot;);
      unsigned index = count_trailing_zeros(~allocated);
      uintx new_value = allocated | bitmask_for_index(index);
<span class="line-modified">!     uintx fetched = Atomic::cmpxchg(new_value, &amp;_allocated_bitmask, allocated);</span>
      if (fetched == allocated) {
        return get_pointer(index); // CAS succeeded; return entry for index.
      }
      allocated = fetched;       // CAS failed; retry with latest value.
    }
<span class="line-new-header">--- 306,11 ---</span>
    uintx allocated = allocated_bitmask();
    while (true) {
      assert(!is_full_bitmask(allocated), &quot;attempt to allocate from full block&quot;);
      unsigned index = count_trailing_zeros(~allocated);
      uintx new_value = allocated | bitmask_for_index(index);
<span class="line-modified">!     uintx fetched = Atomic::cmpxchg(&amp;_allocated_bitmask, allocated, new_value);</span>
      if (fetched == allocated) {
        return get_pointer(index); // CAS succeeded; return entry for index.
      }
      allocated = fetched;       // CAS failed; retry with latest value.
    }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 353,13 ***</span>
    // start position, the value at the owner position will be some oop
    // (possibly NULL), which can never match the owner.
    intptr_t owner_addr = reinterpret_cast&lt;intptr_t&gt;(owner);
    for (unsigned i = 0; i &lt; section_count; ++i, section += section_size) {
      Block* candidate = reinterpret_cast&lt;Block*&gt;(section);
<span class="line-modified">!     intptr_t* candidate_owner_addr</span>
<span class="line-removed">-       = reinterpret_cast&lt;intptr_t*&gt;(&amp;candidate-&gt;_owner);</span>
<span class="line-removed">-     if (SafeFetchN(candidate_owner_addr, 0) == owner_addr) {</span>
        return candidate;
      }
    }
    return NULL;
  }
<span class="line-new-header">--- 355,11 ---</span>
    // start position, the value at the owner position will be some oop
    // (possibly NULL), which can never match the owner.
    intptr_t owner_addr = reinterpret_cast&lt;intptr_t&gt;(owner);
    for (unsigned i = 0; i &lt; section_count; ++i, section += section_size) {
      Block* candidate = reinterpret_cast&lt;Block*&gt;(section);
<span class="line-modified">!     if (SafeFetchN(&amp;candidate-&gt;_owner_address, 0) == owner_addr) {</span>
        return candidate;
      }
    }
    return NULL;
  }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 410,46 ***</span>
  // added to the _allocation_list if not already present and the bitmask is not
  // full.  The block is moved to the end of the _allocation_list if the bitmask
  // is empty, for ease of empty block deletion processing.
  
  oop* OopStorage::allocate() {
<span class="line-modified">!   MutexLockerEx ml(_allocation_mutex, Mutex::_no_safepoint_check_flag);</span>
<span class="line-removed">- </span>
<span class="line-removed">-   // Note: Without this we might never perform cleanup.  As it is,</span>
<span class="line-removed">-   // cleanup is only requested here, when completing a concurrent</span>
<span class="line-removed">-   // iteration, or when someone entirely else wakes up the service</span>
<span class="line-removed">-   // thread, which isn&#39;t ideal.  But we can&#39;t notify in release().</span>
<span class="line-removed">-   if (reduce_deferred_updates()) {</span>
<span class="line-removed">-     notify_needs_cleanup();</span>
<span class="line-removed">-   }</span>
  
    Block* block = block_for_allocation();
    if (block == NULL) return NULL; // Block allocation failed.
    assert(!block-&gt;is_full(), &quot;invariant&quot;);
    if (block-&gt;is_empty()) {
      // Transitioning from empty to not empty.
<span class="line-modified">!     log_debug(oopstorage, blocks)(&quot;%s: block not empty &quot; PTR_FORMAT, name(), p2i(block));</span>
    }
    oop* result = block-&gt;allocate();
    assert(result != NULL, &quot;allocation failed&quot;);
    assert(!block-&gt;is_empty(), &quot;postcondition&quot;);
    Atomic::inc(&amp;_allocation_count); // release updates outside lock.
    if (block-&gt;is_full()) {
      // Transitioning from not full to full.
      // Remove full blocks from consideration by future allocates.
<span class="line-modified">!     log_debug(oopstorage, blocks)(&quot;%s: block full &quot; PTR_FORMAT, name(), p2i(block));</span>
      _allocation_list.unlink(*block);
    }
    log_trace(oopstorage, ref)(&quot;%s: allocated &quot; PTR_FORMAT, name(), p2i(result));
    return result;
  }
  
  bool OopStorage::try_add_block() {
    assert_lock_strong(_allocation_mutex);
    Block* block;
    {
<span class="line-modified">!     MutexUnlockerEx ul(_allocation_mutex, Mutex::_no_safepoint_check_flag);</span>
      block = Block::new_block(this);
    }
    if (block == NULL) return false;
  
    // Add new block to the _active_array, growing if needed.
<span class="line-new-header">--- 410,38 ---</span>
  // added to the _allocation_list if not already present and the bitmask is not
  // full.  The block is moved to the end of the _allocation_list if the bitmask
  // is empty, for ease of empty block deletion processing.
  
  oop* OopStorage::allocate() {
<span class="line-modified">!   MutexLocker ml(_allocation_mutex, Mutex::_no_safepoint_check_flag);</span>
  
    Block* block = block_for_allocation();
    if (block == NULL) return NULL; // Block allocation failed.
    assert(!block-&gt;is_full(), &quot;invariant&quot;);
    if (block-&gt;is_empty()) {
      // Transitioning from empty to not empty.
<span class="line-modified">!     log_trace(oopstorage, blocks)(&quot;%s: block not empty &quot; PTR_FORMAT, name(), p2i(block));</span>
    }
    oop* result = block-&gt;allocate();
    assert(result != NULL, &quot;allocation failed&quot;);
    assert(!block-&gt;is_empty(), &quot;postcondition&quot;);
    Atomic::inc(&amp;_allocation_count); // release updates outside lock.
    if (block-&gt;is_full()) {
      // Transitioning from not full to full.
      // Remove full blocks from consideration by future allocates.
<span class="line-modified">!     log_trace(oopstorage, blocks)(&quot;%s: block full &quot; PTR_FORMAT, name(), p2i(block));</span>
      _allocation_list.unlink(*block);
    }
    log_trace(oopstorage, ref)(&quot;%s: allocated &quot; PTR_FORMAT, name(), p2i(result));
    return result;
  }
  
  bool OopStorage::try_add_block() {
    assert_lock_strong(_allocation_mutex);
    Block* block;
    {
<span class="line-modified">!     MutexUnlocker ul(_allocation_mutex, Mutex::_no_safepoint_check_flag);</span>
      block = Block::new_block(this);
    }
    if (block == NULL) return false;
  
    // Add new block to the _active_array, growing if needed.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 472,30 ***</span>
    return true;
  }
  
  OopStorage::Block* OopStorage::block_for_allocation() {
    assert_lock_strong(_allocation_mutex);
<span class="line-removed">- </span>
    while (true) {
      // Use the first block in _allocation_list for the allocation.
      Block* block = _allocation_list.head();
      if (block != NULL) {
        return block;
      } else if (reduce_deferred_updates()) {
<span class="line-modified">!       MutexUnlockerEx ul(_allocation_mutex, Mutex::_no_safepoint_check_flag);</span>
<span class="line-removed">-       notify_needs_cleanup();</span>
      } else if (try_add_block()) {
<span class="line-modified">!       block = _allocation_list.head();</span>
<span class="line-modified">!       assert(block != NULL, &quot;invariant&quot;);</span>
<span class="line-modified">!       return block;</span>
<span class="line-modified">!     } else if (reduce_deferred_updates()) { // Once more before failure.</span>
<span class="line-modified">!       MutexUnlockerEx ul(_allocation_mutex, Mutex::_no_safepoint_check_flag);</span>
<span class="line-modified">!       notify_needs_cleanup();</span>
<span class="line-removed">-     } else {</span>
        // Attempt to add a block failed, no other thread added a block,
        // and no deferred updated added a block, then allocation failed.
<span class="line-modified">!       log_debug(oopstorage, blocks)(&quot;%s: failed block allocation&quot;, name());</span>
        return NULL;
      }
    }
  }
  
<span class="line-new-header">--- 464,27 ---</span>
    return true;
  }
  
  OopStorage::Block* OopStorage::block_for_allocation() {
    assert_lock_strong(_allocation_mutex);
    while (true) {
      // Use the first block in _allocation_list for the allocation.
      Block* block = _allocation_list.head();
      if (block != NULL) {
        return block;
      } else if (reduce_deferred_updates()) {
<span class="line-modified">!       // Might have added a block to the _allocation_list, so retry.</span>
      } else if (try_add_block()) {
<span class="line-modified">!       // Successfully added a new block to the list, so retry.</span>
<span class="line-modified">!       assert(_allocation_list.chead() != NULL, &quot;invariant&quot;);</span>
<span class="line-modified">!     } else if (_allocation_list.chead() != NULL) {</span>
<span class="line-modified">!       // Trying to add a block failed, but some other thread added to the</span>
<span class="line-modified">!       // list while we&#39;d dropped the lock over the new block allocation.</span>
<span class="line-modified">!     } else if (!reduce_deferred_updates()) { // Once more before failure.</span>
        // Attempt to add a block failed, no other thread added a block,
        // and no deferred updated added a block, then allocation failed.
<span class="line-modified">!       log_info(oopstorage, blocks)(&quot;%s: failed block allocation&quot;, name());</span>
        return NULL;
      }
    }
  }
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 524,11 ***</span>
  void OopStorage::replace_active_array(ActiveArray* new_array) {
    // Caller has the old array that is the current value of _active_array.
    // Update new_array refcount to account for the new reference.
    new_array-&gt;increment_refcount();
    // Install new_array, ensuring its initialization is complete first.
<span class="line-modified">!   OrderAccess::release_store(&amp;_active_array, new_array);</span>
    // Wait for any readers that could read the old array from _active_array.
    // Can&#39;t use GlobalCounter here, because this is called from allocate(),
    // which may be called in the scope of a GlobalCounter critical section
    // when inserting a StringTable entry.
    _protect_active.synchronize();
<span class="line-new-header">--- 513,11 ---</span>
  void OopStorage::replace_active_array(ActiveArray* new_array) {
    // Caller has the old array that is the current value of _active_array.
    // Update new_array refcount to account for the new reference.
    new_array-&gt;increment_refcount();
    // Install new_array, ensuring its initialization is complete first.
<span class="line-modified">!   Atomic::release_store(&amp;_active_array, new_array);</span>
    // Wait for any readers that could read the old array from _active_array.
    // Can&#39;t use GlobalCounter here, because this is called from allocate(),
    // which may be called in the scope of a GlobalCounter critical section
    // when inserting a StringTable entry.
    _protect_active.synchronize();
</pre>
<hr />
<pre>
<span class="line-old-header">*** 542,11 ***</span>
  // even if an allocate operation expands and replaces the value of
  // _active_array.  The caller must relinquish the array when done
  // using it.
  OopStorage::ActiveArray* OopStorage::obtain_active_array() const {
    SingleWriterSynchronizer::CriticalSection cs(&amp;_protect_active);
<span class="line-modified">!   ActiveArray* result = OrderAccess::load_acquire(&amp;_active_array);</span>
    result-&gt;increment_refcount();
    return result;
  }
  
  // Decrement refcount of array and destroy if refcount is zero.
<span class="line-new-header">--- 531,11 ---</span>
  // even if an allocate operation expands and replaces the value of
  // _active_array.  The caller must relinquish the array when done
  // using it.
  OopStorage::ActiveArray* OopStorage::obtain_active_array() const {
    SingleWriterSynchronizer::CriticalSection cs(&amp;_protect_active);
<span class="line-modified">!   ActiveArray* result = Atomic::load_acquire(&amp;_active_array);</span>
    result-&gt;increment_refcount();
    return result;
  }
  
  // Decrement refcount of array and destroy if refcount is zero.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 583,17 ***</span>
  
  static void log_release_transitions(uintx releasing,
                                      uintx old_allocated,
                                      const OopStorage* owner,
                                      const void* block) {
<span class="line-modified">!   Log(oopstorage, blocks) log;</span>
<span class="line-modified">!   LogStream ls(log.debug());</span>
<span class="line-modified">!   if (is_full_bitmask(old_allocated)) {</span>
<span class="line-modified">!     ls.print_cr(&quot;%s: block not full &quot; PTR_FORMAT, owner-&gt;name(), p2i(block));</span>
<span class="line-modified">!   }</span>
<span class="line-modified">!   if (releasing == old_allocated) {</span>
<span class="line-modified">!     ls.print_cr(&quot;%s: block empty &quot; PTR_FORMAT, owner-&gt;name(), p2i(block));</span>
    }
  }
  
  void OopStorage::Block::release_entries(uintx releasing, OopStorage* owner) {
    assert(releasing != 0, &quot;preconditon&quot;);
<span class="line-new-header">--- 572,19 ---</span>
  
  static void log_release_transitions(uintx releasing,
                                      uintx old_allocated,
                                      const OopStorage* owner,
                                      const void* block) {
<span class="line-modified">!   LogTarget(Trace, oopstorage, blocks) lt;</span>
<span class="line-modified">!   if (lt.is_enabled()) {</span>
<span class="line-modified">!     LogStream ls(lt);</span>
<span class="line-modified">!     if (is_full_bitmask(old_allocated)) {</span>
<span class="line-modified">!       ls.print_cr(&quot;%s: block not full &quot; PTR_FORMAT, owner-&gt;name(), p2i(block));</span>
<span class="line-modified">!     }</span>
<span class="line-modified">!     if (releasing == old_allocated) {</span>
<span class="line-added">+       ls.print_cr(&quot;%s: block empty &quot; PTR_FORMAT, owner-&gt;name(), p2i(block));</span>
<span class="line-added">+     }</span>
    }
  }
  
  void OopStorage::Block::release_entries(uintx releasing, OopStorage* owner) {
    assert(releasing != 0, &quot;preconditon&quot;);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 603,11 ***</span>
    // Atomically update allocated bitmask.
    uintx old_allocated = _allocated_bitmask;
    while (true) {
      assert((releasing &amp; ~old_allocated) == 0, &quot;releasing unallocated entries&quot;);
      uintx new_value = old_allocated ^ releasing;
<span class="line-modified">!     uintx fetched = Atomic::cmpxchg(new_value, &amp;_allocated_bitmask, old_allocated);</span>
      if (fetched == old_allocated) break; // Successful update.
      old_allocated = fetched;             // Retry with updated bitmask.
    }
  
    // Now that the bitmask has been updated, if we have a state transition
<span class="line-new-header">--- 594,11 ---</span>
    // Atomically update allocated bitmask.
    uintx old_allocated = _allocated_bitmask;
    while (true) {
      assert((releasing &amp; ~old_allocated) == 0, &quot;releasing unallocated entries&quot;);
      uintx new_value = old_allocated ^ releasing;
<span class="line-modified">!     uintx fetched = Atomic::cmpxchg(&amp;_allocated_bitmask, old_allocated, new_value);</span>
      if (fetched == old_allocated) break; // Successful update.
      old_allocated = fetched;             // Retry with updated bitmask.
    }
  
    // Now that the bitmask has been updated, if we have a state transition
</pre>
<hr />
<pre>
<span class="line-old-header">*** 616,30 ***</span>
    // reduce_deferred_updates will make any needed changes related to this
    // block and _allocation_list.  This deferral avoids _allocation_list
    // updates and the associated locking here.
    if ((releasing == old_allocated) || is_full_bitmask(old_allocated)) {
      // Log transitions.  Both transitions are possible in a single update.
<span class="line-modified">!     if (log_is_enabled(Debug, oopstorage, blocks)) {</span>
<span class="line-removed">-       log_release_transitions(releasing, old_allocated, _owner, this);</span>
<span class="line-removed">-     }</span>
      // Attempt to claim responsibility for adding this block to the deferred
      // list, by setting the link to non-NULL by self-looping.  If this fails,
      // then someone else has made such a claim and the deferred update has not
      // yet been processed and will include our change, so we don&#39;t need to do
      // anything further.
<span class="line-modified">!     if (Atomic::replace_if_null(this, &amp;_deferred_updates_next)) {</span>
        // Successfully claimed.  Push, with self-loop for end-of-list.
        Block* head = owner-&gt;_deferred_updates;
        while (true) {
          _deferred_updates_next = (head == NULL) ? this : head;
<span class="line-modified">!         Block* fetched = Atomic::cmpxchg(this, &amp;owner-&gt;_deferred_updates, head);</span>
          if (fetched == head) break; // Successful update.
          head = fetched;             // Retry with updated head.
        }
<span class="line-modified">!       owner-&gt;record_needs_cleanup();</span>
<span class="line-modified">!       log_debug(oopstorage, blocks)(&quot;%s: deferred update &quot; PTR_FORMAT,</span>
<span class="line-modified">!                                     _owner-&gt;name(), p2i(this));</span>
      }
    }
    // Release hold on empty block deletion.
    Atomic::dec(&amp;_release_refcount);
  }
<span class="line-new-header">--- 607,35 ---</span>
    // reduce_deferred_updates will make any needed changes related to this
    // block and _allocation_list.  This deferral avoids _allocation_list
    // updates and the associated locking here.
    if ((releasing == old_allocated) || is_full_bitmask(old_allocated)) {
      // Log transitions.  Both transitions are possible in a single update.
<span class="line-modified">!     log_release_transitions(releasing, old_allocated, owner, this);</span>
      // Attempt to claim responsibility for adding this block to the deferred
      // list, by setting the link to non-NULL by self-looping.  If this fails,
      // then someone else has made such a claim and the deferred update has not
      // yet been processed and will include our change, so we don&#39;t need to do
      // anything further.
<span class="line-modified">!     if (Atomic::replace_if_null(&amp;_deferred_updates_next, this)) {</span>
        // Successfully claimed.  Push, with self-loop for end-of-list.
        Block* head = owner-&gt;_deferred_updates;
        while (true) {
          _deferred_updates_next = (head == NULL) ? this : head;
<span class="line-modified">!         Block* fetched = Atomic::cmpxchg(&amp;owner-&gt;_deferred_updates, head, this);</span>
          if (fetched == head) break; // Successful update.
          head = fetched;             // Retry with updated head.
        }
<span class="line-modified">!       // Only request cleanup for to-empty transitions, not for from-full.</span>
<span class="line-modified">!       // There isn&#39;t any rush to process from-full transitions.  Allocation</span>
<span class="line-modified">!       // will reduce deferrals before allocating new blocks, so may process</span>
<span class="line-added">+       // some.  And the service thread will drain the entire deferred list</span>
<span class="line-added">+       // if there are any pending to-empty transitions.</span>
<span class="line-added">+       if (releasing == old_allocated) {</span>
<span class="line-added">+         owner-&gt;record_needs_cleanup();</span>
<span class="line-added">+       }</span>
<span class="line-added">+       log_trace(oopstorage, blocks)(&quot;%s: deferred update &quot; PTR_FORMAT,</span>
<span class="line-added">+                                     owner-&gt;name(), p2i(this));</span>
      }
    }
    // Release hold on empty block deletion.
    Atomic::dec(&amp;_release_refcount);
  }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 648,17 ***</span>
  bool OopStorage::reduce_deferred_updates() {
    assert_lock_strong(_allocation_mutex);
    // Atomically pop a block off the list, if any available.
    // No ABA issue because this is only called by one thread at a time.
    // The atomicity is wrto pushes by release().
<span class="line-modified">!   Block* block = OrderAccess::load_acquire(&amp;_deferred_updates);</span>
    while (true) {
      if (block == NULL) return false;
      // Try atomic pop of block from list.
      Block* tail = block-&gt;deferred_updates_next();
      if (block == tail) tail = NULL; // Handle self-loop end marker.
<span class="line-modified">!     Block* fetched = Atomic::cmpxchg(tail, &amp;_deferred_updates, block);</span>
      if (fetched == block) break; // Update successful.
      block = fetched;             // Retry with updated block.
    }
    block-&gt;set_deferred_updates_next(NULL); // Clear tail after updating head.
    // Ensure bitmask read after pop is complete, including clearing tail, for
<span class="line-new-header">--- 644,17 ---</span>
  bool OopStorage::reduce_deferred_updates() {
    assert_lock_strong(_allocation_mutex);
    // Atomically pop a block off the list, if any available.
    // No ABA issue because this is only called by one thread at a time.
    // The atomicity is wrto pushes by release().
<span class="line-modified">!   Block* block = Atomic::load_acquire(&amp;_deferred_updates);</span>
    while (true) {
      if (block == NULL) return false;
      // Try atomic pop of block from list.
      Block* tail = block-&gt;deferred_updates_next();
      if (block == tail) tail = NULL; // Handle self-loop end marker.
<span class="line-modified">!     Block* fetched = Atomic::cmpxchg(&amp;_deferred_updates, block, tail);</span>
      if (fetched == block) break; // Update successful.
      block = fetched;             // Retry with updated block.
    }
    block-&gt;set_deferred_updates_next(NULL); // Clear tail after updating head.
    // Ensure bitmask read after pop is complete, including clearing tail, for
</pre>
<hr />
<pre>
<span class="line-old-header">*** 682,14 ***</span>
  
    // Move empty block to end of list, for possible deletion.
    if (is_empty_bitmask(allocated)) {
      _allocation_list.unlink(*block);
      _allocation_list.push_back(*block);
<span class="line-removed">-     notify_needs_cleanup();</span>
    }
  
<span class="line-modified">!   log_debug(oopstorage, blocks)(&quot;%s: processed deferred update &quot; PTR_FORMAT,</span>
                                  name(), p2i(block));
    return true;              // Processed one pending update.
  }
  
  inline void check_release_entry(const oop* entry) {
<span class="line-new-header">--- 678,13 ---</span>
  
    // Move empty block to end of list, for possible deletion.
    if (is_empty_bitmask(allocated)) {
      _allocation_list.unlink(*block);
      _allocation_list.push_back(*block);
    }
  
<span class="line-modified">!   log_trace(oopstorage, blocks)(&quot;%s: processed deferred update &quot; PTR_FORMAT,</span>
                                  name(), p2i(block));
    return true;              // Processed one pending update.
  }
  
  inline void check_release_entry(const oop* entry) {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 728,49 ***</span>
        releasing |= entry_bitmask;
        ++count;
      }
      // Release the contiguous entries that are in block.
      block-&gt;release_entries(releasing, this);
<span class="line-modified">!     Atomic::sub(count, &amp;_allocation_count);</span>
    }
  }
  
<span class="line-removed">- const char* dup_name(const char* name) {</span>
<span class="line-removed">-   char* dup = NEW_C_HEAP_ARRAY(char, strlen(name) + 1, mtGC);</span>
<span class="line-removed">-   strcpy(dup, name);</span>
<span class="line-removed">-   return dup;</span>
<span class="line-removed">- }</span>
<span class="line-removed">- </span>
<span class="line-removed">- // Possible values for OopStorage::_needs_cleanup.</span>
<span class="line-removed">- const uint needs_cleanup_none = 0;     // No cleanup needed.</span>
<span class="line-removed">- const uint needs_cleanup_marked = 1;   // Requested, but no notification made.</span>
<span class="line-removed">- const uint needs_cleanup_notified = 2; // Requested and Service thread notified.</span>
<span class="line-removed">- </span>
  const size_t initial_active_array_size = 8;
  
  OopStorage::OopStorage(const char* name,
                         Mutex* allocation_mutex,
                         Mutex* active_mutex) :
<span class="line-modified">!   _name(dup_name(name)),</span>
    _active_array(ActiveArray::create(initial_active_array_size)),
    _allocation_list(),
    _deferred_updates(NULL),
    _allocation_mutex(allocation_mutex),
    _active_mutex(active_mutex),
    _allocation_count(0),
    _concurrent_iteration_count(0),
<span class="line-modified">!   _needs_cleanup(needs_cleanup_none)</span>
  {
    _active_array-&gt;increment_refcount();
    assert(_active_mutex-&gt;rank() &lt; _allocation_mutex-&gt;rank(),
           &quot;%s: active_mutex must have lower rank than allocation_mutex&quot;, _name);
    assert(Service_lock-&gt;rank() &lt; _active_mutex-&gt;rank(),
           &quot;%s: active_mutex must have higher rank than Service_lock&quot;, _name);
<span class="line-modified">!   assert(_active_mutex-&gt;_safepoint_check_required != Mutex::_safepoint_check_always,</span>
<span class="line-modified">!          &quot;%s: active mutex requires safepoint check&quot;, _name);</span>
<span class="line-modified">!   assert(_allocation_mutex-&gt;_safepoint_check_required != Mutex::_safepoint_check_always,</span>
<span class="line-modified">!          &quot;%s: allocation mutex requires safepoint check&quot;, _name);</span>
  }
  
  void OopStorage::delete_empty_block(const Block&amp; block) {
    assert(block.is_empty(), &quot;discarding non-empty block&quot;);
    log_debug(oopstorage, blocks)(&quot;%s: delete empty block &quot; PTR_FORMAT, name(), p2i(&amp;block));
<span class="line-new-header">--- 723,38 ---</span>
        releasing |= entry_bitmask;
        ++count;
      }
      // Release the contiguous entries that are in block.
      block-&gt;release_entries(releasing, this);
<span class="line-modified">!     Atomic::sub(&amp;_allocation_count, count);</span>
    }
  }
  
  const size_t initial_active_array_size = 8;
  
  OopStorage::OopStorage(const char* name,
                         Mutex* allocation_mutex,
                         Mutex* active_mutex) :
<span class="line-modified">!   _name(os::strdup(name)),</span>
    _active_array(ActiveArray::create(initial_active_array_size)),
    _allocation_list(),
    _deferred_updates(NULL),
    _allocation_mutex(allocation_mutex),
    _active_mutex(active_mutex),
    _allocation_count(0),
    _concurrent_iteration_count(0),
<span class="line-modified">!   _needs_cleanup(false)</span>
  {
    _active_array-&gt;increment_refcount();
    assert(_active_mutex-&gt;rank() &lt; _allocation_mutex-&gt;rank(),
           &quot;%s: active_mutex must have lower rank than allocation_mutex&quot;, _name);
    assert(Service_lock-&gt;rank() &lt; _active_mutex-&gt;rank(),
           &quot;%s: active_mutex must have higher rank than Service_lock&quot;, _name);
<span class="line-modified">!   assert(_active_mutex-&gt;_safepoint_check_required == Mutex::_safepoint_check_never,</span>
<span class="line-modified">!          &quot;%s: active mutex requires never safepoint check&quot;, _name);</span>
<span class="line-modified">!   assert(_allocation_mutex-&gt;_safepoint_check_required == Mutex::_safepoint_check_never,</span>
<span class="line-modified">!          &quot;%s: allocation mutex requires never safepoint check&quot;, _name);</span>
  }
  
  void OopStorage::delete_empty_block(const Block&amp; block) {
    assert(block.is_empty(), &quot;discarding non-empty block&quot;);
    log_debug(oopstorage, blocks)(&quot;%s: delete empty block &quot; PTR_FORMAT, name(), p2i(&amp;block));
</pre>
<hr />
<pre>
<span class="line-old-header">*** 791,55 ***</span>
    for (size_t i = _active_array-&gt;block_count(); 0 &lt; i; ) {
      block = _active_array-&gt;at(--i);
      Block::delete_block(*block);
    }
    ActiveArray::destroy(_active_array);
<span class="line-modified">!   FREE_C_HEAP_ARRAY(char, _name);</span>
  }
  
<span class="line-modified">! // Called by service thread to check for pending work.</span>
<span class="line-modified">! bool OopStorage::needs_delete_empty_blocks() const {</span>
<span class="line-modified">!   return Atomic::load(&amp;_needs_cleanup) != needs_cleanup_none;</span>
  }
  
  // Record that cleanup is needed, without notifying the Service thread.
  // Used by release(), where we can&#39;t lock even Service_lock.
  void OopStorage::record_needs_cleanup() {
<span class="line-modified">!   Atomic::cmpxchg(needs_cleanup_marked, &amp;_needs_cleanup, needs_cleanup_none);</span>
  }
  
<span class="line-modified">! // Record that cleanup is needed, and notify the Service thread.</span>
<span class="line-modified">! void OopStorage::notify_needs_cleanup() {</span>
<span class="line-modified">!   // Avoid re-notification if already notified.</span>
<span class="line-modified">!   const uint notified = needs_cleanup_notified;</span>
<span class="line-modified">!   if (Atomic::xchg(notified, &amp;_needs_cleanup) != notified) {</span>
<span class="line-modified">!     MonitorLockerEx ml(Service_lock, Monitor::_no_safepoint_check_flag);</span>
<span class="line-modified">!     ml.notify_all();</span>
    }
<span class="line-removed">- }</span>
  
<span class="line-modified">! bool OopStorage::delete_empty_blocks() {</span>
<span class="line-removed">-   MutexLockerEx ml(_allocation_mutex, Mutex::_no_safepoint_check_flag);</span>
  
    // Clear the request before processing.
<span class="line-modified">!   Atomic::store(needs_cleanup_none, &amp;_needs_cleanup);</span>
<span class="line-removed">-   OrderAccess::fence();</span>
  
    // Other threads could be adding to the empty block count or the
    // deferred update list while we&#39;re working.  Set an upper bound on
    // how many updates we&#39;ll process and blocks we&#39;ll try to release,
    // so other threads can&#39;t cause an unbounded stay in this function.
<span class="line-modified">!   size_t limit = block_count();</span>
<span class="line-modified">!   if (limit == 0) return false; // Empty storage; nothing at all to do.</span>
  
    for (size_t i = 0; i &lt; limit; ++i) {
      // Process deferred updates, which might make empty blocks available.
      // Continue checking once deletion starts, since additional updates
      // might become available while we&#39;re working.
      if (reduce_deferred_updates()) {
        // Be safepoint-polite while looping.
<span class="line-modified">!       MutexUnlockerEx ul(_allocation_mutex, Mutex::_no_safepoint_check_flag);</span>
        ThreadBlockInVM tbiv(JavaThread::current());
      } else {
        Block* block = _allocation_list.tail();
        if ((block == NULL) || !block-&gt;is_empty()) {
          return false;
<span class="line-new-header">--- 775,104 ---</span>
    for (size_t i = _active_array-&gt;block_count(); 0 &lt; i; ) {
      block = _active_array-&gt;at(--i);
      Block::delete_block(*block);
    }
    ActiveArray::destroy(_active_array);
<span class="line-modified">!   os::free(const_cast&lt;char*&gt;(_name));</span>
  }
  
<span class="line-modified">! // Managing service thread notifications.</span>
<span class="line-modified">! //</span>
<span class="line-modified">! // We don&#39;t want cleanup work to linger indefinitely, but we also don&#39;t want</span>
<span class="line-added">+ // to run the service thread too often.  We&#39;re also very limited in what we</span>
<span class="line-added">+ // can do in a release operation, where cleanup work is created.</span>
<span class="line-added">+ //</span>
<span class="line-added">+ // When a release operation changes a block&#39;s state to empty, it records the</span>
<span class="line-added">+ // need for cleanup in both the associated storage object and in the global</span>
<span class="line-added">+ // request state.  A safepoint cleanup task notifies the service thread when</span>
<span class="line-added">+ // there may be cleanup work for any storage object, based on the global</span>
<span class="line-added">+ // request state.  But that notification is deferred if the service thread</span>
<span class="line-added">+ // has run recently, and we also avoid duplicate notifications.  The service</span>
<span class="line-added">+ // thread updates the timestamp and resets the state flags on every iteration.</span>
<span class="line-added">+ </span>
<span class="line-added">+ // Global cleanup request state.</span>
<span class="line-added">+ static volatile bool needs_cleanup_requested = false;</span>
<span class="line-added">+ </span>
<span class="line-added">+ // Flag for avoiding duplicate notifications.</span>
<span class="line-added">+ static bool needs_cleanup_triggered = false;</span>
<span class="line-added">+ </span>
<span class="line-added">+ // Time after which a notification can be made.</span>
<span class="line-added">+ static jlong cleanup_trigger_permit_time = 0;</span>
<span class="line-added">+ </span>
<span class="line-added">+ // Minimum time since last service thread check before notification is</span>
<span class="line-added">+ // permitted.  The value of 500ms was an arbitrary choice; frequent, but not</span>
<span class="line-added">+ // too frequent.</span>
<span class="line-added">+ const jlong cleanup_trigger_defer_period = 500 * NANOSECS_PER_MILLISEC;</span>
<span class="line-added">+ </span>
<span class="line-added">+ void OopStorage::trigger_cleanup_if_needed() {</span>
<span class="line-added">+   MonitorLocker ml(Service_lock, Monitor::_no_safepoint_check_flag);</span>
<span class="line-added">+   if (Atomic::load(&amp;needs_cleanup_requested) &amp;&amp;</span>
<span class="line-added">+       !needs_cleanup_triggered &amp;&amp;</span>
<span class="line-added">+       (os::javaTimeNanos() &gt; cleanup_trigger_permit_time)) {</span>
<span class="line-added">+     needs_cleanup_triggered = true;</span>
<span class="line-added">+     ml.notify_all();</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ bool OopStorage::has_cleanup_work_and_reset() {</span>
<span class="line-added">+   assert_lock_strong(Service_lock);</span>
<span class="line-added">+   cleanup_trigger_permit_time =</span>
<span class="line-added">+     os::javaTimeNanos() + cleanup_trigger_defer_period;</span>
<span class="line-added">+   needs_cleanup_triggered = false;</span>
<span class="line-added">+   // Set the request flag false and return its old value.</span>
<span class="line-added">+   // Needs to be atomic to avoid dropping a concurrent request.</span>
<span class="line-added">+   // Can&#39;t use Atomic::xchg, which may not support bool.</span>
<span class="line-added">+   return Atomic::cmpxchg(&amp;needs_cleanup_requested, true, false);</span>
  }
  
  // Record that cleanup is needed, without notifying the Service thread.
  // Used by release(), where we can&#39;t lock even Service_lock.
  void OopStorage::record_needs_cleanup() {
<span class="line-modified">!   // Set local flag first, else service thread could wake up and miss</span>
<span class="line-added">+   // the request.  This order may instead (rarely) unnecessarily notify.</span>
<span class="line-added">+   Atomic::release_store(&amp;_needs_cleanup, true);</span>
<span class="line-added">+   Atomic::release_store_fence(&amp;needs_cleanup_requested, true);</span>
  }
  
<span class="line-modified">! bool OopStorage::delete_empty_blocks() {</span>
<span class="line-modified">!   // Service thread might have oopstorage work, but not for this object.</span>
<span class="line-modified">!   // Check for deferred updates even though that&#39;s not a service thread</span>
<span class="line-modified">!   // trigger; since we&#39;re here, we might as well process them.</span>
<span class="line-modified">!   if (!Atomic::load_acquire(&amp;_needs_cleanup) &amp;&amp;</span>
<span class="line-modified">!       (Atomic::load_acquire(&amp;_deferred_updates) == NULL)) {</span>
<span class="line-modified">!     return false;</span>
    }
  
<span class="line-modified">!   MutexLocker ml(_allocation_mutex, Mutex::_no_safepoint_check_flag);</span>
  
    // Clear the request before processing.
<span class="line-modified">!   Atomic::release_store_fence(&amp;_needs_cleanup, false);</span>
  
    // Other threads could be adding to the empty block count or the
    // deferred update list while we&#39;re working.  Set an upper bound on
    // how many updates we&#39;ll process and blocks we&#39;ll try to release,
    // so other threads can&#39;t cause an unbounded stay in this function.
<span class="line-modified">!   // We add a bit of slop because the reduce_deferred_updates clause</span>
<span class="line-modified">!   // can cause blocks to be double counted.  If there are few blocks</span>
<span class="line-added">+   // and many of them are deferred and empty, we might hit the limit</span>
<span class="line-added">+   // and spin the caller without doing very much work.  Otherwise,</span>
<span class="line-added">+   // we don&#39;t normally hit the limit anyway, instead running out of</span>
<span class="line-added">+   // work to do.</span>
<span class="line-added">+   size_t limit = block_count() + 10;</span>
  
    for (size_t i = 0; i &lt; limit; ++i) {
      // Process deferred updates, which might make empty blocks available.
      // Continue checking once deletion starts, since additional updates
      // might become available while we&#39;re working.
      if (reduce_deferred_updates()) {
        // Be safepoint-polite while looping.
<span class="line-modified">!       MutexUnlocker ul(_allocation_mutex, Mutex::_no_safepoint_check_flag);</span>
        ThreadBlockInVM tbiv(JavaThread::current());
      } else {
        Block* block = _allocation_list.tail();
        if ((block == NULL) || !block-&gt;is_empty()) {
          return false;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 848,22 ***</span>
          break;
        }
  
        // Try to delete the block.  First, try to remove from _active_array.
        {
<span class="line-modified">!         MutexLockerEx aml(_active_mutex, Mutex::_no_safepoint_check_flag);</span>
          // Don&#39;t interfere with an active concurrent iteration.
          // Instead, give up immediately.  There is more work to do,
          // but don&#39;t re-notify, to avoid useless spinning of the
          // service thread.  Instead, iteration completion notifies.
          if (_concurrent_iteration_count &gt; 0) return true;
          _active_array-&gt;remove(block);
        }
        // Remove block from _allocation_list and delete it.
        _allocation_list.unlink(*block);
        // Be safepoint-polite while deleting and looping.
<span class="line-modified">!       MutexUnlockerEx ul(_allocation_mutex, Mutex::_no_safepoint_check_flag);</span>
        delete_empty_block(*block);
        ThreadBlockInVM tbiv(JavaThread::current());
      }
    }
    // Exceeded work limit or can&#39;t delete last block.  This will
<span class="line-new-header">--- 881,22 ---</span>
          break;
        }
  
        // Try to delete the block.  First, try to remove from _active_array.
        {
<span class="line-modified">!         MutexLocker aml(_active_mutex, Mutex::_no_safepoint_check_flag);</span>
          // Don&#39;t interfere with an active concurrent iteration.
          // Instead, give up immediately.  There is more work to do,
          // but don&#39;t re-notify, to avoid useless spinning of the
          // service thread.  Instead, iteration completion notifies.
          if (_concurrent_iteration_count &gt; 0) return true;
          _active_array-&gt;remove(block);
        }
        // Remove block from _allocation_list and delete it.
        _allocation_list.unlink(*block);
        // Be safepoint-polite while deleting and looping.
<span class="line-modified">!       MutexUnlocker ul(_allocation_mutex, Mutex::_no_safepoint_check_flag);</span>
        delete_empty_block(*block);
        ThreadBlockInVM tbiv(JavaThread::current());
      }
    }
    // Exceeded work limit or can&#39;t delete last block.  This will
</pre>
<hr />
<pre>
<span class="line-old-header">*** 876,11 ***</span>
  
  OopStorage::EntryStatus OopStorage::allocation_status(const oop* ptr) const {
    const Block* block = find_block_or_null(ptr);
    if (block != NULL) {
      // Prevent block deletion and _active_array modification.
<span class="line-modified">!     MutexLockerEx ml(_allocation_mutex, Mutex::_no_safepoint_check_flag);</span>
      // Block could be a false positive, so get index carefully.
      size_t index = Block::active_index_safe(block);
      if ((index &lt; _active_array-&gt;block_count()) &amp;&amp;
          (block == _active_array-&gt;at(index)) &amp;&amp;
          block-&gt;contains(ptr)) {
<span class="line-new-header">--- 909,11 ---</span>
  
  OopStorage::EntryStatus OopStorage::allocation_status(const oop* ptr) const {
    const Block* block = find_block_or_null(ptr);
    if (block != NULL) {
      // Prevent block deletion and _active_array modification.
<span class="line-modified">!     MutexLocker ml(_allocation_mutex, Mutex::_no_safepoint_check_flag);</span>
      // Block could be a false positive, so get index carefully.
      size_t index = Block::active_index_safe(block);
      if ((index &lt; _active_array-&gt;block_count()) &amp;&amp;
          (block == _active_array-&gt;at(index)) &amp;&amp;
          block-&gt;contains(ptr)) {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 944,26 ***</span>
  
  OopStorage::BasicParState::~BasicParState() {
    _storage-&gt;relinquish_block_array(_active_array);
    update_concurrent_iteration_count(-1);
    if (_concurrent) {
<span class="line-modified">!     // We may have deferred some work.</span>
<span class="line-modified">!     const_cast&lt;OopStorage*&gt;(_storage)-&gt;notify_needs_cleanup();</span>
    }
  }
  
  void OopStorage::BasicParState::update_concurrent_iteration_count(int value) {
    if (_concurrent) {
<span class="line-modified">!     MutexLockerEx ml(_storage-&gt;_active_mutex, Mutex::_no_safepoint_check_flag);</span>
      _storage-&gt;_concurrent_iteration_count += value;
      assert(_storage-&gt;_concurrent_iteration_count &gt;= 0, &quot;invariant&quot;);
    }
  }
  
  bool OopStorage::BasicParState::claim_next_segment(IterationData* data) {
    data-&gt;_processed += data-&gt;_segment_end - data-&gt;_segment_start;
<span class="line-modified">!   size_t start = OrderAccess::load_acquire(&amp;_next_block);</span>
    if (start &gt;= _block_count) {
      return finish_iteration(data); // No more blocks available.
    }
    // Try to claim several at a time, but not *too* many.  We want to
    // avoid deciding there are many available and selecting a large
<span class="line-new-header">--- 977,26 ---</span>
  
  OopStorage::BasicParState::~BasicParState() {
    _storage-&gt;relinquish_block_array(_active_array);
    update_concurrent_iteration_count(-1);
    if (_concurrent) {
<span class="line-modified">!     // We may have deferred some cleanup work.</span>
<span class="line-modified">!     const_cast&lt;OopStorage*&gt;(_storage)-&gt;record_needs_cleanup();</span>
    }
  }
  
  void OopStorage::BasicParState::update_concurrent_iteration_count(int value) {
    if (_concurrent) {
<span class="line-modified">!     MutexLocker ml(_storage-&gt;_active_mutex, Mutex::_no_safepoint_check_flag);</span>
      _storage-&gt;_concurrent_iteration_count += value;
      assert(_storage-&gt;_concurrent_iteration_count &gt;= 0, &quot;invariant&quot;);
    }
  }
  
  bool OopStorage::BasicParState::claim_next_segment(IterationData* data) {
    data-&gt;_processed += data-&gt;_segment_end - data-&gt;_segment_start;
<span class="line-modified">!   size_t start = Atomic::load_acquire(&amp;_next_block);</span>
    if (start &gt;= _block_count) {
      return finish_iteration(data); // No more blocks available.
    }
    // Try to claim several at a time, but not *too* many.  We want to
    // avoid deciding there are many available and selecting a large
</pre>
<hr />
<pre>
<span class="line-old-header">*** 976,11 ***</span>
    size_t step = MIN2(max_step, 1 + (remaining / _estimated_thread_count));
    // Atomic::add with possible overshoot.  This can perform better
    // than a CAS loop on some platforms when there is contention.
    // We can cope with the uncertainty by recomputing start/end from
    // the result of the add, and dealing with potential overshoot.
<span class="line-modified">!   size_t end = Atomic::add(step, &amp;_next_block);</span>
    // _next_block may have changed, so recompute start from result of add.
    start = end - step;
    // _next_block may have changed so much that end has overshot.
    end = MIN2(end, _block_count);
    // _next_block may have changed so much that even start has overshot.
<span class="line-new-header">--- 1009,11 ---</span>
    size_t step = MIN2(max_step, 1 + (remaining / _estimated_thread_count));
    // Atomic::add with possible overshoot.  This can perform better
    // than a CAS loop on some platforms when there is contention.
    // We can cope with the uncertainty by recomputing start/end from
    // the result of the add, and dealing with potential overshoot.
<span class="line-modified">!   size_t end = Atomic::add(&amp;_next_block, step);</span>
    // _next_block may have changed, so recompute start from result of add.
    start = end - step;
    // _next_block may have changed so much that end has overshot.
    end = MIN2(end, _block_count);
    // _next_block may have changed so much that even start has overshot.
</pre>
<center><a href="modRefBarrierSet.inline.hpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="oopStorage.hpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>