<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/shared/ptrQueue.hpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="ptrQueue.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="referenceProcessor.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/shared/ptrQueue.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2001, 2019, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_GC_SHARED_PTRQUEUE_HPP
 26 #define SHARE_GC_SHARED_PTRQUEUE_HPP
 27 
 28 #include &quot;memory/padded.hpp&quot;
 29 #include &quot;utilities/align.hpp&quot;
 30 #include &quot;utilities/debug.hpp&quot;

 31 #include &quot;utilities/lockFreeStack.hpp&quot;
 32 #include &quot;utilities/sizes.hpp&quot;
 33 
 34 class Mutex;
 35 class Monitor;
 36 
 37 // There are various techniques that require threads to be able to log
 38 // addresses.  For example, a generational write barrier might log
 39 // the addresses of modified old-generation objects.  This type supports
 40 // this operation.
 41 
 42 class BufferNode;
 43 class PtrQueueSet;
 44 class PtrQueue {
 45   friend class VMStructs;
 46 
<span class="line-modified"> 47   // Noncopyable - not defined.</span>
<span class="line-removed"> 48   PtrQueue(const PtrQueue&amp;);</span>
<span class="line-removed"> 49   PtrQueue&amp; operator=(const PtrQueue&amp;);</span>
 50 
 51   // The ptr queue set to which this queue belongs.
 52   PtrQueueSet* const _qset;
 53 
 54   // Whether updates should be logged.
 55   bool _active;
 56 
<span class="line-removed"> 57   // If true, the queue is permanent, and doesn&#39;t need to deallocate</span>
<span class="line-removed"> 58   // its buffer in the destructor (since that obtains a lock which may not</span>
<span class="line-removed"> 59   // be legally locked by then.</span>
<span class="line-removed"> 60   const bool _permanent;</span>
<span class="line-removed"> 61 </span>
 62   // The (byte) index at which an object was last enqueued.  Starts at
 63   // capacity_in_bytes (indicating an empty buffer) and goes towards zero.
 64   // Value is always pointer-size aligned.
 65   size_t _index;
 66 
 67   // Size of the current buffer, in bytes.
 68   // Value is always pointer-size aligned.
 69   size_t _capacity_in_bytes;
 70 
 71   static const size_t _element_size = sizeof(void*);
 72 
 73   // Get the capacity, in bytes.  The capacity must have been set.
 74   size_t capacity_in_bytes() const {
 75     assert(_capacity_in_bytes &gt; 0, &quot;capacity not set&quot;);
 76     return _capacity_in_bytes;
 77   }
 78 
<span class="line-removed"> 79   void set_capacity(size_t entries) {</span>
<span class="line-removed"> 80     size_t byte_capacity = index_to_byte_index(entries);</span>
<span class="line-removed"> 81     assert(_capacity_in_bytes == 0 || _capacity_in_bytes == byte_capacity,</span>
<span class="line-removed"> 82            &quot;changing capacity &quot; SIZE_FORMAT &quot; -&gt; &quot; SIZE_FORMAT,</span>
<span class="line-removed"> 83            _capacity_in_bytes, byte_capacity);</span>
<span class="line-removed"> 84     _capacity_in_bytes = byte_capacity;</span>
<span class="line-removed"> 85   }</span>
<span class="line-removed"> 86 </span>
 87   static size_t byte_index_to_index(size_t ind) {
 88     assert(is_aligned(ind, _element_size), &quot;precondition&quot;);
 89     return ind / _element_size;
 90   }
 91 
 92   static size_t index_to_byte_index(size_t ind) {
 93     return ind * _element_size;
 94   }
 95 
 96 protected:
 97   // The buffer.
 98   void** _buf;
 99 
100   size_t index() const {
101     return byte_index_to_index(_index);
102   }
103 
104   void set_index(size_t new_index) {
105     size_t byte_index = index_to_byte_index(new_index);
106     assert(byte_index &lt;= capacity_in_bytes(), &quot;precondition&quot;);
107     _index = byte_index;
108   }
109 
110   size_t capacity() const {
111     return byte_index_to_index(capacity_in_bytes());
112   }
113 
<span class="line-modified">114   // If there is a lock associated with this buffer, this is that lock.</span>
<span class="line-removed">115   Mutex* _lock;</span>
<span class="line-removed">116 </span>
<span class="line-removed">117   PtrQueueSet* qset() { return _qset; }</span>
<span class="line-removed">118   bool is_permanent() const { return _permanent; }</span>
119 
120   // Process queue entries and release resources.
121   void flush_impl();
122 









123   // Initialize this queue to contain a null buffer, and be part of the
124   // given PtrQueueSet.
<span class="line-modified">125   PtrQueue(PtrQueueSet* qset, bool permanent = false, bool active = false);</span>
126 
<span class="line-modified">127   // Requires queue flushed or permanent.</span>
128   ~PtrQueue();
129 
130 public:
131 
<span class="line-removed">132   // Associate a lock with a ptr queue.</span>
<span class="line-removed">133   void set_lock(Mutex* lock) { _lock = lock; }</span>
<span class="line-removed">134 </span>
135   // Forcibly set empty.
136   void reset() {
137     if (_buf != NULL) {
138       _index = capacity_in_bytes();
139     }
140   }
141 
142   void enqueue(volatile void* ptr) {
143     enqueue((void*)(ptr));
144   }
145 
146   // Enqueues the given &quot;obj&quot;.
147   void enqueue(void* ptr) {
148     if (!_active) return;
149     else enqueue_known_active(ptr);
150   }
151 
<span class="line-removed">152   // This method is called when we&#39;re doing the zero index handling</span>
<span class="line-removed">153   // and gives a chance to the queues to do any pre-enqueueing</span>
<span class="line-removed">154   // processing they might want to do on the buffer. It should return</span>
<span class="line-removed">155   // true if the buffer should be enqueued, or false if enough</span>
<span class="line-removed">156   // entries were cleared from it so that it can be re-used. It should</span>
<span class="line-removed">157   // not return false if the buffer is still full (otherwise we can</span>
<span class="line-removed">158   // get into an infinite loop).</span>
<span class="line-removed">159   virtual bool should_enqueue_buffer() { return true; }</span>
160   void handle_zero_index();
161 
162   void enqueue_known_active(void* ptr);
163 
164   // Return the size of the in-use region.
165   size_t size() const {
166     size_t result = 0;
167     if (_buf != NULL) {
168       assert(_index &lt;= capacity_in_bytes(), &quot;Invariant&quot;);
169       result = byte_index_to_index(capacity_in_bytes() - _index);
170     }
171     return result;
172   }
173 
174   bool is_empty() const {
175     return _buf == NULL || capacity_in_bytes() == _index;
176   }
177 
178   // Set the &quot;active&quot; property of the queue to &quot;b&quot;.  An enqueue to an
179   // inactive thread is a no-op.  Setting a queue to inactive resets its
</pre>
<hr />
<pre>
207 
208   static ByteSize byte_width_of_buf() { return in_ByteSize(_element_size); }
209 
210   template&lt;typename Derived&gt;
211   static ByteSize byte_offset_of_active() {
212     return byte_offset_of(Derived, _active);
213   }
214 
215   static ByteSize byte_width_of_active() { return in_ByteSize(sizeof(bool)); }
216 
217 };
218 
219 class BufferNode {
220   size_t _index;
221   BufferNode* volatile _next;
222   void* _buffer[1];             // Pseudo flexible array member.
223 
224   BufferNode() : _index(0), _next(NULL) { }
225   ~BufferNode() { }
226 


227   static size_t buffer_offset() {
228     return offset_of(BufferNode, _buffer);
229   }
230 
<span class="line-removed">231   static BufferNode* volatile* next_ptr(BufferNode&amp; bn) { return &amp;bn._next; }</span>
<span class="line-removed">232 </span>
<span class="line-removed">233 AIX_ONLY(public:)               // xlC 12 on AIX doesn&#39;t implement C++ DR45.</span>
234   // Allocate a new BufferNode with the &quot;buffer&quot; having size elements.
235   static BufferNode* allocate(size_t size);
236 
237   // Free a BufferNode.
238   static void deallocate(BufferNode* node);
239 
240 public:

241   typedef LockFreeStack&lt;BufferNode, &amp;next_ptr&gt; Stack;
242 
243   BufferNode* next() const     { return _next;  }
244   void set_next(BufferNode* n) { _next = n;     }
245   size_t index() const         { return _index; }
246   void set_index(size_t i)     { _index = i; }
247 
248   // Return the BufferNode containing the buffer, after setting its index.
249   static BufferNode* make_node_from_buffer(void** buffer, size_t index) {
250     BufferNode* node =
251       reinterpret_cast&lt;BufferNode*&gt;(
252         reinterpret_cast&lt;char*&gt;(buffer) - buffer_offset());
253     node-&gt;set_index(index);
254     return node;
255   }
256 
257   // Return the buffer for node.
258   static void** make_buffer_from_node(BufferNode *node) {
259     // &amp;_buffer[0] might lead to index out of bounds warnings.
260     return reinterpret_cast&lt;void**&gt;(
</pre>
<hr />
<pre>
276   friend class TestSupport;
277 
278   // Since we don&#39;t expect many instances, and measured &gt;15% speedup
279   // on stress gtest, padding seems like a good tradeoff here.
280 #define DECLARE_PADDED_MEMBER(Id, Type, Name) \
281   Type Name; DEFINE_PAD_MINUS_SIZE(Id, DEFAULT_CACHE_LINE_SIZE, sizeof(Type))
282 
283   const size_t _buffer_size;
284   char _name[DEFAULT_CACHE_LINE_SIZE - sizeof(size_t)]; // Use name as padding.
285   DECLARE_PADDED_MEMBER(1, Stack, _pending_list);
286   DECLARE_PADDED_MEMBER(2, Stack, _free_list);
287   DECLARE_PADDED_MEMBER(3, volatile size_t, _pending_count);
288   DECLARE_PADDED_MEMBER(4, volatile size_t, _free_count);
289   DECLARE_PADDED_MEMBER(5, volatile bool, _transfer_lock);
290 
291 #undef DECLARE_PADDED_MEMBER
292 
293   void delete_list(BufferNode* list);
294   bool try_transfer_pending();
295 


296 public:
297   Allocator(const char* name, size_t buffer_size);
298   ~Allocator();
299 
300   const char* name() const { return _name; }
301   size_t buffer_size() const { return _buffer_size; }
302   size_t free_count() const;
303   BufferNode* allocate();
304   void release(BufferNode* node);
305 
306   // Deallocate some of the available buffers.  remove_goal is the target
307   // number to remove.  Returns the number actually deallocated, which may
308   // be less than the goal if there were fewer available.
309   size_t reduce_free_list(size_t remove_goal);
310 };
311 
312 // A PtrQueueSet represents resources common to a set of pointer queues.
313 // In particular, the individual queues allocate buffers from this shared
314 // set, and return completed buffers to the set.
315 class PtrQueueSet {
316   BufferNode::Allocator* _allocator;
317 
<span class="line-modified">318   Monitor* _cbl_mon;  // Protects the fields below.</span>
<span class="line-removed">319   BufferNode* _completed_buffers_head;</span>
<span class="line-removed">320   BufferNode* _completed_buffers_tail;</span>
<span class="line-removed">321   size_t _n_completed_buffers;</span>
<span class="line-removed">322 </span>
<span class="line-removed">323   size_t _process_completed_buffers_threshold;</span>
<span class="line-removed">324   volatile bool _process_completed_buffers;</span>
<span class="line-removed">325 </span>
<span class="line-removed">326   // If true, notify_all on _cbl_mon when the threshold is reached.</span>
<span class="line-removed">327   bool _notify_when_complete;</span>
<span class="line-removed">328 </span>
<span class="line-removed">329   // Maximum number of elements allowed on completed queue: after that,</span>
<span class="line-removed">330   // enqueuer does the work itself.</span>
<span class="line-removed">331   size_t _max_completed_buffers;</span>
<span class="line-removed">332   size_t _completed_buffers_padding;</span>
<span class="line-removed">333 </span>
<span class="line-removed">334   void assert_completed_buffers_list_len_correct_locked() NOT_DEBUG_RETURN;</span>
335 
336 protected:
337   bool _all_active;
338 
<span class="line-removed">339   // A mutator thread does the the work of processing a buffer.</span>
<span class="line-removed">340   // Returns &quot;true&quot; iff the work is complete (and the buffer may be</span>
<span class="line-removed">341   // deallocated).</span>
<span class="line-removed">342   virtual bool mut_process_buffer(BufferNode* node) {</span>
<span class="line-removed">343     ShouldNotReachHere();</span>
<span class="line-removed">344     return false;</span>
<span class="line-removed">345   }</span>
<span class="line-removed">346 </span>
347   // Create an empty ptr queue set.
<span class="line-modified">348   PtrQueueSet(bool notify_when_complete = false);</span>
349   ~PtrQueueSet();
350 
<span class="line-removed">351   // Because of init-order concerns, we can&#39;t pass these as constructor</span>
<span class="line-removed">352   // arguments.</span>
<span class="line-removed">353   void initialize(Monitor* cbl_mon, BufferNode::Allocator* allocator);</span>
<span class="line-removed">354 </span>
<span class="line-removed">355   // For (unlocked!) iteration over the completed buffers.</span>
<span class="line-removed">356   BufferNode* completed_buffers_head() const { return _completed_buffers_head; }</span>
<span class="line-removed">357 </span>
<span class="line-removed">358   // Deallocate all of the completed buffers.</span>
<span class="line-removed">359   void abandon_completed_buffers();</span>
<span class="line-removed">360 </span>
361 public:
362 



363   // Return the buffer for a BufferNode of size buffer_size().
364   void** allocate_buffer();
365 
366   // Return an empty buffer to the free list.  The node is required
367   // to have been allocated with a size of buffer_size().
368   void deallocate_buffer(BufferNode* node);
369 
370   // A completed buffer is a buffer the mutator is finished with, and
371   // is ready to be processed by the collector.  It need not be full.
372 
373   // Adds node to the completed buffer list.
<span class="line-modified">374   void enqueue_completed_buffer(BufferNode* node);</span>
<span class="line-removed">375 </span>
<span class="line-removed">376   // If the number of completed buffers is &gt; stop_at, then remove and</span>
<span class="line-removed">377   // return a completed buffer from the list.  Otherwise, return NULL.</span>
<span class="line-removed">378   BufferNode* get_completed_buffer(size_t stop_at = 0);</span>
<span class="line-removed">379 </span>
<span class="line-removed">380   // To be invoked by the mutator.</span>
<span class="line-removed">381   bool process_or_enqueue_completed_buffer(BufferNode* node);</span>
<span class="line-removed">382 </span>
<span class="line-removed">383   bool process_completed_buffers() { return _process_completed_buffers; }</span>
<span class="line-removed">384   void set_process_completed_buffers(bool x) { _process_completed_buffers = x; }</span>
385 
386   bool is_active() { return _all_active; }
387 
388   size_t buffer_size() const {
389     return _allocator-&gt;buffer_size();
390   }
<span class="line-removed">391 </span>
<span class="line-removed">392   // Get/Set the number of completed buffers that triggers log processing.</span>
<span class="line-removed">393   // Log processing should be done when the number of buffers exceeds the</span>
<span class="line-removed">394   // threshold.</span>
<span class="line-removed">395   void set_process_completed_buffers_threshold(size_t sz) {</span>
<span class="line-removed">396     _process_completed_buffers_threshold = sz;</span>
<span class="line-removed">397   }</span>
<span class="line-removed">398   size_t process_completed_buffers_threshold() const {</span>
<span class="line-removed">399     return _process_completed_buffers_threshold;</span>
<span class="line-removed">400   }</span>
<span class="line-removed">401   static const size_t ProcessCompletedBuffersThresholdNever = ~size_t(0);</span>
<span class="line-removed">402 </span>
<span class="line-removed">403   size_t completed_buffers_num() const { return _n_completed_buffers; }</span>
<span class="line-removed">404 </span>
<span class="line-removed">405   void merge_bufferlists(PtrQueueSet* src);</span>
<span class="line-removed">406 </span>
<span class="line-removed">407   void set_max_completed_buffers(size_t m) {</span>
<span class="line-removed">408     _max_completed_buffers = m;</span>
<span class="line-removed">409   }</span>
<span class="line-removed">410   size_t max_completed_buffers() const {</span>
<span class="line-removed">411     return _max_completed_buffers;</span>
<span class="line-removed">412   }</span>
<span class="line-removed">413   static const size_t MaxCompletedBuffersUnlimited = ~size_t(0);</span>
<span class="line-removed">414 </span>
<span class="line-removed">415   void set_completed_buffers_padding(size_t padding) {</span>
<span class="line-removed">416     _completed_buffers_padding = padding;</span>
<span class="line-removed">417   }</span>
<span class="line-removed">418   size_t completed_buffers_padding() const {</span>
<span class="line-removed">419     return _completed_buffers_padding;</span>
<span class="line-removed">420   }</span>
<span class="line-removed">421 </span>
<span class="line-removed">422   // Notify the consumer if the number of buffers crossed the threshold</span>
<span class="line-removed">423   void notify_if_necessary();</span>
424 };
425 
426 #endif // SHARE_GC_SHARED_PTRQUEUE_HPP
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2001, 2020, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_GC_SHARED_PTRQUEUE_HPP
 26 #define SHARE_GC_SHARED_PTRQUEUE_HPP
 27 
 28 #include &quot;memory/padded.hpp&quot;
 29 #include &quot;utilities/align.hpp&quot;
 30 #include &quot;utilities/debug.hpp&quot;
<span class="line-added"> 31 #include &quot;utilities/globalDefinitions.hpp&quot;</span>
 32 #include &quot;utilities/lockFreeStack.hpp&quot;
 33 #include &quot;utilities/sizes.hpp&quot;
 34 
 35 class Mutex;
 36 class Monitor;
 37 
 38 // There are various techniques that require threads to be able to log
 39 // addresses.  For example, a generational write barrier might log
 40 // the addresses of modified old-generation objects.  This type supports
 41 // this operation.
 42 
 43 class BufferNode;
 44 class PtrQueueSet;
 45 class PtrQueue {
 46   friend class VMStructs;
 47 
<span class="line-modified"> 48   NONCOPYABLE(PtrQueue);</span>


 49 
 50   // The ptr queue set to which this queue belongs.
 51   PtrQueueSet* const _qset;
 52 
 53   // Whether updates should be logged.
 54   bool _active;
 55 





 56   // The (byte) index at which an object was last enqueued.  Starts at
 57   // capacity_in_bytes (indicating an empty buffer) and goes towards zero.
 58   // Value is always pointer-size aligned.
 59   size_t _index;
 60 
 61   // Size of the current buffer, in bytes.
 62   // Value is always pointer-size aligned.
 63   size_t _capacity_in_bytes;
 64 
 65   static const size_t _element_size = sizeof(void*);
 66 
 67   // Get the capacity, in bytes.  The capacity must have been set.
 68   size_t capacity_in_bytes() const {
 69     assert(_capacity_in_bytes &gt; 0, &quot;capacity not set&quot;);
 70     return _capacity_in_bytes;
 71   }
 72 








 73   static size_t byte_index_to_index(size_t ind) {
 74     assert(is_aligned(ind, _element_size), &quot;precondition&quot;);
 75     return ind / _element_size;
 76   }
 77 
 78   static size_t index_to_byte_index(size_t ind) {
 79     return ind * _element_size;
 80   }
 81 
 82 protected:
 83   // The buffer.
 84   void** _buf;
 85 
 86   size_t index() const {
 87     return byte_index_to_index(_index);
 88   }
 89 
 90   void set_index(size_t new_index) {
 91     size_t byte_index = index_to_byte_index(new_index);
 92     assert(byte_index &lt;= capacity_in_bytes(), &quot;precondition&quot;);
 93     _index = byte_index;
 94   }
 95 
 96   size_t capacity() const {
 97     return byte_index_to_index(capacity_in_bytes());
 98   }
 99 
<span class="line-modified">100   PtrQueueSet* qset() const { return _qset; }</span>




101 
102   // Process queue entries and release resources.
103   void flush_impl();
104 
<span class="line-added">105   // Process (some of) the buffer and leave it in place for further use,</span>
<span class="line-added">106   // or enqueue the buffer and allocate a new one.</span>
<span class="line-added">107   virtual void handle_completed_buffer() = 0;</span>
<span class="line-added">108 </span>
<span class="line-added">109   void allocate_buffer();</span>
<span class="line-added">110 </span>
<span class="line-added">111   // Enqueue the current buffer in the qset and allocate a new buffer.</span>
<span class="line-added">112   void enqueue_completed_buffer();</span>
<span class="line-added">113 </span>
114   // Initialize this queue to contain a null buffer, and be part of the
115   // given PtrQueueSet.
<span class="line-modified">116   PtrQueue(PtrQueueSet* qset, bool active = false);</span>
117 
<span class="line-modified">118   // Requires queue flushed.</span>
119   ~PtrQueue();
120 
121 public:
122 



123   // Forcibly set empty.
124   void reset() {
125     if (_buf != NULL) {
126       _index = capacity_in_bytes();
127     }
128   }
129 
130   void enqueue(volatile void* ptr) {
131     enqueue((void*)(ptr));
132   }
133 
134   // Enqueues the given &quot;obj&quot;.
135   void enqueue(void* ptr) {
136     if (!_active) return;
137     else enqueue_known_active(ptr);
138   }
139 








140   void handle_zero_index();
141 
142   void enqueue_known_active(void* ptr);
143 
144   // Return the size of the in-use region.
145   size_t size() const {
146     size_t result = 0;
147     if (_buf != NULL) {
148       assert(_index &lt;= capacity_in_bytes(), &quot;Invariant&quot;);
149       result = byte_index_to_index(capacity_in_bytes() - _index);
150     }
151     return result;
152   }
153 
154   bool is_empty() const {
155     return _buf == NULL || capacity_in_bytes() == _index;
156   }
157 
158   // Set the &quot;active&quot; property of the queue to &quot;b&quot;.  An enqueue to an
159   // inactive thread is a no-op.  Setting a queue to inactive resets its
</pre>
<hr />
<pre>
187 
188   static ByteSize byte_width_of_buf() { return in_ByteSize(_element_size); }
189 
190   template&lt;typename Derived&gt;
191   static ByteSize byte_offset_of_active() {
192     return byte_offset_of(Derived, _active);
193   }
194 
195   static ByteSize byte_width_of_active() { return in_ByteSize(sizeof(bool)); }
196 
197 };
198 
199 class BufferNode {
200   size_t _index;
201   BufferNode* volatile _next;
202   void* _buffer[1];             // Pseudo flexible array member.
203 
204   BufferNode() : _index(0), _next(NULL) { }
205   ~BufferNode() { }
206 
<span class="line-added">207   NONCOPYABLE(BufferNode);</span>
<span class="line-added">208 </span>
209   static size_t buffer_offset() {
210     return offset_of(BufferNode, _buffer);
211   }
212 



213   // Allocate a new BufferNode with the &quot;buffer&quot; having size elements.
214   static BufferNode* allocate(size_t size);
215 
216   // Free a BufferNode.
217   static void deallocate(BufferNode* node);
218 
219 public:
<span class="line-added">220   static BufferNode* volatile* next_ptr(BufferNode&amp; bn) { return &amp;bn._next; }</span>
221   typedef LockFreeStack&lt;BufferNode, &amp;next_ptr&gt; Stack;
222 
223   BufferNode* next() const     { return _next;  }
224   void set_next(BufferNode* n) { _next = n;     }
225   size_t index() const         { return _index; }
226   void set_index(size_t i)     { _index = i; }
227 
228   // Return the BufferNode containing the buffer, after setting its index.
229   static BufferNode* make_node_from_buffer(void** buffer, size_t index) {
230     BufferNode* node =
231       reinterpret_cast&lt;BufferNode*&gt;(
232         reinterpret_cast&lt;char*&gt;(buffer) - buffer_offset());
233     node-&gt;set_index(index);
234     return node;
235   }
236 
237   // Return the buffer for node.
238   static void** make_buffer_from_node(BufferNode *node) {
239     // &amp;_buffer[0] might lead to index out of bounds warnings.
240     return reinterpret_cast&lt;void**&gt;(
</pre>
<hr />
<pre>
256   friend class TestSupport;
257 
258   // Since we don&#39;t expect many instances, and measured &gt;15% speedup
259   // on stress gtest, padding seems like a good tradeoff here.
260 #define DECLARE_PADDED_MEMBER(Id, Type, Name) \
261   Type Name; DEFINE_PAD_MINUS_SIZE(Id, DEFAULT_CACHE_LINE_SIZE, sizeof(Type))
262 
263   const size_t _buffer_size;
264   char _name[DEFAULT_CACHE_LINE_SIZE - sizeof(size_t)]; // Use name as padding.
265   DECLARE_PADDED_MEMBER(1, Stack, _pending_list);
266   DECLARE_PADDED_MEMBER(2, Stack, _free_list);
267   DECLARE_PADDED_MEMBER(3, volatile size_t, _pending_count);
268   DECLARE_PADDED_MEMBER(4, volatile size_t, _free_count);
269   DECLARE_PADDED_MEMBER(5, volatile bool, _transfer_lock);
270 
271 #undef DECLARE_PADDED_MEMBER
272 
273   void delete_list(BufferNode* list);
274   bool try_transfer_pending();
275 
<span class="line-added">276   NONCOPYABLE(Allocator);</span>
<span class="line-added">277 </span>
278 public:
279   Allocator(const char* name, size_t buffer_size);
280   ~Allocator();
281 
282   const char* name() const { return _name; }
283   size_t buffer_size() const { return _buffer_size; }
284   size_t free_count() const;
285   BufferNode* allocate();
286   void release(BufferNode* node);
287 
288   // Deallocate some of the available buffers.  remove_goal is the target
289   // number to remove.  Returns the number actually deallocated, which may
290   // be less than the goal if there were fewer available.
291   size_t reduce_free_list(size_t remove_goal);
292 };
293 
294 // A PtrQueueSet represents resources common to a set of pointer queues.
295 // In particular, the individual queues allocate buffers from this shared
296 // set, and return completed buffers to the set.
297 class PtrQueueSet {
298   BufferNode::Allocator* _allocator;
299 
<span class="line-modified">300   NONCOPYABLE(PtrQueueSet);</span>
















301 
302 protected:
303   bool _all_active;
304 








305   // Create an empty ptr queue set.
<span class="line-modified">306   PtrQueueSet(BufferNode::Allocator* allocator);</span>
307   ~PtrQueueSet();
308 










309 public:
310 
<span class="line-added">311   // Return the associated BufferNode allocator.</span>
<span class="line-added">312   BufferNode::Allocator* allocator() const { return _allocator; }</span>
<span class="line-added">313 </span>
314   // Return the buffer for a BufferNode of size buffer_size().
315   void** allocate_buffer();
316 
317   // Return an empty buffer to the free list.  The node is required
318   // to have been allocated with a size of buffer_size().
319   void deallocate_buffer(BufferNode* node);
320 
321   // A completed buffer is a buffer the mutator is finished with, and
322   // is ready to be processed by the collector.  It need not be full.
323 
324   // Adds node to the completed buffer list.
<span class="line-modified">325   virtual void enqueue_completed_buffer(BufferNode* node) = 0;</span>










326 
327   bool is_active() { return _all_active; }
328 
329   size_t buffer_size() const {
330     return _allocator-&gt;buffer_size();
331   }

































332 };
333 
334 #endif // SHARE_GC_SHARED_PTRQUEUE_HPP
</pre>
</td>
</tr>
</table>
<center><a href="ptrQueue.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="referenceProcessor.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>