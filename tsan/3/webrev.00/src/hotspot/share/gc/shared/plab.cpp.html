<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/share/gc/shared/plab.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2001, 2018, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/shared/collectedHeap.hpp&quot;
 27 #include &quot;gc/shared/plab.inline.hpp&quot;
 28 #include &quot;gc/shared/threadLocalAllocBuffer.hpp&quot;
 29 #include &quot;logging/log.hpp&quot;
 30 #include &quot;memory/universe.hpp&quot;
 31 #include &quot;oops/oop.inline.hpp&quot;
 32 
 33 size_t PLAB::min_size() {
 34   // Make sure that we return something that is larger than AlignmentReserve
 35   return align_object_size(MAX2(MinTLABSize / HeapWordSize, (size_t)oopDesc::header_size())) + AlignmentReserve;
 36 }
 37 
 38 size_t PLAB::max_size() {
 39   return ThreadLocalAllocBuffer::max_size();
 40 }
 41 
 42 PLAB::PLAB(size_t desired_plab_sz_) :
 43   _word_sz(desired_plab_sz_), _bottom(NULL), _top(NULL),
 44   _end(NULL), _hard_end(NULL), _allocated(0), _wasted(0), _undo_wasted(0)
 45 {
 46   AlignmentReserve = Universe::heap()-&gt;tlab_alloc_reserve();
 47   assert(min_size() &gt; AlignmentReserve,
 48          &quot;Minimum PLAB size &quot; SIZE_FORMAT &quot; must be larger than alignment reserve &quot; SIZE_FORMAT &quot; &quot;
 49          &quot;to be able to contain objects&quot;, min_size(), AlignmentReserve);
 50 }
 51 
 52 // If the minimum object size is greater than MinObjAlignment, we can
 53 // end up with a shard at the end of the buffer that&#39;s smaller than
 54 // the smallest object.  We can&#39;t allow that because the buffer must
 55 // look like it&#39;s full of objects when we retire it, so we make
 56 // sure we have enough space for a filler int array object.
 57 size_t PLAB::AlignmentReserve;
 58 
 59 void PLAB::flush_and_retire_stats(PLABStats* stats) {
 60   // Retire the last allocation buffer.
 61   size_t unused = retire_internal();
 62 
 63   // Now flush the statistics.
 64   stats-&gt;add_allocated(_allocated);
 65   stats-&gt;add_wasted(_wasted);
 66   stats-&gt;add_undo_wasted(_undo_wasted);
 67   stats-&gt;add_unused(unused);
 68 
 69   // Since we have flushed the stats we need to clear  the _allocated and _wasted
 70   // fields in case somebody retains an instance of this over GCs. Not doing so
 71   // will artifically inflate the values in the statistics.
 72   _allocated   = 0;
 73   _wasted      = 0;
 74   _undo_wasted = 0;
 75 }
 76 
 77 void PLAB::retire() {
 78   _wasted += retire_internal();
 79 }
 80 
 81 size_t PLAB::retire_internal() {
 82   size_t result = 0;
 83   if (_top &lt; _hard_end) {
 84     Universe::heap()-&gt;fill_with_dummy_object(_top, _hard_end, true);
 85     result += invalidate();
 86   }
 87   return result;
 88 }
 89 
 90 void PLAB::add_undo_waste(HeapWord* obj, size_t word_sz) {
 91   Universe::heap()-&gt;fill_with_dummy_object(obj, obj + word_sz, true);
 92   _undo_wasted += word_sz;
 93 }
 94 
 95 void PLAB::undo_last_allocation(HeapWord* obj, size_t word_sz) {
 96   assert(pointer_delta(_top, _bottom) &gt;= word_sz, &quot;Bad undo&quot;);
 97   assert(pointer_delta(_top, obj) == word_sz, &quot;Bad undo&quot;);
 98   _top = obj;
 99 }
100 
101 void PLAB::undo_allocation(HeapWord* obj, size_t word_sz) {
102   // Is the alloc in the current alloc buffer?
103   if (contains(obj)) {
104     assert(contains(obj + word_sz - 1),
105       &quot;should contain whole object&quot;);
106     undo_last_allocation(obj, word_sz);
107   } else {
108     add_undo_waste(obj, word_sz);
109   }
110 }
111 
112 void PLABStats::log_plab_allocation() {
113   log_debug(gc, plab)(&quot;%s PLAB allocation: &quot;
114                       &quot;allocated: &quot; SIZE_FORMAT &quot;B, &quot;
115                       &quot;wasted: &quot; SIZE_FORMAT &quot;B, &quot;
116                       &quot;unused: &quot; SIZE_FORMAT &quot;B, &quot;
117                       &quot;used: &quot; SIZE_FORMAT &quot;B, &quot;
118                       &quot;undo waste: &quot; SIZE_FORMAT &quot;B, &quot;,
119                       _description,
120                       _allocated * HeapWordSize,
121                       _wasted * HeapWordSize,
122                       _unused * HeapWordSize,
123                       used() * HeapWordSize,
124                       _undo_wasted * HeapWordSize);
125 }
126 
127 void PLABStats::log_sizing(size_t calculated_words, size_t net_desired_words) {
128   log_debug(gc, plab)(&quot;%s sizing: &quot;
129                       &quot;calculated: &quot; SIZE_FORMAT &quot;B, &quot;
130                       &quot;actual: &quot; SIZE_FORMAT &quot;B&quot;,
131                       _description,
132                       calculated_words * HeapWordSize,
133                       net_desired_words * HeapWordSize);
134 }
135 
136 // Calculates plab size for current number of gc worker threads.
137 size_t PLABStats::desired_plab_sz(uint no_of_gc_workers) {
138   return align_object_size(clamp(_desired_net_plab_sz / no_of_gc_workers, min_size(), max_size()));
139 }
140 
141 // Compute desired plab size for one gc worker thread and latch result for later
142 // use. This should be called once at the end of parallel
143 // scavenge; it clears the sensor accumulators.
144 void PLABStats::adjust_desired_plab_sz() {
145   log_plab_allocation();
146 
147   if (!ResizePLAB) {
148     // Clear accumulators for next round.
149     reset();
150     return;
151   }
152 
153   assert(is_object_aligned(max_size()) &amp;&amp; min_size() &lt;= max_size(),
154          &quot;PLAB clipping computation may be incorrect&quot;);
155 
156   assert(_allocated != 0 || _unused == 0,
157          &quot;Inconsistency in PLAB stats: &quot;
158          &quot;_allocated: &quot; SIZE_FORMAT &quot;, &quot;
159          &quot;_wasted: &quot; SIZE_FORMAT &quot;, &quot;
160          &quot;_unused: &quot; SIZE_FORMAT &quot;, &quot;
161          &quot;_undo_wasted: &quot; SIZE_FORMAT,
162          _allocated, _wasted, _unused, _undo_wasted);
163 
164   size_t plab_sz = compute_desired_plab_sz();
165   // Take historical weighted average
166   _filter.sample(plab_sz);
167   _desired_net_plab_sz = MAX2(min_size(), (size_t)_filter.average());
168 
169   log_sizing(plab_sz, _desired_net_plab_sz);
170   // Clear accumulators for next round
171   reset();
172 }
173 
174 size_t PLABStats::compute_desired_plab_sz() {
175   size_t allocated      = MAX2(_allocated, size_t(1));
176   double wasted_frac    = (double)_unused / (double)allocated;
177   size_t target_refills = (size_t)((wasted_frac * TargetSurvivorRatio) / TargetPLABWastePct);
178   if (target_refills == 0) {
179     target_refills = 1;
180   }
181   size_t used = allocated - _wasted - _unused;
182   // Assumed to have 1 gc worker thread
183   size_t recent_plab_sz = used / target_refills;
184   return recent_plab_sz;
185 }
    </pre>
  </body>
</html>