<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/shared/space.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="scavengableNMethods.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="space.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/shared/space.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;classfile/systemDictionary.hpp&quot;
 27 #include &quot;classfile/vmSymbols.hpp&quot;
 28 #include &quot;gc/shared/blockOffsetTable.inline.hpp&quot;
 29 #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
 30 #include &quot;gc/shared/genCollectedHeap.hpp&quot;
 31 #include &quot;gc/shared/genOopClosures.inline.hpp&quot;
 32 #include &quot;gc/shared/space.hpp&quot;
 33 #include &quot;gc/shared/space.inline.hpp&quot;
<span class="line-modified"> 34 #include &quot;gc/shared/spaceDecorator.hpp&quot;</span>
 35 #include &quot;memory/iterator.inline.hpp&quot;
 36 #include &quot;memory/universe.hpp&quot;
 37 #include &quot;oops/oop.inline.hpp&quot;
 38 #include &quot;runtime/atomic.hpp&quot;
 39 #include &quot;runtime/java.hpp&quot;
<span class="line-removed"> 40 #include &quot;runtime/orderAccess.hpp&quot;</span>
 41 #include &quot;runtime/prefetch.inline.hpp&quot;
 42 #include &quot;runtime/safepoint.hpp&quot;
 43 #include &quot;utilities/align.hpp&quot;
 44 #include &quot;utilities/copy.hpp&quot;
 45 #include &quot;utilities/globalDefinitions.hpp&quot;
 46 #include &quot;utilities/macros.hpp&quot;
 47 #if INCLUDE_SERIALGC
 48 #include &quot;gc/serial/defNewGeneration.hpp&quot;
 49 #endif
 50 
 51 HeapWord* DirtyCardToOopClosure::get_actual_top(HeapWord* top,
 52                                                 HeapWord* top_obj) {
 53   if (top_obj != NULL) {
 54     if (_sp-&gt;block_is_obj(top_obj)) {
 55       if (_precision == CardTable::ObjHeadPreciseArray) {
 56         if (oop(top_obj)-&gt;is_objArray() || oop(top_obj)-&gt;is_typeArray()) {
 57           // An arrayOop is starting on the dirty card - since we do exact
 58           // store checks for objArrays we are done.
 59         } else {
 60           // Otherwise, it is possible that the object starting on the dirty
</pre>
<hr />
<pre>
 93     // current pointer; unfortunately, that won&#39;t work in CMS because
 94     // we&#39;d need an interface change (it seems) to have the space
 95     // &quot;adjust the object size&quot; (for instance pad it up to its
 96     // block alignment or minimum block size restrictions. XXX
 97     if (_sp-&gt;block_is_obj(bottom) &amp;&amp;
 98         !_sp-&gt;obj_allocated_since_save_marks(oop(bottom))) {
 99       oop(bottom)-&gt;oop_iterate(_cl, mr);
100     }
101   }
102 }
103 
104 // We get called with &quot;mr&quot; representing the dirty region
105 // that we want to process. Because of imprecise marking,
106 // we may need to extend the incoming &quot;mr&quot; to the right,
107 // and scan more. However, because we may already have
108 // scanned some of that extended region, we may need to
109 // trim its right-end back some so we do not scan what
110 // we (or another worker thread) may already have scanned
111 // or planning to scan.
112 void DirtyCardToOopClosure::do_MemRegion(MemRegion mr) {
<span class="line-removed">113 </span>
<span class="line-removed">114   // Some collectors need to do special things whenever their dirty</span>
<span class="line-removed">115   // cards are processed. For instance, CMS must remember mutator updates</span>
<span class="line-removed">116   // (i.e. dirty cards) so as to re-scan mutated objects.</span>
<span class="line-removed">117   // Such work can be piggy-backed here on dirty card scanning, so as to make</span>
<span class="line-removed">118   // it slightly more efficient than doing a complete non-destructive pre-scan</span>
<span class="line-removed">119   // of the card table.</span>
<span class="line-removed">120   MemRegionClosure* pCl = _sp-&gt;preconsumptionDirtyCardClosure();</span>
<span class="line-removed">121   if (pCl != NULL) {</span>
<span class="line-removed">122     pCl-&gt;do_MemRegion(mr);</span>
<span class="line-removed">123   }</span>
<span class="line-removed">124 </span>
125   HeapWord* bottom = mr.start();
126   HeapWord* last = mr.last();
127   HeapWord* top = mr.end();
128   HeapWord* bottom_obj;
129   HeapWord* top_obj;
130 
131   assert(_precision == CardTable::ObjHeadPreciseArray ||
132          _precision == CardTable::Precise,
133          &quot;Only ones we deal with for now.&quot;);
134 
135   assert(_precision != CardTable::ObjHeadPreciseArray ||
136          _last_bottom == NULL || top &lt;= _last_bottom,
137          &quot;Not decreasing&quot;);
138   NOT_PRODUCT(_last_bottom = mr.start());
139 
140   bottom_obj = _sp-&gt;block_start(bottom);
141   top_obj    = _sp-&gt;block_start(last);
142 
143   assert(bottom_obj &lt;= bottom, &quot;just checking&quot;);
144   assert(top_obj    &lt;= top,    &quot;just checking&quot;);
</pre>
<hr />
<pre>
369   // First check if we should switch compaction space
370   assert(this == cp-&gt;space, &quot;&#39;this&#39; should be current compaction space.&quot;);
371   size_t compaction_max_size = pointer_delta(end(), compact_top);
372   while (size &gt; compaction_max_size) {
373     // switch to next compaction space
374     cp-&gt;space-&gt;set_compaction_top(compact_top);
375     cp-&gt;space = cp-&gt;space-&gt;next_compaction_space();
376     if (cp-&gt;space == NULL) {
377       cp-&gt;gen = GenCollectedHeap::heap()-&gt;young_gen();
378       assert(cp-&gt;gen != NULL, &quot;compaction must succeed&quot;);
379       cp-&gt;space = cp-&gt;gen-&gt;first_compaction_space();
380       assert(cp-&gt;space != NULL, &quot;generation must have a first compaction space&quot;);
381     }
382     compact_top = cp-&gt;space-&gt;bottom();
383     cp-&gt;space-&gt;set_compaction_top(compact_top);
384     cp-&gt;threshold = cp-&gt;space-&gt;initialize_threshold();
385     compaction_max_size = pointer_delta(cp-&gt;space-&gt;end(), compact_top);
386   }
387 
388   // store the forwarding pointer into the mark word
<span class="line-modified">389   if ((HeapWord*)q != compact_top) {</span>
390     q-&gt;forward_to(oop(compact_top));
391     assert(q-&gt;is_gc_marked(), &quot;encoding the pointer should preserve the mark&quot;);
392   } else {
393     // if the object isn&#39;t moving we can just set the mark to the default
394     // mark and handle it specially later on.
395     q-&gt;init_mark_raw();
396     assert(q-&gt;forwardee() == NULL, &quot;should be forwarded to NULL&quot;);
397   }
398 
399   compact_top += size;
400 
401   // we need to update the offset table so that the beginnings of objects can be
402   // found during scavenge.  Note that we are updating the offset table based on
403   // where the object will be once the compaction phase finishes.
404   if (compact_top &gt; cp-&gt;threshold)
405     cp-&gt;threshold =
406       cp-&gt;space-&gt;cross_threshold(compact_top - size, compact_top);
407   return compact_top;
408 }
409 
</pre>
<hr />
<pre>
481 bool Space::obj_is_alive(const HeapWord* p) const {
482   assert (block_is_obj(p), &quot;The address should point to an object&quot;);
483   return true;
484 }
485 
486 void ContiguousSpace::oop_iterate(OopIterateClosure* blk) {
487   if (is_empty()) return;
488   HeapWord* obj_addr = bottom();
489   HeapWord* t = top();
490   // Could call objects iterate, but this is easier.
491   while (obj_addr &lt; t) {
492     obj_addr += oop(obj_addr)-&gt;oop_iterate_size(blk);
493   }
494 }
495 
496 void ContiguousSpace::object_iterate(ObjectClosure* blk) {
497   if (is_empty()) return;
498   object_iterate_from(bottom(), blk);
499 }
500 
<span class="line-removed">501 // For a ContiguousSpace object_iterate() and safe_object_iterate()</span>
<span class="line-removed">502 // are the same.</span>
<span class="line-removed">503 void ContiguousSpace::safe_object_iterate(ObjectClosure* blk) {</span>
<span class="line-removed">504   object_iterate(blk);</span>
<span class="line-removed">505 }</span>
<span class="line-removed">506 </span>
507 void ContiguousSpace::object_iterate_from(HeapWord* mark, ObjectClosure* blk) {
508   while (mark &lt; top()) {
509     blk-&gt;do_object(oop(mark));
510     mark += oop(mark)-&gt;size();
511   }
512 }
513 
<span class="line-removed">514 HeapWord*</span>
<span class="line-removed">515 ContiguousSpace::object_iterate_careful(ObjectClosureCareful* blk) {</span>
<span class="line-removed">516   HeapWord * limit = concurrent_iteration_safe_limit();</span>
<span class="line-removed">517   assert(limit &lt;= top(), &quot;sanity check&quot;);</span>
<span class="line-removed">518   for (HeapWord* p = bottom(); p &lt; limit;) {</span>
<span class="line-removed">519     size_t size = blk-&gt;do_object_careful(oop(p));</span>
<span class="line-removed">520     if (size == 0) {</span>
<span class="line-removed">521       return p;  // failed at p</span>
<span class="line-removed">522     } else {</span>
<span class="line-removed">523       p += size;</span>
<span class="line-removed">524     }</span>
<span class="line-removed">525   }</span>
<span class="line-removed">526   return NULL; // all done</span>
<span class="line-removed">527 }</span>
<span class="line-removed">528 </span>
529 // Very general, slow implementation.
530 HeapWord* ContiguousSpace::block_start_const(const void* p) const {
531   assert(MemRegion(bottom(), end()).contains(p),
532          &quot;p (&quot; PTR_FORMAT &quot;) not in space [&quot; PTR_FORMAT &quot;, &quot; PTR_FORMAT &quot;)&quot;,
533          p2i(p), p2i(bottom()), p2i(end()));
534   if (p &gt;= top()) {
535     return top();
536   } else {
537     HeapWord* last = bottom();
538     HeapWord* cur = last;
539     while (cur &lt;= p) {
540       last = cur;
541       cur += oop(cur)-&gt;size();
542     }
543     assert(oopDesc::is_oop(oop(last)), PTR_FORMAT &quot; should be an object start&quot;, p2i(last));
544     return last;
545   }
546 }
547 
548 size_t ContiguousSpace::block_size(const HeapWord* p) const {
</pre>
<hr />
<pre>
570   assert(Heap_lock-&gt;owned_by_self() ||
571          (SafepointSynchronize::is_at_safepoint() &amp;&amp; Thread::current()-&gt;is_VM_thread()),
572          &quot;not locked&quot;);
573   HeapWord* obj = top();
574   if (pointer_delta(end(), obj) &gt;= size) {
575     HeapWord* new_top = obj + size;
576     set_top(new_top);
577     assert(is_aligned(obj) &amp;&amp; is_aligned(new_top), &quot;checking alignment&quot;);
578     return obj;
579   } else {
580     return NULL;
581   }
582 }
583 
584 // This version is lock-free.
585 inline HeapWord* ContiguousSpace::par_allocate_impl(size_t size) {
586   do {
587     HeapWord* obj = top();
588     if (pointer_delta(end(), obj) &gt;= size) {
589       HeapWord* new_top = obj + size;
<span class="line-modified">590       HeapWord* result = Atomic::cmpxchg(new_top, top_addr(), obj);</span>
591       // result can be one of two:
592       //  the old top value: the exchange succeeded
593       //  otherwise: the new value of the top is returned.
594       if (result == obj) {
595         assert(is_aligned(obj) &amp;&amp; is_aligned(new_top), &quot;checking alignment&quot;);
596         return obj;
597       }
598     } else {
599       return NULL;
600     }
601   } while (true);
602 }
603 
604 HeapWord* ContiguousSpace::allocate_aligned(size_t size) {
605   assert(Heap_lock-&gt;owned_by_self() || (SafepointSynchronize::is_at_safepoint() &amp;&amp; Thread::current()-&gt;is_VM_thread()), &quot;not locked&quot;);
606   HeapWord* end_value = end();
607 
608   HeapWord* obj = CollectedHeap::align_allocation_or_fail(top(), end_value, SurvivorAlignmentInBytes);
609   if (obj == NULL) {
610     return NULL;
</pre>
<hr />
<pre>
634 
635 void ContiguousSpace::allocate_temporary_filler(int factor) {
636   // allocate temporary type array decreasing free size with factor &#39;factor&#39;
637   assert(factor &gt;= 0, &quot;just checking&quot;);
638   size_t size = pointer_delta(end(), top());
639 
640   // if space is full, return
641   if (size == 0) return;
642 
643   if (factor &gt; 0) {
644     size -= size/factor;
645   }
646   size = align_object_size(size);
647 
648   const size_t array_header_size = typeArrayOopDesc::header_size(T_INT);
649   if (size &gt;= align_object_size(array_header_size)) {
650     size_t length = (size - array_header_size) * (HeapWordSize / sizeof(jint));
651     // allocate uninitialized int array
652     typeArrayOop t = (typeArrayOop) allocate(size);
653     assert(t != NULL, &quot;allocation should succeed&quot;);
<span class="line-modified">654     t-&gt;set_mark_raw(markOopDesc::prototype());</span>
655     t-&gt;set_klass(Universe::intArrayKlassObj());
656     t-&gt;set_length((int)length);
657   } else {
658     assert(size == CollectedHeap::min_fill_size(),
659            &quot;size for smallest fake object doesn&#39;t match&quot;);
660     instanceOop obj = (instanceOop) allocate(size);
<span class="line-modified">661     obj-&gt;set_mark_raw(markOopDesc::prototype());</span>
662     obj-&gt;set_klass_gap(0);
663     obj-&gt;set_klass(SystemDictionary::Object_klass());
664   }
665 }
666 
667 HeapWord* OffsetTableContigSpace::initialize_threshold() {
668   return _offsets.initialize_threshold();
669 }
670 
671 HeapWord* OffsetTableContigSpace::cross_threshold(HeapWord* start, HeapWord* end) {
672   _offsets.alloc_block(start, end);
673   return _offsets.threshold();
674 }
675 
676 OffsetTableContigSpace::OffsetTableContigSpace(BlockOffsetSharedArray* sharedOffsetArray,
677                                                MemRegion mr) :
678   _offsets(sharedOffsetArray, mr),
679   _par_alloc_lock(Mutex::leaf, &quot;OffsetTableContigSpace par alloc lock&quot;, true)
680 {
681   _offsets.set_contig_space(this);
</pre>
</td>
<td>
<hr />
<pre>
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;classfile/systemDictionary.hpp&quot;
 27 #include &quot;classfile/vmSymbols.hpp&quot;
 28 #include &quot;gc/shared/blockOffsetTable.inline.hpp&quot;
 29 #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
 30 #include &quot;gc/shared/genCollectedHeap.hpp&quot;
 31 #include &quot;gc/shared/genOopClosures.inline.hpp&quot;
 32 #include &quot;gc/shared/space.hpp&quot;
 33 #include &quot;gc/shared/space.inline.hpp&quot;
<span class="line-modified"> 34 #include &quot;gc/shared/spaceDecorator.inline.hpp&quot;</span>
 35 #include &quot;memory/iterator.inline.hpp&quot;
 36 #include &quot;memory/universe.hpp&quot;
 37 #include &quot;oops/oop.inline.hpp&quot;
 38 #include &quot;runtime/atomic.hpp&quot;
 39 #include &quot;runtime/java.hpp&quot;

 40 #include &quot;runtime/prefetch.inline.hpp&quot;
 41 #include &quot;runtime/safepoint.hpp&quot;
 42 #include &quot;utilities/align.hpp&quot;
 43 #include &quot;utilities/copy.hpp&quot;
 44 #include &quot;utilities/globalDefinitions.hpp&quot;
 45 #include &quot;utilities/macros.hpp&quot;
 46 #if INCLUDE_SERIALGC
 47 #include &quot;gc/serial/defNewGeneration.hpp&quot;
 48 #endif
 49 
 50 HeapWord* DirtyCardToOopClosure::get_actual_top(HeapWord* top,
 51                                                 HeapWord* top_obj) {
 52   if (top_obj != NULL) {
 53     if (_sp-&gt;block_is_obj(top_obj)) {
 54       if (_precision == CardTable::ObjHeadPreciseArray) {
 55         if (oop(top_obj)-&gt;is_objArray() || oop(top_obj)-&gt;is_typeArray()) {
 56           // An arrayOop is starting on the dirty card - since we do exact
 57           // store checks for objArrays we are done.
 58         } else {
 59           // Otherwise, it is possible that the object starting on the dirty
</pre>
<hr />
<pre>
 92     // current pointer; unfortunately, that won&#39;t work in CMS because
 93     // we&#39;d need an interface change (it seems) to have the space
 94     // &quot;adjust the object size&quot; (for instance pad it up to its
 95     // block alignment or minimum block size restrictions. XXX
 96     if (_sp-&gt;block_is_obj(bottom) &amp;&amp;
 97         !_sp-&gt;obj_allocated_since_save_marks(oop(bottom))) {
 98       oop(bottom)-&gt;oop_iterate(_cl, mr);
 99     }
100   }
101 }
102 
103 // We get called with &quot;mr&quot; representing the dirty region
104 // that we want to process. Because of imprecise marking,
105 // we may need to extend the incoming &quot;mr&quot; to the right,
106 // and scan more. However, because we may already have
107 // scanned some of that extended region, we may need to
108 // trim its right-end back some so we do not scan what
109 // we (or another worker thread) may already have scanned
110 // or planning to scan.
111 void DirtyCardToOopClosure::do_MemRegion(MemRegion mr) {












112   HeapWord* bottom = mr.start();
113   HeapWord* last = mr.last();
114   HeapWord* top = mr.end();
115   HeapWord* bottom_obj;
116   HeapWord* top_obj;
117 
118   assert(_precision == CardTable::ObjHeadPreciseArray ||
119          _precision == CardTable::Precise,
120          &quot;Only ones we deal with for now.&quot;);
121 
122   assert(_precision != CardTable::ObjHeadPreciseArray ||
123          _last_bottom == NULL || top &lt;= _last_bottom,
124          &quot;Not decreasing&quot;);
125   NOT_PRODUCT(_last_bottom = mr.start());
126 
127   bottom_obj = _sp-&gt;block_start(bottom);
128   top_obj    = _sp-&gt;block_start(last);
129 
130   assert(bottom_obj &lt;= bottom, &quot;just checking&quot;);
131   assert(top_obj    &lt;= top,    &quot;just checking&quot;);
</pre>
<hr />
<pre>
356   // First check if we should switch compaction space
357   assert(this == cp-&gt;space, &quot;&#39;this&#39; should be current compaction space.&quot;);
358   size_t compaction_max_size = pointer_delta(end(), compact_top);
359   while (size &gt; compaction_max_size) {
360     // switch to next compaction space
361     cp-&gt;space-&gt;set_compaction_top(compact_top);
362     cp-&gt;space = cp-&gt;space-&gt;next_compaction_space();
363     if (cp-&gt;space == NULL) {
364       cp-&gt;gen = GenCollectedHeap::heap()-&gt;young_gen();
365       assert(cp-&gt;gen != NULL, &quot;compaction must succeed&quot;);
366       cp-&gt;space = cp-&gt;gen-&gt;first_compaction_space();
367       assert(cp-&gt;space != NULL, &quot;generation must have a first compaction space&quot;);
368     }
369     compact_top = cp-&gt;space-&gt;bottom();
370     cp-&gt;space-&gt;set_compaction_top(compact_top);
371     cp-&gt;threshold = cp-&gt;space-&gt;initialize_threshold();
372     compaction_max_size = pointer_delta(cp-&gt;space-&gt;end(), compact_top);
373   }
374 
375   // store the forwarding pointer into the mark word
<span class="line-modified">376   if (cast_from_oop&lt;HeapWord*&gt;(q) != compact_top) {</span>
377     q-&gt;forward_to(oop(compact_top));
378     assert(q-&gt;is_gc_marked(), &quot;encoding the pointer should preserve the mark&quot;);
379   } else {
380     // if the object isn&#39;t moving we can just set the mark to the default
381     // mark and handle it specially later on.
382     q-&gt;init_mark_raw();
383     assert(q-&gt;forwardee() == NULL, &quot;should be forwarded to NULL&quot;);
384   }
385 
386   compact_top += size;
387 
388   // we need to update the offset table so that the beginnings of objects can be
389   // found during scavenge.  Note that we are updating the offset table based on
390   // where the object will be once the compaction phase finishes.
391   if (compact_top &gt; cp-&gt;threshold)
392     cp-&gt;threshold =
393       cp-&gt;space-&gt;cross_threshold(compact_top - size, compact_top);
394   return compact_top;
395 }
396 
</pre>
<hr />
<pre>
468 bool Space::obj_is_alive(const HeapWord* p) const {
469   assert (block_is_obj(p), &quot;The address should point to an object&quot;);
470   return true;
471 }
472 
473 void ContiguousSpace::oop_iterate(OopIterateClosure* blk) {
474   if (is_empty()) return;
475   HeapWord* obj_addr = bottom();
476   HeapWord* t = top();
477   // Could call objects iterate, but this is easier.
478   while (obj_addr &lt; t) {
479     obj_addr += oop(obj_addr)-&gt;oop_iterate_size(blk);
480   }
481 }
482 
483 void ContiguousSpace::object_iterate(ObjectClosure* blk) {
484   if (is_empty()) return;
485   object_iterate_from(bottom(), blk);
486 }
487 






488 void ContiguousSpace::object_iterate_from(HeapWord* mark, ObjectClosure* blk) {
489   while (mark &lt; top()) {
490     blk-&gt;do_object(oop(mark));
491     mark += oop(mark)-&gt;size();
492   }
493 }
494 















495 // Very general, slow implementation.
496 HeapWord* ContiguousSpace::block_start_const(const void* p) const {
497   assert(MemRegion(bottom(), end()).contains(p),
498          &quot;p (&quot; PTR_FORMAT &quot;) not in space [&quot; PTR_FORMAT &quot;, &quot; PTR_FORMAT &quot;)&quot;,
499          p2i(p), p2i(bottom()), p2i(end()));
500   if (p &gt;= top()) {
501     return top();
502   } else {
503     HeapWord* last = bottom();
504     HeapWord* cur = last;
505     while (cur &lt;= p) {
506       last = cur;
507       cur += oop(cur)-&gt;size();
508     }
509     assert(oopDesc::is_oop(oop(last)), PTR_FORMAT &quot; should be an object start&quot;, p2i(last));
510     return last;
511   }
512 }
513 
514 size_t ContiguousSpace::block_size(const HeapWord* p) const {
</pre>
<hr />
<pre>
536   assert(Heap_lock-&gt;owned_by_self() ||
537          (SafepointSynchronize::is_at_safepoint() &amp;&amp; Thread::current()-&gt;is_VM_thread()),
538          &quot;not locked&quot;);
539   HeapWord* obj = top();
540   if (pointer_delta(end(), obj) &gt;= size) {
541     HeapWord* new_top = obj + size;
542     set_top(new_top);
543     assert(is_aligned(obj) &amp;&amp; is_aligned(new_top), &quot;checking alignment&quot;);
544     return obj;
545   } else {
546     return NULL;
547   }
548 }
549 
550 // This version is lock-free.
551 inline HeapWord* ContiguousSpace::par_allocate_impl(size_t size) {
552   do {
553     HeapWord* obj = top();
554     if (pointer_delta(end(), obj) &gt;= size) {
555       HeapWord* new_top = obj + size;
<span class="line-modified">556       HeapWord* result = Atomic::cmpxchg(top_addr(), obj, new_top);</span>
557       // result can be one of two:
558       //  the old top value: the exchange succeeded
559       //  otherwise: the new value of the top is returned.
560       if (result == obj) {
561         assert(is_aligned(obj) &amp;&amp; is_aligned(new_top), &quot;checking alignment&quot;);
562         return obj;
563       }
564     } else {
565       return NULL;
566     }
567   } while (true);
568 }
569 
570 HeapWord* ContiguousSpace::allocate_aligned(size_t size) {
571   assert(Heap_lock-&gt;owned_by_self() || (SafepointSynchronize::is_at_safepoint() &amp;&amp; Thread::current()-&gt;is_VM_thread()), &quot;not locked&quot;);
572   HeapWord* end_value = end();
573 
574   HeapWord* obj = CollectedHeap::align_allocation_or_fail(top(), end_value, SurvivorAlignmentInBytes);
575   if (obj == NULL) {
576     return NULL;
</pre>
<hr />
<pre>
600 
601 void ContiguousSpace::allocate_temporary_filler(int factor) {
602   // allocate temporary type array decreasing free size with factor &#39;factor&#39;
603   assert(factor &gt;= 0, &quot;just checking&quot;);
604   size_t size = pointer_delta(end(), top());
605 
606   // if space is full, return
607   if (size == 0) return;
608 
609   if (factor &gt; 0) {
610     size -= size/factor;
611   }
612   size = align_object_size(size);
613 
614   const size_t array_header_size = typeArrayOopDesc::header_size(T_INT);
615   if (size &gt;= align_object_size(array_header_size)) {
616     size_t length = (size - array_header_size) * (HeapWordSize / sizeof(jint));
617     // allocate uninitialized int array
618     typeArrayOop t = (typeArrayOop) allocate(size);
619     assert(t != NULL, &quot;allocation should succeed&quot;);
<span class="line-modified">620     t-&gt;set_mark_raw(markWord::prototype());</span>
621     t-&gt;set_klass(Universe::intArrayKlassObj());
622     t-&gt;set_length((int)length);
623   } else {
624     assert(size == CollectedHeap::min_fill_size(),
625            &quot;size for smallest fake object doesn&#39;t match&quot;);
626     instanceOop obj = (instanceOop) allocate(size);
<span class="line-modified">627     obj-&gt;set_mark_raw(markWord::prototype());</span>
628     obj-&gt;set_klass_gap(0);
629     obj-&gt;set_klass(SystemDictionary::Object_klass());
630   }
631 }
632 
633 HeapWord* OffsetTableContigSpace::initialize_threshold() {
634   return _offsets.initialize_threshold();
635 }
636 
637 HeapWord* OffsetTableContigSpace::cross_threshold(HeapWord* start, HeapWord* end) {
638   _offsets.alloc_block(start, end);
639   return _offsets.threshold();
640 }
641 
642 OffsetTableContigSpace::OffsetTableContigSpace(BlockOffsetSharedArray* sharedOffsetArray,
643                                                MemRegion mr) :
644   _offsets(sharedOffsetArray, mr),
645   _par_alloc_lock(Mutex::leaf, &quot;OffsetTableContigSpace par alloc lock&quot;, true)
646 {
647   _offsets.set_contig_space(this);
</pre>
</td>
</tr>
</table>
<center><a href="scavengableNMethods.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="space.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>