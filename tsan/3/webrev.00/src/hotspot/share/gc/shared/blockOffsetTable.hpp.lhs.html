<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/gc/shared/blockOffsetTable.hpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
    <script type="text/javascript" src="../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_GC_SHARED_BLOCKOFFSETTABLE_HPP
 26 #define SHARE_GC_SHARED_BLOCKOFFSETTABLE_HPP
 27 
 28 #include &quot;gc/shared/memset_with_concurrent_readers.hpp&quot;
 29 #include &quot;memory/allocation.hpp&quot;
 30 #include &quot;memory/memRegion.hpp&quot;
 31 #include &quot;memory/virtualspace.hpp&quot;
 32 #include &quot;runtime/globals.hpp&quot;
 33 #include &quot;utilities/globalDefinitions.hpp&quot;
 34 #include &quot;utilities/macros.hpp&quot;
 35 
 36 // The CollectedHeap type requires subtypes to implement a method
 37 // &quot;block_start&quot;.  For some subtypes, notably generational
 38 // systems using card-table-based write barriers, the efficiency of this
 39 // operation may be important.  Implementations of the &quot;BlockOffsetArray&quot;
 40 // class may be useful in providing such efficient implementations.
 41 //
 42 // BlockOffsetTable (abstract)
 43 //   - BlockOffsetArray (abstract)
<a name="1" id="anc1"></a><span class="line-removed"> 44 //     - BlockOffsetArrayNonContigSpace</span>
 45 //     - BlockOffsetArrayContigSpace
 46 //
 47 
 48 class ContiguousSpace;
 49 
 50 class BOTConstants : public AllStatic {
 51 public:
 52   static const uint LogN = 9;
 53   static const uint LogN_words = LogN - LogHeapWordSize;
 54   static const uint N_bytes = 1 &lt;&lt; LogN;
 55   static const uint N_words = 1 &lt;&lt; LogN_words;
 56   // entries &quot;e&quot; of at least N_words mean &quot;go back by Base^(e-N_words).&quot;
 57   // All entries are less than &quot;N_words + N_powers&quot;.
 58   static const uint LogBase = 4;
 59   static const uint Base = (1 &lt;&lt; LogBase);
 60   static const uint N_powers = 14;
 61 
 62   static size_t power_to_cards_back(uint i) {
 63     return (size_t)1 &lt;&lt; (LogBase * i);
 64   }
 65   static size_t power_to_words_back(uint i) {
 66     return power_to_cards_back(i) * N_words;
 67   }
 68   static size_t entry_to_cards_back(u_char entry) {
 69     assert(entry &gt;= N_words, &quot;Precondition&quot;);
 70     return power_to_cards_back(entry - N_words);
 71   }
 72   static size_t entry_to_words_back(u_char entry) {
 73     assert(entry &gt;= N_words, &quot;Precondition&quot;);
 74     return power_to_words_back(entry - N_words);
 75   }
 76 };
 77 
 78 //////////////////////////////////////////////////////////////////////////
 79 // The BlockOffsetTable &quot;interface&quot;
 80 //////////////////////////////////////////////////////////////////////////
 81 class BlockOffsetTable {
 82   friend class VMStructs;
 83 protected:
 84   // These members describe the region covered by the table.
 85 
 86   // The space this table is covering.
 87   HeapWord* _bottom;    // == reserved.start
 88   HeapWord* _end;       // End of currently allocated region.
 89 
 90 public:
 91   // Initialize the table to cover the given space.
 92   // The contents of the initial table are undefined.
 93   BlockOffsetTable(HeapWord* bottom, HeapWord* end):
 94     _bottom(bottom), _end(end) {
 95     assert(_bottom &lt;= _end, &quot;arguments out of order&quot;);
 96   }
 97 
 98   // Note that the committed size of the covered space may have changed,
 99   // so the table size might also wish to change.
100   virtual void resize(size_t new_word_size) = 0;
101 
102   virtual void set_bottom(HeapWord* new_bottom) {
103     assert(new_bottom &lt;= _end, &quot;new_bottom &gt; _end&quot;);
104     _bottom = new_bottom;
105     resize(pointer_delta(_end, _bottom));
106   }
107 
108   // Requires &quot;addr&quot; to be contained by a block, and returns the address of
109   // the start of that block.
110   virtual HeapWord* block_start_unsafe(const void* addr) const = 0;
111 
112   // Returns the address of the start of the block containing &quot;addr&quot;, or
113   // else &quot;null&quot; if it is covered by no block.
114   HeapWord* block_start(const void* addr) const;
115 };
116 
117 //////////////////////////////////////////////////////////////////////////
118 // One implementation of &quot;BlockOffsetTable,&quot; the BlockOffsetArray,
119 // divides the covered region into &quot;N&quot;-word subregions (where
120 // &quot;N&quot; = 2^&quot;LogN&quot;.  An array with an entry for each such subregion
121 // indicates how far back one must go to find the start of the
122 // chunk that includes the first word of the subregion.
123 //
124 // Each BlockOffsetArray is owned by a Space.  However, the actual array
125 // may be shared by several BlockOffsetArrays; this is useful
126 // when a single resizable area (such as a generation) is divided up into
127 // several spaces in which contiguous allocation takes place.  (Consider,
128 // for example, the garbage-first generation.)
129 
130 // Here is the shared array type.
131 //////////////////////////////////////////////////////////////////////////
132 // BlockOffsetSharedArray
133 //////////////////////////////////////////////////////////////////////////
134 class BlockOffsetSharedArray: public CHeapObj&lt;mtGC&gt; {
135   friend class BlockOffsetArray;
136   friend class BlockOffsetArrayNonContigSpace;
137   friend class BlockOffsetArrayContigSpace;
138   friend class VMStructs;
139 
140  private:
141   bool _init_to_zero;
142 
143   // The reserved region covered by the shared array.
144   MemRegion _reserved;
145 
146   // End of the current committed region.
147   HeapWord* _end;
148 
149   // Array for keeping offsets for retrieving object start fast given an
150   // address.
151   VirtualSpace _vs;
152   u_char* _offset_array;          // byte array keeping backwards offsets
153 
154   void fill_range(size_t start, size_t num_cards, u_char offset) {
155     void* start_ptr = &amp;_offset_array[start];
156     // If collector is concurrent, special handling may be needed.
157     G1GC_ONLY(assert(!UseG1GC, &quot;Shouldn&#39;t be here when using G1&quot;);)
<a name="2" id="anc2"></a><span class="line-removed">158 #if INCLUDE_CMSGC</span>
<span class="line-removed">159     if (UseConcMarkSweepGC) {</span>
<span class="line-removed">160       memset_with_concurrent_readers(start_ptr, offset, num_cards);</span>
<span class="line-removed">161       return;</span>
<span class="line-removed">162     }</span>
<span class="line-removed">163 #endif // INCLUDE_CMSGC</span>
164     memset(start_ptr, offset, num_cards);
165   }
166 
167  protected:
168   // Bounds checking accessors:
169   // For performance these have to devolve to array accesses in product builds.
170   u_char offset_array(size_t index) const {
171     assert(index &lt; _vs.committed_size(), &quot;index out of range&quot;);
172     return _offset_array[index];
173   }
174   // An assertion-checking helper method for the set_offset_array() methods below.
175   void check_reducing_assertion(bool reducing);
176 
177   void set_offset_array(size_t index, u_char offset, bool reducing = false) {
178     check_reducing_assertion(reducing);
179     assert(index &lt; _vs.committed_size(), &quot;index out of range&quot;);
180     assert(!reducing || _offset_array[index] &gt;= offset, &quot;Not reducing&quot;);
181     _offset_array[index] = offset;
182   }
183 
184   void set_offset_array(size_t index, HeapWord* high, HeapWord* low, bool reducing = false) {
185     check_reducing_assertion(reducing);
186     assert(index &lt; _vs.committed_size(), &quot;index out of range&quot;);
187     assert(high &gt;= low, &quot;addresses out of order&quot;);
188     assert(pointer_delta(high, low) &lt;= BOTConstants::N_words, &quot;offset too large&quot;);
189     assert(!reducing || _offset_array[index] &gt;=  (u_char)pointer_delta(high, low),
190            &quot;Not reducing&quot;);
191     _offset_array[index] = (u_char)pointer_delta(high, low);
192   }
193 
194   void set_offset_array(HeapWord* left, HeapWord* right, u_char offset, bool reducing = false) {
195     check_reducing_assertion(reducing);
196     assert(index_for(right - 1) &lt; _vs.committed_size(),
197            &quot;right address out of range&quot;);
198     assert(left  &lt; right, &quot;Heap addresses out of order&quot;);
199     size_t num_cards = pointer_delta(right, left) &gt;&gt; BOTConstants::LogN_words;
200 
201     fill_range(index_for(left), num_cards, offset);
202   }
203 
204   void set_offset_array(size_t left, size_t right, u_char offset, bool reducing = false) {
205     check_reducing_assertion(reducing);
206     assert(right &lt; _vs.committed_size(), &quot;right address out of range&quot;);
207     assert(left  &lt;= right, &quot;indexes out of order&quot;);
208     size_t num_cards = right - left + 1;
209 
210     fill_range(left, num_cards, offset);
211   }
212 
213   void check_offset_array(size_t index, HeapWord* high, HeapWord* low) const {
214     assert(index &lt; _vs.committed_size(), &quot;index out of range&quot;);
215     assert(high &gt;= low, &quot;addresses out of order&quot;);
216     assert(pointer_delta(high, low) &lt;= BOTConstants::N_words, &quot;offset too large&quot;);
217     assert(_offset_array[index] == pointer_delta(high, low),
218            &quot;Wrong offset&quot;);
219   }
220 
221   bool is_card_boundary(HeapWord* p) const;
222 
223   // Return the number of slots needed for an offset array
224   // that covers mem_region_words words.
225   // We always add an extra slot because if an object
226   // ends on a card boundary we put a 0 in the next
227   // offset array slot, so we want that slot always
228   // to be reserved.
229 
230   size_t compute_size(size_t mem_region_words) {
231     size_t number_of_slots = (mem_region_words / BOTConstants::N_words) + 1;
232     return ReservedSpace::allocation_align_size_up(number_of_slots);
233   }
234 
235 public:
236   // Initialize the table to cover from &quot;base&quot; to (at least)
237   // &quot;base + init_word_size&quot;.  In the future, the table may be expanded
238   // (see &quot;resize&quot; below) up to the size of &quot;_reserved&quot; (which must be at
239   // least &quot;init_word_size&quot;.)  The contents of the initial table are
240   // undefined; it is the responsibility of the constituent
241   // BlockOffsetTable(s) to initialize cards.
242   BlockOffsetSharedArray(MemRegion reserved, size_t init_word_size);
243 
244   // Notes a change in the committed size of the region covered by the
245   // table.  The &quot;new_word_size&quot; may not be larger than the size of the
246   // reserved region this table covers.
247   void resize(size_t new_word_size);
248 
249   void set_bottom(HeapWord* new_bottom);
250 
251   // Whether entries should be initialized to zero. Used currently only for
252   // error checking.
253   void set_init_to_zero(bool val) { _init_to_zero = val; }
254   bool init_to_zero() { return _init_to_zero; }
255 
256   // Updates all the BlockOffsetArray&#39;s sharing this shared array to
257   // reflect the current &quot;top&quot;&#39;s of their spaces.
258   void update_offset_arrays();   // Not yet implemented!
259 
260   // Return the appropriate index into &quot;_offset_array&quot; for &quot;p&quot;.
261   size_t index_for(const void* p) const;
262 
263   // Return the address indicating the start of the region corresponding to
264   // &quot;index&quot; in &quot;_offset_array&quot;.
265   HeapWord* address_for_index(size_t index) const;
266 };
267 
268 class Space;
269 
270 //////////////////////////////////////////////////////////////////////////
271 // The BlockOffsetArray whose subtypes use the BlockOffsetSharedArray.
272 //////////////////////////////////////////////////////////////////////////
273 class BlockOffsetArray: public BlockOffsetTable {
274   friend class VMStructs;
275  protected:
276   // The following enums are used by do_block_internal() below
277   enum Action {
278     Action_single,      // BOT records a single block (see single_block())
279     Action_mark,        // BOT marks the start of a block (see mark_block())
280     Action_check        // Check that BOT records block correctly
281                         // (see verify_single_block()).
282   };
283 
284   // The shared array, which is shared with other BlockOffsetArray&#39;s
285   // corresponding to different spaces within a generation or span of
286   // memory.
287   BlockOffsetSharedArray* _array;
288 
289   // The space that owns this subregion.
290   Space* _sp;
291 
292   // If true, array entries are initialized to 0; otherwise, they are
293   // initialized to point backwards to the beginning of the covered region.
294   bool _init_to_zero;
295 
296   // An assertion-checking helper method for the set_remainder*() methods below.
297   void check_reducing_assertion(bool reducing) { _array-&gt;check_reducing_assertion(reducing); }
298 
299   // Sets the entries
300   // corresponding to the cards starting at &quot;start&quot; and ending at &quot;end&quot;
301   // to point back to the card before &quot;start&quot;: the interval [start, end)
302   // is right-open. The last parameter, reducing, indicates whether the
303   // updates to individual entries always reduce the entry from a higher
304   // to a lower value. (For example this would hold true during a temporal
305   // regime during which only block splits were updating the BOT.
306   void set_remainder_to_point_to_start(HeapWord* start, HeapWord* end, bool reducing = false);
307   // Same as above, except that the args here are a card _index_ interval
308   // that is closed: [start_index, end_index]
309   void set_remainder_to_point_to_start_incl(size_t start, size_t end, bool reducing = false);
310 
311   // A helper function for BOT adjustment/verification work
312   void do_block_internal(HeapWord* blk_start, HeapWord* blk_end, Action action, bool reducing = false);
313 
314  public:
315   // The space may not have its bottom and top set yet, which is why the
316   // region is passed as a parameter.  If &quot;init_to_zero&quot; is true, the
317   // elements of the array are initialized to zero.  Otherwise, they are
318   // initialized to point backwards to the beginning.
319   BlockOffsetArray(BlockOffsetSharedArray* array, MemRegion mr,
320                    bool init_to_zero_);
321 
322   // Note: this ought to be part of the constructor, but that would require
323   // &quot;this&quot; to be passed as a parameter to a member constructor for
324   // the containing concrete subtype of Space.
325   // This would be legal C++, but MS VC++ doesn&#39;t allow it.
326   void set_space(Space* sp) { _sp = sp; }
327 
328   // Resets the covered region to the given &quot;mr&quot;.
329   void set_region(MemRegion mr) {
330     _bottom = mr.start();
331     _end = mr.end();
332   }
333 
334   // Note that the committed size of the covered space may have changed,
335   // so the table size might also wish to change.
336   virtual void resize(size_t new_word_size) {
337     HeapWord* new_end = _bottom + new_word_size;
338     if (_end &lt; new_end &amp;&amp; !init_to_zero()) {
339       // verify that the old and new boundaries are also card boundaries
340       assert(_array-&gt;is_card_boundary(_end),
341              &quot;_end not a card boundary&quot;);
342       assert(_array-&gt;is_card_boundary(new_end),
343              &quot;new _end would not be a card boundary&quot;);
344       // set all the newly added cards
345       _array-&gt;set_offset_array(_end, new_end, BOTConstants::N_words);
346     }
347     _end = new_end;  // update _end
348   }
349 
350   // Adjust the BOT to show that it has a single block in the
351   // range [blk_start, blk_start + size). All necessary BOT
352   // cards are adjusted, but _unallocated_block isn&#39;t.
353   void single_block(HeapWord* blk_start, HeapWord* blk_end);
354   void single_block(HeapWord* blk, size_t size) {
355     single_block(blk, blk + size);
356   }
357 
358   // When the alloc_block() call returns, the block offset table should
359   // have enough information such that any subsequent block_start() call
360   // with an argument equal to an address that is within the range
361   // [blk_start, blk_end) would return the value blk_start, provided
362   // there have been no calls in between that reset this information
363   // (e.g. see BlockOffsetArrayNonContigSpace::single_block() call
364   // for an appropriate range covering the said interval).
365   // These methods expect to be called with [blk_start, blk_end)
366   // representing a block of memory in the heap.
367   virtual void alloc_block(HeapWord* blk_start, HeapWord* blk_end);
368   void alloc_block(HeapWord* blk, size_t size) {
369     alloc_block(blk, blk + size);
370   }
371 
372   // If true, initialize array slots with no allocated blocks to zero.
373   // Otherwise, make them point back to the front.
374   bool init_to_zero() { return _init_to_zero; }
375   // Corresponding setter
376   void set_init_to_zero(bool val) {
377     _init_to_zero = val;
378     assert(_array != NULL, &quot;_array should be non-NULL&quot;);
379     _array-&gt;set_init_to_zero(val);
380   }
381 
382   // Debugging
383   // Return the index of the last entry in the &quot;active&quot; region.
384   virtual size_t last_active_index() const = 0;
385   // Verify the block offset table
386   void verify() const;
387   void check_all_cards(size_t left_card, size_t right_card) const;
388 };
389 
390 ////////////////////////////////////////////////////////////////////////////
<a name="3" id="anc3"></a><span class="line-removed">391 // A subtype of BlockOffsetArray that takes advantage of the fact</span>
<span class="line-removed">392 // that its underlying space is a NonContiguousSpace, so that some</span>
<span class="line-removed">393 // specialized interfaces can be made available for spaces that</span>
<span class="line-removed">394 // manipulate the table.</span>
<span class="line-removed">395 ////////////////////////////////////////////////////////////////////////////</span>
<span class="line-removed">396 class BlockOffsetArrayNonContigSpace: public BlockOffsetArray {</span>
<span class="line-removed">397   friend class VMStructs;</span>
<span class="line-removed">398  private:</span>
<span class="line-removed">399   // The portion [_unallocated_block, _sp.end()) of the space that</span>
<span class="line-removed">400   // is a single block known not to contain any objects.</span>
<span class="line-removed">401   // NOTE: See BlockOffsetArrayUseUnallocatedBlock flag.</span>
<span class="line-removed">402   HeapWord* _unallocated_block;</span>
<span class="line-removed">403 </span>
<span class="line-removed">404  public:</span>
<span class="line-removed">405   BlockOffsetArrayNonContigSpace(BlockOffsetSharedArray* array, MemRegion mr):</span>
<span class="line-removed">406     BlockOffsetArray(array, mr, false),</span>
<span class="line-removed">407     _unallocated_block(_bottom) { }</span>
<span class="line-removed">408 </span>
<span class="line-removed">409   // Accessor</span>
<span class="line-removed">410   HeapWord* unallocated_block() const {</span>
<span class="line-removed">411     assert(BlockOffsetArrayUseUnallocatedBlock,</span>
<span class="line-removed">412            &quot;_unallocated_block is not being maintained&quot;);</span>
<span class="line-removed">413     return _unallocated_block;</span>
<span class="line-removed">414   }</span>
<span class="line-removed">415 </span>
<span class="line-removed">416   void set_unallocated_block(HeapWord* block) {</span>
<span class="line-removed">417     assert(BlockOffsetArrayUseUnallocatedBlock,</span>
<span class="line-removed">418            &quot;_unallocated_block is not being maintained&quot;);</span>
<span class="line-removed">419     assert(block &gt;= _bottom &amp;&amp; block &lt;= _end, &quot;out of range&quot;);</span>
<span class="line-removed">420     _unallocated_block = block;</span>
<span class="line-removed">421   }</span>
<span class="line-removed">422 </span>
<span class="line-removed">423   // These methods expect to be called with [blk_start, blk_end)</span>
<span class="line-removed">424   // representing a block of memory in the heap.</span>
<span class="line-removed">425   void alloc_block(HeapWord* blk_start, HeapWord* blk_end);</span>
<span class="line-removed">426   void alloc_block(HeapWord* blk, size_t size) {</span>
<span class="line-removed">427     alloc_block(blk, blk + size);</span>
<span class="line-removed">428   }</span>
<span class="line-removed">429 </span>
<span class="line-removed">430   // The following methods are useful and optimized for a</span>
<span class="line-removed">431   // non-contiguous space.</span>
<span class="line-removed">432 </span>
<span class="line-removed">433   // Given a block [blk_start, blk_start + full_blk_size), and</span>
<span class="line-removed">434   // a left_blk_size &lt; full_blk_size, adjust the BOT to show two</span>
<span class="line-removed">435   // blocks [blk_start, blk_start + left_blk_size) and</span>
<span class="line-removed">436   // [blk_start + left_blk_size, blk_start + full_blk_size).</span>
<span class="line-removed">437   // It is assumed (and verified in the non-product VM) that the</span>
<span class="line-removed">438   // BOT was correct for the original block.</span>
<span class="line-removed">439   void split_block(HeapWord* blk_start, size_t full_blk_size,</span>
<span class="line-removed">440                            size_t left_blk_size);</span>
<span class="line-removed">441 </span>
<span class="line-removed">442   // Adjust BOT to show that it has a block in the range</span>
<span class="line-removed">443   // [blk_start, blk_start + size). Only the first card</span>
<span class="line-removed">444   // of BOT is touched. It is assumed (and verified in the</span>
<span class="line-removed">445   // non-product VM) that the remaining cards of the block</span>
<span class="line-removed">446   // are correct.</span>
<span class="line-removed">447   void mark_block(HeapWord* blk_start, HeapWord* blk_end, bool reducing = false);</span>
<span class="line-removed">448   void mark_block(HeapWord* blk, size_t size, bool reducing = false) {</span>
<span class="line-removed">449     mark_block(blk, blk + size, reducing);</span>
<span class="line-removed">450   }</span>
<span class="line-removed">451 </span>
<span class="line-removed">452   // Adjust _unallocated_block to indicate that a particular</span>
<span class="line-removed">453   // block has been newly allocated or freed. It is assumed (and</span>
<span class="line-removed">454   // verified in the non-product VM) that the BOT is correct for</span>
<span class="line-removed">455   // the given block.</span>
<span class="line-removed">456   void allocated(HeapWord* blk_start, HeapWord* blk_end, bool reducing = false) {</span>
<span class="line-removed">457     // Verify that the BOT shows [blk, blk + blk_size) to be one block.</span>
<span class="line-removed">458     verify_single_block(blk_start, blk_end);</span>
<span class="line-removed">459     if (BlockOffsetArrayUseUnallocatedBlock) {</span>
<span class="line-removed">460       _unallocated_block = MAX2(_unallocated_block, blk_end);</span>
<span class="line-removed">461     }</span>
<span class="line-removed">462   }</span>
<span class="line-removed">463 </span>
<span class="line-removed">464   void allocated(HeapWord* blk, size_t size, bool reducing = false) {</span>
<span class="line-removed">465     allocated(blk, blk + size, reducing);</span>
<span class="line-removed">466   }</span>
<span class="line-removed">467 </span>
<span class="line-removed">468   void freed(HeapWord* blk_start, HeapWord* blk_end);</span>
<span class="line-removed">469   void freed(HeapWord* blk, size_t size);</span>
<span class="line-removed">470 </span>
<span class="line-removed">471   HeapWord* block_start_unsafe(const void* addr) const;</span>
<span class="line-removed">472 </span>
<span class="line-removed">473   // Requires &quot;addr&quot; to be the start of a card and returns the</span>
<span class="line-removed">474   // start of the block that contains the given address.</span>
<span class="line-removed">475   HeapWord* block_start_careful(const void* addr) const;</span>
<span class="line-removed">476 </span>
<span class="line-removed">477   // Verification &amp; debugging: ensure that the offset table reflects</span>
<span class="line-removed">478   // the fact that the block [blk_start, blk_end) or [blk, blk + size)</span>
<span class="line-removed">479   // is a single block of storage. NOTE: can&#39;t const this because of</span>
<span class="line-removed">480   // call to non-const do_block_internal() below.</span>
<span class="line-removed">481   void verify_single_block(HeapWord* blk_start, HeapWord* blk_end)</span>
<span class="line-removed">482     PRODUCT_RETURN;</span>
<span class="line-removed">483   void verify_single_block(HeapWord* blk, size_t size) PRODUCT_RETURN;</span>
<span class="line-removed">484 </span>
<span class="line-removed">485   // Verify that the given block is before _unallocated_block</span>
<span class="line-removed">486   void verify_not_unallocated(HeapWord* blk_start, HeapWord* blk_end)</span>
<span class="line-removed">487     const PRODUCT_RETURN;</span>
<span class="line-removed">488   void verify_not_unallocated(HeapWord* blk, size_t size)</span>
<span class="line-removed">489     const PRODUCT_RETURN;</span>
<span class="line-removed">490 </span>
<span class="line-removed">491   // Debugging support</span>
<span class="line-removed">492   virtual size_t last_active_index() const;</span>
<span class="line-removed">493 };</span>
<span class="line-removed">494 </span>
<span class="line-removed">495 ////////////////////////////////////////////////////////////////////////////</span>
496 // A subtype of BlockOffsetArray that takes advantage of the fact
497 // that its underlying space is a ContiguousSpace, so that its &quot;active&quot;
498 // region can be more efficiently tracked (than for a non-contiguous space).
499 ////////////////////////////////////////////////////////////////////////////
500 class BlockOffsetArrayContigSpace: public BlockOffsetArray {
501   friend class VMStructs;
502  private:
503   // allocation boundary at which offset array must be updated
504   HeapWord* _next_offset_threshold;
505   size_t    _next_offset_index;      // index corresponding to that boundary
506 
507   // Work function when allocation start crosses threshold.
508   void alloc_block_work(HeapWord* blk_start, HeapWord* blk_end);
509 
510  public:
511   BlockOffsetArrayContigSpace(BlockOffsetSharedArray* array, MemRegion mr):
512     BlockOffsetArray(array, mr, true) {
513     _next_offset_threshold = NULL;
514     _next_offset_index = 0;
515   }
516 
517   void set_contig_space(ContiguousSpace* sp) { set_space((Space*)sp); }
518 
519   // Initialize the threshold for an empty heap.
520   HeapWord* initialize_threshold();
521   // Zero out the entry for _bottom (offset will be zero)
522   void      zero_bottom_entry();
523 
524   // Return the next threshold, the point at which the table should be
525   // updated.
526   HeapWord* threshold() const { return _next_offset_threshold; }
527 
528   // In general, these methods expect to be called with
529   // [blk_start, blk_end) representing a block of memory in the heap.
530   // In this implementation, however, we are OK even if blk_start and/or
531   // blk_end are NULL because NULL is represented as 0, and thus
532   // never exceeds the &quot;_next_offset_threshold&quot;.
533   void alloc_block(HeapWord* blk_start, HeapWord* blk_end) {
534     if (blk_end &gt; _next_offset_threshold) {
535       alloc_block_work(blk_start, blk_end);
536     }
537   }
538   void alloc_block(HeapWord* blk, size_t size) {
539     alloc_block(blk, blk + size);
540   }
541 
542   HeapWord* block_start_unsafe(const void* addr) const;
543 
544   // Debugging support
545   virtual size_t last_active_index() const;
546 };
547 
548 #endif // SHARE_GC_SHARED_BLOCKOFFSETTABLE_HPP
<a name="4" id="anc4"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="4" type="hidden" />
</body>
</html>