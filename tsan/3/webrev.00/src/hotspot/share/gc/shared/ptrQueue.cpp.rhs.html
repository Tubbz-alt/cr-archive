<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/gc/shared/ptrQueue.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
    <script type="text/javascript" src="../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 2001, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/shared/ptrQueue.hpp&quot;
 27 #include &quot;logging/log.hpp&quot;
 28 #include &quot;memory/allocation.hpp&quot;
 29 #include &quot;memory/allocation.inline.hpp&quot;
 30 #include &quot;runtime/atomic.hpp&quot;
 31 #include &quot;runtime/mutex.hpp&quot;
 32 #include &quot;runtime/mutexLocker.hpp&quot;
<a name="1" id="anc1"></a>
 33 #include &quot;runtime/thread.inline.hpp&quot;
 34 #include &quot;utilities/globalCounter.inline.hpp&quot;
 35 
 36 #include &lt;new&gt;
 37 
<a name="2" id="anc2"></a><span class="line-modified"> 38 PtrQueue::PtrQueue(PtrQueueSet* qset, bool active) :</span>
 39   _qset(qset),
 40   _active(active),
<a name="3" id="anc3"></a>
 41   _index(0),
<a name="4" id="anc4"></a><span class="line-modified"> 42   _capacity_in_bytes(index_to_byte_index(qset-&gt;buffer_size())),</span>
<span class="line-modified"> 43   _buf(NULL)</span>

 44 {}
 45 
 46 PtrQueue::~PtrQueue() {
<a name="5" id="anc5"></a><span class="line-modified"> 47   assert(_buf == NULL, &quot;queue must be flushed before delete&quot;);</span>
 48 }
 49 
 50 void PtrQueue::flush_impl() {
 51   if (_buf != NULL) {
 52     BufferNode* node = BufferNode::make_node_from_buffer(_buf, index());
 53     if (is_empty()) {
 54       // No work to do.
 55       qset()-&gt;deallocate_buffer(node);
 56     } else {
 57       qset()-&gt;enqueue_completed_buffer(node);
 58     }
 59     _buf = NULL;
 60     set_index(0);
 61   }
 62 }
 63 
<a name="6" id="anc6"></a>
 64 void PtrQueue::enqueue_known_active(void* ptr) {
 65   while (_index == 0) {
 66     handle_zero_index();
 67   }
 68 
 69   assert(_buf != NULL, &quot;postcondition&quot;);
 70   assert(index() &gt; 0, &quot;postcondition&quot;);
 71   assert(index() &lt;= capacity(), &quot;invariant&quot;);
 72   _index -= _element_size;
 73   _buf[index()] = ptr;
 74 }
 75 
<a name="7" id="anc7"></a><span class="line-added"> 76 void PtrQueue::handle_zero_index() {</span>
<span class="line-added"> 77   assert(index() == 0, &quot;precondition&quot;);</span>
<span class="line-added"> 78 </span>
<span class="line-added"> 79   if (_buf != NULL) {</span>
<span class="line-added"> 80     handle_completed_buffer();</span>
<span class="line-added"> 81   } else {</span>
<span class="line-added"> 82     allocate_buffer();</span>
<span class="line-added"> 83   }</span>
<span class="line-added"> 84 }</span>
<span class="line-added"> 85 </span>
<span class="line-added"> 86 void PtrQueue::allocate_buffer() {</span>
<span class="line-added"> 87   _buf = qset()-&gt;allocate_buffer();</span>
<span class="line-added"> 88   reset();</span>
<span class="line-added"> 89 }</span>
<span class="line-added"> 90 </span>
<span class="line-added"> 91 void PtrQueue::enqueue_completed_buffer() {</span>
<span class="line-added"> 92   assert(_buf != NULL, &quot;precondition&quot;);</span>
<span class="line-added"> 93   BufferNode* node = BufferNode::make_node_from_buffer(_buf, index());</span>
<span class="line-added"> 94   qset()-&gt;enqueue_completed_buffer(node);</span>
<span class="line-added"> 95   allocate_buffer();</span>
<span class="line-added"> 96 }</span>
<span class="line-added"> 97 </span>
 98 BufferNode* BufferNode::allocate(size_t size) {
 99   size_t byte_size = size * sizeof(void*);
100   void* data = NEW_C_HEAP_ARRAY(char, buffer_offset() + byte_size, mtGC);
101   return new (data) BufferNode;
102 }
103 
104 void BufferNode::deallocate(BufferNode* node) {
105   node-&gt;~BufferNode();
106   FREE_C_HEAP_ARRAY(char, node);
107 }
108 
109 BufferNode::Allocator::Allocator(const char* name, size_t buffer_size) :
110   _buffer_size(buffer_size),
111   _pending_list(),
112   _free_list(),
113   _pending_count(0),
114   _free_count(0),
115   _transfer_lock(false)
116 {
<a name="8" id="anc8"></a><span class="line-modified">117   strncpy(_name, name, sizeof(_name) - 1);</span>
118   _name[sizeof(_name) - 1] = &#39;\0&#39;;
119 }
120 
121 BufferNode::Allocator::~Allocator() {
122   delete_list(_free_list.pop_all());
123   delete_list(_pending_list.pop_all());
124 }
125 
126 void BufferNode::Allocator::delete_list(BufferNode* list) {
127   while (list != NULL) {
128     BufferNode* next = list-&gt;next();
129     DEBUG_ONLY(list-&gt;set_next(NULL);)
130     BufferNode::deallocate(list);
131     list = next;
132   }
133 }
134 
135 size_t BufferNode::Allocator::free_count() const {
136   return Atomic::load(&amp;_free_count);
137 }
138 
139 BufferNode* BufferNode::Allocator::allocate() {
140   BufferNode* node;
141   {
142     // Protect against ABA; see release().
143     GlobalCounter::CriticalSection cs(Thread::current());
144     node = _free_list.pop();
145   }
146   if (node == NULL) {
147     node = BufferNode::allocate(_buffer_size);
148   } else {
149     // Decrement count after getting buffer from free list.  This, along
150     // with incrementing count before adding to free list, ensures count
151     // never underflows.
<a name="9" id="anc9"></a><span class="line-modified">152     size_t count = Atomic::sub(&amp;_free_count, 1u);</span>
153     assert((count + 1) != 0, &quot;_free_count underflow&quot;);
154   }
155   return node;
156 }
157 
158 // To solve the ABA problem for lock-free stack pop, allocate does the
159 // pop inside a critical section, and release synchronizes on the
160 // critical sections before adding to the _free_list.  But we don&#39;t
161 // want to make every release have to do a synchronize.  Instead, we
162 // initially place released nodes on the _pending_list, and transfer
163 // them to the _free_list in batches.  Only one transfer at a time is
164 // permitted, with a lock bit to control access to that phase.  A
165 // transfer takes all the nodes from the _pending_list, synchronizes on
166 // the _free_list pops, and then adds the former pending nodes to the
167 // _free_list.  While that&#39;s happening, other threads might be adding
168 // other nodes to the _pending_list, to be dealt with by some later
169 // transfer.
170 void BufferNode::Allocator::release(BufferNode* node) {
171   assert(node != NULL, &quot;precondition&quot;);
172   assert(node-&gt;next() == NULL, &quot;precondition&quot;);
173 
174   // Desired minimum transfer batch size.  There is relatively little
175   // importance to the specific number.  It shouldn&#39;t be too big, else
176   // we&#39;re wasting space when the release rate is low.  If the release
177   // rate is high, we might accumulate more than this before being
178   // able to start a new transfer, but that&#39;s okay.  Also note that
179   // the allocation rate and the release rate are going to be fairly
180   // similar, due to how the buffers are used.
181   const size_t trigger_transfer = 10;
182 
183   // Add to pending list. Update count first so no underflow in transfer.
<a name="10" id="anc10"></a><span class="line-modified">184   size_t pending_count = Atomic::add(&amp;_pending_count, 1u);</span>
185   _pending_list.push(*node);
186   if (pending_count &gt; trigger_transfer) {
187     try_transfer_pending();
188   }
189 }
190 
191 // Try to transfer nodes from _pending_list to _free_list, with a
192 // synchronization delay for any in-progress pops from the _free_list,
193 // to solve ABA there.  Return true if performed a (possibly empty)
194 // transfer, false if blocked from doing so by some other thread&#39;s
195 // in-progress transfer.
196 bool BufferNode::Allocator::try_transfer_pending() {
197   // Attempt to claim the lock.
198   if (Atomic::load(&amp;_transfer_lock) || // Skip CAS if likely to fail.
<a name="11" id="anc11"></a><span class="line-modified">199       Atomic::cmpxchg(&amp;_transfer_lock, false, true)) {</span>
200     return false;
201   }
202   // Have the lock; perform the transfer.
203 
204   // Claim all the pending nodes.
205   BufferNode* first = _pending_list.pop_all();
206   if (first != NULL) {
207     // Prepare to add the claimed nodes, and update _pending_count.
208     BufferNode* last = first;
209     size_t count = 1;
210     for (BufferNode* next = first-&gt;next(); next != NULL; next = next-&gt;next()) {
211       last = next;
212       ++count;
213     }
<a name="12" id="anc12"></a><span class="line-modified">214     Atomic::sub(&amp;_pending_count, count);</span>
215 
216     // Wait for any in-progress pops, to avoid ABA for them.
217     GlobalCounter::write_synchronize();
218 
219     // Add synchronized nodes to _free_list.
220     // Update count first so no underflow in allocate().
<a name="13" id="anc13"></a><span class="line-modified">221     Atomic::add(&amp;_free_count, count);</span>
222     _free_list.prepend(*first, *last);
223     log_trace(gc, ptrqueue, freelist)
224              (&quot;Transferred %s pending to free: &quot; SIZE_FORMAT, name(), count);
225   }
<a name="14" id="anc14"></a><span class="line-modified">226   Atomic::release_store(&amp;_transfer_lock, false);</span>
227   return true;
228 }
229 
230 size_t BufferNode::Allocator::reduce_free_list(size_t remove_goal) {
231   try_transfer_pending();
232   size_t removed = 0;
233   for ( ; removed &lt; remove_goal; ++removed) {
234     BufferNode* node = _free_list.pop();
235     if (node == NULL) break;
236     BufferNode::deallocate(node);
237   }
<a name="15" id="anc15"></a><span class="line-modified">238   size_t new_count = Atomic::sub(&amp;_free_count, removed);</span>
239   log_debug(gc, ptrqueue, freelist)
240            (&quot;Reduced %s free list by &quot; SIZE_FORMAT &quot; to &quot; SIZE_FORMAT,
241             name(), removed, new_count);
242   return removed;
243 }
244 
<a name="16" id="anc16"></a><span class="line-modified">245 PtrQueueSet::PtrQueueSet(BufferNode::Allocator* allocator) :</span>
<span class="line-modified">246   _allocator(allocator),</span>









247   _all_active(false)
248 {}
249 
<a name="17" id="anc17"></a><span class="line-modified">250 PtrQueueSet::~PtrQueueSet() {}</span>











251 
252 void** PtrQueueSet::allocate_buffer() {
253   BufferNode* node = _allocator-&gt;allocate();
254   return BufferNode::make_buffer_from_node(node);
255 }
256 
257 void PtrQueueSet::deallocate_buffer(BufferNode* node) {
258   _allocator-&gt;release(node);
259 }
<a name="18" id="anc18"></a>














































































































































































<a name="19" id="anc19"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="19" type="hidden" />
</body>
</html>