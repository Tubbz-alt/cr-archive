<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/gc/shared/blockOffsetTable.hpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
    <script type="text/javascript" src="../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_GC_SHARED_BLOCKOFFSETTABLE_HPP
 26 #define SHARE_GC_SHARED_BLOCKOFFSETTABLE_HPP
 27 
 28 #include &quot;gc/shared/memset_with_concurrent_readers.hpp&quot;
 29 #include &quot;memory/allocation.hpp&quot;
 30 #include &quot;memory/memRegion.hpp&quot;
 31 #include &quot;memory/virtualspace.hpp&quot;
 32 #include &quot;runtime/globals.hpp&quot;
 33 #include &quot;utilities/globalDefinitions.hpp&quot;
 34 #include &quot;utilities/macros.hpp&quot;
 35 
 36 // The CollectedHeap type requires subtypes to implement a method
 37 // &quot;block_start&quot;.  For some subtypes, notably generational
 38 // systems using card-table-based write barriers, the efficiency of this
 39 // operation may be important.  Implementations of the &quot;BlockOffsetArray&quot;
 40 // class may be useful in providing such efficient implementations.
 41 //
 42 // BlockOffsetTable (abstract)
 43 //   - BlockOffsetArray (abstract)
<a name="1" id="anc1"></a>
 44 //     - BlockOffsetArrayContigSpace
 45 //
 46 
 47 class ContiguousSpace;
 48 
 49 class BOTConstants : public AllStatic {
 50 public:
 51   static const uint LogN = 9;
 52   static const uint LogN_words = LogN - LogHeapWordSize;
 53   static const uint N_bytes = 1 &lt;&lt; LogN;
 54   static const uint N_words = 1 &lt;&lt; LogN_words;
 55   // entries &quot;e&quot; of at least N_words mean &quot;go back by Base^(e-N_words).&quot;
 56   // All entries are less than &quot;N_words + N_powers&quot;.
 57   static const uint LogBase = 4;
 58   static const uint Base = (1 &lt;&lt; LogBase);
 59   static const uint N_powers = 14;
 60 
 61   static size_t power_to_cards_back(uint i) {
 62     return (size_t)1 &lt;&lt; (LogBase * i);
 63   }
 64   static size_t power_to_words_back(uint i) {
 65     return power_to_cards_back(i) * N_words;
 66   }
 67   static size_t entry_to_cards_back(u_char entry) {
 68     assert(entry &gt;= N_words, &quot;Precondition&quot;);
 69     return power_to_cards_back(entry - N_words);
 70   }
 71   static size_t entry_to_words_back(u_char entry) {
 72     assert(entry &gt;= N_words, &quot;Precondition&quot;);
 73     return power_to_words_back(entry - N_words);
 74   }
 75 };
 76 
 77 //////////////////////////////////////////////////////////////////////////
 78 // The BlockOffsetTable &quot;interface&quot;
 79 //////////////////////////////////////////////////////////////////////////
 80 class BlockOffsetTable {
 81   friend class VMStructs;
 82 protected:
 83   // These members describe the region covered by the table.
 84 
 85   // The space this table is covering.
 86   HeapWord* _bottom;    // == reserved.start
 87   HeapWord* _end;       // End of currently allocated region.
 88 
 89 public:
 90   // Initialize the table to cover the given space.
 91   // The contents of the initial table are undefined.
 92   BlockOffsetTable(HeapWord* bottom, HeapWord* end):
 93     _bottom(bottom), _end(end) {
 94     assert(_bottom &lt;= _end, &quot;arguments out of order&quot;);
 95   }
 96 
 97   // Note that the committed size of the covered space may have changed,
 98   // so the table size might also wish to change.
 99   virtual void resize(size_t new_word_size) = 0;
100 
101   virtual void set_bottom(HeapWord* new_bottom) {
102     assert(new_bottom &lt;= _end, &quot;new_bottom &gt; _end&quot;);
103     _bottom = new_bottom;
104     resize(pointer_delta(_end, _bottom));
105   }
106 
107   // Requires &quot;addr&quot; to be contained by a block, and returns the address of
108   // the start of that block.
109   virtual HeapWord* block_start_unsafe(const void* addr) const = 0;
110 
111   // Returns the address of the start of the block containing &quot;addr&quot;, or
112   // else &quot;null&quot; if it is covered by no block.
113   HeapWord* block_start(const void* addr) const;
114 };
115 
116 //////////////////////////////////////////////////////////////////////////
117 // One implementation of &quot;BlockOffsetTable,&quot; the BlockOffsetArray,
118 // divides the covered region into &quot;N&quot;-word subregions (where
119 // &quot;N&quot; = 2^&quot;LogN&quot;.  An array with an entry for each such subregion
120 // indicates how far back one must go to find the start of the
121 // chunk that includes the first word of the subregion.
122 //
123 // Each BlockOffsetArray is owned by a Space.  However, the actual array
124 // may be shared by several BlockOffsetArrays; this is useful
125 // when a single resizable area (such as a generation) is divided up into
126 // several spaces in which contiguous allocation takes place.  (Consider,
127 // for example, the garbage-first generation.)
128 
129 // Here is the shared array type.
130 //////////////////////////////////////////////////////////////////////////
131 // BlockOffsetSharedArray
132 //////////////////////////////////////////////////////////////////////////
133 class BlockOffsetSharedArray: public CHeapObj&lt;mtGC&gt; {
134   friend class BlockOffsetArray;
135   friend class BlockOffsetArrayNonContigSpace;
136   friend class BlockOffsetArrayContigSpace;
137   friend class VMStructs;
138 
139  private:
140   bool _init_to_zero;
141 
142   // The reserved region covered by the shared array.
143   MemRegion _reserved;
144 
145   // End of the current committed region.
146   HeapWord* _end;
147 
148   // Array for keeping offsets for retrieving object start fast given an
149   // address.
150   VirtualSpace _vs;
151   u_char* _offset_array;          // byte array keeping backwards offsets
152 
153   void fill_range(size_t start, size_t num_cards, u_char offset) {
154     void* start_ptr = &amp;_offset_array[start];
155     // If collector is concurrent, special handling may be needed.
156     G1GC_ONLY(assert(!UseG1GC, &quot;Shouldn&#39;t be here when using G1&quot;);)
<a name="2" id="anc2"></a>





157     memset(start_ptr, offset, num_cards);
158   }
159 
160  protected:
161   // Bounds checking accessors:
162   // For performance these have to devolve to array accesses in product builds.
163   u_char offset_array(size_t index) const {
164     assert(index &lt; _vs.committed_size(), &quot;index out of range&quot;);
165     return _offset_array[index];
166   }
167   // An assertion-checking helper method for the set_offset_array() methods below.
168   void check_reducing_assertion(bool reducing);
169 
170   void set_offset_array(size_t index, u_char offset, bool reducing = false) {
171     check_reducing_assertion(reducing);
172     assert(index &lt; _vs.committed_size(), &quot;index out of range&quot;);
173     assert(!reducing || _offset_array[index] &gt;= offset, &quot;Not reducing&quot;);
174     _offset_array[index] = offset;
175   }
176 
177   void set_offset_array(size_t index, HeapWord* high, HeapWord* low, bool reducing = false) {
178     check_reducing_assertion(reducing);
179     assert(index &lt; _vs.committed_size(), &quot;index out of range&quot;);
180     assert(high &gt;= low, &quot;addresses out of order&quot;);
181     assert(pointer_delta(high, low) &lt;= BOTConstants::N_words, &quot;offset too large&quot;);
182     assert(!reducing || _offset_array[index] &gt;=  (u_char)pointer_delta(high, low),
183            &quot;Not reducing&quot;);
184     _offset_array[index] = (u_char)pointer_delta(high, low);
185   }
186 
187   void set_offset_array(HeapWord* left, HeapWord* right, u_char offset, bool reducing = false) {
188     check_reducing_assertion(reducing);
189     assert(index_for(right - 1) &lt; _vs.committed_size(),
190            &quot;right address out of range&quot;);
191     assert(left  &lt; right, &quot;Heap addresses out of order&quot;);
192     size_t num_cards = pointer_delta(right, left) &gt;&gt; BOTConstants::LogN_words;
193 
194     fill_range(index_for(left), num_cards, offset);
195   }
196 
197   void set_offset_array(size_t left, size_t right, u_char offset, bool reducing = false) {
198     check_reducing_assertion(reducing);
199     assert(right &lt; _vs.committed_size(), &quot;right address out of range&quot;);
200     assert(left  &lt;= right, &quot;indexes out of order&quot;);
201     size_t num_cards = right - left + 1;
202 
203     fill_range(left, num_cards, offset);
204   }
205 
206   void check_offset_array(size_t index, HeapWord* high, HeapWord* low) const {
207     assert(index &lt; _vs.committed_size(), &quot;index out of range&quot;);
208     assert(high &gt;= low, &quot;addresses out of order&quot;);
209     assert(pointer_delta(high, low) &lt;= BOTConstants::N_words, &quot;offset too large&quot;);
210     assert(_offset_array[index] == pointer_delta(high, low),
211            &quot;Wrong offset&quot;);
212   }
213 
214   bool is_card_boundary(HeapWord* p) const;
215 
216   // Return the number of slots needed for an offset array
217   // that covers mem_region_words words.
218   // We always add an extra slot because if an object
219   // ends on a card boundary we put a 0 in the next
220   // offset array slot, so we want that slot always
221   // to be reserved.
222 
223   size_t compute_size(size_t mem_region_words) {
224     size_t number_of_slots = (mem_region_words / BOTConstants::N_words) + 1;
225     return ReservedSpace::allocation_align_size_up(number_of_slots);
226   }
227 
228 public:
229   // Initialize the table to cover from &quot;base&quot; to (at least)
230   // &quot;base + init_word_size&quot;.  In the future, the table may be expanded
231   // (see &quot;resize&quot; below) up to the size of &quot;_reserved&quot; (which must be at
232   // least &quot;init_word_size&quot;.)  The contents of the initial table are
233   // undefined; it is the responsibility of the constituent
234   // BlockOffsetTable(s) to initialize cards.
235   BlockOffsetSharedArray(MemRegion reserved, size_t init_word_size);
236 
237   // Notes a change in the committed size of the region covered by the
238   // table.  The &quot;new_word_size&quot; may not be larger than the size of the
239   // reserved region this table covers.
240   void resize(size_t new_word_size);
241 
242   void set_bottom(HeapWord* new_bottom);
243 
244   // Whether entries should be initialized to zero. Used currently only for
245   // error checking.
246   void set_init_to_zero(bool val) { _init_to_zero = val; }
247   bool init_to_zero() { return _init_to_zero; }
248 
249   // Updates all the BlockOffsetArray&#39;s sharing this shared array to
250   // reflect the current &quot;top&quot;&#39;s of their spaces.
251   void update_offset_arrays();   // Not yet implemented!
252 
253   // Return the appropriate index into &quot;_offset_array&quot; for &quot;p&quot;.
254   size_t index_for(const void* p) const;
255 
256   // Return the address indicating the start of the region corresponding to
257   // &quot;index&quot; in &quot;_offset_array&quot;.
258   HeapWord* address_for_index(size_t index) const;
259 };
260 
261 class Space;
262 
263 //////////////////////////////////////////////////////////////////////////
264 // The BlockOffsetArray whose subtypes use the BlockOffsetSharedArray.
265 //////////////////////////////////////////////////////////////////////////
266 class BlockOffsetArray: public BlockOffsetTable {
267   friend class VMStructs;
268  protected:
269   // The following enums are used by do_block_internal() below
270   enum Action {
271     Action_single,      // BOT records a single block (see single_block())
272     Action_mark,        // BOT marks the start of a block (see mark_block())
273     Action_check        // Check that BOT records block correctly
274                         // (see verify_single_block()).
275   };
276 
277   // The shared array, which is shared with other BlockOffsetArray&#39;s
278   // corresponding to different spaces within a generation or span of
279   // memory.
280   BlockOffsetSharedArray* _array;
281 
282   // The space that owns this subregion.
283   Space* _sp;
284 
285   // If true, array entries are initialized to 0; otherwise, they are
286   // initialized to point backwards to the beginning of the covered region.
287   bool _init_to_zero;
288 
289   // An assertion-checking helper method for the set_remainder*() methods below.
290   void check_reducing_assertion(bool reducing) { _array-&gt;check_reducing_assertion(reducing); }
291 
292   // Sets the entries
293   // corresponding to the cards starting at &quot;start&quot; and ending at &quot;end&quot;
294   // to point back to the card before &quot;start&quot;: the interval [start, end)
295   // is right-open. The last parameter, reducing, indicates whether the
296   // updates to individual entries always reduce the entry from a higher
297   // to a lower value. (For example this would hold true during a temporal
298   // regime during which only block splits were updating the BOT.
299   void set_remainder_to_point_to_start(HeapWord* start, HeapWord* end, bool reducing = false);
300   // Same as above, except that the args here are a card _index_ interval
301   // that is closed: [start_index, end_index]
302   void set_remainder_to_point_to_start_incl(size_t start, size_t end, bool reducing = false);
303 
304   // A helper function for BOT adjustment/verification work
305   void do_block_internal(HeapWord* blk_start, HeapWord* blk_end, Action action, bool reducing = false);
306 
307  public:
308   // The space may not have its bottom and top set yet, which is why the
309   // region is passed as a parameter.  If &quot;init_to_zero&quot; is true, the
310   // elements of the array are initialized to zero.  Otherwise, they are
311   // initialized to point backwards to the beginning.
312   BlockOffsetArray(BlockOffsetSharedArray* array, MemRegion mr,
313                    bool init_to_zero_);
314 
315   // Note: this ought to be part of the constructor, but that would require
316   // &quot;this&quot; to be passed as a parameter to a member constructor for
317   // the containing concrete subtype of Space.
318   // This would be legal C++, but MS VC++ doesn&#39;t allow it.
319   void set_space(Space* sp) { _sp = sp; }
320 
321   // Resets the covered region to the given &quot;mr&quot;.
322   void set_region(MemRegion mr) {
323     _bottom = mr.start();
324     _end = mr.end();
325   }
326 
327   // Note that the committed size of the covered space may have changed,
328   // so the table size might also wish to change.
329   virtual void resize(size_t new_word_size) {
330     HeapWord* new_end = _bottom + new_word_size;
331     if (_end &lt; new_end &amp;&amp; !init_to_zero()) {
332       // verify that the old and new boundaries are also card boundaries
333       assert(_array-&gt;is_card_boundary(_end),
334              &quot;_end not a card boundary&quot;);
335       assert(_array-&gt;is_card_boundary(new_end),
336              &quot;new _end would not be a card boundary&quot;);
337       // set all the newly added cards
338       _array-&gt;set_offset_array(_end, new_end, BOTConstants::N_words);
339     }
340     _end = new_end;  // update _end
341   }
342 
343   // Adjust the BOT to show that it has a single block in the
344   // range [blk_start, blk_start + size). All necessary BOT
345   // cards are adjusted, but _unallocated_block isn&#39;t.
346   void single_block(HeapWord* blk_start, HeapWord* blk_end);
347   void single_block(HeapWord* blk, size_t size) {
348     single_block(blk, blk + size);
349   }
350 
351   // When the alloc_block() call returns, the block offset table should
352   // have enough information such that any subsequent block_start() call
353   // with an argument equal to an address that is within the range
354   // [blk_start, blk_end) would return the value blk_start, provided
355   // there have been no calls in between that reset this information
356   // (e.g. see BlockOffsetArrayNonContigSpace::single_block() call
357   // for an appropriate range covering the said interval).
358   // These methods expect to be called with [blk_start, blk_end)
359   // representing a block of memory in the heap.
360   virtual void alloc_block(HeapWord* blk_start, HeapWord* blk_end);
361   void alloc_block(HeapWord* blk, size_t size) {
362     alloc_block(blk, blk + size);
363   }
364 
365   // If true, initialize array slots with no allocated blocks to zero.
366   // Otherwise, make them point back to the front.
367   bool init_to_zero() { return _init_to_zero; }
368   // Corresponding setter
369   void set_init_to_zero(bool val) {
370     _init_to_zero = val;
371     assert(_array != NULL, &quot;_array should be non-NULL&quot;);
372     _array-&gt;set_init_to_zero(val);
373   }
374 
375   // Debugging
376   // Return the index of the last entry in the &quot;active&quot; region.
377   virtual size_t last_active_index() const = 0;
378   // Verify the block offset table
379   void verify() const;
380   void check_all_cards(size_t left_card, size_t right_card) const;
381 };
382 
383 ////////////////////////////////////////////////////////////////////////////
<a name="3" id="anc3"></a>








































































































384 // A subtype of BlockOffsetArray that takes advantage of the fact
385 // that its underlying space is a ContiguousSpace, so that its &quot;active&quot;
386 // region can be more efficiently tracked (than for a non-contiguous space).
387 ////////////////////////////////////////////////////////////////////////////
388 class BlockOffsetArrayContigSpace: public BlockOffsetArray {
389   friend class VMStructs;
390  private:
391   // allocation boundary at which offset array must be updated
392   HeapWord* _next_offset_threshold;
393   size_t    _next_offset_index;      // index corresponding to that boundary
394 
395   // Work function when allocation start crosses threshold.
396   void alloc_block_work(HeapWord* blk_start, HeapWord* blk_end);
397 
398  public:
399   BlockOffsetArrayContigSpace(BlockOffsetSharedArray* array, MemRegion mr):
400     BlockOffsetArray(array, mr, true) {
401     _next_offset_threshold = NULL;
402     _next_offset_index = 0;
403   }
404 
405   void set_contig_space(ContiguousSpace* sp) { set_space((Space*)sp); }
406 
407   // Initialize the threshold for an empty heap.
408   HeapWord* initialize_threshold();
409   // Zero out the entry for _bottom (offset will be zero)
410   void      zero_bottom_entry();
411 
412   // Return the next threshold, the point at which the table should be
413   // updated.
414   HeapWord* threshold() const { return _next_offset_threshold; }
415 
416   // In general, these methods expect to be called with
417   // [blk_start, blk_end) representing a block of memory in the heap.
418   // In this implementation, however, we are OK even if blk_start and/or
419   // blk_end are NULL because NULL is represented as 0, and thus
420   // never exceeds the &quot;_next_offset_threshold&quot;.
421   void alloc_block(HeapWord* blk_start, HeapWord* blk_end) {
422     if (blk_end &gt; _next_offset_threshold) {
423       alloc_block_work(blk_start, blk_end);
424     }
425   }
426   void alloc_block(HeapWord* blk, size_t size) {
427     alloc_block(blk, blk + size);
428   }
429 
430   HeapWord* block_start_unsafe(const void* addr) const;
431 
432   // Debugging support
433   virtual size_t last_active_index() const;
434 };
435 
436 #endif // SHARE_GC_SHARED_BLOCKOFFSETTABLE_HPP
<a name="4" id="anc4"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="4" type="hidden" />
</body>
</html>