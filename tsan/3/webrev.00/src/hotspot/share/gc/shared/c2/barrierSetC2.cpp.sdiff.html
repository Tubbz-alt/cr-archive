<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/shared/c2/barrierSetC2.cpp</title>
    <link rel="stylesheet" href="../../../../../../style.css" />
  </head>
<body>
<center><a href="../c1/barrierSetC1.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../index.html" target="_top">index</a> <a href="barrierSetC2.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/shared/c2/barrierSetC2.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
 27 #include &quot;opto/arraycopynode.hpp&quot;
 28 #include &quot;opto/convertnode.hpp&quot;
 29 #include &quot;opto/graphKit.hpp&quot;
 30 #include &quot;opto/idealKit.hpp&quot;
 31 #include &quot;opto/macro.hpp&quot;
 32 #include &quot;opto/narrowptrnode.hpp&quot;

 33 #include &quot;utilities/macros.hpp&quot;
 34 
 35 // By default this is a no-op.
 36 void BarrierSetC2::resolve_address(C2Access&amp; access) const { }
 37 
 38 void* C2ParseAccess::barrier_set_state() const {
 39   return _kit-&gt;barrier_set_state();
 40 }
 41 
 42 PhaseGVN&amp; C2ParseAccess::gvn() const { return _kit-&gt;gvn(); }
 43 
 44 bool C2Access::needs_cpu_membar() const {
<span class="line-modified"> 45   bool mismatched = (_decorators &amp; C2_MISMATCHED) != 0;</span>
 46   bool is_unordered = (_decorators &amp; MO_UNORDERED) != 0;

 47   bool anonymous = (_decorators &amp; C2_UNSAFE_ACCESS) != 0;
<span class="line-modified"> 48   bool in_heap = (_decorators &amp; IN_HEAP) != 0;</span>


 49 
<span class="line-modified"> 50   bool is_write = (_decorators &amp; C2_WRITE_ACCESS) != 0;</span>
<span class="line-modified"> 51   bool is_read = (_decorators &amp; C2_READ_ACCESS) != 0;</span>
 52   bool is_atomic = is_read &amp;&amp; is_write;
 53 
 54   if (is_atomic) {
 55     // Atomics always need to be wrapped in CPU membars
 56     return true;
 57   }
 58 
 59   if (anonymous) {
 60     // We will need memory barriers unless we can determine a unique
 61     // alias category for this reference.  (Note:  If for some reason
 62     // the barriers get omitted and the unsafe reference begins to &quot;pollute&quot;
 63     // the alias analysis of the rest of the graph, either Compile::can_alias
 64     // or Compile::must_alias will throw a diagnostic assert.)
<span class="line-modified"> 65     if (!in_heap || !is_unordered || (mismatched &amp;&amp; !_addr.type()-&gt;isa_aryptr())) {</span>
 66       return true;
 67     }


 68   }
 69 
 70   return false;
 71 }
 72 
 73 Node* BarrierSetC2::store_at_resolved(C2Access&amp; access, C2AccessValue&amp; val) const {
 74   DecoratorSet decorators = access.decorators();
 75 
 76   bool mismatched = (decorators &amp; C2_MISMATCHED) != 0;
 77   bool unaligned = (decorators &amp; C2_UNALIGNED) != 0;
 78   bool unsafe = (decorators &amp; C2_UNSAFE_ACCESS) != 0;
 79   bool requires_atomic_access = (decorators &amp; MO_UNORDERED) == 0;
 80 
 81   bool in_native = (decorators &amp; IN_NATIVE) != 0;
<span class="line-modified"> 82   assert(!in_native, &quot;not supported yet&quot;);</span>
 83 
 84   MemNode::MemOrd mo = access.mem_node_mo();
 85 
 86   Node* store;
 87   if (access.is_parse_access()) {
 88     C2ParseAccess&amp; parse_access = static_cast&lt;C2ParseAccess&amp;&gt;(access);
 89 
 90     GraphKit* kit = parse_access.kit();
 91     if (access.type() == T_DOUBLE) {
 92       Node* new_val = kit-&gt;dstore_rounding(val.node());
 93       val.set_node(new_val);
 94     }
 95 
 96     store = kit-&gt;store_to_memory(kit-&gt;control(), access.addr().node(), val.node(), access.type(),
 97                                      access.addr().type(), mo, requires_atomic_access, unaligned, mismatched, unsafe);
<span class="line-removed"> 98     access.set_raw_access(store);</span>
 99   } else {
100     assert(!requires_atomic_access, &quot;not yet supported&quot;);
101     assert(access.is_opt_access(), &quot;either parse or opt access&quot;);
102     C2OptAccess&amp; opt_access = static_cast&lt;C2OptAccess&amp;&gt;(access);
103     Node* ctl = opt_access.ctl();
104     MergeMemNode* mm = opt_access.mem();
105     PhaseGVN&amp; gvn = opt_access.gvn();
106     const TypePtr* adr_type = access.addr().type();
107     int alias = gvn.C-&gt;get_alias_index(adr_type);
108     Node* mem = mm-&gt;memory_at(alias);
109 
110     StoreNode* st = StoreNode::make(gvn, ctl, mem, access.addr().node(), adr_type, val.node(), access.type(), mo);
111     if (unaligned) {
112       st-&gt;set_unaligned_access();
113     }
114     if (mismatched) {
115       st-&gt;set_mismatched_access();
116     }
117     store = gvn.transform(st);
118     if (store == st) {
119       mm-&gt;set_memory_at(alias, st);
120     }
121   }


122   return store;
123 }
124 
125 Node* BarrierSetC2::load_at_resolved(C2Access&amp; access, const Type* val_type) const {
126   DecoratorSet decorators = access.decorators();
127 
128   Node* adr = access.addr().node();
129   const TypePtr* adr_type = access.addr().type();
130 
131   bool mismatched = (decorators &amp; C2_MISMATCHED) != 0;
132   bool requires_atomic_access = (decorators &amp; MO_UNORDERED) == 0;
133   bool unaligned = (decorators &amp; C2_UNALIGNED) != 0;
134   bool control_dependent = (decorators &amp; C2_CONTROL_DEPENDENT_LOAD) != 0;
<span class="line-modified">135   bool pinned = (decorators &amp; C2_PINNED_LOAD) != 0;</span>
136   bool unsafe = (decorators &amp; C2_UNSAFE_ACCESS) != 0;
137 
138   bool in_native = (decorators &amp; IN_NATIVE) != 0;
139 
140   MemNode::MemOrd mo = access.mem_node_mo();
<span class="line-modified">141   LoadNode::ControlDependency dep = pinned ? LoadNode::Pinned : LoadNode::DependsOnlyOnTest;</span>
142 
143   Node* load;
144   if (access.is_parse_access()) {
145     C2ParseAccess&amp; parse_access = static_cast&lt;C2ParseAccess&amp;&gt;(access);
146     GraphKit* kit = parse_access.kit();
147     Node* control = control_dependent ? kit-&gt;control() : NULL;
148 
149     if (in_native) {
<span class="line-modified">150       load = kit-&gt;make_load(control, adr, val_type, access.type(), mo);</span>


151     } else {
152       load = kit-&gt;make_load(control, adr, val_type, access.type(), adr_type, mo,
<span class="line-modified">153                             dep, requires_atomic_access, unaligned, mismatched, unsafe);</span>

154     }
<span class="line-removed">155     access.set_raw_access(load);</span>
156   } else {
157     assert(!requires_atomic_access, &quot;not yet supported&quot;);
158     assert(access.is_opt_access(), &quot;either parse or opt access&quot;);
159     C2OptAccess&amp; opt_access = static_cast&lt;C2OptAccess&amp;&gt;(access);
160     Node* control = control_dependent ? opt_access.ctl() : NULL;
161     MergeMemNode* mm = opt_access.mem();
162     PhaseGVN&amp; gvn = opt_access.gvn();
163     Node* mem = mm-&gt;memory_at(gvn.C-&gt;get_alias_index(adr_type));
<span class="line-modified">164     load = LoadNode::make(gvn, control, mem, adr, adr_type, val_type, access.type(), mo, dep, unaligned, mismatched);</span>

165     load = gvn.transform(load);
166   }

167 
168   return load;
169 }
170 
171 class C2AccessFence: public StackObj {
172   C2Access&amp; _access;
173   Node* _leading_membar;
174 
175 public:
176   C2AccessFence(C2Access&amp; access) :
177     _access(access), _leading_membar(NULL) {
178     GraphKit* kit = NULL;
179     if (access.is_parse_access()) {
180       C2ParseAccess&amp; parse_access = static_cast&lt;C2ParseAccess&amp;&gt;(access);
181       kit = parse_access.kit();
182     }
183     DecoratorSet decorators = access.decorators();
184 
185     bool is_write = (decorators &amp; C2_WRITE_ACCESS) != 0;
186     bool is_read = (decorators &amp; C2_READ_ACCESS) != 0;
</pre>
<hr />
<pre>
332 
333 void C2Access::fixup_decorators() {
334   bool default_mo = (_decorators &amp; MO_DECORATOR_MASK) == 0;
335   bool is_unordered = (_decorators &amp; MO_UNORDERED) != 0 || default_mo;
336   bool anonymous = (_decorators &amp; C2_UNSAFE_ACCESS) != 0;
337 
338   bool is_read = (_decorators &amp; C2_READ_ACCESS) != 0;
339   bool is_write = (_decorators &amp; C2_WRITE_ACCESS) != 0;
340 
341   if (AlwaysAtomicAccesses &amp;&amp; is_unordered) {
342     _decorators &amp;= ~MO_DECORATOR_MASK; // clear the MO bits
343     _decorators |= MO_RELAXED; // Force the MO_RELAXED decorator with AlwaysAtomicAccess
344   }
345 
346   _decorators = AccessInternal::decorator_fixup(_decorators);
347 
348   if (is_read &amp;&amp; !is_write &amp;&amp; anonymous) {
349     // To be valid, unsafe loads may depend on other conditions than
350     // the one that guards them: pin the Load node
351     _decorators |= C2_CONTROL_DEPENDENT_LOAD;
<span class="line-modified">352     _decorators |= C2_PINNED_LOAD;</span>
353     const TypePtr* adr_type = _addr.type();
354     Node* adr = _addr.node();
355     if (!needs_cpu_membar() &amp;&amp; adr_type-&gt;isa_instptr()) {
356       assert(adr_type-&gt;meet(TypePtr::NULL_PTR) != adr_type-&gt;remove_speculative(), &quot;should be not null&quot;);
357       intptr_t offset = Type::OffsetBot;
358       AddPNode::Ideal_base_and_offset(adr, &amp;gvn(), offset);
359       if (offset &gt;= 0) {
360         int s = Klass::layout_helper_size_in_bytes(adr_type-&gt;isa_instptr()-&gt;klass()-&gt;layout_helper());
361         if (offset &lt; s) {
362           // Guaranteed to be a valid access, no need to pin it
363           _decorators ^= C2_CONTROL_DEPENDENT_LOAD;
<span class="line-modified">364           _decorators ^= C2_PINNED_LOAD;</span>
365         }
366       }
367     }
368   }
369 }
370 
371 //--------------------------- atomic operations---------------------------------
372 
373 void BarrierSetC2::pin_atomic_op(C2AtomicParseAccess&amp; access) const {
374   if (!access.needs_pinning()) {
375     return;
376   }
377   // SCMemProjNodes represent the memory state of a LoadStore. Their
378   // main role is to prevent LoadStore nodes from being optimized away
379   // when their results aren&#39;t used.
380   assert(access.is_parse_access(), &quot;entry not supported at optimization time&quot;);
381   C2ParseAccess&amp; parse_access = static_cast&lt;C2ParseAccess&amp;&gt;(access);
382   GraphKit* kit = parse_access.kit();
383   Node* load_store = access.raw_access();
384   assert(load_store != NULL, &quot;must pin atomic op&quot;);
</pre>
<hr />
<pre>
390   Node *mem = _kit-&gt;memory(_alias_idx);
391   _memory = mem;
392 }
393 
394 Node* BarrierSetC2::atomic_cmpxchg_val_at_resolved(C2AtomicParseAccess&amp; access, Node* expected_val,
395                                                    Node* new_val, const Type* value_type) const {
396   GraphKit* kit = access.kit();
397   MemNode::MemOrd mo = access.mem_node_mo();
398   Node* mem = access.memory();
399 
400   Node* adr = access.addr().node();
401   const TypePtr* adr_type = access.addr().type();
402 
403   Node* load_store = NULL;
404 
405   if (access.is_oop()) {
406 #ifdef _LP64
407     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
408       Node *newval_enc = kit-&gt;gvn().transform(new EncodePNode(new_val, new_val-&gt;bottom_type()-&gt;make_narrowoop()));
409       Node *oldval_enc = kit-&gt;gvn().transform(new EncodePNode(expected_val, expected_val-&gt;bottom_type()-&gt;make_narrowoop()));
<span class="line-modified">410       load_store = kit-&gt;gvn().transform(new CompareAndExchangeNNode(kit-&gt;control(), mem, adr, newval_enc, oldval_enc, adr_type, value_type-&gt;make_narrowoop(), mo));</span>
411     } else
412 #endif
413     {
<span class="line-modified">414       load_store = kit-&gt;gvn().transform(new CompareAndExchangePNode(kit-&gt;control(), mem, adr, new_val, expected_val, adr_type, value_type-&gt;is_oopptr(), mo));</span>
415     }
416   } else {
417     switch (access.type()) {
418       case T_BYTE: {
<span class="line-modified">419         load_store = kit-&gt;gvn().transform(new CompareAndExchangeBNode(kit-&gt;control(), mem, adr, new_val, expected_val, adr_type, mo));</span>
420         break;
421       }
422       case T_SHORT: {
<span class="line-modified">423         load_store = kit-&gt;gvn().transform(new CompareAndExchangeSNode(kit-&gt;control(), mem, adr, new_val, expected_val, adr_type, mo));</span>
424         break;
425       }
426       case T_INT: {
<span class="line-modified">427         load_store = kit-&gt;gvn().transform(new CompareAndExchangeINode(kit-&gt;control(), mem, adr, new_val, expected_val, adr_type, mo));</span>
428         break;
429       }
430       case T_LONG: {
<span class="line-modified">431         load_store = kit-&gt;gvn().transform(new CompareAndExchangeLNode(kit-&gt;control(), mem, adr, new_val, expected_val, adr_type, mo));</span>
432         break;
433       }
434       default:
435         ShouldNotReachHere();
436     }
437   }
438 



439   access.set_raw_access(load_store);
440   pin_atomic_op(access);
441 
442 #ifdef _LP64
443   if (access.is_oop() &amp;&amp; adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
444     return kit-&gt;gvn().transform(new DecodeNNode(load_store, load_store-&gt;get_ptr_type()));
445   }
446 #endif
447 
448   return load_store;
449 }
450 
451 Node* BarrierSetC2::atomic_cmpxchg_bool_at_resolved(C2AtomicParseAccess&amp; access, Node* expected_val,
452                                                     Node* new_val, const Type* value_type) const {
453   GraphKit* kit = access.kit();
454   DecoratorSet decorators = access.decorators();
455   MemNode::MemOrd mo = access.mem_node_mo();
456   Node* mem = access.memory();
457   bool is_weak_cas = (decorators &amp; C2_WEAK_CMPXCHG) != 0;
458   Node* load_store = NULL;
459   Node* adr = access.addr().node();
460 
461   if (access.is_oop()) {
462 #ifdef _LP64
463     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
464       Node *newval_enc = kit-&gt;gvn().transform(new EncodePNode(new_val, new_val-&gt;bottom_type()-&gt;make_narrowoop()));
465       Node *oldval_enc = kit-&gt;gvn().transform(new EncodePNode(expected_val, expected_val-&gt;bottom_type()-&gt;make_narrowoop()));
466       if (is_weak_cas) {
<span class="line-modified">467         load_store = kit-&gt;gvn().transform(new WeakCompareAndSwapNNode(kit-&gt;control(), mem, adr, newval_enc, oldval_enc, mo));</span>
468       } else {
<span class="line-modified">469         load_store = kit-&gt;gvn().transform(new CompareAndSwapNNode(kit-&gt;control(), mem, adr, newval_enc, oldval_enc, mo));</span>
470       }
471     } else
472 #endif
473     {
474       if (is_weak_cas) {
<span class="line-modified">475         load_store = kit-&gt;gvn().transform(new WeakCompareAndSwapPNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo));</span>
476       } else {
<span class="line-modified">477         load_store = kit-&gt;gvn().transform(new CompareAndSwapPNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo));</span>
478       }
479     }
480   } else {
481     switch(access.type()) {
482       case T_BYTE: {
483         if (is_weak_cas) {
<span class="line-modified">484           load_store = kit-&gt;gvn().transform(new WeakCompareAndSwapBNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo));</span>
485         } else {
<span class="line-modified">486           load_store = kit-&gt;gvn().transform(new CompareAndSwapBNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo));</span>
487         }
488         break;
489       }
490       case T_SHORT: {
491         if (is_weak_cas) {
<span class="line-modified">492           load_store = kit-&gt;gvn().transform(new WeakCompareAndSwapSNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo));</span>
493         } else {
<span class="line-modified">494           load_store = kit-&gt;gvn().transform(new CompareAndSwapSNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo));</span>
495         }
496         break;
497       }
498       case T_INT: {
499         if (is_weak_cas) {
<span class="line-modified">500           load_store = kit-&gt;gvn().transform(new WeakCompareAndSwapINode(kit-&gt;control(), mem, adr, new_val, expected_val, mo));</span>
501         } else {
<span class="line-modified">502           load_store = kit-&gt;gvn().transform(new CompareAndSwapINode(kit-&gt;control(), mem, adr, new_val, expected_val, mo));</span>
503         }
504         break;
505       }
506       case T_LONG: {
507         if (is_weak_cas) {
<span class="line-modified">508           load_store = kit-&gt;gvn().transform(new WeakCompareAndSwapLNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo));</span>
509         } else {
<span class="line-modified">510           load_store = kit-&gt;gvn().transform(new CompareAndSwapLNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo));</span>
511         }
512         break;
513       }
514       default:
515         ShouldNotReachHere();
516     }
517   }
518 



519   access.set_raw_access(load_store);
520   pin_atomic_op(access);
521 
522   return load_store;
523 }
524 
525 Node* BarrierSetC2::atomic_xchg_at_resolved(C2AtomicParseAccess&amp; access, Node* new_val, const Type* value_type) const {
526   GraphKit* kit = access.kit();
527   Node* mem = access.memory();
528   Node* adr = access.addr().node();
529   const TypePtr* adr_type = access.addr().type();
530   Node* load_store = NULL;
531 
532   if (access.is_oop()) {
533 #ifdef _LP64
534     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
535       Node *newval_enc = kit-&gt;gvn().transform(new EncodePNode(new_val, new_val-&gt;bottom_type()-&gt;make_narrowoop()));
536       load_store = kit-&gt;gvn().transform(new GetAndSetNNode(kit-&gt;control(), mem, adr, newval_enc, adr_type, value_type-&gt;make_narrowoop()));
537     } else
538 #endif
539     {
<span class="line-modified">540       load_store = kit-&gt;gvn().transform(new GetAndSetPNode(kit-&gt;control(), mem, adr, new_val, adr_type, value_type-&gt;is_oopptr()));</span>
541     }
542   } else  {
543     switch (access.type()) {
544       case T_BYTE:
<span class="line-modified">545         load_store = kit-&gt;gvn().transform(new GetAndSetBNode(kit-&gt;control(), mem, adr, new_val, adr_type));</span>
546         break;
547       case T_SHORT:
<span class="line-modified">548         load_store = kit-&gt;gvn().transform(new GetAndSetSNode(kit-&gt;control(), mem, adr, new_val, adr_type));</span>
549         break;
550       case T_INT:
<span class="line-modified">551         load_store = kit-&gt;gvn().transform(new GetAndSetINode(kit-&gt;control(), mem, adr, new_val, adr_type));</span>
552         break;
553       case T_LONG:
<span class="line-modified">554         load_store = kit-&gt;gvn().transform(new GetAndSetLNode(kit-&gt;control(), mem, adr, new_val, adr_type));</span>
555         break;
556       default:
557         ShouldNotReachHere();
558     }
559   }
560 



561   access.set_raw_access(load_store);
562   pin_atomic_op(access);
563 
564 #ifdef _LP64
565   if (access.is_oop() &amp;&amp; adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
566     return kit-&gt;gvn().transform(new DecodeNNode(load_store, load_store-&gt;get_ptr_type()));
567   }
568 #endif
569 
570   return load_store;
571 }
572 
573 Node* BarrierSetC2::atomic_add_at_resolved(C2AtomicParseAccess&amp; access, Node* new_val, const Type* value_type) const {
574   Node* load_store = NULL;
575   GraphKit* kit = access.kit();
576   Node* adr = access.addr().node();
577   const TypePtr* adr_type = access.addr().type();
578   Node* mem = access.memory();
579 
580   switch(access.type()) {
581     case T_BYTE:
<span class="line-modified">582       load_store = kit-&gt;gvn().transform(new GetAndAddBNode(kit-&gt;control(), mem, adr, new_val, adr_type));</span>
583       break;
584     case T_SHORT:
<span class="line-modified">585       load_store = kit-&gt;gvn().transform(new GetAndAddSNode(kit-&gt;control(), mem, adr, new_val, adr_type));</span>
586       break;
587     case T_INT:
<span class="line-modified">588       load_store = kit-&gt;gvn().transform(new GetAndAddINode(kit-&gt;control(), mem, adr, new_val, adr_type));</span>
589       break;
590     case T_LONG:
<span class="line-modified">591       load_store = kit-&gt;gvn().transform(new GetAndAddLNode(kit-&gt;control(), mem, adr, new_val, adr_type));</span>
592       break;
593     default:
594       ShouldNotReachHere();
595   }
596 



597   access.set_raw_access(load_store);
598   pin_atomic_op(access);
599 
600   return load_store;
601 }
602 
603 Node* BarrierSetC2::atomic_cmpxchg_val_at(C2AtomicParseAccess&amp; access, Node* expected_val,
604                                           Node* new_val, const Type* value_type) const {
605   C2AccessFence fence(access);
606   resolve_address(access);
607   return atomic_cmpxchg_val_at_resolved(access, expected_val, new_val, value_type);
608 }
609 
610 Node* BarrierSetC2::atomic_cmpxchg_bool_at(C2AtomicParseAccess&amp; access, Node* expected_val,
611                                            Node* new_val, const Type* value_type) const {
612   C2AccessFence fence(access);
613   resolve_address(access);
614   return atomic_cmpxchg_bool_at_resolved(access, expected_val, new_val, value_type);
615 }
616 
617 Node* BarrierSetC2::atomic_xchg_at(C2AtomicParseAccess&amp; access, Node* new_val, const Type* value_type) const {
618   C2AccessFence fence(access);
619   resolve_address(access);
620   return atomic_xchg_at_resolved(access, new_val, value_type);
621 }
622 
623 Node* BarrierSetC2::atomic_add_at(C2AtomicParseAccess&amp; access, Node* new_val, const Type* value_type) const {
624   C2AccessFence fence(access);
625   resolve_address(access);
626   return atomic_add_at_resolved(access, new_val, value_type);
627 }
628 
<span class="line-modified">629 void BarrierSetC2::clone(GraphKit* kit, Node* src, Node* dst, Node* size, bool is_array) const {</span>
630   // Exclude the header but include array length to copy by 8 bytes words.
631   // Can&#39;t use base_offset_in_bytes(bt) since basic type is unknown.
632   int base_off = is_array ? arrayOopDesc::length_offset_in_bytes() :
<span class="line-modified">633                             instanceOopDesc::base_offset_in_bytes();</span>
634   // base_off:
635   // 8  - 32-bit VM
636   // 12 - 64-bit VM, compressed klass
637   // 16 - 64-bit VM, normal klass
638   if (base_off % BytesPerLong != 0) {
639     assert(UseCompressedClassPointers, &quot;&quot;);
640     if (is_array) {
641       // Exclude length to copy by 8 bytes words.
642       base_off += sizeof(int);
643     } else {
644       // Include klass to copy by 8 bytes words.
645       base_off = instanceOopDesc::klass_offset_in_bytes();
646     }
647     assert(base_off % BytesPerLong == 0, &quot;expect 8 bytes alignment&quot;);
648   }
<span class="line-modified">649   Node* src_base  = kit-&gt;basic_plus_adr(src,  base_off);</span>
<span class="line-modified">650   Node* dst_base = kit-&gt;basic_plus_adr(dst, base_off);</span>





651 
652   // Compute the length also, if needed:
<span class="line-modified">653   Node* countx = size;</span>
<span class="line-modified">654   countx = kit-&gt;gvn().transform(new SubXNode(countx, kit-&gt;MakeConX(base_off)));</span>
<span class="line-modified">655   countx = kit-&gt;gvn().transform(new URShiftXNode(countx, kit-&gt;intcon(LogBytesPerLong) ));</span>
656 
657   const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
658 
<span class="line-modified">659   ArrayCopyNode* ac = ArrayCopyNode::make(kit, false, src_base, NULL, dst_base, NULL, countx, true, false);</span>
<span class="line-modified">660   ac-&gt;set_clonebasic();</span>




661   Node* n = kit-&gt;gvn().transform(ac);
662   if (n == ac) {
663     ac-&gt;_adr_type = TypeRawPtr::BOTTOM;
664     kit-&gt;set_predefined_output_for_runtime_call(ac, ac-&gt;in(TypeFunc::Memory), raw_adr_type);
665   } else {
666     kit-&gt;set_all_memory(n);
667   }
668 }
669 
670 Node* BarrierSetC2::obj_allocate(PhaseMacroExpand* macro, Node* ctrl, Node* mem, Node* toobig_false, Node* size_in_bytes,
671                                  Node*&amp; i_o, Node*&amp; needgc_ctrl,
672                                  Node*&amp; fast_oop_ctrl, Node*&amp; fast_oop_rawmem,
673                                  intx prefetch_lines) const {
674 
675   Node* eden_top_adr;
676   Node* eden_end_adr;
677 
678   macro-&gt;set_eden_pointers(eden_top_adr, eden_end_adr);
679 
680   // Load Eden::end.  Loop invariant and hoisted.
</pre>
<hr />
<pre>
777     Node* thread = new ThreadLocalNode();
778     macro-&gt;transform_later(thread);
779     Node* alloc_bytes_adr = macro-&gt;basic_plus_adr(macro-&gt;top()/*not oop*/, thread,
780                                                   in_bytes(JavaThread::allocated_bytes_offset()));
781     Node* alloc_bytes = macro-&gt;make_load(fast_oop_ctrl, store_eden_top, alloc_bytes_adr,
782                                          0, TypeLong::LONG, T_LONG);
783 #ifdef _LP64
784     Node* alloc_size = size_in_bytes;
785 #else
786     Node* alloc_size = new ConvI2LNode(size_in_bytes);
787     macro-&gt;transform_later(alloc_size);
788 #endif
789     Node* new_alloc_bytes = new AddLNode(alloc_bytes, alloc_size);
790     macro-&gt;transform_later(new_alloc_bytes);
791     fast_oop_rawmem = macro-&gt;make_store(fast_oop_ctrl, store_eden_top, alloc_bytes_adr,
792                                         0, new_alloc_bytes, T_LONG);
793   }
794   return fast_oop;
795 }
796 
<span class="line-modified">797 void BarrierSetC2::clone_barrier_at_expansion(ArrayCopyNode* ac, Node* call, PhaseIterGVN&amp; igvn) const {</span>
<span class="line-modified">798   // no barrier</span>
<span class="line-modified">799   igvn.replace_node(ac, call);</span>























800 }
</pre>
</td>
<td>
<hr />
<pre>
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
 27 #include &quot;opto/arraycopynode.hpp&quot;
 28 #include &quot;opto/convertnode.hpp&quot;
 29 #include &quot;opto/graphKit.hpp&quot;
 30 #include &quot;opto/idealKit.hpp&quot;
 31 #include &quot;opto/macro.hpp&quot;
 32 #include &quot;opto/narrowptrnode.hpp&quot;
<span class="line-added"> 33 #include &quot;opto/runtime.hpp&quot;</span>
 34 #include &quot;utilities/macros.hpp&quot;
 35 
 36 // By default this is a no-op.
 37 void BarrierSetC2::resolve_address(C2Access&amp; access) const { }
 38 
 39 void* C2ParseAccess::barrier_set_state() const {
 40   return _kit-&gt;barrier_set_state();
 41 }
 42 
 43 PhaseGVN&amp; C2ParseAccess::gvn() const { return _kit-&gt;gvn(); }
 44 
 45 bool C2Access::needs_cpu_membar() const {
<span class="line-modified"> 46   bool mismatched   = (_decorators &amp; C2_MISMATCHED) != 0;</span>
 47   bool is_unordered = (_decorators &amp; MO_UNORDERED) != 0;
<span class="line-added"> 48 </span>
 49   bool anonymous = (_decorators &amp; C2_UNSAFE_ACCESS) != 0;
<span class="line-modified"> 50   bool in_heap   = (_decorators &amp; IN_HEAP) != 0;</span>
<span class="line-added"> 51   bool in_native = (_decorators &amp; IN_NATIVE) != 0;</span>
<span class="line-added"> 52   bool is_mixed  = !in_heap &amp;&amp; !in_native;</span>
 53 
<span class="line-modified"> 54   bool is_write  = (_decorators &amp; C2_WRITE_ACCESS) != 0;</span>
<span class="line-modified"> 55   bool is_read   = (_decorators &amp; C2_READ_ACCESS) != 0;</span>
 56   bool is_atomic = is_read &amp;&amp; is_write;
 57 
 58   if (is_atomic) {
 59     // Atomics always need to be wrapped in CPU membars
 60     return true;
 61   }
 62 
 63   if (anonymous) {
 64     // We will need memory barriers unless we can determine a unique
 65     // alias category for this reference.  (Note:  If for some reason
 66     // the barriers get omitted and the unsafe reference begins to &quot;pollute&quot;
 67     // the alias analysis of the rest of the graph, either Compile::can_alias
 68     // or Compile::must_alias will throw a diagnostic assert.)
<span class="line-modified"> 69     if (is_mixed || !is_unordered || (mismatched &amp;&amp; !_addr.type()-&gt;isa_aryptr())) {</span>
 70       return true;
 71     }
<span class="line-added"> 72   } else {</span>
<span class="line-added"> 73     assert(!is_mixed, &quot;not unsafe&quot;);</span>
 74   }
 75 
 76   return false;
 77 }
 78 
 79 Node* BarrierSetC2::store_at_resolved(C2Access&amp; access, C2AccessValue&amp; val) const {
 80   DecoratorSet decorators = access.decorators();
 81 
 82   bool mismatched = (decorators &amp; C2_MISMATCHED) != 0;
 83   bool unaligned = (decorators &amp; C2_UNALIGNED) != 0;
 84   bool unsafe = (decorators &amp; C2_UNSAFE_ACCESS) != 0;
 85   bool requires_atomic_access = (decorators &amp; MO_UNORDERED) == 0;
 86 
 87   bool in_native = (decorators &amp; IN_NATIVE) != 0;
<span class="line-modified"> 88   assert(!in_native || (unsafe &amp;&amp; !access.is_oop()), &quot;not supported yet&quot;);</span>
 89 
 90   MemNode::MemOrd mo = access.mem_node_mo();
 91 
 92   Node* store;
 93   if (access.is_parse_access()) {
 94     C2ParseAccess&amp; parse_access = static_cast&lt;C2ParseAccess&amp;&gt;(access);
 95 
 96     GraphKit* kit = parse_access.kit();
 97     if (access.type() == T_DOUBLE) {
 98       Node* new_val = kit-&gt;dstore_rounding(val.node());
 99       val.set_node(new_val);
100     }
101 
102     store = kit-&gt;store_to_memory(kit-&gt;control(), access.addr().node(), val.node(), access.type(),
103                                      access.addr().type(), mo, requires_atomic_access, unaligned, mismatched, unsafe);

104   } else {
105     assert(!requires_atomic_access, &quot;not yet supported&quot;);
106     assert(access.is_opt_access(), &quot;either parse or opt access&quot;);
107     C2OptAccess&amp; opt_access = static_cast&lt;C2OptAccess&amp;&gt;(access);
108     Node* ctl = opt_access.ctl();
109     MergeMemNode* mm = opt_access.mem();
110     PhaseGVN&amp; gvn = opt_access.gvn();
111     const TypePtr* adr_type = access.addr().type();
112     int alias = gvn.C-&gt;get_alias_index(adr_type);
113     Node* mem = mm-&gt;memory_at(alias);
114 
115     StoreNode* st = StoreNode::make(gvn, ctl, mem, access.addr().node(), adr_type, val.node(), access.type(), mo);
116     if (unaligned) {
117       st-&gt;set_unaligned_access();
118     }
119     if (mismatched) {
120       st-&gt;set_mismatched_access();
121     }
122     store = gvn.transform(st);
123     if (store == st) {
124       mm-&gt;set_memory_at(alias, st);
125     }
126   }
<span class="line-added">127   access.set_raw_access(store);</span>
<span class="line-added">128 </span>
129   return store;
130 }
131 
132 Node* BarrierSetC2::load_at_resolved(C2Access&amp; access, const Type* val_type) const {
133   DecoratorSet decorators = access.decorators();
134 
135   Node* adr = access.addr().node();
136   const TypePtr* adr_type = access.addr().type();
137 
138   bool mismatched = (decorators &amp; C2_MISMATCHED) != 0;
139   bool requires_atomic_access = (decorators &amp; MO_UNORDERED) == 0;
140   bool unaligned = (decorators &amp; C2_UNALIGNED) != 0;
141   bool control_dependent = (decorators &amp; C2_CONTROL_DEPENDENT_LOAD) != 0;
<span class="line-modified">142   bool unknown_control = (decorators &amp; C2_UNKNOWN_CONTROL_LOAD) != 0;</span>
143   bool unsafe = (decorators &amp; C2_UNSAFE_ACCESS) != 0;
144 
145   bool in_native = (decorators &amp; IN_NATIVE) != 0;
146 
147   MemNode::MemOrd mo = access.mem_node_mo();
<span class="line-modified">148   LoadNode::ControlDependency dep = unknown_control ? LoadNode::UnknownControl : LoadNode::DependsOnlyOnTest;</span>
149 
150   Node* load;
151   if (access.is_parse_access()) {
152     C2ParseAccess&amp; parse_access = static_cast&lt;C2ParseAccess&amp;&gt;(access);
153     GraphKit* kit = parse_access.kit();
154     Node* control = control_dependent ? kit-&gt;control() : NULL;
155 
156     if (in_native) {
<span class="line-modified">157       load = kit-&gt;make_load(control, adr, val_type, access.type(), mo, dep,</span>
<span class="line-added">158                             requires_atomic_access, unaligned,</span>
<span class="line-added">159                             mismatched, unsafe, access.barrier_data());</span>
160     } else {
161       load = kit-&gt;make_load(control, adr, val_type, access.type(), adr_type, mo,
<span class="line-modified">162                             dep, requires_atomic_access, unaligned, mismatched, unsafe,</span>
<span class="line-added">163                             access.barrier_data());</span>
164     }

165   } else {
166     assert(!requires_atomic_access, &quot;not yet supported&quot;);
167     assert(access.is_opt_access(), &quot;either parse or opt access&quot;);
168     C2OptAccess&amp; opt_access = static_cast&lt;C2OptAccess&amp;&gt;(access);
169     Node* control = control_dependent ? opt_access.ctl() : NULL;
170     MergeMemNode* mm = opt_access.mem();
171     PhaseGVN&amp; gvn = opt_access.gvn();
172     Node* mem = mm-&gt;memory_at(gvn.C-&gt;get_alias_index(adr_type));
<span class="line-modified">173     load = LoadNode::make(gvn, control, mem, adr, adr_type, val_type, access.type(), mo,</span>
<span class="line-added">174                           dep, unaligned, mismatched, unsafe, access.barrier_data());</span>
175     load = gvn.transform(load);
176   }
<span class="line-added">177   access.set_raw_access(load);</span>
178 
179   return load;
180 }
181 
182 class C2AccessFence: public StackObj {
183   C2Access&amp; _access;
184   Node* _leading_membar;
185 
186 public:
187   C2AccessFence(C2Access&amp; access) :
188     _access(access), _leading_membar(NULL) {
189     GraphKit* kit = NULL;
190     if (access.is_parse_access()) {
191       C2ParseAccess&amp; parse_access = static_cast&lt;C2ParseAccess&amp;&gt;(access);
192       kit = parse_access.kit();
193     }
194     DecoratorSet decorators = access.decorators();
195 
196     bool is_write = (decorators &amp; C2_WRITE_ACCESS) != 0;
197     bool is_read = (decorators &amp; C2_READ_ACCESS) != 0;
</pre>
<hr />
<pre>
343 
344 void C2Access::fixup_decorators() {
345   bool default_mo = (_decorators &amp; MO_DECORATOR_MASK) == 0;
346   bool is_unordered = (_decorators &amp; MO_UNORDERED) != 0 || default_mo;
347   bool anonymous = (_decorators &amp; C2_UNSAFE_ACCESS) != 0;
348 
349   bool is_read = (_decorators &amp; C2_READ_ACCESS) != 0;
350   bool is_write = (_decorators &amp; C2_WRITE_ACCESS) != 0;
351 
352   if (AlwaysAtomicAccesses &amp;&amp; is_unordered) {
353     _decorators &amp;= ~MO_DECORATOR_MASK; // clear the MO bits
354     _decorators |= MO_RELAXED; // Force the MO_RELAXED decorator with AlwaysAtomicAccess
355   }
356 
357   _decorators = AccessInternal::decorator_fixup(_decorators);
358 
359   if (is_read &amp;&amp; !is_write &amp;&amp; anonymous) {
360     // To be valid, unsafe loads may depend on other conditions than
361     // the one that guards them: pin the Load node
362     _decorators |= C2_CONTROL_DEPENDENT_LOAD;
<span class="line-modified">363     _decorators |= C2_UNKNOWN_CONTROL_LOAD;</span>
364     const TypePtr* adr_type = _addr.type();
365     Node* adr = _addr.node();
366     if (!needs_cpu_membar() &amp;&amp; adr_type-&gt;isa_instptr()) {
367       assert(adr_type-&gt;meet(TypePtr::NULL_PTR) != adr_type-&gt;remove_speculative(), &quot;should be not null&quot;);
368       intptr_t offset = Type::OffsetBot;
369       AddPNode::Ideal_base_and_offset(adr, &amp;gvn(), offset);
370       if (offset &gt;= 0) {
371         int s = Klass::layout_helper_size_in_bytes(adr_type-&gt;isa_instptr()-&gt;klass()-&gt;layout_helper());
372         if (offset &lt; s) {
373           // Guaranteed to be a valid access, no need to pin it
374           _decorators ^= C2_CONTROL_DEPENDENT_LOAD;
<span class="line-modified">375           _decorators ^= C2_UNKNOWN_CONTROL_LOAD;</span>
376         }
377       }
378     }
379   }
380 }
381 
382 //--------------------------- atomic operations---------------------------------
383 
384 void BarrierSetC2::pin_atomic_op(C2AtomicParseAccess&amp; access) const {
385   if (!access.needs_pinning()) {
386     return;
387   }
388   // SCMemProjNodes represent the memory state of a LoadStore. Their
389   // main role is to prevent LoadStore nodes from being optimized away
390   // when their results aren&#39;t used.
391   assert(access.is_parse_access(), &quot;entry not supported at optimization time&quot;);
392   C2ParseAccess&amp; parse_access = static_cast&lt;C2ParseAccess&amp;&gt;(access);
393   GraphKit* kit = parse_access.kit();
394   Node* load_store = access.raw_access();
395   assert(load_store != NULL, &quot;must pin atomic op&quot;);
</pre>
<hr />
<pre>
401   Node *mem = _kit-&gt;memory(_alias_idx);
402   _memory = mem;
403 }
404 
405 Node* BarrierSetC2::atomic_cmpxchg_val_at_resolved(C2AtomicParseAccess&amp; access, Node* expected_val,
406                                                    Node* new_val, const Type* value_type) const {
407   GraphKit* kit = access.kit();
408   MemNode::MemOrd mo = access.mem_node_mo();
409   Node* mem = access.memory();
410 
411   Node* adr = access.addr().node();
412   const TypePtr* adr_type = access.addr().type();
413 
414   Node* load_store = NULL;
415 
416   if (access.is_oop()) {
417 #ifdef _LP64
418     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
419       Node *newval_enc = kit-&gt;gvn().transform(new EncodePNode(new_val, new_val-&gt;bottom_type()-&gt;make_narrowoop()));
420       Node *oldval_enc = kit-&gt;gvn().transform(new EncodePNode(expected_val, expected_val-&gt;bottom_type()-&gt;make_narrowoop()));
<span class="line-modified">421       load_store = new CompareAndExchangeNNode(kit-&gt;control(), mem, adr, newval_enc, oldval_enc, adr_type, value_type-&gt;make_narrowoop(), mo);</span>
422     } else
423 #endif
424     {
<span class="line-modified">425       load_store = new CompareAndExchangePNode(kit-&gt;control(), mem, adr, new_val, expected_val, adr_type, value_type-&gt;is_oopptr(), mo);</span>
426     }
427   } else {
428     switch (access.type()) {
429       case T_BYTE: {
<span class="line-modified">430         load_store = new CompareAndExchangeBNode(kit-&gt;control(), mem, adr, new_val, expected_val, adr_type, mo);</span>
431         break;
432       }
433       case T_SHORT: {
<span class="line-modified">434         load_store = new CompareAndExchangeSNode(kit-&gt;control(), mem, adr, new_val, expected_val, adr_type, mo);</span>
435         break;
436       }
437       case T_INT: {
<span class="line-modified">438         load_store = new CompareAndExchangeINode(kit-&gt;control(), mem, adr, new_val, expected_val, adr_type, mo);</span>
439         break;
440       }
441       case T_LONG: {
<span class="line-modified">442         load_store = new CompareAndExchangeLNode(kit-&gt;control(), mem, adr, new_val, expected_val, adr_type, mo);</span>
443         break;
444       }
445       default:
446         ShouldNotReachHere();
447     }
448   }
449 
<span class="line-added">450   load_store-&gt;as_LoadStore()-&gt;set_barrier_data(access.barrier_data());</span>
<span class="line-added">451   load_store = kit-&gt;gvn().transform(load_store);</span>
<span class="line-added">452 </span>
453   access.set_raw_access(load_store);
454   pin_atomic_op(access);
455 
456 #ifdef _LP64
457   if (access.is_oop() &amp;&amp; adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
458     return kit-&gt;gvn().transform(new DecodeNNode(load_store, load_store-&gt;get_ptr_type()));
459   }
460 #endif
461 
462   return load_store;
463 }
464 
465 Node* BarrierSetC2::atomic_cmpxchg_bool_at_resolved(C2AtomicParseAccess&amp; access, Node* expected_val,
466                                                     Node* new_val, const Type* value_type) const {
467   GraphKit* kit = access.kit();
468   DecoratorSet decorators = access.decorators();
469   MemNode::MemOrd mo = access.mem_node_mo();
470   Node* mem = access.memory();
471   bool is_weak_cas = (decorators &amp; C2_WEAK_CMPXCHG) != 0;
472   Node* load_store = NULL;
473   Node* adr = access.addr().node();
474 
475   if (access.is_oop()) {
476 #ifdef _LP64
477     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
478       Node *newval_enc = kit-&gt;gvn().transform(new EncodePNode(new_val, new_val-&gt;bottom_type()-&gt;make_narrowoop()));
479       Node *oldval_enc = kit-&gt;gvn().transform(new EncodePNode(expected_val, expected_val-&gt;bottom_type()-&gt;make_narrowoop()));
480       if (is_weak_cas) {
<span class="line-modified">481         load_store = new WeakCompareAndSwapNNode(kit-&gt;control(), mem, adr, newval_enc, oldval_enc, mo);</span>
482       } else {
<span class="line-modified">483         load_store = new CompareAndSwapNNode(kit-&gt;control(), mem, adr, newval_enc, oldval_enc, mo);</span>
484       }
485     } else
486 #endif
487     {
488       if (is_weak_cas) {
<span class="line-modified">489         load_store = new WeakCompareAndSwapPNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);</span>
490       } else {
<span class="line-modified">491         load_store = new CompareAndSwapPNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);</span>
492       }
493     }
494   } else {
495     switch(access.type()) {
496       case T_BYTE: {
497         if (is_weak_cas) {
<span class="line-modified">498           load_store = new WeakCompareAndSwapBNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);</span>
499         } else {
<span class="line-modified">500           load_store = new CompareAndSwapBNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);</span>
501         }
502         break;
503       }
504       case T_SHORT: {
505         if (is_weak_cas) {
<span class="line-modified">506           load_store = new WeakCompareAndSwapSNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);</span>
507         } else {
<span class="line-modified">508           load_store = new CompareAndSwapSNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);</span>
509         }
510         break;
511       }
512       case T_INT: {
513         if (is_weak_cas) {
<span class="line-modified">514           load_store = new WeakCompareAndSwapINode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);</span>
515         } else {
<span class="line-modified">516           load_store = new CompareAndSwapINode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);</span>
517         }
518         break;
519       }
520       case T_LONG: {
521         if (is_weak_cas) {
<span class="line-modified">522           load_store = new WeakCompareAndSwapLNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);</span>
523         } else {
<span class="line-modified">524           load_store = new CompareAndSwapLNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);</span>
525         }
526         break;
527       }
528       default:
529         ShouldNotReachHere();
530     }
531   }
532 
<span class="line-added">533   load_store-&gt;as_LoadStore()-&gt;set_barrier_data(access.barrier_data());</span>
<span class="line-added">534   load_store = kit-&gt;gvn().transform(load_store);</span>
<span class="line-added">535 </span>
536   access.set_raw_access(load_store);
537   pin_atomic_op(access);
538 
539   return load_store;
540 }
541 
542 Node* BarrierSetC2::atomic_xchg_at_resolved(C2AtomicParseAccess&amp; access, Node* new_val, const Type* value_type) const {
543   GraphKit* kit = access.kit();
544   Node* mem = access.memory();
545   Node* adr = access.addr().node();
546   const TypePtr* adr_type = access.addr().type();
547   Node* load_store = NULL;
548 
549   if (access.is_oop()) {
550 #ifdef _LP64
551     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
552       Node *newval_enc = kit-&gt;gvn().transform(new EncodePNode(new_val, new_val-&gt;bottom_type()-&gt;make_narrowoop()));
553       load_store = kit-&gt;gvn().transform(new GetAndSetNNode(kit-&gt;control(), mem, adr, newval_enc, adr_type, value_type-&gt;make_narrowoop()));
554     } else
555 #endif
556     {
<span class="line-modified">557       load_store = new GetAndSetPNode(kit-&gt;control(), mem, adr, new_val, adr_type, value_type-&gt;is_oopptr());</span>
558     }
559   } else  {
560     switch (access.type()) {
561       case T_BYTE:
<span class="line-modified">562         load_store = new GetAndSetBNode(kit-&gt;control(), mem, adr, new_val, adr_type);</span>
563         break;
564       case T_SHORT:
<span class="line-modified">565         load_store = new GetAndSetSNode(kit-&gt;control(), mem, adr, new_val, adr_type);</span>
566         break;
567       case T_INT:
<span class="line-modified">568         load_store = new GetAndSetINode(kit-&gt;control(), mem, adr, new_val, adr_type);</span>
569         break;
570       case T_LONG:
<span class="line-modified">571         load_store = new GetAndSetLNode(kit-&gt;control(), mem, adr, new_val, adr_type);</span>
572         break;
573       default:
574         ShouldNotReachHere();
575     }
576   }
577 
<span class="line-added">578   load_store-&gt;as_LoadStore()-&gt;set_barrier_data(access.barrier_data());</span>
<span class="line-added">579   load_store = kit-&gt;gvn().transform(load_store);</span>
<span class="line-added">580 </span>
581   access.set_raw_access(load_store);
582   pin_atomic_op(access);
583 
584 #ifdef _LP64
585   if (access.is_oop() &amp;&amp; adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
586     return kit-&gt;gvn().transform(new DecodeNNode(load_store, load_store-&gt;get_ptr_type()));
587   }
588 #endif
589 
590   return load_store;
591 }
592 
593 Node* BarrierSetC2::atomic_add_at_resolved(C2AtomicParseAccess&amp; access, Node* new_val, const Type* value_type) const {
594   Node* load_store = NULL;
595   GraphKit* kit = access.kit();
596   Node* adr = access.addr().node();
597   const TypePtr* adr_type = access.addr().type();
598   Node* mem = access.memory();
599 
600   switch(access.type()) {
601     case T_BYTE:
<span class="line-modified">602       load_store = new GetAndAddBNode(kit-&gt;control(), mem, adr, new_val, adr_type);</span>
603       break;
604     case T_SHORT:
<span class="line-modified">605       load_store = new GetAndAddSNode(kit-&gt;control(), mem, adr, new_val, adr_type);</span>
606       break;
607     case T_INT:
<span class="line-modified">608       load_store = new GetAndAddINode(kit-&gt;control(), mem, adr, new_val, adr_type);</span>
609       break;
610     case T_LONG:
<span class="line-modified">611       load_store = new GetAndAddLNode(kit-&gt;control(), mem, adr, new_val, adr_type);</span>
612       break;
613     default:
614       ShouldNotReachHere();
615   }
616 
<span class="line-added">617   load_store-&gt;as_LoadStore()-&gt;set_barrier_data(access.barrier_data());</span>
<span class="line-added">618   load_store = kit-&gt;gvn().transform(load_store);</span>
<span class="line-added">619 </span>
620   access.set_raw_access(load_store);
621   pin_atomic_op(access);
622 
623   return load_store;
624 }
625 
626 Node* BarrierSetC2::atomic_cmpxchg_val_at(C2AtomicParseAccess&amp; access, Node* expected_val,
627                                           Node* new_val, const Type* value_type) const {
628   C2AccessFence fence(access);
629   resolve_address(access);
630   return atomic_cmpxchg_val_at_resolved(access, expected_val, new_val, value_type);
631 }
632 
633 Node* BarrierSetC2::atomic_cmpxchg_bool_at(C2AtomicParseAccess&amp; access, Node* expected_val,
634                                            Node* new_val, const Type* value_type) const {
635   C2AccessFence fence(access);
636   resolve_address(access);
637   return atomic_cmpxchg_bool_at_resolved(access, expected_val, new_val, value_type);
638 }
639 
640 Node* BarrierSetC2::atomic_xchg_at(C2AtomicParseAccess&amp; access, Node* new_val, const Type* value_type) const {
641   C2AccessFence fence(access);
642   resolve_address(access);
643   return atomic_xchg_at_resolved(access, new_val, value_type);
644 }
645 
646 Node* BarrierSetC2::atomic_add_at(C2AtomicParseAccess&amp; access, Node* new_val, const Type* value_type) const {
647   C2AccessFence fence(access);
648   resolve_address(access);
649   return atomic_add_at_resolved(access, new_val, value_type);
650 }
651 
<span class="line-modified">652 int BarrierSetC2::arraycopy_payload_base_offset(bool is_array) {</span>
653   // Exclude the header but include array length to copy by 8 bytes words.
654   // Can&#39;t use base_offset_in_bytes(bt) since basic type is unknown.
655   int base_off = is_array ? arrayOopDesc::length_offset_in_bytes() :
<span class="line-modified">656                  instanceOopDesc::base_offset_in_bytes();</span>
657   // base_off:
658   // 8  - 32-bit VM
659   // 12 - 64-bit VM, compressed klass
660   // 16 - 64-bit VM, normal klass
661   if (base_off % BytesPerLong != 0) {
662     assert(UseCompressedClassPointers, &quot;&quot;);
663     if (is_array) {
664       // Exclude length to copy by 8 bytes words.
665       base_off += sizeof(int);
666     } else {
667       // Include klass to copy by 8 bytes words.
668       base_off = instanceOopDesc::klass_offset_in_bytes();
669     }
670     assert(base_off % BytesPerLong == 0, &quot;expect 8 bytes alignment&quot;);
671   }
<span class="line-modified">672   return base_off;</span>
<span class="line-modified">673 }</span>
<span class="line-added">674 </span>
<span class="line-added">675 void BarrierSetC2::clone(GraphKit* kit, Node* src_base, Node* dst_base, Node* size, bool is_array) const {</span>
<span class="line-added">676   int base_off = arraycopy_payload_base_offset(is_array);</span>
<span class="line-added">677   Node* payload_src = kit-&gt;basic_plus_adr(src_base,  base_off);</span>
<span class="line-added">678   Node* payload_dst = kit-&gt;basic_plus_adr(dst_base, base_off);</span>
679 
680   // Compute the length also, if needed:
<span class="line-modified">681   Node* payload_size = size;</span>
<span class="line-modified">682   payload_size = kit-&gt;gvn().transform(new SubXNode(payload_size, kit-&gt;MakeConX(base_off)));</span>
<span class="line-modified">683   payload_size = kit-&gt;gvn().transform(new URShiftXNode(payload_size, kit-&gt;intcon(LogBytesPerLong) ));</span>
684 
685   const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
686 
<span class="line-modified">687   ArrayCopyNode* ac = ArrayCopyNode::make(kit, false, payload_src, NULL, payload_dst, NULL, payload_size, true, false);</span>
<span class="line-modified">688   if (is_array) {</span>
<span class="line-added">689     ac-&gt;set_clone_array();</span>
<span class="line-added">690   } else {</span>
<span class="line-added">691     ac-&gt;set_clone_inst();</span>
<span class="line-added">692   }</span>
693   Node* n = kit-&gt;gvn().transform(ac);
694   if (n == ac) {
695     ac-&gt;_adr_type = TypeRawPtr::BOTTOM;
696     kit-&gt;set_predefined_output_for_runtime_call(ac, ac-&gt;in(TypeFunc::Memory), raw_adr_type);
697   } else {
698     kit-&gt;set_all_memory(n);
699   }
700 }
701 
702 Node* BarrierSetC2::obj_allocate(PhaseMacroExpand* macro, Node* ctrl, Node* mem, Node* toobig_false, Node* size_in_bytes,
703                                  Node*&amp; i_o, Node*&amp; needgc_ctrl,
704                                  Node*&amp; fast_oop_ctrl, Node*&amp; fast_oop_rawmem,
705                                  intx prefetch_lines) const {
706 
707   Node* eden_top_adr;
708   Node* eden_end_adr;
709 
710   macro-&gt;set_eden_pointers(eden_top_adr, eden_end_adr);
711 
712   // Load Eden::end.  Loop invariant and hoisted.
</pre>
<hr />
<pre>
809     Node* thread = new ThreadLocalNode();
810     macro-&gt;transform_later(thread);
811     Node* alloc_bytes_adr = macro-&gt;basic_plus_adr(macro-&gt;top()/*not oop*/, thread,
812                                                   in_bytes(JavaThread::allocated_bytes_offset()));
813     Node* alloc_bytes = macro-&gt;make_load(fast_oop_ctrl, store_eden_top, alloc_bytes_adr,
814                                          0, TypeLong::LONG, T_LONG);
815 #ifdef _LP64
816     Node* alloc_size = size_in_bytes;
817 #else
818     Node* alloc_size = new ConvI2LNode(size_in_bytes);
819     macro-&gt;transform_later(alloc_size);
820 #endif
821     Node* new_alloc_bytes = new AddLNode(alloc_bytes, alloc_size);
822     macro-&gt;transform_later(new_alloc_bytes);
823     fast_oop_rawmem = macro-&gt;make_store(fast_oop_ctrl, store_eden_top, alloc_bytes_adr,
824                                         0, new_alloc_bytes, T_LONG);
825   }
826   return fast_oop;
827 }
828 
<span class="line-modified">829 #define XTOP LP64_ONLY(COMMA phase-&gt;top())</span>
<span class="line-modified">830 </span>
<span class="line-modified">831 void BarrierSetC2::clone_at_expansion(PhaseMacroExpand* phase, ArrayCopyNode* ac) const {</span>
<span class="line-added">832   Node* ctrl = ac-&gt;in(TypeFunc::Control);</span>
<span class="line-added">833   Node* mem = ac-&gt;in(TypeFunc::Memory);</span>
<span class="line-added">834   Node* src = ac-&gt;in(ArrayCopyNode::Src);</span>
<span class="line-added">835   Node* src_offset = ac-&gt;in(ArrayCopyNode::SrcPos);</span>
<span class="line-added">836   Node* dest = ac-&gt;in(ArrayCopyNode::Dest);</span>
<span class="line-added">837   Node* dest_offset = ac-&gt;in(ArrayCopyNode::DestPos);</span>
<span class="line-added">838   Node* length = ac-&gt;in(ArrayCopyNode::Length);</span>
<span class="line-added">839 </span>
<span class="line-added">840   assert (src_offset == NULL,  &quot;for clone offsets should be null&quot;);</span>
<span class="line-added">841   assert (dest_offset == NULL, &quot;for clone offsets should be null&quot;);</span>
<span class="line-added">842 </span>
<span class="line-added">843   const char* copyfunc_name = &quot;arraycopy&quot;;</span>
<span class="line-added">844   address     copyfunc_addr =</span>
<span class="line-added">845           phase-&gt;basictype2arraycopy(T_LONG, NULL, NULL,</span>
<span class="line-added">846                               true, copyfunc_name, true);</span>
<span class="line-added">847 </span>
<span class="line-added">848   const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;</span>
<span class="line-added">849   const TypeFunc* call_type = OptoRuntime::fast_arraycopy_Type();</span>
<span class="line-added">850 </span>
<span class="line-added">851   Node* call = phase-&gt;make_leaf_call(ctrl, mem, call_type, copyfunc_addr, copyfunc_name, raw_adr_type, src, dest, length XTOP);</span>
<span class="line-added">852   phase-&gt;transform_later(call);</span>
<span class="line-added">853 </span>
<span class="line-added">854   phase-&gt;igvn().replace_node(ac, call);</span>
855 }
</pre>
</td>
</tr>
</table>
<center><a href="../c1/barrierSetC1.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../index.html" target="_top">index</a> <a href="barrierSetC2.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>