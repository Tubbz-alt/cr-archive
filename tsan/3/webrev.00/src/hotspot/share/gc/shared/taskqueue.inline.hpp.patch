diff a/src/hotspot/share/gc/shared/taskqueue.inline.hpp b/src/hotspot/share/gc/shared/taskqueue.inline.hpp
--- a/src/hotspot/share/gc/shared/taskqueue.inline.hpp
+++ b/src/hotspot/share/gc/shared/taskqueue.inline.hpp
@@ -66,11 +66,11 @@
     // unused, so we cast the volatile away.  We cannot cast directly
     // to void, because gcc treats that as not using the result of the
     // assignment.  However, casting to E& means that we trigger an
     // unused-value warning.  So, we cast the E& to void.
     (void)const_cast<E&>(_elems[localBot] = t);
-    OrderAccess::release_store(&_bottom, increment_index(localBot));
+    Atomic::release_store(&_bottom, increment_index(localBot));
     TASKQUEUE_STATS_ONLY(stats.record_push());
     return true;
   }
   return false;
 }
@@ -87,11 +87,11 @@
     // unused, so we cast the volatile away.  We cannot cast directly
     // to void, because gcc treats that as not using the result of the
     // assignment.  However, casting to E& means that we trigger an
     // unused-value warning.  So, we cast the E& to void.
     (void) const_cast<E&>(_elems[localBot] = t);
-    OrderAccess::release_store(&_bottom, increment_index(localBot));
+    Atomic::release_store(&_bottom, increment_index(localBot));
     TASKQUEUE_STATS_ONLY(stats.record_push());
     return true;
   } else {
     return push_slow(t, dirty_n_elems);
   }
@@ -182,10 +182,15 @@
     TASKQUEUE_STATS_ONLY(stats.record_pop());
     return true;
   } else {
     // Otherwise, the queue contained exactly one element; we take the slow
     // path.
+
+    // The barrier is required to prevent reordering the two reads of _age:
+    // one is the _age.get() below, and the other is _age.top() above the if-stmt.
+    // The algorithm may fail if _age.get() reads an older value than _age.top().
+    OrderAccess::loadload();
     return pop_local_slow(localBot, _age.get());
   }
 }
 
 template <class E, MEMFLAGS F, unsigned int N>
@@ -197,17 +202,31 @@
 }
 
 template<class E, MEMFLAGS F, unsigned int N>
 bool GenericTaskQueue<E, F, N>::pop_global(volatile E& t) {
   Age oldAge = _age.get();
-  // Architectures with weak memory model require a barrier here
-  // to guarantee that bottom is not older than age,
+#ifndef CPU_MULTI_COPY_ATOMIC
+  // Architectures with non-multi-copy-atomic memory model require a
+  // full fence here to guarantee that bottom is not older than age,
   // which is crucial for the correctness of the algorithm.
-#if !(defined SPARC || defined IA32 || defined AMD64)
+  //
+  // We need a full fence here for this case:
+  //
+  // Thread1: set bottom (push)
+  // Thread2: read age, read bottom, set age (pop_global)
+  // Thread3: read age, read bottom (pop_global)
+  //
+  // The requirement is that Thread3 must never read an older bottom
+  // value than Thread2 after Thread3 has seen the age value from
+  // Thread2.
   OrderAccess::fence();
+#else
+  // Everyone else can make do with a LoadLoad barrier to keep reads
+  // from _age and _bottom in order.
+  OrderAccess::loadload();
 #endif
-  uint localBot = OrderAccess::load_acquire(&_bottom);
+  uint localBot = Atomic::load_acquire(&_bottom);
   uint n_elems = size(localBot, oldAge.top());
   if (n_elems == 0) {
     return false;
   }
 
@@ -314,11 +333,11 @@
   return false;
 }
 
 template <unsigned int N, MEMFLAGS F>
 inline typename TaskQueueSuper<N, F>::Age TaskQueueSuper<N, F>::Age::cmpxchg(const Age new_age, const Age old_age) volatile {
-  return Atomic::cmpxchg(new_age._data, &_data, old_age._data);
+  return Atomic::cmpxchg(&_data, old_age._data, new_age._data);
 }
 
 template<class E, MEMFLAGS F, unsigned int N>
 template<class Fn>
 inline void GenericTaskQueue<E, F, N>::iterate(Fn fn) {
