<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/output.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="opaquenode.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="output.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/output.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1998, 2018, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/assembler.inline.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;code/compiledIC.hpp&quot;
  29 #include &quot;code/debugInfo.hpp&quot;
  30 #include &quot;code/debugInfoRec.hpp&quot;
  31 #include &quot;compiler/compileBroker.hpp&quot;
  32 #include &quot;compiler/compilerDirectives.hpp&quot;
  33 #include &quot;compiler/oopMap.hpp&quot;


  34 #include &quot;memory/allocation.inline.hpp&quot;
  35 #include &quot;opto/ad.hpp&quot;
  36 #include &quot;opto/callnode.hpp&quot;
  37 #include &quot;opto/cfgnode.hpp&quot;
  38 #include &quot;opto/locknode.hpp&quot;
  39 #include &quot;opto/machnode.hpp&quot;
  40 #include &quot;opto/optoreg.hpp&quot;
  41 #include &quot;opto/output.hpp&quot;
  42 #include &quot;opto/regalloc.hpp&quot;
  43 #include &quot;opto/runtime.hpp&quot;
  44 #include &quot;opto/subnode.hpp&quot;
  45 #include &quot;opto/type.hpp&quot;
  46 #include &quot;runtime/handles.inline.hpp&quot;

  47 #include &quot;utilities/xmlstream.hpp&quot;
  48 
  49 #ifndef PRODUCT
  50 #define DEBUG_ARG(x) , x
  51 #else
  52 #define DEBUG_ARG(x)
  53 #endif
  54 
  55 // Convert Nodes to instruction bits and pass off to the VM
  56 void Compile::Output() {
  57   // RootNode goes
  58   assert( _cfg-&gt;get_root_block()-&gt;number_of_nodes() == 0, &quot;&quot; );
  59 
  60   // The number of new nodes (mostly MachNop) is proportional to
  61   // the number of java calls and inner loops which are aligned.
  62   if ( C-&gt;check_node_count((NodeLimitFudgeFactor + C-&gt;java_calls()*3 +
  63                             C-&gt;inner_loops()*(OptoLoopAlignment-1)),
  64                            &quot;out of nodes before code generation&quot; ) ) {
  65     return;
  66   }
</pre>
<hr />
<pre>
  97       (OptoBreakpointOSR &amp;&amp; is_osr_compilation())       ||
  98       (OptoBreakpointC2R &amp;&amp; !_method)                   ) {
  99     // checking for _method means that OptoBreakpoint does not apply to
 100     // runtime stubs or frame converters
 101     _cfg-&gt;insert( entry, 1, new MachBreakpointNode() );
 102   }
 103 
 104   // Insert epilogs before every return
 105   for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {
 106     Block* block = _cfg-&gt;get_block(i);
 107     if (!block-&gt;is_connector() &amp;&amp; block-&gt;non_connector_successor(0) == _cfg-&gt;get_root_block()) { // Found a program exit point?
 108       Node* m = block-&gt;end();
 109       if (m-&gt;is_Mach() &amp;&amp; m-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Halt) {
 110         MachEpilogNode* epilog = new MachEpilogNode(m-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Return);
 111         block-&gt;add_inst(epilog);
 112         _cfg-&gt;map_node_to_block(epilog, block);
 113       }
 114     }
 115   }
 116 










 117   uint* blk_starts = NEW_RESOURCE_ARRAY(uint, _cfg-&gt;number_of_blocks() + 1);
 118   blk_starts[0] = 0;

 119 
<span class="line-modified"> 120   // Initialize code buffer and process short branches.</span>
<span class="line-modified"> 121   CodeBuffer* cb = init_buffer(blk_starts);</span>
<span class="line-removed"> 122 </span>
<span class="line-removed"> 123   if (cb == NULL || failing()) {</span>
 124     return;
 125   }
 126 
<span class="line-modified"> 127   ScheduleAndBundle();</span>
<span class="line-modified"> 128 </span>
<span class="line-modified"> 129 #ifndef PRODUCT</span>
<span class="line-modified"> 130   if (trace_opto_output()) {</span>
<span class="line-removed"> 131     tty-&gt;print(&quot;\n---- After ScheduleAndBundle ----\n&quot;);</span>
<span class="line-removed"> 132     for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {</span>
<span class="line-removed"> 133       tty-&gt;print(&quot;\nBB#%03d:\n&quot;, i);</span>
<span class="line-removed"> 134       Block* block = _cfg-&gt;get_block(i);</span>
<span class="line-removed"> 135       for (uint j = 0; j &lt; block-&gt;number_of_nodes(); j++) {</span>
<span class="line-removed"> 136         Node* n = block-&gt;get_node(j);</span>
<span class="line-removed"> 137         OptoReg::Name reg = _regalloc-&gt;get_reg_first(n);</span>
<span class="line-removed"> 138         tty-&gt;print(&quot; %-6s &quot;, reg &gt;= 0 &amp;&amp; reg &lt; REG_COUNT ? Matcher::regName[reg] : &quot;&quot;);</span>
<span class="line-removed"> 139         n-&gt;dump();</span>
<span class="line-removed"> 140       }</span>
<span class="line-removed"> 141     }</span>
<span class="line-removed"> 142   }</span>
<span class="line-removed"> 143 #endif</span>
 144 
<span class="line-modified"> 145   if (failing()) {</span>


 146     return;
 147   }
 148 
 149   BuildOopMaps();
 150 
 151   if (failing())  {
 152     return;
 153   }
 154 
 155   fill_buffer(cb, blk_starts);
 156 }
 157 
 158 bool Compile::need_stack_bang(int frame_size_in_bytes) const {
 159   // Determine if we need to generate a stack overflow check.
 160   // Do it if the method is not a stub function and
 161   // has java calls or has frame size &gt; vm_page_size/8.
 162   // The debug VM checks that deoptimization doesn&#39;t trigger an
 163   // unexpected stack overflow (compiled method stack banging should
 164   // guarantee it doesn&#39;t happen) so we always need the stack bang in
 165   // a debug VM.
</pre>
<hr />
<pre>
 206         // Check subsequent fallthrough blocks if the loop&#39;s first
 207         // block(s) does not have enough instructions.
 208         Block *nb = block;
 209         while(inst_cnt &gt; 0 &amp;&amp;
 210               i &lt; last_block &amp;&amp;
 211               !_cfg-&gt;get_block(i + 1)-&gt;has_loop_alignment() &amp;&amp;
 212               !nb-&gt;has_successor(block)) {
 213           i++;
 214           nb = _cfg-&gt;get_block(i);
 215           inst_cnt  = nb-&gt;compute_first_inst_size(sum_size, inst_cnt, _regalloc);
 216         } // while( inst_cnt &gt; 0 &amp;&amp; i &lt; last_block  )
 217 
 218         block-&gt;set_first_inst_size(sum_size);
 219       } // f( b-&gt;head()-&gt;is_Loop() )
 220     } // for( i &lt;= last_block )
 221   } // if( MaxLoopPad &lt; OptoLoopAlignment-1 )
 222 }
 223 
 224 // The architecture description provides short branch variants for some long
 225 // branch instructions. Replace eligible long branches with short branches.
<span class="line-modified"> 226 void Compile::shorten_branches(uint* blk_starts, int&amp; code_size, int&amp; reloc_size, int&amp; stub_size) {</span>
 227   // Compute size of each block, method size, and relocation information size
 228   uint nblocks  = _cfg-&gt;number_of_blocks();
 229 
 230   uint*      jmp_offset = NEW_RESOURCE_ARRAY(uint,nblocks);
 231   uint*      jmp_size   = NEW_RESOURCE_ARRAY(uint,nblocks);
 232   int*       jmp_nidx   = NEW_RESOURCE_ARRAY(int ,nblocks);
 233 
 234   // Collect worst case block paddings
 235   int* block_worst_case_pad = NEW_RESOURCE_ARRAY(int, nblocks);
 236   memset(block_worst_case_pad, 0, nblocks * sizeof(int));
 237 
 238   DEBUG_ONLY( uint *jmp_target = NEW_RESOURCE_ARRAY(uint,nblocks); )
 239   DEBUG_ONLY( uint *jmp_rule = NEW_RESOURCE_ARRAY(uint,nblocks); )
 240 
 241   bool has_short_branch_candidate = false;
 242 
 243   // Initialize the sizes to 0
<span class="line-modified"> 244   code_size  = 0;          // Size in bytes of generated code</span>
<span class="line-modified"> 245   stub_size  = 0;          // Size in bytes of all stub entries</span>
 246   // Size in bytes of all relocation entries, including those in local stubs.
 247   // Start with 2-bytes of reloc info for the unvalidated entry point
<span class="line-modified"> 248   reloc_size = 1;          // Number of relocation entries</span>
 249 
 250   // Make three passes.  The first computes pessimistic blk_starts,
 251   // relative jmp_offset and reloc_size information.  The second performs
 252   // short branch substitution using the pessimistic sizing.  The
 253   // third inserts nops where needed.
 254 
 255   // Step one, perform a pessimistic sizing pass.
 256   uint last_call_adr = max_juint;
 257   uint last_avoid_back_to_back_adr = max_juint;
 258   uint nop_size = (new MachNopNode())-&gt;size(_regalloc);
 259   for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
 260     Block* block = _cfg-&gt;get_block(i);
 261 
 262     // During short branch replacement, we store the relative (to blk_starts)
 263     // offset of jump in jmp_offset, rather than the absolute offset of jump.
 264     // This is so that we do not need to recompute sizes of all nodes when
 265     // we compute correct blk_starts in our next sizing pass.
 266     jmp_offset[i] = 0;
 267     jmp_size[i]   = 0;
 268     jmp_nidx[i]   = -1;
</pre>
<hr />
<pre>
 462       assert(_matcher-&gt;is_short_branch_offset(jmp_rule[i], br_size, offset), &quot;Displacement too large for short jmp&quot;);
 463     }
 464   }
 465 #endif
 466 
 467   // Step 3, compute the offsets of all blocks, will be done in fill_buffer()
 468   // after ScheduleAndBundle().
 469 
 470   // ------------------
 471   // Compute size for code buffer
 472   code_size = blk_starts[nblocks];
 473 
 474   // Relocation records
 475   reloc_size += 1;              // Relo entry for exception handler
 476 
 477   // Adjust reloc_size to number of record of relocation info
 478   // Min is 2 bytes, max is probably 6 or 8, with a tax up to 25% for
 479   // a relocation index.
 480   // The CodeBuffer will expand the locs array if this estimate is too low.
 481   reloc_size *= 10 / sizeof(relocInfo);




 482 }
 483 
 484 //------------------------------FillLocArray-----------------------------------
 485 // Create a bit of debug info and append it to the array.  The mapping is from
 486 // Java local or expression stack to constant, register or stack-slot.  For
 487 // doubles, insert 2 mappings and return 1 (to tell the caller that the next
 488 // entry has been taken care of and caller should skip it).
 489 static LocationValue *new_loc_value( PhaseRegAlloc *ra, OptoReg::Name regnum, Location::Type l_type ) {
 490   // This should never have accepted Bad before
 491   assert(OptoReg::is_valid(regnum), &quot;location must be valid&quot;);
 492   return (OptoReg::is_reg(regnum))
<span class="line-modified"> 493     ? new LocationValue(Location::new_reg_loc(l_type, OptoReg::as_VMReg(regnum)) )</span>
<span class="line-modified"> 494     : new LocationValue(Location::new_stk_loc(l_type,  ra-&gt;reg2offset(regnum)));</span>
 495 }
 496 
 497 
 498 ObjectValue*
 499 Compile::sv_for_node_id(GrowableArray&lt;ScopeValue*&gt; *objs, int id) {
 500   for (int i = 0; i &lt; objs-&gt;length(); i++) {
 501     assert(objs-&gt;at(i)-&gt;is_object(), &quot;corrupt object cache&quot;);
 502     ObjectValue* sv = (ObjectValue*) objs-&gt;at(i);
 503     if (sv-&gt;id() == id) {
 504       return sv;
 505     }
 506   }
 507   // Otherwise..
 508   return NULL;
 509 }
 510 
 511 void Compile::set_sv_for_object_node(GrowableArray&lt;ScopeValue*&gt; *objs,
 512                                      ObjectValue* sv ) {
 513   assert(sv_for_node_id(objs, sv-&gt;id()) == NULL, &quot;Precondition&quot;);
 514   objs-&gt;append(sv);
</pre>
<hr />
<pre>
 593     if (t-&gt;base() == Type::Long &amp;&amp; OptoReg::is_reg(regnum)) {
 594       // For SPARC we have to swap high and low words for
 595       // long values stored in a single-register (g0-g7).
 596       array-&gt;append(new_loc_value( _regalloc,              regnum   , Location::normal ));
 597       array-&gt;append(new_loc_value( _regalloc, OptoReg::add(regnum,1), Location::normal ));
 598     } else
 599 #endif //SPARC
 600     if( t-&gt;base() == Type::DoubleBot || t-&gt;base() == Type::DoubleCon || t-&gt;base() == Type::Long ) {
 601       // Repack the double/long as two jints.
 602       // The convention the interpreter uses is that the second local
 603       // holds the first raw word of the native double representation.
 604       // This is actually reasonable, since locals and stack arrays
 605       // grow downwards in all implementations.
 606       // (If, on some machine, the interpreter&#39;s Java locals or stack
 607       // were to grow upwards, the embedded doubles would be word-swapped.)
 608       array-&gt;append(new_loc_value( _regalloc, OptoReg::add(regnum,1), Location::normal ));
 609       array-&gt;append(new_loc_value( _regalloc,              regnum   , Location::normal ));
 610     }
 611 #endif //_LP64
 612     else if( (t-&gt;base() == Type::FloatBot || t-&gt;base() == Type::FloatCon) &amp;&amp;
<span class="line-modified"> 613                OptoReg::is_reg(regnum) ) {</span>
 614       array-&gt;append(new_loc_value( _regalloc, regnum, Matcher::float_in_double()
<span class="line-modified"> 615                                    ? Location::float_in_dbl : Location::normal ));</span>
 616     } else if( t-&gt;base() == Type::Int &amp;&amp; OptoReg::is_reg(regnum) ) {
 617       array-&gt;append(new_loc_value( _regalloc, regnum, Matcher::int_in_long
<span class="line-modified"> 618                                    ? Location::int_in_long : Location::normal ));</span>
 619     } else if( t-&gt;base() == Type::NarrowOop ) {
 620       array-&gt;append(new_loc_value( _regalloc, regnum, Location::narrowoop ));
 621     } else {
 622       array-&gt;append(new_loc_value( _regalloc, regnum, _regalloc-&gt;is_oop(local) ? Location::oop : Location::normal ));
 623     }
 624     return;
 625   }
 626 
 627   // No register.  It must be constant data.
 628   switch (t-&gt;base()) {
<span class="line-modified"> 629   case Type::Half:              // Second half of a double</span>
<span class="line-modified"> 630     ShouldNotReachHere();       // Caller should skip 2nd halves</span>
<span class="line-modified"> 631     break;</span>
<span class="line-modified"> 632   case Type::AnyPtr:</span>
<span class="line-removed"> 633     array-&gt;append(new ConstantOopWriteValue(NULL));</span>
<span class="line-removed"> 634     break;</span>
<span class="line-removed"> 635   case Type::AryPtr:</span>
<span class="line-removed"> 636   case Type::InstPtr:          // fall through</span>
<span class="line-removed"> 637     array-&gt;append(new ConstantOopWriteValue(t-&gt;isa_oopptr()-&gt;const_oop()-&gt;constant_encoding()));</span>
<span class="line-removed"> 638     break;</span>
<span class="line-removed"> 639   case Type::NarrowOop:</span>
<span class="line-removed"> 640     if (t == TypeNarrowOop::NULL_PTR) {</span>
 641       array-&gt;append(new ConstantOopWriteValue(NULL));
<span class="line-modified"> 642     } else {</span>
<span class="line-modified"> 643       array-&gt;append(new ConstantOopWriteValue(t-&gt;make_ptr()-&gt;isa_oopptr()-&gt;const_oop()-&gt;constant_encoding()));</span>
<span class="line-modified"> 644     }</span>
<span class="line-modified"> 645     break;</span>
<span class="line-modified"> 646   case Type::Int:</span>
<span class="line-modified"> 647     array-&gt;append(new ConstantIntValue(t-&gt;is_int()-&gt;get_con()));</span>
<span class="line-modified"> 648     break;</span>
<span class="line-modified"> 649   case Type::RawPtr:</span>
<span class="line-modified"> 650     // A return address (T_ADDRESS).</span>
<span class="line-modified"> 651     assert((intptr_t)t-&gt;is_ptr()-&gt;get_con() &lt; (intptr_t)0x10000, &quot;must be a valid BCI&quot;);</span>








 652 #ifdef _LP64
<span class="line-modified"> 653     // Must be restored to the full-width 64-bit stack slot.</span>
<span class="line-modified"> 654     array-&gt;append(new ConstantLongValue(t-&gt;is_ptr()-&gt;get_con()));</span>
 655 #else
<span class="line-modified"> 656     array-&gt;append(new ConstantIntValue(t-&gt;is_ptr()-&gt;get_con()));</span>
 657 #endif
<span class="line-modified"> 658     break;</span>
<span class="line-modified"> 659   case Type::FloatCon: {</span>
<span class="line-modified"> 660     float f = t-&gt;is_float_constant()-&gt;getf();</span>
<span class="line-modified"> 661     array-&gt;append(new ConstantIntValue(jint_cast(f)));</span>
<span class="line-modified"> 662     break;</span>
<span class="line-modified"> 663   }</span>
<span class="line-modified"> 664   case Type::DoubleCon: {</span>
<span class="line-modified"> 665     jdouble d = t-&gt;is_double_constant()-&gt;getd();</span>
 666 #ifdef _LP64
<span class="line-modified"> 667     array-&gt;append(new ConstantIntValue((jint)0));</span>
<span class="line-modified"> 668     array-&gt;append(new ConstantDoubleValue(d));</span>
 669 #else
<span class="line-modified"> 670     // Repack the double as two jints.</span>
 671     // The convention the interpreter uses is that the second local
 672     // holds the first raw word of the native double representation.
 673     // This is actually reasonable, since locals and stack arrays
 674     // grow downwards in all implementations.
 675     // (If, on some machine, the interpreter&#39;s Java locals or stack
 676     // were to grow upwards, the embedded doubles would be word-swapped.)
 677     jlong_accessor acc;
 678     acc.long_value = jlong_cast(d);
 679     array-&gt;append(new ConstantIntValue(acc.words[1]));
 680     array-&gt;append(new ConstantIntValue(acc.words[0]));
 681 #endif
<span class="line-modified"> 682     break;</span>
<span class="line-modified"> 683   }</span>
<span class="line-modified"> 684   case Type::Long: {</span>
<span class="line-modified"> 685     jlong d = t-&gt;is_long()-&gt;get_con();</span>
 686 #ifdef _LP64
<span class="line-modified"> 687     array-&gt;append(new ConstantIntValue((jint)0));</span>
<span class="line-modified"> 688     array-&gt;append(new ConstantLongValue(d));</span>
 689 #else
<span class="line-modified"> 690     // Repack the long as two jints.</span>
 691     // The convention the interpreter uses is that the second local
 692     // holds the first raw word of the native double representation.
 693     // This is actually reasonable, since locals and stack arrays
 694     // grow downwards in all implementations.
 695     // (If, on some machine, the interpreter&#39;s Java locals or stack
 696     // were to grow upwards, the embedded doubles would be word-swapped.)
 697     jlong_accessor acc;
 698     acc.long_value = d;
 699     array-&gt;append(new ConstantIntValue(acc.words[1]));
 700     array-&gt;append(new ConstantIntValue(acc.words[0]));
 701 #endif
<span class="line-modified"> 702     break;</span>
<span class="line-modified"> 703   }</span>
<span class="line-modified"> 704   case Type::Top:               // Add an illegal value here</span>
<span class="line-modified"> 705     array-&gt;append(new LocationValue(Location()));</span>
<span class="line-modified"> 706     break;</span>
<span class="line-modified"> 707   default:</span>
<span class="line-modified"> 708     ShouldNotReachHere();</span>
<span class="line-modified"> 709     break;</span>
 710   }
 711 }
 712 
 713 // Determine if this node starts a bundle
 714 bool Compile::starts_bundle(const Node *n) const {
 715   return (_node_bundling_limit &gt; n-&gt;_idx &amp;&amp;
 716           _node_bundling_base[n-&gt;_idx].starts_bundle());
 717 }
 718 
 719 //--------------------------Process_OopMap_Node--------------------------------
 720 void Compile::Process_OopMap_Node(MachNode *mach, int current_offset) {
 721 
 722   // Handle special safepoint nodes for synchronization
 723   MachSafePointNode *sfn   = mach-&gt;as_MachSafePoint();
 724   MachCallNode      *mcall;
 725 
 726   int safepoint_pc_offset = current_offset;
 727   bool is_method_handle_invoke = false;
 728   bool return_oop = false;
 729 
</pre>
<hr />
<pre>
 854 
 855     // Make method available for all Safepoints
 856     ciMethod* scope_method = method ? method : _method;
 857     // Describe the scope here
 858     assert(jvms-&gt;bci() &gt;= InvocationEntryBci &amp;&amp; jvms-&gt;bci() &lt;= 0x10000, &quot;must be a valid or entry BCI&quot;);
 859     assert(!jvms-&gt;should_reexecute() || depth == max_depth, &quot;reexecute allowed only for the youngest&quot;);
 860     // Now we can describe the scope.
 861     methodHandle null_mh;
 862     bool rethrow_exception = false;
 863     debug_info()-&gt;describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms-&gt;bci(), jvms-&gt;should_reexecute(), rethrow_exception, is_method_handle_invoke, return_oop, locvals, expvals, monvals);
 864   } // End jvms loop
 865 
 866   // Mark the end of the scope set.
 867   debug_info()-&gt;end_safepoint(safepoint_pc_offset);
 868 }
 869 
 870 
 871 
 872 // A simplified version of Process_OopMap_Node, to handle non-safepoints.
 873 class NonSafepointEmitter {
<span class="line-modified"> 874   Compile*  C;</span>
<span class="line-modified"> 875   JVMState* _pending_jvms;</span>
<span class="line-modified"> 876   int       _pending_offset;</span>
 877 
<span class="line-modified"> 878   void emit_non_safepoint();</span>
 879 
 880  public:
<span class="line-modified"> 881   NonSafepointEmitter(Compile* compile) {</span>
<span class="line-modified"> 882     this-&gt;C = compile;</span>
<span class="line-modified"> 883     _pending_jvms = NULL;</span>
<span class="line-modified"> 884     _pending_offset = 0;</span>
<span class="line-modified"> 885   }</span>
 886 
<span class="line-modified"> 887   void observe_instruction(Node* n, int pc_offset) {</span>
<span class="line-modified"> 888     if (!C-&gt;debug_info()-&gt;recording_non_safepoints())  return;</span>
 889 
<span class="line-modified"> 890     Node_Notes* nn = C-&gt;node_notes_at(n-&gt;_idx);</span>
<span class="line-modified"> 891     if (nn == NULL || nn-&gt;jvms() == NULL)  return;</span>
<span class="line-removed"> 892     if (_pending_jvms != NULL &amp;&amp;</span>
<span class="line-removed"> 893         _pending_jvms-&gt;same_calls_as(nn-&gt;jvms())) {</span>
<span class="line-removed"> 894       // Repeated JVMS?  Stretch it up here.</span>
<span class="line-removed"> 895       _pending_offset = pc_offset;</span>
<span class="line-removed"> 896     } else {</span>
 897       if (_pending_jvms != NULL &amp;&amp;
<span class="line-modified"> 898           _pending_offset &lt; pc_offset) {</span>
<span class="line-modified"> 899         emit_non_safepoint();</span>
<span class="line-removed"> 900       }</span>
<span class="line-removed"> 901       _pending_jvms = NULL;</span>
<span class="line-removed"> 902       if (pc_offset &gt; C-&gt;debug_info()-&gt;last_pc_offset()) {</span>
<span class="line-removed"> 903         // This is the only way _pending_jvms can become non-NULL:</span>
<span class="line-removed"> 904         _pending_jvms = nn-&gt;jvms();</span>
 905         _pending_offset = pc_offset;











 906       }
 907     }
<span class="line-removed"> 908   }</span>
 909 
<span class="line-modified"> 910   // Stay out of the way of real safepoints:</span>
<span class="line-modified"> 911   void observe_safepoint(JVMState* jvms, int pc_offset) {</span>
<span class="line-modified"> 912     if (_pending_jvms != NULL &amp;&amp;</span>
<span class="line-modified"> 913         !_pending_jvms-&gt;same_calls_as(jvms) &amp;&amp;</span>
<span class="line-modified"> 914         _pending_offset &lt; pc_offset) {</span>
<span class="line-modified"> 915       emit_non_safepoint();</span>


 916     }
<span class="line-removed"> 917     _pending_jvms = NULL;</span>
<span class="line-removed"> 918   }</span>
 919 
<span class="line-modified"> 920   void flush_at_end() {</span>
<span class="line-modified"> 921     if (_pending_jvms != NULL) {</span>
<span class="line-modified"> 922       emit_non_safepoint();</span>


 923     }
<span class="line-removed"> 924     _pending_jvms = NULL;</span>
<span class="line-removed"> 925   }</span>
 926 };
 927 
 928 void NonSafepointEmitter::emit_non_safepoint() {
 929   JVMState* youngest_jvms = _pending_jvms;
 930   int       pc_offset     = _pending_offset;
 931 
 932   // Clear it now:
 933   _pending_jvms = NULL;
 934 
 935   DebugInformationRecorder* debug_info = C-&gt;debug_info();
 936   assert(debug_info-&gt;recording_non_safepoints(), &quot;sanity&quot;);
 937 
 938   debug_info-&gt;add_non_safepoint(pc_offset);
 939   int max_depth = youngest_jvms-&gt;depth();
 940 
 941   // Visit scopes from oldest to youngest.
 942   for (int depth = 1; depth &lt;= max_depth; depth++) {
 943     JVMState* jvms = youngest_jvms-&gt;of_depth(depth);
 944     ciMethod* method = jvms-&gt;has_method() ? jvms-&gt;method() : NULL;
 945     assert(!jvms-&gt;should_reexecute() || depth==max_depth, &quot;reexecute allowed only for the youngest&quot;);
 946     methodHandle null_mh;
 947     debug_info-&gt;describe_scope(pc_offset, null_mh, method, jvms-&gt;bci(), jvms-&gt;should_reexecute());
 948   }
 949 
 950   // Mark the end of the scope set.
 951   debug_info-&gt;end_non_safepoint(pc_offset);
 952 }
 953 
 954 //------------------------------init_buffer------------------------------------
<span class="line-modified"> 955 CodeBuffer* Compile::init_buffer(uint* blk_starts) {</span>
 956 
 957   // Set the initially allocated size
<span class="line-modified"> 958   int  code_req   = initial_code_capacity;</span>
<span class="line-removed"> 959   int  locs_req   = initial_locs_capacity;</span>
<span class="line-removed"> 960   int  stub_req   = initial_stub_capacity;</span>
<span class="line-removed"> 961   int  const_req  = initial_const_capacity;</span>
 962 
<span class="line-removed"> 963   int  pad_req    = NativeCall::instruction_size;</span>
 964   // The extra spacing after the code is necessary on some platforms.
 965   // Sometimes we need to patch in a jump after the last instruction,
 966   // if the nmethod has been deoptimized.  (See 4932387, 4894843.)
 967 
 968   // Compute the byte offset where we can store the deopt pc.
 969   if (fixed_slots() != 0) {
 970     _orig_pc_slot_offset_in_bytes = _regalloc-&gt;reg2offset(OptoReg::stack2reg(_orig_pc_slot));
 971   }
 972 
 973   // Compute prolog code size
 974   _method_size = 0;
<span class="line-modified"> 975   _frame_slots = OptoReg::reg2stack(_matcher-&gt;_old_SP)+_regalloc-&gt;_framesize;</span>
 976 #if defined(IA64) &amp;&amp; !defined(AIX)
 977   if (save_argument_registers()) {
 978     // 4815101: this is a stub with implicit and unknown precision fp args.
 979     // The usual spill mechanism can only generate stfd&#39;s in this case, which
 980     // doesn&#39;t work if the fp reg to spill contains a single-precision denorm.
 981     // Instead, we hack around the normal spill mechanism using stfspill&#39;s and
 982     // ldffill&#39;s in the MachProlog and MachEpilog emit methods.  We allocate
 983     // space here for the fp arg regs (f8-f15) we&#39;re going to thusly spill.
 984     //
 985     // If we ever implement 16-byte &#39;registers&#39; == stack slots, we can
 986     // get rid of this hack and have SpillCopy generate stfspill/ldffill
 987     // instead of stfd/stfs/ldfd/ldfs.
 988     _frame_slots += 8*(16/BytesPerInt);
 989   }
 990 #endif
 991   assert(_frame_slots &gt;= 0 &amp;&amp; _frame_slots &lt; 1000000, &quot;sanity check&quot;);
 992 
 993   if (has_mach_constant_base_node()) {
 994     uint add_size = 0;
 995     // Fill the constant table.
</pre>
<hr />
<pre>
1004         // value section.
1005         if (n-&gt;is_MachConstant()) {
1006           MachConstantNode* machcon = n-&gt;as_MachConstant();
1007           machcon-&gt;eval_constant(C);
1008         } else if (n-&gt;is_Mach()) {
1009           // On Power there are more nodes that issue constants.
1010           add_size += (n-&gt;as_Mach()-&gt;ins_num_consts() * 8);
1011         }
1012       }
1013     }
1014 
1015     // Calculate the offsets of the constants and the size of the
1016     // constant table (including the padding to the next section).
1017     constant_table().calculate_offsets_and_size();
1018     const_req = constant_table().size() + add_size;
1019   }
1020 
1021   // Initialize the space for the BufferBlob used to find and verify
1022   // instruction size in MachNode::emit_size()
1023   init_scratch_buffer_blob(const_req);
<span class="line-modified">1024   if (failing())  return NULL; // Out of memory</span>
1025 
<span class="line-modified">1026   // Pre-compute the length of blocks and replace</span>
<span class="line-modified">1027   // long branches with short if machine supports it.</span>
<span class="line-modified">1028   shorten_branches(blk_starts, code_req, locs_req, stub_req);</span>







1029 
1030   // nmethod and CodeBuffer count stubs &amp; constants as part of method&#39;s code.
1031   // class HandlerImpl is platform-specific and defined in the *.ad files.
1032   int exception_handler_req = HandlerImpl::size_exception_handler() + MAX_stubs_size; // add marginal slop for handler
1033   int deopt_handler_req     = HandlerImpl::size_deopt_handler()     + MAX_stubs_size; // add marginal slop for handler
1034   stub_req += MAX_stubs_size;   // ensure per-stub margin
1035   code_req += MAX_inst_size;    // ensure per-instruction margin
1036 
1037   if (StressCodeBuffers)
1038     code_req = const_req = stub_req = exception_handler_req = deopt_handler_req = 0x10;  // force expansion
1039 
1040   int total_req =
<span class="line-modified">1041     const_req +</span>
<span class="line-modified">1042     code_req +</span>
<span class="line-modified">1043     pad_req +</span>
<span class="line-modified">1044     stub_req +</span>
<span class="line-modified">1045     exception_handler_req +</span>
<span class="line-modified">1046     deopt_handler_req;               // deopt handler</span>
1047 
1048   if (has_method_handle_invokes())
1049     total_req += deopt_handler_req;  // deopt MH handler
1050 
1051   CodeBuffer* cb = code_buffer();
<span class="line-modified">1052   cb-&gt;initialize(total_req, locs_req);</span>
1053 
1054   // Have we run out of code space?
1055   if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1056     C-&gt;record_failure(&quot;CodeCache is full&quot;);
1057     return NULL;
1058   }
1059   // Configure the code buffer.
1060   cb-&gt;initialize_consts_size(const_req);
1061   cb-&gt;initialize_stubs_size(stub_req);
1062   cb-&gt;initialize_oop_recorder(env()-&gt;oop_recorder());
1063 
1064   // fill in the nop array for bundling computations
1065   MachNode *_nop_list[Bundle::_nop_count];
1066   Bundle::initialize_nops(_nop_list);
1067 
1068   return cb;
1069 }
1070 
1071 //------------------------------fill_buffer------------------------------------
1072 void Compile::fill_buffer(CodeBuffer* cb, uint* blk_starts) {
</pre>
<hr />
<pre>
1089   uint *inct_starts = NEW_RESOURCE_ARRAY(uint, nblocks+1);
1090 
1091   // Count and start of calls
1092   uint *call_returns = NEW_RESOURCE_ARRAY(uint, nblocks+1);
1093 
1094   uint  return_offset = 0;
1095   int nop_size = (new MachNopNode())-&gt;size(_regalloc);
1096 
1097   int previous_offset = 0;
1098   int current_offset  = 0;
1099   int last_call_offset = -1;
1100   int last_avoid_back_to_back_offset = -1;
1101 #ifdef ASSERT
1102   uint* jmp_target = NEW_RESOURCE_ARRAY(uint,nblocks);
1103   uint* jmp_offset = NEW_RESOURCE_ARRAY(uint,nblocks);
1104   uint* jmp_size   = NEW_RESOURCE_ARRAY(uint,nblocks);
1105   uint* jmp_rule   = NEW_RESOURCE_ARRAY(uint,nblocks);
1106 #endif
1107 
1108   // Create an array of unused labels, one for each basic block, if printing is enabled
<span class="line-modified">1109 #ifndef PRODUCT</span>
1110   int *node_offsets      = NULL;
1111   uint node_offset_limit = unique();
1112 
<span class="line-modified">1113   if (print_assembly())</span>
<span class="line-modified">1114     node_offsets         = NEW_RESOURCE_ARRAY(int, node_offset_limit);</span>





1115 #endif
1116 
1117   NonSafepointEmitter non_safepoints(this);  // emit non-safepoints lazily
1118 
1119   // Emit the constant table.
1120   if (has_mach_constant_base_node()) {
1121     constant_table().emit(*cb);
1122   }
1123 
1124   // Create an array of labels, one for each basic block
1125   Label *blk_labels = NEW_RESOURCE_ARRAY(Label, nblocks+1);
1126   for (uint i=0; i &lt;= nblocks; i++) {
1127     blk_labels[i].init();
1128   }
1129 
1130   // ------------------
1131   // Now fill in the code buffer
1132   Node *delay_slot = NULL;
1133 
1134   for (uint i = 0; i &lt; nblocks; i++) {
</pre>
<hr />
<pre>
1246         }
1247 
1248         // sfn will be valid whenever mcall is valid now because of inheritance
1249         if (is_sfn || is_mcall) {
1250 
1251           // Handle special safepoint nodes for synchronization
1252           if (!is_mcall) {
1253             MachSafePointNode *sfn = mach-&gt;as_MachSafePoint();
1254             // !!!!! Stubs only need an oopmap right now, so bail out
1255             if (sfn-&gt;jvms()-&gt;method() == NULL) {
1256               // Write the oopmap directly to the code blob??!!
1257               continue;
1258             }
1259           } // End synchronization
1260 
1261           non_safepoints.observe_safepoint(mach-&gt;as_MachSafePoint()-&gt;jvms(),
1262                                            current_offset);
1263           Process_OopMap_Node(mach, current_offset);
1264         } // End if safepoint
1265 
<span class="line-modified">1266         // If this is a null check, then add the start of the previous instruction to the list</span>
1267         else if( mach-&gt;is_MachNullCheck() ) {
1268           inct_starts[inct_cnt++] = previous_offset;
1269         }
1270 
<span class="line-modified">1271         // If this is a branch, then fill in the label with the target BB&#39;s label</span>
1272         else if (mach-&gt;is_MachBranch()) {
1273           // This requires the TRUE branch target be in succs[0]
1274           uint block_num = block-&gt;non_connector_successor(0)-&gt;_pre_order;
1275 
1276           // Try to replace long branch if delay slot is not used,
1277           // it is mostly for back branches since forward branch&#39;s
1278           // distance is not updated yet.
1279           bool delay_slot_is_used = valid_bundle_info(n) &amp;&amp;
1280                                     node_bundling(n)-&gt;use_unconditional_delay();
1281           if (!delay_slot_is_used &amp;&amp; mach-&gt;may_be_short_branch()) {
<span class="line-modified">1282            assert(delay_slot == NULL, &quot;not expecting delay slot node&quot;);</span>
<span class="line-modified">1283            int br_size = n-&gt;size(_regalloc);</span>
1284             int offset = blk_starts[block_num] - current_offset;
1285             if (block_num &gt;= i) {
1286               // Current and following block&#39;s offset are not
1287               // finalized yet, adjust distance by the difference
1288               // between calculated and final offsets of current block.
1289               offset -= (blk_starts[i] - blk_offset);
1290             }
1291             // In the following code a nop could be inserted before
1292             // the branch which will increase the backward distance.
1293             bool needs_padding = (current_offset == last_avoid_back_to_back_offset);
1294             if (needs_padding &amp;&amp; offset &lt;= 0)
1295               offset -= nop_size;
1296 
1297             if (_matcher-&gt;is_short_branch_offset(mach-&gt;rule(), br_size, offset)) {
1298               // We&#39;ve got a winner.  Replace this branch.
1299               MachNode* replacement = mach-&gt;as_MachBranch()-&gt;short_branch_version();
1300 
1301               // Update the jmp_size.
1302               int new_size = replacement-&gt;size(_regalloc);
1303               assert((br_size - new_size) &gt;= (int)nop_size, &quot;short_branch size should be smaller&quot;);
</pre>
<hr />
<pre>
1321               mach-&gt;subsume_by(replacement, C);
1322               n    = replacement;
1323               mach = replacement;
1324             }
1325           }
1326           mach-&gt;as_MachBranch()-&gt;label_set( &amp;blk_labels[block_num], block_num );
1327         } else if (mach-&gt;ideal_Opcode() == Op_Jump) {
1328           for (uint h = 0; h &lt; block-&gt;_num_succs; h++) {
1329             Block* succs_block = block-&gt;_succs[h];
1330             for (uint j = 1; j &lt; succs_block-&gt;num_preds(); j++) {
1331               Node* jpn = succs_block-&gt;pred(j);
1332               if (jpn-&gt;is_JumpProj() &amp;&amp; jpn-&gt;in(0) == mach) {
1333                 uint block_num = succs_block-&gt;non_connector()-&gt;_pre_order;
1334                 Label *blkLabel = &amp;blk_labels[block_num];
1335                 mach-&gt;add_case_label(jpn-&gt;as_JumpProj()-&gt;proj_no(), blkLabel);
1336               }
1337             }
1338           }
1339         }
1340 #ifdef ASSERT
<span class="line-modified">1341         // Check that oop-store precedes the card-mark</span>
1342         else if (mach-&gt;ideal_Opcode() == Op_StoreCM) {
1343           uint storeCM_idx = j;
1344           int count = 0;
1345           for (uint prec = mach-&gt;req(); prec &lt; mach-&gt;len(); prec++) {
1346             Node *oop_store = mach-&gt;in(prec);  // Precedence edge
1347             if (oop_store == NULL) continue;
1348             count++;
1349             uint i4;
1350             for (i4 = 0; i4 &lt; last_inst; ++i4) {
1351               if (block-&gt;get_node(i4) == oop_store) {
1352                 break;
1353               }
1354             }
1355             // Note: This test can provide a false failure if other precedence
1356             // edges have been added to the storeCMNode.
1357             assert(i4 == last_inst || i4 &lt; storeCM_idx, &quot;CM card-mark executes before oop-store&quot;);
1358           }
1359           assert(count &gt; 0, &quot;storeCM expects at least one precedence edge&quot;);
1360         }
1361 #endif
</pre>
<hr />
<pre>
1364           // it&#39;s followed by a flag-kill and a null-check.  Happens on
1365           // Intel all the time, with add-to-memory kind of opcodes.
1366           previous_offset = current_offset;
1367         }
1368 
1369         // Not an else-if!
1370         // If this is a trap based cmp then add its offset to the list.
1371         if (mach-&gt;is_TrapBasedCheckNode()) {
1372           inct_starts[inct_cnt++] = current_offset;
1373         }
1374       }
1375 
1376       // Verify that there is sufficient space remaining
1377       cb-&gt;insts()-&gt;maybe_expand_to_ensure_remaining(MAX_inst_size);
1378       if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1379         C-&gt;record_failure(&quot;CodeCache is full&quot;);
1380         return;
1381       }
1382 
1383       // Save the offset for the listing
<span class="line-modified">1384 #ifndef PRODUCT</span>
<span class="line-modified">1385       if (node_offsets &amp;&amp; n-&gt;_idx &lt; node_offset_limit)</span>
1386         node_offsets[n-&gt;_idx] = cb-&gt;insts_size();

1387 #endif
1388 
1389       // &quot;Normal&quot; instruction case
1390       DEBUG_ONLY( uint instr_offset = cb-&gt;insts_size(); )
1391       n-&gt;emit(*cb, _regalloc);
1392       current_offset  = cb-&gt;insts_size();
1393 
1394       // Above we only verified that there is enough space in the instruction section.
1395       // However, the instruction may emit stubs that cause code buffer expansion.
1396       // Bail out here if expansion failed due to a lack of code cache space.
1397       if (failing()) {
1398         return;
1399       }
1400 
1401 #ifdef ASSERT
1402       if (n-&gt;size(_regalloc) &lt; (current_offset-instr_offset)) {
1403         n-&gt;dump();
1404         assert(false, &quot;wrong size of mach node&quot;);
1405       }
1406 #endif
</pre>
<hr />
<pre>
1413       // in the case that return address is not actually at current_offset.
1414       // This is a small price to pay.
1415 
1416       if (is_mcall) {
1417         last_call_offset = current_offset;
1418       }
1419 
1420       if (n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;avoid_back_to_back(MachNode::AVOID_AFTER)) {
1421         // Avoid back to back some instructions.
1422         last_avoid_back_to_back_offset = current_offset;
1423       }
1424 
1425       // See if this instruction has a delay slot
1426       if (valid_bundle_info(n) &amp;&amp; node_bundling(n)-&gt;use_unconditional_delay()) {
1427         guarantee(delay_slot != NULL, &quot;expecting delay slot node&quot;);
1428 
1429         // Back up 1 instruction
1430         cb-&gt;set_insts_end(cb-&gt;insts_end() - Pipeline::instr_unit_size());
1431 
1432         // Save the offset for the listing
<span class="line-modified">1433 #ifndef PRODUCT</span>
<span class="line-modified">1434         if (node_offsets &amp;&amp; delay_slot-&gt;_idx &lt; node_offset_limit)</span>
1435           node_offsets[delay_slot-&gt;_idx] = cb-&gt;insts_size();

1436 #endif
1437 
1438         // Support a SafePoint in the delay slot
1439         if (delay_slot-&gt;is_MachSafePoint()) {
1440           MachNode *mach = delay_slot-&gt;as_Mach();
1441           // !!!!! Stubs only need an oopmap right now, so bail out
1442           if (!mach-&gt;is_MachCall() &amp;&amp; mach-&gt;as_MachSafePoint()-&gt;jvms()-&gt;method() == NULL) {
1443             // Write the oopmap directly to the code blob??!!
1444             delay_slot = NULL;
1445             continue;
1446           }
1447 
1448           int adjusted_offset = current_offset - Pipeline::instr_unit_size();
1449           non_safepoints.observe_safepoint(mach-&gt;as_MachSafePoint()-&gt;jvms(),
1450                                            adjusted_offset);
1451           // Generate an OopMap entry
1452           Process_OopMap_Node(mach, adjusted_offset);
1453         }
1454 
1455         // Insert the delay slot instruction
</pre>
<hr />
<pre>
1490 
1491   // Define a pseudo-label at the end of the code
1492   MacroAssembler(cb).bind( blk_labels[nblocks] );
1493 
1494   // Compute the size of the first block
1495   _first_block_size = blk_labels[1].loc_pos() - blk_labels[0].loc_pos();
1496 
1497 #ifdef ASSERT
1498   for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
1499     if (jmp_target[i] != 0) {
1500       int br_size = jmp_size[i];
1501       int offset = blk_starts[jmp_target[i]]-(blk_starts[i] + jmp_offset[i]);
1502       if (!_matcher-&gt;is_short_branch_offset(jmp_rule[i], br_size, offset)) {
1503         tty-&gt;print_cr(&quot;target (%d) - jmp_offset(%d) = offset (%d), jump_size(%d), jmp_block B%d, target_block B%d&quot;, blk_starts[jmp_target[i]], blk_starts[i] + jmp_offset[i], offset, br_size, i, jmp_target[i]);
1504         assert(false, &quot;Displacement too large for short jmp&quot;);
1505       }
1506     }
1507   }
1508 #endif
1509 




1510 #ifndef PRODUCT
1511   // Information on the size of the method, without the extraneous code
1512   Scheduling::increment_method_size(cb-&gt;insts_size());
1513 #endif
1514 
1515   // ------------------
1516   // Fill in exception table entries.
1517   FillExceptionTables(inct_cnt, call_returns, inct_starts, blk_labels);
1518 
1519   // Only java methods have exception handlers and deopt handlers
1520   // class HandlerImpl is platform-specific and defined in the *.ad files.
1521   if (_method) {
1522     // Emit the exception handler code.
1523     _code_offsets.set_value(CodeOffsets::Exceptions, HandlerImpl::emit_exception_handler(*cb));
1524     if (failing()) {
1525       return; // CodeBuffer::expand failed
1526     }
1527     // Emit the deopt handler code.
1528     _code_offsets.set_value(CodeOffsets::Deopt, HandlerImpl::emit_deopt_handler(*cb));
1529 
1530     // Emit the MethodHandle deopt handler code (if required).
1531     if (has_method_handle_invokes() &amp;&amp; !failing()) {
1532       // We can use the same code as for the normal deopt handler, we
1533       // just need a different entry point address.
1534       _code_offsets.set_value(CodeOffsets::DeoptMH, HandlerImpl::emit_deopt_handler(*cb));
1535     }
1536   }
1537 
1538   // One last check for failed CodeBuffer::expand:
1539   if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1540     C-&gt;record_failure(&quot;CodeCache is full&quot;);
1541     return;
1542   }
1543 
<span class="line-modified">1544 #ifndef PRODUCT</span>







1545   // Dump the assembly code, including basic-block numbers
1546   if (print_assembly()) {
1547     ttyLocker ttyl;  // keep the following output all in one block
1548     if (!VMThread::should_terminate()) {  // test this under the tty lock
1549       // This output goes directly to the tty, not the compiler log.
1550       // To enable tools to match it up with the compilation activity,
1551       // be sure to tag this tty output with the compile ID.
1552       if (xtty != NULL) {
1553         xtty-&gt;head(&quot;opto_assembly compile_id=&#39;%d&#39;%s&quot;, compile_id(),
1554                    is_osr_compilation()    ? &quot; compile_kind=&#39;osr&#39;&quot; :
1555                    &quot;&quot;);
1556       }
1557       if (method() != NULL) {

1558         method()-&gt;print_metadata();
1559       } else if (stub_name() != NULL) {
<span class="line-modified">1560         tty-&gt;print_cr(&quot;Generating RuntimeStub - %s&quot;, stub_name());</span>
1561       }


1562       dump_asm(node_offsets, node_offset_limit);

1563       if (xtty != NULL) {
1564         // print_metadata and dump_asm above may safepoint which makes us loose the ttylock.
1565         // Retake lock too make sure the end tag is coherent, and that xmlStream-&gt;pop_tag is done
1566         // thread safe
1567         ttyLocker ttyl2;
1568         xtty-&gt;tail(&quot;opto_assembly&quot;);
1569       }
1570     }
1571   }
1572 #endif
<span class="line-removed">1573 </span>
1574 }
1575 
1576 void Compile::FillExceptionTables(uint cnt, uint *call_returns, uint *inct_starts, Label *blk_labels) {
1577   _inc_table.set_size(cnt);
1578 
1579   uint inct_cnt = 0;
1580   for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {
1581     Block* block = _cfg-&gt;get_block(i);
1582     Node *n = NULL;
1583     int j;
1584 
1585     // Find the branch; ignore trailing NOPs.
1586     for (j = block-&gt;number_of_nodes() - 1; j &gt;= 0; j--) {
1587       n = block-&gt;get_node(j);
1588       if (!n-&gt;is_Mach() || n-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Con) {
1589         break;
1590       }
1591     }
1592 
1593     // If we didn&#39;t find anything, continue
</pre>
<hr />
<pre>
1654     if (n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;is_TrapBasedCheckNode()) {
1655       uint block_num = block-&gt;non_connector_successor(0)-&gt;_pre_order;
1656       _inc_table.append(inct_starts[inct_cnt++], blk_labels[block_num].loc_pos());
1657       continue;
1658     }
1659   } // End of for all blocks fill in exception table entries
1660 }
1661 
1662 // Static Variables
1663 #ifndef PRODUCT
1664 uint Scheduling::_total_nop_size = 0;
1665 uint Scheduling::_total_method_size = 0;
1666 uint Scheduling::_total_branches = 0;
1667 uint Scheduling::_total_unconditional_delays = 0;
1668 uint Scheduling::_total_instructions_per_bundle[Pipeline::_max_instrs_per_cycle+1];
1669 #endif
1670 
1671 // Initializer for class Scheduling
1672 
1673 Scheduling::Scheduling(Arena *arena, Compile &amp;compile)
<span class="line-modified">1674   : _arena(arena),</span>
<span class="line-modified">1675     _cfg(compile.cfg()),</span>
<span class="line-modified">1676     _regalloc(compile.regalloc()),</span>
<span class="line-modified">1677     _scheduled(arena),</span>
<span class="line-modified">1678     _available(arena),</span>
<span class="line-modified">1679     _reg_node(arena),</span>
<span class="line-modified">1680     _pinch_free_list(arena),</span>
<span class="line-modified">1681     _next_node(NULL),</span>
<span class="line-modified">1682     _bundle_instr_count(0),</span>
<span class="line-modified">1683     _bundle_cycle_number(0),</span>
<span class="line-modified">1684     _bundle_use(0, 0, resource_count, &amp;_bundle_use_elements[0])</span>
1685 #ifndef PRODUCT
<span class="line-modified">1686   , _branches(0)</span>
<span class="line-modified">1687   , _unconditional_delays(0)</span>
1688 #endif
1689 {
1690   // Create a MachNopNode
1691   _nop = new MachNopNode();
1692 
1693   // Now that the nops are in the array, save the count
1694   // (but allow entries for the nops)
1695   _node_bundling_limit = compile.unique();
1696   uint node_max = _regalloc-&gt;node_regs_max_index();
1697 
1698   compile.set_node_bundling_limit(_node_bundling_limit);
1699 
1700   // This one is persistent within the Compile class
1701   _node_bundling_base = NEW_ARENA_ARRAY(compile.comp_arena(), Bundle, node_max);
1702 
1703   // Allocate space for fixed-size arrays
1704   _node_latency    = NEW_ARENA_ARRAY(arena, unsigned short, node_max);
1705   _uses            = NEW_ARENA_ARRAY(arena, short,          node_max);
1706   _current_latency = NEW_ARENA_ARRAY(arena, unsigned short, node_max);
1707 
</pre>
<hr />
<pre>
1748   _bundle_use.step(i);
1749 }
1750 
1751 void Scheduling::step_and_clear() {
1752   Bundle *bundle = node_bundling(_next_node);
1753   bundle-&gt;set_starts_bundle();
1754 
1755   // Update the bundle record
1756   if (_bundle_instr_count &gt; 0) {
1757     bundle-&gt;set_instr_count(_bundle_instr_count);
1758     bundle-&gt;set_resources_used(_bundle_use.resourcesUsed());
1759 
1760     _bundle_cycle_number += 1;
1761   }
1762 
1763   // Clear the bundling information
1764   _bundle_instr_count = 0;
1765   _bundle_use.reset();
1766 
1767   memcpy(_bundle_use_elements,
<span class="line-modified">1768     Pipeline_Use::elaborated_elements,</span>
<span class="line-modified">1769     sizeof(Pipeline_Use::elaborated_elements));</span>
1770 }
1771 
1772 // Perform instruction scheduling and bundling over the sequence of
1773 // instructions in backwards order.
1774 void Compile::ScheduleAndBundle() {
1775 
1776   // Don&#39;t optimize this if it isn&#39;t a method
1777   if (!_method)
1778     return;
1779 
1780   // Don&#39;t optimize this if scheduling is disabled
1781   if (!do_scheduling())
1782     return;
1783 
1784   // Scheduling code works only with pairs (16 bytes) maximum.
1785   if (max_vector_size() &gt; 16)
1786     return;
1787 
1788   TracePhase tp(&quot;isched&quot;, &amp;timers[_t_instrSched]);
1789 
1790   // Create a data structure for all the scheduling information
1791   Scheduling scheduling(Thread::current()-&gt;resource_area(), *this);
1792 
1793   // Walk backwards over each basic block, computing the needed alignment
1794   // Walk over all the basic blocks
1795   scheduling.DoScheduling();
















1796 }
1797 
1798 // Compute the latency of all the instructions.  This is fairly simple,
1799 // because we already have a legal ordering.  Walk over the instructions
1800 // from first to last, and compute the latency of the instruction based
1801 // on the latency of the preceding instruction(s).
1802 void Scheduling::ComputeLocalLatenciesForward(const Block *bb) {
1803 #ifndef PRODUCT
1804   if (_cfg-&gt;C-&gt;trace_opto_output())
1805     tty-&gt;print(&quot;# -&gt; ComputeLocalLatenciesForward\n&quot;);
1806 #endif
1807 
1808   // Walk over all the schedulable instructions
1809   for( uint j=_bb_start; j &lt; _bb_end; j++ ) {
1810 
1811     // This is a kludge, forcing all latency calculations to start at 1.
1812     // Used to allow latency 0 to force an instruction to the beginning
1813     // of the bb
1814     uint latency = 1;
1815     Node *use = bb-&gt;get_node(j);
</pre>
<hr />
<pre>
1844 } // end ComputeLocalLatenciesForward
1845 
1846 // See if this node fits into the present instruction bundle
1847 bool Scheduling::NodeFitsInBundle(Node *n) {
1848   uint n_idx = n-&gt;_idx;
1849 
1850   // If this is the unconditional delay instruction, then it fits
1851   if (n == _unconditional_delay_slot) {
1852 #ifndef PRODUCT
1853     if (_cfg-&gt;C-&gt;trace_opto_output())
1854       tty-&gt;print(&quot;#     NodeFitsInBundle [%4d]: TRUE; is in unconditional delay slot\n&quot;, n-&gt;_idx);
1855 #endif
1856     return (true);
1857   }
1858 
1859   // If the node cannot be scheduled this cycle, skip it
1860   if (_current_latency[n_idx] &gt; _bundle_cycle_number) {
1861 #ifndef PRODUCT
1862     if (_cfg-&gt;C-&gt;trace_opto_output())
1863       tty-&gt;print(&quot;#     NodeFitsInBundle [%4d]: FALSE; latency %4d &gt; %d\n&quot;,
<span class="line-modified">1864         n-&gt;_idx, _current_latency[n_idx], _bundle_cycle_number);</span>
1865 #endif
1866     return (false);
1867   }
1868 
1869   const Pipeline *node_pipeline = n-&gt;pipeline();
1870 
1871   uint instruction_count = node_pipeline-&gt;instructionCount();
1872   if (node_pipeline-&gt;mayHaveNoCode() &amp;&amp; n-&gt;size(_regalloc) == 0)
1873     instruction_count = 0;
1874   else if (node_pipeline-&gt;hasBranchDelay() &amp;&amp; !_unconditional_delay_slot)
1875     instruction_count++;
1876 
1877   if (_bundle_instr_count + instruction_count &gt; Pipeline::_max_instrs_per_cycle) {
1878 #ifndef PRODUCT
1879     if (_cfg-&gt;C-&gt;trace_opto_output())
1880       tty-&gt;print(&quot;#     NodeFitsInBundle [%4d]: FALSE; too many instructions: %d &gt; %d\n&quot;,
<span class="line-modified">1881         n-&gt;_idx, _bundle_instr_count + instruction_count, Pipeline::_max_instrs_per_cycle);</span>
1882 #endif
1883     return (false);
1884   }
1885 
1886   // Don&#39;t allow non-machine nodes to be handled this way
1887   if (!n-&gt;is_Mach() &amp;&amp; instruction_count == 0)
1888     return (false);
1889 
1890   // See if there is any overlap
1891   uint delay = _bundle_use.full_latency(0, node_pipeline-&gt;resourceUse());
1892 
1893   if (delay &gt; 0) {
1894 #ifndef PRODUCT
1895     if (_cfg-&gt;C-&gt;trace_opto_output())
1896       tty-&gt;print(&quot;#     NodeFitsInBundle [%4d]: FALSE; functional units overlap\n&quot;, n_idx);
1897 #endif
1898     return false;
1899   }
1900 
1901 #ifndef PRODUCT
</pre>
<hr />
<pre>
2069     // copied to the delay slot, and the branch goes to
2070     // the instruction after that at the branch target
2071     if ( n-&gt;is_MachBranch() ) {
2072 
2073       assert( !n-&gt;is_MachNullCheck(), &quot;should not look for delay slot for Null Check&quot; );
2074       assert( !n-&gt;is_Catch(),         &quot;should not look for delay slot for Catch&quot; );
2075 
2076 #ifndef PRODUCT
2077       _branches++;
2078 #endif
2079 
2080       // At least 1 instruction is on the available list
2081       // that is not dependent on the branch
2082       for (uint i = 0; i &lt; siz; i++) {
2083         Node *d = _available[i];
2084         const Pipeline *avail_pipeline = d-&gt;pipeline();
2085 
2086         // Don&#39;t allow safepoints in the branch shadow, that will
2087         // cause a number of difficulties
2088         if ( avail_pipeline-&gt;instructionCount() == 1 &amp;&amp;
<span class="line-modified">2089             !avail_pipeline-&gt;hasMultipleBundles() &amp;&amp;</span>
<span class="line-modified">2090             !avail_pipeline-&gt;hasBranchDelay() &amp;&amp;</span>
<span class="line-modified">2091             Pipeline::instr_has_unit_size() &amp;&amp;</span>
<span class="line-modified">2092             d-&gt;size(_regalloc) == Pipeline::instr_unit_size() &amp;&amp;</span>
<span class="line-modified">2093             NodeFitsInBundle(d) &amp;&amp;</span>
<span class="line-modified">2094             !node_bundling(d)-&gt;used_in_delay()) {</span>
2095 
2096           if (d-&gt;is_Mach() &amp;&amp; !d-&gt;is_MachSafePoint()) {
2097             // A node that fits in the delay slot was found, so we need to
2098             // set the appropriate bits in the bundle pipeline information so
2099             // that it correctly indicates resource usage.  Later, when we
2100             // attempt to add this instruction to the bundle, we will skip
2101             // setting the resource usage.
2102             _unconditional_delay_slot = d;
2103             node_bundling(n)-&gt;set_use_unconditional_delay();
2104             node_bundling(d)-&gt;set_used_in_unconditional_delay();
2105             _bundle_use.add_usage(avail_pipeline-&gt;resourceUse());
2106             _current_latency[d-&gt;_idx] = _bundle_cycle_number;
2107             _next_node = d;
2108             ++_bundle_instr_count;
2109 #ifndef PRODUCT
2110             _unconditional_delays++;
2111 #endif
2112             break;
2113           }
2114         }
</pre>
<hr />
<pre>
2119     if (!_unconditional_delay_slot) {
2120       // See if adding an instruction in the delay slot will overflow
2121       // the bundle.
2122       if (!NodeFitsInBundle(_nop)) {
2123 #ifndef PRODUCT
2124         if (_cfg-&gt;C-&gt;trace_opto_output())
2125           tty-&gt;print(&quot;#  *** STEP(1 instruction for delay slot) ***\n&quot;);
2126 #endif
2127         step(1);
2128       }
2129 
2130       _bundle_use.add_usage(_nop-&gt;pipeline()-&gt;resourceUse());
2131       _next_node = _nop;
2132       ++_bundle_instr_count;
2133     }
2134 
2135     // See if the instruction in the delay slot requires a
2136     // step of the bundles
2137     if (!NodeFitsInBundle(n)) {
2138 #ifndef PRODUCT
<span class="line-modified">2139         if (_cfg-&gt;C-&gt;trace_opto_output())</span>
<span class="line-modified">2140           tty-&gt;print(&quot;#  *** STEP(branch won&#39;t fit) ***\n&quot;);</span>
2141 #endif
<span class="line-modified">2142         // Update the state information</span>
<span class="line-modified">2143         _bundle_instr_count = 0;</span>
<span class="line-modified">2144         _bundle_cycle_number += 1;</span>
<span class="line-modified">2145         _bundle_use.step(1);</span>
2146     }
2147   }
2148 
2149   // Get the number of instructions
2150   uint instruction_count = node_pipeline-&gt;instructionCount();
2151   if (node_pipeline-&gt;mayHaveNoCode() &amp;&amp; n-&gt;size(_regalloc) == 0)
2152     instruction_count = 0;
2153 
2154   // Compute the latency information
2155   uint delay = 0;
2156 
2157   if (instruction_count &gt; 0 || !node_pipeline-&gt;mayHaveNoCode()) {
2158     int relative_latency = _current_latency[n-&gt;_idx] - _bundle_cycle_number;
2159     if (relative_latency &lt; 0)
2160       relative_latency = 0;
2161 
2162     delay = _bundle_use.full_latency(relative_latency, node_usage);
2163 
2164     // Does not fit in this bundle, start a new one
2165     if (delay &gt; 0) {
</pre>
<hr />
<pre>
2171 #endif
2172     }
2173   }
2174 
2175   // If this was placed in the delay slot, ignore it
2176   if (n != _unconditional_delay_slot) {
2177 
2178     if (delay == 0) {
2179       if (node_pipeline-&gt;hasMultipleBundles()) {
2180 #ifndef PRODUCT
2181         if (_cfg-&gt;C-&gt;trace_opto_output())
2182           tty-&gt;print(&quot;#  *** STEP(multiple instructions) ***\n&quot;);
2183 #endif
2184         step(1);
2185       }
2186 
2187       else if (instruction_count + _bundle_instr_count &gt; Pipeline::_max_instrs_per_cycle) {
2188 #ifndef PRODUCT
2189         if (_cfg-&gt;C-&gt;trace_opto_output())
2190           tty-&gt;print(&quot;#  *** STEP(%d &gt;= %d instructions) ***\n&quot;,
<span class="line-modified">2191             instruction_count + _bundle_instr_count,</span>
<span class="line-modified">2192             Pipeline::_max_instrs_per_cycle);</span>
2193 #endif
2194         step(1);
2195       }
2196     }
2197 
2198     if (node_pipeline-&gt;hasBranchDelay() &amp;&amp; !_unconditional_delay_slot)
2199       _bundle_instr_count++;
2200 
2201     // Set the node&#39;s latency
2202     _current_latency[n-&gt;_idx] = _bundle_cycle_number;
2203 
2204     // Now merge the functional unit information
2205     if (instruction_count &gt; 0 || !node_pipeline-&gt;mayHaveNoCode())
2206       _bundle_use.add_usage(node_usage);
2207 
2208     // Increment the number of instructions in this bundle
2209     _bundle_instr_count += instruction_count;
2210 
2211     // Remember this node for later
2212     if (n-&gt;is_Mach())
</pre>
<hr />
<pre>
2378       if( iop == Op_Node &amp;&amp;     // Do not schedule PhiNodes, ProjNodes
2379           mach-&gt;pipeline() == MachNode::pipeline_class() &amp;&amp;
2380           !n-&gt;is_SpillCopy() &amp;&amp; !n-&gt;is_MachMerge() )  // Breakpoints, Prolog, etc
2381         continue;
2382       break;                    // Funny loop structure to be sure...
2383     }
2384     // Compute last &quot;interesting&quot; instruction in block - last instruction we
2385     // might schedule.  _bb_end points just after last schedulable inst.  We
2386     // normally schedule conditional branches (despite them being forced last
2387     // in the block), because they have delay slots we can fill.  Calls all
2388     // have their delay slots filled in the template expansions, so we don&#39;t
2389     // bother scheduling them.
2390     Node *last = bb-&gt;get_node(_bb_end);
2391     // Ignore trailing NOPs.
2392     while (_bb_end &gt; 0 &amp;&amp; last-&gt;is_Mach() &amp;&amp;
2393            last-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Con) {
2394       last = bb-&gt;get_node(--_bb_end);
2395     }
2396     assert(!last-&gt;is_Mach() || last-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Con, &quot;&quot;);
2397     if( last-&gt;is_Catch() ||
<span class="line-modified">2398        // Exclude unreachable path case when Halt node is in a separate block.</span>
<span class="line-modified">2399        (_bb_end &gt; 1 &amp;&amp; last-&gt;is_Mach() &amp;&amp; last-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Halt) ) {</span>
<span class="line-modified">2400       // There must be a prior call.  Skip it.</span>
<span class="line-removed">2401       while( !bb-&gt;get_node(--_bb_end)-&gt;is_MachCall() ) {</span>
<span class="line-removed">2402         assert( bb-&gt;get_node(_bb_end)-&gt;is_MachProj(), &quot;skipping projections after expected call&quot; );</span>
<span class="line-removed">2403       }</span>
2404     } else if( last-&gt;is_MachNullCheck() ) {
2405       // Backup so the last null-checked memory instruction is
2406       // outside the schedulable range. Skip over the nullcheck,
2407       // projection, and the memory nodes.
2408       Node *mem = last-&gt;in(1);
2409       do {
2410         _bb_end--;
2411       } while (mem != bb-&gt;get_node(_bb_end));
2412     } else {
2413       // Set _bb_end to point after last schedulable inst.
2414       _bb_end++;
2415     }
2416 
2417     assert( _bb_start &lt;= _bb_end, &quot;inverted block ends&quot; );
2418 
2419     // Compute the register antidependencies for the basic block
2420     ComputeRegisterAntidependencies(bb);
2421     if (_cfg-&gt;C-&gt;failing())  return;  // too many D-U pinch points
2422 
2423     // Compute intra-bb latencies for the nodes
</pre>
<hr />
<pre>
2451       bb-&gt;map_node(_scheduled[_bb_end-k-1], k);
2452 
2453 #ifndef PRODUCT
2454     if (_cfg-&gt;C-&gt;trace_opto_output()) {
2455       tty-&gt;print(&quot;#  Schedule BB#%03d (final)\n&quot;, i);
2456       uint current = 0;
2457       for (uint j = 0; j &lt; bb-&gt;number_of_nodes(); j++) {
2458         Node *n = bb-&gt;get_node(j);
2459         if( valid_bundle_info(n) ) {
2460           Bundle *bundle = node_bundling(n);
2461           if (bundle-&gt;instr_count() &gt; 0 || bundle-&gt;flags() &gt; 0) {
2462             tty-&gt;print(&quot;*** Bundle: &quot;);
2463             bundle-&gt;dump();
2464           }
2465           n-&gt;dump();
2466         }
2467       }
2468     }
2469 #endif
2470 #ifdef ASSERT
<span class="line-modified">2471   verify_good_schedule(bb,&quot;after block local scheduling&quot;);</span>
2472 #endif
2473   }
2474 
2475 #ifndef PRODUCT
2476   if (_cfg-&gt;C-&gt;trace_opto_output())
2477     tty-&gt;print(&quot;# &lt;- DoScheduling\n&quot;);
2478 #endif
2479 
2480   // Record final node-bundling array location
2481   _regalloc-&gt;C-&gt;set_node_bundling_base(_node_bundling_base);
2482 
2483 } // end DoScheduling
2484 
2485 // Verify that no live-range used in the block is killed in the block by a
2486 // wrong DEF.  This doesn&#39;t verify live-ranges that span blocks.
2487 
2488 // Check for edge existence.  Used to avoid adding redundant precedence edges.
2489 static bool edge_from_to( Node *from, Node *to ) {
2490   for( uint i=0; i&lt;from-&gt;len(); i++ )
2491     if( from-&gt;in(i) == to )
</pre>
<hr />
<pre>
2799 // register is anti-dependent on a set of uses (or defs), rather
2800 // than adding an edge in the graph between each pair of kill
2801 // and use (or def), a pinch is inserted between them:
2802 //
2803 //            use1   use2  use3
2804 //                \   |   /
2805 //                 \  |  /
2806 //                  pinch
2807 //                 /  |  \
2808 //                /   |   \
2809 //            kill1 kill2 kill3
2810 //
2811 // One pinch node is created per register killed when
2812 // the second call is encountered during a backwards pass
2813 // over the block.  Most of these pinch nodes are never
2814 // wired into the graph because the register is never
2815 // used or def&#39;ed in the block.
2816 //
2817 void Scheduling::garbage_collect_pinch_nodes() {
2818 #ifndef PRODUCT
<span class="line-modified">2819     if (_cfg-&gt;C-&gt;trace_opto_output()) tty-&gt;print(&quot;Reclaimed pinch nodes:&quot;);</span>
<span class="line-modified">2820 #endif</span>
<span class="line-modified">2821     int trace_cnt = 0;</span>
<span class="line-modified">2822     for (uint k = 0; k &lt; _reg_node.Size(); k++) {</span>
<span class="line-modified">2823       Node* pinch = _reg_node[k];</span>
<span class="line-modified">2824       if ((pinch != NULL) &amp;&amp; pinch-&gt;Opcode() == Op_Node &amp;&amp;</span>
<span class="line-modified">2825           // no predecence input edges</span>
<span class="line-modified">2826           (pinch-&gt;req() == pinch-&gt;len() || pinch-&gt;in(pinch-&gt;req()) == NULL) ) {</span>
<span class="line-modified">2827         cleanup_pinch(pinch);</span>
<span class="line-modified">2828         _pinch_free_list.push(pinch);</span>
<span class="line-modified">2829         _reg_node.map(k, NULL);</span>
2830 #ifndef PRODUCT
<span class="line-modified">2831         if (_cfg-&gt;C-&gt;trace_opto_output()) {</span>
<span class="line-modified">2832           trace_cnt++;</span>
<span class="line-modified">2833           if (trace_cnt &gt; 40) {</span>
<span class="line-modified">2834             tty-&gt;print(&quot;\n&quot;);</span>
<span class="line-modified">2835             trace_cnt = 0;</span>
<span class="line-removed">2836           }</span>
<span class="line-removed">2837           tty-&gt;print(&quot; %d&quot;, pinch-&gt;_idx);</span>
2838         }
<span class="line-modified">2839 #endif</span>
2840       }

2841     }

2842 #ifndef PRODUCT
<span class="line-modified">2843     if (_cfg-&gt;C-&gt;trace_opto_output()) tty-&gt;print(&quot;\n&quot;);</span>
2844 #endif
2845 }
2846 
2847 // Clean up a pinch node for reuse.
2848 void Scheduling::cleanup_pinch( Node *pinch ) {
2849   assert (pinch &amp;&amp; pinch-&gt;Opcode() == Op_Node &amp;&amp; pinch-&gt;req() == 1, &quot;just checking&quot;);
2850 
2851   for (DUIterator_Last imin, i = pinch-&gt;last_outs(imin); i &gt;= imin; ) {
2852     Node* use = pinch-&gt;last_out(i);
2853     uint uses_found = 0;
2854     for (uint j = use-&gt;req(); j &lt; use-&gt;len(); j++) {
2855       if (use-&gt;in(j) == pinch) {
2856         use-&gt;rm_prec(j);
2857         uses_found++;
2858       }
2859     }
2860     assert(uses_found &gt; 0, &quot;must be a precedence edge&quot;);
2861     i -= uses_found;    // we deleted 1 or more copies of this edge
2862   }
2863   // May have a later_def entry
2864   pinch-&gt;set_req(0, NULL);
2865 }
2866 
2867 #ifndef PRODUCT
2868 
2869 void Scheduling::dump_available() const {
2870   tty-&gt;print(&quot;#Availist  &quot;);
2871   for (uint i = 0; i &lt; _available.size(); i++)
2872     tty-&gt;print(&quot; N%d/l%d&quot;, _available[i]-&gt;_idx,_current_latency[_available[i]-&gt;_idx]);
2873   tty-&gt;cr();
2874 }
2875 
2876 // Print Scheduling Statistics
2877 void Scheduling::print_statistics() {
2878   // Print the size added by nops for bundling
2879   tty-&gt;print(&quot;Nops added %d bytes to total of %d bytes&quot;,
<span class="line-modified">2880     _total_nop_size, _total_method_size);</span>
2881   if (_total_method_size &gt; 0)
2882     tty-&gt;print(&quot;, for %.2f%%&quot;,
<span class="line-modified">2883       ((double)_total_nop_size) / ((double) _total_method_size) * 100.0);</span>
2884   tty-&gt;print(&quot;\n&quot;);
2885 
2886   // Print the number of branch shadows filled
2887   if (Pipeline::_branch_has_delay_slot) {
2888     tty-&gt;print(&quot;Of %d branches, %d had unconditional delay slots filled&quot;,
<span class="line-modified">2889       _total_branches, _total_unconditional_delays);</span>
2890     if (_total_branches &gt; 0)
2891       tty-&gt;print(&quot;, for %.2f%%&quot;,
<span class="line-modified">2892         ((double)_total_unconditional_delays) / ((double)_total_branches) * 100.0);</span>
2893     tty-&gt;print(&quot;\n&quot;);
2894   }
2895 
2896   uint total_instructions = 0, total_bundles = 0;
2897 
2898   for (uint i = 1; i &lt;= Pipeline::_max_instrs_per_cycle; i++) {
2899     uint bundle_count   = _total_instructions_per_bundle[i];
2900     total_instructions += bundle_count * i;
2901     total_bundles      += bundle_count;
2902   }
2903 
2904   if (total_bundles &gt; 0)
2905     tty-&gt;print(&quot;Average ILP (excluding nops) is %.2f\n&quot;,
<span class="line-modified">2906       ((double)total_instructions) / ((double)total_bundles));</span>
2907 }
2908 #endif
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1998, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/assembler.inline.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;code/compiledIC.hpp&quot;
  29 #include &quot;code/debugInfo.hpp&quot;
  30 #include &quot;code/debugInfoRec.hpp&quot;
  31 #include &quot;compiler/compileBroker.hpp&quot;
  32 #include &quot;compiler/compilerDirectives.hpp&quot;
  33 #include &quot;compiler/oopMap.hpp&quot;
<span class="line-added">  34 #include &quot;gc/shared/barrierSet.hpp&quot;</span>
<span class="line-added">  35 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;</span>
  36 #include &quot;memory/allocation.inline.hpp&quot;
  37 #include &quot;opto/ad.hpp&quot;
  38 #include &quot;opto/callnode.hpp&quot;
  39 #include &quot;opto/cfgnode.hpp&quot;
  40 #include &quot;opto/locknode.hpp&quot;
  41 #include &quot;opto/machnode.hpp&quot;
  42 #include &quot;opto/optoreg.hpp&quot;
  43 #include &quot;opto/output.hpp&quot;
  44 #include &quot;opto/regalloc.hpp&quot;
  45 #include &quot;opto/runtime.hpp&quot;
  46 #include &quot;opto/subnode.hpp&quot;
  47 #include &quot;opto/type.hpp&quot;
  48 #include &quot;runtime/handles.inline.hpp&quot;
<span class="line-added">  49 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  50 #include &quot;utilities/xmlstream.hpp&quot;
  51 
  52 #ifndef PRODUCT
  53 #define DEBUG_ARG(x) , x
  54 #else
  55 #define DEBUG_ARG(x)
  56 #endif
  57 
  58 // Convert Nodes to instruction bits and pass off to the VM
  59 void Compile::Output() {
  60   // RootNode goes
  61   assert( _cfg-&gt;get_root_block()-&gt;number_of_nodes() == 0, &quot;&quot; );
  62 
  63   // The number of new nodes (mostly MachNop) is proportional to
  64   // the number of java calls and inner loops which are aligned.
  65   if ( C-&gt;check_node_count((NodeLimitFudgeFactor + C-&gt;java_calls()*3 +
  66                             C-&gt;inner_loops()*(OptoLoopAlignment-1)),
  67                            &quot;out of nodes before code generation&quot; ) ) {
  68     return;
  69   }
</pre>
<hr />
<pre>
 100       (OptoBreakpointOSR &amp;&amp; is_osr_compilation())       ||
 101       (OptoBreakpointC2R &amp;&amp; !_method)                   ) {
 102     // checking for _method means that OptoBreakpoint does not apply to
 103     // runtime stubs or frame converters
 104     _cfg-&gt;insert( entry, 1, new MachBreakpointNode() );
 105   }
 106 
 107   // Insert epilogs before every return
 108   for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {
 109     Block* block = _cfg-&gt;get_block(i);
 110     if (!block-&gt;is_connector() &amp;&amp; block-&gt;non_connector_successor(0) == _cfg-&gt;get_root_block()) { // Found a program exit point?
 111       Node* m = block-&gt;end();
 112       if (m-&gt;is_Mach() &amp;&amp; m-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Halt) {
 113         MachEpilogNode* epilog = new MachEpilogNode(m-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Return);
 114         block-&gt;add_inst(epilog);
 115         _cfg-&gt;map_node_to_block(epilog, block);
 116       }
 117     }
 118   }
 119 
<span class="line-added"> 120   // Keeper of sizing aspects</span>
<span class="line-added"> 121   BufferSizingData buf_sizes = BufferSizingData();</span>
<span class="line-added"> 122 </span>
<span class="line-added"> 123   // Initialize code buffer</span>
<span class="line-added"> 124   estimate_buffer_size(buf_sizes._const);</span>
<span class="line-added"> 125   if (failing()) return;</span>
<span class="line-added"> 126 </span>
<span class="line-added"> 127   // Pre-compute the length of blocks and replace</span>
<span class="line-added"> 128   // long branches with short if machine supports it.</span>
<span class="line-added"> 129   // Must be done before ScheduleAndBundle due to SPARC delay slots</span>
 130   uint* blk_starts = NEW_RESOURCE_ARRAY(uint, _cfg-&gt;number_of_blocks() + 1);
 131   blk_starts[0] = 0;
<span class="line-added"> 132   shorten_branches(blk_starts, buf_sizes);</span>
 133 
<span class="line-modified"> 134   ScheduleAndBundle();</span>
<span class="line-modified"> 135   if (failing()) {</span>


 136     return;
 137   }
 138 
<span class="line-modified"> 139   // Late barrier analysis must be done after schedule and bundle</span>
<span class="line-modified"> 140   // Otherwise liveness based spilling will fail</span>
<span class="line-modified"> 141   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();</span>
<span class="line-modified"> 142   bs-&gt;late_barrier_analysis();</span>













 143 
<span class="line-modified"> 144   // Complete sizing of codebuffer</span>
<span class="line-added"> 145   CodeBuffer* cb = init_buffer(buf_sizes);</span>
<span class="line-added"> 146   if (cb == NULL || failing()) {</span>
 147     return;
 148   }
 149 
 150   BuildOopMaps();
 151 
 152   if (failing())  {
 153     return;
 154   }
 155 
 156   fill_buffer(cb, blk_starts);
 157 }
 158 
 159 bool Compile::need_stack_bang(int frame_size_in_bytes) const {
 160   // Determine if we need to generate a stack overflow check.
 161   // Do it if the method is not a stub function and
 162   // has java calls or has frame size &gt; vm_page_size/8.
 163   // The debug VM checks that deoptimization doesn&#39;t trigger an
 164   // unexpected stack overflow (compiled method stack banging should
 165   // guarantee it doesn&#39;t happen) so we always need the stack bang in
 166   // a debug VM.
</pre>
<hr />
<pre>
 207         // Check subsequent fallthrough blocks if the loop&#39;s first
 208         // block(s) does not have enough instructions.
 209         Block *nb = block;
 210         while(inst_cnt &gt; 0 &amp;&amp;
 211               i &lt; last_block &amp;&amp;
 212               !_cfg-&gt;get_block(i + 1)-&gt;has_loop_alignment() &amp;&amp;
 213               !nb-&gt;has_successor(block)) {
 214           i++;
 215           nb = _cfg-&gt;get_block(i);
 216           inst_cnt  = nb-&gt;compute_first_inst_size(sum_size, inst_cnt, _regalloc);
 217         } // while( inst_cnt &gt; 0 &amp;&amp; i &lt; last_block  )
 218 
 219         block-&gt;set_first_inst_size(sum_size);
 220       } // f( b-&gt;head()-&gt;is_Loop() )
 221     } // for( i &lt;= last_block )
 222   } // if( MaxLoopPad &lt; OptoLoopAlignment-1 )
 223 }
 224 
 225 // The architecture description provides short branch variants for some long
 226 // branch instructions. Replace eligible long branches with short branches.
<span class="line-modified"> 227 void Compile::shorten_branches(uint* blk_starts, BufferSizingData&amp; buf_sizes) {</span>
 228   // Compute size of each block, method size, and relocation information size
 229   uint nblocks  = _cfg-&gt;number_of_blocks();
 230 
 231   uint*      jmp_offset = NEW_RESOURCE_ARRAY(uint,nblocks);
 232   uint*      jmp_size   = NEW_RESOURCE_ARRAY(uint,nblocks);
 233   int*       jmp_nidx   = NEW_RESOURCE_ARRAY(int ,nblocks);
 234 
 235   // Collect worst case block paddings
 236   int* block_worst_case_pad = NEW_RESOURCE_ARRAY(int, nblocks);
 237   memset(block_worst_case_pad, 0, nblocks * sizeof(int));
 238 
 239   DEBUG_ONLY( uint *jmp_target = NEW_RESOURCE_ARRAY(uint,nblocks); )
 240   DEBUG_ONLY( uint *jmp_rule = NEW_RESOURCE_ARRAY(uint,nblocks); )
 241 
 242   bool has_short_branch_candidate = false;
 243 
 244   // Initialize the sizes to 0
<span class="line-modified"> 245   int code_size  = 0;          // Size in bytes of generated code</span>
<span class="line-modified"> 246   int stub_size  = 0;          // Size in bytes of all stub entries</span>
 247   // Size in bytes of all relocation entries, including those in local stubs.
 248   // Start with 2-bytes of reloc info for the unvalidated entry point
<span class="line-modified"> 249   int reloc_size = 1;          // Number of relocation entries</span>
 250 
 251   // Make three passes.  The first computes pessimistic blk_starts,
 252   // relative jmp_offset and reloc_size information.  The second performs
 253   // short branch substitution using the pessimistic sizing.  The
 254   // third inserts nops where needed.
 255 
 256   // Step one, perform a pessimistic sizing pass.
 257   uint last_call_adr = max_juint;
 258   uint last_avoid_back_to_back_adr = max_juint;
 259   uint nop_size = (new MachNopNode())-&gt;size(_regalloc);
 260   for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
 261     Block* block = _cfg-&gt;get_block(i);
 262 
 263     // During short branch replacement, we store the relative (to blk_starts)
 264     // offset of jump in jmp_offset, rather than the absolute offset of jump.
 265     // This is so that we do not need to recompute sizes of all nodes when
 266     // we compute correct blk_starts in our next sizing pass.
 267     jmp_offset[i] = 0;
 268     jmp_size[i]   = 0;
 269     jmp_nidx[i]   = -1;
</pre>
<hr />
<pre>
 463       assert(_matcher-&gt;is_short_branch_offset(jmp_rule[i], br_size, offset), &quot;Displacement too large for short jmp&quot;);
 464     }
 465   }
 466 #endif
 467 
 468   // Step 3, compute the offsets of all blocks, will be done in fill_buffer()
 469   // after ScheduleAndBundle().
 470 
 471   // ------------------
 472   // Compute size for code buffer
 473   code_size = blk_starts[nblocks];
 474 
 475   // Relocation records
 476   reloc_size += 1;              // Relo entry for exception handler
 477 
 478   // Adjust reloc_size to number of record of relocation info
 479   // Min is 2 bytes, max is probably 6 or 8, with a tax up to 25% for
 480   // a relocation index.
 481   // The CodeBuffer will expand the locs array if this estimate is too low.
 482   reloc_size *= 10 / sizeof(relocInfo);
<span class="line-added"> 483 </span>
<span class="line-added"> 484   buf_sizes._reloc = reloc_size;</span>
<span class="line-added"> 485   buf_sizes._code  = code_size;</span>
<span class="line-added"> 486   buf_sizes._stub  = stub_size;</span>
 487 }
 488 
 489 //------------------------------FillLocArray-----------------------------------
 490 // Create a bit of debug info and append it to the array.  The mapping is from
 491 // Java local or expression stack to constant, register or stack-slot.  For
 492 // doubles, insert 2 mappings and return 1 (to tell the caller that the next
 493 // entry has been taken care of and caller should skip it).
 494 static LocationValue *new_loc_value( PhaseRegAlloc *ra, OptoReg::Name regnum, Location::Type l_type ) {
 495   // This should never have accepted Bad before
 496   assert(OptoReg::is_valid(regnum), &quot;location must be valid&quot;);
 497   return (OptoReg::is_reg(regnum))
<span class="line-modified"> 498          ? new LocationValue(Location::new_reg_loc(l_type, OptoReg::as_VMReg(regnum)) )</span>
<span class="line-modified"> 499          : new LocationValue(Location::new_stk_loc(l_type,  ra-&gt;reg2offset(regnum)));</span>
 500 }
 501 
 502 
 503 ObjectValue*
 504 Compile::sv_for_node_id(GrowableArray&lt;ScopeValue*&gt; *objs, int id) {
 505   for (int i = 0; i &lt; objs-&gt;length(); i++) {
 506     assert(objs-&gt;at(i)-&gt;is_object(), &quot;corrupt object cache&quot;);
 507     ObjectValue* sv = (ObjectValue*) objs-&gt;at(i);
 508     if (sv-&gt;id() == id) {
 509       return sv;
 510     }
 511   }
 512   // Otherwise..
 513   return NULL;
 514 }
 515 
 516 void Compile::set_sv_for_object_node(GrowableArray&lt;ScopeValue*&gt; *objs,
 517                                      ObjectValue* sv ) {
 518   assert(sv_for_node_id(objs, sv-&gt;id()) == NULL, &quot;Precondition&quot;);
 519   objs-&gt;append(sv);
</pre>
<hr />
<pre>
 598     if (t-&gt;base() == Type::Long &amp;&amp; OptoReg::is_reg(regnum)) {
 599       // For SPARC we have to swap high and low words for
 600       // long values stored in a single-register (g0-g7).
 601       array-&gt;append(new_loc_value( _regalloc,              regnum   , Location::normal ));
 602       array-&gt;append(new_loc_value( _regalloc, OptoReg::add(regnum,1), Location::normal ));
 603     } else
 604 #endif //SPARC
 605     if( t-&gt;base() == Type::DoubleBot || t-&gt;base() == Type::DoubleCon || t-&gt;base() == Type::Long ) {
 606       // Repack the double/long as two jints.
 607       // The convention the interpreter uses is that the second local
 608       // holds the first raw word of the native double representation.
 609       // This is actually reasonable, since locals and stack arrays
 610       // grow downwards in all implementations.
 611       // (If, on some machine, the interpreter&#39;s Java locals or stack
 612       // were to grow upwards, the embedded doubles would be word-swapped.)
 613       array-&gt;append(new_loc_value( _regalloc, OptoReg::add(regnum,1), Location::normal ));
 614       array-&gt;append(new_loc_value( _regalloc,              regnum   , Location::normal ));
 615     }
 616 #endif //_LP64
 617     else if( (t-&gt;base() == Type::FloatBot || t-&gt;base() == Type::FloatCon) &amp;&amp;
<span class="line-modified"> 618              OptoReg::is_reg(regnum) ) {</span>
 619       array-&gt;append(new_loc_value( _regalloc, regnum, Matcher::float_in_double()
<span class="line-modified"> 620                                                       ? Location::float_in_dbl : Location::normal ));</span>
 621     } else if( t-&gt;base() == Type::Int &amp;&amp; OptoReg::is_reg(regnum) ) {
 622       array-&gt;append(new_loc_value( _regalloc, regnum, Matcher::int_in_long
<span class="line-modified"> 623                                                       ? Location::int_in_long : Location::normal ));</span>
 624     } else if( t-&gt;base() == Type::NarrowOop ) {
 625       array-&gt;append(new_loc_value( _regalloc, regnum, Location::narrowoop ));
 626     } else {
 627       array-&gt;append(new_loc_value( _regalloc, regnum, _regalloc-&gt;is_oop(local) ? Location::oop : Location::normal ));
 628     }
 629     return;
 630   }
 631 
 632   // No register.  It must be constant data.
 633   switch (t-&gt;base()) {
<span class="line-modified"> 634     case Type::Half:              // Second half of a double</span>
<span class="line-modified"> 635       ShouldNotReachHere();       // Caller should skip 2nd halves</span>
<span class="line-modified"> 636       break;</span>
<span class="line-modified"> 637     case Type::AnyPtr:</span>








 638       array-&gt;append(new ConstantOopWriteValue(NULL));
<span class="line-modified"> 639       break;</span>
<span class="line-modified"> 640     case Type::AryPtr:</span>
<span class="line-modified"> 641     case Type::InstPtr:          // fall through</span>
<span class="line-modified"> 642       array-&gt;append(new ConstantOopWriteValue(t-&gt;isa_oopptr()-&gt;const_oop()-&gt;constant_encoding()));</span>
<span class="line-modified"> 643       break;</span>
<span class="line-modified"> 644     case Type::NarrowOop:</span>
<span class="line-modified"> 645       if (t == TypeNarrowOop::NULL_PTR) {</span>
<span class="line-modified"> 646         array-&gt;append(new ConstantOopWriteValue(NULL));</span>
<span class="line-modified"> 647       } else {</span>
<span class="line-modified"> 648         array-&gt;append(new ConstantOopWriteValue(t-&gt;make_ptr()-&gt;isa_oopptr()-&gt;const_oop()-&gt;constant_encoding()));</span>
<span class="line-added"> 649       }</span>
<span class="line-added"> 650       break;</span>
<span class="line-added"> 651     case Type::Int:</span>
<span class="line-added"> 652       array-&gt;append(new ConstantIntValue(t-&gt;is_int()-&gt;get_con()));</span>
<span class="line-added"> 653       break;</span>
<span class="line-added"> 654     case Type::RawPtr:</span>
<span class="line-added"> 655       // A return address (T_ADDRESS).</span>
<span class="line-added"> 656       assert((intptr_t)t-&gt;is_ptr()-&gt;get_con() &lt; (intptr_t)0x10000, &quot;must be a valid BCI&quot;);</span>
 657 #ifdef _LP64
<span class="line-modified"> 658       // Must be restored to the full-width 64-bit stack slot.</span>
<span class="line-modified"> 659       array-&gt;append(new ConstantLongValue(t-&gt;is_ptr()-&gt;get_con()));</span>
 660 #else
<span class="line-modified"> 661       array-&gt;append(new ConstantIntValue(t-&gt;is_ptr()-&gt;get_con()));</span>
 662 #endif
<span class="line-modified"> 663       break;</span>
<span class="line-modified"> 664     case Type::FloatCon: {</span>
<span class="line-modified"> 665       float f = t-&gt;is_float_constant()-&gt;getf();</span>
<span class="line-modified"> 666       array-&gt;append(new ConstantIntValue(jint_cast(f)));</span>
<span class="line-modified"> 667       break;</span>
<span class="line-modified"> 668     }</span>
<span class="line-modified"> 669     case Type::DoubleCon: {</span>
<span class="line-modified"> 670       jdouble d = t-&gt;is_double_constant()-&gt;getd();</span>
 671 #ifdef _LP64
<span class="line-modified"> 672       array-&gt;append(new ConstantIntValue((jint)0));</span>
<span class="line-modified"> 673       array-&gt;append(new ConstantDoubleValue(d));</span>
 674 #else
<span class="line-modified"> 675       // Repack the double as two jints.</span>
 676     // The convention the interpreter uses is that the second local
 677     // holds the first raw word of the native double representation.
 678     // This is actually reasonable, since locals and stack arrays
 679     // grow downwards in all implementations.
 680     // (If, on some machine, the interpreter&#39;s Java locals or stack
 681     // were to grow upwards, the embedded doubles would be word-swapped.)
 682     jlong_accessor acc;
 683     acc.long_value = jlong_cast(d);
 684     array-&gt;append(new ConstantIntValue(acc.words[1]));
 685     array-&gt;append(new ConstantIntValue(acc.words[0]));
 686 #endif
<span class="line-modified"> 687       break;</span>
<span class="line-modified"> 688     }</span>
<span class="line-modified"> 689     case Type::Long: {</span>
<span class="line-modified"> 690       jlong d = t-&gt;is_long()-&gt;get_con();</span>
 691 #ifdef _LP64
<span class="line-modified"> 692       array-&gt;append(new ConstantIntValue((jint)0));</span>
<span class="line-modified"> 693       array-&gt;append(new ConstantLongValue(d));</span>
 694 #else
<span class="line-modified"> 695       // Repack the long as two jints.</span>
 696     // The convention the interpreter uses is that the second local
 697     // holds the first raw word of the native double representation.
 698     // This is actually reasonable, since locals and stack arrays
 699     // grow downwards in all implementations.
 700     // (If, on some machine, the interpreter&#39;s Java locals or stack
 701     // were to grow upwards, the embedded doubles would be word-swapped.)
 702     jlong_accessor acc;
 703     acc.long_value = d;
 704     array-&gt;append(new ConstantIntValue(acc.words[1]));
 705     array-&gt;append(new ConstantIntValue(acc.words[0]));
 706 #endif
<span class="line-modified"> 707       break;</span>
<span class="line-modified"> 708     }</span>
<span class="line-modified"> 709     case Type::Top:               // Add an illegal value here</span>
<span class="line-modified"> 710       array-&gt;append(new LocationValue(Location()));</span>
<span class="line-modified"> 711       break;</span>
<span class="line-modified"> 712     default:</span>
<span class="line-modified"> 713       ShouldNotReachHere();</span>
<span class="line-modified"> 714       break;</span>
 715   }
 716 }
 717 
 718 // Determine if this node starts a bundle
 719 bool Compile::starts_bundle(const Node *n) const {
 720   return (_node_bundling_limit &gt; n-&gt;_idx &amp;&amp;
 721           _node_bundling_base[n-&gt;_idx].starts_bundle());
 722 }
 723 
 724 //--------------------------Process_OopMap_Node--------------------------------
 725 void Compile::Process_OopMap_Node(MachNode *mach, int current_offset) {
 726 
 727   // Handle special safepoint nodes for synchronization
 728   MachSafePointNode *sfn   = mach-&gt;as_MachSafePoint();
 729   MachCallNode      *mcall;
 730 
 731   int safepoint_pc_offset = current_offset;
 732   bool is_method_handle_invoke = false;
 733   bool return_oop = false;
 734 
</pre>
<hr />
<pre>
 859 
 860     // Make method available for all Safepoints
 861     ciMethod* scope_method = method ? method : _method;
 862     // Describe the scope here
 863     assert(jvms-&gt;bci() &gt;= InvocationEntryBci &amp;&amp; jvms-&gt;bci() &lt;= 0x10000, &quot;must be a valid or entry BCI&quot;);
 864     assert(!jvms-&gt;should_reexecute() || depth == max_depth, &quot;reexecute allowed only for the youngest&quot;);
 865     // Now we can describe the scope.
 866     methodHandle null_mh;
 867     bool rethrow_exception = false;
 868     debug_info()-&gt;describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms-&gt;bci(), jvms-&gt;should_reexecute(), rethrow_exception, is_method_handle_invoke, return_oop, locvals, expvals, monvals);
 869   } // End jvms loop
 870 
 871   // Mark the end of the scope set.
 872   debug_info()-&gt;end_safepoint(safepoint_pc_offset);
 873 }
 874 
 875 
 876 
 877 // A simplified version of Process_OopMap_Node, to handle non-safepoints.
 878 class NonSafepointEmitter {
<span class="line-modified"> 879     Compile*  C;</span>
<span class="line-modified"> 880     JVMState* _pending_jvms;</span>
<span class="line-modified"> 881     int       _pending_offset;</span>
 882 
<span class="line-modified"> 883     void emit_non_safepoint();</span>
 884 
 885  public:
<span class="line-modified"> 886     NonSafepointEmitter(Compile* compile) {</span>
<span class="line-modified"> 887       this-&gt;C = compile;</span>
<span class="line-modified"> 888       _pending_jvms = NULL;</span>
<span class="line-modified"> 889       _pending_offset = 0;</span>
<span class="line-modified"> 890     }</span>
 891 
<span class="line-modified"> 892     void observe_instruction(Node* n, int pc_offset) {</span>
<span class="line-modified"> 893       if (!C-&gt;debug_info()-&gt;recording_non_safepoints())  return;</span>
 894 
<span class="line-modified"> 895       Node_Notes* nn = C-&gt;node_notes_at(n-&gt;_idx);</span>
<span class="line-modified"> 896       if (nn == NULL || nn-&gt;jvms() == NULL)  return;</span>





 897       if (_pending_jvms != NULL &amp;&amp;
<span class="line-modified"> 898           _pending_jvms-&gt;same_calls_as(nn-&gt;jvms())) {</span>
<span class="line-modified"> 899         // Repeated JVMS?  Stretch it up here.</span>





 900         _pending_offset = pc_offset;
<span class="line-added"> 901       } else {</span>
<span class="line-added"> 902         if (_pending_jvms != NULL &amp;&amp;</span>
<span class="line-added"> 903             _pending_offset &lt; pc_offset) {</span>
<span class="line-added"> 904           emit_non_safepoint();</span>
<span class="line-added"> 905         }</span>
<span class="line-added"> 906         _pending_jvms = NULL;</span>
<span class="line-added"> 907         if (pc_offset &gt; C-&gt;debug_info()-&gt;last_pc_offset()) {</span>
<span class="line-added"> 908           // This is the only way _pending_jvms can become non-NULL:</span>
<span class="line-added"> 909           _pending_jvms = nn-&gt;jvms();</span>
<span class="line-added"> 910           _pending_offset = pc_offset;</span>
<span class="line-added"> 911         }</span>
 912       }
 913     }

 914 
<span class="line-modified"> 915     // Stay out of the way of real safepoints:</span>
<span class="line-modified"> 916     void observe_safepoint(JVMState* jvms, int pc_offset) {</span>
<span class="line-modified"> 917       if (_pending_jvms != NULL &amp;&amp;</span>
<span class="line-modified"> 918           !_pending_jvms-&gt;same_calls_as(jvms) &amp;&amp;</span>
<span class="line-modified"> 919           _pending_offset &lt; pc_offset) {</span>
<span class="line-modified"> 920         emit_non_safepoint();</span>
<span class="line-added"> 921       }</span>
<span class="line-added"> 922       _pending_jvms = NULL;</span>
 923     }


 924 
<span class="line-modified"> 925     void flush_at_end() {</span>
<span class="line-modified"> 926       if (_pending_jvms != NULL) {</span>
<span class="line-modified"> 927         emit_non_safepoint();</span>
<span class="line-added"> 928       }</span>
<span class="line-added"> 929       _pending_jvms = NULL;</span>
 930     }


 931 };
 932 
 933 void NonSafepointEmitter::emit_non_safepoint() {
 934   JVMState* youngest_jvms = _pending_jvms;
 935   int       pc_offset     = _pending_offset;
 936 
 937   // Clear it now:
 938   _pending_jvms = NULL;
 939 
 940   DebugInformationRecorder* debug_info = C-&gt;debug_info();
 941   assert(debug_info-&gt;recording_non_safepoints(), &quot;sanity&quot;);
 942 
 943   debug_info-&gt;add_non_safepoint(pc_offset);
 944   int max_depth = youngest_jvms-&gt;depth();
 945 
 946   // Visit scopes from oldest to youngest.
 947   for (int depth = 1; depth &lt;= max_depth; depth++) {
 948     JVMState* jvms = youngest_jvms-&gt;of_depth(depth);
 949     ciMethod* method = jvms-&gt;has_method() ? jvms-&gt;method() : NULL;
 950     assert(!jvms-&gt;should_reexecute() || depth==max_depth, &quot;reexecute allowed only for the youngest&quot;);
 951     methodHandle null_mh;
 952     debug_info-&gt;describe_scope(pc_offset, null_mh, method, jvms-&gt;bci(), jvms-&gt;should_reexecute());
 953   }
 954 
 955   // Mark the end of the scope set.
 956   debug_info-&gt;end_non_safepoint(pc_offset);
 957 }
 958 
 959 //------------------------------init_buffer------------------------------------
<span class="line-modified"> 960 void Compile::estimate_buffer_size(int&amp; const_req) {</span>
 961 
 962   // Set the initially allocated size
<span class="line-modified"> 963   const_req = initial_const_capacity;</span>



 964 

 965   // The extra spacing after the code is necessary on some platforms.
 966   // Sometimes we need to patch in a jump after the last instruction,
 967   // if the nmethod has been deoptimized.  (See 4932387, 4894843.)
 968 
 969   // Compute the byte offset where we can store the deopt pc.
 970   if (fixed_slots() != 0) {
 971     _orig_pc_slot_offset_in_bytes = _regalloc-&gt;reg2offset(OptoReg::stack2reg(_orig_pc_slot));
 972   }
 973 
 974   // Compute prolog code size
 975   _method_size = 0;
<span class="line-modified"> 976   _frame_slots = OptoReg::reg2stack(_matcher-&gt;_old_SP) + _regalloc-&gt;_framesize;</span>
 977 #if defined(IA64) &amp;&amp; !defined(AIX)
 978   if (save_argument_registers()) {
 979     // 4815101: this is a stub with implicit and unknown precision fp args.
 980     // The usual spill mechanism can only generate stfd&#39;s in this case, which
 981     // doesn&#39;t work if the fp reg to spill contains a single-precision denorm.
 982     // Instead, we hack around the normal spill mechanism using stfspill&#39;s and
 983     // ldffill&#39;s in the MachProlog and MachEpilog emit methods.  We allocate
 984     // space here for the fp arg regs (f8-f15) we&#39;re going to thusly spill.
 985     //
 986     // If we ever implement 16-byte &#39;registers&#39; == stack slots, we can
 987     // get rid of this hack and have SpillCopy generate stfspill/ldffill
 988     // instead of stfd/stfs/ldfd/ldfs.
 989     _frame_slots += 8*(16/BytesPerInt);
 990   }
 991 #endif
 992   assert(_frame_slots &gt;= 0 &amp;&amp; _frame_slots &lt; 1000000, &quot;sanity check&quot;);
 993 
 994   if (has_mach_constant_base_node()) {
 995     uint add_size = 0;
 996     // Fill the constant table.
</pre>
<hr />
<pre>
1005         // value section.
1006         if (n-&gt;is_MachConstant()) {
1007           MachConstantNode* machcon = n-&gt;as_MachConstant();
1008           machcon-&gt;eval_constant(C);
1009         } else if (n-&gt;is_Mach()) {
1010           // On Power there are more nodes that issue constants.
1011           add_size += (n-&gt;as_Mach()-&gt;ins_num_consts() * 8);
1012         }
1013       }
1014     }
1015 
1016     // Calculate the offsets of the constants and the size of the
1017     // constant table (including the padding to the next section).
1018     constant_table().calculate_offsets_and_size();
1019     const_req = constant_table().size() + add_size;
1020   }
1021 
1022   // Initialize the space for the BufferBlob used to find and verify
1023   // instruction size in MachNode::emit_size()
1024   init_scratch_buffer_blob(const_req);
<span class="line-modified">1025 }</span>
1026 
<span class="line-modified">1027 CodeBuffer* Compile::init_buffer(BufferSizingData&amp; buf_sizes) {</span>
<span class="line-modified">1028 </span>
<span class="line-modified">1029   int stub_req  = buf_sizes._stub;</span>
<span class="line-added">1030   int code_req  = buf_sizes._code;</span>
<span class="line-added">1031   int const_req = buf_sizes._const;</span>
<span class="line-added">1032 </span>
<span class="line-added">1033   int pad_req   = NativeCall::instruction_size;</span>
<span class="line-added">1034 </span>
<span class="line-added">1035   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();</span>
<span class="line-added">1036   stub_req += bs-&gt;estimate_stub_size();</span>
1037 
1038   // nmethod and CodeBuffer count stubs &amp; constants as part of method&#39;s code.
1039   // class HandlerImpl is platform-specific and defined in the *.ad files.
1040   int exception_handler_req = HandlerImpl::size_exception_handler() + MAX_stubs_size; // add marginal slop for handler
1041   int deopt_handler_req     = HandlerImpl::size_deopt_handler()     + MAX_stubs_size; // add marginal slop for handler
1042   stub_req += MAX_stubs_size;   // ensure per-stub margin
1043   code_req += MAX_inst_size;    // ensure per-instruction margin
1044 
1045   if (StressCodeBuffers)
1046     code_req = const_req = stub_req = exception_handler_req = deopt_handler_req = 0x10;  // force expansion
1047 
1048   int total_req =
<span class="line-modified">1049           const_req +</span>
<span class="line-modified">1050           code_req +</span>
<span class="line-modified">1051           pad_req +</span>
<span class="line-modified">1052           stub_req +</span>
<span class="line-modified">1053           exception_handler_req +</span>
<span class="line-modified">1054           deopt_handler_req;               // deopt handler</span>
1055 
1056   if (has_method_handle_invokes())
1057     total_req += deopt_handler_req;  // deopt MH handler
1058 
1059   CodeBuffer* cb = code_buffer();
<span class="line-modified">1060   cb-&gt;initialize(total_req, buf_sizes._reloc);</span>
1061 
1062   // Have we run out of code space?
1063   if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1064     C-&gt;record_failure(&quot;CodeCache is full&quot;);
1065     return NULL;
1066   }
1067   // Configure the code buffer.
1068   cb-&gt;initialize_consts_size(const_req);
1069   cb-&gt;initialize_stubs_size(stub_req);
1070   cb-&gt;initialize_oop_recorder(env()-&gt;oop_recorder());
1071 
1072   // fill in the nop array for bundling computations
1073   MachNode *_nop_list[Bundle::_nop_count];
1074   Bundle::initialize_nops(_nop_list);
1075 
1076   return cb;
1077 }
1078 
1079 //------------------------------fill_buffer------------------------------------
1080 void Compile::fill_buffer(CodeBuffer* cb, uint* blk_starts) {
</pre>
<hr />
<pre>
1097   uint *inct_starts = NEW_RESOURCE_ARRAY(uint, nblocks+1);
1098 
1099   // Count and start of calls
1100   uint *call_returns = NEW_RESOURCE_ARRAY(uint, nblocks+1);
1101 
1102   uint  return_offset = 0;
1103   int nop_size = (new MachNopNode())-&gt;size(_regalloc);
1104 
1105   int previous_offset = 0;
1106   int current_offset  = 0;
1107   int last_call_offset = -1;
1108   int last_avoid_back_to_back_offset = -1;
1109 #ifdef ASSERT
1110   uint* jmp_target = NEW_RESOURCE_ARRAY(uint,nblocks);
1111   uint* jmp_offset = NEW_RESOURCE_ARRAY(uint,nblocks);
1112   uint* jmp_size   = NEW_RESOURCE_ARRAY(uint,nblocks);
1113   uint* jmp_rule   = NEW_RESOURCE_ARRAY(uint,nblocks);
1114 #endif
1115 
1116   // Create an array of unused labels, one for each basic block, if printing is enabled
<span class="line-modified">1117 #if defined(SUPPORT_OPTO_ASSEMBLY)</span>
1118   int *node_offsets      = NULL;
1119   uint node_offset_limit = unique();
1120 
<span class="line-modified">1121   if (print_assembly()) {</span>
<span class="line-modified">1122     node_offsets = NEW_RESOURCE_ARRAY(int, node_offset_limit);</span>
<span class="line-added">1123   }</span>
<span class="line-added">1124   if (node_offsets != NULL) {</span>
<span class="line-added">1125     // We need to initialize. Unused array elements may contain garbage and mess up PrintOptoAssembly.</span>
<span class="line-added">1126     memset(node_offsets, 0, node_offset_limit*sizeof(int));</span>
<span class="line-added">1127   }</span>
1128 #endif
1129 
1130   NonSafepointEmitter non_safepoints(this);  // emit non-safepoints lazily
1131 
1132   // Emit the constant table.
1133   if (has_mach_constant_base_node()) {
1134     constant_table().emit(*cb);
1135   }
1136 
1137   // Create an array of labels, one for each basic block
1138   Label *blk_labels = NEW_RESOURCE_ARRAY(Label, nblocks+1);
1139   for (uint i=0; i &lt;= nblocks; i++) {
1140     blk_labels[i].init();
1141   }
1142 
1143   // ------------------
1144   // Now fill in the code buffer
1145   Node *delay_slot = NULL;
1146 
1147   for (uint i = 0; i &lt; nblocks; i++) {
</pre>
<hr />
<pre>
1259         }
1260 
1261         // sfn will be valid whenever mcall is valid now because of inheritance
1262         if (is_sfn || is_mcall) {
1263 
1264           // Handle special safepoint nodes for synchronization
1265           if (!is_mcall) {
1266             MachSafePointNode *sfn = mach-&gt;as_MachSafePoint();
1267             // !!!!! Stubs only need an oopmap right now, so bail out
1268             if (sfn-&gt;jvms()-&gt;method() == NULL) {
1269               // Write the oopmap directly to the code blob??!!
1270               continue;
1271             }
1272           } // End synchronization
1273 
1274           non_safepoints.observe_safepoint(mach-&gt;as_MachSafePoint()-&gt;jvms(),
1275                                            current_offset);
1276           Process_OopMap_Node(mach, current_offset);
1277         } // End if safepoint
1278 
<span class="line-modified">1279           // If this is a null check, then add the start of the previous instruction to the list</span>
1280         else if( mach-&gt;is_MachNullCheck() ) {
1281           inct_starts[inct_cnt++] = previous_offset;
1282         }
1283 
<span class="line-modified">1284           // If this is a branch, then fill in the label with the target BB&#39;s label</span>
1285         else if (mach-&gt;is_MachBranch()) {
1286           // This requires the TRUE branch target be in succs[0]
1287           uint block_num = block-&gt;non_connector_successor(0)-&gt;_pre_order;
1288 
1289           // Try to replace long branch if delay slot is not used,
1290           // it is mostly for back branches since forward branch&#39;s
1291           // distance is not updated yet.
1292           bool delay_slot_is_used = valid_bundle_info(n) &amp;&amp;
1293                                     node_bundling(n)-&gt;use_unconditional_delay();
1294           if (!delay_slot_is_used &amp;&amp; mach-&gt;may_be_short_branch()) {
<span class="line-modified">1295             assert(delay_slot == NULL, &quot;not expecting delay slot node&quot;);</span>
<span class="line-modified">1296             int br_size = n-&gt;size(_regalloc);</span>
1297             int offset = blk_starts[block_num] - current_offset;
1298             if (block_num &gt;= i) {
1299               // Current and following block&#39;s offset are not
1300               // finalized yet, adjust distance by the difference
1301               // between calculated and final offsets of current block.
1302               offset -= (blk_starts[i] - blk_offset);
1303             }
1304             // In the following code a nop could be inserted before
1305             // the branch which will increase the backward distance.
1306             bool needs_padding = (current_offset == last_avoid_back_to_back_offset);
1307             if (needs_padding &amp;&amp; offset &lt;= 0)
1308               offset -= nop_size;
1309 
1310             if (_matcher-&gt;is_short_branch_offset(mach-&gt;rule(), br_size, offset)) {
1311               // We&#39;ve got a winner.  Replace this branch.
1312               MachNode* replacement = mach-&gt;as_MachBranch()-&gt;short_branch_version();
1313 
1314               // Update the jmp_size.
1315               int new_size = replacement-&gt;size(_regalloc);
1316               assert((br_size - new_size) &gt;= (int)nop_size, &quot;short_branch size should be smaller&quot;);
</pre>
<hr />
<pre>
1334               mach-&gt;subsume_by(replacement, C);
1335               n    = replacement;
1336               mach = replacement;
1337             }
1338           }
1339           mach-&gt;as_MachBranch()-&gt;label_set( &amp;blk_labels[block_num], block_num );
1340         } else if (mach-&gt;ideal_Opcode() == Op_Jump) {
1341           for (uint h = 0; h &lt; block-&gt;_num_succs; h++) {
1342             Block* succs_block = block-&gt;_succs[h];
1343             for (uint j = 1; j &lt; succs_block-&gt;num_preds(); j++) {
1344               Node* jpn = succs_block-&gt;pred(j);
1345               if (jpn-&gt;is_JumpProj() &amp;&amp; jpn-&gt;in(0) == mach) {
1346                 uint block_num = succs_block-&gt;non_connector()-&gt;_pre_order;
1347                 Label *blkLabel = &amp;blk_labels[block_num];
1348                 mach-&gt;add_case_label(jpn-&gt;as_JumpProj()-&gt;proj_no(), blkLabel);
1349               }
1350             }
1351           }
1352         }
1353 #ifdef ASSERT
<span class="line-modified">1354           // Check that oop-store precedes the card-mark</span>
1355         else if (mach-&gt;ideal_Opcode() == Op_StoreCM) {
1356           uint storeCM_idx = j;
1357           int count = 0;
1358           for (uint prec = mach-&gt;req(); prec &lt; mach-&gt;len(); prec++) {
1359             Node *oop_store = mach-&gt;in(prec);  // Precedence edge
1360             if (oop_store == NULL) continue;
1361             count++;
1362             uint i4;
1363             for (i4 = 0; i4 &lt; last_inst; ++i4) {
1364               if (block-&gt;get_node(i4) == oop_store) {
1365                 break;
1366               }
1367             }
1368             // Note: This test can provide a false failure if other precedence
1369             // edges have been added to the storeCMNode.
1370             assert(i4 == last_inst || i4 &lt; storeCM_idx, &quot;CM card-mark executes before oop-store&quot;);
1371           }
1372           assert(count &gt; 0, &quot;storeCM expects at least one precedence edge&quot;);
1373         }
1374 #endif
</pre>
<hr />
<pre>
1377           // it&#39;s followed by a flag-kill and a null-check.  Happens on
1378           // Intel all the time, with add-to-memory kind of opcodes.
1379           previous_offset = current_offset;
1380         }
1381 
1382         // Not an else-if!
1383         // If this is a trap based cmp then add its offset to the list.
1384         if (mach-&gt;is_TrapBasedCheckNode()) {
1385           inct_starts[inct_cnt++] = current_offset;
1386         }
1387       }
1388 
1389       // Verify that there is sufficient space remaining
1390       cb-&gt;insts()-&gt;maybe_expand_to_ensure_remaining(MAX_inst_size);
1391       if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1392         C-&gt;record_failure(&quot;CodeCache is full&quot;);
1393         return;
1394       }
1395 
1396       // Save the offset for the listing
<span class="line-modified">1397 #if defined(SUPPORT_OPTO_ASSEMBLY)</span>
<span class="line-modified">1398       if ((node_offsets != NULL) &amp;&amp; (n-&gt;_idx &lt; node_offset_limit)) {</span>
1399         node_offsets[n-&gt;_idx] = cb-&gt;insts_size();
<span class="line-added">1400       }</span>
1401 #endif
1402 
1403       // &quot;Normal&quot; instruction case
1404       DEBUG_ONLY( uint instr_offset = cb-&gt;insts_size(); )
1405       n-&gt;emit(*cb, _regalloc);
1406       current_offset  = cb-&gt;insts_size();
1407 
1408       // Above we only verified that there is enough space in the instruction section.
1409       // However, the instruction may emit stubs that cause code buffer expansion.
1410       // Bail out here if expansion failed due to a lack of code cache space.
1411       if (failing()) {
1412         return;
1413       }
1414 
1415 #ifdef ASSERT
1416       if (n-&gt;size(_regalloc) &lt; (current_offset-instr_offset)) {
1417         n-&gt;dump();
1418         assert(false, &quot;wrong size of mach node&quot;);
1419       }
1420 #endif
</pre>
<hr />
<pre>
1427       // in the case that return address is not actually at current_offset.
1428       // This is a small price to pay.
1429 
1430       if (is_mcall) {
1431         last_call_offset = current_offset;
1432       }
1433 
1434       if (n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;avoid_back_to_back(MachNode::AVOID_AFTER)) {
1435         // Avoid back to back some instructions.
1436         last_avoid_back_to_back_offset = current_offset;
1437       }
1438 
1439       // See if this instruction has a delay slot
1440       if (valid_bundle_info(n) &amp;&amp; node_bundling(n)-&gt;use_unconditional_delay()) {
1441         guarantee(delay_slot != NULL, &quot;expecting delay slot node&quot;);
1442 
1443         // Back up 1 instruction
1444         cb-&gt;set_insts_end(cb-&gt;insts_end() - Pipeline::instr_unit_size());
1445 
1446         // Save the offset for the listing
<span class="line-modified">1447 #if defined(SUPPORT_OPTO_ASSEMBLY)</span>
<span class="line-modified">1448         if ((node_offsets != NULL) &amp;&amp; (delay_slot-&gt;_idx &lt; node_offset_limit)) {</span>
1449           node_offsets[delay_slot-&gt;_idx] = cb-&gt;insts_size();
<span class="line-added">1450         }</span>
1451 #endif
1452 
1453         // Support a SafePoint in the delay slot
1454         if (delay_slot-&gt;is_MachSafePoint()) {
1455           MachNode *mach = delay_slot-&gt;as_Mach();
1456           // !!!!! Stubs only need an oopmap right now, so bail out
1457           if (!mach-&gt;is_MachCall() &amp;&amp; mach-&gt;as_MachSafePoint()-&gt;jvms()-&gt;method() == NULL) {
1458             // Write the oopmap directly to the code blob??!!
1459             delay_slot = NULL;
1460             continue;
1461           }
1462 
1463           int adjusted_offset = current_offset - Pipeline::instr_unit_size();
1464           non_safepoints.observe_safepoint(mach-&gt;as_MachSafePoint()-&gt;jvms(),
1465                                            adjusted_offset);
1466           // Generate an OopMap entry
1467           Process_OopMap_Node(mach, adjusted_offset);
1468         }
1469 
1470         // Insert the delay slot instruction
</pre>
<hr />
<pre>
1505 
1506   // Define a pseudo-label at the end of the code
1507   MacroAssembler(cb).bind( blk_labels[nblocks] );
1508 
1509   // Compute the size of the first block
1510   _first_block_size = blk_labels[1].loc_pos() - blk_labels[0].loc_pos();
1511 
1512 #ifdef ASSERT
1513   for (uint i = 0; i &lt; nblocks; i++) { // For all blocks
1514     if (jmp_target[i] != 0) {
1515       int br_size = jmp_size[i];
1516       int offset = blk_starts[jmp_target[i]]-(blk_starts[i] + jmp_offset[i]);
1517       if (!_matcher-&gt;is_short_branch_offset(jmp_rule[i], br_size, offset)) {
1518         tty-&gt;print_cr(&quot;target (%d) - jmp_offset(%d) = offset (%d), jump_size(%d), jmp_block B%d, target_block B%d&quot;, blk_starts[jmp_target[i]], blk_starts[i] + jmp_offset[i], offset, br_size, i, jmp_target[i]);
1519         assert(false, &quot;Displacement too large for short jmp&quot;);
1520       }
1521     }
1522   }
1523 #endif
1524 
<span class="line-added">1525   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();</span>
<span class="line-added">1526   bs-&gt;emit_stubs(*cb);</span>
<span class="line-added">1527   if (failing())  return;</span>
<span class="line-added">1528 </span>
1529 #ifndef PRODUCT
1530   // Information on the size of the method, without the extraneous code
1531   Scheduling::increment_method_size(cb-&gt;insts_size());
1532 #endif
1533 
1534   // ------------------
1535   // Fill in exception table entries.
1536   FillExceptionTables(inct_cnt, call_returns, inct_starts, blk_labels);
1537 
1538   // Only java methods have exception handlers and deopt handlers
1539   // class HandlerImpl is platform-specific and defined in the *.ad files.
1540   if (_method) {
1541     // Emit the exception handler code.
1542     _code_offsets.set_value(CodeOffsets::Exceptions, HandlerImpl::emit_exception_handler(*cb));
1543     if (failing()) {
1544       return; // CodeBuffer::expand failed
1545     }
1546     // Emit the deopt handler code.
1547     _code_offsets.set_value(CodeOffsets::Deopt, HandlerImpl::emit_deopt_handler(*cb));
1548 
1549     // Emit the MethodHandle deopt handler code (if required).
1550     if (has_method_handle_invokes() &amp;&amp; !failing()) {
1551       // We can use the same code as for the normal deopt handler, we
1552       // just need a different entry point address.
1553       _code_offsets.set_value(CodeOffsets::DeoptMH, HandlerImpl::emit_deopt_handler(*cb));
1554     }
1555   }
1556 
1557   // One last check for failed CodeBuffer::expand:
1558   if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1559     C-&gt;record_failure(&quot;CodeCache is full&quot;);
1560     return;
1561   }
1562 
<span class="line-modified">1563 #if defined(SUPPORT_ABSTRACT_ASSEMBLY) || defined(SUPPORT_ASSEMBLY) || defined(SUPPORT_OPTO_ASSEMBLY)</span>
<span class="line-added">1564   if (print_assembly()) {</span>
<span class="line-added">1565     tty-&gt;cr();</span>
<span class="line-added">1566     tty-&gt;print_cr(&quot;============================= C2-compiled nmethod ==============================&quot;);</span>
<span class="line-added">1567   }</span>
<span class="line-added">1568 #endif</span>
<span class="line-added">1569 </span>
<span class="line-added">1570 #if defined(SUPPORT_OPTO_ASSEMBLY)</span>
1571   // Dump the assembly code, including basic-block numbers
1572   if (print_assembly()) {
1573     ttyLocker ttyl;  // keep the following output all in one block
1574     if (!VMThread::should_terminate()) {  // test this under the tty lock
1575       // This output goes directly to the tty, not the compiler log.
1576       // To enable tools to match it up with the compilation activity,
1577       // be sure to tag this tty output with the compile ID.
1578       if (xtty != NULL) {
1579         xtty-&gt;head(&quot;opto_assembly compile_id=&#39;%d&#39;%s&quot;, compile_id(),
1580                    is_osr_compilation()    ? &quot; compile_kind=&#39;osr&#39;&quot; :
1581                    &quot;&quot;);
1582       }
1583       if (method() != NULL) {
<span class="line-added">1584         tty-&gt;print_cr(&quot;----------------------- MetaData before Compile_id = %d ------------------------&quot;, compile_id());</span>
1585         method()-&gt;print_metadata();
1586       } else if (stub_name() != NULL) {
<span class="line-modified">1587         tty-&gt;print_cr(&quot;----------------------------- RuntimeStub %s -------------------------------&quot;, stub_name());</span>
1588       }
<span class="line-added">1589       tty-&gt;cr();</span>
<span class="line-added">1590       tty-&gt;print_cr(&quot;------------------------ OptoAssembly for Compile_id = %d -----------------------&quot;, compile_id());</span>
1591       dump_asm(node_offsets, node_offset_limit);
<span class="line-added">1592       tty-&gt;print_cr(&quot;--------------------------------------------------------------------------------&quot;);</span>
1593       if (xtty != NULL) {
1594         // print_metadata and dump_asm above may safepoint which makes us loose the ttylock.
1595         // Retake lock too make sure the end tag is coherent, and that xmlStream-&gt;pop_tag is done
1596         // thread safe
1597         ttyLocker ttyl2;
1598         xtty-&gt;tail(&quot;opto_assembly&quot;);
1599       }
1600     }
1601   }
1602 #endif

1603 }
1604 
1605 void Compile::FillExceptionTables(uint cnt, uint *call_returns, uint *inct_starts, Label *blk_labels) {
1606   _inc_table.set_size(cnt);
1607 
1608   uint inct_cnt = 0;
1609   for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {
1610     Block* block = _cfg-&gt;get_block(i);
1611     Node *n = NULL;
1612     int j;
1613 
1614     // Find the branch; ignore trailing NOPs.
1615     for (j = block-&gt;number_of_nodes() - 1; j &gt;= 0; j--) {
1616       n = block-&gt;get_node(j);
1617       if (!n-&gt;is_Mach() || n-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Con) {
1618         break;
1619       }
1620     }
1621 
1622     // If we didn&#39;t find anything, continue
</pre>
<hr />
<pre>
1683     if (n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;is_TrapBasedCheckNode()) {
1684       uint block_num = block-&gt;non_connector_successor(0)-&gt;_pre_order;
1685       _inc_table.append(inct_starts[inct_cnt++], blk_labels[block_num].loc_pos());
1686       continue;
1687     }
1688   } // End of for all blocks fill in exception table entries
1689 }
1690 
1691 // Static Variables
1692 #ifndef PRODUCT
1693 uint Scheduling::_total_nop_size = 0;
1694 uint Scheduling::_total_method_size = 0;
1695 uint Scheduling::_total_branches = 0;
1696 uint Scheduling::_total_unconditional_delays = 0;
1697 uint Scheduling::_total_instructions_per_bundle[Pipeline::_max_instrs_per_cycle+1];
1698 #endif
1699 
1700 // Initializer for class Scheduling
1701 
1702 Scheduling::Scheduling(Arena *arena, Compile &amp;compile)
<span class="line-modified">1703         : _arena(arena),</span>
<span class="line-modified">1704           _cfg(compile.cfg()),</span>
<span class="line-modified">1705           _regalloc(compile.regalloc()),</span>
<span class="line-modified">1706           _scheduled(arena),</span>
<span class="line-modified">1707           _available(arena),</span>
<span class="line-modified">1708           _reg_node(arena),</span>
<span class="line-modified">1709           _pinch_free_list(arena),</span>
<span class="line-modified">1710           _next_node(NULL),</span>
<span class="line-modified">1711           _bundle_instr_count(0),</span>
<span class="line-modified">1712           _bundle_cycle_number(0),</span>
<span class="line-modified">1713           _bundle_use(0, 0, resource_count, &amp;_bundle_use_elements[0])</span>
1714 #ifndef PRODUCT
<span class="line-modified">1715         , _branches(0)</span>
<span class="line-modified">1716         , _unconditional_delays(0)</span>
1717 #endif
1718 {
1719   // Create a MachNopNode
1720   _nop = new MachNopNode();
1721 
1722   // Now that the nops are in the array, save the count
1723   // (but allow entries for the nops)
1724   _node_bundling_limit = compile.unique();
1725   uint node_max = _regalloc-&gt;node_regs_max_index();
1726 
1727   compile.set_node_bundling_limit(_node_bundling_limit);
1728 
1729   // This one is persistent within the Compile class
1730   _node_bundling_base = NEW_ARENA_ARRAY(compile.comp_arena(), Bundle, node_max);
1731 
1732   // Allocate space for fixed-size arrays
1733   _node_latency    = NEW_ARENA_ARRAY(arena, unsigned short, node_max);
1734   _uses            = NEW_ARENA_ARRAY(arena, short,          node_max);
1735   _current_latency = NEW_ARENA_ARRAY(arena, unsigned short, node_max);
1736 
</pre>
<hr />
<pre>
1777   _bundle_use.step(i);
1778 }
1779 
1780 void Scheduling::step_and_clear() {
1781   Bundle *bundle = node_bundling(_next_node);
1782   bundle-&gt;set_starts_bundle();
1783 
1784   // Update the bundle record
1785   if (_bundle_instr_count &gt; 0) {
1786     bundle-&gt;set_instr_count(_bundle_instr_count);
1787     bundle-&gt;set_resources_used(_bundle_use.resourcesUsed());
1788 
1789     _bundle_cycle_number += 1;
1790   }
1791 
1792   // Clear the bundling information
1793   _bundle_instr_count = 0;
1794   _bundle_use.reset();
1795 
1796   memcpy(_bundle_use_elements,
<span class="line-modified">1797          Pipeline_Use::elaborated_elements,</span>
<span class="line-modified">1798          sizeof(Pipeline_Use::elaborated_elements));</span>
1799 }
1800 
1801 // Perform instruction scheduling and bundling over the sequence of
1802 // instructions in backwards order.
1803 void Compile::ScheduleAndBundle() {
1804 
1805   // Don&#39;t optimize this if it isn&#39;t a method
1806   if (!_method)
1807     return;
1808 
1809   // Don&#39;t optimize this if scheduling is disabled
1810   if (!do_scheduling())
1811     return;
1812 
1813   // Scheduling code works only with pairs (16 bytes) maximum.
1814   if (max_vector_size() &gt; 16)
1815     return;
1816 
1817   TracePhase tp(&quot;isched&quot;, &amp;timers[_t_instrSched]);
1818 
1819   // Create a data structure for all the scheduling information
1820   Scheduling scheduling(Thread::current()-&gt;resource_area(), *this);
1821 
1822   // Walk backwards over each basic block, computing the needed alignment
1823   // Walk over all the basic blocks
1824   scheduling.DoScheduling();
<span class="line-added">1825 </span>
<span class="line-added">1826 #ifndef PRODUCT</span>
<span class="line-added">1827   if (trace_opto_output()) {</span>
<span class="line-added">1828     tty-&gt;print(&quot;\n---- After ScheduleAndBundle ----\n&quot;);</span>
<span class="line-added">1829     for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {</span>
<span class="line-added">1830       tty-&gt;print(&quot;\nBB#%03d:\n&quot;, i);</span>
<span class="line-added">1831       Block* block = _cfg-&gt;get_block(i);</span>
<span class="line-added">1832       for (uint j = 0; j &lt; block-&gt;number_of_nodes(); j++) {</span>
<span class="line-added">1833         Node* n = block-&gt;get_node(j);</span>
<span class="line-added">1834         OptoReg::Name reg = _regalloc-&gt;get_reg_first(n);</span>
<span class="line-added">1835         tty-&gt;print(&quot; %-6s &quot;, reg &gt;= 0 &amp;&amp; reg &lt; REG_COUNT ? Matcher::regName[reg] : &quot;&quot;);</span>
<span class="line-added">1836         n-&gt;dump();</span>
<span class="line-added">1837       }</span>
<span class="line-added">1838     }</span>
<span class="line-added">1839   }</span>
<span class="line-added">1840 #endif</span>
1841 }
1842 
1843 // Compute the latency of all the instructions.  This is fairly simple,
1844 // because we already have a legal ordering.  Walk over the instructions
1845 // from first to last, and compute the latency of the instruction based
1846 // on the latency of the preceding instruction(s).
1847 void Scheduling::ComputeLocalLatenciesForward(const Block *bb) {
1848 #ifndef PRODUCT
1849   if (_cfg-&gt;C-&gt;trace_opto_output())
1850     tty-&gt;print(&quot;# -&gt; ComputeLocalLatenciesForward\n&quot;);
1851 #endif
1852 
1853   // Walk over all the schedulable instructions
1854   for( uint j=_bb_start; j &lt; _bb_end; j++ ) {
1855 
1856     // This is a kludge, forcing all latency calculations to start at 1.
1857     // Used to allow latency 0 to force an instruction to the beginning
1858     // of the bb
1859     uint latency = 1;
1860     Node *use = bb-&gt;get_node(j);
</pre>
<hr />
<pre>
1889 } // end ComputeLocalLatenciesForward
1890 
1891 // See if this node fits into the present instruction bundle
1892 bool Scheduling::NodeFitsInBundle(Node *n) {
1893   uint n_idx = n-&gt;_idx;
1894 
1895   // If this is the unconditional delay instruction, then it fits
1896   if (n == _unconditional_delay_slot) {
1897 #ifndef PRODUCT
1898     if (_cfg-&gt;C-&gt;trace_opto_output())
1899       tty-&gt;print(&quot;#     NodeFitsInBundle [%4d]: TRUE; is in unconditional delay slot\n&quot;, n-&gt;_idx);
1900 #endif
1901     return (true);
1902   }
1903 
1904   // If the node cannot be scheduled this cycle, skip it
1905   if (_current_latency[n_idx] &gt; _bundle_cycle_number) {
1906 #ifndef PRODUCT
1907     if (_cfg-&gt;C-&gt;trace_opto_output())
1908       tty-&gt;print(&quot;#     NodeFitsInBundle [%4d]: FALSE; latency %4d &gt; %d\n&quot;,
<span class="line-modified">1909                  n-&gt;_idx, _current_latency[n_idx], _bundle_cycle_number);</span>
1910 #endif
1911     return (false);
1912   }
1913 
1914   const Pipeline *node_pipeline = n-&gt;pipeline();
1915 
1916   uint instruction_count = node_pipeline-&gt;instructionCount();
1917   if (node_pipeline-&gt;mayHaveNoCode() &amp;&amp; n-&gt;size(_regalloc) == 0)
1918     instruction_count = 0;
1919   else if (node_pipeline-&gt;hasBranchDelay() &amp;&amp; !_unconditional_delay_slot)
1920     instruction_count++;
1921 
1922   if (_bundle_instr_count + instruction_count &gt; Pipeline::_max_instrs_per_cycle) {
1923 #ifndef PRODUCT
1924     if (_cfg-&gt;C-&gt;trace_opto_output())
1925       tty-&gt;print(&quot;#     NodeFitsInBundle [%4d]: FALSE; too many instructions: %d &gt; %d\n&quot;,
<span class="line-modified">1926                  n-&gt;_idx, _bundle_instr_count + instruction_count, Pipeline::_max_instrs_per_cycle);</span>
1927 #endif
1928     return (false);
1929   }
1930 
1931   // Don&#39;t allow non-machine nodes to be handled this way
1932   if (!n-&gt;is_Mach() &amp;&amp; instruction_count == 0)
1933     return (false);
1934 
1935   // See if there is any overlap
1936   uint delay = _bundle_use.full_latency(0, node_pipeline-&gt;resourceUse());
1937 
1938   if (delay &gt; 0) {
1939 #ifndef PRODUCT
1940     if (_cfg-&gt;C-&gt;trace_opto_output())
1941       tty-&gt;print(&quot;#     NodeFitsInBundle [%4d]: FALSE; functional units overlap\n&quot;, n_idx);
1942 #endif
1943     return false;
1944   }
1945 
1946 #ifndef PRODUCT
</pre>
<hr />
<pre>
2114     // copied to the delay slot, and the branch goes to
2115     // the instruction after that at the branch target
2116     if ( n-&gt;is_MachBranch() ) {
2117 
2118       assert( !n-&gt;is_MachNullCheck(), &quot;should not look for delay slot for Null Check&quot; );
2119       assert( !n-&gt;is_Catch(),         &quot;should not look for delay slot for Catch&quot; );
2120 
2121 #ifndef PRODUCT
2122       _branches++;
2123 #endif
2124 
2125       // At least 1 instruction is on the available list
2126       // that is not dependent on the branch
2127       for (uint i = 0; i &lt; siz; i++) {
2128         Node *d = _available[i];
2129         const Pipeline *avail_pipeline = d-&gt;pipeline();
2130 
2131         // Don&#39;t allow safepoints in the branch shadow, that will
2132         // cause a number of difficulties
2133         if ( avail_pipeline-&gt;instructionCount() == 1 &amp;&amp;
<span class="line-modified">2134              !avail_pipeline-&gt;hasMultipleBundles() &amp;&amp;</span>
<span class="line-modified">2135              !avail_pipeline-&gt;hasBranchDelay() &amp;&amp;</span>
<span class="line-modified">2136              Pipeline::instr_has_unit_size() &amp;&amp;</span>
<span class="line-modified">2137              d-&gt;size(_regalloc) == Pipeline::instr_unit_size() &amp;&amp;</span>
<span class="line-modified">2138              NodeFitsInBundle(d) &amp;&amp;</span>
<span class="line-modified">2139              !node_bundling(d)-&gt;used_in_delay()) {</span>
2140 
2141           if (d-&gt;is_Mach() &amp;&amp; !d-&gt;is_MachSafePoint()) {
2142             // A node that fits in the delay slot was found, so we need to
2143             // set the appropriate bits in the bundle pipeline information so
2144             // that it correctly indicates resource usage.  Later, when we
2145             // attempt to add this instruction to the bundle, we will skip
2146             // setting the resource usage.
2147             _unconditional_delay_slot = d;
2148             node_bundling(n)-&gt;set_use_unconditional_delay();
2149             node_bundling(d)-&gt;set_used_in_unconditional_delay();
2150             _bundle_use.add_usage(avail_pipeline-&gt;resourceUse());
2151             _current_latency[d-&gt;_idx] = _bundle_cycle_number;
2152             _next_node = d;
2153             ++_bundle_instr_count;
2154 #ifndef PRODUCT
2155             _unconditional_delays++;
2156 #endif
2157             break;
2158           }
2159         }
</pre>
<hr />
<pre>
2164     if (!_unconditional_delay_slot) {
2165       // See if adding an instruction in the delay slot will overflow
2166       // the bundle.
2167       if (!NodeFitsInBundle(_nop)) {
2168 #ifndef PRODUCT
2169         if (_cfg-&gt;C-&gt;trace_opto_output())
2170           tty-&gt;print(&quot;#  *** STEP(1 instruction for delay slot) ***\n&quot;);
2171 #endif
2172         step(1);
2173       }
2174 
2175       _bundle_use.add_usage(_nop-&gt;pipeline()-&gt;resourceUse());
2176       _next_node = _nop;
2177       ++_bundle_instr_count;
2178     }
2179 
2180     // See if the instruction in the delay slot requires a
2181     // step of the bundles
2182     if (!NodeFitsInBundle(n)) {
2183 #ifndef PRODUCT
<span class="line-modified">2184       if (_cfg-&gt;C-&gt;trace_opto_output())</span>
<span class="line-modified">2185         tty-&gt;print(&quot;#  *** STEP(branch won&#39;t fit) ***\n&quot;);</span>
2186 #endif
<span class="line-modified">2187       // Update the state information</span>
<span class="line-modified">2188       _bundle_instr_count = 0;</span>
<span class="line-modified">2189       _bundle_cycle_number += 1;</span>
<span class="line-modified">2190       _bundle_use.step(1);</span>
2191     }
2192   }
2193 
2194   // Get the number of instructions
2195   uint instruction_count = node_pipeline-&gt;instructionCount();
2196   if (node_pipeline-&gt;mayHaveNoCode() &amp;&amp; n-&gt;size(_regalloc) == 0)
2197     instruction_count = 0;
2198 
2199   // Compute the latency information
2200   uint delay = 0;
2201 
2202   if (instruction_count &gt; 0 || !node_pipeline-&gt;mayHaveNoCode()) {
2203     int relative_latency = _current_latency[n-&gt;_idx] - _bundle_cycle_number;
2204     if (relative_latency &lt; 0)
2205       relative_latency = 0;
2206 
2207     delay = _bundle_use.full_latency(relative_latency, node_usage);
2208 
2209     // Does not fit in this bundle, start a new one
2210     if (delay &gt; 0) {
</pre>
<hr />
<pre>
2216 #endif
2217     }
2218   }
2219 
2220   // If this was placed in the delay slot, ignore it
2221   if (n != _unconditional_delay_slot) {
2222 
2223     if (delay == 0) {
2224       if (node_pipeline-&gt;hasMultipleBundles()) {
2225 #ifndef PRODUCT
2226         if (_cfg-&gt;C-&gt;trace_opto_output())
2227           tty-&gt;print(&quot;#  *** STEP(multiple instructions) ***\n&quot;);
2228 #endif
2229         step(1);
2230       }
2231 
2232       else if (instruction_count + _bundle_instr_count &gt; Pipeline::_max_instrs_per_cycle) {
2233 #ifndef PRODUCT
2234         if (_cfg-&gt;C-&gt;trace_opto_output())
2235           tty-&gt;print(&quot;#  *** STEP(%d &gt;= %d instructions) ***\n&quot;,
<span class="line-modified">2236                      instruction_count + _bundle_instr_count,</span>
<span class="line-modified">2237                      Pipeline::_max_instrs_per_cycle);</span>
2238 #endif
2239         step(1);
2240       }
2241     }
2242 
2243     if (node_pipeline-&gt;hasBranchDelay() &amp;&amp; !_unconditional_delay_slot)
2244       _bundle_instr_count++;
2245 
2246     // Set the node&#39;s latency
2247     _current_latency[n-&gt;_idx] = _bundle_cycle_number;
2248 
2249     // Now merge the functional unit information
2250     if (instruction_count &gt; 0 || !node_pipeline-&gt;mayHaveNoCode())
2251       _bundle_use.add_usage(node_usage);
2252 
2253     // Increment the number of instructions in this bundle
2254     _bundle_instr_count += instruction_count;
2255 
2256     // Remember this node for later
2257     if (n-&gt;is_Mach())
</pre>
<hr />
<pre>
2423       if( iop == Op_Node &amp;&amp;     // Do not schedule PhiNodes, ProjNodes
2424           mach-&gt;pipeline() == MachNode::pipeline_class() &amp;&amp;
2425           !n-&gt;is_SpillCopy() &amp;&amp; !n-&gt;is_MachMerge() )  // Breakpoints, Prolog, etc
2426         continue;
2427       break;                    // Funny loop structure to be sure...
2428     }
2429     // Compute last &quot;interesting&quot; instruction in block - last instruction we
2430     // might schedule.  _bb_end points just after last schedulable inst.  We
2431     // normally schedule conditional branches (despite them being forced last
2432     // in the block), because they have delay slots we can fill.  Calls all
2433     // have their delay slots filled in the template expansions, so we don&#39;t
2434     // bother scheduling them.
2435     Node *last = bb-&gt;get_node(_bb_end);
2436     // Ignore trailing NOPs.
2437     while (_bb_end &gt; 0 &amp;&amp; last-&gt;is_Mach() &amp;&amp;
2438            last-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Con) {
2439       last = bb-&gt;get_node(--_bb_end);
2440     }
2441     assert(!last-&gt;is_Mach() || last-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Con, &quot;&quot;);
2442     if( last-&gt;is_Catch() ||
<span class="line-modified">2443         (last-&gt;is_Mach() &amp;&amp; last-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Halt) ) {</span>
<span class="line-modified">2444       // There might be a prior call.  Skip it.</span>
<span class="line-modified">2445       while (_bb_start &lt; _bb_end &amp;&amp; bb-&gt;get_node(--_bb_end)-&gt;is_MachProj());</span>



2446     } else if( last-&gt;is_MachNullCheck() ) {
2447       // Backup so the last null-checked memory instruction is
2448       // outside the schedulable range. Skip over the nullcheck,
2449       // projection, and the memory nodes.
2450       Node *mem = last-&gt;in(1);
2451       do {
2452         _bb_end--;
2453       } while (mem != bb-&gt;get_node(_bb_end));
2454     } else {
2455       // Set _bb_end to point after last schedulable inst.
2456       _bb_end++;
2457     }
2458 
2459     assert( _bb_start &lt;= _bb_end, &quot;inverted block ends&quot; );
2460 
2461     // Compute the register antidependencies for the basic block
2462     ComputeRegisterAntidependencies(bb);
2463     if (_cfg-&gt;C-&gt;failing())  return;  // too many D-U pinch points
2464 
2465     // Compute intra-bb latencies for the nodes
</pre>
<hr />
<pre>
2493       bb-&gt;map_node(_scheduled[_bb_end-k-1], k);
2494 
2495 #ifndef PRODUCT
2496     if (_cfg-&gt;C-&gt;trace_opto_output()) {
2497       tty-&gt;print(&quot;#  Schedule BB#%03d (final)\n&quot;, i);
2498       uint current = 0;
2499       for (uint j = 0; j &lt; bb-&gt;number_of_nodes(); j++) {
2500         Node *n = bb-&gt;get_node(j);
2501         if( valid_bundle_info(n) ) {
2502           Bundle *bundle = node_bundling(n);
2503           if (bundle-&gt;instr_count() &gt; 0 || bundle-&gt;flags() &gt; 0) {
2504             tty-&gt;print(&quot;*** Bundle: &quot;);
2505             bundle-&gt;dump();
2506           }
2507           n-&gt;dump();
2508         }
2509       }
2510     }
2511 #endif
2512 #ifdef ASSERT
<span class="line-modified">2513     verify_good_schedule(bb,&quot;after block local scheduling&quot;);</span>
2514 #endif
2515   }
2516 
2517 #ifndef PRODUCT
2518   if (_cfg-&gt;C-&gt;trace_opto_output())
2519     tty-&gt;print(&quot;# &lt;- DoScheduling\n&quot;);
2520 #endif
2521 
2522   // Record final node-bundling array location
2523   _regalloc-&gt;C-&gt;set_node_bundling_base(_node_bundling_base);
2524 
2525 } // end DoScheduling
2526 
2527 // Verify that no live-range used in the block is killed in the block by a
2528 // wrong DEF.  This doesn&#39;t verify live-ranges that span blocks.
2529 
2530 // Check for edge existence.  Used to avoid adding redundant precedence edges.
2531 static bool edge_from_to( Node *from, Node *to ) {
2532   for( uint i=0; i&lt;from-&gt;len(); i++ )
2533     if( from-&gt;in(i) == to )
</pre>
<hr />
<pre>
2841 // register is anti-dependent on a set of uses (or defs), rather
2842 // than adding an edge in the graph between each pair of kill
2843 // and use (or def), a pinch is inserted between them:
2844 //
2845 //            use1   use2  use3
2846 //                \   |   /
2847 //                 \  |  /
2848 //                  pinch
2849 //                 /  |  \
2850 //                /   |   \
2851 //            kill1 kill2 kill3
2852 //
2853 // One pinch node is created per register killed when
2854 // the second call is encountered during a backwards pass
2855 // over the block.  Most of these pinch nodes are never
2856 // wired into the graph because the register is never
2857 // used or def&#39;ed in the block.
2858 //
2859 void Scheduling::garbage_collect_pinch_nodes() {
2860 #ifndef PRODUCT
<span class="line-modified">2861   if (_cfg-&gt;C-&gt;trace_opto_output()) tty-&gt;print(&quot;Reclaimed pinch nodes:&quot;);</span>
<span class="line-modified">2862 #endif</span>
<span class="line-modified">2863   int trace_cnt = 0;</span>
<span class="line-modified">2864   for (uint k = 0; k &lt; _reg_node.Size(); k++) {</span>
<span class="line-modified">2865     Node* pinch = _reg_node[k];</span>
<span class="line-modified">2866     if ((pinch != NULL) &amp;&amp; pinch-&gt;Opcode() == Op_Node &amp;&amp;</span>
<span class="line-modified">2867         // no predecence input edges</span>
<span class="line-modified">2868         (pinch-&gt;req() == pinch-&gt;len() || pinch-&gt;in(pinch-&gt;req()) == NULL) ) {</span>
<span class="line-modified">2869       cleanup_pinch(pinch);</span>
<span class="line-modified">2870       _pinch_free_list.push(pinch);</span>
<span class="line-modified">2871       _reg_node.map(k, NULL);</span>
2872 #ifndef PRODUCT
<span class="line-modified">2873       if (_cfg-&gt;C-&gt;trace_opto_output()) {</span>
<span class="line-modified">2874         trace_cnt++;</span>
<span class="line-modified">2875         if (trace_cnt &gt; 40) {</span>
<span class="line-modified">2876           tty-&gt;print(&quot;\n&quot;);</span>
<span class="line-modified">2877           trace_cnt = 0;</span>


2878         }
<span class="line-modified">2879         tty-&gt;print(&quot; %d&quot;, pinch-&gt;_idx);</span>
2880       }
<span class="line-added">2881 #endif</span>
2882     }
<span class="line-added">2883   }</span>
2884 #ifndef PRODUCT
<span class="line-modified">2885   if (_cfg-&gt;C-&gt;trace_opto_output()) tty-&gt;print(&quot;\n&quot;);</span>
2886 #endif
2887 }
2888 
2889 // Clean up a pinch node for reuse.
2890 void Scheduling::cleanup_pinch( Node *pinch ) {
2891   assert (pinch &amp;&amp; pinch-&gt;Opcode() == Op_Node &amp;&amp; pinch-&gt;req() == 1, &quot;just checking&quot;);
2892 
2893   for (DUIterator_Last imin, i = pinch-&gt;last_outs(imin); i &gt;= imin; ) {
2894     Node* use = pinch-&gt;last_out(i);
2895     uint uses_found = 0;
2896     for (uint j = use-&gt;req(); j &lt; use-&gt;len(); j++) {
2897       if (use-&gt;in(j) == pinch) {
2898         use-&gt;rm_prec(j);
2899         uses_found++;
2900       }
2901     }
2902     assert(uses_found &gt; 0, &quot;must be a precedence edge&quot;);
2903     i -= uses_found;    // we deleted 1 or more copies of this edge
2904   }
2905   // May have a later_def entry
2906   pinch-&gt;set_req(0, NULL);
2907 }
2908 
2909 #ifndef PRODUCT
2910 
2911 void Scheduling::dump_available() const {
2912   tty-&gt;print(&quot;#Availist  &quot;);
2913   for (uint i = 0; i &lt; _available.size(); i++)
2914     tty-&gt;print(&quot; N%d/l%d&quot;, _available[i]-&gt;_idx,_current_latency[_available[i]-&gt;_idx]);
2915   tty-&gt;cr();
2916 }
2917 
2918 // Print Scheduling Statistics
2919 void Scheduling::print_statistics() {
2920   // Print the size added by nops for bundling
2921   tty-&gt;print(&quot;Nops added %d bytes to total of %d bytes&quot;,
<span class="line-modified">2922              _total_nop_size, _total_method_size);</span>
2923   if (_total_method_size &gt; 0)
2924     tty-&gt;print(&quot;, for %.2f%%&quot;,
<span class="line-modified">2925                ((double)_total_nop_size) / ((double) _total_method_size) * 100.0);</span>
2926   tty-&gt;print(&quot;\n&quot;);
2927 
2928   // Print the number of branch shadows filled
2929   if (Pipeline::_branch_has_delay_slot) {
2930     tty-&gt;print(&quot;Of %d branches, %d had unconditional delay slots filled&quot;,
<span class="line-modified">2931                _total_branches, _total_unconditional_delays);</span>
2932     if (_total_branches &gt; 0)
2933       tty-&gt;print(&quot;, for %.2f%%&quot;,
<span class="line-modified">2934                  ((double)_total_unconditional_delays) / ((double)_total_branches) * 100.0);</span>
2935     tty-&gt;print(&quot;\n&quot;);
2936   }
2937 
2938   uint total_instructions = 0, total_bundles = 0;
2939 
2940   for (uint i = 1; i &lt;= Pipeline::_max_instrs_per_cycle; i++) {
2941     uint bundle_count   = _total_instructions_per_bundle[i];
2942     total_instructions += bundle_count * i;
2943     total_bundles      += bundle_count;
2944   }
2945 
2946   if (total_bundles &gt; 0)
2947     tty-&gt;print(&quot;Average ILP (excluding nops) is %.2f\n&quot;,
<span class="line-modified">2948                ((double)total_instructions) / ((double)total_bundles));</span>
2949 }
2950 #endif
</pre>
</td>
</tr>
</table>
<center><a href="opaquenode.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="output.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>