<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/callnode.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="callnode.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="castnode.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/callnode.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  46 class       CallStaticJavaNode;
  47 class       CallDynamicJavaNode;
  48 class     CallRuntimeNode;
  49 class       CallLeafNode;
  50 class         CallLeafNoFPNode;
  51 class     AllocateNode;
  52 class       AllocateArrayNode;
  53 class     BoxLockNode;
  54 class     LockNode;
  55 class     UnlockNode;
  56 class JVMState;
  57 class OopMap;
  58 class State;
  59 class StartNode;
  60 class MachCallNode;
  61 class FastLockNode;
  62 
  63 //------------------------------StartNode--------------------------------------
  64 // The method start node
  65 class StartNode : public MultiNode {
<span class="line-modified">  66   virtual uint cmp( const Node &amp;n ) const;</span>
  67   virtual uint size_of() const; // Size is bigger
  68 public:
  69   const TypeTuple *_domain;
  70   StartNode( Node *root, const TypeTuple *domain ) : MultiNode(2), _domain(domain) {
  71     init_class_id(Class_Start);
  72     init_req(0,this);
  73     init_req(1,root);
  74   }
  75   virtual int Opcode() const;
  76   virtual bool pinned() const { return true; };
  77   virtual const Type *bottom_type() const;
  78   virtual const TypePtr *adr_type() const { return TypePtr::BOTTOM; }
  79   virtual const Type* Value(PhaseGVN* phase) const;
  80   virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);
  81   virtual void  calling_convention( BasicType* sig_bt, VMRegPair *parm_reg, uint length ) const;
  82   virtual const RegMask &amp;in_RegMask(uint) const;
  83   virtual Node *match( const ProjNode *proj, const Matcher *m );
  84   virtual uint ideal_reg() const { return 0; }
  85 #ifndef PRODUCT
  86   virtual void  dump_spec(outputStream *st) const;
</pre>
<hr />
<pre>
 304   JVMState* clone_shallow(Compile* C) const; // retains uncloned caller
 305   void      set_map_deep(SafePointNode *map);// reset map for all callers
 306   void      adapt_position(int delta);       // Adapt offsets in in-array after adding an edge.
 307   int       interpreter_frame_size() const;
 308 
 309 #ifndef PRODUCT
 310   void      format(PhaseRegAlloc *regalloc, const Node *n, outputStream* st) const;
 311   void      dump_spec(outputStream *st) const;
 312   void      dump_on(outputStream* st) const;
 313   void      dump() const {
 314     dump_on(tty);
 315   }
 316 #endif
 317 };
 318 
 319 //------------------------------SafePointNode----------------------------------
 320 // A SafePointNode is a subclass of a MultiNode for convenience (and
 321 // potential code sharing) only - conceptually it is independent of
 322 // the Node semantics.
 323 class SafePointNode : public MultiNode {
<span class="line-modified"> 324   virtual uint           cmp( const Node &amp;n ) const;</span>
 325   virtual uint           size_of() const;       // Size is bigger
 326 
 327 public:
 328   SafePointNode(uint edges, JVMState* jvms,
 329                 // A plain safepoint advertises no memory effects (NULL):
 330                 const TypePtr* adr_type = NULL)
 331     : MultiNode( edges ),
 332       _oop_map(NULL),
 333       _jvms(jvms),
 334       _adr_type(adr_type)
 335   {
 336     init_class_id(Class_SafePoint);
 337   }
 338 
 339   OopMap*         _oop_map;   // Array of OopMap info (8-bit char) for GC
 340   JVMState* const _jvms;      // Pointer to list of JVM State objects
 341   const TypePtr*  _adr_type;  // What type of memory does this node produce?
 342   ReplacedNodes   _replaced_nodes; // During parsing: list of pair of nodes from calls to GraphKit::replace_in_map()
 343 
 344   // Many calls take *all* of memory as input,
</pre>
<hr />
<pre>
 480   static  bool           needs_polling_address_input();
 481 
 482 #ifndef PRODUCT
 483   virtual void           dump_spec(outputStream *st) const;
 484   virtual void           related(GrowableArray&lt;Node*&gt; *in_rel, GrowableArray&lt;Node*&gt; *out_rel, bool compact) const;
 485 #endif
 486 };
 487 
 488 //------------------------------SafePointScalarObjectNode----------------------
 489 // A SafePointScalarObjectNode represents the state of a scalarized object
 490 // at a safepoint.
 491 
 492 class SafePointScalarObjectNode: public TypeNode {
 493   uint _first_index; // First input edge relative index of a SafePoint node where
 494                      // states of the scalarized object fields are collected.
 495                      // It is relative to the last (youngest) jvms-&gt;_scloff.
 496   uint _n_fields;    // Number of non-static fields of the scalarized object.
 497   DEBUG_ONLY(AllocateNode* _alloc;)
 498 
 499   virtual uint hash() const ; // { return NO_HASH; }
<span class="line-modified"> 500   virtual uint cmp( const Node &amp;n ) const;</span>
 501 
 502   uint first_index() const { return _first_index; }
 503 
 504 public:
 505   SafePointScalarObjectNode(const TypeOopPtr* tp,
 506 #ifdef ASSERT
 507                             AllocateNode* alloc,
 508 #endif
 509                             uint first_index, uint n_fields);
 510   virtual int Opcode() const;
 511   virtual uint           ideal_reg() const;
 512   virtual const RegMask &amp;in_RegMask(uint) const;
 513   virtual const RegMask &amp;out_RegMask() const;
 514   virtual uint           match_edge(uint idx) const;
 515 
 516   uint first_index(JVMState* jvms) const {
 517     assert(jvms != NULL, &quot;missed JVMS&quot;);
 518     return jvms-&gt;scloff() + _first_index;
 519   }
 520   uint n_fields()    const { return _n_fields; }
</pre>
<hr />
<pre>
 581       _generator(NULL),
 582       _name(NULL)
 583   {
 584     init_class_id(Class_Call);
 585   }
 586 
 587   const TypeFunc* tf()         const { return _tf; }
 588   const address  entry_point() const { return _entry_point; }
 589   const float    cnt()         const { return _cnt; }
 590   CallGenerator* generator()   const { return _generator; }
 591 
 592   void set_tf(const TypeFunc* tf)       { _tf = tf; }
 593   void set_entry_point(address p)       { _entry_point = p; }
 594   void set_cnt(float c)                 { _cnt = c; }
 595   void set_generator(CallGenerator* cg) { _generator = cg; }
 596 
 597   virtual const Type *bottom_type() const;
 598   virtual const Type* Value(PhaseGVN* phase) const;
 599   virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);
 600   virtual Node* Identity(PhaseGVN* phase) { return this; }
<span class="line-modified"> 601   virtual uint        cmp( const Node &amp;n ) const;</span>
 602   virtual uint        size_of() const = 0;
 603   virtual void        calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const;
 604   virtual Node       *match( const ProjNode *proj, const Matcher *m );
 605   virtual uint        ideal_reg() const { return NotAMachineReg; }
 606   // Are we guaranteed that this node is a safepoint?  Not true for leaf calls and
 607   // for some macro nodes whose expansion does not have a safepoint on the fast path.
 608   virtual bool        guaranteed_safepoint()  { return true; }
 609   // For macro nodes, the JVMState gets modified during expansion. If calls
 610   // use MachConstantBase, it gets modified during matching. So when cloning
 611   // the node the JVMState must be cloned. Default is not to clone.
 612   virtual void clone_jvms(Compile* C) {
 613     if (C-&gt;needs_clone_jvms() &amp;&amp; jvms() != NULL) {
 614       set_jvms(jvms()-&gt;clone_deep(C));
 615       jvms()-&gt;set_map_deep(this);
 616     }
 617   }
 618 
 619   // Returns true if the call may modify n
 620   virtual bool        may_modify(const TypeOopPtr *t_oop, PhaseTransform *phase);
 621   // Does this node have a use of n other than in debug information?
</pre>
<hr />
<pre>
 637   void extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts = true);
 638 
 639   virtual uint match_edge(uint idx) const;
 640 
 641   bool is_call_to_arraycopystub() const;
 642 
 643 #ifndef PRODUCT
 644   virtual void        dump_req(outputStream *st = tty) const;
 645   virtual void        dump_spec(outputStream *st) const;
 646 #endif
 647 };
 648 
 649 
 650 //------------------------------CallJavaNode-----------------------------------
 651 // Make a static or dynamic subroutine call node using Java calling
 652 // convention.  (The &quot;Java&quot; calling convention is the compiler&#39;s calling
 653 // convention, as opposed to the interpreter&#39;s or that of native C.)
 654 class CallJavaNode : public CallNode {
 655   friend class VMStructs;
 656 protected:
<span class="line-modified"> 657   virtual uint cmp( const Node &amp;n ) const;</span>
 658   virtual uint size_of() const; // Size is bigger
 659 
 660   bool    _optimized_virtual;
 661   bool    _method_handle_invoke;
 662   bool    _override_symbolic_info; // Override symbolic call site info from bytecode
 663   ciMethod* _method;               // Method being direct called
 664 public:
 665   const int       _bci;         // Byte Code Index of call byte code
 666   CallJavaNode(const TypeFunc* tf , address addr, ciMethod* method, int bci)
 667     : CallNode(tf, addr, TypePtr::BOTTOM),
 668       _optimized_virtual(false),
 669       _method_handle_invoke(false),
 670       _override_symbolic_info(false),
 671       _method(method), _bci(bci)
 672   {
 673     init_class_id(Class_CallJava);
 674   }
 675 
 676   virtual int   Opcode() const;
 677   ciMethod* method() const                 { return _method; }
</pre>
<hr />
<pre>
 679   void  set_optimized_virtual(bool f)      { _optimized_virtual = f; }
 680   bool  is_optimized_virtual() const       { return _optimized_virtual; }
 681   void  set_method_handle_invoke(bool f)   { _method_handle_invoke = f; }
 682   bool  is_method_handle_invoke() const    { return _method_handle_invoke; }
 683   void  set_override_symbolic_info(bool f) { _override_symbolic_info = f; }
 684   bool  override_symbolic_info() const     { return _override_symbolic_info; }
 685 
 686   DEBUG_ONLY( bool validate_symbolic_info() const; )
 687 
 688 #ifndef PRODUCT
 689   virtual void  dump_spec(outputStream *st) const;
 690   virtual void  dump_compact_spec(outputStream *st) const;
 691 #endif
 692 };
 693 
 694 //------------------------------CallStaticJavaNode-----------------------------
 695 // Make a direct subroutine call using Java calling convention (for static
 696 // calls and optimized virtual calls, plus calls to wrappers for run-time
 697 // routines); generates static stub.
 698 class CallStaticJavaNode : public CallJavaNode {
<span class="line-modified"> 699   virtual uint cmp( const Node &amp;n ) const;</span>
 700   virtual uint size_of() const; // Size is bigger
 701 public:
 702   CallStaticJavaNode(Compile* C, const TypeFunc* tf, address addr, ciMethod* method, int bci)
 703     : CallJavaNode(tf, addr, method, bci) {
 704     init_class_id(Class_CallStaticJava);
 705     if (C-&gt;eliminate_boxing() &amp;&amp; (method != NULL) &amp;&amp; method-&gt;is_boxing_method()) {
 706       init_flags(Flag_is_macro);
 707       C-&gt;add_macro_node(this);
 708     }
 709     _is_scalar_replaceable = false;
 710     _is_non_escaping = false;
 711   }
 712   CallStaticJavaNode(const TypeFunc* tf, address addr, const char* name, int bci,
 713                      const TypePtr* adr_type)
 714     : CallJavaNode(tf, addr, NULL, bci) {
 715     init_class_id(Class_CallStaticJava);
 716     // This node calls a runtime stub, which often has narrow memory effects.
 717     _adr_type = adr_type;
 718     _is_scalar_replaceable = false;
 719     _is_non_escaping = false;
</pre>
<hr />
<pre>
 733   }
 734   // Later inlining modifies the JVMState, so we need to clone it
 735   // when the call node is cloned (because it is macro node).
 736   virtual void  clone_jvms(Compile* C) {
 737     if ((jvms() != NULL) &amp;&amp; is_boxing_method()) {
 738       set_jvms(jvms()-&gt;clone_deep(C));
 739       jvms()-&gt;set_map_deep(this);
 740     }
 741   }
 742 
 743   virtual int         Opcode() const;
 744 #ifndef PRODUCT
 745   virtual void        dump_spec(outputStream *st) const;
 746   virtual void        dump_compact_spec(outputStream *st) const;
 747 #endif
 748 };
 749 
 750 //------------------------------CallDynamicJavaNode----------------------------
 751 // Make a dispatched call using Java calling convention.
 752 class CallDynamicJavaNode : public CallJavaNode {
<span class="line-modified"> 753   virtual uint cmp( const Node &amp;n ) const;</span>
 754   virtual uint size_of() const; // Size is bigger
 755 public:
 756   CallDynamicJavaNode( const TypeFunc *tf , address addr, ciMethod* method, int vtable_index, int bci ) : CallJavaNode(tf,addr,method,bci), _vtable_index(vtable_index) {
 757     init_class_id(Class_CallDynamicJava);
 758   }
 759 
 760   int _vtable_index;
 761   virtual int   Opcode() const;
 762 #ifndef PRODUCT
 763   virtual void  dump_spec(outputStream *st) const;
 764 #endif
 765 };
 766 
 767 //------------------------------CallRuntimeNode--------------------------------
 768 // Make a direct subroutine call node into compiled C++ code.
 769 class CallRuntimeNode : public CallNode {
<span class="line-modified"> 770   virtual uint cmp( const Node &amp;n ) const;</span>
 771   virtual uint size_of() const; // Size is bigger
 772 public:
 773   CallRuntimeNode(const TypeFunc* tf, address addr, const char* name,
 774                   const TypePtr* adr_type)
 775     : CallNode(tf, addr, adr_type)
 776   {
 777     init_class_id(Class_CallRuntime);
 778     _name = name;
 779   }
 780 
 781   virtual int   Opcode() const;
 782   virtual void  calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const;
 783 
 784 #ifndef PRODUCT
 785   virtual void  dump_spec(outputStream *st) const;
 786 #endif
 787 };
 788 
 789 //------------------------------CallLeafNode-----------------------------------
 790 // Make a direct subroutine call node into compiled C++ code, without
</pre>
<hr />
<pre>
 919   bool maybe_set_complete(PhaseGVN* phase);
 920 
 921   // Return true if allocation doesn&#39;t escape thread, its escape state
 922   // needs be noEscape or ArgEscape. InitializeNode._does_not_escape
 923   // is true when its allocation&#39;s escape state is noEscape or
 924   // ArgEscape. In case allocation&#39;s InitializeNode is NULL, check
 925   // AlllocateNode._is_non_escaping flag.
 926   // AlllocateNode._is_non_escaping is true when its escape state is
 927   // noEscape.
 928   bool does_not_escape_thread() {
 929     InitializeNode* init = NULL;
 930     return _is_non_escaping || (((init = initialization()) != NULL) &amp;&amp; init-&gt;does_not_escape());
 931   }
 932 
 933   // If object doesn&#39;t escape in &lt;.init&gt; method and there is memory barrier
 934   // inserted at exit of its &lt;.init&gt;, memory barrier for new is not necessary.
 935   // Inovke this method when MemBar at exit of initializer and post-dominate
 936   // allocation node.
 937   void compute_MemBar_redundancy(ciMethod* initializer);
 938   bool is_allocation_MemBar_redundant() { return _is_allocation_MemBar_redundant; }


 939 };
 940 
 941 //------------------------------AllocateArray---------------------------------
 942 //
 943 // High-level array allocation
 944 //
 945 class AllocateArrayNode : public AllocateNode {
 946 public:
 947   AllocateArrayNode(Compile* C, const TypeFunc *atype, Node *ctrl, Node *mem, Node *abio,
 948                     Node* size, Node* klass_node, Node* initial_test,
 949                     Node* count_val
 950                     )
 951     : AllocateNode(C, atype, ctrl, mem, abio, size, klass_node,
 952                    initial_test)
 953   {
 954     init_class_id(Class_AllocateArray);
 955     set_req(AllocateNode::ALength,        count_val);
 956   }
 957   virtual int Opcode() const;
 958   virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);
</pre>
</td>
<td>
<hr />
<pre>
  46 class       CallStaticJavaNode;
  47 class       CallDynamicJavaNode;
  48 class     CallRuntimeNode;
  49 class       CallLeafNode;
  50 class         CallLeafNoFPNode;
  51 class     AllocateNode;
  52 class       AllocateArrayNode;
  53 class     BoxLockNode;
  54 class     LockNode;
  55 class     UnlockNode;
  56 class JVMState;
  57 class OopMap;
  58 class State;
  59 class StartNode;
  60 class MachCallNode;
  61 class FastLockNode;
  62 
  63 //------------------------------StartNode--------------------------------------
  64 // The method start node
  65 class StartNode : public MultiNode {
<span class="line-modified">  66   virtual bool cmp( const Node &amp;n ) const;</span>
  67   virtual uint size_of() const; // Size is bigger
  68 public:
  69   const TypeTuple *_domain;
  70   StartNode( Node *root, const TypeTuple *domain ) : MultiNode(2), _domain(domain) {
  71     init_class_id(Class_Start);
  72     init_req(0,this);
  73     init_req(1,root);
  74   }
  75   virtual int Opcode() const;
  76   virtual bool pinned() const { return true; };
  77   virtual const Type *bottom_type() const;
  78   virtual const TypePtr *adr_type() const { return TypePtr::BOTTOM; }
  79   virtual const Type* Value(PhaseGVN* phase) const;
  80   virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);
  81   virtual void  calling_convention( BasicType* sig_bt, VMRegPair *parm_reg, uint length ) const;
  82   virtual const RegMask &amp;in_RegMask(uint) const;
  83   virtual Node *match( const ProjNode *proj, const Matcher *m );
  84   virtual uint ideal_reg() const { return 0; }
  85 #ifndef PRODUCT
  86   virtual void  dump_spec(outputStream *st) const;
</pre>
<hr />
<pre>
 304   JVMState* clone_shallow(Compile* C) const; // retains uncloned caller
 305   void      set_map_deep(SafePointNode *map);// reset map for all callers
 306   void      adapt_position(int delta);       // Adapt offsets in in-array after adding an edge.
 307   int       interpreter_frame_size() const;
 308 
 309 #ifndef PRODUCT
 310   void      format(PhaseRegAlloc *regalloc, const Node *n, outputStream* st) const;
 311   void      dump_spec(outputStream *st) const;
 312   void      dump_on(outputStream* st) const;
 313   void      dump() const {
 314     dump_on(tty);
 315   }
 316 #endif
 317 };
 318 
 319 //------------------------------SafePointNode----------------------------------
 320 // A SafePointNode is a subclass of a MultiNode for convenience (and
 321 // potential code sharing) only - conceptually it is independent of
 322 // the Node semantics.
 323 class SafePointNode : public MultiNode {
<span class="line-modified"> 324   virtual bool           cmp( const Node &amp;n ) const;</span>
 325   virtual uint           size_of() const;       // Size is bigger
 326 
 327 public:
 328   SafePointNode(uint edges, JVMState* jvms,
 329                 // A plain safepoint advertises no memory effects (NULL):
 330                 const TypePtr* adr_type = NULL)
 331     : MultiNode( edges ),
 332       _oop_map(NULL),
 333       _jvms(jvms),
 334       _adr_type(adr_type)
 335   {
 336     init_class_id(Class_SafePoint);
 337   }
 338 
 339   OopMap*         _oop_map;   // Array of OopMap info (8-bit char) for GC
 340   JVMState* const _jvms;      // Pointer to list of JVM State objects
 341   const TypePtr*  _adr_type;  // What type of memory does this node produce?
 342   ReplacedNodes   _replaced_nodes; // During parsing: list of pair of nodes from calls to GraphKit::replace_in_map()
 343 
 344   // Many calls take *all* of memory as input,
</pre>
<hr />
<pre>
 480   static  bool           needs_polling_address_input();
 481 
 482 #ifndef PRODUCT
 483   virtual void           dump_spec(outputStream *st) const;
 484   virtual void           related(GrowableArray&lt;Node*&gt; *in_rel, GrowableArray&lt;Node*&gt; *out_rel, bool compact) const;
 485 #endif
 486 };
 487 
 488 //------------------------------SafePointScalarObjectNode----------------------
 489 // A SafePointScalarObjectNode represents the state of a scalarized object
 490 // at a safepoint.
 491 
 492 class SafePointScalarObjectNode: public TypeNode {
 493   uint _first_index; // First input edge relative index of a SafePoint node where
 494                      // states of the scalarized object fields are collected.
 495                      // It is relative to the last (youngest) jvms-&gt;_scloff.
 496   uint _n_fields;    // Number of non-static fields of the scalarized object.
 497   DEBUG_ONLY(AllocateNode* _alloc;)
 498 
 499   virtual uint hash() const ; // { return NO_HASH; }
<span class="line-modified"> 500   virtual bool cmp( const Node &amp;n ) const;</span>
 501 
 502   uint first_index() const { return _first_index; }
 503 
 504 public:
 505   SafePointScalarObjectNode(const TypeOopPtr* tp,
 506 #ifdef ASSERT
 507                             AllocateNode* alloc,
 508 #endif
 509                             uint first_index, uint n_fields);
 510   virtual int Opcode() const;
 511   virtual uint           ideal_reg() const;
 512   virtual const RegMask &amp;in_RegMask(uint) const;
 513   virtual const RegMask &amp;out_RegMask() const;
 514   virtual uint           match_edge(uint idx) const;
 515 
 516   uint first_index(JVMState* jvms) const {
 517     assert(jvms != NULL, &quot;missed JVMS&quot;);
 518     return jvms-&gt;scloff() + _first_index;
 519   }
 520   uint n_fields()    const { return _n_fields; }
</pre>
<hr />
<pre>
 581       _generator(NULL),
 582       _name(NULL)
 583   {
 584     init_class_id(Class_Call);
 585   }
 586 
 587   const TypeFunc* tf()         const { return _tf; }
 588   const address  entry_point() const { return _entry_point; }
 589   const float    cnt()         const { return _cnt; }
 590   CallGenerator* generator()   const { return _generator; }
 591 
 592   void set_tf(const TypeFunc* tf)       { _tf = tf; }
 593   void set_entry_point(address p)       { _entry_point = p; }
 594   void set_cnt(float c)                 { _cnt = c; }
 595   void set_generator(CallGenerator* cg) { _generator = cg; }
 596 
 597   virtual const Type *bottom_type() const;
 598   virtual const Type* Value(PhaseGVN* phase) const;
 599   virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);
 600   virtual Node* Identity(PhaseGVN* phase) { return this; }
<span class="line-modified"> 601   virtual bool        cmp( const Node &amp;n ) const;</span>
 602   virtual uint        size_of() const = 0;
 603   virtual void        calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const;
 604   virtual Node       *match( const ProjNode *proj, const Matcher *m );
 605   virtual uint        ideal_reg() const { return NotAMachineReg; }
 606   // Are we guaranteed that this node is a safepoint?  Not true for leaf calls and
 607   // for some macro nodes whose expansion does not have a safepoint on the fast path.
 608   virtual bool        guaranteed_safepoint()  { return true; }
 609   // For macro nodes, the JVMState gets modified during expansion. If calls
 610   // use MachConstantBase, it gets modified during matching. So when cloning
 611   // the node the JVMState must be cloned. Default is not to clone.
 612   virtual void clone_jvms(Compile* C) {
 613     if (C-&gt;needs_clone_jvms() &amp;&amp; jvms() != NULL) {
 614       set_jvms(jvms()-&gt;clone_deep(C));
 615       jvms()-&gt;set_map_deep(this);
 616     }
 617   }
 618 
 619   // Returns true if the call may modify n
 620   virtual bool        may_modify(const TypeOopPtr *t_oop, PhaseTransform *phase);
 621   // Does this node have a use of n other than in debug information?
</pre>
<hr />
<pre>
 637   void extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts = true);
 638 
 639   virtual uint match_edge(uint idx) const;
 640 
 641   bool is_call_to_arraycopystub() const;
 642 
 643 #ifndef PRODUCT
 644   virtual void        dump_req(outputStream *st = tty) const;
 645   virtual void        dump_spec(outputStream *st) const;
 646 #endif
 647 };
 648 
 649 
 650 //------------------------------CallJavaNode-----------------------------------
 651 // Make a static or dynamic subroutine call node using Java calling
 652 // convention.  (The &quot;Java&quot; calling convention is the compiler&#39;s calling
 653 // convention, as opposed to the interpreter&#39;s or that of native C.)
 654 class CallJavaNode : public CallNode {
 655   friend class VMStructs;
 656 protected:
<span class="line-modified"> 657   virtual bool cmp( const Node &amp;n ) const;</span>
 658   virtual uint size_of() const; // Size is bigger
 659 
 660   bool    _optimized_virtual;
 661   bool    _method_handle_invoke;
 662   bool    _override_symbolic_info; // Override symbolic call site info from bytecode
 663   ciMethod* _method;               // Method being direct called
 664 public:
 665   const int       _bci;         // Byte Code Index of call byte code
 666   CallJavaNode(const TypeFunc* tf , address addr, ciMethod* method, int bci)
 667     : CallNode(tf, addr, TypePtr::BOTTOM),
 668       _optimized_virtual(false),
 669       _method_handle_invoke(false),
 670       _override_symbolic_info(false),
 671       _method(method), _bci(bci)
 672   {
 673     init_class_id(Class_CallJava);
 674   }
 675 
 676   virtual int   Opcode() const;
 677   ciMethod* method() const                 { return _method; }
</pre>
<hr />
<pre>
 679   void  set_optimized_virtual(bool f)      { _optimized_virtual = f; }
 680   bool  is_optimized_virtual() const       { return _optimized_virtual; }
 681   void  set_method_handle_invoke(bool f)   { _method_handle_invoke = f; }
 682   bool  is_method_handle_invoke() const    { return _method_handle_invoke; }
 683   void  set_override_symbolic_info(bool f) { _override_symbolic_info = f; }
 684   bool  override_symbolic_info() const     { return _override_symbolic_info; }
 685 
 686   DEBUG_ONLY( bool validate_symbolic_info() const; )
 687 
 688 #ifndef PRODUCT
 689   virtual void  dump_spec(outputStream *st) const;
 690   virtual void  dump_compact_spec(outputStream *st) const;
 691 #endif
 692 };
 693 
 694 //------------------------------CallStaticJavaNode-----------------------------
 695 // Make a direct subroutine call using Java calling convention (for static
 696 // calls and optimized virtual calls, plus calls to wrappers for run-time
 697 // routines); generates static stub.
 698 class CallStaticJavaNode : public CallJavaNode {
<span class="line-modified"> 699   virtual bool cmp( const Node &amp;n ) const;</span>
 700   virtual uint size_of() const; // Size is bigger
 701 public:
 702   CallStaticJavaNode(Compile* C, const TypeFunc* tf, address addr, ciMethod* method, int bci)
 703     : CallJavaNode(tf, addr, method, bci) {
 704     init_class_id(Class_CallStaticJava);
 705     if (C-&gt;eliminate_boxing() &amp;&amp; (method != NULL) &amp;&amp; method-&gt;is_boxing_method()) {
 706       init_flags(Flag_is_macro);
 707       C-&gt;add_macro_node(this);
 708     }
 709     _is_scalar_replaceable = false;
 710     _is_non_escaping = false;
 711   }
 712   CallStaticJavaNode(const TypeFunc* tf, address addr, const char* name, int bci,
 713                      const TypePtr* adr_type)
 714     : CallJavaNode(tf, addr, NULL, bci) {
 715     init_class_id(Class_CallStaticJava);
 716     // This node calls a runtime stub, which often has narrow memory effects.
 717     _adr_type = adr_type;
 718     _is_scalar_replaceable = false;
 719     _is_non_escaping = false;
</pre>
<hr />
<pre>
 733   }
 734   // Later inlining modifies the JVMState, so we need to clone it
 735   // when the call node is cloned (because it is macro node).
 736   virtual void  clone_jvms(Compile* C) {
 737     if ((jvms() != NULL) &amp;&amp; is_boxing_method()) {
 738       set_jvms(jvms()-&gt;clone_deep(C));
 739       jvms()-&gt;set_map_deep(this);
 740     }
 741   }
 742 
 743   virtual int         Opcode() const;
 744 #ifndef PRODUCT
 745   virtual void        dump_spec(outputStream *st) const;
 746   virtual void        dump_compact_spec(outputStream *st) const;
 747 #endif
 748 };
 749 
 750 //------------------------------CallDynamicJavaNode----------------------------
 751 // Make a dispatched call using Java calling convention.
 752 class CallDynamicJavaNode : public CallJavaNode {
<span class="line-modified"> 753   virtual bool cmp( const Node &amp;n ) const;</span>
 754   virtual uint size_of() const; // Size is bigger
 755 public:
 756   CallDynamicJavaNode( const TypeFunc *tf , address addr, ciMethod* method, int vtable_index, int bci ) : CallJavaNode(tf,addr,method,bci), _vtable_index(vtable_index) {
 757     init_class_id(Class_CallDynamicJava);
 758   }
 759 
 760   int _vtable_index;
 761   virtual int   Opcode() const;
 762 #ifndef PRODUCT
 763   virtual void  dump_spec(outputStream *st) const;
 764 #endif
 765 };
 766 
 767 //------------------------------CallRuntimeNode--------------------------------
 768 // Make a direct subroutine call node into compiled C++ code.
 769 class CallRuntimeNode : public CallNode {
<span class="line-modified"> 770   virtual bool cmp( const Node &amp;n ) const;</span>
 771   virtual uint size_of() const; // Size is bigger
 772 public:
 773   CallRuntimeNode(const TypeFunc* tf, address addr, const char* name,
 774                   const TypePtr* adr_type)
 775     : CallNode(tf, addr, adr_type)
 776   {
 777     init_class_id(Class_CallRuntime);
 778     _name = name;
 779   }
 780 
 781   virtual int   Opcode() const;
 782   virtual void  calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const;
 783 
 784 #ifndef PRODUCT
 785   virtual void  dump_spec(outputStream *st) const;
 786 #endif
 787 };
 788 
 789 //------------------------------CallLeafNode-----------------------------------
 790 // Make a direct subroutine call node into compiled C++ code, without
</pre>
<hr />
<pre>
 919   bool maybe_set_complete(PhaseGVN* phase);
 920 
 921   // Return true if allocation doesn&#39;t escape thread, its escape state
 922   // needs be noEscape or ArgEscape. InitializeNode._does_not_escape
 923   // is true when its allocation&#39;s escape state is noEscape or
 924   // ArgEscape. In case allocation&#39;s InitializeNode is NULL, check
 925   // AlllocateNode._is_non_escaping flag.
 926   // AlllocateNode._is_non_escaping is true when its escape state is
 927   // noEscape.
 928   bool does_not_escape_thread() {
 929     InitializeNode* init = NULL;
 930     return _is_non_escaping || (((init = initialization()) != NULL) &amp;&amp; init-&gt;does_not_escape());
 931   }
 932 
 933   // If object doesn&#39;t escape in &lt;.init&gt; method and there is memory barrier
 934   // inserted at exit of its &lt;.init&gt;, memory barrier for new is not necessary.
 935   // Inovke this method when MemBar at exit of initializer and post-dominate
 936   // allocation node.
 937   void compute_MemBar_redundancy(ciMethod* initializer);
 938   bool is_allocation_MemBar_redundant() { return _is_allocation_MemBar_redundant; }
<span class="line-added"> 939 </span>
<span class="line-added"> 940   Node* make_ideal_mark(PhaseGVN *phase, Node* obj, Node* control, Node* mem);</span>
 941 };
 942 
 943 //------------------------------AllocateArray---------------------------------
 944 //
 945 // High-level array allocation
 946 //
 947 class AllocateArrayNode : public AllocateNode {
 948 public:
 949   AllocateArrayNode(Compile* C, const TypeFunc *atype, Node *ctrl, Node *mem, Node *abio,
 950                     Node* size, Node* klass_node, Node* initial_test,
 951                     Node* count_val
 952                     )
 953     : AllocateNode(C, atype, ctrl, mem, abio, size, klass_node,
 954                    initial_test)
 955   {
 956     init_class_id(Class_AllocateArray);
 957     set_req(AllocateNode::ALength,        count_val);
 958   }
 959   virtual int Opcode() const;
 960   virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);
</pre>
</td>
</tr>
</table>
<center><a href="callnode.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="castnode.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>