<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/matcher.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="matcher.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="memnode.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/matcher.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
108 
109   // Map dense rule number to determine if this is an instruction chain rule
110   const uint _begin_inst_chain_rule;
111   const uint _end_inst_chain_rule;
112 
113   // We want to clone constants and possible CmpI-variants.
114   // If we do not clone CmpI, then we can have many instances of
115   // condition codes alive at once.  This is OK on some chips and
116   // bad on others.  Hence the machine-dependent table lookup.
117   const char *_must_clone;
118 
119   // Find shared Nodes, or Nodes that otherwise are Matcher roots
120   void find_shared( Node *n );
121   bool find_shared_visit(MStack&amp; mstack, Node* n, uint opcode, bool&amp; mem_op, int&amp; mem_addr_idx);
122   void find_shared_post_visit(Node* n, uint opcode);
123 
124 #ifdef X86
125   bool is_bmi_pattern(Node *n, Node *m);
126 #endif
127 


128   // Debug and profile information for nodes in old space:
129   GrowableArray&lt;Node_Notes*&gt;* _old_node_note_array;
130 
131   // Node labeling iterator for instruction selection
132   Node *Label_Root( const Node *n, State *svec, Node *control, const Node *mem );
133 
134   Node *transform( Node *dummy );
135 
136   Node_List _projection_list;        // For Machine nodes killing many values
137 
138   Node_Array _shared_nodes;
139 
140   debug_only(Node_Array _old2new_map;)   // Map roots of ideal-trees to machine-roots
141   debug_only(Node_Array _new2old_map;)   // Maps machine nodes back to ideal
142 
143   // Accessors for the inherited field PhaseTransform::_nodes:
144   void   grow_new_node_array(uint idx_limit) {
145     _nodes.map(idx_limit-1, NULL);
146   }
147   bool    has_new_node(const Node* n) const {
</pre>
<hr />
<pre>
296   // Register number of the stack slot corresponding to the highest outgoing
297   // argument on the stack.  Per the Big Picture in the AD file, it is:
298   //   _new_SP + max outgoing arguments of all calls
299   OptoReg::Name _out_arg_limit;
300 
301   OptoRegPair *_parm_regs;        // Array of machine registers per argument
302   RegMask *_calling_convention_mask; // Array of RegMasks per argument
303 
304   // Does matcher have a match rule for this ideal node?
305   static const bool has_match_rule(int opcode);
306   static const bool _hasMatchRule[_last_opcode];
307 
308   // Does matcher have a match rule for this ideal node and is the
309   // predicate (if there is one) true?
310   // NOTE: If this function is used more commonly in the future, ADLC
311   // should generate this one.
312   static const bool match_rule_supported(int opcode);
313 
314   // identify extra cases that we might want to provide match rules for
315   // e.g. Op_ vector nodes and other intrinsics while guarding with vlen
<span class="line-modified">316   static const bool match_rule_supported_vector(int opcode, int vlen);</span>
317 
318   // Some microarchitectures have mask registers used on vectors
319   static const bool has_predicated_vectors(void);
320 
321   // Some uarchs have different sized float register resources
322   static const int float_pressure(int default_pressure_threshold);
323 
324   // Used to determine if we have fast l2f conversion
325   // USII has it, USIII doesn&#39;t
326   static const bool convL2FSupported(void);
327 
328   // Vector width in bytes
329   static const int vector_width_in_bytes(BasicType bt);
330 
331   // Limits on vector size (number of elements).
332   static const int max_vector_size(const BasicType bt);
333   static const int min_vector_size(const BasicType bt);
334   static const bool vector_size_supported(const BasicType bt, int size) {
335     return (Matcher::max_vector_size(bt) &gt;= size &amp;&amp;
336             Matcher::min_vector_size(bt) &lt;= size);
</pre>
<hr />
<pre>
470   // NullCheck narrow_oop_reg
471   //
472   // When narrow oops can&#39;t fold into address expression (Sparc) and
473   // base is not null use decode_not_null and normal implicit null check.
474   // Note, decode_not_null node can be used here since it is referenced
475   // only on non null path but it requires special handling, see
476   // collect_null_checks():
477   //
478   // decode_not_null narrow_oop_reg, oop_reg // &#39;shift&#39; and &#39;add base&#39;
479   // [oop_reg + offset]
480   // NullCheck oop_reg
481   //
482   // With Zero base and when narrow oops can not fold into address
483   // expression use normal implicit null check since only shift
484   // is needed to decode narrow oop.
485   //
486   // decode narrow_oop_reg, oop_reg // only &#39;shift&#39;
487   // [oop_reg + offset]
488   // NullCheck oop_reg
489   //
<span class="line-modified">490   inline static bool gen_narrow_oop_implicit_null_checks() {</span>
<span class="line-removed">491     // Advice matcher to perform null checks on the narrow oop side.</span>
<span class="line-removed">492     // Implicit checks are not possible on the uncompressed oop side anyway</span>
<span class="line-removed">493     // (at least not for read accesses).</span>
<span class="line-removed">494     // Performs significantly better (especially on Power 6).</span>
<span class="line-removed">495     if (!os::zero_page_read_protected()) {</span>
<span class="line-removed">496       return true;</span>
<span class="line-removed">497     }</span>
<span class="line-removed">498     return Universe::narrow_oop_use_implicit_null_checks() &amp;&amp;</span>
<span class="line-removed">499            (narrow_oop_use_complex_address() ||</span>
<span class="line-removed">500             Universe::narrow_oop_base() != NULL);</span>
<span class="line-removed">501   }</span>
502 
503   // Is it better to copy float constants, or load them directly from memory?
504   // Intel can load a float constant from a direct address, requiring no
505   // extra registers.  Most RISCs will have to materialize an address into a
506   // register first, so they may as well materialize the constant immediately.
507   static const bool rematerialize_float_constants;
508 
509   // If CPU can load and store mis-aligned doubles directly then no fixup is
510   // needed.  Else we split the double into 2 integer pieces and move it
511   // piece-by-piece.  Only happens when passing doubles into C code or when
512   // calling i2c adapters as the Java calling convention forces doubles to be
513   // aligned.
514   static const bool misaligned_doubles_ok;
515 
516   // Does the CPU require postalloc expand (see block.cpp for description of
517   // postalloc expand)?
518   static const bool require_postalloc_expand;
519 
























520   // Perform a platform dependent implicit null fixup.  This is needed
521   // on windows95 to take care of some unusual register constraints.
522   void pd_implicit_null_fixup(MachNode *load, uint idx);
523 
<span class="line-modified">524   // Advertise here if the CPU requires explicit rounding operations</span>
<span class="line-removed">525   // to implement the UseStrictFP mode.</span>
526   static const bool strict_fp_requires_explicit_rounding;
527 
528   // Are floats conerted to double when stored to stack during deoptimization?
529   static bool float_in_double();
530   // Do ints take an entire long register or just half?
531   static const bool int_in_long;
532 
533   // Do the processor&#39;s shift instructions only use the low 5/6 bits
534   // of the count for 32/64 bit ints? If not we need to do the masking
535   // ourselves.
536   static const bool need_masked_shift_count;
537 
538   // Whether code generation need accurate ConvI2L types.
539   static const bool convi2l_type_required;
540 
541   // This routine is run whenever a graph fails to match.
542   // If it returns, the compiler should bailout to interpreter without error.
543   // In non-product mode, SoftMatchFailure is false to detect non-canonical
544   // graphs.  Print a message and exit.
545   static void soft_match_failure() {
</pre>
</td>
<td>
<hr />
<pre>
108 
109   // Map dense rule number to determine if this is an instruction chain rule
110   const uint _begin_inst_chain_rule;
111   const uint _end_inst_chain_rule;
112 
113   // We want to clone constants and possible CmpI-variants.
114   // If we do not clone CmpI, then we can have many instances of
115   // condition codes alive at once.  This is OK on some chips and
116   // bad on others.  Hence the machine-dependent table lookup.
117   const char *_must_clone;
118 
119   // Find shared Nodes, or Nodes that otherwise are Matcher roots
120   void find_shared( Node *n );
121   bool find_shared_visit(MStack&amp; mstack, Node* n, uint opcode, bool&amp; mem_op, int&amp; mem_addr_idx);
122   void find_shared_post_visit(Node* n, uint opcode);
123 
124 #ifdef X86
125   bool is_bmi_pattern(Node *n, Node *m);
126 #endif
127 
<span class="line-added">128   bool is_vshift_con_pattern(Node *n, Node *m);</span>
<span class="line-added">129 </span>
130   // Debug and profile information for nodes in old space:
131   GrowableArray&lt;Node_Notes*&gt;* _old_node_note_array;
132 
133   // Node labeling iterator for instruction selection
134   Node *Label_Root( const Node *n, State *svec, Node *control, const Node *mem );
135 
136   Node *transform( Node *dummy );
137 
138   Node_List _projection_list;        // For Machine nodes killing many values
139 
140   Node_Array _shared_nodes;
141 
142   debug_only(Node_Array _old2new_map;)   // Map roots of ideal-trees to machine-roots
143   debug_only(Node_Array _new2old_map;)   // Maps machine nodes back to ideal
144 
145   // Accessors for the inherited field PhaseTransform::_nodes:
146   void   grow_new_node_array(uint idx_limit) {
147     _nodes.map(idx_limit-1, NULL);
148   }
149   bool    has_new_node(const Node* n) const {
</pre>
<hr />
<pre>
298   // Register number of the stack slot corresponding to the highest outgoing
299   // argument on the stack.  Per the Big Picture in the AD file, it is:
300   //   _new_SP + max outgoing arguments of all calls
301   OptoReg::Name _out_arg_limit;
302 
303   OptoRegPair *_parm_regs;        // Array of machine registers per argument
304   RegMask *_calling_convention_mask; // Array of RegMasks per argument
305 
306   // Does matcher have a match rule for this ideal node?
307   static const bool has_match_rule(int opcode);
308   static const bool _hasMatchRule[_last_opcode];
309 
310   // Does matcher have a match rule for this ideal node and is the
311   // predicate (if there is one) true?
312   // NOTE: If this function is used more commonly in the future, ADLC
313   // should generate this one.
314   static const bool match_rule_supported(int opcode);
315 
316   // identify extra cases that we might want to provide match rules for
317   // e.g. Op_ vector nodes and other intrinsics while guarding with vlen
<span class="line-modified">318   static const bool match_rule_supported_vector(int opcode, int vlen, BasicType bt);</span>
319 
320   // Some microarchitectures have mask registers used on vectors
321   static const bool has_predicated_vectors(void);
322 
323   // Some uarchs have different sized float register resources
324   static const int float_pressure(int default_pressure_threshold);
325 
326   // Used to determine if we have fast l2f conversion
327   // USII has it, USIII doesn&#39;t
328   static const bool convL2FSupported(void);
329 
330   // Vector width in bytes
331   static const int vector_width_in_bytes(BasicType bt);
332 
333   // Limits on vector size (number of elements).
334   static const int max_vector_size(const BasicType bt);
335   static const int min_vector_size(const BasicType bt);
336   static const bool vector_size_supported(const BasicType bt, int size) {
337     return (Matcher::max_vector_size(bt) &gt;= size &amp;&amp;
338             Matcher::min_vector_size(bt) &lt;= size);
</pre>
<hr />
<pre>
472   // NullCheck narrow_oop_reg
473   //
474   // When narrow oops can&#39;t fold into address expression (Sparc) and
475   // base is not null use decode_not_null and normal implicit null check.
476   // Note, decode_not_null node can be used here since it is referenced
477   // only on non null path but it requires special handling, see
478   // collect_null_checks():
479   //
480   // decode_not_null narrow_oop_reg, oop_reg // &#39;shift&#39; and &#39;add base&#39;
481   // [oop_reg + offset]
482   // NullCheck oop_reg
483   //
484   // With Zero base and when narrow oops can not fold into address
485   // expression use normal implicit null check since only shift
486   // is needed to decode narrow oop.
487   //
488   // decode narrow_oop_reg, oop_reg // only &#39;shift&#39;
489   // [oop_reg + offset]
490   // NullCheck oop_reg
491   //
<span class="line-modified">492   static bool gen_narrow_oop_implicit_null_checks();</span>











493 
494   // Is it better to copy float constants, or load them directly from memory?
495   // Intel can load a float constant from a direct address, requiring no
496   // extra registers.  Most RISCs will have to materialize an address into a
497   // register first, so they may as well materialize the constant immediately.
498   static const bool rematerialize_float_constants;
499 
500   // If CPU can load and store mis-aligned doubles directly then no fixup is
501   // needed.  Else we split the double into 2 integer pieces and move it
502   // piece-by-piece.  Only happens when passing doubles into C code or when
503   // calling i2c adapters as the Java calling convention forces doubles to be
504   // aligned.
505   static const bool misaligned_doubles_ok;
506 
507   // Does the CPU require postalloc expand (see block.cpp for description of
508   // postalloc expand)?
509   static const bool require_postalloc_expand;
510 
<span class="line-added">511   // Does the platform support generic vector operands?</span>
<span class="line-added">512   // Requires cleanup after selection phase.</span>
<span class="line-added">513   static const bool supports_generic_vector_operands;</span>
<span class="line-added">514 </span>
<span class="line-added">515  private:</span>
<span class="line-added">516   void do_postselect_cleanup();</span>
<span class="line-added">517 </span>
<span class="line-added">518   void specialize_generic_vector_operands();</span>
<span class="line-added">519   void specialize_mach_node(MachNode* m);</span>
<span class="line-added">520   void specialize_temp_node(MachTempNode* tmp, MachNode* use, uint idx);</span>
<span class="line-added">521   MachOper* specialize_vector_operand(MachNode* m, uint opnd_idx);</span>
<span class="line-added">522   MachOper* specialize_vector_operand_helper(MachNode* m, uint opnd_idx, const TypeVect* vt);</span>
<span class="line-added">523 </span>
<span class="line-added">524   static MachOper* specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp);</span>
<span class="line-added">525 </span>
<span class="line-added">526   static bool is_generic_reg2reg_move(MachNode* m);</span>
<span class="line-added">527   static bool is_generic_vector(MachOper* opnd);</span>
<span class="line-added">528 </span>
<span class="line-added">529   const RegMask* regmask_for_ideal_register(uint ideal_reg, Node* ret);</span>
<span class="line-added">530 </span>
<span class="line-added">531   // Graph verification code</span>
<span class="line-added">532   DEBUG_ONLY( bool verify_after_postselect_cleanup(); )</span>
<span class="line-added">533 </span>
<span class="line-added">534  public:</span>
535   // Perform a platform dependent implicit null fixup.  This is needed
536   // on windows95 to take care of some unusual register constraints.
537   void pd_implicit_null_fixup(MachNode *load, uint idx);
538 
<span class="line-modified">539   // Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.</span>

540   static const bool strict_fp_requires_explicit_rounding;
541 
542   // Are floats conerted to double when stored to stack during deoptimization?
543   static bool float_in_double();
544   // Do ints take an entire long register or just half?
545   static const bool int_in_long;
546 
547   // Do the processor&#39;s shift instructions only use the low 5/6 bits
548   // of the count for 32/64 bit ints? If not we need to do the masking
549   // ourselves.
550   static const bool need_masked_shift_count;
551 
552   // Whether code generation need accurate ConvI2L types.
553   static const bool convi2l_type_required;
554 
555   // This routine is run whenever a graph fails to match.
556   // If it returns, the compiler should bailout to interpreter without error.
557   // In non-product mode, SoftMatchFailure is false to detect non-canonical
558   // graphs.  Print a message and exit.
559   static void soft_match_failure() {
</pre>
</td>
</tr>
</table>
<center><a href="matcher.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="memnode.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>