<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/lcm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="indexSet.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="library_call.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/lcm.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1998, 2018, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.inline.hpp&quot;
  27 #include &quot;memory/allocation.inline.hpp&quot;

  28 #include &quot;opto/ad.hpp&quot;
  29 #include &quot;opto/block.hpp&quot;
  30 #include &quot;opto/c2compiler.hpp&quot;
  31 #include &quot;opto/callnode.hpp&quot;
  32 #include &quot;opto/cfgnode.hpp&quot;
  33 #include &quot;opto/machnode.hpp&quot;
  34 #include &quot;opto/runtime.hpp&quot;
  35 #include &quot;opto/chaitin.hpp&quot;
  36 #include &quot;runtime/sharedRuntime.hpp&quot;
  37 
  38 // Optimization - Graph Style
  39 
  40 // Check whether val is not-null-decoded compressed oop,
  41 // i.e. will grab into the base of the heap if it represents NULL.
  42 static bool accesses_heap_base_zone(Node *val) {
<span class="line-modified">  43   if (Universe::narrow_oop_base() != NULL) { // Implies UseCompressedOops.</span>
  44     if (val &amp;&amp; val-&gt;is_Mach()) {
  45       if (val-&gt;as_Mach()-&gt;ideal_Opcode() == Op_DecodeN) {
  46         // This assumes all Decodes with TypePtr::NotNull are matched to nodes that
  47         // decode NULL to point to the heap base (Decode_NN).
  48         if (val-&gt;bottom_type()-&gt;is_oopptr()-&gt;ptr() == TypePtr::NotNull) {
  49           return true;
  50         }
  51       }
  52       // Must recognize load operation with Decode matched in memory operand.
  53       // We should not reach here exept for PPC/AIX, as os::zero_page_read_protected()
  54       // returns true everywhere else. On PPC, no such memory operands
  55       // exist, therefore we did not yet implement a check for such operands.
  56       NOT_AIX(Unimplemented());
  57     }
  58   }
  59   return false;
  60 }
  61 
  62 static bool needs_explicit_null_check_for_read(Node *val) {
  63   // On some OSes (AIX) the page at address 0 is only write protected.
  64   // If so, only Store operations will trap.
  65   if (os::zero_page_read_protected()) {
  66     return false;  // Implicit null check will work.
  67   }
  68   // Also a read accessing the base of a heap-based compressed heap will trap.
<span class="line-modified">  69   if (accesses_heap_base_zone(val) &amp;&amp;                    // Hits the base zone page.</span>
<span class="line-modified">  70       Universe::narrow_oop_use_implicit_null_checks()) { // Base zone page is protected.</span>
  71     return false;
  72   }
  73 
  74   return true;
  75 }
  76 
  77 //------------------------------implicit_null_check----------------------------
  78 // Detect implicit-null-check opportunities.  Basically, find NULL checks
  79 // with suitable memory ops nearby.  Use the memory op to do the NULL check.
  80 // I can generate a memory op if there is not one nearby.
  81 // The proj is the control projection for the not-null case.
  82 // The val is the pointer being checked for nullness or
  83 // decodeHeapOop_not_null node if it did not fold into address.
  84 void PhaseCFG::implicit_null_check(Block* block, Node *proj, Node *val, int allowed_reasons) {
  85   // Assume if null check need for 0 offset then always needed
  86   // Intel solaris doesn&#39;t support any null checks yet and no
  87   // mechanism exists (yet) to set the switches at an os_cpu level
  88   if( !ImplicitNullChecks || MacroAssembler::needs_explicit_null_check(0)) return;
  89 
  90   // Make sure the ptr-is-null path appears to be uncommon!
</pre>
<hr />
<pre>
 152 
 153   // Search the successor block for a load or store who&#39;s base value is also
 154   // the tested value.  There may be several.
 155   Node_List *out = new Node_List(Thread::current()-&gt;resource_area());
 156   MachNode *best = NULL;        // Best found so far
 157   for (DUIterator i = val-&gt;outs(); val-&gt;has_out(i); i++) {
 158     Node *m = val-&gt;out(i);
 159     if( !m-&gt;is_Mach() ) continue;
 160     MachNode *mach = m-&gt;as_Mach();
 161     was_store = false;
 162     int iop = mach-&gt;ideal_Opcode();
 163     switch( iop ) {
 164     case Op_LoadB:
 165     case Op_LoadUB:
 166     case Op_LoadUS:
 167     case Op_LoadD:
 168     case Op_LoadF:
 169     case Op_LoadI:
 170     case Op_LoadL:
 171     case Op_LoadP:
<span class="line-removed"> 172     case Op_LoadBarrierSlowReg:</span>
<span class="line-removed"> 173     case Op_LoadBarrierWeakSlowReg:</span>
 174     case Op_LoadN:
 175     case Op_LoadS:
 176     case Op_LoadKlass:
 177     case Op_LoadNKlass:
 178     case Op_LoadRange:
 179     case Op_LoadD_unaligned:
 180     case Op_LoadL_unaligned:
<span class="line-removed"> 181     case Op_ShenandoahReadBarrier:</span>
 182       assert(mach-&gt;in(2) == val, &quot;should be address&quot;);
 183       break;
 184     case Op_StoreB:
 185     case Op_StoreC:
 186     case Op_StoreCM:
 187     case Op_StoreD:
 188     case Op_StoreF:
 189     case Op_StoreI:
 190     case Op_StoreL:
 191     case Op_StoreP:
 192     case Op_StoreN:
 193     case Op_StoreNKlass:
 194       was_store = true;         // Memory op is a store op
 195       // Stores will have their address in slot 2 (memory in slot 1).
 196       // If the value being nul-checked is in another slot, it means we
 197       // are storing the checked value, which does NOT check the value!
 198       if( mach-&gt;in(2) != val ) continue;
 199       break;                    // Found a memory op?
 200     case Op_StrComp:
 201     case Op_StrEquals:
</pre>
<hr />
<pre>
 245     }
 246 
 247     // Check that node&#39;s control edge is not-null block&#39;s head or dominates it,
 248     // otherwise we can&#39;t hoist it because there are other control dependencies.
 249     Node* ctrl = mach-&gt;in(0);
 250     if (ctrl != NULL &amp;&amp; !(ctrl == not_null_block-&gt;head() ||
 251         get_block_for_node(ctrl)-&gt;dominates(not_null_block))) {
 252       continue;
 253     }
 254 
 255     // check if the offset is not too high for implicit exception
 256     {
 257       intptr_t offset = 0;
 258       const TypePtr *adr_type = NULL;  // Do not need this return value here
 259       const Node* base = mach-&gt;get_base_and_disp(offset, adr_type);
 260       if (base == NULL || base == NodeSentinel) {
 261         // Narrow oop address doesn&#39;t have base, only index.
 262         // Give up if offset is beyond page size or if heap base is not protected.
 263         if (val-&gt;bottom_type()-&gt;isa_narrowoop() &amp;&amp;
 264             (MacroAssembler::needs_explicit_null_check(offset) ||
<span class="line-modified"> 265              !Universe::narrow_oop_use_implicit_null_checks()))</span>
 266           continue;
 267         // cannot reason about it; is probably not implicit null exception
 268       } else {
 269         const TypePtr* tptr;
<span class="line-modified"> 270         if (UseCompressedOops &amp;&amp; (Universe::narrow_oop_shift() == 0 ||</span>
<span class="line-modified"> 271                                   Universe::narrow_klass_shift() == 0)) {</span>
 272           // 32-bits narrow oop can be the base of address expressions
 273           tptr = base-&gt;get_ptr_type();
 274         } else {
 275           // only regular oops are expected here
 276           tptr = base-&gt;bottom_type()-&gt;is_ptr();
 277         }
 278         // Give up if offset is not a compile-time constant.
 279         if (offset == Type::OffsetBot || tptr-&gt;_offset == Type::OffsetBot)
 280           continue;
 281         offset += tptr-&gt;_offset; // correct if base is offseted
 282         // Give up if reference is beyond page size.
 283         if (MacroAssembler::needs_explicit_null_check(offset))
 284           continue;
 285         // Give up if base is a decode node and the heap base is not protected.
 286         if (base-&gt;is_Mach() &amp;&amp; base-&gt;as_Mach()-&gt;ideal_Opcode() == Op_DecodeN &amp;&amp;
<span class="line-modified"> 287             !Universe::narrow_oop_use_implicit_null_checks())</span>
 288           continue;
 289       }
 290     }
 291 
 292     // Check ctrl input to see if the null-check dominates the memory op
 293     Block *cb = get_block_for_node(mach);
 294     cb = cb-&gt;_idom;             // Always hoist at least 1 block
 295     if( !was_store ) {          // Stores can be hoisted only one block
 296       while( cb-&gt;_dom_depth &gt; (block-&gt;_dom_depth + 1))
 297         cb = cb-&gt;_idom;         // Hoist loads as far as we want
 298       // The non-null-block should dominate the memory op, too. Live
 299       // range spilling will insert a spill in the non-null-block if it is
 300       // needs to spill the memory op for an implicit null check.
 301       if (cb-&gt;_dom_depth == (block-&gt;_dom_depth + 1)) {
 302         if (cb != not_null_block) continue;
 303         cb = cb-&gt;_idom;
 304       }
 305     }
 306     if( cb != block ) continue;
 307 
</pre>
<hr />
<pre>
 943         (n-&gt;is_Proj()  &amp;&amp; n-&gt;in(0) == block-&gt;head()) ) {
 944       // Move guy at &#39;phi_cnt&#39; to the end; makes a hole at phi_cnt
 945       block-&gt;map_node(block-&gt;get_node(phi_cnt), i);
 946       block-&gt;map_node(n, phi_cnt++);  // swap Phi/Parm up front
 947       if (OptoRegScheduling &amp;&amp; block_size_threshold_ok) {
 948         // mark n as scheduled
 949         n-&gt;add_flag(Node::Flag_is_scheduled);
 950       }
 951     } else {                    // All others
 952       // Count block-local inputs to &#39;n&#39;
 953       uint cnt = n-&gt;len();      // Input count
 954       uint local = 0;
 955       for( uint j=0; j&lt;cnt; j++ ) {
 956         Node *m = n-&gt;in(j);
 957         if( m &amp;&amp; get_block_for_node(m) == block &amp;&amp; !m-&gt;is_top() )
 958           local++;              // One more block-local input
 959       }
 960       ready_cnt.at_put(n-&gt;_idx, local); // Count em up
 961 
 962 #ifdef ASSERT
<span class="line-modified"> 963       if( UseConcMarkSweepGC || UseG1GC ) {</span>
 964         if( n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;ideal_Opcode() == Op_StoreCM ) {
 965           // Check the precedence edges
 966           for (uint prec = n-&gt;req(); prec &lt; n-&gt;len(); prec++) {
 967             Node* oop_store = n-&gt;in(prec);
 968             if (oop_store != NULL) {
 969               assert(get_block_for_node(oop_store)-&gt;_dom_depth &lt;= block-&gt;_dom_depth, &quot;oop_store must dominate card-mark&quot;);
 970             }
 971           }
 972         }
 973       }
 974 #endif
 975 
 976       // A few node types require changing a required edge to a precedence edge
 977       // before allocation.
 978       if( n-&gt;is_Mach() &amp;&amp; n-&gt;req() &gt; TypeFunc::Parms &amp;&amp;
 979           (n-&gt;as_Mach()-&gt;ideal_Opcode() == Op_MemBarAcquire ||
 980            n-&gt;as_Mach()-&gt;ideal_Opcode() == Op_MemBarVolatile) ) {
 981         // MemBarAcquire could be created without Precedent edge.
 982         // del_req() replaces the specified edge with the last input edge
 983         // and then removes the last edge. If the specified edge &gt; number of
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1998, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.inline.hpp&quot;
  27 #include &quot;memory/allocation.inline.hpp&quot;
<span class="line-added">  28 #include &quot;oops/compressedOops.hpp&quot;</span>
  29 #include &quot;opto/ad.hpp&quot;
  30 #include &quot;opto/block.hpp&quot;
  31 #include &quot;opto/c2compiler.hpp&quot;
  32 #include &quot;opto/callnode.hpp&quot;
  33 #include &quot;opto/cfgnode.hpp&quot;
  34 #include &quot;opto/machnode.hpp&quot;
  35 #include &quot;opto/runtime.hpp&quot;
  36 #include &quot;opto/chaitin.hpp&quot;
  37 #include &quot;runtime/sharedRuntime.hpp&quot;
  38 
  39 // Optimization - Graph Style
  40 
  41 // Check whether val is not-null-decoded compressed oop,
  42 // i.e. will grab into the base of the heap if it represents NULL.
  43 static bool accesses_heap_base_zone(Node *val) {
<span class="line-modified">  44   if (CompressedOops::base() != NULL) { // Implies UseCompressedOops.</span>
  45     if (val &amp;&amp; val-&gt;is_Mach()) {
  46       if (val-&gt;as_Mach()-&gt;ideal_Opcode() == Op_DecodeN) {
  47         // This assumes all Decodes with TypePtr::NotNull are matched to nodes that
  48         // decode NULL to point to the heap base (Decode_NN).
  49         if (val-&gt;bottom_type()-&gt;is_oopptr()-&gt;ptr() == TypePtr::NotNull) {
  50           return true;
  51         }
  52       }
  53       // Must recognize load operation with Decode matched in memory operand.
  54       // We should not reach here exept for PPC/AIX, as os::zero_page_read_protected()
  55       // returns true everywhere else. On PPC, no such memory operands
  56       // exist, therefore we did not yet implement a check for such operands.
  57       NOT_AIX(Unimplemented());
  58     }
  59   }
  60   return false;
  61 }
  62 
  63 static bool needs_explicit_null_check_for_read(Node *val) {
  64   // On some OSes (AIX) the page at address 0 is only write protected.
  65   // If so, only Store operations will trap.
  66   if (os::zero_page_read_protected()) {
  67     return false;  // Implicit null check will work.
  68   }
  69   // Also a read accessing the base of a heap-based compressed heap will trap.
<span class="line-modified">  70   if (accesses_heap_base_zone(val) &amp;&amp;         // Hits the base zone page.</span>
<span class="line-modified">  71       CompressedOops::use_implicit_null_checks()) { // Base zone page is protected.</span>
  72     return false;
  73   }
  74 
  75   return true;
  76 }
  77 
  78 //------------------------------implicit_null_check----------------------------
  79 // Detect implicit-null-check opportunities.  Basically, find NULL checks
  80 // with suitable memory ops nearby.  Use the memory op to do the NULL check.
  81 // I can generate a memory op if there is not one nearby.
  82 // The proj is the control projection for the not-null case.
  83 // The val is the pointer being checked for nullness or
  84 // decodeHeapOop_not_null node if it did not fold into address.
  85 void PhaseCFG::implicit_null_check(Block* block, Node *proj, Node *val, int allowed_reasons) {
  86   // Assume if null check need for 0 offset then always needed
  87   // Intel solaris doesn&#39;t support any null checks yet and no
  88   // mechanism exists (yet) to set the switches at an os_cpu level
  89   if( !ImplicitNullChecks || MacroAssembler::needs_explicit_null_check(0)) return;
  90 
  91   // Make sure the ptr-is-null path appears to be uncommon!
</pre>
<hr />
<pre>
 153 
 154   // Search the successor block for a load or store who&#39;s base value is also
 155   // the tested value.  There may be several.
 156   Node_List *out = new Node_List(Thread::current()-&gt;resource_area());
 157   MachNode *best = NULL;        // Best found so far
 158   for (DUIterator i = val-&gt;outs(); val-&gt;has_out(i); i++) {
 159     Node *m = val-&gt;out(i);
 160     if( !m-&gt;is_Mach() ) continue;
 161     MachNode *mach = m-&gt;as_Mach();
 162     was_store = false;
 163     int iop = mach-&gt;ideal_Opcode();
 164     switch( iop ) {
 165     case Op_LoadB:
 166     case Op_LoadUB:
 167     case Op_LoadUS:
 168     case Op_LoadD:
 169     case Op_LoadF:
 170     case Op_LoadI:
 171     case Op_LoadL:
 172     case Op_LoadP:


 173     case Op_LoadN:
 174     case Op_LoadS:
 175     case Op_LoadKlass:
 176     case Op_LoadNKlass:
 177     case Op_LoadRange:
 178     case Op_LoadD_unaligned:
 179     case Op_LoadL_unaligned:

 180       assert(mach-&gt;in(2) == val, &quot;should be address&quot;);
 181       break;
 182     case Op_StoreB:
 183     case Op_StoreC:
 184     case Op_StoreCM:
 185     case Op_StoreD:
 186     case Op_StoreF:
 187     case Op_StoreI:
 188     case Op_StoreL:
 189     case Op_StoreP:
 190     case Op_StoreN:
 191     case Op_StoreNKlass:
 192       was_store = true;         // Memory op is a store op
 193       // Stores will have their address in slot 2 (memory in slot 1).
 194       // If the value being nul-checked is in another slot, it means we
 195       // are storing the checked value, which does NOT check the value!
 196       if( mach-&gt;in(2) != val ) continue;
 197       break;                    // Found a memory op?
 198     case Op_StrComp:
 199     case Op_StrEquals:
</pre>
<hr />
<pre>
 243     }
 244 
 245     // Check that node&#39;s control edge is not-null block&#39;s head or dominates it,
 246     // otherwise we can&#39;t hoist it because there are other control dependencies.
 247     Node* ctrl = mach-&gt;in(0);
 248     if (ctrl != NULL &amp;&amp; !(ctrl == not_null_block-&gt;head() ||
 249         get_block_for_node(ctrl)-&gt;dominates(not_null_block))) {
 250       continue;
 251     }
 252 
 253     // check if the offset is not too high for implicit exception
 254     {
 255       intptr_t offset = 0;
 256       const TypePtr *adr_type = NULL;  // Do not need this return value here
 257       const Node* base = mach-&gt;get_base_and_disp(offset, adr_type);
 258       if (base == NULL || base == NodeSentinel) {
 259         // Narrow oop address doesn&#39;t have base, only index.
 260         // Give up if offset is beyond page size or if heap base is not protected.
 261         if (val-&gt;bottom_type()-&gt;isa_narrowoop() &amp;&amp;
 262             (MacroAssembler::needs_explicit_null_check(offset) ||
<span class="line-modified"> 263              !CompressedOops::use_implicit_null_checks()))</span>
 264           continue;
 265         // cannot reason about it; is probably not implicit null exception
 266       } else {
 267         const TypePtr* tptr;
<span class="line-modified"> 268         if (UseCompressedOops &amp;&amp; (CompressedOops::shift() == 0 ||</span>
<span class="line-modified"> 269                                   CompressedKlassPointers::shift() == 0)) {</span>
 270           // 32-bits narrow oop can be the base of address expressions
 271           tptr = base-&gt;get_ptr_type();
 272         } else {
 273           // only regular oops are expected here
 274           tptr = base-&gt;bottom_type()-&gt;is_ptr();
 275         }
 276         // Give up if offset is not a compile-time constant.
 277         if (offset == Type::OffsetBot || tptr-&gt;_offset == Type::OffsetBot)
 278           continue;
 279         offset += tptr-&gt;_offset; // correct if base is offseted
 280         // Give up if reference is beyond page size.
 281         if (MacroAssembler::needs_explicit_null_check(offset))
 282           continue;
 283         // Give up if base is a decode node and the heap base is not protected.
 284         if (base-&gt;is_Mach() &amp;&amp; base-&gt;as_Mach()-&gt;ideal_Opcode() == Op_DecodeN &amp;&amp;
<span class="line-modified"> 285             !CompressedOops::use_implicit_null_checks())</span>
 286           continue;
 287       }
 288     }
 289 
 290     // Check ctrl input to see if the null-check dominates the memory op
 291     Block *cb = get_block_for_node(mach);
 292     cb = cb-&gt;_idom;             // Always hoist at least 1 block
 293     if( !was_store ) {          // Stores can be hoisted only one block
 294       while( cb-&gt;_dom_depth &gt; (block-&gt;_dom_depth + 1))
 295         cb = cb-&gt;_idom;         // Hoist loads as far as we want
 296       // The non-null-block should dominate the memory op, too. Live
 297       // range spilling will insert a spill in the non-null-block if it is
 298       // needs to spill the memory op for an implicit null check.
 299       if (cb-&gt;_dom_depth == (block-&gt;_dom_depth + 1)) {
 300         if (cb != not_null_block) continue;
 301         cb = cb-&gt;_idom;
 302       }
 303     }
 304     if( cb != block ) continue;
 305 
</pre>
<hr />
<pre>
 941         (n-&gt;is_Proj()  &amp;&amp; n-&gt;in(0) == block-&gt;head()) ) {
 942       // Move guy at &#39;phi_cnt&#39; to the end; makes a hole at phi_cnt
 943       block-&gt;map_node(block-&gt;get_node(phi_cnt), i);
 944       block-&gt;map_node(n, phi_cnt++);  // swap Phi/Parm up front
 945       if (OptoRegScheduling &amp;&amp; block_size_threshold_ok) {
 946         // mark n as scheduled
 947         n-&gt;add_flag(Node::Flag_is_scheduled);
 948       }
 949     } else {                    // All others
 950       // Count block-local inputs to &#39;n&#39;
 951       uint cnt = n-&gt;len();      // Input count
 952       uint local = 0;
 953       for( uint j=0; j&lt;cnt; j++ ) {
 954         Node *m = n-&gt;in(j);
 955         if( m &amp;&amp; get_block_for_node(m) == block &amp;&amp; !m-&gt;is_top() )
 956           local++;              // One more block-local input
 957       }
 958       ready_cnt.at_put(n-&gt;_idx, local); // Count em up
 959 
 960 #ifdef ASSERT
<span class="line-modified"> 961       if (UseG1GC) {</span>
 962         if( n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;ideal_Opcode() == Op_StoreCM ) {
 963           // Check the precedence edges
 964           for (uint prec = n-&gt;req(); prec &lt; n-&gt;len(); prec++) {
 965             Node* oop_store = n-&gt;in(prec);
 966             if (oop_store != NULL) {
 967               assert(get_block_for_node(oop_store)-&gt;_dom_depth &lt;= block-&gt;_dom_depth, &quot;oop_store must dominate card-mark&quot;);
 968             }
 969           }
 970         }
 971       }
 972 #endif
 973 
 974       // A few node types require changing a required edge to a precedence edge
 975       // before allocation.
 976       if( n-&gt;is_Mach() &amp;&amp; n-&gt;req() &gt; TypeFunc::Parms &amp;&amp;
 977           (n-&gt;as_Mach()-&gt;ideal_Opcode() == Op_MemBarAcquire ||
 978            n-&gt;as_Mach()-&gt;ideal_Opcode() == Op_MemBarVolatile) ) {
 979         // MemBarAcquire could be created without Precedent edge.
 980         // del_req() replaces the specified edge with the last input edge
 981         // and then removes the last edge. If the specified edge &gt; number of
</pre>
</td>
</tr>
</table>
<center><a href="indexSet.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="library_call.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>