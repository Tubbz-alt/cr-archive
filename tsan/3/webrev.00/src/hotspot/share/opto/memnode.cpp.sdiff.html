<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/memnode.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="matcher.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="memnode.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/memnode.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
</pre>
<hr />
<pre>
  27 #include &quot;compiler/compileLog.hpp&quot;
  28 #include &quot;gc/shared/barrierSet.hpp&quot;
  29 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  30 #include &quot;memory/allocation.inline.hpp&quot;
  31 #include &quot;memory/resourceArea.hpp&quot;
  32 #include &quot;oops/objArrayKlass.hpp&quot;
  33 #include &quot;opto/addnode.hpp&quot;
  34 #include &quot;opto/arraycopynode.hpp&quot;
  35 #include &quot;opto/cfgnode.hpp&quot;
  36 #include &quot;opto/compile.hpp&quot;
  37 #include &quot;opto/connode.hpp&quot;
  38 #include &quot;opto/convertnode.hpp&quot;
  39 #include &quot;opto/loopnode.hpp&quot;
  40 #include &quot;opto/machnode.hpp&quot;
  41 #include &quot;opto/matcher.hpp&quot;
  42 #include &quot;opto/memnode.hpp&quot;
  43 #include &quot;opto/mulnode.hpp&quot;
  44 #include &quot;opto/narrowptrnode.hpp&quot;
  45 #include &quot;opto/phaseX.hpp&quot;
  46 #include &quot;opto/regmask.hpp&quot;

  47 #include &quot;utilities/align.hpp&quot;
  48 #include &quot;utilities/copy.hpp&quot;
  49 #include &quot;utilities/macros.hpp&quot;

  50 #include &quot;utilities/vmError.hpp&quot;
<span class="line-removed">  51 #if INCLUDE_ZGC</span>
<span class="line-removed">  52 #include &quot;gc/z/c2/zBarrierSetC2.hpp&quot;</span>
<span class="line-removed">  53 #endif</span>
  54 
  55 // Portions of code courtesy of Clifford Click
  56 
  57 // Optimization - Graph Style
  58 
  59 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st);
  60 
  61 //=============================================================================
  62 uint MemNode::size_of() const { return sizeof(*this); }
  63 
  64 const TypePtr *MemNode::adr_type() const {
  65   Node* adr = in(Address);
  66   if (adr == NULL)  return NULL; // node is dead
  67   const TypePtr* cross_check = NULL;
  68   DEBUG_ONLY(cross_check = _adr_type);
  69   return calculate_adr_type(adr-&gt;bottom_type(), cross_check);
  70 }
  71 
  72 bool MemNode::check_if_adr_maybe_raw(Node* adr) {
  73   if (adr != NULL) {
</pre>
<hr />
<pre>
 192       if (!is_instance || !ClearArrayNode::step_through(&amp;result, instance_id, phase)) {
 193         // Can not bypass initialization of the instance
 194         // we are looking for.
 195         break;
 196       }
 197       // Otherwise skip it (the call updated &#39;result&#39; value).
 198     } else if (result-&gt;is_MergeMem()) {
 199       result = step_through_mergemem(phase, result-&gt;as_MergeMem(), t_oop, NULL, tty);
 200     }
 201   }
 202   return result;
 203 }
 204 
 205 Node *MemNode::optimize_memory_chain(Node *mchain, const TypePtr *t_adr, Node *load, PhaseGVN *phase) {
 206   const TypeOopPtr* t_oop = t_adr-&gt;isa_oopptr();
 207   if (t_oop == NULL)
 208     return mchain;  // don&#39;t try to optimize non-oop types
 209   Node* result = optimize_simple_memory_chain(mchain, t_oop, load, phase);
 210   bool is_instance = t_oop-&gt;is_known_instance_field();
 211   PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
<span class="line-modified"> 212   if (is_instance &amp;&amp; igvn != NULL  &amp;&amp; result-&gt;is_Phi()) {</span>
 213     PhiNode *mphi = result-&gt;as_Phi();
 214     assert(mphi-&gt;bottom_type() == Type::MEMORY, &quot;memory phi required&quot;);
 215     const TypePtr *t = mphi-&gt;adr_type();
 216     if (t == TypePtr::BOTTOM || t == TypeRawPtr::BOTTOM ||
 217         (t-&gt;isa_oopptr() &amp;&amp; !t-&gt;is_oopptr()-&gt;is_known_instance() &amp;&amp;
 218          t-&gt;is_oopptr()-&gt;cast_to_exactness(true)
 219            -&gt;is_oopptr()-&gt;cast_to_ptr_type(t_oop-&gt;ptr())
 220             -&gt;is_oopptr()-&gt;cast_to_instance_id(t_oop-&gt;instance_id()) == t_oop)) {
 221       // clone the Phi with our address type
 222       result = mphi-&gt;split_out_instance(t_adr, igvn);
 223     } else {
 224       assert(phase-&gt;C-&gt;get_alias_index(t) == phase-&gt;C-&gt;get_alias_index(t_adr), &quot;correct memory chain&quot;);
 225     }
 226   }
 227   return result;
 228 }
 229 
 230 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st) {
 231   uint alias_idx = phase-&gt;C-&gt;get_alias_index(tp);
 232   Node *mem = mmem;
</pre>
<hr />
<pre>
 311       phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 312       return NodeSentinel; // caller will return NULL
 313     }
 314   }
 315   // Ignore if memory is dead, or self-loop
 316   Node *mem = in(MemNode::Memory);
 317   if (phase-&gt;type( mem ) == Type::TOP) return NodeSentinel; // caller will return NULL
 318   assert(mem != this, &quot;dead loop in MemNode::Ideal&quot;);
 319 
 320   if (can_reshape &amp;&amp; igvn != NULL &amp;&amp; igvn-&gt;_worklist.member(mem)) {
 321     // This memory slice may be dead.
 322     // Delay this mem node transformation until the memory is processed.
 323     phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 324     return NodeSentinel; // caller will return NULL
 325   }
 326 
 327   Node *address = in(MemNode::Address);
 328   const Type *t_adr = phase-&gt;type(address);
 329   if (t_adr == Type::TOP)              return NodeSentinel; // caller will return NULL
 330 


















 331   if (can_reshape &amp;&amp; igvn != NULL &amp;&amp;
 332       (igvn-&gt;_worklist.member(address) ||
 333        (igvn-&gt;_worklist.size() &gt; 0 &amp;&amp; t_adr != adr_type())) ) {
 334     // The address&#39;s base and type may change when the address is processed.
 335     // Delay this mem node transformation until the address is processed.
 336     phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 337     return NodeSentinel; // caller will return NULL
 338   }
 339 
 340   // Do NOT remove or optimize the next lines: ensure a new alias index
 341   // is allocated for an oop pointer type before Escape Analysis.
 342   // Note: C++ will not remove it since the call has side effect.
 343   if (t_adr-&gt;isa_oopptr()) {
 344     int alias_idx = phase-&gt;C-&gt;get_alias_index(t_adr-&gt;is_ptr());
 345   }
 346 
 347   Node* base = NULL;
 348   if (address-&gt;is_AddP()) {
 349     base = address-&gt;in(AddPNode::Base);
 350   }
</pre>
<hr />
<pre>
 703       } else if (mem-&gt;is_MergeMem()) {
 704         int alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
 705         mem = mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
 706         continue;           // (a) advance through independent MergeMem memory
 707       }
 708     }
 709 
 710     // Unless there is an explicit &#39;continue&#39;, we must bail out here,
 711     // because &#39;mem&#39; is an inscrutable memory state (e.g., a call).
 712     break;
 713   }
 714 
 715   return NULL;              // bail out
 716 }
 717 
 718 //----------------------calculate_adr_type-------------------------------------
 719 // Helper function.  Notices when the given type of address hits top or bottom.
 720 // Also, asserts a cross-check of the type against the expected address type.
 721 const TypePtr* MemNode::calculate_adr_type(const Type* t, const TypePtr* cross_check) {
 722   if (t == Type::TOP)  return NULL; // does not touch memory any more?
<span class="line-modified"> 723   #ifdef PRODUCT</span>
<span class="line-removed"> 724   cross_check = NULL;</span>
<span class="line-removed"> 725   #else</span>
 726   if (!VerifyAliases || VMError::is_error_reported() || Node::in_dump())  cross_check = NULL;
 727   #endif
 728   const TypePtr* tp = t-&gt;isa_ptr();
 729   if (tp == NULL) {
 730     assert(cross_check == NULL || cross_check == TypePtr::BOTTOM, &quot;expected memory type must be wide&quot;);
 731     return TypePtr::BOTTOM;           // touches lots of memory
 732   } else {
 733     #ifdef ASSERT
 734     // %%%% [phh] We don&#39;t check the alias index if cross_check is
 735     //            TypeRawPtr::BOTTOM.  Needs to be investigated.
 736     if (cross_check != NULL &amp;&amp;
 737         cross_check != TypePtr::BOTTOM &amp;&amp;
 738         cross_check != TypeRawPtr::BOTTOM) {
 739       // Recheck the alias index, to see if it has changed (due to a bug).
 740       Compile* C = Compile::current();
 741       assert(C-&gt;get_alias_index(cross_check) == C-&gt;get_alias_index(tp),
 742              &quot;must stay in the original alias category&quot;);
 743       // The type of the address must be contained in the adr_type,
 744       // disregarding &quot;null&quot;-ness.
 745       // (We make an exception for TypeRawPtr::BOTTOM, which is a bit bucket.)
 746       const TypePtr* tp_notnull = tp-&gt;join(TypePtr::NOTNULL)-&gt;is_ptr();
 747       assert(cross_check-&gt;meet(tp_notnull) == cross_check-&gt;remove_speculative(),
 748              &quot;real address must not escape from expected memory type&quot;);
 749     }
 750     #endif
 751     return tp;
 752   }
 753 }
 754 
 755 //=============================================================================
 756 // Should LoadNode::Ideal() attempt to remove control edges?
 757 bool LoadNode::can_remove_control() const {
 758   return true;
 759 }
 760 uint LoadNode::size_of() const { return sizeof(*this); }
<span class="line-modified"> 761 uint LoadNode::cmp( const Node &amp;n ) const</span>
 762 { return !Type::cmp( _type, ((LoadNode&amp;)n)._type ); }
 763 const Type *LoadNode::bottom_type() const { return _type; }
 764 uint LoadNode::ideal_reg() const {
 765   return _type-&gt;ideal_reg();
 766 }
 767 
 768 #ifndef PRODUCT
 769 void LoadNode::dump_spec(outputStream *st) const {
 770   MemNode::dump_spec(st);
 771   if( !Verbose &amp;&amp; !WizardMode ) {
 772     // standard dump does this in Verbose and WizardMode
 773     st-&gt;print(&quot; #&quot;); _type-&gt;dump_on(st);
 774   }
 775   if (!depends_only_on_test()) {
 776     st-&gt;print(&quot; (does not depend only on test)&quot;);
 777   }
 778 }
 779 #endif
 780 
 781 #ifdef ASSERT
 782 //----------------------------is_immutable_value-------------------------------
 783 // Helper function to allow a raw load without control edge for some cases
 784 bool LoadNode::is_immutable_value(Node* adr) {
 785   return (adr-&gt;is_AddP() &amp;&amp; adr-&gt;in(AddPNode::Base)-&gt;is_top() &amp;&amp;
 786           adr-&gt;in(AddPNode::Address)-&gt;Opcode() == Op_ThreadLocal &amp;&amp;
 787           (adr-&gt;in(AddPNode::Offset)-&gt;find_intptr_t_con(-1) ==
 788            in_bytes(JavaThread::osthread_offset())));
 789 }
 790 #endif
 791 
 792 //----------------------------LoadNode::make-----------------------------------
 793 // Polymorphic factory method:
 794 Node *LoadNode::make(PhaseGVN&amp; gvn, Node *ctl, Node *mem, Node *adr, const TypePtr* adr_type, const Type *rt, BasicType bt, MemOrd mo,
<span class="line-modified"> 795                      ControlDependency control_dependency, bool unaligned, bool mismatched, bool unsafe) {</span>
 796   Compile* C = gvn.C;
 797 
 798   // sanity check the alias category against the created node type
 799   assert(!(adr_type-&gt;isa_oopptr() &amp;&amp;
 800            adr_type-&gt;offset() == oopDesc::klass_offset_in_bytes()),
 801          &quot;use LoadKlassNode instead&quot;);
 802   assert(!(adr_type-&gt;isa_aryptr() &amp;&amp;
 803            adr_type-&gt;offset() == arrayOopDesc::length_offset_in_bytes()),
 804          &quot;use LoadRangeNode instead&quot;);
 805   // Check control edge of raw loads
 806   assert( ctl != NULL || C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
 807           // oop will be recorded in oop map if load crosses safepoint
 808           rt-&gt;isa_oopptr() || is_immutable_value(adr),
 809           &quot;raw memory operations should have control edge&quot;);
 810   LoadNode* load = NULL;
 811   switch (bt) {
 812   case T_BOOLEAN: load = new LoadUBNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 813   case T_BYTE:    load = new LoadBNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 814   case T_INT:     load = new LoadINode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 815   case T_CHAR:    load = new LoadUSNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
</pre>
<hr />
<pre>
 826 #endif
 827     {
 828       assert(!adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop() &amp;&amp; !adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass(), &quot;should have got back a narrow oop&quot;);
 829       load = new LoadPNode(ctl, mem, adr, adr_type, rt-&gt;is_ptr(), mo, control_dependency);
 830     }
 831     break;
 832   default:
 833     ShouldNotReachHere();
 834     break;
 835   }
 836   assert(load != NULL, &quot;LoadNode should have been created&quot;);
 837   if (unaligned) {
 838     load-&gt;set_unaligned_access();
 839   }
 840   if (mismatched) {
 841     load-&gt;set_mismatched_access();
 842   }
 843   if (unsafe) {
 844     load-&gt;set_unsafe_access();
 845   }

 846   if (load-&gt;Opcode() == Op_LoadN) {
 847     Node* ld = gvn.transform(load);
 848     return new DecodeNNode(ld, ld-&gt;bottom_type()-&gt;make_ptr());
 849   }
 850 
 851   return load;
 852 }
 853 
 854 LoadLNode* LoadLNode::make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, const Type* rt, MemOrd mo,
<span class="line-modified"> 855                                   ControlDependency control_dependency, bool unaligned, bool mismatched, bool unsafe) {</span>
 856   bool require_atomic = true;
 857   LoadLNode* load = new LoadLNode(ctl, mem, adr, adr_type, rt-&gt;is_long(), mo, control_dependency, require_atomic);
 858   if (unaligned) {
 859     load-&gt;set_unaligned_access();
 860   }
 861   if (mismatched) {
 862     load-&gt;set_mismatched_access();
 863   }
 864   if (unsafe) {
 865     load-&gt;set_unsafe_access();
 866   }

 867   return load;
 868 }
 869 
 870 LoadDNode* LoadDNode::make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, const Type* rt, MemOrd mo,
<span class="line-modified"> 871                                   ControlDependency control_dependency, bool unaligned, bool mismatched, bool unsafe) {</span>
 872   bool require_atomic = true;
 873   LoadDNode* load = new LoadDNode(ctl, mem, adr, adr_type, rt, mo, control_dependency, require_atomic);
 874   if (unaligned) {
 875     load-&gt;set_unaligned_access();
 876   }
 877   if (mismatched) {
 878     load-&gt;set_mismatched_access();
 879   }
 880   if (unsafe) {
 881     load-&gt;set_unsafe_access();
 882   }

 883   return load;
 884 }
 885 
 886 
 887 
 888 //------------------------------hash-------------------------------------------
 889 uint LoadNode::hash() const {
 890   // unroll addition of interesting fields
 891   return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address);
 892 }
 893 
 894 static bool skip_through_membars(Compile::AliasType* atp, const TypeInstPtr* tp, bool eliminate_boxing) {
 895   if ((atp != NULL) &amp;&amp; (atp-&gt;index() &gt;= Compile::AliasIdxRaw)) {
 896     bool non_volatile = (atp-&gt;field() != NULL) &amp;&amp; !atp-&gt;field()-&gt;is_volatile();
 897     bool is_stable_ary = FoldStableValues &amp;&amp;
 898                          (tp != NULL) &amp;&amp; (tp-&gt;isa_aryptr() != NULL) &amp;&amp;
 899                          tp-&gt;isa_aryptr()-&gt;is_stable();
 900 
 901     return (eliminate_boxing &amp;&amp; non_volatile) || is_stable_ary;
 902   }
 903 
 904   return false;
 905 }
 906 
 907 // Is the value loaded previously stored by an arraycopy? If so return
 908 // a load node that reads from the source array so we may be able to
 909 // optimize out the ArrayCopy node later.
 910 Node* LoadNode::can_see_arraycopy_value(Node* st, PhaseGVN* phase) const {
<span class="line-removed"> 911 #if INCLUDE_ZGC</span>
<span class="line-removed"> 912   if (UseZGC) {</span>
<span class="line-removed"> 913     if (bottom_type()-&gt;make_oopptr() != NULL) {</span>
<span class="line-removed"> 914       return NULL;</span>
<span class="line-removed"> 915     }</span>
<span class="line-removed"> 916   }</span>
<span class="line-removed"> 917 #endif</span>
<span class="line-removed"> 918 </span>
 919   Node* ld_adr = in(MemNode::Address);
 920   intptr_t ld_off = 0;
 921   AllocateNode* ld_alloc = AllocateNode::Ideal_allocation(ld_adr, phase, ld_off);
 922   Node* ac = find_previous_arraycopy(phase, ld_alloc, st, true);
 923   if (ac != NULL) {
 924     assert(ac-&gt;is_ArrayCopy(), &quot;what kind of node can this be?&quot;);
 925 
 926     Node* mem = ac-&gt;in(TypeFunc::Memory);
 927     Node* ctl = ac-&gt;in(0);
 928     Node* src = ac-&gt;in(ArrayCopyNode::Src);
 929 
 930     if (!ac-&gt;as_ArrayCopy()-&gt;is_clonebasic() &amp;&amp; !phase-&gt;type(src)-&gt;isa_aryptr()) {
 931       return NULL;
 932     }
 933 
 934     LoadNode* ld = clone()-&gt;as_Load();
 935     Node* addp = in(MemNode::Address)-&gt;clone();
 936     if (ac-&gt;as_ArrayCopy()-&gt;is_clonebasic()) {
 937       assert(ld_alloc != NULL, &quot;need an alloc&quot;);
 938       assert(addp-&gt;is_AddP(), &quot;address must be addp&quot;);
</pre>
<hr />
<pre>
 956       uint shift  = exact_log2(type2aelembytes(ary_elem));
 957 
 958       Node* diff = phase-&gt;transform(new SubINode(ac-&gt;in(ArrayCopyNode::SrcPos), ac-&gt;in(ArrayCopyNode::DestPos)));
 959 #ifdef _LP64
 960       diff = phase-&gt;transform(new ConvI2LNode(diff));
 961 #endif
 962       diff = phase-&gt;transform(new LShiftXNode(diff, phase-&gt;intcon(shift)));
 963 
 964       Node* offset = phase-&gt;transform(new AddXNode(addp-&gt;in(AddPNode::Offset), diff));
 965       addp-&gt;set_req(AddPNode::Offset, offset);
 966     }
 967     addp = phase-&gt;transform(addp);
 968 #ifdef ASSERT
 969     const TypePtr* adr_type = phase-&gt;type(addp)-&gt;is_ptr();
 970     ld-&gt;_adr_type = adr_type;
 971 #endif
 972     ld-&gt;set_req(MemNode::Address, addp);
 973     ld-&gt;set_req(0, ctl);
 974     ld-&gt;set_req(MemNode::Memory, mem);
 975     // load depends on the tests that validate the arraycopy
<span class="line-modified"> 976     ld-&gt;_control_dependency = Pinned;</span>
 977     return ld;
 978   }
 979   return NULL;
 980 }
 981 
 982 
 983 //---------------------------can_see_stored_value------------------------------
 984 // This routine exists to make sure this set of tests is done the same
 985 // everywhere.  We need to make a coordinated change: first LoadNode::Ideal
 986 // will change the graph shape in a way which makes memory alive twice at the
 987 // same time (uses the Oracle model of aliasing), then some
 988 // LoadXNode::Identity will fold things back to the equivalence-class model
 989 // of aliasing.
 990 Node* MemNode::can_see_stored_value(Node* st, PhaseTransform* phase) const {
 991   Node* ld_adr = in(MemNode::Address);
 992   intptr_t ld_off = 0;
 993   Node* ld_base = AddPNode::Ideal_base_and_offset(ld_adr, phase, ld_off);
 994   Node* ld_alloc = AllocateNode::Ideal_allocation(ld_base, phase);
 995   const TypeInstPtr* tp = phase-&gt;type(ld_adr)-&gt;isa_instptr();
 996   Compile::AliasType* atp = (tp != NULL) ? phase-&gt;C-&gt;alias_type(tp) : NULL;
</pre>
<hr />
<pre>
1030           result = new_st;
1031         }
1032       }
1033       break;
1034     }
1035     if (result != NULL) {
1036       st = result;
1037     }
1038   }
1039 
1040   // Loop around twice in the case Load -&gt; Initialize -&gt; Store.
1041   // (See PhaseIterGVN::add_users_to_worklist, which knows about this case.)
1042   for (int trip = 0; trip &lt;= 1; trip++) {
1043 
1044     if (st-&gt;is_Store()) {
1045       Node* st_adr = st-&gt;in(MemNode::Address);
1046       if (!phase-&gt;eqv(st_adr, ld_adr)) {
1047         // Try harder before giving up. Unify base pointers with casts (e.g., raw/non-raw pointers).
1048         intptr_t st_off = 0;
1049         Node* st_base = AddPNode::Ideal_base_and_offset(st_adr, phase, st_off);
<span class="line-modified">1050         if (ld_base == NULL)                        return NULL;</span>
<span class="line-modified">1051         if (st_base == NULL)                        return NULL;</span>
<span class="line-modified">1052         if (ld_base-&gt;uncast() != st_base-&gt;uncast()) return NULL;</span>
<span class="line-modified">1053         if (ld_off != st_off)                       return NULL;</span>
<span class="line-modified">1054         if (ld_off == Type::OffsetBot)              return NULL;</span>
1055         // Same base, same offset.
1056         // Possible improvement for arrays: check index value instead of absolute offset.
1057 
1058         // At this point we have proven something like this setup:
1059         //   B = &lt;&lt; base &gt;&gt;
1060         //   L =  LoadQ(AddP(Check/CastPP(B), #Off))
1061         //   S = StoreQ(AddP(             B , #Off), V)
1062         // (Actually, we haven&#39;t yet proven the Q&#39;s are the same.)
1063         // In other words, we are loading from a casted version of
1064         // the same pointer-and-offset that we stored to.

1065         // Thus, we are able to replace L by V.
1066       }
1067       // Now prove that we have a LoadQ matched to a StoreQ, for some Q.
1068       if (store_Opcode() != st-&gt;Opcode())
1069         return NULL;
1070       return st-&gt;in(MemNode::ValueIn);
1071     }
1072 
1073     // A load from a freshly-created object always returns zero.
1074     // (This can happen after LoadNode::Ideal resets the load&#39;s memory input
1075     // to find_captured_store, which returned InitializeNode::zero_memory.)
1076     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
1077         (st-&gt;in(0) == ld_alloc) &amp;&amp;
1078         (ld_off &gt;= st-&gt;in(0)-&gt;as_Allocate()-&gt;minimum_header_size())) {
1079       // return a zero value for the load&#39;s basic type
1080       // (This is one of the few places where a generic PhaseTransform
1081       // can create new nodes.  Think of it as lazily manifesting
1082       // virtually pre-existing constants.)
1083       return phase-&gt;zerocon(memory_type());
1084     }
</pre>
<hr />
<pre>
1406           return this; // made change
1407         }
1408       }
1409     }
1410   }
1411   if (base_is_phi) {
1412     if (!stable_phi(base-&gt;as_Phi(), phase)) {
1413       return NULL; // Wait stable graph
1414     }
1415     uint cnt = base-&gt;req();
1416     // Check for loop invariant memory.
1417     if (cnt == 3) {
1418       for (uint i = 1; i &lt; cnt; i++) {
1419         if (base-&gt;in(i) == base) {
1420           return NULL; // Wait stable graph
1421         }
1422       }
1423     }
1424   }
1425 
<span class="line-removed">1426   bool load_boxed_phi = load_boxed_values &amp;&amp; base_is_phi &amp;&amp; (base-&gt;in(0) == mem-&gt;in(0));</span>
<span class="line-removed">1427 </span>
1428   // Split through Phi (see original code in loopopts.cpp).
1429   assert(C-&gt;have_alias_type(t_oop), &quot;instance should have alias type&quot;);
1430 
1431   // Do nothing here if Identity will find a value
1432   // (to avoid infinite chain of value phis generation).
<span class="line-modified">1433   if (!phase-&gt;eqv(this, phase-&gt;apply_identity(this)))</span>
1434     return NULL;

1435 
1436   // Select Region to split through.
1437   Node* region;
1438   if (!base_is_phi) {
1439     assert(mem-&gt;is_Phi(), &quot;sanity&quot;);
1440     region = mem-&gt;in(0);
1441     // Skip if the region dominates some control edge of the address.
1442     if (!MemNode::all_controls_dominate(address, region))
1443       return NULL;
1444   } else if (!mem-&gt;is_Phi()) {
1445     assert(base_is_phi, &quot;sanity&quot;);
1446     region = base-&gt;in(0);
1447     // Skip if the region dominates some control edge of the memory.
1448     if (!MemNode::all_controls_dominate(mem, region))
1449       return NULL;
1450   } else if (base-&gt;in(0) != mem-&gt;in(0)) {
1451     assert(base_is_phi &amp;&amp; mem-&gt;is_Phi(), &quot;sanity&quot;);
1452     if (MemNode::all_controls_dominate(mem, base-&gt;in(0))) {
1453       region = base-&gt;in(0);
1454     } else if (MemNode::all_controls_dominate(address, mem-&gt;in(0))) {
</pre>
<hr />
<pre>
1457       return NULL; // complex graph
1458     }
1459   } else {
1460     assert(base-&gt;in(0) == mem-&gt;in(0), &quot;sanity&quot;);
1461     region = mem-&gt;in(0);
1462   }
1463 
1464   const Type* this_type = this-&gt;bottom_type();
1465   int this_index  = C-&gt;get_alias_index(t_oop);
1466   int this_offset = t_oop-&gt;offset();
1467   int this_iid    = t_oop-&gt;instance_id();
1468   if (!t_oop-&gt;is_known_instance() &amp;&amp; load_boxed_values) {
1469     // Use _idx of address base for boxed values.
1470     this_iid = base-&gt;_idx;
1471   }
1472   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
1473   Node* phi = new PhiNode(region, this_type, NULL, mem-&gt;_idx, this_iid, this_index, this_offset);
1474   for (uint i = 1; i &lt; region-&gt;req(); i++) {
1475     Node* x;
1476     Node* the_clone = NULL;
<span class="line-modified">1477     if (region-&gt;in(i) == C-&gt;top()) {</span>






1478       x = C-&gt;top();      // Dead path?  Use a dead data op
1479     } else {
1480       x = this-&gt;clone();        // Else clone up the data op
1481       the_clone = x;            // Remember for possible deletion.
1482       // Alter data node to use pre-phi inputs
1483       if (this-&gt;in(0) == region) {
<span class="line-modified">1484         x-&gt;set_req(0, region-&gt;in(i));</span>
1485       } else {
1486         x-&gt;set_req(0, NULL);
1487       }
1488       if (mem-&gt;is_Phi() &amp;&amp; (mem-&gt;in(0) == region)) {
1489         x-&gt;set_req(Memory, mem-&gt;in(i)); // Use pre-Phi input for the clone.
1490       }
1491       if (address-&gt;is_Phi() &amp;&amp; address-&gt;in(0) == region) {
1492         x-&gt;set_req(Address, address-&gt;in(i)); // Use pre-Phi input for the clone
1493       }
1494       if (base_is_phi &amp;&amp; (base-&gt;in(0) == region)) {
1495         Node* base_x = base-&gt;in(i); // Clone address for loads from boxed objects.
1496         Node* adr_x = phase-&gt;transform(new AddPNode(base_x,base_x,address-&gt;in(AddPNode::Offset)));
1497         x-&gt;set_req(Address, adr_x);
1498       }
1499     }
1500     // Check for a &#39;win&#39; on some paths
1501     const Type *t = x-&gt;Value(igvn);
1502 
1503     bool singleton = t-&gt;singleton();
1504 
1505     // See comments in PhaseIdealLoop::split_thru_phi().
1506     if (singleton &amp;&amp; t == Type::TOP) {
1507       singleton &amp;= region-&gt;is_Loop() &amp;&amp; (i != LoopNode::EntryControl);
1508     }
1509 
1510     if (singleton) {
1511       x = igvn-&gt;makecon(t);
1512     } else {
1513       // We now call Identity to try to simplify the cloned node.
1514       // Note that some Identity methods call phase-&gt;type(this).
1515       // Make sure that the type array is big enough for
1516       // our new node, even though we may throw the node away.
1517       // (This tweaking with igvn only works because x is a new node.)
1518       igvn-&gt;set_type(x, t);
1519       // If x is a TypeNode, capture any more-precise type permanently into Node
1520       // otherwise it will be not updated during igvn-&gt;transform since
1521       // igvn-&gt;type(x) is set to x-&gt;Value() already.
1522       x-&gt;raise_bottom_type(t);
<span class="line-modified">1523       Node *y = igvn-&gt;apply_identity(x);</span>
1524       if (y != x) {
1525         x = y;
1526       } else {
1527         y = igvn-&gt;hash_find_insert(x);
1528         if (y) {
1529           x = y;
1530         } else {
1531           // Else x is a new node we are keeping
1532           // We do not need register_new_node_with_optimizer
1533           // because set_type has already been called.
1534           igvn-&gt;_worklist.push(x);
1535         }
1536       }
1537     }
1538     if (x != the_clone &amp;&amp; the_clone != NULL) {
1539       igvn-&gt;remove_dead_node(the_clone);
1540     }
1541     phi-&gt;set_req(i, x);
1542   }
1543   // Record Phi
1544   igvn-&gt;register_new_node_with_optimizer(phi);
1545   return phi;
1546 }
1547 
















1548 //------------------------------Ideal------------------------------------------
1549 // If the load is from Field memory and the pointer is non-null, it might be possible to
1550 // zero out the control input.
1551 // If the offset is constant and the base is an object allocation,
1552 // try to hook me up to the exact initializing store.
1553 Node *LoadNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1554   Node* p = MemNode::Ideal_common(phase, can_reshape);
1555   if (p)  return (p == NodeSentinel) ? NULL : p;
1556 
1557   Node* ctrl    = in(MemNode::Control);
1558   Node* address = in(MemNode::Address);
1559   bool progress = false;
1560 
1561   bool addr_mark = ((phase-&gt;type(address)-&gt;isa_oopptr() || phase-&gt;type(address)-&gt;isa_narrowoop()) &amp;&amp;
1562          phase-&gt;type(address)-&gt;is_ptr()-&gt;offset() == oopDesc::mark_offset_in_bytes());
1563 
1564   // Skip up past a SafePoint control.  Cannot do this for Stores because
1565   // pointer stores &amp; cardmarks must stay on the same side of a SafePoint.
1566   if( ctrl != NULL &amp;&amp; ctrl-&gt;Opcode() == Op_SafePoint &amp;&amp;
1567       phase-&gt;C-&gt;get_alias_index(phase-&gt;type(address)-&gt;is_ptr()) != Compile::AliasIdxRaw  &amp;&amp;
<span class="line-modified">1568       !addr_mark ) {</span>

1569     ctrl = ctrl-&gt;in(0);
1570     set_req(MemNode::Control,ctrl);
1571     progress = true;
1572   }
1573 
1574   intptr_t ignore = 0;
1575   Node*    base   = AddPNode::Ideal_base_and_offset(address, phase, ignore);
1576   if (base != NULL
1577       &amp;&amp; phase-&gt;C-&gt;get_alias_index(phase-&gt;type(address)-&gt;is_ptr()) != Compile::AliasIdxRaw) {
1578     // Check for useless control edge in some common special cases
1579     if (in(MemNode::Control) != NULL
1580         &amp;&amp; can_remove_control()
1581         &amp;&amp; phase-&gt;type(base)-&gt;higher_equal(TypePtr::NOTNULL)
1582         &amp;&amp; all_controls_dominate(base, phase-&gt;C-&gt;start())) {
1583       // A method-invariant, non-null address (constant or &#39;this&#39; argument).
1584       set_req(MemNode::Control, NULL);
1585       progress = true;
1586     }
1587   }
1588 
</pre>
<hr />
<pre>
1655   // fold up, do so.
1656   Node* prev_mem = find_previous_store(phase);
1657   if (prev_mem != NULL) {
1658     Node* value = can_see_arraycopy_value(prev_mem, phase);
1659     if (value != NULL) {
1660       return value;
1661     }
1662   }
1663   // Steps (a), (b):  Walk past independent stores to find an exact match.
1664   if (prev_mem != NULL &amp;&amp; prev_mem != in(MemNode::Memory)) {
1665     // (c) See if we can fold up on the spot, but don&#39;t fold up here.
1666     // Fold-up might require truncation (for LoadB/LoadS/LoadUS) or
1667     // just return a prior value, which is done by Identity calls.
1668     if (can_see_stored_value(prev_mem, phase)) {
1669       // Make ready for step (d):
1670       set_req(MemNode::Memory, prev_mem);
1671       return this;
1672     }
1673   }
1674 







1675   return progress ? this : NULL;
1676 }
1677 
1678 // Helper to recognize certain Klass fields which are invariant across
1679 // some group of array types (e.g., int[] or all T[] where T &lt; Object).
1680 const Type*
1681 LoadNode::load_array_final_field(const TypeKlassPtr *tkls,
1682                                  ciKlass* klass) const {
1683   if (tkls-&gt;offset() == in_bytes(Klass::modifier_flags_offset())) {
1684     // The field is Klass::_modifier_flags.  Return its (constant) value.
1685     // (Folds up the 2nd indirection in aClassConstant.getModifiers().)
1686     assert(this-&gt;Opcode() == Op_LoadI, &quot;must load an int from _modifier_flags&quot;);
1687     return TypeInt::make(klass-&gt;modifier_flags());
1688   }
1689   if (tkls-&gt;offset() == in_bytes(Klass::access_flags_offset())) {
1690     // The field is Klass::_access_flags.  Return its (constant) value.
1691     // (Folds up the 2nd indirection in Reflection.getClassAccessFlags(aClassConstant).)
1692     assert(this-&gt;Opcode() == Op_LoadI, &quot;must load an int from _access_flags&quot;);
1693     return TypeInt::make(klass-&gt;access_flags());
1694   }
</pre>
<hr />
<pre>
1913   bool is_instance = (tinst != NULL) &amp;&amp; tinst-&gt;is_known_instance_field();
1914   bool is_boxed_value = (tinst != NULL) &amp;&amp; tinst-&gt;is_ptr_to_boxed_value();
1915   if (ReduceFieldZeroing || is_instance || is_boxed_value) {
1916     Node* value = can_see_stored_value(mem,phase);
1917     if (value != NULL &amp;&amp; value-&gt;is_Con()) {
1918       assert(value-&gt;bottom_type()-&gt;higher_equal(_type),&quot;sanity&quot;);
1919       return value-&gt;bottom_type();
1920     }
1921   }
1922 
1923   if (is_instance) {
1924     // If we have an instance type and our memory input is the
1925     // programs&#39;s initial memory state, there is no matching store,
1926     // so just return a zero of the appropriate type
1927     Node *mem = in(MemNode::Memory);
1928     if (mem-&gt;is_Parm() &amp;&amp; mem-&gt;in(0)-&gt;is_Start()) {
1929       assert(mem-&gt;as_Parm()-&gt;_con == TypeFunc::Memory, &quot;must be memory Parm&quot;);
1930       return Type::get_zero_type(_type-&gt;basic_type());
1931     }
1932   }






1933   return _type;
1934 }
1935 
1936 //------------------------------match_edge-------------------------------------
1937 // Do we Match on this edge index or not?  Match only the address.
1938 uint LoadNode::match_edge(uint idx) const {
1939   return idx == MemNode::Address;
1940 }
1941 
1942 //--------------------------LoadBNode::Ideal--------------------------------------
1943 //
1944 //  If the previous store is to the same address as this load,
1945 //  and the value stored was larger than a byte, replace this load
1946 //  with the value stored truncated to a byte.  If no truncation is
1947 //  needed, the replacement is done in LoadNode::Identity().
1948 //
1949 Node *LoadBNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1950   Node* mem = in(MemNode::Memory);
1951   Node* value = can_see_stored_value(mem,phase);
1952   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) ) {
</pre>
<hr />
<pre>
2610       const TypeOopPtr* t_oop = phase-&gt;type(in(Address))-&gt;isa_oopptr();
2611       assert(t_oop == NULL || t_oop-&gt;is_known_instance_field(), &quot;only for non escaping objects&quot;);
2612 #endif
2613       PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
2614       trailing-&gt;remove(igvn);
2615     }
2616   }
2617 
2618   return result;
2619 }
2620 
2621 //------------------------------match_edge-------------------------------------
2622 // Do we Match on this edge index or not?  Match only memory &amp; value
2623 uint StoreNode::match_edge(uint idx) const {
2624   return idx == MemNode::Address || idx == MemNode::ValueIn;
2625 }
2626 
2627 //------------------------------cmp--------------------------------------------
2628 // Do not common stores up together.  They generally have to be split
2629 // back up anyways, so do not bother.
<span class="line-modified">2630 uint StoreNode::cmp( const Node &amp;n ) const {</span>
2631   return (&amp;n == this);          // Always fail except on self
2632 }
2633 
2634 //------------------------------Ideal_masked_input-----------------------------
2635 // Check for a useless mask before a partial-word store
2636 // (StoreB ... (AndI valIn conIa) )
2637 // If (conIa &amp; mask == mask) this simplifies to
2638 // (StoreB ... (valIn) )
2639 Node *StoreNode::Ideal_masked_input(PhaseGVN *phase, uint mask) {
2640   Node *val = in(MemNode::ValueIn);
2641   if( val-&gt;Opcode() == Op_AndI ) {
2642     const TypeInt *t = phase-&gt;type( val-&gt;in(2) )-&gt;isa_int();
2643     if( t &amp;&amp; t-&gt;is_con() &amp;&amp; (t-&gt;get_con() &amp; mask) == mask ) {
2644       set_req(MemNode::ValueIn, val-&gt;in(1));
2645       return this;
2646     }
2647   }
2648   return NULL;
2649 }
2650 
</pre>
<hr />
<pre>
2793   // If extra input is TOP ==&gt; the result is TOP
2794   t = phase-&gt;type( in(MemNode::OopStore) );
2795   if( t == Type::TOP ) return Type::TOP;
2796 
2797   return StoreNode::Value( phase );
2798 }
2799 
2800 
2801 //=============================================================================
2802 //----------------------------------SCMemProjNode------------------------------
2803 const Type* SCMemProjNode::Value(PhaseGVN* phase) const
2804 {
2805   return bottom_type();
2806 }
2807 
2808 //=============================================================================
2809 //----------------------------------LoadStoreNode------------------------------
2810 LoadStoreNode::LoadStoreNode( Node *c, Node *mem, Node *adr, Node *val, const TypePtr* at, const Type* rt, uint required )
2811   : Node(required),
2812     _type(rt),
<span class="line-modified">2813     _adr_type(at)</span>

2814 {
2815   init_req(MemNode::Control, c  );
2816   init_req(MemNode::Memory , mem);
2817   init_req(MemNode::Address, adr);
2818   init_req(MemNode::ValueIn, val);
2819   init_class_id(Class_LoadStore);
2820 }
2821 
2822 uint LoadStoreNode::ideal_reg() const {
2823   return _type-&gt;ideal_reg();
2824 }
2825 
2826 bool LoadStoreNode::result_not_used() const {
2827   for( DUIterator_Fast imax, i = fast_outs(imax); i &lt; imax; i++ ) {
2828     Node *x = fast_out(i);
2829     if (x-&gt;Opcode() == Op_SCMemProj) continue;
2830     return false;
2831   }
2832   return true;
2833 }
</pre>
<hr />
<pre>
3040 
3041 //=============================================================================
3042 MemBarNode::MemBarNode(Compile* C, int alias_idx, Node* precedent)
3043   : MultiNode(TypeFunc::Parms + (precedent == NULL? 0: 1)),
3044     _adr_type(C-&gt;get_adr_type(alias_idx)), _kind(Standalone)
3045 #ifdef ASSERT
3046   , _pair_idx(0)
3047 #endif
3048 {
3049   init_class_id(Class_MemBar);
3050   Node* top = C-&gt;top();
3051   init_req(TypeFunc::I_O,top);
3052   init_req(TypeFunc::FramePtr,top);
3053   init_req(TypeFunc::ReturnAdr,top);
3054   if (precedent != NULL)
3055     init_req(TypeFunc::Parms, precedent);
3056 }
3057 
3058 //------------------------------cmp--------------------------------------------
3059 uint MemBarNode::hash() const { return NO_HASH; }
<span class="line-modified">3060 uint MemBarNode::cmp( const Node &amp;n ) const {</span>
3061   return (&amp;n == this);          // Always fail except on self
3062 }
3063 
3064 //------------------------------make-------------------------------------------
3065 MemBarNode* MemBarNode::make(Compile* C, int opcode, int atp, Node* pn) {
3066   switch (opcode) {
3067   case Op_MemBarAcquire:     return new MemBarAcquireNode(C, atp, pn);
3068   case Op_LoadFence:         return new LoadFenceNode(C, atp, pn);
3069   case Op_MemBarRelease:     return new MemBarReleaseNode(C, atp, pn);
3070   case Op_StoreFence:        return new StoreFenceNode(C, atp, pn);
3071   case Op_MemBarAcquireLock: return new MemBarAcquireLockNode(C, atp, pn);
3072   case Op_MemBarReleaseLock: return new MemBarReleaseLockNode(C, atp, pn);
3073   case Op_MemBarVolatile:    return new MemBarVolatileNode(C, atp, pn);
3074   case Op_MemBarCPUOrder:    return new MemBarCPUOrderNode(C, atp, pn);
3075   case Op_OnSpinWait:        return new OnSpinWaitNode(C, atp, pn);
3076   case Op_Initialize:        return new InitializeNode(C, atp, pn);
3077   case Op_MemBarStoreStore:  return new MemBarStoreStoreNode(C, atp, pn);
3078   default: ShouldNotReachHere(); return NULL;
3079   }
3080 }
</pre>
<hr />
<pre>
3087     MemBarNode* leading = leading_membar();
3088     if (leading != NULL) {
3089       assert(leading-&gt;trailing_membar() == this, &quot;inconsistent leading/trailing membars&quot;);
3090       leading-&gt;remove(igvn);
3091     }
3092   }
3093   igvn-&gt;replace_node(proj_out(TypeFunc::Memory), in(TypeFunc::Memory));
3094   igvn-&gt;replace_node(proj_out(TypeFunc::Control), in(TypeFunc::Control));
3095 }
3096 
3097 //------------------------------Ideal------------------------------------------
3098 // Return a node which is more &quot;ideal&quot; than the current node.  Strip out
3099 // control copies
3100 Node *MemBarNode::Ideal(PhaseGVN *phase, bool can_reshape) {
3101   if (remove_dead_region(phase, can_reshape)) return this;
3102   // Don&#39;t bother trying to transform a dead node
3103   if (in(0) &amp;&amp; in(0)-&gt;is_top()) {
3104     return NULL;
3105   }
3106 
<span class="line-removed">3107 #if INCLUDE_ZGC</span>
<span class="line-removed">3108   if (UseZGC) {</span>
<span class="line-removed">3109     if (req() == (Precedent+1) &amp;&amp; in(MemBarNode::Precedent)-&gt;in(0) != NULL &amp;&amp; in(MemBarNode::Precedent)-&gt;in(0)-&gt;is_LoadBarrier()) {</span>
<span class="line-removed">3110       Node* load_node = in(MemBarNode::Precedent)-&gt;in(0)-&gt;in(LoadBarrierNode::Oop);</span>
<span class="line-removed">3111       set_req(MemBarNode::Precedent, load_node);</span>
<span class="line-removed">3112       return this;</span>
<span class="line-removed">3113     }</span>
<span class="line-removed">3114   }</span>
<span class="line-removed">3115 #endif</span>
<span class="line-removed">3116 </span>
3117   bool progress = false;
3118   // Eliminate volatile MemBars for scalar replaced objects.
3119   if (can_reshape &amp;&amp; req() == (Precedent+1)) {
3120     bool eliminate = false;
3121     int opc = Opcode();
3122     if ((opc == Op_MemBarAcquire || opc == Op_MemBarVolatile)) {
3123       // Volatile field loads and stores.
3124       Node* my_mem = in(MemBarNode::Precedent);
3125       // The MembarAquire may keep an unused LoadNode alive through the Precedent edge
3126       if ((my_mem != NULL) &amp;&amp; (opc == Op_MemBarAcquire) &amp;&amp; (my_mem-&gt;outcnt() == 1)) {
3127         // if the Precedent is a decodeN and its input (a Load) is used at more than one place,
3128         // replace this Precedent (decodeN) with the Load instead.
3129         if ((my_mem-&gt;Opcode() == Op_DecodeN) &amp;&amp; (my_mem-&gt;in(1)-&gt;outcnt() &gt; 1))  {
3130           Node* load_node = my_mem-&gt;in(1);
3131           set_req(MemBarNode::Precedent, load_node);
3132           phase-&gt;is_IterGVN()-&gt;_worklist.push(my_mem);
3133           my_mem = load_node;
3134         } else {
3135           assert(my_mem-&gt;unique_out() == this, &quot;sanity&quot;);
3136           del_req(Precedent);
</pre>
<hr />
<pre>
3489   }
3490 }
3491 
3492 // Helper for remembering which stores go with which offsets.
3493 intptr_t InitializeNode::get_store_offset(Node* st, PhaseTransform* phase) {
3494   if (!st-&gt;is_Store())  return -1;  // can happen to dead code via subsume_node
3495   intptr_t offset = -1;
3496   Node* base = AddPNode::Ideal_base_and_offset(st-&gt;in(MemNode::Address),
3497                                                phase, offset);
3498   if (base == NULL)     return -1;  // something is dead,
3499   if (offset &lt; 0)       return -1;  //        dead, dead
3500   return offset;
3501 }
3502 
3503 // Helper for proving that an initialization expression is
3504 // &quot;simple enough&quot; to be folded into an object initialization.
3505 // Attempts to prove that a store&#39;s initial value &#39;n&#39; can be captured
3506 // within the initialization without creating a vicious cycle, such as:
3507 //     { Foo p = new Foo(); p.next = p; }
3508 // True for constants and parameters and small combinations thereof.
<span class="line-modified">3509 bool InitializeNode::detect_init_independence(Node* n, int&amp; count) {</span>
<span class="line-modified">3510   if (n == NULL)      return true;   // (can this really happen?)</span>
<span class="line-modified">3511   if (n-&gt;is_Proj())   n = n-&gt;in(0);</span>
<span class="line-modified">3512   if (n == this)      return false;  // found a cycle</span>
<span class="line-modified">3513   if (n-&gt;is_Con())    return true;</span>
<span class="line-modified">3514   if (n-&gt;is_Start())  return true;   // params, etc., are OK</span>
<span class="line-modified">3515   if (n-&gt;is_Root())   return true;   // even better</span>
<span class="line-modified">3516 </span>
<span class="line-modified">3517   Node* ctl = n-&gt;in(0);</span>
<span class="line-modified">3518   if (ctl != NULL &amp;&amp; !ctl-&gt;is_top()) {</span>
<span class="line-modified">3519     if (ctl-&gt;is_Proj())  ctl = ctl-&gt;in(0);</span>
<span class="line-modified">3520     if (ctl == this)  return false;</span>
<span class="line-modified">3521 </span>
<span class="line-modified">3522     // If we already know that the enclosing memory op is pinned right after</span>
<span class="line-modified">3523     // the init, then any control flow that the store has picked up</span>
<span class="line-modified">3524     // must have preceded the init, or else be equal to the init.</span>
<span class="line-modified">3525     // Even after loop optimizations (which might change control edges)</span>
<span class="line-modified">3526     // a store is never pinned *before* the availability of its inputs.</span>
<span class="line-modified">3527     if (!MemNode::all_controls_dominate(n, this))</span>
<span class="line-modified">3528       return false;                  // failed to prove a good control</span>
<span class="line-modified">3529   }</span>
<span class="line-modified">3530 </span>
<span class="line-modified">3531   // Check data edges for possible dependencies on &#39;this&#39;.</span>
<span class="line-modified">3532   if ((count += 1) &gt; 20)  return false;  // complexity limit</span>
<span class="line-modified">3533   for (uint i = 1; i &lt; n-&gt;req(); i++) {</span>
<span class="line-modified">3534     Node* m = n-&gt;in(i);</span>
<span class="line-modified">3535     if (m == NULL || m == n || m-&gt;is_top())  continue;</span>
<span class="line-modified">3536     uint first_i = n-&gt;find_edge(m);</span>
<span class="line-modified">3537     if (i != first_i)  continue;  // process duplicate edge just once</span>
<span class="line-modified">3538     if (!detect_init_independence(m, count)) {</span>
<span class="line-modified">3539       return false;</span>














3540     }
3541   }
3542 
3543   return true;
3544 }
3545 
3546 // Here are all the checks a Store must pass before it can be moved into
3547 // an initialization.  Returns zero if a check fails.
3548 // On success, returns the (constant) offset to which the store applies,
3549 // within the initialized memory.
<span class="line-modified">3550 intptr_t InitializeNode::can_capture_store(StoreNode* st, PhaseTransform* phase, bool can_reshape) {</span>
3551   const int FAIL = 0;
<span class="line-removed">3552   if (st-&gt;is_unaligned_access()) {</span>
<span class="line-removed">3553     return FAIL;</span>
<span class="line-removed">3554   }</span>
3555   if (st-&gt;req() != MemNode::ValueIn + 1)
3556     return FAIL;                // an inscrutable StoreNode (card mark?)
3557   Node* ctl = st-&gt;in(MemNode::Control);
3558   if (!(ctl != NULL &amp;&amp; ctl-&gt;is_Proj() &amp;&amp; ctl-&gt;in(0) == this))
3559     return FAIL;                // must be unconditional after the initialization
3560   Node* mem = st-&gt;in(MemNode::Memory);
3561   if (!(mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0) == this))
3562     return FAIL;                // must not be preceded by other stores
3563   Node* adr = st-&gt;in(MemNode::Address);
3564   intptr_t offset;
3565   AllocateNode* alloc = AllocateNode::Ideal_allocation(adr, phase, offset);
3566   if (alloc == NULL)
3567     return FAIL;                // inscrutable address
3568   if (alloc != allocation())
3569     return FAIL;                // wrong allocation!  (store needs to float up)




3570   Node* val = st-&gt;in(MemNode::ValueIn);
<span class="line-modified">3571   int complexity_count = 0;</span>
<span class="line-modified">3572   if (!detect_init_independence(val, complexity_count))</span>
3573     return FAIL;                // stored value must be &#39;simple enough&#39;
3574 
3575   // The Store can be captured only if nothing after the allocation
3576   // and before the Store is using the memory location that the store
3577   // overwrites.
3578   bool failed = false;
3579   // If is_complete_with_arraycopy() is true the shape of the graph is
3580   // well defined and is safe so no need for extra checks.
3581   if (!is_complete_with_arraycopy()) {
3582     // We are going to look at each use of the memory state following
3583     // the allocation to make sure nothing reads the memory that the
3584     // Store writes.
3585     const TypePtr* t_adr = phase-&gt;type(adr)-&gt;isa_ptr();
3586     int alias_idx = phase-&gt;C-&gt;get_alias_index(t_adr);
3587     ResourceMark rm;
3588     Unique_Node_List mems;
3589     mems.push(mem);
3590     Node* unique_merge = NULL;
3591     for (uint next = 0; next &lt; mems.size(); ++next) {
3592       Node *m  = mems.at(next);
</pre>
<hr />
<pre>
3750 
3751 // Clone the given store, converting it into a raw store
3752 // initializing a field or element of my new object.
3753 // Caller is responsible for retiring the original store,
3754 // with subsume_node or the like.
3755 //
3756 // From the example above InitializeNode::InitializeNode,
3757 // here are the old stores to be captured:
3758 //   store1 = (StoreC init.Control init.Memory (+ oop 12) 1)
3759 //   store2 = (StoreC init.Control store1      (+ oop 14) 2)
3760 //
3761 // Here is the changed code; note the extra edges on init:
3762 //   alloc = (Allocate ...)
3763 //   rawoop = alloc.RawAddress
3764 //   rawstore1 = (StoreC alloc.Control alloc.Memory (+ rawoop 12) 1)
3765 //   rawstore2 = (StoreC alloc.Control alloc.Memory (+ rawoop 14) 2)
3766 //   init = (Initialize alloc.Control alloc.Memory rawoop
3767 //                      rawstore1 rawstore2)
3768 //
3769 Node* InitializeNode::capture_store(StoreNode* st, intptr_t start,
<span class="line-modified">3770                                     PhaseTransform* phase, bool can_reshape) {</span>
3771   assert(stores_are_sane(phase), &quot;&quot;);
3772 
3773   if (start &lt; 0)  return NULL;
3774   assert(can_capture_store(st, phase, can_reshape) == start, &quot;sanity&quot;);
3775 
3776   Compile* C = phase-&gt;C;
3777   int size_in_bytes = st-&gt;memory_size();
3778   int i = captured_store_insertion_point(start, size_in_bytes, phase);
3779   if (i == 0)  return NULL;     // bail out
3780   Node* prev_mem = NULL;        // raw memory for the captured store
3781   if (i &gt; 0) {
3782     prev_mem = in(i);           // there is a pre-existing store under this one
3783     set_req(i, C-&gt;top());       // temporarily disconnect it
3784     // See StoreNode::Ideal &#39;st-&gt;outcnt() == 1&#39; for the reason to disconnect.
3785   } else {
3786     i = -i;                     // no pre-existing store
3787     prev_mem = zero_memory();   // a slice of the newly allocated object
3788     if (i &gt; InitializeNode::RawStores &amp;&amp; in(i-1) == prev_mem)
3789       set_req(--i, C-&gt;top());   // reuse this edge; it has been folded away
3790     else
</pre>
<hr />
<pre>
4108       // We passed the current int, without fully initializing it.
4109       int_map_off = next_int_off;
4110       int_map &gt;&gt;= BytesPerInt;
4111     } else if (next_int_off &gt; this_int_off + BytesPerInt) {
4112       // We passed the current and next int.
4113       return this_int_off + BytesPerInt;
4114     }
4115   }
4116 
4117   return -1;
4118 }
4119 
4120 
4121 // Called when the associated AllocateNode is expanded into CFG.
4122 // At this point, we may perform additional optimizations.
4123 // Linearize the stores by ascending offset, to make memory
4124 // activity as coherent as possible.
4125 Node* InitializeNode::complete_stores(Node* rawctl, Node* rawmem, Node* rawptr,
4126                                       intptr_t header_size,
4127                                       Node* size_in_bytes,
<span class="line-modified">4128                                       PhaseGVN* phase) {</span>
4129   assert(!is_complete(), &quot;not already complete&quot;);
4130   assert(stores_are_sane(phase), &quot;&quot;);
4131   assert(allocation() != NULL, &quot;must be present&quot;);
4132 
4133   remove_extra_zeroes();
4134 
4135   if (ReduceFieldZeroing || ReduceBulkZeroing)
4136     // reduce instruction count for common initialization patterns
4137     coalesce_subword_stores(header_size, size_in_bytes, phase);
4138 
4139   Node* zmem = zero_memory();   // initially zero memory state
4140   Node* inits = zmem;           // accumulating a linearized chain of inits
4141   #ifdef ASSERT
4142   intptr_t first_offset = allocation()-&gt;minimum_header_size();
4143   intptr_t last_init_off = first_offset;  // previous init offset
4144   intptr_t last_init_end = first_offset;  // previous init offset+size
4145   intptr_t last_tile_end = first_offset;  // previous tile offset+size
4146   #endif
4147   intptr_t zeroes_done = header_size;
4148 
</pre>
<hr />
<pre>
4200           assert(next_full_store &gt;= zeroes_needed, &quot;must go forward&quot;);
4201           assert((next_full_store &amp; (BytesPerInt-1)) == 0, &quot;even boundary&quot;);
4202           zeroes_needed = next_full_store;
4203         }
4204       }
4205 
4206       if (zeroes_needed &gt; zeroes_done) {
4207         intptr_t zsize = zeroes_needed - zeroes_done;
4208         // Do some incremental zeroing on rawmem, in parallel with inits.
4209         zeroes_done = align_down(zeroes_done, BytesPerInt);
4210         rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
4211                                               zeroes_done, zeroes_needed,
4212                                               phase);
4213         zeroes_done = zeroes_needed;
4214         if (zsize &gt; InitArrayShortSize &amp;&amp; ++big_init_gaps &gt; 2)
4215           do_zeroing = false;   // leave the hole, next time
4216       }
4217     }
4218 
4219     // Collect the store and move on:
<span class="line-modified">4220     st-&gt;set_req(MemNode::Memory, inits);</span>
4221     inits = st;                 // put it on the linearized chain
4222     set_req(i, zmem);           // unhook from previous position
4223 
4224     if (zeroes_done == st_off)
4225       zeroes_done = next_init_off;
4226 
4227     assert(!do_zeroing || zeroes_done &gt;= next_init_off, &quot;don&#39;t miss any&quot;);
4228 
4229     #ifdef ASSERT
4230     // Various order invariants.  Weaker than stores_are_sane because
4231     // a large constant tile can be filled in by smaller non-constant stores.
4232     assert(st_off &gt;= last_init_off, &quot;inits do not reverse&quot;);
4233     last_init_off = st_off;
4234     const Type* val = NULL;
4235     if (st_size &gt;= BytesPerInt &amp;&amp;
4236         (val = phase-&gt;type(st-&gt;in(MemNode::ValueIn)))-&gt;singleton() &amp;&amp;
4237         (int)val-&gt;basic_type() &lt; (int)T_OBJECT) {
4238       assert(st_off &gt;= last_tile_end, &quot;tiles do not overlap&quot;);
4239       assert(st_off &gt;= last_init_end, &quot;tiles do not overwrite inits&quot;);
4240       last_tile_end = MAX2(last_tile_end, next_init_off);
</pre>
<hr />
<pre>
4421   if( new_base != NULL &amp;&amp; new_base-&gt;is_MergeMem() ) {
4422     MergeMemNode* mdef = new_base-&gt;as_MergeMem();
4423     assert(mdef-&gt;empty_memory() == empty_mem, &quot;consistent sentinels&quot;);
4424     for (MergeMemStream mms(this, mdef); mms.next_non_empty2(); ) {
4425       mms.set_memory(mms.memory2());
4426     }
4427     assert(base_memory() == mdef-&gt;base_memory(), &quot;&quot;);
4428   } else {
4429     set_base_memory(new_base);
4430   }
4431 }
4432 
4433 // Make a new, untransformed MergeMem with the same base as &#39;mem&#39;.
4434 // If mem is itself a MergeMem, populate the result with the same edges.
4435 MergeMemNode* MergeMemNode::make(Node* mem) {
4436   return new MergeMemNode(mem);
4437 }
4438 
4439 //------------------------------cmp--------------------------------------------
4440 uint MergeMemNode::hash() const { return NO_HASH; }
<span class="line-modified">4441 uint MergeMemNode::cmp( const Node &amp;n ) const {</span>
4442   return (&amp;n == this);          // Always fail except on self
4443 }
4444 
4445 //------------------------------Identity---------------------------------------
4446 Node* MergeMemNode::Identity(PhaseGVN* phase) {
4447   // Identity if this merge point does not record any interesting memory
4448   // disambiguations.
4449   Node* base_mem = base_memory();
4450   Node* empty_mem = empty_memory();
4451   if (base_mem != empty_mem) {  // Memory path is not dead?
4452     for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4453       Node* mem = in(i);
4454       if (mem != empty_mem &amp;&amp; mem != base_mem) {
4455         return this;            // Many memory splits; no change
4456       }
4457     }
4458   }
4459   return base_mem;              // No memory splits; ID on the one true input
4460 }
4461 
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
</pre>
<hr />
<pre>
  27 #include &quot;compiler/compileLog.hpp&quot;
  28 #include &quot;gc/shared/barrierSet.hpp&quot;
  29 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  30 #include &quot;memory/allocation.inline.hpp&quot;
  31 #include &quot;memory/resourceArea.hpp&quot;
  32 #include &quot;oops/objArrayKlass.hpp&quot;
  33 #include &quot;opto/addnode.hpp&quot;
  34 #include &quot;opto/arraycopynode.hpp&quot;
  35 #include &quot;opto/cfgnode.hpp&quot;
  36 #include &quot;opto/compile.hpp&quot;
  37 #include &quot;opto/connode.hpp&quot;
  38 #include &quot;opto/convertnode.hpp&quot;
  39 #include &quot;opto/loopnode.hpp&quot;
  40 #include &quot;opto/machnode.hpp&quot;
  41 #include &quot;opto/matcher.hpp&quot;
  42 #include &quot;opto/memnode.hpp&quot;
  43 #include &quot;opto/mulnode.hpp&quot;
  44 #include &quot;opto/narrowptrnode.hpp&quot;
  45 #include &quot;opto/phaseX.hpp&quot;
  46 #include &quot;opto/regmask.hpp&quot;
<span class="line-added">  47 #include &quot;opto/rootnode.hpp&quot;</span>
  48 #include &quot;utilities/align.hpp&quot;
  49 #include &quot;utilities/copy.hpp&quot;
  50 #include &quot;utilities/macros.hpp&quot;
<span class="line-added">  51 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  52 #include &quot;utilities/vmError.hpp&quot;



  53 
  54 // Portions of code courtesy of Clifford Click
  55 
  56 // Optimization - Graph Style
  57 
  58 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st);
  59 
  60 //=============================================================================
  61 uint MemNode::size_of() const { return sizeof(*this); }
  62 
  63 const TypePtr *MemNode::adr_type() const {
  64   Node* adr = in(Address);
  65   if (adr == NULL)  return NULL; // node is dead
  66   const TypePtr* cross_check = NULL;
  67   DEBUG_ONLY(cross_check = _adr_type);
  68   return calculate_adr_type(adr-&gt;bottom_type(), cross_check);
  69 }
  70 
  71 bool MemNode::check_if_adr_maybe_raw(Node* adr) {
  72   if (adr != NULL) {
</pre>
<hr />
<pre>
 191       if (!is_instance || !ClearArrayNode::step_through(&amp;result, instance_id, phase)) {
 192         // Can not bypass initialization of the instance
 193         // we are looking for.
 194         break;
 195       }
 196       // Otherwise skip it (the call updated &#39;result&#39; value).
 197     } else if (result-&gt;is_MergeMem()) {
 198       result = step_through_mergemem(phase, result-&gt;as_MergeMem(), t_oop, NULL, tty);
 199     }
 200   }
 201   return result;
 202 }
 203 
 204 Node *MemNode::optimize_memory_chain(Node *mchain, const TypePtr *t_adr, Node *load, PhaseGVN *phase) {
 205   const TypeOopPtr* t_oop = t_adr-&gt;isa_oopptr();
 206   if (t_oop == NULL)
 207     return mchain;  // don&#39;t try to optimize non-oop types
 208   Node* result = optimize_simple_memory_chain(mchain, t_oop, load, phase);
 209   bool is_instance = t_oop-&gt;is_known_instance_field();
 210   PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
<span class="line-modified"> 211   if (is_instance &amp;&amp; igvn != NULL &amp;&amp; result-&gt;is_Phi()) {</span>
 212     PhiNode *mphi = result-&gt;as_Phi();
 213     assert(mphi-&gt;bottom_type() == Type::MEMORY, &quot;memory phi required&quot;);
 214     const TypePtr *t = mphi-&gt;adr_type();
 215     if (t == TypePtr::BOTTOM || t == TypeRawPtr::BOTTOM ||
 216         (t-&gt;isa_oopptr() &amp;&amp; !t-&gt;is_oopptr()-&gt;is_known_instance() &amp;&amp;
 217          t-&gt;is_oopptr()-&gt;cast_to_exactness(true)
 218            -&gt;is_oopptr()-&gt;cast_to_ptr_type(t_oop-&gt;ptr())
 219             -&gt;is_oopptr()-&gt;cast_to_instance_id(t_oop-&gt;instance_id()) == t_oop)) {
 220       // clone the Phi with our address type
 221       result = mphi-&gt;split_out_instance(t_adr, igvn);
 222     } else {
 223       assert(phase-&gt;C-&gt;get_alias_index(t) == phase-&gt;C-&gt;get_alias_index(t_adr), &quot;correct memory chain&quot;);
 224     }
 225   }
 226   return result;
 227 }
 228 
 229 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st) {
 230   uint alias_idx = phase-&gt;C-&gt;get_alias_index(tp);
 231   Node *mem = mmem;
</pre>
<hr />
<pre>
 310       phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 311       return NodeSentinel; // caller will return NULL
 312     }
 313   }
 314   // Ignore if memory is dead, or self-loop
 315   Node *mem = in(MemNode::Memory);
 316   if (phase-&gt;type( mem ) == Type::TOP) return NodeSentinel; // caller will return NULL
 317   assert(mem != this, &quot;dead loop in MemNode::Ideal&quot;);
 318 
 319   if (can_reshape &amp;&amp; igvn != NULL &amp;&amp; igvn-&gt;_worklist.member(mem)) {
 320     // This memory slice may be dead.
 321     // Delay this mem node transformation until the memory is processed.
 322     phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 323     return NodeSentinel; // caller will return NULL
 324   }
 325 
 326   Node *address = in(MemNode::Address);
 327   const Type *t_adr = phase-&gt;type(address);
 328   if (t_adr == Type::TOP)              return NodeSentinel; // caller will return NULL
 329 
<span class="line-added"> 330   if (can_reshape &amp;&amp; is_unsafe_access() &amp;&amp; (t_adr == TypePtr::NULL_PTR)) {</span>
<span class="line-added"> 331     // Unsafe off-heap access with zero address. Remove access and other control users</span>
<span class="line-added"> 332     // to not confuse optimizations and add a HaltNode to fail if this is ever executed.</span>
<span class="line-added"> 333     assert(ctl != NULL, &quot;unsafe accesses should be control dependent&quot;);</span>
<span class="line-added"> 334     for (DUIterator_Fast imax, i = ctl-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-added"> 335       Node* u = ctl-&gt;fast_out(i);</span>
<span class="line-added"> 336       if (u != ctl) {</span>
<span class="line-added"> 337         igvn-&gt;rehash_node_delayed(u);</span>
<span class="line-added"> 338         int nb = u-&gt;replace_edge(ctl, phase-&gt;C-&gt;top());</span>
<span class="line-added"> 339         --i, imax -= nb;</span>
<span class="line-added"> 340       }</span>
<span class="line-added"> 341     }</span>
<span class="line-added"> 342     Node* frame = igvn-&gt;transform(new ParmNode(phase-&gt;C-&gt;start(), TypeFunc::FramePtr));</span>
<span class="line-added"> 343     Node* halt = igvn-&gt;transform(new HaltNode(ctl, frame, &quot;unsafe off-heap access with zero address&quot;));</span>
<span class="line-added"> 344     phase-&gt;C-&gt;root()-&gt;add_req(halt);</span>
<span class="line-added"> 345     return this;</span>
<span class="line-added"> 346   }</span>
<span class="line-added"> 347 </span>
 348   if (can_reshape &amp;&amp; igvn != NULL &amp;&amp;
 349       (igvn-&gt;_worklist.member(address) ||
 350        (igvn-&gt;_worklist.size() &gt; 0 &amp;&amp; t_adr != adr_type())) ) {
 351     // The address&#39;s base and type may change when the address is processed.
 352     // Delay this mem node transformation until the address is processed.
 353     phase-&gt;is_IterGVN()-&gt;_worklist.push(this);
 354     return NodeSentinel; // caller will return NULL
 355   }
 356 
 357   // Do NOT remove or optimize the next lines: ensure a new alias index
 358   // is allocated for an oop pointer type before Escape Analysis.
 359   // Note: C++ will not remove it since the call has side effect.
 360   if (t_adr-&gt;isa_oopptr()) {
 361     int alias_idx = phase-&gt;C-&gt;get_alias_index(t_adr-&gt;is_ptr());
 362   }
 363 
 364   Node* base = NULL;
 365   if (address-&gt;is_AddP()) {
 366     base = address-&gt;in(AddPNode::Base);
 367   }
</pre>
<hr />
<pre>
 720       } else if (mem-&gt;is_MergeMem()) {
 721         int alias_idx = phase-&gt;C-&gt;get_alias_index(adr_type());
 722         mem = mem-&gt;as_MergeMem()-&gt;memory_at(alias_idx);
 723         continue;           // (a) advance through independent MergeMem memory
 724       }
 725     }
 726 
 727     // Unless there is an explicit &#39;continue&#39;, we must bail out here,
 728     // because &#39;mem&#39; is an inscrutable memory state (e.g., a call).
 729     break;
 730   }
 731 
 732   return NULL;              // bail out
 733 }
 734 
 735 //----------------------calculate_adr_type-------------------------------------
 736 // Helper function.  Notices when the given type of address hits top or bottom.
 737 // Also, asserts a cross-check of the type against the expected address type.
 738 const TypePtr* MemNode::calculate_adr_type(const Type* t, const TypePtr* cross_check) {
 739   if (t == Type::TOP)  return NULL; // does not touch memory any more?
<span class="line-modified"> 740   #ifdef ASSERT</span>


 741   if (!VerifyAliases || VMError::is_error_reported() || Node::in_dump())  cross_check = NULL;
 742   #endif
 743   const TypePtr* tp = t-&gt;isa_ptr();
 744   if (tp == NULL) {
 745     assert(cross_check == NULL || cross_check == TypePtr::BOTTOM, &quot;expected memory type must be wide&quot;);
 746     return TypePtr::BOTTOM;           // touches lots of memory
 747   } else {
 748     #ifdef ASSERT
 749     // %%%% [phh] We don&#39;t check the alias index if cross_check is
 750     //            TypeRawPtr::BOTTOM.  Needs to be investigated.
 751     if (cross_check != NULL &amp;&amp;
 752         cross_check != TypePtr::BOTTOM &amp;&amp;
 753         cross_check != TypeRawPtr::BOTTOM) {
 754       // Recheck the alias index, to see if it has changed (due to a bug).
 755       Compile* C = Compile::current();
 756       assert(C-&gt;get_alias_index(cross_check) == C-&gt;get_alias_index(tp),
 757              &quot;must stay in the original alias category&quot;);
 758       // The type of the address must be contained in the adr_type,
 759       // disregarding &quot;null&quot;-ness.
 760       // (We make an exception for TypeRawPtr::BOTTOM, which is a bit bucket.)
 761       const TypePtr* tp_notnull = tp-&gt;join(TypePtr::NOTNULL)-&gt;is_ptr();
 762       assert(cross_check-&gt;meet(tp_notnull) == cross_check-&gt;remove_speculative(),
 763              &quot;real address must not escape from expected memory type&quot;);
 764     }
 765     #endif
 766     return tp;
 767   }
 768 }
 769 
 770 //=============================================================================
 771 // Should LoadNode::Ideal() attempt to remove control edges?
 772 bool LoadNode::can_remove_control() const {
 773   return true;
 774 }
 775 uint LoadNode::size_of() const { return sizeof(*this); }
<span class="line-modified"> 776 bool LoadNode::cmp( const Node &amp;n ) const</span>
 777 { return !Type::cmp( _type, ((LoadNode&amp;)n)._type ); }
 778 const Type *LoadNode::bottom_type() const { return _type; }
 779 uint LoadNode::ideal_reg() const {
 780   return _type-&gt;ideal_reg();
 781 }
 782 
 783 #ifndef PRODUCT
 784 void LoadNode::dump_spec(outputStream *st) const {
 785   MemNode::dump_spec(st);
 786   if( !Verbose &amp;&amp; !WizardMode ) {
 787     // standard dump does this in Verbose and WizardMode
 788     st-&gt;print(&quot; #&quot;); _type-&gt;dump_on(st);
 789   }
 790   if (!depends_only_on_test()) {
 791     st-&gt;print(&quot; (does not depend only on test)&quot;);
 792   }
 793 }
 794 #endif
 795 
 796 #ifdef ASSERT
 797 //----------------------------is_immutable_value-------------------------------
 798 // Helper function to allow a raw load without control edge for some cases
 799 bool LoadNode::is_immutable_value(Node* adr) {
 800   return (adr-&gt;is_AddP() &amp;&amp; adr-&gt;in(AddPNode::Base)-&gt;is_top() &amp;&amp;
 801           adr-&gt;in(AddPNode::Address)-&gt;Opcode() == Op_ThreadLocal &amp;&amp;
 802           (adr-&gt;in(AddPNode::Offset)-&gt;find_intptr_t_con(-1) ==
 803            in_bytes(JavaThread::osthread_offset())));
 804 }
 805 #endif
 806 
 807 //----------------------------LoadNode::make-----------------------------------
 808 // Polymorphic factory method:
 809 Node *LoadNode::make(PhaseGVN&amp; gvn, Node *ctl, Node *mem, Node *adr, const TypePtr* adr_type, const Type *rt, BasicType bt, MemOrd mo,
<span class="line-modified"> 810                      ControlDependency control_dependency, bool unaligned, bool mismatched, bool unsafe, uint8_t barrier_data) {</span>
 811   Compile* C = gvn.C;
 812 
 813   // sanity check the alias category against the created node type
 814   assert(!(adr_type-&gt;isa_oopptr() &amp;&amp;
 815            adr_type-&gt;offset() == oopDesc::klass_offset_in_bytes()),
 816          &quot;use LoadKlassNode instead&quot;);
 817   assert(!(adr_type-&gt;isa_aryptr() &amp;&amp;
 818            adr_type-&gt;offset() == arrayOopDesc::length_offset_in_bytes()),
 819          &quot;use LoadRangeNode instead&quot;);
 820   // Check control edge of raw loads
 821   assert( ctl != NULL || C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
 822           // oop will be recorded in oop map if load crosses safepoint
 823           rt-&gt;isa_oopptr() || is_immutable_value(adr),
 824           &quot;raw memory operations should have control edge&quot;);
 825   LoadNode* load = NULL;
 826   switch (bt) {
 827   case T_BOOLEAN: load = new LoadUBNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 828   case T_BYTE:    load = new LoadBNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 829   case T_INT:     load = new LoadINode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 830   case T_CHAR:    load = new LoadUSNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
</pre>
<hr />
<pre>
 841 #endif
 842     {
 843       assert(!adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop() &amp;&amp; !adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass(), &quot;should have got back a narrow oop&quot;);
 844       load = new LoadPNode(ctl, mem, adr, adr_type, rt-&gt;is_ptr(), mo, control_dependency);
 845     }
 846     break;
 847   default:
 848     ShouldNotReachHere();
 849     break;
 850   }
 851   assert(load != NULL, &quot;LoadNode should have been created&quot;);
 852   if (unaligned) {
 853     load-&gt;set_unaligned_access();
 854   }
 855   if (mismatched) {
 856     load-&gt;set_mismatched_access();
 857   }
 858   if (unsafe) {
 859     load-&gt;set_unsafe_access();
 860   }
<span class="line-added"> 861   load-&gt;set_barrier_data(barrier_data);</span>
 862   if (load-&gt;Opcode() == Op_LoadN) {
 863     Node* ld = gvn.transform(load);
 864     return new DecodeNNode(ld, ld-&gt;bottom_type()-&gt;make_ptr());
 865   }
 866 
 867   return load;
 868 }
 869 
 870 LoadLNode* LoadLNode::make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, const Type* rt, MemOrd mo,
<span class="line-modified"> 871                                   ControlDependency control_dependency, bool unaligned, bool mismatched, bool unsafe, uint8_t barrier_data) {</span>
 872   bool require_atomic = true;
 873   LoadLNode* load = new LoadLNode(ctl, mem, adr, adr_type, rt-&gt;is_long(), mo, control_dependency, require_atomic);
 874   if (unaligned) {
 875     load-&gt;set_unaligned_access();
 876   }
 877   if (mismatched) {
 878     load-&gt;set_mismatched_access();
 879   }
 880   if (unsafe) {
 881     load-&gt;set_unsafe_access();
 882   }
<span class="line-added"> 883   load-&gt;set_barrier_data(barrier_data);</span>
 884   return load;
 885 }
 886 
 887 LoadDNode* LoadDNode::make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, const Type* rt, MemOrd mo,
<span class="line-modified"> 888                                   ControlDependency control_dependency, bool unaligned, bool mismatched, bool unsafe, uint8_t barrier_data) {</span>
 889   bool require_atomic = true;
 890   LoadDNode* load = new LoadDNode(ctl, mem, adr, adr_type, rt, mo, control_dependency, require_atomic);
 891   if (unaligned) {
 892     load-&gt;set_unaligned_access();
 893   }
 894   if (mismatched) {
 895     load-&gt;set_mismatched_access();
 896   }
 897   if (unsafe) {
 898     load-&gt;set_unsafe_access();
 899   }
<span class="line-added"> 900   load-&gt;set_barrier_data(barrier_data);</span>
 901   return load;
 902 }
 903 
 904 
 905 
 906 //------------------------------hash-------------------------------------------
 907 uint LoadNode::hash() const {
 908   // unroll addition of interesting fields
 909   return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address);
 910 }
 911 
 912 static bool skip_through_membars(Compile::AliasType* atp, const TypeInstPtr* tp, bool eliminate_boxing) {
 913   if ((atp != NULL) &amp;&amp; (atp-&gt;index() &gt;= Compile::AliasIdxRaw)) {
 914     bool non_volatile = (atp-&gt;field() != NULL) &amp;&amp; !atp-&gt;field()-&gt;is_volatile();
 915     bool is_stable_ary = FoldStableValues &amp;&amp;
 916                          (tp != NULL) &amp;&amp; (tp-&gt;isa_aryptr() != NULL) &amp;&amp;
 917                          tp-&gt;isa_aryptr()-&gt;is_stable();
 918 
 919     return (eliminate_boxing &amp;&amp; non_volatile) || is_stable_ary;
 920   }
 921 
 922   return false;
 923 }
 924 
 925 // Is the value loaded previously stored by an arraycopy? If so return
 926 // a load node that reads from the source array so we may be able to
 927 // optimize out the ArrayCopy node later.
 928 Node* LoadNode::can_see_arraycopy_value(Node* st, PhaseGVN* phase) const {








 929   Node* ld_adr = in(MemNode::Address);
 930   intptr_t ld_off = 0;
 931   AllocateNode* ld_alloc = AllocateNode::Ideal_allocation(ld_adr, phase, ld_off);
 932   Node* ac = find_previous_arraycopy(phase, ld_alloc, st, true);
 933   if (ac != NULL) {
 934     assert(ac-&gt;is_ArrayCopy(), &quot;what kind of node can this be?&quot;);
 935 
 936     Node* mem = ac-&gt;in(TypeFunc::Memory);
 937     Node* ctl = ac-&gt;in(0);
 938     Node* src = ac-&gt;in(ArrayCopyNode::Src);
 939 
 940     if (!ac-&gt;as_ArrayCopy()-&gt;is_clonebasic() &amp;&amp; !phase-&gt;type(src)-&gt;isa_aryptr()) {
 941       return NULL;
 942     }
 943 
 944     LoadNode* ld = clone()-&gt;as_Load();
 945     Node* addp = in(MemNode::Address)-&gt;clone();
 946     if (ac-&gt;as_ArrayCopy()-&gt;is_clonebasic()) {
 947       assert(ld_alloc != NULL, &quot;need an alloc&quot;);
 948       assert(addp-&gt;is_AddP(), &quot;address must be addp&quot;);
</pre>
<hr />
<pre>
 966       uint shift  = exact_log2(type2aelembytes(ary_elem));
 967 
 968       Node* diff = phase-&gt;transform(new SubINode(ac-&gt;in(ArrayCopyNode::SrcPos), ac-&gt;in(ArrayCopyNode::DestPos)));
 969 #ifdef _LP64
 970       diff = phase-&gt;transform(new ConvI2LNode(diff));
 971 #endif
 972       diff = phase-&gt;transform(new LShiftXNode(diff, phase-&gt;intcon(shift)));
 973 
 974       Node* offset = phase-&gt;transform(new AddXNode(addp-&gt;in(AddPNode::Offset), diff));
 975       addp-&gt;set_req(AddPNode::Offset, offset);
 976     }
 977     addp = phase-&gt;transform(addp);
 978 #ifdef ASSERT
 979     const TypePtr* adr_type = phase-&gt;type(addp)-&gt;is_ptr();
 980     ld-&gt;_adr_type = adr_type;
 981 #endif
 982     ld-&gt;set_req(MemNode::Address, addp);
 983     ld-&gt;set_req(0, ctl);
 984     ld-&gt;set_req(MemNode::Memory, mem);
 985     // load depends on the tests that validate the arraycopy
<span class="line-modified"> 986     ld-&gt;_control_dependency = UnknownControl;</span>
 987     return ld;
 988   }
 989   return NULL;
 990 }
 991 
 992 
 993 //---------------------------can_see_stored_value------------------------------
 994 // This routine exists to make sure this set of tests is done the same
 995 // everywhere.  We need to make a coordinated change: first LoadNode::Ideal
 996 // will change the graph shape in a way which makes memory alive twice at the
 997 // same time (uses the Oracle model of aliasing), then some
 998 // LoadXNode::Identity will fold things back to the equivalence-class model
 999 // of aliasing.
1000 Node* MemNode::can_see_stored_value(Node* st, PhaseTransform* phase) const {
1001   Node* ld_adr = in(MemNode::Address);
1002   intptr_t ld_off = 0;
1003   Node* ld_base = AddPNode::Ideal_base_and_offset(ld_adr, phase, ld_off);
1004   Node* ld_alloc = AllocateNode::Ideal_allocation(ld_base, phase);
1005   const TypeInstPtr* tp = phase-&gt;type(ld_adr)-&gt;isa_instptr();
1006   Compile::AliasType* atp = (tp != NULL) ? phase-&gt;C-&gt;alias_type(tp) : NULL;
</pre>
<hr />
<pre>
1040           result = new_st;
1041         }
1042       }
1043       break;
1044     }
1045     if (result != NULL) {
1046       st = result;
1047     }
1048   }
1049 
1050   // Loop around twice in the case Load -&gt; Initialize -&gt; Store.
1051   // (See PhaseIterGVN::add_users_to_worklist, which knows about this case.)
1052   for (int trip = 0; trip &lt;= 1; trip++) {
1053 
1054     if (st-&gt;is_Store()) {
1055       Node* st_adr = st-&gt;in(MemNode::Address);
1056       if (!phase-&gt;eqv(st_adr, ld_adr)) {
1057         // Try harder before giving up. Unify base pointers with casts (e.g., raw/non-raw pointers).
1058         intptr_t st_off = 0;
1059         Node* st_base = AddPNode::Ideal_base_and_offset(st_adr, phase, st_off);
<span class="line-modified">1060         if (ld_base == NULL)                                   return NULL;</span>
<span class="line-modified">1061         if (st_base == NULL)                                   return NULL;</span>
<span class="line-modified">1062         if (!ld_base-&gt;eqv_uncast(st_base, /*keep_deps=*/true)) return NULL;</span>
<span class="line-modified">1063         if (ld_off != st_off)                                  return NULL;</span>
<span class="line-modified">1064         if (ld_off == Type::OffsetBot)                         return NULL;</span>
1065         // Same base, same offset.
1066         // Possible improvement for arrays: check index value instead of absolute offset.
1067 
1068         // At this point we have proven something like this setup:
1069         //   B = &lt;&lt; base &gt;&gt;
1070         //   L =  LoadQ(AddP(Check/CastPP(B), #Off))
1071         //   S = StoreQ(AddP(             B , #Off), V)
1072         // (Actually, we haven&#39;t yet proven the Q&#39;s are the same.)
1073         // In other words, we are loading from a casted version of
1074         // the same pointer-and-offset that we stored to.
<span class="line-added">1075         // Casted version may carry a dependency and it is respected.</span>
1076         // Thus, we are able to replace L by V.
1077       }
1078       // Now prove that we have a LoadQ matched to a StoreQ, for some Q.
1079       if (store_Opcode() != st-&gt;Opcode())
1080         return NULL;
1081       return st-&gt;in(MemNode::ValueIn);
1082     }
1083 
1084     // A load from a freshly-created object always returns zero.
1085     // (This can happen after LoadNode::Ideal resets the load&#39;s memory input
1086     // to find_captured_store, which returned InitializeNode::zero_memory.)
1087     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
1088         (st-&gt;in(0) == ld_alloc) &amp;&amp;
1089         (ld_off &gt;= st-&gt;in(0)-&gt;as_Allocate()-&gt;minimum_header_size())) {
1090       // return a zero value for the load&#39;s basic type
1091       // (This is one of the few places where a generic PhaseTransform
1092       // can create new nodes.  Think of it as lazily manifesting
1093       // virtually pre-existing constants.)
1094       return phase-&gt;zerocon(memory_type());
1095     }
</pre>
<hr />
<pre>
1417           return this; // made change
1418         }
1419       }
1420     }
1421   }
1422   if (base_is_phi) {
1423     if (!stable_phi(base-&gt;as_Phi(), phase)) {
1424       return NULL; // Wait stable graph
1425     }
1426     uint cnt = base-&gt;req();
1427     // Check for loop invariant memory.
1428     if (cnt == 3) {
1429       for (uint i = 1; i &lt; cnt; i++) {
1430         if (base-&gt;in(i) == base) {
1431           return NULL; // Wait stable graph
1432         }
1433       }
1434     }
1435   }
1436 


1437   // Split through Phi (see original code in loopopts.cpp).
1438   assert(C-&gt;have_alias_type(t_oop), &quot;instance should have alias type&quot;);
1439 
1440   // Do nothing here if Identity will find a value
1441   // (to avoid infinite chain of value phis generation).
<span class="line-modified">1442   if (!phase-&gt;eqv(this, this-&gt;Identity(phase))) {</span>
1443     return NULL;
<span class="line-added">1444   }</span>
1445 
1446   // Select Region to split through.
1447   Node* region;
1448   if (!base_is_phi) {
1449     assert(mem-&gt;is_Phi(), &quot;sanity&quot;);
1450     region = mem-&gt;in(0);
1451     // Skip if the region dominates some control edge of the address.
1452     if (!MemNode::all_controls_dominate(address, region))
1453       return NULL;
1454   } else if (!mem-&gt;is_Phi()) {
1455     assert(base_is_phi, &quot;sanity&quot;);
1456     region = base-&gt;in(0);
1457     // Skip if the region dominates some control edge of the memory.
1458     if (!MemNode::all_controls_dominate(mem, region))
1459       return NULL;
1460   } else if (base-&gt;in(0) != mem-&gt;in(0)) {
1461     assert(base_is_phi &amp;&amp; mem-&gt;is_Phi(), &quot;sanity&quot;);
1462     if (MemNode::all_controls_dominate(mem, base-&gt;in(0))) {
1463       region = base-&gt;in(0);
1464     } else if (MemNode::all_controls_dominate(address, mem-&gt;in(0))) {
</pre>
<hr />
<pre>
1467       return NULL; // complex graph
1468     }
1469   } else {
1470     assert(base-&gt;in(0) == mem-&gt;in(0), &quot;sanity&quot;);
1471     region = mem-&gt;in(0);
1472   }
1473 
1474   const Type* this_type = this-&gt;bottom_type();
1475   int this_index  = C-&gt;get_alias_index(t_oop);
1476   int this_offset = t_oop-&gt;offset();
1477   int this_iid    = t_oop-&gt;instance_id();
1478   if (!t_oop-&gt;is_known_instance() &amp;&amp; load_boxed_values) {
1479     // Use _idx of address base for boxed values.
1480     this_iid = base-&gt;_idx;
1481   }
1482   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
1483   Node* phi = new PhiNode(region, this_type, NULL, mem-&gt;_idx, this_iid, this_index, this_offset);
1484   for (uint i = 1; i &lt; region-&gt;req(); i++) {
1485     Node* x;
1486     Node* the_clone = NULL;
<span class="line-modified">1487     Node* in = region-&gt;in(i);</span>
<span class="line-added">1488     if (region-&gt;is_CountedLoop() &amp;&amp; region-&gt;as_Loop()-&gt;is_strip_mined() &amp;&amp; i == LoopNode::EntryControl &amp;&amp;</span>
<span class="line-added">1489         in != NULL &amp;&amp; in-&gt;is_OuterStripMinedLoop()) {</span>
<span class="line-added">1490       // No node should go in the outer strip mined loop</span>
<span class="line-added">1491       in = in-&gt;in(LoopNode::EntryControl);</span>
<span class="line-added">1492     }</span>
<span class="line-added">1493     if (in == NULL || in == C-&gt;top()) {</span>
1494       x = C-&gt;top();      // Dead path?  Use a dead data op
1495     } else {
1496       x = this-&gt;clone();        // Else clone up the data op
1497       the_clone = x;            // Remember for possible deletion.
1498       // Alter data node to use pre-phi inputs
1499       if (this-&gt;in(0) == region) {
<span class="line-modified">1500         x-&gt;set_req(0, in);</span>
1501       } else {
1502         x-&gt;set_req(0, NULL);
1503       }
1504       if (mem-&gt;is_Phi() &amp;&amp; (mem-&gt;in(0) == region)) {
1505         x-&gt;set_req(Memory, mem-&gt;in(i)); // Use pre-Phi input for the clone.
1506       }
1507       if (address-&gt;is_Phi() &amp;&amp; address-&gt;in(0) == region) {
1508         x-&gt;set_req(Address, address-&gt;in(i)); // Use pre-Phi input for the clone
1509       }
1510       if (base_is_phi &amp;&amp; (base-&gt;in(0) == region)) {
1511         Node* base_x = base-&gt;in(i); // Clone address for loads from boxed objects.
1512         Node* adr_x = phase-&gt;transform(new AddPNode(base_x,base_x,address-&gt;in(AddPNode::Offset)));
1513         x-&gt;set_req(Address, adr_x);
1514       }
1515     }
1516     // Check for a &#39;win&#39; on some paths
1517     const Type *t = x-&gt;Value(igvn);
1518 
1519     bool singleton = t-&gt;singleton();
1520 
1521     // See comments in PhaseIdealLoop::split_thru_phi().
1522     if (singleton &amp;&amp; t == Type::TOP) {
1523       singleton &amp;= region-&gt;is_Loop() &amp;&amp; (i != LoopNode::EntryControl);
1524     }
1525 
1526     if (singleton) {
1527       x = igvn-&gt;makecon(t);
1528     } else {
1529       // We now call Identity to try to simplify the cloned node.
1530       // Note that some Identity methods call phase-&gt;type(this).
1531       // Make sure that the type array is big enough for
1532       // our new node, even though we may throw the node away.
1533       // (This tweaking with igvn only works because x is a new node.)
1534       igvn-&gt;set_type(x, t);
1535       // If x is a TypeNode, capture any more-precise type permanently into Node
1536       // otherwise it will be not updated during igvn-&gt;transform since
1537       // igvn-&gt;type(x) is set to x-&gt;Value() already.
1538       x-&gt;raise_bottom_type(t);
<span class="line-modified">1539       Node* y = x-&gt;Identity(igvn);</span>
1540       if (y != x) {
1541         x = y;
1542       } else {
1543         y = igvn-&gt;hash_find_insert(x);
1544         if (y) {
1545           x = y;
1546         } else {
1547           // Else x is a new node we are keeping
1548           // We do not need register_new_node_with_optimizer
1549           // because set_type has already been called.
1550           igvn-&gt;_worklist.push(x);
1551         }
1552       }
1553     }
1554     if (x != the_clone &amp;&amp; the_clone != NULL) {
1555       igvn-&gt;remove_dead_node(the_clone);
1556     }
1557     phi-&gt;set_req(i, x);
1558   }
1559   // Record Phi
1560   igvn-&gt;register_new_node_with_optimizer(phi);
1561   return phi;
1562 }
1563 
<span class="line-added">1564 AllocateNode* LoadNode::is_new_object_mark_load(PhaseGVN *phase) const {</span>
<span class="line-added">1565   if (Opcode() == Op_LoadX) {</span>
<span class="line-added">1566     Node* address = in(MemNode::Address);</span>
<span class="line-added">1567     AllocateNode* alloc = AllocateNode::Ideal_allocation(address, phase);</span>
<span class="line-added">1568     Node* mem = in(MemNode::Memory);</span>
<span class="line-added">1569     if (alloc != NULL &amp;&amp; mem-&gt;is_Proj() &amp;&amp;</span>
<span class="line-added">1570         mem-&gt;in(0) != NULL &amp;&amp;</span>
<span class="line-added">1571         mem-&gt;in(0) == alloc-&gt;initialization() &amp;&amp;</span>
<span class="line-added">1572         alloc-&gt;initialization()-&gt;proj_out_or_null(0) != NULL) {</span>
<span class="line-added">1573       return alloc;</span>
<span class="line-added">1574     }</span>
<span class="line-added">1575   }</span>
<span class="line-added">1576   return NULL;</span>
<span class="line-added">1577 }</span>
<span class="line-added">1578 </span>
<span class="line-added">1579 </span>
1580 //------------------------------Ideal------------------------------------------
1581 // If the load is from Field memory and the pointer is non-null, it might be possible to
1582 // zero out the control input.
1583 // If the offset is constant and the base is an object allocation,
1584 // try to hook me up to the exact initializing store.
1585 Node *LoadNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1586   Node* p = MemNode::Ideal_common(phase, can_reshape);
1587   if (p)  return (p == NodeSentinel) ? NULL : p;
1588 
1589   Node* ctrl    = in(MemNode::Control);
1590   Node* address = in(MemNode::Address);
1591   bool progress = false;
1592 
1593   bool addr_mark = ((phase-&gt;type(address)-&gt;isa_oopptr() || phase-&gt;type(address)-&gt;isa_narrowoop()) &amp;&amp;
1594          phase-&gt;type(address)-&gt;is_ptr()-&gt;offset() == oopDesc::mark_offset_in_bytes());
1595 
1596   // Skip up past a SafePoint control.  Cannot do this for Stores because
1597   // pointer stores &amp; cardmarks must stay on the same side of a SafePoint.
1598   if( ctrl != NULL &amp;&amp; ctrl-&gt;Opcode() == Op_SafePoint &amp;&amp;
1599       phase-&gt;C-&gt;get_alias_index(phase-&gt;type(address)-&gt;is_ptr()) != Compile::AliasIdxRaw  &amp;&amp;
<span class="line-modified">1600       !addr_mark &amp;&amp;</span>
<span class="line-added">1601       (depends_only_on_test() || has_unknown_control_dependency())) {</span>
1602     ctrl = ctrl-&gt;in(0);
1603     set_req(MemNode::Control,ctrl);
1604     progress = true;
1605   }
1606 
1607   intptr_t ignore = 0;
1608   Node*    base   = AddPNode::Ideal_base_and_offset(address, phase, ignore);
1609   if (base != NULL
1610       &amp;&amp; phase-&gt;C-&gt;get_alias_index(phase-&gt;type(address)-&gt;is_ptr()) != Compile::AliasIdxRaw) {
1611     // Check for useless control edge in some common special cases
1612     if (in(MemNode::Control) != NULL
1613         &amp;&amp; can_remove_control()
1614         &amp;&amp; phase-&gt;type(base)-&gt;higher_equal(TypePtr::NOTNULL)
1615         &amp;&amp; all_controls_dominate(base, phase-&gt;C-&gt;start())) {
1616       // A method-invariant, non-null address (constant or &#39;this&#39; argument).
1617       set_req(MemNode::Control, NULL);
1618       progress = true;
1619     }
1620   }
1621 
</pre>
<hr />
<pre>
1688   // fold up, do so.
1689   Node* prev_mem = find_previous_store(phase);
1690   if (prev_mem != NULL) {
1691     Node* value = can_see_arraycopy_value(prev_mem, phase);
1692     if (value != NULL) {
1693       return value;
1694     }
1695   }
1696   // Steps (a), (b):  Walk past independent stores to find an exact match.
1697   if (prev_mem != NULL &amp;&amp; prev_mem != in(MemNode::Memory)) {
1698     // (c) See if we can fold up on the spot, but don&#39;t fold up here.
1699     // Fold-up might require truncation (for LoadB/LoadS/LoadUS) or
1700     // just return a prior value, which is done by Identity calls.
1701     if (can_see_stored_value(prev_mem, phase)) {
1702       // Make ready for step (d):
1703       set_req(MemNode::Memory, prev_mem);
1704       return this;
1705     }
1706   }
1707 
<span class="line-added">1708   AllocateNode* alloc = is_new_object_mark_load(phase);</span>
<span class="line-added">1709   if (alloc != NULL &amp;&amp; alloc-&gt;Opcode() == Op_Allocate &amp;&amp; UseBiasedLocking) {</span>
<span class="line-added">1710     InitializeNode* init = alloc-&gt;initialization();</span>
<span class="line-added">1711     Node* control = init-&gt;proj_out(0);</span>
<span class="line-added">1712     return alloc-&gt;make_ideal_mark(phase, address, control, mem);</span>
<span class="line-added">1713   }</span>
<span class="line-added">1714 </span>
1715   return progress ? this : NULL;
1716 }
1717 
1718 // Helper to recognize certain Klass fields which are invariant across
1719 // some group of array types (e.g., int[] or all T[] where T &lt; Object).
1720 const Type*
1721 LoadNode::load_array_final_field(const TypeKlassPtr *tkls,
1722                                  ciKlass* klass) const {
1723   if (tkls-&gt;offset() == in_bytes(Klass::modifier_flags_offset())) {
1724     // The field is Klass::_modifier_flags.  Return its (constant) value.
1725     // (Folds up the 2nd indirection in aClassConstant.getModifiers().)
1726     assert(this-&gt;Opcode() == Op_LoadI, &quot;must load an int from _modifier_flags&quot;);
1727     return TypeInt::make(klass-&gt;modifier_flags());
1728   }
1729   if (tkls-&gt;offset() == in_bytes(Klass::access_flags_offset())) {
1730     // The field is Klass::_access_flags.  Return its (constant) value.
1731     // (Folds up the 2nd indirection in Reflection.getClassAccessFlags(aClassConstant).)
1732     assert(this-&gt;Opcode() == Op_LoadI, &quot;must load an int from _access_flags&quot;);
1733     return TypeInt::make(klass-&gt;access_flags());
1734   }
</pre>
<hr />
<pre>
1953   bool is_instance = (tinst != NULL) &amp;&amp; tinst-&gt;is_known_instance_field();
1954   bool is_boxed_value = (tinst != NULL) &amp;&amp; tinst-&gt;is_ptr_to_boxed_value();
1955   if (ReduceFieldZeroing || is_instance || is_boxed_value) {
1956     Node* value = can_see_stored_value(mem,phase);
1957     if (value != NULL &amp;&amp; value-&gt;is_Con()) {
1958       assert(value-&gt;bottom_type()-&gt;higher_equal(_type),&quot;sanity&quot;);
1959       return value-&gt;bottom_type();
1960     }
1961   }
1962 
1963   if (is_instance) {
1964     // If we have an instance type and our memory input is the
1965     // programs&#39;s initial memory state, there is no matching store,
1966     // so just return a zero of the appropriate type
1967     Node *mem = in(MemNode::Memory);
1968     if (mem-&gt;is_Parm() &amp;&amp; mem-&gt;in(0)-&gt;is_Start()) {
1969       assert(mem-&gt;as_Parm()-&gt;_con == TypeFunc::Memory, &quot;must be memory Parm&quot;);
1970       return Type::get_zero_type(_type-&gt;basic_type());
1971     }
1972   }
<span class="line-added">1973 </span>
<span class="line-added">1974   Node* alloc = is_new_object_mark_load(phase);</span>
<span class="line-added">1975   if (alloc != NULL &amp;&amp; !(alloc-&gt;Opcode() == Op_Allocate &amp;&amp; UseBiasedLocking)) {</span>
<span class="line-added">1976     return TypeX::make(markWord::prototype().value());</span>
<span class="line-added">1977   }</span>
<span class="line-added">1978 </span>
1979   return _type;
1980 }
1981 
1982 //------------------------------match_edge-------------------------------------
1983 // Do we Match on this edge index or not?  Match only the address.
1984 uint LoadNode::match_edge(uint idx) const {
1985   return idx == MemNode::Address;
1986 }
1987 
1988 //--------------------------LoadBNode::Ideal--------------------------------------
1989 //
1990 //  If the previous store is to the same address as this load,
1991 //  and the value stored was larger than a byte, replace this load
1992 //  with the value stored truncated to a byte.  If no truncation is
1993 //  needed, the replacement is done in LoadNode::Identity().
1994 //
1995 Node *LoadBNode::Ideal(PhaseGVN *phase, bool can_reshape) {
1996   Node* mem = in(MemNode::Memory);
1997   Node* value = can_see_stored_value(mem,phase);
1998   if( value &amp;&amp; !phase-&gt;type(value)-&gt;higher_equal( _type ) ) {
</pre>
<hr />
<pre>
2656       const TypeOopPtr* t_oop = phase-&gt;type(in(Address))-&gt;isa_oopptr();
2657       assert(t_oop == NULL || t_oop-&gt;is_known_instance_field(), &quot;only for non escaping objects&quot;);
2658 #endif
2659       PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
2660       trailing-&gt;remove(igvn);
2661     }
2662   }
2663 
2664   return result;
2665 }
2666 
2667 //------------------------------match_edge-------------------------------------
2668 // Do we Match on this edge index or not?  Match only memory &amp; value
2669 uint StoreNode::match_edge(uint idx) const {
2670   return idx == MemNode::Address || idx == MemNode::ValueIn;
2671 }
2672 
2673 //------------------------------cmp--------------------------------------------
2674 // Do not common stores up together.  They generally have to be split
2675 // back up anyways, so do not bother.
<span class="line-modified">2676 bool StoreNode::cmp( const Node &amp;n ) const {</span>
2677   return (&amp;n == this);          // Always fail except on self
2678 }
2679 
2680 //------------------------------Ideal_masked_input-----------------------------
2681 // Check for a useless mask before a partial-word store
2682 // (StoreB ... (AndI valIn conIa) )
2683 // If (conIa &amp; mask == mask) this simplifies to
2684 // (StoreB ... (valIn) )
2685 Node *StoreNode::Ideal_masked_input(PhaseGVN *phase, uint mask) {
2686   Node *val = in(MemNode::ValueIn);
2687   if( val-&gt;Opcode() == Op_AndI ) {
2688     const TypeInt *t = phase-&gt;type( val-&gt;in(2) )-&gt;isa_int();
2689     if( t &amp;&amp; t-&gt;is_con() &amp;&amp; (t-&gt;get_con() &amp; mask) == mask ) {
2690       set_req(MemNode::ValueIn, val-&gt;in(1));
2691       return this;
2692     }
2693   }
2694   return NULL;
2695 }
2696 
</pre>
<hr />
<pre>
2839   // If extra input is TOP ==&gt; the result is TOP
2840   t = phase-&gt;type( in(MemNode::OopStore) );
2841   if( t == Type::TOP ) return Type::TOP;
2842 
2843   return StoreNode::Value( phase );
2844 }
2845 
2846 
2847 //=============================================================================
2848 //----------------------------------SCMemProjNode------------------------------
2849 const Type* SCMemProjNode::Value(PhaseGVN* phase) const
2850 {
2851   return bottom_type();
2852 }
2853 
2854 //=============================================================================
2855 //----------------------------------LoadStoreNode------------------------------
2856 LoadStoreNode::LoadStoreNode( Node *c, Node *mem, Node *adr, Node *val, const TypePtr* at, const Type* rt, uint required )
2857   : Node(required),
2858     _type(rt),
<span class="line-modified">2859     _adr_type(at),</span>
<span class="line-added">2860     _barrier(0)</span>
2861 {
2862   init_req(MemNode::Control, c  );
2863   init_req(MemNode::Memory , mem);
2864   init_req(MemNode::Address, adr);
2865   init_req(MemNode::ValueIn, val);
2866   init_class_id(Class_LoadStore);
2867 }
2868 
2869 uint LoadStoreNode::ideal_reg() const {
2870   return _type-&gt;ideal_reg();
2871 }
2872 
2873 bool LoadStoreNode::result_not_used() const {
2874   for( DUIterator_Fast imax, i = fast_outs(imax); i &lt; imax; i++ ) {
2875     Node *x = fast_out(i);
2876     if (x-&gt;Opcode() == Op_SCMemProj) continue;
2877     return false;
2878   }
2879   return true;
2880 }
</pre>
<hr />
<pre>
3087 
3088 //=============================================================================
3089 MemBarNode::MemBarNode(Compile* C, int alias_idx, Node* precedent)
3090   : MultiNode(TypeFunc::Parms + (precedent == NULL? 0: 1)),
3091     _adr_type(C-&gt;get_adr_type(alias_idx)), _kind(Standalone)
3092 #ifdef ASSERT
3093   , _pair_idx(0)
3094 #endif
3095 {
3096   init_class_id(Class_MemBar);
3097   Node* top = C-&gt;top();
3098   init_req(TypeFunc::I_O,top);
3099   init_req(TypeFunc::FramePtr,top);
3100   init_req(TypeFunc::ReturnAdr,top);
3101   if (precedent != NULL)
3102     init_req(TypeFunc::Parms, precedent);
3103 }
3104 
3105 //------------------------------cmp--------------------------------------------
3106 uint MemBarNode::hash() const { return NO_HASH; }
<span class="line-modified">3107 bool MemBarNode::cmp( const Node &amp;n ) const {</span>
3108   return (&amp;n == this);          // Always fail except on self
3109 }
3110 
3111 //------------------------------make-------------------------------------------
3112 MemBarNode* MemBarNode::make(Compile* C, int opcode, int atp, Node* pn) {
3113   switch (opcode) {
3114   case Op_MemBarAcquire:     return new MemBarAcquireNode(C, atp, pn);
3115   case Op_LoadFence:         return new LoadFenceNode(C, atp, pn);
3116   case Op_MemBarRelease:     return new MemBarReleaseNode(C, atp, pn);
3117   case Op_StoreFence:        return new StoreFenceNode(C, atp, pn);
3118   case Op_MemBarAcquireLock: return new MemBarAcquireLockNode(C, atp, pn);
3119   case Op_MemBarReleaseLock: return new MemBarReleaseLockNode(C, atp, pn);
3120   case Op_MemBarVolatile:    return new MemBarVolatileNode(C, atp, pn);
3121   case Op_MemBarCPUOrder:    return new MemBarCPUOrderNode(C, atp, pn);
3122   case Op_OnSpinWait:        return new OnSpinWaitNode(C, atp, pn);
3123   case Op_Initialize:        return new InitializeNode(C, atp, pn);
3124   case Op_MemBarStoreStore:  return new MemBarStoreStoreNode(C, atp, pn);
3125   default: ShouldNotReachHere(); return NULL;
3126   }
3127 }
</pre>
<hr />
<pre>
3134     MemBarNode* leading = leading_membar();
3135     if (leading != NULL) {
3136       assert(leading-&gt;trailing_membar() == this, &quot;inconsistent leading/trailing membars&quot;);
3137       leading-&gt;remove(igvn);
3138     }
3139   }
3140   igvn-&gt;replace_node(proj_out(TypeFunc::Memory), in(TypeFunc::Memory));
3141   igvn-&gt;replace_node(proj_out(TypeFunc::Control), in(TypeFunc::Control));
3142 }
3143 
3144 //------------------------------Ideal------------------------------------------
3145 // Return a node which is more &quot;ideal&quot; than the current node.  Strip out
3146 // control copies
3147 Node *MemBarNode::Ideal(PhaseGVN *phase, bool can_reshape) {
3148   if (remove_dead_region(phase, can_reshape)) return this;
3149   // Don&#39;t bother trying to transform a dead node
3150   if (in(0) &amp;&amp; in(0)-&gt;is_top()) {
3151     return NULL;
3152   }
3153 










3154   bool progress = false;
3155   // Eliminate volatile MemBars for scalar replaced objects.
3156   if (can_reshape &amp;&amp; req() == (Precedent+1)) {
3157     bool eliminate = false;
3158     int opc = Opcode();
3159     if ((opc == Op_MemBarAcquire || opc == Op_MemBarVolatile)) {
3160       // Volatile field loads and stores.
3161       Node* my_mem = in(MemBarNode::Precedent);
3162       // The MembarAquire may keep an unused LoadNode alive through the Precedent edge
3163       if ((my_mem != NULL) &amp;&amp; (opc == Op_MemBarAcquire) &amp;&amp; (my_mem-&gt;outcnt() == 1)) {
3164         // if the Precedent is a decodeN and its input (a Load) is used at more than one place,
3165         // replace this Precedent (decodeN) with the Load instead.
3166         if ((my_mem-&gt;Opcode() == Op_DecodeN) &amp;&amp; (my_mem-&gt;in(1)-&gt;outcnt() &gt; 1))  {
3167           Node* load_node = my_mem-&gt;in(1);
3168           set_req(MemBarNode::Precedent, load_node);
3169           phase-&gt;is_IterGVN()-&gt;_worklist.push(my_mem);
3170           my_mem = load_node;
3171         } else {
3172           assert(my_mem-&gt;unique_out() == this, &quot;sanity&quot;);
3173           del_req(Precedent);
</pre>
<hr />
<pre>
3526   }
3527 }
3528 
3529 // Helper for remembering which stores go with which offsets.
3530 intptr_t InitializeNode::get_store_offset(Node* st, PhaseTransform* phase) {
3531   if (!st-&gt;is_Store())  return -1;  // can happen to dead code via subsume_node
3532   intptr_t offset = -1;
3533   Node* base = AddPNode::Ideal_base_and_offset(st-&gt;in(MemNode::Address),
3534                                                phase, offset);
3535   if (base == NULL)     return -1;  // something is dead,
3536   if (offset &lt; 0)       return -1;  //        dead, dead
3537   return offset;
3538 }
3539 
3540 // Helper for proving that an initialization expression is
3541 // &quot;simple enough&quot; to be folded into an object initialization.
3542 // Attempts to prove that a store&#39;s initial value &#39;n&#39; can be captured
3543 // within the initialization without creating a vicious cycle, such as:
3544 //     { Foo p = new Foo(); p.next = p; }
3545 // True for constants and parameters and small combinations thereof.
<span class="line-modified">3546 bool InitializeNode::detect_init_independence(Node* value, PhaseGVN* phase) {</span>
<span class="line-modified">3547   ResourceMark rm;</span>
<span class="line-modified">3548   Unique_Node_List worklist;</span>
<span class="line-modified">3549   worklist.push(value);</span>
<span class="line-modified">3550 </span>
<span class="line-modified">3551   uint complexity_limit = 20;</span>
<span class="line-modified">3552   for (uint j = 0; j &lt; worklist.size(); j++) {</span>
<span class="line-modified">3553     if (j &gt;= complexity_limit) {</span>
<span class="line-modified">3554       return false;  // Bail out if processed too many nodes</span>
<span class="line-modified">3555     }</span>
<span class="line-modified">3556 </span>
<span class="line-modified">3557     Node* n = worklist.at(j);</span>
<span class="line-modified">3558     if (n == NULL)      continue;   // (can this really happen?)</span>
<span class="line-modified">3559     if (n-&gt;is_Proj())   n = n-&gt;in(0);</span>
<span class="line-modified">3560     if (n == this)      return false;  // found a cycle</span>
<span class="line-modified">3561     if (n-&gt;is_Con())    continue;</span>
<span class="line-modified">3562     if (n-&gt;is_Start())  continue;   // params, etc., are OK</span>
<span class="line-modified">3563     if (n-&gt;is_Root())   continue;   // even better</span>
<span class="line-modified">3564 </span>
<span class="line-modified">3565     // There cannot be any dependency if &#39;n&#39; is a CFG node that dominates the current allocation</span>
<span class="line-modified">3566     if (n-&gt;is_CFG() &amp;&amp; phase-&gt;is_dominator(n, allocation())) {</span>
<span class="line-modified">3567       continue;</span>
<span class="line-modified">3568     }</span>
<span class="line-modified">3569 </span>
<span class="line-modified">3570     Node* ctl = n-&gt;in(0);</span>
<span class="line-modified">3571     if (ctl != NULL &amp;&amp; !ctl-&gt;is_top()) {</span>
<span class="line-modified">3572       if (ctl-&gt;is_Proj())  ctl = ctl-&gt;in(0);</span>
<span class="line-modified">3573       if (ctl == this)  return false;</span>
<span class="line-modified">3574 </span>
<span class="line-modified">3575       // If we already know that the enclosing memory op is pinned right after</span>
<span class="line-modified">3576       // the init, then any control flow that the store has picked up</span>
<span class="line-added">3577       // must have preceded the init, or else be equal to the init.</span>
<span class="line-added">3578       // Even after loop optimizations (which might change control edges)</span>
<span class="line-added">3579       // a store is never pinned *before* the availability of its inputs.</span>
<span class="line-added">3580       if (!MemNode::all_controls_dominate(n, this))</span>
<span class="line-added">3581         return false;                  // failed to prove a good control</span>
<span class="line-added">3582     }</span>
<span class="line-added">3583 </span>
<span class="line-added">3584     // Check data edges for possible dependencies on &#39;this&#39;.</span>
<span class="line-added">3585     for (uint i = 1; i &lt; n-&gt;req(); i++) {</span>
<span class="line-added">3586       Node* m = n-&gt;in(i);</span>
<span class="line-added">3587       if (m == NULL || m == n || m-&gt;is_top())  continue;</span>
<span class="line-added">3588 </span>
<span class="line-added">3589       // Only process data inputs once</span>
<span class="line-added">3590       worklist.push(m);</span>
3591     }
3592   }
3593 
3594   return true;
3595 }
3596 
3597 // Here are all the checks a Store must pass before it can be moved into
3598 // an initialization.  Returns zero if a check fails.
3599 // On success, returns the (constant) offset to which the store applies,
3600 // within the initialized memory.
<span class="line-modified">3601 intptr_t InitializeNode::can_capture_store(StoreNode* st, PhaseGVN* phase, bool can_reshape) {</span>
3602   const int FAIL = 0;



3603   if (st-&gt;req() != MemNode::ValueIn + 1)
3604     return FAIL;                // an inscrutable StoreNode (card mark?)
3605   Node* ctl = st-&gt;in(MemNode::Control);
3606   if (!(ctl != NULL &amp;&amp; ctl-&gt;is_Proj() &amp;&amp; ctl-&gt;in(0) == this))
3607     return FAIL;                // must be unconditional after the initialization
3608   Node* mem = st-&gt;in(MemNode::Memory);
3609   if (!(mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0) == this))
3610     return FAIL;                // must not be preceded by other stores
3611   Node* adr = st-&gt;in(MemNode::Address);
3612   intptr_t offset;
3613   AllocateNode* alloc = AllocateNode::Ideal_allocation(adr, phase, offset);
3614   if (alloc == NULL)
3615     return FAIL;                // inscrutable address
3616   if (alloc != allocation())
3617     return FAIL;                // wrong allocation!  (store needs to float up)
<span class="line-added">3618   int size_in_bytes = st-&gt;memory_size();</span>
<span class="line-added">3619   if ((size_in_bytes != 0) &amp;&amp; (offset % size_in_bytes) != 0) {</span>
<span class="line-added">3620     return FAIL;                // mismatched access</span>
<span class="line-added">3621   }</span>
3622   Node* val = st-&gt;in(MemNode::ValueIn);
<span class="line-modified">3623 </span>
<span class="line-modified">3624   if (!detect_init_independence(val, phase))</span>
3625     return FAIL;                // stored value must be &#39;simple enough&#39;
3626 
3627   // The Store can be captured only if nothing after the allocation
3628   // and before the Store is using the memory location that the store
3629   // overwrites.
3630   bool failed = false;
3631   // If is_complete_with_arraycopy() is true the shape of the graph is
3632   // well defined and is safe so no need for extra checks.
3633   if (!is_complete_with_arraycopy()) {
3634     // We are going to look at each use of the memory state following
3635     // the allocation to make sure nothing reads the memory that the
3636     // Store writes.
3637     const TypePtr* t_adr = phase-&gt;type(adr)-&gt;isa_ptr();
3638     int alias_idx = phase-&gt;C-&gt;get_alias_index(t_adr);
3639     ResourceMark rm;
3640     Unique_Node_List mems;
3641     mems.push(mem);
3642     Node* unique_merge = NULL;
3643     for (uint next = 0; next &lt; mems.size(); ++next) {
3644       Node *m  = mems.at(next);
</pre>
<hr />
<pre>
3802 
3803 // Clone the given store, converting it into a raw store
3804 // initializing a field or element of my new object.
3805 // Caller is responsible for retiring the original store,
3806 // with subsume_node or the like.
3807 //
3808 // From the example above InitializeNode::InitializeNode,
3809 // here are the old stores to be captured:
3810 //   store1 = (StoreC init.Control init.Memory (+ oop 12) 1)
3811 //   store2 = (StoreC init.Control store1      (+ oop 14) 2)
3812 //
3813 // Here is the changed code; note the extra edges on init:
3814 //   alloc = (Allocate ...)
3815 //   rawoop = alloc.RawAddress
3816 //   rawstore1 = (StoreC alloc.Control alloc.Memory (+ rawoop 12) 1)
3817 //   rawstore2 = (StoreC alloc.Control alloc.Memory (+ rawoop 14) 2)
3818 //   init = (Initialize alloc.Control alloc.Memory rawoop
3819 //                      rawstore1 rawstore2)
3820 //
3821 Node* InitializeNode::capture_store(StoreNode* st, intptr_t start,
<span class="line-modified">3822                                     PhaseGVN* phase, bool can_reshape) {</span>
3823   assert(stores_are_sane(phase), &quot;&quot;);
3824 
3825   if (start &lt; 0)  return NULL;
3826   assert(can_capture_store(st, phase, can_reshape) == start, &quot;sanity&quot;);
3827 
3828   Compile* C = phase-&gt;C;
3829   int size_in_bytes = st-&gt;memory_size();
3830   int i = captured_store_insertion_point(start, size_in_bytes, phase);
3831   if (i == 0)  return NULL;     // bail out
3832   Node* prev_mem = NULL;        // raw memory for the captured store
3833   if (i &gt; 0) {
3834     prev_mem = in(i);           // there is a pre-existing store under this one
3835     set_req(i, C-&gt;top());       // temporarily disconnect it
3836     // See StoreNode::Ideal &#39;st-&gt;outcnt() == 1&#39; for the reason to disconnect.
3837   } else {
3838     i = -i;                     // no pre-existing store
3839     prev_mem = zero_memory();   // a slice of the newly allocated object
3840     if (i &gt; InitializeNode::RawStores &amp;&amp; in(i-1) == prev_mem)
3841       set_req(--i, C-&gt;top());   // reuse this edge; it has been folded away
3842     else
</pre>
<hr />
<pre>
4160       // We passed the current int, without fully initializing it.
4161       int_map_off = next_int_off;
4162       int_map &gt;&gt;= BytesPerInt;
4163     } else if (next_int_off &gt; this_int_off + BytesPerInt) {
4164       // We passed the current and next int.
4165       return this_int_off + BytesPerInt;
4166     }
4167   }
4168 
4169   return -1;
4170 }
4171 
4172 
4173 // Called when the associated AllocateNode is expanded into CFG.
4174 // At this point, we may perform additional optimizations.
4175 // Linearize the stores by ascending offset, to make memory
4176 // activity as coherent as possible.
4177 Node* InitializeNode::complete_stores(Node* rawctl, Node* rawmem, Node* rawptr,
4178                                       intptr_t header_size,
4179                                       Node* size_in_bytes,
<span class="line-modified">4180                                       PhaseIterGVN* phase) {</span>
4181   assert(!is_complete(), &quot;not already complete&quot;);
4182   assert(stores_are_sane(phase), &quot;&quot;);
4183   assert(allocation() != NULL, &quot;must be present&quot;);
4184 
4185   remove_extra_zeroes();
4186 
4187   if (ReduceFieldZeroing || ReduceBulkZeroing)
4188     // reduce instruction count for common initialization patterns
4189     coalesce_subword_stores(header_size, size_in_bytes, phase);
4190 
4191   Node* zmem = zero_memory();   // initially zero memory state
4192   Node* inits = zmem;           // accumulating a linearized chain of inits
4193   #ifdef ASSERT
4194   intptr_t first_offset = allocation()-&gt;minimum_header_size();
4195   intptr_t last_init_off = first_offset;  // previous init offset
4196   intptr_t last_init_end = first_offset;  // previous init offset+size
4197   intptr_t last_tile_end = first_offset;  // previous tile offset+size
4198   #endif
4199   intptr_t zeroes_done = header_size;
4200 
</pre>
<hr />
<pre>
4252           assert(next_full_store &gt;= zeroes_needed, &quot;must go forward&quot;);
4253           assert((next_full_store &amp; (BytesPerInt-1)) == 0, &quot;even boundary&quot;);
4254           zeroes_needed = next_full_store;
4255         }
4256       }
4257 
4258       if (zeroes_needed &gt; zeroes_done) {
4259         intptr_t zsize = zeroes_needed - zeroes_done;
4260         // Do some incremental zeroing on rawmem, in parallel with inits.
4261         zeroes_done = align_down(zeroes_done, BytesPerInt);
4262         rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
4263                                               zeroes_done, zeroes_needed,
4264                                               phase);
4265         zeroes_done = zeroes_needed;
4266         if (zsize &gt; InitArrayShortSize &amp;&amp; ++big_init_gaps &gt; 2)
4267           do_zeroing = false;   // leave the hole, next time
4268       }
4269     }
4270 
4271     // Collect the store and move on:
<span class="line-modified">4272     phase-&gt;replace_input_of(st, MemNode::Memory, inits);</span>
4273     inits = st;                 // put it on the linearized chain
4274     set_req(i, zmem);           // unhook from previous position
4275 
4276     if (zeroes_done == st_off)
4277       zeroes_done = next_init_off;
4278 
4279     assert(!do_zeroing || zeroes_done &gt;= next_init_off, &quot;don&#39;t miss any&quot;);
4280 
4281     #ifdef ASSERT
4282     // Various order invariants.  Weaker than stores_are_sane because
4283     // a large constant tile can be filled in by smaller non-constant stores.
4284     assert(st_off &gt;= last_init_off, &quot;inits do not reverse&quot;);
4285     last_init_off = st_off;
4286     const Type* val = NULL;
4287     if (st_size &gt;= BytesPerInt &amp;&amp;
4288         (val = phase-&gt;type(st-&gt;in(MemNode::ValueIn)))-&gt;singleton() &amp;&amp;
4289         (int)val-&gt;basic_type() &lt; (int)T_OBJECT) {
4290       assert(st_off &gt;= last_tile_end, &quot;tiles do not overlap&quot;);
4291       assert(st_off &gt;= last_init_end, &quot;tiles do not overwrite inits&quot;);
4292       last_tile_end = MAX2(last_tile_end, next_init_off);
</pre>
<hr />
<pre>
4473   if( new_base != NULL &amp;&amp; new_base-&gt;is_MergeMem() ) {
4474     MergeMemNode* mdef = new_base-&gt;as_MergeMem();
4475     assert(mdef-&gt;empty_memory() == empty_mem, &quot;consistent sentinels&quot;);
4476     for (MergeMemStream mms(this, mdef); mms.next_non_empty2(); ) {
4477       mms.set_memory(mms.memory2());
4478     }
4479     assert(base_memory() == mdef-&gt;base_memory(), &quot;&quot;);
4480   } else {
4481     set_base_memory(new_base);
4482   }
4483 }
4484 
4485 // Make a new, untransformed MergeMem with the same base as &#39;mem&#39;.
4486 // If mem is itself a MergeMem, populate the result with the same edges.
4487 MergeMemNode* MergeMemNode::make(Node* mem) {
4488   return new MergeMemNode(mem);
4489 }
4490 
4491 //------------------------------cmp--------------------------------------------
4492 uint MergeMemNode::hash() const { return NO_HASH; }
<span class="line-modified">4493 bool MergeMemNode::cmp( const Node &amp;n ) const {</span>
4494   return (&amp;n == this);          // Always fail except on self
4495 }
4496 
4497 //------------------------------Identity---------------------------------------
4498 Node* MergeMemNode::Identity(PhaseGVN* phase) {
4499   // Identity if this merge point does not record any interesting memory
4500   // disambiguations.
4501   Node* base_mem = base_memory();
4502   Node* empty_mem = empty_memory();
4503   if (base_mem != empty_mem) {  // Memory path is not dead?
4504     for (uint i = Compile::AliasIdxRaw; i &lt; req(); i++) {
4505       Node* mem = in(i);
4506       if (mem != empty_mem &amp;&amp; mem != base_mem) {
4507         return this;            // Many memory splits; no change
4508       }
4509     }
4510   }
4511   return base_mem;              // No memory splits; ID on the one true input
4512 }
4513 
</pre>
</td>
</tr>
</table>
<center><a href="matcher.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="memnode.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>