<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/locknode.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="live.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="locknode.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/locknode.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 38 }
 39 
 40 uint BoxLockNode::size_of() const { return sizeof(*this); }
 41 
 42 BoxLockNode::BoxLockNode( int slot ) : Node( Compile::current()-&gt;root() ),
 43                                        _slot(slot), _is_eliminated(false) {
 44   init_class_id(Class_BoxLock);
 45   init_flags(Flag_rematerialize);
 46   OptoReg::Name reg = OptoReg::stack2reg(_slot);
 47   _inmask.Insert(reg);
 48 }
 49 
 50 //-----------------------------hash--------------------------------------------
 51 uint BoxLockNode::hash() const {
 52   if (EliminateNestedLocks)
 53     return NO_HASH; // Each locked region has own BoxLock node
 54   return Node::hash() + _slot + (_is_eliminated ? Compile::current()-&gt;fixed_slots() : 0);
 55 }
 56 
 57 //------------------------------cmp--------------------------------------------
<span class="line-modified"> 58 uint BoxLockNode::cmp( const Node &amp;n ) const {</span>
 59   if (EliminateNestedLocks)
 60     return (&amp;n == this); // Always fail except on self
 61   const BoxLockNode &amp;bn = (const BoxLockNode &amp;)n;
 62   return bn._slot == _slot &amp;&amp; bn._is_eliminated == _is_eliminated;
 63 }
 64 
 65 BoxLockNode* BoxLockNode::box_node(Node* box) {
 66   // Chase down the BoxNode after RA which may spill box nodes.
 67   while (!box-&gt;is_BoxLock()) {
 68     //    if (box_node-&gt;is_SpillCopy()) {
 69     //      Node *m = box_node-&gt;in(1);
 70     //      if (m-&gt;is_Mach() &amp;&amp; m-&gt;as_Mach()-&gt;ideal_Opcode() == Op_StoreP) {
 71     //        box_node = m-&gt;in(m-&gt;as_Mach()-&gt;operand_index(2));
 72     //        continue;
 73     //      }
 74     //    }
 75     assert(box-&gt;is_SpillCopy() || box-&gt;is_Phi(), &quot;Bad spill of Lock.&quot;);
 76     // Only BoxLock nodes with the same stack slot are merged.
 77     // So it is enough to trace one path to find the slot value.
 78     box = box-&gt;in(1);
</pre>
<hr />
<pre>
122     // be different from the locked object. It could be Phi node of different
123     // cast nodes which point to this locked object.
124     // We assume that no other objects could be referenced in monitor info
125     // associated with this BoxLock node because all associated locks and
126     // unlocks are reference only this one object.
127   }
128 #endif
129   if (unique_lock != NULL &amp;&amp; has_one_lock) {
130     *unique_lock = lock;
131   }
132   return true;
133 }
134 
135 //=============================================================================
136 //-----------------------------hash--------------------------------------------
137 uint FastLockNode::hash() const { return NO_HASH; }
138 
139 uint FastLockNode::size_of() const { return sizeof(*this); }
140 
141 //------------------------------cmp--------------------------------------------
<span class="line-modified">142 uint FastLockNode::cmp( const Node &amp;n ) const {</span>
143   return (&amp;n == this);                // Always fail except on self
144 }
145 
146 //=============================================================================
147 //-----------------------------hash--------------------------------------------
148 uint FastUnlockNode::hash() const { return NO_HASH; }
149 
150 //------------------------------cmp--------------------------------------------
<span class="line-modified">151 uint FastUnlockNode::cmp( const Node &amp;n ) const {</span>
152   return (&amp;n == this);                // Always fail except on self
153 }
154 
155 //
156 // Create a counter which counts the number of times this lock is acquired
157 //
158 void FastLockNode::create_lock_counter(JVMState* state) {
159   BiasedLockingNamedCounter* blnc = (BiasedLockingNamedCounter*)
160            OptoRuntime::new_named_counter(state, NamedCounter::BiasedLockingCounter);
161   _counters = blnc-&gt;counters();
162 }
163 
164 void FastLockNode::create_rtm_lock_counter(JVMState* state) {
165 #if INCLUDE_RTM_OPT
166   Compile* C = Compile::current();
167   if (C-&gt;profile_rtm() || (PrintPreciseRTMLockingStatistics &amp;&amp; C-&gt;use_rtm())) {
168     RTMLockingNamedCounter* rlnc = (RTMLockingNamedCounter*)
169            OptoRuntime::new_named_counter(state, NamedCounter::RTMLockingCounter);
170     _rtm_counters = rlnc-&gt;counters();
171     if (UseRTMForStackLocks) {
</pre>
</td>
<td>
<hr />
<pre>
 38 }
 39 
 40 uint BoxLockNode::size_of() const { return sizeof(*this); }
 41 
 42 BoxLockNode::BoxLockNode( int slot ) : Node( Compile::current()-&gt;root() ),
 43                                        _slot(slot), _is_eliminated(false) {
 44   init_class_id(Class_BoxLock);
 45   init_flags(Flag_rematerialize);
 46   OptoReg::Name reg = OptoReg::stack2reg(_slot);
 47   _inmask.Insert(reg);
 48 }
 49 
 50 //-----------------------------hash--------------------------------------------
 51 uint BoxLockNode::hash() const {
 52   if (EliminateNestedLocks)
 53     return NO_HASH; // Each locked region has own BoxLock node
 54   return Node::hash() + _slot + (_is_eliminated ? Compile::current()-&gt;fixed_slots() : 0);
 55 }
 56 
 57 //------------------------------cmp--------------------------------------------
<span class="line-modified"> 58 bool BoxLockNode::cmp( const Node &amp;n ) const {</span>
 59   if (EliminateNestedLocks)
 60     return (&amp;n == this); // Always fail except on self
 61   const BoxLockNode &amp;bn = (const BoxLockNode &amp;)n;
 62   return bn._slot == _slot &amp;&amp; bn._is_eliminated == _is_eliminated;
 63 }
 64 
 65 BoxLockNode* BoxLockNode::box_node(Node* box) {
 66   // Chase down the BoxNode after RA which may spill box nodes.
 67   while (!box-&gt;is_BoxLock()) {
 68     //    if (box_node-&gt;is_SpillCopy()) {
 69     //      Node *m = box_node-&gt;in(1);
 70     //      if (m-&gt;is_Mach() &amp;&amp; m-&gt;as_Mach()-&gt;ideal_Opcode() == Op_StoreP) {
 71     //        box_node = m-&gt;in(m-&gt;as_Mach()-&gt;operand_index(2));
 72     //        continue;
 73     //      }
 74     //    }
 75     assert(box-&gt;is_SpillCopy() || box-&gt;is_Phi(), &quot;Bad spill of Lock.&quot;);
 76     // Only BoxLock nodes with the same stack slot are merged.
 77     // So it is enough to trace one path to find the slot value.
 78     box = box-&gt;in(1);
</pre>
<hr />
<pre>
122     // be different from the locked object. It could be Phi node of different
123     // cast nodes which point to this locked object.
124     // We assume that no other objects could be referenced in monitor info
125     // associated with this BoxLock node because all associated locks and
126     // unlocks are reference only this one object.
127   }
128 #endif
129   if (unique_lock != NULL &amp;&amp; has_one_lock) {
130     *unique_lock = lock;
131   }
132   return true;
133 }
134 
135 //=============================================================================
136 //-----------------------------hash--------------------------------------------
137 uint FastLockNode::hash() const { return NO_HASH; }
138 
139 uint FastLockNode::size_of() const { return sizeof(*this); }
140 
141 //------------------------------cmp--------------------------------------------
<span class="line-modified">142 bool FastLockNode::cmp( const Node &amp;n ) const {</span>
143   return (&amp;n == this);                // Always fail except on self
144 }
145 
146 //=============================================================================
147 //-----------------------------hash--------------------------------------------
148 uint FastUnlockNode::hash() const { return NO_HASH; }
149 
150 //------------------------------cmp--------------------------------------------
<span class="line-modified">151 bool FastUnlockNode::cmp( const Node &amp;n ) const {</span>
152   return (&amp;n == this);                // Always fail except on self
153 }
154 
155 //
156 // Create a counter which counts the number of times this lock is acquired
157 //
158 void FastLockNode::create_lock_counter(JVMState* state) {
159   BiasedLockingNamedCounter* blnc = (BiasedLockingNamedCounter*)
160            OptoRuntime::new_named_counter(state, NamedCounter::BiasedLockingCounter);
161   _counters = blnc-&gt;counters();
162 }
163 
164 void FastLockNode::create_rtm_lock_counter(JVMState* state) {
165 #if INCLUDE_RTM_OPT
166   Compile* C = Compile::current();
167   if (C-&gt;profile_rtm() || (PrintPreciseRTMLockingStatistics &amp;&amp; C-&gt;use_rtm())) {
168     RTMLockingNamedCounter* rlnc = (RTMLockingNamedCounter*)
169            OptoRuntime::new_named_counter(state, NamedCounter::RTMLockingCounter);
170     _rtm_counters = rlnc-&gt;counters();
171     if (UseRTMForStackLocks) {
</pre>
</td>
</tr>
</table>
<center><a href="live.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="locknode.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>