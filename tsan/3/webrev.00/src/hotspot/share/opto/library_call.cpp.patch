diff a/src/hotspot/share/opto/library_call.cpp b/src/hotspot/share/opto/library_call.cpp
--- a/src/hotspot/share/opto/library_call.cpp
+++ b/src/hotspot/share/opto/library_call.cpp
@@ -30,10 +30,11 @@
 #include "compiler/compileBroker.hpp"
 #include "compiler/compileLog.hpp"
 #include "gc/shared/barrierSet.hpp"
 #include "jfr/support/jfrIntrinsics.hpp"
 #include "memory/resourceArea.hpp"
+#include "oops/klass.inline.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "opto/addnode.hpp"
 #include "opto/arraycopynode.hpp"
 #include "opto/c2compiler.hpp"
 #include "opto/callGenerator.hpp"
@@ -55,11 +56,11 @@
 #include "prims/nativeLookup.hpp"
 #include "prims/unsafe.hpp"
 #include "runtime/objectMonitor.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "utilities/macros.hpp"
-
+#include "utilities/powerOfTwo.hpp"
 
 class LibraryIntrinsic : public InlineCallGenerator {
   // Extend the set of intrinsics known to the runtime:
  public:
  private:
@@ -225,10 +226,11 @@
   bool inline_string_char_access(bool is_store);
   Node* round_double_node(Node* n);
   bool runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName);
   bool inline_math_native(vmIntrinsics::ID id);
   bool inline_math(vmIntrinsics::ID id);
+  bool inline_double_math(vmIntrinsics::ID id);
   template <typename OverflowOp>
   bool inline_math_overflow(Node* arg1, Node* arg2);
   void inline_math_mathExact(Node* math, Node* test);
   bool inline_math_addExactI(bool is_increment);
   bool inline_math_addExactL(bool is_increment);
@@ -250,19 +252,20 @@
   DecoratorSet mo_decorator_for_access_kind(AccessKind kind);
   bool inline_unsafe_access(bool is_store, BasicType type, AccessKind kind, bool is_unaligned);
   static bool klass_needs_init_guard(Node* kls);
   bool inline_unsafe_allocate();
   bool inline_unsafe_newArray(bool uninitialized);
+  bool inline_unsafe_writeback0();
+  bool inline_unsafe_writebackSync0(bool is_pre);
   bool inline_unsafe_copyMemory();
   bool inline_native_currentThread();
 
   bool inline_native_time_funcs(address method, const char* funcName);
 #ifdef JFR_HAVE_INTRINSICS
   bool inline_native_classID();
   bool inline_native_getEventWriter();
 #endif
-  bool inline_native_isInterrupted();
   bool inline_native_Class_query(vmIntrinsics::ID id);
   bool inline_native_subtype_check();
   bool inline_native_getLength();
   bool inline_array_copyOf(bool is_copyOfRange);
   bool inline_array_equals(StrIntrinsicNode::ArgEnc ae);
@@ -290,12 +293,14 @@
   bool inline_number_methods(vmIntrinsics::ID id);
   bool inline_reference_get();
   bool inline_Class_cast();
   bool inline_aescrypt_Block(vmIntrinsics::ID id);
   bool inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id);
+  bool inline_electronicCodeBook_AESCrypt(vmIntrinsics::ID id);
   bool inline_counterMode_AESCrypt(vmIntrinsics::ID id);
   Node* inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting);
+  Node* inline_electronicCodeBook_AESCrypt_predicate(bool decrypting);
   Node* inline_counterMode_AESCrypt_predicate();
   Node* get_key_start_from_aescrypt_object(Node* aescrypt_object);
   Node* get_original_key_start_from_aescrypt_object(Node* aescrypt_object);
   bool inline_ghash_processBlocks();
   bool inline_base64_encodeBlock();
@@ -320,10 +325,11 @@
   bool inline_hasNegatives();
   bool inline_squareToLen();
   bool inline_mulAdd();
   bool inline_montgomeryMultiply();
   bool inline_montgomerySquare();
+  bool inline_bigIntegerShift(bool isRightShift);
   bool inline_vectorizedMismatch();
   bool inline_fma(vmIntrinsics::ID id);
   bool inline_character_compare(vmIntrinsics::ID id);
   bool inline_fp_min_max(vmIntrinsics::ID id);
 
@@ -527,14 +533,20 @@
   switch (intrinsic_id()) {
   case vmIntrinsics::_hashCode:                 return inline_native_hashcode(intrinsic()->is_virtual(), !is_static);
   case vmIntrinsics::_identityHashCode:         return inline_native_hashcode(/*!virtual*/ false,         is_static);
   case vmIntrinsics::_getClass:                 return inline_native_getClass();
 
+  case vmIntrinsics::_ceil:
+  case vmIntrinsics::_floor:
+  case vmIntrinsics::_rint:
   case vmIntrinsics::_dsin:
   case vmIntrinsics::_dcos:
   case vmIntrinsics::_dtan:
   case vmIntrinsics::_dabs:
+  case vmIntrinsics::_fabs:
+  case vmIntrinsics::_iabs:
+  case vmIntrinsics::_labs:
   case vmIntrinsics::_datan2:
   case vmIntrinsics::_dsqrt:
   case vmIntrinsics::_dexp:
   case vmIntrinsics::_dlog:
   case vmIntrinsics::_dlog10:
@@ -738,19 +750,21 @@
   case vmIntrinsics::_fullFence:                return inline_unsafe_fence(intrinsic_id());
 
   case vmIntrinsics::_onSpinWait:               return inline_onspinwait();
 
   case vmIntrinsics::_currentThread:            return inline_native_currentThread();
-  case vmIntrinsics::_isInterrupted:            return inline_native_isInterrupted();
 
 #ifdef JFR_HAVE_INTRINSICS
   case vmIntrinsics::_counterTime:              return inline_native_time_funcs(CAST_FROM_FN_PTR(address, JFR_TIME_FUNCTION), "counterTime");
   case vmIntrinsics::_getClassId:               return inline_native_classID();
   case vmIntrinsics::_getEventWriter:           return inline_native_getEventWriter();
 #endif
   case vmIntrinsics::_currentTimeMillis:        return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeMillis), "currentTimeMillis");
   case vmIntrinsics::_nanoTime:                 return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeNanos), "nanoTime");
+  case vmIntrinsics::_writeback0:               return inline_unsafe_writeback0();
+  case vmIntrinsics::_writebackPreSync0:        return inline_unsafe_writebackSync0(true);
+  case vmIntrinsics::_writebackPostSync0:       return inline_unsafe_writebackSync0(false);
   case vmIntrinsics::_allocateInstance:         return inline_unsafe_allocate();
   case vmIntrinsics::_copyMemory:               return inline_unsafe_copyMemory();
   case vmIntrinsics::_getLength:                return inline_native_getLength();
   case vmIntrinsics::_copyOf:                   return inline_array_copyOf(false);
   case vmIntrinsics::_copyOfRange:              return inline_array_copyOf(true);
@@ -801,10 +815,14 @@
 
   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
     return inline_cipherBlockChaining_AESCrypt(intrinsic_id());
 
+  case vmIntrinsics::_electronicCodeBook_encryptAESCrypt:
+  case vmIntrinsics::_electronicCodeBook_decryptAESCrypt:
+    return inline_electronicCodeBook_AESCrypt(intrinsic_id());
+
   case vmIntrinsics::_counterMode_AESCrypt:
     return inline_counterMode_AESCrypt(intrinsic_id());
 
   case vmIntrinsics::_sha_implCompress:
   case vmIntrinsics::_sha2_implCompress:
@@ -826,10 +844,15 @@
   case vmIntrinsics::_montgomeryMultiply:
     return inline_montgomeryMultiply();
   case vmIntrinsics::_montgomerySquare:
     return inline_montgomerySquare();
 
+  case vmIntrinsics::_bigIntegerRightShiftWorker:
+    return inline_bigIntegerShift(true);
+  case vmIntrinsics::_bigIntegerLeftShiftWorker:
+    return inline_bigIntegerShift(false);
+
   case vmIntrinsics::_vectorizedMismatch:
     return inline_vectorizedMismatch();
 
   case vmIntrinsics::_ghash_processBlocks:
     return inline_ghash_processBlocks();
@@ -906,10 +929,14 @@
   switch (intrinsic_id()) {
   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
     return inline_cipherBlockChaining_AESCrypt_predicate(false);
   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
     return inline_cipherBlockChaining_AESCrypt_predicate(true);
+  case vmIntrinsics::_electronicCodeBook_encryptAESCrypt:
+    return inline_electronicCodeBook_AESCrypt_predicate(false);
+  case vmIntrinsics::_electronicCodeBook_decryptAESCrypt:
+    return inline_electronicCodeBook_AESCrypt_predicate(true);
   case vmIntrinsics::_counterMode_AESCrypt:
     return inline_counterMode_AESCrypt_predicate();
   case vmIntrinsics::_digestBase_implCompressMB:
     return inline_digestBase_implCompressMB_predicate(predicate);
 
@@ -1064,11 +1091,11 @@
 Node* LibraryCallKit::generate_current_thread(Node* &tls_output) {
   ciKlass*    thread_klass = env()->Thread_klass();
   const Type* thread_type  = TypeOopPtr::make_from_klass(thread_klass)->cast_to_ptr_type(TypePtr::NotNull);
   Node* thread = _gvn.transform(new ThreadLocalNode());
   Node* p = basic_plus_adr(top()/*!oop*/, thread, in_bytes(JavaThread::threadObj_offset()));
-  Node* threadObj = make_load(NULL, p, thread_type, T_OBJECT, MemNode::unordered);
+  Node* threadObj = _gvn.transform(LoadNode::make(_gvn, NULL, immutable_memory(), p, p->bottom_type()->is_ptr(), thread_type, T_OBJECT, MemNode::unordered));
   tls_output = thread;
   return threadObj;
 }
 
 
@@ -1112,13 +1139,10 @@
   Node* arg2 = argument(1);
 
   arg1 = must_be_not_null(arg1, true);
   arg2 = must_be_not_null(arg2, true);
 
-  arg1 = access_resolve(arg1, ACCESS_READ);
-  arg2 = access_resolve(arg2, ACCESS_READ);
-
   // Get start addr and length of first argument
   Node* arg1_start  = array_element_address(arg1, intcon(0), T_BYTE);
   Node* arg1_cnt    = load_array_length(arg1);
 
   // Get start addr and length of second argument
@@ -1142,13 +1166,10 @@
   if (!stopped()) {
 
     arg1 = must_be_not_null(arg1, true);
     arg2 = must_be_not_null(arg2, true);
 
-    arg1 = access_resolve(arg1, ACCESS_READ);
-    arg2 = access_resolve(arg2, ACCESS_READ);
-
     // Get start addr and length of first argument
     Node* arg1_start  = array_element_address(arg1, intcon(0), T_BYTE);
     Node* arg1_cnt    = load_array_length(arg1);
 
     // Get start addr and length of second argument
@@ -1185,13 +1206,10 @@
 bool LibraryCallKit::inline_array_equals(StrIntrinsicNode::ArgEnc ae) {
   assert(ae == StrIntrinsicNode::UU || ae == StrIntrinsicNode::LL, "unsupported array types");
   Node* arg1 = argument(0);
   Node* arg2 = argument(1);
 
-  arg1 = access_resolve(arg1, ACCESS_READ);
-  arg2 = access_resolve(arg2, ACCESS_READ);
-
   const TypeAryPtr* mtype = (ae == StrIntrinsicNode::UU) ? TypeAryPtr::CHARS : TypeAryPtr::BYTES;
   set_result(_gvn.transform(new AryEqNode(control(), memory(mtype), arg1, arg2, ae)));
   clear_upper_avx();
 
   return true;
@@ -1214,11 +1232,10 @@
   // Range checks
   generate_string_range_check(ba, offset, len, false);
   if (stopped()) {
     return true;
   }
-  ba = access_resolve(ba, ACCESS_READ);
   Node* ba_start = array_element_address(ba, offset, T_BYTE);
   Node* result = new HasNegativesNode(control(), memory(TypeAryPtr::BYTES), ba_start, len);
   set_result(_gvn.transform(result));
   return true;
 }
@@ -1285,13 +1302,10 @@
   Node*       result_phi = new PhiNode(result_rgn, TypeInt::INT);
 
   src = must_be_not_null(src, true);
   tgt = must_be_not_null(tgt, true);
 
-  src = access_resolve(src, ACCESS_READ);
-  tgt = access_resolve(tgt, ACCESS_READ);
-
   // Get start addr and length of source string
   Node* src_start = array_element_address(src, intcon(0), T_BYTE);
   Node* src_count = load_array_length(src);
 
   // Get start addr and length of substring
@@ -1335,13 +1349,10 @@
   Node* from_index  = argument(4); // char index
 
   src = must_be_not_null(src, true);
   tgt = must_be_not_null(tgt, true);
 
-  src = access_resolve(src, ACCESS_READ);
-  tgt = access_resolve(tgt, ACCESS_READ);
-
   // Multiply byte array index by 2 if String is UTF16 encoded
   Node* src_offset = (ae == StrIntrinsicNode::LL) ? from_index : _gvn.transform(new LShiftINode(from_index, intcon(1)));
   src_count = _gvn.transform(new SubINode(src_count, from_index));
   Node* src_start = array_element_address(src, src_offset, T_BYTE);
   Node* tgt_start = array_element_address(tgt, intcon(0), T_BYTE);
@@ -1424,11 +1435,10 @@
   Node* tgt         = argument(1); // tgt is int ch
   Node* from_index  = argument(2);
   Node* max         = argument(3);
 
   src = must_be_not_null(src, true);
-  src = access_resolve(src, ACCESS_READ);
 
   Node* src_offset = _gvn.transform(new LShiftINode(from_index, intcon(1)));
   Node* src_start = array_element_address(src, src_offset, T_BYTE);
   Node* src_count = _gvn.transform(new SubINode(max, from_index));
 
@@ -1515,13 +1525,10 @@
   generate_string_range_check(dst, dst_offset, length, convert_dst);
   if (stopped()) {
     return true;
   }
 
-  src = access_resolve(src, ACCESS_READ);
-  dst = access_resolve(dst, ACCESS_WRITE);
-
   Node* src_start = array_element_address(src, src_offset, src_elem);
   Node* dst_start = array_element_address(dst, dst_offset, dst_elem);
   // 'src_start' points to src array + scaled offset
   // 'dst_start' points to dst array + scaled offset
   Node* count = NULL;
@@ -1608,11 +1615,10 @@
     Node* klass_node = makecon(TypeKlassPtr::make(ciTypeArrayKlass::make(T_BYTE)));
     newcopy = new_array(klass_node, size, 0);  // no arguments to push
     AllocateArrayNode* alloc = tightly_coupled_allocation(newcopy, NULL);
 
     // Calculate starting addresses.
-    value = access_resolve(value, ACCESS_READ);
     Node* src_start = array_element_address(value, offset, T_CHAR);
     Node* dst_start = basic_plus_adr(newcopy, arrayOopDesc::base_offset_in_bytes(T_BYTE));
 
     // Check if src array address is aligned to HeapWordSize (dst is always aligned)
     const TypeInt* toffset = gvn().type(offset)->is_int();
@@ -1692,13 +1698,10 @@
   if (stopped()) {
     return true;
   }
 
   if (!stopped()) {
-    src = access_resolve(src, ACCESS_READ);
-    dst = access_resolve(dst, ACCESS_WRITE);
-
     // Calculate starting addresses.
     Node* src_start = array_element_address(src, src_begin, T_BYTE);
     Node* dst_start = array_element_address(dst, dst_begin, T_CHAR);
 
     // Check if array addresses are aligned to HeapWordSize
@@ -1763,11 +1766,10 @@
   if (!is_store && value->is_Con() && index->is_Con()) {
     return false;
   }
 
   value = must_be_not_null(value, true);
-  value = access_resolve(value, is_store ? ACCESS_WRITE : ACCESS_READ);
 
   Node* adr = array_element_address(value, index, T_CHAR);
   if (adr->is_top()) {
     return false;
   }
@@ -1781,26 +1783,53 @@
 }
 
 //--------------------------round_double_node--------------------------------
 // Round a double node if necessary.
 Node* LibraryCallKit::round_double_node(Node* n) {
-  if (Matcher::strict_fp_requires_explicit_rounding && UseSSE <= 1)
-    n = _gvn.transform(new RoundDoubleNode(0, n));
+  if (Matcher::strict_fp_requires_explicit_rounding) {
+#ifdef IA32
+    if (UseSSE < 2) {
+      n = _gvn.transform(new RoundDoubleNode(NULL, n));
+    }
+#else
+    Unimplemented();
+#endif // IA32
+  }
   return n;
 }
 
 //------------------------------inline_math-----------------------------------
 // public static double Math.abs(double)
 // public static double Math.sqrt(double)
 // public static double Math.log(double)
 // public static double Math.log10(double)
-bool LibraryCallKit::inline_math(vmIntrinsics::ID id) {
+bool LibraryCallKit::inline_double_math(vmIntrinsics::ID id) {
   Node* arg = round_double_node(argument(0));
   Node* n = NULL;
   switch (id) {
   case vmIntrinsics::_dabs:   n = new AbsDNode(                arg);  break;
   case vmIntrinsics::_dsqrt:  n = new SqrtDNode(C, control(),  arg);  break;
+  case vmIntrinsics::_ceil:   n = RoundDoubleModeNode::make(_gvn, arg, RoundDoubleModeNode::rmode_ceil); break;
+  case vmIntrinsics::_floor:  n = RoundDoubleModeNode::make(_gvn, arg, RoundDoubleModeNode::rmode_floor); break;
+  case vmIntrinsics::_rint:   n = RoundDoubleModeNode::make(_gvn, arg, RoundDoubleModeNode::rmode_rint); break;
+  default:  fatal_unexpected_iid(id);  break;
+  }
+  set_result(_gvn.transform(n));
+  return true;
+}
+
+//------------------------------inline_math-----------------------------------
+// public static float Math.abs(float)
+// public static int Math.abs(int)
+// public static long Math.abs(long)
+bool LibraryCallKit::inline_math(vmIntrinsics::ID id) {
+  Node* arg = argument(0);
+  Node* n = NULL;
+  switch (id) {
+  case vmIntrinsics::_fabs:   n = new AbsFNode(                arg);  break;
+  case vmIntrinsics::_iabs:   n = new AbsINode(                arg);  break;
+  case vmIntrinsics::_labs:   n = new AbsLNode(                arg);  break;
   default:  fatal_unexpected_iid(id);  break;
   }
   set_result(_gvn.transform(n));
   return true;
 }
@@ -1853,12 +1882,18 @@
     return StubRoutines::dlog10() != NULL ?
       runtime_math(OptoRuntime::Math_D_D_Type(), StubRoutines::dlog10(), "dlog10") :
       runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog10), "LOG10");
 
     // These intrinsics are supported on all hardware
-  case vmIntrinsics::_dsqrt:  return Matcher::match_rule_supported(Op_SqrtD) ? inline_math(id) : false;
-  case vmIntrinsics::_dabs:   return Matcher::has_match_rule(Op_AbsD)   ? inline_math(id) : false;
+  case vmIntrinsics::_ceil:
+  case vmIntrinsics::_floor:
+  case vmIntrinsics::_rint:   return Matcher::match_rule_supported(Op_RoundDoubleMode) ? inline_double_math(id) : false;
+  case vmIntrinsics::_dsqrt:  return Matcher::match_rule_supported(Op_SqrtD) ? inline_double_math(id) : false;
+  case vmIntrinsics::_dabs:   return Matcher::has_match_rule(Op_AbsD)   ? inline_double_math(id) : false;
+  case vmIntrinsics::_fabs:   return Matcher::match_rule_supported(Op_AbsF)   ? inline_math(id) : false;
+  case vmIntrinsics::_iabs:   return Matcher::match_rule_supported(Op_AbsI)   ? inline_math(id) : false;
+  case vmIntrinsics::_labs:   return Matcher::match_rule_supported(Op_AbsL)   ? inline_math(id) : false;
 
   case vmIntrinsics::_dexp:
     return StubRoutines::dexp() != NULL ?
       runtime_math(OptoRuntime::Math_D_D_Type(), StubRoutines::dexp(),  "dexp") :
       runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dexp),  "EXP");
@@ -2229,11 +2264,10 @@
         return basic_plus_adr(top(), raw_base, offset);
       }
     }
     // We don't know if it's an on heap or off heap access. Fall back
     // to raw memory access.
-    base = access_resolve(base, decorators);
     Node* raw = _gvn.transform(new CheckCastPPNode(control(), base, TypeRawPtr::BOTTOM));
     return basic_plus_adr(top(), raw, offset);
   } else {
     assert(base == uncasted_base, "unexpected base change");
     // We know it's an on heap access so base can't be null
@@ -2341,11 +2375,11 @@
   DecoratorSet decorators = C2_UNSAFE_ACCESS;
   guarantee(!is_store || kind != Acquire, "Acquire accesses can be produced only for loads");
   guarantee( is_store || kind != Release, "Release accesses can be produced only for stores");
   assert(type != T_OBJECT || !unaligned, "unaligned access not supported with object type");
 
-  if (type == T_OBJECT || type == T_ARRAY) {
+  if (is_reference_type(type)) {
     decorators |= ON_UNKNOWN_OOP_REF;
   }
 
   if (unaligned) {
     decorators |= C2_UNALIGNED;
@@ -2398,26 +2432,33 @@
          "fieldOffset must be byte-scaled");
   // 32-bit machines ignore the high half!
   offset = ConvL2X(offset);
   adr = make_unsafe_address(base, offset, is_store ? ACCESS_WRITE : ACCESS_READ, type, kind == Relaxed);
 
-  if (_gvn.type(base)->isa_ptr() != TypePtr::NULL_PTR) {
-    heap_base_oop = base;
-  } else if (type == T_OBJECT) {
-    return false; // off-heap oop accesses are not supported
+  if (_gvn.type(base)->isa_ptr() == TypePtr::NULL_PTR) {
+    if (type != T_OBJECT) {
+      decorators |= IN_NATIVE; // off-heap primitive access
+    } else {
+      return false; // off-heap oop accesses are not supported
+    }
+  } else {
+    heap_base_oop = base; // on-heap or mixed access
   }
 
   // Can base be NULL? Otherwise, always on-heap access.
-  bool can_access_non_heap = TypePtr::NULL_PTR->higher_equal(_gvn.type(heap_base_oop));
+  bool can_access_non_heap = TypePtr::NULL_PTR->higher_equal(_gvn.type(base));
 
   if (!can_access_non_heap) {
     decorators |= IN_HEAP;
   }
 
   val = is_store ? argument(4) : NULL;
 
-  const TypePtr *adr_type = _gvn.type(adr)->isa_ptr();
+  const TypePtr* adr_type = _gvn.type(adr)->isa_ptr();
+  if (adr_type == TypePtr::NULL_PTR) {
+    return false; // off-heap access with zero address
+  }
 
   // Try to categorize the address.
   Compile::AliasType* alias_type = C->alias_type(adr_type);
   assert(alias_type->index() != Compile::AliasIdxBot, "no bare pointers here");
 
@@ -2686,11 +2727,11 @@
   const TypePtr *adr_type = _gvn.type(adr)->isa_ptr();
 
   Compile::AliasType* alias_type = C->alias_type(adr_type);
   BasicType bt = alias_type->basic_type();
   if (bt != T_ILLEGAL &&
-      ((bt == T_OBJECT || bt == T_ARRAY) != (type == T_OBJECT))) {
+      (is_reference_type(bt) != (type == T_OBJECT))) {
     // Don't intrinsify mismatched object accesses.
     return false;
   }
 
   // For CAS, unlike inline_unsafe_access, there seems no point in
@@ -2723,11 +2764,11 @@
     return true;
   }
 
   int alias_idx = C->get_alias_index(adr_type);
 
-  if (type == T_OBJECT || type == T_ARRAY) {
+  if (is_reference_type(type)) {
     decorators |= IN_HEAP | ON_UNKNOWN_OOP_REF;
 
     // Transformation of a value which could be NULL pointer (CastPP #NULL)
     // could be delayed during Parse (for example, in adjust_map_after_if()).
     // Execute transformation here to avoid barrier generation in such case.
@@ -2809,10 +2850,59 @@
   ciInstanceKlass* ik = klsptr->klass()->as_instance_klass();
   // don't need a guard for a klass that is already initialized
   return !ik->is_initialized();
 }
 
+//----------------------------inline_unsafe_writeback0-------------------------
+// public native void Unsafe.writeback0(long address)
+bool LibraryCallKit::inline_unsafe_writeback0() {
+  if (!Matcher::has_match_rule(Op_CacheWB)) {
+    return false;
+  }
+#ifndef PRODUCT
+  assert(Matcher::has_match_rule(Op_CacheWBPreSync), "found match rule for CacheWB but not CacheWBPreSync");
+  assert(Matcher::has_match_rule(Op_CacheWBPostSync), "found match rule for CacheWB but not CacheWBPostSync");
+  ciSignature* sig = callee()->signature();
+  assert(sig->type_at(0)->basic_type() == T_LONG, "Unsafe_writeback0 address is long!");
+#endif
+  null_check_receiver();  // null-check, then ignore
+  Node *addr = argument(1);
+  addr = new CastX2PNode(addr);
+  addr = _gvn.transform(addr);
+  Node *flush = new CacheWBNode(control(), memory(TypeRawPtr::BOTTOM), addr);
+  flush = _gvn.transform(flush);
+  set_memory(flush, TypeRawPtr::BOTTOM);
+  return true;
+}
+
+//----------------------------inline_unsafe_writeback0-------------------------
+// public native void Unsafe.writeback0(long address)
+bool LibraryCallKit::inline_unsafe_writebackSync0(bool is_pre) {
+  if (is_pre && !Matcher::has_match_rule(Op_CacheWBPreSync)) {
+    return false;
+  }
+  if (!is_pre && !Matcher::has_match_rule(Op_CacheWBPostSync)) {
+    return false;
+  }
+#ifndef PRODUCT
+  assert(Matcher::has_match_rule(Op_CacheWB),
+         (is_pre ? "found match rule for CacheWBPreSync but not CacheWB"
+                : "found match rule for CacheWBPostSync but not CacheWB"));
+
+#endif
+  null_check_receiver();  // null-check, then ignore
+  Node *sync;
+  if (is_pre) {
+    sync = new CacheWBPreSyncNode(control(), memory(TypeRawPtr::BOTTOM));
+  } else {
+    sync = new CacheWBPostSyncNode(control(), memory(TypeRawPtr::BOTTOM));
+  }
+  sync = _gvn.transform(sync);
+  set_memory(sync, TypeRawPtr::BOTTOM);
+  return true;
+}
+
 //----------------------------inline_unsafe_allocate---------------------------
 // public native Object Unsafe.allocateInstance(Class<?> cls);
 bool LibraryCallKit::inline_unsafe_allocate() {
   if (callee()->is_static())  return false;  // caller must have the capability!
 
@@ -2938,132 +3028,10 @@
   Node* junk = NULL;
   set_result(generate_current_thread(junk));
   return true;
 }
 
-//------------------------inline_native_isInterrupted------------------
-// private native boolean java.lang.Thread.isInterrupted(boolean ClearInterrupted);
-bool LibraryCallKit::inline_native_isInterrupted() {
-  // Add a fast path to t.isInterrupted(clear_int):
-  //   (t == Thread.current() &&
-  //    (!TLS._osthread._interrupted || WINDOWS_ONLY(false) NOT_WINDOWS(!clear_int)))
-  //   ? TLS._osthread._interrupted : /*slow path:*/ t.isInterrupted(clear_int)
-  // So, in the common case that the interrupt bit is false,
-  // we avoid making a call into the VM.  Even if the interrupt bit
-  // is true, if the clear_int argument is false, we avoid the VM call.
-  // However, if the receiver is not currentThread, we must call the VM,
-  // because there must be some locking done around the operation.
-
-  // We only go to the fast case code if we pass two guards.
-  // Paths which do not pass are accumulated in the slow_region.
-
-  enum {
-    no_int_result_path   = 1, // t == Thread.current() && !TLS._osthread._interrupted
-    no_clear_result_path = 2, // t == Thread.current() &&  TLS._osthread._interrupted && !clear_int
-    slow_result_path     = 3, // slow path: t.isInterrupted(clear_int)
-    PATH_LIMIT
-  };
-
-  // Ensure that it's not possible to move the load of TLS._osthread._interrupted flag
-  // out of the function.
-  insert_mem_bar(Op_MemBarCPUOrder);
-
-  RegionNode* result_rgn = new RegionNode(PATH_LIMIT);
-  PhiNode*    result_val = new PhiNode(result_rgn, TypeInt::BOOL);
-
-  RegionNode* slow_region = new RegionNode(1);
-  record_for_igvn(slow_region);
-
-  // (a) Receiving thread must be the current thread.
-  Node* rec_thr = argument(0);
-  Node* tls_ptr = NULL;
-  Node* cur_thr = generate_current_thread(tls_ptr);
-
-  // Resolve oops to stable for CmpP below.
-  cur_thr = access_resolve(cur_thr, 0);
-  rec_thr = access_resolve(rec_thr, 0);
-
-  Node* cmp_thr = _gvn.transform(new CmpPNode(cur_thr, rec_thr));
-  Node* bol_thr = _gvn.transform(new BoolNode(cmp_thr, BoolTest::ne));
-
-  generate_slow_guard(bol_thr, slow_region);
-
-  // (b) Interrupt bit on TLS must be false.
-  Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
-  Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
-  p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::interrupted_offset()));
-
-  // Set the control input on the field _interrupted read to prevent it floating up.
-  Node* int_bit = make_load(control(), p, TypeInt::BOOL, T_INT, MemNode::unordered);
-  Node* cmp_bit = _gvn.transform(new CmpINode(int_bit, intcon(0)));
-  Node* bol_bit = _gvn.transform(new BoolNode(cmp_bit, BoolTest::ne));
-
-  IfNode* iff_bit = create_and_map_if(control(), bol_bit, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
-
-  // First fast path:  if (!TLS._interrupted) return false;
-  Node* false_bit = _gvn.transform(new IfFalseNode(iff_bit));
-  result_rgn->init_req(no_int_result_path, false_bit);
-  result_val->init_req(no_int_result_path, intcon(0));
-
-  // drop through to next case
-  set_control( _gvn.transform(new IfTrueNode(iff_bit)));
-
-#ifndef _WINDOWS
-  // (c) Or, if interrupt bit is set and clear_int is false, use 2nd fast path.
-  Node* clr_arg = argument(1);
-  Node* cmp_arg = _gvn.transform(new CmpINode(clr_arg, intcon(0)));
-  Node* bol_arg = _gvn.transform(new BoolNode(cmp_arg, BoolTest::ne));
-  IfNode* iff_arg = create_and_map_if(control(), bol_arg, PROB_FAIR, COUNT_UNKNOWN);
-
-  // Second fast path:  ... else if (!clear_int) return true;
-  Node* false_arg = _gvn.transform(new IfFalseNode(iff_arg));
-  result_rgn->init_req(no_clear_result_path, false_arg);
-  result_val->init_req(no_clear_result_path, intcon(1));
-
-  // drop through to next case
-  set_control( _gvn.transform(new IfTrueNode(iff_arg)));
-#else
-  // To return true on Windows you must read the _interrupted field
-  // and check the event state i.e. take the slow path.
-#endif // _WINDOWS
-
-  // (d) Otherwise, go to the slow path.
-  slow_region->add_req(control());
-  set_control( _gvn.transform(slow_region));
-
-  if (stopped()) {
-    // There is no slow path.
-    result_rgn->init_req(slow_result_path, top());
-    result_val->init_req(slow_result_path, top());
-  } else {
-    // non-virtual because it is a private non-static
-    CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_isInterrupted);
-
-    Node* slow_val = set_results_for_java_call(slow_call);
-    // this->control() comes from set_results_for_java_call
-
-    Node* fast_io  = slow_call->in(TypeFunc::I_O);
-    Node* fast_mem = slow_call->in(TypeFunc::Memory);
-
-    // These two phis are pre-filled with copies of of the fast IO and Memory
-    PhiNode* result_mem  = PhiNode::make(result_rgn, fast_mem, Type::MEMORY, TypePtr::BOTTOM);
-    PhiNode* result_io   = PhiNode::make(result_rgn, fast_io,  Type::ABIO);
-
-    result_rgn->init_req(slow_result_path, control());
-    result_io ->init_req(slow_result_path, i_o());
-    result_mem->init_req(slow_result_path, reset_memory());
-    result_val->init_req(slow_result_path, slow_val);
-
-    set_all_memory(_gvn.transform(result_mem));
-    set_i_o(       _gvn.transform(result_io));
-  }
-
-  C->set_has_split_ifs(true); // Has chance for split-if optimization
-  set_result(result_rgn, result_val);
-  return true;
-}
-
 //---------------------------load_mirror_from_klass----------------------------
 // Given a klass oop, load its java mirror (a java.lang.Class oop).
 Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {
   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
   Node* load = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
@@ -3414,14 +3382,10 @@
     Node* p = basic_plus_adr(arg, class_klass_offset);
     Node* kls = LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, adr_type, kls_type);
     klasses[which_arg] = _gvn.transform(kls);
   }
 
-  // Resolve oops to stable for CmpP below.
-  args[0] = access_resolve(args[0], 0);
-  args[1] = access_resolve(args[1], 0);
-
   // Having loaded both klasses, test each for null.
   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
   for (which_arg = 0; which_arg <= 1; which_arg++) {
     Node* kls = klasses[which_arg];
     Node* null_ctl = top();
@@ -3703,12 +3667,10 @@
       // How many elements will we copy from the original?
       // The answer is MinI(orig_length - start, length).
       Node* orig_tail = _gvn.transform(new SubINode(orig_length, start));
       Node* moved = generate_min_max(vmIntrinsics::_min, orig_tail, length);
 
-      original = access_resolve(original, ACCESS_READ);
-
       // Generate a direct call to the right arraycopy function(s).
       // We know the copy is disjoint but we might not know if the
       // oop stores need checking.
       // Extreme case:  Arrays.copyOf((Integer[])x, 10, String[].class).
       // This will fail a store-check if x contains any non-nulls.
@@ -3735,12 +3697,11 @@
 
       bool validated = false;
       // Reason_class_check rather than Reason_intrinsic because we
       // want to intrinsify even if this traps.
       if (!too_many_traps(Deoptimization::Reason_class_check)) {
-        Node* not_subtype_ctrl = gen_subtype_check(load_object_klass(original),
-                                                   klass_node);
+        Node* not_subtype_ctrl = gen_subtype_check(original, klass_node);
 
         if (not_subtype_ctrl != top()) {
           PreserveJVMState pjvms(this);
           set_control(not_subtype_ctrl);
           uncommon_trap(Deoptimization::Reason_class_check,
@@ -3926,33 +3887,33 @@
   // the null check after castPP removal.
   Node* no_ctrl = NULL;
   Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);
 
   // Test the header to see if it is unlocked.
-  Node *lock_mask      = _gvn.MakeConX(markOopDesc::biased_lock_mask_in_place);
+  Node *lock_mask      = _gvn.MakeConX(markWord::biased_lock_mask_in_place);
   Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));
-  Node *unlocked_val   = _gvn.MakeConX(markOopDesc::unlocked_value);
+  Node *unlocked_val   = _gvn.MakeConX(markWord::unlocked_value);
   Node *chk_unlocked   = _gvn.transform(new CmpXNode( lmasked_header, unlocked_val));
   Node *test_unlocked  = _gvn.transform(new BoolNode( chk_unlocked, BoolTest::ne));
 
   generate_slow_guard(test_unlocked, slow_region);
 
   // Get the hash value and check to see that it has been properly assigned.
   // We depend on hash_mask being at most 32 bits and avoid the use of
   // hash_mask_in_place because it could be larger than 32 bits in a 64-bit
-  // vm: see markOop.hpp.
-  Node *hash_mask      = _gvn.intcon(markOopDesc::hash_mask);
-  Node *hash_shift     = _gvn.intcon(markOopDesc::hash_shift);
+  // vm: see markWord.hpp.
+  Node *hash_mask      = _gvn.intcon(markWord::hash_mask);
+  Node *hash_shift     = _gvn.intcon(markWord::hash_shift);
   Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));
   // This hack lets the hash bits live anywhere in the mark object now, as long
   // as the shift drops the relevant bits into the low 32 bits.  Note that
   // Java spec says that HashCode is an int so there's no point in capturing
   // an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).
   hshifted_header      = ConvX2I(hshifted_header);
   Node *hash_val       = _gvn.transform(new AndINode(hshifted_header, hash_mask));
 
-  Node *no_hash_val    = _gvn.intcon(markOopDesc::no_hash);
+  Node *no_hash_val    = _gvn.intcon(markWord::no_hash);
   Node *chk_assigned   = _gvn.transform(new CmpINode( hash_val, no_hash_val));
   Node *test_assigned  = _gvn.transform(new BoolNode( chk_assigned, BoolTest::eq));
 
   generate_slow_guard(test_assigned, slow_region);
 
@@ -4202,27 +4163,35 @@
   Node* size    = ConvL2X(argument(7));  // type: long
 
   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
          "fieldOffset must be byte-scaled");
 
-  src_ptr = access_resolve(src_ptr, ACCESS_READ);
-  dst_ptr = access_resolve(dst_ptr, ACCESS_WRITE);
   Node* src = make_unsafe_address(src_ptr, src_off, ACCESS_READ);
   Node* dst = make_unsafe_address(dst_ptr, dst_off, ACCESS_WRITE);
 
   // Conservatively insert a memory barrier on all memory slices.
   // Do not let writes of the copy source or destination float below the copy.
   insert_mem_bar(Op_MemBarCPUOrder);
 
+  Node* thread = _gvn.transform(new ThreadLocalNode());
+  Node* doing_unsafe_access_addr = basic_plus_adr(top(), thread, in_bytes(JavaThread::doing_unsafe_access_offset()));
+  BasicType doing_unsafe_access_bt = T_BYTE;
+  assert((sizeof(bool) * CHAR_BIT) == 8, "not implemented");
+
+  // update volatile field
+  store_to_memory(control(), doing_unsafe_access_addr, intcon(1), doing_unsafe_access_bt, Compile::AliasIdxRaw, MemNode::unordered);
+
   // Call it.  Note that the length argument is not scaled.
   make_runtime_call(RC_LEAF|RC_NO_FP,
                     OptoRuntime::fast_arraycopy_Type(),
                     StubRoutines::unsafe_arraycopy(),
                     "unsafe_arraycopy",
                     TypeRawPtr::BOTTOM,
                     src, dst, size XTOP);
 
+  store_to_memory(control(), doing_unsafe_access_addr, intcon(0), doing_unsafe_access_bt, Compile::AliasIdxRaw, MemNode::unordered);
+
   // Do not let reads of the copy destination float above the copy.
   insert_mem_bar(Op_MemBarCPUOrder);
 
   return true;
 }
@@ -4245,14 +4214,11 @@
     // expansion, we know this AllocateNode is initialized by an array
     // copy and a StoreStore barrier exists after the array copy.
     alloc->initialization()->set_complete_with_arraycopy();
   }
 
-  // Copy the fastest available way.
-  // TODO: generate fields copies for small objects instead.
   Node* size = _gvn.transform(obj_size);
-
   access_clone(obj, alloc_obj, size, is_array);
 
   // Do not let reads from the cloned object float above the arraycopy.
   if (alloc != NULL) {
     // Do not let stores that initialize this object be reordered with
@@ -4315,16 +4281,10 @@
           obj = maybe_cast_profiled_obj(obj, obj_type->speculative_type(), false);
         }
       }
     }
 
-    Node* obj_klass = load_object_klass(obj);
-    const TypeKlassPtr* tklass = _gvn.type(obj_klass)->isa_klassptr();
-    const TypeOopPtr*   toop   = ((tklass != NULL)
-                                ? tklass->as_instance_type()
-                                : TypeInstPtr::NOTNULL);
-
     // Conservatively insert a memory barrier on all memory slices.
     // Do not let writes into the original float below the clone.
     insert_mem_bar(Op_MemBarCPUOrder);
 
     // paths into result_reg:
@@ -4339,10 +4299,11 @@
     result_val             = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
     PhiNode*    result_i_o = new PhiNode(result_reg, Type::ABIO);
     PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
     record_for_igvn(result_reg);
 
+    Node* obj_klass = load_object_klass(obj);
     Node* array_ctl = generate_array_guard(obj_klass, (RegionNode*)NULL);
     if (array_ctl != NULL) {
       // It's an array.
       PreserveJVMState pjvms(this);
       set_control(array_ctl);
@@ -4356,15 +4317,14 @@
         // because gc barriers are required when accessing the array.
         Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);
         if (is_obja != NULL) {
           PreserveJVMState pjvms2(this);
           set_control(is_obja);
-          obj = access_resolve(obj, ACCESS_READ);
           // Generate a direct call to the right arraycopy function(s).
           Node* alloc = tightly_coupled_allocation(alloc_obj, NULL);
           ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, alloc != NULL, false);
-          ac->set_cloneoop();
+          ac->set_clone_oop_array();
           Node* n = _gvn.transform(ac);
           assert(n == ac, "cannot disappear");
           ac->connect_outputs(this);
 
           result_reg->init_req(_objArray_path, control());
@@ -4472,31 +4432,31 @@
 JVMState* LibraryCallKit::arraycopy_restore_alloc_state(AllocateArrayNode* alloc, int& saved_reexecute_sp) {
   if (alloc != NULL) {
     ciMethod* trap_method = alloc->jvms()->method();
     int trap_bci = alloc->jvms()->bci();
 
-    if (!C->too_many_traps(trap_method, trap_bci, Deoptimization::Reason_intrinsic) &
-          !C->too_many_traps(trap_method, trap_bci, Deoptimization::Reason_null_check)) {
+    if (!C->too_many_traps(trap_method, trap_bci, Deoptimization::Reason_intrinsic) &&
+        !C->too_many_traps(trap_method, trap_bci, Deoptimization::Reason_null_check)) {
       // Make sure there's no store between the allocation and the
       // arraycopy otherwise visible side effects could be rexecuted
       // in case of deoptimization and cause incorrect execution.
       bool no_interfering_store = true;
       Node* mem = alloc->in(TypeFunc::Memory);
       if (mem->is_MergeMem()) {
         for (MergeMemStream mms(merged_memory(), mem->as_MergeMem()); mms.next_non_empty2(); ) {
           Node* n = mms.memory();
           if (n != mms.memory2() && !(n->is_Proj() && n->in(0) == alloc->initialization())) {
-            assert(n->is_Store() || n->Opcode() == Op_ShenandoahWBMemProj, "what else?");
+            assert(n->is_Store(), "what else?");
             no_interfering_store = false;
             break;
           }
         }
       } else {
         for (MergeMemStream mms(merged_memory()); mms.next_non_empty(); ) {
           Node* n = mms.memory();
           if (n != mem && !(n->is_Proj() && n->in(0) == alloc->initialization())) {
-            assert(n->is_Store() || n->Opcode() == Op_ShenandoahWBMemProj, "what else?");
+            assert(n->is_Store(), "what else?");
             no_interfering_store = false;
             break;
           }
         }
       }
@@ -4714,12 +4674,12 @@
   }
 
   if (has_src && has_dest && can_emit_guards) {
     BasicType src_elem  = top_src->klass()->as_array_klass()->element_type()->basic_type();
     BasicType dest_elem = top_dest->klass()->as_array_klass()->element_type()->basic_type();
-    if (src_elem  == T_ARRAY)  src_elem  = T_OBJECT;
-    if (dest_elem == T_ARRAY)  dest_elem = T_OBJECT;
+    if (is_reference_type(src_elem))   src_elem  = T_OBJECT;
+    if (is_reference_type(dest_elem))  dest_elem = T_OBJECT;
 
     if (src_elem == dest_elem && src_elem == T_OBJECT) {
       // If both arrays are object arrays then having the exact types
       // for both will remove the need for a subtype check at runtime
       // before the call and may make it possible to pick a faster copy
@@ -4803,20 +4763,21 @@
       generate_negative_guard(length, slow_region);
       negative_length_guard_generated = true;
     }
 
     // (9) each element of an oop array must be assignable
-    Node* src_klass  = load_object_klass(src);
     Node* dest_klass = load_object_klass(dest);
-    Node* not_subtype_ctrl = gen_subtype_check(src_klass, dest_klass);
-
-    if (not_subtype_ctrl != top()) {
-      PreserveJVMState pjvms(this);
-      set_control(not_subtype_ctrl);
-      uncommon_trap(Deoptimization::Reason_intrinsic,
-                    Deoptimization::Action_make_not_entrant);
-      assert(stopped(), "Should be stopped");
+    if (src != dest) {
+      Node* not_subtype_ctrl = gen_subtype_check(src, dest_klass);
+
+      if (not_subtype_ctrl != top()) {
+        PreserveJVMState pjvms(this);
+        set_control(not_subtype_ctrl);
+        uncommon_trap(Deoptimization::Reason_intrinsic,
+                      Deoptimization::Action_make_not_entrant);
+        assert(stopped(), "Should be stopped");
+      }
     }
     {
       PreserveJVMState pjvms(this);
       set_control(_gvn.transform(slow_region));
       uncommon_trap(Deoptimization::Reason_intrinsic,
@@ -4833,14 +4794,11 @@
 
   if (stopped()) {
     return true;
   }
 
-  Node* new_src = access_resolve(src, ACCESS_READ);
-  Node* new_dest = access_resolve(dest, ACCESS_WRITE);
-
-  ArrayCopyNode* ac = ArrayCopyNode::make(this, true, new_src, src_offset, new_dest, dest_offset, length, alloc != NULL, negative_length_guard_generated,
+  ArrayCopyNode* ac = ArrayCopyNode::make(this, true, src, src_offset, dest, dest_offset, length, alloc != NULL, negative_length_guard_generated,
                                           // Create LoadRange and LoadKlass nodes for use during macro expansion here
                                           // so the compiler has a chance to eliminate them: during macro expansion,
                                           // we have to set their control (CastPP nodes are eliminated).
                                           load_object_klass(src), load_object_klass(dest),
                                           load_array_length(src), load_array_length(dest));
@@ -4892,12 +4850,10 @@
     }
   }
 
   // This arraycopy must unconditionally follow the allocation of the ptr.
   Node* alloc_ctl = ptr->in(0);
-  assert(just_allocated_object(alloc_ctl) == ptr, "most recent allo");
-
   Node* ctl = control();
   while (ctl != alloc_ctl) {
     // There may be guards which feed into the slow_region.
     // Any other control flow means that we might not get a chance
     // to finish initializing the allocated object.
@@ -4949,13 +4905,10 @@
   Node *length      = argument(4);
 
   src = must_be_not_null(src, true);
   dst = must_be_not_null(dst, true);
 
-  src = access_resolve(src, ACCESS_READ);
-  dst = access_resolve(dst, ACCESS_WRITE);
-
   const Type* src_type = src->Value(&_gvn);
   const Type* dst_type = dst->Value(&_gvn);
   const TypeAryPtr* top_src = src_type->isa_aryptr();
   const TypeAryPtr* top_dest = dst_type->isa_aryptr();
   if (top_src  == NULL || top_src->klass()  == NULL ||
@@ -5007,14 +4960,10 @@
   Node* z    = argument(4);
 
   x = must_be_not_null(x, true);
   y = must_be_not_null(y, true);
 
-  x = access_resolve(x, ACCESS_READ);
-  y = access_resolve(y, ACCESS_READ);
-  z = access_resolve(z, ACCESS_WRITE);
-
   const Type* x_type = x->Value(&_gvn);
   const Type* y_type = y->Value(&_gvn);
   const TypeAryPtr* top_x = x_type->isa_aryptr();
   const TypeAryPtr* top_y = y_type->isa_aryptr();
   if (top_x  == NULL || top_x->klass()  == NULL ||
@@ -5119,13 +5068,10 @@
   Node* zlen = argument(3);
 
   x = must_be_not_null(x, true);
   z = must_be_not_null(z, true);
 
-  x = access_resolve(x, ACCESS_READ);
-  z = access_resolve(z, ACCESS_WRITE);
-
   const Type* x_type = x->Value(&_gvn);
   const Type* z_type = z->Value(&_gvn);
   const TypeAryPtr* top_x = x_type->isa_aryptr();
   const TypeAryPtr* top_z = z_type->isa_aryptr();
   if (top_x  == NULL || top_x->klass()  == NULL ||
@@ -5171,13 +5117,10 @@
   Node* len      = argument(3);
   Node* k        = argument(4);
 
   out = must_be_not_null(out, true);
 
-  in = access_resolve(in, ACCESS_READ);
-  out = access_resolve(out, ACCESS_WRITE);
-
   const Type* out_type = out->Value(&_gvn);
   const Type* in_type = in->Value(&_gvn);
   const TypeAryPtr* top_out = out_type->isa_aryptr();
   const TypeAryPtr* top_in = in_type->isa_aryptr();
   if (top_out  == NULL || top_out->klass()  == NULL ||
@@ -5223,15 +5166,10 @@
   Node* n    = argument(2);
   Node* len  = argument(3);
   Node* inv  = argument(4);
   Node* m    = argument(6);
 
-  a = access_resolve(a, ACCESS_READ);
-  b = access_resolve(b, ACCESS_READ);
-  n = access_resolve(n, ACCESS_READ);
-  m = access_resolve(m, ACCESS_WRITE);
-
   const Type* a_type = a->Value(&_gvn);
   const TypeAryPtr* top_a = a_type->isa_aryptr();
   const Type* b_type = b->Value(&_gvn);
   const TypeAryPtr* top_b = b_type->isa_aryptr();
   const Type* n_type = a->Value(&_gvn);
@@ -5287,14 +5225,10 @@
   Node* n    = argument(1);
   Node* len  = argument(2);
   Node* inv  = argument(3);
   Node* m    = argument(5);
 
-  a = access_resolve(a, ACCESS_READ);
-  n = access_resolve(n, ACCESS_READ);
-  m = access_resolve(m, ACCESS_WRITE);
-
   const Type* a_type = a->Value(&_gvn);
   const TypeAryPtr* top_a = a_type->isa_aryptr();
   const Type* n_type = a->Value(&_gvn);
   const TypeAryPtr* top_n = n_type->isa_aryptr();
   const Type* m_type = a->Value(&_gvn);
@@ -5328,10 +5262,64 @@
   }
 
   return true;
 }
 
+bool LibraryCallKit::inline_bigIntegerShift(bool isRightShift) {
+  address stubAddr = NULL;
+  const char* stubName = NULL;
+
+  stubAddr = isRightShift? StubRoutines::bigIntegerRightShift(): StubRoutines::bigIntegerLeftShift();
+  if (stubAddr == NULL) {
+    return false; // Intrinsic's stub is not implemented on this platform
+  }
+
+  stubName = isRightShift? "bigIntegerRightShiftWorker" : "bigIntegerLeftShiftWorker";
+
+  assert(callee()->signature()->size() == 5, "expected 5 arguments");
+
+  Node* newArr = argument(0);
+  Node* oldArr = argument(1);
+  Node* newIdx = argument(2);
+  Node* shiftCount = argument(3);
+  Node* numIter = argument(4);
+
+  const Type* newArr_type = newArr->Value(&_gvn);
+  const TypeAryPtr* top_newArr = newArr_type->isa_aryptr();
+  const Type* oldArr_type = oldArr->Value(&_gvn);
+  const TypeAryPtr* top_oldArr = oldArr_type->isa_aryptr();
+  if (top_newArr == NULL || top_newArr->klass() == NULL || top_oldArr == NULL
+      || top_oldArr->klass() == NULL) {
+    return false;
+  }
+
+  BasicType newArr_elem = newArr_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();
+  BasicType oldArr_elem = oldArr_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();
+  if (newArr_elem != T_INT || oldArr_elem != T_INT) {
+    return false;
+  }
+
+  // Make the call
+  {
+    Node* newArr_start = array_element_address(newArr, intcon(0), newArr_elem);
+    Node* oldArr_start = array_element_address(oldArr, intcon(0), oldArr_elem);
+
+    Node* call = make_runtime_call(RC_LEAF,
+                                   OptoRuntime::bigIntegerShift_Type(),
+                                   stubAddr,
+                                   stubName,
+                                   TypePtr::BOTTOM,
+                                   newArr_start,
+                                   oldArr_start,
+                                   newIdx,
+                                   shiftCount,
+                                   numIter);
+  }
+
+  return true;
+}
+
 //-------------inline_vectorizedMismatch------------------------------
 bool LibraryCallKit::inline_vectorizedMismatch() {
   assert(UseVectorizedMismatchIntrinsic, "not implementated on this platform");
 
   address stubAddr = StubRoutines::vectorizedMismatch();
@@ -5360,12 +5348,10 @@
   }
 
   Node* call;
   jvms()->set_should_reexecute(true);
 
-  obja = access_resolve(obja, ACCESS_READ);
-  objb = access_resolve(objb, ACCESS_READ);
   Node* obja_adr = make_unsafe_address(obja, aoffset, ACCESS_READ);
   Node* objb_adr = make_unsafe_address(objb, boffset, ACCESS_READ);
 
   call = make_runtime_call(RC_LEAF,
     OptoRuntime::vectorizedMismatch_Type(),
@@ -5438,11 +5424,10 @@
     return false;
   }
 
   // 'src_start' points to src array + scaled offset
   src = must_be_not_null(src, true);
-  src = access_resolve(src, ACCESS_READ);
   Node* src_start = array_element_address(src, offset, src_elem);
 
   // We assume that range check is done by caller.
   // TODO: generate range check (offset+length < src.length) in debug VM.
 
@@ -5528,17 +5513,15 @@
     return false;
   }
 
   // 'src_start' points to src array + scaled offset
   src = must_be_not_null(src, true);
-  src = access_resolve(src, ACCESS_READ);
   Node* src_start = array_element_address(src, offset, src_elem);
 
   // static final int[] byteTable in class CRC32C
   Node* table = get_table_from_crc32c_class(callee()->holder());
   table = must_be_not_null(table, true);
-  table = access_resolve(table, ACCESS_READ);
   Node* table_start = array_element_address(table, intcon(0), T_INT);
 
   // We assume that range check is done by caller.
   // TODO: generate range check (offset+length < src.length) in debug VM.
 
@@ -5579,11 +5562,10 @@
   Node* src_start = basic_plus_adr(top(), base, offset);
 
   // static final int[] byteTable in class CRC32C
   Node* table = get_table_from_crc32c_class(callee()->holder());
   table = must_be_not_null(table, true);
-  table = access_resolve(table, ACCESS_READ);
   Node* table_start = array_element_address(table, intcon(0), T_INT);
 
   // Call the stub.
   address stubAddr = StubRoutines::updateBytesCRC32C();
   const char *stubName = "updateBytesCRC32C";
@@ -5623,11 +5605,10 @@
   if (src_elem != T_BYTE) {
     return false;
   }
 
   // 'src_start' points to src array + scaled offset
-  src = access_resolve(src, ACCESS_READ);
   Node* src_start = array_element_address(src, offset, src_elem);
 
   // We assume that range check is done by caller.
   // TODO: generate range check (offset+length < src.length) in debug VM.
 
@@ -5827,13 +5808,10 @@
   Node* dest_offset     = argument(4);
 
   src = must_be_not_null(src, true);
   dest = must_be_not_null(dest, true);
 
-  src = access_resolve(src, ACCESS_READ);
-  dest = access_resolve(dest, ACCESS_WRITE);
-
   // (1) src and dest are arrays.
   const Type* src_type = src->Value(&_gvn);
   const Type* dest_type = dest->Value(&_gvn);
   const TypeAryPtr* top_src = src_type->isa_aryptr();
   const TypeAryPtr* top_dest = dest_type->isa_aryptr();
@@ -5903,13 +5881,10 @@
   Node* dest_offset                = argument(5);
 
   src = must_be_not_null(src, false);
   dest = must_be_not_null(dest, false);
 
-  src = access_resolve(src, ACCESS_READ);
-  dest = access_resolve(dest, ACCESS_WRITE);
-
   // (1) src and dest are arrays.
   const Type* src_type = src->Value(&_gvn);
   const Type* dest_type = dest->Value(&_gvn);
   const TypeAryPtr* top_src = src_type->isa_aryptr();
   const TypeAryPtr* top_dest = dest_type->isa_aryptr();
@@ -5951,11 +5926,10 @@
   if (k_start == NULL) return false;
 
   // similarly, get the start address of the r vector
   Node* objRvec = load_field_from_object(cipherBlockChaining_object, "r", "[B", /*is_exact*/ false);
   if (objRvec == NULL) return false;
-  objRvec = access_resolve(objRvec, ACCESS_WRITE);
   Node* r_start = array_element_address(objRvec, intcon(0), T_BYTE);
 
   Node* cbcCrypt;
   if (Matcher::pass_original_key_for_aes()) {
     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
@@ -5980,10 +5954,98 @@
   Node* retvalue = _gvn.transform(new ProjNode(cbcCrypt, TypeFunc::Parms));
   set_result(retvalue);
   return true;
 }
 
+//------------------------------inline_electronicCodeBook_AESCrypt-----------------------
+bool LibraryCallKit::inline_electronicCodeBook_AESCrypt(vmIntrinsics::ID id) {
+  address stubAddr = NULL;
+  const char *stubName = NULL;
+
+  assert(UseAES, "need AES instruction support");
+
+  switch (id) {
+  case vmIntrinsics::_electronicCodeBook_encryptAESCrypt:
+    stubAddr = StubRoutines::electronicCodeBook_encryptAESCrypt();
+    stubName = "electronicCodeBook_encryptAESCrypt";
+    break;
+  case vmIntrinsics::_electronicCodeBook_decryptAESCrypt:
+    stubAddr = StubRoutines::electronicCodeBook_decryptAESCrypt();
+    stubName = "electronicCodeBook_decryptAESCrypt";
+    break;
+  default:
+    break;
+  }
+
+  if (stubAddr == NULL) return false;
+
+  Node* electronicCodeBook_object = argument(0);
+  Node* src                       = argument(1);
+  Node* src_offset                = argument(2);
+  Node* len                       = argument(3);
+  Node* dest                      = argument(4);
+  Node* dest_offset               = argument(5);
+
+  // (1) src and dest are arrays.
+  const Type* src_type = src->Value(&_gvn);
+  const Type* dest_type = dest->Value(&_gvn);
+  const TypeAryPtr* top_src = src_type->isa_aryptr();
+  const TypeAryPtr* top_dest = dest_type->isa_aryptr();
+  assert(top_src != NULL && top_src->klass() != NULL
+         &&  top_dest != NULL && top_dest->klass() != NULL, "args are strange");
+
+  // checks are the responsibility of the caller
+  Node* src_start = src;
+  Node* dest_start = dest;
+  if (src_offset != NULL || dest_offset != NULL) {
+    assert(src_offset != NULL && dest_offset != NULL, "");
+    src_start = array_element_address(src, src_offset, T_BYTE);
+    dest_start = array_element_address(dest, dest_offset, T_BYTE);
+  }
+
+  // if we are in this set of code, we "know" the embeddedCipher is an AESCrypt object
+  // (because of the predicated logic executed earlier).
+  // so we cast it here safely.
+  // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
+
+  Node* embeddedCipherObj = load_field_from_object(electronicCodeBook_object, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
+  if (embeddedCipherObj == NULL) return false;
+
+  // cast it to what we know it will be at runtime
+  const TypeInstPtr* tinst = _gvn.type(electronicCodeBook_object)->isa_instptr();
+  assert(tinst != NULL, "ECB obj is null");
+  assert(tinst->klass()->is_loaded(), "ECB obj is not loaded");
+  ciKlass* klass_AESCrypt = tinst->klass()->as_instance_klass()->find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
+  assert(klass_AESCrypt->is_loaded(), "predicate checks that this class is loaded");
+
+  ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt->as_instance_klass();
+  const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_AESCrypt);
+  const TypeOopPtr* xtype = aklass->as_instance_type();
+  Node* aescrypt_object = new CheckCastPPNode(control(), embeddedCipherObj, xtype);
+  aescrypt_object = _gvn.transform(aescrypt_object);
+
+  // we need to get the start of the aescrypt_object's expanded key array
+  Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
+  if (k_start == NULL) return false;
+
+  Node* ecbCrypt;
+  if (Matcher::pass_original_key_for_aes()) {
+    // no SPARC version for AES/ECB intrinsics now.
+    return false;
+  }
+  // Call the stub, passing src_start, dest_start, k_start, r_start and src_len
+  ecbCrypt = make_runtime_call(RC_LEAF | RC_NO_FP,
+                               OptoRuntime::electronicCodeBook_aescrypt_Type(),
+                               stubAddr, stubName, TypePtr::BOTTOM,
+                               src_start, dest_start, k_start, len);
+
+  // return cipher length (int)
+  Node* retvalue = _gvn.transform(new ProjNode(ecbCrypt, TypeFunc::Parms));
+  set_result(retvalue);
+  return true;
+}
+
 //------------------------------inline_counterMode_AESCrypt-----------------------
 bool LibraryCallKit::inline_counterMode_AESCrypt(vmIntrinsics::ID id) {
   assert(UseAES, "need AES instruction support");
   if (!UseAESCTRIntrinsics) return false;
 
@@ -6000,14 +6062,10 @@
   Node* src_offset = argument(2);
   Node* len = argument(3);
   Node* dest = argument(4);
   Node* dest_offset = argument(5);
 
-  src = access_resolve(src, ACCESS_READ);
-  dest = access_resolve(dest, ACCESS_WRITE);
-  counterMode_object = access_resolve(counterMode_object, ACCESS_WRITE);
-
   // (1) src and dest are arrays.
   const Type* src_type = src->Value(&_gvn);
   const Type* dest_type = dest->Value(&_gvn);
   const TypeAryPtr* top_src = src_type->isa_aryptr();
   const TypeAryPtr* top_dest = dest_type->isa_aryptr();
@@ -6044,16 +6102,14 @@
   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
   if (k_start == NULL) return false;
   // similarly, get the start address of the r vector
   Node* obj_counter = load_field_from_object(counterMode_object, "counter", "[B", /*is_exact*/ false);
   if (obj_counter == NULL) return false;
-  obj_counter = access_resolve(obj_counter, ACCESS_WRITE);
   Node* cnt_start = array_element_address(obj_counter, intcon(0), T_BYTE);
 
   Node* saved_encCounter = load_field_from_object(counterMode_object, "encryptedCounter", "[B", /*is_exact*/ false);
   if (saved_encCounter == NULL) return false;
-  saved_encCounter = access_resolve(saved_encCounter, ACCESS_WRITE);
   Node* saved_encCounter_start = array_element_address(saved_encCounter, intcon(0), T_BYTE);
   Node* used = field_address_from_object(counterMode_object, "used", "I", /*is_exact*/ false);
 
   Node* ctrCrypt;
   if (Matcher::pass_original_key_for_aes()) {
@@ -6090,11 +6146,10 @@
 #endif // PPC64
   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
   if (objAESCryptKey == NULL) return (Node *) NULL;
 
   // now have the array, need to get the start address of the K array
-  objAESCryptKey = access_resolve(objAESCryptKey, ACCESS_READ);
   Node* k_start = array_element_address(objAESCryptKey, intcon(0), T_INT);
   return k_start;
 }
 
 //------------------------------get_original_key_start_from_aescrypt_object-----------------------
@@ -6102,11 +6157,10 @@
   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "lastKey", "[B", /*is_exact*/ false);
   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
   if (objAESCryptKey == NULL) return (Node *) NULL;
 
   // now have the array, need to get the start address of the lastKey array
-  objAESCryptKey = access_resolve(objAESCryptKey, ACCESS_READ);
   Node* original_k_start = array_element_address(objAESCryptKey, intcon(0), T_BYTE);
   return original_k_start;
 }
 
 //----------------------------inline_cipherBlockChaining_AESCrypt_predicate----------------------------
@@ -6146,13 +6200,10 @@
 
   src = must_be_not_null(src, true);
   dest = must_be_not_null(dest, true);
 
   // Resolve oops to stable for CmpP below.
-  src = access_resolve(src, 0);
-  dest = access_resolve(dest, 0);
-
   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt->as_instance_klass();
 
   Node* instof = gen_instanceof(embeddedCipherObj, makecon(TypeKlassPtr::make(instklass_AESCrypt)));
   Node* cmp_instof  = _gvn.transform(new CmpINode(instof, intcon(1)));
   Node* bool_instof  = _gvn.transform(new BoolNode(cmp_instof, BoolTest::ne));
@@ -6176,10 +6227,69 @@
 
   record_for_igvn(region);
   return _gvn.transform(region);
 }
 
+//----------------------------inline_electronicCodeBook_AESCrypt_predicate----------------------------
+// Return node representing slow path of predicate check.
+// the pseudo code we want to emulate with this predicate is:
+// for encryption:
+//    if (embeddedCipherObj instanceof AESCrypt) do_intrinsic, else do_javapath
+// for decryption:
+//    if ((embeddedCipherObj instanceof AESCrypt) && (cipher!=plain)) do_intrinsic, else do_javapath
+//    note cipher==plain is more conservative than the original java code but that's OK
+//
+Node* LibraryCallKit::inline_electronicCodeBook_AESCrypt_predicate(bool decrypting) {
+  // The receiver was checked for NULL already.
+  Node* objECB = argument(0);
+
+  // Load embeddedCipher field of ElectronicCodeBook object.
+  Node* embeddedCipherObj = load_field_from_object(objECB, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
+
+  // get AESCrypt klass for instanceOf check
+  // AESCrypt might not be loaded yet if some other SymmetricCipher got us to this compile point
+  // will have same classloader as ElectronicCodeBook object
+  const TypeInstPtr* tinst = _gvn.type(objECB)->isa_instptr();
+  assert(tinst != NULL, "ECBobj is null");
+  assert(tinst->klass()->is_loaded(), "ECBobj is not loaded");
+
+  // we want to do an instanceof comparison against the AESCrypt class
+  ciKlass* klass_AESCrypt = tinst->klass()->as_instance_klass()->find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
+  if (!klass_AESCrypt->is_loaded()) {
+    // if AESCrypt is not even loaded, we never take the intrinsic fast path
+    Node* ctrl = control();
+    set_control(top()); // no regular fast path
+    return ctrl;
+  }
+  ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt->as_instance_klass();
+
+  Node* instof = gen_instanceof(embeddedCipherObj, makecon(TypeKlassPtr::make(instklass_AESCrypt)));
+  Node* cmp_instof = _gvn.transform(new CmpINode(instof, intcon(1)));
+  Node* bool_instof = _gvn.transform(new BoolNode(cmp_instof, BoolTest::ne));
+
+  Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
+
+  // for encryption, we are done
+  if (!decrypting)
+    return instof_false;  // even if it is NULL
+
+  // for decryption, we need to add a further check to avoid
+  // taking the intrinsic path when cipher and plain are the same
+  // see the original java code for why.
+  RegionNode* region = new RegionNode(3);
+  region->init_req(1, instof_false);
+  Node* src = argument(1);
+  Node* dest = argument(4);
+  Node* cmp_src_dest = _gvn.transform(new CmpPNode(src, dest));
+  Node* bool_src_dest = _gvn.transform(new BoolNode(cmp_src_dest, BoolTest::eq));
+  Node* src_dest_conjoint = generate_guard(bool_src_dest, NULL, PROB_MIN);
+  region->init_req(2, src_dest_conjoint);
+
+  record_for_igvn(region);
+  return _gvn.transform(region);
+}
+
 //----------------------------inline_counterMode_AESCrypt_predicate----------------------------
 // Return node representing slow path of predicate check.
 // the pseudo code we want to emulate with this predicate is:
 // for encryption:
 //    if (embeddedCipherObj instanceof AESCrypt) do_intrinsic, else do_javapath
@@ -6237,14 +6347,10 @@
 
   state = must_be_not_null(state, true);
   subkeyH = must_be_not_null(subkeyH, true);
   data = must_be_not_null(data, true);
 
-  state = access_resolve(state, ACCESS_WRITE);
-  subkeyH = access_resolve(subkeyH, ACCESS_READ);
-  data = access_resolve(data, ACCESS_READ);
-
   Node* state_start  = array_element_address(state, intcon(0), T_LONG);
   assert(state_start, "state is NULL");
   Node* subkeyH_start  = array_element_address(subkeyH, intcon(0), T_LONG);
   assert(subkeyH_start, "subkeyH is NULL");
   Node* data_start  = array_element_address(data, offset, T_BYTE);
@@ -6273,13 +6379,11 @@
   Node* dest = argument(4);
   Node* dp = argument(5);
   Node* isURL = argument(6);
 
   src = must_be_not_null(src, true);
-  src = access_resolve(src, ACCESS_READ);
   dest = must_be_not_null(dest, true);
-  dest = access_resolve(dest, ACCESS_WRITE);
 
   Node* src_start = array_element_address(src, intcon(0), T_BYTE);
   assert(src_start, "source array is NULL");
   Node* dest_start = array_element_address(dest, intcon(0), T_BYTE);
   assert(dest_start, "destination array is NULL");
@@ -6320,11 +6424,10 @@
   if (src_elem != T_BYTE) {
     return false;
   }
   // 'src_start' points to src array + offset
   src = must_be_not_null(src, true);
-  src = access_resolve(src, ACCESS_READ);
   Node* src_start = array_element_address(src, ofs, src_elem);
   Node* state = NULL;
   address stubAddr;
   const char *stubName;
 
@@ -6351,10 +6454,13 @@
     fatal_unexpected_iid(id);
     return false;
   }
   if (state == NULL) return false;
 
+  assert(stubAddr != NULL, "Stub is generated");
+  if (stubAddr == NULL) return false;
+
   // Call the stub.
   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::sha_implCompress_Type(),
                                  stubAddr, stubName, TypePtr::BOTTOM,
                                  src_start, state);
 
@@ -6388,11 +6494,10 @@
   if (src_elem != T_BYTE) {
     return false;
   }
   // 'src_start' points to src array + offset
   src = must_be_not_null(src, false);
-  src = access_resolve(src, ACCESS_READ);
   Node* src_start = array_element_address(src, ofs, src_elem);
 
   const char* klass_SHA_name = NULL;
   const char* stub_name = NULL;
   address     stub_addr = NULL;
@@ -6423,10 +6528,13 @@
     break;
   default:
     fatal("unknown SHA intrinsic predicate: %d", predicate);
   }
   if (klass_SHA_name != NULL) {
+    assert(stub_addr != NULL, "Stub is generated");
+    if (stub_addr == NULL) return false;
+
     // get DigestBase klass to lookup for SHA klass
     const TypeInstPtr* tinst = _gvn.type(digestBase_obj)->isa_instptr();
     assert(tinst != NULL, "digestBase_obj is not instance???");
     assert(tinst->klass()->is_loaded(), "DigestBase is not loaded");
 
@@ -6471,11 +6579,10 @@
   Node* sha_state = load_field_from_object(sha_object, "state", "[I", /*is_exact*/ false);
   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA/SHA2");
   if (sha_state == NULL) return (Node *) NULL;
 
   // now have the array, need to get the start address of the state array
-  sha_state = access_resolve(sha_state, ACCESS_WRITE);
   Node* state = array_element_address(sha_state, intcon(0), T_INT);
   return state;
 }
 
 //------------------------------get_state_from_sha5_object-----------------------
@@ -6483,11 +6590,10 @@
   Node* sha_state = load_field_from_object(sha_object, "state", "[J", /*is_exact*/ false);
   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA5");
   if (sha_state == NULL) return (Node *) NULL;
 
   // now have the array, need to get the start address of the state array
-  sha_state = access_resolve(sha_state, ACCESS_WRITE);
   Node* state = array_element_address(sha_state, intcon(0), T_LONG);
   return state;
 }
 
 //----------------------------inline_digestBase_implCompressMB_predicate----------------------------
@@ -6661,13 +6767,10 @@
     break;
   default:
     fatal_unexpected_iid(id);
     break;
   }
-  if (a->is_Con() || b->is_Con()) {
-    return false;
-  }
   switch (id) {
   case vmIntrinsics::_maxF:  n = new MaxFNode(a, b);  break;
   case vmIntrinsics::_minF:  n = new MinFNode(a, b);  break;
   case vmIntrinsics::_maxD:  n = new MaxDNode(a, b);  break;
   case vmIntrinsics::_minD:  n = new MinDNode(a, b);  break;
