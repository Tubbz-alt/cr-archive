<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/macro.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="machnode.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macro.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/macro.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2005, 2018, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;compiler/compileLog.hpp&quot;
  27 #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
  28 #include &quot;libadt/vectset.hpp&quot;

  29 #include &quot;opto/addnode.hpp&quot;
  30 #include &quot;opto/arraycopynode.hpp&quot;
  31 #include &quot;opto/callnode.hpp&quot;
  32 #include &quot;opto/castnode.hpp&quot;
  33 #include &quot;opto/cfgnode.hpp&quot;
  34 #include &quot;opto/compile.hpp&quot;
  35 #include &quot;opto/convertnode.hpp&quot;
  36 #include &quot;opto/graphKit.hpp&quot;

  37 #include &quot;opto/locknode.hpp&quot;
  38 #include &quot;opto/loopnode.hpp&quot;
  39 #include &quot;opto/macro.hpp&quot;
  40 #include &quot;opto/memnode.hpp&quot;
  41 #include &quot;opto/narrowptrnode.hpp&quot;
  42 #include &quot;opto/node.hpp&quot;
  43 #include &quot;opto/opaquenode.hpp&quot;
  44 #include &quot;opto/phaseX.hpp&quot;
  45 #include &quot;opto/rootnode.hpp&quot;
  46 #include &quot;opto/runtime.hpp&quot;
  47 #include &quot;opto/subnode.hpp&quot;

  48 #include &quot;opto/type.hpp&quot;
  49 #include &quot;runtime/sharedRuntime.hpp&quot;
  50 #include &quot;utilities/macros.hpp&quot;

  51 #if INCLUDE_G1GC
  52 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
  53 #endif // INCLUDE_G1GC
  54 #if INCLUDE_SHENANDOAHGC
  55 #include &quot;gc/shenandoah/c2/shenandoahBarrierSetC2.hpp&quot;
  56 #endif
  57 
  58 
  59 //
  60 // Replace any references to &quot;oldref&quot; in inputs to &quot;use&quot; with &quot;newref&quot;.
  61 // Returns the number of replacements made.
  62 //
  63 int PhaseMacroExpand::replace_input(Node *use, Node *oldref, Node *newref) {
  64   int nreplacements = 0;
  65   uint req = use-&gt;req();
  66   for (uint j = 0; j &lt; use-&gt;len(); j++) {
  67     Node *uin = use-&gt;in(j);
  68     if (uin == oldref) {
  69       if (j &lt; req)
  70         use-&gt;set_req(j, newref);
  71       else
  72         use-&gt;set_prec(j, newref);
  73       nreplacements++;
  74     } else if (j &gt;= req &amp;&amp; uin == NULL) {
  75       break;
  76     }
  77   }
  78   return nreplacements;
  79 }
  80 












  81 void PhaseMacroExpand::copy_call_debug_info(CallNode *oldcall, CallNode * newcall) {
  82   // Copy debug information and adjust JVMState information
  83   uint old_dbg_start = oldcall-&gt;tf()-&gt;domain()-&gt;cnt();
  84   uint new_dbg_start = newcall-&gt;tf()-&gt;domain()-&gt;cnt();
  85   int jvms_adj  = new_dbg_start - old_dbg_start;
  86   assert (new_dbg_start == newcall-&gt;req(), &quot;argument count mismatch&quot;);
  87 
  88   // SafePointScalarObject node could be referenced several times in debug info.
  89   // Use Dict to record cloned nodes.
  90   Dict* sosn_map = new Dict(cmpkey,hashkey);
  91   for (uint i = old_dbg_start; i &lt; oldcall-&gt;req(); i++) {
  92     Node* old_in = oldcall-&gt;in(i);
  93     // Clone old SafePointScalarObjectNodes, adjusting their field contents.
  94     if (old_in != NULL &amp;&amp; old_in-&gt;is_SafePointScalarObject()) {
  95       SafePointScalarObjectNode* old_sosn = old_in-&gt;as_SafePointScalarObject();
  96       uint old_unique = C-&gt;unique();
  97       Node* new_in = old_sosn-&gt;clone(sosn_map);
  98       if (old_unique != C-&gt;unique()) { // New node?
  99         new_in-&gt;set_req(0, C-&gt;root()); // reset control edge
 100         new_in = transform_later(new_in); // Register new node.
</pre>
<hr />
<pre>
 330       }
 331       mem = mem-&gt;in(MemNode::Memory);
 332     } else {
 333       return mem;
 334     }
 335     assert(mem != orig_mem, &quot;dead memory loop&quot;);
 336   }
 337 }
 338 
 339 // Generate loads from source of the arraycopy for fields of
 340 // destination needed at a deoptimization point
 341 Node* PhaseMacroExpand::make_arraycopy_load(ArrayCopyNode* ac, intptr_t offset, Node* ctl, Node* mem, BasicType ft, const Type *ftype, AllocateNode *alloc) {
 342   BasicType bt = ft;
 343   const Type *type = ftype;
 344   if (ft == T_NARROWOOP) {
 345     bt = T_OBJECT;
 346     type = ftype-&gt;make_oopptr();
 347   }
 348   Node* res = NULL;
 349   if (ac-&gt;is_clonebasic()) {

 350     Node* base = ac-&gt;in(ArrayCopyNode::Src)-&gt;in(AddPNode::Base);
 351     Node* adr = _igvn.transform(new AddPNode(base, base, MakeConX(offset)));
 352     const TypePtr* adr_type = _igvn.type(base)-&gt;is_ptr()-&gt;add_offset(offset);
<span class="line-modified"> 353     res = LoadNode::make(_igvn, ctl, mem, adr, adr_type, type, bt, MemNode::unordered, LoadNode::Pinned);</span>
 354   } else {
 355     if (ac-&gt;modifies(offset, offset, &amp;_igvn, true)) {
 356       assert(ac-&gt;in(ArrayCopyNode::Dest) == alloc-&gt;result_cast(), &quot;arraycopy destination should be allocation&#39;s result&quot;);
<span class="line-modified"> 357       uint shift  = exact_log2(type2aelembytes(bt));</span>
<span class="line-modified"> 358       Node* diff = _igvn.transform(new SubINode(ac-&gt;in(ArrayCopyNode::SrcPos), ac-&gt;in(ArrayCopyNode::DestPos)));</span>

















 359 #ifdef _LP64
<span class="line-modified"> 360       diff = _igvn.transform(new ConvI2LNode(diff));</span>
 361 #endif
<span class="line-modified"> 362       diff = _igvn.transform(new LShiftXNode(diff, intcon(shift)));</span>
<span class="line-modified"> 363 </span>
<span class="line-modified"> 364       Node* off = _igvn.transform(new AddXNode(MakeConX(offset), diff));</span>
<span class="line-modified"> 365       Node* base = ac-&gt;in(ArrayCopyNode::Src);</span>
<span class="line-modified"> 366       Node* adr = _igvn.transform(new AddPNode(base, base, off));</span>
<span class="line-modified"> 367       const TypePtr* adr_type = _igvn.type(base)-&gt;is_ptr()-&gt;add_offset(offset);</span>
<span class="line-modified"> 368       res = LoadNode::make(_igvn, ctl, mem, adr, adr_type, type, bt, MemNode::unordered, LoadNode::Pinned);</span>






 369     }
 370   }
 371   if (res != NULL) {
 372     res = _igvn.transform(res);
 373     if (ftype-&gt;isa_narrowoop()) {
 374       // PhaseMacroExpand::scalar_replacement adds DecodeN nodes
 375       res = _igvn.transform(new EncodePNode(res, ftype));
 376     }
 377     return res;
 378   }
 379   return NULL;
 380 }
 381 
 382 //
 383 // Given a Memory Phi, compute a value Phi containing the values from stores
 384 // on the input paths.
 385 // Note: this function is recursive, its depth is limited by the &quot;level&quot; argument
 386 // Returns the computed Phi, or NULL if it cannot compute it.
 387 Node *PhaseMacroExpand::value_from_mem_phi(Node *mem, BasicType ft, const Type *phi_type, const TypeOopPtr *adr_t, AllocateNode *alloc, Node_Stack *value_phis, int level) {
 388   assert(mem-&gt;is_Phi(), &quot;sanity&quot;);
</pre>
<hr />
<pre>
 479       phi-&gt;init_req(j, values.at(j));
 480     }
 481   }
 482   return phi;
 483 }
 484 
 485 // Search the last value stored into the object&#39;s field.
 486 Node *PhaseMacroExpand::value_from_mem(Node *sfpt_mem, Node *sfpt_ctl, BasicType ft, const Type *ftype, const TypeOopPtr *adr_t, AllocateNode *alloc) {
 487   assert(adr_t-&gt;is_known_instance_field(), &quot;instance required&quot;);
 488   int instance_id = adr_t-&gt;instance_id();
 489   assert((uint)instance_id == alloc-&gt;_idx, &quot;wrong allocation&quot;);
 490 
 491   int alias_idx = C-&gt;get_alias_index(adr_t);
 492   int offset = adr_t-&gt;offset();
 493   Node *start_mem = C-&gt;start()-&gt;proj_out_or_null(TypeFunc::Memory);
 494   Node *alloc_ctrl = alloc-&gt;in(TypeFunc::Control);
 495   Node *alloc_mem = alloc-&gt;in(TypeFunc::Memory);
 496   Arena *a = Thread::current()-&gt;resource_area();
 497   VectorSet visited(a);
 498 
<span class="line-removed"> 499 </span>
 500   bool done = sfpt_mem == alloc_mem;
 501   Node *mem = sfpt_mem;
 502   while (!done) {
 503     if (visited.test_set(mem-&gt;_idx)) {
 504       return NULL;  // found a loop, give up
 505     }
 506     mem = scan_mem_chain(mem, alias_idx, offset, start_mem, alloc, &amp;_igvn);
 507     if (mem == start_mem || mem == alloc_mem) {
 508       done = true;  // hit a sentinel, return appropriate 0 value
 509     } else if (mem-&gt;is_Initialize()) {
 510       mem = mem-&gt;as_Initialize()-&gt;find_captured_store(offset, type2aelembytes(ft), &amp;_igvn);
 511       if (mem == NULL) {
 512         done = true; // Something go wrong.
 513       } else if (mem-&gt;is_Store()) {
 514         const TypePtr* atype = mem-&gt;as_Store()-&gt;adr_type();
 515         assert(C-&gt;get_alias_index(atype) == Compile::AliasIdxRaw, &quot;store is correct memory slice&quot;);
 516         done = true;
 517       }
 518     } else if (mem-&gt;is_Store()) {
 519       const TypeOopPtr* atype = mem-&gt;as_Store()-&gt;adr_type()-&gt;isa_oopptr();
</pre>
<hr />
<pre>
 769 #endif
 770                                                  first_ind, nfields);
 771     sobj-&gt;init_req(0, C-&gt;root());
 772     transform_later(sobj);
 773 
 774     // Scan object&#39;s fields adding an input to the safepoint for each field.
 775     for (int j = 0; j &lt; nfields; j++) {
 776       intptr_t offset;
 777       ciField* field = NULL;
 778       if (iklass != NULL) {
 779         field = iklass-&gt;nonstatic_field_at(j);
 780         offset = field-&gt;offset();
 781         elem_type = field-&gt;type();
 782         basic_elem_type = field-&gt;layout_type();
 783       } else {
 784         offset = array_base + j * (intptr_t)element_size;
 785       }
 786 
 787       const Type *field_type;
 788       // The next code is taken from Parse::do_get_xxx().
<span class="line-modified"> 789       if (basic_elem_type == T_OBJECT || basic_elem_type == T_ARRAY) {</span>
 790         if (!elem_type-&gt;is_loaded()) {
 791           field_type = TypeInstPtr::BOTTOM;
 792         } else if (field != NULL &amp;&amp; field-&gt;is_static_constant()) {
 793           // This can happen if the constant oop is non-perm.
 794           ciObject* con = field-&gt;constant_value().as_object();
 795           // Do not &quot;join&quot; in the previous type; it doesn&#39;t add value,
 796           // and may yield a vacuous result if the field is of interface type.
 797           field_type = TypeOopPtr::make_from_constant(con)-&gt;isa_oopptr();
 798           assert(field_type != NULL, &quot;field singleton type must be consistent&quot;);
 799         } else {
 800           field_type = TypeOopPtr::make_from_klass(elem_type-&gt;as_klass());
 801         }
 802         if (UseCompressedOops) {
 803           field_type = field_type-&gt;make_narrowoop();
 804           basic_elem_type = T_NARROWOOP;
 805         }
 806       } else {
 807         field_type = Type::get_const_basic_type(basic_elem_type);
 808       }
 809 
</pre>
<hr />
<pre>
 990     // First disconnect stores captured by Initialize node.
 991     // If Initialize node is eliminated first in the following code,
 992     // it will kill such stores and DUIterator_Last will assert.
 993     for (DUIterator_Fast jmax, j = _resproj-&gt;fast_outs(jmax);  j &lt; jmax; j++) {
 994       Node *use = _resproj-&gt;fast_out(j);
 995       if (use-&gt;is_AddP()) {
 996         // raw memory addresses used only by the initialization
 997         _igvn.replace_node(use, C-&gt;top());
 998         --j; --jmax;
 999       }
1000     }
1001     for (DUIterator_Last jmin, j = _resproj-&gt;last_outs(jmin); j &gt;= jmin; ) {
1002       Node *use = _resproj-&gt;last_out(j);
1003       uint oc1 = _resproj-&gt;outcnt();
1004       if (use-&gt;is_Initialize()) {
1005         // Eliminate Initialize node.
1006         InitializeNode *init = use-&gt;as_Initialize();
1007         assert(init-&gt;outcnt() &lt;= 2, &quot;only a control and memory projection expected&quot;);
1008         Node *ctrl_proj = init-&gt;proj_out_or_null(TypeFunc::Control);
1009         if (ctrl_proj != NULL) {
<span class="line-modified">1010            assert(init-&gt;in(TypeFunc::Control) == _fallthroughcatchproj, &quot;allocation control projection&quot;);</span>
<span class="line-modified">1011           _igvn.replace_node(ctrl_proj, _fallthroughcatchproj);</span>



1012         }
1013         Node *mem_proj = init-&gt;proj_out_or_null(TypeFunc::Memory);
1014         if (mem_proj != NULL) {
1015           Node *mem = init-&gt;in(TypeFunc::Memory);
1016 #ifdef ASSERT
1017           if (mem-&gt;is_MergeMem()) {
1018             assert(mem-&gt;in(TypeFunc::Memory) == _memproj_fallthrough, &quot;allocation memory projection&quot;);
1019           } else {
1020             assert(mem == _memproj_fallthrough, &quot;allocation memory projection&quot;);
1021           }
1022 #endif
1023           _igvn.replace_node(mem_proj, mem);
1024         }
1025       } else  {
1026         assert(false, &quot;only Initialize or AddP expected&quot;);
1027       }
1028       j -= (oc1 - _resproj-&gt;outcnt());
1029     }
1030   }
1031   if (_fallthroughcatchproj != NULL) {
</pre>
<hr />
<pre>
1233 // Allocations bigger than this always go the slow route.
1234 // This value must be small enough that allocation attempts that need to
1235 // trigger exceptions go the slow route.  Also, it must be small enough so
1236 // that heap_top + size_in_bytes does not wrap around the 4Gig limit.
1237 //=============================================================================j//
1238 // %%% Here is an old comment from parseHelper.cpp; is it outdated?
1239 // The allocator will coalesce int-&gt;oop copies away.  See comment in
1240 // coalesce.cpp about how this works.  It depends critically on the exact
1241 // code shape produced here, so if you are changing this code shape
1242 // make sure the GC info for the heap-top is correct in and around the
1243 // slow-path call.
1244 //
1245 
1246 void PhaseMacroExpand::expand_allocate_common(
1247             AllocateNode* alloc, // allocation node to be expanded
1248             Node* length,  // array length for an array allocation
1249             const TypeFunc* slow_call_type, // Type of slow call
1250             address slow_call_address  // Address of slow call
1251     )
1252 {
<span class="line-removed">1253 </span>
1254   Node* ctrl = alloc-&gt;in(TypeFunc::Control);
1255   Node* mem  = alloc-&gt;in(TypeFunc::Memory);
1256   Node* i_o  = alloc-&gt;in(TypeFunc::I_O);
1257   Node* size_in_bytes     = alloc-&gt;in(AllocateNode::AllocSize);
1258   Node* klass_node        = alloc-&gt;in(AllocateNode::KlassNode);
1259   Node* initial_slow_test = alloc-&gt;in(AllocateNode::InitialTest);
<span class="line-removed">1260 </span>
1261   assert(ctrl != NULL, &quot;must have control&quot;);

1262   // We need a Region and corresponding Phi&#39;s to merge the slow-path and fast-path results.
1263   // they will not be used if &quot;always_slow&quot; is set
1264   enum { slow_result_path = 1, fast_result_path = 2 };
1265   Node *result_region = NULL;
1266   Node *result_phi_rawmem = NULL;
1267   Node *result_phi_rawoop = NULL;
1268   Node *result_phi_i_o = NULL;
1269 
1270   // The initial slow comparison is a size check, the comparison
1271   // we want to do is a BoolTest::gt
<span class="line-modified">1272   bool always_slow = false;</span>
1273   int tv = _igvn.find_int_con(initial_slow_test, -1);
1274   if (tv &gt;= 0) {
<span class="line-modified">1275     always_slow = (tv == 1);</span>




1276     initial_slow_test = NULL;
1277   } else {
1278     initial_slow_test = BoolNode::make_predicate(initial_slow_test, &amp;_igvn);
1279   }
1280 
1281   if (C-&gt;env()-&gt;dtrace_alloc_probes() ||
1282       (!UseTLAB &amp;&amp; !Universe::heap()-&gt;supports_inline_contig_alloc())) {
1283     // Force slow-path allocation
<span class="line-modified">1284     always_slow = true;</span>
1285     initial_slow_test = NULL;
1286   }
1287 
















1288 
1289   enum { too_big_or_final_path = 1, need_gc_path = 2 };
1290   Node *slow_region = NULL;
1291   Node *toobig_false = ctrl;
1292 
<span class="line-removed">1293   assert (initial_slow_test == NULL || !always_slow, &quot;arguments must be consistent&quot;);</span>
1294   // generate the initial test if necessary
1295   if (initial_slow_test != NULL ) {

1296     slow_region = new RegionNode(3);
1297 
1298     // Now make the initial failure test.  Usually a too-big test but
1299     // might be a TRUE for finalizers or a fancy class check for
1300     // newInstance0.
1301     IfNode *toobig_iff = new IfNode(ctrl, initial_slow_test, PROB_MIN, COUNT_UNKNOWN);
1302     transform_later(toobig_iff);
1303     // Plug the failing-too-big test into the slow-path region
1304     Node *toobig_true = new IfTrueNode( toobig_iff );
1305     transform_later(toobig_true);
1306     slow_region    -&gt;init_req( too_big_or_final_path, toobig_true );
1307     toobig_false = new IfFalseNode( toobig_iff );
1308     transform_later(toobig_false);
<span class="line-modified">1309   } else {         // No initial test, just fall into next case</span>


1310     toobig_false = ctrl;
1311     debug_only(slow_region = NodeSentinel);
1312   }
1313 










1314   Node *slow_mem = mem;  // save the current memory state for slow path
1315   // generate the fast allocation code unless we know that the initial test will always go slow
<span class="line-modified">1316   if (!always_slow) {</span>
1317     // Fast path modifies only raw memory.
1318     if (mem-&gt;is_MergeMem()) {
1319       mem = mem-&gt;as_MergeMem()-&gt;memory_at(Compile::AliasIdxRaw);
1320     }
1321 
1322     // allocate the Region and Phi nodes for the result
1323     result_region = new RegionNode(3);
1324     result_phi_rawmem = new PhiNode(result_region, Type::MEMORY, TypeRawPtr::BOTTOM);
<span class="line-removed">1325     result_phi_rawoop = new PhiNode(result_region, TypeRawPtr::BOTTOM);</span>
1326     result_phi_i_o    = new PhiNode(result_region, Type::ABIO); // I/O is used for Prefetch
1327 
1328     // Grab regular I/O before optional prefetch may change it.
1329     // Slow-path does no I/O so just set it to the original I/O.
1330     result_phi_i_o-&gt;init_req(slow_result_path, i_o);
1331 
<span class="line-removed">1332     Node* needgc_ctrl = NULL;</span>
1333     // Name successful fast-path variables
1334     Node* fast_oop_ctrl;
1335     Node* fast_oop_rawmem;



1336 
<span class="line-modified">1337     intx prefetch_lines = length != NULL ? AllocatePrefetchLines : AllocateInstancePrefetchLines;</span>
<span class="line-modified">1338 </span>
<span class="line-modified">1339     BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();</span>
<span class="line-modified">1340     Node* fast_oop = bs-&gt;obj_allocate(this, ctrl, mem, toobig_false, size_in_bytes, i_o, needgc_ctrl,</span>
<span class="line-modified">1341                                       fast_oop_ctrl, fast_oop_rawmem,</span>
<span class="line-modified">1342                                       prefetch_lines);</span>
<span class="line-modified">1343 </span>
<span class="line-modified">1344     if (initial_slow_test) {</span>
<span class="line-modified">1345       slow_region-&gt;init_req(need_gc_path, needgc_ctrl);</span>
<span class="line-modified">1346       // This completes all paths into the slow merge point</span>
<span class="line-removed">1347       transform_later(slow_region);</span>
<span class="line-removed">1348     } else {                      // No initial slow path needed!</span>
<span class="line-removed">1349       // Just fall from the need-GC path straight into the VM call.</span>
<span class="line-removed">1350       slow_region = needgc_ctrl;</span>
<span class="line-removed">1351     }</span>
<span class="line-removed">1352 </span>
<span class="line-removed">1353     InitializeNode* init = alloc-&gt;initialization();</span>
<span class="line-removed">1354     fast_oop_rawmem = initialize_object(alloc,</span>
<span class="line-removed">1355                                         fast_oop_ctrl, fast_oop_rawmem, fast_oop,</span>
<span class="line-removed">1356                                         klass_node, length, size_in_bytes);</span>
<span class="line-removed">1357 </span>
<span class="line-removed">1358     // If initialization is performed by an array copy, any required</span>
<span class="line-removed">1359     // MemBarStoreStore was already added. If the object does not</span>
<span class="line-removed">1360     // escape no need for a MemBarStoreStore. If the object does not</span>
<span class="line-removed">1361     // escape in its initializer and memory barrier (MemBarStoreStore or</span>
<span class="line-removed">1362     // stronger) is already added at exit of initializer, also no need</span>
<span class="line-removed">1363     // for a MemBarStoreStore. Otherwise we need a MemBarStoreStore</span>
<span class="line-removed">1364     // so that stores that initialize this object can&#39;t be reordered</span>
<span class="line-removed">1365     // with a subsequent store that makes this object accessible by</span>
<span class="line-removed">1366     // other threads.</span>
<span class="line-removed">1367     // Other threads include java threads and JVM internal threads</span>
<span class="line-removed">1368     // (for example concurrent GC threads). Current concurrent GC</span>
<span class="line-removed">1369     // implementation: CMS and G1 will not scan newly created object,</span>
<span class="line-removed">1370     // so it&#39;s safe to skip storestore barrier when allocation does</span>
<span class="line-removed">1371     // not escape.</span>
<span class="line-removed">1372     if (!alloc-&gt;does_not_escape_thread() &amp;&amp;</span>
<span class="line-removed">1373         !alloc-&gt;is_allocation_MemBar_redundant() &amp;&amp;</span>
<span class="line-removed">1374         (init == NULL || !init-&gt;is_complete_with_arraycopy())) {</span>
<span class="line-removed">1375       if (init == NULL || init-&gt;req() &lt; InitializeNode::RawStores) {</span>
<span class="line-removed">1376         // No InitializeNode or no stores captured by zeroing</span>
<span class="line-removed">1377         // elimination. Simply add the MemBarStoreStore after object</span>
<span class="line-removed">1378         // initialization.</span>
<span class="line-removed">1379         MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);</span>
<span class="line-removed">1380         transform_later(mb);</span>
<span class="line-removed">1381 </span>
<span class="line-removed">1382         mb-&gt;init_req(TypeFunc::Memory, fast_oop_rawmem);</span>
<span class="line-removed">1383         mb-&gt;init_req(TypeFunc::Control, fast_oop_ctrl);</span>
<span class="line-removed">1384         fast_oop_ctrl = new ProjNode(mb,TypeFunc::Control);</span>
<span class="line-removed">1385         transform_later(fast_oop_ctrl);</span>
<span class="line-removed">1386         fast_oop_rawmem = new ProjNode(mb,TypeFunc::Memory);</span>
<span class="line-removed">1387         transform_later(fast_oop_rawmem);</span>
1388       } else {
<span class="line-modified">1389         // Add the MemBarStoreStore after the InitializeNode so that</span>
<span class="line-modified">1390         // all stores performing the initialization that were moved</span>
<span class="line-modified">1391         // before the InitializeNode happen before the storestore</span>
<span class="line-removed">1392         // barrier.</span>
<span class="line-removed">1393 </span>
<span class="line-removed">1394         Node* init_ctrl = init-&gt;proj_out_or_null(TypeFunc::Control);</span>
<span class="line-removed">1395         Node* init_mem = init-&gt;proj_out_or_null(TypeFunc::Memory);</span>
<span class="line-removed">1396 </span>
<span class="line-removed">1397         MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);</span>
<span class="line-removed">1398         transform_later(mb);</span>
<span class="line-removed">1399 </span>
<span class="line-removed">1400         Node* ctrl = new ProjNode(init,TypeFunc::Control);</span>
<span class="line-removed">1401         transform_later(ctrl);</span>
<span class="line-removed">1402         Node* mem = new ProjNode(init,TypeFunc::Memory);</span>
<span class="line-removed">1403         transform_later(mem);</span>
<span class="line-removed">1404 </span>
<span class="line-removed">1405         // The MemBarStoreStore depends on control and memory coming</span>
<span class="line-removed">1406         // from the InitializeNode</span>
<span class="line-removed">1407         mb-&gt;init_req(TypeFunc::Memory, mem);</span>
<span class="line-removed">1408         mb-&gt;init_req(TypeFunc::Control, ctrl);</span>
<span class="line-removed">1409 </span>
<span class="line-removed">1410         ctrl = new ProjNode(mb,TypeFunc::Control);</span>
<span class="line-removed">1411         transform_later(ctrl);</span>
<span class="line-removed">1412         mem = new ProjNode(mb,TypeFunc::Memory);</span>
<span class="line-removed">1413         transform_later(mem);</span>
<span class="line-removed">1414 </span>
<span class="line-removed">1415         // All nodes that depended on the InitializeNode for control</span>
<span class="line-removed">1416         // and memory must now depend on the MemBarNode that itself</span>
<span class="line-removed">1417         // depends on the InitializeNode</span>
<span class="line-removed">1418         if (init_ctrl != NULL) {</span>
<span class="line-removed">1419           _igvn.replace_node(init_ctrl, ctrl);</span>
<span class="line-removed">1420         }</span>
<span class="line-removed">1421         if (init_mem != NULL) {</span>
<span class="line-removed">1422           _igvn.replace_node(init_mem, mem);</span>
<span class="line-removed">1423         }</span>
1424       }
<span class="line-removed">1425     }</span>
<span class="line-removed">1426 </span>
<span class="line-removed">1427     if (C-&gt;env()-&gt;dtrace_extended_probes()) {</span>
<span class="line-removed">1428       // Slow-path call</span>
<span class="line-removed">1429       int size = TypeFunc::Parms + 2;</span>
<span class="line-removed">1430       CallLeafNode *call = new CallLeafNode(OptoRuntime::dtrace_object_alloc_Type(),</span>
<span class="line-removed">1431                                             CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc_base),</span>
<span class="line-removed">1432                                             &quot;dtrace_object_alloc&quot;,</span>
<span class="line-removed">1433                                             TypeRawPtr::BOTTOM);</span>
1434 
<span class="line-modified">1435       // Get base of thread-local storage area</span>
<span class="line-modified">1436       Node* thread = new ThreadLocalNode();</span>
<span class="line-modified">1437       transform_later(thread);</span>



1438 
<span class="line-modified">1439       call-&gt;init_req(TypeFunc::Parms+0, thread);</span>
<span class="line-modified">1440       call-&gt;init_req(TypeFunc::Parms+1, fast_oop);</span>
<span class="line-modified">1441       call-&gt;init_req(TypeFunc::Control, fast_oop_ctrl);</span>
<span class="line-modified">1442       call-&gt;init_req(TypeFunc::I_O    , top()); // does no i/o</span>
<span class="line-modified">1443       call-&gt;init_req(TypeFunc::Memory , fast_oop_rawmem);</span>
<span class="line-modified">1444       call-&gt;init_req(TypeFunc::ReturnAdr, alloc-&gt;in(TypeFunc::ReturnAdr));</span>
<span class="line-removed">1445       call-&gt;init_req(TypeFunc::FramePtr, alloc-&gt;in(TypeFunc::FramePtr));</span>
<span class="line-removed">1446       transform_later(call);</span>
<span class="line-removed">1447       fast_oop_ctrl = new ProjNode(call,TypeFunc::Control);</span>
<span class="line-removed">1448       transform_later(fast_oop_ctrl);</span>
<span class="line-removed">1449       fast_oop_rawmem = new ProjNode(call,TypeFunc::Memory);</span>
<span class="line-removed">1450       transform_later(fast_oop_rawmem);</span>
1451     }
1452 
1453     // Plug in the successful fast-path into the result merge point
1454     result_region    -&gt;init_req(fast_result_path, fast_oop_ctrl);
<span class="line-removed">1455     result_phi_rawoop-&gt;init_req(fast_result_path, fast_oop);</span>
1456     result_phi_i_o   -&gt;init_req(fast_result_path, i_o);
1457     result_phi_rawmem-&gt;init_req(fast_result_path, fast_oop_rawmem);
1458   } else {
1459     slow_region = ctrl;
1460     result_phi_i_o = i_o; // Rename it to use in the following code.
1461   }
1462 
1463   // Generate slow-path call
1464   CallNode *call = new CallStaticJavaNode(slow_call_type, slow_call_address,
1465                                OptoRuntime::stub_name(slow_call_address),
1466                                alloc-&gt;jvms()-&gt;bci(),
1467                                TypePtr::BOTTOM);
<span class="line-modified">1468   call-&gt;init_req( TypeFunc::Control, slow_region );</span>
<span class="line-modified">1469   call-&gt;init_req( TypeFunc::I_O    , top() )     ;   // does no i/o</span>
<span class="line-modified">1470   call-&gt;init_req( TypeFunc::Memory , slow_mem ); // may gc ptrs</span>
<span class="line-modified">1471   call-&gt;init_req( TypeFunc::ReturnAdr, alloc-&gt;in(TypeFunc::ReturnAdr) );</span>
<span class="line-modified">1472   call-&gt;init_req( TypeFunc::FramePtr, alloc-&gt;in(TypeFunc::FramePtr) );</span>
1473 
1474   call-&gt;init_req(TypeFunc::Parms+0, klass_node);
1475   if (length != NULL) {
1476     call-&gt;init_req(TypeFunc::Parms+1, length);
1477   }
1478 
1479   // Copy debug information and adjust JVMState information, then replace
1480   // allocate node with the call
1481   copy_call_debug_info((CallNode *) alloc,  call);
<span class="line-modified">1482   if (!always_slow) {</span>
1483     call-&gt;set_cnt(PROB_UNLIKELY_MAG(4));  // Same effect as RC_UNCOMMON.
1484   } else {
1485     // Hook i_o projection to avoid its elimination during allocation
1486     // replacement (when only a slow call is generated).
1487     call-&gt;set_req(TypeFunc::I_O, result_phi_i_o);
1488   }
1489   _igvn.replace_node(alloc, call);
1490   transform_later(call);
1491 
1492   // Identify the output projections from the allocate node and
1493   // adjust any references to them.
1494   // The control and io projections look like:
1495   //
1496   //        v---Proj(ctrl) &lt;-----+   v---CatchProj(ctrl)
1497   //  Allocate                   Catch
1498   //        ^---Proj(io) &lt;-------+   ^---CatchProj(io)
1499   //
1500   //  We are interested in the CatchProj nodes.
1501   //
1502   extract_call_projections(call);
1503 
1504   // An allocate node has separate memory projections for the uses on
1505   // the control and i_o paths. Replace the control memory projection with
1506   // result_phi_rawmem (unless we are only generating a slow call when
1507   // both memory projections are combined)
<span class="line-modified">1508   if (!always_slow &amp;&amp; _memproj_fallthrough != NULL) {</span>
<span class="line-modified">1509     for (DUIterator_Fast imax, i = _memproj_fallthrough-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-removed">1510       Node *use = _memproj_fallthrough-&gt;fast_out(i);</span>
<span class="line-removed">1511       _igvn.rehash_node_delayed(use);</span>
<span class="line-removed">1512       imax -= replace_input(use, _memproj_fallthrough, result_phi_rawmem);</span>
<span class="line-removed">1513       // back up iterator</span>
<span class="line-removed">1514       --i;</span>
<span class="line-removed">1515     }</span>
1516   }
1517   // Now change uses of _memproj_catchall to use _memproj_fallthrough and delete
1518   // _memproj_catchall so we end up with a call that has only 1 memory projection.
1519   if (_memproj_catchall != NULL ) {
1520     if (_memproj_fallthrough == NULL) {
1521       _memproj_fallthrough = new ProjNode(call, TypeFunc::Memory);
1522       transform_later(_memproj_fallthrough);
1523     }
<span class="line-modified">1524     for (DUIterator_Fast imax, i = _memproj_catchall-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-removed">1525       Node *use = _memproj_catchall-&gt;fast_out(i);</span>
<span class="line-removed">1526       _igvn.rehash_node_delayed(use);</span>
<span class="line-removed">1527       imax -= replace_input(use, _memproj_catchall, _memproj_fallthrough);</span>
<span class="line-removed">1528       // back up iterator</span>
<span class="line-removed">1529       --i;</span>
<span class="line-removed">1530     }</span>
<span class="line-removed">1531     assert(_memproj_catchall-&gt;outcnt() == 0, &quot;all uses must be deleted&quot;);</span>
1532     _igvn.remove_dead_node(_memproj_catchall);
1533   }
1534 
1535   // An allocate node has separate i_o projections for the uses on the control
1536   // and i_o paths. Always replace the control i_o projection with result i_o
1537   // otherwise incoming i_o become dead when only a slow call is generated
1538   // (it is different from memory projections where both projections are
1539   // combined in such case).
1540   if (_ioproj_fallthrough != NULL) {
<span class="line-modified">1541     for (DUIterator_Fast imax, i = _ioproj_fallthrough-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-removed">1542       Node *use = _ioproj_fallthrough-&gt;fast_out(i);</span>
<span class="line-removed">1543       _igvn.rehash_node_delayed(use);</span>
<span class="line-removed">1544       imax -= replace_input(use, _ioproj_fallthrough, result_phi_i_o);</span>
<span class="line-removed">1545       // back up iterator</span>
<span class="line-removed">1546       --i;</span>
<span class="line-removed">1547     }</span>
1548   }
1549   // Now change uses of _ioproj_catchall to use _ioproj_fallthrough and delete
1550   // _ioproj_catchall so we end up with a call that has only 1 i_o projection.
1551   if (_ioproj_catchall != NULL ) {
1552     if (_ioproj_fallthrough == NULL) {
1553       _ioproj_fallthrough = new ProjNode(call, TypeFunc::I_O);
1554       transform_later(_ioproj_fallthrough);
1555     }
<span class="line-modified">1556     for (DUIterator_Fast imax, i = _ioproj_catchall-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-removed">1557       Node *use = _ioproj_catchall-&gt;fast_out(i);</span>
<span class="line-removed">1558       _igvn.rehash_node_delayed(use);</span>
<span class="line-removed">1559       imax -= replace_input(use, _ioproj_catchall, _ioproj_fallthrough);</span>
<span class="line-removed">1560       // back up iterator</span>
<span class="line-removed">1561       --i;</span>
<span class="line-removed">1562     }</span>
<span class="line-removed">1563     assert(_ioproj_catchall-&gt;outcnt() == 0, &quot;all uses must be deleted&quot;);</span>
1564     _igvn.remove_dead_node(_ioproj_catchall);
1565   }
1566 
1567   // if we generated only a slow call, we are done
<span class="line-modified">1568   if (always_slow) {</span>
1569     // Now we can unhook i_o.
1570     if (result_phi_i_o-&gt;outcnt() &gt; 1) {
1571       call-&gt;set_req(TypeFunc::I_O, top());
1572     } else {
<span class="line-modified">1573       assert(result_phi_i_o-&gt;unique_ctrl_out() == call, &quot;&quot;);</span>
1574       // Case of new array with negative size known during compilation.
1575       // AllocateArrayNode::Ideal() optimization disconnect unreachable
1576       // following code since call to runtime will throw exception.
1577       // As result there will be no users of i_o after the call.
1578       // Leave i_o attached to this call to avoid problems in preceding graph.
1579     }
1580     return;
1581   }
1582 
<span class="line-removed">1583 </span>
1584   if (_fallthroughcatchproj != NULL) {
1585     ctrl = _fallthroughcatchproj-&gt;clone();
1586     transform_later(ctrl);
1587     _igvn.replace_node(_fallthroughcatchproj, result_region);
1588   } else {
1589     ctrl = top();
1590   }
1591   Node *slow_result;
1592   if (_resproj == NULL) {
1593     // no uses of the allocation result
1594     slow_result = top();
1595   } else {
1596     slow_result = _resproj-&gt;clone();
1597     transform_later(slow_result);
1598     _igvn.replace_node(_resproj, result_phi_rawoop);
1599   }
1600 
1601   // Plug slow-path into result merge point
<span class="line-modified">1602   result_region    -&gt;init_req( slow_result_path, ctrl );</span>
<span class="line-removed">1603   result_phi_rawoop-&gt;init_req( slow_result_path, slow_result);</span>
<span class="line-removed">1604   result_phi_rawmem-&gt;init_req( slow_result_path, _memproj_fallthrough );</span>
1605   transform_later(result_region);
<span class="line-modified">1606   transform_later(result_phi_rawoop);</span>




1607   transform_later(result_phi_rawmem);
1608   transform_later(result_phi_i_o);
1609   // This completes all paths into the result merge point
1610 }
1611 































































































































































1612 
1613 // Helper for PhaseMacroExpand::expand_allocate_common.
1614 // Initializes the newly-allocated storage.
1615 Node*
1616 PhaseMacroExpand::initialize_object(AllocateNode* alloc,
1617                                     Node* control, Node* rawmem, Node* object,
1618                                     Node* klass_node, Node* length,
1619                                     Node* size_in_bytes) {
1620   InitializeNode* init = alloc-&gt;initialization();
1621   // Store the klass &amp; mark bits
<span class="line-modified">1622   Node* mark_node = NULL;</span>
<span class="line-modified">1623   // For now only enable fast locking for non-array types</span>
<span class="line-modified">1624   if (UseBiasedLocking &amp;&amp; (length == NULL)) {</span>
<span class="line-removed">1625     mark_node = make_load(control, rawmem, klass_node, in_bytes(Klass::prototype_header_offset()), TypeRawPtr::BOTTOM, T_ADDRESS);</span>
<span class="line-removed">1626   } else {</span>
<span class="line-removed">1627     mark_node = makecon(TypeRawPtr::make((address)markOopDesc::prototype()));</span>
1628   }
<span class="line-modified">1629   rawmem = make_store(control, rawmem, object, oopDesc::mark_offset_in_bytes(), mark_node, T_ADDRESS);</span>
1630 
1631   rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);
1632   int header_size = alloc-&gt;minimum_header_size();  // conservatively small
1633 
1634   // Array length
1635   if (length != NULL) {         // Arrays need length field
1636     rawmem = make_store(control, rawmem, object, arrayOopDesc::length_offset_in_bytes(), length, T_INT);
1637     // conservatively small header size:
1638     header_size = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1639     ciKlass* k = _igvn.type(klass_node)-&gt;is_klassptr()-&gt;klass();
1640     if (k-&gt;is_array_klass())    // we know the exact header size in most cases:
1641       header_size = Klass::layout_helper_header_size(k-&gt;layout_helper());
1642   }
1643 
1644   // Clear the object body, if necessary.
1645   if (init == NULL) {
1646     // The init has somehow disappeared; be cautious and clear everything.
1647     //
1648     // This can happen if a node is allocated but an uncommon trap occurs
1649     // immediately.  In this case, the Initialize gets associated with the
</pre>
<hr />
<pre>
2165      *      // Done.
2166      *    } else {
2167      *      slow_path:
2168      *      OptoRuntime::complete_monitor_locking_Java(obj);
2169      *    }
2170      *  }
2171      */
2172 
2173     region  = new RegionNode(5);
2174     // create a Phi for the memory state
2175     mem_phi = new PhiNode( region, Type::MEMORY, TypeRawPtr::BOTTOM);
2176 
2177     Node* fast_lock_region  = new RegionNode(3);
2178     Node* fast_lock_mem_phi = new PhiNode( fast_lock_region, Type::MEMORY, TypeRawPtr::BOTTOM);
2179 
2180     // First, check mark word for the biased lock pattern.
2181     Node* mark_node = make_load(ctrl, mem, obj, oopDesc::mark_offset_in_bytes(), TypeX_X, TypeX_X-&gt;basic_type());
2182 
2183     // Get fast path - mark word has the biased lock pattern.
2184     ctrl = opt_bits_test(ctrl, fast_lock_region, 1, mark_node,
<span class="line-modified">2185                          markOopDesc::biased_lock_mask_in_place,</span>
<span class="line-modified">2186                          markOopDesc::biased_lock_pattern, true);</span>
2187     // fast_lock_region-&gt;in(1) is set to slow path.
2188     fast_lock_mem_phi-&gt;init_req(1, mem);
2189 
2190     // Now check that the lock is biased to the current thread and has
2191     // the same epoch and bias as Klass::_prototype_header.
2192 
2193     // Special-case a fresh allocation to avoid building nodes:
2194     Node* klass_node = AllocateNode::Ideal_klass(obj, &amp;_igvn);
2195     if (klass_node == NULL) {
2196       Node* k_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());
2197       klass_node = transform_later(LoadKlassNode::make(_igvn, NULL, mem, k_adr, _igvn.type(k_adr)-&gt;is_ptr()));
2198 #ifdef _LP64
2199       if (UseCompressedClassPointers &amp;&amp; klass_node-&gt;is_DecodeNKlass()) {
2200         assert(klass_node-&gt;in(1)-&gt;Opcode() == Op_LoadNKlass, &quot;sanity&quot;);
2201         klass_node-&gt;in(1)-&gt;init_req(0, ctrl);
2202       } else
2203 #endif
2204       klass_node-&gt;init_req(0, ctrl);
2205     }
2206     Node *proto_node = make_load(ctrl, mem, klass_node, in_bytes(Klass::prototype_header_offset()), TypeX_X, TypeX_X-&gt;basic_type());
2207 
2208     Node* thread = transform_later(new ThreadLocalNode());
2209     Node* cast_thread = transform_later(new CastP2XNode(ctrl, thread));
2210     Node* o_node = transform_later(new OrXNode(cast_thread, proto_node));
2211     Node* x_node = transform_later(new XorXNode(o_node, mark_node));
2212 
2213     // Get slow path - mark word does NOT match the value.

2214     Node* not_biased_ctrl =  opt_bits_test(ctrl, region, 3, x_node,
<span class="line-modified">2215                                       (~markOopDesc::age_mask_in_place), 0);</span>
2216     // region-&gt;in(3) is set to fast path - the object is biased to the current thread.
2217     mem_phi-&gt;init_req(3, mem);
2218 
2219 
2220     // Mark word does NOT match the value (thread | Klass::_prototype_header).
2221 
2222 
2223     // First, check biased pattern.
2224     // Get fast path - _prototype_header has the same biased lock pattern.
2225     ctrl =  opt_bits_test(not_biased_ctrl, fast_lock_region, 2, x_node,
<span class="line-modified">2226                           markOopDesc::biased_lock_mask_in_place, 0, true);</span>
2227 
2228     not_biased_ctrl = fast_lock_region-&gt;in(2); // Slow path
2229     // fast_lock_region-&gt;in(2) - the prototype header is no longer biased
2230     // and we have to revoke the bias on this object.
2231     // We are going to try to reset the mark of this object to the prototype
2232     // value and fall through to the CAS-based locking scheme.
2233     Node* adr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
2234     Node* cas = new StoreXConditionalNode(not_biased_ctrl, mem, adr,
2235                                           proto_node, mark_node);
2236     transform_later(cas);
2237     Node* proj = transform_later(new SCMemProjNode(cas));
2238     fast_lock_mem_phi-&gt;init_req(2, proj);
2239 
2240 
2241     // Second, check epoch bits.
2242     Node* rebiased_region  = new RegionNode(3);
2243     Node* old_phi = new PhiNode( rebiased_region, TypeX_X);
2244     Node* new_phi = new PhiNode( rebiased_region, TypeX_X);
2245 
2246     // Get slow path - mark word does NOT match epoch bits.
2247     Node* epoch_ctrl =  opt_bits_test(ctrl, rebiased_region, 1, x_node,
<span class="line-modified">2248                                       markOopDesc::epoch_mask_in_place, 0);</span>
2249     // The epoch of the current bias is not valid, attempt to rebias the object
2250     // toward the current thread.
2251     rebiased_region-&gt;init_req(2, epoch_ctrl);
2252     old_phi-&gt;init_req(2, mark_node);
2253     new_phi-&gt;init_req(2, o_node);
2254 
2255     // rebiased_region-&gt;in(1) is set to fast path.
2256     // The epoch of the current bias is still valid but we know
2257     // nothing about the owner; it might be set or it might be clear.
<span class="line-modified">2258     Node* cmask   = MakeConX(markOopDesc::biased_lock_mask_in_place |</span>
<span class="line-modified">2259                              markOopDesc::age_mask_in_place |</span>
<span class="line-modified">2260                              markOopDesc::epoch_mask_in_place);</span>
2261     Node* old = transform_later(new AndXNode(mark_node, cmask));
2262     cast_thread = transform_later(new CastP2XNode(ctrl, thread));
2263     Node* new_mark = transform_later(new OrXNode(cast_thread, old));
2264     old_phi-&gt;init_req(1, old);
2265     new_phi-&gt;init_req(1, new_mark);
2266 
2267     transform_later(rebiased_region);
2268     transform_later(old_phi);
2269     transform_later(new_phi);
2270 
2271     // Try to acquire the bias of the object using an atomic operation.
2272     // If this fails we will go in to the runtime to revoke the object&#39;s bias.
2273     cas = new StoreXConditionalNode(rebiased_region, mem, adr, new_phi, old_phi);
2274     transform_later(cas);
2275     proj = transform_later(new SCMemProjNode(cas));
2276 
2277     // Get slow path - Failed to CAS.
2278     not_biased_ctrl = opt_bits_test(rebiased_region, region, 4, cas, 0, 0);
2279     mem_phi-&gt;init_req(4, proj);
2280     // region-&gt;in(4) is set to fast path - the object is rebiased to the current thread.
</pre>
<hr />
<pre>
2355   Node* box = unlock-&gt;box_node();
2356 
2357   assert(!box-&gt;as_BoxLock()-&gt;is_eliminated(), &quot;sanity&quot;);
2358 
2359   // No need for a null check on unlock
2360 
2361   // Make the merge point
2362   Node *region;
2363   Node *mem_phi;
2364 
2365   if (UseOptoBiasInlining) {
2366     // Check for biased locking unlock case, which is a no-op.
2367     // See the full description in MacroAssembler::biased_locking_exit().
2368     region  = new RegionNode(4);
2369     // create a Phi for the memory state
2370     mem_phi = new PhiNode( region, Type::MEMORY, TypeRawPtr::BOTTOM);
2371     mem_phi-&gt;init_req(3, mem);
2372 
2373     Node* mark_node = make_load(ctrl, mem, obj, oopDesc::mark_offset_in_bytes(), TypeX_X, TypeX_X-&gt;basic_type());
2374     ctrl = opt_bits_test(ctrl, region, 3, mark_node,
<span class="line-modified">2375                          markOopDesc::biased_lock_mask_in_place,</span>
<span class="line-modified">2376                          markOopDesc::biased_lock_pattern);</span>
2377   } else {
2378     region  = new RegionNode(3);
2379     // create a Phi for the memory state
2380     mem_phi = new PhiNode( region, Type::MEMORY, TypeRawPtr::BOTTOM);
2381   }
2382 
2383   FastUnlockNode *funlock = new FastUnlockNode( ctrl, obj, box );
2384   funlock = transform_later( funlock )-&gt;as_FastUnlock();
2385   // Optimize test; set region slot 2
2386   Node *slow_path = opt_bits_test(ctrl, region, 2, funlock, 0, 0);
2387   Node *thread = transform_later(new ThreadLocalNode());
2388 
2389   CallNode *call = make_slow_call((CallNode *) unlock, OptoRuntime::complete_monitor_exit_Type(),
2390                                   CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C),
2391                                   &quot;complete_monitor_unlocking_C&quot;, slow_path, obj, box, thread);
2392 
2393   extract_call_projections(call);
2394 
2395   assert ( _ioproj_fallthrough == NULL &amp;&amp; _ioproj_catchall == NULL &amp;&amp;
2396            _memproj_catchall == NULL &amp;&amp; _catchallcatchproj == NULL, &quot;Unexpected projection from Lock&quot;);
</pre>
<hr />
<pre>
2398   // No exceptions for unlocking
2399   // Capture slow path
2400   // disconnect fall-through projection from call and create a new one
2401   // hook up users of fall-through projection to region
2402   Node *slow_ctrl = _fallthroughproj-&gt;clone();
2403   transform_later(slow_ctrl);
2404   _igvn.hash_delete(_fallthroughproj);
2405   _fallthroughproj-&gt;disconnect_inputs(NULL, C);
2406   region-&gt;init_req(1, slow_ctrl);
2407   // region inputs are now complete
2408   transform_later(region);
2409   _igvn.replace_node(_fallthroughproj, region);
2410 
2411   Node *memproj = transform_later(new ProjNode(call, TypeFunc::Memory) );
2412   mem_phi-&gt;init_req(1, memproj );
2413   mem_phi-&gt;init_req(2, mem);
2414   transform_later(mem_phi);
2415   _igvn.replace_node(_memproj_fallthrough, mem_phi);
2416 }
2417 





































2418 //---------------------------eliminate_macro_nodes----------------------
2419 // Eliminate scalar replaced allocations and associated locks.
2420 void PhaseMacroExpand::eliminate_macro_nodes() {
2421   if (C-&gt;macro_count() == 0)
2422     return;
2423 
2424   // First, attempt to eliminate locks
2425   int cnt = C-&gt;macro_count();
2426   for (int i=0; i &lt; cnt; i++) {
2427     Node *n = C-&gt;macro_node(i);
2428     if (n-&gt;is_AbstractLock()) { // Lock and Unlock nodes
2429       // Before elimination mark all associated (same box and obj)
2430       // lock and unlock nodes.
2431       mark_eliminated_locking_nodes(n-&gt;as_AbstractLock());
2432     }
2433   }
2434   bool progress = true;
2435   while (progress) {
2436     progress = false;
2437     for (int i = C-&gt;macro_count(); i &gt; 0; i--) {
</pre>
<hr />
<pre>
2454       Node * n = C-&gt;macro_node(i-1);
2455       bool success = false;
2456       debug_only(int old_macro_count = C-&gt;macro_count(););
2457       switch (n-&gt;class_id()) {
2458       case Node::Class_Allocate:
2459       case Node::Class_AllocateArray:
2460         success = eliminate_allocate_node(n-&gt;as_Allocate());
2461         break;
2462       case Node::Class_CallStaticJava:
2463         success = eliminate_boxing_node(n-&gt;as_CallStaticJava());
2464         break;
2465       case Node::Class_Lock:
2466       case Node::Class_Unlock:
2467         assert(!n-&gt;as_AbstractLock()-&gt;is_eliminated(), &quot;sanity&quot;);
2468         _has_locks = true;
2469         break;
2470       case Node::Class_ArrayCopy:
2471         break;
2472       case Node::Class_OuterStripMinedLoop:
2473         break;


2474       default:
2475         assert(n-&gt;Opcode() == Op_LoopLimit ||
2476                n-&gt;Opcode() == Op_Opaque1   ||
2477                n-&gt;Opcode() == Op_Opaque2   ||
2478                n-&gt;Opcode() == Op_Opaque3   ||
2479                BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;is_gc_barrier_node(n),
2480                &quot;unknown node type in macro list&quot;);
2481       }
2482       assert(success == (C-&gt;macro_count() &lt; old_macro_count), &quot;elimination reduces macro count&quot;);
2483       progress = progress || success;
2484     }
2485   }
2486 }
2487 
2488 //------------------------------expand_macro_nodes----------------------
2489 //  Returns true if a failure occurred.
2490 bool PhaseMacroExpand::expand_macro_nodes() {
2491   // Last attempt to eliminate macro nodes.
2492   eliminate_macro_nodes();
2493 
2494   // Make sure expansion will not cause node limit to be exceeded.
2495   // Worst case is a macro node gets expanded into about 200 nodes.
2496   // Allow 50% more for optimization.
2497   if (C-&gt;check_node_count(C-&gt;macro_count() * 300, &quot;out of nodes before macro expansion&quot; ) )
2498     return true;
2499 
2500   // Eliminate Opaque and LoopLimit nodes. Do it after all loop optimizations.
2501   bool progress = true;
2502   while (progress) {
2503     progress = false;
2504     for (int i = C-&gt;macro_count(); i &gt; 0; i--) {
<span class="line-modified">2505       Node * n = C-&gt;macro_node(i-1);</span>
2506       bool success = false;
2507       debug_only(int old_macro_count = C-&gt;macro_count(););
2508       if (n-&gt;Opcode() == Op_LoopLimit) {
2509         // Remove it from macro list and put on IGVN worklist to optimize.
2510         C-&gt;remove_macro_node(n);
2511         _igvn._worklist.push(n);
2512         success = true;
2513       } else if (n-&gt;Opcode() == Op_CallStaticJava) {
2514         // Remove it from macro list and put on IGVN worklist to optimize.
2515         C-&gt;remove_macro_node(n);
2516         _igvn._worklist.push(n);
2517         success = true;
2518       } else if (n-&gt;Opcode() == Op_Opaque1 || n-&gt;Opcode() == Op_Opaque2) {
2519         _igvn.replace_node(n, n-&gt;in(1));
2520         success = true;
2521 #if INCLUDE_RTM_OPT
2522       } else if ((n-&gt;Opcode() == Op_Opaque3) &amp;&amp; ((Opaque3Node*)n)-&gt;rtm_opt()) {
2523         assert(C-&gt;profile_rtm(), &quot;should be used only in rtm deoptimization code&quot;);
2524         assert((n-&gt;outcnt() == 1) &amp;&amp; n-&gt;unique_out()-&gt;is_Cmp(), &quot;&quot;);
2525         Node* cmp = n-&gt;unique_out();
</pre>
<hr />
<pre>
2530         assert((bol-&gt;outcnt() == 1) &amp;&amp; bol-&gt;unique_out()-&gt;is_If() &amp;&amp;
2531                (bol-&gt;_test._test == BoolTest::ne), &quot;&quot;);
2532         IfNode* ifn = bol-&gt;unique_out()-&gt;as_If();
2533         assert((ifn-&gt;outcnt() == 2) &amp;&amp;
2534                ifn-&gt;proj_out(1)-&gt;is_uncommon_trap_proj(Deoptimization::Reason_rtm_state_change) != NULL, &quot;&quot;);
2535 #endif
2536         Node* repl = n-&gt;in(1);
2537         if (!_has_locks) {
2538           // Remove RTM state check if there are no locks in the code.
2539           // Replace input to compare the same value.
2540           repl = (cmp-&gt;in(1) == n) ? cmp-&gt;in(2) : cmp-&gt;in(1);
2541         }
2542         _igvn.replace_node(n, repl);
2543         success = true;
2544 #endif
2545       } else if (n-&gt;Opcode() == Op_OuterStripMinedLoop) {
2546         n-&gt;as_OuterStripMinedLoop()-&gt;adjust_strip_mined_loop(&amp;_igvn);
2547         C-&gt;remove_macro_node(n);
2548         success = true;
2549       }
<span class="line-modified">2550       assert(success == (C-&gt;macro_count() &lt; old_macro_count), &quot;elimination reduces macro count&quot;);</span>
2551       progress = progress || success;
2552     }
2553   }
2554 
2555   // expand arraycopy &quot;macro&quot; nodes first
2556   // For ReduceBulkZeroing, we must first process all arraycopy nodes
2557   // before the allocate nodes are expanded.
<span class="line-modified">2558   int macro_idx = C-&gt;macro_count() - 1;</span>
<span class="line-modified">2559   while (macro_idx &gt;= 0) {</span>
<span class="line-removed">2560     Node * n = C-&gt;macro_node(macro_idx);</span>
2561     assert(n-&gt;is_macro(), &quot;only macro nodes expected here&quot;);
2562     if (_igvn.type(n) == Type::TOP || (n-&gt;in(0) != NULL &amp;&amp; n-&gt;in(0)-&gt;is_top())) {
2563       // node is unreachable, so don&#39;t try to expand it
2564       C-&gt;remove_macro_node(n);
<span class="line-modified">2565     } else if (n-&gt;is_ArrayCopy()){</span>
<span class="line-modified">2566       int macro_count = C-&gt;macro_count();</span>











2567       expand_arraycopy_node(n-&gt;as_ArrayCopy());
<span class="line-modified">2568       assert(C-&gt;macro_count() &lt; macro_count, &quot;must have deleted a node from macro list&quot;);</span>





2569     }
2570     if (C-&gt;failing())  return true;
<span class="line-removed">2571     macro_idx --;</span>
2572   }
2573 









2574   // expand &quot;macro&quot; nodes
2575   // nodes are removed from the macro list as they are processed
2576   while (C-&gt;macro_count() &gt; 0) {
2577     int macro_count = C-&gt;macro_count();
2578     Node * n = C-&gt;macro_node(macro_count-1);
2579     assert(n-&gt;is_macro(), &quot;only macro nodes expected here&quot;);
2580     if (_igvn.type(n) == Type::TOP || (n-&gt;in(0) != NULL &amp;&amp; n-&gt;in(0)-&gt;is_top())) {
2581       // node is unreachable, so don&#39;t try to expand it
2582       C-&gt;remove_macro_node(n);
2583       continue;
2584     }
2585     switch (n-&gt;class_id()) {
2586     case Node::Class_Allocate:
2587       expand_allocate(n-&gt;as_Allocate());
2588       break;
2589     case Node::Class_AllocateArray:
2590       expand_allocate_array(n-&gt;as_AllocateArray());
2591       break;
<span class="line-removed">2592     case Node::Class_Lock:</span>
<span class="line-removed">2593       expand_lock_node(n-&gt;as_Lock());</span>
<span class="line-removed">2594       break;</span>
<span class="line-removed">2595     case Node::Class_Unlock:</span>
<span class="line-removed">2596       expand_unlock_node(n-&gt;as_Unlock());</span>
<span class="line-removed">2597       break;</span>
2598     default:
2599       assert(false, &quot;unknown node type in macro list&quot;);
2600     }
2601     assert(C-&gt;macro_count() &lt; macro_count, &quot;must have deleted a node from macro list&quot;);
2602     if (C-&gt;failing())  return true;
2603   }
2604 
2605   _igvn.set_delay_transform(false);
2606   _igvn.optimize();
2607   if (C-&gt;failing())  return true;
2608   return false;
2609 }
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 2005, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;compiler/compileLog.hpp&quot;
  27 #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
  28 #include &quot;libadt/vectset.hpp&quot;
<span class="line-added">  29 #include &quot;memory/universe.hpp&quot;</span>
  30 #include &quot;opto/addnode.hpp&quot;
  31 #include &quot;opto/arraycopynode.hpp&quot;
  32 #include &quot;opto/callnode.hpp&quot;
  33 #include &quot;opto/castnode.hpp&quot;
  34 #include &quot;opto/cfgnode.hpp&quot;
  35 #include &quot;opto/compile.hpp&quot;
  36 #include &quot;opto/convertnode.hpp&quot;
  37 #include &quot;opto/graphKit.hpp&quot;
<span class="line-added">  38 #include &quot;opto/intrinsicnode.hpp&quot;</span>
  39 #include &quot;opto/locknode.hpp&quot;
  40 #include &quot;opto/loopnode.hpp&quot;
  41 #include &quot;opto/macro.hpp&quot;
  42 #include &quot;opto/memnode.hpp&quot;
  43 #include &quot;opto/narrowptrnode.hpp&quot;
  44 #include &quot;opto/node.hpp&quot;
  45 #include &quot;opto/opaquenode.hpp&quot;
  46 #include &quot;opto/phaseX.hpp&quot;
  47 #include &quot;opto/rootnode.hpp&quot;
  48 #include &quot;opto/runtime.hpp&quot;
  49 #include &quot;opto/subnode.hpp&quot;
<span class="line-added">  50 #include &quot;opto/subtypenode.hpp&quot;</span>
  51 #include &quot;opto/type.hpp&quot;
  52 #include &quot;runtime/sharedRuntime.hpp&quot;
  53 #include &quot;utilities/macros.hpp&quot;
<span class="line-added">  54 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  55 #if INCLUDE_G1GC
  56 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
  57 #endif // INCLUDE_G1GC
  58 #if INCLUDE_SHENANDOAHGC
  59 #include &quot;gc/shenandoah/c2/shenandoahBarrierSetC2.hpp&quot;
  60 #endif
  61 
  62 
  63 //
  64 // Replace any references to &quot;oldref&quot; in inputs to &quot;use&quot; with &quot;newref&quot;.
  65 // Returns the number of replacements made.
  66 //
  67 int PhaseMacroExpand::replace_input(Node *use, Node *oldref, Node *newref) {
  68   int nreplacements = 0;
  69   uint req = use-&gt;req();
  70   for (uint j = 0; j &lt; use-&gt;len(); j++) {
  71     Node *uin = use-&gt;in(j);
  72     if (uin == oldref) {
  73       if (j &lt; req)
  74         use-&gt;set_req(j, newref);
  75       else
  76         use-&gt;set_prec(j, newref);
  77       nreplacements++;
  78     } else if (j &gt;= req &amp;&amp; uin == NULL) {
  79       break;
  80     }
  81   }
  82   return nreplacements;
  83 }
  84 
<span class="line-added">  85 void PhaseMacroExpand::migrate_outs(Node *old, Node *target) {</span>
<span class="line-added">  86   assert(old != NULL, &quot;sanity&quot;);</span>
<span class="line-added">  87   for (DUIterator_Fast imax, i = old-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-added">  88     Node* use = old-&gt;fast_out(i);</span>
<span class="line-added">  89     _igvn.rehash_node_delayed(use);</span>
<span class="line-added">  90     imax -= replace_input(use, old, target);</span>
<span class="line-added">  91     // back up iterator</span>
<span class="line-added">  92     --i;</span>
<span class="line-added">  93   }</span>
<span class="line-added">  94   assert(old-&gt;outcnt() == 0, &quot;all uses must be deleted&quot;);</span>
<span class="line-added">  95 }</span>
<span class="line-added">  96 </span>
  97 void PhaseMacroExpand::copy_call_debug_info(CallNode *oldcall, CallNode * newcall) {
  98   // Copy debug information and adjust JVMState information
  99   uint old_dbg_start = oldcall-&gt;tf()-&gt;domain()-&gt;cnt();
 100   uint new_dbg_start = newcall-&gt;tf()-&gt;domain()-&gt;cnt();
 101   int jvms_adj  = new_dbg_start - old_dbg_start;
 102   assert (new_dbg_start == newcall-&gt;req(), &quot;argument count mismatch&quot;);
 103 
 104   // SafePointScalarObject node could be referenced several times in debug info.
 105   // Use Dict to record cloned nodes.
 106   Dict* sosn_map = new Dict(cmpkey,hashkey);
 107   for (uint i = old_dbg_start; i &lt; oldcall-&gt;req(); i++) {
 108     Node* old_in = oldcall-&gt;in(i);
 109     // Clone old SafePointScalarObjectNodes, adjusting their field contents.
 110     if (old_in != NULL &amp;&amp; old_in-&gt;is_SafePointScalarObject()) {
 111       SafePointScalarObjectNode* old_sosn = old_in-&gt;as_SafePointScalarObject();
 112       uint old_unique = C-&gt;unique();
 113       Node* new_in = old_sosn-&gt;clone(sosn_map);
 114       if (old_unique != C-&gt;unique()) { // New node?
 115         new_in-&gt;set_req(0, C-&gt;root()); // reset control edge
 116         new_in = transform_later(new_in); // Register new node.
</pre>
<hr />
<pre>
 346       }
 347       mem = mem-&gt;in(MemNode::Memory);
 348     } else {
 349       return mem;
 350     }
 351     assert(mem != orig_mem, &quot;dead memory loop&quot;);
 352   }
 353 }
 354 
 355 // Generate loads from source of the arraycopy for fields of
 356 // destination needed at a deoptimization point
 357 Node* PhaseMacroExpand::make_arraycopy_load(ArrayCopyNode* ac, intptr_t offset, Node* ctl, Node* mem, BasicType ft, const Type *ftype, AllocateNode *alloc) {
 358   BasicType bt = ft;
 359   const Type *type = ftype;
 360   if (ft == T_NARROWOOP) {
 361     bt = T_OBJECT;
 362     type = ftype-&gt;make_oopptr();
 363   }
 364   Node* res = NULL;
 365   if (ac-&gt;is_clonebasic()) {
<span class="line-added"> 366     assert(ac-&gt;in(ArrayCopyNode::Src) != ac-&gt;in(ArrayCopyNode::Dest), &quot;clone source equals destination&quot;);</span>
 367     Node* base = ac-&gt;in(ArrayCopyNode::Src)-&gt;in(AddPNode::Base);
 368     Node* adr = _igvn.transform(new AddPNode(base, base, MakeConX(offset)));
 369     const TypePtr* adr_type = _igvn.type(base)-&gt;is_ptr()-&gt;add_offset(offset);
<span class="line-modified"> 370     res = LoadNode::make(_igvn, ctl, mem, adr, adr_type, type, bt, MemNode::unordered, LoadNode::UnknownControl);</span>
 371   } else {
 372     if (ac-&gt;modifies(offset, offset, &amp;_igvn, true)) {
 373       assert(ac-&gt;in(ArrayCopyNode::Dest) == alloc-&gt;result_cast(), &quot;arraycopy destination should be allocation&#39;s result&quot;);
<span class="line-modified"> 374       uint shift = exact_log2(type2aelembytes(bt));</span>
<span class="line-modified"> 375       Node* src_pos = ac-&gt;in(ArrayCopyNode::SrcPos);</span>
<span class="line-added"> 376       Node* dest_pos = ac-&gt;in(ArrayCopyNode::DestPos);</span>
<span class="line-added"> 377       const TypeInt* src_pos_t = _igvn.type(src_pos)-&gt;is_int();</span>
<span class="line-added"> 378       const TypeInt* dest_pos_t = _igvn.type(dest_pos)-&gt;is_int();</span>
<span class="line-added"> 379 </span>
<span class="line-added"> 380       Node* adr = NULL;</span>
<span class="line-added"> 381       const TypePtr* adr_type = NULL;</span>
<span class="line-added"> 382       if (src_pos_t-&gt;is_con() &amp;&amp; dest_pos_t-&gt;is_con()) {</span>
<span class="line-added"> 383         intptr_t off = ((src_pos_t-&gt;get_con() - dest_pos_t-&gt;get_con()) &lt;&lt; shift) + offset;</span>
<span class="line-added"> 384         Node* base = ac-&gt;in(ArrayCopyNode::Src);</span>
<span class="line-added"> 385         adr = _igvn.transform(new AddPNode(base, base, MakeConX(off)));</span>
<span class="line-added"> 386         adr_type = _igvn.type(base)-&gt;is_ptr()-&gt;add_offset(off);</span>
<span class="line-added"> 387         if (ac-&gt;in(ArrayCopyNode::Src) == ac-&gt;in(ArrayCopyNode::Dest)) {</span>
<span class="line-added"> 388           // Don&#39;t emit a new load from src if src == dst but try to get the value from memory instead</span>
<span class="line-added"> 389           return value_from_mem(ac-&gt;in(TypeFunc::Memory), ctl, ft, ftype, adr_type-&gt;isa_oopptr(), alloc);</span>
<span class="line-added"> 390         }</span>
<span class="line-added"> 391       } else {</span>
<span class="line-added"> 392         Node* diff = _igvn.transform(new SubINode(ac-&gt;in(ArrayCopyNode::SrcPos), ac-&gt;in(ArrayCopyNode::DestPos)));</span>
 393 #ifdef _LP64
<span class="line-modified"> 394         diff = _igvn.transform(new ConvI2LNode(diff));</span>
 395 #endif
<span class="line-modified"> 396         diff = _igvn.transform(new LShiftXNode(diff, intcon(shift)));</span>
<span class="line-modified"> 397 </span>
<span class="line-modified"> 398         Node* off = _igvn.transform(new AddXNode(MakeConX(offset), diff));</span>
<span class="line-modified"> 399         Node* base = ac-&gt;in(ArrayCopyNode::Src);</span>
<span class="line-modified"> 400         adr = _igvn.transform(new AddPNode(base, base, off));</span>
<span class="line-modified"> 401         adr_type = _igvn.type(base)-&gt;is_ptr()-&gt;add_offset(Type::OffsetBot);</span>
<span class="line-modified"> 402         if (ac-&gt;in(ArrayCopyNode::Src) == ac-&gt;in(ArrayCopyNode::Dest)) {</span>
<span class="line-added"> 403           // Non constant offset in the array: we can&#39;t statically</span>
<span class="line-added"> 404           // determine the value</span>
<span class="line-added"> 405           return NULL;</span>
<span class="line-added"> 406         }</span>
<span class="line-added"> 407       }</span>
<span class="line-added"> 408       res = LoadNode::make(_igvn, ctl, mem, adr, adr_type, type, bt, MemNode::unordered, LoadNode::UnknownControl);</span>
 409     }
 410   }
 411   if (res != NULL) {
 412     res = _igvn.transform(res);
 413     if (ftype-&gt;isa_narrowoop()) {
 414       // PhaseMacroExpand::scalar_replacement adds DecodeN nodes
 415       res = _igvn.transform(new EncodePNode(res, ftype));
 416     }
 417     return res;
 418   }
 419   return NULL;
 420 }
 421 
 422 //
 423 // Given a Memory Phi, compute a value Phi containing the values from stores
 424 // on the input paths.
 425 // Note: this function is recursive, its depth is limited by the &quot;level&quot; argument
 426 // Returns the computed Phi, or NULL if it cannot compute it.
 427 Node *PhaseMacroExpand::value_from_mem_phi(Node *mem, BasicType ft, const Type *phi_type, const TypeOopPtr *adr_t, AllocateNode *alloc, Node_Stack *value_phis, int level) {
 428   assert(mem-&gt;is_Phi(), &quot;sanity&quot;);
</pre>
<hr />
<pre>
 519       phi-&gt;init_req(j, values.at(j));
 520     }
 521   }
 522   return phi;
 523 }
 524 
 525 // Search the last value stored into the object&#39;s field.
 526 Node *PhaseMacroExpand::value_from_mem(Node *sfpt_mem, Node *sfpt_ctl, BasicType ft, const Type *ftype, const TypeOopPtr *adr_t, AllocateNode *alloc) {
 527   assert(adr_t-&gt;is_known_instance_field(), &quot;instance required&quot;);
 528   int instance_id = adr_t-&gt;instance_id();
 529   assert((uint)instance_id == alloc-&gt;_idx, &quot;wrong allocation&quot;);
 530 
 531   int alias_idx = C-&gt;get_alias_index(adr_t);
 532   int offset = adr_t-&gt;offset();
 533   Node *start_mem = C-&gt;start()-&gt;proj_out_or_null(TypeFunc::Memory);
 534   Node *alloc_ctrl = alloc-&gt;in(TypeFunc::Control);
 535   Node *alloc_mem = alloc-&gt;in(TypeFunc::Memory);
 536   Arena *a = Thread::current()-&gt;resource_area();
 537   VectorSet visited(a);
 538 

 539   bool done = sfpt_mem == alloc_mem;
 540   Node *mem = sfpt_mem;
 541   while (!done) {
 542     if (visited.test_set(mem-&gt;_idx)) {
 543       return NULL;  // found a loop, give up
 544     }
 545     mem = scan_mem_chain(mem, alias_idx, offset, start_mem, alloc, &amp;_igvn);
 546     if (mem == start_mem || mem == alloc_mem) {
 547       done = true;  // hit a sentinel, return appropriate 0 value
 548     } else if (mem-&gt;is_Initialize()) {
 549       mem = mem-&gt;as_Initialize()-&gt;find_captured_store(offset, type2aelembytes(ft), &amp;_igvn);
 550       if (mem == NULL) {
 551         done = true; // Something go wrong.
 552       } else if (mem-&gt;is_Store()) {
 553         const TypePtr* atype = mem-&gt;as_Store()-&gt;adr_type();
 554         assert(C-&gt;get_alias_index(atype) == Compile::AliasIdxRaw, &quot;store is correct memory slice&quot;);
 555         done = true;
 556       }
 557     } else if (mem-&gt;is_Store()) {
 558       const TypeOopPtr* atype = mem-&gt;as_Store()-&gt;adr_type()-&gt;isa_oopptr();
</pre>
<hr />
<pre>
 808 #endif
 809                                                  first_ind, nfields);
 810     sobj-&gt;init_req(0, C-&gt;root());
 811     transform_later(sobj);
 812 
 813     // Scan object&#39;s fields adding an input to the safepoint for each field.
 814     for (int j = 0; j &lt; nfields; j++) {
 815       intptr_t offset;
 816       ciField* field = NULL;
 817       if (iklass != NULL) {
 818         field = iklass-&gt;nonstatic_field_at(j);
 819         offset = field-&gt;offset();
 820         elem_type = field-&gt;type();
 821         basic_elem_type = field-&gt;layout_type();
 822       } else {
 823         offset = array_base + j * (intptr_t)element_size;
 824       }
 825 
 826       const Type *field_type;
 827       // The next code is taken from Parse::do_get_xxx().
<span class="line-modified"> 828       if (is_reference_type(basic_elem_type)) {</span>
 829         if (!elem_type-&gt;is_loaded()) {
 830           field_type = TypeInstPtr::BOTTOM;
 831         } else if (field != NULL &amp;&amp; field-&gt;is_static_constant()) {
 832           // This can happen if the constant oop is non-perm.
 833           ciObject* con = field-&gt;constant_value().as_object();
 834           // Do not &quot;join&quot; in the previous type; it doesn&#39;t add value,
 835           // and may yield a vacuous result if the field is of interface type.
 836           field_type = TypeOopPtr::make_from_constant(con)-&gt;isa_oopptr();
 837           assert(field_type != NULL, &quot;field singleton type must be consistent&quot;);
 838         } else {
 839           field_type = TypeOopPtr::make_from_klass(elem_type-&gt;as_klass());
 840         }
 841         if (UseCompressedOops) {
 842           field_type = field_type-&gt;make_narrowoop();
 843           basic_elem_type = T_NARROWOOP;
 844         }
 845       } else {
 846         field_type = Type::get_const_basic_type(basic_elem_type);
 847       }
 848 
</pre>
<hr />
<pre>
1029     // First disconnect stores captured by Initialize node.
1030     // If Initialize node is eliminated first in the following code,
1031     // it will kill such stores and DUIterator_Last will assert.
1032     for (DUIterator_Fast jmax, j = _resproj-&gt;fast_outs(jmax);  j &lt; jmax; j++) {
1033       Node *use = _resproj-&gt;fast_out(j);
1034       if (use-&gt;is_AddP()) {
1035         // raw memory addresses used only by the initialization
1036         _igvn.replace_node(use, C-&gt;top());
1037         --j; --jmax;
1038       }
1039     }
1040     for (DUIterator_Last jmin, j = _resproj-&gt;last_outs(jmin); j &gt;= jmin; ) {
1041       Node *use = _resproj-&gt;last_out(j);
1042       uint oc1 = _resproj-&gt;outcnt();
1043       if (use-&gt;is_Initialize()) {
1044         // Eliminate Initialize node.
1045         InitializeNode *init = use-&gt;as_Initialize();
1046         assert(init-&gt;outcnt() &lt;= 2, &quot;only a control and memory projection expected&quot;);
1047         Node *ctrl_proj = init-&gt;proj_out_or_null(TypeFunc::Control);
1048         if (ctrl_proj != NULL) {
<span class="line-modified">1049           _igvn.replace_node(ctrl_proj, init-&gt;in(TypeFunc::Control));</span>
<span class="line-modified">1050 #ifdef ASSERT</span>
<span class="line-added">1051           Node* tmp = init-&gt;in(TypeFunc::Control);</span>
<span class="line-added">1052           assert(tmp == _fallthroughcatchproj, &quot;allocation control projection&quot;);</span>
<span class="line-added">1053 #endif</span>
1054         }
1055         Node *mem_proj = init-&gt;proj_out_or_null(TypeFunc::Memory);
1056         if (mem_proj != NULL) {
1057           Node *mem = init-&gt;in(TypeFunc::Memory);
1058 #ifdef ASSERT
1059           if (mem-&gt;is_MergeMem()) {
1060             assert(mem-&gt;in(TypeFunc::Memory) == _memproj_fallthrough, &quot;allocation memory projection&quot;);
1061           } else {
1062             assert(mem == _memproj_fallthrough, &quot;allocation memory projection&quot;);
1063           }
1064 #endif
1065           _igvn.replace_node(mem_proj, mem);
1066         }
1067       } else  {
1068         assert(false, &quot;only Initialize or AddP expected&quot;);
1069       }
1070       j -= (oc1 - _resproj-&gt;outcnt());
1071     }
1072   }
1073   if (_fallthroughcatchproj != NULL) {
</pre>
<hr />
<pre>
1275 // Allocations bigger than this always go the slow route.
1276 // This value must be small enough that allocation attempts that need to
1277 // trigger exceptions go the slow route.  Also, it must be small enough so
1278 // that heap_top + size_in_bytes does not wrap around the 4Gig limit.
1279 //=============================================================================j//
1280 // %%% Here is an old comment from parseHelper.cpp; is it outdated?
1281 // The allocator will coalesce int-&gt;oop copies away.  See comment in
1282 // coalesce.cpp about how this works.  It depends critically on the exact
1283 // code shape produced here, so if you are changing this code shape
1284 // make sure the GC info for the heap-top is correct in and around the
1285 // slow-path call.
1286 //
1287 
1288 void PhaseMacroExpand::expand_allocate_common(
1289             AllocateNode* alloc, // allocation node to be expanded
1290             Node* length,  // array length for an array allocation
1291             const TypeFunc* slow_call_type, // Type of slow call
1292             address slow_call_address  // Address of slow call
1293     )
1294 {

1295   Node* ctrl = alloc-&gt;in(TypeFunc::Control);
1296   Node* mem  = alloc-&gt;in(TypeFunc::Memory);
1297   Node* i_o  = alloc-&gt;in(TypeFunc::I_O);
1298   Node* size_in_bytes     = alloc-&gt;in(AllocateNode::AllocSize);
1299   Node* klass_node        = alloc-&gt;in(AllocateNode::KlassNode);
1300   Node* initial_slow_test = alloc-&gt;in(AllocateNode::InitialTest);

1301   assert(ctrl != NULL, &quot;must have control&quot;);
<span class="line-added">1302 </span>
1303   // We need a Region and corresponding Phi&#39;s to merge the slow-path and fast-path results.
1304   // they will not be used if &quot;always_slow&quot; is set
1305   enum { slow_result_path = 1, fast_result_path = 2 };
1306   Node *result_region = NULL;
1307   Node *result_phi_rawmem = NULL;
1308   Node *result_phi_rawoop = NULL;
1309   Node *result_phi_i_o = NULL;
1310 
1311   // The initial slow comparison is a size check, the comparison
1312   // we want to do is a BoolTest::gt
<span class="line-modified">1313   bool expand_fast_path = true;</span>
1314   int tv = _igvn.find_int_con(initial_slow_test, -1);
1315   if (tv &gt;= 0) {
<span class="line-modified">1316     // InitialTest has constant result</span>
<span class="line-added">1317     //   0 - can fit in TLAB</span>
<span class="line-added">1318     //   1 - always too big or negative</span>
<span class="line-added">1319     assert(tv &lt;= 1, &quot;0 or 1 if a constant&quot;);</span>
<span class="line-added">1320     expand_fast_path = (tv == 0);</span>
1321     initial_slow_test = NULL;
1322   } else {
1323     initial_slow_test = BoolNode::make_predicate(initial_slow_test, &amp;_igvn);
1324   }
1325 
1326   if (C-&gt;env()-&gt;dtrace_alloc_probes() ||
1327       (!UseTLAB &amp;&amp; !Universe::heap()-&gt;supports_inline_contig_alloc())) {
1328     // Force slow-path allocation
<span class="line-modified">1329     expand_fast_path = false;</span>
1330     initial_slow_test = NULL;
1331   }
1332 
<span class="line-added">1333   bool allocation_has_use = (alloc-&gt;result_cast() != NULL);</span>
<span class="line-added">1334   if (!allocation_has_use) {</span>
<span class="line-added">1335     InitializeNode* init = alloc-&gt;initialization();</span>
<span class="line-added">1336     if (init != NULL) {</span>
<span class="line-added">1337       yank_initalize_node(init);</span>
<span class="line-added">1338       assert(init-&gt;outcnt() == 0, &quot;all uses must be deleted&quot;);</span>
<span class="line-added">1339       _igvn.remove_dead_node(init);</span>
<span class="line-added">1340     }</span>
<span class="line-added">1341     if (expand_fast_path &amp;&amp; (initial_slow_test == NULL)) {</span>
<span class="line-added">1342       // Remove allocation node and return.</span>
<span class="line-added">1343       // Size is a non-negative constant -&gt; no initial check needed -&gt; directly to fast path.</span>
<span class="line-added">1344       // Also, no usages -&gt; empty fast path -&gt; no fall out to slow path -&gt; nothing left.</span>
<span class="line-added">1345       yank_alloc_node(alloc);</span>
<span class="line-added">1346       return;</span>
<span class="line-added">1347     }</span>
<span class="line-added">1348   }</span>
1349 
1350   enum { too_big_or_final_path = 1, need_gc_path = 2 };
1351   Node *slow_region = NULL;
1352   Node *toobig_false = ctrl;
1353 

1354   // generate the initial test if necessary
1355   if (initial_slow_test != NULL ) {
<span class="line-added">1356     assert (expand_fast_path, &quot;Only need test if there is a fast path&quot;);</span>
1357     slow_region = new RegionNode(3);
1358 
1359     // Now make the initial failure test.  Usually a too-big test but
1360     // might be a TRUE for finalizers or a fancy class check for
1361     // newInstance0.
1362     IfNode *toobig_iff = new IfNode(ctrl, initial_slow_test, PROB_MIN, COUNT_UNKNOWN);
1363     transform_later(toobig_iff);
1364     // Plug the failing-too-big test into the slow-path region
1365     Node *toobig_true = new IfTrueNode( toobig_iff );
1366     transform_later(toobig_true);
1367     slow_region    -&gt;init_req( too_big_or_final_path, toobig_true );
1368     toobig_false = new IfFalseNode( toobig_iff );
1369     transform_later(toobig_false);
<span class="line-modified">1370   } else {</span>
<span class="line-added">1371     // No initial test, just fall into next case</span>
<span class="line-added">1372     assert(allocation_has_use || !expand_fast_path, &quot;Should already have been handled&quot;);</span>
1373     toobig_false = ctrl;
1374     debug_only(slow_region = NodeSentinel);
1375   }
1376 
<span class="line-added">1377   // If we are here there are several possibilities</span>
<span class="line-added">1378   // - expand_fast_path is false - then only a slow path is expanded. That&#39;s it.</span>
<span class="line-added">1379   // no_initial_check means a constant allocation.</span>
<span class="line-added">1380   // - If check always evaluates to false -&gt; expand_fast_path is false (see above)</span>
<span class="line-added">1381   // - If check always evaluates to true -&gt; directly into fast path (but may bailout to slowpath)</span>
<span class="line-added">1382   // if !allocation_has_use the fast path is empty</span>
<span class="line-added">1383   // if !allocation_has_use &amp;&amp; no_initial_check</span>
<span class="line-added">1384   // - Then there are no fastpath that can fall out to slowpath -&gt; no allocation code at all.</span>
<span class="line-added">1385   //   removed by yank_alloc_node above.</span>
<span class="line-added">1386 </span>
1387   Node *slow_mem = mem;  // save the current memory state for slow path
1388   // generate the fast allocation code unless we know that the initial test will always go slow
<span class="line-modified">1389   if (expand_fast_path) {</span>
1390     // Fast path modifies only raw memory.
1391     if (mem-&gt;is_MergeMem()) {
1392       mem = mem-&gt;as_MergeMem()-&gt;memory_at(Compile::AliasIdxRaw);
1393     }
1394 
1395     // allocate the Region and Phi nodes for the result
1396     result_region = new RegionNode(3);
1397     result_phi_rawmem = new PhiNode(result_region, Type::MEMORY, TypeRawPtr::BOTTOM);

1398     result_phi_i_o    = new PhiNode(result_region, Type::ABIO); // I/O is used for Prefetch
1399 
1400     // Grab regular I/O before optional prefetch may change it.
1401     // Slow-path does no I/O so just set it to the original I/O.
1402     result_phi_i_o-&gt;init_req(slow_result_path, i_o);
1403 

1404     // Name successful fast-path variables
1405     Node* fast_oop_ctrl;
1406     Node* fast_oop_rawmem;
<span class="line-added">1407     if (allocation_has_use) {</span>
<span class="line-added">1408       Node* needgc_ctrl = NULL;</span>
<span class="line-added">1409       result_phi_rawoop = new PhiNode(result_region, TypeRawPtr::BOTTOM);</span>
1410 
<span class="line-modified">1411       intx prefetch_lines = length != NULL ? AllocatePrefetchLines : AllocateInstancePrefetchLines;</span>
<span class="line-modified">1412       BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();</span>
<span class="line-modified">1413       Node* fast_oop = bs-&gt;obj_allocate(this, ctrl, mem, toobig_false, size_in_bytes, i_o, needgc_ctrl,</span>
<span class="line-modified">1414                                         fast_oop_ctrl, fast_oop_rawmem,</span>
<span class="line-modified">1415                                         prefetch_lines);</span>
<span class="line-modified">1416 </span>
<span class="line-modified">1417       if (initial_slow_test != NULL) {</span>
<span class="line-modified">1418         // This completes all paths into the slow merge point</span>
<span class="line-modified">1419         slow_region-&gt;init_req(need_gc_path, needgc_ctrl);</span>
<span class="line-modified">1420         transform_later(slow_region);</span>









































1421       } else {
<span class="line-modified">1422         // No initial slow path needed!</span>
<span class="line-modified">1423         // Just fall from the need-GC path straight into the VM call.</span>
<span class="line-modified">1424         slow_region = needgc_ctrl;</span>
































1425       }









1426 
<span class="line-modified">1427       InitializeNode* init = alloc-&gt;initialization();</span>
<span class="line-modified">1428       fast_oop_rawmem = initialize_object(alloc,</span>
<span class="line-modified">1429                                           fast_oop_ctrl, fast_oop_rawmem, fast_oop,</span>
<span class="line-added">1430                                           klass_node, length, size_in_bytes);</span>
<span class="line-added">1431       expand_initialize_membar(alloc, init, fast_oop_ctrl, fast_oop_rawmem);</span>
<span class="line-added">1432       expand_dtrace_alloc_probe(alloc, fast_oop, fast_oop_ctrl, fast_oop_rawmem);</span>
1433 
<span class="line-modified">1434       result_phi_rawoop-&gt;init_req(fast_result_path, fast_oop);</span>
<span class="line-modified">1435     } else {</span>
<span class="line-modified">1436       assert (initial_slow_test != NULL, &quot;sanity&quot;);</span>
<span class="line-modified">1437       fast_oop_ctrl   = toobig_false;</span>
<span class="line-modified">1438       fast_oop_rawmem = mem;</span>
<span class="line-modified">1439       transform_later(slow_region);</span>






1440     }
1441 
1442     // Plug in the successful fast-path into the result merge point
1443     result_region    -&gt;init_req(fast_result_path, fast_oop_ctrl);

1444     result_phi_i_o   -&gt;init_req(fast_result_path, i_o);
1445     result_phi_rawmem-&gt;init_req(fast_result_path, fast_oop_rawmem);
1446   } else {
1447     slow_region = ctrl;
1448     result_phi_i_o = i_o; // Rename it to use in the following code.
1449   }
1450 
1451   // Generate slow-path call
1452   CallNode *call = new CallStaticJavaNode(slow_call_type, slow_call_address,
1453                                OptoRuntime::stub_name(slow_call_address),
1454                                alloc-&gt;jvms()-&gt;bci(),
1455                                TypePtr::BOTTOM);
<span class="line-modified">1456   call-&gt;init_req(TypeFunc::Control,   slow_region);</span>
<span class="line-modified">1457   call-&gt;init_req(TypeFunc::I_O,       top());    // does no i/o</span>
<span class="line-modified">1458   call-&gt;init_req(TypeFunc::Memory,    slow_mem); // may gc ptrs</span>
<span class="line-modified">1459   call-&gt;init_req(TypeFunc::ReturnAdr, alloc-&gt;in(TypeFunc::ReturnAdr));</span>
<span class="line-modified">1460   call-&gt;init_req(TypeFunc::FramePtr,  alloc-&gt;in(TypeFunc::FramePtr));</span>
1461 
1462   call-&gt;init_req(TypeFunc::Parms+0, klass_node);
1463   if (length != NULL) {
1464     call-&gt;init_req(TypeFunc::Parms+1, length);
1465   }
1466 
1467   // Copy debug information and adjust JVMState information, then replace
1468   // allocate node with the call
1469   copy_call_debug_info((CallNode *) alloc,  call);
<span class="line-modified">1470   if (expand_fast_path) {</span>
1471     call-&gt;set_cnt(PROB_UNLIKELY_MAG(4));  // Same effect as RC_UNCOMMON.
1472   } else {
1473     // Hook i_o projection to avoid its elimination during allocation
1474     // replacement (when only a slow call is generated).
1475     call-&gt;set_req(TypeFunc::I_O, result_phi_i_o);
1476   }
1477   _igvn.replace_node(alloc, call);
1478   transform_later(call);
1479 
1480   // Identify the output projections from the allocate node and
1481   // adjust any references to them.
1482   // The control and io projections look like:
1483   //
1484   //        v---Proj(ctrl) &lt;-----+   v---CatchProj(ctrl)
1485   //  Allocate                   Catch
1486   //        ^---Proj(io) &lt;-------+   ^---CatchProj(io)
1487   //
1488   //  We are interested in the CatchProj nodes.
1489   //
1490   extract_call_projections(call);
1491 
1492   // An allocate node has separate memory projections for the uses on
1493   // the control and i_o paths. Replace the control memory projection with
1494   // result_phi_rawmem (unless we are only generating a slow call when
1495   // both memory projections are combined)
<span class="line-modified">1496   if (expand_fast_path &amp;&amp; _memproj_fallthrough != NULL) {</span>
<span class="line-modified">1497     migrate_outs(_memproj_fallthrough, result_phi_rawmem);</span>






1498   }
1499   // Now change uses of _memproj_catchall to use _memproj_fallthrough and delete
1500   // _memproj_catchall so we end up with a call that has only 1 memory projection.
1501   if (_memproj_catchall != NULL ) {
1502     if (_memproj_fallthrough == NULL) {
1503       _memproj_fallthrough = new ProjNode(call, TypeFunc::Memory);
1504       transform_later(_memproj_fallthrough);
1505     }
<span class="line-modified">1506     migrate_outs(_memproj_catchall, _memproj_fallthrough);</span>







1507     _igvn.remove_dead_node(_memproj_catchall);
1508   }
1509 
1510   // An allocate node has separate i_o projections for the uses on the control
1511   // and i_o paths. Always replace the control i_o projection with result i_o
1512   // otherwise incoming i_o become dead when only a slow call is generated
1513   // (it is different from memory projections where both projections are
1514   // combined in such case).
1515   if (_ioproj_fallthrough != NULL) {
<span class="line-modified">1516     migrate_outs(_ioproj_fallthrough, result_phi_i_o);</span>






1517   }
1518   // Now change uses of _ioproj_catchall to use _ioproj_fallthrough and delete
1519   // _ioproj_catchall so we end up with a call that has only 1 i_o projection.
1520   if (_ioproj_catchall != NULL ) {
1521     if (_ioproj_fallthrough == NULL) {
1522       _ioproj_fallthrough = new ProjNode(call, TypeFunc::I_O);
1523       transform_later(_ioproj_fallthrough);
1524     }
<span class="line-modified">1525     migrate_outs(_ioproj_catchall, _ioproj_fallthrough);</span>







1526     _igvn.remove_dead_node(_ioproj_catchall);
1527   }
1528 
1529   // if we generated only a slow call, we are done
<span class="line-modified">1530   if (!expand_fast_path) {</span>
1531     // Now we can unhook i_o.
1532     if (result_phi_i_o-&gt;outcnt() &gt; 1) {
1533       call-&gt;set_req(TypeFunc::I_O, top());
1534     } else {
<span class="line-modified">1535       assert(result_phi_i_o-&gt;unique_ctrl_out() == call, &quot;sanity&quot;);</span>
1536       // Case of new array with negative size known during compilation.
1537       // AllocateArrayNode::Ideal() optimization disconnect unreachable
1538       // following code since call to runtime will throw exception.
1539       // As result there will be no users of i_o after the call.
1540       // Leave i_o attached to this call to avoid problems in preceding graph.
1541     }
1542     return;
1543   }
1544 

1545   if (_fallthroughcatchproj != NULL) {
1546     ctrl = _fallthroughcatchproj-&gt;clone();
1547     transform_later(ctrl);
1548     _igvn.replace_node(_fallthroughcatchproj, result_region);
1549   } else {
1550     ctrl = top();
1551   }
1552   Node *slow_result;
1553   if (_resproj == NULL) {
1554     // no uses of the allocation result
1555     slow_result = top();
1556   } else {
1557     slow_result = _resproj-&gt;clone();
1558     transform_later(slow_result);
1559     _igvn.replace_node(_resproj, result_phi_rawoop);
1560   }
1561 
1562   // Plug slow-path into result merge point
<span class="line-modified">1563   result_region-&gt;init_req( slow_result_path, ctrl);</span>


1564   transform_later(result_region);
<span class="line-modified">1565   if (allocation_has_use) {</span>
<span class="line-added">1566     result_phi_rawoop-&gt;init_req(slow_result_path, slow_result);</span>
<span class="line-added">1567     transform_later(result_phi_rawoop);</span>
<span class="line-added">1568   }</span>
<span class="line-added">1569   result_phi_rawmem-&gt;init_req(slow_result_path, _memproj_fallthrough);</span>
1570   transform_later(result_phi_rawmem);
1571   transform_later(result_phi_i_o);
1572   // This completes all paths into the result merge point
1573 }
1574 
<span class="line-added">1575 // Remove alloc node that has no uses.</span>
<span class="line-added">1576 void PhaseMacroExpand::yank_alloc_node(AllocateNode* alloc) {</span>
<span class="line-added">1577   Node* ctrl = alloc-&gt;in(TypeFunc::Control);</span>
<span class="line-added">1578   Node* mem  = alloc-&gt;in(TypeFunc::Memory);</span>
<span class="line-added">1579   Node* i_o  = alloc-&gt;in(TypeFunc::I_O);</span>
<span class="line-added">1580 </span>
<span class="line-added">1581   extract_call_projections(alloc);</span>
<span class="line-added">1582   if (_fallthroughcatchproj != NULL) {</span>
<span class="line-added">1583     migrate_outs(_fallthroughcatchproj, ctrl);</span>
<span class="line-added">1584     _igvn.remove_dead_node(_fallthroughcatchproj);</span>
<span class="line-added">1585   }</span>
<span class="line-added">1586   if (_catchallcatchproj != NULL) {</span>
<span class="line-added">1587     _igvn.rehash_node_delayed(_catchallcatchproj);</span>
<span class="line-added">1588     _catchallcatchproj-&gt;set_req(0, top());</span>
<span class="line-added">1589   }</span>
<span class="line-added">1590   if (_fallthroughproj != NULL) {</span>
<span class="line-added">1591     Node* catchnode = _fallthroughproj-&gt;unique_ctrl_out();</span>
<span class="line-added">1592     _igvn.remove_dead_node(catchnode);</span>
<span class="line-added">1593     _igvn.remove_dead_node(_fallthroughproj);</span>
<span class="line-added">1594   }</span>
<span class="line-added">1595   if (_memproj_fallthrough != NULL) {</span>
<span class="line-added">1596     migrate_outs(_memproj_fallthrough, mem);</span>
<span class="line-added">1597     _igvn.remove_dead_node(_memproj_fallthrough);</span>
<span class="line-added">1598   }</span>
<span class="line-added">1599   if (_ioproj_fallthrough != NULL) {</span>
<span class="line-added">1600     migrate_outs(_ioproj_fallthrough, i_o);</span>
<span class="line-added">1601     _igvn.remove_dead_node(_ioproj_fallthrough);</span>
<span class="line-added">1602   }</span>
<span class="line-added">1603   if (_memproj_catchall != NULL) {</span>
<span class="line-added">1604     _igvn.rehash_node_delayed(_memproj_catchall);</span>
<span class="line-added">1605     _memproj_catchall-&gt;set_req(0, top());</span>
<span class="line-added">1606   }</span>
<span class="line-added">1607   if (_ioproj_catchall != NULL) {</span>
<span class="line-added">1608     _igvn.rehash_node_delayed(_ioproj_catchall);</span>
<span class="line-added">1609     _ioproj_catchall-&gt;set_req(0, top());</span>
<span class="line-added">1610   }</span>
<span class="line-added">1611   _igvn.remove_dead_node(alloc);</span>
<span class="line-added">1612 }</span>
<span class="line-added">1613 </span>
<span class="line-added">1614 void PhaseMacroExpand::expand_initialize_membar(AllocateNode* alloc, InitializeNode* init,</span>
<span class="line-added">1615                                                 Node*&amp; fast_oop_ctrl, Node*&amp; fast_oop_rawmem) {</span>
<span class="line-added">1616   // If initialization is performed by an array copy, any required</span>
<span class="line-added">1617   // MemBarStoreStore was already added. If the object does not</span>
<span class="line-added">1618   // escape no need for a MemBarStoreStore. If the object does not</span>
<span class="line-added">1619   // escape in its initializer and memory barrier (MemBarStoreStore or</span>
<span class="line-added">1620   // stronger) is already added at exit of initializer, also no need</span>
<span class="line-added">1621   // for a MemBarStoreStore. Otherwise we need a MemBarStoreStore</span>
<span class="line-added">1622   // so that stores that initialize this object can&#39;t be reordered</span>
<span class="line-added">1623   // with a subsequent store that makes this object accessible by</span>
<span class="line-added">1624   // other threads.</span>
<span class="line-added">1625   // Other threads include java threads and JVM internal threads</span>
<span class="line-added">1626   // (for example concurrent GC threads). Current concurrent GC</span>
<span class="line-added">1627   // implementation: G1 will not scan newly created object,</span>
<span class="line-added">1628   // so it&#39;s safe to skip storestore barrier when allocation does</span>
<span class="line-added">1629   // not escape.</span>
<span class="line-added">1630   if (!alloc-&gt;does_not_escape_thread() &amp;&amp;</span>
<span class="line-added">1631     !alloc-&gt;is_allocation_MemBar_redundant() &amp;&amp;</span>
<span class="line-added">1632     (init == NULL || !init-&gt;is_complete_with_arraycopy())) {</span>
<span class="line-added">1633     if (init == NULL || init-&gt;req() &lt; InitializeNode::RawStores) {</span>
<span class="line-added">1634       // No InitializeNode or no stores captured by zeroing</span>
<span class="line-added">1635       // elimination. Simply add the MemBarStoreStore after object</span>
<span class="line-added">1636       // initialization.</span>
<span class="line-added">1637       MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);</span>
<span class="line-added">1638       transform_later(mb);</span>
<span class="line-added">1639 </span>
<span class="line-added">1640       mb-&gt;init_req(TypeFunc::Memory, fast_oop_rawmem);</span>
<span class="line-added">1641       mb-&gt;init_req(TypeFunc::Control, fast_oop_ctrl);</span>
<span class="line-added">1642       fast_oop_ctrl = new ProjNode(mb, TypeFunc::Control);</span>
<span class="line-added">1643       transform_later(fast_oop_ctrl);</span>
<span class="line-added">1644       fast_oop_rawmem = new ProjNode(mb, TypeFunc::Memory);</span>
<span class="line-added">1645       transform_later(fast_oop_rawmem);</span>
<span class="line-added">1646     } else {</span>
<span class="line-added">1647       // Add the MemBarStoreStore after the InitializeNode so that</span>
<span class="line-added">1648       // all stores performing the initialization that were moved</span>
<span class="line-added">1649       // before the InitializeNode happen before the storestore</span>
<span class="line-added">1650       // barrier.</span>
<span class="line-added">1651 </span>
<span class="line-added">1652       Node* init_ctrl = init-&gt;proj_out_or_null(TypeFunc::Control);</span>
<span class="line-added">1653       Node* init_mem = init-&gt;proj_out_or_null(TypeFunc::Memory);</span>
<span class="line-added">1654 </span>
<span class="line-added">1655       MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);</span>
<span class="line-added">1656       transform_later(mb);</span>
<span class="line-added">1657 </span>
<span class="line-added">1658       Node* ctrl = new ProjNode(init, TypeFunc::Control);</span>
<span class="line-added">1659       transform_later(ctrl);</span>
<span class="line-added">1660       Node* mem = new ProjNode(init, TypeFunc::Memory);</span>
<span class="line-added">1661       transform_later(mem);</span>
<span class="line-added">1662 </span>
<span class="line-added">1663       // The MemBarStoreStore depends on control and memory coming</span>
<span class="line-added">1664       // from the InitializeNode</span>
<span class="line-added">1665       mb-&gt;init_req(TypeFunc::Memory, mem);</span>
<span class="line-added">1666       mb-&gt;init_req(TypeFunc::Control, ctrl);</span>
<span class="line-added">1667 </span>
<span class="line-added">1668       ctrl = new ProjNode(mb, TypeFunc::Control);</span>
<span class="line-added">1669       transform_later(ctrl);</span>
<span class="line-added">1670       mem = new ProjNode(mb, TypeFunc::Memory);</span>
<span class="line-added">1671       transform_later(mem);</span>
<span class="line-added">1672 </span>
<span class="line-added">1673       // All nodes that depended on the InitializeNode for control</span>
<span class="line-added">1674       // and memory must now depend on the MemBarNode that itself</span>
<span class="line-added">1675       // depends on the InitializeNode</span>
<span class="line-added">1676       if (init_ctrl != NULL) {</span>
<span class="line-added">1677         _igvn.replace_node(init_ctrl, ctrl);</span>
<span class="line-added">1678       }</span>
<span class="line-added">1679       if (init_mem != NULL) {</span>
<span class="line-added">1680         _igvn.replace_node(init_mem, mem);</span>
<span class="line-added">1681       }</span>
<span class="line-added">1682     }</span>
<span class="line-added">1683   }</span>
<span class="line-added">1684 }</span>
<span class="line-added">1685 </span>
<span class="line-added">1686 void PhaseMacroExpand::expand_dtrace_alloc_probe(AllocateNode* alloc, Node* oop,</span>
<span class="line-added">1687                                                 Node*&amp; ctrl, Node*&amp; rawmem) {</span>
<span class="line-added">1688   if (C-&gt;env()-&gt;dtrace_extended_probes()) {</span>
<span class="line-added">1689     // Slow-path call</span>
<span class="line-added">1690     int size = TypeFunc::Parms + 2;</span>
<span class="line-added">1691     CallLeafNode *call = new CallLeafNode(OptoRuntime::dtrace_object_alloc_Type(),</span>
<span class="line-added">1692                                           CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc_base),</span>
<span class="line-added">1693                                           &quot;dtrace_object_alloc&quot;,</span>
<span class="line-added">1694                                           TypeRawPtr::BOTTOM);</span>
<span class="line-added">1695 </span>
<span class="line-added">1696     // Get base of thread-local storage area</span>
<span class="line-added">1697     Node* thread = new ThreadLocalNode();</span>
<span class="line-added">1698     transform_later(thread);</span>
<span class="line-added">1699 </span>
<span class="line-added">1700     call-&gt;init_req(TypeFunc::Parms + 0, thread);</span>
<span class="line-added">1701     call-&gt;init_req(TypeFunc::Parms + 1, oop);</span>
<span class="line-added">1702     call-&gt;init_req(TypeFunc::Control, ctrl);</span>
<span class="line-added">1703     call-&gt;init_req(TypeFunc::I_O    , top()); // does no i/o</span>
<span class="line-added">1704     call-&gt;init_req(TypeFunc::Memory , ctrl);</span>
<span class="line-added">1705     call-&gt;init_req(TypeFunc::ReturnAdr, alloc-&gt;in(TypeFunc::ReturnAdr));</span>
<span class="line-added">1706     call-&gt;init_req(TypeFunc::FramePtr, alloc-&gt;in(TypeFunc::FramePtr));</span>
<span class="line-added">1707     transform_later(call);</span>
<span class="line-added">1708     ctrl = new ProjNode(call, TypeFunc::Control);</span>
<span class="line-added">1709     transform_later(ctrl);</span>
<span class="line-added">1710     rawmem = new ProjNode(call, TypeFunc::Memory);</span>
<span class="line-added">1711     transform_later(rawmem);</span>
<span class="line-added">1712   }</span>
<span class="line-added">1713 }</span>
<span class="line-added">1714 </span>
<span class="line-added">1715 // Remove InitializeNode without use</span>
<span class="line-added">1716 void PhaseMacroExpand::yank_initalize_node(InitializeNode* initnode) {</span>
<span class="line-added">1717   assert(initnode-&gt;proj_out_or_null(TypeFunc::Parms) == NULL, &quot;No uses allowed&quot;);</span>
<span class="line-added">1718 </span>
<span class="line-added">1719   Node* ctrl_out  = initnode-&gt;proj_out_or_null(TypeFunc::Control);</span>
<span class="line-added">1720   Node* mem_out   = initnode-&gt;proj_out_or_null(TypeFunc::Memory);</span>
<span class="line-added">1721 </span>
<span class="line-added">1722   // Move all uses of each to</span>
<span class="line-added">1723   if (ctrl_out != NULL ) {</span>
<span class="line-added">1724     migrate_outs(ctrl_out, initnode-&gt;in(TypeFunc::Control));</span>
<span class="line-added">1725     _igvn.remove_dead_node(ctrl_out);</span>
<span class="line-added">1726   }</span>
<span class="line-added">1727 </span>
<span class="line-added">1728   // Move all uses of each to</span>
<span class="line-added">1729   if (mem_out != NULL ) {</span>
<span class="line-added">1730     migrate_outs(mem_out, initnode-&gt;in(TypeFunc::Memory));</span>
<span class="line-added">1731     _igvn.remove_dead_node(mem_out);</span>
<span class="line-added">1732   }</span>
<span class="line-added">1733 }</span>
1734 
1735 // Helper for PhaseMacroExpand::expand_allocate_common.
1736 // Initializes the newly-allocated storage.
1737 Node*
1738 PhaseMacroExpand::initialize_object(AllocateNode* alloc,
1739                                     Node* control, Node* rawmem, Node* object,
1740                                     Node* klass_node, Node* length,
1741                                     Node* size_in_bytes) {
1742   InitializeNode* init = alloc-&gt;initialization();
1743   // Store the klass &amp; mark bits
<span class="line-modified">1744   Node* mark_node = alloc-&gt;make_ideal_mark(&amp;_igvn, object, control, rawmem);</span>
<span class="line-modified">1745   if (!mark_node-&gt;is_Con()) {</span>
<span class="line-modified">1746     transform_later(mark_node);</span>



1747   }
<span class="line-modified">1748   rawmem = make_store(control, rawmem, object, oopDesc::mark_offset_in_bytes(), mark_node, TypeX_X-&gt;basic_type());</span>
1749 
1750   rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);
1751   int header_size = alloc-&gt;minimum_header_size();  // conservatively small
1752 
1753   // Array length
1754   if (length != NULL) {         // Arrays need length field
1755     rawmem = make_store(control, rawmem, object, arrayOopDesc::length_offset_in_bytes(), length, T_INT);
1756     // conservatively small header size:
1757     header_size = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1758     ciKlass* k = _igvn.type(klass_node)-&gt;is_klassptr()-&gt;klass();
1759     if (k-&gt;is_array_klass())    // we know the exact header size in most cases:
1760       header_size = Klass::layout_helper_header_size(k-&gt;layout_helper());
1761   }
1762 
1763   // Clear the object body, if necessary.
1764   if (init == NULL) {
1765     // The init has somehow disappeared; be cautious and clear everything.
1766     //
1767     // This can happen if a node is allocated but an uncommon trap occurs
1768     // immediately.  In this case, the Initialize gets associated with the
</pre>
<hr />
<pre>
2284      *      // Done.
2285      *    } else {
2286      *      slow_path:
2287      *      OptoRuntime::complete_monitor_locking_Java(obj);
2288      *    }
2289      *  }
2290      */
2291 
2292     region  = new RegionNode(5);
2293     // create a Phi for the memory state
2294     mem_phi = new PhiNode( region, Type::MEMORY, TypeRawPtr::BOTTOM);
2295 
2296     Node* fast_lock_region  = new RegionNode(3);
2297     Node* fast_lock_mem_phi = new PhiNode( fast_lock_region, Type::MEMORY, TypeRawPtr::BOTTOM);
2298 
2299     // First, check mark word for the biased lock pattern.
2300     Node* mark_node = make_load(ctrl, mem, obj, oopDesc::mark_offset_in_bytes(), TypeX_X, TypeX_X-&gt;basic_type());
2301 
2302     // Get fast path - mark word has the biased lock pattern.
2303     ctrl = opt_bits_test(ctrl, fast_lock_region, 1, mark_node,
<span class="line-modified">2304                          markWord::biased_lock_mask_in_place,</span>
<span class="line-modified">2305                          markWord::biased_lock_pattern, true);</span>
2306     // fast_lock_region-&gt;in(1) is set to slow path.
2307     fast_lock_mem_phi-&gt;init_req(1, mem);
2308 
2309     // Now check that the lock is biased to the current thread and has
2310     // the same epoch and bias as Klass::_prototype_header.
2311 
2312     // Special-case a fresh allocation to avoid building nodes:
2313     Node* klass_node = AllocateNode::Ideal_klass(obj, &amp;_igvn);
2314     if (klass_node == NULL) {
2315       Node* k_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());
2316       klass_node = transform_later(LoadKlassNode::make(_igvn, NULL, mem, k_adr, _igvn.type(k_adr)-&gt;is_ptr()));
2317 #ifdef _LP64
2318       if (UseCompressedClassPointers &amp;&amp; klass_node-&gt;is_DecodeNKlass()) {
2319         assert(klass_node-&gt;in(1)-&gt;Opcode() == Op_LoadNKlass, &quot;sanity&quot;);
2320         klass_node-&gt;in(1)-&gt;init_req(0, ctrl);
2321       } else
2322 #endif
2323       klass_node-&gt;init_req(0, ctrl);
2324     }
2325     Node *proto_node = make_load(ctrl, mem, klass_node, in_bytes(Klass::prototype_header_offset()), TypeX_X, TypeX_X-&gt;basic_type());
2326 
2327     Node* thread = transform_later(new ThreadLocalNode());
2328     Node* cast_thread = transform_later(new CastP2XNode(ctrl, thread));
2329     Node* o_node = transform_later(new OrXNode(cast_thread, proto_node));
2330     Node* x_node = transform_later(new XorXNode(o_node, mark_node));
2331 
2332     // Get slow path - mark word does NOT match the value.
<span class="line-added">2333     STATIC_ASSERT(markWord::age_mask_in_place &lt;= INT_MAX);</span>
2334     Node* not_biased_ctrl =  opt_bits_test(ctrl, region, 3, x_node,
<span class="line-modified">2335                                       (~(int)markWord::age_mask_in_place), 0);</span>
2336     // region-&gt;in(3) is set to fast path - the object is biased to the current thread.
2337     mem_phi-&gt;init_req(3, mem);
2338 
2339 
2340     // Mark word does NOT match the value (thread | Klass::_prototype_header).
2341 
2342 
2343     // First, check biased pattern.
2344     // Get fast path - _prototype_header has the same biased lock pattern.
2345     ctrl =  opt_bits_test(not_biased_ctrl, fast_lock_region, 2, x_node,
<span class="line-modified">2346                           markWord::biased_lock_mask_in_place, 0, true);</span>
2347 
2348     not_biased_ctrl = fast_lock_region-&gt;in(2); // Slow path
2349     // fast_lock_region-&gt;in(2) - the prototype header is no longer biased
2350     // and we have to revoke the bias on this object.
2351     // We are going to try to reset the mark of this object to the prototype
2352     // value and fall through to the CAS-based locking scheme.
2353     Node* adr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
2354     Node* cas = new StoreXConditionalNode(not_biased_ctrl, mem, adr,
2355                                           proto_node, mark_node);
2356     transform_later(cas);
2357     Node* proj = transform_later(new SCMemProjNode(cas));
2358     fast_lock_mem_phi-&gt;init_req(2, proj);
2359 
2360 
2361     // Second, check epoch bits.
2362     Node* rebiased_region  = new RegionNode(3);
2363     Node* old_phi = new PhiNode( rebiased_region, TypeX_X);
2364     Node* new_phi = new PhiNode( rebiased_region, TypeX_X);
2365 
2366     // Get slow path - mark word does NOT match epoch bits.
2367     Node* epoch_ctrl =  opt_bits_test(ctrl, rebiased_region, 1, x_node,
<span class="line-modified">2368                                       markWord::epoch_mask_in_place, 0);</span>
2369     // The epoch of the current bias is not valid, attempt to rebias the object
2370     // toward the current thread.
2371     rebiased_region-&gt;init_req(2, epoch_ctrl);
2372     old_phi-&gt;init_req(2, mark_node);
2373     new_phi-&gt;init_req(2, o_node);
2374 
2375     // rebiased_region-&gt;in(1) is set to fast path.
2376     // The epoch of the current bias is still valid but we know
2377     // nothing about the owner; it might be set or it might be clear.
<span class="line-modified">2378     Node* cmask   = MakeConX(markWord::biased_lock_mask_in_place |</span>
<span class="line-modified">2379                              markWord::age_mask_in_place |</span>
<span class="line-modified">2380                              markWord::epoch_mask_in_place);</span>
2381     Node* old = transform_later(new AndXNode(mark_node, cmask));
2382     cast_thread = transform_later(new CastP2XNode(ctrl, thread));
2383     Node* new_mark = transform_later(new OrXNode(cast_thread, old));
2384     old_phi-&gt;init_req(1, old);
2385     new_phi-&gt;init_req(1, new_mark);
2386 
2387     transform_later(rebiased_region);
2388     transform_later(old_phi);
2389     transform_later(new_phi);
2390 
2391     // Try to acquire the bias of the object using an atomic operation.
2392     // If this fails we will go in to the runtime to revoke the object&#39;s bias.
2393     cas = new StoreXConditionalNode(rebiased_region, mem, adr, new_phi, old_phi);
2394     transform_later(cas);
2395     proj = transform_later(new SCMemProjNode(cas));
2396 
2397     // Get slow path - Failed to CAS.
2398     not_biased_ctrl = opt_bits_test(rebiased_region, region, 4, cas, 0, 0);
2399     mem_phi-&gt;init_req(4, proj);
2400     // region-&gt;in(4) is set to fast path - the object is rebiased to the current thread.
</pre>
<hr />
<pre>
2475   Node* box = unlock-&gt;box_node();
2476 
2477   assert(!box-&gt;as_BoxLock()-&gt;is_eliminated(), &quot;sanity&quot;);
2478 
2479   // No need for a null check on unlock
2480 
2481   // Make the merge point
2482   Node *region;
2483   Node *mem_phi;
2484 
2485   if (UseOptoBiasInlining) {
2486     // Check for biased locking unlock case, which is a no-op.
2487     // See the full description in MacroAssembler::biased_locking_exit().
2488     region  = new RegionNode(4);
2489     // create a Phi for the memory state
2490     mem_phi = new PhiNode( region, Type::MEMORY, TypeRawPtr::BOTTOM);
2491     mem_phi-&gt;init_req(3, mem);
2492 
2493     Node* mark_node = make_load(ctrl, mem, obj, oopDesc::mark_offset_in_bytes(), TypeX_X, TypeX_X-&gt;basic_type());
2494     ctrl = opt_bits_test(ctrl, region, 3, mark_node,
<span class="line-modified">2495                          markWord::biased_lock_mask_in_place,</span>
<span class="line-modified">2496                          markWord::biased_lock_pattern);</span>
2497   } else {
2498     region  = new RegionNode(3);
2499     // create a Phi for the memory state
2500     mem_phi = new PhiNode( region, Type::MEMORY, TypeRawPtr::BOTTOM);
2501   }
2502 
2503   FastUnlockNode *funlock = new FastUnlockNode( ctrl, obj, box );
2504   funlock = transform_later( funlock )-&gt;as_FastUnlock();
2505   // Optimize test; set region slot 2
2506   Node *slow_path = opt_bits_test(ctrl, region, 2, funlock, 0, 0);
2507   Node *thread = transform_later(new ThreadLocalNode());
2508 
2509   CallNode *call = make_slow_call((CallNode *) unlock, OptoRuntime::complete_monitor_exit_Type(),
2510                                   CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C),
2511                                   &quot;complete_monitor_unlocking_C&quot;, slow_path, obj, box, thread);
2512 
2513   extract_call_projections(call);
2514 
2515   assert ( _ioproj_fallthrough == NULL &amp;&amp; _ioproj_catchall == NULL &amp;&amp;
2516            _memproj_catchall == NULL &amp;&amp; _catchallcatchproj == NULL, &quot;Unexpected projection from Lock&quot;);
</pre>
<hr />
<pre>
2518   // No exceptions for unlocking
2519   // Capture slow path
2520   // disconnect fall-through projection from call and create a new one
2521   // hook up users of fall-through projection to region
2522   Node *slow_ctrl = _fallthroughproj-&gt;clone();
2523   transform_later(slow_ctrl);
2524   _igvn.hash_delete(_fallthroughproj);
2525   _fallthroughproj-&gt;disconnect_inputs(NULL, C);
2526   region-&gt;init_req(1, slow_ctrl);
2527   // region inputs are now complete
2528   transform_later(region);
2529   _igvn.replace_node(_fallthroughproj, region);
2530 
2531   Node *memproj = transform_later(new ProjNode(call, TypeFunc::Memory) );
2532   mem_phi-&gt;init_req(1, memproj );
2533   mem_phi-&gt;init_req(2, mem);
2534   transform_later(mem_phi);
2535   _igvn.replace_node(_memproj_fallthrough, mem_phi);
2536 }
2537 
<span class="line-added">2538 void PhaseMacroExpand::expand_subtypecheck_node(SubTypeCheckNode *check) {</span>
<span class="line-added">2539   assert(check-&gt;in(SubTypeCheckNode::Control) == NULL, &quot;should be pinned&quot;);</span>
<span class="line-added">2540   Node* bol = check-&gt;unique_out();</span>
<span class="line-added">2541   Node* obj_or_subklass = check-&gt;in(SubTypeCheckNode::ObjOrSubKlass);</span>
<span class="line-added">2542   Node* superklass = check-&gt;in(SubTypeCheckNode::SuperKlass);</span>
<span class="line-added">2543   assert(bol-&gt;is_Bool() &amp;&amp; bol-&gt;as_Bool()-&gt;_test._test == BoolTest::ne, &quot;unexpected bool node&quot;);</span>
<span class="line-added">2544 </span>
<span class="line-added">2545   for (DUIterator_Last imin, i = bol-&gt;last_outs(imin); i &gt;= imin; --i) {</span>
<span class="line-added">2546     Node* iff = bol-&gt;last_out(i);</span>
<span class="line-added">2547     assert(iff-&gt;is_If(), &quot;where&#39;s the if?&quot;);</span>
<span class="line-added">2548 </span>
<span class="line-added">2549     if (iff-&gt;in(0)-&gt;is_top()) {</span>
<span class="line-added">2550       _igvn.replace_input_of(iff, 1, C-&gt;top());</span>
<span class="line-added">2551       continue;</span>
<span class="line-added">2552     }</span>
<span class="line-added">2553 </span>
<span class="line-added">2554     Node* iftrue = iff-&gt;as_If()-&gt;proj_out(1);</span>
<span class="line-added">2555     Node* iffalse = iff-&gt;as_If()-&gt;proj_out(0);</span>
<span class="line-added">2556     Node* ctrl = iff-&gt;in(0);</span>
<span class="line-added">2557 </span>
<span class="line-added">2558     Node* subklass = NULL;</span>
<span class="line-added">2559     if (_igvn.type(obj_or_subklass)-&gt;isa_klassptr()) {</span>
<span class="line-added">2560       subklass = obj_or_subklass;</span>
<span class="line-added">2561     } else {</span>
<span class="line-added">2562       Node* k_adr = basic_plus_adr(obj_or_subklass, oopDesc::klass_offset_in_bytes());</span>
<span class="line-added">2563       subklass = _igvn.transform(LoadKlassNode::make(_igvn, NULL, C-&gt;immutable_memory(), k_adr, TypeInstPtr::KLASS));</span>
<span class="line-added">2564     }</span>
<span class="line-added">2565 </span>
<span class="line-added">2566     Node* not_subtype_ctrl = Phase::gen_subtype_check(subklass, superklass, &amp;ctrl, NULL, _igvn);</span>
<span class="line-added">2567 </span>
<span class="line-added">2568     _igvn.replace_input_of(iff, 0, C-&gt;top());</span>
<span class="line-added">2569     _igvn.replace_node(iftrue, not_subtype_ctrl);</span>
<span class="line-added">2570     _igvn.replace_node(iffalse, ctrl);</span>
<span class="line-added">2571   }</span>
<span class="line-added">2572   _igvn.replace_node(check, C-&gt;top());</span>
<span class="line-added">2573 }</span>
<span class="line-added">2574 </span>
2575 //---------------------------eliminate_macro_nodes----------------------
2576 // Eliminate scalar replaced allocations and associated locks.
2577 void PhaseMacroExpand::eliminate_macro_nodes() {
2578   if (C-&gt;macro_count() == 0)
2579     return;
2580 
2581   // First, attempt to eliminate locks
2582   int cnt = C-&gt;macro_count();
2583   for (int i=0; i &lt; cnt; i++) {
2584     Node *n = C-&gt;macro_node(i);
2585     if (n-&gt;is_AbstractLock()) { // Lock and Unlock nodes
2586       // Before elimination mark all associated (same box and obj)
2587       // lock and unlock nodes.
2588       mark_eliminated_locking_nodes(n-&gt;as_AbstractLock());
2589     }
2590   }
2591   bool progress = true;
2592   while (progress) {
2593     progress = false;
2594     for (int i = C-&gt;macro_count(); i &gt; 0; i--) {
</pre>
<hr />
<pre>
2611       Node * n = C-&gt;macro_node(i-1);
2612       bool success = false;
2613       debug_only(int old_macro_count = C-&gt;macro_count(););
2614       switch (n-&gt;class_id()) {
2615       case Node::Class_Allocate:
2616       case Node::Class_AllocateArray:
2617         success = eliminate_allocate_node(n-&gt;as_Allocate());
2618         break;
2619       case Node::Class_CallStaticJava:
2620         success = eliminate_boxing_node(n-&gt;as_CallStaticJava());
2621         break;
2622       case Node::Class_Lock:
2623       case Node::Class_Unlock:
2624         assert(!n-&gt;as_AbstractLock()-&gt;is_eliminated(), &quot;sanity&quot;);
2625         _has_locks = true;
2626         break;
2627       case Node::Class_ArrayCopy:
2628         break;
2629       case Node::Class_OuterStripMinedLoop:
2630         break;
<span class="line-added">2631       case Node::Class_SubTypeCheck:</span>
<span class="line-added">2632         break;</span>
2633       default:
2634         assert(n-&gt;Opcode() == Op_LoopLimit ||
2635                n-&gt;Opcode() == Op_Opaque1   ||
2636                n-&gt;Opcode() == Op_Opaque2   ||
2637                n-&gt;Opcode() == Op_Opaque3   ||
2638                BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;is_gc_barrier_node(n),
2639                &quot;unknown node type in macro list&quot;);
2640       }
2641       assert(success == (C-&gt;macro_count() &lt; old_macro_count), &quot;elimination reduces macro count&quot;);
2642       progress = progress || success;
2643     }
2644   }
2645 }
2646 
2647 //------------------------------expand_macro_nodes----------------------
2648 //  Returns true if a failure occurred.
2649 bool PhaseMacroExpand::expand_macro_nodes() {
2650   // Last attempt to eliminate macro nodes.
2651   eliminate_macro_nodes();
2652 
2653   // Make sure expansion will not cause node limit to be exceeded.
2654   // Worst case is a macro node gets expanded into about 200 nodes.
2655   // Allow 50% more for optimization.
2656   if (C-&gt;check_node_count(C-&gt;macro_count() * 300, &quot;out of nodes before macro expansion&quot; ) )
2657     return true;
2658 
2659   // Eliminate Opaque and LoopLimit nodes. Do it after all loop optimizations.
2660   bool progress = true;
2661   while (progress) {
2662     progress = false;
2663     for (int i = C-&gt;macro_count(); i &gt; 0; i--) {
<span class="line-modified">2664       Node* n = C-&gt;macro_node(i-1);</span>
2665       bool success = false;
2666       debug_only(int old_macro_count = C-&gt;macro_count(););
2667       if (n-&gt;Opcode() == Op_LoopLimit) {
2668         // Remove it from macro list and put on IGVN worklist to optimize.
2669         C-&gt;remove_macro_node(n);
2670         _igvn._worklist.push(n);
2671         success = true;
2672       } else if (n-&gt;Opcode() == Op_CallStaticJava) {
2673         // Remove it from macro list and put on IGVN worklist to optimize.
2674         C-&gt;remove_macro_node(n);
2675         _igvn._worklist.push(n);
2676         success = true;
2677       } else if (n-&gt;Opcode() == Op_Opaque1 || n-&gt;Opcode() == Op_Opaque2) {
2678         _igvn.replace_node(n, n-&gt;in(1));
2679         success = true;
2680 #if INCLUDE_RTM_OPT
2681       } else if ((n-&gt;Opcode() == Op_Opaque3) &amp;&amp; ((Opaque3Node*)n)-&gt;rtm_opt()) {
2682         assert(C-&gt;profile_rtm(), &quot;should be used only in rtm deoptimization code&quot;);
2683         assert((n-&gt;outcnt() == 1) &amp;&amp; n-&gt;unique_out()-&gt;is_Cmp(), &quot;&quot;);
2684         Node* cmp = n-&gt;unique_out();
</pre>
<hr />
<pre>
2689         assert((bol-&gt;outcnt() == 1) &amp;&amp; bol-&gt;unique_out()-&gt;is_If() &amp;&amp;
2690                (bol-&gt;_test._test == BoolTest::ne), &quot;&quot;);
2691         IfNode* ifn = bol-&gt;unique_out()-&gt;as_If();
2692         assert((ifn-&gt;outcnt() == 2) &amp;&amp;
2693                ifn-&gt;proj_out(1)-&gt;is_uncommon_trap_proj(Deoptimization::Reason_rtm_state_change) != NULL, &quot;&quot;);
2694 #endif
2695         Node* repl = n-&gt;in(1);
2696         if (!_has_locks) {
2697           // Remove RTM state check if there are no locks in the code.
2698           // Replace input to compare the same value.
2699           repl = (cmp-&gt;in(1) == n) ? cmp-&gt;in(2) : cmp-&gt;in(1);
2700         }
2701         _igvn.replace_node(n, repl);
2702         success = true;
2703 #endif
2704       } else if (n-&gt;Opcode() == Op_OuterStripMinedLoop) {
2705         n-&gt;as_OuterStripMinedLoop()-&gt;adjust_strip_mined_loop(&amp;_igvn);
2706         C-&gt;remove_macro_node(n);
2707         success = true;
2708       }
<span class="line-modified">2709       assert(!success || (C-&gt;macro_count() == (old_macro_count - 1)), &quot;elimination must have deleted one node from macro list&quot;);</span>
2710       progress = progress || success;
2711     }
2712   }
2713 
2714   // expand arraycopy &quot;macro&quot; nodes first
2715   // For ReduceBulkZeroing, we must first process all arraycopy nodes
2716   // before the allocate nodes are expanded.
<span class="line-modified">2717   for (int i = C-&gt;macro_count(); i &gt; 0; i--) {</span>
<span class="line-modified">2718     Node* n = C-&gt;macro_node(i-1);</span>

2719     assert(n-&gt;is_macro(), &quot;only macro nodes expected here&quot;);
2720     if (_igvn.type(n) == Type::TOP || (n-&gt;in(0) != NULL &amp;&amp; n-&gt;in(0)-&gt;is_top())) {
2721       // node is unreachable, so don&#39;t try to expand it
2722       C-&gt;remove_macro_node(n);
<span class="line-modified">2723       continue;</span>
<span class="line-modified">2724     }</span>
<span class="line-added">2725     debug_only(int old_macro_count = C-&gt;macro_count(););</span>
<span class="line-added">2726     switch (n-&gt;class_id()) {</span>
<span class="line-added">2727     case Node::Class_Lock:</span>
<span class="line-added">2728       expand_lock_node(n-&gt;as_Lock());</span>
<span class="line-added">2729       assert(C-&gt;macro_count() == (old_macro_count - 1), &quot;expansion must have deleted one node from macro list&quot;);</span>
<span class="line-added">2730       break;</span>
<span class="line-added">2731     case Node::Class_Unlock:</span>
<span class="line-added">2732       expand_unlock_node(n-&gt;as_Unlock());</span>
<span class="line-added">2733       assert(C-&gt;macro_count() == (old_macro_count - 1), &quot;expansion must have deleted one node from macro list&quot;);</span>
<span class="line-added">2734       break;</span>
<span class="line-added">2735     case Node::Class_ArrayCopy:</span>
2736       expand_arraycopy_node(n-&gt;as_ArrayCopy());
<span class="line-modified">2737       assert(C-&gt;macro_count() == (old_macro_count - 1), &quot;expansion must have deleted one node from macro list&quot;);</span>
<span class="line-added">2738       break;</span>
<span class="line-added">2739     case Node::Class_SubTypeCheck:</span>
<span class="line-added">2740       expand_subtypecheck_node(n-&gt;as_SubTypeCheck());</span>
<span class="line-added">2741       assert(C-&gt;macro_count() == (old_macro_count - 1), &quot;expansion must have deleted one node from macro list&quot;);</span>
<span class="line-added">2742       break;</span>
2743     }
2744     if (C-&gt;failing())  return true;

2745   }
2746 
<span class="line-added">2747   // All nodes except Allocate nodes are expanded now. There could be</span>
<span class="line-added">2748   // new optimization opportunities (such as folding newly created</span>
<span class="line-added">2749   // load from a just allocated object). Run IGVN.</span>
<span class="line-added">2750   _igvn.set_delay_transform(false);</span>
<span class="line-added">2751   _igvn.optimize();</span>
<span class="line-added">2752   if (C-&gt;failing())  return true;</span>
<span class="line-added">2753 </span>
<span class="line-added">2754   _igvn.set_delay_transform(true);</span>
<span class="line-added">2755 </span>
2756   // expand &quot;macro&quot; nodes
2757   // nodes are removed from the macro list as they are processed
2758   while (C-&gt;macro_count() &gt; 0) {
2759     int macro_count = C-&gt;macro_count();
2760     Node * n = C-&gt;macro_node(macro_count-1);
2761     assert(n-&gt;is_macro(), &quot;only macro nodes expected here&quot;);
2762     if (_igvn.type(n) == Type::TOP || (n-&gt;in(0) != NULL &amp;&amp; n-&gt;in(0)-&gt;is_top())) {
2763       // node is unreachable, so don&#39;t try to expand it
2764       C-&gt;remove_macro_node(n);
2765       continue;
2766     }
2767     switch (n-&gt;class_id()) {
2768     case Node::Class_Allocate:
2769       expand_allocate(n-&gt;as_Allocate());
2770       break;
2771     case Node::Class_AllocateArray:
2772       expand_allocate_array(n-&gt;as_AllocateArray());
2773       break;






2774     default:
2775       assert(false, &quot;unknown node type in macro list&quot;);
2776     }
2777     assert(C-&gt;macro_count() &lt; macro_count, &quot;must have deleted a node from macro list&quot;);
2778     if (C-&gt;failing())  return true;
2779   }
2780 
2781   _igvn.set_delay_transform(false);
2782   _igvn.optimize();
2783   if (C-&gt;failing())  return true;
2784   return false;
2785 }
</pre>
</td>
</tr>
</table>
<center><a href="machnode.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macro.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>