<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/gcm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="escape.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="graphKit.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/gcm.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  86     uint j = 0;
  87     if (pb-&gt;_num_succs != 1) {  // More then 1 successor?
  88       // Search for successor
  89       uint max = pb-&gt;number_of_nodes();
  90       assert( max &gt; 1, &quot;&quot; );
  91       uint start = max - pb-&gt;_num_succs;
  92       // Find which output path belongs to projection
  93       for (j = start; j &lt; max; j++) {
  94         if( pb-&gt;get_node(j) == in0 )
  95           break;
  96       }
  97       assert( j &lt; max, &quot;must find&quot; );
  98       // Change control to match head of successor basic block
  99       j -= start;
 100     }
 101     n-&gt;set_req(0, pb-&gt;_succs[j]-&gt;head());
 102   }
 103 }
 104 
 105 bool PhaseCFG::is_dominator(Node* dom_node, Node* node) {

 106   if (dom_node == node) {
 107     return true;
 108   }
<span class="line-modified"> 109   Block* d = get_block_for_node(dom_node);</span>
<span class="line-modified"> 110   Block* n = get_block_for_node(node);</span>


 111   if (d == n) {
 112     if (dom_node-&gt;is_block_start()) {
 113       return true;
 114     }
 115     if (node-&gt;is_block_start()) {
 116       return false;
 117     }
 118     if (dom_node-&gt;is_block_proj()) {
 119       return false;
 120     }
 121     if (node-&gt;is_block_proj()) {
 122       return true;
 123     }










 124 #ifdef ASSERT
<span class="line-modified"> 125     node-&gt;dump();</span>
<span class="line-modified"> 126     dom_node-&gt;dump();</span>




 127 #endif
<span class="line-modified"> 128     fatal(&quot;unhandled&quot;);</span>
 129     return false;
 130   }
 131   return d-&gt;dom_lca(n) == d;
 132 }
 133 







































 134 //------------------------------schedule_pinned_nodes--------------------------
 135 // Set the basic block for Nodes pinned into blocks
 136 void PhaseCFG::schedule_pinned_nodes(VectorSet &amp;visited) {
 137   // Allocate node stack of size C-&gt;live_nodes()+8 to avoid frequent realloc
<span class="line-modified"> 138   GrowableArray &lt;Node *&gt; spstack(C-&gt;live_nodes() + 8);</span>
 139   spstack.push(_root);
 140   while (spstack.is_nonempty()) {
 141     Node* node = spstack.pop();
 142     if (!visited.test_set(node-&gt;_idx)) { // Test node and flag it as visited
 143       if (node-&gt;pinned() &amp;&amp; !has_block(node)) {  // Pinned?  Nail it down!
 144         assert(node-&gt;in(0), &quot;pinned Node must have Control&quot;);
 145         // Before setting block replace block_proj control edge
 146         replace_block_proj_ctrl(node);
 147         Node* input = node-&gt;in(0);
 148         while (!input-&gt;is_block_start()) {
 149           input = input-&gt;in(0);
 150         }
 151         Block* block = get_block_for_node(input); // Basic block of controlling input
 152         schedule_node_into_block(node, block);
 153       }
 154 
 155       // If the node has precedence edges (added when CastPP nodes are
 156       // removed in final_graph_reshaping), fix the control of the
 157       // node to cover the precedence edges and remove the
 158       // dependencies.
 159       Node* n = NULL;
 160       for (uint i = node-&gt;len()-1; i &gt;= node-&gt;req(); i--) {
 161         Node* m = node-&gt;in(i);
 162         if (m == NULL) continue;
<span class="line-modified"> 163         // Skip the precedence edge if the test that guarded a CastPP:</span>
<span class="line-modified"> 164         // - was optimized out during escape analysis</span>
<span class="line-modified"> 165         // (OptimizePtrCompare): the CastPP&#39;s control isn&#39;t an end of</span>
<span class="line-removed"> 166         // block.</span>
<span class="line-removed"> 167         // - is moved in the branch of a dominating If: the control of</span>
<span class="line-removed"> 168         // the CastPP is then a Region.</span>
<span class="line-removed"> 169         if (m-&gt;is_block_proj() || m-&gt;is_block_start()) {</span>
 170           node-&gt;rm_prec(i);
 171           if (n == NULL) {
 172             n = m;
 173           } else {
 174             assert(is_dominator(n, m) || is_dominator(m, n), &quot;one must dominate the other&quot;);
 175             n = is_dominator(n, m) ? m : n;
 176           }



 177         }
 178       }
 179       if (n != NULL) {
 180         assert(node-&gt;in(0), &quot;control should have been set&quot;);
 181         assert(is_dominator(n, node-&gt;in(0)) || is_dominator(node-&gt;in(0), n), &quot;one must dominate the other&quot;);
 182         if (!is_dominator(n, node-&gt;in(0))) {
 183           node-&gt;set_req(0, n);
 184         }
 185       }
 186 
 187       // process all inputs that are non NULL
<span class="line-modified"> 188       for (int i = node-&gt;req() - 1; i &gt;= 0; --i) {</span>
 189         if (node-&gt;in(i) != NULL) {
 190           spstack.push(node-&gt;in(i));
 191         }
 192       }
 193     }
 194   }
 195 }
 196 
 197 #ifdef ASSERT
 198 // Assert that new input b2 is dominated by all previous inputs.
 199 // Check this by by seeing that it is dominated by b1, the deepest
 200 // input observed until b2.
 201 static void assert_dom(Block* b1, Block* b2, Node* n, const PhaseCFG* cfg) {
 202   if (b1 == NULL)  return;
 203   assert(b1-&gt;_dom_depth &lt; b2-&gt;_dom_depth, &quot;sanity&quot;);
 204   Block* tmp = b2;
 205   while (tmp != b1 &amp;&amp; tmp != NULL) {
 206     tmp = tmp-&gt;_idom;
 207   }
 208   if (tmp != b1) {
</pre>
<hr />
<pre>
 493 //
 494 // Do not add edges to stores on distinct control-flow paths;
 495 // only add edges to stores which might interfere.
 496 //
 497 // Return the (updated) LCA.  There will not be any possibly interfering
 498 // store between the load&#39;s &quot;early block&quot; and the updated LCA.
 499 // Any stores in the updated LCA will have new precedence edges
 500 // back to the load.  The caller is expected to schedule the load
 501 // in the LCA, in which case the precedence edges will make LCM
 502 // preserve anti-dependences.  The caller may also hoist the load
 503 // above the LCA, if it is not the early block.
 504 Block* PhaseCFG::insert_anti_dependences(Block* LCA, Node* load, bool verify) {
 505   assert(load-&gt;needs_anti_dependence_check(), &quot;must be a load of some sort&quot;);
 506   assert(LCA != NULL, &quot;&quot;);
 507   DEBUG_ONLY(Block* LCA_orig = LCA);
 508 
 509   // Compute the alias index.  Loads and stores with different alias indices
 510   // do not need anti-dependence edges.
 511   int load_alias_idx = C-&gt;get_alias_index(load-&gt;adr_type());
 512 #ifdef ASSERT

 513   if (load_alias_idx == Compile::AliasIdxBot &amp;&amp; C-&gt;AliasLevel() &gt; 0 &amp;&amp;
 514       (PrintOpto || VerifyAliases ||
 515        (PrintMiscellaneous &amp;&amp; (WizardMode || Verbose)))) {
 516     // Load nodes should not consume all of memory.
 517     // Reporting a bottom type indicates a bug in adlc.
 518     // If some particular type of node validly consumes all of memory,
 519     // sharpen the preceding &quot;if&quot; to exclude it, so we can catch bugs here.
 520     tty-&gt;print_cr(&quot;*** Possible Anti-Dependence Bug:  Load consumes all of memory.&quot;);
 521     load-&gt;dump(2);
 522     if (VerifyAliases)  assert(load_alias_idx != Compile::AliasIdxBot, &quot;&quot;);
 523   }
 524 #endif
<span class="line-removed"> 525   assert(load_alias_idx || (load-&gt;is_Mach() &amp;&amp; load-&gt;as_Mach()-&gt;ideal_Opcode() == Op_StrComp),</span>
<span class="line-removed"> 526          &quot;String compare is only known &#39;load&#39; that does not conflict with any stores&quot;);</span>
<span class="line-removed"> 527   assert(load_alias_idx || (load-&gt;is_Mach() &amp;&amp; load-&gt;as_Mach()-&gt;ideal_Opcode() == Op_StrEquals),</span>
<span class="line-removed"> 528          &quot;String equals is a &#39;load&#39; that does not conflict with any stores&quot;);</span>
<span class="line-removed"> 529   assert(load_alias_idx || (load-&gt;is_Mach() &amp;&amp; load-&gt;as_Mach()-&gt;ideal_Opcode() == Op_StrIndexOf),</span>
<span class="line-removed"> 530          &quot;String indexOf is a &#39;load&#39; that does not conflict with any stores&quot;);</span>
<span class="line-removed"> 531   assert(load_alias_idx || (load-&gt;is_Mach() &amp;&amp; load-&gt;as_Mach()-&gt;ideal_Opcode() == Op_StrIndexOfChar),</span>
<span class="line-removed"> 532          &quot;String indexOfChar is a &#39;load&#39; that does not conflict with any stores&quot;);</span>
<span class="line-removed"> 533   assert(load_alias_idx || (load-&gt;is_Mach() &amp;&amp; load-&gt;as_Mach()-&gt;ideal_Opcode() == Op_AryEq),</span>
<span class="line-removed"> 534          &quot;Arrays equals is a &#39;load&#39; that does not conflict with any stores&quot;);</span>
<span class="line-removed"> 535   assert(load_alias_idx || (load-&gt;is_Mach() &amp;&amp; load-&gt;as_Mach()-&gt;ideal_Opcode() == Op_HasNegatives),</span>
<span class="line-removed"> 536          &quot;HasNegatives is a &#39;load&#39; that does not conflict with any stores&quot;);</span>
 537 
 538   if (!C-&gt;alias_type(load_alias_idx)-&gt;is_rewritable()) {
 539     // It is impossible to spoil this load by putting stores before it,
 540     // because we know that the stores will never update the value
 541     // which &#39;load&#39; must witness.
 542     return LCA;
 543   }
 544 
 545   node_idx_t load_index = load-&gt;_idx;
 546 
 547   // Note the earliest legal placement of &#39;load&#39;, as determined by
 548   // by the unique point in the dom tree where all memory effects
 549   // and other inputs are first available.  (Computed by schedule_early.)
 550   // For normal loads, &#39;early&#39; is the shallowest place (dom graph wise)
 551   // to look for anti-deps between this load and any store.
 552   Block* early = get_block_for_node(load);
 553 
 554   // If we are subsuming loads, compute an &quot;early&quot; block that only considers
 555   // memory or address inputs. This block may be different than the
 556   // schedule_early block in that it could be at an even shallower depth in the
</pre>
<hr />
<pre>
 646         // Check for call into the runtime using the Java calling
 647         // convention (and from there into a wrapper); it has no
 648         // _method.  Can&#39;t do this optimization for Native calls because
 649         // they CAN write to Java memory.
 650         if (mstore-&gt;ideal_Opcode() == Op_CallStaticJava) {
 651           assert(mstore-&gt;is_MachSafePoint(), &quot;&quot;);
 652           MachSafePointNode* ms = (MachSafePointNode*) mstore;
 653           assert(ms-&gt;is_MachCallJava(), &quot;&quot;);
 654           MachCallJavaNode* mcj = (MachCallJavaNode*) ms;
 655           if (mcj-&gt;_method == NULL) {
 656             // These runtime calls do not write to Java visible memory
 657             // (other than Raw) and so do not require anti-dependence edges.
 658             continue;
 659           }
 660         }
 661         // Same for SafePoints: they read/write Raw but only read otherwise.
 662         // This is basically a workaround for SafePoints only defining control
 663         // instead of control + memory.
 664         if (mstore-&gt;ideal_Opcode() == Op_SafePoint)
 665           continue;













 666       } else {
 667         // Some raw memory, such as the load of &quot;top&quot; at an allocation,
 668         // can be control dependent on the previous safepoint. See
 669         // comments in GraphKit::allocate_heap() about control input.
 670         // Inserting an anti-dep between such a safepoint and a use
 671         // creates a cycle, and will cause a subsequent failure in
 672         // local scheduling.  (BugId 4919904)
 673         // (%%% How can a control input be a safepoint and not a projection??)
 674         if (mstore-&gt;ideal_Opcode() == Op_SafePoint &amp;&amp; load-&gt;in(0) == mstore)
 675           continue;
 676       }
 677     }
 678 
 679     // Identify a block that the current load must be above,
 680     // or else observe that &#39;store&#39; is all the way up in the
 681     // earliest legal block for &#39;load&#39;.  In the latter case,
 682     // immediately insert an anti-dependence edge.
 683     Block* store_block = get_block_for_node(store);
 684     assert(store_block != NULL, &quot;unused killing projections skipped above&quot;);
 685 
</pre>
<hr />
<pre>
 730                &quot;Expect at least one phi input will not be from original memory state&quot;);
 731 #endif //ASSERT
 732 #endif //TRACK_PHI_INPUTS
 733     } else if (store_block != early) {
 734       // &#39;store&#39; is between the current LCA and earliest possible block.
 735       // Label its block, and decide later on how to raise the LCA
 736       // to include the effect on LCA of this store.
 737       // If this store&#39;s block gets chosen as the raised LCA, we
 738       // will find him on the non_early_stores list and stick him
 739       // with a precedence edge.
 740       // (But, don&#39;t bother if LCA is already raised all the way.)
 741       if (LCA != early) {
 742         store_block-&gt;set_raise_LCA_mark(load_index);
 743         must_raise_LCA = true;
 744         non_early_stores.push(store);
 745       }
 746     } else {
 747       // Found a possibly-interfering store in the load&#39;s &#39;early&#39; block.
 748       // This means &#39;load&#39; cannot sink at all in the dominator tree.
 749       // Add an anti-dep edge, and squeeze &#39;load&#39; into the highest block.
<span class="line-modified"> 750       assert(store != load-&gt;in(0), &quot;dependence cycle found&quot;);</span>
 751       if (verify) {
 752         assert(store-&gt;find_edge(load) != -1, &quot;missing precedence edge&quot;);
 753       } else {
 754         store-&gt;add_prec(load);
 755       }
 756       LCA = early;
 757       // This turns off the process of gathering non_early_stores.
 758     }
 759   }
 760   // (Worklist is now empty; all nearby stores have been visited.)
 761 
 762   // Finished if &#39;load&#39; must be scheduled in its &#39;early&#39; block.
 763   // If we found any stores there, they have already been given
 764   // precedence edges.
 765   if (LCA == early)  return LCA;
 766 
 767   // We get here only if there are no possibly-interfering stores
 768   // in the load&#39;s &#39;early&#39; block.  Move LCA up above all predecessors
 769   // which contain stores we have noted.
 770   //
 771   // The raised LCA block can be a home to such interfering stores,
 772   // but its predecessors must not contain any such stores.
 773   //
 774   // The raised LCA will be a lower bound for placing the load,
 775   // preventing the load from sinking past any block containing
 776   // a store that may invalidate the memory state required by &#39;load&#39;.
 777   if (must_raise_LCA)
 778     LCA = raise_LCA_above_marks(LCA, load-&gt;_idx, early, this);
 779   if (LCA == early)  return LCA;
 780 
 781   // Insert anti-dependence edges from &#39;load&#39; to each store
 782   // in the non-early LCA block.
 783   // Mine the non_early_stores list for such stores.
 784   if (LCA-&gt;raise_LCA_mark() == load_index) {
 785     while (non_early_stores.size() &gt; 0) {
 786       Node* store = non_early_stores.pop();
 787       Block* store_block = get_block_for_node(store);
 788       if (store_block == LCA) {
 789         // add anti_dependence from store to load in its own block
<span class="line-modified"> 790         assert(store != load-&gt;in(0), &quot;dependence cycle found&quot;);</span>
 791         if (verify) {
 792           assert(store-&gt;find_edge(load) != -1, &quot;missing precedence edge&quot;);
 793         } else {
 794           store-&gt;add_prec(load);
 795         }
 796       } else {
 797         assert(store_block-&gt;raise_LCA_mark() == load_index, &quot;block was marked&quot;);
 798         // Any other stores we found must be either inside the new LCA
 799         // or else outside the original LCA.  In the latter case, they
 800         // did not interfere with any use of &#39;load&#39;.
 801         assert(LCA-&gt;dominates(store_block)
 802                || !LCA_orig-&gt;dominates(store_block), &quot;no stray stores&quot;);
 803       }
 804     }
 805   }
 806 
 807   // Return the highest block containing stores; any stores
 808   // within that block have been given anti-dependence edges.
 809   return LCA;
 810 }
</pre>
<hr />
<pre>
 820   // Constructor for the iterator
 821   Node_Backward_Iterator(Node *root, VectorSet &amp;visited, Node_Stack &amp;stack, PhaseCFG &amp;cfg);
 822 
 823   // Postincrement operator to iterate over the nodes
 824   Node *next();
 825 
 826 private:
 827   VectorSet   &amp;_visited;
 828   Node_Stack  &amp;_stack;
 829   PhaseCFG &amp;_cfg;
 830 };
 831 
 832 // Constructor for the Node_Backward_Iterator
 833 Node_Backward_Iterator::Node_Backward_Iterator( Node *root, VectorSet &amp;visited, Node_Stack &amp;stack, PhaseCFG &amp;cfg)
 834   : _visited(visited), _stack(stack), _cfg(cfg) {
 835   // The stack should contain exactly the root
 836   stack.clear();
 837   stack.push(root, root-&gt;outcnt());
 838 
 839   // Clear the visited bits
<span class="line-modified"> 840   visited.Clear();</span>
 841 }
 842 
 843 // Iterator for the Node_Backward_Iterator
 844 Node *Node_Backward_Iterator::next() {
 845 
 846   // If the _stack is empty, then just return NULL: finished.
 847   if ( !_stack.size() )
 848     return NULL;
 849 
 850   // I visit unvisited not-anti-dependence users first, then anti-dependent
 851   // children next. I iterate backwards to support removal of nodes.
 852   // The stack holds states consisting of 3 values:
 853   // current Def node, flag which indicates 1st/2nd pass, index of current out edge
 854   Node *self = (Node*)(((uintptr_t)_stack.node()) &amp; ~1);
 855   bool iterate_anti_dep = (((uintptr_t)_stack.node()) &amp; 1);
 856   uint idx = MIN2(_stack.index(), self-&gt;outcnt()); // Support removal of nodes.
 857   _stack.pop();
 858 
 859   // I cycle here when I am entering a deeper level of recursion.
 860   // The key variable &#39;self&#39; was set prior to jumping here.
</pre>
<hr />
<pre>
1342 
1343 #ifndef PRODUCT
1344   if (trace_opto_pipelining()) {
1345     tty-&gt;print(&quot;\n---- Start GlobalCodeMotion ----\n&quot;);
1346   }
1347 #endif
1348 
1349   // Initialize the node to block mapping for things on the proj_list
1350   for (uint i = 0; i &lt; _matcher.number_of_projections(); i++) {
1351     unmap_node_from_block(_matcher.get_projection(i));
1352   }
1353 
1354   // Set the basic block for Nodes pinned into blocks
1355   Arena* arena = Thread::current()-&gt;resource_area();
1356   VectorSet visited(arena);
1357   schedule_pinned_nodes(visited);
1358 
1359   // Find the earliest Block any instruction can be placed in.  Some
1360   // instructions are pinned into Blocks.  Unpinned instructions can
1361   // appear in last block in which all their inputs occur.
<span class="line-modified">1362   visited.Clear();</span>
1363   Node_Stack stack(arena, (C-&gt;live_nodes() &gt;&gt; 2) + 16); // pre-grow
1364   if (!schedule_early(visited, stack)) {
1365     // Bailout without retry
1366     C-&gt;record_method_not_compilable(&quot;early schedule failed&quot;);
1367     return;
1368   }
1369 
1370   // Build Def-Use edges.
1371   // Compute the latency information (via backwards walk) for all the
1372   // instructions in the graph
1373   _node_latency = new GrowableArray&lt;uint&gt;(); // resource_area allocation
1374 
1375   if (C-&gt;do_scheduling()) {
1376     compute_latencies_backwards(visited, stack);
1377   }
1378 
1379   // Now schedule all codes as LATE as possible.  This is the LCA in the
1380   // dominator tree of all USES of a value.  Pick the block with the least
1381   // loop nesting depth that is lowest in the dominator tree.
<span class="line-modified">1382   // ( visited.Clear() called in schedule_late()-&gt;Node_Backward_Iterator() )</span>
1383   schedule_late(visited, stack);
1384   if (C-&gt;failing()) {
1385     // schedule_late fails only when graph is incorrect.
1386     assert(!VerifyGraphEdges, &quot;verification should have failed&quot;);
1387     return;
1388   }
1389 
1390 #ifndef PRODUCT
1391   if (trace_opto_pipelining()) {
1392     tty-&gt;print(&quot;\n---- Detect implicit null checks ----\n&quot;);
1393   }
1394 #endif
1395 
1396   // Detect implicit-null-check opportunities.  Basically, find NULL checks
1397   // with suitable memory ops nearby.  Use the memory op to do the NULL check.
1398   // I can generate a memory op if there is not one nearby.
1399   if (C-&gt;is_method_compilation()) {
1400     // By reversing the loop direction we get a very minor gain on mpegaudio.
1401     // Feel free to revert to a forward loop for clarity.
1402     // for( int i=0; i &lt; (int)matcher._null_check_tests.size(); i+=2 ) {
</pre>
<hr />
<pre>
1443     regalloc.set_live(live);
1444     regalloc.gather_lrg_masks(false);    // Collect LRG masks
1445     live.compute(node_size); // Compute liveness
1446 
1447     recalc_pressure_nodes = NEW_RESOURCE_ARRAY(intptr_t, node_size);
1448     for (uint i = 0; i &lt; node_size; i++) {
1449       recalc_pressure_nodes[i] = 0;
1450     }
1451   }
1452   _regalloc = &amp;regalloc;
1453 
1454 #ifndef PRODUCT
1455   if (trace_opto_pipelining()) {
1456     tty-&gt;print(&quot;\n---- Start Local Scheduling ----\n&quot;);
1457   }
1458 #endif
1459 
1460   // Schedule locally.  Right now a simple topological sort.
1461   // Later, do a real latency aware scheduler.
1462   GrowableArray&lt;int&gt; ready_cnt(C-&gt;unique(), C-&gt;unique(), -1);
<span class="line-modified">1463   visited.Clear();</span>
1464   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1465     Block* block = get_block(i);
1466     if (!schedule_local(block, ready_cnt, visited, recalc_pressure_nodes)) {
1467       if (!C-&gt;failure_reason_is(C2Compiler::retry_no_subsuming_loads())) {
1468         C-&gt;record_method_not_compilable(&quot;local schedule failed&quot;);
1469       }
1470       _regalloc = NULL;
1471       return;
1472     }
1473   }
1474   _regalloc = NULL;
1475 
1476   // If we inserted any instructions between a Call and his CatchNode,
1477   // clone the instructions on all paths below the Catch.
1478   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1479     Block* block = get_block(i);
1480     call_catch_cleanup(block);
1481   }
1482 
1483 #ifndef PRODUCT
</pre>
</td>
<td>
<hr />
<pre>
  86     uint j = 0;
  87     if (pb-&gt;_num_succs != 1) {  // More then 1 successor?
  88       // Search for successor
  89       uint max = pb-&gt;number_of_nodes();
  90       assert( max &gt; 1, &quot;&quot; );
  91       uint start = max - pb-&gt;_num_succs;
  92       // Find which output path belongs to projection
  93       for (j = start; j &lt; max; j++) {
  94         if( pb-&gt;get_node(j) == in0 )
  95           break;
  96       }
  97       assert( j &lt; max, &quot;must find&quot; );
  98       // Change control to match head of successor basic block
  99       j -= start;
 100     }
 101     n-&gt;set_req(0, pb-&gt;_succs[j]-&gt;head());
 102   }
 103 }
 104 
 105 bool PhaseCFG::is_dominator(Node* dom_node, Node* node) {
<span class="line-added"> 106   assert(is_CFG(node) &amp;&amp; is_CFG(dom_node), &quot;node and dom_node must be CFG nodes&quot;);</span>
 107   if (dom_node == node) {
 108     return true;
 109   }
<span class="line-modified"> 110   Block* d = find_block_for_node(dom_node);</span>
<span class="line-modified"> 111   Block* n = find_block_for_node(node);</span>
<span class="line-added"> 112   assert(n != NULL &amp;&amp; d != NULL, &quot;blocks must exist&quot;);</span>
<span class="line-added"> 113 </span>
 114   if (d == n) {
 115     if (dom_node-&gt;is_block_start()) {
 116       return true;
 117     }
 118     if (node-&gt;is_block_start()) {
 119       return false;
 120     }
 121     if (dom_node-&gt;is_block_proj()) {
 122       return false;
 123     }
 124     if (node-&gt;is_block_proj()) {
 125       return true;
 126     }
<span class="line-added"> 127 </span>
<span class="line-added"> 128     assert(is_control_proj_or_safepoint(node), &quot;node must be control projection or safepoint&quot;);</span>
<span class="line-added"> 129     assert(is_control_proj_or_safepoint(dom_node), &quot;dom_node must be control projection or safepoint&quot;);</span>
<span class="line-added"> 130 </span>
<span class="line-added"> 131     // Neither &#39;node&#39; nor &#39;dom_node&#39; is a block start or block projection.</span>
<span class="line-added"> 132     // Check if &#39;dom_node&#39; is above &#39;node&#39; in the control graph.</span>
<span class="line-added"> 133     if (is_dominating_control(dom_node, node)) {</span>
<span class="line-added"> 134       return true;</span>
<span class="line-added"> 135     }</span>
<span class="line-added"> 136 </span>
 137 #ifdef ASSERT
<span class="line-modified"> 138     // If &#39;dom_node&#39; does not dominate &#39;node&#39; then &#39;node&#39; has to dominate &#39;dom_node&#39;</span>
<span class="line-modified"> 139     if (!is_dominating_control(node, dom_node)) {</span>
<span class="line-added"> 140       node-&gt;dump();</span>
<span class="line-added"> 141       dom_node-&gt;dump();</span>
<span class="line-added"> 142       assert(false, &quot;neither dom_node nor node dominates the other&quot;);</span>
<span class="line-added"> 143     }</span>
 144 #endif
<span class="line-modified"> 145 </span>
 146     return false;
 147   }
 148   return d-&gt;dom_lca(n) == d;
 149 }
 150 
<span class="line-added"> 151 bool PhaseCFG::is_CFG(Node* n) {</span>
<span class="line-added"> 152   return n-&gt;is_block_proj() || n-&gt;is_block_start() || is_control_proj_or_safepoint(n);</span>
<span class="line-added"> 153 }</span>
<span class="line-added"> 154 </span>
<span class="line-added"> 155 bool PhaseCFG::is_control_proj_or_safepoint(Node* n) {</span>
<span class="line-added"> 156   bool result = (n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;ideal_Opcode() == Op_SafePoint) || (n-&gt;is_Proj() &amp;&amp; n-&gt;as_Proj()-&gt;bottom_type() == Type::CONTROL);</span>
<span class="line-added"> 157   assert(!result || (n-&gt;is_Mach() &amp;&amp; n-&gt;as_Mach()-&gt;ideal_Opcode() == Op_SafePoint)</span>
<span class="line-added"> 158           || (n-&gt;is_Proj() &amp;&amp; n-&gt;as_Proj()-&gt;_con == 0), &quot;If control projection, it must be projection 0&quot;);</span>
<span class="line-added"> 159   return result;</span>
<span class="line-added"> 160 }</span>
<span class="line-added"> 161 </span>
<span class="line-added"> 162 Block* PhaseCFG::find_block_for_node(Node* n) {</span>
<span class="line-added"> 163   if (n-&gt;is_block_start() || n-&gt;is_block_proj()) {</span>
<span class="line-added"> 164     return get_block_for_node(n);</span>
<span class="line-added"> 165   } else {</span>
<span class="line-added"> 166     // Walk the control graph up if &#39;n&#39; is not a block start nor a block projection. In this case &#39;n&#39; must be</span>
<span class="line-added"> 167     // an unmatched control projection or a not yet matched safepoint precedence edge in the middle of a block.</span>
<span class="line-added"> 168     assert(is_control_proj_or_safepoint(n), &quot;must be control projection or safepoint&quot;);</span>
<span class="line-added"> 169     Node* ctrl = n-&gt;in(0);</span>
<span class="line-added"> 170     while (!ctrl-&gt;is_block_start()) {</span>
<span class="line-added"> 171       ctrl = ctrl-&gt;in(0);</span>
<span class="line-added"> 172     }</span>
<span class="line-added"> 173     return get_block_for_node(ctrl);</span>
<span class="line-added"> 174   }</span>
<span class="line-added"> 175 }</span>
<span class="line-added"> 176 </span>
<span class="line-added"> 177 // Walk up the control graph from &#39;n&#39; and check if &#39;dom_ctrl&#39; is found.</span>
<span class="line-added"> 178 bool PhaseCFG::is_dominating_control(Node* dom_ctrl, Node* n) {</span>
<span class="line-added"> 179   Node* ctrl = n-&gt;in(0);</span>
<span class="line-added"> 180   while (!ctrl-&gt;is_block_start()) {</span>
<span class="line-added"> 181     if (ctrl == dom_ctrl) {</span>
<span class="line-added"> 182       return true;</span>
<span class="line-added"> 183     }</span>
<span class="line-added"> 184     ctrl = ctrl-&gt;in(0);</span>
<span class="line-added"> 185   }</span>
<span class="line-added"> 186   return false;</span>
<span class="line-added"> 187 }</span>
<span class="line-added"> 188 </span>
<span class="line-added"> 189 </span>
 190 //------------------------------schedule_pinned_nodes--------------------------
 191 // Set the basic block for Nodes pinned into blocks
 192 void PhaseCFG::schedule_pinned_nodes(VectorSet &amp;visited) {
 193   // Allocate node stack of size C-&gt;live_nodes()+8 to avoid frequent realloc
<span class="line-modified"> 194   GrowableArray &lt;Node*&gt; spstack(C-&gt;live_nodes() + 8);</span>
 195   spstack.push(_root);
 196   while (spstack.is_nonempty()) {
 197     Node* node = spstack.pop();
 198     if (!visited.test_set(node-&gt;_idx)) { // Test node and flag it as visited
 199       if (node-&gt;pinned() &amp;&amp; !has_block(node)) {  // Pinned?  Nail it down!
 200         assert(node-&gt;in(0), &quot;pinned Node must have Control&quot;);
 201         // Before setting block replace block_proj control edge
 202         replace_block_proj_ctrl(node);
 203         Node* input = node-&gt;in(0);
 204         while (!input-&gt;is_block_start()) {
 205           input = input-&gt;in(0);
 206         }
 207         Block* block = get_block_for_node(input); // Basic block of controlling input
 208         schedule_node_into_block(node, block);
 209       }
 210 
 211       // If the node has precedence edges (added when CastPP nodes are
 212       // removed in final_graph_reshaping), fix the control of the
 213       // node to cover the precedence edges and remove the
 214       // dependencies.
 215       Node* n = NULL;
 216       for (uint i = node-&gt;len()-1; i &gt;= node-&gt;req(); i--) {
 217         Node* m = node-&gt;in(i);
 218         if (m == NULL) continue;
<span class="line-modified"> 219 </span>
<span class="line-modified"> 220         // Only process precedence edges that are CFG nodes. Safepoints and control projections can be in the middle of a block</span>
<span class="line-modified"> 221         if (is_CFG(m)) {</span>




 222           node-&gt;rm_prec(i);
 223           if (n == NULL) {
 224             n = m;
 225           } else {
 226             assert(is_dominator(n, m) || is_dominator(m, n), &quot;one must dominate the other&quot;);
 227             n = is_dominator(n, m) ? m : n;
 228           }
<span class="line-added"> 229         } else {</span>
<span class="line-added"> 230           assert(node-&gt;is_Mach(), &quot;sanity&quot;);</span>
<span class="line-added"> 231           assert(node-&gt;as_Mach()-&gt;ideal_Opcode() == Op_StoreCM, &quot;must be StoreCM node&quot;);</span>
 232         }
 233       }
 234       if (n != NULL) {
 235         assert(node-&gt;in(0), &quot;control should have been set&quot;);
 236         assert(is_dominator(n, node-&gt;in(0)) || is_dominator(node-&gt;in(0), n), &quot;one must dominate the other&quot;);
 237         if (!is_dominator(n, node-&gt;in(0))) {
 238           node-&gt;set_req(0, n);
 239         }
 240       }
 241 
 242       // process all inputs that are non NULL
<span class="line-modified"> 243       for (int i = node-&gt;req()-1; i &gt;= 0; --i) {</span>
 244         if (node-&gt;in(i) != NULL) {
 245           spstack.push(node-&gt;in(i));
 246         }
 247       }
 248     }
 249   }
 250 }
 251 
 252 #ifdef ASSERT
 253 // Assert that new input b2 is dominated by all previous inputs.
 254 // Check this by by seeing that it is dominated by b1, the deepest
 255 // input observed until b2.
 256 static void assert_dom(Block* b1, Block* b2, Node* n, const PhaseCFG* cfg) {
 257   if (b1 == NULL)  return;
 258   assert(b1-&gt;_dom_depth &lt; b2-&gt;_dom_depth, &quot;sanity&quot;);
 259   Block* tmp = b2;
 260   while (tmp != b1 &amp;&amp; tmp != NULL) {
 261     tmp = tmp-&gt;_idom;
 262   }
 263   if (tmp != b1) {
</pre>
<hr />
<pre>
 548 //
 549 // Do not add edges to stores on distinct control-flow paths;
 550 // only add edges to stores which might interfere.
 551 //
 552 // Return the (updated) LCA.  There will not be any possibly interfering
 553 // store between the load&#39;s &quot;early block&quot; and the updated LCA.
 554 // Any stores in the updated LCA will have new precedence edges
 555 // back to the load.  The caller is expected to schedule the load
 556 // in the LCA, in which case the precedence edges will make LCM
 557 // preserve anti-dependences.  The caller may also hoist the load
 558 // above the LCA, if it is not the early block.
 559 Block* PhaseCFG::insert_anti_dependences(Block* LCA, Node* load, bool verify) {
 560   assert(load-&gt;needs_anti_dependence_check(), &quot;must be a load of some sort&quot;);
 561   assert(LCA != NULL, &quot;&quot;);
 562   DEBUG_ONLY(Block* LCA_orig = LCA);
 563 
 564   // Compute the alias index.  Loads and stores with different alias indices
 565   // do not need anti-dependence edges.
 566   int load_alias_idx = C-&gt;get_alias_index(load-&gt;adr_type());
 567 #ifdef ASSERT
<span class="line-added"> 568   assert(Compile::AliasIdxTop &lt;= load_alias_idx &amp;&amp; load_alias_idx &lt; C-&gt;num_alias_types(), &quot;Invalid alias index&quot;);</span>
 569   if (load_alias_idx == Compile::AliasIdxBot &amp;&amp; C-&gt;AliasLevel() &gt; 0 &amp;&amp;
 570       (PrintOpto || VerifyAliases ||
 571        (PrintMiscellaneous &amp;&amp; (WizardMode || Verbose)))) {
 572     // Load nodes should not consume all of memory.
 573     // Reporting a bottom type indicates a bug in adlc.
 574     // If some particular type of node validly consumes all of memory,
 575     // sharpen the preceding &quot;if&quot; to exclude it, so we can catch bugs here.
 576     tty-&gt;print_cr(&quot;*** Possible Anti-Dependence Bug:  Load consumes all of memory.&quot;);
 577     load-&gt;dump(2);
 578     if (VerifyAliases)  assert(load_alias_idx != Compile::AliasIdxBot, &quot;&quot;);
 579   }
 580 #endif












 581 
 582   if (!C-&gt;alias_type(load_alias_idx)-&gt;is_rewritable()) {
 583     // It is impossible to spoil this load by putting stores before it,
 584     // because we know that the stores will never update the value
 585     // which &#39;load&#39; must witness.
 586     return LCA;
 587   }
 588 
 589   node_idx_t load_index = load-&gt;_idx;
 590 
 591   // Note the earliest legal placement of &#39;load&#39;, as determined by
 592   // by the unique point in the dom tree where all memory effects
 593   // and other inputs are first available.  (Computed by schedule_early.)
 594   // For normal loads, &#39;early&#39; is the shallowest place (dom graph wise)
 595   // to look for anti-deps between this load and any store.
 596   Block* early = get_block_for_node(load);
 597 
 598   // If we are subsuming loads, compute an &quot;early&quot; block that only considers
 599   // memory or address inputs. This block may be different than the
 600   // schedule_early block in that it could be at an even shallower depth in the
</pre>
<hr />
<pre>
 690         // Check for call into the runtime using the Java calling
 691         // convention (and from there into a wrapper); it has no
 692         // _method.  Can&#39;t do this optimization for Native calls because
 693         // they CAN write to Java memory.
 694         if (mstore-&gt;ideal_Opcode() == Op_CallStaticJava) {
 695           assert(mstore-&gt;is_MachSafePoint(), &quot;&quot;);
 696           MachSafePointNode* ms = (MachSafePointNode*) mstore;
 697           assert(ms-&gt;is_MachCallJava(), &quot;&quot;);
 698           MachCallJavaNode* mcj = (MachCallJavaNode*) ms;
 699           if (mcj-&gt;_method == NULL) {
 700             // These runtime calls do not write to Java visible memory
 701             // (other than Raw) and so do not require anti-dependence edges.
 702             continue;
 703           }
 704         }
 705         // Same for SafePoints: they read/write Raw but only read otherwise.
 706         // This is basically a workaround for SafePoints only defining control
 707         // instead of control + memory.
 708         if (mstore-&gt;ideal_Opcode() == Op_SafePoint)
 709           continue;
<span class="line-added"> 710 </span>
<span class="line-added"> 711         // Check if the store is a membar on which the load is control dependent.</span>
<span class="line-added"> 712         // Inserting an anti-dependency between that membar and the load would</span>
<span class="line-added"> 713         // create a cycle that causes local scheduling to fail.</span>
<span class="line-added"> 714         if (mstore-&gt;isa_MachMemBar()) {</span>
<span class="line-added"> 715           Node* dom = load-&gt;find_exact_control(load-&gt;in(0));</span>
<span class="line-added"> 716           while (dom != NULL &amp;&amp; dom != dom-&gt;in(0) &amp;&amp; dom != mstore) {</span>
<span class="line-added"> 717             dom = dom-&gt;in(0);</span>
<span class="line-added"> 718           }</span>
<span class="line-added"> 719           if (dom == mstore) {</span>
<span class="line-added"> 720             continue;</span>
<span class="line-added"> 721           }</span>
<span class="line-added"> 722         }</span>
 723       } else {
 724         // Some raw memory, such as the load of &quot;top&quot; at an allocation,
 725         // can be control dependent on the previous safepoint. See
 726         // comments in GraphKit::allocate_heap() about control input.
 727         // Inserting an anti-dep between such a safepoint and a use
 728         // creates a cycle, and will cause a subsequent failure in
 729         // local scheduling.  (BugId 4919904)
 730         // (%%% How can a control input be a safepoint and not a projection??)
 731         if (mstore-&gt;ideal_Opcode() == Op_SafePoint &amp;&amp; load-&gt;in(0) == mstore)
 732           continue;
 733       }
 734     }
 735 
 736     // Identify a block that the current load must be above,
 737     // or else observe that &#39;store&#39; is all the way up in the
 738     // earliest legal block for &#39;load&#39;.  In the latter case,
 739     // immediately insert an anti-dependence edge.
 740     Block* store_block = get_block_for_node(store);
 741     assert(store_block != NULL, &quot;unused killing projections skipped above&quot;);
 742 
</pre>
<hr />
<pre>
 787                &quot;Expect at least one phi input will not be from original memory state&quot;);
 788 #endif //ASSERT
 789 #endif //TRACK_PHI_INPUTS
 790     } else if (store_block != early) {
 791       // &#39;store&#39; is between the current LCA and earliest possible block.
 792       // Label its block, and decide later on how to raise the LCA
 793       // to include the effect on LCA of this store.
 794       // If this store&#39;s block gets chosen as the raised LCA, we
 795       // will find him on the non_early_stores list and stick him
 796       // with a precedence edge.
 797       // (But, don&#39;t bother if LCA is already raised all the way.)
 798       if (LCA != early) {
 799         store_block-&gt;set_raise_LCA_mark(load_index);
 800         must_raise_LCA = true;
 801         non_early_stores.push(store);
 802       }
 803     } else {
 804       // Found a possibly-interfering store in the load&#39;s &#39;early&#39; block.
 805       // This means &#39;load&#39; cannot sink at all in the dominator tree.
 806       // Add an anti-dep edge, and squeeze &#39;load&#39; into the highest block.
<span class="line-modified"> 807       assert(store != load-&gt;find_exact_control(load-&gt;in(0)), &quot;dependence cycle found&quot;);</span>
 808       if (verify) {
 809         assert(store-&gt;find_edge(load) != -1, &quot;missing precedence edge&quot;);
 810       } else {
 811         store-&gt;add_prec(load);
 812       }
 813       LCA = early;
 814       // This turns off the process of gathering non_early_stores.
 815     }
 816   }
 817   // (Worklist is now empty; all nearby stores have been visited.)
 818 
 819   // Finished if &#39;load&#39; must be scheduled in its &#39;early&#39; block.
 820   // If we found any stores there, they have already been given
 821   // precedence edges.
 822   if (LCA == early)  return LCA;
 823 
 824   // We get here only if there are no possibly-interfering stores
 825   // in the load&#39;s &#39;early&#39; block.  Move LCA up above all predecessors
 826   // which contain stores we have noted.
 827   //
 828   // The raised LCA block can be a home to such interfering stores,
 829   // but its predecessors must not contain any such stores.
 830   //
 831   // The raised LCA will be a lower bound for placing the load,
 832   // preventing the load from sinking past any block containing
 833   // a store that may invalidate the memory state required by &#39;load&#39;.
 834   if (must_raise_LCA)
 835     LCA = raise_LCA_above_marks(LCA, load-&gt;_idx, early, this);
 836   if (LCA == early)  return LCA;
 837 
 838   // Insert anti-dependence edges from &#39;load&#39; to each store
 839   // in the non-early LCA block.
 840   // Mine the non_early_stores list for such stores.
 841   if (LCA-&gt;raise_LCA_mark() == load_index) {
 842     while (non_early_stores.size() &gt; 0) {
 843       Node* store = non_early_stores.pop();
 844       Block* store_block = get_block_for_node(store);
 845       if (store_block == LCA) {
 846         // add anti_dependence from store to load in its own block
<span class="line-modified"> 847         assert(store != load-&gt;find_exact_control(load-&gt;in(0)), &quot;dependence cycle found&quot;);</span>
 848         if (verify) {
 849           assert(store-&gt;find_edge(load) != -1, &quot;missing precedence edge&quot;);
 850         } else {
 851           store-&gt;add_prec(load);
 852         }
 853       } else {
 854         assert(store_block-&gt;raise_LCA_mark() == load_index, &quot;block was marked&quot;);
 855         // Any other stores we found must be either inside the new LCA
 856         // or else outside the original LCA.  In the latter case, they
 857         // did not interfere with any use of &#39;load&#39;.
 858         assert(LCA-&gt;dominates(store_block)
 859                || !LCA_orig-&gt;dominates(store_block), &quot;no stray stores&quot;);
 860       }
 861     }
 862   }
 863 
 864   // Return the highest block containing stores; any stores
 865   // within that block have been given anti-dependence edges.
 866   return LCA;
 867 }
</pre>
<hr />
<pre>
 877   // Constructor for the iterator
 878   Node_Backward_Iterator(Node *root, VectorSet &amp;visited, Node_Stack &amp;stack, PhaseCFG &amp;cfg);
 879 
 880   // Postincrement operator to iterate over the nodes
 881   Node *next();
 882 
 883 private:
 884   VectorSet   &amp;_visited;
 885   Node_Stack  &amp;_stack;
 886   PhaseCFG &amp;_cfg;
 887 };
 888 
 889 // Constructor for the Node_Backward_Iterator
 890 Node_Backward_Iterator::Node_Backward_Iterator( Node *root, VectorSet &amp;visited, Node_Stack &amp;stack, PhaseCFG &amp;cfg)
 891   : _visited(visited), _stack(stack), _cfg(cfg) {
 892   // The stack should contain exactly the root
 893   stack.clear();
 894   stack.push(root, root-&gt;outcnt());
 895 
 896   // Clear the visited bits
<span class="line-modified"> 897   visited.clear();</span>
 898 }
 899 
 900 // Iterator for the Node_Backward_Iterator
 901 Node *Node_Backward_Iterator::next() {
 902 
 903   // If the _stack is empty, then just return NULL: finished.
 904   if ( !_stack.size() )
 905     return NULL;
 906 
 907   // I visit unvisited not-anti-dependence users first, then anti-dependent
 908   // children next. I iterate backwards to support removal of nodes.
 909   // The stack holds states consisting of 3 values:
 910   // current Def node, flag which indicates 1st/2nd pass, index of current out edge
 911   Node *self = (Node*)(((uintptr_t)_stack.node()) &amp; ~1);
 912   bool iterate_anti_dep = (((uintptr_t)_stack.node()) &amp; 1);
 913   uint idx = MIN2(_stack.index(), self-&gt;outcnt()); // Support removal of nodes.
 914   _stack.pop();
 915 
 916   // I cycle here when I am entering a deeper level of recursion.
 917   // The key variable &#39;self&#39; was set prior to jumping here.
</pre>
<hr />
<pre>
1399 
1400 #ifndef PRODUCT
1401   if (trace_opto_pipelining()) {
1402     tty-&gt;print(&quot;\n---- Start GlobalCodeMotion ----\n&quot;);
1403   }
1404 #endif
1405 
1406   // Initialize the node to block mapping for things on the proj_list
1407   for (uint i = 0; i &lt; _matcher.number_of_projections(); i++) {
1408     unmap_node_from_block(_matcher.get_projection(i));
1409   }
1410 
1411   // Set the basic block for Nodes pinned into blocks
1412   Arena* arena = Thread::current()-&gt;resource_area();
1413   VectorSet visited(arena);
1414   schedule_pinned_nodes(visited);
1415 
1416   // Find the earliest Block any instruction can be placed in.  Some
1417   // instructions are pinned into Blocks.  Unpinned instructions can
1418   // appear in last block in which all their inputs occur.
<span class="line-modified">1419   visited.clear();</span>
1420   Node_Stack stack(arena, (C-&gt;live_nodes() &gt;&gt; 2) + 16); // pre-grow
1421   if (!schedule_early(visited, stack)) {
1422     // Bailout without retry
1423     C-&gt;record_method_not_compilable(&quot;early schedule failed&quot;);
1424     return;
1425   }
1426 
1427   // Build Def-Use edges.
1428   // Compute the latency information (via backwards walk) for all the
1429   // instructions in the graph
1430   _node_latency = new GrowableArray&lt;uint&gt;(); // resource_area allocation
1431 
1432   if (C-&gt;do_scheduling()) {
1433     compute_latencies_backwards(visited, stack);
1434   }
1435 
1436   // Now schedule all codes as LATE as possible.  This is the LCA in the
1437   // dominator tree of all USES of a value.  Pick the block with the least
1438   // loop nesting depth that is lowest in the dominator tree.
<span class="line-modified">1439   // ( visited.clear() called in schedule_late()-&gt;Node_Backward_Iterator() )</span>
1440   schedule_late(visited, stack);
1441   if (C-&gt;failing()) {
1442     // schedule_late fails only when graph is incorrect.
1443     assert(!VerifyGraphEdges, &quot;verification should have failed&quot;);
1444     return;
1445   }
1446 
1447 #ifndef PRODUCT
1448   if (trace_opto_pipelining()) {
1449     tty-&gt;print(&quot;\n---- Detect implicit null checks ----\n&quot;);
1450   }
1451 #endif
1452 
1453   // Detect implicit-null-check opportunities.  Basically, find NULL checks
1454   // with suitable memory ops nearby.  Use the memory op to do the NULL check.
1455   // I can generate a memory op if there is not one nearby.
1456   if (C-&gt;is_method_compilation()) {
1457     // By reversing the loop direction we get a very minor gain on mpegaudio.
1458     // Feel free to revert to a forward loop for clarity.
1459     // for( int i=0; i &lt; (int)matcher._null_check_tests.size(); i+=2 ) {
</pre>
<hr />
<pre>
1500     regalloc.set_live(live);
1501     regalloc.gather_lrg_masks(false);    // Collect LRG masks
1502     live.compute(node_size); // Compute liveness
1503 
1504     recalc_pressure_nodes = NEW_RESOURCE_ARRAY(intptr_t, node_size);
1505     for (uint i = 0; i &lt; node_size; i++) {
1506       recalc_pressure_nodes[i] = 0;
1507     }
1508   }
1509   _regalloc = &amp;regalloc;
1510 
1511 #ifndef PRODUCT
1512   if (trace_opto_pipelining()) {
1513     tty-&gt;print(&quot;\n---- Start Local Scheduling ----\n&quot;);
1514   }
1515 #endif
1516 
1517   // Schedule locally.  Right now a simple topological sort.
1518   // Later, do a real latency aware scheduler.
1519   GrowableArray&lt;int&gt; ready_cnt(C-&gt;unique(), C-&gt;unique(), -1);
<span class="line-modified">1520   visited.reset();</span>
1521   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1522     Block* block = get_block(i);
1523     if (!schedule_local(block, ready_cnt, visited, recalc_pressure_nodes)) {
1524       if (!C-&gt;failure_reason_is(C2Compiler::retry_no_subsuming_loads())) {
1525         C-&gt;record_method_not_compilable(&quot;local schedule failed&quot;);
1526       }
1527       _regalloc = NULL;
1528       return;
1529     }
1530   }
1531   _regalloc = NULL;
1532 
1533   // If we inserted any instructions between a Call and his CatchNode,
1534   // clone the instructions on all paths below the Catch.
1535   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1536     Block* block = get_block(i);
1537     call_catch_cleanup(block);
1538   }
1539 
1540 #ifndef PRODUCT
</pre>
</td>
</tr>
</table>
<center><a href="escape.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="graphKit.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>