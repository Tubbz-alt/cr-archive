<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/share/opto/matcher.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="macroArrayCopy.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="matcher.hpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/matcher.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 25,10 ***</span>
<span class="line-new-header">--- 25,11 ---</span>
  #include &quot;precompiled.hpp&quot;
  #include &quot;gc/shared/barrierSet.hpp&quot;
  #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  #include &quot;memory/allocation.inline.hpp&quot;
  #include &quot;memory/resourceArea.hpp&quot;
<span class="line-added">+ #include &quot;oops/compressedOops.hpp&quot;</span>
  #include &quot;opto/ad.hpp&quot;
  #include &quot;opto/addnode.hpp&quot;
  #include &quot;opto/callnode.hpp&quot;
  #include &quot;opto/idealGraphPrinter.hpp&quot;
  #include &quot;opto/matcher.hpp&quot;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 154,11 ***</span>
    Unique_Node_List worklist;
    VectorSet visited(Thread::current()-&gt;resource_area());
    worklist.push(xroot);
    while (worklist.size() &gt; 0) {
      Node* n = worklist.pop();
<span class="line-modified">!     visited &lt;&lt;= n-&gt;_idx;</span>
      assert(C-&gt;node_arena()-&gt;contains(n), &quot;dead node&quot;);
      for (uint j = 0; j &lt; n-&gt;req(); j++) {
        Node* in = n-&gt;in(j);
        if (in != NULL) {
          assert(C-&gt;node_arena()-&gt;contains(in), &quot;dead node&quot;);
<span class="line-new-header">--- 155,11 ---</span>
    Unique_Node_List worklist;
    VectorSet visited(Thread::current()-&gt;resource_area());
    worklist.push(xroot);
    while (worklist.size() &gt; 0) {
      Node* n = worklist.pop();
<span class="line-modified">!     visited.set(n-&gt;_idx);</span>
      assert(C-&gt;node_arena()-&gt;contains(n), &quot;dead node&quot;);
      for (uint j = 0; j &lt; n-&gt;req(); j++) {
        Node* in = n-&gt;in(j);
        if (in != NULL) {
          assert(C-&gt;node_arena()-&gt;contains(in), &quot;dead node&quot;);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 337,11 ***</span>
    C-&gt;set_unique(0);
    C-&gt;reset_dead_node_list();
  
    // Recursively match trees from old space into new space.
    // Correct leaves of new-space Nodes; they point to old-space.
<span class="line-modified">!   _visited.Clear();             // Clear visit bits for xform call</span>
    C-&gt;set_cached_top_node(xform( C-&gt;top(), live_nodes ));
    if (!C-&gt;failing()) {
      Node* xroot =        xform( C-&gt;root(), 1 );
      if (xroot == NULL) {
        Matcher::soft_match_failure();  // recursive matching process failed
<span class="line-new-header">--- 338,11 ---</span>
    C-&gt;set_unique(0);
    C-&gt;reset_dead_node_list();
  
    // Recursively match trees from old space into new space.
    // Correct leaves of new-space Nodes; they point to old-space.
<span class="line-modified">!   _visited.clear();</span>
    C-&gt;set_cached_top_node(xform( C-&gt;top(), live_nodes ));
    if (!C-&gt;failing()) {
      Node* xroot =        xform( C-&gt;root(), 1 );
      if (xroot == NULL) {
        Matcher::soft_match_failure();  // recursive matching process failed
</pre>
<hr />
<pre>
<span class="line-old-header">*** 389,14 ***</span>
  
    // Now smoke old-space
    NOT_DEBUG( old-&gt;destruct_contents() );
  
    // ------------------------
<span class="line-modified">!   // Set up save-on-entry registers</span>
    Fixup_Save_On_Entry( );
<span class="line-removed">- }</span>
  
  
  //------------------------------Fixup_Save_On_Entry----------------------------
  // The stated purpose of this routine is to take care of save-on-entry
  // registers.  However, the overall goal of the Match phase is to convert into
  // machine-specific instructions which have RegMasks to guide allocation.
<span class="line-new-header">--- 390,20 ---</span>
  
    // Now smoke old-space
    NOT_DEBUG( old-&gt;destruct_contents() );
  
    // ------------------------
<span class="line-modified">!   // Set up save-on-entry registers.</span>
    Fixup_Save_On_Entry( );
  
<span class="line-added">+   { // Cleanup mach IR after selection phase is over.</span>
<span class="line-added">+     Compile::TracePhase tp(&quot;postselect_cleanup&quot;, &amp;timers[_t_postselect_cleanup]);</span>
<span class="line-added">+     do_postselect_cleanup();</span>
<span class="line-added">+     if (C-&gt;failing())  return;</span>
<span class="line-added">+     assert(verify_after_postselect_cleanup(), &quot;&quot;);</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
  
  //------------------------------Fixup_Save_On_Entry----------------------------
  // The stated purpose of this routine is to take care of save-on-entry
  // registers.  However, the overall goal of the Match phase is to convert into
  // machine-specific instructions which have RegMasks to guide allocation.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 414,18 ***</span>
    rms[TypeFunc::ReturnAdr] = ret_adr;
    rms[TypeFunc::FramePtr ] = fp;
    return rms;
  }
  
<span class="line-modified">! //---------------------------init_first_stack_mask-----------------------------</span>
  // Create the initial stack mask used by values spilling to the stack.
  // Disallow any debug info in outgoing argument areas by setting the
  // initial mask accordingly.
  void Matcher::init_first_stack_mask() {
  
    // Allocate storage for spill masks as masks for the appropriate load type.
<span class="line-modified">!   RegMask *rms = (RegMask*)C-&gt;comp_arena()-&gt;Amalloc_D(sizeof(RegMask) * (3*6+5));</span>
  
    idealreg2spillmask  [Op_RegN] = &amp;rms[0];
    idealreg2spillmask  [Op_RegI] = &amp;rms[1];
    idealreg2spillmask  [Op_RegL] = &amp;rms[2];
    idealreg2spillmask  [Op_RegF] = &amp;rms[3];
<span class="line-new-header">--- 421,24 ---</span>
    rms[TypeFunc::ReturnAdr] = ret_adr;
    rms[TypeFunc::FramePtr ] = fp;
    return rms;
  }
  
<span class="line-modified">! #define NOF_STACK_MASKS (3*6+5)</span>
<span class="line-added">+ </span>
  // Create the initial stack mask used by values spilling to the stack.
  // Disallow any debug info in outgoing argument areas by setting the
  // initial mask accordingly.
  void Matcher::init_first_stack_mask() {
  
    // Allocate storage for spill masks as masks for the appropriate load type.
<span class="line-modified">!   RegMask *rms = (RegMask*)C-&gt;comp_arena()-&gt;Amalloc_D(sizeof(RegMask) * NOF_STACK_MASKS);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Initialize empty placeholder masks into the newly allocated arena</span>
<span class="line-added">+   for (int i = 0; i &lt; NOF_STACK_MASKS; i++) {</span>
<span class="line-added">+     new (rms + i) RegMask();</span>
<span class="line-added">+   }</span>
  
    idealreg2spillmask  [Op_RegN] = &amp;rms[0];
    idealreg2spillmask  [Op_RegI] = &amp;rms[1];
    idealreg2spillmask  [Op_RegL] = &amp;rms[2];
    idealreg2spillmask  [Op_RegF] = &amp;rms[3];
</pre>
<hr />
<pre>
<span class="line-old-header">*** 842,58 ***</span>
      mreg2regmask[i].Insert(i);
    }
  
    // Grab the Frame Pointer
    Node *fp  = ret-&gt;in(TypeFunc::FramePtr);
<span class="line-removed">-   Node *mem = ret-&gt;in(TypeFunc::Memory);</span>
<span class="line-removed">-   const TypePtr* atp = TypePtr::BOTTOM;</span>
    // Share frame pointer while making spill ops
    set_shared(fp);
  
<span class="line-modified">!   // Compute generic short-offset Loads</span>
<span class="line-removed">- #ifdef _LP64</span>
<span class="line-removed">-   MachNode *spillCP = match_tree(new LoadNNode(NULL,mem,fp,atp,TypeInstPtr::BOTTOM,MemNode::unordered));</span>
<span class="line-removed">- #endif</span>
<span class="line-removed">-   MachNode *spillI  = match_tree(new LoadINode(NULL,mem,fp,atp,TypeInt::INT,MemNode::unordered));</span>
<span class="line-removed">-   MachNode *spillL  = match_tree(new LoadLNode(NULL,mem,fp,atp,TypeLong::LONG,MemNode::unordered, LoadNode::DependsOnlyOnTest, false));</span>
<span class="line-removed">-   MachNode *spillF  = match_tree(new LoadFNode(NULL,mem,fp,atp,Type::FLOAT,MemNode::unordered));</span>
<span class="line-removed">-   MachNode *spillD  = match_tree(new LoadDNode(NULL,mem,fp,atp,Type::DOUBLE,MemNode::unordered));</span>
<span class="line-removed">-   MachNode *spillP  = match_tree(new LoadPNode(NULL,mem,fp,atp,TypeInstPtr::BOTTOM,MemNode::unordered));</span>
<span class="line-removed">-   assert(spillI != NULL &amp;&amp; spillL != NULL &amp;&amp; spillF != NULL &amp;&amp;</span>
<span class="line-removed">-          spillD != NULL &amp;&amp; spillP != NULL, &quot;&quot;);</span>
<span class="line-removed">-   // Get the ADLC notion of the right regmask, for each basic type.</span>
  #ifdef _LP64
<span class="line-modified">!   idealreg2regmask[Op_RegN] = &amp;spillCP-&gt;out_RegMask();</span>
  #endif
<span class="line-modified">!   idealreg2regmask[Op_RegI] = &amp;spillI-&gt;out_RegMask();</span>
<span class="line-modified">!   idealreg2regmask[Op_RegL] = &amp;spillL-&gt;out_RegMask();</span>
<span class="line-modified">!   idealreg2regmask[Op_RegF] = &amp;spillF-&gt;out_RegMask();</span>
<span class="line-modified">!   idealreg2regmask[Op_RegD] = &amp;spillD-&gt;out_RegMask();</span>
<span class="line-modified">!   idealreg2regmask[Op_RegP] = &amp;spillP-&gt;out_RegMask();</span>
<span class="line-modified">! </span>
<span class="line-modified">!   // Vector regmasks.</span>
<span class="line-modified">!   if (Matcher::vector_size_supported(T_BYTE,4)) {</span>
<span class="line-modified">!     TypeVect::VECTS = TypeVect::make(T_BYTE, 4);</span>
<span class="line-modified">!     MachNode *spillVectS = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTS));</span>
<span class="line-removed">-     idealreg2regmask[Op_VecS] = &amp;spillVectS-&gt;out_RegMask();</span>
<span class="line-removed">-   }</span>
<span class="line-removed">-   if (Matcher::vector_size_supported(T_FLOAT,2)) {</span>
<span class="line-removed">-     MachNode *spillVectD = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTD));</span>
<span class="line-removed">-     idealreg2regmask[Op_VecD] = &amp;spillVectD-&gt;out_RegMask();</span>
<span class="line-removed">-   }</span>
<span class="line-removed">-   if (Matcher::vector_size_supported(T_FLOAT,4)) {</span>
<span class="line-removed">-     MachNode *spillVectX = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTX));</span>
<span class="line-removed">-     idealreg2regmask[Op_VecX] = &amp;spillVectX-&gt;out_RegMask();</span>
<span class="line-removed">-   }</span>
<span class="line-removed">-   if (Matcher::vector_size_supported(T_FLOAT,8)) {</span>
<span class="line-removed">-     MachNode *spillVectY = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTY));</span>
<span class="line-removed">-     idealreg2regmask[Op_VecY] = &amp;spillVectY-&gt;out_RegMask();</span>
<span class="line-removed">-   }</span>
<span class="line-removed">-   if (Matcher::vector_size_supported(T_FLOAT,16)) {</span>
<span class="line-removed">-     MachNode *spillVectZ = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTZ));</span>
<span class="line-removed">-     idealreg2regmask[Op_VecZ] = &amp;spillVectZ-&gt;out_RegMask();</span>
<span class="line-removed">-   }</span>
  }
  
  #ifdef ASSERT
  static void match_alias_type(Compile* C, Node* n, Node* m) {
    if (!VerifyAliases)  return;  // do not go looking for trouble by default
<span class="line-new-header">--- 855,27 ---</span>
      mreg2regmask[i].Insert(i);
    }
  
    // Grab the Frame Pointer
    Node *fp  = ret-&gt;in(TypeFunc::FramePtr);
    // Share frame pointer while making spill ops
    set_shared(fp);
  
<span class="line-modified">! // Get the ADLC notion of the right regmask, for each basic type.</span>
  #ifdef _LP64
<span class="line-modified">!   idealreg2regmask[Op_RegN] = regmask_for_ideal_register(Op_RegN, ret);</span>
  #endif
<span class="line-modified">!   idealreg2regmask[Op_RegI] = regmask_for_ideal_register(Op_RegI, ret);</span>
<span class="line-modified">!   idealreg2regmask[Op_RegP] = regmask_for_ideal_register(Op_RegP, ret);</span>
<span class="line-modified">!   idealreg2regmask[Op_RegF] = regmask_for_ideal_register(Op_RegF, ret);</span>
<span class="line-modified">!   idealreg2regmask[Op_RegD] = regmask_for_ideal_register(Op_RegD, ret);</span>
<span class="line-modified">!   idealreg2regmask[Op_RegL] = regmask_for_ideal_register(Op_RegL, ret);</span>
<span class="line-modified">!   idealreg2regmask[Op_VecS] = regmask_for_ideal_register(Op_VecS, ret);</span>
<span class="line-modified">!   idealreg2regmask[Op_VecD] = regmask_for_ideal_register(Op_VecD, ret);</span>
<span class="line-modified">!   idealreg2regmask[Op_VecX] = regmask_for_ideal_register(Op_VecX, ret);</span>
<span class="line-modified">!   idealreg2regmask[Op_VecY] = regmask_for_ideal_register(Op_VecY, ret);</span>
<span class="line-modified">!   idealreg2regmask[Op_VecZ] = regmask_for_ideal_register(Op_VecZ, ret);</span>
  }
  
  #ifdef ASSERT
  static void match_alias_type(Compile* C, Node* n, Node* m) {
    if (!VerifyAliases)  return;  // do not go looking for trouble by default
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1742,10 ***</span>
<span class="line-new-header">--- 1724,17 ---</span>
    if (leaf-&gt;is_Con() || leaf-&gt;is_DecodeNarrowPtr()) {
      // Record the con for sharing
      _shared_nodes.map(leaf-&gt;_idx, ex);
    }
  
<span class="line-added">+   // Have mach nodes inherit GC barrier data</span>
<span class="line-added">+   if (leaf-&gt;is_LoadStore()) {</span>
<span class="line-added">+     mach-&gt;set_barrier_data(leaf-&gt;as_LoadStore()-&gt;barrier_data());</span>
<span class="line-added">+   } else if (leaf-&gt;is_Mem()) {</span>
<span class="line-added">+     mach-&gt;set_barrier_data(leaf-&gt;as_Mem()-&gt;barrier_data());</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
    return ex;
  }
  
  void Matcher::handle_precedence_edges(Node* n, MachNode *mach) {
    for (uint i = n-&gt;req(); i &lt; n-&gt;len(); i++) {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2019,10 ***</span>
<span class="line-new-header">--- 2008,19 ---</span>
    }
    return false;
  }
  #endif // X86
  
<span class="line-added">+ bool Matcher::is_vshift_con_pattern(Node *n, Node *m) {</span>
<span class="line-added">+   if (n != NULL &amp;&amp; m != NULL) {</span>
<span class="line-added">+     return VectorNode::is_vector_shift(n) &amp;&amp;</span>
<span class="line-added">+            VectorNode::is_vector_shift_count(m) &amp;&amp; m-&gt;in(1)-&gt;is_Con();</span>
<span class="line-added">+   }</span>
<span class="line-added">+   return false;</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ </span>
  bool Matcher::clone_base_plus_offset_address(AddPNode* m, Matcher::MStack&amp; mstack, VectorSet&amp; address_visited) {
    Node *off = m-&gt;in(AddPNode::Offset);
    if (off-&gt;is_Con()) {
      address_visited.test_set(m-&gt;_idx); // Flag as address_visited
      mstack.push(m-&gt;in(AddPNode::Address), Pre_Visit);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2057,25 ***</span>
        }
        if (is_visited(n)) {   // Visited already?
          // Node is shared and has no reason to clone.  Flag it as shared.
          // This causes it to match into a register for the sharing.
          set_shared(n);       // Flag as shared and
          mstack.pop();        // remove node from stack
          continue;
        }
        nstate = Visit; // Not already visited; so visit now
      }
      if (nstate == Visit) {
        mstack.set_state(Post_Visit);
        set_visited(n);   // Flag as visited now
        bool mem_op = false;
        int mem_addr_idx = MemNode::Address;
<span class="line-modified">!       bool gc_handled = BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;matcher_find_shared_visit(this, mstack, n, nop, mem_op, mem_addr_idx);</span>
<span class="line-modified">!       if (!gc_handled) {</span>
<span class="line-removed">-         if (find_shared_visit(mstack, n, nop, mem_op, mem_addr_idx)) {</span>
<span class="line-removed">-           continue;</span>
<span class="line-removed">-         }</span>
        }
        for(int i = n-&gt;req() - 1; i &gt;= 0; --i) { // For my children
          Node *m = n-&gt;in(i); // Get ith input
          if (m == NULL) continue;  // Ignore NULLs
          uint mop = m-&gt;Opcode();
<span class="line-new-header">--- 2055,28 ---</span>
        }
        if (is_visited(n)) {   // Visited already?
          // Node is shared and has no reason to clone.  Flag it as shared.
          // This causes it to match into a register for the sharing.
          set_shared(n);       // Flag as shared and
<span class="line-added">+         if (n-&gt;is_DecodeNarrowPtr()) {</span>
<span class="line-added">+           // Oop field/array element loads must be shared but since</span>
<span class="line-added">+           // they are shared through a DecodeN they may appear to have</span>
<span class="line-added">+           // a single use so force sharing here.</span>
<span class="line-added">+           set_shared(n-&gt;in(1));</span>
<span class="line-added">+         }</span>
          mstack.pop();        // remove node from stack
          continue;
        }
        nstate = Visit; // Not already visited; so visit now
      }
      if (nstate == Visit) {
        mstack.set_state(Post_Visit);
        set_visited(n);   // Flag as visited now
        bool mem_op = false;
        int mem_addr_idx = MemNode::Address;
<span class="line-modified">!       if (find_shared_visit(mstack, n, nop, mem_op, mem_addr_idx)) {</span>
<span class="line-modified">!         continue;</span>
        }
        for(int i = n-&gt;req() - 1; i &gt;= 0; --i) { // For my children
          Node *m = n-&gt;in(i); // Get ith input
          if (m == NULL) continue;  // Ignore NULLs
          uint mop = m-&gt;Opcode();
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2089,24 ***</span>
          if( _must_clone[mop] ) {
            mstack.push(m, Visit);
            continue; // for(int i = ...)
          }
  
<span class="line-removed">-         if( mop == Op_AddP &amp;&amp; m-&gt;in(AddPNode::Base)-&gt;is_DecodeNarrowPtr()) {</span>
<span class="line-removed">-           // Bases used in addresses must be shared but since</span>
<span class="line-removed">-           // they are shared through a DecodeN they may appear</span>
<span class="line-removed">-           // to have a single use so force sharing here.</span>
<span class="line-removed">-           set_shared(m-&gt;in(AddPNode::Base)-&gt;in(1));</span>
<span class="line-removed">-         }</span>
<span class="line-removed">- </span>
          // if &#39;n&#39; and &#39;m&#39; are part of a graph for BMI instruction, clone this node.
  #ifdef X86
          if (UseBMI1Instructions &amp;&amp; is_bmi_pattern(n, m)) {
            mstack.push(m, Visit);
            continue;
          }
  #endif
  
          // Clone addressing expressions as they are &quot;free&quot; in memory access instructions
          if (mem_op &amp;&amp; i == mem_addr_idx &amp;&amp; mop == Op_AddP &amp;&amp;
              // When there are other uses besides address expressions
              // put it on stack and mark as shared.
<span class="line-new-header">--- 2090,21 ---</span>
          if( _must_clone[mop] ) {
            mstack.push(m, Visit);
            continue; // for(int i = ...)
          }
  
          // if &#39;n&#39; and &#39;m&#39; are part of a graph for BMI instruction, clone this node.
  #ifdef X86
          if (UseBMI1Instructions &amp;&amp; is_bmi_pattern(n, m)) {
            mstack.push(m, Visit);
            continue;
          }
  #endif
<span class="line-added">+         if (is_vshift_con_pattern(n, m)) {</span>
<span class="line-added">+           mstack.push(m, Visit);</span>
<span class="line-added">+           continue;</span>
<span class="line-added">+         }</span>
  
          // Clone addressing expressions as they are &quot;free&quot; in memory access instructions
          if (mem_op &amp;&amp; i == mem_addr_idx &amp;&amp; mop == Op_AddP &amp;&amp;
              // When there are other uses besides address expressions
              // put it on stack and mark as shared.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2474,10 ***</span>
<span class="line-new-header">--- 2472,182 ---</span>
        i-=2;
      }
    }
  }
  
<span class="line-added">+ bool Matcher::gen_narrow_oop_implicit_null_checks() {</span>
<span class="line-added">+   // Advice matcher to perform null checks on the narrow oop side.</span>
<span class="line-added">+   // Implicit checks are not possible on the uncompressed oop side anyway</span>
<span class="line-added">+   // (at least not for read accesses).</span>
<span class="line-added">+   // Performs significantly better (especially on Power 6).</span>
<span class="line-added">+   if (!os::zero_page_read_protected()) {</span>
<span class="line-added">+     return true;</span>
<span class="line-added">+   }</span>
<span class="line-added">+   return CompressedOops::use_implicit_null_checks() &amp;&amp;</span>
<span class="line-added">+          (narrow_oop_use_complex_address() ||</span>
<span class="line-added">+           CompressedOops::base() != NULL);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ // Compute RegMask for an ideal register.</span>
<span class="line-added">+ const RegMask* Matcher::regmask_for_ideal_register(uint ideal_reg, Node* ret) {</span>
<span class="line-added">+   const Type* t = Type::mreg2type[ideal_reg];</span>
<span class="line-added">+   if (t == NULL) {</span>
<span class="line-added">+     assert(ideal_reg &gt;= Op_VecS &amp;&amp; ideal_reg &lt;= Op_VecZ, &quot;not a vector: %d&quot;, ideal_reg);</span>
<span class="line-added">+     return NULL; // not supported</span>
<span class="line-added">+   }</span>
<span class="line-added">+   Node* fp  = ret-&gt;in(TypeFunc::FramePtr);</span>
<span class="line-added">+   Node* mem = ret-&gt;in(TypeFunc::Memory);</span>
<span class="line-added">+   const TypePtr* atp = TypePtr::BOTTOM;</span>
<span class="line-added">+   MemNode::MemOrd mo = MemNode::unordered;</span>
<span class="line-added">+ </span>
<span class="line-added">+   Node* spill;</span>
<span class="line-added">+   switch (ideal_reg) {</span>
<span class="line-added">+     case Op_RegN: spill = new LoadNNode(NULL, mem, fp, atp, t-&gt;is_narrowoop(), mo); break;</span>
<span class="line-added">+     case Op_RegI: spill = new LoadINode(NULL, mem, fp, atp, t-&gt;is_int(),       mo); break;</span>
<span class="line-added">+     case Op_RegP: spill = new LoadPNode(NULL, mem, fp, atp, t-&gt;is_ptr(),       mo); break;</span>
<span class="line-added">+     case Op_RegF: spill = new LoadFNode(NULL, mem, fp, atp, t,                 mo); break;</span>
<span class="line-added">+     case Op_RegD: spill = new LoadDNode(NULL, mem, fp, atp, t,                 mo); break;</span>
<span class="line-added">+     case Op_RegL: spill = new LoadLNode(NULL, mem, fp, atp, t-&gt;is_long(),      mo); break;</span>
<span class="line-added">+ </span>
<span class="line-added">+     case Op_VecS: // fall-through</span>
<span class="line-added">+     case Op_VecD: // fall-through</span>
<span class="line-added">+     case Op_VecX: // fall-through</span>
<span class="line-added">+     case Op_VecY: // fall-through</span>
<span class="line-added">+     case Op_VecZ: spill = new LoadVectorNode(NULL, mem, fp, atp, t-&gt;is_vect()); break;</span>
<span class="line-added">+ </span>
<span class="line-added">+     default: ShouldNotReachHere();</span>
<span class="line-added">+   }</span>
<span class="line-added">+   MachNode* mspill = match_tree(spill);</span>
<span class="line-added">+   assert(mspill != NULL, &quot;matching failed: %d&quot;, ideal_reg);</span>
<span class="line-added">+   // Handle generic vector operand case</span>
<span class="line-added">+   if (Matcher::supports_generic_vector_operands &amp;&amp; t-&gt;isa_vect()) {</span>
<span class="line-added">+     specialize_mach_node(mspill);</span>
<span class="line-added">+   }</span>
<span class="line-added">+   return &amp;mspill-&gt;out_RegMask();</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ // Process Mach IR right after selection phase is over.</span>
<span class="line-added">+ void Matcher::do_postselect_cleanup() {</span>
<span class="line-added">+   if (supports_generic_vector_operands) {</span>
<span class="line-added">+     specialize_generic_vector_operands();</span>
<span class="line-added">+     if (C-&gt;failing())  return;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ //----------------------------------------------------------------------</span>
<span class="line-added">+ // Generic machine operands elision.</span>
<span class="line-added">+ //----------------------------------------------------------------------</span>
<span class="line-added">+ </span>
<span class="line-added">+ // Convert (leg)Vec to (leg)Vec[SDXYZ].</span>
<span class="line-added">+ MachOper* Matcher::specialize_vector_operand_helper(MachNode* m, uint opnd_idx, const TypeVect* vt) {</span>
<span class="line-added">+   MachOper* original_opnd = m-&gt;_opnds[opnd_idx];</span>
<span class="line-added">+   uint ideal_reg = vt-&gt;ideal_reg();</span>
<span class="line-added">+   // Handle special cases.</span>
<span class="line-added">+   // LShiftCntV/RShiftCntV report wide vector type, but Matcher::vector_shift_count_ideal_reg() as ideal register (see vectornode.hpp).</span>
<span class="line-added">+   // Look for shift count use sites as well (at vector shift nodes).</span>
<span class="line-added">+   int opc = m-&gt;ideal_Opcode();</span>
<span class="line-added">+   if ((VectorNode::is_vector_shift_count(opc)  &amp;&amp; opnd_idx == 0) || // DEF operand of LShiftCntV/RShiftCntV</span>
<span class="line-added">+       (VectorNode::is_vector_shift(opc)        &amp;&amp; opnd_idx == 2)) { // shift operand of a vector shift node</span>
<span class="line-added">+     ideal_reg = Matcher::vector_shift_count_ideal_reg(vt-&gt;length_in_bytes());</span>
<span class="line-added">+   }</span>
<span class="line-added">+   return Matcher::specialize_generic_vector_operand(original_opnd, ideal_reg, false);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ // Compute concrete vector operand for a generic TEMP vector mach node based on its user info.</span>
<span class="line-added">+ void Matcher::specialize_temp_node(MachTempNode* tmp, MachNode* use, uint idx) {</span>
<span class="line-added">+   assert(use-&gt;in(idx) == tmp, &quot;not a user&quot;);</span>
<span class="line-added">+   assert(!Matcher::is_generic_vector(use-&gt;_opnds[0]), &quot;use not processed yet&quot;);</span>
<span class="line-added">+ </span>
<span class="line-added">+   if ((uint)idx == use-&gt;two_adr()) { // DEF_TEMP case</span>
<span class="line-added">+     tmp-&gt;_opnds[0] = use-&gt;_opnds[0]-&gt;clone();</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     uint ideal_vreg = vector_ideal_reg(C-&gt;max_vector_size());</span>
<span class="line-added">+     tmp-&gt;_opnds[0] = specialize_generic_vector_operand(tmp-&gt;_opnds[0], ideal_vreg, true);</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ // Compute concrete vector operand for a generic DEF/USE vector operand (of mach node m at index idx).</span>
<span class="line-added">+ MachOper* Matcher::specialize_vector_operand(MachNode* m, uint opnd_idx) {</span>
<span class="line-added">+   assert(Matcher::is_generic_vector(m-&gt;_opnds[opnd_idx]), &quot;repeated updates&quot;);</span>
<span class="line-added">+   Node* def = NULL;</span>
<span class="line-added">+   if (opnd_idx == 0) { // DEF</span>
<span class="line-added">+     def = m; // use mach node itself to compute vector operand type</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     int base_idx = m-&gt;operand_index(opnd_idx);</span>
<span class="line-added">+     def = m-&gt;in(base_idx);</span>
<span class="line-added">+     if (def-&gt;is_Mach()) {</span>
<span class="line-added">+       if (def-&gt;is_MachTemp() &amp;&amp; Matcher::is_generic_vector(def-&gt;as_Mach()-&gt;_opnds[0])) {</span>
<span class="line-added">+         specialize_temp_node(def-&gt;as_MachTemp(), m, base_idx); // MachTemp node use site</span>
<span class="line-added">+       } else if (is_generic_reg2reg_move(def-&gt;as_Mach())) {</span>
<span class="line-added">+         def = def-&gt;in(1); // skip over generic reg-to-reg moves</span>
<span class="line-added">+       }</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+   return specialize_vector_operand_helper(m, opnd_idx, def-&gt;bottom_type()-&gt;is_vect());</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ void Matcher::specialize_mach_node(MachNode* m) {</span>
<span class="line-added">+   assert(!m-&gt;is_MachTemp(), &quot;processed along with its user&quot;);</span>
<span class="line-added">+   // For generic use operands pull specific register class operands from</span>
<span class="line-added">+   // its def instruction&#39;s output operand (def operand).</span>
<span class="line-added">+   for (uint i = 0; i &lt; m-&gt;num_opnds(); i++) {</span>
<span class="line-added">+     if (Matcher::is_generic_vector(m-&gt;_opnds[i])) {</span>
<span class="line-added">+       m-&gt;_opnds[i] = specialize_vector_operand(m, i);</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ // Replace generic vector operands with concrete vector operands and eliminate generic reg-to-reg moves from the graph.</span>
<span class="line-added">+ void Matcher::specialize_generic_vector_operands() {</span>
<span class="line-added">+   assert(supports_generic_vector_operands, &quot;sanity&quot;);</span>
<span class="line-added">+   ResourceMark rm;</span>
<span class="line-added">+ </span>
<span class="line-added">+   if (C-&gt;max_vector_size() == 0) {</span>
<span class="line-added">+     return; // no vector instructions or operands</span>
<span class="line-added">+   }</span>
<span class="line-added">+   // Replace generic vector operands (vec/legVec) with concrete ones (vec[SDXYZ]/legVec[SDXYZ])</span>
<span class="line-added">+   // and remove reg-to-reg vector moves (MoveVec2Leg and MoveLeg2Vec).</span>
<span class="line-added">+   Unique_Node_List live_nodes;</span>
<span class="line-added">+   C-&gt;identify_useful_nodes(live_nodes);</span>
<span class="line-added">+ </span>
<span class="line-added">+   while (live_nodes.size() &gt; 0) {</span>
<span class="line-added">+     MachNode* m = live_nodes.pop()-&gt;isa_Mach();</span>
<span class="line-added">+     if (m != NULL) {</span>
<span class="line-added">+       if (Matcher::is_generic_reg2reg_move(m)) {</span>
<span class="line-added">+         // Register allocator properly handles vec &lt;=&gt; leg moves using register masks.</span>
<span class="line-added">+         int opnd_idx = m-&gt;operand_index(1);</span>
<span class="line-added">+         Node* def = m-&gt;in(opnd_idx);</span>
<span class="line-added">+         m-&gt;subsume_by(def, C);</span>
<span class="line-added">+       } else if (m-&gt;is_MachTemp()) {</span>
<span class="line-added">+         // process MachTemp nodes at use site (see Matcher::specialize_vector_operand)</span>
<span class="line-added">+       } else {</span>
<span class="line-added">+         specialize_mach_node(m);</span>
<span class="line-added">+       }</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ #ifdef ASSERT</span>
<span class="line-added">+ bool Matcher::verify_after_postselect_cleanup() {</span>
<span class="line-added">+   assert(!C-&gt;failing(), &quot;sanity&quot;);</span>
<span class="line-added">+   if (supports_generic_vector_operands) {</span>
<span class="line-added">+     Unique_Node_List useful;</span>
<span class="line-added">+     C-&gt;identify_useful_nodes(useful);</span>
<span class="line-added">+     for (uint i = 0; i &lt; useful.size(); i++) {</span>
<span class="line-added">+       MachNode* m = useful.at(i)-&gt;isa_Mach();</span>
<span class="line-added">+       if (m != NULL) {</span>
<span class="line-added">+         assert(!Matcher::is_generic_reg2reg_move(m), &quot;no MoveVec nodes allowed&quot;);</span>
<span class="line-added">+         for (uint j = 0; j &lt; m-&gt;num_opnds(); j++) {</span>
<span class="line-added">+           assert(!Matcher::is_generic_vector(m-&gt;_opnds[j]), &quot;no generic vector operands allowed&quot;);</span>
<span class="line-added">+         }</span>
<span class="line-added">+       }</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+   return true;</span>
<span class="line-added">+ }</span>
<span class="line-added">+ #endif // ASSERT</span>
<span class="line-added">+ </span>
  // Used by the DFA in dfa_xxx.cpp.  Check for a following barrier or
  // atomic instruction acting as a store_load barrier without any
  // intervening volatile load, and thus we don&#39;t need a barrier here.
  // We retain the Node to act as a compiler ordering barrier.
  bool Matcher::post_store_load_barrier(const Node* vmb) {
</pre>
<center><a href="macroArrayCopy.cpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="matcher.hpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>