<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/parse1.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="parse.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="parse2.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/parse1.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;compiler/compileLog.hpp&quot;
  27 #include &quot;interpreter/linkResolver.hpp&quot;
  28 #include &quot;memory/resourceArea.hpp&quot;
  29 #include &quot;oops/method.hpp&quot;
  30 #include &quot;opto/addnode.hpp&quot;
  31 #include &quot;opto/c2compiler.hpp&quot;
  32 #include &quot;opto/castnode.hpp&quot;
  33 #include &quot;opto/idealGraphPrinter.hpp&quot;
  34 #include &quot;opto/locknode.hpp&quot;
  35 #include &quot;opto/memnode.hpp&quot;
  36 #include &quot;opto/opaquenode.hpp&quot;
  37 #include &quot;opto/parse.hpp&quot;
  38 #include &quot;opto/rootnode.hpp&quot;
  39 #include &quot;opto/runtime.hpp&quot;
  40 #include &quot;runtime/arguments.hpp&quot;
  41 #include &quot;runtime/handles.inline.hpp&quot;
  42 #include &quot;runtime/safepointMechanism.hpp&quot;
  43 #include &quot;runtime/sharedRuntime.hpp&quot;

  44 #include &quot;utilities/copy.hpp&quot;
  45 
  46 // Static array so we can figure out which bytecodes stop us from compiling
  47 // the most. Some of the non-static variables are needed in bytecodeInfo.cpp
  48 // and eventually should be encapsulated in a proper class (gri 8/18/98).
  49 
  50 #ifndef PRODUCT
  51 int nodes_created              = 0;
  52 int methods_parsed             = 0;
  53 int methods_seen               = 0;
  54 int blocks_parsed              = 0;
  55 int blocks_seen                = 0;
  56 
  57 int explicit_null_checks_inserted = 0;
  58 int explicit_null_checks_elided   = 0;
  59 int all_null_checks_found         = 0;
  60 int implicit_null_checks          = 0;
  61 
  62 bool Parse::BytecodeParseHistogram::_initialized = false;
  63 uint Parse::BytecodeParseHistogram::_bytecodes_parsed [Bytecodes::number_of_codes];
</pre>
<hr />
<pre>
 506     _flow = method()-&gt;get_osr_flow_analysis(osr_bci());
 507     if (_flow-&gt;failing()) {
 508       C-&gt;record_method_not_compilable(_flow-&gt;failure_reason());
 509 #ifndef PRODUCT
 510       if (PrintOpto &amp;&amp; (Verbose || WizardMode)) {
 511         tty-&gt;print_cr(&quot;OSR @%d type flow bailout: %s&quot;, _entry_bci, _flow-&gt;failure_reason());
 512         if (Verbose) {
 513           method()-&gt;print();
 514           method()-&gt;print_codes();
 515           _flow-&gt;print();
 516         }
 517       }
 518 #endif
 519     }
 520     _tf = C-&gt;tf();     // the OSR entry type is different
 521   }
 522 
 523 #ifdef ASSERT
 524   if (depth() == 1) {
 525     assert(C-&gt;is_osr_compilation() == this-&gt;is_osr_parse(), &quot;OSR in sync&quot;);
<span class="line-removed"> 526     if (C-&gt;tf() != tf()) {</span>
<span class="line-removed"> 527       assert(C-&gt;env()-&gt;system_dictionary_modification_counter_changed(),</span>
<span class="line-removed"> 528              &quot;Must invalidate if TypeFuncs differ&quot;);</span>
<span class="line-removed"> 529     }</span>
 530   } else {
 531     assert(!this-&gt;is_osr_parse(), &quot;no recursive OSR&quot;);
 532   }
 533 #endif
 534 
 535 #ifndef PRODUCT
 536   methods_parsed++;
 537   // add method size here to guarantee that inlined methods are added too
 538   if (CITime)
 539     _total_bytes_compiled += method()-&gt;code_size();
 540 
 541   show_parse_info();
 542 #endif
 543 
 544   if (failing()) {
 545     if (log)  log-&gt;done(&quot;parse&quot;);
 546     return;
 547   }
 548 
 549   gvn().set_type(root(), root()-&gt;bottom_type());
</pre>
<hr />
<pre>
 567   Node_Notes* caller_nn = C-&gt;default_node_notes();
 568   // Collect debug info for inlined calls unless -XX:-DebugInlinedCalls.
 569   if (DebugInlinedCalls || depth() == 1) {
 570     C-&gt;set_default_node_notes(make_node_notes(caller_nn));
 571   }
 572 
 573   if (is_osr_parse()) {
 574     Node* osr_buf = entry_map-&gt;in(TypeFunc::Parms+0);
 575     entry_map-&gt;set_req(TypeFunc::Parms+0, top());
 576     set_map(entry_map);
 577     load_interpreter_state(osr_buf);
 578   } else {
 579     set_map(entry_map);
 580     do_method_entry();
 581     if (depth() == 1 &amp;&amp; C-&gt;age_code()) {
 582       decrement_age();
 583     }
 584   }
 585 
 586   if (depth() == 1 &amp;&amp; !failing()) {





 587     // Add check to deoptimize the nmethod if RTM state was changed
 588     rtm_deopt();
 589   }
 590 
 591   // Check for bailouts during method entry or RTM state check setup.
 592   if (failing()) {
 593     if (log)  log-&gt;done(&quot;parse&quot;);
 594     C-&gt;set_default_node_notes(caller_nn);
 595     return;
 596   }
 597 
 598   entry_map = map();  // capture any changes performed by method setup code
 599   assert(jvms()-&gt;endoff() == map()-&gt;req(), &quot;map matches JVMS layout&quot;);
 600 
 601   // We begin parsing as if we have just encountered a jump to the
 602   // method entry.
 603   Block* entry_block = start_block();
 604   assert(entry_block-&gt;start() == (is_osr_parse() ? osr_bci() : 0), &quot;&quot;);
 605   set_map_clone(entry_map);
 606   merge_common(entry_block, entry_block-&gt;next_path_num());
</pre>
<hr />
<pre>
 961 void Parse::do_exits() {
 962   set_parse_bci(InvocationEntryBci);
 963 
 964   // Now peephole on the return bits
 965   Node* region = _exits.control();
 966   _exits.set_control(gvn().transform(region));
 967 
 968   Node* iophi = _exits.i_o();
 969   _exits.set_i_o(gvn().transform(iophi));
 970 
 971   // Figure out if we need to emit the trailing barrier. The barrier is only
 972   // needed in the constructors, and only in three cases:
 973   //
 974   // 1. The constructor wrote a final. The effects of all initializations
 975   //    must be committed to memory before any code after the constructor
 976   //    publishes the reference to the newly constructed object. Rather
 977   //    than wait for the publication, we simply block the writes here.
 978   //    Rather than put a barrier on only those writes which are required
 979   //    to complete, we force all writes to complete.
 980   //
<span class="line-modified"> 981   // 2. On PPC64, also add MemBarRelease for constructors which write</span>
<span class="line-removed"> 982   //    volatile fields. As support_IRIW_for_not_multiple_copy_atomic_cpu</span>
<span class="line-removed"> 983   //    is set on PPC64, no sync instruction is issued after volatile</span>
<span class="line-removed"> 984   //    stores. We want to guarantee the same behavior as on platforms</span>
<span class="line-removed"> 985   //    with total store order, although this is not required by the Java</span>
<span class="line-removed"> 986   //    memory model. So as with finals, we add a barrier here.</span>
<span class="line-removed"> 987   //</span>
<span class="line-removed"> 988   // 3. Experimental VM option is used to force the barrier if any field</span>
 989   //    was written out in the constructor.
 990   //










 991   // &quot;All bets are off&quot; unless the first publication occurs after a
 992   // normal return from the constructor.  We do not attempt to detect
 993   // such unusual early publications.  But no barrier is needed on
 994   // exceptional returns, since they cannot publish normally.
 995   //
 996   if (method()-&gt;is_initializer() &amp;&amp;
<span class="line-modified"> 997         (wrote_final() ||</span>
<span class="line-modified"> 998            PPC64_ONLY(wrote_volatile() ||)</span>
<span class="line-modified"> 999            (AlwaysSafeConstructors &amp;&amp; wrote_fields()))) {</span>
1000     _exits.insert_mem_bar(Op_MemBarRelease, alloc_with_final());
1001 
1002     // If Memory barrier is created for final fields write
1003     // and allocation node does not escape the initialize method,
1004     // then barrier introduced by allocation node can be removed.
1005     if (DoEscapeAnalysis &amp;&amp; alloc_with_final()) {
1006       AllocateNode *alloc = AllocateNode::Ideal_allocation(alloc_with_final(), &amp;_gvn);
1007       alloc-&gt;compute_MemBar_redundancy(method());
1008     }
1009     if (PrintOpto &amp;&amp; (Verbose || WizardMode)) {
1010       method()-&gt;print_name();
1011       tty-&gt;print_cr(&quot; writes finals and needs a memory barrier&quot;);
1012     }
1013   }
1014 
1015   // Any method can write a @Stable field; insert memory barriers
1016   // after those also. Can&#39;t bind predecessor allocation node (if any)
1017   // with barrier because allocation doesn&#39;t always dominate
1018   // MemBarRelease.
1019   if (wrote_stable()) {
1020     _exits.insert_mem_bar(Op_MemBarRelease);
1021     if (PrintOpto &amp;&amp; (Verbose || WizardMode)) {
1022       method()-&gt;print_name();
1023       tty-&gt;print_cr(&quot; writes @Stable and needs a memory barrier&quot;);
1024     }
1025   }
1026 
1027   for (MergeMemStream mms(_exits.merged_memory()); mms.next_non_empty(); ) {
1028     // transform each slice of the original memphi:
1029     mms.set_memory(_gvn.transform(mms.memory()));
1030   }


1031 
1032   if (tf()-&gt;range()-&gt;cnt() &gt; TypeFunc::Parms) {
1033     const Type* ret_type = tf()-&gt;range()-&gt;field_at(TypeFunc::Parms);
1034     Node*       ret_phi  = _gvn.transform( _exits.argument(0) );
1035     if (!_exits.control()-&gt;is_top() &amp;&amp; _gvn.type(ret_phi)-&gt;empty()) {
<span class="line-modified">1036       // In case of concurrent class loading, the type we set for the</span>
<span class="line-modified">1037       // ret_phi in build_exits() may have been too optimistic and the</span>
<span class="line-modified">1038       // ret_phi may be top now.</span>
<span class="line-modified">1039       // Otherwise, we&#39;ve encountered an error and have to mark the method as</span>
<span class="line-modified">1040       // not compilable. Just using an assertion instead would be dangerous</span>
<span class="line-modified">1041       // as this could lead to an infinite compile loop in non-debug builds.</span>
<span class="line-removed">1042       {</span>
<span class="line-removed">1043         if (C-&gt;env()-&gt;system_dictionary_modification_counter_changed()) {</span>
<span class="line-removed">1044           C-&gt;record_failure(C2Compiler::retry_class_loading_during_parsing());</span>
<span class="line-removed">1045         } else {</span>
<span class="line-removed">1046           C-&gt;record_method_not_compilable(&quot;Can&#39;t determine return type.&quot;);</span>
<span class="line-removed">1047         }</span>
<span class="line-removed">1048       }</span>
1049       return;
1050     }
1051     if (ret_type-&gt;isa_int()) {
1052       BasicType ret_bt = method()-&gt;return_type()-&gt;basic_type();
1053       ret_phi = mask_int_value(ret_phi, ret_bt, &amp;_gvn);
1054     }
1055     _exits.push_node(ret_type-&gt;basic_type(), ret_phi);
1056   }
1057 
1058   // Note:  Logic for creating and optimizing the ReturnNode is in Compile.
1059 
1060   // Unlock along the exceptional paths.
1061   // This is done late so that we can common up equivalent exceptions
1062   // (e.g., null checks) arising from multiple points within this method.
1063   // See GraphKit::add_exception_state, which performs the commoning.
1064   bool do_synch = method()-&gt;is_synchronized() &amp;&amp; GenerateSynchronizationCode;
1065 
1066   // record exit from a method if compiled while Dtrace is turned on.
1067   if (do_synch || C-&gt;env()-&gt;dtrace_method_probes() || _replaced_nodes_for_exceptions) {
1068     // First move the exception list out of _exits:
</pre>
<hr />
<pre>
1173   uint arg_size = tf()-&gt;domain()-&gt;cnt();
1174   ensure_stack(arg_size - TypeFunc::Parms);  // OSR methods have funny args
1175   for (i = TypeFunc::Parms; i &lt; arg_size; i++) {
1176     map()-&gt;init_req(i, inmap-&gt;argument(_caller, i - TypeFunc::Parms));
1177   }
1178 
1179   // Clear out the rest of the map (locals and stack)
1180   for (i = arg_size; i &lt; len; i++) {
1181     map()-&gt;init_req(i, top());
1182   }
1183 
1184   SafePointNode* entry_map = stop();
1185   return entry_map;
1186 }
1187 
1188 //-----------------------------do_method_entry--------------------------------
1189 // Emit any code needed in the pseudo-block before BCI zero.
1190 // The main thing to do is lock the receiver of a synchronized method.
1191 void Parse::do_method_entry() {
1192   set_parse_bci(InvocationEntryBci); // Pseudo-BCP
<span class="line-modified">1193   set_sp(0);                      // Java Stack Pointer</span>
1194 
1195   NOT_PRODUCT( count_compiled_calls(true/*at_method_entry*/, false/*is_inline*/); )
1196 
1197   if (C-&gt;env()-&gt;dtrace_method_probes()) {
1198     make_dtrace_method_entry(method());
1199   }
1200 
1201   // If the method is synchronized, we need to construct a lock node, attach
1202   // it to the Start node, and pin it there.
1203   if (method()-&gt;is_synchronized()) {
1204     // Insert a FastLockNode right after the Start which takes as arguments
1205     // the current thread pointer, the &quot;this&quot; pointer &amp; the address of the
1206     // stack slot pair used for the lock.  The &quot;this&quot; pointer is a projection
1207     // off the start node, but the locking spot has to be constructed by
1208     // creating a ConLNode of 0, and boxing it with a BoxLockNode.  The BoxLockNode
1209     // becomes the second argument to the FastLockNode call.  The
1210     // FastLockNode becomes the new control parent to pin it to the start.
1211 
1212     // Setup Object Pointer
1213     Node *lock_obj = NULL;
</pre>
<hr />
<pre>
2083                                    receiver);
2084     make_slow_call_ex(call, env()-&gt;Throwable_klass(), true);
2085 
2086     Node* fast_io  = call-&gt;in(TypeFunc::I_O);
2087     Node* fast_mem = call-&gt;in(TypeFunc::Memory);
2088     // These two phis are pre-filled with copies of of the fast IO and Memory
2089     Node* io_phi   = PhiNode::make(result_rgn, fast_io,  Type::ABIO);
2090     Node* mem_phi  = PhiNode::make(result_rgn, fast_mem, Type::MEMORY, TypePtr::BOTTOM);
2091 
2092     result_rgn-&gt;init_req(2, control());
2093     io_phi    -&gt;init_req(2, i_o());
2094     mem_phi   -&gt;init_req(2, reset_memory());
2095 
2096     set_all_memory( _gvn.transform(mem_phi) );
2097     set_i_o(        _gvn.transform(io_phi) );
2098   }
2099 
2100   set_control( _gvn.transform(result_rgn) );
2101 }
2102 













2103 // Add check to deoptimize if RTM state is not ProfileRTM
2104 void Parse::rtm_deopt() {
2105 #if INCLUDE_RTM_OPT
2106   if (C-&gt;profile_rtm()) {
<span class="line-modified">2107     assert(C-&gt;method() != NULL, &quot;only for normal compilations&quot;);</span>
2108     assert(!C-&gt;method()-&gt;method_data()-&gt;is_empty(), &quot;MDO is needed to record RTM state&quot;);
2109     assert(depth() == 1, &quot;generate check only for main compiled method&quot;);
2110 
2111     // Set starting bci for uncommon trap.
2112     set_parse_bci(is_osr_parse() ? osr_bci() : 0);
2113 
2114     // Load the rtm_state from the MethodData.
2115     const TypePtr* adr_type = TypeMetadataPtr::make(C-&gt;method()-&gt;method_data());
2116     Node* mdo = makecon(adr_type);
2117     int offset = MethodData::rtm_state_offset_in_bytes();
2118     Node* adr_node = basic_plus_adr(mdo, mdo, offset);
2119     Node* rtm_state = make_load(control(), adr_node, TypeInt::INT, T_INT, adr_type, MemNode::unordered);
2120 
2121     // Separate Load from Cmp by Opaque.
2122     // In expand_macro_nodes() it will be replaced either
2123     // with this load when there are locks in the code
2124     // or with ProfileRTM (cmp-&gt;in(2)) otherwise so that
2125     // the check will fold.
2126     Node* profile_state = makecon(TypeInt::make(ProfileRTM));
2127     Node* opq   = _gvn.transform( new Opaque3Node(C, rtm_state, Opaque3Node::RTM_OPT) );
</pre>
</td>
<td>
<hr />
<pre>
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;compiler/compileLog.hpp&quot;
  27 #include &quot;interpreter/linkResolver.hpp&quot;
  28 #include &quot;memory/resourceArea.hpp&quot;
  29 #include &quot;oops/method.hpp&quot;
  30 #include &quot;opto/addnode.hpp&quot;
  31 #include &quot;opto/c2compiler.hpp&quot;
  32 #include &quot;opto/castnode.hpp&quot;
  33 #include &quot;opto/idealGraphPrinter.hpp&quot;
  34 #include &quot;opto/locknode.hpp&quot;
  35 #include &quot;opto/memnode.hpp&quot;
  36 #include &quot;opto/opaquenode.hpp&quot;
  37 #include &quot;opto/parse.hpp&quot;
  38 #include &quot;opto/rootnode.hpp&quot;
  39 #include &quot;opto/runtime.hpp&quot;
  40 #include &quot;runtime/arguments.hpp&quot;
  41 #include &quot;runtime/handles.inline.hpp&quot;
  42 #include &quot;runtime/safepointMechanism.hpp&quot;
  43 #include &quot;runtime/sharedRuntime.hpp&quot;
<span class="line-added">  44 #include &quot;utilities/bitMap.inline.hpp&quot;</span>
  45 #include &quot;utilities/copy.hpp&quot;
  46 
  47 // Static array so we can figure out which bytecodes stop us from compiling
  48 // the most. Some of the non-static variables are needed in bytecodeInfo.cpp
  49 // and eventually should be encapsulated in a proper class (gri 8/18/98).
  50 
  51 #ifndef PRODUCT
  52 int nodes_created              = 0;
  53 int methods_parsed             = 0;
  54 int methods_seen               = 0;
  55 int blocks_parsed              = 0;
  56 int blocks_seen                = 0;
  57 
  58 int explicit_null_checks_inserted = 0;
  59 int explicit_null_checks_elided   = 0;
  60 int all_null_checks_found         = 0;
  61 int implicit_null_checks          = 0;
  62 
  63 bool Parse::BytecodeParseHistogram::_initialized = false;
  64 uint Parse::BytecodeParseHistogram::_bytecodes_parsed [Bytecodes::number_of_codes];
</pre>
<hr />
<pre>
 507     _flow = method()-&gt;get_osr_flow_analysis(osr_bci());
 508     if (_flow-&gt;failing()) {
 509       C-&gt;record_method_not_compilable(_flow-&gt;failure_reason());
 510 #ifndef PRODUCT
 511       if (PrintOpto &amp;&amp; (Verbose || WizardMode)) {
 512         tty-&gt;print_cr(&quot;OSR @%d type flow bailout: %s&quot;, _entry_bci, _flow-&gt;failure_reason());
 513         if (Verbose) {
 514           method()-&gt;print();
 515           method()-&gt;print_codes();
 516           _flow-&gt;print();
 517         }
 518       }
 519 #endif
 520     }
 521     _tf = C-&gt;tf();     // the OSR entry type is different
 522   }
 523 
 524 #ifdef ASSERT
 525   if (depth() == 1) {
 526     assert(C-&gt;is_osr_compilation() == this-&gt;is_osr_parse(), &quot;OSR in sync&quot;);




 527   } else {
 528     assert(!this-&gt;is_osr_parse(), &quot;no recursive OSR&quot;);
 529   }
 530 #endif
 531 
 532 #ifndef PRODUCT
 533   methods_parsed++;
 534   // add method size here to guarantee that inlined methods are added too
 535   if (CITime)
 536     _total_bytes_compiled += method()-&gt;code_size();
 537 
 538   show_parse_info();
 539 #endif
 540 
 541   if (failing()) {
 542     if (log)  log-&gt;done(&quot;parse&quot;);
 543     return;
 544   }
 545 
 546   gvn().set_type(root(), root()-&gt;bottom_type());
</pre>
<hr />
<pre>
 564   Node_Notes* caller_nn = C-&gt;default_node_notes();
 565   // Collect debug info for inlined calls unless -XX:-DebugInlinedCalls.
 566   if (DebugInlinedCalls || depth() == 1) {
 567     C-&gt;set_default_node_notes(make_node_notes(caller_nn));
 568   }
 569 
 570   if (is_osr_parse()) {
 571     Node* osr_buf = entry_map-&gt;in(TypeFunc::Parms+0);
 572     entry_map-&gt;set_req(TypeFunc::Parms+0, top());
 573     set_map(entry_map);
 574     load_interpreter_state(osr_buf);
 575   } else {
 576     set_map(entry_map);
 577     do_method_entry();
 578     if (depth() == 1 &amp;&amp; C-&gt;age_code()) {
 579       decrement_age();
 580     }
 581   }
 582 
 583   if (depth() == 1 &amp;&amp; !failing()) {
<span class="line-added"> 584     if (C-&gt;clinit_barrier_on_entry()) {</span>
<span class="line-added"> 585       // Add check to deoptimize the nmethod once the holder class is fully initialized</span>
<span class="line-added"> 586       clinit_deopt();</span>
<span class="line-added"> 587     }</span>
<span class="line-added"> 588 </span>
 589     // Add check to deoptimize the nmethod if RTM state was changed
 590     rtm_deopt();
 591   }
 592 
 593   // Check for bailouts during method entry or RTM state check setup.
 594   if (failing()) {
 595     if (log)  log-&gt;done(&quot;parse&quot;);
 596     C-&gt;set_default_node_notes(caller_nn);
 597     return;
 598   }
 599 
 600   entry_map = map();  // capture any changes performed by method setup code
 601   assert(jvms()-&gt;endoff() == map()-&gt;req(), &quot;map matches JVMS layout&quot;);
 602 
 603   // We begin parsing as if we have just encountered a jump to the
 604   // method entry.
 605   Block* entry_block = start_block();
 606   assert(entry_block-&gt;start() == (is_osr_parse() ? osr_bci() : 0), &quot;&quot;);
 607   set_map_clone(entry_map);
 608   merge_common(entry_block, entry_block-&gt;next_path_num());
</pre>
<hr />
<pre>
 963 void Parse::do_exits() {
 964   set_parse_bci(InvocationEntryBci);
 965 
 966   // Now peephole on the return bits
 967   Node* region = _exits.control();
 968   _exits.set_control(gvn().transform(region));
 969 
 970   Node* iophi = _exits.i_o();
 971   _exits.set_i_o(gvn().transform(iophi));
 972 
 973   // Figure out if we need to emit the trailing barrier. The barrier is only
 974   // needed in the constructors, and only in three cases:
 975   //
 976   // 1. The constructor wrote a final. The effects of all initializations
 977   //    must be committed to memory before any code after the constructor
 978   //    publishes the reference to the newly constructed object. Rather
 979   //    than wait for the publication, we simply block the writes here.
 980   //    Rather than put a barrier on only those writes which are required
 981   //    to complete, we force all writes to complete.
 982   //
<span class="line-modified"> 983   // 2. Experimental VM option is used to force the barrier if any field</span>







 984   //    was written out in the constructor.
 985   //
<span class="line-added"> 986   // 3. On processors which are not CPU_MULTI_COPY_ATOMIC (e.g. PPC64),</span>
<span class="line-added"> 987   //    support_IRIW_for_not_multiple_copy_atomic_cpu selects that</span>
<span class="line-added"> 988   //    MemBarVolatile is used before volatile load instead of after volatile</span>
<span class="line-added"> 989   //    store, so there&#39;s no barrier after the store.</span>
<span class="line-added"> 990   //    We want to guarantee the same behavior as on platforms with total store</span>
<span class="line-added"> 991   //    order, although this is not required by the Java memory model.</span>
<span class="line-added"> 992   //    In this case, we want to enforce visibility of volatile field</span>
<span class="line-added"> 993   //    initializations which are performed in constructors.</span>
<span class="line-added"> 994   //    So as with finals, we add a barrier here.</span>
<span class="line-added"> 995   //</span>
 996   // &quot;All bets are off&quot; unless the first publication occurs after a
 997   // normal return from the constructor.  We do not attempt to detect
 998   // such unusual early publications.  But no barrier is needed on
 999   // exceptional returns, since they cannot publish normally.
1000   //
1001   if (method()-&gt;is_initializer() &amp;&amp;
<span class="line-modified">1002        (wrote_final() ||</span>
<span class="line-modified">1003          (AlwaysSafeConstructors &amp;&amp; wrote_fields()) ||</span>
<span class="line-modified">1004          (support_IRIW_for_not_multiple_copy_atomic_cpu &amp;&amp; wrote_volatile()))) {</span>
1005     _exits.insert_mem_bar(Op_MemBarRelease, alloc_with_final());
1006 
1007     // If Memory barrier is created for final fields write
1008     // and allocation node does not escape the initialize method,
1009     // then barrier introduced by allocation node can be removed.
1010     if (DoEscapeAnalysis &amp;&amp; alloc_with_final()) {
1011       AllocateNode *alloc = AllocateNode::Ideal_allocation(alloc_with_final(), &amp;_gvn);
1012       alloc-&gt;compute_MemBar_redundancy(method());
1013     }
1014     if (PrintOpto &amp;&amp; (Verbose || WizardMode)) {
1015       method()-&gt;print_name();
1016       tty-&gt;print_cr(&quot; writes finals and needs a memory barrier&quot;);
1017     }
1018   }
1019 
1020   // Any method can write a @Stable field; insert memory barriers
1021   // after those also. Can&#39;t bind predecessor allocation node (if any)
1022   // with barrier because allocation doesn&#39;t always dominate
1023   // MemBarRelease.
1024   if (wrote_stable()) {
1025     _exits.insert_mem_bar(Op_MemBarRelease);
1026     if (PrintOpto &amp;&amp; (Verbose || WizardMode)) {
1027       method()-&gt;print_name();
1028       tty-&gt;print_cr(&quot; writes @Stable and needs a memory barrier&quot;);
1029     }
1030   }
1031 
1032   for (MergeMemStream mms(_exits.merged_memory()); mms.next_non_empty(); ) {
1033     // transform each slice of the original memphi:
1034     mms.set_memory(_gvn.transform(mms.memory()));
1035   }
<span class="line-added">1036   // Clean up input MergeMems created by transforming the slices</span>
<span class="line-added">1037   _gvn.transform(_exits.merged_memory());</span>
1038 
1039   if (tf()-&gt;range()-&gt;cnt() &gt; TypeFunc::Parms) {
1040     const Type* ret_type = tf()-&gt;range()-&gt;field_at(TypeFunc::Parms);
1041     Node*       ret_phi  = _gvn.transform( _exits.argument(0) );
1042     if (!_exits.control()-&gt;is_top() &amp;&amp; _gvn.type(ret_phi)-&gt;empty()) {
<span class="line-modified">1043       // If the type we set for the ret_phi in build_exits() is too optimistic and</span>
<span class="line-modified">1044       // the ret_phi is top now, there&#39;s an extremely small chance that it may be due to class</span>
<span class="line-modified">1045       // loading.  It could also be due to an error, so mark this method as not compilable because</span>
<span class="line-modified">1046       // otherwise this could lead to an infinite compile loop.</span>
<span class="line-modified">1047       // In any case, this code path is rarely (and never in my testing) reached.</span>
<span class="line-modified">1048       C-&gt;record_method_not_compilable(&quot;Can&#39;t determine return type.&quot;);</span>







1049       return;
1050     }
1051     if (ret_type-&gt;isa_int()) {
1052       BasicType ret_bt = method()-&gt;return_type()-&gt;basic_type();
1053       ret_phi = mask_int_value(ret_phi, ret_bt, &amp;_gvn);
1054     }
1055     _exits.push_node(ret_type-&gt;basic_type(), ret_phi);
1056   }
1057 
1058   // Note:  Logic for creating and optimizing the ReturnNode is in Compile.
1059 
1060   // Unlock along the exceptional paths.
1061   // This is done late so that we can common up equivalent exceptions
1062   // (e.g., null checks) arising from multiple points within this method.
1063   // See GraphKit::add_exception_state, which performs the commoning.
1064   bool do_synch = method()-&gt;is_synchronized() &amp;&amp; GenerateSynchronizationCode;
1065 
1066   // record exit from a method if compiled while Dtrace is turned on.
1067   if (do_synch || C-&gt;env()-&gt;dtrace_method_probes() || _replaced_nodes_for_exceptions) {
1068     // First move the exception list out of _exits:
</pre>
<hr />
<pre>
1173   uint arg_size = tf()-&gt;domain()-&gt;cnt();
1174   ensure_stack(arg_size - TypeFunc::Parms);  // OSR methods have funny args
1175   for (i = TypeFunc::Parms; i &lt; arg_size; i++) {
1176     map()-&gt;init_req(i, inmap-&gt;argument(_caller, i - TypeFunc::Parms));
1177   }
1178 
1179   // Clear out the rest of the map (locals and stack)
1180   for (i = arg_size; i &lt; len; i++) {
1181     map()-&gt;init_req(i, top());
1182   }
1183 
1184   SafePointNode* entry_map = stop();
1185   return entry_map;
1186 }
1187 
1188 //-----------------------------do_method_entry--------------------------------
1189 // Emit any code needed in the pseudo-block before BCI zero.
1190 // The main thing to do is lock the receiver of a synchronized method.
1191 void Parse::do_method_entry() {
1192   set_parse_bci(InvocationEntryBci); // Pseudo-BCP
<span class="line-modified">1193   set_sp(0);                         // Java Stack Pointer</span>
1194 
1195   NOT_PRODUCT( count_compiled_calls(true/*at_method_entry*/, false/*is_inline*/); )
1196 
1197   if (C-&gt;env()-&gt;dtrace_method_probes()) {
1198     make_dtrace_method_entry(method());
1199   }
1200 
1201   // If the method is synchronized, we need to construct a lock node, attach
1202   // it to the Start node, and pin it there.
1203   if (method()-&gt;is_synchronized()) {
1204     // Insert a FastLockNode right after the Start which takes as arguments
1205     // the current thread pointer, the &quot;this&quot; pointer &amp; the address of the
1206     // stack slot pair used for the lock.  The &quot;this&quot; pointer is a projection
1207     // off the start node, but the locking spot has to be constructed by
1208     // creating a ConLNode of 0, and boxing it with a BoxLockNode.  The BoxLockNode
1209     // becomes the second argument to the FastLockNode call.  The
1210     // FastLockNode becomes the new control parent to pin it to the start.
1211 
1212     // Setup Object Pointer
1213     Node *lock_obj = NULL;
</pre>
<hr />
<pre>
2083                                    receiver);
2084     make_slow_call_ex(call, env()-&gt;Throwable_klass(), true);
2085 
2086     Node* fast_io  = call-&gt;in(TypeFunc::I_O);
2087     Node* fast_mem = call-&gt;in(TypeFunc::Memory);
2088     // These two phis are pre-filled with copies of of the fast IO and Memory
2089     Node* io_phi   = PhiNode::make(result_rgn, fast_io,  Type::ABIO);
2090     Node* mem_phi  = PhiNode::make(result_rgn, fast_mem, Type::MEMORY, TypePtr::BOTTOM);
2091 
2092     result_rgn-&gt;init_req(2, control());
2093     io_phi    -&gt;init_req(2, i_o());
2094     mem_phi   -&gt;init_req(2, reset_memory());
2095 
2096     set_all_memory( _gvn.transform(mem_phi) );
2097     set_i_o(        _gvn.transform(io_phi) );
2098   }
2099 
2100   set_control( _gvn.transform(result_rgn) );
2101 }
2102 
<span class="line-added">2103 // Add check to deoptimize once holder klass is fully initialized.</span>
<span class="line-added">2104 void Parse::clinit_deopt() {</span>
<span class="line-added">2105   assert(C-&gt;has_method(), &quot;only for normal compilations&quot;);</span>
<span class="line-added">2106   assert(depth() == 1, &quot;only for main compiled method&quot;);</span>
<span class="line-added">2107   assert(is_normal_parse(), &quot;no barrier needed on osr entry&quot;);</span>
<span class="line-added">2108   assert(!method()-&gt;holder()-&gt;is_not_initialized(), &quot;initialization should have been started&quot;);</span>
<span class="line-added">2109 </span>
<span class="line-added">2110   set_parse_bci(0);</span>
<span class="line-added">2111 </span>
<span class="line-added">2112   Node* holder = makecon(TypeKlassPtr::make(method()-&gt;holder()));</span>
<span class="line-added">2113   guard_klass_being_initialized(holder);</span>
<span class="line-added">2114 }</span>
<span class="line-added">2115 </span>
2116 // Add check to deoptimize if RTM state is not ProfileRTM
2117 void Parse::rtm_deopt() {
2118 #if INCLUDE_RTM_OPT
2119   if (C-&gt;profile_rtm()) {
<span class="line-modified">2120     assert(C-&gt;has_method(), &quot;only for normal compilations&quot;);</span>
2121     assert(!C-&gt;method()-&gt;method_data()-&gt;is_empty(), &quot;MDO is needed to record RTM state&quot;);
2122     assert(depth() == 1, &quot;generate check only for main compiled method&quot;);
2123 
2124     // Set starting bci for uncommon trap.
2125     set_parse_bci(is_osr_parse() ? osr_bci() : 0);
2126 
2127     // Load the rtm_state from the MethodData.
2128     const TypePtr* adr_type = TypeMetadataPtr::make(C-&gt;method()-&gt;method_data());
2129     Node* mdo = makecon(adr_type);
2130     int offset = MethodData::rtm_state_offset_in_bytes();
2131     Node* adr_node = basic_plus_adr(mdo, mdo, offset);
2132     Node* rtm_state = make_load(control(), adr_node, TypeInt::INT, T_INT, adr_type, MemNode::unordered);
2133 
2134     // Separate Load from Cmp by Opaque.
2135     // In expand_macro_nodes() it will be replaced either
2136     // with this load when there are locks in the code
2137     // or with ProfileRTM (cmp-&gt;in(2)) otherwise so that
2138     // the check will fold.
2139     Node* profile_state = makecon(TypeInt::make(ProfileRTM));
2140     Node* opq   = _gvn.transform( new Opaque3Node(C, rtm_state, Opaque3Node::RTM_OPT) );
</pre>
</td>
</tr>
</table>
<center><a href="parse.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="parse2.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>