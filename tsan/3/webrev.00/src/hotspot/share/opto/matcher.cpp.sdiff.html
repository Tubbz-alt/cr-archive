<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/matcher.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="macroArrayCopy.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="matcher.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/matcher.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;gc/shared/barrierSet.hpp&quot;
  27 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  28 #include &quot;memory/allocation.inline.hpp&quot;
  29 #include &quot;memory/resourceArea.hpp&quot;

  30 #include &quot;opto/ad.hpp&quot;
  31 #include &quot;opto/addnode.hpp&quot;
  32 #include &quot;opto/callnode.hpp&quot;
  33 #include &quot;opto/idealGraphPrinter.hpp&quot;
  34 #include &quot;opto/matcher.hpp&quot;
  35 #include &quot;opto/memnode.hpp&quot;
  36 #include &quot;opto/movenode.hpp&quot;
  37 #include &quot;opto/opcodes.hpp&quot;
  38 #include &quot;opto/regmask.hpp&quot;
  39 #include &quot;opto/rootnode.hpp&quot;
  40 #include &quot;opto/runtime.hpp&quot;
  41 #include &quot;opto/type.hpp&quot;
  42 #include &quot;opto/vectornode.hpp&quot;
  43 #include &quot;runtime/os.hpp&quot;
  44 #include &quot;runtime/sharedRuntime.hpp&quot;
  45 #include &quot;utilities/align.hpp&quot;
  46 
  47 OptoReg::Name OptoReg::c_frame_pointer;
  48 
  49 const RegMask *Matcher::idealreg2regmask[_last_machine_leaf];
</pre>
<hr />
<pre>
 139 }
 140 
 141 //---------------------------compute_old_SP------------------------------------
 142 OptoReg::Name Compile::compute_old_SP() {
 143   int fixed    = fixed_slots();
 144   int preserve = in_preserve_stack_slots();
 145   return OptoReg::stack2reg(align_up(fixed + preserve, (int)Matcher::stack_alignment_in_slots()));
 146 }
 147 
 148 
 149 
 150 #ifdef ASSERT
 151 void Matcher::verify_new_nodes_only(Node* xroot) {
 152   // Make sure that the new graph only references new nodes
 153   ResourceMark rm;
 154   Unique_Node_List worklist;
 155   VectorSet visited(Thread::current()-&gt;resource_area());
 156   worklist.push(xroot);
 157   while (worklist.size() &gt; 0) {
 158     Node* n = worklist.pop();
<span class="line-modified"> 159     visited &lt;&lt;= n-&gt;_idx;</span>
 160     assert(C-&gt;node_arena()-&gt;contains(n), &quot;dead node&quot;);
 161     for (uint j = 0; j &lt; n-&gt;req(); j++) {
 162       Node* in = n-&gt;in(j);
 163       if (in != NULL) {
 164         assert(C-&gt;node_arena()-&gt;contains(in), &quot;dead node&quot;);
 165         if (!visited.test(in-&gt;_idx)) {
 166           worklist.push(in);
 167         }
 168       }
 169     }
 170   }
 171 }
 172 #endif
 173 
 174 
 175 //---------------------------match---------------------------------------------
 176 void Matcher::match( ) {
 177   if( MaxLabelRootDepth &lt; 100 ) { // Too small?
 178     assert(false, &quot;invalid MaxLabelRootDepth, increase it to 100 minimum&quot;);
 179     MaxLabelRootDepth = 100;
</pre>
<hr />
<pre>
 322   Arena *old = C-&gt;node_arena()-&gt;move_contents(C-&gt;old_arena());
 323 
 324   // Save debug and profile information for nodes in old space:
 325   _old_node_note_array = C-&gt;node_note_array();
 326   if (_old_node_note_array != NULL) {
 327     C-&gt;set_node_note_array(new(C-&gt;comp_arena()) GrowableArray&lt;Node_Notes*&gt;
 328                            (C-&gt;comp_arena(), _old_node_note_array-&gt;length(),
 329                             0, NULL));
 330   }
 331 
 332   // Pre-size the new_node table to avoid the need for range checks.
 333   grow_new_node_array(C-&gt;unique());
 334 
 335   // Reset node counter so MachNodes start with _idx at 0
 336   int live_nodes = C-&gt;live_nodes();
 337   C-&gt;set_unique(0);
 338   C-&gt;reset_dead_node_list();
 339 
 340   // Recursively match trees from old space into new space.
 341   // Correct leaves of new-space Nodes; they point to old-space.
<span class="line-modified"> 342   _visited.Clear();             // Clear visit bits for xform call</span>
 343   C-&gt;set_cached_top_node(xform( C-&gt;top(), live_nodes ));
 344   if (!C-&gt;failing()) {
 345     Node* xroot =        xform( C-&gt;root(), 1 );
 346     if (xroot == NULL) {
 347       Matcher::soft_match_failure();  // recursive matching process failed
 348       C-&gt;record_method_not_compilable(&quot;instruction match failed&quot;);
 349     } else {
 350       // During matching shared constants were attached to C-&gt;root()
 351       // because xroot wasn&#39;t available yet, so transfer the uses to
 352       // the xroot.
 353       for( DUIterator_Fast jmax, j = C-&gt;root()-&gt;fast_outs(jmax); j &lt; jmax; j++ ) {
 354         Node* n = C-&gt;root()-&gt;fast_out(j);
 355         if (C-&gt;node_arena()-&gt;contains(n)) {
 356           assert(n-&gt;in(0) == C-&gt;root(), &quot;should be control user&quot;);
 357           n-&gt;set_req(0, xroot);
 358           --j;
 359           --jmax;
 360         }
 361       }
 362 
</pre>
<hr />
<pre>
 374       verify_new_nodes_only(xroot);
 375 #endif
 376     }
 377   }
 378   if (C-&gt;top() == NULL || C-&gt;root() == NULL) {
 379     C-&gt;record_method_not_compilable(&quot;graph lost&quot;); // %%% cannot happen?
 380   }
 381   if (C-&gt;failing()) {
 382     // delete old;
 383     old-&gt;destruct_contents();
 384     return;
 385   }
 386   assert( C-&gt;top(), &quot;&quot; );
 387   assert( C-&gt;root(), &quot;&quot; );
 388   validate_null_checks();
 389 
 390   // Now smoke old-space
 391   NOT_DEBUG( old-&gt;destruct_contents() );
 392 
 393   // ------------------------
<span class="line-modified"> 394   // Set up save-on-entry registers</span>
 395   Fixup_Save_On_Entry( );
<span class="line-removed"> 396 }</span>
 397 







 398 
 399 //------------------------------Fixup_Save_On_Entry----------------------------
 400 // The stated purpose of this routine is to take care of save-on-entry
 401 // registers.  However, the overall goal of the Match phase is to convert into
 402 // machine-specific instructions which have RegMasks to guide allocation.
 403 // So what this procedure really does is put a valid RegMask on each input
 404 // to the machine-specific variations of all Return, TailCall and Halt
 405 // instructions.  It also adds edgs to define the save-on-entry values (and of
 406 // course gives them a mask).
 407 
 408 static RegMask *init_input_masks( uint size, RegMask &amp;ret_adr, RegMask &amp;fp ) {
 409   RegMask *rms = NEW_RESOURCE_ARRAY( RegMask, size );
 410   // Do all the pre-defined register masks
 411   rms[TypeFunc::Control  ] = RegMask::Empty;
 412   rms[TypeFunc::I_O      ] = RegMask::Empty;
 413   rms[TypeFunc::Memory   ] = RegMask::Empty;
 414   rms[TypeFunc::ReturnAdr] = ret_adr;
 415   rms[TypeFunc::FramePtr ] = fp;
 416   return rms;
 417 }
 418 
<span class="line-modified"> 419 //---------------------------init_first_stack_mask-----------------------------</span>

 420 // Create the initial stack mask used by values spilling to the stack.
 421 // Disallow any debug info in outgoing argument areas by setting the
 422 // initial mask accordingly.
 423 void Matcher::init_first_stack_mask() {
 424 
 425   // Allocate storage for spill masks as masks for the appropriate load type.
<span class="line-modified"> 426   RegMask *rms = (RegMask*)C-&gt;comp_arena()-&gt;Amalloc_D(sizeof(RegMask) * (3*6+5));</span>





 427 
 428   idealreg2spillmask  [Op_RegN] = &amp;rms[0];
 429   idealreg2spillmask  [Op_RegI] = &amp;rms[1];
 430   idealreg2spillmask  [Op_RegL] = &amp;rms[2];
 431   idealreg2spillmask  [Op_RegF] = &amp;rms[3];
 432   idealreg2spillmask  [Op_RegD] = &amp;rms[4];
 433   idealreg2spillmask  [Op_RegP] = &amp;rms[5];
 434 
 435   idealreg2debugmask  [Op_RegN] = &amp;rms[6];
 436   idealreg2debugmask  [Op_RegI] = &amp;rms[7];
 437   idealreg2debugmask  [Op_RegL] = &amp;rms[8];
 438   idealreg2debugmask  [Op_RegF] = &amp;rms[9];
 439   idealreg2debugmask  [Op_RegD] = &amp;rms[10];
 440   idealreg2debugmask  [Op_RegP] = &amp;rms[11];
 441 
 442   idealreg2mhdebugmask[Op_RegN] = &amp;rms[12];
 443   idealreg2mhdebugmask[Op_RegI] = &amp;rms[13];
 444   idealreg2mhdebugmask[Op_RegL] = &amp;rms[14];
 445   idealreg2mhdebugmask[Op_RegF] = &amp;rms[15];
 446   idealreg2mhdebugmask[Op_RegD] = &amp;rms[16];
</pre>
<hr />
<pre>
 827 
 828   // Start at OptoReg::stack0()
 829   STACK_ONLY_mask.Clear();
 830   OptoReg::Name init = OptoReg::stack2reg(0);
 831   // STACK_ONLY_mask is all stack bits
 832   OptoReg::Name i;
 833   for (i = init; RegMask::can_represent(i); i = OptoReg::add(i,1))
 834     STACK_ONLY_mask.Insert(i);
 835   // Also set the &quot;infinite stack&quot; bit.
 836   STACK_ONLY_mask.set_AllStack();
 837 
 838   // Copy the register names over into the shared world
 839   for( i=OptoReg::Name(0); i&lt;OptoReg::Name(_last_Mach_Reg); i = OptoReg::add(i,1) ) {
 840     // SharedInfo::regName[i] = regName[i];
 841     // Handy RegMasks per machine register
 842     mreg2regmask[i].Insert(i);
 843   }
 844 
 845   // Grab the Frame Pointer
 846   Node *fp  = ret-&gt;in(TypeFunc::FramePtr);
<span class="line-removed"> 847   Node *mem = ret-&gt;in(TypeFunc::Memory);</span>
<span class="line-removed"> 848   const TypePtr* atp = TypePtr::BOTTOM;</span>
 849   // Share frame pointer while making spill ops
 850   set_shared(fp);
 851 
<span class="line-modified"> 852   // Compute generic short-offset Loads</span>
<span class="line-removed"> 853 #ifdef _LP64</span>
<span class="line-removed"> 854   MachNode *spillCP = match_tree(new LoadNNode(NULL,mem,fp,atp,TypeInstPtr::BOTTOM,MemNode::unordered));</span>
<span class="line-removed"> 855 #endif</span>
<span class="line-removed"> 856   MachNode *spillI  = match_tree(new LoadINode(NULL,mem,fp,atp,TypeInt::INT,MemNode::unordered));</span>
<span class="line-removed"> 857   MachNode *spillL  = match_tree(new LoadLNode(NULL,mem,fp,atp,TypeLong::LONG,MemNode::unordered, LoadNode::DependsOnlyOnTest, false));</span>
<span class="line-removed"> 858   MachNode *spillF  = match_tree(new LoadFNode(NULL,mem,fp,atp,Type::FLOAT,MemNode::unordered));</span>
<span class="line-removed"> 859   MachNode *spillD  = match_tree(new LoadDNode(NULL,mem,fp,atp,Type::DOUBLE,MemNode::unordered));</span>
<span class="line-removed"> 860   MachNode *spillP  = match_tree(new LoadPNode(NULL,mem,fp,atp,TypeInstPtr::BOTTOM,MemNode::unordered));</span>
<span class="line-removed"> 861   assert(spillI != NULL &amp;&amp; spillL != NULL &amp;&amp; spillF != NULL &amp;&amp;</span>
<span class="line-removed"> 862          spillD != NULL &amp;&amp; spillP != NULL, &quot;&quot;);</span>
<span class="line-removed"> 863   // Get the ADLC notion of the right regmask, for each basic type.</span>
 864 #ifdef _LP64
<span class="line-modified"> 865   idealreg2regmask[Op_RegN] = &amp;spillCP-&gt;out_RegMask();</span>
 866 #endif
<span class="line-modified"> 867   idealreg2regmask[Op_RegI] = &amp;spillI-&gt;out_RegMask();</span>
<span class="line-modified"> 868   idealreg2regmask[Op_RegL] = &amp;spillL-&gt;out_RegMask();</span>
<span class="line-modified"> 869   idealreg2regmask[Op_RegF] = &amp;spillF-&gt;out_RegMask();</span>
<span class="line-modified"> 870   idealreg2regmask[Op_RegD] = &amp;spillD-&gt;out_RegMask();</span>
<span class="line-modified"> 871   idealreg2regmask[Op_RegP] = &amp;spillP-&gt;out_RegMask();</span>
<span class="line-modified"> 872 </span>
<span class="line-modified"> 873   // Vector regmasks.</span>
<span class="line-modified"> 874   if (Matcher::vector_size_supported(T_BYTE,4)) {</span>
<span class="line-modified"> 875     TypeVect::VECTS = TypeVect::make(T_BYTE, 4);</span>
<span class="line-modified"> 876     MachNode *spillVectS = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTS));</span>
<span class="line-removed"> 877     idealreg2regmask[Op_VecS] = &amp;spillVectS-&gt;out_RegMask();</span>
<span class="line-removed"> 878   }</span>
<span class="line-removed"> 879   if (Matcher::vector_size_supported(T_FLOAT,2)) {</span>
<span class="line-removed"> 880     MachNode *spillVectD = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTD));</span>
<span class="line-removed"> 881     idealreg2regmask[Op_VecD] = &amp;spillVectD-&gt;out_RegMask();</span>
<span class="line-removed"> 882   }</span>
<span class="line-removed"> 883   if (Matcher::vector_size_supported(T_FLOAT,4)) {</span>
<span class="line-removed"> 884     MachNode *spillVectX = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTX));</span>
<span class="line-removed"> 885     idealreg2regmask[Op_VecX] = &amp;spillVectX-&gt;out_RegMask();</span>
<span class="line-removed"> 886   }</span>
<span class="line-removed"> 887   if (Matcher::vector_size_supported(T_FLOAT,8)) {</span>
<span class="line-removed"> 888     MachNode *spillVectY = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTY));</span>
<span class="line-removed"> 889     idealreg2regmask[Op_VecY] = &amp;spillVectY-&gt;out_RegMask();</span>
<span class="line-removed"> 890   }</span>
<span class="line-removed"> 891   if (Matcher::vector_size_supported(T_FLOAT,16)) {</span>
<span class="line-removed"> 892     MachNode *spillVectZ = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTZ));</span>
<span class="line-removed"> 893     idealreg2regmask[Op_VecZ] = &amp;spillVectZ-&gt;out_RegMask();</span>
<span class="line-removed"> 894   }</span>
 895 }
 896 
 897 #ifdef ASSERT
 898 static void match_alias_type(Compile* C, Node* n, Node* m) {
 899   if (!VerifyAliases)  return;  // do not go looking for trouble by default
 900   const TypePtr* nat = n-&gt;adr_type();
 901   const TypePtr* mat = m-&gt;adr_type();
 902   int nidx = C-&gt;get_alias_index(nat);
 903   int midx = C-&gt;get_alias_index(mat);
 904   // Detune the assert for cases like (AndI 0xFF (LoadB p)).
 905   if (nidx == Compile::AliasIdxTop &amp;&amp; midx &gt;= Compile::AliasIdxRaw) {
 906     for (uint i = 1; i &lt; n-&gt;req(); i++) {
 907       Node* n1 = n-&gt;in(i);
 908       const TypePtr* n1at = n1-&gt;adr_type();
 909       if (n1at != NULL) {
 910         nat = n1at;
 911         nidx = C-&gt;get_alias_index(n1at);
 912       }
 913     }
 914   }
</pre>
<hr />
<pre>
1727     _new2old_map.map(ex-&gt;_idx, s-&gt;_leaf);
1728 #endif
1729   }
1730 
1731   // PhaseChaitin::fixup_spills will sometimes generate spill code
1732   // via the matcher.  By the time, nodes have been wired into the CFG,
1733   // and any further nodes generated by expand rules will be left hanging
1734   // in space, and will not get emitted as output code.  Catch this.
1735   // Also, catch any new register allocation constraints (&quot;projections&quot;)
1736   // generated belatedly during spill code generation.
1737   if (_allocation_started) {
1738     guarantee(ex == mach, &quot;no expand rules during spill generation&quot;);
1739     guarantee(number_of_projections_prior == number_of_projections(), &quot;no allocation during spill generation&quot;);
1740   }
1741 
1742   if (leaf-&gt;is_Con() || leaf-&gt;is_DecodeNarrowPtr()) {
1743     // Record the con for sharing
1744     _shared_nodes.map(leaf-&gt;_idx, ex);
1745   }
1746 







1747   return ex;
1748 }
1749 
1750 void Matcher::handle_precedence_edges(Node* n, MachNode *mach) {
1751   for (uint i = n-&gt;req(); i &lt; n-&gt;len(); i++) {
1752     if (n-&gt;in(i) != NULL) {
1753       mach-&gt;add_prec(n-&gt;in(i));
1754     }
1755   }
1756 }
1757 
1758 void Matcher::ReduceInst_Chain_Rule( State *s, int rule, Node *&amp;mem, MachNode *mach ) {
1759   // &#39;op&#39; is what I am expecting to receive
1760   int op = _leftOp[rule];
1761   // Operand type to catch childs result
1762   // This is what my child will give me.
1763   int opnd_class_instance = s-&gt;_rule[op];
1764   // Choose between operand class or not.
1765   // This is what I will receive.
1766   int catch_op = (FIRST_OPERAND_CLASS &lt;= op &amp;&amp; op &lt; NUM_OPERANDS) ? opnd_class_instance : op;
</pre>
<hr />
<pre>
2004 
2005 
2006 bool Matcher::is_bmi_pattern(Node *n, Node *m) {
2007   if (n != NULL &amp;&amp; m != NULL) {
2008     if (m-&gt;Opcode() == Op_LoadI) {
2009       FusedPatternMatcher&lt;TypeInt&gt; bmii(n, m, Op_ConI);
2010       return bmii.match(Op_AndI, -1, Op_SubI,  1,  0)  ||
2011              bmii.match(Op_AndI, -1, Op_AddI, -1, -1)  ||
2012              bmii.match(Op_XorI, -1, Op_AddI, -1, -1);
2013     } else if (m-&gt;Opcode() == Op_LoadL) {
2014       FusedPatternMatcher&lt;TypeLong&gt; bmil(n, m, Op_ConL);
2015       return bmil.match(Op_AndL, -1, Op_SubL,  1,  0) ||
2016              bmil.match(Op_AndL, -1, Op_AddL, -1, -1) ||
2017              bmil.match(Op_XorL, -1, Op_AddL, -1, -1);
2018     }
2019   }
2020   return false;
2021 }
2022 #endif // X86
2023 









2024 bool Matcher::clone_base_plus_offset_address(AddPNode* m, Matcher::MStack&amp; mstack, VectorSet&amp; address_visited) {
2025   Node *off = m-&gt;in(AddPNode::Offset);
2026   if (off-&gt;is_Con()) {
2027     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
2028     mstack.push(m-&gt;in(AddPNode::Address), Pre_Visit);
2029     // Clone X+offset as it also folds into most addressing expressions
2030     mstack.push(off, Visit);
2031     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
2032     return true;
2033   }
2034   return false;
2035 }
2036 
2037 // A method-klass-holder may be passed in the inline_cache_reg
2038 // and then expanded into the inline_cache_reg and a method_oop register
2039 //   defined in ad_&lt;arch&gt;.cpp
2040 
2041 //------------------------------find_shared------------------------------------
2042 // Set bits if Node is shared or otherwise a root
2043 void Matcher::find_shared( Node *n ) {
2044   // Allocate stack of size C-&gt;live_nodes() * 2 to avoid frequent realloc
2045   MStack mstack(C-&gt;live_nodes() * 2);
2046   // Mark nodes as address_visited if they are inputs to an address expression
2047   VectorSet address_visited(Thread::current()-&gt;resource_area());
2048   mstack.push(n, Visit);     // Don&#39;t need to pre-visit root node
2049   while (mstack.is_nonempty()) {
2050     n = mstack.node();       // Leave node on stack
2051     Node_State nstate = mstack.state();
2052     uint nop = n-&gt;Opcode();
2053     if (nstate == Pre_Visit) {
2054       if (address_visited.test(n-&gt;_idx)) { // Visited in address already?
2055         // Flag as visited and shared now.
2056         set_visited(n);
2057       }
2058       if (is_visited(n)) {   // Visited already?
2059         // Node is shared and has no reason to clone.  Flag it as shared.
2060         // This causes it to match into a register for the sharing.
2061         set_shared(n);       // Flag as shared and






2062         mstack.pop();        // remove node from stack
2063         continue;
2064       }
2065       nstate = Visit; // Not already visited; so visit now
2066     }
2067     if (nstate == Visit) {
2068       mstack.set_state(Post_Visit);
2069       set_visited(n);   // Flag as visited now
2070       bool mem_op = false;
2071       int mem_addr_idx = MemNode::Address;
<span class="line-modified">2072       bool gc_handled = BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;matcher_find_shared_visit(this, mstack, n, nop, mem_op, mem_addr_idx);</span>
<span class="line-modified">2073       if (!gc_handled) {</span>
<span class="line-removed">2074         if (find_shared_visit(mstack, n, nop, mem_op, mem_addr_idx)) {</span>
<span class="line-removed">2075           continue;</span>
<span class="line-removed">2076         }</span>
2077       }
2078       for(int i = n-&gt;req() - 1; i &gt;= 0; --i) { // For my children
2079         Node *m = n-&gt;in(i); // Get ith input
2080         if (m == NULL) continue;  // Ignore NULLs
2081         uint mop = m-&gt;Opcode();
2082 
2083         // Must clone all producers of flags, or we will not match correctly.
2084         // Suppose a compare setting int-flags is shared (e.g., a switch-tree)
2085         // then it will match into an ideal Op_RegFlags.  Alas, the fp-flags
2086         // are also there, so we may match a float-branch to int-flags and
2087         // expect the allocator to haul the flags from the int-side to the
2088         // fp-side.  No can do.
2089         if( _must_clone[mop] ) {
2090           mstack.push(m, Visit);
2091           continue; // for(int i = ...)
2092         }
2093 
<span class="line-removed">2094         if( mop == Op_AddP &amp;&amp; m-&gt;in(AddPNode::Base)-&gt;is_DecodeNarrowPtr()) {</span>
<span class="line-removed">2095           // Bases used in addresses must be shared but since</span>
<span class="line-removed">2096           // they are shared through a DecodeN they may appear</span>
<span class="line-removed">2097           // to have a single use so force sharing here.</span>
<span class="line-removed">2098           set_shared(m-&gt;in(AddPNode::Base)-&gt;in(1));</span>
<span class="line-removed">2099         }</span>
<span class="line-removed">2100 </span>
2101         // if &#39;n&#39; and &#39;m&#39; are part of a graph for BMI instruction, clone this node.
2102 #ifdef X86
2103         if (UseBMI1Instructions &amp;&amp; is_bmi_pattern(n, m)) {
2104           mstack.push(m, Visit);
2105           continue;
2106         }
2107 #endif




2108 
2109         // Clone addressing expressions as they are &quot;free&quot; in memory access instructions
2110         if (mem_op &amp;&amp; i == mem_addr_idx &amp;&amp; mop == Op_AddP &amp;&amp;
2111             // When there are other uses besides address expressions
2112             // put it on stack and mark as shared.
2113             !is_visited(m)) {
2114           // Some inputs for address expression are not put on stack
2115           // to avoid marking them as shared and forcing them into register
2116           // if they are used only in address expressions.
2117           // But they should be marked as shared if there are other uses
2118           // besides address expressions.
2119 
2120           if (clone_address_expressions(m-&gt;as_AddP(), mstack, address_visited)) {
2121             continue;
2122           }
2123         }   // if( mem_op &amp;&amp;
2124         mstack.push(m, Pre_Visit);
2125       }     // for(int i = ...)
2126     }
2127     else if (nstate == Alt_Post_Visit) {
</pre>
<hr />
<pre>
2459         // Note: new_val may have a control edge if
2460         // the original ideal node DecodeN was matched before
2461         // it was unpinned in Matcher::collect_null_checks().
2462         // Unpin the mach node and mark it.
2463         new_val-&gt;set_req(0, NULL);
2464         new_val = (Node*)(((intptr_t)new_val) | 1);
2465       }
2466       // Is a match-tree root, so replace with the matched value
2467       _null_check_tests.map(i+1, new_val);
2468     } else {
2469       // Yank from candidate list
2470       _null_check_tests.map(i+1,_null_check_tests[--cnt]);
2471       _null_check_tests.map(i,_null_check_tests[--cnt]);
2472       _null_check_tests.pop();
2473       _null_check_tests.pop();
2474       i-=2;
2475     }
2476   }
2477 }
2478 












































































































































































2479 // Used by the DFA in dfa_xxx.cpp.  Check for a following barrier or
2480 // atomic instruction acting as a store_load barrier without any
2481 // intervening volatile load, and thus we don&#39;t need a barrier here.
2482 // We retain the Node to act as a compiler ordering barrier.
2483 bool Matcher::post_store_load_barrier(const Node* vmb) {
2484   Compile* C = Compile::current();
2485   assert(vmb-&gt;is_MemBar(), &quot;&quot;);
2486   assert(vmb-&gt;Opcode() != Op_MemBarAcquire &amp;&amp; vmb-&gt;Opcode() != Op_LoadFence, &quot;&quot;);
2487   const MemBarNode* membar = vmb-&gt;as_MemBar();
2488 
2489   // Get the Ideal Proj node, ctrl, that can be used to iterate forward
2490   Node* ctrl = NULL;
2491   for (DUIterator_Fast imax, i = membar-&gt;fast_outs(imax); i &lt; imax; i++) {
2492     Node* p = membar-&gt;fast_out(i);
2493     assert(p-&gt;is_Proj(), &quot;only projections here&quot;);
2494     if ((p-&gt;as_Proj()-&gt;_con == TypeFunc::Control) &amp;&amp;
2495         !C-&gt;node_arena()-&gt;contains(p)) { // Unmatched old-space only
2496       ctrl = p;
2497       break;
2498     }
</pre>
</td>
<td>
<hr />
<pre>
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;gc/shared/barrierSet.hpp&quot;
  27 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  28 #include &quot;memory/allocation.inline.hpp&quot;
  29 #include &quot;memory/resourceArea.hpp&quot;
<span class="line-added">  30 #include &quot;oops/compressedOops.hpp&quot;</span>
  31 #include &quot;opto/ad.hpp&quot;
  32 #include &quot;opto/addnode.hpp&quot;
  33 #include &quot;opto/callnode.hpp&quot;
  34 #include &quot;opto/idealGraphPrinter.hpp&quot;
  35 #include &quot;opto/matcher.hpp&quot;
  36 #include &quot;opto/memnode.hpp&quot;
  37 #include &quot;opto/movenode.hpp&quot;
  38 #include &quot;opto/opcodes.hpp&quot;
  39 #include &quot;opto/regmask.hpp&quot;
  40 #include &quot;opto/rootnode.hpp&quot;
  41 #include &quot;opto/runtime.hpp&quot;
  42 #include &quot;opto/type.hpp&quot;
  43 #include &quot;opto/vectornode.hpp&quot;
  44 #include &quot;runtime/os.hpp&quot;
  45 #include &quot;runtime/sharedRuntime.hpp&quot;
  46 #include &quot;utilities/align.hpp&quot;
  47 
  48 OptoReg::Name OptoReg::c_frame_pointer;
  49 
  50 const RegMask *Matcher::idealreg2regmask[_last_machine_leaf];
</pre>
<hr />
<pre>
 140 }
 141 
 142 //---------------------------compute_old_SP------------------------------------
 143 OptoReg::Name Compile::compute_old_SP() {
 144   int fixed    = fixed_slots();
 145   int preserve = in_preserve_stack_slots();
 146   return OptoReg::stack2reg(align_up(fixed + preserve, (int)Matcher::stack_alignment_in_slots()));
 147 }
 148 
 149 
 150 
 151 #ifdef ASSERT
 152 void Matcher::verify_new_nodes_only(Node* xroot) {
 153   // Make sure that the new graph only references new nodes
 154   ResourceMark rm;
 155   Unique_Node_List worklist;
 156   VectorSet visited(Thread::current()-&gt;resource_area());
 157   worklist.push(xroot);
 158   while (worklist.size() &gt; 0) {
 159     Node* n = worklist.pop();
<span class="line-modified"> 160     visited.set(n-&gt;_idx);</span>
 161     assert(C-&gt;node_arena()-&gt;contains(n), &quot;dead node&quot;);
 162     for (uint j = 0; j &lt; n-&gt;req(); j++) {
 163       Node* in = n-&gt;in(j);
 164       if (in != NULL) {
 165         assert(C-&gt;node_arena()-&gt;contains(in), &quot;dead node&quot;);
 166         if (!visited.test(in-&gt;_idx)) {
 167           worklist.push(in);
 168         }
 169       }
 170     }
 171   }
 172 }
 173 #endif
 174 
 175 
 176 //---------------------------match---------------------------------------------
 177 void Matcher::match( ) {
 178   if( MaxLabelRootDepth &lt; 100 ) { // Too small?
 179     assert(false, &quot;invalid MaxLabelRootDepth, increase it to 100 minimum&quot;);
 180     MaxLabelRootDepth = 100;
</pre>
<hr />
<pre>
 323   Arena *old = C-&gt;node_arena()-&gt;move_contents(C-&gt;old_arena());
 324 
 325   // Save debug and profile information for nodes in old space:
 326   _old_node_note_array = C-&gt;node_note_array();
 327   if (_old_node_note_array != NULL) {
 328     C-&gt;set_node_note_array(new(C-&gt;comp_arena()) GrowableArray&lt;Node_Notes*&gt;
 329                            (C-&gt;comp_arena(), _old_node_note_array-&gt;length(),
 330                             0, NULL));
 331   }
 332 
 333   // Pre-size the new_node table to avoid the need for range checks.
 334   grow_new_node_array(C-&gt;unique());
 335 
 336   // Reset node counter so MachNodes start with _idx at 0
 337   int live_nodes = C-&gt;live_nodes();
 338   C-&gt;set_unique(0);
 339   C-&gt;reset_dead_node_list();
 340 
 341   // Recursively match trees from old space into new space.
 342   // Correct leaves of new-space Nodes; they point to old-space.
<span class="line-modified"> 343   _visited.clear();</span>
 344   C-&gt;set_cached_top_node(xform( C-&gt;top(), live_nodes ));
 345   if (!C-&gt;failing()) {
 346     Node* xroot =        xform( C-&gt;root(), 1 );
 347     if (xroot == NULL) {
 348       Matcher::soft_match_failure();  // recursive matching process failed
 349       C-&gt;record_method_not_compilable(&quot;instruction match failed&quot;);
 350     } else {
 351       // During matching shared constants were attached to C-&gt;root()
 352       // because xroot wasn&#39;t available yet, so transfer the uses to
 353       // the xroot.
 354       for( DUIterator_Fast jmax, j = C-&gt;root()-&gt;fast_outs(jmax); j &lt; jmax; j++ ) {
 355         Node* n = C-&gt;root()-&gt;fast_out(j);
 356         if (C-&gt;node_arena()-&gt;contains(n)) {
 357           assert(n-&gt;in(0) == C-&gt;root(), &quot;should be control user&quot;);
 358           n-&gt;set_req(0, xroot);
 359           --j;
 360           --jmax;
 361         }
 362       }
 363 
</pre>
<hr />
<pre>
 375       verify_new_nodes_only(xroot);
 376 #endif
 377     }
 378   }
 379   if (C-&gt;top() == NULL || C-&gt;root() == NULL) {
 380     C-&gt;record_method_not_compilable(&quot;graph lost&quot;); // %%% cannot happen?
 381   }
 382   if (C-&gt;failing()) {
 383     // delete old;
 384     old-&gt;destruct_contents();
 385     return;
 386   }
 387   assert( C-&gt;top(), &quot;&quot; );
 388   assert( C-&gt;root(), &quot;&quot; );
 389   validate_null_checks();
 390 
 391   // Now smoke old-space
 392   NOT_DEBUG( old-&gt;destruct_contents() );
 393 
 394   // ------------------------
<span class="line-modified"> 395   // Set up save-on-entry registers.</span>
 396   Fixup_Save_On_Entry( );

 397 
<span class="line-added"> 398   { // Cleanup mach IR after selection phase is over.</span>
<span class="line-added"> 399     Compile::TracePhase tp(&quot;postselect_cleanup&quot;, &amp;timers[_t_postselect_cleanup]);</span>
<span class="line-added"> 400     do_postselect_cleanup();</span>
<span class="line-added"> 401     if (C-&gt;failing())  return;</span>
<span class="line-added"> 402     assert(verify_after_postselect_cleanup(), &quot;&quot;);</span>
<span class="line-added"> 403   }</span>
<span class="line-added"> 404 }</span>
 405 
 406 //------------------------------Fixup_Save_On_Entry----------------------------
 407 // The stated purpose of this routine is to take care of save-on-entry
 408 // registers.  However, the overall goal of the Match phase is to convert into
 409 // machine-specific instructions which have RegMasks to guide allocation.
 410 // So what this procedure really does is put a valid RegMask on each input
 411 // to the machine-specific variations of all Return, TailCall and Halt
 412 // instructions.  It also adds edgs to define the save-on-entry values (and of
 413 // course gives them a mask).
 414 
 415 static RegMask *init_input_masks( uint size, RegMask &amp;ret_adr, RegMask &amp;fp ) {
 416   RegMask *rms = NEW_RESOURCE_ARRAY( RegMask, size );
 417   // Do all the pre-defined register masks
 418   rms[TypeFunc::Control  ] = RegMask::Empty;
 419   rms[TypeFunc::I_O      ] = RegMask::Empty;
 420   rms[TypeFunc::Memory   ] = RegMask::Empty;
 421   rms[TypeFunc::ReturnAdr] = ret_adr;
 422   rms[TypeFunc::FramePtr ] = fp;
 423   return rms;
 424 }
 425 
<span class="line-modified"> 426 #define NOF_STACK_MASKS (3*6+5)</span>
<span class="line-added"> 427 </span>
 428 // Create the initial stack mask used by values spilling to the stack.
 429 // Disallow any debug info in outgoing argument areas by setting the
 430 // initial mask accordingly.
 431 void Matcher::init_first_stack_mask() {
 432 
 433   // Allocate storage for spill masks as masks for the appropriate load type.
<span class="line-modified"> 434   RegMask *rms = (RegMask*)C-&gt;comp_arena()-&gt;Amalloc_D(sizeof(RegMask) * NOF_STACK_MASKS);</span>
<span class="line-added"> 435 </span>
<span class="line-added"> 436   // Initialize empty placeholder masks into the newly allocated arena</span>
<span class="line-added"> 437   for (int i = 0; i &lt; NOF_STACK_MASKS; i++) {</span>
<span class="line-added"> 438     new (rms + i) RegMask();</span>
<span class="line-added"> 439   }</span>
 440 
 441   idealreg2spillmask  [Op_RegN] = &amp;rms[0];
 442   idealreg2spillmask  [Op_RegI] = &amp;rms[1];
 443   idealreg2spillmask  [Op_RegL] = &amp;rms[2];
 444   idealreg2spillmask  [Op_RegF] = &amp;rms[3];
 445   idealreg2spillmask  [Op_RegD] = &amp;rms[4];
 446   idealreg2spillmask  [Op_RegP] = &amp;rms[5];
 447 
 448   idealreg2debugmask  [Op_RegN] = &amp;rms[6];
 449   idealreg2debugmask  [Op_RegI] = &amp;rms[7];
 450   idealreg2debugmask  [Op_RegL] = &amp;rms[8];
 451   idealreg2debugmask  [Op_RegF] = &amp;rms[9];
 452   idealreg2debugmask  [Op_RegD] = &amp;rms[10];
 453   idealreg2debugmask  [Op_RegP] = &amp;rms[11];
 454 
 455   idealreg2mhdebugmask[Op_RegN] = &amp;rms[12];
 456   idealreg2mhdebugmask[Op_RegI] = &amp;rms[13];
 457   idealreg2mhdebugmask[Op_RegL] = &amp;rms[14];
 458   idealreg2mhdebugmask[Op_RegF] = &amp;rms[15];
 459   idealreg2mhdebugmask[Op_RegD] = &amp;rms[16];
</pre>
<hr />
<pre>
 840 
 841   // Start at OptoReg::stack0()
 842   STACK_ONLY_mask.Clear();
 843   OptoReg::Name init = OptoReg::stack2reg(0);
 844   // STACK_ONLY_mask is all stack bits
 845   OptoReg::Name i;
 846   for (i = init; RegMask::can_represent(i); i = OptoReg::add(i,1))
 847     STACK_ONLY_mask.Insert(i);
 848   // Also set the &quot;infinite stack&quot; bit.
 849   STACK_ONLY_mask.set_AllStack();
 850 
 851   // Copy the register names over into the shared world
 852   for( i=OptoReg::Name(0); i&lt;OptoReg::Name(_last_Mach_Reg); i = OptoReg::add(i,1) ) {
 853     // SharedInfo::regName[i] = regName[i];
 854     // Handy RegMasks per machine register
 855     mreg2regmask[i].Insert(i);
 856   }
 857 
 858   // Grab the Frame Pointer
 859   Node *fp  = ret-&gt;in(TypeFunc::FramePtr);


 860   // Share frame pointer while making spill ops
 861   set_shared(fp);
 862 
<span class="line-modified"> 863 // Get the ADLC notion of the right regmask, for each basic type.</span>











 864 #ifdef _LP64
<span class="line-modified"> 865   idealreg2regmask[Op_RegN] = regmask_for_ideal_register(Op_RegN, ret);</span>
 866 #endif
<span class="line-modified"> 867   idealreg2regmask[Op_RegI] = regmask_for_ideal_register(Op_RegI, ret);</span>
<span class="line-modified"> 868   idealreg2regmask[Op_RegP] = regmask_for_ideal_register(Op_RegP, ret);</span>
<span class="line-modified"> 869   idealreg2regmask[Op_RegF] = regmask_for_ideal_register(Op_RegF, ret);</span>
<span class="line-modified"> 870   idealreg2regmask[Op_RegD] = regmask_for_ideal_register(Op_RegD, ret);</span>
<span class="line-modified"> 871   idealreg2regmask[Op_RegL] = regmask_for_ideal_register(Op_RegL, ret);</span>
<span class="line-modified"> 872   idealreg2regmask[Op_VecS] = regmask_for_ideal_register(Op_VecS, ret);</span>
<span class="line-modified"> 873   idealreg2regmask[Op_VecD] = regmask_for_ideal_register(Op_VecD, ret);</span>
<span class="line-modified"> 874   idealreg2regmask[Op_VecX] = regmask_for_ideal_register(Op_VecX, ret);</span>
<span class="line-modified"> 875   idealreg2regmask[Op_VecY] = regmask_for_ideal_register(Op_VecY, ret);</span>
<span class="line-modified"> 876   idealreg2regmask[Op_VecZ] = regmask_for_ideal_register(Op_VecZ, ret);</span>


















 877 }
 878 
 879 #ifdef ASSERT
 880 static void match_alias_type(Compile* C, Node* n, Node* m) {
 881   if (!VerifyAliases)  return;  // do not go looking for trouble by default
 882   const TypePtr* nat = n-&gt;adr_type();
 883   const TypePtr* mat = m-&gt;adr_type();
 884   int nidx = C-&gt;get_alias_index(nat);
 885   int midx = C-&gt;get_alias_index(mat);
 886   // Detune the assert for cases like (AndI 0xFF (LoadB p)).
 887   if (nidx == Compile::AliasIdxTop &amp;&amp; midx &gt;= Compile::AliasIdxRaw) {
 888     for (uint i = 1; i &lt; n-&gt;req(); i++) {
 889       Node* n1 = n-&gt;in(i);
 890       const TypePtr* n1at = n1-&gt;adr_type();
 891       if (n1at != NULL) {
 892         nat = n1at;
 893         nidx = C-&gt;get_alias_index(n1at);
 894       }
 895     }
 896   }
</pre>
<hr />
<pre>
1709     _new2old_map.map(ex-&gt;_idx, s-&gt;_leaf);
1710 #endif
1711   }
1712 
1713   // PhaseChaitin::fixup_spills will sometimes generate spill code
1714   // via the matcher.  By the time, nodes have been wired into the CFG,
1715   // and any further nodes generated by expand rules will be left hanging
1716   // in space, and will not get emitted as output code.  Catch this.
1717   // Also, catch any new register allocation constraints (&quot;projections&quot;)
1718   // generated belatedly during spill code generation.
1719   if (_allocation_started) {
1720     guarantee(ex == mach, &quot;no expand rules during spill generation&quot;);
1721     guarantee(number_of_projections_prior == number_of_projections(), &quot;no allocation during spill generation&quot;);
1722   }
1723 
1724   if (leaf-&gt;is_Con() || leaf-&gt;is_DecodeNarrowPtr()) {
1725     // Record the con for sharing
1726     _shared_nodes.map(leaf-&gt;_idx, ex);
1727   }
1728 
<span class="line-added">1729   // Have mach nodes inherit GC barrier data</span>
<span class="line-added">1730   if (leaf-&gt;is_LoadStore()) {</span>
<span class="line-added">1731     mach-&gt;set_barrier_data(leaf-&gt;as_LoadStore()-&gt;barrier_data());</span>
<span class="line-added">1732   } else if (leaf-&gt;is_Mem()) {</span>
<span class="line-added">1733     mach-&gt;set_barrier_data(leaf-&gt;as_Mem()-&gt;barrier_data());</span>
<span class="line-added">1734   }</span>
<span class="line-added">1735 </span>
1736   return ex;
1737 }
1738 
1739 void Matcher::handle_precedence_edges(Node* n, MachNode *mach) {
1740   for (uint i = n-&gt;req(); i &lt; n-&gt;len(); i++) {
1741     if (n-&gt;in(i) != NULL) {
1742       mach-&gt;add_prec(n-&gt;in(i));
1743     }
1744   }
1745 }
1746 
1747 void Matcher::ReduceInst_Chain_Rule( State *s, int rule, Node *&amp;mem, MachNode *mach ) {
1748   // &#39;op&#39; is what I am expecting to receive
1749   int op = _leftOp[rule];
1750   // Operand type to catch childs result
1751   // This is what my child will give me.
1752   int opnd_class_instance = s-&gt;_rule[op];
1753   // Choose between operand class or not.
1754   // This is what I will receive.
1755   int catch_op = (FIRST_OPERAND_CLASS &lt;= op &amp;&amp; op &lt; NUM_OPERANDS) ? opnd_class_instance : op;
</pre>
<hr />
<pre>
1993 
1994 
1995 bool Matcher::is_bmi_pattern(Node *n, Node *m) {
1996   if (n != NULL &amp;&amp; m != NULL) {
1997     if (m-&gt;Opcode() == Op_LoadI) {
1998       FusedPatternMatcher&lt;TypeInt&gt; bmii(n, m, Op_ConI);
1999       return bmii.match(Op_AndI, -1, Op_SubI,  1,  0)  ||
2000              bmii.match(Op_AndI, -1, Op_AddI, -1, -1)  ||
2001              bmii.match(Op_XorI, -1, Op_AddI, -1, -1);
2002     } else if (m-&gt;Opcode() == Op_LoadL) {
2003       FusedPatternMatcher&lt;TypeLong&gt; bmil(n, m, Op_ConL);
2004       return bmil.match(Op_AndL, -1, Op_SubL,  1,  0) ||
2005              bmil.match(Op_AndL, -1, Op_AddL, -1, -1) ||
2006              bmil.match(Op_XorL, -1, Op_AddL, -1, -1);
2007     }
2008   }
2009   return false;
2010 }
2011 #endif // X86
2012 
<span class="line-added">2013 bool Matcher::is_vshift_con_pattern(Node *n, Node *m) {</span>
<span class="line-added">2014   if (n != NULL &amp;&amp; m != NULL) {</span>
<span class="line-added">2015     return VectorNode::is_vector_shift(n) &amp;&amp;</span>
<span class="line-added">2016            VectorNode::is_vector_shift_count(m) &amp;&amp; m-&gt;in(1)-&gt;is_Con();</span>
<span class="line-added">2017   }</span>
<span class="line-added">2018   return false;</span>
<span class="line-added">2019 }</span>
<span class="line-added">2020 </span>
<span class="line-added">2021 </span>
2022 bool Matcher::clone_base_plus_offset_address(AddPNode* m, Matcher::MStack&amp; mstack, VectorSet&amp; address_visited) {
2023   Node *off = m-&gt;in(AddPNode::Offset);
2024   if (off-&gt;is_Con()) {
2025     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
2026     mstack.push(m-&gt;in(AddPNode::Address), Pre_Visit);
2027     // Clone X+offset as it also folds into most addressing expressions
2028     mstack.push(off, Visit);
2029     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
2030     return true;
2031   }
2032   return false;
2033 }
2034 
2035 // A method-klass-holder may be passed in the inline_cache_reg
2036 // and then expanded into the inline_cache_reg and a method_oop register
2037 //   defined in ad_&lt;arch&gt;.cpp
2038 
2039 //------------------------------find_shared------------------------------------
2040 // Set bits if Node is shared or otherwise a root
2041 void Matcher::find_shared( Node *n ) {
2042   // Allocate stack of size C-&gt;live_nodes() * 2 to avoid frequent realloc
2043   MStack mstack(C-&gt;live_nodes() * 2);
2044   // Mark nodes as address_visited if they are inputs to an address expression
2045   VectorSet address_visited(Thread::current()-&gt;resource_area());
2046   mstack.push(n, Visit);     // Don&#39;t need to pre-visit root node
2047   while (mstack.is_nonempty()) {
2048     n = mstack.node();       // Leave node on stack
2049     Node_State nstate = mstack.state();
2050     uint nop = n-&gt;Opcode();
2051     if (nstate == Pre_Visit) {
2052       if (address_visited.test(n-&gt;_idx)) { // Visited in address already?
2053         // Flag as visited and shared now.
2054         set_visited(n);
2055       }
2056       if (is_visited(n)) {   // Visited already?
2057         // Node is shared and has no reason to clone.  Flag it as shared.
2058         // This causes it to match into a register for the sharing.
2059         set_shared(n);       // Flag as shared and
<span class="line-added">2060         if (n-&gt;is_DecodeNarrowPtr()) {</span>
<span class="line-added">2061           // Oop field/array element loads must be shared but since</span>
<span class="line-added">2062           // they are shared through a DecodeN they may appear to have</span>
<span class="line-added">2063           // a single use so force sharing here.</span>
<span class="line-added">2064           set_shared(n-&gt;in(1));</span>
<span class="line-added">2065         }</span>
2066         mstack.pop();        // remove node from stack
2067         continue;
2068       }
2069       nstate = Visit; // Not already visited; so visit now
2070     }
2071     if (nstate == Visit) {
2072       mstack.set_state(Post_Visit);
2073       set_visited(n);   // Flag as visited now
2074       bool mem_op = false;
2075       int mem_addr_idx = MemNode::Address;
<span class="line-modified">2076       if (find_shared_visit(mstack, n, nop, mem_op, mem_addr_idx)) {</span>
<span class="line-modified">2077         continue;</span>



2078       }
2079       for(int i = n-&gt;req() - 1; i &gt;= 0; --i) { // For my children
2080         Node *m = n-&gt;in(i); // Get ith input
2081         if (m == NULL) continue;  // Ignore NULLs
2082         uint mop = m-&gt;Opcode();
2083 
2084         // Must clone all producers of flags, or we will not match correctly.
2085         // Suppose a compare setting int-flags is shared (e.g., a switch-tree)
2086         // then it will match into an ideal Op_RegFlags.  Alas, the fp-flags
2087         // are also there, so we may match a float-branch to int-flags and
2088         // expect the allocator to haul the flags from the int-side to the
2089         // fp-side.  No can do.
2090         if( _must_clone[mop] ) {
2091           mstack.push(m, Visit);
2092           continue; // for(int i = ...)
2093         }
2094 







2095         // if &#39;n&#39; and &#39;m&#39; are part of a graph for BMI instruction, clone this node.
2096 #ifdef X86
2097         if (UseBMI1Instructions &amp;&amp; is_bmi_pattern(n, m)) {
2098           mstack.push(m, Visit);
2099           continue;
2100         }
2101 #endif
<span class="line-added">2102         if (is_vshift_con_pattern(n, m)) {</span>
<span class="line-added">2103           mstack.push(m, Visit);</span>
<span class="line-added">2104           continue;</span>
<span class="line-added">2105         }</span>
2106 
2107         // Clone addressing expressions as they are &quot;free&quot; in memory access instructions
2108         if (mem_op &amp;&amp; i == mem_addr_idx &amp;&amp; mop == Op_AddP &amp;&amp;
2109             // When there are other uses besides address expressions
2110             // put it on stack and mark as shared.
2111             !is_visited(m)) {
2112           // Some inputs for address expression are not put on stack
2113           // to avoid marking them as shared and forcing them into register
2114           // if they are used only in address expressions.
2115           // But they should be marked as shared if there are other uses
2116           // besides address expressions.
2117 
2118           if (clone_address_expressions(m-&gt;as_AddP(), mstack, address_visited)) {
2119             continue;
2120           }
2121         }   // if( mem_op &amp;&amp;
2122         mstack.push(m, Pre_Visit);
2123       }     // for(int i = ...)
2124     }
2125     else if (nstate == Alt_Post_Visit) {
</pre>
<hr />
<pre>
2457         // Note: new_val may have a control edge if
2458         // the original ideal node DecodeN was matched before
2459         // it was unpinned in Matcher::collect_null_checks().
2460         // Unpin the mach node and mark it.
2461         new_val-&gt;set_req(0, NULL);
2462         new_val = (Node*)(((intptr_t)new_val) | 1);
2463       }
2464       // Is a match-tree root, so replace with the matched value
2465       _null_check_tests.map(i+1, new_val);
2466     } else {
2467       // Yank from candidate list
2468       _null_check_tests.map(i+1,_null_check_tests[--cnt]);
2469       _null_check_tests.map(i,_null_check_tests[--cnt]);
2470       _null_check_tests.pop();
2471       _null_check_tests.pop();
2472       i-=2;
2473     }
2474   }
2475 }
2476 
<span class="line-added">2477 bool Matcher::gen_narrow_oop_implicit_null_checks() {</span>
<span class="line-added">2478   // Advice matcher to perform null checks on the narrow oop side.</span>
<span class="line-added">2479   // Implicit checks are not possible on the uncompressed oop side anyway</span>
<span class="line-added">2480   // (at least not for read accesses).</span>
<span class="line-added">2481   // Performs significantly better (especially on Power 6).</span>
<span class="line-added">2482   if (!os::zero_page_read_protected()) {</span>
<span class="line-added">2483     return true;</span>
<span class="line-added">2484   }</span>
<span class="line-added">2485   return CompressedOops::use_implicit_null_checks() &amp;&amp;</span>
<span class="line-added">2486          (narrow_oop_use_complex_address() ||</span>
<span class="line-added">2487           CompressedOops::base() != NULL);</span>
<span class="line-added">2488 }</span>
<span class="line-added">2489 </span>
<span class="line-added">2490 // Compute RegMask for an ideal register.</span>
<span class="line-added">2491 const RegMask* Matcher::regmask_for_ideal_register(uint ideal_reg, Node* ret) {</span>
<span class="line-added">2492   const Type* t = Type::mreg2type[ideal_reg];</span>
<span class="line-added">2493   if (t == NULL) {</span>
<span class="line-added">2494     assert(ideal_reg &gt;= Op_VecS &amp;&amp; ideal_reg &lt;= Op_VecZ, &quot;not a vector: %d&quot;, ideal_reg);</span>
<span class="line-added">2495     return NULL; // not supported</span>
<span class="line-added">2496   }</span>
<span class="line-added">2497   Node* fp  = ret-&gt;in(TypeFunc::FramePtr);</span>
<span class="line-added">2498   Node* mem = ret-&gt;in(TypeFunc::Memory);</span>
<span class="line-added">2499   const TypePtr* atp = TypePtr::BOTTOM;</span>
<span class="line-added">2500   MemNode::MemOrd mo = MemNode::unordered;</span>
<span class="line-added">2501 </span>
<span class="line-added">2502   Node* spill;</span>
<span class="line-added">2503   switch (ideal_reg) {</span>
<span class="line-added">2504     case Op_RegN: spill = new LoadNNode(NULL, mem, fp, atp, t-&gt;is_narrowoop(), mo); break;</span>
<span class="line-added">2505     case Op_RegI: spill = new LoadINode(NULL, mem, fp, atp, t-&gt;is_int(),       mo); break;</span>
<span class="line-added">2506     case Op_RegP: spill = new LoadPNode(NULL, mem, fp, atp, t-&gt;is_ptr(),       mo); break;</span>
<span class="line-added">2507     case Op_RegF: spill = new LoadFNode(NULL, mem, fp, atp, t,                 mo); break;</span>
<span class="line-added">2508     case Op_RegD: spill = new LoadDNode(NULL, mem, fp, atp, t,                 mo); break;</span>
<span class="line-added">2509     case Op_RegL: spill = new LoadLNode(NULL, mem, fp, atp, t-&gt;is_long(),      mo); break;</span>
<span class="line-added">2510 </span>
<span class="line-added">2511     case Op_VecS: // fall-through</span>
<span class="line-added">2512     case Op_VecD: // fall-through</span>
<span class="line-added">2513     case Op_VecX: // fall-through</span>
<span class="line-added">2514     case Op_VecY: // fall-through</span>
<span class="line-added">2515     case Op_VecZ: spill = new LoadVectorNode(NULL, mem, fp, atp, t-&gt;is_vect()); break;</span>
<span class="line-added">2516 </span>
<span class="line-added">2517     default: ShouldNotReachHere();</span>
<span class="line-added">2518   }</span>
<span class="line-added">2519   MachNode* mspill = match_tree(spill);</span>
<span class="line-added">2520   assert(mspill != NULL, &quot;matching failed: %d&quot;, ideal_reg);</span>
<span class="line-added">2521   // Handle generic vector operand case</span>
<span class="line-added">2522   if (Matcher::supports_generic_vector_operands &amp;&amp; t-&gt;isa_vect()) {</span>
<span class="line-added">2523     specialize_mach_node(mspill);</span>
<span class="line-added">2524   }</span>
<span class="line-added">2525   return &amp;mspill-&gt;out_RegMask();</span>
<span class="line-added">2526 }</span>
<span class="line-added">2527 </span>
<span class="line-added">2528 // Process Mach IR right after selection phase is over.</span>
<span class="line-added">2529 void Matcher::do_postselect_cleanup() {</span>
<span class="line-added">2530   if (supports_generic_vector_operands) {</span>
<span class="line-added">2531     specialize_generic_vector_operands();</span>
<span class="line-added">2532     if (C-&gt;failing())  return;</span>
<span class="line-added">2533   }</span>
<span class="line-added">2534 }</span>
<span class="line-added">2535 </span>
<span class="line-added">2536 //----------------------------------------------------------------------</span>
<span class="line-added">2537 // Generic machine operands elision.</span>
<span class="line-added">2538 //----------------------------------------------------------------------</span>
<span class="line-added">2539 </span>
<span class="line-added">2540 // Convert (leg)Vec to (leg)Vec[SDXYZ].</span>
<span class="line-added">2541 MachOper* Matcher::specialize_vector_operand_helper(MachNode* m, uint opnd_idx, const TypeVect* vt) {</span>
<span class="line-added">2542   MachOper* original_opnd = m-&gt;_opnds[opnd_idx];</span>
<span class="line-added">2543   uint ideal_reg = vt-&gt;ideal_reg();</span>
<span class="line-added">2544   // Handle special cases.</span>
<span class="line-added">2545   // LShiftCntV/RShiftCntV report wide vector type, but Matcher::vector_shift_count_ideal_reg() as ideal register (see vectornode.hpp).</span>
<span class="line-added">2546   // Look for shift count use sites as well (at vector shift nodes).</span>
<span class="line-added">2547   int opc = m-&gt;ideal_Opcode();</span>
<span class="line-added">2548   if ((VectorNode::is_vector_shift_count(opc)  &amp;&amp; opnd_idx == 0) || // DEF operand of LShiftCntV/RShiftCntV</span>
<span class="line-added">2549       (VectorNode::is_vector_shift(opc)        &amp;&amp; opnd_idx == 2)) { // shift operand of a vector shift node</span>
<span class="line-added">2550     ideal_reg = Matcher::vector_shift_count_ideal_reg(vt-&gt;length_in_bytes());</span>
<span class="line-added">2551   }</span>
<span class="line-added">2552   return Matcher::specialize_generic_vector_operand(original_opnd, ideal_reg, false);</span>
<span class="line-added">2553 }</span>
<span class="line-added">2554 </span>
<span class="line-added">2555 // Compute concrete vector operand for a generic TEMP vector mach node based on its user info.</span>
<span class="line-added">2556 void Matcher::specialize_temp_node(MachTempNode* tmp, MachNode* use, uint idx) {</span>
<span class="line-added">2557   assert(use-&gt;in(idx) == tmp, &quot;not a user&quot;);</span>
<span class="line-added">2558   assert(!Matcher::is_generic_vector(use-&gt;_opnds[0]), &quot;use not processed yet&quot;);</span>
<span class="line-added">2559 </span>
<span class="line-added">2560   if ((uint)idx == use-&gt;two_adr()) { // DEF_TEMP case</span>
<span class="line-added">2561     tmp-&gt;_opnds[0] = use-&gt;_opnds[0]-&gt;clone();</span>
<span class="line-added">2562   } else {</span>
<span class="line-added">2563     uint ideal_vreg = vector_ideal_reg(C-&gt;max_vector_size());</span>
<span class="line-added">2564     tmp-&gt;_opnds[0] = specialize_generic_vector_operand(tmp-&gt;_opnds[0], ideal_vreg, true);</span>
<span class="line-added">2565   }</span>
<span class="line-added">2566 }</span>
<span class="line-added">2567 </span>
<span class="line-added">2568 // Compute concrete vector operand for a generic DEF/USE vector operand (of mach node m at index idx).</span>
<span class="line-added">2569 MachOper* Matcher::specialize_vector_operand(MachNode* m, uint opnd_idx) {</span>
<span class="line-added">2570   assert(Matcher::is_generic_vector(m-&gt;_opnds[opnd_idx]), &quot;repeated updates&quot;);</span>
<span class="line-added">2571   Node* def = NULL;</span>
<span class="line-added">2572   if (opnd_idx == 0) { // DEF</span>
<span class="line-added">2573     def = m; // use mach node itself to compute vector operand type</span>
<span class="line-added">2574   } else {</span>
<span class="line-added">2575     int base_idx = m-&gt;operand_index(opnd_idx);</span>
<span class="line-added">2576     def = m-&gt;in(base_idx);</span>
<span class="line-added">2577     if (def-&gt;is_Mach()) {</span>
<span class="line-added">2578       if (def-&gt;is_MachTemp() &amp;&amp; Matcher::is_generic_vector(def-&gt;as_Mach()-&gt;_opnds[0])) {</span>
<span class="line-added">2579         specialize_temp_node(def-&gt;as_MachTemp(), m, base_idx); // MachTemp node use site</span>
<span class="line-added">2580       } else if (is_generic_reg2reg_move(def-&gt;as_Mach())) {</span>
<span class="line-added">2581         def = def-&gt;in(1); // skip over generic reg-to-reg moves</span>
<span class="line-added">2582       }</span>
<span class="line-added">2583     }</span>
<span class="line-added">2584   }</span>
<span class="line-added">2585   return specialize_vector_operand_helper(m, opnd_idx, def-&gt;bottom_type()-&gt;is_vect());</span>
<span class="line-added">2586 }</span>
<span class="line-added">2587 </span>
<span class="line-added">2588 void Matcher::specialize_mach_node(MachNode* m) {</span>
<span class="line-added">2589   assert(!m-&gt;is_MachTemp(), &quot;processed along with its user&quot;);</span>
<span class="line-added">2590   // For generic use operands pull specific register class operands from</span>
<span class="line-added">2591   // its def instruction&#39;s output operand (def operand).</span>
<span class="line-added">2592   for (uint i = 0; i &lt; m-&gt;num_opnds(); i++) {</span>
<span class="line-added">2593     if (Matcher::is_generic_vector(m-&gt;_opnds[i])) {</span>
<span class="line-added">2594       m-&gt;_opnds[i] = specialize_vector_operand(m, i);</span>
<span class="line-added">2595     }</span>
<span class="line-added">2596   }</span>
<span class="line-added">2597 }</span>
<span class="line-added">2598 </span>
<span class="line-added">2599 // Replace generic vector operands with concrete vector operands and eliminate generic reg-to-reg moves from the graph.</span>
<span class="line-added">2600 void Matcher::specialize_generic_vector_operands() {</span>
<span class="line-added">2601   assert(supports_generic_vector_operands, &quot;sanity&quot;);</span>
<span class="line-added">2602   ResourceMark rm;</span>
<span class="line-added">2603 </span>
<span class="line-added">2604   if (C-&gt;max_vector_size() == 0) {</span>
<span class="line-added">2605     return; // no vector instructions or operands</span>
<span class="line-added">2606   }</span>
<span class="line-added">2607   // Replace generic vector operands (vec/legVec) with concrete ones (vec[SDXYZ]/legVec[SDXYZ])</span>
<span class="line-added">2608   // and remove reg-to-reg vector moves (MoveVec2Leg and MoveLeg2Vec).</span>
<span class="line-added">2609   Unique_Node_List live_nodes;</span>
<span class="line-added">2610   C-&gt;identify_useful_nodes(live_nodes);</span>
<span class="line-added">2611 </span>
<span class="line-added">2612   while (live_nodes.size() &gt; 0) {</span>
<span class="line-added">2613     MachNode* m = live_nodes.pop()-&gt;isa_Mach();</span>
<span class="line-added">2614     if (m != NULL) {</span>
<span class="line-added">2615       if (Matcher::is_generic_reg2reg_move(m)) {</span>
<span class="line-added">2616         // Register allocator properly handles vec &lt;=&gt; leg moves using register masks.</span>
<span class="line-added">2617         int opnd_idx = m-&gt;operand_index(1);</span>
<span class="line-added">2618         Node* def = m-&gt;in(opnd_idx);</span>
<span class="line-added">2619         m-&gt;subsume_by(def, C);</span>
<span class="line-added">2620       } else if (m-&gt;is_MachTemp()) {</span>
<span class="line-added">2621         // process MachTemp nodes at use site (see Matcher::specialize_vector_operand)</span>
<span class="line-added">2622       } else {</span>
<span class="line-added">2623         specialize_mach_node(m);</span>
<span class="line-added">2624       }</span>
<span class="line-added">2625     }</span>
<span class="line-added">2626   }</span>
<span class="line-added">2627 }</span>
<span class="line-added">2628 </span>
<span class="line-added">2629 #ifdef ASSERT</span>
<span class="line-added">2630 bool Matcher::verify_after_postselect_cleanup() {</span>
<span class="line-added">2631   assert(!C-&gt;failing(), &quot;sanity&quot;);</span>
<span class="line-added">2632   if (supports_generic_vector_operands) {</span>
<span class="line-added">2633     Unique_Node_List useful;</span>
<span class="line-added">2634     C-&gt;identify_useful_nodes(useful);</span>
<span class="line-added">2635     for (uint i = 0; i &lt; useful.size(); i++) {</span>
<span class="line-added">2636       MachNode* m = useful.at(i)-&gt;isa_Mach();</span>
<span class="line-added">2637       if (m != NULL) {</span>
<span class="line-added">2638         assert(!Matcher::is_generic_reg2reg_move(m), &quot;no MoveVec nodes allowed&quot;);</span>
<span class="line-added">2639         for (uint j = 0; j &lt; m-&gt;num_opnds(); j++) {</span>
<span class="line-added">2640           assert(!Matcher::is_generic_vector(m-&gt;_opnds[j]), &quot;no generic vector operands allowed&quot;);</span>
<span class="line-added">2641         }</span>
<span class="line-added">2642       }</span>
<span class="line-added">2643     }</span>
<span class="line-added">2644   }</span>
<span class="line-added">2645   return true;</span>
<span class="line-added">2646 }</span>
<span class="line-added">2647 #endif // ASSERT</span>
<span class="line-added">2648 </span>
2649 // Used by the DFA in dfa_xxx.cpp.  Check for a following barrier or
2650 // atomic instruction acting as a store_load barrier without any
2651 // intervening volatile load, and thus we don&#39;t need a barrier here.
2652 // We retain the Node to act as a compiler ordering barrier.
2653 bool Matcher::post_store_load_barrier(const Node* vmb) {
2654   Compile* C = Compile::current();
2655   assert(vmb-&gt;is_MemBar(), &quot;&quot;);
2656   assert(vmb-&gt;Opcode() != Op_MemBarAcquire &amp;&amp; vmb-&gt;Opcode() != Op_LoadFence, &quot;&quot;);
2657   const MemBarNode* membar = vmb-&gt;as_MemBar();
2658 
2659   // Get the Ideal Proj node, ctrl, that can be used to iterate forward
2660   Node* ctrl = NULL;
2661   for (DUIterator_Fast imax, i = membar-&gt;fast_outs(imax); i &lt; imax; i++) {
2662     Node* p = membar-&gt;fast_out(i);
2663     assert(p-&gt;is_Proj(), &quot;only projections here&quot;);
2664     if ((p-&gt;as_Proj()-&gt;_con == TypeFunc::Control) &amp;&amp;
2665         !C-&gt;node_arena()-&gt;contains(p)) { // Unmatched old-space only
2666       ctrl = p;
2667       break;
2668     }
</pre>
</td>
</tr>
</table>
<center><a href="macroArrayCopy.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="matcher.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>