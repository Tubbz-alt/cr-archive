<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/graphKit.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="gcm.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="graphKit.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/graphKit.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;ci/ciUtilities.hpp&quot;
  27 #include &quot;compiler/compileLog.hpp&quot;
  28 #include &quot;gc/shared/barrierSet.hpp&quot;
  29 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  30 #include &quot;interpreter/interpreter.hpp&quot;
  31 #include &quot;memory/resourceArea.hpp&quot;
  32 #include &quot;opto/addnode.hpp&quot;
  33 #include &quot;opto/castnode.hpp&quot;
  34 #include &quot;opto/convertnode.hpp&quot;
  35 #include &quot;opto/graphKit.hpp&quot;
  36 #include &quot;opto/idealKit.hpp&quot;
  37 #include &quot;opto/intrinsicnode.hpp&quot;
  38 #include &quot;opto/locknode.hpp&quot;
  39 #include &quot;opto/machnode.hpp&quot;
  40 #include &quot;opto/opaquenode.hpp&quot;
  41 #include &quot;opto/parse.hpp&quot;
  42 #include &quot;opto/rootnode.hpp&quot;
  43 #include &quot;opto/runtime.hpp&quot;

  44 #include &quot;runtime/deoptimization.hpp&quot;
  45 #include &quot;runtime/sharedRuntime.hpp&quot;


  46 
  47 //----------------------------GraphKit-----------------------------------------
  48 // Main utility constructor.
  49 GraphKit::GraphKit(JVMState* jvms)
  50   : Phase(Phase::Parser),
  51     _env(C-&gt;env()),
  52     _gvn(*C-&gt;initial_gvn()),
  53     _barrier_set(BarrierSet::barrier_set()-&gt;barrier_set_c2())
  54 {
  55   _exceptions = jvms-&gt;map()-&gt;next_exception();
  56   if (_exceptions != NULL)  jvms-&gt;map()-&gt;set_next_exception(NULL);
  57   set_jvms(jvms);
  58 }
  59 
  60 // Private constructor for parser.
  61 GraphKit::GraphKit()
  62   : Phase(Phase::Parser),
  63     _env(C-&gt;env()),
  64     _gvn(*C-&gt;initial_gvn()),
  65     _barrier_set(BarrierSet::barrier_set()-&gt;barrier_set_c2())
</pre>
<hr />
<pre>
1345                     NULL, &quot;assert_null&quot;);
1346     } else {
1347       replace_in_map(value, zerocon(type));
1348       builtin_throw(reason);
1349     }
1350   }
1351 
1352   // Must throw exception, fall-thru not possible?
1353   if (stopped()) {
1354     return top();               // No result
1355   }
1356 
1357   if (assert_null) {
1358     // Cast obj to null on this path.
1359     replace_in_map(value, zerocon(type));
1360     return zerocon(type);
1361   }
1362 
1363   // Cast obj to not-null on this path, if there is no null_control.
1364   // (If there is a null_control, a non-null value may come back to haunt us.)
<span class="line-modified">1365   if (type == T_OBJECT) {</span>
<span class="line-removed">1366     Node* cast = cast_not_null(value, false);</span>
<span class="line-removed">1367     if (null_control == NULL || (*null_control) == top())</span>
<span class="line-removed">1368       replace_in_map(value, cast);</span>
<span class="line-removed">1369     value = cast;</span>
<span class="line-removed">1370   }</span>
<span class="line-removed">1371 </span>
<span class="line-removed">1372   return value;</span>
1373 }
1374 
1375 
1376 //------------------------------cast_not_null----------------------------------
1377 // Cast obj to not-null on this path
1378 Node* GraphKit::cast_not_null(Node* obj, bool do_replace_in_map) {
<span class="line-modified">1379   const Type *t = _gvn.type(obj);</span>
<span class="line-modified">1380   const Type *t_not_null = t-&gt;join_speculative(TypePtr::NOTNULL);</span>
<span class="line-modified">1381   // Object is already not-null?</span>
<span class="line-modified">1382   if( t == t_not_null ) return obj;</span>
<span class="line-modified">1383 </span>
<span class="line-modified">1384   Node *cast = new CastPPNode(obj,t_not_null);</span>
<span class="line-modified">1385   cast-&gt;init_req(0, control());</span>
<span class="line-modified">1386   cast = _gvn.transform( cast );</span>









1387 
1388   // Scan for instances of &#39;obj&#39; in the current JVM mapping.
1389   // These instances are known to be not-null after the test.
<span class="line-modified">1390   if (do_replace_in_map)</span>
1391     replace_in_map(obj, cast);
<span class="line-modified">1392 </span>
<span class="line-modified">1393   return cast;                  // Return casted value</span>
1394 }
1395 
1396 // Sometimes in intrinsics, we implicitly know an object is not null
1397 // (there&#39;s no actual null check) so we can cast it to not null. In
1398 // the course of optimizations, the input to the cast can become null.
1399 // In that case that data path will die and we need the control path
1400 // to become dead as well to keep the graph consistent. So we have to
1401 // add a check for null for which one branch can&#39;t be taken. It uses
1402 // an Opaque4 node that will cause the check to be removed after loop
1403 // opts so the test goes away and the compiled code doesn&#39;t execute a
1404 // useless check.
1405 Node* GraphKit::must_be_not_null(Node* value, bool do_replace_in_map) {



1406   Node* chk = _gvn.transform(new CmpPNode(value, null()));
1407   Node *tst = _gvn.transform(new BoolNode(chk, BoolTest::ne));
1408   Node* opaq = _gvn.transform(new Opaque4Node(C, tst, intcon(1)));
1409   IfNode *iff = new IfNode(control(), opaq, PROB_MAX, COUNT_UNKNOWN);
1410   _gvn.set_type(iff, iff-&gt;Value(&amp;_gvn));
1411   Node *if_f = _gvn.transform(new IfFalseNode(iff));
1412   Node *frame = _gvn.transform(new ParmNode(C-&gt;start(), TypeFunc::FramePtr));
<span class="line-modified">1413   Node *halt = _gvn.transform(new HaltNode(if_f, frame));</span>
1414   C-&gt;root()-&gt;add_req(halt);
1415   Node *if_t = _gvn.transform(new IfTrueNode(iff));
1416   set_control(if_t);
1417   return cast_not_null(value, do_replace_in_map);
1418 }
1419 
1420 
1421 //--------------------------replace_in_map-------------------------------------
1422 void GraphKit::replace_in_map(Node* old, Node* neww) {
1423   if (old == neww) {
1424     return;
1425   }
1426 
1427   map()-&gt;replace_edge(old, neww);
1428 
1429   // Note: This operation potentially replaces any edge
1430   // on the map.  This includes locals, stack, and monitors
1431   // of the current (innermost) JVM state.
1432 
1433   // don&#39;t let inconsistent types from profiling escape this
</pre>
<hr />
<pre>
1473   Node* newmem = _gvn.transform( new ProjNode(call, TypeFunc::Memory, separate_io_proj) );
1474   set_all_memory(newmem);
1475 }
1476 
1477 //=============================================================================
1478 //
1479 // parser factory methods for MemNodes
1480 //
1481 // These are layered on top of the factory methods in LoadNode and StoreNode,
1482 // and integrate with the parser&#39;s memory state and _gvn engine.
1483 //
1484 
1485 // factory methods in &quot;int adr_idx&quot;
1486 Node* GraphKit::make_load(Node* ctl, Node* adr, const Type* t, BasicType bt,
1487                           int adr_idx,
1488                           MemNode::MemOrd mo,
1489                           LoadNode::ControlDependency control_dependency,
1490                           bool require_atomic_access,
1491                           bool unaligned,
1492                           bool mismatched,
<span class="line-modified">1493                           bool unsafe) {</span>

1494   assert(adr_idx != Compile::AliasIdxTop, &quot;use other make_load factory&quot; );
1495   const TypePtr* adr_type = NULL; // debug-mode-only argument
1496   debug_only(adr_type = C-&gt;get_adr_type(adr_idx));
1497   Node* mem = memory(adr_idx);
1498   Node* ld;
1499   if (require_atomic_access &amp;&amp; bt == T_LONG) {
<span class="line-modified">1500     ld = LoadLNode::make_atomic(ctl, mem, adr, adr_type, t, mo, control_dependency, unaligned, mismatched, unsafe);</span>
1501   } else if (require_atomic_access &amp;&amp; bt == T_DOUBLE) {
<span class="line-modified">1502     ld = LoadDNode::make_atomic(ctl, mem, adr, adr_type, t, mo, control_dependency, unaligned, mismatched, unsafe);</span>
1503   } else {
<span class="line-modified">1504     ld = LoadNode::make(_gvn, ctl, mem, adr, adr_type, t, bt, mo, control_dependency, unaligned, mismatched, unsafe);</span>
1505   }
1506   ld = _gvn.transform(ld);
1507   if (((bt == T_OBJECT) &amp;&amp; C-&gt;do_escape_analysis()) || C-&gt;eliminate_boxing()) {
1508     // Improve graph before escape analysis and boxing elimination.
1509     record_for_igvn(ld);
1510   }
1511   return ld;
1512 }
1513 
1514 Node* GraphKit::store_to_memory(Node* ctl, Node* adr, Node *val, BasicType bt,
1515                                 int adr_idx,
1516                                 MemNode::MemOrd mo,
1517                                 bool require_atomic_access,
1518                                 bool unaligned,
1519                                 bool mismatched,
1520                                 bool unsafe) {
1521   assert(adr_idx != Compile::AliasIdxTop, &quot;use other store_to_memory factory&quot; );
1522   const TypePtr* adr_type = NULL;
1523   debug_only(adr_type = C-&gt;get_adr_type(adr_idx));
1524   Node *mem = memory(adr_idx);
</pre>
<hr />
<pre>
1675                                      Node* adr,
1676                                      const TypePtr* adr_type,
1677                                      int alias_idx,
1678                                      Node* new_val,
1679                                      const Type* value_type,
1680                                      BasicType bt,
1681                                      DecoratorSet decorators) {
1682   C2AccessValuePtr addr(adr, adr_type);
1683   C2AtomicParseAccess access(this, decorators | C2_READ_ACCESS | C2_WRITE_ACCESS, bt, obj, addr, alias_idx);
1684   if (access.is_raw()) {
1685     return _barrier_set-&gt;BarrierSetC2::atomic_add_at(access, new_val, value_type);
1686   } else {
1687     return _barrier_set-&gt;atomic_add_at(access, new_val, value_type);
1688   }
1689 }
1690 
1691 void GraphKit::access_clone(Node* src, Node* dst, Node* size, bool is_array) {
1692   return _barrier_set-&gt;clone(this, src, dst, size, is_array);
1693 }
1694 
<span class="line-removed">1695 Node* GraphKit::access_resolve(Node* n, DecoratorSet decorators) {</span>
<span class="line-removed">1696   // Use stronger ACCESS_WRITE|ACCESS_READ by default.</span>
<span class="line-removed">1697   if ((decorators &amp; (ACCESS_READ | ACCESS_WRITE)) == 0) {</span>
<span class="line-removed">1698     decorators |= ACCESS_READ | ACCESS_WRITE;</span>
<span class="line-removed">1699   }</span>
<span class="line-removed">1700   return _barrier_set-&gt;resolve(this, n, decorators);</span>
<span class="line-removed">1701 }</span>
<span class="line-removed">1702 </span>
1703 //-------------------------array_element_address-------------------------
1704 Node* GraphKit::array_element_address(Node* ary, Node* idx, BasicType elembt,
1705                                       const TypeInt* sizetype, Node* ctrl) {
1706   uint shift  = exact_log2(type2aelembytes(elembt));
1707   uint header = arrayOopDesc::base_offset_in_bytes(elembt);
1708 
1709   // short-circuit a common case (saves lots of confusing waste motion)
1710   jint idx_con = find_int_con(idx, -1);
1711   if (idx_con &gt;= 0) {
1712     intptr_t offset = header + ((intptr_t)idx_con &lt;&lt; shift);
1713     return basic_plus_adr(ary, offset);
1714   }
1715 
1716   // must be correct type for alignment purposes
1717   Node* base  = basic_plus_adr(ary, header);
1718   idx = Compile::conv_I2X_index(&amp;_gvn, idx, sizetype, ctrl);
1719   Node* scale = _gvn.transform( new LShiftXNode(idx, intcon(shift)) );
1720   return basic_plus_adr(ary, base, scale);
1721 }
1722 
</pre>
<hr />
<pre>
1842     if (hook_mem != NULL) {
1843       // Make memory for the call
1844       Node* mem = _gvn.transform( new ProjNode(call, TypeFunc::Memory) );
1845       // Set the RawPtr memory state only.  This covers all the heap top/GC stuff
1846       // We also use hook_mem to extract specific effects from arraycopy stubs.
1847       set_memory(mem, hook_mem);
1848     }
1849     // ...else the call has NO memory effects.
1850 
1851     // Make sure the call advertises its memory effects precisely.
1852     // This lets us build accurate anti-dependences in gcm.cpp.
1853     assert(C-&gt;alias_type(call-&gt;adr_type()) == C-&gt;alias_type(hook_mem),
1854            &quot;call node must be constructed correctly&quot;);
1855   } else {
1856     assert(hook_mem == NULL, &quot;&quot;);
1857     // This is not a &quot;slow path&quot; call; all memory comes from the call.
1858     set_all_memory_call(call);
1859   }
1860 }
1861 












1862 
1863 // Replace the call with the current state of the kit.
1864 void GraphKit::replace_call(CallNode* call, Node* result, bool do_replaced_nodes) {
1865   JVMState* ejvms = NULL;
1866   if (has_exceptions()) {
1867     ejvms = transfer_exceptions_into_jvms();
1868   }
1869 
1870   ReplacedNodes replaced_nodes = map()-&gt;replaced_nodes();
1871   ReplacedNodes replaced_nodes_exception;
1872   Node* ex_ctl = top();
1873 
1874   SafePointNode* final_state = stop();
1875 
1876   // Find all the needed outputs of this call
1877   CallProjections callprojs;
1878   call-&gt;extract_projections(&amp;callprojs, true);
1879 

1880   Node* init_mem = call-&gt;in(TypeFunc::Memory);
1881   Node* final_mem = final_state-&gt;in(TypeFunc::Memory);
1882   Node* final_ctl = final_state-&gt;in(TypeFunc::Control);
1883   Node* final_io = final_state-&gt;in(TypeFunc::I_O);
1884 
1885   // Replace all the old call edges with the edges from the inlining result
1886   if (callprojs.fallthrough_catchproj != NULL) {
1887     C-&gt;gvn_replace_by(callprojs.fallthrough_catchproj, final_ctl);
1888   }
1889   if (callprojs.fallthrough_memproj != NULL) {
1890     if (final_mem-&gt;is_MergeMem()) {
1891       // Parser&#39;s exits MergeMem was not transformed but may be optimized
1892       final_mem = _gvn.transform(final_mem);
1893     }
1894     C-&gt;gvn_replace_by(callprojs.fallthrough_memproj,   final_mem);

1895   }
1896   if (callprojs.fallthrough_ioproj != NULL) {
1897     C-&gt;gvn_replace_by(callprojs.fallthrough_ioproj,    final_io);
1898   }
1899 
1900   // Replace the result with the new result if it exists and is used
1901   if (callprojs.resproj != NULL &amp;&amp; result != NULL) {
1902     C-&gt;gvn_replace_by(callprojs.resproj, result);
1903   }
1904 
1905   if (ejvms == NULL) {
1906     // No exception edges to simply kill off those paths
1907     if (callprojs.catchall_catchproj != NULL) {
1908       C-&gt;gvn_replace_by(callprojs.catchall_catchproj, C-&gt;top());
1909     }
1910     if (callprojs.catchall_memproj != NULL) {
1911       C-&gt;gvn_replace_by(callprojs.catchall_memproj,   C-&gt;top());
1912     }
1913     if (callprojs.catchall_ioproj != NULL) {
1914       C-&gt;gvn_replace_by(callprojs.catchall_ioproj,    C-&gt;top());
1915     }
1916     // Replace the old exception object with top
1917     if (callprojs.exobj != NULL) {
1918       C-&gt;gvn_replace_by(callprojs.exobj, C-&gt;top());
1919     }
1920   } else {
1921     GraphKit ekit(ejvms);
1922 
1923     // Load my combined exception state into the kit, with all phis transformed:
1924     SafePointNode* ex_map = ekit.combine_and_pop_all_exception_states();
1925     replaced_nodes_exception = ex_map-&gt;replaced_nodes();
1926 
1927     Node* ex_oop = ekit.use_exception_state(ex_map);
1928 
1929     if (callprojs.catchall_catchproj != NULL) {
1930       C-&gt;gvn_replace_by(callprojs.catchall_catchproj, ekit.control());
1931       ex_ctl = ekit.control();
1932     }
1933     if (callprojs.catchall_memproj != NULL) {
<span class="line-modified">1934       C-&gt;gvn_replace_by(callprojs.catchall_memproj,   ekit.reset_memory());</span>


1935     }
1936     if (callprojs.catchall_ioproj != NULL) {
1937       C-&gt;gvn_replace_by(callprojs.catchall_ioproj,    ekit.i_o());
1938     }
1939 
1940     // Replace the old exception object with the newly created one
1941     if (callprojs.exobj != NULL) {
1942       C-&gt;gvn_replace_by(callprojs.exobj, ex_oop);
1943     }
1944   }
1945 
1946   // Disconnect the call from the graph
1947   call-&gt;disconnect_inputs(NULL, C);
1948   C-&gt;gvn_replace_by(call, C-&gt;top());
1949 
1950   // Clean up any MergeMems that feed other MergeMems since the
1951   // optimizer doesn&#39;t like that.
<span class="line-modified">1952   if (final_mem-&gt;is_MergeMem()) {</span>
<span class="line-modified">1953     Node_List wl;</span>
<span class="line-removed">1954     for (SimpleDUIterator i(final_mem); i.has_next(); i.next()) {</span>
<span class="line-removed">1955       Node* m = i.get();</span>
<span class="line-removed">1956       if (m-&gt;is_MergeMem() &amp;&amp; !wl.contains(m)) {</span>
<span class="line-removed">1957         wl.push(m);</span>
<span class="line-removed">1958       }</span>
<span class="line-removed">1959     }</span>
<span class="line-removed">1960     while (wl.size()  &gt; 0) {</span>
<span class="line-removed">1961       _gvn.transform(wl.pop());</span>
<span class="line-removed">1962     }</span>
1963   }
1964 
1965   if (callprojs.fallthrough_catchproj != NULL &amp;&amp; !final_ctl-&gt;is_top() &amp;&amp; do_replaced_nodes) {
1966     replaced_nodes.apply(C, final_ctl);
1967   }
1968   if (!ex_ctl-&gt;is_top() &amp;&amp; do_replaced_nodes) {
1969     replaced_nodes_exception.apply(C, ex_ctl);
1970   }
1971 }
1972 
1973 
1974 //------------------------------increment_counter------------------------------
1975 // for statistics: increment a VM counter by 1
1976 
1977 void GraphKit::increment_counter(address counter_addr) {
1978   Node* adr1 = makecon(TypeRawPtr::make(counter_addr));
1979   increment_counter(adr1);
1980 }
1981 
1982 void GraphKit::increment_counter(Node* counter_addr) {
</pre>
<hr />
<pre>
2090   // Clear out dead values from the debug info.
2091   kill_dead_locals();
2092 
2093   // Now insert the uncommon trap subroutine call
2094   address call_addr = SharedRuntime::uncommon_trap_blob()-&gt;entry_point();
2095   const TypePtr* no_memory_effects = NULL;
2096   // Pass the index of the class to be loaded
2097   Node* call = make_runtime_call(RC_NO_LEAF | RC_UNCOMMON |
2098                                  (must_throw ? RC_MUST_THROW : 0),
2099                                  OptoRuntime::uncommon_trap_Type(),
2100                                  call_addr, &quot;uncommon_trap&quot;, no_memory_effects,
2101                                  intcon(trap_request));
2102   assert(call-&gt;as_CallStaticJava()-&gt;uncommon_trap_request() == trap_request,
2103          &quot;must extract request correctly from the graph&quot;);
2104   assert(trap_request != 0, &quot;zero value reserved by uncommon_trap_request&quot;);
2105 
2106   call-&gt;set_req(TypeFunc::ReturnAdr, returnadr());
2107   // The debug info is the only real input to this call.
2108 
2109   // Halt-and-catch fire here.  The above call should never return!
<span class="line-modified">2110   HaltNode* halt = new HaltNode(control(), frameptr());</span>
2111   _gvn.set_type_bottom(halt);
2112   root()-&gt;add_req(halt);
2113 
2114   stop_and_kill_map();
2115 }
2116 
2117 
2118 //--------------------------just_allocated_object------------------------------
2119 // Report the object that was just allocated.
2120 // It must be the case that there are no intervening safepoints.
2121 // We use this to determine if an object is so &quot;fresh&quot; that
2122 // it does not require card marks.
2123 Node* GraphKit::just_allocated_object(Node* current_control) {
2124   Node* ctrl = current_control;
2125   // Object::&lt;init&gt; is invoked after allocation, most of invoke nodes
2126   // will be reduced, but a region node is kept in parse time, we check
2127   // the pattern and skip the region node if it degraded to a copy.
2128   if (ctrl != NULL &amp;&amp; ctrl-&gt;is_Region() &amp;&amp; ctrl-&gt;req() == 2 &amp;&amp;
2129       ctrl-&gt;as_Region()-&gt;is_copy()) {
2130     ctrl = ctrl-&gt;as_Region()-&gt;is_copy();
2131   }
2132   if (C-&gt;recent_alloc_ctl() == ctrl) {
2133    return C-&gt;recent_alloc_obj();
2134   }
2135   return NULL;
2136 }
2137 
2138 
<span class="line-removed">2139 void GraphKit::round_double_arguments(ciMethod* dest_method) {</span>
<span class="line-removed">2140   // (Note:  TypeFunc::make has a cache that makes this fast.)</span>
<span class="line-removed">2141   const TypeFunc* tf    = TypeFunc::make(dest_method);</span>
<span class="line-removed">2142   int             nargs = tf-&gt;domain()-&gt;cnt() - TypeFunc::Parms;</span>
<span class="line-removed">2143   for (int j = 0; j &lt; nargs; j++) {</span>
<span class="line-removed">2144     const Type *targ = tf-&gt;domain()-&gt;field_at(j + TypeFunc::Parms);</span>
<span class="line-removed">2145     if( targ-&gt;basic_type() == T_DOUBLE ) {</span>
<span class="line-removed">2146       // If any parameters are doubles, they must be rounded before</span>
<span class="line-removed">2147       // the call, dstore_rounding does gvn.transform</span>
<span class="line-removed">2148       Node *arg = argument(j);</span>
<span class="line-removed">2149       arg = dstore_rounding(arg);</span>
<span class="line-removed">2150       set_argument(j, arg);</span>
<span class="line-removed">2151     }</span>
<span class="line-removed">2152   }</span>
<span class="line-removed">2153 }</span>
<span class="line-removed">2154 </span>
2155 /**
2156  * Record profiling data exact_kls for Node n with the type system so
2157  * that it can propagate it (speculation)
2158  *
2159  * @param n          node that the type applies to
2160  * @param exact_kls  type from profiling
2161  * @param maybe_null did profiling see null?
2162  *
2163  * @return           node with improved type
2164  */
2165 Node* GraphKit::record_profile_for_speculation(Node* n, ciKlass* exact_kls, ProfilePtrKind ptr_kind) {
2166   const Type* current_type = _gvn.type(n);
2167   assert(UseTypeSpeculation, &quot;type speculation must be on&quot;);
2168 
2169   const TypePtr* speculative = current_type-&gt;speculative();
2170 
2171   // Should the klass from the profile be recorded in the speculative type?
2172   if (current_type-&gt;would_improve_type(exact_kls, jvms()-&gt;depth())) {
2173     const TypeKlassPtr* tklass = TypeKlassPtr::make(exact_kls);
2174     const TypeOopPtr* xtype = tklass-&gt;as_instance_type();
</pre>
<hr />
<pre>
2250   }
2251   return record_profile_for_speculation(n, exact_kls, ptr_kind);
2252 }
2253 
2254 /**
2255  * Record profiling data from argument profiling at an invoke with the
2256  * type system so that it can propagate it (speculation)
2257  *
2258  * @param dest_method  target method for the call
2259  * @param bc           what invoke bytecode is this?
2260  */
2261 void GraphKit::record_profiled_arguments_for_speculation(ciMethod* dest_method, Bytecodes::Code bc) {
2262   if (!UseTypeSpeculation) {
2263     return;
2264   }
2265   const TypeFunc* tf    = TypeFunc::make(dest_method);
2266   int             nargs = tf-&gt;domain()-&gt;cnt() - TypeFunc::Parms;
2267   int skip = Bytecodes::has_receiver(bc) ? 1 : 0;
2268   for (int j = skip, i = 0; j &lt; nargs &amp;&amp; i &lt; TypeProfileArgsLimit; j++) {
2269     const Type *targ = tf-&gt;domain()-&gt;field_at(j + TypeFunc::Parms);
<span class="line-modified">2270     if (targ-&gt;basic_type() == T_OBJECT || targ-&gt;basic_type() == T_ARRAY) {</span>
2271       ProfilePtrKind ptr_kind = ProfileMaybeNull;
2272       ciKlass* better_type = NULL;
2273       if (method()-&gt;argument_profiled_type(bci(), i, better_type, ptr_kind)) {
2274         record_profile_for_speculation(argument(j), better_type, ptr_kind);
2275       }
2276       i++;
2277     }
2278   }
2279 }
2280 
2281 /**
2282  * Record profiling data from parameter profiling at an invoke with
2283  * the type system so that it can propagate it (speculation)
2284  */
2285 void GraphKit::record_profiled_parameters_for_speculation() {
2286   if (!UseTypeSpeculation) {
2287     return;
2288   }
2289   for (int i = 0, j = 0; i &lt; method()-&gt;arg_size() ; i++) {
2290     if (_gvn.type(local(i))-&gt;isa_oopptr()) {
</pre>
<hr />
<pre>
2300 
2301 /**
2302  * Record profiling data from return value profiling at an invoke with
2303  * the type system so that it can propagate it (speculation)
2304  */
2305 void GraphKit::record_profiled_return_for_speculation() {
2306   if (!UseTypeSpeculation) {
2307     return;
2308   }
2309   ProfilePtrKind ptr_kind = ProfileMaybeNull;
2310   ciKlass* better_type = NULL;
2311   if (method()-&gt;return_profiled_type(bci(), better_type, ptr_kind)) {
2312     // If profiling reports a single type for the return value,
2313     // feed it to the type system so it can propagate it as a
2314     // speculative type
2315     record_profile_for_speculation(stack(sp()-1), better_type, ptr_kind);
2316   }
2317 }
2318 
2319 void GraphKit::round_double_result(ciMethod* dest_method) {
<span class="line-modified">2320   // A non-strict method may return a double value which has an extended</span>
<span class="line-modified">2321   // exponent, but this must not be visible in a caller which is &#39;strict&#39;</span>
<span class="line-modified">2322   // If a strict caller invokes a non-strict callee, round a double result</span>












2323 
<span class="line-modified">2324   BasicType result_type = dest_method-&gt;return_type()-&gt;basic_type();</span>
<span class="line-modified">2325   assert( method() != NULL, &quot;must have caller context&quot;);</span>
<span class="line-modified">2326   if( result_type == T_DOUBLE &amp;&amp; method()-&gt;is_strict() &amp;&amp; !dest_method-&gt;is_strict() ) {</span>
<span class="line-modified">2327     // Destination method&#39;s return value is on top of stack</span>
<span class="line-modified">2328     // dstore_rounding() does gvn.transform</span>
<span class="line-modified">2329     Node *result = pop_pair();</span>
<span class="line-modified">2330     result = dstore_rounding(result);</span>
<span class="line-modified">2331     push_pair(result);</span>







2332   }
2333 }
2334 
2335 // rounding for strict float precision conformance
2336 Node* GraphKit::precision_rounding(Node* n) {
<span class="line-modified">2337   return UseStrictFP &amp;&amp; _method-&gt;flags().is_strict()</span>
<span class="line-modified">2338     &amp;&amp; UseSSE == 0 &amp;&amp; Matcher::strict_fp_requires_explicit_rounding</span>
<span class="line-modified">2339     ? _gvn.transform( new RoundFloatNode(0, n) )</span>
<span class="line-modified">2340     : n;</span>






2341 }
2342 
2343 // rounding for strict double precision conformance
2344 Node* GraphKit::dprecision_rounding(Node *n) {
<span class="line-modified">2345   return UseStrictFP &amp;&amp; _method-&gt;flags().is_strict()</span>
<span class="line-modified">2346     &amp;&amp; UseSSE &lt;= 1 &amp;&amp; Matcher::strict_fp_requires_explicit_rounding</span>
<span class="line-modified">2347     ? _gvn.transform( new RoundDoubleNode(0, n) )</span>
<span class="line-modified">2348     : n;</span>






2349 }
2350 
2351 // rounding for non-strict double stores
2352 Node* GraphKit::dstore_rounding(Node* n) {
<span class="line-modified">2353   return Matcher::strict_fp_requires_explicit_rounding</span>
<span class="line-modified">2354     &amp;&amp; UseSSE &lt;= 1</span>
<span class="line-modified">2355     ? _gvn.transform( new RoundDoubleNode(0, n) )</span>
<span class="line-modified">2356     : n;</span>






2357 }
2358 
2359 //=============================================================================
2360 // Generate a fast path/slow path idiom.  Graph looks like:
2361 // [foo] indicates that &#39;foo&#39; is a parameter
2362 //
2363 //              [in]     NULL
2364 //                 \    /
2365 //                  CmpP
2366 //                  Bool ne
2367 //                   If
2368 //                  /  \
2369 //              True    False-&lt;2&gt;
2370 //              / |
2371 //             /  cast_not_null
2372 //           Load  |    |   ^
2373 //        [fast_test]   |   |
2374 // gvn to   opt_test    |   |
2375 //          /    \      |  &lt;1&gt;
2376 //      True     False  |
</pre>
<hr />
<pre>
2433   // Fast path taken; set region slot 2
2434   Node *fast_taken = _gvn.transform( new IfFalseNode(opt_iff) );
2435   region-&gt;init_req(2,fast_taken); // Capture fast-control
2436 
2437   // Fast path not-taken, i.e. slow path
2438   Node *slow_taken = _gvn.transform( new IfTrueNode(opt_iff) );
2439   return slow_taken;
2440 }
2441 
2442 //-----------------------------make_runtime_call-------------------------------
2443 Node* GraphKit::make_runtime_call(int flags,
2444                                   const TypeFunc* call_type, address call_addr,
2445                                   const char* call_name,
2446                                   const TypePtr* adr_type,
2447                                   // The following parms are all optional.
2448                                   // The first NULL ends the list.
2449                                   Node* parm0, Node* parm1,
2450                                   Node* parm2, Node* parm3,
2451                                   Node* parm4, Node* parm5,
2452                                   Node* parm6, Node* parm7) {


2453   // Slow-path call
2454   bool is_leaf = !(flags &amp; RC_NO_LEAF);
2455   bool has_io  = (!is_leaf &amp;&amp; !(flags &amp; RC_NO_IO));
2456   if (call_name == NULL) {
2457     assert(!is_leaf, &quot;must supply name for leaf&quot;);
2458     call_name = OptoRuntime::stub_name(call_addr);
2459   }
2460   CallNode* call;
2461   if (!is_leaf) {
2462     call = new CallStaticJavaNode(call_type, call_addr, call_name,
2463                                            bci(), adr_type);
2464   } else if (flags &amp; RC_NO_FP) {
2465     call = new CallLeafNoFPNode(call_type, call_addr, call_name, adr_type);
2466   } else {
2467     call = new CallLeafNode(call_type, call_addr, call_name, adr_type);
2468   }
2469 
2470   // The following is similar to set_edges_for_java_call,
2471   // except that the memory effects of the call are restricted to AliasIdxRaw.
2472 
</pre>
<hr />
<pre>
2576 
2577     if (excp != top()) {
2578       if (deoptimize) {
2579         // Deoptimize if an exception is caught. Don&#39;t construct exception state in this case.
2580         uncommon_trap(Deoptimization::Reason_unhandled,
2581                       Deoptimization::Action_none);
2582       } else {
2583         // Create an exception state also.
2584         // Use an exact type if the caller has a specific exception.
2585         const Type* ex_type = TypeOopPtr::make_from_klass_unique(ex_klass)-&gt;cast_to_ptr_type(TypePtr::NotNull);
2586         Node*       ex_oop  = new CreateExNode(ex_type, control(), i_o);
2587         add_exception_state(make_exception_state(_gvn.transform(ex_oop)));
2588       }
2589     }
2590   }
2591 
2592   // Get the no-exception control from the CatchNode.
2593   set_control(norm);
2594 }
2595 
<span class="line-modified">2596 static IfNode* gen_subtype_check_compare(Node* ctrl, Node* in1, Node* in2, BoolTest::mask test, float p, PhaseGVN* gvn, BasicType bt) {</span>
2597   Node* cmp = NULL;
2598   switch(bt) {
2599   case T_INT: cmp = new CmpINode(in1, in2); break;
2600   case T_ADDRESS: cmp = new CmpPNode(in1, in2); break;
2601   default: fatal(&quot;unexpected comparison type %s&quot;, type2name(bt));
2602   }
<span class="line-modified">2603   gvn-&gt;transform(cmp);</span>
<span class="line-modified">2604   Node* bol = gvn-&gt;transform(new BoolNode(cmp, test));</span>
2605   IfNode* iff = new IfNode(ctrl, bol, p, COUNT_UNKNOWN);
<span class="line-modified">2606   gvn-&gt;transform(iff);</span>
<span class="line-modified">2607   if (!bol-&gt;is_Con()) gvn-&gt;record_for_igvn(iff);</span>
2608   return iff;
2609 }
2610 









































































2611 
2612 //-------------------------------gen_subtype_check-----------------------------
2613 // Generate a subtyping check.  Takes as input the subtype and supertype.
2614 // Returns 2 values: sets the default control() to the true path and returns
2615 // the false path.  Only reads invariant memory; sets no (visible) memory.
2616 // The PartialSubtypeCheckNode sets the hidden 1-word cache in the encoding
2617 // but that&#39;s not exposed to the optimizer.  This call also doesn&#39;t take in an
2618 // Object; if you wish to check an Object you need to load the Object&#39;s class
2619 // prior to coming here.
<span class="line-modified">2620 Node* Phase::gen_subtype_check(Node* subklass, Node* superklass, Node** ctrl, MergeMemNode* mem, PhaseGVN* gvn) {</span>
<span class="line-modified">2621   Compile* C = gvn-&gt;C;</span>
<span class="line-removed">2622 </span>
2623   if ((*ctrl)-&gt;is_top()) {
2624     return C-&gt;top();
2625   }
2626 
2627   // Fast check for identical types, perhaps identical constants.
2628   // The types can even be identical non-constants, in cases
2629   // involving Array.newInstance, Object.clone, etc.
2630   if (subklass == superklass)
2631     return C-&gt;top();             // false path is dead; no test needed.
2632 
<span class="line-modified">2633   if (gvn-&gt;type(superklass)-&gt;singleton()) {</span>
<span class="line-modified">2634     ciKlass* superk = gvn-&gt;type(superklass)-&gt;is_klassptr()-&gt;klass();</span>
<span class="line-modified">2635     ciKlass* subk   = gvn-&gt;type(subklass)-&gt;is_klassptr()-&gt;klass();</span>
2636 
2637     // In the common case of an exact superklass, try to fold up the
2638     // test before generating code.  You may ask, why not just generate
2639     // the code and then let it fold up?  The answer is that the generated
2640     // code will necessarily include null checks, which do not always
2641     // completely fold away.  If they are also needless, then they turn
2642     // into a performance loss.  Example:
2643     //    Foo[] fa = blah(); Foo x = fa[0]; fa[1] = x;
2644     // Here, the type of &#39;fa&#39; is often exact, so the store check
2645     // of fa[1]=x will fold up, without testing the nullness of x.
2646     switch (C-&gt;static_subtype_check(superk, subk)) {
2647     case Compile::SSC_always_false:
2648       {
2649         Node* always_fail = *ctrl;
<span class="line-modified">2650         *ctrl = gvn-&gt;C-&gt;top();</span>
2651         return always_fail;
2652       }
2653     case Compile::SSC_always_true:
2654       return C-&gt;top();
2655     case Compile::SSC_easy_test:
2656       {
2657         // Just do a direct pointer compare and be done.
2658         IfNode* iff = gen_subtype_check_compare(*ctrl, subklass, superklass, BoolTest::eq, PROB_STATIC_FREQUENT, gvn, T_ADDRESS);
<span class="line-modified">2659         *ctrl = gvn-&gt;transform(new IfTrueNode(iff));</span>
<span class="line-modified">2660         return gvn-&gt;transform(new IfFalseNode(iff));</span>
2661       }
2662     case Compile::SSC_full_test:
2663       break;
2664     default:
2665       ShouldNotReachHere();
2666     }
2667   }
2668 
2669   // %%% Possible further optimization:  Even if the superklass is not exact,
2670   // if the subklass is the unique subtype of the superklass, the check
2671   // will always succeed.  We could leave a dependency behind to ensure this.
2672 
2673   // First load the super-klass&#39;s check-offset
<span class="line-modified">2674   Node *p1 = gvn-&gt;transform(new AddPNode(superklass, superklass, gvn-&gt;MakeConX(in_bytes(Klass::super_check_offset_offset()))));</span>
<span class="line-modified">2675   Node* m = mem-&gt;memory_at(C-&gt;get_alias_index(gvn-&gt;type(p1)-&gt;is_ptr()));</span>
<span class="line-modified">2676   Node *chk_off = gvn-&gt;transform(new LoadINode(NULL, m, p1, gvn-&gt;type(p1)-&gt;is_ptr(), TypeInt::INT, MemNode::unordered));</span>
2677   int cacheoff_con = in_bytes(Klass::secondary_super_cache_offset());
<span class="line-modified">2678   bool might_be_cache = (gvn-&gt;find_int_con(chk_off, cacheoff_con) == cacheoff_con);</span>
2679 
2680   // Load from the sub-klass&#39;s super-class display list, or a 1-word cache of
2681   // the secondary superclass list, or a failing value with a sentinel offset
2682   // if the super-klass is an interface or exceptionally deep in the Java
2683   // hierarchy and we have to scan the secondary superclass list the hard way.
2684   // Worst-case type is a little odd: NULL is allowed as a result (usually
2685   // klass loads can never produce a NULL).
2686   Node *chk_off_X = chk_off;
2687 #ifdef _LP64
<span class="line-modified">2688   chk_off_X = gvn-&gt;transform(new ConvI2LNode(chk_off_X));</span>
2689 #endif
<span class="line-modified">2690   Node *p2 = gvn-&gt;transform(new AddPNode(subklass,subklass,chk_off_X));</span>
2691   // For some types like interfaces the following loadKlass is from a 1-word
2692   // cache which is mutable so can&#39;t use immutable memory.  Other
2693   // types load from the super-class display table which is immutable.
<span class="line-modified">2694   m = mem-&gt;memory_at(C-&gt;get_alias_index(gvn-&gt;type(p2)-&gt;is_ptr()));</span>
<span class="line-modified">2695   Node *kmem = might_be_cache ? m : C-&gt;immutable_memory();</span>
<span class="line-modified">2696   Node *nkls = gvn-&gt;transform(LoadKlassNode::make(*gvn, NULL, kmem, p2, gvn-&gt;type(p2)-&gt;is_ptr(), TypeKlassPtr::OBJECT_OR_NULL));</span>







2697 
2698   // Compile speed common case: ARE a subtype and we canNOT fail
2699   if( superklass == nkls )
2700     return C-&gt;top();             // false path is dead; no test needed.
2701 
2702   // See if we get an immediate positive hit.  Happens roughly 83% of the
2703   // time.  Test to see if the value loaded just previously from the subklass
2704   // is exactly the superklass.
2705   IfNode *iff1 = gen_subtype_check_compare(*ctrl, superklass, nkls, BoolTest::eq, PROB_LIKELY(0.83f), gvn, T_ADDRESS);
<span class="line-modified">2706   Node *iftrue1 = gvn-&gt;transform( new IfTrueNode (iff1));</span>
<span class="line-modified">2707   *ctrl = gvn-&gt;transform(new IfFalseNode(iff1));</span>
2708 
2709   // Compile speed common case: Check for being deterministic right now.  If
2710   // chk_off is a constant and not equal to cacheoff then we are NOT a
2711   // subklass.  In this case we need exactly the 1 test above and we can
2712   // return those results immediately.
2713   if (!might_be_cache) {
2714     Node* not_subtype_ctrl = *ctrl;
2715     *ctrl = iftrue1; // We need exactly the 1 test above
2716     return not_subtype_ctrl;
2717   }
2718 
2719   // Gather the various success &amp; failures here
2720   RegionNode *r_ok_subtype = new RegionNode(4);
<span class="line-modified">2721   gvn-&gt;record_for_igvn(r_ok_subtype);</span>
2722   RegionNode *r_not_subtype = new RegionNode(3);
<span class="line-modified">2723   gvn-&gt;record_for_igvn(r_not_subtype);</span>
2724 
2725   r_ok_subtype-&gt;init_req(1, iftrue1);
2726 
2727   // Check for immediate negative hit.  Happens roughly 11% of the time (which
2728   // is roughly 63% of the remaining cases).  Test to see if the loaded
2729   // check-offset points into the subklass display list or the 1-element
2730   // cache.  If it points to the display (and NOT the cache) and the display
2731   // missed then it&#39;s not a subtype.
<span class="line-modified">2732   Node *cacheoff = gvn-&gt;intcon(cacheoff_con);</span>
2733   IfNode *iff2 = gen_subtype_check_compare(*ctrl, chk_off, cacheoff, BoolTest::ne, PROB_LIKELY(0.63f), gvn, T_INT);
<span class="line-modified">2734   r_not_subtype-&gt;init_req(1, gvn-&gt;transform(new IfTrueNode (iff2)));</span>
<span class="line-modified">2735   *ctrl = gvn-&gt;transform(new IfFalseNode(iff2));</span>
2736 
2737   // Check for self.  Very rare to get here, but it is taken 1/3 the time.
2738   // No performance impact (too rare) but allows sharing of secondary arrays
2739   // which has some footprint reduction.
2740   IfNode *iff3 = gen_subtype_check_compare(*ctrl, subklass, superklass, BoolTest::eq, PROB_LIKELY(0.36f), gvn, T_ADDRESS);
<span class="line-modified">2741   r_ok_subtype-&gt;init_req(2, gvn-&gt;transform(new IfTrueNode(iff3)));</span>
<span class="line-modified">2742   *ctrl = gvn-&gt;transform(new IfFalseNode(iff3));</span>
2743 
2744   // -- Roads not taken here: --
2745   // We could also have chosen to perform the self-check at the beginning
2746   // of this code sequence, as the assembler does.  This would not pay off
2747   // the same way, since the optimizer, unlike the assembler, can perform
2748   // static type analysis to fold away many successful self-checks.
2749   // Non-foldable self checks work better here in second position, because
2750   // the initial primary superclass check subsumes a self-check for most
2751   // types.  An exception would be a secondary type like array-of-interface,
2752   // which does not appear in its own primary supertype display.
2753   // Finally, we could have chosen to move the self-check into the
2754   // PartialSubtypeCheckNode, and from there out-of-line in a platform
2755   // dependent manner.  But it is worthwhile to have the check here,
2756   // where it can be perhaps be optimized.  The cost in code space is
2757   // small (register compare, branch).
2758 
2759   // Now do a linear scan of the secondary super-klass array.  Again, no real
2760   // performance impact (too rare) but it&#39;s gotta be done.
2761   // Since the code is rarely used, there is no penalty for moving it
2762   // out of line, and it can only improve I-cache density.
2763   // The decision to inline or out-of-line this final check is platform
2764   // dependent, and is found in the AD file definition of PartialSubtypeCheck.
<span class="line-modified">2765   Node* psc = gvn-&gt;transform(</span>
2766     new PartialSubtypeCheckNode(*ctrl, subklass, superklass));
2767 
<span class="line-modified">2768   IfNode *iff4 = gen_subtype_check_compare(*ctrl, psc, gvn-&gt;zerocon(T_OBJECT), BoolTest::ne, PROB_FAIR, gvn, T_ADDRESS);</span>
<span class="line-modified">2769   r_not_subtype-&gt;init_req(2, gvn-&gt;transform(new IfTrueNode (iff4)));</span>
<span class="line-modified">2770   r_ok_subtype -&gt;init_req(3, gvn-&gt;transform(new IfFalseNode(iff4)));</span>
2771 
2772   // Return false path; set default control to true path.
<span class="line-modified">2773   *ctrl = gvn-&gt;transform(r_ok_subtype);</span>
<span class="line-modified">2774   return gvn-&gt;transform(r_not_subtype);</span>






















2775 }
2776 
2777 // Profile-driven exact type check:
2778 Node* GraphKit::type_check_receiver(Node* receiver, ciKlass* klass,
2779                                     float prob,
2780                                     Node* *casted_receiver) {
2781   const TypeKlassPtr* tklass = TypeKlassPtr::make(klass);
2782   Node* recv_klass = load_object_klass(receiver);
2783   Node* want_klass = makecon(tklass);
2784   Node* cmp = _gvn.transform( new CmpPNode(recv_klass, want_klass) );
2785   Node* bol = _gvn.transform( new BoolNode(cmp, BoolTest::eq) );
2786   IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);
2787   set_control( _gvn.transform( new IfTrueNode (iff) ));
2788   Node* fail = _gvn.transform( new IfFalseNode(iff) );
2789 
2790   const TypeOopPtr* recv_xtype = tklass-&gt;as_instance_type();
2791   assert(recv_xtype-&gt;klass_is_exact(), &quot;&quot;);
2792 
2793   // Subsume downstream occurrences of receiver with a cast to
2794   // recv_xtype, since now we know what the type will be.
2795   Node* cast = new CheckCastPPNode(control(), receiver, recv_xtype);
2796   (*casted_receiver) = _gvn.transform(cast);
2797   // (User must make the replace_in_map call.)
2798 
2799   return fail;
2800 }
2801 
2802 //------------------------------subtype_check_receiver-------------------------
2803 Node* GraphKit::subtype_check_receiver(Node* receiver, ciKlass* klass,
2804                                        Node** casted_receiver) {
2805   const TypeKlassPtr* tklass = TypeKlassPtr::make(klass);
<span class="line-removed">2806   Node* recv_klass = load_object_klass(receiver);</span>
2807   Node* want_klass = makecon(tklass);
2808 
<span class="line-modified">2809   Node* slow_ctl = gen_subtype_check(recv_klass, want_klass);</span>
2810 
2811   // Cast receiver after successful check
2812   const TypeOopPtr* recv_type = tklass-&gt;cast_to_exactness(false)-&gt;is_klassptr()-&gt;as_instance_type();
2813   Node* cast = new CheckCastPPNode(control(), receiver, recv_type);
2814   (*casted_receiver) = _gvn.transform(cast);
2815 
2816   return slow_ctl;
2817 }
2818 
2819 //------------------------------seems_never_null-------------------------------
2820 // Use null_seen information if it is available from the profile.
2821 // If we see an unexpected null at a type check we record it and force a
2822 // recompile; the offending check will be recompiled to handle NULLs.
2823 // If we see several offending BCIs, then all checks in the
2824 // method will be recompiled.
2825 bool GraphKit::seems_never_null(Node* obj, ciProfileData* data, bool&amp; speculating) {
2826   speculating = !_gvn.type(obj)-&gt;speculative_maybe_null();
2827   Deoptimization::DeoptReason reason = Deoptimization::reason_null_check(speculating);
2828   if (UncommonNullCast               // Cutout for this technique
2829       &amp;&amp; obj != null()               // And not the -Xcomp stupid case?
2830       &amp;&amp; !too_many_traps(reason)
2831       ) {
2832     if (speculating) {
2833       return true;
2834     }
2835     if (data == NULL)
2836       // Edge case:  no mature data.  Be optimistic here.
2837       return true;
2838     // If the profile has not seen a null, assume it won&#39;t happen.
2839     assert(java_bc() == Bytecodes::_checkcast ||
2840            java_bc() == Bytecodes::_instanceof ||
2841            java_bc() == Bytecodes::_aastore, &quot;MDO must collect null_seen bit here&quot;);
2842     return !data-&gt;as_BitData()-&gt;null_seen();
2843   }
2844   speculating = false;
2845   return false;
2846 }
2847 






















































2848 //------------------------maybe_cast_profiled_receiver-------------------------
2849 // If the profile has seen exactly one type, narrow to exactly that type.
2850 // Subsequent type checks will always fold up.
2851 Node* GraphKit::maybe_cast_profiled_receiver(Node* not_null_obj,
2852                                              ciKlass* require_klass,
2853                                              ciKlass* spec_klass,
2854                                              bool safe_for_replace) {
2855   if (!UseTypeProfile || !TypeProfileCasts) return NULL;
2856 
2857   Deoptimization::DeoptReason reason = Deoptimization::reason_class_check(spec_klass != NULL);
2858 
2859   // Make sure we haven&#39;t already deoptimized from this tactic.
2860   if (too_many_traps_or_recompiles(reason))
2861     return NULL;
2862 
2863   // (No, this isn&#39;t a call, but it&#39;s enough like a virtual call
2864   // to use the same ciMethod accessor to get the profile info...)
2865   // If we have a speculative type use it instead of profiling (which
2866   // may not help us)
2867   ciKlass* exact_kls = spec_klass == NULL ? profile_has_unique_klass() : spec_klass;
</pre>
<hr />
<pre>
3000     }
3001   }
3002 
3003   if (!known_statically) {
3004     const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
3005     // We may not have profiling here or it may not help us. If we
3006     // have a speculative type use it to perform an exact cast.
3007     ciKlass* spec_obj_type = obj_type-&gt;speculative_type();
3008     if (spec_obj_type != NULL || (ProfileDynamicTypes &amp;&amp; data != NULL)) {
3009       Node* cast_obj = maybe_cast_profiled_receiver(not_null_obj, NULL, spec_obj_type, safe_for_replace);
3010       if (stopped()) {            // Profile disagrees with this path.
3011         set_control(null_ctl);    // Null is the only remaining possibility.
3012         return intcon(0);
3013       }
3014       if (cast_obj != NULL) {
3015         not_null_obj = cast_obj;
3016       }
3017     }
3018   }
3019 
<span class="line-removed">3020   // Load the object&#39;s klass</span>
<span class="line-removed">3021   Node* obj_klass = load_object_klass(not_null_obj);</span>
<span class="line-removed">3022 </span>
3023   // Generate the subtype check
<span class="line-modified">3024   Node* not_subtype_ctrl = gen_subtype_check(obj_klass, superklass);</span>
3025 
3026   // Plug in the success path to the general merge in slot 1.
3027   region-&gt;init_req(_obj_path, control());
3028   phi   -&gt;init_req(_obj_path, intcon(1));
3029 
3030   // Plug in the failing path to the general merge in slot 2.
3031   region-&gt;init_req(_fail_path, not_subtype_ctrl);
3032   phi   -&gt;init_req(_fail_path, intcon(0));
3033 
3034   // Return final merged results
3035   set_control( _gvn.transform(region) );
3036   record_for_igvn(region);
3037 
3038   // If we know the type check always succeeds then we don&#39;t use the
3039   // profiling data at this bytecode. Don&#39;t lose it, feed it to the
3040   // type system as a speculative type.
3041   if (safe_for_replace) {
3042     Node* casted_obj = record_profiled_receiver_for_speculation(obj);
3043     replace_in_map(obj, casted_obj);
3044   }
</pre>
<hr />
<pre>
3127     // The following optimization tries to statically cast the speculative type of the object
3128     // (for example obtained during profiling) to the type of the superklass and then do a
3129     // dynamic check that the type of the object is what we expect. To work correctly
3130     // for checkcast and aastore the type of superklass should be exact.
3131     const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
3132     // We may not have profiling here or it may not help us. If we have
3133     // a speculative type use it to perform an exact cast.
3134     ciKlass* spec_obj_type = obj_type-&gt;speculative_type();
3135     if (spec_obj_type != NULL || data != NULL) {
3136       cast_obj = maybe_cast_profiled_receiver(not_null_obj, tk-&gt;klass(), spec_obj_type, safe_for_replace);
3137       if (cast_obj != NULL) {
3138         if (failure_control != NULL) // failure is now impossible
3139           (*failure_control) = top();
3140         // adjust the type of the phi to the exact klass:
3141         phi-&gt;raise_bottom_type(_gvn.type(cast_obj)-&gt;meet_speculative(TypePtr::NULL_PTR));
3142       }
3143     }
3144   }
3145 
3146   if (cast_obj == NULL) {
<span class="line-removed">3147     // Load the object&#39;s klass</span>
<span class="line-removed">3148     Node* obj_klass = load_object_klass(not_null_obj);</span>
<span class="line-removed">3149 </span>
3150     // Generate the subtype check
<span class="line-modified">3151     Node* not_subtype_ctrl = gen_subtype_check( obj_klass, superklass );</span>
3152 
3153     // Plug in success path into the merge
3154     cast_obj = _gvn.transform(new CheckCastPPNode(control(), not_null_obj, toop));
3155     // Failure path ends in uncommon trap (or may be dead - failure impossible)
3156     if (failure_control == NULL) {
3157       if (not_subtype_ctrl != top()) { // If failure is possible
3158         PreserveJVMState pjvms(this);
3159         set_control(not_subtype_ctrl);
<span class="line-modified">3160         builtin_throw(Deoptimization::Reason_class_check, obj_klass);</span>
3161       }
3162     } else {
3163       (*failure_control) = not_subtype_ctrl;
3164     }
3165   }
3166 
3167   region-&gt;init_req(_obj_path, control());
3168   phi   -&gt;init_req(_obj_path, cast_obj);
3169 
3170   // A merge of NULL or Casted-NotNull obj
3171   Node* res = _gvn.transform(phi);
3172 
3173   // Note I do NOT always &#39;replace_in_map(obj,result)&#39; here.
3174   //  if( tk-&gt;klass()-&gt;can_be_primary_super()  )
3175     // This means that if I successfully store an Object into an array-of-String
3176     // I &#39;forget&#39; that the Object is really now known to be a String.  I have to
3177     // do this because we don&#39;t have true union types for interfaces - if I store
3178     // a Baz into an array-of-Interface and then tell the optimizer it&#39;s an
3179     // Interface, I forget that it&#39;s also a Baz and cannot do Baz-like field
3180     // references to it.  FIX THIS WHEN UNION TYPES APPEAR!
</pre>
<hr />
<pre>
3240   } else {
3241     set_memory(_gvn.transform(new ProjNode(membar, TypeFunc::Memory)),alias_idx);
3242   }
3243   return membar;
3244 }
3245 
3246 //------------------------------shared_lock------------------------------------
3247 // Emit locking code.
3248 FastLockNode* GraphKit::shared_lock(Node* obj) {
3249   // bci is either a monitorenter bc or InvocationEntryBci
3250   // %%% SynchronizationEntryBCI is redundant; use InvocationEntryBci in interfaces
3251   assert(SynchronizationEntryBCI == InvocationEntryBci, &quot;&quot;);
3252 
3253   if( !GenerateSynchronizationCode )
3254     return NULL;                // Not locking things?
3255   if (stopped())                // Dead monitor?
3256     return NULL;
3257 
3258   assert(dead_locals_are_killed(), &quot;should kill locals before sync. point&quot;);
3259 
<span class="line-removed">3260   obj = access_resolve(obj, ACCESS_READ | ACCESS_WRITE);</span>
<span class="line-removed">3261 </span>
3262   // Box the stack location
3263   Node* box = _gvn.transform(new BoxLockNode(next_monitor()));
3264   Node* mem = reset_memory();
3265 
3266   FastLockNode * flock = _gvn.transform(new FastLockNode(0, obj, box) )-&gt;as_FastLock();
3267   if (UseBiasedLocking &amp;&amp; PrintPreciseBiasedLockingStatistics) {
3268     // Create the counters for this fast lock.
3269     flock-&gt;create_lock_counter(sync_jvms()); // sync_jvms used to get current bci
3270   }
3271 
3272   // Create the rtm counters for this fast lock if needed.
3273   flock-&gt;create_rtm_lock_counter(sync_jvms()); // sync_jvms used to get current bci
3274 
3275   // Add monitor to debug info for the slow path.  If we block inside the
3276   // slow path and de-opt, we need the monitor hanging around
3277   map()-&gt;push_monitor( flock );
3278 
3279   const TypeFunc *tf = LockNode::lock_type();
3280   LockNode *lock = new LockNode(C, tf);
3281 
</pre>
<hr />
<pre>
3966   set_memory(res_mem, TypeAryPtr::BYTES);
3967   return str;
3968 }
3969 
3970 void GraphKit::inflate_string(Node* src, Node* dst, const TypeAryPtr* dst_type, Node* count) {
3971   assert(Matcher::match_rule_supported(Op_StrInflatedCopy), &quot;Intrinsic not supported&quot;);
3972   assert(dst_type == TypeAryPtr::BYTES || dst_type == TypeAryPtr::CHARS, &quot;invalid dest type&quot;);
3973   // Capture src and dst memory (see comment in &#39;compress_string&#39;).
3974   Node* mem = capture_memory(TypeAryPtr::BYTES, dst_type);
3975   StrInflatedCopyNode* str = new StrInflatedCopyNode(control(), mem, src, dst, count);
3976   set_memory(_gvn.transform(str), dst_type);
3977 }
3978 
3979 void GraphKit::inflate_string_slow(Node* src, Node* dst, Node* start, Node* count) {
3980   /**
3981    * int i_char = start;
3982    * for (int i_byte = 0; i_byte &lt; count; i_byte++) {
3983    *   dst[i_char++] = (char)(src[i_byte] &amp; 0xff);
3984    * }
3985    */
<span class="line-removed">3986   src = access_resolve(src, ACCESS_READ);</span>
<span class="line-removed">3987   dst = access_resolve(dst, ACCESS_WRITE);</span>
3988   add_predicate();
3989   RegionNode* head = new RegionNode(3);
3990   head-&gt;init_req(1, control());
3991   gvn().set_type(head, Type::CONTROL);
3992   record_for_igvn(head);
3993 
3994   Node* i_byte = new PhiNode(head, TypeInt::INT);
3995   i_byte-&gt;init_req(1, intcon(0));
3996   gvn().set_type(i_byte, TypeInt::INT);
3997   record_for_igvn(i_byte);
3998 
3999   Node* i_char = new PhiNode(head, TypeInt::INT);
4000   i_char-&gt;init_req(1, start);
4001   gvn().set_type(i_char, TypeInt::INT);
4002   record_for_igvn(i_char);
4003 
4004   Node* mem = PhiNode::make(head, memory(TypeAryPtr::BYTES), Type::MEMORY, TypeAryPtr::BYTES);
4005   gvn().set_type(mem, Type::MEMORY);
4006   record_for_igvn(mem);
4007   set_control(head);
</pre>
</td>
<td>
<hr />
<pre>
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;ci/ciUtilities.hpp&quot;
  27 #include &quot;compiler/compileLog.hpp&quot;
  28 #include &quot;gc/shared/barrierSet.hpp&quot;
  29 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  30 #include &quot;interpreter/interpreter.hpp&quot;
  31 #include &quot;memory/resourceArea.hpp&quot;
  32 #include &quot;opto/addnode.hpp&quot;
  33 #include &quot;opto/castnode.hpp&quot;
  34 #include &quot;opto/convertnode.hpp&quot;
  35 #include &quot;opto/graphKit.hpp&quot;
  36 #include &quot;opto/idealKit.hpp&quot;
  37 #include &quot;opto/intrinsicnode.hpp&quot;
  38 #include &quot;opto/locknode.hpp&quot;
  39 #include &quot;opto/machnode.hpp&quot;
  40 #include &quot;opto/opaquenode.hpp&quot;
  41 #include &quot;opto/parse.hpp&quot;
  42 #include &quot;opto/rootnode.hpp&quot;
  43 #include &quot;opto/runtime.hpp&quot;
<span class="line-added">  44 #include &quot;opto/subtypenode.hpp&quot;</span>
  45 #include &quot;runtime/deoptimization.hpp&quot;
  46 #include &quot;runtime/sharedRuntime.hpp&quot;
<span class="line-added">  47 #include &quot;utilities/bitMap.inline.hpp&quot;</span>
<span class="line-added">  48 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  49 
  50 //----------------------------GraphKit-----------------------------------------
  51 // Main utility constructor.
  52 GraphKit::GraphKit(JVMState* jvms)
  53   : Phase(Phase::Parser),
  54     _env(C-&gt;env()),
  55     _gvn(*C-&gt;initial_gvn()),
  56     _barrier_set(BarrierSet::barrier_set()-&gt;barrier_set_c2())
  57 {
  58   _exceptions = jvms-&gt;map()-&gt;next_exception();
  59   if (_exceptions != NULL)  jvms-&gt;map()-&gt;set_next_exception(NULL);
  60   set_jvms(jvms);
  61 }
  62 
  63 // Private constructor for parser.
  64 GraphKit::GraphKit()
  65   : Phase(Phase::Parser),
  66     _env(C-&gt;env()),
  67     _gvn(*C-&gt;initial_gvn()),
  68     _barrier_set(BarrierSet::barrier_set()-&gt;barrier_set_c2())
</pre>
<hr />
<pre>
1348                     NULL, &quot;assert_null&quot;);
1349     } else {
1350       replace_in_map(value, zerocon(type));
1351       builtin_throw(reason);
1352     }
1353   }
1354 
1355   // Must throw exception, fall-thru not possible?
1356   if (stopped()) {
1357     return top();               // No result
1358   }
1359 
1360   if (assert_null) {
1361     // Cast obj to null on this path.
1362     replace_in_map(value, zerocon(type));
1363     return zerocon(type);
1364   }
1365 
1366   // Cast obj to not-null on this path, if there is no null_control.
1367   // (If there is a null_control, a non-null value may come back to haunt us.)
<span class="line-modified">1368   return cast_not_null(value, (null_control == NULL || (*null_control) == top()));</span>







1369 }
1370 
1371 
1372 //------------------------------cast_not_null----------------------------------
1373 // Cast obj to not-null on this path
1374 Node* GraphKit::cast_not_null(Node* obj, bool do_replace_in_map) {
<span class="line-modified">1375   Node* cast = NULL;</span>
<span class="line-modified">1376   const Type* t = _gvn.type(obj);</span>
<span class="line-modified">1377   if (t-&gt;make_ptr() != NULL) {</span>
<span class="line-modified">1378     const Type* t_not_null = t-&gt;join_speculative(TypePtr::NOTNULL);</span>
<span class="line-modified">1379     // Object is already not-null?</span>
<span class="line-modified">1380     if (t == t_not_null) {</span>
<span class="line-modified">1381       return obj;</span>
<span class="line-modified">1382     }</span>
<span class="line-added">1383     cast = ConstraintCastNode::make_cast(Op_CastPP, control(), obj, t_not_null, false);</span>
<span class="line-added">1384   } else if (t-&gt;isa_int() != NULL) {</span>
<span class="line-added">1385     cast = ConstraintCastNode::make_cast(Op_CastII, control(), obj, TypeInt::INT, true);</span>
<span class="line-added">1386   } else if (t-&gt;isa_long() != NULL) {</span>
<span class="line-added">1387     cast = ConstraintCastNode::make_cast(Op_CastLL, control(), obj, TypeLong::LONG, true);</span>
<span class="line-added">1388   } else {</span>
<span class="line-added">1389     fatal(&quot;unexpected type: %s&quot;, type2name(t-&gt;basic_type()));</span>
<span class="line-added">1390   }</span>
<span class="line-added">1391   cast = _gvn.transform(cast);</span>
1392 
1393   // Scan for instances of &#39;obj&#39; in the current JVM mapping.
1394   // These instances are known to be not-null after the test.
<span class="line-modified">1395   if (do_replace_in_map) {</span>
1396     replace_in_map(obj, cast);
<span class="line-modified">1397   }</span>
<span class="line-modified">1398   return cast;</span>
1399 }
1400 
1401 // Sometimes in intrinsics, we implicitly know an object is not null
1402 // (there&#39;s no actual null check) so we can cast it to not null. In
1403 // the course of optimizations, the input to the cast can become null.
1404 // In that case that data path will die and we need the control path
1405 // to become dead as well to keep the graph consistent. So we have to
1406 // add a check for null for which one branch can&#39;t be taken. It uses
1407 // an Opaque4 node that will cause the check to be removed after loop
1408 // opts so the test goes away and the compiled code doesn&#39;t execute a
1409 // useless check.
1410 Node* GraphKit::must_be_not_null(Node* value, bool do_replace_in_map) {
<span class="line-added">1411   if (!TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(value))) {</span>
<span class="line-added">1412     return value;</span>
<span class="line-added">1413   }</span>
1414   Node* chk = _gvn.transform(new CmpPNode(value, null()));
1415   Node *tst = _gvn.transform(new BoolNode(chk, BoolTest::ne));
1416   Node* opaq = _gvn.transform(new Opaque4Node(C, tst, intcon(1)));
1417   IfNode *iff = new IfNode(control(), opaq, PROB_MAX, COUNT_UNKNOWN);
1418   _gvn.set_type(iff, iff-&gt;Value(&amp;_gvn));
1419   Node *if_f = _gvn.transform(new IfFalseNode(iff));
1420   Node *frame = _gvn.transform(new ParmNode(C-&gt;start(), TypeFunc::FramePtr));
<span class="line-modified">1421   Node* halt = _gvn.transform(new HaltNode(if_f, frame, &quot;unexpected null in intrinsic&quot;));</span>
1422   C-&gt;root()-&gt;add_req(halt);
1423   Node *if_t = _gvn.transform(new IfTrueNode(iff));
1424   set_control(if_t);
1425   return cast_not_null(value, do_replace_in_map);
1426 }
1427 
1428 
1429 //--------------------------replace_in_map-------------------------------------
1430 void GraphKit::replace_in_map(Node* old, Node* neww) {
1431   if (old == neww) {
1432     return;
1433   }
1434 
1435   map()-&gt;replace_edge(old, neww);
1436 
1437   // Note: This operation potentially replaces any edge
1438   // on the map.  This includes locals, stack, and monitors
1439   // of the current (innermost) JVM state.
1440 
1441   // don&#39;t let inconsistent types from profiling escape this
</pre>
<hr />
<pre>
1481   Node* newmem = _gvn.transform( new ProjNode(call, TypeFunc::Memory, separate_io_proj) );
1482   set_all_memory(newmem);
1483 }
1484 
1485 //=============================================================================
1486 //
1487 // parser factory methods for MemNodes
1488 //
1489 // These are layered on top of the factory methods in LoadNode and StoreNode,
1490 // and integrate with the parser&#39;s memory state and _gvn engine.
1491 //
1492 
1493 // factory methods in &quot;int adr_idx&quot;
1494 Node* GraphKit::make_load(Node* ctl, Node* adr, const Type* t, BasicType bt,
1495                           int adr_idx,
1496                           MemNode::MemOrd mo,
1497                           LoadNode::ControlDependency control_dependency,
1498                           bool require_atomic_access,
1499                           bool unaligned,
1500                           bool mismatched,
<span class="line-modified">1501                           bool unsafe,</span>
<span class="line-added">1502                           uint8_t barrier_data) {</span>
1503   assert(adr_idx != Compile::AliasIdxTop, &quot;use other make_load factory&quot; );
1504   const TypePtr* adr_type = NULL; // debug-mode-only argument
1505   debug_only(adr_type = C-&gt;get_adr_type(adr_idx));
1506   Node* mem = memory(adr_idx);
1507   Node* ld;
1508   if (require_atomic_access &amp;&amp; bt == T_LONG) {
<span class="line-modified">1509     ld = LoadLNode::make_atomic(ctl, mem, adr, adr_type, t, mo, control_dependency, unaligned, mismatched, unsafe, barrier_data);</span>
1510   } else if (require_atomic_access &amp;&amp; bt == T_DOUBLE) {
<span class="line-modified">1511     ld = LoadDNode::make_atomic(ctl, mem, adr, adr_type, t, mo, control_dependency, unaligned, mismatched, unsafe, barrier_data);</span>
1512   } else {
<span class="line-modified">1513     ld = LoadNode::make(_gvn, ctl, mem, adr, adr_type, t, bt, mo, control_dependency, unaligned, mismatched, unsafe, barrier_data);</span>
1514   }
1515   ld = _gvn.transform(ld);
1516   if (((bt == T_OBJECT) &amp;&amp; C-&gt;do_escape_analysis()) || C-&gt;eliminate_boxing()) {
1517     // Improve graph before escape analysis and boxing elimination.
1518     record_for_igvn(ld);
1519   }
1520   return ld;
1521 }
1522 
1523 Node* GraphKit::store_to_memory(Node* ctl, Node* adr, Node *val, BasicType bt,
1524                                 int adr_idx,
1525                                 MemNode::MemOrd mo,
1526                                 bool require_atomic_access,
1527                                 bool unaligned,
1528                                 bool mismatched,
1529                                 bool unsafe) {
1530   assert(adr_idx != Compile::AliasIdxTop, &quot;use other store_to_memory factory&quot; );
1531   const TypePtr* adr_type = NULL;
1532   debug_only(adr_type = C-&gt;get_adr_type(adr_idx));
1533   Node *mem = memory(adr_idx);
</pre>
<hr />
<pre>
1684                                      Node* adr,
1685                                      const TypePtr* adr_type,
1686                                      int alias_idx,
1687                                      Node* new_val,
1688                                      const Type* value_type,
1689                                      BasicType bt,
1690                                      DecoratorSet decorators) {
1691   C2AccessValuePtr addr(adr, adr_type);
1692   C2AtomicParseAccess access(this, decorators | C2_READ_ACCESS | C2_WRITE_ACCESS, bt, obj, addr, alias_idx);
1693   if (access.is_raw()) {
1694     return _barrier_set-&gt;BarrierSetC2::atomic_add_at(access, new_val, value_type);
1695   } else {
1696     return _barrier_set-&gt;atomic_add_at(access, new_val, value_type);
1697   }
1698 }
1699 
1700 void GraphKit::access_clone(Node* src, Node* dst, Node* size, bool is_array) {
1701   return _barrier_set-&gt;clone(this, src, dst, size, is_array);
1702 }
1703 








1704 //-------------------------array_element_address-------------------------
1705 Node* GraphKit::array_element_address(Node* ary, Node* idx, BasicType elembt,
1706                                       const TypeInt* sizetype, Node* ctrl) {
1707   uint shift  = exact_log2(type2aelembytes(elembt));
1708   uint header = arrayOopDesc::base_offset_in_bytes(elembt);
1709 
1710   // short-circuit a common case (saves lots of confusing waste motion)
1711   jint idx_con = find_int_con(idx, -1);
1712   if (idx_con &gt;= 0) {
1713     intptr_t offset = header + ((intptr_t)idx_con &lt;&lt; shift);
1714     return basic_plus_adr(ary, offset);
1715   }
1716 
1717   // must be correct type for alignment purposes
1718   Node* base  = basic_plus_adr(ary, header);
1719   idx = Compile::conv_I2X_index(&amp;_gvn, idx, sizetype, ctrl);
1720   Node* scale = _gvn.transform( new LShiftXNode(idx, intcon(shift)) );
1721   return basic_plus_adr(ary, base, scale);
1722 }
1723 
</pre>
<hr />
<pre>
1843     if (hook_mem != NULL) {
1844       // Make memory for the call
1845       Node* mem = _gvn.transform( new ProjNode(call, TypeFunc::Memory) );
1846       // Set the RawPtr memory state only.  This covers all the heap top/GC stuff
1847       // We also use hook_mem to extract specific effects from arraycopy stubs.
1848       set_memory(mem, hook_mem);
1849     }
1850     // ...else the call has NO memory effects.
1851 
1852     // Make sure the call advertises its memory effects precisely.
1853     // This lets us build accurate anti-dependences in gcm.cpp.
1854     assert(C-&gt;alias_type(call-&gt;adr_type()) == C-&gt;alias_type(hook_mem),
1855            &quot;call node must be constructed correctly&quot;);
1856   } else {
1857     assert(hook_mem == NULL, &quot;&quot;);
1858     // This is not a &quot;slow path&quot; call; all memory comes from the call.
1859     set_all_memory_call(call);
1860   }
1861 }
1862 
<span class="line-added">1863 // Keep track of MergeMems feeding into other MergeMems</span>
<span class="line-added">1864 static void add_mergemem_users_to_worklist(Unique_Node_List&amp; wl, Node* mem) {</span>
<span class="line-added">1865   if (!mem-&gt;is_MergeMem()) {</span>
<span class="line-added">1866     return;</span>
<span class="line-added">1867   }</span>
<span class="line-added">1868   for (SimpleDUIterator i(mem); i.has_next(); i.next()) {</span>
<span class="line-added">1869     Node* use = i.get();</span>
<span class="line-added">1870     if (use-&gt;is_MergeMem()) {</span>
<span class="line-added">1871       wl.push(use);</span>
<span class="line-added">1872     }</span>
<span class="line-added">1873   }</span>
<span class="line-added">1874 }</span>
1875 
1876 // Replace the call with the current state of the kit.
1877 void GraphKit::replace_call(CallNode* call, Node* result, bool do_replaced_nodes) {
1878   JVMState* ejvms = NULL;
1879   if (has_exceptions()) {
1880     ejvms = transfer_exceptions_into_jvms();
1881   }
1882 
1883   ReplacedNodes replaced_nodes = map()-&gt;replaced_nodes();
1884   ReplacedNodes replaced_nodes_exception;
1885   Node* ex_ctl = top();
1886 
1887   SafePointNode* final_state = stop();
1888 
1889   // Find all the needed outputs of this call
1890   CallProjections callprojs;
1891   call-&gt;extract_projections(&amp;callprojs, true);
1892 
<span class="line-added">1893   Unique_Node_List wl;</span>
1894   Node* init_mem = call-&gt;in(TypeFunc::Memory);
1895   Node* final_mem = final_state-&gt;in(TypeFunc::Memory);
1896   Node* final_ctl = final_state-&gt;in(TypeFunc::Control);
1897   Node* final_io = final_state-&gt;in(TypeFunc::I_O);
1898 
1899   // Replace all the old call edges with the edges from the inlining result
1900   if (callprojs.fallthrough_catchproj != NULL) {
1901     C-&gt;gvn_replace_by(callprojs.fallthrough_catchproj, final_ctl);
1902   }
1903   if (callprojs.fallthrough_memproj != NULL) {
1904     if (final_mem-&gt;is_MergeMem()) {
1905       // Parser&#39;s exits MergeMem was not transformed but may be optimized
1906       final_mem = _gvn.transform(final_mem);
1907     }
1908     C-&gt;gvn_replace_by(callprojs.fallthrough_memproj,   final_mem);
<span class="line-added">1909     add_mergemem_users_to_worklist(wl, final_mem);</span>
1910   }
1911   if (callprojs.fallthrough_ioproj != NULL) {
1912     C-&gt;gvn_replace_by(callprojs.fallthrough_ioproj,    final_io);
1913   }
1914 
1915   // Replace the result with the new result if it exists and is used
1916   if (callprojs.resproj != NULL &amp;&amp; result != NULL) {
1917     C-&gt;gvn_replace_by(callprojs.resproj, result);
1918   }
1919 
1920   if (ejvms == NULL) {
1921     // No exception edges to simply kill off those paths
1922     if (callprojs.catchall_catchproj != NULL) {
1923       C-&gt;gvn_replace_by(callprojs.catchall_catchproj, C-&gt;top());
1924     }
1925     if (callprojs.catchall_memproj != NULL) {
1926       C-&gt;gvn_replace_by(callprojs.catchall_memproj,   C-&gt;top());
1927     }
1928     if (callprojs.catchall_ioproj != NULL) {
1929       C-&gt;gvn_replace_by(callprojs.catchall_ioproj,    C-&gt;top());
1930     }
1931     // Replace the old exception object with top
1932     if (callprojs.exobj != NULL) {
1933       C-&gt;gvn_replace_by(callprojs.exobj, C-&gt;top());
1934     }
1935   } else {
1936     GraphKit ekit(ejvms);
1937 
1938     // Load my combined exception state into the kit, with all phis transformed:
1939     SafePointNode* ex_map = ekit.combine_and_pop_all_exception_states();
1940     replaced_nodes_exception = ex_map-&gt;replaced_nodes();
1941 
1942     Node* ex_oop = ekit.use_exception_state(ex_map);
1943 
1944     if (callprojs.catchall_catchproj != NULL) {
1945       C-&gt;gvn_replace_by(callprojs.catchall_catchproj, ekit.control());
1946       ex_ctl = ekit.control();
1947     }
1948     if (callprojs.catchall_memproj != NULL) {
<span class="line-modified">1949       Node* ex_mem = ekit.reset_memory();</span>
<span class="line-added">1950       C-&gt;gvn_replace_by(callprojs.catchall_memproj,   ex_mem);</span>
<span class="line-added">1951       add_mergemem_users_to_worklist(wl, ex_mem);</span>
1952     }
1953     if (callprojs.catchall_ioproj != NULL) {
1954       C-&gt;gvn_replace_by(callprojs.catchall_ioproj,    ekit.i_o());
1955     }
1956 
1957     // Replace the old exception object with the newly created one
1958     if (callprojs.exobj != NULL) {
1959       C-&gt;gvn_replace_by(callprojs.exobj, ex_oop);
1960     }
1961   }
1962 
1963   // Disconnect the call from the graph
1964   call-&gt;disconnect_inputs(NULL, C);
1965   C-&gt;gvn_replace_by(call, C-&gt;top());
1966 
1967   // Clean up any MergeMems that feed other MergeMems since the
1968   // optimizer doesn&#39;t like that.
<span class="line-modified">1969   while (wl.size() &gt; 0) {</span>
<span class="line-modified">1970     _gvn.transform(wl.pop());</span>









1971   }
1972 
1973   if (callprojs.fallthrough_catchproj != NULL &amp;&amp; !final_ctl-&gt;is_top() &amp;&amp; do_replaced_nodes) {
1974     replaced_nodes.apply(C, final_ctl);
1975   }
1976   if (!ex_ctl-&gt;is_top() &amp;&amp; do_replaced_nodes) {
1977     replaced_nodes_exception.apply(C, ex_ctl);
1978   }
1979 }
1980 
1981 
1982 //------------------------------increment_counter------------------------------
1983 // for statistics: increment a VM counter by 1
1984 
1985 void GraphKit::increment_counter(address counter_addr) {
1986   Node* adr1 = makecon(TypeRawPtr::make(counter_addr));
1987   increment_counter(adr1);
1988 }
1989 
1990 void GraphKit::increment_counter(Node* counter_addr) {
</pre>
<hr />
<pre>
2098   // Clear out dead values from the debug info.
2099   kill_dead_locals();
2100 
2101   // Now insert the uncommon trap subroutine call
2102   address call_addr = SharedRuntime::uncommon_trap_blob()-&gt;entry_point();
2103   const TypePtr* no_memory_effects = NULL;
2104   // Pass the index of the class to be loaded
2105   Node* call = make_runtime_call(RC_NO_LEAF | RC_UNCOMMON |
2106                                  (must_throw ? RC_MUST_THROW : 0),
2107                                  OptoRuntime::uncommon_trap_Type(),
2108                                  call_addr, &quot;uncommon_trap&quot;, no_memory_effects,
2109                                  intcon(trap_request));
2110   assert(call-&gt;as_CallStaticJava()-&gt;uncommon_trap_request() == trap_request,
2111          &quot;must extract request correctly from the graph&quot;);
2112   assert(trap_request != 0, &quot;zero value reserved by uncommon_trap_request&quot;);
2113 
2114   call-&gt;set_req(TypeFunc::ReturnAdr, returnadr());
2115   // The debug info is the only real input to this call.
2116 
2117   // Halt-and-catch fire here.  The above call should never return!
<span class="line-modified">2118   HaltNode* halt = new HaltNode(control(), frameptr(), &quot;uncommon trap returned which should never happen&quot;);</span>
2119   _gvn.set_type_bottom(halt);
2120   root()-&gt;add_req(halt);
2121 
2122   stop_and_kill_map();
2123 }
2124 
2125 
2126 //--------------------------just_allocated_object------------------------------
2127 // Report the object that was just allocated.
2128 // It must be the case that there are no intervening safepoints.
2129 // We use this to determine if an object is so &quot;fresh&quot; that
2130 // it does not require card marks.
2131 Node* GraphKit::just_allocated_object(Node* current_control) {
2132   Node* ctrl = current_control;
2133   // Object::&lt;init&gt; is invoked after allocation, most of invoke nodes
2134   // will be reduced, but a region node is kept in parse time, we check
2135   // the pattern and skip the region node if it degraded to a copy.
2136   if (ctrl != NULL &amp;&amp; ctrl-&gt;is_Region() &amp;&amp; ctrl-&gt;req() == 2 &amp;&amp;
2137       ctrl-&gt;as_Region()-&gt;is_copy()) {
2138     ctrl = ctrl-&gt;as_Region()-&gt;is_copy();
2139   }
2140   if (C-&gt;recent_alloc_ctl() == ctrl) {
2141    return C-&gt;recent_alloc_obj();
2142   }
2143   return NULL;
2144 }
2145 
2146 
















2147 /**
2148  * Record profiling data exact_kls for Node n with the type system so
2149  * that it can propagate it (speculation)
2150  *
2151  * @param n          node that the type applies to
2152  * @param exact_kls  type from profiling
2153  * @param maybe_null did profiling see null?
2154  *
2155  * @return           node with improved type
2156  */
2157 Node* GraphKit::record_profile_for_speculation(Node* n, ciKlass* exact_kls, ProfilePtrKind ptr_kind) {
2158   const Type* current_type = _gvn.type(n);
2159   assert(UseTypeSpeculation, &quot;type speculation must be on&quot;);
2160 
2161   const TypePtr* speculative = current_type-&gt;speculative();
2162 
2163   // Should the klass from the profile be recorded in the speculative type?
2164   if (current_type-&gt;would_improve_type(exact_kls, jvms()-&gt;depth())) {
2165     const TypeKlassPtr* tklass = TypeKlassPtr::make(exact_kls);
2166     const TypeOopPtr* xtype = tklass-&gt;as_instance_type();
</pre>
<hr />
<pre>
2242   }
2243   return record_profile_for_speculation(n, exact_kls, ptr_kind);
2244 }
2245 
2246 /**
2247  * Record profiling data from argument profiling at an invoke with the
2248  * type system so that it can propagate it (speculation)
2249  *
2250  * @param dest_method  target method for the call
2251  * @param bc           what invoke bytecode is this?
2252  */
2253 void GraphKit::record_profiled_arguments_for_speculation(ciMethod* dest_method, Bytecodes::Code bc) {
2254   if (!UseTypeSpeculation) {
2255     return;
2256   }
2257   const TypeFunc* tf    = TypeFunc::make(dest_method);
2258   int             nargs = tf-&gt;domain()-&gt;cnt() - TypeFunc::Parms;
2259   int skip = Bytecodes::has_receiver(bc) ? 1 : 0;
2260   for (int j = skip, i = 0; j &lt; nargs &amp;&amp; i &lt; TypeProfileArgsLimit; j++) {
2261     const Type *targ = tf-&gt;domain()-&gt;field_at(j + TypeFunc::Parms);
<span class="line-modified">2262     if (is_reference_type(targ-&gt;basic_type())) {</span>
2263       ProfilePtrKind ptr_kind = ProfileMaybeNull;
2264       ciKlass* better_type = NULL;
2265       if (method()-&gt;argument_profiled_type(bci(), i, better_type, ptr_kind)) {
2266         record_profile_for_speculation(argument(j), better_type, ptr_kind);
2267       }
2268       i++;
2269     }
2270   }
2271 }
2272 
2273 /**
2274  * Record profiling data from parameter profiling at an invoke with
2275  * the type system so that it can propagate it (speculation)
2276  */
2277 void GraphKit::record_profiled_parameters_for_speculation() {
2278   if (!UseTypeSpeculation) {
2279     return;
2280   }
2281   for (int i = 0, j = 0; i &lt; method()-&gt;arg_size() ; i++) {
2282     if (_gvn.type(local(i))-&gt;isa_oopptr()) {
</pre>
<hr />
<pre>
2292 
2293 /**
2294  * Record profiling data from return value profiling at an invoke with
2295  * the type system so that it can propagate it (speculation)
2296  */
2297 void GraphKit::record_profiled_return_for_speculation() {
2298   if (!UseTypeSpeculation) {
2299     return;
2300   }
2301   ProfilePtrKind ptr_kind = ProfileMaybeNull;
2302   ciKlass* better_type = NULL;
2303   if (method()-&gt;return_profiled_type(bci(), better_type, ptr_kind)) {
2304     // If profiling reports a single type for the return value,
2305     // feed it to the type system so it can propagate it as a
2306     // speculative type
2307     record_profile_for_speculation(stack(sp()-1), better_type, ptr_kind);
2308   }
2309 }
2310 
2311 void GraphKit::round_double_result(ciMethod* dest_method) {
<span class="line-modified">2312   if (Matcher::strict_fp_requires_explicit_rounding) {</span>
<span class="line-modified">2313     // If a strict caller invokes a non-strict callee, round a double result.</span>
<span class="line-modified">2314     // A non-strict method may return a double value which has an extended exponent,</span>
<span class="line-added">2315     // but this must not be visible in a caller which is strict.</span>
<span class="line-added">2316     BasicType result_type = dest_method-&gt;return_type()-&gt;basic_type();</span>
<span class="line-added">2317     assert(method() != NULL, &quot;must have caller context&quot;);</span>
<span class="line-added">2318     if( result_type == T_DOUBLE &amp;&amp; method()-&gt;is_strict() &amp;&amp; !dest_method-&gt;is_strict() ) {</span>
<span class="line-added">2319       // Destination method&#39;s return value is on top of stack</span>
<span class="line-added">2320       // dstore_rounding() does gvn.transform</span>
<span class="line-added">2321       Node *result = pop_pair();</span>
<span class="line-added">2322       result = dstore_rounding(result);</span>
<span class="line-added">2323       push_pair(result);</span>
<span class="line-added">2324     }</span>
<span class="line-added">2325   }</span>
<span class="line-added">2326 }</span>
2327 
<span class="line-modified">2328 void GraphKit::round_double_arguments(ciMethod* dest_method) {</span>
<span class="line-modified">2329   if (Matcher::strict_fp_requires_explicit_rounding) {</span>
<span class="line-modified">2330     // (Note:  TypeFunc::make has a cache that makes this fast.)</span>
<span class="line-modified">2331     const TypeFunc* tf    = TypeFunc::make(dest_method);</span>
<span class="line-modified">2332     int             nargs = tf-&gt;domain()-&gt;cnt() - TypeFunc::Parms;</span>
<span class="line-modified">2333     for (int j = 0; j &lt; nargs; j++) {</span>
<span class="line-modified">2334       const Type *targ = tf-&gt;domain()-&gt;field_at(j + TypeFunc::Parms);</span>
<span class="line-modified">2335       if (targ-&gt;basic_type() == T_DOUBLE) {</span>
<span class="line-added">2336         // If any parameters are doubles, they must be rounded before</span>
<span class="line-added">2337         // the call, dstore_rounding does gvn.transform</span>
<span class="line-added">2338         Node *arg = argument(j);</span>
<span class="line-added">2339         arg = dstore_rounding(arg);</span>
<span class="line-added">2340         set_argument(j, arg);</span>
<span class="line-added">2341       }</span>
<span class="line-added">2342     }</span>
2343   }
2344 }
2345 
2346 // rounding for strict float precision conformance
2347 Node* GraphKit::precision_rounding(Node* n) {
<span class="line-modified">2348   if (Matcher::strict_fp_requires_explicit_rounding) {</span>
<span class="line-modified">2349 #ifdef IA32</span>
<span class="line-modified">2350     if (_method-&gt;flags().is_strict() &amp;&amp; UseSSE == 0) {</span>
<span class="line-modified">2351       return _gvn.transform(new RoundFloatNode(0, n));</span>
<span class="line-added">2352     }</span>
<span class="line-added">2353 #else</span>
<span class="line-added">2354     Unimplemented();</span>
<span class="line-added">2355 #endif // IA32</span>
<span class="line-added">2356   }</span>
<span class="line-added">2357   return n;</span>
2358 }
2359 
2360 // rounding for strict double precision conformance
2361 Node* GraphKit::dprecision_rounding(Node *n) {
<span class="line-modified">2362   if (Matcher::strict_fp_requires_explicit_rounding) {</span>
<span class="line-modified">2363 #ifdef IA32</span>
<span class="line-modified">2364     if (_method-&gt;flags().is_strict() &amp;&amp; UseSSE &lt; 2) {</span>
<span class="line-modified">2365       return _gvn.transform(new RoundDoubleNode(0, n));</span>
<span class="line-added">2366     }</span>
<span class="line-added">2367 #else</span>
<span class="line-added">2368     Unimplemented();</span>
<span class="line-added">2369 #endif // IA32</span>
<span class="line-added">2370   }</span>
<span class="line-added">2371   return n;</span>
2372 }
2373 
2374 // rounding for non-strict double stores
2375 Node* GraphKit::dstore_rounding(Node* n) {
<span class="line-modified">2376   if (Matcher::strict_fp_requires_explicit_rounding) {</span>
<span class="line-modified">2377 #ifdef IA32</span>
<span class="line-modified">2378     if (UseSSE &lt; 2) {</span>
<span class="line-modified">2379       return _gvn.transform(new RoundDoubleNode(0, n));</span>
<span class="line-added">2380     }</span>
<span class="line-added">2381 #else</span>
<span class="line-added">2382     Unimplemented();</span>
<span class="line-added">2383 #endif // IA32</span>
<span class="line-added">2384   }</span>
<span class="line-added">2385   return n;</span>
2386 }
2387 
2388 //=============================================================================
2389 // Generate a fast path/slow path idiom.  Graph looks like:
2390 // [foo] indicates that &#39;foo&#39; is a parameter
2391 //
2392 //              [in]     NULL
2393 //                 \    /
2394 //                  CmpP
2395 //                  Bool ne
2396 //                   If
2397 //                  /  \
2398 //              True    False-&lt;2&gt;
2399 //              / |
2400 //             /  cast_not_null
2401 //           Load  |    |   ^
2402 //        [fast_test]   |   |
2403 // gvn to   opt_test    |   |
2404 //          /    \      |  &lt;1&gt;
2405 //      True     False  |
</pre>
<hr />
<pre>
2462   // Fast path taken; set region slot 2
2463   Node *fast_taken = _gvn.transform( new IfFalseNode(opt_iff) );
2464   region-&gt;init_req(2,fast_taken); // Capture fast-control
2465 
2466   // Fast path not-taken, i.e. slow path
2467   Node *slow_taken = _gvn.transform( new IfTrueNode(opt_iff) );
2468   return slow_taken;
2469 }
2470 
2471 //-----------------------------make_runtime_call-------------------------------
2472 Node* GraphKit::make_runtime_call(int flags,
2473                                   const TypeFunc* call_type, address call_addr,
2474                                   const char* call_name,
2475                                   const TypePtr* adr_type,
2476                                   // The following parms are all optional.
2477                                   // The first NULL ends the list.
2478                                   Node* parm0, Node* parm1,
2479                                   Node* parm2, Node* parm3,
2480                                   Node* parm4, Node* parm5,
2481                                   Node* parm6, Node* parm7) {
<span class="line-added">2482   assert(call_addr != NULL, &quot;must not call NULL targets&quot;);</span>
<span class="line-added">2483 </span>
2484   // Slow-path call
2485   bool is_leaf = !(flags &amp; RC_NO_LEAF);
2486   bool has_io  = (!is_leaf &amp;&amp; !(flags &amp; RC_NO_IO));
2487   if (call_name == NULL) {
2488     assert(!is_leaf, &quot;must supply name for leaf&quot;);
2489     call_name = OptoRuntime::stub_name(call_addr);
2490   }
2491   CallNode* call;
2492   if (!is_leaf) {
2493     call = new CallStaticJavaNode(call_type, call_addr, call_name,
2494                                            bci(), adr_type);
2495   } else if (flags &amp; RC_NO_FP) {
2496     call = new CallLeafNoFPNode(call_type, call_addr, call_name, adr_type);
2497   } else {
2498     call = new CallLeafNode(call_type, call_addr, call_name, adr_type);
2499   }
2500 
2501   // The following is similar to set_edges_for_java_call,
2502   // except that the memory effects of the call are restricted to AliasIdxRaw.
2503 
</pre>
<hr />
<pre>
2607 
2608     if (excp != top()) {
2609       if (deoptimize) {
2610         // Deoptimize if an exception is caught. Don&#39;t construct exception state in this case.
2611         uncommon_trap(Deoptimization::Reason_unhandled,
2612                       Deoptimization::Action_none);
2613       } else {
2614         // Create an exception state also.
2615         // Use an exact type if the caller has a specific exception.
2616         const Type* ex_type = TypeOopPtr::make_from_klass_unique(ex_klass)-&gt;cast_to_ptr_type(TypePtr::NotNull);
2617         Node*       ex_oop  = new CreateExNode(ex_type, control(), i_o);
2618         add_exception_state(make_exception_state(_gvn.transform(ex_oop)));
2619       }
2620     }
2621   }
2622 
2623   // Get the no-exception control from the CatchNode.
2624   set_control(norm);
2625 }
2626 
<span class="line-modified">2627 static IfNode* gen_subtype_check_compare(Node* ctrl, Node* in1, Node* in2, BoolTest::mask test, float p, PhaseGVN&amp; gvn, BasicType bt) {</span>
2628   Node* cmp = NULL;
2629   switch(bt) {
2630   case T_INT: cmp = new CmpINode(in1, in2); break;
2631   case T_ADDRESS: cmp = new CmpPNode(in1, in2); break;
2632   default: fatal(&quot;unexpected comparison type %s&quot;, type2name(bt));
2633   }
<span class="line-modified">2634   gvn.transform(cmp);</span>
<span class="line-modified">2635   Node* bol = gvn.transform(new BoolNode(cmp, test));</span>
2636   IfNode* iff = new IfNode(ctrl, bol, p, COUNT_UNKNOWN);
<span class="line-modified">2637   gvn.transform(iff);</span>
<span class="line-modified">2638   if (!bol-&gt;is_Con()) gvn.record_for_igvn(iff);</span>
2639   return iff;
2640 }
2641 
<span class="line-added">2642 // Find the memory state for the secondary super type cache load when</span>
<span class="line-added">2643 // a subtype check is expanded at macro expansion time. That field is</span>
<span class="line-added">2644 // mutable so should not use immutable memory but</span>
<span class="line-added">2645 // PartialSubtypeCheckNode that might modify it doesn&#39;t produce a new</span>
<span class="line-added">2646 // memory state so bottom memory is the most accurate memory state to</span>
<span class="line-added">2647 // hook the load with. This follows the implementation used when the</span>
<span class="line-added">2648 // subtype check is expanded at parse time.</span>
<span class="line-added">2649 static Node* find_bottom_mem(Node* ctrl, Compile* C) {</span>
<span class="line-added">2650   const TypePtr* adr_type = TypeKlassPtr::make(TypePtr::NotNull, C-&gt;env()-&gt;Object_klass(), Type::OffsetBot);</span>
<span class="line-added">2651   Node_Stack stack(0);</span>
<span class="line-added">2652   VectorSet seen(Thread::current()-&gt;resource_area());</span>
<span class="line-added">2653 </span>
<span class="line-added">2654   Node* c = ctrl;</span>
<span class="line-added">2655   Node* mem = NULL;</span>
<span class="line-added">2656   uint iter = 0;</span>
<span class="line-added">2657   do {</span>
<span class="line-added">2658     iter++;</span>
<span class="line-added">2659     assert(iter &lt; C-&gt;live_nodes(), &quot;infinite loop&quot;);</span>
<span class="line-added">2660     if (c-&gt;is_Region()) {</span>
<span class="line-added">2661       for (DUIterator_Fast imax, i = c-&gt;fast_outs(imax); i &lt; imax &amp;&amp; mem == NULL; i++) {</span>
<span class="line-added">2662         Node* u = c-&gt;fast_out(i);</span>
<span class="line-added">2663         if (u-&gt;is_Phi() &amp;&amp; u-&gt;bottom_type() == Type::MEMORY &amp;&amp;</span>
<span class="line-added">2664             (u-&gt;adr_type() == TypePtr::BOTTOM || u-&gt;adr_type() == adr_type)) {</span>
<span class="line-added">2665           mem = u;</span>
<span class="line-added">2666         }</span>
<span class="line-added">2667       }</span>
<span class="line-added">2668       if (mem == NULL) {</span>
<span class="line-added">2669         if (!seen.test_set(c-&gt;_idx)) {</span>
<span class="line-added">2670           stack.push(c, 2);</span>
<span class="line-added">2671           c = c-&gt;in(1);</span>
<span class="line-added">2672         } else {</span>
<span class="line-added">2673           Node* phi = NULL;</span>
<span class="line-added">2674           uint idx = 0;</span>
<span class="line-added">2675           for (;;) {</span>
<span class="line-added">2676             phi = stack.node();</span>
<span class="line-added">2677             idx = stack.index();</span>
<span class="line-added">2678             if (idx &lt; phi-&gt;req()) {</span>
<span class="line-added">2679               break;</span>
<span class="line-added">2680             }</span>
<span class="line-added">2681             stack.pop();</span>
<span class="line-added">2682           }</span>
<span class="line-added">2683           c = phi-&gt;in(idx);</span>
<span class="line-added">2684           stack.set_index(idx+1);</span>
<span class="line-added">2685         }</span>
<span class="line-added">2686       }</span>
<span class="line-added">2687     } else if (c-&gt;is_Proj() &amp;&amp; c-&gt;in(0)-&gt;adr_type() == TypePtr::BOTTOM) {</span>
<span class="line-added">2688       for (DUIterator_Fast imax, i = c-&gt;in(0)-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-added">2689         Node* u = c-&gt;in(0)-&gt;fast_out(i);</span>
<span class="line-added">2690         if (u-&gt;bottom_type() == Type::MEMORY &amp;&amp; u-&gt;as_Proj()-&gt;_is_io_use == c-&gt;as_Proj()-&gt;_is_io_use) {</span>
<span class="line-added">2691           assert(mem == NULL, &quot;&quot;);</span>
<span class="line-added">2692           mem = u;</span>
<span class="line-added">2693         }</span>
<span class="line-added">2694       }</span>
<span class="line-added">2695     } else if (c-&gt;is_CatchProj() &amp;&amp; c-&gt;in(0)-&gt;in(0)-&gt;in(0)-&gt;adr_type() == TypePtr::BOTTOM) {</span>
<span class="line-added">2696       Node* call = c-&gt;in(0)-&gt;in(0)-&gt;in(0);</span>
<span class="line-added">2697       assert(call-&gt;is_Call(), &quot;CatchProj with no call?&quot;);</span>
<span class="line-added">2698       CallProjections projs;</span>
<span class="line-added">2699       call-&gt;as_Call()-&gt;extract_projections(&amp;projs, false, false);</span>
<span class="line-added">2700       if (projs.catchall_memproj == NULL) {</span>
<span class="line-added">2701         mem = projs.fallthrough_memproj;</span>
<span class="line-added">2702       } else if (c == projs.fallthrough_catchproj) {</span>
<span class="line-added">2703         mem = projs.fallthrough_memproj;</span>
<span class="line-added">2704       } else {</span>
<span class="line-added">2705         assert(c == projs.catchall_catchproj, &quot;strange control&quot;);</span>
<span class="line-added">2706         mem = projs.catchall_memproj;</span>
<span class="line-added">2707       }</span>
<span class="line-added">2708     } else {</span>
<span class="line-added">2709       assert(!c-&gt;is_Start(), &quot;should stop before start&quot;);</span>
<span class="line-added">2710       c = c-&gt;in(0);</span>
<span class="line-added">2711     }</span>
<span class="line-added">2712   } while (mem == NULL);</span>
<span class="line-added">2713   return mem;</span>
<span class="line-added">2714 }</span>
2715 
2716 //-------------------------------gen_subtype_check-----------------------------
2717 // Generate a subtyping check.  Takes as input the subtype and supertype.
2718 // Returns 2 values: sets the default control() to the true path and returns
2719 // the false path.  Only reads invariant memory; sets no (visible) memory.
2720 // The PartialSubtypeCheckNode sets the hidden 1-word cache in the encoding
2721 // but that&#39;s not exposed to the optimizer.  This call also doesn&#39;t take in an
2722 // Object; if you wish to check an Object you need to load the Object&#39;s class
2723 // prior to coming here.
<span class="line-modified">2724 Node* Phase::gen_subtype_check(Node* subklass, Node* superklass, Node** ctrl, Node* mem, PhaseGVN&amp; gvn) {</span>
<span class="line-modified">2725   Compile* C = gvn.C;</span>

2726   if ((*ctrl)-&gt;is_top()) {
2727     return C-&gt;top();
2728   }
2729 
2730   // Fast check for identical types, perhaps identical constants.
2731   // The types can even be identical non-constants, in cases
2732   // involving Array.newInstance, Object.clone, etc.
2733   if (subklass == superklass)
2734     return C-&gt;top();             // false path is dead; no test needed.
2735 
<span class="line-modified">2736   if (gvn.type(superklass)-&gt;singleton()) {</span>
<span class="line-modified">2737     ciKlass* superk = gvn.type(superklass)-&gt;is_klassptr()-&gt;klass();</span>
<span class="line-modified">2738     ciKlass* subk   = gvn.type(subklass)-&gt;is_klassptr()-&gt;klass();</span>
2739 
2740     // In the common case of an exact superklass, try to fold up the
2741     // test before generating code.  You may ask, why not just generate
2742     // the code and then let it fold up?  The answer is that the generated
2743     // code will necessarily include null checks, which do not always
2744     // completely fold away.  If they are also needless, then they turn
2745     // into a performance loss.  Example:
2746     //    Foo[] fa = blah(); Foo x = fa[0]; fa[1] = x;
2747     // Here, the type of &#39;fa&#39; is often exact, so the store check
2748     // of fa[1]=x will fold up, without testing the nullness of x.
2749     switch (C-&gt;static_subtype_check(superk, subk)) {
2750     case Compile::SSC_always_false:
2751       {
2752         Node* always_fail = *ctrl;
<span class="line-modified">2753         *ctrl = gvn.C-&gt;top();</span>
2754         return always_fail;
2755       }
2756     case Compile::SSC_always_true:
2757       return C-&gt;top();
2758     case Compile::SSC_easy_test:
2759       {
2760         // Just do a direct pointer compare and be done.
2761         IfNode* iff = gen_subtype_check_compare(*ctrl, subklass, superklass, BoolTest::eq, PROB_STATIC_FREQUENT, gvn, T_ADDRESS);
<span class="line-modified">2762         *ctrl = gvn.transform(new IfTrueNode(iff));</span>
<span class="line-modified">2763         return gvn.transform(new IfFalseNode(iff));</span>
2764       }
2765     case Compile::SSC_full_test:
2766       break;
2767     default:
2768       ShouldNotReachHere();
2769     }
2770   }
2771 
2772   // %%% Possible further optimization:  Even if the superklass is not exact,
2773   // if the subklass is the unique subtype of the superklass, the check
2774   // will always succeed.  We could leave a dependency behind to ensure this.
2775 
2776   // First load the super-klass&#39;s check-offset
<span class="line-modified">2777   Node *p1 = gvn.transform(new AddPNode(superklass, superklass, gvn.MakeConX(in_bytes(Klass::super_check_offset_offset()))));</span>
<span class="line-modified">2778   Node* m = C-&gt;immutable_memory();</span>
<span class="line-modified">2779   Node *chk_off = gvn.transform(new LoadINode(NULL, m, p1, gvn.type(p1)-&gt;is_ptr(), TypeInt::INT, MemNode::unordered));</span>
2780   int cacheoff_con = in_bytes(Klass::secondary_super_cache_offset());
<span class="line-modified">2781   bool might_be_cache = (gvn.find_int_con(chk_off, cacheoff_con) == cacheoff_con);</span>
2782 
2783   // Load from the sub-klass&#39;s super-class display list, or a 1-word cache of
2784   // the secondary superclass list, or a failing value with a sentinel offset
2785   // if the super-klass is an interface or exceptionally deep in the Java
2786   // hierarchy and we have to scan the secondary superclass list the hard way.
2787   // Worst-case type is a little odd: NULL is allowed as a result (usually
2788   // klass loads can never produce a NULL).
2789   Node *chk_off_X = chk_off;
2790 #ifdef _LP64
<span class="line-modified">2791   chk_off_X = gvn.transform(new ConvI2LNode(chk_off_X));</span>
2792 #endif
<span class="line-modified">2793   Node *p2 = gvn.transform(new AddPNode(subklass,subklass,chk_off_X));</span>
2794   // For some types like interfaces the following loadKlass is from a 1-word
2795   // cache which is mutable so can&#39;t use immutable memory.  Other
2796   // types load from the super-class display table which is immutable.
<span class="line-modified">2797   Node *kmem = C-&gt;immutable_memory();</span>
<span class="line-modified">2798   if (might_be_cache) {</span>
<span class="line-modified">2799     assert((C-&gt;get_alias_index(TypeKlassPtr::make(TypePtr::NotNull, C-&gt;env()-&gt;Object_klass(), Type::OffsetBot)) ==</span>
<span class="line-added">2800             C-&gt;get_alias_index(gvn.type(p2)-&gt;is_ptr())), &quot;&quot;);</span>
<span class="line-added">2801     if (mem == NULL) {</span>
<span class="line-added">2802       mem = find_bottom_mem(*ctrl, C);</span>
<span class="line-added">2803     }</span>
<span class="line-added">2804     kmem = mem-&gt;is_MergeMem() ? mem-&gt;as_MergeMem()-&gt;memory_at(C-&gt;get_alias_index(gvn.type(p2)-&gt;is_ptr())) : mem;</span>
<span class="line-added">2805   }</span>
<span class="line-added">2806   Node *nkls = gvn.transform(LoadKlassNode::make(gvn, NULL, kmem, p2, gvn.type(p2)-&gt;is_ptr(), TypeKlassPtr::OBJECT_OR_NULL));</span>
2807 
2808   // Compile speed common case: ARE a subtype and we canNOT fail
2809   if( superklass == nkls )
2810     return C-&gt;top();             // false path is dead; no test needed.
2811 
2812   // See if we get an immediate positive hit.  Happens roughly 83% of the
2813   // time.  Test to see if the value loaded just previously from the subklass
2814   // is exactly the superklass.
2815   IfNode *iff1 = gen_subtype_check_compare(*ctrl, superklass, nkls, BoolTest::eq, PROB_LIKELY(0.83f), gvn, T_ADDRESS);
<span class="line-modified">2816   Node *iftrue1 = gvn.transform( new IfTrueNode (iff1));</span>
<span class="line-modified">2817   *ctrl = gvn.transform(new IfFalseNode(iff1));</span>
2818 
2819   // Compile speed common case: Check for being deterministic right now.  If
2820   // chk_off is a constant and not equal to cacheoff then we are NOT a
2821   // subklass.  In this case we need exactly the 1 test above and we can
2822   // return those results immediately.
2823   if (!might_be_cache) {
2824     Node* not_subtype_ctrl = *ctrl;
2825     *ctrl = iftrue1; // We need exactly the 1 test above
2826     return not_subtype_ctrl;
2827   }
2828 
2829   // Gather the various success &amp; failures here
2830   RegionNode *r_ok_subtype = new RegionNode(4);
<span class="line-modified">2831   gvn.record_for_igvn(r_ok_subtype);</span>
2832   RegionNode *r_not_subtype = new RegionNode(3);
<span class="line-modified">2833   gvn.record_for_igvn(r_not_subtype);</span>
2834 
2835   r_ok_subtype-&gt;init_req(1, iftrue1);
2836 
2837   // Check for immediate negative hit.  Happens roughly 11% of the time (which
2838   // is roughly 63% of the remaining cases).  Test to see if the loaded
2839   // check-offset points into the subklass display list or the 1-element
2840   // cache.  If it points to the display (and NOT the cache) and the display
2841   // missed then it&#39;s not a subtype.
<span class="line-modified">2842   Node *cacheoff = gvn.intcon(cacheoff_con);</span>
2843   IfNode *iff2 = gen_subtype_check_compare(*ctrl, chk_off, cacheoff, BoolTest::ne, PROB_LIKELY(0.63f), gvn, T_INT);
<span class="line-modified">2844   r_not_subtype-&gt;init_req(1, gvn.transform(new IfTrueNode (iff2)));</span>
<span class="line-modified">2845   *ctrl = gvn.transform(new IfFalseNode(iff2));</span>
2846 
2847   // Check for self.  Very rare to get here, but it is taken 1/3 the time.
2848   // No performance impact (too rare) but allows sharing of secondary arrays
2849   // which has some footprint reduction.
2850   IfNode *iff3 = gen_subtype_check_compare(*ctrl, subklass, superklass, BoolTest::eq, PROB_LIKELY(0.36f), gvn, T_ADDRESS);
<span class="line-modified">2851   r_ok_subtype-&gt;init_req(2, gvn.transform(new IfTrueNode(iff3)));</span>
<span class="line-modified">2852   *ctrl = gvn.transform(new IfFalseNode(iff3));</span>
2853 
2854   // -- Roads not taken here: --
2855   // We could also have chosen to perform the self-check at the beginning
2856   // of this code sequence, as the assembler does.  This would not pay off
2857   // the same way, since the optimizer, unlike the assembler, can perform
2858   // static type analysis to fold away many successful self-checks.
2859   // Non-foldable self checks work better here in second position, because
2860   // the initial primary superclass check subsumes a self-check for most
2861   // types.  An exception would be a secondary type like array-of-interface,
2862   // which does not appear in its own primary supertype display.
2863   // Finally, we could have chosen to move the self-check into the
2864   // PartialSubtypeCheckNode, and from there out-of-line in a platform
2865   // dependent manner.  But it is worthwhile to have the check here,
2866   // where it can be perhaps be optimized.  The cost in code space is
2867   // small (register compare, branch).
2868 
2869   // Now do a linear scan of the secondary super-klass array.  Again, no real
2870   // performance impact (too rare) but it&#39;s gotta be done.
2871   // Since the code is rarely used, there is no penalty for moving it
2872   // out of line, and it can only improve I-cache density.
2873   // The decision to inline or out-of-line this final check is platform
2874   // dependent, and is found in the AD file definition of PartialSubtypeCheck.
<span class="line-modified">2875   Node* psc = gvn.transform(</span>
2876     new PartialSubtypeCheckNode(*ctrl, subklass, superklass));
2877 
<span class="line-modified">2878   IfNode *iff4 = gen_subtype_check_compare(*ctrl, psc, gvn.zerocon(T_OBJECT), BoolTest::ne, PROB_FAIR, gvn, T_ADDRESS);</span>
<span class="line-modified">2879   r_not_subtype-&gt;init_req(2, gvn.transform(new IfTrueNode (iff4)));</span>
<span class="line-modified">2880   r_ok_subtype -&gt;init_req(3, gvn.transform(new IfFalseNode(iff4)));</span>
2881 
2882   // Return false path; set default control to true path.
<span class="line-modified">2883   *ctrl = gvn.transform(r_ok_subtype);</span>
<span class="line-modified">2884   return gvn.transform(r_not_subtype);</span>
<span class="line-added">2885 }</span>
<span class="line-added">2886 </span>
<span class="line-added">2887 Node* GraphKit::gen_subtype_check(Node* obj_or_subklass, Node* superklass) {</span>
<span class="line-added">2888   if (ExpandSubTypeCheckAtParseTime) {</span>
<span class="line-added">2889     MergeMemNode* mem = merged_memory();</span>
<span class="line-added">2890     Node* ctrl = control();</span>
<span class="line-added">2891     Node* subklass = obj_or_subklass;</span>
<span class="line-added">2892     if (!_gvn.type(obj_or_subklass)-&gt;isa_klassptr()) {</span>
<span class="line-added">2893       subklass = load_object_klass(obj_or_subklass);</span>
<span class="line-added">2894     }</span>
<span class="line-added">2895 </span>
<span class="line-added">2896     Node* n = Phase::gen_subtype_check(subklass, superklass, &amp;ctrl, mem, _gvn);</span>
<span class="line-added">2897     set_control(ctrl);</span>
<span class="line-added">2898     return n;</span>
<span class="line-added">2899   }</span>
<span class="line-added">2900 </span>
<span class="line-added">2901   const TypePtr* adr_type = TypeKlassPtr::make(TypePtr::NotNull, C-&gt;env()-&gt;Object_klass(), Type::OffsetBot);</span>
<span class="line-added">2902   Node* check = _gvn.transform(new SubTypeCheckNode(C, obj_or_subklass, superklass));</span>
<span class="line-added">2903   Node* bol = _gvn.transform(new BoolNode(check, BoolTest::eq));</span>
<span class="line-added">2904   IfNode* iff = create_and_xform_if(control(), bol, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);</span>
<span class="line-added">2905   set_control(_gvn.transform(new IfTrueNode(iff)));</span>
<span class="line-added">2906   return _gvn.transform(new IfFalseNode(iff));</span>
2907 }
2908 
2909 // Profile-driven exact type check:
2910 Node* GraphKit::type_check_receiver(Node* receiver, ciKlass* klass,
2911                                     float prob,
2912                                     Node* *casted_receiver) {
2913   const TypeKlassPtr* tklass = TypeKlassPtr::make(klass);
2914   Node* recv_klass = load_object_klass(receiver);
2915   Node* want_klass = makecon(tklass);
2916   Node* cmp = _gvn.transform( new CmpPNode(recv_klass, want_klass) );
2917   Node* bol = _gvn.transform( new BoolNode(cmp, BoolTest::eq) );
2918   IfNode* iff = create_and_xform_if(control(), bol, prob, COUNT_UNKNOWN);
2919   set_control( _gvn.transform( new IfTrueNode (iff) ));
2920   Node* fail = _gvn.transform( new IfFalseNode(iff) );
2921 
2922   const TypeOopPtr* recv_xtype = tklass-&gt;as_instance_type();
2923   assert(recv_xtype-&gt;klass_is_exact(), &quot;&quot;);
2924 
2925   // Subsume downstream occurrences of receiver with a cast to
2926   // recv_xtype, since now we know what the type will be.
2927   Node* cast = new CheckCastPPNode(control(), receiver, recv_xtype);
2928   (*casted_receiver) = _gvn.transform(cast);
2929   // (User must make the replace_in_map call.)
2930 
2931   return fail;
2932 }
2933 
2934 //------------------------------subtype_check_receiver-------------------------
2935 Node* GraphKit::subtype_check_receiver(Node* receiver, ciKlass* klass,
2936                                        Node** casted_receiver) {
2937   const TypeKlassPtr* tklass = TypeKlassPtr::make(klass);

2938   Node* want_klass = makecon(tklass);
2939 
<span class="line-modified">2940   Node* slow_ctl = gen_subtype_check(receiver, want_klass);</span>
2941 
2942   // Cast receiver after successful check
2943   const TypeOopPtr* recv_type = tklass-&gt;cast_to_exactness(false)-&gt;is_klassptr()-&gt;as_instance_type();
2944   Node* cast = new CheckCastPPNode(control(), receiver, recv_type);
2945   (*casted_receiver) = _gvn.transform(cast);
2946 
2947   return slow_ctl;
2948 }
2949 
2950 //------------------------------seems_never_null-------------------------------
2951 // Use null_seen information if it is available from the profile.
2952 // If we see an unexpected null at a type check we record it and force a
2953 // recompile; the offending check will be recompiled to handle NULLs.
2954 // If we see several offending BCIs, then all checks in the
2955 // method will be recompiled.
2956 bool GraphKit::seems_never_null(Node* obj, ciProfileData* data, bool&amp; speculating) {
2957   speculating = !_gvn.type(obj)-&gt;speculative_maybe_null();
2958   Deoptimization::DeoptReason reason = Deoptimization::reason_null_check(speculating);
2959   if (UncommonNullCast               // Cutout for this technique
2960       &amp;&amp; obj != null()               // And not the -Xcomp stupid case?
2961       &amp;&amp; !too_many_traps(reason)
2962       ) {
2963     if (speculating) {
2964       return true;
2965     }
2966     if (data == NULL)
2967       // Edge case:  no mature data.  Be optimistic here.
2968       return true;
2969     // If the profile has not seen a null, assume it won&#39;t happen.
2970     assert(java_bc() == Bytecodes::_checkcast ||
2971            java_bc() == Bytecodes::_instanceof ||
2972            java_bc() == Bytecodes::_aastore, &quot;MDO must collect null_seen bit here&quot;);
2973     return !data-&gt;as_BitData()-&gt;null_seen();
2974   }
2975   speculating = false;
2976   return false;
2977 }
2978 
<span class="line-added">2979 void GraphKit::guard_klass_being_initialized(Node* klass) {</span>
<span class="line-added">2980   int init_state_off = in_bytes(InstanceKlass::init_state_offset());</span>
<span class="line-added">2981   Node* adr = basic_plus_adr(top(), klass, init_state_off);</span>
<span class="line-added">2982   Node* init_state = LoadNode::make(_gvn, NULL, immutable_memory(), adr,</span>
<span class="line-added">2983                                     adr-&gt;bottom_type()-&gt;is_ptr(), TypeInt::BYTE,</span>
<span class="line-added">2984                                     T_BYTE, MemNode::unordered);</span>
<span class="line-added">2985   init_state = _gvn.transform(init_state);</span>
<span class="line-added">2986 </span>
<span class="line-added">2987   Node* being_initialized_state = makecon(TypeInt::make(InstanceKlass::being_initialized));</span>
<span class="line-added">2988 </span>
<span class="line-added">2989   Node* chk = _gvn.transform(new CmpINode(being_initialized_state, init_state));</span>
<span class="line-added">2990   Node* tst = _gvn.transform(new BoolNode(chk, BoolTest::eq));</span>
<span class="line-added">2991 </span>
<span class="line-added">2992   { BuildCutout unless(this, tst, PROB_MAX);</span>
<span class="line-added">2993     uncommon_trap(Deoptimization::Reason_initialized, Deoptimization::Action_reinterpret);</span>
<span class="line-added">2994   }</span>
<span class="line-added">2995 }</span>
<span class="line-added">2996 </span>
<span class="line-added">2997 void GraphKit::guard_init_thread(Node* klass) {</span>
<span class="line-added">2998   int init_thread_off = in_bytes(InstanceKlass::init_thread_offset());</span>
<span class="line-added">2999   Node* adr = basic_plus_adr(top(), klass, init_thread_off);</span>
<span class="line-added">3000 </span>
<span class="line-added">3001   Node* init_thread = LoadNode::make(_gvn, NULL, immutable_memory(), adr,</span>
<span class="line-added">3002                                      adr-&gt;bottom_type()-&gt;is_ptr(), TypePtr::NOTNULL,</span>
<span class="line-added">3003                                      T_ADDRESS, MemNode::unordered);</span>
<span class="line-added">3004   init_thread = _gvn.transform(init_thread);</span>
<span class="line-added">3005 </span>
<span class="line-added">3006   Node* cur_thread = _gvn.transform(new ThreadLocalNode());</span>
<span class="line-added">3007 </span>
<span class="line-added">3008   Node* chk = _gvn.transform(new CmpPNode(cur_thread, init_thread));</span>
<span class="line-added">3009   Node* tst = _gvn.transform(new BoolNode(chk, BoolTest::eq));</span>
<span class="line-added">3010 </span>
<span class="line-added">3011   { BuildCutout unless(this, tst, PROB_MAX);</span>
<span class="line-added">3012     uncommon_trap(Deoptimization::Reason_uninitialized, Deoptimization::Action_none);</span>
<span class="line-added">3013   }</span>
<span class="line-added">3014 }</span>
<span class="line-added">3015 </span>
<span class="line-added">3016 void GraphKit::clinit_barrier(ciInstanceKlass* ik, ciMethod* context) {</span>
<span class="line-added">3017   if (ik-&gt;is_being_initialized()) {</span>
<span class="line-added">3018     if (C-&gt;needs_clinit_barrier(ik, context)) {</span>
<span class="line-added">3019       Node* klass = makecon(TypeKlassPtr::make(ik));</span>
<span class="line-added">3020       guard_klass_being_initialized(klass);</span>
<span class="line-added">3021       guard_init_thread(klass);</span>
<span class="line-added">3022       insert_mem_bar(Op_MemBarCPUOrder);</span>
<span class="line-added">3023     }</span>
<span class="line-added">3024   } else if (ik-&gt;is_initialized()) {</span>
<span class="line-added">3025     return; // no barrier needed</span>
<span class="line-added">3026   } else {</span>
<span class="line-added">3027     uncommon_trap(Deoptimization::Reason_uninitialized,</span>
<span class="line-added">3028                   Deoptimization::Action_reinterpret,</span>
<span class="line-added">3029                   NULL);</span>
<span class="line-added">3030   }</span>
<span class="line-added">3031 }</span>
<span class="line-added">3032 </span>
3033 //------------------------maybe_cast_profiled_receiver-------------------------
3034 // If the profile has seen exactly one type, narrow to exactly that type.
3035 // Subsequent type checks will always fold up.
3036 Node* GraphKit::maybe_cast_profiled_receiver(Node* not_null_obj,
3037                                              ciKlass* require_klass,
3038                                              ciKlass* spec_klass,
3039                                              bool safe_for_replace) {
3040   if (!UseTypeProfile || !TypeProfileCasts) return NULL;
3041 
3042   Deoptimization::DeoptReason reason = Deoptimization::reason_class_check(spec_klass != NULL);
3043 
3044   // Make sure we haven&#39;t already deoptimized from this tactic.
3045   if (too_many_traps_or_recompiles(reason))
3046     return NULL;
3047 
3048   // (No, this isn&#39;t a call, but it&#39;s enough like a virtual call
3049   // to use the same ciMethod accessor to get the profile info...)
3050   // If we have a speculative type use it instead of profiling (which
3051   // may not help us)
3052   ciKlass* exact_kls = spec_klass == NULL ? profile_has_unique_klass() : spec_klass;
</pre>
<hr />
<pre>
3185     }
3186   }
3187 
3188   if (!known_statically) {
3189     const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
3190     // We may not have profiling here or it may not help us. If we
3191     // have a speculative type use it to perform an exact cast.
3192     ciKlass* spec_obj_type = obj_type-&gt;speculative_type();
3193     if (spec_obj_type != NULL || (ProfileDynamicTypes &amp;&amp; data != NULL)) {
3194       Node* cast_obj = maybe_cast_profiled_receiver(not_null_obj, NULL, spec_obj_type, safe_for_replace);
3195       if (stopped()) {            // Profile disagrees with this path.
3196         set_control(null_ctl);    // Null is the only remaining possibility.
3197         return intcon(0);
3198       }
3199       if (cast_obj != NULL) {
3200         not_null_obj = cast_obj;
3201       }
3202     }
3203   }
3204 



3205   // Generate the subtype check
<span class="line-modified">3206   Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass);</span>
3207 
3208   // Plug in the success path to the general merge in slot 1.
3209   region-&gt;init_req(_obj_path, control());
3210   phi   -&gt;init_req(_obj_path, intcon(1));
3211 
3212   // Plug in the failing path to the general merge in slot 2.
3213   region-&gt;init_req(_fail_path, not_subtype_ctrl);
3214   phi   -&gt;init_req(_fail_path, intcon(0));
3215 
3216   // Return final merged results
3217   set_control( _gvn.transform(region) );
3218   record_for_igvn(region);
3219 
3220   // If we know the type check always succeeds then we don&#39;t use the
3221   // profiling data at this bytecode. Don&#39;t lose it, feed it to the
3222   // type system as a speculative type.
3223   if (safe_for_replace) {
3224     Node* casted_obj = record_profiled_receiver_for_speculation(obj);
3225     replace_in_map(obj, casted_obj);
3226   }
</pre>
<hr />
<pre>
3309     // The following optimization tries to statically cast the speculative type of the object
3310     // (for example obtained during profiling) to the type of the superklass and then do a
3311     // dynamic check that the type of the object is what we expect. To work correctly
3312     // for checkcast and aastore the type of superklass should be exact.
3313     const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
3314     // We may not have profiling here or it may not help us. If we have
3315     // a speculative type use it to perform an exact cast.
3316     ciKlass* spec_obj_type = obj_type-&gt;speculative_type();
3317     if (spec_obj_type != NULL || data != NULL) {
3318       cast_obj = maybe_cast_profiled_receiver(not_null_obj, tk-&gt;klass(), spec_obj_type, safe_for_replace);
3319       if (cast_obj != NULL) {
3320         if (failure_control != NULL) // failure is now impossible
3321           (*failure_control) = top();
3322         // adjust the type of the phi to the exact klass:
3323         phi-&gt;raise_bottom_type(_gvn.type(cast_obj)-&gt;meet_speculative(TypePtr::NULL_PTR));
3324       }
3325     }
3326   }
3327 
3328   if (cast_obj == NULL) {



3329     // Generate the subtype check
<span class="line-modified">3330     Node* not_subtype_ctrl = gen_subtype_check(not_null_obj, superklass );</span>
3331 
3332     // Plug in success path into the merge
3333     cast_obj = _gvn.transform(new CheckCastPPNode(control(), not_null_obj, toop));
3334     // Failure path ends in uncommon trap (or may be dead - failure impossible)
3335     if (failure_control == NULL) {
3336       if (not_subtype_ctrl != top()) { // If failure is possible
3337         PreserveJVMState pjvms(this);
3338         set_control(not_subtype_ctrl);
<span class="line-modified">3339         builtin_throw(Deoptimization::Reason_class_check, load_object_klass(not_null_obj));</span>
3340       }
3341     } else {
3342       (*failure_control) = not_subtype_ctrl;
3343     }
3344   }
3345 
3346   region-&gt;init_req(_obj_path, control());
3347   phi   -&gt;init_req(_obj_path, cast_obj);
3348 
3349   // A merge of NULL or Casted-NotNull obj
3350   Node* res = _gvn.transform(phi);
3351 
3352   // Note I do NOT always &#39;replace_in_map(obj,result)&#39; here.
3353   //  if( tk-&gt;klass()-&gt;can_be_primary_super()  )
3354     // This means that if I successfully store an Object into an array-of-String
3355     // I &#39;forget&#39; that the Object is really now known to be a String.  I have to
3356     // do this because we don&#39;t have true union types for interfaces - if I store
3357     // a Baz into an array-of-Interface and then tell the optimizer it&#39;s an
3358     // Interface, I forget that it&#39;s also a Baz and cannot do Baz-like field
3359     // references to it.  FIX THIS WHEN UNION TYPES APPEAR!
</pre>
<hr />
<pre>
3419   } else {
3420     set_memory(_gvn.transform(new ProjNode(membar, TypeFunc::Memory)),alias_idx);
3421   }
3422   return membar;
3423 }
3424 
3425 //------------------------------shared_lock------------------------------------
3426 // Emit locking code.
3427 FastLockNode* GraphKit::shared_lock(Node* obj) {
3428   // bci is either a monitorenter bc or InvocationEntryBci
3429   // %%% SynchronizationEntryBCI is redundant; use InvocationEntryBci in interfaces
3430   assert(SynchronizationEntryBCI == InvocationEntryBci, &quot;&quot;);
3431 
3432   if( !GenerateSynchronizationCode )
3433     return NULL;                // Not locking things?
3434   if (stopped())                // Dead monitor?
3435     return NULL;
3436 
3437   assert(dead_locals_are_killed(), &quot;should kill locals before sync. point&quot;);
3438 


3439   // Box the stack location
3440   Node* box = _gvn.transform(new BoxLockNode(next_monitor()));
3441   Node* mem = reset_memory();
3442 
3443   FastLockNode * flock = _gvn.transform(new FastLockNode(0, obj, box) )-&gt;as_FastLock();
3444   if (UseBiasedLocking &amp;&amp; PrintPreciseBiasedLockingStatistics) {
3445     // Create the counters for this fast lock.
3446     flock-&gt;create_lock_counter(sync_jvms()); // sync_jvms used to get current bci
3447   }
3448 
3449   // Create the rtm counters for this fast lock if needed.
3450   flock-&gt;create_rtm_lock_counter(sync_jvms()); // sync_jvms used to get current bci
3451 
3452   // Add monitor to debug info for the slow path.  If we block inside the
3453   // slow path and de-opt, we need the monitor hanging around
3454   map()-&gt;push_monitor( flock );
3455 
3456   const TypeFunc *tf = LockNode::lock_type();
3457   LockNode *lock = new LockNode(C, tf);
3458 
</pre>
<hr />
<pre>
4143   set_memory(res_mem, TypeAryPtr::BYTES);
4144   return str;
4145 }
4146 
4147 void GraphKit::inflate_string(Node* src, Node* dst, const TypeAryPtr* dst_type, Node* count) {
4148   assert(Matcher::match_rule_supported(Op_StrInflatedCopy), &quot;Intrinsic not supported&quot;);
4149   assert(dst_type == TypeAryPtr::BYTES || dst_type == TypeAryPtr::CHARS, &quot;invalid dest type&quot;);
4150   // Capture src and dst memory (see comment in &#39;compress_string&#39;).
4151   Node* mem = capture_memory(TypeAryPtr::BYTES, dst_type);
4152   StrInflatedCopyNode* str = new StrInflatedCopyNode(control(), mem, src, dst, count);
4153   set_memory(_gvn.transform(str), dst_type);
4154 }
4155 
4156 void GraphKit::inflate_string_slow(Node* src, Node* dst, Node* start, Node* count) {
4157   /**
4158    * int i_char = start;
4159    * for (int i_byte = 0; i_byte &lt; count; i_byte++) {
4160    *   dst[i_char++] = (char)(src[i_byte] &amp; 0xff);
4161    * }
4162    */


4163   add_predicate();
4164   RegionNode* head = new RegionNode(3);
4165   head-&gt;init_req(1, control());
4166   gvn().set_type(head, Type::CONTROL);
4167   record_for_igvn(head);
4168 
4169   Node* i_byte = new PhiNode(head, TypeInt::INT);
4170   i_byte-&gt;init_req(1, intcon(0));
4171   gvn().set_type(i_byte, TypeInt::INT);
4172   record_for_igvn(i_byte);
4173 
4174   Node* i_char = new PhiNode(head, TypeInt::INT);
4175   i_char-&gt;init_req(1, start);
4176   gvn().set_type(i_char, TypeInt::INT);
4177   record_for_igvn(i_char);
4178 
4179   Node* mem = PhiNode::make(head, memory(TypeAryPtr::BYTES), Type::MEMORY, TypeAryPtr::BYTES);
4180   gvn().set_type(mem, Type::MEMORY);
4181   record_for_igvn(mem);
4182   set_control(head);
</pre>
</td>
</tr>
</table>
<center><a href="gcm.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="graphKit.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>