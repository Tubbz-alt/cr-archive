<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/graphKit.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="graphKit.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="idealGraphPrinter.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/graphKit.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
369     return null_check_common(value, type, true, NULL, _gvn.type(value)-&gt;speculative_always_null());
370   }
371 
372   // Check if value is null and abort if it is
373   Node* must_be_not_null(Node* value, bool do_replace_in_map);
374 
375   // Null check oop.  Return null-path control into (*null_control).
376   // Return a cast-not-null node which depends on the not-null control.
377   // If never_see_null, use an uncommon trap (*null_control sees a top).
378   // The cast is not valid along the null path; keep a copy of the original.
379   // If safe_for_replace, then we can replace the value with the cast
380   // in the parsing map (the cast is guaranteed to dominate the map)
381   Node* null_check_oop(Node* value, Node* *null_control,
382                        bool never_see_null = false,
383                        bool safe_for_replace = false,
384                        bool speculative = false);
385 
386   // Check the null_seen bit.
387   bool seems_never_null(Node* obj, ciProfileData* data, bool&amp; speculating);
388 





389   // Check for unique class for receiver at call
390   ciKlass* profile_has_unique_klass() {
391     ciCallProfile profile = method()-&gt;call_profile_at_bci(bci());
392     if (profile.count() &gt;= 0 &amp;&amp;         // no cast failures here
393         profile.has_receiver(0) &amp;&amp;
394         profile.morphism() == 1) {
395       return profile.receiver(0);
396     }
397     return NULL;
398   }
399 
400   // record type from profiling with the type system
401   Node* record_profile_for_speculation(Node* n, ciKlass* exact_kls, ProfilePtrKind ptr_kind);
402   void record_profiled_arguments_for_speculation(ciMethod* dest_method, Bytecodes::Code bc);
403   void record_profiled_parameters_for_speculation();
404   void record_profiled_return_for_speculation();
405   Node* record_profiled_receiver_for_speculation(Node* n);
406 
407   // Use the type profile to narrow an object type.
408   Node* maybe_cast_profiled_receiver(Node* not_null_obj,
</pre>
<hr />
<pre>
501     Node* mem = map_not_null()-&gt;memory();
502     assert(mem-&gt;is_MergeMem(), &quot;parse memory is always pre-split&quot;);
503     return mem-&gt;as_MergeMem();
504   }
505 
506   // Set the entire memory state; produce a new MergeMemNode.
507   void set_all_memory(Node* newmem);
508 
509   // Create a memory projection from the call, then set_all_memory.
510   void set_all_memory_call(Node* call, bool separate_io_proj = false);
511 
512   // Create a LoadNode, reading from the parser&#39;s memory state.
513   // (Note:  require_atomic_access is useful only with T_LONG.)
514   //
515   // We choose the unordered semantics by default because we have
516   // adapted the `do_put_xxx&#39; and `do_get_xxx&#39; procedures for the case
517   // of volatile fields.
518   Node* make_load(Node* ctl, Node* adr, const Type* t, BasicType bt,
519                   MemNode::MemOrd mo, LoadNode::ControlDependency control_dependency = LoadNode::DependsOnlyOnTest,
520                   bool require_atomic_access = false, bool unaligned = false,
<span class="line-modified">521                   bool mismatched = false, bool unsafe = false) {</span>
522     // This version computes alias_index from bottom_type
523     return make_load(ctl, adr, t, bt, adr-&gt;bottom_type()-&gt;is_ptr(),
524                      mo, control_dependency, require_atomic_access,
<span class="line-modified">525                      unaligned, mismatched, unsafe);</span>
526   }
527   Node* make_load(Node* ctl, Node* adr, const Type* t, BasicType bt, const TypePtr* adr_type,
528                   MemNode::MemOrd mo, LoadNode::ControlDependency control_dependency = LoadNode::DependsOnlyOnTest,
529                   bool require_atomic_access = false, bool unaligned = false,
<span class="line-modified">530                   bool mismatched = false, bool unsafe = false) {</span>
531     // This version computes alias_index from an address type
532     assert(adr_type != NULL, &quot;use other make_load factory&quot;);
533     return make_load(ctl, adr, t, bt, C-&gt;get_alias_index(adr_type),
534                      mo, control_dependency, require_atomic_access,
<span class="line-modified">535                      unaligned, mismatched, unsafe);</span>
536   }
537   // This is the base version which is given an alias index.
538   Node* make_load(Node* ctl, Node* adr, const Type* t, BasicType bt, int adr_idx,
539                   MemNode::MemOrd mo, LoadNode::ControlDependency control_dependency = LoadNode::DependsOnlyOnTest,
540                   bool require_atomic_access = false, bool unaligned = false,
<span class="line-modified">541                   bool mismatched = false, bool unsafe = false);</span>
542 
543   // Create &amp; transform a StoreNode and store the effect into the
544   // parser&#39;s memory state.
545   //
546   // We must ensure that stores of object references will be visible
547   // only after the object&#39;s initialization. So the clients of this
548   // procedure must indicate that the store requires `release&#39;
549   // semantics, if the stored value is an object reference that might
550   // point to a new object and may become externally visible.
551   Node* store_to_memory(Node* ctl, Node* adr, Node* val, BasicType bt,
552                         const TypePtr* adr_type,
553                         MemNode::MemOrd mo,
554                         bool require_atomic_access = false,
555                         bool unaligned = false,
556                         bool mismatched = false,
557                         bool unsafe = false) {
558     // This version computes alias_index from an address type
559     assert(adr_type != NULL, &quot;use other store_to_memory factory&quot;);
560     return store_to_memory(ctl, adr, val, bt,
561                            C-&gt;get_alias_index(adr_type),
562                            mo, require_atomic_access,
<span class="line-modified">563                            unaligned, mismatched);</span>
564   }
565   // This is the base version which is given alias index
566   // Return the new StoreXNode
567   Node* store_to_memory(Node* ctl, Node* adr, Node* val, BasicType bt,
568                         int adr_idx,
569                         MemNode::MemOrd,
570                         bool require_atomic_access = false,
571                         bool unaligned = false,
572                         bool mismatched = false,
573                         bool unsafe = false);
574 
575   // Perform decorated accesses
576 
577   Node* access_store_at(Node* obj,   // containing obj
578                         Node* adr,   // actual adress to store val at
579                         const TypePtr* adr_type,
580                         Node* val,
581                         const Type* val_type,
582                         BasicType bt,
583                         DecoratorSet decorators);
</pre>
<hr />
<pre>
617   Node* access_atomic_xchg_at(Node* obj,
618                               Node* adr,
619                               const TypePtr* adr_type,
620                               int alias_idx,
621                               Node* new_val,
622                               const Type* value_type,
623                               BasicType bt,
624                               DecoratorSet decorators);
625 
626   Node* access_atomic_add_at(Node* obj,
627                              Node* adr,
628                              const TypePtr* adr_type,
629                              int alias_idx,
630                              Node* new_val,
631                              const Type* value_type,
632                              BasicType bt,
633                              DecoratorSet decorators);
634 
635   void access_clone(Node* src, Node* dst, Node* size, bool is_array);
636 
<span class="line-removed">637   Node* access_resolve(Node* n, DecoratorSet decorators);</span>
<span class="line-removed">638 </span>
639   // Return addressing for an array element.
640   Node* array_element_address(Node* ary, Node* idx, BasicType elembt,
641                               // Optional constraint on the array size:
642                               const TypeInt* sizetype = NULL,
643                               // Optional control dependency (for example, on range check)
644                               Node* ctrl = NULL);
645 
646   // Return a load of array element at idx.
647   Node* load_array_element(Node* ctl, Node* ary, Node* idx, const TypeAryPtr* arytype);
648 
649   //---------------- Dtrace support --------------------
650   void make_dtrace_method_entry_exit(ciMethod* method, bool is_entry);
651   void make_dtrace_method_entry(ciMethod* method) {
652     make_dtrace_method_entry_exit(method, true);
653   }
654   void make_dtrace_method_exit(ciMethod* method) {
655     make_dtrace_method_entry_exit(method, false);
656   }
657 
658   //--------------- stub generation -------------------
</pre>
<hr />
<pre>
805   // Helper functions to build synchronizations
806   int next_monitor();
807   Node* insert_mem_bar(int opcode, Node* precedent = NULL);
808   Node* insert_mem_bar_volatile(int opcode, int alias_idx, Node* precedent = NULL);
809   // Optional &#39;precedent&#39; is appended as an extra edge, to force ordering.
810   FastLockNode* shared_lock(Node* obj);
811   void shared_unlock(Node* box, Node* obj);
812 
813   // helper functions for the fast path/slow path idioms
814   Node* fast_and_slow(Node* in, const Type *result_type, Node* null_result, IfNode* fast_test, Node* fast_result, address slow_call, const TypeFunc *slow_call_type, Node* slow_arg, Klass* ex_klass, Node* slow_result);
815 
816   // Generate an instance-of idiom.  Used by both the instance-of bytecode
817   // and the reflective instance-of call.
818   Node* gen_instanceof(Node *subobj, Node* superkls, bool safe_for_replace = false);
819 
820   // Generate a check-cast idiom.  Used by both the check-cast bytecode
821   // and the array-store bytecode
822   Node* gen_checkcast( Node *subobj, Node* superkls,
823                        Node* *failure_control = NULL );
824 
<span class="line-modified">825   Node* gen_subtype_check(Node* subklass, Node* superklass) {</span>
<span class="line-removed">826     MergeMemNode* mem = merged_memory();</span>
<span class="line-removed">827     Node* ctrl = control();</span>
<span class="line-removed">828     Node* n = Phase::gen_subtype_check(subklass, superklass, &amp;ctrl, mem, &amp;_gvn);</span>
<span class="line-removed">829     set_control(ctrl);</span>
<span class="line-removed">830     return n;</span>
<span class="line-removed">831   }</span>
832 
833   // Exact type check used for predicted calls and casts.
834   // Rewrites (*casted_receiver) to be casted to the stronger type.
835   // (Caller is responsible for doing replace_in_map.)
836   Node* type_check_receiver(Node* receiver, ciKlass* klass, float prob,
837                             Node* *casted_receiver);
838 
839   // Inexact type check used for predicted calls.
840   Node* subtype_check_receiver(Node* receiver, ciKlass* klass,
841                                Node** casted_receiver);
842 
843   // implementation of object creation
844   Node* set_output_for_allocation(AllocateNode* alloc,
845                                   const TypeOopPtr* oop_type,
846                                   bool deoptimize_on_exception=false);
847   Node* get_layout_helper(Node* klass_node, jint&amp; constant_value);
848   Node* new_instance(Node* klass_node,
849                      Node* slow_test = NULL,
850                      Node* *return_size_val = NULL,
851                      bool deoptimize_on_exception = false);
</pre>
</td>
<td>
<hr />
<pre>
369     return null_check_common(value, type, true, NULL, _gvn.type(value)-&gt;speculative_always_null());
370   }
371 
372   // Check if value is null and abort if it is
373   Node* must_be_not_null(Node* value, bool do_replace_in_map);
374 
375   // Null check oop.  Return null-path control into (*null_control).
376   // Return a cast-not-null node which depends on the not-null control.
377   // If never_see_null, use an uncommon trap (*null_control sees a top).
378   // The cast is not valid along the null path; keep a copy of the original.
379   // If safe_for_replace, then we can replace the value with the cast
380   // in the parsing map (the cast is guaranteed to dominate the map)
381   Node* null_check_oop(Node* value, Node* *null_control,
382                        bool never_see_null = false,
383                        bool safe_for_replace = false,
384                        bool speculative = false);
385 
386   // Check the null_seen bit.
387   bool seems_never_null(Node* obj, ciProfileData* data, bool&amp; speculating);
388 
<span class="line-added">389   void guard_klass_being_initialized(Node* klass);</span>
<span class="line-added">390   void guard_init_thread(Node* klass);</span>
<span class="line-added">391 </span>
<span class="line-added">392   void clinit_barrier(ciInstanceKlass* ik, ciMethod* context);</span>
<span class="line-added">393 </span>
394   // Check for unique class for receiver at call
395   ciKlass* profile_has_unique_klass() {
396     ciCallProfile profile = method()-&gt;call_profile_at_bci(bci());
397     if (profile.count() &gt;= 0 &amp;&amp;         // no cast failures here
398         profile.has_receiver(0) &amp;&amp;
399         profile.morphism() == 1) {
400       return profile.receiver(0);
401     }
402     return NULL;
403   }
404 
405   // record type from profiling with the type system
406   Node* record_profile_for_speculation(Node* n, ciKlass* exact_kls, ProfilePtrKind ptr_kind);
407   void record_profiled_arguments_for_speculation(ciMethod* dest_method, Bytecodes::Code bc);
408   void record_profiled_parameters_for_speculation();
409   void record_profiled_return_for_speculation();
410   Node* record_profiled_receiver_for_speculation(Node* n);
411 
412   // Use the type profile to narrow an object type.
413   Node* maybe_cast_profiled_receiver(Node* not_null_obj,
</pre>
<hr />
<pre>
506     Node* mem = map_not_null()-&gt;memory();
507     assert(mem-&gt;is_MergeMem(), &quot;parse memory is always pre-split&quot;);
508     return mem-&gt;as_MergeMem();
509   }
510 
511   // Set the entire memory state; produce a new MergeMemNode.
512   void set_all_memory(Node* newmem);
513 
514   // Create a memory projection from the call, then set_all_memory.
515   void set_all_memory_call(Node* call, bool separate_io_proj = false);
516 
517   // Create a LoadNode, reading from the parser&#39;s memory state.
518   // (Note:  require_atomic_access is useful only with T_LONG.)
519   //
520   // We choose the unordered semantics by default because we have
521   // adapted the `do_put_xxx&#39; and `do_get_xxx&#39; procedures for the case
522   // of volatile fields.
523   Node* make_load(Node* ctl, Node* adr, const Type* t, BasicType bt,
524                   MemNode::MemOrd mo, LoadNode::ControlDependency control_dependency = LoadNode::DependsOnlyOnTest,
525                   bool require_atomic_access = false, bool unaligned = false,
<span class="line-modified">526                   bool mismatched = false, bool unsafe = false, uint8_t barrier_data = 0) {</span>
527     // This version computes alias_index from bottom_type
528     return make_load(ctl, adr, t, bt, adr-&gt;bottom_type()-&gt;is_ptr(),
529                      mo, control_dependency, require_atomic_access,
<span class="line-modified">530                      unaligned, mismatched, unsafe, barrier_data);</span>
531   }
532   Node* make_load(Node* ctl, Node* adr, const Type* t, BasicType bt, const TypePtr* adr_type,
533                   MemNode::MemOrd mo, LoadNode::ControlDependency control_dependency = LoadNode::DependsOnlyOnTest,
534                   bool require_atomic_access = false, bool unaligned = false,
<span class="line-modified">535                   bool mismatched = false, bool unsafe = false, uint8_t barrier_data = 0) {</span>
536     // This version computes alias_index from an address type
537     assert(adr_type != NULL, &quot;use other make_load factory&quot;);
538     return make_load(ctl, adr, t, bt, C-&gt;get_alias_index(adr_type),
539                      mo, control_dependency, require_atomic_access,
<span class="line-modified">540                      unaligned, mismatched, unsafe, barrier_data);</span>
541   }
542   // This is the base version which is given an alias index.
543   Node* make_load(Node* ctl, Node* adr, const Type* t, BasicType bt, int adr_idx,
544                   MemNode::MemOrd mo, LoadNode::ControlDependency control_dependency = LoadNode::DependsOnlyOnTest,
545                   bool require_atomic_access = false, bool unaligned = false,
<span class="line-modified">546                   bool mismatched = false, bool unsafe = false, uint8_t barrier_data = 0);</span>
547 
548   // Create &amp; transform a StoreNode and store the effect into the
549   // parser&#39;s memory state.
550   //
551   // We must ensure that stores of object references will be visible
552   // only after the object&#39;s initialization. So the clients of this
553   // procedure must indicate that the store requires `release&#39;
554   // semantics, if the stored value is an object reference that might
555   // point to a new object and may become externally visible.
556   Node* store_to_memory(Node* ctl, Node* adr, Node* val, BasicType bt,
557                         const TypePtr* adr_type,
558                         MemNode::MemOrd mo,
559                         bool require_atomic_access = false,
560                         bool unaligned = false,
561                         bool mismatched = false,
562                         bool unsafe = false) {
563     // This version computes alias_index from an address type
564     assert(adr_type != NULL, &quot;use other store_to_memory factory&quot;);
565     return store_to_memory(ctl, adr, val, bt,
566                            C-&gt;get_alias_index(adr_type),
567                            mo, require_atomic_access,
<span class="line-modified">568                            unaligned, mismatched, unsafe);</span>
569   }
570   // This is the base version which is given alias index
571   // Return the new StoreXNode
572   Node* store_to_memory(Node* ctl, Node* adr, Node* val, BasicType bt,
573                         int adr_idx,
574                         MemNode::MemOrd,
575                         bool require_atomic_access = false,
576                         bool unaligned = false,
577                         bool mismatched = false,
578                         bool unsafe = false);
579 
580   // Perform decorated accesses
581 
582   Node* access_store_at(Node* obj,   // containing obj
583                         Node* adr,   // actual adress to store val at
584                         const TypePtr* adr_type,
585                         Node* val,
586                         const Type* val_type,
587                         BasicType bt,
588                         DecoratorSet decorators);
</pre>
<hr />
<pre>
622   Node* access_atomic_xchg_at(Node* obj,
623                               Node* adr,
624                               const TypePtr* adr_type,
625                               int alias_idx,
626                               Node* new_val,
627                               const Type* value_type,
628                               BasicType bt,
629                               DecoratorSet decorators);
630 
631   Node* access_atomic_add_at(Node* obj,
632                              Node* adr,
633                              const TypePtr* adr_type,
634                              int alias_idx,
635                              Node* new_val,
636                              const Type* value_type,
637                              BasicType bt,
638                              DecoratorSet decorators);
639 
640   void access_clone(Node* src, Node* dst, Node* size, bool is_array);
641 


642   // Return addressing for an array element.
643   Node* array_element_address(Node* ary, Node* idx, BasicType elembt,
644                               // Optional constraint on the array size:
645                               const TypeInt* sizetype = NULL,
646                               // Optional control dependency (for example, on range check)
647                               Node* ctrl = NULL);
648 
649   // Return a load of array element at idx.
650   Node* load_array_element(Node* ctl, Node* ary, Node* idx, const TypeAryPtr* arytype);
651 
652   //---------------- Dtrace support --------------------
653   void make_dtrace_method_entry_exit(ciMethod* method, bool is_entry);
654   void make_dtrace_method_entry(ciMethod* method) {
655     make_dtrace_method_entry_exit(method, true);
656   }
657   void make_dtrace_method_exit(ciMethod* method) {
658     make_dtrace_method_entry_exit(method, false);
659   }
660 
661   //--------------- stub generation -------------------
</pre>
<hr />
<pre>
808   // Helper functions to build synchronizations
809   int next_monitor();
810   Node* insert_mem_bar(int opcode, Node* precedent = NULL);
811   Node* insert_mem_bar_volatile(int opcode, int alias_idx, Node* precedent = NULL);
812   // Optional &#39;precedent&#39; is appended as an extra edge, to force ordering.
813   FastLockNode* shared_lock(Node* obj);
814   void shared_unlock(Node* box, Node* obj);
815 
816   // helper functions for the fast path/slow path idioms
817   Node* fast_and_slow(Node* in, const Type *result_type, Node* null_result, IfNode* fast_test, Node* fast_result, address slow_call, const TypeFunc *slow_call_type, Node* slow_arg, Klass* ex_klass, Node* slow_result);
818 
819   // Generate an instance-of idiom.  Used by both the instance-of bytecode
820   // and the reflective instance-of call.
821   Node* gen_instanceof(Node *subobj, Node* superkls, bool safe_for_replace = false);
822 
823   // Generate a check-cast idiom.  Used by both the check-cast bytecode
824   // and the array-store bytecode
825   Node* gen_checkcast( Node *subobj, Node* superkls,
826                        Node* *failure_control = NULL );
827 
<span class="line-modified">828   Node* gen_subtype_check(Node* obj, Node* superklass);</span>






829 
830   // Exact type check used for predicted calls and casts.
831   // Rewrites (*casted_receiver) to be casted to the stronger type.
832   // (Caller is responsible for doing replace_in_map.)
833   Node* type_check_receiver(Node* receiver, ciKlass* klass, float prob,
834                             Node* *casted_receiver);
835 
836   // Inexact type check used for predicted calls.
837   Node* subtype_check_receiver(Node* receiver, ciKlass* klass,
838                                Node** casted_receiver);
839 
840   // implementation of object creation
841   Node* set_output_for_allocation(AllocateNode* alloc,
842                                   const TypeOopPtr* oop_type,
843                                   bool deoptimize_on_exception=false);
844   Node* get_layout_helper(Node* klass_node, jint&amp; constant_value);
845   Node* new_instance(Node* klass_node,
846                      Node* slow_test = NULL,
847                      Node* *return_size_val = NULL,
848                      bool deoptimize_on_exception = false);
</pre>
</td>
</tr>
</table>
<center><a href="graphKit.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="idealGraphPrinter.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>