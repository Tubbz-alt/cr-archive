<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Udiff src/hotspot/share/opto/parse1.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="parse.hpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="parse2.cpp.udiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/parse1.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-new-header">@@ -39,10 +39,11 @@</span>
  #include &quot;opto/runtime.hpp&quot;
  #include &quot;runtime/arguments.hpp&quot;
  #include &quot;runtime/handles.inline.hpp&quot;
  #include &quot;runtime/safepointMechanism.hpp&quot;
  #include &quot;runtime/sharedRuntime.hpp&quot;
<span class="udiff-line-added">+ #include &quot;utilities/bitMap.inline.hpp&quot;</span>
  #include &quot;utilities/copy.hpp&quot;
  
  // Static array so we can figure out which bytecodes stop us from compiling
  // the most. Some of the non-static variables are needed in bytecodeInfo.cpp
  // and eventually should be encapsulated in a proper class (gri 8/18/98).
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -521,14 +522,10 @@</span>
    }
  
  #ifdef ASSERT
    if (depth() == 1) {
      assert(C-&gt;is_osr_compilation() == this-&gt;is_osr_parse(), &quot;OSR in sync&quot;);
<span class="udiff-line-removed">-     if (C-&gt;tf() != tf()) {</span>
<span class="udiff-line-removed">-       assert(C-&gt;env()-&gt;system_dictionary_modification_counter_changed(),</span>
<span class="udiff-line-removed">-              &quot;Must invalidate if TypeFuncs differ&quot;);</span>
<span class="udiff-line-removed">-     }</span>
    } else {
      assert(!this-&gt;is_osr_parse(), &quot;no recursive OSR&quot;);
    }
  #endif
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -582,10 +579,15 @@</span>
        decrement_age();
      }
    }
  
    if (depth() == 1 &amp;&amp; !failing()) {
<span class="udiff-line-added">+     if (C-&gt;clinit_barrier_on_entry()) {</span>
<span class="udiff-line-added">+       // Add check to deoptimize the nmethod once the holder class is fully initialized</span>
<span class="udiff-line-added">+       clinit_deopt();</span>
<span class="udiff-line-added">+     }</span>
<span class="udiff-line-added">+ </span>
      // Add check to deoptimize the nmethod if RTM state was changed
      rtm_deopt();
    }
  
    // Check for bailouts during method entry or RTM state check setup.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -976,29 +978,32 @@</span>
    //    publishes the reference to the newly constructed object. Rather
    //    than wait for the publication, we simply block the writes here.
    //    Rather than put a barrier on only those writes which are required
    //    to complete, we force all writes to complete.
    //
<span class="udiff-line-modified-removed">-   // 2. On PPC64, also add MemBarRelease for constructors which write</span>
<span class="udiff-line-removed">-   //    volatile fields. As support_IRIW_for_not_multiple_copy_atomic_cpu</span>
<span class="udiff-line-removed">-   //    is set on PPC64, no sync instruction is issued after volatile</span>
<span class="udiff-line-removed">-   //    stores. We want to guarantee the same behavior as on platforms</span>
<span class="udiff-line-removed">-   //    with total store order, although this is not required by the Java</span>
<span class="udiff-line-removed">-   //    memory model. So as with finals, we add a barrier here.</span>
<span class="udiff-line-removed">-   //</span>
<span class="udiff-line-removed">-   // 3. Experimental VM option is used to force the barrier if any field</span>
<span class="udiff-line-modified-added">+   // 2. Experimental VM option is used to force the barrier if any field</span>
    //    was written out in the constructor.
    //
<span class="udiff-line-added">+   // 3. On processors which are not CPU_MULTI_COPY_ATOMIC (e.g. PPC64),</span>
<span class="udiff-line-added">+   //    support_IRIW_for_not_multiple_copy_atomic_cpu selects that</span>
<span class="udiff-line-added">+   //    MemBarVolatile is used before volatile load instead of after volatile</span>
<span class="udiff-line-added">+   //    store, so there&#39;s no barrier after the store.</span>
<span class="udiff-line-added">+   //    We want to guarantee the same behavior as on platforms with total store</span>
<span class="udiff-line-added">+   //    order, although this is not required by the Java memory model.</span>
<span class="udiff-line-added">+   //    In this case, we want to enforce visibility of volatile field</span>
<span class="udiff-line-added">+   //    initializations which are performed in constructors.</span>
<span class="udiff-line-added">+   //    So as with finals, we add a barrier here.</span>
<span class="udiff-line-added">+   //</span>
    // &quot;All bets are off&quot; unless the first publication occurs after a
    // normal return from the constructor.  We do not attempt to detect
    // such unusual early publications.  But no barrier is needed on
    // exceptional returns, since they cannot publish normally.
    //
    if (method()-&gt;is_initializer() &amp;&amp;
<span class="udiff-line-modified-removed">-         (wrote_final() ||</span>
<span class="udiff-line-modified-removed">-            PPC64_ONLY(wrote_volatile() ||)</span>
<span class="udiff-line-modified-removed">-            (AlwaysSafeConstructors &amp;&amp; wrote_fields()))) {</span>
<span class="udiff-line-modified-added">+        (wrote_final() ||</span>
<span class="udiff-line-modified-added">+          (AlwaysSafeConstructors &amp;&amp; wrote_fields()) ||</span>
<span class="udiff-line-modified-added">+          (support_IRIW_for_not_multiple_copy_atomic_cpu &amp;&amp; wrote_volatile()))) {</span>
      _exits.insert_mem_bar(Op_MemBarRelease, alloc_with_final());
  
      // If Memory barrier is created for final fields write
      // and allocation node does not escape the initialize method,
      // then barrier introduced by allocation node can be removed.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1026,28 +1031,23 @@</span>
  
    for (MergeMemStream mms(_exits.merged_memory()); mms.next_non_empty(); ) {
      // transform each slice of the original memphi:
      mms.set_memory(_gvn.transform(mms.memory()));
    }
<span class="udiff-line-added">+   // Clean up input MergeMems created by transforming the slices</span>
<span class="udiff-line-added">+   _gvn.transform(_exits.merged_memory());</span>
  
    if (tf()-&gt;range()-&gt;cnt() &gt; TypeFunc::Parms) {
      const Type* ret_type = tf()-&gt;range()-&gt;field_at(TypeFunc::Parms);
      Node*       ret_phi  = _gvn.transform( _exits.argument(0) );
      if (!_exits.control()-&gt;is_top() &amp;&amp; _gvn.type(ret_phi)-&gt;empty()) {
<span class="udiff-line-modified-removed">-       // In case of concurrent class loading, the type we set for the</span>
<span class="udiff-line-modified-removed">-       // ret_phi in build_exits() may have been too optimistic and the</span>
<span class="udiff-line-modified-removed">-       // ret_phi may be top now.</span>
<span class="udiff-line-modified-removed">-       // Otherwise, we&#39;ve encountered an error and have to mark the method as</span>
<span class="udiff-line-modified-removed">-       // not compilable. Just using an assertion instead would be dangerous</span>
<span class="udiff-line-modified-removed">-       // as this could lead to an infinite compile loop in non-debug builds.</span>
<span class="udiff-line-removed">-       {</span>
<span class="udiff-line-removed">-         if (C-&gt;env()-&gt;system_dictionary_modification_counter_changed()) {</span>
<span class="udiff-line-removed">-           C-&gt;record_failure(C2Compiler::retry_class_loading_during_parsing());</span>
<span class="udiff-line-removed">-         } else {</span>
<span class="udiff-line-removed">-           C-&gt;record_method_not_compilable(&quot;Can&#39;t determine return type.&quot;);</span>
<span class="udiff-line-removed">-         }</span>
<span class="udiff-line-removed">-       }</span>
<span class="udiff-line-modified-added">+       // If the type we set for the ret_phi in build_exits() is too optimistic and</span>
<span class="udiff-line-modified-added">+       // the ret_phi is top now, there&#39;s an extremely small chance that it may be due to class</span>
<span class="udiff-line-modified-added">+       // loading.  It could also be due to an error, so mark this method as not compilable because</span>
<span class="udiff-line-modified-added">+       // otherwise this could lead to an infinite compile loop.</span>
<span class="udiff-line-modified-added">+       // In any case, this code path is rarely (and never in my testing) reached.</span>
<span class="udiff-line-modified-added">+       C-&gt;record_method_not_compilable(&quot;Can&#39;t determine return type.&quot;);</span>
        return;
      }
      if (ret_type-&gt;isa_int()) {
        BasicType ret_bt = method()-&gt;return_type()-&gt;basic_type();
        ret_phi = mask_int_value(ret_phi, ret_bt, &amp;_gvn);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1188,11 +1188,11 @@</span>
  //-----------------------------do_method_entry--------------------------------
  // Emit any code needed in the pseudo-block before BCI zero.
  // The main thing to do is lock the receiver of a synchronized method.
  void Parse::do_method_entry() {
    set_parse_bci(InvocationEntryBci); // Pseudo-BCP
<span class="udiff-line-modified-removed">-   set_sp(0);                      // Java Stack Pointer</span>
<span class="udiff-line-modified-added">+   set_sp(0);                         // Java Stack Pointer</span>
  
    NOT_PRODUCT( count_compiled_calls(true/*at_method_entry*/, false/*is_inline*/); )
  
    if (C-&gt;env()-&gt;dtrace_method_probes()) {
      make_dtrace_method_entry(method());
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -2098,15 +2098,28 @@</span>
    }
  
    set_control( _gvn.transform(result_rgn) );
  }
  
<span class="udiff-line-added">+ // Add check to deoptimize once holder klass is fully initialized.</span>
<span class="udiff-line-added">+ void Parse::clinit_deopt() {</span>
<span class="udiff-line-added">+   assert(C-&gt;has_method(), &quot;only for normal compilations&quot;);</span>
<span class="udiff-line-added">+   assert(depth() == 1, &quot;only for main compiled method&quot;);</span>
<span class="udiff-line-added">+   assert(is_normal_parse(), &quot;no barrier needed on osr entry&quot;);</span>
<span class="udiff-line-added">+   assert(!method()-&gt;holder()-&gt;is_not_initialized(), &quot;initialization should have been started&quot;);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   set_parse_bci(0);</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   Node* holder = makecon(TypeKlassPtr::make(method()-&gt;holder()));</span>
<span class="udiff-line-added">+   guard_klass_being_initialized(holder);</span>
<span class="udiff-line-added">+ }</span>
<span class="udiff-line-added">+ </span>
  // Add check to deoptimize if RTM state is not ProfileRTM
  void Parse::rtm_deopt() {
  #if INCLUDE_RTM_OPT
    if (C-&gt;profile_rtm()) {
<span class="udiff-line-modified-removed">-     assert(C-&gt;method() != NULL, &quot;only for normal compilations&quot;);</span>
<span class="udiff-line-modified-added">+     assert(C-&gt;has_method(), &quot;only for normal compilations&quot;);</span>
      assert(!C-&gt;method()-&gt;method_data()-&gt;is_empty(), &quot;MDO is needed to record RTM state&quot;);
      assert(depth() == 1, &quot;generate check only for main compiled method&quot;);
  
      // Set starting bci for uncommon trap.
      set_parse_bci(is_osr_parse() ? osr_bci() : 0);
</pre>
<center><a href="parse.hpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="parse2.cpp.udiff.html" target="_top">next &gt;</a></center>  </body>
</html>