<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/loopopts.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="loopnode.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="machnode.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/loopopts.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1999, 2018, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;gc/shared/barrierSet.hpp&quot;
  27 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  28 #include &quot;memory/allocation.inline.hpp&quot;
  29 #include &quot;memory/resourceArea.hpp&quot;
  30 #include &quot;opto/addnode.hpp&quot;
  31 #include &quot;opto/callnode.hpp&quot;
  32 #include &quot;opto/castnode.hpp&quot;
  33 #include &quot;opto/connode.hpp&quot;
  34 #include &quot;opto/castnode.hpp&quot;
  35 #include &quot;opto/divnode.hpp&quot;
  36 #include &quot;opto/loopnode.hpp&quot;
  37 #include &quot;opto/matcher.hpp&quot;
  38 #include &quot;opto/mulnode.hpp&quot;
  39 #include &quot;opto/movenode.hpp&quot;
  40 #include &quot;opto/opaquenode.hpp&quot;
  41 #include &quot;opto/rootnode.hpp&quot;
  42 #include &quot;opto/subnode.hpp&quot;

  43 #include &quot;utilities/macros.hpp&quot;
<span class="line-removed">  44 #if INCLUDE_ZGC</span>
<span class="line-removed">  45 #include &quot;gc/z/c2/zBarrierSetC2.hpp&quot;</span>
<span class="line-removed">  46 #endif</span>
  47 
  48 //=============================================================================
  49 //------------------------------split_thru_phi---------------------------------
  50 // Split Node &#39;n&#39; through merge point if there is enough win.
  51 Node *PhaseIdealLoop::split_thru_phi( Node *n, Node *region, int policy ) {
  52   if (n-&gt;Opcode() == Op_ConvI2L &amp;&amp; n-&gt;bottom_type() != TypeLong::LONG) {
  53     // ConvI2L may have type information on it which is unsafe to push up
  54     // so disable this for now
  55     return NULL;
  56   }
  57 
  58   // Splitting range check CastIIs through a loop induction Phi can
  59   // cause new Phis to be created that are left unrelated to the loop
  60   // induction Phi and prevent optimizations (vectorization)
  61   if (n-&gt;Opcode() == Op_CastII &amp;&amp; n-&gt;as_CastII()-&gt;has_range_check() &amp;&amp;
  62       region-&gt;is_CountedLoop() &amp;&amp; n-&gt;in(1) == region-&gt;as_CountedLoop()-&gt;phi()) {
  63     return NULL;
  64   }
  65 
  66   int wins = 0;
</pre>
<hr />
<pre>
 112       // irreducible loop may not be indicated by an affirmative is_Loop());
 113       // therefore, the only top we can split thru a phi is on a backedge of
 114       // a loop.
 115       singleton &amp;= region-&gt;is_Loop() &amp;&amp; (i != LoopNode::EntryControl);
 116     }
 117 
 118     if (singleton) {
 119       wins++;
 120       x = ((PhaseGVN&amp;)_igvn).makecon(t);
 121     } else {
 122       // We now call Identity to try to simplify the cloned node.
 123       // Note that some Identity methods call phase-&gt;type(this).
 124       // Make sure that the type array is big enough for
 125       // our new node, even though we may throw the node away.
 126       // (Note: This tweaking with igvn only works because x is a new node.)
 127       _igvn.set_type(x, t);
 128       // If x is a TypeNode, capture any more-precise type permanently into Node
 129       // otherwise it will be not updated during igvn-&gt;transform since
 130       // igvn-&gt;type(x) is set to x-&gt;Value() already.
 131       x-&gt;raise_bottom_type(t);
<span class="line-modified"> 132       Node *y = _igvn.apply_identity(x);</span>
 133       if (y != x) {
 134         wins++;
 135         x = y;
 136       } else {
 137         y = _igvn.hash_find(x);
 138         if (y) {
 139           wins++;
 140           x = y;
 141         } else {
 142           // Else x is a new node we are keeping
 143           // We do not need register_new_node_with_optimizer
 144           // because set_type has already been called.
 145           _igvn._worklist.push(x);
 146         }
 147       }
 148     }
 149     if (x != the_clone &amp;&amp; the_clone != NULL)
 150       _igvn.remove_dead_node(the_clone);
 151     phi-&gt;set_req( i, x );
 152   }
</pre>
<hr />
<pre>
 307   // when splitting &#39;n&#39;.  Since this is unlikely we simply give up.
 308   for( i = 1; i &lt; n-&gt;req(); i++ ) {
 309     Node *m = n-&gt;in(i);
 310     if( get_ctrl(m) == n_ctrl &amp;&amp; !m-&gt;is_Phi() ) {
 311       // We allow the special case of AddP&#39;s with no local inputs.
 312       // This allows us to split-up address expressions.
 313       if (m-&gt;is_AddP() &amp;&amp;
 314           get_ctrl(m-&gt;in(2)) != n_ctrl &amp;&amp;
 315           get_ctrl(m-&gt;in(3)) != n_ctrl) {
 316         // Move the AddP up to dominating point
 317         Node* c = find_non_split_ctrl(idom(n_ctrl));
 318         if (c-&gt;is_OuterStripMinedLoop()) {
 319           c-&gt;as_Loop()-&gt;verify_strip_mined(1);
 320           c = c-&gt;in(LoopNode::EntryControl);
 321         }
 322         set_ctrl_and_loop(m, c);
 323         continue;
 324       }
 325       return NULL;
 326     }
<span class="line-modified"> 327     assert(m-&gt;is_Phi() || is_dominator(get_ctrl(m), n_ctrl), &quot;m has strange control&quot;);</span>
 328   }
 329 
 330   return n_ctrl;
 331 }
 332 
 333 //------------------------------remix_address_expressions----------------------
 334 // Rework addressing expressions to get the most loop-invariant stuff
 335 // moved out.  We&#39;d like to do all associative operators, but it&#39;s especially
 336 // important (common) to do address expressions.
 337 Node *PhaseIdealLoop::remix_address_expressions( Node *n ) {
 338   if (!has_ctrl(n))  return NULL;
 339   Node *n_ctrl = get_ctrl(n);
 340   IdealLoopTree *n_loop = get_loop(n_ctrl);
 341 
 342   // See if &#39;n&#39; mixes loop-varying and loop-invariant inputs and
 343   // itself is loop-varying.
 344 
 345   // Only interested in binary ops (and AddP)
 346   if( n-&gt;req() &lt; 3 || n-&gt;req() &gt; 4 ) return NULL;
 347 
</pre>
<hr />
<pre>
 484           Node *add2 = new AddPNode(n-&gt;in(1), add1, V);
 485           register_new_node(add2, n_ctrl);
 486           _igvn.replace_node(n, add2);
 487           return add2;
 488         }
 489       }
 490     }
 491   }
 492 
 493   return NULL;
 494 }
 495 
 496 // Optimize ((in1[2*i] * in2[2*i]) + (in1[2*i+1] * in2[2*i+1]))
 497 Node *PhaseIdealLoop::convert_add_to_muladd(Node* n) {
 498   assert(n-&gt;Opcode() == Op_AddI, &quot;sanity&quot;);
 499   Node * nn = NULL;
 500   Node * in1 = n-&gt;in(1);
 501   Node * in2 = n-&gt;in(2);
 502   if (in1-&gt;Opcode() == Op_MulI &amp;&amp; in2-&gt;Opcode() == Op_MulI) {
 503     IdealLoopTree* loop_n = get_loop(get_ctrl(n));
<span class="line-modified"> 504     if (loop_n-&gt;_head-&gt;as_Loop()-&gt;is_valid_counted_loop() &amp;&amp;</span>
<span class="line-modified"> 505         Matcher::match_rule_supported(Op_MulAddS2I) &amp;&amp;</span>
<span class="line-modified"> 506         Matcher::match_rule_supported(Op_MulAddVS2VI)) {</span>

 507       Node* mul_in1 = in1-&gt;in(1);
 508       Node* mul_in2 = in1-&gt;in(2);
 509       Node* mul_in3 = in2-&gt;in(1);
 510       Node* mul_in4 = in2-&gt;in(2);
 511       if (mul_in1-&gt;Opcode() == Op_LoadS &amp;&amp;
 512           mul_in2-&gt;Opcode() == Op_LoadS &amp;&amp;
 513           mul_in3-&gt;Opcode() == Op_LoadS &amp;&amp;
 514           mul_in4-&gt;Opcode() == Op_LoadS) {
 515         IdealLoopTree* loop1 = get_loop(get_ctrl(mul_in1));
 516         IdealLoopTree* loop2 = get_loop(get_ctrl(mul_in2));
 517         IdealLoopTree* loop3 = get_loop(get_ctrl(mul_in3));
 518         IdealLoopTree* loop4 = get_loop(get_ctrl(mul_in4));
 519         IdealLoopTree* loop5 = get_loop(get_ctrl(in1));
 520         IdealLoopTree* loop6 = get_loop(get_ctrl(in2));
 521         // All nodes should be in the same counted loop.
 522         if (loop_n == loop1 &amp;&amp; loop_n == loop2 &amp;&amp; loop_n == loop3 &amp;&amp;
 523             loop_n == loop4 &amp;&amp; loop_n == loop5 &amp;&amp; loop_n == loop6) {
 524           Node* adr1 = mul_in1-&gt;in(MemNode::Address);
 525           Node* adr2 = mul_in2-&gt;in(MemNode::Address);
 526           Node* adr3 = mul_in3-&gt;in(MemNode::Address);
</pre>
<hr />
<pre>
 636             cost += ConditionalMoveLimit; // Too much speculative goo
 637       }
 638     }
 639     // See if the Phi is used by a Cmp or Narrow oop Decode/Encode.
 640     // This will likely Split-If, a higher-payoff operation.
 641     for (DUIterator_Fast kmax, k = phi-&gt;fast_outs(kmax); k &lt; kmax; k++) {
 642       Node* use = phi-&gt;fast_out(k);
 643       if (use-&gt;is_Cmp() || use-&gt;is_DecodeNarrowPtr() || use-&gt;is_EncodeNarrowPtr())
 644         cost += ConditionalMoveLimit;
 645       // Is there a use inside the loop?
 646       // Note: check only basic types since CMoveP is pinned.
 647       if (!used_inside_loop &amp;&amp; is_java_primitive(bt)) {
 648         IdealLoopTree* u_loop = get_loop(has_ctrl(use) ? get_ctrl(use) : use);
 649         if (r_loop == u_loop || r_loop-&gt;is_member(u_loop)) {
 650           used_inside_loop = true;
 651         }
 652       }
 653     }
 654   }//for
 655   Node* bol = iff-&gt;in(1);
<span class="line-modified"> 656   assert(bol-&gt;Opcode() == Op_Bool, &quot;&quot;);</span>



 657   int cmp_op = bol-&gt;in(1)-&gt;Opcode();



 658   // It is expensive to generate flags from a float compare.
 659   // Avoid duplicated float compare.
 660   if (phis &gt; 1 &amp;&amp; (cmp_op == Op_CmpF || cmp_op == Op_CmpD)) return NULL;
 661 
 662   float infrequent_prob = PROB_UNLIKELY_MAG(3);
 663   // Ignore cost and blocks frequency if CMOVE can be moved outside the loop.
 664   if (used_inside_loop) {
 665     if (cost &gt;= ConditionalMoveLimit) return NULL; // Too much goo
 666 
 667     // BlockLayoutByFrequency optimization moves infrequent branch
 668     // from hot path. No point in CMOV&#39;ing in such case (110 is used
 669     // instead of 100 to take into account not exactness of float value).
 670     if (BlockLayoutByFrequency) {
 671       infrequent_prob = MAX2(infrequent_prob, (float)BlockLayoutMinDiamondPercentage/110.0f);
 672     }
 673   }
 674   // Check for highly predictable branch.  No point in CMOV&#39;ing if
 675   // we are going to predict accurately all the time.
 676   if (C-&gt;use_cmove() &amp;&amp; (cmp_op == Op_CmpF || cmp_op == Op_CmpD)) {
 677     //keep going
</pre>
<hr />
<pre>
 880             for (uint next = 0; next &lt; wq.size() &amp;&amp; mem_ok; ++next) {
 881               Node *m = wq.at(next);
 882               for (DUIterator_Fast imax, i = m-&gt;fast_outs(imax); i &lt; imax &amp;&amp; mem_ok; i++) {
 883                 Node* u = m-&gt;fast_out(i);
 884                 if (u-&gt;is_Store() || u-&gt;is_Phi()) {
 885                   if (u != n) {
 886                     wq.push(u);
 887                     mem_ok = (wq.size() &lt;= 10);
 888                   }
 889                 } else {
 890                   mem_ok = false;
 891                   break;
 892                 }
 893               }
 894             }
 895           }
 896           if (mem_ok) {
 897             // Move the store out of the loop if the LCA of all
 898             // users (except for the phi) is outside the loop.
 899             Node* hook = new Node(1);

 900             _igvn.rehash_node_delayed(phi);
 901             int count = phi-&gt;replace_edge(n, hook);
 902             assert(count &gt; 0, &quot;inconsistent phi&quot;);
 903 
 904             // Compute latest point this store can go
 905             Node* lca = get_late_ctrl(n, get_ctrl(n));
 906             if (n_loop-&gt;is_member(get_loop(lca))) {
 907               // LCA is in the loop - bail out
 908               _igvn.replace_node(hook, n);
 909               return;
 910             }
 911 #ifdef ASSERT
 912             if (n_loop-&gt;_head-&gt;is_Loop() &amp;&amp; n_loop-&gt;_head-&gt;as_Loop()-&gt;is_strip_mined()) {
 913               assert(n_loop-&gt;_head-&gt;Opcode() == Op_CountedLoop, &quot;outer loop is a strip mined&quot;);
 914               n_loop-&gt;_head-&gt;as_Loop()-&gt;verify_strip_mined(1);
 915               Node* outer = n_loop-&gt;_head-&gt;as_CountedLoop()-&gt;outer_loop();
 916               IdealLoopTree* outer_loop = get_loop(outer);
 917               assert(n_loop-&gt;_parent == outer_loop, &quot;broken loop tree&quot;);
 918               assert(get_loop(lca) == outer_loop, &quot;safepoint in outer loop consume all memory state&quot;);
 919             }
</pre>
<hr />
<pre>
 924             _igvn.replace_input_of(n, 0, lca);
 925             set_ctrl_and_loop(n, lca);
 926 
 927             // Disconnect the phi now. An empty phi can confuse other
 928             // optimizations in this pass of loop opts..
 929             if (phi-&gt;in(LoopNode::LoopBackControl) == phi) {
 930               _igvn.replace_node(phi, phi-&gt;in(LoopNode::EntryControl));
 931               n_loop-&gt;_body.yank(phi);
 932             }
 933           }
 934         }
 935       }
 936     }
 937   }
 938 }
 939 
 940 //------------------------------split_if_with_blocks_pre-----------------------
 941 // Do the real work in a non-recursive function.  Data nodes want to be
 942 // cloned in the pre-order so they can feed each other nicely.
 943 Node *PhaseIdealLoop::split_if_with_blocks_pre( Node *n ) {
<span class="line-removed"> 944   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();</span>
<span class="line-removed"> 945   Node* bs_res = bs-&gt;split_if_pre(this, n);</span>
<span class="line-removed"> 946   if (bs_res != NULL) {</span>
<span class="line-removed"> 947     return bs_res;</span>
<span class="line-removed"> 948   }</span>
 949   // Cloning these guys is unlikely to win
 950   int n_op = n-&gt;Opcode();
 951   if( n_op == Op_MergeMem ) return n;
 952   if( n-&gt;is_Proj() ) return n;
 953   // Do not clone-up CmpFXXX variations, as these are always
 954   // followed by a CmpI
 955   if( n-&gt;is_Cmp() ) return n;
 956   // Attempt to use a conditional move instead of a phi/branch
 957   if( ConditionalMoveLimit &gt; 0 &amp;&amp; n_op == Op_Region ) {
 958     Node *cmov = conditional_move( n );
 959     if( cmov ) return cmov;
 960   }
 961   if( n-&gt;is_CFG() || n-&gt;is_LoadStore() )
 962     return n;
 963   if( n_op == Op_Opaque1 ||     // Opaque nodes cannot be mod&#39;d
 964       n_op == Op_Opaque2 ) {
 965     if( !C-&gt;major_progress() )   // If chance of no more loop opts...
 966       _igvn._worklist.push(n);  // maybe we&#39;ll remove them
 967     return n;
 968   }
</pre>
<hr />
<pre>
1055   }
1056   int nodes_left = C-&gt;max_node_limit() - C-&gt;live_nodes();
1057   if (weight * 8 &gt; nodes_left) {
1058     if (PrintOpto) {
1059       tty-&gt;print_cr(&quot;*** Split-if bails out:  %d nodes, region weight %d&quot;, C-&gt;unique(), weight);
1060     }
1061     return true;
1062   } else {
1063     return false;
1064   }
1065 }
1066 
1067 static bool merge_point_safe(Node* region) {
1068   // 4799512: Stop split_if_with_blocks from splitting a block with a ConvI2LNode
1069   // having a PhiNode input. This sidesteps the dangerous case where the split
1070   // ConvI2LNode may become TOP if the input Value() does not
1071   // overlap the ConvI2L range, leaving a node which may not dominate its
1072   // uses.
1073   // A better fix for this problem can be found in the BugTraq entry, but
1074   // expediency for Mantis demands this hack.
<span class="line-modified">1075   // 6855164: If the merge point has a FastLockNode with a PhiNode input, we stop</span>
<span class="line-removed">1076   // split_if_with_blocks from splitting a block because we could not move around</span>
<span class="line-removed">1077   // the FastLockNode.</span>
1078   for (DUIterator_Fast imax, i = region-&gt;fast_outs(imax); i &lt; imax; i++) {
1079     Node* n = region-&gt;fast_out(i);
1080     if (n-&gt;is_Phi()) {
1081       for (DUIterator_Fast jmax, j = n-&gt;fast_outs(jmax); j &lt; jmax; j++) {
1082         Node* m = n-&gt;fast_out(j);
<span class="line-removed">1083         if (m-&gt;is_FastLock())</span>
<span class="line-removed">1084           return false;</span>
<span class="line-removed">1085 #if INCLUDE_SHENANDOAHGC</span>
<span class="line-removed">1086         if (m-&gt;is_ShenandoahBarrier() &amp;&amp; m-&gt;has_out_with(Op_FastLock)) {</span>
<span class="line-removed">1087           return false;</span>
<span class="line-removed">1088         }</span>
<span class="line-removed">1089 #endif</span>
<span class="line-removed">1090 #ifdef _LP64</span>
1091         if (m-&gt;Opcode() == Op_ConvI2L)
1092           return false;
1093         if (m-&gt;is_CastII() &amp;&amp; m-&gt;isa_CastII()-&gt;has_range_check()) {
1094           return false;
1095         }
<span class="line-removed">1096 #endif</span>
1097       }
1098     }
1099   }

1100   return true;
1101 }
1102 
1103 
1104 //------------------------------place_near_use---------------------------------
1105 // Place some computation next to use but not inside inner loops.
1106 // For inner loop uses move it to the preheader area.
1107 Node *PhaseIdealLoop::place_near_use(Node *useblock) const {
1108   IdealLoopTree *u_loop = get_loop( useblock );
1109   if (u_loop-&gt;_irreducible) {
1110     return useblock;
1111   }
1112   if (u_loop-&gt;_child) {
1113     if (useblock == u_loop-&gt;_head &amp;&amp; u_loop-&gt;_head-&gt;is_OuterStripMinedLoop()) {
1114       return u_loop-&gt;_head-&gt;in(LoopNode::EntryControl);
1115     }
1116     return useblock;
1117   }
1118   return u_loop-&gt;_head-&gt;as_Loop()-&gt;skip_strip_mined()-&gt;in(LoopNode::EntryControl);
1119 }
</pre>
<hr />
<pre>
1179   // body selection for the cloned code.  Also, make sure we check
1180   // for any input path not being in the same loop as n_ctrl.  For
1181   // irreducible loops we cannot check for &#39;n_ctrl-&gt;is_Loop()&#39;
1182   // because the alternative loop entry points won&#39;t be converted
1183   // into LoopNodes.
1184   IdealLoopTree *n_loop = get_loop(n_ctrl);
1185   for (uint j = 1; j &lt; n_ctrl-&gt;req(); j++) {
1186     if (get_loop(n_ctrl-&gt;in(j)) != n_loop) {
1187       return false;
1188     }
1189   }
1190 
1191   // Check for safety of the merge point.
1192   if (!merge_point_safe(n_ctrl)) {
1193     return false;
1194   }
1195 
1196   return true;
1197 }
1198 
















1199 //------------------------------split_if_with_blocks_post----------------------
1200 // Do the real work in a non-recursive function.  CFG hackery wants to be
1201 // in the post-order, so it can dirty the I-DOM info and not use the dirtied
1202 // info.
<span class="line-modified">1203 void PhaseIdealLoop::split_if_with_blocks_post(Node *n, bool last_round) {</span>
1204 
1205   // Cloning Cmp through Phi&#39;s involves the split-if transform.
1206   // FastLock is not used by an If
<span class="line-modified">1207   if (n-&gt;is_Cmp() &amp;&amp; !n-&gt;is_FastLock() &amp;&amp; !last_round) {</span>
1208     Node *n_ctrl = get_ctrl(n);
1209     // Determine if the Node has inputs from some local Phi.
1210     // Returns the block to clone thru.
1211     Node *n_blk = has_local_phi_input(n);
1212     if (n_blk != n_ctrl) {
1213       return;
1214     }
1215 
1216     if (!can_split_if(n_ctrl)) {
1217       return;
1218     }
1219 
1220     if (n-&gt;outcnt() != 1) {
1221       return; // Multiple bool&#39;s from 1 compare?
1222     }
1223     Node *bol = n-&gt;unique_out();
1224     assert(bol-&gt;is_Bool(), &quot;expect a bool here&quot;);
1225     if (bol-&gt;outcnt() != 1) {
1226       return;// Multiple branches from 1 compare?
1227     }
</pre>
<hr />
<pre>
1312 
1313   // Check for an IF ready to split; one that has its
1314   // condition codes input coming from a Phi at the block start.
1315   int n_op = n-&gt;Opcode();
1316 
1317   // Check for an IF being dominated by another IF same test
1318   if (n_op == Op_If ||
1319       n_op == Op_RangeCheck) {
1320     Node *bol = n-&gt;in(1);
1321     uint max = bol-&gt;outcnt();
1322     // Check for same test used more than once?
1323     if (max &gt; 1 &amp;&amp; bol-&gt;is_Bool()) {
1324       // Search up IDOMs to see if this IF is dominated.
1325       Node *cutoff = get_ctrl(bol);
1326 
1327       // Now search up IDOMs till cutoff, looking for a dominating test
1328       Node *prevdom = n;
1329       Node *dom = idom(prevdom);
1330       while (dom != cutoff) {
1331         if (dom-&gt;req() &gt; 1 &amp;&amp; dom-&gt;in(1) == bol &amp;&amp; prevdom-&gt;in(0) == dom) {









1332           // Replace the dominated test with an obvious true or false.
1333           // Place it on the IGVN worklist for later cleanup.
1334           C-&gt;set_major_progress();
1335           dominated_by(prevdom, n, false, true);
1336 #ifndef PRODUCT
1337           if( VerifyLoopOptimizations ) verify();
1338 #endif
1339           return;
1340         }
1341         prevdom = dom;
1342         dom = idom(prevdom);
1343       }
1344     }
1345   }
1346 
1347   // See if a shared loop-varying computation has no loop-varying uses.
1348   // Happens if something is only used for JVM state in uncommon trap exits,
1349   // like various versions of induction variable+offset.  Clone the
1350   // computation per usage to allow it to sink out of the loop.
1351   if (has_ctrl(n) &amp;&amp; !n-&gt;in(0)) {// n not dead and has no control edge (can float about)
</pre>
<hr />
<pre>
1357         Node* u = n-&gt;fast_out(i);
1358         if( !has_ctrl(u) )     break; // Found control user
1359         IdealLoopTree *u_loop = get_loop(get_ctrl(u));
1360         if( u_loop == n_loop ) break; // Found loop-varying use
1361         if( n_loop-&gt;is_member( u_loop ) ) break; // Found use in inner loop
1362         if( u-&gt;Opcode() == Op_Opaque1 ) break; // Found loop limit, bugfix for 4677003
1363       }
1364       bool did_break = (i &lt; imax);  // Did we break out of the previous loop?
1365       if (!did_break &amp;&amp; n-&gt;outcnt() &gt; 1) { // All uses in outer loops!
1366         Node *late_load_ctrl = NULL;
1367         if (n-&gt;is_Load()) {
1368           // If n is a load, get and save the result from get_late_ctrl(),
1369           // to be later used in calculating the control for n&#39;s clones.
1370           clear_dom_lca_tags();
1371           late_load_ctrl = get_late_ctrl(n, n_ctrl);
1372         }
1373         // If n is a load, and the late control is the same as the current
1374         // control, then the cloning of n is a pointless exercise, because
1375         // GVN will ensure that we end up where we started.
1376         if (!n-&gt;is_Load() || late_load_ctrl != n_ctrl) {
<span class="line-removed">1377           BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();</span>
1378           for (DUIterator_Last jmin, j = n-&gt;last_outs(jmin); j &gt;= jmin; ) {
1379             Node *u = n-&gt;last_out(j); // Clone private computation per use
1380             _igvn.rehash_node_delayed(u);
1381             Node *x = n-&gt;clone(); // Clone computation
1382             Node *x_ctrl = NULL;
1383             if( u-&gt;is_Phi() ) {
1384               // Replace all uses of normal nodes.  Replace Phi uses
1385               // individually, so the separate Nodes can sink down
1386               // different paths.
1387               uint k = 1;
1388               while( u-&gt;in(k) != n ) k++;
1389               u-&gt;set_req( k, x );
1390               // x goes next to Phi input path
1391               x_ctrl = u-&gt;in(0)-&gt;in(k);
1392               --j;
1393             } else {              // Normal use
1394               // Replace all uses
1395               for( uint k = 0; k &lt; u-&gt;req(); k++ ) {
1396                 if( u-&gt;in(k) == n ) {
1397                   u-&gt;set_req( k, x );
1398                   --j;
1399                 }
1400               }
1401               x_ctrl = get_ctrl(u);
1402             }
1403 
1404             // Find control for &#39;x&#39; next to use but not inside inner loops.
1405             // For inner loop uses get the preheader area.
1406             x_ctrl = place_near_use(x_ctrl);
1407 
<span class="line-removed">1408             if (bs-&gt;sink_node(this, n, x, x_ctrl, n_ctrl)) {</span>
<span class="line-removed">1409               continue;</span>
<span class="line-removed">1410             }</span>
<span class="line-removed">1411 </span>
1412             if (n-&gt;is_Load()) {
1413               // For loads, add a control edge to a CFG node outside of the loop
1414               // to force them to not combine and return back inside the loop
1415               // during GVN optimization (4641526).
1416               //
1417               // Because we are setting the actual control input, factor in
1418               // the result from get_late_ctrl() so we respect any
1419               // anti-dependences. (6233005).
1420               x_ctrl = dom_lca(late_load_ctrl, x_ctrl);
1421 
1422               // Don&#39;t allow the control input to be a CFG splitting node.
1423               // Such nodes should only have ProjNodes as outs, e.g. IfNode
1424               // should only have IfTrueNode and IfFalseNode (4985384).
1425               x_ctrl = find_non_split_ctrl(x_ctrl);














1426               assert(dom_depth(n_ctrl) &lt;= dom_depth(x_ctrl), &quot;n is later than its clone&quot;);
1427 
1428               x-&gt;set_req(0, x_ctrl);
1429             }
1430             register_new_node(x, x_ctrl);
1431 
1432             // Some institutional knowledge is needed here: &#39;x&#39; is
1433             // yanked because if the optimizer runs GVN on it all the
1434             // cloned x&#39;s will common up and undo this optimization and
<span class="line-modified">1435             // be forced back in the loop.  This is annoying because it</span>
<span class="line-modified">1436             // makes +VerifyOpto report false-positives on progress.  I</span>
<span class="line-removed">1437             // tried setting control edges on the x&#39;s to force them to</span>
1438             // not combine, but the matching gets worried when it tries
1439             // to fold a StoreP and an AddP together (as part of an
1440             // address expression) and the AddP and StoreP have
1441             // different controls.
1442             if (!x-&gt;is_Load() &amp;&amp; !x-&gt;is_DecodeNarrowPtr()) _igvn._worklist.yank(x);
1443           }
1444           _igvn.remove_dead_node(n);
1445         }
1446       }
1447     }
1448   }
1449 
1450   try_move_store_after_loop(n);
1451 
1452   // Check for Opaque2&#39;s who&#39;s loop has disappeared - who&#39;s input is in the
1453   // same loop nest as their output.  Remove &#39;em, they are no longer useful.
1454   if( n_op == Op_Opaque2 &amp;&amp;
1455       n-&gt;in(1) != NULL &amp;&amp;
1456       get_loop(get_ctrl(n)) == get_loop(get_ctrl(n-&gt;in(1))) ) {
1457     _igvn.replace_node( n, n-&gt;in(1) );
1458   }
<span class="line-removed">1459 </span>
<span class="line-removed">1460 #if INCLUDE_ZGC</span>
<span class="line-removed">1461   if (UseZGC) {</span>
<span class="line-removed">1462     ZBarrierSetC2::loop_optimize_gc_barrier(this, n, last_round);</span>
<span class="line-removed">1463   }</span>
<span class="line-removed">1464 #endif</span>
1465 }
1466 
1467 //------------------------------split_if_with_blocks---------------------------
1468 // Check for aggressive application of &#39;split-if&#39; optimization,
1469 // using basic block level info.
<span class="line-modified">1470 void PhaseIdealLoop::split_if_with_blocks(VectorSet &amp;visited, Node_Stack &amp;nstack, bool last_round) {</span>
1471   Node* root = C-&gt;root();
1472   visited.set(root-&gt;_idx); // first, mark root as visited
1473   // Do pre-visit work for root
1474   Node* n   = split_if_with_blocks_pre(root);
1475   uint  cnt = n-&gt;outcnt();
1476   uint  i   = 0;
1477 
1478   while (true) {
1479     // Visit all children
1480     if (i &lt; cnt) {
1481       Node* use = n-&gt;raw_out(i);
1482       ++i;
1483       if (use-&gt;outcnt() != 0 &amp;&amp; !visited.test_set(use-&gt;_idx)) {
1484         // Now do pre-visit work for this use
1485         use = split_if_with_blocks_pre(use);
1486         nstack.push(n, i); // Save parent and next use&#39;s index.
1487         n   = use;         // Process all children of current use.
1488         cnt = use-&gt;outcnt();
1489         i   = 0;
1490       }
1491     }
1492     else {
1493       // All of n&#39;s children have been processed, complete post-processing.
1494       if (cnt != 0 &amp;&amp; !n-&gt;is_Con()) {
1495         assert(has_node(n), &quot;no dead nodes&quot;);
<span class="line-modified">1496         split_if_with_blocks_post(n, last_round);</span>
1497       }
1498       if (must_throttle_split_if()) {
1499         nstack.clear();
1500       }
1501       if (nstack.is_empty()) {
1502         // Finished all nodes on stack.
1503         break;
1504       }
1505       // Get saved parent node and next use&#39;s index. Visit the rest of uses.
1506       n   = nstack.node();
1507       cnt = n-&gt;outcnt();
1508       i   = nstack.index();
1509       nstack.pop();
1510     }
1511   }
1512 }
1513 
1514 
1515 //=============================================================================
1516 //
</pre>
<hr />
<pre>
1688                                                  IdealLoopTree* loop, IdealLoopTree* outer_loop,
1689                                                  Node_List*&amp; split_if_set, Node_List*&amp; split_bool_set,
1690                                                  Node_List*&amp; split_cex_set, Node_List&amp; worklist,
1691                                                  uint new_counter, CloneLoopMode mode) {
1692   Node* nnn = old_new[old-&gt;_idx];
1693   // Copy uses to a worklist, so I can munge the def-use info
1694   // with impunity.
1695   for (DUIterator_Fast jmax, j = old-&gt;fast_outs(jmax); j &lt; jmax; j++)
1696     worklist.push(old-&gt;fast_out(j));
1697 
1698   while( worklist.size() ) {
1699     Node *use = worklist.pop();
1700     if (!has_node(use))  continue; // Ignore dead nodes
1701     if (use-&gt;in(0) == C-&gt;top())  continue;
1702     IdealLoopTree *use_loop = get_loop( has_ctrl(use) ? get_ctrl(use) : use );
1703     // Check for data-use outside of loop - at least one of OLD or USE
1704     // must not be a CFG node.
1705 #ifdef ASSERT
1706     if (loop-&gt;_head-&gt;as_Loop()-&gt;is_strip_mined() &amp;&amp; outer_loop-&gt;is_member(use_loop) &amp;&amp; !loop-&gt;is_member(use_loop) &amp;&amp; old_new[use-&gt;_idx] == NULL) {
1707       Node* sfpt = loop-&gt;_head-&gt;as_CountedLoop()-&gt;outer_safepoint();
<span class="line-modified">1708       assert(mode == ControlAroundStripMined &amp;&amp; use == sfpt, &quot;missed a node&quot;);</span>

1709     }
1710 #endif
1711     if (!loop-&gt;is_member(use_loop) &amp;&amp; !outer_loop-&gt;is_member(use_loop) &amp;&amp; (!old-&gt;is_CFG() || !use-&gt;is_CFG())) {
1712 
1713       // If the Data use is an IF, that means we have an IF outside of the
1714       // loop that is switching on a condition that is set inside of the
1715       // loop.  Happens if people set a loop-exit flag; then test the flag
1716       // in the loop to break the loop, then test is again outside of the
1717       // loop to determine which way the loop exited.
1718       // Loop predicate If node connects to Bool node through Opaque1 node.
1719       if (use-&gt;is_If() || use-&gt;is_CMove() || C-&gt;is_predicate_opaq(use) || use-&gt;Opcode() == Op_Opaque4) {
1720         // Since this code is highly unlikely, we lazily build the worklist
1721         // of such Nodes to go split.
1722         if (!split_if_set) {
1723           ResourceArea *area = Thread::current()-&gt;resource_area();
1724           split_if_set = new Node_List(area);
1725         }
1726         split_if_set-&gt;push(use);
1727       }
1728       if (use-&gt;is_Bool()) {
</pre>
<hr />
<pre>
2650     for (j = 0; j &lt; use-&gt;req(); j++) {
2651       if (use-&gt;in(j) == n) break;
2652     }
2653     assert(j &lt; use-&gt;req(), &quot;must be there&quot;);
2654 
2655     // clone &quot;n&quot; and insert it between the inputs of &quot;n&quot; and the use outside the loop
2656     Node* n_clone = n-&gt;clone();
2657     _igvn.replace_input_of(use, j, n_clone);
2658     cloned++;
2659     Node* use_c;
2660     if (!use-&gt;is_Phi()) {
2661       use_c = has_ctrl(use) ? get_ctrl(use) : use-&gt;in(0);
2662     } else {
2663       // Use in a phi is considered a use in the associated predecessor block
2664       use_c = use-&gt;in(0)-&gt;in(j);
2665     }
2666     set_ctrl(n_clone, use_c);
2667     assert(!loop-&gt;is_member(get_loop(use_c)), &quot;should be outside loop&quot;);
2668     get_loop(use_c)-&gt;_body.push(n_clone);
2669     _igvn.register_new_node_with_optimizer(n_clone);
<span class="line-modified">2670 #if !defined(PRODUCT)</span>
2671     if (TracePartialPeeling) {
2672       tty-&gt;print_cr(&quot;loop exit cloning old: %d new: %d newbb: %d&quot;, n-&gt;_idx, n_clone-&gt;_idx, get_ctrl(n_clone)-&gt;_idx);
2673     }
2674 #endif
2675   }
2676   return cloned;
2677 }
2678 
2679 
2680 //------------------------------ clone_for_special_use_inside_loop -------------------------------------
2681 // clone &quot;n&quot; for special uses that are in the not_peeled region.
2682 // If these def-uses occur in separate blocks, the code generator
2683 // marks the method as not compilable.  For example, if a &quot;BoolNode&quot;
2684 // is in a different basic block than the &quot;IfNode&quot; that uses it, then
2685 // the compilation is aborted in the code generator.
2686 void PhaseIdealLoop::clone_for_special_use_inside_loop( IdealLoopTree *loop, Node* n,
2687                                                         VectorSet&amp; not_peel, Node_List&amp; sink_list, Node_List&amp; worklist ) {
2688   if (n-&gt;is_Phi() || n-&gt;is_Load()) {
2689     return;
2690   }
2691   assert(worklist.size() == 0, &quot;should be empty&quot;);
2692   for (DUIterator_Fast jmax, j = n-&gt;fast_outs(jmax); j &lt; jmax; j++) {
2693     Node* use = n-&gt;fast_out(j);
2694     if ( not_peel.test(use-&gt;_idx) &amp;&amp;
2695          (use-&gt;is_If() || use-&gt;is_CMove() || use-&gt;is_Bool()) &amp;&amp;
2696          use-&gt;in(1) == n)  {
2697       worklist.push(use);
2698     }
2699   }
2700   if (worklist.size() &gt; 0) {
2701     // clone &quot;n&quot; and insert it between inputs of &quot;n&quot; and the use
2702     Node* n_clone = n-&gt;clone();
2703     loop-&gt;_body.push(n_clone);
2704     _igvn.register_new_node_with_optimizer(n_clone);
2705     set_ctrl(n_clone, get_ctrl(n));
2706     sink_list.push(n_clone);
<span class="line-modified">2707     not_peel &lt;&lt;= n_clone-&gt;_idx;  // add n_clone to not_peel set.</span>
<span class="line-modified">2708 #if !defined(PRODUCT)</span>
2709     if (TracePartialPeeling) {
2710       tty-&gt;print_cr(&quot;special not_peeled cloning old: %d new: %d&quot;, n-&gt;_idx, n_clone-&gt;_idx);
2711     }
2712 #endif
2713     while( worklist.size() ) {
2714       Node *use = worklist.pop();
2715       _igvn.rehash_node_delayed(use);
2716       for (uint j = 1; j &lt; use-&gt;req(); j++) {
2717         if (use-&gt;in(j) == n) {
2718           use-&gt;set_req(j, n_clone);
2719         }
2720       }
2721     }
2722   }
2723 }
2724 
2725 
2726 //------------------------------ insert_phi_for_loop -------------------------------------
2727 // Insert phi(lp_entry_val, back_edge_val) at use-&gt;in(idx) for loop lp if phi does not already exist
2728 void PhaseIdealLoop::insert_phi_for_loop( Node* use, uint idx, Node* lp_entry_val, Node* back_edge_val, LoopNode* lp ) {
</pre>
<hr />
<pre>
3017 //         :  |   |     |       |
3018 //         :  |   v    stmt2    |
3019 //         :  | exitB:  |       |
3020 //         :  | stmt4   v       |
3021 //         :  |       ifA orig  |
3022 //         :  |      /  \       |
3023 //         :  |     /    \      |
3024 //         :  |    v     v      |
3025 //         :  |  false  true    |
3026 //         :  |  /        \     |
3027 //         :  v  v         -----+
3028 //          RegionA
3029 //             |
3030 //             v
3031 //           exitA
3032 //
3033 bool PhaseIdealLoop::partial_peel( IdealLoopTree *loop, Node_List &amp;old_new ) {
3034 
3035   assert(!loop-&gt;_head-&gt;is_CountedLoop(), &quot;Non-counted loop only&quot;);
3036   if (!loop-&gt;_head-&gt;is_Loop()) {
<span class="line-modified">3037     return false;  }</span>
<span class="line-modified">3038 </span>
<span class="line-modified">3039   LoopNode *head  = loop-&gt;_head-&gt;as_Loop();</span>
3040 
3041   if (head-&gt;is_partial_peel_loop() || head-&gt;partial_peel_has_failed()) {
3042     return false;
3043   }
3044 
3045   // Check for complex exit control
<span class="line-modified">3046   for(uint ii = 0; ii &lt; loop-&gt;_body.size(); ii++ ) {</span>
3047     Node *n = loop-&gt;_body.at(ii);
3048     int opc = n-&gt;Opcode();
3049     if (n-&gt;is_Call()        ||
3050         opc == Op_Catch     ||
3051         opc == Op_CatchProj ||
3052         opc == Op_Jump      ||
3053         opc == Op_JumpProj) {
<span class="line-modified">3054 #if !defined(PRODUCT)</span>
3055       if (TracePartialPeeling) {
3056         tty-&gt;print_cr(&quot;\nExit control too complex: lp: %d&quot;, head-&gt;_idx);
3057       }
3058 #endif
3059       return false;
3060     }
3061   }
3062 
3063   int dd = dom_depth(head);
3064 
3065   // Step 1: find cut point
3066 
3067   // Walk up dominators to loop head looking for first loop exit
3068   // which is executed on every path thru loop.
3069   IfNode *peel_if = NULL;
3070   IfNode *peel_if_cmpu = NULL;
3071 
3072   Node *iff = loop-&gt;tail();
<span class="line-modified">3073   while( iff != head ) {</span>
<span class="line-modified">3074     if( iff-&gt;is_If() ) {</span>
3075       Node *ctrl = get_ctrl(iff-&gt;in(1));
3076       if (ctrl-&gt;is_top()) return false; // Dead test on live IF.
3077       // If loop-varying exit-test, check for induction variable
<span class="line-modified">3078       if( loop-&gt;is_member(get_loop(ctrl)) &amp;&amp;</span>
3079           loop-&gt;is_loop_exit(iff) &amp;&amp;
3080           is_possible_iv_test(iff)) {
3081         Node* cmp = iff-&gt;in(1)-&gt;in(1);
3082         if (cmp-&gt;Opcode() == Op_CmpI) {
3083           peel_if = iff-&gt;as_If();
3084         } else {
3085           assert(cmp-&gt;Opcode() == Op_CmpU, &quot;must be CmpI or CmpU&quot;);
3086           peel_if_cmpu = iff-&gt;as_If();
3087         }
3088       }
3089     }
3090     iff = idom(iff);
3091   }

3092   // Prefer signed compare over unsigned compare.
3093   IfNode* new_peel_if = NULL;
3094   if (peel_if == NULL) {
3095     if (!PartialPeelAtUnsignedTests || peel_if_cmpu == NULL) {
3096       return false;   // No peel point found
3097     }
3098     new_peel_if = insert_cmpi_loop_exit(peel_if_cmpu, loop);
3099     if (new_peel_if == NULL) {
3100       return false;   // No peel point found
3101     }
3102     peel_if = new_peel_if;
3103   }
3104   Node* last_peel        = stay_in_loop(peel_if, loop);
3105   Node* first_not_peeled = stay_in_loop(last_peel, loop);
3106   if (first_not_peeled == NULL || first_not_peeled == head) {
3107     return false;
3108   }
3109 
<span class="line-modified">3110 #if !defined(PRODUCT)</span>
3111   if (TraceLoopOpts) {
3112     tty-&gt;print(&quot;PartialPeel  &quot;);
3113     loop-&gt;dump_head();
3114   }
3115 
3116   if (TracePartialPeeling) {
3117     tty-&gt;print_cr(&quot;before partial peel one iteration&quot;);
3118     Node_List wl;
3119     Node* t = head-&gt;in(2);
3120     while (true) {
3121       wl.push(t);
3122       if (t == head) break;
3123       t = idom(t);
3124     }
3125     while (wl.size() &gt; 0) {
3126       Node* tt = wl.pop();
3127       tt-&gt;dump();
3128       if (tt == last_peel) tty-&gt;print_cr(&quot;-- cut --&quot;);
3129     }
3130   }
3131 #endif
3132   ResourceArea *area = Thread::current()-&gt;resource_area();
3133   VectorSet peel(area);
3134   VectorSet not_peel(area);
3135   Node_List peel_list(area);
3136   Node_List worklist(area);
3137   Node_List sink_list(area);
3138 





3139   // Set of cfg nodes to peel are those that are executable from
3140   // the head through last_peel.
3141   assert(worklist.size() == 0, &quot;should be empty&quot;);
3142   worklist.push(head);
3143   peel.set(head-&gt;_idx);
3144   while (worklist.size() &gt; 0) {
3145     Node *n = worklist.pop();
3146     if (n != last_peel) {
3147       for (DUIterator_Fast jmax, j = n-&gt;fast_outs(jmax); j &lt; jmax; j++) {
3148         Node* use = n-&gt;fast_out(j);
3149         if (use-&gt;is_CFG() &amp;&amp;
3150             loop-&gt;is_member(get_loop(use)) &amp;&amp;
3151             !peel.test_set(use-&gt;_idx)) {
3152           worklist.push(use);
3153         }
3154       }
3155     }
3156   }
3157 
3158   // Set of non-cfg nodes to peel are those that are control
3159   // dependent on the cfg nodes.
<span class="line-modified">3160   uint i;</span>
<span class="line-removed">3161   for(i = 0; i &lt; loop-&gt;_body.size(); i++ ) {</span>
3162     Node *n = loop-&gt;_body.at(i);
3163     Node *n_c = has_ctrl(n) ? get_ctrl(n) : n;
3164     if (peel.test(n_c-&gt;_idx)) {
3165       peel.set(n-&gt;_idx);
3166     } else {
3167       not_peel.set(n-&gt;_idx);
3168     }
3169   }
3170 
3171   // Step 2: move operations from the peeled section down into the
3172   //         not-peeled section
3173 
3174   // Get a post order schedule of nodes in the peel region
3175   // Result in right-most operand.
<span class="line-modified">3176   scheduled_nodelist(loop, peel, peel_list );</span>
3177 
3178   assert(is_valid_loop_partition(loop, peel, peel_list, not_peel), &quot;bad partition&quot;);
3179 
3180   // For future check for too many new phis
3181   uint old_phi_cnt = 0;
3182   for (DUIterator_Fast jmax, j = head-&gt;fast_outs(jmax); j &lt; jmax; j++) {
3183     Node* use = head-&gt;fast_out(j);
3184     if (use-&gt;is_Phi()) old_phi_cnt++;
3185   }
3186 
<span class="line-modified">3187 #if !defined(PRODUCT)</span>
3188   if (TracePartialPeeling) {
3189     tty-&gt;print_cr(&quot;\npeeled list&quot;);
3190   }
3191 #endif
3192 
3193   // Evacuate nodes in peel region into the not_peeled region if possible
3194   uint new_phi_cnt = 0;
3195   uint cloned_for_outside_use = 0;
<span class="line-modified">3196   for (i = 0; i &lt; peel_list.size();) {</span>
3197     Node* n = peel_list.at(i);
<span class="line-modified">3198 #if !defined(PRODUCT)</span>
3199   if (TracePartialPeeling) n-&gt;dump();
3200 #endif
3201     bool incr = true;
<span class="line-modified">3202     if ( !n-&gt;is_CFG() ) {</span>
<span class="line-modified">3203 </span>
<span class="line-removed">3204       if ( has_use_in_set(n, not_peel) ) {</span>
<span class="line-removed">3205 </span>
3206         // If not used internal to the peeled region,
3207         // move &quot;n&quot; from peeled to not_peeled region.
<span class="line-modified">3208 </span>
<span class="line-removed">3209         if ( !has_use_internal_to_set(n, peel, loop) ) {</span>
<span class="line-removed">3210 </span>
3211           // if not pinned and not a load (which maybe anti-dependent on a store)
3212           // and not a CMove (Matcher expects only bool-&gt;cmove).
<span class="line-modified">3213           if (n-&gt;in(0) == NULL &amp;&amp; !n-&gt;is_Load() &amp;&amp; !n-&gt;is_CMove() &amp;&amp; n-&gt;Opcode() != Op_ShenandoahWBMemProj) {</span>
<span class="line-modified">3214             cloned_for_outside_use += clone_for_use_outside_loop( loop, n, worklist );</span>
3215             sink_list.push(n);
<span class="line-modified">3216             peel     &gt;&gt;= n-&gt;_idx; // delete n from peel set.</span>
<span class="line-modified">3217             not_peel &lt;&lt;= n-&gt;_idx; // add n to not_peel set.</span>
3218             peel_list.remove(i);
3219             incr = false;
<span class="line-modified">3220 #if !defined(PRODUCT)</span>
3221             if (TracePartialPeeling) {
3222               tty-&gt;print_cr(&quot;sink to not_peeled region: %d newbb: %d&quot;,
3223                             n-&gt;_idx, get_ctrl(n)-&gt;_idx);
3224             }
3225 #endif
3226           }
3227         } else {
3228           // Otherwise check for special def-use cases that span
3229           // the peel/not_peel boundary such as bool-&gt;if
<span class="line-modified">3230           clone_for_special_use_inside_loop( loop, n, not_peel, sink_list, worklist );</span>
3231           new_phi_cnt++;
3232         }
3233       }
3234     }
3235     if (incr) i++;
3236   }
3237 
<span class="line-modified">3238   if (new_phi_cnt &gt; old_phi_cnt + PartialPeelNewPhiDelta) {</span>
<span class="line-modified">3239 #if !defined(PRODUCT)</span>




3240     if (TracePartialPeeling) {
3241       tty-&gt;print_cr(&quot;\nToo many new phis: %d  old %d new cmpi: %c&quot;,
3242                     new_phi_cnt, old_phi_cnt, new_peel_if != NULL?&#39;T&#39;:&#39;F&#39;);
3243     }
3244 #endif
3245     if (new_peel_if != NULL) {
3246       remove_cmpi_loop_exit(new_peel_if, loop);
3247     }
3248     // Inhibit more partial peeling on this loop
3249     assert(!head-&gt;is_partial_peel_loop(), &quot;not partial peeled&quot;);
3250     head-&gt;mark_partial_peel_failed();
3251     if (cloned_for_outside_use &gt; 0) {
3252       // Terminate this round of loop opts because
3253       // the graph outside this loop was changed.
3254       C-&gt;set_major_progress();
3255       return true;
3256     }
3257     return false;
3258   }
3259 
</pre>
<hr />
<pre>
3266   _igvn.register_new_node_with_optimizer(new_head);
3267   assert(first_not_peeled-&gt;in(0) == last_peel, &quot;last_peel &lt;- first_not_peeled&quot;);
3268   _igvn.replace_input_of(first_not_peeled, 0, new_head);
3269   set_loop(new_head, loop);
3270   loop-&gt;_body.push(new_head);
3271   not_peel.set(new_head-&gt;_idx);
3272   set_idom(new_head, last_peel, dom_depth(first_not_peeled));
3273   set_idom(first_not_peeled, new_head, dom_depth(first_not_peeled));
3274 
3275   while (sink_list.size() &gt; 0) {
3276     Node* n = sink_list.pop();
3277     set_ctrl(n, new_head);
3278   }
3279 
3280   assert(is_valid_loop_partition(loop, peel, peel_list, not_peel), &quot;bad partition&quot;);
3281 
3282   clone_loop(loop, old_new, dd, IgnoreStripMined);
3283 
3284   const uint clone_exit_idx = 1;
3285   const uint orig_exit_idx  = 2;
<span class="line-modified">3286   assert(is_valid_clone_loop_form( loop, peel_list, orig_exit_idx, clone_exit_idx ), &quot;bad clone loop&quot;);</span>
3287 
3288   Node* head_clone             = old_new[head-&gt;_idx];
3289   LoopNode* new_head_clone     = old_new[new_head-&gt;_idx]-&gt;as_Loop();
3290   Node* orig_tail_clone        = head_clone-&gt;in(2);
3291 
3292   // Add phi if &quot;def&quot; node is in peel set and &quot;use&quot; is not
3293 
<span class="line-modified">3294   for(i = 0; i &lt; peel_list.size(); i++ ) {</span>
3295     Node *def  = peel_list.at(i);
3296     if (!def-&gt;is_CFG()) {
3297       for (DUIterator_Fast jmax, j = def-&gt;fast_outs(jmax); j &lt; jmax; j++) {
3298         Node *use = def-&gt;fast_out(j);
3299         if (has_node(use) &amp;&amp; use-&gt;in(0) != C-&gt;top() &amp;&amp;
3300             (!peel.test(use-&gt;_idx) ||
3301              (use-&gt;is_Phi() &amp;&amp; use-&gt;in(0) == head)) ) {
3302           worklist.push(use);
3303         }
3304       }
3305       while( worklist.size() ) {
3306         Node *use = worklist.pop();
3307         for (uint j = 1; j &lt; use-&gt;req(); j++) {
3308           Node* n = use-&gt;in(j);
3309           if (n == def) {
3310 
3311             // &quot;def&quot; is in peel set, &quot;use&quot; is not in peel set
3312             // or &quot;use&quot; is in the entry boundary (a phi) of the peel set
3313 
3314             Node* use_c = has_ctrl(use) ? get_ctrl(use) : use;
</pre>
<hr />
<pre>
3330               }
3331             }
3332           }
3333         }
3334       }
3335     }
3336   }
3337 
3338   // Step 3b: retarget control
3339 
3340   // Redirect control to the new loop head if a cloned node in
3341   // the not_peeled region has control that points into the peeled region.
3342   // This necessary because the cloned peeled region will be outside
3343   // the loop.
3344   //                            from    to
3345   //          cloned-peeled    &lt;---+
3346   //    new_head_clone:            |    &lt;--+
3347   //          cloned-not_peeled  in(0)    in(0)
3348   //          orig-peeled
3349 
<span class="line-modified">3350   for(i = 0; i &lt; loop-&gt;_body.size(); i++ ) {</span>
3351     Node *n = loop-&gt;_body.at(i);
3352     if (!n-&gt;is_CFG()           &amp;&amp; n-&gt;in(0) != NULL        &amp;&amp;
3353         not_peel.test(n-&gt;_idx) &amp;&amp; peel.test(n-&gt;in(0)-&gt;_idx)) {
3354       Node* n_clone = old_new[n-&gt;_idx];
3355       _igvn.replace_input_of(n_clone, 0, new_head_clone);
3356     }
3357   }
3358 
3359   // Backedge of the surviving new_head (the clone) is original last_peel
3360   _igvn.replace_input_of(new_head_clone, LoopNode::LoopBackControl, last_peel);
3361 
3362   // Cut first node in original not_peel set
3363   _igvn.rehash_node_delayed(new_head);                     // Multiple edge updates:
3364   new_head-&gt;set_req(LoopNode::EntryControl,    C-&gt;top());  //   use rehash_node_delayed / set_req instead of
3365   new_head-&gt;set_req(LoopNode::LoopBackControl, C-&gt;top());  //   multiple replace_input_of calls
3366 
3367   // Copy head_clone back-branch info to original head
3368   // and remove original head&#39;s loop entry and
3369   // clone head&#39;s back-branch
3370   _igvn.rehash_node_delayed(head); // Multiple edge updates
</pre>
<hr />
<pre>
3377     Node* use = head-&gt;fast_out(k);
3378     if (use-&gt;is_Phi() &amp;&amp; use-&gt;outcnt() &gt; 0) {
3379       Node* use_clone = old_new[use-&gt;_idx];
3380       _igvn.rehash_node_delayed(use); // Multiple edge updates
3381       use-&gt;set_req(LoopNode::EntryControl,    use_clone-&gt;in(LoopNode::LoopBackControl));
3382       use-&gt;set_req(LoopNode::LoopBackControl, C-&gt;top());
3383       _igvn.replace_input_of(use_clone, LoopNode::LoopBackControl, C-&gt;top());
3384     }
3385   }
3386 
3387   // Step 4: update dominator tree and dominator depth
3388 
3389   set_idom(head, orig_tail_clone, dd);
3390   recompute_dom_depth();
3391 
3392   // Inhibit more partial peeling on this loop
3393   new_head_clone-&gt;set_partial_peel_loop();
3394   C-&gt;set_major_progress();
3395   loop-&gt;record_for_igvn();
3396 
<span class="line-modified">3397 #if !defined(PRODUCT)</span>
3398   if (TracePartialPeeling) {
3399     tty-&gt;print_cr(&quot;\nafter partial peel one iteration&quot;);
3400     Node_List wl(area);
3401     Node* t = last_peel;
3402     while (true) {
3403       wl.push(t);
3404       if (t == head_clone) break;
3405       t = idom(t);
3406     }
3407     while (wl.size() &gt; 0) {
3408       Node* tt = wl.pop();
3409       if (tt == head) tty-&gt;print_cr(&quot;orig head&quot;);
3410       else if (tt == new_head_clone) tty-&gt;print_cr(&quot;new head&quot;);
3411       else if (tt == head_clone) tty-&gt;print_cr(&quot;clone head&quot;);
3412       tt-&gt;dump();
3413     }
3414   }
3415 #endif
3416   return true;
3417 }
3418 
3419 //------------------------------reorg_offsets----------------------------------
3420 // Reorganize offset computations to lower register pressure.  Mostly
3421 // prevent loop-fallout uses of the pre-incremented trip counter (which are
3422 // then alive with the post-incremented trip counter forcing an extra
3423 // register move)
3424 void PhaseIdealLoop::reorg_offsets(IdealLoopTree *loop) {
3425   // Perform it only for canonical counted loops.
3426   // Loop&#39;s shape could be messed up by iteration_split_impl.
3427   if (!loop-&gt;_head-&gt;is_CountedLoop())
3428     return;
3429   if (!loop-&gt;_head-&gt;as_Loop()-&gt;is_valid_counted_loop())
3430     return;
3431 
3432   CountedLoopNode *cl = loop-&gt;_head-&gt;as_CountedLoop();
3433   CountedLoopEndNode *cle = cl-&gt;loopexit();
3434   Node *exit = cle-&gt;proj_out(false);
3435   Node *phi = cl-&gt;phi();
3436 
<span class="line-modified">3437   // Check for the special case of folks using the pre-incremented</span>
<span class="line-modified">3438   // trip-counter on the fall-out path (forces the pre-incremented</span>
<span class="line-modified">3439   // and post-incremented trip counter to be live at the same time).</span>
<span class="line-modified">3440   // Fix this by adjusting to use the post-increment trip counter.</span>
3441 
3442   bool progress = true;
3443   while (progress) {
3444     progress = false;
3445     for (DUIterator_Fast imax, i = phi-&gt;fast_outs(imax); i &lt; imax; i++) {
3446       Node* use = phi-&gt;fast_out(i);   // User of trip-counter
3447       if (!has_ctrl(use))  continue;
3448       Node *u_ctrl = get_ctrl(use);
3449       if (use-&gt;is_Phi()) {
3450         u_ctrl = NULL;
3451         for (uint j = 1; j &lt; use-&gt;req(); j++)
3452           if (use-&gt;in(j) == phi)
3453             u_ctrl = dom_lca(u_ctrl, use-&gt;in(0)-&gt;in(j));
3454       }
3455       IdealLoopTree *u_loop = get_loop(u_ctrl);
3456       // Look for loop-invariant use
3457       if (u_loop == loop) continue;
3458       if (loop-&gt;is_member(u_loop)) continue;
3459       // Check that use is live out the bottom.  Assuming the trip-counter
3460       // update is right at the bottom, uses of of the loop middle are ok.
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;gc/shared/barrierSet.hpp&quot;
  27 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  28 #include &quot;memory/allocation.inline.hpp&quot;
  29 #include &quot;memory/resourceArea.hpp&quot;
  30 #include &quot;opto/addnode.hpp&quot;
  31 #include &quot;opto/callnode.hpp&quot;
  32 #include &quot;opto/castnode.hpp&quot;
  33 #include &quot;opto/connode.hpp&quot;
  34 #include &quot;opto/castnode.hpp&quot;
  35 #include &quot;opto/divnode.hpp&quot;
  36 #include &quot;opto/loopnode.hpp&quot;
  37 #include &quot;opto/matcher.hpp&quot;
  38 #include &quot;opto/mulnode.hpp&quot;
  39 #include &quot;opto/movenode.hpp&quot;
  40 #include &quot;opto/opaquenode.hpp&quot;
  41 #include &quot;opto/rootnode.hpp&quot;
  42 #include &quot;opto/subnode.hpp&quot;
<span class="line-added">  43 #include &quot;opto/subtypenode.hpp&quot;</span>
  44 #include &quot;utilities/macros.hpp&quot;



  45 
  46 //=============================================================================
  47 //------------------------------split_thru_phi---------------------------------
  48 // Split Node &#39;n&#39; through merge point if there is enough win.
  49 Node *PhaseIdealLoop::split_thru_phi( Node *n, Node *region, int policy ) {
  50   if (n-&gt;Opcode() == Op_ConvI2L &amp;&amp; n-&gt;bottom_type() != TypeLong::LONG) {
  51     // ConvI2L may have type information on it which is unsafe to push up
  52     // so disable this for now
  53     return NULL;
  54   }
  55 
  56   // Splitting range check CastIIs through a loop induction Phi can
  57   // cause new Phis to be created that are left unrelated to the loop
  58   // induction Phi and prevent optimizations (vectorization)
  59   if (n-&gt;Opcode() == Op_CastII &amp;&amp; n-&gt;as_CastII()-&gt;has_range_check() &amp;&amp;
  60       region-&gt;is_CountedLoop() &amp;&amp; n-&gt;in(1) == region-&gt;as_CountedLoop()-&gt;phi()) {
  61     return NULL;
  62   }
  63 
  64   int wins = 0;
</pre>
<hr />
<pre>
 110       // irreducible loop may not be indicated by an affirmative is_Loop());
 111       // therefore, the only top we can split thru a phi is on a backedge of
 112       // a loop.
 113       singleton &amp;= region-&gt;is_Loop() &amp;&amp; (i != LoopNode::EntryControl);
 114     }
 115 
 116     if (singleton) {
 117       wins++;
 118       x = ((PhaseGVN&amp;)_igvn).makecon(t);
 119     } else {
 120       // We now call Identity to try to simplify the cloned node.
 121       // Note that some Identity methods call phase-&gt;type(this).
 122       // Make sure that the type array is big enough for
 123       // our new node, even though we may throw the node away.
 124       // (Note: This tweaking with igvn only works because x is a new node.)
 125       _igvn.set_type(x, t);
 126       // If x is a TypeNode, capture any more-precise type permanently into Node
 127       // otherwise it will be not updated during igvn-&gt;transform since
 128       // igvn-&gt;type(x) is set to x-&gt;Value() already.
 129       x-&gt;raise_bottom_type(t);
<span class="line-modified"> 130       Node* y = x-&gt;Identity(&amp;_igvn);</span>
 131       if (y != x) {
 132         wins++;
 133         x = y;
 134       } else {
 135         y = _igvn.hash_find(x);
 136         if (y) {
 137           wins++;
 138           x = y;
 139         } else {
 140           // Else x is a new node we are keeping
 141           // We do not need register_new_node_with_optimizer
 142           // because set_type has already been called.
 143           _igvn._worklist.push(x);
 144         }
 145       }
 146     }
 147     if (x != the_clone &amp;&amp; the_clone != NULL)
 148       _igvn.remove_dead_node(the_clone);
 149     phi-&gt;set_req( i, x );
 150   }
</pre>
<hr />
<pre>
 305   // when splitting &#39;n&#39;.  Since this is unlikely we simply give up.
 306   for( i = 1; i &lt; n-&gt;req(); i++ ) {
 307     Node *m = n-&gt;in(i);
 308     if( get_ctrl(m) == n_ctrl &amp;&amp; !m-&gt;is_Phi() ) {
 309       // We allow the special case of AddP&#39;s with no local inputs.
 310       // This allows us to split-up address expressions.
 311       if (m-&gt;is_AddP() &amp;&amp;
 312           get_ctrl(m-&gt;in(2)) != n_ctrl &amp;&amp;
 313           get_ctrl(m-&gt;in(3)) != n_ctrl) {
 314         // Move the AddP up to dominating point
 315         Node* c = find_non_split_ctrl(idom(n_ctrl));
 316         if (c-&gt;is_OuterStripMinedLoop()) {
 317           c-&gt;as_Loop()-&gt;verify_strip_mined(1);
 318           c = c-&gt;in(LoopNode::EntryControl);
 319         }
 320         set_ctrl_and_loop(m, c);
 321         continue;
 322       }
 323       return NULL;
 324     }
<span class="line-modified"> 325     assert(n-&gt;is_Phi() || m-&gt;is_Phi() || is_dominator(get_ctrl(m), n_ctrl), &quot;m has strange control&quot;);</span>
 326   }
 327 
 328   return n_ctrl;
 329 }
 330 
 331 //------------------------------remix_address_expressions----------------------
 332 // Rework addressing expressions to get the most loop-invariant stuff
 333 // moved out.  We&#39;d like to do all associative operators, but it&#39;s especially
 334 // important (common) to do address expressions.
 335 Node *PhaseIdealLoop::remix_address_expressions( Node *n ) {
 336   if (!has_ctrl(n))  return NULL;
 337   Node *n_ctrl = get_ctrl(n);
 338   IdealLoopTree *n_loop = get_loop(n_ctrl);
 339 
 340   // See if &#39;n&#39; mixes loop-varying and loop-invariant inputs and
 341   // itself is loop-varying.
 342 
 343   // Only interested in binary ops (and AddP)
 344   if( n-&gt;req() &lt; 3 || n-&gt;req() &gt; 4 ) return NULL;
 345 
</pre>
<hr />
<pre>
 482           Node *add2 = new AddPNode(n-&gt;in(1), add1, V);
 483           register_new_node(add2, n_ctrl);
 484           _igvn.replace_node(n, add2);
 485           return add2;
 486         }
 487       }
 488     }
 489   }
 490 
 491   return NULL;
 492 }
 493 
 494 // Optimize ((in1[2*i] * in2[2*i]) + (in1[2*i+1] * in2[2*i+1]))
 495 Node *PhaseIdealLoop::convert_add_to_muladd(Node* n) {
 496   assert(n-&gt;Opcode() == Op_AddI, &quot;sanity&quot;);
 497   Node * nn = NULL;
 498   Node * in1 = n-&gt;in(1);
 499   Node * in2 = n-&gt;in(2);
 500   if (in1-&gt;Opcode() == Op_MulI &amp;&amp; in2-&gt;Opcode() == Op_MulI) {
 501     IdealLoopTree* loop_n = get_loop(get_ctrl(n));
<span class="line-modified"> 502     if (loop_n-&gt;is_counted() &amp;&amp;</span>
<span class="line-modified"> 503         loop_n-&gt;_head-&gt;as_Loop()-&gt;is_valid_counted_loop() &amp;&amp;</span>
<span class="line-modified"> 504         Matcher::match_rule_supported(Op_MulAddVS2VI) &amp;&amp;</span>
<span class="line-added"> 505         Matcher::match_rule_supported(Op_MulAddS2I)) {</span>
 506       Node* mul_in1 = in1-&gt;in(1);
 507       Node* mul_in2 = in1-&gt;in(2);
 508       Node* mul_in3 = in2-&gt;in(1);
 509       Node* mul_in4 = in2-&gt;in(2);
 510       if (mul_in1-&gt;Opcode() == Op_LoadS &amp;&amp;
 511           mul_in2-&gt;Opcode() == Op_LoadS &amp;&amp;
 512           mul_in3-&gt;Opcode() == Op_LoadS &amp;&amp;
 513           mul_in4-&gt;Opcode() == Op_LoadS) {
 514         IdealLoopTree* loop1 = get_loop(get_ctrl(mul_in1));
 515         IdealLoopTree* loop2 = get_loop(get_ctrl(mul_in2));
 516         IdealLoopTree* loop3 = get_loop(get_ctrl(mul_in3));
 517         IdealLoopTree* loop4 = get_loop(get_ctrl(mul_in4));
 518         IdealLoopTree* loop5 = get_loop(get_ctrl(in1));
 519         IdealLoopTree* loop6 = get_loop(get_ctrl(in2));
 520         // All nodes should be in the same counted loop.
 521         if (loop_n == loop1 &amp;&amp; loop_n == loop2 &amp;&amp; loop_n == loop3 &amp;&amp;
 522             loop_n == loop4 &amp;&amp; loop_n == loop5 &amp;&amp; loop_n == loop6) {
 523           Node* adr1 = mul_in1-&gt;in(MemNode::Address);
 524           Node* adr2 = mul_in2-&gt;in(MemNode::Address);
 525           Node* adr3 = mul_in3-&gt;in(MemNode::Address);
</pre>
<hr />
<pre>
 635             cost += ConditionalMoveLimit; // Too much speculative goo
 636       }
 637     }
 638     // See if the Phi is used by a Cmp or Narrow oop Decode/Encode.
 639     // This will likely Split-If, a higher-payoff operation.
 640     for (DUIterator_Fast kmax, k = phi-&gt;fast_outs(kmax); k &lt; kmax; k++) {
 641       Node* use = phi-&gt;fast_out(k);
 642       if (use-&gt;is_Cmp() || use-&gt;is_DecodeNarrowPtr() || use-&gt;is_EncodeNarrowPtr())
 643         cost += ConditionalMoveLimit;
 644       // Is there a use inside the loop?
 645       // Note: check only basic types since CMoveP is pinned.
 646       if (!used_inside_loop &amp;&amp; is_java_primitive(bt)) {
 647         IdealLoopTree* u_loop = get_loop(has_ctrl(use) ? get_ctrl(use) : use);
 648         if (r_loop == u_loop || r_loop-&gt;is_member(u_loop)) {
 649           used_inside_loop = true;
 650         }
 651       }
 652     }
 653   }//for
 654   Node* bol = iff-&gt;in(1);
<span class="line-modified"> 655   if (bol-&gt;Opcode() == Op_Opaque4) {</span>
<span class="line-added"> 656     return NULL; // Ignore loop predicate checks (the Opaque4 ensures they will go away)</span>
<span class="line-added"> 657   }</span>
<span class="line-added"> 658   assert(bol-&gt;Opcode() == Op_Bool, &quot;Unexpected node&quot;);</span>
 659   int cmp_op = bol-&gt;in(1)-&gt;Opcode();
<span class="line-added"> 660   if (cmp_op == Op_SubTypeCheck) { // SubTypeCheck expansion expects an IfNode</span>
<span class="line-added"> 661     return NULL;</span>
<span class="line-added"> 662   }</span>
 663   // It is expensive to generate flags from a float compare.
 664   // Avoid duplicated float compare.
 665   if (phis &gt; 1 &amp;&amp; (cmp_op == Op_CmpF || cmp_op == Op_CmpD)) return NULL;
 666 
 667   float infrequent_prob = PROB_UNLIKELY_MAG(3);
 668   // Ignore cost and blocks frequency if CMOVE can be moved outside the loop.
 669   if (used_inside_loop) {
 670     if (cost &gt;= ConditionalMoveLimit) return NULL; // Too much goo
 671 
 672     // BlockLayoutByFrequency optimization moves infrequent branch
 673     // from hot path. No point in CMOV&#39;ing in such case (110 is used
 674     // instead of 100 to take into account not exactness of float value).
 675     if (BlockLayoutByFrequency) {
 676       infrequent_prob = MAX2(infrequent_prob, (float)BlockLayoutMinDiamondPercentage/110.0f);
 677     }
 678   }
 679   // Check for highly predictable branch.  No point in CMOV&#39;ing if
 680   // we are going to predict accurately all the time.
 681   if (C-&gt;use_cmove() &amp;&amp; (cmp_op == Op_CmpF || cmp_op == Op_CmpD)) {
 682     //keep going
</pre>
<hr />
<pre>
 885             for (uint next = 0; next &lt; wq.size() &amp;&amp; mem_ok; ++next) {
 886               Node *m = wq.at(next);
 887               for (DUIterator_Fast imax, i = m-&gt;fast_outs(imax); i &lt; imax &amp;&amp; mem_ok; i++) {
 888                 Node* u = m-&gt;fast_out(i);
 889                 if (u-&gt;is_Store() || u-&gt;is_Phi()) {
 890                   if (u != n) {
 891                     wq.push(u);
 892                     mem_ok = (wq.size() &lt;= 10);
 893                   }
 894                 } else {
 895                   mem_ok = false;
 896                   break;
 897                 }
 898               }
 899             }
 900           }
 901           if (mem_ok) {
 902             // Move the store out of the loop if the LCA of all
 903             // users (except for the phi) is outside the loop.
 904             Node* hook = new Node(1);
<span class="line-added"> 905             hook-&gt;init_req(0, n_ctrl); // Add an input to prevent hook from being dead</span>
 906             _igvn.rehash_node_delayed(phi);
 907             int count = phi-&gt;replace_edge(n, hook);
 908             assert(count &gt; 0, &quot;inconsistent phi&quot;);
 909 
 910             // Compute latest point this store can go
 911             Node* lca = get_late_ctrl(n, get_ctrl(n));
 912             if (n_loop-&gt;is_member(get_loop(lca))) {
 913               // LCA is in the loop - bail out
 914               _igvn.replace_node(hook, n);
 915               return;
 916             }
 917 #ifdef ASSERT
 918             if (n_loop-&gt;_head-&gt;is_Loop() &amp;&amp; n_loop-&gt;_head-&gt;as_Loop()-&gt;is_strip_mined()) {
 919               assert(n_loop-&gt;_head-&gt;Opcode() == Op_CountedLoop, &quot;outer loop is a strip mined&quot;);
 920               n_loop-&gt;_head-&gt;as_Loop()-&gt;verify_strip_mined(1);
 921               Node* outer = n_loop-&gt;_head-&gt;as_CountedLoop()-&gt;outer_loop();
 922               IdealLoopTree* outer_loop = get_loop(outer);
 923               assert(n_loop-&gt;_parent == outer_loop, &quot;broken loop tree&quot;);
 924               assert(get_loop(lca) == outer_loop, &quot;safepoint in outer loop consume all memory state&quot;);
 925             }
</pre>
<hr />
<pre>
 930             _igvn.replace_input_of(n, 0, lca);
 931             set_ctrl_and_loop(n, lca);
 932 
 933             // Disconnect the phi now. An empty phi can confuse other
 934             // optimizations in this pass of loop opts..
 935             if (phi-&gt;in(LoopNode::LoopBackControl) == phi) {
 936               _igvn.replace_node(phi, phi-&gt;in(LoopNode::EntryControl));
 937               n_loop-&gt;_body.yank(phi);
 938             }
 939           }
 940         }
 941       }
 942     }
 943   }
 944 }
 945 
 946 //------------------------------split_if_with_blocks_pre-----------------------
 947 // Do the real work in a non-recursive function.  Data nodes want to be
 948 // cloned in the pre-order so they can feed each other nicely.
 949 Node *PhaseIdealLoop::split_if_with_blocks_pre( Node *n ) {





 950   // Cloning these guys is unlikely to win
 951   int n_op = n-&gt;Opcode();
 952   if( n_op == Op_MergeMem ) return n;
 953   if( n-&gt;is_Proj() ) return n;
 954   // Do not clone-up CmpFXXX variations, as these are always
 955   // followed by a CmpI
 956   if( n-&gt;is_Cmp() ) return n;
 957   // Attempt to use a conditional move instead of a phi/branch
 958   if( ConditionalMoveLimit &gt; 0 &amp;&amp; n_op == Op_Region ) {
 959     Node *cmov = conditional_move( n );
 960     if( cmov ) return cmov;
 961   }
 962   if( n-&gt;is_CFG() || n-&gt;is_LoadStore() )
 963     return n;
 964   if( n_op == Op_Opaque1 ||     // Opaque nodes cannot be mod&#39;d
 965       n_op == Op_Opaque2 ) {
 966     if( !C-&gt;major_progress() )   // If chance of no more loop opts...
 967       _igvn._worklist.push(n);  // maybe we&#39;ll remove them
 968     return n;
 969   }
</pre>
<hr />
<pre>
1056   }
1057   int nodes_left = C-&gt;max_node_limit() - C-&gt;live_nodes();
1058   if (weight * 8 &gt; nodes_left) {
1059     if (PrintOpto) {
1060       tty-&gt;print_cr(&quot;*** Split-if bails out:  %d nodes, region weight %d&quot;, C-&gt;unique(), weight);
1061     }
1062     return true;
1063   } else {
1064     return false;
1065   }
1066 }
1067 
1068 static bool merge_point_safe(Node* region) {
1069   // 4799512: Stop split_if_with_blocks from splitting a block with a ConvI2LNode
1070   // having a PhiNode input. This sidesteps the dangerous case where the split
1071   // ConvI2LNode may become TOP if the input Value() does not
1072   // overlap the ConvI2L range, leaving a node which may not dominate its
1073   // uses.
1074   // A better fix for this problem can be found in the BugTraq entry, but
1075   // expediency for Mantis demands this hack.
<span class="line-modified">1076 #ifdef _LP64</span>


1077   for (DUIterator_Fast imax, i = region-&gt;fast_outs(imax); i &lt; imax; i++) {
1078     Node* n = region-&gt;fast_out(i);
1079     if (n-&gt;is_Phi()) {
1080       for (DUIterator_Fast jmax, j = n-&gt;fast_outs(jmax); j &lt; jmax; j++) {
1081         Node* m = n-&gt;fast_out(j);








1082         if (m-&gt;Opcode() == Op_ConvI2L)
1083           return false;
1084         if (m-&gt;is_CastII() &amp;&amp; m-&gt;isa_CastII()-&gt;has_range_check()) {
1085           return false;
1086         }

1087       }
1088     }
1089   }
<span class="line-added">1090 #endif</span>
1091   return true;
1092 }
1093 
1094 
1095 //------------------------------place_near_use---------------------------------
1096 // Place some computation next to use but not inside inner loops.
1097 // For inner loop uses move it to the preheader area.
1098 Node *PhaseIdealLoop::place_near_use(Node *useblock) const {
1099   IdealLoopTree *u_loop = get_loop( useblock );
1100   if (u_loop-&gt;_irreducible) {
1101     return useblock;
1102   }
1103   if (u_loop-&gt;_child) {
1104     if (useblock == u_loop-&gt;_head &amp;&amp; u_loop-&gt;_head-&gt;is_OuterStripMinedLoop()) {
1105       return u_loop-&gt;_head-&gt;in(LoopNode::EntryControl);
1106     }
1107     return useblock;
1108   }
1109   return u_loop-&gt;_head-&gt;as_Loop()-&gt;skip_strip_mined()-&gt;in(LoopNode::EntryControl);
1110 }
</pre>
<hr />
<pre>
1170   // body selection for the cloned code.  Also, make sure we check
1171   // for any input path not being in the same loop as n_ctrl.  For
1172   // irreducible loops we cannot check for &#39;n_ctrl-&gt;is_Loop()&#39;
1173   // because the alternative loop entry points won&#39;t be converted
1174   // into LoopNodes.
1175   IdealLoopTree *n_loop = get_loop(n_ctrl);
1176   for (uint j = 1; j &lt; n_ctrl-&gt;req(); j++) {
1177     if (get_loop(n_ctrl-&gt;in(j)) != n_loop) {
1178       return false;
1179     }
1180   }
1181 
1182   // Check for safety of the merge point.
1183   if (!merge_point_safe(n_ctrl)) {
1184     return false;
1185   }
1186 
1187   return true;
1188 }
1189 
<span class="line-added">1190 // Detect if the node is the inner strip-mined loop</span>
<span class="line-added">1191 // Return: NULL if it&#39;s not the case, or the exit of outer strip-mined loop</span>
<span class="line-added">1192 static Node* is_inner_of_stripmined_loop(const Node* out) {</span>
<span class="line-added">1193   Node* out_le = NULL;</span>
<span class="line-added">1194 </span>
<span class="line-added">1195   if (out-&gt;is_CountedLoopEnd()) {</span>
<span class="line-added">1196       const CountedLoopNode* loop = out-&gt;as_CountedLoopEnd()-&gt;loopnode();</span>
<span class="line-added">1197 </span>
<span class="line-added">1198       if (loop != NULL &amp;&amp; loop-&gt;is_strip_mined()) {</span>
<span class="line-added">1199         out_le = loop-&gt;in(LoopNode::EntryControl)-&gt;as_OuterStripMinedLoop()-&gt;outer_loop_exit();</span>
<span class="line-added">1200       }</span>
<span class="line-added">1201   }</span>
<span class="line-added">1202 </span>
<span class="line-added">1203   return out_le;</span>
<span class="line-added">1204 }</span>
<span class="line-added">1205 </span>
1206 //------------------------------split_if_with_blocks_post----------------------
1207 // Do the real work in a non-recursive function.  CFG hackery wants to be
1208 // in the post-order, so it can dirty the I-DOM info and not use the dirtied
1209 // info.
<span class="line-modified">1210 void PhaseIdealLoop::split_if_with_blocks_post(Node *n) {</span>
1211 
1212   // Cloning Cmp through Phi&#39;s involves the split-if transform.
1213   // FastLock is not used by an If
<span class="line-modified">1214   if (n-&gt;is_Cmp() &amp;&amp; !n-&gt;is_FastLock()) {</span>
1215     Node *n_ctrl = get_ctrl(n);
1216     // Determine if the Node has inputs from some local Phi.
1217     // Returns the block to clone thru.
1218     Node *n_blk = has_local_phi_input(n);
1219     if (n_blk != n_ctrl) {
1220       return;
1221     }
1222 
1223     if (!can_split_if(n_ctrl)) {
1224       return;
1225     }
1226 
1227     if (n-&gt;outcnt() != 1) {
1228       return; // Multiple bool&#39;s from 1 compare?
1229     }
1230     Node *bol = n-&gt;unique_out();
1231     assert(bol-&gt;is_Bool(), &quot;expect a bool here&quot;);
1232     if (bol-&gt;outcnt() != 1) {
1233       return;// Multiple branches from 1 compare?
1234     }
</pre>
<hr />
<pre>
1319 
1320   // Check for an IF ready to split; one that has its
1321   // condition codes input coming from a Phi at the block start.
1322   int n_op = n-&gt;Opcode();
1323 
1324   // Check for an IF being dominated by another IF same test
1325   if (n_op == Op_If ||
1326       n_op == Op_RangeCheck) {
1327     Node *bol = n-&gt;in(1);
1328     uint max = bol-&gt;outcnt();
1329     // Check for same test used more than once?
1330     if (max &gt; 1 &amp;&amp; bol-&gt;is_Bool()) {
1331       // Search up IDOMs to see if this IF is dominated.
1332       Node *cutoff = get_ctrl(bol);
1333 
1334       // Now search up IDOMs till cutoff, looking for a dominating test
1335       Node *prevdom = n;
1336       Node *dom = idom(prevdom);
1337       while (dom != cutoff) {
1338         if (dom-&gt;req() &gt; 1 &amp;&amp; dom-&gt;in(1) == bol &amp;&amp; prevdom-&gt;in(0) == dom) {
<span class="line-added">1339           // It&#39;s invalid to move control dependent data nodes in the inner</span>
<span class="line-added">1340           // strip-mined loop, because:</span>
<span class="line-added">1341           //  1) break validation of LoopNode::verify_strip_mined()</span>
<span class="line-added">1342           //  2) move code with side-effect in strip-mined loop</span>
<span class="line-added">1343           // Move to the exit of outer strip-mined loop in that case.</span>
<span class="line-added">1344           Node* out_le = is_inner_of_stripmined_loop(dom);</span>
<span class="line-added">1345           if (out_le != NULL) {</span>
<span class="line-added">1346             prevdom = out_le;</span>
<span class="line-added">1347           }</span>
1348           // Replace the dominated test with an obvious true or false.
1349           // Place it on the IGVN worklist for later cleanup.
1350           C-&gt;set_major_progress();
1351           dominated_by(prevdom, n, false, true);
1352 #ifndef PRODUCT
1353           if( VerifyLoopOptimizations ) verify();
1354 #endif
1355           return;
1356         }
1357         prevdom = dom;
1358         dom = idom(prevdom);
1359       }
1360     }
1361   }
1362 
1363   // See if a shared loop-varying computation has no loop-varying uses.
1364   // Happens if something is only used for JVM state in uncommon trap exits,
1365   // like various versions of induction variable+offset.  Clone the
1366   // computation per usage to allow it to sink out of the loop.
1367   if (has_ctrl(n) &amp;&amp; !n-&gt;in(0)) {// n not dead and has no control edge (can float about)
</pre>
<hr />
<pre>
1373         Node* u = n-&gt;fast_out(i);
1374         if( !has_ctrl(u) )     break; // Found control user
1375         IdealLoopTree *u_loop = get_loop(get_ctrl(u));
1376         if( u_loop == n_loop ) break; // Found loop-varying use
1377         if( n_loop-&gt;is_member( u_loop ) ) break; // Found use in inner loop
1378         if( u-&gt;Opcode() == Op_Opaque1 ) break; // Found loop limit, bugfix for 4677003
1379       }
1380       bool did_break = (i &lt; imax);  // Did we break out of the previous loop?
1381       if (!did_break &amp;&amp; n-&gt;outcnt() &gt; 1) { // All uses in outer loops!
1382         Node *late_load_ctrl = NULL;
1383         if (n-&gt;is_Load()) {
1384           // If n is a load, get and save the result from get_late_ctrl(),
1385           // to be later used in calculating the control for n&#39;s clones.
1386           clear_dom_lca_tags();
1387           late_load_ctrl = get_late_ctrl(n, n_ctrl);
1388         }
1389         // If n is a load, and the late control is the same as the current
1390         // control, then the cloning of n is a pointless exercise, because
1391         // GVN will ensure that we end up where we started.
1392         if (!n-&gt;is_Load() || late_load_ctrl != n_ctrl) {

1393           for (DUIterator_Last jmin, j = n-&gt;last_outs(jmin); j &gt;= jmin; ) {
1394             Node *u = n-&gt;last_out(j); // Clone private computation per use
1395             _igvn.rehash_node_delayed(u);
1396             Node *x = n-&gt;clone(); // Clone computation
1397             Node *x_ctrl = NULL;
1398             if( u-&gt;is_Phi() ) {
1399               // Replace all uses of normal nodes.  Replace Phi uses
1400               // individually, so the separate Nodes can sink down
1401               // different paths.
1402               uint k = 1;
1403               while( u-&gt;in(k) != n ) k++;
1404               u-&gt;set_req( k, x );
1405               // x goes next to Phi input path
1406               x_ctrl = u-&gt;in(0)-&gt;in(k);
1407               --j;
1408             } else {              // Normal use
1409               // Replace all uses
1410               for( uint k = 0; k &lt; u-&gt;req(); k++ ) {
1411                 if( u-&gt;in(k) == n ) {
1412                   u-&gt;set_req( k, x );
1413                   --j;
1414                 }
1415               }
1416               x_ctrl = get_ctrl(u);
1417             }
1418 
1419             // Find control for &#39;x&#39; next to use but not inside inner loops.
1420             // For inner loop uses get the preheader area.
1421             x_ctrl = place_near_use(x_ctrl);
1422 




1423             if (n-&gt;is_Load()) {
1424               // For loads, add a control edge to a CFG node outside of the loop
1425               // to force them to not combine and return back inside the loop
1426               // during GVN optimization (4641526).
1427               //
1428               // Because we are setting the actual control input, factor in
1429               // the result from get_late_ctrl() so we respect any
1430               // anti-dependences. (6233005).
1431               x_ctrl = dom_lca(late_load_ctrl, x_ctrl);
1432 
1433               // Don&#39;t allow the control input to be a CFG splitting node.
1434               // Such nodes should only have ProjNodes as outs, e.g. IfNode
1435               // should only have IfTrueNode and IfFalseNode (4985384).
1436               x_ctrl = find_non_split_ctrl(x_ctrl);
<span class="line-added">1437 </span>
<span class="line-added">1438               IdealLoopTree* x_loop = get_loop(x_ctrl);</span>
<span class="line-added">1439               Node* x_head = x_loop-&gt;_head;</span>
<span class="line-added">1440               if (x_head-&gt;is_Loop() &amp;&amp; (x_head-&gt;is_OuterStripMinedLoop() || x_head-&gt;as_Loop()-&gt;is_strip_mined()) &amp;&amp; is_dominator(n_ctrl, x_head)) {</span>
<span class="line-added">1441                 // Anti dependence analysis is sometimes too</span>
<span class="line-added">1442                 // conservative: a store in the outer strip mined loop</span>
<span class="line-added">1443                 // can prevent a load from floating out of the outer</span>
<span class="line-added">1444                 // strip mined loop but the load may not be referenced</span>
<span class="line-added">1445                 // from the safepoint: loop strip mining verification</span>
<span class="line-added">1446                 // code reports a problem in that case. Make sure the</span>
<span class="line-added">1447                 // load is not moved in the outer strip mined loop in</span>
<span class="line-added">1448                 // that case.</span>
<span class="line-added">1449                 x_ctrl = x_head-&gt;as_Loop()-&gt;skip_strip_mined()-&gt;in(LoopNode::EntryControl);</span>
<span class="line-added">1450               }</span>
1451               assert(dom_depth(n_ctrl) &lt;= dom_depth(x_ctrl), &quot;n is later than its clone&quot;);
1452 
1453               x-&gt;set_req(0, x_ctrl);
1454             }
1455             register_new_node(x, x_ctrl);
1456 
1457             // Some institutional knowledge is needed here: &#39;x&#39; is
1458             // yanked because if the optimizer runs GVN on it all the
1459             // cloned x&#39;s will common up and undo this optimization and
<span class="line-modified">1460             // be forced back in the loop.</span>
<span class="line-modified">1461             // I tried setting control edges on the x&#39;s to force them to</span>

1462             // not combine, but the matching gets worried when it tries
1463             // to fold a StoreP and an AddP together (as part of an
1464             // address expression) and the AddP and StoreP have
1465             // different controls.
1466             if (!x-&gt;is_Load() &amp;&amp; !x-&gt;is_DecodeNarrowPtr()) _igvn._worklist.yank(x);
1467           }
1468           _igvn.remove_dead_node(n);
1469         }
1470       }
1471     }
1472   }
1473 
1474   try_move_store_after_loop(n);
1475 
1476   // Check for Opaque2&#39;s who&#39;s loop has disappeared - who&#39;s input is in the
1477   // same loop nest as their output.  Remove &#39;em, they are no longer useful.
1478   if( n_op == Op_Opaque2 &amp;&amp;
1479       n-&gt;in(1) != NULL &amp;&amp;
1480       get_loop(get_ctrl(n)) == get_loop(get_ctrl(n-&gt;in(1))) ) {
1481     _igvn.replace_node( n, n-&gt;in(1) );
1482   }






1483 }
1484 
1485 //------------------------------split_if_with_blocks---------------------------
1486 // Check for aggressive application of &#39;split-if&#39; optimization,
1487 // using basic block level info.
<span class="line-modified">1488 void PhaseIdealLoop::split_if_with_blocks(VectorSet &amp;visited, Node_Stack &amp;nstack) {</span>
1489   Node* root = C-&gt;root();
1490   visited.set(root-&gt;_idx); // first, mark root as visited
1491   // Do pre-visit work for root
1492   Node* n   = split_if_with_blocks_pre(root);
1493   uint  cnt = n-&gt;outcnt();
1494   uint  i   = 0;
1495 
1496   while (true) {
1497     // Visit all children
1498     if (i &lt; cnt) {
1499       Node* use = n-&gt;raw_out(i);
1500       ++i;
1501       if (use-&gt;outcnt() != 0 &amp;&amp; !visited.test_set(use-&gt;_idx)) {
1502         // Now do pre-visit work for this use
1503         use = split_if_with_blocks_pre(use);
1504         nstack.push(n, i); // Save parent and next use&#39;s index.
1505         n   = use;         // Process all children of current use.
1506         cnt = use-&gt;outcnt();
1507         i   = 0;
1508       }
1509     }
1510     else {
1511       // All of n&#39;s children have been processed, complete post-processing.
1512       if (cnt != 0 &amp;&amp; !n-&gt;is_Con()) {
1513         assert(has_node(n), &quot;no dead nodes&quot;);
<span class="line-modified">1514         split_if_with_blocks_post(n);</span>
1515       }
1516       if (must_throttle_split_if()) {
1517         nstack.clear();
1518       }
1519       if (nstack.is_empty()) {
1520         // Finished all nodes on stack.
1521         break;
1522       }
1523       // Get saved parent node and next use&#39;s index. Visit the rest of uses.
1524       n   = nstack.node();
1525       cnt = n-&gt;outcnt();
1526       i   = nstack.index();
1527       nstack.pop();
1528     }
1529   }
1530 }
1531 
1532 
1533 //=============================================================================
1534 //
</pre>
<hr />
<pre>
1706                                                  IdealLoopTree* loop, IdealLoopTree* outer_loop,
1707                                                  Node_List*&amp; split_if_set, Node_List*&amp; split_bool_set,
1708                                                  Node_List*&amp; split_cex_set, Node_List&amp; worklist,
1709                                                  uint new_counter, CloneLoopMode mode) {
1710   Node* nnn = old_new[old-&gt;_idx];
1711   // Copy uses to a worklist, so I can munge the def-use info
1712   // with impunity.
1713   for (DUIterator_Fast jmax, j = old-&gt;fast_outs(jmax); j &lt; jmax; j++)
1714     worklist.push(old-&gt;fast_out(j));
1715 
1716   while( worklist.size() ) {
1717     Node *use = worklist.pop();
1718     if (!has_node(use))  continue; // Ignore dead nodes
1719     if (use-&gt;in(0) == C-&gt;top())  continue;
1720     IdealLoopTree *use_loop = get_loop( has_ctrl(use) ? get_ctrl(use) : use );
1721     // Check for data-use outside of loop - at least one of OLD or USE
1722     // must not be a CFG node.
1723 #ifdef ASSERT
1724     if (loop-&gt;_head-&gt;as_Loop()-&gt;is_strip_mined() &amp;&amp; outer_loop-&gt;is_member(use_loop) &amp;&amp; !loop-&gt;is_member(use_loop) &amp;&amp; old_new[use-&gt;_idx] == NULL) {
1725       Node* sfpt = loop-&gt;_head-&gt;as_CountedLoop()-&gt;outer_safepoint();
<span class="line-modified">1726       assert(mode != IgnoreStripMined, &quot;incorrect cloning mode&quot;);</span>
<span class="line-added">1727       assert((mode == ControlAroundStripMined &amp;&amp; use == sfpt) || !use-&gt;is_reachable_from_root(), &quot;missed a node&quot;);</span>
1728     }
1729 #endif
1730     if (!loop-&gt;is_member(use_loop) &amp;&amp; !outer_loop-&gt;is_member(use_loop) &amp;&amp; (!old-&gt;is_CFG() || !use-&gt;is_CFG())) {
1731 
1732       // If the Data use is an IF, that means we have an IF outside of the
1733       // loop that is switching on a condition that is set inside of the
1734       // loop.  Happens if people set a loop-exit flag; then test the flag
1735       // in the loop to break the loop, then test is again outside of the
1736       // loop to determine which way the loop exited.
1737       // Loop predicate If node connects to Bool node through Opaque1 node.
1738       if (use-&gt;is_If() || use-&gt;is_CMove() || C-&gt;is_predicate_opaq(use) || use-&gt;Opcode() == Op_Opaque4) {
1739         // Since this code is highly unlikely, we lazily build the worklist
1740         // of such Nodes to go split.
1741         if (!split_if_set) {
1742           ResourceArea *area = Thread::current()-&gt;resource_area();
1743           split_if_set = new Node_List(area);
1744         }
1745         split_if_set-&gt;push(use);
1746       }
1747       if (use-&gt;is_Bool()) {
</pre>
<hr />
<pre>
2669     for (j = 0; j &lt; use-&gt;req(); j++) {
2670       if (use-&gt;in(j) == n) break;
2671     }
2672     assert(j &lt; use-&gt;req(), &quot;must be there&quot;);
2673 
2674     // clone &quot;n&quot; and insert it between the inputs of &quot;n&quot; and the use outside the loop
2675     Node* n_clone = n-&gt;clone();
2676     _igvn.replace_input_of(use, j, n_clone);
2677     cloned++;
2678     Node* use_c;
2679     if (!use-&gt;is_Phi()) {
2680       use_c = has_ctrl(use) ? get_ctrl(use) : use-&gt;in(0);
2681     } else {
2682       // Use in a phi is considered a use in the associated predecessor block
2683       use_c = use-&gt;in(0)-&gt;in(j);
2684     }
2685     set_ctrl(n_clone, use_c);
2686     assert(!loop-&gt;is_member(get_loop(use_c)), &quot;should be outside loop&quot;);
2687     get_loop(use_c)-&gt;_body.push(n_clone);
2688     _igvn.register_new_node_with_optimizer(n_clone);
<span class="line-modified">2689 #ifndef PRODUCT</span>
2690     if (TracePartialPeeling) {
2691       tty-&gt;print_cr(&quot;loop exit cloning old: %d new: %d newbb: %d&quot;, n-&gt;_idx, n_clone-&gt;_idx, get_ctrl(n_clone)-&gt;_idx);
2692     }
2693 #endif
2694   }
2695   return cloned;
2696 }
2697 
2698 
2699 //------------------------------ clone_for_special_use_inside_loop -------------------------------------
2700 // clone &quot;n&quot; for special uses that are in the not_peeled region.
2701 // If these def-uses occur in separate blocks, the code generator
2702 // marks the method as not compilable.  For example, if a &quot;BoolNode&quot;
2703 // is in a different basic block than the &quot;IfNode&quot; that uses it, then
2704 // the compilation is aborted in the code generator.
2705 void PhaseIdealLoop::clone_for_special_use_inside_loop( IdealLoopTree *loop, Node* n,
2706                                                         VectorSet&amp; not_peel, Node_List&amp; sink_list, Node_List&amp; worklist ) {
2707   if (n-&gt;is_Phi() || n-&gt;is_Load()) {
2708     return;
2709   }
2710   assert(worklist.size() == 0, &quot;should be empty&quot;);
2711   for (DUIterator_Fast jmax, j = n-&gt;fast_outs(jmax); j &lt; jmax; j++) {
2712     Node* use = n-&gt;fast_out(j);
2713     if ( not_peel.test(use-&gt;_idx) &amp;&amp;
2714          (use-&gt;is_If() || use-&gt;is_CMove() || use-&gt;is_Bool()) &amp;&amp;
2715          use-&gt;in(1) == n)  {
2716       worklist.push(use);
2717     }
2718   }
2719   if (worklist.size() &gt; 0) {
2720     // clone &quot;n&quot; and insert it between inputs of &quot;n&quot; and the use
2721     Node* n_clone = n-&gt;clone();
2722     loop-&gt;_body.push(n_clone);
2723     _igvn.register_new_node_with_optimizer(n_clone);
2724     set_ctrl(n_clone, get_ctrl(n));
2725     sink_list.push(n_clone);
<span class="line-modified">2726     not_peel.set(n_clone-&gt;_idx);</span>
<span class="line-modified">2727 #ifndef PRODUCT</span>
2728     if (TracePartialPeeling) {
2729       tty-&gt;print_cr(&quot;special not_peeled cloning old: %d new: %d&quot;, n-&gt;_idx, n_clone-&gt;_idx);
2730     }
2731 #endif
2732     while( worklist.size() ) {
2733       Node *use = worklist.pop();
2734       _igvn.rehash_node_delayed(use);
2735       for (uint j = 1; j &lt; use-&gt;req(); j++) {
2736         if (use-&gt;in(j) == n) {
2737           use-&gt;set_req(j, n_clone);
2738         }
2739       }
2740     }
2741   }
2742 }
2743 
2744 
2745 //------------------------------ insert_phi_for_loop -------------------------------------
2746 // Insert phi(lp_entry_val, back_edge_val) at use-&gt;in(idx) for loop lp if phi does not already exist
2747 void PhaseIdealLoop::insert_phi_for_loop( Node* use, uint idx, Node* lp_entry_val, Node* back_edge_val, LoopNode* lp ) {
</pre>
<hr />
<pre>
3036 //         :  |   |     |       |
3037 //         :  |   v    stmt2    |
3038 //         :  | exitB:  |       |
3039 //         :  | stmt4   v       |
3040 //         :  |       ifA orig  |
3041 //         :  |      /  \       |
3042 //         :  |     /    \      |
3043 //         :  |    v     v      |
3044 //         :  |  false  true    |
3045 //         :  |  /        \     |
3046 //         :  v  v         -----+
3047 //          RegionA
3048 //             |
3049 //             v
3050 //           exitA
3051 //
3052 bool PhaseIdealLoop::partial_peel( IdealLoopTree *loop, Node_List &amp;old_new ) {
3053 
3054   assert(!loop-&gt;_head-&gt;is_CountedLoop(), &quot;Non-counted loop only&quot;);
3055   if (!loop-&gt;_head-&gt;is_Loop()) {
<span class="line-modified">3056     return false;</span>
<span class="line-modified">3057   }</span>
<span class="line-modified">3058   LoopNode *head = loop-&gt;_head-&gt;as_Loop();</span>
3059 
3060   if (head-&gt;is_partial_peel_loop() || head-&gt;partial_peel_has_failed()) {
3061     return false;
3062   }
3063 
3064   // Check for complex exit control
<span class="line-modified">3065   for (uint ii = 0; ii &lt; loop-&gt;_body.size(); ii++) {</span>
3066     Node *n = loop-&gt;_body.at(ii);
3067     int opc = n-&gt;Opcode();
3068     if (n-&gt;is_Call()        ||
3069         opc == Op_Catch     ||
3070         opc == Op_CatchProj ||
3071         opc == Op_Jump      ||
3072         opc == Op_JumpProj) {
<span class="line-modified">3073 #ifndef PRODUCT</span>
3074       if (TracePartialPeeling) {
3075         tty-&gt;print_cr(&quot;\nExit control too complex: lp: %d&quot;, head-&gt;_idx);
3076       }
3077 #endif
3078       return false;
3079     }
3080   }
3081 
3082   int dd = dom_depth(head);
3083 
3084   // Step 1: find cut point
3085 
3086   // Walk up dominators to loop head looking for first loop exit
3087   // which is executed on every path thru loop.
3088   IfNode *peel_if = NULL;
3089   IfNode *peel_if_cmpu = NULL;
3090 
3091   Node *iff = loop-&gt;tail();
<span class="line-modified">3092   while (iff != head) {</span>
<span class="line-modified">3093     if (iff-&gt;is_If()) {</span>
3094       Node *ctrl = get_ctrl(iff-&gt;in(1));
3095       if (ctrl-&gt;is_top()) return false; // Dead test on live IF.
3096       // If loop-varying exit-test, check for induction variable
<span class="line-modified">3097       if (loop-&gt;is_member(get_loop(ctrl)) &amp;&amp;</span>
3098           loop-&gt;is_loop_exit(iff) &amp;&amp;
3099           is_possible_iv_test(iff)) {
3100         Node* cmp = iff-&gt;in(1)-&gt;in(1);
3101         if (cmp-&gt;Opcode() == Op_CmpI) {
3102           peel_if = iff-&gt;as_If();
3103         } else {
3104           assert(cmp-&gt;Opcode() == Op_CmpU, &quot;must be CmpI or CmpU&quot;);
3105           peel_if_cmpu = iff-&gt;as_If();
3106         }
3107       }
3108     }
3109     iff = idom(iff);
3110   }
<span class="line-added">3111 </span>
3112   // Prefer signed compare over unsigned compare.
3113   IfNode* new_peel_if = NULL;
3114   if (peel_if == NULL) {
3115     if (!PartialPeelAtUnsignedTests || peel_if_cmpu == NULL) {
3116       return false;   // No peel point found
3117     }
3118     new_peel_if = insert_cmpi_loop_exit(peel_if_cmpu, loop);
3119     if (new_peel_if == NULL) {
3120       return false;   // No peel point found
3121     }
3122     peel_if = new_peel_if;
3123   }
3124   Node* last_peel        = stay_in_loop(peel_if, loop);
3125   Node* first_not_peeled = stay_in_loop(last_peel, loop);
3126   if (first_not_peeled == NULL || first_not_peeled == head) {
3127     return false;
3128   }
3129 
<span class="line-modified">3130 #ifndef PRODUCT</span>
3131   if (TraceLoopOpts) {
3132     tty-&gt;print(&quot;PartialPeel  &quot;);
3133     loop-&gt;dump_head();
3134   }
3135 
3136   if (TracePartialPeeling) {
3137     tty-&gt;print_cr(&quot;before partial peel one iteration&quot;);
3138     Node_List wl;
3139     Node* t = head-&gt;in(2);
3140     while (true) {
3141       wl.push(t);
3142       if (t == head) break;
3143       t = idom(t);
3144     }
3145     while (wl.size() &gt; 0) {
3146       Node* tt = wl.pop();
3147       tt-&gt;dump();
3148       if (tt == last_peel) tty-&gt;print_cr(&quot;-- cut --&quot;);
3149     }
3150   }
3151 #endif
3152   ResourceArea *area = Thread::current()-&gt;resource_area();
3153   VectorSet peel(area);
3154   VectorSet not_peel(area);
3155   Node_List peel_list(area);
3156   Node_List worklist(area);
3157   Node_List sink_list(area);
3158 
<span class="line-added">3159   uint estimate = loop-&gt;est_loop_clone_sz(1);</span>
<span class="line-added">3160   if (exceeding_node_budget(estimate)) {</span>
<span class="line-added">3161     return false;</span>
<span class="line-added">3162   }</span>
<span class="line-added">3163 </span>
3164   // Set of cfg nodes to peel are those that are executable from
3165   // the head through last_peel.
3166   assert(worklist.size() == 0, &quot;should be empty&quot;);
3167   worklist.push(head);
3168   peel.set(head-&gt;_idx);
3169   while (worklist.size() &gt; 0) {
3170     Node *n = worklist.pop();
3171     if (n != last_peel) {
3172       for (DUIterator_Fast jmax, j = n-&gt;fast_outs(jmax); j &lt; jmax; j++) {
3173         Node* use = n-&gt;fast_out(j);
3174         if (use-&gt;is_CFG() &amp;&amp;
3175             loop-&gt;is_member(get_loop(use)) &amp;&amp;
3176             !peel.test_set(use-&gt;_idx)) {
3177           worklist.push(use);
3178         }
3179       }
3180     }
3181   }
3182 
3183   // Set of non-cfg nodes to peel are those that are control
3184   // dependent on the cfg nodes.
<span class="line-modified">3185   for (uint i = 0; i &lt; loop-&gt;_body.size(); i++) {</span>

3186     Node *n = loop-&gt;_body.at(i);
3187     Node *n_c = has_ctrl(n) ? get_ctrl(n) : n;
3188     if (peel.test(n_c-&gt;_idx)) {
3189       peel.set(n-&gt;_idx);
3190     } else {
3191       not_peel.set(n-&gt;_idx);
3192     }
3193   }
3194 
3195   // Step 2: move operations from the peeled section down into the
3196   //         not-peeled section
3197 
3198   // Get a post order schedule of nodes in the peel region
3199   // Result in right-most operand.
<span class="line-modified">3200   scheduled_nodelist(loop, peel, peel_list);</span>
3201 
3202   assert(is_valid_loop_partition(loop, peel, peel_list, not_peel), &quot;bad partition&quot;);
3203 
3204   // For future check for too many new phis
3205   uint old_phi_cnt = 0;
3206   for (DUIterator_Fast jmax, j = head-&gt;fast_outs(jmax); j &lt; jmax; j++) {
3207     Node* use = head-&gt;fast_out(j);
3208     if (use-&gt;is_Phi()) old_phi_cnt++;
3209   }
3210 
<span class="line-modified">3211 #ifndef PRODUCT</span>
3212   if (TracePartialPeeling) {
3213     tty-&gt;print_cr(&quot;\npeeled list&quot;);
3214   }
3215 #endif
3216 
3217   // Evacuate nodes in peel region into the not_peeled region if possible
3218   uint new_phi_cnt = 0;
3219   uint cloned_for_outside_use = 0;
<span class="line-modified">3220   for (uint i = 0; i &lt; peel_list.size();) {</span>
3221     Node* n = peel_list.at(i);
<span class="line-modified">3222 #ifndef PRODUCT</span>
3223   if (TracePartialPeeling) n-&gt;dump();
3224 #endif
3225     bool incr = true;
<span class="line-modified">3226     if (!n-&gt;is_CFG()) {</span>
<span class="line-modified">3227       if (has_use_in_set(n, not_peel)) {</span>


3228         // If not used internal to the peeled region,
3229         // move &quot;n&quot; from peeled to not_peeled region.
<span class="line-modified">3230         if (!has_use_internal_to_set(n, peel, loop)) {</span>


3231           // if not pinned and not a load (which maybe anti-dependent on a store)
3232           // and not a CMove (Matcher expects only bool-&gt;cmove).
<span class="line-modified">3233           if (n-&gt;in(0) == NULL &amp;&amp; !n-&gt;is_Load() &amp;&amp; !n-&gt;is_CMove()) {</span>
<span class="line-modified">3234             cloned_for_outside_use += clone_for_use_outside_loop(loop, n, worklist);</span>
3235             sink_list.push(n);
<span class="line-modified">3236             peel.remove(n-&gt;_idx);</span>
<span class="line-modified">3237             not_peel.set(n-&gt;_idx);</span>
3238             peel_list.remove(i);
3239             incr = false;
<span class="line-modified">3240 #ifndef PRODUCT</span>
3241             if (TracePartialPeeling) {
3242               tty-&gt;print_cr(&quot;sink to not_peeled region: %d newbb: %d&quot;,
3243                             n-&gt;_idx, get_ctrl(n)-&gt;_idx);
3244             }
3245 #endif
3246           }
3247         } else {
3248           // Otherwise check for special def-use cases that span
3249           // the peel/not_peel boundary such as bool-&gt;if
<span class="line-modified">3250           clone_for_special_use_inside_loop(loop, n, not_peel, sink_list, worklist);</span>
3251           new_phi_cnt++;
3252         }
3253       }
3254     }
3255     if (incr) i++;
3256   }
3257 
<span class="line-modified">3258   estimate += cloned_for_outside_use + new_phi_cnt;</span>
<span class="line-modified">3259   bool exceed_node_budget = !may_require_nodes(estimate);</span>
<span class="line-added">3260   bool exceed_phi_limit = new_phi_cnt &gt; old_phi_cnt + PartialPeelNewPhiDelta;</span>
<span class="line-added">3261 </span>
<span class="line-added">3262   if (exceed_node_budget || exceed_phi_limit) {</span>
<span class="line-added">3263 #ifndef PRODUCT</span>
3264     if (TracePartialPeeling) {
3265       tty-&gt;print_cr(&quot;\nToo many new phis: %d  old %d new cmpi: %c&quot;,
3266                     new_phi_cnt, old_phi_cnt, new_peel_if != NULL?&#39;T&#39;:&#39;F&#39;);
3267     }
3268 #endif
3269     if (new_peel_if != NULL) {
3270       remove_cmpi_loop_exit(new_peel_if, loop);
3271     }
3272     // Inhibit more partial peeling on this loop
3273     assert(!head-&gt;is_partial_peel_loop(), &quot;not partial peeled&quot;);
3274     head-&gt;mark_partial_peel_failed();
3275     if (cloned_for_outside_use &gt; 0) {
3276       // Terminate this round of loop opts because
3277       // the graph outside this loop was changed.
3278       C-&gt;set_major_progress();
3279       return true;
3280     }
3281     return false;
3282   }
3283 
</pre>
<hr />
<pre>
3290   _igvn.register_new_node_with_optimizer(new_head);
3291   assert(first_not_peeled-&gt;in(0) == last_peel, &quot;last_peel &lt;- first_not_peeled&quot;);
3292   _igvn.replace_input_of(first_not_peeled, 0, new_head);
3293   set_loop(new_head, loop);
3294   loop-&gt;_body.push(new_head);
3295   not_peel.set(new_head-&gt;_idx);
3296   set_idom(new_head, last_peel, dom_depth(first_not_peeled));
3297   set_idom(first_not_peeled, new_head, dom_depth(first_not_peeled));
3298 
3299   while (sink_list.size() &gt; 0) {
3300     Node* n = sink_list.pop();
3301     set_ctrl(n, new_head);
3302   }
3303 
3304   assert(is_valid_loop_partition(loop, peel, peel_list, not_peel), &quot;bad partition&quot;);
3305 
3306   clone_loop(loop, old_new, dd, IgnoreStripMined);
3307 
3308   const uint clone_exit_idx = 1;
3309   const uint orig_exit_idx  = 2;
<span class="line-modified">3310   assert(is_valid_clone_loop_form(loop, peel_list, orig_exit_idx, clone_exit_idx), &quot;bad clone loop&quot;);</span>
3311 
3312   Node* head_clone             = old_new[head-&gt;_idx];
3313   LoopNode* new_head_clone     = old_new[new_head-&gt;_idx]-&gt;as_Loop();
3314   Node* orig_tail_clone        = head_clone-&gt;in(2);
3315 
3316   // Add phi if &quot;def&quot; node is in peel set and &quot;use&quot; is not
3317 
<span class="line-modified">3318   for (uint i = 0; i &lt; peel_list.size(); i++) {</span>
3319     Node *def  = peel_list.at(i);
3320     if (!def-&gt;is_CFG()) {
3321       for (DUIterator_Fast jmax, j = def-&gt;fast_outs(jmax); j &lt; jmax; j++) {
3322         Node *use = def-&gt;fast_out(j);
3323         if (has_node(use) &amp;&amp; use-&gt;in(0) != C-&gt;top() &amp;&amp;
3324             (!peel.test(use-&gt;_idx) ||
3325              (use-&gt;is_Phi() &amp;&amp; use-&gt;in(0) == head)) ) {
3326           worklist.push(use);
3327         }
3328       }
3329       while( worklist.size() ) {
3330         Node *use = worklist.pop();
3331         for (uint j = 1; j &lt; use-&gt;req(); j++) {
3332           Node* n = use-&gt;in(j);
3333           if (n == def) {
3334 
3335             // &quot;def&quot; is in peel set, &quot;use&quot; is not in peel set
3336             // or &quot;use&quot; is in the entry boundary (a phi) of the peel set
3337 
3338             Node* use_c = has_ctrl(use) ? get_ctrl(use) : use;
</pre>
<hr />
<pre>
3354               }
3355             }
3356           }
3357         }
3358       }
3359     }
3360   }
3361 
3362   // Step 3b: retarget control
3363 
3364   // Redirect control to the new loop head if a cloned node in
3365   // the not_peeled region has control that points into the peeled region.
3366   // This necessary because the cloned peeled region will be outside
3367   // the loop.
3368   //                            from    to
3369   //          cloned-peeled    &lt;---+
3370   //    new_head_clone:            |    &lt;--+
3371   //          cloned-not_peeled  in(0)    in(0)
3372   //          orig-peeled
3373 
<span class="line-modified">3374   for (uint i = 0; i &lt; loop-&gt;_body.size(); i++) {</span>
3375     Node *n = loop-&gt;_body.at(i);
3376     if (!n-&gt;is_CFG()           &amp;&amp; n-&gt;in(0) != NULL        &amp;&amp;
3377         not_peel.test(n-&gt;_idx) &amp;&amp; peel.test(n-&gt;in(0)-&gt;_idx)) {
3378       Node* n_clone = old_new[n-&gt;_idx];
3379       _igvn.replace_input_of(n_clone, 0, new_head_clone);
3380     }
3381   }
3382 
3383   // Backedge of the surviving new_head (the clone) is original last_peel
3384   _igvn.replace_input_of(new_head_clone, LoopNode::LoopBackControl, last_peel);
3385 
3386   // Cut first node in original not_peel set
3387   _igvn.rehash_node_delayed(new_head);                     // Multiple edge updates:
3388   new_head-&gt;set_req(LoopNode::EntryControl,    C-&gt;top());  //   use rehash_node_delayed / set_req instead of
3389   new_head-&gt;set_req(LoopNode::LoopBackControl, C-&gt;top());  //   multiple replace_input_of calls
3390 
3391   // Copy head_clone back-branch info to original head
3392   // and remove original head&#39;s loop entry and
3393   // clone head&#39;s back-branch
3394   _igvn.rehash_node_delayed(head); // Multiple edge updates
</pre>
<hr />
<pre>
3401     Node* use = head-&gt;fast_out(k);
3402     if (use-&gt;is_Phi() &amp;&amp; use-&gt;outcnt() &gt; 0) {
3403       Node* use_clone = old_new[use-&gt;_idx];
3404       _igvn.rehash_node_delayed(use); // Multiple edge updates
3405       use-&gt;set_req(LoopNode::EntryControl,    use_clone-&gt;in(LoopNode::LoopBackControl));
3406       use-&gt;set_req(LoopNode::LoopBackControl, C-&gt;top());
3407       _igvn.replace_input_of(use_clone, LoopNode::LoopBackControl, C-&gt;top());
3408     }
3409   }
3410 
3411   // Step 4: update dominator tree and dominator depth
3412 
3413   set_idom(head, orig_tail_clone, dd);
3414   recompute_dom_depth();
3415 
3416   // Inhibit more partial peeling on this loop
3417   new_head_clone-&gt;set_partial_peel_loop();
3418   C-&gt;set_major_progress();
3419   loop-&gt;record_for_igvn();
3420 
<span class="line-modified">3421 #ifndef PRODUCT</span>
3422   if (TracePartialPeeling) {
3423     tty-&gt;print_cr(&quot;\nafter partial peel one iteration&quot;);
3424     Node_List wl(area);
3425     Node* t = last_peel;
3426     while (true) {
3427       wl.push(t);
3428       if (t == head_clone) break;
3429       t = idom(t);
3430     }
3431     while (wl.size() &gt; 0) {
3432       Node* tt = wl.pop();
3433       if (tt == head) tty-&gt;print_cr(&quot;orig head&quot;);
3434       else if (tt == new_head_clone) tty-&gt;print_cr(&quot;new head&quot;);
3435       else if (tt == head_clone) tty-&gt;print_cr(&quot;clone head&quot;);
3436       tt-&gt;dump();
3437     }
3438   }
3439 #endif
3440   return true;
3441 }
3442 
3443 //------------------------------reorg_offsets----------------------------------
3444 // Reorganize offset computations to lower register pressure.  Mostly
3445 // prevent loop-fallout uses of the pre-incremented trip counter (which are
3446 // then alive with the post-incremented trip counter forcing an extra
3447 // register move)
3448 void PhaseIdealLoop::reorg_offsets(IdealLoopTree *loop) {
3449   // Perform it only for canonical counted loops.
3450   // Loop&#39;s shape could be messed up by iteration_split_impl.
3451   if (!loop-&gt;_head-&gt;is_CountedLoop())
3452     return;
3453   if (!loop-&gt;_head-&gt;as_Loop()-&gt;is_valid_counted_loop())
3454     return;
3455 
3456   CountedLoopNode *cl = loop-&gt;_head-&gt;as_CountedLoop();
3457   CountedLoopEndNode *cle = cl-&gt;loopexit();
3458   Node *exit = cle-&gt;proj_out(false);
3459   Node *phi = cl-&gt;phi();
3460 
<span class="line-modified">3461   // Check for the special case when using the pre-incremented trip-counter on</span>
<span class="line-modified">3462   // the fall-out  path (forces the pre-incremented  and post-incremented trip</span>
<span class="line-modified">3463   // counter to be live  at the same time).  Fix this by  adjusting to use the</span>
<span class="line-modified">3464   // post-increment trip counter.</span>
3465 
3466   bool progress = true;
3467   while (progress) {
3468     progress = false;
3469     for (DUIterator_Fast imax, i = phi-&gt;fast_outs(imax); i &lt; imax; i++) {
3470       Node* use = phi-&gt;fast_out(i);   // User of trip-counter
3471       if (!has_ctrl(use))  continue;
3472       Node *u_ctrl = get_ctrl(use);
3473       if (use-&gt;is_Phi()) {
3474         u_ctrl = NULL;
3475         for (uint j = 1; j &lt; use-&gt;req(); j++)
3476           if (use-&gt;in(j) == phi)
3477             u_ctrl = dom_lca(u_ctrl, use-&gt;in(0)-&gt;in(j));
3478       }
3479       IdealLoopTree *u_loop = get_loop(u_ctrl);
3480       // Look for loop-invariant use
3481       if (u_loop == loop) continue;
3482       if (loop-&gt;is_member(u_loop)) continue;
3483       // Check that use is live out the bottom.  Assuming the trip-counter
3484       // update is right at the bottom, uses of of the loop middle are ok.
</pre>
</td>
</tr>
</table>
<center><a href="loopnode.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="machnode.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>