<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Udiff src/hotspot/share/opto/memnode.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="memnode.cpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="mulnode.cpp.udiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/memnode.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-new-header">@@ -41,10 +41,12 @@</span>
  class MemNode : public Node {
  private:
    bool _unaligned_access; // Unaligned access from unsafe
    bool _mismatched_access; // Mismatched access from unsafe: byte read in integer array for instance
    bool _unsafe_access;     // Access of unsafe origin.
<span class="udiff-line-added">+   uint8_t _barrier; // Bit field with barrier information</span>
<span class="udiff-line-added">+ </span>
  protected:
  #ifdef ASSERT
    const TypePtr* _adr_type;     // What kind of memory is being addressed?
  #endif
    virtual uint size_of() const;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -60,22 +62,34 @@</span>
                   release,       // Store has to release or be preceded by MemBarRelease.
                   seqcst,        // LoadStore has to have both acquire and release semantics.
                   unset          // The memory ordering is not set (used for testing)
    } MemOrd;
  protected:
<span class="udiff-line-modified-removed">-   MemNode( Node *c0, Node *c1, Node *c2, const TypePtr* at )</span>
<span class="udiff-line-modified-removed">-     : Node(c0,c1,c2   ), _unaligned_access(false), _mismatched_access(false), _unsafe_access(false) {</span>
<span class="udiff-line-modified-added">+   MemNode( Node *c0, Node *c1, Node *c2, const TypePtr* at ) :</span>
<span class="udiff-line-modified-added">+       Node(c0,c1,c2),</span>
<span class="udiff-line-added">+       _unaligned_access(false),</span>
<span class="udiff-line-added">+       _mismatched_access(false),</span>
<span class="udiff-line-added">+       _unsafe_access(false),</span>
<span class="udiff-line-added">+       _barrier(0) {</span>
      init_class_id(Class_Mem);
      debug_only(_adr_type=at; adr_type();)
    }
<span class="udiff-line-modified-removed">-   MemNode( Node *c0, Node *c1, Node *c2, const TypePtr* at, Node *c3 )</span>
<span class="udiff-line-modified-removed">-     : Node(c0,c1,c2,c3), _unaligned_access(false), _mismatched_access(false), _unsafe_access(false) {</span>
<span class="udiff-line-modified-added">+   MemNode( Node *c0, Node *c1, Node *c2, const TypePtr* at, Node *c3 ) :</span>
<span class="udiff-line-modified-added">+       Node(c0,c1,c2,c3),</span>
<span class="udiff-line-added">+       _unaligned_access(false),</span>
<span class="udiff-line-added">+       _mismatched_access(false),</span>
<span class="udiff-line-added">+       _unsafe_access(false),</span>
<span class="udiff-line-added">+       _barrier(0) {</span>
      init_class_id(Class_Mem);
      debug_only(_adr_type=at; adr_type();)
    }
<span class="udiff-line-modified-removed">-   MemNode( Node *c0, Node *c1, Node *c2, const TypePtr* at, Node *c3, Node *c4)</span>
<span class="udiff-line-modified-removed">-     : Node(c0,c1,c2,c3,c4), _unaligned_access(false), _mismatched_access(false), _unsafe_access(false) {</span>
<span class="udiff-line-modified-added">+   MemNode( Node *c0, Node *c1, Node *c2, const TypePtr* at, Node *c3, Node *c4) :</span>
<span class="udiff-line-modified-added">+       Node(c0,c1,c2,c3,c4),</span>
<span class="udiff-line-added">+       _unaligned_access(false),</span>
<span class="udiff-line-added">+       _mismatched_access(false),</span>
<span class="udiff-line-added">+       _unsafe_access(false),</span>
<span class="udiff-line-added">+       _barrier(0) {</span>
      init_class_id(Class_Mem);
      debug_only(_adr_type=at; adr_type();)
    }
  
    virtual Node* find_previous_arraycopy(PhaseTransform* phase, Node* ld_alloc, Node*&amp; mem, bool can_see_stored_value) const { return NULL; }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -123,10 +137,13 @@</span>
  #else
      return type2aelembytes(memory_type());
  #endif
    }
  
<span class="udiff-line-added">+   uint8_t barrier_data() { return _barrier; }</span>
<span class="udiff-line-added">+   void set_barrier_data(uint8_t barrier_data) { _barrier = barrier_data; }</span>
<span class="udiff-line-added">+ </span>
    // Search through memory states which precede this node (load or store).
    // Look for an exact match for the address, with no intervening
    // aliased stores.
    Node* find_previous_store(PhaseTransform* phase);
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -152,20 +169,19 @@</span>
  class LoadNode : public MemNode {
  public:
    // Some loads (from unsafe) should be pinned: they don&#39;t depend only
    // on the dominating test.  The field _control_dependency below records
    // whether that node depends only on the dominating test.
<span class="udiff-line-modified-removed">-   // Methods used to build LoadNodes pass an argument of type enum</span>
<span class="udiff-line-modified-removed">-   // ControlDependency instead of a boolean because those methods</span>
<span class="udiff-line-modified-removed">-   // typically have multiple boolean parameters with default values:</span>
<span class="udiff-line-removed">-   // passing the wrong boolean to one of these parameters by mistake</span>
<span class="udiff-line-removed">-   // goes easily unnoticed. Using an enum, the compiler can check that</span>
<span class="udiff-line-removed">-   // the type of a value and the type of the parameter match.</span>
<span class="udiff-line-modified-added">+   // Pinned and UnknownControl are similar, but differ in that Pinned</span>
<span class="udiff-line-modified-added">+   // loads are not allowed to float across safepoints, whereas UnknownControl</span>
<span class="udiff-line-modified-added">+   // loads are allowed to do that. Therefore, Pinned is stricter.</span>
    enum ControlDependency {
      Pinned,
<span class="udiff-line-added">+     UnknownControl,</span>
      DependsOnlyOnTest
    };
<span class="udiff-line-added">+ </span>
  private:
    // LoadNode::hash() doesn&#39;t take the _control_dependency field
    // into account: If the graph already has a non-pinned LoadNode and
    // we add a pinned LoadNode with the same inputs, it&#39;s safe for GVN
    // to replace the pinned LoadNode with the non-pinned LoadNode,
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -180,12 +196,14 @@</span>
    // loads that can be reordered, and such requiring acquire semantics to
    // adhere to the Java specification.  The required behaviour is stored in
    // this field.
    const MemOrd _mo;
  
<span class="udiff-line-added">+   AllocateNode* is_new_object_mark_load(PhaseGVN *phase) const;</span>
<span class="udiff-line-added">+ </span>
  protected:
<span class="udiff-line-modified-removed">-   virtual uint cmp(const Node &amp;n) const;</span>
<span class="udiff-line-modified-added">+   virtual bool cmp(const Node &amp;n) const;</span>
    virtual uint size_of() const; // Size is bigger
    // Should LoadNode::Ideal() attempt to remove control edges?
    virtual bool can_remove_control() const;
    const Type* const _type;      // What kind of value is loaded?
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -208,11 +226,12 @@</span>
  
    // Polymorphic factory method:
    static Node* make(PhaseGVN&amp; gvn, Node *c, Node *mem, Node *adr,
                      const TypePtr* at, const Type *rt, BasicType bt,
                      MemOrd mo, ControlDependency control_dependency = DependsOnlyOnTest,
<span class="udiff-line-modified-removed">-                     bool unaligned = false, bool mismatched = false, bool unsafe = false);</span>
<span class="udiff-line-modified-added">+                     bool unaligned = false, bool mismatched = false, bool unsafe = false,</span>
<span class="udiff-line-added">+                     uint8_t barrier_data = 0);</span>
  
    virtual uint hash()   const;  // Check the type
  
    // Handle algebraic identities here.  If we have an identity, return the Node
    // we are equivalent to.  We look for Load of a Store.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -260,10 +279,13 @@</span>
    bool is_instance_field_load_with_local_phi(Node* ctrl);
  
    Node* convert_to_unsigned_load(PhaseGVN&amp; gvn);
    Node* convert_to_signed_load(PhaseGVN&amp; gvn);
  
<span class="udiff-line-added">+   void pin() { _control_dependency = Pinned; }</span>
<span class="udiff-line-added">+   bool has_unknown_control_dependency() const { return _control_dependency == UnknownControl; }</span>
<span class="udiff-line-added">+ </span>
  #ifndef PRODUCT
    virtual void dump_spec(outputStream *st) const;
  #endif
  #ifdef ASSERT
    // Helper function to allow a raw load without control edge for some cases
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -371,11 +393,11 @@</span>
  
  //------------------------------LoadLNode--------------------------------------
  // Load a long from memory
  class LoadLNode : public LoadNode {
    virtual uint hash() const { return LoadNode::hash() + _require_atomic_access; }
<span class="udiff-line-modified-removed">-   virtual uint cmp( const Node &amp;n ) const {</span>
<span class="udiff-line-modified-added">+   virtual bool cmp( const Node &amp;n ) const {</span>
      return _require_atomic_access == ((LoadLNode&amp;)n)._require_atomic_access
        &amp;&amp; LoadNode::cmp(n);
    }
    virtual uint size_of() const { return sizeof(*this); }
    const bool _require_atomic_access;  // is piecewise load forbidden?
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -389,11 +411,11 @@</span>
    virtual int store_Opcode() const { return Op_StoreL; }
    virtual BasicType memory_type() const { return T_LONG; }
    bool require_atomic_access() const { return _require_atomic_access; }
    static LoadLNode* make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type,
                                  const Type* rt, MemOrd mo, ControlDependency control_dependency = DependsOnlyOnTest,
<span class="udiff-line-modified-removed">-                                 bool unaligned = false, bool mismatched = false, bool unsafe = false);</span>
<span class="udiff-line-modified-added">+                                 bool unaligned = false, bool mismatched = false, bool unsafe = false, uint8_t barrier_data = 0);</span>
  #ifndef PRODUCT
    virtual void dump_spec(outputStream *st) const {
      LoadNode::dump_spec(st);
      if (_require_atomic_access)  st-&gt;print(&quot; Atomic!&quot;);
    }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -423,11 +445,11 @@</span>
  
  //------------------------------LoadDNode--------------------------------------
  // Load a double (64 bits) from memory
  class LoadDNode : public LoadNode {
    virtual uint hash() const { return LoadNode::hash() + _require_atomic_access; }
<span class="udiff-line-modified-removed">-   virtual uint cmp( const Node &amp;n ) const {</span>
<span class="udiff-line-modified-added">+   virtual bool cmp( const Node &amp;n ) const {</span>
      return _require_atomic_access == ((LoadDNode&amp;)n)._require_atomic_access
        &amp;&amp; LoadNode::cmp(n);
    }
    virtual uint size_of() const { return sizeof(*this); }
    const bool _require_atomic_access;  // is piecewise load forbidden?
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -441,11 +463,11 @@</span>
    virtual int store_Opcode() const { return Op_StoreD; }
    virtual BasicType memory_type() const { return T_DOUBLE; }
    bool require_atomic_access() const { return _require_atomic_access; }
    static LoadDNode* make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type,
                                  const Type* rt, MemOrd mo, ControlDependency control_dependency = DependsOnlyOnTest,
<span class="udiff-line-modified-removed">-                                 bool unaligned = false, bool mismatched = false, bool unsafe = false);</span>
<span class="udiff-line-modified-added">+                                 bool unaligned = false, bool mismatched = false, bool unsafe = false, uint8_t barrier_data = 0);</span>
  #ifndef PRODUCT
    virtual void dump_spec(outputStream *st) const {
      LoadNode::dump_spec(st);
      if (_require_atomic_access)  st-&gt;print(&quot; Atomic!&quot;);
    }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -533,11 +555,11 @@</span>
    // this field.
    const MemOrd _mo;
    // Needed for proper cloning.
    virtual uint size_of() const { return sizeof(*this); }
  protected:
<span class="udiff-line-modified-removed">-   virtual uint cmp( const Node &amp;n ) const;</span>
<span class="udiff-line-modified-added">+   virtual bool cmp( const Node &amp;n ) const;</span>
    virtual bool depends_only_on_test() const { return false; }
  
    Node *Ideal_masked_input       (PhaseGVN *phase, uint mask);
    Node *Ideal_sign_extended_input(PhaseGVN *phase, int  num_bits);
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -648,11 +670,11 @@</span>
  
  //------------------------------StoreLNode-------------------------------------
  // Store long to memory
  class StoreLNode : public StoreNode {
    virtual uint hash() const { return StoreNode::hash() + _require_atomic_access; }
<span class="udiff-line-modified-removed">-   virtual uint cmp( const Node &amp;n ) const {</span>
<span class="udiff-line-modified-added">+   virtual bool cmp( const Node &amp;n ) const {</span>
      return _require_atomic_access == ((StoreLNode&amp;)n)._require_atomic_access
        &amp;&amp; StoreNode::cmp(n);
    }
    virtual uint size_of() const { return sizeof(*this); }
    const bool _require_atomic_access;  // is piecewise store forbidden?
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -684,11 +706,11 @@</span>
  
  //------------------------------StoreDNode-------------------------------------
  // Store double to memory
  class StoreDNode : public StoreNode {
    virtual uint hash() const { return StoreNode::hash() + _require_atomic_access; }
<span class="udiff-line-modified-removed">-   virtual uint cmp( const Node &amp;n ) const {</span>
<span class="udiff-line-modified-added">+   virtual bool cmp( const Node &amp;n ) const {</span>
      return _require_atomic_access == ((StoreDNode&amp;)n)._require_atomic_access
        &amp;&amp; StoreNode::cmp(n);
    }
    virtual uint size_of() const { return sizeof(*this); }
    const bool _require_atomic_access;  // is piecewise store forbidden?
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -744,11 +766,11 @@</span>
  // The last StoreCM before a SafePoint must be preserved and occur after its &quot;oop&quot; store
  // Preceeding equivalent StoreCMs may be eliminated.
  class StoreCMNode : public StoreNode {
   private:
    virtual uint hash() const { return StoreNode::hash() + _oop_alias_idx; }
<span class="udiff-line-modified-removed">-   virtual uint cmp( const Node &amp;n ) const {</span>
<span class="udiff-line-modified-added">+   virtual bool cmp( const Node &amp;n ) const {</span>
      return _oop_alias_idx == ((StoreCMNode&amp;)n)._oop_alias_idx
        &amp;&amp; StoreNode::cmp(n);
    }
    virtual uint size_of() const { return sizeof(*this); }
    int _oop_alias_idx;   // The alias_idx of OopStore
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -808,10 +830,11 @@</span>
  // Note: is_Mem() method returns &#39;true&#39; for this class.
  class LoadStoreNode : public Node {
  private:
    const Type* const _type;      // What kind of value is loaded?
    const TypePtr* _adr_type;     // What kind of memory is being addressed?
<span class="udiff-line-added">+   uint8_t _barrier; // Bit field with barrier information</span>
    virtual uint size_of() const; // Size is bigger
  public:
    LoadStoreNode( Node *c, Node *mem, Node *adr, Node *val, const TypePtr* at, const Type* rt, uint required );
    virtual bool depends_only_on_test() const { return false; }
    virtual uint match_edge(uint idx) const { return idx == MemNode::Address || idx == MemNode::ValueIn; }
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -820,10 +843,13 @@</span>
    virtual uint ideal_reg() const;
    virtual const class TypePtr *adr_type() const { return _adr_type; }  // returns bottom_type of address
  
    bool result_not_used() const;
    MemBarNode* trailing_membar() const;
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+   uint8_t barrier_data() { return _barrier; }</span>
<span class="udiff-line-added">+   void set_barrier_data(uint8_t barrier_data) { _barrier = barrier_data; }</span>
  };
  
  class LoadStoreConditionalNode : public LoadStoreNode {
  public:
    enum {
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -871,10 +897,11 @@</span>
  public:
    CompareAndSwapNode( Node *c, Node *mem, Node *adr, Node *val, Node *ex, MemNode::MemOrd mem_ord) : LoadStoreConditionalNode(c, mem, adr, val, ex), _mem_ord(mem_ord) {}
    MemNode::MemOrd order() const {
      return _mem_ord;
    }
<span class="udiff-line-added">+   virtual uint size_of() const { return sizeof(*this); }</span>
  };
  
  class CompareAndExchangeNode : public LoadStoreNode {
  private:
    const MemNode::MemOrd _mem_ord;
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -888,10 +915,11 @@</span>
    }
  
    MemNode::MemOrd order() const {
      return _mem_ord;
    }
<span class="udiff-line-added">+   virtual uint size_of() const { return sizeof(*this); }</span>
  };
  
  //------------------------------CompareAndSwapBNode---------------------------
  class CompareAndSwapBNode : public CompareAndSwapNode {
  public:
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1140,11 +1168,11 @@</span>
  // before a FastUnlock or volatile-store.  All volatiles need to be
  // serialized, so we follow all volatile-stores with a MemBar-Volatile to
  // separate it from any following volatile-load.
  class MemBarNode: public MultiNode {
    virtual uint hash() const ;                  // { return NO_HASH; }
<span class="udiff-line-modified-removed">-   virtual uint cmp( const Node &amp;n ) const ;    // Always fail, except on self</span>
<span class="udiff-line-modified-added">+   virtual bool cmp( const Node &amp;n ) const ;    // Always fail, except on self</span>
  
    virtual uint size_of() const { return sizeof(*this); }
    // Memory type this node is serializing.  Usually either rawptr or bottom.
    const TypePtr* _adr_type;
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1358,25 +1386,25 @@</span>
    bool stores_are_sane(PhaseTransform* phase);
  #endif //ASSERT
  
    // See if this store can be captured; return offset where it initializes.
    // Return 0 if the store cannot be moved (any sort of problem).
<span class="udiff-line-modified-removed">-   intptr_t can_capture_store(StoreNode* st, PhaseTransform* phase, bool can_reshape);</span>
<span class="udiff-line-modified-added">+   intptr_t can_capture_store(StoreNode* st, PhaseGVN* phase, bool can_reshape);</span>
  
    // Capture another store; reformat it to write my internal raw memory.
    // Return the captured copy, else NULL if there is some sort of problem.
<span class="udiff-line-modified-removed">-   Node* capture_store(StoreNode* st, intptr_t start, PhaseTransform* phase, bool can_reshape);</span>
<span class="udiff-line-modified-added">+   Node* capture_store(StoreNode* st, intptr_t start, PhaseGVN* phase, bool can_reshape);</span>
  
    // Find captured store which corresponds to the range [start..start+size).
    // Return my own memory projection (meaning the initial zero bits)
    // if there is no such store.  Return NULL if there is a problem.
    Node* find_captured_store(intptr_t start, int size_in_bytes, PhaseTransform* phase);
  
    // Called when the associated AllocateNode is expanded into CFG.
    Node* complete_stores(Node* rawctl, Node* rawmem, Node* rawptr,
                          intptr_t header_size, Node* size_in_bytes,
<span class="udiff-line-modified-removed">-                         PhaseGVN* phase);</span>
<span class="udiff-line-modified-added">+                         PhaseIterGVN* phase);</span>
  
   private:
    void remove_extra_zeroes();
  
    // Find out where a captured store should be placed (or already is placed).
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1385,11 +1413,11 @@</span>
  
    static intptr_t get_store_offset(Node* st, PhaseTransform* phase);
  
    Node* make_raw_address(intptr_t offset, PhaseTransform* phase);
  
<span class="udiff-line-modified-removed">-   bool detect_init_independence(Node* n, int&amp; count);</span>
<span class="udiff-line-modified-added">+   bool detect_init_independence(Node* value, PhaseGVN* phase);</span>
  
    void coalesce_subword_stores(intptr_t header_size, Node* size_in_bytes,
                                 PhaseGVN* phase);
  
    intptr_t find_next_fullword_store(uint i, PhaseGVN* phase);
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1397,11 +1425,11 @@</span>
  
  //------------------------------MergeMem---------------------------------------
  // (See comment in memnode.cpp near MergeMemNode::MergeMemNode for semantics.)
  class MergeMemNode: public Node {
    virtual uint hash() const ;                  // { return NO_HASH; }
<span class="udiff-line-modified-removed">-   virtual uint cmp( const Node &amp;n ) const ;    // Always fail, except on self</span>
<span class="udiff-line-modified-added">+   virtual bool cmp( const Node &amp;n ) const ;    // Always fail, except on self</span>
    friend class MergeMemStream;
    MergeMemNode(Node* def);  // clients use MergeMemNode::make
  
  public:
    // If the input is a whole memory state, clone it with all its slices intact.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -1629,10 +1657,46 @@</span>
      }
      return false;
    }
  };
  
<span class="udiff-line-added">+ // cachewb node for guaranteeing writeback of the cache line at a</span>
<span class="udiff-line-added">+ // given address to (non-volatile) RAM</span>
<span class="udiff-line-added">+ class CacheWBNode : public Node {</span>
<span class="udiff-line-added">+ public:</span>
<span class="udiff-line-added">+   CacheWBNode(Node *ctrl, Node *mem, Node *addr) : Node(ctrl, mem, addr) {}</span>
<span class="udiff-line-added">+   virtual int Opcode() const;</span>
<span class="udiff-line-added">+   virtual uint ideal_reg() const { return NotAMachineReg; }</span>
<span class="udiff-line-added">+   virtual uint match_edge(uint idx) const { return (idx == 2); }</span>
<span class="udiff-line-added">+   virtual const TypePtr *adr_type() const { return TypePtr::BOTTOM; }</span>
<span class="udiff-line-added">+   virtual const Type *bottom_type() const { return Type::MEMORY; }</span>
<span class="udiff-line-added">+ };</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ // cachewb pre sync node for ensuring that writebacks are serialised</span>
<span class="udiff-line-added">+ // relative to preceding or following stores</span>
<span class="udiff-line-added">+ class CacheWBPreSyncNode : public Node {</span>
<span class="udiff-line-added">+ public:</span>
<span class="udiff-line-added">+   CacheWBPreSyncNode(Node *ctrl, Node *mem) : Node(ctrl, mem) {}</span>
<span class="udiff-line-added">+   virtual int Opcode() const;</span>
<span class="udiff-line-added">+   virtual uint ideal_reg() const { return NotAMachineReg; }</span>
<span class="udiff-line-added">+   virtual uint match_edge(uint idx) const { return false; }</span>
<span class="udiff-line-added">+   virtual const TypePtr *adr_type() const { return TypePtr::BOTTOM; }</span>
<span class="udiff-line-added">+   virtual const Type *bottom_type() const { return Type::MEMORY; }</span>
<span class="udiff-line-added">+ };</span>
<span class="udiff-line-added">+ </span>
<span class="udiff-line-added">+ // cachewb pre sync node for ensuring that writebacks are serialised</span>
<span class="udiff-line-added">+ // relative to preceding or following stores</span>
<span class="udiff-line-added">+ class CacheWBPostSyncNode : public Node {</span>
<span class="udiff-line-added">+ public:</span>
<span class="udiff-line-added">+   CacheWBPostSyncNode(Node *ctrl, Node *mem) : Node(ctrl, mem) {}</span>
<span class="udiff-line-added">+   virtual int Opcode() const;</span>
<span class="udiff-line-added">+   virtual uint ideal_reg() const { return NotAMachineReg; }</span>
<span class="udiff-line-added">+   virtual uint match_edge(uint idx) const { return false; }</span>
<span class="udiff-line-added">+   virtual const TypePtr *adr_type() const { return TypePtr::BOTTOM; }</span>
<span class="udiff-line-added">+   virtual const Type *bottom_type() const { return Type::MEMORY; }</span>
<span class="udiff-line-added">+ };</span>
<span class="udiff-line-added">+ </span>
  //------------------------------Prefetch---------------------------------------
  
  // Allocation prefetch which may fault, TLAB size have to be adjusted.
  class PrefetchAllocationNode : public Node {
  public:
</pre>
<center><a href="memnode.cpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="mulnode.cpp.udiff.html" target="_top">next &gt;</a></center>  </body>
</html>