diff a/src/hotspot/share/opto/matcher.cpp b/src/hotspot/share/opto/matcher.cpp
--- a/src/hotspot/share/opto/matcher.cpp
+++ b/src/hotspot/share/opto/matcher.cpp
@@ -25,10 +25,11 @@
 #include "precompiled.hpp"
 #include "gc/shared/barrierSet.hpp"
 #include "gc/shared/c2/barrierSetC2.hpp"
 #include "memory/allocation.inline.hpp"
 #include "memory/resourceArea.hpp"
+#include "oops/compressedOops.hpp"
 #include "opto/ad.hpp"
 #include "opto/addnode.hpp"
 #include "opto/callnode.hpp"
 #include "opto/idealGraphPrinter.hpp"
 #include "opto/matcher.hpp"
@@ -154,11 +155,11 @@
   Unique_Node_List worklist;
   VectorSet visited(Thread::current()->resource_area());
   worklist.push(xroot);
   while (worklist.size() > 0) {
     Node* n = worklist.pop();
-    visited <<= n->_idx;
+    visited.set(n->_idx);
     assert(C->node_arena()->contains(n), "dead node");
     for (uint j = 0; j < n->req(); j++) {
       Node* in = n->in(j);
       if (in != NULL) {
         assert(C->node_arena()->contains(in), "dead node");
@@ -337,11 +338,11 @@
   C->set_unique(0);
   C->reset_dead_node_list();
 
   // Recursively match trees from old space into new space.
   // Correct leaves of new-space Nodes; they point to old-space.
-  _visited.Clear();             // Clear visit bits for xform call
+  _visited.clear();
   C->set_cached_top_node(xform( C->top(), live_nodes ));
   if (!C->failing()) {
     Node* xroot =        xform( C->root(), 1 );
     if (xroot == NULL) {
       Matcher::soft_match_failure();  // recursive matching process failed
@@ -389,14 +390,20 @@
 
   // Now smoke old-space
   NOT_DEBUG( old->destruct_contents() );
 
   // ------------------------
-  // Set up save-on-entry registers
+  // Set up save-on-entry registers.
   Fixup_Save_On_Entry( );
-}
 
+  { // Cleanup mach IR after selection phase is over.
+    Compile::TracePhase tp("postselect_cleanup", &timers[_t_postselect_cleanup]);
+    do_postselect_cleanup();
+    if (C->failing())  return;
+    assert(verify_after_postselect_cleanup(), "");
+  }
+}
 
 //------------------------------Fixup_Save_On_Entry----------------------------
 // The stated purpose of this routine is to take care of save-on-entry
 // registers.  However, the overall goal of the Match phase is to convert into
 // machine-specific instructions which have RegMasks to guide allocation.
@@ -414,18 +421,24 @@
   rms[TypeFunc::ReturnAdr] = ret_adr;
   rms[TypeFunc::FramePtr ] = fp;
   return rms;
 }
 
-//---------------------------init_first_stack_mask-----------------------------
+#define NOF_STACK_MASKS (3*6+5)
+
 // Create the initial stack mask used by values spilling to the stack.
 // Disallow any debug info in outgoing argument areas by setting the
 // initial mask accordingly.
 void Matcher::init_first_stack_mask() {
 
   // Allocate storage for spill masks as masks for the appropriate load type.
-  RegMask *rms = (RegMask*)C->comp_arena()->Amalloc_D(sizeof(RegMask) * (3*6+5));
+  RegMask *rms = (RegMask*)C->comp_arena()->Amalloc_D(sizeof(RegMask) * NOF_STACK_MASKS);
+
+  // Initialize empty placeholder masks into the newly allocated arena
+  for (int i = 0; i < NOF_STACK_MASKS; i++) {
+    new (rms + i) RegMask();
+  }
 
   idealreg2spillmask  [Op_RegN] = &rms[0];
   idealreg2spillmask  [Op_RegI] = &rms[1];
   idealreg2spillmask  [Op_RegL] = &rms[2];
   idealreg2spillmask  [Op_RegF] = &rms[3];
@@ -842,58 +855,27 @@
     mreg2regmask[i].Insert(i);
   }
 
   // Grab the Frame Pointer
   Node *fp  = ret->in(TypeFunc::FramePtr);
-  Node *mem = ret->in(TypeFunc::Memory);
-  const TypePtr* atp = TypePtr::BOTTOM;
   // Share frame pointer while making spill ops
   set_shared(fp);
 
-  // Compute generic short-offset Loads
-#ifdef _LP64
-  MachNode *spillCP = match_tree(new LoadNNode(NULL,mem,fp,atp,TypeInstPtr::BOTTOM,MemNode::unordered));
-#endif
-  MachNode *spillI  = match_tree(new LoadINode(NULL,mem,fp,atp,TypeInt::INT,MemNode::unordered));
-  MachNode *spillL  = match_tree(new LoadLNode(NULL,mem,fp,atp,TypeLong::LONG,MemNode::unordered, LoadNode::DependsOnlyOnTest, false));
-  MachNode *spillF  = match_tree(new LoadFNode(NULL,mem,fp,atp,Type::FLOAT,MemNode::unordered));
-  MachNode *spillD  = match_tree(new LoadDNode(NULL,mem,fp,atp,Type::DOUBLE,MemNode::unordered));
-  MachNode *spillP  = match_tree(new LoadPNode(NULL,mem,fp,atp,TypeInstPtr::BOTTOM,MemNode::unordered));
-  assert(spillI != NULL && spillL != NULL && spillF != NULL &&
-         spillD != NULL && spillP != NULL, "");
-  // Get the ADLC notion of the right regmask, for each basic type.
+// Get the ADLC notion of the right regmask, for each basic type.
 #ifdef _LP64
-  idealreg2regmask[Op_RegN] = &spillCP->out_RegMask();
+  idealreg2regmask[Op_RegN] = regmask_for_ideal_register(Op_RegN, ret);
 #endif
-  idealreg2regmask[Op_RegI] = &spillI->out_RegMask();
-  idealreg2regmask[Op_RegL] = &spillL->out_RegMask();
-  idealreg2regmask[Op_RegF] = &spillF->out_RegMask();
-  idealreg2regmask[Op_RegD] = &spillD->out_RegMask();
-  idealreg2regmask[Op_RegP] = &spillP->out_RegMask();
-
-  // Vector regmasks.
-  if (Matcher::vector_size_supported(T_BYTE,4)) {
-    TypeVect::VECTS = TypeVect::make(T_BYTE, 4);
-    MachNode *spillVectS = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTS));
-    idealreg2regmask[Op_VecS] = &spillVectS->out_RegMask();
-  }
-  if (Matcher::vector_size_supported(T_FLOAT,2)) {
-    MachNode *spillVectD = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTD));
-    idealreg2regmask[Op_VecD] = &spillVectD->out_RegMask();
-  }
-  if (Matcher::vector_size_supported(T_FLOAT,4)) {
-    MachNode *spillVectX = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTX));
-    idealreg2regmask[Op_VecX] = &spillVectX->out_RegMask();
-  }
-  if (Matcher::vector_size_supported(T_FLOAT,8)) {
-    MachNode *spillVectY = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTY));
-    idealreg2regmask[Op_VecY] = &spillVectY->out_RegMask();
-  }
-  if (Matcher::vector_size_supported(T_FLOAT,16)) {
-    MachNode *spillVectZ = match_tree(new LoadVectorNode(NULL,mem,fp,atp,TypeVect::VECTZ));
-    idealreg2regmask[Op_VecZ] = &spillVectZ->out_RegMask();
-  }
+  idealreg2regmask[Op_RegI] = regmask_for_ideal_register(Op_RegI, ret);
+  idealreg2regmask[Op_RegP] = regmask_for_ideal_register(Op_RegP, ret);
+  idealreg2regmask[Op_RegF] = regmask_for_ideal_register(Op_RegF, ret);
+  idealreg2regmask[Op_RegD] = regmask_for_ideal_register(Op_RegD, ret);
+  idealreg2regmask[Op_RegL] = regmask_for_ideal_register(Op_RegL, ret);
+  idealreg2regmask[Op_VecS] = regmask_for_ideal_register(Op_VecS, ret);
+  idealreg2regmask[Op_VecD] = regmask_for_ideal_register(Op_VecD, ret);
+  idealreg2regmask[Op_VecX] = regmask_for_ideal_register(Op_VecX, ret);
+  idealreg2regmask[Op_VecY] = regmask_for_ideal_register(Op_VecY, ret);
+  idealreg2regmask[Op_VecZ] = regmask_for_ideal_register(Op_VecZ, ret);
 }
 
 #ifdef ASSERT
 static void match_alias_type(Compile* C, Node* n, Node* m) {
   if (!VerifyAliases)  return;  // do not go looking for trouble by default
@@ -1742,10 +1724,17 @@
   if (leaf->is_Con() || leaf->is_DecodeNarrowPtr()) {
     // Record the con for sharing
     _shared_nodes.map(leaf->_idx, ex);
   }
 
+  // Have mach nodes inherit GC barrier data
+  if (leaf->is_LoadStore()) {
+    mach->set_barrier_data(leaf->as_LoadStore()->barrier_data());
+  } else if (leaf->is_Mem()) {
+    mach->set_barrier_data(leaf->as_Mem()->barrier_data());
+  }
+
   return ex;
 }
 
 void Matcher::handle_precedence_edges(Node* n, MachNode *mach) {
   for (uint i = n->req(); i < n->len(); i++) {
@@ -2019,10 +2008,19 @@
   }
   return false;
 }
 #endif // X86
 
+bool Matcher::is_vshift_con_pattern(Node *n, Node *m) {
+  if (n != NULL && m != NULL) {
+    return VectorNode::is_vector_shift(n) &&
+           VectorNode::is_vector_shift_count(m) && m->in(1)->is_Con();
+  }
+  return false;
+}
+
+
 bool Matcher::clone_base_plus_offset_address(AddPNode* m, Matcher::MStack& mstack, VectorSet& address_visited) {
   Node *off = m->in(AddPNode::Offset);
   if (off->is_Con()) {
     address_visited.test_set(m->_idx); // Flag as address_visited
     mstack.push(m->in(AddPNode::Address), Pre_Visit);
@@ -2057,25 +2055,28 @@
       }
       if (is_visited(n)) {   // Visited already?
         // Node is shared and has no reason to clone.  Flag it as shared.
         // This causes it to match into a register for the sharing.
         set_shared(n);       // Flag as shared and
+        if (n->is_DecodeNarrowPtr()) {
+          // Oop field/array element loads must be shared but since
+          // they are shared through a DecodeN they may appear to have
+          // a single use so force sharing here.
+          set_shared(n->in(1));
+        }
         mstack.pop();        // remove node from stack
         continue;
       }
       nstate = Visit; // Not already visited; so visit now
     }
     if (nstate == Visit) {
       mstack.set_state(Post_Visit);
       set_visited(n);   // Flag as visited now
       bool mem_op = false;
       int mem_addr_idx = MemNode::Address;
-      bool gc_handled = BarrierSet::barrier_set()->barrier_set_c2()->matcher_find_shared_visit(this, mstack, n, nop, mem_op, mem_addr_idx);
-      if (!gc_handled) {
-        if (find_shared_visit(mstack, n, nop, mem_op, mem_addr_idx)) {
-          continue;
-        }
+      if (find_shared_visit(mstack, n, nop, mem_op, mem_addr_idx)) {
+        continue;
       }
       for(int i = n->req() - 1; i >= 0; --i) { // For my children
         Node *m = n->in(i); // Get ith input
         if (m == NULL) continue;  // Ignore NULLs
         uint mop = m->Opcode();
@@ -2089,24 +2090,21 @@
         if( _must_clone[mop] ) {
           mstack.push(m, Visit);
           continue; // for(int i = ...)
         }
 
-        if( mop == Op_AddP && m->in(AddPNode::Base)->is_DecodeNarrowPtr()) {
-          // Bases used in addresses must be shared but since
-          // they are shared through a DecodeN they may appear
-          // to have a single use so force sharing here.
-          set_shared(m->in(AddPNode::Base)->in(1));
-        }
-
         // if 'n' and 'm' are part of a graph for BMI instruction, clone this node.
 #ifdef X86
         if (UseBMI1Instructions && is_bmi_pattern(n, m)) {
           mstack.push(m, Visit);
           continue;
         }
 #endif
+        if (is_vshift_con_pattern(n, m)) {
+          mstack.push(m, Visit);
+          continue;
+        }
 
         // Clone addressing expressions as they are "free" in memory access instructions
         if (mem_op && i == mem_addr_idx && mop == Op_AddP &&
             // When there are other uses besides address expressions
             // put it on stack and mark as shared.
@@ -2474,10 +2472,182 @@
       i-=2;
     }
   }
 }
 
+bool Matcher::gen_narrow_oop_implicit_null_checks() {
+  // Advice matcher to perform null checks on the narrow oop side.
+  // Implicit checks are not possible on the uncompressed oop side anyway
+  // (at least not for read accesses).
+  // Performs significantly better (especially on Power 6).
+  if (!os::zero_page_read_protected()) {
+    return true;
+  }
+  return CompressedOops::use_implicit_null_checks() &&
+         (narrow_oop_use_complex_address() ||
+          CompressedOops::base() != NULL);
+}
+
+// Compute RegMask for an ideal register.
+const RegMask* Matcher::regmask_for_ideal_register(uint ideal_reg, Node* ret) {
+  const Type* t = Type::mreg2type[ideal_reg];
+  if (t == NULL) {
+    assert(ideal_reg >= Op_VecS && ideal_reg <= Op_VecZ, "not a vector: %d", ideal_reg);
+    return NULL; // not supported
+  }
+  Node* fp  = ret->in(TypeFunc::FramePtr);
+  Node* mem = ret->in(TypeFunc::Memory);
+  const TypePtr* atp = TypePtr::BOTTOM;
+  MemNode::MemOrd mo = MemNode::unordered;
+
+  Node* spill;
+  switch (ideal_reg) {
+    case Op_RegN: spill = new LoadNNode(NULL, mem, fp, atp, t->is_narrowoop(), mo); break;
+    case Op_RegI: spill = new LoadINode(NULL, mem, fp, atp, t->is_int(),       mo); break;
+    case Op_RegP: spill = new LoadPNode(NULL, mem, fp, atp, t->is_ptr(),       mo); break;
+    case Op_RegF: spill = new LoadFNode(NULL, mem, fp, atp, t,                 mo); break;
+    case Op_RegD: spill = new LoadDNode(NULL, mem, fp, atp, t,                 mo); break;
+    case Op_RegL: spill = new LoadLNode(NULL, mem, fp, atp, t->is_long(),      mo); break;
+
+    case Op_VecS: // fall-through
+    case Op_VecD: // fall-through
+    case Op_VecX: // fall-through
+    case Op_VecY: // fall-through
+    case Op_VecZ: spill = new LoadVectorNode(NULL, mem, fp, atp, t->is_vect()); break;
+
+    default: ShouldNotReachHere();
+  }
+  MachNode* mspill = match_tree(spill);
+  assert(mspill != NULL, "matching failed: %d", ideal_reg);
+  // Handle generic vector operand case
+  if (Matcher::supports_generic_vector_operands && t->isa_vect()) {
+    specialize_mach_node(mspill);
+  }
+  return &mspill->out_RegMask();
+}
+
+// Process Mach IR right after selection phase is over.
+void Matcher::do_postselect_cleanup() {
+  if (supports_generic_vector_operands) {
+    specialize_generic_vector_operands();
+    if (C->failing())  return;
+  }
+}
+
+//----------------------------------------------------------------------
+// Generic machine operands elision.
+//----------------------------------------------------------------------
+
+// Convert (leg)Vec to (leg)Vec[SDXYZ].
+MachOper* Matcher::specialize_vector_operand_helper(MachNode* m, uint opnd_idx, const TypeVect* vt) {
+  MachOper* original_opnd = m->_opnds[opnd_idx];
+  uint ideal_reg = vt->ideal_reg();
+  // Handle special cases.
+  // LShiftCntV/RShiftCntV report wide vector type, but Matcher::vector_shift_count_ideal_reg() as ideal register (see vectornode.hpp).
+  // Look for shift count use sites as well (at vector shift nodes).
+  int opc = m->ideal_Opcode();
+  if ((VectorNode::is_vector_shift_count(opc)  && opnd_idx == 0) || // DEF operand of LShiftCntV/RShiftCntV
+      (VectorNode::is_vector_shift(opc)        && opnd_idx == 2)) { // shift operand of a vector shift node
+    ideal_reg = Matcher::vector_shift_count_ideal_reg(vt->length_in_bytes());
+  }
+  return Matcher::specialize_generic_vector_operand(original_opnd, ideal_reg, false);
+}
+
+// Compute concrete vector operand for a generic TEMP vector mach node based on its user info.
+void Matcher::specialize_temp_node(MachTempNode* tmp, MachNode* use, uint idx) {
+  assert(use->in(idx) == tmp, "not a user");
+  assert(!Matcher::is_generic_vector(use->_opnds[0]), "use not processed yet");
+
+  if ((uint)idx == use->two_adr()) { // DEF_TEMP case
+    tmp->_opnds[0] = use->_opnds[0]->clone();
+  } else {
+    uint ideal_vreg = vector_ideal_reg(C->max_vector_size());
+    tmp->_opnds[0] = specialize_generic_vector_operand(tmp->_opnds[0], ideal_vreg, true);
+  }
+}
+
+// Compute concrete vector operand for a generic DEF/USE vector operand (of mach node m at index idx).
+MachOper* Matcher::specialize_vector_operand(MachNode* m, uint opnd_idx) {
+  assert(Matcher::is_generic_vector(m->_opnds[opnd_idx]), "repeated updates");
+  Node* def = NULL;
+  if (opnd_idx == 0) { // DEF
+    def = m; // use mach node itself to compute vector operand type
+  } else {
+    int base_idx = m->operand_index(opnd_idx);
+    def = m->in(base_idx);
+    if (def->is_Mach()) {
+      if (def->is_MachTemp() && Matcher::is_generic_vector(def->as_Mach()->_opnds[0])) {
+        specialize_temp_node(def->as_MachTemp(), m, base_idx); // MachTemp node use site
+      } else if (is_generic_reg2reg_move(def->as_Mach())) {
+        def = def->in(1); // skip over generic reg-to-reg moves
+      }
+    }
+  }
+  return specialize_vector_operand_helper(m, opnd_idx, def->bottom_type()->is_vect());
+}
+
+void Matcher::specialize_mach_node(MachNode* m) {
+  assert(!m->is_MachTemp(), "processed along with its user");
+  // For generic use operands pull specific register class operands from
+  // its def instruction's output operand (def operand).
+  for (uint i = 0; i < m->num_opnds(); i++) {
+    if (Matcher::is_generic_vector(m->_opnds[i])) {
+      m->_opnds[i] = specialize_vector_operand(m, i);
+    }
+  }
+}
+
+// Replace generic vector operands with concrete vector operands and eliminate generic reg-to-reg moves from the graph.
+void Matcher::specialize_generic_vector_operands() {
+  assert(supports_generic_vector_operands, "sanity");
+  ResourceMark rm;
+
+  if (C->max_vector_size() == 0) {
+    return; // no vector instructions or operands
+  }
+  // Replace generic vector operands (vec/legVec) with concrete ones (vec[SDXYZ]/legVec[SDXYZ])
+  // and remove reg-to-reg vector moves (MoveVec2Leg and MoveLeg2Vec).
+  Unique_Node_List live_nodes;
+  C->identify_useful_nodes(live_nodes);
+
+  while (live_nodes.size() > 0) {
+    MachNode* m = live_nodes.pop()->isa_Mach();
+    if (m != NULL) {
+      if (Matcher::is_generic_reg2reg_move(m)) {
+        // Register allocator properly handles vec <=> leg moves using register masks.
+        int opnd_idx = m->operand_index(1);
+        Node* def = m->in(opnd_idx);
+        m->subsume_by(def, C);
+      } else if (m->is_MachTemp()) {
+        // process MachTemp nodes at use site (see Matcher::specialize_vector_operand)
+      } else {
+        specialize_mach_node(m);
+      }
+    }
+  }
+}
+
+#ifdef ASSERT
+bool Matcher::verify_after_postselect_cleanup() {
+  assert(!C->failing(), "sanity");
+  if (supports_generic_vector_operands) {
+    Unique_Node_List useful;
+    C->identify_useful_nodes(useful);
+    for (uint i = 0; i < useful.size(); i++) {
+      MachNode* m = useful.at(i)->isa_Mach();
+      if (m != NULL) {
+        assert(!Matcher::is_generic_reg2reg_move(m), "no MoveVec nodes allowed");
+        for (uint j = 0; j < m->num_opnds(); j++) {
+          assert(!Matcher::is_generic_vector(m->_opnds[j]), "no generic vector operands allowed");
+        }
+      }
+    }
+  }
+  return true;
+}
+#endif // ASSERT
+
 // Used by the DFA in dfa_xxx.cpp.  Check for a following barrier or
 // atomic instruction acting as a store_load barrier without any
 // intervening volatile load, and thus we don't need a barrier here.
 // We retain the Node to act as a compiler ordering barrier.
 bool Matcher::post_store_load_barrier(const Node* vmb) {
