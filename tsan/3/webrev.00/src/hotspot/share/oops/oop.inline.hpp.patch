diff a/src/hotspot/share/oops/oop.inline.hpp b/src/hotspot/share/oops/oop.inline.hpp
--- a/src/hotspot/share/oops/oop.inline.hpp
+++ b/src/hotspot/share/oops/oop.inline.hpp
@@ -24,94 +24,96 @@
 
 #ifndef SHARE_OOPS_OOP_INLINE_HPP
 #define SHARE_OOPS_OOP_INLINE_HPP
 
 #include "gc/shared/collectedHeap.hpp"
+#include "memory/universe.hpp"
 #include "oops/access.inline.hpp"
 #include "oops/arrayKlass.hpp"
 #include "oops/arrayOop.hpp"
 #include "oops/compressedOops.inline.hpp"
 #include "oops/klass.inline.hpp"
-#include "oops/markOop.inline.hpp"
+#include "oops/markWord.inline.hpp"
 #include "oops/oop.hpp"
 #include "runtime/atomic.hpp"
-#include "runtime/orderAccess.hpp"
 #include "runtime/os.hpp"
 #include "utilities/align.hpp"
 #include "utilities/macros.hpp"
 
 // Implementation of all inlined member functions defined in oop.hpp
 // We need a separate file to avoid circular references
 
-markOop  oopDesc::mark()      const {
-  return HeapAccess<MO_VOLATILE>::load_at(as_oop(), mark_offset_in_bytes());
+markWord oopDesc::mark() const {
+  uintptr_t v = HeapAccess<MO_VOLATILE>::load_at(as_oop(), mark_offset_in_bytes());
+  return markWord(v);
 }
 
-markOop  oopDesc::mark_raw()  const {
-  return _mark;
+markWord oopDesc::mark_raw() const {
+  return Atomic::load(&_mark);
 }
 
-markOop* oopDesc::mark_addr_raw() const {
-  return (markOop*) &_mark;
+markWord* oopDesc::mark_addr_raw() const {
+  return (markWord*) &_mark;
 }
 
-void oopDesc::set_mark(volatile markOop m) {
-  HeapAccess<MO_VOLATILE>::store_at(as_oop(), mark_offset_in_bytes(), m);
+void oopDesc::set_mark(markWord m) {
+  HeapAccess<MO_VOLATILE>::store_at(as_oop(), mark_offset_in_bytes(), m.value());
 }
 
-void oopDesc::set_mark_raw(volatile markOop m) {
-  _mark = m;
+void oopDesc::set_mark_raw(markWord m) {
+  Atomic::store(&_mark, m);
 }
 
-void oopDesc::set_mark_raw(HeapWord* mem, markOop m) {
-  *(markOop*)(((char*)mem) + mark_offset_in_bytes()) = m;
+void oopDesc::set_mark_raw(HeapWord* mem, markWord m) {
+  *(markWord*)(((char*)mem) + mark_offset_in_bytes()) = m;
 }
 
-void oopDesc::release_set_mark(markOop m) {
-  HeapAccess<MO_RELEASE>::store_at(as_oop(), mark_offset_in_bytes(), m);
+void oopDesc::release_set_mark(markWord m) {
+  HeapAccess<MO_RELEASE>::store_at(as_oop(), mark_offset_in_bytes(), m.value());
 }
 
-markOop oopDesc::cas_set_mark(markOop new_mark, markOop old_mark) {
-  return HeapAccess<>::atomic_cmpxchg_at(new_mark, as_oop(), mark_offset_in_bytes(), old_mark);
+markWord oopDesc::cas_set_mark(markWord new_mark, markWord old_mark) {
+  uintptr_t v = HeapAccess<>::atomic_cmpxchg_at(as_oop(), mark_offset_in_bytes(), old_mark.value(), new_mark.value());
+  return markWord(v);
 }
 
-markOop oopDesc::cas_set_mark_raw(markOop new_mark, markOop old_mark, atomic_memory_order order) {
-  return Atomic::cmpxchg(new_mark, &_mark, old_mark, order);
+markWord oopDesc::cas_set_mark_raw(markWord new_mark, markWord old_mark, atomic_memory_order order) {
+  return Atomic::cmpxchg(&_mark, old_mark, new_mark, order);
 }
 
 void oopDesc::init_mark() {
-  set_mark(markOopDesc::prototype_for_object(this));
+  set_mark(markWord::prototype_for_klass(klass()));
 }
 
 void oopDesc::init_mark_raw() {
-  set_mark_raw(markOopDesc::prototype_for_object(this));
+  set_mark_raw(markWord::prototype_for_klass(klass()));
 }
 
 Klass* oopDesc::klass() const {
   if (UseCompressedClassPointers) {
-    return Klass::decode_klass_not_null(_metadata._compressed_klass);
+    return CompressedKlassPointers::decode_not_null(_metadata._compressed_klass);
   } else {
     return _metadata._klass;
   }
 }
 
 Klass* oopDesc::klass_or_null() const volatile {
   if (UseCompressedClassPointers) {
-    return Klass::decode_klass(_metadata._compressed_klass);
+    return CompressedKlassPointers::decode(_metadata._compressed_klass);
   } else {
     return _metadata._klass;
   }
 }
 
 Klass* oopDesc::klass_or_null_acquire() const volatile {
   if (UseCompressedClassPointers) {
     // Workaround for non-const load_acquire parameter.
     const volatile narrowKlass* addr = &_metadata._compressed_klass;
     volatile narrowKlass* xaddr = const_cast<volatile narrowKlass*>(addr);
-    return Klass::decode_klass(OrderAccess::load_acquire(xaddr));
+    return CompressedKlassPointers::decode(Atomic::load_acquire(xaddr));
   } else {
-    return OrderAccess::load_acquire(&_metadata._klass);
+    return Atomic::load_acquire(&_metadata._klass);
   }
 }
 
 Klass** oopDesc::klass_addr(HeapWord* mem) {
   // Only used internally and with CMS and will not work with
@@ -142,23 +144,23 @@
   } while (0)
 
 void oopDesc::set_klass(Klass* k) {
   CHECK_SET_KLASS(k);
   if (UseCompressedClassPointers) {
-    *compressed_klass_addr() = Klass::encode_klass_not_null(k);
+    *compressed_klass_addr() = CompressedKlassPointers::encode_not_null(k);
   } else {
     *klass_addr() = k;
   }
 }
 
 void oopDesc::release_set_klass(HeapWord* mem, Klass* klass) {
   CHECK_SET_KLASS(klass);
   if (UseCompressedClassPointers) {
-    OrderAccess::release_store(compressed_klass_addr(mem),
-                               Klass::encode_klass_not_null(klass));
+    Atomic::release_store(compressed_klass_addr(mem),
+                          CompressedKlassPointers::encode_not_null(klass));
   } else {
-    OrderAccess::release_store(klass_addr(mem), klass);
+    Atomic::release_store(klass_addr(mem), klass);
   }
 }
 
 #undef CHECK_SET_KLASS
 
@@ -174,30 +176,10 @@
 
 void oopDesc::set_klass_gap(int v) {
   set_klass_gap((HeapWord*)this, v);
 }
 
-void oopDesc::set_klass_to_list_ptr(oop k) {
-  // This is only to be used during GC, for from-space objects, so no
-  // barrier is needed.
-  if (UseCompressedClassPointers) {
-    _metadata._compressed_klass = (narrowKlass)CompressedOops::encode(k);  // may be null (parnew overflow handling)
-  } else {
-    _metadata._klass = (Klass*)(address)k;
-  }
-}
-
-oop oopDesc::list_ptr_from_klass() {
-  // This is only to be used during GC, for from-space objects.
-  if (UseCompressedClassPointers) {
-    return CompressedOops::decode((narrowOop)_metadata._compressed_klass);
-  } else {
-    // Special case for GC
-    return (oop)(address)_metadata._klass;
-  }
-}
-
 bool oopDesc::is_a(Klass* k) const {
   return klass()->is_subtype_of(k);
 }
 
 int oopDesc::size()  {
@@ -239,29 +221,17 @@
       // This code could be simplified, but by keeping array_header_in_bytes
       // in units of bytes and doing it this way we can round up just once,
       // skipping the intermediate round to HeapWordSize.
       s = (int)(align_up(size_in_bytes, MinObjAlignmentInBytes) / HeapWordSize);
 
-      // ParNew (used by CMS), UseParallelGC and UseG1GC can change the length field
+      // UseParallelGC and UseG1GC can change the length field
       // of an "old copy" of an object array in the young gen so it indicates
       // the grey portion of an already copied array. This will cause the first
       // disjunct below to fail if the two comparands are computed across such
       // a concurrent change.
-      // ParNew also runs with promotion labs (which look like int
-      // filler arrays) which are subject to changing their declared size
-      // when finally retiring a PLAB; this also can cause the first disjunct
-      // to fail for another worker thread that is concurrently walking the block
-      // offset table. Both these invariant failures are benign for their
-      // current uses; we relax the assertion checking to cover these two cases below:
-      //     is_objArray() && is_forwarded()   // covers first scenario above
-      //  || is_typeArray()                    // covers second scenario above
-      // If and when UseParallelGC uses the same obj array oop stealing/chunking
-      // technique, we will need to suitably modify the assertion.
       assert((s == klass->oop_size(this)) ||
-             (Universe::heap()->is_gc_active() &&
-              ((is_typeArray() && UseConcMarkSweepGC) ||
-               (is_objArray()  && is_forwarded() && (UseConcMarkSweepGC || UseParallelGC || UseG1GC)))),
+             (Universe::heap()->is_gc_active() && is_objArray() && is_forwarded() && (UseParallelGC || UseG1GC)),
              "wrong array object size");
     } else {
       // Must be zero, so bite the bullet and take the virtual call.
       s = klass->oop_size(this);
     }
@@ -296,13 +266,14 @@
 inline void  oopDesc::byte_field_put(int offset, jbyte value)       { HeapAccess<>::store_at(as_oop(), offset, value); }
 
 inline jchar oopDesc::char_field(int offset) const                  { return HeapAccess<>::load_at(as_oop(), offset);  }
 inline void  oopDesc::char_field_put(int offset, jchar value)       { HeapAccess<>::store_at(as_oop(), offset, value); }
 
-inline jboolean oopDesc::bool_field(int offset) const               { return HeapAccess<>::load_at(as_oop(), offset);                }
+inline jboolean oopDesc::bool_field(int offset) const               { return HeapAccess<>::load_at(as_oop(), offset); }
 inline void     oopDesc::bool_field_put(int offset, jboolean value) { HeapAccess<>::store_at(as_oop(), offset, jboolean(value & 1)); }
-
+inline jboolean oopDesc::bool_field_volatile(int offset) const      { return HeapAccess<MO_SEQ_CST>::load_at(as_oop(), offset); }
+inline void     oopDesc::bool_field_put_volatile(int offset, jboolean value) { HeapAccess<MO_SEQ_CST>::store_at(as_oop(), offset, jboolean(value & 1)); }
 inline jshort oopDesc::short_field(int offset) const                { return HeapAccess<>::load_at(as_oop(), offset);  }
 inline void   oopDesc::short_field_put(int offset, jshort value)    { HeapAccess<>::store_at(as_oop(), offset, value); }
 
 inline jint oopDesc::int_field(int offset) const                    { return HeapAccess<>::load_at(as_oop(), offset);  }
 inline jint oopDesc::int_field_raw(int offset) const                { return RawAccess<>::load_at(as_oop(), offset);   }
@@ -316,110 +287,95 @@
 
 inline jdouble oopDesc::double_field(int offset) const              { return HeapAccess<>::load_at(as_oop(), offset);  }
 inline void    oopDesc::double_field_put(int offset, jdouble value) { HeapAccess<>::store_at(as_oop(), offset, value); }
 
 bool oopDesc::is_locked() const {
-  return mark()->is_locked();
+  return mark().is_locked();
 }
 
 bool oopDesc::is_unlocked() const {
-  return mark()->is_unlocked();
+  return mark().is_unlocked();
 }
 
 bool oopDesc::has_bias_pattern() const {
-  return mark()->has_bias_pattern();
+  return mark().has_bias_pattern();
 }
 
 bool oopDesc::has_bias_pattern_raw() const {
-  return mark_raw()->has_bias_pattern();
+  return mark_raw().has_bias_pattern();
 }
 
 // Used only for markSweep, scavenging
 bool oopDesc::is_gc_marked() const {
-  return mark_raw()->is_marked();
+  return mark_raw().is_marked();
 }
 
 // Used by scavengers
 bool oopDesc::is_forwarded() const {
   // The extra heap check is needed since the obj might be locked, in which case the
   // mark would point to a stack location and have the sentinel bit cleared
-  return mark_raw()->is_marked();
+  return mark_raw().is_marked();
 }
 
 // Used by scavengers
 void oopDesc::forward_to(oop p) {
-  assert(check_obj_alignment(p),
-         "forwarding to something not aligned");
-  assert(Universe::heap()->is_in_reserved(p),
-         "forwarding to something not in heap");
-  assert(!is_archived_object(oop(this)) &&
-         !is_archived_object(p),
-         "forwarding archive object");
-  markOop m = markOopDesc::encode_pointer_as_mark(p);
-  assert(m->decode_pointer() == p, "encoding must be reversable");
+  verify_forwardee(p);
+  markWord m = markWord::encode_pointer_as_mark(p);
+  assert(m.decode_pointer() == p, "encoding must be reversable");
   set_mark_raw(m);
 }
 
 // Used by parallel scavengers
-bool oopDesc::cas_forward_to(oop p, markOop compare, atomic_memory_order order) {
-  assert(check_obj_alignment(p),
-         "forwarding to something not aligned");
-  assert(Universe::heap()->is_in_reserved(p),
-         "forwarding to something not in heap");
-  markOop m = markOopDesc::encode_pointer_as_mark(p);
-  assert(m->decode_pointer() == p, "encoding must be reversable");
+bool oopDesc::cas_forward_to(oop p, markWord compare, atomic_memory_order order) {
+  verify_forwardee(p);
+  markWord m = markWord::encode_pointer_as_mark(p);
+  assert(m.decode_pointer() == p, "encoding must be reversable");
   return cas_set_mark_raw(m, compare, order) == compare;
 }
 
-oop oopDesc::forward_to_atomic(oop p, markOop compare, atomic_memory_order order) {
-  // CMS forwards some non-heap value into the mark oop to reserve oops during
-  // promotion, so the next two asserts do not hold.
-  assert(UseConcMarkSweepGC || check_obj_alignment(p),
-         "forwarding to something not aligned");
-  assert(UseConcMarkSweepGC || Universe::heap()->is_in_reserved(p),
-         "forwarding to something not in heap");
-  markOop m = markOopDesc::encode_pointer_as_mark(p);
-  assert(m->decode_pointer() == p, "encoding must be reversable");
-  markOop old_mark = cas_set_mark_raw(m, compare, order);
+oop oopDesc::forward_to_atomic(oop p, markWord compare, atomic_memory_order order) {
+  verify_forwardee(p);
+  markWord m = markWord::encode_pointer_as_mark(p);
+  assert(m.decode_pointer() == p, "encoding must be reversable");
+  markWord old_mark = cas_set_mark_raw(m, compare, order);
   if (old_mark == compare) {
     return NULL;
   } else {
-    return (oop)old_mark->decode_pointer();
+    return (oop)old_mark.decode_pointer();
   }
 }
 
 // Note that the forwardee is not the same thing as the displaced_mark.
 // The forwardee is used when copying during scavenge and mark-sweep.
 // It does need to clear the low two locking- and GC-related bits.
 oop oopDesc::forwardee() const {
-  return (oop) mark_raw()->decode_pointer();
+  return (oop) mark_raw().decode_pointer();
 }
 
 // Note that the forwardee is not the same thing as the displaced_mark.
 // The forwardee is used when copying during scavenge and mark-sweep.
 // It does need to clear the low two locking- and GC-related bits.
 oop oopDesc::forwardee_acquire() const {
-  markOop m = OrderAccess::load_acquire(&_mark);
-  return (oop) m->decode_pointer();
+  return (oop) Atomic::load_acquire(&_mark).decode_pointer();
 }
 
 // The following method needs to be MT safe.
 uint oopDesc::age() const {
   assert(!is_forwarded(), "Attempt to read age from forwarded mark");
   if (has_displaced_mark_raw()) {
-    return displaced_mark_raw()->age();
+    return displaced_mark_raw().age();
   } else {
-    return mark_raw()->age();
+    return mark_raw().age();
   }
 }
 
 void oopDesc::incr_age() {
   assert(!is_forwarded(), "Attempt to increment age of forwarded mark");
   if (has_displaced_mark_raw()) {
-    set_displaced_mark_raw(displaced_mark_raw()->incr_age());
+    set_displaced_mark_raw(displaced_mark_raw().incr_age());
   } else {
-    set_mark_raw(mark_raw()->incr_age());
+    set_mark_raw(mark_raw().incr_age());
   }
 }
 
 template <typename OopClosureType>
 void oopDesc::oop_iterate(OopClosureType* cl) {
@@ -457,28 +413,58 @@
 }
 
 intptr_t oopDesc::identity_hash() {
   // Fast case; if the object is unlocked and the hash value is set, no locking is needed
   // Note: The mark must be read into local variable to avoid concurrent updates.
-  markOop mrk = mark();
-  if (mrk->is_unlocked() && !mrk->has_no_hash()) {
-    return mrk->hash();
-  } else if (mrk->is_marked()) {
-    return mrk->hash();
+  markWord mrk = mark();
+  if (mrk.is_unlocked() && !mrk.has_no_hash()) {
+    return mrk.hash();
+  } else if (mrk.is_marked()) {
+    return mrk.hash();
   } else {
     return slow_identity_hash();
   }
 }
 
 bool oopDesc::has_displaced_mark_raw() const {
-  return mark_raw()->has_displaced_mark_helper();
+  return mark_raw().has_displaced_mark_helper();
+}
+
+markWord oopDesc::displaced_mark_raw() const {
+  return mark_raw().displaced_mark_helper();
+}
+
+void oopDesc::set_displaced_mark_raw(markWord m) {
+  mark_raw().set_displaced_mark_helper(m);
+}
+
+// Supports deferred calling of obj->klass().
+class DeferredObjectToKlass {
+  const oopDesc* _obj;
+
+public:
+  DeferredObjectToKlass(const oopDesc* obj) : _obj(obj) {}
+
+  // Implicitly convertible to const Klass*.
+  operator const Klass*() const {
+    return _obj->klass();
+  }
+};
+
+bool oopDesc::mark_must_be_preserved() const {
+  return mark_must_be_preserved(mark_raw());
 }
 
-markOop oopDesc::displaced_mark_raw() const {
-  return mark_raw()->displaced_mark_helper();
+bool oopDesc::mark_must_be_preserved(markWord m) const {
+  // There's a circular dependency between oop.inline.hpp and
+  // markWord.inline.hpp because markWord::must_be_preserved wants to call
+  // oopDesc::klass(). This could be solved by calling klass() here. However,
+  // not all paths inside must_be_preserved calls klass(). Defer the call until
+  // the klass is actually needed.
+  return m.must_be_preserved(DeferredObjectToKlass(this));
 }
 
-void oopDesc::set_displaced_mark_raw(markOop m) {
-  mark_raw()->set_displaced_mark_helper(m);
+bool oopDesc::mark_must_be_preserved_for_promotion_failure(markWord m) const {
+  return m.must_be_preserved_for_promotion_failure(DeferredObjectToKlass(this));
 }
 
 #endif // SHARE_OOPS_OOP_INLINE_HPP
