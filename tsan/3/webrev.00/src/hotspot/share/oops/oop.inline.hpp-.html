<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/share/oops/oop.inline.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_OOPS_OOP_INLINE_HPP
 26 #define SHARE_OOPS_OOP_INLINE_HPP
 27 
 28 #include &quot;gc/shared/collectedHeap.hpp&quot;
 29 #include &quot;oops/access.inline.hpp&quot;
 30 #include &quot;oops/arrayKlass.hpp&quot;
 31 #include &quot;oops/arrayOop.hpp&quot;
 32 #include &quot;oops/compressedOops.inline.hpp&quot;
 33 #include &quot;oops/klass.inline.hpp&quot;
 34 #include &quot;oops/markOop.inline.hpp&quot;
 35 #include &quot;oops/oop.hpp&quot;
 36 #include &quot;runtime/atomic.hpp&quot;
 37 #include &quot;runtime/orderAccess.hpp&quot;
 38 #include &quot;runtime/os.hpp&quot;
 39 #include &quot;utilities/align.hpp&quot;
 40 #include &quot;utilities/macros.hpp&quot;
 41 
 42 // Implementation of all inlined member functions defined in oop.hpp
 43 // We need a separate file to avoid circular references
 44 
 45 markOop  oopDesc::mark()      const {
 46   return HeapAccess&lt;MO_VOLATILE&gt;::load_at(as_oop(), mark_offset_in_bytes());
 47 }
 48 
 49 markOop  oopDesc::mark_raw()  const {
 50   return _mark;
 51 }
 52 
 53 markOop* oopDesc::mark_addr_raw() const {
 54   return (markOop*) &amp;_mark;
 55 }
 56 
 57 void oopDesc::set_mark(volatile markOop m) {
 58   HeapAccess&lt;MO_VOLATILE&gt;::store_at(as_oop(), mark_offset_in_bytes(), m);
 59 }
 60 
 61 void oopDesc::set_mark_raw(volatile markOop m) {
 62   _mark = m;
 63 }
 64 
 65 void oopDesc::set_mark_raw(HeapWord* mem, markOop m) {
 66   *(markOop*)(((char*)mem) + mark_offset_in_bytes()) = m;
 67 }
 68 
 69 void oopDesc::release_set_mark(markOop m) {
 70   HeapAccess&lt;MO_RELEASE&gt;::store_at(as_oop(), mark_offset_in_bytes(), m);
 71 }
 72 
 73 markOop oopDesc::cas_set_mark(markOop new_mark, markOop old_mark) {
 74   return HeapAccess&lt;&gt;::atomic_cmpxchg_at(new_mark, as_oop(), mark_offset_in_bytes(), old_mark);
 75 }
 76 
 77 markOop oopDesc::cas_set_mark_raw(markOop new_mark, markOop old_mark, atomic_memory_order order) {
 78   return Atomic::cmpxchg(new_mark, &amp;_mark, old_mark, order);
 79 }
 80 
 81 void oopDesc::init_mark() {
 82   set_mark(markOopDesc::prototype_for_object(this));
 83 }
 84 
 85 void oopDesc::init_mark_raw() {
 86   set_mark_raw(markOopDesc::prototype_for_object(this));
 87 }
 88 
 89 Klass* oopDesc::klass() const {
 90   if (UseCompressedClassPointers) {
 91     return Klass::decode_klass_not_null(_metadata._compressed_klass);
 92   } else {
 93     return _metadata._klass;
 94   }
 95 }
 96 
 97 Klass* oopDesc::klass_or_null() const volatile {
 98   if (UseCompressedClassPointers) {
 99     return Klass::decode_klass(_metadata._compressed_klass);
100   } else {
101     return _metadata._klass;
102   }
103 }
104 
105 Klass* oopDesc::klass_or_null_acquire() const volatile {
106   if (UseCompressedClassPointers) {
107     // Workaround for non-const load_acquire parameter.
108     const volatile narrowKlass* addr = &amp;_metadata._compressed_klass;
109     volatile narrowKlass* xaddr = const_cast&lt;volatile narrowKlass*&gt;(addr);
110     return Klass::decode_klass(OrderAccess::load_acquire(xaddr));
111   } else {
112     return OrderAccess::load_acquire(&amp;_metadata._klass);
113   }
114 }
115 
116 Klass** oopDesc::klass_addr(HeapWord* mem) {
117   // Only used internally and with CMS and will not work with
118   // UseCompressedOops
119   assert(!UseCompressedClassPointers, &quot;only supported with uncompressed klass pointers&quot;);
120   ByteSize offset = byte_offset_of(oopDesc, _metadata._klass);
121   return (Klass**) (((char*)mem) + in_bytes(offset));
122 }
123 
124 narrowKlass* oopDesc::compressed_klass_addr(HeapWord* mem) {
125   assert(UseCompressedClassPointers, &quot;only called by compressed klass pointers&quot;);
126   ByteSize offset = byte_offset_of(oopDesc, _metadata._compressed_klass);
127   return (narrowKlass*) (((char*)mem) + in_bytes(offset));
128 }
129 
130 Klass** oopDesc::klass_addr() {
131   return klass_addr((HeapWord*)this);
132 }
133 
134 narrowKlass* oopDesc::compressed_klass_addr() {
135   return compressed_klass_addr((HeapWord*)this);
136 }
137 
138 #define CHECK_SET_KLASS(k)                                                \
139   do {                                                                    \
140     assert(Universe::is_bootstrapping() || k != NULL, &quot;NULL Klass&quot;);      \
141     assert(Universe::is_bootstrapping() || k-&gt;is_klass(), &quot;not a Klass&quot;); \
142   } while (0)
143 
144 void oopDesc::set_klass(Klass* k) {
145   CHECK_SET_KLASS(k);
146   if (UseCompressedClassPointers) {
147     *compressed_klass_addr() = Klass::encode_klass_not_null(k);
148   } else {
149     *klass_addr() = k;
150   }
151 }
152 
153 void oopDesc::release_set_klass(HeapWord* mem, Klass* klass) {
154   CHECK_SET_KLASS(klass);
155   if (UseCompressedClassPointers) {
156     OrderAccess::release_store(compressed_klass_addr(mem),
157                                Klass::encode_klass_not_null(klass));
158   } else {
159     OrderAccess::release_store(klass_addr(mem), klass);
160   }
161 }
162 
163 #undef CHECK_SET_KLASS
164 
165 int oopDesc::klass_gap() const {
166   return *(int*)(((intptr_t)this) + klass_gap_offset_in_bytes());
167 }
168 
169 void oopDesc::set_klass_gap(HeapWord* mem, int v) {
170   if (UseCompressedClassPointers) {
171     *(int*)(((char*)mem) + klass_gap_offset_in_bytes()) = v;
172   }
173 }
174 
175 void oopDesc::set_klass_gap(int v) {
176   set_klass_gap((HeapWord*)this, v);
177 }
178 
179 void oopDesc::set_klass_to_list_ptr(oop k) {
180   // This is only to be used during GC, for from-space objects, so no
181   // barrier is needed.
182   if (UseCompressedClassPointers) {
183     _metadata._compressed_klass = (narrowKlass)CompressedOops::encode(k);  // may be null (parnew overflow handling)
184   } else {
185     _metadata._klass = (Klass*)(address)k;
186   }
187 }
188 
189 oop oopDesc::list_ptr_from_klass() {
190   // This is only to be used during GC, for from-space objects.
191   if (UseCompressedClassPointers) {
192     return CompressedOops::decode((narrowOop)_metadata._compressed_klass);
193   } else {
194     // Special case for GC
195     return (oop)(address)_metadata._klass;
196   }
197 }
198 
199 bool oopDesc::is_a(Klass* k) const {
200   return klass()-&gt;is_subtype_of(k);
201 }
202 
203 int oopDesc::size()  {
204   return size_given_klass(klass());
205 }
206 
207 int oopDesc::size_given_klass(Klass* klass)  {
208   int lh = klass-&gt;layout_helper();
209   int s;
210 
211   // lh is now a value computed at class initialization that may hint
212   // at the size.  For instances, this is positive and equal to the
213   // size.  For arrays, this is negative and provides log2 of the
214   // array element size.  For other oops, it is zero and thus requires
215   // a virtual call.
216   //
217   // We go to all this trouble because the size computation is at the
218   // heart of phase 2 of mark-compaction, and called for every object,
219   // alive or dead.  So the speed here is equal in importance to the
220   // speed of allocation.
221 
222   if (lh &gt; Klass::_lh_neutral_value) {
223     if (!Klass::layout_helper_needs_slow_path(lh)) {
224       s = lh &gt;&gt; LogHeapWordSize;  // deliver size scaled by wordSize
225     } else {
226       s = klass-&gt;oop_size(this);
227     }
228   } else if (lh &lt;= Klass::_lh_neutral_value) {
229     // The most common case is instances; fall through if so.
230     if (lh &lt; Klass::_lh_neutral_value) {
231       // Second most common case is arrays.  We have to fetch the
232       // length of the array, shift (multiply) it appropriately,
233       // up to wordSize, add the header, and align to object size.
234       size_t size_in_bytes;
235       size_t array_length = (size_t) ((arrayOop)this)-&gt;length();
236       size_in_bytes = array_length &lt;&lt; Klass::layout_helper_log2_element_size(lh);
237       size_in_bytes += Klass::layout_helper_header_size(lh);
238 
239       // This code could be simplified, but by keeping array_header_in_bytes
240       // in units of bytes and doing it this way we can round up just once,
241       // skipping the intermediate round to HeapWordSize.
242       s = (int)(align_up(size_in_bytes, MinObjAlignmentInBytes) / HeapWordSize);
243 
244       // ParNew (used by CMS), UseParallelGC and UseG1GC can change the length field
245       // of an &quot;old copy&quot; of an object array in the young gen so it indicates
246       // the grey portion of an already copied array. This will cause the first
247       // disjunct below to fail if the two comparands are computed across such
248       // a concurrent change.
249       // ParNew also runs with promotion labs (which look like int
250       // filler arrays) which are subject to changing their declared size
251       // when finally retiring a PLAB; this also can cause the first disjunct
252       // to fail for another worker thread that is concurrently walking the block
253       // offset table. Both these invariant failures are benign for their
254       // current uses; we relax the assertion checking to cover these two cases below:
255       //     is_objArray() &amp;&amp; is_forwarded()   // covers first scenario above
256       //  || is_typeArray()                    // covers second scenario above
257       // If and when UseParallelGC uses the same obj array oop stealing/chunking
258       // technique, we will need to suitably modify the assertion.
259       assert((s == klass-&gt;oop_size(this)) ||
260              (Universe::heap()-&gt;is_gc_active() &amp;&amp;
261               ((is_typeArray() &amp;&amp; UseConcMarkSweepGC) ||
262                (is_objArray()  &amp;&amp; is_forwarded() &amp;&amp; (UseConcMarkSweepGC || UseParallelGC || UseG1GC)))),
263              &quot;wrong array object size&quot;);
264     } else {
265       // Must be zero, so bite the bullet and take the virtual call.
266       s = klass-&gt;oop_size(this);
267     }
268   }
269 
270   assert(s &gt; 0, &quot;Oop size must be greater than zero, not %d&quot;, s);
271   assert(is_object_aligned(s), &quot;Oop size is not properly aligned: %d&quot;, s);
272   return s;
273 }
274 
275 bool oopDesc::is_instance()  const { return klass()-&gt;is_instance_klass();  }
276 bool oopDesc::is_array()     const { return klass()-&gt;is_array_klass();     }
277 bool oopDesc::is_objArray()  const { return klass()-&gt;is_objArray_klass();  }
278 bool oopDesc::is_typeArray() const { return klass()-&gt;is_typeArray_klass(); }
279 
280 void*    oopDesc::field_addr_raw(int offset)     const { return reinterpret_cast&lt;void*&gt;(cast_from_oop&lt;intptr_t&gt;(as_oop()) + offset); }
281 void*    oopDesc::field_addr(int offset)         const { return Access&lt;&gt;::resolve(as_oop())-&gt;field_addr_raw(offset); }
282 
283 template &lt;class T&gt;
284 T*       oopDesc::obj_field_addr_raw(int offset) const { return (T*) field_addr_raw(offset); }
285 
286 template &lt;typename T&gt;
287 size_t   oopDesc::field_offset(T* p) const { return pointer_delta((void*)p, (void*)this, 1); }
288 
289 template &lt;DecoratorSet decorators&gt;
290 inline oop  oopDesc::obj_field_access(int offset) const             { return HeapAccess&lt;decorators&gt;::oop_load_at(as_oop(), offset); }
291 inline oop  oopDesc::obj_field(int offset) const                    { return HeapAccess&lt;&gt;::oop_load_at(as_oop(), offset);  }
292 
293 inline void oopDesc::obj_field_put(int offset, oop value)           { HeapAccess&lt;&gt;::oop_store_at(as_oop(), offset, value); }
294 
295 inline jbyte oopDesc::byte_field(int offset) const                  { return HeapAccess&lt;&gt;::load_at(as_oop(), offset);  }
296 inline void  oopDesc::byte_field_put(int offset, jbyte value)       { HeapAccess&lt;&gt;::store_at(as_oop(), offset, value); }
297 
298 inline jchar oopDesc::char_field(int offset) const                  { return HeapAccess&lt;&gt;::load_at(as_oop(), offset);  }
299 inline void  oopDesc::char_field_put(int offset, jchar value)       { HeapAccess&lt;&gt;::store_at(as_oop(), offset, value); }
300 
301 inline jboolean oopDesc::bool_field(int offset) const               { return HeapAccess&lt;&gt;::load_at(as_oop(), offset);                }
302 inline void     oopDesc::bool_field_put(int offset, jboolean value) { HeapAccess&lt;&gt;::store_at(as_oop(), offset, jboolean(value &amp; 1)); }
303 
304 inline jshort oopDesc::short_field(int offset) const                { return HeapAccess&lt;&gt;::load_at(as_oop(), offset);  }
305 inline void   oopDesc::short_field_put(int offset, jshort value)    { HeapAccess&lt;&gt;::store_at(as_oop(), offset, value); }
306 
307 inline jint oopDesc::int_field(int offset) const                    { return HeapAccess&lt;&gt;::load_at(as_oop(), offset);  }
308 inline jint oopDesc::int_field_raw(int offset) const                { return RawAccess&lt;&gt;::load_at(as_oop(), offset);   }
309 inline void oopDesc::int_field_put(int offset, jint value)          { HeapAccess&lt;&gt;::store_at(as_oop(), offset, value); }
310 
311 inline jlong oopDesc::long_field(int offset) const                  { return HeapAccess&lt;&gt;::load_at(as_oop(), offset);  }
312 inline void  oopDesc::long_field_put(int offset, jlong value)       { HeapAccess&lt;&gt;::store_at(as_oop(), offset, value); }
313 
314 inline jfloat oopDesc::float_field(int offset) const                { return HeapAccess&lt;&gt;::load_at(as_oop(), offset);  }
315 inline void   oopDesc::float_field_put(int offset, jfloat value)    { HeapAccess&lt;&gt;::store_at(as_oop(), offset, value); }
316 
317 inline jdouble oopDesc::double_field(int offset) const              { return HeapAccess&lt;&gt;::load_at(as_oop(), offset);  }
318 inline void    oopDesc::double_field_put(int offset, jdouble value) { HeapAccess&lt;&gt;::store_at(as_oop(), offset, value); }
319 
320 bool oopDesc::is_locked() const {
321   return mark()-&gt;is_locked();
322 }
323 
324 bool oopDesc::is_unlocked() const {
325   return mark()-&gt;is_unlocked();
326 }
327 
328 bool oopDesc::has_bias_pattern() const {
329   return mark()-&gt;has_bias_pattern();
330 }
331 
332 bool oopDesc::has_bias_pattern_raw() const {
333   return mark_raw()-&gt;has_bias_pattern();
334 }
335 
336 // Used only for markSweep, scavenging
337 bool oopDesc::is_gc_marked() const {
338   return mark_raw()-&gt;is_marked();
339 }
340 
341 // Used by scavengers
342 bool oopDesc::is_forwarded() const {
343   // The extra heap check is needed since the obj might be locked, in which case the
344   // mark would point to a stack location and have the sentinel bit cleared
345   return mark_raw()-&gt;is_marked();
346 }
347 
348 // Used by scavengers
349 void oopDesc::forward_to(oop p) {
350   assert(check_obj_alignment(p),
351          &quot;forwarding to something not aligned&quot;);
352   assert(Universe::heap()-&gt;is_in_reserved(p),
353          &quot;forwarding to something not in heap&quot;);
354   assert(!is_archived_object(oop(this)) &amp;&amp;
355          !is_archived_object(p),
356          &quot;forwarding archive object&quot;);
357   markOop m = markOopDesc::encode_pointer_as_mark(p);
358   assert(m-&gt;decode_pointer() == p, &quot;encoding must be reversable&quot;);
359   set_mark_raw(m);
360 }
361 
362 // Used by parallel scavengers
363 bool oopDesc::cas_forward_to(oop p, markOop compare, atomic_memory_order order) {
364   assert(check_obj_alignment(p),
365          &quot;forwarding to something not aligned&quot;);
366   assert(Universe::heap()-&gt;is_in_reserved(p),
367          &quot;forwarding to something not in heap&quot;);
368   markOop m = markOopDesc::encode_pointer_as_mark(p);
369   assert(m-&gt;decode_pointer() == p, &quot;encoding must be reversable&quot;);
370   return cas_set_mark_raw(m, compare, order) == compare;
371 }
372 
373 oop oopDesc::forward_to_atomic(oop p, markOop compare, atomic_memory_order order) {
374   // CMS forwards some non-heap value into the mark oop to reserve oops during
375   // promotion, so the next two asserts do not hold.
376   assert(UseConcMarkSweepGC || check_obj_alignment(p),
377          &quot;forwarding to something not aligned&quot;);
378   assert(UseConcMarkSweepGC || Universe::heap()-&gt;is_in_reserved(p),
379          &quot;forwarding to something not in heap&quot;);
380   markOop m = markOopDesc::encode_pointer_as_mark(p);
381   assert(m-&gt;decode_pointer() == p, &quot;encoding must be reversable&quot;);
382   markOop old_mark = cas_set_mark_raw(m, compare, order);
383   if (old_mark == compare) {
384     return NULL;
385   } else {
386     return (oop)old_mark-&gt;decode_pointer();
387   }
388 }
389 
390 // Note that the forwardee is not the same thing as the displaced_mark.
391 // The forwardee is used when copying during scavenge and mark-sweep.
392 // It does need to clear the low two locking- and GC-related bits.
393 oop oopDesc::forwardee() const {
394   return (oop) mark_raw()-&gt;decode_pointer();
395 }
396 
397 // Note that the forwardee is not the same thing as the displaced_mark.
398 // The forwardee is used when copying during scavenge and mark-sweep.
399 // It does need to clear the low two locking- and GC-related bits.
400 oop oopDesc::forwardee_acquire() const {
401   markOop m = OrderAccess::load_acquire(&amp;_mark);
402   return (oop) m-&gt;decode_pointer();
403 }
404 
405 // The following method needs to be MT safe.
406 uint oopDesc::age() const {
407   assert(!is_forwarded(), &quot;Attempt to read age from forwarded mark&quot;);
408   if (has_displaced_mark_raw()) {
409     return displaced_mark_raw()-&gt;age();
410   } else {
411     return mark_raw()-&gt;age();
412   }
413 }
414 
415 void oopDesc::incr_age() {
416   assert(!is_forwarded(), &quot;Attempt to increment age of forwarded mark&quot;);
417   if (has_displaced_mark_raw()) {
418     set_displaced_mark_raw(displaced_mark_raw()-&gt;incr_age());
419   } else {
420     set_mark_raw(mark_raw()-&gt;incr_age());
421   }
422 }
423 
424 template &lt;typename OopClosureType&gt;
425 void oopDesc::oop_iterate(OopClosureType* cl) {
426   OopIteratorClosureDispatch::oop_oop_iterate(cl, this, klass());
427 }
428 
429 template &lt;typename OopClosureType&gt;
430 void oopDesc::oop_iterate(OopClosureType* cl, MemRegion mr) {
431   OopIteratorClosureDispatch::oop_oop_iterate(cl, this, klass(), mr);
432 }
433 
434 template &lt;typename OopClosureType&gt;
435 int oopDesc::oop_iterate_size(OopClosureType* cl) {
436   Klass* k = klass();
437   int size = size_given_klass(k);
438   OopIteratorClosureDispatch::oop_oop_iterate(cl, this, k);
439   return size;
440 }
441 
442 template &lt;typename OopClosureType&gt;
443 int oopDesc::oop_iterate_size(OopClosureType* cl, MemRegion mr) {
444   Klass* k = klass();
445   int size = size_given_klass(k);
446   OopIteratorClosureDispatch::oop_oop_iterate(cl, this, k, mr);
447   return size;
448 }
449 
450 template &lt;typename OopClosureType&gt;
451 void oopDesc::oop_iterate_backwards(OopClosureType* cl) {
452   OopIteratorClosureDispatch::oop_oop_iterate_backwards(cl, this, klass());
453 }
454 
455 bool oopDesc::is_instanceof_or_null(oop obj, Klass* klass) {
456   return obj == NULL || obj-&gt;klass()-&gt;is_subtype_of(klass);
457 }
458 
459 intptr_t oopDesc::identity_hash() {
460   // Fast case; if the object is unlocked and the hash value is set, no locking is needed
461   // Note: The mark must be read into local variable to avoid concurrent updates.
462   markOop mrk = mark();
463   if (mrk-&gt;is_unlocked() &amp;&amp; !mrk-&gt;has_no_hash()) {
464     return mrk-&gt;hash();
465   } else if (mrk-&gt;is_marked()) {
466     return mrk-&gt;hash();
467   } else {
468     return slow_identity_hash();
469   }
470 }
471 
472 bool oopDesc::has_displaced_mark_raw() const {
473   return mark_raw()-&gt;has_displaced_mark_helper();
474 }
475 
476 markOop oopDesc::displaced_mark_raw() const {
477   return mark_raw()-&gt;displaced_mark_helper();
478 }
479 
480 void oopDesc::set_displaced_mark_raw(markOop m) {
481   mark_raw()-&gt;set_displaced_mark_helper(m);
482 }
483 
484 #endif // SHARE_OOPS_OOP_INLINE_HPP
    </pre>
  </body>
</html>