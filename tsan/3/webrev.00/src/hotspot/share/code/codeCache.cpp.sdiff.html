<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/code/codeCache.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="codeBlob.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="codeCache.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/code/codeCache.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;aot/aotLoader.hpp&quot;
  27 #include &quot;code/codeBlob.hpp&quot;
  28 #include &quot;code/codeCache.hpp&quot;
  29 #include &quot;code/codeHeapState.hpp&quot;
  30 #include &quot;code/compiledIC.hpp&quot;
  31 #include &quot;code/dependencies.hpp&quot;
  32 #include &quot;code/dependencyContext.hpp&quot;
  33 #include &quot;code/icBuffer.hpp&quot;
  34 #include &quot;code/nmethod.hpp&quot;
  35 #include &quot;code/pcDesc.hpp&quot;

  36 #include &quot;compiler/compileBroker.hpp&quot;
  37 #include &quot;jfr/jfrEvents.hpp&quot;
  38 #include &quot;logging/log.hpp&quot;
  39 #include &quot;logging/logStream.hpp&quot;
  40 #include &quot;memory/allocation.inline.hpp&quot;
  41 #include &quot;memory/iterator.hpp&quot;
  42 #include &quot;memory/resourceArea.hpp&quot;

  43 #include &quot;oops/method.inline.hpp&quot;
  44 #include &quot;oops/objArrayOop.hpp&quot;
  45 #include &quot;oops/oop.inline.hpp&quot;
  46 #include &quot;oops/verifyOopClosure.hpp&quot;
  47 #include &quot;runtime/arguments.hpp&quot;
<span class="line-modified">  48 #include &quot;runtime/compilationPolicy.hpp&quot;</span>
  49 #include &quot;runtime/deoptimization.hpp&quot;
  50 #include &quot;runtime/handles.inline.hpp&quot;
  51 #include &quot;runtime/icache.hpp&quot;
  52 #include &quot;runtime/java.hpp&quot;
  53 #include &quot;runtime/mutexLocker.hpp&quot;
  54 #include &quot;runtime/safepointVerifiers.hpp&quot;
  55 #include &quot;runtime/sweeper.hpp&quot;
  56 #include &quot;runtime/vmThread.hpp&quot;
  57 #include &quot;services/memoryService.hpp&quot;
  58 #include &quot;utilities/align.hpp&quot;
  59 #include &quot;utilities/vmError.hpp&quot;
  60 #include &quot;utilities/xmlstream.hpp&quot;
  61 #ifdef COMPILER1
  62 #include &quot;c1/c1_Compilation.hpp&quot;
  63 #include &quot;c1/c1_Compiler.hpp&quot;
  64 #endif
  65 #ifdef COMPILER2
  66 #include &quot;opto/c2compiler.hpp&quot;
  67 #include &quot;opto/compile.hpp&quot;
  68 #include &quot;opto/node.hpp&quot;
</pre>
<hr />
<pre>
 265   // We do not need the profiled CodeHeap, use all space for the non-profiled CodeHeap
 266   if (!heap_available(CodeBlobType::MethodProfiled)) {
 267     non_profiled_size += profiled_size;
 268     profiled_size = 0;
 269   }
 270   // We do not need the non-profiled CodeHeap, use all space for the non-nmethod CodeHeap
 271   if (!heap_available(CodeBlobType::MethodNonProfiled)) {
 272     non_nmethod_size += non_profiled_size;
 273     non_profiled_size = 0;
 274   }
 275   // Make sure we have enough space for VM internal code
 276   uint min_code_cache_size = CodeCacheMinimumUseSpace DEBUG_ONLY(* 3);
 277   if (non_nmethod_size &lt; min_code_cache_size) {
 278     vm_exit_during_initialization(err_msg(
 279         &quot;Not enough space in non-nmethod code heap to run VM: &quot; SIZE_FORMAT &quot;K &lt; &quot; SIZE_FORMAT &quot;K&quot;,
 280         non_nmethod_size/K, min_code_cache_size/K));
 281   }
 282 
 283   // Verify sizes and update flag values
 284   assert(non_profiled_size + profiled_size + non_nmethod_size == cache_size, &quot;Invalid code heap sizes&quot;);
<span class="line-modified"> 285   FLAG_SET_ERGO(uintx, NonNMethodCodeHeapSize, non_nmethod_size);</span>
<span class="line-modified"> 286   FLAG_SET_ERGO(uintx, ProfiledCodeHeapSize, profiled_size);</span>
<span class="line-modified"> 287   FLAG_SET_ERGO(uintx, NonProfiledCodeHeapSize, non_profiled_size);</span>
 288 
 289   // If large page support is enabled, align code heaps according to large
 290   // page size to make sure that code cache is covered by large pages.
 291   const size_t alignment = MAX2(page_size(false, 8), (size_t) os::vm_allocation_granularity());
 292   non_nmethod_size = align_up(non_nmethod_size, alignment);
 293   profiled_size    = align_down(profiled_size, alignment);
 294 
 295   // Reserve one continuous chunk of memory for CodeHeaps and split it into
 296   // parts for the individual heaps. The memory layout looks like this:
 297   // ---------- high -----------
 298   //    Non-profiled nmethods
 299   //      Profiled nmethods
 300   //         Non-nmethods
 301   // ---------- low ------------
 302   ReservedCodeSpace rs = reserve_heap_memory(cache_size);
 303   ReservedSpace non_method_space    = rs.first_part(non_nmethod_size);
 304   ReservedSpace rest                = rs.last_part(non_nmethod_size);
 305   ReservedSpace profiled_space      = rest.first_part(profiled_size);
 306   ReservedSpace non_profiled_space  = rest.last_part(profiled_size);
 307 
</pre>
<hr />
<pre>
 514           type = CodeBlobType::MethodNonProfiled;
 515           break;
 516         case CodeBlobType::MethodNonProfiled:
 517           type = CodeBlobType::MethodProfiled;
 518           break;
 519         case CodeBlobType::MethodProfiled:
 520           // Avoid loop if we already tried that code heap
 521           if (type == orig_code_blob_type) {
 522             type = CodeBlobType::MethodNonProfiled;
 523           }
 524           break;
 525         }
 526         if (type != code_blob_type &amp;&amp; type != orig_code_blob_type &amp;&amp; heap_available(type)) {
 527           if (PrintCodeCacheExtension) {
 528             tty-&gt;print_cr(&quot;Extension of %s failed. Trying to allocate in %s.&quot;,
 529                           heap-&gt;name(), get_code_heap(type)-&gt;name());
 530           }
 531           return allocate(size, type, orig_code_blob_type);
 532         }
 533       }
<span class="line-modified"> 534       MutexUnlockerEx mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
 535       CompileBroker::handle_full_code_cache(orig_code_blob_type);
 536       return NULL;
 537     }
 538     if (PrintCodeCacheExtension) {
 539       ResourceMark rm;
 540       if (_nmethod_heaps-&gt;length() &gt;= 1) {
 541         tty-&gt;print(&quot;%s&quot;, heap-&gt;name());
 542       } else {
 543         tty-&gt;print(&quot;CodeCache&quot;);
 544       }
 545       tty-&gt;print_cr(&quot; extended to [&quot; INTPTR_FORMAT &quot;, &quot; INTPTR_FORMAT &quot;] (&quot; SSIZE_FORMAT &quot; bytes)&quot;,
 546                     (intptr_t)heap-&gt;low_boundary(), (intptr_t)heap-&gt;high(),
 547                     (address)heap-&gt;high() - (address)heap-&gt;low_boundary());
 548     }
 549   }
 550   print_trace(&quot;allocation&quot;, cb, size);
 551   return cb;
 552 }
 553 
 554 void CodeCache::free(CodeBlob* cb) {
</pre>
<hr />
<pre>
 650   return (nmethod*)cb;
 651 }
 652 
 653 void CodeCache::blobs_do(void f(CodeBlob* nm)) {
 654   assert_locked_or_safepoint(CodeCache_lock);
 655   FOR_ALL_HEAPS(heap) {
 656     FOR_ALL_BLOBS(cb, *heap) {
 657       f(cb);
 658     }
 659   }
 660 }
 661 
 662 void CodeCache::nmethods_do(void f(nmethod* nm)) {
 663   assert_locked_or_safepoint(CodeCache_lock);
 664   NMethodIterator iter(NMethodIterator::all_blobs);
 665   while(iter.next()) {
 666     f(iter.method());
 667   }
 668 }
 669 
<span class="line-modified"> 670 void CodeCache::metadata_do(void f(Metadata* m)) {</span>
 671   assert_locked_or_safepoint(CodeCache_lock);
 672   NMethodIterator iter(NMethodIterator::only_alive_and_not_unloading);
 673   while(iter.next()) {
 674     iter.method()-&gt;metadata_do(f);
 675   }
 676   AOTLoader::metadata_do(f);
 677 }
 678 
 679 int CodeCache::alignment_unit() {
 680   return (int)_heaps-&gt;first()-&gt;alignment_unit();
 681 }
 682 
 683 int CodeCache::alignment_offset() {
 684   return (int)_heaps-&gt;first()-&gt;alignment_offset();
 685 }
 686 
 687 // Mark nmethods for unloading if they contain otherwise unreachable oops.
 688 void CodeCache::do_unloading(BoolObjectClosure* is_alive, bool unloading_occurred) {
 689   assert_locked_or_safepoint(CodeCache_lock);
 690   UnloadingScope scope(is_alive);
</pre>
<hr />
<pre>
 731       CompiledMethod *nm = cb-&gt;as_compiled_method_or_null();
 732       if (nm != NULL) {
 733         count += nm-&gt;verify_icholder_relocations();
 734       }
 735     }
 736   }
 737   assert(count + InlineCacheBuffer::pending_icholder_count() + CompiledICHolder::live_not_claimed_count() ==
 738          CompiledICHolder::live_count(), &quot;must agree&quot;);
 739 #endif
 740 }
 741 
 742 // Defer freeing of concurrently cleaned ExceptionCache entries until
 743 // after a global handshake operation.
 744 void CodeCache::release_exception_cache(ExceptionCache* entry) {
 745   if (SafepointSynchronize::is_at_safepoint()) {
 746     delete entry;
 747   } else {
 748     for (;;) {
 749       ExceptionCache* purge_list_head = Atomic::load(&amp;_exception_cache_purge_list);
 750       entry-&gt;set_purge_list_next(purge_list_head);
<span class="line-modified"> 751       if (Atomic::cmpxchg(entry, &amp;_exception_cache_purge_list, purge_list_head) == purge_list_head) {</span>
 752         break;
 753       }
 754     }
 755   }
 756 }
 757 
 758 // Delete exception caches that have been concurrently unlinked,
 759 // followed by a global handshake operation.
 760 void CodeCache::purge_exception_caches() {
 761   ExceptionCache* curr = _exception_cache_purge_list;
 762   while (curr != NULL) {
 763     ExceptionCache* next = curr-&gt;purge_list_next();
 764     delete curr;
 765     curr = next;
 766   }
 767   _exception_cache_purge_list = NULL;
 768 }
 769 
 770 uint8_t CodeCache::_unloading_cycle = 1;
 771 
 772 void CodeCache::increment_unloading_cycle() {
<span class="line-modified"> 773   if (_unloading_cycle == 1) {</span>
<span class="line-modified"> 774     _unloading_cycle = 2;</span>
<span class="line-modified"> 775   } else {</span>

 776     _unloading_cycle = 1;
 777   }
 778 }
 779 
 780 CodeCache::UnloadingScope::UnloadingScope(BoolObjectClosure* is_alive)
 781   : _is_unloading_behaviour(is_alive)
 782 {

 783   IsUnloadingBehaviour::set_current(&amp;_is_unloading_behaviour);
 784   increment_unloading_cycle();
 785   DependencyContext::cleaning_start();
 786 }
 787 
 788 CodeCache::UnloadingScope::~UnloadingScope() {
<span class="line-modified"> 789   IsUnloadingBehaviour::set_current(NULL);</span>
 790   DependencyContext::cleaning_end();
 791 }
 792 
 793 void CodeCache::verify_oops() {
<span class="line-modified"> 794   MutexLockerEx mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
 795   VerifyOopClosure voc;
 796   NMethodIterator iter(NMethodIterator::only_alive_and_not_unloading);
 797   while(iter.next()) {
 798     nmethod* nm = iter.method();
 799     nm-&gt;oops_do(&amp;voc);
 800     nm-&gt;verify_oop_relocations();
 801   }
 802 }
 803 
 804 int CodeCache::blob_count(int code_blob_type) {
 805   CodeHeap* heap = get_code_heap(code_blob_type);
 806   return (heap != NULL) ? heap-&gt;blob_count() : 0;
 807 }
 808 
 809 int CodeCache::blob_count() {
 810   int count = 0;
 811   FOR_ALL_HEAPS(heap) {
 812     count += (*heap)-&gt;blob_count();
 813   }
 814   return count;
</pre>
<hr />
<pre>
 922 }
 923 
 924 void icache_init();
 925 
 926 void CodeCache::initialize() {
 927   assert(CodeCacheSegmentSize &gt;= (uintx)CodeEntryAlignment, &quot;CodeCacheSegmentSize must be large enough to align entry points&quot;);
 928 #ifdef COMPILER2
 929   assert(CodeCacheSegmentSize &gt;= (uintx)OptoLoopAlignment,  &quot;CodeCacheSegmentSize must be large enough to align inner loops&quot;);
 930 #endif
 931   assert(CodeCacheSegmentSize &gt;= sizeof(jdouble),    &quot;CodeCacheSegmentSize must be large enough to align constants&quot;);
 932   // This was originally just a check of the alignment, causing failure, instead, round
 933   // the code cache to the page size.  In particular, Solaris is moving to a larger
 934   // default page size.
 935   CodeCacheExpansionSize = align_up(CodeCacheExpansionSize, os::vm_page_size());
 936 
 937   if (SegmentedCodeCache) {
 938     // Use multiple code heaps
 939     initialize_heaps();
 940   } else {
 941     // Use a single code heap
<span class="line-modified"> 942     FLAG_SET_ERGO(uintx, NonNMethodCodeHeapSize, 0);</span>
<span class="line-modified"> 943     FLAG_SET_ERGO(uintx, ProfiledCodeHeapSize, 0);</span>
<span class="line-modified"> 944     FLAG_SET_ERGO(uintx, NonProfiledCodeHeapSize, 0);</span>
 945     ReservedCodeSpace rs = reserve_heap_memory(ReservedCodeCacheSize);
 946     add_heap(rs, &quot;CodeCache&quot;, CodeBlobType::All);
 947   }
 948 
 949   // Initialize ICache flush mechanism
 950   // This service is needed for os::register_code_area
 951   icache_init();
 952 
 953   // Give OS a chance to register generated code area.
 954   // This is used on Windows 64 bit platforms to register
 955   // Structured Exception Handlers for our generated code.
 956   os::register_code_area((char*)low_bound(), (char*)high_bound());
 957 }
 958 
 959 void codeCache_init() {
 960   CodeCache::initialize();
 961   // Load AOT libraries and add AOT code heaps.
 962   AOTLoader::initialize();
 963 }
 964 
</pre>
<hr />
<pre>
 971 void CodeCache::clear_inline_caches() {
 972   assert_locked_or_safepoint(CodeCache_lock);
 973   CompiledMethodIterator iter(CompiledMethodIterator::only_alive_and_not_unloading);
 974   while(iter.next()) {
 975     iter.method()-&gt;clear_inline_caches();
 976   }
 977 }
 978 
 979 void CodeCache::cleanup_inline_caches() {
 980   assert_locked_or_safepoint(CodeCache_lock);
 981   NMethodIterator iter(NMethodIterator::only_alive_and_not_unloading);
 982   while(iter.next()) {
 983     iter.method()-&gt;cleanup_inline_caches(/*clean_all=*/true);
 984   }
 985 }
 986 
 987 // Keeps track of time spent for checking dependencies
 988 NOT_PRODUCT(static elapsedTimer dependentCheckTime;)
 989 
 990 int CodeCache::mark_for_deoptimization(KlassDepChange&amp; changes) {
<span class="line-modified"> 991   MutexLockerEx mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
 992   int number_of_marked_CodeBlobs = 0;
 993 
 994   // search the hierarchy looking for nmethods which are affected by the loading of this class
 995 
 996   // then search the interfaces this class implements looking for nmethods
 997   // which might be dependent of the fact that an interface only had one
 998   // implementor.
 999   // nmethod::check_all_dependencies works only correctly, if no safepoint
1000   // can happen
1001   NoSafepointVerifier nsv;
1002   for (DepChange::ContextStream str(changes, nsv); str.next(); ) {
1003     Klass* d = str.klass();
1004     number_of_marked_CodeBlobs += InstanceKlass::cast(d)-&gt;mark_dependent_nmethods(changes);
1005   }
1006 
1007 #ifndef PRODUCT
1008   if (VerifyDependencies) {
1009     // Object pointers are used as unique identifiers for dependency arguments. This
1010     // is only possible if no safepoint, i.e., GC occurs during the verification code.
1011     dependentCheckTime.start();
</pre>
<hr />
<pre>
1015 #endif
1016 
1017   return number_of_marked_CodeBlobs;
1018 }
1019 
1020 CompiledMethod* CodeCache::find_compiled(void* start) {
1021   CodeBlob *cb = find_blob(start);
1022   assert(cb == NULL || cb-&gt;is_compiled(), &quot;did not find an compiled_method&quot;);
1023   return (CompiledMethod*)cb;
1024 }
1025 
1026 bool CodeCache::is_far_target(address target) {
1027 #if INCLUDE_AOT
1028   return NativeCall::is_far_call(_low_bound,  target) ||
1029          NativeCall::is_far_call(_high_bound, target);
1030 #else
1031   return false;
1032 #endif
1033 }
1034 
<span class="line-modified">1035 // Just marks the methods in this class as needing deoptimization</span>
<span class="line-modified">1036 void CodeCache::mark_for_evol_deoptimization(InstanceKlass* dependee) {</span>
<span class="line-modified">1037   MutexLockerEx mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>

1038 
<span class="line-modified">1039   // Deoptimize all methods of the evolving class itself</span>
<span class="line-modified">1040   Array&lt;Method*&gt;* old_methods = dependee-&gt;methods();</span>
<span class="line-modified">1041   for (int i = 0; i &lt; old_methods-&gt;length(); i++) {</span>
<span class="line-modified">1042     ResourceMark rm;</span>
<span class="line-modified">1043     Method* old_method = old_methods-&gt;at(i);</span>
<span class="line-modified">1044     CompiledMethod* nm = old_method-&gt;code();</span>
<span class="line-modified">1045     if (nm != NULL) {</span>
<span class="line-modified">1046       nm-&gt;mark_for_deoptimization();</span>




























1047     }
1048   }






1049 
1050   // Mark dependent AOT nmethods, which are only found via the class redefined.


1051   AOTLoader::mark_evol_dependent_methods(dependee);
1052 }
1053 

1054 // Walk compiled methods and mark dependent methods for deoptimization.
1055 int CodeCache::mark_dependents_for_evol_deoptimization() {





1056   int number_of_marked_CodeBlobs = 0;
1057   CompiledMethodIterator iter(CompiledMethodIterator::only_alive_and_not_unloading);
1058   while(iter.next()) {
1059     CompiledMethod* nm = iter.method();
<span class="line-modified">1060     if (nm-&gt;is_marked_for_deoptimization()) {</span>
<span class="line-modified">1061       // ...Already marked in the previous pass; count it here.</span>
<span class="line-modified">1062       // Also counts AOT compiled methods, already marked.</span>
<span class="line-modified">1063       number_of_marked_CodeBlobs++;</span>
<span class="line-removed">1064     } else if (nm-&gt;is_evol_dependent()) {</span>
<span class="line-removed">1065       ResourceMark rm;</span>
1066       nm-&gt;mark_for_deoptimization();

1067       number_of_marked_CodeBlobs++;
<span class="line-removed">1068     } else  {</span>
<span class="line-removed">1069       // flush caches in case they refer to a redefined Method*</span>
<span class="line-removed">1070       nm-&gt;clear_inline_caches();</span>
1071     }
1072   }
1073 
1074   // return total count of nmethods marked for deoptimization, if zero the caller
1075   // can skip deoptimization
1076   return number_of_marked_CodeBlobs;
1077 }
1078 
<span class="line-modified">1079 // Deoptimize all methods</span>
<span class="line-modified">1080 void CodeCache::mark_all_nmethods_for_deoptimization() {</span>
<span class="line-removed">1081   MutexLockerEx mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
1082   CompiledMethodIterator iter(CompiledMethodIterator::only_alive_and_not_unloading);
1083   while(iter.next()) {
1084     CompiledMethod* nm = iter.method();
1085     if (!nm-&gt;method()-&gt;is_method_handle_intrinsic()) {
1086       nm-&gt;mark_for_deoptimization();






























1087     }
1088   }
1089 }
1090 
1091 int CodeCache::mark_for_deoptimization(Method* dependee) {
<span class="line-modified">1092   MutexLockerEx mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
1093   int number_of_marked_CodeBlobs = 0;
1094 
1095   CompiledMethodIterator iter(CompiledMethodIterator::only_alive_and_not_unloading);
1096   while(iter.next()) {
1097     CompiledMethod* nm = iter.method();
1098     if (nm-&gt;is_dependent_on_method(dependee)) {
1099       ResourceMark rm;
1100       nm-&gt;mark_for_deoptimization();
1101       number_of_marked_CodeBlobs++;
1102     }
1103   }
1104 
1105   return number_of_marked_CodeBlobs;
1106 }
1107 
1108 void CodeCache::make_marked_nmethods_not_entrant() {
1109   assert_locked_or_safepoint(CodeCache_lock);
1110   CompiledMethodIterator iter(CompiledMethodIterator::only_alive_and_not_unloading);
1111   while(iter.next()) {
1112     CompiledMethod* nm = iter.method();
<span class="line-modified">1113     if (nm-&gt;is_marked_for_deoptimization() &amp;&amp; !nm-&gt;is_not_entrant()) {</span>
1114       nm-&gt;make_not_entrant();
1115     }
1116   }
1117 }
1118 
1119 // Flushes compiled methods dependent on dependee.
1120 void CodeCache::flush_dependents_on(InstanceKlass* dependee) {
1121   assert_lock_strong(Compile_lock);
1122 
1123   if (number_of_nmethods_with_dependencies() == 0) return;
1124 
<span class="line-removed">1125   // CodeCache can only be updated by a thread_in_VM and they will all be</span>
<span class="line-removed">1126   // stopped during the safepoint so CodeCache will be safe to update without</span>
<span class="line-removed">1127   // holding the CodeCache_lock.</span>
<span class="line-removed">1128 </span>
1129   KlassDepChange changes(dependee);
1130 
1131   // Compute the dependent nmethods
1132   if (mark_for_deoptimization(changes) &gt; 0) {
1133     // At least one nmethod has been marked for deoptimization
<span class="line-modified">1134     VM_Deoptimize op;</span>
<span class="line-removed">1135     VMThread::execute(&amp;op);</span>
1136   }
1137 }
1138 
<span class="line-removed">1139 // Flushes compiled methods dependent on redefined classes, that have already been</span>
<span class="line-removed">1140 // marked for deoptimization.</span>
<span class="line-removed">1141 void CodeCache::flush_evol_dependents() {</span>
<span class="line-removed">1142   // --- Compile_lock is not held. However we are at a safepoint.</span>
<span class="line-removed">1143   assert_locked_or_safepoint(Compile_lock);</span>
<span class="line-removed">1144 </span>
<span class="line-removed">1145   // CodeCache can only be updated by a thread_in_VM and they will all be</span>
<span class="line-removed">1146   // stopped during the safepoint so CodeCache will be safe to update without</span>
<span class="line-removed">1147   // holding the CodeCache_lock.</span>
<span class="line-removed">1148 </span>
<span class="line-removed">1149   // At least one nmethod has been marked for deoptimization</span>
<span class="line-removed">1150 </span>
<span class="line-removed">1151   // All this already happens inside a VM_Operation, so we&#39;ll do all the work here.</span>
<span class="line-removed">1152   // Stuff copied from VM_Deoptimize and modified slightly.</span>
<span class="line-removed">1153 </span>
<span class="line-removed">1154   // We do not want any GCs to happen while we are in the middle of this VM operation</span>
<span class="line-removed">1155   ResourceMark rm;</span>
<span class="line-removed">1156   DeoptimizationMarker dm;</span>
<span class="line-removed">1157 </span>
<span class="line-removed">1158   // Deoptimize all activations depending on marked nmethods</span>
<span class="line-removed">1159   Deoptimization::deoptimize_dependents();</span>
<span class="line-removed">1160 </span>
<span class="line-removed">1161   // Make the dependent methods not entrant</span>
<span class="line-removed">1162   make_marked_nmethods_not_entrant();</span>
<span class="line-removed">1163 }</span>
<span class="line-removed">1164 </span>
1165 // Flushes compiled methods dependent on dependee
1166 void CodeCache::flush_dependents_on_method(const methodHandle&amp; m_h) {
1167   // --- Compile_lock is not held. However we are at a safepoint.
1168   assert_locked_or_safepoint(Compile_lock);
1169 
<span class="line-removed">1170   // CodeCache can only be updated by a thread_in_VM and they will all be</span>
<span class="line-removed">1171   // stopped dring the safepoint so CodeCache will be safe to update without</span>
<span class="line-removed">1172   // holding the CodeCache_lock.</span>
<span class="line-removed">1173 </span>
1174   // Compute the dependent nmethods
1175   if (mark_for_deoptimization(m_h()) &gt; 0) {
<span class="line-modified">1176     // At least one nmethod has been marked for deoptimization</span>
<span class="line-removed">1177 </span>
<span class="line-removed">1178     // All this already happens inside a VM_Operation, so we&#39;ll do all the work here.</span>
<span class="line-removed">1179     // Stuff copied from VM_Deoptimize and modified slightly.</span>
<span class="line-removed">1180 </span>
<span class="line-removed">1181     // We do not want any GCs to happen while we are in the middle of this VM operation</span>
<span class="line-removed">1182     ResourceMark rm;</span>
<span class="line-removed">1183     DeoptimizationMarker dm;</span>
<span class="line-removed">1184 </span>
<span class="line-removed">1185     // Deoptimize all activations depending on marked nmethods</span>
<span class="line-removed">1186     Deoptimization::deoptimize_dependents();</span>
<span class="line-removed">1187 </span>
<span class="line-removed">1188     // Make the dependent methods not entrant</span>
<span class="line-removed">1189     make_marked_nmethods_not_entrant();</span>
1190   }
1191 }
1192 
1193 void CodeCache::verify() {
1194   assert_locked_or_safepoint(CodeCache_lock);
1195   FOR_ALL_HEAPS(heap) {
1196     (*heap)-&gt;verify();
1197     FOR_ALL_BLOBS(cb, *heap) {
1198       if (cb-&gt;is_alive()) {
1199         cb-&gt;verify();
1200       }
1201     }
1202   }
1203 }
1204 
1205 // A CodeHeap is full. Print out warning and report event.
1206 PRAGMA_DIAG_PUSH
1207 PRAGMA_FORMAT_NONLITERAL_IGNORED
1208 void CodeCache::report_codemem_full(int code_blob_type, bool print) {
1209   // Get nmethod heap for the given CodeBlobType and build CodeCacheFull event
</pre>
<hr />
<pre>
1222       const char *msg1 = msg1_stream.as_string();
1223       const char *msg2 = msg2_stream.as_string();
1224 
1225       log_warning(codecache)(&quot;%s&quot;, msg1);
1226       log_warning(codecache)(&quot;%s&quot;, msg2);
1227       warning(&quot;%s&quot;, msg1);
1228       warning(&quot;%s&quot;, msg2);
1229     } else {
1230       const char *msg1 = &quot;CodeCache is full. Compiler has been disabled.&quot;;
1231       const char *msg2 = &quot;Try increasing the code cache size using -XX:ReservedCodeCacheSize=&quot;;
1232 
1233       log_warning(codecache)(&quot;%s&quot;, msg1);
1234       log_warning(codecache)(&quot;%s&quot;, msg2);
1235       warning(&quot;%s&quot;, msg1);
1236       warning(&quot;%s&quot;, msg2);
1237     }
1238     ResourceMark rm;
1239     stringStream s;
1240     // Dump code cache into a buffer before locking the tty.
1241     {
<span class="line-modified">1242       MutexLockerEx mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
1243       print_summary(&amp;s);
1244     }
1245     {
1246       ttyLocker ttyl;
1247       tty-&gt;print(&quot;%s&quot;, s.as_string());
1248     }
1249 
1250     if (heap-&gt;full_count() == 0) {
1251       if (PrintCodeHeapAnalytics) {
<span class="line-modified">1252         CompileBroker::print_heapinfo(tty, &quot;all&quot;, &quot;4096&quot;); // details, may be a lot!</span>
1253       }
1254     }
1255   }
1256 
1257   heap-&gt;report_full();
1258 
1259   EventCodeCacheFull event;
1260   if (event.should_commit()) {
1261     event.set_codeBlobType((u1)code_blob_type);
1262     event.set_startAddress((u8)heap-&gt;low_boundary());
1263     event.set_commitedTopAddress((u8)heap-&gt;high());
1264     event.set_reservedTopAddress((u8)heap-&gt;high_boundary());
1265     event.set_entryCount(heap-&gt;blob_count());
1266     event.set_methodCount(heap-&gt;nmethod_count());
1267     event.set_adaptorCount(heap-&gt;adapter_count());
1268     event.set_unallocatedCapacity(heap-&gt;unallocated_capacity());
1269     event.set_fullCount(heap-&gt;full_count());
1270     event.commit();
1271   }
1272 }
</pre>
<hr />
<pre>
1490       full_count += get_codemem_full_count(heap-&gt;code_blob_type());
1491     }
1492   }
1493 
1494   if (detailed) {
1495     st-&gt;print_cr(&quot; total_blobs=&quot; UINT32_FORMAT &quot; nmethods=&quot; UINT32_FORMAT
1496                        &quot; adapters=&quot; UINT32_FORMAT,
1497                        blob_count(), nmethod_count(), adapter_count());
1498     st-&gt;print_cr(&quot; compilation: %s&quot;, CompileBroker::should_compile_new_jobs() ?
1499                  &quot;enabled&quot; : Arguments::mode() == Arguments::_int ?
1500                  &quot;disabled (interpreter mode)&quot; :
1501                  &quot;disabled (not enough contiguous free space left)&quot;);
1502     st-&gt;print_cr(&quot;              stopped_count=%d, restarted_count=%d&quot;,
1503                  CompileBroker::get_total_compiler_stopped_count(),
1504                  CompileBroker::get_total_compiler_restarted_count());
1505     st-&gt;print_cr(&quot; full_count=%d&quot;, full_count);
1506   }
1507 }
1508 
1509 void CodeCache::print_codelist(outputStream* st) {
<span class="line-modified">1510   MutexLockerEx mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
1511 
1512   CompiledMethodIterator iter(CompiledMethodIterator::only_alive_and_not_unloading);
1513   while (iter.next()) {
1514     CompiledMethod* cm = iter.method();
1515     ResourceMark rm;
1516     char* method_name = cm-&gt;method()-&gt;name_and_sig_as_C_string();
1517     st-&gt;print_cr(&quot;%d %d %d %s [&quot; INTPTR_FORMAT &quot;, &quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;]&quot;,
1518                  cm-&gt;compile_id(), cm-&gt;comp_level(), cm-&gt;get_state(),
1519                  method_name,
1520                  (intptr_t)cm-&gt;header_begin(), (intptr_t)cm-&gt;code_begin(), (intptr_t)cm-&gt;code_end());
1521   }
1522 }
1523 
1524 void CodeCache::print_layout(outputStream* st) {
<span class="line-modified">1525   MutexLockerEx mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
1526   ResourceMark rm;
1527   print_summary(st, true);
1528 }
1529 
1530 void CodeCache::log_state(outputStream* st) {
1531   st-&gt;print(&quot; total_blobs=&#39;&quot; UINT32_FORMAT &quot;&#39; nmethods=&#39;&quot; UINT32_FORMAT &quot;&#39;&quot;
1532             &quot; adapters=&#39;&quot; UINT32_FORMAT &quot;&#39; free_code_cache=&#39;&quot; SIZE_FORMAT &quot;&#39;&quot;,
1533             blob_count(), nmethod_count(), adapter_count(),
1534             unallocated_capacity());
1535 }
1536 
1537 //---&lt;  BEGIN  &gt;--- CodeHeap State Analytics.
1538 
<span class="line-modified">1539 void CodeCache::aggregate(outputStream *out, const char* granularity) {</span>
1540   FOR_ALL_ALLOCABLE_HEAPS(heap) {
1541     CodeHeapState::aggregate(out, (*heap), granularity);
1542   }
1543 }
1544 
1545 void CodeCache::discard(outputStream *out) {
1546   FOR_ALL_ALLOCABLE_HEAPS(heap) {
1547     CodeHeapState::discard(out, (*heap));
1548   }
1549 }
1550 
1551 void CodeCache::print_usedSpace(outputStream *out) {
1552   FOR_ALL_ALLOCABLE_HEAPS(heap) {
1553     CodeHeapState::print_usedSpace(out, (*heap));
1554   }
1555 }
1556 
1557 void CodeCache::print_freeSpace(outputStream *out) {
1558   FOR_ALL_ALLOCABLE_HEAPS(heap) {
1559     CodeHeapState::print_freeSpace(out, (*heap));
</pre>
</td>
<td>
<hr />
<pre>
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;aot/aotLoader.hpp&quot;
  27 #include &quot;code/codeBlob.hpp&quot;
  28 #include &quot;code/codeCache.hpp&quot;
  29 #include &quot;code/codeHeapState.hpp&quot;
  30 #include &quot;code/compiledIC.hpp&quot;
  31 #include &quot;code/dependencies.hpp&quot;
  32 #include &quot;code/dependencyContext.hpp&quot;
  33 #include &quot;code/icBuffer.hpp&quot;
  34 #include &quot;code/nmethod.hpp&quot;
  35 #include &quot;code/pcDesc.hpp&quot;
<span class="line-added">  36 #include &quot;compiler/compilationPolicy.hpp&quot;</span>
  37 #include &quot;compiler/compileBroker.hpp&quot;
  38 #include &quot;jfr/jfrEvents.hpp&quot;
  39 #include &quot;logging/log.hpp&quot;
  40 #include &quot;logging/logStream.hpp&quot;
  41 #include &quot;memory/allocation.inline.hpp&quot;
  42 #include &quot;memory/iterator.hpp&quot;
  43 #include &quot;memory/resourceArea.hpp&quot;
<span class="line-added">  44 #include &quot;memory/universe.hpp&quot;</span>
  45 #include &quot;oops/method.inline.hpp&quot;
  46 #include &quot;oops/objArrayOop.hpp&quot;
  47 #include &quot;oops/oop.inline.hpp&quot;
  48 #include &quot;oops/verifyOopClosure.hpp&quot;
  49 #include &quot;runtime/arguments.hpp&quot;
<span class="line-modified">  50 #include &quot;runtime/atomic.hpp&quot;</span>
  51 #include &quot;runtime/deoptimization.hpp&quot;
  52 #include &quot;runtime/handles.inline.hpp&quot;
  53 #include &quot;runtime/icache.hpp&quot;
  54 #include &quot;runtime/java.hpp&quot;
  55 #include &quot;runtime/mutexLocker.hpp&quot;
  56 #include &quot;runtime/safepointVerifiers.hpp&quot;
  57 #include &quot;runtime/sweeper.hpp&quot;
  58 #include &quot;runtime/vmThread.hpp&quot;
  59 #include &quot;services/memoryService.hpp&quot;
  60 #include &quot;utilities/align.hpp&quot;
  61 #include &quot;utilities/vmError.hpp&quot;
  62 #include &quot;utilities/xmlstream.hpp&quot;
  63 #ifdef COMPILER1
  64 #include &quot;c1/c1_Compilation.hpp&quot;
  65 #include &quot;c1/c1_Compiler.hpp&quot;
  66 #endif
  67 #ifdef COMPILER2
  68 #include &quot;opto/c2compiler.hpp&quot;
  69 #include &quot;opto/compile.hpp&quot;
  70 #include &quot;opto/node.hpp&quot;
</pre>
<hr />
<pre>
 267   // We do not need the profiled CodeHeap, use all space for the non-profiled CodeHeap
 268   if (!heap_available(CodeBlobType::MethodProfiled)) {
 269     non_profiled_size += profiled_size;
 270     profiled_size = 0;
 271   }
 272   // We do not need the non-profiled CodeHeap, use all space for the non-nmethod CodeHeap
 273   if (!heap_available(CodeBlobType::MethodNonProfiled)) {
 274     non_nmethod_size += non_profiled_size;
 275     non_profiled_size = 0;
 276   }
 277   // Make sure we have enough space for VM internal code
 278   uint min_code_cache_size = CodeCacheMinimumUseSpace DEBUG_ONLY(* 3);
 279   if (non_nmethod_size &lt; min_code_cache_size) {
 280     vm_exit_during_initialization(err_msg(
 281         &quot;Not enough space in non-nmethod code heap to run VM: &quot; SIZE_FORMAT &quot;K &lt; &quot; SIZE_FORMAT &quot;K&quot;,
 282         non_nmethod_size/K, min_code_cache_size/K));
 283   }
 284 
 285   // Verify sizes and update flag values
 286   assert(non_profiled_size + profiled_size + non_nmethod_size == cache_size, &quot;Invalid code heap sizes&quot;);
<span class="line-modified"> 287   FLAG_SET_ERGO(NonNMethodCodeHeapSize, non_nmethod_size);</span>
<span class="line-modified"> 288   FLAG_SET_ERGO(ProfiledCodeHeapSize, profiled_size);</span>
<span class="line-modified"> 289   FLAG_SET_ERGO(NonProfiledCodeHeapSize, non_profiled_size);</span>
 290 
 291   // If large page support is enabled, align code heaps according to large
 292   // page size to make sure that code cache is covered by large pages.
 293   const size_t alignment = MAX2(page_size(false, 8), (size_t) os::vm_allocation_granularity());
 294   non_nmethod_size = align_up(non_nmethod_size, alignment);
 295   profiled_size    = align_down(profiled_size, alignment);
 296 
 297   // Reserve one continuous chunk of memory for CodeHeaps and split it into
 298   // parts for the individual heaps. The memory layout looks like this:
 299   // ---------- high -----------
 300   //    Non-profiled nmethods
 301   //      Profiled nmethods
 302   //         Non-nmethods
 303   // ---------- low ------------
 304   ReservedCodeSpace rs = reserve_heap_memory(cache_size);
 305   ReservedSpace non_method_space    = rs.first_part(non_nmethod_size);
 306   ReservedSpace rest                = rs.last_part(non_nmethod_size);
 307   ReservedSpace profiled_space      = rest.first_part(profiled_size);
 308   ReservedSpace non_profiled_space  = rest.last_part(profiled_size);
 309 
</pre>
<hr />
<pre>
 516           type = CodeBlobType::MethodNonProfiled;
 517           break;
 518         case CodeBlobType::MethodNonProfiled:
 519           type = CodeBlobType::MethodProfiled;
 520           break;
 521         case CodeBlobType::MethodProfiled:
 522           // Avoid loop if we already tried that code heap
 523           if (type == orig_code_blob_type) {
 524             type = CodeBlobType::MethodNonProfiled;
 525           }
 526           break;
 527         }
 528         if (type != code_blob_type &amp;&amp; type != orig_code_blob_type &amp;&amp; heap_available(type)) {
 529           if (PrintCodeCacheExtension) {
 530             tty-&gt;print_cr(&quot;Extension of %s failed. Trying to allocate in %s.&quot;,
 531                           heap-&gt;name(), get_code_heap(type)-&gt;name());
 532           }
 533           return allocate(size, type, orig_code_blob_type);
 534         }
 535       }
<span class="line-modified"> 536       MutexUnlocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
 537       CompileBroker::handle_full_code_cache(orig_code_blob_type);
 538       return NULL;
 539     }
 540     if (PrintCodeCacheExtension) {
 541       ResourceMark rm;
 542       if (_nmethod_heaps-&gt;length() &gt;= 1) {
 543         tty-&gt;print(&quot;%s&quot;, heap-&gt;name());
 544       } else {
 545         tty-&gt;print(&quot;CodeCache&quot;);
 546       }
 547       tty-&gt;print_cr(&quot; extended to [&quot; INTPTR_FORMAT &quot;, &quot; INTPTR_FORMAT &quot;] (&quot; SSIZE_FORMAT &quot; bytes)&quot;,
 548                     (intptr_t)heap-&gt;low_boundary(), (intptr_t)heap-&gt;high(),
 549                     (address)heap-&gt;high() - (address)heap-&gt;low_boundary());
 550     }
 551   }
 552   print_trace(&quot;allocation&quot;, cb, size);
 553   return cb;
 554 }
 555 
 556 void CodeCache::free(CodeBlob* cb) {
</pre>
<hr />
<pre>
 652   return (nmethod*)cb;
 653 }
 654 
 655 void CodeCache::blobs_do(void f(CodeBlob* nm)) {
 656   assert_locked_or_safepoint(CodeCache_lock);
 657   FOR_ALL_HEAPS(heap) {
 658     FOR_ALL_BLOBS(cb, *heap) {
 659       f(cb);
 660     }
 661   }
 662 }
 663 
 664 void CodeCache::nmethods_do(void f(nmethod* nm)) {
 665   assert_locked_or_safepoint(CodeCache_lock);
 666   NMethodIterator iter(NMethodIterator::all_blobs);
 667   while(iter.next()) {
 668     f(iter.method());
 669   }
 670 }
 671 
<span class="line-modified"> 672 void CodeCache::metadata_do(MetadataClosure* f) {</span>
 673   assert_locked_or_safepoint(CodeCache_lock);
 674   NMethodIterator iter(NMethodIterator::only_alive_and_not_unloading);
 675   while(iter.next()) {
 676     iter.method()-&gt;metadata_do(f);
 677   }
 678   AOTLoader::metadata_do(f);
 679 }
 680 
 681 int CodeCache::alignment_unit() {
 682   return (int)_heaps-&gt;first()-&gt;alignment_unit();
 683 }
 684 
 685 int CodeCache::alignment_offset() {
 686   return (int)_heaps-&gt;first()-&gt;alignment_offset();
 687 }
 688 
 689 // Mark nmethods for unloading if they contain otherwise unreachable oops.
 690 void CodeCache::do_unloading(BoolObjectClosure* is_alive, bool unloading_occurred) {
 691   assert_locked_or_safepoint(CodeCache_lock);
 692   UnloadingScope scope(is_alive);
</pre>
<hr />
<pre>
 733       CompiledMethod *nm = cb-&gt;as_compiled_method_or_null();
 734       if (nm != NULL) {
 735         count += nm-&gt;verify_icholder_relocations();
 736       }
 737     }
 738   }
 739   assert(count + InlineCacheBuffer::pending_icholder_count() + CompiledICHolder::live_not_claimed_count() ==
 740          CompiledICHolder::live_count(), &quot;must agree&quot;);
 741 #endif
 742 }
 743 
 744 // Defer freeing of concurrently cleaned ExceptionCache entries until
 745 // after a global handshake operation.
 746 void CodeCache::release_exception_cache(ExceptionCache* entry) {
 747   if (SafepointSynchronize::is_at_safepoint()) {
 748     delete entry;
 749   } else {
 750     for (;;) {
 751       ExceptionCache* purge_list_head = Atomic::load(&amp;_exception_cache_purge_list);
 752       entry-&gt;set_purge_list_next(purge_list_head);
<span class="line-modified"> 753       if (Atomic::cmpxchg(&amp;_exception_cache_purge_list, purge_list_head, entry) == purge_list_head) {</span>
 754         break;
 755       }
 756     }
 757   }
 758 }
 759 
 760 // Delete exception caches that have been concurrently unlinked,
 761 // followed by a global handshake operation.
 762 void CodeCache::purge_exception_caches() {
 763   ExceptionCache* curr = _exception_cache_purge_list;
 764   while (curr != NULL) {
 765     ExceptionCache* next = curr-&gt;purge_list_next();
 766     delete curr;
 767     curr = next;
 768   }
 769   _exception_cache_purge_list = NULL;
 770 }
 771 
 772 uint8_t CodeCache::_unloading_cycle = 1;
 773 
 774 void CodeCache::increment_unloading_cycle() {
<span class="line-modified"> 775   // 2-bit value (see IsUnloadingState in nmethod.cpp for details)</span>
<span class="line-modified"> 776   // 0 is reserved for new methods.</span>
<span class="line-modified"> 777   _unloading_cycle = (_unloading_cycle + 1) % 4;</span>
<span class="line-added"> 778   if (_unloading_cycle == 0) {</span>
 779     _unloading_cycle = 1;
 780   }
 781 }
 782 
 783 CodeCache::UnloadingScope::UnloadingScope(BoolObjectClosure* is_alive)
 784   : _is_unloading_behaviour(is_alive)
 785 {
<span class="line-added"> 786   _saved_behaviour = IsUnloadingBehaviour::current();</span>
 787   IsUnloadingBehaviour::set_current(&amp;_is_unloading_behaviour);
 788   increment_unloading_cycle();
 789   DependencyContext::cleaning_start();
 790 }
 791 
 792 CodeCache::UnloadingScope::~UnloadingScope() {
<span class="line-modified"> 793   IsUnloadingBehaviour::set_current(_saved_behaviour);</span>
 794   DependencyContext::cleaning_end();
 795 }
 796 
 797 void CodeCache::verify_oops() {
<span class="line-modified"> 798   MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
 799   VerifyOopClosure voc;
 800   NMethodIterator iter(NMethodIterator::only_alive_and_not_unloading);
 801   while(iter.next()) {
 802     nmethod* nm = iter.method();
 803     nm-&gt;oops_do(&amp;voc);
 804     nm-&gt;verify_oop_relocations();
 805   }
 806 }
 807 
 808 int CodeCache::blob_count(int code_blob_type) {
 809   CodeHeap* heap = get_code_heap(code_blob_type);
 810   return (heap != NULL) ? heap-&gt;blob_count() : 0;
 811 }
 812 
 813 int CodeCache::blob_count() {
 814   int count = 0;
 815   FOR_ALL_HEAPS(heap) {
 816     count += (*heap)-&gt;blob_count();
 817   }
 818   return count;
</pre>
<hr />
<pre>
 926 }
 927 
 928 void icache_init();
 929 
 930 void CodeCache::initialize() {
 931   assert(CodeCacheSegmentSize &gt;= (uintx)CodeEntryAlignment, &quot;CodeCacheSegmentSize must be large enough to align entry points&quot;);
 932 #ifdef COMPILER2
 933   assert(CodeCacheSegmentSize &gt;= (uintx)OptoLoopAlignment,  &quot;CodeCacheSegmentSize must be large enough to align inner loops&quot;);
 934 #endif
 935   assert(CodeCacheSegmentSize &gt;= sizeof(jdouble),    &quot;CodeCacheSegmentSize must be large enough to align constants&quot;);
 936   // This was originally just a check of the alignment, causing failure, instead, round
 937   // the code cache to the page size.  In particular, Solaris is moving to a larger
 938   // default page size.
 939   CodeCacheExpansionSize = align_up(CodeCacheExpansionSize, os::vm_page_size());
 940 
 941   if (SegmentedCodeCache) {
 942     // Use multiple code heaps
 943     initialize_heaps();
 944   } else {
 945     // Use a single code heap
<span class="line-modified"> 946     FLAG_SET_ERGO(NonNMethodCodeHeapSize, 0);</span>
<span class="line-modified"> 947     FLAG_SET_ERGO(ProfiledCodeHeapSize, 0);</span>
<span class="line-modified"> 948     FLAG_SET_ERGO(NonProfiledCodeHeapSize, 0);</span>
 949     ReservedCodeSpace rs = reserve_heap_memory(ReservedCodeCacheSize);
 950     add_heap(rs, &quot;CodeCache&quot;, CodeBlobType::All);
 951   }
 952 
 953   // Initialize ICache flush mechanism
 954   // This service is needed for os::register_code_area
 955   icache_init();
 956 
 957   // Give OS a chance to register generated code area.
 958   // This is used on Windows 64 bit platforms to register
 959   // Structured Exception Handlers for our generated code.
 960   os::register_code_area((char*)low_bound(), (char*)high_bound());
 961 }
 962 
 963 void codeCache_init() {
 964   CodeCache::initialize();
 965   // Load AOT libraries and add AOT code heaps.
 966   AOTLoader::initialize();
 967 }
 968 
</pre>
<hr />
<pre>
 975 void CodeCache::clear_inline_caches() {
 976   assert_locked_or_safepoint(CodeCache_lock);
 977   CompiledMethodIterator iter(CompiledMethodIterator::only_alive_and_not_unloading);
 978   while(iter.next()) {
 979     iter.method()-&gt;clear_inline_caches();
 980   }
 981 }
 982 
 983 void CodeCache::cleanup_inline_caches() {
 984   assert_locked_or_safepoint(CodeCache_lock);
 985   NMethodIterator iter(NMethodIterator::only_alive_and_not_unloading);
 986   while(iter.next()) {
 987     iter.method()-&gt;cleanup_inline_caches(/*clean_all=*/true);
 988   }
 989 }
 990 
 991 // Keeps track of time spent for checking dependencies
 992 NOT_PRODUCT(static elapsedTimer dependentCheckTime;)
 993 
 994 int CodeCache::mark_for_deoptimization(KlassDepChange&amp; changes) {
<span class="line-modified"> 995   MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
 996   int number_of_marked_CodeBlobs = 0;
 997 
 998   // search the hierarchy looking for nmethods which are affected by the loading of this class
 999 
1000   // then search the interfaces this class implements looking for nmethods
1001   // which might be dependent of the fact that an interface only had one
1002   // implementor.
1003   // nmethod::check_all_dependencies works only correctly, if no safepoint
1004   // can happen
1005   NoSafepointVerifier nsv;
1006   for (DepChange::ContextStream str(changes, nsv); str.next(); ) {
1007     Klass* d = str.klass();
1008     number_of_marked_CodeBlobs += InstanceKlass::cast(d)-&gt;mark_dependent_nmethods(changes);
1009   }
1010 
1011 #ifndef PRODUCT
1012   if (VerifyDependencies) {
1013     // Object pointers are used as unique identifiers for dependency arguments. This
1014     // is only possible if no safepoint, i.e., GC occurs during the verification code.
1015     dependentCheckTime.start();
</pre>
<hr />
<pre>
1019 #endif
1020 
1021   return number_of_marked_CodeBlobs;
1022 }
1023 
1024 CompiledMethod* CodeCache::find_compiled(void* start) {
1025   CodeBlob *cb = find_blob(start);
1026   assert(cb == NULL || cb-&gt;is_compiled(), &quot;did not find an compiled_method&quot;);
1027   return (CompiledMethod*)cb;
1028 }
1029 
1030 bool CodeCache::is_far_target(address target) {
1031 #if INCLUDE_AOT
1032   return NativeCall::is_far_call(_low_bound,  target) ||
1033          NativeCall::is_far_call(_high_bound, target);
1034 #else
1035   return false;
1036 #endif
1037 }
1038 
<span class="line-modified">1039 #ifdef INCLUDE_JVMTI</span>
<span class="line-modified">1040 // RedefineClasses support for unloading nmethods that are dependent on &quot;old&quot; methods.</span>
<span class="line-modified">1041 // We don&#39;t really expect this table to grow very large.  If it does, it can become a hashtable.</span>
<span class="line-added">1042 static GrowableArray&lt;CompiledMethod*&gt;* old_compiled_method_table = NULL;</span>
1043 
<span class="line-modified">1044 static void add_to_old_table(CompiledMethod* c) {</span>
<span class="line-modified">1045   if (old_compiled_method_table == NULL) {</span>
<span class="line-modified">1046     old_compiled_method_table = new (ResourceObj::C_HEAP, mtCode) GrowableArray&lt;CompiledMethod*&gt;(100, true);</span>
<span class="line-modified">1047   }</span>
<span class="line-modified">1048   old_compiled_method_table-&gt;push(c);</span>
<span class="line-modified">1049 }</span>
<span class="line-modified">1050 </span>
<span class="line-modified">1051 static void reset_old_method_table() {</span>
<span class="line-added">1052   if (old_compiled_method_table != NULL) {</span>
<span class="line-added">1053     delete old_compiled_method_table;</span>
<span class="line-added">1054     old_compiled_method_table = NULL;</span>
<span class="line-added">1055   }</span>
<span class="line-added">1056 }</span>
<span class="line-added">1057 </span>
<span class="line-added">1058 // Remove this method when zombied or unloaded.</span>
<span class="line-added">1059 void CodeCache::unregister_old_nmethod(CompiledMethod* c) {</span>
<span class="line-added">1060   assert_lock_strong(CodeCache_lock);</span>
<span class="line-added">1061   if (old_compiled_method_table != NULL) {</span>
<span class="line-added">1062     int index = old_compiled_method_table-&gt;find(c);</span>
<span class="line-added">1063     if (index != -1) {</span>
<span class="line-added">1064       old_compiled_method_table-&gt;delete_at(index);</span>
<span class="line-added">1065     }</span>
<span class="line-added">1066   }</span>
<span class="line-added">1067 }</span>
<span class="line-added">1068 </span>
<span class="line-added">1069 void CodeCache::old_nmethods_do(MetadataClosure* f) {</span>
<span class="line-added">1070   // Walk old method table and mark those on stack.</span>
<span class="line-added">1071   int length = 0;</span>
<span class="line-added">1072   if (old_compiled_method_table != NULL) {</span>
<span class="line-added">1073     length = old_compiled_method_table-&gt;length();</span>
<span class="line-added">1074     for (int i = 0; i &lt; length; i++) {</span>
<span class="line-added">1075       CompiledMethod* cm = old_compiled_method_table-&gt;at(i);</span>
<span class="line-added">1076       // Only walk alive nmethods, the dead ones will get removed by the sweeper.</span>
<span class="line-added">1077       if (cm-&gt;is_alive()) {</span>
<span class="line-added">1078         old_compiled_method_table-&gt;at(i)-&gt;metadata_do(f);</span>
<span class="line-added">1079       }</span>
1080     }
1081   }
<span class="line-added">1082   log_debug(redefine, class, nmethod)(&quot;Walked %d nmethods for mark_on_stack&quot;, length);</span>
<span class="line-added">1083 }</span>
<span class="line-added">1084 </span>
<span class="line-added">1085 // Just marks the methods in this class as needing deoptimization</span>
<span class="line-added">1086 void CodeCache::mark_for_evol_deoptimization(InstanceKlass* dependee) {</span>
<span class="line-added">1087   assert(SafepointSynchronize::is_at_safepoint(), &quot;Can only do this at a safepoint!&quot;);</span>
1088 
1089   // Mark dependent AOT nmethods, which are only found via the class redefined.
<span class="line-added">1090   // TODO: add dependencies to aotCompiledMethod&#39;s metadata section so this isn&#39;t</span>
<span class="line-added">1091   // needed.</span>
1092   AOTLoader::mark_evol_dependent_methods(dependee);
1093 }
1094 
<span class="line-added">1095 </span>
1096 // Walk compiled methods and mark dependent methods for deoptimization.
1097 int CodeCache::mark_dependents_for_evol_deoptimization() {
<span class="line-added">1098   assert(SafepointSynchronize::is_at_safepoint(), &quot;Can only do this at a safepoint!&quot;);</span>
<span class="line-added">1099   // Each redefinition creates a new set of nmethods that have references to &quot;old&quot; Methods</span>
<span class="line-added">1100   // So delete old method table and create a new one.</span>
<span class="line-added">1101   reset_old_method_table();</span>
<span class="line-added">1102 </span>
1103   int number_of_marked_CodeBlobs = 0;
1104   CompiledMethodIterator iter(CompiledMethodIterator::only_alive_and_not_unloading);
1105   while(iter.next()) {
1106     CompiledMethod* nm = iter.method();
<span class="line-modified">1107     // Walk all alive nmethods to check for old Methods.</span>
<span class="line-modified">1108     // This includes methods whose inline caches point to old methods, so</span>
<span class="line-modified">1109     // inline cache clearing is unnecessary.</span>
<span class="line-modified">1110     if (nm-&gt;has_evol_metadata()) {</span>


1111       nm-&gt;mark_for_deoptimization();
<span class="line-added">1112       add_to_old_table(nm);</span>
1113       number_of_marked_CodeBlobs++;



1114     }
1115   }
1116 
1117   // return total count of nmethods marked for deoptimization, if zero the caller
1118   // can skip deoptimization
1119   return number_of_marked_CodeBlobs;
1120 }
1121 
<span class="line-modified">1122 void CodeCache::mark_all_nmethods_for_evol_deoptimization() {</span>
<span class="line-modified">1123   assert(SafepointSynchronize::is_at_safepoint(), &quot;Can only do this at a safepoint!&quot;);</span>

1124   CompiledMethodIterator iter(CompiledMethodIterator::only_alive_and_not_unloading);
1125   while(iter.next()) {
1126     CompiledMethod* nm = iter.method();
1127     if (!nm-&gt;method()-&gt;is_method_handle_intrinsic()) {
1128       nm-&gt;mark_for_deoptimization();
<span class="line-added">1129       if (nm-&gt;has_evol_metadata()) {</span>
<span class="line-added">1130         add_to_old_table(nm);</span>
<span class="line-added">1131       }</span>
<span class="line-added">1132     }</span>
<span class="line-added">1133   }</span>
<span class="line-added">1134 }</span>
<span class="line-added">1135 </span>
<span class="line-added">1136 // Flushes compiled methods dependent on redefined classes, that have already been</span>
<span class="line-added">1137 // marked for deoptimization.</span>
<span class="line-added">1138 void CodeCache::flush_evol_dependents() {</span>
<span class="line-added">1139   assert(SafepointSynchronize::is_at_safepoint(), &quot;Can only do this at a safepoint!&quot;);</span>
<span class="line-added">1140 </span>
<span class="line-added">1141   // CodeCache can only be updated by a thread_in_VM and they will all be</span>
<span class="line-added">1142   // stopped during the safepoint so CodeCache will be safe to update without</span>
<span class="line-added">1143   // holding the CodeCache_lock.</span>
<span class="line-added">1144 </span>
<span class="line-added">1145   // At least one nmethod has been marked for deoptimization</span>
<span class="line-added">1146 </span>
<span class="line-added">1147   Deoptimization::deoptimize_all_marked();</span>
<span class="line-added">1148 }</span>
<span class="line-added">1149 #endif // INCLUDE_JVMTI</span>
<span class="line-added">1150 </span>
<span class="line-added">1151 // Mark methods for deopt (if safe or possible).</span>
<span class="line-added">1152 void CodeCache::mark_all_nmethods_for_deoptimization() {</span>
<span class="line-added">1153   MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="line-added">1154   CompiledMethodIterator iter(CompiledMethodIterator::only_alive_and_not_unloading);</span>
<span class="line-added">1155   while(iter.next()) {</span>
<span class="line-added">1156     CompiledMethod* nm = iter.method();</span>
<span class="line-added">1157     if (!nm-&gt;is_native_method()) {</span>
<span class="line-added">1158       nm-&gt;mark_for_deoptimization();</span>
1159     }
1160   }
1161 }
1162 
1163 int CodeCache::mark_for_deoptimization(Method* dependee) {
<span class="line-modified">1164   MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
1165   int number_of_marked_CodeBlobs = 0;
1166 
1167   CompiledMethodIterator iter(CompiledMethodIterator::only_alive_and_not_unloading);
1168   while(iter.next()) {
1169     CompiledMethod* nm = iter.method();
1170     if (nm-&gt;is_dependent_on_method(dependee)) {
1171       ResourceMark rm;
1172       nm-&gt;mark_for_deoptimization();
1173       number_of_marked_CodeBlobs++;
1174     }
1175   }
1176 
1177   return number_of_marked_CodeBlobs;
1178 }
1179 
1180 void CodeCache::make_marked_nmethods_not_entrant() {
1181   assert_locked_or_safepoint(CodeCache_lock);
1182   CompiledMethodIterator iter(CompiledMethodIterator::only_alive_and_not_unloading);
1183   while(iter.next()) {
1184     CompiledMethod* nm = iter.method();
<span class="line-modified">1185     if (nm-&gt;is_marked_for_deoptimization()) {</span>
1186       nm-&gt;make_not_entrant();
1187     }
1188   }
1189 }
1190 
1191 // Flushes compiled methods dependent on dependee.
1192 void CodeCache::flush_dependents_on(InstanceKlass* dependee) {
1193   assert_lock_strong(Compile_lock);
1194 
1195   if (number_of_nmethods_with_dependencies() == 0) return;
1196 




1197   KlassDepChange changes(dependee);
1198 
1199   // Compute the dependent nmethods
1200   if (mark_for_deoptimization(changes) &gt; 0) {
1201     // At least one nmethod has been marked for deoptimization
<span class="line-modified">1202     Deoptimization::deoptimize_all_marked();</span>

1203   }
1204 }
1205 


























1206 // Flushes compiled methods dependent on dependee
1207 void CodeCache::flush_dependents_on_method(const methodHandle&amp; m_h) {
1208   // --- Compile_lock is not held. However we are at a safepoint.
1209   assert_locked_or_safepoint(Compile_lock);
1210 




1211   // Compute the dependent nmethods
1212   if (mark_for_deoptimization(m_h()) &gt; 0) {
<span class="line-modified">1213     Deoptimization::deoptimize_all_marked();</span>













1214   }
1215 }
1216 
1217 void CodeCache::verify() {
1218   assert_locked_or_safepoint(CodeCache_lock);
1219   FOR_ALL_HEAPS(heap) {
1220     (*heap)-&gt;verify();
1221     FOR_ALL_BLOBS(cb, *heap) {
1222       if (cb-&gt;is_alive()) {
1223         cb-&gt;verify();
1224       }
1225     }
1226   }
1227 }
1228 
1229 // A CodeHeap is full. Print out warning and report event.
1230 PRAGMA_DIAG_PUSH
1231 PRAGMA_FORMAT_NONLITERAL_IGNORED
1232 void CodeCache::report_codemem_full(int code_blob_type, bool print) {
1233   // Get nmethod heap for the given CodeBlobType and build CodeCacheFull event
</pre>
<hr />
<pre>
1246       const char *msg1 = msg1_stream.as_string();
1247       const char *msg2 = msg2_stream.as_string();
1248 
1249       log_warning(codecache)(&quot;%s&quot;, msg1);
1250       log_warning(codecache)(&quot;%s&quot;, msg2);
1251       warning(&quot;%s&quot;, msg1);
1252       warning(&quot;%s&quot;, msg2);
1253     } else {
1254       const char *msg1 = &quot;CodeCache is full. Compiler has been disabled.&quot;;
1255       const char *msg2 = &quot;Try increasing the code cache size using -XX:ReservedCodeCacheSize=&quot;;
1256 
1257       log_warning(codecache)(&quot;%s&quot;, msg1);
1258       log_warning(codecache)(&quot;%s&quot;, msg2);
1259       warning(&quot;%s&quot;, msg1);
1260       warning(&quot;%s&quot;, msg2);
1261     }
1262     ResourceMark rm;
1263     stringStream s;
1264     // Dump code cache into a buffer before locking the tty.
1265     {
<span class="line-modified">1266       MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
1267       print_summary(&amp;s);
1268     }
1269     {
1270       ttyLocker ttyl;
1271       tty-&gt;print(&quot;%s&quot;, s.as_string());
1272     }
1273 
1274     if (heap-&gt;full_count() == 0) {
1275       if (PrintCodeHeapAnalytics) {
<span class="line-modified">1276         CompileBroker::print_heapinfo(tty, &quot;all&quot;, 4096); // details, may be a lot!</span>
1277       }
1278     }
1279   }
1280 
1281   heap-&gt;report_full();
1282 
1283   EventCodeCacheFull event;
1284   if (event.should_commit()) {
1285     event.set_codeBlobType((u1)code_blob_type);
1286     event.set_startAddress((u8)heap-&gt;low_boundary());
1287     event.set_commitedTopAddress((u8)heap-&gt;high());
1288     event.set_reservedTopAddress((u8)heap-&gt;high_boundary());
1289     event.set_entryCount(heap-&gt;blob_count());
1290     event.set_methodCount(heap-&gt;nmethod_count());
1291     event.set_adaptorCount(heap-&gt;adapter_count());
1292     event.set_unallocatedCapacity(heap-&gt;unallocated_capacity());
1293     event.set_fullCount(heap-&gt;full_count());
1294     event.commit();
1295   }
1296 }
</pre>
<hr />
<pre>
1514       full_count += get_codemem_full_count(heap-&gt;code_blob_type());
1515     }
1516   }
1517 
1518   if (detailed) {
1519     st-&gt;print_cr(&quot; total_blobs=&quot; UINT32_FORMAT &quot; nmethods=&quot; UINT32_FORMAT
1520                        &quot; adapters=&quot; UINT32_FORMAT,
1521                        blob_count(), nmethod_count(), adapter_count());
1522     st-&gt;print_cr(&quot; compilation: %s&quot;, CompileBroker::should_compile_new_jobs() ?
1523                  &quot;enabled&quot; : Arguments::mode() == Arguments::_int ?
1524                  &quot;disabled (interpreter mode)&quot; :
1525                  &quot;disabled (not enough contiguous free space left)&quot;);
1526     st-&gt;print_cr(&quot;              stopped_count=%d, restarted_count=%d&quot;,
1527                  CompileBroker::get_total_compiler_stopped_count(),
1528                  CompileBroker::get_total_compiler_restarted_count());
1529     st-&gt;print_cr(&quot; full_count=%d&quot;, full_count);
1530   }
1531 }
1532 
1533 void CodeCache::print_codelist(outputStream* st) {
<span class="line-modified">1534   MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
1535 
1536   CompiledMethodIterator iter(CompiledMethodIterator::only_alive_and_not_unloading);
1537   while (iter.next()) {
1538     CompiledMethod* cm = iter.method();
1539     ResourceMark rm;
1540     char* method_name = cm-&gt;method()-&gt;name_and_sig_as_C_string();
1541     st-&gt;print_cr(&quot;%d %d %d %s [&quot; INTPTR_FORMAT &quot;, &quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;]&quot;,
1542                  cm-&gt;compile_id(), cm-&gt;comp_level(), cm-&gt;get_state(),
1543                  method_name,
1544                  (intptr_t)cm-&gt;header_begin(), (intptr_t)cm-&gt;code_begin(), (intptr_t)cm-&gt;code_end());
1545   }
1546 }
1547 
1548 void CodeCache::print_layout(outputStream* st) {
<span class="line-modified">1549   MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
1550   ResourceMark rm;
1551   print_summary(st, true);
1552 }
1553 
1554 void CodeCache::log_state(outputStream* st) {
1555   st-&gt;print(&quot; total_blobs=&#39;&quot; UINT32_FORMAT &quot;&#39; nmethods=&#39;&quot; UINT32_FORMAT &quot;&#39;&quot;
1556             &quot; adapters=&#39;&quot; UINT32_FORMAT &quot;&#39; free_code_cache=&#39;&quot; SIZE_FORMAT &quot;&#39;&quot;,
1557             blob_count(), nmethod_count(), adapter_count(),
1558             unallocated_capacity());
1559 }
1560 
1561 //---&lt;  BEGIN  &gt;--- CodeHeap State Analytics.
1562 
<span class="line-modified">1563 void CodeCache::aggregate(outputStream *out, size_t granularity) {</span>
1564   FOR_ALL_ALLOCABLE_HEAPS(heap) {
1565     CodeHeapState::aggregate(out, (*heap), granularity);
1566   }
1567 }
1568 
1569 void CodeCache::discard(outputStream *out) {
1570   FOR_ALL_ALLOCABLE_HEAPS(heap) {
1571     CodeHeapState::discard(out, (*heap));
1572   }
1573 }
1574 
1575 void CodeCache::print_usedSpace(outputStream *out) {
1576   FOR_ALL_ALLOCABLE_HEAPS(heap) {
1577     CodeHeapState::print_usedSpace(out, (*heap));
1578   }
1579 }
1580 
1581 void CodeCache::print_freeSpace(outputStream *out) {
1582   FOR_ALL_ALLOCABLE_HEAPS(heap) {
1583     CodeHeapState::print_freeSpace(out, (*heap));
</pre>
</td>
</tr>
</table>
<center><a href="codeBlob.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="codeCache.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>