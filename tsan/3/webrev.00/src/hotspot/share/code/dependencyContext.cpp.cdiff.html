<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/share/code/dependencyContext.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="dependencies.hpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="dependencyContext.hpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/code/dependencyContext.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 26,10 ***</span>
<span class="line-new-header">--- 26,11 ---</span>
  #include &quot;code/nmethod.hpp&quot;
  #include &quot;code/dependencies.hpp&quot;
  #include &quot;code/dependencyContext.hpp&quot;
  #include &quot;memory/resourceArea.hpp&quot;
  #include &quot;runtime/atomic.hpp&quot;
<span class="line-added">+ #include &quot;runtime/orderAccess.hpp&quot;</span>
  #include &quot;runtime/perfData.hpp&quot;
  #include &quot;utilities/exceptions.hpp&quot;
  
  PerfCounter* DependencyContext::_perf_total_buckets_allocated_count   = NULL;
  PerfCounter* DependencyContext::_perf_total_buckets_deallocated_count = NULL;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 99,11 ***</span>
    }
    nmethodBucket* new_head = new nmethodBucket(nm, NULL);
    for (;;) {
      nmethodBucket* head = Atomic::load(_dependency_context_addr);
      new_head-&gt;set_next(head);
<span class="line-modified">!     if (Atomic::cmpxchg(new_head, _dependency_context_addr, head) == head) {</span>
        break;
      }
    }
    if (UsePerfData) {
      _perf_total_buckets_allocated_count-&gt;inc();
<span class="line-new-header">--- 100,11 ---</span>
    }
    nmethodBucket* new_head = new nmethodBucket(nm, NULL);
    for (;;) {
      nmethodBucket* head = Atomic::load(_dependency_context_addr);
      new_head-&gt;set_next(head);
<span class="line-modified">!     if (Atomic::cmpxchg(_dependency_context_addr, head, new_head) == head) {</span>
        break;
      }
    }
    if (UsePerfData) {
      _perf_total_buckets_allocated_count-&gt;inc();
</pre>
<hr />
<pre>
<span class="line-old-header">*** 122,11 ***</span>
      // Mark the context as having stale entries, since it is not safe to
      // expunge the list right now.
      for (;;) {
        nmethodBucket* purge_list_head = Atomic::load(&amp;_purge_list);
        b-&gt;set_purge_list_next(purge_list_head);
<span class="line-modified">!       if (Atomic::cmpxchg(b, &amp;_purge_list, purge_list_head) == purge_list_head) {</span>
          break;
        }
      }
      if (UsePerfData) {
        _perf_total_buckets_stale_count-&gt;inc();
<span class="line-new-header">--- 123,11 ---</span>
      // Mark the context as having stale entries, since it is not safe to
      // expunge the list right now.
      for (;;) {
        nmethodBucket* purge_list_head = Atomic::load(&amp;_purge_list);
        b-&gt;set_purge_list_next(purge_list_head);
<span class="line-modified">!       if (Atomic::cmpxchg(&amp;_purge_list, purge_list_head, b) == purge_list_head) {</span>
          break;
        }
      }
      if (UsePerfData) {
        _perf_total_buckets_stale_count-&gt;inc();
</pre>
<hr />
<pre>
<span class="line-old-header">*** 258,11 ***</span>
  }
  
  #endif //PRODUCT
  
  int nmethodBucket::decrement() {
<span class="line-modified">!   return Atomic::sub(1, &amp;_count);</span>
  }
  
  // We use a monotonically increasing epoch counter to track the last epoch a given
  // dependency context was cleaned. GC threads claim cleanup tasks by performing
  // a CAS on this value.
<span class="line-new-header">--- 259,11 ---</span>
  }
  
  #endif //PRODUCT
  
  int nmethodBucket::decrement() {
<span class="line-modified">!   return Atomic::sub(&amp;_count, 1);</span>
  }
  
  // We use a monotonically increasing epoch counter to track the last epoch a given
  // dependency context was cleaned. GC threads claim cleanup tasks by performing
  // a CAS on this value.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 270,39 ***</span>
    uint64_t cleaning_epoch = Atomic::load(&amp;_cleaning_epoch);
    uint64_t last_cleanup = Atomic::load(_last_cleanup_addr);
    if (last_cleanup &gt;= cleaning_epoch) {
      return false;
    }
<span class="line-modified">!   return Atomic::cmpxchg(cleaning_epoch, _last_cleanup_addr, last_cleanup) == last_cleanup;</span>
  }
  
  // Retrieve the first nmethodBucket that has a dependent that does not correspond to
  // an is_unloading nmethod. Any nmethodBucket entries observed from the original head
  // that is_unloading() will be unlinked and placed on the purge list.
  nmethodBucket* DependencyContext::dependencies_not_unloading() {
    for (;;) {
      // Need acquire becase the read value could come from a concurrent insert.
<span class="line-modified">!     nmethodBucket* head = OrderAccess::load_acquire(_dependency_context_addr);</span>
      if (head == NULL || !head-&gt;get_nmethod()-&gt;is_unloading()) {
        return head;
      }
      nmethodBucket* head_next = head-&gt;next();
      OrderAccess::loadload();
      if (Atomic::load(_dependency_context_addr) != head) {
        // Unstable load of head w.r.t. head-&gt;next
        continue;
      }
<span class="line-modified">!     if (Atomic::cmpxchg(head_next, _dependency_context_addr, head) == head) {</span>
        // Release is_unloading entries if unlinking was claimed
        DependencyContext::release(head);
      }
    }
  }
  
  // Relaxed accessors
  void DependencyContext::set_dependencies(nmethodBucket* b) {
<span class="line-modified">!   Atomic::store(b, _dependency_context_addr);</span>
  }
  
  nmethodBucket* DependencyContext::dependencies() {
    return Atomic::load(_dependency_context_addr);
  }
<span class="line-new-header">--- 271,39 ---</span>
    uint64_t cleaning_epoch = Atomic::load(&amp;_cleaning_epoch);
    uint64_t last_cleanup = Atomic::load(_last_cleanup_addr);
    if (last_cleanup &gt;= cleaning_epoch) {
      return false;
    }
<span class="line-modified">!   return Atomic::cmpxchg(_last_cleanup_addr, last_cleanup, cleaning_epoch) == last_cleanup;</span>
  }
  
  // Retrieve the first nmethodBucket that has a dependent that does not correspond to
  // an is_unloading nmethod. Any nmethodBucket entries observed from the original head
  // that is_unloading() will be unlinked and placed on the purge list.
  nmethodBucket* DependencyContext::dependencies_not_unloading() {
    for (;;) {
      // Need acquire becase the read value could come from a concurrent insert.
<span class="line-modified">!     nmethodBucket* head = Atomic::load_acquire(_dependency_context_addr);</span>
      if (head == NULL || !head-&gt;get_nmethod()-&gt;is_unloading()) {
        return head;
      }
      nmethodBucket* head_next = head-&gt;next();
      OrderAccess::loadload();
      if (Atomic::load(_dependency_context_addr) != head) {
        // Unstable load of head w.r.t. head-&gt;next
        continue;
      }
<span class="line-modified">!     if (Atomic::cmpxchg(_dependency_context_addr, head, head_next) == head) {</span>
        // Release is_unloading entries if unlinking was claimed
        DependencyContext::release(head);
      }
    }
  }
  
  // Relaxed accessors
  void DependencyContext::set_dependencies(nmethodBucket* b) {
<span class="line-modified">!   Atomic::store(_dependency_context_addr, b);</span>
  }
  
  nmethodBucket* DependencyContext::dependencies() {
    return Atomic::load(_dependency_context_addr);
  }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 311,21 ***</span>
  // and releasing of nmethodBucket entries will be deferred and placed on
  // a purge list to be deleted later.
  void DependencyContext::cleaning_start() {
    assert(SafepointSynchronize::is_at_safepoint(), &quot;must be&quot;);
    uint64_t epoch = ++_cleaning_epoch_monotonic;
<span class="line-modified">!   Atomic::store(epoch, &amp;_cleaning_epoch);</span>
  }
  
  // The epilogue marks the end of dependency context cleanup by the GC,
  // and also makes subsequent releases of nmethodBuckets cause immediate
  // deletion. It is okay to delay calling of cleaning_end() to a concurrent
  // phase, subsequent to the safepoint operation in which cleaning_start()
  // was called. That allows dependency contexts to be cleaned concurrently.
  void DependencyContext::cleaning_end() {
    uint64_t epoch = 0;
<span class="line-modified">!   Atomic::store(epoch, &amp;_cleaning_epoch);</span>
  }
  
  // This function skips over nmethodBuckets in the list corresponding to
  // nmethods that are is_unloading. This allows exposing a view of the
  // dependents as-if they were already cleaned, despite being cleaned
<span class="line-new-header">--- 312,21 ---</span>
  // and releasing of nmethodBucket entries will be deferred and placed on
  // a purge list to be deleted later.
  void DependencyContext::cleaning_start() {
    assert(SafepointSynchronize::is_at_safepoint(), &quot;must be&quot;);
    uint64_t epoch = ++_cleaning_epoch_monotonic;
<span class="line-modified">!   Atomic::store(&amp;_cleaning_epoch, epoch);</span>
  }
  
  // The epilogue marks the end of dependency context cleanup by the GC,
  // and also makes subsequent releases of nmethodBuckets cause immediate
  // deletion. It is okay to delay calling of cleaning_end() to a concurrent
  // phase, subsequent to the safepoint operation in which cleaning_start()
  // was called. That allows dependency contexts to be cleaned concurrently.
  void DependencyContext::cleaning_end() {
    uint64_t epoch = 0;
<span class="line-modified">!   Atomic::store(&amp;_cleaning_epoch, epoch);</span>
  }
  
  // This function skips over nmethodBuckets in the list corresponding to
  // nmethods that are is_unloading. This allows exposing a view of the
  // dependents as-if they were already cleaned, despite being cleaned
</pre>
<hr />
<pre>
<span class="line-old-header">*** 343,11 ***</span>
      OrderAccess::loadload();
      if (Atomic::load(&amp;_next) != next) {
        // Unstable load of next w.r.t. next-&gt;next
        continue;
      }
<span class="line-modified">!     if (Atomic::cmpxchg(next_next, &amp;_next, next) == next) {</span>
        // Release is_unloading entries if unlinking was claimed
        DependencyContext::release(next);
      }
    }
  }
<span class="line-new-header">--- 344,11 ---</span>
      OrderAccess::loadload();
      if (Atomic::load(&amp;_next) != next) {
        // Unstable load of next w.r.t. next-&gt;next
        continue;
      }
<span class="line-modified">!     if (Atomic::cmpxchg(&amp;_next, next, next_next) == next) {</span>
        // Release is_unloading entries if unlinking was claimed
        DependencyContext::release(next);
      }
    }
  }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 356,15 ***</span>
  nmethodBucket* nmethodBucket::next() {
    return Atomic::load(&amp;_next);
  }
  
  void nmethodBucket::set_next(nmethodBucket* b) {
<span class="line-modified">!   Atomic::store(b, &amp;_next);</span>
  }
  
  nmethodBucket* nmethodBucket::purge_list_next() {
    return Atomic::load(&amp;_purge_list_next);
  }
  
  void nmethodBucket::set_purge_list_next(nmethodBucket* b) {
<span class="line-modified">!   Atomic::store(b, &amp;_purge_list_next);</span>
  }
<span class="line-new-header">--- 357,15 ---</span>
  nmethodBucket* nmethodBucket::next() {
    return Atomic::load(&amp;_next);
  }
  
  void nmethodBucket::set_next(nmethodBucket* b) {
<span class="line-modified">!   Atomic::store(&amp;_next, b);</span>
  }
  
  nmethodBucket* nmethodBucket::purge_list_next() {
    return Atomic::load(&amp;_purge_list_next);
  }
  
  void nmethodBucket::set_purge_list_next(nmethodBucket* b) {
<span class="line-modified">!   Atomic::store(&amp;_purge_list_next, b);</span>
  }
</pre>
<center><a href="dependencies.hpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="dependencyContext.hpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>