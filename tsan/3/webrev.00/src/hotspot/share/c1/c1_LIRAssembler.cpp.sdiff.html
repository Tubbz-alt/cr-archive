<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/c1/c1_LIRAssembler.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c1_LIR.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_LIRAssembler.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/c1/c1_LIRAssembler.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
145       _masm-&gt;block_comment(st.as_string());
146     }
147 #endif
148     s-&gt;emit_code(this);
149 #ifdef ASSERT
150     s-&gt;assert_no_unbound_labels();
151 #endif
152   }
153 }
154 
155 
156 void LIR_Assembler::emit_slow_case_stubs() {
157   emit_stubs(_slow_case_stubs);
158 }
159 
160 
161 bool LIR_Assembler::needs_icache(ciMethod* method) const {
162   return !method-&gt;is_static();
163 }
164 



165 
166 int LIR_Assembler::code_offset() const {
167   return _masm-&gt;offset();
168 }
169 
170 
171 address LIR_Assembler::pc() const {
172   return _masm-&gt;pc();
173 }
174 
175 // To bang the stack of this compiled method we use the stack size
176 // that the interpreter would need in case of a deoptimization. This
177 // removes the need to bang the stack in the deoptimization blob which
178 // in turn simplifies stack overflow handling.
179 int LIR_Assembler::bang_size_in_bytes() const {
180   return MAX2(initial_frame_size_in_bytes() + os::extra_bang_size_in_bytes(), _compilation-&gt;interpreter_frame_size());
181 }
182 
183 void LIR_Assembler::emit_exception_entries(ExceptionInfoList* info_list) {
184   for (int i = 0; i &lt; info_list-&gt;length(); i++) {
</pre>
<hr />
<pre>
461   case lir_optvirtual_call:
462     call(op, relocInfo::opt_virtual_call_type);
463     break;
464   case lir_icvirtual_call:
465     ic_call(op);
466     break;
467   case lir_virtual_call:
468     vtable_call(op);
469     break;
470   default:
471     fatal(&quot;unexpected op code: %s&quot;, op-&gt;name());
472     break;
473   }
474 
475   // JSR 292
476   // Record if this method has MethodHandle invokes.
477   if (op-&gt;is_method_handle_invoke()) {
478     compilation()-&gt;set_has_method_handle_invokes(true);
479   }
480 
<span class="line-modified">481 #if defined(X86) &amp;&amp; defined(TIERED)</span>
482   // C2 leave fpu stack dirty clean it
483   if (UseSSE &lt; 2) {
484     int i;
485     for ( i = 1; i &lt;= 7 ; i++ ) {
486       ffree(i);
487     }
488     if (!op-&gt;result_opr()-&gt;is_float_kind()) {
489       ffree(0);
490     }
491   }
492 #endif // X86 &amp;&amp; TIERED
493 }
494 
495 
496 void LIR_Assembler::emit_opLabel(LIR_OpLabel* op) {
497   _masm-&gt;bind (*(op-&gt;label()));
498 }
499 
500 
501 void LIR_Assembler::emit_op1(LIR_Op1* op) {
</pre>
<hr />
<pre>
512       }
513       break;
514 
515     case lir_roundfp: {
516       LIR_OpRoundFP* round_op = op-&gt;as_OpRoundFP();
517       roundfp_op(round_op-&gt;in_opr(), round_op-&gt;tmp(), round_op-&gt;result_opr(), round_op-&gt;pop_fpu_stack());
518       break;
519     }
520 
521     case lir_return:
522       return_op(op-&gt;in_opr());
523       break;
524 
525     case lir_safepoint:
526       if (compilation()-&gt;debug_info_recorder()-&gt;last_pc_offset() == code_offset()) {
527         _masm-&gt;nop();
528       }
529       safepoint_poll(op-&gt;in_opr(), op-&gt;info());
530       break;
531 

532     case lir_fxch:
533       fxch(op-&gt;in_opr()-&gt;as_jint());
534       break;
535 
536     case lir_fld:
537       fld(op-&gt;in_opr()-&gt;as_jint());
538       break;
<span class="line-modified">539 </span>
<span class="line-removed">540     case lir_ffree:</span>
<span class="line-removed">541       ffree(op-&gt;in_opr()-&gt;as_jint());</span>
<span class="line-removed">542       break;</span>
543 
544     case lir_branch:
545       break;
546 
547     case lir_push:
548       push(op-&gt;in_opr());
549       break;
550 
551     case lir_pop:
552       pop(op-&gt;in_opr());
553       break;
554 
555     case lir_leal:
556       leal(op-&gt;in_opr(), op-&gt;result_opr(), op-&gt;patch_code(), op-&gt;info());
557       break;
558 
559     case lir_null_check: {
560       ImplicitNullCheckStub* stub = add_debug_info_for_null_check_here(op-&gt;info());
561 
562       if (op-&gt;in_opr()-&gt;is_single_cpu()) {
</pre>
<hr />
<pre>
604       _masm-&gt;nop();
605       break;
606 
607     case lir_label:
608       Unimplemented();
609       break;
610 
611     case lir_build_frame:
612       build_frame();
613       break;
614 
615     case lir_std_entry:
616       // init offsets
617       offsets()-&gt;set_value(CodeOffsets::OSR_Entry, _masm-&gt;offset());
618       _masm-&gt;align(CodeEntryAlignment);
619       if (needs_icache(compilation()-&gt;method())) {
620         check_icache();
621       }
622       offsets()-&gt;set_value(CodeOffsets::Verified_Entry, _masm-&gt;offset());
623       _masm-&gt;verified_entry();



624       build_frame();
625       offsets()-&gt;set_value(CodeOffsets::Frame_Complete, _masm-&gt;offset());
626       break;
627 
628     case lir_osr_entry:
629       offsets()-&gt;set_value(CodeOffsets::OSR_Entry, _masm-&gt;offset());
630       osr_entry();
631       break;
632 
<span class="line-modified">633     case lir_24bit_FPU:</span>
<span class="line-modified">634       set_24bit_FPU();</span>
<span class="line-modified">635       break;</span>
<span class="line-removed">636 </span>
<span class="line-removed">637     case lir_reset_FPU:</span>
<span class="line-removed">638       reset_FPU();</span>
639       break;

640 
641     case lir_breakpoint:
642       breakpoint();
643       break;
644 
<span class="line-removed">645     case lir_fpop_raw:</span>
<span class="line-removed">646       fpop();</span>
<span class="line-removed">647       break;</span>
<span class="line-removed">648 </span>
649     case lir_membar:
650       membar();
651       break;
652 
653     case lir_membar_acquire:
654       membar_acquire();
655       break;
656 
657     case lir_membar_release:
658       membar_release();
659       break;
660 
661     case lir_membar_loadload:
662       membar_loadload();
663       break;
664 
665     case lir_membar_storestore:
666       membar_storestore();
667       break;
668 
</pre>
<hr />
<pre>
763       break;
764 
765     case lir_xadd:
766     case lir_xchg:
767       atomic_op(op-&gt;code(), op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;result_opr(), op-&gt;tmp1_opr());
768       break;
769 
770     default:
771       Unimplemented();
772       break;
773   }
774 }
775 
776 
777 void LIR_Assembler::build_frame() {
778   _masm-&gt;build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());
779 }
780 
781 
782 void LIR_Assembler::roundfp_op(LIR_Opr src, LIR_Opr tmp, LIR_Opr dest, bool pop_fpu_stack) {

783   assert((src-&gt;is_single_fpu() &amp;&amp; dest-&gt;is_single_stack()) ||
784          (src-&gt;is_double_fpu() &amp;&amp; dest-&gt;is_double_stack()),
785          &quot;round_fp: rounds register -&gt; stack location&quot;);
786 
787   reg2stack (src, dest, src-&gt;type(), pop_fpu_stack);
788 }
789 
790 
791 void LIR_Assembler::move_op(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool unaligned, bool wide) {
792   if (src-&gt;is_register()) {
793     if (dest-&gt;is_register()) {
794       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
795       reg2reg(src,  dest);
796     } else if (dest-&gt;is_stack()) {
797       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
798       reg2stack(src, dest, type, pop_fpu_stack);
799     } else if (dest-&gt;is_address()) {
800       reg2mem(src, dest, type, patch_code, info, pop_fpu_stack, wide, unaligned);
801     } else {
802       ShouldNotReachHere();
</pre>
</td>
<td>
<hr />
<pre>
145       _masm-&gt;block_comment(st.as_string());
146     }
147 #endif
148     s-&gt;emit_code(this);
149 #ifdef ASSERT
150     s-&gt;assert_no_unbound_labels();
151 #endif
152   }
153 }
154 
155 
156 void LIR_Assembler::emit_slow_case_stubs() {
157   emit_stubs(_slow_case_stubs);
158 }
159 
160 
161 bool LIR_Assembler::needs_icache(ciMethod* method) const {
162   return !method-&gt;is_static();
163 }
164 
<span class="line-added">165 bool LIR_Assembler::needs_clinit_barrier_on_entry(ciMethod* method) const {</span>
<span class="line-added">166   return VM_Version::supports_fast_class_init_checks() &amp;&amp; method-&gt;needs_clinit_barrier();</span>
<span class="line-added">167 }</span>
168 
169 int LIR_Assembler::code_offset() const {
170   return _masm-&gt;offset();
171 }
172 
173 
174 address LIR_Assembler::pc() const {
175   return _masm-&gt;pc();
176 }
177 
178 // To bang the stack of this compiled method we use the stack size
179 // that the interpreter would need in case of a deoptimization. This
180 // removes the need to bang the stack in the deoptimization blob which
181 // in turn simplifies stack overflow handling.
182 int LIR_Assembler::bang_size_in_bytes() const {
183   return MAX2(initial_frame_size_in_bytes() + os::extra_bang_size_in_bytes(), _compilation-&gt;interpreter_frame_size());
184 }
185 
186 void LIR_Assembler::emit_exception_entries(ExceptionInfoList* info_list) {
187   for (int i = 0; i &lt; info_list-&gt;length(); i++) {
</pre>
<hr />
<pre>
464   case lir_optvirtual_call:
465     call(op, relocInfo::opt_virtual_call_type);
466     break;
467   case lir_icvirtual_call:
468     ic_call(op);
469     break;
470   case lir_virtual_call:
471     vtable_call(op);
472     break;
473   default:
474     fatal(&quot;unexpected op code: %s&quot;, op-&gt;name());
475     break;
476   }
477 
478   // JSR 292
479   // Record if this method has MethodHandle invokes.
480   if (op-&gt;is_method_handle_invoke()) {
481     compilation()-&gt;set_has_method_handle_invokes(true);
482   }
483 
<span class="line-modified">484 #if defined(IA32) &amp;&amp; defined(TIERED)</span>
485   // C2 leave fpu stack dirty clean it
486   if (UseSSE &lt; 2) {
487     int i;
488     for ( i = 1; i &lt;= 7 ; i++ ) {
489       ffree(i);
490     }
491     if (!op-&gt;result_opr()-&gt;is_float_kind()) {
492       ffree(0);
493     }
494   }
495 #endif // X86 &amp;&amp; TIERED
496 }
497 
498 
499 void LIR_Assembler::emit_opLabel(LIR_OpLabel* op) {
500   _masm-&gt;bind (*(op-&gt;label()));
501 }
502 
503 
504 void LIR_Assembler::emit_op1(LIR_Op1* op) {
</pre>
<hr />
<pre>
515       }
516       break;
517 
518     case lir_roundfp: {
519       LIR_OpRoundFP* round_op = op-&gt;as_OpRoundFP();
520       roundfp_op(round_op-&gt;in_opr(), round_op-&gt;tmp(), round_op-&gt;result_opr(), round_op-&gt;pop_fpu_stack());
521       break;
522     }
523 
524     case lir_return:
525       return_op(op-&gt;in_opr());
526       break;
527 
528     case lir_safepoint:
529       if (compilation()-&gt;debug_info_recorder()-&gt;last_pc_offset() == code_offset()) {
530         _masm-&gt;nop();
531       }
532       safepoint_poll(op-&gt;in_opr(), op-&gt;info());
533       break;
534 
<span class="line-added">535 #ifdef IA32</span>
536     case lir_fxch:
537       fxch(op-&gt;in_opr()-&gt;as_jint());
538       break;
539 
540     case lir_fld:
541       fld(op-&gt;in_opr()-&gt;as_jint());
542       break;
<span class="line-modified">543 #endif // IA32</span>



544 
545     case lir_branch:
546       break;
547 
548     case lir_push:
549       push(op-&gt;in_opr());
550       break;
551 
552     case lir_pop:
553       pop(op-&gt;in_opr());
554       break;
555 
556     case lir_leal:
557       leal(op-&gt;in_opr(), op-&gt;result_opr(), op-&gt;patch_code(), op-&gt;info());
558       break;
559 
560     case lir_null_check: {
561       ImplicitNullCheckStub* stub = add_debug_info_for_null_check_here(op-&gt;info());
562 
563       if (op-&gt;in_opr()-&gt;is_single_cpu()) {
</pre>
<hr />
<pre>
605       _masm-&gt;nop();
606       break;
607 
608     case lir_label:
609       Unimplemented();
610       break;
611 
612     case lir_build_frame:
613       build_frame();
614       break;
615 
616     case lir_std_entry:
617       // init offsets
618       offsets()-&gt;set_value(CodeOffsets::OSR_Entry, _masm-&gt;offset());
619       _masm-&gt;align(CodeEntryAlignment);
620       if (needs_icache(compilation()-&gt;method())) {
621         check_icache();
622       }
623       offsets()-&gt;set_value(CodeOffsets::Verified_Entry, _masm-&gt;offset());
624       _masm-&gt;verified_entry();
<span class="line-added">625       if (needs_clinit_barrier_on_entry(compilation()-&gt;method())) {</span>
<span class="line-added">626         clinit_barrier(compilation()-&gt;method());</span>
<span class="line-added">627       }</span>
628       build_frame();
629       offsets()-&gt;set_value(CodeOffsets::Frame_Complete, _masm-&gt;offset());
630       break;
631 
632     case lir_osr_entry:
633       offsets()-&gt;set_value(CodeOffsets::OSR_Entry, _masm-&gt;offset());
634       osr_entry();
635       break;
636 
<span class="line-modified">637 #ifdef IA32</span>
<span class="line-modified">638     case lir_fpop_raw:</span>
<span class="line-modified">639       fpop();</span>



640       break;
<span class="line-added">641 #endif // IA32</span>
642 
643     case lir_breakpoint:
644       breakpoint();
645       break;
646 




647     case lir_membar:
648       membar();
649       break;
650 
651     case lir_membar_acquire:
652       membar_acquire();
653       break;
654 
655     case lir_membar_release:
656       membar_release();
657       break;
658 
659     case lir_membar_loadload:
660       membar_loadload();
661       break;
662 
663     case lir_membar_storestore:
664       membar_storestore();
665       break;
666 
</pre>
<hr />
<pre>
761       break;
762 
763     case lir_xadd:
764     case lir_xchg:
765       atomic_op(op-&gt;code(), op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;result_opr(), op-&gt;tmp1_opr());
766       break;
767 
768     default:
769       Unimplemented();
770       break;
771   }
772 }
773 
774 
775 void LIR_Assembler::build_frame() {
776   _masm-&gt;build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());
777 }
778 
779 
780 void LIR_Assembler::roundfp_op(LIR_Opr src, LIR_Opr tmp, LIR_Opr dest, bool pop_fpu_stack) {
<span class="line-added">781   assert(strict_fp_requires_explicit_rounding, &quot;not required&quot;);</span>
782   assert((src-&gt;is_single_fpu() &amp;&amp; dest-&gt;is_single_stack()) ||
783          (src-&gt;is_double_fpu() &amp;&amp; dest-&gt;is_double_stack()),
784          &quot;round_fp: rounds register -&gt; stack location&quot;);
785 
786   reg2stack (src, dest, src-&gt;type(), pop_fpu_stack);
787 }
788 
789 
790 void LIR_Assembler::move_op(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool unaligned, bool wide) {
791   if (src-&gt;is_register()) {
792     if (dest-&gt;is_register()) {
793       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
794       reg2reg(src,  dest);
795     } else if (dest-&gt;is_stack()) {
796       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
797       reg2stack(src, dest, type, pop_fpu_stack);
798     } else if (dest-&gt;is_address()) {
799       reg2mem(src, dest, type, patch_code, info, pop_fpu_stack, wide, unaligned);
800     } else {
801       ShouldNotReachHere();
</pre>
</td>
</tr>
</table>
<center><a href="c1_LIR.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_LIRAssembler.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>