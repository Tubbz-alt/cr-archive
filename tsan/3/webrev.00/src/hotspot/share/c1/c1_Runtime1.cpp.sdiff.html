<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/c1/c1_Runtime1.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c1_RangeCheckElimination.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_ValueMap.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/c1/c1_Runtime1.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/codeBuffer.hpp&quot;
  27 #include &quot;c1/c1_CodeStubs.hpp&quot;
  28 #include &quot;c1/c1_Defs.hpp&quot;
  29 #include &quot;c1/c1_FrameMap.hpp&quot;
  30 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  31 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  32 #include &quot;c1/c1_Runtime1.hpp&quot;
  33 #include &quot;classfile/systemDictionary.hpp&quot;
  34 #include &quot;classfile/vmSymbols.hpp&quot;
  35 #include &quot;code/codeBlob.hpp&quot;
  36 #include &quot;code/compiledIC.hpp&quot;
  37 #include &quot;code/pcDesc.hpp&quot;
  38 #include &quot;code/scopeDesc.hpp&quot;
  39 #include &quot;code/vtableStubs.hpp&quot;

  40 #include &quot;compiler/disassembler.hpp&quot;
  41 #include &quot;gc/shared/barrierSet.hpp&quot;
  42 #include &quot;gc/shared/c1/barrierSetC1.hpp&quot;
  43 #include &quot;gc/shared/collectedHeap.hpp&quot;
  44 #include &quot;interpreter/bytecode.hpp&quot;
  45 #include &quot;interpreter/interpreter.hpp&quot;
  46 #include &quot;jfr/support/jfrIntrinsics.hpp&quot;
  47 #include &quot;logging/log.hpp&quot;
  48 #include &quot;memory/allocation.inline.hpp&quot;
  49 #include &quot;memory/oopFactory.hpp&quot;
  50 #include &quot;memory/resourceArea.hpp&quot;

  51 #include &quot;oops/access.inline.hpp&quot;
  52 #include &quot;oops/objArrayOop.inline.hpp&quot;
  53 #include &quot;oops/objArrayKlass.hpp&quot;
  54 #include &quot;oops/oop.inline.hpp&quot;
  55 #include &quot;runtime/atomic.hpp&quot;
  56 #include &quot;runtime/biasedLocking.hpp&quot;
<span class="line-removed">  57 #include &quot;runtime/compilationPolicy.hpp&quot;</span>
  58 #include &quot;runtime/fieldDescriptor.inline.hpp&quot;
  59 #include &quot;runtime/frame.inline.hpp&quot;
  60 #include &quot;runtime/handles.inline.hpp&quot;
  61 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  62 #include &quot;runtime/javaCalls.hpp&quot;
  63 #include &quot;runtime/sharedRuntime.hpp&quot;
  64 #include &quot;runtime/threadCritical.hpp&quot;
  65 #include &quot;runtime/vframe.inline.hpp&quot;
  66 #include &quot;runtime/vframeArray.hpp&quot;
  67 #include &quot;runtime/vm_version.hpp&quot;
  68 #include &quot;utilities/copy.hpp&quot;
  69 #include &quot;utilities/events.hpp&quot;
  70 
  71 
  72 // Implementation of StubAssembler
  73 
  74 StubAssembler::StubAssembler(CodeBuffer* code, const char * name, int stub_id) : C1_MacroAssembler(code) {
  75   _name = name;
  76   _must_gc_arguments = false;
  77   _frame_size = no_frame_size;
</pre>
<hr />
<pre>
 276   BarrierSetC1* bs = BarrierSet::barrier_set()-&gt;barrier_set_c1();
 277   bs-&gt;generate_c1_runtime_stubs(blob);
 278 }
 279 
 280 CodeBlob* Runtime1::blob_for(StubID id) {
 281   assert(0 &lt;= id &amp;&amp; id &lt; number_of_ids, &quot;illegal stub id&quot;);
 282   return _blobs[id];
 283 }
 284 
 285 
 286 const char* Runtime1::name_for(StubID id) {
 287   assert(0 &lt;= id &amp;&amp; id &lt; number_of_ids, &quot;illegal stub id&quot;);
 288   return _blob_names[id];
 289 }
 290 
 291 const char* Runtime1::name_for_address(address entry) {
 292   for (int id = 0; id &lt; number_of_ids; id++) {
 293     if (entry == entry_for((StubID)id)) return name_for((StubID)id);
 294   }
 295 






 296 #define FUNCTION_CASE(a, f) \
 297   if ((intptr_t)a == CAST_FROM_FN_PTR(intptr_t, f))  return #f
 298 
 299   FUNCTION_CASE(entry, os::javaTimeMillis);
 300   FUNCTION_CASE(entry, os::javaTimeNanos);
 301   FUNCTION_CASE(entry, SharedRuntime::OSR_migration_end);
 302   FUNCTION_CASE(entry, SharedRuntime::d2f);
 303   FUNCTION_CASE(entry, SharedRuntime::d2i);
 304   FUNCTION_CASE(entry, SharedRuntime::d2l);
 305   FUNCTION_CASE(entry, SharedRuntime::dcos);
 306   FUNCTION_CASE(entry, SharedRuntime::dexp);
 307   FUNCTION_CASE(entry, SharedRuntime::dlog);
 308   FUNCTION_CASE(entry, SharedRuntime::dlog10);
 309   FUNCTION_CASE(entry, SharedRuntime::dpow);
 310   FUNCTION_CASE(entry, SharedRuntime::drem);
 311   FUNCTION_CASE(entry, SharedRuntime::dsin);
 312   FUNCTION_CASE(entry, SharedRuntime::dtan);
 313   FUNCTION_CASE(entry, SharedRuntime::f2i);
 314   FUNCTION_CASE(entry, SharedRuntime::f2l);
 315   FUNCTION_CASE(entry, SharedRuntime::frem);
</pre>
<hr />
<pre>
 557   }
 558 
 559   // If the stack guard pages are enabled, check whether there is a handler in
 560   // the current method.  Otherwise (guard pages disabled), force an unwind and
 561   // skip the exception cache update (i.e., just leave continuation==NULL).
 562   address continuation = NULL;
 563   if (guard_pages_enabled) {
 564 
 565     // New exception handling mechanism can support inlined methods
 566     // with exception handlers since the mappings are from PC to PC
 567 
 568     // debugging support
 569     // tracing
 570     if (log_is_enabled(Info, exceptions)) {
 571       ResourceMark rm;
 572       stringStream tempst;
 573       assert(nm-&gt;method() != NULL, &quot;Unexpected NULL method()&quot;);
 574       tempst.print(&quot;compiled method &lt;%s&gt;\n&quot;
 575                    &quot; at PC&quot; INTPTR_FORMAT &quot; for thread &quot; INTPTR_FORMAT,
 576                    nm-&gt;method()-&gt;print_value_string(), p2i(pc), p2i(thread));
<span class="line-modified"> 577       Exceptions::log_exception(exception, tempst);</span>
 578     }
 579     // for AbortVMOnException flag
 580     Exceptions::debug_check_abort(exception);
 581 
 582     // Clear out the exception oop and pc since looking up an
 583     // exception handler can cause class loading, which might throw an
 584     // exception and those fields are expected to be clear during
 585     // normal bytecode execution.
 586     thread-&gt;clear_exception_oop_and_pc();
 587 
 588     bool recursive_exception = false;
 589     continuation = SharedRuntime::compute_compiled_exc_handler(nm, pc, exception, false, false, recursive_exception);
 590     // If an exception was thrown during exception dispatch, the exception oop may have changed
 591     thread-&gt;set_exception_oop(exception());
 592     thread-&gt;set_exception_pc(pc);
 593 
 594     // the exception cache is used only by non-implicit exceptions
 595     // Update the exception cache only when there didn&#39;t happen
 596     // another exception during the computation of the compiled
 597     // exception handler. Checking for exception oop equality is not
</pre>
<hr />
<pre>
 681   char* message = SharedRuntime::generate_class_cast_message(
 682     thread, object-&gt;klass());
 683   SharedRuntime::throw_and_post_jvmti_exception(
 684     thread, vmSymbols::java_lang_ClassCastException(), message);
 685 JRT_END
 686 
 687 
 688 JRT_ENTRY(void, Runtime1::throw_incompatible_class_change_error(JavaThread* thread))
 689   NOT_PRODUCT(_throw_incompatible_class_change_error_count++;)
 690   ResourceMark rm(thread);
 691   SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_IncompatibleClassChangeError());
 692 JRT_END
 693 
 694 
 695 JRT_ENTRY_NO_ASYNC(void, Runtime1::monitorenter(JavaThread* thread, oopDesc* obj, BasicObjectLock* lock))
 696   NOT_PRODUCT(_monitorenter_slowcase_cnt++;)
 697   if (PrintBiasedLockingStatistics) {
 698     Atomic::inc(BiasedLocking::slow_path_entry_count_addr());
 699   }
 700   Handle h_obj(thread, obj);
<span class="line-modified"> 701   if (UseBiasedLocking) {</span>
<span class="line-modified"> 702     // Retry fast entry if bias is revoked to avoid unnecessary inflation</span>
<span class="line-removed"> 703     ObjectSynchronizer::fast_enter(h_obj, lock-&gt;lock(), true, CHECK);</span>
<span class="line-removed"> 704   } else {</span>
<span class="line-removed"> 705     if (UseFastLocking) {</span>
<span class="line-removed"> 706       // When using fast locking, the compiled code has already tried the fast case</span>
<span class="line-removed"> 707       assert(obj == lock-&gt;obj(), &quot;must match&quot;);</span>
<span class="line-removed"> 708       ObjectSynchronizer::slow_enter(h_obj, lock-&gt;lock(), THREAD);</span>
<span class="line-removed"> 709     } else {</span>
<span class="line-removed"> 710       lock-&gt;set_obj(obj);</span>
<span class="line-removed"> 711       ObjectSynchronizer::fast_enter(h_obj, lock-&gt;lock(), false, THREAD);</span>
<span class="line-removed"> 712     }</span>
 713   }


 714 JRT_END
 715 
 716 
 717 JRT_LEAF(void, Runtime1::monitorexit(JavaThread* thread, BasicObjectLock* lock))
 718   NOT_PRODUCT(_monitorexit_slowcase_cnt++;)
 719   assert(thread == JavaThread::current(), &quot;threads must correspond&quot;);
 720   assert(thread-&gt;last_Java_sp(), &quot;last_Java_sp must be set&quot;);
 721   // monitorexit is non-blocking (leaf routine) =&gt; no exceptions can be thrown
 722   EXCEPTION_MARK;
 723 
 724   oop obj = lock-&gt;obj();
 725   assert(oopDesc::is_oop(obj), &quot;must be NULL or an object&quot;);
<span class="line-modified"> 726   if (UseFastLocking) {</span>
<span class="line-removed"> 727     // When using fast locking, the compiled code has already tried the fast case</span>
<span class="line-removed"> 728     ObjectSynchronizer::slow_exit(obj, lock-&gt;lock(), THREAD);</span>
<span class="line-removed"> 729   } else {</span>
<span class="line-removed"> 730     ObjectSynchronizer::fast_exit(obj, lock-&gt;lock(), THREAD);</span>
<span class="line-removed"> 731   }</span>
 732 JRT_END
 733 
 734 // Cf. OptoRuntime::deoptimize_caller_frame
 735 JRT_ENTRY(void, Runtime1::deoptimize(JavaThread* thread, jint trap_request))
 736   // Called from within the owner thread, so no need for safepoint
 737   RegisterMap reg_map(thread, false);
 738   frame stub_frame = thread-&gt;last_frame();
 739   assert(stub_frame.is_runtime_frame(), &quot;Sanity check&quot;);
 740   frame caller_frame = stub_frame.sender(&amp;reg_map);
 741   nmethod* nm = caller_frame.cb()-&gt;as_nmethod_or_null();
 742   assert(nm != NULL, &quot;Sanity check&quot;);
 743   methodHandle method(thread, nm-&gt;method());
 744   assert(nm == CodeCache::find_nmethod(caller_frame.pc()), &quot;Should be the same&quot;);
 745   Deoptimization::DeoptAction action = Deoptimization::trap_request_action(trap_request);
 746   Deoptimization::DeoptReason reason = Deoptimization::trap_request_reason(trap_request);
 747 
 748   if (action == Deoptimization::Action_make_not_entrant) {
 749     if (nm-&gt;make_not_entrant()) {
 750       if (reason == Deoptimization::Reason_tenured) {
 751         MethodData* trap_mdo = Deoptimization::get_method_data(thread, method, true /*create_if_missing*/);
</pre>
<hr />
<pre>
1029       if (deoptimize_for_atomic) {
1030         tty-&gt;print_cr(&quot;Deoptimizing for patching atomic field reference&quot;);
1031       }
1032     }
1033 
1034     // It&#39;s possible the nmethod was invalidated in the last
1035     // safepoint, but if it&#39;s still alive then make it not_entrant.
1036     nmethod* nm = CodeCache::find_nmethod(caller_frame.pc());
1037     if (nm != NULL) {
1038       nm-&gt;make_not_entrant();
1039     }
1040 
1041     Deoptimization::deoptimize_frame(thread, caller_frame.id());
1042 
1043     // Return to the now deoptimized frame.
1044   }
1045 
1046   // Now copy code back
1047 
1048   {
<span class="line-modified">1049     MutexLockerEx ml_patch (Patching_lock, Mutex::_no_safepoint_check_flag);</span>
1050     //
1051     // Deoptimization may have happened while we waited for the lock.
1052     // In that case we don&#39;t bother to do any patching we just return
1053     // and let the deopt happen
1054     if (!caller_is_deopted()) {
1055       NativeGeneralJump* jump = nativeGeneralJump_at(caller_frame.pc());
1056       address instr_pc = jump-&gt;jump_destination();
1057       NativeInstruction* ni = nativeInstruction_at(instr_pc);
1058       if (ni-&gt;is_jump() ) {
1059         // the jump has not been patched yet
1060         // The jump destination is slow case and therefore not part of the stubs
1061         // (stubs are only for StaticCalls)
1062 
1063         // format of buffer
1064         //    ....
1065         //    instr byte 0     &lt;-- copy_buff
1066         //    instr byte 1
1067         //    ..
1068         //    instr byte n-1
1069         //      n
</pre>
<hr />
<pre>
1248 #ifdef PPC32
1249           { address instr_pc2 = instr_pc + NativeMovConstReg::lo_offset;
1250             RelocIterator iter2(nm, instr_pc2, instr_pc2 + 1);
1251             relocInfo::change_reloc_info_for_address(&amp;iter2, (address) instr_pc2,
1252                                                      relocInfo::none, rtype);
1253           }
1254 #endif
1255           }
1256 
1257         } else {
1258           ICache::invalidate_range(copy_buff, *byte_count);
1259           NativeGeneralJump::insert_unconditional(instr_pc, being_initialized_entry);
1260         }
1261       }
1262     }
1263   }
1264 
1265   // If we are patching in a non-perm oop, make sure the nmethod
1266   // is on the right list.
1267   {
<span class="line-modified">1268     MutexLockerEx ml_code (CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
1269     nmethod* nm = CodeCache::find_nmethod(caller_frame.pc());
1270     guarantee(nm != NULL, &quot;only nmethods can contain non-perm oops&quot;);
1271 
1272     // Since we&#39;ve patched some oops in the nmethod,
1273     // (re)register it with the heap.
1274     Universe::heap()-&gt;register_nmethod(nm);
1275   }
1276 JRT_END
1277 
1278 #else // DEOPTIMIZE_WHEN_PATCHING
1279 
1280 JRT_ENTRY(void, Runtime1::patch_code(JavaThread* thread, Runtime1::StubID stub_id ))
1281   RegisterMap reg_map(thread, false);
1282 
1283   NOT_PRODUCT(_patch_code_slowcase_cnt++;)
1284   if (TracePatching) {
1285     tty-&gt;print_cr(&quot;Deoptimizing because patch is needed&quot;);
1286   }
1287 
1288   frame runtime_frame = thread-&gt;last_frame();
</pre>
<hr />
<pre>
1411   // the return value as a boolean true.
1412 
1413   assert(mirror != NULL, &quot;should null-check on mirror before calling&quot;);
1414   Klass* k = java_lang_Class::as_Klass(mirror);
1415   return (k != NULL &amp;&amp; obj != NULL &amp;&amp; obj-&gt;is_a(k)) ? 1 : 0;
1416 JRT_END
1417 
1418 JRT_ENTRY(void, Runtime1::predicate_failed_trap(JavaThread* thread))
1419   ResourceMark rm;
1420 
1421   assert(!TieredCompilation, &quot;incompatible with tiered compilation&quot;);
1422 
1423   RegisterMap reg_map(thread, false);
1424   frame runtime_frame = thread-&gt;last_frame();
1425   frame caller_frame = runtime_frame.sender(&amp;reg_map);
1426 
1427   nmethod* nm = CodeCache::find_nmethod(caller_frame.pc());
1428   assert (nm != NULL, &quot;no more nmethod?&quot;);
1429   nm-&gt;make_not_entrant();
1430 
<span class="line-modified">1431   methodHandle m(nm-&gt;method());</span>
1432   MethodData* mdo = m-&gt;method_data();
1433 
1434   if (mdo == NULL &amp;&amp; !HAS_PENDING_EXCEPTION) {
1435     // Build an MDO.  Ignore errors like OutOfMemory;
1436     // that simply means we won&#39;t have an MDO to update.
1437     Method::build_interpreter_method_data(m, THREAD);
1438     if (HAS_PENDING_EXCEPTION) {
1439       assert((PENDING_EXCEPTION-&gt;is_a(SystemDictionary::OutOfMemoryError_klass())), &quot;we expect only an OOM error here&quot;);
1440       CLEAR_PENDING_EXCEPTION;
1441     }
1442     mdo = m-&gt;method_data();
1443   }
1444 
1445   if (mdo != NULL) {
1446     mdo-&gt;inc_trap_count(Deoptimization::Reason_none);
1447   }
1448 
1449   if (TracePredicateFailedTraps) {
1450     stringStream ss1, ss2;
1451     vframeStream vfst(thread);
<span class="line-modified">1452     methodHandle inlinee = methodHandle(vfst.method());</span>
1453     inlinee-&gt;print_short_name(&amp;ss1);
1454     m-&gt;print_short_name(&amp;ss2);
1455     tty-&gt;print_cr(&quot;Predicate failed trap in method %s at bci %d inlined in %s at pc &quot; INTPTR_FORMAT, ss1.as_string(), vfst.bci(), ss2.as_string(), p2i(caller_frame.pc()));
1456   }
1457 
1458 
1459   Deoptimization::deoptimize_frame(thread, caller_frame.id());
1460 
1461 JRT_END
1462 
1463 #ifndef PRODUCT
1464 void Runtime1::print_statistics() {
1465   tty-&gt;print_cr(&quot;C1 Runtime statistics:&quot;);
1466   tty-&gt;print_cr(&quot; _resolve_invoke_virtual_cnt:     %d&quot;, SharedRuntime::_resolve_virtual_ctr);
1467   tty-&gt;print_cr(&quot; _resolve_invoke_opt_virtual_cnt: %d&quot;, SharedRuntime::_resolve_opt_virtual_ctr);
1468   tty-&gt;print_cr(&quot; _resolve_invoke_static_cnt:      %d&quot;, SharedRuntime::_resolve_static_ctr);
1469   tty-&gt;print_cr(&quot; _handle_wrong_method_cnt:        %d&quot;, SharedRuntime::_wrong_method_ctr);
1470   tty-&gt;print_cr(&quot; _ic_miss_cnt:                    %d&quot;, SharedRuntime::_ic_miss_ctr);
1471   tty-&gt;print_cr(&quot; _generic_arraycopy_cnt:          %d&quot;, _generic_arraycopy_cnt);
1472   tty-&gt;print_cr(&quot; _generic_arraycopystub_cnt:      %d&quot;, _generic_arraycopystub_cnt);
</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1999, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/codeBuffer.hpp&quot;
  27 #include &quot;c1/c1_CodeStubs.hpp&quot;
  28 #include &quot;c1/c1_Defs.hpp&quot;
  29 #include &quot;c1/c1_FrameMap.hpp&quot;
  30 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  31 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  32 #include &quot;c1/c1_Runtime1.hpp&quot;
  33 #include &quot;classfile/systemDictionary.hpp&quot;
  34 #include &quot;classfile/vmSymbols.hpp&quot;
  35 #include &quot;code/codeBlob.hpp&quot;
  36 #include &quot;code/compiledIC.hpp&quot;
  37 #include &quot;code/pcDesc.hpp&quot;
  38 #include &quot;code/scopeDesc.hpp&quot;
  39 #include &quot;code/vtableStubs.hpp&quot;
<span class="line-added">  40 #include &quot;compiler/compilationPolicy.hpp&quot;</span>
  41 #include &quot;compiler/disassembler.hpp&quot;
  42 #include &quot;gc/shared/barrierSet.hpp&quot;
  43 #include &quot;gc/shared/c1/barrierSetC1.hpp&quot;
  44 #include &quot;gc/shared/collectedHeap.hpp&quot;
  45 #include &quot;interpreter/bytecode.hpp&quot;
  46 #include &quot;interpreter/interpreter.hpp&quot;
  47 #include &quot;jfr/support/jfrIntrinsics.hpp&quot;
  48 #include &quot;logging/log.hpp&quot;
  49 #include &quot;memory/allocation.inline.hpp&quot;
  50 #include &quot;memory/oopFactory.hpp&quot;
  51 #include &quot;memory/resourceArea.hpp&quot;
<span class="line-added">  52 #include &quot;memory/universe.hpp&quot;</span>
  53 #include &quot;oops/access.inline.hpp&quot;
  54 #include &quot;oops/objArrayOop.inline.hpp&quot;
  55 #include &quot;oops/objArrayKlass.hpp&quot;
  56 #include &quot;oops/oop.inline.hpp&quot;
  57 #include &quot;runtime/atomic.hpp&quot;
  58 #include &quot;runtime/biasedLocking.hpp&quot;

  59 #include &quot;runtime/fieldDescriptor.inline.hpp&quot;
  60 #include &quot;runtime/frame.inline.hpp&quot;
  61 #include &quot;runtime/handles.inline.hpp&quot;
  62 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  63 #include &quot;runtime/javaCalls.hpp&quot;
  64 #include &quot;runtime/sharedRuntime.hpp&quot;
  65 #include &quot;runtime/threadCritical.hpp&quot;
  66 #include &quot;runtime/vframe.inline.hpp&quot;
  67 #include &quot;runtime/vframeArray.hpp&quot;
  68 #include &quot;runtime/vm_version.hpp&quot;
  69 #include &quot;utilities/copy.hpp&quot;
  70 #include &quot;utilities/events.hpp&quot;
  71 
  72 
  73 // Implementation of StubAssembler
  74 
  75 StubAssembler::StubAssembler(CodeBuffer* code, const char * name, int stub_id) : C1_MacroAssembler(code) {
  76   _name = name;
  77   _must_gc_arguments = false;
  78   _frame_size = no_frame_size;
</pre>
<hr />
<pre>
 277   BarrierSetC1* bs = BarrierSet::barrier_set()-&gt;barrier_set_c1();
 278   bs-&gt;generate_c1_runtime_stubs(blob);
 279 }
 280 
 281 CodeBlob* Runtime1::blob_for(StubID id) {
 282   assert(0 &lt;= id &amp;&amp; id &lt; number_of_ids, &quot;illegal stub id&quot;);
 283   return _blobs[id];
 284 }
 285 
 286 
 287 const char* Runtime1::name_for(StubID id) {
 288   assert(0 &lt;= id &amp;&amp; id &lt; number_of_ids, &quot;illegal stub id&quot;);
 289   return _blob_names[id];
 290 }
 291 
 292 const char* Runtime1::name_for_address(address entry) {
 293   for (int id = 0; id &lt; number_of_ids; id++) {
 294     if (entry == entry_for((StubID)id)) return name_for((StubID)id);
 295   }
 296 
<span class="line-added"> 297   BarrierSetC1* bsc1 = BarrierSet::barrier_set()-&gt;barrier_set_c1();</span>
<span class="line-added"> 298   const char* name = bsc1-&gt;rtcall_name_for_address(entry);</span>
<span class="line-added"> 299   if (name != NULL) {</span>
<span class="line-added"> 300     return name;</span>
<span class="line-added"> 301   }</span>
<span class="line-added"> 302 </span>
 303 #define FUNCTION_CASE(a, f) \
 304   if ((intptr_t)a == CAST_FROM_FN_PTR(intptr_t, f))  return #f
 305 
 306   FUNCTION_CASE(entry, os::javaTimeMillis);
 307   FUNCTION_CASE(entry, os::javaTimeNanos);
 308   FUNCTION_CASE(entry, SharedRuntime::OSR_migration_end);
 309   FUNCTION_CASE(entry, SharedRuntime::d2f);
 310   FUNCTION_CASE(entry, SharedRuntime::d2i);
 311   FUNCTION_CASE(entry, SharedRuntime::d2l);
 312   FUNCTION_CASE(entry, SharedRuntime::dcos);
 313   FUNCTION_CASE(entry, SharedRuntime::dexp);
 314   FUNCTION_CASE(entry, SharedRuntime::dlog);
 315   FUNCTION_CASE(entry, SharedRuntime::dlog10);
 316   FUNCTION_CASE(entry, SharedRuntime::dpow);
 317   FUNCTION_CASE(entry, SharedRuntime::drem);
 318   FUNCTION_CASE(entry, SharedRuntime::dsin);
 319   FUNCTION_CASE(entry, SharedRuntime::dtan);
 320   FUNCTION_CASE(entry, SharedRuntime::f2i);
 321   FUNCTION_CASE(entry, SharedRuntime::f2l);
 322   FUNCTION_CASE(entry, SharedRuntime::frem);
</pre>
<hr />
<pre>
 564   }
 565 
 566   // If the stack guard pages are enabled, check whether there is a handler in
 567   // the current method.  Otherwise (guard pages disabled), force an unwind and
 568   // skip the exception cache update (i.e., just leave continuation==NULL).
 569   address continuation = NULL;
 570   if (guard_pages_enabled) {
 571 
 572     // New exception handling mechanism can support inlined methods
 573     // with exception handlers since the mappings are from PC to PC
 574 
 575     // debugging support
 576     // tracing
 577     if (log_is_enabled(Info, exceptions)) {
 578       ResourceMark rm;
 579       stringStream tempst;
 580       assert(nm-&gt;method() != NULL, &quot;Unexpected NULL method()&quot;);
 581       tempst.print(&quot;compiled method &lt;%s&gt;\n&quot;
 582                    &quot; at PC&quot; INTPTR_FORMAT &quot; for thread &quot; INTPTR_FORMAT,
 583                    nm-&gt;method()-&gt;print_value_string(), p2i(pc), p2i(thread));
<span class="line-modified"> 584       Exceptions::log_exception(exception, tempst.as_string());</span>
 585     }
 586     // for AbortVMOnException flag
 587     Exceptions::debug_check_abort(exception);
 588 
 589     // Clear out the exception oop and pc since looking up an
 590     // exception handler can cause class loading, which might throw an
 591     // exception and those fields are expected to be clear during
 592     // normal bytecode execution.
 593     thread-&gt;clear_exception_oop_and_pc();
 594 
 595     bool recursive_exception = false;
 596     continuation = SharedRuntime::compute_compiled_exc_handler(nm, pc, exception, false, false, recursive_exception);
 597     // If an exception was thrown during exception dispatch, the exception oop may have changed
 598     thread-&gt;set_exception_oop(exception());
 599     thread-&gt;set_exception_pc(pc);
 600 
 601     // the exception cache is used only by non-implicit exceptions
 602     // Update the exception cache only when there didn&#39;t happen
 603     // another exception during the computation of the compiled
 604     // exception handler. Checking for exception oop equality is not
</pre>
<hr />
<pre>
 688   char* message = SharedRuntime::generate_class_cast_message(
 689     thread, object-&gt;klass());
 690   SharedRuntime::throw_and_post_jvmti_exception(
 691     thread, vmSymbols::java_lang_ClassCastException(), message);
 692 JRT_END
 693 
 694 
 695 JRT_ENTRY(void, Runtime1::throw_incompatible_class_change_error(JavaThread* thread))
 696   NOT_PRODUCT(_throw_incompatible_class_change_error_count++;)
 697   ResourceMark rm(thread);
 698   SharedRuntime::throw_and_post_jvmti_exception(thread, vmSymbols::java_lang_IncompatibleClassChangeError());
 699 JRT_END
 700 
 701 
 702 JRT_ENTRY_NO_ASYNC(void, Runtime1::monitorenter(JavaThread* thread, oopDesc* obj, BasicObjectLock* lock))
 703   NOT_PRODUCT(_monitorenter_slowcase_cnt++;)
 704   if (PrintBiasedLockingStatistics) {
 705     Atomic::inc(BiasedLocking::slow_path_entry_count_addr());
 706   }
 707   Handle h_obj(thread, obj);
<span class="line-modified"> 708   if (!UseFastLocking) {</span>
<span class="line-modified"> 709     lock-&gt;set_obj(obj);</span>










 710   }
<span class="line-added"> 711   assert(obj == lock-&gt;obj(), &quot;must match&quot;);</span>
<span class="line-added"> 712   ObjectSynchronizer::enter(h_obj, lock-&gt;lock(), THREAD);</span>
 713 JRT_END
 714 
 715 
 716 JRT_LEAF(void, Runtime1::monitorexit(JavaThread* thread, BasicObjectLock* lock))
 717   NOT_PRODUCT(_monitorexit_slowcase_cnt++;)
 718   assert(thread == JavaThread::current(), &quot;threads must correspond&quot;);
 719   assert(thread-&gt;last_Java_sp(), &quot;last_Java_sp must be set&quot;);
 720   // monitorexit is non-blocking (leaf routine) =&gt; no exceptions can be thrown
 721   EXCEPTION_MARK;
 722 
 723   oop obj = lock-&gt;obj();
 724   assert(oopDesc::is_oop(obj), &quot;must be NULL or an object&quot;);
<span class="line-modified"> 725   ObjectSynchronizer::exit(obj, lock-&gt;lock(), THREAD);</span>





 726 JRT_END
 727 
 728 // Cf. OptoRuntime::deoptimize_caller_frame
 729 JRT_ENTRY(void, Runtime1::deoptimize(JavaThread* thread, jint trap_request))
 730   // Called from within the owner thread, so no need for safepoint
 731   RegisterMap reg_map(thread, false);
 732   frame stub_frame = thread-&gt;last_frame();
 733   assert(stub_frame.is_runtime_frame(), &quot;Sanity check&quot;);
 734   frame caller_frame = stub_frame.sender(&amp;reg_map);
 735   nmethod* nm = caller_frame.cb()-&gt;as_nmethod_or_null();
 736   assert(nm != NULL, &quot;Sanity check&quot;);
 737   methodHandle method(thread, nm-&gt;method());
 738   assert(nm == CodeCache::find_nmethod(caller_frame.pc()), &quot;Should be the same&quot;);
 739   Deoptimization::DeoptAction action = Deoptimization::trap_request_action(trap_request);
 740   Deoptimization::DeoptReason reason = Deoptimization::trap_request_reason(trap_request);
 741 
 742   if (action == Deoptimization::Action_make_not_entrant) {
 743     if (nm-&gt;make_not_entrant()) {
 744       if (reason == Deoptimization::Reason_tenured) {
 745         MethodData* trap_mdo = Deoptimization::get_method_data(thread, method, true /*create_if_missing*/);
</pre>
<hr />
<pre>
1023       if (deoptimize_for_atomic) {
1024         tty-&gt;print_cr(&quot;Deoptimizing for patching atomic field reference&quot;);
1025       }
1026     }
1027 
1028     // It&#39;s possible the nmethod was invalidated in the last
1029     // safepoint, but if it&#39;s still alive then make it not_entrant.
1030     nmethod* nm = CodeCache::find_nmethod(caller_frame.pc());
1031     if (nm != NULL) {
1032       nm-&gt;make_not_entrant();
1033     }
1034 
1035     Deoptimization::deoptimize_frame(thread, caller_frame.id());
1036 
1037     // Return to the now deoptimized frame.
1038   }
1039 
1040   // Now copy code back
1041 
1042   {
<span class="line-modified">1043     MutexLocker ml_patch (THREAD, Patching_lock, Mutex::_no_safepoint_check_flag);</span>
1044     //
1045     // Deoptimization may have happened while we waited for the lock.
1046     // In that case we don&#39;t bother to do any patching we just return
1047     // and let the deopt happen
1048     if (!caller_is_deopted()) {
1049       NativeGeneralJump* jump = nativeGeneralJump_at(caller_frame.pc());
1050       address instr_pc = jump-&gt;jump_destination();
1051       NativeInstruction* ni = nativeInstruction_at(instr_pc);
1052       if (ni-&gt;is_jump() ) {
1053         // the jump has not been patched yet
1054         // The jump destination is slow case and therefore not part of the stubs
1055         // (stubs are only for StaticCalls)
1056 
1057         // format of buffer
1058         //    ....
1059         //    instr byte 0     &lt;-- copy_buff
1060         //    instr byte 1
1061         //    ..
1062         //    instr byte n-1
1063         //      n
</pre>
<hr />
<pre>
1242 #ifdef PPC32
1243           { address instr_pc2 = instr_pc + NativeMovConstReg::lo_offset;
1244             RelocIterator iter2(nm, instr_pc2, instr_pc2 + 1);
1245             relocInfo::change_reloc_info_for_address(&amp;iter2, (address) instr_pc2,
1246                                                      relocInfo::none, rtype);
1247           }
1248 #endif
1249           }
1250 
1251         } else {
1252           ICache::invalidate_range(copy_buff, *byte_count);
1253           NativeGeneralJump::insert_unconditional(instr_pc, being_initialized_entry);
1254         }
1255       }
1256     }
1257   }
1258 
1259   // If we are patching in a non-perm oop, make sure the nmethod
1260   // is on the right list.
1261   {
<span class="line-modified">1262     MutexLocker ml_code (THREAD, CodeCache_lock, Mutex::_no_safepoint_check_flag);</span>
1263     nmethod* nm = CodeCache::find_nmethod(caller_frame.pc());
1264     guarantee(nm != NULL, &quot;only nmethods can contain non-perm oops&quot;);
1265 
1266     // Since we&#39;ve patched some oops in the nmethod,
1267     // (re)register it with the heap.
1268     Universe::heap()-&gt;register_nmethod(nm);
1269   }
1270 JRT_END
1271 
1272 #else // DEOPTIMIZE_WHEN_PATCHING
1273 
1274 JRT_ENTRY(void, Runtime1::patch_code(JavaThread* thread, Runtime1::StubID stub_id ))
1275   RegisterMap reg_map(thread, false);
1276 
1277   NOT_PRODUCT(_patch_code_slowcase_cnt++;)
1278   if (TracePatching) {
1279     tty-&gt;print_cr(&quot;Deoptimizing because patch is needed&quot;);
1280   }
1281 
1282   frame runtime_frame = thread-&gt;last_frame();
</pre>
<hr />
<pre>
1405   // the return value as a boolean true.
1406 
1407   assert(mirror != NULL, &quot;should null-check on mirror before calling&quot;);
1408   Klass* k = java_lang_Class::as_Klass(mirror);
1409   return (k != NULL &amp;&amp; obj != NULL &amp;&amp; obj-&gt;is_a(k)) ? 1 : 0;
1410 JRT_END
1411 
1412 JRT_ENTRY(void, Runtime1::predicate_failed_trap(JavaThread* thread))
1413   ResourceMark rm;
1414 
1415   assert(!TieredCompilation, &quot;incompatible with tiered compilation&quot;);
1416 
1417   RegisterMap reg_map(thread, false);
1418   frame runtime_frame = thread-&gt;last_frame();
1419   frame caller_frame = runtime_frame.sender(&amp;reg_map);
1420 
1421   nmethod* nm = CodeCache::find_nmethod(caller_frame.pc());
1422   assert (nm != NULL, &quot;no more nmethod?&quot;);
1423   nm-&gt;make_not_entrant();
1424 
<span class="line-modified">1425   methodHandle m(thread, nm-&gt;method());</span>
1426   MethodData* mdo = m-&gt;method_data();
1427 
1428   if (mdo == NULL &amp;&amp; !HAS_PENDING_EXCEPTION) {
1429     // Build an MDO.  Ignore errors like OutOfMemory;
1430     // that simply means we won&#39;t have an MDO to update.
1431     Method::build_interpreter_method_data(m, THREAD);
1432     if (HAS_PENDING_EXCEPTION) {
1433       assert((PENDING_EXCEPTION-&gt;is_a(SystemDictionary::OutOfMemoryError_klass())), &quot;we expect only an OOM error here&quot;);
1434       CLEAR_PENDING_EXCEPTION;
1435     }
1436     mdo = m-&gt;method_data();
1437   }
1438 
1439   if (mdo != NULL) {
1440     mdo-&gt;inc_trap_count(Deoptimization::Reason_none);
1441   }
1442 
1443   if (TracePredicateFailedTraps) {
1444     stringStream ss1, ss2;
1445     vframeStream vfst(thread);
<span class="line-modified">1446     Method* inlinee = vfst.method();</span>
1447     inlinee-&gt;print_short_name(&amp;ss1);
1448     m-&gt;print_short_name(&amp;ss2);
1449     tty-&gt;print_cr(&quot;Predicate failed trap in method %s at bci %d inlined in %s at pc &quot; INTPTR_FORMAT, ss1.as_string(), vfst.bci(), ss2.as_string(), p2i(caller_frame.pc()));
1450   }
1451 
1452 
1453   Deoptimization::deoptimize_frame(thread, caller_frame.id());
1454 
1455 JRT_END
1456 
1457 #ifndef PRODUCT
1458 void Runtime1::print_statistics() {
1459   tty-&gt;print_cr(&quot;C1 Runtime statistics:&quot;);
1460   tty-&gt;print_cr(&quot; _resolve_invoke_virtual_cnt:     %d&quot;, SharedRuntime::_resolve_virtual_ctr);
1461   tty-&gt;print_cr(&quot; _resolve_invoke_opt_virtual_cnt: %d&quot;, SharedRuntime::_resolve_opt_virtual_ctr);
1462   tty-&gt;print_cr(&quot; _resolve_invoke_static_cnt:      %d&quot;, SharedRuntime::_resolve_static_ctr);
1463   tty-&gt;print_cr(&quot; _handle_wrong_method_cnt:        %d&quot;, SharedRuntime::_wrong_method_ctr);
1464   tty-&gt;print_cr(&quot; _ic_miss_cnt:                    %d&quot;, SharedRuntime::_ic_miss_ctr);
1465   tty-&gt;print_cr(&quot; _generic_arraycopy_cnt:          %d&quot;, _generic_arraycopy_cnt);
1466   tty-&gt;print_cr(&quot; _generic_arraycopystub_cnt:      %d&quot;, _generic_arraycopystub_cnt);
</pre>
</td>
</tr>
</table>
<center><a href="c1_RangeCheckElimination.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_ValueMap.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>