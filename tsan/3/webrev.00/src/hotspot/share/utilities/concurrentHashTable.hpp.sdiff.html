<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/utilities/concurrentHashTable.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="compilerWarnings_gcc.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="concurrentHashTable.inline.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/utilities/concurrentHashTable.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_UTILITIES_CONCURRENTHASHTABLE_HPP
 26 #define SHARE_UTILITIES_CONCURRENTHASHTABLE_HPP
 27 
 28 #include &quot;memory/allocation.hpp&quot;
 29 #include &quot;utilities/globalCounter.hpp&quot;
 30 #include &quot;utilities/globalDefinitions.hpp&quot;

 31 
 32 // A mostly concurrent-hash-table where the read-side is wait-free, inserts are
 33 // CAS and deletes mutual exclude each other on per bucket-basis. VALUE is the
 34 // type kept inside each Node and CONFIG contains hash and allocation methods.
 35 // A CALLBACK_FUNC and LOOKUP_FUNC needs to be provided for get and insert.
 36 
 37 class Thread;
 38 class Mutex;
 39 
<span class="line-modified"> 40 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
 41 class ConcurrentHashTable : public CHeapObj&lt;F&gt; {

 42  private:
 43   // This is the internal node structure.
 44   // Only constructed with placement new from memory allocated with MEMFLAGS of
 45   // the InternalTable or user-defined memory.
 46   class Node {
 47    private:
 48     Node * volatile _next;
 49     VALUE _value;
 50    public:
 51     Node(const VALUE&amp; value, Node* next = NULL)
 52       : _next(next), _value(value) {
 53       assert((((uintptr_t)this) &amp; ((uintptr_t)0x3)) == 0,
 54              &quot;Must 16 bit aligned.&quot;);
 55     }
 56 
 57     Node* next() const;
 58     void set_next(Node* node)         { _next = node; }
 59     Node* const volatile * next_ptr() { return &amp;_next; }
 60 
 61     VALUE* value()                    { return &amp;_value; }
</pre>
<hr />
<pre>
234   void lock_resize_lock(Thread* locker);
235   // Unlocks mutex and state.
236   void unlock_resize_lock(Thread* locker);
237 
238   // This method sets the _invisible_epoch and do a write_synchronize.
239   // Subsequent calls check the state of _invisible_epoch and determine if the
240   // write_synchronize can be avoided. If not, it sets the _invisible_epoch
241   // again and do a write_synchronize.
242   void write_synchonize_on_visible_epoch(Thread* thread);
243   // To be-able to avoid write_synchronize in resize and other bulk operation,
244   // this field keep tracks if a version of the hash-table was ever been seen.
245   // We the working thread pointer as tag for debugging. The _invisible_epoch
246   // can only be used by the owner of _resize_lock.
247   volatile Thread* _invisible_epoch;
248 
249   // Scoped critical section, which also handles the invisible epochs.
250   // An invisible epoch/version do not need a write_synchronize().
251   class ScopedCS: public StackObj {
252    protected:
253     Thread* _thread;
<span class="line-modified">254     ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;* _cht;</span>
255     GlobalCounter::CSContext _cs_context;
256    public:
<span class="line-modified">257     ScopedCS(Thread* thread, ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;* cht);</span>
258     ~ScopedCS();
259   };
260 
261 
262   // Max number of deletes in one bucket chain during bulk delete.
263   static const size_t BULK_DELETE_LIMIT = 256;
264 
265   // Simple getters and setters for the internal table.
266   InternalTable* get_table() const;
267   InternalTable* get_new_table() const;
268   InternalTable* set_table_from_new();
269 
270   // Destroys all nodes.
271   void free_nodes();
272 
273   // Mask away high bits of hash.
274   static size_t bucket_idx_hash(InternalTable* table, const uintx hash) {
275     return ((size_t)hash) &amp; table-&gt;_hash_mask;
276   }
277 
</pre>
<hr />
<pre>
363 
364   // Check for dead items in this table with range. During shrink/grow we cannot
365   // guarantee that we only visit nodes once. To keep it simple caller will
366   // have locked _resize_lock.
367   template &lt;typename EVALUATE_FUNC, typename DELETE_FUNC&gt;
368   void do_bulk_delete_locked_for(Thread* thread, size_t start_idx,
369                                  size_t stop_idx, EVALUATE_FUNC&amp; eval_f,
370                                  DELETE_FUNC&amp; del_f, bool is_mt = false);
371 
372   // Method to delete one items.
373   template &lt;typename LOOKUP_FUNC&gt;
374   void delete_in_bucket(Thread* thread, Bucket* bucket, LOOKUP_FUNC&amp; lookup_f);
375 
376  public:
377   ConcurrentHashTable(size_t log2size = DEFAULT_START_SIZE_LOG2,
378                       size_t log2size_limit = DEFAULT_MAX_SIZE_LOG2,
379                       size_t grow_hint = DEFAULT_GROW_HINT);
380 
381   ~ConcurrentHashTable();
382 


383   size_t get_size_log2(Thread* thread);
384   size_t get_node_size() const { return sizeof(Node); }
385   bool is_max_size_reached() { return _size_limit_reached; }
386 
387   // This means no paused bucket resize operation is going to resume
388   // on this table.
389   bool is_safepoint_safe() { return _resize_lock_owner == NULL; }
390 
391   // Re-size operations.
392   bool shrink(Thread* thread, size_t size_limit_log2 = 0);
393   bool grow(Thread* thread, size_t size_limit_log2 = 0);
394 
395   // All callbacks for get are under critical sections. Other callbacks may be
396   // under critical section or may have locked parts of table. Calling any
397   // methods on the table during a callback is not supported.Only MultiGetHandle
398   // supports multiple gets.
399 
400   // Get methods return true on found item with LOOKUP_FUNC and FOUND_FUNC is
401   // called.
402   template &lt;typename LOOKUP_FUNC, typename FOUND_FUNC&gt;
</pre>
<hr />
<pre>
437   template &lt;typename SCAN_FUNC&gt;
438   void do_scan(Thread* thread, SCAN_FUNC&amp; scan_f);
439 
440   // Visit all items with SCAN_FUNC without any protection.
441   // It will assume there is no other thread accessing this
442   // table during the safepoint. Must be called with VM thread.
443   template &lt;typename SCAN_FUNC&gt;
444   void do_safepoint_scan(SCAN_FUNC&amp; scan_f);
445 
446   // Destroying items matching EVALUATE_FUNC, before destroying items
447   // DELETE_FUNC is called, if resize lock is obtained. Else returns false.
448   template &lt;typename EVALUATE_FUNC, typename DELETE_FUNC&gt;
449   bool try_bulk_delete(Thread* thread, EVALUATE_FUNC&amp; eval_f,
450                        DELETE_FUNC&amp; del_f);
451 
452   // Destroying items matching EVALUATE_FUNC, before destroying items
453   // DELETE_FUNC is called, when the resize lock is successfully obtained.
454   template &lt;typename EVALUATE_FUNC, typename DELETE_FUNC&gt;
455   void bulk_delete(Thread* thread, EVALUATE_FUNC&amp; eval_f, DELETE_FUNC&amp; del_f);
456 









457   // Writes statistics to the outputStream. Item sizes are calculated with
458   // VALUE_SIZE_FUNC.
459   template &lt;typename VALUE_SIZE_FUNC&gt;
460   void statistics_to(Thread* thread, VALUE_SIZE_FUNC&amp; vs_f, outputStream* st,
461                      const char* table_name);
462 
463   // Moves all nodes from this table to to_cht
<span class="line-modified">464   bool try_move_nodes_to(Thread* thread, ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;* to_cht);</span>
<span class="line-removed">465 </span>
<span class="line-removed">466   // This is a Curiously Recurring Template Pattern (CRPT) interface for the</span>
<span class="line-removed">467   // specialization.</span>
<span class="line-removed">468   struct BaseConfig {</span>
<span class="line-removed">469    public:</span>
<span class="line-removed">470     // Called when the hash table needs the hash for a VALUE.</span>
<span class="line-removed">471     static uintx get_hash(const VALUE&amp; value, bool* dead) {</span>
<span class="line-removed">472       return CONFIG::get_hash(value, dead);</span>
<span class="line-removed">473     }</span>
<span class="line-removed">474     // Default node allocation.</span>
<span class="line-removed">475     static void* allocate_node(size_t size, const VALUE&amp; value);</span>
<span class="line-removed">476     // Default node reclamation.</span>
<span class="line-removed">477     static void free_node(void* memory, const VALUE&amp; value);</span>
<span class="line-removed">478   };</span>
479 
480   // Scoped multi getter.
481   class MultiGetHandle : private ScopedCS {
482    public:
<span class="line-modified">483     MultiGetHandle(Thread* thread, ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;* cht)</span>
484       : ScopedCS(thread, cht) {}
485     // In the MultiGetHandle scope you can lookup items matching LOOKUP_FUNC.
486     // The VALUEs are safe as long as you never save the VALUEs outside the
487     // scope, e.g. after ~MultiGetHandle().
488     template &lt;typename LOOKUP_FUNC&gt;
489     VALUE* get(LOOKUP_FUNC&amp; lookup_f, bool* grow_hint = NULL);
490   };
491 
492  private:
493   class BucketsOperation;
494 
495  public:
496   class BulkDeleteTask;
497   class GrowTask;
498 };
499 
500 #endif // SHARE_UTILITIES_CONCURRENTHASHTABLE_HPP
</pre>
</td>
<td>
<hr />
<pre>
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_UTILITIES_CONCURRENTHASHTABLE_HPP
 26 #define SHARE_UTILITIES_CONCURRENTHASHTABLE_HPP
 27 
 28 #include &quot;memory/allocation.hpp&quot;
 29 #include &quot;utilities/globalCounter.hpp&quot;
 30 #include &quot;utilities/globalDefinitions.hpp&quot;
<span class="line-added"> 31 #include &quot;utilities/tableStatistics.hpp&quot;</span>
 32 
 33 // A mostly concurrent-hash-table where the read-side is wait-free, inserts are
 34 // CAS and deletes mutual exclude each other on per bucket-basis. VALUE is the
 35 // type kept inside each Node and CONFIG contains hash and allocation methods.
 36 // A CALLBACK_FUNC and LOOKUP_FUNC needs to be provided for get and insert.
 37 
 38 class Thread;
 39 class Mutex;
 40 
<span class="line-modified"> 41 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
 42 class ConcurrentHashTable : public CHeapObj&lt;F&gt; {
<span class="line-added"> 43   typedef typename CONFIG::Value VALUE;</span>
 44  private:
 45   // This is the internal node structure.
 46   // Only constructed with placement new from memory allocated with MEMFLAGS of
 47   // the InternalTable or user-defined memory.
 48   class Node {
 49    private:
 50     Node * volatile _next;
 51     VALUE _value;
 52    public:
 53     Node(const VALUE&amp; value, Node* next = NULL)
 54       : _next(next), _value(value) {
 55       assert((((uintptr_t)this) &amp; ((uintptr_t)0x3)) == 0,
 56              &quot;Must 16 bit aligned.&quot;);
 57     }
 58 
 59     Node* next() const;
 60     void set_next(Node* node)         { _next = node; }
 61     Node* const volatile * next_ptr() { return &amp;_next; }
 62 
 63     VALUE* value()                    { return &amp;_value; }
</pre>
<hr />
<pre>
236   void lock_resize_lock(Thread* locker);
237   // Unlocks mutex and state.
238   void unlock_resize_lock(Thread* locker);
239 
240   // This method sets the _invisible_epoch and do a write_synchronize.
241   // Subsequent calls check the state of _invisible_epoch and determine if the
242   // write_synchronize can be avoided. If not, it sets the _invisible_epoch
243   // again and do a write_synchronize.
244   void write_synchonize_on_visible_epoch(Thread* thread);
245   // To be-able to avoid write_synchronize in resize and other bulk operation,
246   // this field keep tracks if a version of the hash-table was ever been seen.
247   // We the working thread pointer as tag for debugging. The _invisible_epoch
248   // can only be used by the owner of _resize_lock.
249   volatile Thread* _invisible_epoch;
250 
251   // Scoped critical section, which also handles the invisible epochs.
252   // An invisible epoch/version do not need a write_synchronize().
253   class ScopedCS: public StackObj {
254    protected:
255     Thread* _thread;
<span class="line-modified">256     ConcurrentHashTable&lt;CONFIG, F&gt;* _cht;</span>
257     GlobalCounter::CSContext _cs_context;
258    public:
<span class="line-modified">259     ScopedCS(Thread* thread, ConcurrentHashTable&lt;CONFIG, F&gt;* cht);</span>
260     ~ScopedCS();
261   };
262 
263 
264   // Max number of deletes in one bucket chain during bulk delete.
265   static const size_t BULK_DELETE_LIMIT = 256;
266 
267   // Simple getters and setters for the internal table.
268   InternalTable* get_table() const;
269   InternalTable* get_new_table() const;
270   InternalTable* set_table_from_new();
271 
272   // Destroys all nodes.
273   void free_nodes();
274 
275   // Mask away high bits of hash.
276   static size_t bucket_idx_hash(InternalTable* table, const uintx hash) {
277     return ((size_t)hash) &amp; table-&gt;_hash_mask;
278   }
279 
</pre>
<hr />
<pre>
365 
366   // Check for dead items in this table with range. During shrink/grow we cannot
367   // guarantee that we only visit nodes once. To keep it simple caller will
368   // have locked _resize_lock.
369   template &lt;typename EVALUATE_FUNC, typename DELETE_FUNC&gt;
370   void do_bulk_delete_locked_for(Thread* thread, size_t start_idx,
371                                  size_t stop_idx, EVALUATE_FUNC&amp; eval_f,
372                                  DELETE_FUNC&amp; del_f, bool is_mt = false);
373 
374   // Method to delete one items.
375   template &lt;typename LOOKUP_FUNC&gt;
376   void delete_in_bucket(Thread* thread, Bucket* bucket, LOOKUP_FUNC&amp; lookup_f);
377 
378  public:
379   ConcurrentHashTable(size_t log2size = DEFAULT_START_SIZE_LOG2,
380                       size_t log2size_limit = DEFAULT_MAX_SIZE_LOG2,
381                       size_t grow_hint = DEFAULT_GROW_HINT);
382 
383   ~ConcurrentHashTable();
384 
<span class="line-added">385   TableRateStatistics _stats_rate;</span>
<span class="line-added">386 </span>
387   size_t get_size_log2(Thread* thread);
388   size_t get_node_size() const { return sizeof(Node); }
389   bool is_max_size_reached() { return _size_limit_reached; }
390 
391   // This means no paused bucket resize operation is going to resume
392   // on this table.
393   bool is_safepoint_safe() { return _resize_lock_owner == NULL; }
394 
395   // Re-size operations.
396   bool shrink(Thread* thread, size_t size_limit_log2 = 0);
397   bool grow(Thread* thread, size_t size_limit_log2 = 0);
398 
399   // All callbacks for get are under critical sections. Other callbacks may be
400   // under critical section or may have locked parts of table. Calling any
401   // methods on the table during a callback is not supported.Only MultiGetHandle
402   // supports multiple gets.
403 
404   // Get methods return true on found item with LOOKUP_FUNC and FOUND_FUNC is
405   // called.
406   template &lt;typename LOOKUP_FUNC, typename FOUND_FUNC&gt;
</pre>
<hr />
<pre>
441   template &lt;typename SCAN_FUNC&gt;
442   void do_scan(Thread* thread, SCAN_FUNC&amp; scan_f);
443 
444   // Visit all items with SCAN_FUNC without any protection.
445   // It will assume there is no other thread accessing this
446   // table during the safepoint. Must be called with VM thread.
447   template &lt;typename SCAN_FUNC&gt;
448   void do_safepoint_scan(SCAN_FUNC&amp; scan_f);
449 
450   // Destroying items matching EVALUATE_FUNC, before destroying items
451   // DELETE_FUNC is called, if resize lock is obtained. Else returns false.
452   template &lt;typename EVALUATE_FUNC, typename DELETE_FUNC&gt;
453   bool try_bulk_delete(Thread* thread, EVALUATE_FUNC&amp; eval_f,
454                        DELETE_FUNC&amp; del_f);
455 
456   // Destroying items matching EVALUATE_FUNC, before destroying items
457   // DELETE_FUNC is called, when the resize lock is successfully obtained.
458   template &lt;typename EVALUATE_FUNC, typename DELETE_FUNC&gt;
459   void bulk_delete(Thread* thread, EVALUATE_FUNC&amp; eval_f, DELETE_FUNC&amp; del_f);
460 
<span class="line-added">461   // Calcuate statistics. Item sizes are calculated with VALUE_SIZE_FUNC.</span>
<span class="line-added">462   template &lt;typename VALUE_SIZE_FUNC&gt;</span>
<span class="line-added">463   TableStatistics statistics_calculate(Thread* thread, VALUE_SIZE_FUNC&amp; vs_f);</span>
<span class="line-added">464 </span>
<span class="line-added">465   // Gets statistics if available, if not return old one. Item sizes are calculated with</span>
<span class="line-added">466   // VALUE_SIZE_FUNC.</span>
<span class="line-added">467   template &lt;typename VALUE_SIZE_FUNC&gt;</span>
<span class="line-added">468   TableStatistics statistics_get(Thread* thread, VALUE_SIZE_FUNC&amp; vs_f, TableStatistics old);</span>
<span class="line-added">469 </span>
470   // Writes statistics to the outputStream. Item sizes are calculated with
471   // VALUE_SIZE_FUNC.
472   template &lt;typename VALUE_SIZE_FUNC&gt;
473   void statistics_to(Thread* thread, VALUE_SIZE_FUNC&amp; vs_f, outputStream* st,
474                      const char* table_name);
475 
476   // Moves all nodes from this table to to_cht
<span class="line-modified">477   bool try_move_nodes_to(Thread* thread, ConcurrentHashTable&lt;CONFIG, F&gt;* to_cht);</span>














478 
479   // Scoped multi getter.
480   class MultiGetHandle : private ScopedCS {
481    public:
<span class="line-modified">482     MultiGetHandle(Thread* thread, ConcurrentHashTable&lt;CONFIG, F&gt;* cht)</span>
483       : ScopedCS(thread, cht) {}
484     // In the MultiGetHandle scope you can lookup items matching LOOKUP_FUNC.
485     // The VALUEs are safe as long as you never save the VALUEs outside the
486     // scope, e.g. after ~MultiGetHandle().
487     template &lt;typename LOOKUP_FUNC&gt;
488     VALUE* get(LOOKUP_FUNC&amp; lookup_f, bool* grow_hint = NULL);
489   };
490 
491  private:
492   class BucketsOperation;
493 
494  public:
495   class BulkDeleteTask;
496   class GrowTask;
497 };
498 
499 #endif // SHARE_UTILITIES_CONCURRENTHASHTABLE_HPP
</pre>
</td>
</tr>
</table>
<center><a href="compilerWarnings_gcc.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="concurrentHashTable.inline.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>