<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/utilities/concurrentHashTable.inline.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="concurrentHashTable.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="concurrentHashTableTasks.inline.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/utilities/concurrentHashTable.inline.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  36 
  37 // 2^30 = 1G buckets
  38 #define SIZE_BIG_LOG2 30
  39 // 2^5  = 32 buckets
  40 #define SIZE_SMALL_LOG2 5
  41 
  42 // Number from spinYield.hpp. In some loops SpinYield would be unfair.
  43 #define SPINPAUSES_PER_YIELD 8192
  44 
  45 #ifdef ASSERT
  46 #ifdef _LP64
  47 // Two low bits are not usable.
  48 static const void* POISON_PTR = (void*)UCONST64(0xfbadbadbadbadbac);
  49 #else
  50 // Two low bits are not usable.
  51 static const void* POISON_PTR = (void*)0xffbadbac;
  52 #endif
  53 #endif
  54 
  55 // Node
<span class="line-modified">  56 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">  57 inline typename ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::Node*</span>
<span class="line-modified">  58 ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
  59   Node::next() const
  60 {
<span class="line-modified">  61   return OrderAccess::load_acquire(&amp;_next);</span>
  62 }
  63 
  64 // Bucket
<span class="line-modified">  65 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">  66 inline typename ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::Node*</span>
<span class="line-modified">  67 ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
  68   Bucket::first_raw() const
  69 {
<span class="line-modified">  70   return OrderAccess::load_acquire(&amp;_first);</span>
  71 }
  72 
<span class="line-modified">  73 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">  74 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
  75   Bucket::release_assign_node_ptr(
<span class="line-modified">  76     typename ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::Node* const volatile * dst,</span>
<span class="line-modified">  77     typename ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::Node* node) const</span>
  78 {
  79   // Due to this assert this methods is not static.
  80   assert(is_locked(), &quot;Must be locked.&quot;);
  81   Node** tmp = (Node**)dst;
<span class="line-modified">  82   OrderAccess::release_store(tmp, clear_set_state(node, *dst));</span>
  83 }
  84 
<span class="line-modified">  85 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">  86 inline typename ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::Node*</span>
<span class="line-modified">  87 ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
  88   Bucket::first() const
  89 {
  90   // We strip the states bit before returning the ptr.
<span class="line-modified">  91   return clear_state(OrderAccess::load_acquire(&amp;_first));</span>
  92 }
  93 
<span class="line-modified">  94 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">  95 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
  96   Bucket::have_redirect() const
  97 {
  98   return is_state(first_raw(), STATE_REDIRECT_BIT);
  99 }
 100 
<span class="line-modified"> 101 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 102 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 103   Bucket::is_locked() const
 104 {
 105   return is_state(first_raw(), STATE_LOCK_BIT);
 106 }
 107 
<span class="line-modified"> 108 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 109 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 110   Bucket::lock()
 111 {
 112   int i = 0;
 113   // SpinYield would be unfair here
 114   while (!this-&gt;trylock()) {
 115     if ((++i) == SPINPAUSES_PER_YIELD) {
 116       // On contemporary OS yielding will give CPU to another runnable thread if
 117       // there is no CPU available.
 118       os::naked_yield();
 119       i = 0;
 120     } else {
 121       SpinPause();
 122     }
 123   }
 124 }
 125 
<span class="line-modified"> 126 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 127 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 128   Bucket::release_assign_last_node_next(
<span class="line-modified"> 129      typename ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::Node* node)</span>
 130 {
 131   assert(is_locked(), &quot;Must be locked.&quot;);
 132   Node* const volatile * ret = first_ptr();
 133   while (clear_state(*ret) != NULL) {
 134     ret = clear_state(*ret)-&gt;next_ptr();
 135   }
 136   release_assign_node_ptr(ret, node);
 137 }
 138 
<span class="line-modified"> 139 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 140 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
<span class="line-modified"> 141   Bucket::cas_first(typename ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::Node* node,</span>
<span class="line-modified"> 142                     typename ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::Node* expect</span>
 143                     )
 144 {
 145   if (is_locked()) {
 146     return false;
 147   }
<span class="line-modified"> 148   if (Atomic::cmpxchg(node, &amp;_first, expect) == expect) {</span>
 149     return true;
 150   }
 151   return false;
 152 }
 153 
<span class="line-modified"> 154 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 155 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 156   Bucket::trylock()
 157 {
 158   if (is_locked()) {
 159     return false;
 160   }
 161   // We will expect a clean first pointer.
 162   Node* tmp = first();
<span class="line-modified"> 163   if (Atomic::cmpxchg(set_state(tmp, STATE_LOCK_BIT), &amp;_first, tmp) == tmp) {</span>
 164     return true;
 165   }
 166   return false;
 167 }
 168 
<span class="line-modified"> 169 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 170 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 171   Bucket::unlock()
 172 {
 173   assert(is_locked(), &quot;Must be locked.&quot;);
 174   assert(!have_redirect(),
 175          &quot;Unlocking a bucket after it has reached terminal state.&quot;);
<span class="line-modified"> 176   OrderAccess::release_store(&amp;_first, clear_state(first()));</span>
 177 }
 178 
<span class="line-modified"> 179 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 180 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 181   Bucket::redirect()
 182 {
 183   assert(is_locked(), &quot;Must be locked.&quot;);
<span class="line-modified"> 184   OrderAccess::release_store(&amp;_first, set_state(_first, STATE_REDIRECT_BIT));</span>
 185 }
 186 
 187 // InternalTable
<span class="line-modified"> 188 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 189 inline ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 190   InternalTable::InternalTable(size_t log2_size)
 191     : _log2_size(log2_size), _size(((size_t)1ul) &lt;&lt; _log2_size),
 192       _hash_mask(~(~((size_t)0) &lt;&lt; _log2_size))
 193 {
 194   assert(_log2_size &gt;= SIZE_SMALL_LOG2 &amp;&amp; _log2_size &lt;= SIZE_BIG_LOG2,
 195          &quot;Bad size&quot;);
 196   _buckets = NEW_C_HEAP_ARRAY(Bucket, _size, F);
 197   // Use placement new for each element instead of new[] which could use more
 198   // memory than allocated.
 199   for (size_t i = 0; i &lt; _size; ++i) {
 200     new (_buckets + i) Bucket();
 201   }
 202 }
 203 
<span class="line-modified"> 204 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 205 inline ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 206   InternalTable::~InternalTable()
 207 {
 208   FREE_C_HEAP_ARRAY(Bucket, _buckets);
 209 }
 210 
 211 // ScopedCS
<span class="line-modified"> 212 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 213 inline ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
<span class="line-modified"> 214   ScopedCS::ScopedCS(Thread* thread, ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;* cht)</span>
 215     : _thread(thread),
 216       _cht(cht),
 217       _cs_context(GlobalCounter::critical_section_begin(_thread))
 218 {
 219   // This version is published now.
<span class="line-modified"> 220   if (OrderAccess::load_acquire(&amp;_cht-&gt;_invisible_epoch) != NULL) {</span>
<span class="line-modified"> 221     OrderAccess::release_store_fence(&amp;_cht-&gt;_invisible_epoch, (Thread*)NULL);</span>
 222   }
 223 }
 224 
<span class="line-modified"> 225 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 226 inline ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 227   ScopedCS::~ScopedCS()
 228 {
 229   GlobalCounter::critical_section_end(_thread, _cs_context);
 230 }
 231 
<span class="line-modified"> 232 // BaseConfig</span>
<span class="line-removed"> 233 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-removed"> 234 inline void* ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
<span class="line-removed"> 235   BaseConfig::allocate_node(size_t size, const VALUE&amp; value)</span>
<span class="line-removed"> 236 {</span>
<span class="line-removed"> 237   return AllocateHeap(size, F);</span>
<span class="line-removed"> 238 }</span>
<span class="line-removed"> 239 </span>
<span class="line-removed"> 240 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-removed"> 241 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
<span class="line-removed"> 242   BaseConfig::free_node(void* memory, const VALUE&amp; value)</span>
<span class="line-removed"> 243 {</span>
<span class="line-removed"> 244   FreeHeap(memory);</span>
<span class="line-removed"> 245 }</span>
<span class="line-removed"> 246 </span>
<span class="line-removed"> 247 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
 248 template &lt;typename LOOKUP_FUNC&gt;
<span class="line-modified"> 249 inline VALUE* ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 250   MultiGetHandle::get(LOOKUP_FUNC&amp; lookup_f, bool* grow_hint)
 251 {
 252   return ScopedCS::_cht-&gt;internal_get(ScopedCS::_thread, lookup_f, grow_hint);
 253 }
 254 
 255 // HaveDeletables
<span class="line-modified"> 256 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
 257 template &lt;typename EVALUATE_FUNC&gt;
<span class="line-modified"> 258 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 259   HaveDeletables&lt;true, EVALUATE_FUNC&gt;::have_deletable(Bucket* bucket,
 260                                                       EVALUATE_FUNC&amp; eval_f,
 261                                                       Bucket* prefetch_bucket)
 262 {
 263   // Instantiated for pointer type (true), so we can use prefetch.
 264   // When visiting all Nodes doing this prefetch give around 30%.
 265   Node* pref = prefetch_bucket != NULL ? prefetch_bucket-&gt;first() : NULL;
 266   for (Node* next = bucket-&gt;first(); next != NULL ; next = next-&gt;next()) {
 267     if (pref != NULL) {
 268       Prefetch::read(*pref-&gt;value(), 0);
 269       pref = pref-&gt;next();
 270     }
 271     // Read next() Node* once.  May be racing with a thread moving the next
 272     // pointers.
 273     Node* next_pref = next-&gt;next();
 274     if (next_pref != NULL) {
 275       Prefetch::read(*next_pref-&gt;value(), 0);
 276     }
 277     if (eval_f(next-&gt;value())) {
 278       return true;
 279     }
 280   }
 281   return false;
 282 }
 283 
<span class="line-modified"> 284 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
 285 template &lt;bool b, typename EVALUATE_FUNC&gt;
<span class="line-modified"> 286 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 287   HaveDeletables&lt;b, EVALUATE_FUNC&gt;::have_deletable(Bucket* bucket,
 288                                                    EVALUATE_FUNC&amp; eval_f,
 289                                                    Bucket* preb)
 290 {
 291   for (Node* next = bucket-&gt;first(); next != NULL ; next = next-&gt;next()) {
 292     if (eval_f(next-&gt;value())) {
 293       return true;
 294     }
 295   }
 296   return false;
 297 }
 298 
 299 // ConcurrentHashTable
<span class="line-modified"> 300 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 301 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 302   write_synchonize_on_visible_epoch(Thread* thread)
 303 {
 304   assert(_resize_lock_owner == thread, &quot;Re-size lock not held&quot;);
 305   OrderAccess::fence(); // Prevent below load from floating up.
 306   // If no reader saw this version we can skip write_synchronize.
<span class="line-modified"> 307   if (OrderAccess::load_acquire(&amp;_invisible_epoch) == thread) {</span>
 308     return;
 309   }
 310   assert(_invisible_epoch == NULL, &quot;Two thread doing bulk operations&quot;);
 311   // We set this/next version that we are synchronizing for to not published.
 312   // A reader will zero this flag if it reads this/next version.
<span class="line-modified"> 313   OrderAccess::release_store(&amp;_invisible_epoch, thread);</span>
 314   GlobalCounter::write_synchronize();
 315 }
 316 
<span class="line-modified"> 317 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 318 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 319   try_resize_lock(Thread* locker)
 320 {
 321   if (_resize_lock-&gt;try_lock()) {
 322     if (_resize_lock_owner != NULL) {
 323       assert(locker != _resize_lock_owner, &quot;Already own lock&quot;);
 324       // We got mutex but internal state is locked.
 325       _resize_lock-&gt;unlock();
 326       return false;
 327     }
 328   } else {
 329     return false;
 330   }
 331   _invisible_epoch = 0;
 332   _resize_lock_owner = locker;
 333   return true;
 334 }
 335 
<span class="line-modified"> 336 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 337 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 338   lock_resize_lock(Thread* locker)
 339 {
 340   size_t i = 0;
 341   // If lock is hold by some other thread, the chances that it is return quick
 342   // is low. So we will prefer yielding.
 343   SpinYield yield(1, 512);
 344   do {
 345     _resize_lock-&gt;lock_without_safepoint_check();
 346     // If holder of lock dropped mutex for safepoint mutex might be unlocked,
 347     // and _resize_lock_owner will contain the owner.
 348     if (_resize_lock_owner != NULL) {
 349       assert(locker != _resize_lock_owner, &quot;Already own lock&quot;);
 350       // We got mutex but internal state is locked.
 351       _resize_lock-&gt;unlock();
 352       yield.wait();
 353     } else {
 354       break;
 355     }
 356   } while(true);
 357   _resize_lock_owner = locker;
 358   _invisible_epoch = 0;
 359 }
 360 
<span class="line-modified"> 361 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 362 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 363   unlock_resize_lock(Thread* locker)
 364 {
 365   _invisible_epoch = 0;
 366   assert(locker == _resize_lock_owner, &quot;Not unlocked by locker.&quot;);
 367   _resize_lock_owner = NULL;
 368   _resize_lock-&gt;unlock();
 369 }
 370 
<span class="line-modified"> 371 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 372 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 373   free_nodes()
 374 {
 375   // We assume we are not MT during freeing.
 376   for (size_t node_it = 0; node_it &lt; _table-&gt;_size; node_it++) {
 377     Bucket* bucket = _table-&gt;get_buckets() + node_it;
 378     Node* node = bucket-&gt;first();
 379     while (node != NULL) {
 380       Node* free_node = node;
 381       node = node-&gt;next();
 382       Node::destroy_node(free_node);
 383     }
 384   }
 385 }
 386 
<span class="line-modified"> 387 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 388 inline typename ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::InternalTable*</span>
<span class="line-modified"> 389 ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 390   get_table() const
 391 {
<span class="line-modified"> 392   return OrderAccess::load_acquire(&amp;_table);</span>
 393 }
 394 
<span class="line-modified"> 395 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 396 inline typename ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::InternalTable*</span>
<span class="line-modified"> 397 ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 398   get_new_table() const
 399 {
<span class="line-modified"> 400   return OrderAccess::load_acquire(&amp;_new_table);</span>
 401 }
 402 
<span class="line-modified"> 403 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 404 inline typename ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::InternalTable*</span>
<span class="line-modified"> 405 ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 406   set_table_from_new()
 407 {
 408   InternalTable* old_table = _table;
 409   // Publish the new table.
<span class="line-modified"> 410   OrderAccess::release_store(&amp;_table, _new_table);</span>
 411   // All must see this.
 412   GlobalCounter::write_synchronize();
 413   // _new_table not read any more.
 414   _new_table = NULL;
 415   DEBUG_ONLY(_new_table = (InternalTable*)POISON_PTR;)
 416   return old_table;
 417 }
 418 
<span class="line-modified"> 419 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 420 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 421   internal_grow_range(Thread* thread, size_t start, size_t stop)
 422 {
 423   assert(stop &lt;= _table-&gt;_size, &quot;Outside backing array&quot;);
 424   assert(_new_table != NULL, &quot;Grow not proper setup before start&quot;);
 425   // The state is also copied here. Hence all buckets in new table will be
 426   // locked. I call the siblings odd/even, where even have high bit 0 and odd
 427   // have high bit 1.
 428   for (size_t even_index = start; even_index &lt; stop; even_index++) {
 429     Bucket* bucket = _table-&gt;get_bucket(even_index);
 430 
 431     bucket-&gt;lock();
 432 
 433     size_t odd_index = even_index + _table-&gt;_size;
 434     _new_table-&gt;get_buckets()[even_index] = *bucket;
 435     _new_table-&gt;get_buckets()[odd_index] = *bucket;
 436 
 437     // Moves lockers go to new table, where they will wait until unlock() below.
 438     bucket-&gt;redirect(); /* Must release stores above */
 439 
 440     // When this is done we have separated the nodes into corresponding buckets
 441     // in new table.
 442     if (!unzip_bucket(thread, _table, _new_table, even_index, odd_index)) {
 443       // If bucket is empty, unzip does nothing.
 444       // We must make sure readers go to new table before we poison the bucket.
 445       DEBUG_ONLY(GlobalCounter::write_synchronize();)
 446     }
 447 
 448     // Unlock for writes into the new table buckets.
 449     _new_table-&gt;get_bucket(even_index)-&gt;unlock();
 450     _new_table-&gt;get_bucket(odd_index)-&gt;unlock();
 451 
 452     DEBUG_ONLY(
 453        bucket-&gt;release_assign_node_ptr(
 454           _table-&gt;get_bucket(even_index)-&gt;first_ptr(), (Node*)POISON_PTR);
 455     )
 456   }
 457 }
 458 
<span class="line-modified"> 459 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
 460 template &lt;typename LOOKUP_FUNC, typename DELETE_FUNC&gt;
<span class="line-modified"> 461 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 462   internal_remove(Thread* thread, LOOKUP_FUNC&amp; lookup_f, DELETE_FUNC&amp; delete_f)
 463 {
 464   Bucket* bucket = get_bucket_locked(thread, lookup_f.get_hash());
 465   assert(bucket-&gt;is_locked(), &quot;Must be locked.&quot;);
 466   Node* const volatile * rem_n_prev = bucket-&gt;first_ptr();
 467   Node* rem_n = bucket-&gt;first();
 468   bool have_dead = false;
 469   while (rem_n != NULL) {
 470     if (lookup_f.equals(rem_n-&gt;value(), &amp;have_dead)) {
 471       bucket-&gt;release_assign_node_ptr(rem_n_prev, rem_n-&gt;next());
 472       break;
 473     } else {
 474       rem_n_prev = rem_n-&gt;next_ptr();
 475       rem_n = rem_n-&gt;next();
 476     }
 477   }
 478 
 479   bucket-&gt;unlock();
 480 
 481   if (rem_n == NULL) {
 482     return false;
 483   }
 484   // Publish the deletion.
 485   GlobalCounter::write_synchronize();
 486   delete_f(rem_n-&gt;value());
 487   Node::destroy_node(rem_n);

 488   return true;
 489 }
 490 
<span class="line-modified"> 491 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
 492 template &lt;typename EVALUATE_FUNC, typename DELETE_FUNC&gt;
<span class="line-modified"> 493 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 494   do_bulk_delete_locked_for(Thread* thread, size_t start_idx, size_t stop_idx,
 495                             EVALUATE_FUNC&amp; eval_f, DELETE_FUNC&amp; del_f, bool is_mt)
 496 {
 497   // Here we have resize lock so table is SMR safe, and there is no new
 498   // table. Can do this in parallel if we want.
 499   assert((is_mt &amp;&amp; _resize_lock_owner != NULL) ||
 500          (!is_mt &amp;&amp; _resize_lock_owner == thread), &quot;Re-size lock not held&quot;);
 501   Node* ndel[BULK_DELETE_LIMIT];
 502   InternalTable* table = get_table();
 503   assert(start_idx &lt; stop_idx, &quot;Must be&quot;);
 504   assert(stop_idx &lt;= _table-&gt;_size, &quot;Must be&quot;);
 505   // Here manual do critical section since we don&#39;t want to take the cost of
 506   // locking the bucket if there is nothing to delete. But we can have
 507   // concurrent single deletes. The _invisible_epoch can only be used by the
 508   // owner of _resize_lock, us here. There we should not changed it in our
 509   // own read-side.
 510   GlobalCounter::CSContext cs_context = GlobalCounter::critical_section_begin(thread);
 511   for (size_t bucket_it = start_idx; bucket_it &lt; stop_idx; bucket_it++) {
 512     Bucket* bucket = table-&gt;get_bucket(bucket_it);
 513     Bucket* prefetch_bucket = (bucket_it+1) &lt; stop_idx ?
</pre>
<hr />
<pre>
 516     if (!HaveDeletables&lt;IsPointer&lt;VALUE&gt;::value, EVALUATE_FUNC&gt;::
 517         have_deletable(bucket, eval_f, prefetch_bucket)) {
 518         // Nothing to remove in this bucket.
 519         continue;
 520     }
 521 
 522     GlobalCounter::critical_section_end(thread, cs_context);
 523     // We left critical section but the bucket cannot be removed while we hold
 524     // the _resize_lock.
 525     bucket-&gt;lock();
 526     size_t nd = delete_check_nodes(bucket, eval_f, BULK_DELETE_LIMIT, ndel);
 527     bucket-&gt;unlock();
 528     if (is_mt) {
 529       GlobalCounter::write_synchronize();
 530     } else {
 531       write_synchonize_on_visible_epoch(thread);
 532     }
 533     for (size_t node_it = 0; node_it &lt; nd; node_it++) {
 534       del_f(ndel[node_it]-&gt;value());
 535       Node::destroy_node(ndel[node_it]);

 536       DEBUG_ONLY(ndel[node_it] = (Node*)POISON_PTR;)
 537     }
 538     cs_context = GlobalCounter::critical_section_begin(thread);
 539   }
 540   GlobalCounter::critical_section_end(thread, cs_context);
 541 }
 542 
<span class="line-modified"> 543 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
 544 template &lt;typename LOOKUP_FUNC&gt;
<span class="line-modified"> 545 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 546   delete_in_bucket(Thread* thread, Bucket* bucket, LOOKUP_FUNC&amp; lookup_f)
 547 {
 548   assert(bucket-&gt;is_locked(), &quot;Must be locked.&quot;);
 549 
 550   size_t dels = 0;
 551   Node* ndel[BULK_DELETE_LIMIT];
 552   Node* const volatile * rem_n_prev = bucket-&gt;first_ptr();
 553   Node* rem_n = bucket-&gt;first();
 554   while (rem_n != NULL) {
 555     bool is_dead = false;
 556     lookup_f.equals(rem_n-&gt;value(), &amp;is_dead);
 557     if (is_dead) {
 558       ndel[dels++] = rem_n;
 559       Node* next_node = rem_n-&gt;next();
 560       bucket-&gt;release_assign_node_ptr(rem_n_prev, next_node);
 561       rem_n = next_node;
 562       if (dels == BULK_DELETE_LIMIT) {
 563         break;
 564       }
 565     } else {
 566       rem_n_prev = rem_n-&gt;next_ptr();
 567       rem_n = rem_n-&gt;next();
 568     }
 569   }
 570   if (dels &gt; 0) {
 571     GlobalCounter::write_synchronize();
 572     for (size_t node_it = 0; node_it &lt; dels; node_it++) {
 573       Node::destroy_node(ndel[node_it]);

 574       DEBUG_ONLY(ndel[node_it] = (Node*)POISON_PTR;)
 575     }
 576   }
 577 }
 578 
<span class="line-modified"> 579 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 580 inline typename ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::Bucket*</span>
<span class="line-modified"> 581 ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 582   get_bucket(uintx hash) const
 583 {
 584   InternalTable* table = get_table();
 585   Bucket* bucket = get_bucket_in(table, hash);
 586   if (bucket-&gt;have_redirect()) {
 587     table = get_new_table();
 588     bucket = get_bucket_in(table, hash);
 589   }
 590   return bucket;
 591 }
 592 
<span class="line-modified"> 593 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 594 inline typename ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::Bucket*</span>
<span class="line-modified"> 595 ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 596   get_bucket_locked(Thread* thread, const uintx hash)
 597 {
 598   Bucket* bucket;
 599   int i = 0;
 600   // SpinYield would be unfair here
 601   while(true) {
 602     {
 603       // We need a critical section to protect the table itself. But if we fail
 604       // we must leave critical section otherwise we would deadlock.
 605       ScopedCS cs(thread, this);
 606       bucket = get_bucket(hash);
 607       if (bucket-&gt;trylock()) {
 608         break; /* ends critical section */
 609       }
 610     } /* ends critical section */
 611     if ((++i) == SPINPAUSES_PER_YIELD) {
 612       // On contemporary OS yielding will give CPU to another runnable thread if
 613       // there is no CPU available.
 614       os::naked_yield();
 615       i = 0;
 616     } else {
 617       SpinPause();
 618     }
 619   }
 620   return bucket;
 621 }
 622 
 623 // Always called within critical section
<span class="line-modified"> 624 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
 625 template &lt;typename LOOKUP_FUNC&gt;
<span class="line-modified"> 626 typename ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::Node*</span>
<span class="line-modified"> 627 ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 628   get_node(const Bucket* const bucket, LOOKUP_FUNC&amp; lookup_f,
 629            bool* have_dead, size_t* loops) const
 630 {
 631   size_t loop_count = 0;
 632   Node* node = bucket-&gt;first();
 633   while (node != NULL) {
 634     bool is_dead = false;
 635     ++loop_count;
 636     if (lookup_f.equals(node-&gt;value(), &amp;is_dead)) {
 637       break;
 638     }
 639     if (is_dead &amp;&amp; !(*have_dead)) {
 640       *have_dead = true;
 641     }
 642     node = node-&gt;next();
 643   }
 644   if (loops != NULL) {
 645     *loops = loop_count;
 646   }
 647   return node;
 648 }
 649 
<span class="line-modified"> 650 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 651 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 652   unzip_bucket(Thread* thread, InternalTable* old_table,
 653                InternalTable* new_table, size_t even_index, size_t odd_index)
 654 {
 655   Node* aux = old_table-&gt;get_bucket(even_index)-&gt;first();
 656   if (aux == NULL) {
 657     // This is an empty bucket and in debug we poison first ptr in bucket.
 658     // Therefore we must make sure no readers are looking at this bucket.
 659     // If we don&#39;t do a write_synch here, caller must do it.
 660     return false;
 661   }
 662   Node* delete_me = NULL;
 663   Node* const volatile * even = new_table-&gt;get_bucket(even_index)-&gt;first_ptr();
 664   Node* const volatile * odd = new_table-&gt;get_bucket(odd_index)-&gt;first_ptr();
 665   while (aux != NULL) {
 666     bool dead_hash = false;
 667     size_t aux_hash = CONFIG::get_hash(*aux-&gt;value(), &amp;dead_hash);
 668     Node* aux_next = aux-&gt;next();
 669     if (dead_hash) {
 670       delete_me = aux;
 671       // This item is dead, move both list to next
</pre>
<hr />
<pre>
 688         // Keep in odd list
 689         odd = aux-&gt;next_ptr();
 690       } else {
 691         fatal(&quot;aux_index does not match even or odd indices&quot;);
 692       }
 693     }
 694     aux = aux_next;
 695 
 696     // We can only move 1 pointer otherwise a reader might be moved to the wrong
 697     // chain. E.g. looking for even hash value but got moved to the odd bucket
 698     // chain.
 699     write_synchonize_on_visible_epoch(thread);
 700     if (delete_me != NULL) {
 701       Node::destroy_node(delete_me);
 702       delete_me = NULL;
 703     }
 704   }
 705   return true;
 706 }
 707 
<span class="line-modified"> 708 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 709 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 710   internal_shrink_prolog(Thread* thread, size_t log2_size)
 711 {
 712   if (!try_resize_lock(thread)) {
 713     return false;
 714   }
 715   assert(_resize_lock_owner == thread, &quot;Re-size lock not held&quot;);
 716   if (_table-&gt;_log2_size == _log2_start_size ||
 717       _table-&gt;_log2_size &lt;= log2_size) {
 718     unlock_resize_lock(thread);
 719     return false;
 720   }
 721   _new_table = new InternalTable(_table-&gt;_log2_size - 1);
 722   return true;
 723 }
 724 
<span class="line-modified"> 725 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 726 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 727   internal_shrink_epilog(Thread* thread)
 728 {
 729   assert(_resize_lock_owner == thread, &quot;Re-size lock not held&quot;);
 730 
 731   InternalTable* old_table = set_table_from_new();
 732   _size_limit_reached = false;
 733   unlock_resize_lock(thread);
 734 #ifdef ASSERT
 735   for (size_t i = 0; i &lt; old_table-&gt;_size; i++) {
 736     assert(old_table-&gt;get_bucket(i++)-&gt;first() == POISON_PTR,
 737            &quot;No poison found&quot;);
 738   }
 739 #endif
 740   // ABA safe, old_table not visible to any other threads.
 741   delete old_table;
 742 }
 743 
<span class="line-modified"> 744 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 745 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 746   internal_shrink_range(Thread* thread, size_t start, size_t stop)
 747 {
 748   // The state is also copied here.
 749   // Hence all buckets in new table will be locked.
 750   for (size_t bucket_it = start; bucket_it &lt; stop; bucket_it++) {
 751     size_t even_hash_index = bucket_it; // High bit 0
 752     size_t odd_hash_index = bucket_it + _new_table-&gt;_size; // High bit 1
 753 
 754     Bucket* b_old_even = _table-&gt;get_bucket(even_hash_index);
 755     Bucket* b_old_odd  = _table-&gt;get_bucket(odd_hash_index);
 756 
 757     b_old_even-&gt;lock();
 758     b_old_odd-&gt;lock();
 759 
 760     _new_table-&gt;get_buckets()[bucket_it] = *b_old_even;
 761 
 762     // Put chains together.
 763     _new_table-&gt;get_bucket(bucket_it)-&gt;
 764       release_assign_last_node_next(*(b_old_odd-&gt;first_ptr()));
 765 
 766     b_old_even-&gt;redirect();
 767     b_old_odd-&gt;redirect();
 768 
 769     write_synchonize_on_visible_epoch(thread);
 770 
 771     // Unlock for writes into new smaller table.
 772     _new_table-&gt;get_bucket(bucket_it)-&gt;unlock();
 773 
 774     DEBUG_ONLY(b_old_even-&gt;release_assign_node_ptr(b_old_even-&gt;first_ptr(),
 775                                                    (Node*)POISON_PTR);)
 776     DEBUG_ONLY(b_old_odd-&gt;release_assign_node_ptr(b_old_odd-&gt;first_ptr(),
 777                                                   (Node*)POISON_PTR);)
 778   }
 779 }
 780 
<span class="line-modified"> 781 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 782 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 783   internal_shrink(Thread* thread, size_t log2_size)
 784 {
 785   if (!internal_shrink_prolog(thread, log2_size)) {
 786     assert(_resize_lock_owner != thread, &quot;Re-size lock held&quot;);
 787     return false;
 788   }
 789   assert(_resize_lock_owner == thread, &quot;Should be locked by me&quot;);
 790   internal_shrink_range(thread, 0, _new_table-&gt;_size);
 791   internal_shrink_epilog(thread);
 792   assert(_resize_lock_owner != thread, &quot;Re-size lock held&quot;);
 793   return true;
 794 }
 795 
<span class="line-modified"> 796 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 797 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 798   internal_grow_prolog(Thread* thread, size_t log2_size)
 799 {
 800   // This double checking of _size_limit_reached/is_max_size_reached()
 801   //  we only do in grow path, since grow means high load on table
 802   // while shrink means low load.
 803   if (is_max_size_reached()) {
 804     return false;
 805   }
 806   if (!try_resize_lock(thread)) {
 807     // Either we have an ongoing resize or an operation which doesn&#39;t want us
 808     // to resize now.
 809     return false;
 810   }
 811   if (is_max_size_reached() || _table-&gt;_log2_size &gt;= log2_size) {
 812     unlock_resize_lock(thread);
 813     return false;
 814   }
 815 
 816   _new_table = new InternalTable(_table-&gt;_log2_size + 1);
 817 
 818   if (_new_table-&gt;_log2_size == _log2_size_limit) {
 819     _size_limit_reached = true;
 820   }
 821 
 822   return true;
 823 }
 824 
<span class="line-modified"> 825 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 826 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 827   internal_grow_epilog(Thread* thread)
 828 {
 829   assert(_resize_lock_owner == thread, &quot;Should be locked&quot;);
 830 
 831   InternalTable* old_table = set_table_from_new();
 832   unlock_resize_lock(thread);
 833 #ifdef ASSERT
 834   for (size_t i = 0; i &lt; old_table-&gt;_size; i++) {
 835     assert(old_table-&gt;get_bucket(i++)-&gt;first() == POISON_PTR,
 836            &quot;No poison found&quot;);
 837   }
 838 #endif
 839   // ABA safe, old_table not visible to any other threads.
 840   delete old_table;
 841 }
 842 
<span class="line-modified"> 843 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 844 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 845   internal_grow(Thread* thread, size_t log2_size)
 846 {
 847   if (!internal_grow_prolog(thread, log2_size)) {
 848     assert(_resize_lock_owner != thread, &quot;Re-size lock held&quot;);
 849     return false;
 850   }
 851   assert(_resize_lock_owner == thread, &quot;Should be locked by me&quot;);
 852   internal_grow_range(thread, 0, _table-&gt;_size);
 853   internal_grow_epilog(thread);
 854   assert(_resize_lock_owner != thread, &quot;Re-size lock held&quot;);
 855   return true;
 856 }
 857 
 858 // Always called within critical section
<span class="line-modified"> 859 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
 860 template &lt;typename LOOKUP_FUNC&gt;
<span class="line-modified"> 861 inline VALUE* ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 862   internal_get(Thread* thread, LOOKUP_FUNC&amp; lookup_f, bool* grow_hint)
 863 {
 864   bool clean = false;
 865   size_t loops = 0;
 866   VALUE* ret = NULL;
 867 
 868   const Bucket* bucket = get_bucket(lookup_f.get_hash());
 869   Node* node = get_node(bucket, lookup_f, &amp;clean, &amp;loops);
 870   if (node != NULL) {
 871     ret = node-&gt;value();
 872   }
 873   if (grow_hint != NULL) {
 874     *grow_hint = loops &gt; _grow_hint;
 875   }
 876 
 877   return ret;
 878 }
 879 
<span class="line-modified"> 880 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
 881 template &lt;typename LOOKUP_FUNC&gt;
<span class="line-modified"> 882 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 883   internal_insert(Thread* thread, LOOKUP_FUNC&amp; lookup_f, const VALUE&amp; value,
 884                   bool* grow_hint, bool* clean_hint)
 885 {
 886   bool ret = false;
 887   bool clean = false;
 888   bool locked;
 889   size_t loops = 0;
 890   size_t i = 0;
 891   uintx hash = lookup_f.get_hash();
 892   Node* new_node = Node::create_node(value, NULL);
 893 
 894   while (true) {
 895     {
 896       ScopedCS cs(thread, this); /* protected the table/bucket */
 897       Bucket* bucket = get_bucket(hash);
 898       Node* first_at_start = bucket-&gt;first();
 899       Node* old = get_node(bucket, lookup_f, &amp;clean, &amp;loops);
 900       if (old == NULL) {
 901         new_node-&gt;set_next(first_at_start);
 902         if (bucket-&gt;cas_first(new_node, first_at_start)) {

 903           new_node = NULL;
 904           ret = true;
 905           break; /* leave critical section */
 906         }
 907         // CAS failed we must leave critical section and retry.
 908         locked = bucket-&gt;is_locked();
 909       } else {
 910         // There is a duplicate.
 911         break; /* leave critical section */
 912       }
 913     } /* leave critical section */
 914     i++;
 915     if (locked) {
 916       os::naked_yield();
 917     } else {
 918       SpinPause();
 919     }
 920   }
 921 
 922   if (new_node != NULL) {
</pre>
<hr />
<pre>
 924     Node::destroy_node(new_node);
 925   } else if (i == 0 &amp;&amp; clean) {
 926     // We only do cleaning on fast inserts.
 927     Bucket* bucket = get_bucket_locked(thread, lookup_f.get_hash());
 928     delete_in_bucket(thread, bucket, lookup_f);
 929     bucket-&gt;unlock();
 930     clean = false;
 931   }
 932 
 933   if (grow_hint != NULL) {
 934     *grow_hint = loops &gt; _grow_hint;
 935   }
 936 
 937   if (clean_hint != NULL) {
 938     *clean_hint = clean;
 939   }
 940 
 941   return ret;
 942 }
 943 
<span class="line-modified"> 944 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
 945 template &lt;typename FUNC&gt;
<span class="line-modified"> 946 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 947   visit_nodes(Bucket* bucket, FUNC&amp; visitor_f)
 948 {
 949   Node* current_node = bucket-&gt;first();
 950   while (current_node != NULL) {
 951     if (!visitor_f(current_node-&gt;value())) {
 952       return false;
 953     }
 954     current_node = current_node-&gt;next();
 955   }
 956   return true;
 957 }
 958 
<span class="line-modified"> 959 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
 960 template &lt;typename FUNC&gt;
<span class="line-modified"> 961 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 962   do_scan_locked(Thread* thread, FUNC&amp; scan_f)
 963 {
 964   assert(_resize_lock_owner == thread, &quot;Re-size lock not held&quot;);
 965   // We can do a critical section over the entire loop but that would block
 966   // updates for a long time. Instead we choose to block resizes.
 967   InternalTable* table = get_table();
 968   for (size_t bucket_it = 0; bucket_it &lt; table-&gt;_size; bucket_it++) {
 969     ScopedCS cs(thread, this);
 970     if (!visit_nodes(table-&gt;get_bucket(bucket_it), scan_f)) {
 971       break; /* ends critical section */
 972     }
 973   } /* ends critical section */
 974 }
 975 
<span class="line-modified"> 976 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
 977 template &lt;typename EVALUATE_FUNC&gt;
<span class="line-modified"> 978 inline size_t ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
 979   delete_check_nodes(Bucket* bucket, EVALUATE_FUNC&amp; eval_f,
 980                      size_t num_del, Node** ndel)
 981 {
 982   size_t dels = 0;
 983   Node* const volatile * rem_n_prev = bucket-&gt;first_ptr();
 984   Node* rem_n = bucket-&gt;first();
 985   while (rem_n != NULL) {
 986     if (eval_f(rem_n-&gt;value())) {
 987       ndel[dels++] = rem_n;
 988       Node* next_node = rem_n-&gt;next();
 989       bucket-&gt;release_assign_node_ptr(rem_n_prev, next_node);
 990       rem_n = next_node;
 991       if (dels == num_del) {
 992         break;
 993       }
 994     } else {
 995       rem_n_prev = rem_n-&gt;next_ptr();
 996       rem_n = rem_n-&gt;next();
 997     }
 998   }
 999   return dels;
1000 }
1001 
1002 // Constructor
<span class="line-modified">1003 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">1004 inline ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
1005   ConcurrentHashTable(size_t log2size, size_t log2size_limit, size_t grow_hint)
1006     : _new_table(NULL), _log2_size_limit(log2size_limit),
1007        _log2_start_size(log2size), _grow_hint(grow_hint),
1008        _size_limit_reached(false), _resize_lock_owner(NULL),
1009        _invisible_epoch(0)
1010 {

1011   _resize_lock =
<span class="line-modified">1012     new Mutex(Mutex::leaf, &quot;ConcurrentHashTable&quot;, false,</span>
<span class="line-modified">1013               Monitor::_safepoint_check_never);</span>
1014   _table = new InternalTable(log2size);
1015   assert(log2size_limit &gt;= log2size, &quot;bad ergo&quot;);
1016   _size_limit_reached = _table-&gt;_log2_size == _log2_size_limit;
1017 }
1018 
<span class="line-modified">1019 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">1020 inline ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
1021   ~ConcurrentHashTable()
1022 {
1023   delete _resize_lock;
1024   free_nodes();
1025   delete _table;
1026 }
1027 
<span class="line-modified">1028 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">1029 inline size_t ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
1030   get_size_log2(Thread* thread)
1031 {
1032   ScopedCS cs(thread, this);
1033   return _table-&gt;_log2_size;
1034 }
1035 
<span class="line-modified">1036 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">1037 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
1038   shrink(Thread* thread, size_t size_limit_log2)
1039 {
1040   size_t tmp = size_limit_log2 == 0 ? _log2_start_size : size_limit_log2;
1041   bool ret = internal_shrink(thread, tmp);
1042   return ret;
1043 }
1044 
<span class="line-modified">1045 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">1046 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
1047   grow(Thread* thread, size_t size_limit_log2)
1048 {
1049   size_t tmp = size_limit_log2 == 0 ? _log2_size_limit : size_limit_log2;
1050   return internal_grow(thread, tmp);
1051 }
1052 
<span class="line-modified">1053 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
1054 template &lt;typename LOOKUP_FUNC, typename FOUND_FUNC&gt;
<span class="line-modified">1055 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
1056   get(Thread* thread, LOOKUP_FUNC&amp; lookup_f, FOUND_FUNC&amp; found_f, bool* grow_hint)
1057 {
1058   bool ret = false;
1059   ScopedCS cs(thread, this);
1060   VALUE* val = internal_get(thread, lookup_f, grow_hint);
1061   if (val != NULL) {
1062     found_f(val);
1063     ret = true;
1064   }
1065   return ret;
1066 }
1067 
<span class="line-modified">1068 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">1069 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
1070   unsafe_insert(const VALUE&amp; value) {
1071   bool dead_hash = false;
1072   size_t hash = CONFIG::get_hash(value, &amp;dead_hash);
1073   if (dead_hash) {
1074     return false;
1075   }
1076   // This is an unsafe operation.
1077   InternalTable* table = get_table();
1078   Bucket* bucket = get_bucket_in(table, hash);
1079   assert(!bucket-&gt;have_redirect() &amp;&amp; !bucket-&gt;is_locked(), &quot;bad&quot;);
1080   Node* new_node = Node::create_node(value, bucket-&gt;first());
1081   if (!bucket-&gt;cas_first(new_node, bucket-&gt;first())) {
1082     assert(false, &quot;bad&quot;);
1083   }

1084   return true;
1085 }
1086 
<span class="line-modified">1087 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
1088 template &lt;typename SCAN_FUNC&gt;
<span class="line-modified">1089 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
1090   try_scan(Thread* thread, SCAN_FUNC&amp; scan_f)
1091 {
1092   if (!try_resize_lock(thread)) {
1093     return false;
1094   }
1095   do_scan_locked(thread, scan_f);
1096   unlock_resize_lock(thread);
1097   return true;
1098 }
1099 
<span class="line-modified">1100 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
1101 template &lt;typename SCAN_FUNC&gt;
<span class="line-modified">1102 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
1103   do_scan(Thread* thread, SCAN_FUNC&amp; scan_f)
1104 {
1105   assert(!SafepointSynchronize::is_at_safepoint(),
1106          &quot;must be outside a safepoint&quot;);
1107   assert(_resize_lock_owner != thread, &quot;Re-size lock held&quot;);
1108   lock_resize_lock(thread);
1109   do_scan_locked(thread, scan_f);
1110   unlock_resize_lock(thread);
1111   assert(_resize_lock_owner != thread, &quot;Re-size lock held&quot;);
1112 }
1113 
<span class="line-modified">1114 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
1115 template &lt;typename SCAN_FUNC&gt;
<span class="line-modified">1116 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
1117   do_safepoint_scan(SCAN_FUNC&amp; scan_f)
1118 {
1119   // We only allow this method to be used during a safepoint.
1120   assert(SafepointSynchronize::is_at_safepoint(),
1121          &quot;must only be called in a safepoint&quot;);
1122   assert(Thread::current()-&gt;is_VM_thread(),
1123          &quot;should be in vm thread&quot;);
1124 
1125   // Here we skip protection,
1126   // thus no other thread may use this table at the same time.
1127   InternalTable* table = get_table();
1128   for (size_t bucket_it = 0; bucket_it &lt; table-&gt;_size; bucket_it++) {
1129     Bucket* bucket = table-&gt;get_bucket(bucket_it);
1130     // If bucket have a redirect the items will be in the new table.
1131     // We must visit them there since the new table will contain any
1132     // concurrent inserts done after this bucket was resized.
1133     // If the bucket don&#39;t have redirect flag all items is in this table.
1134     if (!bucket-&gt;have_redirect()) {
1135       if(!visit_nodes(bucket, scan_f)) {
1136         return;
1137       }
1138     } else {
1139       assert(bucket-&gt;is_locked(), &quot;Bucket must be locked.&quot;);
1140     }
1141   }
1142   // If there is a paused resize we also need to visit the already resized items.
1143   table = get_new_table();
1144   if (table == NULL) {
1145     return;
1146   }
1147   DEBUG_ONLY(if (table == POISON_PTR) { return; })
1148   for (size_t bucket_it = 0; bucket_it &lt; table-&gt;_size; bucket_it++) {
1149     Bucket* bucket = table-&gt;get_bucket(bucket_it);
1150     assert(!bucket-&gt;is_locked(), &quot;Bucket must be unlocked.&quot;);
1151     if (!visit_nodes(bucket, scan_f)) {
1152       return;
1153     }
1154   }
1155 }
1156 
<span class="line-modified">1157 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
1158 template &lt;typename EVALUATE_FUNC, typename DELETE_FUNC&gt;
<span class="line-modified">1159 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
1160   try_bulk_delete(Thread* thread, EVALUATE_FUNC&amp; eval_f, DELETE_FUNC&amp; del_f)
1161 {
1162   if (!try_resize_lock(thread)) {
1163     return false;
1164   }
1165   do_bulk_delete_locked(thread, eval_f, del_f);
1166   unlock_resize_lock(thread);
1167   assert(_resize_lock_owner != thread, &quot;Re-size lock held&quot;);
1168   return true;
1169 }
1170 
<span class="line-modified">1171 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
1172 template &lt;typename EVALUATE_FUNC, typename DELETE_FUNC&gt;
<span class="line-modified">1173 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
1174   bulk_delete(Thread* thread, EVALUATE_FUNC&amp; eval_f, DELETE_FUNC&amp; del_f)
1175 {
1176   assert(!SafepointSynchronize::is_at_safepoint(),
1177          &quot;must be outside a safepoint&quot;);
1178   lock_resize_lock(thread);
1179   do_bulk_delete_locked(thread, eval_f, del_f);
1180   unlock_resize_lock(thread);
1181 }
1182 
<span class="line-modified">1183 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
1184 template &lt;typename VALUE_SIZE_FUNC&gt;
<span class="line-modified">1185 inline void ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
<span class="line-modified">1186   statistics_to(Thread* thread, VALUE_SIZE_FUNC&amp; vs_f,</span>
<span class="line-removed">1187                 outputStream* st, const char* table_name)</span>
1188 {
1189   NumberSeq summary;
1190   size_t literal_bytes = 0;
<span class="line-removed">1191   if (!try_resize_lock(thread)) {</span>
<span class="line-removed">1192     st-&gt;print_cr(&quot;statistics unavailable at this moment&quot;);</span>
<span class="line-removed">1193     return;</span>
<span class="line-removed">1194   }</span>
<span class="line-removed">1195 </span>
1196   InternalTable* table = get_table();
1197   for (size_t bucket_it = 0; bucket_it &lt; table-&gt;_size; bucket_it++) {
1198     ScopedCS cs(thread, this);
1199     size_t count = 0;
1200     Bucket* bucket = table-&gt;get_bucket(bucket_it);
1201     if (bucket-&gt;have_redirect() || bucket-&gt;is_locked()) {
<span class="line-modified">1202         continue;</span>
1203     }
1204     Node* current_node = bucket-&gt;first();
1205     while (current_node != NULL) {
1206       ++count;
1207       literal_bytes += vs_f(current_node-&gt;value());
1208       current_node = current_node-&gt;next();
1209     }
1210     summary.add((double)count);
1211   }
1212 
<span class="line-modified">1213   double num_buckets = summary.num();</span>
<span class="line-modified">1214   double num_entries = summary.sum();</span>
<span class="line-modified">1215 </span>
<span class="line-modified">1216   size_t bucket_bytes = num_buckets * sizeof(Bucket);</span>
<span class="line-modified">1217   size_t entry_bytes  = num_entries * sizeof(Node);</span>
<span class="line-modified">1218   size_t total_bytes = literal_bytes +  bucket_bytes + entry_bytes;</span>
<span class="line-modified">1219 </span>
<span class="line-modified">1220   size_t bucket_size  = (num_buckets &lt;= 0) ? 0 : (bucket_bytes  / num_buckets);</span>
<span class="line-modified">1221   size_t entry_size   = (num_entries &lt;= 0) ? 0 : (entry_bytes   / num_entries);</span>
<span class="line-modified">1222 </span>
<span class="line-modified">1223   st-&gt;print_cr(&quot;%s statistics:&quot;, table_name);</span>
<span class="line-modified">1224   st-&gt;print_cr(&quot;Number of buckets       : %9&quot; PRIuPTR &quot; = %9&quot; PRIuPTR</span>
<span class="line-modified">1225                &quot; bytes, each &quot; SIZE_FORMAT,</span>
<span class="line-modified">1226                (size_t)num_buckets, bucket_bytes,  bucket_size);</span>
<span class="line-modified">1227   st-&gt;print_cr(&quot;Number of entries       : %9&quot; PRIuPTR &quot; = %9&quot; PRIuPTR</span>
<span class="line-modified">1228                &quot; bytes, each &quot; SIZE_FORMAT,</span>
<span class="line-modified">1229                (size_t)num_entries, entry_bytes,   entry_size);</span>
<span class="line-modified">1230   if (literal_bytes != 0) {</span>
<span class="line-modified">1231     double literal_avg = (num_entries &lt;= 0) ? 0 : (literal_bytes / num_entries);</span>
<span class="line-modified">1232     st-&gt;print_cr(&quot;Number of literals      : %9&quot; PRIuPTR &quot; = %9&quot; PRIuPTR</span>
<span class="line-modified">1233                  &quot; bytes, avg %7.3f&quot;,</span>
<span class="line-modified">1234                  (size_t)num_entries, literal_bytes, literal_avg);</span>





1235   }
<span class="line-modified">1236   st-&gt;print_cr(&quot;Total footprsize_t         : %9s = %9&quot; PRIuPTR &quot; bytes&quot;, &quot;&quot;</span>
<span class="line-modified">1237                , total_bytes);</span>
<span class="line-removed">1238   st-&gt;print_cr(&quot;Average bucket size     : %9.3f&quot;, summary.avg());</span>
<span class="line-removed">1239   st-&gt;print_cr(&quot;Variance of bucket size : %9.3f&quot;, summary.variance());</span>
<span class="line-removed">1240   st-&gt;print_cr(&quot;Std. dev. of bucket size: %9.3f&quot;, summary.sd());</span>
<span class="line-removed">1241   st-&gt;print_cr(&quot;Maximum bucket size     : %9&quot; PRIuPTR,</span>
<span class="line-removed">1242                (size_t)summary.maximum());</span>
1243   unlock_resize_lock(thread);


1244 }
1245 
<span class="line-modified">1246 template &lt;typename VALUE, typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">1247 inline bool ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;::</span>
<span class="line-modified">1248   try_move_nodes_to(Thread* thread, ConcurrentHashTable&lt;VALUE, CONFIG, F&gt;* to_cht)</span>
1249 {
1250   if (!try_resize_lock(thread)) {
1251     return false;
1252   }
1253   assert(_new_table == NULL || _new_table == POISON_PTR, &quot;Must be NULL&quot;);
1254   for (size_t bucket_it = 0; bucket_it &lt; _table-&gt;_size; bucket_it++) {
1255     Bucket* bucket = _table-&gt;get_bucket(bucket_it);
1256     assert(!bucket-&gt;have_redirect() &amp;&amp; !bucket-&gt;is_locked(), &quot;Table must be uncontended&quot;);
1257     while (bucket-&gt;first() != NULL) {
1258       Node* move_node = bucket-&gt;first();
1259       bool ok = bucket-&gt;cas_first(move_node-&gt;next(), move_node);
1260       assert(ok, &quot;Uncontended cas must work&quot;);
1261       bool dead_hash = false;
1262       size_t insert_hash = CONFIG::get_hash(*move_node-&gt;value(), &amp;dead_hash);
1263       if (!dead_hash) {
1264         Bucket* insert_bucket = to_cht-&gt;get_bucket(insert_hash);
1265         assert(!bucket-&gt;have_redirect() &amp;&amp; !bucket-&gt;is_locked(), &quot;Not bit should be present&quot;);
1266         move_node-&gt;set_next(insert_bucket-&gt;first());
1267         ok = insert_bucket-&gt;cas_first(move_node, insert_bucket-&gt;first());
1268         assert(ok, &quot;Uncontended cas must work&quot;);
</pre>
</td>
<td>
<hr />
<pre>
  36 
  37 // 2^30 = 1G buckets
  38 #define SIZE_BIG_LOG2 30
  39 // 2^5  = 32 buckets
  40 #define SIZE_SMALL_LOG2 5
  41 
  42 // Number from spinYield.hpp. In some loops SpinYield would be unfair.
  43 #define SPINPAUSES_PER_YIELD 8192
  44 
  45 #ifdef ASSERT
  46 #ifdef _LP64
  47 // Two low bits are not usable.
  48 static const void* POISON_PTR = (void*)UCONST64(0xfbadbadbadbadbac);
  49 #else
  50 // Two low bits are not usable.
  51 static const void* POISON_PTR = (void*)0xffbadbac;
  52 #endif
  53 #endif
  54 
  55 // Node
<span class="line-modified">  56 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">  57 inline typename ConcurrentHashTable&lt;CONFIG, F&gt;::Node*</span>
<span class="line-modified">  58 ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
  59   Node::next() const
  60 {
<span class="line-modified">  61   return Atomic::load_acquire(&amp;_next);</span>
  62 }
  63 
  64 // Bucket
<span class="line-modified">  65 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">  66 inline typename ConcurrentHashTable&lt;CONFIG, F&gt;::Node*</span>
<span class="line-modified">  67 ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
  68   Bucket::first_raw() const
  69 {
<span class="line-modified">  70   return Atomic::load_acquire(&amp;_first);</span>
  71 }
  72 
<span class="line-modified">  73 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">  74 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
  75   Bucket::release_assign_node_ptr(
<span class="line-modified">  76     typename ConcurrentHashTable&lt;CONFIG, F&gt;::Node* const volatile * dst,</span>
<span class="line-modified">  77     typename ConcurrentHashTable&lt;CONFIG, F&gt;::Node* node) const</span>
  78 {
  79   // Due to this assert this methods is not static.
  80   assert(is_locked(), &quot;Must be locked.&quot;);
  81   Node** tmp = (Node**)dst;
<span class="line-modified">  82   Atomic::release_store(tmp, clear_set_state(node, *dst));</span>
  83 }
  84 
<span class="line-modified">  85 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">  86 inline typename ConcurrentHashTable&lt;CONFIG, F&gt;::Node*</span>
<span class="line-modified">  87 ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
  88   Bucket::first() const
  89 {
  90   // We strip the states bit before returning the ptr.
<span class="line-modified">  91   return clear_state(Atomic::load_acquire(&amp;_first));</span>
  92 }
  93 
<span class="line-modified">  94 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">  95 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
  96   Bucket::have_redirect() const
  97 {
  98   return is_state(first_raw(), STATE_REDIRECT_BIT);
  99 }
 100 
<span class="line-modified"> 101 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 102 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 103   Bucket::is_locked() const
 104 {
 105   return is_state(first_raw(), STATE_LOCK_BIT);
 106 }
 107 
<span class="line-modified"> 108 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 109 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 110   Bucket::lock()
 111 {
 112   int i = 0;
 113   // SpinYield would be unfair here
 114   while (!this-&gt;trylock()) {
 115     if ((++i) == SPINPAUSES_PER_YIELD) {
 116       // On contemporary OS yielding will give CPU to another runnable thread if
 117       // there is no CPU available.
 118       os::naked_yield();
 119       i = 0;
 120     } else {
 121       SpinPause();
 122     }
 123   }
 124 }
 125 
<span class="line-modified"> 126 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 127 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 128   Bucket::release_assign_last_node_next(
<span class="line-modified"> 129      typename ConcurrentHashTable&lt;CONFIG, F&gt;::Node* node)</span>
 130 {
 131   assert(is_locked(), &quot;Must be locked.&quot;);
 132   Node* const volatile * ret = first_ptr();
 133   while (clear_state(*ret) != NULL) {
 134     ret = clear_state(*ret)-&gt;next_ptr();
 135   }
 136   release_assign_node_ptr(ret, node);
 137 }
 138 
<span class="line-modified"> 139 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 140 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
<span class="line-modified"> 141   Bucket::cas_first(typename ConcurrentHashTable&lt;CONFIG, F&gt;::Node* node,</span>
<span class="line-modified"> 142                     typename ConcurrentHashTable&lt;CONFIG, F&gt;::Node* expect</span>
 143                     )
 144 {
 145   if (is_locked()) {
 146     return false;
 147   }
<span class="line-modified"> 148   if (Atomic::cmpxchg(&amp;_first, expect, node) == expect) {</span>
 149     return true;
 150   }
 151   return false;
 152 }
 153 
<span class="line-modified"> 154 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 155 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 156   Bucket::trylock()
 157 {
 158   if (is_locked()) {
 159     return false;
 160   }
 161   // We will expect a clean first pointer.
 162   Node* tmp = first();
<span class="line-modified"> 163   if (Atomic::cmpxchg(&amp;_first, tmp, set_state(tmp, STATE_LOCK_BIT)) == tmp) {</span>
 164     return true;
 165   }
 166   return false;
 167 }
 168 
<span class="line-modified"> 169 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 170 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 171   Bucket::unlock()
 172 {
 173   assert(is_locked(), &quot;Must be locked.&quot;);
 174   assert(!have_redirect(),
 175          &quot;Unlocking a bucket after it has reached terminal state.&quot;);
<span class="line-modified"> 176   Atomic::release_store(&amp;_first, clear_state(first()));</span>
 177 }
 178 
<span class="line-modified"> 179 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 180 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 181   Bucket::redirect()
 182 {
 183   assert(is_locked(), &quot;Must be locked.&quot;);
<span class="line-modified"> 184   Atomic::release_store(&amp;_first, set_state(_first, STATE_REDIRECT_BIT));</span>
 185 }
 186 
 187 // InternalTable
<span class="line-modified"> 188 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 189 inline ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 190   InternalTable::InternalTable(size_t log2_size)
 191     : _log2_size(log2_size), _size(((size_t)1ul) &lt;&lt; _log2_size),
 192       _hash_mask(~(~((size_t)0) &lt;&lt; _log2_size))
 193 {
 194   assert(_log2_size &gt;= SIZE_SMALL_LOG2 &amp;&amp; _log2_size &lt;= SIZE_BIG_LOG2,
 195          &quot;Bad size&quot;);
 196   _buckets = NEW_C_HEAP_ARRAY(Bucket, _size, F);
 197   // Use placement new for each element instead of new[] which could use more
 198   // memory than allocated.
 199   for (size_t i = 0; i &lt; _size; ++i) {
 200     new (_buckets + i) Bucket();
 201   }
 202 }
 203 
<span class="line-modified"> 204 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 205 inline ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 206   InternalTable::~InternalTable()
 207 {
 208   FREE_C_HEAP_ARRAY(Bucket, _buckets);
 209 }
 210 
 211 // ScopedCS
<span class="line-modified"> 212 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 213 inline ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
<span class="line-modified"> 214   ScopedCS::ScopedCS(Thread* thread, ConcurrentHashTable&lt;CONFIG, F&gt;* cht)</span>
 215     : _thread(thread),
 216       _cht(cht),
 217       _cs_context(GlobalCounter::critical_section_begin(_thread))
 218 {
 219   // This version is published now.
<span class="line-modified"> 220   if (Atomic::load_acquire(&amp;_cht-&gt;_invisible_epoch) != NULL) {</span>
<span class="line-modified"> 221     Atomic::release_store_fence(&amp;_cht-&gt;_invisible_epoch, (Thread*)NULL);</span>
 222   }
 223 }
 224 
<span class="line-modified"> 225 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 226 inline ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 227   ScopedCS::~ScopedCS()
 228 {
 229   GlobalCounter::critical_section_end(_thread, _cs_context);
 230 }
 231 
<span class="line-modified"> 232 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>















 233 template &lt;typename LOOKUP_FUNC&gt;
<span class="line-modified"> 234 inline typename CONFIG::Value* ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 235   MultiGetHandle::get(LOOKUP_FUNC&amp; lookup_f, bool* grow_hint)
 236 {
 237   return ScopedCS::_cht-&gt;internal_get(ScopedCS::_thread, lookup_f, grow_hint);
 238 }
 239 
 240 // HaveDeletables
<span class="line-modified"> 241 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
 242 template &lt;typename EVALUATE_FUNC&gt;
<span class="line-modified"> 243 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 244   HaveDeletables&lt;true, EVALUATE_FUNC&gt;::have_deletable(Bucket* bucket,
 245                                                       EVALUATE_FUNC&amp; eval_f,
 246                                                       Bucket* prefetch_bucket)
 247 {
 248   // Instantiated for pointer type (true), so we can use prefetch.
 249   // When visiting all Nodes doing this prefetch give around 30%.
 250   Node* pref = prefetch_bucket != NULL ? prefetch_bucket-&gt;first() : NULL;
 251   for (Node* next = bucket-&gt;first(); next != NULL ; next = next-&gt;next()) {
 252     if (pref != NULL) {
 253       Prefetch::read(*pref-&gt;value(), 0);
 254       pref = pref-&gt;next();
 255     }
 256     // Read next() Node* once.  May be racing with a thread moving the next
 257     // pointers.
 258     Node* next_pref = next-&gt;next();
 259     if (next_pref != NULL) {
 260       Prefetch::read(*next_pref-&gt;value(), 0);
 261     }
 262     if (eval_f(next-&gt;value())) {
 263       return true;
 264     }
 265   }
 266   return false;
 267 }
 268 
<span class="line-modified"> 269 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
 270 template &lt;bool b, typename EVALUATE_FUNC&gt;
<span class="line-modified"> 271 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 272   HaveDeletables&lt;b, EVALUATE_FUNC&gt;::have_deletable(Bucket* bucket,
 273                                                    EVALUATE_FUNC&amp; eval_f,
 274                                                    Bucket* preb)
 275 {
 276   for (Node* next = bucket-&gt;first(); next != NULL ; next = next-&gt;next()) {
 277     if (eval_f(next-&gt;value())) {
 278       return true;
 279     }
 280   }
 281   return false;
 282 }
 283 
 284 // ConcurrentHashTable
<span class="line-modified"> 285 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 286 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 287   write_synchonize_on_visible_epoch(Thread* thread)
 288 {
 289   assert(_resize_lock_owner == thread, &quot;Re-size lock not held&quot;);
 290   OrderAccess::fence(); // Prevent below load from floating up.
 291   // If no reader saw this version we can skip write_synchronize.
<span class="line-modified"> 292   if (Atomic::load_acquire(&amp;_invisible_epoch) == thread) {</span>
 293     return;
 294   }
 295   assert(_invisible_epoch == NULL, &quot;Two thread doing bulk operations&quot;);
 296   // We set this/next version that we are synchronizing for to not published.
 297   // A reader will zero this flag if it reads this/next version.
<span class="line-modified"> 298   Atomic::release_store(&amp;_invisible_epoch, thread);</span>
 299   GlobalCounter::write_synchronize();
 300 }
 301 
<span class="line-modified"> 302 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 303 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 304   try_resize_lock(Thread* locker)
 305 {
 306   if (_resize_lock-&gt;try_lock()) {
 307     if (_resize_lock_owner != NULL) {
 308       assert(locker != _resize_lock_owner, &quot;Already own lock&quot;);
 309       // We got mutex but internal state is locked.
 310       _resize_lock-&gt;unlock();
 311       return false;
 312     }
 313   } else {
 314     return false;
 315   }
 316   _invisible_epoch = 0;
 317   _resize_lock_owner = locker;
 318   return true;
 319 }
 320 
<span class="line-modified"> 321 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 322 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 323   lock_resize_lock(Thread* locker)
 324 {
 325   size_t i = 0;
 326   // If lock is hold by some other thread, the chances that it is return quick
 327   // is low. So we will prefer yielding.
 328   SpinYield yield(1, 512);
 329   do {
 330     _resize_lock-&gt;lock_without_safepoint_check();
 331     // If holder of lock dropped mutex for safepoint mutex might be unlocked,
 332     // and _resize_lock_owner will contain the owner.
 333     if (_resize_lock_owner != NULL) {
 334       assert(locker != _resize_lock_owner, &quot;Already own lock&quot;);
 335       // We got mutex but internal state is locked.
 336       _resize_lock-&gt;unlock();
 337       yield.wait();
 338     } else {
 339       break;
 340     }
 341   } while(true);
 342   _resize_lock_owner = locker;
 343   _invisible_epoch = 0;
 344 }
 345 
<span class="line-modified"> 346 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 347 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 348   unlock_resize_lock(Thread* locker)
 349 {
 350   _invisible_epoch = 0;
 351   assert(locker == _resize_lock_owner, &quot;Not unlocked by locker.&quot;);
 352   _resize_lock_owner = NULL;
 353   _resize_lock-&gt;unlock();
 354 }
 355 
<span class="line-modified"> 356 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 357 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 358   free_nodes()
 359 {
 360   // We assume we are not MT during freeing.
 361   for (size_t node_it = 0; node_it &lt; _table-&gt;_size; node_it++) {
 362     Bucket* bucket = _table-&gt;get_buckets() + node_it;
 363     Node* node = bucket-&gt;first();
 364     while (node != NULL) {
 365       Node* free_node = node;
 366       node = node-&gt;next();
 367       Node::destroy_node(free_node);
 368     }
 369   }
 370 }
 371 
<span class="line-modified"> 372 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 373 inline typename ConcurrentHashTable&lt;CONFIG, F&gt;::InternalTable*</span>
<span class="line-modified"> 374 ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 375   get_table() const
 376 {
<span class="line-modified"> 377   return Atomic::load_acquire(&amp;_table);</span>
 378 }
 379 
<span class="line-modified"> 380 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 381 inline typename ConcurrentHashTable&lt;CONFIG, F&gt;::InternalTable*</span>
<span class="line-modified"> 382 ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 383   get_new_table() const
 384 {
<span class="line-modified"> 385   return Atomic::load_acquire(&amp;_new_table);</span>
 386 }
 387 
<span class="line-modified"> 388 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 389 inline typename ConcurrentHashTable&lt;CONFIG, F&gt;::InternalTable*</span>
<span class="line-modified"> 390 ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 391   set_table_from_new()
 392 {
 393   InternalTable* old_table = _table;
 394   // Publish the new table.
<span class="line-modified"> 395   Atomic::release_store(&amp;_table, _new_table);</span>
 396   // All must see this.
 397   GlobalCounter::write_synchronize();
 398   // _new_table not read any more.
 399   _new_table = NULL;
 400   DEBUG_ONLY(_new_table = (InternalTable*)POISON_PTR;)
 401   return old_table;
 402 }
 403 
<span class="line-modified"> 404 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 405 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 406   internal_grow_range(Thread* thread, size_t start, size_t stop)
 407 {
 408   assert(stop &lt;= _table-&gt;_size, &quot;Outside backing array&quot;);
 409   assert(_new_table != NULL, &quot;Grow not proper setup before start&quot;);
 410   // The state is also copied here. Hence all buckets in new table will be
 411   // locked. I call the siblings odd/even, where even have high bit 0 and odd
 412   // have high bit 1.
 413   for (size_t even_index = start; even_index &lt; stop; even_index++) {
 414     Bucket* bucket = _table-&gt;get_bucket(even_index);
 415 
 416     bucket-&gt;lock();
 417 
 418     size_t odd_index = even_index + _table-&gt;_size;
 419     _new_table-&gt;get_buckets()[even_index] = *bucket;
 420     _new_table-&gt;get_buckets()[odd_index] = *bucket;
 421 
 422     // Moves lockers go to new table, where they will wait until unlock() below.
 423     bucket-&gt;redirect(); /* Must release stores above */
 424 
 425     // When this is done we have separated the nodes into corresponding buckets
 426     // in new table.
 427     if (!unzip_bucket(thread, _table, _new_table, even_index, odd_index)) {
 428       // If bucket is empty, unzip does nothing.
 429       // We must make sure readers go to new table before we poison the bucket.
 430       DEBUG_ONLY(GlobalCounter::write_synchronize();)
 431     }
 432 
 433     // Unlock for writes into the new table buckets.
 434     _new_table-&gt;get_bucket(even_index)-&gt;unlock();
 435     _new_table-&gt;get_bucket(odd_index)-&gt;unlock();
 436 
 437     DEBUG_ONLY(
 438        bucket-&gt;release_assign_node_ptr(
 439           _table-&gt;get_bucket(even_index)-&gt;first_ptr(), (Node*)POISON_PTR);
 440     )
 441   }
 442 }
 443 
<span class="line-modified"> 444 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
 445 template &lt;typename LOOKUP_FUNC, typename DELETE_FUNC&gt;
<span class="line-modified"> 446 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 447   internal_remove(Thread* thread, LOOKUP_FUNC&amp; lookup_f, DELETE_FUNC&amp; delete_f)
 448 {
 449   Bucket* bucket = get_bucket_locked(thread, lookup_f.get_hash());
 450   assert(bucket-&gt;is_locked(), &quot;Must be locked.&quot;);
 451   Node* const volatile * rem_n_prev = bucket-&gt;first_ptr();
 452   Node* rem_n = bucket-&gt;first();
 453   bool have_dead = false;
 454   while (rem_n != NULL) {
 455     if (lookup_f.equals(rem_n-&gt;value(), &amp;have_dead)) {
 456       bucket-&gt;release_assign_node_ptr(rem_n_prev, rem_n-&gt;next());
 457       break;
 458     } else {
 459       rem_n_prev = rem_n-&gt;next_ptr();
 460       rem_n = rem_n-&gt;next();
 461     }
 462   }
 463 
 464   bucket-&gt;unlock();
 465 
 466   if (rem_n == NULL) {
 467     return false;
 468   }
 469   // Publish the deletion.
 470   GlobalCounter::write_synchronize();
 471   delete_f(rem_n-&gt;value());
 472   Node::destroy_node(rem_n);
<span class="line-added"> 473   JFR_ONLY(_stats_rate.remove();)</span>
 474   return true;
 475 }
 476 
<span class="line-modified"> 477 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
 478 template &lt;typename EVALUATE_FUNC, typename DELETE_FUNC&gt;
<span class="line-modified"> 479 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 480   do_bulk_delete_locked_for(Thread* thread, size_t start_idx, size_t stop_idx,
 481                             EVALUATE_FUNC&amp; eval_f, DELETE_FUNC&amp; del_f, bool is_mt)
 482 {
 483   // Here we have resize lock so table is SMR safe, and there is no new
 484   // table. Can do this in parallel if we want.
 485   assert((is_mt &amp;&amp; _resize_lock_owner != NULL) ||
 486          (!is_mt &amp;&amp; _resize_lock_owner == thread), &quot;Re-size lock not held&quot;);
 487   Node* ndel[BULK_DELETE_LIMIT];
 488   InternalTable* table = get_table();
 489   assert(start_idx &lt; stop_idx, &quot;Must be&quot;);
 490   assert(stop_idx &lt;= _table-&gt;_size, &quot;Must be&quot;);
 491   // Here manual do critical section since we don&#39;t want to take the cost of
 492   // locking the bucket if there is nothing to delete. But we can have
 493   // concurrent single deletes. The _invisible_epoch can only be used by the
 494   // owner of _resize_lock, us here. There we should not changed it in our
 495   // own read-side.
 496   GlobalCounter::CSContext cs_context = GlobalCounter::critical_section_begin(thread);
 497   for (size_t bucket_it = start_idx; bucket_it &lt; stop_idx; bucket_it++) {
 498     Bucket* bucket = table-&gt;get_bucket(bucket_it);
 499     Bucket* prefetch_bucket = (bucket_it+1) &lt; stop_idx ?
</pre>
<hr />
<pre>
 502     if (!HaveDeletables&lt;IsPointer&lt;VALUE&gt;::value, EVALUATE_FUNC&gt;::
 503         have_deletable(bucket, eval_f, prefetch_bucket)) {
 504         // Nothing to remove in this bucket.
 505         continue;
 506     }
 507 
 508     GlobalCounter::critical_section_end(thread, cs_context);
 509     // We left critical section but the bucket cannot be removed while we hold
 510     // the _resize_lock.
 511     bucket-&gt;lock();
 512     size_t nd = delete_check_nodes(bucket, eval_f, BULK_DELETE_LIMIT, ndel);
 513     bucket-&gt;unlock();
 514     if (is_mt) {
 515       GlobalCounter::write_synchronize();
 516     } else {
 517       write_synchonize_on_visible_epoch(thread);
 518     }
 519     for (size_t node_it = 0; node_it &lt; nd; node_it++) {
 520       del_f(ndel[node_it]-&gt;value());
 521       Node::destroy_node(ndel[node_it]);
<span class="line-added"> 522       JFR_ONLY(_stats_rate.remove();)</span>
 523       DEBUG_ONLY(ndel[node_it] = (Node*)POISON_PTR;)
 524     }
 525     cs_context = GlobalCounter::critical_section_begin(thread);
 526   }
 527   GlobalCounter::critical_section_end(thread, cs_context);
 528 }
 529 
<span class="line-modified"> 530 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
 531 template &lt;typename LOOKUP_FUNC&gt;
<span class="line-modified"> 532 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 533   delete_in_bucket(Thread* thread, Bucket* bucket, LOOKUP_FUNC&amp; lookup_f)
 534 {
 535   assert(bucket-&gt;is_locked(), &quot;Must be locked.&quot;);
 536 
 537   size_t dels = 0;
 538   Node* ndel[BULK_DELETE_LIMIT];
 539   Node* const volatile * rem_n_prev = bucket-&gt;first_ptr();
 540   Node* rem_n = bucket-&gt;first();
 541   while (rem_n != NULL) {
 542     bool is_dead = false;
 543     lookup_f.equals(rem_n-&gt;value(), &amp;is_dead);
 544     if (is_dead) {
 545       ndel[dels++] = rem_n;
 546       Node* next_node = rem_n-&gt;next();
 547       bucket-&gt;release_assign_node_ptr(rem_n_prev, next_node);
 548       rem_n = next_node;
 549       if (dels == BULK_DELETE_LIMIT) {
 550         break;
 551       }
 552     } else {
 553       rem_n_prev = rem_n-&gt;next_ptr();
 554       rem_n = rem_n-&gt;next();
 555     }
 556   }
 557   if (dels &gt; 0) {
 558     GlobalCounter::write_synchronize();
 559     for (size_t node_it = 0; node_it &lt; dels; node_it++) {
 560       Node::destroy_node(ndel[node_it]);
<span class="line-added"> 561       JFR_ONLY(_stats_rate.remove();)</span>
 562       DEBUG_ONLY(ndel[node_it] = (Node*)POISON_PTR;)
 563     }
 564   }
 565 }
 566 
<span class="line-modified"> 567 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 568 inline typename ConcurrentHashTable&lt;CONFIG, F&gt;::Bucket*</span>
<span class="line-modified"> 569 ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 570   get_bucket(uintx hash) const
 571 {
 572   InternalTable* table = get_table();
 573   Bucket* bucket = get_bucket_in(table, hash);
 574   if (bucket-&gt;have_redirect()) {
 575     table = get_new_table();
 576     bucket = get_bucket_in(table, hash);
 577   }
 578   return bucket;
 579 }
 580 
<span class="line-modified"> 581 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 582 inline typename ConcurrentHashTable&lt;CONFIG, F&gt;::Bucket*</span>
<span class="line-modified"> 583 ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 584   get_bucket_locked(Thread* thread, const uintx hash)
 585 {
 586   Bucket* bucket;
 587   int i = 0;
 588   // SpinYield would be unfair here
 589   while(true) {
 590     {
 591       // We need a critical section to protect the table itself. But if we fail
 592       // we must leave critical section otherwise we would deadlock.
 593       ScopedCS cs(thread, this);
 594       bucket = get_bucket(hash);
 595       if (bucket-&gt;trylock()) {
 596         break; /* ends critical section */
 597       }
 598     } /* ends critical section */
 599     if ((++i) == SPINPAUSES_PER_YIELD) {
 600       // On contemporary OS yielding will give CPU to another runnable thread if
 601       // there is no CPU available.
 602       os::naked_yield();
 603       i = 0;
 604     } else {
 605       SpinPause();
 606     }
 607   }
 608   return bucket;
 609 }
 610 
 611 // Always called within critical section
<span class="line-modified"> 612 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
 613 template &lt;typename LOOKUP_FUNC&gt;
<span class="line-modified"> 614 typename ConcurrentHashTable&lt;CONFIG, F&gt;::Node*</span>
<span class="line-modified"> 615 ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 616   get_node(const Bucket* const bucket, LOOKUP_FUNC&amp; lookup_f,
 617            bool* have_dead, size_t* loops) const
 618 {
 619   size_t loop_count = 0;
 620   Node* node = bucket-&gt;first();
 621   while (node != NULL) {
 622     bool is_dead = false;
 623     ++loop_count;
 624     if (lookup_f.equals(node-&gt;value(), &amp;is_dead)) {
 625       break;
 626     }
 627     if (is_dead &amp;&amp; !(*have_dead)) {
 628       *have_dead = true;
 629     }
 630     node = node-&gt;next();
 631   }
 632   if (loops != NULL) {
 633     *loops = loop_count;
 634   }
 635   return node;
 636 }
 637 
<span class="line-modified"> 638 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 639 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 640   unzip_bucket(Thread* thread, InternalTable* old_table,
 641                InternalTable* new_table, size_t even_index, size_t odd_index)
 642 {
 643   Node* aux = old_table-&gt;get_bucket(even_index)-&gt;first();
 644   if (aux == NULL) {
 645     // This is an empty bucket and in debug we poison first ptr in bucket.
 646     // Therefore we must make sure no readers are looking at this bucket.
 647     // If we don&#39;t do a write_synch here, caller must do it.
 648     return false;
 649   }
 650   Node* delete_me = NULL;
 651   Node* const volatile * even = new_table-&gt;get_bucket(even_index)-&gt;first_ptr();
 652   Node* const volatile * odd = new_table-&gt;get_bucket(odd_index)-&gt;first_ptr();
 653   while (aux != NULL) {
 654     bool dead_hash = false;
 655     size_t aux_hash = CONFIG::get_hash(*aux-&gt;value(), &amp;dead_hash);
 656     Node* aux_next = aux-&gt;next();
 657     if (dead_hash) {
 658       delete_me = aux;
 659       // This item is dead, move both list to next
</pre>
<hr />
<pre>
 676         // Keep in odd list
 677         odd = aux-&gt;next_ptr();
 678       } else {
 679         fatal(&quot;aux_index does not match even or odd indices&quot;);
 680       }
 681     }
 682     aux = aux_next;
 683 
 684     // We can only move 1 pointer otherwise a reader might be moved to the wrong
 685     // chain. E.g. looking for even hash value but got moved to the odd bucket
 686     // chain.
 687     write_synchonize_on_visible_epoch(thread);
 688     if (delete_me != NULL) {
 689       Node::destroy_node(delete_me);
 690       delete_me = NULL;
 691     }
 692   }
 693   return true;
 694 }
 695 
<span class="line-modified"> 696 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 697 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 698   internal_shrink_prolog(Thread* thread, size_t log2_size)
 699 {
 700   if (!try_resize_lock(thread)) {
 701     return false;
 702   }
 703   assert(_resize_lock_owner == thread, &quot;Re-size lock not held&quot;);
 704   if (_table-&gt;_log2_size == _log2_start_size ||
 705       _table-&gt;_log2_size &lt;= log2_size) {
 706     unlock_resize_lock(thread);
 707     return false;
 708   }
 709   _new_table = new InternalTable(_table-&gt;_log2_size - 1);
 710   return true;
 711 }
 712 
<span class="line-modified"> 713 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 714 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 715   internal_shrink_epilog(Thread* thread)
 716 {
 717   assert(_resize_lock_owner == thread, &quot;Re-size lock not held&quot;);
 718 
 719   InternalTable* old_table = set_table_from_new();
 720   _size_limit_reached = false;
 721   unlock_resize_lock(thread);
 722 #ifdef ASSERT
 723   for (size_t i = 0; i &lt; old_table-&gt;_size; i++) {
 724     assert(old_table-&gt;get_bucket(i++)-&gt;first() == POISON_PTR,
 725            &quot;No poison found&quot;);
 726   }
 727 #endif
 728   // ABA safe, old_table not visible to any other threads.
 729   delete old_table;
 730 }
 731 
<span class="line-modified"> 732 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 733 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 734   internal_shrink_range(Thread* thread, size_t start, size_t stop)
 735 {
 736   // The state is also copied here.
 737   // Hence all buckets in new table will be locked.
 738   for (size_t bucket_it = start; bucket_it &lt; stop; bucket_it++) {
 739     size_t even_hash_index = bucket_it; // High bit 0
 740     size_t odd_hash_index = bucket_it + _new_table-&gt;_size; // High bit 1
 741 
 742     Bucket* b_old_even = _table-&gt;get_bucket(even_hash_index);
 743     Bucket* b_old_odd  = _table-&gt;get_bucket(odd_hash_index);
 744 
 745     b_old_even-&gt;lock();
 746     b_old_odd-&gt;lock();
 747 
 748     _new_table-&gt;get_buckets()[bucket_it] = *b_old_even;
 749 
 750     // Put chains together.
 751     _new_table-&gt;get_bucket(bucket_it)-&gt;
 752       release_assign_last_node_next(*(b_old_odd-&gt;first_ptr()));
 753 
 754     b_old_even-&gt;redirect();
 755     b_old_odd-&gt;redirect();
 756 
 757     write_synchonize_on_visible_epoch(thread);
 758 
 759     // Unlock for writes into new smaller table.
 760     _new_table-&gt;get_bucket(bucket_it)-&gt;unlock();
 761 
 762     DEBUG_ONLY(b_old_even-&gt;release_assign_node_ptr(b_old_even-&gt;first_ptr(),
 763                                                    (Node*)POISON_PTR);)
 764     DEBUG_ONLY(b_old_odd-&gt;release_assign_node_ptr(b_old_odd-&gt;first_ptr(),
 765                                                   (Node*)POISON_PTR);)
 766   }
 767 }
 768 
<span class="line-modified"> 769 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 770 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 771   internal_shrink(Thread* thread, size_t log2_size)
 772 {
 773   if (!internal_shrink_prolog(thread, log2_size)) {
 774     assert(_resize_lock_owner != thread, &quot;Re-size lock held&quot;);
 775     return false;
 776   }
 777   assert(_resize_lock_owner == thread, &quot;Should be locked by me&quot;);
 778   internal_shrink_range(thread, 0, _new_table-&gt;_size);
 779   internal_shrink_epilog(thread);
 780   assert(_resize_lock_owner != thread, &quot;Re-size lock held&quot;);
 781   return true;
 782 }
 783 
<span class="line-modified"> 784 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 785 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 786   internal_grow_prolog(Thread* thread, size_t log2_size)
 787 {
 788   // This double checking of _size_limit_reached/is_max_size_reached()
 789   //  we only do in grow path, since grow means high load on table
 790   // while shrink means low load.
 791   if (is_max_size_reached()) {
 792     return false;
 793   }
 794   if (!try_resize_lock(thread)) {
 795     // Either we have an ongoing resize or an operation which doesn&#39;t want us
 796     // to resize now.
 797     return false;
 798   }
 799   if (is_max_size_reached() || _table-&gt;_log2_size &gt;= log2_size) {
 800     unlock_resize_lock(thread);
 801     return false;
 802   }
 803 
 804   _new_table = new InternalTable(_table-&gt;_log2_size + 1);
 805 
 806   if (_new_table-&gt;_log2_size == _log2_size_limit) {
 807     _size_limit_reached = true;
 808   }
 809 
 810   return true;
 811 }
 812 
<span class="line-modified"> 813 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 814 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 815   internal_grow_epilog(Thread* thread)
 816 {
 817   assert(_resize_lock_owner == thread, &quot;Should be locked&quot;);
 818 
 819   InternalTable* old_table = set_table_from_new();
 820   unlock_resize_lock(thread);
 821 #ifdef ASSERT
 822   for (size_t i = 0; i &lt; old_table-&gt;_size; i++) {
 823     assert(old_table-&gt;get_bucket(i++)-&gt;first() == POISON_PTR,
 824            &quot;No poison found&quot;);
 825   }
 826 #endif
 827   // ABA safe, old_table not visible to any other threads.
 828   delete old_table;
 829 }
 830 
<span class="line-modified"> 831 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 832 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 833   internal_grow(Thread* thread, size_t log2_size)
 834 {
 835   if (!internal_grow_prolog(thread, log2_size)) {
 836     assert(_resize_lock_owner != thread, &quot;Re-size lock held&quot;);
 837     return false;
 838   }
 839   assert(_resize_lock_owner == thread, &quot;Should be locked by me&quot;);
 840   internal_grow_range(thread, 0, _table-&gt;_size);
 841   internal_grow_epilog(thread);
 842   assert(_resize_lock_owner != thread, &quot;Re-size lock held&quot;);
 843   return true;
 844 }
 845 
 846 // Always called within critical section
<span class="line-modified"> 847 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
 848 template &lt;typename LOOKUP_FUNC&gt;
<span class="line-modified"> 849 inline typename CONFIG::Value* ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 850   internal_get(Thread* thread, LOOKUP_FUNC&amp; lookup_f, bool* grow_hint)
 851 {
 852   bool clean = false;
 853   size_t loops = 0;
 854   VALUE* ret = NULL;
 855 
 856   const Bucket* bucket = get_bucket(lookup_f.get_hash());
 857   Node* node = get_node(bucket, lookup_f, &amp;clean, &amp;loops);
 858   if (node != NULL) {
 859     ret = node-&gt;value();
 860   }
 861   if (grow_hint != NULL) {
 862     *grow_hint = loops &gt; _grow_hint;
 863   }
 864 
 865   return ret;
 866 }
 867 
<span class="line-modified"> 868 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
 869 template &lt;typename LOOKUP_FUNC&gt;
<span class="line-modified"> 870 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 871   internal_insert(Thread* thread, LOOKUP_FUNC&amp; lookup_f, const VALUE&amp; value,
 872                   bool* grow_hint, bool* clean_hint)
 873 {
 874   bool ret = false;
 875   bool clean = false;
 876   bool locked;
 877   size_t loops = 0;
 878   size_t i = 0;
 879   uintx hash = lookup_f.get_hash();
 880   Node* new_node = Node::create_node(value, NULL);
 881 
 882   while (true) {
 883     {
 884       ScopedCS cs(thread, this); /* protected the table/bucket */
 885       Bucket* bucket = get_bucket(hash);
 886       Node* first_at_start = bucket-&gt;first();
 887       Node* old = get_node(bucket, lookup_f, &amp;clean, &amp;loops);
 888       if (old == NULL) {
 889         new_node-&gt;set_next(first_at_start);
 890         if (bucket-&gt;cas_first(new_node, first_at_start)) {
<span class="line-added"> 891           JFR_ONLY(_stats_rate.add();)</span>
 892           new_node = NULL;
 893           ret = true;
 894           break; /* leave critical section */
 895         }
 896         // CAS failed we must leave critical section and retry.
 897         locked = bucket-&gt;is_locked();
 898       } else {
 899         // There is a duplicate.
 900         break; /* leave critical section */
 901       }
 902     } /* leave critical section */
 903     i++;
 904     if (locked) {
 905       os::naked_yield();
 906     } else {
 907       SpinPause();
 908     }
 909   }
 910 
 911   if (new_node != NULL) {
</pre>
<hr />
<pre>
 913     Node::destroy_node(new_node);
 914   } else if (i == 0 &amp;&amp; clean) {
 915     // We only do cleaning on fast inserts.
 916     Bucket* bucket = get_bucket_locked(thread, lookup_f.get_hash());
 917     delete_in_bucket(thread, bucket, lookup_f);
 918     bucket-&gt;unlock();
 919     clean = false;
 920   }
 921 
 922   if (grow_hint != NULL) {
 923     *grow_hint = loops &gt; _grow_hint;
 924   }
 925 
 926   if (clean_hint != NULL) {
 927     *clean_hint = clean;
 928   }
 929 
 930   return ret;
 931 }
 932 
<span class="line-modified"> 933 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
 934 template &lt;typename FUNC&gt;
<span class="line-modified"> 935 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 936   visit_nodes(Bucket* bucket, FUNC&amp; visitor_f)
 937 {
 938   Node* current_node = bucket-&gt;first();
 939   while (current_node != NULL) {
 940     if (!visitor_f(current_node-&gt;value())) {
 941       return false;
 942     }
 943     current_node = current_node-&gt;next();
 944   }
 945   return true;
 946 }
 947 
<span class="line-modified"> 948 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
 949 template &lt;typename FUNC&gt;
<span class="line-modified"> 950 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 951   do_scan_locked(Thread* thread, FUNC&amp; scan_f)
 952 {
 953   assert(_resize_lock_owner == thread, &quot;Re-size lock not held&quot;);
 954   // We can do a critical section over the entire loop but that would block
 955   // updates for a long time. Instead we choose to block resizes.
 956   InternalTable* table = get_table();
 957   for (size_t bucket_it = 0; bucket_it &lt; table-&gt;_size; bucket_it++) {
 958     ScopedCS cs(thread, this);
 959     if (!visit_nodes(table-&gt;get_bucket(bucket_it), scan_f)) {
 960       break; /* ends critical section */
 961     }
 962   } /* ends critical section */
 963 }
 964 
<span class="line-modified"> 965 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
 966 template &lt;typename EVALUATE_FUNC&gt;
<span class="line-modified"> 967 inline size_t ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 968   delete_check_nodes(Bucket* bucket, EVALUATE_FUNC&amp; eval_f,
 969                      size_t num_del, Node** ndel)
 970 {
 971   size_t dels = 0;
 972   Node* const volatile * rem_n_prev = bucket-&gt;first_ptr();
 973   Node* rem_n = bucket-&gt;first();
 974   while (rem_n != NULL) {
 975     if (eval_f(rem_n-&gt;value())) {
 976       ndel[dels++] = rem_n;
 977       Node* next_node = rem_n-&gt;next();
 978       bucket-&gt;release_assign_node_ptr(rem_n_prev, next_node);
 979       rem_n = next_node;
 980       if (dels == num_del) {
 981         break;
 982       }
 983     } else {
 984       rem_n_prev = rem_n-&gt;next_ptr();
 985       rem_n = rem_n-&gt;next();
 986     }
 987   }
 988   return dels;
 989 }
 990 
 991 // Constructor
<span class="line-modified"> 992 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified"> 993 inline ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
 994   ConcurrentHashTable(size_t log2size, size_t log2size_limit, size_t grow_hint)
 995     : _new_table(NULL), _log2_size_limit(log2size_limit),
 996        _log2_start_size(log2size), _grow_hint(grow_hint),
 997        _size_limit_reached(false), _resize_lock_owner(NULL),
 998        _invisible_epoch(0)
 999 {
<span class="line-added">1000   _stats_rate = TableRateStatistics();</span>
1001   _resize_lock =
<span class="line-modified">1002     new Mutex(Mutex::leaf, &quot;ConcurrentHashTable&quot;, true,</span>
<span class="line-modified">1003               Mutex::_safepoint_check_never);</span>
1004   _table = new InternalTable(log2size);
1005   assert(log2size_limit &gt;= log2size, &quot;bad ergo&quot;);
1006   _size_limit_reached = _table-&gt;_log2_size == _log2_size_limit;
1007 }
1008 
<span class="line-modified">1009 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">1010 inline ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
1011   ~ConcurrentHashTable()
1012 {
1013   delete _resize_lock;
1014   free_nodes();
1015   delete _table;
1016 }
1017 
<span class="line-modified">1018 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">1019 inline size_t ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
1020   get_size_log2(Thread* thread)
1021 {
1022   ScopedCS cs(thread, this);
1023   return _table-&gt;_log2_size;
1024 }
1025 
<span class="line-modified">1026 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">1027 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
1028   shrink(Thread* thread, size_t size_limit_log2)
1029 {
1030   size_t tmp = size_limit_log2 == 0 ? _log2_start_size : size_limit_log2;
1031   bool ret = internal_shrink(thread, tmp);
1032   return ret;
1033 }
1034 
<span class="line-modified">1035 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">1036 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
1037   grow(Thread* thread, size_t size_limit_log2)
1038 {
1039   size_t tmp = size_limit_log2 == 0 ? _log2_size_limit : size_limit_log2;
1040   return internal_grow(thread, tmp);
1041 }
1042 
<span class="line-modified">1043 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
1044 template &lt;typename LOOKUP_FUNC, typename FOUND_FUNC&gt;
<span class="line-modified">1045 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
1046   get(Thread* thread, LOOKUP_FUNC&amp; lookup_f, FOUND_FUNC&amp; found_f, bool* grow_hint)
1047 {
1048   bool ret = false;
1049   ScopedCS cs(thread, this);
1050   VALUE* val = internal_get(thread, lookup_f, grow_hint);
1051   if (val != NULL) {
1052     found_f(val);
1053     ret = true;
1054   }
1055   return ret;
1056 }
1057 
<span class="line-modified">1058 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">1059 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
1060   unsafe_insert(const VALUE&amp; value) {
1061   bool dead_hash = false;
1062   size_t hash = CONFIG::get_hash(value, &amp;dead_hash);
1063   if (dead_hash) {
1064     return false;
1065   }
1066   // This is an unsafe operation.
1067   InternalTable* table = get_table();
1068   Bucket* bucket = get_bucket_in(table, hash);
1069   assert(!bucket-&gt;have_redirect() &amp;&amp; !bucket-&gt;is_locked(), &quot;bad&quot;);
1070   Node* new_node = Node::create_node(value, bucket-&gt;first());
1071   if (!bucket-&gt;cas_first(new_node, bucket-&gt;first())) {
1072     assert(false, &quot;bad&quot;);
1073   }
<span class="line-added">1074   JFR_ONLY(_stats_rate.add();)</span>
1075   return true;
1076 }
1077 
<span class="line-modified">1078 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
1079 template &lt;typename SCAN_FUNC&gt;
<span class="line-modified">1080 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
1081   try_scan(Thread* thread, SCAN_FUNC&amp; scan_f)
1082 {
1083   if (!try_resize_lock(thread)) {
1084     return false;
1085   }
1086   do_scan_locked(thread, scan_f);
1087   unlock_resize_lock(thread);
1088   return true;
1089 }
1090 
<span class="line-modified">1091 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
1092 template &lt;typename SCAN_FUNC&gt;
<span class="line-modified">1093 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
1094   do_scan(Thread* thread, SCAN_FUNC&amp; scan_f)
1095 {
1096   assert(!SafepointSynchronize::is_at_safepoint(),
1097          &quot;must be outside a safepoint&quot;);
1098   assert(_resize_lock_owner != thread, &quot;Re-size lock held&quot;);
1099   lock_resize_lock(thread);
1100   do_scan_locked(thread, scan_f);
1101   unlock_resize_lock(thread);
1102   assert(_resize_lock_owner != thread, &quot;Re-size lock held&quot;);
1103 }
1104 
<span class="line-modified">1105 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
1106 template &lt;typename SCAN_FUNC&gt;
<span class="line-modified">1107 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
1108   do_safepoint_scan(SCAN_FUNC&amp; scan_f)
1109 {
1110   // We only allow this method to be used during a safepoint.
1111   assert(SafepointSynchronize::is_at_safepoint(),
1112          &quot;must only be called in a safepoint&quot;);
1113   assert(Thread::current()-&gt;is_VM_thread(),
1114          &quot;should be in vm thread&quot;);
1115 
1116   // Here we skip protection,
1117   // thus no other thread may use this table at the same time.
1118   InternalTable* table = get_table();
1119   for (size_t bucket_it = 0; bucket_it &lt; table-&gt;_size; bucket_it++) {
1120     Bucket* bucket = table-&gt;get_bucket(bucket_it);
1121     // If bucket have a redirect the items will be in the new table.
1122     // We must visit them there since the new table will contain any
1123     // concurrent inserts done after this bucket was resized.
1124     // If the bucket don&#39;t have redirect flag all items is in this table.
1125     if (!bucket-&gt;have_redirect()) {
1126       if(!visit_nodes(bucket, scan_f)) {
1127         return;
1128       }
1129     } else {
1130       assert(bucket-&gt;is_locked(), &quot;Bucket must be locked.&quot;);
1131     }
1132   }
1133   // If there is a paused resize we also need to visit the already resized items.
1134   table = get_new_table();
1135   if (table == NULL) {
1136     return;
1137   }
1138   DEBUG_ONLY(if (table == POISON_PTR) { return; })
1139   for (size_t bucket_it = 0; bucket_it &lt; table-&gt;_size; bucket_it++) {
1140     Bucket* bucket = table-&gt;get_bucket(bucket_it);
1141     assert(!bucket-&gt;is_locked(), &quot;Bucket must be unlocked.&quot;);
1142     if (!visit_nodes(bucket, scan_f)) {
1143       return;
1144     }
1145   }
1146 }
1147 
<span class="line-modified">1148 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
1149 template &lt;typename EVALUATE_FUNC, typename DELETE_FUNC&gt;
<span class="line-modified">1150 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
1151   try_bulk_delete(Thread* thread, EVALUATE_FUNC&amp; eval_f, DELETE_FUNC&amp; del_f)
1152 {
1153   if (!try_resize_lock(thread)) {
1154     return false;
1155   }
1156   do_bulk_delete_locked(thread, eval_f, del_f);
1157   unlock_resize_lock(thread);
1158   assert(_resize_lock_owner != thread, &quot;Re-size lock held&quot;);
1159   return true;
1160 }
1161 
<span class="line-modified">1162 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
1163 template &lt;typename EVALUATE_FUNC, typename DELETE_FUNC&gt;
<span class="line-modified">1164 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
1165   bulk_delete(Thread* thread, EVALUATE_FUNC&amp; eval_f, DELETE_FUNC&amp; del_f)
1166 {
1167   assert(!SafepointSynchronize::is_at_safepoint(),
1168          &quot;must be outside a safepoint&quot;);
1169   lock_resize_lock(thread);
1170   do_bulk_delete_locked(thread, eval_f, del_f);
1171   unlock_resize_lock(thread);
1172 }
1173 
<span class="line-modified">1174 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
1175 template &lt;typename VALUE_SIZE_FUNC&gt;
<span class="line-modified">1176 inline TableStatistics ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
<span class="line-modified">1177   statistics_calculate(Thread* thread, VALUE_SIZE_FUNC&amp; vs_f)</span>

1178 {
1179   NumberSeq summary;
1180   size_t literal_bytes = 0;





1181   InternalTable* table = get_table();
1182   for (size_t bucket_it = 0; bucket_it &lt; table-&gt;_size; bucket_it++) {
1183     ScopedCS cs(thread, this);
1184     size_t count = 0;
1185     Bucket* bucket = table-&gt;get_bucket(bucket_it);
1186     if (bucket-&gt;have_redirect() || bucket-&gt;is_locked()) {
<span class="line-modified">1187       continue;</span>
1188     }
1189     Node* current_node = bucket-&gt;first();
1190     while (current_node != NULL) {
1191       ++count;
1192       literal_bytes += vs_f(current_node-&gt;value());
1193       current_node = current_node-&gt;next();
1194     }
1195     summary.add((double)count);
1196   }
1197 
<span class="line-modified">1198   return TableStatistics(_stats_rate, summary, literal_bytes, sizeof(Bucket), sizeof(Node));</span>
<span class="line-modified">1199 }</span>
<span class="line-modified">1200 </span>
<span class="line-modified">1201 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">1202 template &lt;typename VALUE_SIZE_FUNC&gt;</span>
<span class="line-modified">1203 inline TableStatistics ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
<span class="line-modified">1204   statistics_get(Thread* thread, VALUE_SIZE_FUNC&amp; vs_f, TableStatistics old)</span>
<span class="line-modified">1205 {</span>
<span class="line-modified">1206   if (!try_resize_lock(thread)) {</span>
<span class="line-modified">1207     return old;</span>
<span class="line-modified">1208   }</span>
<span class="line-modified">1209 </span>
<span class="line-modified">1210   TableStatistics ts = statistics_calculate(thread, vs_f);</span>
<span class="line-modified">1211   unlock_resize_lock(thread);</span>
<span class="line-modified">1212 </span>
<span class="line-modified">1213   return ts;</span>
<span class="line-modified">1214 }</span>
<span class="line-modified">1215 </span>
<span class="line-modified">1216 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">1217 template &lt;typename VALUE_SIZE_FUNC&gt;</span>
<span class="line-modified">1218 inline void ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
<span class="line-modified">1219   statistics_to(Thread* thread, VALUE_SIZE_FUNC&amp; vs_f,</span>
<span class="line-added">1220                 outputStream* st, const char* table_name)</span>
<span class="line-added">1221 {</span>
<span class="line-added">1222   if (!try_resize_lock(thread)) {</span>
<span class="line-added">1223     st-&gt;print_cr(&quot;statistics unavailable at this moment&quot;);</span>
<span class="line-added">1224     return;</span>
1225   }
<span class="line-modified">1226 </span>
<span class="line-modified">1227   TableStatistics ts = statistics_calculate(thread, vs_f);</span>





1228   unlock_resize_lock(thread);
<span class="line-added">1229 </span>
<span class="line-added">1230   ts.print(st, table_name);</span>
1231 }
1232 
<span class="line-modified">1233 template &lt;typename CONFIG, MEMFLAGS F&gt;</span>
<span class="line-modified">1234 inline bool ConcurrentHashTable&lt;CONFIG, F&gt;::</span>
<span class="line-modified">1235   try_move_nodes_to(Thread* thread, ConcurrentHashTable&lt;CONFIG, F&gt;* to_cht)</span>
1236 {
1237   if (!try_resize_lock(thread)) {
1238     return false;
1239   }
1240   assert(_new_table == NULL || _new_table == POISON_PTR, &quot;Must be NULL&quot;);
1241   for (size_t bucket_it = 0; bucket_it &lt; _table-&gt;_size; bucket_it++) {
1242     Bucket* bucket = _table-&gt;get_bucket(bucket_it);
1243     assert(!bucket-&gt;have_redirect() &amp;&amp; !bucket-&gt;is_locked(), &quot;Table must be uncontended&quot;);
1244     while (bucket-&gt;first() != NULL) {
1245       Node* move_node = bucket-&gt;first();
1246       bool ok = bucket-&gt;cas_first(move_node-&gt;next(), move_node);
1247       assert(ok, &quot;Uncontended cas must work&quot;);
1248       bool dead_hash = false;
1249       size_t insert_hash = CONFIG::get_hash(*move_node-&gt;value(), &amp;dead_hash);
1250       if (!dead_hash) {
1251         Bucket* insert_bucket = to_cht-&gt;get_bucket(insert_hash);
1252         assert(!bucket-&gt;have_redirect() &amp;&amp; !bucket-&gt;is_locked(), &quot;Not bit should be present&quot;);
1253         move_node-&gt;set_next(insert_bucket-&gt;first());
1254         ok = insert_bucket-&gt;cas_first(move_node, insert_bucket-&gt;first());
1255         assert(ok, &quot;Uncontended cas must work&quot;);
</pre>
</td>
</tr>
</table>
<center><a href="concurrentHashTable.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="concurrentHashTableTasks.inline.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>