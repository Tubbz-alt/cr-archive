diff a/src/hotspot/share/utilities/concurrentHashTableTasks.inline.hpp b/src/hotspot/share/utilities/concurrentHashTableTasks.inline.hpp
--- a/src/hotspot/share/utilities/concurrentHashTableTasks.inline.hpp
+++ b/src/hotspot/share/utilities/concurrentHashTableTasks.inline.hpp
@@ -23,21 +23,22 @@
  */
 
 #ifndef SHARE_UTILITIES_CONCURRENTHASHTABLETASKS_INLINE_HPP
 #define SHARE_UTILITIES_CONCURRENTHASHTABLETASKS_INLINE_HPP
 
+#include "runtime/atomic.hpp"
 #include "utilities/globalDefinitions.hpp"
 #include "utilities/concurrentHashTable.inline.hpp"
 
 // This inline file contains BulkDeleteTask and GrowTasks which are both bucket
 // operations, which they are serialized with each other.
 
 // Base class for pause and/or parallel bulk operations.
-template <typename VALUE, typename CONFIG, MEMFLAGS F>
-class ConcurrentHashTable<VALUE, CONFIG, F>::BucketsOperation {
+template <typename CONFIG, MEMFLAGS F>
+class ConcurrentHashTable<CONFIG, F>::BucketsOperation {
  protected:
-  ConcurrentHashTable<VALUE, CONFIG, F>* _cht;
+  ConcurrentHashTable<CONFIG, F>* _cht;
 
   // Default size of _task_size_log2
   static const size_t DEFAULT_TASK_SIZE_LOG2 = 12;
 
   // The table is split into ranges, every increment is one range.
@@ -45,17 +46,17 @@
   size_t _task_size_log2; // Number of buckets.
   size_t _stop_task;      // Last task
   size_t _size_log2;      // Table size.
   bool   _is_mt;
 
-  BucketsOperation(ConcurrentHashTable<VALUE, CONFIG, F>* cht, bool is_mt = false)
+  BucketsOperation(ConcurrentHashTable<CONFIG, F>* cht, bool is_mt = false)
     : _cht(cht), _next_to_claim(0), _task_size_log2(DEFAULT_TASK_SIZE_LOG2),
     _stop_task(0), _size_log2(0), _is_mt(is_mt) {}
 
   // Returns true if you succeeded to claim the range start -> (stop-1).
   bool claim(size_t* start, size_t* stop) {
-    size_t claimed = Atomic::add((size_t)1, &_next_to_claim) - 1;
+    size_t claimed = Atomic::fetch_and_add(&_next_to_claim, 1u);
     if (claimed >= _stop_task) {
       return false;
     }
     *start = claimed * (((size_t)1) << _task_size_log2);
     *stop  = ((*start) + (((size_t)1) << _task_size_log2));
@@ -72,11 +73,11 @@
     _stop_task = (((size_t)1) << tmp);
   }
 
   // Returns false if all ranges are claimed.
   bool have_more_work() {
-    return OrderAccess::load_acquire(&_next_to_claim) >= _stop_task;
+    return Atomic::load_acquire(&_next_to_claim) >= _stop_task;
   }
 
   void thread_owns_resize_lock(Thread* thread) {
     assert(BucketsOperation::_cht->_resize_lock_owner == thread,
            "Should be locked by me");
@@ -114,16 +115,16 @@
     this->thread_owns_resize_lock(thread);
   }
 };
 
 // For doing pausable/parallel bulk delete.
-template <typename VALUE, typename CONFIG, MEMFLAGS F>
-class ConcurrentHashTable<VALUE, CONFIG, F>::BulkDeleteTask :
+template <typename CONFIG, MEMFLAGS F>
+class ConcurrentHashTable<CONFIG, F>::BulkDeleteTask :
   public BucketsOperation
 {
  public:
-  BulkDeleteTask(ConcurrentHashTable<VALUE, CONFIG, F>* cht, bool is_mt = false)
+  BulkDeleteTask(ConcurrentHashTable<CONFIG, F>* cht, bool is_mt = false)
     : BucketsOperation(cht, is_mt) {
   }
   // Before start prepare must be called.
   bool prepare(Thread* thread) {
     bool lock = BucketsOperation::_cht->try_resize_lock(thread);
@@ -158,16 +159,16 @@
     BucketsOperation::_cht->unlock_resize_lock(thread);
     this->thread_do_not_own_resize_lock(thread);
   }
 };
 
-template <typename VALUE, typename CONFIG, MEMFLAGS F>
-class ConcurrentHashTable<VALUE, CONFIG, F>::GrowTask :
+template <typename CONFIG, MEMFLAGS F>
+class ConcurrentHashTable<CONFIG, F>::GrowTask :
   public BucketsOperation
 {
  public:
-  GrowTask(ConcurrentHashTable<VALUE, CONFIG, F>* cht) : BucketsOperation(cht) {
+  GrowTask(ConcurrentHashTable<CONFIG, F>* cht) : BucketsOperation(cht) {
   }
   // Before start prepare must be called.
   bool prepare(Thread* thread) {
     if (!BucketsOperation::_cht->internal_grow_prolog(
           thread, BucketsOperation::_cht->_log2_size_limit)) {
