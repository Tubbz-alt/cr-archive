<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/share/jfr/leakprofiler/emitEventOperation.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2014, 2018, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 #include &quot;precompiled.hpp&quot;
 25 #include &quot;gc/shared/collectedHeap.hpp&quot;
 26 #include &quot;jfr/jfrEvents.hpp&quot;
 27 #include &quot;jfr/leakprofiler/utilities/granularTimer.hpp&quot;
 28 #include &quot;jfr/leakprofiler/chains/rootSetClosure.hpp&quot;
 29 #include &quot;jfr/leakprofiler/chains/edge.hpp&quot;
 30 #include &quot;jfr/leakprofiler/chains/edgeQueue.hpp&quot;
 31 #include &quot;jfr/leakprofiler/chains/edgeStore.hpp&quot;
 32 #include &quot;jfr/leakprofiler/chains/bitset.hpp&quot;
 33 #include &quot;jfr/leakprofiler/sampling/objectSample.hpp&quot;
 34 #include &quot;jfr/leakprofiler/leakProfiler.hpp&quot;
 35 #include &quot;jfr/leakprofiler/checkpoint/objectSampleCheckpoint.hpp&quot;
 36 #include &quot;jfr/leakprofiler/sampling/objectSampler.hpp&quot;
 37 #include &quot;jfr/leakprofiler/emitEventOperation.hpp&quot;
 38 #include &quot;jfr/leakprofiler/chains/bfsClosure.hpp&quot;
 39 #include &quot;jfr/leakprofiler/chains/dfsClosure.hpp&quot;
 40 #include &quot;jfr/leakprofiler/chains/objectSampleMarker.hpp&quot;
 41 #include &quot;jfr/recorder/checkpoint/jfrCheckpointWriter.hpp&quot;
 42 #include &quot;jfr/support/jfrThreadId.hpp&quot;
 43 #include &quot;logging/log.hpp&quot;
 44 #include &quot;memory/resourceArea.hpp&quot;
 45 #include &quot;memory/universe.hpp&quot;
 46 #include &quot;oops/markOop.hpp&quot;
 47 #include &quot;oops/oop.inline.hpp&quot;
 48 #include &quot;runtime/safepoint.hpp&quot;
 49 #include &quot;runtime/vmThread.hpp&quot;
 50 #include &quot;utilities/globalDefinitions.hpp&quot;
 51 
 52 /* The EdgeQueue is backed by directly managed virtual memory.
 53  * We will attempt to dimension an initial reservation
 54  * in proportion to the size of the heap (represented by heap_region).
 55  * Initial memory reservation: 5% of the heap OR at least 32 Mb
 56  * Commit ratio: 1 : 10 (subject to allocation granularties)
 57  */
 58 static size_t edge_queue_memory_reservation(const MemRegion&amp; heap_region) {
 59   const size_t memory_reservation_bytes = MAX2(heap_region.byte_size() / 20, 32*M);
 60   assert(memory_reservation_bytes &gt;= (size_t)32*M, &quot;invariant&quot;);
 61   return memory_reservation_bytes;
 62 }
 63 
 64 static size_t edge_queue_memory_commit_size(size_t memory_reservation_bytes) {
 65   const size_t memory_commit_block_size_bytes = memory_reservation_bytes / 10;
 66   assert(memory_commit_block_size_bytes &gt;= (size_t)3*M, &quot;invariant&quot;);
 67   return memory_commit_block_size_bytes;
 68 }
 69 
 70 static void log_edge_queue_summary(const EdgeQueue&amp; edge_queue) {
 71   log_trace(jfr, system)(&quot;EdgeQueue reserved size total: &quot; SIZE_FORMAT &quot; [KB]&quot;, edge_queue.reserved_size() / K);
 72   log_trace(jfr, system)(&quot;EdgeQueue edges total: &quot; SIZE_FORMAT, edge_queue.top());
 73   log_trace(jfr, system)(&quot;EdgeQueue liveset total: &quot; SIZE_FORMAT &quot; [KB]&quot;, edge_queue.live_set() / K);
 74   if (edge_queue.reserved_size() &gt; 0) {
 75     log_trace(jfr, system)(&quot;EdgeQueue commit reserve ratio: %f\n&quot;,
 76       ((double)edge_queue.live_set() / (double)edge_queue.reserved_size()));
 77   }
 78 }
 79 
 80 void EmitEventOperation::doit() {
 81   assert(LeakProfiler::is_running(), &quot;invariant&quot;);
 82   _object_sampler = LeakProfiler::object_sampler();
 83   assert(_object_sampler != NULL, &quot;invariant&quot;);
 84 
 85   _vm_thread = VMThread::vm_thread();
 86   assert(_vm_thread == Thread::current(), &quot;invariant&quot;);
 87   _vm_thread_local = _vm_thread-&gt;jfr_thread_local();
 88   assert(_vm_thread_local != NULL, &quot;invariant&quot;);
 89   assert(_vm_thread-&gt;jfr_thread_local()-&gt;thread_id() == JFR_THREAD_ID(_vm_thread), &quot;invariant&quot;);
 90 
 91   // The VM_Operation::evaluate() which invoked doit()
 92   // contains a top level ResourceMark
 93 
 94   // save the original markWord for the potential leak objects
 95   // to be restored on function exit
 96   ObjectSampleMarker marker;
 97   if (ObjectSampleCheckpoint::mark(marker, _emit_all) == 0) {
 98     return;
 99   }
100 
101   EdgeStore edge_store;
102 
103   GranularTimer::start(_cutoff_ticks, 1000000);
104   if (_cutoff_ticks &lt;= 0) {
105     // no chains
106     write_events(&amp;edge_store);
107     return;
108   }
109 
110   assert(_cutoff_ticks &gt; 0, &quot;invariant&quot;);
111 
112   // The bitset used for marking is dimensioned as a function of the heap size
113   const MemRegion heap_region = Universe::heap()-&gt;reserved_region();
114   BitSet mark_bits(heap_region);
115 
116   // The edge queue is dimensioned as a fraction of the heap size
117   const size_t edge_queue_reservation_size = edge_queue_memory_reservation(heap_region);
118   EdgeQueue edge_queue(edge_queue_reservation_size, edge_queue_memory_commit_size(edge_queue_reservation_size));
119 
120   // The initialize() routines will attempt to reserve and allocate backing storage memory.
121   // Failure to accommodate will render root chain processing impossible.
122   // As a fallback on failure, just write out the existing samples, flat, without chains.
123   if (!(mark_bits.initialize() &amp;&amp; edge_queue.initialize())) {
124     log_warning(jfr)(&quot;Unable to allocate memory for root chain processing&quot;);
125     write_events(&amp;edge_store);
126     return;
127   }
128 
129   // necessary condition for attempting a root set iteration
130   Universe::heap()-&gt;ensure_parsability(false);
131 
132   RootSetClosure::add_to_queue(&amp;edge_queue);
133   if (edge_queue.is_full()) {
134     // Pathological case where roots don&#39;t fit in queue
135     // Do a depth-first search, but mark roots first
136     // to avoid walking sideways over roots
137     DFSClosure::find_leaks_from_root_set(&amp;edge_store, &amp;mark_bits);
138   } else {
139     BFSClosure bfs(&amp;edge_queue, &amp;edge_store, &amp;mark_bits);
140     bfs.process();
141   }
142   GranularTimer::stop();
143   write_events(&amp;edge_store);
144   log_edge_queue_summary(edge_queue);
145 }
146 
147 int EmitEventOperation::write_events(EdgeStore* edge_store) {
148   assert(_object_sampler != NULL, &quot;invariant&quot;);
149   assert(edge_store != NULL, &quot;invariant&quot;);
150   assert(_vm_thread != NULL, &quot;invariant&quot;);
151   assert(_vm_thread_local != NULL, &quot;invariant&quot;);
152   assert(SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
153 
154   // save thread id in preparation for thread local trace data manipulations
155   const traceid vmthread_id = _vm_thread_local-&gt;thread_id();
156   assert(_vm_thread_local-&gt;thread_id() == JFR_THREAD_ID(_vm_thread), &quot;invariant&quot;);
157 
158   const jlong last_sweep = _emit_all ? max_jlong : _object_sampler-&gt;last_sweep().value();
159   int count = 0;
160 
161   const ObjectSample* current = _object_sampler-&gt;first();
162   while (current != NULL) {
163     ObjectSample* prev = current-&gt;prev();
164     if (current-&gt;is_alive_and_older_than(last_sweep)) {
165       write_event(current, edge_store);
166       ++count;
167     }
168     current = prev;
169   }
170 
171   // restore thread local stack trace and thread id
172   _vm_thread_local-&gt;set_thread_id(vmthread_id);
173   _vm_thread_local-&gt;clear_cached_stack_trace();
174   assert(_vm_thread_local-&gt;thread_id() == JFR_THREAD_ID(_vm_thread), &quot;invariant&quot;);
175 
176   if (count &gt; 0) {
177     // serialize assoicated checkpoints
178     ObjectSampleCheckpoint::write(edge_store, _emit_all, _vm_thread);
179   }
180   return count;
181 }
182 
183 static int array_size(const oop object) {
184   assert(object != NULL, &quot;invariant&quot;);
185   if (object-&gt;is_array()) {
186     return arrayOop(object)-&gt;length();
187   }
188   return min_jint;
189 }
190 
191 void EmitEventOperation::write_event(const ObjectSample* sample, EdgeStore* edge_store) {
192   assert(sample != NULL, &quot;invariant&quot;);
193   assert(!sample-&gt;is_dead(), &quot;invariant&quot;);
194   assert(edge_store != NULL, &quot;invariant&quot;);
195   assert(_vm_thread_local != NULL, &quot;invariant&quot;);
196   const oop* object_addr = sample-&gt;object_addr();
197   assert(*object_addr != NULL, &quot;invariant&quot;);
198 
199   const Edge* edge = (const Edge*)(*object_addr)-&gt;mark();
200   traceid gc_root_id = 0;
201   if (edge == NULL) {
202     // In order to dump out a representation of the event
203     // even though it was not reachable / too long to reach,
204     // we need to register a top level edge for this object
205     Edge e(NULL, object_addr);
206     edge_store-&gt;add_chain(&amp;e, 1);
207     edge = (const Edge*)(*object_addr)-&gt;mark();
208   } else {
209     gc_root_id = edge_store-&gt;get_root_id(edge);
210   }
211 
212   assert(edge != NULL, &quot;invariant&quot;);
213   assert(edge-&gt;pointee() == *object_addr, &quot;invariant&quot;);
214   const traceid object_id = edge_store-&gt;get_id(edge);
215   assert(object_id != 0, &quot;invariant&quot;);
216 
217   EventOldObjectSample e(UNTIMED);
218   e.set_starttime(GranularTimer::start_time());
219   e.set_endtime(GranularTimer::end_time());
220   e.set_allocationTime(sample-&gt;allocation_time());
221   e.set_lastKnownHeapUsage(sample-&gt;heap_used_at_last_gc());
222   e.set_object(object_id);
223   e.set_arrayElements(array_size(*object_addr));
224   e.set_root(gc_root_id);
225 
226   // Temporarily assigning both the stack trace id and thread id
227   // onto the thread local data structure of the VMThread (for the duration
228   // of the commit() call). This trick provides a means to override
229   // the event generation mechanism by injecting externally provided id&#39;s.
230   // Here, in particular, this allows us to emit an old object event
231   // supplying information from where the actual sampling occurred.
232   _vm_thread_local-&gt;set_cached_stack_trace_id(sample-&gt;stack_trace_id());
233   assert(sample-&gt;has_thread(), &quot;invariant&quot;);
234   _vm_thread_local-&gt;set_thread_id(sample-&gt;thread_id());
235   e.commit();
236 }
    </pre>
  </body>
</html>