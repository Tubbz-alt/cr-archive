<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/memory/virtualspace.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="universe.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="virtualspace.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/memory/virtualspace.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;logging/log.hpp&quot;
  27 #include &quot;memory/resourceArea.hpp&quot;
  28 #include &quot;memory/virtualspace.hpp&quot;
<span class="line-modified">  29 #include &quot;oops/markOop.hpp&quot;</span>

  30 #include &quot;oops/oop.inline.hpp&quot;
  31 #include &quot;runtime/os.inline.hpp&quot;
  32 #include &quot;services/memTracker.hpp&quot;
  33 #include &quot;utilities/align.hpp&quot;

  34 
  35 // ReservedSpace
  36 
  37 // Dummy constructor
  38 ReservedSpace::ReservedSpace() : _base(NULL), _size(0), _noaccess_prefix(0),
  39     _alignment(0), _special(false), _fd_for_heap(-1), _executable(false) {
  40 }
  41 
  42 ReservedSpace::ReservedSpace(size_t size, size_t preferred_page_size) : _fd_for_heap(-1) {
  43   bool has_preferred_page_size = preferred_page_size != 0;
  44   // Want to use large pages where possible and pad with small pages.
  45   size_t page_size = has_preferred_page_size ? preferred_page_size : os::page_size_for_region_unaligned(size, 1);
  46   bool large_pages = page_size != (size_t)os::vm_page_size();
  47   size_t alignment;
  48   if (large_pages &amp;&amp; has_preferred_page_size) {
  49     alignment = MAX2(page_size, (size_t)os::vm_allocation_granularity());
  50     // ReservedSpace initialization requires size to be aligned to the given
  51     // alignment. Align the size up.
  52     size = align_up(size, alignment);
  53   } else {
</pre>
<hr />
<pre>
 292   return lcm(os::vm_page_size(), alignment);
 293 }
 294 
 295 void ReservedHeapSpace::establish_noaccess_prefix() {
 296   assert(_alignment &gt;= (size_t)os::vm_page_size(), &quot;must be at least page size big&quot;);
 297   _noaccess_prefix = noaccess_prefix_size(_alignment);
 298 
 299   if (base() &amp;&amp; base() + _size &gt; (char *)OopEncodingHeapMax) {
 300     if (true
 301         WIN64_ONLY(&amp;&amp; !UseLargePages)
 302         AIX_ONLY(&amp;&amp; os::vm_page_size() != 64*K)) {
 303       // Protect memory at the base of the allocated region.
 304       // If special, the page was committed (only matters on windows)
 305       if (!os::protect_memory(_base, _noaccess_prefix, os::MEM_PROT_NONE, _special)) {
 306         fatal(&quot;cannot protect protection page&quot;);
 307       }
 308       log_debug(gc, heap, coops)(&quot;Protected page at the reserved heap base: &quot;
 309                                  PTR_FORMAT &quot; / &quot; INTX_FORMAT &quot; bytes&quot;,
 310                                  p2i(_base),
 311                                  _noaccess_prefix);
<span class="line-modified"> 312       assert(Universe::narrow_oop_use_implicit_null_checks() == true, &quot;not initialized?&quot;);</span>
 313     } else {
<span class="line-modified"> 314       Universe::set_narrow_oop_use_implicit_null_checks(false);</span>
 315     }
 316   }
 317 
 318   _base += _noaccess_prefix;
 319   _size -= _noaccess_prefix;
 320   assert(((uintptr_t)_base % _alignment == 0), &quot;must be exactly of required alignment&quot;);
 321 }
 322 
 323 // Tries to allocate memory of size &#39;size&#39; at address requested_address with alignment &#39;alignment&#39;.
 324 // Does not check whether the reserved memory actually is at requested_address, as the memory returned
 325 // might still fulfill the wishes of the caller.
 326 // Assures the memory is aligned to &#39;alignment&#39;.
 327 // NOTE: If ReservedHeapSpace already points to some reserved memory this is freed, first.
 328 void ReservedHeapSpace::try_reserve_heap(size_t size,
 329                                          size_t alignment,
 330                                          bool large,
 331                                          char* requested_address) {
 332   if (_base != NULL) {
 333     // We tried before, but we didn&#39;t like the address delivered.
 334     release();
</pre>
<hr />
<pre>
 561       char *lowest_start = aligned_heap_base_min_address;
 562       uint64_t unscaled_end = UnscaledOopHeapMax - size;
 563       if (unscaled_end &lt; UnscaledOopHeapMax) { // unscaled_end wrapped if size is large
 564         lowest_start = MAX2(lowest_start, (char*)unscaled_end);
 565       }
 566       lowest_start = align_up(lowest_start, attach_point_alignment);
 567       try_reserve_range(highest_start, lowest_start, attach_point_alignment,
 568                         aligned_heap_base_min_address, zerobased_max, size, alignment, large);
 569     }
 570 
 571     // Now we go for heaps with base != 0.  We need a noaccess prefix to efficiently
 572     // implement null checks.
 573     noaccess_prefix = noaccess_prefix_size(alignment);
 574 
 575     // Try to attach at addresses that are aligned to OopEncodingHeapMax. Disjointbase mode.
 576     char** addresses = get_attach_addresses_for_disjoint_mode();
 577     int i = 0;
 578     while (addresses[i] &amp;&amp;                                 // End of array not yet reached.
 579            ((_base == NULL) ||                             // No previous try succeeded.
 580             (_base + size &gt;  (char *)OopEncodingHeapMax &amp;&amp; // Not zerobased or unscaled address.
<span class="line-modified"> 581              !Universe::is_disjoint_heap_base_address((address)_base)))) {  // Not disjoint address.</span>
 582       char* const attach_point = addresses[i];
 583       assert(attach_point &gt;= aligned_heap_base_min_address, &quot;Flag support broken&quot;);
 584       try_reserve_heap(size + noaccess_prefix, alignment, large, attach_point);
 585       i++;
 586     }
 587 
 588     // Last, desperate try without any placement.
 589     if (_base == NULL) {
 590       log_trace(gc, heap, coops)(&quot;Trying to allocate at address NULL heap of size &quot; SIZE_FORMAT_HEX, size + noaccess_prefix);
 591       initialize(size + noaccess_prefix, alignment, large, NULL, false);
 592     }
 593   }
 594 }
 595 
 596 ReservedHeapSpace::ReservedHeapSpace(size_t size, size_t alignment, bool large, const char* heap_allocation_directory) : ReservedSpace() {
 597 
 598   if (size == 0) {
 599     return;
 600   }
 601 
</pre>
<hr />
<pre>
 605       vm_exit_during_initialization(
 606         err_msg(&quot;Could not create file for Heap at location %s&quot;, heap_allocation_directory));
 607     }
 608   }
 609 
 610   // Heap size should be aligned to alignment, too.
 611   guarantee(is_aligned(size, alignment), &quot;set by caller&quot;);
 612 
 613   if (UseCompressedOops) {
 614     initialize_compressed_heap(size, alignment, large);
 615     if (_size &gt; size) {
 616       // We allocated heap with noaccess prefix.
 617       // It can happen we get a zerobased/unscaled heap with noaccess prefix,
 618       // if we had to try at arbitrary address.
 619       establish_noaccess_prefix();
 620     }
 621   } else {
 622     initialize(size, alignment, large, NULL, false);
 623   }
 624 
<span class="line-modified"> 625   assert(markOopDesc::encode_pointer_as_mark(_base)-&gt;decode_pointer() == _base,</span>
 626          &quot;area must be distinguishable from marks for mark-sweep&quot;);
<span class="line-modified"> 627   assert(markOopDesc::encode_pointer_as_mark(&amp;_base[size])-&gt;decode_pointer() == &amp;_base[size],</span>
 628          &quot;area must be distinguishable from marks for mark-sweep&quot;);
 629 
 630   if (base() != NULL) {
 631     MemTracker::record_virtual_memory_type((address)base(), mtJavaHeap);
 632   }
 633 
 634   if (_fd_for_heap != -1) {
 635     os::close(_fd_for_heap);
 636   }
 637 }
 638 




 639 // Reserve space for code segment.  Same as Java heap only we mark this as
 640 // executable.
 641 ReservedCodeSpace::ReservedCodeSpace(size_t r_size,
 642                                      size_t rs_align,
 643                                      bool large) :
 644   ReservedSpace(r_size, rs_align, large, /*executable*/ true) {
 645   MemTracker::record_virtual_memory_type((address)base(), mtCode);
 646 }
 647 
 648 // VirtualSpace
 649 
 650 VirtualSpace::VirtualSpace() {
 651   _low_boundary           = NULL;
 652   _high_boundary          = NULL;
 653   _low                    = NULL;
 654   _high                   = NULL;
 655   _lower_high             = NULL;
 656   _middle_high            = NULL;
 657   _upper_high             = NULL;
 658   _lower_high_boundary    = NULL;
</pre>
</td>
<td>
<hr />
<pre>
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;logging/log.hpp&quot;
  27 #include &quot;memory/resourceArea.hpp&quot;
  28 #include &quot;memory/virtualspace.hpp&quot;
<span class="line-modified">  29 #include &quot;oops/compressedOops.hpp&quot;</span>
<span class="line-added">  30 #include &quot;oops/markWord.hpp&quot;</span>
  31 #include &quot;oops/oop.inline.hpp&quot;
  32 #include &quot;runtime/os.inline.hpp&quot;
  33 #include &quot;services/memTracker.hpp&quot;
  34 #include &quot;utilities/align.hpp&quot;
<span class="line-added">  35 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  36 
  37 // ReservedSpace
  38 
  39 // Dummy constructor
  40 ReservedSpace::ReservedSpace() : _base(NULL), _size(0), _noaccess_prefix(0),
  41     _alignment(0), _special(false), _fd_for_heap(-1), _executable(false) {
  42 }
  43 
  44 ReservedSpace::ReservedSpace(size_t size, size_t preferred_page_size) : _fd_for_heap(-1) {
  45   bool has_preferred_page_size = preferred_page_size != 0;
  46   // Want to use large pages where possible and pad with small pages.
  47   size_t page_size = has_preferred_page_size ? preferred_page_size : os::page_size_for_region_unaligned(size, 1);
  48   bool large_pages = page_size != (size_t)os::vm_page_size();
  49   size_t alignment;
  50   if (large_pages &amp;&amp; has_preferred_page_size) {
  51     alignment = MAX2(page_size, (size_t)os::vm_allocation_granularity());
  52     // ReservedSpace initialization requires size to be aligned to the given
  53     // alignment. Align the size up.
  54     size = align_up(size, alignment);
  55   } else {
</pre>
<hr />
<pre>
 294   return lcm(os::vm_page_size(), alignment);
 295 }
 296 
 297 void ReservedHeapSpace::establish_noaccess_prefix() {
 298   assert(_alignment &gt;= (size_t)os::vm_page_size(), &quot;must be at least page size big&quot;);
 299   _noaccess_prefix = noaccess_prefix_size(_alignment);
 300 
 301   if (base() &amp;&amp; base() + _size &gt; (char *)OopEncodingHeapMax) {
 302     if (true
 303         WIN64_ONLY(&amp;&amp; !UseLargePages)
 304         AIX_ONLY(&amp;&amp; os::vm_page_size() != 64*K)) {
 305       // Protect memory at the base of the allocated region.
 306       // If special, the page was committed (only matters on windows)
 307       if (!os::protect_memory(_base, _noaccess_prefix, os::MEM_PROT_NONE, _special)) {
 308         fatal(&quot;cannot protect protection page&quot;);
 309       }
 310       log_debug(gc, heap, coops)(&quot;Protected page at the reserved heap base: &quot;
 311                                  PTR_FORMAT &quot; / &quot; INTX_FORMAT &quot; bytes&quot;,
 312                                  p2i(_base),
 313                                  _noaccess_prefix);
<span class="line-modified"> 314       assert(CompressedOops::use_implicit_null_checks() == true, &quot;not initialized?&quot;);</span>
 315     } else {
<span class="line-modified"> 316       CompressedOops::set_use_implicit_null_checks(false);</span>
 317     }
 318   }
 319 
 320   _base += _noaccess_prefix;
 321   _size -= _noaccess_prefix;
 322   assert(((uintptr_t)_base % _alignment == 0), &quot;must be exactly of required alignment&quot;);
 323 }
 324 
 325 // Tries to allocate memory of size &#39;size&#39; at address requested_address with alignment &#39;alignment&#39;.
 326 // Does not check whether the reserved memory actually is at requested_address, as the memory returned
 327 // might still fulfill the wishes of the caller.
 328 // Assures the memory is aligned to &#39;alignment&#39;.
 329 // NOTE: If ReservedHeapSpace already points to some reserved memory this is freed, first.
 330 void ReservedHeapSpace::try_reserve_heap(size_t size,
 331                                          size_t alignment,
 332                                          bool large,
 333                                          char* requested_address) {
 334   if (_base != NULL) {
 335     // We tried before, but we didn&#39;t like the address delivered.
 336     release();
</pre>
<hr />
<pre>
 563       char *lowest_start = aligned_heap_base_min_address;
 564       uint64_t unscaled_end = UnscaledOopHeapMax - size;
 565       if (unscaled_end &lt; UnscaledOopHeapMax) { // unscaled_end wrapped if size is large
 566         lowest_start = MAX2(lowest_start, (char*)unscaled_end);
 567       }
 568       lowest_start = align_up(lowest_start, attach_point_alignment);
 569       try_reserve_range(highest_start, lowest_start, attach_point_alignment,
 570                         aligned_heap_base_min_address, zerobased_max, size, alignment, large);
 571     }
 572 
 573     // Now we go for heaps with base != 0.  We need a noaccess prefix to efficiently
 574     // implement null checks.
 575     noaccess_prefix = noaccess_prefix_size(alignment);
 576 
 577     // Try to attach at addresses that are aligned to OopEncodingHeapMax. Disjointbase mode.
 578     char** addresses = get_attach_addresses_for_disjoint_mode();
 579     int i = 0;
 580     while (addresses[i] &amp;&amp;                                 // End of array not yet reached.
 581            ((_base == NULL) ||                             // No previous try succeeded.
 582             (_base + size &gt;  (char *)OopEncodingHeapMax &amp;&amp; // Not zerobased or unscaled address.
<span class="line-modified"> 583              !CompressedOops::is_disjoint_heap_base_address((address)_base)))) {  // Not disjoint address.</span>
 584       char* const attach_point = addresses[i];
 585       assert(attach_point &gt;= aligned_heap_base_min_address, &quot;Flag support broken&quot;);
 586       try_reserve_heap(size + noaccess_prefix, alignment, large, attach_point);
 587       i++;
 588     }
 589 
 590     // Last, desperate try without any placement.
 591     if (_base == NULL) {
 592       log_trace(gc, heap, coops)(&quot;Trying to allocate at address NULL heap of size &quot; SIZE_FORMAT_HEX, size + noaccess_prefix);
 593       initialize(size + noaccess_prefix, alignment, large, NULL, false);
 594     }
 595   }
 596 }
 597 
 598 ReservedHeapSpace::ReservedHeapSpace(size_t size, size_t alignment, bool large, const char* heap_allocation_directory) : ReservedSpace() {
 599 
 600   if (size == 0) {
 601     return;
 602   }
 603 
</pre>
<hr />
<pre>
 607       vm_exit_during_initialization(
 608         err_msg(&quot;Could not create file for Heap at location %s&quot;, heap_allocation_directory));
 609     }
 610   }
 611 
 612   // Heap size should be aligned to alignment, too.
 613   guarantee(is_aligned(size, alignment), &quot;set by caller&quot;);
 614 
 615   if (UseCompressedOops) {
 616     initialize_compressed_heap(size, alignment, large);
 617     if (_size &gt; size) {
 618       // We allocated heap with noaccess prefix.
 619       // It can happen we get a zerobased/unscaled heap with noaccess prefix,
 620       // if we had to try at arbitrary address.
 621       establish_noaccess_prefix();
 622     }
 623   } else {
 624     initialize(size, alignment, large, NULL, false);
 625   }
 626 
<span class="line-modified"> 627   assert(markWord::encode_pointer_as_mark(_base).decode_pointer() == _base,</span>
 628          &quot;area must be distinguishable from marks for mark-sweep&quot;);
<span class="line-modified"> 629   assert(markWord::encode_pointer_as_mark(&amp;_base[size]).decode_pointer() == &amp;_base[size],</span>
 630          &quot;area must be distinguishable from marks for mark-sweep&quot;);
 631 
 632   if (base() != NULL) {
 633     MemTracker::record_virtual_memory_type((address)base(), mtJavaHeap);
 634   }
 635 
 636   if (_fd_for_heap != -1) {
 637     os::close(_fd_for_heap);
 638   }
 639 }
 640 
<span class="line-added"> 641 MemRegion ReservedHeapSpace::region() const {</span>
<span class="line-added"> 642   return MemRegion((HeapWord*)base(), (HeapWord*)end());</span>
<span class="line-added"> 643 }</span>
<span class="line-added"> 644 </span>
 645 // Reserve space for code segment.  Same as Java heap only we mark this as
 646 // executable.
 647 ReservedCodeSpace::ReservedCodeSpace(size_t r_size,
 648                                      size_t rs_align,
 649                                      bool large) :
 650   ReservedSpace(r_size, rs_align, large, /*executable*/ true) {
 651   MemTracker::record_virtual_memory_type((address)base(), mtCode);
 652 }
 653 
 654 // VirtualSpace
 655 
 656 VirtualSpace::VirtualSpace() {
 657   _low_boundary           = NULL;
 658   _high_boundary          = NULL;
 659   _low                    = NULL;
 660   _high                   = NULL;
 661   _lower_high             = NULL;
 662   _middle_high            = NULL;
 663   _upper_high             = NULL;
 664   _lower_high_boundary    = NULL;
</pre>
</td>
</tr>
</table>
<center><a href="universe.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="virtualspace.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>