<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/share/memory/metaspaceShared.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="metaspaceClosure.hpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="metaspaceShared.hpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/memory/metaspaceShared.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 1,7 ***</span>
  /*
<span class="line-modified">!  * Copyright (c) 2012, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
<span class="line-new-header">--- 1,7 ---</span>
  /*
<span class="line-modified">!  * Copyright (c) 2012, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 34,24 ***</span>
<span class="line-new-header">--- 34,29 ---</span>
  #include &quot;classfile/symbolTable.hpp&quot;
  #include &quot;classfile/stringTable.hpp&quot;
  #include &quot;classfile/systemDictionary.hpp&quot;
  #include &quot;classfile/systemDictionaryShared.hpp&quot;
  #include &quot;code/codeCache.hpp&quot;
<span class="line-added">+ #include &quot;gc/shared/softRefPolicy.hpp&quot;</span>
  #include &quot;interpreter/bytecodeStream.hpp&quot;
  #include &quot;interpreter/bytecodes.hpp&quot;
  #include &quot;logging/log.hpp&quot;
  #include &quot;logging/logMessage.hpp&quot;
<span class="line-added">+ #include &quot;memory/archiveUtils.inline.hpp&quot;</span>
<span class="line-added">+ #include &quot;memory/dynamicArchive.hpp&quot;</span>
  #include &quot;memory/filemap.hpp&quot;
  #include &quot;memory/heapShared.inline.hpp&quot;
  #include &quot;memory/metaspace.hpp&quot;
  #include &quot;memory/metaspaceClosure.hpp&quot;
  #include &quot;memory/metaspaceShared.hpp&quot;
  #include &quot;memory/resourceArea.hpp&quot;
<span class="line-added">+ #include &quot;memory/universe.hpp&quot;</span>
  #include &quot;oops/compressedOops.inline.hpp&quot;
  #include &quot;oops/instanceClassLoaderKlass.hpp&quot;
  #include &quot;oops/instanceMirrorKlass.hpp&quot;
  #include &quot;oops/instanceRefKlass.hpp&quot;
<span class="line-added">+ #include &quot;oops/methodData.hpp&quot;</span>
  #include &quot;oops/objArrayKlass.hpp&quot;
  #include &quot;oops/objArrayOop.hpp&quot;
  #include &quot;oops/oop.inline.hpp&quot;
  #include &quot;oops/typeArrayKlass.hpp&quot;
  #include &quot;prims/jvmtiRedefineClasses.hpp&quot;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 61,11 ***</span>
  #include &quot;runtime/signature.hpp&quot;
  #include &quot;runtime/timerTrace.hpp&quot;
  #include &quot;runtime/vmThread.hpp&quot;
  #include &quot;runtime/vmOperations.hpp&quot;
  #include &quot;utilities/align.hpp&quot;
<span class="line-modified">! #include &quot;utilities/bitMap.hpp&quot;</span>
  #include &quot;utilities/defaultStream.hpp&quot;
  #include &quot;utilities/hashtable.inline.hpp&quot;
  #if INCLUDE_G1GC
  #include &quot;gc/g1/g1CollectedHeap.hpp&quot;
  #endif
<span class="line-new-header">--- 66,11 ---</span>
  #include &quot;runtime/signature.hpp&quot;
  #include &quot;runtime/timerTrace.hpp&quot;
  #include &quot;runtime/vmThread.hpp&quot;
  #include &quot;runtime/vmOperations.hpp&quot;
  #include &quot;utilities/align.hpp&quot;
<span class="line-modified">! #include &quot;utilities/bitMap.inline.hpp&quot;</span>
  #include &quot;utilities/defaultStream.hpp&quot;
  #include &quot;utilities/hashtable.inline.hpp&quot;
  #if INCLUDE_G1GC
  #include &quot;gc/g1/g1CollectedHeap.hpp&quot;
  #endif
</pre>
<hr />
<pre>
<span class="line-old-header">*** 74,218 ***</span>
  VirtualSpace MetaspaceShared::_shared_vs;
  MetaspaceSharedStats MetaspaceShared::_stats;
  bool MetaspaceShared::_has_error_classes;
  bool MetaspaceShared::_archive_loading_failed = false;
  bool MetaspaceShared::_remapped_readwrite = false;
<span class="line-modified">! address MetaspaceShared::_cds_i2i_entry_code_buffers = NULL;</span>
<span class="line-modified">! size_t MetaspaceShared::_cds_i2i_entry_code_buffers_size = 0;</span>
<span class="line-modified">! size_t MetaspaceShared::_core_spaces_size = 0;</span>
  
  // The CDS archive is divided into the following regions:
<span class="line-modified">! //     mc  - misc code (the method entry trampolines)</span>
  //     rw  - read-write metadata
  //     ro  - read-only metadata and read-only tables
<span class="line-removed">- //     md  - misc data (the c++ vtables)</span>
<span class="line-removed">- //     od  - optional data (original class files)</span>
  //
  //     ca0 - closed archive heap space #0
  //     ca1 - closed archive heap space #1 (may be empty)
  //     oa0 - open archive heap space #0
  //     oa1 - open archive heap space #1 (may be empty)
  //
<span class="line-modified">! // The mc, rw, ro, md and od regions are linearly allocated, starting from</span>
<span class="line-modified">! // SharedBaseAddress, in the order of mc-&gt;rw-&gt;ro-&gt;md-&gt;od. The size of these 5 regions</span>
  // are page-aligned, and there&#39;s no gap between any consecutive regions.
  //
<span class="line-modified">! // These 5 regions are populated in the following steps:</span>
  // [1] All classes are loaded in MetaspaceShared::preload_classes(). All metadata are
  //     temporarily allocated outside of the shared regions. Only the method entry
  //     trampolines are written into the mc region.
<span class="line-modified">! // [2] ArchiveCompactor copies RW metadata into the rw region.</span>
<span class="line-modified">! // [3] ArchiveCompactor copies RO metadata into the ro region.</span>
<span class="line-modified">! // [4] SymbolTable, StringTable, SystemDictionary, and a few other read-only data</span>
  //     are copied into the ro region as read-only tables.
<span class="line-removed">- // [5] C++ vtables are copied into the md region.</span>
<span class="line-removed">- // [6] Original class files are copied into the od region.</span>
  //
  // The s0/s1 and oa0/oa1 regions are populated inside HeapShared::archive_java_heap_objects.
<span class="line-modified">! // Their layout is independent of the other 5 regions.</span>
  
<span class="line-modified">! class DumpRegion {</span>
<span class="line-modified">! private:</span>
<span class="line-modified">!   const char* _name;</span>
<span class="line-modified">!   char* _base;</span>
<span class="line-removed">-   char* _top;</span>
<span class="line-removed">-   char* _end;</span>
<span class="line-removed">-   bool _is_packed;</span>
<span class="line-removed">- </span>
<span class="line-removed">-   char* expand_top_to(char* newtop) {</span>
<span class="line-removed">-     assert(is_allocatable(), &quot;must be initialized and not packed&quot;);</span>
<span class="line-removed">-     assert(newtop &gt;= _top, &quot;must not grow backwards&quot;);</span>
<span class="line-removed">-     if (newtop &gt; _end) {</span>
<span class="line-removed">-       MetaspaceShared::report_out_of_space(_name, newtop - _top);</span>
<span class="line-removed">-       ShouldNotReachHere();</span>
<span class="line-removed">-     }</span>
<span class="line-removed">-     uintx delta = MetaspaceShared::object_delta_uintx(newtop);</span>
<span class="line-removed">-     if (delta &gt; MAX_SHARED_DELTA) {</span>
<span class="line-removed">-       // This is just a sanity check and should not appear in any real world usage. This</span>
<span class="line-removed">-       // happens only if you allocate more than 2GB of shared objects and would require</span>
<span class="line-removed">-       // millions of shared classes.</span>
<span class="line-removed">-       vm_exit_during_initialization(&quot;Out of memory in the CDS archive&quot;,</span>
<span class="line-removed">-                                     &quot;Please reduce the number of shared classes.&quot;);</span>
<span class="line-removed">-     }</span>
  
<span class="line-modified">!     MetaspaceShared::commit_shared_space_to(newtop);</span>
<span class="line-modified">!     _top = newtop;</span>
<span class="line-modified">!     return _top;</span>
<span class="line-modified">!   }</span>
  
<span class="line-modified">! public:</span>
<span class="line-modified">!   DumpRegion(const char* name) : _name(name), _base(NULL), _top(NULL), _end(NULL), _is_packed(false) {}</span>
<span class="line-modified">! </span>
<span class="line-modified">!   char* allocate(size_t num_bytes, size_t alignment=BytesPerWord) {</span>
<span class="line-modified">!     char* p = (char*)align_up(_top, alignment);</span>
<span class="line-modified">!     char* newtop = p + align_up(num_bytes, alignment);</span>
<span class="line-modified">!     expand_top_to(newtop);</span>
<span class="line-modified">!     memset(p, 0, newtop - p);</span>
<span class="line-removed">-     return p;</span>
<span class="line-removed">-   }</span>
<span class="line-removed">- </span>
<span class="line-removed">-   void append_intptr_t(intptr_t n) {</span>
<span class="line-removed">-     assert(is_aligned(_top, sizeof(intptr_t)), &quot;bad alignment&quot;);</span>
<span class="line-removed">-     intptr_t *p = (intptr_t*)_top;</span>
<span class="line-removed">-     char* newtop = _top + sizeof(intptr_t);</span>
<span class="line-removed">-     expand_top_to(newtop);</span>
<span class="line-removed">-     *p = n;</span>
<span class="line-removed">-   }</span>
<span class="line-removed">- </span>
<span class="line-removed">-   char* base()      const { return _base;        }</span>
<span class="line-removed">-   char* top()       const { return _top;         }</span>
<span class="line-removed">-   char* end()       const { return _end;         }</span>
<span class="line-removed">-   size_t reserved() const { return _end - _base; }</span>
<span class="line-removed">-   size_t used()     const { return _top - _base; }</span>
<span class="line-removed">-   bool is_packed()  const { return _is_packed;   }</span>
<span class="line-removed">-   bool is_allocatable() const {</span>
<span class="line-removed">-     return !is_packed() &amp;&amp; _base != NULL;</span>
<span class="line-removed">-   }</span>
<span class="line-removed">- </span>
<span class="line-removed">-   void print(size_t total_bytes) const {</span>
<span class="line-removed">-     tty-&gt;print_cr(&quot;%-3s space: &quot; SIZE_FORMAT_W(9) &quot; [ %4.1f%% of total] out of &quot; SIZE_FORMAT_W(9) &quot; bytes [%5.1f%% used] at &quot; INTPTR_FORMAT,</span>
<span class="line-removed">-                   _name, used(), percent_of(used(), total_bytes), reserved(), percent_of(used(), reserved()), p2i(_base));</span>
<span class="line-removed">-   }</span>
<span class="line-removed">-   void print_out_of_space_msg(const char* failing_region, size_t needed_bytes) {</span>
<span class="line-removed">-     tty-&gt;print(&quot;[%-8s] &quot; PTR_FORMAT &quot; - &quot; PTR_FORMAT &quot; capacity =%9d, allocated =%9d&quot;,</span>
<span class="line-removed">-                _name, p2i(_base), p2i(_top), int(_end - _base), int(_top - _base));</span>
<span class="line-removed">-     if (strcmp(_name, failing_region) == 0) {</span>
<span class="line-removed">-       tty-&gt;print_cr(&quot; required = %d&quot;, int(needed_bytes));</span>
<span class="line-removed">-     } else {</span>
<span class="line-removed">-       tty-&gt;cr();</span>
<span class="line-removed">-     }</span>
    }
  
<span class="line-modified">!   void init(const ReservedSpace* rs) {</span>
<span class="line-modified">!     _base = _top = rs-&gt;base();</span>
<span class="line-modified">!     _end = rs-&gt;end();</span>
<span class="line-modified">!   }</span>
<span class="line-modified">!   void init(char* b, char* t, char* e) {</span>
<span class="line-modified">!     _base = b;</span>
<span class="line-modified">!     _top = t;</span>
<span class="line-modified">!     _end = e;</span>
    }
  
<span class="line-modified">!   void pack(DumpRegion* next = NULL) {</span>
<span class="line-modified">!     assert(!is_packed(), &quot;sanity&quot;);</span>
<span class="line-modified">!     _end = (char*)align_up(_top, Metaspace::reserve_alignment());</span>
<span class="line-modified">!     _is_packed = true;</span>
<span class="line-modified">!     if (next != NULL) {</span>
<span class="line-modified">!       next-&gt;_base = next-&gt;_top = this-&gt;_end;</span>
<span class="line-modified">!       next-&gt;_end = MetaspaceShared::shared_rs()-&gt;end();</span>
<span class="line-removed">-     }</span>
    }
<span class="line-modified">!   bool contains(char* p) {</span>
<span class="line-modified">!     return base() &lt;= p &amp;&amp; p &lt; top();</span>
    }
<span class="line-modified">! };</span>
  
  
<span class="line-modified">! DumpRegion _mc_region(&quot;mc&quot;), _ro_region(&quot;ro&quot;), _rw_region(&quot;rw&quot;), _md_region(&quot;md&quot;);</span>
<span class="line-modified">! size_t _total_closed_archive_region_size = 0, _total_open_archive_region_size = 0;</span>
  
  char* MetaspaceShared::misc_code_space_alloc(size_t num_bytes) {
    return _mc_region.allocate(num_bytes);
  }
  
  char* MetaspaceShared::read_only_space_alloc(size_t num_bytes) {
    return _ro_region.allocate(num_bytes);
  }
  
<span class="line-modified">! void MetaspaceShared::initialize_runtime_shared_and_meta_spaces() {</span>
<span class="line-modified">!   assert(UseSharedSpaces, &quot;Must be called when UseSharedSpaces is enabled&quot;);</span>
<span class="line-modified">! </span>
<span class="line-modified">!   // If using shared space, open the file that contains the shared space</span>
<span class="line-modified">!   // and map in the memory before initializing the rest of metaspace (so</span>
<span class="line-modified">!   // the addresses don&#39;t conflict)</span>
<span class="line-modified">!   address cds_address = NULL;</span>
<span class="line-modified">!   FileMapInfo* mapinfo = new FileMapInfo();</span>
<span class="line-modified">! </span>
<span class="line-modified">!   // Open the shared archive file, read and validate the header. If</span>
<span class="line-modified">!   // initialization fails, shared spaces [UseSharedSpaces] are</span>
<span class="line-removed">-   // disabled and the file is closed.</span>
<span class="line-removed">-   // Map in spaces now also</span>
<span class="line-removed">-   if (mapinfo-&gt;initialize() &amp;&amp; map_shared_spaces(mapinfo)) {</span>
<span class="line-removed">-     size_t cds_total = core_spaces_size();</span>
<span class="line-removed">-     cds_address = (address)mapinfo-&gt;region_addr(0);</span>
<span class="line-removed">- #ifdef _LP64</span>
<span class="line-removed">-     if (Metaspace::using_class_space()) {</span>
<span class="line-removed">-       char* cds_end = (char*)(cds_address + cds_total);</span>
<span class="line-removed">-       cds_end = (char *)align_up(cds_end, Metaspace::reserve_alignment());</span>
<span class="line-removed">-       // If UseCompressedClassPointers is set then allocate the metaspace area</span>
<span class="line-removed">-       // above the heap and above the CDS area (if it exists).</span>
<span class="line-removed">-       Metaspace::allocate_metaspace_compressed_klass_ptrs(cds_end, cds_address);</span>
<span class="line-removed">-       // map_heap_regions() compares the current narrow oop and klass encodings</span>
<span class="line-removed">-       // with the archived ones, so it must be done after all encodings are determined.</span>
<span class="line-removed">-       mapinfo-&gt;map_heap_regions();</span>
<span class="line-removed">-     }</span>
<span class="line-removed">-     Universe::set_narrow_klass_range(CompressedClassSpaceSize);</span>
<span class="line-removed">- #endif // _LP64</span>
    } else {
<span class="line-modified">!     assert(!mapinfo-&gt;is_open() &amp;&amp; !UseSharedSpaces,</span>
<span class="line-modified">!            &quot;archive file not closed or shared spaces not disabled.&quot;);</span>
    }
  }
  
  void MetaspaceShared::initialize_dumptime_shared_and_meta_spaces() {
    assert(DumpSharedSpaces, &quot;should be called for dump time only&quot;);
<span class="line-modified">!   const size_t reserve_alignment = Metaspace::reserve_alignment();</span>
<span class="line-removed">-   bool large_pages = false; // No large pages when dumping the CDS archive.</span>
    char* shared_base = (char*)align_up((char*)SharedBaseAddress, reserve_alignment);
  
  #ifdef _LP64
    // On 64-bit VM, the heap and class space layout will be the same as if
    // you&#39;re running in -Xshare:on mode:
    //
    //                              +-- SharedBaseAddress (default = 0x800000000)
    //                              v
<span class="line-modified">!   // +-..---------+---------+ ... +----+----+----+----+----+---------------+</span>
<span class="line-modified">!   // |    Heap    | Archive |     | MC | RW | RO | MD | OD | class space   |</span>
<span class="line-modified">!   // +-..---------+---------+ ... +----+----+----+----+----+---------------+</span>
<span class="line-modified">!   // |&lt;--   MaxHeapSize  --&gt;|     |&lt;-- UnscaledClassSpaceMax = 4GB -------&gt;|</span>
    //
    const uint64_t UnscaledClassSpaceMax = (uint64_t(max_juint) + 1);
    const size_t cds_total = align_down(UnscaledClassSpaceMax, reserve_alignment);
  #else
    // We don&#39;t support archives larger than 256MB on 32-bit due to limited virtual address space.
    size_t cds_total = align_down(256*M, reserve_alignment);
  #endif
  
    // First try to reserve the space at the specified SharedBaseAddress.
<span class="line-modified">!   _shared_rs = ReservedSpace(cds_total, reserve_alignment, large_pages, shared_base);</span>
    if (_shared_rs.is_reserved()) {
      assert(shared_base == 0 || _shared_rs.base() == shared_base, &quot;should match&quot;);
    } else {
      // Get a mmap region anywhere if the SharedBaseAddress fails.
<span class="line-modified">!     _shared_rs = ReservedSpace(cds_total, reserve_alignment, large_pages);</span>
    }
    if (!_shared_rs.is_reserved()) {
      vm_exit_during_initialization(&quot;Unable to reserve memory for shared space&quot;,
                                    err_msg(SIZE_FORMAT &quot; bytes.&quot;, cds_total));
    }
<span class="line-new-header">--- 79,209 ---</span>
  VirtualSpace MetaspaceShared::_shared_vs;
  MetaspaceSharedStats MetaspaceShared::_stats;
  bool MetaspaceShared::_has_error_classes;
  bool MetaspaceShared::_archive_loading_failed = false;
  bool MetaspaceShared::_remapped_readwrite = false;
<span class="line-modified">! address MetaspaceShared::_i2i_entry_code_buffers = NULL;</span>
<span class="line-modified">! size_t MetaspaceShared::_i2i_entry_code_buffers_size = 0;</span>
<span class="line-modified">! void* MetaspaceShared::_shared_metaspace_static_top = NULL;</span>
<span class="line-added">+ intx MetaspaceShared::_relocation_delta;</span>
  
  // The CDS archive is divided into the following regions:
<span class="line-modified">! //     mc  - misc code (the method entry trampolines, c++ vtables)</span>
  //     rw  - read-write metadata
  //     ro  - read-only metadata and read-only tables
  //
  //     ca0 - closed archive heap space #0
  //     ca1 - closed archive heap space #1 (may be empty)
  //     oa0 - open archive heap space #0
  //     oa1 - open archive heap space #1 (may be empty)
  //
<span class="line-modified">! // The mc, rw, and ro regions are linearly allocated, starting from</span>
<span class="line-modified">! // SharedBaseAddress, in the order of mc-&gt;rw-&gt;ro. The size of these 3 regions</span>
  // are page-aligned, and there&#39;s no gap between any consecutive regions.
  //
<span class="line-modified">! // These 3 regions are populated in the following steps:</span>
  // [1] All classes are loaded in MetaspaceShared::preload_classes(). All metadata are
  //     temporarily allocated outside of the shared regions. Only the method entry
  //     trampolines are written into the mc region.
<span class="line-modified">! // [2] C++ vtables are copied into the mc region.</span>
<span class="line-modified">! // [3] ArchiveCompactor copies RW metadata into the rw region.</span>
<span class="line-modified">! // [4] ArchiveCompactor copies RO metadata into the ro region.</span>
<span class="line-added">+ // [5] SymbolTable, StringTable, SystemDictionary, and a few other read-only data</span>
  //     are copied into the ro region as read-only tables.
  //
  // The s0/s1 and oa0/oa1 regions are populated inside HeapShared::archive_java_heap_objects.
<span class="line-modified">! // Their layout is independent of the other 4 regions.</span>
<span class="line-added">+ </span>
<span class="line-added">+ char* DumpRegion::expand_top_to(char* newtop) {</span>
<span class="line-added">+   assert(is_allocatable(), &quot;must be initialized and not packed&quot;);</span>
<span class="line-added">+   assert(newtop &gt;= _top, &quot;must not grow backwards&quot;);</span>
<span class="line-added">+   if (newtop &gt; _end) {</span>
<span class="line-added">+     MetaspaceShared::report_out_of_space(_name, newtop - _top);</span>
<span class="line-added">+     ShouldNotReachHere();</span>
<span class="line-added">+   }</span>
<span class="line-added">+   uintx delta;</span>
<span class="line-added">+   if (DynamicDumpSharedSpaces) {</span>
<span class="line-added">+     delta = DynamicArchive::object_delta_uintx(newtop);</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     delta = MetaspaceShared::object_delta_uintx(newtop);</span>
<span class="line-added">+   }</span>
<span class="line-added">+   if (delta &gt; MAX_SHARED_DELTA) {</span>
<span class="line-added">+     // This is just a sanity check and should not appear in any real world usage. This</span>
<span class="line-added">+     // happens only if you allocate more than 2GB of shared objects and would require</span>
<span class="line-added">+     // millions of shared classes.</span>
<span class="line-added">+     vm_exit_during_initialization(&quot;Out of memory in the CDS archive&quot;,</span>
<span class="line-added">+                                   &quot;Please reduce the number of shared classes.&quot;);</span>
<span class="line-added">+   }</span>
  
<span class="line-modified">!   MetaspaceShared::commit_shared_space_to(newtop);</span>
<span class="line-modified">!   _top = newtop;</span>
<span class="line-modified">!   return _top;</span>
<span class="line-modified">! }</span>
  
<span class="line-modified">! char* DumpRegion::allocate(size_t num_bytes, size_t alignment) {</span>
<span class="line-modified">!   char* p = (char*)align_up(_top, alignment);</span>
<span class="line-modified">!   char* newtop = p + align_up(num_bytes, alignment);</span>
<span class="line-modified">!   expand_top_to(newtop);</span>
<span class="line-added">+   memset(p, 0, newtop - p);</span>
<span class="line-added">+   return p;</span>
<span class="line-added">+ }</span>
  
<span class="line-modified">! void DumpRegion::append_intptr_t(intptr_t n, bool need_to_mark) {</span>
<span class="line-modified">!   assert(is_aligned(_top, sizeof(intptr_t)), &quot;bad alignment&quot;);</span>
<span class="line-modified">!   intptr_t *p = (intptr_t*)_top;</span>
<span class="line-modified">!   char* newtop = _top + sizeof(intptr_t);</span>
<span class="line-modified">!   expand_top_to(newtop);</span>
<span class="line-modified">!   *p = n;</span>
<span class="line-modified">!   if (need_to_mark) {</span>
<span class="line-modified">!     ArchivePtrMarker::mark_pointer(p);</span>
    }
<span class="line-added">+ }</span>
  
<span class="line-modified">! void DumpRegion::print(size_t total_bytes) const {</span>
<span class="line-modified">!   log_debug(cds)(&quot;%-3s space: &quot; SIZE_FORMAT_W(9) &quot; [ %4.1f%% of total] out of &quot; SIZE_FORMAT_W(9) &quot; bytes [%5.1f%% used] at &quot; INTPTR_FORMAT,</span>
<span class="line-modified">!                  _name, used(), percent_of(used(), total_bytes), reserved(), percent_of(used(), reserved()),</span>
<span class="line-modified">!                  p2i(_base + MetaspaceShared::final_delta()));</span>
<span class="line-modified">! }</span>
<span class="line-modified">! </span>
<span class="line-modified">! void DumpRegion::print_out_of_space_msg(const char* failing_region, size_t needed_bytes) {</span>
<span class="line-modified">!   log_error(cds)(&quot;[%-8s] &quot; PTR_FORMAT &quot; - &quot; PTR_FORMAT &quot; capacity =%9d, allocated =%9d&quot;,</span>
<span class="line-added">+                  _name, p2i(_base), p2i(_top), int(_end - _base), int(_top - _base));</span>
<span class="line-added">+   if (strcmp(_name, failing_region) == 0) {</span>
<span class="line-added">+     log_error(cds)(&quot; required = %d&quot;, int(needed_bytes));</span>
    }
<span class="line-added">+ }</span>
  
<span class="line-modified">! void DumpRegion::pack(DumpRegion* next) {</span>
<span class="line-modified">!   assert(!is_packed(), &quot;sanity&quot;);</span>
<span class="line-modified">!   _end = (char*)align_up(_top, Metaspace::reserve_alignment());</span>
<span class="line-modified">!   _is_packed = true;</span>
<span class="line-modified">!   if (next != NULL) {</span>
<span class="line-modified">!     next-&gt;_base = next-&gt;_top = this-&gt;_end;</span>
<span class="line-modified">!     next-&gt;_end = MetaspaceShared::shared_rs()-&gt;end();</span>
    }
<span class="line-modified">! }</span>
<span class="line-modified">! </span>
<span class="line-added">+ static DumpRegion _mc_region(&quot;mc&quot;), _ro_region(&quot;ro&quot;), _rw_region(&quot;rw&quot;);</span>
<span class="line-added">+ static size_t _total_closed_archive_region_size = 0, _total_open_archive_region_size = 0;</span>
<span class="line-added">+ </span>
<span class="line-added">+ void MetaspaceShared::init_shared_dump_space(DumpRegion* first_space, address first_space_bottom) {</span>
<span class="line-added">+   // Start with 0 committed bytes. The memory will be committed as needed by</span>
<span class="line-added">+   // MetaspaceShared::commit_shared_space_to().</span>
<span class="line-added">+   if (!_shared_vs.initialize(_shared_rs, 0)) {</span>
<span class="line-added">+     fatal(&quot;Unable to allocate memory for shared space&quot;);</span>
    }
<span class="line-modified">!   first_space-&gt;init(&amp;_shared_rs, (char*)first_space_bottom);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ DumpRegion* MetaspaceShared::misc_code_dump_space() {</span>
<span class="line-added">+   return &amp;_mc_region;</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ DumpRegion* MetaspaceShared::read_write_dump_space() {</span>
<span class="line-added">+   return &amp;_rw_region;</span>
<span class="line-added">+ }</span>
  
<span class="line-added">+ DumpRegion* MetaspaceShared::read_only_dump_space() {</span>
<span class="line-added">+   return &amp;_ro_region;</span>
<span class="line-added">+ }</span>
  
<span class="line-modified">! void MetaspaceShared::pack_dump_space(DumpRegion* current, DumpRegion* next,</span>
<span class="line-modified">!                                       ReservedSpace* rs) {</span>
<span class="line-added">+   current-&gt;pack(next);</span>
<span class="line-added">+ }</span>
  
  char* MetaspaceShared::misc_code_space_alloc(size_t num_bytes) {
    return _mc_region.allocate(num_bytes);
  }
  
  char* MetaspaceShared::read_only_space_alloc(size_t num_bytes) {
    return _ro_region.allocate(num_bytes);
  }
  
<span class="line-modified">! // When reserving an address range using ReservedSpace, we need an alignment that satisfies both:</span>
<span class="line-modified">! // os::vm_allocation_granularity() -- so that we can sub-divide this range into multiple mmap regions,</span>
<span class="line-modified">! //                                    while keeping the first range at offset 0 of this range.</span>
<span class="line-modified">! // Metaspace::reserve_alignment()  -- so we can pass the region to</span>
<span class="line-modified">! //                                    Metaspace::allocate_metaspace_compressed_klass_ptrs.</span>
<span class="line-modified">! size_t MetaspaceShared::reserved_space_alignment() {</span>
<span class="line-modified">!   size_t os_align = os::vm_allocation_granularity();</span>
<span class="line-modified">!   size_t ms_align = Metaspace::reserve_alignment();</span>
<span class="line-modified">!   if (os_align &gt;= ms_align) {</span>
<span class="line-modified">!     assert(os_align % ms_align == 0, &quot;must be a multiple&quot;);</span>
<span class="line-modified">!     return os_align;</span>
    } else {
<span class="line-modified">!     assert(ms_align % os_align == 0, &quot;must be a multiple&quot;);</span>
<span class="line-modified">!     return ms_align;</span>
    }
  }
  
<span class="line-added">+ ReservedSpace MetaspaceShared::reserve_shared_space(size_t size, char* requested_address) {</span>
<span class="line-added">+   return Metaspace::reserve_space(size, reserved_space_alignment(),</span>
<span class="line-added">+                                   requested_address, requested_address != NULL);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
  void MetaspaceShared::initialize_dumptime_shared_and_meta_spaces() {
    assert(DumpSharedSpaces, &quot;should be called for dump time only&quot;);
<span class="line-modified">!   const size_t reserve_alignment = reserved_space_alignment();</span>
    char* shared_base = (char*)align_up((char*)SharedBaseAddress, reserve_alignment);
  
  #ifdef _LP64
    // On 64-bit VM, the heap and class space layout will be the same as if
    // you&#39;re running in -Xshare:on mode:
    //
    //                              +-- SharedBaseAddress (default = 0x800000000)
    //                              v
<span class="line-modified">!   // +-..---------+---------+ ... +----+----+----+--------------------+</span>
<span class="line-modified">!   // |    Heap    | Archive |     | MC | RW | RO |    class space     |</span>
<span class="line-modified">!   // +-..---------+---------+ ... +----+----+----+--------------------+</span>
<span class="line-modified">!   // |&lt;--   MaxHeapSize  --&gt;|     |&lt;-- UnscaledClassSpaceMax = 4GB --&gt;|</span>
    //
    const uint64_t UnscaledClassSpaceMax = (uint64_t(max_juint) + 1);
    const size_t cds_total = align_down(UnscaledClassSpaceMax, reserve_alignment);
  #else
    // We don&#39;t support archives larger than 256MB on 32-bit due to limited virtual address space.
    size_t cds_total = align_down(256*M, reserve_alignment);
  #endif
  
<span class="line-added">+   bool use_requested_base = true;</span>
<span class="line-added">+   if (ArchiveRelocationMode == 1) {</span>
<span class="line-added">+     log_info(cds)(&quot;ArchiveRelocationMode == 1: always allocate class space at an alternative address&quot;);</span>
<span class="line-added">+     use_requested_base = false;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
    // First try to reserve the space at the specified SharedBaseAddress.
<span class="line-modified">!   assert(!_shared_rs.is_reserved(), &quot;must be&quot;);</span>
<span class="line-added">+   if (use_requested_base) {</span>
<span class="line-added">+     _shared_rs = reserve_shared_space(cds_total, shared_base);</span>
<span class="line-added">+   }</span>
    if (_shared_rs.is_reserved()) {
      assert(shared_base == 0 || _shared_rs.base() == shared_base, &quot;should match&quot;);
    } else {
      // Get a mmap region anywhere if the SharedBaseAddress fails.
<span class="line-modified">!     _shared_rs = reserve_shared_space(cds_total);</span>
    }
    if (!_shared_rs.is_reserved()) {
      vm_exit_during_initialization(&quot;Unable to reserve memory for shared space&quot;,
                                    err_msg(SIZE_FORMAT &quot; bytes.&quot;, cds_total));
    }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 296,55 ***</span>
    //   will store Klasses into this space.
    // + The lower 3 GB is used for the archive -- when preload_classes() is done,
    //   ArchiveCompactor will copy the class metadata into this space, first the RW parts,
    //   then the RO parts.
  
<span class="line-removed">-   assert(UseCompressedOops &amp;&amp; UseCompressedClassPointers,</span>
<span class="line-removed">-       &quot;UseCompressedOops and UseCompressedClassPointers must be set&quot;);</span>
<span class="line-removed">- </span>
    size_t max_archive_size = align_down(cds_total * 3 / 4, reserve_alignment);
    ReservedSpace tmp_class_space = _shared_rs.last_part(max_archive_size);
    CompressedClassSpaceSize = align_down(tmp_class_space.size(), reserve_alignment);
    _shared_rs = _shared_rs.first_part(max_archive_size);
  
<span class="line-modified">!   // Set up compress class pointers.</span>
<span class="line-modified">!   Universe::set_narrow_klass_base((address)_shared_rs.base());</span>
<span class="line-modified">!   // Set narrow_klass_shift to be LogKlassAlignmentInBytes. This is consistent</span>
<span class="line-modified">!   // with AOT.</span>
<span class="line-modified">!   Universe::set_narrow_klass_shift(LogKlassAlignmentInBytes);</span>
<span class="line-modified">!   // Set the range of klass addresses to 4GB.</span>
<span class="line-modified">!   Universe::set_narrow_klass_range(cds_total);</span>
<span class="line-modified">! </span>
<span class="line-modified">!   Metaspace::initialize_class_space(tmp_class_space);</span>
    log_info(cds)(&quot;narrow_klass_base = &quot; PTR_FORMAT &quot;, narrow_klass_shift = %d&quot;,
<span class="line-modified">!                 p2i(Universe::narrow_klass_base()), Universe::narrow_klass_shift());</span>
  
    log_info(cds)(&quot;Allocated temporary class space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,
                  CompressedClassSpaceSize, p2i(tmp_class_space.base()));
  #endif
  
<span class="line-modified">!   // Start with 0 committed bytes. The memory will be committed as needed by</span>
<span class="line-removed">-   // MetaspaceShared::commit_shared_space_to().</span>
<span class="line-removed">-   if (!_shared_vs.initialize(_shared_rs, 0)) {</span>
<span class="line-removed">-     vm_exit_during_initialization(&quot;Unable to allocate memory for shared space&quot;);</span>
<span class="line-removed">-   }</span>
<span class="line-removed">- </span>
<span class="line-removed">-   _mc_region.init(&amp;_shared_rs);</span>
    SharedBaseAddress = (size_t)_shared_rs.base();
<span class="line-modified">!   tty-&gt;print_cr(&quot;Allocated shared space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,</span>
                  _shared_rs.size(), p2i(_shared_rs.base()));
  }
  
  // Called by universe_post_init()
  void MetaspaceShared::post_initialize(TRAPS) {
    if (UseSharedSpaces) {
      int size = FileMapInfo::get_number_of_shared_paths();
      if (size &gt; 0) {
        SystemDictionaryShared::allocate_shared_data_arrays(size, THREAD);
<span class="line-modified">!       FileMapHeader* header = FileMapInfo::current_info()-&gt;header();</span>
<span class="line-modified">!       ClassLoaderExt::init_paths_start_index(header-&gt;_app_class_paths_start_index);</span>
<span class="line-modified">!       ClassLoaderExt::init_app_module_paths_start_index(header-&gt;_app_module_paths_start_index);</span>
      }
    }
  }
  
  static GrowableArray&lt;Handle&gt;* _extra_interned_strings = NULL;
<span class="line-new-header">--- 292,54 ---</span>
    //   will store Klasses into this space.
    // + The lower 3 GB is used for the archive -- when preload_classes() is done,
    //   ArchiveCompactor will copy the class metadata into this space, first the RW parts,
    //   then the RO parts.
  
    size_t max_archive_size = align_down(cds_total * 3 / 4, reserve_alignment);
    ReservedSpace tmp_class_space = _shared_rs.last_part(max_archive_size);
    CompressedClassSpaceSize = align_down(tmp_class_space.size(), reserve_alignment);
    _shared_rs = _shared_rs.first_part(max_archive_size);
  
<span class="line-modified">!   if (UseCompressedClassPointers) {</span>
<span class="line-modified">!     // Set up compress class pointers.</span>
<span class="line-modified">!     CompressedKlassPointers::set_base((address)_shared_rs.base());</span>
<span class="line-modified">!     // Set narrow_klass_shift to be LogKlassAlignmentInBytes. This is consistent</span>
<span class="line-modified">!     // with AOT.</span>
<span class="line-modified">!     CompressedKlassPointers::set_shift(LogKlassAlignmentInBytes);</span>
<span class="line-modified">!     // Set the range of klass addresses to 4GB.</span>
<span class="line-modified">!     CompressedKlassPointers::set_range(cds_total);</span>
<span class="line-modified">!     Metaspace::initialize_class_space(tmp_class_space);</span>
<span class="line-added">+   }</span>
    log_info(cds)(&quot;narrow_klass_base = &quot; PTR_FORMAT &quot;, narrow_klass_shift = %d&quot;,
<span class="line-modified">!                 p2i(CompressedKlassPointers::base()), CompressedKlassPointers::shift());</span>
  
    log_info(cds)(&quot;Allocated temporary class space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,
                  CompressedClassSpaceSize, p2i(tmp_class_space.base()));
  #endif
  
<span class="line-modified">!   init_shared_dump_space(&amp;_mc_region);</span>
    SharedBaseAddress = (size_t)_shared_rs.base();
<span class="line-modified">!   log_info(cds)(&quot;Allocated shared space: &quot; SIZE_FORMAT &quot; bytes at &quot; PTR_FORMAT,</span>
                  _shared_rs.size(), p2i(_shared_rs.base()));
  }
  
  // Called by universe_post_init()
  void MetaspaceShared::post_initialize(TRAPS) {
    if (UseSharedSpaces) {
      int size = FileMapInfo::get_number_of_shared_paths();
      if (size &gt; 0) {
        SystemDictionaryShared::allocate_shared_data_arrays(size, THREAD);
<span class="line-modified">!       if (!DynamicDumpSharedSpaces) {</span>
<span class="line-modified">!         FileMapInfo* info;</span>
<span class="line-modified">!         if (FileMapInfo::dynamic_info() == NULL) {</span>
<span class="line-added">+           info = FileMapInfo::current_info();</span>
<span class="line-added">+         } else {</span>
<span class="line-added">+           info = FileMapInfo::dynamic_info();</span>
<span class="line-added">+         }</span>
<span class="line-added">+         ClassLoaderExt::init_paths_start_index(info-&gt;app_class_paths_start_index());</span>
<span class="line-added">+         ClassLoaderExt::init_app_module_paths_start_index(info-&gt;app_module_paths_start_index());</span>
<span class="line-added">+       }</span>
      }
    }
  }
  
  static GrowableArray&lt;Handle&gt;* _extra_interned_strings = NULL;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 367,11 ***</span>
      char* utf8_buffer = NEW_RESOURCE_ARRAY(char, buf_len);
      reader.get_utf8(utf8_buffer, utf8_length);
      utf8_buffer[utf8_length] = &#39;\0&#39;;
  
      if (prefix_type == HashtableTextDump::SymbolPrefix) {
<span class="line-modified">!       SymbolTable::new_permanent_symbol(utf8_buffer, THREAD);</span>
      } else{
        assert(prefix_type == HashtableTextDump::StringPrefix, &quot;Sanity&quot;);
        oop s = StringTable::intern(utf8_buffer, THREAD);
  
        if (HAS_PENDING_EXCEPTION) {
<span class="line-new-header">--- 362,11 ---</span>
      char* utf8_buffer = NEW_RESOURCE_ARRAY(char, buf_len);
      reader.get_utf8(utf8_buffer, utf8_length);
      utf8_buffer[utf8_length] = &#39;\0&#39;;
  
      if (prefix_type == HashtableTextDump::SymbolPrefix) {
<span class="line-modified">!       SymbolTable::new_permanent_symbol(utf8_buffer);</span>
      } else{
        assert(prefix_type == HashtableTextDump::StringPrefix, &quot;Sanity&quot;);
        oop s = StringTable::intern(utf8_buffer, THREAD);
  
        if (HAS_PENDING_EXCEPTION) {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 401,11 ***</span>
      }
    }
  }
  
  void MetaspaceShared::commit_shared_space_to(char* newtop) {
<span class="line-modified">!   assert(DumpSharedSpaces, &quot;dump-time only&quot;);</span>
    char* base = _shared_rs.base();
    size_t need_committed_size = newtop - base;
    size_t has_committed_size = _shared_vs.committed_size();
    if (need_committed_size &lt; has_committed_size) {
      return;
<span class="line-new-header">--- 396,11 ---</span>
      }
    }
  }
  
  void MetaspaceShared::commit_shared_space_to(char* newtop) {
<span class="line-modified">!   Arguments::assert_is_dumping_archive();</span>
    char* base = _shared_rs.base();
    size_t need_committed_size = newtop - base;
    size_t has_committed_size = _shared_vs.committed_size();
    if (need_committed_size &lt; has_committed_size) {
      return;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 413,21 ***</span>
  
    size_t min_bytes = need_committed_size - has_committed_size;
    size_t preferred_bytes = 1 * M;
    size_t uncommitted = _shared_vs.reserved_size() - has_committed_size;
  
<span class="line-modified">!   size_t commit = MAX2(min_bytes, preferred_bytes);</span>
    assert(commit &lt;= uncommitted, &quot;sanity&quot;);
  
    bool result = _shared_vs.expand_by(commit, false);
    if (!result) {
      vm_exit_during_initialization(err_msg(&quot;Failed to expand shared space to &quot; SIZE_FORMAT &quot; bytes&quot;,
                                            need_committed_size));
    }
  
<span class="line-modified">!   log_info(cds)(&quot;Expanding shared spaces by &quot; SIZE_FORMAT_W(7) &quot; bytes [total &quot; SIZE_FORMAT_W(9)  &quot; bytes ending at %p]&quot;,</span>
<span class="line-modified">!                 commit, _shared_vs.actual_committed_size(), _shared_vs.high());</span>
  }
  
  // Read/write a data stream for restoring/preserving metadata pointers and
  // miscellaneous data from/to the shared archive file.
  
<span class="line-new-header">--- 408,28 ---</span>
  
    size_t min_bytes = need_committed_size - has_committed_size;
    size_t preferred_bytes = 1 * M;
    size_t uncommitted = _shared_vs.reserved_size() - has_committed_size;
  
<span class="line-modified">!   size_t commit =MAX2(min_bytes, preferred_bytes);</span>
<span class="line-added">+   commit = MIN2(commit, uncommitted);</span>
    assert(commit &lt;= uncommitted, &quot;sanity&quot;);
  
    bool result = _shared_vs.expand_by(commit, false);
<span class="line-added">+   ArchivePtrMarker::expand_ptr_end((address*)_shared_vs.high());</span>
<span class="line-added">+ </span>
    if (!result) {
      vm_exit_during_initialization(err_msg(&quot;Failed to expand shared space to &quot; SIZE_FORMAT &quot; bytes&quot;,
                                            need_committed_size));
    }
  
<span class="line-modified">!   log_debug(cds)(&quot;Expanding shared spaces by &quot; SIZE_FORMAT_W(7) &quot; bytes [total &quot; SIZE_FORMAT_W(9)  &quot; bytes ending at %p]&quot;,</span>
<span class="line-modified">!                  commit, _shared_vs.actual_committed_size(), _shared_vs.high());</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ void MetaspaceShared::initialize_ptr_marker(CHeapBitMap* ptrmap) {</span>
<span class="line-added">+   ArchivePtrMarker::initialize(ptrmap, (address*)_shared_vs.low(), (address*)_shared_vs.high());</span>
  }
  
  // Read/write a data stream for restoring/preserving metadata pointers and
  // miscellaneous data from/to the shared archive file.
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 444,10 ***</span>
<span class="line-new-header">--- 446,11 ---</span>
    soc-&gt;do_tag(objArrayOopDesc::base_offset_in_bytes());
    soc-&gt;do_tag(typeArrayOopDesc::base_offset_in_bytes(T_BYTE));
    soc-&gt;do_tag(sizeof(Symbol));
  
    // Dump/restore miscellaneous metadata.
<span class="line-added">+   JavaClasses::serialize_offsets(soc);</span>
    Universe::serialize(soc);
    soc-&gt;do_tag(--tag);
  
    // Dump/restore references to commonly used names and signatures.
    vmSymbols::serialize(soc);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 457,31 ***</span>
    SymbolTable::serialize_shared_table_header(soc);
    StringTable::serialize_shared_table_header(soc);
    HeapShared::serialize_subgraph_info_table_header(soc);
    SystemDictionaryShared::serialize_dictionary_headers(soc);
  
<span class="line-removed">-   JavaClasses::serialize_offsets(soc);</span>
    InstanceMirrorKlass::serialize_offsets(soc);
    soc-&gt;do_tag(--tag);
  
    soc-&gt;do_tag(666);
  }
  
<span class="line-modified">! address MetaspaceShared::cds_i2i_entry_code_buffers(size_t total_size) {</span>
    if (DumpSharedSpaces) {
<span class="line-modified">!     if (_cds_i2i_entry_code_buffers == NULL) {</span>
<span class="line-modified">!       _cds_i2i_entry_code_buffers = (address)misc_code_space_alloc(total_size);</span>
<span class="line-modified">!       _cds_i2i_entry_code_buffers_size = total_size;</span>
      }
    } else if (UseSharedSpaces) {
<span class="line-modified">!     assert(_cds_i2i_entry_code_buffers != NULL, &quot;must already been initialized&quot;);</span>
    } else {
      return NULL;
    }
  
<span class="line-modified">!   assert(_cds_i2i_entry_code_buffers_size == total_size, &quot;must not change&quot;);</span>
<span class="line-modified">!   return _cds_i2i_entry_code_buffers;</span>
  }
  
  // Global object for holding classes that have been loaded.  Since this
  // is run at a safepoint just before exit, this is the entire set of classes.
  static GrowableArray&lt;Klass*&gt;* _global_klass_objects;
<span class="line-new-header">--- 460,45 ---</span>
    SymbolTable::serialize_shared_table_header(soc);
    StringTable::serialize_shared_table_header(soc);
    HeapShared::serialize_subgraph_info_table_header(soc);
    SystemDictionaryShared::serialize_dictionary_headers(soc);
  
    InstanceMirrorKlass::serialize_offsets(soc);
    soc-&gt;do_tag(--tag);
  
<span class="line-added">+   serialize_cloned_cpp_vtptrs(soc);</span>
<span class="line-added">+   soc-&gt;do_tag(--tag);</span>
<span class="line-added">+ </span>
    soc-&gt;do_tag(666);
  }
  
<span class="line-modified">! address MetaspaceShared::i2i_entry_code_buffers(size_t total_size) {</span>
    if (DumpSharedSpaces) {
<span class="line-modified">!     if (_i2i_entry_code_buffers == NULL) {</span>
<span class="line-modified">!       _i2i_entry_code_buffers = (address)misc_code_space_alloc(total_size);</span>
<span class="line-modified">!       _i2i_entry_code_buffers_size = total_size;</span>
      }
    } else if (UseSharedSpaces) {
<span class="line-modified">!     assert(_i2i_entry_code_buffers != NULL, &quot;must already been initialized&quot;);</span>
    } else {
      return NULL;
    }
  
<span class="line-modified">!   assert(_i2i_entry_code_buffers_size == total_size, &quot;must not change&quot;);</span>
<span class="line-modified">!   return _i2i_entry_code_buffers;</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ uintx MetaspaceShared::object_delta_uintx(void* obj) {</span>
<span class="line-added">+   Arguments::assert_is_dumping_archive();</span>
<span class="line-added">+   if (DumpSharedSpaces) {</span>
<span class="line-added">+     assert(shared_rs()-&gt;contains(obj), &quot;must be&quot;);</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     assert(is_in_shared_metaspace(obj) || DynamicArchive::is_in_target_space(obj), &quot;must be&quot;);</span>
<span class="line-added">+   }</span>
<span class="line-added">+   address base_address = address(SharedBaseAddress);</span>
<span class="line-added">+   uintx deltax = address(obj) - base_address;</span>
<span class="line-added">+   return deltax;</span>
  }
  
  // Global object for holding classes that have been loaded.  Since this
  // is run at a safepoint just before exit, this is the entire set of classes.
  static GrowableArray&lt;Klass*&gt;* _global_klass_objects;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 556,11 ***</span>
    Universe::set_long_mirror(NULL);
    Universe::set_short_mirror(NULL);
    Universe::set_void_mirror(NULL);
  }
  
<span class="line-modified">! static void rewrite_nofast_bytecode(Method* method) {</span>
    BytecodeStream bcs(method);
    while (!bcs.is_last_bytecode()) {
      Bytecodes::Code opcode = bcs.next();
      switch (opcode) {
      case Bytecodes::_getfield:      *bcs.bcp() = Bytecodes::_nofast_getfield;      break;
<span class="line-new-header">--- 573,11 ---</span>
    Universe::set_long_mirror(NULL);
    Universe::set_short_mirror(NULL);
    Universe::set_void_mirror(NULL);
  }
  
<span class="line-modified">! static void rewrite_nofast_bytecode(const methodHandle&amp; method) {</span>
    BytecodeStream bcs(method);
    while (!bcs.is_last_bytecode()) {
      Bytecodes::Code opcode = bcs.next();
      switch (opcode) {
      case Bytecodes::_getfield:      *bcs.bcp() = Bytecodes::_nofast_getfield;      break;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 580,26 ***</span>
  // Walk all methods in the class list to ensure that they won&#39;t be modified at
  // run time. This includes:
  // [1] Rewrite all bytecodes as needed, so that the ConstMethod* will not be modified
  //     at run time by RewriteBytecodes/RewriteFrequentPairs
  // [2] Assign a fingerprint, so one doesn&#39;t need to be assigned at run-time.
<span class="line-modified">! static void rewrite_nofast_bytecodes_and_calculate_fingerprints() {</span>
    for (int i = 0; i &lt; _global_klass_objects-&gt;length(); i++) {
      Klass* k = _global_klass_objects-&gt;at(i);
      if (k-&gt;is_instance_klass()) {
        InstanceKlass* ik = InstanceKlass::cast(k);
<span class="line-modified">!       for (int i = 0; i &lt; ik-&gt;methods()-&gt;length(); i++) {</span>
<span class="line-removed">-         Method* m = ik-&gt;methods()-&gt;at(i);</span>
<span class="line-removed">-         rewrite_nofast_bytecode(m);</span>
<span class="line-removed">-         Fingerprinter fp(m);</span>
<span class="line-removed">-         // The side effect of this call sets method&#39;s fingerprint field.</span>
<span class="line-removed">-         fp.fingerprint();</span>
<span class="line-removed">-       }</span>
      }
    }
  }
  
  // Objects of the Metadata types (such as Klass and ConstantPool) have C++ vtables.
  // (In GCC this is the field &lt;Type&gt;::_vptr, i.e., first word in the object.)
  //
  // Addresses of the vtables and the methods may be different across JVM runs,
  // if libjvm.so is dynamically loaded at a different base address.
<span class="line-new-header">--- 597,30 ---</span>
  // Walk all methods in the class list to ensure that they won&#39;t be modified at
  // run time. This includes:
  // [1] Rewrite all bytecodes as needed, so that the ConstMethod* will not be modified
  //     at run time by RewriteBytecodes/RewriteFrequentPairs
  // [2] Assign a fingerprint, so one doesn&#39;t need to be assigned at run-time.
<span class="line-modified">! static void rewrite_nofast_bytecodes_and_calculate_fingerprints(Thread* thread) {</span>
    for (int i = 0; i &lt; _global_klass_objects-&gt;length(); i++) {
      Klass* k = _global_klass_objects-&gt;at(i);
      if (k-&gt;is_instance_klass()) {
        InstanceKlass* ik = InstanceKlass::cast(k);
<span class="line-modified">!       MetaspaceShared::rewrite_nofast_bytecodes_and_calculate_fingerprints(thread, ik);</span>
      }
    }
  }
  
<span class="line-added">+ void MetaspaceShared::rewrite_nofast_bytecodes_and_calculate_fingerprints(Thread* thread, InstanceKlass* ik) {</span>
<span class="line-added">+   for (int i = 0; i &lt; ik-&gt;methods()-&gt;length(); i++) {</span>
<span class="line-added">+     methodHandle m(thread, ik-&gt;methods()-&gt;at(i));</span>
<span class="line-added">+     rewrite_nofast_bytecode(m);</span>
<span class="line-added">+     Fingerprinter fp(m);</span>
<span class="line-added">+     // The side effect of this call sets method&#39;s fingerprint field.</span>
<span class="line-added">+     fp.fingerprint();</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
  // Objects of the Metadata types (such as Klass and ConstantPool) have C++ vtables.
  // (In GCC this is the field &lt;Type&gt;::_vptr, i.e., first word in the object.)
  //
  // Addresses of the vtables and the methods may be different across JVM runs,
  // if libjvm.so is dynamically loaded at a different base address.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 658,35 ***</span>
    static void zero_vtable_clone() {
      assert(DumpSharedSpaces, &quot;dump-time only&quot;);
      _info-&gt;zero();
    }
  
<span class="line-removed">-   // Switch the vtable pointer to point to the cloned vtable.</span>
<span class="line-removed">-   static void patch(Metadata* obj) {</span>
<span class="line-removed">-     assert(DumpSharedSpaces, &quot;dump-time only&quot;);</span>
<span class="line-removed">-     *(void**)obj = (void*)(_info-&gt;cloned_vtable());</span>
<span class="line-removed">-   }</span>
<span class="line-removed">- </span>
    static bool is_valid_shared_object(const T* obj) {
      intptr_t* vptr = *(intptr_t**)obj;
      return vptr == _info-&gt;cloned_vtable();
    }
  };
  
  template &lt;class T&gt; CppVtableInfo* CppVtableCloner&lt;T&gt;::_info = NULL;
  
  template &lt;class T&gt;
  intptr_t* CppVtableCloner&lt;T&gt;::allocate(const char* name) {
<span class="line-modified">!   assert(is_aligned(_md_region.top(), sizeof(intptr_t)), &quot;bad alignment&quot;);</span>
    int n = get_vtable_length(name);
<span class="line-modified">!   _info = (CppVtableInfo*)_md_region.allocate(CppVtableInfo::byte_size(n), sizeof(intptr_t));</span>
    _info-&gt;set_vtable_size(n);
  
    intptr_t* p = clone_vtable(name, _info);
<span class="line-modified">!   assert((char*)p == _md_region.top(), &quot;must be&quot;);</span>
  
<span class="line-modified">!   return p;</span>
  }
  
  template &lt;class T&gt;
  intptr_t* CppVtableCloner&lt;T&gt;::clone_vtable(const char* name, CppVtableInfo* info) {
    if (!DumpSharedSpaces) {
<span class="line-new-header">--- 679,29 ---</span>
    static void zero_vtable_clone() {
      assert(DumpSharedSpaces, &quot;dump-time only&quot;);
      _info-&gt;zero();
    }
  
    static bool is_valid_shared_object(const T* obj) {
      intptr_t* vptr = *(intptr_t**)obj;
      return vptr == _info-&gt;cloned_vtable();
    }
  };
  
  template &lt;class T&gt; CppVtableInfo* CppVtableCloner&lt;T&gt;::_info = NULL;
  
  template &lt;class T&gt;
  intptr_t* CppVtableCloner&lt;T&gt;::allocate(const char* name) {
<span class="line-modified">!   assert(is_aligned(_mc_region.top(), sizeof(intptr_t)), &quot;bad alignment&quot;);</span>
    int n = get_vtable_length(name);
<span class="line-modified">!   _info = (CppVtableInfo*)_mc_region.allocate(CppVtableInfo::byte_size(n), sizeof(intptr_t));</span>
    _info-&gt;set_vtable_size(n);
  
    intptr_t* p = clone_vtable(name, _info);
<span class="line-modified">!   assert((char*)p == _mc_region.top(), &quot;must be&quot;);</span>
  
<span class="line-modified">!   return _info-&gt;cloned_vtable();</span>
  }
  
  template &lt;class T&gt;
  intptr_t* CppVtableCloner&lt;T&gt;::clone_vtable(const char* name, CppVtableInfo* info) {
    if (!DumpSharedSpaces) {
</pre>
<hr />
<pre>
<span class="line-old-header">*** 755,130 ***</span>
  
    return vtable_len;
  }
  
  #define ALLOC_CPP_VTABLE_CLONE(c) \
<span class="line-modified">!   CppVtableCloner&lt;c&gt;::allocate(#c);</span>
  
  #define CLONE_CPP_VTABLE(c) \
    p = CppVtableCloner&lt;c&gt;::clone_vtable(#c, (CppVtableInfo*)p);
  
  #define ZERO_CPP_VTABLE(c) \
   CppVtableCloner&lt;c&gt;::zero_vtable_clone();
  
<span class="line-modified">! // This can be called at both dump time and run time.</span>
<span class="line-modified">! intptr_t* MetaspaceShared::clone_cpp_vtables(intptr_t* p) {</span>
    assert(DumpSharedSpaces || UseSharedSpaces, &quot;sanity&quot;);
    CPP_VTABLE_PATCH_TYPES_DO(CLONE_CPP_VTABLE);
<span class="line-removed">-   return p;</span>
  }
  
  void MetaspaceShared::zero_cpp_vtable_clones_for_writing() {
    assert(DumpSharedSpaces, &quot;dump-time only&quot;);
    CPP_VTABLE_PATCH_TYPES_DO(ZERO_CPP_VTABLE);
  }
  
  // Allocate and initialize the C++ vtables, starting from top, but do not go past end.
<span class="line-modified">! void MetaspaceShared::allocate_cpp_vtable_clones() {</span>
    assert(DumpSharedSpaces, &quot;dump-time only&quot;);
    // Layout (each slot is a intptr_t):
    //   [number of slots in the first vtable = n1]
    //   [ &lt;n1&gt; slots for the first vtable]
    //   [number of slots in the first second = n2]
    //   [ &lt;n2&gt; slots for the second vtable]
    //   ...
    // The order of the vtables is the same as the CPP_VTAB_PATCH_TYPES_DO macro.
    CPP_VTABLE_PATCH_TYPES_DO(ALLOC_CPP_VTABLE_CLONE);
<span class="line-removed">- }</span>
  
<span class="line-modified">! // Switch the vtable pointer to point to the cloned vtable. We assume the</span>
<span class="line-removed">- // vtable pointer is in first slot in object.</span>
<span class="line-removed">- void MetaspaceShared::patch_cpp_vtable_pointers() {</span>
<span class="line-removed">-   int n = _global_klass_objects-&gt;length();</span>
<span class="line-removed">-   for (int i = 0; i &lt; n; i++) {</span>
<span class="line-removed">-     Klass* obj = _global_klass_objects-&gt;at(i);</span>
<span class="line-removed">-     if (obj-&gt;is_instance_klass()) {</span>
<span class="line-removed">-       InstanceKlass* ik = InstanceKlass::cast(obj);</span>
<span class="line-removed">-       if (ik-&gt;is_class_loader_instance_klass()) {</span>
<span class="line-removed">-         CppVtableCloner&lt;InstanceClassLoaderKlass&gt;::patch(ik);</span>
<span class="line-removed">-       } else if (ik-&gt;is_reference_instance_klass()) {</span>
<span class="line-removed">-         CppVtableCloner&lt;InstanceRefKlass&gt;::patch(ik);</span>
<span class="line-removed">-       } else if (ik-&gt;is_mirror_instance_klass()) {</span>
<span class="line-removed">-         CppVtableCloner&lt;InstanceMirrorKlass&gt;::patch(ik);</span>
<span class="line-removed">-       } else {</span>
<span class="line-removed">-         CppVtableCloner&lt;InstanceKlass&gt;::patch(ik);</span>
<span class="line-removed">-       }</span>
<span class="line-removed">-       ConstantPool* cp = ik-&gt;constants();</span>
<span class="line-removed">-       CppVtableCloner&lt;ConstantPool&gt;::patch(cp);</span>
<span class="line-removed">-       for (int j = 0; j &lt; ik-&gt;methods()-&gt;length(); j++) {</span>
<span class="line-removed">-         Method* m = ik-&gt;methods()-&gt;at(j);</span>
<span class="line-removed">-         CppVtableCloner&lt;Method&gt;::patch(m);</span>
<span class="line-removed">-         assert(CppVtableCloner&lt;Method&gt;::is_valid_shared_object(m), &quot;must be&quot;);</span>
<span class="line-removed">-       }</span>
<span class="line-removed">-     } else if (obj-&gt;is_objArray_klass()) {</span>
<span class="line-removed">-       CppVtableCloner&lt;ObjArrayKlass&gt;::patch(obj);</span>
<span class="line-removed">-     } else {</span>
<span class="line-removed">-       assert(obj-&gt;is_typeArray_klass(), &quot;sanity&quot;);</span>
<span class="line-removed">-       CppVtableCloner&lt;TypeArrayKlass&gt;::patch(obj);</span>
<span class="line-removed">-     }</span>
<span class="line-removed">-   }</span>
  }
  
  bool MetaspaceShared::is_valid_shared_method(const Method* m) {
    assert(is_in_shared_metaspace(m), &quot;must be&quot;);
    return CppVtableCloner&lt;Method&gt;::is_valid_shared_object(m);
  }
  
<span class="line-modified">! // Closure for serializing initialization data out to a data area to be</span>
<span class="line-modified">! // written to the shared file.</span>
<span class="line-modified">! </span>
<span class="line-modified">! class WriteClosure : public SerializeClosure {</span>
<span class="line-modified">! private:</span>
<span class="line-modified">!   DumpRegion* _dump_region;</span>
<span class="line-modified">! </span>
<span class="line-modified">! public:</span>
<span class="line-removed">-   WriteClosure(DumpRegion* r) {</span>
<span class="line-removed">-     _dump_region = r;</span>
<span class="line-removed">-   }</span>
<span class="line-removed">- </span>
<span class="line-removed">-   void do_ptr(void** p) {</span>
<span class="line-removed">-     _dump_region-&gt;append_intptr_t((intptr_t)*p);</span>
<span class="line-removed">-   }</span>
<span class="line-removed">- </span>
<span class="line-removed">-   void do_u4(u4* p) {</span>
<span class="line-removed">-     void* ptr = (void*)(uintx(*p));</span>
<span class="line-removed">-     do_ptr(&amp;ptr);</span>
<span class="line-removed">-   }</span>
<span class="line-removed">- </span>
<span class="line-removed">-   void do_tag(int tag) {</span>
<span class="line-removed">-     _dump_region-&gt;append_intptr_t((intptr_t)tag);</span>
<span class="line-removed">-   }</span>
<span class="line-removed">- </span>
<span class="line-removed">-   void do_oop(oop* o) {</span>
<span class="line-removed">-     if (*o == NULL) {</span>
<span class="line-removed">-       _dump_region-&gt;append_intptr_t(0);</span>
<span class="line-removed">-     } else {</span>
<span class="line-removed">-       assert(HeapShared::is_heap_object_archiving_allowed(),</span>
<span class="line-removed">-              &quot;Archiving heap object is not allowed&quot;);</span>
<span class="line-removed">-       _dump_region-&gt;append_intptr_t(</span>
<span class="line-removed">-         (intptr_t)CompressedOops::encode_not_null(*o));</span>
<span class="line-removed">-     }</span>
    }
  
<span class="line-modified">!   void do_region(u_char* start, size_t size) {</span>
<span class="line-modified">!     assert((intptr_t)start % sizeof(intptr_t) == 0, &quot;bad alignment&quot;);</span>
<span class="line-modified">!     assert(size % sizeof(intptr_t) == 0, &quot;bad size&quot;);</span>
<span class="line-modified">!     do_tag((int)size);</span>
<span class="line-modified">!     while (size &gt; 0) {</span>
<span class="line-modified">!       _dump_region-&gt;append_intptr_t(*(intptr_t*)start);</span>
<span class="line-modified">!       start += sizeof(intptr_t);</span>
<span class="line-modified">!       size -= sizeof(intptr_t);</span>
<span class="line-removed">-     }</span>
    }
<span class="line-modified">! </span>
<span class="line-removed">-   bool reading() const { return false; }</span>
<span class="line-removed">- };</span>
  
  // This is for dumping detailed statistics for the allocations
  // in the shared spaces.
  class DumpAllocStats : public ResourceObj {
  public:
<span class="line-new-header">--- 770,178 ---</span>
  
    return vtable_len;
  }
  
  #define ALLOC_CPP_VTABLE_CLONE(c) \
<span class="line-modified">!   _cloned_cpp_vtptrs[c##_Kind] = CppVtableCloner&lt;c&gt;::allocate(#c); \</span>
<span class="line-added">+   ArchivePtrMarker::mark_pointer(&amp;_cloned_cpp_vtptrs[c##_Kind]);</span>
  
  #define CLONE_CPP_VTABLE(c) \
    p = CppVtableCloner&lt;c&gt;::clone_vtable(#c, (CppVtableInfo*)p);
  
  #define ZERO_CPP_VTABLE(c) \
   CppVtableCloner&lt;c&gt;::zero_vtable_clone();
  
<span class="line-modified">! //------------------------------ for DynamicDumpSharedSpaces - start</span>
<span class="line-modified">! #define DECLARE_CLONED_VTABLE_KIND(c) c ## _Kind,</span>
<span class="line-added">+ </span>
<span class="line-added">+ enum {</span>
<span class="line-added">+   // E.g., ConstantPool_Kind == 0, InstanceKlass == 1, etc.</span>
<span class="line-added">+   CPP_VTABLE_PATCH_TYPES_DO(DECLARE_CLONED_VTABLE_KIND)</span>
<span class="line-added">+   _num_cloned_vtable_kinds</span>
<span class="line-added">+ };</span>
<span class="line-added">+ </span>
<span class="line-added">+ // This is the index of all the cloned vtables. E.g., for</span>
<span class="line-added">+ //     ConstantPool* cp = ....; // an archived constant pool</span>
<span class="line-added">+ //     InstanceKlass* ik = ....;// an archived class</span>
<span class="line-added">+ // the following holds true:</span>
<span class="line-added">+ //     _cloned_cpp_vtptrs[ConstantPool_Kind]  == ((intptr_t**)cp)[0]</span>
<span class="line-added">+ //     _cloned_cpp_vtptrs[InstanceKlass_Kind] == ((intptr_t**)ik)[0]</span>
<span class="line-added">+ static intptr_t** _cloned_cpp_vtptrs = NULL;</span>
<span class="line-added">+ </span>
<span class="line-added">+ void MetaspaceShared::allocate_cloned_cpp_vtptrs() {</span>
<span class="line-added">+   assert(DumpSharedSpaces, &quot;must&quot;);</span>
<span class="line-added">+   size_t vtptrs_bytes = _num_cloned_vtable_kinds * sizeof(intptr_t*);</span>
<span class="line-added">+   _cloned_cpp_vtptrs = (intptr_t**)_mc_region.allocate(vtptrs_bytes, sizeof(intptr_t*));</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ void MetaspaceShared::serialize_cloned_cpp_vtptrs(SerializeClosure* soc) {</span>
<span class="line-added">+   soc-&gt;do_ptr((void**)&amp;_cloned_cpp_vtptrs);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ intptr_t* MetaspaceShared::fix_cpp_vtable_for_dynamic_archive(MetaspaceObj::Type msotype, address obj) {</span>
<span class="line-added">+   Arguments::assert_is_dumping_archive();</span>
<span class="line-added">+   int kind = -1;</span>
<span class="line-added">+   switch (msotype) {</span>
<span class="line-added">+   case MetaspaceObj::SymbolType:</span>
<span class="line-added">+   case MetaspaceObj::TypeArrayU1Type:</span>
<span class="line-added">+   case MetaspaceObj::TypeArrayU2Type:</span>
<span class="line-added">+   case MetaspaceObj::TypeArrayU4Type:</span>
<span class="line-added">+   case MetaspaceObj::TypeArrayU8Type:</span>
<span class="line-added">+   case MetaspaceObj::TypeArrayOtherType:</span>
<span class="line-added">+   case MetaspaceObj::ConstMethodType:</span>
<span class="line-added">+   case MetaspaceObj::ConstantPoolCacheType:</span>
<span class="line-added">+   case MetaspaceObj::AnnotationsType:</span>
<span class="line-added">+   case MetaspaceObj::MethodCountersType:</span>
<span class="line-added">+   case MetaspaceObj::RecordComponentType:</span>
<span class="line-added">+     // These have no vtables.</span>
<span class="line-added">+     break;</span>
<span class="line-added">+   case MetaspaceObj::ClassType:</span>
<span class="line-added">+     {</span>
<span class="line-added">+       Klass* k = (Klass*)obj;</span>
<span class="line-added">+       assert(k-&gt;is_klass(), &quot;must be&quot;);</span>
<span class="line-added">+       if (k-&gt;is_instance_klass()) {</span>
<span class="line-added">+         InstanceKlass* ik = InstanceKlass::cast(k);</span>
<span class="line-added">+         if (ik-&gt;is_class_loader_instance_klass()) {</span>
<span class="line-added">+           kind = InstanceClassLoaderKlass_Kind;</span>
<span class="line-added">+         } else if (ik-&gt;is_reference_instance_klass()) {</span>
<span class="line-added">+           kind = InstanceRefKlass_Kind;</span>
<span class="line-added">+         } else if (ik-&gt;is_mirror_instance_klass()) {</span>
<span class="line-added">+           kind = InstanceMirrorKlass_Kind;</span>
<span class="line-added">+         } else {</span>
<span class="line-added">+           kind = InstanceKlass_Kind;</span>
<span class="line-added">+         }</span>
<span class="line-added">+       } else if (k-&gt;is_typeArray_klass()) {</span>
<span class="line-added">+         kind = TypeArrayKlass_Kind;</span>
<span class="line-added">+       } else {</span>
<span class="line-added">+         assert(k-&gt;is_objArray_klass(), &quot;must be&quot;);</span>
<span class="line-added">+         kind = ObjArrayKlass_Kind;</span>
<span class="line-added">+       }</span>
<span class="line-added">+     }</span>
<span class="line-added">+     break;</span>
<span class="line-added">+ </span>
<span class="line-added">+   case MetaspaceObj::MethodType:</span>
<span class="line-added">+     {</span>
<span class="line-added">+       Method* m = (Method*)obj;</span>
<span class="line-added">+       assert(m-&gt;is_method(), &quot;must be&quot;);</span>
<span class="line-added">+       kind = Method_Kind;</span>
<span class="line-added">+     }</span>
<span class="line-added">+     break;</span>
<span class="line-added">+ </span>
<span class="line-added">+   case MetaspaceObj::MethodDataType:</span>
<span class="line-added">+     // We don&#39;t archive MethodData &lt;-- should have been removed in removed_unsharable_info</span>
<span class="line-added">+     ShouldNotReachHere();</span>
<span class="line-added">+     break;</span>
<span class="line-added">+ </span>
<span class="line-added">+   case MetaspaceObj::ConstantPoolType:</span>
<span class="line-added">+     {</span>
<span class="line-added">+       ConstantPool *cp = (ConstantPool*)obj;</span>
<span class="line-added">+       assert(cp-&gt;is_constantPool(), &quot;must be&quot;);</span>
<span class="line-added">+       kind = ConstantPool_Kind;</span>
<span class="line-added">+     }</span>
<span class="line-added">+     break;</span>
<span class="line-added">+ </span>
<span class="line-added">+   default:</span>
<span class="line-added">+     ShouldNotReachHere();</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   if (kind &gt;= 0) {</span>
<span class="line-added">+     assert(kind &lt; _num_cloned_vtable_kinds, &quot;must be&quot;);</span>
<span class="line-added">+     return _cloned_cpp_vtptrs[kind];</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     return NULL;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ //------------------------------ for DynamicDumpSharedSpaces - end</span>
<span class="line-added">+ </span>
<span class="line-added">+ // This can be called at both dump time and run time:</span>
<span class="line-added">+ // - clone the contents of the c++ vtables into the space</span>
<span class="line-added">+ //   allocated by allocate_cpp_vtable_clones()</span>
<span class="line-added">+ void MetaspaceShared::clone_cpp_vtables(intptr_t* p) {</span>
    assert(DumpSharedSpaces || UseSharedSpaces, &quot;sanity&quot;);
    CPP_VTABLE_PATCH_TYPES_DO(CLONE_CPP_VTABLE);
  }
  
  void MetaspaceShared::zero_cpp_vtable_clones_for_writing() {
    assert(DumpSharedSpaces, &quot;dump-time only&quot;);
    CPP_VTABLE_PATCH_TYPES_DO(ZERO_CPP_VTABLE);
  }
  
  // Allocate and initialize the C++ vtables, starting from top, but do not go past end.
<span class="line-modified">! char* MetaspaceShared::allocate_cpp_vtable_clones() {</span>
<span class="line-added">+   char* cloned_vtables = _mc_region.top(); // This is the beginning of all the cloned vtables</span>
<span class="line-added">+ </span>
    assert(DumpSharedSpaces, &quot;dump-time only&quot;);
    // Layout (each slot is a intptr_t):
    //   [number of slots in the first vtable = n1]
    //   [ &lt;n1&gt; slots for the first vtable]
    //   [number of slots in the first second = n2]
    //   [ &lt;n2&gt; slots for the second vtable]
    //   ...
    // The order of the vtables is the same as the CPP_VTAB_PATCH_TYPES_DO macro.
    CPP_VTABLE_PATCH_TYPES_DO(ALLOC_CPP_VTABLE_CLONE);
  
<span class="line-modified">!   return cloned_vtables;</span>
  }
  
  bool MetaspaceShared::is_valid_shared_method(const Method* m) {
    assert(is_in_shared_metaspace(m), &quot;must be&quot;);
    return CppVtableCloner&lt;Method&gt;::is_valid_shared_object(m);
  }
  
<span class="line-modified">! void WriteClosure::do_oop(oop* o) {</span>
<span class="line-modified">!   if (*o == NULL) {</span>
<span class="line-modified">!     _dump_region-&gt;append_intptr_t(0);</span>
<span class="line-modified">!   } else {</span>
<span class="line-modified">!     assert(HeapShared::is_heap_object_archiving_allowed(),</span>
<span class="line-modified">!            &quot;Archiving heap object is not allowed&quot;);</span>
<span class="line-modified">!     _dump_region-&gt;append_intptr_t(</span>
<span class="line-modified">!       (intptr_t)CompressedOops::encode_not_null(*o));</span>
    }
<span class="line-added">+ }</span>
  
<span class="line-modified">! void WriteClosure::do_region(u_char* start, size_t size) {</span>
<span class="line-modified">!   assert((intptr_t)start % sizeof(intptr_t) == 0, &quot;bad alignment&quot;);</span>
<span class="line-modified">!   assert(size % sizeof(intptr_t) == 0, &quot;bad size&quot;);</span>
<span class="line-modified">!   do_tag((int)size);</span>
<span class="line-modified">!   while (size &gt; 0) {</span>
<span class="line-modified">!     _dump_region-&gt;append_intptr_t(*(intptr_t*)start, true);</span>
<span class="line-modified">!     start += sizeof(intptr_t);</span>
<span class="line-modified">!     size -= sizeof(intptr_t);</span>
    }
<span class="line-modified">! }</span>
  
  // This is for dumping detailed statistics for the allocations
  // in the shared spaces.
  class DumpAllocStats : public ResourceObj {
  public:
</pre>
<hr />
<pre>
<span class="line-old-header">*** 927,14 ***</span>
  
    void record_other_type(int byte_size, bool read_only) {
      int which = (read_only) ? RO : RW;
      _bytes [which][OtherType] += byte_size;
    }
<span class="line-modified">!   void print_stats(int ro_all, int rw_all, int mc_all, int md_all);</span>
  };
  
<span class="line-modified">! void DumpAllocStats::print_stats(int ro_all, int rw_all, int mc_all, int md_all) {</span>
    // Calculate size of data that was not allocated by Metaspace::allocate()
    MetaspaceSharedStats *stats = MetaspaceShared::stats();
  
    // symbols
    _counts[RO][SymbolHashentryType] = stats-&gt;symbol.hashentry_count;
<span class="line-new-header">--- 990,14 ---</span>
  
    void record_other_type(int byte_size, bool read_only) {
      int which = (read_only) ? RO : RW;
      _bytes [which][OtherType] += byte_size;
    }
<span class="line-modified">!   void print_stats(int ro_all, int rw_all, int mc_all);</span>
  };
  
<span class="line-modified">! void DumpAllocStats::print_stats(int ro_all, int rw_all, int mc_all) {</span>
    // Calculate size of data that was not allocated by Metaspace::allocate()
    MetaspaceSharedStats *stats = MetaspaceShared::stats();
  
    // symbols
    _counts[RO][SymbolHashentryType] = stats-&gt;symbol.hashentry_count;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 949,12 ***</span>
  
    _counts[RO][StringBucketType] = stats-&gt;string.bucket_count;
    _bytes [RO][StringBucketType] = stats-&gt;string.bucket_bytes;
  
    // TODO: count things like dictionary, vtable, etc
<span class="line-modified">!   _bytes[RW][OtherType] += mc_all + md_all;</span>
<span class="line-modified">!   rw_all += mc_all + md_all; // mc/md are mapped Read/Write</span>
  
    // prevent divide-by-zero
    if (ro_all &lt; 1) {
      ro_all = 1;
    }
<span class="line-new-header">--- 1012,12 ---</span>
  
    _counts[RO][StringBucketType] = stats-&gt;string.bucket_count;
    _bytes [RO][StringBucketType] = stats-&gt;string.bucket_bytes;
  
    // TODO: count things like dictionary, vtable, etc
<span class="line-modified">!   _bytes[RW][OtherType] += mc_all;</span>
<span class="line-modified">!   rw_all += mc_all; // mc is mapped Read/Write</span>
  
    // prevent divide-by-zero
    if (ro_all &lt; 1) {
      ro_all = 1;
    }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 972,13 ***</span>
    const char *sep = &quot;--------------------+---------------------------+---------------------------+--------------------------&quot;;
    const char *hdr = &quot;                        ro_cnt   ro_bytes     % |   rw_cnt   rw_bytes     % |  all_cnt  all_bytes     %&quot;;
  
    LogMessage(cds) msg;
  
<span class="line-modified">!   msg.info(&quot;Detailed metadata info (excluding od/st regions; rw stats include md/mc regions):&quot;);</span>
<span class="line-modified">!   msg.info(&quot;%s&quot;, hdr);</span>
<span class="line-modified">!   msg.info(&quot;%s&quot;, sep);</span>
    for (int type = 0; type &lt; int(_number_of_types); type ++) {
      const char *name = type_name((Type)type);
      int ro_count = _counts[RO][type];
      int ro_bytes = _bytes [RO][type];
      int rw_count = _counts[RW][type];
<span class="line-new-header">--- 1035,13 ---</span>
    const char *sep = &quot;--------------------+---------------------------+---------------------------+--------------------------&quot;;
    const char *hdr = &quot;                        ro_cnt   ro_bytes     % |   rw_cnt   rw_bytes     % |  all_cnt  all_bytes     %&quot;;
  
    LogMessage(cds) msg;
  
<span class="line-modified">!   msg.debug(&quot;Detailed metadata info (excluding st regions; rw stats include mc regions):&quot;);</span>
<span class="line-modified">!   msg.debug(&quot;%s&quot;, hdr);</span>
<span class="line-modified">!   msg.debug(&quot;%s&quot;, sep);</span>
    for (int type = 0; type &lt; int(_number_of_types); type ++) {
      const char *name = type_name((Type)type);
      int ro_count = _counts[RO][type];
      int ro_bytes = _bytes [RO][type];
      int rw_count = _counts[RW][type];
</pre>
<hr />
<pre>
<span class="line-old-header">*** 988,11 ***</span>
  
      double ro_perc = percent_of(ro_bytes, ro_all);
      double rw_perc = percent_of(rw_bytes, rw_all);
      double perc    = percent_of(bytes, ro_all + rw_all);
  
<span class="line-modified">!     msg.info(fmt_stats, name,</span>
                           ro_count, ro_bytes, ro_perc,
                           rw_count, rw_bytes, rw_perc,
                           count, bytes, perc);
  
      all_ro_count += ro_count;
<span class="line-new-header">--- 1051,11 ---</span>
  
      double ro_perc = percent_of(ro_bytes, ro_all);
      double rw_perc = percent_of(rw_bytes, rw_all);
      double perc    = percent_of(bytes, ro_all + rw_all);
  
<span class="line-modified">!     msg.debug(fmt_stats, name,</span>
                           ro_count, ro_bytes, ro_perc,
                           rw_count, rw_bytes, rw_perc,
                           count, bytes, perc);
  
      all_ro_count += ro_count;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1006,12 ***</span>
  
    double all_ro_perc = percent_of(all_ro_bytes, ro_all);
    double all_rw_perc = percent_of(all_rw_bytes, rw_all);
    double all_perc    = percent_of(all_bytes, ro_all + rw_all);
  
<span class="line-modified">!   msg.info(&quot;%s&quot;, sep);</span>
<span class="line-modified">!   msg.info(fmt_stats, &quot;Total&quot;,</span>
                         all_ro_count, all_ro_bytes, all_ro_perc,
                         all_rw_count, all_rw_bytes, all_rw_perc,
                         all_count, all_bytes, all_perc);
  
    assert(all_ro_bytes == ro_all, &quot;everything should have been counted&quot;);
<span class="line-new-header">--- 1069,12 ---</span>
  
    double all_ro_perc = percent_of(all_ro_bytes, ro_all);
    double all_rw_perc = percent_of(all_rw_bytes, rw_all);
    double all_perc    = percent_of(all_bytes, ro_all + rw_all);
  
<span class="line-modified">!   msg.debug(&quot;%s&quot;, sep);</span>
<span class="line-modified">!   msg.debug(fmt_stats, &quot;Total&quot;,</span>
                         all_ro_count, all_ro_bytes, all_ro_perc,
                         all_rw_count, all_rw_bytes, all_rw_perc,
                         all_count, all_bytes, all_perc);
  
    assert(all_ro_bytes == ro_all, &quot;everything should have been counted&quot;);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1034,18 ***</span>
    void dump_archive_heap_oopmaps() NOT_CDS_JAVA_HEAP_RETURN;
    void dump_archive_heap_oopmaps(GrowableArray&lt;MemRegion&gt;* regions,
                                   GrowableArray&lt;ArchiveHeapOopmapInfo&gt;* oopmaps);
    void dump_symbols();
    char* dump_read_only_tables();
    void print_region_stats();
    void print_heap_region_stats(GrowableArray&lt;MemRegion&gt; *heap_mem,
<span class="line-modified">!                                const char *name, const size_t total_size);</span>
  public:
  
    VMOp_Type type() const { return VMOp_PopulateDumpSharedSpace; }
    void doit();   // outline because gdb sucks
<span class="line-removed">-   static void write_region(FileMapInfo* mapinfo, int region, DumpRegion* space, bool read_only,  bool allow_exec);</span>
    bool allow_nested_vm_operations() const { return true; }
  }; // class VM_PopulateDumpSharedSpace
  
  class SortedSymbolClosure: public SymbolClosure {
    GrowableArray&lt;Symbol*&gt; _symbols;
<span class="line-new-header">--- 1097,21 ---</span>
    void dump_archive_heap_oopmaps() NOT_CDS_JAVA_HEAP_RETURN;
    void dump_archive_heap_oopmaps(GrowableArray&lt;MemRegion&gt;* regions,
                                   GrowableArray&lt;ArchiveHeapOopmapInfo&gt;* oopmaps);
    void dump_symbols();
    char* dump_read_only_tables();
<span class="line-added">+   void print_class_stats();</span>
    void print_region_stats();
<span class="line-added">+   void print_bitmap_region_stats(size_t size, size_t total_size);</span>
    void print_heap_region_stats(GrowableArray&lt;MemRegion&gt; *heap_mem,
<span class="line-modified">!                                const char *name, size_t total_size);</span>
<span class="line-added">+   void relocate_to_default_base_address(CHeapBitMap* ptrmap);</span>
<span class="line-added">+ </span>
  public:
  
    VMOp_Type type() const { return VMOp_PopulateDumpSharedSpace; }
    void doit();   // outline because gdb sucks
    bool allow_nested_vm_operations() const { return true; }
  }; // class VM_PopulateDumpSharedSpace
  
  class SortedSymbolClosure: public SymbolClosure {
    GrowableArray&lt;Symbol*&gt; _symbols;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1140,10 ***</span>
<span class="line-new-header">--- 1206,17 ---</span>
        }
        p = _rw_region.allocate(bytes, alignment);
        newtop = _rw_region.top();
      }
      memcpy(p, obj, bytes);
<span class="line-added">+ </span>
<span class="line-added">+     intptr_t* cloned_vtable = MetaspaceShared::fix_cpp_vtable_for_dynamic_archive(ref-&gt;msotype(), (address)p);</span>
<span class="line-added">+     if (cloned_vtable != NULL) {</span>
<span class="line-added">+       *(address*)p = (address)cloned_vtable;</span>
<span class="line-added">+       ArchivePtrMarker::mark_pointer((address*)p);</span>
<span class="line-added">+     }</span>
<span class="line-added">+ </span>
      assert(_new_loc_table-&gt;lookup(obj) == NULL, &quot;each object can be relocated at most once&quot;);
      _new_loc_table-&gt;add(obj, (address)p);
      log_trace(cds)(&quot;Copy: &quot; PTR_FORMAT &quot; ==&gt; &quot; PTR_FORMAT &quot; %d&quot;, p2i(obj), p2i(p), bytes);
      if (_new_loc_table-&gt;maybe_grow(MAX_TABLE_SIZE)) {
        log_info(cds, hashtables)(&quot;Expanded _new_loc_table to %d&quot;, _new_loc_table-&gt;table_size());
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1162,33 ***</span>
    class ShallowCopier: public UniqueMetaspaceClosure {
      bool _read_only;
    public:
      ShallowCopier(bool read_only) : _read_only(read_only) {}
  
<span class="line-modified">!     virtual void do_unique_ref(Ref* ref, bool read_only) {</span>
        if (read_only == _read_only) {
          allocate(ref, read_only);
        }
      }
    };
  
    // Relocate embedded pointers within a MetaspaceObj&#39;s shallow copy
    class ShallowCopyEmbeddedRefRelocator: public UniqueMetaspaceClosure {
    public:
<span class="line-modified">!     virtual void do_unique_ref(Ref* ref, bool read_only) {</span>
        address new_loc = get_new_loc(ref);
        RefRelocator refer;
        ref-&gt;metaspace_pointers_do_at(&amp;refer, new_loc);
      }
    };
  
    // Relocate a reference to point to its shallow copy
    class RefRelocator: public MetaspaceClosure {
    public:
      virtual bool do_ref(Ref* ref, bool read_only) {
        if (ref-&gt;not_null()) {
          ref-&gt;update(get_new_loc(ref));
        }
        return false; // Do not recurse.
      }
    };
  
<span class="line-new-header">--- 1235,45 ---</span>
    class ShallowCopier: public UniqueMetaspaceClosure {
      bool _read_only;
    public:
      ShallowCopier(bool read_only) : _read_only(read_only) {}
  
<span class="line-modified">!     virtual bool do_unique_ref(Ref* ref, bool read_only) {</span>
        if (read_only == _read_only) {
          allocate(ref, read_only);
        }
<span class="line-added">+       return true; // recurse into ref.obj()</span>
      }
    };
  
    // Relocate embedded pointers within a MetaspaceObj&#39;s shallow copy
    class ShallowCopyEmbeddedRefRelocator: public UniqueMetaspaceClosure {
    public:
<span class="line-modified">!     virtual bool do_unique_ref(Ref* ref, bool read_only) {</span>
        address new_loc = get_new_loc(ref);
        RefRelocator refer;
        ref-&gt;metaspace_pointers_do_at(&amp;refer, new_loc);
<span class="line-added">+       return true; // recurse into ref.obj()</span>
<span class="line-added">+     }</span>
<span class="line-added">+     virtual void push_special(SpecialRef type, Ref* ref, intptr_t* p) {</span>
<span class="line-added">+       assert(type == _method_entry_ref, &quot;only special type allowed for now&quot;);</span>
<span class="line-added">+       address obj = ref-&gt;obj();</span>
<span class="line-added">+       address new_obj = get_new_loc(ref);</span>
<span class="line-added">+       size_t offset = pointer_delta(p, obj,  sizeof(u1));</span>
<span class="line-added">+       intptr_t* new_p = (intptr_t*)(new_obj + offset);</span>
<span class="line-added">+       assert(*p == *new_p, &quot;must be a copy&quot;);</span>
<span class="line-added">+       ArchivePtrMarker::mark_pointer((address*)new_p);</span>
      }
    };
  
    // Relocate a reference to point to its shallow copy
    class RefRelocator: public MetaspaceClosure {
    public:
      virtual bool do_ref(Ref* ref, bool read_only) {
        if (ref-&gt;not_null()) {
          ref-&gt;update(get_new_loc(ref));
<span class="line-added">+         ArchivePtrMarker::mark_pointer(ref-&gt;addr());</span>
        }
        return false; // Do not recurse.
      }
    };
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1210,45 ***</span>
    static void copy_and_compact() {
      ResourceMark rm;
      SortedSymbolClosure the_ssc; // StackObj
      _ssc = &amp;the_ssc;
  
<span class="line-modified">!     tty-&gt;print_cr(&quot;Scanning all metaspace objects ... &quot;);</span>
      {
        // allocate and shallow-copy RW objects, immediately following the MC region
<span class="line-modified">!       tty-&gt;print_cr(&quot;Allocating RW objects ... &quot;);</span>
        _mc_region.pack(&amp;_rw_region);
  
        ResourceMark rm;
        ShallowCopier rw_copier(false);
        iterate_roots(&amp;rw_copier);
      }
      {
        // allocate and shallow-copy of RO object, immediately following the RW region
<span class="line-modified">!       tty-&gt;print_cr(&quot;Allocating RO objects ... &quot;);</span>
        _rw_region.pack(&amp;_ro_region);
  
        ResourceMark rm;
        ShallowCopier ro_copier(true);
        iterate_roots(&amp;ro_copier);
      }
      {
<span class="line-modified">!       tty-&gt;print_cr(&quot;Relocating embedded pointers ... &quot;);</span>
        ResourceMark rm;
        ShallowCopyEmbeddedRefRelocator emb_reloc;
        iterate_roots(&amp;emb_reloc);
      }
      {
<span class="line-modified">!       tty-&gt;print_cr(&quot;Relocating external roots ... &quot;);</span>
        ResourceMark rm;
        RefRelocator ext_reloc;
        iterate_roots(&amp;ext_reloc);
      }
  
  #ifdef ASSERT
      {
<span class="line-modified">!       tty-&gt;print_cr(&quot;Verifying external roots ... &quot;);</span>
        ResourceMark rm;
        IsRefInArchiveChecker checker;
        iterate_roots(&amp;checker);
      }
  #endif
<span class="line-new-header">--- 1295,45 ---</span>
    static void copy_and_compact() {
      ResourceMark rm;
      SortedSymbolClosure the_ssc; // StackObj
      _ssc = &amp;the_ssc;
  
<span class="line-modified">!     log_info(cds)(&quot;Scanning all metaspace objects ... &quot;);</span>
      {
        // allocate and shallow-copy RW objects, immediately following the MC region
<span class="line-modified">!       log_info(cds)(&quot;Allocating RW objects ... &quot;);</span>
        _mc_region.pack(&amp;_rw_region);
  
        ResourceMark rm;
        ShallowCopier rw_copier(false);
        iterate_roots(&amp;rw_copier);
      }
      {
        // allocate and shallow-copy of RO object, immediately following the RW region
<span class="line-modified">!       log_info(cds)(&quot;Allocating RO objects ... &quot;);</span>
        _rw_region.pack(&amp;_ro_region);
  
        ResourceMark rm;
        ShallowCopier ro_copier(true);
        iterate_roots(&amp;ro_copier);
      }
      {
<span class="line-modified">!       log_info(cds)(&quot;Relocating embedded pointers ... &quot;);</span>
        ResourceMark rm;
        ShallowCopyEmbeddedRefRelocator emb_reloc;
        iterate_roots(&amp;emb_reloc);
      }
      {
<span class="line-modified">!       log_info(cds)(&quot;Relocating external roots ... &quot;);</span>
        ResourceMark rm;
        RefRelocator ext_reloc;
        iterate_roots(&amp;ext_reloc);
      }
  
  #ifdef ASSERT
      {
<span class="line-modified">!       log_info(cds)(&quot;Verifying external roots ... &quot;);</span>
        ResourceMark rm;
        IsRefInArchiveChecker checker;
        iterate_roots(&amp;checker);
      }
  #endif
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1261,11 ***</span>
    // We must relocate the System::_well_known_klasses only after we have copied the
    // java objects in during dump_java_heap_objects(): during the object copy, we operate on
    // old objects which assert that their klass is the original klass.
    static void relocate_well_known_klasses() {
      {
<span class="line-modified">!       tty-&gt;print_cr(&quot;Relocating SystemDictionary::_well_known_klasses[] ... &quot;);</span>
        ResourceMark rm;
        RefRelocator ext_reloc;
        SystemDictionary::well_known_klasses_do(&amp;ext_reloc);
      }
      // NOTE: after this point, we shouldn&#39;t have any globals that can reach the old
<span class="line-new-header">--- 1346,11 ---</span>
    // We must relocate the System::_well_known_klasses only after we have copied the
    // java objects in during dump_java_heap_objects(): during the object copy, we operate on
    // old objects which assert that their klass is the original klass.
    static void relocate_well_known_klasses() {
      {
<span class="line-modified">!       log_info(cds)(&quot;Relocating SystemDictionary::_well_known_klasses[] ... &quot;);</span>
        ResourceMark rm;
        RefRelocator ext_reloc;
        SystemDictionary::well_known_klasses_do(&amp;ext_reloc);
      }
      // NOTE: after this point, we shouldn&#39;t have any globals that can reach the old
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1290,10 ***</span>
<span class="line-new-header">--- 1375,12 ---</span>
      FileMapInfo::metaspace_pointers_do(it);
      SystemDictionaryShared::dumptime_classes_do(it);
      Universe::metaspace_pointers_do(it);
      SymbolTable::metaspace_pointers_do(it);
      vmSymbols::metaspace_pointers_do(it);
<span class="line-added">+ </span>
<span class="line-added">+     it-&gt;finish();</span>
    }
  
    static Klass* get_relocated_klass(Klass* orig_klass) {
      assert(DumpSharedSpaces, &quot;dump time only&quot;);
      address* pp = _new_loc_table-&gt;lookup((address)orig_klass);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1306,54 ***</span>
  
  DumpAllocStats* ArchiveCompactor::_alloc_stats;
  SortedSymbolClosure* ArchiveCompactor::_ssc;
  ArchiveCompactor::RelocationTable* ArchiveCompactor::_new_loc_table;
  
<span class="line-removed">- void VM_PopulateDumpSharedSpace::write_region(FileMapInfo* mapinfo, int region_idx,</span>
<span class="line-removed">-                                               DumpRegion* dump_region, bool read_only,  bool allow_exec) {</span>
<span class="line-removed">-   mapinfo-&gt;write_region(region_idx, dump_region-&gt;base(), dump_region-&gt;used(), read_only, allow_exec);</span>
<span class="line-removed">- }</span>
<span class="line-removed">- </span>
  void VM_PopulateDumpSharedSpace::dump_symbols() {
<span class="line-modified">!   tty-&gt;print_cr(&quot;Dumping symbol table ...&quot;);</span>
  
    NOT_PRODUCT(SymbolTable::verify());
    SymbolTable::write_to_archive();
  }
  
  char* VM_PopulateDumpSharedSpace::dump_read_only_tables() {
    ArchiveCompactor::OtherROAllocMark mark;
  
<span class="line-modified">!   tty-&gt;print(&quot;Removing java_mirror ... &quot;);</span>
    if (!HeapShared::is_heap_object_archiving_allowed()) {
      clear_basic_type_mirrors();
    }
    remove_java_mirror_in_classes();
<span class="line-modified">!   tty-&gt;print_cr(&quot;done. &quot;);</span>
  
    SystemDictionaryShared::write_to_archive();
  
<span class="line-removed">-   char* start = _ro_region.top();</span>
<span class="line-removed">- </span>
    // Write the other data to the output array.
    WriteClosure wc(&amp;_ro_region);
    MetaspaceShared::serialize(&amp;wc);
  
    // Write the bitmaps for patching the archive heap regions
    dump_archive_heap_oopmaps();
  
    return start;
  }
  
  void VM_PopulateDumpSharedSpace::doit() {
    // We should no longer allocate anything from the metaspace, so that:
    //
    // (1) Metaspace::allocate might trigger GC if we have run out of
    //     committed metaspace, but we can&#39;t GC because we&#39;re running
    //     in the VM thread.
    // (2) ArchiveCompactor needs to work with a stable set of MetaspaceObjs.
    Metaspace::freeze();
  
    Thread* THREAD = VMThread::vm_thread();
  
    FileMapInfo::check_nonempty_dir_in_shared_path_table();
  
<span class="line-new-header">--- 1393,113 ---</span>
  
  DumpAllocStats* ArchiveCompactor::_alloc_stats;
  SortedSymbolClosure* ArchiveCompactor::_ssc;
  ArchiveCompactor::RelocationTable* ArchiveCompactor::_new_loc_table;
  
  void VM_PopulateDumpSharedSpace::dump_symbols() {
<span class="line-modified">!   log_info(cds)(&quot;Dumping symbol table ...&quot;);</span>
  
    NOT_PRODUCT(SymbolTable::verify());
    SymbolTable::write_to_archive();
  }
  
  char* VM_PopulateDumpSharedSpace::dump_read_only_tables() {
    ArchiveCompactor::OtherROAllocMark mark;
  
<span class="line-modified">!   log_info(cds)(&quot;Removing java_mirror ... &quot;);</span>
    if (!HeapShared::is_heap_object_archiving_allowed()) {
      clear_basic_type_mirrors();
    }
    remove_java_mirror_in_classes();
<span class="line-modified">!   log_info(cds)(&quot;done. &quot;);</span>
  
    SystemDictionaryShared::write_to_archive();
  
    // Write the other data to the output array.
<span class="line-added">+   char* start = _ro_region.top();</span>
    WriteClosure wc(&amp;_ro_region);
    MetaspaceShared::serialize(&amp;wc);
  
    // Write the bitmaps for patching the archive heap regions
    dump_archive_heap_oopmaps();
  
    return start;
  }
  
<span class="line-added">+ void VM_PopulateDumpSharedSpace::print_class_stats() {</span>
<span class="line-added">+   log_info(cds)(&quot;Number of classes %d&quot;, _global_klass_objects-&gt;length());</span>
<span class="line-added">+   {</span>
<span class="line-added">+     int num_type_array = 0, num_obj_array = 0, num_inst = 0;</span>
<span class="line-added">+     for (int i = 0; i &lt; _global_klass_objects-&gt;length(); i++) {</span>
<span class="line-added">+       Klass* k = _global_klass_objects-&gt;at(i);</span>
<span class="line-added">+       if (k-&gt;is_instance_klass()) {</span>
<span class="line-added">+         num_inst ++;</span>
<span class="line-added">+       } else if (k-&gt;is_objArray_klass()) {</span>
<span class="line-added">+         num_obj_array ++;</span>
<span class="line-added">+       } else {</span>
<span class="line-added">+         assert(k-&gt;is_typeArray_klass(), &quot;sanity&quot;);</span>
<span class="line-added">+         num_type_array ++;</span>
<span class="line-added">+       }</span>
<span class="line-added">+     }</span>
<span class="line-added">+     log_info(cds)(&quot;    instance classes   = %5d&quot;, num_inst);</span>
<span class="line-added">+     log_info(cds)(&quot;    obj array classes  = %5d&quot;, num_obj_array);</span>
<span class="line-added">+     log_info(cds)(&quot;    type array classes = %5d&quot;, num_type_array);</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ void VM_PopulateDumpSharedSpace::relocate_to_default_base_address(CHeapBitMap* ptrmap) {</span>
<span class="line-added">+   intx addr_delta = MetaspaceShared::final_delta();</span>
<span class="line-added">+   if (addr_delta == 0) {</span>
<span class="line-added">+     ArchivePtrMarker::compact((address)SharedBaseAddress, (address)_ro_region.top());</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     // We are not able to reserve space at Arguments::default_SharedBaseAddress() (due to ASLR).</span>
<span class="line-added">+     // This means that the current content of the archive is based on a random</span>
<span class="line-added">+     // address. Let&#39;s relocate all the pointers, so that it can be mapped to</span>
<span class="line-added">+     // Arguments::default_SharedBaseAddress() without runtime relocation.</span>
<span class="line-added">+     //</span>
<span class="line-added">+     // Note: both the base and dynamic archive are written with</span>
<span class="line-added">+     // FileMapHeader::_shared_base_address == Arguments::default_SharedBaseAddress()</span>
<span class="line-added">+ </span>
<span class="line-added">+     // Patch all pointers that are marked by ptrmap within this region,</span>
<span class="line-added">+     // where we have just dumped all the metaspace data.</span>
<span class="line-added">+     address patch_base = (address)SharedBaseAddress;</span>
<span class="line-added">+     address patch_end  = (address)_ro_region.top();</span>
<span class="line-added">+     size_t size = patch_end - patch_base;</span>
<span class="line-added">+ </span>
<span class="line-added">+     // the current value of the pointers to be patched must be within this</span>
<span class="line-added">+     // range (i.e., must point to valid metaspace objects)</span>
<span class="line-added">+     address valid_old_base = patch_base;</span>
<span class="line-added">+     address valid_old_end  = patch_end;</span>
<span class="line-added">+ </span>
<span class="line-added">+     // after patching, the pointers must point inside this range</span>
<span class="line-added">+     // (the requested location of the archive, as mapped at runtime).</span>
<span class="line-added">+     address valid_new_base = (address)Arguments::default_SharedBaseAddress();</span>
<span class="line-added">+     address valid_new_end  = valid_new_base + size;</span>
<span class="line-added">+ </span>
<span class="line-added">+     log_debug(cds)(&quot;Relocating archive from [&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot; ] to &quot;</span>
<span class="line-added">+                    &quot;[&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot; ]&quot;, p2i(patch_base), p2i(patch_end),</span>
<span class="line-added">+                    p2i(valid_new_base), p2i(valid_new_end));</span>
<span class="line-added">+ </span>
<span class="line-added">+     SharedDataRelocator&lt;true&gt; patcher((address*)patch_base, (address*)patch_end, valid_old_base, valid_old_end,</span>
<span class="line-added">+                                       valid_new_base, valid_new_end, addr_delta, ptrmap);</span>
<span class="line-added">+     ptrmap-&gt;iterate(&amp;patcher);</span>
<span class="line-added">+     ArchivePtrMarker::compact(patcher.max_non_null_offset());</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
  void VM_PopulateDumpSharedSpace::doit() {
<span class="line-added">+   CHeapBitMap ptrmap;</span>
<span class="line-added">+   MetaspaceShared::initialize_ptr_marker(&amp;ptrmap);</span>
<span class="line-added">+ </span>
    // We should no longer allocate anything from the metaspace, so that:
    //
    // (1) Metaspace::allocate might trigger GC if we have run out of
    //     committed metaspace, but we can&#39;t GC because we&#39;re running
    //     in the VM thread.
    // (2) ArchiveCompactor needs to work with a stable set of MetaspaceObjs.
    Metaspace::freeze();
<span class="line-added">+   DEBUG_ONLY(SystemDictionaryShared::NoClassLoadingMark nclm);</span>
  
    Thread* THREAD = VMThread::vm_thread();
  
    FileMapInfo::check_nonempty_dir_in_shared_path_table();
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1373,38 ***</span>
    SystemDictionaryShared::check_excluded_classes();
    _global_klass_objects = new GrowableArray&lt;Klass*&gt;(1000);
    CollectClassesClosure collect_classes;
    ClassLoaderDataGraph::loaded_classes_do(&amp;collect_classes);
  
<span class="line-modified">!   tty-&gt;print_cr(&quot;Number of classes %d&quot;, _global_klass_objects-&gt;length());</span>
<span class="line-removed">-   {</span>
<span class="line-removed">-     int num_type_array = 0, num_obj_array = 0, num_inst = 0;</span>
<span class="line-removed">-     for (int i = 0; i &lt; _global_klass_objects-&gt;length(); i++) {</span>
<span class="line-removed">-       Klass* k = _global_klass_objects-&gt;at(i);</span>
<span class="line-removed">-       if (k-&gt;is_instance_klass()) {</span>
<span class="line-removed">-         num_inst ++;</span>
<span class="line-removed">-       } else if (k-&gt;is_objArray_klass()) {</span>
<span class="line-removed">-         num_obj_array ++;</span>
<span class="line-removed">-       } else {</span>
<span class="line-removed">-         assert(k-&gt;is_typeArray_klass(), &quot;sanity&quot;);</span>
<span class="line-removed">-         num_type_array ++;</span>
<span class="line-removed">-       }</span>
<span class="line-removed">-     }</span>
<span class="line-removed">-     tty-&gt;print_cr(&quot;    instance classes   = %5d&quot;, num_inst);</span>
<span class="line-removed">-     tty-&gt;print_cr(&quot;    obj array classes  = %5d&quot;, num_obj_array);</span>
<span class="line-removed">-     tty-&gt;print_cr(&quot;    type array classes = %5d&quot;, num_type_array);</span>
<span class="line-removed">-   }</span>
  
    // Ensure the ConstMethods won&#39;t be modified at run-time
<span class="line-modified">!   tty-&gt;print(&quot;Updating ConstMethods ... &quot;);</span>
<span class="line-modified">!   rewrite_nofast_bytecodes_and_calculate_fingerprints();</span>
<span class="line-modified">!   tty-&gt;print_cr(&quot;done. &quot;);</span>
  
    // Remove all references outside the metadata
<span class="line-modified">!   tty-&gt;print(&quot;Removing unshareable information ... &quot;);</span>
    remove_unshareable_in_classes();
<span class="line-modified">!   tty-&gt;print_cr(&quot;done. &quot;);</span>
  
    ArchiveCompactor::initialize();
    ArchiveCompactor::copy_and_compact();
  
    dump_symbols();
<span class="line-new-header">--- 1519,25 ---</span>
    SystemDictionaryShared::check_excluded_classes();
    _global_klass_objects = new GrowableArray&lt;Klass*&gt;(1000);
    CollectClassesClosure collect_classes;
    ClassLoaderDataGraph::loaded_classes_do(&amp;collect_classes);
  
<span class="line-modified">!   print_class_stats();</span>
  
    // Ensure the ConstMethods won&#39;t be modified at run-time
<span class="line-modified">!   log_info(cds)(&quot;Updating ConstMethods ... &quot;);</span>
<span class="line-modified">!   rewrite_nofast_bytecodes_and_calculate_fingerprints(THREAD);</span>
<span class="line-modified">!   log_info(cds)(&quot;done. &quot;);</span>
  
    // Remove all references outside the metadata
<span class="line-modified">!   log_info(cds)(&quot;Removing unshareable information ... &quot;);</span>
    remove_unshareable_in_classes();
<span class="line-modified">!   log_info(cds)(&quot;done. &quot;);</span>
<span class="line-added">+ </span>
<span class="line-added">+   MetaspaceShared::allocate_cloned_cpp_vtptrs();</span>
<span class="line-added">+   char* cloned_vtables = _mc_region.top();</span>
<span class="line-added">+   MetaspaceShared::allocate_cpp_vtable_clones();</span>
  
    ArchiveCompactor::initialize();
    ArchiveCompactor::copy_and_compact();
  
    dump_symbols();
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1414,86 ***</span>
    _open_archive_heap_regions = NULL;
    dump_java_heap_objects();
  
    ArchiveCompactor::relocate_well_known_klasses();
  
<span class="line-modified">!   char* read_only_tables_start = dump_read_only_tables();</span>
<span class="line-modified">!   _ro_region.pack(&amp;_md_region);</span>
<span class="line-removed">- </span>
<span class="line-removed">-   char* vtbl_list = _md_region.top();</span>
<span class="line-removed">-   MetaspaceShared::allocate_cpp_vtable_clones();</span>
<span class="line-removed">-   _md_region.pack();</span>
<span class="line-removed">- </span>
<span class="line-removed">-   // The 4 core spaces are allocated consecutively mc-&gt;rw-&gt;ro-&gt;md, so there total size</span>
<span class="line-removed">-   // is just the spaces between the two ends.</span>
<span class="line-removed">-   size_t core_spaces_size = _md_region.end() - _mc_region.base();</span>
<span class="line-removed">-   assert(core_spaces_size == (size_t)align_up(core_spaces_size, Metaspace::reserve_alignment()),</span>
<span class="line-removed">-          &quot;should already be aligned&quot;);</span>
<span class="line-removed">- </span>
<span class="line-removed">-   // During patching, some virtual methods may be called, so at this point</span>
<span class="line-removed">-   // the vtables must contain valid methods (as filled in by CppVtableCloner::allocate).</span>
<span class="line-removed">-   MetaspaceShared::patch_cpp_vtable_pointers();</span>
  
    // The vtable clones contain addresses of the current process.
    // We don&#39;t want to write these addresses into the archive.
    MetaspaceShared::zero_cpp_vtable_clones_for_writing();
  
    // Create and write the archive file that maps the shared spaces.
  
<span class="line-modified">!   FileMapInfo* mapinfo = new FileMapInfo();</span>
    mapinfo-&gt;populate_header(os::vm_allocation_granularity());
<span class="line-modified">!   mapinfo-&gt;set_read_only_tables_start(read_only_tables_start);</span>
<span class="line-modified">!   mapinfo-&gt;set_misc_data_patching_start(vtbl_list);</span>
<span class="line-modified">!   mapinfo-&gt;set_cds_i2i_entry_code_buffers(MetaspaceShared::cds_i2i_entry_code_buffers());</span>
<span class="line-modified">!   mapinfo-&gt;set_cds_i2i_entry_code_buffers_size(MetaspaceShared::cds_i2i_entry_code_buffers_size());</span>
<span class="line-modified">!   mapinfo-&gt;set_core_spaces_size(core_spaces_size);</span>
<span class="line-modified">! </span>
<span class="line-modified">!   for (int pass=1; pass&lt;=2; pass++) {</span>
<span class="line-removed">-     bool print_archive_log = (pass==1);</span>
<span class="line-removed">-     if (pass == 1) {</span>
<span class="line-removed">-       // The first pass doesn&#39;t actually write the data to disk. All it</span>
<span class="line-removed">-       // does is to update the fields in the mapinfo-&gt;_header.</span>
<span class="line-removed">-     } else {</span>
<span class="line-removed">-       // After the first pass, the contents of mapinfo-&gt;_header are finalized,</span>
<span class="line-removed">-       // so we can compute the header&#39;s CRC, and write the contents of the header</span>
<span class="line-removed">-       // and the regions into disk.</span>
<span class="line-removed">-       mapinfo-&gt;open_for_write();</span>
<span class="line-removed">-       mapinfo-&gt;set_header_crc(mapinfo-&gt;compute_header_crc());</span>
<span class="line-removed">-     }</span>
<span class="line-removed">-     mapinfo-&gt;write_header();</span>
<span class="line-removed">- </span>
<span class="line-removed">-     // NOTE: md contains the trampoline code for method entries, which are patched at run time,</span>
<span class="line-removed">-     // so it needs to be read/write.</span>
<span class="line-removed">-     write_region(mapinfo, MetaspaceShared::mc, &amp;_mc_region, /*read_only=*/false,/*allow_exec=*/true);</span>
<span class="line-removed">-     write_region(mapinfo, MetaspaceShared::rw, &amp;_rw_region, /*read_only=*/false,/*allow_exec=*/false);</span>
<span class="line-removed">-     write_region(mapinfo, MetaspaceShared::ro, &amp;_ro_region, /*read_only=*/true, /*allow_exec=*/false);</span>
<span class="line-removed">-     write_region(mapinfo, MetaspaceShared::md, &amp;_md_region, /*read_only=*/false,/*allow_exec=*/false);</span>
<span class="line-removed">- </span>
<span class="line-removed">-     _total_closed_archive_region_size = mapinfo-&gt;write_archive_heap_regions(</span>
                                          _closed_archive_heap_regions,
                                          _closed_archive_heap_oopmaps,
                                          MetaspaceShared::first_closed_archive_heap_region,
<span class="line-modified">!                                         MetaspaceShared::max_closed_archive_heap_region,</span>
<span class="line-modified">!                                         print_archive_log);</span>
<span class="line-removed">-     _total_open_archive_region_size = mapinfo-&gt;write_archive_heap_regions(</span>
                                          _open_archive_heap_regions,
                                          _open_archive_heap_oopmaps,
                                          MetaspaceShared::first_open_archive_heap_region,
<span class="line-modified">!                                         MetaspaceShared::max_open_archive_heap_region,</span>
<span class="line-removed">-                                         print_archive_log);</span>
<span class="line-removed">-   }</span>
  
    mapinfo-&gt;close();
  
<span class="line-removed">-   // Restore the vtable in case we invoke any virtual methods.</span>
<span class="line-removed">-   MetaspaceShared::clone_cpp_vtables((intptr_t*)vtbl_list);</span>
<span class="line-removed">- </span>
    print_region_stats();
  
    if (log_is_enabled(Info, cds)) {
      ArchiveCompactor::alloc_stats()-&gt;print_stats(int(_ro_region.used()), int(_rw_region.used()),
<span class="line-modified">!                                                  int(_mc_region.used()), int(_md_region.used()));</span>
    }
  
    if (PrintSystemDictionaryAtExit) {
      SystemDictionary::print();
    }
<span class="line-new-header">--- 1547,52 ---</span>
    _open_archive_heap_regions = NULL;
    dump_java_heap_objects();
  
    ArchiveCompactor::relocate_well_known_klasses();
  
<span class="line-modified">!   char* serialized_data = dump_read_only_tables();</span>
<span class="line-modified">!   _ro_region.pack();</span>
  
    // The vtable clones contain addresses of the current process.
    // We don&#39;t want to write these addresses into the archive.
    MetaspaceShared::zero_cpp_vtable_clones_for_writing();
  
<span class="line-added">+   // relocate the data so that it can be mapped to Arguments::default_SharedBaseAddress()</span>
<span class="line-added">+   // without runtime relocation.</span>
<span class="line-added">+   relocate_to_default_base_address(&amp;ptrmap);</span>
<span class="line-added">+ </span>
    // Create and write the archive file that maps the shared spaces.
  
<span class="line-modified">!   FileMapInfo* mapinfo = new FileMapInfo(true);</span>
    mapinfo-&gt;populate_header(os::vm_allocation_granularity());
<span class="line-modified">!   mapinfo-&gt;set_serialized_data(serialized_data);</span>
<span class="line-modified">!   mapinfo-&gt;set_cloned_vtables(cloned_vtables);</span>
<span class="line-modified">!   mapinfo-&gt;set_i2i_entry_code_buffers(MetaspaceShared::i2i_entry_code_buffers(),</span>
<span class="line-modified">!                                       MetaspaceShared::i2i_entry_code_buffers_size());</span>
<span class="line-modified">!   mapinfo-&gt;open_for_write();</span>
<span class="line-modified">!   MetaspaceShared::write_core_archive_regions(mapinfo);</span>
<span class="line-modified">!   _total_closed_archive_region_size = mapinfo-&gt;write_archive_heap_regions(</span>
                                          _closed_archive_heap_regions,
                                          _closed_archive_heap_oopmaps,
                                          MetaspaceShared::first_closed_archive_heap_region,
<span class="line-modified">!                                         MetaspaceShared::max_closed_archive_heap_region);</span>
<span class="line-modified">!   _total_open_archive_region_size = mapinfo-&gt;write_archive_heap_regions(</span>
                                          _open_archive_heap_regions,
                                          _open_archive_heap_oopmaps,
                                          MetaspaceShared::first_open_archive_heap_region,
<span class="line-modified">!                                         MetaspaceShared::max_open_archive_heap_region);</span>
  
<span class="line-added">+   mapinfo-&gt;set_final_requested_base((char*)Arguments::default_SharedBaseAddress());</span>
<span class="line-added">+   mapinfo-&gt;set_header_crc(mapinfo-&gt;compute_header_crc());</span>
<span class="line-added">+   mapinfo-&gt;write_header();</span>
    mapinfo-&gt;close();
  
    print_region_stats();
  
    if (log_is_enabled(Info, cds)) {
      ArchiveCompactor::alloc_stats()-&gt;print_stats(int(_ro_region.used()), int(_rw_region.used()),
<span class="line-modified">!                                                  int(_mc_region.used()));</span>
    }
  
    if (PrintSystemDictionaryAtExit) {
      SystemDictionary::print();
    }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1509,55 ***</span>
    vm_direct_exit(0);
  }
  
  void VM_PopulateDumpSharedSpace::print_region_stats() {
    // Print statistics of all the regions
    const size_t total_reserved = _ro_region.reserved()  + _rw_region.reserved() +
<span class="line-modified">!                                 _mc_region.reserved()  + _md_region.reserved() +</span>
                                  _total_closed_archive_region_size +
                                  _total_open_archive_region_size;
    const size_t total_bytes = _ro_region.used()  + _rw_region.used() +
<span class="line-modified">!                              _mc_region.used()  + _md_region.used() +</span>
                               _total_closed_archive_region_size +
                               _total_open_archive_region_size;
    const double total_u_perc = percent_of(total_bytes, total_reserved);
  
    _mc_region.print(total_reserved);
    _rw_region.print(total_reserved);
    _ro_region.print(total_reserved);
<span class="line-modified">!   _md_region.print(total_reserved);</span>
    print_heap_region_stats(_closed_archive_heap_regions, &quot;ca&quot;, total_reserved);
    print_heap_region_stats(_open_archive_heap_regions, &quot;oa&quot;, total_reserved);
  
<span class="line-modified">!   tty-&gt;print_cr(&quot;total    : &quot; SIZE_FORMAT_W(9) &quot; [100.0%% of total] out of &quot; SIZE_FORMAT_W(9) &quot; bytes [%5.1f%% used]&quot;,</span>
                   total_bytes, total_reserved, total_u_perc);
  }
  
  void VM_PopulateDumpSharedSpace::print_heap_region_stats(GrowableArray&lt;MemRegion&gt; *heap_mem,
<span class="line-modified">!                                                          const char *name, const size_t total_size) {</span>
    int arr_len = heap_mem == NULL ? 0 : heap_mem-&gt;length();
    for (int i = 0; i &lt; arr_len; i++) {
        char* start = (char*)heap_mem-&gt;at(i).start();
        size_t size = heap_mem-&gt;at(i).byte_size();
        char* top = start + size;
<span class="line-modified">!       tty-&gt;print_cr(&quot;%s%d space: &quot; SIZE_FORMAT_W(9) &quot; [ %4.1f%% of total] out of &quot; SIZE_FORMAT_W(9) &quot; bytes [100.0%% used] at &quot; INTPTR_FORMAT,</span>
<span class="line-modified">!                     name, i, size, size/double(total_size)*100.0, size, p2i(start));</span>
  
    }
  }
  
  // Update a Java object to point its Klass* to the new location after
  // shared archive has been compacted.
  void MetaspaceShared::relocate_klass_ptr(oop o) {
    assert(DumpSharedSpaces, &quot;sanity&quot;);
    Klass* k = ArchiveCompactor::get_relocated_klass(o-&gt;klass());
    o-&gt;set_klass(k);
  }
  
<span class="line-modified">! Klass* MetaspaceShared::get_relocated_klass(Klass *k) {</span>
    assert(DumpSharedSpaces, &quot;sanity&quot;);
<span class="line-modified">!   return ArchiveCompactor::get_relocated_klass(k);</span>
  }
  
  class LinkSharedClassesClosure : public KlassClosure {
    Thread* THREAD;
    bool    _made_progress;
<span class="line-new-header">--- 1608,85 ---</span>
    vm_direct_exit(0);
  }
  
  void VM_PopulateDumpSharedSpace::print_region_stats() {
    // Print statistics of all the regions
<span class="line-added">+   const size_t bitmap_used = ArchivePtrMarker::ptrmap()-&gt;size_in_bytes();</span>
<span class="line-added">+   const size_t bitmap_reserved = align_up(bitmap_used, Metaspace::reserve_alignment());</span>
    const size_t total_reserved = _ro_region.reserved()  + _rw_region.reserved() +
<span class="line-modified">!                                 _mc_region.reserved()  +</span>
<span class="line-added">+                                 bitmap_reserved +</span>
                                  _total_closed_archive_region_size +
                                  _total_open_archive_region_size;
    const size_t total_bytes = _ro_region.used()  + _rw_region.used() +
<span class="line-modified">!                              _mc_region.used()  +</span>
<span class="line-added">+                              bitmap_used +</span>
                               _total_closed_archive_region_size +
                               _total_open_archive_region_size;
    const double total_u_perc = percent_of(total_bytes, total_reserved);
  
    _mc_region.print(total_reserved);
    _rw_region.print(total_reserved);
    _ro_region.print(total_reserved);
<span class="line-modified">!   print_bitmap_region_stats(bitmap_reserved, total_reserved);</span>
    print_heap_region_stats(_closed_archive_heap_regions, &quot;ca&quot;, total_reserved);
    print_heap_region_stats(_open_archive_heap_regions, &quot;oa&quot;, total_reserved);
  
<span class="line-modified">!   log_debug(cds)(&quot;total    : &quot; SIZE_FORMAT_W(9) &quot; [100.0%% of total] out of &quot; SIZE_FORMAT_W(9) &quot; bytes [%5.1f%% used]&quot;,</span>
                   total_bytes, total_reserved, total_u_perc);
  }
  
<span class="line-added">+ void VM_PopulateDumpSharedSpace::print_bitmap_region_stats(size_t size, size_t total_size) {</span>
<span class="line-added">+   log_debug(cds)(&quot;bm  space: &quot; SIZE_FORMAT_W(9) &quot; [ %4.1f%% of total] out of &quot; SIZE_FORMAT_W(9) &quot; bytes [100.0%% used] at &quot; INTPTR_FORMAT,</span>
<span class="line-added">+                  size, size/double(total_size)*100.0, size, p2i(NULL));</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
  void VM_PopulateDumpSharedSpace::print_heap_region_stats(GrowableArray&lt;MemRegion&gt; *heap_mem,
<span class="line-modified">!                                                          const char *name, size_t total_size) {</span>
    int arr_len = heap_mem == NULL ? 0 : heap_mem-&gt;length();
    for (int i = 0; i &lt; arr_len; i++) {
        char* start = (char*)heap_mem-&gt;at(i).start();
        size_t size = heap_mem-&gt;at(i).byte_size();
        char* top = start + size;
<span class="line-modified">!       log_debug(cds)(&quot;%s%d space: &quot; SIZE_FORMAT_W(9) &quot; [ %4.1f%% of total] out of &quot; SIZE_FORMAT_W(9) &quot; bytes [100.0%% used] at &quot; INTPTR_FORMAT,</span>
<span class="line-modified">!                      name, i, size, size/double(total_size)*100.0, size, p2i(start));</span>
  
    }
  }
  
<span class="line-added">+ void MetaspaceShared::write_core_archive_regions(FileMapInfo* mapinfo) {</span>
<span class="line-added">+   // Make sure NUM_CDS_REGIONS (exported in cds.h) agrees with</span>
<span class="line-added">+   // MetaspaceShared::n_regions (internal to hotspot).</span>
<span class="line-added">+   assert(NUM_CDS_REGIONS == MetaspaceShared::n_regions, &quot;sanity&quot;);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // mc contains the trampoline code for method entries, which are patched at run time,</span>
<span class="line-added">+   // so it needs to be read/write.</span>
<span class="line-added">+   write_region(mapinfo, mc, &amp;_mc_region, /*read_only=*/false,/*allow_exec=*/true);</span>
<span class="line-added">+   write_region(mapinfo, rw, &amp;_rw_region, /*read_only=*/false,/*allow_exec=*/false);</span>
<span class="line-added">+   write_region(mapinfo, ro, &amp;_ro_region, /*read_only=*/true, /*allow_exec=*/false);</span>
<span class="line-added">+   mapinfo-&gt;write_bitmap_region(ArchivePtrMarker::ptrmap());</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ void MetaspaceShared::write_region(FileMapInfo* mapinfo, int region_idx, DumpRegion* dump_region, bool read_only,  bool allow_exec) {</span>
<span class="line-added">+   mapinfo-&gt;write_region(region_idx, dump_region-&gt;base(), dump_region-&gt;used(), read_only, allow_exec);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
  // Update a Java object to point its Klass* to the new location after
  // shared archive has been compacted.
  void MetaspaceShared::relocate_klass_ptr(oop o) {
    assert(DumpSharedSpaces, &quot;sanity&quot;);
    Klass* k = ArchiveCompactor::get_relocated_klass(o-&gt;klass());
    o-&gt;set_klass(k);
  }
  
<span class="line-modified">! Klass* MetaspaceShared::get_relocated_klass(Klass *k, bool is_final) {</span>
    assert(DumpSharedSpaces, &quot;sanity&quot;);
<span class="line-modified">!   k = ArchiveCompactor::get_relocated_klass(k);</span>
<span class="line-added">+   if (is_final) {</span>
<span class="line-added">+     k = (Klass*)(address(k) + final_delta());</span>
<span class="line-added">+   }</span>
<span class="line-added">+   return k;</span>
  }
  
  class LinkSharedClassesClosure : public KlassClosure {
    Thread* THREAD;
    bool    _made_progress;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1625,11 ***</span>
  
  // Preload classes from a list, populate the shared spaces and dump to a
  // file.
  void MetaspaceShared::preload_and_dump(TRAPS) {
    { TraceTime timer(&quot;Dump Shared Spaces&quot;, TRACETIME_LOG(Info, startuptime));
<span class="line-modified">!     ResourceMark rm;</span>
      char class_list_path_str[JVM_MAXPATHLEN];
      // Preload classes to be shared.
      const char* class_list_path;
      if (SharedClassListFile == NULL) {
        // Construct the path to the class list (in jre/lib)
<span class="line-new-header">--- 1754,11 ---</span>
  
  // Preload classes from a list, populate the shared spaces and dump to a
  // file.
  void MetaspaceShared::preload_and_dump(TRAPS) {
    { TraceTime timer(&quot;Dump Shared Spaces&quot;, TRACETIME_LOG(Info, startuptime));
<span class="line-modified">!     ResourceMark rm(THREAD);</span>
      char class_list_path_str[JVM_MAXPATHLEN];
      // Preload classes to be shared.
      const char* class_list_path;
      if (SharedClassListFile == NULL) {
        // Construct the path to the class list (in jre/lib)
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1659,37 ***</span>
        class_list_path = class_list_path_str;
      } else {
        class_list_path = SharedClassListFile;
      }
  
<span class="line-modified">!     tty-&gt;print_cr(&quot;Loading classes to share ...&quot;);</span>
      _has_error_classes = false;
      int class_count = preload_classes(class_list_path, THREAD);
      if (ExtraSharedClassListFile) {
        class_count += preload_classes(ExtraSharedClassListFile, THREAD);
      }
<span class="line-modified">!     tty-&gt;print_cr(&quot;Loading classes to share: done.&quot;);</span>
  
      log_info(cds)(&quot;Shared spaces: preloaded %d classes&quot;, class_count);
  
      if (SharedArchiveConfigFile) {
<span class="line-modified">!       tty-&gt;print_cr(&quot;Reading extra data from %s ...&quot;, SharedArchiveConfigFile);</span>
        read_extra_data(SharedArchiveConfigFile, THREAD);
      }
<span class="line-modified">!     tty-&gt;print_cr(&quot;Reading extra data: done.&quot;);</span>
  
      HeapShared::init_subgraph_entry_fields(THREAD);
  
      // Rewrite and link classes
<span class="line-modified">!     tty-&gt;print_cr(&quot;Rewriting and linking classes ...&quot;);</span>
  
      // Link any classes which got missed. This would happen if we have loaded classes that
      // were not explicitly specified in the classlist. E.g., if an interface implemented by class K
      // fails verification, all other interfaces that were not specified in the classlist but
      // are implemented by K are not verified.
      link_and_cleanup_shared_classes(CATCH);
<span class="line-modified">!     tty-&gt;print_cr(&quot;Rewriting and linking classes: done&quot;);</span>
  
      if (HeapShared::is_heap_object_archiving_allowed()) {
        // Avoid fragmentation while archiving heap objects.
        Universe::heap()-&gt;soft_ref_policy()-&gt;set_should_clear_all_soft_refs(true);
        Universe::heap()-&gt;collect(GCCause::_archive_time_gc);
<span class="line-new-header">--- 1788,37 ---</span>
        class_list_path = class_list_path_str;
      } else {
        class_list_path = SharedClassListFile;
      }
  
<span class="line-modified">!     log_info(cds)(&quot;Loading classes to share ...&quot;);</span>
      _has_error_classes = false;
      int class_count = preload_classes(class_list_path, THREAD);
      if (ExtraSharedClassListFile) {
        class_count += preload_classes(ExtraSharedClassListFile, THREAD);
      }
<span class="line-modified">!     log_info(cds)(&quot;Loading classes to share: done.&quot;);</span>
  
      log_info(cds)(&quot;Shared spaces: preloaded %d classes&quot;, class_count);
  
      if (SharedArchiveConfigFile) {
<span class="line-modified">!       log_info(cds)(&quot;Reading extra data from %s ...&quot;, SharedArchiveConfigFile);</span>
        read_extra_data(SharedArchiveConfigFile, THREAD);
      }
<span class="line-modified">!     log_info(cds)(&quot;Reading extra data: done.&quot;);</span>
  
      HeapShared::init_subgraph_entry_fields(THREAD);
  
      // Rewrite and link classes
<span class="line-modified">!     log_info(cds)(&quot;Rewriting and linking classes ...&quot;);</span>
  
      // Link any classes which got missed. This would happen if we have loaded classes that
      // were not explicitly specified in the classlist. E.g., if an interface implemented by class K
      // fails verification, all other interfaces that were not specified in the classlist but
      // are implemented by K are not verified.
      link_and_cleanup_shared_classes(CATCH);
<span class="line-modified">!     log_info(cds)(&quot;Rewriting and linking classes: done&quot;);</span>
  
      if (HeapShared::is_heap_object_archiving_allowed()) {
        // Avoid fragmentation while archiving heap objects.
        Universe::heap()-&gt;soft_ref_policy()-&gt;set_should_clear_all_soft_refs(true);
        Universe::heap()-&gt;collect(GCCause::_archive_time_gc);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1710,17 ***</span>
      Klass* klass = parser.load_current_class(THREAD);
      if (HAS_PENDING_EXCEPTION) {
        if (klass == NULL &amp;&amp;
            (PENDING_EXCEPTION-&gt;klass()-&gt;name() == vmSymbols::java_lang_ClassNotFoundException())) {
          // print a warning only when the pending exception is class not found
<span class="line-modified">!         tty-&gt;print_cr(&quot;Preload Warning: Cannot find %s&quot;, parser.current_class_name());</span>
        }
        CLEAR_PENDING_EXCEPTION;
      }
      if (klass != NULL) {
        if (log_is_enabled(Trace, cds)) {
<span class="line-modified">!         ResourceMark rm;</span>
          log_trace(cds)(&quot;Shared spaces preloaded: %s&quot;, klass-&gt;external_name());
        }
  
        if (klass-&gt;is_instance_klass()) {
          InstanceKlass* ik = InstanceKlass::cast(klass);
<span class="line-new-header">--- 1839,17 ---</span>
      Klass* klass = parser.load_current_class(THREAD);
      if (HAS_PENDING_EXCEPTION) {
        if (klass == NULL &amp;&amp;
            (PENDING_EXCEPTION-&gt;klass()-&gt;name() == vmSymbols::java_lang_ClassNotFoundException())) {
          // print a warning only when the pending exception is class not found
<span class="line-modified">!         log_warning(cds)(&quot;Preload Warning: Cannot find %s&quot;, parser.current_class_name());</span>
        }
        CLEAR_PENDING_EXCEPTION;
      }
      if (klass != NULL) {
        if (log_is_enabled(Trace, cds)) {
<span class="line-modified">!         ResourceMark rm(THREAD);</span>
          log_trace(cds)(&quot;Shared spaces preloaded: %s&quot;, klass-&gt;external_name());
        }
  
        if (klass-&gt;is_instance_klass()) {
          InstanceKlass* ik = InstanceKlass::cast(klass);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1755,12 ***</span>
        // dumping.
        BytecodeVerificationLocal = BytecodeVerificationRemote;
      }
      ik-&gt;link_class(THREAD);
      if (HAS_PENDING_EXCEPTION) {
<span class="line-modified">!       ResourceMark rm;</span>
<span class="line-modified">!       tty-&gt;print_cr(&quot;Preload Warning: Verification failed for %s&quot;,</span>
                      ik-&gt;external_name());
        CLEAR_PENDING_EXCEPTION;
        ik-&gt;set_in_error_state();
        _has_error_classes = true;
      }
<span class="line-new-header">--- 1884,12 ---</span>
        // dumping.
        BytecodeVerificationLocal = BytecodeVerificationRemote;
      }
      ik-&gt;link_class(THREAD);
      if (HAS_PENDING_EXCEPTION) {
<span class="line-modified">!       ResourceMark rm(THREAD);</span>
<span class="line-modified">!       log_warning(cds)(&quot;Preload Warning: Verification failed for %s&quot;,</span>
                      ik-&gt;external_name());
        CLEAR_PENDING_EXCEPTION;
        ik-&gt;set_in_error_state();
        _has_error_classes = true;
      }
</pre>
<hr />
<pre>
<span class="line-old-header">*** 1799,197 ***</span>
      ResourceBitMap oopmap = HeapShared::calculate_oopmap(regions-&gt;at(i));
      size_t size_in_bits = oopmap.size();
      size_t size_in_bytes = oopmap.size_in_bytes();
      uintptr_t* buffer = (uintptr_t*)_ro_region.allocate(size_in_bytes, sizeof(intptr_t));
      oopmap.write_to(buffer, size_in_bytes);
<span class="line-modified">!     log_info(cds)(&quot;Oopmap = &quot; INTPTR_FORMAT &quot; (&quot; SIZE_FORMAT_W(6) &quot; bytes) for heap region &quot;</span>
<span class="line-modified">!                   INTPTR_FORMAT &quot; (&quot; SIZE_FORMAT_W(8) &quot; bytes)&quot;,</span>
<span class="line-modified">!                   p2i(buffer), size_in_bytes,</span>
<span class="line-modified">!                   p2i(regions-&gt;at(i).start()), regions-&gt;at(i).byte_size());</span>
  
      ArchiveHeapOopmapInfo info;
      info._oopmap = (address)buffer;
      info._oopmap_size_in_bits = size_in_bits;
      oopmaps-&gt;append(info);
    }
  }
  #endif // INCLUDE_CDS_JAVA_HEAP
  
<span class="line-modified">! // Closure for serializing initialization data in from a data area</span>
<span class="line-modified">! // (ptr_array) read from the shared file.</span>
  
<span class="line-modified">! class ReadClosure : public SerializeClosure {</span>
<span class="line-modified">! private:</span>
<span class="line-modified">!   intptr_t** _ptr_array;</span>
  
<span class="line-modified">!   inline intptr_t nextPtr() {</span>
<span class="line-modified">!     return *(*_ptr_array)++;</span>
    }
  
<span class="line-modified">! public:</span>
<span class="line-modified">!   ReadClosure(intptr_t** ptr_array) { _ptr_array = ptr_array; }</span>
  
<span class="line-modified">!   void do_ptr(void** p) {</span>
<span class="line-modified">!     assert(*p == NULL, &quot;initializing previous initialized pointer.&quot;);</span>
<span class="line-modified">!     intptr_t obj = nextPtr();</span>
<span class="line-removed">-     assert((intptr_t)obj &gt;= 0 || (intptr_t)obj &lt; -100,</span>
<span class="line-removed">-            &quot;hit tag while initializing ptrs.&quot;);</span>
<span class="line-removed">-     *p = (void*)obj;</span>
    }
  
<span class="line-modified">!   void do_u4(u4* p) {</span>
<span class="line-modified">!     intptr_t obj = nextPtr();</span>
<span class="line-modified">!     *p = (u4)(uintx(obj));</span>
    }
  
<span class="line-modified">!   void do_tag(int tag) {</span>
<span class="line-modified">!     int old_tag;</span>
<span class="line-modified">!     old_tag = (int)(intptr_t)nextPtr();</span>
<span class="line-modified">!     // do_int(&amp;old_tag);</span>
<span class="line-modified">!     assert(tag == old_tag, &quot;old tag doesn&#39;t match&quot;);</span>
<span class="line-modified">!     FileMapInfo::assert_mark(tag == old_tag);</span>
    }
  
<span class="line-modified">!   void do_oop(oop *p) {</span>
<span class="line-modified">!     narrowOop o = (narrowOop)nextPtr();</span>
<span class="line-modified">!     if (o == 0 || !HeapShared::open_archive_heap_region_mapped()) {</span>
<span class="line-modified">!       p = NULL;</span>
      } else {
<span class="line-modified">!       assert(HeapShared::is_heap_object_archiving_allowed(),</span>
<span class="line-modified">!              &quot;Archived heap object is not allowed&quot;);</span>
<span class="line-modified">!       assert(HeapShared::open_archive_heap_region_mapped(),</span>
<span class="line-modified">!              &quot;Open archive heap region is not mapped&quot;);</span>
<span class="line-modified">!       *p = HeapShared::decode_from_archive(o);</span>
      }
    }
  
<span class="line-modified">!   void do_region(u_char* start, size_t size) {</span>
<span class="line-modified">!     assert((intptr_t)start % sizeof(intptr_t) == 0, &quot;bad alignment&quot;);</span>
<span class="line-modified">!     assert(size % sizeof(intptr_t) == 0, &quot;bad size&quot;);</span>
<span class="line-modified">!     do_tag((int)size);</span>
<span class="line-modified">!     while (size &gt; 0) {</span>
<span class="line-removed">-       *(intptr_t*)start = nextPtr();</span>
<span class="line-removed">-       start += sizeof(intptr_t);</span>
<span class="line-removed">-       size -= sizeof(intptr_t);</span>
<span class="line-removed">-     }</span>
    }
  
<span class="line-modified">!   bool reading() const { return true; }</span>
<span class="line-modified">! };</span>
  
<span class="line-modified">! // Return true if given address is in the misc data region</span>
<span class="line-modified">! bool MetaspaceShared::is_in_shared_region(const void* p, int idx) {</span>
<span class="line-modified">!   return UseSharedSpaces &amp;&amp; FileMapInfo::current_info()-&gt;is_in_shared_region(p, idx);</span>
  }
  
<span class="line-modified">! bool MetaspaceShared::is_in_trampoline_frame(address addr) {</span>
<span class="line-modified">!   if (UseSharedSpaces &amp;&amp; is_in_shared_region(addr, MetaspaceShared::mc)) {</span>
<span class="line-modified">!     return true;</span>
    }
<span class="line-modified">!   return false;</span>
  }
  
<span class="line-modified">! // Map shared spaces at requested addresses and return if succeeded.</span>
<span class="line-modified">! bool MetaspaceShared::map_shared_spaces(FileMapInfo* mapinfo) {</span>
<span class="line-modified">!   size_t image_alignment = mapinfo-&gt;alignment();</span>
  
<span class="line-modified">! #ifndef _WINDOWS</span>
<span class="line-modified">!   // Map in the shared memory and then map the regions on top of it.</span>
<span class="line-modified">!   // On Windows, don&#39;t map the memory here because it will cause the</span>
<span class="line-modified">!   // mappings of the regions to fail.</span>
<span class="line-modified">!   ReservedSpace shared_rs = mapinfo-&gt;reserve_shared_memory();</span>
<span class="line-modified">!   if (!shared_rs.is_reserved()) return false;</span>
<span class="line-removed">- #endif</span>
  
<span class="line-modified">!   assert(!DumpSharedSpaces, &quot;Should not be called with DumpSharedSpaces&quot;);</span>
<span class="line-modified">! </span>
<span class="line-modified">!   char* ro_base = NULL; char* ro_top;</span>
<span class="line-modified">!   char* rw_base = NULL; char* rw_top;</span>
<span class="line-modified">!   char* mc_base = NULL; char* mc_top;</span>
<span class="line-modified">!   char* md_base = NULL; char* md_top;</span>
<span class="line-modified">! </span>
<span class="line-modified">!   // Map each shared region</span>
<span class="line-modified">!   if ((mc_base = mapinfo-&gt;map_region(mc, &amp;mc_top)) != NULL &amp;&amp;</span>
<span class="line-modified">!       (rw_base = mapinfo-&gt;map_region(rw, &amp;rw_top)) != NULL &amp;&amp;</span>
<span class="line-modified">!       (ro_base = mapinfo-&gt;map_region(ro, &amp;ro_top)) != NULL &amp;&amp;</span>
<span class="line-modified">!       (md_base = mapinfo-&gt;map_region(md, &amp;md_top)) != NULL &amp;&amp;</span>
<span class="line-modified">!       (image_alignment == (size_t)os::vm_allocation_granularity()) &amp;&amp;</span>
<span class="line-modified">!       mapinfo-&gt;validate_shared_path_table()) {</span>
<span class="line-modified">!     // Success -- set up MetaspaceObj::_shared_metaspace_{base,top} for</span>
<span class="line-modified">!     // fast checking in MetaspaceShared::is_in_shared_metaspace() and</span>
<span class="line-modified">!     // MetaspaceObj::is_shared().</span>
<span class="line-modified">!     //</span>
<span class="line-modified">!     // We require that mc-&gt;rw-&gt;ro-&gt;md to be laid out consecutively, with no</span>
<span class="line-modified">!     // gaps between them. That way, we can ensure that the OS won&#39;t be able to</span>
<span class="line-modified">!     // allocate any new memory spaces inside _shared_metaspace_{base,top}, which</span>
<span class="line-modified">!     // would mess up the simple comparision in MetaspaceShared::is_in_shared_metaspace().</span>
<span class="line-modified">!     assert(mc_base &lt; ro_base &amp;&amp; mc_base &lt; rw_base &amp;&amp; mc_base &lt; md_base, &quot;must be&quot;);</span>
<span class="line-modified">!     assert(md_top  &gt; ro_top  &amp;&amp; md_top  &gt; rw_top  &amp;&amp; md_top  &gt; mc_top , &quot;must be&quot;);</span>
<span class="line-modified">!     assert(mc_top == rw_base, &quot;must be&quot;);</span>
<span class="line-modified">!     assert(rw_top == ro_base, &quot;must be&quot;);</span>
<span class="line-modified">!     assert(ro_top == md_base, &quot;must be&quot;);</span>
<span class="line-modified">! </span>
<span class="line-modified">!     _core_spaces_size = mapinfo-&gt;core_spaces_size();</span>
<span class="line-modified">!     MetaspaceObj::set_shared_metaspace_range((void*)mc_base, (void*)md_top);</span>
<span class="line-modified">!     return true;</span>
    } else {
<span class="line-modified">!     // If there was a failure in mapping any of the spaces, unmap the ones</span>
<span class="line-modified">!     // that succeeded</span>
<span class="line-modified">!     if (ro_base != NULL) mapinfo-&gt;unmap_region(ro);</span>
<span class="line-modified">!     if (rw_base != NULL) mapinfo-&gt;unmap_region(rw);</span>
<span class="line-modified">!     if (mc_base != NULL) mapinfo-&gt;unmap_region(mc);</span>
<span class="line-modified">!     if (md_base != NULL) mapinfo-&gt;unmap_region(md);</span>
<span class="line-removed">- #ifndef _WINDOWS</span>
<span class="line-removed">-     // Release the entire mapped region</span>
<span class="line-removed">-     shared_rs.release();</span>
<span class="line-removed">- #endif</span>
<span class="line-removed">-     // If -Xshare:on is specified, print out the error message and exit VM,</span>
<span class="line-removed">-     // otherwise, set UseSharedSpaces to false and continue.</span>
<span class="line-removed">-     if (RequireSharedSpaces || PrintSharedArchiveAndExit) {</span>
<span class="line-removed">-       vm_exit_during_initialization(&quot;Unable to use shared archive.&quot;, &quot;Failed map_region for using -Xshare:on.&quot;);</span>
      } else {
<span class="line-modified">!       FLAG_SET_DEFAULT(UseSharedSpaces, false);</span>
      }
<span class="line-modified">!     return false;</span>
    }
  }
  
  // Read the miscellaneous data from the shared file, and
  // serialize it out to its various destinations.
  
  void MetaspaceShared::initialize_shared_spaces() {
<span class="line-modified">!   FileMapInfo *mapinfo = FileMapInfo::current_info();</span>
<span class="line-modified">!   _cds_i2i_entry_code_buffers = mapinfo-&gt;cds_i2i_entry_code_buffers();</span>
<span class="line-modified">!   _cds_i2i_entry_code_buffers_size = mapinfo-&gt;cds_i2i_entry_code_buffers_size();</span>
<span class="line-modified">!   // _core_spaces_size is loaded from the shared archive immediatelly after mapping</span>
<span class="line-removed">-   assert(_core_spaces_size == mapinfo-&gt;core_spaces_size(), &quot;sanity&quot;);</span>
<span class="line-removed">-   char* buffer = mapinfo-&gt;misc_data_patching_start();</span>
    clone_cpp_vtables((intptr_t*)buffer);
  
<span class="line-removed">-   // The rest of the data is now stored in the RW region</span>
<span class="line-removed">-   buffer = mapinfo-&gt;read_only_tables_start();</span>
<span class="line-removed">- </span>
    // Verify various attributes of the archive, plus initialize the
    // shared string/symbol tables
    intptr_t* array = (intptr_t*)buffer;
    ReadClosure rc(&amp;array);
    serialize(&amp;rc);
  
    // Initialize the run-time symbol table.
    SymbolTable::create_table();
  
<span class="line-modified">!   mapinfo-&gt;patch_archived_heap_embedded_pointers();</span>
  
    // Close the mapinfo file
<span class="line-modified">!   mapinfo-&gt;close();</span>
  
    if (PrintSharedArchiveAndExit) {
      if (PrintSharedDictionary) {
        tty-&gt;print_cr(&quot;\nShared classes:\n&quot;);
        SystemDictionaryShared::print_on(tty);
      }
<span class="line-modified">!     if (_archive_loading_failed) {</span>
        tty-&gt;print_cr(&quot;archive is invalid&quot;);
        vm_exit(1);
      } else {
        tty-&gt;print_cr(&quot;archive is valid&quot;);
        vm_exit(0);
<span class="line-new-header">--- 1928,456 ---</span>
      ResourceBitMap oopmap = HeapShared::calculate_oopmap(regions-&gt;at(i));
      size_t size_in_bits = oopmap.size();
      size_t size_in_bytes = oopmap.size_in_bytes();
      uintptr_t* buffer = (uintptr_t*)_ro_region.allocate(size_in_bytes, sizeof(intptr_t));
      oopmap.write_to(buffer, size_in_bytes);
<span class="line-modified">!     log_info(cds, heap)(&quot;Oopmap = &quot; INTPTR_FORMAT &quot; (&quot; SIZE_FORMAT_W(6) &quot; bytes) for heap region &quot;</span>
<span class="line-modified">!                         INTPTR_FORMAT &quot; (&quot; SIZE_FORMAT_W(8) &quot; bytes)&quot;,</span>
<span class="line-modified">!                         p2i(buffer), size_in_bytes,</span>
<span class="line-modified">!                         p2i(regions-&gt;at(i).start()), regions-&gt;at(i).byte_size());</span>
  
      ArchiveHeapOopmapInfo info;
      info._oopmap = (address)buffer;
      info._oopmap_size_in_bits = size_in_bits;
      oopmaps-&gt;append(info);
    }
  }
  #endif // INCLUDE_CDS_JAVA_HEAP
  
<span class="line-modified">! void ReadClosure::do_ptr(void** p) {</span>
<span class="line-modified">!   assert(*p == NULL, &quot;initializing previous initialized pointer.&quot;);</span>
<span class="line-added">+   intptr_t obj = nextPtr();</span>
<span class="line-added">+   assert((intptr_t)obj &gt;= 0 || (intptr_t)obj &lt; -100,</span>
<span class="line-added">+          &quot;hit tag while initializing ptrs.&quot;);</span>
<span class="line-added">+   *p = (void*)obj;</span>
<span class="line-added">+ }</span>
  
<span class="line-modified">! void ReadClosure::do_u4(u4* p) {</span>
<span class="line-modified">!   intptr_t obj = nextPtr();</span>
<span class="line-modified">!   *p = (u4)(uintx(obj));</span>
<span class="line-added">+ }</span>
  
<span class="line-modified">! void ReadClosure::do_bool(bool* p) {</span>
<span class="line-modified">!   intptr_t obj = nextPtr();</span>
<span class="line-added">+   *p = (bool)(uintx(obj));</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ void ReadClosure::do_tag(int tag) {</span>
<span class="line-added">+   int old_tag;</span>
<span class="line-added">+   old_tag = (int)(intptr_t)nextPtr();</span>
<span class="line-added">+   // do_int(&amp;old_tag);</span>
<span class="line-added">+   assert(tag == old_tag, &quot;old tag doesn&#39;t match&quot;);</span>
<span class="line-added">+   FileMapInfo::assert_mark(tag == old_tag);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ void ReadClosure::do_oop(oop *p) {</span>
<span class="line-added">+   narrowOop o = (narrowOop)nextPtr();</span>
<span class="line-added">+   if (o == 0 || !HeapShared::open_archive_heap_region_mapped()) {</span>
<span class="line-added">+     p = NULL;</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     assert(HeapShared::is_heap_object_archiving_allowed(),</span>
<span class="line-added">+            &quot;Archived heap object is not allowed&quot;);</span>
<span class="line-added">+     assert(HeapShared::open_archive_heap_region_mapped(),</span>
<span class="line-added">+            &quot;Open archive heap region is not mapped&quot;);</span>
<span class="line-added">+     *p = HeapShared::decode_from_archive(o);</span>
    }
<span class="line-added">+ }</span>
  
<span class="line-modified">! void ReadClosure::do_region(u_char* start, size_t size) {</span>
<span class="line-modified">!   assert((intptr_t)start % sizeof(intptr_t) == 0, &quot;bad alignment&quot;);</span>
<span class="line-added">+   assert(size % sizeof(intptr_t) == 0, &quot;bad size&quot;);</span>
<span class="line-added">+   do_tag((int)size);</span>
<span class="line-added">+   while (size &gt; 0) {</span>
<span class="line-added">+     *(intptr_t*)start = nextPtr();</span>
<span class="line-added">+     start += sizeof(intptr_t);</span>
<span class="line-added">+     size -= sizeof(intptr_t);</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ void MetaspaceShared::set_shared_metaspace_range(void* base, void *static_top, void* top) {</span>
<span class="line-added">+   assert(base &lt;= static_top &amp;&amp; static_top &lt;= top, &quot;must be&quot;);</span>
<span class="line-added">+   _shared_metaspace_static_top = static_top;</span>
<span class="line-added">+   MetaspaceObj::set_shared_metaspace_range(base, top);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ // Return true if given address is in the misc data region</span>
<span class="line-added">+ bool MetaspaceShared::is_in_shared_region(const void* p, int idx) {</span>
<span class="line-added">+   return UseSharedSpaces &amp;&amp; FileMapInfo::current_info()-&gt;is_in_shared_region(p, idx);</span>
<span class="line-added">+ }</span>
  
<span class="line-modified">! bool MetaspaceShared::is_in_trampoline_frame(address addr) {</span>
<span class="line-modified">!   if (UseSharedSpaces &amp;&amp; is_in_shared_region(addr, MetaspaceShared::mc)) {</span>
<span class="line-modified">!     return true;</span>
    }
<span class="line-added">+   return false;</span>
<span class="line-added">+ }</span>
  
<span class="line-modified">! bool MetaspaceShared::is_shared_dynamic(void* p) {</span>
<span class="line-modified">!   if ((p &lt; MetaspaceObj::shared_metaspace_top()) &amp;&amp;</span>
<span class="line-modified">!       (p &gt;= _shared_metaspace_static_top)) {</span>
<span class="line-added">+     return true;</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     return false;</span>
    }
<span class="line-added">+ }</span>
  
<span class="line-modified">! void MetaspaceShared::initialize_runtime_shared_and_meta_spaces() {</span>
<span class="line-modified">!   assert(UseSharedSpaces, &quot;Must be called when UseSharedSpaces is enabled&quot;);</span>
<span class="line-modified">!   MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;</span>
<span class="line-modified">!   FileMapInfo* static_mapinfo = open_static_archive();</span>
<span class="line-modified">!   FileMapInfo* dynamic_mapinfo = NULL;</span>
<span class="line-modified">! </span>
<span class="line-added">+   if (static_mapinfo != NULL) {</span>
<span class="line-added">+     dynamic_mapinfo = open_dynamic_archive();</span>
<span class="line-added">+ </span>
<span class="line-added">+     // First try to map at the requested address</span>
<span class="line-added">+     result = map_archives(static_mapinfo, dynamic_mapinfo, true);</span>
<span class="line-added">+     if (result == MAP_ARCHIVE_MMAP_FAILURE) {</span>
<span class="line-added">+       // Mapping has failed (probably due to ASLR). Let&#39;s map at an address chosen</span>
<span class="line-added">+       // by the OS.</span>
<span class="line-added">+       log_info(cds)(&quot;Try to map archive(s) at an alternative address&quot;);</span>
<span class="line-added">+       result = map_archives(static_mapinfo, dynamic_mapinfo, false);</span>
<span class="line-added">+     }</span>
    }
  
<span class="line-modified">!   if (result == MAP_ARCHIVE_SUCCESS) {</span>
<span class="line-modified">!     bool dynamic_mapped = (dynamic_mapinfo != NULL &amp;&amp; dynamic_mapinfo-&gt;is_mapped());</span>
<span class="line-modified">!     char* cds_base = static_mapinfo-&gt;mapped_base();</span>
<span class="line-modified">!     char* cds_end =  dynamic_mapped ? dynamic_mapinfo-&gt;mapped_end() : static_mapinfo-&gt;mapped_end();</span>
<span class="line-added">+     set_shared_metaspace_range(cds_base, static_mapinfo-&gt;mapped_end(), cds_end);</span>
<span class="line-added">+     _relocation_delta = static_mapinfo-&gt;relocation_delta();</span>
<span class="line-added">+     if (dynamic_mapped) {</span>
<span class="line-added">+       FileMapInfo::set_shared_path_table(dynamic_mapinfo);</span>
      } else {
<span class="line-modified">!       FileMapInfo::set_shared_path_table(static_mapinfo);</span>
<span class="line-modified">!     }</span>
<span class="line-modified">!   } else {</span>
<span class="line-modified">!     set_shared_metaspace_range(NULL, NULL, NULL);</span>
<span class="line-modified">!     UseSharedSpaces = false;</span>
<span class="line-added">+     FileMapInfo::fail_continue(&quot;Unable to map shared spaces&quot;);</span>
<span class="line-added">+     if (PrintSharedArchiveAndExit) {</span>
<span class="line-added">+       vm_exit_during_initialization(&quot;Unable to use shared archive.&quot;);</span>
      }
    }
  
<span class="line-modified">!   if (static_mapinfo != NULL &amp;&amp; !static_mapinfo-&gt;is_mapped()) {</span>
<span class="line-modified">!     delete static_mapinfo;</span>
<span class="line-modified">!   }</span>
<span class="line-modified">!   if (dynamic_mapinfo != NULL &amp;&amp; !dynamic_mapinfo-&gt;is_mapped()) {</span>
<span class="line-modified">!     delete dynamic_mapinfo;</span>
    }
<span class="line-added">+ }</span>
  
<span class="line-modified">! FileMapInfo* MetaspaceShared::open_static_archive() {</span>
<span class="line-modified">!   FileMapInfo* mapinfo = new FileMapInfo(true);</span>
<span class="line-added">+   if (!mapinfo-&gt;initialize()) {</span>
<span class="line-added">+     delete(mapinfo);</span>
<span class="line-added">+     return NULL;</span>
<span class="line-added">+   }</span>
<span class="line-added">+   return mapinfo;</span>
<span class="line-added">+ }</span>
  
<span class="line-modified">! FileMapInfo* MetaspaceShared::open_dynamic_archive() {</span>
<span class="line-modified">!   if (DynamicDumpSharedSpaces) {</span>
<span class="line-modified">!     return NULL;</span>
<span class="line-added">+   }</span>
<span class="line-added">+   if (Arguments::GetSharedDynamicArchivePath() == NULL) {</span>
<span class="line-added">+     return NULL;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   FileMapInfo* mapinfo = new FileMapInfo(false);</span>
<span class="line-added">+   if (!mapinfo-&gt;initialize()) {</span>
<span class="line-added">+     delete(mapinfo);</span>
<span class="line-added">+     return NULL;</span>
<span class="line-added">+   }</span>
<span class="line-added">+   return mapinfo;</span>
  }
  
<span class="line-modified">! // use_requested_addr:</span>
<span class="line-modified">! //  true  = map at FileMapHeader::_requested_base_address</span>
<span class="line-modified">! //  false = map at an alternative address picked by OS.</span>
<span class="line-added">+ MapArchiveResult MetaspaceShared::map_archives(FileMapInfo* static_mapinfo, FileMapInfo* dynamic_mapinfo,</span>
<span class="line-added">+                                                bool use_requested_addr) {</span>
<span class="line-added">+   PRODUCT_ONLY(if (ArchiveRelocationMode == 1 &amp;&amp; use_requested_addr) {</span>
<span class="line-added">+       // For product build only -- this is for benchmarking the cost of doing relocation.</span>
<span class="line-added">+       // For debug builds, the check is done in FileMapInfo::map_regions for better test coverage.</span>
<span class="line-added">+       log_info(cds)(&quot;ArchiveRelocationMode == 1: always map archive(s) at an alternative address&quot;);</span>
<span class="line-added">+       return MAP_ARCHIVE_MMAP_FAILURE;</span>
<span class="line-added">+     });</span>
<span class="line-added">+ </span>
<span class="line-added">+   if (ArchiveRelocationMode == 2 &amp;&amp; !use_requested_addr) {</span>
<span class="line-added">+     log_info(cds)(&quot;ArchiveRelocationMode == 2: never map archive(s) at an alternative address&quot;);</span>
<span class="line-added">+     return MAP_ARCHIVE_MMAP_FAILURE;</span>
<span class="line-added">+   };</span>
<span class="line-added">+ </span>
<span class="line-added">+   if (dynamic_mapinfo != NULL) {</span>
<span class="line-added">+     // Ensure that the OS won&#39;t be able to allocate new memory spaces between the two</span>
<span class="line-added">+     // archives, or else it would mess up the simple comparision in MetaspaceObj::is_shared().</span>
<span class="line-added">+     assert(static_mapinfo-&gt;mapping_end_offset() == dynamic_mapinfo-&gt;mapping_base_offset(), &quot;no gap&quot;);</span>
    }
<span class="line-modified">! </span>
<span class="line-added">+   ReservedSpace main_rs, archive_space_rs, class_space_rs;</span>
<span class="line-added">+   MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;</span>
<span class="line-added">+   char* mapped_base_address = reserve_address_space_for_archives(static_mapinfo, dynamic_mapinfo,</span>
<span class="line-added">+                                                                  use_requested_addr, main_rs, archive_space_rs,</span>
<span class="line-added">+                                                                  class_space_rs);</span>
<span class="line-added">+   if (mapped_base_address == NULL) {</span>
<span class="line-added">+     result = MAP_ARCHIVE_MMAP_FAILURE;</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     log_debug(cds)(&quot;Reserved archive_space_rs     [&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;] (&quot; SIZE_FORMAT &quot;) bytes&quot;,</span>
<span class="line-added">+                    p2i(archive_space_rs.base()), p2i(archive_space_rs.end()), archive_space_rs.size());</span>
<span class="line-added">+     log_debug(cds)(&quot;Reserved class_space_rs [&quot; INTPTR_FORMAT &quot; - &quot; INTPTR_FORMAT &quot;] (&quot; SIZE_FORMAT &quot;) bytes&quot;,</span>
<span class="line-added">+                    p2i(class_space_rs.base()), p2i(class_space_rs.end()), class_space_rs.size());</span>
<span class="line-added">+     MapArchiveResult static_result = map_archive(static_mapinfo, mapped_base_address, archive_space_rs);</span>
<span class="line-added">+     MapArchiveResult dynamic_result = (static_result == MAP_ARCHIVE_SUCCESS) ?</span>
<span class="line-added">+                                      map_archive(dynamic_mapinfo, mapped_base_address, archive_space_rs) : MAP_ARCHIVE_OTHER_FAILURE;</span>
<span class="line-added">+ </span>
<span class="line-added">+     DEBUG_ONLY(if (ArchiveRelocationMode == 1 &amp;&amp; use_requested_addr) {</span>
<span class="line-added">+       // This is for simulating mmap failures at the requested address. In debug builds, we do it</span>
<span class="line-added">+       // here (after all archives have possibly been mapped), so we can thoroughly test the code for</span>
<span class="line-added">+       // failure handling (releasing all allocated resource, etc).</span>
<span class="line-added">+       log_info(cds)(&quot;ArchiveRelocationMode == 1: always map archive(s) at an alternative address&quot;);</span>
<span class="line-added">+       if (static_result == MAP_ARCHIVE_SUCCESS) {</span>
<span class="line-added">+         static_result = MAP_ARCHIVE_MMAP_FAILURE;</span>
<span class="line-added">+       }</span>
<span class="line-added">+       if (dynamic_result == MAP_ARCHIVE_SUCCESS) {</span>
<span class="line-added">+         dynamic_result = MAP_ARCHIVE_MMAP_FAILURE;</span>
<span class="line-added">+       }</span>
<span class="line-added">+     });</span>
<span class="line-added">+ </span>
<span class="line-added">+     if (static_result == MAP_ARCHIVE_SUCCESS) {</span>
<span class="line-added">+       if (dynamic_result == MAP_ARCHIVE_SUCCESS) {</span>
<span class="line-added">+         result = MAP_ARCHIVE_SUCCESS;</span>
<span class="line-added">+       } else if (dynamic_result == MAP_ARCHIVE_OTHER_FAILURE) {</span>
<span class="line-added">+         assert(dynamic_mapinfo != NULL &amp;&amp; !dynamic_mapinfo-&gt;is_mapped(), &quot;must have failed&quot;);</span>
<span class="line-added">+         // No need to retry mapping the dynamic archive again, as it will never succeed</span>
<span class="line-added">+         // (bad file, etc) -- just keep the base archive.</span>
<span class="line-added">+         log_warning(cds, dynamic)(&quot;Unable to use shared archive. The top archive failed to load: %s&quot;,</span>
<span class="line-added">+                                   dynamic_mapinfo-&gt;full_path());</span>
<span class="line-added">+         result = MAP_ARCHIVE_SUCCESS;</span>
<span class="line-added">+         // TODO, we can give the unused space for the dynamic archive to class_space_rs, but there&#39;s no</span>
<span class="line-added">+         // easy API to do that right now.</span>
<span class="line-added">+       } else {</span>
<span class="line-added">+         result = MAP_ARCHIVE_MMAP_FAILURE;</span>
<span class="line-added">+       }</span>
<span class="line-added">+     } else if (static_result == MAP_ARCHIVE_OTHER_FAILURE) {</span>
<span class="line-added">+       result = MAP_ARCHIVE_OTHER_FAILURE;</span>
<span class="line-added">+     } else {</span>
<span class="line-added">+       result = MAP_ARCHIVE_MMAP_FAILURE;</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   if (result == MAP_ARCHIVE_SUCCESS) {</span>
<span class="line-added">+     if (!main_rs.is_reserved() &amp;&amp; class_space_rs.is_reserved()) {</span>
<span class="line-added">+       MemTracker::record_virtual_memory_type((address)class_space_rs.base(), mtClass);</span>
<span class="line-added">+     }</span>
<span class="line-added">+     SharedBaseAddress = (size_t)mapped_base_address;</span>
<span class="line-added">+     LP64_ONLY({</span>
<span class="line-added">+         if (Metaspace::using_class_space()) {</span>
<span class="line-added">+           assert(class_space_rs.is_reserved(), &quot;must be&quot;);</span>
<span class="line-added">+           char* cds_base = static_mapinfo-&gt;mapped_base();</span>
<span class="line-added">+           Metaspace::allocate_metaspace_compressed_klass_ptrs(class_space_rs, NULL, (address)cds_base);</span>
<span class="line-added">+           // map_heap_regions() compares the current narrow oop and klass encodings</span>
<span class="line-added">+           // with the archived ones, so it must be done after all encodings are determined.</span>
<span class="line-added">+           static_mapinfo-&gt;map_heap_regions();</span>
<span class="line-added">+           CompressedKlassPointers::set_range(CompressedClassSpaceSize);</span>
<span class="line-added">+         }</span>
<span class="line-added">+       });</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     unmap_archive(static_mapinfo);</span>
<span class="line-added">+     unmap_archive(dynamic_mapinfo);</span>
<span class="line-added">+     release_reserved_spaces(main_rs, archive_space_rs, class_space_rs);</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   return result;</span>
  }
  
<span class="line-modified">! char* MetaspaceShared::reserve_address_space_for_archives(FileMapInfo* static_mapinfo,</span>
<span class="line-modified">!                                                           FileMapInfo* dynamic_mapinfo,</span>
<span class="line-modified">!                                                           bool use_requested_addr,</span>
<span class="line-added">+                                                           ReservedSpace&amp; main_rs,</span>
<span class="line-added">+                                                           ReservedSpace&amp; archive_space_rs,</span>
<span class="line-added">+                                                           ReservedSpace&amp; class_space_rs) {</span>
<span class="line-added">+   const bool use_klass_space = NOT_LP64(false) LP64_ONLY(Metaspace::using_class_space());</span>
<span class="line-added">+   const size_t class_space_size = NOT_LP64(0) LP64_ONLY(Metaspace::compressed_class_space_size());</span>
  
<span class="line-modified">!   if (use_klass_space) {</span>
<span class="line-modified">!     assert(class_space_size &gt; 0, &quot;CompressedClassSpaceSize must have been validated&quot;);</span>
<span class="line-modified">!   }</span>
<span class="line-modified">!   if (use_requested_addr &amp;&amp; !is_aligned(static_mapinfo-&gt;requested_base_address(), reserved_space_alignment())) {</span>
<span class="line-modified">!     return NULL;</span>
<span class="line-modified">!   }</span>
  
<span class="line-modified">!   // Size and requested location of the archive_space_rs (for both static and dynamic archives)</span>
<span class="line-modified">!   size_t base_offset = static_mapinfo-&gt;mapping_base_offset();</span>
<span class="line-modified">!   size_t end_offset  = (dynamic_mapinfo == NULL) ? static_mapinfo-&gt;mapping_end_offset() : dynamic_mapinfo-&gt;mapping_end_offset();</span>
<span class="line-modified">!   assert(base_offset == 0, &quot;must be&quot;);</span>
<span class="line-modified">!   assert(is_aligned(end_offset,  os::vm_allocation_granularity()), &quot;must be&quot;);</span>
<span class="line-modified">!   assert(is_aligned(base_offset, os::vm_allocation_granularity()), &quot;must be&quot;);</span>
<span class="line-modified">! </span>
<span class="line-modified">!   // In case reserved_space_alignment() != os::vm_allocation_granularity()</span>
<span class="line-modified">!   assert((size_t)os::vm_allocation_granularity() &lt;= reserved_space_alignment(), &quot;must be&quot;);</span>
<span class="line-modified">!   end_offset = align_up(end_offset, reserved_space_alignment());</span>
<span class="line-modified">! </span>
<span class="line-modified">!   size_t archive_space_size = end_offset - base_offset;</span>
<span class="line-modified">! </span>
<span class="line-modified">!   // Special handling for Windows because it cannot mmap into a reserved space:</span>
<span class="line-modified">!   //    use_requested_addr: We just map each region individually, and give up if any one of them fails.</span>
<span class="line-modified">!   //   !use_requested_addr: We reserve the space first, and then os::read in all the regions (instead of mmap).</span>
<span class="line-modified">!   //                        We&#39;re going to patch all the pointers anyway so there&#39;s no benefit for mmap.</span>
<span class="line-modified">! </span>
<span class="line-modified">!   if (use_requested_addr) {</span>
<span class="line-modified">!     char* archive_space_base = static_mapinfo-&gt;requested_base_address() + base_offset;</span>
<span class="line-modified">!     char* archive_space_end  = archive_space_base + archive_space_size;</span>
<span class="line-modified">!     if (!MetaspaceShared::use_windows_memory_mapping()) {</span>
<span class="line-modified">!       archive_space_rs = reserve_shared_space(archive_space_size, archive_space_base);</span>
<span class="line-modified">!       if (!archive_space_rs.is_reserved()) {</span>
<span class="line-modified">!         return NULL;</span>
<span class="line-modified">!       }</span>
<span class="line-modified">!     }</span>
<span class="line-modified">!     if (use_klass_space) {</span>
<span class="line-modified">!       // Make sure we can map the klass space immediately following the archive_space space</span>
<span class="line-modified">!       // Don&#39;t call reserve_shared_space here as that may try to enforce platform-specific</span>
<span class="line-modified">!       // alignment rules which only apply to the archive base address</span>
<span class="line-added">+       char* class_space_base = archive_space_end;</span>
<span class="line-added">+       class_space_rs = ReservedSpace(class_space_size, reserved_space_alignment(),</span>
<span class="line-added">+                                      false /* large_pages */, class_space_base);</span>
<span class="line-added">+       if (!class_space_rs.is_reserved()) {</span>
<span class="line-added">+         return NULL;</span>
<span class="line-added">+       }</span>
<span class="line-added">+     }</span>
<span class="line-added">+     return static_mapinfo-&gt;requested_base_address();</span>
    } else {
<span class="line-modified">!     if (use_klass_space) {</span>
<span class="line-modified">!       main_rs = reserve_shared_space(archive_space_size + class_space_size);</span>
<span class="line-modified">!       if (main_rs.is_reserved()) {</span>
<span class="line-modified">!         archive_space_rs = main_rs.first_part(archive_space_size, reserved_space_alignment(), /*split=*/true);</span>
<span class="line-modified">!         class_space_rs = main_rs.last_part(archive_space_size);</span>
<span class="line-modified">!       }</span>
      } else {
<span class="line-modified">!       main_rs = reserve_shared_space(archive_space_size);</span>
<span class="line-added">+       archive_space_rs = main_rs;</span>
      }
<span class="line-modified">!     if (archive_space_rs.is_reserved()) {</span>
<span class="line-added">+       return archive_space_rs.base();</span>
<span class="line-added">+     } else {</span>
<span class="line-added">+       return NULL;</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ void MetaspaceShared::release_reserved_spaces(ReservedSpace&amp; main_rs,</span>
<span class="line-added">+                                               ReservedSpace&amp; archive_space_rs,</span>
<span class="line-added">+                                               ReservedSpace&amp; class_space_rs) {</span>
<span class="line-added">+   if (main_rs.is_reserved()) {</span>
<span class="line-added">+     assert(main_rs.contains(archive_space_rs.base()), &quot;must be&quot;);</span>
<span class="line-added">+     assert(main_rs.contains(class_space_rs.base()), &quot;must be&quot;);</span>
<span class="line-added">+     log_debug(cds)(&quot;Released shared space (archive+classes) &quot; INTPTR_FORMAT, p2i(main_rs.base()));</span>
<span class="line-added">+     main_rs.release();</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     if (archive_space_rs.is_reserved()) {</span>
<span class="line-added">+       log_debug(cds)(&quot;Released shared space (archive) &quot; INTPTR_FORMAT, p2i(archive_space_rs.base()));</span>
<span class="line-added">+       archive_space_rs.release();</span>
<span class="line-added">+     }</span>
<span class="line-added">+     if (class_space_rs.is_reserved()) {</span>
<span class="line-added">+       log_debug(cds)(&quot;Released shared space (classes) &quot; INTPTR_FORMAT, p2i(class_space_rs.base()));</span>
<span class="line-added">+       class_space_rs.release();</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ static int archive_regions[]  = {MetaspaceShared::mc,</span>
<span class="line-added">+                                  MetaspaceShared::rw,</span>
<span class="line-added">+                                  MetaspaceShared::ro};</span>
<span class="line-added">+ static int archive_regions_count  = 3;</span>
<span class="line-added">+ </span>
<span class="line-added">+ MapArchiveResult MetaspaceShared::map_archive(FileMapInfo* mapinfo, char* mapped_base_address, ReservedSpace rs) {</span>
<span class="line-added">+   assert(UseSharedSpaces, &quot;must be runtime&quot;);</span>
<span class="line-added">+   if (mapinfo == NULL) {</span>
<span class="line-added">+     return MAP_ARCHIVE_SUCCESS; // The dynamic archive has not been specified. No error has happened -- trivially succeeded.</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   mapinfo-&gt;set_is_mapped(false);</span>
<span class="line-added">+ </span>
<span class="line-added">+   if (mapinfo-&gt;alignment() != (size_t)os::vm_allocation_granularity()) {</span>
<span class="line-added">+     log_error(cds)(&quot;Unable to map CDS archive -- os::vm_allocation_granularity() expected: &quot; SIZE_FORMAT</span>
<span class="line-added">+                    &quot; actual: %d&quot;, mapinfo-&gt;alignment(), os::vm_allocation_granularity());</span>
<span class="line-added">+     return MAP_ARCHIVE_OTHER_FAILURE;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   MapArchiveResult result =</span>
<span class="line-added">+     mapinfo-&gt;map_regions(archive_regions, archive_regions_count, mapped_base_address, rs);</span>
<span class="line-added">+ </span>
<span class="line-added">+   if (result != MAP_ARCHIVE_SUCCESS) {</span>
<span class="line-added">+     unmap_archive(mapinfo);</span>
<span class="line-added">+     return result;</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   if (mapinfo-&gt;is_static()) {</span>
<span class="line-added">+     if (!mapinfo-&gt;validate_shared_path_table()) {</span>
<span class="line-added">+       unmap_archive(mapinfo);</span>
<span class="line-added">+       return MAP_ARCHIVE_OTHER_FAILURE;</span>
<span class="line-added">+     }</span>
<span class="line-added">+   } else {</span>
<span class="line-added">+     if (!DynamicArchive::validate(mapinfo)) {</span>
<span class="line-added">+       unmap_archive(mapinfo);</span>
<span class="line-added">+       return MAP_ARCHIVE_OTHER_FAILURE;</span>
<span class="line-added">+     }</span>
<span class="line-added">+   }</span>
<span class="line-added">+ </span>
<span class="line-added">+   mapinfo-&gt;set_is_mapped(true);</span>
<span class="line-added">+   return MAP_ARCHIVE_SUCCESS;</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ void MetaspaceShared::unmap_archive(FileMapInfo* mapinfo) {</span>
<span class="line-added">+   assert(UseSharedSpaces, &quot;must be runtime&quot;);</span>
<span class="line-added">+   if (mapinfo != NULL) {</span>
<span class="line-added">+     mapinfo-&gt;unmap_regions(archive_regions, archive_regions_count);</span>
<span class="line-added">+     mapinfo-&gt;set_is_mapped(false);</span>
    }
  }
  
  // Read the miscellaneous data from the shared file, and
  // serialize it out to its various destinations.
  
  void MetaspaceShared::initialize_shared_spaces() {
<span class="line-modified">!   FileMapInfo *static_mapinfo = FileMapInfo::current_info();</span>
<span class="line-modified">!   _i2i_entry_code_buffers = static_mapinfo-&gt;i2i_entry_code_buffers();</span>
<span class="line-modified">!   _i2i_entry_code_buffers_size = static_mapinfo-&gt;i2i_entry_code_buffers_size();</span>
<span class="line-modified">!   char* buffer = static_mapinfo-&gt;cloned_vtables();</span>
    clone_cpp_vtables((intptr_t*)buffer);
  
    // Verify various attributes of the archive, plus initialize the
    // shared string/symbol tables
<span class="line-added">+   buffer = static_mapinfo-&gt;serialized_data();</span>
    intptr_t* array = (intptr_t*)buffer;
    ReadClosure rc(&amp;array);
    serialize(&amp;rc);
  
    // Initialize the run-time symbol table.
    SymbolTable::create_table();
  
<span class="line-modified">!   static_mapinfo-&gt;patch_archived_heap_embedded_pointers();</span>
  
    // Close the mapinfo file
<span class="line-modified">!   static_mapinfo-&gt;close();</span>
<span class="line-added">+ </span>
<span class="line-added">+   FileMapInfo *dynamic_mapinfo = FileMapInfo::dynamic_info();</span>
<span class="line-added">+   if (dynamic_mapinfo != NULL) {</span>
<span class="line-added">+     intptr_t* buffer = (intptr_t*)dynamic_mapinfo-&gt;serialized_data();</span>
<span class="line-added">+     ReadClosure rc(&amp;buffer);</span>
<span class="line-added">+     SymbolTable::serialize_shared_table_header(&amp;rc, false);</span>
<span class="line-added">+     SystemDictionaryShared::serialize_dictionary_headers(&amp;rc, false);</span>
<span class="line-added">+     dynamic_mapinfo-&gt;close();</span>
<span class="line-added">+   }</span>
  
    if (PrintSharedArchiveAndExit) {
      if (PrintSharedDictionary) {
        tty-&gt;print_cr(&quot;\nShared classes:\n&quot;);
        SystemDictionaryShared::print_on(tty);
      }
<span class="line-modified">!     if (FileMapInfo::current_info() == NULL || _archive_loading_failed) {</span>
        tty-&gt;print_cr(&quot;archive is invalid&quot;);
        vm_exit(1);
      } else {
        tty-&gt;print_cr(&quot;archive is valid&quot;);
        vm_exit(0);
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2005,10 ***</span>
<span class="line-new-header">--- 2393,16 ---</span>
      // remap the shared readonly space to shared readwrite, private
      FileMapInfo* mapinfo = FileMapInfo::current_info();
      if (!mapinfo-&gt;remap_shared_readonly_as_readwrite()) {
        return false;
      }
<span class="line-added">+     if (FileMapInfo::dynamic_info() != NULL) {</span>
<span class="line-added">+       mapinfo = FileMapInfo::dynamic_info();</span>
<span class="line-added">+       if (!mapinfo-&gt;remap_shared_readonly_as_readwrite()) {</span>
<span class="line-added">+         return false;</span>
<span class="line-added">+       }</span>
<span class="line-added">+     }</span>
      _remapped_readwrite = true;
    }
    return true;
  }
  
</pre>
<hr />
<pre>
<span class="line-old-header">*** 2017,10 ***</span>
    // On 32-bit we reserve only 256MB so you could run out of space with 100,000 classes
    // or so.
    _mc_region.print_out_of_space_msg(name, needed_bytes);
    _rw_region.print_out_of_space_msg(name, needed_bytes);
    _ro_region.print_out_of_space_msg(name, needed_bytes);
<span class="line-removed">-   _md_region.print_out_of_space_msg(name, needed_bytes);</span>
  
    vm_exit_during_initialization(err_msg(&quot;Unable to allocate from &#39;%s&#39; region&quot;, name),
                                  &quot;Please reduce the number of shared classes.&quot;);
  }
<span class="line-new-header">--- 2411,16 ---</span>
    // On 32-bit we reserve only 256MB so you could run out of space with 100,000 classes
    // or so.
    _mc_region.print_out_of_space_msg(name, needed_bytes);
    _rw_region.print_out_of_space_msg(name, needed_bytes);
    _ro_region.print_out_of_space_msg(name, needed_bytes);
  
    vm_exit_during_initialization(err_msg(&quot;Unable to allocate from &#39;%s&#39; region&quot;, name),
                                  &quot;Please reduce the number of shared classes.&quot;);
  }
<span class="line-added">+ </span>
<span class="line-added">+ // This is used to relocate the pointers so that the archive can be mapped at</span>
<span class="line-added">+ // Arguments::default_SharedBaseAddress() without runtime relocation.</span>
<span class="line-added">+ intx MetaspaceShared::final_delta() {</span>
<span class="line-added">+   return intx(Arguments::default_SharedBaseAddress())  // We want the archive to be mapped to here at runtime</span>
<span class="line-added">+        - intx(SharedBaseAddress);                      // .. but the archive is mapped at here at dump time</span>
<span class="line-added">+ }</span>
</pre>
<center><a href="metaspaceClosure.hpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="metaspaceShared.hpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>