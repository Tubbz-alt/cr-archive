<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/services/mallocSiteTable.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="lowMemoryDetector.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="mallocSiteTable.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/services/mallocSiteTable.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
111  *  This method should not return NULL under normal circumstance.
112  *  If NULL is returned, it indicates:
113  *    1. Out of memory, it cannot allocate new hash entry.
114  *    2. Overflow hash bucket.
115  *  Under any of above circumstances, caller should handle the situation.
116  */
117 MallocSite* MallocSiteTable::lookup_or_add(const NativeCallStack&amp; key, size_t* bucket_idx,
118   size_t* pos_idx, MEMFLAGS flags) {
119   assert(flags != mtNone, &quot;Should have a real memory type&quot;);
120   unsigned int index = hash_to_index(key.hash());
121   *bucket_idx = (size_t)index;
122   *pos_idx = 0;
123 
124   // First entry for this hash bucket
125   if (_table[index] == NULL) {
126     MallocSiteHashtableEntry* entry = new_entry(key, flags);
127     // OOM check
128     if (entry == NULL) return NULL;
129 
130     // swap in the head
<span class="line-modified">131     if (Atomic::replace_if_null(entry, &amp;_table[index])) {</span>
132       return entry-&gt;data();
133     }
134 
135     delete entry;
136   }
137 
138   MallocSiteHashtableEntry* head = _table[index];
139   while (head != NULL &amp;&amp; (*pos_idx) &lt;= MAX_BUCKET_LENGTH) {
140     MallocSite* site = head-&gt;data();
141     if (site-&gt;flag() == flags &amp;&amp; site-&gt;equals(key)) {
142       return head-&gt;data();
143     }
144 
145     if (head-&gt;next() == NULL &amp;&amp; (*pos_idx) &lt; MAX_BUCKET_LENGTH) {
146       MallocSiteHashtableEntry* entry = new_entry(key, flags);
147       // OOM check
148       if (entry == NULL) return NULL;
149       if (head-&gt;atomic_insert(entry)) {
150         (*pos_idx) ++;
151         return entry-&gt;data();
</pre>
<hr />
<pre>
212   AccessLock locker(&amp;_access_count);
213   if (locker.sharedLock()) {
214     NOT_PRODUCT(_peak_count = MAX2(_peak_count, _access_count);)
215     return walk(walker);
216   }
217   return false;
218 }
219 
220 
221 void MallocSiteTable::AccessLock::exclusiveLock() {
222   int target;
223   int val;
224 
225   assert(_lock_state != ExclusiveLock, &quot;Can only call once&quot;);
226   assert(*_lock &gt;= 0, &quot;Can not content exclusive lock&quot;);
227 
228   // make counter negative to block out shared locks
229   do {
230     val = *_lock;
231     target = _MAGIC_ + *_lock;
<span class="line-modified">232   } while (Atomic::cmpxchg(target, _lock, val) != val);</span>
233 
234   // wait for all readers to exit
235   while (*_lock != _MAGIC_) {
236 #ifdef _WINDOWS
237     os::naked_short_sleep(1);
238 #else
239     os::naked_yield();
240 #endif
241   }
242   _lock_state = ExclusiveLock;
243 }
244 
245 bool MallocSiteHashtableEntry::atomic_insert(MallocSiteHashtableEntry* entry) {
<span class="line-modified">246   return Atomic::replace_if_null(entry, &amp;_next);</span>
247 }
</pre>
</td>
<td>
<hr />
<pre>
111  *  This method should not return NULL under normal circumstance.
112  *  If NULL is returned, it indicates:
113  *    1. Out of memory, it cannot allocate new hash entry.
114  *    2. Overflow hash bucket.
115  *  Under any of above circumstances, caller should handle the situation.
116  */
117 MallocSite* MallocSiteTable::lookup_or_add(const NativeCallStack&amp; key, size_t* bucket_idx,
118   size_t* pos_idx, MEMFLAGS flags) {
119   assert(flags != mtNone, &quot;Should have a real memory type&quot;);
120   unsigned int index = hash_to_index(key.hash());
121   *bucket_idx = (size_t)index;
122   *pos_idx = 0;
123 
124   // First entry for this hash bucket
125   if (_table[index] == NULL) {
126     MallocSiteHashtableEntry* entry = new_entry(key, flags);
127     // OOM check
128     if (entry == NULL) return NULL;
129 
130     // swap in the head
<span class="line-modified">131     if (Atomic::replace_if_null(&amp;_table[index], entry)) {</span>
132       return entry-&gt;data();
133     }
134 
135     delete entry;
136   }
137 
138   MallocSiteHashtableEntry* head = _table[index];
139   while (head != NULL &amp;&amp; (*pos_idx) &lt;= MAX_BUCKET_LENGTH) {
140     MallocSite* site = head-&gt;data();
141     if (site-&gt;flag() == flags &amp;&amp; site-&gt;equals(key)) {
142       return head-&gt;data();
143     }
144 
145     if (head-&gt;next() == NULL &amp;&amp; (*pos_idx) &lt; MAX_BUCKET_LENGTH) {
146       MallocSiteHashtableEntry* entry = new_entry(key, flags);
147       // OOM check
148       if (entry == NULL) return NULL;
149       if (head-&gt;atomic_insert(entry)) {
150         (*pos_idx) ++;
151         return entry-&gt;data();
</pre>
<hr />
<pre>
212   AccessLock locker(&amp;_access_count);
213   if (locker.sharedLock()) {
214     NOT_PRODUCT(_peak_count = MAX2(_peak_count, _access_count);)
215     return walk(walker);
216   }
217   return false;
218 }
219 
220 
221 void MallocSiteTable::AccessLock::exclusiveLock() {
222   int target;
223   int val;
224 
225   assert(_lock_state != ExclusiveLock, &quot;Can only call once&quot;);
226   assert(*_lock &gt;= 0, &quot;Can not content exclusive lock&quot;);
227 
228   // make counter negative to block out shared locks
229   do {
230     val = *_lock;
231     target = _MAGIC_ + *_lock;
<span class="line-modified">232   } while (Atomic::cmpxchg(_lock, val, target) != val);</span>
233 
234   // wait for all readers to exit
235   while (*_lock != _MAGIC_) {
236 #ifdef _WINDOWS
237     os::naked_short_sleep(1);
238 #else
239     os::naked_yield();
240 #endif
241   }
242   _lock_state = ExclusiveLock;
243 }
244 
245 bool MallocSiteHashtableEntry::atomic_insert(MallocSiteHashtableEntry* entry) {
<span class="line-modified">246   return Atomic::replace_if_null(&amp;_next, entry);</span>
247 }
</pre>
</td>
</tr>
</table>
<center><a href="lowMemoryDetector.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="mallocSiteTable.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>