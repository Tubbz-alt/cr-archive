<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/share/runtime/atomic.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_RUNTIME_ATOMIC_HPP
 26 #define SHARE_RUNTIME_ATOMIC_HPP
 27 
 28 #include &quot;memory/allocation.hpp&quot;
 29 #include &quot;metaprogramming/conditional.hpp&quot;
 30 #include &quot;metaprogramming/enableIf.hpp&quot;
 31 #include &quot;metaprogramming/isIntegral.hpp&quot;
 32 #include &quot;metaprogramming/isPointer.hpp&quot;
 33 #include &quot;metaprogramming/isSame.hpp&quot;
 34 #include &quot;metaprogramming/primitiveConversions.hpp&quot;
 35 #include &quot;metaprogramming/removeCV.hpp&quot;
 36 #include &quot;metaprogramming/removePointer.hpp&quot;
 37 #include &quot;runtime/orderAccess.hpp&quot;
 38 #include &quot;utilities/align.hpp&quot;
 39 #include &quot;utilities/bytes.hpp&quot;
 40 #include &quot;utilities/macros.hpp&quot;
 41 
 42 enum atomic_memory_order {
 43   // The modes that align with C++11 are intended to
 44   // follow the same semantics.
 45   memory_order_relaxed = 0,
 46   memory_order_acquire = 2,
 47   memory_order_release = 3,
 48   memory_order_acq_rel = 4,
 49   // Strong two-way memory barrier.
 50   memory_order_conservative = 8
 51 };
 52 
 53 enum ScopedFenceType {
 54     X_ACQUIRE
 55   , RELEASE_X
 56   , RELEASE_X_FENCE
 57 };
 58 
 59 class Atomic : AllStatic {
 60 public:
 61   // Atomic operations on int64 types are not available on all 32-bit
 62   // platforms. If atomic ops on int64 are defined here they must only
 63   // be used from code that verifies they are available at runtime and
 64   // can provide an alternative action if not - see supports_cx8() for
 65   // a means to test availability.
 66 
 67   // The memory operations that are mentioned with each of the atomic
 68   // function families come from src/share/vm/runtime/orderAccess.hpp,
 69   // e.g., &lt;fence&gt; is described in that file and is implemented by the
 70   // OrderAccess::fence() function. See that file for the gory details
 71   // on the Memory Access Ordering Model.
 72 
 73   // All of the atomic operations that imply a read-modify-write action
 74   // guarantee a two-way memory barrier across that operation. Historically
 75   // these semantics reflect the strength of atomic operations that are
 76   // provided on SPARC/X86. We assume that strength is necessary unless
 77   // we can prove that a weaker form is sufficiently safe.
 78 
 79   // Atomically store to a location
 80   // The type T must be either a pointer type convertible to or equal
 81   // to D, an integral/enum type equal to D, or a type equal to D that
 82   // is primitive convertible using PrimitiveConversions.
 83   template&lt;typename D, typename T&gt;
 84   inline static void store(volatile D* dest, T store_value);
 85 
 86   template &lt;typename D, typename T&gt;
 87   inline static void release_store(volatile D* dest, T store_value);
 88 
 89   template &lt;typename D, typename T&gt;
 90   inline static void release_store_fence(volatile D* dest, T store_value);
 91 
 92   // Atomically load from a location
 93   // The type T must be either a pointer type, an integral/enum type,
 94   // or a type that is primitive convertible using PrimitiveConversions.
 95   template&lt;typename T&gt;
 96   inline static T load(const volatile T* dest);
 97 
 98   template &lt;typename T&gt;
 99   inline static T load_acquire(const volatile T* dest);
100 
101   // Atomically add to a location. *add*() provide:
102   // &lt;fence&gt; add-value-to-dest &lt;membar StoreLoad|StoreStore&gt;
103 
104   // Returns updated value.
105   template&lt;typename D, typename I&gt;
106   inline static D add(D volatile* dest, I add_value,
107                       atomic_memory_order order = memory_order_conservative);
108 
109   // Returns previous value.
110   template&lt;typename D, typename I&gt;
111   inline static D fetch_and_add(D volatile* dest, I add_value,
112                                 atomic_memory_order order = memory_order_conservative);
113 
114   template&lt;typename D, typename I&gt;
115   inline static D sub(D volatile* dest, I sub_value,
116                       atomic_memory_order order = memory_order_conservative);
117 
118   // Atomically increment location. inc() provide:
119   // &lt;fence&gt; increment-dest &lt;membar StoreLoad|StoreStore&gt;
120   // The type D may be either a pointer type, or an integral
121   // type. If it is a pointer type, then the increment is
122   // scaled to the size of the type pointed to by the pointer.
123   template&lt;typename D&gt;
124   inline static void inc(D volatile* dest,
125                          atomic_memory_order order = memory_order_conservative);
126 
127   // Atomically decrement a location. dec() provide:
128   // &lt;fence&gt; decrement-dest &lt;membar StoreLoad|StoreStore&gt;
129   // The type D may be either a pointer type, or an integral
130   // type. If it is a pointer type, then the decrement is
131   // scaled to the size of the type pointed to by the pointer.
132   template&lt;typename D&gt;
133   inline static void dec(D volatile* dest,
134                          atomic_memory_order order = memory_order_conservative);
135 
136   // Performs atomic exchange of *dest with exchange_value. Returns old
137   // prior value of *dest. xchg*() provide:
138   // &lt;fence&gt; exchange-value-with-dest &lt;membar StoreLoad|StoreStore&gt;
139   // The type T must be either a pointer type convertible to or equal
140   // to D, an integral/enum type equal to D, or a type equal to D that
141   // is primitive convertible using PrimitiveConversions.
142   template&lt;typename D, typename T&gt;
143   inline static D xchg(volatile D* dest, T exchange_value,
144                        atomic_memory_order order = memory_order_conservative);
145 
146   // Performs atomic compare of *dest and compare_value, and exchanges
147   // *dest with exchange_value if the comparison succeeded. Returns prior
148   // value of *dest. cmpxchg*() provide:
149   // &lt;fence&gt; compare-and-exchange &lt;membar StoreLoad|StoreStore&gt;
150 
151   template&lt;typename D, typename U, typename T&gt;
152   inline static D cmpxchg(D volatile* dest,
153                           U compare_value,
154                           T exchange_value,
155                           atomic_memory_order order = memory_order_conservative);
156 
157   // Performs atomic compare of *dest and NULL, and replaces *dest
158   // with exchange_value if the comparison succeeded.  Returns true if
159   // the comparison succeeded and the exchange occurred.  This is
160   // often used as part of lazy initialization, as a lock-free
161   // alternative to the Double-Checked Locking Pattern.
162   template&lt;typename D, typename T&gt;
163   inline static bool replace_if_null(D* volatile* dest, T* value,
164                                      atomic_memory_order order = memory_order_conservative);
165 
166 private:
167 WINDOWS_ONLY(public:) // VS2017 warns (C2027) use of undefined type if IsPointerConvertible is declared private
168   // Test whether From is implicitly convertible to To.
169   // From and To must be pointer types.
170   // Note: Provides the limited subset of C++11 std::is_convertible
171   // that is needed here.
172   template&lt;typename From, typename To&gt; struct IsPointerConvertible;
173 
174 protected:
175   // Dispatch handler for store.  Provides type-based validity
176   // checking and limited conversions around calls to the platform-
177   // specific implementation layer provided by PlatformOp.
178   template&lt;typename D, typename T, typename PlatformOp, typename Enable = void&gt;
179   struct StoreImpl;
180 
181   // Platform-specific implementation of store.  Support for sizes
182   // of 1, 2, 4, and (if different) pointer size bytes are required.
183   // The class is a function object that must be default constructable,
184   // with these requirements:
185   //
186   // either:
187   // - dest is of type D*, an integral, enum or pointer type.
188   // - new_value are of type T, an integral, enum or pointer type D or
189   //   pointer type convertible to D.
190   // or:
191   // - T and D are the same and are primitive convertible using PrimitiveConversions
192   // and either way:
193   // - platform_store is an object of type PlatformStore&lt;sizeof(T)&gt;.
194   //
195   // Then
196   //   platform_store(new_value, dest)
197   // must be a valid expression.
198   //
199   // The default implementation is a volatile store. If a platform
200   // requires more for e.g. 64 bit stores, a specialization is required
201   template&lt;size_t byte_size&gt; struct PlatformStore;
202 
203   // Dispatch handler for load.  Provides type-based validity
204   // checking and limited conversions around calls to the platform-
205   // specific implementation layer provided by PlatformOp.
206   template&lt;typename T, typename PlatformOp, typename Enable = void&gt;
207   struct LoadImpl;
208 
209   // Platform-specific implementation of load. Support for sizes of
210   // 1, 2, 4 bytes and (if different) pointer size bytes are required.
211   // The class is a function object that must be default
212   // constructable, with these requirements:
213   //
214   // - dest is of type T*, an integral, enum or pointer type, or
215   //   T is convertible to a primitive type using PrimitiveConversions
216   // - platform_load is an object of type PlatformLoad&lt;sizeof(T)&gt;.
217   //
218   // Then
219   //   platform_load(src)
220   // must be a valid expression, returning a result convertible to T.
221   //
222   // The default implementation is a volatile load. If a platform
223   // requires more for e.g. 64 bit loads, a specialization is required
224   template&lt;size_t byte_size&gt; struct PlatformLoad;
225 
226   // Give platforms a variation point to specialize.
227   template&lt;size_t byte_size, ScopedFenceType type&gt; struct PlatformOrderedStore;
228   template&lt;size_t byte_size, ScopedFenceType type&gt; struct PlatformOrderedLoad;
229 
230 private:
231   // Dispatch handler for add.  Provides type-based validity checking
232   // and limited conversions around calls to the platform-specific
233   // implementation layer provided by PlatformAdd.
234   template&lt;typename D, typename I, typename Enable = void&gt;
235   struct AddImpl;
236 
237   // Platform-specific implementation of add.  Support for sizes of 4
238   // bytes and (if different) pointer size bytes are required.  The
239   // class must be default constructable, with these requirements:
240   //
241   // - dest is of type D*, an integral or pointer type.
242   // - add_value is of type I, an integral type.
243   // - sizeof(I) == sizeof(D).
244   // - if D is an integral type, I == D.
245   // - order is of type atomic_memory_order.
246   // - platform_add is an object of type PlatformAdd&lt;sizeof(D)&gt;.
247   //
248   // Then both
249   //   platform_add.add_and_fetch(dest, add_value, order)
250   //   platform_add.fetch_and_add(dest, add_value, order)
251   // must be valid expressions returning a result convertible to D.
252   //
253   // add_and_fetch atomically adds add_value to the value of dest,
254   // returning the new value.
255   //
256   // fetch_and_add atomically adds add_value to the value of dest,
257   // returning the old value.
258   //
259   // When D is a pointer type P*, both add_and_fetch and fetch_and_add
260   // treat it as if it were an uintptr_t; they do not perform any
261   // scaling of add_value, as that has already been done by the caller.
262   //
263   // No definition is provided; all platforms must explicitly define
264   // this class and any needed specializations.
265   template&lt;size_t byte_size&gt; struct PlatformAdd;
266 
267   // Support for platforms that implement some variants of add using a
268   // (typically out of line) non-template helper function.  The
269   // generic arguments passed to PlatformAdd need to be translated to
270   // the appropriate type for the helper function, the helper function
271   // invoked on the translated arguments, and the result translated
272   // back.  Type is the parameter / return type of the helper
273   // function.  No scaling of add_value is performed when D is a pointer
274   // type, so this function can be used to implement the support function
275   // required by AddAndFetch.
276   template&lt;typename Type, typename Fn, typename D, typename I&gt;
277   static D add_using_helper(Fn fn, D volatile* dest, I add_value);
278 
279   // Dispatch handler for cmpxchg.  Provides type-based validity
280   // checking and limited conversions around calls to the
281   // platform-specific implementation layer provided by
282   // PlatformCmpxchg.
283   template&lt;typename D, typename U, typename T, typename Enable = void&gt;
284   struct CmpxchgImpl;
285 
286   // Platform-specific implementation of cmpxchg.  Support for sizes
287   // of 1, 4, and 8 are required.  The class is a function object that
288   // must be default constructable, with these requirements:
289   //
290   // - dest is of type T*.
291   // - exchange_value and compare_value are of type T.
292   // - order is of type atomic_memory_order.
293   // - platform_cmpxchg is an object of type PlatformCmpxchg&lt;sizeof(T)&gt;.
294   //
295   // Then
296   //   platform_cmpxchg(dest, compare_value, exchange_value, order)
297   // must be a valid expression, returning a result convertible to T.
298   //
299   // A default definition is provided, which declares a function template
300   //   T operator()(T volatile*, T, T, atomic_memory_order) const
301   //
302   // For each required size, a platform must either provide an
303   // appropriate definition of that function, or must entirely
304   // specialize the class template for that size.
305   template&lt;size_t byte_size&gt; struct PlatformCmpxchg;
306 
307   // Support for platforms that implement some variants of cmpxchg
308   // using a (typically out of line) non-template helper function.
309   // The generic arguments passed to PlatformCmpxchg need to be
310   // translated to the appropriate type for the helper function, the
311   // helper invoked on the translated arguments, and the result
312   // translated back.  Type is the parameter / return type of the
313   // helper function.
314   template&lt;typename Type, typename Fn, typename T&gt;
315   static T cmpxchg_using_helper(Fn fn,
316                                 T volatile* dest,
317                                 T compare_value,
318                                 T exchange_value);
319 
320   // Support platforms that do not provide Read-Modify-Write
321   // byte-level atomic access. To use, derive PlatformCmpxchg&lt;1&gt; from
322   // this class.
323 public: // Temporary, can&#39;t be private: C++03 11.4/2. Fixed by C++11.
324   struct CmpxchgByteUsingInt;
325 private:
326 
327   // Dispatch handler for xchg.  Provides type-based validity
328   // checking and limited conversions around calls to the
329   // platform-specific implementation layer provided by
330   // PlatformXchg.
331   template&lt;typename D, typename T, typename Enable = void&gt;
332   struct XchgImpl;
333 
334   // Platform-specific implementation of xchg.  Support for sizes
335   // of 4, and sizeof(intptr_t) are required.  The class is a function
336   // object that must be default constructable, with these requirements:
337   //
338   // - dest is of type T*.
339   // - exchange_value is of type T.
340   // - platform_xchg is an object of type PlatformXchg&lt;sizeof(T)&gt;.
341   //
342   // Then
343   //   platform_xchg(dest, exchange_value)
344   // must be a valid expression, returning a result convertible to T.
345   //
346   // A default definition is provided, which declares a function template
347   //   T operator()(T volatile*, T, atomic_memory_order) const
348   //
349   // For each required size, a platform must either provide an
350   // appropriate definition of that function, or must entirely
351   // specialize the class template for that size.
352   template&lt;size_t byte_size&gt; struct PlatformXchg;
353 
354   // Support for platforms that implement some variants of xchg
355   // using a (typically out of line) non-template helper function.
356   // The generic arguments passed to PlatformXchg need to be
357   // translated to the appropriate type for the helper function, the
358   // helper invoked on the translated arguments, and the result
359   // translated back.  Type is the parameter / return type of the
360   // helper function.
361   template&lt;typename Type, typename Fn, typename T&gt;
362   static T xchg_using_helper(Fn fn,
363                              T volatile* dest,
364                              T exchange_value);
365 };
366 
367 template&lt;typename From, typename To&gt;
368 struct Atomic::IsPointerConvertible&lt;From*, To*&gt; : AllStatic {
369   // Determine whether From* is implicitly convertible to To*, using
370   // the &quot;sizeof trick&quot;.
371   typedef char yes;
372   typedef char (&amp;no)[2];
373 
374   static yes test(To*);
375   static no test(...);
376   static From* test_value;
377 
378   static const bool value = (sizeof(yes) == sizeof(test(test_value)));
379 };
380 
381 // Handle load for pointer, integral and enum types.
382 template&lt;typename T, typename PlatformOp&gt;
383 struct Atomic::LoadImpl&lt;
384   T,
385   PlatformOp,
386   typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value || IsPointer&lt;T&gt;::value&gt;::type&gt;
387 {
388   T operator()(T const volatile* dest) const {
389     // Forward to the platform handler for the size of T.
390     return PlatformOp()(dest);
391   }
392 };
393 
394 // Handle load for types that have a translator.
395 //
396 // All the involved types must be identical.
397 //
398 // This translates the original call into a call on the decayed
399 // arguments, and returns the recovered result of that translated
400 // call.
401 template&lt;typename T, typename PlatformOp&gt;
402 struct Atomic::LoadImpl&lt;
403   T,
404   PlatformOp,
405   typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
406 {
407   T operator()(T const volatile* dest) const {
408     typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
409     typedef typename Translator::Decayed Decayed;
410     STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
411     Decayed result = PlatformOp()(reinterpret_cast&lt;Decayed const volatile*&gt;(dest));
412     return Translator::recover(result);
413   }
414 };
415 
416 // Default implementation of atomic load if a specific platform
417 // does not provide a specialization for a certain size class.
418 // For increased safety, the default implementation only allows
419 // load types that are pointer sized or smaller. If a platform still
420 // supports wide atomics, then it has to use specialization
421 // of Atomic::PlatformLoad for that wider size class.
422 template&lt;size_t byte_size&gt;
423 struct Atomic::PlatformLoad {
424   template&lt;typename T&gt;
425   T operator()(T const volatile* dest) const {
426     STATIC_ASSERT(sizeof(T) &lt;= sizeof(void*)); // wide atomics need specialization
427     return *dest;
428   }
429 };
430 
431 // Handle store for integral and enum types.
432 //
433 // All the involved types must be identical.
434 template&lt;typename T, typename PlatformOp&gt;
435 struct Atomic::StoreImpl&lt;
436   T, T,
437   PlatformOp,
438   typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
439 {
440   void operator()(T volatile* dest, T new_value) const {
441     // Forward to the platform handler for the size of T.
442     PlatformOp()(dest, new_value);
443   }
444 };
445 
446 // Handle store for pointer types.
447 //
448 // The new_value must be implicitly convertible to the
449 // destination&#39;s type; it must be type-correct to store the
450 // new_value in the destination.
451 template&lt;typename D, typename T, typename PlatformOp&gt;
452 struct Atomic::StoreImpl&lt;
453   D*, T*,
454   PlatformOp,
455   typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value&gt;::type&gt;
456 {
457   void operator()(D* volatile* dest, T* new_value) const {
458     // Allow derived to base conversion, and adding cv-qualifiers.
459     D* value = new_value;
460     PlatformOp()(dest, value);
461   }
462 };
463 
464 // Handle store for types that have a translator.
465 //
466 // All the involved types must be identical.
467 //
468 // This translates the original call into a call on the decayed
469 // arguments.
470 template&lt;typename T, typename PlatformOp&gt;
471 struct Atomic::StoreImpl&lt;
472   T, T,
473   PlatformOp,
474   typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
475 {
476   void operator()(T volatile* dest, T new_value) const {
477     typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
478     typedef typename Translator::Decayed Decayed;
479     STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
480     PlatformOp()(reinterpret_cast&lt;Decayed volatile*&gt;(dest),
481                  Translator::decay(new_value));
482   }
483 };
484 
485 // Default implementation of atomic store if a specific platform
486 // does not provide a specialization for a certain size class.
487 // For increased safety, the default implementation only allows
488 // storing types that are pointer sized or smaller. If a platform still
489 // supports wide atomics, then it has to use specialization
490 // of Atomic::PlatformStore for that wider size class.
491 template&lt;size_t byte_size&gt;
492 struct Atomic::PlatformStore {
493   template&lt;typename T&gt;
494   void operator()(T volatile* dest,
495                   T new_value) const {
496     STATIC_ASSERT(sizeof(T) &lt;= sizeof(void*)); // wide atomics need specialization
497     (void)const_cast&lt;T&amp;&gt;(*dest = new_value);
498   }
499 };
500 
501 template&lt;typename D&gt;
502 inline void Atomic::inc(D volatile* dest, atomic_memory_order order) {
503   STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
504   typedef typename Conditional&lt;IsPointer&lt;D&gt;::value, ptrdiff_t, D&gt;::type I;
505   Atomic::add(dest, I(1), order);
506 }
507 
508 template&lt;typename D&gt;
509 inline void Atomic::dec(D volatile* dest, atomic_memory_order order) {
510   STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
511   typedef typename Conditional&lt;IsPointer&lt;D&gt;::value, ptrdiff_t, D&gt;::type I;
512   // Assumes two&#39;s complement integer representation.
513   #pragma warning(suppress: 4146)
514   Atomic::add(dest, I(-1), order);
515 }
516 
517 template&lt;typename D, typename I&gt;
518 inline D Atomic::sub(D volatile* dest, I sub_value, atomic_memory_order order) {
519   STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
520   STATIC_ASSERT(IsIntegral&lt;I&gt;::value);
521   // If D is a pointer type, use [u]intptr_t as the addend type,
522   // matching signedness of I.  Otherwise, use D as the addend type.
523   typedef typename Conditional&lt;IsSigned&lt;I&gt;::value, intptr_t, uintptr_t&gt;::type PI;
524   typedef typename Conditional&lt;IsPointer&lt;D&gt;::value, PI, D&gt;::type AddendType;
525   // Only allow conversions that can&#39;t change the value.
526   STATIC_ASSERT(IsSigned&lt;I&gt;::value == IsSigned&lt;AddendType&gt;::value);
527   STATIC_ASSERT(sizeof(I) &lt;= sizeof(AddendType));
528   AddendType addend = sub_value;
529   // Assumes two&#39;s complement integer representation.
530   #pragma warning(suppress: 4146) // In case AddendType is not signed.
531   return Atomic::add(dest, -addend, order);
532 }
533 
534 // Define the class before including platform file, which may specialize
535 // the operator definition.  No generic definition of specializations
536 // of the operator template are provided, nor are there any generic
537 // specializations of the class.  The platform file is responsible for
538 // providing those.
539 template&lt;size_t byte_size&gt;
540 struct Atomic::PlatformCmpxchg {
541   template&lt;typename T&gt;
542   T operator()(T volatile* dest,
543                T compare_value,
544                T exchange_value,
545                atomic_memory_order order) const;
546 };
547 
548 // Define the class before including platform file, which may use this
549 // as a base class, requiring it be complete.  The definition is later
550 // in this file, near the other definitions related to cmpxchg.
551 struct Atomic::CmpxchgByteUsingInt {
552   static uint8_t get_byte_in_int(uint32_t n, uint32_t idx);
553   static uint32_t set_byte_in_int(uint32_t n, uint8_t b, uint32_t idx);
554   template&lt;typename T&gt;
555   T operator()(T volatile* dest,
556                T compare_value,
557                T exchange_value,
558                atomic_memory_order order) const;
559 };
560 
561 // Define the class before including platform file, which may specialize
562 // the operator definition.  No generic definition of specializations
563 // of the operator template are provided, nor are there any generic
564 // specializations of the class.  The platform file is responsible for
565 // providing those.
566 template&lt;size_t byte_size&gt;
567 struct Atomic::PlatformXchg {
568   template&lt;typename T&gt;
569   T operator()(T volatile* dest,
570                T exchange_value,
571                atomic_memory_order order) const;
572 };
573 
574 template &lt;ScopedFenceType T&gt;
575 class ScopedFenceGeneral: public StackObj {
576  public:
577   void prefix() {}
578   void postfix() {}
579 };
580 
581 // The following methods can be specialized using simple template specialization
582 // in the platform specific files for optimization purposes. Otherwise the
583 // generalized variant is used.
584 
585 template&lt;&gt; inline void ScopedFenceGeneral&lt;X_ACQUIRE&gt;::postfix()       { OrderAccess::acquire(); }
586 template&lt;&gt; inline void ScopedFenceGeneral&lt;RELEASE_X&gt;::prefix()        { OrderAccess::release(); }
587 template&lt;&gt; inline void ScopedFenceGeneral&lt;RELEASE_X_FENCE&gt;::prefix()  { OrderAccess::release(); }
588 template&lt;&gt; inline void ScopedFenceGeneral&lt;RELEASE_X_FENCE&gt;::postfix() { OrderAccess::fence();   }
589 
590 template &lt;ScopedFenceType T&gt;
591 class ScopedFence : public ScopedFenceGeneral&lt;T&gt; {
592   void *const _field;
593  public:
594   ScopedFence(void *const field) : _field(field) { prefix(); }
595   ~ScopedFence() { postfix(); }
596   void prefix() { ScopedFenceGeneral&lt;T&gt;::prefix(); }
597   void postfix() { ScopedFenceGeneral&lt;T&gt;::postfix(); }
598 };
599 
600 // platform specific in-line definitions - must come before shared definitions
601 
602 #include OS_CPU_HEADER(atomic)
603 
604 // shared in-line definitions
605 
606 // size_t casts...
607 #if (SIZE_MAX != UINTPTR_MAX)
608 #error size_t is not WORD_SIZE, interesting platform, but missing implementation here
609 #endif
610 
611 template&lt;typename T&gt;
612 inline T Atomic::load(const volatile T* dest) {
613   return LoadImpl&lt;T, PlatformLoad&lt;sizeof(T)&gt; &gt;()(dest);
614 }
615 
616 template&lt;size_t byte_size, ScopedFenceType type&gt;
617 struct Atomic::PlatformOrderedLoad {
618   template &lt;typename T&gt;
619   T operator()(const volatile T* p) const {
620     ScopedFence&lt;type&gt; f((void*)p);
621     return Atomic::load(p);
622   }
623 };
624 
625 template &lt;typename T&gt;
626 inline T Atomic::load_acquire(const volatile T* p) {
627   return LoadImpl&lt;T, PlatformOrderedLoad&lt;sizeof(T), X_ACQUIRE&gt; &gt;()(p);
628 }
629 
630 template&lt;typename D, typename T&gt;
631 inline void Atomic::store(volatile D* dest, T store_value) {
632   StoreImpl&lt;D, T, PlatformStore&lt;sizeof(D)&gt; &gt;()(dest, store_value);
633 }
634 
635 template&lt;size_t byte_size, ScopedFenceType type&gt;
636 struct Atomic::PlatformOrderedStore {
637   template &lt;typename T&gt;
638   void operator()(volatile T* p, T v) const {
639     ScopedFence&lt;type&gt; f((void*)p);
640     Atomic::store(p, v);
641   }
642 };
643 
644 template &lt;typename D, typename T&gt;
645 inline void Atomic::release_store(volatile D* p, T v) {
646   StoreImpl&lt;D, T, PlatformOrderedStore&lt;sizeof(D), RELEASE_X&gt; &gt;()(p, v);
647 }
648 
649 template &lt;typename D, typename T&gt;
650 inline void Atomic::release_store_fence(volatile D* p, T v) {
651   StoreImpl&lt;D, T, PlatformOrderedStore&lt;sizeof(D), RELEASE_X_FENCE&gt; &gt;()(p, v);
652 }
653 
654 template&lt;typename D, typename I&gt;
655 inline D Atomic::add(D volatile* dest, I add_value,
656                      atomic_memory_order order) {
657   return AddImpl&lt;D, I&gt;::add_and_fetch(dest, add_value, order);
658 }
659 
660 template&lt;typename D, typename I&gt;
661 inline D Atomic::fetch_and_add(D volatile* dest, I add_value,
662                                atomic_memory_order order) {
663   return AddImpl&lt;D, I&gt;::fetch_and_add(dest, add_value, order);
664 }
665 
666 template&lt;typename D, typename I&gt;
667 struct Atomic::AddImpl&lt;
668   D, I,
669   typename EnableIf&lt;IsIntegral&lt;I&gt;::value &amp;&amp;
670                     IsIntegral&lt;D&gt;::value &amp;&amp;
671                     (sizeof(I) &lt;= sizeof(D)) &amp;&amp;
672                     (IsSigned&lt;I&gt;::value == IsSigned&lt;D&gt;::value)&gt;::type&gt;
673 {
674   static D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) {
675     D addend = add_value;
676     return PlatformAdd&lt;sizeof(D)&gt;().add_and_fetch(dest, addend, order);
677   }
678   static D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order order) {
679     D addend = add_value;
680     return PlatformAdd&lt;sizeof(D)&gt;().fetch_and_add(dest, addend, order);
681   }
682 };
683 
684 template&lt;typename P, typename I&gt;
685 struct Atomic::AddImpl&lt;
686   P*, I,
687   typename EnableIf&lt;IsIntegral&lt;I&gt;::value &amp;&amp; (sizeof(I) &lt;= sizeof(P*))&gt;::type&gt;
688 {
689   STATIC_ASSERT(sizeof(intptr_t) == sizeof(P*));
690   STATIC_ASSERT(sizeof(uintptr_t) == sizeof(P*));
691   typedef typename Conditional&lt;IsSigned&lt;I&gt;::value,
692                                intptr_t,
693                                uintptr_t&gt;::type CI;
694 
695   static CI scale_addend(CI add_value) {
696     return add_value * sizeof(P);
697   }
698 
699   static P* add_and_fetch(P* volatile* dest, I add_value, atomic_memory_order order) {
700     CI addend = add_value;
701     return PlatformAdd&lt;sizeof(P*)&gt;().add_and_fetch(dest, scale_addend(addend), order);
702   }
703   static P* fetch_and_add(P* volatile* dest, I add_value, atomic_memory_order order) {
704     CI addend = add_value;
705     return PlatformAdd&lt;sizeof(P*)&gt;().fetch_and_add(dest, scale_addend(addend), order);
706   }
707 };
708 
709 template&lt;typename Type, typename Fn, typename D, typename I&gt;
710 inline D Atomic::add_using_helper(Fn fn, D volatile* dest, I add_value) {
711   return PrimitiveConversions::cast&lt;D&gt;(
712     fn(PrimitiveConversions::cast&lt;Type&gt;(add_value),
713        reinterpret_cast&lt;Type volatile*&gt;(dest)));
714 }
715 
716 template&lt;typename D, typename U, typename T&gt;
717 inline D Atomic::cmpxchg(D volatile* dest,
718                          U compare_value,
719                          T exchange_value,
720                          atomic_memory_order order) {
721   return CmpxchgImpl&lt;D, U, T&gt;()(dest, compare_value, exchange_value, order);
722 }
723 
724 template&lt;typename D, typename T&gt;
725 inline bool Atomic::replace_if_null(D* volatile* dest, T* value,
726                                     atomic_memory_order order) {
727   // Presently using a trivial implementation in terms of cmpxchg.
728   // Consider adding platform support, to permit the use of compiler
729   // intrinsics like gcc&#39;s __sync_bool_compare_and_swap.
730   D* expected_null = NULL;
731   return expected_null == cmpxchg(dest, expected_null, value, order);
732 }
733 
734 // Handle cmpxchg for integral and enum types.
735 //
736 // All the involved types must be identical.
737 template&lt;typename T&gt;
738 struct Atomic::CmpxchgImpl&lt;
739   T, T, T,
740   typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
741 {
742   T operator()(T volatile* dest, T compare_value, T exchange_value,
743                atomic_memory_order order) const {
744     // Forward to the platform handler for the size of T.
745     return PlatformCmpxchg&lt;sizeof(T)&gt;()(dest,
746                                         compare_value,
747                                         exchange_value,
748                                         order);
749   }
750 };
751 
752 // Handle cmpxchg for pointer types.
753 //
754 // The destination&#39;s type and the compare_value type must be the same,
755 // ignoring cv-qualifiers; we don&#39;t care about the cv-qualifiers of
756 // the compare_value.
757 //
758 // The exchange_value must be implicitly convertible to the
759 // destination&#39;s type; it must be type-correct to store the
760 // exchange_value in the destination.
761 template&lt;typename D, typename U, typename T&gt;
762 struct Atomic::CmpxchgImpl&lt;
763   D*, U*, T*,
764   typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value &amp;&amp;
765                     IsSame&lt;typename RemoveCV&lt;D&gt;::type,
766                            typename RemoveCV&lt;U&gt;::type&gt;::value&gt;::type&gt;
767 {
768   D* operator()(D* volatile* dest, U* compare_value, T* exchange_value,
769                atomic_memory_order order) const {
770     // Allow derived to base conversion, and adding cv-qualifiers.
771     D* new_value = exchange_value;
772     // Don&#39;t care what the CV qualifiers for compare_value are,
773     // but we need to match D* when calling platform support.
774     D* old_value = const_cast&lt;D*&gt;(compare_value);
775     return PlatformCmpxchg&lt;sizeof(D*)&gt;()(dest, old_value, new_value, order);
776   }
777 };
778 
779 // Handle cmpxchg for types that have a translator.
780 //
781 // All the involved types must be identical.
782 //
783 // This translates the original call into a call on the decayed
784 // arguments, and returns the recovered result of that translated
785 // call.
786 template&lt;typename T&gt;
787 struct Atomic::CmpxchgImpl&lt;
788   T, T, T,
789   typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
790 {
791   T operator()(T volatile* dest, T compare_value, T exchange_value,
792                atomic_memory_order order) const {
793     typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
794     typedef typename Translator::Decayed Decayed;
795     STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
796     return Translator::recover(
797       cmpxchg(reinterpret_cast&lt;Decayed volatile*&gt;(dest),
798               Translator::decay(compare_value),
799               Translator::decay(exchange_value),
800               order));
801   }
802 };
803 
804 template&lt;typename Type, typename Fn, typename T&gt;
805 inline T Atomic::cmpxchg_using_helper(Fn fn,
806                                       T volatile* dest,
807                                       T compare_value,
808                                       T exchange_value) {
809   STATIC_ASSERT(sizeof(Type) == sizeof(T));
810   return PrimitiveConversions::cast&lt;T&gt;(
811     fn(PrimitiveConversions::cast&lt;Type&gt;(exchange_value),
812        reinterpret_cast&lt;Type volatile*&gt;(dest),
813        PrimitiveConversions::cast&lt;Type&gt;(compare_value)));
814 }
815 
816 inline uint32_t Atomic::CmpxchgByteUsingInt::set_byte_in_int(uint32_t n,
817                                                              uint8_t b,
818                                                              uint32_t idx) {
819   int bitsIdx = BitsPerByte * idx;
820   return (n &amp; ~(0xff &lt;&lt; bitsIdx)) | (b &lt;&lt; bitsIdx);
821 }
822 
823 inline uint8_t Atomic::CmpxchgByteUsingInt::get_byte_in_int(uint32_t n,
824                                                             uint32_t idx) {
825   int bitsIdx = BitsPerByte * idx;
826   return (uint8_t)(n &gt;&gt; bitsIdx);
827 }
828 
829 template&lt;typename T&gt;
830 inline T Atomic::CmpxchgByteUsingInt::operator()(T volatile* dest,
831                                                  T compare_value,
832                                                  T exchange_value,
833                                                  atomic_memory_order order) const {
834   STATIC_ASSERT(sizeof(T) == sizeof(uint8_t));
835   uint8_t canon_exchange_value = exchange_value;
836   uint8_t canon_compare_value = compare_value;
837   volatile uint32_t* aligned_dest
838     = reinterpret_cast&lt;volatile uint32_t*&gt;(align_down(dest, sizeof(uint32_t)));
839   size_t offset = pointer_delta(dest, aligned_dest, 1);
840 
841   uint32_t idx = (Endian::NATIVE == Endian::BIG)
842                    ? (sizeof(uint32_t) - 1 - offset)
843                    : offset;
844 
845   // current value may not be what we are looking for, so force it
846   // to that value so the initial cmpxchg will fail if it is different
847   uint32_t cur = set_byte_in_int(Atomic::load(aligned_dest), canon_compare_value, idx);
848 
849   // always execute a real cmpxchg so that we get the required memory
850   // barriers even on initial failure
851   do {
852     // value to swap in matches current value
853     // except for the one byte we want to update
854     uint32_t new_value = set_byte_in_int(cur, canon_exchange_value, idx);
855 
856     uint32_t res = cmpxchg(aligned_dest, cur, new_value, order);
857     if (res == cur) break;      // success
858 
859     // at least one byte in the int changed value, so update
860     // our view of the current int
861     cur = res;
862     // if our byte is still as cur we loop and try again
863   } while (get_byte_in_int(cur, idx) == canon_compare_value);
864 
865   return PrimitiveConversions::cast&lt;T&gt;(get_byte_in_int(cur, idx));
866 }
867 
868 // Handle xchg for integral and enum types.
869 //
870 // All the involved types must be identical.
871 template&lt;typename T&gt;
872 struct Atomic::XchgImpl&lt;
873   T, T,
874   typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
875 {
876   T operator()(T volatile* dest, T exchange_value, atomic_memory_order order) const {
877     // Forward to the platform handler for the size of T.
878     return PlatformXchg&lt;sizeof(T)&gt;()(dest, exchange_value, order);
879   }
880 };
881 
882 // Handle xchg for pointer types.
883 //
884 // The exchange_value must be implicitly convertible to the
885 // destination&#39;s type; it must be type-correct to store the
886 // exchange_value in the destination.
887 template&lt;typename D, typename T&gt;
888 struct Atomic::XchgImpl&lt;
889   D*, T*,
890   typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value&gt;::type&gt;
891 {
892   D* operator()(D* volatile* dest, T* exchange_value, atomic_memory_order order) const {
893     // Allow derived to base conversion, and adding cv-qualifiers.
894     D* new_value = exchange_value;
895     return PlatformXchg&lt;sizeof(D*)&gt;()(dest, new_value, order);
896   }
897 };
898 
899 // Handle xchg for types that have a translator.
900 //
901 // All the involved types must be identical.
902 //
903 // This translates the original call into a call on the decayed
904 // arguments, and returns the recovered result of that translated
905 // call.
906 template&lt;typename T&gt;
907 struct Atomic::XchgImpl&lt;
908   T, T,
909   typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
910 {
911   T operator()(T volatile* dest, T exchange_value, atomic_memory_order order) const {
912     typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
913     typedef typename Translator::Decayed Decayed;
914     STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
915     return Translator::recover(
916       xchg(reinterpret_cast&lt;Decayed volatile*&gt;(dest),
917            Translator::decay(exchange_value),
918            order));
919   }
920 };
921 
922 template&lt;typename Type, typename Fn, typename T&gt;
923 inline T Atomic::xchg_using_helper(Fn fn,
924                                    T volatile* dest,
925                                    T exchange_value) {
926   STATIC_ASSERT(sizeof(Type) == sizeof(T));
927   // Notice the swapped order of arguments. Change when/if stubs are rewritten.
928   return PrimitiveConversions::cast&lt;T&gt;(
929     fn(PrimitiveConversions::cast&lt;Type&gt;(exchange_value),
930        reinterpret_cast&lt;Type volatile*&gt;(dest)));
931 }
932 
933 template&lt;typename D, typename T&gt;
934 inline D Atomic::xchg(volatile D* dest, T exchange_value, atomic_memory_order order) {
935   return XchgImpl&lt;D, T&gt;()(dest, exchange_value, order);
936 }
937 
938 #endif // SHARE_RUNTIME_ATOMIC_HPP
    </pre>
  </body>
</html>