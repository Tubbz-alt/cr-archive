diff a/src/hotspot/share/runtime/sweeper.cpp b/src/hotspot/share/runtime/sweeper.cpp
--- a/src/hotspot/share/runtime/sweeper.cpp
+++ b/src/hotspot/share/runtime/sweeper.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -35,12 +35,10 @@
 #include "logging/logStream.hpp"
 #include "memory/allocation.inline.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "oops/method.hpp"
-#include "runtime/atomic.hpp"
-#include "runtime/compilationPolicy.hpp"
 #include "runtime/interfaceSupport.inline.hpp"
 #include "runtime/handshake.hpp"
 #include "runtime/mutexLocker.hpp"
 #include "runtime/orderAccess.hpp"
 #include "runtime/os.hpp"
@@ -197,31 +195,31 @@
 }
 bool NMethodSweeper::wait_for_stack_scanning() {
   return _current.end();
 }
 
-class NMethodMarkingThreadClosure : public ThreadClosure {
+class NMethodMarkingClosure : public HandshakeClosure {
 private:
   CodeBlobClosure* _cl;
 public:
-  NMethodMarkingThreadClosure(CodeBlobClosure* cl) : _cl(cl) {}
+  NMethodMarkingClosure(CodeBlobClosure* cl) : HandshakeClosure("NMethodMarking"), _cl(cl) {}
   void do_thread(Thread* thread) {
     if (thread->is_Java_thread() && ! thread->is_Code_cache_sweeper_thread()) {
       JavaThread* jt = (JavaThread*) thread;
       jt->nmethods_do(_cl);
     }
   }
 };
 
 class NMethodMarkingTask : public AbstractGangTask {
 private:
-  NMethodMarkingThreadClosure* _cl;
+  NMethodMarkingClosure* _cl;
 public:
-  NMethodMarkingTask(NMethodMarkingThreadClosure* cl) :
+  NMethodMarkingTask(NMethodMarkingClosure* cl) :
     AbstractGangTask("Parallel NMethod Marking"),
     _cl(cl) {
-    Threads::change_thread_claim_parity();
+    Threads::change_thread_claim_token();
   }
 
   ~NMethodMarkingTask() {
     Threads::assert_all_threads_claimed();
   }
@@ -239,22 +237,22 @@
 void NMethodSweeper::mark_active_nmethods() {
   CodeBlobClosure* cl = prepare_mark_active_nmethods();
   if (cl != NULL) {
     WorkGang* workers = Universe::heap()->get_safepoint_workers();
     if (workers != NULL) {
-      NMethodMarkingThreadClosure tcl(cl);
+      NMethodMarkingClosure tcl(cl);
       NMethodMarkingTask task(&tcl);
       workers->run_task(&task);
     } else {
       Threads::nmethods_do(cl);
     }
   }
 }
 
 CodeBlobClosure* NMethodSweeper::prepare_mark_active_nmethods() {
 #ifdef ASSERT
-  if (ThreadLocalHandshakes) {
+  if (SafepointMechanism::uses_thread_local_poll()) {
     assert(Thread::current()->is_Code_cache_sweeper_thread(), "must be executed under CodeCache_lock and in sweeper thread");
     assert_lock_strong(CodeCache_lock);
   } else {
     assert(SafepointSynchronize::is_at_safepoint(), "must be executed at a safepoint");
   }
@@ -317,19 +315,19 @@
   * methods. Stack scanning is mandatory for the sweeper to make progress.
   */
 void NMethodSweeper::do_stack_scanning() {
   assert(!CodeCache_lock->owned_by_self(), "just checking");
   if (wait_for_stack_scanning()) {
-    if (ThreadLocalHandshakes) {
+    if (SafepointMechanism::uses_thread_local_poll()) {
       CodeBlobClosure* code_cl;
       {
-        MutexLockerEx ccl(CodeCache_lock, Mutex::_no_safepoint_check_flag);
+        MutexLocker ccl(CodeCache_lock, Mutex::_no_safepoint_check_flag);
         code_cl = prepare_mark_active_nmethods();
       }
       if (code_cl != NULL) {
-        NMethodMarkingThreadClosure tcl(code_cl);
-        Handshake::execute(&tcl);
+        NMethodMarkingClosure nm_cl(code_cl);
+        Handshake::execute(&nm_cl);
       }
     } else {
       VM_MarkActiveNMethods op;
       VMThread::execute(&op);
     }
@@ -339,13 +337,13 @@
 void NMethodSweeper::sweeper_loop() {
   bool timeout;
   while (true) {
     {
       ThreadBlockInVM tbivm(JavaThread::current());
-      MutexLockerEx waiter(CodeCache_lock, Mutex::_no_safepoint_check_flag);
+      MonitorLocker waiter(CodeCache_lock, Mutex::_no_safepoint_check_flag);
       const long wait_time = 60*60*24 * 1000;
-      timeout = CodeCache_lock->wait(Mutex::_no_safepoint_check_flag, wait_time);
+      timeout = waiter.wait(wait_time);
     }
     if (!timeout) {
       possibly_sweep();
     }
   }
@@ -367,19 +365,19 @@
 /**
   * Wakes up the sweeper thread and forces a sweep. Blocks until it finished.
   */
 void NMethodSweeper::force_sweep() {
   ThreadBlockInVM tbivm(JavaThread::current());
-  MutexLockerEx waiter(CodeCache_lock, Mutex::_no_safepoint_check_flag);
+  MonitorLocker waiter(CodeCache_lock, Mutex::_no_safepoint_check_flag);
   // Request forced sweep
   _force_sweep = true;
   while (_force_sweep) {
     // Notify sweeper that we want to force a sweep and wait for completion.
     // In case a sweep currently takes place we timeout and try again because
     // we want to enforce a full sweep.
     CodeCache_lock->notify();
-    CodeCache_lock->wait(Mutex::_no_safepoint_check_flag, 1000);
+    waiter.wait(1000);
   }
 }
 
 /**
  * Handle a safepoint request
@@ -388,11 +386,11 @@
   JavaThread* thread = JavaThread::current();
   if (SafepointMechanism::should_block(thread)) {
     if (PrintMethodFlushing && Verbose) {
       tty->print_cr("### Sweep at %d out of %d, yielding to safepoint", _seen, CodeCache::nmethod_count());
     }
-    MutexUnlockerEx mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);
+    MutexUnlocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);
 
     ThreadBlockInVM tbivm(thread);
     thread->java_suspend_self();
   }
 }
@@ -473,11 +471,11 @@
   }
 
   if (forced) {
     // Notify requester that forced sweep finished
     assert(_force_sweep, "Should be a forced sweep");
-    MutexLockerEx mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);
+    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);
     _force_sweep = false;
     CodeCache_lock->notify();
   }
 }
 
@@ -517,11 +515,11 @@
   assert(!SafepointSynchronize::is_at_safepoint(), "should not be in safepoint when we get here");
   assert(!CodeCache_lock->owned_by_self(), "just checking");
 
   int freed_memory = 0;
   {
-    MutexLockerEx mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);
+    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);
 
     while (!_current.end()) {
       swept_count++;
       // Since we will give up the CodeCache_lock, always skip ahead
       // to the next nmethod.  Other blobs can be deleted by other
@@ -529,11 +527,11 @@
       CompiledMethod* nm = _current.method();
       _current.next();
 
       // Now ready to process nmethod and give up CodeCache_lock
       {
-        MutexUnlockerEx mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);
+        MutexUnlocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);
         // Save information before potentially flushing the nmethod
         // Only flushing nmethods so size only matters for them.
         int size = nm->is_nmethod() ? ((nmethod*)nm)->total_size() : 0;
         bool is_c2_method = nm->is_compiled_by_c2();
         bool is_osr = nm->is_osr_method();
@@ -574,11 +572,11 @@
   assert(_current.end(), "must have scanned the whole cache");
 
   const Ticks sweep_end_counter = Ticks::now();
   const Tickspan sweep_time = sweep_end_counter - sweep_start_counter;
   {
-    MutexLockerEx mu(NMethodSweeperStats_lock, Mutex::_no_safepoint_check_flag);
+    MutexLocker mu(NMethodSweeperStats_lock, Mutex::_no_safepoint_check_flag);
     _total_time_sweeping  += sweep_time;
     _total_time_this_sweep += sweep_time;
     _peak_sweep_fraction_time = MAX2(sweep_time, _peak_sweep_fraction_time);
     _total_flushed_size += freed_memory;
     _total_nof_methods_reclaimed += flushed_count;
@@ -697,47 +695,26 @@
     OrderAccess::loadload(); // _stack_traversal_mark and _state
     if (cm->can_convert_to_zombie()) {
       // Code cache state change is tracked in make_zombie()
       cm->make_zombie();
       SWEEP(cm);
-      // The nmethod may have been locked by JVMTI after being made zombie (see
-      // JvmtiDeferredEvent::compiled_method_unload_event()). If so, we cannot
-      // flush the osr nmethod directly but have to wait for a later sweeper cycle.
-      if (cm->is_osr_method() && !cm->is_locked_by_vm()) {
-        // No inline caches will ever point to osr methods, so we can just remove it.
-        // Make sure that we unregistered the nmethod with the heap and flushed all
-        // dependencies before removing the nmethod (done in make_zombie()).
-        assert(cm->is_zombie(), "nmethod must be unregistered");
-        cm->flush();
-        assert(result == None, "sanity");
-        result = Flushed;
-      } else {
-        assert(result == None, "sanity");
-        result = MadeZombie;
-        assert(cm->is_zombie(), "nmethod must be zombie");
-      }
+      assert(result == None, "sanity");
+      result = MadeZombie;
+      assert(cm->is_zombie(), "nmethod must be zombie");
     } else {
       // Still alive, clean up its inline caches
       cm->cleanup_inline_caches(false);
       SWEEP(cm);
     }
   } else if (cm->is_unloaded()) {
     // Code is unloaded, so there are no activations on the stack.
-    // Convert the nmethod to zombie or flush it directly in the OSR case.
-    if (cm->is_osr_method()) {
-      SWEEP(cm);
-      // No inline caches will ever point to osr methods, so we can just remove it
-      cm->flush();
-      assert(result == None, "sanity");
-      result = Flushed;
-    } else {
-      // Code cache state change is tracked in make_zombie()
-      cm->make_zombie();
-      SWEEP(cm);
-      assert(result == None, "sanity");
-      result = MadeZombie;
-    }
+    // Convert the nmethod to zombie.
+    // Code cache state change is tracked in make_zombie()
+    cm->make_zombie();
+    SWEEP(cm);
+    assert(result == None, "sanity");
+    result = MadeZombie;
   } else {
     if (cm->is_nmethod()) {
       possibly_flush((nmethod*)cm);
     }
     // Clean inline caches that point to zombie/non-entrant/unloaded nmethods
