<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/hotspot/share/runtime/atomic.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="arguments.hpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="basicLock.cpp.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/runtime/atomic.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 32,11 ***</span>
<span class="line-new-header">--- 32,13 ---</span>
  #include &quot;metaprogramming/isPointer.hpp&quot;
  #include &quot;metaprogramming/isSame.hpp&quot;
  #include &quot;metaprogramming/primitiveConversions.hpp&quot;
  #include &quot;metaprogramming/removeCV.hpp&quot;
  #include &quot;metaprogramming/removePointer.hpp&quot;
<span class="line-added">+ #include &quot;runtime/orderAccess.hpp&quot;</span>
  #include &quot;utilities/align.hpp&quot;
<span class="line-added">+ #include &quot;utilities/bytes.hpp&quot;</span>
  #include &quot;utilities/macros.hpp&quot;
  
  enum atomic_memory_order {
    // The modes that align with C++11 are intended to
    // follow the same semantics.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 46,10 ***</span>
<span class="line-new-header">--- 48,16 ---</span>
    memory_order_acq_rel = 4,
    // Strong two-way memory barrier.
    memory_order_conservative = 8
  };
  
<span class="line-added">+ enum ScopedFenceType {</span>
<span class="line-added">+     X_ACQUIRE</span>
<span class="line-added">+   , RELEASE_X</span>
<span class="line-added">+   , RELEASE_X_FENCE</span>
<span class="line-added">+ };</span>
<span class="line-added">+ </span>
  class Atomic : AllStatic {
  public:
    // Atomic operations on int64 types are not available on all 32-bit
    // platforms. If atomic ops on int64 are defined here they must only
    // be used from code that verifies they are available at runtime and
</pre>
<hr />
<pre>
<span class="line-old-header">*** 70,28 ***</span>
  
    // Atomically store to a location
    // The type T must be either a pointer type convertible to or equal
    // to D, an integral/enum type equal to D, or a type equal to D that
    // is primitive convertible using PrimitiveConversions.
<span class="line-modified">!   template&lt;typename T, typename D&gt;</span>
<span class="line-modified">!   inline static void store(T store_value, volatile D* dest);</span>
  
    // Atomically load from a location
    // The type T must be either a pointer type, an integral/enum type,
    // or a type that is primitive convertible using PrimitiveConversions.
    template&lt;typename T&gt;
    inline static T load(const volatile T* dest);
  
<span class="line-modified">!   // Atomically add to a location. Returns updated value. add*() provide:</span>
    // &lt;fence&gt; add-value-to-dest &lt;membar StoreLoad|StoreStore&gt;
  
<span class="line-modified">!   template&lt;typename I, typename D&gt;</span>
<span class="line-modified">!   inline static D add(I add_value, D volatile* dest,</span>
                        atomic_memory_order order = memory_order_conservative);
  
<span class="line-modified">!   template&lt;typename I, typename D&gt;</span>
<span class="line-modified">!   inline static D sub(I sub_value, D volatile* dest,</span>
                        atomic_memory_order order = memory_order_conservative);
  
    // Atomically increment location. inc() provide:
    // &lt;fence&gt; increment-dest &lt;membar StoreLoad|StoreStore&gt;
    // The type D may be either a pointer type, or an integral
<span class="line-new-header">--- 78,43 ---</span>
  
    // Atomically store to a location
    // The type T must be either a pointer type convertible to or equal
    // to D, an integral/enum type equal to D, or a type equal to D that
    // is primitive convertible using PrimitiveConversions.
<span class="line-modified">!   template&lt;typename D, typename T&gt;</span>
<span class="line-modified">!   inline static void store(volatile D* dest, T store_value);</span>
<span class="line-added">+ </span>
<span class="line-added">+   template &lt;typename D, typename T&gt;</span>
<span class="line-added">+   inline static void release_store(volatile D* dest, T store_value);</span>
<span class="line-added">+ </span>
<span class="line-added">+   template &lt;typename D, typename T&gt;</span>
<span class="line-added">+   inline static void release_store_fence(volatile D* dest, T store_value);</span>
  
    // Atomically load from a location
    // The type T must be either a pointer type, an integral/enum type,
    // or a type that is primitive convertible using PrimitiveConversions.
    template&lt;typename T&gt;
    inline static T load(const volatile T* dest);
  
<span class="line-modified">!   template &lt;typename T&gt;</span>
<span class="line-added">+   inline static T load_acquire(const volatile T* dest);</span>
<span class="line-added">+ </span>
<span class="line-added">+   // Atomically add to a location. *add*() provide:</span>
    // &lt;fence&gt; add-value-to-dest &lt;membar StoreLoad|StoreStore&gt;
  
<span class="line-modified">!   // Returns updated value.</span>
<span class="line-modified">!   template&lt;typename D, typename I&gt;</span>
<span class="line-added">+   inline static D add(D volatile* dest, I add_value,</span>
                        atomic_memory_order order = memory_order_conservative);
  
<span class="line-modified">!   // Returns previous value.</span>
<span class="line-modified">!   template&lt;typename D, typename I&gt;</span>
<span class="line-added">+   inline static D fetch_and_add(D volatile* dest, I add_value,</span>
<span class="line-added">+                                 atomic_memory_order order = memory_order_conservative);</span>
<span class="line-added">+ </span>
<span class="line-added">+   template&lt;typename D, typename I&gt;</span>
<span class="line-added">+   inline static D sub(D volatile* dest, I sub_value,</span>
                        atomic_memory_order order = memory_order_conservative);
  
    // Atomically increment location. inc() provide:
    // &lt;fence&gt; increment-dest &lt;membar StoreLoad|StoreStore&gt;
    // The type D may be either a pointer type, or an integral
</pre>
<hr />
<pre>
<span class="line-old-header">*** 114,32 ***</span>
    // prior value of *dest. xchg*() provide:
    // &lt;fence&gt; exchange-value-with-dest &lt;membar StoreLoad|StoreStore&gt;
    // The type T must be either a pointer type convertible to or equal
    // to D, an integral/enum type equal to D, or a type equal to D that
    // is primitive convertible using PrimitiveConversions.
<span class="line-modified">!   template&lt;typename T, typename D&gt;</span>
<span class="line-modified">!   inline static D xchg(T exchange_value, volatile D* dest,</span>
                         atomic_memory_order order = memory_order_conservative);
  
    // Performs atomic compare of *dest and compare_value, and exchanges
    // *dest with exchange_value if the comparison succeeded. Returns prior
    // value of *dest. cmpxchg*() provide:
    // &lt;fence&gt; compare-and-exchange &lt;membar StoreLoad|StoreStore&gt;
  
<span class="line-modified">!   template&lt;typename T, typename D, typename U&gt;</span>
<span class="line-modified">!   inline static D cmpxchg(T exchange_value,</span>
<span class="line-removed">-                           D volatile* dest,</span>
                            U compare_value,
                            atomic_memory_order order = memory_order_conservative);
  
    // Performs atomic compare of *dest and NULL, and replaces *dest
    // with exchange_value if the comparison succeeded.  Returns true if
    // the comparison succeeded and the exchange occurred.  This is
    // often used as part of lazy initialization, as a lock-free
    // alternative to the Double-Checked Locking Pattern.
<span class="line-modified">!   template&lt;typename T, typename D&gt;</span>
<span class="line-modified">!   inline static bool replace_if_null(T* value, D* volatile* dest,</span>
                                       atomic_memory_order order = memory_order_conservative);
  
  private:
  WINDOWS_ONLY(public:) // VS2017 warns (C2027) use of undefined type if IsPointerConvertible is declared private
    // Test whether From is implicitly convertible to To.
<span class="line-new-header">--- 137,32 ---</span>
    // prior value of *dest. xchg*() provide:
    // &lt;fence&gt; exchange-value-with-dest &lt;membar StoreLoad|StoreStore&gt;
    // The type T must be either a pointer type convertible to or equal
    // to D, an integral/enum type equal to D, or a type equal to D that
    // is primitive convertible using PrimitiveConversions.
<span class="line-modified">!   template&lt;typename D, typename T&gt;</span>
<span class="line-modified">!   inline static D xchg(volatile D* dest, T exchange_value,</span>
                         atomic_memory_order order = memory_order_conservative);
  
    // Performs atomic compare of *dest and compare_value, and exchanges
    // *dest with exchange_value if the comparison succeeded. Returns prior
    // value of *dest. cmpxchg*() provide:
    // &lt;fence&gt; compare-and-exchange &lt;membar StoreLoad|StoreStore&gt;
  
<span class="line-modified">!   template&lt;typename D, typename U, typename T&gt;</span>
<span class="line-modified">!   inline static D cmpxchg(D volatile* dest,</span>
                            U compare_value,
<span class="line-added">+                           T exchange_value,</span>
                            atomic_memory_order order = memory_order_conservative);
  
    // Performs atomic compare of *dest and NULL, and replaces *dest
    // with exchange_value if the comparison succeeded.  Returns true if
    // the comparison succeeded and the exchange occurred.  This is
    // often used as part of lazy initialization, as a lock-free
    // alternative to the Double-Checked Locking Pattern.
<span class="line-modified">!   template&lt;typename D, typename T&gt;</span>
<span class="line-modified">!   inline static bool replace_if_null(D* volatile* dest, T* value,</span>
                                       atomic_memory_order order = memory_order_conservative);
  
  private:
  WINDOWS_ONLY(public:) // VS2017 warns (C2027) use of undefined type if IsPointerConvertible is declared private
    // Test whether From is implicitly convertible to To.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 150,11 ***</span>
  
  protected:
    // Dispatch handler for store.  Provides type-based validity
    // checking and limited conversions around calls to the platform-
    // specific implementation layer provided by PlatformOp.
<span class="line-modified">!   template&lt;typename T, typename D, typename PlatformOp, typename Enable = void&gt;</span>
    struct StoreImpl;
  
    // Platform-specific implementation of store.  Support for sizes
    // of 1, 2, 4, and (if different) pointer size bytes are required.
    // The class is a function object that must be default constructable,
<span class="line-new-header">--- 173,11 ---</span>
  
  protected:
    // Dispatch handler for store.  Provides type-based validity
    // checking and limited conversions around calls to the platform-
    // specific implementation layer provided by PlatformOp.
<span class="line-modified">!   template&lt;typename D, typename T, typename PlatformOp, typename Enable = void&gt;</span>
    struct StoreImpl;
  
    // Platform-specific implementation of store.  Support for sizes
    // of 1, 2, 4, and (if different) pointer size bytes are required.
    // The class is a function object that must be default constructable,
</pre>
<hr />
<pre>
<span class="line-old-header">*** 198,84 ***</span>
    //
    // The default implementation is a volatile load. If a platform
    // requires more for e.g. 64 bit loads, a specialization is required
    template&lt;size_t byte_size&gt; struct PlatformLoad;
  
  private:
    // Dispatch handler for add.  Provides type-based validity checking
    // and limited conversions around calls to the platform-specific
    // implementation layer provided by PlatformAdd.
<span class="line-modified">!   template&lt;typename I, typename D, typename Enable = void&gt;</span>
    struct AddImpl;
  
    // Platform-specific implementation of add.  Support for sizes of 4
    // bytes and (if different) pointer size bytes are required.  The
<span class="line-modified">!   // class is a function object that must be default constructable,</span>
<span class="line-removed">-   // with these requirements:</span>
    //
    // - dest is of type D*, an integral or pointer type.
    // - add_value is of type I, an integral type.
    // - sizeof(I) == sizeof(D).
    // - if D is an integral type, I == D.
    // - platform_add is an object of type PlatformAdd&lt;sizeof(D)&gt;.
    //
<span class="line-modified">!   // Then</span>
<span class="line-modified">!   //   platform_add(add_value, dest)</span>
<span class="line-modified">!   // must be a valid expression, returning a result convertible to D.</span>
<span class="line-modified">!   //</span>
<span class="line-removed">-   // No definition is provided; all platforms must explicitly define</span>
<span class="line-removed">-   // this class and any needed specializations.</span>
<span class="line-removed">-   template&lt;size_t byte_size&gt; struct PlatformAdd;</span>
<span class="line-removed">- </span>
<span class="line-removed">-   // Helper base classes for defining PlatformAdd.  To use, define</span>
<span class="line-removed">-   // PlatformAdd or a specialization that derives from one of these,</span>
<span class="line-removed">-   // and include in the PlatformAdd definition the support function</span>
<span class="line-removed">-   // (described below) required by the base class.</span>
    //
<span class="line-modified">!   // These classes implement the required function object protocol for</span>
<span class="line-modified">!   // PlatformAdd, using a support function template provided by the</span>
<span class="line-removed">-   // derived class.  Let add_value (of type I) and dest (of type D) be</span>
<span class="line-removed">-   // the arguments the object is called with.  If D is a pointer type</span>
<span class="line-removed">-   // P*, then let addend (of type I) be add_value * sizeof(P);</span>
<span class="line-removed">-   // otherwise, addend is add_value.</span>
    //
<span class="line-modified">!   // FetchAndAdd requires the derived class to provide</span>
<span class="line-modified">!   //   fetch_and_add(addend, dest)</span>
<span class="line-removed">-   // atomically adding addend to the value of dest, and returning the</span>
<span class="line-removed">-   // old value.</span>
    //
<span class="line-modified">!   // AddAndFetch requires the derived class to provide</span>
<span class="line-modified">!   //   add_and_fetch(addend, dest)</span>
<span class="line-modified">!   // atomically adding addend to the value of dest, and returning the</span>
<span class="line-removed">-   // new value.</span>
    //
<span class="line-modified">!   // When D is a pointer type P*, both fetch_and_add and add_and_fetch</span>
<span class="line-modified">!   // treat it as if it were a uintptr_t; they do not perform any</span>
<span class="line-modified">!   // scaling of the addend, as that has already been done by the</span>
<span class="line-removed">-   // caller.</span>
<span class="line-removed">- public: // Temporary, can&#39;t be private: C++03 11.4/2. Fixed by C++11.</span>
<span class="line-removed">-   template&lt;typename Derived&gt; struct FetchAndAdd;</span>
<span class="line-removed">-   template&lt;typename Derived&gt; struct AddAndFetch;</span>
<span class="line-removed">- private:</span>
  
    // Support for platforms that implement some variants of add using a
    // (typically out of line) non-template helper function.  The
    // generic arguments passed to PlatformAdd need to be translated to
    // the appropriate type for the helper function, the helper function
    // invoked on the translated arguments, and the result translated
    // back.  Type is the parameter / return type of the helper
    // function.  No scaling of add_value is performed when D is a pointer
    // type, so this function can be used to implement the support function
    // required by AddAndFetch.
<span class="line-modified">!   template&lt;typename Type, typename Fn, typename I, typename D&gt;</span>
<span class="line-modified">!   static D add_using_helper(Fn fn, I add_value, D volatile* dest);</span>
  
    // Dispatch handler for cmpxchg.  Provides type-based validity
    // checking and limited conversions around calls to the
    // platform-specific implementation layer provided by
    // PlatformCmpxchg.
<span class="line-modified">!   template&lt;typename T, typename D, typename U, typename Enable = void&gt;</span>
    struct CmpxchgImpl;
  
    // Platform-specific implementation of cmpxchg.  Support for sizes
    // of 1, 4, and 8 are required.  The class is a function object that
    // must be default constructable, with these requirements:
<span class="line-new-header">--- 221,68 ---</span>
    //
    // The default implementation is a volatile load. If a platform
    // requires more for e.g. 64 bit loads, a specialization is required
    template&lt;size_t byte_size&gt; struct PlatformLoad;
  
<span class="line-added">+   // Give platforms a variation point to specialize.</span>
<span class="line-added">+   template&lt;size_t byte_size, ScopedFenceType type&gt; struct PlatformOrderedStore;</span>
<span class="line-added">+   template&lt;size_t byte_size, ScopedFenceType type&gt; struct PlatformOrderedLoad;</span>
<span class="line-added">+ </span>
  private:
    // Dispatch handler for add.  Provides type-based validity checking
    // and limited conversions around calls to the platform-specific
    // implementation layer provided by PlatformAdd.
<span class="line-modified">!   template&lt;typename D, typename I, typename Enable = void&gt;</span>
    struct AddImpl;
  
    // Platform-specific implementation of add.  Support for sizes of 4
    // bytes and (if different) pointer size bytes are required.  The
<span class="line-modified">!   // class must be default constructable, with these requirements:</span>
    //
    // - dest is of type D*, an integral or pointer type.
    // - add_value is of type I, an integral type.
    // - sizeof(I) == sizeof(D).
    // - if D is an integral type, I == D.
<span class="line-added">+   // - order is of type atomic_memory_order.</span>
    // - platform_add is an object of type PlatformAdd&lt;sizeof(D)&gt;.
    //
<span class="line-modified">!   // Then both</span>
<span class="line-modified">!   //   platform_add.add_and_fetch(dest, add_value, order)</span>
<span class="line-modified">!   //   platform_add.fetch_and_add(dest, add_value, order)</span>
<span class="line-modified">!   // must be valid expressions returning a result convertible to D.</span>
    //
<span class="line-modified">!   // add_and_fetch atomically adds add_value to the value of dest,</span>
<span class="line-modified">!   // returning the new value.</span>
    //
<span class="line-modified">!   // fetch_and_add atomically adds add_value to the value of dest,</span>
<span class="line-modified">!   // returning the old value.</span>
    //
<span class="line-modified">!   // When D is a pointer type P*, both add_and_fetch and fetch_and_add</span>
<span class="line-modified">!   // treat it as if it were an uintptr_t; they do not perform any</span>
<span class="line-modified">!   // scaling of add_value, as that has already been done by the caller.</span>
    //
<span class="line-modified">!   // No definition is provided; all platforms must explicitly define</span>
<span class="line-modified">!   // this class and any needed specializations.</span>
<span class="line-modified">!   template&lt;size_t byte_size&gt; struct PlatformAdd;</span>
  
    // Support for platforms that implement some variants of add using a
    // (typically out of line) non-template helper function.  The
    // generic arguments passed to PlatformAdd need to be translated to
    // the appropriate type for the helper function, the helper function
    // invoked on the translated arguments, and the result translated
    // back.  Type is the parameter / return type of the helper
    // function.  No scaling of add_value is performed when D is a pointer
    // type, so this function can be used to implement the support function
    // required by AddAndFetch.
<span class="line-modified">!   template&lt;typename Type, typename Fn, typename D, typename I&gt;</span>
<span class="line-modified">!   static D add_using_helper(Fn fn, D volatile* dest, I add_value);</span>
  
    // Dispatch handler for cmpxchg.  Provides type-based validity
    // checking and limited conversions around calls to the
    // platform-specific implementation layer provided by
    // PlatformCmpxchg.
<span class="line-modified">!   template&lt;typename D, typename U, typename T, typename Enable = void&gt;</span>
    struct CmpxchgImpl;
  
    // Platform-specific implementation of cmpxchg.  Support for sizes
    // of 1, 4, and 8 are required.  The class is a function object that
    // must be default constructable, with these requirements:
</pre>
<hr />
<pre>
<span class="line-old-header">*** 284,15 ***</span>
    // - exchange_value and compare_value are of type T.
    // - order is of type atomic_memory_order.
    // - platform_cmpxchg is an object of type PlatformCmpxchg&lt;sizeof(T)&gt;.
    //
    // Then
<span class="line-modified">!   //   platform_cmpxchg(exchange_value, dest, compare_value, order)</span>
    // must be a valid expression, returning a result convertible to T.
    //
    // A default definition is provided, which declares a function template
<span class="line-modified">!   //   T operator()(T, T volatile*, T, atomic_memory_order) const</span>
    //
    // For each required size, a platform must either provide an
    // appropriate definition of that function, or must entirely
    // specialize the class template for that size.
    template&lt;size_t byte_size&gt; struct PlatformCmpxchg;
<span class="line-new-header">--- 291,15 ---</span>
    // - exchange_value and compare_value are of type T.
    // - order is of type atomic_memory_order.
    // - platform_cmpxchg is an object of type PlatformCmpxchg&lt;sizeof(T)&gt;.
    //
    // Then
<span class="line-modified">!   //   platform_cmpxchg(dest, compare_value, exchange_value, order)</span>
    // must be a valid expression, returning a result convertible to T.
    //
    // A default definition is provided, which declares a function template
<span class="line-modified">!   //   T operator()(T volatile*, T, T, atomic_memory_order) const</span>
    //
    // For each required size, a platform must either provide an
    // appropriate definition of that function, or must entirely
    // specialize the class template for that size.
    template&lt;size_t byte_size&gt; struct PlatformCmpxchg;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 304,13 ***</span>
    // helper invoked on the translated arguments, and the result
    // translated back.  Type is the parameter / return type of the
    // helper function.
    template&lt;typename Type, typename Fn, typename T&gt;
    static T cmpxchg_using_helper(Fn fn,
<span class="line-removed">-                                 T exchange_value,</span>
                                  T volatile* dest,
<span class="line-modified">!                                 T compare_value);</span>
  
    // Support platforms that do not provide Read-Modify-Write
    // byte-level atomic access. To use, derive PlatformCmpxchg&lt;1&gt; from
    // this class.
  public: // Temporary, can&#39;t be private: C++03 11.4/2. Fixed by C++11.
<span class="line-new-header">--- 311,13 ---</span>
    // helper invoked on the translated arguments, and the result
    // translated back.  Type is the parameter / return type of the
    // helper function.
    template&lt;typename Type, typename Fn, typename T&gt;
    static T cmpxchg_using_helper(Fn fn,
                                  T volatile* dest,
<span class="line-modified">!                                 T compare_value,</span>
<span class="line-added">+                                 T exchange_value);</span>
  
    // Support platforms that do not provide Read-Modify-Write
    // byte-level atomic access. To use, derive PlatformCmpxchg&lt;1&gt; from
    // this class.
  public: // Temporary, can&#39;t be private: C++03 11.4/2. Fixed by C++11.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 319,11 ***</span>
  
    // Dispatch handler for xchg.  Provides type-based validity
    // checking and limited conversions around calls to the
    // platform-specific implementation layer provided by
    // PlatformXchg.
<span class="line-modified">!   template&lt;typename T, typename D, typename Enable = void&gt;</span>
    struct XchgImpl;
  
    // Platform-specific implementation of xchg.  Support for sizes
    // of 4, and sizeof(intptr_t) are required.  The class is a function
    // object that must be default constructable, with these requirements:
<span class="line-new-header">--- 326,11 ---</span>
  
    // Dispatch handler for xchg.  Provides type-based validity
    // checking and limited conversions around calls to the
    // platform-specific implementation layer provided by
    // PlatformXchg.
<span class="line-modified">!   template&lt;typename D, typename T, typename Enable = void&gt;</span>
    struct XchgImpl;
  
    // Platform-specific implementation of xchg.  Support for sizes
    // of 4, and sizeof(intptr_t) are required.  The class is a function
    // object that must be default constructable, with these requirements:
</pre>
<hr />
<pre>
<span class="line-old-header">*** 331,15 ***</span>
    // - dest is of type T*.
    // - exchange_value is of type T.
    // - platform_xchg is an object of type PlatformXchg&lt;sizeof(T)&gt;.
    //
    // Then
<span class="line-modified">!   //   platform_xchg(exchange_value, dest)</span>
    // must be a valid expression, returning a result convertible to T.
    //
    // A default definition is provided, which declares a function template
<span class="line-modified">!   //   T operator()(T, T volatile*, T, atomic_memory_order) const</span>
    //
    // For each required size, a platform must either provide an
    // appropriate definition of that function, or must entirely
    // specialize the class template for that size.
    template&lt;size_t byte_size&gt; struct PlatformXchg;
<span class="line-new-header">--- 338,15 ---</span>
    // - dest is of type T*.
    // - exchange_value is of type T.
    // - platform_xchg is an object of type PlatformXchg&lt;sizeof(T)&gt;.
    //
    // Then
<span class="line-modified">!   //   platform_xchg(dest, exchange_value)</span>
    // must be a valid expression, returning a result convertible to T.
    //
    // A default definition is provided, which declares a function template
<span class="line-modified">!   //   T operator()(T volatile*, T, atomic_memory_order) const</span>
    //
    // For each required size, a platform must either provide an
    // appropriate definition of that function, or must entirely
    // specialize the class template for that size.
    template&lt;size_t byte_size&gt; struct PlatformXchg;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 351,12 ***</span>
    // helper invoked on the translated arguments, and the result
    // translated back.  Type is the parameter / return type of the
    // helper function.
    template&lt;typename Type, typename Fn, typename T&gt;
    static T xchg_using_helper(Fn fn,
<span class="line-modified">!                              T exchange_value,</span>
<span class="line-modified">!                              T volatile* dest);</span>
  };
  
  template&lt;typename From, typename To&gt;
  struct Atomic::IsPointerConvertible&lt;From*, To*&gt; : AllStatic {
    // Determine whether From* is implicitly convertible to To*, using
<span class="line-new-header">--- 358,12 ---</span>
    // helper invoked on the translated arguments, and the result
    // translated back.  Type is the parameter / return type of the
    // helper function.
    template&lt;typename Type, typename Fn, typename T&gt;
    static T xchg_using_helper(Fn fn,
<span class="line-modified">!                              T volatile* dest,</span>
<span class="line-modified">!                              T exchange_value);</span>
  };
  
  template&lt;typename From, typename To&gt;
  struct Atomic::IsPointerConvertible&lt;From*, To*&gt; : AllStatic {
    // Determine whether From* is implicitly convertible to To*, using
</pre>
<hr />
<pre>
<span class="line-old-header">*** 428,31 ***</span>
  struct Atomic::StoreImpl&lt;
    T, T,
    PlatformOp,
    typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   void operator()(T new_value, T volatile* dest) const {</span>
      // Forward to the platform handler for the size of T.
<span class="line-modified">!     PlatformOp()(new_value, dest);</span>
    }
  };
  
  // Handle store for pointer types.
  //
  // The new_value must be implicitly convertible to the
  // destination&#39;s type; it must be type-correct to store the
  // new_value in the destination.
<span class="line-modified">! template&lt;typename T, typename D, typename PlatformOp&gt;</span>
  struct Atomic::StoreImpl&lt;
<span class="line-modified">!   T*, D*,</span>
    PlatformOp,
    typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   void operator()(T* new_value, D* volatile* dest) const {</span>
      // Allow derived to base conversion, and adding cv-qualifiers.
      D* value = new_value;
<span class="line-modified">!     PlatformOp()(value, dest);</span>
    }
  };
  
  // Handle store for types that have a translator.
  //
<span class="line-new-header">--- 435,31 ---</span>
  struct Atomic::StoreImpl&lt;
    T, T,
    PlatformOp,
    typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   void operator()(T volatile* dest, T new_value) const {</span>
      // Forward to the platform handler for the size of T.
<span class="line-modified">!     PlatformOp()(dest, new_value);</span>
    }
  };
  
  // Handle store for pointer types.
  //
  // The new_value must be implicitly convertible to the
  // destination&#39;s type; it must be type-correct to store the
  // new_value in the destination.
<span class="line-modified">! template&lt;typename D, typename T, typename PlatformOp&gt;</span>
  struct Atomic::StoreImpl&lt;
<span class="line-modified">!   D*, T*,</span>
    PlatformOp,
    typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   void operator()(D* volatile* dest, T* new_value) const {</span>
      // Allow derived to base conversion, and adding cv-qualifiers.
      D* value = new_value;
<span class="line-modified">!     PlatformOp()(dest, value);</span>
    }
  };
  
  // Handle store for types that have a translator.
  //
</pre>
<hr />
<pre>
<span class="line-old-header">*** 464,16 ***</span>
  struct Atomic::StoreImpl&lt;
    T, T,
    PlatformOp,
    typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   void operator()(T new_value, T volatile* dest) const {</span>
      typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
      typedef typename Translator::Decayed Decayed;
      STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
<span class="line-modified">!     PlatformOp()(Translator::decay(new_value),</span>
<span class="line-modified">!                  reinterpret_cast&lt;Decayed volatile*&gt;(dest));</span>
    }
  };
  
  // Default implementation of atomic store if a specific platform
  // does not provide a specialization for a certain size class.
<span class="line-new-header">--- 471,16 ---</span>
  struct Atomic::StoreImpl&lt;
    T, T,
    PlatformOp,
    typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   void operator()(T volatile* dest, T new_value) const {</span>
      typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
      typedef typename Translator::Decayed Decayed;
      STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
<span class="line-modified">!     PlatformOp()(reinterpret_cast&lt;Decayed volatile*&gt;(dest),</span>
<span class="line-modified">!                  Translator::decay(new_value));</span>
    }
  };
  
  // Default implementation of atomic store if a specific platform
  // does not provide a specialization for a certain size class.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 482,51 ***</span>
  // supports wide atomics, then it has to use specialization
  // of Atomic::PlatformStore for that wider size class.
  template&lt;size_t byte_size&gt;
  struct Atomic::PlatformStore {
    template&lt;typename T&gt;
<span class="line-modified">!   void operator()(T new_value,</span>
<span class="line-modified">!                   T volatile* dest) const {</span>
      STATIC_ASSERT(sizeof(T) &lt;= sizeof(void*)); // wide atomics need specialization
      (void)const_cast&lt;T&amp;&gt;(*dest = new_value);
    }
  };
  
<span class="line-removed">- // Define FetchAndAdd and AddAndFetch helper classes before including</span>
<span class="line-removed">- // platform file, which may use these as base classes, requiring they</span>
<span class="line-removed">- // be complete.</span>
<span class="line-removed">- </span>
<span class="line-removed">- template&lt;typename Derived&gt;</span>
<span class="line-removed">- struct Atomic::FetchAndAdd {</span>
<span class="line-removed">-   template&lt;typename I, typename D&gt;</span>
<span class="line-removed">-   D operator()(I add_value, D volatile* dest, atomic_memory_order order) const;</span>
<span class="line-removed">- };</span>
<span class="line-removed">- </span>
<span class="line-removed">- template&lt;typename Derived&gt;</span>
<span class="line-removed">- struct Atomic::AddAndFetch {</span>
<span class="line-removed">-   template&lt;typename I, typename D&gt;</span>
<span class="line-removed">-   D operator()(I add_value, D volatile* dest, atomic_memory_order order) const;</span>
<span class="line-removed">- };</span>
<span class="line-removed">- </span>
  template&lt;typename D&gt;
  inline void Atomic::inc(D volatile* dest, atomic_memory_order order) {
    STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
    typedef typename Conditional&lt;IsPointer&lt;D&gt;::value, ptrdiff_t, D&gt;::type I;
<span class="line-modified">!   Atomic::add(I(1), dest, order);</span>
  }
  
  template&lt;typename D&gt;
  inline void Atomic::dec(D volatile* dest, atomic_memory_order order) {
    STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
    typedef typename Conditional&lt;IsPointer&lt;D&gt;::value, ptrdiff_t, D&gt;::type I;
    // Assumes two&#39;s complement integer representation.
    #pragma warning(suppress: 4146)
<span class="line-modified">!   Atomic::add(I(-1), dest, order);</span>
  }
  
<span class="line-modified">! template&lt;typename I, typename D&gt;</span>
<span class="line-modified">! inline D Atomic::sub(I sub_value, D volatile* dest, atomic_memory_order order) {</span>
    STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
    STATIC_ASSERT(IsIntegral&lt;I&gt;::value);
    // If D is a pointer type, use [u]intptr_t as the addend type,
    // matching signedness of I.  Otherwise, use D as the addend type.
    typedef typename Conditional&lt;IsSigned&lt;I&gt;::value, intptr_t, uintptr_t&gt;::type PI;
<span class="line-new-header">--- 489,35 ---</span>
  // supports wide atomics, then it has to use specialization
  // of Atomic::PlatformStore for that wider size class.
  template&lt;size_t byte_size&gt;
  struct Atomic::PlatformStore {
    template&lt;typename T&gt;
<span class="line-modified">!   void operator()(T volatile* dest,</span>
<span class="line-modified">!                   T new_value) const {</span>
      STATIC_ASSERT(sizeof(T) &lt;= sizeof(void*)); // wide atomics need specialization
      (void)const_cast&lt;T&amp;&gt;(*dest = new_value);
    }
  };
  
  template&lt;typename D&gt;
  inline void Atomic::inc(D volatile* dest, atomic_memory_order order) {
    STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
    typedef typename Conditional&lt;IsPointer&lt;D&gt;::value, ptrdiff_t, D&gt;::type I;
<span class="line-modified">!   Atomic::add(dest, I(1), order);</span>
  }
  
  template&lt;typename D&gt;
  inline void Atomic::dec(D volatile* dest, atomic_memory_order order) {
    STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
    typedef typename Conditional&lt;IsPointer&lt;D&gt;::value, ptrdiff_t, D&gt;::type I;
    // Assumes two&#39;s complement integer representation.
    #pragma warning(suppress: 4146)
<span class="line-modified">!   Atomic::add(dest, I(-1), order);</span>
  }
  
<span class="line-modified">! template&lt;typename D, typename I&gt;</span>
<span class="line-modified">! inline D Atomic::sub(D volatile* dest, I sub_value, atomic_memory_order order) {</span>
    STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
    STATIC_ASSERT(IsIntegral&lt;I&gt;::value);
    // If D is a pointer type, use [u]intptr_t as the addend type,
    // matching signedness of I.  Otherwise, use D as the addend type.
    typedef typename Conditional&lt;IsSigned&lt;I&gt;::value, intptr_t, uintptr_t&gt;::type PI;
</pre>
<hr />
<pre>
<span class="line-old-header">*** 535,35 ***</span>
    STATIC_ASSERT(IsSigned&lt;I&gt;::value == IsSigned&lt;AddendType&gt;::value);
    STATIC_ASSERT(sizeof(I) &lt;= sizeof(AddendType));
    AddendType addend = sub_value;
    // Assumes two&#39;s complement integer representation.
    #pragma warning(suppress: 4146) // In case AddendType is not signed.
<span class="line-modified">!   return Atomic::add(-addend, dest, order);</span>
  }
  
  // Define the class before including platform file, which may specialize
  // the operator definition.  No generic definition of specializations
  // of the operator template are provided, nor are there any generic
  // specializations of the class.  The platform file is responsible for
  // providing those.
  template&lt;size_t byte_size&gt;
  struct Atomic::PlatformCmpxchg {
    template&lt;typename T&gt;
<span class="line-modified">!   T operator()(T exchange_value,</span>
<span class="line-removed">-                T volatile* dest,</span>
                 T compare_value,
                 atomic_memory_order order) const;
  };
  
  // Define the class before including platform file, which may use this
  // as a base class, requiring it be complete.  The definition is later
  // in this file, near the other definitions related to cmpxchg.
  struct Atomic::CmpxchgByteUsingInt {
    template&lt;typename T&gt;
<span class="line-modified">!   T operator()(T exchange_value,</span>
<span class="line-removed">-                T volatile* dest,</span>
                 T compare_value,
                 atomic_memory_order order) const;
  };
  
  // Define the class before including platform file, which may specialize
  // the operator definition.  No generic definition of specializations
<span class="line-new-header">--- 526,37 ---</span>
    STATIC_ASSERT(IsSigned&lt;I&gt;::value == IsSigned&lt;AddendType&gt;::value);
    STATIC_ASSERT(sizeof(I) &lt;= sizeof(AddendType));
    AddendType addend = sub_value;
    // Assumes two&#39;s complement integer representation.
    #pragma warning(suppress: 4146) // In case AddendType is not signed.
<span class="line-modified">!   return Atomic::add(dest, -addend, order);</span>
  }
  
  // Define the class before including platform file, which may specialize
  // the operator definition.  No generic definition of specializations
  // of the operator template are provided, nor are there any generic
  // specializations of the class.  The platform file is responsible for
  // providing those.
  template&lt;size_t byte_size&gt;
  struct Atomic::PlatformCmpxchg {
    template&lt;typename T&gt;
<span class="line-modified">!   T operator()(T volatile* dest,</span>
                 T compare_value,
<span class="line-added">+                T exchange_value,</span>
                 atomic_memory_order order) const;
  };
  
  // Define the class before including platform file, which may use this
  // as a base class, requiring it be complete.  The definition is later
  // in this file, near the other definitions related to cmpxchg.
  struct Atomic::CmpxchgByteUsingInt {
<span class="line-added">+   static uint8_t get_byte_in_int(uint32_t n, uint32_t idx);</span>
<span class="line-added">+   static uint32_t set_byte_in_int(uint32_t n, uint8_t b, uint32_t idx);</span>
    template&lt;typename T&gt;
<span class="line-modified">!   T operator()(T volatile* dest,</span>
                 T compare_value,
<span class="line-added">+                T exchange_value,</span>
                 atomic_memory_order order) const;
  };
  
  // Define the class before including platform file, which may specialize
  // the operator definition.  No generic definition of specializations
</pre>
<hr />
<pre>
<span class="line-old-header">*** 571,15 ***</span>
  // specializations of the class.  The platform file is responsible for
  // providing those.
  template&lt;size_t byte_size&gt;
  struct Atomic::PlatformXchg {
    template&lt;typename T&gt;
<span class="line-modified">!   T operator()(T exchange_value,</span>
<span class="line-modified">!                T volatile* dest,</span>
                 atomic_memory_order order) const;
  };
  
  // platform specific in-line definitions - must come before shared definitions
  
  #include OS_CPU_HEADER(atomic)
  
  // shared in-line definitions
<span class="line-new-header">--- 564,41 ---</span>
  // specializations of the class.  The platform file is responsible for
  // providing those.
  template&lt;size_t byte_size&gt;
  struct Atomic::PlatformXchg {
    template&lt;typename T&gt;
<span class="line-modified">!   T operator()(T volatile* dest,</span>
<span class="line-modified">!                T exchange_value,</span>
                 atomic_memory_order order) const;
  };
  
<span class="line-added">+ template &lt;ScopedFenceType T&gt;</span>
<span class="line-added">+ class ScopedFenceGeneral: public StackObj {</span>
<span class="line-added">+  public:</span>
<span class="line-added">+   void prefix() {}</span>
<span class="line-added">+   void postfix() {}</span>
<span class="line-added">+ };</span>
<span class="line-added">+ </span>
<span class="line-added">+ // The following methods can be specialized using simple template specialization</span>
<span class="line-added">+ // in the platform specific files for optimization purposes. Otherwise the</span>
<span class="line-added">+ // generalized variant is used.</span>
<span class="line-added">+ </span>
<span class="line-added">+ template&lt;&gt; inline void ScopedFenceGeneral&lt;X_ACQUIRE&gt;::postfix()       { OrderAccess::acquire(); }</span>
<span class="line-added">+ template&lt;&gt; inline void ScopedFenceGeneral&lt;RELEASE_X&gt;::prefix()        { OrderAccess::release(); }</span>
<span class="line-added">+ template&lt;&gt; inline void ScopedFenceGeneral&lt;RELEASE_X_FENCE&gt;::prefix()  { OrderAccess::release(); }</span>
<span class="line-added">+ template&lt;&gt; inline void ScopedFenceGeneral&lt;RELEASE_X_FENCE&gt;::postfix() { OrderAccess::fence();   }</span>
<span class="line-added">+ </span>
<span class="line-added">+ template &lt;ScopedFenceType T&gt;</span>
<span class="line-added">+ class ScopedFence : public ScopedFenceGeneral&lt;T&gt; {</span>
<span class="line-added">+   void *const _field;</span>
<span class="line-added">+  public:</span>
<span class="line-added">+   ScopedFence(void *const field) : _field(field) { prefix(); }</span>
<span class="line-added">+   ~ScopedFence() { postfix(); }</span>
<span class="line-added">+   void prefix() { ScopedFenceGeneral&lt;T&gt;::prefix(); }</span>
<span class="line-added">+   void postfix() { ScopedFenceGeneral&lt;T&gt;::postfix(); }</span>
<span class="line-added">+ };</span>
<span class="line-added">+ </span>
  // platform specific in-line definitions - must come before shared definitions
  
  #include OS_CPU_HEADER(atomic)
  
  // shared in-line definitions
</pre>
<hr />
<pre>
<span class="line-old-header">*** 592,114 ***</span>
  template&lt;typename T&gt;
  inline T Atomic::load(const volatile T* dest) {
    return LoadImpl&lt;T, PlatformLoad&lt;sizeof(T)&gt; &gt;()(dest);
  }
  
<span class="line-modified">! template&lt;typename T, typename D&gt;</span>
<span class="line-modified">! inline void Atomic::store(T store_value, volatile D* dest) {</span>
<span class="line-modified">!   StoreImpl&lt;T, D, PlatformStore&lt;sizeof(D)&gt; &gt;()(store_value, dest);</span>
  }
  
<span class="line-modified">! template&lt;typename I, typename D&gt;</span>
<span class="line-modified">! inline D Atomic::add(I add_value, D volatile* dest,</span>
                       atomic_memory_order order) {
<span class="line-modified">!   return AddImpl&lt;I, D&gt;()(add_value, dest, order);</span>
  }
  
<span class="line-modified">! template&lt;typename I, typename D&gt;</span>
  struct Atomic::AddImpl&lt;
<span class="line-modified">!   I, D,</span>
    typename EnableIf&lt;IsIntegral&lt;I&gt;::value &amp;&amp;
                      IsIntegral&lt;D&gt;::value &amp;&amp;
                      (sizeof(I) &lt;= sizeof(D)) &amp;&amp;
                      (IsSigned&lt;I&gt;::value == IsSigned&lt;D&gt;::value)&gt;::type&gt;
  {
<span class="line-modified">!   D operator()(I add_value, D volatile* dest, atomic_memory_order order) const {</span>
      D addend = add_value;
<span class="line-modified">!     return PlatformAdd&lt;sizeof(D)&gt;()(addend, dest, order);</span>
    }
  };
  
<span class="line-modified">! template&lt;typename I, typename P&gt;</span>
  struct Atomic::AddImpl&lt;
<span class="line-modified">!   I, P*,</span>
    typename EnableIf&lt;IsIntegral&lt;I&gt;::value &amp;&amp; (sizeof(I) &lt;= sizeof(P*))&gt;::type&gt;
  {
<span class="line-modified">!   P* operator()(I add_value, P* volatile* dest, atomic_memory_order order) const {</span>
<span class="line-modified">!     STATIC_ASSERT(sizeof(intptr_t) == sizeof(P*));</span>
<span class="line-modified">!     STATIC_ASSERT(sizeof(uintptr_t) == sizeof(P*));</span>
<span class="line-modified">!     typedef typename Conditional&lt;IsSigned&lt;I&gt;::value,</span>
<span class="line-modified">!                                  intptr_t,</span>
<span class="line-modified">!                                  uintptr_t&gt;::type CI;</span>
<span class="line-modified">!     CI addend = add_value;</span>
<span class="line-modified">!     return PlatformAdd&lt;sizeof(P*)&gt;()(addend, dest, order);</span>
    }
<span class="line-removed">- };</span>
  
<span class="line-modified">! template&lt;typename Derived&gt;</span>
<span class="line-modified">! template&lt;typename I, typename D&gt;</span>
<span class="line-modified">! inline D Atomic::FetchAndAdd&lt;Derived&gt;::operator()(I add_value, D volatile* dest,</span>
<span class="line-removed">-                                                   atomic_memory_order order) const {</span>
<span class="line-removed">-   I addend = add_value;</span>
<span class="line-removed">-   // If D is a pointer type P*, scale by sizeof(P).</span>
<span class="line-removed">-   if (IsPointer&lt;D&gt;::value) {</span>
<span class="line-removed">-     addend *= sizeof(typename RemovePointer&lt;D&gt;::type);</span>
    }
<span class="line-modified">!   D old = static_cast&lt;const Derived*&gt;(this)-&gt;fetch_and_add(addend, dest, order);</span>
<span class="line-modified">!   return old + add_value;</span>
<span class="line-modified">! }</span>
<span class="line-removed">- </span>
<span class="line-removed">- template&lt;typename Derived&gt;</span>
<span class="line-removed">- template&lt;typename I, typename D&gt;</span>
<span class="line-removed">- inline D Atomic::AddAndFetch&lt;Derived&gt;::operator()(I add_value, D volatile* dest,</span>
<span class="line-removed">-                                                   atomic_memory_order order) const {</span>
<span class="line-removed">-   // If D is a pointer type P*, scale by sizeof(P).</span>
<span class="line-removed">-   if (IsPointer&lt;D&gt;::value) {</span>
<span class="line-removed">-     add_value *= sizeof(typename RemovePointer&lt;D&gt;::type);</span>
    }
<span class="line-modified">!   return static_cast&lt;const Derived*&gt;(this)-&gt;add_and_fetch(add_value, dest, order);</span>
<span class="line-removed">- }</span>
  
<span class="line-modified">! template&lt;typename Type, typename Fn, typename I, typename D&gt;</span>
<span class="line-modified">! inline D Atomic::add_using_helper(Fn fn, I add_value, D volatile* dest) {</span>
    return PrimitiveConversions::cast&lt;D&gt;(
      fn(PrimitiveConversions::cast&lt;Type&gt;(add_value),
         reinterpret_cast&lt;Type volatile*&gt;(dest)));
  }
  
<span class="line-modified">! template&lt;typename T, typename D, typename U&gt;</span>
<span class="line-modified">! inline D Atomic::cmpxchg(T exchange_value,</span>
<span class="line-removed">-                          D volatile* dest,</span>
                           U compare_value,
                           atomic_memory_order order) {
<span class="line-modified">!   return CmpxchgImpl&lt;T, D, U&gt;()(exchange_value, dest, compare_value, order);</span>
  }
  
<span class="line-modified">! template&lt;typename T, typename D&gt;</span>
<span class="line-modified">! inline bool Atomic::replace_if_null(T* value, D* volatile* dest,</span>
                                      atomic_memory_order order) {
    // Presently using a trivial implementation in terms of cmpxchg.
    // Consider adding platform support, to permit the use of compiler
    // intrinsics like gcc&#39;s __sync_bool_compare_and_swap.
    D* expected_null = NULL;
<span class="line-modified">!   return expected_null == cmpxchg(value, dest, expected_null, order);</span>
  }
  
  // Handle cmpxchg for integral and enum types.
  //
  // All the involved types must be identical.
  template&lt;typename T&gt;
  struct Atomic::CmpxchgImpl&lt;
    T, T, T,
    typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   T operator()(T exchange_value, T volatile* dest, T compare_value,</span>
                 atomic_memory_order order) const {
      // Forward to the platform handler for the size of T.
<span class="line-modified">!     return PlatformCmpxchg&lt;sizeof(T)&gt;()(exchange_value,</span>
<span class="line-removed">-                                         dest,</span>
                                          compare_value,
                                          order);
    }
  };
  
  // Handle cmpxchg for pointer types.
<span class="line-new-header">--- 611,142 ---</span>
  template&lt;typename T&gt;
  inline T Atomic::load(const volatile T* dest) {
    return LoadImpl&lt;T, PlatformLoad&lt;sizeof(T)&gt; &gt;()(dest);
  }
  
<span class="line-modified">! template&lt;size_t byte_size, ScopedFenceType type&gt;</span>
<span class="line-modified">! struct Atomic::PlatformOrderedLoad {</span>
<span class="line-modified">!   template &lt;typename T&gt;</span>
<span class="line-added">+   T operator()(const volatile T* p) const {</span>
<span class="line-added">+     ScopedFence&lt;type&gt; f((void*)p);</span>
<span class="line-added">+     return Atomic::load(p);</span>
<span class="line-added">+   }</span>
<span class="line-added">+ };</span>
<span class="line-added">+ </span>
<span class="line-added">+ template &lt;typename T&gt;</span>
<span class="line-added">+ inline T Atomic::load_acquire(const volatile T* p) {</span>
<span class="line-added">+   return LoadImpl&lt;T, PlatformOrderedLoad&lt;sizeof(T), X_ACQUIRE&gt; &gt;()(p);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ template&lt;typename D, typename T&gt;</span>
<span class="line-added">+ inline void Atomic::store(volatile D* dest, T store_value) {</span>
<span class="line-added">+   StoreImpl&lt;D, T, PlatformStore&lt;sizeof(D)&gt; &gt;()(dest, store_value);</span>
  }
  
<span class="line-modified">! template&lt;size_t byte_size, ScopedFenceType type&gt;</span>
<span class="line-modified">! struct Atomic::PlatformOrderedStore {</span>
<span class="line-added">+   template &lt;typename T&gt;</span>
<span class="line-added">+   void operator()(volatile T* p, T v) const {</span>
<span class="line-added">+     ScopedFence&lt;type&gt; f((void*)p);</span>
<span class="line-added">+     Atomic::store(p, v);</span>
<span class="line-added">+   }</span>
<span class="line-added">+ };</span>
<span class="line-added">+ </span>
<span class="line-added">+ template &lt;typename D, typename T&gt;</span>
<span class="line-added">+ inline void Atomic::release_store(volatile D* p, T v) {</span>
<span class="line-added">+   StoreImpl&lt;D, T, PlatformOrderedStore&lt;sizeof(D), RELEASE_X&gt; &gt;()(p, v);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ template &lt;typename D, typename T&gt;</span>
<span class="line-added">+ inline void Atomic::release_store_fence(volatile D* p, T v) {</span>
<span class="line-added">+   StoreImpl&lt;D, T, PlatformOrderedStore&lt;sizeof(D), RELEASE_X_FENCE&gt; &gt;()(p, v);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ template&lt;typename D, typename I&gt;</span>
<span class="line-added">+ inline D Atomic::add(D volatile* dest, I add_value,</span>
                       atomic_memory_order order) {
<span class="line-modified">!   return AddImpl&lt;D, I&gt;::add_and_fetch(dest, add_value, order);</span>
  }
  
<span class="line-modified">! template&lt;typename D, typename I&gt;</span>
<span class="line-added">+ inline D Atomic::fetch_and_add(D volatile* dest, I add_value,</span>
<span class="line-added">+                                atomic_memory_order order) {</span>
<span class="line-added">+   return AddImpl&lt;D, I&gt;::fetch_and_add(dest, add_value, order);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ template&lt;typename D, typename I&gt;</span>
  struct Atomic::AddImpl&lt;
<span class="line-modified">!   D, I,</span>
    typename EnableIf&lt;IsIntegral&lt;I&gt;::value &amp;&amp;
                      IsIntegral&lt;D&gt;::value &amp;&amp;
                      (sizeof(I) &lt;= sizeof(D)) &amp;&amp;
                      (IsSigned&lt;I&gt;::value == IsSigned&lt;D&gt;::value)&gt;::type&gt;
  {
<span class="line-modified">!   static D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) {</span>
      D addend = add_value;
<span class="line-modified">!     return PlatformAdd&lt;sizeof(D)&gt;().add_and_fetch(dest, addend, order);</span>
<span class="line-added">+   }</span>
<span class="line-added">+   static D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order order) {</span>
<span class="line-added">+     D addend = add_value;</span>
<span class="line-added">+     return PlatformAdd&lt;sizeof(D)&gt;().fetch_and_add(dest, addend, order);</span>
    }
  };
  
<span class="line-modified">! template&lt;typename P, typename I&gt;</span>
  struct Atomic::AddImpl&lt;
<span class="line-modified">!   P*, I,</span>
    typename EnableIf&lt;IsIntegral&lt;I&gt;::value &amp;&amp; (sizeof(I) &lt;= sizeof(P*))&gt;::type&gt;
  {
<span class="line-modified">!   STATIC_ASSERT(sizeof(intptr_t) == sizeof(P*));</span>
<span class="line-modified">!   STATIC_ASSERT(sizeof(uintptr_t) == sizeof(P*));</span>
<span class="line-modified">!   typedef typename Conditional&lt;IsSigned&lt;I&gt;::value,</span>
<span class="line-modified">!                                intptr_t,</span>
<span class="line-modified">!                                uintptr_t&gt;::type CI;</span>
<span class="line-modified">! </span>
<span class="line-modified">!   static CI scale_addend(CI add_value) {</span>
<span class="line-modified">!     return add_value * sizeof(P);</span>
    }
  
<span class="line-modified">!   static P* add_and_fetch(P* volatile* dest, I add_value, atomic_memory_order order) {</span>
<span class="line-modified">!     CI addend = add_value;</span>
<span class="line-modified">!     return PlatformAdd&lt;sizeof(P*)&gt;().add_and_fetch(dest, scale_addend(addend), order);</span>
    }
<span class="line-modified">!   static P* fetch_and_add(P* volatile* dest, I add_value, atomic_memory_order order) {</span>
<span class="line-modified">!     CI addend = add_value;</span>
<span class="line-modified">!     return PlatformAdd&lt;sizeof(P*)&gt;().fetch_and_add(dest, scale_addend(addend), order);</span>
    }
<span class="line-modified">! };</span>
  
<span class="line-modified">! template&lt;typename Type, typename Fn, typename D, typename I&gt;</span>
<span class="line-modified">! inline D Atomic::add_using_helper(Fn fn, D volatile* dest, I add_value) {</span>
    return PrimitiveConversions::cast&lt;D&gt;(
      fn(PrimitiveConversions::cast&lt;Type&gt;(add_value),
         reinterpret_cast&lt;Type volatile*&gt;(dest)));
  }
  
<span class="line-modified">! template&lt;typename D, typename U, typename T&gt;</span>
<span class="line-modified">! inline D Atomic::cmpxchg(D volatile* dest,</span>
                           U compare_value,
<span class="line-added">+                          T exchange_value,</span>
                           atomic_memory_order order) {
<span class="line-modified">!   return CmpxchgImpl&lt;D, U, T&gt;()(dest, compare_value, exchange_value, order);</span>
  }
  
<span class="line-modified">! template&lt;typename D, typename T&gt;</span>
<span class="line-modified">! inline bool Atomic::replace_if_null(D* volatile* dest, T* value,</span>
                                      atomic_memory_order order) {
    // Presently using a trivial implementation in terms of cmpxchg.
    // Consider adding platform support, to permit the use of compiler
    // intrinsics like gcc&#39;s __sync_bool_compare_and_swap.
    D* expected_null = NULL;
<span class="line-modified">!   return expected_null == cmpxchg(dest, expected_null, value, order);</span>
  }
  
  // Handle cmpxchg for integral and enum types.
  //
  // All the involved types must be identical.
  template&lt;typename T&gt;
  struct Atomic::CmpxchgImpl&lt;
    T, T, T,
    typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   T operator()(T volatile* dest, T compare_value, T exchange_value,</span>
                 atomic_memory_order order) const {
      // Forward to the platform handler for the size of T.
<span class="line-modified">!     return PlatformCmpxchg&lt;sizeof(T)&gt;()(dest,</span>
                                          compare_value,
<span class="line-added">+                                         exchange_value,</span>
                                          order);
    }
  };
  
  // Handle cmpxchg for pointer types.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 709,25 ***</span>
  // the compare_value.
  //
  // The exchange_value must be implicitly convertible to the
  // destination&#39;s type; it must be type-correct to store the
  // exchange_value in the destination.
<span class="line-modified">! template&lt;typename T, typename D, typename U&gt;</span>
  struct Atomic::CmpxchgImpl&lt;
<span class="line-modified">!   T*, D*, U*,</span>
    typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value &amp;&amp;
                      IsSame&lt;typename RemoveCV&lt;D&gt;::type,
                             typename RemoveCV&lt;U&gt;::type&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   D* operator()(T* exchange_value, D* volatile* dest, U* compare_value,</span>
                 atomic_memory_order order) const {
      // Allow derived to base conversion, and adding cv-qualifiers.
      D* new_value = exchange_value;
      // Don&#39;t care what the CV qualifiers for compare_value are,
      // but we need to match D* when calling platform support.
      D* old_value = const_cast&lt;D*&gt;(compare_value);
<span class="line-modified">!     return PlatformCmpxchg&lt;sizeof(D*)&gt;()(new_value, dest, old_value, order);</span>
    }
  };
  
  // Handle cmpxchg for types that have a translator.
  //
<span class="line-new-header">--- 756,25 ---</span>
  // the compare_value.
  //
  // The exchange_value must be implicitly convertible to the
  // destination&#39;s type; it must be type-correct to store the
  // exchange_value in the destination.
<span class="line-modified">! template&lt;typename D, typename U, typename T&gt;</span>
  struct Atomic::CmpxchgImpl&lt;
<span class="line-modified">!   D*, U*, T*,</span>
    typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value &amp;&amp;
                      IsSame&lt;typename RemoveCV&lt;D&gt;::type,
                             typename RemoveCV&lt;U&gt;::type&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   D* operator()(D* volatile* dest, U* compare_value, T* exchange_value,</span>
                 atomic_memory_order order) const {
      // Allow derived to base conversion, and adding cv-qualifiers.
      D* new_value = exchange_value;
      // Don&#39;t care what the CV qualifiers for compare_value are,
      // but we need to match D* when calling platform support.
      D* old_value = const_cast&lt;D*&gt;(compare_value);
<span class="line-modified">!     return PlatformCmpxchg&lt;sizeof(D*)&gt;()(dest, old_value, new_value, order);</span>
    }
  };
  
  // Handle cmpxchg for types that have a translator.
  //
</pre>
<hr />
<pre>
<span class="line-old-header">*** 739,101 ***</span>
  template&lt;typename T&gt;
  struct Atomic::CmpxchgImpl&lt;
    T, T, T,
    typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   T operator()(T exchange_value, T volatile* dest, T compare_value,</span>
                 atomic_memory_order order) const {
      typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
      typedef typename Translator::Decayed Decayed;
      STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
      return Translator::recover(
<span class="line-modified">!       cmpxchg(Translator::decay(exchange_value),</span>
<span class="line-removed">-               reinterpret_cast&lt;Decayed volatile*&gt;(dest),</span>
                Translator::decay(compare_value),
                order));
    }
  };
  
  template&lt;typename Type, typename Fn, typename T&gt;
  inline T Atomic::cmpxchg_using_helper(Fn fn,
<span class="line-removed">-                                       T exchange_value,</span>
                                        T volatile* dest,
<span class="line-modified">!                                       T compare_value) {</span>
    STATIC_ASSERT(sizeof(Type) == sizeof(T));
    return PrimitiveConversions::cast&lt;T&gt;(
      fn(PrimitiveConversions::cast&lt;Type&gt;(exchange_value),
         reinterpret_cast&lt;Type volatile*&gt;(dest),
         PrimitiveConversions::cast&lt;Type&gt;(compare_value)));
  }
  
  template&lt;typename T&gt;
<span class="line-modified">! inline T Atomic::CmpxchgByteUsingInt::operator()(T exchange_value,</span>
<span class="line-removed">-                                                  T volatile* dest,</span>
                                                   T compare_value,
                                                   atomic_memory_order order) const {
    STATIC_ASSERT(sizeof(T) == sizeof(uint8_t));
    uint8_t canon_exchange_value = exchange_value;
    uint8_t canon_compare_value = compare_value;
    volatile uint32_t* aligned_dest
      = reinterpret_cast&lt;volatile uint32_t*&gt;(align_down(dest, sizeof(uint32_t)));
    size_t offset = pointer_delta(dest, aligned_dest, 1);
<span class="line-modified">!   uint32_t cur = *aligned_dest;</span>
<span class="line-modified">!   uint8_t* cur_as_bytes = reinterpret_cast&lt;uint8_t*&gt;(&amp;cur);</span>
  
    // current value may not be what we are looking for, so force it
    // to that value so the initial cmpxchg will fail if it is different
<span class="line-modified">!   cur_as_bytes[offset] = canon_compare_value;</span>
  
    // always execute a real cmpxchg so that we get the required memory
    // barriers even on initial failure
    do {
<span class="line-modified">!     // value to swap in matches current value ...</span>
<span class="line-modified">!     uint32_t new_value = cur;</span>
<span class="line-modified">!     // ... except for the one byte we want to update</span>
<span class="line-removed">-     reinterpret_cast&lt;uint8_t*&gt;(&amp;new_value)[offset] = canon_exchange_value;</span>
  
<span class="line-modified">!     uint32_t res = cmpxchg(new_value, aligned_dest, cur, order);</span>
      if (res == cur) break;      // success
  
      // at least one byte in the int changed value, so update
      // our view of the current int
      cur = res;
      // if our byte is still as cur we loop and try again
<span class="line-modified">!   } while (cur_as_bytes[offset] == canon_compare_value);</span>
  
<span class="line-modified">!   return PrimitiveConversions::cast&lt;T&gt;(cur_as_bytes[offset]);</span>
  }
  
  // Handle xchg for integral and enum types.
  //
  // All the involved types must be identical.
  template&lt;typename T&gt;
  struct Atomic::XchgImpl&lt;
    T, T,
    typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   T operator()(T exchange_value, T volatile* dest, atomic_memory_order order) const {</span>
      // Forward to the platform handler for the size of T.
<span class="line-modified">!     return PlatformXchg&lt;sizeof(T)&gt;()(exchange_value, dest, order);</span>
    }
  };
  
  // Handle xchg for pointer types.
  //
  // The exchange_value must be implicitly convertible to the
  // destination&#39;s type; it must be type-correct to store the
  // exchange_value in the destination.
<span class="line-modified">! template&lt;typename T, typename D&gt;</span>
  struct Atomic::XchgImpl&lt;
<span class="line-modified">!   T*, D*,</span>
    typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   D* operator()(T* exchange_value, D* volatile* dest, atomic_memory_order order) const {</span>
      // Allow derived to base conversion, and adding cv-qualifiers.
      D* new_value = exchange_value;
<span class="line-modified">!     return PlatformXchg&lt;sizeof(D*)&gt;()(new_value, dest, order);</span>
    }
  };
  
  // Handle xchg for types that have a translator.
  //
<span class="line-new-header">--- 786,115 ---</span>
  template&lt;typename T&gt;
  struct Atomic::CmpxchgImpl&lt;
    T, T, T,
    typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   T operator()(T volatile* dest, T compare_value, T exchange_value,</span>
                 atomic_memory_order order) const {
      typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
      typedef typename Translator::Decayed Decayed;
      STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
      return Translator::recover(
<span class="line-modified">!       cmpxchg(reinterpret_cast&lt;Decayed volatile*&gt;(dest),</span>
                Translator::decay(compare_value),
<span class="line-added">+               Translator::decay(exchange_value),</span>
                order));
    }
  };
  
  template&lt;typename Type, typename Fn, typename T&gt;
  inline T Atomic::cmpxchg_using_helper(Fn fn,
                                        T volatile* dest,
<span class="line-modified">!                                       T compare_value,</span>
<span class="line-added">+                                       T exchange_value) {</span>
    STATIC_ASSERT(sizeof(Type) == sizeof(T));
    return PrimitiveConversions::cast&lt;T&gt;(
      fn(PrimitiveConversions::cast&lt;Type&gt;(exchange_value),
         reinterpret_cast&lt;Type volatile*&gt;(dest),
         PrimitiveConversions::cast&lt;Type&gt;(compare_value)));
  }
  
<span class="line-added">+ inline uint32_t Atomic::CmpxchgByteUsingInt::set_byte_in_int(uint32_t n,</span>
<span class="line-added">+                                                              uint8_t b,</span>
<span class="line-added">+                                                              uint32_t idx) {</span>
<span class="line-added">+   int bitsIdx = BitsPerByte * idx;</span>
<span class="line-added">+   return (n &amp; ~(0xff &lt;&lt; bitsIdx)) | (b &lt;&lt; bitsIdx);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
<span class="line-added">+ inline uint8_t Atomic::CmpxchgByteUsingInt::get_byte_in_int(uint32_t n,</span>
<span class="line-added">+                                                             uint32_t idx) {</span>
<span class="line-added">+   int bitsIdx = BitsPerByte * idx;</span>
<span class="line-added">+   return (uint8_t)(n &gt;&gt; bitsIdx);</span>
<span class="line-added">+ }</span>
<span class="line-added">+ </span>
  template&lt;typename T&gt;
<span class="line-modified">! inline T Atomic::CmpxchgByteUsingInt::operator()(T volatile* dest,</span>
                                                   T compare_value,
<span class="line-added">+                                                  T exchange_value,</span>
                                                   atomic_memory_order order) const {
    STATIC_ASSERT(sizeof(T) == sizeof(uint8_t));
    uint8_t canon_exchange_value = exchange_value;
    uint8_t canon_compare_value = compare_value;
    volatile uint32_t* aligned_dest
      = reinterpret_cast&lt;volatile uint32_t*&gt;(align_down(dest, sizeof(uint32_t)));
    size_t offset = pointer_delta(dest, aligned_dest, 1);
<span class="line-modified">! </span>
<span class="line-modified">!   uint32_t idx = (Endian::NATIVE == Endian::BIG)</span>
<span class="line-added">+                    ? (sizeof(uint32_t) - 1 - offset)</span>
<span class="line-added">+                    : offset;</span>
  
    // current value may not be what we are looking for, so force it
    // to that value so the initial cmpxchg will fail if it is different
<span class="line-modified">!   uint32_t cur = set_byte_in_int(Atomic::load(aligned_dest), canon_compare_value, idx);</span>
  
    // always execute a real cmpxchg so that we get the required memory
    // barriers even on initial failure
    do {
<span class="line-modified">!     // value to swap in matches current value</span>
<span class="line-modified">!     // except for the one byte we want to update</span>
<span class="line-modified">!     uint32_t new_value = set_byte_in_int(cur, canon_exchange_value, idx);</span>
  
<span class="line-modified">!     uint32_t res = cmpxchg(aligned_dest, cur, new_value, order);</span>
      if (res == cur) break;      // success
  
      // at least one byte in the int changed value, so update
      // our view of the current int
      cur = res;
      // if our byte is still as cur we loop and try again
<span class="line-modified">!   } while (get_byte_in_int(cur, idx) == canon_compare_value);</span>
  
<span class="line-modified">!   return PrimitiveConversions::cast&lt;T&gt;(get_byte_in_int(cur, idx));</span>
  }
  
  // Handle xchg for integral and enum types.
  //
  // All the involved types must be identical.
  template&lt;typename T&gt;
  struct Atomic::XchgImpl&lt;
    T, T,
    typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   T operator()(T volatile* dest, T exchange_value, atomic_memory_order order) const {</span>
      // Forward to the platform handler for the size of T.
<span class="line-modified">!     return PlatformXchg&lt;sizeof(T)&gt;()(dest, exchange_value, order);</span>
    }
  };
  
  // Handle xchg for pointer types.
  //
  // The exchange_value must be implicitly convertible to the
  // destination&#39;s type; it must be type-correct to store the
  // exchange_value in the destination.
<span class="line-modified">! template&lt;typename D, typename T&gt;</span>
  struct Atomic::XchgImpl&lt;
<span class="line-modified">!   D*, T*,</span>
    typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   D* operator()(D* volatile* dest, T* exchange_value, atomic_memory_order order) const {</span>
      // Allow derived to base conversion, and adding cv-qualifiers.
      D* new_value = exchange_value;
<span class="line-modified">!     return PlatformXchg&lt;sizeof(D*)&gt;()(dest, new_value, order);</span>
    }
  };
  
  // Handle xchg for types that have a translator.
  //
</pre>
<hr />
<pre>
<span class="line-old-header">*** 845,32 ***</span>
  template&lt;typename T&gt;
  struct Atomic::XchgImpl&lt;
    T, T,
    typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   T operator()(T exchange_value, T volatile* dest, atomic_memory_order order) const {</span>
      typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
      typedef typename Translator::Decayed Decayed;
      STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
      return Translator::recover(
<span class="line-modified">!       xchg(Translator::decay(exchange_value),</span>
<span class="line-modified">!            reinterpret_cast&lt;Decayed volatile*&gt;(dest),</span>
             order));
    }
  };
  
  template&lt;typename Type, typename Fn, typename T&gt;
  inline T Atomic::xchg_using_helper(Fn fn,
<span class="line-modified">!                                    T exchange_value,</span>
<span class="line-modified">!                                    T volatile* dest) {</span>
    STATIC_ASSERT(sizeof(Type) == sizeof(T));
    return PrimitiveConversions::cast&lt;T&gt;(
      fn(PrimitiveConversions::cast&lt;Type&gt;(exchange_value),
         reinterpret_cast&lt;Type volatile*&gt;(dest)));
  }
  
<span class="line-modified">! template&lt;typename T, typename D&gt;</span>
<span class="line-modified">! inline D Atomic::xchg(T exchange_value, volatile D* dest, atomic_memory_order order) {</span>
<span class="line-modified">!   return XchgImpl&lt;T, D&gt;()(exchange_value, dest, order);</span>
  }
  
  #endif // SHARE_RUNTIME_ATOMIC_HPP
<span class="line-new-header">--- 906,33 ---</span>
  template&lt;typename T&gt;
  struct Atomic::XchgImpl&lt;
    T, T,
    typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
  {
<span class="line-modified">!   T operator()(T volatile* dest, T exchange_value, atomic_memory_order order) const {</span>
      typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
      typedef typename Translator::Decayed Decayed;
      STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
      return Translator::recover(
<span class="line-modified">!       xchg(reinterpret_cast&lt;Decayed volatile*&gt;(dest),</span>
<span class="line-modified">!            Translator::decay(exchange_value),</span>
             order));
    }
  };
  
  template&lt;typename Type, typename Fn, typename T&gt;
  inline T Atomic::xchg_using_helper(Fn fn,
<span class="line-modified">!                                    T volatile* dest,</span>
<span class="line-modified">!                                    T exchange_value) {</span>
    STATIC_ASSERT(sizeof(Type) == sizeof(T));
<span class="line-added">+   // Notice the swapped order of arguments. Change when/if stubs are rewritten.</span>
    return PrimitiveConversions::cast&lt;T&gt;(
      fn(PrimitiveConversions::cast&lt;Type&gt;(exchange_value),
         reinterpret_cast&lt;Type volatile*&gt;(dest)));
  }
  
<span class="line-modified">! template&lt;typename D, typename T&gt;</span>
<span class="line-modified">! inline D Atomic::xchg(volatile D* dest, T exchange_value, atomic_memory_order order) {</span>
<span class="line-modified">!   return XchgImpl&lt;D, T&gt;()(dest, exchange_value, order);</span>
  }
  
  #endif // SHARE_RUNTIME_ATOMIC_HPP
</pre>
<center><a href="arguments.hpp.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="basicLock.cpp.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>