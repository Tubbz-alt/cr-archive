<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/share/runtime/compilationPolicy.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;classfile/classLoaderDataGraph.inline.hpp&quot;
 27 #include &quot;code/compiledIC.hpp&quot;
 28 #include &quot;code/nmethod.hpp&quot;
 29 #include &quot;code/scopeDesc.hpp&quot;
 30 #include &quot;interpreter/interpreter.hpp&quot;
 31 #include &quot;memory/resourceArea.hpp&quot;
 32 #include &quot;oops/methodData.hpp&quot;
 33 #include &quot;oops/method.inline.hpp&quot;
 34 #include &quot;oops/oop.inline.hpp&quot;
 35 #include &quot;prims/nativeLookup.hpp&quot;
 36 #include &quot;runtime/compilationPolicy.hpp&quot;
 37 #include &quot;runtime/frame.hpp&quot;
 38 #include &quot;runtime/handles.inline.hpp&quot;
 39 #include &quot;runtime/rframe.hpp&quot;
 40 #include &quot;runtime/stubRoutines.hpp&quot;
 41 #include &quot;runtime/thread.hpp&quot;
 42 #include &quot;runtime/tieredThresholdPolicy.hpp&quot;
 43 #include &quot;runtime/vframe.hpp&quot;
 44 #include &quot;runtime/vmOperations.hpp&quot;
 45 #include &quot;utilities/events.hpp&quot;
 46 #include &quot;utilities/globalDefinitions.hpp&quot;
 47 
 48 #ifdef COMPILER1
 49 #include &quot;c1/c1_Compiler.hpp&quot;
 50 #endif
 51 #ifdef COMPILER2
 52 #include &quot;opto/c2compiler.hpp&quot;
 53 #endif
 54 
 55 CompilationPolicy* CompilationPolicy::_policy;
 56 
 57 // Determine compilation policy based on command line argument
 58 void compilationPolicy_init() {
 59   switch(CompilationPolicyChoice) {
 60   case 0:
 61     CompilationPolicy::set_policy(new SimpleCompPolicy());
 62     break;
 63 
 64   case 1:
 65 #ifdef COMPILER2
 66     CompilationPolicy::set_policy(new StackWalkCompPolicy());
 67 #else
 68     Unimplemented();
 69 #endif
 70     break;
 71   case 2:
 72 #ifdef TIERED
 73     CompilationPolicy::set_policy(new TieredThresholdPolicy());
 74 #else
 75     Unimplemented();
 76 #endif
 77     break;
 78   default:
 79     fatal(&quot;CompilationPolicyChoice must be in the range: [0-2]&quot;);
 80   }
 81   CompilationPolicy::policy()-&gt;initialize();
 82 }
 83 
 84 // Returns true if m must be compiled before executing it
 85 // This is intended to force compiles for methods (usually for
 86 // debugging) that would otherwise be interpreted for some reason.
 87 bool CompilationPolicy::must_be_compiled(const methodHandle&amp; m, int comp_level) {
 88   // Don&#39;t allow Xcomp to cause compiles in replay mode
 89   if (ReplayCompiles) return false;
 90 
 91   if (m-&gt;has_compiled_code()) return false;       // already compiled
 92   if (!can_be_compiled(m, comp_level)) return false;
 93 
 94   return !UseInterpreter ||                                              // must compile all methods
 95          (UseCompiler &amp;&amp; AlwaysCompileLoopMethods &amp;&amp; m-&gt;has_loops() &amp;&amp; CompileBroker::should_compile_new_jobs()); // eagerly compile loop methods
 96 }
 97 
 98 void CompilationPolicy::compile_if_required(const methodHandle&amp; selected_method, TRAPS) {
 99   if (must_be_compiled(selected_method)) {
100     // This path is unusual, mostly used by the &#39;-Xcomp&#39; stress test mode.
101 
102     // Note: with several active threads, the must_be_compiled may be true
103     //       while can_be_compiled is false; remove assert
104     // assert(CompilationPolicy::can_be_compiled(selected_method), &quot;cannot compile&quot;);
105     if (!THREAD-&gt;can_call_java() || THREAD-&gt;is_Compiler_thread()) {
106       // don&#39;t force compilation, resolve was on behalf of compiler
107       return;
108     }
109     if (selected_method-&gt;method_holder()-&gt;is_not_initialized()) {
110       // &#39;is_not_initialized&#39; means not only &#39;!is_initialized&#39;, but also that
111       // initialization has not been started yet (&#39;!being_initialized&#39;)
112       // Do not force compilation of methods in uninitialized classes.
113       // Note that doing this would throw an assert later,
114       // in CompileBroker::compile_method.
115       // We sometimes use the link resolver to do reflective lookups
116       // even before classes are initialized.
117       return;
118     }
119     CompileBroker::compile_method(selected_method, InvocationEntryBci,
120         CompilationPolicy::policy()-&gt;initial_compile_level(),
121         methodHandle(), 0, CompileTask::Reason_MustBeCompiled, CHECK);
122   }
123 }
124 
125 // Returns true if m is allowed to be compiled
126 bool CompilationPolicy::can_be_compiled(const methodHandle&amp; m, int comp_level) {
127   // allow any levels for WhiteBox
128   assert(WhiteBoxAPI || comp_level == CompLevel_all || is_compile(comp_level), &quot;illegal compilation level&quot;);
129 
130   if (m-&gt;is_abstract()) return false;
131   if (DontCompileHugeMethods &amp;&amp; m-&gt;code_size() &gt; HugeMethodLimit) return false;
132 
133   // Math intrinsics should never be compiled as this can lead to
134   // monotonicity problems because the interpreter will prefer the
135   // compiled code to the intrinsic version.  This can&#39;t happen in
136   // production because the invocation counter can&#39;t be incremented
137   // but we shouldn&#39;t expose the system to this problem in testing
138   // modes.
139   if (!AbstractInterpreter::can_be_compiled(m)) {
140     return false;
141   }
142   if (comp_level == CompLevel_all) {
143     if (TieredCompilation) {
144       // enough to be compilable at any level for tiered
145       return !m-&gt;is_not_compilable(CompLevel_simple) || !m-&gt;is_not_compilable(CompLevel_full_optimization);
146     } else {
147       // must be compilable at available level for non-tiered
148       return !m-&gt;is_not_compilable(CompLevel_highest_tier);
149     }
150   } else if (is_compile(comp_level)) {
151     return !m-&gt;is_not_compilable(comp_level);
152   }
153   return false;
154 }
155 
156 // Returns true if m is allowed to be osr compiled
157 bool CompilationPolicy::can_be_osr_compiled(const methodHandle&amp; m, int comp_level) {
158   bool result = false;
159   if (comp_level == CompLevel_all) {
160     if (TieredCompilation) {
161       // enough to be osr compilable at any level for tiered
162       result = !m-&gt;is_not_osr_compilable(CompLevel_simple) || !m-&gt;is_not_osr_compilable(CompLevel_full_optimization);
163     } else {
164       // must be osr compilable at available level for non-tiered
165       result = !m-&gt;is_not_osr_compilable(CompLevel_highest_tier);
166     }
167   } else if (is_compile(comp_level)) {
168     result = !m-&gt;is_not_osr_compilable(comp_level);
169   }
170   return (result &amp;&amp; can_be_compiled(m, comp_level));
171 }
172 
173 bool CompilationPolicy::is_compilation_enabled() {
174   // NOTE: CompileBroker::should_compile_new_jobs() checks for UseCompiler
175   return CompileBroker::should_compile_new_jobs();
176 }
177 
178 CompileTask* CompilationPolicy::select_task_helper(CompileQueue* compile_queue) {
179   // Remove unloaded methods from the queue
180   for (CompileTask* task = compile_queue-&gt;first(); task != NULL; ) {
181     CompileTask* next = task-&gt;next();
182     if (task-&gt;is_unloaded()) {
183       compile_queue-&gt;remove_and_mark_stale(task);
184     }
185     task = next;
186   }
187 #if INCLUDE_JVMCI
188   if (UseJVMCICompiler &amp;&amp; !BackgroundCompilation) {
189     /*
190      * In blocking compilation mode, the CompileBroker will make
191      * compilations submitted by a JVMCI compiler thread non-blocking. These
192      * compilations should be scheduled after all blocking compilations
193      * to service non-compiler related compilations sooner and reduce the
194      * chance of such compilations timing out.
195      */
196     for (CompileTask* task = compile_queue-&gt;first(); task != NULL; task = task-&gt;next()) {
197       if (task-&gt;is_blocking()) {
198         return task;
199       }
200     }
201   }
202 #endif
203   return compile_queue-&gt;first();
204 }
205 
206 #ifndef PRODUCT
207 void NonTieredCompPolicy::trace_osr_completion(nmethod* osr_nm) {
208   if (TraceOnStackReplacement) {
209     if (osr_nm == NULL) tty-&gt;print_cr(&quot;compilation failed&quot;);
210     else tty-&gt;print_cr(&quot;nmethod &quot; INTPTR_FORMAT, p2i(osr_nm));
211   }
212 }
213 #endif // !PRODUCT
214 
215 void NonTieredCompPolicy::initialize() {
216   // Setup the compiler thread numbers
217   if (CICompilerCountPerCPU) {
218     // Example: if CICompilerCountPerCPU is true, then we get
219     // max(log2(8)-1,1) = 2 compiler threads on an 8-way machine.
220     // May help big-app startup time.
221     _compiler_count = MAX2(log2_int(os::active_processor_count())-1,1);
222     // Make sure there is enough space in the code cache to hold all the compiler buffers
223     size_t buffer_size = 1;
224 #ifdef COMPILER1
225     buffer_size = is_client_compilation_mode_vm() ? Compiler::code_buffer_size() : buffer_size;
226 #endif
227 #ifdef COMPILER2
228     buffer_size = is_server_compilation_mode_vm() ? C2Compiler::initial_code_buffer_size() : buffer_size;
229 #endif
230     int max_count = (ReservedCodeCacheSize - (CodeCacheMinimumUseSpace DEBUG_ONLY(* 3))) / (int)buffer_size;
231     if (_compiler_count &gt; max_count) {
232       // Lower the compiler count such that all buffers fit into the code cache
233       _compiler_count = MAX2(max_count, 1);
234     }
235     FLAG_SET_ERGO(intx, CICompilerCount, _compiler_count);
236   } else {
237     _compiler_count = CICompilerCount;
238   }
239 }
240 
241 // Note: this policy is used ONLY if TieredCompilation is off.
242 // compiler_count() behaves the following way:
243 // - with TIERED build (with both COMPILER1 and COMPILER2 defined) it should return
244 //   zero for the c1 compilation levels in server compilation mode runs
245 //   and c2 compilation levels in client compilation mode runs.
246 // - with COMPILER2 not defined it should return zero for c2 compilation levels.
247 // - with COMPILER1 not defined it should return zero for c1 compilation levels.
248 // - if neither is defined - always return zero.
249 int NonTieredCompPolicy::compiler_count(CompLevel comp_level) {
250   assert(!TieredCompilation, &quot;This policy should not be used with TieredCompilation&quot;);
251   if (COMPILER2_PRESENT(is_server_compilation_mode_vm() &amp;&amp; is_c2_compile(comp_level) ||)
252       is_client_compilation_mode_vm() &amp;&amp; is_c1_compile(comp_level)) {
253     return _compiler_count;
254   }
255   return 0;
256 }
257 
258 void NonTieredCompPolicy::reset_counter_for_invocation_event(const methodHandle&amp; m) {
259   // Make sure invocation and backedge counter doesn&#39;t overflow again right away
260   // as would be the case for native methods.
261 
262   // BUT also make sure the method doesn&#39;t look like it was never executed.
263   // Set carry bit and reduce counter&#39;s value to min(count, CompileThreshold/2).
264   MethodCounters* mcs = m-&gt;method_counters();
265   assert(mcs != NULL, &quot;MethodCounters cannot be NULL for profiling&quot;);
266   mcs-&gt;invocation_counter()-&gt;set_carry();
267   mcs-&gt;backedge_counter()-&gt;set_carry();
268 
269   assert(!m-&gt;was_never_executed(), &quot;don&#39;t reset to 0 -- could be mistaken for never-executed&quot;);
270 }
271 
272 void NonTieredCompPolicy::reset_counter_for_back_branch_event(const methodHandle&amp; m) {
273   // Delay next back-branch event but pump up invocation counter to trigger
274   // whole method compilation.
275   MethodCounters* mcs = m-&gt;method_counters();
276   assert(mcs != NULL, &quot;MethodCounters cannot be NULL for profiling&quot;);
277   InvocationCounter* i = mcs-&gt;invocation_counter();
278   InvocationCounter* b = mcs-&gt;backedge_counter();
279 
280   // Don&#39;t set invocation_counter&#39;s value too low otherwise the method will
281   // look like immature (ic &lt; ~5300) which prevents the inlining based on
282   // the type profiling.
283   i-&gt;set(i-&gt;state(), CompileThreshold);
284   // Don&#39;t reset counter too low - it is used to check if OSR method is ready.
285   b-&gt;set(b-&gt;state(), CompileThreshold / 2);
286 }
287 
288 //
289 // CounterDecay
290 //
291 // Iterates through invocation counters and decrements them. This
292 // is done at each safepoint.
293 //
294 class CounterDecay : public AllStatic {
295   static jlong _last_timestamp;
296   static void do_method(Method* m) {
297     MethodCounters* mcs = m-&gt;method_counters();
298     if (mcs != NULL) {
299       mcs-&gt;invocation_counter()-&gt;decay();
300     }
301   }
302 public:
303   static void decay();
304   static bool is_decay_needed() {
305     return (os::javaTimeMillis() - _last_timestamp) &gt; CounterDecayMinIntervalLength;
306   }
307 };
308 
309 jlong CounterDecay::_last_timestamp = 0;
310 
311 void CounterDecay::decay() {
312   _last_timestamp = os::javaTimeMillis();
313 
314   // This operation is going to be performed only at the end of a safepoint
315   // and hence GC&#39;s will not be going on, all Java mutators are suspended
316   // at this point and hence SystemDictionary_lock is also not needed.
317   assert(SafepointSynchronize::is_at_safepoint(), &quot;can only be executed at a safepoint&quot;);
318   size_t nclasses = ClassLoaderDataGraph::num_instance_classes();
319   size_t classes_per_tick = nclasses * (CounterDecayMinIntervalLength * 1e-3 /
320                                         CounterHalfLifeTime);
321   for (size_t i = 0; i &lt; classes_per_tick; i++) {
322     InstanceKlass* k = ClassLoaderDataGraph::try_get_next_class();
323     if (k != NULL) {
324       k-&gt;methods_do(do_method);
325     }
326   }
327 }
328 
329 // Called at the end of the safepoint
330 void NonTieredCompPolicy::do_safepoint_work() {
331   if(UseCounterDecay &amp;&amp; CounterDecay::is_decay_needed()) {
332     CounterDecay::decay();
333   }
334 }
335 
336 void NonTieredCompPolicy::reprofile(ScopeDesc* trap_scope, bool is_osr) {
337   ScopeDesc* sd = trap_scope;
338   MethodCounters* mcs;
339   InvocationCounter* c;
340   for (; !sd-&gt;is_top(); sd = sd-&gt;sender()) {
341     mcs = sd-&gt;method()-&gt;method_counters();
342     if (mcs != NULL) {
343       // Reset ICs of inlined methods, since they can trigger compilations also.
344       mcs-&gt;invocation_counter()-&gt;reset();
345     }
346   }
347   mcs = sd-&gt;method()-&gt;method_counters();
348   if (mcs != NULL) {
349     c = mcs-&gt;invocation_counter();
350     if (is_osr) {
351       // It was an OSR method, so bump the count higher.
352       c-&gt;set(c-&gt;state(), CompileThreshold);
353     } else {
354       c-&gt;reset();
355     }
356     mcs-&gt;backedge_counter()-&gt;reset();
357   }
358 }
359 
360 // This method can be called by any component of the runtime to notify the policy
361 // that it&#39;s recommended to delay the compilation of this method.
362 void NonTieredCompPolicy::delay_compilation(Method* method) {
363   MethodCounters* mcs = method-&gt;method_counters();
364   if (mcs != NULL) {
365     mcs-&gt;invocation_counter()-&gt;decay();
366     mcs-&gt;backedge_counter()-&gt;decay();
367   }
368 }
369 
370 void NonTieredCompPolicy::disable_compilation(Method* method) {
371   MethodCounters* mcs = method-&gt;method_counters();
372   if (mcs != NULL) {
373     mcs-&gt;invocation_counter()-&gt;set_state(InvocationCounter::wait_for_nothing);
374     mcs-&gt;backedge_counter()-&gt;set_state(InvocationCounter::wait_for_nothing);
375   }
376 }
377 
378 CompileTask* NonTieredCompPolicy::select_task(CompileQueue* compile_queue) {
379   return select_task_helper(compile_queue);
380 }
381 
382 bool NonTieredCompPolicy::is_mature(Method* method) {
383   MethodData* mdo = method-&gt;method_data();
384   assert(mdo != NULL, &quot;Should be&quot;);
385   uint current = mdo-&gt;mileage_of(method);
386   uint initial = mdo-&gt;creation_mileage();
387   if (current &lt; initial)
388     return true;  // some sort of overflow
389   uint target;
390   if (ProfileMaturityPercentage &lt;= 0)
391     target = (uint) -ProfileMaturityPercentage;  // absolute value
392   else
393     target = (uint)( (ProfileMaturityPercentage * CompileThreshold) / 100 );
394   return (current &gt;= initial + target);
395 }
396 
397 nmethod* NonTieredCompPolicy::event(const methodHandle&amp; method, const methodHandle&amp; inlinee, int branch_bci,
398                                     int bci, CompLevel comp_level, CompiledMethod* nm, JavaThread* thread) {
399   assert(comp_level == CompLevel_none, &quot;This should be only called from the interpreter&quot;);
400   NOT_PRODUCT(trace_frequency_counter_overflow(method, branch_bci, bci));
401   if (JvmtiExport::can_post_interpreter_events() &amp;&amp; thread-&gt;is_interp_only_mode()) {
402     // If certain JVMTI events (e.g. frame pop event) are requested then the
403     // thread is forced to remain in interpreted code. This is
404     // implemented partly by a check in the run_compiled_code
405     // section of the interpreter whether we should skip running
406     // compiled code, and partly by skipping OSR compiles for
407     // interpreted-only threads.
408     if (bci != InvocationEntryBci) {
409       reset_counter_for_back_branch_event(method);
410       return NULL;
411     }
412   }
413   if (ReplayCompiles) {
414     // Don&#39;t trigger other compiles in testing mode
415     if (bci == InvocationEntryBci) {
416       reset_counter_for_invocation_event(method);
417     } else {
418       reset_counter_for_back_branch_event(method);
419     }
420     return NULL;
421   }
422 
423   if (bci == InvocationEntryBci) {
424     // when code cache is full, compilation gets switched off, UseCompiler
425     // is set to false
426     if (!method-&gt;has_compiled_code() &amp;&amp; UseCompiler) {
427       method_invocation_event(method, thread);
428     } else {
429       // Force counter overflow on method entry, even if no compilation
430       // happened.  (The method_invocation_event call does this also.)
431       reset_counter_for_invocation_event(method);
432     }
433     // compilation at an invocation overflow no longer goes and retries test for
434     // compiled method. We always run the loser of the race as interpreted.
435     // so return NULL
436     return NULL;
437   } else {
438     // counter overflow in a loop =&gt; try to do on-stack-replacement
439     nmethod* osr_nm = method-&gt;lookup_osr_nmethod_for(bci, CompLevel_highest_tier, true);
440     NOT_PRODUCT(trace_osr_request(method, osr_nm, bci));
441     // when code cache is full, we should not compile any more...
442     if (osr_nm == NULL &amp;&amp; UseCompiler) {
443       method_back_branch_event(method, bci, thread);
444       osr_nm = method-&gt;lookup_osr_nmethod_for(bci, CompLevel_highest_tier, true);
445     }
446     if (osr_nm == NULL) {
447       reset_counter_for_back_branch_event(method);
448       return NULL;
449     }
450     return osr_nm;
451   }
452   return NULL;
453 }
454 
455 #ifndef PRODUCT
456 void NonTieredCompPolicy::trace_frequency_counter_overflow(const methodHandle&amp; m, int branch_bci, int bci) {
457   if (TraceInvocationCounterOverflow) {
458     MethodCounters* mcs = m-&gt;method_counters();
459     assert(mcs != NULL, &quot;MethodCounters cannot be NULL for profiling&quot;);
460     InvocationCounter* ic = mcs-&gt;invocation_counter();
461     InvocationCounter* bc = mcs-&gt;backedge_counter();
462     ResourceMark rm;
463     if (bci == InvocationEntryBci) {
464       tty-&gt;print(&quot;comp-policy cntr ovfl @ %d in entry of &quot;, bci);
465     } else {
466       tty-&gt;print(&quot;comp-policy cntr ovfl @ %d in loop of &quot;, bci);
467     }
468     m-&gt;print_value();
469     tty-&gt;cr();
470     ic-&gt;print();
471     bc-&gt;print();
472     if (ProfileInterpreter) {
473       if (bci != InvocationEntryBci) {
474         MethodData* mdo = m-&gt;method_data();
475         if (mdo != NULL) {
476           ProfileData *pd = mdo-&gt;bci_to_data(branch_bci);
477           if (pd == NULL) {
478             tty-&gt;print_cr(&quot;back branch count = N/A (missing ProfileData)&quot;);
479           } else {
480             tty-&gt;print_cr(&quot;back branch count = %d&quot;, pd-&gt;as_JumpData()-&gt;taken());
481           }
482         }
483       }
484     }
485   }
486 }
487 
488 void NonTieredCompPolicy::trace_osr_request(const methodHandle&amp; method, nmethod* osr, int bci) {
489   if (TraceOnStackReplacement) {
490     ResourceMark rm;
491     tty-&gt;print(osr != NULL ? &quot;Reused OSR entry for &quot; : &quot;Requesting OSR entry for &quot;);
492     method-&gt;print_short_name(tty);
493     tty-&gt;print_cr(&quot; at bci %d&quot;, bci);
494   }
495 }
496 #endif // !PRODUCT
497 
498 // SimpleCompPolicy - compile current method
499 
500 void SimpleCompPolicy::method_invocation_event(const methodHandle&amp; m, JavaThread* thread) {
501   const int comp_level = CompLevel_highest_tier;
502   const int hot_count = m-&gt;invocation_count();
503   reset_counter_for_invocation_event(m);
504 
505   if (is_compilation_enabled() &amp;&amp; can_be_compiled(m, comp_level)) {
506     CompiledMethod* nm = m-&gt;code();
507     if (nm == NULL ) {
508       CompileBroker::compile_method(m, InvocationEntryBci, comp_level, m, hot_count, CompileTask::Reason_InvocationCount, thread);
509     }
510   }
511 }
512 
513 void SimpleCompPolicy::method_back_branch_event(const methodHandle&amp; m, int bci, JavaThread* thread) {
514   const int comp_level = CompLevel_highest_tier;
515   const int hot_count = m-&gt;backedge_count();
516 
517   if (is_compilation_enabled() &amp;&amp; can_be_osr_compiled(m, comp_level)) {
518     CompileBroker::compile_method(m, bci, comp_level, m, hot_count, CompileTask::Reason_BackedgeCount, thread);
519     NOT_PRODUCT(trace_osr_completion(m-&gt;lookup_osr_nmethod_for(bci, comp_level, true));)
520   }
521 }
522 // StackWalkCompPolicy - walk up stack to find a suitable method to compile
523 
524 #ifdef COMPILER2
525 const char* StackWalkCompPolicy::_msg = NULL;
526 
527 
528 // Consider m for compilation
529 void StackWalkCompPolicy::method_invocation_event(const methodHandle&amp; m, JavaThread* thread) {
530   const int comp_level = CompLevel_highest_tier;
531   const int hot_count = m-&gt;invocation_count();
532   reset_counter_for_invocation_event(m);
533 
534   if (is_compilation_enabled() &amp;&amp; m-&gt;code() == NULL &amp;&amp; can_be_compiled(m, comp_level)) {
535     ResourceMark rm(thread);
536     frame       fr     = thread-&gt;last_frame();
537     assert(fr.is_interpreted_frame(), &quot;must be interpreted&quot;);
538     assert(fr.interpreter_frame_method() == m(), &quot;bad method&quot;);
539 
540     RegisterMap reg_map(thread, false);
541     javaVFrame* triggerVF = thread-&gt;last_java_vframe(&amp;reg_map);
542     // triggerVF is the frame that triggered its counter
543     RFrame* first = new InterpretedRFrame(triggerVF-&gt;fr(), thread, m());
544 
545     if (first-&gt;top_method()-&gt;code() != NULL) {
546       // called obsolete method/nmethod -- no need to recompile
547     } else {
548       GrowableArray&lt;RFrame*&gt;* stack = new GrowableArray&lt;RFrame*&gt;(50);
549       stack-&gt;push(first);
550       RFrame* top = findTopInlinableFrame(stack);
551       assert(top != NULL, &quot;findTopInlinableFrame returned null&quot;);
552       CompileBroker::compile_method(top-&gt;top_method(), InvocationEntryBci, comp_level,
553                                     m, hot_count, CompileTask::Reason_InvocationCount, thread);
554     }
555   }
556 }
557 
558 void StackWalkCompPolicy::method_back_branch_event(const methodHandle&amp; m, int bci, JavaThread* thread) {
559   const int comp_level = CompLevel_highest_tier;
560   const int hot_count = m-&gt;backedge_count();
561 
562   if (is_compilation_enabled() &amp;&amp; can_be_osr_compiled(m, comp_level)) {
563     CompileBroker::compile_method(m, bci, comp_level, m, hot_count, CompileTask::Reason_BackedgeCount, thread);
564     NOT_PRODUCT(trace_osr_completion(m-&gt;lookup_osr_nmethod_for(bci, comp_level, true));)
565   }
566 }
567 
568 RFrame* StackWalkCompPolicy::findTopInlinableFrame(GrowableArray&lt;RFrame*&gt;* stack) {
569   // go up the stack until finding a frame that (probably) won&#39;t be inlined
570   // into its caller
571   RFrame* current = stack-&gt;at(0); // current choice for stopping
572   assert( current &amp;&amp; !current-&gt;is_compiled(), &quot;&quot; );
573   const char* msg = NULL;
574 
575   while (1) {
576 
577     // before going up the stack further, check if doing so would get us into
578     // compiled code
579     RFrame* next = senderOf(current, stack);
580     if( !next )               // No next frame up the stack?
581       break;                  // Then compile with current frame
582 
583     Method* m = current-&gt;top_method();
584     Method* next_m = next-&gt;top_method();
585 
586     if( !Inline ) {           // Inlining turned off
587       msg = &quot;Inlining turned off&quot;;
588       break;
589     }
590     if (next_m-&gt;is_not_compilable()) { // Did fail to compile this before/
591       msg = &quot;caller not compilable&quot;;
592       break;
593     }
594     if (next-&gt;num() &gt; MaxRecompilationSearchLength) {
595       // don&#39;t go up too high when searching for recompilees
596       msg = &quot;don&#39;t go up any further: &gt; MaxRecompilationSearchLength&quot;;
597       break;
598     }
599     if (next-&gt;distance() &gt; MaxInterpretedSearchLength) {
600       // don&#39;t go up too high when searching for recompilees
601       msg = &quot;don&#39;t go up any further: next &gt; MaxInterpretedSearchLength&quot;;
602       break;
603     }
604     // Compiled frame above already decided not to inline;
605     // do not recompile him.
606     if (next-&gt;is_compiled()) {
607       msg = &quot;not going up into optimized code&quot;;
608       break;
609     }
610 
611     // Interpreted frame above us was already compiled.  Do not force
612     // a recompile, although if the frame above us runs long enough an
613     // OSR might still happen.
614     if( current-&gt;is_interpreted() &amp;&amp; next_m-&gt;has_compiled_code() ) {
615       msg = &quot;not going up -- already compiled caller&quot;;
616       break;
617     }
618 
619     // Compute how frequent this call site is.  We have current method &#39;m&#39;.
620     // We know next method &#39;next_m&#39; is interpreted.  Find the call site and
621     // check the various invocation counts.
622     int invcnt = 0;             // Caller counts
623     if (ProfileInterpreter) {
624       invcnt = next_m-&gt;interpreter_invocation_count();
625     }
626     int cnt = 0;                // Call site counts
627     if (ProfileInterpreter &amp;&amp; next_m-&gt;method_data() != NULL) {
628       ResourceMark rm;
629       int bci = next-&gt;top_vframe()-&gt;bci();
630       ProfileData* data = next_m-&gt;method_data()-&gt;bci_to_data(bci);
631       if (data != NULL &amp;&amp; data-&gt;is_CounterData())
632         cnt = data-&gt;as_CounterData()-&gt;count();
633     }
634 
635     // Caller counts / call-site counts; i.e. is this call site
636     // a hot call site for method next_m?
637     int freq = (invcnt) ? cnt/invcnt : cnt;
638 
639     // Check size and frequency limits
640     if ((msg = shouldInline(m, freq, cnt)) != NULL) {
641       break;
642     }
643     // Check inlining negative tests
644     if ((msg = shouldNotInline(m)) != NULL) {
645       break;
646     }
647 
648 
649     // If the caller method is too big or something then we do not want to
650     // compile it just to inline a method
651     if (!can_be_compiled(next_m, CompLevel_any)) {
652       msg = &quot;caller cannot be compiled&quot;;
653       break;
654     }
655 
656     if( next_m-&gt;name() == vmSymbols::class_initializer_name() ) {
657       msg = &quot;do not compile class initializer (OSR ok)&quot;;
658       break;
659     }
660 
661     current = next;
662   }
663 
664   assert( !current || !current-&gt;is_compiled(), &quot;&quot; );
665 
666   return current;
667 }
668 
669 RFrame* StackWalkCompPolicy::senderOf(RFrame* rf, GrowableArray&lt;RFrame*&gt;* stack) {
670   RFrame* sender = rf-&gt;caller();
671   if (sender &amp;&amp; sender-&gt;num() == stack-&gt;length()) stack-&gt;push(sender);
672   return sender;
673 }
674 
675 
676 const char* StackWalkCompPolicy::shouldInline(const methodHandle&amp; m, float freq, int cnt) {
677   // Allows targeted inlining
678   // positive filter: should send be inlined?  returns NULL (--&gt; yes)
679   // or rejection msg
680   int max_size = MaxInlineSize;
681   int cost = m-&gt;code_size();
682 
683   // Check for too many throws (and not too huge)
684   if (m-&gt;interpreter_throwout_count() &gt; InlineThrowCount &amp;&amp; cost &lt; InlineThrowMaxSize ) {
685     return NULL;
686   }
687 
688   // bump the max size if the call is frequent
689   if ((freq &gt;= InlineFrequencyRatio) || (cnt &gt;= InlineFrequencyCount)) {
690     if (TraceFrequencyInlining) {
691       tty-&gt;print(&quot;(Inlined frequent method)\n&quot;);
692       m-&gt;print();
693     }
694     max_size = FreqInlineSize;
695   }
696   if (cost &gt; max_size) {
697     return (_msg = &quot;too big&quot;);
698   }
699   return NULL;
700 }
701 
702 
703 const char* StackWalkCompPolicy::shouldNotInline(const methodHandle&amp; m) {
704   // negative filter: should send NOT be inlined?  returns NULL (--&gt; inline) or rejection msg
705   if (m-&gt;is_abstract()) return (_msg = &quot;abstract method&quot;);
706   // note: we allow ik-&gt;is_abstract()
707   if (!m-&gt;method_holder()-&gt;is_initialized()) return (_msg = &quot;method holder not initialized&quot;);
708   if (m-&gt;is_native()) return (_msg = &quot;native method&quot;);
709   CompiledMethod* m_code = m-&gt;code();
710   if (m_code != NULL &amp;&amp; m_code-&gt;code_size() &gt; InlineSmallCode)
711     return (_msg = &quot;already compiled into a big method&quot;);
712 
713   // use frequency-based objections only for non-trivial methods
714   if (m-&gt;code_size() &lt;= MaxTrivialSize) return NULL;
715   if (UseInterpreter) {     // don&#39;t use counts with -Xcomp
716     if ((m-&gt;code() == NULL) &amp;&amp; m-&gt;was_never_executed()) return (_msg = &quot;never executed&quot;);
717     if (!m-&gt;was_executed_more_than(MIN2(MinInliningThreshold, CompileThreshold &gt;&gt; 1))) return (_msg = &quot;executed &lt; MinInliningThreshold times&quot;);
718   }
719   if (Method::has_unloaded_classes_in_signature(m, JavaThread::current())) return (_msg = &quot;unloaded signature classes&quot;);
720 
721   return NULL;
722 }
723 
724 
725 
726 #endif // COMPILER2
    </pre>
  </body>
</html>