<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/runtime/atomic.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="arguments.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="basicLock.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/runtime/atomic.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_RUNTIME_ATOMIC_HPP
 26 #define SHARE_RUNTIME_ATOMIC_HPP
 27 
 28 #include &quot;memory/allocation.hpp&quot;
 29 #include &quot;metaprogramming/conditional.hpp&quot;
 30 #include &quot;metaprogramming/enableIf.hpp&quot;
 31 #include &quot;metaprogramming/isIntegral.hpp&quot;
 32 #include &quot;metaprogramming/isPointer.hpp&quot;
 33 #include &quot;metaprogramming/isSame.hpp&quot;
 34 #include &quot;metaprogramming/primitiveConversions.hpp&quot;
 35 #include &quot;metaprogramming/removeCV.hpp&quot;
 36 #include &quot;metaprogramming/removePointer.hpp&quot;

 37 #include &quot;utilities/align.hpp&quot;

 38 #include &quot;utilities/macros.hpp&quot;
 39 
 40 enum atomic_memory_order {
 41   // The modes that align with C++11 are intended to
 42   // follow the same semantics.
 43   memory_order_relaxed = 0,
 44   memory_order_acquire = 2,
 45   memory_order_release = 3,
 46   memory_order_acq_rel = 4,
 47   // Strong two-way memory barrier.
 48   memory_order_conservative = 8
 49 };
 50 






 51 class Atomic : AllStatic {
 52 public:
 53   // Atomic operations on int64 types are not available on all 32-bit
 54   // platforms. If atomic ops on int64 are defined here they must only
 55   // be used from code that verifies they are available at runtime and
 56   // can provide an alternative action if not - see supports_cx8() for
 57   // a means to test availability.
 58 
 59   // The memory operations that are mentioned with each of the atomic
 60   // function families come from src/share/vm/runtime/orderAccess.hpp,
 61   // e.g., &lt;fence&gt; is described in that file and is implemented by the
 62   // OrderAccess::fence() function. See that file for the gory details
 63   // on the Memory Access Ordering Model.
 64 
 65   // All of the atomic operations that imply a read-modify-write action
 66   // guarantee a two-way memory barrier across that operation. Historically
 67   // these semantics reflect the strength of atomic operations that are
 68   // provided on SPARC/X86. We assume that strength is necessary unless
 69   // we can prove that a weaker form is sufficiently safe.
 70 
 71   // Atomically store to a location
 72   // The type T must be either a pointer type convertible to or equal
 73   // to D, an integral/enum type equal to D, or a type equal to D that
 74   // is primitive convertible using PrimitiveConversions.
<span class="line-modified"> 75   template&lt;typename T, typename D&gt;</span>
<span class="line-modified"> 76   inline static void store(T store_value, volatile D* dest);</span>






 77 
 78   // Atomically load from a location
 79   // The type T must be either a pointer type, an integral/enum type,
 80   // or a type that is primitive convertible using PrimitiveConversions.
 81   template&lt;typename T&gt;
 82   inline static T load(const volatile T* dest);
 83 
<span class="line-modified"> 84   // Atomically add to a location. Returns updated value. add*() provide:</span>



 85   // &lt;fence&gt; add-value-to-dest &lt;membar StoreLoad|StoreStore&gt;
 86 
<span class="line-modified"> 87   template&lt;typename I, typename D&gt;</span>
<span class="line-modified"> 88   inline static D add(I add_value, D volatile* dest,</span>

 89                       atomic_memory_order order = memory_order_conservative);
 90 
<span class="line-modified"> 91   template&lt;typename I, typename D&gt;</span>
<span class="line-modified"> 92   inline static D sub(I sub_value, D volatile* dest,</span>





 93                       atomic_memory_order order = memory_order_conservative);
 94 
 95   // Atomically increment location. inc() provide:
 96   // &lt;fence&gt; increment-dest &lt;membar StoreLoad|StoreStore&gt;
 97   // The type D may be either a pointer type, or an integral
 98   // type. If it is a pointer type, then the increment is
 99   // scaled to the size of the type pointed to by the pointer.
100   template&lt;typename D&gt;
101   inline static void inc(D volatile* dest,
102                          atomic_memory_order order = memory_order_conservative);
103 
104   // Atomically decrement a location. dec() provide:
105   // &lt;fence&gt; decrement-dest &lt;membar StoreLoad|StoreStore&gt;
106   // The type D may be either a pointer type, or an integral
107   // type. If it is a pointer type, then the decrement is
108   // scaled to the size of the type pointed to by the pointer.
109   template&lt;typename D&gt;
110   inline static void dec(D volatile* dest,
111                          atomic_memory_order order = memory_order_conservative);
112 
113   // Performs atomic exchange of *dest with exchange_value. Returns old
114   // prior value of *dest. xchg*() provide:
115   // &lt;fence&gt; exchange-value-with-dest &lt;membar StoreLoad|StoreStore&gt;
116   // The type T must be either a pointer type convertible to or equal
117   // to D, an integral/enum type equal to D, or a type equal to D that
118   // is primitive convertible using PrimitiveConversions.
<span class="line-modified">119   template&lt;typename T, typename D&gt;</span>
<span class="line-modified">120   inline static D xchg(T exchange_value, volatile D* dest,</span>
121                        atomic_memory_order order = memory_order_conservative);
122 
123   // Performs atomic compare of *dest and compare_value, and exchanges
124   // *dest with exchange_value if the comparison succeeded. Returns prior
125   // value of *dest. cmpxchg*() provide:
126   // &lt;fence&gt; compare-and-exchange &lt;membar StoreLoad|StoreStore&gt;
127 
<span class="line-modified">128   template&lt;typename T, typename D, typename U&gt;</span>
<span class="line-modified">129   inline static D cmpxchg(T exchange_value,</span>
<span class="line-removed">130                           D volatile* dest,</span>
131                           U compare_value,

132                           atomic_memory_order order = memory_order_conservative);
133 
134   // Performs atomic compare of *dest and NULL, and replaces *dest
135   // with exchange_value if the comparison succeeded.  Returns true if
136   // the comparison succeeded and the exchange occurred.  This is
137   // often used as part of lazy initialization, as a lock-free
138   // alternative to the Double-Checked Locking Pattern.
<span class="line-modified">139   template&lt;typename T, typename D&gt;</span>
<span class="line-modified">140   inline static bool replace_if_null(T* value, D* volatile* dest,</span>
141                                      atomic_memory_order order = memory_order_conservative);
142 
143 private:
144 WINDOWS_ONLY(public:) // VS2017 warns (C2027) use of undefined type if IsPointerConvertible is declared private
145   // Test whether From is implicitly convertible to To.
146   // From and To must be pointer types.
147   // Note: Provides the limited subset of C++11 std::is_convertible
148   // that is needed here.
149   template&lt;typename From, typename To&gt; struct IsPointerConvertible;
150 
151 protected:
152   // Dispatch handler for store.  Provides type-based validity
153   // checking and limited conversions around calls to the platform-
154   // specific implementation layer provided by PlatformOp.
<span class="line-modified">155   template&lt;typename T, typename D, typename PlatformOp, typename Enable = void&gt;</span>
156   struct StoreImpl;
157 
158   // Platform-specific implementation of store.  Support for sizes
159   // of 1, 2, 4, and (if different) pointer size bytes are required.
160   // The class is a function object that must be default constructable,
161   // with these requirements:
162   //
163   // either:
164   // - dest is of type D*, an integral, enum or pointer type.
165   // - new_value are of type T, an integral, enum or pointer type D or
166   //   pointer type convertible to D.
167   // or:
168   // - T and D are the same and are primitive convertible using PrimitiveConversions
169   // and either way:
170   // - platform_store is an object of type PlatformStore&lt;sizeof(T)&gt;.
171   //
172   // Then
173   //   platform_store(new_value, dest)
174   // must be a valid expression.
175   //
</pre>
<hr />
<pre>
183   template&lt;typename T, typename PlatformOp, typename Enable = void&gt;
184   struct LoadImpl;
185 
186   // Platform-specific implementation of load. Support for sizes of
187   // 1, 2, 4 bytes and (if different) pointer size bytes are required.
188   // The class is a function object that must be default
189   // constructable, with these requirements:
190   //
191   // - dest is of type T*, an integral, enum or pointer type, or
192   //   T is convertible to a primitive type using PrimitiveConversions
193   // - platform_load is an object of type PlatformLoad&lt;sizeof(T)&gt;.
194   //
195   // Then
196   //   platform_load(src)
197   // must be a valid expression, returning a result convertible to T.
198   //
199   // The default implementation is a volatile load. If a platform
200   // requires more for e.g. 64 bit loads, a specialization is required
201   template&lt;size_t byte_size&gt; struct PlatformLoad;
202 




203 private:
204   // Dispatch handler for add.  Provides type-based validity checking
205   // and limited conversions around calls to the platform-specific
206   // implementation layer provided by PlatformAdd.
<span class="line-modified">207   template&lt;typename I, typename D, typename Enable = void&gt;</span>
208   struct AddImpl;
209 
210   // Platform-specific implementation of add.  Support for sizes of 4
211   // bytes and (if different) pointer size bytes are required.  The
<span class="line-modified">212   // class is a function object that must be default constructable,</span>
<span class="line-removed">213   // with these requirements:</span>
214   //
215   // - dest is of type D*, an integral or pointer type.
216   // - add_value is of type I, an integral type.
217   // - sizeof(I) == sizeof(D).
218   // - if D is an integral type, I == D.

219   // - platform_add is an object of type PlatformAdd&lt;sizeof(D)&gt;.
220   //
<span class="line-modified">221   // Then</span>
<span class="line-modified">222   //   platform_add(add_value, dest)</span>
<span class="line-modified">223   // must be a valid expression, returning a result convertible to D.</span>
<span class="line-modified">224   //</span>
<span class="line-removed">225   // No definition is provided; all platforms must explicitly define</span>
<span class="line-removed">226   // this class and any needed specializations.</span>
<span class="line-removed">227   template&lt;size_t byte_size&gt; struct PlatformAdd;</span>
<span class="line-removed">228 </span>
<span class="line-removed">229   // Helper base classes for defining PlatformAdd.  To use, define</span>
<span class="line-removed">230   // PlatformAdd or a specialization that derives from one of these,</span>
<span class="line-removed">231   // and include in the PlatformAdd definition the support function</span>
<span class="line-removed">232   // (described below) required by the base class.</span>
233   //
<span class="line-modified">234   // These classes implement the required function object protocol for</span>
<span class="line-modified">235   // PlatformAdd, using a support function template provided by the</span>
<span class="line-removed">236   // derived class.  Let add_value (of type I) and dest (of type D) be</span>
<span class="line-removed">237   // the arguments the object is called with.  If D is a pointer type</span>
<span class="line-removed">238   // P*, then let addend (of type I) be add_value * sizeof(P);</span>
<span class="line-removed">239   // otherwise, addend is add_value.</span>
240   //
<span class="line-modified">241   // FetchAndAdd requires the derived class to provide</span>
<span class="line-modified">242   //   fetch_and_add(addend, dest)</span>
<span class="line-removed">243   // atomically adding addend to the value of dest, and returning the</span>
<span class="line-removed">244   // old value.</span>
245   //
<span class="line-modified">246   // AddAndFetch requires the derived class to provide</span>
<span class="line-modified">247   //   add_and_fetch(addend, dest)</span>
<span class="line-modified">248   // atomically adding addend to the value of dest, and returning the</span>
<span class="line-removed">249   // new value.</span>
250   //
<span class="line-modified">251   // When D is a pointer type P*, both fetch_and_add and add_and_fetch</span>
<span class="line-modified">252   // treat it as if it were a uintptr_t; they do not perform any</span>
<span class="line-modified">253   // scaling of the addend, as that has already been done by the</span>
<span class="line-removed">254   // caller.</span>
<span class="line-removed">255 public: // Temporary, can&#39;t be private: C++03 11.4/2. Fixed by C++11.</span>
<span class="line-removed">256   template&lt;typename Derived&gt; struct FetchAndAdd;</span>
<span class="line-removed">257   template&lt;typename Derived&gt; struct AddAndFetch;</span>
<span class="line-removed">258 private:</span>
259 
260   // Support for platforms that implement some variants of add using a
261   // (typically out of line) non-template helper function.  The
262   // generic arguments passed to PlatformAdd need to be translated to
263   // the appropriate type for the helper function, the helper function
264   // invoked on the translated arguments, and the result translated
265   // back.  Type is the parameter / return type of the helper
266   // function.  No scaling of add_value is performed when D is a pointer
267   // type, so this function can be used to implement the support function
268   // required by AddAndFetch.
<span class="line-modified">269   template&lt;typename Type, typename Fn, typename I, typename D&gt;</span>
<span class="line-modified">270   static D add_using_helper(Fn fn, I add_value, D volatile* dest);</span>
271 
272   // Dispatch handler for cmpxchg.  Provides type-based validity
273   // checking and limited conversions around calls to the
274   // platform-specific implementation layer provided by
275   // PlatformCmpxchg.
<span class="line-modified">276   template&lt;typename T, typename D, typename U, typename Enable = void&gt;</span>
277   struct CmpxchgImpl;
278 
279   // Platform-specific implementation of cmpxchg.  Support for sizes
280   // of 1, 4, and 8 are required.  The class is a function object that
281   // must be default constructable, with these requirements:
282   //
283   // - dest is of type T*.
284   // - exchange_value and compare_value are of type T.
285   // - order is of type atomic_memory_order.
286   // - platform_cmpxchg is an object of type PlatformCmpxchg&lt;sizeof(T)&gt;.
287   //
288   // Then
<span class="line-modified">289   //   platform_cmpxchg(exchange_value, dest, compare_value, order)</span>
290   // must be a valid expression, returning a result convertible to T.
291   //
292   // A default definition is provided, which declares a function template
<span class="line-modified">293   //   T operator()(T, T volatile*, T, atomic_memory_order) const</span>
294   //
295   // For each required size, a platform must either provide an
296   // appropriate definition of that function, or must entirely
297   // specialize the class template for that size.
298   template&lt;size_t byte_size&gt; struct PlatformCmpxchg;
299 
300   // Support for platforms that implement some variants of cmpxchg
301   // using a (typically out of line) non-template helper function.
302   // The generic arguments passed to PlatformCmpxchg need to be
303   // translated to the appropriate type for the helper function, the
304   // helper invoked on the translated arguments, and the result
305   // translated back.  Type is the parameter / return type of the
306   // helper function.
307   template&lt;typename Type, typename Fn, typename T&gt;
308   static T cmpxchg_using_helper(Fn fn,
<span class="line-removed">309                                 T exchange_value,</span>
310                                 T volatile* dest,
<span class="line-modified">311                                 T compare_value);</span>

312 
313   // Support platforms that do not provide Read-Modify-Write
314   // byte-level atomic access. To use, derive PlatformCmpxchg&lt;1&gt; from
315   // this class.
316 public: // Temporary, can&#39;t be private: C++03 11.4/2. Fixed by C++11.
317   struct CmpxchgByteUsingInt;
318 private:
319 
320   // Dispatch handler for xchg.  Provides type-based validity
321   // checking and limited conversions around calls to the
322   // platform-specific implementation layer provided by
323   // PlatformXchg.
<span class="line-modified">324   template&lt;typename T, typename D, typename Enable = void&gt;</span>
325   struct XchgImpl;
326 
327   // Platform-specific implementation of xchg.  Support for sizes
328   // of 4, and sizeof(intptr_t) are required.  The class is a function
329   // object that must be default constructable, with these requirements:
330   //
331   // - dest is of type T*.
332   // - exchange_value is of type T.
333   // - platform_xchg is an object of type PlatformXchg&lt;sizeof(T)&gt;.
334   //
335   // Then
<span class="line-modified">336   //   platform_xchg(exchange_value, dest)</span>
337   // must be a valid expression, returning a result convertible to T.
338   //
339   // A default definition is provided, which declares a function template
<span class="line-modified">340   //   T operator()(T, T volatile*, T, atomic_memory_order) const</span>
341   //
342   // For each required size, a platform must either provide an
343   // appropriate definition of that function, or must entirely
344   // specialize the class template for that size.
345   template&lt;size_t byte_size&gt; struct PlatformXchg;
346 
347   // Support for platforms that implement some variants of xchg
348   // using a (typically out of line) non-template helper function.
349   // The generic arguments passed to PlatformXchg need to be
350   // translated to the appropriate type for the helper function, the
351   // helper invoked on the translated arguments, and the result
352   // translated back.  Type is the parameter / return type of the
353   // helper function.
354   template&lt;typename Type, typename Fn, typename T&gt;
355   static T xchg_using_helper(Fn fn,
<span class="line-modified">356                              T exchange_value,</span>
<span class="line-modified">357                              T volatile* dest);</span>
358 };
359 
360 template&lt;typename From, typename To&gt;
361 struct Atomic::IsPointerConvertible&lt;From*, To*&gt; : AllStatic {
362   // Determine whether From* is implicitly convertible to To*, using
363   // the &quot;sizeof trick&quot;.
364   typedef char yes;
365   typedef char (&amp;no)[2];
366 
367   static yes test(To*);
368   static no test(...);
369   static From* test_value;
370 
371   static const bool value = (sizeof(yes) == sizeof(test(test_value)));
372 };
373 
374 // Handle load for pointer, integral and enum types.
375 template&lt;typename T, typename PlatformOp&gt;
376 struct Atomic::LoadImpl&lt;
377   T,
</pre>
<hr />
<pre>
413 // supports wide atomics, then it has to use specialization
414 // of Atomic::PlatformLoad for that wider size class.
415 template&lt;size_t byte_size&gt;
416 struct Atomic::PlatformLoad {
417   template&lt;typename T&gt;
418   T operator()(T const volatile* dest) const {
419     STATIC_ASSERT(sizeof(T) &lt;= sizeof(void*)); // wide atomics need specialization
420     return *dest;
421   }
422 };
423 
424 // Handle store for integral and enum types.
425 //
426 // All the involved types must be identical.
427 template&lt;typename T, typename PlatformOp&gt;
428 struct Atomic::StoreImpl&lt;
429   T, T,
430   PlatformOp,
431   typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
432 {
<span class="line-modified">433   void operator()(T new_value, T volatile* dest) const {</span>
434     // Forward to the platform handler for the size of T.
<span class="line-modified">435     PlatformOp()(new_value, dest);</span>
436   }
437 };
438 
439 // Handle store for pointer types.
440 //
441 // The new_value must be implicitly convertible to the
442 // destination&#39;s type; it must be type-correct to store the
443 // new_value in the destination.
<span class="line-modified">444 template&lt;typename T, typename D, typename PlatformOp&gt;</span>
445 struct Atomic::StoreImpl&lt;
<span class="line-modified">446   T*, D*,</span>
447   PlatformOp,
448   typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value&gt;::type&gt;
449 {
<span class="line-modified">450   void operator()(T* new_value, D* volatile* dest) const {</span>
451     // Allow derived to base conversion, and adding cv-qualifiers.
452     D* value = new_value;
<span class="line-modified">453     PlatformOp()(value, dest);</span>
454   }
455 };
456 
457 // Handle store for types that have a translator.
458 //
459 // All the involved types must be identical.
460 //
461 // This translates the original call into a call on the decayed
462 // arguments.
463 template&lt;typename T, typename PlatformOp&gt;
464 struct Atomic::StoreImpl&lt;
465   T, T,
466   PlatformOp,
467   typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
468 {
<span class="line-modified">469   void operator()(T new_value, T volatile* dest) const {</span>
470     typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
471     typedef typename Translator::Decayed Decayed;
472     STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
<span class="line-modified">473     PlatformOp()(Translator::decay(new_value),</span>
<span class="line-modified">474                  reinterpret_cast&lt;Decayed volatile*&gt;(dest));</span>
475   }
476 };
477 
478 // Default implementation of atomic store if a specific platform
479 // does not provide a specialization for a certain size class.
480 // For increased safety, the default implementation only allows
481 // storing types that are pointer sized or smaller. If a platform still
482 // supports wide atomics, then it has to use specialization
483 // of Atomic::PlatformStore for that wider size class.
484 template&lt;size_t byte_size&gt;
485 struct Atomic::PlatformStore {
486   template&lt;typename T&gt;
<span class="line-modified">487   void operator()(T new_value,</span>
<span class="line-modified">488                   T volatile* dest) const {</span>
489     STATIC_ASSERT(sizeof(T) &lt;= sizeof(void*)); // wide atomics need specialization
490     (void)const_cast&lt;T&amp;&gt;(*dest = new_value);
491   }
492 };
493 
<span class="line-removed">494 // Define FetchAndAdd and AddAndFetch helper classes before including</span>
<span class="line-removed">495 // platform file, which may use these as base classes, requiring they</span>
<span class="line-removed">496 // be complete.</span>
<span class="line-removed">497 </span>
<span class="line-removed">498 template&lt;typename Derived&gt;</span>
<span class="line-removed">499 struct Atomic::FetchAndAdd {</span>
<span class="line-removed">500   template&lt;typename I, typename D&gt;</span>
<span class="line-removed">501   D operator()(I add_value, D volatile* dest, atomic_memory_order order) const;</span>
<span class="line-removed">502 };</span>
<span class="line-removed">503 </span>
<span class="line-removed">504 template&lt;typename Derived&gt;</span>
<span class="line-removed">505 struct Atomic::AddAndFetch {</span>
<span class="line-removed">506   template&lt;typename I, typename D&gt;</span>
<span class="line-removed">507   D operator()(I add_value, D volatile* dest, atomic_memory_order order) const;</span>
<span class="line-removed">508 };</span>
<span class="line-removed">509 </span>
510 template&lt;typename D&gt;
511 inline void Atomic::inc(D volatile* dest, atomic_memory_order order) {
512   STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
513   typedef typename Conditional&lt;IsPointer&lt;D&gt;::value, ptrdiff_t, D&gt;::type I;
<span class="line-modified">514   Atomic::add(I(1), dest, order);</span>
515 }
516 
517 template&lt;typename D&gt;
518 inline void Atomic::dec(D volatile* dest, atomic_memory_order order) {
519   STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
520   typedef typename Conditional&lt;IsPointer&lt;D&gt;::value, ptrdiff_t, D&gt;::type I;
521   // Assumes two&#39;s complement integer representation.
522   #pragma warning(suppress: 4146)
<span class="line-modified">523   Atomic::add(I(-1), dest, order);</span>
524 }
525 
<span class="line-modified">526 template&lt;typename I, typename D&gt;</span>
<span class="line-modified">527 inline D Atomic::sub(I sub_value, D volatile* dest, atomic_memory_order order) {</span>
528   STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
529   STATIC_ASSERT(IsIntegral&lt;I&gt;::value);
530   // If D is a pointer type, use [u]intptr_t as the addend type,
531   // matching signedness of I.  Otherwise, use D as the addend type.
532   typedef typename Conditional&lt;IsSigned&lt;I&gt;::value, intptr_t, uintptr_t&gt;::type PI;
533   typedef typename Conditional&lt;IsPointer&lt;D&gt;::value, PI, D&gt;::type AddendType;
534   // Only allow conversions that can&#39;t change the value.
535   STATIC_ASSERT(IsSigned&lt;I&gt;::value == IsSigned&lt;AddendType&gt;::value);
536   STATIC_ASSERT(sizeof(I) &lt;= sizeof(AddendType));
537   AddendType addend = sub_value;
538   // Assumes two&#39;s complement integer representation.
539   #pragma warning(suppress: 4146) // In case AddendType is not signed.
<span class="line-modified">540   return Atomic::add(-addend, dest, order);</span>
541 }
542 
543 // Define the class before including platform file, which may specialize
544 // the operator definition.  No generic definition of specializations
545 // of the operator template are provided, nor are there any generic
546 // specializations of the class.  The platform file is responsible for
547 // providing those.
548 template&lt;size_t byte_size&gt;
549 struct Atomic::PlatformCmpxchg {
550   template&lt;typename T&gt;
<span class="line-modified">551   T operator()(T exchange_value,</span>
<span class="line-removed">552                T volatile* dest,</span>
553                T compare_value,

554                atomic_memory_order order) const;
555 };
556 
557 // Define the class before including platform file, which may use this
558 // as a base class, requiring it be complete.  The definition is later
559 // in this file, near the other definitions related to cmpxchg.
560 struct Atomic::CmpxchgByteUsingInt {


561   template&lt;typename T&gt;
<span class="line-modified">562   T operator()(T exchange_value,</span>
<span class="line-removed">563                T volatile* dest,</span>
564                T compare_value,

565                atomic_memory_order order) const;
566 };
567 
568 // Define the class before including platform file, which may specialize
569 // the operator definition.  No generic definition of specializations
570 // of the operator template are provided, nor are there any generic
571 // specializations of the class.  The platform file is responsible for
572 // providing those.
573 template&lt;size_t byte_size&gt;
574 struct Atomic::PlatformXchg {
575   template&lt;typename T&gt;
<span class="line-modified">576   T operator()(T exchange_value,</span>
<span class="line-modified">577                T volatile* dest,</span>
578                atomic_memory_order order) const;
579 };
580 


























581 // platform specific in-line definitions - must come before shared definitions
582 
583 #include OS_CPU_HEADER(atomic)
584 
585 // shared in-line definitions
586 
587 // size_t casts...
588 #if (SIZE_MAX != UINTPTR_MAX)
589 #error size_t is not WORD_SIZE, interesting platform, but missing implementation here
590 #endif
591 
592 template&lt;typename T&gt;
593 inline T Atomic::load(const volatile T* dest) {
594   return LoadImpl&lt;T, PlatformLoad&lt;sizeof(T)&gt; &gt;()(dest);
595 }
596 
<span class="line-modified">597 template&lt;typename T, typename D&gt;</span>
<span class="line-modified">598 inline void Atomic::store(T store_value, volatile D* dest) {</span>
<span class="line-modified">599   StoreImpl&lt;T, D, PlatformStore&lt;sizeof(D)&gt; &gt;()(store_value, dest);</span>














600 }
601 
<span class="line-modified">602 template&lt;typename I, typename D&gt;</span>
<span class="line-modified">603 inline D Atomic::add(I add_value, D volatile* dest,</span>



















604                      atomic_memory_order order) {
<span class="line-modified">605   return AddImpl&lt;I, D&gt;()(add_value, dest, order);</span>
606 }
607 
<span class="line-modified">608 template&lt;typename I, typename D&gt;</span>






609 struct Atomic::AddImpl&lt;
<span class="line-modified">610   I, D,</span>
611   typename EnableIf&lt;IsIntegral&lt;I&gt;::value &amp;&amp;
612                     IsIntegral&lt;D&gt;::value &amp;&amp;
613                     (sizeof(I) &lt;= sizeof(D)) &amp;&amp;
614                     (IsSigned&lt;I&gt;::value == IsSigned&lt;D&gt;::value)&gt;::type&gt;
615 {
<span class="line-modified">616   D operator()(I add_value, D volatile* dest, atomic_memory_order order) const {</span>
617     D addend = add_value;
<span class="line-modified">618     return PlatformAdd&lt;sizeof(D)&gt;()(addend, dest, order);</span>




619   }
620 };
621 
<span class="line-modified">622 template&lt;typename I, typename P&gt;</span>
623 struct Atomic::AddImpl&lt;
<span class="line-modified">624   I, P*,</span>
625   typename EnableIf&lt;IsIntegral&lt;I&gt;::value &amp;&amp; (sizeof(I) &lt;= sizeof(P*))&gt;::type&gt;
626 {
<span class="line-modified">627   P* operator()(I add_value, P* volatile* dest, atomic_memory_order order) const {</span>
<span class="line-modified">628     STATIC_ASSERT(sizeof(intptr_t) == sizeof(P*));</span>
<span class="line-modified">629     STATIC_ASSERT(sizeof(uintptr_t) == sizeof(P*));</span>
<span class="line-modified">630     typedef typename Conditional&lt;IsSigned&lt;I&gt;::value,</span>
<span class="line-modified">631                                  intptr_t,</span>
<span class="line-modified">632                                  uintptr_t&gt;::type CI;</span>
<span class="line-modified">633     CI addend = add_value;</span>
<span class="line-modified">634     return PlatformAdd&lt;sizeof(P*)&gt;()(addend, dest, order);</span>
635   }
<span class="line-removed">636 };</span>
637 
<span class="line-modified">638 template&lt;typename Derived&gt;</span>
<span class="line-modified">639 template&lt;typename I, typename D&gt;</span>
<span class="line-modified">640 inline D Atomic::FetchAndAdd&lt;Derived&gt;::operator()(I add_value, D volatile* dest,</span>
<span class="line-removed">641                                                   atomic_memory_order order) const {</span>
<span class="line-removed">642   I addend = add_value;</span>
<span class="line-removed">643   // If D is a pointer type P*, scale by sizeof(P).</span>
<span class="line-removed">644   if (IsPointer&lt;D&gt;::value) {</span>
<span class="line-removed">645     addend *= sizeof(typename RemovePointer&lt;D&gt;::type);</span>
646   }
<span class="line-modified">647   D old = static_cast&lt;const Derived*&gt;(this)-&gt;fetch_and_add(addend, dest, order);</span>
<span class="line-modified">648   return old + add_value;</span>
<span class="line-modified">649 }</span>
<span class="line-removed">650 </span>
<span class="line-removed">651 template&lt;typename Derived&gt;</span>
<span class="line-removed">652 template&lt;typename I, typename D&gt;</span>
<span class="line-removed">653 inline D Atomic::AddAndFetch&lt;Derived&gt;::operator()(I add_value, D volatile* dest,</span>
<span class="line-removed">654                                                   atomic_memory_order order) const {</span>
<span class="line-removed">655   // If D is a pointer type P*, scale by sizeof(P).</span>
<span class="line-removed">656   if (IsPointer&lt;D&gt;::value) {</span>
<span class="line-removed">657     add_value *= sizeof(typename RemovePointer&lt;D&gt;::type);</span>
658   }
<span class="line-modified">659   return static_cast&lt;const Derived*&gt;(this)-&gt;add_and_fetch(add_value, dest, order);</span>
<span class="line-removed">660 }</span>
661 
<span class="line-modified">662 template&lt;typename Type, typename Fn, typename I, typename D&gt;</span>
<span class="line-modified">663 inline D Atomic::add_using_helper(Fn fn, I add_value, D volatile* dest) {</span>
664   return PrimitiveConversions::cast&lt;D&gt;(
665     fn(PrimitiveConversions::cast&lt;Type&gt;(add_value),
666        reinterpret_cast&lt;Type volatile*&gt;(dest)));
667 }
668 
<span class="line-modified">669 template&lt;typename T, typename D, typename U&gt;</span>
<span class="line-modified">670 inline D Atomic::cmpxchg(T exchange_value,</span>
<span class="line-removed">671                          D volatile* dest,</span>
672                          U compare_value,

673                          atomic_memory_order order) {
<span class="line-modified">674   return CmpxchgImpl&lt;T, D, U&gt;()(exchange_value, dest, compare_value, order);</span>
675 }
676 
<span class="line-modified">677 template&lt;typename T, typename D&gt;</span>
<span class="line-modified">678 inline bool Atomic::replace_if_null(T* value, D* volatile* dest,</span>
679                                     atomic_memory_order order) {
680   // Presently using a trivial implementation in terms of cmpxchg.
681   // Consider adding platform support, to permit the use of compiler
682   // intrinsics like gcc&#39;s __sync_bool_compare_and_swap.
683   D* expected_null = NULL;
<span class="line-modified">684   return expected_null == cmpxchg(value, dest, expected_null, order);</span>
685 }
686 
687 // Handle cmpxchg for integral and enum types.
688 //
689 // All the involved types must be identical.
690 template&lt;typename T&gt;
691 struct Atomic::CmpxchgImpl&lt;
692   T, T, T,
693   typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
694 {
<span class="line-modified">695   T operator()(T exchange_value, T volatile* dest, T compare_value,</span>
696                atomic_memory_order order) const {
697     // Forward to the platform handler for the size of T.
<span class="line-modified">698     return PlatformCmpxchg&lt;sizeof(T)&gt;()(exchange_value,</span>
<span class="line-removed">699                                         dest,</span>
700                                         compare_value,

701                                         order);
702   }
703 };
704 
705 // Handle cmpxchg for pointer types.
706 //
707 // The destination&#39;s type and the compare_value type must be the same,
708 // ignoring cv-qualifiers; we don&#39;t care about the cv-qualifiers of
709 // the compare_value.
710 //
711 // The exchange_value must be implicitly convertible to the
712 // destination&#39;s type; it must be type-correct to store the
713 // exchange_value in the destination.
<span class="line-modified">714 template&lt;typename T, typename D, typename U&gt;</span>
715 struct Atomic::CmpxchgImpl&lt;
<span class="line-modified">716   T*, D*, U*,</span>
717   typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value &amp;&amp;
718                     IsSame&lt;typename RemoveCV&lt;D&gt;::type,
719                            typename RemoveCV&lt;U&gt;::type&gt;::value&gt;::type&gt;
720 {
<span class="line-modified">721   D* operator()(T* exchange_value, D* volatile* dest, U* compare_value,</span>
722                atomic_memory_order order) const {
723     // Allow derived to base conversion, and adding cv-qualifiers.
724     D* new_value = exchange_value;
725     // Don&#39;t care what the CV qualifiers for compare_value are,
726     // but we need to match D* when calling platform support.
727     D* old_value = const_cast&lt;D*&gt;(compare_value);
<span class="line-modified">728     return PlatformCmpxchg&lt;sizeof(D*)&gt;()(new_value, dest, old_value, order);</span>
729   }
730 };
731 
732 // Handle cmpxchg for types that have a translator.
733 //
734 // All the involved types must be identical.
735 //
736 // This translates the original call into a call on the decayed
737 // arguments, and returns the recovered result of that translated
738 // call.
739 template&lt;typename T&gt;
740 struct Atomic::CmpxchgImpl&lt;
741   T, T, T,
742   typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
743 {
<span class="line-modified">744   T operator()(T exchange_value, T volatile* dest, T compare_value,</span>
745                atomic_memory_order order) const {
746     typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
747     typedef typename Translator::Decayed Decayed;
748     STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
749     return Translator::recover(
<span class="line-modified">750       cmpxchg(Translator::decay(exchange_value),</span>
<span class="line-removed">751               reinterpret_cast&lt;Decayed volatile*&gt;(dest),</span>
752               Translator::decay(compare_value),

753               order));
754   }
755 };
756 
757 template&lt;typename Type, typename Fn, typename T&gt;
758 inline T Atomic::cmpxchg_using_helper(Fn fn,
<span class="line-removed">759                                       T exchange_value,</span>
760                                       T volatile* dest,
<span class="line-modified">761                                       T compare_value) {</span>

762   STATIC_ASSERT(sizeof(Type) == sizeof(T));
763   return PrimitiveConversions::cast&lt;T&gt;(
764     fn(PrimitiveConversions::cast&lt;Type&gt;(exchange_value),
765        reinterpret_cast&lt;Type volatile*&gt;(dest),
766        PrimitiveConversions::cast&lt;Type&gt;(compare_value)));
767 }
768 













769 template&lt;typename T&gt;
<span class="line-modified">770 inline T Atomic::CmpxchgByteUsingInt::operator()(T exchange_value,</span>
<span class="line-removed">771                                                  T volatile* dest,</span>
772                                                  T compare_value,

773                                                  atomic_memory_order order) const {
774   STATIC_ASSERT(sizeof(T) == sizeof(uint8_t));
775   uint8_t canon_exchange_value = exchange_value;
776   uint8_t canon_compare_value = compare_value;
777   volatile uint32_t* aligned_dest
778     = reinterpret_cast&lt;volatile uint32_t*&gt;(align_down(dest, sizeof(uint32_t)));
779   size_t offset = pointer_delta(dest, aligned_dest, 1);
<span class="line-modified">780   uint32_t cur = *aligned_dest;</span>
<span class="line-modified">781   uint8_t* cur_as_bytes = reinterpret_cast&lt;uint8_t*&gt;(&amp;cur);</span>


782 
783   // current value may not be what we are looking for, so force it
784   // to that value so the initial cmpxchg will fail if it is different
<span class="line-modified">785   cur_as_bytes[offset] = canon_compare_value;</span>
786 
787   // always execute a real cmpxchg so that we get the required memory
788   // barriers even on initial failure
789   do {
<span class="line-modified">790     // value to swap in matches current value ...</span>
<span class="line-modified">791     uint32_t new_value = cur;</span>
<span class="line-modified">792     // ... except for the one byte we want to update</span>
<span class="line-removed">793     reinterpret_cast&lt;uint8_t*&gt;(&amp;new_value)[offset] = canon_exchange_value;</span>
794 
<span class="line-modified">795     uint32_t res = cmpxchg(new_value, aligned_dest, cur, order);</span>
796     if (res == cur) break;      // success
797 
798     // at least one byte in the int changed value, so update
799     // our view of the current int
800     cur = res;
801     // if our byte is still as cur we loop and try again
<span class="line-modified">802   } while (cur_as_bytes[offset] == canon_compare_value);</span>
803 
<span class="line-modified">804   return PrimitiveConversions::cast&lt;T&gt;(cur_as_bytes[offset]);</span>
805 }
806 
807 // Handle xchg for integral and enum types.
808 //
809 // All the involved types must be identical.
810 template&lt;typename T&gt;
811 struct Atomic::XchgImpl&lt;
812   T, T,
813   typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
814 {
<span class="line-modified">815   T operator()(T exchange_value, T volatile* dest, atomic_memory_order order) const {</span>
816     // Forward to the platform handler for the size of T.
<span class="line-modified">817     return PlatformXchg&lt;sizeof(T)&gt;()(exchange_value, dest, order);</span>
818   }
819 };
820 
821 // Handle xchg for pointer types.
822 //
823 // The exchange_value must be implicitly convertible to the
824 // destination&#39;s type; it must be type-correct to store the
825 // exchange_value in the destination.
<span class="line-modified">826 template&lt;typename T, typename D&gt;</span>
827 struct Atomic::XchgImpl&lt;
<span class="line-modified">828   T*, D*,</span>
829   typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value&gt;::type&gt;
830 {
<span class="line-modified">831   D* operator()(T* exchange_value, D* volatile* dest, atomic_memory_order order) const {</span>
832     // Allow derived to base conversion, and adding cv-qualifiers.
833     D* new_value = exchange_value;
<span class="line-modified">834     return PlatformXchg&lt;sizeof(D*)&gt;()(new_value, dest, order);</span>
835   }
836 };
837 
838 // Handle xchg for types that have a translator.
839 //
840 // All the involved types must be identical.
841 //
842 // This translates the original call into a call on the decayed
843 // arguments, and returns the recovered result of that translated
844 // call.
845 template&lt;typename T&gt;
846 struct Atomic::XchgImpl&lt;
847   T, T,
848   typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
849 {
<span class="line-modified">850   T operator()(T exchange_value, T volatile* dest, atomic_memory_order order) const {</span>
851     typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
852     typedef typename Translator::Decayed Decayed;
853     STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
854     return Translator::recover(
<span class="line-modified">855       xchg(Translator::decay(exchange_value),</span>
<span class="line-modified">856            reinterpret_cast&lt;Decayed volatile*&gt;(dest),</span>
857            order));
858   }
859 };
860 
861 template&lt;typename Type, typename Fn, typename T&gt;
862 inline T Atomic::xchg_using_helper(Fn fn,
<span class="line-modified">863                                    T exchange_value,</span>
<span class="line-modified">864                                    T volatile* dest) {</span>
865   STATIC_ASSERT(sizeof(Type) == sizeof(T));

866   return PrimitiveConversions::cast&lt;T&gt;(
867     fn(PrimitiveConversions::cast&lt;Type&gt;(exchange_value),
868        reinterpret_cast&lt;Type volatile*&gt;(dest)));
869 }
870 
<span class="line-modified">871 template&lt;typename T, typename D&gt;</span>
<span class="line-modified">872 inline D Atomic::xchg(T exchange_value, volatile D* dest, atomic_memory_order order) {</span>
<span class="line-modified">873   return XchgImpl&lt;T, D&gt;()(exchange_value, dest, order);</span>
874 }
875 
876 #endif // SHARE_RUNTIME_ATOMIC_HPP
</pre>
</td>
<td>
<hr />
<pre>
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_RUNTIME_ATOMIC_HPP
 26 #define SHARE_RUNTIME_ATOMIC_HPP
 27 
 28 #include &quot;memory/allocation.hpp&quot;
 29 #include &quot;metaprogramming/conditional.hpp&quot;
 30 #include &quot;metaprogramming/enableIf.hpp&quot;
 31 #include &quot;metaprogramming/isIntegral.hpp&quot;
 32 #include &quot;metaprogramming/isPointer.hpp&quot;
 33 #include &quot;metaprogramming/isSame.hpp&quot;
 34 #include &quot;metaprogramming/primitiveConversions.hpp&quot;
 35 #include &quot;metaprogramming/removeCV.hpp&quot;
 36 #include &quot;metaprogramming/removePointer.hpp&quot;
<span class="line-added"> 37 #include &quot;runtime/orderAccess.hpp&quot;</span>
 38 #include &quot;utilities/align.hpp&quot;
<span class="line-added"> 39 #include &quot;utilities/bytes.hpp&quot;</span>
 40 #include &quot;utilities/macros.hpp&quot;
 41 
 42 enum atomic_memory_order {
 43   // The modes that align with C++11 are intended to
 44   // follow the same semantics.
 45   memory_order_relaxed = 0,
 46   memory_order_acquire = 2,
 47   memory_order_release = 3,
 48   memory_order_acq_rel = 4,
 49   // Strong two-way memory barrier.
 50   memory_order_conservative = 8
 51 };
 52 
<span class="line-added"> 53 enum ScopedFenceType {</span>
<span class="line-added"> 54     X_ACQUIRE</span>
<span class="line-added"> 55   , RELEASE_X</span>
<span class="line-added"> 56   , RELEASE_X_FENCE</span>
<span class="line-added"> 57 };</span>
<span class="line-added"> 58 </span>
 59 class Atomic : AllStatic {
 60 public:
 61   // Atomic operations on int64 types are not available on all 32-bit
 62   // platforms. If atomic ops on int64 are defined here they must only
 63   // be used from code that verifies they are available at runtime and
 64   // can provide an alternative action if not - see supports_cx8() for
 65   // a means to test availability.
 66 
 67   // The memory operations that are mentioned with each of the atomic
 68   // function families come from src/share/vm/runtime/orderAccess.hpp,
 69   // e.g., &lt;fence&gt; is described in that file and is implemented by the
 70   // OrderAccess::fence() function. See that file for the gory details
 71   // on the Memory Access Ordering Model.
 72 
 73   // All of the atomic operations that imply a read-modify-write action
 74   // guarantee a two-way memory barrier across that operation. Historically
 75   // these semantics reflect the strength of atomic operations that are
 76   // provided on SPARC/X86. We assume that strength is necessary unless
 77   // we can prove that a weaker form is sufficiently safe.
 78 
 79   // Atomically store to a location
 80   // The type T must be either a pointer type convertible to or equal
 81   // to D, an integral/enum type equal to D, or a type equal to D that
 82   // is primitive convertible using PrimitiveConversions.
<span class="line-modified"> 83   template&lt;typename D, typename T&gt;</span>
<span class="line-modified"> 84   inline static void store(volatile D* dest, T store_value);</span>
<span class="line-added"> 85 </span>
<span class="line-added"> 86   template &lt;typename D, typename T&gt;</span>
<span class="line-added"> 87   inline static void release_store(volatile D* dest, T store_value);</span>
<span class="line-added"> 88 </span>
<span class="line-added"> 89   template &lt;typename D, typename T&gt;</span>
<span class="line-added"> 90   inline static void release_store_fence(volatile D* dest, T store_value);</span>
 91 
 92   // Atomically load from a location
 93   // The type T must be either a pointer type, an integral/enum type,
 94   // or a type that is primitive convertible using PrimitiveConversions.
 95   template&lt;typename T&gt;
 96   inline static T load(const volatile T* dest);
 97 
<span class="line-modified"> 98   template &lt;typename T&gt;</span>
<span class="line-added"> 99   inline static T load_acquire(const volatile T* dest);</span>
<span class="line-added">100 </span>
<span class="line-added">101   // Atomically add to a location. *add*() provide:</span>
102   // &lt;fence&gt; add-value-to-dest &lt;membar StoreLoad|StoreStore&gt;
103 
<span class="line-modified">104   // Returns updated value.</span>
<span class="line-modified">105   template&lt;typename D, typename I&gt;</span>
<span class="line-added">106   inline static D add(D volatile* dest, I add_value,</span>
107                       atomic_memory_order order = memory_order_conservative);
108 
<span class="line-modified">109   // Returns previous value.</span>
<span class="line-modified">110   template&lt;typename D, typename I&gt;</span>
<span class="line-added">111   inline static D fetch_and_add(D volatile* dest, I add_value,</span>
<span class="line-added">112                                 atomic_memory_order order = memory_order_conservative);</span>
<span class="line-added">113 </span>
<span class="line-added">114   template&lt;typename D, typename I&gt;</span>
<span class="line-added">115   inline static D sub(D volatile* dest, I sub_value,</span>
116                       atomic_memory_order order = memory_order_conservative);
117 
118   // Atomically increment location. inc() provide:
119   // &lt;fence&gt; increment-dest &lt;membar StoreLoad|StoreStore&gt;
120   // The type D may be either a pointer type, or an integral
121   // type. If it is a pointer type, then the increment is
122   // scaled to the size of the type pointed to by the pointer.
123   template&lt;typename D&gt;
124   inline static void inc(D volatile* dest,
125                          atomic_memory_order order = memory_order_conservative);
126 
127   // Atomically decrement a location. dec() provide:
128   // &lt;fence&gt; decrement-dest &lt;membar StoreLoad|StoreStore&gt;
129   // The type D may be either a pointer type, or an integral
130   // type. If it is a pointer type, then the decrement is
131   // scaled to the size of the type pointed to by the pointer.
132   template&lt;typename D&gt;
133   inline static void dec(D volatile* dest,
134                          atomic_memory_order order = memory_order_conservative);
135 
136   // Performs atomic exchange of *dest with exchange_value. Returns old
137   // prior value of *dest. xchg*() provide:
138   // &lt;fence&gt; exchange-value-with-dest &lt;membar StoreLoad|StoreStore&gt;
139   // The type T must be either a pointer type convertible to or equal
140   // to D, an integral/enum type equal to D, or a type equal to D that
141   // is primitive convertible using PrimitiveConversions.
<span class="line-modified">142   template&lt;typename D, typename T&gt;</span>
<span class="line-modified">143   inline static D xchg(volatile D* dest, T exchange_value,</span>
144                        atomic_memory_order order = memory_order_conservative);
145 
146   // Performs atomic compare of *dest and compare_value, and exchanges
147   // *dest with exchange_value if the comparison succeeded. Returns prior
148   // value of *dest. cmpxchg*() provide:
149   // &lt;fence&gt; compare-and-exchange &lt;membar StoreLoad|StoreStore&gt;
150 
<span class="line-modified">151   template&lt;typename D, typename U, typename T&gt;</span>
<span class="line-modified">152   inline static D cmpxchg(D volatile* dest,</span>

153                           U compare_value,
<span class="line-added">154                           T exchange_value,</span>
155                           atomic_memory_order order = memory_order_conservative);
156 
157   // Performs atomic compare of *dest and NULL, and replaces *dest
158   // with exchange_value if the comparison succeeded.  Returns true if
159   // the comparison succeeded and the exchange occurred.  This is
160   // often used as part of lazy initialization, as a lock-free
161   // alternative to the Double-Checked Locking Pattern.
<span class="line-modified">162   template&lt;typename D, typename T&gt;</span>
<span class="line-modified">163   inline static bool replace_if_null(D* volatile* dest, T* value,</span>
164                                      atomic_memory_order order = memory_order_conservative);
165 
166 private:
167 WINDOWS_ONLY(public:) // VS2017 warns (C2027) use of undefined type if IsPointerConvertible is declared private
168   // Test whether From is implicitly convertible to To.
169   // From and To must be pointer types.
170   // Note: Provides the limited subset of C++11 std::is_convertible
171   // that is needed here.
172   template&lt;typename From, typename To&gt; struct IsPointerConvertible;
173 
174 protected:
175   // Dispatch handler for store.  Provides type-based validity
176   // checking and limited conversions around calls to the platform-
177   // specific implementation layer provided by PlatformOp.
<span class="line-modified">178   template&lt;typename D, typename T, typename PlatformOp, typename Enable = void&gt;</span>
179   struct StoreImpl;
180 
181   // Platform-specific implementation of store.  Support for sizes
182   // of 1, 2, 4, and (if different) pointer size bytes are required.
183   // The class is a function object that must be default constructable,
184   // with these requirements:
185   //
186   // either:
187   // - dest is of type D*, an integral, enum or pointer type.
188   // - new_value are of type T, an integral, enum or pointer type D or
189   //   pointer type convertible to D.
190   // or:
191   // - T and D are the same and are primitive convertible using PrimitiveConversions
192   // and either way:
193   // - platform_store is an object of type PlatformStore&lt;sizeof(T)&gt;.
194   //
195   // Then
196   //   platform_store(new_value, dest)
197   // must be a valid expression.
198   //
</pre>
<hr />
<pre>
206   template&lt;typename T, typename PlatformOp, typename Enable = void&gt;
207   struct LoadImpl;
208 
209   // Platform-specific implementation of load. Support for sizes of
210   // 1, 2, 4 bytes and (if different) pointer size bytes are required.
211   // The class is a function object that must be default
212   // constructable, with these requirements:
213   //
214   // - dest is of type T*, an integral, enum or pointer type, or
215   //   T is convertible to a primitive type using PrimitiveConversions
216   // - platform_load is an object of type PlatformLoad&lt;sizeof(T)&gt;.
217   //
218   // Then
219   //   platform_load(src)
220   // must be a valid expression, returning a result convertible to T.
221   //
222   // The default implementation is a volatile load. If a platform
223   // requires more for e.g. 64 bit loads, a specialization is required
224   template&lt;size_t byte_size&gt; struct PlatformLoad;
225 
<span class="line-added">226   // Give platforms a variation point to specialize.</span>
<span class="line-added">227   template&lt;size_t byte_size, ScopedFenceType type&gt; struct PlatformOrderedStore;</span>
<span class="line-added">228   template&lt;size_t byte_size, ScopedFenceType type&gt; struct PlatformOrderedLoad;</span>
<span class="line-added">229 </span>
230 private:
231   // Dispatch handler for add.  Provides type-based validity checking
232   // and limited conversions around calls to the platform-specific
233   // implementation layer provided by PlatformAdd.
<span class="line-modified">234   template&lt;typename D, typename I, typename Enable = void&gt;</span>
235   struct AddImpl;
236 
237   // Platform-specific implementation of add.  Support for sizes of 4
238   // bytes and (if different) pointer size bytes are required.  The
<span class="line-modified">239   // class must be default constructable, with these requirements:</span>

240   //
241   // - dest is of type D*, an integral or pointer type.
242   // - add_value is of type I, an integral type.
243   // - sizeof(I) == sizeof(D).
244   // - if D is an integral type, I == D.
<span class="line-added">245   // - order is of type atomic_memory_order.</span>
246   // - platform_add is an object of type PlatformAdd&lt;sizeof(D)&gt;.
247   //
<span class="line-modified">248   // Then both</span>
<span class="line-modified">249   //   platform_add.add_and_fetch(dest, add_value, order)</span>
<span class="line-modified">250   //   platform_add.fetch_and_add(dest, add_value, order)</span>
<span class="line-modified">251   // must be valid expressions returning a result convertible to D.</span>








252   //
<span class="line-modified">253   // add_and_fetch atomically adds add_value to the value of dest,</span>
<span class="line-modified">254   // returning the new value.</span>




255   //
<span class="line-modified">256   // fetch_and_add atomically adds add_value to the value of dest,</span>
<span class="line-modified">257   // returning the old value.</span>


258   //
<span class="line-modified">259   // When D is a pointer type P*, both add_and_fetch and fetch_and_add</span>
<span class="line-modified">260   // treat it as if it were an uintptr_t; they do not perform any</span>
<span class="line-modified">261   // scaling of add_value, as that has already been done by the caller.</span>

262   //
<span class="line-modified">263   // No definition is provided; all platforms must explicitly define</span>
<span class="line-modified">264   // this class and any needed specializations.</span>
<span class="line-modified">265   template&lt;size_t byte_size&gt; struct PlatformAdd;</span>





266 
267   // Support for platforms that implement some variants of add using a
268   // (typically out of line) non-template helper function.  The
269   // generic arguments passed to PlatformAdd need to be translated to
270   // the appropriate type for the helper function, the helper function
271   // invoked on the translated arguments, and the result translated
272   // back.  Type is the parameter / return type of the helper
273   // function.  No scaling of add_value is performed when D is a pointer
274   // type, so this function can be used to implement the support function
275   // required by AddAndFetch.
<span class="line-modified">276   template&lt;typename Type, typename Fn, typename D, typename I&gt;</span>
<span class="line-modified">277   static D add_using_helper(Fn fn, D volatile* dest, I add_value);</span>
278 
279   // Dispatch handler for cmpxchg.  Provides type-based validity
280   // checking and limited conversions around calls to the
281   // platform-specific implementation layer provided by
282   // PlatformCmpxchg.
<span class="line-modified">283   template&lt;typename D, typename U, typename T, typename Enable = void&gt;</span>
284   struct CmpxchgImpl;
285 
286   // Platform-specific implementation of cmpxchg.  Support for sizes
287   // of 1, 4, and 8 are required.  The class is a function object that
288   // must be default constructable, with these requirements:
289   //
290   // - dest is of type T*.
291   // - exchange_value and compare_value are of type T.
292   // - order is of type atomic_memory_order.
293   // - platform_cmpxchg is an object of type PlatformCmpxchg&lt;sizeof(T)&gt;.
294   //
295   // Then
<span class="line-modified">296   //   platform_cmpxchg(dest, compare_value, exchange_value, order)</span>
297   // must be a valid expression, returning a result convertible to T.
298   //
299   // A default definition is provided, which declares a function template
<span class="line-modified">300   //   T operator()(T volatile*, T, T, atomic_memory_order) const</span>
301   //
302   // For each required size, a platform must either provide an
303   // appropriate definition of that function, or must entirely
304   // specialize the class template for that size.
305   template&lt;size_t byte_size&gt; struct PlatformCmpxchg;
306 
307   // Support for platforms that implement some variants of cmpxchg
308   // using a (typically out of line) non-template helper function.
309   // The generic arguments passed to PlatformCmpxchg need to be
310   // translated to the appropriate type for the helper function, the
311   // helper invoked on the translated arguments, and the result
312   // translated back.  Type is the parameter / return type of the
313   // helper function.
314   template&lt;typename Type, typename Fn, typename T&gt;
315   static T cmpxchg_using_helper(Fn fn,

316                                 T volatile* dest,
<span class="line-modified">317                                 T compare_value,</span>
<span class="line-added">318                                 T exchange_value);</span>
319 
320   // Support platforms that do not provide Read-Modify-Write
321   // byte-level atomic access. To use, derive PlatformCmpxchg&lt;1&gt; from
322   // this class.
323 public: // Temporary, can&#39;t be private: C++03 11.4/2. Fixed by C++11.
324   struct CmpxchgByteUsingInt;
325 private:
326 
327   // Dispatch handler for xchg.  Provides type-based validity
328   // checking and limited conversions around calls to the
329   // platform-specific implementation layer provided by
330   // PlatformXchg.
<span class="line-modified">331   template&lt;typename D, typename T, typename Enable = void&gt;</span>
332   struct XchgImpl;
333 
334   // Platform-specific implementation of xchg.  Support for sizes
335   // of 4, and sizeof(intptr_t) are required.  The class is a function
336   // object that must be default constructable, with these requirements:
337   //
338   // - dest is of type T*.
339   // - exchange_value is of type T.
340   // - platform_xchg is an object of type PlatformXchg&lt;sizeof(T)&gt;.
341   //
342   // Then
<span class="line-modified">343   //   platform_xchg(dest, exchange_value)</span>
344   // must be a valid expression, returning a result convertible to T.
345   //
346   // A default definition is provided, which declares a function template
<span class="line-modified">347   //   T operator()(T volatile*, T, atomic_memory_order) const</span>
348   //
349   // For each required size, a platform must either provide an
350   // appropriate definition of that function, or must entirely
351   // specialize the class template for that size.
352   template&lt;size_t byte_size&gt; struct PlatformXchg;
353 
354   // Support for platforms that implement some variants of xchg
355   // using a (typically out of line) non-template helper function.
356   // The generic arguments passed to PlatformXchg need to be
357   // translated to the appropriate type for the helper function, the
358   // helper invoked on the translated arguments, and the result
359   // translated back.  Type is the parameter / return type of the
360   // helper function.
361   template&lt;typename Type, typename Fn, typename T&gt;
362   static T xchg_using_helper(Fn fn,
<span class="line-modified">363                              T volatile* dest,</span>
<span class="line-modified">364                              T exchange_value);</span>
365 };
366 
367 template&lt;typename From, typename To&gt;
368 struct Atomic::IsPointerConvertible&lt;From*, To*&gt; : AllStatic {
369   // Determine whether From* is implicitly convertible to To*, using
370   // the &quot;sizeof trick&quot;.
371   typedef char yes;
372   typedef char (&amp;no)[2];
373 
374   static yes test(To*);
375   static no test(...);
376   static From* test_value;
377 
378   static const bool value = (sizeof(yes) == sizeof(test(test_value)));
379 };
380 
381 // Handle load for pointer, integral and enum types.
382 template&lt;typename T, typename PlatformOp&gt;
383 struct Atomic::LoadImpl&lt;
384   T,
</pre>
<hr />
<pre>
420 // supports wide atomics, then it has to use specialization
421 // of Atomic::PlatformLoad for that wider size class.
422 template&lt;size_t byte_size&gt;
423 struct Atomic::PlatformLoad {
424   template&lt;typename T&gt;
425   T operator()(T const volatile* dest) const {
426     STATIC_ASSERT(sizeof(T) &lt;= sizeof(void*)); // wide atomics need specialization
427     return *dest;
428   }
429 };
430 
431 // Handle store for integral and enum types.
432 //
433 // All the involved types must be identical.
434 template&lt;typename T, typename PlatformOp&gt;
435 struct Atomic::StoreImpl&lt;
436   T, T,
437   PlatformOp,
438   typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
439 {
<span class="line-modified">440   void operator()(T volatile* dest, T new_value) const {</span>
441     // Forward to the platform handler for the size of T.
<span class="line-modified">442     PlatformOp()(dest, new_value);</span>
443   }
444 };
445 
446 // Handle store for pointer types.
447 //
448 // The new_value must be implicitly convertible to the
449 // destination&#39;s type; it must be type-correct to store the
450 // new_value in the destination.
<span class="line-modified">451 template&lt;typename D, typename T, typename PlatformOp&gt;</span>
452 struct Atomic::StoreImpl&lt;
<span class="line-modified">453   D*, T*,</span>
454   PlatformOp,
455   typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value&gt;::type&gt;
456 {
<span class="line-modified">457   void operator()(D* volatile* dest, T* new_value) const {</span>
458     // Allow derived to base conversion, and adding cv-qualifiers.
459     D* value = new_value;
<span class="line-modified">460     PlatformOp()(dest, value);</span>
461   }
462 };
463 
464 // Handle store for types that have a translator.
465 //
466 // All the involved types must be identical.
467 //
468 // This translates the original call into a call on the decayed
469 // arguments.
470 template&lt;typename T, typename PlatformOp&gt;
471 struct Atomic::StoreImpl&lt;
472   T, T,
473   PlatformOp,
474   typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
475 {
<span class="line-modified">476   void operator()(T volatile* dest, T new_value) const {</span>
477     typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
478     typedef typename Translator::Decayed Decayed;
479     STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
<span class="line-modified">480     PlatformOp()(reinterpret_cast&lt;Decayed volatile*&gt;(dest),</span>
<span class="line-modified">481                  Translator::decay(new_value));</span>
482   }
483 };
484 
485 // Default implementation of atomic store if a specific platform
486 // does not provide a specialization for a certain size class.
487 // For increased safety, the default implementation only allows
488 // storing types that are pointer sized or smaller. If a platform still
489 // supports wide atomics, then it has to use specialization
490 // of Atomic::PlatformStore for that wider size class.
491 template&lt;size_t byte_size&gt;
492 struct Atomic::PlatformStore {
493   template&lt;typename T&gt;
<span class="line-modified">494   void operator()(T volatile* dest,</span>
<span class="line-modified">495                   T new_value) const {</span>
496     STATIC_ASSERT(sizeof(T) &lt;= sizeof(void*)); // wide atomics need specialization
497     (void)const_cast&lt;T&amp;&gt;(*dest = new_value);
498   }
499 };
500 
















501 template&lt;typename D&gt;
502 inline void Atomic::inc(D volatile* dest, atomic_memory_order order) {
503   STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
504   typedef typename Conditional&lt;IsPointer&lt;D&gt;::value, ptrdiff_t, D&gt;::type I;
<span class="line-modified">505   Atomic::add(dest, I(1), order);</span>
506 }
507 
508 template&lt;typename D&gt;
509 inline void Atomic::dec(D volatile* dest, atomic_memory_order order) {
510   STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
511   typedef typename Conditional&lt;IsPointer&lt;D&gt;::value, ptrdiff_t, D&gt;::type I;
512   // Assumes two&#39;s complement integer representation.
513   #pragma warning(suppress: 4146)
<span class="line-modified">514   Atomic::add(dest, I(-1), order);</span>
515 }
516 
<span class="line-modified">517 template&lt;typename D, typename I&gt;</span>
<span class="line-modified">518 inline D Atomic::sub(D volatile* dest, I sub_value, atomic_memory_order order) {</span>
519   STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
520   STATIC_ASSERT(IsIntegral&lt;I&gt;::value);
521   // If D is a pointer type, use [u]intptr_t as the addend type,
522   // matching signedness of I.  Otherwise, use D as the addend type.
523   typedef typename Conditional&lt;IsSigned&lt;I&gt;::value, intptr_t, uintptr_t&gt;::type PI;
524   typedef typename Conditional&lt;IsPointer&lt;D&gt;::value, PI, D&gt;::type AddendType;
525   // Only allow conversions that can&#39;t change the value.
526   STATIC_ASSERT(IsSigned&lt;I&gt;::value == IsSigned&lt;AddendType&gt;::value);
527   STATIC_ASSERT(sizeof(I) &lt;= sizeof(AddendType));
528   AddendType addend = sub_value;
529   // Assumes two&#39;s complement integer representation.
530   #pragma warning(suppress: 4146) // In case AddendType is not signed.
<span class="line-modified">531   return Atomic::add(dest, -addend, order);</span>
532 }
533 
534 // Define the class before including platform file, which may specialize
535 // the operator definition.  No generic definition of specializations
536 // of the operator template are provided, nor are there any generic
537 // specializations of the class.  The platform file is responsible for
538 // providing those.
539 template&lt;size_t byte_size&gt;
540 struct Atomic::PlatformCmpxchg {
541   template&lt;typename T&gt;
<span class="line-modified">542   T operator()(T volatile* dest,</span>

543                T compare_value,
<span class="line-added">544                T exchange_value,</span>
545                atomic_memory_order order) const;
546 };
547 
548 // Define the class before including platform file, which may use this
549 // as a base class, requiring it be complete.  The definition is later
550 // in this file, near the other definitions related to cmpxchg.
551 struct Atomic::CmpxchgByteUsingInt {
<span class="line-added">552   static uint8_t get_byte_in_int(uint32_t n, uint32_t idx);</span>
<span class="line-added">553   static uint32_t set_byte_in_int(uint32_t n, uint8_t b, uint32_t idx);</span>
554   template&lt;typename T&gt;
<span class="line-modified">555   T operator()(T volatile* dest,</span>

556                T compare_value,
<span class="line-added">557                T exchange_value,</span>
558                atomic_memory_order order) const;
559 };
560 
561 // Define the class before including platform file, which may specialize
562 // the operator definition.  No generic definition of specializations
563 // of the operator template are provided, nor are there any generic
564 // specializations of the class.  The platform file is responsible for
565 // providing those.
566 template&lt;size_t byte_size&gt;
567 struct Atomic::PlatformXchg {
568   template&lt;typename T&gt;
<span class="line-modified">569   T operator()(T volatile* dest,</span>
<span class="line-modified">570                T exchange_value,</span>
571                atomic_memory_order order) const;
572 };
573 
<span class="line-added">574 template &lt;ScopedFenceType T&gt;</span>
<span class="line-added">575 class ScopedFenceGeneral: public StackObj {</span>
<span class="line-added">576  public:</span>
<span class="line-added">577   void prefix() {}</span>
<span class="line-added">578   void postfix() {}</span>
<span class="line-added">579 };</span>
<span class="line-added">580 </span>
<span class="line-added">581 // The following methods can be specialized using simple template specialization</span>
<span class="line-added">582 // in the platform specific files for optimization purposes. Otherwise the</span>
<span class="line-added">583 // generalized variant is used.</span>
<span class="line-added">584 </span>
<span class="line-added">585 template&lt;&gt; inline void ScopedFenceGeneral&lt;X_ACQUIRE&gt;::postfix()       { OrderAccess::acquire(); }</span>
<span class="line-added">586 template&lt;&gt; inline void ScopedFenceGeneral&lt;RELEASE_X&gt;::prefix()        { OrderAccess::release(); }</span>
<span class="line-added">587 template&lt;&gt; inline void ScopedFenceGeneral&lt;RELEASE_X_FENCE&gt;::prefix()  { OrderAccess::release(); }</span>
<span class="line-added">588 template&lt;&gt; inline void ScopedFenceGeneral&lt;RELEASE_X_FENCE&gt;::postfix() { OrderAccess::fence();   }</span>
<span class="line-added">589 </span>
<span class="line-added">590 template &lt;ScopedFenceType T&gt;</span>
<span class="line-added">591 class ScopedFence : public ScopedFenceGeneral&lt;T&gt; {</span>
<span class="line-added">592   void *const _field;</span>
<span class="line-added">593  public:</span>
<span class="line-added">594   ScopedFence(void *const field) : _field(field) { prefix(); }</span>
<span class="line-added">595   ~ScopedFence() { postfix(); }</span>
<span class="line-added">596   void prefix() { ScopedFenceGeneral&lt;T&gt;::prefix(); }</span>
<span class="line-added">597   void postfix() { ScopedFenceGeneral&lt;T&gt;::postfix(); }</span>
<span class="line-added">598 };</span>
<span class="line-added">599 </span>
600 // platform specific in-line definitions - must come before shared definitions
601 
602 #include OS_CPU_HEADER(atomic)
603 
604 // shared in-line definitions
605 
606 // size_t casts...
607 #if (SIZE_MAX != UINTPTR_MAX)
608 #error size_t is not WORD_SIZE, interesting platform, but missing implementation here
609 #endif
610 
611 template&lt;typename T&gt;
612 inline T Atomic::load(const volatile T* dest) {
613   return LoadImpl&lt;T, PlatformLoad&lt;sizeof(T)&gt; &gt;()(dest);
614 }
615 
<span class="line-modified">616 template&lt;size_t byte_size, ScopedFenceType type&gt;</span>
<span class="line-modified">617 struct Atomic::PlatformOrderedLoad {</span>
<span class="line-modified">618   template &lt;typename T&gt;</span>
<span class="line-added">619   T operator()(const volatile T* p) const {</span>
<span class="line-added">620     ScopedFence&lt;type&gt; f((void*)p);</span>
<span class="line-added">621     return Atomic::load(p);</span>
<span class="line-added">622   }</span>
<span class="line-added">623 };</span>
<span class="line-added">624 </span>
<span class="line-added">625 template &lt;typename T&gt;</span>
<span class="line-added">626 inline T Atomic::load_acquire(const volatile T* p) {</span>
<span class="line-added">627   return LoadImpl&lt;T, PlatformOrderedLoad&lt;sizeof(T), X_ACQUIRE&gt; &gt;()(p);</span>
<span class="line-added">628 }</span>
<span class="line-added">629 </span>
<span class="line-added">630 template&lt;typename D, typename T&gt;</span>
<span class="line-added">631 inline void Atomic::store(volatile D* dest, T store_value) {</span>
<span class="line-added">632   StoreImpl&lt;D, T, PlatformStore&lt;sizeof(D)&gt; &gt;()(dest, store_value);</span>
633 }
634 
<span class="line-modified">635 template&lt;size_t byte_size, ScopedFenceType type&gt;</span>
<span class="line-modified">636 struct Atomic::PlatformOrderedStore {</span>
<span class="line-added">637   template &lt;typename T&gt;</span>
<span class="line-added">638   void operator()(volatile T* p, T v) const {</span>
<span class="line-added">639     ScopedFence&lt;type&gt; f((void*)p);</span>
<span class="line-added">640     Atomic::store(p, v);</span>
<span class="line-added">641   }</span>
<span class="line-added">642 };</span>
<span class="line-added">643 </span>
<span class="line-added">644 template &lt;typename D, typename T&gt;</span>
<span class="line-added">645 inline void Atomic::release_store(volatile D* p, T v) {</span>
<span class="line-added">646   StoreImpl&lt;D, T, PlatformOrderedStore&lt;sizeof(D), RELEASE_X&gt; &gt;()(p, v);</span>
<span class="line-added">647 }</span>
<span class="line-added">648 </span>
<span class="line-added">649 template &lt;typename D, typename T&gt;</span>
<span class="line-added">650 inline void Atomic::release_store_fence(volatile D* p, T v) {</span>
<span class="line-added">651   StoreImpl&lt;D, T, PlatformOrderedStore&lt;sizeof(D), RELEASE_X_FENCE&gt; &gt;()(p, v);</span>
<span class="line-added">652 }</span>
<span class="line-added">653 </span>
<span class="line-added">654 template&lt;typename D, typename I&gt;</span>
<span class="line-added">655 inline D Atomic::add(D volatile* dest, I add_value,</span>
656                      atomic_memory_order order) {
<span class="line-modified">657   return AddImpl&lt;D, I&gt;::add_and_fetch(dest, add_value, order);</span>
658 }
659 
<span class="line-modified">660 template&lt;typename D, typename I&gt;</span>
<span class="line-added">661 inline D Atomic::fetch_and_add(D volatile* dest, I add_value,</span>
<span class="line-added">662                                atomic_memory_order order) {</span>
<span class="line-added">663   return AddImpl&lt;D, I&gt;::fetch_and_add(dest, add_value, order);</span>
<span class="line-added">664 }</span>
<span class="line-added">665 </span>
<span class="line-added">666 template&lt;typename D, typename I&gt;</span>
667 struct Atomic::AddImpl&lt;
<span class="line-modified">668   D, I,</span>
669   typename EnableIf&lt;IsIntegral&lt;I&gt;::value &amp;&amp;
670                     IsIntegral&lt;D&gt;::value &amp;&amp;
671                     (sizeof(I) &lt;= sizeof(D)) &amp;&amp;
672                     (IsSigned&lt;I&gt;::value == IsSigned&lt;D&gt;::value)&gt;::type&gt;
673 {
<span class="line-modified">674   static D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) {</span>
675     D addend = add_value;
<span class="line-modified">676     return PlatformAdd&lt;sizeof(D)&gt;().add_and_fetch(dest, addend, order);</span>
<span class="line-added">677   }</span>
<span class="line-added">678   static D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order order) {</span>
<span class="line-added">679     D addend = add_value;</span>
<span class="line-added">680     return PlatformAdd&lt;sizeof(D)&gt;().fetch_and_add(dest, addend, order);</span>
681   }
682 };
683 
<span class="line-modified">684 template&lt;typename P, typename I&gt;</span>
685 struct Atomic::AddImpl&lt;
<span class="line-modified">686   P*, I,</span>
687   typename EnableIf&lt;IsIntegral&lt;I&gt;::value &amp;&amp; (sizeof(I) &lt;= sizeof(P*))&gt;::type&gt;
688 {
<span class="line-modified">689   STATIC_ASSERT(sizeof(intptr_t) == sizeof(P*));</span>
<span class="line-modified">690   STATIC_ASSERT(sizeof(uintptr_t) == sizeof(P*));</span>
<span class="line-modified">691   typedef typename Conditional&lt;IsSigned&lt;I&gt;::value,</span>
<span class="line-modified">692                                intptr_t,</span>
<span class="line-modified">693                                uintptr_t&gt;::type CI;</span>
<span class="line-modified">694 </span>
<span class="line-modified">695   static CI scale_addend(CI add_value) {</span>
<span class="line-modified">696     return add_value * sizeof(P);</span>
697   }

698 
<span class="line-modified">699   static P* add_and_fetch(P* volatile* dest, I add_value, atomic_memory_order order) {</span>
<span class="line-modified">700     CI addend = add_value;</span>
<span class="line-modified">701     return PlatformAdd&lt;sizeof(P*)&gt;().add_and_fetch(dest, scale_addend(addend), order);</span>





702   }
<span class="line-modified">703   static P* fetch_and_add(P* volatile* dest, I add_value, atomic_memory_order order) {</span>
<span class="line-modified">704     CI addend = add_value;</span>
<span class="line-modified">705     return PlatformAdd&lt;sizeof(P*)&gt;().fetch_and_add(dest, scale_addend(addend), order);</span>








706   }
<span class="line-modified">707 };</span>

708 
<span class="line-modified">709 template&lt;typename Type, typename Fn, typename D, typename I&gt;</span>
<span class="line-modified">710 inline D Atomic::add_using_helper(Fn fn, D volatile* dest, I add_value) {</span>
711   return PrimitiveConversions::cast&lt;D&gt;(
712     fn(PrimitiveConversions::cast&lt;Type&gt;(add_value),
713        reinterpret_cast&lt;Type volatile*&gt;(dest)));
714 }
715 
<span class="line-modified">716 template&lt;typename D, typename U, typename T&gt;</span>
<span class="line-modified">717 inline D Atomic::cmpxchg(D volatile* dest,</span>

718                          U compare_value,
<span class="line-added">719                          T exchange_value,</span>
720                          atomic_memory_order order) {
<span class="line-modified">721   return CmpxchgImpl&lt;D, U, T&gt;()(dest, compare_value, exchange_value, order);</span>
722 }
723 
<span class="line-modified">724 template&lt;typename D, typename T&gt;</span>
<span class="line-modified">725 inline bool Atomic::replace_if_null(D* volatile* dest, T* value,</span>
726                                     atomic_memory_order order) {
727   // Presently using a trivial implementation in terms of cmpxchg.
728   // Consider adding platform support, to permit the use of compiler
729   // intrinsics like gcc&#39;s __sync_bool_compare_and_swap.
730   D* expected_null = NULL;
<span class="line-modified">731   return expected_null == cmpxchg(dest, expected_null, value, order);</span>
732 }
733 
734 // Handle cmpxchg for integral and enum types.
735 //
736 // All the involved types must be identical.
737 template&lt;typename T&gt;
738 struct Atomic::CmpxchgImpl&lt;
739   T, T, T,
740   typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
741 {
<span class="line-modified">742   T operator()(T volatile* dest, T compare_value, T exchange_value,</span>
743                atomic_memory_order order) const {
744     // Forward to the platform handler for the size of T.
<span class="line-modified">745     return PlatformCmpxchg&lt;sizeof(T)&gt;()(dest,</span>

746                                         compare_value,
<span class="line-added">747                                         exchange_value,</span>
748                                         order);
749   }
750 };
751 
752 // Handle cmpxchg for pointer types.
753 //
754 // The destination&#39;s type and the compare_value type must be the same,
755 // ignoring cv-qualifiers; we don&#39;t care about the cv-qualifiers of
756 // the compare_value.
757 //
758 // The exchange_value must be implicitly convertible to the
759 // destination&#39;s type; it must be type-correct to store the
760 // exchange_value in the destination.
<span class="line-modified">761 template&lt;typename D, typename U, typename T&gt;</span>
762 struct Atomic::CmpxchgImpl&lt;
<span class="line-modified">763   D*, U*, T*,</span>
764   typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value &amp;&amp;
765                     IsSame&lt;typename RemoveCV&lt;D&gt;::type,
766                            typename RemoveCV&lt;U&gt;::type&gt;::value&gt;::type&gt;
767 {
<span class="line-modified">768   D* operator()(D* volatile* dest, U* compare_value, T* exchange_value,</span>
769                atomic_memory_order order) const {
770     // Allow derived to base conversion, and adding cv-qualifiers.
771     D* new_value = exchange_value;
772     // Don&#39;t care what the CV qualifiers for compare_value are,
773     // but we need to match D* when calling platform support.
774     D* old_value = const_cast&lt;D*&gt;(compare_value);
<span class="line-modified">775     return PlatformCmpxchg&lt;sizeof(D*)&gt;()(dest, old_value, new_value, order);</span>
776   }
777 };
778 
779 // Handle cmpxchg for types that have a translator.
780 //
781 // All the involved types must be identical.
782 //
783 // This translates the original call into a call on the decayed
784 // arguments, and returns the recovered result of that translated
785 // call.
786 template&lt;typename T&gt;
787 struct Atomic::CmpxchgImpl&lt;
788   T, T, T,
789   typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
790 {
<span class="line-modified">791   T operator()(T volatile* dest, T compare_value, T exchange_value,</span>
792                atomic_memory_order order) const {
793     typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
794     typedef typename Translator::Decayed Decayed;
795     STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
796     return Translator::recover(
<span class="line-modified">797       cmpxchg(reinterpret_cast&lt;Decayed volatile*&gt;(dest),</span>

798               Translator::decay(compare_value),
<span class="line-added">799               Translator::decay(exchange_value),</span>
800               order));
801   }
802 };
803 
804 template&lt;typename Type, typename Fn, typename T&gt;
805 inline T Atomic::cmpxchg_using_helper(Fn fn,

806                                       T volatile* dest,
<span class="line-modified">807                                       T compare_value,</span>
<span class="line-added">808                                       T exchange_value) {</span>
809   STATIC_ASSERT(sizeof(Type) == sizeof(T));
810   return PrimitiveConversions::cast&lt;T&gt;(
811     fn(PrimitiveConversions::cast&lt;Type&gt;(exchange_value),
812        reinterpret_cast&lt;Type volatile*&gt;(dest),
813        PrimitiveConversions::cast&lt;Type&gt;(compare_value)));
814 }
815 
<span class="line-added">816 inline uint32_t Atomic::CmpxchgByteUsingInt::set_byte_in_int(uint32_t n,</span>
<span class="line-added">817                                                              uint8_t b,</span>
<span class="line-added">818                                                              uint32_t idx) {</span>
<span class="line-added">819   int bitsIdx = BitsPerByte * idx;</span>
<span class="line-added">820   return (n &amp; ~(0xff &lt;&lt; bitsIdx)) | (b &lt;&lt; bitsIdx);</span>
<span class="line-added">821 }</span>
<span class="line-added">822 </span>
<span class="line-added">823 inline uint8_t Atomic::CmpxchgByteUsingInt::get_byte_in_int(uint32_t n,</span>
<span class="line-added">824                                                             uint32_t idx) {</span>
<span class="line-added">825   int bitsIdx = BitsPerByte * idx;</span>
<span class="line-added">826   return (uint8_t)(n &gt;&gt; bitsIdx);</span>
<span class="line-added">827 }</span>
<span class="line-added">828 </span>
829 template&lt;typename T&gt;
<span class="line-modified">830 inline T Atomic::CmpxchgByteUsingInt::operator()(T volatile* dest,</span>

831                                                  T compare_value,
<span class="line-added">832                                                  T exchange_value,</span>
833                                                  atomic_memory_order order) const {
834   STATIC_ASSERT(sizeof(T) == sizeof(uint8_t));
835   uint8_t canon_exchange_value = exchange_value;
836   uint8_t canon_compare_value = compare_value;
837   volatile uint32_t* aligned_dest
838     = reinterpret_cast&lt;volatile uint32_t*&gt;(align_down(dest, sizeof(uint32_t)));
839   size_t offset = pointer_delta(dest, aligned_dest, 1);
<span class="line-modified">840 </span>
<span class="line-modified">841   uint32_t idx = (Endian::NATIVE == Endian::BIG)</span>
<span class="line-added">842                    ? (sizeof(uint32_t) - 1 - offset)</span>
<span class="line-added">843                    : offset;</span>
844 
845   // current value may not be what we are looking for, so force it
846   // to that value so the initial cmpxchg will fail if it is different
<span class="line-modified">847   uint32_t cur = set_byte_in_int(Atomic::load(aligned_dest), canon_compare_value, idx);</span>
848 
849   // always execute a real cmpxchg so that we get the required memory
850   // barriers even on initial failure
851   do {
<span class="line-modified">852     // value to swap in matches current value</span>
<span class="line-modified">853     // except for the one byte we want to update</span>
<span class="line-modified">854     uint32_t new_value = set_byte_in_int(cur, canon_exchange_value, idx);</span>

855 
<span class="line-modified">856     uint32_t res = cmpxchg(aligned_dest, cur, new_value, order);</span>
857     if (res == cur) break;      // success
858 
859     // at least one byte in the int changed value, so update
860     // our view of the current int
861     cur = res;
862     // if our byte is still as cur we loop and try again
<span class="line-modified">863   } while (get_byte_in_int(cur, idx) == canon_compare_value);</span>
864 
<span class="line-modified">865   return PrimitiveConversions::cast&lt;T&gt;(get_byte_in_int(cur, idx));</span>
866 }
867 
868 // Handle xchg for integral and enum types.
869 //
870 // All the involved types must be identical.
871 template&lt;typename T&gt;
872 struct Atomic::XchgImpl&lt;
873   T, T,
874   typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
875 {
<span class="line-modified">876   T operator()(T volatile* dest, T exchange_value, atomic_memory_order order) const {</span>
877     // Forward to the platform handler for the size of T.
<span class="line-modified">878     return PlatformXchg&lt;sizeof(T)&gt;()(dest, exchange_value, order);</span>
879   }
880 };
881 
882 // Handle xchg for pointer types.
883 //
884 // The exchange_value must be implicitly convertible to the
885 // destination&#39;s type; it must be type-correct to store the
886 // exchange_value in the destination.
<span class="line-modified">887 template&lt;typename D, typename T&gt;</span>
888 struct Atomic::XchgImpl&lt;
<span class="line-modified">889   D*, T*,</span>
890   typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value&gt;::type&gt;
891 {
<span class="line-modified">892   D* operator()(D* volatile* dest, T* exchange_value, atomic_memory_order order) const {</span>
893     // Allow derived to base conversion, and adding cv-qualifiers.
894     D* new_value = exchange_value;
<span class="line-modified">895     return PlatformXchg&lt;sizeof(D*)&gt;()(dest, new_value, order);</span>
896   }
897 };
898 
899 // Handle xchg for types that have a translator.
900 //
901 // All the involved types must be identical.
902 //
903 // This translates the original call into a call on the decayed
904 // arguments, and returns the recovered result of that translated
905 // call.
906 template&lt;typename T&gt;
907 struct Atomic::XchgImpl&lt;
908   T, T,
909   typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
910 {
<span class="line-modified">911   T operator()(T volatile* dest, T exchange_value, atomic_memory_order order) const {</span>
912     typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
913     typedef typename Translator::Decayed Decayed;
914     STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
915     return Translator::recover(
<span class="line-modified">916       xchg(reinterpret_cast&lt;Decayed volatile*&gt;(dest),</span>
<span class="line-modified">917            Translator::decay(exchange_value),</span>
918            order));
919   }
920 };
921 
922 template&lt;typename Type, typename Fn, typename T&gt;
923 inline T Atomic::xchg_using_helper(Fn fn,
<span class="line-modified">924                                    T volatile* dest,</span>
<span class="line-modified">925                                    T exchange_value) {</span>
926   STATIC_ASSERT(sizeof(Type) == sizeof(T));
<span class="line-added">927   // Notice the swapped order of arguments. Change when/if stubs are rewritten.</span>
928   return PrimitiveConversions::cast&lt;T&gt;(
929     fn(PrimitiveConversions::cast&lt;Type&gt;(exchange_value),
930        reinterpret_cast&lt;Type volatile*&gt;(dest)));
931 }
932 
<span class="line-modified">933 template&lt;typename D, typename T&gt;</span>
<span class="line-modified">934 inline D Atomic::xchg(volatile D* dest, T exchange_value, atomic_memory_order order) {</span>
<span class="line-modified">935   return XchgImpl&lt;D, T&gt;()(dest, exchange_value, order);</span>
936 }
937 
938 #endif // SHARE_RUNTIME_ATOMIC_HPP
</pre>
</td>
</tr>
</table>
<center><a href="arguments.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="basicLock.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>