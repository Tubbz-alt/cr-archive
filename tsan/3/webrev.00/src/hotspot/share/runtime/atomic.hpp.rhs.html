<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/runtime/atomic.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_RUNTIME_ATOMIC_HPP
 26 #define SHARE_RUNTIME_ATOMIC_HPP
 27 
 28 #include &quot;memory/allocation.hpp&quot;
 29 #include &quot;metaprogramming/conditional.hpp&quot;
 30 #include &quot;metaprogramming/enableIf.hpp&quot;
 31 #include &quot;metaprogramming/isIntegral.hpp&quot;
 32 #include &quot;metaprogramming/isPointer.hpp&quot;
 33 #include &quot;metaprogramming/isSame.hpp&quot;
 34 #include &quot;metaprogramming/primitiveConversions.hpp&quot;
 35 #include &quot;metaprogramming/removeCV.hpp&quot;
 36 #include &quot;metaprogramming/removePointer.hpp&quot;
<a name="1" id="anc1"></a><span class="line-added"> 37 #include &quot;runtime/orderAccess.hpp&quot;</span>
 38 #include &quot;utilities/align.hpp&quot;
<a name="2" id="anc2"></a><span class="line-added"> 39 #include &quot;utilities/bytes.hpp&quot;</span>
 40 #include &quot;utilities/macros.hpp&quot;
 41 
 42 enum atomic_memory_order {
 43   // The modes that align with C++11 are intended to
 44   // follow the same semantics.
 45   memory_order_relaxed = 0,
 46   memory_order_acquire = 2,
 47   memory_order_release = 3,
 48   memory_order_acq_rel = 4,
 49   // Strong two-way memory barrier.
 50   memory_order_conservative = 8
 51 };
 52 
<a name="3" id="anc3"></a><span class="line-added"> 53 enum ScopedFenceType {</span>
<span class="line-added"> 54     X_ACQUIRE</span>
<span class="line-added"> 55   , RELEASE_X</span>
<span class="line-added"> 56   , RELEASE_X_FENCE</span>
<span class="line-added"> 57 };</span>
<span class="line-added"> 58 </span>
 59 class Atomic : AllStatic {
 60 public:
 61   // Atomic operations on int64 types are not available on all 32-bit
 62   // platforms. If atomic ops on int64 are defined here they must only
 63   // be used from code that verifies they are available at runtime and
 64   // can provide an alternative action if not - see supports_cx8() for
 65   // a means to test availability.
 66 
 67   // The memory operations that are mentioned with each of the atomic
 68   // function families come from src/share/vm/runtime/orderAccess.hpp,
 69   // e.g., &lt;fence&gt; is described in that file and is implemented by the
 70   // OrderAccess::fence() function. See that file for the gory details
 71   // on the Memory Access Ordering Model.
 72 
 73   // All of the atomic operations that imply a read-modify-write action
 74   // guarantee a two-way memory barrier across that operation. Historically
 75   // these semantics reflect the strength of atomic operations that are
 76   // provided on SPARC/X86. We assume that strength is necessary unless
 77   // we can prove that a weaker form is sufficiently safe.
 78 
 79   // Atomically store to a location
 80   // The type T must be either a pointer type convertible to or equal
 81   // to D, an integral/enum type equal to D, or a type equal to D that
 82   // is primitive convertible using PrimitiveConversions.
<a name="4" id="anc4"></a><span class="line-modified"> 83   template&lt;typename D, typename T&gt;</span>
<span class="line-modified"> 84   inline static void store(volatile D* dest, T store_value);</span>
<span class="line-added"> 85 </span>
<span class="line-added"> 86   template &lt;typename D, typename T&gt;</span>
<span class="line-added"> 87   inline static void release_store(volatile D* dest, T store_value);</span>
<span class="line-added"> 88 </span>
<span class="line-added"> 89   template &lt;typename D, typename T&gt;</span>
<span class="line-added"> 90   inline static void release_store_fence(volatile D* dest, T store_value);</span>
 91 
 92   // Atomically load from a location
 93   // The type T must be either a pointer type, an integral/enum type,
 94   // or a type that is primitive convertible using PrimitiveConversions.
 95   template&lt;typename T&gt;
 96   inline static T load(const volatile T* dest);
 97 
<a name="5" id="anc5"></a><span class="line-modified"> 98   template &lt;typename T&gt;</span>
<span class="line-added"> 99   inline static T load_acquire(const volatile T* dest);</span>
<span class="line-added">100 </span>
<span class="line-added">101   // Atomically add to a location. *add*() provide:</span>
102   // &lt;fence&gt; add-value-to-dest &lt;membar StoreLoad|StoreStore&gt;
103 
<a name="6" id="anc6"></a><span class="line-modified">104   // Returns updated value.</span>
<span class="line-modified">105   template&lt;typename D, typename I&gt;</span>
<span class="line-added">106   inline static D add(D volatile* dest, I add_value,</span>
107                       atomic_memory_order order = memory_order_conservative);
108 
<a name="7" id="anc7"></a><span class="line-modified">109   // Returns previous value.</span>
<span class="line-modified">110   template&lt;typename D, typename I&gt;</span>
<span class="line-added">111   inline static D fetch_and_add(D volatile* dest, I add_value,</span>
<span class="line-added">112                                 atomic_memory_order order = memory_order_conservative);</span>
<span class="line-added">113 </span>
<span class="line-added">114   template&lt;typename D, typename I&gt;</span>
<span class="line-added">115   inline static D sub(D volatile* dest, I sub_value,</span>
116                       atomic_memory_order order = memory_order_conservative);
117 
118   // Atomically increment location. inc() provide:
119   // &lt;fence&gt; increment-dest &lt;membar StoreLoad|StoreStore&gt;
120   // The type D may be either a pointer type, or an integral
121   // type. If it is a pointer type, then the increment is
122   // scaled to the size of the type pointed to by the pointer.
123   template&lt;typename D&gt;
124   inline static void inc(D volatile* dest,
125                          atomic_memory_order order = memory_order_conservative);
126 
127   // Atomically decrement a location. dec() provide:
128   // &lt;fence&gt; decrement-dest &lt;membar StoreLoad|StoreStore&gt;
129   // The type D may be either a pointer type, or an integral
130   // type. If it is a pointer type, then the decrement is
131   // scaled to the size of the type pointed to by the pointer.
132   template&lt;typename D&gt;
133   inline static void dec(D volatile* dest,
134                          atomic_memory_order order = memory_order_conservative);
135 
136   // Performs atomic exchange of *dest with exchange_value. Returns old
137   // prior value of *dest. xchg*() provide:
138   // &lt;fence&gt; exchange-value-with-dest &lt;membar StoreLoad|StoreStore&gt;
139   // The type T must be either a pointer type convertible to or equal
140   // to D, an integral/enum type equal to D, or a type equal to D that
141   // is primitive convertible using PrimitiveConversions.
<a name="8" id="anc8"></a><span class="line-modified">142   template&lt;typename D, typename T&gt;</span>
<span class="line-modified">143   inline static D xchg(volatile D* dest, T exchange_value,</span>
144                        atomic_memory_order order = memory_order_conservative);
145 
146   // Performs atomic compare of *dest and compare_value, and exchanges
147   // *dest with exchange_value if the comparison succeeded. Returns prior
148   // value of *dest. cmpxchg*() provide:
149   // &lt;fence&gt; compare-and-exchange &lt;membar StoreLoad|StoreStore&gt;
150 
<a name="9" id="anc9"></a><span class="line-modified">151   template&lt;typename D, typename U, typename T&gt;</span>
<span class="line-modified">152   inline static D cmpxchg(D volatile* dest,</span>

153                           U compare_value,
<a name="10" id="anc10"></a><span class="line-added">154                           T exchange_value,</span>
155                           atomic_memory_order order = memory_order_conservative);
156 
157   // Performs atomic compare of *dest and NULL, and replaces *dest
158   // with exchange_value if the comparison succeeded.  Returns true if
159   // the comparison succeeded and the exchange occurred.  This is
160   // often used as part of lazy initialization, as a lock-free
161   // alternative to the Double-Checked Locking Pattern.
<a name="11" id="anc11"></a><span class="line-modified">162   template&lt;typename D, typename T&gt;</span>
<span class="line-modified">163   inline static bool replace_if_null(D* volatile* dest, T* value,</span>
164                                      atomic_memory_order order = memory_order_conservative);
165 
166 private:
167 WINDOWS_ONLY(public:) // VS2017 warns (C2027) use of undefined type if IsPointerConvertible is declared private
168   // Test whether From is implicitly convertible to To.
169   // From and To must be pointer types.
170   // Note: Provides the limited subset of C++11 std::is_convertible
171   // that is needed here.
172   template&lt;typename From, typename To&gt; struct IsPointerConvertible;
173 
174 protected:
175   // Dispatch handler for store.  Provides type-based validity
176   // checking and limited conversions around calls to the platform-
177   // specific implementation layer provided by PlatformOp.
<a name="12" id="anc12"></a><span class="line-modified">178   template&lt;typename D, typename T, typename PlatformOp, typename Enable = void&gt;</span>
179   struct StoreImpl;
180 
181   // Platform-specific implementation of store.  Support for sizes
182   // of 1, 2, 4, and (if different) pointer size bytes are required.
183   // The class is a function object that must be default constructable,
184   // with these requirements:
185   //
186   // either:
187   // - dest is of type D*, an integral, enum or pointer type.
188   // - new_value are of type T, an integral, enum or pointer type D or
189   //   pointer type convertible to D.
190   // or:
191   // - T and D are the same and are primitive convertible using PrimitiveConversions
192   // and either way:
193   // - platform_store is an object of type PlatformStore&lt;sizeof(T)&gt;.
194   //
195   // Then
196   //   platform_store(new_value, dest)
197   // must be a valid expression.
198   //
199   // The default implementation is a volatile store. If a platform
200   // requires more for e.g. 64 bit stores, a specialization is required
201   template&lt;size_t byte_size&gt; struct PlatformStore;
202 
203   // Dispatch handler for load.  Provides type-based validity
204   // checking and limited conversions around calls to the platform-
205   // specific implementation layer provided by PlatformOp.
206   template&lt;typename T, typename PlatformOp, typename Enable = void&gt;
207   struct LoadImpl;
208 
209   // Platform-specific implementation of load. Support for sizes of
210   // 1, 2, 4 bytes and (if different) pointer size bytes are required.
211   // The class is a function object that must be default
212   // constructable, with these requirements:
213   //
214   // - dest is of type T*, an integral, enum or pointer type, or
215   //   T is convertible to a primitive type using PrimitiveConversions
216   // - platform_load is an object of type PlatformLoad&lt;sizeof(T)&gt;.
217   //
218   // Then
219   //   platform_load(src)
220   // must be a valid expression, returning a result convertible to T.
221   //
222   // The default implementation is a volatile load. If a platform
223   // requires more for e.g. 64 bit loads, a specialization is required
224   template&lt;size_t byte_size&gt; struct PlatformLoad;
225 
<a name="13" id="anc13"></a><span class="line-added">226   // Give platforms a variation point to specialize.</span>
<span class="line-added">227   template&lt;size_t byte_size, ScopedFenceType type&gt; struct PlatformOrderedStore;</span>
<span class="line-added">228   template&lt;size_t byte_size, ScopedFenceType type&gt; struct PlatformOrderedLoad;</span>
<span class="line-added">229 </span>
230 private:
231   // Dispatch handler for add.  Provides type-based validity checking
232   // and limited conversions around calls to the platform-specific
233   // implementation layer provided by PlatformAdd.
<a name="14" id="anc14"></a><span class="line-modified">234   template&lt;typename D, typename I, typename Enable = void&gt;</span>
235   struct AddImpl;
236 
237   // Platform-specific implementation of add.  Support for sizes of 4
238   // bytes and (if different) pointer size bytes are required.  The
<a name="15" id="anc15"></a><span class="line-modified">239   // class must be default constructable, with these requirements:</span>

240   //
241   // - dest is of type D*, an integral or pointer type.
242   // - add_value is of type I, an integral type.
243   // - sizeof(I) == sizeof(D).
244   // - if D is an integral type, I == D.
<a name="16" id="anc16"></a><span class="line-added">245   // - order is of type atomic_memory_order.</span>
246   // - platform_add is an object of type PlatformAdd&lt;sizeof(D)&gt;.
247   //
<a name="17" id="anc17"></a><span class="line-modified">248   // Then both</span>
<span class="line-modified">249   //   platform_add.add_and_fetch(dest, add_value, order)</span>
<span class="line-modified">250   //   platform_add.fetch_and_add(dest, add_value, order)</span>
<span class="line-modified">251   // must be valid expressions returning a result convertible to D.</span>








252   //
<a name="18" id="anc18"></a><span class="line-modified">253   // add_and_fetch atomically adds add_value to the value of dest,</span>
<span class="line-modified">254   // returning the new value.</span>




255   //
<a name="19" id="anc19"></a><span class="line-modified">256   // fetch_and_add atomically adds add_value to the value of dest,</span>
<span class="line-modified">257   // returning the old value.</span>


258   //
<a name="20" id="anc20"></a><span class="line-modified">259   // When D is a pointer type P*, both add_and_fetch and fetch_and_add</span>
<span class="line-modified">260   // treat it as if it were an uintptr_t; they do not perform any</span>
<span class="line-modified">261   // scaling of add_value, as that has already been done by the caller.</span>

262   //
<a name="21" id="anc21"></a><span class="line-modified">263   // No definition is provided; all platforms must explicitly define</span>
<span class="line-modified">264   // this class and any needed specializations.</span>
<span class="line-modified">265   template&lt;size_t byte_size&gt; struct PlatformAdd;</span>





266 
267   // Support for platforms that implement some variants of add using a
268   // (typically out of line) non-template helper function.  The
269   // generic arguments passed to PlatformAdd need to be translated to
270   // the appropriate type for the helper function, the helper function
271   // invoked on the translated arguments, and the result translated
272   // back.  Type is the parameter / return type of the helper
273   // function.  No scaling of add_value is performed when D is a pointer
274   // type, so this function can be used to implement the support function
275   // required by AddAndFetch.
<a name="22" id="anc22"></a><span class="line-modified">276   template&lt;typename Type, typename Fn, typename D, typename I&gt;</span>
<span class="line-modified">277   static D add_using_helper(Fn fn, D volatile* dest, I add_value);</span>
278 
279   // Dispatch handler for cmpxchg.  Provides type-based validity
280   // checking and limited conversions around calls to the
281   // platform-specific implementation layer provided by
282   // PlatformCmpxchg.
<a name="23" id="anc23"></a><span class="line-modified">283   template&lt;typename D, typename U, typename T, typename Enable = void&gt;</span>
284   struct CmpxchgImpl;
285 
286   // Platform-specific implementation of cmpxchg.  Support for sizes
287   // of 1, 4, and 8 are required.  The class is a function object that
288   // must be default constructable, with these requirements:
289   //
290   // - dest is of type T*.
291   // - exchange_value and compare_value are of type T.
292   // - order is of type atomic_memory_order.
293   // - platform_cmpxchg is an object of type PlatformCmpxchg&lt;sizeof(T)&gt;.
294   //
295   // Then
<a name="24" id="anc24"></a><span class="line-modified">296   //   platform_cmpxchg(dest, compare_value, exchange_value, order)</span>
297   // must be a valid expression, returning a result convertible to T.
298   //
299   // A default definition is provided, which declares a function template
<a name="25" id="anc25"></a><span class="line-modified">300   //   T operator()(T volatile*, T, T, atomic_memory_order) const</span>
301   //
302   // For each required size, a platform must either provide an
303   // appropriate definition of that function, or must entirely
304   // specialize the class template for that size.
305   template&lt;size_t byte_size&gt; struct PlatformCmpxchg;
306 
307   // Support for platforms that implement some variants of cmpxchg
308   // using a (typically out of line) non-template helper function.
309   // The generic arguments passed to PlatformCmpxchg need to be
310   // translated to the appropriate type for the helper function, the
311   // helper invoked on the translated arguments, and the result
312   // translated back.  Type is the parameter / return type of the
313   // helper function.
314   template&lt;typename Type, typename Fn, typename T&gt;
315   static T cmpxchg_using_helper(Fn fn,
<a name="26" id="anc26"></a>
316                                 T volatile* dest,
<a name="27" id="anc27"></a><span class="line-modified">317                                 T compare_value,</span>
<span class="line-added">318                                 T exchange_value);</span>
319 
320   // Support platforms that do not provide Read-Modify-Write
321   // byte-level atomic access. To use, derive PlatformCmpxchg&lt;1&gt; from
322   // this class.
323 public: // Temporary, can&#39;t be private: C++03 11.4/2. Fixed by C++11.
324   struct CmpxchgByteUsingInt;
325 private:
326 
327   // Dispatch handler for xchg.  Provides type-based validity
328   // checking and limited conversions around calls to the
329   // platform-specific implementation layer provided by
330   // PlatformXchg.
<a name="28" id="anc28"></a><span class="line-modified">331   template&lt;typename D, typename T, typename Enable = void&gt;</span>
332   struct XchgImpl;
333 
334   // Platform-specific implementation of xchg.  Support for sizes
335   // of 4, and sizeof(intptr_t) are required.  The class is a function
336   // object that must be default constructable, with these requirements:
337   //
338   // - dest is of type T*.
339   // - exchange_value is of type T.
340   // - platform_xchg is an object of type PlatformXchg&lt;sizeof(T)&gt;.
341   //
342   // Then
<a name="29" id="anc29"></a><span class="line-modified">343   //   platform_xchg(dest, exchange_value)</span>
344   // must be a valid expression, returning a result convertible to T.
345   //
346   // A default definition is provided, which declares a function template
<a name="30" id="anc30"></a><span class="line-modified">347   //   T operator()(T volatile*, T, atomic_memory_order) const</span>
348   //
349   // For each required size, a platform must either provide an
350   // appropriate definition of that function, or must entirely
351   // specialize the class template for that size.
352   template&lt;size_t byte_size&gt; struct PlatformXchg;
353 
354   // Support for platforms that implement some variants of xchg
355   // using a (typically out of line) non-template helper function.
356   // The generic arguments passed to PlatformXchg need to be
357   // translated to the appropriate type for the helper function, the
358   // helper invoked on the translated arguments, and the result
359   // translated back.  Type is the parameter / return type of the
360   // helper function.
361   template&lt;typename Type, typename Fn, typename T&gt;
362   static T xchg_using_helper(Fn fn,
<a name="31" id="anc31"></a><span class="line-modified">363                              T volatile* dest,</span>
<span class="line-modified">364                              T exchange_value);</span>
365 };
366 
367 template&lt;typename From, typename To&gt;
368 struct Atomic::IsPointerConvertible&lt;From*, To*&gt; : AllStatic {
369   // Determine whether From* is implicitly convertible to To*, using
370   // the &quot;sizeof trick&quot;.
371   typedef char yes;
372   typedef char (&amp;no)[2];
373 
374   static yes test(To*);
375   static no test(...);
376   static From* test_value;
377 
378   static const bool value = (sizeof(yes) == sizeof(test(test_value)));
379 };
380 
381 // Handle load for pointer, integral and enum types.
382 template&lt;typename T, typename PlatformOp&gt;
383 struct Atomic::LoadImpl&lt;
384   T,
385   PlatformOp,
386   typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value || IsPointer&lt;T&gt;::value&gt;::type&gt;
387 {
388   T operator()(T const volatile* dest) const {
389     // Forward to the platform handler for the size of T.
390     return PlatformOp()(dest);
391   }
392 };
393 
394 // Handle load for types that have a translator.
395 //
396 // All the involved types must be identical.
397 //
398 // This translates the original call into a call on the decayed
399 // arguments, and returns the recovered result of that translated
400 // call.
401 template&lt;typename T, typename PlatformOp&gt;
402 struct Atomic::LoadImpl&lt;
403   T,
404   PlatformOp,
405   typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
406 {
407   T operator()(T const volatile* dest) const {
408     typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
409     typedef typename Translator::Decayed Decayed;
410     STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
411     Decayed result = PlatformOp()(reinterpret_cast&lt;Decayed const volatile*&gt;(dest));
412     return Translator::recover(result);
413   }
414 };
415 
416 // Default implementation of atomic load if a specific platform
417 // does not provide a specialization for a certain size class.
418 // For increased safety, the default implementation only allows
419 // load types that are pointer sized or smaller. If a platform still
420 // supports wide atomics, then it has to use specialization
421 // of Atomic::PlatformLoad for that wider size class.
422 template&lt;size_t byte_size&gt;
423 struct Atomic::PlatformLoad {
424   template&lt;typename T&gt;
425   T operator()(T const volatile* dest) const {
426     STATIC_ASSERT(sizeof(T) &lt;= sizeof(void*)); // wide atomics need specialization
427     return *dest;
428   }
429 };
430 
431 // Handle store for integral and enum types.
432 //
433 // All the involved types must be identical.
434 template&lt;typename T, typename PlatformOp&gt;
435 struct Atomic::StoreImpl&lt;
436   T, T,
437   PlatformOp,
438   typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
439 {
<a name="32" id="anc32"></a><span class="line-modified">440   void operator()(T volatile* dest, T new_value) const {</span>
441     // Forward to the platform handler for the size of T.
<a name="33" id="anc33"></a><span class="line-modified">442     PlatformOp()(dest, new_value);</span>
443   }
444 };
445 
446 // Handle store for pointer types.
447 //
448 // The new_value must be implicitly convertible to the
449 // destination&#39;s type; it must be type-correct to store the
450 // new_value in the destination.
<a name="34" id="anc34"></a><span class="line-modified">451 template&lt;typename D, typename T, typename PlatformOp&gt;</span>
452 struct Atomic::StoreImpl&lt;
<a name="35" id="anc35"></a><span class="line-modified">453   D*, T*,</span>
454   PlatformOp,
455   typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value&gt;::type&gt;
456 {
<a name="36" id="anc36"></a><span class="line-modified">457   void operator()(D* volatile* dest, T* new_value) const {</span>
458     // Allow derived to base conversion, and adding cv-qualifiers.
459     D* value = new_value;
<a name="37" id="anc37"></a><span class="line-modified">460     PlatformOp()(dest, value);</span>
461   }
462 };
463 
464 // Handle store for types that have a translator.
465 //
466 // All the involved types must be identical.
467 //
468 // This translates the original call into a call on the decayed
469 // arguments.
470 template&lt;typename T, typename PlatformOp&gt;
471 struct Atomic::StoreImpl&lt;
472   T, T,
473   PlatformOp,
474   typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
475 {
<a name="38" id="anc38"></a><span class="line-modified">476   void operator()(T volatile* dest, T new_value) const {</span>
477     typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
478     typedef typename Translator::Decayed Decayed;
479     STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
<a name="39" id="anc39"></a><span class="line-modified">480     PlatformOp()(reinterpret_cast&lt;Decayed volatile*&gt;(dest),</span>
<span class="line-modified">481                  Translator::decay(new_value));</span>
482   }
483 };
484 
485 // Default implementation of atomic store if a specific platform
486 // does not provide a specialization for a certain size class.
487 // For increased safety, the default implementation only allows
488 // storing types that are pointer sized or smaller. If a platform still
489 // supports wide atomics, then it has to use specialization
490 // of Atomic::PlatformStore for that wider size class.
491 template&lt;size_t byte_size&gt;
492 struct Atomic::PlatformStore {
493   template&lt;typename T&gt;
<a name="40" id="anc40"></a><span class="line-modified">494   void operator()(T volatile* dest,</span>
<span class="line-modified">495                   T new_value) const {</span>
496     STATIC_ASSERT(sizeof(T) &lt;= sizeof(void*)); // wide atomics need specialization
497     (void)const_cast&lt;T&amp;&gt;(*dest = new_value);
498   }
499 };
500 
<a name="41" id="anc41"></a>















501 template&lt;typename D&gt;
502 inline void Atomic::inc(D volatile* dest, atomic_memory_order order) {
503   STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
504   typedef typename Conditional&lt;IsPointer&lt;D&gt;::value, ptrdiff_t, D&gt;::type I;
<a name="42" id="anc42"></a><span class="line-modified">505   Atomic::add(dest, I(1), order);</span>
506 }
507 
508 template&lt;typename D&gt;
509 inline void Atomic::dec(D volatile* dest, atomic_memory_order order) {
510   STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
511   typedef typename Conditional&lt;IsPointer&lt;D&gt;::value, ptrdiff_t, D&gt;::type I;
512   // Assumes two&#39;s complement integer representation.
513   #pragma warning(suppress: 4146)
<a name="43" id="anc43"></a><span class="line-modified">514   Atomic::add(dest, I(-1), order);</span>
515 }
516 
<a name="44" id="anc44"></a><span class="line-modified">517 template&lt;typename D, typename I&gt;</span>
<span class="line-modified">518 inline D Atomic::sub(D volatile* dest, I sub_value, atomic_memory_order order) {</span>
519   STATIC_ASSERT(IsPointer&lt;D&gt;::value || IsIntegral&lt;D&gt;::value);
520   STATIC_ASSERT(IsIntegral&lt;I&gt;::value);
521   // If D is a pointer type, use [u]intptr_t as the addend type,
522   // matching signedness of I.  Otherwise, use D as the addend type.
523   typedef typename Conditional&lt;IsSigned&lt;I&gt;::value, intptr_t, uintptr_t&gt;::type PI;
524   typedef typename Conditional&lt;IsPointer&lt;D&gt;::value, PI, D&gt;::type AddendType;
525   // Only allow conversions that can&#39;t change the value.
526   STATIC_ASSERT(IsSigned&lt;I&gt;::value == IsSigned&lt;AddendType&gt;::value);
527   STATIC_ASSERT(sizeof(I) &lt;= sizeof(AddendType));
528   AddendType addend = sub_value;
529   // Assumes two&#39;s complement integer representation.
530   #pragma warning(suppress: 4146) // In case AddendType is not signed.
<a name="45" id="anc45"></a><span class="line-modified">531   return Atomic::add(dest, -addend, order);</span>
532 }
533 
534 // Define the class before including platform file, which may specialize
535 // the operator definition.  No generic definition of specializations
536 // of the operator template are provided, nor are there any generic
537 // specializations of the class.  The platform file is responsible for
538 // providing those.
539 template&lt;size_t byte_size&gt;
540 struct Atomic::PlatformCmpxchg {
541   template&lt;typename T&gt;
<a name="46" id="anc46"></a><span class="line-modified">542   T operator()(T volatile* dest,</span>

543                T compare_value,
<a name="47" id="anc47"></a><span class="line-added">544                T exchange_value,</span>
545                atomic_memory_order order) const;
546 };
547 
548 // Define the class before including platform file, which may use this
549 // as a base class, requiring it be complete.  The definition is later
550 // in this file, near the other definitions related to cmpxchg.
551 struct Atomic::CmpxchgByteUsingInt {
<a name="48" id="anc48"></a><span class="line-added">552   static uint8_t get_byte_in_int(uint32_t n, uint32_t idx);</span>
<span class="line-added">553   static uint32_t set_byte_in_int(uint32_t n, uint8_t b, uint32_t idx);</span>
554   template&lt;typename T&gt;
<a name="49" id="anc49"></a><span class="line-modified">555   T operator()(T volatile* dest,</span>

556                T compare_value,
<a name="50" id="anc50"></a><span class="line-added">557                T exchange_value,</span>
558                atomic_memory_order order) const;
559 };
560 
561 // Define the class before including platform file, which may specialize
562 // the operator definition.  No generic definition of specializations
563 // of the operator template are provided, nor are there any generic
564 // specializations of the class.  The platform file is responsible for
565 // providing those.
566 template&lt;size_t byte_size&gt;
567 struct Atomic::PlatformXchg {
568   template&lt;typename T&gt;
<a name="51" id="anc51"></a><span class="line-modified">569   T operator()(T volatile* dest,</span>
<span class="line-modified">570                T exchange_value,</span>
571                atomic_memory_order order) const;
572 };
573 
<a name="52" id="anc52"></a><span class="line-added">574 template &lt;ScopedFenceType T&gt;</span>
<span class="line-added">575 class ScopedFenceGeneral: public StackObj {</span>
<span class="line-added">576  public:</span>
<span class="line-added">577   void prefix() {}</span>
<span class="line-added">578   void postfix() {}</span>
<span class="line-added">579 };</span>
<span class="line-added">580 </span>
<span class="line-added">581 // The following methods can be specialized using simple template specialization</span>
<span class="line-added">582 // in the platform specific files for optimization purposes. Otherwise the</span>
<span class="line-added">583 // generalized variant is used.</span>
<span class="line-added">584 </span>
<span class="line-added">585 template&lt;&gt; inline void ScopedFenceGeneral&lt;X_ACQUIRE&gt;::postfix()       { OrderAccess::acquire(); }</span>
<span class="line-added">586 template&lt;&gt; inline void ScopedFenceGeneral&lt;RELEASE_X&gt;::prefix()        { OrderAccess::release(); }</span>
<span class="line-added">587 template&lt;&gt; inline void ScopedFenceGeneral&lt;RELEASE_X_FENCE&gt;::prefix()  { OrderAccess::release(); }</span>
<span class="line-added">588 template&lt;&gt; inline void ScopedFenceGeneral&lt;RELEASE_X_FENCE&gt;::postfix() { OrderAccess::fence();   }</span>
<span class="line-added">589 </span>
<span class="line-added">590 template &lt;ScopedFenceType T&gt;</span>
<span class="line-added">591 class ScopedFence : public ScopedFenceGeneral&lt;T&gt; {</span>
<span class="line-added">592   void *const _field;</span>
<span class="line-added">593  public:</span>
<span class="line-added">594   ScopedFence(void *const field) : _field(field) { prefix(); }</span>
<span class="line-added">595   ~ScopedFence() { postfix(); }</span>
<span class="line-added">596   void prefix() { ScopedFenceGeneral&lt;T&gt;::prefix(); }</span>
<span class="line-added">597   void postfix() { ScopedFenceGeneral&lt;T&gt;::postfix(); }</span>
<span class="line-added">598 };</span>
<span class="line-added">599 </span>
600 // platform specific in-line definitions - must come before shared definitions
601 
602 #include OS_CPU_HEADER(atomic)
603 
604 // shared in-line definitions
605 
606 // size_t casts...
607 #if (SIZE_MAX != UINTPTR_MAX)
608 #error size_t is not WORD_SIZE, interesting platform, but missing implementation here
609 #endif
610 
611 template&lt;typename T&gt;
612 inline T Atomic::load(const volatile T* dest) {
613   return LoadImpl&lt;T, PlatformLoad&lt;sizeof(T)&gt; &gt;()(dest);
614 }
615 
<a name="53" id="anc53"></a><span class="line-modified">616 template&lt;size_t byte_size, ScopedFenceType type&gt;</span>
<span class="line-modified">617 struct Atomic::PlatformOrderedLoad {</span>
<span class="line-modified">618   template &lt;typename T&gt;</span>
<span class="line-added">619   T operator()(const volatile T* p) const {</span>
<span class="line-added">620     ScopedFence&lt;type&gt; f((void*)p);</span>
<span class="line-added">621     return Atomic::load(p);</span>
<span class="line-added">622   }</span>
<span class="line-added">623 };</span>
<span class="line-added">624 </span>
<span class="line-added">625 template &lt;typename T&gt;</span>
<span class="line-added">626 inline T Atomic::load_acquire(const volatile T* p) {</span>
<span class="line-added">627   return LoadImpl&lt;T, PlatformOrderedLoad&lt;sizeof(T), X_ACQUIRE&gt; &gt;()(p);</span>
<span class="line-added">628 }</span>
<span class="line-added">629 </span>
<span class="line-added">630 template&lt;typename D, typename T&gt;</span>
<span class="line-added">631 inline void Atomic::store(volatile D* dest, T store_value) {</span>
<span class="line-added">632   StoreImpl&lt;D, T, PlatformStore&lt;sizeof(D)&gt; &gt;()(dest, store_value);</span>
633 }
634 
<a name="54" id="anc54"></a><span class="line-modified">635 template&lt;size_t byte_size, ScopedFenceType type&gt;</span>
<span class="line-modified">636 struct Atomic::PlatformOrderedStore {</span>
<span class="line-added">637   template &lt;typename T&gt;</span>
<span class="line-added">638   void operator()(volatile T* p, T v) const {</span>
<span class="line-added">639     ScopedFence&lt;type&gt; f((void*)p);</span>
<span class="line-added">640     Atomic::store(p, v);</span>
<span class="line-added">641   }</span>
<span class="line-added">642 };</span>
<span class="line-added">643 </span>
<span class="line-added">644 template &lt;typename D, typename T&gt;</span>
<span class="line-added">645 inline void Atomic::release_store(volatile D* p, T v) {</span>
<span class="line-added">646   StoreImpl&lt;D, T, PlatformOrderedStore&lt;sizeof(D), RELEASE_X&gt; &gt;()(p, v);</span>
<span class="line-added">647 }</span>
<span class="line-added">648 </span>
<span class="line-added">649 template &lt;typename D, typename T&gt;</span>
<span class="line-added">650 inline void Atomic::release_store_fence(volatile D* p, T v) {</span>
<span class="line-added">651   StoreImpl&lt;D, T, PlatformOrderedStore&lt;sizeof(D), RELEASE_X_FENCE&gt; &gt;()(p, v);</span>
<span class="line-added">652 }</span>
<span class="line-added">653 </span>
<span class="line-added">654 template&lt;typename D, typename I&gt;</span>
<span class="line-added">655 inline D Atomic::add(D volatile* dest, I add_value,</span>
656                      atomic_memory_order order) {
<a name="55" id="anc55"></a><span class="line-modified">657   return AddImpl&lt;D, I&gt;::add_and_fetch(dest, add_value, order);</span>
658 }
659 
<a name="56" id="anc56"></a><span class="line-modified">660 template&lt;typename D, typename I&gt;</span>
<span class="line-added">661 inline D Atomic::fetch_and_add(D volatile* dest, I add_value,</span>
<span class="line-added">662                                atomic_memory_order order) {</span>
<span class="line-added">663   return AddImpl&lt;D, I&gt;::fetch_and_add(dest, add_value, order);</span>
<span class="line-added">664 }</span>
<span class="line-added">665 </span>
<span class="line-added">666 template&lt;typename D, typename I&gt;</span>
667 struct Atomic::AddImpl&lt;
<a name="57" id="anc57"></a><span class="line-modified">668   D, I,</span>
669   typename EnableIf&lt;IsIntegral&lt;I&gt;::value &amp;&amp;
670                     IsIntegral&lt;D&gt;::value &amp;&amp;
671                     (sizeof(I) &lt;= sizeof(D)) &amp;&amp;
672                     (IsSigned&lt;I&gt;::value == IsSigned&lt;D&gt;::value)&gt;::type&gt;
673 {
<a name="58" id="anc58"></a><span class="line-modified">674   static D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) {</span>
675     D addend = add_value;
<a name="59" id="anc59"></a><span class="line-modified">676     return PlatformAdd&lt;sizeof(D)&gt;().add_and_fetch(dest, addend, order);</span>
<span class="line-added">677   }</span>
<span class="line-added">678   static D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order order) {</span>
<span class="line-added">679     D addend = add_value;</span>
<span class="line-added">680     return PlatformAdd&lt;sizeof(D)&gt;().fetch_and_add(dest, addend, order);</span>
681   }
682 };
683 
<a name="60" id="anc60"></a><span class="line-modified">684 template&lt;typename P, typename I&gt;</span>
685 struct Atomic::AddImpl&lt;
<a name="61" id="anc61"></a><span class="line-modified">686   P*, I,</span>
687   typename EnableIf&lt;IsIntegral&lt;I&gt;::value &amp;&amp; (sizeof(I) &lt;= sizeof(P*))&gt;::type&gt;
688 {
<a name="62" id="anc62"></a><span class="line-modified">689   STATIC_ASSERT(sizeof(intptr_t) == sizeof(P*));</span>
<span class="line-modified">690   STATIC_ASSERT(sizeof(uintptr_t) == sizeof(P*));</span>
<span class="line-modified">691   typedef typename Conditional&lt;IsSigned&lt;I&gt;::value,</span>
<span class="line-modified">692                                intptr_t,</span>
<span class="line-modified">693                                uintptr_t&gt;::type CI;</span>
<span class="line-modified">694 </span>
<span class="line-modified">695   static CI scale_addend(CI add_value) {</span>
<span class="line-modified">696     return add_value * sizeof(P);</span>
697   }
<a name="63" id="anc63"></a>
698 
<a name="64" id="anc64"></a><span class="line-modified">699   static P* add_and_fetch(P* volatile* dest, I add_value, atomic_memory_order order) {</span>
<span class="line-modified">700     CI addend = add_value;</span>
<span class="line-modified">701     return PlatformAdd&lt;sizeof(P*)&gt;().add_and_fetch(dest, scale_addend(addend), order);</span>





702   }
<a name="65" id="anc65"></a><span class="line-modified">703   static P* fetch_and_add(P* volatile* dest, I add_value, atomic_memory_order order) {</span>
<span class="line-modified">704     CI addend = add_value;</span>
<span class="line-modified">705     return PlatformAdd&lt;sizeof(P*)&gt;().fetch_and_add(dest, scale_addend(addend), order);</span>








706   }
<a name="66" id="anc66"></a><span class="line-modified">707 };</span>

708 
<a name="67" id="anc67"></a><span class="line-modified">709 template&lt;typename Type, typename Fn, typename D, typename I&gt;</span>
<span class="line-modified">710 inline D Atomic::add_using_helper(Fn fn, D volatile* dest, I add_value) {</span>
711   return PrimitiveConversions::cast&lt;D&gt;(
712     fn(PrimitiveConversions::cast&lt;Type&gt;(add_value),
713        reinterpret_cast&lt;Type volatile*&gt;(dest)));
714 }
715 
<a name="68" id="anc68"></a><span class="line-modified">716 template&lt;typename D, typename U, typename T&gt;</span>
<span class="line-modified">717 inline D Atomic::cmpxchg(D volatile* dest,</span>

718                          U compare_value,
<a name="69" id="anc69"></a><span class="line-added">719                          T exchange_value,</span>
720                          atomic_memory_order order) {
<a name="70" id="anc70"></a><span class="line-modified">721   return CmpxchgImpl&lt;D, U, T&gt;()(dest, compare_value, exchange_value, order);</span>
722 }
723 
<a name="71" id="anc71"></a><span class="line-modified">724 template&lt;typename D, typename T&gt;</span>
<span class="line-modified">725 inline bool Atomic::replace_if_null(D* volatile* dest, T* value,</span>
726                                     atomic_memory_order order) {
727   // Presently using a trivial implementation in terms of cmpxchg.
728   // Consider adding platform support, to permit the use of compiler
729   // intrinsics like gcc&#39;s __sync_bool_compare_and_swap.
730   D* expected_null = NULL;
<a name="72" id="anc72"></a><span class="line-modified">731   return expected_null == cmpxchg(dest, expected_null, value, order);</span>
732 }
733 
734 // Handle cmpxchg for integral and enum types.
735 //
736 // All the involved types must be identical.
737 template&lt;typename T&gt;
738 struct Atomic::CmpxchgImpl&lt;
739   T, T, T,
740   typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
741 {
<a name="73" id="anc73"></a><span class="line-modified">742   T operator()(T volatile* dest, T compare_value, T exchange_value,</span>
743                atomic_memory_order order) const {
744     // Forward to the platform handler for the size of T.
<a name="74" id="anc74"></a><span class="line-modified">745     return PlatformCmpxchg&lt;sizeof(T)&gt;()(dest,</span>

746                                         compare_value,
<a name="75" id="anc75"></a><span class="line-added">747                                         exchange_value,</span>
748                                         order);
749   }
750 };
751 
752 // Handle cmpxchg for pointer types.
753 //
754 // The destination&#39;s type and the compare_value type must be the same,
755 // ignoring cv-qualifiers; we don&#39;t care about the cv-qualifiers of
756 // the compare_value.
757 //
758 // The exchange_value must be implicitly convertible to the
759 // destination&#39;s type; it must be type-correct to store the
760 // exchange_value in the destination.
<a name="76" id="anc76"></a><span class="line-modified">761 template&lt;typename D, typename U, typename T&gt;</span>
762 struct Atomic::CmpxchgImpl&lt;
<a name="77" id="anc77"></a><span class="line-modified">763   D*, U*, T*,</span>
764   typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value &amp;&amp;
765                     IsSame&lt;typename RemoveCV&lt;D&gt;::type,
766                            typename RemoveCV&lt;U&gt;::type&gt;::value&gt;::type&gt;
767 {
<a name="78" id="anc78"></a><span class="line-modified">768   D* operator()(D* volatile* dest, U* compare_value, T* exchange_value,</span>
769                atomic_memory_order order) const {
770     // Allow derived to base conversion, and adding cv-qualifiers.
771     D* new_value = exchange_value;
772     // Don&#39;t care what the CV qualifiers for compare_value are,
773     // but we need to match D* when calling platform support.
774     D* old_value = const_cast&lt;D*&gt;(compare_value);
<a name="79" id="anc79"></a><span class="line-modified">775     return PlatformCmpxchg&lt;sizeof(D*)&gt;()(dest, old_value, new_value, order);</span>
776   }
777 };
778 
779 // Handle cmpxchg for types that have a translator.
780 //
781 // All the involved types must be identical.
782 //
783 // This translates the original call into a call on the decayed
784 // arguments, and returns the recovered result of that translated
785 // call.
786 template&lt;typename T&gt;
787 struct Atomic::CmpxchgImpl&lt;
788   T, T, T,
789   typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
790 {
<a name="80" id="anc80"></a><span class="line-modified">791   T operator()(T volatile* dest, T compare_value, T exchange_value,</span>
792                atomic_memory_order order) const {
793     typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
794     typedef typename Translator::Decayed Decayed;
795     STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
796     return Translator::recover(
<a name="81" id="anc81"></a><span class="line-modified">797       cmpxchg(reinterpret_cast&lt;Decayed volatile*&gt;(dest),</span>

798               Translator::decay(compare_value),
<a name="82" id="anc82"></a><span class="line-added">799               Translator::decay(exchange_value),</span>
800               order));
801   }
802 };
803 
804 template&lt;typename Type, typename Fn, typename T&gt;
805 inline T Atomic::cmpxchg_using_helper(Fn fn,
<a name="83" id="anc83"></a>
806                                       T volatile* dest,
<a name="84" id="anc84"></a><span class="line-modified">807                                       T compare_value,</span>
<span class="line-added">808                                       T exchange_value) {</span>
809   STATIC_ASSERT(sizeof(Type) == sizeof(T));
810   return PrimitiveConversions::cast&lt;T&gt;(
811     fn(PrimitiveConversions::cast&lt;Type&gt;(exchange_value),
812        reinterpret_cast&lt;Type volatile*&gt;(dest),
813        PrimitiveConversions::cast&lt;Type&gt;(compare_value)));
814 }
815 
<a name="85" id="anc85"></a><span class="line-added">816 inline uint32_t Atomic::CmpxchgByteUsingInt::set_byte_in_int(uint32_t n,</span>
<span class="line-added">817                                                              uint8_t b,</span>
<span class="line-added">818                                                              uint32_t idx) {</span>
<span class="line-added">819   int bitsIdx = BitsPerByte * idx;</span>
<span class="line-added">820   return (n &amp; ~(0xff &lt;&lt; bitsIdx)) | (b &lt;&lt; bitsIdx);</span>
<span class="line-added">821 }</span>
<span class="line-added">822 </span>
<span class="line-added">823 inline uint8_t Atomic::CmpxchgByteUsingInt::get_byte_in_int(uint32_t n,</span>
<span class="line-added">824                                                             uint32_t idx) {</span>
<span class="line-added">825   int bitsIdx = BitsPerByte * idx;</span>
<span class="line-added">826   return (uint8_t)(n &gt;&gt; bitsIdx);</span>
<span class="line-added">827 }</span>
<span class="line-added">828 </span>
829 template&lt;typename T&gt;
<a name="86" id="anc86"></a><span class="line-modified">830 inline T Atomic::CmpxchgByteUsingInt::operator()(T volatile* dest,</span>

831                                                  T compare_value,
<a name="87" id="anc87"></a><span class="line-added">832                                                  T exchange_value,</span>
833                                                  atomic_memory_order order) const {
834   STATIC_ASSERT(sizeof(T) == sizeof(uint8_t));
835   uint8_t canon_exchange_value = exchange_value;
836   uint8_t canon_compare_value = compare_value;
837   volatile uint32_t* aligned_dest
838     = reinterpret_cast&lt;volatile uint32_t*&gt;(align_down(dest, sizeof(uint32_t)));
839   size_t offset = pointer_delta(dest, aligned_dest, 1);
<a name="88" id="anc88"></a><span class="line-modified">840 </span>
<span class="line-modified">841   uint32_t idx = (Endian::NATIVE == Endian::BIG)</span>
<span class="line-added">842                    ? (sizeof(uint32_t) - 1 - offset)</span>
<span class="line-added">843                    : offset;</span>
844 
845   // current value may not be what we are looking for, so force it
846   // to that value so the initial cmpxchg will fail if it is different
<a name="89" id="anc89"></a><span class="line-modified">847   uint32_t cur = set_byte_in_int(Atomic::load(aligned_dest), canon_compare_value, idx);</span>
848 
849   // always execute a real cmpxchg so that we get the required memory
850   // barriers even on initial failure
851   do {
<a name="90" id="anc90"></a><span class="line-modified">852     // value to swap in matches current value</span>
<span class="line-modified">853     // except for the one byte we want to update</span>
<span class="line-modified">854     uint32_t new_value = set_byte_in_int(cur, canon_exchange_value, idx);</span>

855 
<a name="91" id="anc91"></a><span class="line-modified">856     uint32_t res = cmpxchg(aligned_dest, cur, new_value, order);</span>
857     if (res == cur) break;      // success
858 
859     // at least one byte in the int changed value, so update
860     // our view of the current int
861     cur = res;
862     // if our byte is still as cur we loop and try again
<a name="92" id="anc92"></a><span class="line-modified">863   } while (get_byte_in_int(cur, idx) == canon_compare_value);</span>
864 
<a name="93" id="anc93"></a><span class="line-modified">865   return PrimitiveConversions::cast&lt;T&gt;(get_byte_in_int(cur, idx));</span>
866 }
867 
868 // Handle xchg for integral and enum types.
869 //
870 // All the involved types must be identical.
871 template&lt;typename T&gt;
872 struct Atomic::XchgImpl&lt;
873   T, T,
874   typename EnableIf&lt;IsIntegral&lt;T&gt;::value || IsRegisteredEnum&lt;T&gt;::value&gt;::type&gt;
875 {
<a name="94" id="anc94"></a><span class="line-modified">876   T operator()(T volatile* dest, T exchange_value, atomic_memory_order order) const {</span>
877     // Forward to the platform handler for the size of T.
<a name="95" id="anc95"></a><span class="line-modified">878     return PlatformXchg&lt;sizeof(T)&gt;()(dest, exchange_value, order);</span>
879   }
880 };
881 
882 // Handle xchg for pointer types.
883 //
884 // The exchange_value must be implicitly convertible to the
885 // destination&#39;s type; it must be type-correct to store the
886 // exchange_value in the destination.
<a name="96" id="anc96"></a><span class="line-modified">887 template&lt;typename D, typename T&gt;</span>
888 struct Atomic::XchgImpl&lt;
<a name="97" id="anc97"></a><span class="line-modified">889   D*, T*,</span>
890   typename EnableIf&lt;Atomic::IsPointerConvertible&lt;T*, D*&gt;::value&gt;::type&gt;
891 {
<a name="98" id="anc98"></a><span class="line-modified">892   D* operator()(D* volatile* dest, T* exchange_value, atomic_memory_order order) const {</span>
893     // Allow derived to base conversion, and adding cv-qualifiers.
894     D* new_value = exchange_value;
<a name="99" id="anc99"></a><span class="line-modified">895     return PlatformXchg&lt;sizeof(D*)&gt;()(dest, new_value, order);</span>
896   }
897 };
898 
899 // Handle xchg for types that have a translator.
900 //
901 // All the involved types must be identical.
902 //
903 // This translates the original call into a call on the decayed
904 // arguments, and returns the recovered result of that translated
905 // call.
906 template&lt;typename T&gt;
907 struct Atomic::XchgImpl&lt;
908   T, T,
909   typename EnableIf&lt;PrimitiveConversions::Translate&lt;T&gt;::value&gt;::type&gt;
910 {
<a name="100" id="anc100"></a><span class="line-modified">911   T operator()(T volatile* dest, T exchange_value, atomic_memory_order order) const {</span>
912     typedef PrimitiveConversions::Translate&lt;T&gt; Translator;
913     typedef typename Translator::Decayed Decayed;
914     STATIC_ASSERT(sizeof(T) == sizeof(Decayed));
915     return Translator::recover(
<a name="101" id="anc101"></a><span class="line-modified">916       xchg(reinterpret_cast&lt;Decayed volatile*&gt;(dest),</span>
<span class="line-modified">917            Translator::decay(exchange_value),</span>
918            order));
919   }
920 };
921 
922 template&lt;typename Type, typename Fn, typename T&gt;
923 inline T Atomic::xchg_using_helper(Fn fn,
<a name="102" id="anc102"></a><span class="line-modified">924                                    T volatile* dest,</span>
<span class="line-modified">925                                    T exchange_value) {</span>
926   STATIC_ASSERT(sizeof(Type) == sizeof(T));
<a name="103" id="anc103"></a><span class="line-added">927   // Notice the swapped order of arguments. Change when/if stubs are rewritten.</span>
928   return PrimitiveConversions::cast&lt;T&gt;(
929     fn(PrimitiveConversions::cast&lt;Type&gt;(exchange_value),
930        reinterpret_cast&lt;Type volatile*&gt;(dest)));
931 }
932 
<a name="104" id="anc104"></a><span class="line-modified">933 template&lt;typename D, typename T&gt;</span>
<span class="line-modified">934 inline D Atomic::xchg(volatile D* dest, T exchange_value, atomic_memory_order order) {</span>
<span class="line-modified">935   return XchgImpl&lt;D, T&gt;()(dest, exchange_value, order);</span>
936 }
937 
938 #endif // SHARE_RUNTIME_ATOMIC_HPP
<a name="105" id="anc105"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="105" type="hidden" />
</body>
</html>