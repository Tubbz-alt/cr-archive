<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/runtime/objectMonitor.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="mutexLocker.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="objectMonitor.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/runtime/objectMonitor.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1998, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;classfile/vmSymbols.hpp&quot;
  27 #include &quot;jfr/jfrEvents.hpp&quot;
  28 #include &quot;jfr/support/jfrThreadId.hpp&quot;


  29 #include &quot;memory/allocation.inline.hpp&quot;
  30 #include &quot;memory/resourceArea.hpp&quot;
<span class="line-modified">  31 #include &quot;oops/markOop.hpp&quot;</span>
  32 #include &quot;oops/oop.inline.hpp&quot;
  33 #include &quot;runtime/atomic.hpp&quot;
  34 #include &quot;runtime/handles.inline.hpp&quot;
  35 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  36 #include &quot;runtime/mutexLocker.hpp&quot;
  37 #include &quot;runtime/objectMonitor.hpp&quot;
  38 #include &quot;runtime/objectMonitor.inline.hpp&quot;
  39 #include &quot;runtime/orderAccess.hpp&quot;
  40 #include &quot;runtime/osThread.hpp&quot;
  41 #include &quot;runtime/safepointMechanism.inline.hpp&quot;
  42 #include &quot;runtime/sharedRuntime.hpp&quot;
  43 #include &quot;runtime/stubRoutines.hpp&quot;
  44 #include &quot;runtime/thread.inline.hpp&quot;
  45 #include &quot;services/threadService.hpp&quot;
  46 #include &quot;utilities/dtrace.hpp&quot;
  47 #include &quot;utilities/macros.hpp&quot;
  48 #include &quot;utilities/preserveException.hpp&quot;
  49 #if INCLUDE_JFR
  50 #include &quot;jfr/support/jfrFlush.hpp&quot;
  51 #endif
</pre>
<hr />
<pre>
 226   return AllocateHeap(size, mtInternal);
 227 }
 228 void* ObjectMonitor::operator new[] (size_t size) throw() {
 229   return operator new (size);
 230 }
 231 void ObjectMonitor::operator delete(void* p) {
 232   FreeHeap(p);
 233 }
 234 void ObjectMonitor::operator delete[] (void *p) {
 235   operator delete(p);
 236 }
 237 
 238 // -----------------------------------------------------------------------------
 239 // Enter support
 240 
 241 void ObjectMonitor::enter(TRAPS) {
 242   // The following code is ordered to check the most common cases first
 243   // and to reduce RTS-&gt;RTO cache line upgrades on SPARC and IA32 processors.
 244   Thread * const Self = THREAD;
 245 
<span class="line-modified"> 246   void * cur = Atomic::cmpxchg(Self, &amp;_owner, (void*)NULL);</span>
 247   if (cur == NULL) {
<span class="line-removed"> 248     // Either ASSERT _recursions == 0 or explicitly set _recursions = 0.</span>
 249     assert(_recursions == 0, &quot;invariant&quot;);
<span class="line-removed"> 250     assert(_owner == Self, &quot;invariant&quot;);</span>
 251     return;
 252   }
 253 
 254   if (cur == Self) {
 255     // TODO-FIXME: check for integer overflow!  BUGID 6557169.
 256     _recursions++;
 257     return;
 258   }
 259 
<span class="line-modified"> 260   if (Self-&gt;is_lock_owned ((address)cur)) {</span>
 261     assert(_recursions == 0, &quot;internal state error&quot;);
 262     _recursions = 1;
<span class="line-modified"> 263     // Commute owner from a thread-specific on-stack BasicLockObject address to</span>
<span class="line-removed"> 264     // a full-fledged &quot;Thread *&quot;.</span>
<span class="line-removed"> 265     _owner = Self;</span>
 266     return;
 267   }
 268 
 269   // We&#39;ve encountered genuine contention.
 270   assert(Self-&gt;_Stalled == 0, &quot;invariant&quot;);
 271   Self-&gt;_Stalled = intptr_t(this);
 272 
 273   // Try one round of spinning *before* enqueueing Self
 274   // and before going through the awkward and expensive state
 275   // transitions.  The following spin is strictly optional ...
 276   // Note that if we acquire the monitor from an initial spin
 277   // we forgo posting JVMTI events and firing DTRACE probes.
 278   if (TrySpin(Self) &gt; 0) {
<span class="line-modified"> 279     assert(_owner == Self, &quot;invariant&quot;);</span>
<span class="line-modified"> 280     assert(_recursions == 0, &quot;invariant&quot;);</span>
<span class="line-modified"> 281     assert(((oop)(object()))-&gt;mark() == markOopDesc::encode(this), &quot;invariant&quot;);</span>



 282     Self-&gt;_Stalled = 0;
 283     return;
 284   }
 285 
 286   assert(_owner != Self, &quot;invariant&quot;);
 287   assert(_succ != Self, &quot;invariant&quot;);
 288   assert(Self-&gt;is_Java_thread(), &quot;invariant&quot;);
 289   JavaThread * jt = (JavaThread *) Self;
 290   assert(!SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
 291   assert(jt-&gt;thread_state() != _thread_blocked, &quot;invariant&quot;);
 292   assert(this-&gt;object() != NULL, &quot;invariant&quot;);
<span class="line-modified"> 293   assert(_count &gt;= 0, &quot;invariant&quot;);</span>
 294 
 295   // Prevent deflation at STW-time.  See deflate_idle_monitors() and is_busy().
 296   // Ensure the object-monitor relationship remains stable while there&#39;s contention.
<span class="line-modified"> 297   Atomic::inc(&amp;_count);</span>
 298 
 299   JFR_ONLY(JfrConditionalFlushWithStacktrace&lt;EventJavaMonitorEnter&gt; flush(jt);)
 300   EventJavaMonitorEnter event;
 301   if (event.should_commit()) {
 302     event.set_monitorClass(((oop)this-&gt;object())-&gt;klass());
 303     event.set_address((uintptr_t)(this-&gt;object_addr()));
 304   }
 305 
 306   { // Change java thread status to indicate blocked on monitor enter.
 307     JavaThreadBlockedOnMonitorEnterState jtbmes(jt, this);
 308 
 309     Self-&gt;set_current_pending_monitor(this);
 310 
 311     DTRACE_MONITOR_PROBE(contended__enter, this, object(), jt);
 312     if (JvmtiExport::should_post_monitor_contended_enter()) {
 313       JvmtiExport::post_monitor_contended_enter(jt, this);
 314 
 315       // The current thread does not yet own the monitor and does not
 316       // yet appear on any queues that would get it made the successor.
 317       // This means that the JVMTI_EVENT_MONITOR_CONTENDED_ENTER event
</pre>
<hr />
<pre>
 338       // thread that suspended us.
 339       //
 340       _recursions = 0;
 341       _succ = NULL;
 342       exit(false, Self);
 343 
 344       jt-&gt;java_suspend_self();
 345     }
 346     Self-&gt;set_current_pending_monitor(NULL);
 347 
 348     // We cleared the pending monitor info since we&#39;ve just gotten past
 349     // the enter-check-for-suspend dance and we now own the monitor free
 350     // and clear, i.e., it is no longer pending. The ThreadBlockInVM
 351     // destructor can go to a safepoint at the end of this block. If we
 352     // do a thread dump during that safepoint, then this thread will show
 353     // as having &quot;-locked&quot; the monitor, but the OS and java.lang.Thread
 354     // states will still report that the thread is blocked trying to
 355     // acquire it.
 356   }
 357 
<span class="line-modified"> 358   Atomic::dec(&amp;_count);</span>
<span class="line-modified"> 359   assert(_count &gt;= 0, &quot;invariant&quot;);</span>
 360   Self-&gt;_Stalled = 0;
 361 
 362   // Must either set _recursions = 0 or ASSERT _recursions == 0.
 363   assert(_recursions == 0, &quot;invariant&quot;);
 364   assert(_owner == Self, &quot;invariant&quot;);
 365   assert(_succ != Self, &quot;invariant&quot;);
<span class="line-modified"> 366   assert(((oop)(object()))-&gt;mark() == markOopDesc::encode(this), &quot;invariant&quot;);</span>
 367 
 368   // The thread -- now the owner -- is back in vm mode.
 369   // Report the glorious news via TI,DTrace and jvmstat.
 370   // The probe effect is non-trivial.  All the reportage occurs
 371   // while we hold the monitor, increasing the length of the critical
 372   // section.  Amdahl&#39;s parallel speedup law comes vividly into play.
 373   //
 374   // Another option might be to aggregate the events (thread local or
 375   // per-monitor aggregation) and defer reporting until a more opportune
 376   // time -- such as next time some thread encounters contention but has
 377   // yet to acquire the lock.  While spinning that thread could
 378   // spinning we could increment JVMStat counters, etc.
 379 
 380   DTRACE_MONITOR_PROBE(contended__entered, this, object(), jt);
 381   if (JvmtiExport::should_post_monitor_contended_entered()) {
 382     JvmtiExport::post_monitor_contended_entered(jt, this);
 383 
 384     // The current thread already owns the monitor and is not going to
 385     // call park() for the remainder of the monitor enter protocol. So
 386     // it doesn&#39;t matter if the JVMTI_EVENT_MONITOR_CONTENDED_ENTERED
 387     // event handler consumed an unpark() issued by the thread that
 388     // just exited the monitor.
 389   }
 390   if (event.should_commit()) {
 391     event.set_previousOwner((uintptr_t)_previous_owner_tid);
 392     event.commit();
 393   }
 394   OM_PERFDATA_OP(ContendedLockAttempts, inc());
 395 }
 396 
 397 // Caveat: TryLock() is not necessarily serializing if it returns failure.
 398 // Callers must compensate as needed.
 399 
 400 int ObjectMonitor::TryLock(Thread * Self) {
 401   void * own = _owner;
 402   if (own != NULL) return 0;
<span class="line-modified"> 403   if (Atomic::replace_if_null(Self, &amp;_owner)) {</span>
<span class="line-removed"> 404     // Either guarantee _recursions == 0 or set _recursions = 0.</span>
 405     assert(_recursions == 0, &quot;invariant&quot;);
<span class="line-removed"> 406     assert(_owner == Self, &quot;invariant&quot;);</span>
 407     return 1;
 408   }
 409   // The lock had been free momentarily, but we lost the race to the lock.
 410   // Interference -- the CAS failed.
 411   // We can either return -1 or retry.
 412   // Retry doesn&#39;t make as much sense because the lock was just acquired.
 413   return -1;
 414 }
 415 









 416 #define MAX_RECHECK_INTERVAL 1000
 417 
 418 void ObjectMonitor::EnterI(TRAPS) {
 419   Thread * const Self = THREAD;
 420   assert(Self-&gt;is_Java_thread(), &quot;invariant&quot;);
 421   assert(((JavaThread *) Self)-&gt;thread_state() == _thread_blocked, &quot;invariant&quot;);
 422 
 423   // Try the lock - TATAS
 424   if (TryLock (Self) &gt; 0) {
 425     assert(_succ != Self, &quot;invariant&quot;);
 426     assert(_owner == Self, &quot;invariant&quot;);
 427     assert(_Responsible != Self, &quot;invariant&quot;);
 428     return;
 429   }
 430 
 431   assert(InitDone, &quot;Unexpectedly not initialized&quot;);
 432 
 433   // We try one round of spinning *before* enqueueing Self.
 434   //
 435   // If the _owner is ready but OFFPROC we could use a YieldTo()
</pre>
<hr />
<pre>
 453   //
 454   // Node acts as a proxy for Self.
 455   // As an aside, if were to ever rewrite the synchronization code mostly
 456   // in Java, WaitNodes, ObjectMonitors, and Events would become 1st-class
 457   // Java objects.  This would avoid awkward lifecycle and liveness issues,
 458   // as well as eliminate a subset of ABA issues.
 459   // TODO: eliminate ObjectWaiter and enqueue either Threads or Events.
 460 
 461   ObjectWaiter node(Self);
 462   Self-&gt;_ParkEvent-&gt;reset();
 463   node._prev   = (ObjectWaiter *) 0xBAD;
 464   node.TState  = ObjectWaiter::TS_CXQ;
 465 
 466   // Push &quot;Self&quot; onto the front of the _cxq.
 467   // Once on cxq/EntryList, Self stays on-queue until it acquires the lock.
 468   // Note that spinning tends to reduce the rate at which threads
 469   // enqueue and dequeue on EntryList|cxq.
 470   ObjectWaiter * nxt;
 471   for (;;) {
 472     node._next = nxt = _cxq;
<span class="line-modified"> 473     if (Atomic::cmpxchg(&amp;node, &amp;_cxq, nxt) == nxt) break;</span>
 474 
 475     // Interference - the CAS failed because _cxq changed.  Just retry.
 476     // As an optional optimization we retry the lock.
 477     if (TryLock (Self) &gt; 0) {
 478       assert(_succ != Self, &quot;invariant&quot;);
 479       assert(_owner == Self, &quot;invariant&quot;);
 480       assert(_Responsible != Self, &quot;invariant&quot;);
 481       return;
 482     }
 483   }
 484 
 485   // Check for cxq|EntryList edge transition to non-null.  This indicates
 486   // the onset of contention.  While contention persists exiting threads
 487   // will use a ST:MEMBAR:LD 1-1 exit protocol.  When contention abates exit
 488   // operations revert to the faster 1-0 mode.  This enter operation may interleave
 489   // (race) a concurrent 1-0 exit operation, resulting in stranding, so we
 490   // arrange for one of the contending thread to use a timed park() operations
 491   // to detect and recover from the race.  (Stranding is form of progress failure
 492   // where the monitor is unlocked but all the contending threads remain parked).
 493   // That is, at least one of the contended threads will periodically poll _owner.
 494   // One of the contending threads will become the designated &quot;Responsible&quot; thread.
 495   // The Responsible thread uses a timed park instead of a normal indefinite park
 496   // operation -- it periodically wakes and checks for and recovers from potential
 497   // strandings admitted by 1-0 exit operations.   We need at most one Responsible
 498   // thread per-monitor at any given moment.  Only threads on cxq|EntryList may
 499   // be responsible for a monitor.
 500   //
 501   // Currently, one of the contended threads takes on the added role of &quot;Responsible&quot;.
 502   // A viable alternative would be to use a dedicated &quot;stranding checker&quot; thread
 503   // that periodically iterated over all the threads (or active monitors) and unparked
 504   // successors where there was risk of stranding.  This would help eliminate the
 505   // timer scalability issues we see on some platforms as we&#39;d only have one thread
 506   // -- the checker -- parked on a timer.
 507 
 508   if (nxt == NULL &amp;&amp; _EntryList == NULL) {
 509     // Try to assume the role of responsible thread for the monitor.
 510     // CONSIDER:  ST vs CAS vs { if (Responsible==null) Responsible=Self }
<span class="line-modified"> 511     Atomic::replace_if_null(Self, &amp;_Responsible);</span>
 512   }
 513 
 514   // The lock might have been released while this thread was occupied queueing
 515   // itself onto _cxq.  To close the race and avoid &quot;stranding&quot; and
 516   // progress-liveness failure we must resample-retry _owner before parking.
 517   // Note the Dekker/Lamport duality: ST cxq; MEMBAR; LD Owner.
 518   // In this case the ST-MEMBAR is accomplished with CAS().
 519   //
 520   // TODO: Defer all thread state transitions until park-time.
 521   // Since state transitions are heavy and inefficient we&#39;d like
 522   // to defer the state transitions until absolutely necessary,
 523   // and in doing so avoid some transitions ...
 524 
 525   int nWakeups = 0;
 526   int recheckInterval = 1;
 527 
 528   for (;;) {
 529 
 530     if (TryLock(Self) &gt; 0) break;
 531     assert(_owner != Self, &quot;invariant&quot;);
</pre>
<hr />
<pre>
 567     // just spin again.  This pattern can repeat, leaving _succ to simply
 568     // spin on a CPU.
 569 
 570     if (_succ == Self) _succ = NULL;
 571 
 572     // Invariant: after clearing _succ a thread *must* retry _owner before parking.
 573     OrderAccess::fence();
 574   }
 575 
 576   // Egress :
 577   // Self has acquired the lock -- Unlink Self from the cxq or EntryList.
 578   // Normally we&#39;ll find Self on the EntryList .
 579   // From the perspective of the lock owner (this thread), the
 580   // EntryList is stable and cxq is prepend-only.
 581   // The head of cxq is volatile but the interior is stable.
 582   // In addition, Self.TState is stable.
 583 
 584   assert(_owner == Self, &quot;invariant&quot;);
 585   assert(object() != NULL, &quot;invariant&quot;);
 586   // I&#39;d like to write:
<span class="line-modified"> 587   //   guarantee (((oop)(object()))-&gt;mark() == markOopDesc::encode(this), &quot;invariant&quot;) ;</span>
 588   // but as we&#39;re at a safepoint that&#39;s not safe.
 589 
 590   UnlinkAfterAcquire(Self, &amp;node);
 591   if (_succ == Self) _succ = NULL;
 592 
 593   assert(_succ != Self, &quot;invariant&quot;);
 594   if (_Responsible == Self) {
 595     _Responsible = NULL;
 596     OrderAccess::fence(); // Dekker pivot-point
 597 
 598     // We may leave threads on cxq|EntryList without a designated
 599     // &quot;Responsible&quot; thread.  This is benign.  When this thread subsequently
 600     // exits the monitor it can &quot;see&quot; such preexisting &quot;old&quot; threads --
 601     // threads that arrived on the cxq|EntryList before the fence, above --
 602     // by LDing cxq|EntryList.  Newly arrived threads -- that is, threads
 603     // that arrive on cxq after the ST:MEMBAR, above -- will set Responsible
 604     // non-null and elect a new &quot;Responsible&quot; timer thread.
 605     //
 606     // This thread executes:
 607     //    ST Responsible=null; MEMBAR    (in enter epilogue - here)
</pre>
<hr />
<pre>
 635   //
 636   // Critically, any prior STs to _succ or EntryList must be visible before
 637   // the ST of null into _owner in the *subsequent* (following) corresponding
 638   // monitorexit.  Recall too, that in 1-0 mode monitorexit does not necessarily
 639   // execute a serializing instruction.
 640 
 641   return;
 642 }
 643 
 644 // ReenterI() is a specialized inline form of the latter half of the
 645 // contended slow-path from EnterI().  We use ReenterI() only for
 646 // monitor reentry in wait().
 647 //
 648 // In the future we should reconcile EnterI() and ReenterI().
 649 
 650 void ObjectMonitor::ReenterI(Thread * Self, ObjectWaiter * SelfNode) {
 651   assert(Self != NULL, &quot;invariant&quot;);
 652   assert(SelfNode != NULL, &quot;invariant&quot;);
 653   assert(SelfNode-&gt;_thread == Self, &quot;invariant&quot;);
 654   assert(_waiters &gt; 0, &quot;invariant&quot;);
<span class="line-modified"> 655   assert(((oop)(object()))-&gt;mark() == markOopDesc::encode(this), &quot;invariant&quot;);</span>
 656   assert(((JavaThread *)Self)-&gt;thread_state() != _thread_blocked, &quot;invariant&quot;);
 657   JavaThread * jt = (JavaThread *) Self;
 658 
 659   int nWakeups = 0;
 660   for (;;) {
 661     ObjectWaiter::TStates v = SelfNode-&gt;TState;
 662     guarantee(v == ObjectWaiter::TS_ENTER || v == ObjectWaiter::TS_CXQ, &quot;invariant&quot;);
 663     assert(_owner != Self, &quot;invariant&quot;);
 664 
 665     if (TryLock(Self) &gt; 0) break;
 666     if (TrySpin(Self) &gt; 0) break;
 667 
 668     // State transition wrappers around park() ...
 669     // ReenterI() wisely defers state transitions until
 670     // it&#39;s clear we must park the thread.
 671     {
 672       OSThreadContendState osts(Self-&gt;osthread());
 673       ThreadBlockInVM tbivm(jt);
 674 
 675       // cleared by handle_special_suspend_equivalent_condition()
</pre>
<hr />
<pre>
 703     if (_succ == Self) _succ = NULL;
 704 
 705     // Invariant: after clearing _succ a contending thread
 706     // *must* retry  _owner before parking.
 707     OrderAccess::fence();
 708 
 709     // This PerfData object can be used in parallel with a safepoint.
 710     // See the work around in PerfDataManager::destroy().
 711     OM_PERFDATA_OP(FutileWakeups, inc());
 712   }
 713 
 714   // Self has acquired the lock -- Unlink Self from the cxq or EntryList .
 715   // Normally we&#39;ll find Self on the EntryList.
 716   // Unlinking from the EntryList is constant-time and atomic-free.
 717   // From the perspective of the lock owner (this thread), the
 718   // EntryList is stable and cxq is prepend-only.
 719   // The head of cxq is volatile but the interior is stable.
 720   // In addition, Self.TState is stable.
 721 
 722   assert(_owner == Self, &quot;invariant&quot;);
<span class="line-modified"> 723   assert(((oop)(object()))-&gt;mark() == markOopDesc::encode(this), &quot;invariant&quot;);</span>
 724   UnlinkAfterAcquire(Self, SelfNode);
 725   if (_succ == Self) _succ = NULL;
 726   assert(_succ != Self, &quot;invariant&quot;);
 727   SelfNode-&gt;TState = ObjectWaiter::TS_RUN;
 728   OrderAccess::fence();      // see comments at the end of EnterI()
 729 }
 730 
 731 // By convention we unlink a contending thread from EntryList|cxq immediately
 732 // after the thread acquires the lock in ::enter().  Equally, we could defer
 733 // unlinking the thread until ::exit()-time.
 734 
 735 void ObjectMonitor::UnlinkAfterAcquire(Thread *Self, ObjectWaiter *SelfNode) {
 736   assert(_owner == Self, &quot;invariant&quot;);
 737   assert(SelfNode-&gt;_thread == Self, &quot;invariant&quot;);
 738 
 739   if (SelfNode-&gt;TState == ObjectWaiter::TS_ENTER) {
 740     // Normal case: remove Self from the DLL EntryList .
 741     // This is a constant-time operation.
 742     ObjectWaiter * nxt = SelfNode-&gt;_next;
 743     ObjectWaiter * prv = SelfNode-&gt;_prev;
</pre>
<hr />
<pre>
 746     if (SelfNode == _EntryList) _EntryList = nxt;
 747     assert(nxt == NULL || nxt-&gt;TState == ObjectWaiter::TS_ENTER, &quot;invariant&quot;);
 748     assert(prv == NULL || prv-&gt;TState == ObjectWaiter::TS_ENTER, &quot;invariant&quot;);
 749   } else {
 750     assert(SelfNode-&gt;TState == ObjectWaiter::TS_CXQ, &quot;invariant&quot;);
 751     // Inopportune interleaving -- Self is still on the cxq.
 752     // This usually means the enqueue of self raced an exiting thread.
 753     // Normally we&#39;ll find Self near the front of the cxq, so
 754     // dequeueing is typically fast.  If needbe we can accelerate
 755     // this with some MCS/CHL-like bidirectional list hints and advisory
 756     // back-links so dequeueing from the interior will normally operate
 757     // in constant-time.
 758     // Dequeue Self from either the head (with CAS) or from the interior
 759     // with a linear-time scan and normal non-atomic memory operations.
 760     // CONSIDER: if Self is on the cxq then simply drain cxq into EntryList
 761     // and then unlink Self from EntryList.  We have to drain eventually,
 762     // so it might as well be now.
 763 
 764     ObjectWaiter * v = _cxq;
 765     assert(v != NULL, &quot;invariant&quot;);
<span class="line-modified"> 766     if (v != SelfNode || Atomic::cmpxchg(SelfNode-&gt;_next, &amp;_cxq, v) != v) {</span>
 767       // The CAS above can fail from interference IFF a &quot;RAT&quot; arrived.
 768       // In that case Self must be in the interior and can no longer be
 769       // at the head of cxq.
 770       if (v == SelfNode) {
 771         assert(_cxq != v, &quot;invariant&quot;);
 772         v = _cxq;          // CAS above failed - start scan at head of list
 773       }
 774       ObjectWaiter * p;
 775       ObjectWaiter * q = NULL;
 776       for (p = v; p != NULL &amp;&amp; p != SelfNode; p = p-&gt;_next) {
 777         q = p;
 778         assert(p-&gt;TState == ObjectWaiter::TS_CXQ, &quot;invariant&quot;);
 779       }
 780       assert(v != SelfNode, &quot;invariant&quot;);
 781       assert(p == SelfNode, &quot;Node not found on cxq&quot;);
 782       assert(p != _cxq, &quot;invariant&quot;);
 783       assert(q != NULL, &quot;invariant&quot;);
 784       assert(q-&gt;_next == p, &quot;invariant&quot;);
 785       q-&gt;_next = p-&gt;_next;
 786     }
</pre>
<hr />
<pre>
 792   SelfNode-&gt;_next  = (ObjectWaiter *) 0xBAD;
 793   SelfNode-&gt;TState = ObjectWaiter::TS_RUN;
 794 #endif
 795 }
 796 
 797 // -----------------------------------------------------------------------------
 798 // Exit support
 799 //
 800 // exit()
 801 // ~~~~~~
 802 // Note that the collector can&#39;t reclaim the objectMonitor or deflate
 803 // the object out from underneath the thread calling ::exit() as the
 804 // thread calling ::exit() never transitions to a stable state.
 805 // This inhibits GC, which in turn inhibits asynchronous (and
 806 // inopportune) reclamation of &quot;this&quot;.
 807 //
 808 // We&#39;d like to assert that: (THREAD-&gt;thread_state() != _thread_blocked) ;
 809 // There&#39;s one exception to the claim above, however.  EnterI() can call
 810 // exit() to drop a lock if the acquirer has been externally suspended.
 811 // In that case exit() is called with _thread_state as _thread_blocked,
<span class="line-modified"> 812 // but the monitor&#39;s _count field is &gt; 0, which inhibits reclamation.</span>
 813 //
 814 // 1-0 exit
 815 // ~~~~~~~~
 816 // ::exit() uses a canonical 1-1 idiom with a MEMBAR although some of
 817 // the fast-path operators have been optimized so the common ::exit()
 818 // operation is 1-0, e.g., see macroAssembler_x86.cpp: fast_unlock().
 819 // The code emitted by fast_unlock() elides the usual MEMBAR.  This
 820 // greatly improves latency -- MEMBAR and CAS having considerable local
 821 // latency on modern processors -- but at the cost of &quot;stranding&quot;.  Absent the
 822 // MEMBAR, a thread in fast_unlock() can race a thread in the slow
 823 // ::enter() path, resulting in the entering thread being stranding
 824 // and a progress-liveness failure.   Stranding is extremely rare.
 825 // We use timers (timed park operations) &amp; periodic polling to detect
 826 // and recover from stranding.  Potentially stranded threads periodically
 827 // wake up and poll the lock.  See the usage of the _Responsible variable.
 828 //
 829 // The CAS() in enter provides for safety and exclusion, while the CAS or
 830 // MEMBAR in exit provides for progress and avoids stranding.  1-0 locking
 831 // eliminates the CAS/MEMBAR from the exit path, but it admits stranding.
 832 // We detect and recover from stranding with timers.
</pre>
<hr />
<pre>
 835 // thread acquires the lock and then drops the lock, at which time the
 836 // exiting thread will notice and unpark the stranded thread, or, (b)
 837 // the timer expires.  If the lock is high traffic then the stranding latency
 838 // will be low due to (a).  If the lock is low traffic then the odds of
 839 // stranding are lower, although the worst-case stranding latency
 840 // is longer.  Critically, we don&#39;t want to put excessive load in the
 841 // platform&#39;s timer subsystem.  We want to minimize both the timer injection
 842 // rate (timers created/sec) as well as the number of timers active at
 843 // any one time.  (more precisely, we want to minimize timer-seconds, which is
 844 // the integral of the # of active timers at any instant over time).
 845 // Both impinge on OS scalability.  Given that, at most one thread parked on
 846 // a monitor will use a timer.
 847 //
 848 // There is also the risk of a futile wake-up. If we drop the lock
 849 // another thread can reacquire the lock immediately, and we can
 850 // then wake a thread unnecessarily. This is benign, and we&#39;ve
 851 // structured the code so the windows are short and the frequency
 852 // of such futile wakups is low.
 853 
 854 void ObjectMonitor::exit(bool not_suspended, TRAPS) {
<span class="line-modified"> 855   Thread * const Self = THREAD;</span>
<span class="line-modified"> 856   if (THREAD != _owner) {</span>
<span class="line-modified"> 857     if (THREAD-&gt;is_lock_owned((address) _owner)) {</span>
<span class="line-modified"> 858       // Transmute _owner from a BasicLock pointer to a Thread address.</span>
<span class="line-removed"> 859       // We don&#39;t need to hold _mutex for this transition.</span>
<span class="line-removed"> 860       // Non-null to Non-null is safe as long as all readers can</span>
<span class="line-removed"> 861       // tolerate either flavor.</span>
 862       assert(_recursions == 0, &quot;invariant&quot;);
<span class="line-modified"> 863       _owner = THREAD;</span>
 864       _recursions = 0;
 865     } else {
 866       // Apparent unbalanced locking ...
 867       // Naively we&#39;d like to throw IllegalMonitorStateException.
 868       // As a practical matter we can neither allocate nor throw an
 869       // exception as ::exit() can be called from leaf routines.
 870       // see x86_32.ad Fast_Unlock() and the I1 and I2 properties.
 871       // Upon deeper reflection, however, in a properly run JVM the only
 872       // way we should encounter this situation is in the presence of
 873       // unbalanced JNI locking. TODO: CheckJNICalls.
 874       // See also: CR4414101
<span class="line-modified"> 875       assert(false, &quot;Non-balanced monitor enter/exit! Likely JNI locking&quot;);</span>







 876       return;
 877     }
 878   }
 879 
 880   if (_recursions != 0) {
 881     _recursions--;        // this is simple recursive enter
 882     return;
 883   }
 884 
 885   // Invariant: after setting Responsible=null an thread must execute
 886   // a MEMBAR or other serializing instruction before fetching EntryList|cxq.
 887   _Responsible = NULL;
 888 
 889 #if INCLUDE_JFR
 890   // get the owner&#39;s thread id for the MonitorEnter event
 891   // if it is enabled and the thread isn&#39;t suspended
 892   if (not_suspended &amp;&amp; EventJavaMonitorEnter::is_enabled()) {
 893     _previous_owner_tid = JFR_THREAD_ID(Self);
 894   }
 895 #endif
 896 
 897   for (;;) {
 898     assert(THREAD == _owner, &quot;invariant&quot;);
 899 

 900     // release semantics: prior loads and stores from within the critical section
 901     // must not float (reorder) past the following store that drops the lock.
<span class="line-modified"> 902     // On SPARC that requires MEMBAR #loadstore|#storestore.</span>
<span class="line-modified"> 903     // But of course in TSO #loadstore|#storestore is not required.</span>
<span class="line-modified"> 904     OrderAccess::release_store(&amp;_owner, (void*)NULL);   // drop the lock</span>
<span class="line-modified"> 905     OrderAccess::storeload();                        // See if we need to wake a successor</span>


 906     if ((intptr_t(_EntryList)|intptr_t(_cxq)) == 0 || _succ != NULL) {
 907       return;
 908     }
 909     // Other threads are blocked trying to acquire the lock.
 910 
 911     // Normally the exiting thread is responsible for ensuring succession,
 912     // but if other successors are ready or other entering threads are spinning
 913     // then this thread can simply store NULL into _owner and exit without
 914     // waking a successor.  The existence of spinners or ready successors
 915     // guarantees proper succession (liveness).  Responsibility passes to the
 916     // ready or running successors.  The exiting thread delegates the duty.
 917     // More precisely, if a successor already exists this thread is absolved
 918     // of the responsibility of waking (unparking) one.
 919     //
 920     // The _succ variable is critical to reducing futile wakeup frequency.
 921     // _succ identifies the &quot;heir presumptive&quot; thread that has been made
 922     // ready (unparked) but that has not yet run.  We need only one such
 923     // successor thread to guarantee progress.
 924     // See http://www.usenix.org/events/jvm01/full_papers/dice/dice.pdf
 925     // section 3.3 &quot;Futile Wakeup Throttling&quot; for details.
</pre>
<hr />
<pre>
 927     // Note that spinners in Enter() also set _succ non-null.
 928     // In the current implementation spinners opportunistically set
 929     // _succ so that exiting threads might avoid waking a successor.
 930     // Another less appealing alternative would be for the exiting thread
 931     // to drop the lock and then spin briefly to see if a spinner managed
 932     // to acquire the lock.  If so, the exiting thread could exit
 933     // immediately without waking a successor, otherwise the exiting
 934     // thread would need to dequeue and wake a successor.
 935     // (Note that we&#39;d need to make the post-drop spin short, but no
 936     // shorter than the worst-case round-trip cache-line migration time.
 937     // The dropped lock needs to become visible to the spinner, and then
 938     // the acquisition of the lock by the spinner must become visible to
 939     // the exiting thread).
 940 
 941     // It appears that an heir-presumptive (successor) must be made ready.
 942     // Only the current lock owner can manipulate the EntryList or
 943     // drain _cxq, so we need to reacquire the lock.  If we fail
 944     // to reacquire the lock the responsibility for ensuring succession
 945     // falls to the new owner.
 946     //
<span class="line-modified"> 947     if (!Atomic::replace_if_null(THREAD, &amp;_owner)) {</span>
 948       return;
 949     }
 950 
 951     guarantee(_owner == THREAD, &quot;invariant&quot;);
 952 
 953     ObjectWaiter * w = NULL;
 954 
 955     w = _EntryList;
 956     if (w != NULL) {
 957       // I&#39;d like to write: guarantee (w-&gt;_thread != Self).
 958       // But in practice an exiting thread may find itself on the EntryList.
 959       // Let&#39;s say thread T1 calls O.wait().  Wait() enqueues T1 on O&#39;s waitset and
 960       // then calls exit().  Exit release the lock by setting O._owner to NULL.
 961       // Let&#39;s say T1 then stalls.  T2 acquires O and calls O.notify().  The
 962       // notify() operation moves T1 from O&#39;s waitset to O&#39;s EntryList. T2 then
 963       // release the lock &quot;O&quot;.  T2 resumes immediately after the ST of null into
 964       // _owner, above.  T2 notices that the EntryList is populated, so it
 965       // reacquires the lock and then finds itself on the EntryList.
 966       // Given all that, we have to tolerate the circumstance where &quot;w&quot; is
 967       // associated with Self.
 968       assert(w-&gt;TState == ObjectWaiter::TS_ENTER, &quot;invariant&quot;);
 969       ExitEpilog(Self, w);
 970       return;
 971     }
 972 
 973     // If we find that both _cxq and EntryList are null then just
 974     // re-run the exit protocol from the top.
 975     w = _cxq;
 976     if (w == NULL) continue;
 977 
 978     // Drain _cxq into EntryList - bulk transfer.
 979     // First, detach _cxq.
 980     // The following loop is tantamount to: w = swap(&amp;cxq, NULL)
 981     for (;;) {
 982       assert(w != NULL, &quot;Invariant&quot;);
<span class="line-modified"> 983       ObjectWaiter * u = Atomic::cmpxchg((ObjectWaiter*)NULL, &amp;_cxq, w);</span>
 984       if (u == w) break;
 985       w = u;
 986     }
 987 
 988     assert(w != NULL, &quot;invariant&quot;);
 989     assert(_EntryList == NULL, &quot;invariant&quot;);
 990 
 991     // Convert the LIFO SLL anchored by _cxq into a DLL.
 992     // The list reorganization step operates in O(LENGTH(w)) time.
 993     // It&#39;s critical that this step operate quickly as
 994     // &quot;Self&quot; still holds the outer-lock, restricting parallelism
 995     // and effectively lengthening the critical section.
 996     // Invariant: s chases t chases u.
 997     // TODO-FIXME: consider changing EntryList from a DLL to a CDLL so
 998     // we have faster access to the tail.
 999 
1000     _EntryList = w;
1001     ObjectWaiter * q = NULL;
1002     ObjectWaiter * p;
1003     for (p = w; p != NULL; p = p-&gt;_next) {
</pre>
<hr />
<pre>
1060 
1061 
1062 void ObjectMonitor::ExitEpilog(Thread * Self, ObjectWaiter * Wakee) {
1063   assert(_owner == Self, &quot;invariant&quot;);
1064 
1065   // Exit protocol:
1066   // 1. ST _succ = wakee
1067   // 2. membar #loadstore|#storestore;
1068   // 2. ST _owner = NULL
1069   // 3. unpark(wakee)
1070 
1071   _succ = Wakee-&gt;_thread;
1072   ParkEvent * Trigger = Wakee-&gt;_event;
1073 
1074   // Hygiene -- once we&#39;ve set _owner = NULL we can&#39;t safely dereference Wakee again.
1075   // The thread associated with Wakee may have grabbed the lock and &quot;Wakee&quot; may be
1076   // out-of-scope (non-extant).
1077   Wakee  = NULL;
1078 
1079   // Drop the lock
<span class="line-modified">1080   OrderAccess::release_store(&amp;_owner, (void*)NULL);</span>
<span class="line-modified">1081   OrderAccess::fence();                               // ST _owner vs LD in unpark()</span>

1082 
1083   DTRACE_MONITOR_PROBE(contended__exit, this, object(), Self);
1084   Trigger-&gt;unpark();
1085 
1086   // Maintain stats and report events to JVMTI
1087   OM_PERFDATA_OP(Parks, inc());
1088 }
1089 
1090 
1091 // -----------------------------------------------------------------------------
1092 // Class Loader deadlock handling.
1093 //
1094 // complete_exit exits a lock returning recursion count
1095 // complete_exit/reenter operate as a wait without waiting
1096 // complete_exit requires an inflated monitor
1097 // The _owner field is not always the Thread addr even with an
1098 // inflated monitor, e.g. the monitor can be inflated by a non-owning
1099 // thread due to contention.
<span class="line-modified">1100 intptr_t ObjectMonitor::complete_exit(TRAPS) {</span>
1101   Thread * const Self = THREAD;
1102   assert(Self-&gt;is_Java_thread(), &quot;Must be Java thread!&quot;);
1103   JavaThread *jt = (JavaThread *)THREAD;
1104 
1105   assert(InitDone, &quot;Unexpectedly not initialized&quot;);
1106 
<span class="line-modified">1107   if (THREAD != _owner) {</span>
<span class="line-modified">1108     if (THREAD-&gt;is_lock_owned ((address)_owner)) {</span>

1109       assert(_recursions == 0, &quot;internal state error&quot;);
<span class="line-modified">1110       _owner = THREAD;   // Convert from basiclock addr to Thread addr</span>
1111       _recursions = 0;
1112     }
1113   }
1114 
1115   guarantee(Self == _owner, &quot;complete_exit not owner&quot;);
<span class="line-modified">1116   intptr_t save = _recursions; // record the old recursion count</span>
1117   _recursions = 0;        // set the recursion level to be 0
1118   exit(true, Self);           // exit the monitor
1119   guarantee(_owner != Self, &quot;invariant&quot;);
1120   return save;
1121 }
1122 
1123 // reenter() enters a lock and sets recursion count
1124 // complete_exit/reenter operate as a wait without waiting
<span class="line-modified">1125 void ObjectMonitor::reenter(intptr_t recursions, TRAPS) {</span>
1126   Thread * const Self = THREAD;
1127   assert(Self-&gt;is_Java_thread(), &quot;Must be Java thread!&quot;);
1128   JavaThread *jt = (JavaThread *)THREAD;
1129 
1130   guarantee(_owner != Self, &quot;reenter already owner&quot;);
1131   enter(THREAD);       // enter the monitor
1132   guarantee(_recursions == 0, &quot;reenter recursion&quot;);
1133   _recursions = recursions;
1134   return;
1135 }
1136 
<span class="line-modified">1137 </span>
<span class="line-modified">1138 // -----------------------------------------------------------------------------</span>
<span class="line-modified">1139 // A macro is used below because there may already be a pending</span>
<span class="line-modified">1140 // exception which should not abort the execution of the routines</span>
<span class="line-modified">1141 // which use this (which is why we don&#39;t put this into check_slow and</span>
<span class="line-modified">1142 // call it with a CHECK argument).</span>
<span class="line-modified">1143 </span>
<span class="line-modified">1144 #define CHECK_OWNER()                                                       \</span>
<span class="line-modified">1145   do {                                                                      \</span>
<span class="line-modified">1146     if (THREAD != _owner) {                                                 \</span>
<span class="line-modified">1147       if (THREAD-&gt;is_lock_owned((address) _owner)) {                        \</span>
<span class="line-removed">1148         _owner = THREAD;  /* Convert from basiclock addr to Thread addr */  \</span>
<span class="line-removed">1149         _recursions = 0;                                                    \</span>
<span class="line-removed">1150       } else {                                                              \</span>
<span class="line-removed">1151         THROW(vmSymbols::java_lang_IllegalMonitorStateException());         \</span>
<span class="line-removed">1152       }                                                                     \</span>
<span class="line-removed">1153     }                                                                       \</span>
1154   } while (false)
1155 
<span class="line-modified">1156 // check_slow() is a misnomer.  It&#39;s called to simply to throw an IMSX exception.</span>
<span class="line-modified">1157 // TODO-FIXME: remove check_slow() -- it&#39;s likely dead.</span>
<span class="line-modified">1158 </span>
<span class="line-modified">1159 void ObjectMonitor::check_slow(TRAPS) {</span>
<span class="line-modified">1160   assert(THREAD != _owner &amp;&amp; !THREAD-&gt;is_lock_owned((address) _owner), &quot;must not be owner&quot;);</span>
<span class="line-modified">1161   THROW_MSG(vmSymbols::java_lang_IllegalMonitorStateException(), &quot;current thread not owner&quot;);</span>










1162 }
1163 
1164 static void post_monitor_wait_event(EventJavaMonitorWait* event,
1165                                     ObjectMonitor* monitor,
1166                                     jlong notifier_tid,
1167                                     jlong timeout,
1168                                     bool timedout) {
1169   assert(event != NULL, &quot;invariant&quot;);
1170   assert(monitor != NULL, &quot;invariant&quot;);
1171   event-&gt;set_monitorClass(((oop)monitor-&gt;object())-&gt;klass());
1172   event-&gt;set_timeout(timeout);
1173   event-&gt;set_address((uintptr_t)monitor-&gt;object_addr());
1174   event-&gt;set_notifier(notifier_tid);
1175   event-&gt;set_timedOut(timedout);
1176   event-&gt;commit();
1177 }
1178 
1179 // -----------------------------------------------------------------------------
1180 // Wait/Notify/NotifyAll
1181 //
1182 // Note: a subset of changes to ObjectMonitor::wait()
1183 // will need to be replicated in complete_exit
1184 void ObjectMonitor::wait(jlong millis, bool interruptible, TRAPS) {
1185   Thread * const Self = THREAD;
1186   assert(Self-&gt;is_Java_thread(), &quot;Must be Java thread!&quot;);
1187   JavaThread *jt = (JavaThread *)THREAD;
1188 
1189   assert(InitDone, &quot;Unexpectedly not initialized&quot;);
1190 
<span class="line-modified">1191   // Throw IMSX or IEX.</span>
<span class="line-removed">1192   CHECK_OWNER();</span>
1193 
1194   EventJavaMonitorWait event;
1195 
1196   // check for a pending interrupt
<span class="line-modified">1197   if (interruptible &amp;&amp; Thread::is_interrupted(Self, true) &amp;&amp; !HAS_PENDING_EXCEPTION) {</span>
1198     // post monitor waited event.  Note that this is past-tense, we are done waiting.
1199     if (JvmtiExport::should_post_monitor_waited()) {
1200       // Note: &#39;false&#39; parameter is passed here because the
1201       // wait was not timed out due to thread interrupt.
1202       JvmtiExport::post_monitor_waited(jt, this, false);
1203 
1204       // In this short circuit of the monitor wait protocol, the
1205       // current thread never drops ownership of the monitor and
1206       // never gets added to the wait queue so the current thread
1207       // cannot be made the successor. This means that the
1208       // JVMTI_EVENT_MONITOR_WAITED event handler cannot accidentally
1209       // consume an unpark() meant for the ParkEvent associated with
1210       // this ObjectMonitor.
1211     }
1212     if (event.should_commit()) {
1213       post_monitor_wait_event(&amp;event, this, 0, millis, false);
1214     }
1215     THROW(vmSymbols::java_lang_InterruptedException());
1216     return;
1217   }
</pre>
<hr />
<pre>
1224   // Critically, after we reset() the event but prior to park(), we must check
1225   // for a pending interrupt.
1226   ObjectWaiter node(Self);
1227   node.TState = ObjectWaiter::TS_WAIT;
1228   Self-&gt;_ParkEvent-&gt;reset();
1229   OrderAccess::fence();          // ST into Event; membar ; LD interrupted-flag
1230 
1231   // Enter the waiting queue, which is a circular doubly linked list in this case
1232   // but it could be a priority queue or any data structure.
1233   // _WaitSetLock protects the wait queue.  Normally the wait queue is accessed only
1234   // by the the owner of the monitor *except* in the case where park()
1235   // returns because of a timeout of interrupt.  Contention is exceptionally rare
1236   // so we use a simple spin-lock instead of a heavier-weight blocking lock.
1237 
1238   Thread::SpinAcquire(&amp;_WaitSetLock, &quot;WaitSet - add&quot;);
1239   AddWaiter(&amp;node);
1240   Thread::SpinRelease(&amp;_WaitSetLock);
1241 
1242   _Responsible = NULL;
1243 
<span class="line-modified">1244   intptr_t save = _recursions; // record the old recursion count</span>
1245   _waiters++;                  // increment the number of waiters
1246   _recursions = 0;             // set the recursion level to be 1
1247   exit(true, Self);                    // exit the monitor
1248   guarantee(_owner != Self, &quot;invariant&quot;);
1249 
1250   // The thread is on the WaitSet list - now park() it.
1251   // On MP systems it&#39;s conceivable that a brief spin before we park
1252   // could be profitable.
1253   //
1254   // TODO-FIXME: change the following logic to a loop of the form
1255   //   while (!timeout &amp;&amp; !interrupted &amp;&amp; _notified == 0) park()
1256 
1257   int ret = OS_OK;
1258   int WasNotified = 0;




1259   { // State transition wrappers
1260     OSThread* osthread = Self-&gt;osthread();
1261     OSThreadWaitState osts(osthread, true);
1262     {
1263       ThreadBlockInVM tbivm(jt);
1264       // Thread is in thread_blocked state and oop access is unsafe.
1265       jt-&gt;set_suspend_equivalent();
1266 
<span class="line-modified">1267       if (interruptible &amp;&amp; (Thread::is_interrupted(THREAD, false) || HAS_PENDING_EXCEPTION)) {</span>
1268         // Intentionally empty
1269       } else if (node._notified == 0) {
1270         if (millis &lt;= 0) {
1271           Self-&gt;_ParkEvent-&gt;park();
1272         } else {
1273           ret = Self-&gt;_ParkEvent-&gt;park(millis);
1274         }
1275       }
1276 
1277       // were we externally suspended while we were waiting?
1278       if (ExitSuspendEquivalent (jt)) {
1279         // TODO-FIXME: add -- if succ == Self then succ = null.
1280         jt-&gt;java_suspend_self();
1281       }
1282 
1283     } // Exit thread safepoint: transition _thread_blocked -&gt; _thread_in_vm
1284 
1285     // Node may be on the WaitSet, the EntryList (or cxq), or in transition
1286     // from the WaitSet to the EntryList.
1287     // See if we need to remove Node from the WaitSet.
</pre>
<hr />
<pre>
1367     }
1368 
1369     // Self has reacquired the lock.
1370     // Lifecycle - the node representing Self must not appear on any queues.
1371     // Node is about to go out-of-scope, but even if it were immortal we wouldn&#39;t
1372     // want residual elements associated with this thread left on any lists.
1373     guarantee(node.TState == ObjectWaiter::TS_RUN, &quot;invariant&quot;);
1374     assert(_owner == Self, &quot;invariant&quot;);
1375     assert(_succ != Self, &quot;invariant&quot;);
1376   } // OSThreadWaitState()
1377 
1378   jt-&gt;set_current_waiting_monitor(NULL);
1379 
1380   guarantee(_recursions == 0, &quot;invariant&quot;);
1381   _recursions = save;     // restore the old recursion count
1382   _waiters--;             // decrement the number of waiters
1383 
1384   // Verify a few postconditions
1385   assert(_owner == Self, &quot;invariant&quot;);
1386   assert(_succ != Self, &quot;invariant&quot;);
<span class="line-modified">1387   assert(((oop)(object()))-&gt;mark() == markOopDesc::encode(this), &quot;invariant&quot;);</span>
1388 
1389   // check if the notification happened
1390   if (!WasNotified) {
1391     // no, it could be timeout or Thread.interrupt() or both
1392     // check for interrupt event, otherwise it is timeout
<span class="line-modified">1393     if (interruptible &amp;&amp; Thread::is_interrupted(Self, true) &amp;&amp; !HAS_PENDING_EXCEPTION) {</span>
1394       THROW(vmSymbols::java_lang_InterruptedException());
1395     }
1396   }
1397 
1398   // NOTE: Spurious wake up will be consider as timeout.
1399   // Monitor notify has precedence over thread interrupt.
1400 }
1401 
1402 
1403 // Consider:
1404 // If the lock is cool (cxq == null &amp;&amp; succ == null) and we&#39;re on an MP system
1405 // then instead of transferring a thread from the WaitSet to the EntryList
1406 // we might just dequeue a thread from the WaitSet and directly unpark() it.
1407 
1408 void ObjectMonitor::INotify(Thread * Self) {
1409   Thread::SpinAcquire(&amp;_WaitSetLock, &quot;WaitSet - notify&quot;);
1410   ObjectWaiter * iterator = DequeueWaiter();
1411   if (iterator != NULL) {
1412     guarantee(iterator-&gt;TState == ObjectWaiter::TS_WAIT, &quot;invariant&quot;);
1413     guarantee(iterator-&gt;_notified == 0, &quot;invariant&quot;);
</pre>
<hr />
<pre>
1421 
1422     iterator-&gt;_notified = 1;
1423     iterator-&gt;_notifier_tid = JFR_THREAD_ID(Self);
1424 
1425     ObjectWaiter * list = _EntryList;
1426     if (list != NULL) {
1427       assert(list-&gt;_prev == NULL, &quot;invariant&quot;);
1428       assert(list-&gt;TState == ObjectWaiter::TS_ENTER, &quot;invariant&quot;);
1429       assert(list != iterator, &quot;invariant&quot;);
1430     }
1431 
1432     // prepend to cxq
1433     if (list == NULL) {
1434       iterator-&gt;_next = iterator-&gt;_prev = NULL;
1435       _EntryList = iterator;
1436     } else {
1437       iterator-&gt;TState = ObjectWaiter::TS_CXQ;
1438       for (;;) {
1439         ObjectWaiter * front = _cxq;
1440         iterator-&gt;_next = front;
<span class="line-modified">1441         if (Atomic::cmpxchg(iterator, &amp;_cxq, front) == front) {</span>
1442           break;
1443         }
1444       }
1445     }
1446 
1447     // _WaitSetLock protects the wait queue, not the EntryList.  We could
1448     // move the add-to-EntryList operation, above, outside the critical section
1449     // protected by _WaitSetLock.  In practice that&#39;s not useful.  With the
1450     // exception of  wait() timeouts and interrupts the monitor owner
1451     // is the only thread that grabs _WaitSetLock.  There&#39;s almost no contention
1452     // on _WaitSetLock so it&#39;s not profitable to reduce the length of the
1453     // critical section.
1454 
1455     iterator-&gt;wait_reenter_begin(this);
1456   }
1457   Thread::SpinRelease(&amp;_WaitSetLock);
1458 }
1459 
1460 // Consider: a not-uncommon synchronization bug is to use notify() when
1461 // notifyAll() is more appropriate, potentially resulting in stranded
1462 // threads; this is one example of a lost wakeup. A useful diagnostic
1463 // option is to force all notify() operations to behave as notifyAll().
1464 //
1465 // Note: We can also detect many such problems with a &quot;minimum wait&quot;.
1466 // When the &quot;minimum wait&quot; is set to a small non-zero timeout value
1467 // and the program does not hang whereas it did absent &quot;minimum wait&quot;,
1468 // that suggests a lost wakeup bug.
1469 
1470 void ObjectMonitor::notify(TRAPS) {
<span class="line-modified">1471   CHECK_OWNER();</span>
1472   if (_WaitSet == NULL) {
1473     return;
1474   }
1475   DTRACE_MONITOR_PROBE(notify, this, object(), THREAD);
1476   INotify(THREAD);
1477   OM_PERFDATA_OP(Notifications, inc(1));
1478 }
1479 
1480 
1481 // The current implementation of notifyAll() transfers the waiters one-at-a-time
1482 // from the waitset to the EntryList. This could be done more efficiently with a
1483 // single bulk transfer but in practice it&#39;s not time-critical. Beware too,
1484 // that in prepend-mode we invert the order of the waiters. Let&#39;s say that the
1485 // waitset is &quot;ABCD&quot; and the EntryList is &quot;XYZ&quot;. After a notifyAll() in prepend
1486 // mode the waitset will be empty and the EntryList will be &quot;DCBAXYZ&quot;.
1487 
1488 void ObjectMonitor::notifyAll(TRAPS) {
<span class="line-modified">1489   CHECK_OWNER();</span>
1490   if (_WaitSet == NULL) {
1491     return;
1492   }
1493 
1494   DTRACE_MONITOR_PROBE(notifyAll, this, object(), THREAD);
1495   int tally = 0;
1496   while (_WaitSet != NULL) {
1497     tally++;
1498     INotify(THREAD);
1499   }
1500 
1501   OM_PERFDATA_OP(Notifications, inc(tally));
1502 }
1503 
1504 // -----------------------------------------------------------------------------
1505 // Adaptive Spinning Support
1506 //
1507 // Adaptive spin-then-block - rational spinning
1508 //
1509 // Note that we spin &quot;globally&quot; on _owner with a classic SMP-polite TATAS
</pre>
<hr />
<pre>
1642     // We periodically check to see if there&#39;s a safepoint pending.
1643     if ((ctr &amp; 0xFF) == 0) {
1644       if (SafepointMechanism::should_block(Self)) {
1645         goto Abort;           // abrupt spin egress
1646       }
1647       SpinPause();
1648     }
1649 
1650     // Probe _owner with TATAS
1651     // If this thread observes the monitor transition or flicker
1652     // from locked to unlocked to locked, then the odds that this
1653     // thread will acquire the lock in this spin attempt go down
1654     // considerably.  The same argument applies if the CAS fails
1655     // or if we observe _owner change from one non-null value to
1656     // another non-null value.   In such cases we might abort
1657     // the spin without prejudice or apply a &quot;penalty&quot; to the
1658     // spin count-down variable &quot;ctr&quot;, reducing it by 100, say.
1659 
1660     Thread * ox = (Thread *) _owner;
1661     if (ox == NULL) {
<span class="line-modified">1662       ox = (Thread*)Atomic::cmpxchg(Self, &amp;_owner, (void*)NULL);</span>
1663       if (ox == NULL) {
1664         // The CAS succeeded -- this thread acquired ownership
1665         // Take care of some bookkeeping to exit spin state.
1666         if (_succ == Self) {
1667           _succ = NULL;
1668         }
1669 
1670         // Increase _SpinDuration :
1671         // The spin was successful (profitable) so we tend toward
1672         // longer spin attempts in the future.
1673         // CONSIDER: factor &quot;ctr&quot; into the _SpinDuration adjustment.
1674         // If we acquired the lock early in the spin cycle it
1675         // makes sense to increase _SpinDuration proportionally.
1676         // Note that we don&#39;t clamp SpinDuration precisely at SpinLimit.
1677         int x = _SpinDuration;
1678         if (x &lt; Knob_SpinLimit) {
1679           if (x &lt; Knob_Poverty) x = Knob_Poverty;
1680           _SpinDuration = x + Knob_Bonus;
1681         }
1682         return 1;
</pre>
<hr />
<pre>
1905                                         CHECK);                          \
1906   }
1907 #define NEWPERFVARIABLE(n)                                                \
1908   {                                                                       \
1909     n = PerfDataManager::create_variable(SUN_RT, #n, PerfData::U_Events,  \
1910                                          CHECK);                          \
1911   }
1912     NEWPERFCOUNTER(_sync_Inflations);
1913     NEWPERFCOUNTER(_sync_Deflations);
1914     NEWPERFCOUNTER(_sync_ContendedLockAttempts);
1915     NEWPERFCOUNTER(_sync_FutileWakeups);
1916     NEWPERFCOUNTER(_sync_Parks);
1917     NEWPERFCOUNTER(_sync_Notifications);
1918     NEWPERFVARIABLE(_sync_MonExtant);
1919 #undef NEWPERFCOUNTER
1920 #undef NEWPERFVARIABLE
1921   }
1922 
1923   DEBUG_ONLY(InitDone = true;)
1924 }































































</pre>
</td>
<td>
<hr />
<pre>
   1 /*
<span class="line-modified">   2  * Copyright (c) 1998, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;classfile/vmSymbols.hpp&quot;
  27 #include &quot;jfr/jfrEvents.hpp&quot;
  28 #include &quot;jfr/support/jfrThreadId.hpp&quot;
<span class="line-added">  29 #include &quot;logging/log.hpp&quot;</span>
<span class="line-added">  30 #include &quot;logging/logStream.hpp&quot;</span>
  31 #include &quot;memory/allocation.inline.hpp&quot;
  32 #include &quot;memory/resourceArea.hpp&quot;
<span class="line-modified">  33 #include &quot;oops/markWord.hpp&quot;</span>
  34 #include &quot;oops/oop.inline.hpp&quot;
  35 #include &quot;runtime/atomic.hpp&quot;
  36 #include &quot;runtime/handles.inline.hpp&quot;
  37 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  38 #include &quot;runtime/mutexLocker.hpp&quot;
  39 #include &quot;runtime/objectMonitor.hpp&quot;
  40 #include &quot;runtime/objectMonitor.inline.hpp&quot;
  41 #include &quot;runtime/orderAccess.hpp&quot;
  42 #include &quot;runtime/osThread.hpp&quot;
  43 #include &quot;runtime/safepointMechanism.inline.hpp&quot;
  44 #include &quot;runtime/sharedRuntime.hpp&quot;
  45 #include &quot;runtime/stubRoutines.hpp&quot;
  46 #include &quot;runtime/thread.inline.hpp&quot;
  47 #include &quot;services/threadService.hpp&quot;
  48 #include &quot;utilities/dtrace.hpp&quot;
  49 #include &quot;utilities/macros.hpp&quot;
  50 #include &quot;utilities/preserveException.hpp&quot;
  51 #if INCLUDE_JFR
  52 #include &quot;jfr/support/jfrFlush.hpp&quot;
  53 #endif
</pre>
<hr />
<pre>
 228   return AllocateHeap(size, mtInternal);
 229 }
 230 void* ObjectMonitor::operator new[] (size_t size) throw() {
 231   return operator new (size);
 232 }
 233 void ObjectMonitor::operator delete(void* p) {
 234   FreeHeap(p);
 235 }
 236 void ObjectMonitor::operator delete[] (void *p) {
 237   operator delete(p);
 238 }
 239 
 240 // -----------------------------------------------------------------------------
 241 // Enter support
 242 
 243 void ObjectMonitor::enter(TRAPS) {
 244   // The following code is ordered to check the most common cases first
 245   // and to reduce RTS-&gt;RTO cache line upgrades on SPARC and IA32 processors.
 246   Thread * const Self = THREAD;
 247 
<span class="line-modified"> 248   void* cur = try_set_owner_from(NULL, Self);</span>
 249   if (cur == NULL) {

 250     assert(_recursions == 0, &quot;invariant&quot;);

 251     return;
 252   }
 253 
 254   if (cur == Self) {
 255     // TODO-FIXME: check for integer overflow!  BUGID 6557169.
 256     _recursions++;
 257     return;
 258   }
 259 
<span class="line-modified"> 260   if (Self-&gt;is_lock_owned((address)cur)) {</span>
 261     assert(_recursions == 0, &quot;internal state error&quot;);
 262     _recursions = 1;
<span class="line-modified"> 263     set_owner_from_BasicLock(cur, Self);  // Convert from BasicLock* to Thread*.</span>


 264     return;
 265   }
 266 
 267   // We&#39;ve encountered genuine contention.
 268   assert(Self-&gt;_Stalled == 0, &quot;invariant&quot;);
 269   Self-&gt;_Stalled = intptr_t(this);
 270 
 271   // Try one round of spinning *before* enqueueing Self
 272   // and before going through the awkward and expensive state
 273   // transitions.  The following spin is strictly optional ...
 274   // Note that if we acquire the monitor from an initial spin
 275   // we forgo posting JVMTI events and firing DTRACE probes.
 276   if (TrySpin(Self) &gt; 0) {
<span class="line-modified"> 277     assert(_owner == Self, &quot;must be Self: owner=&quot; INTPTR_FORMAT, p2i(_owner));</span>
<span class="line-modified"> 278     assert(_recursions == 0, &quot;must be 0: recursions=&quot; INTX_FORMAT, _recursions);</span>
<span class="line-modified"> 279     assert(((oop)object())-&gt;mark() == markWord::encode(this),</span>
<span class="line-added"> 280            &quot;object mark must match encoded this: mark=&quot; INTPTR_FORMAT</span>
<span class="line-added"> 281            &quot;, encoded this=&quot; INTPTR_FORMAT, ((oop)object())-&gt;mark().value(),</span>
<span class="line-added"> 282            markWord::encode(this).value());</span>
 283     Self-&gt;_Stalled = 0;
 284     return;
 285   }
 286 
 287   assert(_owner != Self, &quot;invariant&quot;);
 288   assert(_succ != Self, &quot;invariant&quot;);
 289   assert(Self-&gt;is_Java_thread(), &quot;invariant&quot;);
 290   JavaThread * jt = (JavaThread *) Self;
 291   assert(!SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
 292   assert(jt-&gt;thread_state() != _thread_blocked, &quot;invariant&quot;);
 293   assert(this-&gt;object() != NULL, &quot;invariant&quot;);
<span class="line-modified"> 294   assert(_contentions &gt;= 0, &quot;invariant&quot;);</span>
 295 
 296   // Prevent deflation at STW-time.  See deflate_idle_monitors() and is_busy().
 297   // Ensure the object-monitor relationship remains stable while there&#39;s contention.
<span class="line-modified"> 298   Atomic::inc(&amp;_contentions);</span>
 299 
 300   JFR_ONLY(JfrConditionalFlushWithStacktrace&lt;EventJavaMonitorEnter&gt; flush(jt);)
 301   EventJavaMonitorEnter event;
 302   if (event.should_commit()) {
 303     event.set_monitorClass(((oop)this-&gt;object())-&gt;klass());
 304     event.set_address((uintptr_t)(this-&gt;object_addr()));
 305   }
 306 
 307   { // Change java thread status to indicate blocked on monitor enter.
 308     JavaThreadBlockedOnMonitorEnterState jtbmes(jt, this);
 309 
 310     Self-&gt;set_current_pending_monitor(this);
 311 
 312     DTRACE_MONITOR_PROBE(contended__enter, this, object(), jt);
 313     if (JvmtiExport::should_post_monitor_contended_enter()) {
 314       JvmtiExport::post_monitor_contended_enter(jt, this);
 315 
 316       // The current thread does not yet own the monitor and does not
 317       // yet appear on any queues that would get it made the successor.
 318       // This means that the JVMTI_EVENT_MONITOR_CONTENDED_ENTER event
</pre>
<hr />
<pre>
 339       // thread that suspended us.
 340       //
 341       _recursions = 0;
 342       _succ = NULL;
 343       exit(false, Self);
 344 
 345       jt-&gt;java_suspend_self();
 346     }
 347     Self-&gt;set_current_pending_monitor(NULL);
 348 
 349     // We cleared the pending monitor info since we&#39;ve just gotten past
 350     // the enter-check-for-suspend dance and we now own the monitor free
 351     // and clear, i.e., it is no longer pending. The ThreadBlockInVM
 352     // destructor can go to a safepoint at the end of this block. If we
 353     // do a thread dump during that safepoint, then this thread will show
 354     // as having &quot;-locked&quot; the monitor, but the OS and java.lang.Thread
 355     // states will still report that the thread is blocked trying to
 356     // acquire it.
 357   }
 358 
<span class="line-modified"> 359   Atomic::dec(&amp;_contentions);</span>
<span class="line-modified"> 360   assert(_contentions &gt;= 0, &quot;invariant&quot;);</span>
 361   Self-&gt;_Stalled = 0;
 362 
 363   // Must either set _recursions = 0 or ASSERT _recursions == 0.
 364   assert(_recursions == 0, &quot;invariant&quot;);
 365   assert(_owner == Self, &quot;invariant&quot;);
 366   assert(_succ != Self, &quot;invariant&quot;);
<span class="line-modified"> 367   assert(((oop)(object()))-&gt;mark() == markWord::encode(this), &quot;invariant&quot;);</span>
 368 
 369   // The thread -- now the owner -- is back in vm mode.
 370   // Report the glorious news via TI,DTrace and jvmstat.
 371   // The probe effect is non-trivial.  All the reportage occurs
 372   // while we hold the monitor, increasing the length of the critical
 373   // section.  Amdahl&#39;s parallel speedup law comes vividly into play.
 374   //
 375   // Another option might be to aggregate the events (thread local or
 376   // per-monitor aggregation) and defer reporting until a more opportune
 377   // time -- such as next time some thread encounters contention but has
 378   // yet to acquire the lock.  While spinning that thread could
 379   // spinning we could increment JVMStat counters, etc.
 380 
 381   DTRACE_MONITOR_PROBE(contended__entered, this, object(), jt);
 382   if (JvmtiExport::should_post_monitor_contended_entered()) {
 383     JvmtiExport::post_monitor_contended_entered(jt, this);
 384 
 385     // The current thread already owns the monitor and is not going to
 386     // call park() for the remainder of the monitor enter protocol. So
 387     // it doesn&#39;t matter if the JVMTI_EVENT_MONITOR_CONTENDED_ENTERED
 388     // event handler consumed an unpark() issued by the thread that
 389     // just exited the monitor.
 390   }
 391   if (event.should_commit()) {
 392     event.set_previousOwner((uintptr_t)_previous_owner_tid);
 393     event.commit();
 394   }
 395   OM_PERFDATA_OP(ContendedLockAttempts, inc());
 396 }
 397 
 398 // Caveat: TryLock() is not necessarily serializing if it returns failure.
 399 // Callers must compensate as needed.
 400 
 401 int ObjectMonitor::TryLock(Thread * Self) {
 402   void * own = _owner;
 403   if (own != NULL) return 0;
<span class="line-modified"> 404   if (try_set_owner_from(NULL, Self) == NULL) {</span>

 405     assert(_recursions == 0, &quot;invariant&quot;);

 406     return 1;
 407   }
 408   // The lock had been free momentarily, but we lost the race to the lock.
 409   // Interference -- the CAS failed.
 410   // We can either return -1 or retry.
 411   // Retry doesn&#39;t make as much sense because the lock was just acquired.
 412   return -1;
 413 }
 414 
<span class="line-added"> 415 // Convert the fields used by is_busy() to a string that can be</span>
<span class="line-added"> 416 // used for diagnostic output.</span>
<span class="line-added"> 417 const char* ObjectMonitor::is_busy_to_string(stringStream* ss) {</span>
<span class="line-added"> 418   ss-&gt;print(&quot;is_busy: contentions=%d, waiters=%d, owner=&quot; INTPTR_FORMAT</span>
<span class="line-added"> 419             &quot;, cxq=&quot; INTPTR_FORMAT &quot;, EntryList=&quot; INTPTR_FORMAT, _contentions,</span>
<span class="line-added"> 420             _waiters, p2i(_owner), p2i(_cxq), p2i(_EntryList));</span>
<span class="line-added"> 421   return ss-&gt;base();</span>
<span class="line-added"> 422 }</span>
<span class="line-added"> 423 </span>
 424 #define MAX_RECHECK_INTERVAL 1000
 425 
 426 void ObjectMonitor::EnterI(TRAPS) {
 427   Thread * const Self = THREAD;
 428   assert(Self-&gt;is_Java_thread(), &quot;invariant&quot;);
 429   assert(((JavaThread *) Self)-&gt;thread_state() == _thread_blocked, &quot;invariant&quot;);
 430 
 431   // Try the lock - TATAS
 432   if (TryLock (Self) &gt; 0) {
 433     assert(_succ != Self, &quot;invariant&quot;);
 434     assert(_owner == Self, &quot;invariant&quot;);
 435     assert(_Responsible != Self, &quot;invariant&quot;);
 436     return;
 437   }
 438 
 439   assert(InitDone, &quot;Unexpectedly not initialized&quot;);
 440 
 441   // We try one round of spinning *before* enqueueing Self.
 442   //
 443   // If the _owner is ready but OFFPROC we could use a YieldTo()
</pre>
<hr />
<pre>
 461   //
 462   // Node acts as a proxy for Self.
 463   // As an aside, if were to ever rewrite the synchronization code mostly
 464   // in Java, WaitNodes, ObjectMonitors, and Events would become 1st-class
 465   // Java objects.  This would avoid awkward lifecycle and liveness issues,
 466   // as well as eliminate a subset of ABA issues.
 467   // TODO: eliminate ObjectWaiter and enqueue either Threads or Events.
 468 
 469   ObjectWaiter node(Self);
 470   Self-&gt;_ParkEvent-&gt;reset();
 471   node._prev   = (ObjectWaiter *) 0xBAD;
 472   node.TState  = ObjectWaiter::TS_CXQ;
 473 
 474   // Push &quot;Self&quot; onto the front of the _cxq.
 475   // Once on cxq/EntryList, Self stays on-queue until it acquires the lock.
 476   // Note that spinning tends to reduce the rate at which threads
 477   // enqueue and dequeue on EntryList|cxq.
 478   ObjectWaiter * nxt;
 479   for (;;) {
 480     node._next = nxt = _cxq;
<span class="line-modified"> 481     if (Atomic::cmpxchg(&amp;_cxq, nxt, &amp;node) == nxt) break;</span>
 482 
 483     // Interference - the CAS failed because _cxq changed.  Just retry.
 484     // As an optional optimization we retry the lock.
 485     if (TryLock (Self) &gt; 0) {
 486       assert(_succ != Self, &quot;invariant&quot;);
 487       assert(_owner == Self, &quot;invariant&quot;);
 488       assert(_Responsible != Self, &quot;invariant&quot;);
 489       return;
 490     }
 491   }
 492 
 493   // Check for cxq|EntryList edge transition to non-null.  This indicates
 494   // the onset of contention.  While contention persists exiting threads
 495   // will use a ST:MEMBAR:LD 1-1 exit protocol.  When contention abates exit
 496   // operations revert to the faster 1-0 mode.  This enter operation may interleave
 497   // (race) a concurrent 1-0 exit operation, resulting in stranding, so we
 498   // arrange for one of the contending thread to use a timed park() operations
 499   // to detect and recover from the race.  (Stranding is form of progress failure
 500   // where the monitor is unlocked but all the contending threads remain parked).
 501   // That is, at least one of the contended threads will periodically poll _owner.
 502   // One of the contending threads will become the designated &quot;Responsible&quot; thread.
 503   // The Responsible thread uses a timed park instead of a normal indefinite park
 504   // operation -- it periodically wakes and checks for and recovers from potential
 505   // strandings admitted by 1-0 exit operations.   We need at most one Responsible
 506   // thread per-monitor at any given moment.  Only threads on cxq|EntryList may
 507   // be responsible for a monitor.
 508   //
 509   // Currently, one of the contended threads takes on the added role of &quot;Responsible&quot;.
 510   // A viable alternative would be to use a dedicated &quot;stranding checker&quot; thread
 511   // that periodically iterated over all the threads (or active monitors) and unparked
 512   // successors where there was risk of stranding.  This would help eliminate the
 513   // timer scalability issues we see on some platforms as we&#39;d only have one thread
 514   // -- the checker -- parked on a timer.
 515 
 516   if (nxt == NULL &amp;&amp; _EntryList == NULL) {
 517     // Try to assume the role of responsible thread for the monitor.
 518     // CONSIDER:  ST vs CAS vs { if (Responsible==null) Responsible=Self }
<span class="line-modified"> 519     Atomic::replace_if_null(&amp;_Responsible, Self);</span>
 520   }
 521 
 522   // The lock might have been released while this thread was occupied queueing
 523   // itself onto _cxq.  To close the race and avoid &quot;stranding&quot; and
 524   // progress-liveness failure we must resample-retry _owner before parking.
 525   // Note the Dekker/Lamport duality: ST cxq; MEMBAR; LD Owner.
 526   // In this case the ST-MEMBAR is accomplished with CAS().
 527   //
 528   // TODO: Defer all thread state transitions until park-time.
 529   // Since state transitions are heavy and inefficient we&#39;d like
 530   // to defer the state transitions until absolutely necessary,
 531   // and in doing so avoid some transitions ...
 532 
 533   int nWakeups = 0;
 534   int recheckInterval = 1;
 535 
 536   for (;;) {
 537 
 538     if (TryLock(Self) &gt; 0) break;
 539     assert(_owner != Self, &quot;invariant&quot;);
</pre>
<hr />
<pre>
 575     // just spin again.  This pattern can repeat, leaving _succ to simply
 576     // spin on a CPU.
 577 
 578     if (_succ == Self) _succ = NULL;
 579 
 580     // Invariant: after clearing _succ a thread *must* retry _owner before parking.
 581     OrderAccess::fence();
 582   }
 583 
 584   // Egress :
 585   // Self has acquired the lock -- Unlink Self from the cxq or EntryList.
 586   // Normally we&#39;ll find Self on the EntryList .
 587   // From the perspective of the lock owner (this thread), the
 588   // EntryList is stable and cxq is prepend-only.
 589   // The head of cxq is volatile but the interior is stable.
 590   // In addition, Self.TState is stable.
 591 
 592   assert(_owner == Self, &quot;invariant&quot;);
 593   assert(object() != NULL, &quot;invariant&quot;);
 594   // I&#39;d like to write:
<span class="line-modified"> 595   //   guarantee (((oop)(object()))-&gt;mark() == markWord::encode(this), &quot;invariant&quot;) ;</span>
 596   // but as we&#39;re at a safepoint that&#39;s not safe.
 597 
 598   UnlinkAfterAcquire(Self, &amp;node);
 599   if (_succ == Self) _succ = NULL;
 600 
 601   assert(_succ != Self, &quot;invariant&quot;);
 602   if (_Responsible == Self) {
 603     _Responsible = NULL;
 604     OrderAccess::fence(); // Dekker pivot-point
 605 
 606     // We may leave threads on cxq|EntryList without a designated
 607     // &quot;Responsible&quot; thread.  This is benign.  When this thread subsequently
 608     // exits the monitor it can &quot;see&quot; such preexisting &quot;old&quot; threads --
 609     // threads that arrived on the cxq|EntryList before the fence, above --
 610     // by LDing cxq|EntryList.  Newly arrived threads -- that is, threads
 611     // that arrive on cxq after the ST:MEMBAR, above -- will set Responsible
 612     // non-null and elect a new &quot;Responsible&quot; timer thread.
 613     //
 614     // This thread executes:
 615     //    ST Responsible=null; MEMBAR    (in enter epilogue - here)
</pre>
<hr />
<pre>
 643   //
 644   // Critically, any prior STs to _succ or EntryList must be visible before
 645   // the ST of null into _owner in the *subsequent* (following) corresponding
 646   // monitorexit.  Recall too, that in 1-0 mode monitorexit does not necessarily
 647   // execute a serializing instruction.
 648 
 649   return;
 650 }
 651 
 652 // ReenterI() is a specialized inline form of the latter half of the
 653 // contended slow-path from EnterI().  We use ReenterI() only for
 654 // monitor reentry in wait().
 655 //
 656 // In the future we should reconcile EnterI() and ReenterI().
 657 
 658 void ObjectMonitor::ReenterI(Thread * Self, ObjectWaiter * SelfNode) {
 659   assert(Self != NULL, &quot;invariant&quot;);
 660   assert(SelfNode != NULL, &quot;invariant&quot;);
 661   assert(SelfNode-&gt;_thread == Self, &quot;invariant&quot;);
 662   assert(_waiters &gt; 0, &quot;invariant&quot;);
<span class="line-modified"> 663   assert(((oop)(object()))-&gt;mark() == markWord::encode(this), &quot;invariant&quot;);</span>
 664   assert(((JavaThread *)Self)-&gt;thread_state() != _thread_blocked, &quot;invariant&quot;);
 665   JavaThread * jt = (JavaThread *) Self;
 666 
 667   int nWakeups = 0;
 668   for (;;) {
 669     ObjectWaiter::TStates v = SelfNode-&gt;TState;
 670     guarantee(v == ObjectWaiter::TS_ENTER || v == ObjectWaiter::TS_CXQ, &quot;invariant&quot;);
 671     assert(_owner != Self, &quot;invariant&quot;);
 672 
 673     if (TryLock(Self) &gt; 0) break;
 674     if (TrySpin(Self) &gt; 0) break;
 675 
 676     // State transition wrappers around park() ...
 677     // ReenterI() wisely defers state transitions until
 678     // it&#39;s clear we must park the thread.
 679     {
 680       OSThreadContendState osts(Self-&gt;osthread());
 681       ThreadBlockInVM tbivm(jt);
 682 
 683       // cleared by handle_special_suspend_equivalent_condition()
</pre>
<hr />
<pre>
 711     if (_succ == Self) _succ = NULL;
 712 
 713     // Invariant: after clearing _succ a contending thread
 714     // *must* retry  _owner before parking.
 715     OrderAccess::fence();
 716 
 717     // This PerfData object can be used in parallel with a safepoint.
 718     // See the work around in PerfDataManager::destroy().
 719     OM_PERFDATA_OP(FutileWakeups, inc());
 720   }
 721 
 722   // Self has acquired the lock -- Unlink Self from the cxq or EntryList .
 723   // Normally we&#39;ll find Self on the EntryList.
 724   // Unlinking from the EntryList is constant-time and atomic-free.
 725   // From the perspective of the lock owner (this thread), the
 726   // EntryList is stable and cxq is prepend-only.
 727   // The head of cxq is volatile but the interior is stable.
 728   // In addition, Self.TState is stable.
 729 
 730   assert(_owner == Self, &quot;invariant&quot;);
<span class="line-modified"> 731   assert(((oop)(object()))-&gt;mark() == markWord::encode(this), &quot;invariant&quot;);</span>
 732   UnlinkAfterAcquire(Self, SelfNode);
 733   if (_succ == Self) _succ = NULL;
 734   assert(_succ != Self, &quot;invariant&quot;);
 735   SelfNode-&gt;TState = ObjectWaiter::TS_RUN;
 736   OrderAccess::fence();      // see comments at the end of EnterI()
 737 }
 738 
 739 // By convention we unlink a contending thread from EntryList|cxq immediately
 740 // after the thread acquires the lock in ::enter().  Equally, we could defer
 741 // unlinking the thread until ::exit()-time.
 742 
 743 void ObjectMonitor::UnlinkAfterAcquire(Thread *Self, ObjectWaiter *SelfNode) {
 744   assert(_owner == Self, &quot;invariant&quot;);
 745   assert(SelfNode-&gt;_thread == Self, &quot;invariant&quot;);
 746 
 747   if (SelfNode-&gt;TState == ObjectWaiter::TS_ENTER) {
 748     // Normal case: remove Self from the DLL EntryList .
 749     // This is a constant-time operation.
 750     ObjectWaiter * nxt = SelfNode-&gt;_next;
 751     ObjectWaiter * prv = SelfNode-&gt;_prev;
</pre>
<hr />
<pre>
 754     if (SelfNode == _EntryList) _EntryList = nxt;
 755     assert(nxt == NULL || nxt-&gt;TState == ObjectWaiter::TS_ENTER, &quot;invariant&quot;);
 756     assert(prv == NULL || prv-&gt;TState == ObjectWaiter::TS_ENTER, &quot;invariant&quot;);
 757   } else {
 758     assert(SelfNode-&gt;TState == ObjectWaiter::TS_CXQ, &quot;invariant&quot;);
 759     // Inopportune interleaving -- Self is still on the cxq.
 760     // This usually means the enqueue of self raced an exiting thread.
 761     // Normally we&#39;ll find Self near the front of the cxq, so
 762     // dequeueing is typically fast.  If needbe we can accelerate
 763     // this with some MCS/CHL-like bidirectional list hints and advisory
 764     // back-links so dequeueing from the interior will normally operate
 765     // in constant-time.
 766     // Dequeue Self from either the head (with CAS) or from the interior
 767     // with a linear-time scan and normal non-atomic memory operations.
 768     // CONSIDER: if Self is on the cxq then simply drain cxq into EntryList
 769     // and then unlink Self from EntryList.  We have to drain eventually,
 770     // so it might as well be now.
 771 
 772     ObjectWaiter * v = _cxq;
 773     assert(v != NULL, &quot;invariant&quot;);
<span class="line-modified"> 774     if (v != SelfNode || Atomic::cmpxchg(&amp;_cxq, v, SelfNode-&gt;_next) != v) {</span>
 775       // The CAS above can fail from interference IFF a &quot;RAT&quot; arrived.
 776       // In that case Self must be in the interior and can no longer be
 777       // at the head of cxq.
 778       if (v == SelfNode) {
 779         assert(_cxq != v, &quot;invariant&quot;);
 780         v = _cxq;          // CAS above failed - start scan at head of list
 781       }
 782       ObjectWaiter * p;
 783       ObjectWaiter * q = NULL;
 784       for (p = v; p != NULL &amp;&amp; p != SelfNode; p = p-&gt;_next) {
 785         q = p;
 786         assert(p-&gt;TState == ObjectWaiter::TS_CXQ, &quot;invariant&quot;);
 787       }
 788       assert(v != SelfNode, &quot;invariant&quot;);
 789       assert(p == SelfNode, &quot;Node not found on cxq&quot;);
 790       assert(p != _cxq, &quot;invariant&quot;);
 791       assert(q != NULL, &quot;invariant&quot;);
 792       assert(q-&gt;_next == p, &quot;invariant&quot;);
 793       q-&gt;_next = p-&gt;_next;
 794     }
</pre>
<hr />
<pre>
 800   SelfNode-&gt;_next  = (ObjectWaiter *) 0xBAD;
 801   SelfNode-&gt;TState = ObjectWaiter::TS_RUN;
 802 #endif
 803 }
 804 
 805 // -----------------------------------------------------------------------------
 806 // Exit support
 807 //
 808 // exit()
 809 // ~~~~~~
 810 // Note that the collector can&#39;t reclaim the objectMonitor or deflate
 811 // the object out from underneath the thread calling ::exit() as the
 812 // thread calling ::exit() never transitions to a stable state.
 813 // This inhibits GC, which in turn inhibits asynchronous (and
 814 // inopportune) reclamation of &quot;this&quot;.
 815 //
 816 // We&#39;d like to assert that: (THREAD-&gt;thread_state() != _thread_blocked) ;
 817 // There&#39;s one exception to the claim above, however.  EnterI() can call
 818 // exit() to drop a lock if the acquirer has been externally suspended.
 819 // In that case exit() is called with _thread_state as _thread_blocked,
<span class="line-modified"> 820 // but the monitor&#39;s _contentions field is &gt; 0, which inhibits reclamation.</span>
 821 //
 822 // 1-0 exit
 823 // ~~~~~~~~
 824 // ::exit() uses a canonical 1-1 idiom with a MEMBAR although some of
 825 // the fast-path operators have been optimized so the common ::exit()
 826 // operation is 1-0, e.g., see macroAssembler_x86.cpp: fast_unlock().
 827 // The code emitted by fast_unlock() elides the usual MEMBAR.  This
 828 // greatly improves latency -- MEMBAR and CAS having considerable local
 829 // latency on modern processors -- but at the cost of &quot;stranding&quot;.  Absent the
 830 // MEMBAR, a thread in fast_unlock() can race a thread in the slow
 831 // ::enter() path, resulting in the entering thread being stranding
 832 // and a progress-liveness failure.   Stranding is extremely rare.
 833 // We use timers (timed park operations) &amp; periodic polling to detect
 834 // and recover from stranding.  Potentially stranded threads periodically
 835 // wake up and poll the lock.  See the usage of the _Responsible variable.
 836 //
 837 // The CAS() in enter provides for safety and exclusion, while the CAS or
 838 // MEMBAR in exit provides for progress and avoids stranding.  1-0 locking
 839 // eliminates the CAS/MEMBAR from the exit path, but it admits stranding.
 840 // We detect and recover from stranding with timers.
</pre>
<hr />
<pre>
 843 // thread acquires the lock and then drops the lock, at which time the
 844 // exiting thread will notice and unpark the stranded thread, or, (b)
 845 // the timer expires.  If the lock is high traffic then the stranding latency
 846 // will be low due to (a).  If the lock is low traffic then the odds of
 847 // stranding are lower, although the worst-case stranding latency
 848 // is longer.  Critically, we don&#39;t want to put excessive load in the
 849 // platform&#39;s timer subsystem.  We want to minimize both the timer injection
 850 // rate (timers created/sec) as well as the number of timers active at
 851 // any one time.  (more precisely, we want to minimize timer-seconds, which is
 852 // the integral of the # of active timers at any instant over time).
 853 // Both impinge on OS scalability.  Given that, at most one thread parked on
 854 // a monitor will use a timer.
 855 //
 856 // There is also the risk of a futile wake-up. If we drop the lock
 857 // another thread can reacquire the lock immediately, and we can
 858 // then wake a thread unnecessarily. This is benign, and we&#39;ve
 859 // structured the code so the windows are short and the frequency
 860 // of such futile wakups is low.
 861 
 862 void ObjectMonitor::exit(bool not_suspended, TRAPS) {
<span class="line-modified"> 863   Thread* const Self = THREAD;</span>
<span class="line-modified"> 864   void* cur = Atomic::load(&amp;_owner);</span>
<span class="line-modified"> 865   if (THREAD != cur) {</span>
<span class="line-modified"> 866     if (THREAD-&gt;is_lock_owned((address)cur)) {</span>



 867       assert(_recursions == 0, &quot;invariant&quot;);
<span class="line-modified"> 868       set_owner_from_BasicLock(cur, Self);  // Convert from BasicLock* to Thread*.</span>
 869       _recursions = 0;
 870     } else {
 871       // Apparent unbalanced locking ...
 872       // Naively we&#39;d like to throw IllegalMonitorStateException.
 873       // As a practical matter we can neither allocate nor throw an
 874       // exception as ::exit() can be called from leaf routines.
 875       // see x86_32.ad Fast_Unlock() and the I1 and I2 properties.
 876       // Upon deeper reflection, however, in a properly run JVM the only
 877       // way we should encounter this situation is in the presence of
 878       // unbalanced JNI locking. TODO: CheckJNICalls.
 879       // See also: CR4414101
<span class="line-modified"> 880 #ifdef ASSERT</span>
<span class="line-added"> 881       LogStreamHandle(Error, monitorinflation) lsh;</span>
<span class="line-added"> 882       lsh.print_cr(&quot;ERROR: ObjectMonitor::exit(): thread=&quot; INTPTR_FORMAT</span>
<span class="line-added"> 883                     &quot; is exiting an ObjectMonitor it does not own.&quot;, p2i(THREAD));</span>
<span class="line-added"> 884       lsh.print_cr(&quot;The imbalance is possibly caused by JNI locking.&quot;);</span>
<span class="line-added"> 885       print_debug_style_on(&amp;lsh);</span>
<span class="line-added"> 886 #endif</span>
<span class="line-added"> 887       assert(false, &quot;Non-balanced monitor enter/exit!&quot;);</span>
 888       return;
 889     }
 890   }
 891 
 892   if (_recursions != 0) {
 893     _recursions--;        // this is simple recursive enter
 894     return;
 895   }
 896 
 897   // Invariant: after setting Responsible=null an thread must execute
 898   // a MEMBAR or other serializing instruction before fetching EntryList|cxq.
 899   _Responsible = NULL;
 900 
 901 #if INCLUDE_JFR
 902   // get the owner&#39;s thread id for the MonitorEnter event
 903   // if it is enabled and the thread isn&#39;t suspended
 904   if (not_suspended &amp;&amp; EventJavaMonitorEnter::is_enabled()) {
 905     _previous_owner_tid = JFR_THREAD_ID(Self);
 906   }
 907 #endif
 908 
 909   for (;;) {
 910     assert(THREAD == _owner, &quot;invariant&quot;);
 911 
<span class="line-added"> 912     // Drop the lock.</span>
 913     // release semantics: prior loads and stores from within the critical section
 914     // must not float (reorder) past the following store that drops the lock.
<span class="line-modified"> 915     // Uses a storeload to separate release_store(owner) from the</span>
<span class="line-modified"> 916     // successor check. The try_set_owner() below uses cmpxchg() so</span>
<span class="line-modified"> 917     // we get the fence down there.</span>
<span class="line-modified"> 918     release_clear_owner(Self);</span>
<span class="line-added"> 919     OrderAccess::storeload();</span>
<span class="line-added"> 920 </span>
 921     if ((intptr_t(_EntryList)|intptr_t(_cxq)) == 0 || _succ != NULL) {
 922       return;
 923     }
 924     // Other threads are blocked trying to acquire the lock.
 925 
 926     // Normally the exiting thread is responsible for ensuring succession,
 927     // but if other successors are ready or other entering threads are spinning
 928     // then this thread can simply store NULL into _owner and exit without
 929     // waking a successor.  The existence of spinners or ready successors
 930     // guarantees proper succession (liveness).  Responsibility passes to the
 931     // ready or running successors.  The exiting thread delegates the duty.
 932     // More precisely, if a successor already exists this thread is absolved
 933     // of the responsibility of waking (unparking) one.
 934     //
 935     // The _succ variable is critical to reducing futile wakeup frequency.
 936     // _succ identifies the &quot;heir presumptive&quot; thread that has been made
 937     // ready (unparked) but that has not yet run.  We need only one such
 938     // successor thread to guarantee progress.
 939     // See http://www.usenix.org/events/jvm01/full_papers/dice/dice.pdf
 940     // section 3.3 &quot;Futile Wakeup Throttling&quot; for details.
</pre>
<hr />
<pre>
 942     // Note that spinners in Enter() also set _succ non-null.
 943     // In the current implementation spinners opportunistically set
 944     // _succ so that exiting threads might avoid waking a successor.
 945     // Another less appealing alternative would be for the exiting thread
 946     // to drop the lock and then spin briefly to see if a spinner managed
 947     // to acquire the lock.  If so, the exiting thread could exit
 948     // immediately without waking a successor, otherwise the exiting
 949     // thread would need to dequeue and wake a successor.
 950     // (Note that we&#39;d need to make the post-drop spin short, but no
 951     // shorter than the worst-case round-trip cache-line migration time.
 952     // The dropped lock needs to become visible to the spinner, and then
 953     // the acquisition of the lock by the spinner must become visible to
 954     // the exiting thread).
 955 
 956     // It appears that an heir-presumptive (successor) must be made ready.
 957     // Only the current lock owner can manipulate the EntryList or
 958     // drain _cxq, so we need to reacquire the lock.  If we fail
 959     // to reacquire the lock the responsibility for ensuring succession
 960     // falls to the new owner.
 961     //
<span class="line-modified"> 962     if (try_set_owner_from(NULL, Self) != NULL) {</span>
 963       return;
 964     }
 965 
 966     guarantee(_owner == THREAD, &quot;invariant&quot;);
 967 
 968     ObjectWaiter * w = NULL;
 969 
 970     w = _EntryList;
 971     if (w != NULL) {
 972       // I&#39;d like to write: guarantee (w-&gt;_thread != Self).
 973       // But in practice an exiting thread may find itself on the EntryList.
 974       // Let&#39;s say thread T1 calls O.wait().  Wait() enqueues T1 on O&#39;s waitset and
 975       // then calls exit().  Exit release the lock by setting O._owner to NULL.
 976       // Let&#39;s say T1 then stalls.  T2 acquires O and calls O.notify().  The
 977       // notify() operation moves T1 from O&#39;s waitset to O&#39;s EntryList. T2 then
 978       // release the lock &quot;O&quot;.  T2 resumes immediately after the ST of null into
 979       // _owner, above.  T2 notices that the EntryList is populated, so it
 980       // reacquires the lock and then finds itself on the EntryList.
 981       // Given all that, we have to tolerate the circumstance where &quot;w&quot; is
 982       // associated with Self.
 983       assert(w-&gt;TState == ObjectWaiter::TS_ENTER, &quot;invariant&quot;);
 984       ExitEpilog(Self, w);
 985       return;
 986     }
 987 
 988     // If we find that both _cxq and EntryList are null then just
 989     // re-run the exit protocol from the top.
 990     w = _cxq;
 991     if (w == NULL) continue;
 992 
 993     // Drain _cxq into EntryList - bulk transfer.
 994     // First, detach _cxq.
 995     // The following loop is tantamount to: w = swap(&amp;cxq, NULL)
 996     for (;;) {
 997       assert(w != NULL, &quot;Invariant&quot;);
<span class="line-modified"> 998       ObjectWaiter * u = Atomic::cmpxchg(&amp;_cxq, w, (ObjectWaiter*)NULL);</span>
 999       if (u == w) break;
1000       w = u;
1001     }
1002 
1003     assert(w != NULL, &quot;invariant&quot;);
1004     assert(_EntryList == NULL, &quot;invariant&quot;);
1005 
1006     // Convert the LIFO SLL anchored by _cxq into a DLL.
1007     // The list reorganization step operates in O(LENGTH(w)) time.
1008     // It&#39;s critical that this step operate quickly as
1009     // &quot;Self&quot; still holds the outer-lock, restricting parallelism
1010     // and effectively lengthening the critical section.
1011     // Invariant: s chases t chases u.
1012     // TODO-FIXME: consider changing EntryList from a DLL to a CDLL so
1013     // we have faster access to the tail.
1014 
1015     _EntryList = w;
1016     ObjectWaiter * q = NULL;
1017     ObjectWaiter * p;
1018     for (p = w; p != NULL; p = p-&gt;_next) {
</pre>
<hr />
<pre>
1075 
1076 
1077 void ObjectMonitor::ExitEpilog(Thread * Self, ObjectWaiter * Wakee) {
1078   assert(_owner == Self, &quot;invariant&quot;);
1079 
1080   // Exit protocol:
1081   // 1. ST _succ = wakee
1082   // 2. membar #loadstore|#storestore;
1083   // 2. ST _owner = NULL
1084   // 3. unpark(wakee)
1085 
1086   _succ = Wakee-&gt;_thread;
1087   ParkEvent * Trigger = Wakee-&gt;_event;
1088 
1089   // Hygiene -- once we&#39;ve set _owner = NULL we can&#39;t safely dereference Wakee again.
1090   // The thread associated with Wakee may have grabbed the lock and &quot;Wakee&quot; may be
1091   // out-of-scope (non-extant).
1092   Wakee  = NULL;
1093 
1094   // Drop the lock
<span class="line-modified">1095   // Uses a fence to separate release_store(owner) from the LD in unpark().</span>
<span class="line-modified">1096   release_clear_owner(Self);</span>
<span class="line-added">1097   OrderAccess::fence();</span>
1098 
1099   DTRACE_MONITOR_PROBE(contended__exit, this, object(), Self);
1100   Trigger-&gt;unpark();
1101 
1102   // Maintain stats and report events to JVMTI
1103   OM_PERFDATA_OP(Parks, inc());
1104 }
1105 
1106 
1107 // -----------------------------------------------------------------------------
1108 // Class Loader deadlock handling.
1109 //
1110 // complete_exit exits a lock returning recursion count
1111 // complete_exit/reenter operate as a wait without waiting
1112 // complete_exit requires an inflated monitor
1113 // The _owner field is not always the Thread addr even with an
1114 // inflated monitor, e.g. the monitor can be inflated by a non-owning
1115 // thread due to contention.
<span class="line-modified">1116 intx ObjectMonitor::complete_exit(TRAPS) {</span>
1117   Thread * const Self = THREAD;
1118   assert(Self-&gt;is_Java_thread(), &quot;Must be Java thread!&quot;);
1119   JavaThread *jt = (JavaThread *)THREAD;
1120 
1121   assert(InitDone, &quot;Unexpectedly not initialized&quot;);
1122 
<span class="line-modified">1123   void* cur = Atomic::load(&amp;_owner);</span>
<span class="line-modified">1124   if (THREAD != cur) {</span>
<span class="line-added">1125     if (THREAD-&gt;is_lock_owned((address)cur)) {</span>
1126       assert(_recursions == 0, &quot;internal state error&quot;);
<span class="line-modified">1127       set_owner_from_BasicLock(cur, Self);  // Convert from BasicLock* to Thread*.</span>
1128       _recursions = 0;
1129     }
1130   }
1131 
1132   guarantee(Self == _owner, &quot;complete_exit not owner&quot;);
<span class="line-modified">1133   intx save = _recursions; // record the old recursion count</span>
1134   _recursions = 0;        // set the recursion level to be 0
1135   exit(true, Self);           // exit the monitor
1136   guarantee(_owner != Self, &quot;invariant&quot;);
1137   return save;
1138 }
1139 
1140 // reenter() enters a lock and sets recursion count
1141 // complete_exit/reenter operate as a wait without waiting
<span class="line-modified">1142 void ObjectMonitor::reenter(intx recursions, TRAPS) {</span>
1143   Thread * const Self = THREAD;
1144   assert(Self-&gt;is_Java_thread(), &quot;Must be Java thread!&quot;);
1145   JavaThread *jt = (JavaThread *)THREAD;
1146 
1147   guarantee(_owner != Self, &quot;reenter already owner&quot;);
1148   enter(THREAD);       // enter the monitor
1149   guarantee(_recursions == 0, &quot;reenter recursion&quot;);
1150   _recursions = recursions;
1151   return;
1152 }
1153 
<span class="line-modified">1154 // Checks that the current THREAD owns this monitor and causes an</span>
<span class="line-modified">1155 // immediate return if it doesn&#39;t. We don&#39;t use the CHECK macro</span>
<span class="line-modified">1156 // because we want the IMSE to be the only exception that is thrown</span>
<span class="line-modified">1157 // from the call site when false is returned. Any other pending</span>
<span class="line-modified">1158 // exception is ignored.</span>
<span class="line-modified">1159 #define CHECK_OWNER()                                                  \</span>
<span class="line-modified">1160   do {                                                                 \</span>
<span class="line-modified">1161     if (!check_owner(THREAD)) {                                        \</span>
<span class="line-modified">1162        assert(HAS_PENDING_EXCEPTION, &quot;expected a pending IMSE here.&quot;); \</span>
<span class="line-modified">1163        return;                                                         \</span>
<span class="line-modified">1164      }                                                                 \</span>






1165   } while (false)
1166 
<span class="line-modified">1167 // Returns true if the specified thread owns the ObjectMonitor.</span>
<span class="line-modified">1168 // Otherwise returns false and throws IllegalMonitorStateException</span>
<span class="line-modified">1169 // (IMSE). If there is a pending exception and the specified thread</span>
<span class="line-modified">1170 // is not the owner, that exception will be replaced by the IMSE.</span>
<span class="line-modified">1171 bool ObjectMonitor::check_owner(Thread* THREAD) {</span>
<span class="line-modified">1172   void* cur = Atomic::load(&amp;_owner);</span>
<span class="line-added">1173   if (cur == THREAD) {</span>
<span class="line-added">1174     return true;</span>
<span class="line-added">1175   }</span>
<span class="line-added">1176   if (THREAD-&gt;is_lock_owned((address)cur)) {</span>
<span class="line-added">1177     set_owner_from_BasicLock(cur, THREAD);  // Convert from BasicLock* to Thread*.</span>
<span class="line-added">1178     _recursions = 0;</span>
<span class="line-added">1179     return true;</span>
<span class="line-added">1180   }</span>
<span class="line-added">1181   THROW_MSG_(vmSymbols::java_lang_IllegalMonitorStateException(),</span>
<span class="line-added">1182              &quot;current thread is not owner&quot;, false);</span>
1183 }
1184 
1185 static void post_monitor_wait_event(EventJavaMonitorWait* event,
1186                                     ObjectMonitor* monitor,
1187                                     jlong notifier_tid,
1188                                     jlong timeout,
1189                                     bool timedout) {
1190   assert(event != NULL, &quot;invariant&quot;);
1191   assert(monitor != NULL, &quot;invariant&quot;);
1192   event-&gt;set_monitorClass(((oop)monitor-&gt;object())-&gt;klass());
1193   event-&gt;set_timeout(timeout);
1194   event-&gt;set_address((uintptr_t)monitor-&gt;object_addr());
1195   event-&gt;set_notifier(notifier_tid);
1196   event-&gt;set_timedOut(timedout);
1197   event-&gt;commit();
1198 }
1199 
1200 // -----------------------------------------------------------------------------
1201 // Wait/Notify/NotifyAll
1202 //
1203 // Note: a subset of changes to ObjectMonitor::wait()
1204 // will need to be replicated in complete_exit
1205 void ObjectMonitor::wait(jlong millis, bool interruptible, TRAPS) {
1206   Thread * const Self = THREAD;
1207   assert(Self-&gt;is_Java_thread(), &quot;Must be Java thread!&quot;);
1208   JavaThread *jt = (JavaThread *)THREAD;
1209 
1210   assert(InitDone, &quot;Unexpectedly not initialized&quot;);
1211 
<span class="line-modified">1212   CHECK_OWNER();  // Throws IMSE if not owner.</span>

1213 
1214   EventJavaMonitorWait event;
1215 
1216   // check for a pending interrupt
<span class="line-modified">1217   if (interruptible &amp;&amp; jt-&gt;is_interrupted(true) &amp;&amp; !HAS_PENDING_EXCEPTION) {</span>
1218     // post monitor waited event.  Note that this is past-tense, we are done waiting.
1219     if (JvmtiExport::should_post_monitor_waited()) {
1220       // Note: &#39;false&#39; parameter is passed here because the
1221       // wait was not timed out due to thread interrupt.
1222       JvmtiExport::post_monitor_waited(jt, this, false);
1223 
1224       // In this short circuit of the monitor wait protocol, the
1225       // current thread never drops ownership of the monitor and
1226       // never gets added to the wait queue so the current thread
1227       // cannot be made the successor. This means that the
1228       // JVMTI_EVENT_MONITOR_WAITED event handler cannot accidentally
1229       // consume an unpark() meant for the ParkEvent associated with
1230       // this ObjectMonitor.
1231     }
1232     if (event.should_commit()) {
1233       post_monitor_wait_event(&amp;event, this, 0, millis, false);
1234     }
1235     THROW(vmSymbols::java_lang_InterruptedException());
1236     return;
1237   }
</pre>
<hr />
<pre>
1244   // Critically, after we reset() the event but prior to park(), we must check
1245   // for a pending interrupt.
1246   ObjectWaiter node(Self);
1247   node.TState = ObjectWaiter::TS_WAIT;
1248   Self-&gt;_ParkEvent-&gt;reset();
1249   OrderAccess::fence();          // ST into Event; membar ; LD interrupted-flag
1250 
1251   // Enter the waiting queue, which is a circular doubly linked list in this case
1252   // but it could be a priority queue or any data structure.
1253   // _WaitSetLock protects the wait queue.  Normally the wait queue is accessed only
1254   // by the the owner of the monitor *except* in the case where park()
1255   // returns because of a timeout of interrupt.  Contention is exceptionally rare
1256   // so we use a simple spin-lock instead of a heavier-weight blocking lock.
1257 
1258   Thread::SpinAcquire(&amp;_WaitSetLock, &quot;WaitSet - add&quot;);
1259   AddWaiter(&amp;node);
1260   Thread::SpinRelease(&amp;_WaitSetLock);
1261 
1262   _Responsible = NULL;
1263 
<span class="line-modified">1264   intx save = _recursions;     // record the old recursion count</span>
1265   _waiters++;                  // increment the number of waiters
1266   _recursions = 0;             // set the recursion level to be 1
1267   exit(true, Self);                    // exit the monitor
1268   guarantee(_owner != Self, &quot;invariant&quot;);
1269 
1270   // The thread is on the WaitSet list - now park() it.
1271   // On MP systems it&#39;s conceivable that a brief spin before we park
1272   // could be profitable.
1273   //
1274   // TODO-FIXME: change the following logic to a loop of the form
1275   //   while (!timeout &amp;&amp; !interrupted &amp;&amp; _notified == 0) park()
1276 
1277   int ret = OS_OK;
1278   int WasNotified = 0;
<span class="line-added">1279 </span>
<span class="line-added">1280   // Need to check interrupt state whilst still _thread_in_vm</span>
<span class="line-added">1281   bool interrupted = interruptible &amp;&amp; jt-&gt;is_interrupted(false);</span>
<span class="line-added">1282 </span>
1283   { // State transition wrappers
1284     OSThread* osthread = Self-&gt;osthread();
1285     OSThreadWaitState osts(osthread, true);
1286     {
1287       ThreadBlockInVM tbivm(jt);
1288       // Thread is in thread_blocked state and oop access is unsafe.
1289       jt-&gt;set_suspend_equivalent();
1290 
<span class="line-modified">1291       if (interrupted || HAS_PENDING_EXCEPTION) {</span>
1292         // Intentionally empty
1293       } else if (node._notified == 0) {
1294         if (millis &lt;= 0) {
1295           Self-&gt;_ParkEvent-&gt;park();
1296         } else {
1297           ret = Self-&gt;_ParkEvent-&gt;park(millis);
1298         }
1299       }
1300 
1301       // were we externally suspended while we were waiting?
1302       if (ExitSuspendEquivalent (jt)) {
1303         // TODO-FIXME: add -- if succ == Self then succ = null.
1304         jt-&gt;java_suspend_self();
1305       }
1306 
1307     } // Exit thread safepoint: transition _thread_blocked -&gt; _thread_in_vm
1308 
1309     // Node may be on the WaitSet, the EntryList (or cxq), or in transition
1310     // from the WaitSet to the EntryList.
1311     // See if we need to remove Node from the WaitSet.
</pre>
<hr />
<pre>
1391     }
1392 
1393     // Self has reacquired the lock.
1394     // Lifecycle - the node representing Self must not appear on any queues.
1395     // Node is about to go out-of-scope, but even if it were immortal we wouldn&#39;t
1396     // want residual elements associated with this thread left on any lists.
1397     guarantee(node.TState == ObjectWaiter::TS_RUN, &quot;invariant&quot;);
1398     assert(_owner == Self, &quot;invariant&quot;);
1399     assert(_succ != Self, &quot;invariant&quot;);
1400   } // OSThreadWaitState()
1401 
1402   jt-&gt;set_current_waiting_monitor(NULL);
1403 
1404   guarantee(_recursions == 0, &quot;invariant&quot;);
1405   _recursions = save;     // restore the old recursion count
1406   _waiters--;             // decrement the number of waiters
1407 
1408   // Verify a few postconditions
1409   assert(_owner == Self, &quot;invariant&quot;);
1410   assert(_succ != Self, &quot;invariant&quot;);
<span class="line-modified">1411   assert(((oop)(object()))-&gt;mark() == markWord::encode(this), &quot;invariant&quot;);</span>
1412 
1413   // check if the notification happened
1414   if (!WasNotified) {
1415     // no, it could be timeout or Thread.interrupt() or both
1416     // check for interrupt event, otherwise it is timeout
<span class="line-modified">1417     if (interruptible &amp;&amp; jt-&gt;is_interrupted(true) &amp;&amp; !HAS_PENDING_EXCEPTION) {</span>
1418       THROW(vmSymbols::java_lang_InterruptedException());
1419     }
1420   }
1421 
1422   // NOTE: Spurious wake up will be consider as timeout.
1423   // Monitor notify has precedence over thread interrupt.
1424 }
1425 
1426 
1427 // Consider:
1428 // If the lock is cool (cxq == null &amp;&amp; succ == null) and we&#39;re on an MP system
1429 // then instead of transferring a thread from the WaitSet to the EntryList
1430 // we might just dequeue a thread from the WaitSet and directly unpark() it.
1431 
1432 void ObjectMonitor::INotify(Thread * Self) {
1433   Thread::SpinAcquire(&amp;_WaitSetLock, &quot;WaitSet - notify&quot;);
1434   ObjectWaiter * iterator = DequeueWaiter();
1435   if (iterator != NULL) {
1436     guarantee(iterator-&gt;TState == ObjectWaiter::TS_WAIT, &quot;invariant&quot;);
1437     guarantee(iterator-&gt;_notified == 0, &quot;invariant&quot;);
</pre>
<hr />
<pre>
1445 
1446     iterator-&gt;_notified = 1;
1447     iterator-&gt;_notifier_tid = JFR_THREAD_ID(Self);
1448 
1449     ObjectWaiter * list = _EntryList;
1450     if (list != NULL) {
1451       assert(list-&gt;_prev == NULL, &quot;invariant&quot;);
1452       assert(list-&gt;TState == ObjectWaiter::TS_ENTER, &quot;invariant&quot;);
1453       assert(list != iterator, &quot;invariant&quot;);
1454     }
1455 
1456     // prepend to cxq
1457     if (list == NULL) {
1458       iterator-&gt;_next = iterator-&gt;_prev = NULL;
1459       _EntryList = iterator;
1460     } else {
1461       iterator-&gt;TState = ObjectWaiter::TS_CXQ;
1462       for (;;) {
1463         ObjectWaiter * front = _cxq;
1464         iterator-&gt;_next = front;
<span class="line-modified">1465         if (Atomic::cmpxchg(&amp;_cxq, front, iterator) == front) {</span>
1466           break;
1467         }
1468       }
1469     }
1470 
1471     // _WaitSetLock protects the wait queue, not the EntryList.  We could
1472     // move the add-to-EntryList operation, above, outside the critical section
1473     // protected by _WaitSetLock.  In practice that&#39;s not useful.  With the
1474     // exception of  wait() timeouts and interrupts the monitor owner
1475     // is the only thread that grabs _WaitSetLock.  There&#39;s almost no contention
1476     // on _WaitSetLock so it&#39;s not profitable to reduce the length of the
1477     // critical section.
1478 
1479     iterator-&gt;wait_reenter_begin(this);
1480   }
1481   Thread::SpinRelease(&amp;_WaitSetLock);
1482 }
1483 
1484 // Consider: a not-uncommon synchronization bug is to use notify() when
1485 // notifyAll() is more appropriate, potentially resulting in stranded
1486 // threads; this is one example of a lost wakeup. A useful diagnostic
1487 // option is to force all notify() operations to behave as notifyAll().
1488 //
1489 // Note: We can also detect many such problems with a &quot;minimum wait&quot;.
1490 // When the &quot;minimum wait&quot; is set to a small non-zero timeout value
1491 // and the program does not hang whereas it did absent &quot;minimum wait&quot;,
1492 // that suggests a lost wakeup bug.
1493 
1494 void ObjectMonitor::notify(TRAPS) {
<span class="line-modified">1495   CHECK_OWNER();  // Throws IMSE if not owner.</span>
1496   if (_WaitSet == NULL) {
1497     return;
1498   }
1499   DTRACE_MONITOR_PROBE(notify, this, object(), THREAD);
1500   INotify(THREAD);
1501   OM_PERFDATA_OP(Notifications, inc(1));
1502 }
1503 
1504 
1505 // The current implementation of notifyAll() transfers the waiters one-at-a-time
1506 // from the waitset to the EntryList. This could be done more efficiently with a
1507 // single bulk transfer but in practice it&#39;s not time-critical. Beware too,
1508 // that in prepend-mode we invert the order of the waiters. Let&#39;s say that the
1509 // waitset is &quot;ABCD&quot; and the EntryList is &quot;XYZ&quot;. After a notifyAll() in prepend
1510 // mode the waitset will be empty and the EntryList will be &quot;DCBAXYZ&quot;.
1511 
1512 void ObjectMonitor::notifyAll(TRAPS) {
<span class="line-modified">1513   CHECK_OWNER();  // Throws IMSE if not owner.</span>
1514   if (_WaitSet == NULL) {
1515     return;
1516   }
1517 
1518   DTRACE_MONITOR_PROBE(notifyAll, this, object(), THREAD);
1519   int tally = 0;
1520   while (_WaitSet != NULL) {
1521     tally++;
1522     INotify(THREAD);
1523   }
1524 
1525   OM_PERFDATA_OP(Notifications, inc(tally));
1526 }
1527 
1528 // -----------------------------------------------------------------------------
1529 // Adaptive Spinning Support
1530 //
1531 // Adaptive spin-then-block - rational spinning
1532 //
1533 // Note that we spin &quot;globally&quot; on _owner with a classic SMP-polite TATAS
</pre>
<hr />
<pre>
1666     // We periodically check to see if there&#39;s a safepoint pending.
1667     if ((ctr &amp; 0xFF) == 0) {
1668       if (SafepointMechanism::should_block(Self)) {
1669         goto Abort;           // abrupt spin egress
1670       }
1671       SpinPause();
1672     }
1673 
1674     // Probe _owner with TATAS
1675     // If this thread observes the monitor transition or flicker
1676     // from locked to unlocked to locked, then the odds that this
1677     // thread will acquire the lock in this spin attempt go down
1678     // considerably.  The same argument applies if the CAS fails
1679     // or if we observe _owner change from one non-null value to
1680     // another non-null value.   In such cases we might abort
1681     // the spin without prejudice or apply a &quot;penalty&quot; to the
1682     // spin count-down variable &quot;ctr&quot;, reducing it by 100, say.
1683 
1684     Thread * ox = (Thread *) _owner;
1685     if (ox == NULL) {
<span class="line-modified">1686       ox = (Thread*)try_set_owner_from(NULL, Self);</span>
1687       if (ox == NULL) {
1688         // The CAS succeeded -- this thread acquired ownership
1689         // Take care of some bookkeeping to exit spin state.
1690         if (_succ == Self) {
1691           _succ = NULL;
1692         }
1693 
1694         // Increase _SpinDuration :
1695         // The spin was successful (profitable) so we tend toward
1696         // longer spin attempts in the future.
1697         // CONSIDER: factor &quot;ctr&quot; into the _SpinDuration adjustment.
1698         // If we acquired the lock early in the spin cycle it
1699         // makes sense to increase _SpinDuration proportionally.
1700         // Note that we don&#39;t clamp SpinDuration precisely at SpinLimit.
1701         int x = _SpinDuration;
1702         if (x &lt; Knob_SpinLimit) {
1703           if (x &lt; Knob_Poverty) x = Knob_Poverty;
1704           _SpinDuration = x + Knob_Bonus;
1705         }
1706         return 1;
</pre>
<hr />
<pre>
1929                                         CHECK);                          \
1930   }
1931 #define NEWPERFVARIABLE(n)                                                \
1932   {                                                                       \
1933     n = PerfDataManager::create_variable(SUN_RT, #n, PerfData::U_Events,  \
1934                                          CHECK);                          \
1935   }
1936     NEWPERFCOUNTER(_sync_Inflations);
1937     NEWPERFCOUNTER(_sync_Deflations);
1938     NEWPERFCOUNTER(_sync_ContendedLockAttempts);
1939     NEWPERFCOUNTER(_sync_FutileWakeups);
1940     NEWPERFCOUNTER(_sync_Parks);
1941     NEWPERFCOUNTER(_sync_Notifications);
1942     NEWPERFVARIABLE(_sync_MonExtant);
1943 #undef NEWPERFCOUNTER
1944 #undef NEWPERFVARIABLE
1945   }
1946 
1947   DEBUG_ONLY(InitDone = true;)
1948 }
<span class="line-added">1949 </span>
<span class="line-added">1950 void ObjectMonitor::print_on(outputStream* st) const {</span>
<span class="line-added">1951   // The minimal things to print for markWord printing, more can be added for debugging and logging.</span>
<span class="line-added">1952   st-&gt;print(&quot;{contentions=0x%08x,waiters=0x%08x&quot;</span>
<span class="line-added">1953             &quot;,recursions=&quot; INTX_FORMAT &quot;,owner=&quot; INTPTR_FORMAT &quot;}&quot;,</span>
<span class="line-added">1954             contentions(), waiters(), recursions(),</span>
<span class="line-added">1955             p2i(owner()));</span>
<span class="line-added">1956 }</span>
<span class="line-added">1957 void ObjectMonitor::print() const { print_on(tty); }</span>
<span class="line-added">1958 </span>
<span class="line-added">1959 #ifdef ASSERT</span>
<span class="line-added">1960 // Print the ObjectMonitor like a debugger would:</span>
<span class="line-added">1961 //</span>
<span class="line-added">1962 // (ObjectMonitor) 0x00007fdfb6012e40 = {</span>
<span class="line-added">1963 //   _header = 0x0000000000000001</span>
<span class="line-added">1964 //   _object = 0x000000070ff45fd0</span>
<span class="line-added">1965 //   _next_om = 0x0000000000000000</span>
<span class="line-added">1966 //   _pad_buf0 = {</span>
<span class="line-added">1967 //     [0] = &#39;\0&#39;</span>
<span class="line-added">1968 //     ...</span>
<span class="line-added">1969 //     [103] = &#39;\0&#39;</span>
<span class="line-added">1970 //   }</span>
<span class="line-added">1971 //   _owner = 0x0000000000000000</span>
<span class="line-added">1972 //   _previous_owner_tid = 0</span>
<span class="line-added">1973 //   _recursions = 0</span>
<span class="line-added">1974 //   _EntryList = 0x0000000000000000</span>
<span class="line-added">1975 //   _cxq = 0x0000000000000000</span>
<span class="line-added">1976 //   _succ = 0x0000000000000000</span>
<span class="line-added">1977 //   _Responsible = 0x0000000000000000</span>
<span class="line-added">1978 //   _Spinner = 0</span>
<span class="line-added">1979 //   _SpinDuration = 5000</span>
<span class="line-added">1980 //   _contentions = 0</span>
<span class="line-added">1981 //   _WaitSet = 0x0000700009756248</span>
<span class="line-added">1982 //   _waiters = 1</span>
<span class="line-added">1983 //   _WaitSetLock = 0</span>
<span class="line-added">1984 // }</span>
<span class="line-added">1985 //</span>
<span class="line-added">1986 void ObjectMonitor::print_debug_style_on(outputStream* st) const {</span>
<span class="line-added">1987   st-&gt;print_cr(&quot;(ObjectMonitor*) &quot; INTPTR_FORMAT &quot; = {&quot;, p2i(this));</span>
<span class="line-added">1988   st-&gt;print_cr(&quot;  _header = &quot; INTPTR_FORMAT, header().value());</span>
<span class="line-added">1989   st-&gt;print_cr(&quot;  _object = &quot; INTPTR_FORMAT, p2i(_object));</span>
<span class="line-added">1990   st-&gt;print_cr(&quot;  _next_om = &quot; INTPTR_FORMAT, p2i(next_om()));</span>
<span class="line-added">1991   st-&gt;print_cr(&quot;  _pad_buf0 = {&quot;);</span>
<span class="line-added">1992   st-&gt;print_cr(&quot;    [0] = &#39;\\0&#39;&quot;);</span>
<span class="line-added">1993   st-&gt;print_cr(&quot;    ...&quot;);</span>
<span class="line-added">1994   st-&gt;print_cr(&quot;    [%d] = &#39;\\0&#39;&quot;, (int)sizeof(_pad_buf0) - 1);</span>
<span class="line-added">1995   st-&gt;print_cr(&quot;  }&quot;);</span>
<span class="line-added">1996   st-&gt;print_cr(&quot;  _owner = &quot; INTPTR_FORMAT, p2i(_owner));</span>
<span class="line-added">1997   st-&gt;print_cr(&quot;  _previous_owner_tid = &quot; JLONG_FORMAT, _previous_owner_tid);</span>
<span class="line-added">1998   st-&gt;print_cr(&quot;  _recursions = &quot; INTX_FORMAT, _recursions);</span>
<span class="line-added">1999   st-&gt;print_cr(&quot;  _EntryList = &quot; INTPTR_FORMAT, p2i(_EntryList));</span>
<span class="line-added">2000   st-&gt;print_cr(&quot;  _cxq = &quot; INTPTR_FORMAT, p2i(_cxq));</span>
<span class="line-added">2001   st-&gt;print_cr(&quot;  _succ = &quot; INTPTR_FORMAT, p2i(_succ));</span>
<span class="line-added">2002   st-&gt;print_cr(&quot;  _Responsible = &quot; INTPTR_FORMAT, p2i(_Responsible));</span>
<span class="line-added">2003   st-&gt;print_cr(&quot;  _Spinner = %d&quot;, _Spinner);</span>
<span class="line-added">2004   st-&gt;print_cr(&quot;  _SpinDuration = %d&quot;, _SpinDuration);</span>
<span class="line-added">2005   st-&gt;print_cr(&quot;  _contentions = %d&quot;, _contentions);</span>
<span class="line-added">2006   st-&gt;print_cr(&quot;  _WaitSet = &quot; INTPTR_FORMAT, p2i(_WaitSet));</span>
<span class="line-added">2007   st-&gt;print_cr(&quot;  _waiters = %d&quot;, _waiters);</span>
<span class="line-added">2008   st-&gt;print_cr(&quot;  _WaitSetLock = %d&quot;, _WaitSetLock);</span>
<span class="line-added">2009   st-&gt;print_cr(&quot;}&quot;);</span>
<span class="line-added">2010 }</span>
<span class="line-added">2011 #endif</span>
</pre>
</td>
</tr>
</table>
<center><a href="mutexLocker.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="objectMonitor.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>