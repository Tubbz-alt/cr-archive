<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/runtime/biasedLocking.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="biasedLocking.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="deoptimization.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/runtime/biasedLocking.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 59 // or NULL, indicating that the lock is &quot;anonymously biased&quot;. The
 60 // first thread which locks an anonymously biased object biases the
 61 // lock toward that thread. If another thread subsequently attempts to
 62 // lock the same object, the bias is revoked.
 63 //
 64 // Because there are no updates to the object header at all during
 65 // recursive locking while the lock is biased, the biased lock entry
 66 // code is simply a test of the object header&#39;s value. If this test
 67 // succeeds, the lock has been acquired by the thread. If this test
 68 // fails, a bit test is done to see whether the bias bit is still
 69 // set. If not, we fall back to HotSpot&#39;s original CAS-based locking
 70 // scheme. If it is set, we attempt to CAS in a bias toward this
 71 // thread. The latter operation is expected to be the rarest operation
 72 // performed on these locks. We optimistically expect the biased lock
 73 // entry to hit most of the time, and want the CAS-based fallthrough
 74 // to occur quickly in the situations where the bias has been revoked.
 75 //
 76 // Revocation of the lock&#39;s bias is fairly straightforward. We want to
 77 // restore the object&#39;s header and stack-based BasicObjectLocks and
 78 // BasicLocks to the state they would have been in had the object been
<span class="line-modified"> 79 // locked by HotSpot&#39;s usual fast locking scheme. To do this, we bring</span>
<span class="line-modified"> 80 // the system to a safepoint and walk the stack of the thread toward</span>
<span class="line-modified"> 81 // which the lock is biased. We find all of the lock records on the</span>
<span class="line-modified"> 82 // stack corresponding to this object, in particular the first /</span>
<span class="line-modified"> 83 // &quot;highest&quot; record. We fill in the highest lock record with the</span>
<span class="line-modified"> 84 // object&#39;s displaced header (which is a well-known value given that</span>
<span class="line-modified"> 85 // we don&#39;t maintain an identity hash nor age bits for the object</span>
<span class="line-modified"> 86 // while it&#39;s in the biased state) and all other lock records with 0,</span>
<span class="line-modified"> 87 // the value for recursive locks. When the safepoint is released, the</span>
<span class="line-modified"> 88 // formerly-biased thread and all other threads revert back to</span>
<span class="line-modified"> 89 // HotSpot&#39;s CAS-based locking.</span>
 90 //
 91 // This scheme can not handle transfers of biases of single objects
 92 // from thread to thread efficiently, but it can handle bulk transfers
 93 // of such biases, which is a usage pattern showing up in some
 94 // applications and benchmarks. We implement &quot;bulk rebias&quot; and &quot;bulk
 95 // revoke&quot; operations using a &quot;bias epoch&quot; on a per-data-type basis.
 96 // If too many bias revocations are occurring for a particular data
 97 // type, the bias epoch for the data type is incremented at a
 98 // safepoint, effectively meaning that all previous biases are
 99 // invalid. The fast path locking case checks for an invalid epoch in
100 // the object header and attempts to rebias the object with a CAS if
101 // found, avoiding safepoints or bulk heap sweeps (the latter which
102 // was used in a prior version of this algorithm and did not scale
103 // well). If too many bias revocations persist, biasing is completely
104 // disabled for the data type by resetting the prototype header to the
<span class="line-modified">105 // unbiased markOop. The fast-path locking code checks to see whether</span>
106 // the instance&#39;s bias pattern differs from the prototype header&#39;s and
107 // causes the bias to be revoked without reaching a safepoint or,
108 // again, a bulk heap sweep.
109 
110 // Biased locking counters
111 class BiasedLockingCounters {
112  private:
113   int _total_entry_count;
114   int _biased_lock_entry_count;
115   int _anonymously_biased_lock_entry_count;
116   int _rebiased_lock_entry_count;
117   int _revoked_lock_entry_count;

118   int _fast_path_entry_count;
119   int _slow_path_entry_count;
120 
121  public:
122   BiasedLockingCounters() :
123     _total_entry_count(0),
124     _biased_lock_entry_count(0),
125     _anonymously_biased_lock_entry_count(0),
126     _rebiased_lock_entry_count(0),
127     _revoked_lock_entry_count(0),

128     _fast_path_entry_count(0),
129     _slow_path_entry_count(0) {}
130 
<span class="line-modified">131   int slow_path_entry_count(); // Compute this field if necessary</span>
132 
133   int* total_entry_count_addr()                   { return &amp;_total_entry_count; }
134   int* biased_lock_entry_count_addr()             { return &amp;_biased_lock_entry_count; }
135   int* anonymously_biased_lock_entry_count_addr() { return &amp;_anonymously_biased_lock_entry_count; }
136   int* rebiased_lock_entry_count_addr()           { return &amp;_rebiased_lock_entry_count; }
137   int* revoked_lock_entry_count_addr()            { return &amp;_revoked_lock_entry_count; }

138   int* fast_path_entry_count_addr()               { return &amp;_fast_path_entry_count; }
139   int* slow_path_entry_count_addr()               { return &amp;_slow_path_entry_count; }
140 
141   bool nonzero() { return _total_entry_count &gt; 0; }
142 
<span class="line-modified">143   void print_on(outputStream* st);</span>
<span class="line-modified">144   void print() { print_on(tty); }</span>
145 };
146 
147 
148 class BiasedLocking : AllStatic {



149 private:
150   static BiasedLockingCounters _counters;
151 
152 public:
153   static int* total_entry_count_addr();
154   static int* biased_lock_entry_count_addr();
155   static int* anonymously_biased_lock_entry_count_addr();
156   static int* rebiased_lock_entry_count_addr();
157   static int* revoked_lock_entry_count_addr();

158   static int* fast_path_entry_count_addr();
159   static int* slow_path_entry_count_addr();
160 
161   enum Condition {
162     NOT_BIASED = 1,
163     BIAS_REVOKED = 2,
<span class="line-modified">164     BIAS_REVOKED_AND_REBIASED = 3</span>
165   };
166 







167   // This initialization routine should only be called once and
168   // schedules a PeriodicTask to turn on biased locking a few seconds
169   // into the VM run to avoid startup time regressions
170   static void init();
171 
172   // This provides a global switch for leaving biased locking disabled
173   // for the first part of a run and enabling it later
174   static bool enabled();
175 
176   // This should be called by JavaThreads to revoke the bias of an object
<span class="line-modified">177   static Condition revoke_and_rebias(Handle obj, bool attempt_rebias, TRAPS);</span>



178 
<span class="line-removed">179   // These do not allow rebiasing; they are used by deoptimization to</span>
<span class="line-removed">180   // ensure that monitors on the stack can be migrated</span>
<span class="line-removed">181   static void revoke(GrowableArray&lt;Handle&gt;* objs);</span>
182   static void revoke_at_safepoint(Handle obj);
<span class="line-modified">183   static void revoke_at_safepoint(GrowableArray&lt;Handle&gt;* objs);</span>



184 
185   static void print_counters() { _counters.print(); }
186   static BiasedLockingCounters* counters() { return &amp;_counters; }
187 
188   // These routines are GC-related and should not be called by end
189   // users. GCs which do not do preservation of mark words do not need
190   // to call these routines.
191   static void preserve_marks();
192   static void restore_marks();
193 };
194 
195 #endif // SHARE_RUNTIME_BIASEDLOCKING_HPP
</pre>
</td>
<td>
<hr />
<pre>
 59 // or NULL, indicating that the lock is &quot;anonymously biased&quot;. The
 60 // first thread which locks an anonymously biased object biases the
 61 // lock toward that thread. If another thread subsequently attempts to
 62 // lock the same object, the bias is revoked.
 63 //
 64 // Because there are no updates to the object header at all during
 65 // recursive locking while the lock is biased, the biased lock entry
 66 // code is simply a test of the object header&#39;s value. If this test
 67 // succeeds, the lock has been acquired by the thread. If this test
 68 // fails, a bit test is done to see whether the bias bit is still
 69 // set. If not, we fall back to HotSpot&#39;s original CAS-based locking
 70 // scheme. If it is set, we attempt to CAS in a bias toward this
 71 // thread. The latter operation is expected to be the rarest operation
 72 // performed on these locks. We optimistically expect the biased lock
 73 // entry to hit most of the time, and want the CAS-based fallthrough
 74 // to occur quickly in the situations where the bias has been revoked.
 75 //
 76 // Revocation of the lock&#39;s bias is fairly straightforward. We want to
 77 // restore the object&#39;s header and stack-based BasicObjectLocks and
 78 // BasicLocks to the state they would have been in had the object been
<span class="line-modified"> 79 // locked by HotSpot&#39;s usual fast locking scheme. To do this, we execute</span>
<span class="line-modified"> 80 // a handshake with the JavaThread that biased the lock. Inside the</span>
<span class="line-modified"> 81 // handshake we walk the biaser stack searching for all of the lock</span>
<span class="line-modified"> 82 // records corresponding to this object, in particular the first / &quot;highest&quot;</span>
<span class="line-modified"> 83 // record. We fill in the highest lock record with the object&#39;s displaced</span>
<span class="line-modified"> 84 // header (which is a well-known value given that we don&#39;t maintain an</span>
<span class="line-modified"> 85 // identity hash nor age bits for the object while it&#39;s in the biased</span>
<span class="line-modified"> 86 // state) and all other lock records with 0, the value for recursive locks.</span>
<span class="line-modified"> 87 // Alternatively, we can revoke the bias of an object inside a safepoint</span>
<span class="line-modified"> 88 // if we are already in one and we detect that we need to perform a</span>
<span class="line-modified"> 89 // revocation.</span>
 90 //
 91 // This scheme can not handle transfers of biases of single objects
 92 // from thread to thread efficiently, but it can handle bulk transfers
 93 // of such biases, which is a usage pattern showing up in some
 94 // applications and benchmarks. We implement &quot;bulk rebias&quot; and &quot;bulk
 95 // revoke&quot; operations using a &quot;bias epoch&quot; on a per-data-type basis.
 96 // If too many bias revocations are occurring for a particular data
 97 // type, the bias epoch for the data type is incremented at a
 98 // safepoint, effectively meaning that all previous biases are
 99 // invalid. The fast path locking case checks for an invalid epoch in
100 // the object header and attempts to rebias the object with a CAS if
101 // found, avoiding safepoints or bulk heap sweeps (the latter which
102 // was used in a prior version of this algorithm and did not scale
103 // well). If too many bias revocations persist, biasing is completely
104 // disabled for the data type by resetting the prototype header to the
<span class="line-modified">105 // unbiased markWord. The fast-path locking code checks to see whether</span>
106 // the instance&#39;s bias pattern differs from the prototype header&#39;s and
107 // causes the bias to be revoked without reaching a safepoint or,
108 // again, a bulk heap sweep.
109 
110 // Biased locking counters
111 class BiasedLockingCounters {
112  private:
113   int _total_entry_count;
114   int _biased_lock_entry_count;
115   int _anonymously_biased_lock_entry_count;
116   int _rebiased_lock_entry_count;
117   int _revoked_lock_entry_count;
<span class="line-added">118   int _handshakes_count;</span>
119   int _fast_path_entry_count;
120   int _slow_path_entry_count;
121 
122  public:
123   BiasedLockingCounters() :
124     _total_entry_count(0),
125     _biased_lock_entry_count(0),
126     _anonymously_biased_lock_entry_count(0),
127     _rebiased_lock_entry_count(0),
128     _revoked_lock_entry_count(0),
<span class="line-added">129     _handshakes_count(0),</span>
130     _fast_path_entry_count(0),
131     _slow_path_entry_count(0) {}
132 
<span class="line-modified">133   int slow_path_entry_count() const; // Compute this field if necessary</span>
134 
135   int* total_entry_count_addr()                   { return &amp;_total_entry_count; }
136   int* biased_lock_entry_count_addr()             { return &amp;_biased_lock_entry_count; }
137   int* anonymously_biased_lock_entry_count_addr() { return &amp;_anonymously_biased_lock_entry_count; }
138   int* rebiased_lock_entry_count_addr()           { return &amp;_rebiased_lock_entry_count; }
139   int* revoked_lock_entry_count_addr()            { return &amp;_revoked_lock_entry_count; }
<span class="line-added">140   int* handshakes_count_addr()                    { return &amp;_handshakes_count; }</span>
141   int* fast_path_entry_count_addr()               { return &amp;_fast_path_entry_count; }
142   int* slow_path_entry_count_addr()               { return &amp;_slow_path_entry_count; }
143 
144   bool nonzero() { return _total_entry_count &gt; 0; }
145 
<span class="line-modified">146   void print_on(outputStream* st) const;</span>
<span class="line-modified">147   void print() const;</span>
148 };
149 
150 
151 class BiasedLocking : AllStatic {
<span class="line-added">152 friend class VM_BulkRevokeBias;</span>
<span class="line-added">153 friend class RevokeOneBias;</span>
<span class="line-added">154 </span>
155 private:
156   static BiasedLockingCounters _counters;
157 
158 public:
159   static int* total_entry_count_addr();
160   static int* biased_lock_entry_count_addr();
161   static int* anonymously_biased_lock_entry_count_addr();
162   static int* rebiased_lock_entry_count_addr();
163   static int* revoked_lock_entry_count_addr();
<span class="line-added">164   static int* handshakes_count_addr();</span>
165   static int* fast_path_entry_count_addr();
166   static int* slow_path_entry_count_addr();
167 
168   enum Condition {
169     NOT_BIASED = 1,
170     BIAS_REVOKED = 2,
<span class="line-modified">171     NOT_REVOKED = 3</span>
172   };
173 
<span class="line-added">174 private:</span>
<span class="line-added">175   static void single_revoke_at_safepoint(oop obj, bool is_bulk, JavaThread* requester, JavaThread** biaser);</span>
<span class="line-added">176   static void bulk_revoke_at_safepoint(oop o, bool bulk_rebias, JavaThread* requester);</span>
<span class="line-added">177   static Condition single_revoke_with_handshake(Handle obj, JavaThread *requester, JavaThread *biaser);</span>
<span class="line-added">178   static void walk_stack_and_revoke(oop obj, JavaThread* biased_locker);</span>
<span class="line-added">179 </span>
<span class="line-added">180 public:</span>
181   // This initialization routine should only be called once and
182   // schedules a PeriodicTask to turn on biased locking a few seconds
183   // into the VM run to avoid startup time regressions
184   static void init();
185 
186   // This provides a global switch for leaving biased locking disabled
187   // for the first part of a run and enabling it later
188   static bool enabled();
189 
190   // This should be called by JavaThreads to revoke the bias of an object
<span class="line-modified">191   static void revoke(Handle obj, TRAPS);</span>
<span class="line-added">192 </span>
<span class="line-added">193   // This must only be called by a JavaThread to revoke the bias of an owned object.</span>
<span class="line-added">194   static void revoke_own_lock(Handle obj, TRAPS);</span>
195 



196   static void revoke_at_safepoint(Handle obj);
<span class="line-modified">197 </span>
<span class="line-added">198   // These are used by deoptimization to ensure that monitors on the stack</span>
<span class="line-added">199   // can be migrated</span>
<span class="line-added">200   static void revoke(GrowableArray&lt;Handle&gt;* objs, JavaThread *biaser);</span>
201 
202   static void print_counters() { _counters.print(); }
203   static BiasedLockingCounters* counters() { return &amp;_counters; }
204 
205   // These routines are GC-related and should not be called by end
206   // users. GCs which do not do preservation of mark words do not need
207   // to call these routines.
208   static void preserve_marks();
209   static void restore_marks();
210 };
211 
212 #endif // SHARE_RUNTIME_BIASEDLOCKING_HPP
</pre>
</td>
</tr>
</table>
<center><a href="biasedLocking.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="deoptimization.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>