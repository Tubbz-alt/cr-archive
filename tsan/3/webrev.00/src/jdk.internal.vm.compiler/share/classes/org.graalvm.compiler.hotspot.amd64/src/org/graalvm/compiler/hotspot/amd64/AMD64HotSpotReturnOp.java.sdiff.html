<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.hotspot.amd64/src/org/graalvm/compiler/hotspot/amd64/AMD64HotSpotReturnOp.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../style.css" />
  </head>
<body>
<center><a href="AMD64HotSpotRestoreRbpOp.java.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../../../index.html" target="_top">index</a> <a href="AMD64HotSpotStrategySwitchOp.java.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.hotspot.amd64/src/org/graalvm/compiler/hotspot/amd64/AMD64HotSpotReturnOp.java</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2012, 2018, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
</pre>
<hr />
<pre>
 94                 CallingConvention cc = enableStackReservedZone.getOutgoingCallingConvention();
 95                 assert cc.getArgumentCount() == 1;
 96                 Register arg0 = ((RegisterValue) cc.getArgument(0)).getRegister();
 97                 masm.movq(arg0, thread);
 98                 AMD64Call.directCall(crb, masm, enableStackReservedZone, scratchForSafepointOnReturn, false, null);
 99                 if (stackAdjust &gt; 0) {
100                     masm.addq(rsp, stackAdjust);
101                 }
102                 AMD64Call.directJmp(crb, masm, foreignCalls.lookupForeignCall(THROW_DELAYED_STACKOVERFLOW_ERROR), scratchForSafepointOnReturn);
103                 masm.bind(noReserved);
104             }
105 
106             // Every non-stub compile method must have a poll before the return.
107             AMD64HotSpotSafepointOp.emitCode(crb, masm, config, true, null, thread, scratchForSafepointOnReturn);
108 
109             /*
110              * We potentially return to the interpreter, and that&#39;s an AVX-SSE transition. The only
111              * live value at this point should be the return value in either rax, or in xmm0 with
112              * the upper half of the register unused, so we don&#39;t destroy any value here.
113              */
<span class="line-modified">114             if (masm.supports(CPUFeature.AVX)) {</span>



115                 masm.vzeroupper();
116             }
117         }
118         masm.ret(0);
119     }

120 }
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2012, 2019, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
</pre>
<hr />
<pre>
 94                 CallingConvention cc = enableStackReservedZone.getOutgoingCallingConvention();
 95                 assert cc.getArgumentCount() == 1;
 96                 Register arg0 = ((RegisterValue) cc.getArgument(0)).getRegister();
 97                 masm.movq(arg0, thread);
 98                 AMD64Call.directCall(crb, masm, enableStackReservedZone, scratchForSafepointOnReturn, false, null);
 99                 if (stackAdjust &gt; 0) {
100                     masm.addq(rsp, stackAdjust);
101                 }
102                 AMD64Call.directJmp(crb, masm, foreignCalls.lookupForeignCall(THROW_DELAYED_STACKOVERFLOW_ERROR), scratchForSafepointOnReturn);
103                 masm.bind(noReserved);
104             }
105 
106             // Every non-stub compile method must have a poll before the return.
107             AMD64HotSpotSafepointOp.emitCode(crb, masm, config, true, null, thread, scratchForSafepointOnReturn);
108 
109             /*
110              * We potentially return to the interpreter, and that&#39;s an AVX-SSE transition. The only
111              * live value at this point should be the return value in either rax, or in xmm0 with
112              * the upper half of the register unused, so we don&#39;t destroy any value here.
113              */
<span class="line-modified">114             if (masm.supports(CPUFeature.AVX) &amp;&amp; crb.needsClearUpperVectorRegisters()) {</span>
<span class="line-added">115                 // If we decide to perform vzeroupper also for stubs (like what JDK9+ C2 does for</span>
<span class="line-added">116                 // intrinsics that employ AVX2 instruction), we need to be careful that it kills all</span>
<span class="line-added">117                 // the xmm registers (at least the upper halves).</span>
118                 masm.vzeroupper();
119             }
120         }
121         masm.ret(0);
122     }
<span class="line-added">123 </span>
124 }
</pre>
</td>
</tr>
</table>
<center><a href="AMD64HotSpotRestoreRbpOp.java.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../../../index.html" target="_top">index</a> <a href="AMD64HotSpotStrategySwitchOp.java.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>