<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.amd64/src/org/graalvm/compiler/lir/amd64/vector/AMD64VectorMove.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../../style.css" />
  </head>
<body>
<center><a href="AMD64VectorCompareOp.java.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../../../../index.html" target="_top">index</a> <a href="AMD64VectorUnary.java.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.amd64/src/org/graalvm/compiler/lir/amd64/vector/AMD64VectorMove.java</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2013, 2018, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 
 25 package org.graalvm.compiler.lir.amd64.vector;
 26 
 27 import static jdk.vm.ci.code.ValueUtil.asRegister;
 28 import static jdk.vm.ci.code.ValueUtil.isRegister;
 29 import static jdk.vm.ci.code.ValueUtil.isStackSlot;
 30 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp.VMOVD;
<span class="line-modified"> 31 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp.VMOVDQU;</span>
 32 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp.VMOVQ;
 33 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp.VMOVSD;
 34 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp.VMOVSS;
 35 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp.VMOVUPD;
 36 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp.VMOVUPS;
 37 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.VexRVMOp.VXORPD;
 38 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.COMPOSITE;
 39 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.HINT;
 40 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.REG;
 41 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.STACK;
 42 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.UNINITIALIZED;
 43 
 44 import org.graalvm.compiler.asm.amd64.AMD64Address;
 45 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp;
 46 import org.graalvm.compiler.asm.amd64.AMD64MacroAssembler;
 47 import org.graalvm.compiler.asm.amd64.AVXKind;
 48 import org.graalvm.compiler.asm.amd64.AVXKind.AVXSize;
 49 import org.graalvm.compiler.debug.GraalError;
 50 import org.graalvm.compiler.lir.LIRFrameState;
 51 import org.graalvm.compiler.lir.LIRInstructionClass;
</pre>
<hr />
<pre>
 57 import org.graalvm.compiler.lir.amd64.AMD64Move;
 58 import org.graalvm.compiler.lir.amd64.AMD64RestoreRegistersOp;
 59 import org.graalvm.compiler.lir.amd64.AMD64SaveRegistersOp;
 60 import org.graalvm.compiler.lir.asm.CompilationResultBuilder;
 61 
 62 import jdk.vm.ci.amd64.AMD64Kind;
 63 import jdk.vm.ci.code.Register;
 64 import jdk.vm.ci.code.RegisterValue;
 65 import jdk.vm.ci.code.StackSlot;
 66 import jdk.vm.ci.meta.AllocatableValue;
 67 import jdk.vm.ci.meta.Constant;
 68 import jdk.vm.ci.meta.JavaConstant;
 69 import jdk.vm.ci.meta.Value;
 70 
 71 public class AMD64VectorMove {
 72 
 73     @Opcode(&quot;VMOVE&quot;)
 74     public static final class MoveToRegOp extends AMD64LIRInstruction implements ValueMoveOp {
 75         public static final LIRInstructionClass&lt;MoveToRegOp&gt; TYPE = LIRInstructionClass.create(MoveToRegOp.class);
 76 
<span class="line-modified"> 77         @Def({REG, HINT}) protected AllocatableValue result;</span>
 78         @Use({REG, STACK}) protected AllocatableValue input;
 79 
 80         public MoveToRegOp(AllocatableValue result, AllocatableValue input) {
 81             super(TYPE);
 82             this.result = result;
 83             this.input = input;
 84         }
 85 
 86         @Override
 87         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 88             move(crb, masm, result, input);
 89         }
 90 
 91         @Override
 92         public AllocatableValue getInput() {
 93             return input;
 94         }
 95 
 96         @Override
 97         public AllocatableValue getResult() {
</pre>
<hr />
<pre>
186         }
187 
188         @Override
189         public AllocatableValue getResult() {
190             return result;
191         }
192 
193         @Override
194         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
195             // backup scratch register
196             move(crb, masm, backupSlot, scratch.asValue(backupSlot.getValueKind()));
197             // move stack slot
198             move(crb, masm, scratch.asValue(getInput().getValueKind()), getInput());
199             move(crb, masm, getResult(), scratch.asValue(getResult().getValueKind()));
200             // restore scratch register
201             move(crb, masm, scratch.asValue(backupSlot.getValueKind()), backupSlot);
202 
203         }
204     }
205 
<span class="line-modified">206     public abstract static class VectorMemOp extends AMD64LIRInstruction {</span>
207 
<span class="line-removed">208         protected final AVXSize size;</span>
209         protected final VexMoveOp op;
210 
211         @Use({COMPOSITE}) protected AMD64AddressValue address;
212         @State protected LIRFrameState state;
213 
214         protected VectorMemOp(LIRInstructionClass&lt;? extends VectorMemOp&gt; c, AVXSize size, VexMoveOp op, AMD64AddressValue address, LIRFrameState state) {
<span class="line-modified">215             super(c);</span>
<span class="line-removed">216             this.size = size;</span>
217             this.op = op;
218             this.address = address;
219             this.state = state;
220         }
221 
222         protected abstract void emitMemAccess(AMD64MacroAssembler masm);
223 
224         @Override
225         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
226             if (state != null) {
227                 crb.recordImplicitException(masm.position(), state);
228             }
229             emitMemAccess(masm);
230         }
231     }
232 
233     public static final class VectorLoadOp extends VectorMemOp {
234         public static final LIRInstructionClass&lt;VectorLoadOp&gt; TYPE = LIRInstructionClass.create(VectorLoadOp.class);
235 
236         @Def({REG}) protected AllocatableValue result;
</pre>
<hr />
<pre>
249     public static class VectorStoreOp extends VectorMemOp {
250         public static final LIRInstructionClass&lt;VectorStoreOp&gt; TYPE = LIRInstructionClass.create(VectorStoreOp.class);
251 
252         @Use({REG}) protected AllocatableValue input;
253 
254         public VectorStoreOp(AVXSize size, VexMoveOp op, AMD64AddressValue address, AllocatableValue input, LIRFrameState state) {
255             super(TYPE, size, op, address, state);
256             this.input = input;
257         }
258 
259         @Override
260         public void emitMemAccess(AMD64MacroAssembler masm) {
261             op.emit(masm, size, address.toAddress(), asRegister(input));
262         }
263     }
264 
265     @Opcode(&quot;SAVE_REGISTER&quot;)
266     public static class SaveRegistersOp extends AMD64SaveRegistersOp {
267         public static final LIRInstructionClass&lt;SaveRegistersOp&gt; TYPE = LIRInstructionClass.create(SaveRegistersOp.class);
268 
<span class="line-modified">269         public SaveRegistersOp(Register[] savedRegisters, AllocatableValue[] slots, boolean supportsRemove) {</span>
<span class="line-modified">270             super(TYPE, savedRegisters, slots, supportsRemove);</span>
271         }
272 
273         @Override
274         protected void saveRegister(CompilationResultBuilder crb, AMD64MacroAssembler masm, StackSlot result, Register register) {
275             AMD64Kind kind = (AMD64Kind) result.getPlatformKind();
276             if (kind.isXMM()) {
277                 VexMoveOp op;
278                 if (kind.getVectorLength() &gt; 1) {
279                     op = getVectorMoveOp(kind.getScalar());
280                 } else {
281                     op = getScalarMoveOp(kind);
282                 }
283 
284                 AMD64Address addr = (AMD64Address) crb.asAddress(result);
285                 op.emit(masm, AVXKind.getRegisterSize(kind), addr, register);
286             } else {
287                 super.saveRegister(crb, masm, result, register);
288             }
289         }
290     }
</pre>
<hr />
<pre>
317     }
318 
319     private static VexMoveOp getScalarMoveOp(AMD64Kind kind) {
320         switch (kind) {
321             case SINGLE:
322                 return VMOVSS;
323             case DOUBLE:
324                 return VMOVSD;
325             default:
326                 throw GraalError.shouldNotReachHere();
327         }
328     }
329 
330     private static VexMoveOp getVectorMoveOp(AMD64Kind kind) {
331         switch (kind) {
332             case SINGLE:
333                 return VMOVUPS;
334             case DOUBLE:
335                 return VMOVUPD;
336             default:
<span class="line-modified">337                 return VMOVDQU;</span>
338         }
339     }
340 
341     private static VexMoveOp getVectorMemMoveOp(AMD64Kind kind) {
342         switch (AVXKind.getDataSize(kind)) {
343             case DWORD:
344                 return VMOVD;
345             case QWORD:
346                 return VMOVQ;
347             default:
348                 return getVectorMoveOp(kind.getScalar());
349         }
350     }
351 
352     private static void move(CompilationResultBuilder crb, AMD64MacroAssembler masm, AllocatableValue result, Value input) {
353         VexMoveOp op;
354         AVXSize size;
355         AMD64Kind kind = (AMD64Kind) result.getPlatformKind();
356         if (kind.getVectorLength() &gt; 1) {
357             size = AVXKind.getRegisterSize(kind);
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2013, 2019, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 
 25 package org.graalvm.compiler.lir.amd64.vector;
 26 
 27 import static jdk.vm.ci.code.ValueUtil.asRegister;
 28 import static jdk.vm.ci.code.ValueUtil.isRegister;
 29 import static jdk.vm.ci.code.ValueUtil.isStackSlot;
 30 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp.VMOVD;
<span class="line-modified"> 31 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp.VMOVDQU32;</span>
 32 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp.VMOVQ;
 33 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp.VMOVSD;
 34 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp.VMOVSS;
 35 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp.VMOVUPD;
 36 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp.VMOVUPS;
 37 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.VexRVMOp.VXORPD;
 38 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.COMPOSITE;
 39 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.HINT;
 40 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.REG;
 41 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.STACK;
 42 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.UNINITIALIZED;
 43 
 44 import org.graalvm.compiler.asm.amd64.AMD64Address;
 45 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp;
 46 import org.graalvm.compiler.asm.amd64.AMD64MacroAssembler;
 47 import org.graalvm.compiler.asm.amd64.AVXKind;
 48 import org.graalvm.compiler.asm.amd64.AVXKind.AVXSize;
 49 import org.graalvm.compiler.debug.GraalError;
 50 import org.graalvm.compiler.lir.LIRFrameState;
 51 import org.graalvm.compiler.lir.LIRInstructionClass;
</pre>
<hr />
<pre>
 57 import org.graalvm.compiler.lir.amd64.AMD64Move;
 58 import org.graalvm.compiler.lir.amd64.AMD64RestoreRegistersOp;
 59 import org.graalvm.compiler.lir.amd64.AMD64SaveRegistersOp;
 60 import org.graalvm.compiler.lir.asm.CompilationResultBuilder;
 61 
 62 import jdk.vm.ci.amd64.AMD64Kind;
 63 import jdk.vm.ci.code.Register;
 64 import jdk.vm.ci.code.RegisterValue;
 65 import jdk.vm.ci.code.StackSlot;
 66 import jdk.vm.ci.meta.AllocatableValue;
 67 import jdk.vm.ci.meta.Constant;
 68 import jdk.vm.ci.meta.JavaConstant;
 69 import jdk.vm.ci.meta.Value;
 70 
 71 public class AMD64VectorMove {
 72 
 73     @Opcode(&quot;VMOVE&quot;)
 74     public static final class MoveToRegOp extends AMD64LIRInstruction implements ValueMoveOp {
 75         public static final LIRInstructionClass&lt;MoveToRegOp&gt; TYPE = LIRInstructionClass.create(MoveToRegOp.class);
 76 
<span class="line-modified"> 77         @Def({REG, STACK, HINT}) protected AllocatableValue result;</span>
 78         @Use({REG, STACK}) protected AllocatableValue input;
 79 
 80         public MoveToRegOp(AllocatableValue result, AllocatableValue input) {
 81             super(TYPE);
 82             this.result = result;
 83             this.input = input;
 84         }
 85 
 86         @Override
 87         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 88             move(crb, masm, result, input);
 89         }
 90 
 91         @Override
 92         public AllocatableValue getInput() {
 93             return input;
 94         }
 95 
 96         @Override
 97         public AllocatableValue getResult() {
</pre>
<hr />
<pre>
186         }
187 
188         @Override
189         public AllocatableValue getResult() {
190             return result;
191         }
192 
193         @Override
194         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
195             // backup scratch register
196             move(crb, masm, backupSlot, scratch.asValue(backupSlot.getValueKind()));
197             // move stack slot
198             move(crb, masm, scratch.asValue(getInput().getValueKind()), getInput());
199             move(crb, masm, getResult(), scratch.asValue(getResult().getValueKind()));
200             // restore scratch register
201             move(crb, masm, scratch.asValue(backupSlot.getValueKind()), backupSlot);
202 
203         }
204     }
205 
<span class="line-modified">206     public abstract static class VectorMemOp extends AMD64VectorInstruction {</span>
207 

208         protected final VexMoveOp op;
209 
210         @Use({COMPOSITE}) protected AMD64AddressValue address;
211         @State protected LIRFrameState state;
212 
213         protected VectorMemOp(LIRInstructionClass&lt;? extends VectorMemOp&gt; c, AVXSize size, VexMoveOp op, AMD64AddressValue address, LIRFrameState state) {
<span class="line-modified">214             super(c, size);</span>

215             this.op = op;
216             this.address = address;
217             this.state = state;
218         }
219 
220         protected abstract void emitMemAccess(AMD64MacroAssembler masm);
221 
222         @Override
223         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
224             if (state != null) {
225                 crb.recordImplicitException(masm.position(), state);
226             }
227             emitMemAccess(masm);
228         }
229     }
230 
231     public static final class VectorLoadOp extends VectorMemOp {
232         public static final LIRInstructionClass&lt;VectorLoadOp&gt; TYPE = LIRInstructionClass.create(VectorLoadOp.class);
233 
234         @Def({REG}) protected AllocatableValue result;
</pre>
<hr />
<pre>
247     public static class VectorStoreOp extends VectorMemOp {
248         public static final LIRInstructionClass&lt;VectorStoreOp&gt; TYPE = LIRInstructionClass.create(VectorStoreOp.class);
249 
250         @Use({REG}) protected AllocatableValue input;
251 
252         public VectorStoreOp(AVXSize size, VexMoveOp op, AMD64AddressValue address, AllocatableValue input, LIRFrameState state) {
253             super(TYPE, size, op, address, state);
254             this.input = input;
255         }
256 
257         @Override
258         public void emitMemAccess(AMD64MacroAssembler masm) {
259             op.emit(masm, size, address.toAddress(), asRegister(input));
260         }
261     }
262 
263     @Opcode(&quot;SAVE_REGISTER&quot;)
264     public static class SaveRegistersOp extends AMD64SaveRegistersOp {
265         public static final LIRInstructionClass&lt;SaveRegistersOp&gt; TYPE = LIRInstructionClass.create(SaveRegistersOp.class);
266 
<span class="line-modified">267         public SaveRegistersOp(Register[] savedRegisters, AllocatableValue[] slots) {</span>
<span class="line-modified">268             super(TYPE, savedRegisters, slots);</span>
269         }
270 
271         @Override
272         protected void saveRegister(CompilationResultBuilder crb, AMD64MacroAssembler masm, StackSlot result, Register register) {
273             AMD64Kind kind = (AMD64Kind) result.getPlatformKind();
274             if (kind.isXMM()) {
275                 VexMoveOp op;
276                 if (kind.getVectorLength() &gt; 1) {
277                     op = getVectorMoveOp(kind.getScalar());
278                 } else {
279                     op = getScalarMoveOp(kind);
280                 }
281 
282                 AMD64Address addr = (AMD64Address) crb.asAddress(result);
283                 op.emit(masm, AVXKind.getRegisterSize(kind), addr, register);
284             } else {
285                 super.saveRegister(crb, masm, result, register);
286             }
287         }
288     }
</pre>
<hr />
<pre>
315     }
316 
317     private static VexMoveOp getScalarMoveOp(AMD64Kind kind) {
318         switch (kind) {
319             case SINGLE:
320                 return VMOVSS;
321             case DOUBLE:
322                 return VMOVSD;
323             default:
324                 throw GraalError.shouldNotReachHere();
325         }
326     }
327 
328     private static VexMoveOp getVectorMoveOp(AMD64Kind kind) {
329         switch (kind) {
330             case SINGLE:
331                 return VMOVUPS;
332             case DOUBLE:
333                 return VMOVUPD;
334             default:
<span class="line-modified">335                 return VMOVDQU32;</span>
336         }
337     }
338 
339     private static VexMoveOp getVectorMemMoveOp(AMD64Kind kind) {
340         switch (AVXKind.getDataSize(kind)) {
341             case DWORD:
342                 return VMOVD;
343             case QWORD:
344                 return VMOVQ;
345             default:
346                 return getVectorMoveOp(kind.getScalar());
347         }
348     }
349 
350     private static void move(CompilationResultBuilder crb, AMD64MacroAssembler masm, AllocatableValue result, Value input) {
351         VexMoveOp op;
352         AVXSize size;
353         AMD64Kind kind = (AMD64Kind) result.getPlatformKind();
354         if (kind.getVectorLength() &gt; 1) {
355             size = AVXKind.getRegisterSize(kind);
</pre>
</td>
</tr>
</table>
<center><a href="AMD64VectorCompareOp.java.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../../../../index.html" target="_top">index</a> <a href="AMD64VectorUnary.java.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>