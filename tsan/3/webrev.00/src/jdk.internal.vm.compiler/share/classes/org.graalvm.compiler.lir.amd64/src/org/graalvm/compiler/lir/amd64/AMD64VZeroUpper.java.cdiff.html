<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Cdiff src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.amd64/src/org/graalvm/compiler/lir/amd64/AMD64VZeroUpper.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../style.css" />
  </head>
<body>
<center><a href="AMD64StringUTF16CompressOp.java.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../../../index.html" target="_top">index</a> <a href="AMD64ZapRegistersOp.java.cdiff.html" target="_top">next &gt;</a></center>    <h2>src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.amd64/src/org/graalvm/compiler/lir/amd64/AMD64VZeroUpper.java</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-old-header">*** 1,7 ***</span>
  /*
<span class="line-modified">!  * Copyright (c) 2016, 2018, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
<span class="line-new-header">--- 1,7 ---</span>
  /*
<span class="line-modified">!  * Copyright (c) 2016, 2019, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
</pre>
<hr />
<pre>
<span class="line-old-header">*** 25,51 ***</span>
  package org.graalvm.compiler.lir.amd64;
  
  import static jdk.vm.ci.code.ValueUtil.asRegister;
  import static jdk.vm.ci.code.ValueUtil.isRegister;
  
  import java.util.BitSet;
  
  import org.graalvm.compiler.asm.amd64.AMD64MacroAssembler;
  import org.graalvm.compiler.lir.LIRInstructionClass;
  import org.graalvm.compiler.lir.asm.CompilationResultBuilder;
  
  import jdk.vm.ci.amd64.AMD64;
  import jdk.vm.ci.code.Register;
  import jdk.vm.ci.code.RegisterValue;
  import jdk.vm.ci.meta.Value;
  
  public class AMD64VZeroUpper extends AMD64LIRInstruction {
  
      public static final LIRInstructionClass&lt;AMD64VZeroUpper&gt; TYPE = LIRInstructionClass.create(AMD64VZeroUpper.class);
  
      @Temp protected final RegisterValue[] xmmRegisters;
  
<span class="line-modified">!     public AMD64VZeroUpper(Value[] exclude) {</span>
          super(TYPE);
<span class="line-modified">!         xmmRegisters = initRegisterValues(exclude);</span>
      }
  
<span class="line-modified">!     private static RegisterValue[] initRegisterValues(Value[] exclude) {</span>
          BitSet skippedRegs = new BitSet();
<span class="line-removed">-         int numSkipped = 0;</span>
          if (exclude != null) {
              for (Value value : exclude) {
                  if (isRegister(value) &amp;&amp; asRegister(value).getRegisterCategory().equals(AMD64.XMM)) {
                      skippedRegs.set(asRegister(value).number);
<span class="line-removed">-                     numSkipped++;</span>
                  }
              }
          }
<span class="line-modified">!         RegisterValue[] regs = new RegisterValue[AMD64.xmmRegistersAVX512.length - numSkipped];</span>
<span class="line-modified">!         for (int i = 0, j = 0; i &lt; AMD64.xmmRegistersAVX512.length; i++) {</span>
<span class="line-modified">!             Register reg = AMD64.xmmRegistersAVX512[i];</span>
<span class="line-modified">!             if (!skippedRegs.get(reg.number)) {</span>
<span class="line-removed">-                 regs[j++] = reg.asValue();</span>
              }
          }
<span class="line-modified">!         return regs;</span>
      }
  
      @Override
      public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler asm) {
          asm.vzeroupper();
<span class="line-new-header">--- 25,94 ---</span>
  package org.graalvm.compiler.lir.amd64;
  
  import static jdk.vm.ci.code.ValueUtil.asRegister;
  import static jdk.vm.ci.code.ValueUtil.isRegister;
  
<span class="line-added">+ import java.util.ArrayList;</span>
  import java.util.BitSet;
  
  import org.graalvm.compiler.asm.amd64.AMD64MacroAssembler;
  import org.graalvm.compiler.lir.LIRInstructionClass;
<span class="line-added">+ import org.graalvm.compiler.lir.amd64.AMD64Call.ForeignCallOp;</span>
<span class="line-added">+ import org.graalvm.compiler.lir.amd64.vector.AMD64VectorInstruction;</span>
  import org.graalvm.compiler.lir.asm.CompilationResultBuilder;
  
  import jdk.vm.ci.amd64.AMD64;
  import jdk.vm.ci.code.Register;
<span class="line-added">+ import jdk.vm.ci.code.RegisterConfig;</span>
  import jdk.vm.ci.code.RegisterValue;
  import jdk.vm.ci.meta.Value;
  
<span class="line-added">+ /**</span>
<span class="line-added">+  * vzeroupper is essential to avoid performance penalty during SSE-AVX transition. Specifically,</span>
<span class="line-added">+  * once we have executed instructions that modify the upper bits (i.e., 128+) of the YMM registers,</span>
<span class="line-added">+  * we need to perform vzeroupper to transit the state to 128bits before executing any SSE</span>
<span class="line-added">+  * instructions. We don&#39;t need to place vzeroupper between VEX-encoded SSE instructions and legacy</span>
<span class="line-added">+  * SSE instructions, nor between AVX instructions and VEX-encoded SSE instructions.</span>
<span class="line-added">+  *</span>
<span class="line-added">+  * When running Graal on HotSpot, we emit a vzeroupper LIR operation (i.e. an instance of this</span>
<span class="line-added">+  * class) before a foreign call to the runtime function where Graal has no knowledge. The underlying</span>
<span class="line-added">+  * reason is that HotSpot is SSE-compiled so as to support older CPUs. We also emit a vzeroupper</span>
<span class="line-added">+  * instruction (see {@code AMD64HotSpotReturnOp.emitCode}) upon returning, if the current LIR graph</span>
<span class="line-added">+  * contains LIR operations that touch the upper bits of the YMM registers, including but not limited</span>
<span class="line-added">+  * to {@link AMD64VectorInstruction}, {@link AMD64ArrayCompareToOp}, {@link AMD64ArrayEqualsOp},</span>
<span class="line-added">+  * {@link AMD64ArrayIndexOfOp}, and {@link ForeignCallOp} that invokes to Graal-compiled stubs. For</span>
<span class="line-added">+  * the last case, since Graal-compiled stubs is under our control, we don&#39;t emit vzeroupper upon</span>
<span class="line-added">+  * returning of the stub, but rather do that upon returning of the current method.</span>
<span class="line-added">+  *</span>
<span class="line-added">+  * On JDK8, C2 does not emit many vzeroupper instructions, potentially because that YMM registers</span>
<span class="line-added">+  * are not heavily employed (C2 vectorization starts using YMM registers in 9, source</span>
<span class="line-added">+  * https://cr.openjdk.java.net/~vlivanov/talks/2017_Vectorization_in_HotSpot_JVM.pdf) and thus less</span>
<span class="line-added">+  * care has been taken to place these instructions. One example is that many intrinsics employ YMM</span>
<span class="line-added">+  * registers starting from https://bugs.openjdk.java.net/browse/JDK-8005419, but does not properly</span>
<span class="line-added">+  * place vzeroupper upon returning of the intrinsic stub or the caller of the stub.</span>
<span class="line-added">+  *</span>
<span class="line-added">+  * Most vzeroupper were added in JDK 10 (https://bugs.openjdk.java.net/browse/JDK-8178811), and was</span>
<span class="line-added">+  * later restricted on Haswell Xeon due to performance regression</span>
<span class="line-added">+  * (https://bugs.openjdk.java.net/browse/JDK-8190934). The actual condition for placing vzeroupper</span>
<span class="line-added">+  * is at http://hg.openjdk.java.net/jdk/jdk/file/c7d9df2e470c/src/hotspot/cpu/x86/x86_64.ad#l428. To</span>
<span class="line-added">+  * summarize, if nmethod employs YMM registers (or intrinsics which use them, search for</span>
<span class="line-added">+  * clear_upper_avx() in opto/library_call.cpp) vzeroupper will be generated on nmethod&#39;s exit and</span>
<span class="line-added">+  * before any calls in nmethod, because even compiled nmethods can still use only SSE instructions.</span>
<span class="line-added">+  *</span>
<span class="line-added">+  * This means, if a Java method performs a call to an intrinsic that employs YMM registers,</span>
<span class="line-added">+  * C2-compiled code will place a vzeroupper before the call, upon exit of the stub and upon exit of</span>
<span class="line-added">+  * this method. Graal will only place the last, because it ensures that Graal-compiled Java method</span>
<span class="line-added">+  * and stubs will be consistent on using VEX-encoding.</span>
<span class="line-added">+  *</span>
<span class="line-added">+  * In SubstrateVM, since the whole image is compiled consistently with or without VEX encoding (the</span>
<span class="line-added">+  * later is the default behavior, see {@code NativeImageGenerator.createTarget}), there is no need</span>
<span class="line-added">+  * for vzeroupper. For dynamic compilation on a SubstrateVM image, if the image is SSE-compiled, we</span>
<span class="line-added">+  * then need vzeroupper when returning from the dynamic compiled code to the pre-built image code.</span>
<span class="line-added">+  */</span>
  public class AMD64VZeroUpper extends AMD64LIRInstruction {
  
      public static final LIRInstructionClass&lt;AMD64VZeroUpper&gt; TYPE = LIRInstructionClass.create(AMD64VZeroUpper.class);
  
      @Temp protected final RegisterValue[] xmmRegisters;
  
<span class="line-modified">!     public AMD64VZeroUpper(Value[] exclude, RegisterConfig registerConfig) {</span>
          super(TYPE);
<span class="line-modified">!         xmmRegisters = initRegisterValues(exclude, registerConfig);</span>
      }
  
<span class="line-modified">!     private static RegisterValue[] initRegisterValues(Value[] exclude, RegisterConfig registerConfig) {</span>
          BitSet skippedRegs = new BitSet();
          if (exclude != null) {
              for (Value value : exclude) {
                  if (isRegister(value) &amp;&amp; asRegister(value).getRegisterCategory().equals(AMD64.XMM)) {
                      skippedRegs.set(asRegister(value).number);
                  }
              }
          }
<span class="line-modified">!         ArrayList&lt;RegisterValue&gt; regs = new ArrayList&lt;&gt;();</span>
<span class="line-modified">!         for (Register r : registerConfig.getCallerSaveRegisters()) {</span>
<span class="line-modified">!             if (r.getRegisterCategory().equals(AMD64.XMM) &amp;&amp; !skippedRegs.get(r.number)) {</span>
<span class="line-modified">!                 regs.add(r.asValue());</span>
              }
          }
<span class="line-modified">!         return regs.toArray(new RegisterValue[regs.size()]);</span>
      }
  
      @Override
      public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler asm) {
          asm.vzeroupper();
</pre>
<center><a href="AMD64StringUTF16CompressOp.java.cdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../../../index.html" target="_top">index</a> <a href="AMD64ZapRegistersOp.java.cdiff.html" target="_top">next &gt;</a></center>  </body>
</html>