<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.amd64/src/org/graalvm/compiler/lir/amd64/AMD64StringUTF16CompressOp.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2018, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 
 25 package org.graalvm.compiler.lir.amd64;
 26 
 27 import static jdk.vm.ci.amd64.AMD64.k1;
 28 import static jdk.vm.ci.amd64.AMD64.k2;
 29 import static jdk.vm.ci.amd64.AMD64.k3;
 30 import static jdk.vm.ci.amd64.AMD64.rax;
 31 import static jdk.vm.ci.amd64.AMD64.rdi;
 32 import static jdk.vm.ci.amd64.AMD64.rdx;
 33 import static jdk.vm.ci.amd64.AMD64.rsi;
 34 import static jdk.vm.ci.amd64.AMD64.rsp;
 35 import static jdk.vm.ci.code.ValueUtil.asRegister;
 36 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.REG;
 37 import static org.graalvm.compiler.lir.amd64.AMD64StringLatin1InflateOp.useAVX512ForStringInflateCompress;
 38 
 39 import org.graalvm.compiler.asm.Label;
 40 import org.graalvm.compiler.asm.amd64.AMD64Address;
 41 import org.graalvm.compiler.asm.amd64.AMD64Assembler;
 42 import org.graalvm.compiler.asm.amd64.AMD64MacroAssembler;
 43 import org.graalvm.compiler.core.common.LIRKind;
 44 import org.graalvm.compiler.lir.LIRInstructionClass;
 45 import org.graalvm.compiler.lir.Opcode;
 46 import org.graalvm.compiler.lir.asm.CompilationResultBuilder;
 47 import org.graalvm.compiler.lir.gen.LIRGeneratorTool;
 48 
 49 import jdk.vm.ci.amd64.AMD64;
 50 import jdk.vm.ci.amd64.AMD64Kind;
 51 import jdk.vm.ci.code.Register;
 52 import jdk.vm.ci.meta.Value;
 53 
 54 @Opcode(&quot;AMD64_STRING_COMPRESS&quot;)
 55 public final class AMD64StringUTF16CompressOp extends AMD64LIRInstruction {
 56     public static final LIRInstructionClass&lt;AMD64StringUTF16CompressOp&gt; TYPE = LIRInstructionClass.create(AMD64StringUTF16CompressOp.class);
 57 
 58     @Def({REG}) private Value rres;
 59     @Use({REG}) private Value rsrc;
 60     @Use({REG}) private Value rdst;
 61     @Use({REG}) private Value rlen;
 62 
 63     @Temp({REG}) private Value rsrcTemp;
 64     @Temp({REG}) private Value rdstTemp;
 65     @Temp({REG}) private Value rlenTemp;
 66 
 67     @Temp({REG}) private Value vtmp1;
 68     @Temp({REG}) private Value vtmp2;
 69     @Temp({REG}) private Value vtmp3;
 70     @Temp({REG}) private Value vtmp4;
 71     @Temp({REG}) private Value rtmp5;
 72 
 73     public AMD64StringUTF16CompressOp(LIRGeneratorTool tool, Value res, Value src, Value dst, Value len) {
 74         super(TYPE);
 75 
 76         assert asRegister(src).equals(rsi);
 77         assert asRegister(dst).equals(rdi);
 78         assert asRegister(len).equals(rdx);
 79         assert asRegister(res).equals(rax);
 80 
 81         rres = res;
 82         rsrcTemp = rsrc = src;
 83         rdstTemp = rdst = dst;
 84         rlenTemp = rlen = len;
 85 
 86         LIRKind vkind = useAVX512ForStringInflateCompress(tool.target()) ? LIRKind.value(AMD64Kind.V512_BYTE) : LIRKind.value(AMD64Kind.V128_BYTE);
 87 
 88         vtmp1 = tool.newVariable(vkind);
 89         vtmp2 = tool.newVariable(vkind);
 90         vtmp3 = tool.newVariable(vkind);
 91         vtmp4 = tool.newVariable(vkind);
 92 
 93         rtmp5 = tool.newVariable(LIRKind.value(AMD64Kind.DWORD));
 94     }
 95 
 96     @Override
 97     public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 98         Register res = asRegister(rres);
 99         Register src = asRegister(rsrc);
100         Register dst = asRegister(rdst);
101         Register len = asRegister(rlen);
102 
103         Register tmp1 = asRegister(vtmp1);
104         Register tmp2 = asRegister(vtmp2);
105         Register tmp3 = asRegister(vtmp3);
106         Register tmp4 = asRegister(vtmp4);
107         Register tmp5 = asRegister(rtmp5);
108 
109         charArrayCompress(masm, src, dst, len, tmp1, tmp2, tmp3, tmp4, tmp5, res);
110     }
111 
112     /**
113      * Compress a UTF16 string which de facto is a Latin1 string into a byte array representation
114      * (buffer).
115      *
116      * @param masm the assembler
117      * @param src (rsi) the start address of source char[] to be compressed
118      * @param dst (rdi) the start address of destination byte[] vector
119      * @param len (rdx) the length
120      * @param tmp1 (xmm) temporary xmm register
121      * @param tmp2 (xmm) temporary xmm register
122      * @param tmp3 (xmm) temporary xmm register
123      * @param tmp4 (xmm) temporary xmm register
124      * @param tmp (gpr) temporary gpr register
125      * @param res (rax) the result code (length on success, zero otherwise)
126      */
127     private static void charArrayCompress(AMD64MacroAssembler masm, Register src, Register dst, Register len, Register tmp1,
128                     Register tmp2, Register tmp3, Register tmp4, Register tmp, Register res) {
129         assert tmp1.getRegisterCategory().equals(AMD64.XMM);
130         assert tmp2.getRegisterCategory().equals(AMD64.XMM);
131         assert tmp3.getRegisterCategory().equals(AMD64.XMM);
132         assert tmp4.getRegisterCategory().equals(AMD64.XMM);
133 
134         Label labelReturnLength = new Label();
135         Label labelReturnZero = new Label();
136         Label labelDone = new Label();
137         Label labelBelowThreshold = new Label();
138 
139         assert len.number != res.number;
140 
141         masm.push(len);      // Save length for return.
142 
143         if (useAVX512ForStringInflateCompress(masm.target)) {
144             Label labelRestoreK1ReturnZero = new Label();
145             Label labelAvxPostAlignment = new Label();
146 
147             // If the length of the string is less than 32, we chose not to use the
148             // AVX512 instructions.
149             masm.testl(len, -32);
150             masm.jcc(AMD64Assembler.ConditionFlag.Zero, labelBelowThreshold);
151 
152             // First check whether a character is compressible (&lt;= 0xff).
153             // Create mask to test for Unicode chars inside (zmm) vector.
154             masm.movl(res, 0x00ff);
155             masm.evpbroadcastw(tmp2, res);
156 
157             masm.kmovq(k3, k1);      // Save k1
158 
159             masm.testl(len, -64);
160             masm.jcc(AMD64Assembler.ConditionFlag.Zero, labelAvxPostAlignment);
161 
162             masm.movl(tmp, dst);
163             masm.andl(tmp, (32 - 1));
164             masm.negl(tmp);
165             masm.andl(tmp, (32 - 1));
166 
167             // bail out when there is nothing to be done
168             masm.testl(tmp, tmp);
169             masm.jcc(AMD64Assembler.ConditionFlag.Zero, labelAvxPostAlignment);
170 
171             // Compute (1 &lt;&lt; N) - 1 = ~(~0 &lt;&lt; N), where N is the remaining number
172             // of characters to process.
173             masm.movl(res, -1);
174             masm.shlxl(res, res, tmp);
175             masm.notl(res);
176 
177             masm.kmovd(k1, res);
178             masm.evmovdqu16(tmp1, k1, new AMD64Address(src));
179             masm.evpcmpuw(k2, k1, tmp1, tmp2, 2 /* le */);
180             masm.ktestd(k2, k1);
181             masm.jcc(AMD64Assembler.ConditionFlag.CarryClear, labelRestoreK1ReturnZero);
182 
183             masm.evpmovwb(new AMD64Address(dst), k1, tmp1);
184 
185             masm.addq(src, tmp);
186             masm.addq(src, tmp);
187             masm.addq(dst, tmp);
188             masm.subl(len, tmp);
189 
190             masm.bind(labelAvxPostAlignment);
191             // end of alignment
192             Label labelAvx512LoopTail = new Label();
193 
194             masm.movl(tmp, len);
195             masm.andl(tmp, -32);         // The vector count (in chars).
196             masm.jcc(AMD64Assembler.ConditionFlag.Zero, labelAvx512LoopTail);
197             masm.andl(len, 32 - 1);      // The tail count (in chars).
198 
199             masm.leaq(src, new AMD64Address(src, tmp, AMD64Address.Scale.Times2));
200             masm.leaq(dst, new AMD64Address(dst, tmp, AMD64Address.Scale.Times1));
201             masm.negq(tmp);
202 
203             Label labelAvx512Loop = new Label();
204             // Test and compress 32 chars per iteration, reading 512-bit vectors and
205             // writing 256-bit compressed ditto.
206             masm.bind(labelAvx512Loop);
207             masm.evmovdqu16(tmp1, new AMD64Address(src, tmp, AMD64Address.Scale.Times2));
208             masm.evpcmpuw(k2, tmp1, tmp2, 2 /* le */);
209             masm.kortestd(k2, k2);
210             masm.jcc(AMD64Assembler.ConditionFlag.CarryClear, labelRestoreK1ReturnZero);
211 
212             // All 32 chars in the current vector (chunk) are valid for compression,
213             // write truncated byte elements to memory.
214             masm.evpmovwb(new AMD64Address(dst, tmp, AMD64Address.Scale.Times1), tmp1);
215             masm.addq(tmp, 32);
216             masm.jcc(AMD64Assembler.ConditionFlag.NotZero, labelAvx512Loop);
217 
218             masm.bind(labelAvx512LoopTail);
219             masm.kmovq(k1, k3);      // Restore k1
220 
221             // All done if the tail count is zero.
222             masm.testl(len, len);
223             masm.jcc(AMD64Assembler.ConditionFlag.Zero, labelReturnLength);
224 
225             // Compute (1 &lt;&lt; N) - 1 = ~(~0 &lt;&lt; N), where N is the remaining number
226             // of characters to process.
227             masm.movl(res, -1);
228             masm.shlxl(res, res, len);
229             masm.notl(res);
230 
231             masm.kmovd(k1, res);
232             masm.evmovdqu16(tmp1, k1, new AMD64Address(src));
233             masm.evpcmpuw(k2, k1, tmp1, tmp2, 2 /* le */);
234             masm.ktestd(k2, k1);
235             masm.jcc(AMD64Assembler.ConditionFlag.CarryClear, labelRestoreK1ReturnZero);
236 
237             masm.evpmovwb(new AMD64Address(dst), k1, tmp1);
238 
239             masm.kmovq(k1, k3);      // Restore k1
240             masm.jmp(labelReturnLength);
241 
242             masm.bind(labelRestoreK1ReturnZero);
243             masm.kmovq(k1, k3);      // Restore k1
244             masm.jmp(labelReturnZero);
245         }
246 
247         if (masm.supports(AMD64.CPUFeature.SSE4_2)) {
248 
249             Label labelSSETail = new Label();
250 
251             masm.bind(labelBelowThreshold);
252 
253             masm.movl(tmp, 0xff00ff00);  // Create mask to test for Unicode chars in vectors.
254 
255             masm.movl(res, len);
256             masm.andl(res, -16);
257             masm.jccb(AMD64Assembler.ConditionFlag.Zero, labelSSETail);
258             masm.andl(len, 16 - 1);
259 
260             // Compress 16 chars per iteration.
261             masm.movdl(tmp1, tmp);
262             masm.pshufd(tmp1, tmp1, 0);    // Store Unicode mask in &#39;vtmp1&#39;.
263             masm.pxor(tmp4, tmp4);
264 
265             masm.leaq(src, new AMD64Address(src, res, AMD64Address.Scale.Times2));
266             masm.leaq(dst, new AMD64Address(dst, res, AMD64Address.Scale.Times1));
267             masm.negq(res);
268 
269             Label lSSELoop = new Label();
270             // Test and compress 16 chars per iteration, reading 128-bit vectors and
271             // writing 64-bit compressed ditto.
272             masm.bind(lSSELoop);
273             masm.movdqu(tmp2, new AMD64Address(src, res, AMD64Address.Scale.Times2));     // load
274                                                                                           // 1st 8
275                                                                                           // characters
276             masm.movdqu(tmp3, new AMD64Address(src, res, AMD64Address.Scale.Times2, 16)); // load
277                                                                                           // next 8
278                                                                                           // characters
279             masm.por(tmp4, tmp2);
280             masm.por(tmp4, tmp3);
281             masm.ptest(tmp4, tmp1);        // Check for Unicode chars in vector.
282             masm.jcc(AMD64Assembler.ConditionFlag.NotZero, labelReturnZero);
283 
284             masm.packuswb(tmp2, tmp3);     // Only ASCII chars; compress each to a byte.
285             masm.movdqu(new AMD64Address(dst, res, AMD64Address.Scale.Times1), tmp2);
286             masm.addq(res, 16);
287             masm.jcc(AMD64Assembler.ConditionFlag.NotZero, lSSELoop);
288 
289             Label labelCopyChars = new Label();
290             // Test and compress another 8 chars before final tail copy.
291             masm.bind(labelSSETail);
292             masm.movl(res, len);
293             masm.andl(res, -8);
294             masm.jccb(AMD64Assembler.ConditionFlag.Zero, labelCopyChars);
295             masm.andl(len, 8 - 1);
296 
297             masm.movdl(tmp1, tmp);
298             masm.pshufd(tmp1, tmp1, 0);    // Store Unicode mask in &#39;vtmp1&#39;.
299             masm.pxor(tmp3, tmp3);
300 
301             masm.movdqu(tmp2, new AMD64Address(src));
302             masm.ptest(tmp2, tmp1);        // Check for Unicode chars in vector.
303             masm.jccb(AMD64Assembler.ConditionFlag.NotZero, labelReturnZero);
304             masm.packuswb(tmp2, tmp3);     // Only ASCII chars; compress each to a byte.
305             masm.movq(new AMD64Address(dst), tmp2);
306             masm.addq(src, 16);
307             masm.addq(dst, 8);
308 
309             masm.bind(labelCopyChars);
310         }
311 
312         // Compress any remaining characters using a vanilla implementation.
313         masm.testl(len, len);
314         masm.jccb(AMD64Assembler.ConditionFlag.Zero, labelReturnLength);
315         masm.leaq(src, new AMD64Address(src, len, AMD64Address.Scale.Times2));
316         masm.leaq(dst, new AMD64Address(dst, len, AMD64Address.Scale.Times1));
317         masm.negq(len);
318 
319         Label labelCopyCharsLoop = new Label();
320         // Compress a single character per iteration.
321         masm.bind(labelCopyCharsLoop);
322         masm.movzwl(res, new AMD64Address(src, len, AMD64Address.Scale.Times2));
323         masm.testl(res, 0xff00);     // Check if Unicode character.
324         masm.jccb(AMD64Assembler.ConditionFlag.NotZero, labelReturnZero);
325         // An ASCII character; compress to a byte.
326         masm.movb(new AMD64Address(dst, len, AMD64Address.Scale.Times1), res);
327         masm.incrementq(len, 1);
328         masm.jcc(AMD64Assembler.ConditionFlag.NotZero, labelCopyCharsLoop);
329 
330         // If compression succeeded, return the length.
331         masm.bind(labelReturnLength);
332         masm.pop(res);
333         masm.jmpb(labelDone);
334 
335         // If compression failed, return 0.
336         masm.bind(labelReturnZero);
337         masm.xorl(res, res);
338         masm.addq(rsp, 8 /* wordSize */);
339 
340         masm.bind(labelDone);
341     }
342 
343     @Override
344     public boolean needsClearUpperVectorRegisters() {
345         return true;
346     }
347 }
    </pre>
  </body>
</html>