<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.aarch64/src/org/graalvm/compiler/lir/aarch64/AArch64ZeroMemoryOp.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2019, Oracle and/or its affiliates. All rights reserved.
  3  * Copyright (c) 2019, Arm Limited and affiliates. All rights reserved.
  4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  5  *
  6  * This code is free software; you can redistribute it and/or modify it
  7  * under the terms of the GNU General Public License version 2 only, as
  8  * published by the Free Software Foundation.
  9  *
 10  * This code is distributed in the hope that it will be useful, but WITHOUT
 11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 13  * version 2 for more details (a copy is included in the LICENSE file that
 14  * accompanied this code).
 15  *
 16  * You should have received a copy of the GNU General Public License version
 17  * 2 along with this work; if not, write to the Free Software Foundation,
 18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 19  *
 20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 21  * or visit www.oracle.com if you need additional information or have any
 22  * questions.
 23  */
 24 
 25 
 26 package org.graalvm.compiler.lir.aarch64;
 27 
 28 import static jdk.vm.ci.aarch64.AArch64.zr;
 29 import static jdk.vm.ci.code.ValueUtil.asRegister;
 30 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.REG;
 31 
 32 import org.graalvm.compiler.asm.Label;
 33 import org.graalvm.compiler.asm.aarch64.AArch64Address;
 34 import org.graalvm.compiler.asm.aarch64.AArch64Assembler;
 35 import org.graalvm.compiler.asm.aarch64.AArch64Assembler.ConditionFlag;
 36 import org.graalvm.compiler.asm.aarch64.AArch64MacroAssembler;
 37 import org.graalvm.compiler.lir.LIRInstructionClass;
 38 import org.graalvm.compiler.lir.Opcode;
 39 import org.graalvm.compiler.lir.asm.CompilationResultBuilder;
 40 
 41 import jdk.vm.ci.code.CodeUtil;
 42 import jdk.vm.ci.code.Register;
 43 import jdk.vm.ci.meta.Value;
 44 
 45 /**
 46  * Zero a chunk of memory on AArch64.
 47  */
 48 @Opcode(&quot;ZERO_MEMORY&quot;)
 49 public final class AArch64ZeroMemoryOp extends AArch64LIRInstruction {
 50     public static final LIRInstructionClass&lt;AArch64ZeroMemoryOp&gt; TYPE = LIRInstructionClass.create(AArch64ZeroMemoryOp.class);
 51 
 52     @Use({REG}) protected Value addressValue;
 53     @Use({REG}) protected Value lengthValue;
 54 
 55     @Temp({REG}) protected Value addressValueTemp;
 56     @Temp({REG}) protected Value lengthValueTemp;
 57 
 58     private final boolean isAligned;
 59     private final boolean useDcZva;
 60     private final int zvaLength;
 61 
 62     /**
 63      * Constructor of AArch64ZeroMemoryOp.
 64      *
 65      * @param address starting address of the memory chunk to be zeroed.
 66      * @param length size of the memory chunk to be zeroed, in bytes.
 67      * @param isAligned whether both address and size are aligned to 8 bytes.
 68      * @param useDcZva is DC ZVA instruction is able to use.
 69      * @param zvaLength the ZVA length info of current AArch64 CPU, negative value indicates length
 70      *            is unknown at compile time.
 71      */
 72     public AArch64ZeroMemoryOp(Value address, Value length, boolean isAligned, boolean useDcZva, int zvaLength) {
 73         super(TYPE);
 74         this.addressValue = address;
 75         this.lengthValue = length;
 76         this.addressValueTemp = address;
 77         this.lengthValueTemp = length;
 78         this.useDcZva = useDcZva;
 79         this.zvaLength = zvaLength;
 80         this.isAligned = isAligned;
 81     }
 82 
 83     @Override
 84     protected void emitCode(CompilationResultBuilder crb, AArch64MacroAssembler masm) {
 85         Register base = asRegister(addressValue);
 86         Register size = asRegister(lengthValue);
 87 
 88         try (AArch64MacroAssembler.ScratchRegister scratchRegister = masm.getScratchRegister()) {
 89             Register alignmentBits = scratchRegister.getRegister();
 90 
 91             Label tail = new Label();
 92             Label done = new Label();
 93 
 94             // Jump to DONE if size is zero.
 95             masm.cbz(64, size, done);
 96 
 97             if (!isAligned) {
 98                 Label baseAlignedTo2Bytes = new Label();
 99                 Label baseAlignedTo4Bytes = new Label();
100                 Label baseAlignedTo8Bytes = new Label();
101 
102                 // Jump to per-byte zeroing loop if the zeroing size is less than 8
103                 masm.cmp(64, size, 8);
104                 masm.branchConditionally(ConditionFlag.LT, tail);
105 
106                 // Make base 8-byte aligned
107                 masm.neg(64, alignmentBits, base);
108                 masm.and(64, alignmentBits, alignmentBits, 7);
109 
110                 masm.tbz(alignmentBits, 0, baseAlignedTo2Bytes);
111                 masm.sub(64, size, size, 1);
112                 masm.str(8, zr, AArch64Address.createPostIndexedImmediateAddress(base, 1));
113                 masm.bind(baseAlignedTo2Bytes);
114 
115                 masm.tbz(alignmentBits, 1, baseAlignedTo4Bytes);
116                 masm.sub(64, size, size, 2);
117                 masm.str(16, zr, AArch64Address.createPostIndexedImmediateAddress(base, 2));
118                 masm.bind(baseAlignedTo4Bytes);
119 
120                 masm.tbz(alignmentBits, 2, baseAlignedTo8Bytes);
121                 masm.sub(64, size, size, 4);
122                 masm.str(32, zr, AArch64Address.createPostIndexedImmediateAddress(base, 4));
123                 masm.bind(baseAlignedTo8Bytes);
124                 // At this point base is 8-byte aligned.
125             }
126 
127             if (useDcZva &amp;&amp; zvaLength &gt; 0) {
128                 // From ARMv8-A architecture reference manual D12.2.35 Data Cache Zero ID register:
129                 // A valid ZVA length should be a power-of-2 value in [4, 2048]
130                 assert (CodeUtil.isPowerOf2(zvaLength) &amp;&amp; 4 &lt;= zvaLength &amp;&amp; zvaLength &lt;= 2048);
131 
132                 Label preCheck = new Label();
133                 Label preLoop = new Label();
134                 Label mainCheck = new Label();
135                 Label mainLoop = new Label();
136                 Label postCheck = new Label();
137                 Label postLoop = new Label();
138 
139                 masm.neg(64, alignmentBits, base);
140                 masm.and(64, alignmentBits, alignmentBits, zvaLength - 1);
141 
142                 // Is size less than number of bytes to be pre-zeroed? Jump to post check if so.
143                 masm.cmp(64, size, alignmentBits);
144                 masm.branchConditionally(AArch64Assembler.ConditionFlag.LE, postCheck);
145                 masm.sub(64, size, size, alignmentBits);
146 
147                 // Pre loop: align base according to the supported bulk zeroing stride.
148                 masm.jmp(preCheck);
149 
150                 masm.align(crb.target.wordSize * 2);
151                 masm.bind(preLoop);
152                 masm.str(64, zr, AArch64Address.createPostIndexedImmediateAddress(base, 8));
153                 masm.bind(preCheck);
154                 masm.subs(64, alignmentBits, alignmentBits, 8);
155                 masm.branchConditionally(AArch64Assembler.ConditionFlag.GE, preLoop);
156 
157                 // Main loop: bulk zeroing
158                 masm.jmp(mainCheck);
159 
160                 masm.align(crb.target.wordSize * 2);
161                 masm.bind(mainLoop);
162                 masm.dc(AArch64Assembler.DataCacheOperationType.ZVA, base);
163                 masm.add(64, base, base, zvaLength);
164                 masm.bind(mainCheck);
165                 masm.subs(64, size, size, zvaLength);
166                 masm.branchConditionally(AArch64Assembler.ConditionFlag.GE, mainLoop);
167 
168                 masm.add(64, size, size, zvaLength);
169 
170                 // Post loop: handle bytes after the main loop
171                 masm.jmp(postCheck);
172 
173                 masm.align(crb.target.wordSize * 2);
174                 masm.bind(postLoop);
175                 masm.str(64, zr, AArch64Address.createPostIndexedImmediateAddress(base, 8));
176                 masm.bind(postCheck);
177                 masm.subs(64, size, size, 8);
178                 masm.branchConditionally(AArch64Assembler.ConditionFlag.GE, postLoop);
179 
180                 if (!isAligned) {
181                     // Restore size for tail zeroing
182                     masm.add(64, size, size, 8);
183                 }
184             } else {
185                 Label mainCheck = new Label();
186                 Label mainLoop = new Label();
187 
188                 if (!isAligned) {
189                     // After aligning base, we may have size less than 8. Need to check again.
190                     masm.cmp(64, size, 8);
191                     masm.branchConditionally(ConditionFlag.LT, tail);
192                 }
193 
194                 masm.tbz(base, 3, mainCheck);
195                 masm.sub(64, size, size, 8);
196                 masm.str(64, zr, AArch64Address.createPostIndexedImmediateAddress(base, 8));
197                 masm.jmp(mainCheck);
198 
199                 // The STP loop that zeros 16 bytes in each iteration.
200                 masm.align(crb.target.wordSize * 2);
201                 masm.bind(mainLoop);
202                 masm.stp(64, zr, zr, AArch64Address.createPostIndexedImmediateAddress(base, 2));
203                 masm.bind(mainCheck);
204                 masm.subs(64, size, size, 16);
205                 masm.branchConditionally(AArch64Assembler.ConditionFlag.GE, mainLoop);
206 
207                 // We may need to zero the tail 8 bytes of the memory chunk.
208                 masm.add(64, size, size, 16);
209                 masm.tbz(size, 3, tail);
210                 masm.str(64, zr, AArch64Address.createPostIndexedImmediateAddress(base, 8));
211 
212                 if (!isAligned) {
213                     // Adjust size for tail zeroing
214                     masm.sub(64, size, size, 8);
215                 }
216             }
217 
218             masm.bind(tail);
219             if (!isAligned) {
220                 Label perByteZeroingLoop = new Label();
221 
222                 masm.cbz(64, size, done);
223                 // We have to ensure size &gt; 0 when entering the following loop
224                 masm.align(crb.target.wordSize * 2);
225                 masm.bind(perByteZeroingLoop);
226                 masm.str(8, zr, AArch64Address.createPostIndexedImmediateAddress(base, 1));
227                 masm.subs(64, size, size, 1);
228                 masm.branchConditionally(AArch64Assembler.ConditionFlag.NE, perByteZeroingLoop);
229             }
230             masm.bind(done);
231         }
232     }
233 
234 }
    </pre>
  </body>
</html>