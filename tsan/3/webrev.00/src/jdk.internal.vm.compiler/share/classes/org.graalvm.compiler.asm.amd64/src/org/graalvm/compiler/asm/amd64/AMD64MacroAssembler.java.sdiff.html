<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.asm.amd64/src/org/graalvm/compiler/asm/amd64/AMD64MacroAssembler.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../style.css" />
  </head>
<body>
<center><a href="AMD64BaseAssembler.java.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../../../index.html" target="_top">index</a> <a href="AVXKind.java.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.asm.amd64/src/org/graalvm/compiler/asm/amd64/AMD64MacroAssembler.java</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2009, 2018, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 
 25 package org.graalvm.compiler.asm.amd64;
 26 
<span class="line-removed"> 27 import static jdk.vm.ci.amd64.AMD64.rbp;</span>
<span class="line-removed"> 28 import static jdk.vm.ci.amd64.AMD64.rsp;</span>
 29 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseIncDec;
 30 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseXmmLoadAndClearUpper;
 31 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseXmmRegToRegMoveAll;
 32 

 33 import org.graalvm.compiler.core.common.NumUtil;
 34 
 35 import jdk.vm.ci.amd64.AMD64;
 36 import jdk.vm.ci.amd64.AMD64Kind;
 37 import jdk.vm.ci.code.Register;
 38 import jdk.vm.ci.code.TargetDescription;
 39 
 40 /**
 41  * This class implements commonly used X86 code patterns.
 42  */
 43 public class AMD64MacroAssembler extends AMD64Assembler {
 44 
 45     public AMD64MacroAssembler(TargetDescription target) {
 46         super(target);
 47     }
 48 
 49     public final void decrementq(Register reg, int value) {
 50         if (value == Integer.MIN_VALUE) {
 51             subq(reg, value);
 52             return;
</pre>
<hr />
<pre>
 67 
 68     public final void decrementq(AMD64Address dst, int value) {
 69         if (value == Integer.MIN_VALUE) {
 70             subq(dst, value);
 71             return;
 72         }
 73         if (value &lt; 0) {
 74             incrementq(dst, -value);
 75             return;
 76         }
 77         if (value == 0) {
 78             return;
 79         }
 80         if (value == 1 &amp;&amp; UseIncDec) {
 81             decq(dst);
 82         } else {
 83             subq(dst, value);
 84         }
 85     }
 86 
<span class="line-removed"> 87     public final void enter(int frameSize) {</span>
<span class="line-removed"> 88         if (NumUtil.isUShort(frameSize)) {</span>
<span class="line-removed"> 89             // Can use enter instruction only for frame size that fits in 16 bits.</span>
<span class="line-removed"> 90             emitByte(0xC8);</span>
<span class="line-removed"> 91             emitShort(frameSize);</span>
<span class="line-removed"> 92             emitByte(0x00);</span>
<span class="line-removed"> 93         } else {</span>
<span class="line-removed"> 94             // Fall back to manual sequence.</span>
<span class="line-removed"> 95             push(rbp);</span>
<span class="line-removed"> 96             movq(rbp, rsp);</span>
<span class="line-removed"> 97             decrementq(rsp, frameSize);</span>
<span class="line-removed"> 98         }</span>
<span class="line-removed"> 99     }</span>
<span class="line-removed">100 </span>
101     public void incrementq(Register reg, int value) {
102         if (value == Integer.MIN_VALUE) {
103             addq(reg, value);
104             return;
105         }
106         if (value &lt; 0) {
107             decrementq(reg, -value);
108             return;
109         }
110         if (value == 0) {
111             return;
112         }
113         if (value == 1 &amp;&amp; UseIncDec) {
114             incq(reg);
115         } else {
116             addq(reg, value);
117         }
118     }
119 
120     public final void incrementq(AMD64Address dst, int value) {
</pre>
<hr />
<pre>
222             addl(dst, value);
223             return;
224         }
225         if (value &lt; 0) {
226             decrementl(dst, -value);
227             return;
228         }
229         if (value == 0) {
230             return;
231         }
232         if (value == 1 &amp;&amp; UseIncDec) {
233             incl(dst);
234         } else {
235             addl(dst, value);
236         }
237     }
238 
239     public void movflt(Register dst, Register src) {
240         assert dst.getRegisterCategory().equals(AMD64.XMM) &amp;&amp; src.getRegisterCategory().equals(AMD64.XMM);
241         if (UseXmmRegToRegMoveAll) {
<span class="line-modified">242             movaps(dst, src);</span>




243         } else {
<span class="line-modified">244             movss(dst, src);</span>




245         }
246     }
247 
248     public void movflt(Register dst, AMD64Address src) {
249         assert dst.getRegisterCategory().equals(AMD64.XMM);
<span class="line-modified">250         movss(dst, src);</span>




251     }
252 
253     public void movflt(AMD64Address dst, Register src) {
254         assert src.getRegisterCategory().equals(AMD64.XMM);
<span class="line-modified">255         movss(dst, src);</span>




256     }
257 
258     public void movdbl(Register dst, Register src) {
259         assert dst.getRegisterCategory().equals(AMD64.XMM) &amp;&amp; src.getRegisterCategory().equals(AMD64.XMM);
260         if (UseXmmRegToRegMoveAll) {
<span class="line-modified">261             movapd(dst, src);</span>




262         } else {
<span class="line-modified">263             movsd(dst, src);</span>




264         }
265     }
266 
267     public void movdbl(Register dst, AMD64Address src) {
268         assert dst.getRegisterCategory().equals(AMD64.XMM);
269         if (UseXmmLoadAndClearUpper) {
<span class="line-modified">270             movsd(dst, src);</span>




271         } else {

272             movlpd(dst, src);
273         }
274     }
275 
276     public void movdbl(AMD64Address dst, Register src) {
277         assert src.getRegisterCategory().equals(AMD64.XMM);
<span class="line-modified">278         movsd(dst, src);</span>




279     }
280 
281     /**
282      * Non-atomic write of a 64-bit constant to memory. Do not use if the address might be a
283      * volatile field!
284      */
285     public final void movlong(AMD64Address dst, long src) {
286         if (NumUtil.isInt(src)) {
287             AMD64MIOp.MOV.emit(this, OperandSize.QWORD, dst, (int) src);
288         } else {
289             AMD64Address high = new AMD64Address(dst.getBase(), dst.getIndex(), dst.getScale(), dst.getDisplacement() + 4);
290             movl(dst, (int) (src &amp; 0xFFFFFFFF));
291             movl(high, (int) (src &gt;&gt; 32));
292         }
293     }
294 
295     public final void setl(ConditionFlag cc, Register dst) {
296         setb(cc, dst);
297         movzbl(dst, dst);
298     }
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2009, 2019, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 
 25 package org.graalvm.compiler.asm.amd64;
 26 


 27 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseIncDec;
 28 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseXmmLoadAndClearUpper;
 29 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseXmmRegToRegMoveAll;
 30 
<span class="line-added"> 31 import org.graalvm.compiler.asm.amd64.AVXKind.AVXSize;</span>
 32 import org.graalvm.compiler.core.common.NumUtil;
 33 
 34 import jdk.vm.ci.amd64.AMD64;
 35 import jdk.vm.ci.amd64.AMD64Kind;
 36 import jdk.vm.ci.code.Register;
 37 import jdk.vm.ci.code.TargetDescription;
 38 
 39 /**
 40  * This class implements commonly used X86 code patterns.
 41  */
 42 public class AMD64MacroAssembler extends AMD64Assembler {
 43 
 44     public AMD64MacroAssembler(TargetDescription target) {
 45         super(target);
 46     }
 47 
 48     public final void decrementq(Register reg, int value) {
 49         if (value == Integer.MIN_VALUE) {
 50             subq(reg, value);
 51             return;
</pre>
<hr />
<pre>
 66 
 67     public final void decrementq(AMD64Address dst, int value) {
 68         if (value == Integer.MIN_VALUE) {
 69             subq(dst, value);
 70             return;
 71         }
 72         if (value &lt; 0) {
 73             incrementq(dst, -value);
 74             return;
 75         }
 76         if (value == 0) {
 77             return;
 78         }
 79         if (value == 1 &amp;&amp; UseIncDec) {
 80             decq(dst);
 81         } else {
 82             subq(dst, value);
 83         }
 84     }
 85 














 86     public void incrementq(Register reg, int value) {
 87         if (value == Integer.MIN_VALUE) {
 88             addq(reg, value);
 89             return;
 90         }
 91         if (value &lt; 0) {
 92             decrementq(reg, -value);
 93             return;
 94         }
 95         if (value == 0) {
 96             return;
 97         }
 98         if (value == 1 &amp;&amp; UseIncDec) {
 99             incq(reg);
100         } else {
101             addq(reg, value);
102         }
103     }
104 
105     public final void incrementq(AMD64Address dst, int value) {
</pre>
<hr />
<pre>
207             addl(dst, value);
208             return;
209         }
210         if (value &lt; 0) {
211             decrementl(dst, -value);
212             return;
213         }
214         if (value == 0) {
215             return;
216         }
217         if (value == 1 &amp;&amp; UseIncDec) {
218             incl(dst);
219         } else {
220             addl(dst, value);
221         }
222     }
223 
224     public void movflt(Register dst, Register src) {
225         assert dst.getRegisterCategory().equals(AMD64.XMM) &amp;&amp; src.getRegisterCategory().equals(AMD64.XMM);
226         if (UseXmmRegToRegMoveAll) {
<span class="line-modified">227             if (isAVX512Register(dst) || isAVX512Register(src)) {</span>
<span class="line-added">228                 VexMoveOp.VMOVAPS.emit(this, AVXSize.XMM, dst, src);</span>
<span class="line-added">229             } else {</span>
<span class="line-added">230                 movaps(dst, src);</span>
<span class="line-added">231             }</span>
232         } else {
<span class="line-modified">233             if (isAVX512Register(dst) || isAVX512Register(src)) {</span>
<span class="line-added">234                 VexMoveOp.VMOVSS.emit(this, AVXSize.XMM, dst, src);</span>
<span class="line-added">235             } else {</span>
<span class="line-added">236                 movss(dst, src);</span>
<span class="line-added">237             }</span>
238         }
239     }
240 
241     public void movflt(Register dst, AMD64Address src) {
242         assert dst.getRegisterCategory().equals(AMD64.XMM);
<span class="line-modified">243         if (isAVX512Register(dst)) {</span>
<span class="line-added">244             VexMoveOp.VMOVSS.emit(this, AVXSize.XMM, dst, src);</span>
<span class="line-added">245         } else {</span>
<span class="line-added">246             movss(dst, src);</span>
<span class="line-added">247         }</span>
248     }
249 
250     public void movflt(AMD64Address dst, Register src) {
251         assert src.getRegisterCategory().equals(AMD64.XMM);
<span class="line-modified">252         if (isAVX512Register(src)) {</span>
<span class="line-added">253             VexMoveOp.VMOVSS.emit(this, AVXSize.XMM, dst, src);</span>
<span class="line-added">254         } else {</span>
<span class="line-added">255             movss(dst, src);</span>
<span class="line-added">256         }</span>
257     }
258 
259     public void movdbl(Register dst, Register src) {
260         assert dst.getRegisterCategory().equals(AMD64.XMM) &amp;&amp; src.getRegisterCategory().equals(AMD64.XMM);
261         if (UseXmmRegToRegMoveAll) {
<span class="line-modified">262             if (isAVX512Register(dst) || isAVX512Register(src)) {</span>
<span class="line-added">263                 VexMoveOp.VMOVAPD.emit(this, AVXSize.XMM, dst, src);</span>
<span class="line-added">264             } else {</span>
<span class="line-added">265                 movapd(dst, src);</span>
<span class="line-added">266             }</span>
267         } else {
<span class="line-modified">268             if (isAVX512Register(dst) || isAVX512Register(src)) {</span>
<span class="line-added">269                 VexMoveOp.VMOVSD.emit(this, AVXSize.XMM, dst, src);</span>
<span class="line-added">270             } else {</span>
<span class="line-added">271                 movsd(dst, src);</span>
<span class="line-added">272             }</span>
273         }
274     }
275 
276     public void movdbl(Register dst, AMD64Address src) {
277         assert dst.getRegisterCategory().equals(AMD64.XMM);
278         if (UseXmmLoadAndClearUpper) {
<span class="line-modified">279             if (isAVX512Register(dst)) {</span>
<span class="line-added">280                 VexMoveOp.VMOVSD.emit(this, AVXSize.XMM, dst, src);</span>
<span class="line-added">281             } else {</span>
<span class="line-added">282                 movsd(dst, src);</span>
<span class="line-added">283             }</span>
284         } else {
<span class="line-added">285             assert !isAVX512Register(dst);</span>
286             movlpd(dst, src);
287         }
288     }
289 
290     public void movdbl(AMD64Address dst, Register src) {
291         assert src.getRegisterCategory().equals(AMD64.XMM);
<span class="line-modified">292         if (isAVX512Register(src)) {</span>
<span class="line-added">293             VexMoveOp.VMOVSD.emit(this, AVXSize.XMM, dst, src);</span>
<span class="line-added">294         } else {</span>
<span class="line-added">295             movsd(dst, src);</span>
<span class="line-added">296         }</span>
297     }
298 
299     /**
300      * Non-atomic write of a 64-bit constant to memory. Do not use if the address might be a
301      * volatile field!
302      */
303     public final void movlong(AMD64Address dst, long src) {
304         if (NumUtil.isInt(src)) {
305             AMD64MIOp.MOV.emit(this, OperandSize.QWORD, dst, (int) src);
306         } else {
307             AMD64Address high = new AMD64Address(dst.getBase(), dst.getIndex(), dst.getScale(), dst.getDisplacement() + 4);
308             movl(dst, (int) (src &amp; 0xFFFFFFFF));
309             movl(high, (int) (src &gt;&gt; 32));
310         }
311     }
312 
313     public final void setl(ConditionFlag cc, Register dst) {
314         setb(cc, dst);
315         movzbl(dst, dst);
316     }
</pre>
</td>
</tr>
</table>
<center><a href="AMD64BaseAssembler.java.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../../../index.html" target="_top">index</a> <a href="AVXKind.java.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>