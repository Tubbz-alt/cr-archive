<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.asm.amd64/src/org/graalvm/compiler/asm/amd64/AMD64Assembler.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../style.css" />
  </head>
<body>
<center><a href="AMD64AsmOptions.java.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../../../index.html" target="_top">index</a> <a href="AMD64BaseAssembler.java.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.asm.amd64/src/org/graalvm/compiler/asm/amd64/AMD64Assembler.java</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  */
  23 
  24 
  25 package org.graalvm.compiler.asm.amd64;
  26 
  27 import static jdk.vm.ci.amd64.AMD64.CPU;
  28 import static jdk.vm.ci.amd64.AMD64.MASK;
  29 import static jdk.vm.ci.amd64.AMD64.XMM;





  30 import static jdk.vm.ci.code.MemoryBarriers.STORE_LOAD;
  31 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseAddressNop;

  32 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseNormalNop;
  33 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.ADD;
  34 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.AND;
  35 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.CMP;
  36 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.OR;
  37 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.SBB;
  38 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.SUB;
  39 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.XOR;
  40 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MOp.DEC;
  41 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MOp.INC;
  42 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MOp.NEG;
  43 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MOp.NOT;
  44 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.EVEXPrefixConfig.B0;
  45 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.EVEXPrefixConfig.Z0;
  46 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.EVEXPrefixConfig.Z1;
  47 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.BYTE;
  48 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.DWORD;
  49 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.PD;
  50 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.PS;
  51 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.QWORD;
  52 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.SD;
  53 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.SS;
  54 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.WORD;
  55 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.L128;
  56 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.L256;

  57 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.LZ;
  58 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.M_0F;
  59 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.M_0F38;
  60 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.M_0F3A;
  61 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.P_;
  62 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.P_66;
  63 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.P_F2;
  64 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.P_F3;
  65 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.W0;
  66 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.W1;
  67 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.WIG;
  68 import static org.graalvm.compiler.core.common.NumUtil.isByte;
  69 import static org.graalvm.compiler.core.common.NumUtil.isInt;
  70 import static org.graalvm.compiler.core.common.NumUtil.isShiftCount;
  71 import static org.graalvm.compiler.core.common.NumUtil.isUByte;
  72 
  73 import java.util.EnumSet;
  74 
  75 import org.graalvm.compiler.asm.Label;
  76 import org.graalvm.compiler.asm.amd64.AMD64Address.Scale;
  77 import org.graalvm.compiler.asm.amd64.AVXKind.AVXSize;
<span class="line-removed">  78 import org.graalvm.compiler.core.common.NumUtil;</span>
  79 import org.graalvm.compiler.core.common.calc.Condition;
  80 import org.graalvm.compiler.debug.GraalError;
  81 
  82 import jdk.vm.ci.amd64.AMD64;
  83 import jdk.vm.ci.amd64.AMD64.CPUFeature;
  84 import jdk.vm.ci.code.Register;
  85 import jdk.vm.ci.code.Register.RegisterCategory;
  86 import jdk.vm.ci.code.TargetDescription;
  87 
  88 /**
  89  * This class implements an assembler that can encode most X86 instructions.
  90  */
  91 public class AMD64Assembler extends AMD64BaseAssembler {
  92 
  93     /**
  94      * Constructs an assembler for the AMD64 architecture.
  95      */
  96     public AMD64Assembler(TargetDescription target) {
  97         super(target);
  98     }
</pre>
<hr />
<pre>
 878         public static final AMD64Shift ROL = new AMD64Shift(&quot;ROL&quot;, 0);
 879         public static final AMD64Shift ROR = new AMD64Shift(&quot;ROR&quot;, 1);
 880         public static final AMD64Shift RCL = new AMD64Shift(&quot;RCL&quot;, 2);
 881         public static final AMD64Shift RCR = new AMD64Shift(&quot;RCR&quot;, 3);
 882         public static final AMD64Shift SHL = new AMD64Shift(&quot;SHL&quot;, 4);
 883         public static final AMD64Shift SHR = new AMD64Shift(&quot;SHR&quot;, 5);
 884         public static final AMD64Shift SAR = new AMD64Shift(&quot;SAR&quot;, 7);
 885         // @formatter:on
 886 
 887         public final AMD64MOp m1Op;
 888         public final AMD64MOp mcOp;
 889         public final AMD64MIOp miOp;
 890 
 891         private AMD64Shift(String opcode, int code) {
 892             m1Op = new AMD64MOp(opcode, 0, 0xD1, code, OpAssertion.WordOrLargerAssertion);
 893             mcOp = new AMD64MOp(opcode, 0, 0xD3, code, OpAssertion.WordOrLargerAssertion);
 894             miOp = new AMD64MIOp(opcode, true, 0, 0xC1, code, OpAssertion.WordOrLargerAssertion);
 895         }
 896     }
 897 















































 898     private enum VEXOpAssertion {
<span class="line-modified"> 899         AVX1(CPUFeature.AVX, CPUFeature.AVX),</span>
<span class="line-modified"> 900         AVX1_2(CPUFeature.AVX, CPUFeature.AVX2),</span>
<span class="line-modified"> 901         AVX2(CPUFeature.AVX2, CPUFeature.AVX2),</span>
<span class="line-modified"> 902         AVX1_128ONLY(CPUFeature.AVX, null),</span>
<span class="line-modified"> 903         AVX1_256ONLY(null, CPUFeature.AVX),</span>
<span class="line-modified"> 904         AVX2_256ONLY(null, CPUFeature.AVX2),</span>
<span class="line-modified"> 905         XMM_CPU(CPUFeature.AVX, null, XMM, null, CPU, null),</span>
<span class="line-modified"> 906         XMM_XMM_CPU(CPUFeature.AVX, null, XMM, XMM, CPU, null),</span>
<span class="line-modified"> 907         CPU_XMM(CPUFeature.AVX, null, CPU, null, XMM, null),</span>
<span class="line-modified"> 908         AVX1_2_CPU_XMM(CPUFeature.AVX, CPUFeature.AVX2, CPU, null, XMM, null),</span>
<span class="line-modified"> 909         BMI1(CPUFeature.BMI1, null, CPU, CPU, CPU, null),</span>
<span class="line-modified"> 910         BMI2(CPUFeature.BMI2, null, CPU, CPU, CPU, null);</span>





 911 
 912         private final CPUFeature l128feature;
 913         private final CPUFeature l256feature;

 914 
 915         private final RegisterCategory rCategory;
 916         private final RegisterCategory vCategory;
 917         private final RegisterCategory mCategory;
 918         private final RegisterCategory imm8Category;
 919 
<span class="line-modified"> 920         VEXOpAssertion(CPUFeature l128feature, CPUFeature l256feature) {</span>
<span class="line-modified"> 921             this(l128feature, l256feature, XMM, XMM, XMM, XMM);</span>
 922         }
 923 
<span class="line-modified"> 924         VEXOpAssertion(CPUFeature l128feature, CPUFeature l256feature, RegisterCategory rCategory, RegisterCategory vCategory, RegisterCategory mCategory, RegisterCategory imm8Category) {</span>

 925             this.l128feature = l128feature;
 926             this.l256feature = l256feature;

 927             this.rCategory = rCategory;
 928             this.vCategory = vCategory;
 929             this.mCategory = mCategory;
 930             this.imm8Category = imm8Category;
 931         }
 932 
 933         public boolean check(AMD64 arch, AVXSize size, Register r, Register v, Register m) {
 934             return check(arch, getLFlag(size), r, v, m, null);
 935         }
 936 
 937         public boolean check(AMD64 arch, AVXSize size, Register r, Register v, Register m, Register imm8) {
 938             return check(arch, getLFlag(size), r, v, m, imm8);
 939         }
 940 
 941         public boolean check(AMD64 arch, int l, Register r, Register v, Register m, Register imm8) {
<span class="line-modified"> 942             switch (l) {</span>
<span class="line-modified"> 943                 case L128:</span>
<span class="line-modified"> 944                     assert l128feature != null &amp;&amp; arch.getFeatures().contains(l128feature) : &quot;emitting illegal 128 bit instruction&quot;;</span>
<span class="line-modified"> 945                     break;</span>
<span class="line-modified"> 946                 case L256:</span>
<span class="line-modified"> 947                     assert l256feature != null &amp;&amp; arch.getFeatures().contains(l256feature) : &quot;emitting illegal 256 bit instruction&quot;;</span>
<span class="line-removed"> 948                     break;</span>
 949             }
 950             if (r != null) {
 951                 assert r.getRegisterCategory().equals(rCategory);
 952             }
 953             if (v != null) {
 954                 assert v.getRegisterCategory().equals(vCategory);
 955             }
 956             if (m != null) {
 957                 assert m.getRegisterCategory().equals(mCategory);
 958             }
 959             if (imm8 != null) {
 960                 assert imm8.getRegisterCategory().equals(imm8Category);
 961             }
 962             return true;
 963         }
 964 
<span class="line-modified"> 965         public boolean supports(EnumSet&lt;CPUFeature&gt; features, AVXSize avxSize) {</span>
<span class="line-modified"> 966             switch (avxSize) {</span>
<span class="line-modified"> 967                 case XMM:</span>
<span class="line-modified"> 968                     return l128feature != null &amp;&amp; features.contains(l128feature);</span>
<span class="line-modified"> 969                 case YMM:</span>
<span class="line-modified"> 970                     return l256feature != null &amp;&amp; features.contains(l256feature);</span>
<span class="line-modified"> 971                 default:</span>
<span class="line-removed"> 972                     throw GraalError.shouldNotReachHere();</span>
 973             }

 974         }
 975     }
 976 
 977     /**
 978      * Base class for VEX-encoded instructions.
 979      */
 980     public static class VexOp {
 981         protected final int pp;
 982         protected final int mmmmm;
 983         protected final int w;
 984         protected final int op;
 985 
 986         private final String opcode;
 987         protected final VEXOpAssertion assertion;
 988 
<span class="line-modified"> 989         protected VexOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {</span>



 990             this.pp = pp;
 991             this.mmmmm = mmmmm;
 992             this.w = w;
 993             this.op = op;
 994             this.opcode = opcode;
 995             this.assertion = assertion;






 996         }
 997 
 998         public final boolean isSupported(AMD64Assembler vasm, AVXSize size) {
<span class="line-modified"> 999             return assertion.supports(((AMD64) vasm.target.arch).getFeatures(), size);</span>




1000         }
1001 
1002         @Override
1003         public String toString() {
1004             return opcode;
1005         }





1006     }
1007 
1008     /**
1009      * VEX-encoded instructions with an operand order of RM, but the M operand must be a register.
1010      */
1011     public static class VexRROp extends VexOp {
1012         // @formatter:off
<span class="line-modified">1013         public static final VexRROp VMASKMOVDQU = new VexRROp(&quot;VMASKMOVDQU&quot;, P_66, M_0F, WIG, 0xF7, VEXOpAssertion.AVX1_128ONLY);</span>
1014         // @formatter:on
1015 
<span class="line-modified">1016         protected VexRROp(String opcode, int pp, int mmmmm, int w, int op) {</span>
<span class="line-modified">1017             this(opcode, pp, mmmmm, w, op, VEXOpAssertion.AVX1);</span>
<span class="line-removed">1018         }</span>
<span class="line-removed">1019 </span>
<span class="line-removed">1020         protected VexRROp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {</span>
<span class="line-removed">1021             super(opcode, pp, mmmmm, w, op, assertion);</span>
1022         }
1023 
1024         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src) {
1025             assert assertion.check((AMD64) asm.target.arch, size, dst, null, src);
1026             assert op != 0x1A || op != 0x5A;
<span class="line-modified">1027             asm.vexPrefix(dst, Register.None, src, size, pp, mmmmm, w, false);</span>
1028             asm.emitByte(op);
1029             asm.emitModRM(dst, src);
1030         }
1031     }
1032 
1033     /**
1034      * VEX-encoded instructions with an operand order of RM.
1035      */
1036     public static class VexRMOp extends VexRROp {
1037         // @formatter:off
1038         public static final VexRMOp VCVTTSS2SI      = new VexRMOp(&quot;VCVTTSS2SI&quot;,      P_F3, M_0F,   W0,  0x2C, VEXOpAssertion.CPU_XMM);
1039         public static final VexRMOp VCVTTSS2SQ      = new VexRMOp(&quot;VCVTTSS2SQ&quot;,      P_F3, M_0F,   W1,  0x2C, VEXOpAssertion.CPU_XMM);
1040         public static final VexRMOp VCVTTSD2SI      = new VexRMOp(&quot;VCVTTSD2SI&quot;,      P_F2, M_0F,   W0,  0x2C, VEXOpAssertion.CPU_XMM);
1041         public static final VexRMOp VCVTTSD2SQ      = new VexRMOp(&quot;VCVTTSD2SQ&quot;,      P_F2, M_0F,   W1,  0x2C, VEXOpAssertion.CPU_XMM);
1042         public static final VexRMOp VCVTPS2PD       = new VexRMOp(&quot;VCVTPS2PD&quot;,       P_,   M_0F,   WIG, 0x5A);
1043         public static final VexRMOp VCVTPD2PS       = new VexRMOp(&quot;VCVTPD2PS&quot;,       P_66, M_0F,   WIG, 0x5A);
1044         public static final VexRMOp VCVTDQ2PS       = new VexRMOp(&quot;VCVTDQ2PS&quot;,       P_,   M_0F,   WIG, 0x5B);
1045         public static final VexRMOp VCVTTPS2DQ      = new VexRMOp(&quot;VCVTTPS2DQ&quot;,      P_F3, M_0F,   WIG, 0x5B);
1046         public static final VexRMOp VCVTTPD2DQ      = new VexRMOp(&quot;VCVTTPD2DQ&quot;,      P_66, M_0F,   WIG, 0xE6);
1047         public static final VexRMOp VCVTDQ2PD       = new VexRMOp(&quot;VCVTDQ2PD&quot;,       P_F3, M_0F,   WIG, 0xE6);
</pre>
<hr />
<pre>
1059         public static final VexRMOp VPMOVSXBQ       = new VexRMOp(&quot;VPMOVSXBQ&quot;,       P_66, M_0F38, WIG, 0x22);
1060         public static final VexRMOp VPMOVSXWD       = new VexRMOp(&quot;VPMOVSXWD&quot;,       P_66, M_0F38, WIG, 0x23);
1061         public static final VexRMOp VPMOVSXWQ       = new VexRMOp(&quot;VPMOVSXWQ&quot;,       P_66, M_0F38, WIG, 0x24);
1062         public static final VexRMOp VPMOVSXDQ       = new VexRMOp(&quot;VPMOVSXDQ&quot;,       P_66, M_0F38, WIG, 0x25);
1063         public static final VexRMOp VPMOVZXBW       = new VexRMOp(&quot;VPMOVZXBW&quot;,       P_66, M_0F38, WIG, 0x30);
1064         public static final VexRMOp VPMOVZXBD       = new VexRMOp(&quot;VPMOVZXBD&quot;,       P_66, M_0F38, WIG, 0x31);
1065         public static final VexRMOp VPMOVZXBQ       = new VexRMOp(&quot;VPMOVZXBQ&quot;,       P_66, M_0F38, WIG, 0x32);
1066         public static final VexRMOp VPMOVZXWD       = new VexRMOp(&quot;VPMOVZXWD&quot;,       P_66, M_0F38, WIG, 0x33);
1067         public static final VexRMOp VPMOVZXWQ       = new VexRMOp(&quot;VPMOVZXWQ&quot;,       P_66, M_0F38, WIG, 0x34);
1068         public static final VexRMOp VPMOVZXDQ       = new VexRMOp(&quot;VPMOVZXDQ&quot;,       P_66, M_0F38, WIG, 0x35);
1069         public static final VexRMOp VPTEST          = new VexRMOp(&quot;VPTEST&quot;,          P_66, M_0F38, WIG, 0x17);
1070         public static final VexRMOp VSQRTPD         = new VexRMOp(&quot;VSQRTPD&quot;,         P_66, M_0F,   WIG, 0x51);
1071         public static final VexRMOp VSQRTPS         = new VexRMOp(&quot;VSQRTPS&quot;,         P_,   M_0F,   WIG, 0x51);
1072         public static final VexRMOp VSQRTSD         = new VexRMOp(&quot;VSQRTSD&quot;,         P_F2, M_0F,   WIG, 0x51);
1073         public static final VexRMOp VSQRTSS         = new VexRMOp(&quot;VSQRTSS&quot;,         P_F3, M_0F,   WIG, 0x51);
1074         public static final VexRMOp VUCOMISS        = new VexRMOp(&quot;VUCOMISS&quot;,        P_,   M_0F,   WIG, 0x2E);
1075         public static final VexRMOp VUCOMISD        = new VexRMOp(&quot;VUCOMISD&quot;,        P_66, M_0F,   WIG, 0x2E);
1076         // @formatter:on
1077 
1078         protected VexRMOp(String opcode, int pp, int mmmmm, int w, int op) {
<span class="line-modified">1079             this(opcode, pp, mmmmm, w, op, VEXOpAssertion.AVX1);</span>
1080         }
1081 
1082         protected VexRMOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
<span class="line-modified">1083             super(opcode, pp, mmmmm, w, op, assertion);</span>




1084         }
1085 
1086         public void emit(AMD64Assembler asm, AVXSize size, Register dst, AMD64Address src) {
1087             assert assertion.check((AMD64) asm.target.arch, size, dst, null, null);
<span class="line-modified">1088             asm.vexPrefix(dst, Register.None, src, size, pp, mmmmm, w, false);</span>
1089             asm.emitByte(op);
<span class="line-modified">1090             asm.emitOperandHelper(dst, src, 0);</span>
1091         }
1092     }
1093 
1094     /**
1095      * VEX-encoded move instructions.
1096      * &lt;p&gt;
1097      * These instructions have two opcodes: op is the forward move instruction with an operand order
1098      * of RM, and opReverse is the reverse move instruction with an operand order of MR.
1099      */
1100     public static final class VexMoveOp extends VexRMOp {
1101         // @formatter:off
<span class="line-modified">1102         public static final VexMoveOp VMOVDQA = new VexMoveOp(&quot;VMOVDQA&quot;, P_66, M_0F, WIG, 0x6F, 0x7F);</span>
<span class="line-modified">1103         public static final VexMoveOp VMOVDQU = new VexMoveOp(&quot;VMOVDQU&quot;, P_F3, M_0F, WIG, 0x6F, 0x7F);</span>
<span class="line-modified">1104         public static final VexMoveOp VMOVAPS = new VexMoveOp(&quot;VMOVAPS&quot;, P_,   M_0F, WIG, 0x28, 0x29);</span>
<span class="line-modified">1105         public static final VexMoveOp VMOVAPD = new VexMoveOp(&quot;VMOVAPD&quot;, P_66, M_0F, WIG, 0x28, 0x29);</span>
<span class="line-modified">1106         public static final VexMoveOp VMOVUPS = new VexMoveOp(&quot;VMOVUPS&quot;, P_,   M_0F, WIG, 0x10, 0x11);</span>
<span class="line-modified">1107         public static final VexMoveOp VMOVUPD = new VexMoveOp(&quot;VMOVUPD&quot;, P_66, M_0F, WIG, 0x10, 0x11);</span>
<span class="line-modified">1108         public static final VexMoveOp VMOVSS  = new VexMoveOp(&quot;VMOVSS&quot;,  P_F3, M_0F, WIG, 0x10, 0x11);</span>
<span class="line-modified">1109         public static final VexMoveOp VMOVSD  = new VexMoveOp(&quot;VMOVSD&quot;,  P_F2, M_0F, WIG, 0x10, 0x11);</span>
<span class="line-modified">1110         public static final VexMoveOp VMOVD   = new VexMoveOp(&quot;VMOVD&quot;,   P_66, M_0F, W0,  0x6E, 0x7E, VEXOpAssertion.XMM_CPU);</span>
<span class="line-modified">1111         public static final VexMoveOp VMOVQ   = new VexMoveOp(&quot;VMOVQ&quot;,   P_66, M_0F, W1,  0x6E, 0x7E, VEXOpAssertion.XMM_CPU);</span>


1112         // @formatter:on
1113 
1114         private final int opReverse;
1115 
1116         private VexMoveOp(String opcode, int pp, int mmmmm, int w, int op, int opReverse) {
<span class="line-modified">1117             this(opcode, pp, mmmmm, w, op, opReverse, VEXOpAssertion.AVX1);</span>
1118         }
1119 
1120         private VexMoveOp(String opcode, int pp, int mmmmm, int w, int op, int opReverse, VEXOpAssertion assertion) {
<span class="line-modified">1121             super(opcode, pp, mmmmm, w, op, assertion);</span>




1122             this.opReverse = opReverse;
1123         }
1124 
1125         public void emit(AMD64Assembler asm, AVXSize size, AMD64Address dst, Register src) {
1126             assert assertion.check((AMD64) asm.target.arch, size, src, null, null);
<span class="line-modified">1127             asm.vexPrefix(src, Register.None, dst, size, pp, mmmmm, w, false);</span>
1128             asm.emitByte(opReverse);
<span class="line-modified">1129             asm.emitOperandHelper(src, dst, 0);</span>
1130         }
1131 
1132         public void emitReverse(AMD64Assembler asm, AVXSize size, Register dst, Register src) {
1133             assert assertion.check((AMD64) asm.target.arch, size, src, null, dst);
<span class="line-modified">1134             asm.vexPrefix(src, Register.None, dst, size, pp, mmmmm, w, false);</span>
1135             asm.emitByte(opReverse);
1136             asm.emitModRM(src, dst);
1137         }
1138     }
1139 
1140     public interface VexRRIOp {
1141         void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src, int imm8);
1142     }
1143 
1144     /**
1145      * VEX-encoded instructions with an operand order of RMI.
1146      */
1147     public static final class VexRMIOp extends VexOp implements VexRRIOp {
1148         // @formatter:off
1149         public static final VexRMIOp VPERMQ   = new VexRMIOp(&quot;VPERMQ&quot;,   P_66, M_0F3A, W1,  0x00, VEXOpAssertion.AVX2_256ONLY);
1150         public static final VexRMIOp VPSHUFLW = new VexRMIOp(&quot;VPSHUFLW&quot;, P_F2, M_0F,   WIG, 0x70, VEXOpAssertion.AVX1_2);
1151         public static final VexRMIOp VPSHUFHW = new VexRMIOp(&quot;VPSHUFHW&quot;, P_F3, M_0F,   WIG, 0x70, VEXOpAssertion.AVX1_2);
1152         public static final VexRMIOp VPSHUFD  = new VexRMIOp(&quot;VPSHUFD&quot;,  P_66, M_0F,   WIG, 0x70, VEXOpAssertion.AVX1_2);
1153         // @formatter:on
1154 
1155         private VexRMIOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1156             super(opcode, pp, mmmmm, w, op, assertion);
1157         }
1158 
1159         @Override
1160         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src, int imm8) {
1161             assert assertion.check((AMD64) asm.target.arch, size, dst, null, src);
<span class="line-modified">1162             asm.vexPrefix(dst, Register.None, src, size, pp, mmmmm, w, false);</span>
1163             asm.emitByte(op);
1164             asm.emitModRM(dst, src);
1165             asm.emitByte(imm8);
1166         }
1167 
1168         public void emit(AMD64Assembler asm, AVXSize size, Register dst, AMD64Address src, int imm8) {
1169             assert assertion.check((AMD64) asm.target.arch, size, dst, null, null);
<span class="line-modified">1170             asm.vexPrefix(dst, Register.None, src, size, pp, mmmmm, w, false);</span>
1171             asm.emitByte(op);
<span class="line-modified">1172             asm.emitOperandHelper(dst, src, 1);</span>
1173             asm.emitByte(imm8);
1174         }
1175     }
1176 
1177     /**
1178      * VEX-encoded instructions with an operand order of MRI.
1179      */
1180     public static final class VexMRIOp extends VexOp implements VexRRIOp {
1181         // @formatter:off
1182         public static final VexMRIOp VEXTRACTF128 = new VexMRIOp(&quot;VEXTRACTF128&quot;, P_66, M_0F3A, W0, 0x19, VEXOpAssertion.AVX1_256ONLY);
1183         public static final VexMRIOp VEXTRACTI128 = new VexMRIOp(&quot;VEXTRACTI128&quot;, P_66, M_0F3A, W0, 0x39, VEXOpAssertion.AVX2_256ONLY);
1184         public static final VexMRIOp VPEXTRB      = new VexMRIOp(&quot;VPEXTRB&quot;,      P_66, M_0F3A, W0, 0x14, VEXOpAssertion.XMM_CPU);
1185         public static final VexMRIOp VPEXTRW      = new VexMRIOp(&quot;VPEXTRW&quot;,      P_66, M_0F3A, W0, 0x15, VEXOpAssertion.XMM_CPU);
1186         public static final VexMRIOp VPEXTRD      = new VexMRIOp(&quot;VPEXTRD&quot;,      P_66, M_0F3A, W0, 0x16, VEXOpAssertion.XMM_CPU);
1187         public static final VexMRIOp VPEXTRQ      = new VexMRIOp(&quot;VPEXTRQ&quot;,      P_66, M_0F3A, W1, 0x16, VEXOpAssertion.XMM_CPU);
1188         // @formatter:on
1189 
1190         private VexMRIOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1191             super(opcode, pp, mmmmm, w, op, assertion);
1192         }
1193 
1194         @Override
1195         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src, int imm8) {
1196             assert assertion.check((AMD64) asm.target.arch, size, src, null, dst);
<span class="line-modified">1197             asm.vexPrefix(src, Register.None, dst, size, pp, mmmmm, w, false);</span>
1198             asm.emitByte(op);
1199             asm.emitModRM(src, dst);
1200             asm.emitByte(imm8);
1201         }
1202 
1203         public void emit(AMD64Assembler asm, AVXSize size, AMD64Address dst, Register src, int imm8) {
1204             assert assertion.check((AMD64) asm.target.arch, size, src, null, null);
<span class="line-modified">1205             asm.vexPrefix(src, Register.None, dst, size, pp, mmmmm, w, false);</span>
1206             asm.emitByte(op);
<span class="line-modified">1207             asm.emitOperandHelper(src, dst, 1);</span>
1208             asm.emitByte(imm8);
1209         }
1210     }
1211 
1212     /**
1213      * VEX-encoded instructions with an operand order of RVMR.
1214      */
1215     public static class VexRVMROp extends VexOp {
1216         // @formatter:off
<span class="line-modified">1217         public static final VexRVMROp VPBLENDVB  = new VexRVMROp(&quot;VPBLENDVB&quot;,  P_66, M_0F3A, W0, 0x4C, VEXOpAssertion.AVX1_2);</span>
<span class="line-modified">1218         public static final VexRVMROp VPBLENDVPS = new VexRVMROp(&quot;VPBLENDVPS&quot;, P_66, M_0F3A, W0, 0x4A, VEXOpAssertion.AVX1);</span>
<span class="line-modified">1219         public static final VexRVMROp VPBLENDVPD = new VexRVMROp(&quot;VPBLENDVPD&quot;, P_66, M_0F3A, W0, 0x4B, VEXOpAssertion.AVX1);</span>
1220         // @formatter:on
1221 
1222         protected VexRVMROp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1223             super(opcode, pp, mmmmm, w, op, assertion);
1224         }
1225 
1226         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register mask, Register src1, Register src2) {
1227             assert assertion.check((AMD64) asm.target.arch, size, dst, mask, src1, src2);
<span class="line-modified">1228             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, false);</span>
1229             asm.emitByte(op);
1230             asm.emitModRM(dst, src2);
1231             asm.emitByte(mask.encoding() &lt;&lt; 4);
1232         }
1233 
1234         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register mask, Register src1, AMD64Address src2) {
1235             assert assertion.check((AMD64) asm.target.arch, size, dst, mask, src1, null);
<span class="line-modified">1236             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, false);</span>
1237             asm.emitByte(op);
<span class="line-modified">1238             asm.emitOperandHelper(dst, src2, 0);</span>
1239             asm.emitByte(mask.encoding() &lt;&lt; 4);
1240         }
1241     }
1242 
1243     /**
1244      * VEX-encoded instructions with an operand order of RVM.
1245      */
1246     public static class VexRVMOp extends VexOp {
1247         // @formatter:off
1248         public static final VexRVMOp VANDPS    = new VexRVMOp(&quot;VANDPS&quot;,    P_,   M_0F,   WIG, 0x54);
1249         public static final VexRVMOp VANDPD    = new VexRVMOp(&quot;VANDPD&quot;,    P_66, M_0F,   WIG, 0x54);
1250         public static final VexRVMOp VANDNPS   = new VexRVMOp(&quot;VANDNPS&quot;,   P_,   M_0F,   WIG, 0x55);
1251         public static final VexRVMOp VANDNPD   = new VexRVMOp(&quot;VANDNPD&quot;,   P_66, M_0F,   WIG, 0x55);
1252         public static final VexRVMOp VORPS     = new VexRVMOp(&quot;VORPS&quot;,     P_,   M_0F,   WIG, 0x56);
1253         public static final VexRVMOp VORPD     = new VexRVMOp(&quot;VORPD&quot;,     P_66, M_0F,   WIG, 0x56);
1254         public static final VexRVMOp VXORPS    = new VexRVMOp(&quot;VXORPS&quot;,    P_,   M_0F,   WIG, 0x57);
1255         public static final VexRVMOp VXORPD    = new VexRVMOp(&quot;VXORPD&quot;,    P_66, M_0F,   WIG, 0x57);
1256         public static final VexRVMOp VADDPS    = new VexRVMOp(&quot;VADDPS&quot;,    P_,   M_0F,   WIG, 0x58);
1257         public static final VexRVMOp VADDPD    = new VexRVMOp(&quot;VADDPD&quot;,    P_66, M_0F,   WIG, 0x58);
1258         public static final VexRVMOp VADDSS    = new VexRVMOp(&quot;VADDSS&quot;,    P_F3, M_0F,   WIG, 0x58);
</pre>
<hr />
<pre>
1292         public static final VexRVMOp VPMULLD   = new VexRVMOp(&quot;VPMULLD&quot;,   P_66, M_0F38, WIG, 0x40, VEXOpAssertion.AVX1_2);
1293         public static final VexRVMOp VPSUBB    = new VexRVMOp(&quot;VPSUBB&quot;,    P_66, M_0F,   WIG, 0xF8, VEXOpAssertion.AVX1_2);
1294         public static final VexRVMOp VPSUBW    = new VexRVMOp(&quot;VPSUBW&quot;,    P_66, M_0F,   WIG, 0xF9, VEXOpAssertion.AVX1_2);
1295         public static final VexRVMOp VPSUBD    = new VexRVMOp(&quot;VPSUBD&quot;,    P_66, M_0F,   WIG, 0xFA, VEXOpAssertion.AVX1_2);
1296         public static final VexRVMOp VPSUBQ    = new VexRVMOp(&quot;VPSUBQ&quot;,    P_66, M_0F,   WIG, 0xFB, VEXOpAssertion.AVX1_2);
1297         public static final VexRVMOp VPSHUFB   = new VexRVMOp(&quot;VPSHUFB&quot;,   P_66, M_0F38, WIG, 0x00, VEXOpAssertion.AVX1_2);
1298         public static final VexRVMOp VCVTSD2SS = new VexRVMOp(&quot;VCVTSD2SS&quot;, P_F2, M_0F,   WIG, 0x5A);
1299         public static final VexRVMOp VCVTSS2SD = new VexRVMOp(&quot;VCVTSS2SD&quot;, P_F3, M_0F,   WIG, 0x5A);
1300         public static final VexRVMOp VCVTSI2SD = new VexRVMOp(&quot;VCVTSI2SD&quot;, P_F2, M_0F,   W0,  0x2A, VEXOpAssertion.XMM_XMM_CPU);
1301         public static final VexRVMOp VCVTSQ2SD = new VexRVMOp(&quot;VCVTSQ2SD&quot;, P_F2, M_0F,   W1,  0x2A, VEXOpAssertion.XMM_XMM_CPU);
1302         public static final VexRVMOp VCVTSI2SS = new VexRVMOp(&quot;VCVTSI2SS&quot;, P_F3, M_0F,   W0,  0x2A, VEXOpAssertion.XMM_XMM_CPU);
1303         public static final VexRVMOp VCVTSQ2SS = new VexRVMOp(&quot;VCVTSQ2SS&quot;, P_F3, M_0F,   W1,  0x2A, VEXOpAssertion.XMM_XMM_CPU);
1304         public static final VexRVMOp VPCMPEQB  = new VexRVMOp(&quot;VPCMPEQB&quot;,  P_66, M_0F,   WIG, 0x74, VEXOpAssertion.AVX1_2);
1305         public static final VexRVMOp VPCMPEQW  = new VexRVMOp(&quot;VPCMPEQW&quot;,  P_66, M_0F,   WIG, 0x75, VEXOpAssertion.AVX1_2);
1306         public static final VexRVMOp VPCMPEQD  = new VexRVMOp(&quot;VPCMPEQD&quot;,  P_66, M_0F,   WIG, 0x76, VEXOpAssertion.AVX1_2);
1307         public static final VexRVMOp VPCMPEQQ  = new VexRVMOp(&quot;VPCMPEQQ&quot;,  P_66, M_0F38, WIG, 0x29, VEXOpAssertion.AVX1_2);
1308         public static final VexRVMOp VPCMPGTB  = new VexRVMOp(&quot;VPCMPGTB&quot;,  P_66, M_0F,   WIG, 0x64, VEXOpAssertion.AVX1_2);
1309         public static final VexRVMOp VPCMPGTW  = new VexRVMOp(&quot;VPCMPGTW&quot;,  P_66, M_0F,   WIG, 0x65, VEXOpAssertion.AVX1_2);
1310         public static final VexRVMOp VPCMPGTD  = new VexRVMOp(&quot;VPCMPGTD&quot;,  P_66, M_0F,   WIG, 0x66, VEXOpAssertion.AVX1_2);
1311         public static final VexRVMOp VPCMPGTQ  = new VexRVMOp(&quot;VPCMPGTQ&quot;,  P_66, M_0F38, WIG, 0x37, VEXOpAssertion.AVX1_2);


1312         // @formatter:on
1313 
1314         private VexRVMOp(String opcode, int pp, int mmmmm, int w, int op) {
1315             this(opcode, pp, mmmmm, w, op, VEXOpAssertion.AVX1);
1316         }
1317 
1318         protected VexRVMOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1319             super(opcode, pp, mmmmm, w, op, assertion);
1320         }
1321 
1322         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, Register src2) {
1323             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, src2);
<span class="line-modified">1324             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, false);</span>
1325             asm.emitByte(op);
1326             asm.emitModRM(dst, src2);
1327         }
1328 
1329         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, AMD64Address src2) {
1330             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, null);
<span class="line-modified">1331             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, false);</span>
1332             asm.emitByte(op);
<span class="line-modified">1333             asm.emitOperandHelper(dst, src2, 0);</span>
1334         }
1335     }
1336 
1337     public static final class VexGeneralPurposeRVMOp extends VexRVMOp {
1338         // @formatter:off
1339         public static final VexGeneralPurposeRVMOp ANDN   = new VexGeneralPurposeRVMOp(&quot;ANDN&quot;,   P_,   M_0F38, WIG, 0xF2, VEXOpAssertion.BMI1);
1340         public static final VexGeneralPurposeRVMOp MULX   = new VexGeneralPurposeRVMOp(&quot;MULX&quot;,   P_F2, M_0F38, WIG, 0xF6, VEXOpAssertion.BMI2);
1341         public static final VexGeneralPurposeRVMOp PDEP   = new VexGeneralPurposeRVMOp(&quot;PDEP&quot;,   P_F2, M_0F38, WIG, 0xF5, VEXOpAssertion.BMI2);
1342         public static final VexGeneralPurposeRVMOp PEXT   = new VexGeneralPurposeRVMOp(&quot;PEXT&quot;,   P_F3, M_0F38, WIG, 0xF5, VEXOpAssertion.BMI2);
1343         // @formatter:on
1344 
1345         private VexGeneralPurposeRVMOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1346             super(opcode, pp, mmmmm, w, op, assertion);
1347         }
1348 
1349         @Override
1350         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, Register src2) {
1351             assert assertion.check((AMD64) asm.target.arch, LZ, dst, src1, src2, null);
1352             assert size == AVXSize.DWORD || size == AVXSize.QWORD;
<span class="line-modified">1353             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, false);</span>
1354             asm.emitByte(op);
1355             asm.emitModRM(dst, src2);
1356         }
1357 
1358         @Override
1359         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, AMD64Address src2) {
1360             assert assertion.check((AMD64) asm.target.arch, LZ, dst, src1, null, null);
1361             assert size == AVXSize.DWORD || size == AVXSize.QWORD;
<span class="line-modified">1362             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, false);</span>
1363             asm.emitByte(op);
1364             asm.emitOperandHelper(dst, src2, 0);
1365         }
1366     }
1367 
1368     public static final class VexGeneralPurposeRMVOp extends VexOp {
1369         // @formatter:off
1370         public static final VexGeneralPurposeRMVOp BEXTR  = new VexGeneralPurposeRMVOp(&quot;BEXTR&quot;,  P_,   M_0F38, WIG, 0xF7, VEXOpAssertion.BMI1);
1371         public static final VexGeneralPurposeRMVOp BZHI   = new VexGeneralPurposeRMVOp(&quot;BZHI&quot;,   P_,   M_0F38, WIG, 0xF5, VEXOpAssertion.BMI2);
1372         public static final VexGeneralPurposeRMVOp SARX   = new VexGeneralPurposeRMVOp(&quot;SARX&quot;,   P_F3, M_0F38, WIG, 0xF7, VEXOpAssertion.BMI2);
1373         public static final VexGeneralPurposeRMVOp SHRX   = new VexGeneralPurposeRMVOp(&quot;SHRX&quot;,   P_F2, M_0F38, WIG, 0xF7, VEXOpAssertion.BMI2);
1374         public static final VexGeneralPurposeRMVOp SHLX   = new VexGeneralPurposeRMVOp(&quot;SHLX&quot;,   P_66, M_0F38, WIG, 0xF7, VEXOpAssertion.BMI2);
1375         // @formatter:on
1376 
1377         private VexGeneralPurposeRMVOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1378             super(opcode, pp, mmmmm, w, op, assertion);
1379         }
1380 
1381         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, Register src2) {
1382             assert assertion.check((AMD64) asm.target.arch, LZ, dst, src2, src1, null);
1383             assert size == AVXSize.DWORD || size == AVXSize.QWORD;
<span class="line-modified">1384             asm.vexPrefix(dst, src2, src1, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, false);</span>
1385             asm.emitByte(op);
1386             asm.emitModRM(dst, src1);
1387         }
1388 
1389         public void emit(AMD64Assembler asm, AVXSize size, Register dst, AMD64Address src1, Register src2) {
1390             assert assertion.check((AMD64) asm.target.arch, LZ, dst, src2, null, null);
1391             assert size == AVXSize.DWORD || size == AVXSize.QWORD;
<span class="line-modified">1392             asm.vexPrefix(dst, src2, src1, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, false);</span>
1393             asm.emitByte(op);
1394             asm.emitOperandHelper(dst, src1, 0);
1395         }
1396     }
1397 
1398     public static final class VexGeneralPurposeRMOp extends VexRMOp {
1399         // @formatter:off
1400         public static final VexGeneralPurposeRMOp BLSI    = new VexGeneralPurposeRMOp(&quot;BLSI&quot;,   P_,    M_0F38, WIG, 0xF3, 3, VEXOpAssertion.BMI1);
1401         public static final VexGeneralPurposeRMOp BLSMSK  = new VexGeneralPurposeRMOp(&quot;BLSMSK&quot;, P_,    M_0F38, WIG, 0xF3, 2, VEXOpAssertion.BMI1);
1402         public static final VexGeneralPurposeRMOp BLSR    = new VexGeneralPurposeRMOp(&quot;BLSR&quot;,   P_,    M_0F38, WIG, 0xF3, 1, VEXOpAssertion.BMI1);
1403         // @formatter:on
1404         private final int ext;
1405 
1406         private VexGeneralPurposeRMOp(String opcode, int pp, int mmmmm, int w, int op, int ext, VEXOpAssertion assertion) {
1407             super(opcode, pp, mmmmm, w, op, assertion);
1408             this.ext = ext;
1409         }
1410 
1411         @Override
1412         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src) {
1413             assert assertion.check((AMD64) asm.target.arch, size, dst, null, null);
<span class="line-modified">1414             asm.vexPrefix(AMD64.cpuRegisters[ext], dst, src, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, false);</span>
1415             asm.emitByte(op);
1416             asm.emitModRM(ext, src);
1417         }
1418 
1419         @Override
1420         public void emit(AMD64Assembler asm, AVXSize size, Register dst, AMD64Address src) {
1421             assert assertion.check((AMD64) asm.target.arch, size, dst, null, null);
<span class="line-modified">1422             asm.vexPrefix(AMD64.cpuRegisters[ext], dst, src, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, false);</span>
1423             asm.emitByte(op);
1424             asm.emitOperandHelper(ext, src, 0);
1425         }
1426     }
1427 
1428     /**
1429      * VEX-encoded shift instructions with an operand order of either RVM or VMI.
1430      */
1431     public static final class VexShiftOp extends VexRVMOp implements VexRRIOp {
1432         // @formatter:off
1433         public static final VexShiftOp VPSRLW = new VexShiftOp(&quot;VPSRLW&quot;, P_66, M_0F, WIG, 0xD1, 0x71, 2);
1434         public static final VexShiftOp VPSRLD = new VexShiftOp(&quot;VPSRLD&quot;, P_66, M_0F, WIG, 0xD2, 0x72, 2);
1435         public static final VexShiftOp VPSRLQ = new VexShiftOp(&quot;VPSRLQ&quot;, P_66, M_0F, WIG, 0xD3, 0x73, 2);
1436         public static final VexShiftOp VPSRAW = new VexShiftOp(&quot;VPSRAW&quot;, P_66, M_0F, WIG, 0xE1, 0x71, 4);
1437         public static final VexShiftOp VPSRAD = new VexShiftOp(&quot;VPSRAD&quot;, P_66, M_0F, WIG, 0xE2, 0x72, 4);
1438         public static final VexShiftOp VPSLLW = new VexShiftOp(&quot;VPSLLW&quot;, P_66, M_0F, WIG, 0xF1, 0x71, 6);
1439         public static final VexShiftOp VPSLLD = new VexShiftOp(&quot;VPSLLD&quot;, P_66, M_0F, WIG, 0xF2, 0x72, 6);
1440         public static final VexShiftOp VPSLLQ = new VexShiftOp(&quot;VPSLLQ&quot;, P_66, M_0F, WIG, 0xF3, 0x73, 6);
1441         // @formatter:on
1442 
1443         private final int immOp;
1444         private final int r;
1445 
1446         private VexShiftOp(String opcode, int pp, int mmmmm, int w, int op, int immOp, int r) {
1447             super(opcode, pp, mmmmm, w, op, VEXOpAssertion.AVX1_2);
1448             this.immOp = immOp;
1449             this.r = r;
1450         }
1451 
1452         @Override
1453         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src, int imm8) {
1454             assert assertion.check((AMD64) asm.target.arch, size, null, dst, src);
<span class="line-modified">1455             asm.vexPrefix(null, dst, src, size, pp, mmmmm, w, false);</span>
1456             asm.emitByte(immOp);
1457             asm.emitModRM(r, src);
1458             asm.emitByte(imm8);
1459         }
1460     }
1461 
1462     public static final class VexMaskMoveOp extends VexOp {
1463         // @formatter:off
1464         public static final VexMaskMoveOp VMASKMOVPS = new VexMaskMoveOp(&quot;VMASKMOVPS&quot;, P_66, M_0F38, W0, 0x2C, 0x2E);
1465         public static final VexMaskMoveOp VMASKMOVPD = new VexMaskMoveOp(&quot;VMASKMOVPD&quot;, P_66, M_0F38, W0, 0x2D, 0x2F);
1466         public static final VexMaskMoveOp VPMASKMOVD = new VexMaskMoveOp(&quot;VPMASKMOVD&quot;, P_66, M_0F38, W0, 0x8C, 0x8E, VEXOpAssertion.AVX2);
1467         public static final VexMaskMoveOp VPMASKMOVQ = new VexMaskMoveOp(&quot;VPMASKMOVQ&quot;, P_66, M_0F38, W1, 0x8C, 0x8E, VEXOpAssertion.AVX2);
1468         // @formatter:on
1469 
1470         private final int opReverse;
1471 
1472         private VexMaskMoveOp(String opcode, int pp, int mmmmm, int w, int op, int opReverse) {
1473             this(opcode, pp, mmmmm, w, op, opReverse, VEXOpAssertion.AVX1);
1474         }
1475 
1476         private VexMaskMoveOp(String opcode, int pp, int mmmmm, int w, int op, int opReverse, VEXOpAssertion assertion) {
1477             super(opcode, pp, mmmmm, w, op, assertion);
1478             this.opReverse = opReverse;
1479         }
1480 
1481         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register mask, AMD64Address src) {
1482             assert assertion.check((AMD64) asm.target.arch, size, dst, mask, null);
<span class="line-modified">1483             asm.vexPrefix(dst, mask, src, size, pp, mmmmm, w, false);</span>
1484             asm.emitByte(op);
1485             asm.emitOperandHelper(dst, src, 0);
1486         }
1487 
1488         public void emit(AMD64Assembler asm, AVXSize size, AMD64Address dst, Register mask, Register src) {
1489             assert assertion.check((AMD64) asm.target.arch, size, src, mask, null);
<span class="line-modified">1490             asm.vexPrefix(src, mask, dst, size, pp, mmmmm, w, false);</span>
1491             asm.emitByte(opReverse);
<span class="line-modified">1492             asm.emitOperandHelper(src, dst, 0);</span>
1493         }
1494     }
1495 
1496     /**
1497      * VEX-encoded instructions with an operand order of RVMI.
1498      */
1499     public static final class VexRVMIOp extends VexOp {
1500         // @formatter:off
1501         public static final VexRVMIOp VSHUFPS     = new VexRVMIOp(&quot;VSHUFPS&quot;,     P_,   M_0F,   WIG, 0xC6);
1502         public static final VexRVMIOp VSHUFPD     = new VexRVMIOp(&quot;VSHUFPD&quot;,     P_66, M_0F,   WIG, 0xC6);
1503         public static final VexRVMIOp VINSERTF128 = new VexRVMIOp(&quot;VINSERTF128&quot;, P_66, M_0F3A, W0,  0x18, VEXOpAssertion.AVX1_256ONLY);
1504         public static final VexRVMIOp VINSERTI128 = new VexRVMIOp(&quot;VINSERTI128&quot;, P_66, M_0F3A, W0,  0x38, VEXOpAssertion.AVX2_256ONLY);
1505         // @formatter:on
1506 
1507         private VexRVMIOp(String opcode, int pp, int mmmmm, int w, int op) {
1508             this(opcode, pp, mmmmm, w, op, VEXOpAssertion.AVX1);
1509         }
1510 
1511         private VexRVMIOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1512             super(opcode, pp, mmmmm, w, op, assertion);
1513         }
1514 
1515         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, Register src2, int imm8) {
1516             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, src2);
1517             assert (imm8 &amp; 0xFF) == imm8;
<span class="line-modified">1518             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, false);</span>
1519             asm.emitByte(op);
1520             asm.emitModRM(dst, src2);
1521             asm.emitByte(imm8);
1522         }
1523 
1524         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, AMD64Address src2, int imm8) {
1525             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, null);
1526             assert (imm8 &amp; 0xFF) == imm8;
<span class="line-modified">1527             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, false);</span>
1528             asm.emitByte(op);
<span class="line-modified">1529             asm.emitOperandHelper(dst, src2, 1);</span>
1530             asm.emitByte(imm8);
1531         }
1532     }
1533 
1534     /**
1535      * VEX-encoded comparison operation with an operand order of RVMI. The immediate operand is a
1536      * comparison operator.
1537      */
1538     public static final class VexFloatCompareOp extends VexOp {
1539         // @formatter:off
1540         public static final VexFloatCompareOp VCMPPS = new VexFloatCompareOp(&quot;VCMPPS&quot;, P_,   M_0F, WIG, 0xC2);
1541         public static final VexFloatCompareOp VCMPPD = new VexFloatCompareOp(&quot;VCMPPD&quot;, P_66, M_0F, WIG, 0xC2);
1542         public static final VexFloatCompareOp VCMPSS = new VexFloatCompareOp(&quot;VCMPSS&quot;, P_F2, M_0F, WIG, 0xC2);
1543         public static final VexFloatCompareOp VCMPSD = new VexFloatCompareOp(&quot;VCMPSD&quot;, P_F2, M_0F, WIG, 0xC2);
1544         // @formatter:on
1545 
1546         public enum Predicate {
1547             EQ_OQ(0x00),
1548             LT_OS(0x01),
1549             LE_OS(0x02),
</pre>
<hr />
<pre>
1611                             return LT_OQ;
1612                         case LE:
1613                             return LE_OQ;
1614                         case GT:
1615                             return GT_OQ;
1616                         case GE:
1617                             return GE_OQ;
1618                         default:
1619                             throw GraalError.shouldNotReachHere();
1620                     }
1621                 }
1622             }
1623         }
1624 
1625         private VexFloatCompareOp(String opcode, int pp, int mmmmm, int w, int op) {
1626             super(opcode, pp, mmmmm, w, op, VEXOpAssertion.AVX1);
1627         }
1628 
1629         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, Register src2, Predicate p) {
1630             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, src2);
<span class="line-modified">1631             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, false);</span>
1632             asm.emitByte(op);
1633             asm.emitModRM(dst, src2);
1634             asm.emitByte(p.imm8);
1635         }
1636 
1637         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, AMD64Address src2, Predicate p) {
1638             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, null);
<span class="line-modified">1639             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, false);</span>
1640             asm.emitByte(op);
<span class="line-modified">1641             asm.emitOperandHelper(dst, src2, 1);</span>
1642             asm.emitByte(p.imm8);
1643         }
1644     }
1645 
1646     public final void addl(AMD64Address dst, int imm32) {
1647         ADD.getMIOpcode(DWORD, isByte(imm32)).emit(this, DWORD, dst, imm32);
1648     }
1649 
1650     public final void addl(Register dst, int imm32) {
1651         ADD.getMIOpcode(DWORD, isByte(imm32)).emit(this, DWORD, dst, imm32);
1652     }
1653 
1654     public final void addl(Register dst, Register src) {
1655         ADD.rmOp.emit(this, DWORD, dst, src);
1656     }
1657 
1658     public final void addpd(Register dst, Register src) {
1659         SSEOp.ADD.emit(this, PD, dst, src);
1660     }
1661 
</pre>
<hr />
<pre>
1865             emitByte(0x70 | cc.getValue());
1866             emitByte((int) ((disp - shortSize) &amp; 0xFF));
1867         } else {
1868             // 0000 1111 1000 tttn #32-bit disp
1869             assert isInt(disp - longSize) : &quot;must be 32bit offset (call4)&quot;;
1870             emitByte(0x0F);
1871             emitByte(0x80 | cc.getValue());
1872             emitInt((int) (disp - longSize));
1873         }
1874     }
1875 
1876     public final void jcc(ConditionFlag cc, Label l) {
1877         assert (0 &lt;= cc.getValue()) &amp;&amp; (cc.getValue() &lt; 16) : &quot;illegal cc&quot;;
1878         if (l.isBound()) {
1879             jcc(cc, l.position(), false);
1880         } else {
1881             // Note: could eliminate cond. jumps to this jump if condition
1882             // is the same however, seems to be rather unlikely case.
1883             // Note: use jccb() if label to be bound is very close to get
1884             // an 8-bit displacement
<span class="line-modified">1885             l.addPatchAt(position());</span>
1886             emitByte(0x0F);
1887             emitByte(0x80 | cc.getValue());
1888             emitInt(0);
1889         }
1890 
1891     }
1892 
1893     public final void jccb(ConditionFlag cc, Label l) {
1894         if (l.isBound()) {
1895             int shortSize = 2;
1896             int entry = l.position();
1897             assert isByte(entry - (position() + shortSize)) : &quot;Dispacement too large for a short jmp&quot;;
1898             long disp = entry - position();
1899             // 0111 tttn #8-bit disp
1900             emitByte(0x70 | cc.getValue());
1901             emitByte((int) ((disp - shortSize) &amp; 0xFF));
1902         } else {
<span class="line-modified">1903             l.addPatchAt(position());</span>
1904             emitByte(0x70 | cc.getValue());
1905             emitByte(0);
1906         }
1907     }
1908 
1909     public final void jmp(int jumpTarget, boolean forceDisp32) {
1910         int shortSize = 2;
1911         int longSize = 5;
1912         long disp = jumpTarget - position();
1913         if (!forceDisp32 &amp;&amp; isByte(disp - shortSize)) {
1914             emitByte(0xEB);
1915             emitByte((int) ((disp - shortSize) &amp; 0xFF));
1916         } else {
1917             emitByte(0xE9);
1918             emitInt((int) (disp - longSize));
1919         }
1920     }
1921 
1922     @Override
1923     public final void jmp(Label l) {
1924         if (l.isBound()) {
1925             jmp(l.position(), false);
1926         } else {
1927             // By default, forward jumps are always 32-bit displacements, since
1928             // we can&#39;t yet know where the label will be bound. If you&#39;re sure that
1929             // the forward jump will not run beyond 256 bytes, use jmpb to
1930             // force an 8-bit displacement.
1931 
<span class="line-modified">1932             l.addPatchAt(position());</span>
1933             emitByte(0xE9);
1934             emitInt(0);
1935         }
1936     }
1937 
1938     public final void jmp(Register entry) {
1939         prefix(entry);
1940         emitByte(0xFF);
1941         emitModRM(4, entry);
1942     }
1943 
1944     public final void jmp(AMD64Address adr) {
1945         prefix(adr);
1946         emitByte(0xFF);
1947         emitOperandHelper(AMD64.rsp, adr, 0);
1948     }
1949 
1950     public final void jmpb(Label l) {
1951         if (l.isBound()) {
1952             int shortSize = 2;
<span class="line-modified">1953             int entry = l.position();</span>
<span class="line-modified">1954             assert isByte((entry - position()) + shortSize) : &quot;Dispacement too large for a short jmp&quot;;</span>
<span class="line-modified">1955             long offs = entry - position();</span>
1956             emitByte(0xEB);
<span class="line-modified">1957             emitByte((int) ((offs - shortSize) &amp; 0xFF));</span>
1958         } else {
<span class="line-modified">1959 </span>
<span class="line-removed">1960             l.addPatchAt(position());</span>
1961             emitByte(0xEB);
1962             emitByte(0);
1963         }
1964     }
1965 
1966     public final void lead(Register dst, AMD64Address src) {
1967         prefix(src, dst);
1968         emitByte(0x8D);
1969         emitOperandHelper(dst, src, 0);
1970     }
1971 
1972     public final void leaq(Register dst, AMD64Address src) {
1973         prefixq(src, dst);
1974         emitByte(0x8D);
1975         emitOperandHelper(dst, src, 0);
1976     }
1977 
1978     public final void leave() {
1979         emitByte(0xC9);
1980     }
</pre>
<hr />
<pre>
2277     public final void nop() {
2278         nop(1);
2279     }
2280 
2281     public void nop(int count) {
2282         int i = count;
2283         if (UseNormalNop) {
2284             assert i &gt; 0 : &quot; &quot;;
2285             // The fancy nops aren&#39;t currently recognized by debuggers making it a
2286             // pain to disassemble code while debugging. If assert are on clearly
2287             // speed is not an issue so simply use the single byte traditional nop
2288             // to do alignment.
2289 
2290             for (; i &gt; 0; i--) {
2291                 emitByte(0x90);
2292             }
2293             return;
2294         }
2295 
2296         if (UseAddressNop) {
<span class="line-modified">2297             //</span>
<span class="line-modified">2298             // Using multi-bytes nops &quot;0x0F 0x1F [Address]&quot; for AMD.</span>
<span class="line-modified">2299             // 1: 0x90</span>
<span class="line-modified">2300             // 2: 0x66 0x90</span>
<span class="line-removed">2301             // 3: 0x66 0x66 0x90 (don&#39;t use &quot;0x0F 0x1F 0x00&quot; - need patching safe padding)</span>
<span class="line-removed">2302             // 4: 0x0F 0x1F 0x40 0x00</span>
<span class="line-removed">2303             // 5: 0x0F 0x1F 0x44 0x00 0x00</span>
<span class="line-removed">2304             // 6: 0x66 0x0F 0x1F 0x44 0x00 0x00</span>
<span class="line-removed">2305             // 7: 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00</span>
<span class="line-removed">2306             // 8: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00</span>
<span class="line-removed">2307             // 9: 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00</span>
<span class="line-removed">2308             // 10: 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00</span>
<span class="line-removed">2309             // 11: 0x66 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00</span>
<span class="line-removed">2310 </span>
<span class="line-removed">2311             // The rest coding is AMD specific - use consecutive Address nops</span>
<span class="line-removed">2312 </span>
<span class="line-removed">2313             // 12: 0x66 0x0F 0x1F 0x44 0x00 0x00 0x66 0x0F 0x1F 0x44 0x00 0x00</span>
<span class="line-removed">2314             // 13: 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00 0x66 0x0F 0x1F 0x44 0x00 0x00</span>
<span class="line-removed">2315             // 14: 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00</span>
<span class="line-removed">2316             // 15: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00</span>
<span class="line-removed">2317             // 16: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00</span>
<span class="line-removed">2318             // Size prefixes (0x66) are added for larger sizes</span>
<span class="line-removed">2319 </span>
<span class="line-removed">2320             while (i &gt;= 22) {</span>
<span class="line-removed">2321                 i -= 11;</span>
<span class="line-removed">2322                 emitByte(0x66); // size prefix</span>
<span class="line-removed">2323                 emitByte(0x66); // size prefix</span>
<span class="line-removed">2324                 emitByte(0x66); // size prefix</span>
<span class="line-removed">2325                 addrNop8();</span>
<span class="line-removed">2326             }</span>
<span class="line-removed">2327             // Generate first nop for size between 21-12</span>
<span class="line-removed">2328             switch (i) {</span>
<span class="line-removed">2329                 case 21:</span>
<span class="line-removed">2330                     i -= 11;</span>
<span class="line-removed">2331                     emitByte(0x66); // size prefix</span>
<span class="line-removed">2332                     emitByte(0x66); // size prefix</span>
<span class="line-removed">2333                     emitByte(0x66); // size prefix</span>
<span class="line-removed">2334                     addrNop8();</span>
<span class="line-removed">2335                     break;</span>
<span class="line-removed">2336                 case 20:</span>
<span class="line-removed">2337                 case 19:</span>
<span class="line-removed">2338                     i -= 10;</span>
<span class="line-removed">2339                     emitByte(0x66); // size prefix</span>
<span class="line-removed">2340                     emitByte(0x66); // size prefix</span>
<span class="line-removed">2341                     addrNop8();</span>
<span class="line-removed">2342                     break;</span>
<span class="line-removed">2343                 case 18:</span>
<span class="line-removed">2344                 case 17:</span>
<span class="line-removed">2345                     i -= 9;</span>
<span class="line-removed">2346                     emitByte(0x66); // size prefix</span>
<span class="line-removed">2347                     addrNop8();</span>
<span class="line-removed">2348                     break;</span>
<span class="line-removed">2349                 case 16:</span>
<span class="line-removed">2350                 case 15:</span>
<span class="line-removed">2351                     i -= 8;</span>
<span class="line-removed">2352                     addrNop8();</span>
<span class="line-removed">2353                     break;</span>
<span class="line-removed">2354                 case 14:</span>
<span class="line-removed">2355                 case 13:</span>
<span class="line-removed">2356                     i -= 7;</span>
<span class="line-removed">2357                     addrNop7();</span>
<span class="line-removed">2358                     break;</span>
<span class="line-removed">2359                 case 12:</span>
<span class="line-removed">2360                     i -= 6;</span>
<span class="line-removed">2361                     emitByte(0x66); // size prefix</span>
<span class="line-removed">2362                     addrNop5();</span>
<span class="line-removed">2363                     break;</span>
<span class="line-removed">2364                 default:</span>
<span class="line-removed">2365                     assert i &lt; 12;</span>
<span class="line-removed">2366             }</span>
<span class="line-removed">2367 </span>
<span class="line-removed">2368             // Generate second nop for size between 11-1</span>
<span class="line-removed">2369             switch (i) {</span>
<span class="line-removed">2370                 case 11:</span>
<span class="line-removed">2371                     emitByte(0x66); // size prefix</span>
<span class="line-removed">2372                     emitByte(0x66); // size prefix</span>
<span class="line-removed">2373                     emitByte(0x66); // size prefix</span>
<span class="line-removed">2374                     addrNop8();</span>
<span class="line-removed">2375                     break;</span>
<span class="line-removed">2376                 case 10:</span>
<span class="line-removed">2377                     emitByte(0x66); // size prefix</span>
<span class="line-removed">2378                     emitByte(0x66); // size prefix</span>
<span class="line-removed">2379                     addrNop8();</span>
<span class="line-removed">2380                     break;</span>
<span class="line-removed">2381                 case 9:</span>
<span class="line-removed">2382                     emitByte(0x66); // size prefix</span>
<span class="line-removed">2383                     addrNop8();</span>
<span class="line-removed">2384                     break;</span>
<span class="line-removed">2385                 case 8:</span>
<span class="line-removed">2386                     addrNop8();</span>
<span class="line-removed">2387                     break;</span>
<span class="line-removed">2388                 case 7:</span>
<span class="line-removed">2389                     addrNop7();</span>
<span class="line-removed">2390                     break;</span>
<span class="line-removed">2391                 case 6:</span>
<span class="line-removed">2392                     emitByte(0x66); // size prefix</span>
<span class="line-removed">2393                     addrNop5();</span>
<span class="line-removed">2394                     break;</span>
<span class="line-removed">2395                 case 5:</span>
<span class="line-removed">2396                     addrNop5();</span>
<span class="line-removed">2397                     break;</span>
<span class="line-removed">2398                 case 4:</span>
<span class="line-removed">2399                     addrNop4();</span>
<span class="line-removed">2400                     break;</span>
<span class="line-removed">2401                 case 3:</span>
<span class="line-removed">2402                     // Don&#39;t use &quot;0x0F 0x1F 0x00&quot; - need patching safe padding</span>
<span class="line-removed">2403                     emitByte(0x66); // size prefix</span>
<span class="line-removed">2404                     emitByte(0x66); // size prefix</span>
<span class="line-removed">2405                     emitByte(0x90); // nop</span>
<span class="line-removed">2406                     break;</span>
<span class="line-removed">2407                 case 2:</span>
<span class="line-removed">2408                     emitByte(0x66); // size prefix</span>
<span class="line-removed">2409                     emitByte(0x90); // nop</span>
<span class="line-removed">2410                     break;</span>
<span class="line-removed">2411                 case 1:</span>
<span class="line-removed">2412                     emitByte(0x90); // nop</span>
<span class="line-removed">2413                     break;</span>
<span class="line-removed">2414                 default:</span>
<span class="line-removed">2415                     assert i == 0;</span>
2416             }
2417             return;
2418         }
2419 
2420         // Using nops with size prefixes &quot;0x66 0x90&quot;.
2421         // From AMD Optimization Guide:
2422         // 1: 0x90
2423         // 2: 0x66 0x90
2424         // 3: 0x66 0x66 0x90
2425         // 4: 0x66 0x66 0x66 0x90
2426         // 5: 0x66 0x66 0x90 0x66 0x90
2427         // 6: 0x66 0x66 0x90 0x66 0x66 0x90
2428         // 7: 0x66 0x66 0x66 0x90 0x66 0x66 0x90
2429         // 8: 0x66 0x66 0x66 0x90 0x66 0x66 0x66 0x90
2430         // 9: 0x66 0x66 0x90 0x66 0x66 0x90 0x66 0x66 0x90
2431         // 10: 0x66 0x66 0x66 0x90 0x66 0x66 0x90 0x66 0x66 0x90
2432         //
2433         while (i &gt; 12) {
2434             i -= 4;
2435             emitByte(0x66); // size prefix
</pre>
<hr />
<pre>
2466                 emitByte(0x66);
2467                 emitByte(0x90);
2468                 break;
2469             case 3:
2470                 emitByte(0x66);
2471                 emitByte(0x66);
2472                 emitByte(0x90);
2473                 break;
2474             case 2:
2475                 emitByte(0x66);
2476                 emitByte(0x90);
2477                 break;
2478             case 1:
2479                 emitByte(0x90);
2480                 break;
2481             default:
2482                 assert i == 0;
2483         }
2484     }
2485 
























































































































































































































2486     public final void orl(Register dst, Register src) {
2487         OR.rmOp.emit(this, DWORD, dst, src);
2488     }
2489 
2490     public final void orl(Register dst, int imm32) {
2491         OR.getMIOpcode(DWORD, isByte(imm32)).emit(this, DWORD, dst, imm32);
2492     }
2493 
2494     // Insn: VPACKUSWB xmm1, xmm2, xmm3/m128
2495     // -----
2496     // Insn: VPACKUSWB xmm1, xmm1, xmm2
2497 
2498     public final void packuswb(Register dst, Register src) {
2499         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
2500         // Code: VEX.NDS.128.66.0F.WIG 67 /r
2501         simdPrefix(dst, dst, src, PD, P_0F, false);
2502         emitByte(0x67);
2503         emitModRM(dst, src);
2504     }
2505 
</pre>
<hr />
<pre>
2936         emitByte(0x14);
2937         emitModRM(dst, src);
2938     }
2939 
2940     public final void xorl(Register dst, Register src) {
2941         XOR.rmOp.emit(this, DWORD, dst, src);
2942     }
2943 
2944     public final void xorq(Register dst, Register src) {
2945         XOR.rmOp.emit(this, QWORD, dst, src);
2946     }
2947 
2948     public final void xorpd(Register dst, Register src) {
2949         SSEOp.XOR.emit(this, PD, dst, src);
2950     }
2951 
2952     public final void xorps(Register dst, Register src) {
2953         SSEOp.XOR.emit(this, PS, dst, src);
2954     }
2955 
<span class="line-modified">2956     protected final void decl(Register dst) {</span>
2957         // Use two-byte form (one-byte form is a REX prefix in 64-bit mode)
2958         prefix(dst);
2959         emitByte(0xFF);
2960         emitModRM(1, dst);
2961     }
2962 
<span class="line-modified">2963     protected final void incl(Register dst) {</span>
2964         // Use two-byte form (one-byte from is a REX prefix in 64-bit mode)
2965         prefix(dst);
2966         emitByte(0xFF);
2967         emitModRM(0, dst);
2968     }
2969 
2970     public final void addq(Register dst, int imm32) {
2971         ADD.getMIOpcode(QWORD, isByte(imm32)).emit(this, QWORD, dst, imm32);
2972     }
2973 
2974     public final void addq(AMD64Address dst, int imm32) {
2975         ADD.getMIOpcode(QWORD, isByte(imm32)).emit(this, QWORD, dst, imm32);
2976     }
2977 
2978     public final void addq(Register dst, Register src) {
2979         ADD.rmOp.emit(this, QWORD, dst, src);
2980     }
2981 
2982     public final void addq(AMD64Address dst, Register src) {
2983         ADD.mrOp.emit(this, QWORD, dst, src);
</pre>
<hr />
<pre>
2988     }
2989 
2990     public final void bsrq(Register dst, Register src) {
2991         prefixq(dst, src);
2992         emitByte(0x0F);
2993         emitByte(0xBD);
2994         emitModRM(dst, src);
2995     }
2996 
2997     public final void bswapq(Register reg) {
2998         prefixq(reg);
2999         emitByte(0x0F);
3000         emitByte(0xC8 + encode(reg));
3001     }
3002 
3003     public final void cdqq() {
3004         rexw();
3005         emitByte(0x99);
3006     }
3007 












3008     public final void cmovq(ConditionFlag cc, Register dst, Register src) {
3009         prefixq(dst, src);
3010         emitByte(0x0F);
3011         emitByte(0x40 | cc.getValue());
3012         emitModRM(dst, src);
3013     }
3014 
3015     public final void setb(ConditionFlag cc, Register dst) {
3016         prefix(dst, true);
3017         emitByte(0x0F);
3018         emitByte(0x90 | cc.getValue());
3019         emitModRM(0, dst);
3020     }
3021 
3022     public final void cmovq(ConditionFlag cc, Register dst, AMD64Address src) {
3023         prefixq(src, dst);
3024         emitByte(0x0F);
3025         emitByte(0x40 | cc.getValue());
3026         emitOperandHelper(dst, src, 0);
3027     }
</pre>
<hr />
<pre>
3380                         || op == 0x00 // jump table entry
3381                         || op == 0xE9 // jmp
3382                         || op == 0xEB // short jmp
3383                         || (op &amp; 0xF0) == 0x70 // short jcc
3384                         || op == 0x0F &amp;&amp; (getByte(branch + 1) &amp; 0xF0) == 0x80 // jcc
3385         : &quot;Invalid opcode at patch point branch=&quot; + branch + &quot;, branchTarget=&quot; + branchTarget + &quot;, op=&quot; + op;
3386 
3387         if (op == 0x00) {
3388             int offsetToJumpTableBase = getShort(branch + 1);
3389             int jumpTableBase = branch - offsetToJumpTableBase;
3390             int imm32 = branchTarget - jumpTableBase;
3391             emitInt(imm32, branch);
3392         } else if (op == 0xEB || (op &amp; 0xF0) == 0x70) {
3393 
3394             // short offset operators (jmp and jcc)
3395             final int imm8 = branchTarget - (branch + 2);
3396             /*
3397              * Since a wrongly patched short branch can potentially lead to working but really bad
3398              * behaving code we should always fail with an exception instead of having an assert.
3399              */
<span class="line-modified">3400             if (!NumUtil.isByte(imm8)) {</span>
<span class="line-removed">3401                 throw new InternalError(&quot;branch displacement out of range: &quot; + imm8);</span>
<span class="line-removed">3402             }</span>
3403             emitByte(imm8, branch + 1);
3404 
3405         } else {
3406 
3407             int off = 1;
3408             if (op == 0x0F) {
3409                 off = 2;
3410             }
3411 
3412             int imm32 = branchTarget - (branch + 4 + off);
3413             emitInt(imm32, branch + off);
3414         }
3415     }
3416 
3417     public void nullCheck(AMD64Address address) {
3418         testl(AMD64.rax, address);
3419     }
3420 
3421     @Override
3422     public void align(int modulus) {
</pre>
<hr />
<pre>
3618 
3619     public void lfence() {
3620         emitByte(0x0f);
3621         emitByte(0xae);
3622         emitByte(0xe8);
3623     }
3624 
3625     public final void vptest(Register dst, Register src) {
3626         VexRMOp.VPTEST.emit(this, AVXSize.YMM, dst, src);
3627     }
3628 
3629     public final void vpxor(Register dst, Register nds, Register src) {
3630         VexRVMOp.VPXOR.emit(this, AVXSize.YMM, dst, nds, src);
3631     }
3632 
3633     public final void vpxor(Register dst, Register nds, AMD64Address src) {
3634         VexRVMOp.VPXOR.emit(this, AVXSize.YMM, dst, nds, src);
3635     }
3636 
3637     public final void vmovdqu(Register dst, AMD64Address src) {
<span class="line-modified">3638         VexMoveOp.VMOVDQU.emit(this, AVXSize.YMM, dst, src);</span>
3639     }
3640 
3641     public final void vmovdqu(AMD64Address dst, Register src) {
3642         assert inRC(XMM, src);
<span class="line-modified">3643         VexMoveOp.VMOVDQU.emit(this, AVXSize.YMM, dst, src);</span>
3644     }
3645 
3646     public final void vpmovzxbw(Register dst, AMD64Address src) {
3647         assert supports(CPUFeature.AVX2);
3648         VexRMOp.VPMOVZXBW.emit(this, AVXSize.YMM, dst, src);
3649     }
3650 
3651     public final void vzeroupper() {
3652         emitVEX(L128, P_, M_0F, W0, 0, 0, true);
3653         emitByte(0x77);
3654     }
3655 
3656     // Insn: KORTESTD k1, k2
3657 
3658     // This instruction produces ZF or CF flags
3659     public final void kortestd(Register src1, Register src2) {
3660         assert supports(CPUFeature.AVX512BW);
3661         assert inRC(MASK, src1) &amp;&amp; inRC(MASK, src2);
3662         // Code: VEX.L0.66.0F.W1 98 /r
<span class="line-modified">3663         vexPrefix(src1, Register.None, src2, AVXSize.XMM, P_66, M_0F, W1, true);</span>
3664         emitByte(0x98);
3665         emitModRM(src1, src2);
3666     }
3667 
3668     // Insn: KORTESTQ k1, k2
3669 
3670     // This instruction produces ZF or CF flags
3671     public final void kortestq(Register src1, Register src2) {
3672         assert supports(CPUFeature.AVX512BW);
3673         assert inRC(MASK, src1) &amp;&amp; inRC(MASK, src2);
3674         // Code: VEX.L0.0F.W1 98 /r
<span class="line-modified">3675         vexPrefix(src1, Register.None, src2, AVXSize.XMM, P_, M_0F, W1, true);</span>
3676         emitByte(0x98);
3677         emitModRM(src1, src2);
3678     }
3679 
3680     public final void kmovd(Register dst, Register src) {
3681         assert supports(CPUFeature.AVX512BW);
3682         assert inRC(MASK, dst) || inRC(CPU, dst);
3683         assert inRC(MASK, src) || inRC(CPU, src);
3684         assert !(inRC(CPU, dst) &amp;&amp; inRC(CPU, src));
3685 
3686         if (inRC(MASK, dst)) {
3687             if (inRC(MASK, src)) {
3688                 // kmovd(KRegister dst, KRegister src):
3689                 // Insn: KMOVD k1, k2/m32
3690                 // Code: VEX.L0.66.0F.W1 90 /r
<span class="line-modified">3691                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_66, M_0F, W1, true);</span>
3692                 emitByte(0x90);
3693                 emitModRM(dst, src);
3694             } else {
3695                 // kmovd(KRegister dst, Register src)
3696                 // Insn: KMOVD k1, r32
3697                 // Code: VEX.L0.F2.0F.W0 92 /r
<span class="line-modified">3698                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_F2, M_0F, W0, true);</span>
3699                 emitByte(0x92);
3700                 emitModRM(dst, src);
3701             }
3702         } else {
3703             if (inRC(MASK, src)) {
3704                 // kmovd(Register dst, KRegister src)
3705                 // Insn: KMOVD r32, k1
3706                 // Code: VEX.L0.F2.0F.W0 93 /r
<span class="line-modified">3707                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_F2, M_0F, W0, true);</span>
3708                 emitByte(0x93);
3709                 emitModRM(dst, src);
3710             } else {
3711                 throw GraalError.shouldNotReachHere();
3712             }
3713         }
3714     }
3715 
3716     public final void kmovq(Register dst, Register src) {
3717         assert supports(CPUFeature.AVX512BW);
3718         assert inRC(MASK, dst) || inRC(CPU, dst);
3719         assert inRC(MASK, src) || inRC(CPU, src);
3720         assert !(inRC(CPU, dst) &amp;&amp; inRC(CPU, src));
3721 
3722         if (inRC(MASK, dst)) {
3723             if (inRC(MASK, src)) {
3724                 // kmovq(KRegister dst, KRegister src):
3725                 // Insn: KMOVQ k1, k2/m64
3726                 // Code: VEX.L0.0F.W1 90 /r
<span class="line-modified">3727                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_, M_0F, W1, true);</span>
3728                 emitByte(0x90);
3729                 emitModRM(dst, src);
3730             } else {
3731                 // kmovq(KRegister dst, Register src)
3732                 // Insn: KMOVQ k1, r64
3733                 // Code: VEX.L0.F2.0F.W1 92 /r
<span class="line-modified">3734                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_F2, M_0F, W1, true);</span>
3735                 emitByte(0x92);
3736                 emitModRM(dst, src);
3737             }
3738         } else {
3739             if (inRC(MASK, src)) {
3740                 // kmovq(Register dst, KRegister src)
3741                 // Insn: KMOVQ r64, k1
3742                 // Code: VEX.L0.F2.0F.W1 93 /r
<span class="line-modified">3743                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_F2, M_0F, W1, true);</span>
3744                 emitByte(0x93);
3745                 emitModRM(dst, src);
3746             } else {
3747                 throw GraalError.shouldNotReachHere();
3748             }
3749         }
3750     }
3751 
3752     // Insn: KTESTD k1, k2
3753 
3754     public final void ktestd(Register src1, Register src2) {
3755         assert supports(CPUFeature.AVX512BW);
3756         assert inRC(MASK, src1) &amp;&amp; inRC(MASK, src2);
3757         // Code: VEX.L0.66.0F.W1 99 /r
<span class="line-modified">3758         vexPrefix(src1, Register.None, src2, AVXSize.XMM, P_66, M_0F, W1, true);</span>
3759         emitByte(0x99);
3760         emitModRM(src1, src2);
3761     }
3762 
3763     public final void evmovdqu64(Register dst, AMD64Address src) {
3764         assert supports(CPUFeature.AVX512F);
3765         assert inRC(XMM, dst);
3766         evexPrefix(dst, Register.None, Register.None, src, AVXSize.ZMM, P_F3, M_0F, W1, Z0, B0);
3767         emitByte(0x6F);
<span class="line-modified">3768         emitEVEXOperandHelper(dst, src, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
3769     }
3770 
3771     // Insn: VPMOVZXBW zmm1, m256
3772 
3773     public final void evpmovzxbw(Register dst, AMD64Address src) {
3774         assert supports(CPUFeature.AVX512BW);
3775         assert inRC(XMM, dst);
3776         // Code: EVEX.512.66.0F38.WIG 30 /r
3777         evexPrefix(dst, Register.None, Register.None, src, AVXSize.ZMM, P_66, M_0F38, WIG, Z0, B0);
3778         emitByte(0x30);
<span class="line-modified">3779         emitEVEXOperandHelper(dst, src, 0, EVEXTuple.HVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
3780     }
3781 
3782     public final void evpcmpeqb(Register kdst, Register nds, AMD64Address src) {
3783         assert supports(CPUFeature.AVX512BW);
3784         assert inRC(MASK, kdst) &amp;&amp; inRC(XMM, nds);
3785         evexPrefix(kdst, Register.None, nds, src, AVXSize.ZMM, P_66, M_0F, WIG, Z0, B0);
3786         emitByte(0x74);
<span class="line-modified">3787         emitEVEXOperandHelper(kdst, src, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
3788     }
3789 
3790     // Insn: VMOVDQU16 zmm1 {k1}{z}, zmm2/m512
3791     // -----
3792     // Insn: VMOVDQU16 zmm1, m512
3793 
3794     public final void evmovdqu16(Register dst, AMD64Address src) {
3795         assert supports(CPUFeature.AVX512BW);
3796         assert inRC(XMM, dst);
3797         // Code: EVEX.512.F2.0F.W1 6F /r
3798         evexPrefix(dst, Register.None, Register.None, src, AVXSize.ZMM, P_F2, M_0F, W1, Z0, B0);
3799         emitByte(0x6F);
<span class="line-modified">3800         emitEVEXOperandHelper(dst, src, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
3801     }
3802 
3803     // Insn: VMOVDQU16 zmm1, k1:z, m512
3804 
3805     public final void evmovdqu16(Register dst, Register mask, AMD64Address src) {
3806         assert supports(CPUFeature.AVX512BW);
3807         assert inRC(XMM, dst) &amp;&amp; inRC(MASK, mask);
3808         // Code: EVEX.512.F2.0F.W1 6F /r
3809         evexPrefix(dst, mask, Register.None, src, AVXSize.ZMM, P_F2, M_0F, W1, Z1, B0);
3810         emitByte(0x6F);
<span class="line-modified">3811         emitEVEXOperandHelper(dst, src, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
3812     }
3813 
3814     // Insn: VMOVDQU16 zmm2/m512 {k1}{z}, zmm1
3815     // -----
3816     // Insn: VMOVDQU16 m512, zmm1
3817 
3818     public final void evmovdqu16(AMD64Address dst, Register src) {
3819         assert supports(CPUFeature.AVX512BW);
3820         assert inRC(XMM, src);
3821         // Code: EVEX.512.F2.0F.W1 7F /r
3822         evexPrefix(src, Register.None, Register.None, dst, AVXSize.ZMM, P_F2, M_0F, W1, Z0, B0);
3823         emitByte(0x7F);
<span class="line-modified">3824         emitEVEXOperandHelper(src, dst, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
3825     }
3826 
3827     // Insn: VMOVDQU16 m512, k1, zmm1
3828 
3829     public final void evmovdqu16(AMD64Address dst, Register mask, Register src) {
3830         assert supports(CPUFeature.AVX512BW);
3831         assert inRC(MASK, mask) &amp;&amp; inRC(XMM, src);
3832         // Code: EVEX.512.F2.0F.W1 7F /r
3833         evexPrefix(src, mask, Register.None, dst, AVXSize.ZMM, P_F2, M_0F, W1, Z0, B0);
3834         emitByte(0x7F);
<span class="line-modified">3835         emitEVEXOperandHelper(src, dst, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
3836     }
3837 
3838     // Insn: VPBROADCASTW zmm1 {k1}{z}, reg
3839     // -----
3840     // Insn: VPBROADCASTW zmm1, reg
3841 
3842     public final void evpbroadcastw(Register dst, Register src) {
3843         assert supports(CPUFeature.AVX512BW);
3844         assert inRC(XMM, dst) &amp;&amp; inRC(CPU, src);
3845         // Code: EVEX.512.66.0F38.W0 7B /r
3846         evexPrefix(dst, Register.None, Register.None, src, AVXSize.ZMM, P_66, M_0F38, W0, Z0, B0);
3847         emitByte(0x7B);
3848         emitModRM(dst, src);
3849     }
3850 
3851     // Insn: VPCMPUW k1 {k2}, zmm2, zmm3/m512, imm8
3852     // -----
3853     // Insn: VPCMPUW k1, zmm2, zmm3, imm8
3854 
3855     public final void evpcmpuw(Register kdst, Register nds, Register src, int vcc) {
</pre>
<hr />
<pre>
3870         assert supports(CPUFeature.AVX512BW);
3871         assert inRC(MASK, kdst) &amp;&amp; inRC(MASK, mask);
3872         assert inRC(XMM, nds) &amp;&amp; inRC(XMM, src);
3873         // Code: EVEX.NDS.512.66.0F3A.W1 3E /r ib
3874         evexPrefix(kdst, mask, nds, src, AVXSize.ZMM, P_66, M_0F3A, W1, Z0, B0);
3875         emitByte(0x3E);
3876         emitModRM(kdst, src);
3877         emitByte(vcc);
3878     }
3879 
3880     // Insn: VPMOVWB ymm1/m256 {k1}{z}, zmm2
3881     // -----
3882     // Insn: VPMOVWB m256, zmm2
3883 
3884     public final void evpmovwb(AMD64Address dst, Register src) {
3885         assert supports(CPUFeature.AVX512BW);
3886         assert inRC(XMM, src);
3887         // Code: EVEX.512.F3.0F38.W0 30 /r
3888         evexPrefix(src, Register.None, Register.None, dst, AVXSize.ZMM, P_F3, M_0F38, W0, Z0, B0);
3889         emitByte(0x30);
<span class="line-modified">3890         emitEVEXOperandHelper(src, dst, 0, EVEXTuple.HVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
3891     }
3892 
3893     // Insn: VPMOVWB m256, k1, zmm2
3894 
3895     public final void evpmovwb(AMD64Address dst, Register mask, Register src) {
3896         assert supports(CPUFeature.AVX512BW);
3897         assert inRC(MASK, mask) &amp;&amp; inRC(XMM, src);
3898         // Code: EVEX.512.F3.0F38.W0 30 /r
3899         evexPrefix(src, mask, Register.None, dst, AVXSize.ZMM, P_F3, M_0F38, W0, Z0, B0);
3900         emitByte(0x30);
<span class="line-modified">3901         emitEVEXOperandHelper(src, dst, 0, EVEXTuple.HVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
3902     }
3903 
3904     // Insn: VPMOVZXBW zmm1 {k1}{z}, ymm2/m256
3905     // -----
3906     // Insn: VPMOVZXBW zmm1, k1, m256
3907 
3908     public final void evpmovzxbw(Register dst, Register mask, AMD64Address src) {
3909         assert supports(CPUFeature.AVX512BW);
3910         assert inRC(MASK, mask) &amp;&amp; inRC(XMM, dst);
3911         // Code: EVEX.512.66.0F38.WIG 30 /r
3912         evexPrefix(dst, mask, Register.None, src, AVXSize.ZMM, P_66, M_0F38, WIG, Z0, B0);
3913         emitByte(0x30);
<span class="line-modified">3914         emitEVEXOperandHelper(dst, src, 0, EVEXTuple.HVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
3915     }
3916 
3917 }
</pre>
</td>
<td>
<hr />
<pre>
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  */
  23 
  24 
  25 package org.graalvm.compiler.asm.amd64;
  26 
  27 import static jdk.vm.ci.amd64.AMD64.CPU;
  28 import static jdk.vm.ci.amd64.AMD64.MASK;
  29 import static jdk.vm.ci.amd64.AMD64.XMM;
<span class="line-added">  30 import static jdk.vm.ci.amd64.AMD64.CPUFeature.AVX512BW;</span>
<span class="line-added">  31 import static jdk.vm.ci.amd64.AMD64.CPUFeature.AVX512CD;</span>
<span class="line-added">  32 import static jdk.vm.ci.amd64.AMD64.CPUFeature.AVX512DQ;</span>
<span class="line-added">  33 import static jdk.vm.ci.amd64.AMD64.CPUFeature.AVX512F;</span>
<span class="line-added">  34 import static jdk.vm.ci.amd64.AMD64.CPUFeature.AVX512VL;</span>
  35 import static jdk.vm.ci.code.MemoryBarriers.STORE_LOAD;
  36 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseAddressNop;
<span class="line-added">  37 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseIntelNops;</span>
  38 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseNormalNop;
  39 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.ADD;
  40 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.AND;
  41 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.CMP;
  42 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.OR;
  43 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.SBB;
  44 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.SUB;
  45 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.XOR;
  46 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MOp.DEC;
  47 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MOp.INC;
  48 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MOp.NEG;
  49 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MOp.NOT;
  50 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.EVEXPrefixConfig.B0;
  51 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.EVEXPrefixConfig.Z0;
  52 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.EVEXPrefixConfig.Z1;
  53 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.BYTE;
  54 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.DWORD;
  55 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.PD;
  56 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.PS;
  57 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.QWORD;
  58 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.SD;
  59 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.SS;
  60 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.WORD;
  61 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.L128;
  62 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.L256;
<span class="line-added">  63 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.L512;</span>
  64 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.LZ;
  65 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.M_0F;
  66 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.M_0F38;
  67 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.M_0F3A;
  68 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.P_;
  69 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.P_66;
  70 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.P_F2;
  71 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.P_F3;
  72 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.W0;
  73 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.W1;
  74 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.WIG;
  75 import static org.graalvm.compiler.core.common.NumUtil.isByte;
  76 import static org.graalvm.compiler.core.common.NumUtil.isInt;
  77 import static org.graalvm.compiler.core.common.NumUtil.isShiftCount;
  78 import static org.graalvm.compiler.core.common.NumUtil.isUByte;
  79 
  80 import java.util.EnumSet;
  81 
  82 import org.graalvm.compiler.asm.Label;
  83 import org.graalvm.compiler.asm.amd64.AMD64Address.Scale;
  84 import org.graalvm.compiler.asm.amd64.AVXKind.AVXSize;

  85 import org.graalvm.compiler.core.common.calc.Condition;
  86 import org.graalvm.compiler.debug.GraalError;
  87 
  88 import jdk.vm.ci.amd64.AMD64;
  89 import jdk.vm.ci.amd64.AMD64.CPUFeature;
  90 import jdk.vm.ci.code.Register;
  91 import jdk.vm.ci.code.Register.RegisterCategory;
  92 import jdk.vm.ci.code.TargetDescription;
  93 
  94 /**
  95  * This class implements an assembler that can encode most X86 instructions.
  96  */
  97 public class AMD64Assembler extends AMD64BaseAssembler {
  98 
  99     /**
 100      * Constructs an assembler for the AMD64 architecture.
 101      */
 102     public AMD64Assembler(TargetDescription target) {
 103         super(target);
 104     }
</pre>
<hr />
<pre>
 884         public static final AMD64Shift ROL = new AMD64Shift(&quot;ROL&quot;, 0);
 885         public static final AMD64Shift ROR = new AMD64Shift(&quot;ROR&quot;, 1);
 886         public static final AMD64Shift RCL = new AMD64Shift(&quot;RCL&quot;, 2);
 887         public static final AMD64Shift RCR = new AMD64Shift(&quot;RCR&quot;, 3);
 888         public static final AMD64Shift SHL = new AMD64Shift(&quot;SHL&quot;, 4);
 889         public static final AMD64Shift SHR = new AMD64Shift(&quot;SHR&quot;, 5);
 890         public static final AMD64Shift SAR = new AMD64Shift(&quot;SAR&quot;, 7);
 891         // @formatter:on
 892 
 893         public final AMD64MOp m1Op;
 894         public final AMD64MOp mcOp;
 895         public final AMD64MIOp miOp;
 896 
 897         private AMD64Shift(String opcode, int code) {
 898             m1Op = new AMD64MOp(opcode, 0, 0xD1, code, OpAssertion.WordOrLargerAssertion);
 899             mcOp = new AMD64MOp(opcode, 0, 0xD3, code, OpAssertion.WordOrLargerAssertion);
 900             miOp = new AMD64MIOp(opcode, true, 0, 0xC1, code, OpAssertion.WordOrLargerAssertion);
 901         }
 902     }
 903 
<span class="line-added"> 904     private enum EVEXFeatureAssertion {</span>
<span class="line-added"> 905         AVX512F_ALL(EnumSet.of(AVX512F), EnumSet.of(AVX512F), EnumSet.of(AVX512F)),</span>
<span class="line-added"> 906         AVX512F_128ONLY(EnumSet.of(AVX512F), null, null),</span>
<span class="line-added"> 907         AVX512F_VL(EnumSet.of(AVX512F, AVX512VL), EnumSet.of(AVX512F, AVX512VL), EnumSet.of(AVX512F)),</span>
<span class="line-added"> 908         AVX512CD_VL(EnumSet.of(AVX512F, AVX512CD, AVX512VL), EnumSet.of(AVX512F, AVX512CD, AVX512VL), EnumSet.of(AVX512F, AVX512CD)),</span>
<span class="line-added"> 909         AVX512DQ_VL(EnumSet.of(AVX512F, AVX512DQ, AVX512VL), EnumSet.of(AVX512F, AVX512DQ, AVX512VL), EnumSet.of(AVX512F, AVX512DQ)),</span>
<span class="line-added"> 910         AVX512BW_VL(EnumSet.of(AVX512F, AVX512BW, AVX512VL), EnumSet.of(AVX512F, AVX512BW, AVX512VL), EnumSet.of(AVX512F, AVX512BW));</span>
<span class="line-added"> 911 </span>
<span class="line-added"> 912         private final EnumSet&lt;CPUFeature&gt; l128features;</span>
<span class="line-added"> 913         private final EnumSet&lt;CPUFeature&gt; l256features;</span>
<span class="line-added"> 914         private final EnumSet&lt;CPUFeature&gt; l512features;</span>
<span class="line-added"> 915 </span>
<span class="line-added"> 916         EVEXFeatureAssertion(EnumSet&lt;CPUFeature&gt; l128features, EnumSet&lt;CPUFeature&gt; l256features, EnumSet&lt;CPUFeature&gt; l512features) {</span>
<span class="line-added"> 917             this.l128features = l128features;</span>
<span class="line-added"> 918             this.l256features = l256features;</span>
<span class="line-added"> 919             this.l512features = l512features;</span>
<span class="line-added"> 920         }</span>
<span class="line-added"> 921 </span>
<span class="line-added"> 922         public boolean check(AMD64 arch, int l) {</span>
<span class="line-added"> 923             switch (l) {</span>
<span class="line-added"> 924                 case L128:</span>
<span class="line-added"> 925                     assert l128features != null &amp;&amp; arch.getFeatures().containsAll(l128features) : &quot;emitting illegal 128 bit instruction&quot;;</span>
<span class="line-added"> 926                     break;</span>
<span class="line-added"> 927                 case L256:</span>
<span class="line-added"> 928                     assert l256features != null &amp;&amp; arch.getFeatures().containsAll(l256features) : &quot;emitting illegal 256 bit instruction&quot;;</span>
<span class="line-added"> 929                     break;</span>
<span class="line-added"> 930                 case L512:</span>
<span class="line-added"> 931                     assert l512features != null &amp;&amp; arch.getFeatures().containsAll(l512features) : &quot;emitting illegal 512 bit instruction&quot;;</span>
<span class="line-added"> 932                     break;</span>
<span class="line-added"> 933             }</span>
<span class="line-added"> 934             return true;</span>
<span class="line-added"> 935         }</span>
<span class="line-added"> 936 </span>
<span class="line-added"> 937         public boolean supports(EnumSet&lt;CPUFeature&gt; features, AVXSize avxSize) {</span>
<span class="line-added"> 938             switch (avxSize) {</span>
<span class="line-added"> 939                 case XMM:</span>
<span class="line-added"> 940                     return l128features != null &amp;&amp; features.containsAll(l128features);</span>
<span class="line-added"> 941                 case YMM:</span>
<span class="line-added"> 942                     return l256features != null &amp;&amp; features.containsAll(l256features);</span>
<span class="line-added"> 943                 case ZMM:</span>
<span class="line-added"> 944                     return l512features != null &amp;&amp; features.containsAll(l512features);</span>
<span class="line-added"> 945                 default:</span>
<span class="line-added"> 946                     throw GraalError.shouldNotReachHere();</span>
<span class="line-added"> 947             }</span>
<span class="line-added"> 948         }</span>
<span class="line-added"> 949     }</span>
<span class="line-added"> 950 </span>
 951     private enum VEXOpAssertion {
<span class="line-modified"> 952         AVX1(CPUFeature.AVX, CPUFeature.AVX, null),</span>
<span class="line-modified"> 953         AVX1_2(CPUFeature.AVX, CPUFeature.AVX2, null),</span>
<span class="line-modified"> 954         AVX2(CPUFeature.AVX2, CPUFeature.AVX2, null),</span>
<span class="line-modified"> 955         AVX1_128ONLY(CPUFeature.AVX, null, null),</span>
<span class="line-modified"> 956         AVX1_256ONLY(null, CPUFeature.AVX, null),</span>
<span class="line-modified"> 957         AVX2_256ONLY(null, CPUFeature.AVX2, null),</span>
<span class="line-modified"> 958         XMM_CPU(CPUFeature.AVX, null, null, XMM, null, CPU, null),</span>
<span class="line-modified"> 959         XMM_XMM_CPU(CPUFeature.AVX, null, null, XMM, XMM, CPU, null),</span>
<span class="line-modified"> 960         CPU_XMM(CPUFeature.AVX, null, null, CPU, null, XMM, null),</span>
<span class="line-modified"> 961         AVX1_2_CPU_XMM(CPUFeature.AVX, CPUFeature.AVX2, null, CPU, null, XMM, null),</span>
<span class="line-modified"> 962         BMI1(CPUFeature.BMI1, null, null, CPU, CPU, CPU, null),</span>
<span class="line-modified"> 963         BMI2(CPUFeature.BMI2, null, null, CPU, CPU, CPU, null),</span>
<span class="line-added"> 964         FMA(CPUFeature.FMA, null, null, XMM, XMM, XMM, null),</span>
<span class="line-added"> 965 </span>
<span class="line-added"> 966         XMM_CPU_AVX512F_128ONLY(CPUFeature.AVX, null, EVEXFeatureAssertion.AVX512F_128ONLY, XMM, null, CPU, null),</span>
<span class="line-added"> 967         AVX1_AVX512F_ALL(CPUFeature.AVX, CPUFeature.AVX, EVEXFeatureAssertion.AVX512F_ALL),</span>
<span class="line-added"> 968         AVX1_AVX512F_VL(CPUFeature.AVX, CPUFeature.AVX, EVEXFeatureAssertion.AVX512F_VL);</span>
 969 
 970         private final CPUFeature l128feature;
 971         private final CPUFeature l256feature;
<span class="line-added"> 972         private final EVEXFeatureAssertion l512features;</span>
 973 
 974         private final RegisterCategory rCategory;
 975         private final RegisterCategory vCategory;
 976         private final RegisterCategory mCategory;
 977         private final RegisterCategory imm8Category;
 978 
<span class="line-modified"> 979         VEXOpAssertion(CPUFeature l128feature, CPUFeature l256feature, EVEXFeatureAssertion l512features) {</span>
<span class="line-modified"> 980             this(l128feature, l256feature, l512features, XMM, XMM, XMM, XMM);</span>
 981         }
 982 
<span class="line-modified"> 983         VEXOpAssertion(CPUFeature l128feature, CPUFeature l256feature, EVEXFeatureAssertion l512features, RegisterCategory rCategory, RegisterCategory vCategory, RegisterCategory mCategory,</span>
<span class="line-added"> 984                         RegisterCategory imm8Category) {</span>
 985             this.l128feature = l128feature;
 986             this.l256feature = l256feature;
<span class="line-added"> 987             this.l512features = l512features;</span>
 988             this.rCategory = rCategory;
 989             this.vCategory = vCategory;
 990             this.mCategory = mCategory;
 991             this.imm8Category = imm8Category;
 992         }
 993 
 994         public boolean check(AMD64 arch, AVXSize size, Register r, Register v, Register m) {
 995             return check(arch, getLFlag(size), r, v, m, null);
 996         }
 997 
 998         public boolean check(AMD64 arch, AVXSize size, Register r, Register v, Register m, Register imm8) {
 999             return check(arch, getLFlag(size), r, v, m, imm8);
1000         }
1001 
1002         public boolean check(AMD64 arch, int l, Register r, Register v, Register m, Register imm8) {
<span class="line-modified">1003             if (isAVX512Register(r) || isAVX512Register(v) || isAVX512Register(m) || l == L512) {</span>
<span class="line-modified">1004                 assert l512features != null &amp;&amp; l512features.check(arch, l);</span>
<span class="line-modified">1005             } else if (l == L128) {</span>
<span class="line-modified">1006                 assert l128feature != null &amp;&amp; arch.getFeatures().contains(l128feature) : &quot;emitting illegal 128 bit instruction&quot;;</span>
<span class="line-modified">1007             } else if (l == L256) {</span>
<span class="line-modified">1008                 assert l256feature != null &amp;&amp; arch.getFeatures().contains(l256feature) : &quot;emitting illegal 256 bit instruction&quot;;</span>

1009             }
1010             if (r != null) {
1011                 assert r.getRegisterCategory().equals(rCategory);
1012             }
1013             if (v != null) {
1014                 assert v.getRegisterCategory().equals(vCategory);
1015             }
1016             if (m != null) {
1017                 assert m.getRegisterCategory().equals(mCategory);
1018             }
1019             if (imm8 != null) {
1020                 assert imm8.getRegisterCategory().equals(imm8Category);
1021             }
1022             return true;
1023         }
1024 
<span class="line-modified">1025         public boolean supports(EnumSet&lt;CPUFeature&gt; features, AVXSize avxSize, boolean useZMMRegisters) {</span>
<span class="line-modified">1026             if (useZMMRegisters || avxSize == AVXSize.ZMM) {</span>
<span class="line-modified">1027                 return l512features != null &amp;&amp; l512features.supports(features, avxSize);</span>
<span class="line-modified">1028             } else if (avxSize == AVXSize.XMM) {</span>
<span class="line-modified">1029                 return l128feature != null &amp;&amp; features.contains(l128feature);</span>
<span class="line-modified">1030             } else if (avxSize == AVXSize.YMM) {</span>
<span class="line-modified">1031                 return l256feature != null &amp;&amp; features.contains(l256feature);</span>

1032             }
<span class="line-added">1033             throw GraalError.shouldNotReachHere();</span>
1034         }
1035     }
1036 
1037     /**
1038      * Base class for VEX-encoded instructions.
1039      */
1040     public static class VexOp {
1041         protected final int pp;
1042         protected final int mmmmm;
1043         protected final int w;
1044         protected final int op;
1045 
1046         private final String opcode;
1047         protected final VEXOpAssertion assertion;
1048 
<span class="line-modified">1049         protected final EVEXTuple evexTuple;</span>
<span class="line-added">1050         protected final int wEvex;</span>
<span class="line-added">1051 </span>
<span class="line-added">1052         protected VexOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion, EVEXTuple evexTuple, int wEvex) {</span>
1053             this.pp = pp;
1054             this.mmmmm = mmmmm;
1055             this.w = w;
1056             this.op = op;
1057             this.opcode = opcode;
1058             this.assertion = assertion;
<span class="line-added">1059             this.evexTuple = evexTuple;</span>
<span class="line-added">1060             this.wEvex = wEvex;</span>
<span class="line-added">1061         }</span>
<span class="line-added">1062 </span>
<span class="line-added">1063         protected VexOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {</span>
<span class="line-added">1064             this(opcode, pp, mmmmm, w, op, assertion, EVEXTuple.INVALID, WIG);</span>
1065         }
1066 
1067         public final boolean isSupported(AMD64Assembler vasm, AVXSize size) {
<span class="line-modified">1068             return isSupported(vasm, size, false);</span>
<span class="line-added">1069         }</span>
<span class="line-added">1070 </span>
<span class="line-added">1071         public final boolean isSupported(AMD64Assembler vasm, AVXSize size, boolean useZMMRegisters) {</span>
<span class="line-added">1072             return assertion.supports(((AMD64) vasm.target.arch).getFeatures(), size, useZMMRegisters);</span>
1073         }
1074 
1075         @Override
1076         public String toString() {
1077             return opcode;
1078         }
<span class="line-added">1079 </span>
<span class="line-added">1080         protected final int getDisp8Scale(boolean useEvex, AVXSize size) {</span>
<span class="line-added">1081             return useEvex ? evexTuple.getDisp8ScalingFactor(size) : DEFAULT_DISP8_SCALE;</span>
<span class="line-added">1082         }</span>
<span class="line-added">1083 </span>
1084     }
1085 
1086     /**
1087      * VEX-encoded instructions with an operand order of RM, but the M operand must be a register.
1088      */
1089     public static class VexRROp extends VexOp {
1090         // @formatter:off
<span class="line-modified">1091         public static final VexRROp VMASKMOVDQU = new VexRROp(&quot;VMASKMOVDQU&quot;, P_66, M_0F, WIG, 0xF7, VEXOpAssertion.AVX1_128ONLY, EVEXTuple.INVALID, WIG);</span>
1092         // @formatter:on
1093 
<span class="line-modified">1094         protected VexRROp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion, EVEXTuple evexTuple, int wEvex) {</span>
<span class="line-modified">1095             super(opcode, pp, mmmmm, w, op, assertion, evexTuple, wEvex);</span>




1096         }
1097 
1098         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src) {
1099             assert assertion.check((AMD64) asm.target.arch, size, dst, null, src);
1100             assert op != 0x1A || op != 0x5A;
<span class="line-modified">1101             asm.vexPrefix(dst, Register.None, src, size, pp, mmmmm, w, wEvex, false);</span>
1102             asm.emitByte(op);
1103             asm.emitModRM(dst, src);
1104         }
1105     }
1106 
1107     /**
1108      * VEX-encoded instructions with an operand order of RM.
1109      */
1110     public static class VexRMOp extends VexRROp {
1111         // @formatter:off
1112         public static final VexRMOp VCVTTSS2SI      = new VexRMOp(&quot;VCVTTSS2SI&quot;,      P_F3, M_0F,   W0,  0x2C, VEXOpAssertion.CPU_XMM);
1113         public static final VexRMOp VCVTTSS2SQ      = new VexRMOp(&quot;VCVTTSS2SQ&quot;,      P_F3, M_0F,   W1,  0x2C, VEXOpAssertion.CPU_XMM);
1114         public static final VexRMOp VCVTTSD2SI      = new VexRMOp(&quot;VCVTTSD2SI&quot;,      P_F2, M_0F,   W0,  0x2C, VEXOpAssertion.CPU_XMM);
1115         public static final VexRMOp VCVTTSD2SQ      = new VexRMOp(&quot;VCVTTSD2SQ&quot;,      P_F2, M_0F,   W1,  0x2C, VEXOpAssertion.CPU_XMM);
1116         public static final VexRMOp VCVTPS2PD       = new VexRMOp(&quot;VCVTPS2PD&quot;,       P_,   M_0F,   WIG, 0x5A);
1117         public static final VexRMOp VCVTPD2PS       = new VexRMOp(&quot;VCVTPD2PS&quot;,       P_66, M_0F,   WIG, 0x5A);
1118         public static final VexRMOp VCVTDQ2PS       = new VexRMOp(&quot;VCVTDQ2PS&quot;,       P_,   M_0F,   WIG, 0x5B);
1119         public static final VexRMOp VCVTTPS2DQ      = new VexRMOp(&quot;VCVTTPS2DQ&quot;,      P_F3, M_0F,   WIG, 0x5B);
1120         public static final VexRMOp VCVTTPD2DQ      = new VexRMOp(&quot;VCVTTPD2DQ&quot;,      P_66, M_0F,   WIG, 0xE6);
1121         public static final VexRMOp VCVTDQ2PD       = new VexRMOp(&quot;VCVTDQ2PD&quot;,       P_F3, M_0F,   WIG, 0xE6);
</pre>
<hr />
<pre>
1133         public static final VexRMOp VPMOVSXBQ       = new VexRMOp(&quot;VPMOVSXBQ&quot;,       P_66, M_0F38, WIG, 0x22);
1134         public static final VexRMOp VPMOVSXWD       = new VexRMOp(&quot;VPMOVSXWD&quot;,       P_66, M_0F38, WIG, 0x23);
1135         public static final VexRMOp VPMOVSXWQ       = new VexRMOp(&quot;VPMOVSXWQ&quot;,       P_66, M_0F38, WIG, 0x24);
1136         public static final VexRMOp VPMOVSXDQ       = new VexRMOp(&quot;VPMOVSXDQ&quot;,       P_66, M_0F38, WIG, 0x25);
1137         public static final VexRMOp VPMOVZXBW       = new VexRMOp(&quot;VPMOVZXBW&quot;,       P_66, M_0F38, WIG, 0x30);
1138         public static final VexRMOp VPMOVZXBD       = new VexRMOp(&quot;VPMOVZXBD&quot;,       P_66, M_0F38, WIG, 0x31);
1139         public static final VexRMOp VPMOVZXBQ       = new VexRMOp(&quot;VPMOVZXBQ&quot;,       P_66, M_0F38, WIG, 0x32);
1140         public static final VexRMOp VPMOVZXWD       = new VexRMOp(&quot;VPMOVZXWD&quot;,       P_66, M_0F38, WIG, 0x33);
1141         public static final VexRMOp VPMOVZXWQ       = new VexRMOp(&quot;VPMOVZXWQ&quot;,       P_66, M_0F38, WIG, 0x34);
1142         public static final VexRMOp VPMOVZXDQ       = new VexRMOp(&quot;VPMOVZXDQ&quot;,       P_66, M_0F38, WIG, 0x35);
1143         public static final VexRMOp VPTEST          = new VexRMOp(&quot;VPTEST&quot;,          P_66, M_0F38, WIG, 0x17);
1144         public static final VexRMOp VSQRTPD         = new VexRMOp(&quot;VSQRTPD&quot;,         P_66, M_0F,   WIG, 0x51);
1145         public static final VexRMOp VSQRTPS         = new VexRMOp(&quot;VSQRTPS&quot;,         P_,   M_0F,   WIG, 0x51);
1146         public static final VexRMOp VSQRTSD         = new VexRMOp(&quot;VSQRTSD&quot;,         P_F2, M_0F,   WIG, 0x51);
1147         public static final VexRMOp VSQRTSS         = new VexRMOp(&quot;VSQRTSS&quot;,         P_F3, M_0F,   WIG, 0x51);
1148         public static final VexRMOp VUCOMISS        = new VexRMOp(&quot;VUCOMISS&quot;,        P_,   M_0F,   WIG, 0x2E);
1149         public static final VexRMOp VUCOMISD        = new VexRMOp(&quot;VUCOMISD&quot;,        P_66, M_0F,   WIG, 0x2E);
1150         // @formatter:on
1151 
1152         protected VexRMOp(String opcode, int pp, int mmmmm, int w, int op) {
<span class="line-modified">1153             this(opcode, pp, mmmmm, w, op, VEXOpAssertion.AVX1, EVEXTuple.INVALID, WIG);</span>
1154         }
1155 
1156         protected VexRMOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
<span class="line-modified">1157             this(opcode, pp, mmmmm, w, op, assertion, EVEXTuple.INVALID, WIG);</span>
<span class="line-added">1158         }</span>
<span class="line-added">1159 </span>
<span class="line-added">1160         protected VexRMOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion, EVEXTuple evexTuple, int wEvex) {</span>
<span class="line-added">1161             super(opcode, pp, mmmmm, w, op, assertion, evexTuple, wEvex);</span>
1162         }
1163 
1164         public void emit(AMD64Assembler asm, AVXSize size, Register dst, AMD64Address src) {
1165             assert assertion.check((AMD64) asm.target.arch, size, dst, null, null);
<span class="line-modified">1166             boolean useEvex = asm.vexPrefix(dst, Register.None, src, size, pp, mmmmm, w, wEvex, false);</span>
1167             asm.emitByte(op);
<span class="line-modified">1168             asm.emitOperandHelper(dst, src, 0, getDisp8Scale(useEvex, size));</span>
1169         }
1170     }
1171 
1172     /**
1173      * VEX-encoded move instructions.
1174      * &lt;p&gt;
1175      * These instructions have two opcodes: op is the forward move instruction with an operand order
1176      * of RM, and opReverse is the reverse move instruction with an operand order of MR.
1177      */
1178     public static final class VexMoveOp extends VexRMOp {
1179         // @formatter:off
<span class="line-modified">1180         public static final VexMoveOp VMOVDQA32 = new VexMoveOp(&quot;VMOVDQA32&quot;, P_66, M_0F, WIG, 0x6F, 0x7F, VEXOpAssertion.AVX1_AVX512F_VL,         EVEXTuple.FVM,       W0);</span>
<span class="line-modified">1181         public static final VexMoveOp VMOVDQA64 = new VexMoveOp(&quot;VMOVDQA64&quot;, P_66, M_0F, WIG, 0x6F, 0x7F, VEXOpAssertion.AVX1_AVX512F_VL,         EVEXTuple.FVM,       W1);</span>
<span class="line-modified">1182         public static final VexMoveOp VMOVDQU32 = new VexMoveOp(&quot;VMOVDQU32&quot;, P_F3, M_0F, WIG, 0x6F, 0x7F, VEXOpAssertion.AVX1_AVX512F_VL,         EVEXTuple.FVM,       W0);</span>
<span class="line-modified">1183         public static final VexMoveOp VMOVDQU64 = new VexMoveOp(&quot;VMOVDQU64&quot;, P_F3, M_0F, WIG, 0x6F, 0x7F, VEXOpAssertion.AVX1_AVX512F_VL,         EVEXTuple.FVM,       W1);</span>
<span class="line-modified">1184         public static final VexMoveOp VMOVAPS   = new VexMoveOp(&quot;VMOVAPS&quot;,   P_,   M_0F, WIG, 0x28, 0x29, VEXOpAssertion.AVX1_AVX512F_VL,         EVEXTuple.FVM,       W0);</span>
<span class="line-modified">1185         public static final VexMoveOp VMOVAPD   = new VexMoveOp(&quot;VMOVAPD&quot;,   P_66, M_0F, WIG, 0x28, 0x29, VEXOpAssertion.AVX1_AVX512F_VL,         EVEXTuple.FVM,       W1);</span>
<span class="line-modified">1186         public static final VexMoveOp VMOVUPS   = new VexMoveOp(&quot;VMOVUPS&quot;,   P_,   M_0F, WIG, 0x10, 0x11, VEXOpAssertion.AVX1_AVX512F_VL,         EVEXTuple.FVM,       W0);</span>
<span class="line-modified">1187         public static final VexMoveOp VMOVUPD   = new VexMoveOp(&quot;VMOVUPD&quot;,   P_66, M_0F, WIG, 0x10, 0x11, VEXOpAssertion.AVX1_AVX512F_VL,         EVEXTuple.FVM,       W1);</span>
<span class="line-modified">1188         public static final VexMoveOp VMOVSS    = new VexMoveOp(&quot;VMOVSS&quot;,    P_F3, M_0F, WIG, 0x10, 0x11, VEXOpAssertion.AVX1_AVX512F_ALL,        EVEXTuple.T1S_32BIT, W0);</span>
<span class="line-modified">1189         public static final VexMoveOp VMOVSD    = new VexMoveOp(&quot;VMOVSD&quot;,    P_F2, M_0F, WIG, 0x10, 0x11, VEXOpAssertion.AVX1_AVX512F_ALL,        EVEXTuple.T1S_64BIT, W1);</span>
<span class="line-added">1190         public static final VexMoveOp VMOVD     = new VexMoveOp(&quot;VMOVD&quot;,     P_66, M_0F, W0,  0x6E, 0x7E, VEXOpAssertion.XMM_CPU_AVX512F_128ONLY, EVEXTuple.T1F_32BIT, W0);</span>
<span class="line-added">1191         public static final VexMoveOp VMOVQ     = new VexMoveOp(&quot;VMOVQ&quot;,     P_66, M_0F, W1,  0x6E, 0x7E, VEXOpAssertion.XMM_CPU_AVX512F_128ONLY, EVEXTuple.T1S_64BIT, W1);</span>
1192         // @formatter:on
1193 
1194         private final int opReverse;
1195 
1196         private VexMoveOp(String opcode, int pp, int mmmmm, int w, int op, int opReverse) {
<span class="line-modified">1197             this(opcode, pp, mmmmm, w, op, opReverse, VEXOpAssertion.AVX1, EVEXTuple.INVALID, WIG);</span>
1198         }
1199 
1200         private VexMoveOp(String opcode, int pp, int mmmmm, int w, int op, int opReverse, VEXOpAssertion assertion) {
<span class="line-modified">1201             this(opcode, pp, mmmmm, w, op, opReverse, assertion, EVEXTuple.INVALID, WIG);</span>
<span class="line-added">1202         }</span>
<span class="line-added">1203 </span>
<span class="line-added">1204         private VexMoveOp(String opcode, int pp, int mmmmm, int w, int op, int opReverse, VEXOpAssertion assertion, EVEXTuple evexTuple, int wEvex) {</span>
<span class="line-added">1205             super(opcode, pp, mmmmm, w, op, assertion, evexTuple, wEvex);</span>
1206             this.opReverse = opReverse;
1207         }
1208 
1209         public void emit(AMD64Assembler asm, AVXSize size, AMD64Address dst, Register src) {
1210             assert assertion.check((AMD64) asm.target.arch, size, src, null, null);
<span class="line-modified">1211             boolean useEvex = asm.vexPrefix(src, Register.None, dst, size, pp, mmmmm, w, wEvex, false);</span>
1212             asm.emitByte(opReverse);
<span class="line-modified">1213             asm.emitOperandHelper(src, dst, 0, getDisp8Scale(useEvex, size));</span>
1214         }
1215 
1216         public void emitReverse(AMD64Assembler asm, AVXSize size, Register dst, Register src) {
1217             assert assertion.check((AMD64) asm.target.arch, size, src, null, dst);
<span class="line-modified">1218             asm.vexPrefix(src, Register.None, dst, size, pp, mmmmm, w, wEvex, false);</span>
1219             asm.emitByte(opReverse);
1220             asm.emitModRM(src, dst);
1221         }
1222     }
1223 
1224     public interface VexRRIOp {
1225         void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src, int imm8);
1226     }
1227 
1228     /**
1229      * VEX-encoded instructions with an operand order of RMI.
1230      */
1231     public static final class VexRMIOp extends VexOp implements VexRRIOp {
1232         // @formatter:off
1233         public static final VexRMIOp VPERMQ   = new VexRMIOp(&quot;VPERMQ&quot;,   P_66, M_0F3A, W1,  0x00, VEXOpAssertion.AVX2_256ONLY);
1234         public static final VexRMIOp VPSHUFLW = new VexRMIOp(&quot;VPSHUFLW&quot;, P_F2, M_0F,   WIG, 0x70, VEXOpAssertion.AVX1_2);
1235         public static final VexRMIOp VPSHUFHW = new VexRMIOp(&quot;VPSHUFHW&quot;, P_F3, M_0F,   WIG, 0x70, VEXOpAssertion.AVX1_2);
1236         public static final VexRMIOp VPSHUFD  = new VexRMIOp(&quot;VPSHUFD&quot;,  P_66, M_0F,   WIG, 0x70, VEXOpAssertion.AVX1_2);
1237         // @formatter:on
1238 
1239         private VexRMIOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1240             super(opcode, pp, mmmmm, w, op, assertion);
1241         }
1242 
1243         @Override
1244         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src, int imm8) {
1245             assert assertion.check((AMD64) asm.target.arch, size, dst, null, src);
<span class="line-modified">1246             asm.vexPrefix(dst, Register.None, src, size, pp, mmmmm, w, wEvex, false);</span>
1247             asm.emitByte(op);
1248             asm.emitModRM(dst, src);
1249             asm.emitByte(imm8);
1250         }
1251 
1252         public void emit(AMD64Assembler asm, AVXSize size, Register dst, AMD64Address src, int imm8) {
1253             assert assertion.check((AMD64) asm.target.arch, size, dst, null, null);
<span class="line-modified">1254             boolean useEvex = asm.vexPrefix(dst, Register.None, src, size, pp, mmmmm, w, wEvex, false);</span>
1255             asm.emitByte(op);
<span class="line-modified">1256             asm.emitOperandHelper(dst, src, 1, getDisp8Scale(useEvex, size));</span>
1257             asm.emitByte(imm8);
1258         }
1259     }
1260 
1261     /**
1262      * VEX-encoded instructions with an operand order of MRI.
1263      */
1264     public static final class VexMRIOp extends VexOp implements VexRRIOp {
1265         // @formatter:off
1266         public static final VexMRIOp VEXTRACTF128 = new VexMRIOp(&quot;VEXTRACTF128&quot;, P_66, M_0F3A, W0, 0x19, VEXOpAssertion.AVX1_256ONLY);
1267         public static final VexMRIOp VEXTRACTI128 = new VexMRIOp(&quot;VEXTRACTI128&quot;, P_66, M_0F3A, W0, 0x39, VEXOpAssertion.AVX2_256ONLY);
1268         public static final VexMRIOp VPEXTRB      = new VexMRIOp(&quot;VPEXTRB&quot;,      P_66, M_0F3A, W0, 0x14, VEXOpAssertion.XMM_CPU);
1269         public static final VexMRIOp VPEXTRW      = new VexMRIOp(&quot;VPEXTRW&quot;,      P_66, M_0F3A, W0, 0x15, VEXOpAssertion.XMM_CPU);
1270         public static final VexMRIOp VPEXTRD      = new VexMRIOp(&quot;VPEXTRD&quot;,      P_66, M_0F3A, W0, 0x16, VEXOpAssertion.XMM_CPU);
1271         public static final VexMRIOp VPEXTRQ      = new VexMRIOp(&quot;VPEXTRQ&quot;,      P_66, M_0F3A, W1, 0x16, VEXOpAssertion.XMM_CPU);
1272         // @formatter:on
1273 
1274         private VexMRIOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1275             super(opcode, pp, mmmmm, w, op, assertion);
1276         }
1277 
1278         @Override
1279         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src, int imm8) {
1280             assert assertion.check((AMD64) asm.target.arch, size, src, null, dst);
<span class="line-modified">1281             asm.vexPrefix(src, Register.None, dst, size, pp, mmmmm, w, wEvex, false);</span>
1282             asm.emitByte(op);
1283             asm.emitModRM(src, dst);
1284             asm.emitByte(imm8);
1285         }
1286 
1287         public void emit(AMD64Assembler asm, AVXSize size, AMD64Address dst, Register src, int imm8) {
1288             assert assertion.check((AMD64) asm.target.arch, size, src, null, null);
<span class="line-modified">1289             boolean useEvex = asm.vexPrefix(src, Register.None, dst, size, pp, mmmmm, w, wEvex, false);</span>
1290             asm.emitByte(op);
<span class="line-modified">1291             asm.emitOperandHelper(src, dst, 1, getDisp8Scale(useEvex, size));</span>
1292             asm.emitByte(imm8);
1293         }
1294     }
1295 
1296     /**
1297      * VEX-encoded instructions with an operand order of RVMR.
1298      */
1299     public static class VexRVMROp extends VexOp {
1300         // @formatter:off
<span class="line-modified">1301         public static final VexRVMROp VPBLENDVB = new VexRVMROp(&quot;VPBLENDVB&quot;, P_66, M_0F3A, W0, 0x4C, VEXOpAssertion.AVX1_2);</span>
<span class="line-modified">1302         public static final VexRVMROp VBLENDVPS = new VexRVMROp(&quot;VBLENDVPS&quot;, P_66, M_0F3A, W0, 0x4A, VEXOpAssertion.AVX1);</span>
<span class="line-modified">1303         public static final VexRVMROp VBLENDVPD = new VexRVMROp(&quot;VBLENDVPD&quot;, P_66, M_0F3A, W0, 0x4B, VEXOpAssertion.AVX1);</span>
1304         // @formatter:on
1305 
1306         protected VexRVMROp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1307             super(opcode, pp, mmmmm, w, op, assertion);
1308         }
1309 
1310         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register mask, Register src1, Register src2) {
1311             assert assertion.check((AMD64) asm.target.arch, size, dst, mask, src1, src2);
<span class="line-modified">1312             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, wEvex, false);</span>
1313             asm.emitByte(op);
1314             asm.emitModRM(dst, src2);
1315             asm.emitByte(mask.encoding() &lt;&lt; 4);
1316         }
1317 
1318         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register mask, Register src1, AMD64Address src2) {
1319             assert assertion.check((AMD64) asm.target.arch, size, dst, mask, src1, null);
<span class="line-modified">1320             boolean useEvex = asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, wEvex, false);</span>
1321             asm.emitByte(op);
<span class="line-modified">1322             asm.emitOperandHelper(dst, src2, 0, getDisp8Scale(useEvex, size));</span>
1323             asm.emitByte(mask.encoding() &lt;&lt; 4);
1324         }
1325     }
1326 
1327     /**
1328      * VEX-encoded instructions with an operand order of RVM.
1329      */
1330     public static class VexRVMOp extends VexOp {
1331         // @formatter:off
1332         public static final VexRVMOp VANDPS    = new VexRVMOp(&quot;VANDPS&quot;,    P_,   M_0F,   WIG, 0x54);
1333         public static final VexRVMOp VANDPD    = new VexRVMOp(&quot;VANDPD&quot;,    P_66, M_0F,   WIG, 0x54);
1334         public static final VexRVMOp VANDNPS   = new VexRVMOp(&quot;VANDNPS&quot;,   P_,   M_0F,   WIG, 0x55);
1335         public static final VexRVMOp VANDNPD   = new VexRVMOp(&quot;VANDNPD&quot;,   P_66, M_0F,   WIG, 0x55);
1336         public static final VexRVMOp VORPS     = new VexRVMOp(&quot;VORPS&quot;,     P_,   M_0F,   WIG, 0x56);
1337         public static final VexRVMOp VORPD     = new VexRVMOp(&quot;VORPD&quot;,     P_66, M_0F,   WIG, 0x56);
1338         public static final VexRVMOp VXORPS    = new VexRVMOp(&quot;VXORPS&quot;,    P_,   M_0F,   WIG, 0x57);
1339         public static final VexRVMOp VXORPD    = new VexRVMOp(&quot;VXORPD&quot;,    P_66, M_0F,   WIG, 0x57);
1340         public static final VexRVMOp VADDPS    = new VexRVMOp(&quot;VADDPS&quot;,    P_,   M_0F,   WIG, 0x58);
1341         public static final VexRVMOp VADDPD    = new VexRVMOp(&quot;VADDPD&quot;,    P_66, M_0F,   WIG, 0x58);
1342         public static final VexRVMOp VADDSS    = new VexRVMOp(&quot;VADDSS&quot;,    P_F3, M_0F,   WIG, 0x58);
</pre>
<hr />
<pre>
1376         public static final VexRVMOp VPMULLD   = new VexRVMOp(&quot;VPMULLD&quot;,   P_66, M_0F38, WIG, 0x40, VEXOpAssertion.AVX1_2);
1377         public static final VexRVMOp VPSUBB    = new VexRVMOp(&quot;VPSUBB&quot;,    P_66, M_0F,   WIG, 0xF8, VEXOpAssertion.AVX1_2);
1378         public static final VexRVMOp VPSUBW    = new VexRVMOp(&quot;VPSUBW&quot;,    P_66, M_0F,   WIG, 0xF9, VEXOpAssertion.AVX1_2);
1379         public static final VexRVMOp VPSUBD    = new VexRVMOp(&quot;VPSUBD&quot;,    P_66, M_0F,   WIG, 0xFA, VEXOpAssertion.AVX1_2);
1380         public static final VexRVMOp VPSUBQ    = new VexRVMOp(&quot;VPSUBQ&quot;,    P_66, M_0F,   WIG, 0xFB, VEXOpAssertion.AVX1_2);
1381         public static final VexRVMOp VPSHUFB   = new VexRVMOp(&quot;VPSHUFB&quot;,   P_66, M_0F38, WIG, 0x00, VEXOpAssertion.AVX1_2);
1382         public static final VexRVMOp VCVTSD2SS = new VexRVMOp(&quot;VCVTSD2SS&quot;, P_F2, M_0F,   WIG, 0x5A);
1383         public static final VexRVMOp VCVTSS2SD = new VexRVMOp(&quot;VCVTSS2SD&quot;, P_F3, M_0F,   WIG, 0x5A);
1384         public static final VexRVMOp VCVTSI2SD = new VexRVMOp(&quot;VCVTSI2SD&quot;, P_F2, M_0F,   W0,  0x2A, VEXOpAssertion.XMM_XMM_CPU);
1385         public static final VexRVMOp VCVTSQ2SD = new VexRVMOp(&quot;VCVTSQ2SD&quot;, P_F2, M_0F,   W1,  0x2A, VEXOpAssertion.XMM_XMM_CPU);
1386         public static final VexRVMOp VCVTSI2SS = new VexRVMOp(&quot;VCVTSI2SS&quot;, P_F3, M_0F,   W0,  0x2A, VEXOpAssertion.XMM_XMM_CPU);
1387         public static final VexRVMOp VCVTSQ2SS = new VexRVMOp(&quot;VCVTSQ2SS&quot;, P_F3, M_0F,   W1,  0x2A, VEXOpAssertion.XMM_XMM_CPU);
1388         public static final VexRVMOp VPCMPEQB  = new VexRVMOp(&quot;VPCMPEQB&quot;,  P_66, M_0F,   WIG, 0x74, VEXOpAssertion.AVX1_2);
1389         public static final VexRVMOp VPCMPEQW  = new VexRVMOp(&quot;VPCMPEQW&quot;,  P_66, M_0F,   WIG, 0x75, VEXOpAssertion.AVX1_2);
1390         public static final VexRVMOp VPCMPEQD  = new VexRVMOp(&quot;VPCMPEQD&quot;,  P_66, M_0F,   WIG, 0x76, VEXOpAssertion.AVX1_2);
1391         public static final VexRVMOp VPCMPEQQ  = new VexRVMOp(&quot;VPCMPEQQ&quot;,  P_66, M_0F38, WIG, 0x29, VEXOpAssertion.AVX1_2);
1392         public static final VexRVMOp VPCMPGTB  = new VexRVMOp(&quot;VPCMPGTB&quot;,  P_66, M_0F,   WIG, 0x64, VEXOpAssertion.AVX1_2);
1393         public static final VexRVMOp VPCMPGTW  = new VexRVMOp(&quot;VPCMPGTW&quot;,  P_66, M_0F,   WIG, 0x65, VEXOpAssertion.AVX1_2);
1394         public static final VexRVMOp VPCMPGTD  = new VexRVMOp(&quot;VPCMPGTD&quot;,  P_66, M_0F,   WIG, 0x66, VEXOpAssertion.AVX1_2);
1395         public static final VexRVMOp VPCMPGTQ  = new VexRVMOp(&quot;VPCMPGTQ&quot;,  P_66, M_0F38, WIG, 0x37, VEXOpAssertion.AVX1_2);
<span class="line-added">1396         public static final VexRVMOp VFMADD231SS = new VexRVMOp(&quot;VFMADD231SS&quot;, P_66, M_0F38, W0, 0xB9, VEXOpAssertion.FMA);</span>
<span class="line-added">1397         public static final VexRVMOp VFMADD231SD = new VexRVMOp(&quot;VFMADD231SD&quot;, P_66, M_0F38, W1, 0xB9, VEXOpAssertion.FMA);</span>
1398         // @formatter:on
1399 
1400         private VexRVMOp(String opcode, int pp, int mmmmm, int w, int op) {
1401             this(opcode, pp, mmmmm, w, op, VEXOpAssertion.AVX1);
1402         }
1403 
1404         protected VexRVMOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1405             super(opcode, pp, mmmmm, w, op, assertion);
1406         }
1407 
1408         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, Register src2) {
1409             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, src2);
<span class="line-modified">1410             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, wEvex, false);</span>
1411             asm.emitByte(op);
1412             asm.emitModRM(dst, src2);
1413         }
1414 
1415         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, AMD64Address src2) {
1416             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, null);
<span class="line-modified">1417             boolean useEvex = asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, wEvex, false);</span>
1418             asm.emitByte(op);
<span class="line-modified">1419             asm.emitOperandHelper(dst, src2, 0, getDisp8Scale(useEvex, size));</span>
1420         }
1421     }
1422 
1423     public static final class VexGeneralPurposeRVMOp extends VexRVMOp {
1424         // @formatter:off
1425         public static final VexGeneralPurposeRVMOp ANDN   = new VexGeneralPurposeRVMOp(&quot;ANDN&quot;,   P_,   M_0F38, WIG, 0xF2, VEXOpAssertion.BMI1);
1426         public static final VexGeneralPurposeRVMOp MULX   = new VexGeneralPurposeRVMOp(&quot;MULX&quot;,   P_F2, M_0F38, WIG, 0xF6, VEXOpAssertion.BMI2);
1427         public static final VexGeneralPurposeRVMOp PDEP   = new VexGeneralPurposeRVMOp(&quot;PDEP&quot;,   P_F2, M_0F38, WIG, 0xF5, VEXOpAssertion.BMI2);
1428         public static final VexGeneralPurposeRVMOp PEXT   = new VexGeneralPurposeRVMOp(&quot;PEXT&quot;,   P_F3, M_0F38, WIG, 0xF5, VEXOpAssertion.BMI2);
1429         // @formatter:on
1430 
1431         private VexGeneralPurposeRVMOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1432             super(opcode, pp, mmmmm, w, op, assertion);
1433         }
1434 
1435         @Override
1436         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, Register src2) {
1437             assert assertion.check((AMD64) asm.target.arch, LZ, dst, src1, src2, null);
1438             assert size == AVXSize.DWORD || size == AVXSize.QWORD;
<span class="line-modified">1439             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, wEvex, false);</span>
1440             asm.emitByte(op);
1441             asm.emitModRM(dst, src2);
1442         }
1443 
1444         @Override
1445         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, AMD64Address src2) {
1446             assert assertion.check((AMD64) asm.target.arch, LZ, dst, src1, null, null);
1447             assert size == AVXSize.DWORD || size == AVXSize.QWORD;
<span class="line-modified">1448             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, wEvex, false);</span>
1449             asm.emitByte(op);
1450             asm.emitOperandHelper(dst, src2, 0);
1451         }
1452     }
1453 
1454     public static final class VexGeneralPurposeRMVOp extends VexOp {
1455         // @formatter:off
1456         public static final VexGeneralPurposeRMVOp BEXTR  = new VexGeneralPurposeRMVOp(&quot;BEXTR&quot;,  P_,   M_0F38, WIG, 0xF7, VEXOpAssertion.BMI1);
1457         public static final VexGeneralPurposeRMVOp BZHI   = new VexGeneralPurposeRMVOp(&quot;BZHI&quot;,   P_,   M_0F38, WIG, 0xF5, VEXOpAssertion.BMI2);
1458         public static final VexGeneralPurposeRMVOp SARX   = new VexGeneralPurposeRMVOp(&quot;SARX&quot;,   P_F3, M_0F38, WIG, 0xF7, VEXOpAssertion.BMI2);
1459         public static final VexGeneralPurposeRMVOp SHRX   = new VexGeneralPurposeRMVOp(&quot;SHRX&quot;,   P_F2, M_0F38, WIG, 0xF7, VEXOpAssertion.BMI2);
1460         public static final VexGeneralPurposeRMVOp SHLX   = new VexGeneralPurposeRMVOp(&quot;SHLX&quot;,   P_66, M_0F38, WIG, 0xF7, VEXOpAssertion.BMI2);
1461         // @formatter:on
1462 
1463         private VexGeneralPurposeRMVOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1464             super(opcode, pp, mmmmm, w, op, assertion);
1465         }
1466 
1467         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, Register src2) {
1468             assert assertion.check((AMD64) asm.target.arch, LZ, dst, src2, src1, null);
1469             assert size == AVXSize.DWORD || size == AVXSize.QWORD;
<span class="line-modified">1470             asm.vexPrefix(dst, src2, src1, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, wEvex, false);</span>
1471             asm.emitByte(op);
1472             asm.emitModRM(dst, src1);
1473         }
1474 
1475         public void emit(AMD64Assembler asm, AVXSize size, Register dst, AMD64Address src1, Register src2) {
1476             assert assertion.check((AMD64) asm.target.arch, LZ, dst, src2, null, null);
1477             assert size == AVXSize.DWORD || size == AVXSize.QWORD;
<span class="line-modified">1478             asm.vexPrefix(dst, src2, src1, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, wEvex, false);</span>
1479             asm.emitByte(op);
1480             asm.emitOperandHelper(dst, src1, 0);
1481         }
1482     }
1483 
1484     public static final class VexGeneralPurposeRMOp extends VexRMOp {
1485         // @formatter:off
1486         public static final VexGeneralPurposeRMOp BLSI    = new VexGeneralPurposeRMOp(&quot;BLSI&quot;,   P_,    M_0F38, WIG, 0xF3, 3, VEXOpAssertion.BMI1);
1487         public static final VexGeneralPurposeRMOp BLSMSK  = new VexGeneralPurposeRMOp(&quot;BLSMSK&quot;, P_,    M_0F38, WIG, 0xF3, 2, VEXOpAssertion.BMI1);
1488         public static final VexGeneralPurposeRMOp BLSR    = new VexGeneralPurposeRMOp(&quot;BLSR&quot;,   P_,    M_0F38, WIG, 0xF3, 1, VEXOpAssertion.BMI1);
1489         // @formatter:on
1490         private final int ext;
1491 
1492         private VexGeneralPurposeRMOp(String opcode, int pp, int mmmmm, int w, int op, int ext, VEXOpAssertion assertion) {
1493             super(opcode, pp, mmmmm, w, op, assertion);
1494             this.ext = ext;
1495         }
1496 
1497         @Override
1498         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src) {
1499             assert assertion.check((AMD64) asm.target.arch, size, dst, null, null);
<span class="line-modified">1500             asm.vexPrefix(AMD64.cpuRegisters[ext], dst, src, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, wEvex, false);</span>
1501             asm.emitByte(op);
1502             asm.emitModRM(ext, src);
1503         }
1504 
1505         @Override
1506         public void emit(AMD64Assembler asm, AVXSize size, Register dst, AMD64Address src) {
1507             assert assertion.check((AMD64) asm.target.arch, size, dst, null, null);
<span class="line-modified">1508             asm.vexPrefix(AMD64.cpuRegisters[ext], dst, src, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, wEvex, false);</span>
1509             asm.emitByte(op);
1510             asm.emitOperandHelper(ext, src, 0);
1511         }
1512     }
1513 
1514     /**
1515      * VEX-encoded shift instructions with an operand order of either RVM or VMI.
1516      */
1517     public static final class VexShiftOp extends VexRVMOp implements VexRRIOp {
1518         // @formatter:off
1519         public static final VexShiftOp VPSRLW = new VexShiftOp(&quot;VPSRLW&quot;, P_66, M_0F, WIG, 0xD1, 0x71, 2);
1520         public static final VexShiftOp VPSRLD = new VexShiftOp(&quot;VPSRLD&quot;, P_66, M_0F, WIG, 0xD2, 0x72, 2);
1521         public static final VexShiftOp VPSRLQ = new VexShiftOp(&quot;VPSRLQ&quot;, P_66, M_0F, WIG, 0xD3, 0x73, 2);
1522         public static final VexShiftOp VPSRAW = new VexShiftOp(&quot;VPSRAW&quot;, P_66, M_0F, WIG, 0xE1, 0x71, 4);
1523         public static final VexShiftOp VPSRAD = new VexShiftOp(&quot;VPSRAD&quot;, P_66, M_0F, WIG, 0xE2, 0x72, 4);
1524         public static final VexShiftOp VPSLLW = new VexShiftOp(&quot;VPSLLW&quot;, P_66, M_0F, WIG, 0xF1, 0x71, 6);
1525         public static final VexShiftOp VPSLLD = new VexShiftOp(&quot;VPSLLD&quot;, P_66, M_0F, WIG, 0xF2, 0x72, 6);
1526         public static final VexShiftOp VPSLLQ = new VexShiftOp(&quot;VPSLLQ&quot;, P_66, M_0F, WIG, 0xF3, 0x73, 6);
1527         // @formatter:on
1528 
1529         private final int immOp;
1530         private final int r;
1531 
1532         private VexShiftOp(String opcode, int pp, int mmmmm, int w, int op, int immOp, int r) {
1533             super(opcode, pp, mmmmm, w, op, VEXOpAssertion.AVX1_2);
1534             this.immOp = immOp;
1535             this.r = r;
1536         }
1537 
1538         @Override
1539         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src, int imm8) {
1540             assert assertion.check((AMD64) asm.target.arch, size, null, dst, src);
<span class="line-modified">1541             asm.vexPrefix(null, dst, src, size, pp, mmmmm, w, wEvex, false);</span>
1542             asm.emitByte(immOp);
1543             asm.emitModRM(r, src);
1544             asm.emitByte(imm8);
1545         }
1546     }
1547 
1548     public static final class VexMaskMoveOp extends VexOp {
1549         // @formatter:off
1550         public static final VexMaskMoveOp VMASKMOVPS = new VexMaskMoveOp(&quot;VMASKMOVPS&quot;, P_66, M_0F38, W0, 0x2C, 0x2E);
1551         public static final VexMaskMoveOp VMASKMOVPD = new VexMaskMoveOp(&quot;VMASKMOVPD&quot;, P_66, M_0F38, W0, 0x2D, 0x2F);
1552         public static final VexMaskMoveOp VPMASKMOVD = new VexMaskMoveOp(&quot;VPMASKMOVD&quot;, P_66, M_0F38, W0, 0x8C, 0x8E, VEXOpAssertion.AVX2);
1553         public static final VexMaskMoveOp VPMASKMOVQ = new VexMaskMoveOp(&quot;VPMASKMOVQ&quot;, P_66, M_0F38, W1, 0x8C, 0x8E, VEXOpAssertion.AVX2);
1554         // @formatter:on
1555 
1556         private final int opReverse;
1557 
1558         private VexMaskMoveOp(String opcode, int pp, int mmmmm, int w, int op, int opReverse) {
1559             this(opcode, pp, mmmmm, w, op, opReverse, VEXOpAssertion.AVX1);
1560         }
1561 
1562         private VexMaskMoveOp(String opcode, int pp, int mmmmm, int w, int op, int opReverse, VEXOpAssertion assertion) {
1563             super(opcode, pp, mmmmm, w, op, assertion);
1564             this.opReverse = opReverse;
1565         }
1566 
1567         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register mask, AMD64Address src) {
1568             assert assertion.check((AMD64) asm.target.arch, size, dst, mask, null);
<span class="line-modified">1569             asm.vexPrefix(dst, mask, src, size, pp, mmmmm, w, wEvex, false);</span>
1570             asm.emitByte(op);
1571             asm.emitOperandHelper(dst, src, 0);
1572         }
1573 
1574         public void emit(AMD64Assembler asm, AVXSize size, AMD64Address dst, Register mask, Register src) {
1575             assert assertion.check((AMD64) asm.target.arch, size, src, mask, null);
<span class="line-modified">1576             boolean useEvex = asm.vexPrefix(src, mask, dst, size, pp, mmmmm, w, wEvex, false);</span>
1577             asm.emitByte(opReverse);
<span class="line-modified">1578             asm.emitOperandHelper(src, dst, 0, getDisp8Scale(useEvex, size));</span>
1579         }
1580     }
1581 
1582     /**
1583      * VEX-encoded instructions with an operand order of RVMI.
1584      */
1585     public static final class VexRVMIOp extends VexOp {
1586         // @formatter:off
1587         public static final VexRVMIOp VSHUFPS     = new VexRVMIOp(&quot;VSHUFPS&quot;,     P_,   M_0F,   WIG, 0xC6);
1588         public static final VexRVMIOp VSHUFPD     = new VexRVMIOp(&quot;VSHUFPD&quot;,     P_66, M_0F,   WIG, 0xC6);
1589         public static final VexRVMIOp VINSERTF128 = new VexRVMIOp(&quot;VINSERTF128&quot;, P_66, M_0F3A, W0,  0x18, VEXOpAssertion.AVX1_256ONLY);
1590         public static final VexRVMIOp VINSERTI128 = new VexRVMIOp(&quot;VINSERTI128&quot;, P_66, M_0F3A, W0,  0x38, VEXOpAssertion.AVX2_256ONLY);
1591         // @formatter:on
1592 
1593         private VexRVMIOp(String opcode, int pp, int mmmmm, int w, int op) {
1594             this(opcode, pp, mmmmm, w, op, VEXOpAssertion.AVX1);
1595         }
1596 
1597         private VexRVMIOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1598             super(opcode, pp, mmmmm, w, op, assertion);
1599         }
1600 
1601         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, Register src2, int imm8) {
1602             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, src2);
1603             assert (imm8 &amp; 0xFF) == imm8;
<span class="line-modified">1604             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, wEvex, false);</span>
1605             asm.emitByte(op);
1606             asm.emitModRM(dst, src2);
1607             asm.emitByte(imm8);
1608         }
1609 
1610         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, AMD64Address src2, int imm8) {
1611             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, null);
1612             assert (imm8 &amp; 0xFF) == imm8;
<span class="line-modified">1613             boolean useEvex = asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, wEvex, false);</span>
1614             asm.emitByte(op);
<span class="line-modified">1615             asm.emitOperandHelper(dst, src2, 1, getDisp8Scale(useEvex, size));</span>
1616             asm.emitByte(imm8);
1617         }
1618     }
1619 
1620     /**
1621      * VEX-encoded comparison operation with an operand order of RVMI. The immediate operand is a
1622      * comparison operator.
1623      */
1624     public static final class VexFloatCompareOp extends VexOp {
1625         // @formatter:off
1626         public static final VexFloatCompareOp VCMPPS = new VexFloatCompareOp(&quot;VCMPPS&quot;, P_,   M_0F, WIG, 0xC2);
1627         public static final VexFloatCompareOp VCMPPD = new VexFloatCompareOp(&quot;VCMPPD&quot;, P_66, M_0F, WIG, 0xC2);
1628         public static final VexFloatCompareOp VCMPSS = new VexFloatCompareOp(&quot;VCMPSS&quot;, P_F2, M_0F, WIG, 0xC2);
1629         public static final VexFloatCompareOp VCMPSD = new VexFloatCompareOp(&quot;VCMPSD&quot;, P_F2, M_0F, WIG, 0xC2);
1630         // @formatter:on
1631 
1632         public enum Predicate {
1633             EQ_OQ(0x00),
1634             LT_OS(0x01),
1635             LE_OS(0x02),
</pre>
<hr />
<pre>
1697                             return LT_OQ;
1698                         case LE:
1699                             return LE_OQ;
1700                         case GT:
1701                             return GT_OQ;
1702                         case GE:
1703                             return GE_OQ;
1704                         default:
1705                             throw GraalError.shouldNotReachHere();
1706                     }
1707                 }
1708             }
1709         }
1710 
1711         private VexFloatCompareOp(String opcode, int pp, int mmmmm, int w, int op) {
1712             super(opcode, pp, mmmmm, w, op, VEXOpAssertion.AVX1);
1713         }
1714 
1715         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, Register src2, Predicate p) {
1716             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, src2);
<span class="line-modified">1717             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, wEvex, false);</span>
1718             asm.emitByte(op);
1719             asm.emitModRM(dst, src2);
1720             asm.emitByte(p.imm8);
1721         }
1722 
1723         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, AMD64Address src2, Predicate p) {
1724             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, null);
<span class="line-modified">1725             boolean useEvex = asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, wEvex, false);</span>
1726             asm.emitByte(op);
<span class="line-modified">1727             asm.emitOperandHelper(dst, src2, 1, getDisp8Scale(useEvex, size));</span>
1728             asm.emitByte(p.imm8);
1729         }
1730     }
1731 
1732     public final void addl(AMD64Address dst, int imm32) {
1733         ADD.getMIOpcode(DWORD, isByte(imm32)).emit(this, DWORD, dst, imm32);
1734     }
1735 
1736     public final void addl(Register dst, int imm32) {
1737         ADD.getMIOpcode(DWORD, isByte(imm32)).emit(this, DWORD, dst, imm32);
1738     }
1739 
1740     public final void addl(Register dst, Register src) {
1741         ADD.rmOp.emit(this, DWORD, dst, src);
1742     }
1743 
1744     public final void addpd(Register dst, Register src) {
1745         SSEOp.ADD.emit(this, PD, dst, src);
1746     }
1747 
</pre>
<hr />
<pre>
1951             emitByte(0x70 | cc.getValue());
1952             emitByte((int) ((disp - shortSize) &amp; 0xFF));
1953         } else {
1954             // 0000 1111 1000 tttn #32-bit disp
1955             assert isInt(disp - longSize) : &quot;must be 32bit offset (call4)&quot;;
1956             emitByte(0x0F);
1957             emitByte(0x80 | cc.getValue());
1958             emitInt((int) (disp - longSize));
1959         }
1960     }
1961 
1962     public final void jcc(ConditionFlag cc, Label l) {
1963         assert (0 &lt;= cc.getValue()) &amp;&amp; (cc.getValue() &lt; 16) : &quot;illegal cc&quot;;
1964         if (l.isBound()) {
1965             jcc(cc, l.position(), false);
1966         } else {
1967             // Note: could eliminate cond. jumps to this jump if condition
1968             // is the same however, seems to be rather unlikely case.
1969             // Note: use jccb() if label to be bound is very close to get
1970             // an 8-bit displacement
<span class="line-modified">1971             l.addPatchAt(position(), this);</span>
1972             emitByte(0x0F);
1973             emitByte(0x80 | cc.getValue());
1974             emitInt(0);
1975         }
1976 
1977     }
1978 
1979     public final void jccb(ConditionFlag cc, Label l) {
1980         if (l.isBound()) {
1981             int shortSize = 2;
1982             int entry = l.position();
1983             assert isByte(entry - (position() + shortSize)) : &quot;Dispacement too large for a short jmp&quot;;
1984             long disp = entry - position();
1985             // 0111 tttn #8-bit disp
1986             emitByte(0x70 | cc.getValue());
1987             emitByte((int) ((disp - shortSize) &amp; 0xFF));
1988         } else {
<span class="line-modified">1989             l.addPatchAt(position(), this);</span>
1990             emitByte(0x70 | cc.getValue());
1991             emitByte(0);
1992         }
1993     }
1994 
1995     public final void jmp(int jumpTarget, boolean forceDisp32) {
1996         int shortSize = 2;
1997         int longSize = 5;
1998         long disp = jumpTarget - position();
1999         if (!forceDisp32 &amp;&amp; isByte(disp - shortSize)) {
2000             emitByte(0xEB);
2001             emitByte((int) ((disp - shortSize) &amp; 0xFF));
2002         } else {
2003             emitByte(0xE9);
2004             emitInt((int) (disp - longSize));
2005         }
2006     }
2007 
2008     @Override
2009     public final void jmp(Label l) {
2010         if (l.isBound()) {
2011             jmp(l.position(), false);
2012         } else {
2013             // By default, forward jumps are always 32-bit displacements, since
2014             // we can&#39;t yet know where the label will be bound. If you&#39;re sure that
2015             // the forward jump will not run beyond 256 bytes, use jmpb to
2016             // force an 8-bit displacement.
2017 
<span class="line-modified">2018             l.addPatchAt(position(), this);</span>
2019             emitByte(0xE9);
2020             emitInt(0);
2021         }
2022     }
2023 
2024     public final void jmp(Register entry) {
2025         prefix(entry);
2026         emitByte(0xFF);
2027         emitModRM(4, entry);
2028     }
2029 
2030     public final void jmp(AMD64Address adr) {
2031         prefix(adr);
2032         emitByte(0xFF);
2033         emitOperandHelper(AMD64.rsp, adr, 0);
2034     }
2035 
2036     public final void jmpb(Label l) {
2037         if (l.isBound()) {
2038             int shortSize = 2;
<span class="line-modified">2039             // Displacement is relative to byte just after jmpb instruction</span>
<span class="line-modified">2040             int displacement = l.position() - position() - shortSize;</span>
<span class="line-modified">2041             GraalError.guarantee(isByte(displacement), &quot;Displacement too large to be encoded as a byte: %d&quot;, displacement);</span>
2042             emitByte(0xEB);
<span class="line-modified">2043             emitByte(displacement &amp; 0xFF);</span>
2044         } else {
<span class="line-modified">2045             l.addPatchAt(position(), this);</span>

2046             emitByte(0xEB);
2047             emitByte(0);
2048         }
2049     }
2050 
2051     public final void lead(Register dst, AMD64Address src) {
2052         prefix(src, dst);
2053         emitByte(0x8D);
2054         emitOperandHelper(dst, src, 0);
2055     }
2056 
2057     public final void leaq(Register dst, AMD64Address src) {
2058         prefixq(src, dst);
2059         emitByte(0x8D);
2060         emitOperandHelper(dst, src, 0);
2061     }
2062 
2063     public final void leave() {
2064         emitByte(0xC9);
2065     }
</pre>
<hr />
<pre>
2362     public final void nop() {
2363         nop(1);
2364     }
2365 
2366     public void nop(int count) {
2367         int i = count;
2368         if (UseNormalNop) {
2369             assert i &gt; 0 : &quot; &quot;;
2370             // The fancy nops aren&#39;t currently recognized by debuggers making it a
2371             // pain to disassemble code while debugging. If assert are on clearly
2372             // speed is not an issue so simply use the single byte traditional nop
2373             // to do alignment.
2374 
2375             for (; i &gt; 0; i--) {
2376                 emitByte(0x90);
2377             }
2378             return;
2379         }
2380 
2381         if (UseAddressNop) {
<span class="line-modified">2382             if (UseIntelNops) {</span>
<span class="line-modified">2383                 intelNops(i);</span>
<span class="line-modified">2384             } else {</span>
<span class="line-modified">2385                 amdNops(i);</span>



















































































































2386             }
2387             return;
2388         }
2389 
2390         // Using nops with size prefixes &quot;0x66 0x90&quot;.
2391         // From AMD Optimization Guide:
2392         // 1: 0x90
2393         // 2: 0x66 0x90
2394         // 3: 0x66 0x66 0x90
2395         // 4: 0x66 0x66 0x66 0x90
2396         // 5: 0x66 0x66 0x90 0x66 0x90
2397         // 6: 0x66 0x66 0x90 0x66 0x66 0x90
2398         // 7: 0x66 0x66 0x66 0x90 0x66 0x66 0x90
2399         // 8: 0x66 0x66 0x66 0x90 0x66 0x66 0x66 0x90
2400         // 9: 0x66 0x66 0x90 0x66 0x66 0x90 0x66 0x66 0x90
2401         // 10: 0x66 0x66 0x66 0x90 0x66 0x66 0x90 0x66 0x66 0x90
2402         //
2403         while (i &gt; 12) {
2404             i -= 4;
2405             emitByte(0x66); // size prefix
</pre>
<hr />
<pre>
2436                 emitByte(0x66);
2437                 emitByte(0x90);
2438                 break;
2439             case 3:
2440                 emitByte(0x66);
2441                 emitByte(0x66);
2442                 emitByte(0x90);
2443                 break;
2444             case 2:
2445                 emitByte(0x66);
2446                 emitByte(0x90);
2447                 break;
2448             case 1:
2449                 emitByte(0x90);
2450                 break;
2451             default:
2452                 assert i == 0;
2453         }
2454     }
2455 
<span class="line-added">2456     private void amdNops(int count) {</span>
<span class="line-added">2457         int i = count;</span>
<span class="line-added">2458         //</span>
<span class="line-added">2459         // Using multi-bytes nops &quot;0x0F 0x1F [Address]&quot; for AMD.</span>
<span class="line-added">2460         // 1: 0x90</span>
<span class="line-added">2461         // 2: 0x66 0x90</span>
<span class="line-added">2462         // 3: 0x66 0x66 0x90 (don&#39;t use &quot;0x0F 0x1F 0x00&quot; - need patching safe padding)</span>
<span class="line-added">2463         // 4: 0x0F 0x1F 0x40 0x00</span>
<span class="line-added">2464         // 5: 0x0F 0x1F 0x44 0x00 0x00</span>
<span class="line-added">2465         // 6: 0x66 0x0F 0x1F 0x44 0x00 0x00</span>
<span class="line-added">2466         // 7: 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00</span>
<span class="line-added">2467         // 8: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00</span>
<span class="line-added">2468         // 9: 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00</span>
<span class="line-added">2469         // 10: 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00</span>
<span class="line-added">2470         // 11: 0x66 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00</span>
<span class="line-added">2471 </span>
<span class="line-added">2472         // The rest coding is AMD specific - use consecutive Address nops</span>
<span class="line-added">2473 </span>
<span class="line-added">2474         // 12: 0x66 0x0F 0x1F 0x44 0x00 0x00 0x66 0x0F 0x1F 0x44 0x00 0x00</span>
<span class="line-added">2475         // 13: 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00 0x66 0x0F 0x1F 0x44 0x00 0x00</span>
<span class="line-added">2476         // 14: 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00</span>
<span class="line-added">2477         // 15: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00</span>
<span class="line-added">2478         // 16: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00</span>
<span class="line-added">2479         // Size prefixes (0x66) are added for larger sizes</span>
<span class="line-added">2480 </span>
<span class="line-added">2481         while (i &gt;= 22) {</span>
<span class="line-added">2482             i -= 11;</span>
<span class="line-added">2483             emitByte(0x66); // size prefix</span>
<span class="line-added">2484             emitByte(0x66); // size prefix</span>
<span class="line-added">2485             emitByte(0x66); // size prefix</span>
<span class="line-added">2486             addrNop8();</span>
<span class="line-added">2487         }</span>
<span class="line-added">2488         // Generate first nop for size between 21-12</span>
<span class="line-added">2489         switch (i) {</span>
<span class="line-added">2490             case 21:</span>
<span class="line-added">2491                 i -= 11;</span>
<span class="line-added">2492                 emitByte(0x66); // size prefix</span>
<span class="line-added">2493                 emitByte(0x66); // size prefix</span>
<span class="line-added">2494                 emitByte(0x66); // size prefix</span>
<span class="line-added">2495                 addrNop8();</span>
<span class="line-added">2496                 break;</span>
<span class="line-added">2497             case 20:</span>
<span class="line-added">2498             case 19:</span>
<span class="line-added">2499                 i -= 10;</span>
<span class="line-added">2500                 emitByte(0x66); // size prefix</span>
<span class="line-added">2501                 emitByte(0x66); // size prefix</span>
<span class="line-added">2502                 addrNop8();</span>
<span class="line-added">2503                 break;</span>
<span class="line-added">2504             case 18:</span>
<span class="line-added">2505             case 17:</span>
<span class="line-added">2506                 i -= 9;</span>
<span class="line-added">2507                 emitByte(0x66); // size prefix</span>
<span class="line-added">2508                 addrNop8();</span>
<span class="line-added">2509                 break;</span>
<span class="line-added">2510             case 16:</span>
<span class="line-added">2511             case 15:</span>
<span class="line-added">2512                 i -= 8;</span>
<span class="line-added">2513                 addrNop8();</span>
<span class="line-added">2514                 break;</span>
<span class="line-added">2515             case 14:</span>
<span class="line-added">2516             case 13:</span>
<span class="line-added">2517                 i -= 7;</span>
<span class="line-added">2518                 addrNop7();</span>
<span class="line-added">2519                 break;</span>
<span class="line-added">2520             case 12:</span>
<span class="line-added">2521                 i -= 6;</span>
<span class="line-added">2522                 emitByte(0x66); // size prefix</span>
<span class="line-added">2523                 addrNop5();</span>
<span class="line-added">2524                 break;</span>
<span class="line-added">2525             default:</span>
<span class="line-added">2526                 assert i &lt; 12;</span>
<span class="line-added">2527         }</span>
<span class="line-added">2528 </span>
<span class="line-added">2529         // Generate second nop for size between 11-1</span>
<span class="line-added">2530         switch (i) {</span>
<span class="line-added">2531             case 11:</span>
<span class="line-added">2532                 emitByte(0x66); // size prefix</span>
<span class="line-added">2533                 emitByte(0x66); // size prefix</span>
<span class="line-added">2534                 emitByte(0x66); // size prefix</span>
<span class="line-added">2535                 addrNop8();</span>
<span class="line-added">2536                 break;</span>
<span class="line-added">2537             case 10:</span>
<span class="line-added">2538                 emitByte(0x66); // size prefix</span>
<span class="line-added">2539                 emitByte(0x66); // size prefix</span>
<span class="line-added">2540                 addrNop8();</span>
<span class="line-added">2541                 break;</span>
<span class="line-added">2542             case 9:</span>
<span class="line-added">2543                 emitByte(0x66); // size prefix</span>
<span class="line-added">2544                 addrNop8();</span>
<span class="line-added">2545                 break;</span>
<span class="line-added">2546             case 8:</span>
<span class="line-added">2547                 addrNop8();</span>
<span class="line-added">2548                 break;</span>
<span class="line-added">2549             case 7:</span>
<span class="line-added">2550                 addrNop7();</span>
<span class="line-added">2551                 break;</span>
<span class="line-added">2552             case 6:</span>
<span class="line-added">2553                 emitByte(0x66); // size prefix</span>
<span class="line-added">2554                 addrNop5();</span>
<span class="line-added">2555                 break;</span>
<span class="line-added">2556             case 5:</span>
<span class="line-added">2557                 addrNop5();</span>
<span class="line-added">2558                 break;</span>
<span class="line-added">2559             case 4:</span>
<span class="line-added">2560                 addrNop4();</span>
<span class="line-added">2561                 break;</span>
<span class="line-added">2562             case 3:</span>
<span class="line-added">2563                 // Don&#39;t use &quot;0x0F 0x1F 0x00&quot; - need patching safe padding</span>
<span class="line-added">2564                 emitByte(0x66); // size prefix</span>
<span class="line-added">2565                 emitByte(0x66); // size prefix</span>
<span class="line-added">2566                 emitByte(0x90); // nop</span>
<span class="line-added">2567                 break;</span>
<span class="line-added">2568             case 2:</span>
<span class="line-added">2569                 emitByte(0x66); // size prefix</span>
<span class="line-added">2570                 emitByte(0x90); // nop</span>
<span class="line-added">2571                 break;</span>
<span class="line-added">2572             case 1:</span>
<span class="line-added">2573                 emitByte(0x90); // nop</span>
<span class="line-added">2574                 break;</span>
<span class="line-added">2575             default:</span>
<span class="line-added">2576                 assert i == 0;</span>
<span class="line-added">2577         }</span>
<span class="line-added">2578     }</span>
<span class="line-added">2579 </span>
<span class="line-added">2580     @SuppressWarnings(&quot;fallthrough&quot;)</span>
<span class="line-added">2581     private void intelNops(int count) {</span>
<span class="line-added">2582         //</span>
<span class="line-added">2583         // Using multi-bytes nops &quot;0x0F 0x1F [address]&quot; for Intel</span>
<span class="line-added">2584         // 1: 0x90</span>
<span class="line-added">2585         // 2: 0x66 0x90</span>
<span class="line-added">2586         // 3: 0x66 0x66 0x90 (don&#39;t use &quot;0x0F 0x1F 0x00&quot; - need patching safe padding)</span>
<span class="line-added">2587         // 4: 0x0F 0x1F 0x40 0x00</span>
<span class="line-added">2588         // 5: 0x0F 0x1F 0x44 0x00 0x00</span>
<span class="line-added">2589         // 6: 0x66 0x0F 0x1F 0x44 0x00 0x00</span>
<span class="line-added">2590         // 7: 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00</span>
<span class="line-added">2591         // 8: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00</span>
<span class="line-added">2592         // 9: 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00</span>
<span class="line-added">2593         // 10: 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00</span>
<span class="line-added">2594         // 11: 0x66 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00</span>
<span class="line-added">2595 </span>
<span class="line-added">2596         // The rest coding is Intel specific - don&#39;t use consecutive address nops</span>
<span class="line-added">2597 </span>
<span class="line-added">2598         // 12: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x66 0x66 0x66 0x90</span>
<span class="line-added">2599         // 13: 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x66 0x66 0x66 0x90</span>
<span class="line-added">2600         // 14: 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x66 0x66 0x66 0x90</span>
<span class="line-added">2601         // 15: 0x66 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x66 0x66 0x66 0x90</span>
<span class="line-added">2602 </span>
<span class="line-added">2603         int i = count;</span>
<span class="line-added">2604         while (i &gt;= 15) {</span>
<span class="line-added">2605             // For Intel don&#39;t generate consecutive addess nops (mix with regular nops)</span>
<span class="line-added">2606             i -= 15;</span>
<span class="line-added">2607             emitByte(0x66);   // size prefix</span>
<span class="line-added">2608             emitByte(0x66);   // size prefix</span>
<span class="line-added">2609             emitByte(0x66);   // size prefix</span>
<span class="line-added">2610             addrNop8();</span>
<span class="line-added">2611             emitByte(0x66);   // size prefix</span>
<span class="line-added">2612             emitByte(0x66);   // size prefix</span>
<span class="line-added">2613             emitByte(0x66);   // size prefix</span>
<span class="line-added">2614             emitByte(0x90);</span>
<span class="line-added">2615             // nop</span>
<span class="line-added">2616         }</span>
<span class="line-added">2617         switch (i) {</span>
<span class="line-added">2618             case 14:</span>
<span class="line-added">2619                 emitByte(0x66); // size prefix</span>
<span class="line-added">2620                 // fall through</span>
<span class="line-added">2621             case 13:</span>
<span class="line-added">2622                 emitByte(0x66); // size prefix</span>
<span class="line-added">2623                 // fall through</span>
<span class="line-added">2624             case 12:</span>
<span class="line-added">2625                 addrNop8();</span>
<span class="line-added">2626                 emitByte(0x66); // size prefix</span>
<span class="line-added">2627                 emitByte(0x66); // size prefix</span>
<span class="line-added">2628                 emitByte(0x66); // size prefix</span>
<span class="line-added">2629                 emitByte(0x90);</span>
<span class="line-added">2630                 // nop</span>
<span class="line-added">2631                 break;</span>
<span class="line-added">2632             case 11:</span>
<span class="line-added">2633                 emitByte(0x66); // size prefix</span>
<span class="line-added">2634                 // fall through</span>
<span class="line-added">2635             case 10:</span>
<span class="line-added">2636                 emitByte(0x66); // size prefix</span>
<span class="line-added">2637                 // fall through</span>
<span class="line-added">2638             case 9:</span>
<span class="line-added">2639                 emitByte(0x66); // size prefix</span>
<span class="line-added">2640                 // fall through</span>
<span class="line-added">2641             case 8:</span>
<span class="line-added">2642                 addrNop8();</span>
<span class="line-added">2643                 break;</span>
<span class="line-added">2644             case 7:</span>
<span class="line-added">2645                 addrNop7();</span>
<span class="line-added">2646                 break;</span>
<span class="line-added">2647             case 6:</span>
<span class="line-added">2648                 emitByte(0x66); // size prefix</span>
<span class="line-added">2649                 // fall through</span>
<span class="line-added">2650             case 5:</span>
<span class="line-added">2651                 addrNop5();</span>
<span class="line-added">2652                 break;</span>
<span class="line-added">2653             case 4:</span>
<span class="line-added">2654                 addrNop4();</span>
<span class="line-added">2655                 break;</span>
<span class="line-added">2656             case 3:</span>
<span class="line-added">2657                 // Don&#39;t use &quot;0x0F 0x1F 0x00&quot; - need patching safe padding</span>
<span class="line-added">2658                 emitByte(0x66); // size prefix</span>
<span class="line-added">2659                 // fall through</span>
<span class="line-added">2660             case 2:</span>
<span class="line-added">2661                 emitByte(0x66); // size prefix</span>
<span class="line-added">2662                 // fall through</span>
<span class="line-added">2663             case 1:</span>
<span class="line-added">2664                 emitByte(0x90);</span>
<span class="line-added">2665                 // nop</span>
<span class="line-added">2666                 break;</span>
<span class="line-added">2667             default:</span>
<span class="line-added">2668                 assert i == 0;</span>
<span class="line-added">2669         }</span>
<span class="line-added">2670     }</span>
<span class="line-added">2671 </span>
2672     public final void orl(Register dst, Register src) {
2673         OR.rmOp.emit(this, DWORD, dst, src);
2674     }
2675 
2676     public final void orl(Register dst, int imm32) {
2677         OR.getMIOpcode(DWORD, isByte(imm32)).emit(this, DWORD, dst, imm32);
2678     }
2679 
2680     // Insn: VPACKUSWB xmm1, xmm2, xmm3/m128
2681     // -----
2682     // Insn: VPACKUSWB xmm1, xmm1, xmm2
2683 
2684     public final void packuswb(Register dst, Register src) {
2685         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
2686         // Code: VEX.NDS.128.66.0F.WIG 67 /r
2687         simdPrefix(dst, dst, src, PD, P_0F, false);
2688         emitByte(0x67);
2689         emitModRM(dst, src);
2690     }
2691 
</pre>
<hr />
<pre>
3122         emitByte(0x14);
3123         emitModRM(dst, src);
3124     }
3125 
3126     public final void xorl(Register dst, Register src) {
3127         XOR.rmOp.emit(this, DWORD, dst, src);
3128     }
3129 
3130     public final void xorq(Register dst, Register src) {
3131         XOR.rmOp.emit(this, QWORD, dst, src);
3132     }
3133 
3134     public final void xorpd(Register dst, Register src) {
3135         SSEOp.XOR.emit(this, PD, dst, src);
3136     }
3137 
3138     public final void xorps(Register dst, Register src) {
3139         SSEOp.XOR.emit(this, PS, dst, src);
3140     }
3141 
<span class="line-modified">3142     public final void decl(Register dst) {</span>
3143         // Use two-byte form (one-byte form is a REX prefix in 64-bit mode)
3144         prefix(dst);
3145         emitByte(0xFF);
3146         emitModRM(1, dst);
3147     }
3148 
<span class="line-modified">3149     public final void incl(Register dst) {</span>
3150         // Use two-byte form (one-byte from is a REX prefix in 64-bit mode)
3151         prefix(dst);
3152         emitByte(0xFF);
3153         emitModRM(0, dst);
3154     }
3155 
3156     public final void addq(Register dst, int imm32) {
3157         ADD.getMIOpcode(QWORD, isByte(imm32)).emit(this, QWORD, dst, imm32);
3158     }
3159 
3160     public final void addq(AMD64Address dst, int imm32) {
3161         ADD.getMIOpcode(QWORD, isByte(imm32)).emit(this, QWORD, dst, imm32);
3162     }
3163 
3164     public final void addq(Register dst, Register src) {
3165         ADD.rmOp.emit(this, QWORD, dst, src);
3166     }
3167 
3168     public final void addq(AMD64Address dst, Register src) {
3169         ADD.mrOp.emit(this, QWORD, dst, src);
</pre>
<hr />
<pre>
3174     }
3175 
3176     public final void bsrq(Register dst, Register src) {
3177         prefixq(dst, src);
3178         emitByte(0x0F);
3179         emitByte(0xBD);
3180         emitModRM(dst, src);
3181     }
3182 
3183     public final void bswapq(Register reg) {
3184         prefixq(reg);
3185         emitByte(0x0F);
3186         emitByte(0xC8 + encode(reg));
3187     }
3188 
3189     public final void cdqq() {
3190         rexw();
3191         emitByte(0x99);
3192     }
3193 
<span class="line-added">3194     public final void repStosb() {</span>
<span class="line-added">3195         emitByte(0xf3);</span>
<span class="line-added">3196         rexw();</span>
<span class="line-added">3197         emitByte(0xaa);</span>
<span class="line-added">3198     }</span>
<span class="line-added">3199 </span>
<span class="line-added">3200     public final void repStosq() {</span>
<span class="line-added">3201         emitByte(0xf3);</span>
<span class="line-added">3202         rexw();</span>
<span class="line-added">3203         emitByte(0xab);</span>
<span class="line-added">3204     }</span>
<span class="line-added">3205 </span>
3206     public final void cmovq(ConditionFlag cc, Register dst, Register src) {
3207         prefixq(dst, src);
3208         emitByte(0x0F);
3209         emitByte(0x40 | cc.getValue());
3210         emitModRM(dst, src);
3211     }
3212 
3213     public final void setb(ConditionFlag cc, Register dst) {
3214         prefix(dst, true);
3215         emitByte(0x0F);
3216         emitByte(0x90 | cc.getValue());
3217         emitModRM(0, dst);
3218     }
3219 
3220     public final void cmovq(ConditionFlag cc, Register dst, AMD64Address src) {
3221         prefixq(src, dst);
3222         emitByte(0x0F);
3223         emitByte(0x40 | cc.getValue());
3224         emitOperandHelper(dst, src, 0);
3225     }
</pre>
<hr />
<pre>
3578                         || op == 0x00 // jump table entry
3579                         || op == 0xE9 // jmp
3580                         || op == 0xEB // short jmp
3581                         || (op &amp; 0xF0) == 0x70 // short jcc
3582                         || op == 0x0F &amp;&amp; (getByte(branch + 1) &amp; 0xF0) == 0x80 // jcc
3583         : &quot;Invalid opcode at patch point branch=&quot; + branch + &quot;, branchTarget=&quot; + branchTarget + &quot;, op=&quot; + op;
3584 
3585         if (op == 0x00) {
3586             int offsetToJumpTableBase = getShort(branch + 1);
3587             int jumpTableBase = branch - offsetToJumpTableBase;
3588             int imm32 = branchTarget - jumpTableBase;
3589             emitInt(imm32, branch);
3590         } else if (op == 0xEB || (op &amp; 0xF0) == 0x70) {
3591 
3592             // short offset operators (jmp and jcc)
3593             final int imm8 = branchTarget - (branch + 2);
3594             /*
3595              * Since a wrongly patched short branch can potentially lead to working but really bad
3596              * behaving code we should always fail with an exception instead of having an assert.
3597              */
<span class="line-modified">3598             GraalError.guarantee(isByte(imm8), &quot;Displacement too large to be encoded as a byte: %d&quot;, imm8);</span>


3599             emitByte(imm8, branch + 1);
3600 
3601         } else {
3602 
3603             int off = 1;
3604             if (op == 0x0F) {
3605                 off = 2;
3606             }
3607 
3608             int imm32 = branchTarget - (branch + 4 + off);
3609             emitInt(imm32, branch + off);
3610         }
3611     }
3612 
3613     public void nullCheck(AMD64Address address) {
3614         testl(AMD64.rax, address);
3615     }
3616 
3617     @Override
3618     public void align(int modulus) {
</pre>
<hr />
<pre>
3814 
3815     public void lfence() {
3816         emitByte(0x0f);
3817         emitByte(0xae);
3818         emitByte(0xe8);
3819     }
3820 
3821     public final void vptest(Register dst, Register src) {
3822         VexRMOp.VPTEST.emit(this, AVXSize.YMM, dst, src);
3823     }
3824 
3825     public final void vpxor(Register dst, Register nds, Register src) {
3826         VexRVMOp.VPXOR.emit(this, AVXSize.YMM, dst, nds, src);
3827     }
3828 
3829     public final void vpxor(Register dst, Register nds, AMD64Address src) {
3830         VexRVMOp.VPXOR.emit(this, AVXSize.YMM, dst, nds, src);
3831     }
3832 
3833     public final void vmovdqu(Register dst, AMD64Address src) {
<span class="line-modified">3834         VexMoveOp.VMOVDQU32.emit(this, AVXSize.YMM, dst, src);</span>
3835     }
3836 
3837     public final void vmovdqu(AMD64Address dst, Register src) {
3838         assert inRC(XMM, src);
<span class="line-modified">3839         VexMoveOp.VMOVDQU32.emit(this, AVXSize.YMM, dst, src);</span>
3840     }
3841 
3842     public final void vpmovzxbw(Register dst, AMD64Address src) {
3843         assert supports(CPUFeature.AVX2);
3844         VexRMOp.VPMOVZXBW.emit(this, AVXSize.YMM, dst, src);
3845     }
3846 
3847     public final void vzeroupper() {
3848         emitVEX(L128, P_, M_0F, W0, 0, 0, true);
3849         emitByte(0x77);
3850     }
3851 
3852     // Insn: KORTESTD k1, k2
3853 
3854     // This instruction produces ZF or CF flags
3855     public final void kortestd(Register src1, Register src2) {
3856         assert supports(CPUFeature.AVX512BW);
3857         assert inRC(MASK, src1) &amp;&amp; inRC(MASK, src2);
3858         // Code: VEX.L0.66.0F.W1 98 /r
<span class="line-modified">3859         vexPrefix(src1, Register.None, src2, AVXSize.XMM, P_66, M_0F, W1, W1, true);</span>
3860         emitByte(0x98);
3861         emitModRM(src1, src2);
3862     }
3863 
3864     // Insn: KORTESTQ k1, k2
3865 
3866     // This instruction produces ZF or CF flags
3867     public final void kortestq(Register src1, Register src2) {
3868         assert supports(CPUFeature.AVX512BW);
3869         assert inRC(MASK, src1) &amp;&amp; inRC(MASK, src2);
3870         // Code: VEX.L0.0F.W1 98 /r
<span class="line-modified">3871         vexPrefix(src1, Register.None, src2, AVXSize.XMM, P_, M_0F, W1, W1, true);</span>
3872         emitByte(0x98);
3873         emitModRM(src1, src2);
3874     }
3875 
3876     public final void kmovd(Register dst, Register src) {
3877         assert supports(CPUFeature.AVX512BW);
3878         assert inRC(MASK, dst) || inRC(CPU, dst);
3879         assert inRC(MASK, src) || inRC(CPU, src);
3880         assert !(inRC(CPU, dst) &amp;&amp; inRC(CPU, src));
3881 
3882         if (inRC(MASK, dst)) {
3883             if (inRC(MASK, src)) {
3884                 // kmovd(KRegister dst, KRegister src):
3885                 // Insn: KMOVD k1, k2/m32
3886                 // Code: VEX.L0.66.0F.W1 90 /r
<span class="line-modified">3887                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_66, M_0F, W1, W1, true);</span>
3888                 emitByte(0x90);
3889                 emitModRM(dst, src);
3890             } else {
3891                 // kmovd(KRegister dst, Register src)
3892                 // Insn: KMOVD k1, r32
3893                 // Code: VEX.L0.F2.0F.W0 92 /r
<span class="line-modified">3894                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_F2, M_0F, W0, W0, true);</span>
3895                 emitByte(0x92);
3896                 emitModRM(dst, src);
3897             }
3898         } else {
3899             if (inRC(MASK, src)) {
3900                 // kmovd(Register dst, KRegister src)
3901                 // Insn: KMOVD r32, k1
3902                 // Code: VEX.L0.F2.0F.W0 93 /r
<span class="line-modified">3903                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_F2, M_0F, W0, W0, true);</span>
3904                 emitByte(0x93);
3905                 emitModRM(dst, src);
3906             } else {
3907                 throw GraalError.shouldNotReachHere();
3908             }
3909         }
3910     }
3911 
3912     public final void kmovq(Register dst, Register src) {
3913         assert supports(CPUFeature.AVX512BW);
3914         assert inRC(MASK, dst) || inRC(CPU, dst);
3915         assert inRC(MASK, src) || inRC(CPU, src);
3916         assert !(inRC(CPU, dst) &amp;&amp; inRC(CPU, src));
3917 
3918         if (inRC(MASK, dst)) {
3919             if (inRC(MASK, src)) {
3920                 // kmovq(KRegister dst, KRegister src):
3921                 // Insn: KMOVQ k1, k2/m64
3922                 // Code: VEX.L0.0F.W1 90 /r
<span class="line-modified">3923                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_, M_0F, W1, W1, true);</span>
3924                 emitByte(0x90);
3925                 emitModRM(dst, src);
3926             } else {
3927                 // kmovq(KRegister dst, Register src)
3928                 // Insn: KMOVQ k1, r64
3929                 // Code: VEX.L0.F2.0F.W1 92 /r
<span class="line-modified">3930                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_F2, M_0F, W1, W1, true);</span>
3931                 emitByte(0x92);
3932                 emitModRM(dst, src);
3933             }
3934         } else {
3935             if (inRC(MASK, src)) {
3936                 // kmovq(Register dst, KRegister src)
3937                 // Insn: KMOVQ r64, k1
3938                 // Code: VEX.L0.F2.0F.W1 93 /r
<span class="line-modified">3939                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_F2, M_0F, W1, W1, true);</span>
3940                 emitByte(0x93);
3941                 emitModRM(dst, src);
3942             } else {
3943                 throw GraalError.shouldNotReachHere();
3944             }
3945         }
3946     }
3947 
3948     // Insn: KTESTD k1, k2
3949 
3950     public final void ktestd(Register src1, Register src2) {
3951         assert supports(CPUFeature.AVX512BW);
3952         assert inRC(MASK, src1) &amp;&amp; inRC(MASK, src2);
3953         // Code: VEX.L0.66.0F.W1 99 /r
<span class="line-modified">3954         vexPrefix(src1, Register.None, src2, AVXSize.XMM, P_66, M_0F, W1, W1, true);</span>
3955         emitByte(0x99);
3956         emitModRM(src1, src2);
3957     }
3958 
3959     public final void evmovdqu64(Register dst, AMD64Address src) {
3960         assert supports(CPUFeature.AVX512F);
3961         assert inRC(XMM, dst);
3962         evexPrefix(dst, Register.None, Register.None, src, AVXSize.ZMM, P_F3, M_0F, W1, Z0, B0);
3963         emitByte(0x6F);
<span class="line-modified">3964         emitOperandHelper(dst, src, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
3965     }
3966 
3967     // Insn: VPMOVZXBW zmm1, m256
3968 
3969     public final void evpmovzxbw(Register dst, AMD64Address src) {
3970         assert supports(CPUFeature.AVX512BW);
3971         assert inRC(XMM, dst);
3972         // Code: EVEX.512.66.0F38.WIG 30 /r
3973         evexPrefix(dst, Register.None, Register.None, src, AVXSize.ZMM, P_66, M_0F38, WIG, Z0, B0);
3974         emitByte(0x30);
<span class="line-modified">3975         emitOperandHelper(dst, src, 0, EVEXTuple.HVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
3976     }
3977 
3978     public final void evpcmpeqb(Register kdst, Register nds, AMD64Address src) {
3979         assert supports(CPUFeature.AVX512BW);
3980         assert inRC(MASK, kdst) &amp;&amp; inRC(XMM, nds);
3981         evexPrefix(kdst, Register.None, nds, src, AVXSize.ZMM, P_66, M_0F, WIG, Z0, B0);
3982         emitByte(0x74);
<span class="line-modified">3983         emitOperandHelper(kdst, src, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
3984     }
3985 
3986     // Insn: VMOVDQU16 zmm1 {k1}{z}, zmm2/m512
3987     // -----
3988     // Insn: VMOVDQU16 zmm1, m512
3989 
3990     public final void evmovdqu16(Register dst, AMD64Address src) {
3991         assert supports(CPUFeature.AVX512BW);
3992         assert inRC(XMM, dst);
3993         // Code: EVEX.512.F2.0F.W1 6F /r
3994         evexPrefix(dst, Register.None, Register.None, src, AVXSize.ZMM, P_F2, M_0F, W1, Z0, B0);
3995         emitByte(0x6F);
<span class="line-modified">3996         emitOperandHelper(dst, src, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
3997     }
3998 
3999     // Insn: VMOVDQU16 zmm1, k1:z, m512
4000 
4001     public final void evmovdqu16(Register dst, Register mask, AMD64Address src) {
4002         assert supports(CPUFeature.AVX512BW);
4003         assert inRC(XMM, dst) &amp;&amp; inRC(MASK, mask);
4004         // Code: EVEX.512.F2.0F.W1 6F /r
4005         evexPrefix(dst, mask, Register.None, src, AVXSize.ZMM, P_F2, M_0F, W1, Z1, B0);
4006         emitByte(0x6F);
<span class="line-modified">4007         emitOperandHelper(dst, src, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
4008     }
4009 
4010     // Insn: VMOVDQU16 zmm2/m512 {k1}{z}, zmm1
4011     // -----
4012     // Insn: VMOVDQU16 m512, zmm1
4013 
4014     public final void evmovdqu16(AMD64Address dst, Register src) {
4015         assert supports(CPUFeature.AVX512BW);
4016         assert inRC(XMM, src);
4017         // Code: EVEX.512.F2.0F.W1 7F /r
4018         evexPrefix(src, Register.None, Register.None, dst, AVXSize.ZMM, P_F2, M_0F, W1, Z0, B0);
4019         emitByte(0x7F);
<span class="line-modified">4020         emitOperandHelper(src, dst, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
4021     }
4022 
4023     // Insn: VMOVDQU16 m512, k1, zmm1
4024 
4025     public final void evmovdqu16(AMD64Address dst, Register mask, Register src) {
4026         assert supports(CPUFeature.AVX512BW);
4027         assert inRC(MASK, mask) &amp;&amp; inRC(XMM, src);
4028         // Code: EVEX.512.F2.0F.W1 7F /r
4029         evexPrefix(src, mask, Register.None, dst, AVXSize.ZMM, P_F2, M_0F, W1, Z0, B0);
4030         emitByte(0x7F);
<span class="line-modified">4031         emitOperandHelper(src, dst, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
4032     }
4033 
4034     // Insn: VPBROADCASTW zmm1 {k1}{z}, reg
4035     // -----
4036     // Insn: VPBROADCASTW zmm1, reg
4037 
4038     public final void evpbroadcastw(Register dst, Register src) {
4039         assert supports(CPUFeature.AVX512BW);
4040         assert inRC(XMM, dst) &amp;&amp; inRC(CPU, src);
4041         // Code: EVEX.512.66.0F38.W0 7B /r
4042         evexPrefix(dst, Register.None, Register.None, src, AVXSize.ZMM, P_66, M_0F38, W0, Z0, B0);
4043         emitByte(0x7B);
4044         emitModRM(dst, src);
4045     }
4046 
4047     // Insn: VPCMPUW k1 {k2}, zmm2, zmm3/m512, imm8
4048     // -----
4049     // Insn: VPCMPUW k1, zmm2, zmm3, imm8
4050 
4051     public final void evpcmpuw(Register kdst, Register nds, Register src, int vcc) {
</pre>
<hr />
<pre>
4066         assert supports(CPUFeature.AVX512BW);
4067         assert inRC(MASK, kdst) &amp;&amp; inRC(MASK, mask);
4068         assert inRC(XMM, nds) &amp;&amp; inRC(XMM, src);
4069         // Code: EVEX.NDS.512.66.0F3A.W1 3E /r ib
4070         evexPrefix(kdst, mask, nds, src, AVXSize.ZMM, P_66, M_0F3A, W1, Z0, B0);
4071         emitByte(0x3E);
4072         emitModRM(kdst, src);
4073         emitByte(vcc);
4074     }
4075 
4076     // Insn: VPMOVWB ymm1/m256 {k1}{z}, zmm2
4077     // -----
4078     // Insn: VPMOVWB m256, zmm2
4079 
4080     public final void evpmovwb(AMD64Address dst, Register src) {
4081         assert supports(CPUFeature.AVX512BW);
4082         assert inRC(XMM, src);
4083         // Code: EVEX.512.F3.0F38.W0 30 /r
4084         evexPrefix(src, Register.None, Register.None, dst, AVXSize.ZMM, P_F3, M_0F38, W0, Z0, B0);
4085         emitByte(0x30);
<span class="line-modified">4086         emitOperandHelper(src, dst, 0, EVEXTuple.HVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
4087     }
4088 
4089     // Insn: VPMOVWB m256, k1, zmm2
4090 
4091     public final void evpmovwb(AMD64Address dst, Register mask, Register src) {
4092         assert supports(CPUFeature.AVX512BW);
4093         assert inRC(MASK, mask) &amp;&amp; inRC(XMM, src);
4094         // Code: EVEX.512.F3.0F38.W0 30 /r
4095         evexPrefix(src, mask, Register.None, dst, AVXSize.ZMM, P_F3, M_0F38, W0, Z0, B0);
4096         emitByte(0x30);
<span class="line-modified">4097         emitOperandHelper(src, dst, 0, EVEXTuple.HVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
4098     }
4099 
4100     // Insn: VPMOVZXBW zmm1 {k1}{z}, ymm2/m256
4101     // -----
4102     // Insn: VPMOVZXBW zmm1, k1, m256
4103 
4104     public final void evpmovzxbw(Register dst, Register mask, AMD64Address src) {
4105         assert supports(CPUFeature.AVX512BW);
4106         assert inRC(MASK, mask) &amp;&amp; inRC(XMM, dst);
4107         // Code: EVEX.512.66.0F38.WIG 30 /r
4108         evexPrefix(dst, mask, Register.None, src, AVXSize.ZMM, P_66, M_0F38, WIG, Z0, B0);
4109         emitByte(0x30);
<span class="line-modified">4110         emitOperandHelper(dst, src, 0, EVEXTuple.HVM.getDisp8ScalingFactor(AVXSize.ZMM));</span>
4111     }
4112 
4113 }
</pre>
</td>
</tr>
</table>
<center><a href="AMD64AsmOptions.java.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../../../index.html" target="_top">index</a> <a href="AMD64BaseAssembler.java.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>