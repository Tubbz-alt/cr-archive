diff a/src/hotspot/cpu/aarch64/interp_masm_aarch64.cpp b/src/hotspot/cpu/aarch64/interp_masm_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/interp_masm_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/interp_masm_aarch64.cpp
@@ -703,10 +703,13 @@
 //      c_rarg0, c_rarg1, c_rarg2, c_rarg3, .. (param regs)
 //      rscratch1, rscratch2 (scratch regs)
 void InterpreterMacroAssembler::lock_object(Register lock_reg)
 {
   assert(lock_reg == c_rarg1, "The argument is only for looks. It must be c_rarg1");
+
+  TSAN_RUNTIME_ONLY(push_ptr(lock_reg));
+
   if (UseHeavyMonitors) {
     call_VM(noreg,
             CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),
             lock_reg);
   } else {
@@ -785,10 +788,19 @@
             CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),
             lock_reg);
 
     bind(done);
   }
+
+  TSAN_RUNTIME_ONLY(
+    pop_ptr(lock_reg);
+    pusha();
+    call_VM(noreg,
+            CAST_FROM_FN_PTR(address, SharedRuntime::tsan_interp_lock),
+            lock_reg);
+    popa();
+  );
 }
 
 
 // Unlocks an object. Used in monitorexit bytecode and
 // remove_activation.  Throws an IllegalMonitorException if object is
@@ -803,10 +815,18 @@
 //      rscratch1, rscratch2 (scratch regs)
 void InterpreterMacroAssembler::unlock_object(Register lock_reg)
 {
   assert(lock_reg == c_rarg1, "The argument is only for looks. It must be rarg1");
 
+  TSAN_RUNTIME_ONLY(
+    pusha();
+    call_VM(noreg,
+            CAST_FROM_FN_PTR(address, SharedRuntime::tsan_interp_unlock),
+            lock_reg);
+    popa();
+  );
+
   if (UseHeavyMonitors) {
     call_VM(noreg,
             CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit),
             lock_reg);
   } else {
diff a/src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp b/src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/sharedRuntime_aarch64.cpp
@@ -1787,10 +1787,18 @@
     __ br(Assembler::NE, slow_path_lock);
 
     // Slow path will re-enter here
 
     __ bind(lock_done);
+
+    TSAN_RUNTIME_ONLY(
+      __ pusha();
+      __ call_VM(noreg,
+                 CAST_FROM_FN_PTR(address, SharedRuntime::tsan_oop_lock),
+                 obj_reg);
+      __ popa();
+    );
   }
 
 
   // Finally just about ready to make the JNI call
 
@@ -1903,10 +1911,19 @@
     // Get locked oop from the handle we passed to jni
     __ ldr(obj_reg, Address(oop_handle_reg, 0));
 
     __ resolve(IS_NOT_NULL, obj_reg);
 
+    TSAN_RUNTIME_ONLY(
+      __ pusha();
+      __ call_VM(noreg,
+                 CAST_FROM_FN_PTR(address, SharedRuntime::tsan_oop_unlock),
+                 obj_reg);
+      __ popa();
+    );
+
+
     Label done;
 
     if (UseBiasedLocking) {
       __ biased_locking_exit(obj_reg, old_hdr, done);
     }
diff a/src/hotspot/share/classfile/classFileParser.cpp b/src/hotspot/share/classfile/classFileParser.cpp
--- a/src/hotspot/share/classfile/classFileParser.cpp
+++ b/src/hotspot/share/classfile/classFileParser.cpp
@@ -5846,11 +5846,11 @@
       // This allows us to avoid allocating jmethodIDs during program execution.
       jmethodID id = ik->methods()->at(index)->jmethod_id();
 #ifdef ASSERT
       u8 id_u8 = reinterpret_cast<u8>(id);
       assert((id_u8 & right_n_bits(3)) == 0, "jmethodID is not aligned");
-      assert((id_u8 & left_n_bits(17)) == 0, "jmethodID is not aligned");
+      assert((id_u8 & left_n_bits(16)) == 0, "jmethodID is not aligned");
 #endif
     }
   }
 #endif // INCLUDE_TSAN
 
