<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/runtime/thread.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="thread.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="unhandledOops.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/runtime/thread.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 667   // If &quot;is_par&quot; is false, sets the token of &quot;this&quot; to
 668   // &quot;claim_token&quot;, and returns &quot;true&quot;.  If &quot;is_par&quot; is true,
 669   // uses an atomic instruction to set the current thread&#39;s token to
 670   // &quot;claim_token&quot;, if it is not already.  Returns &quot;true&quot; iff the
 671   // calling thread does the update, this indicates that the calling thread
 672   // has claimed the thread in the current iteration.
 673   bool claim_threads_do(bool is_par, uintx claim_token) {
 674     if (!is_par) {
 675       _threads_do_token = claim_token;
 676       return true;
 677     } else {
 678       return claim_par_threads_do(claim_token);
 679     }
 680   }
 681 
 682   uintx threads_do_token() const { return _threads_do_token; }
 683 
 684   // jvmtiRedefineClasses support
 685   void metadata_handles_do(void f(Metadata*));
 686 










 687   // Used by fast lock support
 688   virtual bool is_lock_owned(address adr) const;
 689 
<span class="line-modified"> 690   // Check if address is in the stack of the thread (not just for locks).</span>
<span class="line-modified"> 691   // Warning: the method can only be used on the running thread</span>
<span class="line-modified"> 692   bool is_in_stack(address adr) const;</span>
<span class="line-modified"> 693   // Check if address is in the usable part of the stack (excludes protected</span>
<span class="line-modified"> 694   // guard pages)</span>
<span class="line-modified"> 695   bool is_in_usable_stack(address adr) const;</span>


















 696 
 697   // Sets this thread as starting thread. Returns failure if thread
 698   // creation fails due to lack of memory, too many threads etc.
 699   bool set_as_starting_thread();
 700 
 701 protected:
 702   // OS data associated with the thread
 703   OSThread* _osthread;  // Platform-specific thread information
 704 
 705   // Thread local resource area for temporary allocation within the VM
 706   ResourceArea* _resource_area;
 707 
 708   DEBUG_ONLY(ResourceMark* _current_resource_mark;)
 709 
 710   // Thread local handle area for allocation of handles within the VM
 711   HandleArea* _handle_area;
 712   GrowableArray&lt;Metadata*&gt;* _metadata_handles;
 713 
 714   // Support for stack overflow handling, get_thread, etc.
 715   address          _stack_base;
 716   size_t           _stack_size;
 717   int              _lgrp_id;
 718 
 719   volatile void** polling_page_addr() { return &amp;_polling_page; }
 720 
 721  public:
 722   // Stack overflow support
<span class="line-modified"> 723   address stack_base() const           { assert(_stack_base != NULL,&quot;Sanity check&quot;); return _stack_base; }</span>
 724   void    set_stack_base(address base) { _stack_base = base; }
 725   size_t  stack_size() const           { return _stack_size; }
 726   void    set_stack_size(size_t size)  { _stack_size = size; }
 727   address stack_end()  const           { return stack_base() - stack_size(); }
 728   void    record_stack_base_and_size();
 729   void    register_thread_stack_with_NMT() NOT_NMT_RETURN;
 730 
<span class="line-removed"> 731   bool    on_local_stack(address adr) const {</span>
<span class="line-removed"> 732     // QQQ this has knowledge of direction, ought to be a stack method</span>
<span class="line-removed"> 733     return (_stack_base &gt; adr &amp;&amp; adr &gt;= stack_end());</span>
<span class="line-removed"> 734   }</span>
<span class="line-removed"> 735 </span>
 736   int     lgrp_id() const        { return _lgrp_id; }
 737   void    set_lgrp_id(int value) { _lgrp_id = value; }
 738 
 739   // Printing
 740   void print_on(outputStream* st, bool print_extended_info) const;
 741   virtual void print_on(outputStream* st) const { print_on(st, false); }
 742   void print() const;
 743   virtual void print_on_error(outputStream* st, char* buf, int buflen) const;
 744   void print_value_on(outputStream* st) const;
 745 
 746   // Debug-only code
 747 #ifdef ASSERT
 748  private:
 749   // Deadlock detection support for Mutex locks. List of locks own by thread.
 750   Mutex* _owned_locks;
 751   // Mutex::set_owner_implementation is the only place where _owned_locks is modified,
 752   // thus the friendship
 753   friend class Mutex;
 754   friend class Monitor;
 755 
</pre>
<hr />
<pre>
1634     assert(_stack_yellow_zone_size &gt; 0, &quot;Don&#39;t call this before the field is initialized.&quot;);
1635     return _stack_yellow_zone_size;
1636   }
1637   static void set_stack_yellow_zone_size(size_t s) {
1638     assert(is_aligned(s, os::vm_page_size()),
1639            &quot;We can not protect if the yellow zone size is not page aligned.&quot;);
1640     assert(_stack_yellow_zone_size == 0, &quot;This should be called only once.&quot;);
1641     _stack_yellow_zone_size = s;
1642   }
1643 
1644   static size_t stack_reserved_zone_size() {
1645     // _stack_reserved_zone_size may be 0. This indicates the feature is off.
1646     return _stack_reserved_zone_size;
1647   }
1648   static void set_stack_reserved_zone_size(size_t s) {
1649     assert(is_aligned(s, os::vm_page_size()),
1650            &quot;We can not protect if the reserved zone size is not page aligned.&quot;);
1651     assert(_stack_reserved_zone_size == 0, &quot;This should be called only once.&quot;);
1652     _stack_reserved_zone_size = s;
1653   }
<span class="line-modified">1654   address stack_reserved_zone_base() {</span>
1655     return (address)(stack_end() +
1656                      (stack_red_zone_size() + stack_yellow_zone_size() + stack_reserved_zone_size()));
1657   }
1658   bool in_stack_reserved_zone(address a) {
1659     return (a &lt;= stack_reserved_zone_base()) &amp;&amp;
1660            (a &gt;= (address)((intptr_t)stack_reserved_zone_base() - stack_reserved_zone_size()));
1661   }
1662 
1663   static size_t stack_yellow_reserved_zone_size() {
1664     return _stack_yellow_zone_size + _stack_reserved_zone_size;
1665   }
1666   bool in_stack_yellow_reserved_zone(address a) {
1667     return (a &lt;= stack_reserved_zone_base()) &amp;&amp; (a &gt;= stack_red_zone_base());
1668   }
1669 
1670   // Size of red + yellow + reserved zones.
1671   static size_t stack_guard_zone_size() {
1672     return stack_red_zone_size() + stack_yellow_reserved_zone_size();
1673   }
1674 
</pre>
<hr />
<pre>
1715     _reserved_stack_activation = addr;
1716   }
1717 
1718   // Attempt to reguard the stack after a stack overflow may have occurred.
1719   // Returns true if (a) guard pages are not needed on this thread, (b) the
1720   // pages are already guarded, or (c) the pages were successfully reguarded.
1721   // Returns false if there is not enough stack space to reguard the pages, in
1722   // which case the caller should unwind a frame and try again.  The argument
1723   // should be the caller&#39;s (approximate) sp.
1724   bool reguard_stack(address cur_sp);
1725   // Similar to above but see if current stackpoint is out of the guard area
1726   // and reguard if possible.
1727   bool reguard_stack(void);
1728 
1729   address stack_overflow_limit() { return _stack_overflow_limit; }
1730   void set_stack_overflow_limit() {
1731     _stack_overflow_limit =
1732       stack_end() + MAX2(JavaThread::stack_guard_zone_size(), JavaThread::stack_shadow_zone_size());
1733   }
1734 







1735   // Misc. accessors/mutators
1736   void set_do_not_unlock(void)                   { _do_not_unlock_if_synchronized = true; }
1737   void clr_do_not_unlock(void)                   { _do_not_unlock_if_synchronized = false; }
1738   bool do_not_unlock(void)                       { return _do_not_unlock_if_synchronized; }
1739 
1740   // For assembly stub generation
1741   static ByteSize threadObj_offset()             { return byte_offset_of(JavaThread, _threadObj); }
1742   static ByteSize jni_environment_offset()       { return byte_offset_of(JavaThread, _jni_environment); }
1743   static ByteSize pending_jni_exception_check_fn_offset() {
1744     return byte_offset_of(JavaThread, _pending_jni_exception_check_fn);
1745   }
1746   static ByteSize last_Java_sp_offset() {
1747     return byte_offset_of(JavaThread, _anchor) + JavaFrameAnchor::last_Java_sp_offset();
1748   }
1749   static ByteSize last_Java_pc_offset() {
1750     return byte_offset_of(JavaThread, _anchor) + JavaFrameAnchor::last_Java_pc_offset();
1751   }
1752   static ByteSize frame_anchor_offset() {
1753     return byte_offset_of(JavaThread, _anchor);
1754   }
</pre>
</td>
<td>
<hr />
<pre>
 667   // If &quot;is_par&quot; is false, sets the token of &quot;this&quot; to
 668   // &quot;claim_token&quot;, and returns &quot;true&quot;.  If &quot;is_par&quot; is true,
 669   // uses an atomic instruction to set the current thread&#39;s token to
 670   // &quot;claim_token&quot;, if it is not already.  Returns &quot;true&quot; iff the
 671   // calling thread does the update, this indicates that the calling thread
 672   // has claimed the thread in the current iteration.
 673   bool claim_threads_do(bool is_par, uintx claim_token) {
 674     if (!is_par) {
 675       _threads_do_token = claim_token;
 676       return true;
 677     } else {
 678       return claim_par_threads_do(claim_token);
 679     }
 680   }
 681 
 682   uintx threads_do_token() const { return _threads_do_token; }
 683 
 684   // jvmtiRedefineClasses support
 685   void metadata_handles_do(void f(Metadata*));
 686 
<span class="line-added"> 687  private:</span>
<span class="line-added"> 688   // Check if address is within the given range of this thread&#39;s</span>
<span class="line-added"> 689   // stack:  stack_base() &gt; adr &gt;/&gt;= limit</span>
<span class="line-added"> 690   // The check is inclusive of limit if passed true, else exclusive.</span>
<span class="line-added"> 691   bool is_in_stack_range(address adr, address limit, bool inclusive) const {</span>
<span class="line-added"> 692     assert(stack_base() &gt; limit &amp;&amp; limit &gt;= stack_end(), &quot;limit is outside of stack&quot;);</span>
<span class="line-added"> 693     return stack_base() &gt; adr &amp;&amp; (inclusive ? adr &gt;= limit : adr &gt; limit);</span>
<span class="line-added"> 694   }</span>
<span class="line-added"> 695 </span>
<span class="line-added"> 696  public:</span>
 697   // Used by fast lock support
 698   virtual bool is_lock_owned(address adr) const;
 699 
<span class="line-modified"> 700   // Check if address is within the given range of this thread&#39;s</span>
<span class="line-modified"> 701   // stack:  stack_base() &gt; adr &gt;= limit</span>
<span class="line-modified"> 702   bool is_in_stack_range_incl(address adr, address limit) const {</span>
<span class="line-modified"> 703     return is_in_stack_range(adr, limit, true);</span>
<span class="line-modified"> 704   }</span>
<span class="line-modified"> 705 </span>
<span class="line-added"> 706   // Check if address is within the given range of this thread&#39;s</span>
<span class="line-added"> 707   // stack:  stack_base() &gt; adr &gt; limit</span>
<span class="line-added"> 708   bool is_in_stack_range_excl(address adr, address limit) const {</span>
<span class="line-added"> 709     return is_in_stack_range(adr, limit, false);</span>
<span class="line-added"> 710   }</span>
<span class="line-added"> 711 </span>
<span class="line-added"> 712   // Check if address is in the stack mapped to this thread. Used mainly in</span>
<span class="line-added"> 713   // error reporting (so has to include guard zone) and frame printing.</span>
<span class="line-added"> 714   bool is_in_full_stack(address adr) const {</span>
<span class="line-added"> 715     return is_in_stack_range_incl(adr, stack_end());</span>
<span class="line-added"> 716   }</span>
<span class="line-added"> 717 </span>
<span class="line-added"> 718   // Check if address is in the live stack of this thread (not just for locks).</span>
<span class="line-added"> 719   // Warning: can only be called by the current thread on itself.</span>
<span class="line-added"> 720   bool is_in_live_stack(address adr) const {</span>
<span class="line-added"> 721     assert(Thread::current() == this, &quot;is_in_live_stack can only be called from current thread&quot;);</span>
<span class="line-added"> 722     return is_in_stack_range_incl(adr, os::current_stack_pointer());</span>
<span class="line-added"> 723   }</span>
 724 
 725   // Sets this thread as starting thread. Returns failure if thread
 726   // creation fails due to lack of memory, too many threads etc.
 727   bool set_as_starting_thread();
 728 
 729 protected:
 730   // OS data associated with the thread
 731   OSThread* _osthread;  // Platform-specific thread information
 732 
 733   // Thread local resource area for temporary allocation within the VM
 734   ResourceArea* _resource_area;
 735 
 736   DEBUG_ONLY(ResourceMark* _current_resource_mark;)
 737 
 738   // Thread local handle area for allocation of handles within the VM
 739   HandleArea* _handle_area;
 740   GrowableArray&lt;Metadata*&gt;* _metadata_handles;
 741 
 742   // Support for stack overflow handling, get_thread, etc.
 743   address          _stack_base;
 744   size_t           _stack_size;
 745   int              _lgrp_id;
 746 
 747   volatile void** polling_page_addr() { return &amp;_polling_page; }
 748 
 749  public:
 750   // Stack overflow support
<span class="line-modified"> 751   address stack_base() const           { assert(_stack_base != NULL,&quot;Sanity check failed for %s&quot;, name()); return _stack_base; }</span>
 752   void    set_stack_base(address base) { _stack_base = base; }
 753   size_t  stack_size() const           { return _stack_size; }
 754   void    set_stack_size(size_t size)  { _stack_size = size; }
 755   address stack_end()  const           { return stack_base() - stack_size(); }
 756   void    record_stack_base_and_size();
 757   void    register_thread_stack_with_NMT() NOT_NMT_RETURN;
 758 





 759   int     lgrp_id() const        { return _lgrp_id; }
 760   void    set_lgrp_id(int value) { _lgrp_id = value; }
 761 
 762   // Printing
 763   void print_on(outputStream* st, bool print_extended_info) const;
 764   virtual void print_on(outputStream* st) const { print_on(st, false); }
 765   void print() const;
 766   virtual void print_on_error(outputStream* st, char* buf, int buflen) const;
 767   void print_value_on(outputStream* st) const;
 768 
 769   // Debug-only code
 770 #ifdef ASSERT
 771  private:
 772   // Deadlock detection support for Mutex locks. List of locks own by thread.
 773   Mutex* _owned_locks;
 774   // Mutex::set_owner_implementation is the only place where _owned_locks is modified,
 775   // thus the friendship
 776   friend class Mutex;
 777   friend class Monitor;
 778 
</pre>
<hr />
<pre>
1657     assert(_stack_yellow_zone_size &gt; 0, &quot;Don&#39;t call this before the field is initialized.&quot;);
1658     return _stack_yellow_zone_size;
1659   }
1660   static void set_stack_yellow_zone_size(size_t s) {
1661     assert(is_aligned(s, os::vm_page_size()),
1662            &quot;We can not protect if the yellow zone size is not page aligned.&quot;);
1663     assert(_stack_yellow_zone_size == 0, &quot;This should be called only once.&quot;);
1664     _stack_yellow_zone_size = s;
1665   }
1666 
1667   static size_t stack_reserved_zone_size() {
1668     // _stack_reserved_zone_size may be 0. This indicates the feature is off.
1669     return _stack_reserved_zone_size;
1670   }
1671   static void set_stack_reserved_zone_size(size_t s) {
1672     assert(is_aligned(s, os::vm_page_size()),
1673            &quot;We can not protect if the reserved zone size is not page aligned.&quot;);
1674     assert(_stack_reserved_zone_size == 0, &quot;This should be called only once.&quot;);
1675     _stack_reserved_zone_size = s;
1676   }
<span class="line-modified">1677   address stack_reserved_zone_base() const {</span>
1678     return (address)(stack_end() +
1679                      (stack_red_zone_size() + stack_yellow_zone_size() + stack_reserved_zone_size()));
1680   }
1681   bool in_stack_reserved_zone(address a) {
1682     return (a &lt;= stack_reserved_zone_base()) &amp;&amp;
1683            (a &gt;= (address)((intptr_t)stack_reserved_zone_base() - stack_reserved_zone_size()));
1684   }
1685 
1686   static size_t stack_yellow_reserved_zone_size() {
1687     return _stack_yellow_zone_size + _stack_reserved_zone_size;
1688   }
1689   bool in_stack_yellow_reserved_zone(address a) {
1690     return (a &lt;= stack_reserved_zone_base()) &amp;&amp; (a &gt;= stack_red_zone_base());
1691   }
1692 
1693   // Size of red + yellow + reserved zones.
1694   static size_t stack_guard_zone_size() {
1695     return stack_red_zone_size() + stack_yellow_reserved_zone_size();
1696   }
1697 
</pre>
<hr />
<pre>
1738     _reserved_stack_activation = addr;
1739   }
1740 
1741   // Attempt to reguard the stack after a stack overflow may have occurred.
1742   // Returns true if (a) guard pages are not needed on this thread, (b) the
1743   // pages are already guarded, or (c) the pages were successfully reguarded.
1744   // Returns false if there is not enough stack space to reguard the pages, in
1745   // which case the caller should unwind a frame and try again.  The argument
1746   // should be the caller&#39;s (approximate) sp.
1747   bool reguard_stack(address cur_sp);
1748   // Similar to above but see if current stackpoint is out of the guard area
1749   // and reguard if possible.
1750   bool reguard_stack(void);
1751 
1752   address stack_overflow_limit() { return _stack_overflow_limit; }
1753   void set_stack_overflow_limit() {
1754     _stack_overflow_limit =
1755       stack_end() + MAX2(JavaThread::stack_guard_zone_size(), JavaThread::stack_shadow_zone_size());
1756   }
1757 
<span class="line-added">1758   // Check if address is in the usable part of the stack (excludes protected</span>
<span class="line-added">1759   // guard pages). Can be applied to any thread and is an approximation for</span>
<span class="line-added">1760   // using is_in_live_stack when the query has to happen from another thread.</span>
<span class="line-added">1761   bool is_in_usable_stack(address adr) const {</span>
<span class="line-added">1762     return is_in_stack_range_incl(adr, stack_reserved_zone_base());</span>
<span class="line-added">1763   }</span>
<span class="line-added">1764 </span>
1765   // Misc. accessors/mutators
1766   void set_do_not_unlock(void)                   { _do_not_unlock_if_synchronized = true; }
1767   void clr_do_not_unlock(void)                   { _do_not_unlock_if_synchronized = false; }
1768   bool do_not_unlock(void)                       { return _do_not_unlock_if_synchronized; }
1769 
1770   // For assembly stub generation
1771   static ByteSize threadObj_offset()             { return byte_offset_of(JavaThread, _threadObj); }
1772   static ByteSize jni_environment_offset()       { return byte_offset_of(JavaThread, _jni_environment); }
1773   static ByteSize pending_jni_exception_check_fn_offset() {
1774     return byte_offset_of(JavaThread, _pending_jni_exception_check_fn);
1775   }
1776   static ByteSize last_Java_sp_offset() {
1777     return byte_offset_of(JavaThread, _anchor) + JavaFrameAnchor::last_Java_sp_offset();
1778   }
1779   static ByteSize last_Java_pc_offset() {
1780     return byte_offset_of(JavaThread, _anchor) + JavaFrameAnchor::last_Java_pc_offset();
1781   }
1782   static ByteSize frame_anchor_offset() {
1783     return byte_offset_of(JavaThread, _anchor);
1784   }
</pre>
</td>
</tr>
</table>
<center><a href="thread.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="unhandledOops.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>