<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/heapRegionRemSet.hpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="heapRegionRemSet.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="heapRegionRemSet.inline.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/heapRegionRemSet.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 64 //      other times).
 65 //   2) We find PRT&#39;s in an attempt to add entries.  If a PRT is deleted,
 66 //      it&#39;s _coarse_map bit is set, so the that we were attempting to add
 67 //      is represented.  If a deleted PRT is re-used, a thread adding a bit,
 68 //      thinking the PRT is for a different region, does no harm.
 69 
 70 class OtherRegionsTable {
 71   G1CollectedHeap* _g1h;
 72   Mutex*           _m;
 73 
 74   size_t volatile _num_occupied;
 75 
 76   // These are protected by &quot;_m&quot;.
 77   CHeapBitMap _coarse_map;
 78   size_t      _n_coarse_entries;
 79   static jint _n_coarsenings;
 80 
 81   PerRegionTable** _fine_grain_regions;
 82   size_t           _n_fine_entries;
 83 
<span class="line-modified"> 84   // The fine grain remembered sets are doubly linked together using</span>
<span class="line-modified"> 85   // their &#39;next&#39; and &#39;prev&#39; fields.</span>
 86   // This allows fast bulk freeing of all the fine grain remembered
 87   // set entries, and fast finding of all of them without iterating
 88   // over the _fine_grain_regions table.
 89   PerRegionTable * _first_all_fine_prts;
 90   PerRegionTable * _last_all_fine_prts;
 91 
 92   // Used to sample a subset of the fine grain PRTs to determine which
 93   // PRT to evict and coarsen.
 94   size_t        _fine_eviction_start;
 95   static size_t _fine_eviction_stride;
 96   static size_t _fine_eviction_sample_size;
 97 
 98   SparsePRT   _sparse_table;
 99 
100   // These are static after init.
101   static size_t _max_fine_entries;
102   static size_t _mod_max_fine_entries_mask;
103 
104   // Requires &quot;prt&quot; to be the first element of the bucket list appropriate
105   // for &quot;hr&quot;.  If this list contains an entry for &quot;hr&quot;, return it,
106   // otherwise return &quot;NULL&quot;.
107   PerRegionTable* find_region_table(size_t ind, HeapRegion* hr) const;
108 
109   // Find, delete, and return a candidate PerRegionTable, if any exists,
110   // adding the deleted region to the coarse bitmap.  Requires the caller
111   // to hold _m, and the fine-grain table to be full.
112   PerRegionTable* delete_region_table(size_t&amp; added_by_deleted);
113 
114   // link/add the given fine grain remembered set into the &quot;all&quot; list
115   void link_to_all(PerRegionTable * prt);
<span class="line-removed">116   // unlink/remove the given fine grain remembered set into the &quot;all&quot; list</span>
<span class="line-removed">117   void unlink_from_all(PerRegionTable * prt);</span>
118 
119   bool contains_reference_locked(OopOrNarrowOopStar from) const;
120 
121 public:
122   // Create a new remembered set. The given mutex is used to ensure consistency.
123   OtherRegionsTable(Mutex* m);
124 
125   template &lt;class Closure&gt;
126   void iterate(Closure&amp; v);
127 
128   // Returns the card index of the given within_region pointer relative to the bottom
129   // of the given heap region.
130   static CardIdx_t card_within_region(OopOrNarrowOopStar within_region, HeapRegion* hr);
131   // Adds the reference from &quot;from to this remembered set.
132   void add_reference(OopOrNarrowOopStar from, uint tid);
133 
134   // Returns whether the remembered set contains the given reference.
135   bool contains_reference(OopOrNarrowOopStar from) const;
136 
137   // Returns whether this remembered set (and all sub-sets) have an occupancy
</pre>
<hr />
<pre>
150   size_t mem_size() const;
151   // Returns the size of static data in bytes.
152   static size_t static_mem_size();
153   // Returns the size of the free list content in bytes.
154   static size_t fl_mem_size();
155 
156   // Clear the entire contents of this remembered set.
157   void clear();
158 };
159 
160 class PerRegionTable: public CHeapObj&lt;mtGC&gt; {
161   friend class OtherRegionsTable;
162 
163   HeapRegion*     _hr;
164   CHeapBitMap     _bm;
165   jint            _occupied;
166 
167   // next pointer for free/allocated &#39;all&#39; list
168   PerRegionTable* _next;
169 
<span class="line-removed">170   // prev pointer for the allocated &#39;all&#39; list</span>
<span class="line-removed">171   PerRegionTable* _prev;</span>
<span class="line-removed">172 </span>
173   // next pointer in collision list
174   PerRegionTable * _collision_list_next;
175 
176   // Global free list of PRTs
177   static PerRegionTable* volatile _free_list;
178 
179 protected:
180   PerRegionTable(HeapRegion* hr) :
181     _hr(hr),
182     _bm(HeapRegion::CardsPerRegion, mtGC),
183     _occupied(0),
<span class="line-modified">184     _next(NULL), _prev(NULL),</span>
185     _collision_list_next(NULL)
186   {}
187 
188 public:
189   // We need access in order to union things into the base table.
190   BitMap* bm() { return &amp;_bm; }
191 
192   HeapRegion* hr() const { return Atomic::load_acquire(&amp;_hr); }
193 
194   jint occupied() const {
195     return _occupied;
196   }
197 
198   void init(HeapRegion* hr, bool clear_links_to_all_list);
199 
200   inline bool add_reference(OopOrNarrowOopStar from);
201 
202   inline bool add_card(CardIdx_t from_card_index);
203 
204   // (Destructively) union the bitmap of the current table into the given
</pre>
<hr />
<pre>
226     while (true) {
227       PerRegionTable* fl = _free_list;
228       last-&gt;set_next(fl);
229       PerRegionTable* res = Atomic::cmpxchg(&amp;_free_list, fl, prt);
230       if (res == fl) {
231         return;
232       }
233     }
234     ShouldNotReachHere();
235   }
236 
237   static void free(PerRegionTable* prt) {
238     bulk_free(prt, prt);
239   }
240 
241   // Returns an initialized PerRegionTable instance.
242   static PerRegionTable* alloc(HeapRegion* hr);
243 
244   PerRegionTable* next() const { return _next; }
245   void set_next(PerRegionTable* next) { _next = next; }
<span class="line-removed">246   PerRegionTable* prev() const { return _prev; }</span>
<span class="line-removed">247   void set_prev(PerRegionTable* prev) { _prev = prev; }</span>
248 
249   // Accessor and Modification routines for the pointer for the
250   // singly linked collision list that links the PRTs within the
251   // OtherRegionsTable::_fine_grain_regions hash table.
252   //
<span class="line-removed">253   // It might be useful to also make the collision list doubly linked</span>
<span class="line-removed">254   // to avoid iteration over the collisions list during scrubbing/deletion.</span>
<span class="line-removed">255   // OTOH there might not be many collisions.</span>
256 
257   PerRegionTable* collision_list_next() const {
258     return _collision_list_next;
259   }
260 
261   void set_collision_list_next(PerRegionTable* next) {
262     _collision_list_next = next;
263   }
264 
265   PerRegionTable** collision_list_next_addr() {
266     return &amp;_collision_list_next;
267   }
268 
269   static size_t fl_mem_size() {
270     PerRegionTable* cur = _free_list;
271     size_t res = 0;
272     while (cur != NULL) {
273       res += cur-&gt;mem_size();
274       cur = cur-&gt;next();
275     }
</pre>
</td>
<td>
<hr />
<pre>
 64 //      other times).
 65 //   2) We find PRT&#39;s in an attempt to add entries.  If a PRT is deleted,
 66 //      it&#39;s _coarse_map bit is set, so the that we were attempting to add
 67 //      is represented.  If a deleted PRT is re-used, a thread adding a bit,
 68 //      thinking the PRT is for a different region, does no harm.
 69 
 70 class OtherRegionsTable {
 71   G1CollectedHeap* _g1h;
 72   Mutex*           _m;
 73 
 74   size_t volatile _num_occupied;
 75 
 76   // These are protected by &quot;_m&quot;.
 77   CHeapBitMap _coarse_map;
 78   size_t      _n_coarse_entries;
 79   static jint _n_coarsenings;
 80 
 81   PerRegionTable** _fine_grain_regions;
 82   size_t           _n_fine_entries;
 83 
<span class="line-modified"> 84   // The fine grain remembered sets are linked together using</span>
<span class="line-modified"> 85   // their &#39;next&#39; fields.</span>
 86   // This allows fast bulk freeing of all the fine grain remembered
 87   // set entries, and fast finding of all of them without iterating
 88   // over the _fine_grain_regions table.
 89   PerRegionTable * _first_all_fine_prts;
 90   PerRegionTable * _last_all_fine_prts;
 91 
 92   // Used to sample a subset of the fine grain PRTs to determine which
 93   // PRT to evict and coarsen.
 94   size_t        _fine_eviction_start;
 95   static size_t _fine_eviction_stride;
 96   static size_t _fine_eviction_sample_size;
 97 
 98   SparsePRT   _sparse_table;
 99 
100   // These are static after init.
101   static size_t _max_fine_entries;
102   static size_t _mod_max_fine_entries_mask;
103 
104   // Requires &quot;prt&quot; to be the first element of the bucket list appropriate
105   // for &quot;hr&quot;.  If this list contains an entry for &quot;hr&quot;, return it,
106   // otherwise return &quot;NULL&quot;.
107   PerRegionTable* find_region_table(size_t ind, HeapRegion* hr) const;
108 
109   // Find, delete, and return a candidate PerRegionTable, if any exists,
110   // adding the deleted region to the coarse bitmap.  Requires the caller
111   // to hold _m, and the fine-grain table to be full.
112   PerRegionTable* delete_region_table(size_t&amp; added_by_deleted);
113 
114   // link/add the given fine grain remembered set into the &quot;all&quot; list
115   void link_to_all(PerRegionTable * prt);


116 
117   bool contains_reference_locked(OopOrNarrowOopStar from) const;
118 
119 public:
120   // Create a new remembered set. The given mutex is used to ensure consistency.
121   OtherRegionsTable(Mutex* m);
122 
123   template &lt;class Closure&gt;
124   void iterate(Closure&amp; v);
125 
126   // Returns the card index of the given within_region pointer relative to the bottom
127   // of the given heap region.
128   static CardIdx_t card_within_region(OopOrNarrowOopStar within_region, HeapRegion* hr);
129   // Adds the reference from &quot;from to this remembered set.
130   void add_reference(OopOrNarrowOopStar from, uint tid);
131 
132   // Returns whether the remembered set contains the given reference.
133   bool contains_reference(OopOrNarrowOopStar from) const;
134 
135   // Returns whether this remembered set (and all sub-sets) have an occupancy
</pre>
<hr />
<pre>
148   size_t mem_size() const;
149   // Returns the size of static data in bytes.
150   static size_t static_mem_size();
151   // Returns the size of the free list content in bytes.
152   static size_t fl_mem_size();
153 
154   // Clear the entire contents of this remembered set.
155   void clear();
156 };
157 
158 class PerRegionTable: public CHeapObj&lt;mtGC&gt; {
159   friend class OtherRegionsTable;
160 
161   HeapRegion*     _hr;
162   CHeapBitMap     _bm;
163   jint            _occupied;
164 
165   // next pointer for free/allocated &#39;all&#39; list
166   PerRegionTable* _next;
167 



168   // next pointer in collision list
169   PerRegionTable * _collision_list_next;
170 
171   // Global free list of PRTs
172   static PerRegionTable* volatile _free_list;
173 
174 protected:
175   PerRegionTable(HeapRegion* hr) :
176     _hr(hr),
177     _bm(HeapRegion::CardsPerRegion, mtGC),
178     _occupied(0),
<span class="line-modified">179     _next(NULL),</span>
180     _collision_list_next(NULL)
181   {}
182 
183 public:
184   // We need access in order to union things into the base table.
185   BitMap* bm() { return &amp;_bm; }
186 
187   HeapRegion* hr() const { return Atomic::load_acquire(&amp;_hr); }
188 
189   jint occupied() const {
190     return _occupied;
191   }
192 
193   void init(HeapRegion* hr, bool clear_links_to_all_list);
194 
195   inline bool add_reference(OopOrNarrowOopStar from);
196 
197   inline bool add_card(CardIdx_t from_card_index);
198 
199   // (Destructively) union the bitmap of the current table into the given
</pre>
<hr />
<pre>
221     while (true) {
222       PerRegionTable* fl = _free_list;
223       last-&gt;set_next(fl);
224       PerRegionTable* res = Atomic::cmpxchg(&amp;_free_list, fl, prt);
225       if (res == fl) {
226         return;
227       }
228     }
229     ShouldNotReachHere();
230   }
231 
232   static void free(PerRegionTable* prt) {
233     bulk_free(prt, prt);
234   }
235 
236   // Returns an initialized PerRegionTable instance.
237   static PerRegionTable* alloc(HeapRegion* hr);
238 
239   PerRegionTable* next() const { return _next; }
240   void set_next(PerRegionTable* next) { _next = next; }


241 
242   // Accessor and Modification routines for the pointer for the
243   // singly linked collision list that links the PRTs within the
244   // OtherRegionsTable::_fine_grain_regions hash table.
245   //



246 
247   PerRegionTable* collision_list_next() const {
248     return _collision_list_next;
249   }
250 
251   void set_collision_list_next(PerRegionTable* next) {
252     _collision_list_next = next;
253   }
254 
255   PerRegionTable** collision_list_next_addr() {
256     return &amp;_collision_list_next;
257   }
258 
259   static size_t fl_mem_size() {
260     PerRegionTable* cur = _free_list;
261     size_t res = 0;
262     while (cur != NULL) {
263       res += cur-&gt;mem_size();
264       cur = cur-&gt;next();
265     }
</pre>
</td>
</tr>
</table>
<center><a href="heapRegionRemSet.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="heapRegionRemSet.inline.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>