<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1DirtyCardQueue.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1ConcurrentRefine.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1DirtyCardQueue.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1DirtyCardQueue.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 38 #include &quot;runtime/atomic.hpp&quot;
 39 #include &quot;runtime/os.hpp&quot;
 40 #include &quot;runtime/safepoint.hpp&quot;
 41 #include &quot;runtime/thread.inline.hpp&quot;
 42 #include &quot;runtime/threadSMR.hpp&quot;
 43 #include &quot;utilities/globalCounter.inline.hpp&quot;
 44 #include &quot;utilities/macros.hpp&quot;
 45 #include &quot;utilities/quickSort.hpp&quot;
 46 
 47 G1DirtyCardQueue::G1DirtyCardQueue(G1DirtyCardQueueSet* qset) :
 48   // Dirty card queues are always active, so we create them with their
 49   // active field set to true.
 50   PtrQueue(qset, true /* active */)
 51 { }
 52 
 53 G1DirtyCardQueue::~G1DirtyCardQueue() {
 54   flush();
 55 }
 56 
 57 void G1DirtyCardQueue::handle_completed_buffer() {
<span class="line-modified"> 58   assert(_buf != NULL, &quot;precondition&quot;);</span>
 59   BufferNode* node = BufferNode::make_node_from_buffer(_buf, index());
<span class="line-modified"> 60   G1DirtyCardQueueSet* dcqs = dirty_card_qset();</span>
<span class="line-modified"> 61   if (dcqs-&gt;process_or_enqueue_completed_buffer(node)) {</span>
<span class="line-removed"> 62     reset();                    // Buffer fully processed, reset index.</span>
<span class="line-removed"> 63   } else {</span>
<span class="line-removed"> 64     allocate_buffer();          // Buffer enqueued, get a new one.</span>
<span class="line-removed"> 65   }</span>
 66 }
 67 
 68 // Assumed to be zero by concurrent threads.
 69 static uint par_ids_start() { return 0; }
 70 
 71 G1DirtyCardQueueSet::G1DirtyCardQueueSet(BufferNode::Allocator* allocator) :
 72   PtrQueueSet(allocator),
 73   _primary_refinement_thread(NULL),
 74   _num_cards(0),
 75   _completed(),
 76   _paused(),
 77   _free_ids(par_ids_start(), num_par_ids()),
 78   _process_cards_threshold(ProcessCardsThresholdNever),
 79   _max_cards(MaxCardsUnlimited),
<span class="line-modified"> 80   _max_cards_padding(0),</span>
 81   _mutator_refined_cards_counters(NEW_C_HEAP_ARRAY(size_t, num_par_ids(), mtGC))
 82 {
 83   ::memset(_mutator_refined_cards_counters, 0, num_par_ids() * sizeof(size_t));
 84   _all_active = true;
 85 }
 86 
 87 G1DirtyCardQueueSet::~G1DirtyCardQueueSet() {
 88   abandon_completed_buffers();
 89   FREE_C_HEAP_ARRAY(size_t, _mutator_refined_cards_counters);
 90 }
 91 
 92 // Determines how many mutator threads can process the buffers in parallel.
 93 uint G1DirtyCardQueueSet::num_par_ids() {
 94   return (uint)os::initial_active_processor_count();
 95 }
 96 
 97 size_t G1DirtyCardQueueSet::total_mutator_refined_cards() const {
 98   size_t sum = 0;
 99   for (uint i = 0; i &lt; num_par_ids(); ++i) {
100     sum += _mutator_refined_cards_counters[i];
</pre>
<hr />
<pre>
121 // It then sets the &quot;next&quot; value of the old tail to the head of the list being
122 // appended; it is an invariant that the old tail&#39;s &quot;next&quot; value is NULL.
123 // But if the old tail is NULL then the queue was empty.  In this case the
124 // head of the list being appended is instead stored in the queue head; it is
125 // an invariant that the queue head is NULL in this case.
126 //
127 // This means there is a period between the exchange and the old tail update
128 // where the queue sequence is split into two parts, the list from the queue
129 // head to the old tail, and the list being appended.  If there are concurrent
130 // push/append operations, each may introduce another such segment.  But they
131 // all eventually get resolved by their respective updates of their old tail&#39;s
132 // &quot;next&quot; value.  This also means that pop operations must handle a buffer
133 // with a NULL &quot;next&quot; value specially.
134 //
135 // A push operation is just a degenerate append, where the buffer being pushed
136 // is both the head and the tail of the list being appended.
137 void G1DirtyCardQueueSet::Queue::append(BufferNode&amp; first, BufferNode&amp; last) {
138   assert(last.next() == NULL, &quot;precondition&quot;);
139   BufferNode* old_tail = Atomic::xchg(&amp;_tail, &amp;last);
140   if (old_tail == NULL) {       // Was empty.
<span class="line-removed">141     assert(Atomic::load(&amp;_head) == NULL, &quot;invariant&quot;);</span>
142     Atomic::store(&amp;_head, &amp;first);
143   } else {
144     assert(old_tail-&gt;next() == NULL, &quot;invariant&quot;);
145     old_tail-&gt;set_next(&amp;first);
146   }
147 }
148 
<span class="line-removed">149 // pop gets the queue head as the candidate result (returning NULL if the</span>
<span class="line-removed">150 // queue head was NULL), and then gets that result node&#39;s &quot;next&quot; value.  If</span>
<span class="line-removed">151 // that &quot;next&quot; value is NULL and the queue head hasn&#39;t changed, then there</span>
<span class="line-removed">152 // is only one element in the accessible part of the list (the sequence from</span>
<span class="line-removed">153 // head to a node with a NULL &quot;next&quot; value).  We can&#39;t return that element,</span>
<span class="line-removed">154 // because it may be the old tail of a concurrent push/append that has not</span>
<span class="line-removed">155 // yet had its &quot;next&quot; field set to the new tail.  So return NULL in this case.</span>
<span class="line-removed">156 // Otherwise, attempt to cmpxchg that &quot;next&quot; value into the queue head,</span>
<span class="line-removed">157 // retrying the whole operation if that fails. This is the &quot;usual&quot; lock-free</span>
<span class="line-removed">158 // pop from the head of a singly linked list, with the additional restriction</span>
<span class="line-removed">159 // on taking the last element.</span>
160 BufferNode* G1DirtyCardQueueSet::Queue::pop() {
161   Thread* current_thread = Thread::current();
162   while (true) {
163     // Use a critical section per iteration, rather than over the whole
<span class="line-modified">164     // operation.  We&#39;re not guaranteed to make progress, because of possible</span>
<span class="line-modified">165     // contention on the queue head.  Lingering in one CS the whole time could</span>
<span class="line-modified">166     // lead to excessive allocation of buffers, because the CS blocks return</span>
<span class="line-removed">167     // of released buffers to the free list for reuse.</span>
168     GlobalCounter::CriticalSection cs(current_thread);
169 
170     BufferNode* result = Atomic::load_acquire(&amp;_head);
<span class="line-modified">171     // Check for empty queue.  Only needs to be done on first iteration,</span>
<span class="line-removed">172     // since we never take the last element, but it&#39;s messy to make use</span>
<span class="line-removed">173     // of that and we expect one iteration to be the common case.</span>
<span class="line-removed">174     if (result == NULL) return NULL;</span>
175 
176     BufferNode* next = Atomic::load_acquire(BufferNode::next_ptr(*result));
177     if (next != NULL) {
<span class="line-modified">178       next = Atomic::cmpxchg(&amp;_head, result, next);</span>
<span class="line-modified">179       if (next == result) {</span>
180         // Former head successfully taken; it is not the last.
181         assert(Atomic::load(&amp;_tail) != result, &quot;invariant&quot;);
182         assert(result-&gt;next() != NULL, &quot;invariant&quot;);
183         result-&gt;set_next(NULL);
184         return result;
185       }
<span class="line-modified">186       // cmpxchg failed; try again.</span>
<span class="line-modified">187     } else if (result == Atomic::load_acquire(&amp;_head)) {</span>
<span class="line-modified">188       // If follower of head is NULL and head hasn&#39;t changed, then only</span>
<span class="line-modified">189       // the one element is currently accessible.  We don&#39;t take the last</span>
<span class="line-modified">190       // accessible element, because there may be a concurrent add using it.</span>
<span class="line-modified">191       // The check for unchanged head isn&#39;t needed for correctness, but the</span>
<span class="line-modified">192       // retry on change may sometimes let us get a buffer after all.</span>
<span class="line-modified">193       return NULL;</span>

















194     }
<span class="line-modified">195     // Head changed; try again.</span>










196   }
197 }
198 
199 G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::Queue::take_all() {
200   assert_at_safepoint();
201   HeadTail result(Atomic::load(&amp;_head), Atomic::load(&amp;_tail));
202   Atomic::store(&amp;_head, (BufferNode*)NULL);
203   Atomic::store(&amp;_tail, (BufferNode*)NULL);
204   return result;
205 }
206 
207 void G1DirtyCardQueueSet::enqueue_completed_buffer(BufferNode* cbn) {
208   assert(cbn != NULL, &quot;precondition&quot;);
209   // Increment _num_cards before adding to queue, so queue removal doesn&#39;t
210   // need to deal with _num_cards possibly going negative.
211   size_t new_num_cards = Atomic::add(&amp;_num_cards, buffer_size() - cbn-&gt;index());
212   _completed.push(*cbn);
213   if ((new_num_cards &gt; process_cards_threshold()) &amp;&amp;
214       (_primary_refinement_thread != NULL)) {
215     _primary_refinement_thread-&gt;activate();
216   }
217 }
218 
<span class="line-modified">219 BufferNode* G1DirtyCardQueueSet::get_completed_buffer(size_t stop_at) {</span>
<span class="line-removed">220   enqueue_previous_paused_buffers();</span>
<span class="line-removed">221 </span>
<span class="line-removed">222   // Check for insufficient cards to satisfy request.  We only do this once,</span>
<span class="line-removed">223   // up front, rather than on each iteration below, since the test is racy</span>
<span class="line-removed">224   // regardless of when we do it.</span>
<span class="line-removed">225   if (Atomic::load_acquire(&amp;_num_cards) &lt;= stop_at) {</span>
<span class="line-removed">226     return NULL;</span>
<span class="line-removed">227   }</span>
<span class="line-removed">228 </span>
229   BufferNode* result = _completed.pop();
<span class="line-modified">230   if (result != NULL) {</span>
<span class="line-modified">231     Atomic::sub(&amp;_num_cards, buffer_size() - result-&gt;index());</span>


232   }

233   return result;
234 }
235 
236 #ifdef ASSERT
237 void G1DirtyCardQueueSet::verify_num_cards() const {
238   size_t actual = 0;
239   BufferNode* cur = _completed.top();
240   for ( ; cur != NULL; cur = cur-&gt;next()) {
241     actual += buffer_size() - cur-&gt;index();
242   }
243   assert(actual == Atomic::load(&amp;_num_cards),
244          &quot;Num entries in completed buffers should be &quot; SIZE_FORMAT &quot; but are &quot; SIZE_FORMAT,
245          Atomic::load(&amp;_num_cards), actual);
246 }
247 #endif // ASSERT
248 
249 G1DirtyCardQueueSet::PausedBuffers::PausedList::PausedList() :
250   _head(NULL), _tail(NULL),
251   _safepoint_id(SafepointSynchronize::safepoint_id())
252 {}
</pre>
<hr />
<pre>
270   if (old_head == NULL) {
271     assert(_tail == NULL, &quot;invariant&quot;);
272     _tail = node;
273   } else {
274     node-&gt;set_next(old_head);
275   }
276 }
277 
278 G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::PausedBuffers::PausedList::take() {
279   BufferNode* head = Atomic::load(&amp;_head);
280   BufferNode* tail = _tail;
281   Atomic::store(&amp;_head, (BufferNode*)NULL);
282   _tail = NULL;
283   return HeadTail(head, tail);
284 }
285 
286 G1DirtyCardQueueSet::PausedBuffers::PausedBuffers() : _plist(NULL) {}
287 
288 #ifdef ASSERT
289 G1DirtyCardQueueSet::PausedBuffers::~PausedBuffers() {
<span class="line-modified">290   assert(is_empty(), &quot;invariant&quot;);</span>
291 }
292 #endif // ASSERT
293 
<span class="line-removed">294 bool G1DirtyCardQueueSet::PausedBuffers::is_empty() const {</span>
<span class="line-removed">295   return Atomic::load(&amp;_plist) == NULL;</span>
<span class="line-removed">296 }</span>
<span class="line-removed">297 </span>
298 void G1DirtyCardQueueSet::PausedBuffers::add(BufferNode* node) {
299   assert_not_at_safepoint();
300   PausedList* plist = Atomic::load_acquire(&amp;_plist);
<span class="line-modified">301   if (plist != NULL) {</span>
<span class="line-removed">302     // Already have a next list, so use it.  We know it&#39;s a next list because</span>
<span class="line-removed">303     // of the precondition that take_previous() has already been called.</span>
<span class="line-removed">304     assert(plist-&gt;is_next(), &quot;invariant&quot;);</span>
<span class="line-removed">305   } else {</span>
306     // Try to install a new next list.
307     plist = new PausedList();
308     PausedList* old_plist = Atomic::cmpxchg(&amp;_plist, (PausedList*)NULL, plist);
309     if (old_plist != NULL) {
<span class="line-modified">310       // Some other thread installed a new next list. Use it instead.</span>
311       delete plist;
312       plist = old_plist;
313     }
314   }

315   plist-&gt;add(node);
316 }
317 
318 G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::PausedBuffers::take_previous() {
319   assert_not_at_safepoint();
320   PausedList* previous;
321   {
322     // Deal with plist in a critical section, to prevent it from being
323     // deleted out from under us by a concurrent take_previous().
324     GlobalCounter::CriticalSection cs(Thread::current());
325     previous = Atomic::load_acquire(&amp;_plist);
326     if ((previous == NULL) ||   // Nothing to take.
327         previous-&gt;is_next() ||  // Not from a previous safepoint.
328         // Some other thread stole it.
329         (Atomic::cmpxchg(&amp;_plist, previous, (PausedList*)NULL) != previous)) {
330       return HeadTail();
331     }
332   }
333   // We now own previous.
334   HeadTail result = previous-&gt;take();
</pre>
<hr />
<pre>
338   GlobalCounter::write_synchronize();
339   delete previous;
340   return result;
341 }
342 
343 G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::PausedBuffers::take_all() {
344   assert_at_safepoint();
345   HeadTail result;
346   PausedList* plist = Atomic::load(&amp;_plist);
347   if (plist != NULL) {
348     Atomic::store(&amp;_plist, (PausedList*)NULL);
349     result = plist-&gt;take();
350     delete plist;
351   }
352   return result;
353 }
354 
355 void G1DirtyCardQueueSet::record_paused_buffer(BufferNode* node) {
356   assert_not_at_safepoint();
357   assert(node-&gt;next() == NULL, &quot;precondition&quot;);


358   // Cards for paused buffers are included in count, to contribute to
359   // notification checking after the coming safepoint if it doesn&#39;t GC.
360   // Note that this means the queue&#39;s _num_cards differs from the number
361   // of cards in the queued buffers when there are paused buffers.
362   Atomic::add(&amp;_num_cards, buffer_size() - node-&gt;index());
363   _paused.add(node);
364 }
365 
366 void G1DirtyCardQueueSet::enqueue_paused_buffers_aux(const HeadTail&amp; paused) {
367   if (paused._head != NULL) {
368     assert(paused._tail != NULL, &quot;invariant&quot;);
369     // Cards from paused buffers are already recorded in the queue count.
370     _completed.append(*paused._head, *paused._tail);
371   }
372 }
373 
374 void G1DirtyCardQueueSet::enqueue_previous_paused_buffers() {
375   assert_not_at_safepoint();
<span class="line-modified">376   // The fast-path still satisfies the precondition for record_paused_buffer</span>
<span class="line-removed">377   // and PausedBuffers::add, even with a racy test.  If there are paused</span>
<span class="line-removed">378   // buffers from a previous safepoint, is_empty() will return false; there</span>
<span class="line-removed">379   // will have been a safepoint between recording and test, so there can&#39;t be</span>
<span class="line-removed">380   // a false negative (is_empty() returns true) while such buffers are present.</span>
<span class="line-removed">381   // If is_empty() is false, there are two cases:</span>
<span class="line-removed">382   //</span>
<span class="line-removed">383   // (1) There were paused buffers from a previous safepoint.  A concurrent</span>
<span class="line-removed">384   // caller may take and enqueue them first, but that&#39;s okay; the precondition</span>
<span class="line-removed">385   // for a possible later record_paused_buffer by this thread will still hold.</span>
<span class="line-removed">386   //</span>
<span class="line-removed">387   // (2) There are paused buffers for a requested next safepoint.</span>
<span class="line-removed">388   //</span>
<span class="line-removed">389   // In each of those cases some effort may be spent detecting and dealing</span>
<span class="line-removed">390   // with those circumstances; any wasted effort in such cases is expected to</span>
<span class="line-removed">391   // be well compensated by the fast path.</span>
<span class="line-removed">392   if (!_paused.is_empty()) {</span>
<span class="line-removed">393     enqueue_paused_buffers_aux(_paused.take_previous());</span>
<span class="line-removed">394   }</span>
395 }
396 
397 void G1DirtyCardQueueSet::enqueue_all_paused_buffers() {
398   assert_at_safepoint();
399   enqueue_paused_buffers_aux(_paused.take_all());
400 }
401 
402 void G1DirtyCardQueueSet::abandon_completed_buffers() {
403   enqueue_all_paused_buffers();
404   verify_num_cards();
405   G1BufferNodeList list = take_all_completed_buffers();
406   BufferNode* buffers_to_delete = list._head;
407   while (buffers_to_delete != NULL) {
408     BufferNode* bn = buffers_to_delete;
409     buffers_to_delete = bn-&gt;next();
410     bn-&gt;set_next(NULL);
411     deallocate_buffer(bn);
412   }
413 }
414 
</pre>
<hr />
<pre>
545     // humongous object allocation (see comment at the StoreStore fence before
546     // setting the regions&#39; tops in humongous allocation path).
547     // It&#39;s okay that reading region&#39;s top and reading region&#39;s type were racy
548     // wrto each other. We need both set, in any order, to proceed.
549     OrderAccess::fence();
550     sort_cards(first_clean_index);
551     return refine_cleaned_cards(first_clean_index);
552   }
553 };
554 
555 bool G1DirtyCardQueueSet::refine_buffer(BufferNode* node,
556                                         uint worker_id,
557                                         size_t* total_refined_cards) {
558   G1RefineBufferedCards buffered_cards(node,
559                                        buffer_size(),
560                                        worker_id,
561                                        total_refined_cards);
562   return buffered_cards.refine();
563 }
564 
<span class="line-modified">565 #ifndef ASSERT</span>
<span class="line-modified">566 #define assert_fully_consumed(node, buffer_size)</span>
<span class="line-modified">567 #else</span>
<span class="line-modified">568 #define assert_fully_consumed(node, buffer_size)                \</span>
<span class="line-modified">569   do {                                                          \</span>
<span class="line-modified">570     size_t _afc_index = (node)-&gt;index();                        \</span>
<span class="line-modified">571     size_t _afc_size = (buffer_size);                           \</span>
<span class="line-modified">572     assert(_afc_index == _afc_size,                             \</span>
<span class="line-modified">573            &quot;Buffer was not fully consumed as claimed: index: &quot;  \</span>
<span class="line-modified">574            SIZE_FORMAT &quot;, size: &quot; SIZE_FORMAT,                  \</span>
<span class="line-modified">575             _afc_index, _afc_size);                             \</span>
<span class="line-modified">576   } while (0)</span>
<span class="line-removed">577 #endif // ASSERT</span>
<span class="line-removed">578 </span>
<span class="line-removed">579 bool G1DirtyCardQueueSet::process_or_enqueue_completed_buffer(BufferNode* node) {</span>
<span class="line-removed">580   if (Thread::current()-&gt;is_Java_thread()) {</span>
<span class="line-removed">581     // If the number of buffers exceeds the limit, make this Java</span>
<span class="line-removed">582     // thread do the processing itself.  Calculation is racy but we</span>
<span class="line-removed">583     // don&#39;t need precision here.  The add of padding could overflow,</span>
<span class="line-removed">584     // which is treated as unlimited.</span>
<span class="line-removed">585     size_t limit = max_cards() + max_cards_padding();</span>
<span class="line-removed">586     if ((num_cards() &gt; limit) &amp;&amp; (limit &gt;= max_cards())) {</span>
<span class="line-removed">587       if (mut_process_buffer(node)) {</span>
<span class="line-removed">588         return true;</span>
<span class="line-removed">589       }</span>
<span class="line-removed">590       // Buffer was incompletely processed because of a pending safepoint</span>
<span class="line-removed">591       // request.  Unlike with refinement thread processing, for mutator</span>
<span class="line-removed">592       // processing the buffer did not come from the completed buffer queue,</span>
<span class="line-removed">593       // so it is okay to add it to the queue rather than to the paused set.</span>
<span class="line-removed">594       // Indeed, it can&#39;t be added to the paused set because we didn&#39;t pass</span>
<span class="line-removed">595       // through enqueue_previous_paused_buffers.</span>
<span class="line-removed">596     }</span>
597   }
<span class="line-removed">598   enqueue_completed_buffer(node);</span>
<span class="line-removed">599   return false;</span>
600 }
601 
<span class="line-modified">602 bool G1DirtyCardQueueSet::mut_process_buffer(BufferNode* node) {</span>

















603   uint worker_id = _free_ids.claim_par_id(); // temporarily claim an id
604   uint counter_index = worker_id - par_ids_start();
605   size_t* counter = &amp;_mutator_refined_cards_counters[counter_index];
<span class="line-modified">606   bool result = refine_buffer(node, worker_id, counter);</span>
607   _free_ids.release_par_id(worker_id); // release the id
608 
<span class="line-modified">609   if (result) {</span>
<span class="line-modified">610     assert_fully_consumed(node, buffer_size());</span>
<span class="line-removed">611   }</span>
<span class="line-removed">612   return result;</span>
613 }
614 
615 bool G1DirtyCardQueueSet::refine_completed_buffer_concurrently(uint worker_id,
616                                                                size_t stop_at,
617                                                                size_t* total_refined_cards) {
<span class="line-modified">618   BufferNode* node = get_completed_buffer(stop_at);</span>
<span class="line-modified">619   if (node == NULL) {</span>
<span class="line-modified">620     return false;</span>
<span class="line-modified">621   } else if (refine_buffer(node, worker_id, total_refined_cards)) {</span>
<span class="line-modified">622     assert_fully_consumed(node, buffer_size());</span>
<span class="line-modified">623     // Done with fully processed buffer.</span>
<span class="line-modified">624     deallocate_buffer(node);</span>
<span class="line-modified">625     return true;</span>
<span class="line-modified">626   } else {</span>
<span class="line-removed">627     // Buffer incompletely processed because there is a pending safepoint.</span>
<span class="line-removed">628     // Record partially processed buffer, to be finished later.</span>
<span class="line-removed">629     record_paused_buffer(node);</span>
<span class="line-removed">630     return true;</span>
<span class="line-removed">631   }</span>
632 }
633 
634 void G1DirtyCardQueueSet::abandon_logs() {
635   assert_at_safepoint();
636   abandon_completed_buffers();
637 
638   // Since abandon is done only at safepoints, we can safely manipulate
639   // these queues.
640   struct AbandonThreadLogClosure : public ThreadClosure {
641     virtual void do_thread(Thread* t) {
642       G1ThreadLocalData::dirty_card_queue(t).reset();
643     }
644   } closure;
645   Threads::threads_do(&amp;closure);
646 
647   G1BarrierSet::shared_dirty_card_queue().reset();
648 }
649 
650 void G1DirtyCardQueueSet::concatenate_logs() {
651   // Iterate over all the threads, if we find a partial log add it to
</pre>
<hr />
<pre>
653   // of outstanding buffers.
654   assert_at_safepoint();
655   size_t old_limit = max_cards();
656   set_max_cards(MaxCardsUnlimited);
657 
658   struct ConcatenateThreadLogClosure : public ThreadClosure {
659     virtual void do_thread(Thread* t) {
660       G1DirtyCardQueue&amp; dcq = G1ThreadLocalData::dirty_card_queue(t);
661       if (!dcq.is_empty()) {
662         dcq.flush();
663       }
664     }
665   } closure;
666   Threads::threads_do(&amp;closure);
667 
668   G1BarrierSet::shared_dirty_card_queue().flush();
669   enqueue_all_paused_buffers();
670   verify_num_cards();
671   set_max_cards(old_limit);
672 }

























</pre>
</td>
<td>
<hr />
<pre>
 38 #include &quot;runtime/atomic.hpp&quot;
 39 #include &quot;runtime/os.hpp&quot;
 40 #include &quot;runtime/safepoint.hpp&quot;
 41 #include &quot;runtime/thread.inline.hpp&quot;
 42 #include &quot;runtime/threadSMR.hpp&quot;
 43 #include &quot;utilities/globalCounter.inline.hpp&quot;
 44 #include &quot;utilities/macros.hpp&quot;
 45 #include &quot;utilities/quickSort.hpp&quot;
 46 
 47 G1DirtyCardQueue::G1DirtyCardQueue(G1DirtyCardQueueSet* qset) :
 48   // Dirty card queues are always active, so we create them with their
 49   // active field set to true.
 50   PtrQueue(qset, true /* active */)
 51 { }
 52 
 53 G1DirtyCardQueue::~G1DirtyCardQueue() {
 54   flush();
 55 }
 56 
 57 void G1DirtyCardQueue::handle_completed_buffer() {
<span class="line-modified"> 58   assert(!is_empty(), &quot;precondition&quot;);</span>
 59   BufferNode* node = BufferNode::make_node_from_buffer(_buf, index());
<span class="line-modified"> 60   allocate_buffer();</span>
<span class="line-modified"> 61   dirty_card_qset()-&gt;handle_completed_buffer(node);</span>




 62 }
 63 
 64 // Assumed to be zero by concurrent threads.
 65 static uint par_ids_start() { return 0; }
 66 
 67 G1DirtyCardQueueSet::G1DirtyCardQueueSet(BufferNode::Allocator* allocator) :
 68   PtrQueueSet(allocator),
 69   _primary_refinement_thread(NULL),
 70   _num_cards(0),
 71   _completed(),
 72   _paused(),
 73   _free_ids(par_ids_start(), num_par_ids()),
 74   _process_cards_threshold(ProcessCardsThresholdNever),
 75   _max_cards(MaxCardsUnlimited),
<span class="line-modified"> 76   _padded_max_cards(MaxCardsUnlimited),</span>
 77   _mutator_refined_cards_counters(NEW_C_HEAP_ARRAY(size_t, num_par_ids(), mtGC))
 78 {
 79   ::memset(_mutator_refined_cards_counters, 0, num_par_ids() * sizeof(size_t));
 80   _all_active = true;
 81 }
 82 
 83 G1DirtyCardQueueSet::~G1DirtyCardQueueSet() {
 84   abandon_completed_buffers();
 85   FREE_C_HEAP_ARRAY(size_t, _mutator_refined_cards_counters);
 86 }
 87 
 88 // Determines how many mutator threads can process the buffers in parallel.
 89 uint G1DirtyCardQueueSet::num_par_ids() {
 90   return (uint)os::initial_active_processor_count();
 91 }
 92 
 93 size_t G1DirtyCardQueueSet::total_mutator_refined_cards() const {
 94   size_t sum = 0;
 95   for (uint i = 0; i &lt; num_par_ids(); ++i) {
 96     sum += _mutator_refined_cards_counters[i];
</pre>
<hr />
<pre>
117 // It then sets the &quot;next&quot; value of the old tail to the head of the list being
118 // appended; it is an invariant that the old tail&#39;s &quot;next&quot; value is NULL.
119 // But if the old tail is NULL then the queue was empty.  In this case the
120 // head of the list being appended is instead stored in the queue head; it is
121 // an invariant that the queue head is NULL in this case.
122 //
123 // This means there is a period between the exchange and the old tail update
124 // where the queue sequence is split into two parts, the list from the queue
125 // head to the old tail, and the list being appended.  If there are concurrent
126 // push/append operations, each may introduce another such segment.  But they
127 // all eventually get resolved by their respective updates of their old tail&#39;s
128 // &quot;next&quot; value.  This also means that pop operations must handle a buffer
129 // with a NULL &quot;next&quot; value specially.
130 //
131 // A push operation is just a degenerate append, where the buffer being pushed
132 // is both the head and the tail of the list being appended.
133 void G1DirtyCardQueueSet::Queue::append(BufferNode&amp; first, BufferNode&amp; last) {
134   assert(last.next() == NULL, &quot;precondition&quot;);
135   BufferNode* old_tail = Atomic::xchg(&amp;_tail, &amp;last);
136   if (old_tail == NULL) {       // Was empty.

137     Atomic::store(&amp;_head, &amp;first);
138   } else {
139     assert(old_tail-&gt;next() == NULL, &quot;invariant&quot;);
140     old_tail-&gt;set_next(&amp;first);
141   }
142 }
143 











144 BufferNode* G1DirtyCardQueueSet::Queue::pop() {
145   Thread* current_thread = Thread::current();
146   while (true) {
147     // Use a critical section per iteration, rather than over the whole
<span class="line-modified">148     // operation.  We&#39;re not guaranteed to make progress.  Lingering in one</span>
<span class="line-modified">149     // CS could lead to excessive allocation of buffers, because the CS</span>
<span class="line-modified">150     // blocks return of released buffers to the free list for reuse.</span>

151     GlobalCounter::CriticalSection cs(current_thread);
152 
153     BufferNode* result = Atomic::load_acquire(&amp;_head);
<span class="line-modified">154     if (result == NULL) return NULL; // Queue is empty.</span>



155 
156     BufferNode* next = Atomic::load_acquire(BufferNode::next_ptr(*result));
157     if (next != NULL) {
<span class="line-modified">158       // The &quot;usual&quot; lock-free pop from the head of a singly linked list.</span>
<span class="line-modified">159       if (result == Atomic::cmpxchg(&amp;_head, result, next)) {</span>
160         // Former head successfully taken; it is not the last.
161         assert(Atomic::load(&amp;_tail) != result, &quot;invariant&quot;);
162         assert(result-&gt;next() != NULL, &quot;invariant&quot;);
163         result-&gt;set_next(NULL);
164         return result;
165       }
<span class="line-modified">166       // Lost the race; try again.</span>
<span class="line-modified">167       continue;</span>
<span class="line-modified">168     }</span>
<span class="line-modified">169 </span>
<span class="line-modified">170     // next is NULL.  This case is handled differently from the &quot;usual&quot;</span>
<span class="line-modified">171     // lock-free pop from the head of a singly linked list.</span>
<span class="line-modified">172 </span>
<span class="line-modified">173     // If _tail == result then result is the only element in the list. We can</span>
<span class="line-added">174     // remove it from the list by first setting _tail to NULL and then setting</span>
<span class="line-added">175     // _head to NULL, the order being important.  We set _tail with cmpxchg in</span>
<span class="line-added">176     // case of a concurrent push/append/pop also changing _tail.  If we win</span>
<span class="line-added">177     // then we&#39;ve claimed result.</span>
<span class="line-added">178     if (Atomic::cmpxchg(&amp;_tail, result, (BufferNode*)NULL) == result) {</span>
<span class="line-added">179       assert(result-&gt;next() == NULL, &quot;invariant&quot;);</span>
<span class="line-added">180       // Now that we&#39;ve claimed result, also set _head to NULL.  But we must</span>
<span class="line-added">181       // be careful of a concurrent push/append after we NULLed _tail, since</span>
<span class="line-added">182       // it may have already performed its list-was-empty update of _head,</span>
<span class="line-added">183       // which we must not overwrite.</span>
<span class="line-added">184       Atomic::cmpxchg(&amp;_head, result, (BufferNode*)NULL);</span>
<span class="line-added">185       return result;</span>
<span class="line-added">186     }</span>
<span class="line-added">187 </span>
<span class="line-added">188     // If _head != result then we lost the race to take result; try again.</span>
<span class="line-added">189     if (result != Atomic::load_acquire(&amp;_head)) {</span>
<span class="line-added">190       continue;</span>
191     }
<span class="line-modified">192 </span>
<span class="line-added">193     // An in-progress concurrent operation interfered with taking the head</span>
<span class="line-added">194     // element when it was the only element.  A concurrent pop may have won</span>
<span class="line-added">195     // the race to clear the tail but not yet cleared the head. Alternatively,</span>
<span class="line-added">196     // a concurrent push/append may have changed the tail but not yet linked</span>
<span class="line-added">197     // result-&gt;next().  We cannot take result in either case.  We don&#39;t just</span>
<span class="line-added">198     // try again, because we could spin for a long time waiting for that</span>
<span class="line-added">199     // concurrent operation to finish.  In the first case, returning NULL is</span>
<span class="line-added">200     // fine; we lost the race for the only element to another thread.  We</span>
<span class="line-added">201     // also return NULL for the second case, and let the caller cope.</span>
<span class="line-added">202     return NULL;</span>
203   }
204 }
205 
206 G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::Queue::take_all() {
207   assert_at_safepoint();
208   HeadTail result(Atomic::load(&amp;_head), Atomic::load(&amp;_tail));
209   Atomic::store(&amp;_head, (BufferNode*)NULL);
210   Atomic::store(&amp;_tail, (BufferNode*)NULL);
211   return result;
212 }
213 
214 void G1DirtyCardQueueSet::enqueue_completed_buffer(BufferNode* cbn) {
215   assert(cbn != NULL, &quot;precondition&quot;);
216   // Increment _num_cards before adding to queue, so queue removal doesn&#39;t
217   // need to deal with _num_cards possibly going negative.
218   size_t new_num_cards = Atomic::add(&amp;_num_cards, buffer_size() - cbn-&gt;index());
219   _completed.push(*cbn);
220   if ((new_num_cards &gt; process_cards_threshold()) &amp;&amp;
221       (_primary_refinement_thread != NULL)) {
222     _primary_refinement_thread-&gt;activate();
223   }
224 }
225 
<span class="line-modified">226 BufferNode* G1DirtyCardQueueSet::get_completed_buffer() {</span>









227   BufferNode* result = _completed.pop();
<span class="line-modified">228   if (result == NULL) {         // Unlikely if no paused buffers.</span>
<span class="line-modified">229     enqueue_previous_paused_buffers();</span>
<span class="line-added">230     result = _completed.pop();</span>
<span class="line-added">231     if (result == NULL) return NULL;</span>
232   }
<span class="line-added">233   Atomic::sub(&amp;_num_cards, buffer_size() - result-&gt;index());</span>
234   return result;
235 }
236 
237 #ifdef ASSERT
238 void G1DirtyCardQueueSet::verify_num_cards() const {
239   size_t actual = 0;
240   BufferNode* cur = _completed.top();
241   for ( ; cur != NULL; cur = cur-&gt;next()) {
242     actual += buffer_size() - cur-&gt;index();
243   }
244   assert(actual == Atomic::load(&amp;_num_cards),
245          &quot;Num entries in completed buffers should be &quot; SIZE_FORMAT &quot; but are &quot; SIZE_FORMAT,
246          Atomic::load(&amp;_num_cards), actual);
247 }
248 #endif // ASSERT
249 
250 G1DirtyCardQueueSet::PausedBuffers::PausedList::PausedList() :
251   _head(NULL), _tail(NULL),
252   _safepoint_id(SafepointSynchronize::safepoint_id())
253 {}
</pre>
<hr />
<pre>
271   if (old_head == NULL) {
272     assert(_tail == NULL, &quot;invariant&quot;);
273     _tail = node;
274   } else {
275     node-&gt;set_next(old_head);
276   }
277 }
278 
279 G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::PausedBuffers::PausedList::take() {
280   BufferNode* head = Atomic::load(&amp;_head);
281   BufferNode* tail = _tail;
282   Atomic::store(&amp;_head, (BufferNode*)NULL);
283   _tail = NULL;
284   return HeadTail(head, tail);
285 }
286 
287 G1DirtyCardQueueSet::PausedBuffers::PausedBuffers() : _plist(NULL) {}
288 
289 #ifdef ASSERT
290 G1DirtyCardQueueSet::PausedBuffers::~PausedBuffers() {
<span class="line-modified">291   assert(Atomic::load(&amp;_plist) == NULL, &quot;invariant&quot;);</span>
292 }
293 #endif // ASSERT
294 




295 void G1DirtyCardQueueSet::PausedBuffers::add(BufferNode* node) {
296   assert_not_at_safepoint();
297   PausedList* plist = Atomic::load_acquire(&amp;_plist);
<span class="line-modified">298   if (plist == NULL) {</span>




299     // Try to install a new next list.
300     plist = new PausedList();
301     PausedList* old_plist = Atomic::cmpxchg(&amp;_plist, (PausedList*)NULL, plist);
302     if (old_plist != NULL) {
<span class="line-modified">303       // Some other thread installed a new next list.  Use it instead.</span>
304       delete plist;
305       plist = old_plist;
306     }
307   }
<span class="line-added">308   assert(plist-&gt;is_next(), &quot;invariant&quot;);</span>
309   plist-&gt;add(node);
310 }
311 
312 G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::PausedBuffers::take_previous() {
313   assert_not_at_safepoint();
314   PausedList* previous;
315   {
316     // Deal with plist in a critical section, to prevent it from being
317     // deleted out from under us by a concurrent take_previous().
318     GlobalCounter::CriticalSection cs(Thread::current());
319     previous = Atomic::load_acquire(&amp;_plist);
320     if ((previous == NULL) ||   // Nothing to take.
321         previous-&gt;is_next() ||  // Not from a previous safepoint.
322         // Some other thread stole it.
323         (Atomic::cmpxchg(&amp;_plist, previous, (PausedList*)NULL) != previous)) {
324       return HeadTail();
325     }
326   }
327   // We now own previous.
328   HeadTail result = previous-&gt;take();
</pre>
<hr />
<pre>
332   GlobalCounter::write_synchronize();
333   delete previous;
334   return result;
335 }
336 
337 G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::PausedBuffers::take_all() {
338   assert_at_safepoint();
339   HeadTail result;
340   PausedList* plist = Atomic::load(&amp;_plist);
341   if (plist != NULL) {
342     Atomic::store(&amp;_plist, (PausedList*)NULL);
343     result = plist-&gt;take();
344     delete plist;
345   }
346   return result;
347 }
348 
349 void G1DirtyCardQueueSet::record_paused_buffer(BufferNode* node) {
350   assert_not_at_safepoint();
351   assert(node-&gt;next() == NULL, &quot;precondition&quot;);
<span class="line-added">352   // Ensure there aren&#39;t any paused buffers from a previous safepoint.</span>
<span class="line-added">353   enqueue_previous_paused_buffers();</span>
354   // Cards for paused buffers are included in count, to contribute to
355   // notification checking after the coming safepoint if it doesn&#39;t GC.
356   // Note that this means the queue&#39;s _num_cards differs from the number
357   // of cards in the queued buffers when there are paused buffers.
358   Atomic::add(&amp;_num_cards, buffer_size() - node-&gt;index());
359   _paused.add(node);
360 }
361 
362 void G1DirtyCardQueueSet::enqueue_paused_buffers_aux(const HeadTail&amp; paused) {
363   if (paused._head != NULL) {
364     assert(paused._tail != NULL, &quot;invariant&quot;);
365     // Cards from paused buffers are already recorded in the queue count.
366     _completed.append(*paused._head, *paused._tail);
367   }
368 }
369 
370 void G1DirtyCardQueueSet::enqueue_previous_paused_buffers() {
371   assert_not_at_safepoint();
<span class="line-modified">372   enqueue_paused_buffers_aux(_paused.take_previous());</span>


















373 }
374 
375 void G1DirtyCardQueueSet::enqueue_all_paused_buffers() {
376   assert_at_safepoint();
377   enqueue_paused_buffers_aux(_paused.take_all());
378 }
379 
380 void G1DirtyCardQueueSet::abandon_completed_buffers() {
381   enqueue_all_paused_buffers();
382   verify_num_cards();
383   G1BufferNodeList list = take_all_completed_buffers();
384   BufferNode* buffers_to_delete = list._head;
385   while (buffers_to_delete != NULL) {
386     BufferNode* bn = buffers_to_delete;
387     buffers_to_delete = bn-&gt;next();
388     bn-&gt;set_next(NULL);
389     deallocate_buffer(bn);
390   }
391 }
392 
</pre>
<hr />
<pre>
523     // humongous object allocation (see comment at the StoreStore fence before
524     // setting the regions&#39; tops in humongous allocation path).
525     // It&#39;s okay that reading region&#39;s top and reading region&#39;s type were racy
526     // wrto each other. We need both set, in any order, to proceed.
527     OrderAccess::fence();
528     sort_cards(first_clean_index);
529     return refine_cleaned_cards(first_clean_index);
530   }
531 };
532 
533 bool G1DirtyCardQueueSet::refine_buffer(BufferNode* node,
534                                         uint worker_id,
535                                         size_t* total_refined_cards) {
536   G1RefineBufferedCards buffered_cards(node,
537                                        buffer_size(),
538                                        worker_id,
539                                        total_refined_cards);
540   return buffered_cards.refine();
541 }
542 
<span class="line-modified">543 void G1DirtyCardQueueSet::handle_refined_buffer(BufferNode* node,</span>
<span class="line-modified">544                                                 bool fully_processed) {</span>
<span class="line-modified">545   if (fully_processed) {</span>
<span class="line-modified">546     assert(node-&gt;index() == buffer_size(),</span>
<span class="line-modified">547            &quot;Buffer not fully consumed: index: &quot; SIZE_FORMAT &quot;, size: &quot; SIZE_FORMAT,</span>
<span class="line-modified">548            node-&gt;index(), buffer_size());</span>
<span class="line-modified">549     deallocate_buffer(node);</span>
<span class="line-modified">550   } else {</span>
<span class="line-modified">551     assert(node-&gt;index() &lt; buffer_size(), &quot;Buffer fully consumed.&quot;);</span>
<span class="line-modified">552     // Buffer incompletely processed because there is a pending safepoint.</span>
<span class="line-modified">553     // Record partially processed buffer, to be finished later.</span>
<span class="line-modified">554     record_paused_buffer(node);</span>




















555   }


556 }
557 
<span class="line-modified">558 void G1DirtyCardQueueSet::handle_completed_buffer(BufferNode* new_node) {</span>
<span class="line-added">559   enqueue_completed_buffer(new_node);</span>
<span class="line-added">560 </span>
<span class="line-added">561   // No need for mutator refinement if number of cards is below limit.</span>
<span class="line-added">562   if (Atomic::load(&amp;_num_cards) &lt;= Atomic::load(&amp;_padded_max_cards)) {</span>
<span class="line-added">563     return;</span>
<span class="line-added">564   }</span>
<span class="line-added">565 </span>
<span class="line-added">566   // Only Java threads perform mutator refinement.</span>
<span class="line-added">567   if (!Thread::current()-&gt;is_Java_thread()) {</span>
<span class="line-added">568     return;</span>
<span class="line-added">569   }</span>
<span class="line-added">570 </span>
<span class="line-added">571   BufferNode* node = get_completed_buffer();</span>
<span class="line-added">572   if (node == NULL) return;     // Didn&#39;t get a buffer to process.</span>
<span class="line-added">573 </span>
<span class="line-added">574   // Refine cards in buffer.</span>
<span class="line-added">575 </span>
576   uint worker_id = _free_ids.claim_par_id(); // temporarily claim an id
577   uint counter_index = worker_id - par_ids_start();
578   size_t* counter = &amp;_mutator_refined_cards_counters[counter_index];
<span class="line-modified">579   bool fully_processed = refine_buffer(node, worker_id, counter);</span>
580   _free_ids.release_par_id(worker_id); // release the id
581 
<span class="line-modified">582   // Deal with buffer after releasing id, to let another thread use id.</span>
<span class="line-modified">583   handle_refined_buffer(node, fully_processed);</span>


584 }
585 
586 bool G1DirtyCardQueueSet::refine_completed_buffer_concurrently(uint worker_id,
587                                                                size_t stop_at,
588                                                                size_t* total_refined_cards) {
<span class="line-modified">589   // Not enough cards to trigger processing.</span>
<span class="line-modified">590   if (Atomic::load(&amp;_num_cards) &lt;= stop_at) return false;</span>
<span class="line-modified">591 </span>
<span class="line-modified">592   BufferNode* node = get_completed_buffer();</span>
<span class="line-modified">593   if (node == NULL) return false; // Didn&#39;t get a buffer to process.</span>
<span class="line-modified">594 </span>
<span class="line-modified">595   bool fully_processed = refine_buffer(node, worker_id, total_refined_cards);</span>
<span class="line-modified">596   handle_refined_buffer(node, fully_processed);</span>
<span class="line-modified">597   return true;</span>





598 }
599 
600 void G1DirtyCardQueueSet::abandon_logs() {
601   assert_at_safepoint();
602   abandon_completed_buffers();
603 
604   // Since abandon is done only at safepoints, we can safely manipulate
605   // these queues.
606   struct AbandonThreadLogClosure : public ThreadClosure {
607     virtual void do_thread(Thread* t) {
608       G1ThreadLocalData::dirty_card_queue(t).reset();
609     }
610   } closure;
611   Threads::threads_do(&amp;closure);
612 
613   G1BarrierSet::shared_dirty_card_queue().reset();
614 }
615 
616 void G1DirtyCardQueueSet::concatenate_logs() {
617   // Iterate over all the threads, if we find a partial log add it to
</pre>
<hr />
<pre>
619   // of outstanding buffers.
620   assert_at_safepoint();
621   size_t old_limit = max_cards();
622   set_max_cards(MaxCardsUnlimited);
623 
624   struct ConcatenateThreadLogClosure : public ThreadClosure {
625     virtual void do_thread(Thread* t) {
626       G1DirtyCardQueue&amp; dcq = G1ThreadLocalData::dirty_card_queue(t);
627       if (!dcq.is_empty()) {
628         dcq.flush();
629       }
630     }
631   } closure;
632   Threads::threads_do(&amp;closure);
633 
634   G1BarrierSet::shared_dirty_card_queue().flush();
635   enqueue_all_paused_buffers();
636   verify_num_cards();
637   set_max_cards(old_limit);
638 }
<span class="line-added">639 </span>
<span class="line-added">640 size_t G1DirtyCardQueueSet::max_cards() const {</span>
<span class="line-added">641   return _max_cards;</span>
<span class="line-added">642 }</span>
<span class="line-added">643 </span>
<span class="line-added">644 void G1DirtyCardQueueSet::set_max_cards(size_t value) {</span>
<span class="line-added">645   _max_cards = value;</span>
<span class="line-added">646   Atomic::store(&amp;_padded_max_cards, value);</span>
<span class="line-added">647 }</span>
<span class="line-added">648 </span>
<span class="line-added">649 void G1DirtyCardQueueSet::set_max_cards_padding(size_t padding) {</span>
<span class="line-added">650   // Compute sum, clipping to max.</span>
<span class="line-added">651   size_t limit = _max_cards + padding;</span>
<span class="line-added">652   if (limit &lt; padding) {        // Check for overflow.</span>
<span class="line-added">653     limit = MaxCardsUnlimited;</span>
<span class="line-added">654   }</span>
<span class="line-added">655   Atomic::store(&amp;_padded_max_cards, limit);</span>
<span class="line-added">656 }</span>
<span class="line-added">657 </span>
<span class="line-added">658 void G1DirtyCardQueueSet::discard_max_cards_padding() {</span>
<span class="line-added">659   // Being racy here is okay, since all threads store the same value.</span>
<span class="line-added">660   if (_max_cards != Atomic::load(&amp;_padded_max_cards)) {</span>
<span class="line-added">661     Atomic::store(&amp;_padded_max_cards, _max_cards);</span>
<span class="line-added">662   }</span>
<span class="line-added">663 }</span>
</pre>
</td>
</tr>
</table>
<center><a href="g1ConcurrentRefine.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1DirtyCardQueue.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>