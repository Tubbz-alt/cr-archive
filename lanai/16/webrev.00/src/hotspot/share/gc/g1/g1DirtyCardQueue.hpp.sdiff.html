<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1DirtyCardQueue.hpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1DirtyCardQueue.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1HeapSizingPolicy.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1DirtyCardQueue.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 60   using PtrQueue::byte_width_of_index;
 61 
 62   static ByteSize byte_offset_of_buf() {
 63     return PtrQueue::byte_offset_of_buf&lt;G1DirtyCardQueue&gt;();
 64   }
 65   using PtrQueue::byte_width_of_buf;
 66 
 67 };
 68 
 69 class G1DirtyCardQueueSet: public PtrQueueSet {
 70   // Head and tail of a list of BufferNodes, linked through their next()
 71   // fields.  Similar to G1BufferNodeList, but without the _entry_count.
 72   struct HeadTail {
 73     BufferNode* _head;
 74     BufferNode* _tail;
 75     HeadTail() : _head(NULL), _tail(NULL) {}
 76     HeadTail(BufferNode* head, BufferNode* tail) : _head(head), _tail(tail) {}
 77   };
 78 
 79   // A lock-free FIFO of BufferNodes, linked through their next() fields.
<span class="line-modified"> 80   // This class has a restriction that pop() cannot return the last buffer</span>
<span class="line-modified"> 81   // in the queue, or what was the last buffer for a concurrent push/append</span>
<span class="line-removed"> 82   // operation.  It is expected that there will be a later push/append that</span>
<span class="line-removed"> 83   // will make that buffer available to a future pop(), or there will</span>
<span class="line-removed"> 84   // eventually be a complete transfer via take_all().</span>
 85   class Queue {
 86     BufferNode* volatile _head;
 87     DEFINE_PAD_MINUS_SIZE(1, DEFAULT_CACHE_LINE_SIZE, sizeof(BufferNode*));
 88     BufferNode* volatile _tail;
 89     DEFINE_PAD_MINUS_SIZE(2, DEFAULT_CACHE_LINE_SIZE, sizeof(BufferNode*));
 90 
 91     NONCOPYABLE(Queue);
 92 
 93   public:
 94     Queue() : _head(NULL), _tail(NULL) {}
 95     DEBUG_ONLY(~Queue();)
 96 
 97     // Return the first buffer in the queue.
 98     // Thread-safe, but the result may change immediately.
 99     BufferNode* top() const;
100 
101     // Thread-safe add the buffer to the end of the queue.
102     void push(BufferNode&amp; node) { append(node, node); }
103 
104     // Thread-safe add the buffers from first to last to the end of the queue.
105     void append(BufferNode&amp; first, BufferNode&amp; last);
106 
107     // Thread-safe attempt to remove and return the first buffer in the queue.
<span class="line-modified">108     // Returns NULL if the queue is empty, or if only one buffer is found.</span>
<span class="line-modified">109     // Uses GlobalCounter critical sections to address the ABA problem; this</span>
<span class="line-modified">110     // works with the buffer allocator&#39;s use of GlobalCounter synchronization.</span>

111     BufferNode* pop();
112 
113     // Take all the buffers from the queue, leaving the queue empty.
114     // Not thread-safe.
115     HeadTail take_all();
116   };
117 
118   // Concurrent refinement may stop processing in the middle of a buffer if
119   // there is a pending safepoint, to avoid long delays to safepoint.  A
120   // partially processed buffer needs to be recorded for processing by the
121   // safepoint if it&#39;s a GC safepoint; otherwise it needs to be recorded for
122   // further concurrent refinement work after the safepoint.  But if the
123   // buffer was obtained from the completed buffer queue then it can&#39;t simply
124   // be added back to the queue, as that would introduce a new source of ABA
125   // for the queue.
126   //
127   // The PausedBuffer object is used to record such buffers for the upcoming
128   // safepoint, and provides access to the buffers recorded for previous
129   // safepoints.  Before obtaining a buffer from the completed buffers queue,
130   // we first transfer any buffers from previous safepoints to the queue.
</pre>
<hr />
<pre>
163       HeadTail take();
164     };
165 
166     // The most recently created list, which might be for either the next or
167     // a previous safepoint, or might be NULL if the next list hasn&#39;t been
168     // created yet.  We only need one list because of the requirement that
169     // threads calling add() must first ensure there are no paused buffers
170     // from a previous safepoint.  There might be many list instances existing
171     // at the same time though; there can be many threads competing to create
172     // and install the next list, and meanwhile there can be a thread dealing
173     // with the previous list.
174     PausedList* volatile _plist;
175     DEFINE_PAD_MINUS_SIZE(1, DEFAULT_CACHE_LINE_SIZE, sizeof(PausedList*));
176 
177     NONCOPYABLE(PausedBuffers);
178 
179   public:
180     PausedBuffers();
181     DEBUG_ONLY(~PausedBuffers();)
182 
<span class="line-removed">183     // Test whether there are any paused lists.</span>
<span class="line-removed">184     // Thread-safe, but the answer may change immediately.</span>
<span class="line-removed">185     bool is_empty() const;</span>
<span class="line-removed">186 </span>
187     // Thread-safe add the buffer to paused list for next safepoint.
188     // precondition: not at safepoint.
189     // precondition: does not have paused buffers from a previous safepoint.
190     void add(BufferNode* node);
191 
192     // Thread-safe take all paused buffers for previous safepoints.
193     // precondition: not at safepoint.
194     HeadTail take_previous();
195 
196     // Take all the paused buffers.
197     // precondition: at safepoint.
198     HeadTail take_all();
199   };
200 
201   // The primary refinement thread, for activation when the processing
202   // threshold is reached.  NULL if there aren&#39;t any refinement threads.
203   G1ConcurrentRefineThread* _primary_refinement_thread;
204   DEFINE_PAD_MINUS_SIZE(1, DEFAULT_CACHE_LINE_SIZE, sizeof(G1ConcurrentRefineThread*));
205   // Upper bound on the number of cards in the completed and paused buffers.
206   volatile size_t _num_cards;
207   DEFINE_PAD_MINUS_SIZE(2, DEFAULT_CACHE_LINE_SIZE, sizeof(size_t));
208   // Buffers ready for refinement.
209   Queue _completed;           // Has inner padding, including trailer.
210   // Buffers for which refinement is temporarily paused.
211   PausedBuffers _paused;      // Has inner padding, including trailer.
212 
213   G1FreeIdSet _free_ids;
214 
215   // Activation threshold for the primary refinement thread.
216   size_t _process_cards_threshold;
217 
218   // If the queue contains more cards than configured here, the
219   // mutator must start doing some of the concurrent refinement work.
220   size_t _max_cards;
<span class="line-modified">221   size_t _max_cards_padding;</span>
222   static const size_t MaxCardsUnlimited = SIZE_MAX;
223 
224   // Array of cumulative dirty cards refined by mutator threads.
225   // Array has an entry per id in _free_ids.
226   size_t* _mutator_refined_cards_counters;
227 
228   // Verify _num_cards == sum of cards in the completed queue.
229   void verify_num_cards() const NOT_DEBUG_RETURN;
230 
231   // Thread-safe add a buffer to paused list for next safepoint.
232   // precondition: not at safepoint.
<span class="line-removed">233   // precondition: does not have paused buffers from a previous safepoint.</span>
234   void record_paused_buffer(BufferNode* node);
235   void enqueue_paused_buffers_aux(const HeadTail&amp; paused);
236   // Thread-safe transfer paused buffers for previous safepoints to the queue.
237   // precondition: not at safepoint.
238   void enqueue_previous_paused_buffers();
239   // Transfer all paused buffers to the queue.
240   // precondition: at safepoint.
241   void enqueue_all_paused_buffers();
242 
243   void abandon_completed_buffers();
244 
245   // Refine the cards in &quot;node&quot; from its index to buffer_size.
246   // Stops processing if SuspendibleThreadSet::should_yield() is true.
247   // Returns true if the entire buffer was processed, false if there
248   // is a pending yield request.  The node&#39;s index is updated to exclude
249   // the processed elements, e.g. up to the element before processing
250   // stopped, or one past the last element if the entire buffer was
251   // processed. Increments *total_refined_cards by the number of cards
252   // processed and removed from the buffer.
253   bool refine_buffer(BufferNode* node, uint worker_id, size_t* total_refined_cards);
254 
<span class="line-modified">255   bool mut_process_buffer(BufferNode* node);</span>


256 
<span class="line-modified">257   // If the number of completed buffers is &gt; stop_at, then remove and</span>
<span class="line-modified">258   // return a completed buffer from the list.  Otherwise, return NULL.</span>
<span class="line-modified">259   BufferNode* get_completed_buffer(size_t stop_at = 0);</span>
260 
261 public:
262   G1DirtyCardQueueSet(BufferNode::Allocator* allocator);
263   ~G1DirtyCardQueueSet();
264 
265   void set_primary_refinement_thread(G1ConcurrentRefineThread* thread) {
266     _primary_refinement_thread = thread;
267   }
268 
269   // The number of parallel ids that can be claimed to allow collector or
270   // mutator threads to do card-processing work.
271   static uint num_par_ids();
272 
273   static void handle_zero_index_for_thread(Thread* t);
274 
<span class="line-removed">275   // Either process the entire buffer and return true, or enqueue the</span>
<span class="line-removed">276   // buffer and return false.  If the buffer is completely processed,</span>
<span class="line-removed">277   // it can be reused in place.</span>
<span class="line-removed">278   bool process_or_enqueue_completed_buffer(BufferNode* node);</span>
<span class="line-removed">279 </span>
280   virtual void enqueue_completed_buffer(BufferNode* node);
281 
282   // Upper bound on the number of cards currently in in this queue set.
283   // Read without synchronization.  The value may be high because there
284   // is a concurrent modification of the set of buffers.
285   size_t num_cards() const { return _num_cards; }
286 
287   // Get/Set the number of cards that triggers log processing.
288   // Log processing should be done when the number of cards exceeds the
289   // threshold.
290   void set_process_cards_threshold(size_t sz) {
291     _process_cards_threshold = sz;
292   }
293   size_t process_cards_threshold() const {
294     return _process_cards_threshold;
295   }
296   static const size_t ProcessCardsThresholdNever = SIZE_MAX;
297 
298   // Notify the consumer if the number of buffers crossed the threshold
299   void notify_if_necessary();
300 
301   void merge_bufferlists(G1RedirtyCardsQueueSet* src);
302 
303   G1BufferNodeList take_all_completed_buffers();
304 











305   // If there are more than stop_at cards in the completed buffers, pop
306   // a buffer, refine its contents, and return true.  Otherwise return
307   // false.
308   //
309   // Stops processing a buffer if SuspendibleThreadSet::should_yield(),
310   // recording the incompletely processed buffer for later processing of
311   // the remainder.
312   //
313   // Increments *total_refined_cards by the number of cards processed and
314   // removed from the buffer.
315   bool refine_completed_buffer_concurrently(uint worker_id,
316                                             size_t stop_at,
317                                             size_t* total_refined_cards);
318 
319   // If a full collection is happening, reset partial logs, and release
320   // completed ones: the full collection will make them all irrelevant.
321   void abandon_logs();
322 
323   // If any threads have partial logs, add them to the global list of logs.
324   void concatenate_logs();
325 
<span class="line-modified">326   void set_max_cards(size_t m) {</span>
<span class="line-modified">327     _max_cards = m;</span>
<span class="line-modified">328   }</span>
<span class="line-removed">329   size_t max_cards() const {</span>
<span class="line-removed">330     return _max_cards;</span>
<span class="line-removed">331   }</span>
332 
<span class="line-modified">333   void set_max_cards_padding(size_t padding) {</span>
<span class="line-modified">334     _max_cards_padding = padding;</span>
<span class="line-modified">335   }</span>
<span class="line-modified">336   size_t max_cards_padding() const {</span>
<span class="line-modified">337     return _max_cards_padding;</span>
<span class="line-modified">338   }</span>


339 
340   // Total dirty cards refined by mutator threads.
341   size_t total_mutator_refined_cards() const;
342 };
343 
344 inline G1DirtyCardQueueSet* G1DirtyCardQueue::dirty_card_qset() const {
345   return static_cast&lt;G1DirtyCardQueueSet*&gt;(qset());
346 }
347 
348 #endif // SHARE_GC_G1_G1DIRTYCARDQUEUE_HPP
</pre>
</td>
<td>
<hr />
<pre>
 60   using PtrQueue::byte_width_of_index;
 61 
 62   static ByteSize byte_offset_of_buf() {
 63     return PtrQueue::byte_offset_of_buf&lt;G1DirtyCardQueue&gt;();
 64   }
 65   using PtrQueue::byte_width_of_buf;
 66 
 67 };
 68 
 69 class G1DirtyCardQueueSet: public PtrQueueSet {
 70   // Head and tail of a list of BufferNodes, linked through their next()
 71   // fields.  Similar to G1BufferNodeList, but without the _entry_count.
 72   struct HeadTail {
 73     BufferNode* _head;
 74     BufferNode* _tail;
 75     HeadTail() : _head(NULL), _tail(NULL) {}
 76     HeadTail(BufferNode* head, BufferNode* tail) : _head(head), _tail(tail) {}
 77   };
 78 
 79   // A lock-free FIFO of BufferNodes, linked through their next() fields.
<span class="line-modified"> 80   // This class has a restriction that pop() may return NULL when there are</span>
<span class="line-modified"> 81   // buffers in the queue if there is a concurrent push/append operation.</span>



 82   class Queue {
 83     BufferNode* volatile _head;
 84     DEFINE_PAD_MINUS_SIZE(1, DEFAULT_CACHE_LINE_SIZE, sizeof(BufferNode*));
 85     BufferNode* volatile _tail;
 86     DEFINE_PAD_MINUS_SIZE(2, DEFAULT_CACHE_LINE_SIZE, sizeof(BufferNode*));
 87 
 88     NONCOPYABLE(Queue);
 89 
 90   public:
 91     Queue() : _head(NULL), _tail(NULL) {}
 92     DEBUG_ONLY(~Queue();)
 93 
 94     // Return the first buffer in the queue.
 95     // Thread-safe, but the result may change immediately.
 96     BufferNode* top() const;
 97 
 98     // Thread-safe add the buffer to the end of the queue.
 99     void push(BufferNode&amp; node) { append(node, node); }
100 
101     // Thread-safe add the buffers from first to last to the end of the queue.
102     void append(BufferNode&amp; first, BufferNode&amp; last);
103 
104     // Thread-safe attempt to remove and return the first buffer in the queue.
<span class="line-modified">105     // Returns NULL if the queue is empty, or if a concurrent push/append</span>
<span class="line-modified">106     // interferes.  Uses GlobalCounter critical sections to address the ABA</span>
<span class="line-modified">107     // problem; this works with the buffer allocator&#39;s use of GlobalCounter</span>
<span class="line-added">108     // synchronization.</span>
109     BufferNode* pop();
110 
111     // Take all the buffers from the queue, leaving the queue empty.
112     // Not thread-safe.
113     HeadTail take_all();
114   };
115 
116   // Concurrent refinement may stop processing in the middle of a buffer if
117   // there is a pending safepoint, to avoid long delays to safepoint.  A
118   // partially processed buffer needs to be recorded for processing by the
119   // safepoint if it&#39;s a GC safepoint; otherwise it needs to be recorded for
120   // further concurrent refinement work after the safepoint.  But if the
121   // buffer was obtained from the completed buffer queue then it can&#39;t simply
122   // be added back to the queue, as that would introduce a new source of ABA
123   // for the queue.
124   //
125   // The PausedBuffer object is used to record such buffers for the upcoming
126   // safepoint, and provides access to the buffers recorded for previous
127   // safepoints.  Before obtaining a buffer from the completed buffers queue,
128   // we first transfer any buffers from previous safepoints to the queue.
</pre>
<hr />
<pre>
161       HeadTail take();
162     };
163 
164     // The most recently created list, which might be for either the next or
165     // a previous safepoint, or might be NULL if the next list hasn&#39;t been
166     // created yet.  We only need one list because of the requirement that
167     // threads calling add() must first ensure there are no paused buffers
168     // from a previous safepoint.  There might be many list instances existing
169     // at the same time though; there can be many threads competing to create
170     // and install the next list, and meanwhile there can be a thread dealing
171     // with the previous list.
172     PausedList* volatile _plist;
173     DEFINE_PAD_MINUS_SIZE(1, DEFAULT_CACHE_LINE_SIZE, sizeof(PausedList*));
174 
175     NONCOPYABLE(PausedBuffers);
176 
177   public:
178     PausedBuffers();
179     DEBUG_ONLY(~PausedBuffers();)
180 




181     // Thread-safe add the buffer to paused list for next safepoint.
182     // precondition: not at safepoint.
183     // precondition: does not have paused buffers from a previous safepoint.
184     void add(BufferNode* node);
185 
186     // Thread-safe take all paused buffers for previous safepoints.
187     // precondition: not at safepoint.
188     HeadTail take_previous();
189 
190     // Take all the paused buffers.
191     // precondition: at safepoint.
192     HeadTail take_all();
193   };
194 
195   // The primary refinement thread, for activation when the processing
196   // threshold is reached.  NULL if there aren&#39;t any refinement threads.
197   G1ConcurrentRefineThread* _primary_refinement_thread;
198   DEFINE_PAD_MINUS_SIZE(1, DEFAULT_CACHE_LINE_SIZE, sizeof(G1ConcurrentRefineThread*));
199   // Upper bound on the number of cards in the completed and paused buffers.
200   volatile size_t _num_cards;
201   DEFINE_PAD_MINUS_SIZE(2, DEFAULT_CACHE_LINE_SIZE, sizeof(size_t));
202   // Buffers ready for refinement.
203   Queue _completed;           // Has inner padding, including trailer.
204   // Buffers for which refinement is temporarily paused.
205   PausedBuffers _paused;      // Has inner padding, including trailer.
206 
207   G1FreeIdSet _free_ids;
208 
209   // Activation threshold for the primary refinement thread.
210   size_t _process_cards_threshold;
211 
212   // If the queue contains more cards than configured here, the
213   // mutator must start doing some of the concurrent refinement work.
214   size_t _max_cards;
<span class="line-modified">215   volatile size_t _padded_max_cards;</span>
216   static const size_t MaxCardsUnlimited = SIZE_MAX;
217 
218   // Array of cumulative dirty cards refined by mutator threads.
219   // Array has an entry per id in _free_ids.
220   size_t* _mutator_refined_cards_counters;
221 
222   // Verify _num_cards == sum of cards in the completed queue.
223   void verify_num_cards() const NOT_DEBUG_RETURN;
224 
225   // Thread-safe add a buffer to paused list for next safepoint.
226   // precondition: not at safepoint.

227   void record_paused_buffer(BufferNode* node);
228   void enqueue_paused_buffers_aux(const HeadTail&amp; paused);
229   // Thread-safe transfer paused buffers for previous safepoints to the queue.
230   // precondition: not at safepoint.
231   void enqueue_previous_paused_buffers();
232   // Transfer all paused buffers to the queue.
233   // precondition: at safepoint.
234   void enqueue_all_paused_buffers();
235 
236   void abandon_completed_buffers();
237 
238   // Refine the cards in &quot;node&quot; from its index to buffer_size.
239   // Stops processing if SuspendibleThreadSet::should_yield() is true.
240   // Returns true if the entire buffer was processed, false if there
241   // is a pending yield request.  The node&#39;s index is updated to exclude
242   // the processed elements, e.g. up to the element before processing
243   // stopped, or one past the last element if the entire buffer was
244   // processed. Increments *total_refined_cards by the number of cards
245   // processed and removed from the buffer.
246   bool refine_buffer(BufferNode* node, uint worker_id, size_t* total_refined_cards);
247 
<span class="line-modified">248   // Deal with buffer after a call to refine_buffer.  If fully processed,</span>
<span class="line-added">249   // deallocate the buffer.  Otherwise, record it as paused.</span>
<span class="line-added">250   void handle_refined_buffer(BufferNode* node, bool fully_processed);</span>
251 
<span class="line-modified">252   // Remove and return a completed buffer from the list, or return NULL</span>
<span class="line-modified">253   // if none available.</span>
<span class="line-modified">254   BufferNode* get_completed_buffer();</span>
255 
256 public:
257   G1DirtyCardQueueSet(BufferNode::Allocator* allocator);
258   ~G1DirtyCardQueueSet();
259 
260   void set_primary_refinement_thread(G1ConcurrentRefineThread* thread) {
261     _primary_refinement_thread = thread;
262   }
263 
264   // The number of parallel ids that can be claimed to allow collector or
265   // mutator threads to do card-processing work.
266   static uint num_par_ids();
267 
268   static void handle_zero_index_for_thread(Thread* t);
269 





270   virtual void enqueue_completed_buffer(BufferNode* node);
271 
272   // Upper bound on the number of cards currently in in this queue set.
273   // Read without synchronization.  The value may be high because there
274   // is a concurrent modification of the set of buffers.
275   size_t num_cards() const { return _num_cards; }
276 
277   // Get/Set the number of cards that triggers log processing.
278   // Log processing should be done when the number of cards exceeds the
279   // threshold.
280   void set_process_cards_threshold(size_t sz) {
281     _process_cards_threshold = sz;
282   }
283   size_t process_cards_threshold() const {
284     return _process_cards_threshold;
285   }
286   static const size_t ProcessCardsThresholdNever = SIZE_MAX;
287 
288   // Notify the consumer if the number of buffers crossed the threshold
289   void notify_if_necessary();
290 
291   void merge_bufferlists(G1RedirtyCardsQueueSet* src);
292 
293   G1BufferNodeList take_all_completed_buffers();
294 
<span class="line-added">295   // Helper for G1DirtyCardQueue::handle_completed_buffer().</span>
<span class="line-added">296   // Enqueue the buffer, and optionally perform refinement by the mutator.</span>
<span class="line-added">297   // Mutator refinement is only done by Java threads, and only if there</span>
<span class="line-added">298   // are more than max_cards (possibly padded) cards in the completed</span>
<span class="line-added">299   // buffers.</span>
<span class="line-added">300   //</span>
<span class="line-added">301   // Mutator refinement, if performed, stops processing a buffer if</span>
<span class="line-added">302   // SuspendibleThreadSet::should_yield(), recording the incompletely</span>
<span class="line-added">303   // processed buffer for later processing of the remainder.</span>
<span class="line-added">304   void handle_completed_buffer(BufferNode* node);</span>
<span class="line-added">305 </span>
306   // If there are more than stop_at cards in the completed buffers, pop
307   // a buffer, refine its contents, and return true.  Otherwise return
308   // false.
309   //
310   // Stops processing a buffer if SuspendibleThreadSet::should_yield(),
311   // recording the incompletely processed buffer for later processing of
312   // the remainder.
313   //
314   // Increments *total_refined_cards by the number of cards processed and
315   // removed from the buffer.
316   bool refine_completed_buffer_concurrently(uint worker_id,
317                                             size_t stop_at,
318                                             size_t* total_refined_cards);
319 
320   // If a full collection is happening, reset partial logs, and release
321   // completed ones: the full collection will make them all irrelevant.
322   void abandon_logs();
323 
324   // If any threads have partial logs, add them to the global list of logs.
325   void concatenate_logs();
326 
<span class="line-modified">327   // Threshold for mutator threads to also do refinement when there</span>
<span class="line-modified">328   // are concurrent refinement threads.</span>
<span class="line-modified">329   size_t max_cards() const;</span>



330 
<span class="line-modified">331   // Set threshold for mutator threads to also do refinement.</span>
<span class="line-modified">332   void set_max_cards(size_t value);</span>
<span class="line-modified">333 </span>
<span class="line-modified">334   // Artificially increase mutator refinement threshold.</span>
<span class="line-modified">335   void set_max_cards_padding(size_t padding);</span>
<span class="line-modified">336 </span>
<span class="line-added">337   // Discard artificial increase of mutator refinement threshold.</span>
<span class="line-added">338   void discard_max_cards_padding();</span>
339 
340   // Total dirty cards refined by mutator threads.
341   size_t total_mutator_refined_cards() const;
342 };
343 
344 inline G1DirtyCardQueueSet* G1DirtyCardQueue::dirty_card_qset() const {
345   return static_cast&lt;G1DirtyCardQueueSet*&gt;(qset());
346 }
347 
348 #endif // SHARE_GC_G1_G1DIRTYCARDQUEUE_HPP
</pre>
</td>
</tr>
</table>
<center><a href="g1DirtyCardQueue.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1HeapSizingPolicy.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>