<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/g1/g1CollectedHeap.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="g1BiasedArray.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/g1/g1CollectedHeap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  49 #include &quot;gc/g1/g1MemoryPool.hpp&quot;
  50 #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
  51 #include &quot;gc/g1/g1ParallelCleaning.hpp&quot;
  52 #include &quot;gc/g1/g1ParScanThreadState.inline.hpp&quot;
  53 #include &quot;gc/g1/g1Policy.hpp&quot;
  54 #include &quot;gc/g1/g1RedirtyCardsQueue.hpp&quot;
  55 #include &quot;gc/g1/g1RegionToSpaceMapper.hpp&quot;
  56 #include &quot;gc/g1/g1RemSet.hpp&quot;
  57 #include &quot;gc/g1/g1RootClosures.hpp&quot;
  58 #include &quot;gc/g1/g1RootProcessor.hpp&quot;
  59 #include &quot;gc/g1/g1SATBMarkQueueSet.hpp&quot;
  60 #include &quot;gc/g1/g1StringDedup.hpp&quot;
  61 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
  62 #include &quot;gc/g1/g1Trace.hpp&quot;
  63 #include &quot;gc/g1/g1YCTypes.hpp&quot;
  64 #include &quot;gc/g1/g1YoungRemSetSamplingThread.hpp&quot;
  65 #include &quot;gc/g1/g1VMOperations.hpp&quot;
  66 #include &quot;gc/g1/heapRegion.inline.hpp&quot;
  67 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
  68 #include &quot;gc/g1/heapRegionSet.inline.hpp&quot;

  69 #include &quot;gc/shared/gcBehaviours.hpp&quot;
  70 #include &quot;gc/shared/gcHeapSummary.hpp&quot;
  71 #include &quot;gc/shared/gcId.hpp&quot;
  72 #include &quot;gc/shared/gcLocker.hpp&quot;
  73 #include &quot;gc/shared/gcTimer.hpp&quot;
  74 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  75 #include &quot;gc/shared/generationSpec.hpp&quot;
  76 #include &quot;gc/shared/isGCActiveMark.hpp&quot;
  77 #include &quot;gc/shared/locationPrinter.inline.hpp&quot;
  78 #include &quot;gc/shared/oopStorageParState.hpp&quot;
  79 #include &quot;gc/shared/preservedMarks.inline.hpp&quot;
  80 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
  81 #include &quot;gc/shared/referenceProcessor.inline.hpp&quot;
  82 #include &quot;gc/shared/taskTerminator.hpp&quot;
  83 #include &quot;gc/shared/taskqueue.inline.hpp&quot;
  84 #include &quot;gc/shared/weakProcessor.inline.hpp&quot;
  85 #include &quot;gc/shared/workerPolicy.hpp&quot;
  86 #include &quot;logging/log.hpp&quot;
  87 #include &quot;memory/allocation.hpp&quot;
  88 #include &quot;memory/iterator.hpp&quot;
</pre>
<hr />
<pre>
1782     HeapWord* end = _hrm-&gt;reserved().end();
1783     size_t granularity = HeapRegion::GrainBytes;
1784 
1785     _region_attr.initialize(start, end, granularity);
1786     _humongous_reclaim_candidates.initialize(start, end, granularity);
1787   }
1788 
1789   _workers = new WorkGang(&quot;GC Thread&quot;, ParallelGCThreads,
1790                           true /* are_GC_task_threads */,
1791                           false /* are_ConcurrentGC_threads */);
1792   if (_workers == NULL) {
1793     return JNI_ENOMEM;
1794   }
1795   _workers-&gt;initialize_workers();
1796 
1797   _numa-&gt;set_region_info(HeapRegion::GrainBytes, page_size);
1798 
1799   // Create the G1ConcurrentMark data structure and thread.
1800   // (Must do this late, so that &quot;max_regions&quot; is defined.)
1801   _cm = new G1ConcurrentMark(this, prev_bitmap_storage, next_bitmap_storage);
<span class="line-modified">1802   if (_cm == NULL || !_cm-&gt;completed_initialization()) {</span>
<span class="line-modified">1803     vm_shutdown_during_initialization(&quot;Could not create/initialize G1ConcurrentMark&quot;);</span>
1804     return JNI_ENOMEM;
1805   }
1806   _cm_thread = _cm-&gt;cm_thread();
1807 
1808   // Now expand into the initial heap size.
1809   if (!expand(init_byte_size, _workers)) {
1810     vm_shutdown_during_initialization(&quot;Failed to allocate initial heap.&quot;);
1811     return JNI_ENOMEM;
1812   }
1813 
1814   // Perform any initialization actions delegated to the policy.
1815   policy()-&gt;init(this, &amp;_collection_set);
1816 
1817   jint ecode = initialize_concurrent_refinement();
1818   if (ecode != JNI_OK) {
1819     return ecode;
1820   }
1821 
1822   ecode = initialize_young_gen_sampling_thread();
1823   if (ecode != JNI_OK) {
</pre>
<hr />
<pre>
1986 
1987 size_t G1CollectedHeap::recalculate_used() const {
1988   SumUsedClosure blk;
1989   heap_region_iterate(&amp;blk);
1990   return blk.result();
1991 }
1992 
1993 bool  G1CollectedHeap::is_user_requested_concurrent_full_gc(GCCause::Cause cause) {
1994   switch (cause) {
1995     case GCCause::_java_lang_system_gc:                 return ExplicitGCInvokesConcurrent;
1996     case GCCause::_dcmd_gc_run:                         return ExplicitGCInvokesConcurrent;
1997     case GCCause::_wb_conc_mark:                        return true;
1998     default :                                           return false;
1999   }
2000 }
2001 
2002 bool G1CollectedHeap::should_do_concurrent_full_gc(GCCause::Cause cause) {
2003   switch (cause) {
2004     case GCCause::_g1_humongous_allocation: return true;
2005     case GCCause::_g1_periodic_collection:  return G1PeriodicGCInvokesConcurrent;

2006     default:                                return is_user_requested_concurrent_full_gc(cause);
2007   }
2008 }
2009 
2010 bool G1CollectedHeap::should_upgrade_to_full_gc(GCCause::Cause cause) {
2011   if (policy()-&gt;force_upgrade_to_full()) {
2012     return true;
2013   } else if (should_do_concurrent_full_gc(_gc_cause)) {
2014     return false;
2015   } else if (has_regions_left_for_allocation()) {
2016     return false;
2017   } else {
2018     return true;
2019   }
2020 }
2021 
2022 #ifndef PRODUCT
2023 void G1CollectedHeap::allocate_dummy_regions() {
2024   // Let&#39;s fill up most of the region
2025   size_t word_size = HeapRegion::GrainWords - 1024;
</pre>
<hr />
<pre>
2156       return false;
2157     }
2158 
2159     // Lock to get consistent set of values.
2160     uint old_marking_started_after;
2161     uint old_marking_completed_after;
2162     {
2163       MutexLocker ml(Heap_lock);
2164       // Update gc_counter for retrying VMOp if needed. Captured here to be
2165       // consistent with the values we use below for termination tests.  If
2166       // a retry is needed after a possible wait, and another collection
2167       // occurs in the meantime, it will cause our retry to be skipped and
2168       // we&#39;ll recheck for termination with updated conditions from that
2169       // more recent collection.  That&#39;s what we want, rather than having
2170       // our retry possibly perform an unnecessary collection.
2171       gc_counter = total_collections();
2172       old_marking_started_after = _old_marking_cycles_started;
2173       old_marking_completed_after = _old_marking_cycles_completed;
2174     }
2175 
<span class="line-modified">2176     if (!GCCause::is_user_requested_gc(cause)) {</span>
















2177       // For an &quot;automatic&quot; (not user-requested) collection, we just need to
2178       // ensure that progress is made.
2179       //
2180       // Request is finished if any of
2181       // (1) the VMOp successfully performed a GC,
2182       // (2) a concurrent cycle was already in progress,
<span class="line-modified">2183       // (3) a new cycle was started (by this thread or some other), or</span>
<span class="line-modified">2184       // (4) a Full GC was performed.</span>
<span class="line-modified">2185       // Cases (3) and (4) are detected together by a change to</span>

2186       // _old_marking_cycles_started.
2187       //
<span class="line-modified">2188       // Note that (1) does not imply (3).  If we&#39;re still in the mixed</span>
2189       // phase of an earlier concurrent collection, the request to make the
2190       // collection an initial-mark won&#39;t be honored.  If we don&#39;t check for
2191       // both conditions we&#39;ll spin doing back-to-back collections.
2192       if (op.gc_succeeded() ||
2193           op.cycle_already_in_progress() ||

2194           (old_marking_started_before != old_marking_started_after)) {
2195         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);
2196         return true;
2197       }
2198     } else {                    // User-requested GC.
2199       // For a user-requested collection, we want to ensure that a complete
2200       // full collection has been performed before returning, but without
2201       // waiting for more than needed.
2202 
2203       // For user-requested GCs (unlike non-UR), a successful VMOp implies a
2204       // new cycle was started.  That&#39;s good, because it&#39;s not clear what we
2205       // should do otherwise.  Trying again just does back to back GCs.
2206       // Can&#39;t wait for someone else to start a cycle.  And returning fails
2207       // to meet the goal of ensuring a full collection was performed.
2208       assert(!op.gc_succeeded() ||
2209              (old_marking_started_before != old_marking_started_after),
2210              &quot;invariant: succeeded %s, started before %u, started after %u&quot;,
2211              BOOL_TO_STR(op.gc_succeeded()),
2212              old_marking_started_before, old_marking_started_after);
2213 
</pre>
<hr />
<pre>
2227         LOG_COLLECT_CONCURRENTLY(cause, &quot;wait&quot;);
2228         MonitorLocker ml(G1OldGCCount_lock);
2229         while (gc_counter_less_than(_old_marking_cycles_completed,
2230                                     old_marking_started_after)) {
2231           ml.wait();
2232         }
2233         // Request is finished if the collection we just waited for was
2234         // started after this request.
2235         if (old_marking_started_before != old_marking_started_after) {
2236           LOG_COLLECT_CONCURRENTLY(cause, &quot;complete after wait&quot;);
2237           return true;
2238         }
2239       }
2240 
2241       // If VMOp was successful then it started a new cycle that the above
2242       // wait &amp;etc should have recognized as finishing this request.  This
2243       // differs from a non-user-request, where gc_succeeded does not imply
2244       // a new cycle was started.
2245       assert(!op.gc_succeeded(), &quot;invariant&quot;);
2246 
<span class="line-removed">2247       // If VMOp failed because a cycle was already in progress, it is now</span>
<span class="line-removed">2248       // complete.  But it didn&#39;t finish this user-requested GC, so try</span>
<span class="line-removed">2249       // again.</span>
2250       if (op.cycle_already_in_progress()) {



2251         LOG_COLLECT_CONCURRENTLY(cause, &quot;retry after in-progress&quot;);
2252         continue;












2253       }
2254     }
2255 
2256     // Collection failed and should be retried.
2257     assert(op.transient_failure(), &quot;invariant&quot;);
2258 
<span class="line-removed">2259     // If GCLocker is active, wait until clear before retrying.</span>
2260     if (GCLocker::is_active_and_needs_gc()) {

2261       LOG_COLLECT_CONCURRENTLY(cause, &quot;gc-locker stall&quot;);
2262       GCLocker::stall_until_clear();
2263     }
2264 
2265     LOG_COLLECT_CONCURRENTLY(cause, &quot;retry&quot;);
2266   }
2267 }
2268 
2269 bool G1CollectedHeap::try_collect(GCCause::Cause cause) {
2270   assert_heap_not_locked();
2271 
2272   // Lock to get consistent set of values.
2273   uint gc_count_before;
2274   uint full_gc_count_before;
2275   uint old_marking_started_before;
2276   {
2277     MutexLocker ml(Heap_lock);
2278     gc_count_before = total_collections();
2279     full_gc_count_before = total_full_collections();
2280     old_marking_started_before = _old_marking_cycles_started;
</pre>
<hr />
<pre>
2436   }
2437   return ret_val;
2438 }
2439 
2440 void G1CollectedHeap::deduplicate_string(oop str) {
2441   assert(java_lang_String::is_instance(str), &quot;invariant&quot;);
2442 
2443   if (G1StringDedup::is_enabled()) {
2444     G1StringDedup::deduplicate(str);
2445   }
2446 }
2447 
2448 void G1CollectedHeap::prepare_for_verify() {
2449   _verifier-&gt;prepare_for_verify();
2450 }
2451 
2452 void G1CollectedHeap::verify(VerifyOption vo) {
2453   _verifier-&gt;verify(vo);
2454 }
2455 
<span class="line-modified">2456 bool G1CollectedHeap::supports_concurrent_phase_control() const {</span>
2457   return true;
2458 }
2459 
<span class="line-removed">2460 bool G1CollectedHeap::request_concurrent_phase(const char* phase) {</span>
<span class="line-removed">2461   return _cm_thread-&gt;request_concurrent_phase(phase);</span>
<span class="line-removed">2462 }</span>
<span class="line-removed">2463 </span>
2464 bool G1CollectedHeap::is_heterogeneous_heap() const {
2465   return G1Arguments::is_heterogeneous_heap();
2466 }
2467 
2468 class PrintRegionClosure: public HeapRegionClosure {
2469   outputStream* _st;
2470 public:
2471   PrintRegionClosure(outputStream* st) : _st(st) {}
2472   bool do_heap_region(HeapRegion* r) {
2473     r-&gt;print_on(_st);
2474     return false;
2475   }
2476 };
2477 
2478 bool G1CollectedHeap::is_obj_dead_cond(const oop obj,
2479                                        const HeapRegion* hr,
2480                                        const VerifyOption vo) const {
2481   switch (vo) {
2482   case VerifyOption_G1UsePrevMarking: return is_obj_dead(obj, hr);
2483   case VerifyOption_G1UseNextMarking: return is_obj_ill(obj, hr);
</pre>
<hr />
<pre>
3161     // TraceMemoryManagerStats is called) so that the G1 memory pools are updated
3162     // before any GC notifications are raised.
3163     g1mm()-&gt;update_sizes();
3164 
3165     _gc_tracer_stw-&gt;report_evacuation_info(&amp;evacuation_info);
3166     _gc_tracer_stw-&gt;report_tenuring_threshold(_policy-&gt;tenuring_threshold());
3167     _gc_timer_stw-&gt;register_gc_end();
3168     _gc_tracer_stw-&gt;report_gc_end(_gc_timer_stw-&gt;gc_end(), _gc_timer_stw-&gt;time_partitions());
3169   }
3170   // It should now be safe to tell the concurrent mark thread to start
3171   // without its logging output interfering with the logging output
3172   // that came from the pause.
3173 
3174   if (should_start_conc_mark) {
3175     // CAUTION: after the doConcurrentMark() call below, the concurrent marking
3176     // thread(s) could be running concurrently with us. Make sure that anything
3177     // after this point does not assume that we are the only GC thread running.
3178     // Note: of course, the actual marking work will not start until the safepoint
3179     // itself is released in SuspendibleThreadSet::desynchronize().
3180     do_concurrent_mark();

3181   }
3182 }
3183 
3184 void G1CollectedHeap::remove_self_forwarding_pointers(G1RedirtyCardsQueueSet* rdcqs) {
3185   G1ParRemoveSelfForwardPtrsTask rsfp_task(rdcqs);
3186   workers()-&gt;run_task(&amp;rsfp_task);
3187 }
3188 
3189 void G1CollectedHeap::restore_after_evac_failure(G1RedirtyCardsQueueSet* rdcqs) {
3190   double remove_self_forwards_start = os::elapsedTime();
3191 
3192   remove_self_forwarding_pointers(rdcqs);
3193   _preserved_marks_set.restore(workers());
3194 
3195   phase_times()-&gt;record_evac_fail_remove_self_forwards((os::elapsedTime() - remove_self_forwards_start) * 1000.0);
3196 }
3197 
3198 void G1CollectedHeap::preserve_mark_during_evac_failure(uint worker_id, oop obj, markWord m) {
3199   if (!_evacuation_failed) {
3200     _evacuation_failed = true;
</pre>
</td>
<td>
<hr />
<pre>
  49 #include &quot;gc/g1/g1MemoryPool.hpp&quot;
  50 #include &quot;gc/g1/g1OopClosures.inline.hpp&quot;
  51 #include &quot;gc/g1/g1ParallelCleaning.hpp&quot;
  52 #include &quot;gc/g1/g1ParScanThreadState.inline.hpp&quot;
  53 #include &quot;gc/g1/g1Policy.hpp&quot;
  54 #include &quot;gc/g1/g1RedirtyCardsQueue.hpp&quot;
  55 #include &quot;gc/g1/g1RegionToSpaceMapper.hpp&quot;
  56 #include &quot;gc/g1/g1RemSet.hpp&quot;
  57 #include &quot;gc/g1/g1RootClosures.hpp&quot;
  58 #include &quot;gc/g1/g1RootProcessor.hpp&quot;
  59 #include &quot;gc/g1/g1SATBMarkQueueSet.hpp&quot;
  60 #include &quot;gc/g1/g1StringDedup.hpp&quot;
  61 #include &quot;gc/g1/g1ThreadLocalData.hpp&quot;
  62 #include &quot;gc/g1/g1Trace.hpp&quot;
  63 #include &quot;gc/g1/g1YCTypes.hpp&quot;
  64 #include &quot;gc/g1/g1YoungRemSetSamplingThread.hpp&quot;
  65 #include &quot;gc/g1/g1VMOperations.hpp&quot;
  66 #include &quot;gc/g1/heapRegion.inline.hpp&quot;
  67 #include &quot;gc/g1/heapRegionRemSet.hpp&quot;
  68 #include &quot;gc/g1/heapRegionSet.inline.hpp&quot;
<span class="line-added">  69 #include &quot;gc/shared/concurrentGCBreakpoints.hpp&quot;</span>
  70 #include &quot;gc/shared/gcBehaviours.hpp&quot;
  71 #include &quot;gc/shared/gcHeapSummary.hpp&quot;
  72 #include &quot;gc/shared/gcId.hpp&quot;
  73 #include &quot;gc/shared/gcLocker.hpp&quot;
  74 #include &quot;gc/shared/gcTimer.hpp&quot;
  75 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
  76 #include &quot;gc/shared/generationSpec.hpp&quot;
  77 #include &quot;gc/shared/isGCActiveMark.hpp&quot;
  78 #include &quot;gc/shared/locationPrinter.inline.hpp&quot;
  79 #include &quot;gc/shared/oopStorageParState.hpp&quot;
  80 #include &quot;gc/shared/preservedMarks.inline.hpp&quot;
  81 #include &quot;gc/shared/suspendibleThreadSet.hpp&quot;
  82 #include &quot;gc/shared/referenceProcessor.inline.hpp&quot;
  83 #include &quot;gc/shared/taskTerminator.hpp&quot;
  84 #include &quot;gc/shared/taskqueue.inline.hpp&quot;
  85 #include &quot;gc/shared/weakProcessor.inline.hpp&quot;
  86 #include &quot;gc/shared/workerPolicy.hpp&quot;
  87 #include &quot;logging/log.hpp&quot;
  88 #include &quot;memory/allocation.hpp&quot;
  89 #include &quot;memory/iterator.hpp&quot;
</pre>
<hr />
<pre>
1783     HeapWord* end = _hrm-&gt;reserved().end();
1784     size_t granularity = HeapRegion::GrainBytes;
1785 
1786     _region_attr.initialize(start, end, granularity);
1787     _humongous_reclaim_candidates.initialize(start, end, granularity);
1788   }
1789 
1790   _workers = new WorkGang(&quot;GC Thread&quot;, ParallelGCThreads,
1791                           true /* are_GC_task_threads */,
1792                           false /* are_ConcurrentGC_threads */);
1793   if (_workers == NULL) {
1794     return JNI_ENOMEM;
1795   }
1796   _workers-&gt;initialize_workers();
1797 
1798   _numa-&gt;set_region_info(HeapRegion::GrainBytes, page_size);
1799 
1800   // Create the G1ConcurrentMark data structure and thread.
1801   // (Must do this late, so that &quot;max_regions&quot; is defined.)
1802   _cm = new G1ConcurrentMark(this, prev_bitmap_storage, next_bitmap_storage);
<span class="line-modified">1803   if (!_cm-&gt;completed_initialization()) {</span>
<span class="line-modified">1804     vm_shutdown_during_initialization(&quot;Could not initialize G1ConcurrentMark&quot;);</span>
1805     return JNI_ENOMEM;
1806   }
1807   _cm_thread = _cm-&gt;cm_thread();
1808 
1809   // Now expand into the initial heap size.
1810   if (!expand(init_byte_size, _workers)) {
1811     vm_shutdown_during_initialization(&quot;Failed to allocate initial heap.&quot;);
1812     return JNI_ENOMEM;
1813   }
1814 
1815   // Perform any initialization actions delegated to the policy.
1816   policy()-&gt;init(this, &amp;_collection_set);
1817 
1818   jint ecode = initialize_concurrent_refinement();
1819   if (ecode != JNI_OK) {
1820     return ecode;
1821   }
1822 
1823   ecode = initialize_young_gen_sampling_thread();
1824   if (ecode != JNI_OK) {
</pre>
<hr />
<pre>
1987 
1988 size_t G1CollectedHeap::recalculate_used() const {
1989   SumUsedClosure blk;
1990   heap_region_iterate(&amp;blk);
1991   return blk.result();
1992 }
1993 
1994 bool  G1CollectedHeap::is_user_requested_concurrent_full_gc(GCCause::Cause cause) {
1995   switch (cause) {
1996     case GCCause::_java_lang_system_gc:                 return ExplicitGCInvokesConcurrent;
1997     case GCCause::_dcmd_gc_run:                         return ExplicitGCInvokesConcurrent;
1998     case GCCause::_wb_conc_mark:                        return true;
1999     default :                                           return false;
2000   }
2001 }
2002 
2003 bool G1CollectedHeap::should_do_concurrent_full_gc(GCCause::Cause cause) {
2004   switch (cause) {
2005     case GCCause::_g1_humongous_allocation: return true;
2006     case GCCause::_g1_periodic_collection:  return G1PeriodicGCInvokesConcurrent;
<span class="line-added">2007     case GCCause::_wb_breakpoint:           return true;</span>
2008     default:                                return is_user_requested_concurrent_full_gc(cause);
2009   }
2010 }
2011 
2012 bool G1CollectedHeap::should_upgrade_to_full_gc(GCCause::Cause cause) {
2013   if (policy()-&gt;force_upgrade_to_full()) {
2014     return true;
2015   } else if (should_do_concurrent_full_gc(_gc_cause)) {
2016     return false;
2017   } else if (has_regions_left_for_allocation()) {
2018     return false;
2019   } else {
2020     return true;
2021   }
2022 }
2023 
2024 #ifndef PRODUCT
2025 void G1CollectedHeap::allocate_dummy_regions() {
2026   // Let&#39;s fill up most of the region
2027   size_t word_size = HeapRegion::GrainWords - 1024;
</pre>
<hr />
<pre>
2158       return false;
2159     }
2160 
2161     // Lock to get consistent set of values.
2162     uint old_marking_started_after;
2163     uint old_marking_completed_after;
2164     {
2165       MutexLocker ml(Heap_lock);
2166       // Update gc_counter for retrying VMOp if needed. Captured here to be
2167       // consistent with the values we use below for termination tests.  If
2168       // a retry is needed after a possible wait, and another collection
2169       // occurs in the meantime, it will cause our retry to be skipped and
2170       // we&#39;ll recheck for termination with updated conditions from that
2171       // more recent collection.  That&#39;s what we want, rather than having
2172       // our retry possibly perform an unnecessary collection.
2173       gc_counter = total_collections();
2174       old_marking_started_after = _old_marking_cycles_started;
2175       old_marking_completed_after = _old_marking_cycles_completed;
2176     }
2177 
<span class="line-modified">2178     if (cause == GCCause::_wb_breakpoint) {</span>
<span class="line-added">2179       if (op.gc_succeeded()) {</span>
<span class="line-added">2180         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);</span>
<span class="line-added">2181         return true;</span>
<span class="line-added">2182       }</span>
<span class="line-added">2183       // When _wb_breakpoint there can&#39;t be another cycle or deferred.</span>
<span class="line-added">2184       assert(!op.cycle_already_in_progress(), &quot;invariant&quot;);</span>
<span class="line-added">2185       assert(!op.whitebox_attached(), &quot;invariant&quot;);</span>
<span class="line-added">2186       // Concurrent cycle attempt might have been cancelled by some other</span>
<span class="line-added">2187       // collection, so retry.  Unlike other cases below, we want to retry</span>
<span class="line-added">2188       // even if cancelled by a STW full collection, because we really want</span>
<span class="line-added">2189       // to start a concurrent cycle.</span>
<span class="line-added">2190       if (old_marking_started_before != old_marking_started_after) {</span>
<span class="line-added">2191         LOG_COLLECT_CONCURRENTLY(cause, &quot;ignoring STW full GC&quot;);</span>
<span class="line-added">2192         old_marking_started_before = old_marking_started_after;</span>
<span class="line-added">2193       }</span>
<span class="line-added">2194     } else if (!GCCause::is_user_requested_gc(cause)) {</span>
2195       // For an &quot;automatic&quot; (not user-requested) collection, we just need to
2196       // ensure that progress is made.
2197       //
2198       // Request is finished if any of
2199       // (1) the VMOp successfully performed a GC,
2200       // (2) a concurrent cycle was already in progress,
<span class="line-modified">2201       // (3) whitebox is controlling concurrent cycles,</span>
<span class="line-modified">2202       // (4) a new cycle was started (by this thread or some other), or</span>
<span class="line-modified">2203       // (5) a Full GC was performed.</span>
<span class="line-added">2204       // Cases (4) and (5) are detected together by a change to</span>
2205       // _old_marking_cycles_started.
2206       //
<span class="line-modified">2207       // Note that (1) does not imply (4).  If we&#39;re still in the mixed</span>
2208       // phase of an earlier concurrent collection, the request to make the
2209       // collection an initial-mark won&#39;t be honored.  If we don&#39;t check for
2210       // both conditions we&#39;ll spin doing back-to-back collections.
2211       if (op.gc_succeeded() ||
2212           op.cycle_already_in_progress() ||
<span class="line-added">2213           op.whitebox_attached() ||</span>
2214           (old_marking_started_before != old_marking_started_after)) {
2215         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);
2216         return true;
2217       }
2218     } else {                    // User-requested GC.
2219       // For a user-requested collection, we want to ensure that a complete
2220       // full collection has been performed before returning, but without
2221       // waiting for more than needed.
2222 
2223       // For user-requested GCs (unlike non-UR), a successful VMOp implies a
2224       // new cycle was started.  That&#39;s good, because it&#39;s not clear what we
2225       // should do otherwise.  Trying again just does back to back GCs.
2226       // Can&#39;t wait for someone else to start a cycle.  And returning fails
2227       // to meet the goal of ensuring a full collection was performed.
2228       assert(!op.gc_succeeded() ||
2229              (old_marking_started_before != old_marking_started_after),
2230              &quot;invariant: succeeded %s, started before %u, started after %u&quot;,
2231              BOOL_TO_STR(op.gc_succeeded()),
2232              old_marking_started_before, old_marking_started_after);
2233 
</pre>
<hr />
<pre>
2247         LOG_COLLECT_CONCURRENTLY(cause, &quot;wait&quot;);
2248         MonitorLocker ml(G1OldGCCount_lock);
2249         while (gc_counter_less_than(_old_marking_cycles_completed,
2250                                     old_marking_started_after)) {
2251           ml.wait();
2252         }
2253         // Request is finished if the collection we just waited for was
2254         // started after this request.
2255         if (old_marking_started_before != old_marking_started_after) {
2256           LOG_COLLECT_CONCURRENTLY(cause, &quot;complete after wait&quot;);
2257           return true;
2258         }
2259       }
2260 
2261       // If VMOp was successful then it started a new cycle that the above
2262       // wait &amp;etc should have recognized as finishing this request.  This
2263       // differs from a non-user-request, where gc_succeeded does not imply
2264       // a new cycle was started.
2265       assert(!op.gc_succeeded(), &quot;invariant&quot;);
2266 



2267       if (op.cycle_already_in_progress()) {
<span class="line-added">2268         // If VMOp failed because a cycle was already in progress, it</span>
<span class="line-added">2269         // is now complete.  But it didn&#39;t finish this user-requested</span>
<span class="line-added">2270         // GC, so try again.</span>
2271         LOG_COLLECT_CONCURRENTLY(cause, &quot;retry after in-progress&quot;);
2272         continue;
<span class="line-added">2273       } else if (op.whitebox_attached()) {</span>
<span class="line-added">2274         // If WhiteBox wants control, wait for notification of a state</span>
<span class="line-added">2275         // change in the controller, then try again.  Don&#39;t wait for</span>
<span class="line-added">2276         // release of control, since collections may complete while in</span>
<span class="line-added">2277         // control.  Note: This won&#39;t recognize a STW full collection</span>
<span class="line-added">2278         // while waiting; we can&#39;t wait on multiple monitors.</span>
<span class="line-added">2279         LOG_COLLECT_CONCURRENTLY(cause, &quot;whitebox control stall&quot;);</span>
<span class="line-added">2280         MonitorLocker ml(ConcurrentGCBreakpoints::monitor());</span>
<span class="line-added">2281         if (ConcurrentGCBreakpoints::is_controlled()) {</span>
<span class="line-added">2282           ml.wait();</span>
<span class="line-added">2283         }</span>
<span class="line-added">2284         continue;</span>
2285       }
2286     }
2287 
2288     // Collection failed and should be retried.
2289     assert(op.transient_failure(), &quot;invariant&quot;);
2290 

2291     if (GCLocker::is_active_and_needs_gc()) {
<span class="line-added">2292       // If GCLocker is active, wait until clear before retrying.</span>
2293       LOG_COLLECT_CONCURRENTLY(cause, &quot;gc-locker stall&quot;);
2294       GCLocker::stall_until_clear();
2295     }
2296 
2297     LOG_COLLECT_CONCURRENTLY(cause, &quot;retry&quot;);
2298   }
2299 }
2300 
2301 bool G1CollectedHeap::try_collect(GCCause::Cause cause) {
2302   assert_heap_not_locked();
2303 
2304   // Lock to get consistent set of values.
2305   uint gc_count_before;
2306   uint full_gc_count_before;
2307   uint old_marking_started_before;
2308   {
2309     MutexLocker ml(Heap_lock);
2310     gc_count_before = total_collections();
2311     full_gc_count_before = total_full_collections();
2312     old_marking_started_before = _old_marking_cycles_started;
</pre>
<hr />
<pre>
2468   }
2469   return ret_val;
2470 }
2471 
2472 void G1CollectedHeap::deduplicate_string(oop str) {
2473   assert(java_lang_String::is_instance(str), &quot;invariant&quot;);
2474 
2475   if (G1StringDedup::is_enabled()) {
2476     G1StringDedup::deduplicate(str);
2477   }
2478 }
2479 
2480 void G1CollectedHeap::prepare_for_verify() {
2481   _verifier-&gt;prepare_for_verify();
2482 }
2483 
2484 void G1CollectedHeap::verify(VerifyOption vo) {
2485   _verifier-&gt;verify(vo);
2486 }
2487 
<span class="line-modified">2488 bool G1CollectedHeap::supports_concurrent_gc_breakpoints() const {</span>
2489   return true;
2490 }
2491 




2492 bool G1CollectedHeap::is_heterogeneous_heap() const {
2493   return G1Arguments::is_heterogeneous_heap();
2494 }
2495 
2496 class PrintRegionClosure: public HeapRegionClosure {
2497   outputStream* _st;
2498 public:
2499   PrintRegionClosure(outputStream* st) : _st(st) {}
2500   bool do_heap_region(HeapRegion* r) {
2501     r-&gt;print_on(_st);
2502     return false;
2503   }
2504 };
2505 
2506 bool G1CollectedHeap::is_obj_dead_cond(const oop obj,
2507                                        const HeapRegion* hr,
2508                                        const VerifyOption vo) const {
2509   switch (vo) {
2510   case VerifyOption_G1UsePrevMarking: return is_obj_dead(obj, hr);
2511   case VerifyOption_G1UseNextMarking: return is_obj_ill(obj, hr);
</pre>
<hr />
<pre>
3189     // TraceMemoryManagerStats is called) so that the G1 memory pools are updated
3190     // before any GC notifications are raised.
3191     g1mm()-&gt;update_sizes();
3192 
3193     _gc_tracer_stw-&gt;report_evacuation_info(&amp;evacuation_info);
3194     _gc_tracer_stw-&gt;report_tenuring_threshold(_policy-&gt;tenuring_threshold());
3195     _gc_timer_stw-&gt;register_gc_end();
3196     _gc_tracer_stw-&gt;report_gc_end(_gc_timer_stw-&gt;gc_end(), _gc_timer_stw-&gt;time_partitions());
3197   }
3198   // It should now be safe to tell the concurrent mark thread to start
3199   // without its logging output interfering with the logging output
3200   // that came from the pause.
3201 
3202   if (should_start_conc_mark) {
3203     // CAUTION: after the doConcurrentMark() call below, the concurrent marking
3204     // thread(s) could be running concurrently with us. Make sure that anything
3205     // after this point does not assume that we are the only GC thread running.
3206     // Note: of course, the actual marking work will not start until the safepoint
3207     // itself is released in SuspendibleThreadSet::desynchronize().
3208     do_concurrent_mark();
<span class="line-added">3209     ConcurrentGCBreakpoints::notify_idle_to_active();</span>
3210   }
3211 }
3212 
3213 void G1CollectedHeap::remove_self_forwarding_pointers(G1RedirtyCardsQueueSet* rdcqs) {
3214   G1ParRemoveSelfForwardPtrsTask rsfp_task(rdcqs);
3215   workers()-&gt;run_task(&amp;rsfp_task);
3216 }
3217 
3218 void G1CollectedHeap::restore_after_evac_failure(G1RedirtyCardsQueueSet* rdcqs) {
3219   double remove_self_forwards_start = os::elapsedTime();
3220 
3221   remove_self_forwarding_pointers(rdcqs);
3222   _preserved_marks_set.restore(workers());
3223 
3224   phase_times()-&gt;record_evac_fail_remove_self_forwards((os::elapsedTime() - remove_self_forwards_start) * 1000.0);
3225 }
3226 
3227 void G1CollectedHeap::preserve_mark_during_evac_failure(uint worker_id, oop obj, markWord m) {
3228   if (!_evacuation_failed) {
3229     _evacuation_failed = true;
</pre>
</td>
</tr>
</table>
<center><a href="g1BiasedArray.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="g1CollectedHeap.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>