<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/opto/gcm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;libadt/vectset.hpp&quot;
  27 #include &quot;memory/allocation.inline.hpp&quot;
  28 #include &quot;memory/resourceArea.hpp&quot;
  29 #include &quot;opto/block.hpp&quot;
  30 #include &quot;opto/c2compiler.hpp&quot;
  31 #include &quot;opto/callnode.hpp&quot;
  32 #include &quot;opto/cfgnode.hpp&quot;
  33 #include &quot;opto/machnode.hpp&quot;
  34 #include &quot;opto/opcodes.hpp&quot;
  35 #include &quot;opto/phaseX.hpp&quot;
  36 #include &quot;opto/rootnode.hpp&quot;
  37 #include &quot;opto/runtime.hpp&quot;
  38 #include &quot;opto/chaitin.hpp&quot;
  39 #include &quot;runtime/deoptimization.hpp&quot;
  40 
  41 // Portions of code courtesy of Clifford Click
  42 
  43 // Optimization - Graph Style
  44 
  45 // To avoid float value underflow
  46 #define MIN_BLOCK_FREQUENCY 1.e-35f
  47 
  48 //----------------------------schedule_node_into_block-------------------------
  49 // Insert node n into block b. Look for projections of n and make sure they
  50 // are in b also.
  51 void PhaseCFG::schedule_node_into_block( Node *n, Block *b ) {
  52   // Set basic block of n, Add n to b,
  53   map_node_to_block(n, b);
  54   b-&gt;add_inst(n);
  55 
  56   // After Matching, nearly any old Node may have projections trailing it.
  57   // These are usually machine-dependent flags.  In any case, they might
  58   // float to another block below this one.  Move them up.
  59   for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {
  60     Node*  use  = n-&gt;fast_out(i);
  61     if (use-&gt;is_Proj()) {
  62       Block* buse = get_block_for_node(use);
  63       if (buse != b) {              // In wrong block?
  64         if (buse != NULL) {
  65           buse-&gt;find_remove(use);   // Remove from wrong block
  66         }
  67         map_node_to_block(use, b);
  68         b-&gt;add_inst(use);
  69       }
  70     }
  71   }
  72 }
  73 
  74 //----------------------------replace_block_proj_ctrl-------------------------
  75 // Nodes that have is_block_proj() nodes as their control need to use
  76 // the appropriate Region for their actual block as their control since
  77 // the projection will be in a predecessor block.
  78 void PhaseCFG::replace_block_proj_ctrl( Node *n ) {
  79   const Node *in0 = n-&gt;in(0);
  80   assert(in0 != NULL, &quot;Only control-dependent&quot;);
  81   const Node *p = in0-&gt;is_block_proj();
  82   if (p != NULL &amp;&amp; p != n) {    // Control from a block projection?
  83     assert(!n-&gt;pinned() || n-&gt;is_MachConstantBase(), &quot;only pinned MachConstantBase node is expected here&quot;);
  84     // Find trailing Region
  85     Block *pb = get_block_for_node(in0); // Block-projection already has basic block
  86     uint j = 0;
  87     if (pb-&gt;_num_succs != 1) {  // More then 1 successor?
  88       // Search for successor
  89       uint max = pb-&gt;number_of_nodes();
  90       assert( max &gt; 1, &quot;&quot; );
  91       uint start = max - pb-&gt;_num_succs;
  92       // Find which output path belongs to projection
  93       for (j = start; j &lt; max; j++) {
  94         if( pb-&gt;get_node(j) == in0 )
  95           break;
  96       }
  97       assert( j &lt; max, &quot;must find&quot; );
  98       // Change control to match head of successor basic block
  99       j -= start;
 100     }
 101     n-&gt;set_req(0, pb-&gt;_succs[j]-&gt;head());
 102   }
 103 }
 104 
 105 bool PhaseCFG::is_dominator(Node* dom_node, Node* node) {
<a name="1" id="anc1"></a>
 106   if (dom_node == node) {
 107     return true;
 108   }
<a name="2" id="anc2"></a><span class="line-modified"> 109   Block* d = get_block_for_node(dom_node);</span>
<span class="line-modified"> 110   Block* n = get_block_for_node(node);</span>


 111   if (d == n) {
 112     if (dom_node-&gt;is_block_start()) {
 113       return true;
 114     }
 115     if (node-&gt;is_block_start()) {
 116       return false;
 117     }
 118     if (dom_node-&gt;is_block_proj()) {
 119       return false;
 120     }
 121     if (node-&gt;is_block_proj()) {
 122       return true;
 123     }
<a name="3" id="anc3"></a>









 124 #ifdef ASSERT
<a name="4" id="anc4"></a><span class="line-modified"> 125     node-&gt;dump();</span>
<span class="line-modified"> 126     dom_node-&gt;dump();</span>




 127 #endif
<a name="5" id="anc5"></a><span class="line-modified"> 128     fatal(&quot;unhandled&quot;);</span>
 129     return false;
 130   }
 131   return d-&gt;dom_lca(n) == d;
 132 }
 133 
<a name="6" id="anc6"></a>






































 134 //------------------------------schedule_pinned_nodes--------------------------
 135 // Set the basic block for Nodes pinned into blocks
 136 void PhaseCFG::schedule_pinned_nodes(VectorSet &amp;visited) {
 137   // Allocate node stack of size C-&gt;live_nodes()+8 to avoid frequent realloc
<a name="7" id="anc7"></a><span class="line-modified"> 138   GrowableArray &lt;Node *&gt; spstack(C-&gt;live_nodes() + 8);</span>
 139   spstack.push(_root);
 140   while (spstack.is_nonempty()) {
 141     Node* node = spstack.pop();
 142     if (!visited.test_set(node-&gt;_idx)) { // Test node and flag it as visited
 143       if (node-&gt;pinned() &amp;&amp; !has_block(node)) {  // Pinned?  Nail it down!
 144         assert(node-&gt;in(0), &quot;pinned Node must have Control&quot;);
 145         // Before setting block replace block_proj control edge
 146         replace_block_proj_ctrl(node);
 147         Node* input = node-&gt;in(0);
 148         while (!input-&gt;is_block_start()) {
 149           input = input-&gt;in(0);
 150         }
 151         Block* block = get_block_for_node(input); // Basic block of controlling input
 152         schedule_node_into_block(node, block);
 153       }
 154 
 155       // If the node has precedence edges (added when CastPP nodes are
 156       // removed in final_graph_reshaping), fix the control of the
 157       // node to cover the precedence edges and remove the
 158       // dependencies.
 159       Node* n = NULL;
 160       for (uint i = node-&gt;len()-1; i &gt;= node-&gt;req(); i--) {
 161         Node* m = node-&gt;in(i);
 162         if (m == NULL) continue;
<a name="8" id="anc8"></a><span class="line-modified"> 163         // Skip the precedence edge if the test that guarded a CastPP:</span>
<span class="line-modified"> 164         // - was optimized out during escape analysis</span>
<span class="line-modified"> 165         // (OptimizePtrCompare): the CastPP&#39;s control isn&#39;t an end of</span>
<span class="line-removed"> 166         // block.</span>
<span class="line-removed"> 167         // - is moved in the branch of a dominating If: the control of</span>
<span class="line-removed"> 168         // the CastPP is then a Region.</span>
<span class="line-removed"> 169         if (m-&gt;is_block_proj() || m-&gt;is_block_start()) {</span>
 170           node-&gt;rm_prec(i);
 171           if (n == NULL) {
 172             n = m;
 173           } else {
 174             assert(is_dominator(n, m) || is_dominator(m, n), &quot;one must dominate the other&quot;);
 175             n = is_dominator(n, m) ? m : n;
 176           }
<a name="9" id="anc9"></a>


 177         }
 178       }
 179       if (n != NULL) {
 180         assert(node-&gt;in(0), &quot;control should have been set&quot;);
 181         assert(is_dominator(n, node-&gt;in(0)) || is_dominator(node-&gt;in(0), n), &quot;one must dominate the other&quot;);
 182         if (!is_dominator(n, node-&gt;in(0))) {
 183           node-&gt;set_req(0, n);
 184         }
 185       }
 186 
 187       // process all inputs that are non NULL
<a name="10" id="anc10"></a><span class="line-modified"> 188       for (int i = node-&gt;req() - 1; i &gt;= 0; --i) {</span>
 189         if (node-&gt;in(i) != NULL) {
 190           spstack.push(node-&gt;in(i));
 191         }
 192       }
 193     }
 194   }
 195 }
 196 
 197 #ifdef ASSERT
 198 // Assert that new input b2 is dominated by all previous inputs.
 199 // Check this by by seeing that it is dominated by b1, the deepest
 200 // input observed until b2.
 201 static void assert_dom(Block* b1, Block* b2, Node* n, const PhaseCFG* cfg) {
 202   if (b1 == NULL)  return;
 203   assert(b1-&gt;_dom_depth &lt; b2-&gt;_dom_depth, &quot;sanity&quot;);
 204   Block* tmp = b2;
 205   while (tmp != b1 &amp;&amp; tmp != NULL) {
 206     tmp = tmp-&gt;_idom;
 207   }
 208   if (tmp != b1) {
 209     // Detected an unschedulable graph.  Print some nice stuff and die.
 210     tty-&gt;print_cr(&quot;!!! Unschedulable graph !!!&quot;);
 211     for (uint j=0; j&lt;n-&gt;len(); j++) { // For all inputs
 212       Node* inn = n-&gt;in(j); // Get input
 213       if (inn == NULL)  continue;  // Ignore NULL, missing inputs
 214       Block* inb = cfg-&gt;get_block_for_node(inn);
 215       tty-&gt;print(&quot;B%d idom=B%d depth=%2d &quot;,inb-&gt;_pre_order,
 216                  inb-&gt;_idom ? inb-&gt;_idom-&gt;_pre_order : 0, inb-&gt;_dom_depth);
 217       inn-&gt;dump();
 218     }
 219     tty-&gt;print(&quot;Failing node: &quot;);
 220     n-&gt;dump();
 221     assert(false, &quot;unscheduable graph&quot;);
 222   }
 223 }
 224 #endif
 225 
 226 static Block* find_deepest_input(Node* n, const PhaseCFG* cfg) {
 227   // Find the last input dominated by all other inputs.
 228   Block* deepb           = NULL;        // Deepest block so far
 229   int    deepb_dom_depth = 0;
 230   for (uint k = 0; k &lt; n-&gt;len(); k++) { // For all inputs
 231     Node* inn = n-&gt;in(k);               // Get input
 232     if (inn == NULL)  continue;         // Ignore NULL, missing inputs
 233     Block* inb = cfg-&gt;get_block_for_node(inn);
 234     assert(inb != NULL, &quot;must already have scheduled this input&quot;);
 235     if (deepb_dom_depth &lt; (int) inb-&gt;_dom_depth) {
 236       // The new inb must be dominated by the previous deepb.
 237       // The various inputs must be linearly ordered in the dom
 238       // tree, or else there will not be a unique deepest block.
 239       DEBUG_ONLY(assert_dom(deepb, inb, n, cfg));
 240       deepb = inb;                      // Save deepest block
 241       deepb_dom_depth = deepb-&gt;_dom_depth;
 242     }
 243   }
 244   assert(deepb != NULL, &quot;must be at least one input to n&quot;);
 245   return deepb;
 246 }
 247 
 248 
 249 //------------------------------schedule_early---------------------------------
 250 // Find the earliest Block any instruction can be placed in.  Some instructions
 251 // are pinned into Blocks.  Unpinned instructions can appear in last block in
 252 // which all their inputs occur.
 253 bool PhaseCFG::schedule_early(VectorSet &amp;visited, Node_Stack &amp;roots) {
 254   // Allocate stack with enough space to avoid frequent realloc
 255   Node_Stack nstack(roots.size() + 8);
 256   // _root will be processed among C-&gt;top() inputs
 257   roots.push(C-&gt;top(), 0);
 258   visited.set(C-&gt;top()-&gt;_idx);
 259 
 260   while (roots.size() != 0) {
 261     // Use local variables nstack_top_n &amp; nstack_top_i to cache values
 262     // on stack&#39;s top.
 263     Node* parent_node = roots.node();
 264     uint  input_index = 0;
 265     roots.pop();
 266 
 267     while (true) {
 268       if (input_index == 0) {
 269         // Fixup some control.  Constants without control get attached
 270         // to root and nodes that use is_block_proj() nodes should be attached
 271         // to the region that starts their block.
 272         const Node* control_input = parent_node-&gt;in(0);
 273         if (control_input != NULL) {
 274           replace_block_proj_ctrl(parent_node);
 275         } else {
 276           // Is a constant with NO inputs?
 277           if (parent_node-&gt;req() == 1) {
 278             parent_node-&gt;set_req(0, _root);
 279           }
 280         }
 281       }
 282 
 283       // First, visit all inputs and force them to get a block.  If an
 284       // input is already in a block we quit following inputs (to avoid
 285       // cycles). Instead we put that Node on a worklist to be handled
 286       // later (since IT&#39;S inputs may not have a block yet).
 287 
 288       // Assume all n&#39;s inputs will be processed
 289       bool done = true;
 290 
 291       while (input_index &lt; parent_node-&gt;len()) {
 292         Node* in = parent_node-&gt;in(input_index++);
 293         if (in == NULL) {
 294           continue;
 295         }
 296 
 297         int is_visited = visited.test_set(in-&gt;_idx);
 298         if (!has_block(in)) {
 299           if (is_visited) {
 300             assert(false, &quot;graph should be schedulable&quot;);
 301             return false;
 302           }
 303           // Save parent node and next input&#39;s index.
 304           nstack.push(parent_node, input_index);
 305           // Process current input now.
 306           parent_node = in;
 307           input_index = 0;
 308           // Not all n&#39;s inputs processed.
 309           done = false;
 310           break;
 311         } else if (!is_visited) {
 312           // Visit this guy later, using worklist
 313           roots.push(in, 0);
 314         }
 315       }
 316 
 317       if (done) {
 318         // All of n&#39;s inputs have been processed, complete post-processing.
 319 
 320         // Some instructions are pinned into a block.  These include Region,
 321         // Phi, Start, Return, and other control-dependent instructions and
 322         // any projections which depend on them.
 323         if (!parent_node-&gt;pinned()) {
 324           // Set earliest legal block.
 325           Block* earliest_block = find_deepest_input(parent_node, this);
 326           map_node_to_block(parent_node, earliest_block);
 327         } else {
 328           assert(get_block_for_node(parent_node) == get_block_for_node(parent_node-&gt;in(0)), &quot;Pinned Node should be at the same block as its control edge&quot;);
 329         }
 330 
 331         if (nstack.is_empty()) {
 332           // Finished all nodes on stack.
 333           // Process next node on the worklist &#39;roots&#39;.
 334           break;
 335         }
 336         // Get saved parent node and next input&#39;s index.
 337         parent_node = nstack.node();
 338         input_index = nstack.index();
 339         nstack.pop();
 340       }
 341     }
 342   }
 343   return true;
 344 }
 345 
 346 //------------------------------dom_lca----------------------------------------
 347 // Find least common ancestor in dominator tree
 348 // LCA is a current notion of LCA, to be raised above &#39;this&#39;.
 349 // As a convenient boundary condition, return &#39;this&#39; if LCA is NULL.
 350 // Find the LCA of those two nodes.
 351 Block* Block::dom_lca(Block* LCA) {
 352   if (LCA == NULL || LCA == this)  return this;
 353 
 354   Block* anc = this;
 355   while (anc-&gt;_dom_depth &gt; LCA-&gt;_dom_depth)
 356     anc = anc-&gt;_idom;           // Walk up till anc is as high as LCA
 357 
 358   while (LCA-&gt;_dom_depth &gt; anc-&gt;_dom_depth)
 359     LCA = LCA-&gt;_idom;           // Walk up till LCA is as high as anc
 360 
 361   while (LCA != anc) {          // Walk both up till they are the same
 362     LCA = LCA-&gt;_idom;
 363     anc = anc-&gt;_idom;
 364   }
 365 
 366   return LCA;
 367 }
 368 
 369 //--------------------------raise_LCA_above_use--------------------------------
 370 // We are placing a definition, and have been given a def-&gt;use edge.
 371 // The definition must dominate the use, so move the LCA upward in the
 372 // dominator tree to dominate the use.  If the use is a phi, adjust
 373 // the LCA only with the phi input paths which actually use this def.
 374 static Block* raise_LCA_above_use(Block* LCA, Node* use, Node* def, const PhaseCFG* cfg) {
 375   Block* buse = cfg-&gt;get_block_for_node(use);
 376   if (buse == NULL)    return LCA;   // Unused killing Projs have no use block
 377   if (!use-&gt;is_Phi())  return buse-&gt;dom_lca(LCA);
 378   uint pmax = use-&gt;req();       // Number of Phi inputs
 379   // Why does not this loop just break after finding the matching input to
 380   // the Phi?  Well...it&#39;s like this.  I do not have true def-use/use-def
 381   // chains.  Means I cannot distinguish, from the def-use direction, which
 382   // of many use-defs lead from the same use to the same def.  That is, this
 383   // Phi might have several uses of the same def.  Each use appears in a
 384   // different predecessor block.  But when I enter here, I cannot distinguish
 385   // which use-def edge I should find the predecessor block for.  So I find
 386   // them all.  Means I do a little extra work if a Phi uses the same value
 387   // more than once.
 388   for (uint j=1; j&lt;pmax; j++) { // For all inputs
 389     if (use-&gt;in(j) == def) {    // Found matching input?
 390       Block* pred = cfg-&gt;get_block_for_node(buse-&gt;pred(j));
 391       LCA = pred-&gt;dom_lca(LCA);
 392     }
 393   }
 394   return LCA;
 395 }
 396 
 397 //----------------------------raise_LCA_above_marks----------------------------
 398 // Return a new LCA that dominates LCA and any of its marked predecessors.
 399 // Search all my parents up to &#39;early&#39; (exclusive), looking for predecessors
 400 // which are marked with the given index.  Return the LCA (in the dom tree)
 401 // of all marked blocks.  If there are none marked, return the original
 402 // LCA.
 403 static Block* raise_LCA_above_marks(Block* LCA, node_idx_t mark, Block* early, const PhaseCFG* cfg) {
 404   Block_List worklist;
 405   worklist.push(LCA);
 406   while (worklist.size() &gt; 0) {
 407     Block* mid = worklist.pop();
 408     if (mid == early)  continue;  // stop searching here
 409 
 410     // Test and set the visited bit.
 411     if (mid-&gt;raise_LCA_visited() == mark)  continue;  // already visited
 412 
 413     // Don&#39;t process the current LCA, otherwise the search may terminate early
 414     if (mid != LCA &amp;&amp; mid-&gt;raise_LCA_mark() == mark) {
 415       // Raise the LCA.
 416       LCA = mid-&gt;dom_lca(LCA);
 417       if (LCA == early)  break;   // stop searching everywhere
 418       assert(early-&gt;dominates(LCA), &quot;early is high enough&quot;);
 419       // Resume searching at that point, skipping intermediate levels.
 420       worklist.push(LCA);
 421       if (LCA == mid)
 422         continue; // Don&#39;t mark as visited to avoid early termination.
 423     } else {
 424       // Keep searching through this block&#39;s predecessors.
 425       for (uint j = 1, jmax = mid-&gt;num_preds(); j &lt; jmax; j++) {
 426         Block* mid_parent = cfg-&gt;get_block_for_node(mid-&gt;pred(j));
 427         worklist.push(mid_parent);
 428       }
 429     }
 430     mid-&gt;set_raise_LCA_visited(mark);
 431   }
 432   return LCA;
 433 }
 434 
 435 //--------------------------memory_early_block--------------------------------
 436 // This is a variation of find_deepest_input, the heart of schedule_early.
 437 // Find the &quot;early&quot; block for a load, if we considered only memory and
 438 // address inputs, that is, if other data inputs were ignored.
 439 //
 440 // Because a subset of edges are considered, the resulting block will
 441 // be earlier (at a shallower dom_depth) than the true schedule_early
 442 // point of the node. We compute this earlier block as a more permissive
 443 // site for anti-dependency insertion, but only if subsume_loads is enabled.
 444 static Block* memory_early_block(Node* load, Block* early, const PhaseCFG* cfg) {
 445   Node* base;
 446   Node* index;
 447   Node* store = load-&gt;in(MemNode::Memory);
 448   load-&gt;as_Mach()-&gt;memory_inputs(base, index);
 449 
 450   assert(base != NodeSentinel &amp;&amp; index != NodeSentinel,
 451          &quot;unexpected base/index inputs&quot;);
 452 
 453   Node* mem_inputs[4];
 454   int mem_inputs_length = 0;
 455   if (base != NULL)  mem_inputs[mem_inputs_length++] = base;
 456   if (index != NULL) mem_inputs[mem_inputs_length++] = index;
 457   if (store != NULL) mem_inputs[mem_inputs_length++] = store;
 458 
 459   // In the comparision below, add one to account for the control input,
 460   // which may be null, but always takes up a spot in the in array.
 461   if (mem_inputs_length + 1 &lt; (int) load-&gt;req()) {
 462     // This &quot;load&quot; has more inputs than just the memory, base and index inputs.
 463     // For purposes of checking anti-dependences, we need to start
 464     // from the early block of only the address portion of the instruction,
 465     // and ignore other blocks that may have factored into the wider
 466     // schedule_early calculation.
 467     if (load-&gt;in(0) != NULL) mem_inputs[mem_inputs_length++] = load-&gt;in(0);
 468 
 469     Block* deepb           = NULL;        // Deepest block so far
 470     int    deepb_dom_depth = 0;
 471     for (int i = 0; i &lt; mem_inputs_length; i++) {
 472       Block* inb = cfg-&gt;get_block_for_node(mem_inputs[i]);
 473       if (deepb_dom_depth &lt; (int) inb-&gt;_dom_depth) {
 474         // The new inb must be dominated by the previous deepb.
 475         // The various inputs must be linearly ordered in the dom
 476         // tree, or else there will not be a unique deepest block.
 477         DEBUG_ONLY(assert_dom(deepb, inb, load, cfg));
 478         deepb = inb;                      // Save deepest block
 479         deepb_dom_depth = deepb-&gt;_dom_depth;
 480       }
 481     }
 482     early = deepb;
 483   }
 484 
 485   return early;
 486 }
 487 
 488 //--------------------------insert_anti_dependences---------------------------
 489 // A load may need to witness memory that nearby stores can overwrite.
 490 // For each nearby store, either insert an &quot;anti-dependence&quot; edge
 491 // from the load to the store, or else move LCA upward to force the
 492 // load to (eventually) be scheduled in a block above the store.
 493 //
 494 // Do not add edges to stores on distinct control-flow paths;
 495 // only add edges to stores which might interfere.
 496 //
 497 // Return the (updated) LCA.  There will not be any possibly interfering
 498 // store between the load&#39;s &quot;early block&quot; and the updated LCA.
 499 // Any stores in the updated LCA will have new precedence edges
 500 // back to the load.  The caller is expected to schedule the load
 501 // in the LCA, in which case the precedence edges will make LCM
 502 // preserve anti-dependences.  The caller may also hoist the load
 503 // above the LCA, if it is not the early block.
 504 Block* PhaseCFG::insert_anti_dependences(Block* LCA, Node* load, bool verify) {
 505   assert(load-&gt;needs_anti_dependence_check(), &quot;must be a load of some sort&quot;);
 506   assert(LCA != NULL, &quot;&quot;);
 507   DEBUG_ONLY(Block* LCA_orig = LCA);
 508 
 509   // Compute the alias index.  Loads and stores with different alias indices
 510   // do not need anti-dependence edges.
 511   int load_alias_idx = C-&gt;get_alias_index(load-&gt;adr_type());
 512 #ifdef ASSERT
 513   assert(Compile::AliasIdxTop &lt;= load_alias_idx &amp;&amp; load_alias_idx &lt; C-&gt;num_alias_types(), &quot;Invalid alias index&quot;);
 514   if (load_alias_idx == Compile::AliasIdxBot &amp;&amp; C-&gt;AliasLevel() &gt; 0 &amp;&amp;
 515       (PrintOpto || VerifyAliases ||
 516        (PrintMiscellaneous &amp;&amp; (WizardMode || Verbose)))) {
 517     // Load nodes should not consume all of memory.
 518     // Reporting a bottom type indicates a bug in adlc.
 519     // If some particular type of node validly consumes all of memory,
 520     // sharpen the preceding &quot;if&quot; to exclude it, so we can catch bugs here.
 521     tty-&gt;print_cr(&quot;*** Possible Anti-Dependence Bug:  Load consumes all of memory.&quot;);
 522     load-&gt;dump(2);
 523     if (VerifyAliases)  assert(load_alias_idx != Compile::AliasIdxBot, &quot;&quot;);
 524   }
 525 #endif
 526 
 527   if (!C-&gt;alias_type(load_alias_idx)-&gt;is_rewritable()) {
 528     // It is impossible to spoil this load by putting stores before it,
 529     // because we know that the stores will never update the value
 530     // which &#39;load&#39; must witness.
 531     return LCA;
 532   }
 533 
 534   node_idx_t load_index = load-&gt;_idx;
 535 
 536   // Note the earliest legal placement of &#39;load&#39;, as determined by
 537   // by the unique point in the dom tree where all memory effects
 538   // and other inputs are first available.  (Computed by schedule_early.)
 539   // For normal loads, &#39;early&#39; is the shallowest place (dom graph wise)
 540   // to look for anti-deps between this load and any store.
 541   Block* early = get_block_for_node(load);
 542 
 543   // If we are subsuming loads, compute an &quot;early&quot; block that only considers
 544   // memory or address inputs. This block may be different than the
 545   // schedule_early block in that it could be at an even shallower depth in the
 546   // dominator tree, and allow for a broader discovery of anti-dependences.
 547   if (C-&gt;subsume_loads()) {
 548     early = memory_early_block(load, early, this);
 549   }
 550 
 551   ResourceArea *area = Thread::current()-&gt;resource_area();
 552   Node_List worklist_mem(area);     // prior memory state to store
 553   Node_List worklist_store(area);   // possible-def to explore
 554   Node_List worklist_visited(area); // visited mergemem nodes
 555   Node_List non_early_stores(area); // all relevant stores outside of early
 556   bool must_raise_LCA = false;
 557 
 558 #ifdef TRACK_PHI_INPUTS
 559   // %%% This extra checking fails because MergeMem nodes are not GVNed.
 560   // Provide &quot;phi_inputs&quot; to check if every input to a PhiNode is from the
 561   // original memory state.  This indicates a PhiNode for which should not
 562   // prevent the load from sinking.  For such a block, set_raise_LCA_mark
 563   // may be overly conservative.
 564   // Mechanism: count inputs seen for each Phi encountered in worklist_store.
 565   DEBUG_ONLY(GrowableArray&lt;uint&gt; phi_inputs(area, C-&gt;unique(),0,0));
 566 #endif
 567 
 568   // &#39;load&#39; uses some memory state; look for users of the same state.
 569   // Recurse through MergeMem nodes to the stores that use them.
 570 
 571   // Each of these stores is a possible definition of memory
 572   // that &#39;load&#39; needs to use.  We need to force &#39;load&#39;
 573   // to occur before each such store.  When the store is in
 574   // the same block as &#39;load&#39;, we insert an anti-dependence
 575   // edge load-&gt;store.
 576 
 577   // The relevant stores &quot;nearby&quot; the load consist of a tree rooted
 578   // at initial_mem, with internal nodes of type MergeMem.
 579   // Therefore, the branches visited by the worklist are of this form:
 580   //    initial_mem -&gt; (MergeMem -&gt;)* store
 581   // The anti-dependence constraints apply only to the fringe of this tree.
 582 
 583   Node* initial_mem = load-&gt;in(MemNode::Memory);
 584   worklist_store.push(initial_mem);
 585   worklist_visited.push(initial_mem);
 586   worklist_mem.push(NULL);
 587   while (worklist_store.size() &gt; 0) {
 588     // Examine a nearby store to see if it might interfere with our load.
 589     Node* mem   = worklist_mem.pop();
 590     Node* store = worklist_store.pop();
 591     uint op = store-&gt;Opcode();
 592 
 593     // MergeMems do not directly have anti-deps.
 594     // Treat them as internal nodes in a forward tree of memory states,
 595     // the leaves of which are each a &#39;possible-def&#39;.
 596     if (store == initial_mem    // root (exclusive) of tree we are searching
 597         || op == Op_MergeMem    // internal node of tree we are searching
 598         ) {
 599       mem = store;   // It&#39;s not a possibly interfering store.
 600       if (store == initial_mem)
 601         initial_mem = NULL;  // only process initial memory once
 602 
 603       for (DUIterator_Fast imax, i = mem-&gt;fast_outs(imax); i &lt; imax; i++) {
 604         store = mem-&gt;fast_out(i);
 605         if (store-&gt;is_MergeMem()) {
 606           // Be sure we don&#39;t get into combinatorial problems.
 607           // (Allow phis to be repeated; they can merge two relevant states.)
 608           uint j = worklist_visited.size();
 609           for (; j &gt; 0; j--) {
 610             if (worklist_visited.at(j-1) == store)  break;
 611           }
 612           if (j &gt; 0)  continue; // already on work list; do not repeat
 613           worklist_visited.push(store);
 614         }
 615         worklist_mem.push(mem);
 616         worklist_store.push(store);
 617       }
 618       continue;
 619     }
 620 
 621     if (op == Op_MachProj || op == Op_Catch)   continue;
 622     if (store-&gt;needs_anti_dependence_check())  continue;  // not really a store
 623 
 624     // Compute the alias index.  Loads and stores with different alias
 625     // indices do not need anti-dependence edges.  Wide MemBar&#39;s are
 626     // anti-dependent on everything (except immutable memories).
 627     const TypePtr* adr_type = store-&gt;adr_type();
 628     if (!C-&gt;can_alias(adr_type, load_alias_idx))  continue;
 629 
 630     // Most slow-path runtime calls do NOT modify Java memory, but
 631     // they can block and so write Raw memory.
 632     if (store-&gt;is_Mach()) {
 633       MachNode* mstore = store-&gt;as_Mach();
 634       if (load_alias_idx != Compile::AliasIdxRaw) {
 635         // Check for call into the runtime using the Java calling
 636         // convention (and from there into a wrapper); it has no
 637         // _method.  Can&#39;t do this optimization for Native calls because
 638         // they CAN write to Java memory.
 639         if (mstore-&gt;ideal_Opcode() == Op_CallStaticJava) {
 640           assert(mstore-&gt;is_MachSafePoint(), &quot;&quot;);
 641           MachSafePointNode* ms = (MachSafePointNode*) mstore;
 642           assert(ms-&gt;is_MachCallJava(), &quot;&quot;);
 643           MachCallJavaNode* mcj = (MachCallJavaNode*) ms;
 644           if (mcj-&gt;_method == NULL) {
 645             // These runtime calls do not write to Java visible memory
 646             // (other than Raw) and so do not require anti-dependence edges.
 647             continue;
 648           }
 649         }
 650         // Same for SafePoints: they read/write Raw but only read otherwise.
 651         // This is basically a workaround for SafePoints only defining control
 652         // instead of control + memory.
 653         if (mstore-&gt;ideal_Opcode() == Op_SafePoint)
 654           continue;
<a name="11" id="anc11"></a><span class="line-removed"> 655 </span>
<span class="line-removed"> 656         // Check if the store is a membar on which the load is control dependent.</span>
<span class="line-removed"> 657         // Inserting an anti-dependency between that membar and the load would</span>
<span class="line-removed"> 658         // create a cycle that causes local scheduling to fail.</span>
<span class="line-removed"> 659         if (mstore-&gt;isa_MachMemBar()) {</span>
<span class="line-removed"> 660           Node* dom = load-&gt;find_exact_control(load-&gt;in(0));</span>
<span class="line-removed"> 661           while (dom != NULL &amp;&amp; dom != dom-&gt;in(0) &amp;&amp; dom != mstore) {</span>
<span class="line-removed"> 662             dom = dom-&gt;in(0);</span>
<span class="line-removed"> 663           }</span>
<span class="line-removed"> 664           if (dom == mstore) {</span>
<span class="line-removed"> 665             continue;</span>
<span class="line-removed"> 666           }</span>
<span class="line-removed"> 667         }</span>
 668       } else {
 669         // Some raw memory, such as the load of &quot;top&quot; at an allocation,
 670         // can be control dependent on the previous safepoint. See
 671         // comments in GraphKit::allocate_heap() about control input.
 672         // Inserting an anti-dep between such a safepoint and a use
 673         // creates a cycle, and will cause a subsequent failure in
 674         // local scheduling.  (BugId 4919904)
 675         // (%%% How can a control input be a safepoint and not a projection??)
 676         if (mstore-&gt;ideal_Opcode() == Op_SafePoint &amp;&amp; load-&gt;in(0) == mstore)
 677           continue;
 678       }
 679     }
 680 
 681     // Identify a block that the current load must be above,
 682     // or else observe that &#39;store&#39; is all the way up in the
 683     // earliest legal block for &#39;load&#39;.  In the latter case,
 684     // immediately insert an anti-dependence edge.
 685     Block* store_block = get_block_for_node(store);
 686     assert(store_block != NULL, &quot;unused killing projections skipped above&quot;);
 687 
 688     if (store-&gt;is_Phi()) {
 689       // Loop-phis need to raise load before input. (Other phis are treated
 690       // as store below.)
 691       //
 692       // &#39;load&#39; uses memory which is one (or more) of the Phi&#39;s inputs.
 693       // It must be scheduled not before the Phi, but rather before
 694       // each of the relevant Phi inputs.
 695       //
 696       // Instead of finding the LCA of all inputs to a Phi that match &#39;mem&#39;,
 697       // we mark each corresponding predecessor block and do a combined
 698       // hoisting operation later (raise_LCA_above_marks).
 699       //
 700       // Do not assert(store_block != early, &quot;Phi merging memory after access&quot;)
 701       // PhiNode may be at start of block &#39;early&#39; with backedge to &#39;early&#39;
 702       DEBUG_ONLY(bool found_match = false);
 703       for (uint j = PhiNode::Input, jmax = store-&gt;req(); j &lt; jmax; j++) {
 704         if (store-&gt;in(j) == mem) {   // Found matching input?
 705           DEBUG_ONLY(found_match = true);
 706           Block* pred_block = get_block_for_node(store_block-&gt;pred(j));
 707           if (pred_block != early) {
 708             // If any predecessor of the Phi matches the load&#39;s &quot;early block&quot;,
 709             // we do not need a precedence edge between the Phi and &#39;load&#39;
 710             // since the load will be forced into a block preceding the Phi.
 711             pred_block-&gt;set_raise_LCA_mark(load_index);
 712             assert(!LCA_orig-&gt;dominates(pred_block) ||
 713                    early-&gt;dominates(pred_block), &quot;early is high enough&quot;);
 714             must_raise_LCA = true;
 715           } else {
 716             // anti-dependent upon PHI pinned below &#39;early&#39;, no edge needed
 717             LCA = early;             // but can not schedule below &#39;early&#39;
 718           }
 719         }
 720       }
 721       assert(found_match, &quot;no worklist bug&quot;);
 722 #ifdef TRACK_PHI_INPUTS
 723 #ifdef ASSERT
 724         // This assert asks about correct handling of PhiNodes, which may not
 725         // have all input edges directly from &#39;mem&#39;. See BugId 4621264
 726         int num_mem_inputs = phi_inputs.at_grow(store-&gt;_idx,0) + 1;
 727         // Increment by exactly one even if there are multiple copies of &#39;mem&#39;
 728         // coming into the phi, because we will run this block several times
 729         // if there are several copies of &#39;mem&#39;.  (That&#39;s how DU iterators work.)
 730         phi_inputs.at_put(store-&gt;_idx, num_mem_inputs);
 731         assert(PhiNode::Input + num_mem_inputs &lt; store-&gt;req(),
 732                &quot;Expect at least one phi input will not be from original memory state&quot;);
 733 #endif //ASSERT
 734 #endif //TRACK_PHI_INPUTS
 735     } else if (store_block != early) {
 736       // &#39;store&#39; is between the current LCA and earliest possible block.
 737       // Label its block, and decide later on how to raise the LCA
 738       // to include the effect on LCA of this store.
 739       // If this store&#39;s block gets chosen as the raised LCA, we
 740       // will find him on the non_early_stores list and stick him
 741       // with a precedence edge.
 742       // (But, don&#39;t bother if LCA is already raised all the way.)
 743       if (LCA != early) {
 744         store_block-&gt;set_raise_LCA_mark(load_index);
 745         must_raise_LCA = true;
 746         non_early_stores.push(store);
 747       }
 748     } else {
 749       // Found a possibly-interfering store in the load&#39;s &#39;early&#39; block.
 750       // This means &#39;load&#39; cannot sink at all in the dominator tree.
 751       // Add an anti-dep edge, and squeeze &#39;load&#39; into the highest block.
 752       assert(store != load-&gt;find_exact_control(load-&gt;in(0)), &quot;dependence cycle found&quot;);
 753       if (verify) {
 754         assert(store-&gt;find_edge(load) != -1, &quot;missing precedence edge&quot;);
 755       } else {
 756         store-&gt;add_prec(load);
 757       }
 758       LCA = early;
 759       // This turns off the process of gathering non_early_stores.
 760     }
 761   }
 762   // (Worklist is now empty; all nearby stores have been visited.)
 763 
 764   // Finished if &#39;load&#39; must be scheduled in its &#39;early&#39; block.
 765   // If we found any stores there, they have already been given
 766   // precedence edges.
 767   if (LCA == early)  return LCA;
 768 
 769   // We get here only if there are no possibly-interfering stores
 770   // in the load&#39;s &#39;early&#39; block.  Move LCA up above all predecessors
 771   // which contain stores we have noted.
 772   //
 773   // The raised LCA block can be a home to such interfering stores,
 774   // but its predecessors must not contain any such stores.
 775   //
 776   // The raised LCA will be a lower bound for placing the load,
 777   // preventing the load from sinking past any block containing
 778   // a store that may invalidate the memory state required by &#39;load&#39;.
 779   if (must_raise_LCA)
 780     LCA = raise_LCA_above_marks(LCA, load-&gt;_idx, early, this);
 781   if (LCA == early)  return LCA;
 782 
 783   // Insert anti-dependence edges from &#39;load&#39; to each store
 784   // in the non-early LCA block.
 785   // Mine the non_early_stores list for such stores.
 786   if (LCA-&gt;raise_LCA_mark() == load_index) {
 787     while (non_early_stores.size() &gt; 0) {
 788       Node* store = non_early_stores.pop();
 789       Block* store_block = get_block_for_node(store);
 790       if (store_block == LCA) {
 791         // add anti_dependence from store to load in its own block
 792         assert(store != load-&gt;find_exact_control(load-&gt;in(0)), &quot;dependence cycle found&quot;);
 793         if (verify) {
 794           assert(store-&gt;find_edge(load) != -1, &quot;missing precedence edge&quot;);
 795         } else {
 796           store-&gt;add_prec(load);
 797         }
 798       } else {
 799         assert(store_block-&gt;raise_LCA_mark() == load_index, &quot;block was marked&quot;);
 800         // Any other stores we found must be either inside the new LCA
 801         // or else outside the original LCA.  In the latter case, they
 802         // did not interfere with any use of &#39;load&#39;.
 803         assert(LCA-&gt;dominates(store_block)
 804                || !LCA_orig-&gt;dominates(store_block), &quot;no stray stores&quot;);
 805       }
 806     }
 807   }
 808 
 809   // Return the highest block containing stores; any stores
 810   // within that block have been given anti-dependence edges.
 811   return LCA;
 812 }
 813 
 814 // This class is used to iterate backwards over the nodes in the graph.
 815 
 816 class Node_Backward_Iterator {
 817 
 818 private:
 819   Node_Backward_Iterator();
 820 
 821 public:
 822   // Constructor for the iterator
 823   Node_Backward_Iterator(Node *root, VectorSet &amp;visited, Node_Stack &amp;stack, PhaseCFG &amp;cfg);
 824 
 825   // Postincrement operator to iterate over the nodes
 826   Node *next();
 827 
 828 private:
 829   VectorSet   &amp;_visited;
 830   Node_Stack  &amp;_stack;
 831   PhaseCFG &amp;_cfg;
 832 };
 833 
 834 // Constructor for the Node_Backward_Iterator
 835 Node_Backward_Iterator::Node_Backward_Iterator( Node *root, VectorSet &amp;visited, Node_Stack &amp;stack, PhaseCFG &amp;cfg)
 836   : _visited(visited), _stack(stack), _cfg(cfg) {
 837   // The stack should contain exactly the root
 838   stack.clear();
 839   stack.push(root, root-&gt;outcnt());
 840 
 841   // Clear the visited bits
 842   visited.clear();
 843 }
 844 
 845 // Iterator for the Node_Backward_Iterator
 846 Node *Node_Backward_Iterator::next() {
 847 
 848   // If the _stack is empty, then just return NULL: finished.
 849   if ( !_stack.size() )
 850     return NULL;
 851 
 852   // I visit unvisited not-anti-dependence users first, then anti-dependent
 853   // children next. I iterate backwards to support removal of nodes.
 854   // The stack holds states consisting of 3 values:
 855   // current Def node, flag which indicates 1st/2nd pass, index of current out edge
 856   Node *self = (Node*)(((uintptr_t)_stack.node()) &amp; ~1);
 857   bool iterate_anti_dep = (((uintptr_t)_stack.node()) &amp; 1);
 858   uint idx = MIN2(_stack.index(), self-&gt;outcnt()); // Support removal of nodes.
 859   _stack.pop();
 860 
 861   // I cycle here when I am entering a deeper level of recursion.
 862   // The key variable &#39;self&#39; was set prior to jumping here.
 863   while( 1 ) {
 864 
 865     _visited.set(self-&gt;_idx);
 866 
 867     // Now schedule all uses as late as possible.
 868     const Node* src = self-&gt;is_Proj() ? self-&gt;in(0) : self;
 869     uint src_rpo = _cfg.get_block_for_node(src)-&gt;_rpo;
 870 
 871     // Schedule all nodes in a post-order visit
 872     Node *unvisited = NULL;  // Unvisited anti-dependent Node, if any
 873 
 874     // Scan for unvisited nodes
 875     while (idx &gt; 0) {
 876       // For all uses, schedule late
 877       Node* n = self-&gt;raw_out(--idx); // Use
 878 
 879       // Skip already visited children
 880       if ( _visited.test(n-&gt;_idx) )
 881         continue;
 882 
 883       // do not traverse backward control edges
 884       Node *use = n-&gt;is_Proj() ? n-&gt;in(0) : n;
 885       uint use_rpo = _cfg.get_block_for_node(use)-&gt;_rpo;
 886 
 887       if ( use_rpo &lt; src_rpo )
 888         continue;
 889 
 890       // Phi nodes always precede uses in a basic block
 891       if ( use_rpo == src_rpo &amp;&amp; use-&gt;is_Phi() )
 892         continue;
 893 
 894       unvisited = n;      // Found unvisited
 895 
 896       // Check for possible-anti-dependent
 897       // 1st pass: No such nodes, 2nd pass: Only such nodes.
 898       if (n-&gt;needs_anti_dependence_check() == iterate_anti_dep) {
 899         unvisited = n;      // Found unvisited
 900         break;
 901       }
 902     }
 903 
 904     // Did I find an unvisited not-anti-dependent Node?
 905     if (!unvisited) {
 906       if (!iterate_anti_dep) {
 907         // 2nd pass: Iterate over nodes which needs_anti_dependence_check.
 908         iterate_anti_dep = true;
 909         idx = self-&gt;outcnt();
 910         continue;
 911       }
 912       break;                  // All done with children; post-visit &#39;self&#39;
 913     }
 914 
 915     // Visit the unvisited Node.  Contains the obvious push to
 916     // indicate I&#39;m entering a deeper level of recursion.  I push the
 917     // old state onto the _stack and set a new state and loop (recurse).
 918     _stack.push((Node*)((uintptr_t)self | (uintptr_t)iterate_anti_dep), idx);
 919     self = unvisited;
 920     iterate_anti_dep = false;
 921     idx = self-&gt;outcnt();
 922   } // End recursion loop
 923 
 924   return self;
 925 }
 926 
 927 //------------------------------ComputeLatenciesBackwards----------------------
 928 // Compute the latency of all the instructions.
 929 void PhaseCFG::compute_latencies_backwards(VectorSet &amp;visited, Node_Stack &amp;stack) {
 930 #ifndef PRODUCT
 931   if (trace_opto_pipelining())
 932     tty-&gt;print(&quot;\n#---- ComputeLatenciesBackwards ----\n&quot;);
 933 #endif
 934 
 935   Node_Backward_Iterator iter((Node *)_root, visited, stack, *this);
 936   Node *n;
 937 
 938   // Walk over all the nodes from last to first
 939   while ((n = iter.next())) {
 940     // Set the latency for the definitions of this instruction
 941     partial_latency_of_defs(n);
 942   }
 943 } // end ComputeLatenciesBackwards
 944 
 945 //------------------------------partial_latency_of_defs------------------------
 946 // Compute the latency impact of this node on all defs.  This computes
 947 // a number that increases as we approach the beginning of the routine.
 948 void PhaseCFG::partial_latency_of_defs(Node *n) {
 949   // Set the latency for this instruction
 950 #ifndef PRODUCT
 951   if (trace_opto_pipelining()) {
 952     tty-&gt;print(&quot;# latency_to_inputs: node_latency[%d] = %d for node&quot;, n-&gt;_idx, get_latency_for_node(n));
 953     dump();
 954   }
 955 #endif
 956 
 957   if (n-&gt;is_Proj()) {
 958     n = n-&gt;in(0);
 959   }
 960 
 961   if (n-&gt;is_Root()) {
 962     return;
 963   }
 964 
 965   uint nlen = n-&gt;len();
 966   uint use_latency = get_latency_for_node(n);
 967   uint use_pre_order = get_block_for_node(n)-&gt;_pre_order;
 968 
 969   for (uint j = 0; j &lt; nlen; j++) {
 970     Node *def = n-&gt;in(j);
 971 
 972     if (!def || def == n) {
 973       continue;
 974     }
 975 
 976     // Walk backwards thru projections
 977     if (def-&gt;is_Proj()) {
 978       def = def-&gt;in(0);
 979     }
 980 
 981 #ifndef PRODUCT
 982     if (trace_opto_pipelining()) {
 983       tty-&gt;print(&quot;#    in(%2d): &quot;, j);
 984       def-&gt;dump();
 985     }
 986 #endif
 987 
 988     // If the defining block is not known, assume it is ok
 989     Block *def_block = get_block_for_node(def);
 990     uint def_pre_order = def_block ? def_block-&gt;_pre_order : 0;
 991 
 992     if ((use_pre_order &lt;  def_pre_order) || (use_pre_order == def_pre_order &amp;&amp; n-&gt;is_Phi())) {
 993       continue;
 994     }
 995 
 996     uint delta_latency = n-&gt;latency(j);
 997     uint current_latency = delta_latency + use_latency;
 998 
 999     if (get_latency_for_node(def) &lt; current_latency) {
1000       set_latency_for_node(def, current_latency);
1001     }
1002 
1003 #ifndef PRODUCT
1004     if (trace_opto_pipelining()) {
1005       tty-&gt;print_cr(&quot;#      %d + edge_latency(%d) == %d -&gt; %d, node_latency[%d] = %d&quot;, use_latency, j, delta_latency, current_latency, def-&gt;_idx, get_latency_for_node(def));
1006     }
1007 #endif
1008   }
1009 }
1010 
1011 //------------------------------latency_from_use-------------------------------
1012 // Compute the latency of a specific use
1013 int PhaseCFG::latency_from_use(Node *n, const Node *def, Node *use) {
1014   // If self-reference, return no latency
1015   if (use == n || use-&gt;is_Root()) {
1016     return 0;
1017   }
1018 
1019   uint def_pre_order = get_block_for_node(def)-&gt;_pre_order;
1020   uint latency = 0;
1021 
1022   // If the use is not a projection, then it is simple...
1023   if (!use-&gt;is_Proj()) {
1024 #ifndef PRODUCT
1025     if (trace_opto_pipelining()) {
1026       tty-&gt;print(&quot;#    out(): &quot;);
1027       use-&gt;dump();
1028     }
1029 #endif
1030 
1031     uint use_pre_order = get_block_for_node(use)-&gt;_pre_order;
1032 
1033     if (use_pre_order &lt; def_pre_order)
1034       return 0;
1035 
1036     if (use_pre_order == def_pre_order &amp;&amp; use-&gt;is_Phi())
1037       return 0;
1038 
1039     uint nlen = use-&gt;len();
1040     uint nl = get_latency_for_node(use);
1041 
1042     for ( uint j=0; j&lt;nlen; j++ ) {
1043       if (use-&gt;in(j) == n) {
1044         // Change this if we want local latencies
1045         uint ul = use-&gt;latency(j);
1046         uint  l = ul + nl;
1047         if (latency &lt; l) latency = l;
1048 #ifndef PRODUCT
1049         if (trace_opto_pipelining()) {
1050           tty-&gt;print_cr(&quot;#      %d + edge_latency(%d) == %d -&gt; %d, latency = %d&quot;,
1051                         nl, j, ul, l, latency);
1052         }
1053 #endif
1054       }
1055     }
1056   } else {
1057     // This is a projection, just grab the latency of the use(s)
1058     for (DUIterator_Fast jmax, j = use-&gt;fast_outs(jmax); j &lt; jmax; j++) {
1059       uint l = latency_from_use(use, def, use-&gt;fast_out(j));
1060       if (latency &lt; l) latency = l;
1061     }
1062   }
1063 
1064   return latency;
1065 }
1066 
1067 //------------------------------latency_from_uses------------------------------
1068 // Compute the latency of this instruction relative to all of it&#39;s uses.
1069 // This computes a number that increases as we approach the beginning of the
1070 // routine.
1071 void PhaseCFG::latency_from_uses(Node *n) {
1072   // Set the latency for this instruction
1073 #ifndef PRODUCT
1074   if (trace_opto_pipelining()) {
1075     tty-&gt;print(&quot;# latency_from_outputs: node_latency[%d] = %d for node&quot;, n-&gt;_idx, get_latency_for_node(n));
1076     dump();
1077   }
1078 #endif
1079   uint latency=0;
1080   const Node *def = n-&gt;is_Proj() ? n-&gt;in(0): n;
1081 
1082   for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {
1083     uint l = latency_from_use(n, def, n-&gt;fast_out(i));
1084 
1085     if (latency &lt; l) latency = l;
1086   }
1087 
1088   set_latency_for_node(n, latency);
1089 }
1090 
1091 //------------------------------hoist_to_cheaper_block-------------------------
1092 // Pick a block for node self, between early and LCA, that is a cheaper
1093 // alternative to LCA.
1094 Block* PhaseCFG::hoist_to_cheaper_block(Block* LCA, Block* early, Node* self) {
1095   const double delta = 1+PROB_UNLIKELY_MAG(4);
1096   Block* least       = LCA;
1097   double least_freq  = least-&gt;_freq;
1098   uint target        = get_latency_for_node(self);
1099   uint start_latency = get_latency_for_node(LCA-&gt;head());
1100   uint end_latency   = get_latency_for_node(LCA-&gt;get_node(LCA-&gt;end_idx()));
1101   bool in_latency    = (target &lt;= start_latency);
1102   const Block* root_block = get_block_for_node(_root);
1103 
1104   // Turn off latency scheduling if scheduling is just plain off
1105   if (!C-&gt;do_scheduling())
1106     in_latency = true;
1107 
1108   // Do not hoist (to cover latency) instructions which target a
1109   // single register.  Hoisting stretches the live range of the
1110   // single register and may force spilling.
1111   MachNode* mach = self-&gt;is_Mach() ? self-&gt;as_Mach() : NULL;
1112   if (mach &amp;&amp; mach-&gt;out_RegMask().is_bound1() &amp;&amp; mach-&gt;out_RegMask().is_NotEmpty())
1113     in_latency = true;
1114 
1115 #ifndef PRODUCT
1116   if (trace_opto_pipelining()) {
1117     tty-&gt;print(&quot;# Find cheaper block for latency %d: &quot;, get_latency_for_node(self));
1118     self-&gt;dump();
1119     tty-&gt;print_cr(&quot;#   B%d: start latency for [%4d]=%d, end latency for [%4d]=%d, freq=%g&quot;,
1120       LCA-&gt;_pre_order,
1121       LCA-&gt;head()-&gt;_idx,
1122       start_latency,
1123       LCA-&gt;get_node(LCA-&gt;end_idx())-&gt;_idx,
1124       end_latency,
1125       least_freq);
1126   }
1127 #endif
1128 
1129   int cand_cnt = 0;  // number of candidates tried
1130 
1131   // Walk up the dominator tree from LCA (Lowest common ancestor) to
1132   // the earliest legal location.  Capture the least execution frequency.
1133   while (LCA != early) {
1134     LCA = LCA-&gt;_idom;         // Follow up the dominator tree
1135 
1136     if (LCA == NULL) {
1137       // Bailout without retry
1138       assert(false, &quot;graph should be schedulable&quot;);
1139       C-&gt;record_method_not_compilable(&quot;late schedule failed: LCA == NULL&quot;);
1140       return least;
1141     }
1142 
1143     // Don&#39;t hoist machine instructions to the root basic block
1144     if (mach &amp;&amp; LCA == root_block)
1145       break;
1146 
1147     uint start_lat = get_latency_for_node(LCA-&gt;head());
1148     uint end_idx   = LCA-&gt;end_idx();
1149     uint end_lat   = get_latency_for_node(LCA-&gt;get_node(end_idx));
1150     double LCA_freq = LCA-&gt;_freq;
1151 #ifndef PRODUCT
1152     if (trace_opto_pipelining()) {
1153       tty-&gt;print_cr(&quot;#   B%d: start latency for [%4d]=%d, end latency for [%4d]=%d, freq=%g&quot;,
1154         LCA-&gt;_pre_order, LCA-&gt;head()-&gt;_idx, start_lat, end_idx, end_lat, LCA_freq);
1155     }
1156 #endif
1157     cand_cnt++;
1158     if (LCA_freq &lt; least_freq              || // Better Frequency
1159         (StressGCM &amp;&amp; Compile::randomized_select(cand_cnt)) || // Should be randomly accepted in stress mode
1160          (!StressGCM                    &amp;&amp;    // Otherwise, choose with latency
1161           !in_latency                   &amp;&amp;    // No block containing latency
1162           LCA_freq &lt; least_freq * delta &amp;&amp;    // No worse frequency
1163           target &gt;= end_lat             &amp;&amp;    // within latency range
1164           !self-&gt;is_iteratively_computed() )  // But don&#39;t hoist IV increments
1165              // because they may end up above other uses of their phi forcing
1166              // their result register to be different from their input.
1167        ) {
1168       least = LCA;            // Found cheaper block
1169       least_freq = LCA_freq;
1170       start_latency = start_lat;
1171       end_latency = end_lat;
1172       if (target &lt;= start_lat)
1173         in_latency = true;
1174     }
1175   }
1176 
1177 #ifndef PRODUCT
1178   if (trace_opto_pipelining()) {
1179     tty-&gt;print_cr(&quot;#  Choose block B%d with start latency=%d and freq=%g&quot;,
1180       least-&gt;_pre_order, start_latency, least_freq);
1181   }
1182 #endif
1183 
1184   // See if the latency needs to be updated
1185   if (target &lt; end_latency) {
1186 #ifndef PRODUCT
1187     if (trace_opto_pipelining()) {
1188       tty-&gt;print_cr(&quot;#  Change latency for [%4d] from %d to %d&quot;, self-&gt;_idx, target, end_latency);
1189     }
1190 #endif
1191     set_latency_for_node(self, end_latency);
1192     partial_latency_of_defs(self);
1193   }
1194 
1195   return least;
1196 }
1197 
1198 
1199 //------------------------------schedule_late-----------------------------------
1200 // Now schedule all codes as LATE as possible.  This is the LCA in the
1201 // dominator tree of all USES of a value.  Pick the block with the least
1202 // loop nesting depth that is lowest in the dominator tree.
1203 extern const char must_clone[];
1204 void PhaseCFG::schedule_late(VectorSet &amp;visited, Node_Stack &amp;stack) {
1205 #ifndef PRODUCT
1206   if (trace_opto_pipelining())
1207     tty-&gt;print(&quot;\n#---- schedule_late ----\n&quot;);
1208 #endif
1209 
1210   Node_Backward_Iterator iter((Node *)_root, visited, stack, *this);
1211   Node *self;
1212 
1213   // Walk over all the nodes from last to first
1214   while ((self = iter.next())) {
1215     Block* early = get_block_for_node(self); // Earliest legal placement
1216 
1217     if (self-&gt;is_top()) {
1218       // Top node goes in bb #2 with other constants.
1219       // It must be special-cased, because it has no out edges.
1220       early-&gt;add_inst(self);
1221       continue;
1222     }
1223 
1224     // No uses, just terminate
1225     if (self-&gt;outcnt() == 0) {
1226       assert(self-&gt;is_MachProj(), &quot;sanity&quot;);
1227       continue;                   // Must be a dead machine projection
1228     }
1229 
1230     // If node is pinned in the block, then no scheduling can be done.
1231     if( self-&gt;pinned() )          // Pinned in block?
1232       continue;
1233 
1234     MachNode* mach = self-&gt;is_Mach() ? self-&gt;as_Mach() : NULL;
1235     if (mach) {
1236       switch (mach-&gt;ideal_Opcode()) {
1237       case Op_CreateEx:
1238         // Don&#39;t move exception creation
1239         early-&gt;add_inst(self);
1240         continue;
1241         break;
1242       case Op_CheckCastPP: {
1243         // Don&#39;t move CheckCastPP nodes away from their input, if the input
1244         // is a rawptr (5071820).
1245         Node *def = self-&gt;in(1);
1246         if (def != NULL &amp;&amp; def-&gt;bottom_type()-&gt;base() == Type::RawPtr) {
1247           early-&gt;add_inst(self);
1248 #ifdef ASSERT
1249           _raw_oops.push(def);
1250 #endif
1251           continue;
1252         }
1253         break;
1254       }
1255       default:
1256         break;
1257       }
1258     }
1259 
1260     // Gather LCA of all uses
1261     Block *LCA = NULL;
1262     {
1263       for (DUIterator_Fast imax, i = self-&gt;fast_outs(imax); i &lt; imax; i++) {
1264         // For all uses, find LCA
1265         Node* use = self-&gt;fast_out(i);
1266         LCA = raise_LCA_above_use(LCA, use, self, this);
1267       }
1268       guarantee(LCA != NULL, &quot;There must be a LCA&quot;);
1269     }  // (Hide defs of imax, i from rest of block.)
1270 
1271     // Place temps in the block of their use.  This isn&#39;t a
1272     // requirement for correctness but it reduces useless
1273     // interference between temps and other nodes.
1274     if (mach != NULL &amp;&amp; mach-&gt;is_MachTemp()) {
1275       map_node_to_block(self, LCA);
1276       LCA-&gt;add_inst(self);
1277       continue;
1278     }
1279 
1280     // Check if &#39;self&#39; could be anti-dependent on memory
1281     if (self-&gt;needs_anti_dependence_check()) {
1282       // Hoist LCA above possible-defs and insert anti-dependences to
1283       // defs in new LCA block.
1284       LCA = insert_anti_dependences(LCA, self);
1285     }
1286 
1287     if (early-&gt;_dom_depth &gt; LCA-&gt;_dom_depth) {
1288       // Somehow the LCA has moved above the earliest legal point.
1289       // (One way this can happen is via memory_early_block.)
1290       if (C-&gt;subsume_loads() == true &amp;&amp; !C-&gt;failing()) {
1291         // Retry with subsume_loads == false
1292         // If this is the first failure, the sentinel string will &quot;stick&quot;
1293         // to the Compile object, and the C2Compiler will see it and retry.
1294         C-&gt;record_failure(C2Compiler::retry_no_subsuming_loads());
1295       } else {
1296         // Bailout without retry when (early-&gt;_dom_depth &gt; LCA-&gt;_dom_depth)
1297         assert(false, &quot;graph should be schedulable&quot;);
1298         C-&gt;record_method_not_compilable(&quot;late schedule failed: incorrect graph&quot;);
1299       }
1300       return;
1301     }
1302 
1303     // If there is no opportunity to hoist, then we&#39;re done.
1304     // In stress mode, try to hoist even the single operations.
1305     bool try_to_hoist = StressGCM || (LCA != early);
1306 
1307     // Must clone guys stay next to use; no hoisting allowed.
1308     // Also cannot hoist guys that alter memory or are otherwise not
1309     // allocatable (hoisting can make a value live longer, leading to
1310     // anti and output dependency problems which are normally resolved
1311     // by the register allocator giving everyone a different register).
1312     if (mach != NULL &amp;&amp; must_clone[mach-&gt;ideal_Opcode()])
1313       try_to_hoist = false;
1314 
1315     Block* late = NULL;
1316     if (try_to_hoist) {
1317       // Now find the block with the least execution frequency.
1318       // Start at the latest schedule and work up to the earliest schedule
1319       // in the dominator tree.  Thus the Node will dominate all its uses.
1320       late = hoist_to_cheaper_block(LCA, early, self);
1321     } else {
1322       // Just use the LCA of the uses.
1323       late = LCA;
1324     }
1325 
1326     // Put the node into target block
1327     schedule_node_into_block(self, late);
1328 
1329 #ifdef ASSERT
1330     if (self-&gt;needs_anti_dependence_check()) {
1331       // since precedence edges are only inserted when we&#39;re sure they
1332       // are needed make sure that after placement in a block we don&#39;t
1333       // need any new precedence edges.
1334       verify_anti_dependences(late, self);
1335     }
1336 #endif
1337   } // Loop until all nodes have been visited
1338 
1339 } // end ScheduleLate
1340 
1341 //------------------------------GlobalCodeMotion-------------------------------
1342 void PhaseCFG::global_code_motion() {
1343   ResourceMark rm;
1344 
1345 #ifndef PRODUCT
1346   if (trace_opto_pipelining()) {
1347     tty-&gt;print(&quot;\n---- Start GlobalCodeMotion ----\n&quot;);
1348   }
1349 #endif
1350 
1351   // Initialize the node to block mapping for things on the proj_list
1352   for (uint i = 0; i &lt; _matcher.number_of_projections(); i++) {
1353     unmap_node_from_block(_matcher.get_projection(i));
1354   }
1355 
1356   // Set the basic block for Nodes pinned into blocks
1357   Arena* arena = Thread::current()-&gt;resource_area();
1358   VectorSet visited(arena);
1359   schedule_pinned_nodes(visited);
1360 
1361   // Find the earliest Block any instruction can be placed in.  Some
1362   // instructions are pinned into Blocks.  Unpinned instructions can
1363   // appear in last block in which all their inputs occur.
1364   visited.clear();
1365   Node_Stack stack(arena, (C-&gt;live_nodes() &gt;&gt; 2) + 16); // pre-grow
1366   if (!schedule_early(visited, stack)) {
1367     // Bailout without retry
1368     C-&gt;record_method_not_compilable(&quot;early schedule failed&quot;);
1369     return;
1370   }
1371 
1372   // Build Def-Use edges.
1373   // Compute the latency information (via backwards walk) for all the
1374   // instructions in the graph
1375   _node_latency = new GrowableArray&lt;uint&gt;(); // resource_area allocation
1376 
1377   if (C-&gt;do_scheduling()) {
1378     compute_latencies_backwards(visited, stack);
1379   }
1380 
1381   // Now schedule all codes as LATE as possible.  This is the LCA in the
1382   // dominator tree of all USES of a value.  Pick the block with the least
1383   // loop nesting depth that is lowest in the dominator tree.
1384   // ( visited.clear() called in schedule_late()-&gt;Node_Backward_Iterator() )
1385   schedule_late(visited, stack);
1386   if (C-&gt;failing()) {
<a name="12" id="anc12"></a><span class="line-removed">1387     // schedule_late fails only when graph is incorrect.</span>
<span class="line-removed">1388     assert(!VerifyGraphEdges, &quot;verification should have failed&quot;);</span>
1389     return;
1390   }
1391 
1392 #ifndef PRODUCT
1393   if (trace_opto_pipelining()) {
1394     tty-&gt;print(&quot;\n---- Detect implicit null checks ----\n&quot;);
1395   }
1396 #endif
1397 
1398   // Detect implicit-null-check opportunities.  Basically, find NULL checks
1399   // with suitable memory ops nearby.  Use the memory op to do the NULL check.
1400   // I can generate a memory op if there is not one nearby.
1401   if (C-&gt;is_method_compilation()) {
1402     // By reversing the loop direction we get a very minor gain on mpegaudio.
1403     // Feel free to revert to a forward loop for clarity.
1404     // for( int i=0; i &lt; (int)matcher._null_check_tests.size(); i+=2 ) {
1405     for (int i = _matcher._null_check_tests.size() - 2; i &gt;= 0; i -= 2) {
1406       Node* proj = _matcher._null_check_tests[i];
1407       Node* val  = _matcher._null_check_tests[i + 1];
1408       Block* block = get_block_for_node(proj);
1409       implicit_null_check(block, proj, val, C-&gt;allowed_deopt_reasons());
1410       // The implicit_null_check will only perform the transformation
1411       // if the null branch is truly uncommon, *and* it leads to an
1412       // uncommon trap.  Combined with the too_many_traps guards
1413       // above, this prevents SEGV storms reported in 6366351,
1414       // by recompiling offending methods without this optimization.
1415     }
1416   }
1417 
1418   bool block_size_threshold_ok = false;
1419   intptr_t *recalc_pressure_nodes = NULL;
1420   if (OptoRegScheduling) {
1421     for (uint i = 0; i &lt; number_of_blocks(); i++) {
1422       Block* block = get_block(i);
1423       if (block-&gt;number_of_nodes() &gt; 10) {
1424         block_size_threshold_ok = true;
1425         break;
1426       }
1427     }
1428   }
1429 
1430   // Enabling the scheduler for register pressure plus finding blocks of size to schedule for it
1431   // is key to enabling this feature.
1432   PhaseChaitin regalloc(C-&gt;unique(), *this, _matcher, true);
1433   ResourceArea live_arena(mtCompiler);      // Arena for liveness
1434   ResourceMark rm_live(&amp;live_arena);
1435   PhaseLive live(*this, regalloc._lrg_map.names(), &amp;live_arena, true);
1436   PhaseIFG ifg(&amp;live_arena);
1437   if (OptoRegScheduling &amp;&amp; block_size_threshold_ok) {
1438     regalloc.mark_ssa();
1439     Compile::TracePhase tp(&quot;computeLive&quot;, &amp;timers[_t_computeLive]);
1440     rm_live.reset_to_mark();           // Reclaim working storage
1441     IndexSet::reset_memory(C, &amp;live_arena);
1442     uint node_size = regalloc._lrg_map.max_lrg_id();
1443     ifg.init(node_size); // Empty IFG
1444     regalloc.set_ifg(ifg);
1445     regalloc.set_live(live);
1446     regalloc.gather_lrg_masks(false);    // Collect LRG masks
1447     live.compute(node_size); // Compute liveness
1448 
1449     recalc_pressure_nodes = NEW_RESOURCE_ARRAY(intptr_t, node_size);
1450     for (uint i = 0; i &lt; node_size; i++) {
1451       recalc_pressure_nodes[i] = 0;
1452     }
1453   }
1454   _regalloc = &amp;regalloc;
1455 
1456 #ifndef PRODUCT
1457   if (trace_opto_pipelining()) {
1458     tty-&gt;print(&quot;\n---- Start Local Scheduling ----\n&quot;);
1459   }
1460 #endif
1461 
1462   // Schedule locally.  Right now a simple topological sort.
1463   // Later, do a real latency aware scheduler.
1464   GrowableArray&lt;int&gt; ready_cnt(C-&gt;unique(), C-&gt;unique(), -1);
1465   visited.reset();
1466   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1467     Block* block = get_block(i);
1468     if (!schedule_local(block, ready_cnt, visited, recalc_pressure_nodes)) {
1469       if (!C-&gt;failure_reason_is(C2Compiler::retry_no_subsuming_loads())) {
1470         C-&gt;record_method_not_compilable(&quot;local schedule failed&quot;);
1471       }
1472       _regalloc = NULL;
1473       return;
1474     }
1475   }
1476   _regalloc = NULL;
1477 
1478   // If we inserted any instructions between a Call and his CatchNode,
1479   // clone the instructions on all paths below the Catch.
1480   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1481     Block* block = get_block(i);
1482     call_catch_cleanup(block);
1483   }
1484 
1485 #ifndef PRODUCT
1486   if (trace_opto_pipelining()) {
1487     tty-&gt;print(&quot;\n---- After GlobalCodeMotion ----\n&quot;);
1488     for (uint i = 0; i &lt; number_of_blocks(); i++) {
1489       Block* block = get_block(i);
1490       block-&gt;dump();
1491     }
1492   }
1493 #endif
1494   // Dead.
1495   _node_latency = (GrowableArray&lt;uint&gt; *)((intptr_t)0xdeadbeef);
1496 }
1497 
1498 bool PhaseCFG::do_global_code_motion() {
1499 
1500   build_dominator_tree();
1501   if (C-&gt;failing()) {
1502     return false;
1503   }
1504 
1505   NOT_PRODUCT( C-&gt;verify_graph_edges(); )
1506 
1507   estimate_block_frequency();
1508 
1509   global_code_motion();
1510 
1511   if (C-&gt;failing()) {
1512     return false;
1513   }
1514 
1515   return true;
1516 }
1517 
1518 //------------------------------Estimate_Block_Frequency-----------------------
1519 // Estimate block frequencies based on IfNode probabilities.
1520 void PhaseCFG::estimate_block_frequency() {
1521 
1522   // Force conditional branches leading to uncommon traps to be unlikely,
1523   // not because we get to the uncommon_trap with less relative frequency,
1524   // but because an uncommon_trap typically causes a deopt, so we only get
1525   // there once.
1526   if (C-&gt;do_freq_based_layout()) {
1527     Block_List worklist;
1528     Block* root_blk = get_block(0);
1529     for (uint i = 1; i &lt; root_blk-&gt;num_preds(); i++) {
1530       Block *pb = get_block_for_node(root_blk-&gt;pred(i));
1531       if (pb-&gt;has_uncommon_code()) {
1532         worklist.push(pb);
1533       }
1534     }
1535     while (worklist.size() &gt; 0) {
1536       Block* uct = worklist.pop();
1537       if (uct == get_root_block()) {
1538         continue;
1539       }
1540       for (uint i = 1; i &lt; uct-&gt;num_preds(); i++) {
1541         Block *pb = get_block_for_node(uct-&gt;pred(i));
1542         if (pb-&gt;_num_succs == 1) {
1543           worklist.push(pb);
1544         } else if (pb-&gt;num_fall_throughs() == 2) {
1545           pb-&gt;update_uncommon_branch(uct);
1546         }
1547       }
1548     }
1549   }
1550 
1551   // Create the loop tree and calculate loop depth.
1552   _root_loop = create_loop_tree();
1553   _root_loop-&gt;compute_loop_depth(0);
1554 
1555   // Compute block frequency of each block, relative to a single loop entry.
1556   _root_loop-&gt;compute_freq();
1557 
1558   // Adjust all frequencies to be relative to a single method entry
1559   _root_loop-&gt;_freq = 1.0;
1560   _root_loop-&gt;scale_freq();
1561 
1562   // Save outmost loop frequency for LRG frequency threshold
1563   _outer_loop_frequency = _root_loop-&gt;outer_loop_freq();
1564 
1565   // force paths ending at uncommon traps to be infrequent
1566   if (!C-&gt;do_freq_based_layout()) {
1567     Block_List worklist;
1568     Block* root_blk = get_block(0);
1569     for (uint i = 1; i &lt; root_blk-&gt;num_preds(); i++) {
1570       Block *pb = get_block_for_node(root_blk-&gt;pred(i));
1571       if (pb-&gt;has_uncommon_code()) {
1572         worklist.push(pb);
1573       }
1574     }
1575     while (worklist.size() &gt; 0) {
1576       Block* uct = worklist.pop();
1577       uct-&gt;_freq = PROB_MIN;
1578       for (uint i = 1; i &lt; uct-&gt;num_preds(); i++) {
1579         Block *pb = get_block_for_node(uct-&gt;pred(i));
1580         if (pb-&gt;_num_succs == 1 &amp;&amp; pb-&gt;_freq &gt; PROB_MIN) {
1581           worklist.push(pb);
1582         }
1583       }
1584     }
1585   }
1586 
1587 #ifdef ASSERT
1588   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1589     Block* b = get_block(i);
1590     assert(b-&gt;_freq &gt;= MIN_BLOCK_FREQUENCY, &quot;Register Allocator requires meaningful block frequency&quot;);
1591   }
1592 #endif
1593 
1594 #ifndef PRODUCT
1595   if (PrintCFGBlockFreq) {
1596     tty-&gt;print_cr(&quot;CFG Block Frequencies&quot;);
1597     _root_loop-&gt;dump_tree();
1598     if (Verbose) {
1599       tty-&gt;print_cr(&quot;PhaseCFG dump&quot;);
1600       dump();
1601       tty-&gt;print_cr(&quot;Node dump&quot;);
1602       _root-&gt;dump(99999);
1603     }
1604   }
1605 #endif
1606 }
1607 
1608 //----------------------------create_loop_tree--------------------------------
1609 // Create a loop tree from the CFG
1610 CFGLoop* PhaseCFG::create_loop_tree() {
1611 
1612 #ifdef ASSERT
1613   assert(get_block(0) == get_root_block(), &quot;first block should be root block&quot;);
1614   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1615     Block* block = get_block(i);
1616     // Check that _loop field are clear...we could clear them if not.
1617     assert(block-&gt;_loop == NULL, &quot;clear _loop expected&quot;);
1618     // Sanity check that the RPO numbering is reflected in the _blocks array.
1619     // It doesn&#39;t have to be for the loop tree to be built, but if it is not,
1620     // then the blocks have been reordered since dom graph building...which
1621     // may question the RPO numbering
1622     assert(block-&gt;_rpo == i, &quot;unexpected reverse post order number&quot;);
1623   }
1624 #endif
1625 
1626   int idct = 0;
1627   CFGLoop* root_loop = new CFGLoop(idct++);
1628 
1629   Block_List worklist;
1630 
1631   // Assign blocks to loops
1632   for(uint i = number_of_blocks() - 1; i &gt; 0; i-- ) { // skip Root block
1633     Block* block = get_block(i);
1634 
1635     if (block-&gt;head()-&gt;is_Loop()) {
1636       Block* loop_head = block;
1637       assert(loop_head-&gt;num_preds() - 1 == 2, &quot;loop must have 2 predecessors&quot;);
1638       Node* tail_n = loop_head-&gt;pred(LoopNode::LoopBackControl);
1639       Block* tail = get_block_for_node(tail_n);
1640 
1641       // Defensively filter out Loop nodes for non-single-entry loops.
1642       // For all reasonable loops, the head occurs before the tail in RPO.
1643       if (i &lt;= tail-&gt;_rpo) {
1644 
1645         // The tail and (recursive) predecessors of the tail
1646         // are made members of a new loop.
1647 
1648         assert(worklist.size() == 0, &quot;nonempty worklist&quot;);
1649         CFGLoop* nloop = new CFGLoop(idct++);
1650         assert(loop_head-&gt;_loop == NULL, &quot;just checking&quot;);
1651         loop_head-&gt;_loop = nloop;
1652         // Add to nloop so push_pred() will skip over inner loops
1653         nloop-&gt;add_member(loop_head);
1654         nloop-&gt;push_pred(loop_head, LoopNode::LoopBackControl, worklist, this);
1655 
1656         while (worklist.size() &gt; 0) {
1657           Block* member = worklist.pop();
1658           if (member != loop_head) {
1659             for (uint j = 1; j &lt; member-&gt;num_preds(); j++) {
1660               nloop-&gt;push_pred(member, j, worklist, this);
1661             }
1662           }
1663         }
1664       }
1665     }
1666   }
1667 
1668   // Create a member list for each loop consisting
1669   // of both blocks and (immediate child) loops.
1670   for (uint i = 0; i &lt; number_of_blocks(); i++) {
1671     Block* block = get_block(i);
1672     CFGLoop* lp = block-&gt;_loop;
1673     if (lp == NULL) {
1674       // Not assigned to a loop. Add it to the method&#39;s pseudo loop.
1675       block-&gt;_loop = root_loop;
1676       lp = root_loop;
1677     }
1678     if (lp == root_loop || block != lp-&gt;head()) { // loop heads are already members
1679       lp-&gt;add_member(block);
1680     }
1681     if (lp != root_loop) {
1682       if (lp-&gt;parent() == NULL) {
1683         // Not a nested loop. Make it a child of the method&#39;s pseudo loop.
1684         root_loop-&gt;add_nested_loop(lp);
1685       }
1686       if (block == lp-&gt;head()) {
1687         // Add nested loop to member list of parent loop.
1688         lp-&gt;parent()-&gt;add_member(lp);
1689       }
1690     }
1691   }
1692 
1693   return root_loop;
1694 }
1695 
1696 //------------------------------push_pred--------------------------------------
1697 void CFGLoop::push_pred(Block* blk, int i, Block_List&amp; worklist, PhaseCFG* cfg) {
1698   Node* pred_n = blk-&gt;pred(i);
1699   Block* pred = cfg-&gt;get_block_for_node(pred_n);
1700   CFGLoop *pred_loop = pred-&gt;_loop;
1701   if (pred_loop == NULL) {
1702     // Filter out blocks for non-single-entry loops.
1703     // For all reasonable loops, the head occurs before the tail in RPO.
1704     if (pred-&gt;_rpo &gt; head()-&gt;_rpo) {
1705       pred-&gt;_loop = this;
1706       worklist.push(pred);
1707     }
1708   } else if (pred_loop != this) {
1709     // Nested loop.
1710     while (pred_loop-&gt;_parent != NULL &amp;&amp; pred_loop-&gt;_parent != this) {
1711       pred_loop = pred_loop-&gt;_parent;
1712     }
1713     // Make pred&#39;s loop be a child
1714     if (pred_loop-&gt;_parent == NULL) {
1715       add_nested_loop(pred_loop);
1716       // Continue with loop entry predecessor.
1717       Block* pred_head = pred_loop-&gt;head();
1718       assert(pred_head-&gt;num_preds() - 1 == 2, &quot;loop must have 2 predecessors&quot;);
1719       assert(pred_head != head(), &quot;loop head in only one loop&quot;);
1720       push_pred(pred_head, LoopNode::EntryControl, worklist, cfg);
1721     } else {
1722       assert(pred_loop-&gt;_parent == this &amp;&amp; _parent == NULL, &quot;just checking&quot;);
1723     }
1724   }
1725 }
1726 
1727 //------------------------------add_nested_loop--------------------------------
1728 // Make cl a child of the current loop in the loop tree.
1729 void CFGLoop::add_nested_loop(CFGLoop* cl) {
1730   assert(_parent == NULL, &quot;no parent yet&quot;);
1731   assert(cl != this, &quot;not my own parent&quot;);
1732   cl-&gt;_parent = this;
1733   CFGLoop* ch = _child;
1734   if (ch == NULL) {
1735     _child = cl;
1736   } else {
1737     while (ch-&gt;_sibling != NULL) { ch = ch-&gt;_sibling; }
1738     ch-&gt;_sibling = cl;
1739   }
1740 }
1741 
1742 //------------------------------compute_loop_depth-----------------------------
1743 // Store the loop depth in each CFGLoop object.
1744 // Recursively walk the children to do the same for them.
1745 void CFGLoop::compute_loop_depth(int depth) {
1746   _depth = depth;
1747   CFGLoop* ch = _child;
1748   while (ch != NULL) {
1749     ch-&gt;compute_loop_depth(depth + 1);
1750     ch = ch-&gt;_sibling;
1751   }
1752 }
1753 
1754 //------------------------------compute_freq-----------------------------------
1755 // Compute the frequency of each block and loop, relative to a single entry
1756 // into the dominating loop head.
1757 void CFGLoop::compute_freq() {
1758   // Bottom up traversal of loop tree (visit inner loops first.)
1759   // Set loop head frequency to 1.0, then transitively
1760   // compute frequency for all successors in the loop,
1761   // as well as for each exit edge.  Inner loops are
1762   // treated as single blocks with loop exit targets
1763   // as the successor blocks.
1764 
1765   // Nested loops first
1766   CFGLoop* ch = _child;
1767   while (ch != NULL) {
1768     ch-&gt;compute_freq();
1769     ch = ch-&gt;_sibling;
1770   }
1771   assert (_members.length() &gt; 0, &quot;no empty loops&quot;);
1772   Block* hd = head();
1773   hd-&gt;_freq = 1.0;
1774   for (int i = 0; i &lt; _members.length(); i++) {
1775     CFGElement* s = _members.at(i);
1776     double freq = s-&gt;_freq;
1777     if (s-&gt;is_block()) {
1778       Block* b = s-&gt;as_Block();
1779       for (uint j = 0; j &lt; b-&gt;_num_succs; j++) {
1780         Block* sb = b-&gt;_succs[j];
1781         update_succ_freq(sb, freq * b-&gt;succ_prob(j));
1782       }
1783     } else {
1784       CFGLoop* lp = s-&gt;as_CFGLoop();
1785       assert(lp-&gt;_parent == this, &quot;immediate child&quot;);
1786       for (int k = 0; k &lt; lp-&gt;_exits.length(); k++) {
1787         Block* eb = lp-&gt;_exits.at(k).get_target();
1788         double prob = lp-&gt;_exits.at(k).get_prob();
1789         update_succ_freq(eb, freq * prob);
1790       }
1791     }
1792   }
1793 
1794   // For all loops other than the outer, &quot;method&quot; loop,
1795   // sum and normalize the exit probability. The &quot;method&quot; loop
1796   // should keep the initial exit probability of 1, so that
1797   // inner blocks do not get erroneously scaled.
1798   if (_depth != 0) {
1799     // Total the exit probabilities for this loop.
1800     double exits_sum = 0.0f;
1801     for (int i = 0; i &lt; _exits.length(); i++) {
1802       exits_sum += _exits.at(i).get_prob();
1803     }
1804 
1805     // Normalize the exit probabilities. Until now, the
1806     // probabilities estimate the possibility of exit per
1807     // a single loop iteration; afterward, they estimate
1808     // the probability of exit per loop entry.
1809     for (int i = 0; i &lt; _exits.length(); i++) {
1810       Block* et = _exits.at(i).get_target();
1811       float new_prob = 0.0f;
1812       if (_exits.at(i).get_prob() &gt; 0.0f) {
1813         new_prob = _exits.at(i).get_prob() / exits_sum;
1814       }
1815       BlockProbPair bpp(et, new_prob);
1816       _exits.at_put(i, bpp);
1817     }
1818 
1819     // Save the total, but guard against unreasonable probability,
1820     // as the value is used to estimate the loop trip count.
1821     // An infinite trip count would blur relative block
1822     // frequencies.
1823     if (exits_sum &gt; 1.0f) exits_sum = 1.0;
1824     if (exits_sum &lt; PROB_MIN) exits_sum = PROB_MIN;
1825     _exit_prob = exits_sum;
1826   }
1827 }
1828 
1829 //------------------------------succ_prob-------------------------------------
1830 // Determine the probability of reaching successor &#39;i&#39; from the receiver block.
1831 float Block::succ_prob(uint i) {
1832   int eidx = end_idx();
1833   Node *n = get_node(eidx);  // Get ending Node
1834 
1835   int op = n-&gt;Opcode();
1836   if (n-&gt;is_Mach()) {
1837     if (n-&gt;is_MachNullCheck()) {
1838       // Can only reach here if called after lcm. The original Op_If is gone,
1839       // so we attempt to infer the probability from one or both of the
1840       // successor blocks.
1841       assert(_num_succs == 2, &quot;expecting 2 successors of a null check&quot;);
1842       // If either successor has only one predecessor, then the
1843       // probability estimate can be derived using the
1844       // relative frequency of the successor and this block.
1845       if (_succs[i]-&gt;num_preds() == 2) {
1846         return _succs[i]-&gt;_freq / _freq;
1847       } else if (_succs[1-i]-&gt;num_preds() == 2) {
1848         return 1 - (_succs[1-i]-&gt;_freq / _freq);
1849       } else {
1850         // Estimate using both successor frequencies
1851         float freq = _succs[i]-&gt;_freq;
1852         return freq / (freq + _succs[1-i]-&gt;_freq);
1853       }
1854     }
1855     op = n-&gt;as_Mach()-&gt;ideal_Opcode();
1856   }
1857 
1858 
1859   // Switch on branch type
1860   switch( op ) {
1861   case Op_CountedLoopEnd:
1862   case Op_If: {
1863     assert (i &lt; 2, &quot;just checking&quot;);
1864     // Conditionals pass on only part of their frequency
1865     float prob  = n-&gt;as_MachIf()-&gt;_prob;
1866     assert(prob &gt;= 0.0 &amp;&amp; prob &lt;= 1.0, &quot;out of range probability&quot;);
1867     // If succ[i] is the FALSE branch, invert path info
1868     if( get_node(i + eidx + 1)-&gt;Opcode() == Op_IfFalse ) {
1869       return 1.0f - prob; // not taken
1870     } else {
1871       return prob; // taken
1872     }
1873   }
1874 
1875   case Op_Jump:
1876     return n-&gt;as_MachJump()-&gt;_probs[get_node(i + eidx + 1)-&gt;as_JumpProj()-&gt;_con];
1877 
1878   case Op_Catch: {
1879     const CatchProjNode *ci = get_node(i + eidx + 1)-&gt;as_CatchProj();
1880     if (ci-&gt;_con == CatchProjNode::fall_through_index) {
1881       // Fall-thru path gets the lion&#39;s share.
1882       return 1.0f - PROB_UNLIKELY_MAG(5)*_num_succs;
1883     } else {
1884       // Presume exceptional paths are equally unlikely
1885       return PROB_UNLIKELY_MAG(5);
1886     }
1887   }
1888 
1889   case Op_Root:
1890   case Op_Goto:
1891     // Pass frequency straight thru to target
1892     return 1.0f;
1893 
1894   case Op_NeverBranch:
1895     return 0.0f;
1896 
1897   case Op_TailCall:
1898   case Op_TailJump:
1899   case Op_Return:
1900   case Op_Halt:
1901   case Op_Rethrow:
1902     // Do not push out freq to root block
1903     return 0.0f;
1904 
1905   default:
1906     ShouldNotReachHere();
1907   }
1908 
1909   return 0.0f;
1910 }
1911 
1912 //------------------------------num_fall_throughs-----------------------------
1913 // Return the number of fall-through candidates for a block
1914 int Block::num_fall_throughs() {
1915   int eidx = end_idx();
1916   Node *n = get_node(eidx);  // Get ending Node
1917 
1918   int op = n-&gt;Opcode();
1919   if (n-&gt;is_Mach()) {
1920     if (n-&gt;is_MachNullCheck()) {
1921       // In theory, either side can fall-thru, for simplicity sake,
1922       // let&#39;s say only the false branch can now.
1923       return 1;
1924     }
1925     op = n-&gt;as_Mach()-&gt;ideal_Opcode();
1926   }
1927 
1928   // Switch on branch type
1929   switch( op ) {
1930   case Op_CountedLoopEnd:
1931   case Op_If:
1932     return 2;
1933 
1934   case Op_Root:
1935   case Op_Goto:
1936     return 1;
1937 
1938   case Op_Catch: {
1939     for (uint i = 0; i &lt; _num_succs; i++) {
1940       const CatchProjNode *ci = get_node(i + eidx + 1)-&gt;as_CatchProj();
1941       if (ci-&gt;_con == CatchProjNode::fall_through_index) {
1942         return 1;
1943       }
1944     }
1945     return 0;
1946   }
1947 
1948   case Op_Jump:
1949   case Op_NeverBranch:
1950   case Op_TailCall:
1951   case Op_TailJump:
1952   case Op_Return:
1953   case Op_Halt:
1954   case Op_Rethrow:
1955     return 0;
1956 
1957   default:
1958     ShouldNotReachHere();
1959   }
1960 
1961   return 0;
1962 }
1963 
1964 //------------------------------succ_fall_through-----------------------------
1965 // Return true if a specific successor could be fall-through target.
1966 bool Block::succ_fall_through(uint i) {
1967   int eidx = end_idx();
1968   Node *n = get_node(eidx);  // Get ending Node
1969 
1970   int op = n-&gt;Opcode();
1971   if (n-&gt;is_Mach()) {
1972     if (n-&gt;is_MachNullCheck()) {
1973       // In theory, either side can fall-thru, for simplicity sake,
1974       // let&#39;s say only the false branch can now.
1975       return get_node(i + eidx + 1)-&gt;Opcode() == Op_IfFalse;
1976     }
1977     op = n-&gt;as_Mach()-&gt;ideal_Opcode();
1978   }
1979 
1980   // Switch on branch type
1981   switch( op ) {
1982   case Op_CountedLoopEnd:
1983   case Op_If:
1984   case Op_Root:
1985   case Op_Goto:
1986     return true;
1987 
1988   case Op_Catch: {
1989     const CatchProjNode *ci = get_node(i + eidx + 1)-&gt;as_CatchProj();
1990     return ci-&gt;_con == CatchProjNode::fall_through_index;
1991   }
1992 
1993   case Op_Jump:
1994   case Op_NeverBranch:
1995   case Op_TailCall:
1996   case Op_TailJump:
1997   case Op_Return:
1998   case Op_Halt:
1999   case Op_Rethrow:
2000     return false;
2001 
2002   default:
2003     ShouldNotReachHere();
2004   }
2005 
2006   return false;
2007 }
2008 
2009 //------------------------------update_uncommon_branch------------------------
2010 // Update the probability of a two-branch to be uncommon
2011 void Block::update_uncommon_branch(Block* ub) {
2012   int eidx = end_idx();
2013   Node *n = get_node(eidx);  // Get ending Node
2014 
2015   int op = n-&gt;as_Mach()-&gt;ideal_Opcode();
2016 
2017   assert(op == Op_CountedLoopEnd || op == Op_If, &quot;must be a If&quot;);
2018   assert(num_fall_throughs() == 2, &quot;must be a two way branch block&quot;);
2019 
2020   // Which successor is ub?
2021   uint s;
2022   for (s = 0; s &lt;_num_succs; s++) {
2023     if (_succs[s] == ub) break;
2024   }
2025   assert(s &lt; 2, &quot;uncommon successor must be found&quot;);
2026 
2027   // If ub is the true path, make the proability small, else
2028   // ub is the false path, and make the probability large
2029   bool invert = (get_node(s + eidx + 1)-&gt;Opcode() == Op_IfFalse);
2030 
2031   // Get existing probability
2032   float p = n-&gt;as_MachIf()-&gt;_prob;
2033 
2034   if (invert) p = 1.0 - p;
2035   if (p &gt; PROB_MIN) {
2036     p = PROB_MIN;
2037   }
2038   if (invert) p = 1.0 - p;
2039 
2040   n-&gt;as_MachIf()-&gt;_prob = p;
2041 }
2042 
2043 //------------------------------update_succ_freq-------------------------------
2044 // Update the appropriate frequency associated with block &#39;b&#39;, a successor of
2045 // a block in this loop.
2046 void CFGLoop::update_succ_freq(Block* b, double freq) {
2047   if (b-&gt;_loop == this) {
2048     if (b == head()) {
2049       // back branch within the loop
2050       // Do nothing now, the loop carried frequency will be
2051       // adjust later in scale_freq().
2052     } else {
2053       // simple branch within the loop
2054       b-&gt;_freq += freq;
2055     }
2056   } else if (!in_loop_nest(b)) {
2057     // branch is exit from this loop
2058     BlockProbPair bpp(b, freq);
2059     _exits.append(bpp);
2060   } else {
2061     // branch into nested loop
2062     CFGLoop* ch = b-&gt;_loop;
2063     ch-&gt;_freq += freq;
2064   }
2065 }
2066 
2067 //------------------------------in_loop_nest-----------------------------------
2068 // Determine if block b is in the receiver&#39;s loop nest.
2069 bool CFGLoop::in_loop_nest(Block* b) {
2070   int depth = _depth;
2071   CFGLoop* b_loop = b-&gt;_loop;
2072   int b_depth = b_loop-&gt;_depth;
2073   if (depth == b_depth) {
2074     return true;
2075   }
2076   while (b_depth &gt; depth) {
2077     b_loop = b_loop-&gt;_parent;
2078     b_depth = b_loop-&gt;_depth;
2079   }
2080   return b_loop == this;
2081 }
2082 
2083 //------------------------------scale_freq-------------------------------------
2084 // Scale frequency of loops and blocks by trip counts from outer loops
2085 // Do a top down traversal of loop tree (visit outer loops first.)
2086 void CFGLoop::scale_freq() {
2087   double loop_freq = _freq * trip_count();
2088   _freq = loop_freq;
2089   for (int i = 0; i &lt; _members.length(); i++) {
2090     CFGElement* s = _members.at(i);
2091     double block_freq = s-&gt;_freq * loop_freq;
2092     if (g_isnan(block_freq) || block_freq &lt; MIN_BLOCK_FREQUENCY)
2093       block_freq = MIN_BLOCK_FREQUENCY;
2094     s-&gt;_freq = block_freq;
2095   }
2096   CFGLoop* ch = _child;
2097   while (ch != NULL) {
2098     ch-&gt;scale_freq();
2099     ch = ch-&gt;_sibling;
2100   }
2101 }
2102 
2103 // Frequency of outer loop
2104 double CFGLoop::outer_loop_freq() const {
2105   if (_child != NULL) {
2106     return _child-&gt;_freq;
2107   }
2108   return _freq;
2109 }
2110 
2111 #ifndef PRODUCT
2112 //------------------------------dump_tree--------------------------------------
2113 void CFGLoop::dump_tree() const {
2114   dump();
2115   if (_child != NULL)   _child-&gt;dump_tree();
2116   if (_sibling != NULL) _sibling-&gt;dump_tree();
2117 }
2118 
2119 //------------------------------dump-------------------------------------------
2120 void CFGLoop::dump() const {
2121   for (int i = 0; i &lt; _depth; i++) tty-&gt;print(&quot;   &quot;);
2122   tty-&gt;print(&quot;%s: %d  trip_count: %6.0f freq: %6.0f\n&quot;,
2123              _depth == 0 ? &quot;Method&quot; : &quot;Loop&quot;, _id, trip_count(), _freq);
2124   for (int i = 0; i &lt; _depth; i++) tty-&gt;print(&quot;   &quot;);
2125   tty-&gt;print(&quot;         members:&quot;);
2126   int k = 0;
2127   for (int i = 0; i &lt; _members.length(); i++) {
2128     if (k++ &gt;= 6) {
2129       tty-&gt;print(&quot;\n              &quot;);
2130       for (int j = 0; j &lt; _depth+1; j++) tty-&gt;print(&quot;   &quot;);
2131       k = 0;
2132     }
2133     CFGElement *s = _members.at(i);
2134     if (s-&gt;is_block()) {
2135       Block *b = s-&gt;as_Block();
2136       tty-&gt;print(&quot; B%d(%6.3f)&quot;, b-&gt;_pre_order, b-&gt;_freq);
2137     } else {
2138       CFGLoop* lp = s-&gt;as_CFGLoop();
2139       tty-&gt;print(&quot; L%d(%6.3f)&quot;, lp-&gt;_id, lp-&gt;_freq);
2140     }
2141   }
2142   tty-&gt;print(&quot;\n&quot;);
2143   for (int i = 0; i &lt; _depth; i++) tty-&gt;print(&quot;   &quot;);
2144   tty-&gt;print(&quot;         exits:  &quot;);
2145   k = 0;
2146   for (int i = 0; i &lt; _exits.length(); i++) {
2147     if (k++ &gt;= 7) {
2148       tty-&gt;print(&quot;\n              &quot;);
2149       for (int j = 0; j &lt; _depth+1; j++) tty-&gt;print(&quot;   &quot;);
2150       k = 0;
2151     }
2152     Block *blk = _exits.at(i).get_target();
2153     double prob = _exits.at(i).get_prob();
2154     tty-&gt;print(&quot; -&gt;%d@%d%%&quot;, blk-&gt;_pre_order, (int)(prob*100));
2155   }
2156   tty-&gt;print(&quot;\n&quot;);
2157 }
2158 #endif
<a name="13" id="anc13"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="13" type="hidden" />
</body>
</html>