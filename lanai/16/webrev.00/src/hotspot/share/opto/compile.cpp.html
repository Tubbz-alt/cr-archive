<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/share/opto/compile.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;ci/ciReplay.hpp&quot;
  29 #include &quot;classfile/systemDictionary.hpp&quot;
  30 #include &quot;code/exceptionHandlerTable.hpp&quot;
  31 #include &quot;code/nmethod.hpp&quot;
  32 #include &quot;compiler/compileBroker.hpp&quot;
  33 #include &quot;compiler/compileLog.hpp&quot;
  34 #include &quot;compiler/disassembler.hpp&quot;
  35 #include &quot;compiler/oopMap.hpp&quot;
  36 #include &quot;gc/shared/barrierSet.hpp&quot;
  37 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  38 #include &quot;memory/resourceArea.hpp&quot;
  39 #include &quot;opto/addnode.hpp&quot;
  40 #include &quot;opto/block.hpp&quot;
  41 #include &quot;opto/c2compiler.hpp&quot;
  42 #include &quot;opto/callGenerator.hpp&quot;
  43 #include &quot;opto/callnode.hpp&quot;
  44 #include &quot;opto/castnode.hpp&quot;
  45 #include &quot;opto/cfgnode.hpp&quot;
  46 #include &quot;opto/chaitin.hpp&quot;
  47 #include &quot;opto/compile.hpp&quot;
  48 #include &quot;opto/connode.hpp&quot;
  49 #include &quot;opto/convertnode.hpp&quot;
  50 #include &quot;opto/divnode.hpp&quot;
  51 #include &quot;opto/escape.hpp&quot;
  52 #include &quot;opto/idealGraphPrinter.hpp&quot;
  53 #include &quot;opto/loopnode.hpp&quot;
  54 #include &quot;opto/machnode.hpp&quot;
  55 #include &quot;opto/macro.hpp&quot;
  56 #include &quot;opto/matcher.hpp&quot;
  57 #include &quot;opto/mathexactnode.hpp&quot;
  58 #include &quot;opto/memnode.hpp&quot;
  59 #include &quot;opto/mulnode.hpp&quot;
  60 #include &quot;opto/narrowptrnode.hpp&quot;
  61 #include &quot;opto/node.hpp&quot;
  62 #include &quot;opto/opcodes.hpp&quot;
  63 #include &quot;opto/output.hpp&quot;
  64 #include &quot;opto/parse.hpp&quot;
  65 #include &quot;opto/phaseX.hpp&quot;
  66 #include &quot;opto/rootnode.hpp&quot;
  67 #include &quot;opto/runtime.hpp&quot;
  68 #include &quot;opto/stringopts.hpp&quot;
  69 #include &quot;opto/type.hpp&quot;
  70 #include &quot;opto/vectornode.hpp&quot;
  71 #include &quot;runtime/arguments.hpp&quot;
  72 #include &quot;runtime/sharedRuntime.hpp&quot;
  73 #include &quot;runtime/signature.hpp&quot;
  74 #include &quot;runtime/stubRoutines.hpp&quot;
  75 #include &quot;runtime/timer.hpp&quot;
  76 #include &quot;utilities/align.hpp&quot;
  77 #include &quot;utilities/copy.hpp&quot;
  78 #include &quot;utilities/macros.hpp&quot;
  79 
  80 
  81 // -------------------- Compile::mach_constant_base_node -----------------------
  82 // Constant table base node singleton.
  83 MachConstantBaseNode* Compile::mach_constant_base_node() {
  84   if (_mach_constant_base_node == NULL) {
  85     _mach_constant_base_node = new MachConstantBaseNode();
  86     _mach_constant_base_node-&gt;add_req(C-&gt;root());
  87   }
  88   return _mach_constant_base_node;
  89 }
  90 
  91 
  92 /// Support for intrinsics.
  93 
  94 // Return the index at which m must be inserted (or already exists).
  95 // The sort order is by the address of the ciMethod, with is_virtual as minor key.
  96 class IntrinsicDescPair {
  97  private:
  98   ciMethod* _m;
  99   bool _is_virtual;
 100  public:
 101   IntrinsicDescPair(ciMethod* m, bool is_virtual) : _m(m), _is_virtual(is_virtual) {}
 102   static int compare(IntrinsicDescPair* const&amp; key, CallGenerator* const&amp; elt) {
 103     ciMethod* m= elt-&gt;method();
 104     ciMethod* key_m = key-&gt;_m;
 105     if (key_m &lt; m)      return -1;
 106     else if (key_m &gt; m) return 1;
 107     else {
 108       bool is_virtual = elt-&gt;is_virtual();
 109       bool key_virtual = key-&gt;_is_virtual;
 110       if (key_virtual &lt; is_virtual)      return -1;
 111       else if (key_virtual &gt; is_virtual) return 1;
 112       else                               return 0;
 113     }
 114   }
 115 };
 116 int Compile::intrinsic_insertion_index(ciMethod* m, bool is_virtual, bool&amp; found) {
 117 #ifdef ASSERT
 118   for (int i = 1; i &lt; _intrinsics-&gt;length(); i++) {
 119     CallGenerator* cg1 = _intrinsics-&gt;at(i-1);
 120     CallGenerator* cg2 = _intrinsics-&gt;at(i);
 121     assert(cg1-&gt;method() != cg2-&gt;method()
 122            ? cg1-&gt;method()     &lt; cg2-&gt;method()
 123            : cg1-&gt;is_virtual() &lt; cg2-&gt;is_virtual(),
 124            &quot;compiler intrinsics list must stay sorted&quot;);
 125   }
 126 #endif
 127   IntrinsicDescPair pair(m, is_virtual);
 128   return _intrinsics-&gt;find_sorted&lt;IntrinsicDescPair*, IntrinsicDescPair::compare&gt;(&amp;pair, found);
 129 }
 130 
 131 void Compile::register_intrinsic(CallGenerator* cg) {
 132   if (_intrinsics == NULL) {
 133     _intrinsics = new (comp_arena())GrowableArray&lt;CallGenerator*&gt;(comp_arena(), 60, 0, NULL);
 134   }
 135   int len = _intrinsics-&gt;length();
 136   bool found = false;
 137   int index = intrinsic_insertion_index(cg-&gt;method(), cg-&gt;is_virtual(), found);
 138   assert(!found, &quot;registering twice&quot;);
 139   _intrinsics-&gt;insert_before(index, cg);
 140   assert(find_intrinsic(cg-&gt;method(), cg-&gt;is_virtual()) == cg, &quot;registration worked&quot;);
 141 }
 142 
 143 CallGenerator* Compile::find_intrinsic(ciMethod* m, bool is_virtual) {
 144   assert(m-&gt;is_loaded(), &quot;don&#39;t try this on unloaded methods&quot;);
 145   if (_intrinsics != NULL) {
 146     bool found = false;
 147     int index = intrinsic_insertion_index(m, is_virtual, found);
 148      if (found) {
 149       return _intrinsics-&gt;at(index);
 150     }
 151   }
 152   // Lazily create intrinsics for intrinsic IDs well-known in the runtime.
 153   if (m-&gt;intrinsic_id() != vmIntrinsics::_none &amp;&amp;
 154       m-&gt;intrinsic_id() &lt;= vmIntrinsics::LAST_COMPILER_INLINE) {
 155     CallGenerator* cg = make_vm_intrinsic(m, is_virtual);
 156     if (cg != NULL) {
 157       // Save it for next time:
 158       register_intrinsic(cg);
 159       return cg;
 160     } else {
 161       gather_intrinsic_statistics(m-&gt;intrinsic_id(), is_virtual, _intrinsic_disabled);
 162     }
 163   }
 164   return NULL;
 165 }
 166 
 167 // Compile:: register_library_intrinsics and make_vm_intrinsic are defined
 168 // in library_call.cpp.
 169 
 170 
 171 #ifndef PRODUCT
 172 // statistics gathering...
 173 
 174 juint  Compile::_intrinsic_hist_count[vmIntrinsics::ID_LIMIT] = {0};
 175 jubyte Compile::_intrinsic_hist_flags[vmIntrinsics::ID_LIMIT] = {0};
 176 
 177 bool Compile::gather_intrinsic_statistics(vmIntrinsics::ID id, bool is_virtual, int flags) {
 178   assert(id &gt; vmIntrinsics::_none &amp;&amp; id &lt; vmIntrinsics::ID_LIMIT, &quot;oob&quot;);
 179   int oflags = _intrinsic_hist_flags[id];
 180   assert(flags != 0, &quot;what happened?&quot;);
 181   if (is_virtual) {
 182     flags |= _intrinsic_virtual;
 183   }
 184   bool changed = (flags != oflags);
 185   if ((flags &amp; _intrinsic_worked) != 0) {
 186     juint count = (_intrinsic_hist_count[id] += 1);
 187     if (count == 1) {
 188       changed = true;           // first time
 189     }
 190     // increment the overall count also:
 191     _intrinsic_hist_count[vmIntrinsics::_none] += 1;
 192   }
 193   if (changed) {
 194     if (((oflags ^ flags) &amp; _intrinsic_virtual) != 0) {
 195       // Something changed about the intrinsic&#39;s virtuality.
 196       if ((flags &amp; _intrinsic_virtual) != 0) {
 197         // This is the first use of this intrinsic as a virtual call.
 198         if (oflags != 0) {
 199           // We already saw it as a non-virtual, so note both cases.
 200           flags |= _intrinsic_both;
 201         }
 202       } else if ((oflags &amp; _intrinsic_both) == 0) {
 203         // This is the first use of this intrinsic as a non-virtual
 204         flags |= _intrinsic_both;
 205       }
 206     }
 207     _intrinsic_hist_flags[id] = (jubyte) (oflags | flags);
 208   }
 209   // update the overall flags also:
 210   _intrinsic_hist_flags[vmIntrinsics::_none] |= (jubyte) flags;
 211   return changed;
 212 }
 213 
 214 static char* format_flags(int flags, char* buf) {
 215   buf[0] = 0;
 216   if ((flags &amp; Compile::_intrinsic_worked) != 0)    strcat(buf, &quot;,worked&quot;);
 217   if ((flags &amp; Compile::_intrinsic_failed) != 0)    strcat(buf, &quot;,failed&quot;);
 218   if ((flags &amp; Compile::_intrinsic_disabled) != 0)  strcat(buf, &quot;,disabled&quot;);
 219   if ((flags &amp; Compile::_intrinsic_virtual) != 0)   strcat(buf, &quot;,virtual&quot;);
 220   if ((flags &amp; Compile::_intrinsic_both) != 0)      strcat(buf, &quot;,nonvirtual&quot;);
 221   if (buf[0] == 0)  strcat(buf, &quot;,&quot;);
 222   assert(buf[0] == &#39;,&#39;, &quot;must be&quot;);
 223   return &amp;buf[1];
 224 }
 225 
 226 void Compile::print_intrinsic_statistics() {
 227   char flagsbuf[100];
 228   ttyLocker ttyl;
 229   if (xtty != NULL)  xtty-&gt;head(&quot;statistics type=&#39;intrinsic&#39;&quot;);
 230   tty-&gt;print_cr(&quot;Compiler intrinsic usage:&quot;);
 231   juint total = _intrinsic_hist_count[vmIntrinsics::_none];
 232   if (total == 0)  total = 1;  // avoid div0 in case of no successes
 233   #define PRINT_STAT_LINE(name, c, f) \
 234     tty-&gt;print_cr(&quot;  %4d (%4.1f%%) %s (%s)&quot;, (int)(c), ((c) * 100.0) / total, name, f);
 235   for (int index = 1 + (int)vmIntrinsics::_none; index &lt; (int)vmIntrinsics::ID_LIMIT; index++) {
 236     vmIntrinsics::ID id = (vmIntrinsics::ID) index;
 237     int   flags = _intrinsic_hist_flags[id];
 238     juint count = _intrinsic_hist_count[id];
 239     if ((flags | count) != 0) {
 240       PRINT_STAT_LINE(vmIntrinsics::name_at(id), count, format_flags(flags, flagsbuf));
 241     }
 242   }
 243   PRINT_STAT_LINE(&quot;total&quot;, total, format_flags(_intrinsic_hist_flags[vmIntrinsics::_none], flagsbuf));
 244   if (xtty != NULL)  xtty-&gt;tail(&quot;statistics&quot;);
 245 }
 246 
 247 void Compile::print_statistics() {
 248   { ttyLocker ttyl;
 249     if (xtty != NULL)  xtty-&gt;head(&quot;statistics type=&#39;opto&#39;&quot;);
 250     Parse::print_statistics();
 251     PhaseCCP::print_statistics();
 252     PhaseRegAlloc::print_statistics();
 253     PhaseOutput::print_statistics();
 254     PhasePeephole::print_statistics();
 255     PhaseIdealLoop::print_statistics();
 256     if (xtty != NULL)  xtty-&gt;tail(&quot;statistics&quot;);
 257   }
 258   if (_intrinsic_hist_flags[vmIntrinsics::_none] != 0) {
 259     // put this under its own &lt;statistics&gt; element.
 260     print_intrinsic_statistics();
 261   }
 262 }
 263 #endif //PRODUCT
 264 
 265 void Compile::gvn_replace_by(Node* n, Node* nn) {
 266   for (DUIterator_Last imin, i = n-&gt;last_outs(imin); i &gt;= imin; ) {
 267     Node* use = n-&gt;last_out(i);
 268     bool is_in_table = initial_gvn()-&gt;hash_delete(use);
 269     uint uses_found = 0;
 270     for (uint j = 0; j &lt; use-&gt;len(); j++) {
 271       if (use-&gt;in(j) == n) {
 272         if (j &lt; use-&gt;req())
 273           use-&gt;set_req(j, nn);
 274         else
 275           use-&gt;set_prec(j, nn);
 276         uses_found++;
 277       }
 278     }
 279     if (is_in_table) {
 280       // reinsert into table
 281       initial_gvn()-&gt;hash_find_insert(use);
 282     }
 283     record_for_igvn(use);
 284     i -= uses_found;    // we deleted 1 or more copies of this edge
 285   }
 286 }
 287 
 288 
 289 static inline bool not_a_node(const Node* n) {
 290   if (n == NULL)                   return true;
 291   if (((intptr_t)n &amp; 1) != 0)      return true;  // uninitialized, etc.
 292   if (*(address*)n == badAddress)  return true;  // kill by Node::destruct
 293   return false;
 294 }
 295 
 296 // Identify all nodes that are reachable from below, useful.
 297 // Use breadth-first pass that records state in a Unique_Node_List,
 298 // recursive traversal is slower.
 299 void Compile::identify_useful_nodes(Unique_Node_List &amp;useful) {
 300   int estimated_worklist_size = live_nodes();
 301   useful.map( estimated_worklist_size, NULL );  // preallocate space
 302 
 303   // Initialize worklist
 304   if (root() != NULL)     { useful.push(root()); }
 305   // If &#39;top&#39; is cached, declare it useful to preserve cached node
 306   if( cached_top_node() ) { useful.push(cached_top_node()); }
 307 
 308   // Push all useful nodes onto the list, breadthfirst
 309   for( uint next = 0; next &lt; useful.size(); ++next ) {
 310     assert( next &lt; unique(), &quot;Unique useful nodes &lt; total nodes&quot;);
 311     Node *n  = useful.at(next);
 312     uint max = n-&gt;len();
 313     for( uint i = 0; i &lt; max; ++i ) {
 314       Node *m = n-&gt;in(i);
 315       if (not_a_node(m))  continue;
 316       useful.push(m);
 317     }
 318   }
 319 }
 320 
 321 // Update dead_node_list with any missing dead nodes using useful
 322 // list. Consider all non-useful nodes to be useless i.e., dead nodes.
 323 void Compile::update_dead_node_list(Unique_Node_List &amp;useful) {
 324   uint max_idx = unique();
 325   VectorSet&amp; useful_node_set = useful.member_set();
 326 
 327   for (uint node_idx = 0; node_idx &lt; max_idx; node_idx++) {
 328     // If node with index node_idx is not in useful set,
 329     // mark it as dead in dead node list.
 330     if (!useful_node_set.test(node_idx)) {
 331       record_dead_node(node_idx);
 332     }
 333   }
 334 }
 335 
 336 void Compile::remove_useless_late_inlines(GrowableArray&lt;CallGenerator*&gt;* inlines, Unique_Node_List &amp;useful) {
 337   int shift = 0;
 338   for (int i = 0; i &lt; inlines-&gt;length(); i++) {
 339     CallGenerator* cg = inlines-&gt;at(i);
 340     CallNode* call = cg-&gt;call_node();
 341     if (shift &gt; 0) {
 342       inlines-&gt;at_put(i-shift, cg);
 343     }
 344     if (!useful.member(call)) {
 345       shift++;
 346     }
 347   }
 348   inlines-&gt;trunc_to(inlines-&gt;length()-shift);
 349 }
 350 
 351 // Disconnect all useless nodes by disconnecting those at the boundary.
 352 void Compile::remove_useless_nodes(Unique_Node_List &amp;useful) {
 353   uint next = 0;
 354   while (next &lt; useful.size()) {
 355     Node *n = useful.at(next++);
 356     if (n-&gt;is_SafePoint()) {
 357       // We&#39;re done with a parsing phase. Replaced nodes are not valid
 358       // beyond that point.
 359       n-&gt;as_SafePoint()-&gt;delete_replaced_nodes();
 360     }
 361     // Use raw traversal of out edges since this code removes out edges
 362     int max = n-&gt;outcnt();
 363     for (int j = 0; j &lt; max; ++j) {
 364       Node* child = n-&gt;raw_out(j);
 365       if (! useful.member(child)) {
 366         assert(!child-&gt;is_top() || child != top(),
 367                &quot;If top is cached in Compile object it is in useful list&quot;);
 368         // Only need to remove this out-edge to the useless node
 369         n-&gt;raw_del_out(j);
 370         --j;
 371         --max;
 372       }
 373     }
 374     if (n-&gt;outcnt() == 1 &amp;&amp; n-&gt;has_special_unique_user()) {
 375       record_for_igvn(n-&gt;unique_out());
 376     }
 377   }
 378   // Remove useless macro and predicate opaq nodes
 379   for (int i = C-&gt;macro_count()-1; i &gt;= 0; i--) {
 380     Node* n = C-&gt;macro_node(i);
 381     if (!useful.member(n)) {
 382       remove_macro_node(n);
 383     }
 384   }
 385   // Remove useless CastII nodes with range check dependency
 386   for (int i = range_check_cast_count() - 1; i &gt;= 0; i--) {
 387     Node* cast = range_check_cast_node(i);
 388     if (!useful.member(cast)) {
 389       remove_range_check_cast(cast);
 390     }
 391   }
 392   // Remove useless expensive nodes
 393   for (int i = C-&gt;expensive_count()-1; i &gt;= 0; i--) {
 394     Node* n = C-&gt;expensive_node(i);
 395     if (!useful.member(n)) {
 396       remove_expensive_node(n);
 397     }
 398   }
 399   // Remove useless Opaque4 nodes
 400   for (int i = opaque4_count() - 1; i &gt;= 0; i--) {
 401     Node* opaq = opaque4_node(i);
 402     if (!useful.member(opaq)) {
 403       remove_opaque4_node(opaq);
 404     }
 405   }
 406   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 407   bs-&gt;eliminate_useless_gc_barriers(useful, this);
 408   // clean up the late inline lists
 409   remove_useless_late_inlines(&amp;_string_late_inlines, useful);
 410   remove_useless_late_inlines(&amp;_boxing_late_inlines, useful);
 411   remove_useless_late_inlines(&amp;_late_inlines, useful);
 412   debug_only(verify_graph_edges(true/*check for no_dead_code*/);)
 413 }
 414 
 415 // ============================================================================
 416 //------------------------------CompileWrapper---------------------------------
 417 class CompileWrapper : public StackObj {
 418   Compile *const _compile;
 419  public:
 420   CompileWrapper(Compile* compile);
 421 
 422   ~CompileWrapper();
 423 };
 424 
 425 CompileWrapper::CompileWrapper(Compile* compile) : _compile(compile) {
 426   // the Compile* pointer is stored in the current ciEnv:
 427   ciEnv* env = compile-&gt;env();
 428   assert(env == ciEnv::current(), &quot;must already be a ciEnv active&quot;);
 429   assert(env-&gt;compiler_data() == NULL, &quot;compile already active?&quot;);
 430   env-&gt;set_compiler_data(compile);
 431   assert(compile == Compile::current(), &quot;sanity&quot;);
 432 
 433   compile-&gt;set_type_dict(NULL);
 434   compile-&gt;set_clone_map(new Dict(cmpkey, hashkey, _compile-&gt;comp_arena()));
 435   compile-&gt;clone_map().set_clone_idx(0);
 436   compile-&gt;set_type_last_size(0);
 437   compile-&gt;set_last_tf(NULL, NULL);
 438   compile-&gt;set_indexSet_arena(NULL);
 439   compile-&gt;set_indexSet_free_block_list(NULL);
 440   compile-&gt;init_type_arena();
 441   Type::Initialize(compile);
 442   _compile-&gt;begin_method();
 443   _compile-&gt;clone_map().set_debug(_compile-&gt;has_method() &amp;&amp; _compile-&gt;directive()-&gt;CloneMapDebugOption);
 444 }
 445 CompileWrapper::~CompileWrapper() {
 446   _compile-&gt;end_method();
 447   _compile-&gt;env()-&gt;set_compiler_data(NULL);
 448 }
 449 
 450 
 451 //----------------------------print_compile_messages---------------------------
 452 void Compile::print_compile_messages() {
 453 #ifndef PRODUCT
 454   // Check if recompiling
 455   if (_subsume_loads == false &amp;&amp; PrintOpto) {
 456     // Recompiling without allowing machine instructions to subsume loads
 457     tty-&gt;print_cr(&quot;*********************************************************&quot;);
 458     tty-&gt;print_cr(&quot;** Bailout: Recompile without subsuming loads          **&quot;);
 459     tty-&gt;print_cr(&quot;*********************************************************&quot;);
 460   }
 461   if (_do_escape_analysis != DoEscapeAnalysis &amp;&amp; PrintOpto) {
 462     // Recompiling without escape analysis
 463     tty-&gt;print_cr(&quot;*********************************************************&quot;);
 464     tty-&gt;print_cr(&quot;** Bailout: Recompile without escape analysis          **&quot;);
 465     tty-&gt;print_cr(&quot;*********************************************************&quot;);
 466   }
 467   if (_eliminate_boxing != EliminateAutoBox &amp;&amp; PrintOpto) {
 468     // Recompiling without boxing elimination
 469     tty-&gt;print_cr(&quot;*********************************************************&quot;);
 470     tty-&gt;print_cr(&quot;** Bailout: Recompile without boxing elimination       **&quot;);
 471     tty-&gt;print_cr(&quot;*********************************************************&quot;);
 472   }
 473   if (C-&gt;directive()-&gt;BreakAtCompileOption) {
 474     // Open the debugger when compiling this method.
 475     tty-&gt;print(&quot;### Breaking when compiling: &quot;);
 476     method()-&gt;print_short_name();
 477     tty-&gt;cr();
 478     BREAKPOINT;
 479   }
 480 
 481   if( PrintOpto ) {
 482     if (is_osr_compilation()) {
 483       tty-&gt;print(&quot;[OSR]%3d&quot;, _compile_id);
 484     } else {
 485       tty-&gt;print(&quot;%3d&quot;, _compile_id);
 486     }
 487   }
 488 #endif
 489 }
 490 
 491 // ============================================================================
 492 //------------------------------Compile standard-------------------------------
 493 debug_only( int Compile::_debug_idx = 100000; )
 494 
 495 // Compile a method.  entry_bci is -1 for normal compilations and indicates
 496 // the continuation bci for on stack replacement.
 497 
 498 
 499 Compile::Compile( ciEnv* ci_env, ciMethod* target, int osr_bci,
 500                   bool subsume_loads, bool do_escape_analysis, bool eliminate_boxing, DirectiveSet* directive)
 501                 : Phase(Compiler),
 502                   _compile_id(ci_env-&gt;compile_id()),
 503                   _save_argument_registers(false),
 504                   _subsume_loads(subsume_loads),
 505                   _do_escape_analysis(do_escape_analysis),
 506                   _eliminate_boxing(eliminate_boxing),
 507                   _method(target),
 508                   _entry_bci(osr_bci),
 509                   _stub_function(NULL),
 510                   _stub_name(NULL),
 511                   _stub_entry_point(NULL),
 512                   _max_node_limit(MaxNodeLimit),
 513                   _inlining_progress(false),
 514                   _inlining_incrementally(false),
 515                   _do_cleanup(false),
 516                   _has_reserved_stack_access(target-&gt;has_reserved_stack_access()),
 517 #ifndef PRODUCT
 518                   _trace_opto_output(directive-&gt;TraceOptoOutputOption),
 519                   _print_ideal(directive-&gt;PrintIdealOption),
 520 #endif
 521                   _has_method_handle_invokes(false),
 522                   _clinit_barrier_on_entry(false),
 523                   _comp_arena(mtCompiler),
 524                   _barrier_set_state(BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;create_barrier_state(comp_arena())),
 525                   _env(ci_env),
 526                   _directive(directive),
 527                   _log(ci_env-&gt;log()),
 528                   _failure_reason(NULL),
 529                   _congraph(NULL),
 530 #ifndef PRODUCT
 531                   _printer(IdealGraphPrinter::printer()),
 532 #endif
 533                   _dead_node_list(comp_arena()),
 534                   _dead_node_count(0),
 535                   _node_arena(mtCompiler),
 536                   _old_arena(mtCompiler),
 537                   _mach_constant_base_node(NULL),
 538                   _Compile_types(mtCompiler),
 539                   _initial_gvn(NULL),
 540                   _for_igvn(NULL),
 541                   _warm_calls(NULL),
 542                   _late_inlines(comp_arena(), 2, 0, NULL),
 543                   _string_late_inlines(comp_arena(), 2, 0, NULL),
 544                   _boxing_late_inlines(comp_arena(), 2, 0, NULL),
 545                   _late_inlines_pos(0),
 546                   _number_of_mh_late_inlines(0),
 547                   _print_inlining_stream(NULL),
 548                   _print_inlining_list(NULL),
 549                   _print_inlining_idx(0),
 550                   _print_inlining_output(NULL),
 551                   _replay_inline_data(NULL),
 552                   _java_calls(0),
 553                   _inner_loops(0),
 554                   _interpreter_frame_size(0)
 555 #ifndef PRODUCT
 556                   , _in_dump_cnt(0)
 557 #endif
 558 {
 559   C = this;
 560 #ifndef PRODUCT
 561   if (_printer != NULL) {
 562     _printer-&gt;set_compile(this);
 563   }
 564 #endif
 565   CompileWrapper cw(this);
 566 
 567   if (CITimeVerbose) {
 568     tty-&gt;print(&quot; &quot;);
 569     target-&gt;holder()-&gt;name()-&gt;print();
 570     tty-&gt;print(&quot;.&quot;);
 571     target-&gt;print_short_name();
 572     tty-&gt;print(&quot;  &quot;);
 573   }
 574   TraceTime t1(&quot;Total compilation time&quot;, &amp;_t_totalCompilation, CITime, CITimeVerbose);
 575   TraceTime t2(NULL, &amp;_t_methodCompilation, CITime, false);
 576 
 577 #if defined(SUPPORT_ASSEMBLY) || defined(SUPPORT_ABSTRACT_ASSEMBLY)
 578   bool print_opto_assembly = directive-&gt;PrintOptoAssemblyOption;
 579   // We can always print a disassembly, either abstract (hex dump) or
 580   // with the help of a suitable hsdis library. Thus, we should not
 581   // couple print_assembly and print_opto_assembly controls.
 582   // But: always print opto and regular assembly on compile command &#39;print&#39;.
 583   bool print_assembly = directive-&gt;PrintAssemblyOption;
 584   set_print_assembly(print_opto_assembly || print_assembly);
 585 #else
 586   set_print_assembly(false); // must initialize.
 587 #endif
 588 
 589 #ifndef PRODUCT
 590   set_parsed_irreducible_loop(false);
 591 
 592   if (directive-&gt;ReplayInlineOption) {
 593     _replay_inline_data = ciReplay::load_inline_data(method(), entry_bci(), ci_env-&gt;comp_level());
 594   }
 595 #endif
 596   set_print_inlining(directive-&gt;PrintInliningOption || PrintOptoInlining);
 597   set_print_intrinsics(directive-&gt;PrintIntrinsicsOption);
 598   set_has_irreducible_loop(true); // conservative until build_loop_tree() reset it
 599 
 600   if (ProfileTraps RTM_OPT_ONLY( || UseRTMLocking )) {
 601     // Make sure the method being compiled gets its own MDO,
 602     // so we can at least track the decompile_count().
 603     // Need MDO to record RTM code generation state.
 604     method()-&gt;ensure_method_data();
 605   }
 606 
 607   Init(::AliasLevel);
 608 
 609 
 610   print_compile_messages();
 611 
 612   _ilt = InlineTree::build_inline_tree_root();
 613 
 614   // Even if NO memory addresses are used, MergeMem nodes must have at least 1 slice
 615   assert(num_alias_types() &gt;= AliasIdxRaw, &quot;&quot;);
 616 
 617 #define MINIMUM_NODE_HASH  1023
 618   // Node list that Iterative GVN will start with
 619   Unique_Node_List for_igvn(comp_arena());
 620   set_for_igvn(&amp;for_igvn);
 621 
 622   // GVN that will be run immediately on new nodes
 623   uint estimated_size = method()-&gt;code_size()*4+64;
 624   estimated_size = (estimated_size &lt; MINIMUM_NODE_HASH ? MINIMUM_NODE_HASH : estimated_size);
 625   PhaseGVN gvn(node_arena(), estimated_size);
 626   set_initial_gvn(&amp;gvn);
 627 
 628   print_inlining_init();
 629   { // Scope for timing the parser
 630     TracePhase tp(&quot;parse&quot;, &amp;timers[_t_parser]);
 631 
 632     // Put top into the hash table ASAP.
 633     initial_gvn()-&gt;transform_no_reclaim(top());
 634 
 635     // Set up tf(), start(), and find a CallGenerator.
 636     CallGenerator* cg = NULL;
 637     if (is_osr_compilation()) {
 638       const TypeTuple *domain = StartOSRNode::osr_domain();
 639       const TypeTuple *range = TypeTuple::make_range(method()-&gt;signature());
 640       init_tf(TypeFunc::make(domain, range));
 641       StartNode* s = new StartOSRNode(root(), domain);
 642       initial_gvn()-&gt;set_type_bottom(s);
 643       init_start(s);
 644       cg = CallGenerator::for_osr(method(), entry_bci());
 645     } else {
 646       // Normal case.
 647       init_tf(TypeFunc::make(method()));
 648       StartNode* s = new StartNode(root(), tf()-&gt;domain());
 649       initial_gvn()-&gt;set_type_bottom(s);
 650       init_start(s);
 651       if (method()-&gt;intrinsic_id() == vmIntrinsics::_Reference_get) {
 652         // With java.lang.ref.reference.get() we must go through the
 653         // intrinsic - even when get() is the root
 654         // method of the compile - so that, if necessary, the value in
 655         // the referent field of the reference object gets recorded by
 656         // the pre-barrier code.
 657         cg = find_intrinsic(method(), false);
 658       }
 659       if (cg == NULL) {
 660         float past_uses = method()-&gt;interpreter_invocation_count();
 661         float expected_uses = past_uses;
 662         cg = CallGenerator::for_inline(method(), expected_uses);
 663       }
 664     }
 665     if (failing())  return;
 666     if (cg == NULL) {
 667       record_method_not_compilable(&quot;cannot parse method&quot;);
 668       return;
 669     }
 670     JVMState* jvms = build_start_state(start(), tf());
 671     if ((jvms = cg-&gt;generate(jvms)) == NULL) {
 672       if (!failure_reason_is(C2Compiler::retry_class_loading_during_parsing())) {
 673         record_method_not_compilable(&quot;method parse failed&quot;);
 674       }
 675       return;
 676     }
 677     GraphKit kit(jvms);
 678 
 679     if (!kit.stopped()) {
 680       // Accept return values, and transfer control we know not where.
 681       // This is done by a special, unique ReturnNode bound to root.
 682       return_values(kit.jvms());
 683     }
 684 
 685     if (kit.has_exceptions()) {
 686       // Any exceptions that escape from this call must be rethrown
 687       // to whatever caller is dynamically above us on the stack.
 688       // This is done by a special, unique RethrowNode bound to root.
 689       rethrow_exceptions(kit.transfer_exceptions_into_jvms());
 690     }
 691 
 692     assert(IncrementalInline || (_late_inlines.length() == 0 &amp;&amp; !has_mh_late_inlines()), &quot;incremental inlining is off&quot;);
 693 
 694     if (_late_inlines.length() == 0 &amp;&amp; !has_mh_late_inlines() &amp;&amp; !failing() &amp;&amp; has_stringbuilder()) {
 695       inline_string_calls(true);
 696     }
 697 
 698     if (failing())  return;
 699 
 700     print_method(PHASE_BEFORE_REMOVEUSELESS, 3);
 701 
 702     // Remove clutter produced by parsing.
 703     if (!failing()) {
 704       ResourceMark rm;
 705       PhaseRemoveUseless pru(initial_gvn(), &amp;for_igvn);
 706     }
 707   }
 708 
 709   // Note:  Large methods are capped off in do_one_bytecode().
 710   if (failing())  return;
 711 
 712   // After parsing, node notes are no longer automagic.
 713   // They must be propagated by register_new_node_with_optimizer(),
 714   // clone(), or the like.
 715   set_default_node_notes(NULL);
 716 
 717   for (;;) {
 718     int successes = Inline_Warm();
 719     if (failing())  return;
 720     if (successes == 0)  break;
 721   }
 722 
 723   // Drain the list.
 724   Finish_Warm();
 725 #ifndef PRODUCT
 726   if (_printer &amp;&amp; _printer-&gt;should_print(1)) {
 727     _printer-&gt;print_inlining();
 728   }
 729 #endif
 730 
 731   if (failing())  return;
 732   NOT_PRODUCT( verify_graph_edges(); )
 733 
 734   // Now optimize
 735   Optimize();
 736   if (failing())  return;
 737   NOT_PRODUCT( verify_graph_edges(); )
 738 
 739 #ifndef PRODUCT
 740   if (print_ideal()) {
 741     ttyLocker ttyl;  // keep the following output all in one block
 742     // This output goes directly to the tty, not the compiler log.
 743     // To enable tools to match it up with the compilation activity,
 744     // be sure to tag this tty output with the compile ID.
 745     if (xtty != NULL) {
 746       xtty-&gt;head(&quot;ideal compile_id=&#39;%d&#39;%s&quot;, compile_id(),
 747                  is_osr_compilation()    ? &quot; compile_kind=&#39;osr&#39;&quot; :
 748                  &quot;&quot;);
 749     }
 750     root()-&gt;dump(9999);
 751     if (xtty != NULL) {
 752       xtty-&gt;tail(&quot;ideal&quot;);
 753     }
 754   }
 755 #endif
 756 
 757 #ifdef ASSERT
 758   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 759   bs-&gt;verify_gc_barriers(this, BarrierSetC2::BeforeCodeGen);
 760 #endif
 761 
 762   // Dump compilation data to replay it.
 763   if (directive-&gt;DumpReplayOption) {
 764     env()-&gt;dump_replay_data(_compile_id);
 765   }
 766   if (directive-&gt;DumpInlineOption &amp;&amp; (ilt() != NULL)) {
 767     env()-&gt;dump_inline_data(_compile_id);
 768   }
 769 
 770   // Now that we know the size of all the monitors we can add a fixed slot
 771   // for the original deopt pc.
 772   int next_slot = fixed_slots() + (sizeof(address) / VMRegImpl::stack_slot_size);
 773   set_fixed_slots(next_slot);
 774 
 775   // Compute when to use implicit null checks. Used by matching trap based
 776   // nodes and NullCheck optimization.
 777   set_allowed_deopt_reasons();
 778 
 779   // Now generate code
 780   Code_Gen();
 781 }
 782 
 783 //------------------------------Compile----------------------------------------
 784 // Compile a runtime stub
 785 Compile::Compile( ciEnv* ci_env,
 786                   TypeFunc_generator generator,
 787                   address stub_function,
 788                   const char *stub_name,
 789                   int is_fancy_jump,
 790                   bool pass_tls,
 791                   bool save_arg_registers,
 792                   bool return_pc,
 793                   DirectiveSet* directive)
 794   : Phase(Compiler),
 795     _compile_id(0),
 796     _save_argument_registers(save_arg_registers),
 797     _subsume_loads(true),
 798     _do_escape_analysis(false),
 799     _eliminate_boxing(false),
 800     _method(NULL),
 801     _entry_bci(InvocationEntryBci),
 802     _stub_function(stub_function),
 803     _stub_name(stub_name),
 804     _stub_entry_point(NULL),
 805     _max_node_limit(MaxNodeLimit),
 806     _inlining_progress(false),
 807     _inlining_incrementally(false),
 808     _has_reserved_stack_access(false),
 809 #ifndef PRODUCT
 810     _trace_opto_output(directive-&gt;TraceOptoOutputOption),
 811     _print_ideal(directive-&gt;PrintIdealOption),
 812 #endif
 813     _has_method_handle_invokes(false),
 814     _clinit_barrier_on_entry(false),
 815     _comp_arena(mtCompiler),
 816     _barrier_set_state(BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;create_barrier_state(comp_arena())),
 817     _env(ci_env),
 818     _directive(directive),
 819     _log(ci_env-&gt;log()),
 820     _failure_reason(NULL),
 821     _congraph(NULL),
 822 #ifndef PRODUCT
 823     _printer(NULL),
 824 #endif
 825     _dead_node_list(comp_arena()),
 826     _dead_node_count(0),
 827     _node_arena(mtCompiler),
 828     _old_arena(mtCompiler),
 829     _mach_constant_base_node(NULL),
 830     _Compile_types(mtCompiler),
 831     _initial_gvn(NULL),
 832     _for_igvn(NULL),
 833     _warm_calls(NULL),
 834     _number_of_mh_late_inlines(0),
 835     _print_inlining_stream(NULL),
 836     _print_inlining_list(NULL),
 837     _print_inlining_idx(0),
 838     _print_inlining_output(NULL),
 839     _replay_inline_data(NULL),
 840     _java_calls(0),
 841     _inner_loops(0),
 842     _interpreter_frame_size(0),
 843 #ifndef PRODUCT
 844     _in_dump_cnt(0),
 845 #endif
 846     _allowed_reasons(0) {
 847   C = this;
 848 
 849   TraceTime t1(NULL, &amp;_t_totalCompilation, CITime, false);
 850   TraceTime t2(NULL, &amp;_t_stubCompilation, CITime, false);
 851 
 852 #ifndef PRODUCT
 853   set_print_assembly(PrintFrameConverterAssembly);
 854   set_parsed_irreducible_loop(false);
 855 #else
 856   set_print_assembly(false); // Must initialize.
 857 #endif
 858   set_has_irreducible_loop(false); // no loops
 859 
 860   CompileWrapper cw(this);
 861   Init(/*AliasLevel=*/ 0);
 862   init_tf((*generator)());
 863 
 864   {
 865     // The following is a dummy for the sake of GraphKit::gen_stub
 866     Unique_Node_List for_igvn(comp_arena());
 867     set_for_igvn(&amp;for_igvn);  // not used, but some GraphKit guys push on this
 868     PhaseGVN gvn(Thread::current()-&gt;resource_area(),255);
 869     set_initial_gvn(&amp;gvn);    // not significant, but GraphKit guys use it pervasively
 870     gvn.transform_no_reclaim(top());
 871 
 872     GraphKit kit;
 873     kit.gen_stub(stub_function, stub_name, is_fancy_jump, pass_tls, return_pc);
 874   }
 875 
 876   NOT_PRODUCT( verify_graph_edges(); )
 877 
 878   Code_Gen();
 879 }
 880 
 881 //------------------------------Init-------------------------------------------
 882 // Prepare for a single compilation
 883 void Compile::Init(int aliaslevel) {
 884   _unique  = 0;
 885   _regalloc = NULL;
 886 
 887   _tf      = NULL;  // filled in later
 888   _top     = NULL;  // cached later
 889   _matcher = NULL;  // filled in later
 890   _cfg     = NULL;  // filled in later
 891 
 892   IA32_ONLY( set_24_bit_selection_and_mode(true, false); )
 893 
 894   _node_note_array = NULL;
 895   _default_node_notes = NULL;
 896   DEBUG_ONLY( _modified_nodes = NULL; ) // Used in Optimize()
 897 
 898   _immutable_memory = NULL; // filled in at first inquiry
 899 
 900   // Globally visible Nodes
 901   // First set TOP to NULL to give safe behavior during creation of RootNode
 902   set_cached_top_node(NULL);
 903   set_root(new RootNode());
 904   // Now that you have a Root to point to, create the real TOP
 905   set_cached_top_node( new ConNode(Type::TOP) );
 906   set_recent_alloc(NULL, NULL);
 907 
 908   // Create Debug Information Recorder to record scopes, oopmaps, etc.
 909   env()-&gt;set_oop_recorder(new OopRecorder(env()-&gt;arena()));
 910   env()-&gt;set_debug_info(new DebugInformationRecorder(env()-&gt;oop_recorder()));
 911   env()-&gt;set_dependencies(new Dependencies(env()));
 912 
 913   _fixed_slots = 0;
 914   set_has_split_ifs(false);
 915   set_has_loops(has_method() &amp;&amp; method()-&gt;has_loops()); // first approximation
 916   set_has_stringbuilder(false);
 917   set_has_boxed_value(false);
 918   _trap_can_recompile = false;  // no traps emitted yet
 919   _major_progress = true; // start out assuming good things will happen
 920   set_has_unsafe_access(false);
 921   set_max_vector_size(0);
 922   set_clear_upper_avx(false);  //false as default for clear upper bits of ymm registers
 923   Copy::zero_to_bytes(_trap_hist, sizeof(_trap_hist));
 924   set_decompile_count(0);
 925 
 926   set_do_freq_based_layout(_directive-&gt;BlockLayoutByFrequencyOption);
 927   _loop_opts_cnt = LoopOptsCount;
 928   set_do_inlining(Inline);
 929   set_max_inline_size(MaxInlineSize);
 930   set_freq_inline_size(FreqInlineSize);
 931   set_do_scheduling(OptoScheduling);
 932   set_do_count_invocations(false);
 933   set_do_method_data_update(false);
 934 
 935   set_do_vector_loop(false);
 936 
 937   if (AllowVectorizeOnDemand) {
 938     if (has_method() &amp;&amp; (_directive-&gt;VectorizeOption || _directive-&gt;VectorizeDebugOption)) {
 939       set_do_vector_loop(true);
 940       NOT_PRODUCT(if (do_vector_loop() &amp;&amp; Verbose) {tty-&gt;print(&quot;Compile::Init: do vectorized loops (SIMD like) for method %s\n&quot;,  method()-&gt;name()-&gt;as_quoted_ascii());})
 941     } else if (has_method() &amp;&amp; method()-&gt;name() != 0 &amp;&amp;
 942                method()-&gt;intrinsic_id() == vmIntrinsics::_forEachRemaining) {
 943       set_do_vector_loop(true);
 944     }
 945   }
 946   set_use_cmove(UseCMoveUnconditionally /* || do_vector_loop()*/); //TODO: consider do_vector_loop() mandate use_cmove unconditionally
 947   NOT_PRODUCT(if (use_cmove() &amp;&amp; Verbose &amp;&amp; has_method()) {tty-&gt;print(&quot;Compile::Init: use CMove without profitability tests for method %s\n&quot;,  method()-&gt;name()-&gt;as_quoted_ascii());})
 948 
 949   set_age_code(has_method() &amp;&amp; method()-&gt;profile_aging());
 950   set_rtm_state(NoRTM); // No RTM lock eliding by default
 951   _max_node_limit = _directive-&gt;MaxNodeLimitOption;
 952 
 953 #if INCLUDE_RTM_OPT
 954   if (UseRTMLocking &amp;&amp; has_method() &amp;&amp; (method()-&gt;method_data_or_null() != NULL)) {
 955     int rtm_state = method()-&gt;method_data()-&gt;rtm_state();
 956     if (method_has_option(&quot;NoRTMLockEliding&quot;) || ((rtm_state &amp; NoRTM) != 0)) {
 957       // Don&#39;t generate RTM lock eliding code.
 958       set_rtm_state(NoRTM);
 959     } else if (method_has_option(&quot;UseRTMLockEliding&quot;) || ((rtm_state &amp; UseRTM) != 0) || !UseRTMDeopt) {
 960       // Generate RTM lock eliding code without abort ratio calculation code.
 961       set_rtm_state(UseRTM);
 962     } else if (UseRTMDeopt) {
 963       // Generate RTM lock eliding code and include abort ratio calculation
 964       // code if UseRTMDeopt is on.
 965       set_rtm_state(ProfileRTM);
 966     }
 967   }
 968 #endif
 969   if (VM_Version::supports_fast_class_init_checks() &amp;&amp; has_method() &amp;&amp; !is_osr_compilation() &amp;&amp; method()-&gt;needs_clinit_barrier()) {
 970     set_clinit_barrier_on_entry(true);
 971   }
 972   if (debug_info()-&gt;recording_non_safepoints()) {
 973     set_node_note_array(new(comp_arena()) GrowableArray&lt;Node_Notes*&gt;
 974                         (comp_arena(), 8, 0, NULL));
 975     set_default_node_notes(Node_Notes::make(this));
 976   }
 977 
 978   // // -- Initialize types before each compile --
 979   // // Update cached type information
 980   // if( _method &amp;&amp; _method-&gt;constants() )
 981   //   Type::update_loaded_types(_method, _method-&gt;constants());
 982 
 983   // Init alias_type map.
 984   if (!_do_escape_analysis &amp;&amp; aliaslevel == 3)
 985     aliaslevel = 2;  // No unique types without escape analysis
 986   _AliasLevel = aliaslevel;
 987   const int grow_ats = 16;
 988   _max_alias_types = grow_ats;
 989   _alias_types   = NEW_ARENA_ARRAY(comp_arena(), AliasType*, grow_ats);
 990   AliasType* ats = NEW_ARENA_ARRAY(comp_arena(), AliasType,  grow_ats);
 991   Copy::zero_to_bytes(ats, sizeof(AliasType)*grow_ats);
 992   {
 993     for (int i = 0; i &lt; grow_ats; i++)  _alias_types[i] = &amp;ats[i];
 994   }
 995   // Initialize the first few types.
 996   _alias_types[AliasIdxTop]-&gt;Init(AliasIdxTop, NULL);
 997   _alias_types[AliasIdxBot]-&gt;Init(AliasIdxBot, TypePtr::BOTTOM);
 998   _alias_types[AliasIdxRaw]-&gt;Init(AliasIdxRaw, TypeRawPtr::BOTTOM);
 999   _num_alias_types = AliasIdxRaw+1;
1000   // Zero out the alias type cache.
1001   Copy::zero_to_bytes(_alias_cache, sizeof(_alias_cache));
1002   // A NULL adr_type hits in the cache right away.  Preload the right answer.
1003   probe_alias_cache(NULL)-&gt;_index = AliasIdxTop;
1004 
1005   _intrinsics = NULL;
1006   _macro_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1007   _predicate_opaqs = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1008   _expensive_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1009   _range_check_casts = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1010   _opaque4_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1011   register_library_intrinsics();
1012 }
1013 
1014 //---------------------------init_start----------------------------------------
1015 // Install the StartNode on this compile object.
1016 void Compile::init_start(StartNode* s) {
1017   if (failing())
1018     return; // already failing
1019   assert(s == start(), &quot;&quot;);
1020 }
1021 
1022 /**
1023  * Return the &#39;StartNode&#39;. We must not have a pending failure, since the ideal graph
1024  * can be in an inconsistent state, i.e., we can get segmentation faults when traversing
1025  * the ideal graph.
1026  */
1027 StartNode* Compile::start() const {
1028   assert (!failing(), &quot;Must not have pending failure. Reason is: %s&quot;, failure_reason());
1029   for (DUIterator_Fast imax, i = root()-&gt;fast_outs(imax); i &lt; imax; i++) {
1030     Node* start = root()-&gt;fast_out(i);
1031     if (start-&gt;is_Start()) {
1032       return start-&gt;as_Start();
1033     }
1034   }
1035   fatal(&quot;Did not find Start node!&quot;);
1036   return NULL;
1037 }
1038 
1039 //-------------------------------immutable_memory-------------------------------------
1040 // Access immutable memory
1041 Node* Compile::immutable_memory() {
1042   if (_immutable_memory != NULL) {
1043     return _immutable_memory;
1044   }
1045   StartNode* s = start();
1046   for (DUIterator_Fast imax, i = s-&gt;fast_outs(imax); true; i++) {
1047     Node *p = s-&gt;fast_out(i);
1048     if (p != s &amp;&amp; p-&gt;as_Proj()-&gt;_con == TypeFunc::Memory) {
1049       _immutable_memory = p;
1050       return _immutable_memory;
1051     }
1052   }
1053   ShouldNotReachHere();
1054   return NULL;
1055 }
1056 
1057 //----------------------set_cached_top_node------------------------------------
1058 // Install the cached top node, and make sure Node::is_top works correctly.
1059 void Compile::set_cached_top_node(Node* tn) {
1060   if (tn != NULL)  verify_top(tn);
1061   Node* old_top = _top;
1062   _top = tn;
1063   // Calling Node::setup_is_top allows the nodes the chance to adjust
1064   // their _out arrays.
1065   if (_top != NULL)     _top-&gt;setup_is_top();
1066   if (old_top != NULL)  old_top-&gt;setup_is_top();
1067   assert(_top == NULL || top()-&gt;is_top(), &quot;&quot;);
1068 }
1069 
1070 #ifdef ASSERT
1071 uint Compile::count_live_nodes_by_graph_walk() {
1072   Unique_Node_List useful(comp_arena());
1073   // Get useful node list by walking the graph.
1074   identify_useful_nodes(useful);
1075   return useful.size();
1076 }
1077 
1078 void Compile::print_missing_nodes() {
1079 
1080   // Return if CompileLog is NULL and PrintIdealNodeCount is false.
1081   if ((_log == NULL) &amp;&amp; (! PrintIdealNodeCount)) {
1082     return;
1083   }
1084 
1085   // This is an expensive function. It is executed only when the user
1086   // specifies VerifyIdealNodeCount option or otherwise knows the
1087   // additional work that needs to be done to identify reachable nodes
1088   // by walking the flow graph and find the missing ones using
1089   // _dead_node_list.
1090 
1091   Unique_Node_List useful(comp_arena());
1092   // Get useful node list by walking the graph.
1093   identify_useful_nodes(useful);
1094 
1095   uint l_nodes = C-&gt;live_nodes();
1096   uint l_nodes_by_walk = useful.size();
1097 
1098   if (l_nodes != l_nodes_by_walk) {
1099     if (_log != NULL) {
1100       _log-&gt;begin_head(&quot;mismatched_nodes count=&#39;%d&#39;&quot;, abs((int) (l_nodes - l_nodes_by_walk)));
1101       _log-&gt;stamp();
1102       _log-&gt;end_head();
1103     }
1104     VectorSet&amp; useful_member_set = useful.member_set();
1105     int last_idx = l_nodes_by_walk;
1106     for (int i = 0; i &lt; last_idx; i++) {
1107       if (useful_member_set.test(i)) {
1108         if (_dead_node_list.test(i)) {
1109           if (_log != NULL) {
1110             _log-&gt;elem(&quot;mismatched_node_info node_idx=&#39;%d&#39; type=&#39;both live and dead&#39;&quot;, i);
1111           }
1112           if (PrintIdealNodeCount) {
1113             // Print the log message to tty
1114               tty-&gt;print_cr(&quot;mismatched_node idx=&#39;%d&#39; both live and dead&#39;&quot;, i);
1115               useful.at(i)-&gt;dump();
1116           }
1117         }
1118       }
1119       else if (! _dead_node_list.test(i)) {
1120         if (_log != NULL) {
1121           _log-&gt;elem(&quot;mismatched_node_info node_idx=&#39;%d&#39; type=&#39;neither live nor dead&#39;&quot;, i);
1122         }
1123         if (PrintIdealNodeCount) {
1124           // Print the log message to tty
1125           tty-&gt;print_cr(&quot;mismatched_node idx=&#39;%d&#39; type=&#39;neither live nor dead&#39;&quot;, i);
1126         }
1127       }
1128     }
1129     if (_log != NULL) {
1130       _log-&gt;tail(&quot;mismatched_nodes&quot;);
1131     }
1132   }
1133 }
1134 void Compile::record_modified_node(Node* n) {
1135   if (_modified_nodes != NULL &amp;&amp; !_inlining_incrementally &amp;&amp;
1136       n-&gt;outcnt() != 0 &amp;&amp; !n-&gt;is_Con()) {
1137     _modified_nodes-&gt;push(n);
1138   }
1139 }
1140 
1141 void Compile::remove_modified_node(Node* n) {
1142   if (_modified_nodes != NULL) {
1143     _modified_nodes-&gt;remove(n);
1144   }
1145 }
1146 #endif
1147 
1148 #ifndef PRODUCT
1149 void Compile::verify_top(Node* tn) const {
1150   if (tn != NULL) {
1151     assert(tn-&gt;is_Con(), &quot;top node must be a constant&quot;);
1152     assert(((ConNode*)tn)-&gt;type() == Type::TOP, &quot;top node must have correct type&quot;);
1153     assert(tn-&gt;in(0) != NULL, &quot;must have live top node&quot;);
1154   }
1155 }
1156 #endif
1157 
1158 
1159 ///-------------------Managing Per-Node Debug &amp; Profile Info-------------------
1160 
1161 void Compile::grow_node_notes(GrowableArray&lt;Node_Notes*&gt;* arr, int grow_by) {
1162   guarantee(arr != NULL, &quot;&quot;);
1163   int num_blocks = arr-&gt;length();
1164   if (grow_by &lt; num_blocks)  grow_by = num_blocks;
1165   int num_notes = grow_by * _node_notes_block_size;
1166   Node_Notes* notes = NEW_ARENA_ARRAY(node_arena(), Node_Notes, num_notes);
1167   Copy::zero_to_bytes(notes, num_notes * sizeof(Node_Notes));
1168   while (num_notes &gt; 0) {
1169     arr-&gt;append(notes);
1170     notes     += _node_notes_block_size;
1171     num_notes -= _node_notes_block_size;
1172   }
1173   assert(num_notes == 0, &quot;exact multiple, please&quot;);
1174 }
1175 
1176 bool Compile::copy_node_notes_to(Node* dest, Node* source) {
1177   if (source == NULL || dest == NULL)  return false;
1178 
1179   if (dest-&gt;is_Con())
1180     return false;               // Do not push debug info onto constants.
1181 
1182 #ifdef ASSERT
1183   // Leave a bread crumb trail pointing to the original node:
1184   if (dest != NULL &amp;&amp; dest != source &amp;&amp; dest-&gt;debug_orig() == NULL) {
1185     dest-&gt;set_debug_orig(source);
1186   }
1187 #endif
1188 
1189   if (node_note_array() == NULL)
1190     return false;               // Not collecting any notes now.
1191 
1192   // This is a copy onto a pre-existing node, which may already have notes.
1193   // If both nodes have notes, do not overwrite any pre-existing notes.
1194   Node_Notes* source_notes = node_notes_at(source-&gt;_idx);
1195   if (source_notes == NULL || source_notes-&gt;is_clear())  return false;
1196   Node_Notes* dest_notes   = node_notes_at(dest-&gt;_idx);
1197   if (dest_notes == NULL || dest_notes-&gt;is_clear()) {
1198     return set_node_notes_at(dest-&gt;_idx, source_notes);
1199   }
1200 
1201   Node_Notes merged_notes = (*source_notes);
1202   // The order of operations here ensures that dest notes will win...
1203   merged_notes.update_from(dest_notes);
1204   return set_node_notes_at(dest-&gt;_idx, &amp;merged_notes);
1205 }
1206 
1207 
1208 //--------------------------allow_range_check_smearing-------------------------
1209 // Gating condition for coalescing similar range checks.
1210 // Sometimes we try &#39;speculatively&#39; replacing a series of a range checks by a
1211 // single covering check that is at least as strong as any of them.
1212 // If the optimization succeeds, the simplified (strengthened) range check
1213 // will always succeed.  If it fails, we will deopt, and then give up
1214 // on the optimization.
1215 bool Compile::allow_range_check_smearing() const {
1216   // If this method has already thrown a range-check,
1217   // assume it was because we already tried range smearing
1218   // and it failed.
1219   uint already_trapped = trap_count(Deoptimization::Reason_range_check);
1220   return !already_trapped;
1221 }
1222 
1223 
1224 //------------------------------flatten_alias_type-----------------------------
1225 const TypePtr *Compile::flatten_alias_type( const TypePtr *tj ) const {
1226   int offset = tj-&gt;offset();
1227   TypePtr::PTR ptr = tj-&gt;ptr();
1228 
1229   // Known instance (scalarizable allocation) alias only with itself.
1230   bool is_known_inst = tj-&gt;isa_oopptr() != NULL &amp;&amp;
1231                        tj-&gt;is_oopptr()-&gt;is_known_instance();
1232 
1233   // Process weird unsafe references.
1234   if (offset == Type::OffsetBot &amp;&amp; (tj-&gt;isa_instptr() /*|| tj-&gt;isa_klassptr()*/)) {
1235     assert(InlineUnsafeOps, &quot;indeterminate pointers come only from unsafe ops&quot;);
1236     assert(!is_known_inst, &quot;scalarizable allocation should not have unsafe references&quot;);
1237     tj = TypeOopPtr::BOTTOM;
1238     ptr = tj-&gt;ptr();
1239     offset = tj-&gt;offset();
1240   }
1241 
1242   // Array pointers need some flattening
1243   const TypeAryPtr *ta = tj-&gt;isa_aryptr();
1244   if (ta &amp;&amp; ta-&gt;is_stable()) {
1245     // Erase stability property for alias analysis.
1246     tj = ta = ta-&gt;cast_to_stable(false);
1247   }
1248   if( ta &amp;&amp; is_known_inst ) {
1249     if ( offset != Type::OffsetBot &amp;&amp;
1250          offset &gt; arrayOopDesc::length_offset_in_bytes() ) {
1251       offset = Type::OffsetBot; // Flatten constant access into array body only
1252       tj = ta = TypeAryPtr::make(ptr, ta-&gt;ary(), ta-&gt;klass(), true, offset, ta-&gt;instance_id());
1253     }
1254   } else if( ta &amp;&amp; _AliasLevel &gt;= 2 ) {
1255     // For arrays indexed by constant indices, we flatten the alias
1256     // space to include all of the array body.  Only the header, klass
1257     // and array length can be accessed un-aliased.
1258     if( offset != Type::OffsetBot ) {
1259       if( ta-&gt;const_oop() ) { // MethodData* or Method*
1260         offset = Type::OffsetBot;   // Flatten constant access into array body
1261         tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),ta-&gt;ary(),ta-&gt;klass(),false,offset);
1262       } else if( offset == arrayOopDesc::length_offset_in_bytes() ) {
1263         // range is OK as-is.
1264         tj = ta = TypeAryPtr::RANGE;
1265       } else if( offset == oopDesc::klass_offset_in_bytes() ) {
1266         tj = TypeInstPtr::KLASS; // all klass loads look alike
1267         ta = TypeAryPtr::RANGE; // generic ignored junk
1268         ptr = TypePtr::BotPTR;
1269       } else if( offset == oopDesc::mark_offset_in_bytes() ) {
1270         tj = TypeInstPtr::MARK;
1271         ta = TypeAryPtr::RANGE; // generic ignored junk
1272         ptr = TypePtr::BotPTR;
1273       } else {                  // Random constant offset into array body
1274         offset = Type::OffsetBot;   // Flatten constant access into array body
1275         tj = ta = TypeAryPtr::make(ptr,ta-&gt;ary(),ta-&gt;klass(),false,offset);
1276       }
1277     }
1278     // Arrays of fixed size alias with arrays of unknown size.
1279     if (ta-&gt;size() != TypeInt::POS) {
1280       const TypeAry *tary = TypeAry::make(ta-&gt;elem(), TypeInt::POS);
1281       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,ta-&gt;klass(),false,offset);
1282     }
1283     // Arrays of known objects become arrays of unknown objects.
1284     if (ta-&gt;elem()-&gt;isa_narrowoop() &amp;&amp; ta-&gt;elem() != TypeNarrowOop::BOTTOM) {
1285       const TypeAry *tary = TypeAry::make(TypeNarrowOop::BOTTOM, ta-&gt;size());
1286       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,offset);
1287     }
1288     if (ta-&gt;elem()-&gt;isa_oopptr() &amp;&amp; ta-&gt;elem() != TypeInstPtr::BOTTOM) {
1289       const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta-&gt;size());
1290       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,offset);
1291     }
1292     // Arrays of bytes and of booleans both use &#39;bastore&#39; and &#39;baload&#39; so
1293     // cannot be distinguished by bytecode alone.
1294     if (ta-&gt;elem() == TypeInt::BOOL) {
1295       const TypeAry *tary = TypeAry::make(TypeInt::BYTE, ta-&gt;size());
1296       ciKlass* aklass = ciTypeArrayKlass::make(T_BYTE);
1297       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,aklass,false,offset);
1298     }
1299     // During the 2nd round of IterGVN, NotNull castings are removed.
1300     // Make sure the Bottom and NotNull variants alias the same.
1301     // Also, make sure exact and non-exact variants alias the same.
1302     if (ptr == TypePtr::NotNull || ta-&gt;klass_is_exact() || ta-&gt;speculative() != NULL) {
1303       tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta-&gt;ary(),ta-&gt;klass(),false,offset);
1304     }
1305   }
1306 
1307   // Oop pointers need some flattening
1308   const TypeInstPtr *to = tj-&gt;isa_instptr();
1309   if( to &amp;&amp; _AliasLevel &gt;= 2 &amp;&amp; to != TypeOopPtr::BOTTOM ) {
1310     ciInstanceKlass *k = to-&gt;klass()-&gt;as_instance_klass();
1311     if( ptr == TypePtr::Constant ) {
1312       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass() ||
1313           offset &lt; k-&gt;size_helper() * wordSize) {
1314         // No constant oop pointers (such as Strings); they alias with
1315         // unknown strings.
1316         assert(!is_known_inst, &quot;not scalarizable allocation&quot;);
1317         tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,offset);
1318       }
1319     } else if( is_known_inst ) {
1320       tj = to; // Keep NotNull and klass_is_exact for instance type
1321     } else if( ptr == TypePtr::NotNull || to-&gt;klass_is_exact() ) {
1322       // During the 2nd round of IterGVN, NotNull castings are removed.
1323       // Make sure the Bottom and NotNull variants alias the same.
1324       // Also, make sure exact and non-exact variants alias the same.
1325       tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,offset);
1326     }
1327     if (to-&gt;speculative() != NULL) {
1328       tj = to = TypeInstPtr::make(to-&gt;ptr(),to-&gt;klass(),to-&gt;klass_is_exact(),to-&gt;const_oop(),to-&gt;offset(), to-&gt;instance_id());
1329     }
1330     // Canonicalize the holder of this field
1331     if (offset &gt;= 0 &amp;&amp; offset &lt; instanceOopDesc::base_offset_in_bytes()) {
1332       // First handle header references such as a LoadKlassNode, even if the
1333       // object&#39;s klass is unloaded at compile time (4965979).
1334       if (!is_known_inst) { // Do it only for non-instance types
1335         tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()-&gt;Object_klass(), false, NULL, offset);
1336       }
1337     } else if (offset &lt; 0 || offset &gt;= k-&gt;size_helper() * wordSize) {
1338       // Static fields are in the space above the normal instance
1339       // fields in the java.lang.Class instance.
1340       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass()) {
1341         to = NULL;
1342         tj = TypeOopPtr::BOTTOM;
1343         offset = tj-&gt;offset();
1344       }
1345     } else {
1346       ciInstanceKlass *canonical_holder = k-&gt;get_canonical_holder(offset);
1347       if (!k-&gt;equals(canonical_holder) || tj-&gt;offset() != offset) {
1348         if( is_known_inst ) {
1349           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, true, NULL, offset, to-&gt;instance_id());
1350         } else {
1351           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, false, NULL, offset);
1352         }
1353       }
1354     }
1355   }
1356 
1357   // Klass pointers to object array klasses need some flattening
1358   const TypeKlassPtr *tk = tj-&gt;isa_klassptr();
1359   if( tk ) {
1360     // If we are referencing a field within a Klass, we need
1361     // to assume the worst case of an Object.  Both exact and
1362     // inexact types must flatten to the same alias class so
1363     // use NotNull as the PTR.
1364     if ( offset == Type::OffsetBot || (offset &gt;= 0 &amp;&amp; (size_t)offset &lt; sizeof(Klass)) ) {
1365 
1366       tj = tk = TypeKlassPtr::make(TypePtr::NotNull,
1367                                    TypeKlassPtr::OBJECT-&gt;klass(),
1368                                    offset);
1369     }
1370 
1371     ciKlass* klass = tk-&gt;klass();
1372     if( klass-&gt;is_obj_array_klass() ) {
1373       ciKlass* k = TypeAryPtr::OOPS-&gt;klass();
1374       if( !k || !k-&gt;is_loaded() )                  // Only fails for some -Xcomp runs
1375         k = TypeInstPtr::BOTTOM-&gt;klass();
1376       tj = tk = TypeKlassPtr::make( TypePtr::NotNull, k, offset );
1377     }
1378 
1379     // Check for precise loads from the primary supertype array and force them
1380     // to the supertype cache alias index.  Check for generic array loads from
1381     // the primary supertype array and also force them to the supertype cache
1382     // alias index.  Since the same load can reach both, we need to merge
1383     // these 2 disparate memories into the same alias class.  Since the
1384     // primary supertype array is read-only, there&#39;s no chance of confusion
1385     // where we bypass an array load and an array store.
1386     int primary_supers_offset = in_bytes(Klass::primary_supers_offset());
1387     if (offset == Type::OffsetBot ||
1388         (offset &gt;= primary_supers_offset &amp;&amp;
1389          offset &lt; (int)(primary_supers_offset + Klass::primary_super_limit() * wordSize)) ||
1390         offset == (int)in_bytes(Klass::secondary_super_cache_offset())) {
1391       offset = in_bytes(Klass::secondary_super_cache_offset());
1392       tj = tk = TypeKlassPtr::make( TypePtr::NotNull, tk-&gt;klass(), offset );
1393     }
1394   }
1395 
1396   // Flatten all Raw pointers together.
1397   if (tj-&gt;base() == Type::RawPtr)
1398     tj = TypeRawPtr::BOTTOM;
1399 
1400   if (tj-&gt;base() == Type::AnyPtr)
1401     tj = TypePtr::BOTTOM;      // An error, which the caller must check for.
1402 
1403   // Flatten all to bottom for now
1404   switch( _AliasLevel ) {
1405   case 0:
1406     tj = TypePtr::BOTTOM;
1407     break;
1408   case 1:                       // Flatten to: oop, static, field or array
1409     switch (tj-&gt;base()) {
1410     //case Type::AryPtr: tj = TypeAryPtr::RANGE;    break;
1411     case Type::RawPtr:   tj = TypeRawPtr::BOTTOM;   break;
1412     case Type::AryPtr:   // do not distinguish arrays at all
1413     case Type::InstPtr:  tj = TypeInstPtr::BOTTOM;  break;
1414     case Type::KlassPtr: tj = TypeKlassPtr::OBJECT; break;
1415     case Type::AnyPtr:   tj = TypePtr::BOTTOM;      break;  // caller checks it
1416     default: ShouldNotReachHere();
1417     }
1418     break;
1419   case 2:                       // No collapsing at level 2; keep all splits
1420   case 3:                       // No collapsing at level 3; keep all splits
1421     break;
1422   default:
1423     Unimplemented();
1424   }
1425 
1426   offset = tj-&gt;offset();
1427   assert( offset != Type::OffsetTop, &quot;Offset has fallen from constant&quot; );
1428 
1429   assert( (offset != Type::OffsetBot &amp;&amp; tj-&gt;base() != Type::AryPtr) ||
1430           (offset == Type::OffsetBot &amp;&amp; tj-&gt;base() == Type::AryPtr) ||
1431           (offset == Type::OffsetBot &amp;&amp; tj == TypeOopPtr::BOTTOM) ||
1432           (offset == Type::OffsetBot &amp;&amp; tj == TypePtr::BOTTOM) ||
1433           (offset == oopDesc::mark_offset_in_bytes() &amp;&amp; tj-&gt;base() == Type::AryPtr) ||
1434           (offset == oopDesc::klass_offset_in_bytes() &amp;&amp; tj-&gt;base() == Type::AryPtr) ||
1435           (offset == arrayOopDesc::length_offset_in_bytes() &amp;&amp; tj-&gt;base() == Type::AryPtr),
1436           &quot;For oops, klasses, raw offset must be constant; for arrays the offset is never known&quot; );
1437   assert( tj-&gt;ptr() != TypePtr::TopPTR &amp;&amp;
1438           tj-&gt;ptr() != TypePtr::AnyNull &amp;&amp;
1439           tj-&gt;ptr() != TypePtr::Null, &quot;No imprecise addresses&quot; );
1440 //    assert( tj-&gt;ptr() != TypePtr::Constant ||
1441 //            tj-&gt;base() == Type::RawPtr ||
1442 //            tj-&gt;base() == Type::KlassPtr, &quot;No constant oop addresses&quot; );
1443 
1444   return tj;
1445 }
1446 
1447 void Compile::AliasType::Init(int i, const TypePtr* at) {
1448   assert(AliasIdxTop &lt;= i &amp;&amp; i &lt; Compile::current()-&gt;_max_alias_types, &quot;Invalid alias index&quot;);
1449   _index = i;
1450   _adr_type = at;
1451   _field = NULL;
1452   _element = NULL;
1453   _is_rewritable = true; // default
1454   const TypeOopPtr *atoop = (at != NULL) ? at-&gt;isa_oopptr() : NULL;
1455   if (atoop != NULL &amp;&amp; atoop-&gt;is_known_instance()) {
1456     const TypeOopPtr *gt = atoop-&gt;cast_to_instance_id(TypeOopPtr::InstanceBot);
1457     _general_index = Compile::current()-&gt;get_alias_index(gt);
1458   } else {
1459     _general_index = 0;
1460   }
1461 }
1462 
1463 BasicType Compile::AliasType::basic_type() const {
1464   if (element() != NULL) {
1465     const Type* element = adr_type()-&gt;is_aryptr()-&gt;elem();
1466     return element-&gt;isa_narrowoop() ? T_OBJECT : element-&gt;array_element_basic_type();
1467   } if (field() != NULL) {
1468     return field()-&gt;layout_type();
1469   } else {
1470     return T_ILLEGAL; // unknown
1471   }
1472 }
1473 
1474 //---------------------------------print_on------------------------------------
1475 #ifndef PRODUCT
1476 void Compile::AliasType::print_on(outputStream* st) {
1477   if (index() &lt; 10)
1478         st-&gt;print(&quot;@ &lt;%d&gt; &quot;, index());
1479   else  st-&gt;print(&quot;@ &lt;%d&gt;&quot;,  index());
1480   st-&gt;print(is_rewritable() ? &quot;   &quot; : &quot; RO&quot;);
1481   int offset = adr_type()-&gt;offset();
1482   if (offset == Type::OffsetBot)
1483         st-&gt;print(&quot; +any&quot;);
1484   else  st-&gt;print(&quot; +%-3d&quot;, offset);
1485   st-&gt;print(&quot; in &quot;);
1486   adr_type()-&gt;dump_on(st);
1487   const TypeOopPtr* tjp = adr_type()-&gt;isa_oopptr();
1488   if (field() != NULL &amp;&amp; tjp) {
1489     if (tjp-&gt;klass()  != field()-&gt;holder() ||
1490         tjp-&gt;offset() != field()-&gt;offset_in_bytes()) {
1491       st-&gt;print(&quot; != &quot;);
1492       field()-&gt;print();
1493       st-&gt;print(&quot; ***&quot;);
1494     }
1495   }
1496 }
1497 
1498 void print_alias_types() {
1499   Compile* C = Compile::current();
1500   tty-&gt;print_cr(&quot;--- Alias types, AliasIdxBot .. %d&quot;, C-&gt;num_alias_types()-1);
1501   for (int idx = Compile::AliasIdxBot; idx &lt; C-&gt;num_alias_types(); idx++) {
1502     C-&gt;alias_type(idx)-&gt;print_on(tty);
1503     tty-&gt;cr();
1504   }
1505 }
1506 #endif
1507 
1508 
1509 //----------------------------probe_alias_cache--------------------------------
1510 Compile::AliasCacheEntry* Compile::probe_alias_cache(const TypePtr* adr_type) {
1511   intptr_t key = (intptr_t) adr_type;
1512   key ^= key &gt;&gt; logAliasCacheSize;
1513   return &amp;_alias_cache[key &amp; right_n_bits(logAliasCacheSize)];
1514 }
1515 
1516 
1517 //-----------------------------grow_alias_types--------------------------------
1518 void Compile::grow_alias_types() {
1519   const int old_ats  = _max_alias_types; // how many before?
1520   const int new_ats  = old_ats;          // how many more?
1521   const int grow_ats = old_ats+new_ats;  // how many now?
1522   _max_alias_types = grow_ats;
1523   _alias_types =  REALLOC_ARENA_ARRAY(comp_arena(), AliasType*, _alias_types, old_ats, grow_ats);
1524   AliasType* ats =    NEW_ARENA_ARRAY(comp_arena(), AliasType, new_ats);
1525   Copy::zero_to_bytes(ats, sizeof(AliasType)*new_ats);
1526   for (int i = 0; i &lt; new_ats; i++)  _alias_types[old_ats+i] = &amp;ats[i];
1527 }
1528 
1529 
1530 //--------------------------------find_alias_type------------------------------
1531 Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {
1532   if (_AliasLevel == 0)
1533     return alias_type(AliasIdxBot);
1534 
1535   AliasCacheEntry* ace = probe_alias_cache(adr_type);
1536   if (ace-&gt;_adr_type == adr_type) {
1537     return alias_type(ace-&gt;_index);
1538   }
1539 
1540   // Handle special cases.
1541   if (adr_type == NULL)             return alias_type(AliasIdxTop);
1542   if (adr_type == TypePtr::BOTTOM)  return alias_type(AliasIdxBot);
1543 
1544   // Do it the slow way.
1545   const TypePtr* flat = flatten_alias_type(adr_type);
1546 
1547 #ifdef ASSERT
1548   {
1549     ResourceMark rm;
1550     assert(flat == flatten_alias_type(flat), &quot;not idempotent: adr_type = %s; flat = %s =&gt; %s&quot;,
1551            Type::str(adr_type), Type::str(flat), Type::str(flatten_alias_type(flat)));
1552     assert(flat != TypePtr::BOTTOM, &quot;cannot alias-analyze an untyped ptr: adr_type = %s&quot;,
1553            Type::str(adr_type));
1554     if (flat-&gt;isa_oopptr() &amp;&amp; !flat-&gt;isa_klassptr()) {
1555       const TypeOopPtr* foop = flat-&gt;is_oopptr();
1556       // Scalarizable allocations have exact klass always.
1557       bool exact = !foop-&gt;klass_is_exact() || foop-&gt;is_known_instance();
1558       const TypePtr* xoop = foop-&gt;cast_to_exactness(exact)-&gt;is_ptr();
1559       assert(foop == flatten_alias_type(xoop), &quot;exactness must not affect alias type: foop = %s; xoop = %s&quot;,
1560              Type::str(foop), Type::str(xoop));
1561     }
1562   }
1563 #endif
1564 
1565   int idx = AliasIdxTop;
1566   for (int i = 0; i &lt; num_alias_types(); i++) {
1567     if (alias_type(i)-&gt;adr_type() == flat) {
1568       idx = i;
1569       break;
1570     }
1571   }
1572 
1573   if (idx == AliasIdxTop) {
1574     if (no_create)  return NULL;
1575     // Grow the array if necessary.
1576     if (_num_alias_types == _max_alias_types)  grow_alias_types();
1577     // Add a new alias type.
1578     idx = _num_alias_types++;
1579     _alias_types[idx]-&gt;Init(idx, flat);
1580     if (flat == TypeInstPtr::KLASS)  alias_type(idx)-&gt;set_rewritable(false);
1581     if (flat == TypeAryPtr::RANGE)   alias_type(idx)-&gt;set_rewritable(false);
1582     if (flat-&gt;isa_instptr()) {
1583       if (flat-&gt;offset() == java_lang_Class::klass_offset_in_bytes()
1584           &amp;&amp; flat-&gt;is_instptr()-&gt;klass() == env()-&gt;Class_klass())
1585         alias_type(idx)-&gt;set_rewritable(false);
1586     }
1587     if (flat-&gt;isa_aryptr()) {
1588 #ifdef ASSERT
1589       const int header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1590       // (T_BYTE has the weakest alignment and size restrictions...)
1591       assert(flat-&gt;offset() &lt; header_size_min, &quot;array body reference must be OffsetBot&quot;);
1592 #endif
1593       if (flat-&gt;offset() == TypePtr::OffsetBot) {
1594         alias_type(idx)-&gt;set_element(flat-&gt;is_aryptr()-&gt;elem());
1595       }
1596     }
1597     if (flat-&gt;isa_klassptr()) {
1598       if (flat-&gt;offset() == in_bytes(Klass::super_check_offset_offset()))
1599         alias_type(idx)-&gt;set_rewritable(false);
1600       if (flat-&gt;offset() == in_bytes(Klass::modifier_flags_offset()))
1601         alias_type(idx)-&gt;set_rewritable(false);
1602       if (flat-&gt;offset() == in_bytes(Klass::access_flags_offset()))
1603         alias_type(idx)-&gt;set_rewritable(false);
1604       if (flat-&gt;offset() == in_bytes(Klass::java_mirror_offset()))
1605         alias_type(idx)-&gt;set_rewritable(false);
1606       if (flat-&gt;offset() == in_bytes(Klass::secondary_super_cache_offset()))
1607         alias_type(idx)-&gt;set_rewritable(false);
1608     }
1609     // %%% (We would like to finalize JavaThread::threadObj_offset(),
1610     // but the base pointer type is not distinctive enough to identify
1611     // references into JavaThread.)
1612 
1613     // Check for final fields.
1614     const TypeInstPtr* tinst = flat-&gt;isa_instptr();
1615     if (tinst &amp;&amp; tinst-&gt;offset() &gt;= instanceOopDesc::base_offset_in_bytes()) {
1616       ciField* field;
1617       if (tinst-&gt;const_oop() != NULL &amp;&amp;
1618           tinst-&gt;klass() == ciEnv::current()-&gt;Class_klass() &amp;&amp;
1619           tinst-&gt;offset() &gt;= (tinst-&gt;klass()-&gt;as_instance_klass()-&gt;size_helper() * wordSize)) {
1620         // static field
1621         ciInstanceKlass* k = tinst-&gt;const_oop()-&gt;as_instance()-&gt;java_lang_Class_klass()-&gt;as_instance_klass();
1622         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), true);
1623       } else {
1624         ciInstanceKlass *k = tinst-&gt;klass()-&gt;as_instance_klass();
1625         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1626       }
1627       assert(field == NULL ||
1628              original_field == NULL ||
1629              (field-&gt;holder() == original_field-&gt;holder() &amp;&amp;
1630               field-&gt;offset() == original_field-&gt;offset() &amp;&amp;
1631               field-&gt;is_static() == original_field-&gt;is_static()), &quot;wrong field?&quot;);
1632       // Set field() and is_rewritable() attributes.
1633       if (field != NULL)  alias_type(idx)-&gt;set_field(field);
1634     }
1635   }
1636 
1637   // Fill the cache for next time.
1638   ace-&gt;_adr_type = adr_type;
1639   ace-&gt;_index    = idx;
1640   assert(alias_type(adr_type) == alias_type(idx),  &quot;type must be installed&quot;);
1641 
1642   // Might as well try to fill the cache for the flattened version, too.
1643   AliasCacheEntry* face = probe_alias_cache(flat);
1644   if (face-&gt;_adr_type == NULL) {
1645     face-&gt;_adr_type = flat;
1646     face-&gt;_index    = idx;
1647     assert(alias_type(flat) == alias_type(idx), &quot;flat type must work too&quot;);
1648   }
1649 
1650   return alias_type(idx);
1651 }
1652 
1653 
1654 Compile::AliasType* Compile::alias_type(ciField* field) {
1655   const TypeOopPtr* t;
1656   if (field-&gt;is_static())
1657     t = TypeInstPtr::make(field-&gt;holder()-&gt;java_mirror());
1658   else
1659     t = TypeOopPtr::make_from_klass_raw(field-&gt;holder());
1660   AliasType* atp = alias_type(t-&gt;add_offset(field-&gt;offset_in_bytes()), field);
1661   assert((field-&gt;is_final() || field-&gt;is_stable()) == !atp-&gt;is_rewritable(), &quot;must get the rewritable bits correct&quot;);
1662   return atp;
1663 }
1664 
1665 
1666 //------------------------------have_alias_type--------------------------------
1667 bool Compile::have_alias_type(const TypePtr* adr_type) {
1668   AliasCacheEntry* ace = probe_alias_cache(adr_type);
1669   if (ace-&gt;_adr_type == adr_type) {
1670     return true;
1671   }
1672 
1673   // Handle special cases.
1674   if (adr_type == NULL)             return true;
1675   if (adr_type == TypePtr::BOTTOM)  return true;
1676 
1677   return find_alias_type(adr_type, true, NULL) != NULL;
1678 }
1679 
1680 //-----------------------------must_alias--------------------------------------
1681 // True if all values of the given address type are in the given alias category.
1682 bool Compile::must_alias(const TypePtr* adr_type, int alias_idx) {
1683   if (alias_idx == AliasIdxBot)         return true;  // the universal category
1684   if (adr_type == NULL)                 return true;  // NULL serves as TypePtr::TOP
1685   if (alias_idx == AliasIdxTop)         return false; // the empty category
1686   if (adr_type-&gt;base() == Type::AnyPtr) return false; // TypePtr::BOTTOM or its twins
1687 
1688   // the only remaining possible overlap is identity
1689   int adr_idx = get_alias_index(adr_type);
1690   assert(adr_idx != AliasIdxBot &amp;&amp; adr_idx != AliasIdxTop, &quot;&quot;);
1691   assert(adr_idx == alias_idx ||
1692          (alias_type(alias_idx)-&gt;adr_type() != TypeOopPtr::BOTTOM
1693           &amp;&amp; adr_type                       != TypeOopPtr::BOTTOM),
1694          &quot;should not be testing for overlap with an unsafe pointer&quot;);
1695   return adr_idx == alias_idx;
1696 }
1697 
1698 //------------------------------can_alias--------------------------------------
1699 // True if any values of the given address type are in the given alias category.
1700 bool Compile::can_alias(const TypePtr* adr_type, int alias_idx) {
1701   if (alias_idx == AliasIdxTop)         return false; // the empty category
1702   if (adr_type == NULL)                 return false; // NULL serves as TypePtr::TOP
1703   // Known instance doesn&#39;t alias with bottom memory
1704   if (alias_idx == AliasIdxBot)         return !adr_type-&gt;is_known_instance();                   // the universal category
1705   if (adr_type-&gt;base() == Type::AnyPtr) return !C-&gt;get_adr_type(alias_idx)-&gt;is_known_instance(); // TypePtr::BOTTOM or its twins
1706 
1707   // the only remaining possible overlap is identity
1708   int adr_idx = get_alias_index(adr_type);
1709   assert(adr_idx != AliasIdxBot &amp;&amp; adr_idx != AliasIdxTop, &quot;&quot;);
1710   return adr_idx == alias_idx;
1711 }
1712 
1713 
1714 
1715 //---------------------------pop_warm_call-------------------------------------
1716 WarmCallInfo* Compile::pop_warm_call() {
1717   WarmCallInfo* wci = _warm_calls;
1718   if (wci != NULL)  _warm_calls = wci-&gt;remove_from(wci);
1719   return wci;
1720 }
1721 
1722 //----------------------------Inline_Warm--------------------------------------
1723 int Compile::Inline_Warm() {
1724   // If there is room, try to inline some more warm call sites.
1725   // %%% Do a graph index compaction pass when we think we&#39;re out of space?
1726   if (!InlineWarmCalls)  return 0;
1727 
1728   int calls_made_hot = 0;
1729   int room_to_grow   = NodeCountInliningCutoff - unique();
1730   int amount_to_grow = MIN2(room_to_grow, (int)NodeCountInliningStep);
1731   int amount_grown   = 0;
1732   WarmCallInfo* call;
1733   while (amount_to_grow &gt; 0 &amp;&amp; (call = pop_warm_call()) != NULL) {
1734     int est_size = (int)call-&gt;size();
1735     if (est_size &gt; (room_to_grow - amount_grown)) {
1736       // This one won&#39;t fit anyway.  Get rid of it.
1737       call-&gt;make_cold();
1738       continue;
1739     }
1740     call-&gt;make_hot();
1741     calls_made_hot++;
1742     amount_grown   += est_size;
1743     amount_to_grow -= est_size;
1744   }
1745 
1746   if (calls_made_hot &gt; 0)  set_major_progress();
1747   return calls_made_hot;
1748 }
1749 
1750 
1751 //----------------------------Finish_Warm--------------------------------------
1752 void Compile::Finish_Warm() {
1753   if (!InlineWarmCalls)  return;
1754   if (failing())  return;
1755   if (warm_calls() == NULL)  return;
1756 
1757   // Clean up loose ends, if we are out of space for inlining.
1758   WarmCallInfo* call;
1759   while ((call = pop_warm_call()) != NULL) {
1760     call-&gt;make_cold();
1761   }
1762 }
1763 
1764 //---------------------cleanup_loop_predicates-----------------------
1765 // Remove the opaque nodes that protect the predicates so that all unused
1766 // checks and uncommon_traps will be eliminated from the ideal graph
1767 void Compile::cleanup_loop_predicates(PhaseIterGVN &amp;igvn) {
1768   if (predicate_count()==0) return;
1769   for (int i = predicate_count(); i &gt; 0; i--) {
1770     Node * n = predicate_opaque1_node(i-1);
1771     assert(n-&gt;Opcode() == Op_Opaque1, &quot;must be&quot;);
1772     igvn.replace_node(n, n-&gt;in(1));
1773   }
1774   assert(predicate_count()==0, &quot;should be clean!&quot;);
1775 }
1776 
1777 void Compile::add_range_check_cast(Node* n) {
1778   assert(n-&gt;isa_CastII()-&gt;has_range_check(), &quot;CastII should have range check dependency&quot;);
1779   assert(!_range_check_casts-&gt;contains(n), &quot;duplicate entry in range check casts&quot;);
1780   _range_check_casts-&gt;append(n);
1781 }
1782 
1783 // Remove all range check dependent CastIINodes.
1784 void Compile::remove_range_check_casts(PhaseIterGVN &amp;igvn) {
1785   for (int i = range_check_cast_count(); i &gt; 0; i--) {
1786     Node* cast = range_check_cast_node(i-1);
1787     assert(cast-&gt;isa_CastII()-&gt;has_range_check(), &quot;CastII should have range check dependency&quot;);
1788     igvn.replace_node(cast, cast-&gt;in(1));
1789   }
1790   assert(range_check_cast_count() == 0, &quot;should be empty&quot;);
1791 }
1792 
1793 void Compile::add_opaque4_node(Node* n) {
1794   assert(n-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1795   assert(!_opaque4_nodes-&gt;contains(n), &quot;duplicate entry in Opaque4 list&quot;);
1796   _opaque4_nodes-&gt;append(n);
1797 }
1798 
1799 // Remove all Opaque4 nodes.
1800 void Compile::remove_opaque4_nodes(PhaseIterGVN &amp;igvn) {
1801   for (int i = opaque4_count(); i &gt; 0; i--) {
1802     Node* opaq = opaque4_node(i-1);
1803     assert(opaq-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1804     igvn.replace_node(opaq, opaq-&gt;in(2));
1805   }
1806   assert(opaque4_count() == 0, &quot;should be empty&quot;);
1807 }
1808 
1809 // StringOpts and late inlining of string methods
1810 void Compile::inline_string_calls(bool parse_time) {
1811   {
1812     // remove useless nodes to make the usage analysis simpler
1813     ResourceMark rm;
1814     PhaseRemoveUseless pru(initial_gvn(), for_igvn());
1815   }
1816 
1817   {
1818     ResourceMark rm;
1819     print_method(PHASE_BEFORE_STRINGOPTS, 3);
1820     PhaseStringOpts pso(initial_gvn(), for_igvn());
1821     print_method(PHASE_AFTER_STRINGOPTS, 3);
1822   }
1823 
1824   // now inline anything that we skipped the first time around
1825   if (!parse_time) {
1826     _late_inlines_pos = _late_inlines.length();
1827   }
1828 
1829   while (_string_late_inlines.length() &gt; 0) {
1830     CallGenerator* cg = _string_late_inlines.pop();
1831     cg-&gt;do_late_inline();
1832     if (failing())  return;
1833   }
1834   _string_late_inlines.trunc_to(0);
1835 }
1836 
1837 // Late inlining of boxing methods
1838 void Compile::inline_boxing_calls(PhaseIterGVN&amp; igvn) {
1839   if (_boxing_late_inlines.length() &gt; 0) {
1840     assert(has_boxed_value(), &quot;inconsistent&quot;);
1841 
1842     PhaseGVN* gvn = initial_gvn();
1843     set_inlining_incrementally(true);
1844 
1845     assert( igvn._worklist.size() == 0, &quot;should be done with igvn&quot; );
1846     for_igvn()-&gt;clear();
1847     gvn-&gt;replace_with(&amp;igvn);
1848 
1849     _late_inlines_pos = _late_inlines.length();
1850 
1851     while (_boxing_late_inlines.length() &gt; 0) {
1852       CallGenerator* cg = _boxing_late_inlines.pop();
1853       cg-&gt;do_late_inline();
1854       if (failing())  return;
1855     }
1856     _boxing_late_inlines.trunc_to(0);
1857 
1858     inline_incrementally_cleanup(igvn);
1859 
1860     set_inlining_incrementally(false);
1861   }
1862 }
1863 
1864 bool Compile::inline_incrementally_one() {
1865   assert(IncrementalInline, &quot;incremental inlining should be on&quot;);
1866 
1867   TracePhase tp(&quot;incrementalInline_inline&quot;, &amp;timers[_t_incrInline_inline]);
1868   set_inlining_progress(false);
1869   set_do_cleanup(false);
1870   int i = 0;
1871   for (; i &lt;_late_inlines.length() &amp;&amp; !inlining_progress(); i++) {
1872     CallGenerator* cg = _late_inlines.at(i);
1873     _late_inlines_pos = i+1;
1874     cg-&gt;do_late_inline();
1875     if (failing())  return false;
1876   }
1877   int j = 0;
1878   for (; i &lt; _late_inlines.length(); i++, j++) {
1879     _late_inlines.at_put(j, _late_inlines.at(i));
1880   }
1881   _late_inlines.trunc_to(j);
1882   assert(inlining_progress() || _late_inlines.length() == 0, &quot;&quot;);
1883 
1884   bool needs_cleanup = do_cleanup() || over_inlining_cutoff();
1885 
1886   set_inlining_progress(false);
1887   set_do_cleanup(false);
1888   return (_late_inlines.length() &gt; 0) &amp;&amp; !needs_cleanup;
1889 }
1890 
1891 void Compile::inline_incrementally_cleanup(PhaseIterGVN&amp; igvn) {
1892   {
1893     TracePhase tp(&quot;incrementalInline_pru&quot;, &amp;timers[_t_incrInline_pru]);
1894     ResourceMark rm;
1895     PhaseRemoveUseless pru(initial_gvn(), for_igvn());
1896   }
1897   {
1898     TracePhase tp(&quot;incrementalInline_igvn&quot;, &amp;timers[_t_incrInline_igvn]);
1899     igvn = PhaseIterGVN(initial_gvn());
1900     igvn.optimize();
1901   }
1902 }
1903 
1904 // Perform incremental inlining until bound on number of live nodes is reached
1905 void Compile::inline_incrementally(PhaseIterGVN&amp; igvn) {
1906   TracePhase tp(&quot;incrementalInline&quot;, &amp;timers[_t_incrInline]);
1907 
1908   set_inlining_incrementally(true);
1909   uint low_live_nodes = 0;
1910 
1911   while (_late_inlines.length() &gt; 0) {
1912     if (live_nodes() &gt; (uint)LiveNodeCountInliningCutoff) {
1913       if (low_live_nodes &lt; (uint)LiveNodeCountInliningCutoff * 8 / 10) {
1914         TracePhase tp(&quot;incrementalInline_ideal&quot;, &amp;timers[_t_incrInline_ideal]);
1915         // PhaseIdealLoop is expensive so we only try it once we are
1916         // out of live nodes and we only try it again if the previous
1917         // helped got the number of nodes down significantly
1918         PhaseIdealLoop::optimize(igvn, LoopOptsNone);
1919         if (failing())  return;
1920         low_live_nodes = live_nodes();
1921         _major_progress = true;
1922       }
1923 
1924       if (live_nodes() &gt; (uint)LiveNodeCountInliningCutoff) {
1925         break; // finish
1926       }
1927     }
1928 
1929     for_igvn()-&gt;clear();
1930     initial_gvn()-&gt;replace_with(&amp;igvn);
1931 
1932     while (inline_incrementally_one()) {
1933       assert(!failing(), &quot;inconsistent&quot;);
1934     }
1935 
1936     if (failing())  return;
1937 
1938     inline_incrementally_cleanup(igvn);
1939 
1940     if (failing())  return;
1941   }
1942   assert( igvn._worklist.size() == 0, &quot;should be done with igvn&quot; );
1943 
1944   if (_string_late_inlines.length() &gt; 0) {
1945     assert(has_stringbuilder(), &quot;inconsistent&quot;);
1946     for_igvn()-&gt;clear();
1947     initial_gvn()-&gt;replace_with(&amp;igvn);
1948 
1949     inline_string_calls(false);
1950 
1951     if (failing())  return;
1952 
1953     inline_incrementally_cleanup(igvn);
1954   }
1955 
1956   set_inlining_incrementally(false);
1957 }
1958 
1959 
1960 bool Compile::optimize_loops(PhaseIterGVN&amp; igvn, LoopOptsMode mode) {
1961   if(_loop_opts_cnt &gt; 0) {
1962     debug_only( int cnt = 0; );
1963     while(major_progress() &amp;&amp; (_loop_opts_cnt &gt; 0)) {
1964       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
1965       assert( cnt++ &lt; 40, &quot;infinite cycle in loop optimization&quot; );
1966       PhaseIdealLoop::optimize(igvn, mode);
1967       _loop_opts_cnt--;
1968       if (failing())  return false;
1969       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP_ITERATIONS, 2);
1970     }
1971   }
1972   return true;
1973 }
1974 
1975 // Remove edges from &quot;root&quot; to each SafePoint at a backward branch.
1976 // They were inserted during parsing (see add_safepoint()) to make
1977 // infinite loops without calls or exceptions visible to root, i.e.,
1978 // useful.
1979 void Compile::remove_root_to_sfpts_edges(PhaseIterGVN&amp; igvn) {
1980   Node *r = root();
1981   if (r != NULL) {
1982     for (uint i = r-&gt;req(); i &lt; r-&gt;len(); ++i) {
1983       Node *n = r-&gt;in(i);
1984       if (n != NULL &amp;&amp; n-&gt;is_SafePoint()) {
1985         r-&gt;rm_prec(i);
1986         if (n-&gt;outcnt() == 0) {
1987           igvn.remove_dead_node(n);
1988         }
1989         --i;
1990       }
1991     }
1992     // Parsing may have added top inputs to the root node (Path
1993     // leading to the Halt node proven dead). Make sure we get a
1994     // chance to clean them up.
1995     igvn._worklist.push(r);
1996     igvn.optimize();
1997   }
1998 }
1999 
2000 //------------------------------Optimize---------------------------------------
2001 // Given a graph, optimize it.
2002 void Compile::Optimize() {
2003   TracePhase tp(&quot;optimizer&quot;, &amp;timers[_t_optimizer]);
2004 
2005 #ifndef PRODUCT
2006   if (_directive-&gt;BreakAtCompileOption) {
2007     BREAKPOINT;
2008   }
2009 
2010 #endif
2011 
2012   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
2013 #ifdef ASSERT
2014   bs-&gt;verify_gc_barriers(this, BarrierSetC2::BeforeOptimize);
2015 #endif
2016 
2017   ResourceMark rm;
2018 
2019   print_inlining_reinit();
2020 
2021   NOT_PRODUCT( verify_graph_edges(); )
2022 
2023   print_method(PHASE_AFTER_PARSING);
2024 
2025  {
2026   // Iterative Global Value Numbering, including ideal transforms
2027   // Initialize IterGVN with types and values from parse-time GVN
2028   PhaseIterGVN igvn(initial_gvn());
2029 #ifdef ASSERT
2030   _modified_nodes = new (comp_arena()) Unique_Node_List(comp_arena());
2031 #endif
2032   {
2033     TracePhase tp(&quot;iterGVN&quot;, &amp;timers[_t_iterGVN]);
2034     igvn.optimize();
2035   }
2036 
2037   if (failing())  return;
2038 
2039   print_method(PHASE_ITER_GVN1, 2);
2040 
2041   inline_incrementally(igvn);
2042 
2043   print_method(PHASE_INCREMENTAL_INLINE, 2);
2044 
2045   if (failing())  return;
2046 
2047   if (eliminate_boxing()) {
2048     // Inline valueOf() methods now.
2049     inline_boxing_calls(igvn);
2050 
2051     if (AlwaysIncrementalInline) {
2052       inline_incrementally(igvn);
2053     }
2054 
2055     print_method(PHASE_INCREMENTAL_BOXING_INLINE, 2);
2056 
2057     if (failing())  return;
2058   }
2059 
2060   // Now that all inlining is over, cut edge from root to loop
2061   // safepoints
2062   remove_root_to_sfpts_edges(igvn);
2063 
2064   // Remove the speculative part of types and clean up the graph from
2065   // the extra CastPP nodes whose only purpose is to carry them. Do
2066   // that early so that optimizations are not disrupted by the extra
2067   // CastPP nodes.
2068   remove_speculative_types(igvn);
2069 
2070   // No more new expensive nodes will be added to the list from here
2071   // so keep only the actual candidates for optimizations.
2072   cleanup_expensive_nodes(igvn);
2073 
2074   if (!failing() &amp;&amp; RenumberLiveNodes &amp;&amp; live_nodes() + NodeLimitFudgeFactor &lt; unique()) {
2075     Compile::TracePhase tp(&quot;&quot;, &amp;timers[_t_renumberLive]);
2076     initial_gvn()-&gt;replace_with(&amp;igvn);
2077     for_igvn()-&gt;clear();
2078     Unique_Node_List new_worklist(C-&gt;comp_arena());
2079     {
2080       ResourceMark rm;
2081       PhaseRenumberLive prl = PhaseRenumberLive(initial_gvn(), for_igvn(), &amp;new_worklist);
2082     }
2083     set_for_igvn(&amp;new_worklist);
2084     igvn = PhaseIterGVN(initial_gvn());
2085     igvn.optimize();
2086   }
2087 
2088   // Perform escape analysis
2089   if (_do_escape_analysis &amp;&amp; ConnectionGraph::has_candidates(this)) {
2090     if (has_loops()) {
2091       // Cleanup graph (remove dead nodes).
2092       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2093       PhaseIdealLoop::optimize(igvn, LoopOptsMaxUnroll);
2094       if (major_progress()) print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);
2095       if (failing())  return;
2096     }
2097     ConnectionGraph::do_analysis(this, &amp;igvn);
2098 
2099     if (failing())  return;
2100 
2101     // Optimize out fields loads from scalar replaceable allocations.
2102     igvn.optimize();
2103     print_method(PHASE_ITER_GVN_AFTER_EA, 2);
2104 
2105     if (failing())  return;
2106 
2107     if (congraph() != NULL &amp;&amp; macro_count() &gt; 0) {
2108       TracePhase tp(&quot;macroEliminate&quot;, &amp;timers[_t_macroEliminate]);
2109       PhaseMacroExpand mexp(igvn);
2110       mexp.eliminate_macro_nodes();
2111       igvn.set_delay_transform(false);
2112 
2113       igvn.optimize();
2114       print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);
2115 
2116       if (failing())  return;
2117     }
2118   }
2119 
2120   // Loop transforms on the ideal graph.  Range Check Elimination,
2121   // peeling, unrolling, etc.
2122 
2123   // Set loop opts counter
2124   if((_loop_opts_cnt &gt; 0) &amp;&amp; (has_loops() || has_split_ifs())) {
2125     {
2126       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2127       PhaseIdealLoop::optimize(igvn, LoopOptsDefault);
2128       _loop_opts_cnt--;
2129       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP1, 2);
2130       if (failing())  return;
2131     }
2132     // Loop opts pass if partial peeling occurred in previous pass
2133     if(PartialPeelLoop &amp;&amp; major_progress() &amp;&amp; (_loop_opts_cnt &gt; 0)) {
2134       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2135       PhaseIdealLoop::optimize(igvn, LoopOptsSkipSplitIf);
2136       _loop_opts_cnt--;
2137       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP2, 2);
2138       if (failing())  return;
2139     }
2140     // Loop opts pass for loop-unrolling before CCP
2141     if(major_progress() &amp;&amp; (_loop_opts_cnt &gt; 0)) {
2142       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2143       PhaseIdealLoop::optimize(igvn, LoopOptsSkipSplitIf);
2144       _loop_opts_cnt--;
2145       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP3, 2);
2146     }
2147     if (!failing()) {
2148       // Verify that last round of loop opts produced a valid graph
2149       TracePhase tp(&quot;idealLoopVerify&quot;, &amp;timers[_t_idealLoopVerify]);
2150       PhaseIdealLoop::verify(igvn);
2151     }
2152   }
2153   if (failing())  return;
2154 
2155   // Conditional Constant Propagation;
2156   PhaseCCP ccp( &amp;igvn );
2157   assert( true, &quot;Break here to ccp.dump_nodes_and_types(_root,999,1)&quot;);
2158   {
2159     TracePhase tp(&quot;ccp&quot;, &amp;timers[_t_ccp]);
2160     ccp.do_transform();
2161   }
2162   print_method(PHASE_CPP1, 2);
2163 
2164   assert( true, &quot;Break here to ccp.dump_old2new_map()&quot;);
2165 
2166   // Iterative Global Value Numbering, including ideal transforms
2167   {
2168     TracePhase tp(&quot;iterGVN2&quot;, &amp;timers[_t_iterGVN2]);
2169     igvn = ccp;
2170     igvn.optimize();
2171   }
2172   print_method(PHASE_ITER_GVN2, 2);
2173 
2174   if (failing())  return;
2175 
2176   // Loop transforms on the ideal graph.  Range Check Elimination,
2177   // peeling, unrolling, etc.
2178   if (!optimize_loops(igvn, LoopOptsDefault)) {
2179     return;
2180   }
2181 
2182   if (failing())  return;
2183 
2184   // Ensure that major progress is now clear
2185   C-&gt;clear_major_progress();
2186 
2187   {
2188     // Verify that all previous optimizations produced a valid graph
2189     // at least to this point, even if no loop optimizations were done.
2190     TracePhase tp(&quot;idealLoopVerify&quot;, &amp;timers[_t_idealLoopVerify]);
2191     PhaseIdealLoop::verify(igvn);
2192   }
2193 
2194   if (range_check_cast_count() &gt; 0) {
2195     // No more loop optimizations. Remove all range check dependent CastIINodes.
2196     C-&gt;remove_range_check_casts(igvn);
2197     igvn.optimize();
2198   }
2199 
2200 #ifdef ASSERT
2201   bs-&gt;verify_gc_barriers(this, BarrierSetC2::BeforeMacroExpand);
2202 #endif
2203 
2204   {
2205     TracePhase tp(&quot;macroExpand&quot;, &amp;timers[_t_macroExpand]);
2206     PhaseMacroExpand  mex(igvn);
2207     if (mex.expand_macro_nodes()) {
2208       assert(failing(), &quot;must bail out w/ explicit message&quot;);
2209       return;
2210     }
2211     print_method(PHASE_MACRO_EXPANSION, 2);
2212   }
2213 
2214   {
2215     TracePhase tp(&quot;barrierExpand&quot;, &amp;timers[_t_barrierExpand]);
2216     if (bs-&gt;expand_barriers(this, igvn)) {
2217       assert(failing(), &quot;must bail out w/ explicit message&quot;);
2218       return;
2219     }
2220     print_method(PHASE_BARRIER_EXPANSION, 2);
2221   }
2222 
2223   if (opaque4_count() &gt; 0) {
2224     C-&gt;remove_opaque4_nodes(igvn);
2225     igvn.optimize();
2226   }
2227 
2228   DEBUG_ONLY( _modified_nodes = NULL; )
2229  } // (End scope of igvn; run destructor if necessary for asserts.)
2230 
2231  process_print_inlining();
2232  // A method with only infinite loops has no edges entering loops from root
2233  {
2234    TracePhase tp(&quot;graphReshape&quot;, &amp;timers[_t_graphReshaping]);
2235    if (final_graph_reshaping()) {
2236      assert(failing(), &quot;must bail out w/ explicit message&quot;);
2237      return;
2238    }
2239  }
2240 
2241  print_method(PHASE_OPTIMIZE_FINISHED, 2);
2242 }
2243 
2244 
2245 //------------------------------Code_Gen---------------------------------------
2246 // Given a graph, generate code for it
2247 void Compile::Code_Gen() {
2248   if (failing()) {
2249     return;
2250   }
2251 
2252   // Perform instruction selection.  You might think we could reclaim Matcher
2253   // memory PDQ, but actually the Matcher is used in generating spill code.
2254   // Internals of the Matcher (including some VectorSets) must remain live
2255   // for awhile - thus I cannot reclaim Matcher memory lest a VectorSet usage
2256   // set a bit in reclaimed memory.
2257 
2258   // In debug mode can dump m._nodes.dump() for mapping of ideal to machine
2259   // nodes.  Mapping is only valid at the root of each matched subtree.
2260   NOT_PRODUCT( verify_graph_edges(); )
2261 
2262   Matcher matcher;
2263   _matcher = &amp;matcher;
2264   {
2265     TracePhase tp(&quot;matcher&quot;, &amp;timers[_t_matcher]);
2266     matcher.match();
2267     if (failing()) {
2268       return;
2269     }
2270   }
2271 
2272   // In debug mode can dump m._nodes.dump() for mapping of ideal to machine
2273   // nodes.  Mapping is only valid at the root of each matched subtree.
2274   NOT_PRODUCT( verify_graph_edges(); )
2275 
2276   // If you have too many nodes, or if matching has failed, bail out
2277   check_node_count(0, &quot;out of nodes matching instructions&quot;);
2278   if (failing()) {
2279     return;
2280   }
2281 
2282   print_method(PHASE_MATCHING, 2);
2283 
2284   // Build a proper-looking CFG
2285   PhaseCFG cfg(node_arena(), root(), matcher);
2286   _cfg = &amp;cfg;
2287   {
2288     TracePhase tp(&quot;scheduler&quot;, &amp;timers[_t_scheduler]);
2289     bool success = cfg.do_global_code_motion();
2290     if (!success) {
2291       return;
2292     }
2293 
2294     print_method(PHASE_GLOBAL_CODE_MOTION, 2);
2295     NOT_PRODUCT( verify_graph_edges(); )
2296     debug_only( cfg.verify(); )
2297   }
2298 
2299   PhaseChaitin regalloc(unique(), cfg, matcher, false);
2300   _regalloc = &amp;regalloc;
2301   {
2302     TracePhase tp(&quot;regalloc&quot;, &amp;timers[_t_registerAllocation]);
2303     // Perform register allocation.  After Chaitin, use-def chains are
2304     // no longer accurate (at spill code) and so must be ignored.
2305     // Node-&gt;LRG-&gt;reg mappings are still accurate.
2306     _regalloc-&gt;Register_Allocate();
2307 
2308     // Bail out if the allocator builds too many nodes
2309     if (failing()) {
2310       return;
2311     }
2312   }
2313 
2314   // Prior to register allocation we kept empty basic blocks in case the
2315   // the allocator needed a place to spill.  After register allocation we
2316   // are not adding any new instructions.  If any basic block is empty, we
2317   // can now safely remove it.
2318   {
2319     TracePhase tp(&quot;blockOrdering&quot;, &amp;timers[_t_blockOrdering]);
2320     cfg.remove_empty_blocks();
2321     if (do_freq_based_layout()) {
2322       PhaseBlockLayout layout(cfg);
2323     } else {
2324       cfg.set_loop_alignment();
2325     }
2326     cfg.fixup_flow();
2327   }
2328 
2329   // Apply peephole optimizations
2330   if( OptoPeephole ) {
2331     TracePhase tp(&quot;peephole&quot;, &amp;timers[_t_peephole]);
2332     PhasePeephole peep( _regalloc, cfg);
2333     peep.do_transform();
2334   }
2335 
2336   // Do late expand if CPU requires this.
2337   if (Matcher::require_postalloc_expand) {
2338     TracePhase tp(&quot;postalloc_expand&quot;, &amp;timers[_t_postalloc_expand]);
2339     cfg.postalloc_expand(_regalloc);
2340   }
2341 
2342   // Convert Nodes to instruction bits in a buffer
2343   {
2344     TracePhase tp(&quot;output&quot;, &amp;timers[_t_output]);
2345     PhaseOutput output;
2346     output.Output();
2347     if (failing())  return;
2348     output.install();
2349   }
2350 
2351   print_method(PHASE_FINAL_CODE);
2352 
2353   // He&#39;s dead, Jim.
2354   _cfg     = (PhaseCFG*)((intptr_t)0xdeadbeef);
2355   _regalloc = (PhaseChaitin*)((intptr_t)0xdeadbeef);
2356 }
2357 
2358 //------------------------------Final_Reshape_Counts---------------------------
2359 // This class defines counters to help identify when a method
2360 // may/must be executed using hardware with only 24-bit precision.
2361 struct Final_Reshape_Counts : public StackObj {
2362   int  _call_count;             // count non-inlined &#39;common&#39; calls
2363   int  _float_count;            // count float ops requiring 24-bit precision
2364   int  _double_count;           // count double ops requiring more precision
2365   int  _java_call_count;        // count non-inlined &#39;java&#39; calls
2366   int  _inner_loop_count;       // count loops which need alignment
2367   VectorSet _visited;           // Visitation flags
2368   Node_List _tests;             // Set of IfNodes &amp; PCTableNodes
2369 
2370   Final_Reshape_Counts() :
2371     _call_count(0), _float_count(0), _double_count(0),
2372     _java_call_count(0), _inner_loop_count(0),
2373     _visited( Thread::current()-&gt;resource_area() ) { }
2374 
2375   void inc_call_count  () { _call_count  ++; }
2376   void inc_float_count () { _float_count ++; }
2377   void inc_double_count() { _double_count++; }
2378   void inc_java_call_count() { _java_call_count++; }
2379   void inc_inner_loop_count() { _inner_loop_count++; }
2380 
2381   int  get_call_count  () const { return _call_count  ; }
2382   int  get_float_count () const { return _float_count ; }
2383   int  get_double_count() const { return _double_count; }
2384   int  get_java_call_count() const { return _java_call_count; }
2385   int  get_inner_loop_count() const { return _inner_loop_count; }
2386 };
2387 
2388 #ifdef ASSERT
2389 static bool oop_offset_is_sane(const TypeInstPtr* tp) {
2390   ciInstanceKlass *k = tp-&gt;klass()-&gt;as_instance_klass();
2391   // Make sure the offset goes inside the instance layout.
2392   return k-&gt;contains_field_offset(tp-&gt;offset());
2393   // Note that OffsetBot and OffsetTop are very negative.
2394 }
2395 #endif
2396 
2397 // Eliminate trivially redundant StoreCMs and accumulate their
2398 // precedence edges.
2399 void Compile::eliminate_redundant_card_marks(Node* n) {
2400   assert(n-&gt;Opcode() == Op_StoreCM, &quot;expected StoreCM&quot;);
2401   if (n-&gt;in(MemNode::Address)-&gt;outcnt() &gt; 1) {
2402     // There are multiple users of the same address so it might be
2403     // possible to eliminate some of the StoreCMs
2404     Node* mem = n-&gt;in(MemNode::Memory);
2405     Node* adr = n-&gt;in(MemNode::Address);
2406     Node* val = n-&gt;in(MemNode::ValueIn);
2407     Node* prev = n;
2408     bool done = false;
2409     // Walk the chain of StoreCMs eliminating ones that match.  As
2410     // long as it&#39;s a chain of single users then the optimization is
2411     // safe.  Eliminating partially redundant StoreCMs would require
2412     // cloning copies down the other paths.
2413     while (mem-&gt;Opcode() == Op_StoreCM &amp;&amp; mem-&gt;outcnt() == 1 &amp;&amp; !done) {
2414       if (adr == mem-&gt;in(MemNode::Address) &amp;&amp;
2415           val == mem-&gt;in(MemNode::ValueIn)) {
2416         // redundant StoreCM
2417         if (mem-&gt;req() &gt; MemNode::OopStore) {
2418           // Hasn&#39;t been processed by this code yet.
2419           n-&gt;add_prec(mem-&gt;in(MemNode::OopStore));
2420         } else {
2421           // Already converted to precedence edge
2422           for (uint i = mem-&gt;req(); i &lt; mem-&gt;len(); i++) {
2423             // Accumulate any precedence edges
2424             if (mem-&gt;in(i) != NULL) {
2425               n-&gt;add_prec(mem-&gt;in(i));
2426             }
2427           }
2428           // Everything above this point has been processed.
2429           done = true;
2430         }
2431         // Eliminate the previous StoreCM
2432         prev-&gt;set_req(MemNode::Memory, mem-&gt;in(MemNode::Memory));
2433         assert(mem-&gt;outcnt() == 0, &quot;should be dead&quot;);
2434         mem-&gt;disconnect_inputs(NULL, this);
2435       } else {
2436         prev = mem;
2437       }
2438       mem = prev-&gt;in(MemNode::Memory);
2439     }
2440   }
2441 }
2442 
2443 //------------------------------final_graph_reshaping_impl----------------------
2444 // Implement items 1-5 from final_graph_reshaping below.
2445 void Compile::final_graph_reshaping_impl( Node *n, Final_Reshape_Counts &amp;frc) {
2446 
2447   if ( n-&gt;outcnt() == 0 ) return; // dead node
2448   uint nop = n-&gt;Opcode();
2449 
2450   // Check for 2-input instruction with &quot;last use&quot; on right input.
2451   // Swap to left input.  Implements item (2).
2452   if( n-&gt;req() == 3 &amp;&amp;          // two-input instruction
2453       n-&gt;in(1)-&gt;outcnt() &gt; 1 &amp;&amp; // left use is NOT a last use
2454       (!n-&gt;in(1)-&gt;is_Phi() || n-&gt;in(1)-&gt;in(2) != n) &amp;&amp; // it is not data loop
2455       n-&gt;in(2)-&gt;outcnt() == 1 &amp;&amp;// right use IS a last use
2456       !n-&gt;in(2)-&gt;is_Con() ) {   // right use is not a constant
2457     // Check for commutative opcode
2458     switch( nop ) {
2459     case Op_AddI:  case Op_AddF:  case Op_AddD:  case Op_AddL:
2460     case Op_MaxI:  case Op_MinI:
2461     case Op_MulI:  case Op_MulF:  case Op_MulD:  case Op_MulL:
2462     case Op_AndL:  case Op_XorL:  case Op_OrL:
2463     case Op_AndI:  case Op_XorI:  case Op_OrI: {
2464       // Move &quot;last use&quot; input to left by swapping inputs
2465       n-&gt;swap_edges(1, 2);
2466       break;
2467     }
2468     default:
2469       break;
2470     }
2471   }
2472 
2473 #ifdef ASSERT
2474   if( n-&gt;is_Mem() ) {
2475     int alias_idx = get_alias_index(n-&gt;as_Mem()-&gt;adr_type());
2476     assert( n-&gt;in(0) != NULL || alias_idx != Compile::AliasIdxRaw ||
2477             // oop will be recorded in oop map if load crosses safepoint
2478             n-&gt;is_Load() &amp;&amp; (n-&gt;as_Load()-&gt;bottom_type()-&gt;isa_oopptr() ||
2479                              LoadNode::is_immutable_value(n-&gt;in(MemNode::Address))),
2480             &quot;raw memory operations should have control edge&quot;);
2481   }
2482   if (n-&gt;is_MemBar()) {
2483     MemBarNode* mb = n-&gt;as_MemBar();
2484     if (mb-&gt;trailing_store() || mb-&gt;trailing_load_store()) {
2485       assert(mb-&gt;leading_membar()-&gt;trailing_membar() == mb, &quot;bad membar pair&quot;);
2486       Node* mem = BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;step_over_gc_barrier(mb-&gt;in(MemBarNode::Precedent));
2487       assert((mb-&gt;trailing_store() &amp;&amp; mem-&gt;is_Store() &amp;&amp; mem-&gt;as_Store()-&gt;is_release()) ||
2488              (mb-&gt;trailing_load_store() &amp;&amp; mem-&gt;is_LoadStore()), &quot;missing mem op&quot;);
2489     } else if (mb-&gt;leading()) {
2490       assert(mb-&gt;trailing_membar()-&gt;leading_membar() == mb, &quot;bad membar pair&quot;);
2491     }
2492   }
2493 #endif
2494   // Count FPU ops and common calls, implements item (3)
2495   bool gc_handled = BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;final_graph_reshaping(this, n, nop);
2496   if (!gc_handled) {
2497     final_graph_reshaping_main_switch(n, frc, nop);
2498   }
2499 
2500   // Collect CFG split points
2501   if (n-&gt;is_MultiBranch() &amp;&amp; !n-&gt;is_RangeCheck()) {
2502     frc._tests.push(n);
2503   }
2504 }
2505 
2506 void Compile::final_graph_reshaping_main_switch(Node* n, Final_Reshape_Counts&amp; frc, uint nop) {
2507   switch( nop ) {
2508   // Count all float operations that may use FPU
2509   case Op_AddF:
2510   case Op_SubF:
2511   case Op_MulF:
2512   case Op_DivF:
2513   case Op_NegF:
2514   case Op_ModF:
2515   case Op_ConvI2F:
2516   case Op_ConF:
2517   case Op_CmpF:
2518   case Op_CmpF3:
2519   // case Op_ConvL2F: // longs are split into 32-bit halves
2520     frc.inc_float_count();
2521     break;
2522 
2523   case Op_ConvF2D:
2524   case Op_ConvD2F:
2525     frc.inc_float_count();
2526     frc.inc_double_count();
2527     break;
2528 
2529   // Count all double operations that may use FPU
2530   case Op_AddD:
2531   case Op_SubD:
2532   case Op_MulD:
2533   case Op_DivD:
2534   case Op_NegD:
2535   case Op_ModD:
2536   case Op_ConvI2D:
2537   case Op_ConvD2I:
2538   // case Op_ConvL2D: // handled by leaf call
2539   // case Op_ConvD2L: // handled by leaf call
2540   case Op_ConD:
2541   case Op_CmpD:
2542   case Op_CmpD3:
2543     frc.inc_double_count();
2544     break;
2545   case Op_Opaque1:              // Remove Opaque Nodes before matching
2546   case Op_Opaque2:              // Remove Opaque Nodes before matching
2547   case Op_Opaque3:
2548     n-&gt;subsume_by(n-&gt;in(1), this);
2549     break;
2550   case Op_CallStaticJava:
2551   case Op_CallJava:
2552   case Op_CallDynamicJava:
2553     frc.inc_java_call_count(); // Count java call site;
2554   case Op_CallRuntime:
2555   case Op_CallLeaf:
2556   case Op_CallLeafNoFP: {
2557     assert (n-&gt;is_Call(), &quot;&quot;);
2558     CallNode *call = n-&gt;as_Call();
2559     // Count call sites where the FP mode bit would have to be flipped.
2560     // Do not count uncommon runtime calls:
2561     // uncommon_trap, _complete_monitor_locking, _complete_monitor_unlocking,
2562     // _new_Java, _new_typeArray, _new_objArray, _rethrow_Java, ...
2563     if (!call-&gt;is_CallStaticJava() || !call-&gt;as_CallStaticJava()-&gt;_name) {
2564       frc.inc_call_count();   // Count the call site
2565     } else {                  // See if uncommon argument is shared
2566       Node *n = call-&gt;in(TypeFunc::Parms);
2567       int nop = n-&gt;Opcode();
2568       // Clone shared simple arguments to uncommon calls, item (1).
2569       if (n-&gt;outcnt() &gt; 1 &amp;&amp;
2570           !n-&gt;is_Proj() &amp;&amp;
2571           nop != Op_CreateEx &amp;&amp;
2572           nop != Op_CheckCastPP &amp;&amp;
2573           nop != Op_DecodeN &amp;&amp;
2574           nop != Op_DecodeNKlass &amp;&amp;
2575           !n-&gt;is_Mem() &amp;&amp;
2576           !n-&gt;is_Phi()) {
2577         Node *x = n-&gt;clone();
2578         call-&gt;set_req(TypeFunc::Parms, x);
2579       }
2580     }
2581     break;
2582   }
2583 
2584   case Op_StoreD:
2585   case Op_LoadD:
2586   case Op_LoadD_unaligned:
2587     frc.inc_double_count();
2588     goto handle_mem;
2589   case Op_StoreF:
2590   case Op_LoadF:
2591     frc.inc_float_count();
2592     goto handle_mem;
2593 
2594   case Op_StoreCM:
2595     {
2596       // Convert OopStore dependence into precedence edge
2597       Node* prec = n-&gt;in(MemNode::OopStore);
2598       n-&gt;del_req(MemNode::OopStore);
2599       n-&gt;add_prec(prec);
2600       eliminate_redundant_card_marks(n);
2601     }
2602 
2603     // fall through
2604 
2605   case Op_StoreB:
2606   case Op_StoreC:
2607   case Op_StorePConditional:
2608   case Op_StoreI:
2609   case Op_StoreL:
2610   case Op_StoreIConditional:
2611   case Op_StoreLConditional:
2612   case Op_CompareAndSwapB:
2613   case Op_CompareAndSwapS:
2614   case Op_CompareAndSwapI:
2615   case Op_CompareAndSwapL:
2616   case Op_CompareAndSwapP:
2617   case Op_CompareAndSwapN:
2618   case Op_WeakCompareAndSwapB:
2619   case Op_WeakCompareAndSwapS:
2620   case Op_WeakCompareAndSwapI:
2621   case Op_WeakCompareAndSwapL:
2622   case Op_WeakCompareAndSwapP:
2623   case Op_WeakCompareAndSwapN:
2624   case Op_CompareAndExchangeB:
2625   case Op_CompareAndExchangeS:
2626   case Op_CompareAndExchangeI:
2627   case Op_CompareAndExchangeL:
2628   case Op_CompareAndExchangeP:
2629   case Op_CompareAndExchangeN:
2630   case Op_GetAndAddS:
2631   case Op_GetAndAddB:
2632   case Op_GetAndAddI:
2633   case Op_GetAndAddL:
2634   case Op_GetAndSetS:
2635   case Op_GetAndSetB:
2636   case Op_GetAndSetI:
2637   case Op_GetAndSetL:
2638   case Op_GetAndSetP:
2639   case Op_GetAndSetN:
2640   case Op_StoreP:
2641   case Op_StoreN:
2642   case Op_StoreNKlass:
2643   case Op_LoadB:
2644   case Op_LoadUB:
2645   case Op_LoadUS:
2646   case Op_LoadI:
2647   case Op_LoadKlass:
2648   case Op_LoadNKlass:
2649   case Op_LoadL:
2650   case Op_LoadL_unaligned:
2651   case Op_LoadPLocked:
2652   case Op_LoadP:
2653   case Op_LoadN:
2654   case Op_LoadRange:
2655   case Op_LoadS: {
2656   handle_mem:
2657 #ifdef ASSERT
2658     if( VerifyOptoOopOffsets ) {
2659       MemNode* mem  = n-&gt;as_Mem();
2660       // Check to see if address types have grounded out somehow.
2661       const TypeInstPtr *tp = mem-&gt;in(MemNode::Address)-&gt;bottom_type()-&gt;isa_instptr();
2662       assert( !tp || oop_offset_is_sane(tp), &quot;&quot; );
2663     }
2664 #endif
2665     break;
2666   }
2667 
2668   case Op_AddP: {               // Assert sane base pointers
2669     Node *addp = n-&gt;in(AddPNode::Address);
2670     assert( !addp-&gt;is_AddP() ||
2671             addp-&gt;in(AddPNode::Base)-&gt;is_top() || // Top OK for allocation
2672             addp-&gt;in(AddPNode::Base) == n-&gt;in(AddPNode::Base),
2673             &quot;Base pointers must match (addp %u)&quot;, addp-&gt;_idx );
2674 #ifdef _LP64
2675     if ((UseCompressedOops || UseCompressedClassPointers) &amp;&amp;
2676         addp-&gt;Opcode() == Op_ConP &amp;&amp;
2677         addp == n-&gt;in(AddPNode::Base) &amp;&amp;
2678         n-&gt;in(AddPNode::Offset)-&gt;is_Con()) {
2679       // If the transformation of ConP to ConN+DecodeN is beneficial depends
2680       // on the platform and on the compressed oops mode.
2681       // Use addressing with narrow klass to load with offset on x86.
2682       // Some platforms can use the constant pool to load ConP.
2683       // Do this transformation here since IGVN will convert ConN back to ConP.
2684       const Type* t = addp-&gt;bottom_type();
2685       bool is_oop   = t-&gt;isa_oopptr() != NULL;
2686       bool is_klass = t-&gt;isa_klassptr() != NULL;
2687 
2688       if ((is_oop   &amp;&amp; Matcher::const_oop_prefer_decode()  ) ||
2689           (is_klass &amp;&amp; Matcher::const_klass_prefer_decode())) {
2690         Node* nn = NULL;
2691 
2692         int op = is_oop ? Op_ConN : Op_ConNKlass;
2693 
2694         // Look for existing ConN node of the same exact type.
2695         Node* r  = root();
2696         uint cnt = r-&gt;outcnt();
2697         for (uint i = 0; i &lt; cnt; i++) {
2698           Node* m = r-&gt;raw_out(i);
2699           if (m!= NULL &amp;&amp; m-&gt;Opcode() == op &amp;&amp;
2700               m-&gt;bottom_type()-&gt;make_ptr() == t) {
2701             nn = m;
2702             break;
2703           }
2704         }
2705         if (nn != NULL) {
2706           // Decode a narrow oop to match address
2707           // [R12 + narrow_oop_reg&lt;&lt;3 + offset]
2708           if (is_oop) {
2709             nn = new DecodeNNode(nn, t);
2710           } else {
2711             nn = new DecodeNKlassNode(nn, t);
2712           }
2713           // Check for succeeding AddP which uses the same Base.
2714           // Otherwise we will run into the assertion above when visiting that guy.
2715           for (uint i = 0; i &lt; n-&gt;outcnt(); ++i) {
2716             Node *out_i = n-&gt;raw_out(i);
2717             if (out_i &amp;&amp; out_i-&gt;is_AddP() &amp;&amp; out_i-&gt;in(AddPNode::Base) == addp) {
2718               out_i-&gt;set_req(AddPNode::Base, nn);
2719 #ifdef ASSERT
2720               for (uint j = 0; j &lt; out_i-&gt;outcnt(); ++j) {
2721                 Node *out_j = out_i-&gt;raw_out(j);
2722                 assert(out_j == NULL || !out_j-&gt;is_AddP() || out_j-&gt;in(AddPNode::Base) != addp,
2723                        &quot;more than 2 AddP nodes in a chain (out_j %u)&quot;, out_j-&gt;_idx);
2724               }
2725 #endif
2726             }
2727           }
2728           n-&gt;set_req(AddPNode::Base, nn);
2729           n-&gt;set_req(AddPNode::Address, nn);
2730           if (addp-&gt;outcnt() == 0) {
2731             addp-&gt;disconnect_inputs(NULL, this);
2732           }
2733         }
2734       }
2735     }
2736 #endif
2737     // platform dependent reshaping of the address expression
2738     reshape_address(n-&gt;as_AddP());
2739     break;
2740   }
2741 
2742   case Op_CastPP: {
2743     // Remove CastPP nodes to gain more freedom during scheduling but
2744     // keep the dependency they encode as control or precedence edges
2745     // (if control is set already) on memory operations. Some CastPP
2746     // nodes don&#39;t have a control (don&#39;t carry a dependency): skip
2747     // those.
2748     if (n-&gt;in(0) != NULL) {
2749       ResourceMark rm;
2750       Unique_Node_List wq;
2751       wq.push(n);
2752       for (uint next = 0; next &lt; wq.size(); ++next) {
2753         Node *m = wq.at(next);
2754         for (DUIterator_Fast imax, i = m-&gt;fast_outs(imax); i &lt; imax; i++) {
2755           Node* use = m-&gt;fast_out(i);
2756           if (use-&gt;is_Mem() || use-&gt;is_EncodeNarrowPtr()) {
2757             use-&gt;ensure_control_or_add_prec(n-&gt;in(0));
2758           } else {
2759             switch(use-&gt;Opcode()) {
2760             case Op_AddP:
2761             case Op_DecodeN:
2762             case Op_DecodeNKlass:
2763             case Op_CheckCastPP:
2764             case Op_CastPP:
2765               wq.push(use);
2766               break;
2767             }
2768           }
2769         }
2770       }
2771     }
2772     const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);
2773     if (is_LP64 &amp;&amp; n-&gt;in(1)-&gt;is_DecodeN() &amp;&amp; Matcher::gen_narrow_oop_implicit_null_checks()) {
2774       Node* in1 = n-&gt;in(1);
2775       const Type* t = n-&gt;bottom_type();
2776       Node* new_in1 = in1-&gt;clone();
2777       new_in1-&gt;as_DecodeN()-&gt;set_type(t);
2778 
2779       if (!Matcher::narrow_oop_use_complex_address()) {
2780         //
2781         // x86, ARM and friends can handle 2 adds in addressing mode
2782         // and Matcher can fold a DecodeN node into address by using
2783         // a narrow oop directly and do implicit NULL check in address:
2784         //
2785         // [R12 + narrow_oop_reg&lt;&lt;3 + offset]
2786         // NullCheck narrow_oop_reg
2787         //
2788         // On other platforms (Sparc) we have to keep new DecodeN node and
2789         // use it to do implicit NULL check in address:
2790         //
2791         // decode_not_null narrow_oop_reg, base_reg
2792         // [base_reg + offset]
2793         // NullCheck base_reg
2794         //
2795         // Pin the new DecodeN node to non-null path on these platform (Sparc)
2796         // to keep the information to which NULL check the new DecodeN node
2797         // corresponds to use it as value in implicit_null_check().
2798         //
2799         new_in1-&gt;set_req(0, n-&gt;in(0));
2800       }
2801 
2802       n-&gt;subsume_by(new_in1, this);
2803       if (in1-&gt;outcnt() == 0) {
2804         in1-&gt;disconnect_inputs(NULL, this);
2805       }
2806     } else {
2807       n-&gt;subsume_by(n-&gt;in(1), this);
2808       if (n-&gt;outcnt() == 0) {
2809         n-&gt;disconnect_inputs(NULL, this);
2810       }
2811     }
2812     break;
2813   }
2814 #ifdef _LP64
2815   case Op_CmpP:
2816     // Do this transformation here to preserve CmpPNode::sub() and
2817     // other TypePtr related Ideal optimizations (for example, ptr nullness).
2818     if (n-&gt;in(1)-&gt;is_DecodeNarrowPtr() || n-&gt;in(2)-&gt;is_DecodeNarrowPtr()) {
2819       Node* in1 = n-&gt;in(1);
2820       Node* in2 = n-&gt;in(2);
2821       if (!in1-&gt;is_DecodeNarrowPtr()) {
2822         in2 = in1;
2823         in1 = n-&gt;in(2);
2824       }
2825       assert(in1-&gt;is_DecodeNarrowPtr(), &quot;sanity&quot;);
2826 
2827       Node* new_in2 = NULL;
2828       if (in2-&gt;is_DecodeNarrowPtr()) {
2829         assert(in2-&gt;Opcode() == in1-&gt;Opcode(), &quot;must be same node type&quot;);
2830         new_in2 = in2-&gt;in(1);
2831       } else if (in2-&gt;Opcode() == Op_ConP) {
2832         const Type* t = in2-&gt;bottom_type();
2833         if (t == TypePtr::NULL_PTR) {
2834           assert(in1-&gt;is_DecodeN(), &quot;compare klass to null?&quot;);
2835           // Don&#39;t convert CmpP null check into CmpN if compressed
2836           // oops implicit null check is not generated.
2837           // This will allow to generate normal oop implicit null check.
2838           if (Matcher::gen_narrow_oop_implicit_null_checks())
2839             new_in2 = ConNode::make(TypeNarrowOop::NULL_PTR);
2840           //
2841           // This transformation together with CastPP transformation above
2842           // will generated code for implicit NULL checks for compressed oops.
2843           //
2844           // The original code after Optimize()
2845           //
2846           //    LoadN memory, narrow_oop_reg
2847           //    decode narrow_oop_reg, base_reg
2848           //    CmpP base_reg, NULL
2849           //    CastPP base_reg // NotNull
2850           //    Load [base_reg + offset], val_reg
2851           //
2852           // after these transformations will be
2853           //
2854           //    LoadN memory, narrow_oop_reg
2855           //    CmpN narrow_oop_reg, NULL
2856           //    decode_not_null narrow_oop_reg, base_reg
2857           //    Load [base_reg + offset], val_reg
2858           //
2859           // and the uncommon path (== NULL) will use narrow_oop_reg directly
2860           // since narrow oops can be used in debug info now (see the code in
2861           // final_graph_reshaping_walk()).
2862           //
2863           // At the end the code will be matched to
2864           // on x86:
2865           //
2866           //    Load_narrow_oop memory, narrow_oop_reg
2867           //    Load [R12 + narrow_oop_reg&lt;&lt;3 + offset], val_reg
2868           //    NullCheck narrow_oop_reg
2869           //
2870           // and on sparc:
2871           //
2872           //    Load_narrow_oop memory, narrow_oop_reg
2873           //    decode_not_null narrow_oop_reg, base_reg
2874           //    Load [base_reg + offset], val_reg
2875           //    NullCheck base_reg
2876           //
2877         } else if (t-&gt;isa_oopptr()) {
2878           new_in2 = ConNode::make(t-&gt;make_narrowoop());
2879         } else if (t-&gt;isa_klassptr()) {
2880           new_in2 = ConNode::make(t-&gt;make_narrowklass());
2881         }
2882       }
2883       if (new_in2 != NULL) {
2884         Node* cmpN = new CmpNNode(in1-&gt;in(1), new_in2);
2885         n-&gt;subsume_by(cmpN, this);
2886         if (in1-&gt;outcnt() == 0) {
2887           in1-&gt;disconnect_inputs(NULL, this);
2888         }
2889         if (in2-&gt;outcnt() == 0) {
2890           in2-&gt;disconnect_inputs(NULL, this);
2891         }
2892       }
2893     }
2894     break;
2895 
2896   case Op_DecodeN:
2897   case Op_DecodeNKlass:
2898     assert(!n-&gt;in(1)-&gt;is_EncodeNarrowPtr(), &quot;should be optimized out&quot;);
2899     // DecodeN could be pinned when it can&#39;t be fold into
2900     // an address expression, see the code for Op_CastPP above.
2901     assert(n-&gt;in(0) == NULL || (UseCompressedOops &amp;&amp; !Matcher::narrow_oop_use_complex_address()), &quot;no control&quot;);
2902     break;
2903 
2904   case Op_EncodeP:
2905   case Op_EncodePKlass: {
2906     Node* in1 = n-&gt;in(1);
2907     if (in1-&gt;is_DecodeNarrowPtr()) {
2908       n-&gt;subsume_by(in1-&gt;in(1), this);
2909     } else if (in1-&gt;Opcode() == Op_ConP) {
2910       const Type* t = in1-&gt;bottom_type();
2911       if (t == TypePtr::NULL_PTR) {
2912         assert(t-&gt;isa_oopptr(), &quot;null klass?&quot;);
2913         n-&gt;subsume_by(ConNode::make(TypeNarrowOop::NULL_PTR), this);
2914       } else if (t-&gt;isa_oopptr()) {
2915         n-&gt;subsume_by(ConNode::make(t-&gt;make_narrowoop()), this);
2916       } else if (t-&gt;isa_klassptr()) {
2917         n-&gt;subsume_by(ConNode::make(t-&gt;make_narrowklass()), this);
2918       }
2919     }
2920     if (in1-&gt;outcnt() == 0) {
2921       in1-&gt;disconnect_inputs(NULL, this);
2922     }
2923     break;
2924   }
2925 
2926   case Op_Proj: {
2927     if (OptimizeStringConcat) {
2928       ProjNode* p = n-&gt;as_Proj();
2929       if (p-&gt;_is_io_use) {
2930         // Separate projections were used for the exception path which
2931         // are normally removed by a late inline.  If it wasn&#39;t inlined
2932         // then they will hang around and should just be replaced with
2933         // the original one.
2934         Node* proj = NULL;
2935         // Replace with just one
2936         for (SimpleDUIterator i(p-&gt;in(0)); i.has_next(); i.next()) {
2937           Node *use = i.get();
2938           if (use-&gt;is_Proj() &amp;&amp; p != use &amp;&amp; use-&gt;as_Proj()-&gt;_con == p-&gt;_con) {
2939             proj = use;
2940             break;
2941           }
2942         }
2943         assert(proj != NULL || p-&gt;_con == TypeFunc::I_O, &quot;io may be dropped at an infinite loop&quot;);
2944         if (proj != NULL) {
2945           p-&gt;subsume_by(proj, this);
2946         }
2947       }
2948     }
2949     break;
2950   }
2951 
2952   case Op_Phi:
2953     if (n-&gt;as_Phi()-&gt;bottom_type()-&gt;isa_narrowoop() || n-&gt;as_Phi()-&gt;bottom_type()-&gt;isa_narrowklass()) {
2954       // The EncodeP optimization may create Phi with the same edges
2955       // for all paths. It is not handled well by Register Allocator.
2956       Node* unique_in = n-&gt;in(1);
2957       assert(unique_in != NULL, &quot;&quot;);
2958       uint cnt = n-&gt;req();
2959       for (uint i = 2; i &lt; cnt; i++) {
2960         Node* m = n-&gt;in(i);
2961         assert(m != NULL, &quot;&quot;);
2962         if (unique_in != m)
2963           unique_in = NULL;
2964       }
2965       if (unique_in != NULL) {
2966         n-&gt;subsume_by(unique_in, this);
2967       }
2968     }
2969     break;
2970 
2971 #endif
2972 
2973 #ifdef ASSERT
2974   case Op_CastII:
2975     // Verify that all range check dependent CastII nodes were removed.
2976     if (n-&gt;isa_CastII()-&gt;has_range_check()) {
2977       n-&gt;dump(3);
2978       assert(false, &quot;Range check dependent CastII node was not removed&quot;);
2979     }
2980     break;
2981 #endif
2982 
2983   case Op_ModI:
2984     if (UseDivMod) {
2985       // Check if a%b and a/b both exist
2986       Node* d = n-&gt;find_similar(Op_DivI);
2987       if (d) {
2988         // Replace them with a fused divmod if supported
2989         if (Matcher::has_match_rule(Op_DivModI)) {
2990           DivModINode* divmod = DivModINode::make(n);
2991           d-&gt;subsume_by(divmod-&gt;div_proj(), this);
2992           n-&gt;subsume_by(divmod-&gt;mod_proj(), this);
2993         } else {
2994           // replace a%b with a-((a/b)*b)
2995           Node* mult = new MulINode(d, d-&gt;in(2));
2996           Node* sub  = new SubINode(d-&gt;in(1), mult);
2997           n-&gt;subsume_by(sub, this);
2998         }
2999       }
3000     }
3001     break;
3002 
3003   case Op_ModL:
3004     if (UseDivMod) {
3005       // Check if a%b and a/b both exist
3006       Node* d = n-&gt;find_similar(Op_DivL);
3007       if (d) {
3008         // Replace them with a fused divmod if supported
3009         if (Matcher::has_match_rule(Op_DivModL)) {
3010           DivModLNode* divmod = DivModLNode::make(n);
3011           d-&gt;subsume_by(divmod-&gt;div_proj(), this);
3012           n-&gt;subsume_by(divmod-&gt;mod_proj(), this);
3013         } else {
3014           // replace a%b with a-((a/b)*b)
3015           Node* mult = new MulLNode(d, d-&gt;in(2));
3016           Node* sub  = new SubLNode(d-&gt;in(1), mult);
3017           n-&gt;subsume_by(sub, this);
3018         }
3019       }
3020     }
3021     break;
3022 
3023   case Op_LoadVector:
3024   case Op_StoreVector:
3025     break;
3026 
3027   case Op_AddReductionVI:
3028   case Op_AddReductionVL:
3029   case Op_AddReductionVF:
3030   case Op_AddReductionVD:
3031   case Op_MulReductionVI:
3032   case Op_MulReductionVL:
3033   case Op_MulReductionVF:
3034   case Op_MulReductionVD:
3035   case Op_MinReductionV:
3036   case Op_MaxReductionV:
3037     break;
3038 
3039   case Op_PackB:
3040   case Op_PackS:
3041   case Op_PackI:
3042   case Op_PackF:
3043   case Op_PackL:
3044   case Op_PackD:
3045     if (n-&gt;req()-1 &gt; 2) {
3046       // Replace many operand PackNodes with a binary tree for matching
3047       PackNode* p = (PackNode*) n;
3048       Node* btp = p-&gt;binary_tree_pack(1, n-&gt;req());
3049       n-&gt;subsume_by(btp, this);
3050     }
3051     break;
3052   case Op_Loop:
3053   case Op_CountedLoop:
3054   case Op_OuterStripMinedLoop:
3055     if (n-&gt;as_Loop()-&gt;is_inner_loop()) {
3056       frc.inc_inner_loop_count();
3057     }
3058     n-&gt;as_Loop()-&gt;verify_strip_mined(0);
3059     break;
3060   case Op_LShiftI:
3061   case Op_RShiftI:
3062   case Op_URShiftI:
3063   case Op_LShiftL:
3064   case Op_RShiftL:
3065   case Op_URShiftL:
3066     if (Matcher::need_masked_shift_count) {
3067       // The cpu&#39;s shift instructions don&#39;t restrict the count to the
3068       // lower 5/6 bits. We need to do the masking ourselves.
3069       Node* in2 = n-&gt;in(2);
3070       juint mask = (n-&gt;bottom_type() == TypeInt::INT) ? (BitsPerInt - 1) : (BitsPerLong - 1);
3071       const TypeInt* t = in2-&gt;find_int_type();
3072       if (t != NULL &amp;&amp; t-&gt;is_con()) {
3073         juint shift = t-&gt;get_con();
3074         if (shift &gt; mask) { // Unsigned cmp
3075           n-&gt;set_req(2, ConNode::make(TypeInt::make(shift &amp; mask)));
3076         }
3077       } else {
3078         if (t == NULL || t-&gt;_lo &lt; 0 || t-&gt;_hi &gt; (int)mask) {
3079           Node* shift = new AndINode(in2, ConNode::make(TypeInt::make(mask)));
3080           n-&gt;set_req(2, shift);
3081         }
3082       }
3083       if (in2-&gt;outcnt() == 0) { // Remove dead node
3084         in2-&gt;disconnect_inputs(NULL, this);
3085       }
3086     }
3087     break;
3088   case Op_MemBarStoreStore:
3089   case Op_MemBarRelease:
3090     // Break the link with AllocateNode: it is no longer useful and
3091     // confuses register allocation.
3092     if (n-&gt;req() &gt; MemBarNode::Precedent) {
3093       n-&gt;set_req(MemBarNode::Precedent, top());
3094     }
3095     break;
3096   case Op_MemBarAcquire: {
3097     if (n-&gt;as_MemBar()-&gt;trailing_load() &amp;&amp; n-&gt;req() &gt; MemBarNode::Precedent) {
3098       // At parse time, the trailing MemBarAcquire for a volatile load
3099       // is created with an edge to the load. After optimizations,
3100       // that input may be a chain of Phis. If those phis have no
3101       // other use, then the MemBarAcquire keeps them alive and
3102       // register allocation can be confused.
3103       ResourceMark rm;
3104       Unique_Node_List wq;
3105       wq.push(n-&gt;in(MemBarNode::Precedent));
3106       n-&gt;set_req(MemBarNode::Precedent, top());
3107       while (wq.size() &gt; 0) {
3108         Node* m = wq.pop();
3109         if (m-&gt;outcnt() == 0) {
3110           for (uint j = 0; j &lt; m-&gt;req(); j++) {
3111             Node* in = m-&gt;in(j);
3112             if (in != NULL) {
3113               wq.push(in);
3114             }
3115           }
3116           m-&gt;disconnect_inputs(NULL, this);
3117         }
3118       }
3119     }
3120     break;
3121   }
3122   case Op_RangeCheck: {
3123     RangeCheckNode* rc = n-&gt;as_RangeCheck();
3124     Node* iff = new IfNode(rc-&gt;in(0), rc-&gt;in(1), rc-&gt;_prob, rc-&gt;_fcnt);
3125     n-&gt;subsume_by(iff, this);
3126     frc._tests.push(iff);
3127     break;
3128   }
3129   case Op_ConvI2L: {
3130     if (!Matcher::convi2l_type_required) {
3131       // Code generation on some platforms doesn&#39;t need accurate
3132       // ConvI2L types. Widening the type can help remove redundant
3133       // address computations.
3134       n-&gt;as_Type()-&gt;set_type(TypeLong::INT);
3135       ResourceMark rm;
3136       Unique_Node_List wq;
3137       wq.push(n);
3138       for (uint next = 0; next &lt; wq.size(); next++) {
3139         Node *m = wq.at(next);
3140 
3141         for(;;) {
3142           // Loop over all nodes with identical inputs edges as m
3143           Node* k = m-&gt;find_similar(m-&gt;Opcode());
3144           if (k == NULL) {
3145             break;
3146           }
3147           // Push their uses so we get a chance to remove node made
3148           // redundant
3149           for (DUIterator_Fast imax, i = k-&gt;fast_outs(imax); i &lt; imax; i++) {
3150             Node* u = k-&gt;fast_out(i);
3151             if (u-&gt;Opcode() == Op_LShiftL ||
3152                 u-&gt;Opcode() == Op_AddL ||
3153                 u-&gt;Opcode() == Op_SubL ||
3154                 u-&gt;Opcode() == Op_AddP) {
3155               wq.push(u);
3156             }
3157           }
3158           // Replace all nodes with identical edges as m with m
3159           k-&gt;subsume_by(m, this);
3160         }
3161       }
3162     }
3163     break;
3164   }
3165   case Op_CmpUL: {
3166     if (!Matcher::has_match_rule(Op_CmpUL)) {
3167       // No support for unsigned long comparisons
3168       ConINode* sign_pos = new ConINode(TypeInt::make(BitsPerLong - 1));
3169       Node* sign_bit_mask = new RShiftLNode(n-&gt;in(1), sign_pos);
3170       Node* orl = new OrLNode(n-&gt;in(1), sign_bit_mask);
3171       ConLNode* remove_sign_mask = new ConLNode(TypeLong::make(max_jlong));
3172       Node* andl = new AndLNode(orl, remove_sign_mask);
3173       Node* cmp = new CmpLNode(andl, n-&gt;in(2));
3174       n-&gt;subsume_by(cmp, this);
3175     }
3176     break;
3177   }
3178   default:
3179     assert(!n-&gt;is_Call(), &quot;&quot;);
3180     assert(!n-&gt;is_Mem(), &quot;&quot;);
3181     assert(nop != Op_ProfileBoolean, &quot;should be eliminated during IGVN&quot;);
3182     break;
3183   }
3184 }
3185 
3186 //------------------------------final_graph_reshaping_walk---------------------
3187 // Replacing Opaque nodes with their input in final_graph_reshaping_impl(),
3188 // requires that the walk visits a node&#39;s inputs before visiting the node.
3189 void Compile::final_graph_reshaping_walk( Node_Stack &amp;nstack, Node *root, Final_Reshape_Counts &amp;frc ) {
3190   ResourceArea *area = Thread::current()-&gt;resource_area();
3191   Unique_Node_List sfpt(area);
3192 
3193   frc._visited.set(root-&gt;_idx); // first, mark node as visited
3194   uint cnt = root-&gt;req();
3195   Node *n = root;
3196   uint  i = 0;
3197   while (true) {
3198     if (i &lt; cnt) {
3199       // Place all non-visited non-null inputs onto stack
3200       Node* m = n-&gt;in(i);
3201       ++i;
3202       if (m != NULL &amp;&amp; !frc._visited.test_set(m-&gt;_idx)) {
3203         if (m-&gt;is_SafePoint() &amp;&amp; m-&gt;as_SafePoint()-&gt;jvms() != NULL) {
3204           // compute worst case interpreter size in case of a deoptimization
3205           update_interpreter_frame_size(m-&gt;as_SafePoint()-&gt;jvms()-&gt;interpreter_frame_size());
3206 
3207           sfpt.push(m);
3208         }
3209         cnt = m-&gt;req();
3210         nstack.push(n, i); // put on stack parent and next input&#39;s index
3211         n = m;
3212         i = 0;
3213       }
3214     } else {
3215       // Now do post-visit work
3216       final_graph_reshaping_impl( n, frc );
3217       if (nstack.is_empty())
3218         break;             // finished
3219       n = nstack.node();   // Get node from stack
3220       cnt = n-&gt;req();
3221       i = nstack.index();
3222       nstack.pop();        // Shift to the next node on stack
3223     }
3224   }
3225 
3226   // Skip next transformation if compressed oops are not used.
3227   if ((UseCompressedOops &amp;&amp; !Matcher::gen_narrow_oop_implicit_null_checks()) ||
3228       (!UseCompressedOops &amp;&amp; !UseCompressedClassPointers))
3229     return;
3230 
3231   // Go over safepoints nodes to skip DecodeN/DecodeNKlass nodes for debug edges.
3232   // It could be done for an uncommon traps or any safepoints/calls
3233   // if the DecodeN/DecodeNKlass node is referenced only in a debug info.
3234   while (sfpt.size() &gt; 0) {
3235     n = sfpt.pop();
3236     JVMState *jvms = n-&gt;as_SafePoint()-&gt;jvms();
3237     assert(jvms != NULL, &quot;sanity&quot;);
3238     int start = jvms-&gt;debug_start();
3239     int end   = n-&gt;req();
3240     bool is_uncommon = (n-&gt;is_CallStaticJava() &amp;&amp;
3241                         n-&gt;as_CallStaticJava()-&gt;uncommon_trap_request() != 0);
3242     for (int j = start; j &lt; end; j++) {
3243       Node* in = n-&gt;in(j);
3244       if (in-&gt;is_DecodeNarrowPtr()) {
3245         bool safe_to_skip = true;
3246         if (!is_uncommon ) {
3247           // Is it safe to skip?
3248           for (uint i = 0; i &lt; in-&gt;outcnt(); i++) {
3249             Node* u = in-&gt;raw_out(i);
3250             if (!u-&gt;is_SafePoint() ||
3251                 (u-&gt;is_Call() &amp;&amp; u-&gt;as_Call()-&gt;has_non_debug_use(n))) {
3252               safe_to_skip = false;
3253             }
3254           }
3255         }
3256         if (safe_to_skip) {
3257           n-&gt;set_req(j, in-&gt;in(1));
3258         }
3259         if (in-&gt;outcnt() == 0) {
3260           in-&gt;disconnect_inputs(NULL, this);
3261         }
3262       }
3263     }
3264   }
3265 }
3266 
3267 //------------------------------final_graph_reshaping--------------------------
3268 // Final Graph Reshaping.
3269 //
3270 // (1) Clone simple inputs to uncommon calls, so they can be scheduled late
3271 //     and not commoned up and forced early.  Must come after regular
3272 //     optimizations to avoid GVN undoing the cloning.  Clone constant
3273 //     inputs to Loop Phis; these will be split by the allocator anyways.
3274 //     Remove Opaque nodes.
3275 // (2) Move last-uses by commutative operations to the left input to encourage
3276 //     Intel update-in-place two-address operations and better register usage
3277 //     on RISCs.  Must come after regular optimizations to avoid GVN Ideal
3278 //     calls canonicalizing them back.
3279 // (3) Count the number of double-precision FP ops, single-precision FP ops
3280 //     and call sites.  On Intel, we can get correct rounding either by
3281 //     forcing singles to memory (requires extra stores and loads after each
3282 //     FP bytecode) or we can set a rounding mode bit (requires setting and
3283 //     clearing the mode bit around call sites).  The mode bit is only used
3284 //     if the relative frequency of single FP ops to calls is low enough.
3285 //     This is a key transform for SPEC mpeg_audio.
3286 // (4) Detect infinite loops; blobs of code reachable from above but not
3287 //     below.  Several of the Code_Gen algorithms fail on such code shapes,
3288 //     so we simply bail out.  Happens a lot in ZKM.jar, but also happens
3289 //     from time to time in other codes (such as -Xcomp finalizer loops, etc).
3290 //     Detection is by looking for IfNodes where only 1 projection is
3291 //     reachable from below or CatchNodes missing some targets.
3292 // (5) Assert for insane oop offsets in debug mode.
3293 
3294 bool Compile::final_graph_reshaping() {
3295   // an infinite loop may have been eliminated by the optimizer,
3296   // in which case the graph will be empty.
3297   if (root()-&gt;req() == 1) {
3298     record_method_not_compilable(&quot;trivial infinite loop&quot;);
3299     return true;
3300   }
3301 
3302   // Expensive nodes have their control input set to prevent the GVN
3303   // from freely commoning them. There&#39;s no GVN beyond this point so
3304   // no need to keep the control input. We want the expensive nodes to
3305   // be freely moved to the least frequent code path by gcm.
3306   assert(OptimizeExpensiveOps || expensive_count() == 0, &quot;optimization off but list non empty?&quot;);
3307   for (int i = 0; i &lt; expensive_count(); i++) {
3308     _expensive_nodes-&gt;at(i)-&gt;set_req(0, NULL);
3309   }
3310 
3311   Final_Reshape_Counts frc;
3312 
3313   // Visit everybody reachable!
3314   // Allocate stack of size C-&gt;live_nodes()/2 to avoid frequent realloc
3315   Node_Stack nstack(live_nodes() &gt;&gt; 1);
3316   final_graph_reshaping_walk(nstack, root(), frc);
3317 
3318   // Check for unreachable (from below) code (i.e., infinite loops).
3319   for( uint i = 0; i &lt; frc._tests.size(); i++ ) {
3320     MultiBranchNode *n = frc._tests[i]-&gt;as_MultiBranch();
3321     // Get number of CFG targets.
3322     // Note that PCTables include exception targets after calls.
3323     uint required_outcnt = n-&gt;required_outcnt();
3324     if (n-&gt;outcnt() != required_outcnt) {
3325       // Check for a few special cases.  Rethrow Nodes never take the
3326       // &#39;fall-thru&#39; path, so expected kids is 1 less.
3327       if (n-&gt;is_PCTable() &amp;&amp; n-&gt;in(0) &amp;&amp; n-&gt;in(0)-&gt;in(0)) {
3328         if (n-&gt;in(0)-&gt;in(0)-&gt;is_Call()) {
3329           CallNode *call = n-&gt;in(0)-&gt;in(0)-&gt;as_Call();
3330           if (call-&gt;entry_point() == OptoRuntime::rethrow_stub()) {
3331             required_outcnt--;      // Rethrow always has 1 less kid
3332           } else if (call-&gt;req() &gt; TypeFunc::Parms &amp;&amp;
3333                      call-&gt;is_CallDynamicJava()) {
3334             // Check for null receiver. In such case, the optimizer has
3335             // detected that the virtual call will always result in a null
3336             // pointer exception. The fall-through projection of this CatchNode
3337             // will not be populated.
3338             Node *arg0 = call-&gt;in(TypeFunc::Parms);
3339             if (arg0-&gt;is_Type() &amp;&amp;
3340                 arg0-&gt;as_Type()-&gt;type()-&gt;higher_equal(TypePtr::NULL_PTR)) {
3341               required_outcnt--;
3342             }
3343           } else if (call-&gt;entry_point() == OptoRuntime::new_array_Java() &amp;&amp;
3344                      call-&gt;req() &gt; TypeFunc::Parms+1 &amp;&amp;
3345                      call-&gt;is_CallStaticJava()) {
3346             // Check for negative array length. In such case, the optimizer has
3347             // detected that the allocation attempt will always result in an
3348             // exception. There is no fall-through projection of this CatchNode .
3349             Node *arg1 = call-&gt;in(TypeFunc::Parms+1);
3350             if (arg1-&gt;is_Type() &amp;&amp;
3351                 arg1-&gt;as_Type()-&gt;type()-&gt;join(TypeInt::POS)-&gt;empty()) {
3352               required_outcnt--;
3353             }
3354           }
3355         }
3356       }
3357       // Recheck with a better notion of &#39;required_outcnt&#39;
3358       if (n-&gt;outcnt() != required_outcnt) {
3359         record_method_not_compilable(&quot;malformed control flow&quot;);
3360         return true;            // Not all targets reachable!
3361       }
3362     }
3363     // Check that I actually visited all kids.  Unreached kids
3364     // must be infinite loops.
3365     for (DUIterator_Fast jmax, j = n-&gt;fast_outs(jmax); j &lt; jmax; j++)
3366       if (!frc._visited.test(n-&gt;fast_out(j)-&gt;_idx)) {
3367         record_method_not_compilable(&quot;infinite loop&quot;);
3368         return true;            // Found unvisited kid; must be unreach
3369       }
3370 
3371     // Here so verification code in final_graph_reshaping_walk()
3372     // always see an OuterStripMinedLoopEnd
3373     if (n-&gt;is_OuterStripMinedLoopEnd()) {
3374       IfNode* init_iff = n-&gt;as_If();
3375       Node* iff = new IfNode(init_iff-&gt;in(0), init_iff-&gt;in(1), init_iff-&gt;_prob, init_iff-&gt;_fcnt);
3376       n-&gt;subsume_by(iff, this);
3377     }
3378   }
3379 
3380 #ifdef IA32
3381   // If original bytecodes contained a mixture of floats and doubles
3382   // check if the optimizer has made it homogenous, item (3).
3383   if (UseSSE == 0 &amp;&amp;
3384       frc.get_float_count() &gt; 32 &amp;&amp;
3385       frc.get_double_count() == 0 &amp;&amp;
3386       (10 * frc.get_call_count() &lt; frc.get_float_count()) ) {
3387     set_24_bit_selection_and_mode(false, true);
3388   }
3389 #endif // IA32
3390 
3391   set_java_calls(frc.get_java_call_count());
3392   set_inner_loops(frc.get_inner_loop_count());
3393 
3394   // No infinite loops, no reason to bail out.
3395   return false;
3396 }
3397 
3398 //-----------------------------too_many_traps----------------------------------
3399 // Report if there are too many traps at the current method and bci.
3400 // Return true if there was a trap, and/or PerMethodTrapLimit is exceeded.
3401 bool Compile::too_many_traps(ciMethod* method,
3402                              int bci,
3403                              Deoptimization::DeoptReason reason) {
3404   ciMethodData* md = method-&gt;method_data();
3405   if (md-&gt;is_empty()) {
3406     // Assume the trap has not occurred, or that it occurred only
3407     // because of a transient condition during start-up in the interpreter.
3408     return false;
3409   }
3410   ciMethod* m = Deoptimization::reason_is_speculate(reason) ? this-&gt;method() : NULL;
3411   if (md-&gt;has_trap_at(bci, m, reason) != 0) {
3412     // Assume PerBytecodeTrapLimit==0, for a more conservative heuristic.
3413     // Also, if there are multiple reasons, or if there is no per-BCI record,
3414     // assume the worst.
3415     if (log())
3416       log()-&gt;elem(&quot;observe trap=&#39;%s&#39; count=&#39;%d&#39;&quot;,
3417                   Deoptimization::trap_reason_name(reason),
3418                   md-&gt;trap_count(reason));
3419     return true;
3420   } else {
3421     // Ignore method/bci and see if there have been too many globally.
3422     return too_many_traps(reason, md);
3423   }
3424 }
3425 
3426 // Less-accurate variant which does not require a method and bci.
3427 bool Compile::too_many_traps(Deoptimization::DeoptReason reason,
3428                              ciMethodData* logmd) {
3429   if (trap_count(reason) &gt;= Deoptimization::per_method_trap_limit(reason)) {
3430     // Too many traps globally.
3431     // Note that we use cumulative trap_count, not just md-&gt;trap_count.
3432     if (log()) {
3433       int mcount = (logmd == NULL)? -1: (int)logmd-&gt;trap_count(reason);
3434       log()-&gt;elem(&quot;observe trap=&#39;%s&#39; count=&#39;0&#39; mcount=&#39;%d&#39; ccount=&#39;%d&#39;&quot;,
3435                   Deoptimization::trap_reason_name(reason),
3436                   mcount, trap_count(reason));
3437     }
3438     return true;
3439   } else {
3440     // The coast is clear.
3441     return false;
3442   }
3443 }
3444 
3445 //--------------------------too_many_recompiles--------------------------------
3446 // Report if there are too many recompiles at the current method and bci.
3447 // Consults PerBytecodeRecompilationCutoff and PerMethodRecompilationCutoff.
3448 // Is not eager to return true, since this will cause the compiler to use
3449 // Action_none for a trap point, to avoid too many recompilations.
3450 bool Compile::too_many_recompiles(ciMethod* method,
3451                                   int bci,
3452                                   Deoptimization::DeoptReason reason) {
3453   ciMethodData* md = method-&gt;method_data();
3454   if (md-&gt;is_empty()) {
3455     // Assume the trap has not occurred, or that it occurred only
3456     // because of a transient condition during start-up in the interpreter.
3457     return false;
3458   }
3459   // Pick a cutoff point well within PerBytecodeRecompilationCutoff.
3460   uint bc_cutoff = (uint) PerBytecodeRecompilationCutoff / 8;
3461   uint m_cutoff  = (uint) PerMethodRecompilationCutoff / 2 + 1;  // not zero
3462   Deoptimization::DeoptReason per_bc_reason
3463     = Deoptimization::reason_recorded_per_bytecode_if_any(reason);
3464   ciMethod* m = Deoptimization::reason_is_speculate(reason) ? this-&gt;method() : NULL;
3465   if ((per_bc_reason == Deoptimization::Reason_none
3466        || md-&gt;has_trap_at(bci, m, reason) != 0)
3467       // The trap frequency measure we care about is the recompile count:
3468       &amp;&amp; md-&gt;trap_recompiled_at(bci, m)
3469       &amp;&amp; md-&gt;overflow_recompile_count() &gt;= bc_cutoff) {
3470     // Do not emit a trap here if it has already caused recompilations.
3471     // Also, if there are multiple reasons, or if there is no per-BCI record,
3472     // assume the worst.
3473     if (log())
3474       log()-&gt;elem(&quot;observe trap=&#39;%s recompiled&#39; count=&#39;%d&#39; recompiles2=&#39;%d&#39;&quot;,
3475                   Deoptimization::trap_reason_name(reason),
3476                   md-&gt;trap_count(reason),
3477                   md-&gt;overflow_recompile_count());
3478     return true;
3479   } else if (trap_count(reason) != 0
3480              &amp;&amp; decompile_count() &gt;= m_cutoff) {
3481     // Too many recompiles globally, and we have seen this sort of trap.
3482     // Use cumulative decompile_count, not just md-&gt;decompile_count.
3483     if (log())
3484       log()-&gt;elem(&quot;observe trap=&#39;%s&#39; count=&#39;%d&#39; mcount=&#39;%d&#39; decompiles=&#39;%d&#39; mdecompiles=&#39;%d&#39;&quot;,
3485                   Deoptimization::trap_reason_name(reason),
3486                   md-&gt;trap_count(reason), trap_count(reason),
3487                   md-&gt;decompile_count(), decompile_count());
3488     return true;
3489   } else {
3490     // The coast is clear.
3491     return false;
3492   }
3493 }
3494 
3495 // Compute when not to trap. Used by matching trap based nodes and
3496 // NullCheck optimization.
3497 void Compile::set_allowed_deopt_reasons() {
3498   _allowed_reasons = 0;
3499   if (is_method_compilation()) {
3500     for (int rs = (int)Deoptimization::Reason_none+1; rs &lt; Compile::trapHistLength; rs++) {
3501       assert(rs &lt; BitsPerInt, &quot;recode bit map&quot;);
3502       if (!too_many_traps((Deoptimization::DeoptReason) rs)) {
3503         _allowed_reasons |= nth_bit(rs);
3504       }
3505     }
3506   }
3507 }
3508 
3509 bool Compile::needs_clinit_barrier(ciMethod* method, ciMethod* accessing_method) {
3510   return method-&gt;is_static() &amp;&amp; needs_clinit_barrier(method-&gt;holder(), accessing_method);
3511 }
3512 
3513 bool Compile::needs_clinit_barrier(ciField* field, ciMethod* accessing_method) {
3514   return field-&gt;is_static() &amp;&amp; needs_clinit_barrier(field-&gt;holder(), accessing_method);
3515 }
3516 
3517 bool Compile::needs_clinit_barrier(ciInstanceKlass* holder, ciMethod* accessing_method) {
3518   if (holder-&gt;is_initialized()) {
3519     return false;
3520   }
3521   if (holder-&gt;is_being_initialized()) {
3522     if (accessing_method-&gt;holder() == holder) {
3523       // Access inside a class. The barrier can be elided when access happens in &lt;clinit&gt;,
3524       // &lt;init&gt;, or a static method. In all those cases, there was an initialization
3525       // barrier on the holder klass passed.
3526       if (accessing_method-&gt;is_static_initializer() ||
3527           accessing_method-&gt;is_object_initializer() ||
3528           accessing_method-&gt;is_static()) {
3529         return false;
3530       }
3531     } else if (accessing_method-&gt;holder()-&gt;is_subclass_of(holder)) {
3532       // Access from a subclass. The barrier can be elided only when access happens in &lt;clinit&gt;.
3533       // In case of &lt;init&gt; or a static method, the barrier is on the subclass is not enough:
3534       // child class can become fully initialized while its parent class is still being initialized.
3535       if (accessing_method-&gt;is_static_initializer()) {
3536         return false;
3537       }
3538     }
3539     ciMethod* root = method(); // the root method of compilation
3540     if (root != accessing_method) {
3541       return needs_clinit_barrier(holder, root); // check access in the context of compilation root
3542     }
3543   }
3544   return true;
3545 }
3546 
3547 #ifndef PRODUCT
3548 //------------------------------verify_graph_edges---------------------------
3549 // Walk the Graph and verify that there is a one-to-one correspondence
3550 // between Use-Def edges and Def-Use edges in the graph.
3551 void Compile::verify_graph_edges(bool no_dead_code) {
3552   if (VerifyGraphEdges) {
3553     ResourceArea *area = Thread::current()-&gt;resource_area();
3554     Unique_Node_List visited(area);
3555     // Call recursive graph walk to check edges
3556     _root-&gt;verify_edges(visited);
3557     if (no_dead_code) {
3558       // Now make sure that no visited node is used by an unvisited node.
3559       bool dead_nodes = false;
3560       Unique_Node_List checked(area);
3561       while (visited.size() &gt; 0) {
3562         Node* n = visited.pop();
3563         checked.push(n);
3564         for (uint i = 0; i &lt; n-&gt;outcnt(); i++) {
3565           Node* use = n-&gt;raw_out(i);
3566           if (checked.member(use))  continue;  // already checked
3567           if (visited.member(use))  continue;  // already in the graph
3568           if (use-&gt;is_Con())        continue;  // a dead ConNode is OK
3569           // At this point, we have found a dead node which is DU-reachable.
3570           if (!dead_nodes) {
3571             tty-&gt;print_cr(&quot;*** Dead nodes reachable via DU edges:&quot;);
3572             dead_nodes = true;
3573           }
3574           use-&gt;dump(2);
3575           tty-&gt;print_cr(&quot;---&quot;);
3576           checked.push(use);  // No repeats; pretend it is now checked.
3577         }
3578       }
3579       assert(!dead_nodes, &quot;using nodes must be reachable from root&quot;);
3580     }
3581   }
3582 }
3583 #endif
3584 
3585 // The Compile object keeps track of failure reasons separately from the ciEnv.
3586 // This is required because there is not quite a 1-1 relation between the
3587 // ciEnv and its compilation task and the Compile object.  Note that one
3588 // ciEnv might use two Compile objects, if C2Compiler::compile_method decides
3589 // to backtrack and retry without subsuming loads.  Other than this backtracking
3590 // behavior, the Compile&#39;s failure reason is quietly copied up to the ciEnv
3591 // by the logic in C2Compiler.
3592 void Compile::record_failure(const char* reason) {
3593   if (log() != NULL) {
3594     log()-&gt;elem(&quot;failure reason=&#39;%s&#39; phase=&#39;compile&#39;&quot;, reason);
3595   }
3596   if (_failure_reason == NULL) {
3597     // Record the first failure reason.
3598     _failure_reason = reason;
3599   }
3600 
3601   if (!C-&gt;failure_reason_is(C2Compiler::retry_no_subsuming_loads())) {
3602     C-&gt;print_method(PHASE_FAILURE);
3603   }
3604   _root = NULL;  // flush the graph, too
3605 }
3606 
3607 Compile::TracePhase::TracePhase(const char* name, elapsedTimer* accumulator)
3608   : TraceTime(name, accumulator, CITime, CITimeVerbose),
3609     _phase_name(name), _dolog(CITimeVerbose)
3610 {
3611   if (_dolog) {
3612     C = Compile::current();
3613     _log = C-&gt;log();
3614   } else {
3615     C = NULL;
3616     _log = NULL;
3617   }
3618   if (_log != NULL) {
3619     _log-&gt;begin_head(&quot;phase name=&#39;%s&#39; nodes=&#39;%d&#39; live=&#39;%d&#39;&quot;, _phase_name, C-&gt;unique(), C-&gt;live_nodes());
3620     _log-&gt;stamp();
3621     _log-&gt;end_head();
3622   }
3623 }
3624 
3625 Compile::TracePhase::~TracePhase() {
3626 
3627   C = Compile::current();
3628   if (_dolog) {
3629     _log = C-&gt;log();
3630   } else {
3631     _log = NULL;
3632   }
3633 
3634 #ifdef ASSERT
3635   if (PrintIdealNodeCount) {
3636     tty-&gt;print_cr(&quot;phase name=&#39;%s&#39; nodes=&#39;%d&#39; live=&#39;%d&#39; live_graph_walk=&#39;%d&#39;&quot;,
3637                   _phase_name, C-&gt;unique(), C-&gt;live_nodes(), C-&gt;count_live_nodes_by_graph_walk());
3638   }
3639 
3640   if (VerifyIdealNodeCount) {
3641     Compile::current()-&gt;print_missing_nodes();
3642   }
3643 #endif
3644 
3645   if (_log != NULL) {
3646     _log-&gt;done(&quot;phase name=&#39;%s&#39; nodes=&#39;%d&#39; live=&#39;%d&#39;&quot;, _phase_name, C-&gt;unique(), C-&gt;live_nodes());
3647   }
3648 }
3649 
3650 //----------------------------static_subtype_check-----------------------------
3651 // Shortcut important common cases when superklass is exact:
3652 // (0) superklass is java.lang.Object (can occur in reflective code)
3653 // (1) subklass is already limited to a subtype of superklass =&gt; always ok
3654 // (2) subklass does not overlap with superklass =&gt; always fail
3655 // (3) superklass has NO subtypes and we can check with a simple compare.
3656 int Compile::static_subtype_check(ciKlass* superk, ciKlass* subk) {
3657   if (StressReflectiveCode) {
3658     return SSC_full_test;       // Let caller generate the general case.
3659   }
3660 
3661   if (superk == env()-&gt;Object_klass()) {
3662     return SSC_always_true;     // (0) this test cannot fail
3663   }
3664 
3665   ciType* superelem = superk;
3666   if (superelem-&gt;is_array_klass())
3667     superelem = superelem-&gt;as_array_klass()-&gt;base_element_type();
3668 
3669   if (!subk-&gt;is_interface()) {  // cannot trust static interface types yet
3670     if (subk-&gt;is_subtype_of(superk)) {
3671       return SSC_always_true;   // (1) false path dead; no dynamic test needed
3672     }
3673     if (!(superelem-&gt;is_klass() &amp;&amp; superelem-&gt;as_klass()-&gt;is_interface()) &amp;&amp;
3674         !superk-&gt;is_subtype_of(subk)) {
3675       return SSC_always_false;
3676     }
3677   }
3678 
3679   // If casting to an instance klass, it must have no subtypes
3680   if (superk-&gt;is_interface()) {
3681     // Cannot trust interfaces yet.
3682     // %%% S.B. superk-&gt;nof_implementors() == 1
3683   } else if (superelem-&gt;is_instance_klass()) {
3684     ciInstanceKlass* ik = superelem-&gt;as_instance_klass();
3685     if (!ik-&gt;has_subklass() &amp;&amp; !ik-&gt;is_interface()) {
3686       if (!ik-&gt;is_final()) {
3687         // Add a dependency if there is a chance of a later subclass.
3688         dependencies()-&gt;assert_leaf_type(ik);
3689       }
3690       return SSC_easy_test;     // (3) caller can do a simple ptr comparison
3691     }
3692   } else {
3693     // A primitive array type has no subtypes.
3694     return SSC_easy_test;       // (3) caller can do a simple ptr comparison
3695   }
3696 
3697   return SSC_full_test;
3698 }
3699 
3700 Node* Compile::conv_I2X_index(PhaseGVN* phase, Node* idx, const TypeInt* sizetype, Node* ctrl) {
3701 #ifdef _LP64
3702   // The scaled index operand to AddP must be a clean 64-bit value.
3703   // Java allows a 32-bit int to be incremented to a negative
3704   // value, which appears in a 64-bit register as a large
3705   // positive number.  Using that large positive number as an
3706   // operand in pointer arithmetic has bad consequences.
3707   // On the other hand, 32-bit overflow is rare, and the possibility
3708   // can often be excluded, if we annotate the ConvI2L node with
3709   // a type assertion that its value is known to be a small positive
3710   // number.  (The prior range check has ensured this.)
3711   // This assertion is used by ConvI2LNode::Ideal.
3712   int index_max = max_jint - 1;  // array size is max_jint, index is one less
3713   if (sizetype != NULL) index_max = sizetype-&gt;_hi - 1;
3714   const TypeInt* iidxtype = TypeInt::make(0, index_max, Type::WidenMax);
3715   idx = constrained_convI2L(phase, idx, iidxtype, ctrl);
3716 #endif
3717   return idx;
3718 }
3719 
3720 // Convert integer value to a narrowed long type dependent on ctrl (for example, a range check)
3721 Node* Compile::constrained_convI2L(PhaseGVN* phase, Node* value, const TypeInt* itype, Node* ctrl) {
3722   if (ctrl != NULL) {
3723     // Express control dependency by a CastII node with a narrow type.
3724     value = new CastIINode(value, itype, false, true /* range check dependency */);
3725     // Make the CastII node dependent on the control input to prevent the narrowed ConvI2L
3726     // node from floating above the range check during loop optimizations. Otherwise, the
3727     // ConvI2L node may be eliminated independently of the range check, causing the data path
3728     // to become TOP while the control path is still there (although it&#39;s unreachable).
3729     value-&gt;set_req(0, ctrl);
3730     // Save CastII node to remove it after loop optimizations.
3731     phase-&gt;C-&gt;add_range_check_cast(value);
3732     value = phase-&gt;transform(value);
3733   }
3734   const TypeLong* ltype = TypeLong::make(itype-&gt;_lo, itype-&gt;_hi, itype-&gt;_widen);
3735   return phase-&gt;transform(new ConvI2LNode(value, ltype));
3736 }
3737 
3738 void Compile::print_inlining_stream_free() {
3739   if (_print_inlining_stream != NULL) {
3740     _print_inlining_stream-&gt;~stringStream();
3741     _print_inlining_stream = NULL;
3742   }
3743 }
3744 
3745 // The message about the current inlining is accumulated in
3746 // _print_inlining_stream and transfered into the _print_inlining_list
3747 // once we know whether inlining succeeds or not. For regular
3748 // inlining, messages are appended to the buffer pointed by
3749 // _print_inlining_idx in the _print_inlining_list. For late inlining,
3750 // a new buffer is added after _print_inlining_idx in the list. This
3751 // way we can update the inlining message for late inlining call site
3752 // when the inlining is attempted again.
3753 void Compile::print_inlining_init() {
3754   if (print_inlining() || print_intrinsics()) {
3755     // print_inlining_init is actually called several times.
3756     print_inlining_stream_free();
3757     _print_inlining_stream = new stringStream();
3758     // Watch out: The memory initialized by the constructor call PrintInliningBuffer()
3759     // will be copied into the only initial element. The default destructor of
3760     // PrintInliningBuffer will be called when leaving the scope here. If it
3761     // would destuct the  enclosed stringStream _print_inlining_list[0]-&gt;_ss
3762     // would be destructed, too!
3763     _print_inlining_list = new (comp_arena())GrowableArray&lt;PrintInliningBuffer&gt;(comp_arena(), 1, 1, PrintInliningBuffer());
3764   }
3765 }
3766 
3767 void Compile::print_inlining_reinit() {
3768   if (print_inlining() || print_intrinsics()) {
3769     print_inlining_stream_free();
3770     // Re allocate buffer when we change ResourceMark
3771     _print_inlining_stream = new stringStream();
3772   }
3773 }
3774 
3775 void Compile::print_inlining_reset() {
3776   _print_inlining_stream-&gt;reset();
3777 }
3778 
3779 void Compile::print_inlining_commit() {
3780   assert(print_inlining() || print_intrinsics(), &quot;PrintInlining off?&quot;);
3781   // Transfer the message from _print_inlining_stream to the current
3782   // _print_inlining_list buffer and clear _print_inlining_stream.
3783   _print_inlining_list-&gt;at(_print_inlining_idx).ss()-&gt;write(_print_inlining_stream-&gt;base(), _print_inlining_stream-&gt;size());
3784   print_inlining_reset();
3785 }
3786 
3787 void Compile::print_inlining_push() {
3788   // Add new buffer to the _print_inlining_list at current position
3789   _print_inlining_idx++;
3790   _print_inlining_list-&gt;insert_before(_print_inlining_idx, PrintInliningBuffer());
3791 }
3792 
3793 Compile::PrintInliningBuffer&amp; Compile::print_inlining_current() {
3794   return _print_inlining_list-&gt;at(_print_inlining_idx);
3795 }
3796 
3797 void Compile::print_inlining_update(CallGenerator* cg) {
3798   if (print_inlining() || print_intrinsics()) {
3799     if (!cg-&gt;is_late_inline()) {
3800       if (print_inlining_current().cg() != NULL) {
3801         print_inlining_push();
3802       }
3803       print_inlining_commit();
3804     } else {
3805       if (print_inlining_current().cg() != cg &amp;&amp;
3806           (print_inlining_current().cg() != NULL ||
3807            print_inlining_current().ss()-&gt;size() != 0)) {
3808         print_inlining_push();
3809       }
3810       print_inlining_commit();
3811       print_inlining_current().set_cg(cg);
3812     }
3813   }
3814 }
3815 
3816 void Compile::print_inlining_move_to(CallGenerator* cg) {
3817   // We resume inlining at a late inlining call site. Locate the
3818   // corresponding inlining buffer so that we can update it.
3819   if (print_inlining()) {
3820     for (int i = 0; i &lt; _print_inlining_list-&gt;length(); i++) {
3821       if (_print_inlining_list-&gt;adr_at(i)-&gt;cg() == cg) {
3822         _print_inlining_idx = i;
3823         return;
3824       }
3825     }
3826     ShouldNotReachHere();
3827   }
3828 }
3829 
3830 void Compile::print_inlining_update_delayed(CallGenerator* cg) {
3831   if (print_inlining()) {
3832     assert(_print_inlining_stream-&gt;size() &gt; 0, &quot;missing inlining msg&quot;);
3833     assert(print_inlining_current().cg() == cg, &quot;wrong entry&quot;);
3834     // replace message with new message
3835     _print_inlining_list-&gt;at_put(_print_inlining_idx, PrintInliningBuffer());
3836     print_inlining_commit();
3837     print_inlining_current().set_cg(cg);
3838   }
3839 }
3840 
3841 void Compile::print_inlining_assert_ready() {
3842   assert(!_print_inlining || _print_inlining_stream-&gt;size() == 0, &quot;loosing data&quot;);
3843 }
3844 
3845 void Compile::process_print_inlining() {
3846   bool do_print_inlining = print_inlining() || print_intrinsics();
3847   if (do_print_inlining || log() != NULL) {
3848     // Print inlining message for candidates that we couldn&#39;t inline
3849     // for lack of space
3850     for (int i = 0; i &lt; _late_inlines.length(); i++) {
3851       CallGenerator* cg = _late_inlines.at(i);
3852       if (!cg-&gt;is_mh_late_inline()) {
3853         const char* msg = &quot;live nodes &gt; LiveNodeCountInliningCutoff&quot;;
3854         if (do_print_inlining) {
3855           cg-&gt;print_inlining_late(msg);
3856         }
3857         log_late_inline_failure(cg, msg);
3858       }
3859     }
3860   }
3861   if (do_print_inlining) {
3862     ResourceMark rm;
3863     stringStream ss;
3864     assert(_print_inlining_list != NULL, &quot;process_print_inlining should be called only once.&quot;);
3865     for (int i = 0; i &lt; _print_inlining_list-&gt;length(); i++) {
3866       ss.print(&quot;%s&quot;, _print_inlining_list-&gt;adr_at(i)-&gt;ss()-&gt;as_string());
3867       _print_inlining_list-&gt;at(i).freeStream();
3868     }
3869     // Reset _print_inlining_list, it only contains destructed objects.
3870     // It is on the arena, so it will be freed when the arena is reset.
3871     _print_inlining_list = NULL;
3872     // _print_inlining_stream won&#39;t be used anymore, either.
3873     print_inlining_stream_free();
3874     size_t end = ss.size();
3875     _print_inlining_output = NEW_ARENA_ARRAY(comp_arena(), char, end+1);
3876     strncpy(_print_inlining_output, ss.base(), end+1);
3877     _print_inlining_output[end] = 0;
3878   }
3879 }
3880 
3881 void Compile::dump_print_inlining() {
3882   if (_print_inlining_output != NULL) {
3883     tty-&gt;print_raw(_print_inlining_output);
3884   }
3885 }
3886 
3887 void Compile::log_late_inline(CallGenerator* cg) {
3888   if (log() != NULL) {
3889     log()-&gt;head(&quot;late_inline method=&#39;%d&#39;  inline_id=&#39;&quot; JLONG_FORMAT &quot;&#39;&quot;, log()-&gt;identify(cg-&gt;method()),
3890                 cg-&gt;unique_id());
3891     JVMState* p = cg-&gt;call_node()-&gt;jvms();
3892     while (p != NULL) {
3893       log()-&gt;elem(&quot;jvms bci=&#39;%d&#39; method=&#39;%d&#39;&quot;, p-&gt;bci(), log()-&gt;identify(p-&gt;method()));
3894       p = p-&gt;caller();
3895     }
3896     log()-&gt;tail(&quot;late_inline&quot;);
3897   }
3898 }
3899 
3900 void Compile::log_late_inline_failure(CallGenerator* cg, const char* msg) {
3901   log_late_inline(cg);
3902   if (log() != NULL) {
3903     log()-&gt;inline_fail(msg);
3904   }
3905 }
3906 
3907 void Compile::log_inline_id(CallGenerator* cg) {
3908   if (log() != NULL) {
3909     // The LogCompilation tool needs a unique way to identify late
3910     // inline call sites. This id must be unique for this call site in
3911     // this compilation. Try to have it unique across compilations as
3912     // well because it can be convenient when grepping through the log
3913     // file.
3914     // Distinguish OSR compilations from others in case CICountOSR is
3915     // on.
3916     jlong id = ((jlong)unique()) + (((jlong)compile_id()) &lt;&lt; 33) + (CICountOSR &amp;&amp; is_osr_compilation() ? ((jlong)1) &lt;&lt; 32 : 0);
3917     cg-&gt;set_unique_id(id);
3918     log()-&gt;elem(&quot;inline_id id=&#39;&quot; JLONG_FORMAT &quot;&#39;&quot;, id);
3919   }
3920 }
3921 
3922 void Compile::log_inline_failure(const char* msg) {
3923   if (C-&gt;log() != NULL) {
3924     C-&gt;log()-&gt;inline_fail(msg);
3925   }
3926 }
3927 
3928 
3929 // Dump inlining replay data to the stream.
3930 // Don&#39;t change thread state and acquire any locks.
3931 void Compile::dump_inline_data(outputStream* out) {
3932   InlineTree* inl_tree = ilt();
3933   if (inl_tree != NULL) {
3934     out-&gt;print(&quot; inline %d&quot;, inl_tree-&gt;count());
3935     inl_tree-&gt;dump_replay_data(out);
3936   }
3937 }
3938 
3939 int Compile::cmp_expensive_nodes(Node* n1, Node* n2) {
3940   if (n1-&gt;Opcode() &lt; n2-&gt;Opcode())      return -1;
3941   else if (n1-&gt;Opcode() &gt; n2-&gt;Opcode()) return 1;
3942 
3943   assert(n1-&gt;req() == n2-&gt;req(), &quot;can&#39;t compare %s nodes: n1-&gt;req() = %d, n2-&gt;req() = %d&quot;, NodeClassNames[n1-&gt;Opcode()], n1-&gt;req(), n2-&gt;req());
3944   for (uint i = 1; i &lt; n1-&gt;req(); i++) {
3945     if (n1-&gt;in(i) &lt; n2-&gt;in(i))      return -1;
3946     else if (n1-&gt;in(i) &gt; n2-&gt;in(i)) return 1;
3947   }
3948 
3949   return 0;
3950 }
3951 
3952 int Compile::cmp_expensive_nodes(Node** n1p, Node** n2p) {
3953   Node* n1 = *n1p;
3954   Node* n2 = *n2p;
3955 
3956   return cmp_expensive_nodes(n1, n2);
3957 }
3958 
3959 void Compile::sort_expensive_nodes() {
3960   if (!expensive_nodes_sorted()) {
3961     _expensive_nodes-&gt;sort(cmp_expensive_nodes);
3962   }
3963 }
3964 
3965 bool Compile::expensive_nodes_sorted() const {
3966   for (int i = 1; i &lt; _expensive_nodes-&gt;length(); i++) {
3967     if (cmp_expensive_nodes(_expensive_nodes-&gt;adr_at(i), _expensive_nodes-&gt;adr_at(i-1)) &lt; 0) {
3968       return false;
3969     }
3970   }
3971   return true;
3972 }
3973 
3974 bool Compile::should_optimize_expensive_nodes(PhaseIterGVN &amp;igvn) {
3975   if (_expensive_nodes-&gt;length() == 0) {
3976     return false;
3977   }
3978 
3979   assert(OptimizeExpensiveOps, &quot;optimization off?&quot;);
3980 
3981   // Take this opportunity to remove dead nodes from the list
3982   int j = 0;
3983   for (int i = 0; i &lt; _expensive_nodes-&gt;length(); i++) {
3984     Node* n = _expensive_nodes-&gt;at(i);
3985     if (!n-&gt;is_unreachable(igvn)) {
3986       assert(n-&gt;is_expensive(), &quot;should be expensive&quot;);
3987       _expensive_nodes-&gt;at_put(j, n);
3988       j++;
3989     }
3990   }
3991   _expensive_nodes-&gt;trunc_to(j);
3992 
3993   // Then sort the list so that similar nodes are next to each other
3994   // and check for at least two nodes of identical kind with same data
3995   // inputs.
3996   sort_expensive_nodes();
3997 
3998   for (int i = 0; i &lt; _expensive_nodes-&gt;length()-1; i++) {
3999     if (cmp_expensive_nodes(_expensive_nodes-&gt;adr_at(i), _expensive_nodes-&gt;adr_at(i+1)) == 0) {
4000       return true;
4001     }
4002   }
4003 
4004   return false;
4005 }
4006 
4007 void Compile::cleanup_expensive_nodes(PhaseIterGVN &amp;igvn) {
4008   if (_expensive_nodes-&gt;length() == 0) {
4009     return;
4010   }
4011 
4012   assert(OptimizeExpensiveOps, &quot;optimization off?&quot;);
4013 
4014   // Sort to bring similar nodes next to each other and clear the
4015   // control input of nodes for which there&#39;s only a single copy.
4016   sort_expensive_nodes();
4017 
4018   int j = 0;
4019   int identical = 0;
4020   int i = 0;
4021   bool modified = false;
4022   for (; i &lt; _expensive_nodes-&gt;length()-1; i++) {
4023     assert(j &lt;= i, &quot;can&#39;t write beyond current index&quot;);
4024     if (_expensive_nodes-&gt;at(i)-&gt;Opcode() == _expensive_nodes-&gt;at(i+1)-&gt;Opcode()) {
4025       identical++;
4026       _expensive_nodes-&gt;at_put(j++, _expensive_nodes-&gt;at(i));
4027       continue;
4028     }
4029     if (identical &gt; 0) {
4030       _expensive_nodes-&gt;at_put(j++, _expensive_nodes-&gt;at(i));
4031       identical = 0;
4032     } else {
4033       Node* n = _expensive_nodes-&gt;at(i);
4034       igvn.replace_input_of(n, 0, NULL);
4035       igvn.hash_insert(n);
4036       modified = true;
4037     }
4038   }
4039   if (identical &gt; 0) {
4040     _expensive_nodes-&gt;at_put(j++, _expensive_nodes-&gt;at(i));
4041   } else if (_expensive_nodes-&gt;length() &gt;= 1) {
4042     Node* n = _expensive_nodes-&gt;at(i);
4043     igvn.replace_input_of(n, 0, NULL);
4044     igvn.hash_insert(n);
4045     modified = true;
4046   }
4047   _expensive_nodes-&gt;trunc_to(j);
4048   if (modified) {
4049     igvn.optimize();
4050   }
4051 }
4052 
4053 void Compile::add_expensive_node(Node * n) {
4054   assert(!_expensive_nodes-&gt;contains(n), &quot;duplicate entry in expensive list&quot;);
4055   assert(n-&gt;is_expensive(), &quot;expensive nodes with non-null control here only&quot;);
4056   assert(!n-&gt;is_CFG() &amp;&amp; !n-&gt;is_Mem(), &quot;no cfg or memory nodes here&quot;);
4057   if (OptimizeExpensiveOps) {
4058     _expensive_nodes-&gt;append(n);
4059   } else {
4060     // Clear control input and let IGVN optimize expensive nodes if
4061     // OptimizeExpensiveOps is off.
4062     n-&gt;set_req(0, NULL);
4063   }
4064 }
4065 
4066 /**
4067  * Remove the speculative part of types and clean up the graph
4068  */
4069 void Compile::remove_speculative_types(PhaseIterGVN &amp;igvn) {
4070   if (UseTypeSpeculation) {
4071     Unique_Node_List worklist;
4072     worklist.push(root());
4073     int modified = 0;
4074     // Go over all type nodes that carry a speculative type, drop the
4075     // speculative part of the type and enqueue the node for an igvn
4076     // which may optimize it out.
4077     for (uint next = 0; next &lt; worklist.size(); ++next) {
4078       Node *n  = worklist.at(next);
4079       if (n-&gt;is_Type()) {
4080         TypeNode* tn = n-&gt;as_Type();
4081         const Type* t = tn-&gt;type();
4082         const Type* t_no_spec = t-&gt;remove_speculative();
4083         if (t_no_spec != t) {
4084           bool in_hash = igvn.hash_delete(n);
4085           assert(in_hash, &quot;node should be in igvn hash table&quot;);
4086           tn-&gt;set_type(t_no_spec);
4087           igvn.hash_insert(n);
4088           igvn._worklist.push(n); // give it a chance to go away
4089           modified++;
4090         }
4091       }
4092       uint max = n-&gt;len();
4093       for( uint i = 0; i &lt; max; ++i ) {
4094         Node *m = n-&gt;in(i);
4095         if (not_a_node(m))  continue;
4096         worklist.push(m);
4097       }
4098     }
4099     // Drop the speculative part of all types in the igvn&#39;s type table
4100     igvn.remove_speculative_types();
4101     if (modified &gt; 0) {
4102       igvn.optimize();
4103     }
4104 #ifdef ASSERT
4105     // Verify that after the IGVN is over no speculative type has resurfaced
4106     worklist.clear();
4107     worklist.push(root());
4108     for (uint next = 0; next &lt; worklist.size(); ++next) {
4109       Node *n  = worklist.at(next);
4110       const Type* t = igvn.type_or_null(n);
4111       assert((t == NULL) || (t == t-&gt;remove_speculative()), &quot;no more speculative types&quot;);
4112       if (n-&gt;is_Type()) {
4113         t = n-&gt;as_Type()-&gt;type();
4114         assert(t == t-&gt;remove_speculative(), &quot;no more speculative types&quot;);
4115       }
4116       uint max = n-&gt;len();
4117       for( uint i = 0; i &lt; max; ++i ) {
4118         Node *m = n-&gt;in(i);
4119         if (not_a_node(m))  continue;
4120         worklist.push(m);
4121       }
4122     }
4123     igvn.check_no_speculative_types();
4124 #endif
4125   }
4126 }
4127 
4128 // Auxiliary method to support randomized stressing/fuzzing.
4129 //
4130 // This method can be called the arbitrary number of times, with current count
4131 // as the argument. The logic allows selecting a single candidate from the
4132 // running list of candidates as follows:
4133 //    int count = 0;
4134 //    Cand* selected = null;
4135 //    while(cand = cand-&gt;next()) {
4136 //      if (randomized_select(++count)) {
4137 //        selected = cand;
4138 //      }
4139 //    }
4140 //
4141 // Including count equalizes the chances any candidate is &quot;selected&quot;.
4142 // This is useful when we don&#39;t have the complete list of candidates to choose
4143 // from uniformly. In this case, we need to adjust the randomicity of the
4144 // selection, or else we will end up biasing the selection towards the latter
4145 // candidates.
4146 //
4147 // Quick back-envelope calculation shows that for the list of n candidates
4148 // the equal probability for the candidate to persist as &quot;best&quot; can be
4149 // achieved by replacing it with &quot;next&quot; k-th candidate with the probability
4150 // of 1/k. It can be easily shown that by the end of the run, the
4151 // probability for any candidate is converged to 1/n, thus giving the
4152 // uniform distribution among all the candidates.
4153 //
4154 // We don&#39;t care about the domain size as long as (RANDOMIZED_DOMAIN / count) is large.
4155 #define RANDOMIZED_DOMAIN_POW 29
4156 #define RANDOMIZED_DOMAIN (1 &lt;&lt; RANDOMIZED_DOMAIN_POW)
4157 #define RANDOMIZED_DOMAIN_MASK ((1 &lt;&lt; (RANDOMIZED_DOMAIN_POW + 1)) - 1)
4158 bool Compile::randomized_select(int count) {
4159   assert(count &gt; 0, &quot;only positive&quot;);
4160   return (os::random() &amp; RANDOMIZED_DOMAIN_MASK) &lt; (RANDOMIZED_DOMAIN / count);
4161 }
4162 
4163 CloneMap&amp;     Compile::clone_map()                 { return _clone_map; }
4164 void          Compile::set_clone_map(Dict* d)      { _clone_map._dict = d; }
4165 
4166 void NodeCloneInfo::dump() const {
4167   tty-&gt;print(&quot; {%d:%d} &quot;, idx(), gen());
4168 }
4169 
4170 void CloneMap::clone(Node* old, Node* nnn, int gen) {
4171   uint64_t val = value(old-&gt;_idx);
4172   NodeCloneInfo cio(val);
4173   assert(val != 0, &quot;old node should be in the map&quot;);
4174   NodeCloneInfo cin(cio.idx(), gen + cio.gen());
4175   insert(nnn-&gt;_idx, cin.get());
4176 #ifndef PRODUCT
4177   if (is_debug()) {
4178     tty-&gt;print_cr(&quot;CloneMap::clone inserted node %d info {%d:%d} into CloneMap&quot;, nnn-&gt;_idx, cin.idx(), cin.gen());
4179   }
4180 #endif
4181 }
4182 
4183 void CloneMap::verify_insert_and_clone(Node* old, Node* nnn, int gen) {
4184   NodeCloneInfo cio(value(old-&gt;_idx));
4185   if (cio.get() == 0) {
4186     cio.set(old-&gt;_idx, 0);
4187     insert(old-&gt;_idx, cio.get());
4188 #ifndef PRODUCT
4189     if (is_debug()) {
4190       tty-&gt;print_cr(&quot;CloneMap::verify_insert_and_clone inserted node %d info {%d:%d} into CloneMap&quot;, old-&gt;_idx, cio.idx(), cio.gen());
4191     }
4192 #endif
4193   }
4194   clone(old, nnn, gen);
4195 }
4196 
4197 int CloneMap::max_gen() const {
4198   int g = 0;
4199   DictI di(_dict);
4200   for(; di.test(); ++di) {
4201     int t = gen(di._key);
4202     if (g &lt; t) {
4203       g = t;
4204 #ifndef PRODUCT
4205       if (is_debug()) {
4206         tty-&gt;print_cr(&quot;CloneMap::max_gen() update max=%d from %d&quot;, g, _2_node_idx_t(di._key));
4207       }
4208 #endif
4209     }
4210   }
4211   return g;
4212 }
4213 
4214 void CloneMap::dump(node_idx_t key) const {
4215   uint64_t val = value(key);
4216   if (val != 0) {
4217     NodeCloneInfo ni(val);
4218     ni.dump();
4219   }
4220 }
    </pre>
  </body>
</html>