<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/memory/virtualspace.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;logging/log.hpp&quot;
  27 #include &quot;memory/resourceArea.hpp&quot;
  28 #include &quot;memory/virtualspace.hpp&quot;
  29 #include &quot;oops/compressedOops.hpp&quot;
  30 #include &quot;oops/markWord.hpp&quot;
  31 #include &quot;oops/oop.inline.hpp&quot;
  32 #include &quot;runtime/os.inline.hpp&quot;
  33 #include &quot;services/memTracker.hpp&quot;
  34 #include &quot;utilities/align.hpp&quot;
<a name="1" id="anc1"></a>
  35 
  36 // ReservedSpace
  37 
  38 // Dummy constructor
  39 ReservedSpace::ReservedSpace() : _base(NULL), _size(0), _noaccess_prefix(0),
  40     _alignment(0), _special(false), _fd_for_heap(-1), _executable(false) {
  41 }
  42 
  43 ReservedSpace::ReservedSpace(size_t size, size_t preferred_page_size) : _fd_for_heap(-1) {
  44   bool has_preferred_page_size = preferred_page_size != 0;
  45   // Want to use large pages where possible and pad with small pages.
  46   size_t page_size = has_preferred_page_size ? preferred_page_size : os::page_size_for_region_unaligned(size, 1);
  47   bool large_pages = page_size != (size_t)os::vm_page_size();
  48   size_t alignment;
  49   if (large_pages &amp;&amp; has_preferred_page_size) {
  50     alignment = MAX2(page_size, (size_t)os::vm_allocation_granularity());
  51     // ReservedSpace initialization requires size to be aligned to the given
  52     // alignment. Align the size up.
  53     size = align_up(size, alignment);
  54   } else {
  55     // Don&#39;t force the alignment to be large page aligned,
  56     // since that will waste memory.
  57     alignment = os::vm_allocation_granularity();
  58   }
  59   initialize(size, alignment, large_pages, NULL, false);
  60 }
  61 
  62 ReservedSpace::ReservedSpace(size_t size, size_t alignment,
  63                              bool large,
  64                              char* requested_address) : _fd_for_heap(-1) {
  65   initialize(size, alignment, large, requested_address, false);
  66 }
  67 
  68 ReservedSpace::ReservedSpace(size_t size, size_t alignment,
  69                              bool large,
  70                              bool executable) : _fd_for_heap(-1) {
  71   initialize(size, alignment, large, NULL, executable);
  72 }
  73 
  74 ReservedSpace::ReservedSpace(char* base, size_t size, size_t alignment,
  75                              bool special, bool executable) : _fd_for_heap(-1) {
  76   assert((size % os::vm_allocation_granularity()) == 0,
  77          &quot;size not allocation aligned&quot;);
  78   _base = base;
  79   _size = size;
  80   _alignment = alignment;
  81   _noaccess_prefix = 0;
  82   _special = special;
  83   _executable = executable;
  84 }
  85 
  86 // Helper method
  87 static void unmap_or_release_memory(char* base, size_t size, bool is_file_mapped) {
  88   if (is_file_mapped) {
  89     if (!os::unmap_memory(base, size)) {
  90       fatal(&quot;os::unmap_memory failed&quot;);
  91     }
  92   } else if (!os::release_memory(base, size)) {
  93     fatal(&quot;os::release_memory failed&quot;);
  94   }
  95 }
  96 
  97 // Helper method.
  98 static bool failed_to_reserve_as_requested(char* base, char* requested_address,
  99                                            const size_t size, bool special, bool is_file_mapped = false)
 100 {
 101   if (base == requested_address || requested_address == NULL)
 102     return false; // did not fail
 103 
 104   if (base != NULL) {
 105     // Different reserve address may be acceptable in other cases
 106     // but for compressed oops heap should be at requested address.
 107     assert(UseCompressedOops, &quot;currently requested address used only for compressed oops&quot;);
 108     log_debug(gc, heap, coops)(&quot;Reserved memory not at requested address: &quot; PTR_FORMAT &quot; vs &quot; PTR_FORMAT, p2i(base), p2i(requested_address));
 109     // OS ignored requested address. Try different address.
 110     if (special) {
 111       if (!os::release_memory_special(base, size)) {
 112         fatal(&quot;os::release_memory_special failed&quot;);
 113       }
 114     } else {
 115       unmap_or_release_memory(base, size, is_file_mapped);
 116     }
 117   }
 118   return true;
 119 }
 120 
 121 void ReservedSpace::initialize(size_t size, size_t alignment, bool large,
 122                                char* requested_address,
 123                                bool executable) {
 124   const size_t granularity = os::vm_allocation_granularity();
 125   assert((size &amp; (granularity - 1)) == 0,
 126          &quot;size not aligned to os::vm_allocation_granularity()&quot;);
 127   assert((alignment &amp; (granularity - 1)) == 0,
 128          &quot;alignment not aligned to os::vm_allocation_granularity()&quot;);
 129   assert(alignment == 0 || is_power_of_2((intptr_t)alignment),
 130          &quot;not a power of 2&quot;);
 131 
 132   alignment = MAX2(alignment, (size_t)os::vm_page_size());
 133 
 134   _base = NULL;
 135   _size = 0;
 136   _special = false;
 137   _executable = executable;
 138   _alignment = 0;
 139   _noaccess_prefix = 0;
 140   if (size == 0) {
 141     return;
 142   }
 143 
 144   // If OS doesn&#39;t support demand paging for large page memory, we need
 145   // to use reserve_memory_special() to reserve and pin the entire region.
 146   // If there is a backing file directory for this space then whether
 147   // large pages are allocated is up to the filesystem of the backing file.
 148   // So we ignore the UseLargePages flag in this case.
 149   bool special = large &amp;&amp; !os::can_commit_large_page_memory();
 150   if (special &amp;&amp; _fd_for_heap != -1) {
 151     special = false;
 152     if (UseLargePages &amp;&amp; (!FLAG_IS_DEFAULT(UseLargePages) ||
 153       !FLAG_IS_DEFAULT(LargePageSizeInBytes))) {
 154       log_debug(gc, heap)(&quot;Ignoring UseLargePages since large page support is up to the file system of the backing file for Java heap&quot;);
 155     }
 156   }
 157 
 158   char* base = NULL;
 159 
 160   if (special) {
 161 
 162     base = os::reserve_memory_special(size, alignment, requested_address, executable);
 163 
 164     if (base != NULL) {
 165       if (failed_to_reserve_as_requested(base, requested_address, size, true)) {
 166         // OS ignored requested address. Try different address.
 167         return;
 168       }
 169       // Check alignment constraints.
 170       assert((uintptr_t) base % alignment == 0,
 171              &quot;Large pages returned a non-aligned address, base: &quot;
 172              PTR_FORMAT &quot; alignment: &quot; SIZE_FORMAT_HEX,
 173              p2i(base), alignment);
 174       _special = true;
 175     } else {
 176       // failed; try to reserve regular memory below
 177       if (UseLargePages &amp;&amp; (!FLAG_IS_DEFAULT(UseLargePages) ||
 178                             !FLAG_IS_DEFAULT(LargePageSizeInBytes))) {
 179         log_debug(gc, heap, coops)(&quot;Reserve regular memory without large pages&quot;);
 180       }
 181     }
 182   }
 183 
 184   if (base == NULL) {
 185     // Optimistically assume that the OSes returns an aligned base pointer.
 186     // When reserving a large address range, most OSes seem to align to at
 187     // least 64K.
 188 
 189     // If the memory was requested at a particular address, use
 190     // os::attempt_reserve_memory_at() to avoid over mapping something
 191     // important.  If available space is not detected, return NULL.
 192 
 193     if (requested_address != 0) {
 194       base = os::attempt_reserve_memory_at(size, requested_address, _fd_for_heap);
 195       if (failed_to_reserve_as_requested(base, requested_address, size, false, _fd_for_heap != -1)) {
 196         // OS ignored requested address. Try different address.
 197         base = NULL;
 198       }
 199     } else {
 200       base = os::reserve_memory(size, NULL, alignment, _fd_for_heap);
 201     }
 202 
 203     if (base == NULL) return;
 204 
 205     // Check alignment constraints
 206     if ((((size_t)base) &amp; (alignment - 1)) != 0) {
 207       // Base not aligned, retry
 208       unmap_or_release_memory(base, size, _fd_for_heap != -1 /*is_file_mapped*/);
 209 
 210       // Make sure that size is aligned
 211       size = align_up(size, alignment);
 212       base = os::reserve_memory_aligned(size, alignment, _fd_for_heap);
 213 
 214       if (requested_address != 0 &amp;&amp;
 215           failed_to_reserve_as_requested(base, requested_address, size, false, _fd_for_heap != -1)) {
 216         // As a result of the alignment constraints, the allocated base differs
 217         // from the requested address. Return back to the caller who can
 218         // take remedial action (like try again without a requested address).
 219         assert(_base == NULL, &quot;should be&quot;);
 220         return;
 221       }
 222     }
 223   }
 224   // Done
 225   _base = base;
 226   _size = size;
 227   _alignment = alignment;
 228   // If heap is reserved with a backing file, the entire space has been committed. So set the _special flag to true
 229   if (_fd_for_heap != -1) {
 230     _special = true;
 231   }
 232 }
 233 
 234 ReservedSpace ReservedSpace::first_part(size_t partition_size, size_t alignment,
 235                                         bool split, bool realloc) {
 236   assert(partition_size &lt;= size(), &quot;partition failed&quot;);
 237   if (split) {
 238     os::split_reserved_memory(base(), size(), partition_size, realloc);
 239   }
 240   ReservedSpace result(base(), partition_size, alignment, special(),
 241                        executable());
 242   return result;
 243 }
 244 
 245 
 246 ReservedSpace
 247 ReservedSpace::last_part(size_t partition_size, size_t alignment) {
 248   assert(partition_size &lt;= size(), &quot;partition failed&quot;);
 249   ReservedSpace result(base() + partition_size, size() - partition_size,
 250                        alignment, special(), executable());
 251   return result;
 252 }
 253 
 254 
 255 size_t ReservedSpace::page_align_size_up(size_t size) {
 256   return align_up(size, os::vm_page_size());
 257 }
 258 
 259 
 260 size_t ReservedSpace::page_align_size_down(size_t size) {
 261   return align_down(size, os::vm_page_size());
 262 }
 263 
 264 
 265 size_t ReservedSpace::allocation_align_size_up(size_t size) {
 266   return align_up(size, os::vm_allocation_granularity());
 267 }
 268 
 269 
 270 void ReservedSpace::release() {
 271   if (is_reserved()) {
 272     char *real_base = _base - _noaccess_prefix;
 273     const size_t real_size = _size + _noaccess_prefix;
 274     if (special()) {
 275       if (_fd_for_heap != -1) {
 276         os::unmap_memory(real_base, real_size);
 277       } else {
 278         os::release_memory_special(real_base, real_size);
 279       }
 280     } else{
 281       os::release_memory(real_base, real_size);
 282     }
 283     _base = NULL;
 284     _size = 0;
 285     _noaccess_prefix = 0;
 286     _alignment = 0;
 287     _special = false;
 288     _executable = false;
 289   }
 290 }
 291 
 292 static size_t noaccess_prefix_size(size_t alignment) {
 293   return lcm(os::vm_page_size(), alignment);
 294 }
 295 
 296 void ReservedHeapSpace::establish_noaccess_prefix() {
 297   assert(_alignment &gt;= (size_t)os::vm_page_size(), &quot;must be at least page size big&quot;);
 298   _noaccess_prefix = noaccess_prefix_size(_alignment);
 299 
 300   if (base() &amp;&amp; base() + _size &gt; (char *)OopEncodingHeapMax) {
 301     if (true
 302         WIN64_ONLY(&amp;&amp; !UseLargePages)
 303         AIX_ONLY(&amp;&amp; os::vm_page_size() != 64*K)) {
 304       // Protect memory at the base of the allocated region.
 305       // If special, the page was committed (only matters on windows)
 306       if (!os::protect_memory(_base, _noaccess_prefix, os::MEM_PROT_NONE, _special)) {
 307         fatal(&quot;cannot protect protection page&quot;);
 308       }
 309       log_debug(gc, heap, coops)(&quot;Protected page at the reserved heap base: &quot;
 310                                  PTR_FORMAT &quot; / &quot; INTX_FORMAT &quot; bytes&quot;,
 311                                  p2i(_base),
 312                                  _noaccess_prefix);
 313       assert(CompressedOops::use_implicit_null_checks() == true, &quot;not initialized?&quot;);
 314     } else {
 315       CompressedOops::set_use_implicit_null_checks(false);
 316     }
 317   }
 318 
 319   _base += _noaccess_prefix;
 320   _size -= _noaccess_prefix;
 321   assert(((uintptr_t)_base % _alignment == 0), &quot;must be exactly of required alignment&quot;);
 322 }
 323 
 324 // Tries to allocate memory of size &#39;size&#39; at address requested_address with alignment &#39;alignment&#39;.
 325 // Does not check whether the reserved memory actually is at requested_address, as the memory returned
 326 // might still fulfill the wishes of the caller.
 327 // Assures the memory is aligned to &#39;alignment&#39;.
 328 // NOTE: If ReservedHeapSpace already points to some reserved memory this is freed, first.
 329 void ReservedHeapSpace::try_reserve_heap(size_t size,
 330                                          size_t alignment,
 331                                          bool large,
 332                                          char* requested_address) {
 333   if (_base != NULL) {
 334     // We tried before, but we didn&#39;t like the address delivered.
 335     release();
 336   }
 337 
 338   // If OS doesn&#39;t support demand paging for large page memory, we need
 339   // to use reserve_memory_special() to reserve and pin the entire region.
 340   // If there is a backing file directory for this space then whether
 341   // large pages are allocated is up to the filesystem of the backing file.
 342   // So we ignore the UseLargePages flag in this case.
 343   bool special = large &amp;&amp; !os::can_commit_large_page_memory();
 344   if (special &amp;&amp; _fd_for_heap != -1) {
 345     special = false;
 346     if (UseLargePages &amp;&amp; (!FLAG_IS_DEFAULT(UseLargePages) ||
 347                           !FLAG_IS_DEFAULT(LargePageSizeInBytes))) {
 348       log_debug(gc, heap)(&quot;Cannot allocate large pages for Java Heap when AllocateHeapAt option is set.&quot;);
 349     }
 350   }
 351   char* base = NULL;
 352 
 353   log_trace(gc, heap, coops)(&quot;Trying to allocate at address &quot; PTR_FORMAT
 354                              &quot; heap of size &quot; SIZE_FORMAT_HEX,
 355                              p2i(requested_address),
 356                              size);
 357 
 358   if (special) {
 359     base = os::reserve_memory_special(size, alignment, requested_address, false);
 360 
 361     if (base != NULL) {
 362       // Check alignment constraints.
 363       assert((uintptr_t) base % alignment == 0,
 364              &quot;Large pages returned a non-aligned address, base: &quot;
 365              PTR_FORMAT &quot; alignment: &quot; SIZE_FORMAT_HEX,
 366              p2i(base), alignment);
 367       _special = true;
 368     }
 369   }
 370 
 371   if (base == NULL) {
 372     // Failed; try to reserve regular memory below
 373     if (UseLargePages &amp;&amp; (!FLAG_IS_DEFAULT(UseLargePages) ||
 374                           !FLAG_IS_DEFAULT(LargePageSizeInBytes))) {
 375       log_debug(gc, heap, coops)(&quot;Reserve regular memory without large pages&quot;);
 376     }
 377 
 378     // Optimistically assume that the OSes returns an aligned base pointer.
 379     // When reserving a large address range, most OSes seem to align to at
 380     // least 64K.
 381 
 382     // If the memory was requested at a particular address, use
 383     // os::attempt_reserve_memory_at() to avoid over mapping something
 384     // important.  If available space is not detected, return NULL.
 385 
 386     if (requested_address != 0) {
 387       base = os::attempt_reserve_memory_at(size, requested_address, _fd_for_heap);
 388     } else {
 389       base = os::reserve_memory(size, NULL, alignment, _fd_for_heap);
 390     }
 391   }
 392   if (base == NULL) { return; }
 393 
 394   // Done
 395   _base = base;
 396   _size = size;
 397   _alignment = alignment;
 398 
 399   // If heap is reserved with a backing file, the entire space has been committed. So set the _special flag to true
 400   if (_fd_for_heap != -1) {
 401     _special = true;
 402   }
 403 
 404   // Check alignment constraints
 405   if ((((size_t)base) &amp; (alignment - 1)) != 0) {
 406     // Base not aligned, retry.
 407     release();
 408   }
 409 }
 410 
 411 void ReservedHeapSpace::try_reserve_range(char *highest_start,
 412                                           char *lowest_start,
 413                                           size_t attach_point_alignment,
 414                                           char *aligned_heap_base_min_address,
 415                                           char *upper_bound,
 416                                           size_t size,
 417                                           size_t alignment,
 418                                           bool large) {
 419   const size_t attach_range = highest_start - lowest_start;
 420   // Cap num_attempts at possible number.
 421   // At least one is possible even for 0 sized attach range.
 422   const uint64_t num_attempts_possible = (attach_range / attach_point_alignment) + 1;
 423   const uint64_t num_attempts_to_try   = MIN2((uint64_t)HeapSearchSteps, num_attempts_possible);
 424 
 425   const size_t stepsize = (attach_range == 0) ? // Only one try.
 426     (size_t) highest_start : align_up(attach_range / num_attempts_to_try, attach_point_alignment);
 427 
 428   // Try attach points from top to bottom.
 429   char* attach_point = highest_start;
 430   while (attach_point &gt;= lowest_start  &amp;&amp;
 431          attach_point &lt;= highest_start &amp;&amp;  // Avoid wrap around.
 432          ((_base == NULL) ||
 433           (_base &lt; aligned_heap_base_min_address || _base + size &gt; upper_bound))) {
 434     try_reserve_heap(size, alignment, large, attach_point);
 435     attach_point -= stepsize;
 436   }
 437 }
 438 
 439 #define SIZE_64K  ((uint64_t) UCONST64(      0x10000))
 440 #define SIZE_256M ((uint64_t) UCONST64(   0x10000000))
 441 #define SIZE_32G  ((uint64_t) UCONST64(  0x800000000))
 442 
 443 // Helper for heap allocation. Returns an array with addresses
 444 // (OS-specific) which are suited for disjoint base mode. Array is
 445 // NULL terminated.
 446 static char** get_attach_addresses_for_disjoint_mode() {
 447   static uint64_t addresses[] = {
 448      2 * SIZE_32G,
 449      3 * SIZE_32G,
 450      4 * SIZE_32G,
 451      8 * SIZE_32G,
 452     10 * SIZE_32G,
 453      1 * SIZE_64K * SIZE_32G,
 454      2 * SIZE_64K * SIZE_32G,
 455      3 * SIZE_64K * SIZE_32G,
 456      4 * SIZE_64K * SIZE_32G,
 457     16 * SIZE_64K * SIZE_32G,
 458     32 * SIZE_64K * SIZE_32G,
 459     34 * SIZE_64K * SIZE_32G,
 460     0
 461   };
 462 
 463   // Sort out addresses smaller than HeapBaseMinAddress. This assumes
 464   // the array is sorted.
 465   uint i = 0;
 466   while (addresses[i] != 0 &amp;&amp;
 467          (addresses[i] &lt; OopEncodingHeapMax || addresses[i] &lt; HeapBaseMinAddress)) {
 468     i++;
 469   }
 470   uint start = i;
 471 
 472   // Avoid more steps than requested.
 473   i = 0;
 474   while (addresses[start+i] != 0) {
 475     if (i == HeapSearchSteps) {
 476       addresses[start+i] = 0;
 477       break;
 478     }
 479     i++;
 480   }
 481 
 482   return (char**) &amp;addresses[start];
 483 }
 484 
 485 void ReservedHeapSpace::initialize_compressed_heap(const size_t size, size_t alignment, bool large) {
 486   guarantee(size + noaccess_prefix_size(alignment) &lt;= OopEncodingHeapMax,
 487             &quot;can not allocate compressed oop heap for this size&quot;);
 488   guarantee(alignment == MAX2(alignment, (size_t)os::vm_page_size()), &quot;alignment too small&quot;);
 489 
 490   const size_t granularity = os::vm_allocation_granularity();
 491   assert((size &amp; (granularity - 1)) == 0,
 492          &quot;size not aligned to os::vm_allocation_granularity()&quot;);
 493   assert((alignment &amp; (granularity - 1)) == 0,
 494          &quot;alignment not aligned to os::vm_allocation_granularity()&quot;);
 495   assert(alignment == 0 || is_power_of_2((intptr_t)alignment),
 496          &quot;not a power of 2&quot;);
 497 
 498   // The necessary attach point alignment for generated wish addresses.
 499   // This is needed to increase the chance of attaching for mmap and shmat.
 500   const size_t os_attach_point_alignment =
 501     AIX_ONLY(SIZE_256M)  // Known shm boundary alignment.
 502     NOT_AIX(os::vm_allocation_granularity());
 503   const size_t attach_point_alignment = lcm(alignment, os_attach_point_alignment);
 504 
 505   char *aligned_heap_base_min_address = (char *)align_up((void *)HeapBaseMinAddress, alignment);
 506   size_t noaccess_prefix = ((aligned_heap_base_min_address + size) &gt; (char*)OopEncodingHeapMax) ?
 507     noaccess_prefix_size(alignment) : 0;
 508 
 509   // Attempt to alloc at user-given address.
 510   if (!FLAG_IS_DEFAULT(HeapBaseMinAddress)) {
 511     try_reserve_heap(size + noaccess_prefix, alignment, large, aligned_heap_base_min_address);
 512     if (_base != aligned_heap_base_min_address) { // Enforce this exact address.
 513       release();
 514     }
 515   }
 516 
 517   // Keep heap at HeapBaseMinAddress.
 518   if (_base == NULL) {
 519 
 520     // Try to allocate the heap at addresses that allow efficient oop compression.
 521     // Different schemes are tried, in order of decreasing optimization potential.
 522     //
 523     // For this, try_reserve_heap() is called with the desired heap base addresses.
 524     // A call into the os layer to allocate at a given address can return memory
 525     // at a different address than requested.  Still, this might be memory at a useful
 526     // address. try_reserve_heap() always returns this allocated memory, as only here
 527     // the criteria for a good heap are checked.
 528 
 529     // Attempt to allocate so that we can run without base and scale (32-Bit unscaled compressed oops).
 530     // Give it several tries from top of range to bottom.
 531     if (aligned_heap_base_min_address + size &lt;= (char *)UnscaledOopHeapMax) {
 532 
 533       // Calc address range within we try to attach (range of possible start addresses).
 534       char* const highest_start = align_down((char *)UnscaledOopHeapMax - size, attach_point_alignment);
 535       char* const lowest_start  = align_up(aligned_heap_base_min_address, attach_point_alignment);
 536       try_reserve_range(highest_start, lowest_start, attach_point_alignment,
 537                         aligned_heap_base_min_address, (char *)UnscaledOopHeapMax, size, alignment, large);
 538     }
 539 
 540     // zerobased: Attempt to allocate in the lower 32G.
 541     // But leave room for the compressed class pointers, which is allocated above
 542     // the heap.
 543     char *zerobased_max = (char *)OopEncodingHeapMax;
 544     const size_t class_space = align_up(CompressedClassSpaceSize, alignment);
 545     // For small heaps, save some space for compressed class pointer
 546     // space so it can be decoded with no base.
 547     if (UseCompressedClassPointers &amp;&amp; !UseSharedSpaces &amp;&amp;
 548         OopEncodingHeapMax &lt;= KlassEncodingMetaspaceMax &amp;&amp;
 549         (uint64_t)(aligned_heap_base_min_address + size + class_space) &lt;= KlassEncodingMetaspaceMax) {
 550       zerobased_max = (char *)OopEncodingHeapMax - class_space;
 551     }
 552 
 553     // Give it several tries from top of range to bottom.
 554     if (aligned_heap_base_min_address + size &lt;= zerobased_max &amp;&amp;    // Zerobased theoretical possible.
 555         ((_base == NULL) ||                        // No previous try succeeded.
 556          (_base + size &gt; zerobased_max))) {        // Unscaled delivered an arbitrary address.
 557 
 558       // Calc address range within we try to attach (range of possible start addresses).
 559       char *const highest_start = align_down(zerobased_max - size, attach_point_alignment);
 560       // Need to be careful about size being guaranteed to be less
 561       // than UnscaledOopHeapMax due to type constraints.
 562       char *lowest_start = aligned_heap_base_min_address;
 563       uint64_t unscaled_end = UnscaledOopHeapMax - size;
 564       if (unscaled_end &lt; UnscaledOopHeapMax) { // unscaled_end wrapped if size is large
 565         lowest_start = MAX2(lowest_start, (char*)unscaled_end);
 566       }
 567       lowest_start = align_up(lowest_start, attach_point_alignment);
 568       try_reserve_range(highest_start, lowest_start, attach_point_alignment,
 569                         aligned_heap_base_min_address, zerobased_max, size, alignment, large);
 570     }
 571 
 572     // Now we go for heaps with base != 0.  We need a noaccess prefix to efficiently
 573     // implement null checks.
 574     noaccess_prefix = noaccess_prefix_size(alignment);
 575 
 576     // Try to attach at addresses that are aligned to OopEncodingHeapMax. Disjointbase mode.
 577     char** addresses = get_attach_addresses_for_disjoint_mode();
 578     int i = 0;
 579     while (addresses[i] &amp;&amp;                                 // End of array not yet reached.
 580            ((_base == NULL) ||                             // No previous try succeeded.
 581             (_base + size &gt;  (char *)OopEncodingHeapMax &amp;&amp; // Not zerobased or unscaled address.
 582              !CompressedOops::is_disjoint_heap_base_address((address)_base)))) {  // Not disjoint address.
 583       char* const attach_point = addresses[i];
 584       assert(attach_point &gt;= aligned_heap_base_min_address, &quot;Flag support broken&quot;);
 585       try_reserve_heap(size + noaccess_prefix, alignment, large, attach_point);
 586       i++;
 587     }
 588 
 589     // Last, desperate try without any placement.
 590     if (_base == NULL) {
 591       log_trace(gc, heap, coops)(&quot;Trying to allocate at address NULL heap of size &quot; SIZE_FORMAT_HEX, size + noaccess_prefix);
 592       initialize(size + noaccess_prefix, alignment, large, NULL, false);
 593     }
 594   }
 595 }
 596 
 597 ReservedHeapSpace::ReservedHeapSpace(size_t size, size_t alignment, bool large, const char* heap_allocation_directory) : ReservedSpace() {
 598 
 599   if (size == 0) {
 600     return;
 601   }
 602 
 603   if (heap_allocation_directory != NULL) {
 604     _fd_for_heap = os::create_file_for_heap(heap_allocation_directory);
 605     if (_fd_for_heap == -1) {
 606       vm_exit_during_initialization(
 607         err_msg(&quot;Could not create file for Heap at location %s&quot;, heap_allocation_directory));
 608     }
 609   }
 610 
 611   // Heap size should be aligned to alignment, too.
 612   guarantee(is_aligned(size, alignment), &quot;set by caller&quot;);
 613 
 614   if (UseCompressedOops) {
 615     initialize_compressed_heap(size, alignment, large);
 616     if (_size &gt; size) {
 617       // We allocated heap with noaccess prefix.
 618       // It can happen we get a zerobased/unscaled heap with noaccess prefix,
 619       // if we had to try at arbitrary address.
 620       establish_noaccess_prefix();
 621     }
 622   } else {
 623     initialize(size, alignment, large, NULL, false);
 624   }
 625 
 626   assert(markWord::encode_pointer_as_mark(_base).decode_pointer() == _base,
 627          &quot;area must be distinguishable from marks for mark-sweep&quot;);
 628   assert(markWord::encode_pointer_as_mark(&amp;_base[size]).decode_pointer() == &amp;_base[size],
 629          &quot;area must be distinguishable from marks for mark-sweep&quot;);
 630 
 631   if (base() != NULL) {
 632     MemTracker::record_virtual_memory_type((address)base(), mtJavaHeap);
 633   }
 634 
 635   if (_fd_for_heap != -1) {
 636     os::close(_fd_for_heap);
 637   }
 638 }
 639 
 640 MemRegion ReservedHeapSpace::region() const {
 641   return MemRegion((HeapWord*)base(), (HeapWord*)end());
 642 }
 643 
 644 // Reserve space for code segment.  Same as Java heap only we mark this as
 645 // executable.
 646 ReservedCodeSpace::ReservedCodeSpace(size_t r_size,
 647                                      size_t rs_align,
 648                                      bool large) :
 649   ReservedSpace(r_size, rs_align, large, /*executable*/ true) {
 650   MemTracker::record_virtual_memory_type((address)base(), mtCode);
 651 }
 652 
 653 // VirtualSpace
 654 
 655 VirtualSpace::VirtualSpace() {
 656   _low_boundary           = NULL;
 657   _high_boundary          = NULL;
 658   _low                    = NULL;
 659   _high                   = NULL;
 660   _lower_high             = NULL;
 661   _middle_high            = NULL;
 662   _upper_high             = NULL;
 663   _lower_high_boundary    = NULL;
 664   _middle_high_boundary   = NULL;
 665   _upper_high_boundary    = NULL;
 666   _lower_alignment        = 0;
 667   _middle_alignment       = 0;
 668   _upper_alignment        = 0;
 669   _special                = false;
 670   _executable             = false;
 671 }
 672 
 673 
 674 bool VirtualSpace::initialize(ReservedSpace rs, size_t committed_size) {
 675   const size_t max_commit_granularity = os::page_size_for_region_unaligned(rs.size(), 1);
 676   return initialize_with_granularity(rs, committed_size, max_commit_granularity);
 677 }
 678 
 679 bool VirtualSpace::initialize_with_granularity(ReservedSpace rs, size_t committed_size, size_t max_commit_granularity) {
 680   if(!rs.is_reserved()) return false;  // allocation failed.
 681   assert(_low_boundary == NULL, &quot;VirtualSpace already initialized&quot;);
 682   assert(max_commit_granularity &gt; 0, &quot;Granularity must be non-zero.&quot;);
 683 
 684   _low_boundary  = rs.base();
 685   _high_boundary = low_boundary() + rs.size();
 686 
 687   _low = low_boundary();
 688   _high = low();
 689 
 690   _special = rs.special();
 691   _executable = rs.executable();
 692 
 693   // When a VirtualSpace begins life at a large size, make all future expansion
 694   // and shrinking occur aligned to a granularity of large pages.  This avoids
 695   // fragmentation of physical addresses that inhibits the use of large pages
 696   // by the OS virtual memory system.  Empirically,  we see that with a 4MB
 697   // page size, the only spaces that get handled this way are codecache and
 698   // the heap itself, both of which provide a substantial performance
 699   // boost in many benchmarks when covered by large pages.
 700   //
 701   // No attempt is made to force large page alignment at the very top and
 702   // bottom of the space if they are not aligned so already.
 703   _lower_alignment  = os::vm_page_size();
 704   _middle_alignment = max_commit_granularity;
 705   _upper_alignment  = os::vm_page_size();
 706 
 707   // End of each region
 708   _lower_high_boundary = align_up(low_boundary(), middle_alignment());
 709   _middle_high_boundary = align_down(high_boundary(), middle_alignment());
 710   _upper_high_boundary = high_boundary();
 711 
 712   // High address of each region
 713   _lower_high = low_boundary();
 714   _middle_high = lower_high_boundary();
 715   _upper_high = middle_high_boundary();
 716 
 717   // commit to initial size
 718   if (committed_size &gt; 0) {
 719     if (!expand_by(committed_size)) {
 720       return false;
 721     }
 722   }
 723   return true;
 724 }
 725 
 726 
 727 VirtualSpace::~VirtualSpace() {
 728   release();
 729 }
 730 
 731 
 732 void VirtualSpace::release() {
 733   // This does not release memory it reserved.
 734   // Caller must release via rs.release();
 735   _low_boundary           = NULL;
 736   _high_boundary          = NULL;
 737   _low                    = NULL;
 738   _high                   = NULL;
 739   _lower_high             = NULL;
 740   _middle_high            = NULL;
 741   _upper_high             = NULL;
 742   _lower_high_boundary    = NULL;
 743   _middle_high_boundary   = NULL;
 744   _upper_high_boundary    = NULL;
 745   _lower_alignment        = 0;
 746   _middle_alignment       = 0;
 747   _upper_alignment        = 0;
 748   _special                = false;
 749   _executable             = false;
 750 }
 751 
 752 
 753 size_t VirtualSpace::committed_size() const {
 754   return pointer_delta(high(), low(), sizeof(char));
 755 }
 756 
 757 
 758 size_t VirtualSpace::reserved_size() const {
 759   return pointer_delta(high_boundary(), low_boundary(), sizeof(char));
 760 }
 761 
 762 
 763 size_t VirtualSpace::uncommitted_size()  const {
 764   return reserved_size() - committed_size();
 765 }
 766 
 767 size_t VirtualSpace::actual_committed_size() const {
 768   // Special VirtualSpaces commit all reserved space up front.
 769   if (special()) {
 770     return reserved_size();
 771   }
 772 
 773   size_t committed_low    = pointer_delta(_lower_high,  _low_boundary,         sizeof(char));
 774   size_t committed_middle = pointer_delta(_middle_high, _lower_high_boundary,  sizeof(char));
 775   size_t committed_high   = pointer_delta(_upper_high,  _middle_high_boundary, sizeof(char));
 776 
 777 #ifdef ASSERT
 778   size_t lower  = pointer_delta(_lower_high_boundary,  _low_boundary,         sizeof(char));
 779   size_t middle = pointer_delta(_middle_high_boundary, _lower_high_boundary,  sizeof(char));
 780   size_t upper  = pointer_delta(_upper_high_boundary,  _middle_high_boundary, sizeof(char));
 781 
 782   if (committed_high &gt; 0) {
 783     assert(committed_low == lower, &quot;Must be&quot;);
 784     assert(committed_middle == middle, &quot;Must be&quot;);
 785   }
 786 
 787   if (committed_middle &gt; 0) {
 788     assert(committed_low == lower, &quot;Must be&quot;);
 789   }
 790   if (committed_middle &lt; middle) {
 791     assert(committed_high == 0, &quot;Must be&quot;);
 792   }
 793 
 794   if (committed_low &lt; lower) {
 795     assert(committed_high == 0, &quot;Must be&quot;);
 796     assert(committed_middle == 0, &quot;Must be&quot;);
 797   }
 798 #endif
 799 
 800   return committed_low + committed_middle + committed_high;
 801 }
 802 
 803 
 804 bool VirtualSpace::contains(const void* p) const {
 805   return low() &lt;= (const char*) p &amp;&amp; (const char*) p &lt; high();
 806 }
 807 
 808 static void pretouch_expanded_memory(void* start, void* end) {
 809   assert(is_aligned(start, os::vm_page_size()), &quot;Unexpected alignment&quot;);
 810   assert(is_aligned(end,   os::vm_page_size()), &quot;Unexpected alignment&quot;);
 811 
 812   os::pretouch_memory(start, end);
 813 }
 814 
 815 static bool commit_expanded(char* start, size_t size, size_t alignment, bool pre_touch, bool executable) {
 816   if (os::commit_memory(start, size, alignment, executable)) {
 817     if (pre_touch || AlwaysPreTouch) {
 818       pretouch_expanded_memory(start, start + size);
 819     }
 820     return true;
 821   }
 822 
 823   debug_only(warning(
 824       &quot;INFO: os::commit_memory(&quot; PTR_FORMAT &quot;, &quot; PTR_FORMAT
 825       &quot; size=&quot; SIZE_FORMAT &quot;, executable=%d) failed&quot;,
 826       p2i(start), p2i(start + size), size, executable);)
 827 
 828   return false;
 829 }
 830 
 831 /*
 832    First we need to determine if a particular virtual space is using large
 833    pages.  This is done at the initialize function and only virtual spaces
 834    that are larger than LargePageSizeInBytes use large pages.  Once we
 835    have determined this, all expand_by and shrink_by calls must grow and
 836    shrink by large page size chunks.  If a particular request
 837    is within the current large page, the call to commit and uncommit memory
 838    can be ignored.  In the case that the low and high boundaries of this
 839    space is not large page aligned, the pages leading to the first large
 840    page address and the pages after the last large page address must be
 841    allocated with default pages.
 842 */
 843 bool VirtualSpace::expand_by(size_t bytes, bool pre_touch) {
 844   if (uncommitted_size() &lt; bytes) {
 845     return false;
 846   }
 847 
 848   if (special()) {
 849     // don&#39;t commit memory if the entire space is pinned in memory
 850     _high += bytes;
 851     return true;
 852   }
 853 
 854   char* previous_high = high();
 855   char* unaligned_new_high = high() + bytes;
 856   assert(unaligned_new_high &lt;= high_boundary(), &quot;cannot expand by more than upper boundary&quot;);
 857 
 858   // Calculate where the new high for each of the regions should be.  If
 859   // the low_boundary() and high_boundary() are LargePageSizeInBytes aligned
 860   // then the unaligned lower and upper new highs would be the
 861   // lower_high() and upper_high() respectively.
 862   char* unaligned_lower_new_high =  MIN2(unaligned_new_high, lower_high_boundary());
 863   char* unaligned_middle_new_high = MIN2(unaligned_new_high, middle_high_boundary());
 864   char* unaligned_upper_new_high =  MIN2(unaligned_new_high, upper_high_boundary());
 865 
 866   // Align the new highs based on the regions alignment.  lower and upper
 867   // alignment will always be default page size.  middle alignment will be
 868   // LargePageSizeInBytes if the actual size of the virtual space is in
 869   // fact larger than LargePageSizeInBytes.
 870   char* aligned_lower_new_high =  align_up(unaligned_lower_new_high, lower_alignment());
 871   char* aligned_middle_new_high = align_up(unaligned_middle_new_high, middle_alignment());
 872   char* aligned_upper_new_high =  align_up(unaligned_upper_new_high, upper_alignment());
 873 
 874   // Determine which regions need to grow in this expand_by call.
 875   // If you are growing in the lower region, high() must be in that
 876   // region so calculate the size based on high().  For the middle and
 877   // upper regions, determine the starting point of growth based on the
 878   // location of high().  By getting the MAX of the region&#39;s low address
 879   // (or the previous region&#39;s high address) and high(), we can tell if it
 880   // is an intra or inter region growth.
 881   size_t lower_needs = 0;
 882   if (aligned_lower_new_high &gt; lower_high()) {
 883     lower_needs = pointer_delta(aligned_lower_new_high, lower_high(), sizeof(char));
 884   }
 885   size_t middle_needs = 0;
 886   if (aligned_middle_new_high &gt; middle_high()) {
 887     middle_needs = pointer_delta(aligned_middle_new_high, middle_high(), sizeof(char));
 888   }
 889   size_t upper_needs = 0;
 890   if (aligned_upper_new_high &gt; upper_high()) {
 891     upper_needs = pointer_delta(aligned_upper_new_high, upper_high(), sizeof(char));
 892   }
 893 
 894   // Check contiguity.
 895   assert(low_boundary() &lt;= lower_high() &amp;&amp; lower_high() &lt;= lower_high_boundary(),
 896          &quot;high address must be contained within the region&quot;);
 897   assert(lower_high_boundary() &lt;= middle_high() &amp;&amp; middle_high() &lt;= middle_high_boundary(),
 898          &quot;high address must be contained within the region&quot;);
 899   assert(middle_high_boundary() &lt;= upper_high() &amp;&amp; upper_high() &lt;= upper_high_boundary(),
 900          &quot;high address must be contained within the region&quot;);
 901 
 902   // Commit regions
 903   if (lower_needs &gt; 0) {
 904     assert(lower_high() + lower_needs &lt;= lower_high_boundary(), &quot;must not expand beyond region&quot;);
 905     if (!commit_expanded(lower_high(), lower_needs, _lower_alignment, pre_touch, _executable)) {
 906       return false;
 907     }
 908     _lower_high += lower_needs;
 909   }
 910 
 911   if (middle_needs &gt; 0) {
 912     assert(middle_high() + middle_needs &lt;= middle_high_boundary(), &quot;must not expand beyond region&quot;);
 913     if (!commit_expanded(middle_high(), middle_needs, _middle_alignment, pre_touch, _executable)) {
 914       return false;
 915     }
 916     _middle_high += middle_needs;
 917   }
 918 
 919   if (upper_needs &gt; 0) {
 920     assert(upper_high() + upper_needs &lt;= upper_high_boundary(), &quot;must not expand beyond region&quot;);
 921     if (!commit_expanded(upper_high(), upper_needs, _upper_alignment, pre_touch, _executable)) {
 922       return false;
 923     }
 924     _upper_high += upper_needs;
 925   }
 926 
 927   _high += bytes;
 928   return true;
 929 }
 930 
 931 // A page is uncommitted if the contents of the entire page is deemed unusable.
 932 // Continue to decrement the high() pointer until it reaches a page boundary
 933 // in which case that particular page can now be uncommitted.
 934 void VirtualSpace::shrink_by(size_t size) {
 935   if (committed_size() &lt; size)
 936     fatal(&quot;Cannot shrink virtual space to negative size&quot;);
 937 
 938   if (special()) {
 939     // don&#39;t uncommit if the entire space is pinned in memory
 940     _high -= size;
 941     return;
 942   }
 943 
 944   char* unaligned_new_high = high() - size;
 945   assert(unaligned_new_high &gt;= low_boundary(), &quot;cannot shrink past lower boundary&quot;);
 946 
 947   // Calculate new unaligned address
 948   char* unaligned_upper_new_high =
 949     MAX2(unaligned_new_high, middle_high_boundary());
 950   char* unaligned_middle_new_high =
 951     MAX2(unaligned_new_high, lower_high_boundary());
 952   char* unaligned_lower_new_high =
 953     MAX2(unaligned_new_high, low_boundary());
 954 
 955   // Align address to region&#39;s alignment
 956   char* aligned_upper_new_high =  align_up(unaligned_upper_new_high, upper_alignment());
 957   char* aligned_middle_new_high = align_up(unaligned_middle_new_high, middle_alignment());
 958   char* aligned_lower_new_high =  align_up(unaligned_lower_new_high, lower_alignment());
 959 
 960   // Determine which regions need to shrink
 961   size_t upper_needs = 0;
 962   if (aligned_upper_new_high &lt; upper_high()) {
 963     upper_needs =
 964       pointer_delta(upper_high(), aligned_upper_new_high, sizeof(char));
 965   }
 966   size_t middle_needs = 0;
 967   if (aligned_middle_new_high &lt; middle_high()) {
 968     middle_needs =
 969       pointer_delta(middle_high(), aligned_middle_new_high, sizeof(char));
 970   }
 971   size_t lower_needs = 0;
 972   if (aligned_lower_new_high &lt; lower_high()) {
 973     lower_needs =
 974       pointer_delta(lower_high(), aligned_lower_new_high, sizeof(char));
 975   }
 976 
 977   // Check contiguity.
 978   assert(middle_high_boundary() &lt;= upper_high() &amp;&amp;
 979          upper_high() &lt;= upper_high_boundary(),
 980          &quot;high address must be contained within the region&quot;);
 981   assert(lower_high_boundary() &lt;= middle_high() &amp;&amp;
 982          middle_high() &lt;= middle_high_boundary(),
 983          &quot;high address must be contained within the region&quot;);
 984   assert(low_boundary() &lt;= lower_high() &amp;&amp;
 985          lower_high() &lt;= lower_high_boundary(),
 986          &quot;high address must be contained within the region&quot;);
 987 
 988   // Uncommit
 989   if (upper_needs &gt; 0) {
 990     assert(middle_high_boundary() &lt;= aligned_upper_new_high &amp;&amp;
 991            aligned_upper_new_high + upper_needs &lt;= upper_high_boundary(),
 992            &quot;must not shrink beyond region&quot;);
 993     if (!os::uncommit_memory(aligned_upper_new_high, upper_needs)) {
 994       debug_only(warning(&quot;os::uncommit_memory failed&quot;));
 995       return;
 996     } else {
 997       _upper_high -= upper_needs;
 998     }
 999   }
1000   if (middle_needs &gt; 0) {
1001     assert(lower_high_boundary() &lt;= aligned_middle_new_high &amp;&amp;
1002            aligned_middle_new_high + middle_needs &lt;= middle_high_boundary(),
1003            &quot;must not shrink beyond region&quot;);
1004     if (!os::uncommit_memory(aligned_middle_new_high, middle_needs)) {
1005       debug_only(warning(&quot;os::uncommit_memory failed&quot;));
1006       return;
1007     } else {
1008       _middle_high -= middle_needs;
1009     }
1010   }
1011   if (lower_needs &gt; 0) {
1012     assert(low_boundary() &lt;= aligned_lower_new_high &amp;&amp;
1013            aligned_lower_new_high + lower_needs &lt;= lower_high_boundary(),
1014            &quot;must not shrink beyond region&quot;);
1015     if (!os::uncommit_memory(aligned_lower_new_high, lower_needs)) {
1016       debug_only(warning(&quot;os::uncommit_memory failed&quot;));
1017       return;
1018     } else {
1019       _lower_high -= lower_needs;
1020     }
1021   }
1022 
1023   _high -= size;
1024 }
1025 
1026 #ifndef PRODUCT
1027 void VirtualSpace::check_for_contiguity() {
1028   // Check contiguity.
1029   assert(low_boundary() &lt;= lower_high() &amp;&amp;
1030          lower_high() &lt;= lower_high_boundary(),
1031          &quot;high address must be contained within the region&quot;);
1032   assert(lower_high_boundary() &lt;= middle_high() &amp;&amp;
1033          middle_high() &lt;= middle_high_boundary(),
1034          &quot;high address must be contained within the region&quot;);
1035   assert(middle_high_boundary() &lt;= upper_high() &amp;&amp;
1036          upper_high() &lt;= upper_high_boundary(),
1037          &quot;high address must be contained within the region&quot;);
1038   assert(low() &gt;= low_boundary(), &quot;low&quot;);
1039   assert(low_boundary() &lt;= lower_high_boundary(), &quot;lower high boundary&quot;);
1040   assert(upper_high_boundary() &lt;= high_boundary(), &quot;upper high boundary&quot;);
1041   assert(high() &lt;= upper_high(), &quot;upper high&quot;);
1042 }
1043 
1044 void VirtualSpace::print_on(outputStream* out) {
1045   out-&gt;print   (&quot;Virtual space:&quot;);
1046   if (special()) out-&gt;print(&quot; (pinned in memory)&quot;);
1047   out-&gt;cr();
1048   out-&gt;print_cr(&quot; - committed: &quot; SIZE_FORMAT, committed_size());
1049   out-&gt;print_cr(&quot; - reserved:  &quot; SIZE_FORMAT, reserved_size());
1050   out-&gt;print_cr(&quot; - [low, high]:     [&quot; INTPTR_FORMAT &quot;, &quot; INTPTR_FORMAT &quot;]&quot;,  p2i(low()), p2i(high()));
1051   out-&gt;print_cr(&quot; - [low_b, high_b]: [&quot; INTPTR_FORMAT &quot;, &quot; INTPTR_FORMAT &quot;]&quot;,  p2i(low_boundary()), p2i(high_boundary()));
1052 }
1053 
1054 void VirtualSpace::print() {
1055   print_on(tty);
1056 }
1057 
1058 /////////////// Unit tests ///////////////
1059 
1060 #ifndef PRODUCT
1061 
1062 class TestReservedSpace : AllStatic {
1063  public:
1064   static void small_page_write(void* addr, size_t size) {
1065     size_t page_size = os::vm_page_size();
1066 
1067     char* end = (char*)addr + size;
1068     for (char* p = (char*)addr; p &lt; end; p += page_size) {
1069       *p = 1;
1070     }
1071   }
1072 
1073   static void release_memory_for_test(ReservedSpace rs) {
1074     if (rs.special()) {
1075       guarantee(os::release_memory_special(rs.base(), rs.size()), &quot;Shouldn&#39;t fail&quot;);
1076     } else {
1077       guarantee(os::release_memory(rs.base(), rs.size()), &quot;Shouldn&#39;t fail&quot;);
1078     }
1079   }
1080 
1081   static void test_reserved_space1(size_t size, size_t alignment) {
1082     assert(is_aligned(size, alignment), &quot;Incorrect input parameters&quot;);
1083 
1084     ReservedSpace rs(size,          // size
1085                      alignment,     // alignment
1086                      UseLargePages, // large
1087                      (char *)NULL); // requested_address
1088 
1089     assert(rs.base() != NULL, &quot;Must be&quot;);
1090     assert(rs.size() == size, &quot;Must be&quot;);
1091 
1092     assert(is_aligned(rs.base(), alignment), &quot;aligned sizes should always give aligned addresses&quot;);
1093     assert(is_aligned(rs.size(), alignment), &quot;aligned sizes should always give aligned addresses&quot;);
1094 
1095     if (rs.special()) {
1096       small_page_write(rs.base(), size);
1097     }
1098 
1099     release_memory_for_test(rs);
1100   }
1101 
1102   static void test_reserved_space2(size_t size) {
1103     assert(is_aligned(size, os::vm_allocation_granularity()), &quot;Must be at least AG aligned&quot;);
1104 
1105     ReservedSpace rs(size);
1106 
1107     assert(rs.base() != NULL, &quot;Must be&quot;);
1108     assert(rs.size() == size, &quot;Must be&quot;);
1109 
1110     if (rs.special()) {
1111       small_page_write(rs.base(), size);
1112     }
1113 
1114     release_memory_for_test(rs);
1115   }
1116 
1117   static void test_reserved_space3(size_t size, size_t alignment, bool maybe_large) {
1118     if (size &lt; alignment) {
1119       // Tests might set -XX:LargePageSizeInBytes=&lt;small pages&gt; and cause unexpected input arguments for this test.
1120       assert((size_t)os::vm_page_size() == os::large_page_size(), &quot;Test needs further refinement&quot;);
1121       return;
1122     }
1123 
1124     assert(is_aligned(size, os::vm_allocation_granularity()), &quot;Must be at least AG aligned&quot;);
1125     assert(is_aligned(size, alignment), &quot;Must be at least aligned against alignment&quot;);
1126 
1127     bool large = maybe_large &amp;&amp; UseLargePages &amp;&amp; size &gt;= os::large_page_size();
1128 
1129     ReservedSpace rs(size, alignment, large, false);
1130 
1131     assert(rs.base() != NULL, &quot;Must be&quot;);
1132     assert(rs.size() == size, &quot;Must be&quot;);
1133 
1134     if (rs.special()) {
1135       small_page_write(rs.base(), size);
1136     }
1137 
1138     release_memory_for_test(rs);
1139   }
1140 
1141 
1142   static void test_reserved_space1() {
1143     size_t size = 2 * 1024 * 1024;
1144     size_t ag   = os::vm_allocation_granularity();
1145 
1146     test_reserved_space1(size,      ag);
1147     test_reserved_space1(size * 2,  ag);
1148     test_reserved_space1(size * 10, ag);
1149   }
1150 
1151   static void test_reserved_space2() {
1152     size_t size = 2 * 1024 * 1024;
1153     size_t ag = os::vm_allocation_granularity();
1154 
1155     test_reserved_space2(size * 1);
1156     test_reserved_space2(size * 2);
1157     test_reserved_space2(size * 10);
1158     test_reserved_space2(ag);
1159     test_reserved_space2(size - ag);
1160     test_reserved_space2(size);
1161     test_reserved_space2(size + ag);
1162     test_reserved_space2(size * 2);
1163     test_reserved_space2(size * 2 - ag);
1164     test_reserved_space2(size * 2 + ag);
1165     test_reserved_space2(size * 3);
1166     test_reserved_space2(size * 3 - ag);
1167     test_reserved_space2(size * 3 + ag);
1168     test_reserved_space2(size * 10);
1169     test_reserved_space2(size * 10 + size / 2);
1170   }
1171 
1172   static void test_reserved_space3() {
1173     size_t ag = os::vm_allocation_granularity();
1174 
1175     test_reserved_space3(ag,      ag    , false);
1176     test_reserved_space3(ag * 2,  ag    , false);
1177     test_reserved_space3(ag * 3,  ag    , false);
1178     test_reserved_space3(ag * 2,  ag * 2, false);
1179     test_reserved_space3(ag * 4,  ag * 2, false);
1180     test_reserved_space3(ag * 8,  ag * 2, false);
1181     test_reserved_space3(ag * 4,  ag * 4, false);
1182     test_reserved_space3(ag * 8,  ag * 4, false);
1183     test_reserved_space3(ag * 16, ag * 4, false);
1184 
1185     if (UseLargePages) {
1186       size_t lp = os::large_page_size();
1187 
1188       // Without large pages
1189       test_reserved_space3(lp,     ag * 4, false);
1190       test_reserved_space3(lp * 2, ag * 4, false);
1191       test_reserved_space3(lp * 4, ag * 4, false);
1192       test_reserved_space3(lp,     lp    , false);
1193       test_reserved_space3(lp * 2, lp    , false);
1194       test_reserved_space3(lp * 3, lp    , false);
1195       test_reserved_space3(lp * 2, lp * 2, false);
1196       test_reserved_space3(lp * 4, lp * 2, false);
1197       test_reserved_space3(lp * 8, lp * 2, false);
1198 
1199       // With large pages
1200       test_reserved_space3(lp, ag * 4    , true);
1201       test_reserved_space3(lp * 2, ag * 4, true);
1202       test_reserved_space3(lp * 4, ag * 4, true);
1203       test_reserved_space3(lp, lp        , true);
1204       test_reserved_space3(lp * 2, lp    , true);
1205       test_reserved_space3(lp * 3, lp    , true);
1206       test_reserved_space3(lp * 2, lp * 2, true);
1207       test_reserved_space3(lp * 4, lp * 2, true);
1208       test_reserved_space3(lp * 8, lp * 2, true);
1209     }
1210   }
1211 
1212   static void test_reserved_space() {
1213     test_reserved_space1();
1214     test_reserved_space2();
1215     test_reserved_space3();
1216   }
1217 };
1218 
1219 void TestReservedSpace_test() {
1220   TestReservedSpace::test_reserved_space();
1221 }
1222 
1223 #define assert_equals(actual, expected)  \
1224   assert(actual == expected,             \
1225          &quot;Got &quot; SIZE_FORMAT &quot; expected &quot; \
1226          SIZE_FORMAT, actual, expected);
1227 
1228 #define assert_ge(value1, value2)                  \
1229   assert(value1 &gt;= value2,                         \
1230          &quot;&#39;&quot; #value1 &quot;&#39;: &quot; SIZE_FORMAT &quot; &#39;&quot;        \
1231          #value2 &quot;&#39;: &quot; SIZE_FORMAT, value1, value2);
1232 
1233 #define assert_lt(value1, value2)                  \
1234   assert(value1 &lt; value2,                          \
1235          &quot;&#39;&quot; #value1 &quot;&#39;: &quot; SIZE_FORMAT &quot; &#39;&quot;        \
1236          #value2 &quot;&#39;: &quot; SIZE_FORMAT, value1, value2);
1237 
1238 
1239 class TestVirtualSpace : AllStatic {
1240   enum TestLargePages {
1241     Default,
1242     Disable,
1243     Reserve,
1244     Commit
1245   };
1246 
1247   static ReservedSpace reserve_memory(size_t reserve_size_aligned, TestLargePages mode) {
1248     switch(mode) {
1249     default:
1250     case Default:
1251     case Reserve:
1252       return ReservedSpace(reserve_size_aligned);
1253     case Disable:
1254     case Commit:
1255       return ReservedSpace(reserve_size_aligned,
1256                            os::vm_allocation_granularity(),
1257                            /* large */ false, /* exec */ false);
1258     }
1259   }
1260 
1261   static bool initialize_virtual_space(VirtualSpace&amp; vs, ReservedSpace rs, TestLargePages mode) {
1262     switch(mode) {
1263     default:
1264     case Default:
1265     case Reserve:
1266       return vs.initialize(rs, 0);
1267     case Disable:
1268       return vs.initialize_with_granularity(rs, 0, os::vm_page_size());
1269     case Commit:
1270       return vs.initialize_with_granularity(rs, 0, os::page_size_for_region_unaligned(rs.size(), 1));
1271     }
1272   }
1273 
1274  public:
1275   static void test_virtual_space_actual_committed_space(size_t reserve_size, size_t commit_size,
1276                                                         TestLargePages mode = Default) {
1277     size_t granularity = os::vm_allocation_granularity();
1278     size_t reserve_size_aligned = align_up(reserve_size, granularity);
1279 
1280     ReservedSpace reserved = reserve_memory(reserve_size_aligned, mode);
1281 
1282     assert(reserved.is_reserved(), &quot;Must be&quot;);
1283 
1284     VirtualSpace vs;
1285     bool initialized = initialize_virtual_space(vs, reserved, mode);
1286     assert(initialized, &quot;Failed to initialize VirtualSpace&quot;);
1287 
1288     vs.expand_by(commit_size, false);
1289 
1290     if (vs.special()) {
1291       assert_equals(vs.actual_committed_size(), reserve_size_aligned);
1292     } else {
1293       assert_ge(vs.actual_committed_size(), commit_size);
1294       // Approximate the commit granularity.
1295       // Make sure that we don&#39;t commit using large pages
1296       // if large pages has been disabled for this VirtualSpace.
1297       size_t commit_granularity = (mode == Disable || !UseLargePages) ?
1298                                    os::vm_page_size() : os::large_page_size();
1299       assert_lt(vs.actual_committed_size(), commit_size + commit_granularity);
1300     }
1301 
1302     reserved.release();
1303   }
1304 
1305   static void test_virtual_space_actual_committed_space_one_large_page() {
1306     if (!UseLargePages) {
1307       return;
1308     }
1309 
1310     size_t large_page_size = os::large_page_size();
1311 
1312     ReservedSpace reserved(large_page_size, large_page_size, true, false);
1313 
1314     assert(reserved.is_reserved(), &quot;Must be&quot;);
1315 
1316     VirtualSpace vs;
1317     bool initialized = vs.initialize(reserved, 0);
1318     assert(initialized, &quot;Failed to initialize VirtualSpace&quot;);
1319 
1320     vs.expand_by(large_page_size, false);
1321 
1322     assert_equals(vs.actual_committed_size(), large_page_size);
1323 
1324     reserved.release();
1325   }
1326 
1327   static void test_virtual_space_actual_committed_space() {
1328     test_virtual_space_actual_committed_space(4 * K, 0);
1329     test_virtual_space_actual_committed_space(4 * K, 4 * K);
1330     test_virtual_space_actual_committed_space(8 * K, 0);
1331     test_virtual_space_actual_committed_space(8 * K, 4 * K);
1332     test_virtual_space_actual_committed_space(8 * K, 8 * K);
1333     test_virtual_space_actual_committed_space(12 * K, 0);
1334     test_virtual_space_actual_committed_space(12 * K, 4 * K);
1335     test_virtual_space_actual_committed_space(12 * K, 8 * K);
1336     test_virtual_space_actual_committed_space(12 * K, 12 * K);
1337     test_virtual_space_actual_committed_space(64 * K, 0);
1338     test_virtual_space_actual_committed_space(64 * K, 32 * K);
1339     test_virtual_space_actual_committed_space(64 * K, 64 * K);
1340     test_virtual_space_actual_committed_space(2 * M, 0);
1341     test_virtual_space_actual_committed_space(2 * M, 4 * K);
1342     test_virtual_space_actual_committed_space(2 * M, 64 * K);
1343     test_virtual_space_actual_committed_space(2 * M, 1 * M);
1344     test_virtual_space_actual_committed_space(2 * M, 2 * M);
1345     test_virtual_space_actual_committed_space(10 * M, 0);
1346     test_virtual_space_actual_committed_space(10 * M, 4 * K);
1347     test_virtual_space_actual_committed_space(10 * M, 8 * K);
1348     test_virtual_space_actual_committed_space(10 * M, 1 * M);
1349     test_virtual_space_actual_committed_space(10 * M, 2 * M);
1350     test_virtual_space_actual_committed_space(10 * M, 5 * M);
1351     test_virtual_space_actual_committed_space(10 * M, 10 * M);
1352   }
1353 
1354   static void test_virtual_space_disable_large_pages() {
1355     if (!UseLargePages) {
1356       return;
1357     }
1358     // These test cases verify that if we force VirtualSpace to disable large pages
1359     test_virtual_space_actual_committed_space(10 * M, 0, Disable);
1360     test_virtual_space_actual_committed_space(10 * M, 4 * K, Disable);
1361     test_virtual_space_actual_committed_space(10 * M, 8 * K, Disable);
1362     test_virtual_space_actual_committed_space(10 * M, 1 * M, Disable);
1363     test_virtual_space_actual_committed_space(10 * M, 2 * M, Disable);
1364     test_virtual_space_actual_committed_space(10 * M, 5 * M, Disable);
1365     test_virtual_space_actual_committed_space(10 * M, 10 * M, Disable);
1366 
1367     test_virtual_space_actual_committed_space(10 * M, 0, Reserve);
1368     test_virtual_space_actual_committed_space(10 * M, 4 * K, Reserve);
1369     test_virtual_space_actual_committed_space(10 * M, 8 * K, Reserve);
1370     test_virtual_space_actual_committed_space(10 * M, 1 * M, Reserve);
1371     test_virtual_space_actual_committed_space(10 * M, 2 * M, Reserve);
1372     test_virtual_space_actual_committed_space(10 * M, 5 * M, Reserve);
1373     test_virtual_space_actual_committed_space(10 * M, 10 * M, Reserve);
1374 
1375     test_virtual_space_actual_committed_space(10 * M, 0, Commit);
1376     test_virtual_space_actual_committed_space(10 * M, 4 * K, Commit);
1377     test_virtual_space_actual_committed_space(10 * M, 8 * K, Commit);
1378     test_virtual_space_actual_committed_space(10 * M, 1 * M, Commit);
1379     test_virtual_space_actual_committed_space(10 * M, 2 * M, Commit);
1380     test_virtual_space_actual_committed_space(10 * M, 5 * M, Commit);
1381     test_virtual_space_actual_committed_space(10 * M, 10 * M, Commit);
1382   }
1383 
1384   static void test_virtual_space() {
1385     test_virtual_space_actual_committed_space();
1386     test_virtual_space_actual_committed_space_one_large_page();
1387     test_virtual_space_disable_large_pages();
1388   }
1389 };
1390 
1391 void TestVirtualSpace_test() {
1392   TestVirtualSpace::test_virtual_space();
1393 }
1394 
1395 #endif // PRODUCT
1396 
1397 #endif
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="2" type="hidden" />
</body>
</html>