<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/arm/c1_LIRAssembler_arm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (c) 2008, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.inline.hpp&quot;
  27 #include &quot;c1/c1_Compilation.hpp&quot;
  28 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  29 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  30 #include &quot;c1/c1_Runtime1.hpp&quot;
  31 #include &quot;c1/c1_ValueStack.hpp&quot;
  32 #include &quot;ci/ciArrayKlass.hpp&quot;
  33 #include &quot;ci/ciInstance.hpp&quot;
  34 #include &quot;gc/shared/collectedHeap.hpp&quot;
  35 #include &quot;memory/universe.hpp&quot;
  36 #include &quot;nativeInst_arm.hpp&quot;
  37 #include &quot;oops/objArrayKlass.hpp&quot;
  38 #include &quot;runtime/frame.inline.hpp&quot;
  39 #include &quot;runtime/sharedRuntime.hpp&quot;
<a name="1" id="anc1"></a><span class="line-added">  40 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  41 #include &quot;vmreg_arm.inline.hpp&quot;
  42 
  43 #define __ _masm-&gt;
  44 
  45 // Note: Rtemp usage is this file should not impact C2 and should be
  46 // correct as long as it is not implicitly used in lower layers (the
  47 // arm [macro]assembler) and used with care in the other C1 specific
  48 // files.
  49 
  50 bool LIR_Assembler::is_small_constant(LIR_Opr opr) {
  51   ShouldNotCallThis(); // Not used on ARM
  52   return false;
  53 }
  54 
  55 
  56 LIR_Opr LIR_Assembler::receiverOpr() {
  57   // The first register in Java calling conventions
  58   return FrameMap::R0_oop_opr;
  59 }
  60 
  61 LIR_Opr LIR_Assembler::osrBufferPointer() {
  62   return FrameMap::as_pointer_opr(R0);
  63 }
  64 
  65 #ifndef PRODUCT
  66 void LIR_Assembler::verify_reserved_argument_area_size(int args_count) {
  67   assert(args_count * wordSize &lt;= frame_map()-&gt;reserved_argument_area_size(), &quot;not enough space for arguments&quot;);
  68 }
  69 #endif // !PRODUCT
  70 
  71 void LIR_Assembler::store_parameter(jint c, int offset_from_sp_in_words) {
  72   assert(offset_from_sp_in_words &gt;= 0, &quot;invalid offset from sp&quot;);
  73   int offset_from_sp_in_bytes = offset_from_sp_in_words * BytesPerWord;
  74   assert(offset_from_sp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;not enough space&quot;);
  75   __ mov_slow(Rtemp, c);
  76   __ str(Rtemp, Address(SP, offset_from_sp_in_bytes));
  77 }
  78 
  79 void LIR_Assembler::store_parameter(Metadata* m, int offset_from_sp_in_words) {
  80   assert(offset_from_sp_in_words &gt;= 0, &quot;invalid offset from sp&quot;);
  81   int offset_from_sp_in_bytes = offset_from_sp_in_words * BytesPerWord;
  82   assert(offset_from_sp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;not enough space&quot;);
  83   __ mov_metadata(Rtemp, m);
  84   __ str(Rtemp, Address(SP, offset_from_sp_in_bytes));
  85 }
  86 
  87 //--------------fpu register translations-----------------------
  88 
  89 
  90 void LIR_Assembler::breakpoint() {
  91   __ breakpoint();
  92 }
  93 
  94 void LIR_Assembler::push(LIR_Opr opr) {
  95   Unimplemented();
  96 }
  97 
  98 void LIR_Assembler::pop(LIR_Opr opr) {
  99   Unimplemented();
 100 }
 101 
 102 //-------------------------------------------
 103 Address LIR_Assembler::as_Address(LIR_Address* addr) {
 104   Register base = addr-&gt;base()-&gt;as_pointer_register();
 105 
 106 
 107   if (addr-&gt;index()-&gt;is_illegal() || addr-&gt;index()-&gt;is_constant()) {
 108     int offset = addr-&gt;disp();
 109     if (addr-&gt;index()-&gt;is_constant()) {
 110       offset += addr-&gt;index()-&gt;as_constant_ptr()-&gt;as_jint() &lt;&lt; addr-&gt;scale();
 111     }
 112 
 113     if ((offset &lt;= -4096) || (offset &gt;= 4096)) {
 114       BAILOUT_(&quot;offset not in range&quot;, Address(base));
 115     }
 116 
 117     return Address(base, offset);
 118 
 119   } else {
 120     assert(addr-&gt;disp() == 0, &quot;can&#39;t have both&quot;);
 121     int scale = addr-&gt;scale();
 122 
 123     assert(addr-&gt;index()-&gt;is_single_cpu(), &quot;should be&quot;);
 124     return scale &gt;= 0 ? Address(base, addr-&gt;index()-&gt;as_register(), lsl, scale) :
 125                         Address(base, addr-&gt;index()-&gt;as_register(), lsr, -scale);
 126   }
 127 }
 128 
 129 Address LIR_Assembler::as_Address_hi(LIR_Address* addr) {
 130   Address base = as_Address(addr);
 131   assert(base.index() == noreg, &quot;must be&quot;);
 132   if (base.disp() + BytesPerWord &gt;= 4096) { BAILOUT_(&quot;offset not in range&quot;, Address(base.base(),0)); }
 133   return Address(base.base(), base.disp() + BytesPerWord);
 134 }
 135 
 136 Address LIR_Assembler::as_Address_lo(LIR_Address* addr) {
 137   return as_Address(addr);
 138 }
 139 
 140 
 141 void LIR_Assembler::osr_entry() {
 142   offsets()-&gt;set_value(CodeOffsets::OSR_Entry, code_offset());
 143   BlockBegin* osr_entry = compilation()-&gt;hir()-&gt;osr_entry();
 144   ValueStack* entry_state = osr_entry-&gt;end()-&gt;state();
 145   int number_of_locks = entry_state-&gt;locks_size();
 146 
 147   __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());
 148   Register OSR_buf = osrBufferPointer()-&gt;as_pointer_register();
 149 
 150   assert(frame::interpreter_frame_monitor_size() == BasicObjectLock::size(), &quot;adjust code below&quot;);
 151   int monitor_offset = (method()-&gt;max_locals() + 2 * (number_of_locks - 1)) * BytesPerWord;
 152   for (int i = 0; i &lt; number_of_locks; i++) {
 153     int slot_offset = monitor_offset - (i * 2 * BytesPerWord);
 154     __ ldr(R1, Address(OSR_buf, slot_offset + 0*BytesPerWord));
 155     __ ldr(R2, Address(OSR_buf, slot_offset + 1*BytesPerWord));
 156     __ str(R1, frame_map()-&gt;address_for_monitor_lock(i));
 157     __ str(R2, frame_map()-&gt;address_for_monitor_object(i));
 158   }
 159 }
 160 
 161 
 162 int LIR_Assembler::check_icache() {
 163   Register receiver = LIR_Assembler::receiverOpr()-&gt;as_register();
 164   int offset = __ offset();
 165   __ inline_cache_check(receiver, Ricklass);
 166   return offset;
 167 }
 168 
 169 void LIR_Assembler::clinit_barrier(ciMethod* method) {
 170   ShouldNotReachHere(); // not implemented
 171 }
 172 
 173 void LIR_Assembler::jobject2reg_with_patching(Register reg, CodeEmitInfo* info) {
 174   jobject o = (jobject)Universe::non_oop_word();
 175   int index = __ oop_recorder()-&gt;allocate_oop_index(o);
 176 
 177   PatchingStub* patch = new PatchingStub(_masm, patching_id(info), index);
 178 
 179   __ patchable_mov_oop(reg, o, index);
 180   patching_epilog(patch, lir_patch_normal, reg, info);
 181 }
 182 
 183 
 184 void LIR_Assembler::klass2reg_with_patching(Register reg, CodeEmitInfo* info) {
 185   Metadata* o = (Metadata*)Universe::non_oop_word();
 186   int index = __ oop_recorder()-&gt;allocate_metadata_index(o);
 187   PatchingStub* patch = new PatchingStub(_masm, PatchingStub::load_klass_id, index);
 188 
 189   __ patchable_mov_metadata(reg, o, index);
 190   patching_epilog(patch, lir_patch_normal, reg, info);
 191 }
 192 
 193 
 194 int LIR_Assembler::initial_frame_size_in_bytes() const {
 195   // Subtracts two words to account for return address and link
 196   return frame_map()-&gt;framesize()*VMRegImpl::stack_slot_size - 2*wordSize;
 197 }
 198 
 199 
 200 int LIR_Assembler::emit_exception_handler() {
 201   // TODO: ARM
 202   __ nop(); // See comments in other ports
 203 
 204   address handler_base = __ start_a_stub(exception_handler_size());
 205   if (handler_base == NULL) {
 206     bailout(&quot;exception handler overflow&quot;);
 207     return -1;
 208   }
 209 
 210   int offset = code_offset();
 211 
 212   // check that there is really an exception
 213   __ verify_not_null_oop(Rexception_obj);
 214 
 215   __ call(Runtime1::entry_for(Runtime1::handle_exception_from_callee_id), relocInfo::runtime_call_type);
 216   __ should_not_reach_here();
 217 
 218   assert(code_offset() - offset &lt;= exception_handler_size(), &quot;overflow&quot;);
 219   __ end_a_stub();
 220 
 221   return offset;
 222 }
 223 
 224 // Emit the code to remove the frame from the stack in the exception
 225 // unwind path.
 226 int LIR_Assembler::emit_unwind_handler() {
 227 #ifndef PRODUCT
 228   if (CommentedAssembly) {
 229     _masm-&gt;block_comment(&quot;Unwind handler&quot;);
 230   }
 231 #endif
 232 
 233   int offset = code_offset();
 234 
 235   // Fetch the exception from TLS and clear out exception related thread state
 236   Register zero = __ zero_register(Rtemp);
 237   __ ldr(Rexception_obj, Address(Rthread, JavaThread::exception_oop_offset()));
 238   __ str(zero, Address(Rthread, JavaThread::exception_oop_offset()));
 239   __ str(zero, Address(Rthread, JavaThread::exception_pc_offset()));
 240 
 241   __ bind(_unwind_handler_entry);
 242   __ verify_not_null_oop(Rexception_obj);
 243 
 244   // Preform needed unlocking
 245   MonitorExitStub* stub = NULL;
 246   if (method()-&gt;is_synchronized()) {
 247     monitor_address(0, FrameMap::R0_opr);
 248     stub = new MonitorExitStub(FrameMap::R0_opr, true, 0);
 249     __ unlock_object(R2, R1, R0, Rtemp, *stub-&gt;entry());
 250     __ bind(*stub-&gt;continuation());
 251   }
 252 
 253   // remove the activation and dispatch to the unwind handler
 254   __ remove_frame(initial_frame_size_in_bytes()); // restores FP and LR
 255   __ jump(Runtime1::entry_for(Runtime1::unwind_exception_id), relocInfo::runtime_call_type, Rtemp);
 256 
 257   // Emit the slow path assembly
 258   if (stub != NULL) {
 259     stub-&gt;emit_code(this);
 260   }
 261 
 262   return offset;
 263 }
 264 
 265 
 266 int LIR_Assembler::emit_deopt_handler() {
 267   address handler_base = __ start_a_stub(deopt_handler_size());
 268   if (handler_base == NULL) {
 269     bailout(&quot;deopt handler overflow&quot;);
 270     return -1;
 271   }
 272 
 273   int offset = code_offset();
 274 
 275   __ mov_relative_address(LR, __ pc());
 276   __ push(LR); // stub expects LR to be saved
 277   __ jump(SharedRuntime::deopt_blob()-&gt;unpack(), relocInfo::runtime_call_type, noreg);
 278 
 279   assert(code_offset() - offset &lt;= deopt_handler_size(), &quot;overflow&quot;);
 280   __ end_a_stub();
 281 
 282   return offset;
 283 }
 284 
 285 
 286 void LIR_Assembler::return_op(LIR_Opr result) {
 287   // Pop the frame before safepoint polling
 288   __ remove_frame(initial_frame_size_in_bytes());
 289   __ read_polling_page(Rtemp, relocInfo::poll_return_type);
 290   __ ret();
 291 }
 292 
 293 int LIR_Assembler::safepoint_poll(LIR_Opr tmp, CodeEmitInfo* info) {
<a name="2" id="anc2"></a><span class="line-modified"> 294 </span>


 295   int offset = __ offset();
<a name="3" id="anc3"></a><span class="line-modified"> 296   __ get_polling_page(Rtemp);</span>
<span class="line-added"> 297   __ relocate(relocInfo::poll_type);</span>
<span class="line-added"> 298   add_debug_info_for_branch(info); // help pc_desc_at to find correct scope for current PC</span>
<span class="line-added"> 299   __ ldr(Rtemp, Address(Rtemp));</span>
<span class="line-added"> 300 </span>
 301   return offset;
 302 }
 303 
 304 
 305 void LIR_Assembler::move_regs(Register from_reg, Register to_reg) {
 306   if (from_reg != to_reg) {
 307     __ mov(to_reg, from_reg);
 308   }
 309 }
 310 
 311 void LIR_Assembler::const2reg(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
 312   assert(src-&gt;is_constant() &amp;&amp; dest-&gt;is_register(), &quot;must be&quot;);
 313   LIR_Const* c = src-&gt;as_constant_ptr();
 314 
 315   switch (c-&gt;type()) {
 316     case T_ADDRESS:
 317     case T_INT:
 318       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 319       __ mov_slow(dest-&gt;as_register(), c-&gt;as_jint());
 320       break;
 321 
 322     case T_LONG:
 323       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 324       __ mov_slow(dest-&gt;as_register_lo(), c-&gt;as_jint_lo());
 325       __ mov_slow(dest-&gt;as_register_hi(), c-&gt;as_jint_hi());
 326       break;
 327 
 328     case T_OBJECT:
 329       if (patch_code == lir_patch_none) {
 330         __ mov_oop(dest-&gt;as_register(), c-&gt;as_jobject());
 331       } else {
 332         jobject2reg_with_patching(dest-&gt;as_register(), info);
 333       }
 334       break;
 335 
 336     case T_METADATA:
 337       if (patch_code == lir_patch_none) {
 338         __ mov_metadata(dest-&gt;as_register(), c-&gt;as_metadata());
 339       } else {
 340         klass2reg_with_patching(dest-&gt;as_register(), info);
 341       }
 342       break;
 343 
 344     case T_FLOAT:
 345       if (dest-&gt;is_single_fpu()) {
 346         __ mov_float(dest-&gt;as_float_reg(), c-&gt;as_jfloat());
 347       } else {
 348         // Simple getters can return float constant directly into r0
 349         __ mov_slow(dest-&gt;as_register(), c-&gt;as_jint_bits());
 350       }
 351       break;
 352 
 353     case T_DOUBLE:
 354       if (dest-&gt;is_double_fpu()) {
 355         __ mov_double(dest-&gt;as_double_reg(), c-&gt;as_jdouble());
 356       } else {
 357         // Simple getters can return double constant directly into r1r0
 358         __ mov_slow(dest-&gt;as_register_lo(), c-&gt;as_jint_lo_bits());
 359         __ mov_slow(dest-&gt;as_register_hi(), c-&gt;as_jint_hi_bits());
 360       }
 361       break;
 362 
 363     default:
 364       ShouldNotReachHere();
 365   }
 366 }
 367 
 368 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
 369   assert(src-&gt;is_constant(), &quot;must be&quot;);
 370   assert(dest-&gt;is_stack(), &quot;must be&quot;);
 371   LIR_Const* c = src-&gt;as_constant_ptr();
 372 
 373   switch (c-&gt;type()) {
 374     case T_INT:  // fall through
 375     case T_FLOAT:
 376       __ mov_slow(Rtemp, c-&gt;as_jint_bits());
 377       __ str_32(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 378       break;
 379 
 380     case T_ADDRESS:
 381       __ mov_slow(Rtemp, c-&gt;as_jint());
 382       __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 383       break;
 384 
 385     case T_OBJECT:
 386       __ mov_oop(Rtemp, c-&gt;as_jobject());
 387       __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 388       break;
 389 
 390     case T_LONG:  // fall through
 391     case T_DOUBLE:
 392       __ mov_slow(Rtemp, c-&gt;as_jint_lo_bits());
 393       __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), lo_word_offset_in_bytes));
 394       if (c-&gt;as_jint_hi_bits() != c-&gt;as_jint_lo_bits()) {
 395         __ mov_slow(Rtemp, c-&gt;as_jint_hi_bits());
 396       }
 397       __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 398       break;
 399 
 400     default:
 401       ShouldNotReachHere();
 402   }
 403 }
 404 
 405 void LIR_Assembler::const2mem(LIR_Opr src, LIR_Opr dest, BasicType type,
 406                               CodeEmitInfo* info, bool wide) {
 407   assert((src-&gt;as_constant_ptr()-&gt;type() == T_OBJECT &amp;&amp; src-&gt;as_constant_ptr()-&gt;as_jobject() == NULL),&quot;cannot handle otherwise&quot;);
 408   __ mov(Rtemp, 0);
 409 
 410   int null_check_offset = code_offset();
 411   __ str(Rtemp, as_Address(dest-&gt;as_address_ptr()));
 412 
 413   if (info != NULL) {
 414     assert(false, &quot;arm32 didn&#39;t support this before, investigate if bug&quot;);
 415     add_debug_info_for_null_check(null_check_offset, info);
 416   }
 417 }
 418 
 419 void LIR_Assembler::reg2reg(LIR_Opr src, LIR_Opr dest) {
 420   assert(src-&gt;is_register() &amp;&amp; dest-&gt;is_register(), &quot;must be&quot;);
 421 
 422   if (src-&gt;is_single_cpu()) {
 423     if (dest-&gt;is_single_cpu()) {
 424       move_regs(src-&gt;as_register(), dest-&gt;as_register());
 425     } else if (dest-&gt;is_single_fpu()) {
 426       __ fmsr(dest-&gt;as_float_reg(), src-&gt;as_register());
 427     } else {
 428       ShouldNotReachHere();
 429     }
 430   } else if (src-&gt;is_double_cpu()) {
 431     if (dest-&gt;is_double_cpu()) {
 432       __ long_move(dest-&gt;as_register_lo(), dest-&gt;as_register_hi(), src-&gt;as_register_lo(), src-&gt;as_register_hi());
 433     } else {
 434       __ fmdrr(dest-&gt;as_double_reg(), src-&gt;as_register_lo(), src-&gt;as_register_hi());
 435     }
 436   } else if (src-&gt;is_single_fpu()) {
 437     if (dest-&gt;is_single_fpu()) {
 438       __ mov_float(dest-&gt;as_float_reg(), src-&gt;as_float_reg());
 439     } else if (dest-&gt;is_single_cpu()) {
 440       __ mov_fpr2gpr_float(dest-&gt;as_register(), src-&gt;as_float_reg());
 441     } else {
 442       ShouldNotReachHere();
 443     }
 444   } else if (src-&gt;is_double_fpu()) {
 445     if (dest-&gt;is_double_fpu()) {
 446       __ mov_double(dest-&gt;as_double_reg(), src-&gt;as_double_reg());
 447     } else if (dest-&gt;is_double_cpu()) {
 448       __ fmrrd(dest-&gt;as_register_lo(), dest-&gt;as_register_hi(), src-&gt;as_double_reg());
 449     } else {
 450       ShouldNotReachHere();
 451     }
 452   } else {
 453     ShouldNotReachHere();
 454   }
 455 }
 456 
 457 void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {
 458   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 459   assert(dest-&gt;is_stack(), &quot;should not call otherwise&quot;);
 460 
 461   Address addr = dest-&gt;is_single_word() ?
 462     frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()) :
 463     frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
 464 
 465   assert(lo_word_offset_in_bytes == 0 &amp;&amp; hi_word_offset_in_bytes == 4, &quot;little ending&quot;);
 466   if (src-&gt;is_single_fpu() || src-&gt;is_double_fpu()) {
 467     if (addr.disp() &gt;= 1024) { BAILOUT(&quot;Too exotic case to handle here&quot;); }
 468   }
 469 
 470   if (src-&gt;is_single_cpu()) {
 471     switch (type) {
 472       case T_OBJECT:
 473       case T_ARRAY:    __ verify_oop(src-&gt;as_register());   // fall through
 474       case T_ADDRESS:
 475       case T_METADATA: __ str(src-&gt;as_register(), addr);    break;
 476       case T_FLOAT:    // used in intBitsToFloat intrinsic implementation, fall through
 477       case T_INT:      __ str_32(src-&gt;as_register(), addr); break;
 478       default:
 479         ShouldNotReachHere();
 480     }
 481   } else if (src-&gt;is_double_cpu()) {
 482     __ str(src-&gt;as_register_lo(), addr);
 483     __ str(src-&gt;as_register_hi(), frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 484   } else if (src-&gt;is_single_fpu()) {
 485     __ str_float(src-&gt;as_float_reg(), addr);
 486   } else if (src-&gt;is_double_fpu()) {
 487     __ str_double(src-&gt;as_double_reg(), addr);
 488   } else {
 489     ShouldNotReachHere();
 490   }
 491 }
 492 
 493 
 494 void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type,
 495                             LIR_PatchCode patch_code, CodeEmitInfo* info,
 496                             bool pop_fpu_stack, bool wide,
 497                             bool unaligned) {
 498   LIR_Address* to_addr = dest-&gt;as_address_ptr();
 499   Register base_reg = to_addr-&gt;base()-&gt;as_pointer_register();
 500   const bool needs_patching = (patch_code != lir_patch_none);
 501 
 502   PatchingStub* patch = NULL;
 503   if (needs_patching) {
 504     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 505   }
 506 
 507   int null_check_offset = code_offset();
 508 
 509   switch (type) {
 510     case T_ARRAY:
 511     case T_OBJECT:
 512       if (UseCompressedOops &amp;&amp; !wide) {
 513         ShouldNotReachHere();
 514       } else {
 515         __ str(src-&gt;as_register(), as_Address(to_addr));
 516       }
 517       break;
 518 
 519     case T_ADDRESS:
 520       __ str(src-&gt;as_pointer_register(), as_Address(to_addr));
 521       break;
 522 
 523     case T_BYTE:
 524     case T_BOOLEAN:
 525       __ strb(src-&gt;as_register(), as_Address(to_addr));
 526       break;
 527 
 528     case T_CHAR:
 529     case T_SHORT:
 530       __ strh(src-&gt;as_register(), as_Address(to_addr));
 531       break;
 532 
 533     case T_INT:
 534 #ifdef __SOFTFP__
 535     case T_FLOAT:
 536 #endif // __SOFTFP__
 537       __ str_32(src-&gt;as_register(), as_Address(to_addr));
 538       break;
 539 
 540 
 541 #ifdef __SOFTFP__
 542     case T_DOUBLE:
 543 #endif // __SOFTFP__
 544     case T_LONG: {
 545       Register from_lo = src-&gt;as_register_lo();
 546       Register from_hi = src-&gt;as_register_hi();
 547       if (to_addr-&gt;index()-&gt;is_register()) {
 548         assert(to_addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 549         assert(to_addr-&gt;disp() == 0, &quot;Not yet supporting both&quot;);
 550         __ add(Rtemp, base_reg, to_addr-&gt;index()-&gt;as_register());
 551         base_reg = Rtemp;
 552         __ str(from_lo, Address(Rtemp));
 553         if (patch != NULL) {
 554           __ nop(); // see comment before patching_epilog for 2nd str
 555           patching_epilog(patch, lir_patch_low, base_reg, info);
 556           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 557           patch_code = lir_patch_high;
 558         }
 559         __ str(from_hi, Address(Rtemp, BytesPerWord));
 560       } else if (base_reg == from_lo) {
 561         __ str(from_hi, as_Address_hi(to_addr));
 562         if (patch != NULL) {
 563           __ nop(); // see comment before patching_epilog for 2nd str
 564           patching_epilog(patch, lir_patch_high, base_reg, info);
 565           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 566           patch_code = lir_patch_low;
 567         }
 568         __ str(from_lo, as_Address_lo(to_addr));
 569       } else {
 570         __ str(from_lo, as_Address_lo(to_addr));
 571         if (patch != NULL) {
 572           __ nop(); // see comment before patching_epilog for 2nd str
 573           patching_epilog(patch, lir_patch_low, base_reg, info);
 574           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 575           patch_code = lir_patch_high;
 576         }
 577         __ str(from_hi, as_Address_hi(to_addr));
 578       }
 579       break;
 580     }
 581 
 582 #ifndef __SOFTFP__
 583     case T_FLOAT:
 584       if (to_addr-&gt;index()-&gt;is_register()) {
 585         assert(to_addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 586         __ add(Rtemp, base_reg, to_addr-&gt;index()-&gt;as_register());
 587         if ((to_addr-&gt;disp() &lt;= -4096) || (to_addr-&gt;disp() &gt;= 4096)) { BAILOUT(&quot;offset not in range&quot;); }
 588         __ fsts(src-&gt;as_float_reg(), Address(Rtemp, to_addr-&gt;disp()));
 589       } else {
 590         __ fsts(src-&gt;as_float_reg(), as_Address(to_addr));
 591       }
 592       break;
 593 
 594     case T_DOUBLE:
 595       if (to_addr-&gt;index()-&gt;is_register()) {
 596         assert(to_addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 597         __ add(Rtemp, base_reg, to_addr-&gt;index()-&gt;as_register());
 598         if ((to_addr-&gt;disp() &lt;= -4096) || (to_addr-&gt;disp() &gt;= 4096)) { BAILOUT(&quot;offset not in range&quot;); }
 599         __ fstd(src-&gt;as_double_reg(), Address(Rtemp, to_addr-&gt;disp()));
 600       } else {
 601         __ fstd(src-&gt;as_double_reg(), as_Address(to_addr));
 602       }
 603       break;
 604 #endif // __SOFTFP__
 605 
 606 
 607     default:
 608       ShouldNotReachHere();
 609   }
 610 
 611   if (info != NULL) {
 612     add_debug_info_for_null_check(null_check_offset, info);
 613   }
 614 
 615   if (patch != NULL) {
 616     // Offset embedded into LDR/STR instruction may appear not enough
 617     // to address a field. So, provide a space for one more instruction
 618     // that will deal with larger offsets.
 619     __ nop();
 620     patching_epilog(patch, patch_code, base_reg, info);
 621   }
 622 }
 623 
 624 
 625 void LIR_Assembler::stack2reg(LIR_Opr src, LIR_Opr dest, BasicType type) {
 626   assert(src-&gt;is_stack(), &quot;should not call otherwise&quot;);
 627   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 628 
 629   Address addr = src-&gt;is_single_word() ?
 630     frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()) :
 631     frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
 632 
 633   assert(lo_word_offset_in_bytes == 0 &amp;&amp; hi_word_offset_in_bytes == 4, &quot;little ending&quot;);
 634   if (dest-&gt;is_single_fpu() || dest-&gt;is_double_fpu()) {
 635     if (addr.disp() &gt;= 1024) { BAILOUT(&quot;Too exotic case to handle here&quot;); }
 636   }
 637 
 638   if (dest-&gt;is_single_cpu()) {
 639     switch (type) {
 640       case T_OBJECT:
 641       case T_ARRAY:
 642       case T_ADDRESS:
 643       case T_METADATA: __ ldr(dest-&gt;as_register(), addr); break;
 644       case T_FLOAT:    // used in floatToRawIntBits intrinsic implemenation
 645       case T_INT:      __ ldr_u32(dest-&gt;as_register(), addr); break;
 646       default:
 647         ShouldNotReachHere();
 648     }
 649     if ((type == T_OBJECT) || (type == T_ARRAY)) {
 650       __ verify_oop(dest-&gt;as_register());
 651     }
 652   } else if (dest-&gt;is_double_cpu()) {
 653     __ ldr(dest-&gt;as_register_lo(), addr);
 654     __ ldr(dest-&gt;as_register_hi(), frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 655   } else if (dest-&gt;is_single_fpu()) {
 656     __ ldr_float(dest-&gt;as_float_reg(), addr);
 657   } else if (dest-&gt;is_double_fpu()) {
 658     __ ldr_double(dest-&gt;as_double_reg(), addr);
 659   } else {
 660     ShouldNotReachHere();
 661   }
 662 }
 663 
 664 
 665 void LIR_Assembler::stack2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {
 666   if (src-&gt;is_single_stack()) {
 667     switch (src-&gt;type()) {
 668       case T_OBJECT:
 669       case T_ARRAY:
 670       case T_ADDRESS:
 671       case T_METADATA:
 672         __ ldr(Rtemp, frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 673         __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 674         break;
 675 
 676       case T_INT:
 677       case T_FLOAT:
 678         __ ldr_u32(Rtemp, frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 679         __ str_32(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 680         break;
 681 
 682       default:
 683         ShouldNotReachHere();
 684     }
 685   } else {
 686     assert(src-&gt;is_double_stack(), &quot;must be&quot;);
 687     __ ldr(Rtemp, frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), lo_word_offset_in_bytes));
 688     __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), lo_word_offset_in_bytes));
 689     __ ldr(Rtemp, frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 690     __ str(Rtemp, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), hi_word_offset_in_bytes));
 691   }
 692 }
 693 
 694 
 695 void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type,
 696                             LIR_PatchCode patch_code, CodeEmitInfo* info,
 697                             bool wide, bool unaligned) {
 698   assert(src-&gt;is_address(), &quot;should not call otherwise&quot;);
 699   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 700   LIR_Address* addr = src-&gt;as_address_ptr();
 701 
 702   Register base_reg = addr-&gt;base()-&gt;as_pointer_register();
 703 
 704   PatchingStub* patch = NULL;
 705   if (patch_code != lir_patch_none) {
 706     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 707   }
 708   if (info != NULL) {
 709     add_debug_info_for_null_check_here(info);
 710   }
 711 
 712   switch (type) {
 713     case T_OBJECT:  // fall through
 714     case T_ARRAY:
 715       if (UseCompressedOops &amp;&amp; !wide) {
 716         __ ldr_u32(dest-&gt;as_register(), as_Address(addr));
 717       } else {
 718         __ ldr(dest-&gt;as_register(), as_Address(addr));
 719       }
 720       break;
 721 
 722     case T_ADDRESS:
 723       if (UseCompressedClassPointers &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
 724         __ ldr_u32(dest-&gt;as_pointer_register(), as_Address(addr));
 725       } else {
 726         __ ldr(dest-&gt;as_pointer_register(), as_Address(addr));
 727       }
 728       break;
 729 
 730     case T_INT:
 731 #ifdef __SOFTFP__
 732     case T_FLOAT:
 733 #endif // __SOFTFP__
 734       __ ldr(dest-&gt;as_pointer_register(), as_Address(addr));
 735       break;
 736 
 737     case T_BOOLEAN:
 738       __ ldrb(dest-&gt;as_register(), as_Address(addr));
 739       break;
 740 
 741     case T_BYTE:
 742       __ ldrsb(dest-&gt;as_register(), as_Address(addr));
 743       break;
 744 
 745     case T_CHAR:
 746       __ ldrh(dest-&gt;as_register(), as_Address(addr));
 747       break;
 748 
 749     case T_SHORT:
 750       __ ldrsh(dest-&gt;as_register(), as_Address(addr));
 751       break;
 752 
 753 
 754 #ifdef __SOFTFP__
 755     case T_DOUBLE:
 756 #endif // __SOFTFP__
 757     case T_LONG: {
 758       Register to_lo = dest-&gt;as_register_lo();
 759       Register to_hi = dest-&gt;as_register_hi();
 760       if (addr-&gt;index()-&gt;is_register()) {
 761         assert(addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 762         assert(addr-&gt;disp() == 0, &quot;Not yet supporting both&quot;);
 763         __ add(Rtemp, base_reg, addr-&gt;index()-&gt;as_register());
 764         base_reg = Rtemp;
 765         __ ldr(to_lo, Address(Rtemp));
 766         if (patch != NULL) {
 767           __ nop(); // see comment before patching_epilog for 2nd ldr
 768           patching_epilog(patch, lir_patch_low, base_reg, info);
 769           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 770           patch_code = lir_patch_high;
 771         }
 772         __ ldr(to_hi, Address(Rtemp, BytesPerWord));
 773       } else if (base_reg == to_lo) {
 774         __ ldr(to_hi, as_Address_hi(addr));
 775         if (patch != NULL) {
 776           __ nop(); // see comment before patching_epilog for 2nd ldr
 777           patching_epilog(patch, lir_patch_high, base_reg, info);
 778           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 779           patch_code = lir_patch_low;
 780         }
 781         __ ldr(to_lo, as_Address_lo(addr));
 782       } else {
 783         __ ldr(to_lo, as_Address_lo(addr));
 784         if (patch != NULL) {
 785           __ nop(); // see comment before patching_epilog for 2nd ldr
 786           patching_epilog(patch, lir_patch_low, base_reg, info);
 787           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 788           patch_code = lir_patch_high;
 789         }
 790         __ ldr(to_hi, as_Address_hi(addr));
 791       }
 792       break;
 793     }
 794 
 795 #ifndef __SOFTFP__
 796     case T_FLOAT:
 797       if (addr-&gt;index()-&gt;is_register()) {
 798         assert(addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 799         __ add(Rtemp, base_reg, addr-&gt;index()-&gt;as_register());
 800         if ((addr-&gt;disp() &lt;= -4096) || (addr-&gt;disp() &gt;= 4096)) { BAILOUT(&quot;offset not in range&quot;); }
 801         __ flds(dest-&gt;as_float_reg(), Address(Rtemp, addr-&gt;disp()));
 802       } else {
 803         __ flds(dest-&gt;as_float_reg(), as_Address(addr));
 804       }
 805       break;
 806 
 807     case T_DOUBLE:
 808       if (addr-&gt;index()-&gt;is_register()) {
 809         assert(addr-&gt;scale() == LIR_Address::times_1,&quot;Unexpected scaled register&quot;);
 810         __ add(Rtemp, base_reg, addr-&gt;index()-&gt;as_register());
 811         if ((addr-&gt;disp() &lt;= -4096) || (addr-&gt;disp() &gt;= 4096)) { BAILOUT(&quot;offset not in range&quot;); }
 812         __ fldd(dest-&gt;as_double_reg(), Address(Rtemp, addr-&gt;disp()));
 813       } else {
 814         __ fldd(dest-&gt;as_double_reg(), as_Address(addr));
 815       }
 816       break;
 817 #endif // __SOFTFP__
 818 
 819 
 820     default:
 821       ShouldNotReachHere();
 822   }
 823 
 824   if (patch != NULL) {
 825     // Offset embedded into LDR/STR instruction may appear not enough
 826     // to address a field. So, provide a space for one more instruction
 827     // that will deal with larger offsets.
 828     __ nop();
 829     patching_epilog(patch, patch_code, base_reg, info);
 830   }
 831 
 832 }
 833 
 834 
 835 void LIR_Assembler::emit_op3(LIR_Op3* op) {
 836   bool is_32 = op-&gt;result_opr()-&gt;is_single_cpu();
 837 
 838   if (op-&gt;code() == lir_idiv &amp;&amp; op-&gt;in_opr2()-&gt;is_constant() &amp;&amp; is_32) {
 839     int c = op-&gt;in_opr2()-&gt;as_constant_ptr()-&gt;as_jint();
 840     assert(is_power_of_2(c), &quot;non power-of-2 constant should be put in a register&quot;);
 841 
 842     Register left = op-&gt;in_opr1()-&gt;as_register();
 843     Register dest = op-&gt;result_opr()-&gt;as_register();
 844     if (c == 1) {
 845       __ mov(dest, left);
 846     } else if (c == 2) {
 847       __ add_32(dest, left, AsmOperand(left, lsr, 31));
 848       __ asr_32(dest, dest, 1);
 849     } else if (c != (int) 0x80000000) {
 850       int power = log2_intptr(c);
 851       __ asr_32(Rtemp, left, 31);
 852       __ add_32(dest, left, AsmOperand(Rtemp, lsr, 32-power)); // dest = left + (left &lt; 0 ? 2^power - 1 : 0);
 853       __ asr_32(dest, dest, power);                            // dest = dest &gt;&gt;&gt; power;
 854     } else {
 855       // x/0x80000000 is a special case, since dividend is a power of two, but is negative.
 856       // The only possible result values are 0 and 1, with 1 only for dividend == divisor == 0x80000000.
 857       __ cmp_32(left, c);
 858       __ mov(dest, 0, ne);
 859       __ mov(dest, 1, eq);
 860     }
 861   } else {
 862     assert(op-&gt;code() == lir_idiv || op-&gt;code() == lir_irem, &quot;unexpected op3&quot;);
 863     __ call(StubRoutines::Arm::idiv_irem_entry(), relocInfo::runtime_call_type);
 864     add_debug_info_for_div0_here(op-&gt;info());
 865   }
 866 }
 867 
 868 
 869 void LIR_Assembler::emit_opBranch(LIR_OpBranch* op) {
 870 #ifdef ASSERT
 871   assert(op-&gt;block() == NULL || op-&gt;block()-&gt;label() == op-&gt;label(), &quot;wrong label&quot;);
 872   if (op-&gt;block() != NULL)  _branch_target_blocks.append(op-&gt;block());
 873   if (op-&gt;ublock() != NULL) _branch_target_blocks.append(op-&gt;ublock());
 874   assert(op-&gt;info() == NULL, &quot;CodeEmitInfo?&quot;);
 875 #endif // ASSERT
 876 
 877 #ifdef __SOFTFP__
 878   assert (op-&gt;code() != lir_cond_float_branch, &quot;this should be impossible&quot;);
 879 #else
 880   if (op-&gt;code() == lir_cond_float_branch) {
 881     __ fmstat();
 882     __ b(*(op-&gt;ublock()-&gt;label()), vs);
 883   }
 884 #endif // __SOFTFP__
 885 
 886   AsmCondition acond = al;
 887   switch (op-&gt;cond()) {
 888     case lir_cond_equal:        acond = eq; break;
 889     case lir_cond_notEqual:     acond = ne; break;
 890     case lir_cond_less:         acond = lt; break;
 891     case lir_cond_lessEqual:    acond = le; break;
 892     case lir_cond_greaterEqual: acond = ge; break;
 893     case lir_cond_greater:      acond = gt; break;
 894     case lir_cond_aboveEqual:   acond = hs; break;
 895     case lir_cond_belowEqual:   acond = ls; break;
 896     default: assert(op-&gt;cond() == lir_cond_always, &quot;must be&quot;);
 897   }
 898   __ b(*(op-&gt;label()), acond);
 899 }
 900 
 901 
 902 void LIR_Assembler::emit_opConvert(LIR_OpConvert* op) {
 903   LIR_Opr src  = op-&gt;in_opr();
 904   LIR_Opr dest = op-&gt;result_opr();
 905 
 906   switch (op-&gt;bytecode()) {
 907     case Bytecodes::_i2l:
 908       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
 909       __ mov(dest-&gt;as_register_hi(), AsmOperand(src-&gt;as_register(), asr, 31));
 910       break;
 911     case Bytecodes::_l2i:
 912       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
 913       break;
 914     case Bytecodes::_i2b:
 915       __ sign_extend(dest-&gt;as_register(), src-&gt;as_register(), 8);
 916       break;
 917     case Bytecodes::_i2s:
 918       __ sign_extend(dest-&gt;as_register(), src-&gt;as_register(), 16);
 919       break;
 920     case Bytecodes::_i2c:
 921       __ zero_extend(dest-&gt;as_register(), src-&gt;as_register(), 16);
 922       break;
 923     case Bytecodes::_f2d:
 924       __ convert_f2d(dest-&gt;as_double_reg(), src-&gt;as_float_reg());
 925       break;
 926     case Bytecodes::_d2f:
 927       __ convert_d2f(dest-&gt;as_float_reg(), src-&gt;as_double_reg());
 928       break;
 929     case Bytecodes::_i2f:
 930       __ fmsr(Stemp, src-&gt;as_register());
 931       __ fsitos(dest-&gt;as_float_reg(), Stemp);
 932       break;
 933     case Bytecodes::_i2d:
 934       __ fmsr(Stemp, src-&gt;as_register());
 935       __ fsitod(dest-&gt;as_double_reg(), Stemp);
 936       break;
 937     case Bytecodes::_f2i:
 938       __ ftosizs(Stemp, src-&gt;as_float_reg());
 939       __ fmrs(dest-&gt;as_register(), Stemp);
 940       break;
 941     case Bytecodes::_d2i:
 942       __ ftosizd(Stemp, src-&gt;as_double_reg());
 943       __ fmrs(dest-&gt;as_register(), Stemp);
 944       break;
 945     default:
 946       ShouldNotReachHere();
 947   }
 948 }
 949 
 950 
 951 void LIR_Assembler::emit_alloc_obj(LIR_OpAllocObj* op) {
 952   if (op-&gt;init_check()) {
 953     Register tmp = op-&gt;tmp1()-&gt;as_register();
 954     __ ldrb(tmp, Address(op-&gt;klass()-&gt;as_register(), InstanceKlass::init_state_offset()));
 955     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
 956     __ cmp(tmp, InstanceKlass::fully_initialized);
 957     __ b(*op-&gt;stub()-&gt;entry(), ne);
 958   }
 959   __ allocate_object(op-&gt;obj()-&gt;as_register(),
 960                      op-&gt;tmp1()-&gt;as_register(),
 961                      op-&gt;tmp2()-&gt;as_register(),
 962                      op-&gt;tmp3()-&gt;as_register(),
 963                      op-&gt;header_size(),
 964                      op-&gt;object_size(),
 965                      op-&gt;klass()-&gt;as_register(),
 966                      *op-&gt;stub()-&gt;entry());
 967   __ bind(*op-&gt;stub()-&gt;continuation());
 968 }
 969 
 970 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
 971   if (UseSlowPath ||
 972       (!UseFastNewObjectArray &amp;&amp; (op-&gt;type() == T_OBJECT || op-&gt;type() == T_ARRAY)) ||
 973       (!UseFastNewTypeArray   &amp;&amp; (op-&gt;type() != T_OBJECT &amp;&amp; op-&gt;type() != T_ARRAY))) {
 974     __ b(*op-&gt;stub()-&gt;entry());
 975   } else {
 976     __ allocate_array(op-&gt;obj()-&gt;as_register(),
 977                       op-&gt;len()-&gt;as_register(),
 978                       op-&gt;tmp1()-&gt;as_register(),
 979                       op-&gt;tmp2()-&gt;as_register(),
 980                       op-&gt;tmp3()-&gt;as_register(),
 981                       arrayOopDesc::header_size(op-&gt;type()),
 982                       type2aelembytes(op-&gt;type()),
 983                       op-&gt;klass()-&gt;as_register(),
 984                       *op-&gt;stub()-&gt;entry());
 985   }
 986   __ bind(*op-&gt;stub()-&gt;continuation());
 987 }
 988 
 989 void LIR_Assembler::type_profile_helper(Register mdo, int mdo_offset_bias,
 990                                         ciMethodData *md, ciProfileData *data,
 991                                         Register recv, Register tmp1, Label* update_done) {
 992   assert_different_registers(mdo, recv, tmp1);
 993   uint i;
 994   for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
 995     Label next_test;
 996     // See if the receiver is receiver[n].
 997     Address receiver_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i)) -
 998                           mdo_offset_bias);
 999     __ ldr(tmp1, receiver_addr);
1000     __ verify_klass_ptr(tmp1);
1001     __ cmp(recv, tmp1);
1002     __ b(next_test, ne);
1003     Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)) -
1004                       mdo_offset_bias);
1005     __ ldr(tmp1, data_addr);
1006     __ add(tmp1, tmp1, DataLayout::counter_increment);
1007     __ str(tmp1, data_addr);
1008     __ b(*update_done);
1009     __ bind(next_test);
1010   }
1011 
1012   // Didn&#39;t find receiver; find next empty slot and fill it in
1013   for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
1014     Label next_test;
1015     Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i)) -
1016                       mdo_offset_bias);
1017     __ ldr(tmp1, recv_addr);
1018     __ cbnz(tmp1, next_test);
1019     __ str(recv, recv_addr);
1020     __ mov(tmp1, DataLayout::counter_increment);
1021     __ str(tmp1, Address(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)) -
1022                          mdo_offset_bias));
1023     __ b(*update_done);
1024     __ bind(next_test);
1025   }
1026 }
1027 
1028 void LIR_Assembler::setup_md_access(ciMethod* method, int bci,
1029                                     ciMethodData*&amp; md, ciProfileData*&amp; data, int&amp; mdo_offset_bias) {
1030   md = method-&gt;method_data_or_null();
1031   assert(md != NULL, &quot;Sanity&quot;);
1032   data = md-&gt;bci_to_data(bci);
1033   assert(data != NULL,       &quot;need data for checkcast&quot;);
1034   assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
1035   if (md-&gt;byte_offset_of_slot(data, DataLayout::header_offset()) + data-&gt;size_in_bytes() &gt;= 4096) {
1036     // The offset is large so bias the mdo by the base of the slot so
1037     // that the ldr can use an immediate offset to reference the slots of the data
1038     mdo_offset_bias = md-&gt;byte_offset_of_slot(data, DataLayout::header_offset());
1039   }
1040 }
1041 
1042 // On 32-bit ARM, code before this helper should test obj for null (ZF should be set if obj is null).
1043 void LIR_Assembler::typecheck_profile_helper1(ciMethod* method, int bci,
1044                                               ciMethodData*&amp; md, ciProfileData*&amp; data, int&amp; mdo_offset_bias,
1045                                               Register obj, Register mdo, Register data_val, Label* obj_is_null) {
1046   assert(method != NULL, &quot;Should have method&quot;);
1047   assert_different_registers(obj, mdo, data_val);
1048   setup_md_access(method, bci, md, data, mdo_offset_bias);
1049   Label not_null;
1050   __ b(not_null, ne);
1051   __ mov_metadata(mdo, md-&gt;constant_encoding());
1052   if (mdo_offset_bias &gt; 0) {
1053     __ mov_slow(data_val, mdo_offset_bias);
1054     __ add(mdo, mdo, data_val);
1055   }
1056   Address flags_addr(mdo, md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()) - mdo_offset_bias);
1057   __ ldrb(data_val, flags_addr);
1058   __ orr(data_val, data_val, (uint)BitData::null_seen_byte_constant());
1059   __ strb(data_val, flags_addr);
1060   __ b(*obj_is_null);
1061   __ bind(not_null);
1062 }
1063 
1064 void LIR_Assembler::typecheck_profile_helper2(ciMethodData* md, ciProfileData* data, int mdo_offset_bias,
1065                                               Register mdo, Register recv, Register value, Register tmp1,
1066                                               Label* profile_cast_success, Label* profile_cast_failure,
1067                                               Label* success, Label* failure) {
1068   assert_different_registers(mdo, value, tmp1);
1069   __ bind(*profile_cast_success);
1070   __ mov_metadata(mdo, md-&gt;constant_encoding());
1071   if (mdo_offset_bias &gt; 0) {
1072     __ mov_slow(tmp1, mdo_offset_bias);
1073     __ add(mdo, mdo, tmp1);
1074   }
1075   __ load_klass(recv, value);
1076   type_profile_helper(mdo, mdo_offset_bias, md, data, recv, tmp1, success);
1077   __ b(*success);
1078   // Cast failure case
1079   __ bind(*profile_cast_failure);
1080   __ mov_metadata(mdo, md-&gt;constant_encoding());
1081   if (mdo_offset_bias &gt; 0) {
1082     __ mov_slow(tmp1, mdo_offset_bias);
1083     __ add(mdo, mdo, tmp1);
1084   }
1085   Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()) - mdo_offset_bias);
1086   __ ldr(tmp1, data_addr);
1087   __ sub(tmp1, tmp1, DataLayout::counter_increment);
1088   __ str(tmp1, data_addr);
1089   __ b(*failure);
1090 }
1091 
1092 // Sets `res` to true, if `cond` holds.
1093 static void set_instanceof_result(MacroAssembler* _masm, Register res, AsmCondition cond) {
1094   __ mov(res, 1, cond);
1095 }
1096 
1097 
1098 void LIR_Assembler::emit_opTypeCheck(LIR_OpTypeCheck* op) {
1099   // TODO: ARM - can be more effective with one more register
1100   switch (op-&gt;code()) {
1101     case lir_store_check: {
1102       CodeStub* stub = op-&gt;stub();
1103       Register value = op-&gt;object()-&gt;as_register();
1104       Register array = op-&gt;array()-&gt;as_register();
1105       Register klass_RInfo = op-&gt;tmp1()-&gt;as_register();
1106       Register k_RInfo = op-&gt;tmp2()-&gt;as_register();
1107       assert_different_registers(klass_RInfo, k_RInfo, Rtemp);
1108       if (op-&gt;should_profile()) {
1109         assert_different_registers(value, klass_RInfo, k_RInfo, Rtemp);
1110       }
1111 
1112       // check if it needs to be profiled
1113       ciMethodData* md;
1114       ciProfileData* data;
1115       int mdo_offset_bias = 0;
1116       Label profile_cast_success, profile_cast_failure, done;
1117       Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : &amp;done;
1118       Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : stub-&gt;entry();
1119 
1120       if (op-&gt;should_profile()) {
1121         __ cmp(value, 0);
1122         typecheck_profile_helper1(op-&gt;profiled_method(), op-&gt;profiled_bci(), md, data, mdo_offset_bias, value, k_RInfo, Rtemp, &amp;done);
1123       } else {
1124         __ cbz(value, done);
1125       }
1126       assert_different_registers(k_RInfo, value);
1127       add_debug_info_for_null_check_here(op-&gt;info_for_exception());
1128       __ load_klass(k_RInfo, array);
1129       __ load_klass(klass_RInfo, value);
1130       __ ldr(k_RInfo, Address(k_RInfo, ObjArrayKlass::element_klass_offset()));
1131       __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1132       // check for immediate positive hit
1133       __ ldr(Rtemp, Address(klass_RInfo, Rtemp));
1134       __ cmp(klass_RInfo, k_RInfo);
1135       __ cond_cmp(Rtemp, k_RInfo, ne);
1136       __ b(*success_target, eq);
1137       // check for immediate negative hit
1138       __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1139       __ cmp(Rtemp, in_bytes(Klass::secondary_super_cache_offset()));
1140       __ b(*failure_target, ne);
1141       // slow case
1142       assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1143       __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1144       __ cbz(R0, *failure_target);
1145       if (op-&gt;should_profile()) {
1146         Register mdo  = klass_RInfo, recv = k_RInfo, tmp1 = Rtemp;
1147         if (mdo == value) {
1148           mdo = k_RInfo;
1149           recv = klass_RInfo;
1150         }
1151         typecheck_profile_helper2(md, data, mdo_offset_bias, mdo, recv, value, tmp1,
1152                                   &amp;profile_cast_success, &amp;profile_cast_failure,
1153                                   &amp;done, stub-&gt;entry());
1154       }
1155       __ bind(done);
1156       break;
1157     }
1158 
1159     case lir_checkcast: {
1160       CodeStub* stub = op-&gt;stub();
1161       Register obj = op-&gt;object()-&gt;as_register();
1162       Register res = op-&gt;result_opr()-&gt;as_register();
1163       Register klass_RInfo = op-&gt;tmp1()-&gt;as_register();
1164       Register k_RInfo = op-&gt;tmp2()-&gt;as_register();
1165       ciKlass* k = op-&gt;klass();
1166       assert_different_registers(res, k_RInfo, klass_RInfo, Rtemp);
1167 
1168       if (stub-&gt;is_simple_exception_stub()) {
1169       // TODO: ARM - Late binding is used to prevent confusion of register allocator
1170       assert(stub-&gt;is_exception_throw_stub(), &quot;must be&quot;);
1171       ((SimpleExceptionStub*)stub)-&gt;set_obj(op-&gt;result_opr());
1172       }
1173       ciMethodData* md;
1174       ciProfileData* data;
1175       int mdo_offset_bias = 0;
1176 
1177       Label done;
1178 
1179       Label profile_cast_failure, profile_cast_success;
1180       Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : op-&gt;stub()-&gt;entry();
1181       Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : &amp;done;
1182 
1183 
1184       __ movs(res, obj);
1185       if (op-&gt;should_profile()) {
1186         typecheck_profile_helper1(op-&gt;profiled_method(), op-&gt;profiled_bci(), md, data, mdo_offset_bias, res, klass_RInfo, Rtemp, &amp;done);
1187       } else {
1188         __ b(done, eq);
1189       }
1190       if (k-&gt;is_loaded()) {
1191         __ mov_metadata(k_RInfo, k-&gt;constant_encoding());
1192       } else if (k_RInfo != obj) {
1193         klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
1194         __ movs(res, obj);
1195       } else {
1196         // Patching doesn&#39;t update &quot;res&quot; register after GC, so do patching first
1197         klass2reg_with_patching(Rtemp, op-&gt;info_for_patch());
1198         __ movs(res, obj);
1199         __ mov(k_RInfo, Rtemp);
1200       }
1201       __ load_klass(klass_RInfo, res, ne);
1202 
1203       if (op-&gt;fast_check()) {
1204         __ cmp(klass_RInfo, k_RInfo, ne);
1205         __ b(*failure_target, ne);
1206       } else if (k-&gt;is_loaded()) {
1207         __ b(*success_target, eq);
1208         __ ldr(Rtemp, Address(klass_RInfo, k-&gt;super_check_offset()));
1209         if (in_bytes(Klass::secondary_super_cache_offset()) != (int) k-&gt;super_check_offset()) {
1210           __ cmp(Rtemp, k_RInfo);
1211           __ b(*failure_target, ne);
1212         } else {
1213           __ cmp(klass_RInfo, k_RInfo);
1214           __ cmp(Rtemp, k_RInfo, ne);
1215           __ b(*success_target, eq);
1216           assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1217           __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1218           __ cbz(R0, *failure_target);
1219         }
1220       } else {
1221         __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1222         __ b(*success_target, eq);
1223         // check for immediate positive hit
1224         __ ldr(Rtemp, Address(klass_RInfo, Rtemp));
1225         __ cmp(klass_RInfo, k_RInfo);
1226         __ cmp(Rtemp, k_RInfo, ne);
1227         __ b(*success_target, eq);
1228         // check for immediate negative hit
1229         __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1230         __ cmp(Rtemp, in_bytes(Klass::secondary_super_cache_offset()));
1231         __ b(*failure_target, ne);
1232         // slow case
1233         assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1234         __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1235         __ cbz(R0, *failure_target);
1236       }
1237 
1238       if (op-&gt;should_profile()) {
1239         Register mdo  = klass_RInfo, recv = k_RInfo, tmp1 = Rtemp;
1240         typecheck_profile_helper2(md, data, mdo_offset_bias, mdo, recv, res, tmp1,
1241                                   &amp;profile_cast_success, &amp;profile_cast_failure,
1242                                   &amp;done, stub-&gt;entry());
1243       }
1244       __ bind(done);
1245       break;
1246     }
1247 
1248     case lir_instanceof: {
1249       Register obj = op-&gt;object()-&gt;as_register();
1250       Register res = op-&gt;result_opr()-&gt;as_register();
1251       Register klass_RInfo = op-&gt;tmp1()-&gt;as_register();
1252       Register k_RInfo = op-&gt;tmp2()-&gt;as_register();
1253       ciKlass* k = op-&gt;klass();
1254       assert_different_registers(res, klass_RInfo, k_RInfo, Rtemp);
1255 
1256       ciMethodData* md;
1257       ciProfileData* data;
1258       int mdo_offset_bias = 0;
1259 
1260       Label done;
1261 
1262       Label profile_cast_failure, profile_cast_success;
1263       Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : &amp;done;
1264       Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : &amp;done;
1265 
1266       __ movs(res, obj);
1267 
1268       if (op-&gt;should_profile()) {
1269         typecheck_profile_helper1(op-&gt;profiled_method(), op-&gt;profiled_bci(), md, data, mdo_offset_bias, res, klass_RInfo, Rtemp, &amp;done);
1270       } else {
1271         __ b(done, eq);
1272       }
1273 
1274       if (k-&gt;is_loaded()) {
1275         __ mov_metadata(k_RInfo, k-&gt;constant_encoding());
1276       } else {
1277         op-&gt;info_for_patch()-&gt;add_register_oop(FrameMap::as_oop_opr(res));
1278         klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
1279       }
1280       __ load_klass(klass_RInfo, res);
1281 
1282       if (!op-&gt;should_profile()) {
1283         __ mov(res, 0);
1284       }
1285 
1286       if (op-&gt;fast_check()) {
1287         __ cmp(klass_RInfo, k_RInfo);
1288         if (!op-&gt;should_profile()) {
1289           set_instanceof_result(_masm, res, eq);
1290         } else {
1291           __ b(profile_cast_failure, ne);
1292         }
1293       } else if (k-&gt;is_loaded()) {
1294         __ ldr(Rtemp, Address(klass_RInfo, k-&gt;super_check_offset()));
1295         if (in_bytes(Klass::secondary_super_cache_offset()) != (int) k-&gt;super_check_offset()) {
1296           __ cmp(Rtemp, k_RInfo);
1297           if (!op-&gt;should_profile()) {
1298             set_instanceof_result(_masm, res, eq);
1299           } else {
1300             __ b(profile_cast_failure, ne);
1301           }
1302         } else {
1303           __ cmp(klass_RInfo, k_RInfo);
1304           __ cond_cmp(Rtemp, k_RInfo, ne);
1305           if (!op-&gt;should_profile()) {
1306             set_instanceof_result(_masm, res, eq);
1307           }
1308           __ b(*success_target, eq);
1309           assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1310           __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1311           if (!op-&gt;should_profile()) {
1312             move_regs(R0, res);
1313           } else {
1314             __ cbz(R0, *failure_target);
1315           }
1316         }
1317       } else {
1318         __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1319         // check for immediate positive hit
1320         __ cmp(klass_RInfo, k_RInfo);
1321         if (!op-&gt;should_profile()) {
1322           __ ldr(res, Address(klass_RInfo, Rtemp), ne);
1323           __ cond_cmp(res, k_RInfo, ne);
1324           set_instanceof_result(_masm, res, eq);
1325         } else {
1326           __ ldr(Rtemp, Address(klass_RInfo, Rtemp), ne);
1327           __ cond_cmp(Rtemp, k_RInfo, ne);
1328         }
1329         __ b(*success_target, eq);
1330         // check for immediate negative hit
1331         if (op-&gt;should_profile()) {
1332           __ ldr_u32(Rtemp, Address(k_RInfo, Klass::super_check_offset_offset()));
1333         }
1334         __ cmp(Rtemp, in_bytes(Klass::secondary_super_cache_offset()));
1335         if (!op-&gt;should_profile()) {
1336           __ mov(res, 0, ne);
1337         }
1338         __ b(*failure_target, ne);
1339         // slow case
1340         assert(klass_RInfo == R0 &amp;&amp; k_RInfo == R1, &quot;runtime call setup&quot;);
1341         __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type);
1342         if (!op-&gt;should_profile()) {
1343           move_regs(R0, res);
1344         }
1345         if (op-&gt;should_profile()) {
1346           __ cbz(R0, *failure_target);
1347         }
1348       }
1349 
1350       if (op-&gt;should_profile()) {
1351         Label done_ok, done_failure;
1352         Register mdo  = klass_RInfo, recv = k_RInfo, tmp1 = Rtemp;
1353         typecheck_profile_helper2(md, data, mdo_offset_bias, mdo, recv, res, tmp1,
1354                                   &amp;profile_cast_success, &amp;profile_cast_failure,
1355                                   &amp;done_ok, &amp;done_failure);
1356         __ bind(done_failure);
1357         __ mov(res, 0);
1358         __ b(done);
1359         __ bind(done_ok);
1360         __ mov(res, 1);
1361       }
1362       __ bind(done);
1363       break;
1364     }
1365     default:
1366       ShouldNotReachHere();
1367   }
1368 }
1369 
1370 
1371 void LIR_Assembler::emit_compare_and_swap(LIR_OpCompareAndSwap* op) {
1372   //   if (*addr == cmpval) {
1373   //     *addr = newval;
1374   //     dest = 1;
1375   //   } else {
1376   //     dest = 0;
1377   //   }
1378   // FIXME: membar_release
1379   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);
1380   Register addr = op-&gt;addr()-&gt;is_register() ?
1381     op-&gt;addr()-&gt;as_pointer_register() :
1382     op-&gt;addr()-&gt;as_address_ptr()-&gt;base()-&gt;as_pointer_register();
1383   assert(op-&gt;addr()-&gt;is_register() || op-&gt;addr()-&gt;as_address_ptr()-&gt;disp() == 0, &quot;unexpected disp&quot;);
1384   assert(op-&gt;addr()-&gt;is_register() || op-&gt;addr()-&gt;as_address_ptr()-&gt;index() == LIR_OprDesc::illegalOpr(), &quot;unexpected index&quot;);
1385   if (op-&gt;code() == lir_cas_int || op-&gt;code() == lir_cas_obj) {
1386     Register cmpval = op-&gt;cmp_value()-&gt;as_register();
1387     Register newval = op-&gt;new_value()-&gt;as_register();
1388     Register dest = op-&gt;result_opr()-&gt;as_register();
1389     assert_different_registers(dest, addr, cmpval, newval, Rtemp);
1390 
1391     __ atomic_cas_bool(cmpval, newval, addr, 0, Rtemp); // Rtemp free by default at C1 LIR layer
1392     __ mov(dest, 1, eq);
1393     __ mov(dest, 0, ne);
1394   } else if (op-&gt;code() == lir_cas_long) {
1395     assert(VM_Version::supports_cx8(), &quot;wrong machine&quot;);
1396     Register cmp_value_lo = op-&gt;cmp_value()-&gt;as_register_lo();
1397     Register cmp_value_hi = op-&gt;cmp_value()-&gt;as_register_hi();
1398     Register new_value_lo = op-&gt;new_value()-&gt;as_register_lo();
1399     Register new_value_hi = op-&gt;new_value()-&gt;as_register_hi();
1400     Register dest = op-&gt;result_opr()-&gt;as_register();
1401     Register tmp_lo = op-&gt;tmp1()-&gt;as_register_lo();
1402     Register tmp_hi = op-&gt;tmp1()-&gt;as_register_hi();
1403 
1404     assert_different_registers(tmp_lo, tmp_hi, cmp_value_lo, cmp_value_hi, dest, new_value_lo, new_value_hi, addr);
1405     assert(tmp_hi-&gt;encoding() == tmp_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
1406     assert(new_value_hi-&gt;encoding() == new_value_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
1407     assert((tmp_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
1408     assert((new_value_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
1409     __ atomic_cas64(tmp_lo, tmp_hi, dest, cmp_value_lo, cmp_value_hi,
1410                     new_value_lo, new_value_hi, addr, 0);
1411   } else {
1412     Unimplemented();
1413   }
1414   // FIXME: is full membar really needed instead of just membar_acquire?
1415   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreLoad | MacroAssembler::StoreStore), Rtemp);
1416 }
1417 
1418 
1419 void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {
1420   AsmCondition acond = al;
1421   AsmCondition ncond = nv;
1422   if (opr1 != opr2) {
1423     switch (condition) {
1424       case lir_cond_equal:        acond = eq; ncond = ne; break;
1425       case lir_cond_notEqual:     acond = ne; ncond = eq; break;
1426       case lir_cond_less:         acond = lt; ncond = ge; break;
1427       case lir_cond_lessEqual:    acond = le; ncond = gt; break;
1428       case lir_cond_greaterEqual: acond = ge; ncond = lt; break;
1429       case lir_cond_greater:      acond = gt; ncond = le; break;
1430       case lir_cond_aboveEqual:   acond = hs; ncond = lo; break;
1431       case lir_cond_belowEqual:   acond = ls; ncond = hi; break;
1432       default: ShouldNotReachHere();
1433     }
1434   }
1435 
1436   for (;;) {                         // two iterations only
1437     if (opr1 == result) {
1438       // do nothing
1439     } else if (opr1-&gt;is_single_cpu()) {
1440       __ mov(result-&gt;as_register(), opr1-&gt;as_register(), acond);
1441     } else if (opr1-&gt;is_double_cpu()) {
1442       __ long_move(result-&gt;as_register_lo(), result-&gt;as_register_hi(),
1443                    opr1-&gt;as_register_lo(), opr1-&gt;as_register_hi(), acond);
1444     } else if (opr1-&gt;is_single_stack()) {
1445       __ ldr(result-&gt;as_register(), frame_map()-&gt;address_for_slot(opr1-&gt;single_stack_ix()), acond);
1446     } else if (opr1-&gt;is_double_stack()) {
1447       __ ldr(result-&gt;as_register_lo(),
1448              frame_map()-&gt;address_for_slot(opr1-&gt;double_stack_ix(), lo_word_offset_in_bytes), acond);
1449       __ ldr(result-&gt;as_register_hi(),
1450              frame_map()-&gt;address_for_slot(opr1-&gt;double_stack_ix(), hi_word_offset_in_bytes), acond);
1451     } else if (opr1-&gt;is_illegal()) {
1452       // do nothing: this part of the cmove has been optimized away in the peephole optimizer
1453     } else {
1454       assert(opr1-&gt;is_constant(), &quot;must be&quot;);
1455       LIR_Const* c = opr1-&gt;as_constant_ptr();
1456 
1457       switch (c-&gt;type()) {
1458         case T_INT:
1459           __ mov_slow(result-&gt;as_register(), c-&gt;as_jint(), acond);
1460           break;
1461         case T_LONG:
1462           __ mov_slow(result-&gt;as_register_lo(), c-&gt;as_jint_lo(), acond);
1463           __ mov_slow(result-&gt;as_register_hi(), c-&gt;as_jint_hi(), acond);
1464           break;
1465         case T_OBJECT:
1466           __ mov_oop(result-&gt;as_register(), c-&gt;as_jobject(), 0, acond);
1467           break;
1468         case T_FLOAT:
1469 #ifdef __SOFTFP__
1470           // not generated now.
1471           __ mov_slow(result-&gt;as_register(), c-&gt;as_jint(), acond);
1472 #else
1473           __ mov_float(result-&gt;as_float_reg(), c-&gt;as_jfloat(), acond);
1474 #endif // __SOFTFP__
1475           break;
1476         case T_DOUBLE:
1477 #ifdef __SOFTFP__
1478           // not generated now.
1479           __ mov_slow(result-&gt;as_register_lo(), c-&gt;as_jint_lo(), acond);
1480           __ mov_slow(result-&gt;as_register_hi(), c-&gt;as_jint_hi(), acond);
1481 #else
1482           __ mov_double(result-&gt;as_double_reg(), c-&gt;as_jdouble(), acond);
1483 #endif // __SOFTFP__
1484           break;
1485         default:
1486           ShouldNotReachHere();
1487       }
1488     }
1489 
1490     // Negate the condition and repeat the algorithm with the second operand
1491     if (opr1 == opr2) { break; }
1492     opr1 = opr2;
1493     acond = ncond;
1494   }
1495 }
1496 
1497 #ifdef ASSERT
1498 static int reg_size(LIR_Opr op) {
1499   switch (op-&gt;type()) {
1500   case T_FLOAT:
1501   case T_INT:      return BytesPerInt;
1502   case T_LONG:
1503   case T_DOUBLE:   return BytesPerLong;
1504   case T_OBJECT:
1505   case T_ARRAY:
1506   case T_METADATA: return BytesPerWord;
1507   case T_ADDRESS:
1508   case T_ILLEGAL:  // fall through
1509   default: ShouldNotReachHere(); return -1;
1510   }
1511 }
1512 #endif
1513 
1514 void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info, bool pop_fpu_stack) {
1515   assert(info == NULL, &quot;unused on this code path&quot;);
1516   assert(dest-&gt;is_register(), &quot;wrong items state&quot;);
1517 
1518   if (right-&gt;is_address()) {
1519     // special case for adding shifted/extended register
1520     const Register res = dest-&gt;as_pointer_register();
1521     const Register lreg = left-&gt;as_pointer_register();
1522     const LIR_Address* addr = right-&gt;as_address_ptr();
1523 
1524     assert(addr-&gt;base()-&gt;as_pointer_register() == lreg &amp;&amp; addr-&gt;index()-&gt;is_register() &amp;&amp; addr-&gt;disp() == 0, &quot;must be&quot;);
1525 
1526     int scale = addr-&gt;scale();
1527     AsmShift shift = lsl;
1528 
1529 
1530     assert(reg_size(addr-&gt;base()) == reg_size(addr-&gt;index()), &quot;should be&quot;);
1531     assert(reg_size(addr-&gt;base()) == reg_size(dest), &quot;should be&quot;);
1532     assert(reg_size(dest) == wordSize, &quot;should be&quot;);
1533 
1534     AsmOperand operand(addr-&gt;index()-&gt;as_pointer_register(), shift, scale);
1535     switch (code) {
1536       case lir_add: __ add(res, lreg, operand); break;
1537       case lir_sub: __ sub(res, lreg, operand); break;
1538       default: ShouldNotReachHere();
1539     }
1540 
1541   } else if (left-&gt;is_address()) {
1542     assert(code == lir_sub &amp;&amp; right-&gt;is_single_cpu(), &quot;special case used by strength_reduce_multiply()&quot;);
1543     const LIR_Address* addr = left-&gt;as_address_ptr();
1544     const Register res = dest-&gt;as_register();
1545     const Register rreg = right-&gt;as_register();
1546     assert(addr-&gt;base()-&gt;as_register() == rreg &amp;&amp; addr-&gt;index()-&gt;is_register() &amp;&amp; addr-&gt;disp() == 0, &quot;must be&quot;);
1547     __ rsb(res, rreg, AsmOperand(addr-&gt;index()-&gt;as_register(), lsl, addr-&gt;scale()));
1548 
1549   } else if (dest-&gt;is_single_cpu()) {
1550     assert(left-&gt;is_single_cpu(), &quot;unexpected left operand&quot;);
1551 
1552     const Register res = dest-&gt;as_register();
1553     const Register lreg = left-&gt;as_register();
1554 
1555     if (right-&gt;is_single_cpu()) {
1556       const Register rreg = right-&gt;as_register();
1557       switch (code) {
1558         case lir_add: __ add_32(res, lreg, rreg); break;
1559         case lir_sub: __ sub_32(res, lreg, rreg); break;
1560         case lir_mul: __ mul_32(res, lreg, rreg); break;
1561         default: ShouldNotReachHere();
1562       }
1563     } else {
1564       assert(right-&gt;is_constant(), &quot;must be&quot;);
1565       const jint c = right-&gt;as_constant_ptr()-&gt;as_jint();
1566       if (!Assembler::is_arith_imm_in_range(c)) {
1567         BAILOUT(&quot;illegal arithmetic operand&quot;);
1568       }
1569       switch (code) {
1570         case lir_add: __ add_32(res, lreg, c); break;
1571         case lir_sub: __ sub_32(res, lreg, c); break;
1572         default: ShouldNotReachHere();
1573       }
1574     }
1575 
1576   } else if (dest-&gt;is_double_cpu()) {
1577     Register res_lo = dest-&gt;as_register_lo();
1578     Register res_hi = dest-&gt;as_register_hi();
1579     Register lreg_lo = left-&gt;as_register_lo();
1580     Register lreg_hi = left-&gt;as_register_hi();
1581     if (right-&gt;is_double_cpu()) {
1582       Register rreg_lo = right-&gt;as_register_lo();
1583       Register rreg_hi = right-&gt;as_register_hi();
1584       if (res_lo == lreg_hi || res_lo == rreg_hi) {
1585         res_lo = Rtemp;
1586       }
1587       switch (code) {
1588         case lir_add:
1589           __ adds(res_lo, lreg_lo, rreg_lo);
1590           __ adc(res_hi, lreg_hi, rreg_hi);
1591           break;
1592         case lir_sub:
1593           __ subs(res_lo, lreg_lo, rreg_lo);
1594           __ sbc(res_hi, lreg_hi, rreg_hi);
1595           break;
1596         default:
1597           ShouldNotReachHere();
1598       }
1599     } else {
1600       assert(right-&gt;is_constant(), &quot;must be&quot;);
1601       assert((right-&gt;as_constant_ptr()-&gt;as_jlong() &gt;&gt; 32) == 0, &quot;out of range&quot;);
1602       const jint c = (jint) right-&gt;as_constant_ptr()-&gt;as_jlong();
1603       if (res_lo == lreg_hi) {
1604         res_lo = Rtemp;
1605       }
1606       switch (code) {
1607         case lir_add:
1608           __ adds(res_lo, lreg_lo, c);
1609           __ adc(res_hi, lreg_hi, 0);
1610           break;
1611         case lir_sub:
1612           __ subs(res_lo, lreg_lo, c);
1613           __ sbc(res_hi, lreg_hi, 0);
1614           break;
1615         default:
1616           ShouldNotReachHere();
1617       }
1618     }
1619     move_regs(res_lo, dest-&gt;as_register_lo());
1620 
1621   } else if (dest-&gt;is_single_fpu()) {
1622     assert(left-&gt;is_single_fpu(), &quot;must be&quot;);
1623     assert(right-&gt;is_single_fpu(), &quot;must be&quot;);
1624     const FloatRegister res = dest-&gt;as_float_reg();
1625     const FloatRegister lreg = left-&gt;as_float_reg();
1626     const FloatRegister rreg = right-&gt;as_float_reg();
1627     switch (code) {
1628       case lir_add: __ add_float(res, lreg, rreg); break;
1629       case lir_sub: __ sub_float(res, lreg, rreg); break;
1630       case lir_mul_strictfp: // fall through
1631       case lir_mul: __ mul_float(res, lreg, rreg); break;
1632       case lir_div_strictfp: // fall through
1633       case lir_div: __ div_float(res, lreg, rreg); break;
1634       default: ShouldNotReachHere();
1635     }
1636   } else if (dest-&gt;is_double_fpu()) {
1637     assert(left-&gt;is_double_fpu(), &quot;must be&quot;);
1638     assert(right-&gt;is_double_fpu(), &quot;must be&quot;);
1639     const FloatRegister res = dest-&gt;as_double_reg();
1640     const FloatRegister lreg = left-&gt;as_double_reg();
1641     const FloatRegister rreg = right-&gt;as_double_reg();
1642     switch (code) {
1643       case lir_add: __ add_double(res, lreg, rreg); break;
1644       case lir_sub: __ sub_double(res, lreg, rreg); break;
1645       case lir_mul_strictfp: // fall through
1646       case lir_mul: __ mul_double(res, lreg, rreg); break;
1647       case lir_div_strictfp: // fall through
1648       case lir_div: __ div_double(res, lreg, rreg); break;
1649       default: ShouldNotReachHere();
1650     }
1651   } else {
1652     ShouldNotReachHere();
1653   }
1654 }
1655 
1656 
1657 void LIR_Assembler::intrinsic_op(LIR_Code code, LIR_Opr value, LIR_Opr unused, LIR_Opr dest, LIR_Op* op) {
1658   switch (code) {
1659     case lir_abs:
1660       __ abs_double(dest-&gt;as_double_reg(), value-&gt;as_double_reg());
1661       break;
1662     case lir_sqrt:
1663       __ sqrt_double(dest-&gt;as_double_reg(), value-&gt;as_double_reg());
1664       break;
1665     default:
1666       ShouldNotReachHere();
1667   }
1668 }
1669 
1670 
1671 void LIR_Assembler::logic_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest) {
1672   assert(dest-&gt;is_register(), &quot;wrong items state&quot;);
1673   assert(left-&gt;is_register(), &quot;wrong items state&quot;);
1674 
1675   if (dest-&gt;is_single_cpu()) {
1676 
1677     const Register res = dest-&gt;as_register();
1678     const Register lreg = left-&gt;as_register();
1679 
1680     if (right-&gt;is_single_cpu()) {
1681       const Register rreg = right-&gt;as_register();
1682       switch (code) {
1683         case lir_logic_and: __ and_32(res, lreg, rreg); break;
1684         case lir_logic_or:  __ orr_32(res, lreg, rreg); break;
1685         case lir_logic_xor: __ eor_32(res, lreg, rreg); break;
1686         default: ShouldNotReachHere();
1687       }
1688     } else {
1689       assert(right-&gt;is_constant(), &quot;must be&quot;);
1690       const uint c = (uint)right-&gt;as_constant_ptr()-&gt;as_jint();
1691       switch (code) {
1692         case lir_logic_and: __ and_32(res, lreg, c); break;
1693         case lir_logic_or:  __ orr_32(res, lreg, c); break;
1694         case lir_logic_xor: __ eor_32(res, lreg, c); break;
1695         default: ShouldNotReachHere();
1696       }
1697     }
1698   } else {
1699     assert(dest-&gt;is_double_cpu(), &quot;should be&quot;);
1700     Register res_lo = dest-&gt;as_register_lo();
1701 
1702     assert (dest-&gt;type() == T_LONG, &quot;unexpected result type&quot;);
1703     assert (left-&gt;type() == T_LONG, &quot;unexpected left type&quot;);
1704     assert (right-&gt;type() == T_LONG, &quot;unexpected right type&quot;);
1705 
1706     const Register res_hi = dest-&gt;as_register_hi();
1707     const Register lreg_lo = left-&gt;as_register_lo();
1708     const Register lreg_hi = left-&gt;as_register_hi();
1709 
1710     if (right-&gt;is_register()) {
1711       const Register rreg_lo = right-&gt;as_register_lo();
1712       const Register rreg_hi = right-&gt;as_register_hi();
1713       if (res_lo == lreg_hi || res_lo == rreg_hi) {
1714         res_lo = Rtemp; // Temp register helps to avoid overlap between result and input
1715       }
1716       switch (code) {
1717         case lir_logic_and:
1718           __ andr(res_lo, lreg_lo, rreg_lo);
1719           __ andr(res_hi, lreg_hi, rreg_hi);
1720           break;
1721         case lir_logic_or:
1722           __ orr(res_lo, lreg_lo, rreg_lo);
1723           __ orr(res_hi, lreg_hi, rreg_hi);
1724           break;
1725         case lir_logic_xor:
1726           __ eor(res_lo, lreg_lo, rreg_lo);
1727           __ eor(res_hi, lreg_hi, rreg_hi);
1728           break;
1729         default:
1730           ShouldNotReachHere();
1731       }
1732       move_regs(res_lo, dest-&gt;as_register_lo());
1733     } else {
1734       assert(right-&gt;is_constant(), &quot;must be&quot;);
1735       const jint c_lo = (jint) right-&gt;as_constant_ptr()-&gt;as_jlong();
1736       const jint c_hi = (jint) (right-&gt;as_constant_ptr()-&gt;as_jlong() &gt;&gt; 32);
1737       // Case for logic_or from do_ClassIDIntrinsic()
1738       if (c_hi == 0 &amp;&amp; AsmOperand::is_rotated_imm(c_lo)) {
1739         switch (code) {
1740           case lir_logic_and:
1741             __ andr(res_lo, lreg_lo, c_lo);
1742             __ mov(res_hi, 0);
1743             break;
1744           case lir_logic_or:
1745             __ orr(res_lo, lreg_lo, c_lo);
1746             break;
1747           case lir_logic_xor:
1748             __ eor(res_lo, lreg_lo, c_lo);
1749             break;
1750         default:
1751           ShouldNotReachHere();
1752         }
1753       } else if (code == lir_logic_and &amp;&amp;
1754                  c_hi == -1 &amp;&amp;
1755                  (AsmOperand::is_rotated_imm(c_lo) ||
1756                   AsmOperand::is_rotated_imm(~c_lo))) {
1757         // Another case which handles logic_and from do_ClassIDIntrinsic()
1758         if (AsmOperand::is_rotated_imm(c_lo)) {
1759           __ andr(res_lo, lreg_lo, c_lo);
1760         } else {
1761           __ bic(res_lo, lreg_lo, ~c_lo);
1762         }
1763         if (res_hi != lreg_hi) {
1764           __ mov(res_hi, lreg_hi);
1765         }
1766       } else {
1767         BAILOUT(&quot;64 bit constant cannot be inlined&quot;);
1768       }
1769     }
1770   }
1771 }
1772 
1773 
1774 
1775 void LIR_Assembler::comp_op(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Op2* op) {
1776   if (opr1-&gt;is_single_cpu()) {
1777     if (opr2-&gt;is_constant()) {
1778       switch (opr2-&gt;as_constant_ptr()-&gt;type()) {
1779         case T_INT: {
1780           const jint c = opr2-&gt;as_constant_ptr()-&gt;as_jint();
1781           if (Assembler::is_arith_imm_in_range(c)) {
1782             __ cmp_32(opr1-&gt;as_register(), c);
1783           } else if (Assembler::is_arith_imm_in_range(-c)) {
1784             __ cmn_32(opr1-&gt;as_register(), -c);
1785           } else {
1786             // This can happen when compiling lookupswitch
1787             __ mov_slow(Rtemp, c);
1788             __ cmp_32(opr1-&gt;as_register(), Rtemp);
1789           }
1790           break;
1791         }
1792         case T_OBJECT:
1793           assert(opr2-&gt;as_constant_ptr()-&gt;as_jobject() == NULL, &quot;cannot handle otherwise&quot;);
1794           __ cmp(opr1-&gt;as_register(), 0);
1795           break;
1796         case T_METADATA:
1797           assert(condition == lir_cond_equal || condition == lir_cond_notEqual, &quot;Only equality tests&quot;);
1798           assert(opr2-&gt;as_constant_ptr()-&gt;as_metadata() == NULL, &quot;cannot handle otherwise&quot;);
1799           __ cmp(opr1-&gt;as_register(), 0);
1800           break;
1801         default:
1802           ShouldNotReachHere();
1803       }
1804     } else if (opr2-&gt;is_single_cpu()) {
1805       if (opr1-&gt;type() == T_OBJECT || opr1-&gt;type() == T_ARRAY) {
1806         assert(opr2-&gt;type() == T_OBJECT || opr2-&gt;type() == T_ARRAY, &quot;incompatibe type&quot;);
1807         __ cmpoop(opr1-&gt;as_register(), opr2-&gt;as_register());
1808       } else if (opr1-&gt;type() == T_METADATA || opr1-&gt;type() == T_ADDRESS) {
1809         assert(opr2-&gt;type() == T_METADATA || opr2-&gt;type() == T_ADDRESS, &quot;incompatibe type&quot;);
1810         __ cmp(opr1-&gt;as_register(), opr2-&gt;as_register());
1811       } else {
1812         assert(opr2-&gt;type() != T_OBJECT &amp;&amp; opr2-&gt;type() != T_ARRAY &amp;&amp; opr2-&gt;type() != T_METADATA &amp;&amp; opr2-&gt;type() != T_ADDRESS, &quot;incompatibe type&quot;);
1813         __ cmp_32(opr1-&gt;as_register(), opr2-&gt;as_register());
1814       }
1815     } else {
1816       ShouldNotReachHere();
1817     }
1818   } else if (opr1-&gt;is_double_cpu()) {
1819     Register xlo = opr1-&gt;as_register_lo();
1820     Register xhi = opr1-&gt;as_register_hi();
1821     if (opr2-&gt;is_constant() &amp;&amp; opr2-&gt;as_jlong() == 0) {
1822       assert(condition == lir_cond_equal || condition == lir_cond_notEqual, &quot;cannot handle otherwise&quot;);
1823       __ orrs(Rtemp, xlo, xhi);
1824     } else if (opr2-&gt;is_register()) {
1825       Register ylo = opr2-&gt;as_register_lo();
1826       Register yhi = opr2-&gt;as_register_hi();
1827       if (condition == lir_cond_equal || condition == lir_cond_notEqual) {
1828         __ teq(xhi, yhi);
1829         __ teq(xlo, ylo, eq);
1830       } else {
1831         __ subs(xlo, xlo, ylo);
1832         __ sbcs(xhi, xhi, yhi);
1833       }
1834     } else {
1835       ShouldNotReachHere();
1836     }
1837   } else if (opr1-&gt;is_single_fpu()) {
1838     if (opr2-&gt;is_constant()) {
1839       assert(opr2-&gt;as_jfloat() == 0.0f, &quot;cannot handle otherwise&quot;);
1840       __ cmp_zero_float(opr1-&gt;as_float_reg());
1841     } else {
1842       __ cmp_float(opr1-&gt;as_float_reg(), opr2-&gt;as_float_reg());
1843     }
1844   } else if (opr1-&gt;is_double_fpu()) {
1845     if (opr2-&gt;is_constant()) {
1846       assert(opr2-&gt;as_jdouble() == 0.0, &quot;cannot handle otherwise&quot;);
1847       __ cmp_zero_double(opr1-&gt;as_double_reg());
1848     } else {
1849       __ cmp_double(opr1-&gt;as_double_reg(), opr2-&gt;as_double_reg());
1850     }
1851   } else {
1852     ShouldNotReachHere();
1853   }
1854 }
1855 
1856 void LIR_Assembler::comp_fl2i(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst, LIR_Op2* op) {
1857   const Register res = dst-&gt;as_register();
1858   if (code == lir_cmp_fd2i || code == lir_ucmp_fd2i) {
1859     comp_op(lir_cond_unknown, left, right, op);
1860     __ fmstat();
1861     if (code == lir_ucmp_fd2i) {  // unordered is less
1862       __ mvn(res, 0, lt);
1863       __ mov(res, 1, ge);
1864     } else {                      // unordered is greater
1865       __ mov(res, 1, cs);
1866       __ mvn(res, 0, cc);
1867     }
1868     __ mov(res, 0, eq);
1869 
1870   } else {
1871     assert(code == lir_cmp_l2i, &quot;must be&quot;);
1872 
1873     Label done;
1874     const Register xlo = left-&gt;as_register_lo();
1875     const Register xhi = left-&gt;as_register_hi();
1876     const Register ylo = right-&gt;as_register_lo();
1877     const Register yhi = right-&gt;as_register_hi();
1878     __ cmp(xhi, yhi);
1879     __ mov(res, 1, gt);
1880     __ mvn(res, 0, lt);
1881     __ b(done, ne);
1882     __ subs(res, xlo, ylo);
1883     __ mov(res, 1, hi);
1884     __ mvn(res, 0, lo);
1885     __ bind(done);
1886   }
1887 }
1888 
1889 
1890 void LIR_Assembler::align_call(LIR_Code code) {
1891   // Not needed
1892 }
1893 
1894 
1895 void LIR_Assembler::call(LIR_OpJavaCall *op, relocInfo::relocType rtype) {
1896   int ret_addr_offset = __ patchable_call(op-&gt;addr(), rtype);
1897   assert(ret_addr_offset == __ offset(), &quot;embedded return address not allowed&quot;);
1898   add_call_info_here(op-&gt;info());
1899 }
1900 
1901 
1902 void LIR_Assembler::ic_call(LIR_OpJavaCall *op) {
1903   bool near_range = __ cache_fully_reachable();
1904   address oop_address = pc();
1905 
1906   bool use_movw = VM_Version::supports_movw();
1907 
1908   // Ricklass may contain something that is not a metadata pointer so
1909   // mov_metadata can&#39;t be used
1910   InlinedAddress value((address)Universe::non_oop_word());
1911   InlinedAddress addr(op-&gt;addr());
1912   if (use_movw) {
1913     __ movw(Ricklass, ((unsigned int)Universe::non_oop_word()) &amp; 0xffff);
1914     __ movt(Ricklass, ((unsigned int)Universe::non_oop_word()) &gt;&gt; 16);
1915   } else {
1916     // No movw/movt, must be load a pc relative value but no
1917     // relocation so no metadata table to load from.
1918     // Use a b instruction rather than a bl, inline constant after the
1919     // branch, use a PC relative ldr to load the constant, arrange for
1920     // the call to return after the constant(s).
1921     __ ldr_literal(Ricklass, value);
1922   }
1923   __ relocate(virtual_call_Relocation::spec(oop_address));
1924   if (near_range &amp;&amp; use_movw) {
1925     __ bl(op-&gt;addr());
1926   } else {
1927     Label call_return;
1928     __ adr(LR, call_return);
1929     if (near_range) {
1930       __ b(op-&gt;addr());
1931     } else {
1932       __ indirect_jump(addr, Rtemp);
1933       __ bind_literal(addr);
1934     }
1935     if (!use_movw) {
1936       __ bind_literal(value);
1937     }
1938     __ bind(call_return);
1939   }
1940   add_call_info(code_offset(), op-&gt;info());
1941 }
1942 
1943 
1944 /* Currently, vtable-dispatch is only enabled for sparc platforms */
1945 void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {
1946   ShouldNotReachHere();
1947 }
1948 
1949 void LIR_Assembler::emit_static_call_stub() {
1950   address call_pc = __ pc();
1951   address stub = __ start_a_stub(call_stub_size());
1952   if (stub == NULL) {
1953     BAILOUT(&quot;static call stub overflow&quot;);
1954   }
1955 
1956   DEBUG_ONLY(int offset = code_offset();)
1957 
1958   InlinedMetadata metadata_literal(NULL);
1959   __ relocate(static_stub_Relocation::spec(call_pc));
1960   // If not a single instruction, NativeMovConstReg::next_instruction_address()
1961   // must jump over the whole following ldr_literal.
1962   // (See CompiledStaticCall::set_to_interpreted())
1963 #ifdef ASSERT
1964   address ldr_site = __ pc();
1965 #endif
1966   __ ldr_literal(Rmethod, metadata_literal);
1967   assert(nativeMovConstReg_at(ldr_site)-&gt;next_instruction_address() == __ pc(), &quot;Fix ldr_literal or its parsing&quot;);
1968   bool near_range = __ cache_fully_reachable();
1969   InlinedAddress dest((address)-1);
1970   if (near_range) {
1971     address branch_site = __ pc();
1972     __ b(branch_site); // b to self maps to special NativeJump -1 destination
1973   } else {
1974     __ indirect_jump(dest, Rtemp);
1975   }
1976   __ bind_literal(metadata_literal); // includes spec_for_immediate reloc
1977   if (!near_range) {
1978     __ bind_literal(dest); // special NativeJump -1 destination
1979   }
1980 
1981   assert(code_offset() - offset &lt;= call_stub_size(), &quot;overflow&quot;);
1982   __ end_a_stub();
1983 }
1984 
1985 void LIR_Assembler::throw_op(LIR_Opr exceptionPC, LIR_Opr exceptionOop, CodeEmitInfo* info) {
1986   assert(exceptionOop-&gt;as_register() == Rexception_obj, &quot;must match&quot;);
1987   assert(exceptionPC-&gt;as_register()  == Rexception_pc, &quot;must match&quot;);
1988   info-&gt;add_register_oop(exceptionOop);
1989 
1990   Runtime1::StubID handle_id = compilation()-&gt;has_fpu_code() ?
1991                                Runtime1::handle_exception_id :
1992                                Runtime1::handle_exception_nofpu_id;
1993   Label return_address;
1994   __ adr(Rexception_pc, return_address);
1995   __ call(Runtime1::entry_for(handle_id), relocInfo::runtime_call_type);
1996   __ bind(return_address);
1997   add_call_info_here(info);  // for exception handler
1998 }
1999 
2000 void LIR_Assembler::unwind_op(LIR_Opr exceptionOop) {
2001   assert(exceptionOop-&gt;as_register() == Rexception_obj, &quot;must match&quot;);
2002   __ b(_unwind_handler_entry);
2003 }
2004 
2005 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, LIR_Opr count, LIR_Opr dest, LIR_Opr tmp) {
2006   AsmShift shift = lsl;
2007   switch (code) {
2008     case lir_shl:  shift = lsl; break;
2009     case lir_shr:  shift = asr; break;
2010     case lir_ushr: shift = lsr; break;
2011     default: ShouldNotReachHere();
2012   }
2013 
2014   if (dest-&gt;is_single_cpu()) {
2015     __ andr(Rtemp, count-&gt;as_register(), 31);
2016     __ mov(dest-&gt;as_register(), AsmOperand(left-&gt;as_register(), shift, Rtemp));
2017   } else if (dest-&gt;is_double_cpu()) {
2018     Register dest_lo = dest-&gt;as_register_lo();
2019     Register dest_hi = dest-&gt;as_register_hi();
2020     Register src_lo  = left-&gt;as_register_lo();
2021     Register src_hi  = left-&gt;as_register_hi();
2022     Register Rcount  = count-&gt;as_register();
2023     // Resolve possible register conflicts
2024     if (shift == lsl &amp;&amp; dest_hi == src_lo) {
2025       dest_hi = Rtemp;
2026     } else if (shift != lsl &amp;&amp; dest_lo == src_hi) {
2027       dest_lo = Rtemp;
2028     } else if (dest_lo == src_lo &amp;&amp; dest_hi == src_hi) {
2029       dest_lo = Rtemp;
2030     } else if (dest_lo == Rcount || dest_hi == Rcount) {
2031       Rcount = Rtemp;
2032     }
2033     __ andr(Rcount, count-&gt;as_register(), 63);
2034     __ long_shift(dest_lo, dest_hi, src_lo, src_hi, shift, Rcount);
2035     move_regs(dest_lo, dest-&gt;as_register_lo());
2036     move_regs(dest_hi, dest-&gt;as_register_hi());
2037   } else {
2038     ShouldNotReachHere();
2039   }
2040 }
2041 
2042 
2043 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, jint count, LIR_Opr dest) {
2044   AsmShift shift = lsl;
2045   switch (code) {
2046     case lir_shl:  shift = lsl; break;
2047     case lir_shr:  shift = asr; break;
2048     case lir_ushr: shift = lsr; break;
2049     default: ShouldNotReachHere();
2050   }
2051 
2052   if (dest-&gt;is_single_cpu()) {
2053     count &amp;= 31;
2054     if (count != 0) {
2055       __ mov(dest-&gt;as_register(), AsmOperand(left-&gt;as_register(), shift, count));
2056     } else {
2057       move_regs(left-&gt;as_register(), dest-&gt;as_register());
2058     }
2059   } else if (dest-&gt;is_double_cpu()) {
2060     count &amp;= 63;
2061     if (count != 0) {
2062       Register dest_lo = dest-&gt;as_register_lo();
2063       Register dest_hi = dest-&gt;as_register_hi();
2064       Register src_lo  = left-&gt;as_register_lo();
2065       Register src_hi  = left-&gt;as_register_hi();
2066       // Resolve possible register conflicts
2067       if (shift == lsl &amp;&amp; dest_hi == src_lo) {
2068         dest_hi = Rtemp;
2069       } else if (shift != lsl &amp;&amp; dest_lo == src_hi) {
2070         dest_lo = Rtemp;
2071       }
2072       __ long_shift(dest_lo, dest_hi, src_lo, src_hi, shift, count);
2073       move_regs(dest_lo, dest-&gt;as_register_lo());
2074       move_regs(dest_hi, dest-&gt;as_register_hi());
2075     } else {
2076       __ long_move(dest-&gt;as_register_lo(), dest-&gt;as_register_hi(),
2077                    left-&gt;as_register_lo(), left-&gt;as_register_hi());
2078     }
2079   } else {
2080     ShouldNotReachHere();
2081   }
2082 }
2083 
2084 
2085 // Saves 4 given registers in reserved argument area.
2086 void LIR_Assembler::save_in_reserved_area(Register r1, Register r2, Register r3, Register r4) {
2087   verify_reserved_argument_area_size(4);
2088   __ stmia(SP, RegisterSet(r1) | RegisterSet(r2) | RegisterSet(r3) | RegisterSet(r4));
2089 }
2090 
2091 // Restores 4 given registers from reserved argument area.
2092 void LIR_Assembler::restore_from_reserved_area(Register r1, Register r2, Register r3, Register r4) {
2093   __ ldmia(SP, RegisterSet(r1) | RegisterSet(r2) | RegisterSet(r3) | RegisterSet(r4), no_writeback);
2094 }
2095 
2096 
2097 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
2098   ciArrayKlass* default_type = op-&gt;expected_type();
2099   Register src = op-&gt;src()-&gt;as_register();
2100   Register src_pos = op-&gt;src_pos()-&gt;as_register();
2101   Register dst = op-&gt;dst()-&gt;as_register();
2102   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
2103   Register length  = op-&gt;length()-&gt;as_register();
2104   Register tmp = op-&gt;tmp()-&gt;as_register();
2105   Register tmp2 = Rtemp;
2106 
2107   assert(src == R0 &amp;&amp; src_pos == R1 &amp;&amp; dst == R2 &amp;&amp; dst_pos == R3, &quot;code assumption&quot;);
2108 
2109   __ resolve(ACCESS_READ, src);
2110   __ resolve(ACCESS_WRITE, dst);
2111 
2112   CodeStub* stub = op-&gt;stub();
2113 
2114   int flags = op-&gt;flags();
2115   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
2116   if (basic_type == T_ARRAY) basic_type = T_OBJECT;
2117 
2118   // If we don&#39;t know anything or it&#39;s an object array, just go through the generic arraycopy
2119   if (default_type == NULL) {
2120 
2121     // save arguments, because they will be killed by a runtime call
2122     save_in_reserved_area(R0, R1, R2, R3);
2123 
2124     // pass length argument on SP[0]
2125     __ str(length, Address(SP, -2*wordSize, pre_indexed));  // 2 words for a proper stack alignment
2126 
2127     address copyfunc_addr = StubRoutines::generic_arraycopy();
2128     assert(copyfunc_addr != NULL, &quot;generic arraycopy stub required&quot;);
2129 #ifndef PRODUCT
2130     if (PrintC1Statistics) {
2131       __ inc_counter((address)&amp;Runtime1::_generic_arraycopystub_cnt, tmp, tmp2);
2132     }
2133 #endif // !PRODUCT
2134     // the stub is in the code cache so close enough
2135     __ call(copyfunc_addr, relocInfo::runtime_call_type);
2136 
2137     __ add(SP, SP, 2*wordSize);
2138 
2139     __ cbz_32(R0, *stub-&gt;continuation());
2140 
2141     __ mvn_32(tmp, R0);
2142     restore_from_reserved_area(R0, R1, R2, R3);  // load saved arguments in slow case only
2143     __ sub_32(length, length, tmp);
2144     __ add_32(src_pos, src_pos, tmp);
2145     __ add_32(dst_pos, dst_pos, tmp);
2146 
2147     __ b(*stub-&gt;entry());
2148 
2149     __ bind(*stub-&gt;continuation());
2150     return;
2151   }
2152 
2153   assert(default_type != NULL &amp;&amp; default_type-&gt;is_array_klass() &amp;&amp; default_type-&gt;is_loaded(),
2154          &quot;must be true at this point&quot;);
2155   int elem_size = type2aelembytes(basic_type);
2156   int shift = exact_log2(elem_size);
2157 
2158   // Check for NULL
2159   if (flags &amp; LIR_OpArrayCopy::src_null_check) {
2160     if (flags &amp; LIR_OpArrayCopy::dst_null_check) {
2161       __ cmp(src, 0);
2162       __ cond_cmp(dst, 0, ne);  // make one instruction shorter if both checks are needed
2163       __ b(*stub-&gt;entry(), eq);
2164     } else {
2165       __ cbz(src, *stub-&gt;entry());
2166     }
2167   } else if (flags &amp; LIR_OpArrayCopy::dst_null_check) {
2168     __ cbz(dst, *stub-&gt;entry());
2169   }
2170 
2171   // If the compiler was not able to prove that exact type of the source or the destination
2172   // of the arraycopy is an array type, check at runtime if the source or the destination is
2173   // an instance type.
2174   if (flags &amp; LIR_OpArrayCopy::type_check) {
2175     if (!(flags &amp; LIR_OpArrayCopy::LIR_OpArrayCopy::dst_objarray)) {
2176       __ load_klass(tmp, dst);
2177       __ ldr_u32(tmp2, Address(tmp, in_bytes(Klass::layout_helper_offset())));
2178       __ mov_slow(tmp, Klass::_lh_neutral_value);
2179       __ cmp_32(tmp2, tmp);
2180       __ b(*stub-&gt;entry(), ge);
2181     }
2182 
2183     if (!(flags &amp; LIR_OpArrayCopy::LIR_OpArrayCopy::src_objarray)) {
2184       __ load_klass(tmp, src);
2185       __ ldr_u32(tmp2, Address(tmp, in_bytes(Klass::layout_helper_offset())));
2186       __ mov_slow(tmp, Klass::_lh_neutral_value);
2187       __ cmp_32(tmp2, tmp);
2188       __ b(*stub-&gt;entry(), ge);
2189     }
2190   }
2191 
2192   // Check if negative
2193   const int all_positive_checks = LIR_OpArrayCopy::src_pos_positive_check |
2194                                   LIR_OpArrayCopy::dst_pos_positive_check |
2195                                   LIR_OpArrayCopy::length_positive_check;
2196   switch (flags &amp; all_positive_checks) {
2197     case LIR_OpArrayCopy::src_pos_positive_check:
2198       __ branch_if_negative_32(src_pos, *stub-&gt;entry());
2199       break;
2200     case LIR_OpArrayCopy::dst_pos_positive_check:
2201       __ branch_if_negative_32(dst_pos, *stub-&gt;entry());
2202       break;
2203     case LIR_OpArrayCopy::length_positive_check:
2204       __ branch_if_negative_32(length, *stub-&gt;entry());
2205       break;
2206     case LIR_OpArrayCopy::src_pos_positive_check | LIR_OpArrayCopy::dst_pos_positive_check:
2207       __ branch_if_any_negative_32(src_pos, dst_pos, tmp, *stub-&gt;entry());
2208       break;
2209     case LIR_OpArrayCopy::src_pos_positive_check | LIR_OpArrayCopy::length_positive_check:
2210       __ branch_if_any_negative_32(src_pos, length, tmp, *stub-&gt;entry());
2211       break;
2212     case LIR_OpArrayCopy::dst_pos_positive_check | LIR_OpArrayCopy::length_positive_check:
2213       __ branch_if_any_negative_32(dst_pos, length, tmp, *stub-&gt;entry());
2214       break;
2215     case all_positive_checks:
2216       __ branch_if_any_negative_32(src_pos, dst_pos, length, tmp, *stub-&gt;entry());
2217       break;
2218     default:
2219       assert((flags &amp; all_positive_checks) == 0, &quot;the last option&quot;);
2220   }
2221 
2222   // Range checks
2223   if (flags &amp; LIR_OpArrayCopy::src_range_check) {
2224     __ ldr_s32(tmp2, Address(src, arrayOopDesc::length_offset_in_bytes()));
2225     __ add_32(tmp, src_pos, length);
2226     __ cmp_32(tmp, tmp2);
2227     __ b(*stub-&gt;entry(), hi);
2228   }
2229   if (flags &amp; LIR_OpArrayCopy::dst_range_check) {
2230     __ ldr_s32(tmp2, Address(dst, arrayOopDesc::length_offset_in_bytes()));
2231     __ add_32(tmp, dst_pos, length);
2232     __ cmp_32(tmp, tmp2);
2233     __ b(*stub-&gt;entry(), hi);
2234   }
2235 
2236   // Check if src and dst are of the same type
2237   if (flags &amp; LIR_OpArrayCopy::type_check) {
2238     // We don&#39;t know the array types are compatible
2239     if (basic_type != T_OBJECT) {
2240       // Simple test for basic type arrays
2241       if (UseCompressedClassPointers) {
2242         // We don&#39;t need decode because we just need to compare
2243         __ ldr_u32(tmp, Address(src, oopDesc::klass_offset_in_bytes()));
2244         __ ldr_u32(tmp2, Address(dst, oopDesc::klass_offset_in_bytes()));
2245         __ cmp_32(tmp, tmp2);
2246       } else {
2247         __ load_klass(tmp, src);
2248         __ load_klass(tmp2, dst);
2249         __ cmp(tmp, tmp2);
2250       }
2251       __ b(*stub-&gt;entry(), ne);
2252     } else {
2253       // For object arrays, if src is a sub class of dst then we can
2254       // safely do the copy.
2255       Label cont, slow;
2256 
2257       address copyfunc_addr = StubRoutines::checkcast_arraycopy();
2258 
2259       __ load_klass(tmp, src);
2260       __ load_klass(tmp2, dst);
2261 
2262       // We are at a call so all live registers are saved before we
2263       // get here
2264       assert_different_registers(tmp, tmp2, R6, altFP_7_11);
2265 
2266       __ check_klass_subtype_fast_path(tmp, tmp2, R6, altFP_7_11, &amp;cont, copyfunc_addr == NULL ? stub-&gt;entry() : &amp;slow, NULL);
2267 
2268       __ mov(R6, R0);
2269       __ mov(altFP_7_11, R1);
2270       __ mov(R0, tmp);
2271       __ mov(R1, tmp2);
2272       __ call(Runtime1::entry_for(Runtime1::slow_subtype_check_id), relocInfo::runtime_call_type); // does not blow any registers except R0, LR and Rtemp
2273       __ cmp_32(R0, 0);
2274       __ mov(R0, R6);
2275       __ mov(R1, altFP_7_11);
2276 
2277       if (copyfunc_addr != NULL) { // use stub if available
2278         // src is not a sub class of dst so we have to do a
2279         // per-element check.
2280 
2281         __ b(cont, ne);
2282 
2283         __ bind(slow);
2284 
2285         int mask = LIR_OpArrayCopy::src_objarray|LIR_OpArrayCopy::dst_objarray;
2286         if ((flags &amp; mask) != mask) {
2287           // Check that at least both of them object arrays.
2288           assert(flags &amp; mask, &quot;one of the two should be known to be an object array&quot;);
2289 
2290           if (!(flags &amp; LIR_OpArrayCopy::src_objarray)) {
2291             __ load_klass(tmp, src);
2292           } else if (!(flags &amp; LIR_OpArrayCopy::dst_objarray)) {
2293             __ load_klass(tmp, dst);
2294           }
2295           int lh_offset = in_bytes(Klass::layout_helper_offset());
2296 
2297           __ ldr_u32(tmp2, Address(tmp, lh_offset));
2298 
2299           jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
2300           __ mov_slow(tmp, objArray_lh);
2301           __ cmp_32(tmp, tmp2);
2302           __ b(*stub-&gt;entry(), ne);
2303         }
2304 
2305         save_in_reserved_area(R0, R1, R2, R3);
2306 
2307         Register src_ptr = R0;
2308         Register dst_ptr = R1;
2309         Register len     = R2;
2310         Register chk_off = R3;
2311         Register super_k = tmp;
2312 
2313         __ add(src_ptr, src, arrayOopDesc::base_offset_in_bytes(basic_type));
2314         __ add_ptr_scaled_int32(src_ptr, src_ptr, src_pos, shift);
2315 
2316         __ add(dst_ptr, dst, arrayOopDesc::base_offset_in_bytes(basic_type));
2317         __ add_ptr_scaled_int32(dst_ptr, dst_ptr, dst_pos, shift);
2318         __ load_klass(tmp, dst);
2319 
2320         int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());
2321         int sco_offset = in_bytes(Klass::super_check_offset_offset());
2322 
2323         __ ldr(super_k, Address(tmp, ek_offset));
2324 
2325         __ mov(len, length);
2326         __ ldr_u32(chk_off, Address(super_k, sco_offset));
2327         __ push(super_k);
2328 
2329         __ call(copyfunc_addr, relocInfo::runtime_call_type);
2330 
2331 #ifndef PRODUCT
2332         if (PrintC1Statistics) {
2333           Label failed;
2334           __ cbnz_32(R0, failed);
2335           __ inc_counter((address)&amp;Runtime1::_arraycopy_checkcast_cnt, tmp, tmp2);
2336           __ bind(failed);
2337         }
2338 #endif // PRODUCT
2339 
2340         __ add(SP, SP, wordSize);  // Drop super_k argument
2341 
2342         __ cbz_32(R0, *stub-&gt;continuation());
2343         __ mvn_32(tmp, R0);
2344 
2345         // load saved arguments in slow case only
2346         restore_from_reserved_area(R0, R1, R2, R3);
2347 
2348         __ sub_32(length, length, tmp);
2349         __ add_32(src_pos, src_pos, tmp);
2350         __ add_32(dst_pos, dst_pos, tmp);
2351 
2352 #ifndef PRODUCT
2353         if (PrintC1Statistics) {
2354           __ inc_counter((address)&amp;Runtime1::_arraycopy_checkcast_attempt_cnt, tmp, tmp2);
2355         }
2356 #endif
2357 
2358         __ b(*stub-&gt;entry());
2359 
2360         __ bind(cont);
2361       } else {
2362         __ b(*stub-&gt;entry(), eq);
2363         __ bind(cont);
2364       }
2365     }
2366   }
2367 
2368 #ifndef PRODUCT
2369   if (PrintC1Statistics) {
2370     address counter = Runtime1::arraycopy_count_address(basic_type);
2371     __ inc_counter(counter, tmp, tmp2);
2372   }
2373 #endif // !PRODUCT
2374 
2375   bool disjoint = (flags &amp; LIR_OpArrayCopy::overlapping) == 0;
2376   bool aligned = (flags &amp; LIR_OpArrayCopy::unaligned) == 0;
2377   const char *name;
2378   address entry = StubRoutines::select_arraycopy_function(basic_type, aligned, disjoint, name, false);
2379 
2380   Register src_ptr = R0;
2381   Register dst_ptr = R1;
2382   Register len     = R2;
2383 
2384   __ add(src_ptr, src, arrayOopDesc::base_offset_in_bytes(basic_type));
2385   __ add_ptr_scaled_int32(src_ptr, src_ptr, src_pos, shift);
2386 
2387   __ add(dst_ptr, dst, arrayOopDesc::base_offset_in_bytes(basic_type));
2388   __ add_ptr_scaled_int32(dst_ptr, dst_ptr, dst_pos, shift);
2389 
2390   __ mov(len, length);
2391 
2392   __ call(entry, relocInfo::runtime_call_type);
2393 
2394   __ bind(*stub-&gt;continuation());
2395 }
2396 
2397 #ifdef ASSERT
2398  // emit run-time assertion
2399 void LIR_Assembler::emit_assert(LIR_OpAssert* op) {
2400   assert(op-&gt;code() == lir_assert, &quot;must be&quot;);
2401 
2402   if (op-&gt;in_opr1()-&gt;is_valid()) {
2403     assert(op-&gt;in_opr2()-&gt;is_valid(), &quot;both operands must be valid&quot;);
2404     comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
2405   } else {
2406     assert(op-&gt;in_opr2()-&gt;is_illegal(), &quot;both operands must be illegal&quot;);
2407     assert(op-&gt;condition() == lir_cond_always, &quot;no other conditions allowed&quot;);
2408   }
2409 
2410   Label ok;
2411   if (op-&gt;condition() != lir_cond_always) {
2412     AsmCondition acond = al;
2413     switch (op-&gt;condition()) {
2414       case lir_cond_equal:        acond = eq; break;
2415       case lir_cond_notEqual:     acond = ne; break;
2416       case lir_cond_less:         acond = lt; break;
2417       case lir_cond_lessEqual:    acond = le; break;
2418       case lir_cond_greaterEqual: acond = ge; break;
2419       case lir_cond_greater:      acond = gt; break;
2420       case lir_cond_aboveEqual:   acond = hs; break;
2421       case lir_cond_belowEqual:   acond = ls; break;
2422       default:                    ShouldNotReachHere();
2423     }
2424     __ b(ok, acond);
2425   }
2426   if (op-&gt;halt()) {
2427     const char* str = __ code_string(op-&gt;msg());
2428     __ stop(str);
2429   } else {
2430     breakpoint();
2431   }
2432   __ bind(ok);
2433 }
2434 #endif // ASSERT
2435 
2436 void LIR_Assembler::emit_updatecrc32(LIR_OpUpdateCRC32* op) {
2437   fatal(&quot;CRC32 intrinsic is not implemented on this platform&quot;);
2438 }
2439 
2440 void LIR_Assembler::emit_lock(LIR_OpLock* op) {
2441   Register obj = op-&gt;obj_opr()-&gt;as_pointer_register();
2442   Register hdr = op-&gt;hdr_opr()-&gt;as_pointer_register();
2443   Register lock = op-&gt;lock_opr()-&gt;as_pointer_register();
2444   Register tmp = op-&gt;scratch_opr()-&gt;is_illegal() ? noreg :
2445                  op-&gt;scratch_opr()-&gt;as_pointer_register();
2446 
2447   if (!UseFastLocking) {
2448     __ b(*op-&gt;stub()-&gt;entry());
2449   } else if (op-&gt;code() == lir_lock) {
2450     assert(BasicLock::displaced_header_offset_in_bytes() == 0, &quot;lock_reg must point to the displaced header&quot;);
2451     __ resolve(ACCESS_READ | ACCESS_WRITE, obj);
2452     int null_check_offset = __ lock_object(hdr, obj, lock, tmp, *op-&gt;stub()-&gt;entry());
2453     if (op-&gt;info() != NULL) {
2454       add_debug_info_for_null_check(null_check_offset, op-&gt;info());
2455     }
2456   } else if (op-&gt;code() == lir_unlock) {
2457     __ unlock_object(hdr, obj, lock, tmp, *op-&gt;stub()-&gt;entry());
2458   } else {
2459     ShouldNotReachHere();
2460   }
2461   __ bind(*op-&gt;stub()-&gt;continuation());
2462 }
2463 
2464 
2465 void LIR_Assembler::emit_profile_call(LIR_OpProfileCall* op) {
2466   ciMethod* method = op-&gt;profiled_method();
2467   int bci          = op-&gt;profiled_bci();
2468   ciMethod* callee = op-&gt;profiled_callee();
2469 
2470   // Update counter for all call types
2471   ciMethodData* md = method-&gt;method_data_or_null();
2472   assert(md != NULL, &quot;Sanity&quot;);
2473   ciProfileData* data = md-&gt;bci_to_data(bci);
2474   assert(data != NULL &amp;&amp; data-&gt;is_CounterData(), &quot;need CounterData for calls&quot;);
2475   assert(op-&gt;mdo()-&gt;is_single_cpu(),  &quot;mdo must be allocated&quot;);
2476   Register mdo  = op-&gt;mdo()-&gt;as_register();
2477   assert(op-&gt;tmp1()-&gt;is_register(), &quot;tmp1 must be allocated&quot;);
2478   Register tmp1 = op-&gt;tmp1()-&gt;as_pointer_register();
2479   assert_different_registers(mdo, tmp1);
2480   __ mov_metadata(mdo, md-&gt;constant_encoding());
2481   int mdo_offset_bias = 0;
2482   int max_offset = 4096;
2483   if (md-&gt;byte_offset_of_slot(data, CounterData::count_offset()) + data-&gt;size_in_bytes() &gt;= max_offset) {
2484     // The offset is large so bias the mdo by the base of the slot so
2485     // that the ldr can use an immediate offset to reference the slots of the data
2486     mdo_offset_bias = md-&gt;byte_offset_of_slot(data, CounterData::count_offset());
2487     __ mov_slow(tmp1, mdo_offset_bias);
2488     __ add(mdo, mdo, tmp1);
2489   }
2490 
2491   Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()) - mdo_offset_bias);
2492   // Perform additional virtual call profiling for invokevirtual and
2493   // invokeinterface bytecodes
2494   if (op-&gt;should_profile_receiver_type()) {
2495     assert(op-&gt;recv()-&gt;is_single_cpu(), &quot;recv must be allocated&quot;);
2496     Register recv = op-&gt;recv()-&gt;as_register();
2497     assert_different_registers(mdo, tmp1, recv);
2498     assert(data-&gt;is_VirtualCallData(), &quot;need VirtualCallData for virtual calls&quot;);
2499     ciKlass* known_klass = op-&gt;known_holder();
2500     if (C1OptimizeVirtualCallProfiling &amp;&amp; known_klass != NULL) {
2501       // We know the type that will be seen at this call site; we can
2502       // statically update the MethodData* rather than needing to do
2503       // dynamic tests on the receiver type
2504 
2505       // NOTE: we should probably put a lock around this search to
2506       // avoid collisions by concurrent compilations
2507       ciVirtualCallData* vc_data = (ciVirtualCallData*) data;
2508       uint i;
2509       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2510         ciKlass* receiver = vc_data-&gt;receiver(i);
2511         if (known_klass-&gt;equals(receiver)) {
2512           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data,
2513                                                          VirtualCallData::receiver_count_offset(i)) -
2514                             mdo_offset_bias);
2515           __ ldr(tmp1, data_addr);
2516           __ add(tmp1, tmp1, DataLayout::counter_increment);
2517           __ str(tmp1, data_addr);
2518           return;
2519         }
2520       }
2521 
2522       // Receiver type not found in profile data; select an empty slot
2523 
2524       // Note that this is less efficient than it should be because it
2525       // always does a write to the receiver part of the
2526       // VirtualCallData rather than just the first time
2527       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2528         ciKlass* receiver = vc_data-&gt;receiver(i);
2529         if (receiver == NULL) {
2530           Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_offset(i)) -
2531                             mdo_offset_bias);
2532           __ mov_metadata(tmp1, known_klass-&gt;constant_encoding());
2533           __ str(tmp1, recv_addr);
2534           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)) -
2535                             mdo_offset_bias);
2536           __ ldr(tmp1, data_addr);
2537           __ add(tmp1, tmp1, DataLayout::counter_increment);
2538           __ str(tmp1, data_addr);
2539           return;
2540         }
2541       }
2542     } else {
2543       __ load_klass(recv, recv);
2544       Label update_done;
2545       type_profile_helper(mdo, mdo_offset_bias, md, data, recv, tmp1, &amp;update_done);
2546       // Receiver did not match any saved receiver and there is no empty row for it.
2547       // Increment total counter to indicate polymorphic case.
2548       __ ldr(tmp1, counter_addr);
2549       __ add(tmp1, tmp1, DataLayout::counter_increment);
2550       __ str(tmp1, counter_addr);
2551 
2552       __ bind(update_done);
2553     }
2554   } else {
2555     // Static call
2556     __ ldr(tmp1, counter_addr);
2557     __ add(tmp1, tmp1, DataLayout::counter_increment);
2558     __ str(tmp1, counter_addr);
2559   }
2560 }
2561 
2562 void LIR_Assembler::emit_profile_type(LIR_OpProfileType* op) {
2563   fatal(&quot;Type profiling not implemented on this platform&quot;);
2564 }
2565 
2566 void LIR_Assembler::emit_delay(LIR_OpDelay*) {
2567   Unimplemented();
2568 }
2569 
2570 
2571 void LIR_Assembler::monitor_address(int monitor_no, LIR_Opr dst) {
2572   Address mon_addr = frame_map()-&gt;address_for_monitor_lock(monitor_no);
2573   __ add_slow(dst-&gt;as_pointer_register(), mon_addr.base(), mon_addr.disp());
2574 }
2575 
2576 
2577 void LIR_Assembler::align_backward_branch_target() {
2578   // Some ARM processors do better with 8-byte branch target alignment
2579   __ align(8);
2580 }
2581 
2582 
2583 void LIR_Assembler::negate(LIR_Opr left, LIR_Opr dest, LIR_Opr tmp) {
2584   // tmp must be unused
2585   assert(tmp-&gt;is_illegal(), &quot;wasting a register if tmp is allocated&quot;);
2586 
2587   if (left-&gt;is_single_cpu()) {
2588     assert (dest-&gt;type() == T_INT, &quot;unexpected result type&quot;);
2589     assert (left-&gt;type() == T_INT, &quot;unexpected left type&quot;);
2590     __ neg_32(dest-&gt;as_register(), left-&gt;as_register());
2591   } else if (left-&gt;is_double_cpu()) {
2592     Register dest_lo = dest-&gt;as_register_lo();
2593     Register dest_hi = dest-&gt;as_register_hi();
2594     Register src_lo = left-&gt;as_register_lo();
2595     Register src_hi = left-&gt;as_register_hi();
2596     if (dest_lo == src_hi) {
2597       dest_lo = Rtemp;
2598     }
2599     __ rsbs(dest_lo, src_lo, 0);
2600     __ rsc(dest_hi, src_hi, 0);
2601     move_regs(dest_lo, dest-&gt;as_register_lo());
2602   } else if (left-&gt;is_single_fpu()) {
2603     __ neg_float(dest-&gt;as_float_reg(), left-&gt;as_float_reg());
2604   } else if (left-&gt;is_double_fpu()) {
2605     __ neg_double(dest-&gt;as_double_reg(), left-&gt;as_double_reg());
2606   } else {
2607     ShouldNotReachHere();
2608   }
2609 }
2610 
2611 
2612 void LIR_Assembler::leal(LIR_Opr addr_opr, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
2613   assert(patch_code == lir_patch_none, &quot;Patch code not supported&quot;);
2614   LIR_Address* addr = addr_opr-&gt;as_address_ptr();
2615   if (addr-&gt;index()-&gt;is_illegal()) {
2616     jint c = addr-&gt;disp();
2617     if (!Assembler::is_arith_imm_in_range(c)) {
2618       BAILOUT(&quot;illegal arithmetic operand&quot;);
2619     }
2620     __ add(dest-&gt;as_pointer_register(), addr-&gt;base()-&gt;as_pointer_register(), c);
2621   } else {
2622     assert(addr-&gt;disp() == 0, &quot;cannot handle otherwise&quot;);
2623     __ add(dest-&gt;as_pointer_register(), addr-&gt;base()-&gt;as_pointer_register(),
2624            AsmOperand(addr-&gt;index()-&gt;as_pointer_register(), lsl, addr-&gt;scale()));
2625   }
2626 }
2627 
2628 
2629 void LIR_Assembler::rt_call(LIR_Opr result, address dest, const LIR_OprList* args, LIR_Opr tmp, CodeEmitInfo* info) {
2630   assert(!tmp-&gt;is_valid(), &quot;don&#39;t need temporary&quot;);
2631   __ call(dest);
2632   if (info != NULL) {
2633     add_call_info_here(info);
2634   }
2635 }
2636 
2637 
2638 void LIR_Assembler::volatile_move_op(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info) {
2639   assert(src-&gt;is_double_cpu() &amp;&amp; dest-&gt;is_address() ||
2640          src-&gt;is_address() &amp;&amp; dest-&gt;is_double_cpu(),
2641          &quot;Simple move_op is called for all other cases&quot;);
2642 
2643   int null_check_offset;
2644   if (dest-&gt;is_address()) {
2645     // Store
2646     const LIR_Address* addr = dest-&gt;as_address_ptr();
2647     const Register src_lo = src-&gt;as_register_lo();
2648     const Register src_hi = src-&gt;as_register_hi();
2649     assert(addr-&gt;index()-&gt;is_illegal() &amp;&amp; addr-&gt;disp() == 0, &quot;The address is simple already&quot;);
2650 
2651     if (src_lo &lt; src_hi) {
2652       null_check_offset = __ offset();
2653       __ stmia(addr-&gt;base()-&gt;as_register(), RegisterSet(src_lo) | RegisterSet(src_hi));
2654     } else {
2655       assert(src_lo &lt; Rtemp, &quot;Rtemp is higher than any allocatable register&quot;);
2656       __ mov(Rtemp, src_hi);
2657       null_check_offset = __ offset();
2658       __ stmia(addr-&gt;base()-&gt;as_register(), RegisterSet(src_lo) | RegisterSet(Rtemp));
2659     }
2660   } else {
2661     // Load
2662     const LIR_Address* addr = src-&gt;as_address_ptr();
2663     const Register dest_lo = dest-&gt;as_register_lo();
2664     const Register dest_hi = dest-&gt;as_register_hi();
2665     assert(addr-&gt;index()-&gt;is_illegal() &amp;&amp; addr-&gt;disp() == 0, &quot;The address is simple already&quot;);
2666 
2667     null_check_offset = __ offset();
2668     if (dest_lo &lt; dest_hi) {
2669       __ ldmia(addr-&gt;base()-&gt;as_register(), RegisterSet(dest_lo) | RegisterSet(dest_hi));
2670     } else {
2671       assert(dest_lo &lt; Rtemp, &quot;Rtemp is higher than any allocatable register&quot;);
2672       __ ldmia(addr-&gt;base()-&gt;as_register(), RegisterSet(dest_lo) | RegisterSet(Rtemp));
2673       __ mov(dest_hi, Rtemp);
2674     }
2675   }
2676 
2677   if (info != NULL) {
2678     add_debug_info_for_null_check(null_check_offset, info);
2679   }
2680 }
2681 
2682 
2683 void LIR_Assembler::membar() {
2684   __ membar(MacroAssembler::StoreLoad, Rtemp);
2685 }
2686 
2687 void LIR_Assembler::membar_acquire() {
2688   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), Rtemp);
2689 }
2690 
2691 void LIR_Assembler::membar_release() {
2692   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);
2693 }
2694 
2695 void LIR_Assembler::membar_loadload() {
2696   __ membar(MacroAssembler::LoadLoad, Rtemp);
2697 }
2698 
2699 void LIR_Assembler::membar_storestore() {
2700   __ membar(MacroAssembler::StoreStore, Rtemp);
2701 }
2702 
2703 void LIR_Assembler::membar_loadstore() {
2704   __ membar(MacroAssembler::LoadStore, Rtemp);
2705 }
2706 
2707 void LIR_Assembler::membar_storeload() {
2708   __ membar(MacroAssembler::StoreLoad, Rtemp);
2709 }
2710 
2711 void LIR_Assembler::on_spin_wait() {
2712   Unimplemented();
2713 }
2714 
2715 void LIR_Assembler::get_thread(LIR_Opr result_reg) {
2716   // Not used on ARM
2717   Unimplemented();
2718 }
2719 
2720 void LIR_Assembler::peephole(LIR_List* lir) {
2721   LIR_OpList* inst = lir-&gt;instructions_list();
2722   const int inst_length = inst-&gt;length();
2723   for (int i = 0; i &lt; inst_length; i++) {
2724     LIR_Op* op = inst-&gt;at(i);
2725     switch (op-&gt;code()) {
2726       case lir_cmp: {
2727         // Replace:
2728         //   cmp rX, y
2729         //   cmove [EQ] y, z, rX
2730         // with
2731         //   cmp rX, y
2732         //   cmove [EQ] illegalOpr, z, rX
2733         //
2734         // or
2735         //   cmp rX, y
2736         //   cmove [NE] z, y, rX
2737         // with
2738         //   cmp rX, y
2739         //   cmove [NE] z, illegalOpr, rX
2740         //
2741         // moves from illegalOpr should be removed when converting LIR to native assembly
2742 
2743         LIR_Op2* cmp = op-&gt;as_Op2();
2744         assert(cmp != NULL, &quot;cmp LIR instruction is not an op2&quot;);
2745 
2746         if (i + 1 &lt; inst_length) {
2747           LIR_Op2* cmove = inst-&gt;at(i + 1)-&gt;as_Op2();
2748           if (cmove != NULL &amp;&amp; cmove-&gt;code() == lir_cmove) {
2749             LIR_Opr cmove_res = cmove-&gt;result_opr();
2750             bool res_is_op1 = cmove_res == cmp-&gt;in_opr1();
2751             bool res_is_op2 = cmove_res == cmp-&gt;in_opr2();
2752             LIR_Opr cmp_res, cmp_arg;
2753             if (res_is_op1) {
2754               cmp_res = cmp-&gt;in_opr1();
2755               cmp_arg = cmp-&gt;in_opr2();
2756             } else if (res_is_op2) {
2757               cmp_res = cmp-&gt;in_opr2();
2758               cmp_arg = cmp-&gt;in_opr1();
2759             } else {
2760               cmp_res = LIR_OprFact::illegalOpr;
2761               cmp_arg = LIR_OprFact::illegalOpr;
2762             }
2763 
2764             if (cmp_res != LIR_OprFact::illegalOpr) {
2765               LIR_Condition cond = cmove-&gt;condition();
2766               if (cond == lir_cond_equal &amp;&amp; cmove-&gt;in_opr1() == cmp_arg) {
2767                 cmove-&gt;set_in_opr1(LIR_OprFact::illegalOpr);
2768               } else if (cond == lir_cond_notEqual &amp;&amp; cmove-&gt;in_opr2() == cmp_arg) {
2769                 cmove-&gt;set_in_opr2(LIR_OprFact::illegalOpr);
2770               }
2771             }
2772           }
2773         }
2774         break;
2775       }
2776 
2777       default:
2778         break;
2779     }
2780   }
2781 }
2782 
2783 void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp) {
2784   assert(src-&gt;is_address(), &quot;sanity&quot;);
2785   Address addr = as_Address(src-&gt;as_address_ptr());
2786 
2787   if (code == lir_xchg) {
2788   } else {
2789     assert (!data-&gt;is_oop(), &quot;xadd for oops&quot;);
2790   }
2791 
2792   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreStore | MacroAssembler::LoadStore), Rtemp);
2793 
2794   Label retry;
2795   __ bind(retry);
2796 
2797   if (data-&gt;type() == T_INT || data-&gt;is_oop()) {
2798     Register dst = dest-&gt;as_register();
2799     Register new_val = noreg;
2800     __ ldrex(dst, addr);
2801     if (code == lir_xadd) {
2802       Register tmp_reg = tmp-&gt;as_register();
2803       if (data-&gt;is_constant()) {
2804         assert_different_registers(dst, tmp_reg);
2805         __ add_32(tmp_reg, dst, data-&gt;as_constant_ptr()-&gt;as_jint());
2806       } else {
2807         assert_different_registers(dst, tmp_reg, data-&gt;as_register());
2808         __ add_32(tmp_reg, dst, data-&gt;as_register());
2809       }
2810       new_val = tmp_reg;
2811     } else {
2812       if (UseCompressedOops &amp;&amp; data-&gt;is_oop()) {
2813         new_val = tmp-&gt;as_pointer_register();
2814       } else {
2815         new_val = data-&gt;as_register();
2816       }
2817       assert_different_registers(dst, new_val);
2818     }
2819     __ strex(Rtemp, new_val, addr);
2820 
2821   } else if (data-&gt;type() == T_LONG) {
2822     Register dst_lo = dest-&gt;as_register_lo();
2823     Register new_val_lo = noreg;
2824     Register dst_hi = dest-&gt;as_register_hi();
2825 
2826     assert(dst_hi-&gt;encoding() == dst_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
2827     assert((dst_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
2828 
2829     __ bind(retry);
2830     __ ldrexd(dst_lo, addr);
2831     if (code == lir_xadd) {
2832       Register tmp_lo = tmp-&gt;as_register_lo();
2833       Register tmp_hi = tmp-&gt;as_register_hi();
2834 
2835       assert(tmp_hi-&gt;encoding() == tmp_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
2836       assert((tmp_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
2837 
2838       if (data-&gt;is_constant()) {
2839         jlong c = data-&gt;as_constant_ptr()-&gt;as_jlong();
2840         assert((jlong)((jint)c) == c, &quot;overflow&quot;);
2841         assert_different_registers(dst_lo, dst_hi, tmp_lo, tmp_hi);
2842         __ adds(tmp_lo, dst_lo, (jint)c);
2843         __ adc(tmp_hi, dst_hi, 0);
2844       } else {
2845         Register new_val_lo = data-&gt;as_register_lo();
2846         Register new_val_hi = data-&gt;as_register_hi();
2847         __ adds(tmp_lo, dst_lo, new_val_lo);
2848         __ adc(tmp_hi, dst_hi, new_val_hi);
2849         assert_different_registers(dst_lo, dst_hi, tmp_lo, tmp_hi, new_val_lo, new_val_hi);
2850       }
2851       new_val_lo = tmp_lo;
2852     } else {
2853       new_val_lo = data-&gt;as_register_lo();
2854       Register new_val_hi = data-&gt;as_register_hi();
2855 
2856       assert_different_registers(dst_lo, dst_hi, new_val_lo, new_val_hi);
2857       assert(new_val_hi-&gt;encoding() == new_val_lo-&gt;encoding() + 1, &quot;non aligned register pair&quot;);
2858       assert((new_val_lo-&gt;encoding() &amp; 0x1) == 0, &quot;misaligned register pair&quot;);
2859     }
2860     __ strexd(Rtemp, new_val_lo, addr);
2861   } else {
2862     ShouldNotReachHere();
2863   }
2864 
2865   __ cbnz_32(Rtemp, retry);
2866   __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreLoad | MacroAssembler::StoreStore), Rtemp);
2867 
2868 }
2869 
2870 #undef __
<a name="4" id="anc4"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="4" type="hidden" />
</body>
</html>