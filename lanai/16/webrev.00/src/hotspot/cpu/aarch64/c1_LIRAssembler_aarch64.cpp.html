<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2014, Red Hat Inc. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;asm/assembler.hpp&quot;
  29 #include &quot;c1/c1_CodeStubs.hpp&quot;
  30 #include &quot;c1/c1_Compilation.hpp&quot;
  31 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  32 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  33 #include &quot;c1/c1_Runtime1.hpp&quot;
  34 #include &quot;c1/c1_ValueStack.hpp&quot;
  35 #include &quot;ci/ciArrayKlass.hpp&quot;
  36 #include &quot;ci/ciInstance.hpp&quot;
  37 #include &quot;code/compiledIC.hpp&quot;
  38 #include &quot;gc/shared/collectedHeap.hpp&quot;
  39 #include &quot;nativeInst_aarch64.hpp&quot;
  40 #include &quot;oops/objArrayKlass.hpp&quot;
  41 #include &quot;runtime/frame.inline.hpp&quot;
  42 #include &quot;runtime/sharedRuntime.hpp&quot;
  43 #include &quot;utilities/powerOfTwo.hpp&quot;
  44 #include &quot;vmreg_aarch64.inline.hpp&quot;
  45 
  46 
  47 #ifndef PRODUCT
  48 #define COMMENT(x)   do { __ block_comment(x); } while (0)
  49 #else
  50 #define COMMENT(x)
  51 #endif
  52 
  53 NEEDS_CLEANUP // remove this definitions ?
  54 const Register IC_Klass    = rscratch2;   // where the IC klass is cached
  55 const Register SYNC_header = r0;   // synchronization header
  56 const Register SHIFT_count = r0;   // where count for shift operations must be
  57 
  58 #define __ _masm-&gt;
  59 
  60 
  61 static void select_different_registers(Register preserve,
  62                                        Register extra,
  63                                        Register &amp;tmp1,
  64                                        Register &amp;tmp2) {
  65   if (tmp1 == preserve) {
  66     assert_different_registers(tmp1, tmp2, extra);
  67     tmp1 = extra;
  68   } else if (tmp2 == preserve) {
  69     assert_different_registers(tmp1, tmp2, extra);
  70     tmp2 = extra;
  71   }
  72   assert_different_registers(preserve, tmp1, tmp2);
  73 }
  74 
  75 
  76 
  77 static void select_different_registers(Register preserve,
  78                                        Register extra,
  79                                        Register &amp;tmp1,
  80                                        Register &amp;tmp2,
  81                                        Register &amp;tmp3) {
  82   if (tmp1 == preserve) {
  83     assert_different_registers(tmp1, tmp2, tmp3, extra);
  84     tmp1 = extra;
  85   } else if (tmp2 == preserve) {
  86     assert_different_registers(tmp1, tmp2, tmp3, extra);
  87     tmp2 = extra;
  88   } else if (tmp3 == preserve) {
  89     assert_different_registers(tmp1, tmp2, tmp3, extra);
  90     tmp3 = extra;
  91   }
  92   assert_different_registers(preserve, tmp1, tmp2, tmp3);
  93 }
  94 
  95 
  96 bool LIR_Assembler::is_small_constant(LIR_Opr opr) { Unimplemented(); return false; }
  97 
  98 
  99 LIR_Opr LIR_Assembler::receiverOpr() {
 100   return FrameMap::receiver_opr;
 101 }
 102 
 103 LIR_Opr LIR_Assembler::osrBufferPointer() {
 104   return FrameMap::as_pointer_opr(receiverOpr()-&gt;as_register());
 105 }
 106 
 107 //--------------fpu register translations-----------------------
 108 
 109 
 110 address LIR_Assembler::float_constant(float f) {
 111   address const_addr = __ float_constant(f);
 112   if (const_addr == NULL) {
 113     bailout(&quot;const section overflow&quot;);
 114     return __ code()-&gt;consts()-&gt;start();
 115   } else {
 116     return const_addr;
 117   }
 118 }
 119 
 120 
 121 address LIR_Assembler::double_constant(double d) {
 122   address const_addr = __ double_constant(d);
 123   if (const_addr == NULL) {
 124     bailout(&quot;const section overflow&quot;);
 125     return __ code()-&gt;consts()-&gt;start();
 126   } else {
 127     return const_addr;
 128   }
 129 }
 130 
 131 address LIR_Assembler::int_constant(jlong n) {
 132   address const_addr = __ long_constant(n);
 133   if (const_addr == NULL) {
 134     bailout(&quot;const section overflow&quot;);
 135     return __ code()-&gt;consts()-&gt;start();
 136   } else {
 137     return const_addr;
 138   }
 139 }
 140 
 141 void LIR_Assembler::breakpoint() { Unimplemented(); }
 142 
 143 void LIR_Assembler::push(LIR_Opr opr) { Unimplemented(); }
 144 
 145 void LIR_Assembler::pop(LIR_Opr opr) { Unimplemented(); }
 146 
 147 bool LIR_Assembler::is_literal_address(LIR_Address* addr) { Unimplemented(); return false; }
 148 //-------------------------------------------
 149 
 150 static Register as_reg(LIR_Opr op) {
 151   return op-&gt;is_double_cpu() ? op-&gt;as_register_lo() : op-&gt;as_register();
 152 }
 153 
 154 static jlong as_long(LIR_Opr data) {
 155   jlong result;
 156   switch (data-&gt;type()) {
 157   case T_INT:
 158     result = (data-&gt;as_jint());
 159     break;
 160   case T_LONG:
 161     result = (data-&gt;as_jlong());
 162     break;
 163   default:
 164     ShouldNotReachHere();
 165     result = 0;  // unreachable
 166   }
 167   return result;
 168 }
 169 
 170 Address LIR_Assembler::as_Address(LIR_Address* addr, Register tmp) {
 171   Register base = addr-&gt;base()-&gt;as_pointer_register();
 172   LIR_Opr opr = addr-&gt;index();
 173   if (opr-&gt;is_cpu_register()) {
 174     Register index;
 175     if (opr-&gt;is_single_cpu())
 176       index = opr-&gt;as_register();
 177     else
 178       index = opr-&gt;as_register_lo();
 179     assert(addr-&gt;disp() == 0, &quot;must be&quot;);
 180     switch(opr-&gt;type()) {
 181       case T_INT:
 182         return Address(base, index, Address::sxtw(addr-&gt;scale()));
 183       case T_LONG:
 184         return Address(base, index, Address::lsl(addr-&gt;scale()));
 185       default:
 186         ShouldNotReachHere();
 187       }
 188   } else  {
 189     intptr_t addr_offset = intptr_t(addr-&gt;disp());
 190     if (Address::offset_ok_for_immed(addr_offset, addr-&gt;scale()))
 191       return Address(base, addr_offset, Address::lsl(addr-&gt;scale()));
 192     else {
 193       __ mov(tmp, addr_offset);
 194       return Address(base, tmp, Address::lsl(addr-&gt;scale()));
 195     }
 196   }
 197   return Address();
 198 }
 199 
 200 Address LIR_Assembler::as_Address_hi(LIR_Address* addr) {
 201   ShouldNotReachHere();
 202   return Address();
 203 }
 204 
 205 Address LIR_Assembler::as_Address(LIR_Address* addr) {
 206   return as_Address(addr, rscratch1);
 207 }
 208 
 209 Address LIR_Assembler::as_Address_lo(LIR_Address* addr) {
 210   return as_Address(addr, rscratch1);  // Ouch
 211   // FIXME: This needs to be much more clever.  See x86.
 212 }
 213 
 214 
 215 void LIR_Assembler::osr_entry() {
 216   offsets()-&gt;set_value(CodeOffsets::OSR_Entry, code_offset());
 217   BlockBegin* osr_entry = compilation()-&gt;hir()-&gt;osr_entry();
 218   ValueStack* entry_state = osr_entry-&gt;state();
 219   int number_of_locks = entry_state-&gt;locks_size();
 220 
 221   // we jump here if osr happens with the interpreter
 222   // state set up to continue at the beginning of the
 223   // loop that triggered osr - in particular, we have
 224   // the following registers setup:
 225   //
 226   // r2: osr buffer
 227   //
 228 
 229   // build frame
 230   ciMethod* m = compilation()-&gt;method();
 231   __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());
 232 
 233   // OSR buffer is
 234   //
 235   // locals[nlocals-1..0]
 236   // monitors[0..number_of_locks]
 237   //
 238   // locals is a direct copy of the interpreter frame so in the osr buffer
 239   // so first slot in the local array is the last local from the interpreter
 240   // and last slot is local[0] (receiver) from the interpreter
 241   //
 242   // Similarly with locks. The first lock slot in the osr buffer is the nth lock
 243   // from the interpreter frame, the nth lock slot in the osr buffer is 0th lock
 244   // in the interpreter frame (the method lock if a sync method)
 245 
 246   // Initialize monitors in the compiled activation.
 247   //   r2: pointer to osr buffer
 248   //
 249   // All other registers are dead at this point and the locals will be
 250   // copied into place by code emitted in the IR.
 251 
 252   Register OSR_buf = osrBufferPointer()-&gt;as_pointer_register();
 253   { assert(frame::interpreter_frame_monitor_size() == BasicObjectLock::size(), &quot;adjust code below&quot;);
 254     int monitor_offset = BytesPerWord * method()-&gt;max_locals() +
 255       (2 * BytesPerWord) * (number_of_locks - 1);
 256     // SharedRuntime::OSR_migration_begin() packs BasicObjectLocks in
 257     // the OSR buffer using 2 word entries: first the lock and then
 258     // the oop.
 259     for (int i = 0; i &lt; number_of_locks; i++) {
 260       int slot_offset = monitor_offset - ((i * 2) * BytesPerWord);
 261 #ifdef ASSERT
 262       // verify the interpreter&#39;s monitor has a non-null object
 263       {
 264         Label L;
 265         __ ldr(rscratch1, Address(OSR_buf, slot_offset + 1*BytesPerWord));
 266         __ cbnz(rscratch1, L);
 267         __ stop(&quot;locked object is NULL&quot;);
 268         __ bind(L);
 269       }
 270 #endif
 271       __ ldr(r19, Address(OSR_buf, slot_offset + 0));
 272       __ str(r19, frame_map()-&gt;address_for_monitor_lock(i));
 273       __ ldr(r19, Address(OSR_buf, slot_offset + 1*BytesPerWord));
 274       __ str(r19, frame_map()-&gt;address_for_monitor_object(i));
 275     }
 276   }
 277 }
 278 
 279 
 280 // inline cache check; done before the frame is built.
 281 int LIR_Assembler::check_icache() {
 282   Register receiver = FrameMap::receiver_opr-&gt;as_register();
 283   Register ic_klass = IC_Klass;
 284   int start_offset = __ offset();
 285   __ inline_cache_check(receiver, ic_klass);
 286 
 287   // if icache check fails, then jump to runtime routine
 288   // Note: RECEIVER must still contain the receiver!
 289   Label dont;
 290   __ br(Assembler::EQ, dont);
 291   __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
 292 
 293   // We align the verified entry point unless the method body
 294   // (including its inline cache check) will fit in a single 64-byte
 295   // icache line.
 296   if (! method()-&gt;is_accessor() || __ offset() - start_offset &gt; 4 * 4) {
 297     // force alignment after the cache check.
 298     __ align(CodeEntryAlignment);
 299   }
 300 
 301   __ bind(dont);
 302   return start_offset;
 303 }
 304 
 305 void LIR_Assembler::clinit_barrier(ciMethod* method) {
 306   assert(VM_Version::supports_fast_class_init_checks(), &quot;sanity&quot;);
 307   assert(!method-&gt;holder()-&gt;is_not_initialized(), &quot;initialization should have been started&quot;);
 308 
 309   Label L_skip_barrier;
 310 
 311   __ mov_metadata(rscratch2, method-&gt;holder()-&gt;constant_encoding());
 312   __ clinit_barrier(rscratch2, rscratch1, &amp;L_skip_barrier /*L_fast_path*/);
 313   __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));
 314   __ bind(L_skip_barrier);
 315 }
 316 
 317 void LIR_Assembler::jobject2reg(jobject o, Register reg) {
 318   if (o == NULL) {
 319     __ mov(reg, zr);
 320   } else {
 321     __ movoop(reg, o, /*immediate*/true);
 322   }
 323 }
 324 
 325 void LIR_Assembler::deoptimize_trap(CodeEmitInfo *info) {
 326   address target = NULL;
 327   relocInfo::relocType reloc_type = relocInfo::none;
 328 
 329   switch (patching_id(info)) {
 330   case PatchingStub::access_field_id:
 331     target = Runtime1::entry_for(Runtime1::access_field_patching_id);
 332     reloc_type = relocInfo::section_word_type;
 333     break;
 334   case PatchingStub::load_klass_id:
 335     target = Runtime1::entry_for(Runtime1::load_klass_patching_id);
 336     reloc_type = relocInfo::metadata_type;
 337     break;
 338   case PatchingStub::load_mirror_id:
 339     target = Runtime1::entry_for(Runtime1::load_mirror_patching_id);
 340     reloc_type = relocInfo::oop_type;
 341     break;
 342   case PatchingStub::load_appendix_id:
 343     target = Runtime1::entry_for(Runtime1::load_appendix_patching_id);
 344     reloc_type = relocInfo::oop_type;
 345     break;
 346   default: ShouldNotReachHere();
 347   }
 348 
 349   __ far_call(RuntimeAddress(target));
 350   add_call_info_here(info);
 351 }
 352 
 353 void LIR_Assembler::jobject2reg_with_patching(Register reg, CodeEmitInfo *info) {
 354   deoptimize_trap(info);
 355 }
 356 
 357 
 358 // This specifies the rsp decrement needed to build the frame
 359 int LIR_Assembler::initial_frame_size_in_bytes() const {
 360   // if rounding, must let FrameMap know!
 361 
 362   // The frame_map records size in slots (32bit word)
 363 
 364   // subtract two words to account for return address and link
 365   return (frame_map()-&gt;framesize() - (2*VMRegImpl::slots_per_word))  * VMRegImpl::stack_slot_size;
 366 }
 367 
 368 
 369 int LIR_Assembler::emit_exception_handler() {
 370   // if the last instruction is a call (typically to do a throw which
 371   // is coming at the end after block reordering) the return address
 372   // must still point into the code area in order to avoid assertion
 373   // failures when searching for the corresponding bci =&gt; add a nop
 374   // (was bug 5/14/1999 - gri)
 375   __ nop();
 376 
 377   // generate code for exception handler
 378   address handler_base = __ start_a_stub(exception_handler_size());
 379   if (handler_base == NULL) {
 380     // not enough space left for the handler
 381     bailout(&quot;exception handler overflow&quot;);
 382     return -1;
 383   }
 384 
 385   int offset = code_offset();
 386 
 387   // the exception oop and pc are in r0, and r3
 388   // no other registers need to be preserved, so invalidate them
 389   __ invalidate_registers(false, true, true, false, true, true);
 390 
 391   // check that there is really an exception
 392   __ verify_not_null_oop(r0);
 393 
 394   // search an exception handler (r0: exception oop, r3: throwing pc)
 395   __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::handle_exception_from_callee_id)));  __ should_not_reach_here();
 396   guarantee(code_offset() - offset &lt;= exception_handler_size(), &quot;overflow&quot;);
 397   __ end_a_stub();
 398 
 399   return offset;
 400 }
 401 
 402 
 403 // Emit the code to remove the frame from the stack in the exception
 404 // unwind path.
 405 int LIR_Assembler::emit_unwind_handler() {
 406 #ifndef PRODUCT
 407   if (CommentedAssembly) {
 408     _masm-&gt;block_comment(&quot;Unwind handler&quot;);
 409   }
 410 #endif
 411 
 412   int offset = code_offset();
 413 
 414   // Fetch the exception from TLS and clear out exception related thread state
 415   __ ldr(r0, Address(rthread, JavaThread::exception_oop_offset()));
 416   __ str(zr, Address(rthread, JavaThread::exception_oop_offset()));
 417   __ str(zr, Address(rthread, JavaThread::exception_pc_offset()));
 418 
 419   __ bind(_unwind_handler_entry);
 420   __ verify_not_null_oop(r0);
 421   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 422     __ mov(r19, r0);  // Preserve the exception
 423   }
 424 
 425   // Preform needed unlocking
 426   MonitorExitStub* stub = NULL;
 427   if (method()-&gt;is_synchronized()) {
 428     monitor_address(0, FrameMap::r0_opr);
 429     stub = new MonitorExitStub(FrameMap::r0_opr, true, 0);
 430     __ unlock_object(r5, r4, r0, *stub-&gt;entry());
 431     __ bind(*stub-&gt;continuation());
 432   }
 433 
 434   if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 435     __ mov(c_rarg0, rthread);
 436     __ mov_metadata(c_rarg1, method()-&gt;constant_encoding());
 437     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), c_rarg0, c_rarg1);
 438   }
 439 
 440   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 441     __ mov(r0, r19);  // Restore the exception
 442   }
 443 
 444   // remove the activation and dispatch to the unwind handler
 445   __ block_comment(&quot;remove_frame and dispatch to the unwind handler&quot;);
 446   __ remove_frame(initial_frame_size_in_bytes());
 447   __ far_jump(RuntimeAddress(Runtime1::entry_for(Runtime1::unwind_exception_id)));
 448 
 449   // Emit the slow path assembly
 450   if (stub != NULL) {
 451     stub-&gt;emit_code(this);
 452   }
 453 
 454   return offset;
 455 }
 456 
 457 
 458 int LIR_Assembler::emit_deopt_handler() {
 459   // if the last instruction is a call (typically to do a throw which
 460   // is coming at the end after block reordering) the return address
 461   // must still point into the code area in order to avoid assertion
 462   // failures when searching for the corresponding bci =&gt; add a nop
 463   // (was bug 5/14/1999 - gri)
 464   __ nop();
 465 
 466   // generate code for exception handler
 467   address handler_base = __ start_a_stub(deopt_handler_size());
 468   if (handler_base == NULL) {
 469     // not enough space left for the handler
 470     bailout(&quot;deopt handler overflow&quot;);
 471     return -1;
 472   }
 473 
 474   int offset = code_offset();
 475 
 476   __ adr(lr, pc());
 477   __ far_jump(RuntimeAddress(SharedRuntime::deopt_blob()-&gt;unpack()));
 478   guarantee(code_offset() - offset &lt;= deopt_handler_size(), &quot;overflow&quot;);
 479   __ end_a_stub();
 480 
 481   return offset;
 482 }
 483 
 484 void LIR_Assembler::add_debug_info_for_branch(address adr, CodeEmitInfo* info) {
 485   _masm-&gt;code_section()-&gt;relocate(adr, relocInfo::poll_type);
 486   int pc_offset = code_offset();
 487   flush_debug_info(pc_offset);
 488   info-&gt;record_debug_info(compilation()-&gt;debug_info_recorder(), pc_offset);
 489   if (info-&gt;exception_handlers() != NULL) {
 490     compilation()-&gt;add_exception_handlers_for_pco(pc_offset, info-&gt;exception_handlers());
 491   }
 492 }
 493 
 494 void LIR_Assembler::return_op(LIR_Opr result) {
 495   assert(result-&gt;is_illegal() || !result-&gt;is_single_cpu() || result-&gt;as_register() == r0, &quot;word returns are in r0,&quot;);
 496 
 497   // Pop the stack before the safepoint code
 498   __ remove_frame(initial_frame_size_in_bytes());
 499 
 500   if (StackReservedPages &gt; 0 &amp;&amp; compilation()-&gt;has_reserved_stack_access()) {
 501     __ reserved_stack_check();
 502   }
 503 
 504   address polling_page(os::get_polling_page());
 505   __ read_polling_page(rscratch1, polling_page, relocInfo::poll_return_type);
 506   __ ret(lr);
 507 }
 508 
 509 int LIR_Assembler::safepoint_poll(LIR_Opr tmp, CodeEmitInfo* info) {
 510   address polling_page(os::get_polling_page());
 511   guarantee(info != NULL, &quot;Shouldn&#39;t be NULL&quot;);
 512   assert(os::is_poll_address(polling_page), &quot;should be&quot;);
 513   __ get_polling_page(rscratch1, polling_page, relocInfo::poll_type);
 514   add_debug_info_for_branch(info);  // This isn&#39;t just debug info:
 515                                     // it&#39;s the oop map
 516   __ read_polling_page(rscratch1, relocInfo::poll_type);
 517   return __ offset();
 518 }
 519 
 520 
 521 void LIR_Assembler::move_regs(Register from_reg, Register to_reg) {
 522   if (from_reg == r31_sp)
 523     from_reg = sp;
 524   if (to_reg == r31_sp)
 525     to_reg = sp;
 526   __ mov(to_reg, from_reg);
 527 }
 528 
 529 void LIR_Assembler::swap_reg(Register a, Register b) { Unimplemented(); }
 530 
 531 
 532 void LIR_Assembler::const2reg(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
 533   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 534   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 535   LIR_Const* c = src-&gt;as_constant_ptr();
 536 
 537   switch (c-&gt;type()) {
 538     case T_INT: {
 539       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 540       __ movw(dest-&gt;as_register(), c-&gt;as_jint());
 541       break;
 542     }
 543 
 544     case T_ADDRESS: {
 545       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 546       __ mov(dest-&gt;as_register(), c-&gt;as_jint());
 547       break;
 548     }
 549 
 550     case T_LONG: {
 551       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 552       __ mov(dest-&gt;as_register_lo(), (intptr_t)c-&gt;as_jlong());
 553       break;
 554     }
 555 
 556     case T_OBJECT: {
 557         if (patch_code == lir_patch_none) {
 558           jobject2reg(c-&gt;as_jobject(), dest-&gt;as_register());
 559         } else {
 560           jobject2reg_with_patching(dest-&gt;as_register(), info);
 561         }
 562       break;
 563     }
 564 
 565     case T_METADATA: {
 566       if (patch_code != lir_patch_none) {
 567         klass2reg_with_patching(dest-&gt;as_register(), info);
 568       } else {
 569         __ mov_metadata(dest-&gt;as_register(), c-&gt;as_metadata());
 570       }
 571       break;
 572     }
 573 
 574     case T_FLOAT: {
 575       if (__ operand_valid_for_float_immediate(c-&gt;as_jfloat())) {
 576         __ fmovs(dest-&gt;as_float_reg(), (c-&gt;as_jfloat()));
 577       } else {
 578         __ adr(rscratch1, InternalAddress(float_constant(c-&gt;as_jfloat())));
 579         __ ldrs(dest-&gt;as_float_reg(), Address(rscratch1));
 580       }
 581       break;
 582     }
 583 
 584     case T_DOUBLE: {
 585       if (__ operand_valid_for_float_immediate(c-&gt;as_jdouble())) {
 586         __ fmovd(dest-&gt;as_double_reg(), (c-&gt;as_jdouble()));
 587       } else {
 588         __ adr(rscratch1, InternalAddress(double_constant(c-&gt;as_jdouble())));
 589         __ ldrd(dest-&gt;as_double_reg(), Address(rscratch1));
 590       }
 591       break;
 592     }
 593 
 594     default:
 595       ShouldNotReachHere();
 596   }
 597 }
 598 
 599 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
 600   LIR_Const* c = src-&gt;as_constant_ptr();
 601   switch (c-&gt;type()) {
 602   case T_OBJECT:
 603     {
 604       if (! c-&gt;as_jobject())
 605         __ str(zr, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 606       else {
 607         const2reg(src, FrameMap::rscratch1_opr, lir_patch_none, NULL);
 608         reg2stack(FrameMap::rscratch1_opr, dest, c-&gt;type(), false);
 609       }
 610     }
 611     break;
 612   case T_ADDRESS:
 613     {
 614       const2reg(src, FrameMap::rscratch1_opr, lir_patch_none, NULL);
 615       reg2stack(FrameMap::rscratch1_opr, dest, c-&gt;type(), false);
 616     }
 617   case T_INT:
 618   case T_FLOAT:
 619     {
 620       Register reg = zr;
 621       if (c-&gt;as_jint_bits() == 0)
 622         __ strw(zr, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 623       else {
 624         __ movw(rscratch1, c-&gt;as_jint_bits());
 625         __ strw(rscratch1, frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 626       }
 627     }
 628     break;
 629   case T_LONG:
 630   case T_DOUBLE:
 631     {
 632       Register reg = zr;
 633       if (c-&gt;as_jlong_bits() == 0)
 634         __ str(zr, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 635                                                  lo_word_offset_in_bytes));
 636       else {
 637         __ mov(rscratch1, (intptr_t)c-&gt;as_jlong_bits());
 638         __ str(rscratch1, frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 639                                                         lo_word_offset_in_bytes));
 640       }
 641     }
 642     break;
 643   default:
 644     ShouldNotReachHere();
 645   }
 646 }
 647 
 648 void LIR_Assembler::const2mem(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info, bool wide) {
 649   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 650   LIR_Const* c = src-&gt;as_constant_ptr();
 651   LIR_Address* to_addr = dest-&gt;as_address_ptr();
 652 
 653   void (Assembler::* insn)(Register Rt, const Address &amp;adr);
 654 
 655   switch (type) {
 656   case T_ADDRESS:
 657     assert(c-&gt;as_jint() == 0, &quot;should be&quot;);
 658     insn = &amp;Assembler::str;
 659     break;
 660   case T_LONG:
 661     assert(c-&gt;as_jlong() == 0, &quot;should be&quot;);
 662     insn = &amp;Assembler::str;
 663     break;
 664   case T_INT:
 665     assert(c-&gt;as_jint() == 0, &quot;should be&quot;);
 666     insn = &amp;Assembler::strw;
 667     break;
 668   case T_OBJECT:
 669   case T_ARRAY:
 670     assert(c-&gt;as_jobject() == 0, &quot;should be&quot;);
 671     if (UseCompressedOops &amp;&amp; !wide) {
 672       insn = &amp;Assembler::strw;
 673     } else {
 674       insn = &amp;Assembler::str;
 675     }
 676     break;
 677   case T_CHAR:
 678   case T_SHORT:
 679     assert(c-&gt;as_jint() == 0, &quot;should be&quot;);
 680     insn = &amp;Assembler::strh;
 681     break;
 682   case T_BOOLEAN:
 683   case T_BYTE:
 684     assert(c-&gt;as_jint() == 0, &quot;should be&quot;);
 685     insn = &amp;Assembler::strb;
 686     break;
 687   default:
 688     ShouldNotReachHere();
 689     insn = &amp;Assembler::str;  // unreachable
 690   }
 691 
 692   if (info) add_debug_info_for_null_check_here(info);
 693   (_masm-&gt;*insn)(zr, as_Address(to_addr, rscratch1));
 694 }
 695 
 696 void LIR_Assembler::reg2reg(LIR_Opr src, LIR_Opr dest) {
 697   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 698   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 699 
 700   // move between cpu-registers
 701   if (dest-&gt;is_single_cpu()) {
 702     if (src-&gt;type() == T_LONG) {
 703       // Can do LONG -&gt; OBJECT
 704       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
 705       return;
 706     }
 707     assert(src-&gt;is_single_cpu(), &quot;must match&quot;);
 708     if (src-&gt;type() == T_OBJECT) {
 709       __ verify_oop(src-&gt;as_register());
 710     }
 711     move_regs(src-&gt;as_register(), dest-&gt;as_register());
 712 
 713   } else if (dest-&gt;is_double_cpu()) {
 714     if (is_reference_type(src-&gt;type())) {
 715       // Surprising to me but we can see move of a long to t_object
 716       __ verify_oop(src-&gt;as_register());
 717       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
 718       return;
 719     }
 720     assert(src-&gt;is_double_cpu(), &quot;must match&quot;);
 721     Register f_lo = src-&gt;as_register_lo();
 722     Register f_hi = src-&gt;as_register_hi();
 723     Register t_lo = dest-&gt;as_register_lo();
 724     Register t_hi = dest-&gt;as_register_hi();
 725     assert(f_hi == f_lo, &quot;must be same&quot;);
 726     assert(t_hi == t_lo, &quot;must be same&quot;);
 727     move_regs(f_lo, t_lo);
 728 
 729   } else if (dest-&gt;is_single_fpu()) {
 730     __ fmovs(dest-&gt;as_float_reg(), src-&gt;as_float_reg());
 731 
 732   } else if (dest-&gt;is_double_fpu()) {
 733     __ fmovd(dest-&gt;as_double_reg(), src-&gt;as_double_reg());
 734 
 735   } else {
 736     ShouldNotReachHere();
 737   }
 738 }
 739 
 740 void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {
 741   if (src-&gt;is_single_cpu()) {
 742     if (is_reference_type(type)) {
 743       __ str(src-&gt;as_register(), frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 744       __ verify_oop(src-&gt;as_register());
 745     } else if (type == T_METADATA || type == T_DOUBLE || type == T_ADDRESS) {
 746       __ str(src-&gt;as_register(), frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 747     } else {
 748       __ strw(src-&gt;as_register(), frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
 749     }
 750 
 751   } else if (src-&gt;is_double_cpu()) {
 752     Address dest_addr_LO = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), lo_word_offset_in_bytes);
 753     __ str(src-&gt;as_register_lo(), dest_addr_LO);
 754 
 755   } else if (src-&gt;is_single_fpu()) {
 756     Address dest_addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 757     __ strs(src-&gt;as_float_reg(), dest_addr);
 758 
 759   } else if (src-&gt;is_double_fpu()) {
 760     Address dest_addr = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
 761     __ strd(src-&gt;as_double_reg(), dest_addr);
 762 
 763   } else {
 764     ShouldNotReachHere();
 765   }
 766 
 767 }
 768 
 769 
 770 void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide, bool /* unaligned */) {
 771   LIR_Address* to_addr = dest-&gt;as_address_ptr();
 772   PatchingStub* patch = NULL;
 773   Register compressed_src = rscratch1;
 774 
 775   if (patch_code != lir_patch_none) {
 776     deoptimize_trap(info);
 777     return;
 778   }
 779 
 780   if (is_reference_type(type)) {
 781     __ verify_oop(src-&gt;as_register());
 782 
 783     if (UseCompressedOops &amp;&amp; !wide) {
 784       __ encode_heap_oop(compressed_src, src-&gt;as_register());
 785     } else {
 786       compressed_src = src-&gt;as_register();
 787     }
 788   }
 789 
 790   int null_check_here = code_offset();
 791   switch (type) {
 792     case T_FLOAT: {
 793       __ strs(src-&gt;as_float_reg(), as_Address(to_addr));
 794       break;
 795     }
 796 
 797     case T_DOUBLE: {
 798       __ strd(src-&gt;as_double_reg(), as_Address(to_addr));
 799       break;
 800     }
 801 
 802     case T_ARRAY:   // fall through
 803     case T_OBJECT:  // fall through
 804       if (UseCompressedOops &amp;&amp; !wide) {
 805         __ strw(compressed_src, as_Address(to_addr, rscratch2));
 806       } else {
 807          __ str(compressed_src, as_Address(to_addr));
 808       }
 809       break;
 810     case T_METADATA:
 811       // We get here to store a method pointer to the stack to pass to
 812       // a dtrace runtime call. This can&#39;t work on 64 bit with
 813       // compressed klass ptrs: T_METADATA can be a compressed klass
 814       // ptr or a 64 bit method pointer.
 815       ShouldNotReachHere();
 816       __ str(src-&gt;as_register(), as_Address(to_addr));
 817       break;
 818     case T_ADDRESS:
 819       __ str(src-&gt;as_register(), as_Address(to_addr));
 820       break;
 821     case T_INT:
 822       __ strw(src-&gt;as_register(), as_Address(to_addr));
 823       break;
 824 
 825     case T_LONG: {
 826       __ str(src-&gt;as_register_lo(), as_Address_lo(to_addr));
 827       break;
 828     }
 829 
 830     case T_BYTE:    // fall through
 831     case T_BOOLEAN: {
 832       __ strb(src-&gt;as_register(), as_Address(to_addr));
 833       break;
 834     }
 835 
 836     case T_CHAR:    // fall through
 837     case T_SHORT:
 838       __ strh(src-&gt;as_register(), as_Address(to_addr));
 839       break;
 840 
 841     default:
 842       ShouldNotReachHere();
 843   }
 844   if (info != NULL) {
 845     add_debug_info_for_null_check(null_check_here, info);
 846   }
 847 }
 848 
 849 
 850 void LIR_Assembler::stack2reg(LIR_Opr src, LIR_Opr dest, BasicType type) {
 851   assert(src-&gt;is_stack(), &quot;should not call otherwise&quot;);
 852   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 853 
 854   if (dest-&gt;is_single_cpu()) {
 855     if (is_reference_type(type)) {
 856       __ ldr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 857       __ verify_oop(dest-&gt;as_register());
 858     } else if (type == T_METADATA || type == T_ADDRESS) {
 859       __ ldr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 860     } else {
 861       __ ldrw(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
 862     }
 863 
 864   } else if (dest-&gt;is_double_cpu()) {
 865     Address src_addr_LO = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), lo_word_offset_in_bytes);
 866     __ ldr(dest-&gt;as_register_lo(), src_addr_LO);
 867 
 868   } else if (dest-&gt;is_single_fpu()) {
 869     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
 870     __ ldrs(dest-&gt;as_float_reg(), src_addr);
 871 
 872   } else if (dest-&gt;is_double_fpu()) {
 873     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
 874     __ ldrd(dest-&gt;as_double_reg(), src_addr);
 875 
 876   } else {
 877     ShouldNotReachHere();
 878   }
 879 }
 880 
 881 
 882 void LIR_Assembler::klass2reg_with_patching(Register reg, CodeEmitInfo* info) {
 883   address target = NULL;
 884   relocInfo::relocType reloc_type = relocInfo::none;
 885 
 886   switch (patching_id(info)) {
 887   case PatchingStub::access_field_id:
 888     target = Runtime1::entry_for(Runtime1::access_field_patching_id);
 889     reloc_type = relocInfo::section_word_type;
 890     break;
 891   case PatchingStub::load_klass_id:
 892     target = Runtime1::entry_for(Runtime1::load_klass_patching_id);
 893     reloc_type = relocInfo::metadata_type;
 894     break;
 895   case PatchingStub::load_mirror_id:
 896     target = Runtime1::entry_for(Runtime1::load_mirror_patching_id);
 897     reloc_type = relocInfo::oop_type;
 898     break;
 899   case PatchingStub::load_appendix_id:
 900     target = Runtime1::entry_for(Runtime1::load_appendix_patching_id);
 901     reloc_type = relocInfo::oop_type;
 902     break;
 903   default: ShouldNotReachHere();
 904   }
 905 
 906   __ far_call(RuntimeAddress(target));
 907   add_call_info_here(info);
 908 }
 909 
 910 void LIR_Assembler::stack2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {
 911 
 912   LIR_Opr temp;
 913   if (type == T_LONG || type == T_DOUBLE)
 914     temp = FrameMap::rscratch1_long_opr;
 915   else
 916     temp = FrameMap::rscratch1_opr;
 917 
 918   stack2reg(src, temp, src-&gt;type());
 919   reg2stack(temp, dest, dest-&gt;type(), false);
 920 }
 921 
 922 
 923 void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide, bool /* unaligned */) {
 924   LIR_Address* addr = src-&gt;as_address_ptr();
 925   LIR_Address* from_addr = src-&gt;as_address_ptr();
 926 
 927   if (addr-&gt;base()-&gt;type() == T_OBJECT) {
 928     __ verify_oop(addr-&gt;base()-&gt;as_pointer_register());
 929   }
 930 
 931   if (patch_code != lir_patch_none) {
 932     deoptimize_trap(info);
 933     return;
 934   }
 935 
 936   if (info != NULL) {
 937     add_debug_info_for_null_check_here(info);
 938   }
 939   int null_check_here = code_offset();
 940   switch (type) {
 941     case T_FLOAT: {
 942       __ ldrs(dest-&gt;as_float_reg(), as_Address(from_addr));
 943       break;
 944     }
 945 
 946     case T_DOUBLE: {
 947       __ ldrd(dest-&gt;as_double_reg(), as_Address(from_addr));
 948       break;
 949     }
 950 
 951     case T_ARRAY:   // fall through
 952     case T_OBJECT:  // fall through
 953       if (UseCompressedOops &amp;&amp; !wide) {
 954         __ ldrw(dest-&gt;as_register(), as_Address(from_addr));
 955       } else {
 956          __ ldr(dest-&gt;as_register(), as_Address(from_addr));
 957       }
 958       break;
 959     case T_METADATA:
 960       // We get here to store a method pointer to the stack to pass to
 961       // a dtrace runtime call. This can&#39;t work on 64 bit with
 962       // compressed klass ptrs: T_METADATA can be a compressed klass
 963       // ptr or a 64 bit method pointer.
 964       ShouldNotReachHere();
 965       __ ldr(dest-&gt;as_register(), as_Address(from_addr));
 966       break;
 967     case T_ADDRESS:
 968       // FIXME: OMG this is a horrible kludge.  Any offset from an
 969       // address that matches klass_offset_in_bytes() will be loaded
 970       // as a word, not a long.
 971       if (UseCompressedClassPointers &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
 972         __ ldrw(dest-&gt;as_register(), as_Address(from_addr));
 973       } else {
 974         __ ldr(dest-&gt;as_register(), as_Address(from_addr));
 975       }
 976       break;
 977     case T_INT:
 978       __ ldrw(dest-&gt;as_register(), as_Address(from_addr));
 979       break;
 980 
 981     case T_LONG: {
 982       __ ldr(dest-&gt;as_register_lo(), as_Address_lo(from_addr));
 983       break;
 984     }
 985 
 986     case T_BYTE:
 987       __ ldrsb(dest-&gt;as_register(), as_Address(from_addr));
 988       break;
 989     case T_BOOLEAN: {
 990       __ ldrb(dest-&gt;as_register(), as_Address(from_addr));
 991       break;
 992     }
 993 
 994     case T_CHAR:
 995       __ ldrh(dest-&gt;as_register(), as_Address(from_addr));
 996       break;
 997     case T_SHORT:
 998       __ ldrsh(dest-&gt;as_register(), as_Address(from_addr));
 999       break;
1000 
1001     default:
1002       ShouldNotReachHere();
1003   }
1004 
1005   if (is_reference_type(type)) {
1006     if (UseCompressedOops &amp;&amp; !wide) {
1007       __ decode_heap_oop(dest-&gt;as_register());
1008     }
1009 
1010     if (!UseZGC) {
1011       // Load barrier has not yet been applied, so ZGC can&#39;t verify the oop here
1012       __ verify_oop(dest-&gt;as_register());
1013     }
1014   } else if (type == T_ADDRESS &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1015     if (UseCompressedClassPointers) {
1016       __ decode_klass_not_null(dest-&gt;as_register());
1017     }
1018   }
1019 }
1020 
1021 
1022 int LIR_Assembler::array_element_size(BasicType type) const {
1023   int elem_size = type2aelembytes(type);
1024   return exact_log2(elem_size);
1025 }
1026 
1027 
1028 void LIR_Assembler::emit_op3(LIR_Op3* op) {
1029   switch (op-&gt;code()) {
1030   case lir_idiv:
1031   case lir_irem:
1032     arithmetic_idiv(op-&gt;code(),
1033                     op-&gt;in_opr1(),
1034                     op-&gt;in_opr2(),
1035                     op-&gt;in_opr3(),
1036                     op-&gt;result_opr(),
1037                     op-&gt;info());
1038     break;
1039   case lir_fmad:
1040     __ fmaddd(op-&gt;result_opr()-&gt;as_double_reg(),
1041               op-&gt;in_opr1()-&gt;as_double_reg(),
1042               op-&gt;in_opr2()-&gt;as_double_reg(),
1043               op-&gt;in_opr3()-&gt;as_double_reg());
1044     break;
1045   case lir_fmaf:
1046     __ fmadds(op-&gt;result_opr()-&gt;as_float_reg(),
1047               op-&gt;in_opr1()-&gt;as_float_reg(),
1048               op-&gt;in_opr2()-&gt;as_float_reg(),
1049               op-&gt;in_opr3()-&gt;as_float_reg());
1050     break;
1051   default:      ShouldNotReachHere(); break;
1052   }
1053 }
1054 
1055 void LIR_Assembler::emit_opBranch(LIR_OpBranch* op) {
1056 #ifdef ASSERT
1057   assert(op-&gt;block() == NULL || op-&gt;block()-&gt;label() == op-&gt;label(), &quot;wrong label&quot;);
1058   if (op-&gt;block() != NULL)  _branch_target_blocks.append(op-&gt;block());
1059   if (op-&gt;ublock() != NULL) _branch_target_blocks.append(op-&gt;ublock());
1060 #endif
1061 
1062   if (op-&gt;cond() == lir_cond_always) {
1063     if (op-&gt;info() != NULL) add_debug_info_for_branch(op-&gt;info());
1064     __ b(*(op-&gt;label()));
1065   } else {
1066     Assembler::Condition acond;
1067     if (op-&gt;code() == lir_cond_float_branch) {
1068       bool is_unordered = (op-&gt;ublock() == op-&gt;block());
1069       // Assembler::EQ does not permit unordered branches, so we add
1070       // another branch here.  Likewise, Assembler::NE does not permit
1071       // ordered branches.
1072       if ((is_unordered &amp;&amp; op-&gt;cond() == lir_cond_equal)
1073           || (!is_unordered &amp;&amp; op-&gt;cond() == lir_cond_notEqual))
1074         __ br(Assembler::VS, *(op-&gt;ublock()-&gt;label()));
1075       switch(op-&gt;cond()) {
1076       case lir_cond_equal:        acond = Assembler::EQ; break;
1077       case lir_cond_notEqual:     acond = Assembler::NE; break;
1078       case lir_cond_less:         acond = (is_unordered ? Assembler::LT : Assembler::LO); break;
1079       case lir_cond_lessEqual:    acond = (is_unordered ? Assembler::LE : Assembler::LS); break;
1080       case lir_cond_greaterEqual: acond = (is_unordered ? Assembler::HS : Assembler::GE); break;
1081       case lir_cond_greater:      acond = (is_unordered ? Assembler::HI : Assembler::GT); break;
1082       default:                    ShouldNotReachHere();
1083         acond = Assembler::EQ;  // unreachable
1084       }
1085     } else {
1086       switch (op-&gt;cond()) {
1087         case lir_cond_equal:        acond = Assembler::EQ; break;
1088         case lir_cond_notEqual:     acond = Assembler::NE; break;
1089         case lir_cond_less:         acond = Assembler::LT; break;
1090         case lir_cond_lessEqual:    acond = Assembler::LE; break;
1091         case lir_cond_greaterEqual: acond = Assembler::GE; break;
1092         case lir_cond_greater:      acond = Assembler::GT; break;
1093         case lir_cond_belowEqual:   acond = Assembler::LS; break;
1094         case lir_cond_aboveEqual:   acond = Assembler::HS; break;
1095         default:                    ShouldNotReachHere();
1096           acond = Assembler::EQ;  // unreachable
1097       }
1098     }
1099     __ br(acond,*(op-&gt;label()));
1100   }
1101 }
1102 
1103 
1104 
1105 void LIR_Assembler::emit_opConvert(LIR_OpConvert* op) {
1106   LIR_Opr src  = op-&gt;in_opr();
1107   LIR_Opr dest = op-&gt;result_opr();
1108 
1109   switch (op-&gt;bytecode()) {
1110     case Bytecodes::_i2f:
1111       {
1112         __ scvtfws(dest-&gt;as_float_reg(), src-&gt;as_register());
1113         break;
1114       }
1115     case Bytecodes::_i2d:
1116       {
1117         __ scvtfwd(dest-&gt;as_double_reg(), src-&gt;as_register());
1118         break;
1119       }
1120     case Bytecodes::_l2d:
1121       {
1122         __ scvtfd(dest-&gt;as_double_reg(), src-&gt;as_register_lo());
1123         break;
1124       }
1125     case Bytecodes::_l2f:
1126       {
1127         __ scvtfs(dest-&gt;as_float_reg(), src-&gt;as_register_lo());
1128         break;
1129       }
1130     case Bytecodes::_f2d:
1131       {
1132         __ fcvts(dest-&gt;as_double_reg(), src-&gt;as_float_reg());
1133         break;
1134       }
1135     case Bytecodes::_d2f:
1136       {
1137         __ fcvtd(dest-&gt;as_float_reg(), src-&gt;as_double_reg());
1138         break;
1139       }
1140     case Bytecodes::_i2c:
1141       {
1142         __ ubfx(dest-&gt;as_register(), src-&gt;as_register(), 0, 16);
1143         break;
1144       }
1145     case Bytecodes::_i2l:
1146       {
1147         __ sxtw(dest-&gt;as_register_lo(), src-&gt;as_register());
1148         break;
1149       }
1150     case Bytecodes::_i2s:
1151       {
1152         __ sxth(dest-&gt;as_register(), src-&gt;as_register());
1153         break;
1154       }
1155     case Bytecodes::_i2b:
1156       {
1157         __ sxtb(dest-&gt;as_register(), src-&gt;as_register());
1158         break;
1159       }
1160     case Bytecodes::_l2i:
1161       {
1162         _masm-&gt;block_comment(&quot;FIXME: This could be a no-op&quot;);
1163         __ uxtw(dest-&gt;as_register(), src-&gt;as_register_lo());
1164         break;
1165       }
1166     case Bytecodes::_d2l:
1167       {
1168         __ fcvtzd(dest-&gt;as_register_lo(), src-&gt;as_double_reg());
1169         break;
1170       }
1171     case Bytecodes::_f2i:
1172       {
1173         __ fcvtzsw(dest-&gt;as_register(), src-&gt;as_float_reg());
1174         break;
1175       }
1176     case Bytecodes::_f2l:
1177       {
1178         __ fcvtzs(dest-&gt;as_register_lo(), src-&gt;as_float_reg());
1179         break;
1180       }
1181     case Bytecodes::_d2i:
1182       {
1183         __ fcvtzdw(dest-&gt;as_register(), src-&gt;as_double_reg());
1184         break;
1185       }
1186     default: ShouldNotReachHere();
1187   }
1188 }
1189 
1190 void LIR_Assembler::emit_alloc_obj(LIR_OpAllocObj* op) {
1191   if (op-&gt;init_check()) {
1192     __ ldrb(rscratch1, Address(op-&gt;klass()-&gt;as_register(),
1193                                InstanceKlass::init_state_offset()));
1194     __ cmpw(rscratch1, InstanceKlass::fully_initialized);
1195     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
1196     __ br(Assembler::NE, *op-&gt;stub()-&gt;entry());
1197   }
1198   __ allocate_object(op-&gt;obj()-&gt;as_register(),
1199                      op-&gt;tmp1()-&gt;as_register(),
1200                      op-&gt;tmp2()-&gt;as_register(),
1201                      op-&gt;header_size(),
1202                      op-&gt;object_size(),
1203                      op-&gt;klass()-&gt;as_register(),
1204                      *op-&gt;stub()-&gt;entry());
1205   __ bind(*op-&gt;stub()-&gt;continuation());
1206 }
1207 
1208 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
1209   Register len =  op-&gt;len()-&gt;as_register();
1210   __ uxtw(len, len);
1211 
1212   if (UseSlowPath ||
1213       (!UseFastNewObjectArray &amp;&amp; is_reference_type(op-&gt;type())) ||
1214       (!UseFastNewTypeArray   &amp;&amp; !is_reference_type(op-&gt;type()))) {
1215     __ b(*op-&gt;stub()-&gt;entry());
1216   } else {
1217     Register tmp1 = op-&gt;tmp1()-&gt;as_register();
1218     Register tmp2 = op-&gt;tmp2()-&gt;as_register();
1219     Register tmp3 = op-&gt;tmp3()-&gt;as_register();
1220     if (len == tmp1) {
1221       tmp1 = tmp3;
1222     } else if (len == tmp2) {
1223       tmp2 = tmp3;
1224     } else if (len == tmp3) {
1225       // everything is ok
1226     } else {
1227       __ mov(tmp3, len);
1228     }
1229     __ allocate_array(op-&gt;obj()-&gt;as_register(),
1230                       len,
1231                       tmp1,
1232                       tmp2,
1233                       arrayOopDesc::header_size(op-&gt;type()),
1234                       array_element_size(op-&gt;type()),
1235                       op-&gt;klass()-&gt;as_register(),
1236                       *op-&gt;stub()-&gt;entry());
1237   }
1238   __ bind(*op-&gt;stub()-&gt;continuation());
1239 }
1240 
1241 void LIR_Assembler::type_profile_helper(Register mdo,
1242                                         ciMethodData *md, ciProfileData *data,
1243                                         Register recv, Label* update_done) {
1244   for (uint i = 0; i &lt; ReceiverTypeData::row_limit(); i++) {
1245     Label next_test;
1246     // See if the receiver is receiver[n].
1247     __ lea(rscratch2, Address(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i))));
1248     __ ldr(rscratch1, Address(rscratch2));
1249     __ cmp(recv, rscratch1);
1250     __ br(Assembler::NE, next_test);
1251     Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)));
1252     __ addptr(data_addr, DataLayout::counter_increment);
1253     __ b(*update_done);
1254     __ bind(next_test);
1255   }
1256 
1257   // Didn&#39;t find receiver; find next empty slot and fill it in
1258   for (uint i = 0; i &lt; ReceiverTypeData::row_limit(); i++) {
1259     Label next_test;
1260     __ lea(rscratch2,
1261            Address(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i))));
1262     Address recv_addr(rscratch2);
1263     __ ldr(rscratch1, recv_addr);
1264     __ cbnz(rscratch1, next_test);
1265     __ str(recv, recv_addr);
1266     __ mov(rscratch1, DataLayout::counter_increment);
1267     __ lea(rscratch2, Address(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i))));
1268     __ str(rscratch1, Address(rscratch2));
1269     __ b(*update_done);
1270     __ bind(next_test);
1271   }
1272 }
1273 
1274 void LIR_Assembler::emit_typecheck_helper(LIR_OpTypeCheck *op, Label* success, Label* failure, Label* obj_is_null) {
1275   // we always need a stub for the failure case.
1276   CodeStub* stub = op-&gt;stub();
1277   Register obj = op-&gt;object()-&gt;as_register();
1278   Register k_RInfo = op-&gt;tmp1()-&gt;as_register();
1279   Register klass_RInfo = op-&gt;tmp2()-&gt;as_register();
1280   Register dst = op-&gt;result_opr()-&gt;as_register();
1281   ciKlass* k = op-&gt;klass();
1282   Register Rtmp1 = noreg;
1283 
1284   // check if it needs to be profiled
1285   ciMethodData* md;
1286   ciProfileData* data;
1287 
1288   const bool should_profile = op-&gt;should_profile();
1289 
1290   if (should_profile) {
1291     ciMethod* method = op-&gt;profiled_method();
1292     assert(method != NULL, &quot;Should have method&quot;);
1293     int bci = op-&gt;profiled_bci();
1294     md = method-&gt;method_data_or_null();
1295     assert(md != NULL, &quot;Sanity&quot;);
1296     data = md-&gt;bci_to_data(bci);
1297     assert(data != NULL,                &quot;need data for type check&quot;);
1298     assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
1299   }
1300   Label profile_cast_success, profile_cast_failure;
1301   Label *success_target = should_profile ? &amp;profile_cast_success : success;
1302   Label *failure_target = should_profile ? &amp;profile_cast_failure : failure;
1303 
1304   if (obj == k_RInfo) {
1305     k_RInfo = dst;
1306   } else if (obj == klass_RInfo) {
1307     klass_RInfo = dst;
1308   }
1309   if (k-&gt;is_loaded() &amp;&amp; !UseCompressedClassPointers) {
1310     select_different_registers(obj, dst, k_RInfo, klass_RInfo);
1311   } else {
1312     Rtmp1 = op-&gt;tmp3()-&gt;as_register();
1313     select_different_registers(obj, dst, k_RInfo, klass_RInfo, Rtmp1);
1314   }
1315 
1316   assert_different_registers(obj, k_RInfo, klass_RInfo);
1317 
1318     if (should_profile) {
1319       Label not_null;
1320       __ cbnz(obj, not_null);
1321       // Object is null; update MDO and exit
1322       Register mdo  = klass_RInfo;
1323       __ mov_metadata(mdo, md-&gt;constant_encoding());
1324       Address data_addr
1325         = __ form_address(rscratch2, mdo,
1326                           md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()),
1327                           0);
1328       __ ldrb(rscratch1, data_addr);
1329       __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());
1330       __ strb(rscratch1, data_addr);
1331       __ b(*obj_is_null);
1332       __ bind(not_null);
1333     } else {
1334       __ cbz(obj, *obj_is_null);
1335     }
1336 
1337   if (!k-&gt;is_loaded()) {
1338     klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
1339   } else {
1340     __ mov_metadata(k_RInfo, k-&gt;constant_encoding());
1341   }
1342   __ verify_oop(obj);
1343 
1344   if (op-&gt;fast_check()) {
1345     // get object class
1346     // not a safepoint as obj null check happens earlier
1347     __ load_klass(rscratch1, obj);
1348     __ cmp( rscratch1, k_RInfo);
1349 
1350     __ br(Assembler::NE, *failure_target);
1351     // successful cast, fall through to profile or jump
1352   } else {
1353     // get object class
1354     // not a safepoint as obj null check happens earlier
1355     __ load_klass(klass_RInfo, obj);
1356     if (k-&gt;is_loaded()) {
1357       // See if we get an immediate positive hit
1358       __ ldr(rscratch1, Address(klass_RInfo, long(k-&gt;super_check_offset())));
1359       __ cmp(k_RInfo, rscratch1);
1360       if ((juint)in_bytes(Klass::secondary_super_cache_offset()) != k-&gt;super_check_offset()) {
1361         __ br(Assembler::NE, *failure_target);
1362         // successful cast, fall through to profile or jump
1363       } else {
1364         // See if we get an immediate positive hit
1365         __ br(Assembler::EQ, *success_target);
1366         // check for self
1367         __ cmp(klass_RInfo, k_RInfo);
1368         __ br(Assembler::EQ, *success_target);
1369 
1370         __ stp(klass_RInfo, k_RInfo, Address(__ pre(sp, -2 * wordSize)));
1371         __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
1372         __ ldr(klass_RInfo, Address(__ post(sp, 2 * wordSize)));
1373         // result is a boolean
1374         __ cbzw(klass_RInfo, *failure_target);
1375         // successful cast, fall through to profile or jump
1376       }
1377     } else {
1378       // perform the fast part of the checking logic
1379       __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);
1380       // call out-of-line instance of __ check_klass_subtype_slow_path(...):
1381       __ stp(klass_RInfo, k_RInfo, Address(__ pre(sp, -2 * wordSize)));
1382       __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
1383       __ ldp(k_RInfo, klass_RInfo, Address(__ post(sp, 2 * wordSize)));
1384       // result is a boolean
1385       __ cbz(k_RInfo, *failure_target);
1386       // successful cast, fall through to profile or jump
1387     }
1388   }
1389   if (should_profile) {
1390     Register mdo  = klass_RInfo, recv = k_RInfo;
1391     __ bind(profile_cast_success);
1392     __ mov_metadata(mdo, md-&gt;constant_encoding());
1393     __ load_klass(recv, obj);
1394     Label update_done;
1395     type_profile_helper(mdo, md, data, recv, success);
1396     __ b(*success);
1397 
1398     __ bind(profile_cast_failure);
1399     __ mov_metadata(mdo, md-&gt;constant_encoding());
1400     Address counter_addr
1401       = __ form_address(rscratch2, mdo,
1402                         md-&gt;byte_offset_of_slot(data, CounterData::count_offset()),
1403                         0);
1404     __ ldr(rscratch1, counter_addr);
1405     __ sub(rscratch1, rscratch1, DataLayout::counter_increment);
1406     __ str(rscratch1, counter_addr);
1407     __ b(*failure);
1408   }
1409   __ b(*success);
1410 }
1411 
1412 
1413 void LIR_Assembler::emit_opTypeCheck(LIR_OpTypeCheck* op) {
1414   const bool should_profile = op-&gt;should_profile();
1415 
1416   LIR_Code code = op-&gt;code();
1417   if (code == lir_store_check) {
1418     Register value = op-&gt;object()-&gt;as_register();
1419     Register array = op-&gt;array()-&gt;as_register();
1420     Register k_RInfo = op-&gt;tmp1()-&gt;as_register();
1421     Register klass_RInfo = op-&gt;tmp2()-&gt;as_register();
1422     Register Rtmp1 = op-&gt;tmp3()-&gt;as_register();
1423 
1424     CodeStub* stub = op-&gt;stub();
1425 
1426     // check if it needs to be profiled
1427     ciMethodData* md;
1428     ciProfileData* data;
1429 
1430     if (should_profile) {
1431       ciMethod* method = op-&gt;profiled_method();
1432       assert(method != NULL, &quot;Should have method&quot;);
1433       int bci = op-&gt;profiled_bci();
1434       md = method-&gt;method_data_or_null();
1435       assert(md != NULL, &quot;Sanity&quot;);
1436       data = md-&gt;bci_to_data(bci);
1437       assert(data != NULL,                &quot;need data for type check&quot;);
1438       assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
1439     }
1440     Label profile_cast_success, profile_cast_failure, done;
1441     Label *success_target = should_profile ? &amp;profile_cast_success : &amp;done;
1442     Label *failure_target = should_profile ? &amp;profile_cast_failure : stub-&gt;entry();
1443 
1444     if (should_profile) {
1445       Label not_null;
1446       __ cbnz(value, not_null);
1447       // Object is null; update MDO and exit
1448       Register mdo  = klass_RInfo;
1449       __ mov_metadata(mdo, md-&gt;constant_encoding());
1450       Address data_addr
1451         = __ form_address(rscratch2, mdo,
1452                           md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()),
1453                           0);
1454       __ ldrb(rscratch1, data_addr);
1455       __ orr(rscratch1, rscratch1, BitData::null_seen_byte_constant());
1456       __ strb(rscratch1, data_addr);
1457       __ b(done);
1458       __ bind(not_null);
1459     } else {
1460       __ cbz(value, done);
1461     }
1462 
1463     add_debug_info_for_null_check_here(op-&gt;info_for_exception());
1464     __ load_klass(k_RInfo, array);
1465     __ load_klass(klass_RInfo, value);
1466 
1467     // get instance klass (it&#39;s already uncompressed)
1468     __ ldr(k_RInfo, Address(k_RInfo, ObjArrayKlass::element_klass_offset()));
1469     // perform the fast part of the checking logic
1470     __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);
1471     // call out-of-line instance of __ check_klass_subtype_slow_path(...):
1472     __ stp(klass_RInfo, k_RInfo, Address(__ pre(sp, -2 * wordSize)));
1473     __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
1474     __ ldp(k_RInfo, klass_RInfo, Address(__ post(sp, 2 * wordSize)));
1475     // result is a boolean
1476     __ cbzw(k_RInfo, *failure_target);
1477     // fall through to the success case
1478 
1479     if (should_profile) {
1480       Register mdo  = klass_RInfo, recv = k_RInfo;
1481       __ bind(profile_cast_success);
1482       __ mov_metadata(mdo, md-&gt;constant_encoding());
1483       __ load_klass(recv, value);
1484       Label update_done;
1485       type_profile_helper(mdo, md, data, recv, &amp;done);
1486       __ b(done);
1487 
1488       __ bind(profile_cast_failure);
1489       __ mov_metadata(mdo, md-&gt;constant_encoding());
1490       Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()));
1491       __ lea(rscratch2, counter_addr);
1492       __ ldr(rscratch1, Address(rscratch2));
1493       __ sub(rscratch1, rscratch1, DataLayout::counter_increment);
1494       __ str(rscratch1, Address(rscratch2));
1495       __ b(*stub-&gt;entry());
1496     }
1497 
1498     __ bind(done);
1499   } else if (code == lir_checkcast) {
1500     Register obj = op-&gt;object()-&gt;as_register();
1501     Register dst = op-&gt;result_opr()-&gt;as_register();
1502     Label success;
1503     emit_typecheck_helper(op, &amp;success, op-&gt;stub()-&gt;entry(), &amp;success);
1504     __ bind(success);
1505     if (dst != obj) {
1506       __ mov(dst, obj);
1507     }
1508   } else if (code == lir_instanceof) {
1509     Register obj = op-&gt;object()-&gt;as_register();
1510     Register dst = op-&gt;result_opr()-&gt;as_register();
1511     Label success, failure, done;
1512     emit_typecheck_helper(op, &amp;success, &amp;failure, &amp;failure);
1513     __ bind(failure);
1514     __ mov(dst, zr);
1515     __ b(done);
1516     __ bind(success);
1517     __ mov(dst, 1);
1518     __ bind(done);
1519   } else {
1520     ShouldNotReachHere();
1521   }
1522 }
1523 
1524 void LIR_Assembler::casw(Register addr, Register newval, Register cmpval) {
1525   __ cmpxchg(addr, cmpval, newval, Assembler::word, /* acquire*/ true, /* release*/ true, /* weak*/ false, rscratch1);
1526   __ cset(rscratch1, Assembler::NE);
1527   __ membar(__ AnyAny);
1528 }
1529 
1530 void LIR_Assembler::casl(Register addr, Register newval, Register cmpval) {
1531   __ cmpxchg(addr, cmpval, newval, Assembler::xword, /* acquire*/ true, /* release*/ true, /* weak*/ false, rscratch1);
1532   __ cset(rscratch1, Assembler::NE);
1533   __ membar(__ AnyAny);
1534 }
1535 
1536 
1537 void LIR_Assembler::emit_compare_and_swap(LIR_OpCompareAndSwap* op) {
1538   assert(VM_Version::supports_cx8(), &quot;wrong machine&quot;);
1539   Register addr;
1540   if (op-&gt;addr()-&gt;is_register()) {
1541     addr = as_reg(op-&gt;addr());
1542   } else {
1543     assert(op-&gt;addr()-&gt;is_address(), &quot;what else?&quot;);
1544     LIR_Address* addr_ptr = op-&gt;addr()-&gt;as_address_ptr();
1545     assert(addr_ptr-&gt;disp() == 0, &quot;need 0 disp&quot;);
1546     assert(addr_ptr-&gt;index() == LIR_OprDesc::illegalOpr(), &quot;need 0 index&quot;);
1547     addr = as_reg(addr_ptr-&gt;base());
1548   }
1549   Register newval = as_reg(op-&gt;new_value());
1550   Register cmpval = as_reg(op-&gt;cmp_value());
1551 
1552   if (op-&gt;code() == lir_cas_obj) {
1553     if (UseCompressedOops) {
1554       Register t1 = op-&gt;tmp1()-&gt;as_register();
1555       assert(op-&gt;tmp1()-&gt;is_valid(), &quot;must be&quot;);
1556       __ encode_heap_oop(t1, cmpval);
1557       cmpval = t1;
1558       __ encode_heap_oop(rscratch2, newval);
1559       newval = rscratch2;
1560       casw(addr, newval, cmpval);
1561     } else {
1562       casl(addr, newval, cmpval);
1563     }
1564   } else if (op-&gt;code() == lir_cas_int) {
1565     casw(addr, newval, cmpval);
1566   } else {
1567     casl(addr, newval, cmpval);
1568   }
1569 }
1570 
1571 
1572 void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {
1573 
1574   Assembler::Condition acond, ncond;
1575   switch (condition) {
1576   case lir_cond_equal:        acond = Assembler::EQ; ncond = Assembler::NE; break;
1577   case lir_cond_notEqual:     acond = Assembler::NE; ncond = Assembler::EQ; break;
1578   case lir_cond_less:         acond = Assembler::LT; ncond = Assembler::GE; break;
1579   case lir_cond_lessEqual:    acond = Assembler::LE; ncond = Assembler::GT; break;
1580   case lir_cond_greaterEqual: acond = Assembler::GE; ncond = Assembler::LT; break;
1581   case lir_cond_greater:      acond = Assembler::GT; ncond = Assembler::LE; break;
1582   case lir_cond_belowEqual:
1583   case lir_cond_aboveEqual:
1584   default:                    ShouldNotReachHere();
1585     acond = Assembler::EQ; ncond = Assembler::NE;  // unreachable
1586   }
1587 
1588   assert(result-&gt;is_single_cpu() || result-&gt;is_double_cpu(),
1589          &quot;expect single register for result&quot;);
1590   if (opr1-&gt;is_constant() &amp;&amp; opr2-&gt;is_constant()
1591       &amp;&amp; opr1-&gt;type() == T_INT &amp;&amp; opr2-&gt;type() == T_INT) {
1592     jint val1 = opr1-&gt;as_jint();
1593     jint val2 = opr2-&gt;as_jint();
1594     if (val1 == 0 &amp;&amp; val2 == 1) {
1595       __ cset(result-&gt;as_register(), ncond);
1596       return;
1597     } else if (val1 == 1 &amp;&amp; val2 == 0) {
1598       __ cset(result-&gt;as_register(), acond);
1599       return;
1600     }
1601   }
1602 
1603   if (opr1-&gt;is_constant() &amp;&amp; opr2-&gt;is_constant()
1604       &amp;&amp; opr1-&gt;type() == T_LONG &amp;&amp; opr2-&gt;type() == T_LONG) {
1605     jlong val1 = opr1-&gt;as_jlong();
1606     jlong val2 = opr2-&gt;as_jlong();
1607     if (val1 == 0 &amp;&amp; val2 == 1) {
1608       __ cset(result-&gt;as_register_lo(), ncond);
1609       return;
1610     } else if (val1 == 1 &amp;&amp; val2 == 0) {
1611       __ cset(result-&gt;as_register_lo(), acond);
1612       return;
1613     }
1614   }
1615 
1616   if (opr1-&gt;is_stack()) {
1617     stack2reg(opr1, FrameMap::rscratch1_opr, result-&gt;type());
1618     opr1 = FrameMap::rscratch1_opr;
1619   } else if (opr1-&gt;is_constant()) {
1620     LIR_Opr tmp
1621       = opr1-&gt;type() == T_LONG ? FrameMap::rscratch1_long_opr : FrameMap::rscratch1_opr;
1622     const2reg(opr1, tmp, lir_patch_none, NULL);
1623     opr1 = tmp;
1624   }
1625 
1626   if (opr2-&gt;is_stack()) {
1627     stack2reg(opr2, FrameMap::rscratch2_opr, result-&gt;type());
1628     opr2 = FrameMap::rscratch2_opr;
1629   } else if (opr2-&gt;is_constant()) {
1630     LIR_Opr tmp
1631       = opr2-&gt;type() == T_LONG ? FrameMap::rscratch2_long_opr : FrameMap::rscratch2_opr;
1632     const2reg(opr2, tmp, lir_patch_none, NULL);
1633     opr2 = tmp;
1634   }
1635 
1636   if (result-&gt;type() == T_LONG)
1637     __ csel(result-&gt;as_register_lo(), opr1-&gt;as_register_lo(), opr2-&gt;as_register_lo(), acond);
1638   else
1639     __ csel(result-&gt;as_register(), opr1-&gt;as_register(), opr2-&gt;as_register(), acond);
1640 }
1641 
1642 void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info, bool pop_fpu_stack) {
1643   assert(info == NULL, &quot;should never be used, idiv/irem and ldiv/lrem not handled by this method&quot;);
1644 
1645   if (left-&gt;is_single_cpu()) {
1646     Register lreg = left-&gt;as_register();
1647     Register dreg = as_reg(dest);
1648 
1649     if (right-&gt;is_single_cpu()) {
1650       // cpu register - cpu register
1651 
1652       assert(left-&gt;type() == T_INT &amp;&amp; right-&gt;type() == T_INT &amp;&amp; dest-&gt;type() == T_INT,
1653              &quot;should be&quot;);
1654       Register rreg = right-&gt;as_register();
1655       switch (code) {
1656       case lir_add: __ addw (dest-&gt;as_register(), lreg, rreg); break;
1657       case lir_sub: __ subw (dest-&gt;as_register(), lreg, rreg); break;
1658       case lir_mul: __ mulw (dest-&gt;as_register(), lreg, rreg); break;
1659       default:      ShouldNotReachHere();
1660       }
1661 
1662     } else if (right-&gt;is_double_cpu()) {
1663       Register rreg = right-&gt;as_register_lo();
1664       // single_cpu + double_cpu: can happen with obj+long
1665       assert(code == lir_add || code == lir_sub, &quot;mismatched arithmetic op&quot;);
1666       switch (code) {
1667       case lir_add: __ add(dreg, lreg, rreg); break;
1668       case lir_sub: __ sub(dreg, lreg, rreg); break;
1669       default: ShouldNotReachHere();
1670       }
1671     } else if (right-&gt;is_constant()) {
1672       // cpu register - constant
1673       jlong c;
1674 
1675       // FIXME.  This is fugly: we really need to factor all this logic.
1676       switch(right-&gt;type()) {
1677       case T_LONG:
1678         c = right-&gt;as_constant_ptr()-&gt;as_jlong();
1679         break;
1680       case T_INT:
1681       case T_ADDRESS:
1682         c = right-&gt;as_constant_ptr()-&gt;as_jint();
1683         break;
1684       default:
1685         ShouldNotReachHere();
1686         c = 0;  // unreachable
1687         break;
1688       }
1689 
1690       assert(code == lir_add || code == lir_sub, &quot;mismatched arithmetic op&quot;);
1691       if (c == 0 &amp;&amp; dreg == lreg) {
1692         COMMENT(&quot;effective nop elided&quot;);
1693         return;
1694       }
1695       switch(left-&gt;type()) {
1696       case T_INT:
1697         switch (code) {
1698         case lir_add: __ addw(dreg, lreg, c); break;
1699         case lir_sub: __ subw(dreg, lreg, c); break;
1700         default: ShouldNotReachHere();
1701         }
1702         break;
1703       case T_OBJECT:
1704       case T_ADDRESS:
1705         switch (code) {
1706         case lir_add: __ add(dreg, lreg, c); break;
1707         case lir_sub: __ sub(dreg, lreg, c); break;
1708         default: ShouldNotReachHere();
1709         }
1710         break;
1711       default:
1712         ShouldNotReachHere();
1713       }
1714     } else {
1715       ShouldNotReachHere();
1716     }
1717 
1718   } else if (left-&gt;is_double_cpu()) {
1719     Register lreg_lo = left-&gt;as_register_lo();
1720 
1721     if (right-&gt;is_double_cpu()) {
1722       // cpu register - cpu register
1723       Register rreg_lo = right-&gt;as_register_lo();
1724       switch (code) {
1725       case lir_add: __ add (dest-&gt;as_register_lo(), lreg_lo, rreg_lo); break;
1726       case lir_sub: __ sub (dest-&gt;as_register_lo(), lreg_lo, rreg_lo); break;
1727       case lir_mul: __ mul (dest-&gt;as_register_lo(), lreg_lo, rreg_lo); break;
1728       case lir_div: __ corrected_idivq(dest-&gt;as_register_lo(), lreg_lo, rreg_lo, false, rscratch1); break;
1729       case lir_rem: __ corrected_idivq(dest-&gt;as_register_lo(), lreg_lo, rreg_lo, true, rscratch1); break;
1730       default:
1731         ShouldNotReachHere();
1732       }
1733 
1734     } else if (right-&gt;is_constant()) {
1735       jlong c = right-&gt;as_constant_ptr()-&gt;as_jlong();
1736       Register dreg = as_reg(dest);
1737       switch (code) {
1738         case lir_add:
1739         case lir_sub:
1740           if (c == 0 &amp;&amp; dreg == lreg_lo) {
1741             COMMENT(&quot;effective nop elided&quot;);
1742             return;
1743           }
1744           code == lir_add ? __ add(dreg, lreg_lo, c) : __ sub(dreg, lreg_lo, c);
1745           break;
1746         case lir_div:
1747           assert(c &gt; 0 &amp;&amp; is_power_of_2(c), &quot;divisor must be power-of-2 constant&quot;);
1748           if (c == 1) {
1749             // move lreg_lo to dreg if divisor is 1
1750             __ mov(dreg, lreg_lo);
1751           } else {
1752             unsigned int shift = exact_log2_long(c);
1753             // use rscratch1 as intermediate result register
1754             __ asr(rscratch1, lreg_lo, 63);
1755             __ add(rscratch1, lreg_lo, rscratch1, Assembler::LSR, 64 - shift);
1756             __ asr(dreg, rscratch1, shift);
1757           }
1758           break;
1759         case lir_rem:
1760           assert(c &gt; 0 &amp;&amp; is_power_of_2(c), &quot;divisor must be power-of-2 constant&quot;);
1761           if (c == 1) {
1762             // move 0 to dreg if divisor is 1
1763             __ mov(dreg, zr);
1764           } else {
1765             // use rscratch1 as intermediate result register
1766             __ negs(rscratch1, lreg_lo);
1767             __ andr(dreg, lreg_lo, c - 1);
1768             __ andr(rscratch1, rscratch1, c - 1);
1769             __ csneg(dreg, dreg, rscratch1, Assembler::MI);
1770           }
1771           break;
1772         default:
1773           ShouldNotReachHere();
1774       }
1775     } else {
1776       ShouldNotReachHere();
1777     }
1778   } else if (left-&gt;is_single_fpu()) {
1779     assert(right-&gt;is_single_fpu(), &quot;right hand side of float arithmetics needs to be float register&quot;);
1780     switch (code) {
1781     case lir_add: __ fadds (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
1782     case lir_sub: __ fsubs (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
1783     case lir_mul_strictfp: // fall through
1784     case lir_mul: __ fmuls (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
1785     case lir_div_strictfp: // fall through
1786     case lir_div: __ fdivs (dest-&gt;as_float_reg(), left-&gt;as_float_reg(), right-&gt;as_float_reg()); break;
1787     default:
1788       ShouldNotReachHere();
1789     }
1790   } else if (left-&gt;is_double_fpu()) {
1791     if (right-&gt;is_double_fpu()) {
1792       // fpu register - fpu register
1793       switch (code) {
1794       case lir_add: __ faddd (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
1795       case lir_sub: __ fsubd (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
1796       case lir_mul_strictfp: // fall through
1797       case lir_mul: __ fmuld (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
1798       case lir_div_strictfp: // fall through
1799       case lir_div: __ fdivd (dest-&gt;as_double_reg(), left-&gt;as_double_reg(), right-&gt;as_double_reg()); break;
1800       default:
1801         ShouldNotReachHere();
1802       }
1803     } else {
1804       if (right-&gt;is_constant()) {
1805         ShouldNotReachHere();
1806       }
1807       ShouldNotReachHere();
1808     }
1809   } else if (left-&gt;is_single_stack() || left-&gt;is_address()) {
1810     assert(left == dest, &quot;left and dest must be equal&quot;);
1811     ShouldNotReachHere();
1812   } else {
1813     ShouldNotReachHere();
1814   }
1815 }
1816 
1817 void LIR_Assembler::arith_fpu_implementation(LIR_Code code, int left_index, int right_index, int dest_index, bool pop_fpu_stack) { Unimplemented(); }
1818 
1819 
1820 void LIR_Assembler::intrinsic_op(LIR_Code code, LIR_Opr value, LIR_Opr unused, LIR_Opr dest, LIR_Op* op) {
1821   switch(code) {
1822   case lir_abs : __ fabsd(dest-&gt;as_double_reg(), value-&gt;as_double_reg()); break;
1823   case lir_sqrt: __ fsqrtd(dest-&gt;as_double_reg(), value-&gt;as_double_reg()); break;
1824   default      : ShouldNotReachHere();
1825   }
1826 }
1827 
1828 void LIR_Assembler::logic_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst) {
1829 
1830   assert(left-&gt;is_single_cpu() || left-&gt;is_double_cpu(), &quot;expect single or double register&quot;);
1831   Register Rleft = left-&gt;is_single_cpu() ? left-&gt;as_register() :
1832                                            left-&gt;as_register_lo();
1833    if (dst-&gt;is_single_cpu()) {
1834      Register Rdst = dst-&gt;as_register();
1835      if (right-&gt;is_constant()) {
1836        switch (code) {
1837          case lir_logic_and: __ andw (Rdst, Rleft, right-&gt;as_jint()); break;
1838          case lir_logic_or:  __ orrw (Rdst, Rleft, right-&gt;as_jint()); break;
1839          case lir_logic_xor: __ eorw (Rdst, Rleft, right-&gt;as_jint()); break;
1840          default: ShouldNotReachHere(); break;
1841        }
1842      } else {
1843        Register Rright = right-&gt;is_single_cpu() ? right-&gt;as_register() :
1844                                                   right-&gt;as_register_lo();
1845        switch (code) {
1846          case lir_logic_and: __ andw (Rdst, Rleft, Rright); break;
1847          case lir_logic_or:  __ orrw (Rdst, Rleft, Rright); break;
1848          case lir_logic_xor: __ eorw (Rdst, Rleft, Rright); break;
1849          default: ShouldNotReachHere(); break;
1850        }
1851      }
1852    } else {
1853      Register Rdst = dst-&gt;as_register_lo();
1854      if (right-&gt;is_constant()) {
1855        switch (code) {
1856          case lir_logic_and: __ andr (Rdst, Rleft, right-&gt;as_jlong()); break;
1857          case lir_logic_or:  __ orr (Rdst, Rleft, right-&gt;as_jlong()); break;
1858          case lir_logic_xor: __ eor (Rdst, Rleft, right-&gt;as_jlong()); break;
1859          default: ShouldNotReachHere(); break;
1860        }
1861      } else {
1862        Register Rright = right-&gt;is_single_cpu() ? right-&gt;as_register() :
1863                                                   right-&gt;as_register_lo();
1864        switch (code) {
1865          case lir_logic_and: __ andr (Rdst, Rleft, Rright); break;
1866          case lir_logic_or:  __ orr (Rdst, Rleft, Rright); break;
1867          case lir_logic_xor: __ eor (Rdst, Rleft, Rright); break;
1868          default: ShouldNotReachHere(); break;
1869        }
1870      }
1871    }
1872 }
1873 
1874 
1875 
1876 void LIR_Assembler::arithmetic_idiv(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr illegal, LIR_Opr result, CodeEmitInfo* info) {
1877 
1878   // opcode check
1879   assert((code == lir_idiv) || (code == lir_irem), &quot;opcode must be idiv or irem&quot;);
1880   bool is_irem = (code == lir_irem);
1881 
1882   // operand check
1883   assert(left-&gt;is_single_cpu(),   &quot;left must be register&quot;);
1884   assert(right-&gt;is_single_cpu() || right-&gt;is_constant(),  &quot;right must be register or constant&quot;);
1885   assert(result-&gt;is_single_cpu(), &quot;result must be register&quot;);
1886   Register lreg = left-&gt;as_register();
1887   Register dreg = result-&gt;as_register();
1888 
1889   // power-of-2 constant check and codegen
1890   if (right-&gt;is_constant()) {
1891     int c = right-&gt;as_constant_ptr()-&gt;as_jint();
1892     assert(c &gt; 0 &amp;&amp; is_power_of_2(c), &quot;divisor must be power-of-2 constant&quot;);
1893     if (is_irem) {
1894       if (c == 1) {
1895         // move 0 to dreg if divisor is 1
1896         __ movw(dreg, zr);
1897       } else {
1898         // use rscratch1 as intermediate result register
1899         __ negsw(rscratch1, lreg);
1900         __ andw(dreg, lreg, c - 1);
1901         __ andw(rscratch1, rscratch1, c - 1);
1902         __ csnegw(dreg, dreg, rscratch1, Assembler::MI);
1903       }
1904     } else {
1905       if (c == 1) {
1906         // move lreg to dreg if divisor is 1
1907         __ movw(dreg, lreg);
1908       } else {
1909         unsigned int shift = exact_log2(c);
1910         // use rscratch1 as intermediate result register
1911         __ asrw(rscratch1, lreg, 31);
1912         __ addw(rscratch1, lreg, rscratch1, Assembler::LSR, 32 - shift);
1913         __ asrw(dreg, rscratch1, shift);
1914       }
1915     }
1916   } else {
1917     Register rreg = right-&gt;as_register();
1918     __ corrected_idivl(dreg, lreg, rreg, is_irem, rscratch1);
1919   }
1920 }
1921 
1922 
1923 void LIR_Assembler::comp_op(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Op2* op) {
1924   if (opr1-&gt;is_constant() &amp;&amp; opr2-&gt;is_single_cpu()) {
1925     // tableswitch
1926     Register reg = as_reg(opr2);
1927     struct tableswitch &amp;table = switches[opr1-&gt;as_constant_ptr()-&gt;as_jint()];
1928     __ tableswitch(reg, table._first_key, table._last_key, table._branches, table._after);
1929   } else if (opr1-&gt;is_single_cpu() || opr1-&gt;is_double_cpu()) {
1930     Register reg1 = as_reg(opr1);
1931     if (opr2-&gt;is_single_cpu()) {
1932       // cpu register - cpu register
1933       Register reg2 = opr2-&gt;as_register();
1934       if (is_reference_type(opr1-&gt;type())) {
1935         __ cmpoop(reg1, reg2);
1936       } else {
1937         assert(!is_reference_type(opr2-&gt;type()), &quot;cmp int, oop?&quot;);
1938         __ cmpw(reg1, reg2);
1939       }
1940       return;
1941     }
1942     if (opr2-&gt;is_double_cpu()) {
1943       // cpu register - cpu register
1944       Register reg2 = opr2-&gt;as_register_lo();
1945       __ cmp(reg1, reg2);
1946       return;
1947     }
1948 
1949     if (opr2-&gt;is_constant()) {
1950       bool is_32bit = false; // width of register operand
1951       jlong imm;
1952 
1953       switch(opr2-&gt;type()) {
1954       case T_INT:
1955         imm = opr2-&gt;as_constant_ptr()-&gt;as_jint();
1956         is_32bit = true;
1957         break;
1958       case T_LONG:
1959         imm = opr2-&gt;as_constant_ptr()-&gt;as_jlong();
1960         break;
1961       case T_ADDRESS:
1962         imm = opr2-&gt;as_constant_ptr()-&gt;as_jint();
1963         break;
1964       case T_METADATA:
1965         imm = (intptr_t)(opr2-&gt;as_constant_ptr()-&gt;as_metadata());
1966         break;
1967       case T_OBJECT:
1968       case T_ARRAY:
1969         jobject2reg(opr2-&gt;as_constant_ptr()-&gt;as_jobject(), rscratch1);
1970         __ cmpoop(reg1, rscratch1);
1971         return;
1972       default:
1973         ShouldNotReachHere();
1974         imm = 0;  // unreachable
1975         break;
1976       }
1977 
1978       if (Assembler::operand_valid_for_add_sub_immediate(imm)) {
1979         if (is_32bit)
1980           __ cmpw(reg1, imm);
1981         else
1982           __ subs(zr, reg1, imm);
1983         return;
1984       } else {
1985         __ mov(rscratch1, imm);
1986         if (is_32bit)
1987           __ cmpw(reg1, rscratch1);
1988         else
1989           __ cmp(reg1, rscratch1);
1990         return;
1991       }
1992     } else
1993       ShouldNotReachHere();
1994   } else if (opr1-&gt;is_single_fpu()) {
1995     FloatRegister reg1 = opr1-&gt;as_float_reg();
1996     assert(opr2-&gt;is_single_fpu(), &quot;expect single float register&quot;);
1997     FloatRegister reg2 = opr2-&gt;as_float_reg();
1998     __ fcmps(reg1, reg2);
1999   } else if (opr1-&gt;is_double_fpu()) {
2000     FloatRegister reg1 = opr1-&gt;as_double_reg();
2001     assert(opr2-&gt;is_double_fpu(), &quot;expect double float register&quot;);
2002     FloatRegister reg2 = opr2-&gt;as_double_reg();
2003     __ fcmpd(reg1, reg2);
2004   } else {
2005     ShouldNotReachHere();
2006   }
2007 }
2008 
2009 void LIR_Assembler::comp_fl2i(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst, LIR_Op2* op){
2010   if (code == lir_cmp_fd2i || code == lir_ucmp_fd2i) {
2011     bool is_unordered_less = (code == lir_ucmp_fd2i);
2012     if (left-&gt;is_single_fpu()) {
2013       __ float_cmp(true, is_unordered_less ? -1 : 1, left-&gt;as_float_reg(), right-&gt;as_float_reg(), dst-&gt;as_register());
2014     } else if (left-&gt;is_double_fpu()) {
2015       __ float_cmp(false, is_unordered_less ? -1 : 1, left-&gt;as_double_reg(), right-&gt;as_double_reg(), dst-&gt;as_register());
2016     } else {
2017       ShouldNotReachHere();
2018     }
2019   } else if (code == lir_cmp_l2i) {
2020     Label done;
2021     __ cmp(left-&gt;as_register_lo(), right-&gt;as_register_lo());
2022     __ mov(dst-&gt;as_register(), (u_int64_t)-1L);
2023     __ br(Assembler::LT, done);
2024     __ csinc(dst-&gt;as_register(), zr, zr, Assembler::EQ);
2025     __ bind(done);
2026   } else {
2027     ShouldNotReachHere();
2028   }
2029 }
2030 
2031 
2032 void LIR_Assembler::align_call(LIR_Code code) {  }
2033 
2034 
2035 void LIR_Assembler::call(LIR_OpJavaCall* op, relocInfo::relocType rtype) {
2036   address call = __ trampoline_call(Address(op-&gt;addr(), rtype));
2037   if (call == NULL) {
2038     bailout(&quot;trampoline stub overflow&quot;);
2039     return;
2040   }
2041   add_call_info(code_offset(), op-&gt;info());
2042 }
2043 
2044 
2045 void LIR_Assembler::ic_call(LIR_OpJavaCall* op) {
2046   address call = __ ic_call(op-&gt;addr());
2047   if (call == NULL) {
2048     bailout(&quot;trampoline stub overflow&quot;);
2049     return;
2050   }
2051   add_call_info(code_offset(), op-&gt;info());
2052 }
2053 
2054 
2055 /* Currently, vtable-dispatch is only enabled for sparc platforms */
2056 void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {
2057   ShouldNotReachHere();
2058 }
2059 
2060 
2061 void LIR_Assembler::emit_static_call_stub() {
2062   address call_pc = __ pc();
2063   address stub = __ start_a_stub(call_stub_size());
2064   if (stub == NULL) {
2065     bailout(&quot;static call stub overflow&quot;);
2066     return;
2067   }
2068 
2069   int start = __ offset();
2070 
2071   __ relocate(static_stub_Relocation::spec(call_pc));
2072   __ emit_static_call_stub();
2073 
2074   assert(__ offset() - start + CompiledStaticCall::to_trampoline_stub_size()
2075         &lt;= call_stub_size(), &quot;stub too big&quot;);
2076   __ end_a_stub();
2077 }
2078 
2079 
2080 void LIR_Assembler::throw_op(LIR_Opr exceptionPC, LIR_Opr exceptionOop, CodeEmitInfo* info) {
2081   assert(exceptionOop-&gt;as_register() == r0, &quot;must match&quot;);
2082   assert(exceptionPC-&gt;as_register() == r3, &quot;must match&quot;);
2083 
2084   // exception object is not added to oop map by LinearScan
2085   // (LinearScan assumes that no oops are in fixed registers)
2086   info-&gt;add_register_oop(exceptionOop);
2087   Runtime1::StubID unwind_id;
2088 
2089   // get current pc information
2090   // pc is only needed if the method has an exception handler, the unwind code does not need it.
2091   int pc_for_athrow_offset = __ offset();
2092   InternalAddress pc_for_athrow(__ pc());
2093   __ adr(exceptionPC-&gt;as_register(), pc_for_athrow);
2094   add_call_info(pc_for_athrow_offset, info); // for exception handler
2095 
2096   __ verify_not_null_oop(r0);
2097   // search an exception handler (r0: exception oop, r3: throwing pc)
2098   if (compilation()-&gt;has_fpu_code()) {
2099     unwind_id = Runtime1::handle_exception_id;
2100   } else {
2101     unwind_id = Runtime1::handle_exception_nofpu_id;
2102   }
2103   __ far_call(RuntimeAddress(Runtime1::entry_for(unwind_id)));
2104 
2105   // FIXME: enough room for two byte trap   ????
2106   __ nop();
2107 }
2108 
2109 
2110 void LIR_Assembler::unwind_op(LIR_Opr exceptionOop) {
2111   assert(exceptionOop-&gt;as_register() == r0, &quot;must match&quot;);
2112 
2113   __ b(_unwind_handler_entry);
2114 }
2115 
2116 
2117 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, LIR_Opr count, LIR_Opr dest, LIR_Opr tmp) {
2118   Register lreg = left-&gt;is_single_cpu() ? left-&gt;as_register() : left-&gt;as_register_lo();
2119   Register dreg = dest-&gt;is_single_cpu() ? dest-&gt;as_register() : dest-&gt;as_register_lo();
2120 
2121   switch (left-&gt;type()) {
2122     case T_INT: {
2123       switch (code) {
2124       case lir_shl:  __ lslvw (dreg, lreg, count-&gt;as_register()); break;
2125       case lir_shr:  __ asrvw (dreg, lreg, count-&gt;as_register()); break;
2126       case lir_ushr: __ lsrvw (dreg, lreg, count-&gt;as_register()); break;
2127       default:
2128         ShouldNotReachHere();
2129         break;
2130       }
2131       break;
2132     case T_LONG:
2133     case T_ADDRESS:
2134     case T_OBJECT:
2135       switch (code) {
2136       case lir_shl:  __ lslv (dreg, lreg, count-&gt;as_register()); break;
2137       case lir_shr:  __ asrv (dreg, lreg, count-&gt;as_register()); break;
2138       case lir_ushr: __ lsrv (dreg, lreg, count-&gt;as_register()); break;
2139       default:
2140         ShouldNotReachHere();
2141         break;
2142       }
2143       break;
2144     default:
2145       ShouldNotReachHere();
2146       break;
2147     }
2148   }
2149 }
2150 
2151 
2152 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, jint count, LIR_Opr dest) {
2153   Register dreg = dest-&gt;is_single_cpu() ? dest-&gt;as_register() : dest-&gt;as_register_lo();
2154   Register lreg = left-&gt;is_single_cpu() ? left-&gt;as_register() : left-&gt;as_register_lo();
2155 
2156   switch (left-&gt;type()) {
2157     case T_INT: {
2158       switch (code) {
2159       case lir_shl:  __ lslw (dreg, lreg, count); break;
2160       case lir_shr:  __ asrw (dreg, lreg, count); break;
2161       case lir_ushr: __ lsrw (dreg, lreg, count); break;
2162       default:
2163         ShouldNotReachHere();
2164         break;
2165       }
2166       break;
2167     case T_LONG:
2168     case T_ADDRESS:
2169     case T_OBJECT:
2170       switch (code) {
2171       case lir_shl:  __ lsl (dreg, lreg, count); break;
2172       case lir_shr:  __ asr (dreg, lreg, count); break;
2173       case lir_ushr: __ lsr (dreg, lreg, count); break;
2174       default:
2175         ShouldNotReachHere();
2176         break;
2177       }
2178       break;
2179     default:
2180       ShouldNotReachHere();
2181       break;
2182     }
2183   }
2184 }
2185 
2186 
2187 void LIR_Assembler::store_parameter(Register r, int offset_from_rsp_in_words) {
2188   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
2189   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
2190   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
2191   __ str (r, Address(sp, offset_from_rsp_in_bytes));
2192 }
2193 
2194 
2195 void LIR_Assembler::store_parameter(jint c,     int offset_from_rsp_in_words) {
2196   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
2197   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
2198   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
2199   __ mov (rscratch1, c);
2200   __ str (rscratch1, Address(sp, offset_from_rsp_in_bytes));
2201 }
2202 
2203 
2204 void LIR_Assembler::store_parameter(jobject o,  int offset_from_rsp_in_words) {
2205   ShouldNotReachHere();
2206   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
2207   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
2208   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
2209   __ lea(rscratch1, __ constant_oop_address(o));
2210   __ str(rscratch1, Address(sp, offset_from_rsp_in_bytes));
2211 }
2212 
2213 
2214 // This code replaces a call to arraycopy; no exception may
2215 // be thrown in this code, they must be thrown in the System.arraycopy
2216 // activation frame; we could save some checks if this would not be the case
2217 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
2218   ciArrayKlass* default_type = op-&gt;expected_type();
2219   Register src = op-&gt;src()-&gt;as_register();
2220   Register dst = op-&gt;dst()-&gt;as_register();
2221   Register src_pos = op-&gt;src_pos()-&gt;as_register();
2222   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
2223   Register length  = op-&gt;length()-&gt;as_register();
2224   Register tmp = op-&gt;tmp()-&gt;as_register();
2225 
2226   __ resolve(ACCESS_READ, src);
2227   __ resolve(ACCESS_WRITE, dst);
2228 
2229   CodeStub* stub = op-&gt;stub();
2230   int flags = op-&gt;flags();
2231   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
2232   if (is_reference_type(basic_type)) basic_type = T_OBJECT;
2233 
2234   // if we don&#39;t know anything, just go through the generic arraycopy
2235   if (default_type == NULL // || basic_type == T_OBJECT
2236       ) {
2237     Label done;
2238     assert(src == r1 &amp;&amp; src_pos == r2, &quot;mismatch in calling convention&quot;);
2239 
2240     // Save the arguments in case the generic arraycopy fails and we
2241     // have to fall back to the JNI stub
2242     __ stp(dst,     dst_pos, Address(sp, 0*BytesPerWord));
2243     __ stp(length,  src_pos, Address(sp, 2*BytesPerWord));
2244     __ str(src,              Address(sp, 4*BytesPerWord));
2245 
2246     address copyfunc_addr = StubRoutines::generic_arraycopy();
2247     assert(copyfunc_addr != NULL, &quot;generic arraycopy stub required&quot;);
2248 
2249     // The arguments are in java calling convention so we shift them
2250     // to C convention
2251     assert_different_registers(c_rarg0, j_rarg1, j_rarg2, j_rarg3, j_rarg4);
2252     __ mov(c_rarg0, j_rarg0);
2253     assert_different_registers(c_rarg1, j_rarg2, j_rarg3, j_rarg4);
2254     __ mov(c_rarg1, j_rarg1);
2255     assert_different_registers(c_rarg2, j_rarg3, j_rarg4);
2256     __ mov(c_rarg2, j_rarg2);
2257     assert_different_registers(c_rarg3, j_rarg4);
2258     __ mov(c_rarg3, j_rarg3);
2259     __ mov(c_rarg4, j_rarg4);
2260 #ifndef PRODUCT
2261     if (PrintC1Statistics) {
2262       __ incrementw(ExternalAddress((address)&amp;Runtime1::_generic_arraycopystub_cnt));
2263     }
2264 #endif
2265     __ far_call(RuntimeAddress(copyfunc_addr));
2266 
2267     __ cbz(r0, *stub-&gt;continuation());
2268 
2269     // Reload values from the stack so they are where the stub
2270     // expects them.
2271     __ ldp(dst,     dst_pos, Address(sp, 0*BytesPerWord));
2272     __ ldp(length,  src_pos, Address(sp, 2*BytesPerWord));
2273     __ ldr(src,              Address(sp, 4*BytesPerWord));
2274 
2275     // r0 is -1^K where K == partial copied count
2276     __ eonw(rscratch1, r0, zr);
2277     // adjust length down and src/end pos up by partial copied count
2278     __ subw(length, length, rscratch1);
2279     __ addw(src_pos, src_pos, rscratch1);
2280     __ addw(dst_pos, dst_pos, rscratch1);
2281     __ b(*stub-&gt;entry());
2282 
2283     __ bind(*stub-&gt;continuation());
2284     return;
2285   }
2286 
2287   assert(default_type != NULL &amp;&amp; default_type-&gt;is_array_klass() &amp;&amp; default_type-&gt;is_loaded(), &quot;must be true at this point&quot;);
2288 
2289   int elem_size = type2aelembytes(basic_type);
2290   int shift_amount;
2291   int scale = exact_log2(elem_size);
2292 
2293   Address src_length_addr = Address(src, arrayOopDesc::length_offset_in_bytes());
2294   Address dst_length_addr = Address(dst, arrayOopDesc::length_offset_in_bytes());
2295   Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());
2296   Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());
2297 
2298   // test for NULL
2299   if (flags &amp; LIR_OpArrayCopy::src_null_check) {
2300     __ cbz(src, *stub-&gt;entry());
2301   }
2302   if (flags &amp; LIR_OpArrayCopy::dst_null_check) {
2303     __ cbz(dst, *stub-&gt;entry());
2304   }
2305 
2306   // If the compiler was not able to prove that exact type of the source or the destination
2307   // of the arraycopy is an array type, check at runtime if the source or the destination is
2308   // an instance type.
2309   if (flags &amp; LIR_OpArrayCopy::type_check) {
2310     if (!(flags &amp; LIR_OpArrayCopy::LIR_OpArrayCopy::dst_objarray)) {
2311       __ load_klass(tmp, dst);
2312       __ ldrw(rscratch1, Address(tmp, in_bytes(Klass::layout_helper_offset())));
2313       __ cmpw(rscratch1, Klass::_lh_neutral_value);
2314       __ br(Assembler::GE, *stub-&gt;entry());
2315     }
2316 
2317     if (!(flags &amp; LIR_OpArrayCopy::LIR_OpArrayCopy::src_objarray)) {
2318       __ load_klass(tmp, src);
2319       __ ldrw(rscratch1, Address(tmp, in_bytes(Klass::layout_helper_offset())));
2320       __ cmpw(rscratch1, Klass::_lh_neutral_value);
2321       __ br(Assembler::GE, *stub-&gt;entry());
2322     }
2323   }
2324 
2325   // check if negative
2326   if (flags &amp; LIR_OpArrayCopy::src_pos_positive_check) {
2327     __ cmpw(src_pos, 0);
2328     __ br(Assembler::LT, *stub-&gt;entry());
2329   }
2330   if (flags &amp; LIR_OpArrayCopy::dst_pos_positive_check) {
2331     __ cmpw(dst_pos, 0);
2332     __ br(Assembler::LT, *stub-&gt;entry());
2333   }
2334 
2335   if (flags &amp; LIR_OpArrayCopy::length_positive_check) {
2336     __ cmpw(length, 0);
2337     __ br(Assembler::LT, *stub-&gt;entry());
2338   }
2339 
2340   if (flags &amp; LIR_OpArrayCopy::src_range_check) {
2341     __ addw(tmp, src_pos, length);
2342     __ ldrw(rscratch1, src_length_addr);
2343     __ cmpw(tmp, rscratch1);
2344     __ br(Assembler::HI, *stub-&gt;entry());
2345   }
2346   if (flags &amp; LIR_OpArrayCopy::dst_range_check) {
2347     __ addw(tmp, dst_pos, length);
2348     __ ldrw(rscratch1, dst_length_addr);
2349     __ cmpw(tmp, rscratch1);
2350     __ br(Assembler::HI, *stub-&gt;entry());
2351   }
2352 
2353   if (flags &amp; LIR_OpArrayCopy::type_check) {
2354     // We don&#39;t know the array types are compatible
2355     if (basic_type != T_OBJECT) {
2356       // Simple test for basic type arrays
2357       if (UseCompressedClassPointers) {
2358         __ ldrw(tmp, src_klass_addr);
2359         __ ldrw(rscratch1, dst_klass_addr);
2360         __ cmpw(tmp, rscratch1);
2361       } else {
2362         __ ldr(tmp, src_klass_addr);
2363         __ ldr(rscratch1, dst_klass_addr);
2364         __ cmp(tmp, rscratch1);
2365       }
2366       __ br(Assembler::NE, *stub-&gt;entry());
2367     } else {
2368       // For object arrays, if src is a sub class of dst then we can
2369       // safely do the copy.
2370       Label cont, slow;
2371 
2372 #define PUSH(r1, r2)                                    \
2373       stp(r1, r2, __ pre(sp, -2 * wordSize));
2374 
2375 #define POP(r1, r2)                                     \
2376       ldp(r1, r2, __ post(sp, 2 * wordSize));
2377 
2378       __ PUSH(src, dst);
2379 
2380       __ load_klass(src, src);
2381       __ load_klass(dst, dst);
2382 
2383       __ check_klass_subtype_fast_path(src, dst, tmp, &amp;cont, &amp;slow, NULL);
2384 
2385       __ PUSH(src, dst);
2386       __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
2387       __ POP(src, dst);
2388 
2389       __ cbnz(src, cont);
2390 
2391       __ bind(slow);
2392       __ POP(src, dst);
2393 
2394       address copyfunc_addr = StubRoutines::checkcast_arraycopy();
2395       if (copyfunc_addr != NULL) { // use stub if available
2396         // src is not a sub class of dst so we have to do a
2397         // per-element check.
2398 
2399         int mask = LIR_OpArrayCopy::src_objarray|LIR_OpArrayCopy::dst_objarray;
2400         if ((flags &amp; mask) != mask) {
2401           // Check that at least both of them object arrays.
2402           assert(flags &amp; mask, &quot;one of the two should be known to be an object array&quot;);
2403 
2404           if (!(flags &amp; LIR_OpArrayCopy::src_objarray)) {
2405             __ load_klass(tmp, src);
2406           } else if (!(flags &amp; LIR_OpArrayCopy::dst_objarray)) {
2407             __ load_klass(tmp, dst);
2408           }
2409           int lh_offset = in_bytes(Klass::layout_helper_offset());
2410           Address klass_lh_addr(tmp, lh_offset);
2411           jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
2412           __ ldrw(rscratch1, klass_lh_addr);
2413           __ mov(rscratch2, objArray_lh);
2414           __ eorw(rscratch1, rscratch1, rscratch2);
2415           __ cbnzw(rscratch1, *stub-&gt;entry());
2416         }
2417 
2418        // Spill because stubs can use any register they like and it&#39;s
2419        // easier to restore just those that we care about.
2420         __ stp(dst,     dst_pos, Address(sp, 0*BytesPerWord));
2421         __ stp(length,  src_pos, Address(sp, 2*BytesPerWord));
2422         __ str(src,              Address(sp, 4*BytesPerWord));
2423 
2424         __ lea(c_rarg0, Address(src, src_pos, Address::uxtw(scale)));
2425         __ add(c_rarg0, c_rarg0, arrayOopDesc::base_offset_in_bytes(basic_type));
2426         assert_different_registers(c_rarg0, dst, dst_pos, length);
2427         __ lea(c_rarg1, Address(dst, dst_pos, Address::uxtw(scale)));
2428         __ add(c_rarg1, c_rarg1, arrayOopDesc::base_offset_in_bytes(basic_type));
2429         assert_different_registers(c_rarg1, dst, length);
2430         __ uxtw(c_rarg2, length);
2431         assert_different_registers(c_rarg2, dst);
2432 
2433         __ load_klass(c_rarg4, dst);
2434         __ ldr(c_rarg4, Address(c_rarg4, ObjArrayKlass::element_klass_offset()));
2435         __ ldrw(c_rarg3, Address(c_rarg4, Klass::super_check_offset_offset()));
2436         __ far_call(RuntimeAddress(copyfunc_addr));
2437 
2438 #ifndef PRODUCT
2439         if (PrintC1Statistics) {
2440           Label failed;
2441           __ cbnz(r0, failed);
2442           __ incrementw(ExternalAddress((address)&amp;Runtime1::_arraycopy_checkcast_cnt));
2443           __ bind(failed);
2444         }
2445 #endif
2446 
2447         __ cbz(r0, *stub-&gt;continuation());
2448 
2449 #ifndef PRODUCT
2450         if (PrintC1Statistics) {
2451           __ incrementw(ExternalAddress((address)&amp;Runtime1::_arraycopy_checkcast_attempt_cnt));
2452         }
2453 #endif
2454         assert_different_registers(dst, dst_pos, length, src_pos, src, r0, rscratch1);
2455 
2456         // Restore previously spilled arguments
2457         __ ldp(dst,     dst_pos, Address(sp, 0*BytesPerWord));
2458         __ ldp(length,  src_pos, Address(sp, 2*BytesPerWord));
2459         __ ldr(src,              Address(sp, 4*BytesPerWord));
2460 
2461         // return value is -1^K where K is partial copied count
2462         __ eonw(rscratch1, r0, zr);
2463         // adjust length down and src/end pos up by partial copied count
2464         __ subw(length, length, rscratch1);
2465         __ addw(src_pos, src_pos, rscratch1);
2466         __ addw(dst_pos, dst_pos, rscratch1);
2467       }
2468 
2469       __ b(*stub-&gt;entry());
2470 
2471       __ bind(cont);
2472       __ POP(src, dst);
2473     }
2474   }
2475 
2476 #ifdef ASSERT
2477   if (basic_type != T_OBJECT || !(flags &amp; LIR_OpArrayCopy::type_check)) {
2478     // Sanity check the known type with the incoming class.  For the
2479     // primitive case the types must match exactly with src.klass and
2480     // dst.klass each exactly matching the default type.  For the
2481     // object array case, if no type check is needed then either the
2482     // dst type is exactly the expected type and the src type is a
2483     // subtype which we can&#39;t check or src is the same array as dst
2484     // but not necessarily exactly of type default_type.
2485     Label known_ok, halt;
2486     __ mov_metadata(tmp, default_type-&gt;constant_encoding());
2487     if (UseCompressedClassPointers) {
2488       __ encode_klass_not_null(tmp);
2489     }
2490 
2491     if (basic_type != T_OBJECT) {
2492 
2493       if (UseCompressedClassPointers) {
2494         __ ldrw(rscratch1, dst_klass_addr);
2495         __ cmpw(tmp, rscratch1);
2496       } else {
2497         __ ldr(rscratch1, dst_klass_addr);
2498         __ cmp(tmp, rscratch1);
2499       }
2500       __ br(Assembler::NE, halt);
2501       if (UseCompressedClassPointers) {
2502         __ ldrw(rscratch1, src_klass_addr);
2503         __ cmpw(tmp, rscratch1);
2504       } else {
2505         __ ldr(rscratch1, src_klass_addr);
2506         __ cmp(tmp, rscratch1);
2507       }
2508       __ br(Assembler::EQ, known_ok);
2509     } else {
2510       if (UseCompressedClassPointers) {
2511         __ ldrw(rscratch1, dst_klass_addr);
2512         __ cmpw(tmp, rscratch1);
2513       } else {
2514         __ ldr(rscratch1, dst_klass_addr);
2515         __ cmp(tmp, rscratch1);
2516       }
2517       __ br(Assembler::EQ, known_ok);
2518       __ cmp(src, dst);
2519       __ br(Assembler::EQ, known_ok);
2520     }
2521     __ bind(halt);
2522     __ stop(&quot;incorrect type information in arraycopy&quot;);
2523     __ bind(known_ok);
2524   }
2525 #endif
2526 
2527 #ifndef PRODUCT
2528   if (PrintC1Statistics) {
2529     __ incrementw(ExternalAddress(Runtime1::arraycopy_count_address(basic_type)));
2530   }
2531 #endif
2532 
2533   __ lea(c_rarg0, Address(src, src_pos, Address::uxtw(scale)));
2534   __ add(c_rarg0, c_rarg0, arrayOopDesc::base_offset_in_bytes(basic_type));
2535   assert_different_registers(c_rarg0, dst, dst_pos, length);
2536   __ lea(c_rarg1, Address(dst, dst_pos, Address::uxtw(scale)));
2537   __ add(c_rarg1, c_rarg1, arrayOopDesc::base_offset_in_bytes(basic_type));
2538   assert_different_registers(c_rarg1, dst, length);
2539   __ uxtw(c_rarg2, length);
2540   assert_different_registers(c_rarg2, dst);
2541 
2542   bool disjoint = (flags &amp; LIR_OpArrayCopy::overlapping) == 0;
2543   bool aligned = (flags &amp; LIR_OpArrayCopy::unaligned) == 0;
2544   const char *name;
2545   address entry = StubRoutines::select_arraycopy_function(basic_type, aligned, disjoint, name, false);
2546 
2547  CodeBlob *cb = CodeCache::find_blob(entry);
2548  if (cb) {
2549    __ far_call(RuntimeAddress(entry));
2550  } else {
2551    __ call_VM_leaf(entry, 3);
2552  }
2553 
2554   __ bind(*stub-&gt;continuation());
2555 }
2556 
2557 
2558 
2559 
2560 void LIR_Assembler::emit_lock(LIR_OpLock* op) {
2561   Register obj = op-&gt;obj_opr()-&gt;as_register();  // may not be an oop
2562   Register hdr = op-&gt;hdr_opr()-&gt;as_register();
2563   Register lock = op-&gt;lock_opr()-&gt;as_register();
2564   if (!UseFastLocking) {
2565     __ b(*op-&gt;stub()-&gt;entry());
2566   } else if (op-&gt;code() == lir_lock) {
2567     Register scratch = noreg;
2568     if (UseBiasedLocking) {
2569       scratch = op-&gt;scratch_opr()-&gt;as_register();
2570     }
2571     assert(BasicLock::displaced_header_offset_in_bytes() == 0, &quot;lock_reg must point to the displaced header&quot;);
2572     __ resolve(ACCESS_READ | ACCESS_WRITE, obj);
2573     // add debug info for NullPointerException only if one is possible
2574     int null_check_offset = __ lock_object(hdr, obj, lock, scratch, *op-&gt;stub()-&gt;entry());
2575     if (op-&gt;info() != NULL) {
2576       add_debug_info_for_null_check(null_check_offset, op-&gt;info());
2577     }
2578     // done
2579   } else if (op-&gt;code() == lir_unlock) {
2580     assert(BasicLock::displaced_header_offset_in_bytes() == 0, &quot;lock_reg must point to the displaced header&quot;);
2581     __ unlock_object(hdr, obj, lock, *op-&gt;stub()-&gt;entry());
2582   } else {
2583     Unimplemented();
2584   }
2585   __ bind(*op-&gt;stub()-&gt;continuation());
2586 }
2587 
2588 
2589 void LIR_Assembler::emit_profile_call(LIR_OpProfileCall* op) {
2590   ciMethod* method = op-&gt;profiled_method();
2591   int bci          = op-&gt;profiled_bci();
2592   ciMethod* callee = op-&gt;profiled_callee();
2593 
2594   // Update counter for all call types
2595   ciMethodData* md = method-&gt;method_data_or_null();
2596   assert(md != NULL, &quot;Sanity&quot;);
2597   ciProfileData* data = md-&gt;bci_to_data(bci);
2598   assert(data != NULL &amp;&amp; data-&gt;is_CounterData(), &quot;need CounterData for calls&quot;);
2599   assert(op-&gt;mdo()-&gt;is_single_cpu(),  &quot;mdo must be allocated&quot;);
2600   Register mdo  = op-&gt;mdo()-&gt;as_register();
2601   __ mov_metadata(mdo, md-&gt;constant_encoding());
2602   Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()));
2603   // Perform additional virtual call profiling for invokevirtual and
2604   // invokeinterface bytecodes
2605   if (op-&gt;should_profile_receiver_type()) {
2606     assert(op-&gt;recv()-&gt;is_single_cpu(), &quot;recv must be allocated&quot;);
2607     Register recv = op-&gt;recv()-&gt;as_register();
2608     assert_different_registers(mdo, recv);
2609     assert(data-&gt;is_VirtualCallData(), &quot;need VirtualCallData for virtual calls&quot;);
2610     ciKlass* known_klass = op-&gt;known_holder();
2611     if (C1OptimizeVirtualCallProfiling &amp;&amp; known_klass != NULL) {
2612       // We know the type that will be seen at this call site; we can
2613       // statically update the MethodData* rather than needing to do
2614       // dynamic tests on the receiver type
2615 
2616       // NOTE: we should probably put a lock around this search to
2617       // avoid collisions by concurrent compilations
2618       ciVirtualCallData* vc_data = (ciVirtualCallData*) data;
2619       uint i;
2620       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2621         ciKlass* receiver = vc_data-&gt;receiver(i);
2622         if (known_klass-&gt;equals(receiver)) {
2623           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)));
2624           __ addptr(data_addr, DataLayout::counter_increment);
2625           return;
2626         }
2627       }
2628 
2629       // Receiver type not found in profile data; select an empty slot
2630 
2631       // Note that this is less efficient than it should be because it
2632       // always does a write to the receiver part of the
2633       // VirtualCallData rather than just the first time
2634       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2635         ciKlass* receiver = vc_data-&gt;receiver(i);
2636         if (receiver == NULL) {
2637           Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_offset(i)));
2638           __ mov_metadata(rscratch1, known_klass-&gt;constant_encoding());
2639           __ lea(rscratch2, recv_addr);
2640           __ str(rscratch1, Address(rscratch2));
2641           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)));
2642           __ addptr(data_addr, DataLayout::counter_increment);
2643           return;
2644         }
2645       }
2646     } else {
2647       __ load_klass(recv, recv);
2648       Label update_done;
2649       type_profile_helper(mdo, md, data, recv, &amp;update_done);
2650       // Receiver did not match any saved receiver and there is no empty row for it.
2651       // Increment total counter to indicate polymorphic case.
2652       __ addptr(counter_addr, DataLayout::counter_increment);
2653 
2654       __ bind(update_done);
2655     }
2656   } else {
2657     // Static call
2658     __ addptr(counter_addr, DataLayout::counter_increment);
2659   }
2660 }
2661 
2662 
2663 void LIR_Assembler::emit_delay(LIR_OpDelay*) {
2664   Unimplemented();
2665 }
2666 
2667 
2668 void LIR_Assembler::monitor_address(int monitor_no, LIR_Opr dst) {
2669   __ lea(dst-&gt;as_register(), frame_map()-&gt;address_for_monitor_lock(monitor_no));
2670 }
2671 
2672 void LIR_Assembler::emit_updatecrc32(LIR_OpUpdateCRC32* op) {
2673   assert(op-&gt;crc()-&gt;is_single_cpu(),  &quot;crc must be register&quot;);
2674   assert(op-&gt;val()-&gt;is_single_cpu(),  &quot;byte value must be register&quot;);
2675   assert(op-&gt;result_opr()-&gt;is_single_cpu(), &quot;result must be register&quot;);
2676   Register crc = op-&gt;crc()-&gt;as_register();
2677   Register val = op-&gt;val()-&gt;as_register();
2678   Register res = op-&gt;result_opr()-&gt;as_register();
2679 
2680   assert_different_registers(val, crc, res);
2681   unsigned long offset;
2682   __ adrp(res, ExternalAddress(StubRoutines::crc_table_addr()), offset);
2683   if (offset) __ add(res, res, offset);
2684 
2685   __ mvnw(crc, crc); // ~crc
2686   __ update_byte_crc32(crc, val, res);
2687   __ mvnw(res, crc); // ~crc
2688 }
2689 
2690 void LIR_Assembler::emit_profile_type(LIR_OpProfileType* op) {
2691   COMMENT(&quot;emit_profile_type {&quot;);
2692   Register obj = op-&gt;obj()-&gt;as_register();
2693   Register tmp = op-&gt;tmp()-&gt;as_pointer_register();
2694   Address mdo_addr = as_Address(op-&gt;mdp()-&gt;as_address_ptr());
2695   ciKlass* exact_klass = op-&gt;exact_klass();
2696   intptr_t current_klass = op-&gt;current_klass();
2697   bool not_null = op-&gt;not_null();
2698   bool no_conflict = op-&gt;no_conflict();
2699 
2700   Label update, next, none;
2701 
2702   bool do_null = !not_null;
2703   bool exact_klass_set = exact_klass != NULL &amp;&amp; ciTypeEntries::valid_ciklass(current_klass) == exact_klass;
2704   bool do_update = !TypeEntries::is_type_unknown(current_klass) &amp;&amp; !exact_klass_set;
2705 
2706   assert(do_null || do_update, &quot;why are we here?&quot;);
2707   assert(!TypeEntries::was_null_seen(current_klass) || do_update, &quot;why are we here?&quot;);
2708   assert(mdo_addr.base() != rscratch1, &quot;wrong register&quot;);
2709 
2710   __ verify_oop(obj);
2711 
2712   if (tmp != obj) {
2713     __ mov(tmp, obj);
2714   }
2715   if (do_null) {
2716     __ cbnz(tmp, update);
2717     if (!TypeEntries::was_null_seen(current_klass)) {
2718       __ ldr(rscratch2, mdo_addr);
2719       __ orr(rscratch2, rscratch2, TypeEntries::null_seen);
2720       __ str(rscratch2, mdo_addr);
2721     }
2722     if (do_update) {
2723 #ifndef ASSERT
2724       __ b(next);
2725     }
2726 #else
2727       __ b(next);
2728     }
2729   } else {
2730     __ cbnz(tmp, update);
2731     __ stop(&quot;unexpected null obj&quot;);
2732 #endif
2733   }
2734 
2735   __ bind(update);
2736 
2737   if (do_update) {
2738 #ifdef ASSERT
2739     if (exact_klass != NULL) {
2740       Label ok;
2741       __ load_klass(tmp, tmp);
2742       __ mov_metadata(rscratch1, exact_klass-&gt;constant_encoding());
2743       __ eor(rscratch1, tmp, rscratch1);
2744       __ cbz(rscratch1, ok);
2745       __ stop(&quot;exact klass and actual klass differ&quot;);
2746       __ bind(ok);
2747     }
2748 #endif
2749     if (!no_conflict) {
2750       if (exact_klass == NULL || TypeEntries::is_type_none(current_klass)) {
2751         if (exact_klass != NULL) {
2752           __ mov_metadata(tmp, exact_klass-&gt;constant_encoding());
2753         } else {
2754           __ load_klass(tmp, tmp);
2755         }
2756 
2757         __ ldr(rscratch2, mdo_addr);
2758         __ eor(tmp, tmp, rscratch2);
2759         __ andr(rscratch1, tmp, TypeEntries::type_klass_mask);
2760         // klass seen before, nothing to do. The unknown bit may have been
2761         // set already but no need to check.
2762         __ cbz(rscratch1, next);
2763 
2764         __ tbnz(tmp, exact_log2(TypeEntries::type_unknown), next); // already unknown. Nothing to do anymore.
2765 
2766         if (TypeEntries::is_type_none(current_klass)) {
2767           __ cbz(rscratch2, none);
2768           __ cmp(rscratch2, (u1)TypeEntries::null_seen);
2769           __ br(Assembler::EQ, none);
2770           // There is a chance that the checks above (re-reading profiling
2771           // data from memory) fail if another thread has just set the
2772           // profiling to this obj&#39;s klass
2773           __ dmb(Assembler::ISHLD);
2774           __ ldr(rscratch2, mdo_addr);
2775           __ eor(tmp, tmp, rscratch2);
2776           __ andr(rscratch1, tmp, TypeEntries::type_klass_mask);
2777           __ cbz(rscratch1, next);
2778         }
2779       } else {
2780         assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &amp;&amp;
2781                ciTypeEntries::valid_ciklass(current_klass) != exact_klass, &quot;conflict only&quot;);
2782 
2783         __ ldr(tmp, mdo_addr);
2784         __ tbnz(tmp, exact_log2(TypeEntries::type_unknown), next); // already unknown. Nothing to do anymore.
2785       }
2786 
2787       // different than before. Cannot keep accurate profile.
2788       __ ldr(rscratch2, mdo_addr);
2789       __ orr(rscratch2, rscratch2, TypeEntries::type_unknown);
2790       __ str(rscratch2, mdo_addr);
2791 
2792       if (TypeEntries::is_type_none(current_klass)) {
2793         __ b(next);
2794 
2795         __ bind(none);
2796         // first time here. Set profile type.
2797         __ str(tmp, mdo_addr);
2798       }
2799     } else {
2800       // There&#39;s a single possible klass at this profile point
2801       assert(exact_klass != NULL, &quot;should be&quot;);
2802       if (TypeEntries::is_type_none(current_klass)) {
2803         __ mov_metadata(tmp, exact_klass-&gt;constant_encoding());
2804         __ ldr(rscratch2, mdo_addr);
2805         __ eor(tmp, tmp, rscratch2);
2806         __ andr(rscratch1, tmp, TypeEntries::type_klass_mask);
2807         __ cbz(rscratch1, next);
2808 #ifdef ASSERT
2809         {
2810           Label ok;
2811           __ ldr(rscratch1, mdo_addr);
2812           __ cbz(rscratch1, ok);
2813           __ cmp(rscratch1, (u1)TypeEntries::null_seen);
2814           __ br(Assembler::EQ, ok);
2815           // may have been set by another thread
2816           __ dmb(Assembler::ISHLD);
2817           __ mov_metadata(rscratch1, exact_klass-&gt;constant_encoding());
2818           __ ldr(rscratch2, mdo_addr);
2819           __ eor(rscratch2, rscratch1, rscratch2);
2820           __ andr(rscratch2, rscratch2, TypeEntries::type_mask);
2821           __ cbz(rscratch2, ok);
2822 
2823           __ stop(&quot;unexpected profiling mismatch&quot;);
2824           __ bind(ok);
2825         }
2826 #endif
2827         // first time here. Set profile type.
2828         __ ldr(tmp, mdo_addr);
2829       } else {
2830         assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &amp;&amp;
2831                ciTypeEntries::valid_ciklass(current_klass) != exact_klass, &quot;inconsistent&quot;);
2832 
2833         __ ldr(tmp, mdo_addr);
2834         __ tbnz(tmp, exact_log2(TypeEntries::type_unknown), next); // already unknown. Nothing to do anymore.
2835 
2836         __ orr(tmp, tmp, TypeEntries::type_unknown);
2837         __ str(tmp, mdo_addr);
2838         // FIXME: Write barrier needed here?
2839       }
2840     }
2841 
2842     __ bind(next);
2843   }
2844   COMMENT(&quot;} emit_profile_type&quot;);
2845 }
2846 
2847 
2848 void LIR_Assembler::align_backward_branch_target() {
2849 }
2850 
2851 
2852 void LIR_Assembler::negate(LIR_Opr left, LIR_Opr dest, LIR_Opr tmp) {
2853   // tmp must be unused
2854   assert(tmp-&gt;is_illegal(), &quot;wasting a register if tmp is allocated&quot;);
2855 
2856   if (left-&gt;is_single_cpu()) {
2857     assert(dest-&gt;is_single_cpu(), &quot;expect single result reg&quot;);
2858     __ negw(dest-&gt;as_register(), left-&gt;as_register());
2859   } else if (left-&gt;is_double_cpu()) {
2860     assert(dest-&gt;is_double_cpu(), &quot;expect double result reg&quot;);
2861     __ neg(dest-&gt;as_register_lo(), left-&gt;as_register_lo());
2862   } else if (left-&gt;is_single_fpu()) {
2863     assert(dest-&gt;is_single_fpu(), &quot;expect single float result reg&quot;);
2864     __ fnegs(dest-&gt;as_float_reg(), left-&gt;as_float_reg());
2865   } else {
2866     assert(left-&gt;is_double_fpu(), &quot;expect double float operand reg&quot;);
2867     assert(dest-&gt;is_double_fpu(), &quot;expect double float result reg&quot;);
2868     __ fnegd(dest-&gt;as_double_reg(), left-&gt;as_double_reg());
2869   }
2870 }
2871 
2872 
2873 void LIR_Assembler::leal(LIR_Opr addr, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
2874   if (patch_code != lir_patch_none) {
2875     deoptimize_trap(info);
2876     return;
2877   }
2878 
2879   __ lea(dest-&gt;as_register_lo(), as_Address(addr-&gt;as_address_ptr()));
2880 }
2881 
2882 
2883 void LIR_Assembler::rt_call(LIR_Opr result, address dest, const LIR_OprList* args, LIR_Opr tmp, CodeEmitInfo* info) {
2884   assert(!tmp-&gt;is_valid(), &quot;don&#39;t need temporary&quot;);
2885 
2886   CodeBlob *cb = CodeCache::find_blob(dest);
2887   if (cb) {
2888     __ far_call(RuntimeAddress(dest));
2889   } else {
2890     __ mov(rscratch1, RuntimeAddress(dest));
2891     __ blr(rscratch1);
2892   }
2893 
2894   if (info != NULL) {
2895     add_call_info_here(info);
2896   }
2897   __ maybe_isb();
2898 }
2899 
2900 void LIR_Assembler::volatile_move_op(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info) {
2901   if (dest-&gt;is_address() || src-&gt;is_address()) {
2902     move_op(src, dest, type, lir_patch_none, info,
2903             /*pop_fpu_stack*/false, /*unaligned*/false, /*wide*/false);
2904   } else {
2905     ShouldNotReachHere();
2906   }
2907 }
2908 
2909 #ifdef ASSERT
2910 // emit run-time assertion
2911 void LIR_Assembler::emit_assert(LIR_OpAssert* op) {
2912   assert(op-&gt;code() == lir_assert, &quot;must be&quot;);
2913 
2914   if (op-&gt;in_opr1()-&gt;is_valid()) {
2915     assert(op-&gt;in_opr2()-&gt;is_valid(), &quot;both operands must be valid&quot;);
2916     comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
2917   } else {
2918     assert(op-&gt;in_opr2()-&gt;is_illegal(), &quot;both operands must be illegal&quot;);
2919     assert(op-&gt;condition() == lir_cond_always, &quot;no other conditions allowed&quot;);
2920   }
2921 
2922   Label ok;
2923   if (op-&gt;condition() != lir_cond_always) {
2924     Assembler::Condition acond = Assembler::AL;
2925     switch (op-&gt;condition()) {
2926       case lir_cond_equal:        acond = Assembler::EQ;  break;
2927       case lir_cond_notEqual:     acond = Assembler::NE;  break;
2928       case lir_cond_less:         acond = Assembler::LT;  break;
2929       case lir_cond_lessEqual:    acond = Assembler::LE;  break;
2930       case lir_cond_greaterEqual: acond = Assembler::GE;  break;
2931       case lir_cond_greater:      acond = Assembler::GT;  break;
2932       case lir_cond_belowEqual:   acond = Assembler::LS;  break;
2933       case lir_cond_aboveEqual:   acond = Assembler::HS;  break;
2934       default:                    ShouldNotReachHere();
2935     }
2936     __ br(acond, ok);
2937   }
2938   if (op-&gt;halt()) {
2939     const char* str = __ code_string(op-&gt;msg());
2940     __ stop(str);
2941   } else {
2942     breakpoint();
2943   }
2944   __ bind(ok);
2945 }
2946 #endif
2947 
2948 #ifndef PRODUCT
2949 #define COMMENT(x)   do { __ block_comment(x); } while (0)
2950 #else
2951 #define COMMENT(x)
2952 #endif
2953 
2954 void LIR_Assembler::membar() {
2955   COMMENT(&quot;membar&quot;);
2956   __ membar(MacroAssembler::AnyAny);
2957 }
2958 
2959 void LIR_Assembler::membar_acquire() {
2960   __ membar(Assembler::LoadLoad|Assembler::LoadStore);
2961 }
2962 
2963 void LIR_Assembler::membar_release() {
2964   __ membar(Assembler::LoadStore|Assembler::StoreStore);
2965 }
2966 
2967 void LIR_Assembler::membar_loadload() {
2968   __ membar(Assembler::LoadLoad);
2969 }
2970 
2971 void LIR_Assembler::membar_storestore() {
2972   __ membar(MacroAssembler::StoreStore);
2973 }
2974 
2975 void LIR_Assembler::membar_loadstore() { __ membar(MacroAssembler::LoadStore); }
2976 
2977 void LIR_Assembler::membar_storeload() { __ membar(MacroAssembler::StoreLoad); }
2978 
2979 void LIR_Assembler::on_spin_wait() {
2980   Unimplemented();
2981 }
2982 
2983 void LIR_Assembler::get_thread(LIR_Opr result_reg) {
2984   __ mov(result_reg-&gt;as_register(), rthread);
2985 }
2986 
2987 
2988 void LIR_Assembler::peephole(LIR_List *lir) {
2989 #if 0
2990   if (tableswitch_count &gt;= max_tableswitches)
2991     return;
2992 
2993   /*
2994     This finite-state automaton recognizes sequences of compare-and-
2995     branch instructions.  We will turn them into a tableswitch.  You
2996     could argue that C1 really shouldn&#39;t be doing this sort of
2997     optimization, but without it the code is really horrible.
2998   */
2999 
3000   enum { start_s, cmp1_s, beq_s, cmp_s } state;
3001   int first_key, last_key = -2147483648;
3002   int next_key = 0;
3003   int start_insn = -1;
3004   int last_insn = -1;
3005   Register reg = noreg;
3006   LIR_Opr reg_opr;
3007   state = start_s;
3008 
3009   LIR_OpList* inst = lir-&gt;instructions_list();
3010   for (int i = 0; i &lt; inst-&gt;length(); i++) {
3011     LIR_Op* op = inst-&gt;at(i);
3012     switch (state) {
3013     case start_s:
3014       first_key = -1;
3015       start_insn = i;
3016       switch (op-&gt;code()) {
3017       case lir_cmp:
3018         LIR_Opr opr1 = op-&gt;as_Op2()-&gt;in_opr1();
3019         LIR_Opr opr2 = op-&gt;as_Op2()-&gt;in_opr2();
3020         if (opr1-&gt;is_cpu_register() &amp;&amp; opr1-&gt;is_single_cpu()
3021             &amp;&amp; opr2-&gt;is_constant()
3022             &amp;&amp; opr2-&gt;type() == T_INT) {
3023           reg_opr = opr1;
3024           reg = opr1-&gt;as_register();
3025           first_key = opr2-&gt;as_constant_ptr()-&gt;as_jint();
3026           next_key = first_key + 1;
3027           state = cmp_s;
3028           goto next_state;
3029         }
3030         break;
3031       }
3032       break;
3033     case cmp_s:
3034       switch (op-&gt;code()) {
3035       case lir_branch:
3036         if (op-&gt;as_OpBranch()-&gt;cond() == lir_cond_equal) {
3037           state = beq_s;
3038           last_insn = i;
3039           goto next_state;
3040         }
3041       }
3042       state = start_s;
3043       break;
3044     case beq_s:
3045       switch (op-&gt;code()) {
3046       case lir_cmp: {
3047         LIR_Opr opr1 = op-&gt;as_Op2()-&gt;in_opr1();
3048         LIR_Opr opr2 = op-&gt;as_Op2()-&gt;in_opr2();
3049         if (opr1-&gt;is_cpu_register() &amp;&amp; opr1-&gt;is_single_cpu()
3050             &amp;&amp; opr1-&gt;as_register() == reg
3051             &amp;&amp; opr2-&gt;is_constant()
3052             &amp;&amp; opr2-&gt;type() == T_INT
3053             &amp;&amp; opr2-&gt;as_constant_ptr()-&gt;as_jint() == next_key) {
3054           last_key = next_key;
3055           next_key++;
3056           state = cmp_s;
3057           goto next_state;
3058         }
3059       }
3060       }
3061       last_key = next_key;
3062       state = start_s;
3063       break;
3064     default:
3065       assert(false, &quot;impossible state&quot;);
3066     }
3067     if (state == start_s) {
3068       if (first_key &lt; last_key - 5L &amp;&amp; reg != noreg) {
3069         {
3070           // printf(&quot;found run register %d starting at insn %d low value %d high value %d\n&quot;,
3071           //        reg-&gt;encoding(),
3072           //        start_insn, first_key, last_key);
3073           //   for (int i = 0; i &lt; inst-&gt;length(); i++) {
3074           //     inst-&gt;at(i)-&gt;print();
3075           //     tty-&gt;print(&quot;\n&quot;);
3076           //   }
3077           //   tty-&gt;print(&quot;\n&quot;);
3078         }
3079 
3080         struct tableswitch *sw = &amp;switches[tableswitch_count];
3081         sw-&gt;_insn_index = start_insn, sw-&gt;_first_key = first_key,
3082           sw-&gt;_last_key = last_key, sw-&gt;_reg = reg;
3083         inst-&gt;insert_before(last_insn + 1, new LIR_OpLabel(&amp;sw-&gt;_after));
3084         {
3085           // Insert the new table of branches
3086           int offset = last_insn;
3087           for (int n = first_key; n &lt; last_key; n++) {
3088             inst-&gt;insert_before
3089               (last_insn + 1,
3090                new LIR_OpBranch(lir_cond_always, T_ILLEGAL,
3091                                 inst-&gt;at(offset)-&gt;as_OpBranch()-&gt;label()));
3092             offset -= 2, i++;
3093           }
3094         }
3095         // Delete all the old compare-and-branch instructions
3096         for (int n = first_key; n &lt; last_key; n++) {
3097           inst-&gt;remove_at(start_insn);
3098           inst-&gt;remove_at(start_insn);
3099         }
3100         // Insert the tableswitch instruction
3101         inst-&gt;insert_before(start_insn,
3102                             new LIR_Op2(lir_cmp, lir_cond_always,
3103                                         LIR_OprFact::intConst(tableswitch_count),
3104                                         reg_opr));
3105         inst-&gt;insert_before(start_insn + 1, new LIR_OpLabel(&amp;sw-&gt;_branches));
3106         tableswitch_count++;
3107       }
3108       reg = noreg;
3109       last_key = -2147483648;
3110     }
3111   next_state:
3112     ;
3113   }
3114 #endif
3115 }
3116 
3117 void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp_op) {
3118   Address addr = as_Address(src-&gt;as_address_ptr());
3119   BasicType type = src-&gt;type();
3120   bool is_oop = is_reference_type(type);
3121 
3122   void (MacroAssembler::* add)(Register prev, RegisterOrConstant incr, Register addr);
3123   void (MacroAssembler::* xchg)(Register prev, Register newv, Register addr);
3124 
3125   switch(type) {
3126   case T_INT:
3127     xchg = &amp;MacroAssembler::atomic_xchgalw;
3128     add = &amp;MacroAssembler::atomic_addalw;
3129     break;
3130   case T_LONG:
3131     xchg = &amp;MacroAssembler::atomic_xchgal;
3132     add = &amp;MacroAssembler::atomic_addal;
3133     break;
3134   case T_OBJECT:
3135   case T_ARRAY:
3136     if (UseCompressedOops) {
3137       xchg = &amp;MacroAssembler::atomic_xchgalw;
3138       add = &amp;MacroAssembler::atomic_addalw;
3139     } else {
3140       xchg = &amp;MacroAssembler::atomic_xchgal;
3141       add = &amp;MacroAssembler::atomic_addal;
3142     }
3143     break;
3144   default:
3145     ShouldNotReachHere();
3146     xchg = &amp;MacroAssembler::atomic_xchgal;
3147     add = &amp;MacroAssembler::atomic_addal; // unreachable
3148   }
3149 
3150   switch (code) {
3151   case lir_xadd:
3152     {
3153       RegisterOrConstant inc;
3154       Register tmp = as_reg(tmp_op);
3155       Register dst = as_reg(dest);
3156       if (data-&gt;is_constant()) {
3157         inc = RegisterOrConstant(as_long(data));
3158         assert_different_registers(dst, addr.base(), tmp,
3159                                    rscratch1, rscratch2);
3160       } else {
3161         inc = RegisterOrConstant(as_reg(data));
3162         assert_different_registers(inc.as_register(), dst, addr.base(), tmp,
3163                                    rscratch1, rscratch2);
3164       }
3165       __ lea(tmp, addr);
3166       (_masm-&gt;*add)(dst, inc, tmp);
3167       break;
3168     }
3169   case lir_xchg:
3170     {
3171       Register tmp = tmp_op-&gt;as_register();
3172       Register obj = as_reg(data);
3173       Register dst = as_reg(dest);
3174       if (is_oop &amp;&amp; UseCompressedOops) {
3175         __ encode_heap_oop(rscratch2, obj);
3176         obj = rscratch2;
3177       }
3178       assert_different_registers(obj, addr.base(), tmp, rscratch1, dst);
3179       __ lea(tmp, addr);
3180       (_masm-&gt;*xchg)(dst, obj, tmp);
3181       if (is_oop &amp;&amp; UseCompressedOops) {
3182         __ decode_heap_oop(dst);
3183       }
3184     }
3185     break;
3186   default:
3187     ShouldNotReachHere();
3188   }
3189   __ membar(__ AnyAny);
3190 }
3191 
3192 #undef __
    </pre>
  </body>
</html>