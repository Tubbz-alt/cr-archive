<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/cpu/s390/c1_LIRAssembler_s390.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2016, 2019, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2016, 2019, SAP SE. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;c1/c1_Compilation.hpp&quot;
  29 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  30 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  31 #include &quot;c1/c1_Runtime1.hpp&quot;
  32 #include &quot;c1/c1_ValueStack.hpp&quot;
  33 #include &quot;ci/ciArrayKlass.hpp&quot;
  34 #include &quot;ci/ciInstance.hpp&quot;
  35 #include &quot;gc/shared/collectedHeap.hpp&quot;
  36 #include &quot;memory/universe.hpp&quot;
  37 #include &quot;nativeInst_s390.hpp&quot;
  38 #include &quot;oops/objArrayKlass.hpp&quot;
  39 #include &quot;runtime/frame.inline.hpp&quot;
  40 #include &quot;runtime/safepointMechanism.inline.hpp&quot;
  41 #include &quot;runtime/sharedRuntime.hpp&quot;
  42 #include &quot;vmreg_s390.inline.hpp&quot;
  43 
  44 #define __ _masm-&gt;
  45 
  46 #ifndef PRODUCT
  47 #undef __
  48 #define __ (Verbose ? (_masm-&gt;block_comment(FILE_AND_LINE),_masm) : _masm)-&gt;
  49 #endif
  50 
  51 //------------------------------------------------------------
  52 
  53 bool LIR_Assembler::is_small_constant(LIR_Opr opr) {
  54   // Not used on ZARCH_64
  55   ShouldNotCallThis();
  56   return false;
  57 }
  58 
  59 LIR_Opr LIR_Assembler::receiverOpr() {
  60   return FrameMap::Z_R2_oop_opr;
  61 }
  62 
  63 LIR_Opr LIR_Assembler::osrBufferPointer() {
  64   return FrameMap::Z_R2_opr;
  65 }
  66 
  67 int LIR_Assembler::initial_frame_size_in_bytes() const {
  68   return in_bytes(frame_map()-&gt;framesize_in_bytes());
  69 }
  70 
  71 // Inline cache check: done before the frame is built.
  72 // The inline cached class is in Z_inline_cache(Z_R9).
  73 // We fetch the class of the receiver and compare it with the cached class.
  74 // If they do not match we jump to the slow case.
  75 int LIR_Assembler::check_icache() {
  76   Register receiver = receiverOpr()-&gt;as_register();
  77   int offset = __ offset();
  78   __ inline_cache_check(receiver, Z_inline_cache);
  79   return offset;
  80 }
  81 
  82 void LIR_Assembler::clinit_barrier(ciMethod* method) {
  83   assert(!method-&gt;holder()-&gt;is_not_initialized(), &quot;initialization should have been started&quot;);
  84 
  85   Label L_skip_barrier;
  86   Register klass = Z_R1_scratch;
  87 
  88   metadata2reg(method-&gt;holder()-&gt;constant_encoding(), klass);
  89   __ clinit_barrier(klass, Z_thread, &amp;L_skip_barrier /*L_fast_path*/);
  90 
  91   __ load_const_optimized(klass, SharedRuntime::get_handle_wrong_method_stub());
  92   __ z_br(klass);
  93 
  94   __ bind(L_skip_barrier);
  95 }
  96 
  97 void LIR_Assembler::osr_entry() {
  98   // On-stack-replacement entry sequence (interpreter frame layout described in interpreter_sparc.cpp):
  99   //
 100   //   1. Create a new compiled activation.
 101   //   2. Initialize local variables in the compiled activation. The expression stack must be empty
 102   //      at the osr_bci; it is not initialized.
 103   //   3. Jump to the continuation address in compiled code to resume execution.
 104 
 105   // OSR entry point
 106   offsets()-&gt;set_value(CodeOffsets::OSR_Entry, code_offset());
 107   BlockBegin* osr_entry = compilation()-&gt;hir()-&gt;osr_entry();
 108   ValueStack* entry_state = osr_entry-&gt;end()-&gt;state();
 109   int number_of_locks = entry_state-&gt;locks_size();
 110 
 111   // Create a frame for the compiled activation.
 112   __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());
 113 
 114   // OSR buffer is
 115   //
 116   // locals[nlocals-1..0]
 117   // monitors[number_of_locks-1..0]
 118   //
 119   // Locals is a direct copy of the interpreter frame so in the osr buffer
 120   // the first slot in the local array is the last local from the interpreter
 121   // and the last slot is local[0] (receiver) from the interpreter
 122   //
 123   // Similarly with locks. The first lock slot in the osr buffer is the nth lock
 124   // from the interpreter frame, the nth lock slot in the osr buffer is 0th lock
 125   // in the interpreter frame (the method lock if a sync method)
 126 
 127   // Initialize monitors in the compiled activation.
 128   //   I0: pointer to osr buffer
 129   //
 130   // All other registers are dead at this point and the locals will be
 131   // copied into place by code emitted in the IR.
 132 
 133   Register OSR_buf = osrBufferPointer()-&gt;as_register();
 134   { assert(frame::interpreter_frame_monitor_size() == BasicObjectLock::size(), &quot;adjust code below&quot;);
 135     int monitor_offset = BytesPerWord * method()-&gt;max_locals() +
 136       (2 * BytesPerWord) * (number_of_locks - 1);
 137     // SharedRuntime::OSR_migration_begin() packs BasicObjectLocks in
 138     // the OSR buffer using 2 word entries: first the lock and then
 139     // the oop.
 140     for (int i = 0; i &lt; number_of_locks; i++) {
 141       int slot_offset = monitor_offset - ((i * 2) * BytesPerWord);
 142       // Verify the interpreter&#39;s monitor has a non-null object.
 143       __ asm_assert_mem8_isnot_zero(slot_offset + 1*BytesPerWord, OSR_buf, &quot;locked object is NULL&quot;, __LINE__);
 144       // Copy the lock field into the compiled activation.
 145       __ z_lg(Z_R1_scratch, slot_offset + 0, OSR_buf);
 146       __ z_stg(Z_R1_scratch, frame_map()-&gt;address_for_monitor_lock(i));
 147       __ z_lg(Z_R1_scratch, slot_offset + 1*BytesPerWord, OSR_buf);
 148       __ z_stg(Z_R1_scratch, frame_map()-&gt;address_for_monitor_object(i));
 149     }
 150   }
 151 }
 152 
 153 // --------------------------------------------------------------------------------------------
 154 
 155 address LIR_Assembler::emit_call_c(address a) {
 156   __ align_call_far_patchable(__ pc());
 157   address call_addr = __ call_c_opt(a);
 158   if (call_addr == NULL) {
 159     bailout(&quot;const section overflow&quot;);
 160   }
 161   return call_addr;
 162 }
 163 
 164 int LIR_Assembler::emit_exception_handler() {
 165   // If the last instruction is a call (typically to do a throw which
 166   // is coming at the end after block reordering) the return address
 167   // must still point into the code area in order to avoid assertion
 168   // failures when searching for the corresponding bci. =&gt; Add a nop.
 169   // (was bug 5/14/1999 - gri)
 170   __ nop();
 171 
 172   // Generate code for exception handler.
 173   address handler_base = __ start_a_stub(exception_handler_size());
 174   if (handler_base == NULL) {
 175     // Not enough space left for the handler.
 176     bailout(&quot;exception handler overflow&quot;);
 177     return -1;
 178   }
 179 
 180   int offset = code_offset();
 181 
 182   address a = Runtime1::entry_for (Runtime1::handle_exception_from_callee_id);
 183   address call_addr = emit_call_c(a);
 184   CHECK_BAILOUT_(-1);
 185   __ should_not_reach_here();
 186   guarantee(code_offset() - offset &lt;= exception_handler_size(), &quot;overflow&quot;);
 187   __ end_a_stub();
 188 
 189   return offset;
 190 }
 191 
 192 // Emit the code to remove the frame from the stack in the exception
 193 // unwind path.
 194 int LIR_Assembler::emit_unwind_handler() {
 195 #ifndef PRODUCT
 196   if (CommentedAssembly) {
 197     _masm-&gt;block_comment(&quot;Unwind handler&quot;);
 198   }
 199 #endif
 200 
 201   int offset = code_offset();
 202   Register exception_oop_callee_saved = Z_R10; // Z_R10 is callee-saved.
 203   Register Rtmp1                      = Z_R11;
 204   Register Rtmp2                      = Z_R12;
 205 
 206   // Fetch the exception from TLS and clear out exception related thread state.
 207   Address exc_oop_addr = Address(Z_thread, JavaThread::exception_oop_offset());
 208   Address exc_pc_addr  = Address(Z_thread, JavaThread::exception_pc_offset());
 209   __ z_lg(Z_EXC_OOP, exc_oop_addr);
 210   __ clear_mem(exc_oop_addr, sizeof(oop));
 211   __ clear_mem(exc_pc_addr, sizeof(intptr_t));
 212 
 213   __ bind(_unwind_handler_entry);
 214   __ verify_not_null_oop(Z_EXC_OOP);
 215   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 216     __ lgr_if_needed(exception_oop_callee_saved, Z_EXC_OOP); // Preserve the exception.
 217   }
 218 
 219   // Preform needed unlocking.
 220   MonitorExitStub* stub = NULL;
 221   if (method()-&gt;is_synchronized()) {
 222     // Runtime1::monitorexit_id expects lock address in Z_R1_scratch.
 223     LIR_Opr lock = FrameMap::as_opr(Z_R1_scratch);
 224     monitor_address(0, lock);
 225     stub = new MonitorExitStub(lock, true, 0);
 226     __ unlock_object(Rtmp1, Rtmp2, lock-&gt;as_register(), *stub-&gt;entry());
 227     __ bind(*stub-&gt;continuation());
 228   }
 229 
 230   if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 231     ShouldNotReachHere(); // Not supported.
 232 #if 0
 233     __ mov(rdi, r15_thread);
 234     __ mov_metadata(rsi, method()-&gt;constant_encoding());
 235     __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit)));
 236 #endif
 237   }
 238 
 239   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 240     __ lgr_if_needed(Z_EXC_OOP, exception_oop_callee_saved);  // Restore the exception.
 241   }
 242 
 243   // Remove the activation and dispatch to the unwind handler.
 244   __ pop_frame();
 245   __ z_lg(Z_EXC_PC, _z_abi16(return_pc), Z_SP);
 246 
 247   // Z_EXC_OOP: exception oop
 248   // Z_EXC_PC: exception pc
 249 
 250   // Dispatch to the unwind logic.
 251   __ load_const_optimized(Z_R5, Runtime1::entry_for (Runtime1::unwind_exception_id));
 252   __ z_br(Z_R5);
 253 
 254   // Emit the slow path assembly.
 255   if (stub != NULL) {
 256     stub-&gt;emit_code(this);
 257   }
 258 
 259   return offset;
 260 }
 261 
 262 int LIR_Assembler::emit_deopt_handler() {
 263   // If the last instruction is a call (typically to do a throw which
 264   // is coming at the end after block reordering) the return address
 265   // must still point into the code area in order to avoid assertion
 266   // failures when searching for the corresponding bci. =&gt; Add a nop.
 267   // (was bug 5/14/1999 - gri)
 268   __ nop();
 269 
 270   // Generate code for exception handler.
 271   address handler_base = __ start_a_stub(deopt_handler_size());
 272   if (handler_base == NULL) {
 273     // Not enough space left for the handler.
 274     bailout(&quot;deopt handler overflow&quot;);
 275     return -1;
 276   }  int offset = code_offset();
 277   // Size must be constant (see HandlerImpl::emit_deopt_handler).
 278   __ load_const(Z_R1_scratch, SharedRuntime::deopt_blob()-&gt;unpack());
 279   __ call(Z_R1_scratch);
 280   guarantee(code_offset() - offset &lt;= deopt_handler_size(), &quot;overflow&quot;);
 281   __ end_a_stub();
 282 
 283   return offset;
 284 }
 285 
 286 void LIR_Assembler::jobject2reg(jobject o, Register reg) {
 287   if (o == NULL) {
 288     __ clear_reg(reg, true/*64bit*/, false/*set cc*/); // Must not kill cc set by cmove.
 289   } else {
 290     AddressLiteral a = __ allocate_oop_address(o);
 291     bool success = __ load_oop_from_toc(reg, a, reg);
 292     if (!success) {
 293       bailout(&quot;const section overflow&quot;);
 294     }
 295   }
 296 }
 297 
 298 void LIR_Assembler::jobject2reg_with_patching(Register reg, CodeEmitInfo *info) {
 299   // Allocate a new index in table to hold the object once it&#39;s been patched.
 300   int oop_index = __ oop_recorder()-&gt;allocate_oop_index(NULL);
 301   PatchingStub* patch = new PatchingStub(_masm, patching_id(info), oop_index);
 302 
 303   AddressLiteral addrlit((intptr_t)0, oop_Relocation::spec(oop_index));
 304   assert(addrlit.rspec().type() == relocInfo::oop_type, &quot;must be an oop reloc&quot;);
 305   // The NULL will be dynamically patched later so the sequence to
 306   // load the address literal must not be optimized.
 307   __ load_const(reg, addrlit);
 308 
 309   patching_epilog(patch, lir_patch_normal, reg, info);
 310 }
 311 
 312 void LIR_Assembler::metadata2reg(Metadata* md, Register reg) {
 313   bool success = __ set_metadata_constant(md, reg);
 314   if (!success) {
 315     bailout(&quot;const section overflow&quot;);
 316     return;
 317   }
 318 }
 319 
 320 void LIR_Assembler::klass2reg_with_patching(Register reg, CodeEmitInfo *info) {
 321   // Allocate a new index in table to hold the klass once it&#39;s been patched.
 322   int index = __ oop_recorder()-&gt;allocate_metadata_index(NULL);
 323   PatchingStub* patch = new PatchingStub(_masm, PatchingStub::load_klass_id, index);
 324   AddressLiteral addrlit((intptr_t)0, metadata_Relocation::spec(index));
 325   assert(addrlit.rspec().type() == relocInfo::metadata_type, &quot;must be an metadata reloc&quot;);
 326   // The NULL will be dynamically patched later so the sequence to
 327   // load the address literal must not be optimized.
 328   __ load_const(reg, addrlit);
 329 
 330   patching_epilog(patch, lir_patch_normal, reg, info);
 331 }
 332 
 333 void LIR_Assembler::emit_op3(LIR_Op3* op) {
 334   switch (op-&gt;code()) {
 335     case lir_idiv:
 336     case lir_irem:
 337       arithmetic_idiv(op-&gt;code(),
 338                       op-&gt;in_opr1(),
 339                       op-&gt;in_opr2(),
 340                       op-&gt;in_opr3(),
 341                       op-&gt;result_opr(),
 342                       op-&gt;info());
 343       break;
 344     case lir_fmad: {
 345       const FloatRegister opr1 = op-&gt;in_opr1()-&gt;as_double_reg(),
 346                           opr2 = op-&gt;in_opr2()-&gt;as_double_reg(),
 347                           opr3 = op-&gt;in_opr3()-&gt;as_double_reg(),
 348                           res  = op-&gt;result_opr()-&gt;as_double_reg();
 349       __ z_madbr(opr3, opr1, opr2);
 350       if (res != opr3) { __ z_ldr(res, opr3); }
 351     } break;
 352     case lir_fmaf: {
 353       const FloatRegister opr1 = op-&gt;in_opr1()-&gt;as_float_reg(),
 354                           opr2 = op-&gt;in_opr2()-&gt;as_float_reg(),
 355                           opr3 = op-&gt;in_opr3()-&gt;as_float_reg(),
 356                           res  = op-&gt;result_opr()-&gt;as_float_reg();
 357       __ z_maebr(opr3, opr1, opr2);
 358       if (res != opr3) { __ z_ler(res, opr3); }
 359     } break;
 360     default: ShouldNotReachHere(); break;
 361   }
 362 }
 363 
 364 
 365 void LIR_Assembler::emit_opBranch(LIR_OpBranch* op) {
 366 #ifdef ASSERT
 367   assert(op-&gt;block() == NULL || op-&gt;block()-&gt;label() == op-&gt;label(), &quot;wrong label&quot;);
 368   if (op-&gt;block() != NULL)  { _branch_target_blocks.append(op-&gt;block()); }
 369   if (op-&gt;ublock() != NULL) { _branch_target_blocks.append(op-&gt;ublock()); }
 370 #endif
 371 
 372   if (op-&gt;cond() == lir_cond_always) {
 373     if (op-&gt;info() != NULL) { add_debug_info_for_branch(op-&gt;info()); }
 374     __ branch_optimized(Assembler::bcondAlways, *(op-&gt;label()));
 375   } else {
 376     Assembler::branch_condition acond = Assembler::bcondZero;
 377     if (op-&gt;code() == lir_cond_float_branch) {
 378       assert(op-&gt;ublock() != NULL, &quot;must have unordered successor&quot;);
 379       __ branch_optimized(Assembler::bcondNotOrdered, *(op-&gt;ublock()-&gt;label()));
 380     }
 381     switch (op-&gt;cond()) {
 382       case lir_cond_equal:        acond = Assembler::bcondEqual;     break;
 383       case lir_cond_notEqual:     acond = Assembler::bcondNotEqual;  break;
 384       case lir_cond_less:         acond = Assembler::bcondLow;       break;
 385       case lir_cond_lessEqual:    acond = Assembler::bcondNotHigh;   break;
 386       case lir_cond_greaterEqual: acond = Assembler::bcondNotLow;    break;
 387       case lir_cond_greater:      acond = Assembler::bcondHigh;      break;
 388       case lir_cond_belowEqual:   acond = Assembler::bcondNotHigh;   break;
 389       case lir_cond_aboveEqual:   acond = Assembler::bcondNotLow;    break;
 390       default:                         ShouldNotReachHere();
 391     }
 392     __ branch_optimized(acond,*(op-&gt;label()));
 393   }
 394 }
 395 
 396 
 397 void LIR_Assembler::emit_opConvert(LIR_OpConvert* op) {
 398   LIR_Opr src  = op-&gt;in_opr();
 399   LIR_Opr dest = op-&gt;result_opr();
 400 
 401   switch (op-&gt;bytecode()) {
 402     case Bytecodes::_i2l:
 403       __ move_reg_if_needed(dest-&gt;as_register_lo(), T_LONG, src-&gt;as_register(), T_INT);
 404       break;
 405 
 406     case Bytecodes::_l2i:
 407       __ move_reg_if_needed(dest-&gt;as_register(), T_INT, src-&gt;as_register_lo(), T_LONG);
 408       break;
 409 
 410     case Bytecodes::_i2b:
 411       __ move_reg_if_needed(dest-&gt;as_register(), T_BYTE, src-&gt;as_register(), T_INT);
 412       break;
 413 
 414     case Bytecodes::_i2c:
 415       __ move_reg_if_needed(dest-&gt;as_register(), T_CHAR, src-&gt;as_register(), T_INT);
 416       break;
 417 
 418     case Bytecodes::_i2s:
 419       __ move_reg_if_needed(dest-&gt;as_register(), T_SHORT, src-&gt;as_register(), T_INT);
 420       break;
 421 
 422     case Bytecodes::_f2d:
 423       assert(dest-&gt;is_double_fpu(), &quot;check&quot;);
 424       __ move_freg_if_needed(dest-&gt;as_double_reg(), T_DOUBLE, src-&gt;as_float_reg(), T_FLOAT);
 425       break;
 426 
 427     case Bytecodes::_d2f:
 428       assert(dest-&gt;is_single_fpu(), &quot;check&quot;);
 429       __ move_freg_if_needed(dest-&gt;as_float_reg(), T_FLOAT, src-&gt;as_double_reg(), T_DOUBLE);
 430       break;
 431 
 432     case Bytecodes::_i2f:
 433       __ z_cefbr(dest-&gt;as_float_reg(), src-&gt;as_register());
 434       break;
 435 
 436     case Bytecodes::_i2d:
 437       __ z_cdfbr(dest-&gt;as_double_reg(), src-&gt;as_register());
 438       break;
 439 
 440     case Bytecodes::_l2f:
 441       __ z_cegbr(dest-&gt;as_float_reg(), src-&gt;as_register_lo());
 442       break;
 443     case Bytecodes::_l2d:
 444       __ z_cdgbr(dest-&gt;as_double_reg(), src-&gt;as_register_lo());
 445       break;
 446 
 447     case Bytecodes::_f2i:
 448     case Bytecodes::_f2l: {
 449       Label done;
 450       FloatRegister Rsrc = src-&gt;as_float_reg();
 451       Register Rdst = (op-&gt;bytecode() == Bytecodes::_f2i ? dest-&gt;as_register() : dest-&gt;as_register_lo());
 452       __ clear_reg(Rdst, true, false);
 453       __ z_cebr(Rsrc, Rsrc);
 454       __ z_brno(done); // NaN -&gt; 0
 455       if (op-&gt;bytecode() == Bytecodes::_f2i) {
 456         __ z_cfebr(Rdst, Rsrc, Assembler::to_zero);
 457       } else { // op-&gt;bytecode() == Bytecodes::_f2l
 458         __ z_cgebr(Rdst, Rsrc, Assembler::to_zero);
 459       }
 460       __ bind(done);
 461     }
 462     break;
 463 
 464     case Bytecodes::_d2i:
 465     case Bytecodes::_d2l: {
 466       Label done;
 467       FloatRegister Rsrc = src-&gt;as_double_reg();
 468       Register Rdst = (op-&gt;bytecode() == Bytecodes::_d2i ? dest-&gt;as_register() : dest-&gt;as_register_lo());
 469       __ clear_reg(Rdst, true, false);  // Don&#39;t set CC.
 470       __ z_cdbr(Rsrc, Rsrc);
 471       __ z_brno(done); // NaN -&gt; 0
 472       if (op-&gt;bytecode() == Bytecodes::_d2i) {
 473         __ z_cfdbr(Rdst, Rsrc, Assembler::to_zero);
 474       } else { // Bytecodes::_d2l
 475         __ z_cgdbr(Rdst, Rsrc, Assembler::to_zero);
 476       }
 477       __ bind(done);
 478     }
 479     break;
 480 
 481     default: ShouldNotReachHere();
 482   }
 483 }
 484 
 485 void LIR_Assembler::align_call(LIR_Code code) {
 486   // End of call instruction must be 4 byte aligned.
 487   int offset = __ offset();
 488   switch (code) {
 489     case lir_icvirtual_call:
 490       offset += MacroAssembler::load_const_from_toc_size();
 491       // no break
 492     case lir_static_call:
 493     case lir_optvirtual_call:
 494     case lir_dynamic_call:
 495       offset += NativeCall::call_far_pcrelative_displacement_offset;
 496       break;
 497     case lir_virtual_call:   // currently, sparc-specific for niagara
 498     default: ShouldNotReachHere();
 499   }
 500   if ((offset &amp; (NativeCall::call_far_pcrelative_displacement_alignment-1)) != 0) {
 501     __ nop();
 502   }
 503 }
 504 
 505 void LIR_Assembler::call(LIR_OpJavaCall* op, relocInfo::relocType rtype) {
 506   assert((__ offset() + NativeCall::call_far_pcrelative_displacement_offset) % NativeCall::call_far_pcrelative_displacement_alignment == 0,
 507          &quot;must be aligned (offset=%d)&quot;, __ offset());
 508   assert(rtype == relocInfo::none ||
 509          rtype == relocInfo::opt_virtual_call_type ||
 510          rtype == relocInfo::static_call_type, &quot;unexpected rtype&quot;);
 511   // Prepend each BRASL with a nop.
 512   __ relocate(rtype);
 513   __ z_nop();
 514   __ z_brasl(Z_R14, op-&gt;addr());
 515   add_call_info(code_offset(), op-&gt;info());
 516 }
 517 
 518 void LIR_Assembler::ic_call(LIR_OpJavaCall* op) {
 519   address virtual_call_oop_addr = NULL;
 520   AddressLiteral empty_ic((address) Universe::non_oop_word());
 521   virtual_call_oop_addr = __ pc();
 522   bool success = __ load_const_from_toc(Z_inline_cache, empty_ic);
 523   if (!success) {
 524     bailout(&quot;const section overflow&quot;);
 525     return;
 526   }
 527 
 528   // CALL to fixup routine. Fixup routine uses ScopeDesc info
 529   // to determine who we intended to call.
 530   __ relocate(virtual_call_Relocation::spec(virtual_call_oop_addr));
 531   call(op, relocInfo::none);
 532 }
 533 
 534 // not supported
 535 void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {
 536   ShouldNotReachHere();
 537 }
 538 
 539 void LIR_Assembler::move_regs(Register from_reg, Register to_reg) {
 540   if (from_reg != to_reg) __ z_lgr(to_reg, from_reg);
 541 }
 542 
 543 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
 544   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 545   assert(dest-&gt;is_stack(), &quot;should not call otherwise&quot;);
 546   LIR_Const* c = src-&gt;as_constant_ptr();
 547 
 548   unsigned int lmem = 0;
 549   unsigned int lcon = 0;
 550   int64_t cbits = 0;
 551   Address dest_addr;
 552   switch (c-&gt;type()) {
 553     case T_INT:  // fall through
 554     case T_FLOAT:
 555       dest_addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 556       lmem = 4; lcon = 4; cbits = c-&gt;as_jint_bits();
 557       break;
 558 
 559     case T_ADDRESS:
 560       dest_addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 561       lmem = 8; lcon = 4; cbits = c-&gt;as_jint_bits();
 562       break;
 563 
 564     case T_OBJECT:
 565       dest_addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 566       if (c-&gt;as_jobject() == NULL) {
 567         __ store_const(dest_addr, (int64_t)NULL_WORD, 8, 8);
 568       } else {
 569         jobject2reg(c-&gt;as_jobject(), Z_R1_scratch);
 570         __ reg2mem_opt(Z_R1_scratch, dest_addr, true);
 571       }
 572       return;
 573 
 574     case T_LONG:  // fall through
 575     case T_DOUBLE:
 576       dest_addr = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
 577       lmem = 8; lcon = 8; cbits = (int64_t)(c-&gt;as_jlong_bits());
 578       break;
 579 
 580     default:
 581       ShouldNotReachHere();
 582   }
 583 
 584   __ store_const(dest_addr, cbits, lmem, lcon);
 585 }
 586 
 587 void LIR_Assembler::const2mem(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info, bool wide) {
 588   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 589   assert(dest-&gt;is_address(), &quot;should not call otherwise&quot;);
 590 
 591   LIR_Const* c = src-&gt;as_constant_ptr();
 592   Address addr = as_Address(dest-&gt;as_address_ptr());
 593 
 594   int store_offset = -1;
 595 
 596   if (dest-&gt;as_address_ptr()-&gt;index()-&gt;is_valid()) {
 597     switch (type) {
 598       case T_INT:    // fall through
 599       case T_FLOAT:
 600         __ load_const_optimized(Z_R0_scratch, c-&gt;as_jint_bits());
 601         store_offset = __ offset();
 602         if (Immediate::is_uimm12(addr.disp())) {
 603           __ z_st(Z_R0_scratch, addr);
 604         } else {
 605           __ z_sty(Z_R0_scratch, addr);
 606         }
 607         break;
 608 
 609       case T_ADDRESS:
 610         __ load_const_optimized(Z_R1_scratch, c-&gt;as_jint_bits());
 611         store_offset = __ reg2mem_opt(Z_R1_scratch, addr, true);
 612         break;
 613 
 614       case T_OBJECT:  // fall through
 615       case T_ARRAY:
 616         if (c-&gt;as_jobject() == NULL) {
 617           if (UseCompressedOops &amp;&amp; !wide) {
 618             __ clear_reg(Z_R1_scratch, false);
 619             store_offset = __ reg2mem_opt(Z_R1_scratch, addr, false);
 620           } else {
 621             __ clear_reg(Z_R1_scratch, true);
 622             store_offset = __ reg2mem_opt(Z_R1_scratch, addr, true);
 623           }
 624         } else {
 625           jobject2reg(c-&gt;as_jobject(), Z_R1_scratch);
 626           if (UseCompressedOops &amp;&amp; !wide) {
 627             __ encode_heap_oop(Z_R1_scratch);
 628             store_offset = __ reg2mem_opt(Z_R1_scratch, addr, false);
 629           } else {
 630             store_offset = __ reg2mem_opt(Z_R1_scratch, addr, true);
 631           }
 632         }
 633         assert(store_offset &gt;= 0, &quot;check&quot;);
 634         break;
 635 
 636       case T_LONG:    // fall through
 637       case T_DOUBLE:
 638         __ load_const_optimized(Z_R1_scratch, (int64_t)(c-&gt;as_jlong_bits()));
 639         store_offset = __ reg2mem_opt(Z_R1_scratch, addr, true);
 640         break;
 641 
 642       case T_BOOLEAN: // fall through
 643       case T_BYTE:
 644         __ load_const_optimized(Z_R0_scratch, (int8_t)(c-&gt;as_jint()));
 645         store_offset = __ offset();
 646         if (Immediate::is_uimm12(addr.disp())) {
 647           __ z_stc(Z_R0_scratch, addr);
 648         } else {
 649           __ z_stcy(Z_R0_scratch, addr);
 650         }
 651         break;
 652 
 653       case T_CHAR:    // fall through
 654       case T_SHORT:
 655         __ load_const_optimized(Z_R0_scratch, (int16_t)(c-&gt;as_jint()));
 656         store_offset = __ offset();
 657         if (Immediate::is_uimm12(addr.disp())) {
 658           __ z_sth(Z_R0_scratch, addr);
 659         } else {
 660           __ z_sthy(Z_R0_scratch, addr);
 661         }
 662         break;
 663 
 664       default:
 665         ShouldNotReachHere();
 666     }
 667 
 668   } else { // no index
 669 
 670     unsigned int lmem = 0;
 671     unsigned int lcon = 0;
 672     int64_t cbits = 0;
 673 
 674     switch (type) {
 675       case T_INT:    // fall through
 676       case T_FLOAT:
 677         lmem = 4; lcon = 4; cbits = c-&gt;as_jint_bits();
 678         break;
 679 
 680       case T_ADDRESS:
 681         lmem = 8; lcon = 4; cbits = c-&gt;as_jint_bits();
 682         break;
 683 
 684       case T_OBJECT:  // fall through
 685       case T_ARRAY:
 686         if (c-&gt;as_jobject() == NULL) {
 687           if (UseCompressedOops &amp;&amp; !wide) {
 688             store_offset = __ store_const(addr, (int32_t)NULL_WORD, 4, 4);
 689           } else {
 690             store_offset = __ store_const(addr, (int64_t)NULL_WORD, 8, 8);
 691           }
 692         } else {
 693           jobject2reg(c-&gt;as_jobject(), Z_R1_scratch);
 694           if (UseCompressedOops &amp;&amp; !wide) {
 695             __ encode_heap_oop(Z_R1_scratch);
 696             store_offset = __ reg2mem_opt(Z_R1_scratch, addr, false);
 697           } else {
 698             store_offset = __ reg2mem_opt(Z_R1_scratch, addr, true);
 699           }
 700         }
 701         assert(store_offset &gt;= 0, &quot;check&quot;);
 702         break;
 703 
 704       case T_LONG:    // fall through
 705       case T_DOUBLE:
 706         lmem = 8; lcon = 8; cbits = (int64_t)(c-&gt;as_jlong_bits());
 707         break;
 708 
 709       case T_BOOLEAN: // fall through
 710       case T_BYTE:
 711         lmem = 1; lcon = 1; cbits = (int8_t)(c-&gt;as_jint());
 712         break;
 713 
 714       case T_CHAR:    // fall through
 715       case T_SHORT:
 716         lmem = 2; lcon = 2; cbits = (int16_t)(c-&gt;as_jint());
 717         break;
 718 
 719       default:
 720         ShouldNotReachHere();
 721     }
 722 
 723     if (store_offset == -1) {
 724       store_offset = __ store_const(addr, cbits, lmem, lcon);
 725       assert(store_offset &gt;= 0, &quot;check&quot;);
 726     }
 727   }
 728 
 729   if (info != NULL) {
 730     add_debug_info_for_null_check(store_offset, info);
 731   }
 732 }
 733 
 734 void LIR_Assembler::const2reg(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
 735   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 736   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 737   LIR_Const* c = src-&gt;as_constant_ptr();
 738 
 739   switch (c-&gt;type()) {
 740     case T_INT: {
 741       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 742       __ load_const_optimized(dest-&gt;as_register(), c-&gt;as_jint());
 743       break;
 744     }
 745 
 746     case T_ADDRESS: {
 747       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 748       __ load_const_optimized(dest-&gt;as_register(), c-&gt;as_jint());
 749       break;
 750     }
 751 
 752     case T_LONG: {
 753       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 754       __ load_const_optimized(dest-&gt;as_register_lo(), (intptr_t)c-&gt;as_jlong());
 755       break;
 756     }
 757 
 758     case T_OBJECT: {
 759       if (patch_code != lir_patch_none) {
 760         jobject2reg_with_patching(dest-&gt;as_register(), info);
 761       } else {
 762         jobject2reg(c-&gt;as_jobject(), dest-&gt;as_register());
 763       }
 764       break;
 765     }
 766 
 767     case T_METADATA: {
 768       if (patch_code != lir_patch_none) {
 769         klass2reg_with_patching(dest-&gt;as_register(), info);
 770       } else {
 771         metadata2reg(c-&gt;as_metadata(), dest-&gt;as_register());
 772       }
 773       break;
 774     }
 775 
 776     case T_FLOAT: {
 777       Register toc_reg = Z_R1_scratch;
 778       __ load_toc(toc_reg);
 779       address const_addr = __ float_constant(c-&gt;as_jfloat());
 780       if (const_addr == NULL) {
 781         bailout(&quot;const section overflow&quot;);
 782         break;
 783       }
 784       int displ = const_addr - _masm-&gt;code()-&gt;consts()-&gt;start();
 785       if (dest-&gt;is_single_fpu()) {
 786         __ z_ley(dest-&gt;as_float_reg(), displ, toc_reg);
 787       } else {
 788         assert(dest-&gt;is_single_cpu(), &quot;Must be a cpu register.&quot;);
 789         __ z_ly(dest-&gt;as_register(), displ, toc_reg);
 790       }
 791     }
 792     break;
 793 
 794     case T_DOUBLE: {
 795       Register toc_reg = Z_R1_scratch;
 796       __ load_toc(toc_reg);
 797       address const_addr = __ double_constant(c-&gt;as_jdouble());
 798       if (const_addr == NULL) {
 799         bailout(&quot;const section overflow&quot;);
 800         break;
 801       }
 802       int displ = const_addr - _masm-&gt;code()-&gt;consts()-&gt;start();
 803       if (dest-&gt;is_double_fpu()) {
 804         __ z_ldy(dest-&gt;as_double_reg(), displ, toc_reg);
 805       } else {
 806         assert(dest-&gt;is_double_cpu(), &quot;Must be a long register.&quot;);
 807         __ z_lg(dest-&gt;as_register_lo(), displ, toc_reg);
 808       }
 809     }
 810     break;
 811 
 812     default:
 813       ShouldNotReachHere();
 814   }
 815 }
 816 
 817 Address LIR_Assembler::as_Address(LIR_Address* addr) {
 818   if (addr-&gt;base()-&gt;is_illegal()) {
 819     Unimplemented();
 820   }
 821 
 822   Register base = addr-&gt;base()-&gt;as_pointer_register();
 823 
 824   if (addr-&gt;index()-&gt;is_illegal()) {
 825     return Address(base, addr-&gt;disp());
 826   } else if (addr-&gt;index()-&gt;is_cpu_register()) {
 827     Register index = addr-&gt;index()-&gt;as_pointer_register();
 828     return Address(base, index, addr-&gt;disp());
 829   } else if (addr-&gt;index()-&gt;is_constant()) {
 830     intptr_t addr_offset = addr-&gt;index()-&gt;as_constant_ptr()-&gt;as_jint() + addr-&gt;disp();
 831     return Address(base, addr_offset);
 832   } else {
 833     ShouldNotReachHere();
 834     return Address();
 835   }
 836 }
 837 
 838 void LIR_Assembler::stack2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {
 839   switch (type) {
 840     case T_INT:
 841     case T_FLOAT: {
 842       Register tmp = Z_R1_scratch;
 843       Address from = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
 844       Address to   = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 845       __ mem2reg_opt(tmp, from, false);
 846       __ reg2mem_opt(tmp, to, false);
 847       break;
 848     }
 849     case T_ADDRESS:
 850     case T_OBJECT: {
 851       Register tmp = Z_R1_scratch;
 852       Address from = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
 853       Address to   = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 854       __ mem2reg_opt(tmp, from, true);
 855       __ reg2mem_opt(tmp, to, true);
 856       break;
 857     }
 858     case T_LONG:
 859     case T_DOUBLE: {
 860       Register tmp = Z_R1_scratch;
 861       Address from = frame_map()-&gt;address_for_double_slot(src-&gt;double_stack_ix());
 862       Address to   = frame_map()-&gt;address_for_double_slot(dest-&gt;double_stack_ix());
 863       __ mem2reg_opt(tmp, from, true);
 864       __ reg2mem_opt(tmp, to, true);
 865       break;
 866     }
 867 
 868     default:
 869       ShouldNotReachHere();
 870   }
 871 }
 872 
 873 // 4-byte accesses only! Don&#39;t use it to access 8 bytes!
 874 Address LIR_Assembler::as_Address_hi(LIR_Address* addr) {
 875   ShouldNotCallThis();
 876   return 0; // unused
 877 }
 878 
 879 // 4-byte accesses only! Don&#39;t use it to access 8 bytes!
 880 Address LIR_Assembler::as_Address_lo(LIR_Address* addr) {
 881   ShouldNotCallThis();
 882   return 0; // unused
 883 }
 884 
 885 void LIR_Assembler::mem2reg(LIR_Opr src_opr, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code,
 886                             CodeEmitInfo* info, bool wide, bool unaligned) {
 887 
 888   assert(type != T_METADATA, &quot;load of metadata ptr not supported&quot;);
 889   LIR_Address* addr = src_opr-&gt;as_address_ptr();
 890   LIR_Opr to_reg = dest;
 891 
 892   Register src = addr-&gt;base()-&gt;as_pointer_register();
 893   Register disp_reg = Z_R0;
 894   int disp_value = addr-&gt;disp();
 895   bool needs_patching = (patch_code != lir_patch_none);
 896 
 897   if (addr-&gt;base()-&gt;type() == T_OBJECT) {
 898     __ verify_oop(src, FILE_AND_LINE);
 899   }
 900 
 901   PatchingStub* patch = NULL;
 902   if (needs_patching) {
 903     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
 904     assert(!to_reg-&gt;is_double_cpu() ||
 905            patch_code == lir_patch_none ||
 906            patch_code == lir_patch_normal, &quot;patching doesn&#39;t match register&quot;);
 907   }
 908 
 909   if (addr-&gt;index()-&gt;is_illegal()) {
 910     if (!Immediate::is_simm20(disp_value)) {
 911       if (needs_patching) {
 912         __ load_const(Z_R1_scratch, (intptr_t)0);
 913       } else {
 914         __ load_const_optimized(Z_R1_scratch, disp_value);
 915       }
 916       disp_reg = Z_R1_scratch;
 917       disp_value = 0;
 918     }
 919   } else {
 920     if (!Immediate::is_simm20(disp_value)) {
 921       __ load_const_optimized(Z_R1_scratch, disp_value);
 922       __ z_la(Z_R1_scratch, 0, Z_R1_scratch, addr-&gt;index()-&gt;as_register());
 923       disp_reg = Z_R1_scratch;
 924       disp_value = 0;
 925     }
 926     disp_reg = addr-&gt;index()-&gt;as_pointer_register();
 927   }
 928 
 929   // Remember the offset of the load. The patching_epilog must be done
 930   // before the call to add_debug_info, otherwise the PcDescs don&#39;t get
 931   // entered in increasing order.
 932   int offset = code_offset();
 933 
 934   assert(disp_reg != Z_R0 || Immediate::is_simm20(disp_value), &quot;should have set this up&quot;);
 935 
 936   bool short_disp = Immediate::is_uimm12(disp_value);
 937 
 938   switch (type) {
 939     case T_BOOLEAN: // fall through
 940     case T_BYTE  :  __ z_lb(dest-&gt;as_register(),   disp_value, disp_reg, src); break;
 941     case T_CHAR  :  __ z_llgh(dest-&gt;as_register(), disp_value, disp_reg, src); break;
 942     case T_SHORT :
 943       if (short_disp) {
 944                     __ z_lh(dest-&gt;as_register(),   disp_value, disp_reg, src);
 945       } else {
 946                     __ z_lhy(dest-&gt;as_register(),  disp_value, disp_reg, src);
 947       }
 948       break;
 949     case T_INT   :
 950       if (short_disp) {
 951                     __ z_l(dest-&gt;as_register(),    disp_value, disp_reg, src);
 952       } else {
 953                     __ z_ly(dest-&gt;as_register(),   disp_value, disp_reg, src);
 954       }
 955       break;
 956     case T_ADDRESS:
 957       if (UseCompressedClassPointers &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
 958         __ z_llgf(dest-&gt;as_register(), disp_value, disp_reg, src);
 959         __ decode_klass_not_null(dest-&gt;as_register());
 960       } else {
 961         __ z_lg(dest-&gt;as_register(), disp_value, disp_reg, src);
 962       }
 963       break;
 964     case T_ARRAY : // fall through
 965     case T_OBJECT:
 966     {
 967       if (UseCompressedOops &amp;&amp; !wide) {
 968         __ z_llgf(dest-&gt;as_register(), disp_value, disp_reg, src);
 969         __ oop_decoder(dest-&gt;as_register(), dest-&gt;as_register(), true);
 970       } else {
 971         __ z_lg(dest-&gt;as_register(), disp_value, disp_reg, src);
 972       }
 973       __ verify_oop(dest-&gt;as_register(), FILE_AND_LINE);
 974       break;
 975     }
 976     case T_FLOAT:
 977       if (short_disp) {
 978                     __ z_le(dest-&gt;as_float_reg(),  disp_value, disp_reg, src);
 979       } else {
 980                     __ z_ley(dest-&gt;as_float_reg(), disp_value, disp_reg, src);
 981       }
 982       break;
 983     case T_DOUBLE:
 984       if (short_disp) {
 985                     __ z_ld(dest-&gt;as_double_reg(),  disp_value, disp_reg, src);
 986       } else {
 987                     __ z_ldy(dest-&gt;as_double_reg(), disp_value, disp_reg, src);
 988       }
 989       break;
 990     case T_LONG  :  __ z_lg(dest-&gt;as_register_lo(), disp_value, disp_reg, src); break;
 991     default      : ShouldNotReachHere();
 992   }
 993 
 994   if (patch != NULL) {
 995     patching_epilog(patch, patch_code, src, info);
 996   }
 997   if (info != NULL) add_debug_info_for_null_check(offset, info);
 998 }
 999 
1000 void LIR_Assembler::stack2reg(LIR_Opr src, LIR_Opr dest, BasicType type) {
1001   assert(src-&gt;is_stack(), &quot;should not call otherwise&quot;);
1002   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
1003 
1004   if (dest-&gt;is_single_cpu()) {
1005     if (is_reference_type(type)) {
1006       __ mem2reg_opt(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()), true);
1007       __ verify_oop(dest-&gt;as_register(), FILE_AND_LINE);
1008     } else if (type == T_METADATA || type == T_ADDRESS) {
1009       __ mem2reg_opt(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()), true);
1010     } else {
1011       __ mem2reg_opt(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()), false);
1012     }
1013   } else if (dest-&gt;is_double_cpu()) {
1014     Address src_addr_LO = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
1015     __ mem2reg_opt(dest-&gt;as_register_lo(), src_addr_LO, true);
1016   } else if (dest-&gt;is_single_fpu()) {
1017     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
1018     __ mem2freg_opt(dest-&gt;as_float_reg(), src_addr, false);
1019   } else if (dest-&gt;is_double_fpu()) {
1020     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
1021     __ mem2freg_opt(dest-&gt;as_double_reg(), src_addr, true);
1022   } else {
1023     ShouldNotReachHere();
1024   }
1025 }
1026 
1027 void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {
1028   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
1029   assert(dest-&gt;is_stack(), &quot;should not call otherwise&quot;);
1030 
1031   if (src-&gt;is_single_cpu()) {
1032     const Address dst = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
1033     if (is_reference_type(type)) {
1034       __ verify_oop(src-&gt;as_register(), FILE_AND_LINE);
1035       __ reg2mem_opt(src-&gt;as_register(), dst, true);
1036     } else if (type == T_METADATA || type == T_ADDRESS) {
1037       __ reg2mem_opt(src-&gt;as_register(), dst, true);
1038     } else {
1039       __ reg2mem_opt(src-&gt;as_register(), dst, false);
1040     }
1041   } else if (src-&gt;is_double_cpu()) {
1042     Address dstLO = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
1043     __ reg2mem_opt(src-&gt;as_register_lo(), dstLO, true);
1044   } else if (src-&gt;is_single_fpu()) {
1045     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
1046     __ freg2mem_opt(src-&gt;as_float_reg(), dst_addr, false);
1047   } else if (src-&gt;is_double_fpu()) {
1048     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
1049     __ freg2mem_opt(src-&gt;as_double_reg(), dst_addr, true);
1050   } else {
1051     ShouldNotReachHere();
1052   }
1053 }
1054 
1055 void LIR_Assembler::reg2reg(LIR_Opr from_reg, LIR_Opr to_reg) {
1056   if (from_reg-&gt;is_float_kind() &amp;&amp; to_reg-&gt;is_float_kind()) {
1057     if (from_reg-&gt;is_double_fpu()) {
1058       // double to double moves
1059       assert(to_reg-&gt;is_double_fpu(), &quot;should match&quot;);
1060       __ z_ldr(to_reg-&gt;as_double_reg(), from_reg-&gt;as_double_reg());
1061     } else {
1062       // float to float moves
1063       assert(to_reg-&gt;is_single_fpu(), &quot;should match&quot;);
1064       __ z_ler(to_reg-&gt;as_float_reg(), from_reg-&gt;as_float_reg());
1065     }
1066   } else if (!from_reg-&gt;is_float_kind() &amp;&amp; !to_reg-&gt;is_float_kind()) {
1067     if (from_reg-&gt;is_double_cpu()) {
1068       __ z_lgr(to_reg-&gt;as_pointer_register(), from_reg-&gt;as_pointer_register());
1069     } else if (to_reg-&gt;is_double_cpu()) {
1070       // int to int moves
1071       __ z_lgr(to_reg-&gt;as_register_lo(), from_reg-&gt;as_register());
1072     } else {
1073       // int to int moves
1074       __ z_lgr(to_reg-&gt;as_register(), from_reg-&gt;as_register());
1075     }
1076   } else {
1077     ShouldNotReachHere();
1078   }
1079   if (is_reference_type(to_reg-&gt;type())) {
1080     __ verify_oop(to_reg-&gt;as_register(), FILE_AND_LINE);
1081   }
1082 }
1083 
1084 void LIR_Assembler::reg2mem(LIR_Opr from, LIR_Opr dest_opr, BasicType type,
1085                             LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack,
1086                             bool wide, bool unaligned) {
1087   assert(type != T_METADATA, &quot;store of metadata ptr not supported&quot;);
1088   LIR_Address* addr = dest_opr-&gt;as_address_ptr();
1089 
1090   Register dest = addr-&gt;base()-&gt;as_pointer_register();
1091   Register disp_reg = Z_R0;
1092   int disp_value = addr-&gt;disp();
1093   bool needs_patching = (patch_code != lir_patch_none);
1094 
1095   if (addr-&gt;base()-&gt;is_oop_register()) {
1096     __ verify_oop(dest, FILE_AND_LINE);
1097   }
1098 
1099   PatchingStub* patch = NULL;
1100   if (needs_patching) {
1101     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
1102     assert(!from-&gt;is_double_cpu() ||
1103            patch_code == lir_patch_none ||
1104            patch_code == lir_patch_normal, &quot;patching doesn&#39;t match register&quot;);
1105   }
1106 
1107   assert(!needs_patching || (!Immediate::is_simm20(disp_value) &amp;&amp; addr-&gt;index()-&gt;is_illegal()), &quot;assumption&quot;);
1108   if (addr-&gt;index()-&gt;is_illegal()) {
1109     if (!Immediate::is_simm20(disp_value)) {
1110       if (needs_patching) {
1111         __ load_const(Z_R1_scratch, (intptr_t)0);
1112       } else {
1113         __ load_const_optimized(Z_R1_scratch, disp_value);
1114       }
1115       disp_reg = Z_R1_scratch;
1116       disp_value = 0;
1117     }
1118   } else {
1119     if (!Immediate::is_simm20(disp_value)) {
1120       __ load_const_optimized(Z_R1_scratch, disp_value);
1121       __ z_la(Z_R1_scratch, 0, Z_R1_scratch, addr-&gt;index()-&gt;as_register());
1122       disp_reg = Z_R1_scratch;
1123       disp_value = 0;
1124     }
1125     disp_reg = addr-&gt;index()-&gt;as_pointer_register();
1126   }
1127 
1128   assert(disp_reg != Z_R0 || Immediate::is_simm20(disp_value), &quot;should have set this up&quot;);
1129 
1130   if (is_reference_type(type)) {
1131     __ verify_oop(from-&gt;as_register(), FILE_AND_LINE);
1132   }
1133 
1134   bool short_disp = Immediate::is_uimm12(disp_value);
1135 
1136   // Remember the offset of the store. The patching_epilog must be done
1137   // before the call to add_debug_info_for_null_check, otherwise the PcDescs don&#39;t get
1138   // entered in increasing order.
1139   int offset = code_offset();
1140   switch (type) {
1141     case T_BOOLEAN: // fall through
1142     case T_BYTE  :
1143       if (short_disp) {
1144                     __ z_stc(from-&gt;as_register(),  disp_value, disp_reg, dest);
1145       } else {
1146                     __ z_stcy(from-&gt;as_register(), disp_value, disp_reg, dest);
1147       }
1148       break;
1149     case T_CHAR  : // fall through
1150     case T_SHORT :
1151       if (short_disp) {
1152                     __ z_sth(from-&gt;as_register(),  disp_value, disp_reg, dest);
1153       } else {
1154                     __ z_sthy(from-&gt;as_register(), disp_value, disp_reg, dest);
1155       }
1156       break;
1157     case T_INT   :
1158       if (short_disp) {
1159                     __ z_st(from-&gt;as_register(),  disp_value, disp_reg, dest);
1160       } else {
1161                     __ z_sty(from-&gt;as_register(), disp_value, disp_reg, dest);
1162       }
1163       break;
1164     case T_LONG  :  __ z_stg(from-&gt;as_register_lo(), disp_value, disp_reg, dest); break;
1165     case T_ADDRESS: __ z_stg(from-&gt;as_register(),    disp_value, disp_reg, dest); break;
1166       break;
1167     case T_ARRAY : // fall through
1168     case T_OBJECT:
1169       {
1170         if (UseCompressedOops &amp;&amp; !wide) {
1171           Register compressed_src = Z_R14;
1172           __ oop_encoder(compressed_src, from-&gt;as_register(), true, (disp_reg != Z_R1) ? Z_R1 : Z_R0, -1, true);
1173           offset = code_offset();
1174           if (short_disp) {
1175             __ z_st(compressed_src,  disp_value, disp_reg, dest);
1176           } else {
1177             __ z_sty(compressed_src, disp_value, disp_reg, dest);
1178           }
1179         } else {
1180           __ z_stg(from-&gt;as_register(), disp_value, disp_reg, dest);
1181         }
1182         break;
1183       }
1184     case T_FLOAT :
1185       if (short_disp) {
1186         __ z_ste(from-&gt;as_float_reg(),  disp_value, disp_reg, dest);
1187       } else {
1188         __ z_stey(from-&gt;as_float_reg(), disp_value, disp_reg, dest);
1189       }
1190       break;
1191     case T_DOUBLE:
1192       if (short_disp) {
1193         __ z_std(from-&gt;as_double_reg(),  disp_value, disp_reg, dest);
1194       } else {
1195         __ z_stdy(from-&gt;as_double_reg(), disp_value, disp_reg, dest);
1196       }
1197       break;
1198     default: ShouldNotReachHere();
1199   }
1200 
1201   if (patch != NULL) {
1202     patching_epilog(patch, patch_code, dest, info);
1203   }
1204 
1205   if (info != NULL) add_debug_info_for_null_check(offset, info);
1206 }
1207 
1208 
1209 void LIR_Assembler::return_op(LIR_Opr result) {
1210   assert(result-&gt;is_illegal() ||
1211          (result-&gt;is_single_cpu() &amp;&amp; result-&gt;as_register() == Z_R2) ||
1212          (result-&gt;is_double_cpu() &amp;&amp; result-&gt;as_register_lo() == Z_R2) ||
1213          (result-&gt;is_single_fpu() &amp;&amp; result-&gt;as_float_reg() == Z_F0) ||
1214          (result-&gt;is_double_fpu() &amp;&amp; result-&gt;as_double_reg() == Z_F0), &quot;convention&quot;);
1215 
1216   if (SafepointMechanism::uses_thread_local_poll()) {
1217     __ z_lg(Z_R1_scratch, Address(Z_thread, Thread::polling_page_offset()));
1218   } else {
1219     AddressLiteral pp(os::get_polling_page());
1220     __ load_const_optimized(Z_R1_scratch, pp);
1221   }
1222 
1223   // Pop the frame before the safepoint code.
1224   __ pop_frame_restore_retPC(initial_frame_size_in_bytes());
1225 
1226   if (StackReservedPages &gt; 0 &amp;&amp; compilation()-&gt;has_reserved_stack_access()) {
1227     __ reserved_stack_check(Z_R14);
1228   }
1229 
1230   // We need to mark the code position where the load from the safepoint
1231   // polling page was emitted as relocInfo::poll_return_type here.
1232   __ relocate(relocInfo::poll_return_type);
1233   __ load_from_polling_page(Z_R1_scratch);
1234 
1235   __ z_br(Z_R14); // Return to caller.
1236 }
1237 
1238 int LIR_Assembler::safepoint_poll(LIR_Opr tmp, CodeEmitInfo* info) {
1239   const Register poll_addr = tmp-&gt;as_register_lo();
1240   if (SafepointMechanism::uses_thread_local_poll()) {
1241     __ z_lg(poll_addr, Address(Z_thread, Thread::polling_page_offset()));
1242   } else {
1243     AddressLiteral pp(os::get_polling_page());
1244     __ load_const_optimized(poll_addr, pp);
1245   }
1246   guarantee(info != NULL, &quot;Shouldn&#39;t be NULL&quot;);
1247   add_debug_info_for_branch(info);
1248   int offset = __ offset();
1249   __ relocate(relocInfo::poll_type);
1250   __ load_from_polling_page(poll_addr);
1251   return offset;
1252 }
1253 
1254 void LIR_Assembler::emit_static_call_stub() {
1255 
1256   // Stub is fixed up when the corresponding call is converted from calling
1257   // compiled code to calling interpreted code.
1258 
1259   address call_pc = __ pc();
1260   address stub = __ start_a_stub(call_stub_size());
1261   if (stub == NULL) {
1262     bailout(&quot;static call stub overflow&quot;);
1263     return;
1264   }
1265 
1266   int start = __ offset();
1267 
1268   __ relocate(static_stub_Relocation::spec(call_pc));
1269 
1270   // See also Matcher::interpreter_method_oop_reg().
1271   AddressLiteral meta = __ allocate_metadata_address(NULL);
1272   bool success = __ load_const_from_toc(Z_method, meta);
1273 
1274   __ set_inst_mark();
1275   AddressLiteral a((address)-1);
1276   success = success &amp;&amp; __ load_const_from_toc(Z_R1, a);
1277   if (!success) {
1278     bailout(&quot;const section overflow&quot;);
1279     return;
1280   }
1281 
1282   __ z_br(Z_R1);
1283   assert(__ offset() - start &lt;= call_stub_size(), &quot;stub too big&quot;);
1284   __ end_a_stub(); // Update current stubs pointer and restore insts_end.
1285 }
1286 
1287 void LIR_Assembler::comp_op(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Op2* op) {
1288   bool unsigned_comp = condition == lir_cond_belowEqual || condition == lir_cond_aboveEqual;
1289   if (opr1-&gt;is_single_cpu()) {
1290     Register reg1 = opr1-&gt;as_register();
1291     if (opr2-&gt;is_single_cpu()) {
1292       // cpu register - cpu register
1293       if (is_reference_type(opr1-&gt;type())) {
1294         __ z_clgr(reg1, opr2-&gt;as_register());
1295       } else {
1296         assert(!is_reference_type(opr2-&gt;type()), &quot;cmp int, oop?&quot;);
1297         if (unsigned_comp) {
1298           __ z_clr(reg1, opr2-&gt;as_register());
1299         } else {
1300           __ z_cr(reg1, opr2-&gt;as_register());
1301         }
1302       }
1303     } else if (opr2-&gt;is_stack()) {
1304       // cpu register - stack
1305       if (is_reference_type(opr1-&gt;type())) {
1306         __ z_cg(reg1, frame_map()-&gt;address_for_slot(opr2-&gt;single_stack_ix()));
1307       } else {
1308         if (unsigned_comp) {
1309           __ z_cly(reg1, frame_map()-&gt;address_for_slot(opr2-&gt;single_stack_ix()));
1310         } else {
1311           __ z_cy(reg1, frame_map()-&gt;address_for_slot(opr2-&gt;single_stack_ix()));
1312         }
1313       }
1314     } else if (opr2-&gt;is_constant()) {
1315       // cpu register - constant
1316       LIR_Const* c = opr2-&gt;as_constant_ptr();
1317       if (c-&gt;type() == T_INT) {
1318         if (unsigned_comp) {
1319           __ z_clfi(reg1, c-&gt;as_jint());
1320         } else {
1321           __ z_cfi(reg1, c-&gt;as_jint());
1322         }
1323       } else if (c-&gt;type() == T_METADATA) {
1324         // We only need, for now, comparison with NULL for metadata.
1325         assert(condition == lir_cond_equal || condition == lir_cond_notEqual, &quot;oops&quot;);
1326         Metadata* m = c-&gt;as_metadata();
1327         if (m == NULL) {
1328           __ z_cghi(reg1, 0);
1329         } else {
1330           ShouldNotReachHere();
1331         }
1332       } else if (is_reference_type(c-&gt;type())) {
1333         // In 64bit oops are single register.
1334         jobject o = c-&gt;as_jobject();
1335         if (o == NULL) {
1336           __ z_ltgr(reg1, reg1);
1337         } else {
1338           jobject2reg(o, Z_R1_scratch);
1339           __ z_cgr(reg1, Z_R1_scratch);
1340         }
1341       } else {
1342         fatal(&quot;unexpected type: %s&quot;, basictype_to_str(c-&gt;type()));
1343       }
1344       // cpu register - address
1345     } else if (opr2-&gt;is_address()) {
1346       if (op-&gt;info() != NULL) {
1347         add_debug_info_for_null_check_here(op-&gt;info());
1348       }
1349       if (unsigned_comp) {
1350         __ z_cly(reg1, as_Address(opr2-&gt;as_address_ptr()));
1351       } else {
1352         __ z_cy(reg1, as_Address(opr2-&gt;as_address_ptr()));
1353       }
1354     } else {
1355       ShouldNotReachHere();
1356     }
1357 
1358   } else if (opr1-&gt;is_double_cpu()) {
1359     assert(!unsigned_comp, &quot;unexpected&quot;);
1360     Register xlo = opr1-&gt;as_register_lo();
1361     Register xhi = opr1-&gt;as_register_hi();
1362     if (opr2-&gt;is_double_cpu()) {
1363       __ z_cgr(xlo, opr2-&gt;as_register_lo());
1364     } else if (opr2-&gt;is_constant()) {
1365       // cpu register - constant 0
1366       assert(opr2-&gt;as_jlong() == (jlong)0, &quot;only handles zero&quot;);
1367       __ z_ltgr(xlo, xlo);
1368     } else {
1369       ShouldNotReachHere();
1370     }
1371 
1372   } else if (opr1-&gt;is_single_fpu()) {
1373     if (opr2-&gt;is_single_fpu()) {
1374       __ z_cebr(opr1-&gt;as_float_reg(), opr2-&gt;as_float_reg());
1375     } else {
1376       // stack slot
1377       Address addr = frame_map()-&gt;address_for_slot(opr2-&gt;single_stack_ix());
1378       if (Immediate::is_uimm12(addr.disp())) {
1379         __ z_ceb(opr1-&gt;as_float_reg(), addr);
1380       } else {
1381         __ z_ley(Z_fscratch_1, addr);
1382         __ z_cebr(opr1-&gt;as_float_reg(), Z_fscratch_1);
1383       }
1384     }
1385   } else if (opr1-&gt;is_double_fpu()) {
1386     if (opr2-&gt;is_double_fpu()) {
1387     __ z_cdbr(opr1-&gt;as_double_reg(), opr2-&gt;as_double_reg());
1388     } else {
1389       // stack slot
1390       Address addr = frame_map()-&gt;address_for_slot(opr2-&gt;double_stack_ix());
1391       if (Immediate::is_uimm12(addr.disp())) {
1392         __ z_cdb(opr1-&gt;as_double_reg(), addr);
1393       } else {
1394         __ z_ldy(Z_fscratch_1, addr);
1395         __ z_cdbr(opr1-&gt;as_double_reg(), Z_fscratch_1);
1396       }
1397     }
1398   } else {
1399     ShouldNotReachHere();
1400   }
1401 }
1402 
1403 void LIR_Assembler::comp_fl2i(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst, LIR_Op2* op) {
1404   Label    done;
1405   Register dreg = dst-&gt;as_register();
1406 
1407   if (code == lir_cmp_fd2i || code == lir_ucmp_fd2i) {
1408     assert((left-&gt;is_single_fpu() &amp;&amp; right-&gt;is_single_fpu()) ||
1409            (left-&gt;is_double_fpu() &amp;&amp; right-&gt;is_double_fpu()), &quot;unexpected operand types&quot;);
1410     bool is_single = left-&gt;is_single_fpu();
1411     bool is_unordered_less = (code == lir_ucmp_fd2i);
1412     FloatRegister lreg = is_single ? left-&gt;as_float_reg() : left-&gt;as_double_reg();
1413     FloatRegister rreg = is_single ? right-&gt;as_float_reg() : right-&gt;as_double_reg();
1414     if (is_single) {
1415       __ z_cebr(lreg, rreg);
1416     } else {
1417       __ z_cdbr(lreg, rreg);
1418     }
1419     if (VM_Version::has_LoadStoreConditional()) {
1420       Register one       = Z_R0_scratch;
1421       Register minus_one = Z_R1_scratch;
1422       __ z_lghi(minus_one, -1);
1423       __ z_lghi(one,  1);
1424       __ z_lghi(dreg, 0);
1425       __ z_locgr(dreg, one,       is_unordered_less ? Assembler::bcondHigh            : Assembler::bcondHighOrNotOrdered);
1426       __ z_locgr(dreg, minus_one, is_unordered_less ? Assembler::bcondLowOrNotOrdered : Assembler::bcondLow);
1427     } else {
1428       __ clear_reg(dreg, true, false);
1429       __ z_bre(done); // if (left == right) dst = 0
1430 
1431       // if (left &gt; right || ((code ~= cmpg) &amp;&amp; (left &lt;&gt; right)) dst := 1
1432       __ z_lhi(dreg, 1);
1433       __ z_brc(is_unordered_less ? Assembler::bcondHigh : Assembler::bcondHighOrNotOrdered, done);
1434 
1435       // if (left &lt; right || ((code ~= cmpl) &amp;&amp; (left &lt;&gt; right)) dst := -1
1436       __ z_lhi(dreg, -1);
1437     }
1438   } else {
1439     assert(code == lir_cmp_l2i, &quot;check&quot;);
1440     if (VM_Version::has_LoadStoreConditional()) {
1441       Register one       = Z_R0_scratch;
1442       Register minus_one = Z_R1_scratch;
1443       __ z_cgr(left-&gt;as_register_lo(), right-&gt;as_register_lo());
1444       __ z_lghi(minus_one, -1);
1445       __ z_lghi(one,  1);
1446       __ z_lghi(dreg, 0);
1447       __ z_locgr(dreg, one, Assembler::bcondHigh);
1448       __ z_locgr(dreg, minus_one, Assembler::bcondLow);
1449     } else {
1450       __ z_cgr(left-&gt;as_register_lo(), right-&gt;as_register_lo());
1451       __ z_lghi(dreg,  0);     // eq value
1452       __ z_bre(done);
1453       __ z_lghi(dreg,  1);     // gt value
1454       __ z_brh(done);
1455       __ z_lghi(dreg, -1);     // lt value
1456     }
1457   }
1458   __ bind(done);
1459 }
1460 
1461 // result = condition ? opr1 : opr2
1462 void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {
1463   Assembler::branch_condition acond = Assembler::bcondEqual, ncond = Assembler::bcondNotEqual;
1464   switch (condition) {
1465     case lir_cond_equal:        acond = Assembler::bcondEqual;    ncond = Assembler::bcondNotEqual; break;
1466     case lir_cond_notEqual:     acond = Assembler::bcondNotEqual; ncond = Assembler::bcondEqual;    break;
1467     case lir_cond_less:         acond = Assembler::bcondLow;      ncond = Assembler::bcondNotLow;   break;
1468     case lir_cond_lessEqual:    acond = Assembler::bcondNotHigh;  ncond = Assembler::bcondHigh;     break;
1469     case lir_cond_greaterEqual: acond = Assembler::bcondNotLow;   ncond = Assembler::bcondLow;      break;
1470     case lir_cond_greater:      acond = Assembler::bcondHigh;     ncond = Assembler::bcondNotHigh;  break;
1471     case lir_cond_belowEqual:   acond = Assembler::bcondNotHigh;  ncond = Assembler::bcondHigh;     break;
1472     case lir_cond_aboveEqual:   acond = Assembler::bcondNotLow;   ncond = Assembler::bcondLow;      break;
1473     default:                    ShouldNotReachHere();
1474   }
1475 
1476   if (opr1-&gt;is_cpu_register()) {
1477     reg2reg(opr1, result);
1478   } else if (opr1-&gt;is_stack()) {
1479     stack2reg(opr1, result, result-&gt;type());
1480   } else if (opr1-&gt;is_constant()) {
1481     const2reg(opr1, result, lir_patch_none, NULL);
1482   } else {
1483     ShouldNotReachHere();
1484   }
1485 
1486   if (VM_Version::has_LoadStoreConditional() &amp;&amp; !opr2-&gt;is_constant()) {
1487     // Optimized version that does not require a branch.
1488     if (opr2-&gt;is_single_cpu()) {
1489       assert(opr2-&gt;cpu_regnr() != result-&gt;cpu_regnr(), &quot;opr2 already overwritten by previous move&quot;);
1490       __ z_locgr(result-&gt;as_register(), opr2-&gt;as_register(), ncond);
1491     } else if (opr2-&gt;is_double_cpu()) {
1492       assert(opr2-&gt;cpu_regnrLo() != result-&gt;cpu_regnrLo() &amp;&amp; opr2-&gt;cpu_regnrLo() != result-&gt;cpu_regnrHi(), &quot;opr2 already overwritten by previous move&quot;);
1493       assert(opr2-&gt;cpu_regnrHi() != result-&gt;cpu_regnrLo() &amp;&amp; opr2-&gt;cpu_regnrHi() != result-&gt;cpu_regnrHi(), &quot;opr2 already overwritten by previous move&quot;);
1494       __ z_locgr(result-&gt;as_register_lo(), opr2-&gt;as_register_lo(), ncond);
1495     } else if (opr2-&gt;is_single_stack()) {
1496       __ z_loc(result-&gt;as_register(), frame_map()-&gt;address_for_slot(opr2-&gt;single_stack_ix()), ncond);
1497     } else if (opr2-&gt;is_double_stack()) {
1498       __ z_locg(result-&gt;as_register_lo(), frame_map()-&gt;address_for_slot(opr2-&gt;double_stack_ix()), ncond);
1499     } else {
1500       ShouldNotReachHere();
1501     }
1502   } else {
1503     Label skip;
1504     __ z_brc(acond, skip);
1505     if (opr2-&gt;is_cpu_register()) {
1506       reg2reg(opr2, result);
1507     } else if (opr2-&gt;is_stack()) {
1508       stack2reg(opr2, result, result-&gt;type());
1509     } else if (opr2-&gt;is_constant()) {
1510       const2reg(opr2, result, lir_patch_none, NULL);
1511     } else {
1512       ShouldNotReachHere();
1513     }
1514     __ bind(skip);
1515   }
1516 }
1517 
1518 void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest,
1519                              CodeEmitInfo* info, bool pop_fpu_stack) {
1520   assert(info == NULL, &quot;should never be used, idiv/irem and ldiv/lrem not handled by this method&quot;);
1521 
1522   if (left-&gt;is_single_cpu()) {
1523     assert(left == dest, &quot;left and dest must be equal&quot;);
1524     Register lreg = left-&gt;as_register();
1525 
1526     if (right-&gt;is_single_cpu()) {
1527       // cpu register - cpu register
1528       Register rreg = right-&gt;as_register();
1529       switch (code) {
1530         case lir_add: __ z_ar (lreg, rreg); break;
1531         case lir_sub: __ z_sr (lreg, rreg); break;
1532         case lir_mul: __ z_msr(lreg, rreg); break;
1533         default: ShouldNotReachHere();
1534       }
1535 
1536     } else if (right-&gt;is_stack()) {
1537       // cpu register - stack
1538       Address raddr = frame_map()-&gt;address_for_slot(right-&gt;single_stack_ix());
1539       switch (code) {
1540         case lir_add: __ z_ay(lreg, raddr); break;
1541         case lir_sub: __ z_sy(lreg, raddr); break;
1542         default: ShouldNotReachHere();
1543       }
1544 
1545     } else if (right-&gt;is_constant()) {
1546       // cpu register - constant
1547       jint c = right-&gt;as_constant_ptr()-&gt;as_jint();
1548       switch (code) {
1549         case lir_add: __ z_agfi(lreg, c);  break;
1550         case lir_sub: __ z_agfi(lreg, -c); break; // note: -min_jint == min_jint
1551         case lir_mul: __ z_msfi(lreg, c);  break;
1552         default: ShouldNotReachHere();
1553       }
1554 
1555     } else {
1556       ShouldNotReachHere();
1557     }
1558 
1559   } else if (left-&gt;is_double_cpu()) {
1560     assert(left == dest, &quot;left and dest must be equal&quot;);
1561     Register lreg_lo = left-&gt;as_register_lo();
1562     Register lreg_hi = left-&gt;as_register_hi();
1563 
1564     if (right-&gt;is_double_cpu()) {
1565       // cpu register - cpu register
1566       Register rreg_lo = right-&gt;as_register_lo();
1567       Register rreg_hi = right-&gt;as_register_hi();
1568       assert_different_registers(lreg_lo, rreg_lo);
1569       switch (code) {
1570         case lir_add:
1571           __ z_agr(lreg_lo, rreg_lo);
1572           break;
1573         case lir_sub:
1574           __ z_sgr(lreg_lo, rreg_lo);
1575           break;
1576         case lir_mul:
1577           __ z_msgr(lreg_lo, rreg_lo);
1578           break;
1579         default:
1580           ShouldNotReachHere();
1581       }
1582 
1583     } else if (right-&gt;is_constant()) {
1584       // cpu register - constant
1585       jlong c = right-&gt;as_constant_ptr()-&gt;as_jlong_bits();
1586       switch (code) {
1587         case lir_add: __ z_agfi(lreg_lo, c); break;
1588         case lir_sub:
1589           if (c != min_jint) {
1590                       __ z_agfi(lreg_lo, -c);
1591           } else {
1592             // -min_jint cannot be represented as simm32 in z_agfi
1593             // min_jint sign extended:      0xffffffff80000000
1594             // -min_jint as 64 bit integer: 0x0000000080000000
1595             // 0x80000000 can be represented as uimm32 in z_algfi
1596             // lreg_lo := lreg_lo + -min_jint == lreg_lo + 0x80000000
1597                       __ z_algfi(lreg_lo, UCONST64(0x80000000));
1598           }
1599           break;
1600         case lir_mul: __ z_msgfi(lreg_lo, c); break;
1601         default:
1602           ShouldNotReachHere();
1603       }
1604 
1605     } else {
1606       ShouldNotReachHere();
1607     }
1608 
1609   } else if (left-&gt;is_single_fpu()) {
1610     assert(left == dest, &quot;left and dest must be equal&quot;);
1611     FloatRegister lreg = left-&gt;as_float_reg();
1612     FloatRegister rreg = right-&gt;is_single_fpu() ? right-&gt;as_float_reg() : fnoreg;
1613     Address raddr;
1614 
1615     if (rreg == fnoreg) {
1616       assert(right-&gt;is_single_stack(), &quot;constants should be loaded into register&quot;);
1617       raddr = frame_map()-&gt;address_for_slot(right-&gt;single_stack_ix());
1618       if (!Immediate::is_uimm12(raddr.disp())) {
1619         __ mem2freg_opt(rreg = Z_fscratch_1, raddr, false);
1620       }
1621     }
1622 
1623     if (rreg != fnoreg) {
1624       switch (code) {
1625         case lir_add: __ z_aebr(lreg, rreg);  break;
1626         case lir_sub: __ z_sebr(lreg, rreg);  break;
1627         case lir_mul_strictfp: // fall through
1628         case lir_mul: __ z_meebr(lreg, rreg); break;
1629         case lir_div_strictfp: // fall through
1630         case lir_div: __ z_debr(lreg, rreg);  break;
1631         default: ShouldNotReachHere();
1632       }
1633     } else {
1634       switch (code) {
1635         case lir_add: __ z_aeb(lreg, raddr);  break;
1636         case lir_sub: __ z_seb(lreg, raddr);  break;
1637         case lir_mul_strictfp: // fall through
1638         case lir_mul: __ z_meeb(lreg, raddr);  break;
1639         case lir_div_strictfp: // fall through
1640         case lir_div: __ z_deb(lreg, raddr);  break;
1641         default: ShouldNotReachHere();
1642       }
1643     }
1644   } else if (left-&gt;is_double_fpu()) {
1645     assert(left == dest, &quot;left and dest must be equal&quot;);
1646     FloatRegister lreg = left-&gt;as_double_reg();
1647     FloatRegister rreg = right-&gt;is_double_fpu() ? right-&gt;as_double_reg() : fnoreg;
1648     Address raddr;
1649 
1650     if (rreg == fnoreg) {
1651       assert(right-&gt;is_double_stack(), &quot;constants should be loaded into register&quot;);
1652       raddr = frame_map()-&gt;address_for_slot(right-&gt;double_stack_ix());
1653       if (!Immediate::is_uimm12(raddr.disp())) {
1654         __ mem2freg_opt(rreg = Z_fscratch_1, raddr, true);
1655       }
1656     }
1657 
1658     if (rreg != fnoreg) {
1659       switch (code) {
1660         case lir_add: __ z_adbr(lreg, rreg); break;
1661         case lir_sub: __ z_sdbr(lreg, rreg); break;
1662         case lir_mul_strictfp: // fall through
1663         case lir_mul: __ z_mdbr(lreg, rreg); break;
1664         case lir_div_strictfp: // fall through
1665         case lir_div: __ z_ddbr(lreg, rreg); break;
1666         default: ShouldNotReachHere();
1667       }
1668     } else {
1669       switch (code) {
1670         case lir_add: __ z_adb(lreg, raddr); break;
1671         case lir_sub: __ z_sdb(lreg, raddr); break;
1672         case lir_mul_strictfp: // fall through
1673         case lir_mul: __ z_mdb(lreg, raddr); break;
1674         case lir_div_strictfp: // fall through
1675         case lir_div: __ z_ddb(lreg, raddr); break;
1676         default: ShouldNotReachHere();
1677       }
1678     }
1679   } else if (left-&gt;is_address()) {
1680     assert(left == dest, &quot;left and dest must be equal&quot;);
1681     assert(code == lir_add, &quot;unsupported operation&quot;);
1682     assert(right-&gt;is_constant(), &quot;unsupported operand&quot;);
1683     jint c = right-&gt;as_constant_ptr()-&gt;as_jint();
1684     LIR_Address* lir_addr = left-&gt;as_address_ptr();
1685     Address addr = as_Address(lir_addr);
1686     switch (lir_addr-&gt;type()) {
1687       case T_INT:
1688         __ add2mem_32(addr, c, Z_R1_scratch);
1689         break;
1690       case T_LONG:
1691         __ add2mem_64(addr, c, Z_R1_scratch);
1692         break;
1693       default:
1694         ShouldNotReachHere();
1695     }
1696   } else {
1697     ShouldNotReachHere();
1698   }
1699 }
1700 
1701 void LIR_Assembler::intrinsic_op(LIR_Code code, LIR_Opr value, LIR_Opr thread, LIR_Opr dest, LIR_Op* op) {
1702   switch (code) {
1703     case lir_sqrt: {
1704       assert(!thread-&gt;is_valid(), &quot;there is no need for a thread_reg for dsqrt&quot;);
1705       FloatRegister src_reg = value-&gt;as_double_reg();
1706       FloatRegister dst_reg = dest-&gt;as_double_reg();
1707       __ z_sqdbr(dst_reg, src_reg);
1708       break;
1709     }
1710     case lir_abs: {
1711       assert(!thread-&gt;is_valid(), &quot;there is no need for a thread_reg for fabs&quot;);
1712       FloatRegister src_reg = value-&gt;as_double_reg();
1713       FloatRegister dst_reg = dest-&gt;as_double_reg();
1714       __ z_lpdbr(dst_reg, src_reg);
1715       break;
1716     }
1717     default: {
1718       ShouldNotReachHere();
1719       break;
1720     }
1721   }
1722 }
1723 
1724 void LIR_Assembler::logic_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst) {
1725   if (left-&gt;is_single_cpu()) {
1726     Register reg = left-&gt;as_register();
1727     if (right-&gt;is_constant()) {
1728       int val = right-&gt;as_constant_ptr()-&gt;as_jint();
1729       switch (code) {
1730         case lir_logic_and: __ z_nilf(reg, val); break;
1731         case lir_logic_or:  __ z_oilf(reg, val); break;
1732         case lir_logic_xor: __ z_xilf(reg, val); break;
1733         default: ShouldNotReachHere();
1734       }
1735     } else if (right-&gt;is_stack()) {
1736       Address raddr = frame_map()-&gt;address_for_slot(right-&gt;single_stack_ix());
1737       switch (code) {
1738         case lir_logic_and: __ z_ny(reg, raddr); break;
1739         case lir_logic_or:  __ z_oy(reg, raddr); break;
1740         case lir_logic_xor: __ z_xy(reg, raddr); break;
1741         default: ShouldNotReachHere();
1742       }
1743     } else {
1744       Register rright = right-&gt;as_register();
1745       switch (code) {
1746         case lir_logic_and: __ z_nr(reg, rright); break;
1747         case lir_logic_or : __ z_or(reg, rright); break;
1748         case lir_logic_xor: __ z_xr(reg, rright); break;
1749         default: ShouldNotReachHere();
1750       }
1751     }
1752     move_regs(reg, dst-&gt;as_register());
1753   } else {
1754     Register l_lo = left-&gt;as_register_lo();
1755     if (right-&gt;is_constant()) {
1756       __ load_const_optimized(Z_R1_scratch, right-&gt;as_constant_ptr()-&gt;as_jlong());
1757       switch (code) {
1758         case lir_logic_and:
1759           __ z_ngr(l_lo, Z_R1_scratch);
1760           break;
1761         case lir_logic_or:
1762           __ z_ogr(l_lo, Z_R1_scratch);
1763           break;
1764         case lir_logic_xor:
1765           __ z_xgr(l_lo, Z_R1_scratch);
1766           break;
1767         default: ShouldNotReachHere();
1768       }
1769     } else {
1770       Register r_lo;
1771       if (is_reference_type(right-&gt;type())) {
1772         r_lo = right-&gt;as_register();
1773       } else {
1774         r_lo = right-&gt;as_register_lo();
1775       }
1776       switch (code) {
1777         case lir_logic_and:
1778           __ z_ngr(l_lo, r_lo);
1779           break;
1780         case lir_logic_or:
1781           __ z_ogr(l_lo, r_lo);
1782           break;
1783         case lir_logic_xor:
1784           __ z_xgr(l_lo, r_lo);
1785           break;
1786         default: ShouldNotReachHere();
1787       }
1788     }
1789 
1790     Register dst_lo = dst-&gt;as_register_lo();
1791 
1792     move_regs(l_lo, dst_lo);
1793   }
1794 }
1795 
1796 // See operand selection in LIRGenerator::do_ArithmeticOp_Int().
1797 void LIR_Assembler::arithmetic_idiv(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr temp, LIR_Opr result, CodeEmitInfo* info) {
1798   if (left-&gt;is_double_cpu()) {
1799     // 64 bit integer case
1800     assert(left-&gt;is_double_cpu(), &quot;left must be register&quot;);
1801     assert(right-&gt;is_double_cpu() || is_power_of_2_long(right-&gt;as_jlong()),
1802            &quot;right must be register or power of 2 constant&quot;);
1803     assert(result-&gt;is_double_cpu(), &quot;result must be register&quot;);
1804 
1805     Register lreg = left-&gt;as_register_lo();
1806     Register dreg = result-&gt;as_register_lo();
1807 
1808     if (right-&gt;is_constant()) {
1809       // Convert division by a power of two into some shifts and logical operations.
1810       Register treg1 = Z_R0_scratch;
1811       Register treg2 = Z_R1_scratch;
1812       jlong divisor = right-&gt;as_jlong();
1813       jlong log_divisor = log2_long(right-&gt;as_jlong());
1814 
1815       if (divisor == min_jlong) {
1816         // Min_jlong is special. Result is &#39;0&#39; except for min_jlong/min_jlong = 1.
1817         if (dreg == lreg) {
1818           NearLabel done;
1819           __ load_const_optimized(treg2, min_jlong);
1820           __ z_cgr(lreg, treg2);
1821           __ z_lghi(dreg, 0);           // Preserves condition code.
1822           __ z_brne(done);
1823           __ z_lghi(dreg, 1);           // min_jlong / min_jlong = 1
1824           __ bind(done);
1825         } else {
1826           assert_different_registers(dreg, lreg);
1827           NearLabel done;
1828           __ z_lghi(dreg, 0);
1829           __ compare64_and_branch(lreg, min_jlong, Assembler::bcondNotEqual, done);
1830           __ z_lghi(dreg, 1);
1831           __ bind(done);
1832         }
1833         return;
1834       }
1835       __ move_reg_if_needed(dreg, T_LONG, lreg, T_LONG);
1836       if (divisor == 2) {
1837         __ z_srlg(treg2, dreg, 63);     // dividend &lt; 0 ? 1 : 0
1838       } else {
1839         __ z_srag(treg2, dreg, 63);     // dividend &lt; 0 ? -1 : 0
1840         __ and_imm(treg2, divisor - 1, treg1, true);
1841       }
1842       if (code == lir_idiv) {
1843         __ z_agr(dreg, treg2);
1844         __ z_srag(dreg, dreg, log_divisor);
1845       } else {
1846         assert(code == lir_irem, &quot;check&quot;);
1847         __ z_agr(treg2, dreg);
1848         __ and_imm(treg2, ~(divisor - 1), treg1, true);
1849         __ z_sgr(dreg, treg2);
1850       }
1851       return;
1852     }
1853 
1854     // Divisor is not a power of 2 constant.
1855     Register rreg = right-&gt;as_register_lo();
1856     Register treg = temp-&gt;as_register_lo();
1857     assert(right-&gt;is_double_cpu(), &quot;right must be register&quot;);
1858     assert(lreg == Z_R11, &quot;see ldivInOpr()&quot;);
1859     assert(rreg != lreg, &quot;right register must not be same as left register&quot;);
1860     assert((code == lir_idiv &amp;&amp; dreg == Z_R11 &amp;&amp; treg == Z_R10) ||
1861            (code == lir_irem &amp;&amp; dreg == Z_R10 &amp;&amp; treg == Z_R11), &quot;see ldivInOpr(), ldivOutOpr(), lremOutOpr()&quot;);
1862 
1863     Register R1 = lreg-&gt;predecessor();
1864     Register R2 = rreg;
1865     assert(code != lir_idiv || lreg==dreg, &quot;see code below&quot;);
1866     if (code == lir_idiv) {
1867       __ z_lcgr(lreg, lreg);
1868     } else {
1869       __ clear_reg(dreg, true, false);
1870     }
1871     NearLabel done;
1872     __ compare64_and_branch(R2, -1, Assembler::bcondEqual, done);
1873     if (code == lir_idiv) {
1874       __ z_lcgr(lreg, lreg); // Revert lcgr above.
1875     }
1876     if (ImplicitDiv0Checks) {
1877       // No debug info because the idiv won&#39;t trap.
1878       // Add_debug_info_for_div0 would instantiate another DivByZeroStub,
1879       // which is unnecessary, too.
1880       add_debug_info_for_div0(__ offset(), info);
1881     }
1882     __ z_dsgr(R1, R2);
1883     __ bind(done);
1884     return;
1885   }
1886 
1887   // 32 bit integer case
1888 
1889   assert(left-&gt;is_single_cpu(), &quot;left must be register&quot;);
1890   assert(right-&gt;is_single_cpu() || is_power_of_2(right-&gt;as_jint()), &quot;right must be register or power of 2 constant&quot;);
1891   assert(result-&gt;is_single_cpu(), &quot;result must be register&quot;);
1892 
1893   Register lreg = left-&gt;as_register();
1894   Register dreg = result-&gt;as_register();
1895 
1896   if (right-&gt;is_constant()) {
1897     // Convert division by a power of two into some shifts and logical operations.
1898     Register treg1 = Z_R0_scratch;
1899     Register treg2 = Z_R1_scratch;
1900     jlong divisor = right-&gt;as_jint();
1901     jlong log_divisor = log2_long(right-&gt;as_jint());
1902     __ move_reg_if_needed(dreg, T_LONG, lreg, T_INT); // sign extend
1903     if (divisor == 2) {
1904       __ z_srlg(treg2, dreg, 63);     // dividend &lt; 0 ?  1 : 0
1905     } else {
1906       __ z_srag(treg2, dreg, 63);     // dividend &lt; 0 ? -1 : 0
1907       __ and_imm(treg2, divisor - 1, treg1, true);
1908     }
1909     if (code == lir_idiv) {
1910       __ z_agr(dreg, treg2);
1911       __ z_srag(dreg, dreg, log_divisor);
1912     } else {
1913       assert(code == lir_irem, &quot;check&quot;);
1914       __ z_agr(treg2, dreg);
1915       __ and_imm(treg2, ~(divisor - 1), treg1, true);
1916       __ z_sgr(dreg, treg2);
1917     }
1918     return;
1919   }
1920 
1921   // Divisor is not a power of 2 constant.
1922   Register rreg = right-&gt;as_register();
1923   Register treg = temp-&gt;as_register();
1924   assert(right-&gt;is_single_cpu(), &quot;right must be register&quot;);
1925   assert(lreg == Z_R11, &quot;left register must be rax,&quot;);
1926   assert(rreg != lreg, &quot;right register must not be same as left register&quot;);
1927   assert((code == lir_idiv &amp;&amp; dreg == Z_R11 &amp;&amp; treg == Z_R10)
1928       || (code == lir_irem &amp;&amp; dreg == Z_R10 &amp;&amp; treg == Z_R11), &quot;see divInOpr(), divOutOpr(), remOutOpr()&quot;);
1929 
1930   Register R1 = lreg-&gt;predecessor();
1931   Register R2 = rreg;
1932   __ move_reg_if_needed(lreg, T_LONG, lreg, T_INT); // sign extend
1933   if (ImplicitDiv0Checks) {
1934     // No debug info because the idiv won&#39;t trap.
1935     // Add_debug_info_for_div0 would instantiate another DivByZeroStub,
1936     // which is unnecessary, too.
1937     add_debug_info_for_div0(__ offset(), info);
1938   }
1939   __ z_dsgfr(R1, R2);
1940 }
1941 
1942 void LIR_Assembler::throw_op(LIR_Opr exceptionPC, LIR_Opr exceptionOop, CodeEmitInfo* info) {
1943   assert(exceptionOop-&gt;as_register() == Z_EXC_OOP, &quot;should match&quot;);
1944   assert(exceptionPC-&gt;as_register() == Z_EXC_PC, &quot;should match&quot;);
1945 
1946   // Exception object is not added to oop map by LinearScan
1947   // (LinearScan assumes that no oops are in fixed registers).
1948   info-&gt;add_register_oop(exceptionOop);
1949 
1950   // Reuse the debug info from the safepoint poll for the throw op itself.
1951   __ get_PC(Z_EXC_PC);
1952   add_call_info(__ offset(), info); // for exception handler
1953   address stub = Runtime1::entry_for (compilation()-&gt;has_fpu_code() ? Runtime1::handle_exception_id
1954                                                                     : Runtime1::handle_exception_nofpu_id);
1955   emit_call_c(stub);
1956 }
1957 
1958 void LIR_Assembler::unwind_op(LIR_Opr exceptionOop) {
1959   assert(exceptionOop-&gt;as_register() == Z_EXC_OOP, &quot;should match&quot;);
1960 
1961   __ branch_optimized(Assembler::bcondAlways, _unwind_handler_entry);
1962 }
1963 
1964 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
1965   ciArrayKlass* default_type = op-&gt;expected_type();
1966   Register src = op-&gt;src()-&gt;as_register();
1967   Register dst = op-&gt;dst()-&gt;as_register();
1968   Register src_pos = op-&gt;src_pos()-&gt;as_register();
1969   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
1970   Register length  = op-&gt;length()-&gt;as_register();
1971   Register tmp = op-&gt;tmp()-&gt;as_register();
1972 
1973   CodeStub* stub = op-&gt;stub();
1974   int flags = op-&gt;flags();
1975   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
1976   if (basic_type == T_ARRAY) basic_type = T_OBJECT;
1977 
1978   // If we don&#39;t know anything, just go through the generic arraycopy.
1979   if (default_type == NULL) {
1980     address copyfunc_addr = StubRoutines::generic_arraycopy();
1981 
1982     if (copyfunc_addr == NULL) {
1983       // Take a slow path for generic arraycopy.
1984       __ branch_optimized(Assembler::bcondAlways, *stub-&gt;entry());
1985       __ bind(*stub-&gt;continuation());
1986       return;
1987     }
1988 
1989     // Save outgoing arguments in callee saved registers (C convention) in case
1990     // a call to System.arraycopy is needed.
1991     Register callee_saved_src     = Z_R10;
1992     Register callee_saved_src_pos = Z_R11;
1993     Register callee_saved_dst     = Z_R12;
1994     Register callee_saved_dst_pos = Z_R13;
1995     Register callee_saved_length  = Z_ARG5; // Z_ARG5 == Z_R6 is callee saved.
1996 
1997     __ lgr_if_needed(callee_saved_src, src);
1998     __ lgr_if_needed(callee_saved_src_pos, src_pos);
1999     __ lgr_if_needed(callee_saved_dst, dst);
2000     __ lgr_if_needed(callee_saved_dst_pos, dst_pos);
2001     __ lgr_if_needed(callee_saved_length, length);
2002 
2003     // C function requires 64 bit values.
2004     __ z_lgfr(src_pos, src_pos);
2005     __ z_lgfr(dst_pos, dst_pos);
2006     __ z_lgfr(length, length);
2007 
2008     // Pass arguments: may push as this is not a safepoint; SP must be fix at each safepoint.
2009 
2010     // The arguments are in the corresponding registers.
2011     assert(Z_ARG1 == src,     &quot;assumption&quot;);
2012     assert(Z_ARG2 == src_pos, &quot;assumption&quot;);
2013     assert(Z_ARG3 == dst,     &quot;assumption&quot;);
2014     assert(Z_ARG4 == dst_pos, &quot;assumption&quot;);
2015     assert(Z_ARG5 == length,  &quot;assumption&quot;);
2016 #ifndef PRODUCT
2017     if (PrintC1Statistics) {
2018       __ load_const_optimized(Z_R1_scratch, (address)&amp;Runtime1::_generic_arraycopystub_cnt);
2019       __ add2mem_32(Address(Z_R1_scratch), 1, Z_R0_scratch);
2020     }
2021 #endif
2022     emit_call_c(copyfunc_addr);
2023     CHECK_BAILOUT();
2024 
2025     __ compare32_and_branch(Z_RET, (intptr_t)0, Assembler::bcondEqual, *stub-&gt;continuation());
2026 
2027     __ z_lgr(tmp, Z_RET);
2028     __ z_xilf(tmp, -1);
2029 
2030     // Restore values from callee saved registers so they are where the stub
2031     // expects them.
2032     __ lgr_if_needed(src, callee_saved_src);
2033     __ lgr_if_needed(src_pos, callee_saved_src_pos);
2034     __ lgr_if_needed(dst, callee_saved_dst);
2035     __ lgr_if_needed(dst_pos, callee_saved_dst_pos);
2036     __ lgr_if_needed(length, callee_saved_length);
2037 
2038     __ z_sr(length, tmp);
2039     __ z_ar(src_pos, tmp);
2040     __ z_ar(dst_pos, tmp);
2041     __ branch_optimized(Assembler::bcondAlways, *stub-&gt;entry());
2042 
2043     __ bind(*stub-&gt;continuation());
2044     return;
2045   }
2046 
2047   assert(default_type != NULL &amp;&amp; default_type-&gt;is_array_klass() &amp;&amp; default_type-&gt;is_loaded(), &quot;must be true at this point&quot;);
2048 
2049   int elem_size = type2aelembytes(basic_type);
2050   int shift_amount;
2051 
2052   switch (elem_size) {
2053     case 1 :
2054       shift_amount = 0;
2055       break;
2056     case 2 :
2057       shift_amount = 1;
2058       break;
2059     case 4 :
2060       shift_amount = 2;
2061       break;
2062     case 8 :
2063       shift_amount = 3;
2064       break;
2065     default:
2066       shift_amount = -1;
2067       ShouldNotReachHere();
2068   }
2069 
2070   Address src_length_addr = Address(src, arrayOopDesc::length_offset_in_bytes());
2071   Address dst_length_addr = Address(dst, arrayOopDesc::length_offset_in_bytes());
2072   Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());
2073   Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());
2074 
2075   // Length and pos&#39;s are all sign extended at this point on 64bit.
2076 
2077   // test for NULL
2078   if (flags &amp; LIR_OpArrayCopy::src_null_check) {
2079     __ compareU64_and_branch(src, (intptr_t)0, Assembler::bcondZero, *stub-&gt;entry());
2080   }
2081   if (flags &amp; LIR_OpArrayCopy::dst_null_check) {
2082     __ compareU64_and_branch(dst, (intptr_t)0, Assembler::bcondZero, *stub-&gt;entry());
2083   }
2084 
2085   // Check if negative.
2086   if (flags &amp; LIR_OpArrayCopy::src_pos_positive_check) {
2087     __ compare32_and_branch(src_pos, (intptr_t)0, Assembler::bcondLow, *stub-&gt;entry());
2088   }
2089   if (flags &amp; LIR_OpArrayCopy::dst_pos_positive_check) {
2090     __ compare32_and_branch(dst_pos, (intptr_t)0, Assembler::bcondLow, *stub-&gt;entry());
2091   }
2092 
2093   // If the compiler was not able to prove that exact type of the source or the destination
2094   // of the arraycopy is an array type, check at runtime if the source or the destination is
2095   // an instance type.
2096   if (flags &amp; LIR_OpArrayCopy::type_check) {
2097     assert(Klass::_lh_neutral_value == 0, &quot;or replace z_lt instructions&quot;);
2098 
2099     if (!(flags &amp; LIR_OpArrayCopy::dst_objarray)) {
2100       __ load_klass(tmp, dst);
2101       __ z_lt(tmp, Address(tmp, in_bytes(Klass::layout_helper_offset())));
2102       __ branch_optimized(Assembler::bcondNotLow, *stub-&gt;entry());
2103     }
2104 
2105     if (!(flags &amp; LIR_OpArrayCopy::src_objarray)) {
2106       __ load_klass(tmp, src);
2107       __ z_lt(tmp, Address(tmp, in_bytes(Klass::layout_helper_offset())));
2108       __ branch_optimized(Assembler::bcondNotLow, *stub-&gt;entry());
2109     }
2110   }
2111 
2112   if (flags &amp; LIR_OpArrayCopy::src_range_check) {
2113     __ z_la(tmp, Address(src_pos, length));
2114     __ z_cl(tmp, src_length_addr);
2115     __ branch_optimized(Assembler::bcondHigh, *stub-&gt;entry());
2116   }
2117   if (flags &amp; LIR_OpArrayCopy::dst_range_check) {
2118     __ z_la(tmp, Address(dst_pos, length));
2119     __ z_cl(tmp, dst_length_addr);
2120     __ branch_optimized(Assembler::bcondHigh, *stub-&gt;entry());
2121   }
2122 
2123   if (flags &amp; LIR_OpArrayCopy::length_positive_check) {
2124     __ z_ltr(length, length);
2125     __ branch_optimized(Assembler::bcondNegative, *stub-&gt;entry());
2126   }
2127 
2128   // Stubs require 64 bit values.
2129   __ z_lgfr(src_pos, src_pos); // int -&gt; long
2130   __ z_lgfr(dst_pos, dst_pos); // int -&gt; long
2131   __ z_lgfr(length, length);   // int -&gt; long
2132 
2133   if (flags &amp; LIR_OpArrayCopy::type_check) {
2134     // We don&#39;t know the array types are compatible.
2135     if (basic_type != T_OBJECT) {
2136       // Simple test for basic type arrays.
2137       if (UseCompressedClassPointers) {
2138         __ z_l(tmp, src_klass_addr);
2139         __ z_c(tmp, dst_klass_addr);
2140       } else {
2141         __ z_lg(tmp, src_klass_addr);
2142         __ z_cg(tmp, dst_klass_addr);
2143       }
2144       __ branch_optimized(Assembler::bcondNotEqual, *stub-&gt;entry());
2145     } else {
2146       // For object arrays, if src is a sub class of dst then we can
2147       // safely do the copy.
2148       NearLabel cont, slow;
2149       Register src_klass = Z_R1_scratch;
2150       Register dst_klass = Z_R10;
2151 
2152       __ load_klass(src_klass, src);
2153       __ load_klass(dst_klass, dst);
2154 
2155       __ check_klass_subtype_fast_path(src_klass, dst_klass, tmp, &amp;cont, &amp;slow, NULL);
2156 
2157       store_parameter(src_klass, 0); // sub
2158       store_parameter(dst_klass, 1); // super
2159       emit_call_c(Runtime1::entry_for (Runtime1::slow_subtype_check_id));
2160       CHECK_BAILOUT2(cont, slow);
2161       // Sets condition code 0 for match (2 otherwise).
2162       __ branch_optimized(Assembler::bcondEqual, cont);
2163 
2164       __ bind(slow);
2165 
2166       address copyfunc_addr = StubRoutines::checkcast_arraycopy();
2167       if (copyfunc_addr != NULL) { // use stub if available
2168         // Src is not a sub class of dst so we have to do a
2169         // per-element check.
2170 
2171         int mask = LIR_OpArrayCopy::src_objarray|LIR_OpArrayCopy::dst_objarray;
2172         if ((flags &amp; mask) != mask) {
2173           // Check that at least both of them object arrays.
2174           assert(flags &amp; mask, &quot;one of the two should be known to be an object array&quot;);
2175 
2176           if (!(flags &amp; LIR_OpArrayCopy::src_objarray)) {
2177             __ load_klass(tmp, src);
2178           } else if (!(flags &amp; LIR_OpArrayCopy::dst_objarray)) {
2179             __ load_klass(tmp, dst);
2180           }
2181           Address klass_lh_addr(tmp, Klass::layout_helper_offset());
2182           jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
2183           __ load_const_optimized(Z_R1_scratch, objArray_lh);
2184           __ z_c(Z_R1_scratch, klass_lh_addr);
2185           __ branch_optimized(Assembler::bcondNotEqual, *stub-&gt;entry());
2186         }
2187 
2188         // Save outgoing arguments in callee saved registers (C convention) in case
2189         // a call to System.arraycopy is needed.
2190         Register callee_saved_src     = Z_R10;
2191         Register callee_saved_src_pos = Z_R11;
2192         Register callee_saved_dst     = Z_R12;
2193         Register callee_saved_dst_pos = Z_R13;
2194         Register callee_saved_length  = Z_ARG5; // Z_ARG5 == Z_R6 is callee saved.
2195 
2196         __ lgr_if_needed(callee_saved_src, src);
2197         __ lgr_if_needed(callee_saved_src_pos, src_pos);
2198         __ lgr_if_needed(callee_saved_dst, dst);
2199         __ lgr_if_needed(callee_saved_dst_pos, dst_pos);
2200         __ lgr_if_needed(callee_saved_length, length);
2201 
2202         __ z_llgfr(length, length); // Higher 32bits must be null.
2203 
2204         __ z_sllg(Z_ARG1, src_pos, shift_amount); // index -&gt; byte offset
2205         __ z_sllg(Z_ARG2, dst_pos, shift_amount); // index -&gt; byte offset
2206 
2207         __ z_la(Z_ARG1, Address(src, Z_ARG1, arrayOopDesc::base_offset_in_bytes(basic_type)));
2208         assert_different_registers(Z_ARG1, dst, dst_pos, length);
2209         __ z_la(Z_ARG2, Address(dst, Z_ARG2, arrayOopDesc::base_offset_in_bytes(basic_type)));
2210         assert_different_registers(Z_ARG2, dst, length);
2211 
2212         __ z_lgr(Z_ARG3, length);
2213         assert_different_registers(Z_ARG3, dst);
2214 
2215         __ load_klass(Z_ARG5, dst);
2216         __ z_lg(Z_ARG5, Address(Z_ARG5, ObjArrayKlass::element_klass_offset()));
2217         __ z_lg(Z_ARG4, Address(Z_ARG5, Klass::super_check_offset_offset()));
2218         emit_call_c(copyfunc_addr);
2219         CHECK_BAILOUT2(cont, slow);
2220 
2221 #ifndef PRODUCT
2222         if (PrintC1Statistics) {
2223           NearLabel failed;
2224           __ compareU32_and_branch(Z_RET, (intptr_t)0, Assembler::bcondNotEqual, failed);
2225           __ load_const_optimized(Z_R1_scratch, (address)&amp;Runtime1::_arraycopy_checkcast_cnt);
2226           __ add2mem_32(Address(Z_R1_scratch), 1, Z_R0_scratch);
2227           __ bind(failed);
2228         }
2229 #endif
2230 
2231         __ compareU32_and_branch(Z_RET, (intptr_t)0, Assembler::bcondEqual, *stub-&gt;continuation());
2232 
2233 #ifndef PRODUCT
2234         if (PrintC1Statistics) {
2235           __ load_const_optimized(Z_R1_scratch, (address)&amp;Runtime1::_arraycopy_checkcast_attempt_cnt);
2236           __ add2mem_32(Address(Z_R1_scratch), 1, Z_R0_scratch);
2237         }
2238 #endif
2239 
2240         __ z_lgr(tmp, Z_RET);
2241         __ z_xilf(tmp, -1);
2242 
2243         // Restore previously spilled arguments
2244         __ lgr_if_needed(src, callee_saved_src);
2245         __ lgr_if_needed(src_pos, callee_saved_src_pos);
2246         __ lgr_if_needed(dst, callee_saved_dst);
2247         __ lgr_if_needed(dst_pos, callee_saved_dst_pos);
2248         __ lgr_if_needed(length, callee_saved_length);
2249 
2250         __ z_sr(length, tmp);
2251         __ z_ar(src_pos, tmp);
2252         __ z_ar(dst_pos, tmp);
2253       }
2254 
2255       __ branch_optimized(Assembler::bcondAlways, *stub-&gt;entry());
2256 
2257       __ bind(cont);
2258     }
2259   }
2260 
2261 #ifdef ASSERT
2262   if (basic_type != T_OBJECT || !(flags &amp; LIR_OpArrayCopy::type_check)) {
2263     // Sanity check the known type with the incoming class. For the
2264     // primitive case the types must match exactly with src.klass and
2265     // dst.klass each exactly matching the default type. For the
2266     // object array case, if no type check is needed then either the
2267     // dst type is exactly the expected type and the src type is a
2268     // subtype which we can&#39;t check or src is the same array as dst
2269     // but not necessarily exactly of type default_type.
2270     NearLabel known_ok, halt;
2271     metadata2reg(default_type-&gt;constant_encoding(), tmp);
2272     if (UseCompressedClassPointers) {
2273       __ encode_klass_not_null(tmp);
2274     }
2275 
2276     if (basic_type != T_OBJECT) {
2277       if (UseCompressedClassPointers)         { __ z_c (tmp, dst_klass_addr); }
2278       else                                    { __ z_cg(tmp, dst_klass_addr); }
2279       __ branch_optimized(Assembler::bcondNotEqual, halt);
2280       if (UseCompressedClassPointers)         { __ z_c (tmp, src_klass_addr); }
2281       else                                    { __ z_cg(tmp, src_klass_addr); }
2282       __ branch_optimized(Assembler::bcondEqual, known_ok);
2283     } else {
2284       if (UseCompressedClassPointers)         { __ z_c (tmp, dst_klass_addr); }
2285       else                                    { __ z_cg(tmp, dst_klass_addr); }
2286       __ branch_optimized(Assembler::bcondEqual, known_ok);
2287       __ compareU64_and_branch(src, dst, Assembler::bcondEqual, known_ok);
2288     }
2289     __ bind(halt);
2290     __ stop(&quot;incorrect type information in arraycopy&quot;);
2291     __ bind(known_ok);
2292   }
2293 #endif
2294 
2295 #ifndef PRODUCT
2296   if (PrintC1Statistics) {
2297     __ load_const_optimized(Z_R1_scratch, Runtime1::arraycopy_count_address(basic_type));
2298     __ add2mem_32(Address(Z_R1_scratch), 1, Z_R0_scratch);
2299   }
2300 #endif
2301 
2302   __ z_sllg(tmp, src_pos, shift_amount); // index -&gt; byte offset
2303   __ z_sllg(Z_R1_scratch, dst_pos, shift_amount); // index -&gt; byte offset
2304 
2305   assert_different_registers(Z_ARG1, dst, dst_pos, length);
2306   __ z_la(Z_ARG1, Address(src, tmp, arrayOopDesc::base_offset_in_bytes(basic_type)));
2307   assert_different_registers(Z_ARG2, length);
2308   __ z_la(Z_ARG2, Address(dst, Z_R1_scratch, arrayOopDesc::base_offset_in_bytes(basic_type)));
2309   __ lgr_if_needed(Z_ARG3, length);
2310 
2311   bool disjoint = (flags &amp; LIR_OpArrayCopy::overlapping) == 0;
2312   bool aligned = (flags &amp; LIR_OpArrayCopy::unaligned) == 0;
2313   const char *name;
2314   address entry = StubRoutines::select_arraycopy_function(basic_type, aligned, disjoint, name, false);
2315   __ call_VM_leaf(entry);
2316 
2317   __ bind(*stub-&gt;continuation());
2318 }
2319 
2320 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, LIR_Opr count, LIR_Opr dest, LIR_Opr tmp) {
2321   if (dest-&gt;is_single_cpu()) {
2322     if (left-&gt;type() == T_OBJECT) {
2323       switch (code) {
2324         case lir_shl:  __ z_sllg (dest-&gt;as_register(), left-&gt;as_register(), 0, count-&gt;as_register()); break;
2325         case lir_shr:  __ z_srag (dest-&gt;as_register(), left-&gt;as_register(), 0, count-&gt;as_register()); break;
2326         case lir_ushr: __ z_srlg (dest-&gt;as_register(), left-&gt;as_register(), 0, count-&gt;as_register()); break;
2327         default: ShouldNotReachHere();
2328       }
2329     } else {
2330       assert(code == lir_shl || left == dest, &quot;left and dest must be equal for 2 operand form right shifts&quot;);
2331       Register masked_count = Z_R1_scratch;
2332       __ z_lr(masked_count, count-&gt;as_register());
2333       __ z_nill(masked_count, 31);
2334       switch (code) {
2335         case lir_shl:  __ z_sllg (dest-&gt;as_register(), left-&gt;as_register(), 0, masked_count); break;
2336         case lir_shr:  __ z_sra  (dest-&gt;as_register(), 0, masked_count); break;
2337         case lir_ushr: __ z_srl  (dest-&gt;as_register(), 0, masked_count); break;
2338         default: ShouldNotReachHere();
2339       }
2340     }
2341   } else {
2342     switch (code) {
2343       case lir_shl:  __ z_sllg (dest-&gt;as_register_lo(), left-&gt;as_register_lo(), 0, count-&gt;as_register()); break;
2344       case lir_shr:  __ z_srag (dest-&gt;as_register_lo(), left-&gt;as_register_lo(), 0, count-&gt;as_register()); break;
2345       case lir_ushr: __ z_srlg (dest-&gt;as_register_lo(), left-&gt;as_register_lo(), 0, count-&gt;as_register()); break;
2346       default: ShouldNotReachHere();
2347     }
2348   }
2349 }
2350 
2351 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, jint count, LIR_Opr dest) {
2352   if (left-&gt;type() == T_OBJECT) {
2353     count = count &amp; 63;  // Shouldn&#39;t shift by more than sizeof(intptr_t).
2354     Register l = left-&gt;as_register();
2355     Register d = dest-&gt;as_register_lo();
2356     switch (code) {
2357       case lir_shl:  __ z_sllg (d, l, count); break;
2358       case lir_shr:  __ z_srag (d, l, count); break;
2359       case lir_ushr: __ z_srlg (d, l, count); break;
2360       default: ShouldNotReachHere();
2361     }
2362     return;
2363   }
2364   if (dest-&gt;is_single_cpu()) {
2365     assert(code == lir_shl || left == dest, &quot;left and dest must be equal for 2 operand form right shifts&quot;);
2366     count = count &amp; 0x1F; // Java spec
2367     switch (code) {
2368       case lir_shl:  __ z_sllg (dest-&gt;as_register(), left-&gt;as_register(), count); break;
2369       case lir_shr:  __ z_sra  (dest-&gt;as_register(), count); break;
2370       case lir_ushr: __ z_srl  (dest-&gt;as_register(), count); break;
2371       default: ShouldNotReachHere();
2372     }
2373   } else if (dest-&gt;is_double_cpu()) {
2374     count = count &amp; 63; // Java spec
2375     Register l = left-&gt;as_pointer_register();
2376     Register d = dest-&gt;as_pointer_register();
2377     switch (code) {
2378       case lir_shl:  __ z_sllg (d, l, count); break;
2379       case lir_shr:  __ z_srag (d, l, count); break;
2380       case lir_ushr: __ z_srlg (d, l, count); break;
2381       default: ShouldNotReachHere();
2382     }
2383   } else {
2384     ShouldNotReachHere();
2385   }
2386 }
2387 
2388 void LIR_Assembler::emit_alloc_obj(LIR_OpAllocObj* op) {
2389   if (op-&gt;init_check()) {
2390     // Make sure klass is initialized &amp; doesn&#39;t have finalizer.
2391     const int state_offset = in_bytes(InstanceKlass::init_state_offset());
2392     Register iklass = op-&gt;klass()-&gt;as_register();
2393     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
2394     if (Immediate::is_uimm12(state_offset)) {
2395       __ z_cli(state_offset, iklass, InstanceKlass::fully_initialized);
2396     } else {
2397       __ z_cliy(state_offset, iklass, InstanceKlass::fully_initialized);
2398     }
2399     __ branch_optimized(Assembler::bcondNotEqual, *op-&gt;stub()-&gt;entry()); // Use long branch, because slow_case might be far.
2400   }
2401   __ allocate_object(op-&gt;obj()-&gt;as_register(),
2402                      op-&gt;tmp1()-&gt;as_register(),
2403                      op-&gt;tmp2()-&gt;as_register(),
2404                      op-&gt;header_size(),
2405                      op-&gt;object_size(),
2406                      op-&gt;klass()-&gt;as_register(),
2407                      *op-&gt;stub()-&gt;entry());
2408   __ bind(*op-&gt;stub()-&gt;continuation());
2409   __ verify_oop(op-&gt;obj()-&gt;as_register(), FILE_AND_LINE);
2410 }
2411 
2412 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
2413   Register len = op-&gt;len()-&gt;as_register();
2414   __ move_reg_if_needed(len, T_LONG, len, T_INT); // sign extend
2415 
2416   if (UseSlowPath ||
2417       (!UseFastNewObjectArray &amp;&amp; (is_reference_type(op-&gt;type()))) ||
2418       (!UseFastNewTypeArray   &amp;&amp; (!is_reference_type(op-&gt;type())))) {
2419     __ z_brul(*op-&gt;stub()-&gt;entry());
2420   } else {
2421     __ allocate_array(op-&gt;obj()-&gt;as_register(),
2422                       op-&gt;len()-&gt;as_register(),
2423                       op-&gt;tmp1()-&gt;as_register(),
2424                       op-&gt;tmp2()-&gt;as_register(),
2425                       arrayOopDesc::header_size(op-&gt;type()),
2426                       type2aelembytes(op-&gt;type()),
2427                       op-&gt;klass()-&gt;as_register(),
2428                       *op-&gt;stub()-&gt;entry());
2429   }
2430   __ bind(*op-&gt;stub()-&gt;continuation());
2431 }
2432 
2433 void LIR_Assembler::type_profile_helper(Register mdo, ciMethodData *md, ciProfileData *data,
2434                                         Register recv, Register tmp1, Label* update_done) {
2435   uint i;
2436   for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2437     Label next_test;
2438     // See if the receiver is receiver[n].
2439     Address receiver_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i)));
2440     __ z_cg(recv, receiver_addr);
2441     __ z_brne(next_test);
2442     Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)));
2443     __ add2mem_64(data_addr, DataLayout::counter_increment, tmp1);
2444     __ branch_optimized(Assembler::bcondAlways, *update_done);
2445     __ bind(next_test);
2446   }
2447 
2448   // Didn&#39;t find receiver; find next empty slot and fill it in.
2449   for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2450     Label next_test;
2451     Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i)));
2452     __ z_ltg(Z_R0_scratch, recv_addr);
2453     __ z_brne(next_test);
2454     __ z_stg(recv, recv_addr);
2455     __ load_const_optimized(tmp1, DataLayout::counter_increment);
2456     __ z_stg(tmp1, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)), mdo);
2457     __ branch_optimized(Assembler::bcondAlways, *update_done);
2458     __ bind(next_test);
2459   }
2460 }
2461 
2462 void LIR_Assembler::setup_md_access(ciMethod* method, int bci,
2463                                     ciMethodData*&amp; md, ciProfileData*&amp; data, int&amp; mdo_offset_bias) {
2464   Unimplemented();
2465 }
2466 
2467 void LIR_Assembler::store_parameter(Register r, int param_num) {
2468   assert(param_num &gt;= 0, &quot;invalid num&quot;);
2469   int offset_in_bytes = param_num * BytesPerWord + FrameMap::first_available_sp_in_frame;
2470   assert(offset_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
2471   __ z_stg(r, offset_in_bytes, Z_SP);
2472 }
2473 
2474 void LIR_Assembler::store_parameter(jint c, int param_num) {
2475   assert(param_num &gt;= 0, &quot;invalid num&quot;);
2476   int offset_in_bytes = param_num * BytesPerWord + FrameMap::first_available_sp_in_frame;
2477   assert(offset_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
2478   __ store_const(Address(Z_SP, offset_in_bytes), c, Z_R1_scratch, true);
2479 }
2480 
2481 void LIR_Assembler::emit_typecheck_helper(LIR_OpTypeCheck *op, Label* success, Label* failure, Label* obj_is_null) {
2482   // We always need a stub for the failure case.
2483   CodeStub* stub = op-&gt;stub();
2484   Register obj = op-&gt;object()-&gt;as_register();
2485   Register k_RInfo = op-&gt;tmp1()-&gt;as_register();
2486   Register klass_RInfo = op-&gt;tmp2()-&gt;as_register();
2487   Register dst = op-&gt;result_opr()-&gt;as_register();
2488   Register Rtmp1 = Z_R1_scratch;
2489   ciKlass* k = op-&gt;klass();
2490 
2491   assert(!op-&gt;tmp3()-&gt;is_valid(), &quot;tmp3&#39;s not needed&quot;);
2492 
2493   // Check if it needs to be profiled.
2494   ciMethodData* md = NULL;
2495   ciProfileData* data = NULL;
2496 
2497   if (op-&gt;should_profile()) {
2498     ciMethod* method = op-&gt;profiled_method();
2499     assert(method != NULL, &quot;Should have method&quot;);
2500     int bci = op-&gt;profiled_bci();
2501     md = method-&gt;method_data_or_null();
2502     assert(md != NULL, &quot;Sanity&quot;);
2503     data = md-&gt;bci_to_data(bci);
2504     assert(data != NULL,                &quot;need data for type check&quot;);
2505     assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
2506   }
2507 
2508   // Temp operands do not overlap with inputs, if this is their last
2509   // use (end of range is exclusive), so a register conflict is possible.
2510   if (obj == k_RInfo) {
2511     k_RInfo = dst;
2512   } else if (obj == klass_RInfo) {
2513     klass_RInfo = dst;
2514   }
2515   assert_different_registers(obj, k_RInfo, klass_RInfo);
2516 
2517   if (op-&gt;should_profile()) {
2518     NearLabel not_null;
2519     __ compareU64_and_branch(obj, (intptr_t) 0, Assembler::bcondNotEqual, not_null);
2520     // Object is null; update MDO and exit.
2521     Register mdo = klass_RInfo;
2522     metadata2reg(md-&gt;constant_encoding(), mdo);
2523     Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, DataLayout::header_offset()));
2524     int header_bits = DataLayout::flag_mask_to_header_mask(BitData::null_seen_byte_constant());
2525     __ or2mem_8(data_addr, header_bits);
2526     __ branch_optimized(Assembler::bcondAlways, *obj_is_null);
2527     __ bind(not_null);
2528   } else {
2529     __ compareU64_and_branch(obj, (intptr_t) 0, Assembler::bcondEqual, *obj_is_null);
2530   }
2531 
2532   NearLabel profile_cast_failure, profile_cast_success;
2533   Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : failure;
2534   Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : success;
2535 
2536   // Patching may screw with our temporaries on sparc,
2537   // so let&#39;s do it before loading the class.
2538   if (k-&gt;is_loaded()) {
2539     metadata2reg(k-&gt;constant_encoding(), k_RInfo);
2540   } else {
2541     klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
2542   }
2543   assert(obj != k_RInfo, &quot;must be different&quot;);
2544 
2545   __ verify_oop(obj, FILE_AND_LINE);
2546 
2547   // Get object class.
2548   // Not a safepoint as obj null check happens earlier.
2549   if (op-&gt;fast_check()) {
2550     if (UseCompressedClassPointers) {
2551       __ load_klass(klass_RInfo, obj);
2552       __ compareU64_and_branch(k_RInfo, klass_RInfo, Assembler::bcondNotEqual, *failure_target);
2553     } else {
2554       __ z_cg(k_RInfo, Address(obj, oopDesc::klass_offset_in_bytes()));
2555       __ branch_optimized(Assembler::bcondNotEqual, *failure_target);
2556     }
2557     // Successful cast, fall through to profile or jump.
2558   } else {
2559     bool need_slow_path = !k-&gt;is_loaded() ||
2560                           ((int) k-&gt;super_check_offset() == in_bytes(Klass::secondary_super_cache_offset()));
2561     intptr_t super_check_offset = k-&gt;is_loaded() ? k-&gt;super_check_offset() : -1L;
2562     __ load_klass(klass_RInfo, obj);
2563     // Perform the fast part of the checking logic.
2564     __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1,
2565                                      (need_slow_path ? success_target : NULL),
2566                                      failure_target, NULL,
2567                                      RegisterOrConstant(super_check_offset));
2568     if (need_slow_path) {
2569       // Call out-of-line instance of __ check_klass_subtype_slow_path(...):
2570       address a = Runtime1::entry_for (Runtime1::slow_subtype_check_id);
2571       store_parameter(klass_RInfo, 0); // sub
2572       store_parameter(k_RInfo, 1);     // super
2573       emit_call_c(a); // Sets condition code 0 for match (2 otherwise).
2574       CHECK_BAILOUT2(profile_cast_failure, profile_cast_success);
2575       __ branch_optimized(Assembler::bcondNotEqual, *failure_target);
2576       // Fall through to success case.
2577     }
2578   }
2579 
2580   if (op-&gt;should_profile()) {
2581     Register mdo = klass_RInfo, recv = k_RInfo;
2582     assert_different_registers(obj, mdo, recv);
2583     __ bind(profile_cast_success);
2584     metadata2reg(md-&gt;constant_encoding(), mdo);
2585     __ load_klass(recv, obj);
2586     type_profile_helper(mdo, md, data, recv, Rtmp1, success);
2587     __ branch_optimized(Assembler::bcondAlways, *success);
2588 
2589     __ bind(profile_cast_failure);
2590     metadata2reg(md-&gt;constant_encoding(), mdo);
2591     __ add2mem_64(Address(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset())), -(int)DataLayout::counter_increment, Rtmp1);
2592     __ branch_optimized(Assembler::bcondAlways, *failure);
2593   } else {
2594     __ branch_optimized(Assembler::bcondAlways, *success);
2595   }
2596 }
2597 
2598 void LIR_Assembler::emit_opTypeCheck(LIR_OpTypeCheck* op) {
2599   LIR_Code code = op-&gt;code();
2600   if (code == lir_store_check) {
2601     Register value = op-&gt;object()-&gt;as_register();
2602     Register array = op-&gt;array()-&gt;as_register();
2603     Register k_RInfo = op-&gt;tmp1()-&gt;as_register();
2604     Register klass_RInfo = op-&gt;tmp2()-&gt;as_register();
2605     Register Rtmp1 = Z_R1_scratch;
2606 
2607     CodeStub* stub = op-&gt;stub();
2608 
2609     // Check if it needs to be profiled.
2610     ciMethodData* md = NULL;
2611     ciProfileData* data = NULL;
2612 
2613     assert_different_registers(value, k_RInfo, klass_RInfo);
2614 
2615     if (op-&gt;should_profile()) {
2616       ciMethod* method = op-&gt;profiled_method();
2617       assert(method != NULL, &quot;Should have method&quot;);
2618       int bci = op-&gt;profiled_bci();
2619       md = method-&gt;method_data_or_null();
2620       assert(md != NULL, &quot;Sanity&quot;);
2621       data = md-&gt;bci_to_data(bci);
2622       assert(data != NULL,                &quot;need data for type check&quot;);
2623       assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
2624     }
2625     NearLabel profile_cast_success, profile_cast_failure, done;
2626     Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : &amp;done;
2627     Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : stub-&gt;entry();
2628 
2629     if (op-&gt;should_profile()) {
2630       NearLabel not_null;
2631       __ compareU64_and_branch(value, (intptr_t) 0, Assembler::bcondNotEqual, not_null);
2632       // Object is null; update MDO and exit.
2633       Register mdo = klass_RInfo;
2634       metadata2reg(md-&gt;constant_encoding(), mdo);
2635       Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, DataLayout::header_offset()));
2636       int header_bits = DataLayout::flag_mask_to_header_mask(BitData::null_seen_byte_constant());
2637       __ or2mem_8(data_addr, header_bits);
2638       __ branch_optimized(Assembler::bcondAlways, done);
2639       __ bind(not_null);
2640     } else {
2641       __ compareU64_and_branch(value, (intptr_t) 0, Assembler::bcondEqual, done);
2642     }
2643 
2644     add_debug_info_for_null_check_here(op-&gt;info_for_exception());
2645     __ load_klass(k_RInfo, array);
2646     __ load_klass(klass_RInfo, value);
2647 
2648     // Get instance klass (it&#39;s already uncompressed).
2649     __ z_lg(k_RInfo, Address(k_RInfo, ObjArrayKlass::element_klass_offset()));
2650     // Perform the fast part of the checking logic.
2651     __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);
2652     // Call out-of-line instance of __ check_klass_subtype_slow_path(...):
2653     address a = Runtime1::entry_for (Runtime1::slow_subtype_check_id);
2654     store_parameter(klass_RInfo, 0); // sub
2655     store_parameter(k_RInfo, 1);     // super
2656     emit_call_c(a); // Sets condition code 0 for match (2 otherwise).
2657     CHECK_BAILOUT3(profile_cast_success, profile_cast_failure, done);
2658     __ branch_optimized(Assembler::bcondNotEqual, *failure_target);
2659     // Fall through to success case.
2660 
2661     if (op-&gt;should_profile()) {
2662       Register mdo = klass_RInfo, recv = k_RInfo;
2663       assert_different_registers(value, mdo, recv);
2664       __ bind(profile_cast_success);
2665       metadata2reg(md-&gt;constant_encoding(), mdo);
2666       __ load_klass(recv, value);
2667       type_profile_helper(mdo, md, data, recv, Rtmp1, &amp;done);
2668       __ branch_optimized(Assembler::bcondAlways, done);
2669 
2670       __ bind(profile_cast_failure);
2671       metadata2reg(md-&gt;constant_encoding(), mdo);
2672       __ add2mem_64(Address(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset())), -(int)DataLayout::counter_increment, Rtmp1);
2673       __ branch_optimized(Assembler::bcondAlways, *stub-&gt;entry());
2674     }
2675 
2676     __ bind(done);
2677   } else {
2678     if (code == lir_checkcast) {
2679       Register obj = op-&gt;object()-&gt;as_register();
2680       Register dst = op-&gt;result_opr()-&gt;as_register();
2681       NearLabel success;
2682       emit_typecheck_helper(op, &amp;success, op-&gt;stub()-&gt;entry(), &amp;success);
2683       __ bind(success);
2684       __ lgr_if_needed(dst, obj);
2685     } else {
2686       if (code == lir_instanceof) {
2687         Register obj = op-&gt;object()-&gt;as_register();
2688         Register dst = op-&gt;result_opr()-&gt;as_register();
2689         NearLabel success, failure, done;
2690         emit_typecheck_helper(op, &amp;success, &amp;failure, &amp;failure);
2691         __ bind(failure);
2692         __ clear_reg(dst);
2693         __ branch_optimized(Assembler::bcondAlways, done);
2694         __ bind(success);
2695         __ load_const_optimized(dst, 1);
2696         __ bind(done);
2697       } else {
2698         ShouldNotReachHere();
2699       }
2700     }
2701   }
2702 }
2703 
2704 void LIR_Assembler::emit_compare_and_swap(LIR_OpCompareAndSwap* op) {
2705   Register addr = op-&gt;addr()-&gt;as_pointer_register();
2706   Register t1_cmp = Z_R1_scratch;
2707   if (op-&gt;code() == lir_cas_long) {
2708     assert(VM_Version::supports_cx8(), &quot;wrong machine&quot;);
2709     Register cmp_value_lo = op-&gt;cmp_value()-&gt;as_register_lo();
2710     Register new_value_lo = op-&gt;new_value()-&gt;as_register_lo();
2711     __ z_lgr(t1_cmp, cmp_value_lo);
2712     // Perform the compare and swap operation.
2713     __ z_csg(t1_cmp, new_value_lo, 0, addr);
2714   } else if (op-&gt;code() == lir_cas_int || op-&gt;code() == lir_cas_obj) {
2715     Register cmp_value = op-&gt;cmp_value()-&gt;as_register();
2716     Register new_value = op-&gt;new_value()-&gt;as_register();
2717     if (op-&gt;code() == lir_cas_obj) {
2718       if (UseCompressedOops) {
2719                  t1_cmp = op-&gt;tmp1()-&gt;as_register();
2720         Register t2_new = op-&gt;tmp2()-&gt;as_register();
2721         assert_different_registers(cmp_value, new_value, addr, t1_cmp, t2_new);
2722         __ oop_encoder(t1_cmp, cmp_value, true /*maybe null*/);
2723         __ oop_encoder(t2_new, new_value, true /*maybe null*/);
2724         __ z_cs(t1_cmp, t2_new, 0, addr);
2725       } else {
2726         __ z_lgr(t1_cmp, cmp_value);
2727         __ z_csg(t1_cmp, new_value, 0, addr);
2728       }
2729     } else {
2730       __ z_lr(t1_cmp, cmp_value);
2731       __ z_cs(t1_cmp, new_value, 0, addr);
2732     }
2733   } else {
2734     ShouldNotReachHere(); // new lir_cas_??
2735   }
2736 }
2737 
2738 void LIR_Assembler::breakpoint() {
2739   Unimplemented();
2740   //  __ breakpoint_trap();
2741 }
2742 
2743 void LIR_Assembler::push(LIR_Opr opr) {
2744   ShouldNotCallThis(); // unused
2745 }
2746 
2747 void LIR_Assembler::pop(LIR_Opr opr) {
2748   ShouldNotCallThis(); // unused
2749 }
2750 
2751 void LIR_Assembler::monitor_address(int monitor_no, LIR_Opr dst_opr) {
2752   Address addr = frame_map()-&gt;address_for_monitor_lock(monitor_no);
2753   __ add2reg(dst_opr-&gt;as_register(), addr.disp(), addr.base());
2754 }
2755 
2756 void LIR_Assembler::emit_lock(LIR_OpLock* op) {
2757   Register obj = op-&gt;obj_opr()-&gt;as_register();  // May not be an oop.
2758   Register hdr = op-&gt;hdr_opr()-&gt;as_register();
2759   Register lock = op-&gt;lock_opr()-&gt;as_register();
2760   if (!UseFastLocking) {
2761     __ branch_optimized(Assembler::bcondAlways, *op-&gt;stub()-&gt;entry());
2762   } else if (op-&gt;code() == lir_lock) {
2763     assert(BasicLock::displaced_header_offset_in_bytes() == 0, &quot;lock_reg must point to the displaced header&quot;);
2764     // Add debug info for NullPointerException only if one is possible.
2765     if (op-&gt;info() != NULL) {
2766       add_debug_info_for_null_check_here(op-&gt;info());
2767     }
2768     __ lock_object(hdr, obj, lock, *op-&gt;stub()-&gt;entry());
2769     // done
2770   } else if (op-&gt;code() == lir_unlock) {
2771     assert(BasicLock::displaced_header_offset_in_bytes() == 0, &quot;lock_reg must point to the displaced header&quot;);
2772     __ unlock_object(hdr, obj, lock, *op-&gt;stub()-&gt;entry());
2773   } else {
2774     ShouldNotReachHere();
2775   }
2776   __ bind(*op-&gt;stub()-&gt;continuation());
2777 }
2778 
2779 void LIR_Assembler::emit_profile_call(LIR_OpProfileCall* op) {
2780   ciMethod* method = op-&gt;profiled_method();
2781   int bci          = op-&gt;profiled_bci();
2782   ciMethod* callee = op-&gt;profiled_callee();
2783 
2784   // Update counter for all call types.
2785   ciMethodData* md = method-&gt;method_data_or_null();
2786   assert(md != NULL, &quot;Sanity&quot;);
2787   ciProfileData* data = md-&gt;bci_to_data(bci);
2788   assert(data != NULL &amp;&amp; data-&gt;is_CounterData(), &quot;need CounterData for calls&quot;);
2789   assert(op-&gt;mdo()-&gt;is_single_cpu(),  &quot;mdo must be allocated&quot;);
2790   Register mdo  = op-&gt;mdo()-&gt;as_register();
2791   assert(op-&gt;tmp1()-&gt;is_double_cpu(), &quot;tmp1 must be allocated&quot;);
2792   Register tmp1 = op-&gt;tmp1()-&gt;as_register_lo();
2793   metadata2reg(md-&gt;constant_encoding(), mdo);
2794 
2795   Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()));
2796   // Perform additional virtual call profiling for invokevirtual and
2797   // invokeinterface bytecodes
2798   if (op-&gt;should_profile_receiver_type()) {
2799     assert(op-&gt;recv()-&gt;is_single_cpu(), &quot;recv must be allocated&quot;);
2800     Register recv = op-&gt;recv()-&gt;as_register();
2801     assert_different_registers(mdo, tmp1, recv);
2802     assert(data-&gt;is_VirtualCallData(), &quot;need VirtualCallData for virtual calls&quot;);
2803     ciKlass* known_klass = op-&gt;known_holder();
2804     if (C1OptimizeVirtualCallProfiling &amp;&amp; known_klass != NULL) {
2805       // We know the type that will be seen at this call site; we can
2806       // statically update the MethodData* rather than needing to do
2807       // dynamic tests on the receiver type.
2808 
2809       // NOTE: we should probably put a lock around this search to
2810       // avoid collisions by concurrent compilations.
2811       ciVirtualCallData* vc_data = (ciVirtualCallData*) data;
2812       uint i;
2813       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2814         ciKlass* receiver = vc_data-&gt;receiver(i);
2815         if (known_klass-&gt;equals(receiver)) {
2816           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)));
2817           __ add2mem_64(data_addr, DataLayout::counter_increment, tmp1);
2818           return;
2819         }
2820       }
2821 
2822       // Receiver type not found in profile data. Select an empty slot.
2823 
2824       // Note that this is less efficient than it should be because it
2825       // always does a write to the receiver part of the
2826       // VirtualCallData rather than just the first time.
2827       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
2828         ciKlass* receiver = vc_data-&gt;receiver(i);
2829         if (receiver == NULL) {
2830           Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_offset(i)));
2831           metadata2reg(known_klass-&gt;constant_encoding(), tmp1);
2832           __ z_stg(tmp1, recv_addr);
2833           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)));
2834           __ add2mem_64(data_addr, DataLayout::counter_increment, tmp1);
2835           return;
2836         }
2837       }
2838     } else {
2839       __ load_klass(recv, recv);
2840       NearLabel update_done;
2841       type_profile_helper(mdo, md, data, recv, tmp1, &amp;update_done);
2842       // Receiver did not match any saved receiver and there is no empty row for it.
2843       // Increment total counter to indicate polymorphic case.
2844       __ add2mem_64(counter_addr, DataLayout::counter_increment, tmp1);
2845       __ bind(update_done);
2846     }
2847   } else {
2848     // static call
2849     __ add2mem_64(counter_addr, DataLayout::counter_increment, tmp1);
2850   }
2851 }
2852 
2853 void LIR_Assembler::align_backward_branch_target() {
2854   __ align(OptoLoopAlignment);
2855 }
2856 
2857 void LIR_Assembler::emit_delay(LIR_OpDelay* op) {
2858   ShouldNotCallThis(); // There are no delay slots on ZARCH_64.
2859 }
2860 
2861 void LIR_Assembler::negate(LIR_Opr left, LIR_Opr dest, LIR_Opr tmp) {
2862   // tmp must be unused
2863   assert(tmp-&gt;is_illegal(), &quot;wasting a register if tmp is allocated&quot;);
2864   assert(left-&gt;is_register(), &quot;can only handle registers&quot;);
2865 
2866   if (left-&gt;is_single_cpu()) {
2867     __ z_lcr(dest-&gt;as_register(), left-&gt;as_register());
2868   } else if (left-&gt;is_single_fpu()) {
2869     __ z_lcebr(dest-&gt;as_float_reg(), left-&gt;as_float_reg());
2870   } else if (left-&gt;is_double_fpu()) {
2871     __ z_lcdbr(dest-&gt;as_double_reg(), left-&gt;as_double_reg());
2872   } else {
2873     assert(left-&gt;is_double_cpu(), &quot;Must be a long&quot;);
2874     __ z_lcgr(dest-&gt;as_register_lo(), left-&gt;as_register_lo());
2875   }
2876 }
2877 
2878 void LIR_Assembler::rt_call(LIR_Opr result, address dest,
2879                             const LIR_OprList* args, LIR_Opr tmp, CodeEmitInfo* info) {
2880   assert(!tmp-&gt;is_valid(), &quot;don&#39;t need temporary&quot;);
2881   emit_call_c(dest);
2882   CHECK_BAILOUT();
2883   if (info != NULL) {
2884     add_call_info_here(info);
2885   }
2886 }
2887 
2888 void LIR_Assembler::volatile_move_op(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info) {
2889   ShouldNotCallThis(); // not needed on ZARCH_64
2890 }
2891 
2892 void LIR_Assembler::membar() {
2893   __ z_fence();
2894 }
2895 
2896 void LIR_Assembler::membar_acquire() {
2897   __ z_acquire();
2898 }
2899 
2900 void LIR_Assembler::membar_release() {
2901   __ z_release();
2902 }
2903 
2904 void LIR_Assembler::membar_loadload() {
2905   __ z_acquire();
2906 }
2907 
2908 void LIR_Assembler::membar_storestore() {
2909   __ z_release();
2910 }
2911 
2912 void LIR_Assembler::membar_loadstore() {
2913   __ z_acquire();
2914 }
2915 
2916 void LIR_Assembler::membar_storeload() {
2917   __ z_fence();
2918 }
2919 
2920 void LIR_Assembler::on_spin_wait() {
2921   Unimplemented();
2922 }
2923 
2924 void LIR_Assembler::leal(LIR_Opr addr_opr, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
2925   assert(patch_code == lir_patch_none, &quot;Patch code not supported&quot;);
2926   LIR_Address* addr = addr_opr-&gt;as_address_ptr();
2927   assert(addr-&gt;scale() == LIR_Address::times_1, &quot;scaling unsupported&quot;);
2928   __ load_address(dest-&gt;as_pointer_register(), as_Address(addr));
2929 }
2930 
2931 void LIR_Assembler::get_thread(LIR_Opr result_reg) {
2932   ShouldNotCallThis(); // unused
2933 }
2934 
2935 #ifdef ASSERT
2936 // Emit run-time assertion.
2937 void LIR_Assembler::emit_assert(LIR_OpAssert* op) {
2938   Unimplemented();
2939 }
2940 #endif
2941 
2942 void LIR_Assembler::peephole(LIR_List*) {
2943   // Do nothing for now.
2944 }
2945 
2946 void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp) {
2947   assert(code == lir_xadd, &quot;lir_xchg not supported&quot;);
2948   Address src_addr = as_Address(src-&gt;as_address_ptr());
2949   Register base = src_addr.base();
2950   intptr_t disp = src_addr.disp();
2951   if (src_addr.index()-&gt;is_valid()) {
2952     // LAA and LAAG do not support index register.
2953     __ load_address(Z_R1_scratch, src_addr);
2954     base = Z_R1_scratch;
2955     disp = 0;
2956   }
2957   if (data-&gt;type() == T_INT) {
2958     __ z_laa(dest-&gt;as_register(), data-&gt;as_register(), disp, base);
2959   } else if (data-&gt;type() == T_LONG) {
2960     assert(data-&gt;as_register_lo() == data-&gt;as_register_hi(), &quot;should be a single register&quot;);
2961     __ z_laag(dest-&gt;as_register_lo(), data-&gt;as_register_lo(), disp, base);
2962   } else {
2963     ShouldNotReachHere();
2964   }
2965 }
2966 
2967 void LIR_Assembler::emit_profile_type(LIR_OpProfileType* op) {
2968   Register obj = op-&gt;obj()-&gt;as_register();
2969   Register tmp1 = op-&gt;tmp()-&gt;as_pointer_register();
2970   Register tmp2 = Z_R1_scratch;
2971   Address mdo_addr = as_Address(op-&gt;mdp()-&gt;as_address_ptr());
2972   ciKlass* exact_klass = op-&gt;exact_klass();
2973   intptr_t current_klass = op-&gt;current_klass();
2974   bool not_null = op-&gt;not_null();
2975   bool no_conflict = op-&gt;no_conflict();
2976 
2977   Label update, next, none, null_seen, init_klass;
2978 
2979   bool do_null = !not_null;
2980   bool exact_klass_set = exact_klass != NULL &amp;&amp; ciTypeEntries::valid_ciklass(current_klass) == exact_klass;
2981   bool do_update = !TypeEntries::is_type_unknown(current_klass) &amp;&amp; !exact_klass_set;
2982 
2983   assert(do_null || do_update, &quot;why are we here?&quot;);
2984   assert(!TypeEntries::was_null_seen(current_klass) || do_update, &quot;why are we here?&quot;);
2985 
2986   __ verify_oop(obj, FILE_AND_LINE);
2987 
2988   if (do_null || tmp1 != obj DEBUG_ONLY(|| true)) {
2989     __ z_ltgr(tmp1, obj);
2990   }
2991   if (do_null) {
2992     __ z_brnz(update);
2993     if (!TypeEntries::was_null_seen(current_klass)) {
2994       __ z_lg(tmp1, mdo_addr);
2995       __ z_oill(tmp1, TypeEntries::null_seen);
2996       __ z_stg(tmp1, mdo_addr);
2997     }
2998     if (do_update) {
2999       __ z_bru(next);
3000     }
3001   } else {
3002     __ asm_assert_ne(&quot;unexpect null obj&quot;, __LINE__);
3003   }
3004 
3005   __ bind(update);
3006 
3007   if (do_update) {
3008 #ifdef ASSERT
3009     if (exact_klass != NULL) {
3010       __ load_klass(tmp1, tmp1);
3011       metadata2reg(exact_klass-&gt;constant_encoding(), tmp2);
3012       __ z_cgr(tmp1, tmp2);
3013       __ asm_assert_eq(&quot;exact klass and actual klass differ&quot;, __LINE__);
3014     }
3015 #endif
3016 
3017     Label do_update;
3018     __ z_lg(tmp2, mdo_addr);
3019 
3020     if (!no_conflict) {
3021       if (exact_klass == NULL || TypeEntries::is_type_none(current_klass)) {
3022         if (exact_klass != NULL) {
3023           metadata2reg(exact_klass-&gt;constant_encoding(), tmp1);
3024         } else {
3025           __ load_klass(tmp1, tmp1);
3026         }
3027 
3028         // Klass seen before: nothing to do (regardless of unknown bit).
3029         __ z_lgr(Z_R0_scratch, tmp2);
3030         assert(Immediate::is_uimm(~TypeEntries::type_klass_mask, 16), &quot;or change following instruction&quot;);
3031         __ z_nill(Z_R0_scratch, TypeEntries::type_klass_mask &amp; 0xFFFF);
3032         __ compareU64_and_branch(Z_R0_scratch, tmp1, Assembler::bcondEqual, next);
3033 
3034         // Already unknown: Nothing to do anymore.
3035         __ z_tmll(tmp2, TypeEntries::type_unknown);
3036         __ z_brc(Assembler::bcondAllOne, next);
3037 
3038         if (TypeEntries::is_type_none(current_klass)) {
3039           __ z_lgr(Z_R0_scratch, tmp2);
3040           assert(Immediate::is_uimm(~TypeEntries::type_mask, 16), &quot;or change following instruction&quot;);
3041           __ z_nill(Z_R0_scratch, TypeEntries::type_mask &amp; 0xFFFF);
3042           __ compareU64_and_branch(Z_R0_scratch, (intptr_t)0, Assembler::bcondEqual, init_klass);
3043         }
3044       } else {
3045         assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &amp;&amp;
3046                ciTypeEntries::valid_ciklass(current_klass) != exact_klass, &quot;conflict only&quot;);
3047 
3048         // Already unknown: Nothing to do anymore.
3049         __ z_tmll(tmp2, TypeEntries::type_unknown);
3050         __ z_brc(Assembler::bcondAllOne, next);
3051       }
3052 
3053       // Different than before. Cannot keep accurate profile.
3054       __ z_oill(tmp2, TypeEntries::type_unknown);
3055       __ z_bru(do_update);
3056     } else {
3057       // There&#39;s a single possible klass at this profile point.
3058       assert(exact_klass != NULL, &quot;should be&quot;);
3059       if (TypeEntries::is_type_none(current_klass)) {
3060         metadata2reg(exact_klass-&gt;constant_encoding(), tmp1);
3061         __ z_lgr(Z_R0_scratch, tmp2);
3062         assert(Immediate::is_uimm(~TypeEntries::type_klass_mask, 16), &quot;or change following instruction&quot;);
3063         __ z_nill(Z_R0_scratch, TypeEntries::type_klass_mask &amp; 0xFFFF);
3064         __ compareU64_and_branch(Z_R0_scratch, tmp1, Assembler::bcondEqual, next);
3065 #ifdef ASSERT
3066         {
3067           Label ok;
3068           __ z_lgr(Z_R0_scratch, tmp2);
3069           assert(Immediate::is_uimm(~TypeEntries::type_mask, 16), &quot;or change following instruction&quot;);
3070           __ z_nill(Z_R0_scratch, TypeEntries::type_mask &amp; 0xFFFF);
3071           __ compareU64_and_branch(Z_R0_scratch, (intptr_t)0, Assembler::bcondEqual, ok);
3072           __ stop(&quot;unexpected profiling mismatch&quot;);
3073           __ bind(ok);
3074         }
3075 #endif
3076 
3077       } else {
3078         assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &amp;&amp;
3079                ciTypeEntries::valid_ciklass(current_klass) != exact_klass, &quot;inconsistent&quot;);
3080 
3081         // Already unknown: Nothing to do anymore.
3082         __ z_tmll(tmp2, TypeEntries::type_unknown);
3083         __ z_brc(Assembler::bcondAllOne, next);
3084         __ z_oill(tmp2, TypeEntries::type_unknown);
3085         __ z_bru(do_update);
3086       }
3087     }
3088 
3089     __ bind(init_klass);
3090     // Combine klass and null_seen bit (only used if (tmp &amp; type_mask)==0).
3091     __ z_ogr(tmp2, tmp1);
3092 
3093     __ bind(do_update);
3094     __ z_stg(tmp2, mdo_addr);
3095 
3096     __ bind(next);
3097   }
3098 }
3099 
3100 void LIR_Assembler::emit_updatecrc32(LIR_OpUpdateCRC32* op) {
3101   assert(op-&gt;crc()-&gt;is_single_cpu(), &quot;crc must be register&quot;);
3102   assert(op-&gt;val()-&gt;is_single_cpu(), &quot;byte value must be register&quot;);
3103   assert(op-&gt;result_opr()-&gt;is_single_cpu(), &quot;result must be register&quot;);
3104   Register crc = op-&gt;crc()-&gt;as_register();
3105   Register val = op-&gt;val()-&gt;as_register();
3106   Register res = op-&gt;result_opr()-&gt;as_register();
3107 
3108   assert_different_registers(val, crc, res);
3109 
3110   __ load_const_optimized(res, StubRoutines::crc_table_addr());
3111   __ kernel_crc32_singleByteReg(crc, val, res, true);
3112   __ z_lgfr(res, crc);
3113 }
3114 
3115 #undef __
    </pre>
  </body>
</html>