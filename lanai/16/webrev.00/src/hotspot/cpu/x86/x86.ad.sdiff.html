<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/x86.ad</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="vtableStubs_x86_64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="x86_32.ad.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/x86.ad</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1245   static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }
1246 
1247 //=============================================================================
1248 const bool Matcher::match_rule_supported(int opcode) {
1249   if (!has_match_rule(opcode)) {
1250     return false; // no match rule present
1251   }
1252   switch (opcode) {
1253     case Op_AbsVL:
1254       if (UseAVX &lt; 3) {
1255         return false;
1256       }
1257       break;
1258     case Op_PopCountI:
1259     case Op_PopCountL:
1260       if (!UsePopCountInstruction) {
1261         return false;
1262       }
1263       break;
1264     case Op_PopCountVI:
<span class="line-modified">1265       if (!UsePopCountInstruction || !VM_Version::supports_vpopcntdq()) {</span>
1266         return false;
1267       }
1268       break;
1269     case Op_MulVI:
1270       if ((UseSSE &lt; 4) &amp;&amp; (UseAVX &lt; 1)) { // only with SSE4_1 or AVX
1271         return false;
1272       }
1273       break;
1274     case Op_MulVL:
1275     case Op_MulReductionVL:
1276       if (VM_Version::supports_avx512dq() == false) {
1277         return false;
1278       }
1279       break;
1280     case Op_AddReductionVL:
1281       if (UseAVX &lt; 3) { // only EVEX : vector connectivity becomes an issue here
1282         return false;
1283       }
1284       break;
1285     case Op_AbsVB:
1286     case Op_AbsVS:
1287     case Op_AbsVI:
1288     case Op_AddReductionVI:
1289       if (UseSSE &lt; 3 || !VM_Version::supports_ssse3()) { // requires at least SSSE3
1290         return false;
1291       }
1292       break;
1293     case Op_MulReductionVI:
1294       if (UseSSE &lt; 4) { // requires at least SSE4
1295         return false;
1296       }
1297       break;
<span class="line-removed">1298     case Op_AddReductionVF:</span>
<span class="line-removed">1299     case Op_AddReductionVD:</span>
<span class="line-removed">1300     case Op_MulReductionVF:</span>
<span class="line-removed">1301     case Op_MulReductionVD:</span>
<span class="line-removed">1302       if (UseSSE &lt; 1) { // requires at least SSE</span>
<span class="line-removed">1303         return false;</span>
<span class="line-removed">1304       }</span>
<span class="line-removed">1305       break;</span>
1306     case Op_SqrtVD:
1307     case Op_SqrtVF:
1308       if (UseAVX &lt; 1) { // enabled for AVX only
1309         return false;
1310       }
1311       break;
1312     case Op_CompareAndSwapL:
1313 #ifdef _LP64
1314     case Op_CompareAndSwapP:
1315 #endif
1316       if (!VM_Version::supports_cx8()) {
1317         return false;
1318       }
1319       break;
1320     case Op_CMoveVF:
1321     case Op_CMoveVD:
1322       if (UseAVX &lt; 1 || UseAVX &gt; 2) {
1323         return false;
1324       }
1325       break;
1326     case Op_StrIndexOf:
1327       if (!UseSSE42Intrinsics) {
1328         return false;
1329       }
1330       break;
1331     case Op_StrIndexOfChar:
1332       if (!UseSSE42Intrinsics) {
1333         return false;
1334       }
1335       break;
1336     case Op_OnSpinWait:
1337       if (VM_Version::supports_on_spin_wait() == false) {
1338         return false;
1339       }
1340       break;
<span class="line-removed">1341     case Op_MulAddVS2VI:</span>
<span class="line-removed">1342     case Op_RShiftVL:</span>
<span class="line-removed">1343     case Op_AbsVD:</span>
<span class="line-removed">1344     case Op_NegVD:</span>
<span class="line-removed">1345       if (UseSSE &lt; 2) {</span>
<span class="line-removed">1346         return false;</span>
<span class="line-removed">1347       }</span>
<span class="line-removed">1348       break;</span>
1349     case Op_MulVB:
1350     case Op_LShiftVB:
1351     case Op_RShiftVB:
1352     case Op_URShiftVB:
1353       if (UseSSE &lt; 4) {
1354         return false;
1355       }
1356       break;
1357 #ifdef _LP64
1358     case Op_MaxD:
1359     case Op_MaxF:
1360     case Op_MinD:
1361     case Op_MinF:
1362       if (UseAVX &lt; 1) { // enabled for AVX only
1363         return false;
1364       }
1365       break;
1366 #endif
1367     case Op_CacheWB:
1368     case Op_CacheWBPreSync:
1369     case Op_CacheWBPostSync:
1370       if (!VM_Version::supports_data_cache_line_flush()) {
1371         return false;
1372       }
1373       break;
1374     case Op_RoundDoubleMode:
1375       if (UseSSE &lt; 4) {
1376         return false;
1377       }
1378       break;
1379     case Op_RoundDoubleModeV:
1380       if (VM_Version::supports_avx() == false) {
1381         return false; // 128bit vroundpd is not available
1382       }
1383       break;


















1384   }
1385   return true;  // Match rules are supported by default.
1386 }
1387 
1388 //------------------------------------------------------------------------
1389 
1390 // Identify extra cases that we might want to provide match rules for vector nodes and
1391 // other intrinsics guarded with vector length (vlen) and element type (bt).
1392 const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {
1393   if (!match_rule_supported(opcode)) {
1394     return false;
1395   }
1396   // Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):
1397   //   * SSE2 supports 128bit vectors for all types;
1398   //   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;
1399   //   * AVX2 supports 256bit vectors for all types;
1400   //   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;
1401   //   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.
1402   // There&#39;s also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).
1403   // And MaxVectorSize is taken into account as well.
</pre>
<hr />
<pre>
3105   match(Set mem (StoreVector mem src));
3106   ins_cost(145);
3107   format %{ &quot;store_vector $mem,$src\n\t&quot; %}
3108   ins_encode %{
3109     switch (vector_length_in_bytes(this, $src)) {
3110       case  4: __ movdl    ($mem$$Address, $src$$XMMRegister); break;
3111       case  8: __ movq     ($mem$$Address, $src$$XMMRegister); break;
3112       case 16: __ movdqu   ($mem$$Address, $src$$XMMRegister); break;
3113       case 32: __ vmovdqu  ($mem$$Address, $src$$XMMRegister); break;
3114       case 64: __ evmovdqul($mem$$Address, $src$$XMMRegister, Assembler::AVX_512bit); break;
3115       default: ShouldNotReachHere();
3116     }
3117   %}
3118   ins_pipe( pipe_slow );
3119 %}
3120 
3121 // ====================REPLICATE=======================================
3122 
3123 // Replicate byte scalar to be vector
3124 instruct ReplB_reg(vec dst, rRegI src) %{
<span class="line-removed">3125   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32) ||</span>
<span class="line-removed">3126             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions</span>
3127   match(Set dst (ReplicateB src));
3128   format %{ &quot;replicateB $dst,$src&quot; %}
3129   ins_encode %{
3130     uint vlen = vector_length(this);
3131     if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<span class="line-modified">3132       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
3133       int vlen_enc = vector_length_encoding(this);
3134       __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);
3135     } else {
3136       __ movdl($dst$$XMMRegister, $src$$Register);
3137       __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
3138       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3139       if (vlen &gt;= 16) {
3140         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3141         if (vlen &gt;= 32) {
<span class="line-modified">3142           assert(vlen == 32, &quot;sanity&quot;); // vlen == 64 &amp;&amp; !AVX512BW is covered by ReplB_reg_leg</span>
3143           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3144         }
3145       }
3146     }
3147   %}
3148   ins_pipe( pipe_slow );
3149 %}
3150 
<span class="line-removed">3151 instruct ReplB_reg_leg(legVec dst, rRegI src) %{</span>
<span class="line-removed">3152   predicate(n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; !VM_Version::supports_avx512bw()); // AVX512BW for 512bit byte instructions</span>
<span class="line-removed">3153   match(Set dst (ReplicateB src));</span>
<span class="line-removed">3154   format %{ &quot;replicateB $dst,$src&quot; %}</span>
<span class="line-removed">3155   ins_encode %{</span>
<span class="line-removed">3156     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
<span class="line-removed">3157     __ movdl($dst$$XMMRegister, $src$$Register);</span>
<span class="line-removed">3158     __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3159     __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-removed">3160     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3161     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3162     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3163   %}</span>
<span class="line-removed">3164   ins_pipe( pipe_slow );</span>
<span class="line-removed">3165 %}</span>
<span class="line-removed">3166 </span>
3167 instruct ReplB_mem(vec dst, memory mem) %{
<span class="line-modified">3168   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32 &amp;&amp; VM_Version::supports_avx512vlbw()) || // AVX512VL for &lt;512bit operands</span>
<span class="line-removed">3169             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw()));    // AVX512BW for 512bit byte instructions</span>
3170   match(Set dst (ReplicateB (LoadB mem)));
3171   format %{ &quot;replicateB $dst,$mem&quot; %}
3172   ins_encode %{
<span class="line-removed">3173     assert(UseAVX &gt; 2, &quot;required&quot;);</span>
3174     int vector_len = vector_length_encoding(this);
3175     __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
3176   %}
3177   ins_pipe( pipe_slow );
3178 %}
3179 
3180 instruct ReplB_imm(vec dst, immI con) %{
<span class="line-removed">3181   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32) ||</span>
<span class="line-removed">3182             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions</span>
3183   match(Set dst (ReplicateB con));
3184   format %{ &quot;replicateB $dst,$con&quot; %}
3185   ins_encode %{
3186     uint vlen = vector_length(this);
3187     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 1));
3188     if (vlen == 4) {
3189       __ movdl($dst$$XMMRegister, const_addr);
3190     } else {
3191       __ movq($dst$$XMMRegister, const_addr);
3192       if (vlen &gt;= 16) {
<span class="line-modified">3193         if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands</span>
3194           int vlen_enc = vector_length_encoding(this);
<span class="line-modified">3195           __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
3196         } else {

3197           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-removed">3198           if (vlen &gt;= 32) {</span>
<span class="line-removed">3199              assert(vlen == 32, &quot;sanity&quot;);// vlen == 64 &amp;&amp; !AVX512BW is covered by ReplB_imm_leg</span>
<span class="line-removed">3200             __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3201           }</span>
3202         }
3203       }
3204     }
3205   %}
3206   ins_pipe( pipe_slow );
3207 %}
3208 
<span class="line-removed">3209 instruct ReplB_imm_leg(legVec dst, immI con) %{</span>
<span class="line-removed">3210   predicate(n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3211   match(Set dst (ReplicateB con));</span>
<span class="line-removed">3212   format %{ &quot;replicateB $dst,$con&quot; %}</span>
<span class="line-removed">3213   ins_encode %{</span>
<span class="line-removed">3214     __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 1)));</span>
<span class="line-removed">3215     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3216     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3217     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3218   %}</span>
<span class="line-removed">3219   ins_pipe( pipe_slow );</span>
<span class="line-removed">3220 %}</span>
<span class="line-removed">3221 </span>
3222 // Replicate byte scalar zero to be vector
3223 instruct ReplB_zero(vec dst, immI0 zero) %{
3224   match(Set dst (ReplicateB zero));
3225   format %{ &quot;replicateB $dst,$zero&quot; %}
3226   ins_encode %{
3227     uint vlen = vector_length(this);
3228     if (vlen &lt;= 16) {
3229       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3230     } else {
3231       // Use vpxor since AVX512F does not have 512bit vxorpd (requires AVX512DQ).
3232       int vlen_enc = vector_length_encoding(this);
3233       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3234     }
3235   %}
3236   ins_pipe( fpu_reg_reg );
3237 %}
3238 
3239 // ====================ReplicateS=======================================
3240 
3241 instruct ReplS_reg(vec dst, rRegI src) %{
<span class="line-removed">3242   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 16) ||</span>
<span class="line-removed">3243             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts</span>
3244   match(Set dst (ReplicateS src));
3245   format %{ &quot;replicateS $dst,$src&quot; %}
3246   ins_encode %{
3247     uint vlen = vector_length(this);
3248     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<span class="line-modified">3249       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
3250       int vlen_enc = vector_length_encoding(this);
3251       __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);
3252     } else {
3253       __ movdl($dst$$XMMRegister, $src$$Register);
3254       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3255       if (vlen &gt;= 8) {
3256         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3257         if (vlen &gt;= 16) {
<span class="line-modified">3258           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_reg_leg</span>
3259           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3260         }
3261       }
3262     }
3263   %}
3264   ins_pipe( pipe_slow );
3265 %}
3266 
<span class="line-removed">3267 instruct ReplS_reg_leg(legVec dst, rRegI src) %{</span>
<span class="line-removed">3268   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3269   match(Set dst (ReplicateS src));</span>
<span class="line-removed">3270   format %{ &quot;replicateS $dst,$src&quot; %}</span>
<span class="line-removed">3271   ins_encode %{</span>
<span class="line-removed">3272     __ movdl($dst$$XMMRegister, $src$$Register);</span>
<span class="line-removed">3273     __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-removed">3274     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3275     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3276     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3277   %}</span>
<span class="line-removed">3278   ins_pipe( pipe_slow );</span>
<span class="line-removed">3279 %}</span>
<span class="line-removed">3280 </span>
3281 instruct ReplS_mem(vec dst, memory mem) %{
<span class="line-modified">3282   predicate((n-&gt;as_Vector()-&gt;length() &gt;= 4  &amp;&amp;</span>
<span class="line-removed">3283              n-&gt;as_Vector()-&gt;length() &lt;= 16 &amp;&amp; VM_Version::supports_avx()) ||</span>
<span class="line-removed">3284             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts</span>
3285   match(Set dst (ReplicateS (LoadS mem)));
3286   format %{ &quot;replicateS $dst,$mem&quot; %}
3287   ins_encode %{
<span class="line-modified">3288     uint vlen = vector_length(this);</span>
<span class="line-modified">3289     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands</span>
<span class="line-removed">3290       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
<span class="line-removed">3291       int vlen_enc = vector_length_encoding(this);</span>
<span class="line-removed">3292       __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);</span>
<span class="line-removed">3293     } else {</span>
<span class="line-removed">3294       __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3295       if (vlen &gt;= 8) {</span>
<span class="line-removed">3296         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3297         if (vlen &gt;= 16) {</span>
<span class="line-removed">3298           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_mem_leg</span>
<span class="line-removed">3299           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3300         }</span>
<span class="line-removed">3301       }</span>
<span class="line-removed">3302     }</span>
<span class="line-removed">3303   %}</span>
<span class="line-removed">3304   ins_pipe( pipe_slow );</span>
<span class="line-removed">3305 %}</span>
<span class="line-removed">3306 </span>
<span class="line-removed">3307 instruct ReplS_mem_leg(legVec dst, memory mem) %{</span>
<span class="line-removed">3308   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3309   match(Set dst (ReplicateS (LoadS mem)));</span>
<span class="line-removed">3310   format %{ &quot;replicateS $dst,$mem&quot; %}</span>
<span class="line-removed">3311   ins_encode %{</span>
<span class="line-removed">3312     __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3313     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3314     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3315     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
3316   %}
3317   ins_pipe( pipe_slow );
3318 %}
3319 
3320 instruct ReplS_imm(vec dst, immI con) %{
<span class="line-removed">3321   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 16) ||</span>
<span class="line-removed">3322             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts</span>
3323   match(Set dst (ReplicateS con));
3324   format %{ &quot;replicateS $dst,$con&quot; %}
3325   ins_encode %{
3326     uint vlen = vector_length(this);
<span class="line-modified">3327     InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 2));</span>
3328     if (vlen == 2) {
<span class="line-modified">3329       __ movdl($dst$$XMMRegister, constaddr);</span>
3330     } else {
<span class="line-modified">3331       __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-modified">3332       if (vlen == 32 || VM_Version::supports_avx512vlbw() ) { // AVX512VL for &lt;512bit operands</span>
<span class="line-modified">3333         assert(VM_Version::supports_avx512bw(), &quot;required&quot;);</span>
<span class="line-modified">3334         int vlen_enc = vector_length_encoding(this);</span>
<span class="line-modified">3335         __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
<span class="line-modified">3336       } else {</span>
<span class="line-modified">3337         __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-modified">3338         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3339         if (vlen &gt;= 16) {</span>
<span class="line-removed">3340           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_imm_leg</span>
<span class="line-removed">3341           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3342         }
3343       }
3344     }
3345   %}
3346   ins_pipe( fpu_reg_reg );
3347 %}
3348 
<span class="line-removed">3349 instruct ReplS_imm_leg(legVec dst, immI con) %{</span>
<span class="line-removed">3350   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());</span>
<span class="line-removed">3351   match(Set dst (ReplicateS con));</span>
<span class="line-removed">3352   format %{ &quot;replicateS $dst,$con&quot; %}</span>
<span class="line-removed">3353   ins_encode %{</span>
<span class="line-removed">3354     __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 2)));</span>
<span class="line-removed">3355     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3356     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3357     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3358   %}</span>
<span class="line-removed">3359   ins_pipe( pipe_slow );</span>
<span class="line-removed">3360 %}</span>
<span class="line-removed">3361 </span>
3362 instruct ReplS_zero(vec dst, immI0 zero) %{
3363   match(Set dst (ReplicateS zero));
3364   format %{ &quot;replicateS $dst,$zero&quot; %}
3365   ins_encode %{
3366     uint vlen = vector_length(this);
3367     if (vlen &lt;= 8) {
3368       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3369     } else {
3370       int vlen_enc = vector_length_encoding(this);
3371       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3372     }
3373   %}
3374   ins_pipe( fpu_reg_reg );
3375 %}
3376 
3377 // ====================ReplicateI=======================================
3378 
3379 instruct ReplI_reg(vec dst, rRegI src) %{
<span class="line-removed">3380   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 8) ||</span>
<span class="line-removed">3381             (n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; VM_Version::supports_avx512vl()));</span>
3382   match(Set dst (ReplicateI src));
3383   format %{ &quot;replicateI $dst,$src&quot; %}
3384   ins_encode %{
3385     uint vlen = vector_length(this);
<span class="line-modified">3386     if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>
3387       int vlen_enc = vector_length_encoding(this);
3388       __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vlen_enc);
3389     } else {
3390       __ movdl($dst$$XMMRegister, $src$$Register);
3391       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3392       if (vlen &gt;= 8) {
<span class="line-modified">3393         assert(vlen == 8, &quot;sanity&quot;); // vlen == 16 &amp;&amp; !AVX512VL is covered by ReplI_reg_leg</span>
3394         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3395       }
3396     }
3397   %}
3398   ins_pipe( pipe_slow );
3399 %}
3400 
<span class="line-removed">3401 instruct ReplI_reg_leg(legVec dst, rRegI src) %{</span>
<span class="line-removed">3402   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; !VM_Version::supports_avx512vl());</span>
<span class="line-removed">3403   match(Set dst (ReplicateI src));</span>
<span class="line-removed">3404   format %{ &quot;replicateI  $dst,$src&quot; %}</span>
<span class="line-removed">3405   ins_encode %{</span>
<span class="line-removed">3406     __ movdl($dst$$XMMRegister, $src$$Register);</span>
<span class="line-removed">3407     __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-removed">3408     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3409     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3410   %}</span>
<span class="line-removed">3411   ins_pipe( pipe_slow );</span>
<span class="line-removed">3412 %}</span>
<span class="line-removed">3413 </span>
3414 instruct ReplI_mem(vec dst, memory mem) %{
<span class="line-removed">3415   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 8  &amp;&amp; VM_Version::supports_avx()) ||</span>
<span class="line-removed">3416             (n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; VM_Version::supports_avx512vl()));</span>
3417   match(Set dst (ReplicateI (LoadI mem)));
3418   format %{ &quot;replicateI $dst,$mem&quot; %}
3419   ins_encode %{
3420     uint vlen = vector_length(this);
3421     if (vlen &lt;= 4) {
<span class="line-modified">3422       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-modified">3423     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>


3424       int vector_len = vector_length_encoding(this);
3425       __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);
<span class="line-removed">3426     } else {</span>
<span class="line-removed">3427       assert(vlen == 8, &quot;sanity&quot;); // vlen == 16 &amp;&amp; !AVX512VL is covered by ReplI_mem_leg</span>
<span class="line-removed">3428       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3429       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3430     }
3431   %}
3432   ins_pipe( pipe_slow );
3433 %}
3434 
<span class="line-removed">3435 instruct ReplI_mem_leg(legVec dst, memory mem) %{</span>
<span class="line-removed">3436   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; !VM_Version::supports_avx512vl());</span>
<span class="line-removed">3437   match(Set dst (ReplicateI (LoadI mem)));</span>
<span class="line-removed">3438   format %{ &quot;replicateI $dst,$mem&quot; %}</span>
<span class="line-removed">3439   ins_encode %{</span>
<span class="line-removed">3440     __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3441     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3442     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3443   %}</span>
<span class="line-removed">3444   ins_pipe( pipe_slow );</span>
<span class="line-removed">3445 %}</span>
<span class="line-removed">3446 </span>
3447 instruct ReplI_imm(vec dst, immI con) %{
<span class="line-removed">3448   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 8) ||</span>
<span class="line-removed">3449             (n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; VM_Version::supports_avx512vl()));</span>
3450   match(Set dst (ReplicateI con));
3451   format %{ &quot;replicateI $dst,$con&quot; %}
3452   ins_encode %{
3453     uint vlen = vector_length(this);
<span class="line-modified">3454     InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 4));</span>
<span class="line-modified">3455     if (vlen == 2) {</span>
<span class="line-modified">3456       __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-modified">3457     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>




3458       int vector_len = vector_length_encoding(this);
<span class="line-modified">3459       __ movq($dst$$XMMRegister, constaddr);</span>
3460       __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
<span class="line-removed">3461     } else {</span>
<span class="line-removed">3462       __ movq($dst$$XMMRegister, constaddr);</span>
<span class="line-removed">3463       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3464       if (vlen &gt;= 8) {</span>
<span class="line-removed">3465         assert(vlen == 8, &quot;sanity&quot;);</span>
<span class="line-removed">3466         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3467       }</span>
3468     }
3469   %}
3470   ins_pipe( pipe_slow );
3471 %}
3472 
<span class="line-removed">3473 instruct ReplI_imm_leg(legVec dst, immI con) %{</span>
<span class="line-removed">3474   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; !VM_Version::supports_avx512vl());</span>
<span class="line-removed">3475   match(Set dst (ReplicateI con));</span>
<span class="line-removed">3476   format %{ &quot;replicateI $dst,$con&quot; %}</span>
<span class="line-removed">3477   ins_encode %{</span>
<span class="line-removed">3478     __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 4)));</span>
<span class="line-removed">3479     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3480     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3481     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3482   %}</span>
<span class="line-removed">3483   ins_pipe( pipe_slow );</span>
<span class="line-removed">3484 %}</span>
<span class="line-removed">3485 </span>
3486 // Replicate integer (4 byte) scalar zero to be vector
3487 instruct ReplI_zero(vec dst, immI0 zero) %{
3488   match(Set dst (ReplicateI zero));
3489   format %{ &quot;replicateI $dst,$zero&quot; %}
3490   ins_encode %{
3491     uint vlen = vector_length(this);
3492     if (vlen &lt;= 4) {
3493       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3494     } else {
3495       int vlen_enc = vector_length_encoding(this);
3496       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3497     }
3498   %}
3499   ins_pipe( fpu_reg_reg );
3500 %}
3501 
3502 // ====================ReplicateL=======================================
3503 
3504 #ifdef _LP64
3505 // Replicate long (8 byte) scalar to be vector
3506 instruct ReplL_reg(vec dst, rRegL src) %{
<span class="line-removed">3507   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 4) ||</span>
<span class="line-removed">3508             (n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; VM_Version::supports_avx512vl()));</span>
3509   match(Set dst (ReplicateL src));
3510   format %{ &quot;replicateL $dst,$src&quot; %}
3511   ins_encode %{
3512     uint vlen = vector_length(this);
3513     if (vlen == 2) {
3514       __ movdq($dst$$XMMRegister, $src$$Register);
3515       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3516     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>
3517       int vlen_enc = vector_length_encoding(this);
3518       __ evpbroadcastq($dst$$XMMRegister, $src$$Register, vlen_enc);
3519     } else {
<span class="line-modified">3520       assert(vlen == 4, &quot;sanity&quot;); // vlen == 8 &amp;&amp; !AVX512VL is covered by ReplL_reg_leg</span>
3521       __ movdq($dst$$XMMRegister, $src$$Register);
3522       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3523       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3524     }
3525   %}
3526   ins_pipe( pipe_slow );
3527 %}
<span class="line-removed">3528 </span>
<span class="line-removed">3529 instruct ReplL_reg_leg(legVec dst, rRegL src) %{</span>
<span class="line-removed">3530   predicate(n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; !VM_Version::supports_avx512vl());</span>
<span class="line-removed">3531   match(Set dst (ReplicateL src));</span>
<span class="line-removed">3532   format %{ &quot;replicateL $dst,$src&quot; %}</span>
<span class="line-removed">3533   ins_encode %{</span>
<span class="line-removed">3534     __ movdq($dst$$XMMRegister, $src$$Register);</span>
<span class="line-removed">3535     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3536     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3537     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3538   %}</span>
<span class="line-removed">3539   ins_pipe( pipe_slow );</span>
<span class="line-removed">3540 %}</span>
3541 #else // _LP64
3542 // Replicate long (8 byte) scalar to be vector
3543 instruct ReplL_reg(vec dst, eRegL src, vec tmp) %{
3544   predicate(n-&gt;as_Vector()-&gt;length() &lt;= 4);
3545   match(Set dst (ReplicateL src));
3546   effect(TEMP dst, USE src, TEMP tmp);
3547   format %{ &quot;replicateL $dst,$src&quot; %}
3548   ins_encode %{
3549     uint vlen = vector_length(this);
3550     if (vlen == 2) {
3551       __ movdl($dst$$XMMRegister, $src$$Register);
3552       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3553       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3554       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3555     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3556       int vector_len = Assembler::AVX_256bit;
3557       __ movdl($dst$$XMMRegister, $src$$Register);
3558       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3559       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3560       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
</pre>
<hr />
<pre>
3578     if (VM_Version::supports_avx512vl()) {
3579       __ movdl($dst$$XMMRegister, $src$$Register);
3580       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3581       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3582       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3583       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3584       __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3585     } else {
3586       int vector_len = Assembler::AVX_512bit;
3587       __ movdl($dst$$XMMRegister, $src$$Register);
3588       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3589       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3590       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3591     }
3592   %}
3593   ins_pipe( pipe_slow );
3594 %}
3595 #endif // _LP64
3596 
3597 instruct ReplL_mem(vec dst, memory mem) %{
<span class="line-removed">3598   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 4) ||</span>
<span class="line-removed">3599             (n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; VM_Version::supports_avx512vl()));</span>
3600   match(Set dst (ReplicateL (LoadL mem)));
3601   format %{ &quot;replicateL $dst,$mem&quot; %}
3602   ins_encode %{
3603     uint vlen = vector_length(this);
3604     if (vlen == 2) {
3605       __ movq($dst$$XMMRegister, $mem$$Address);
3606       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3607     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>

3608       int vlen_enc = vector_length_encoding(this);
3609       __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);
<span class="line-removed">3610     } else {</span>
<span class="line-removed">3611       assert(vlen == 4, &quot;sanity&quot;); // vlen == 8 &amp;&amp; !AVX512VL is covered by ReplL_mem_leg</span>
<span class="line-removed">3612       __ movq($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-removed">3613       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3614       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3615     }
3616   %}
3617   ins_pipe( pipe_slow );
3618 %}
3619 
<span class="line-removed">3620 instruct ReplL_mem_leg(legVec dst, memory mem) %{</span>
<span class="line-removed">3621   predicate(n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; !VM_Version::supports_avx512vl());</span>
<span class="line-removed">3622   match(Set dst (ReplicateL (LoadL mem)));</span>
<span class="line-removed">3623   format %{ &quot;replicateL $dst,$mem&quot; %}</span>
<span class="line-removed">3624   ins_encode %{</span>
<span class="line-removed">3625     __ movq($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-removed">3626     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3627     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3628     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3629   %}</span>
<span class="line-removed">3630   ins_pipe( pipe_slow );</span>
<span class="line-removed">3631 %}</span>
<span class="line-removed">3632 </span>
3633 // Replicate long (8 byte) scalar immediate to be vector by loading from const table.
3634 instruct ReplL_imm(vec dst, immL con) %{
<span class="line-removed">3635   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 4) ||</span>
<span class="line-removed">3636             (n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; VM_Version::supports_avx512vl()));</span>
3637   match(Set dst (ReplicateL con));
3638   format %{ &quot;replicateL $dst,$con&quot; %}
3639   ins_encode %{
3640     uint vlen = vector_length(this);
3641     InternalAddress const_addr = $constantaddress($con);
3642     if (vlen == 2) {
3643       __ movq($dst$$XMMRegister, const_addr);
3644       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3645     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>

3646       int vlen_enc = vector_length_encoding(this);
3647       __ movq($dst$$XMMRegister, const_addr);
3648       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
<span class="line-removed">3649     } else {</span>
<span class="line-removed">3650       assert(vlen == 4, &quot;sanity&quot;); // vlen == 8 &amp;&amp; !AVX512VL is covered by ReplL_imm_leg</span>
<span class="line-removed">3651       __ movq($dst$$XMMRegister, const_addr);</span>
<span class="line-removed">3652       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3653       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3654     }
3655   %}
3656   ins_pipe( pipe_slow );
3657 %}
3658 
<span class="line-removed">3659 instruct ReplL_imm_leg(legVec dst, immL con) %{</span>
<span class="line-removed">3660   predicate(n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; !VM_Version::supports_avx512vl());</span>
<span class="line-removed">3661   match(Set dst (ReplicateL con));</span>
<span class="line-removed">3662   format %{ &quot;replicateL $dst,$con&quot; %}</span>
<span class="line-removed">3663   ins_encode %{</span>
<span class="line-removed">3664     __ movq($dst$$XMMRegister, $constantaddress($con));</span>
<span class="line-removed">3665     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3666     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3667     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3668   %}</span>
<span class="line-removed">3669   ins_pipe( pipe_slow );</span>
<span class="line-removed">3670 %}</span>
<span class="line-removed">3671 </span>
3672 instruct ReplL_zero(vec dst, immL0 zero) %{
3673   match(Set dst (ReplicateL zero));
3674   format %{ &quot;replicateL $dst,$zero&quot; %}
3675   ins_encode %{
3676     int vlen = vector_length(this);
3677     if (vlen == 2) {
3678       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3679     } else {
3680       int vlen_enc = vector_length_encoding(this);
3681       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3682     }
3683   %}
3684   ins_pipe( fpu_reg_reg );
3685 %}
3686 
3687 // ====================ReplicateF=======================================
3688 
3689 instruct ReplF_reg(vec dst, vlRegF src) %{
<span class="line-removed">3690   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 8) ||</span>
<span class="line-removed">3691             (n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; VM_Version::supports_avx512vl()));</span>
3692   match(Set dst (ReplicateF src));
3693   format %{ &quot;replicateF $dst,$src&quot; %}
3694   ins_encode %{
3695     uint vlen = vector_length(this);
3696     if (vlen &lt;= 4) {
3697       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
<span class="line-modified">3698     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>
3699       int vector_len = vector_length_encoding(this);
<span class="line-modified">3700       __ vpbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len);</span>
3701     } else {
<span class="line-modified">3702       assert(vlen == 8, &quot;sanity&quot;); // vlen == 16 &amp;&amp; !AVX512VL is covered by ReplF_reg_leg</span>
3703       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3704       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3705     }
3706   %}
3707   ins_pipe( pipe_slow );
3708 %}
3709 
<span class="line-removed">3710 instruct ReplF_reg_leg(legVec dst, vlRegF src) %{</span>
<span class="line-removed">3711   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; !VM_Version::supports_avx512vl());</span>
<span class="line-removed">3712   match(Set dst (ReplicateF src));</span>
<span class="line-removed">3713   format %{ &quot;replicateF $dst,$src&quot; %}</span>
<span class="line-removed">3714   ins_encode %{</span>
<span class="line-removed">3715     __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);</span>
<span class="line-removed">3716     __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3717     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3718   %}</span>
<span class="line-removed">3719   ins_pipe( pipe_slow );</span>
<span class="line-removed">3720 %}</span>
<span class="line-removed">3721 </span>
3722 instruct ReplF_mem(vec dst, memory mem) %{
<span class="line-removed">3723   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 8  &amp;&amp; VM_Version::supports_avx()) ||</span>
<span class="line-removed">3724             (n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; VM_Version::supports_avx512vl()));</span>
3725   match(Set dst (ReplicateF (LoadF mem)));
3726   format %{ &quot;replicateF $dst,$mem&quot; %}
3727   ins_encode %{
3728     uint vlen = vector_length(this);
3729     if (vlen &lt;= 4) {
<span class="line-modified">3730       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-modified">3731     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>
<span class="line-removed">3732       int vector_len = vector_length_encoding(this);</span>
<span class="line-removed">3733       __ vpbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);</span>
3734     } else {
<span class="line-modified">3735       assert(vlen == 8, &quot;sanity&quot;); // vlen == 16 &amp;&amp; !AVX512VL is covered by ReplF_mem_leg</span>
<span class="line-modified">3736       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-modified">3737       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3738     }
3739   %}
3740   ins_pipe( pipe_slow );
3741 %}
3742 
<span class="line-removed">3743 instruct ReplF_mem_leg(legVec dst, memory mem) %{</span>
<span class="line-removed">3744   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; !VM_Version::supports_avx512vl());</span>
<span class="line-removed">3745   match(Set dst (ReplicateF (LoadF mem)));</span>
<span class="line-removed">3746   format %{ &quot;replicateF $dst,$mem&quot; %}</span>
<span class="line-removed">3747   ins_encode %{</span>
<span class="line-removed">3748     __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);</span>
<span class="line-removed">3749     __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3750     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3751   %}</span>
<span class="line-removed">3752   ins_pipe( pipe_slow );</span>
<span class="line-removed">3753 %}</span>
<span class="line-removed">3754 </span>
3755 instruct ReplF_zero(vec dst, immF0 zero) %{
3756   match(Set dst (ReplicateF zero));
3757   format %{ &quot;replicateF $dst,$zero&quot; %}
3758   ins_encode %{
3759     uint vlen = vector_length(this);
3760     if (vlen &lt;= 4) {
3761       __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
3762     } else {
3763       int vlen_enc = vector_length_encoding(this);
3764       __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3765     }
3766   %}
3767   ins_pipe( fpu_reg_reg );
3768 %}
3769 
3770 // ====================ReplicateD=======================================
3771 
3772 // Replicate double (8 bytes) scalar to be vector
3773 instruct ReplD_reg(vec dst, vlRegD src) %{
<span class="line-removed">3774   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 4) ||</span>
<span class="line-removed">3775             (n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; VM_Version::supports_avx512vl()));</span>
3776   match(Set dst (ReplicateD src));
3777   format %{ &quot;replicateD $dst,$src&quot; %}
3778   ins_encode %{
3779     uint vlen = vector_length(this);
3780     if (vlen == 2) {
3781       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
<span class="line-modified">3782     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>
3783       int vector_len = vector_length_encoding(this);
<span class="line-modified">3784       __ vpbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len);</span>
3785     } else {
<span class="line-modified">3786       assert(vlen == 4, &quot;sanity&quot;); // vlen == 8 &amp;&amp; !AVX512VL is covered by ReplD_reg_leg</span>
3787       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3788       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3789     }
3790   %}
3791   ins_pipe( pipe_slow );
3792 %}
3793 
<span class="line-removed">3794 instruct ReplD_reg_leg(legVec dst, vlRegD src) %{</span>
<span class="line-removed">3795   predicate(n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; !VM_Version::supports_avx512vl());</span>
<span class="line-removed">3796   match(Set dst (ReplicateD src));</span>
<span class="line-removed">3797   format %{ &quot;replicateD $dst,$src&quot; %}</span>
<span class="line-removed">3798   ins_encode %{</span>
<span class="line-removed">3799     __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);</span>
<span class="line-removed">3800     __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3801     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3802   %}</span>
<span class="line-removed">3803   ins_pipe( pipe_slow );</span>
<span class="line-removed">3804 %}</span>
<span class="line-removed">3805 </span>
3806 instruct ReplD_mem(vec dst, memory mem) %{
<span class="line-removed">3807   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 4 &amp;&amp; VM_Version::supports_avx()) ||</span>
<span class="line-removed">3808             (n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; VM_Version::supports_avx512vl()));</span>
3809   match(Set dst (ReplicateD (LoadD mem)));
3810   format %{ &quot;replicateD $dst,$mem&quot; %}
3811   ins_encode %{
3812     uint vlen = vector_length(this);
3813     if (vlen == 2) {
<span class="line-modified">3814       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);</span>
<span class="line-modified">3815     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>
<span class="line-removed">3816       int vector_len = vector_length_encoding(this);</span>
<span class="line-removed">3817       __ vpbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);</span>
3818     } else {
<span class="line-modified">3819       assert(vlen == 4, &quot;sanity&quot;); // vlen == 8 &amp;&amp; !AVX512VL is covered by ReplD_mem_leg</span>
<span class="line-modified">3820       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);</span>
<span class="line-modified">3821       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
3822     }
3823   %}
3824   ins_pipe( pipe_slow );
3825 %}
3826 
<span class="line-removed">3827 instruct ReplD_mem_leg(legVec dst, memory mem) %{</span>
<span class="line-removed">3828   predicate(n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; !VM_Version::supports_avx512vl());</span>
<span class="line-removed">3829   match(Set dst (ReplicateD (LoadD mem)));</span>
<span class="line-removed">3830   format %{ &quot;replicateD $dst,$mem&quot; %}</span>
<span class="line-removed">3831   ins_encode %{</span>
<span class="line-removed">3832     __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);</span>
<span class="line-removed">3833     __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-removed">3834     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);</span>
<span class="line-removed">3835   %}</span>
<span class="line-removed">3836   ins_pipe( pipe_slow );</span>
<span class="line-removed">3837 %}</span>
<span class="line-removed">3838 </span>
3839 instruct ReplD_zero(vec dst, immD0 zero) %{
3840   match(Set dst (ReplicateD zero));
3841   format %{ &quot;replicateD $dst,$zero&quot; %}
3842   ins_encode %{
3843     uint vlen = vector_length(this);
3844     if (vlen == 2) {
3845       __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);
3846     } else {
3847       int vlen_enc = vector_length_encoding(this);
3848       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3849     }
3850   %}
3851   ins_pipe( fpu_reg_reg );
3852 %}
3853 
3854 // ====================REDUCTION ARITHMETIC=======================================
3855 
3856 // =======================AddReductionVI==========================================
3857 
3858 instruct vadd2I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
</pre>
<hr />
<pre>
5379     int vector_len = vector_length_encoding(this);
5380     __ vsqrtpd($dst$$XMMRegister, $mem$$Address, vector_len);
5381   %}
5382   ins_pipe( pipe_slow );
5383 %}
5384 
5385 // ------------------------------ Shift ---------------------------------------
5386 
5387 // Left and right shift count vectors are the same on x86
5388 // (only lowest bits of xmm reg are used for count).
5389 instruct vshiftcnt(vec dst, rRegI cnt) %{
5390   match(Set dst (LShiftCntV cnt));
5391   match(Set dst (RShiftCntV cnt));
5392   format %{ &quot;movdl    $dst,$cnt\t! load shift count&quot; %}
5393   ins_encode %{
5394     __ movdl($dst$$XMMRegister, $cnt$$Register);
5395   %}
5396   ins_pipe( pipe_slow );
5397 %}
5398 
<span class="line-removed">5399 instruct vshiftcntimm(vec dst, immI8 cnt, rRegI tmp) %{</span>
<span class="line-removed">5400   match(Set dst cnt);</span>
<span class="line-removed">5401   effect(TEMP tmp);</span>
<span class="line-removed">5402   format %{ &quot;movl    $tmp,$cnt\t&quot;</span>
<span class="line-removed">5403             &quot;movdl   $dst,$tmp\t! load shift count&quot; %}</span>
<span class="line-removed">5404   ins_encode %{</span>
<span class="line-removed">5405     __ movl($tmp$$Register, $cnt$$constant);</span>
<span class="line-removed">5406     __ movdl($dst$$XMMRegister, $tmp$$Register);</span>
<span class="line-removed">5407   %}</span>
<span class="line-removed">5408   ins_pipe( pipe_slow );</span>
<span class="line-removed">5409 %}</span>
<span class="line-removed">5410 </span>
5411 // Byte vector shift
5412 instruct vshiftB(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
5413   predicate(n-&gt;as_Vector()-&gt;length() &lt;= 8);
5414   match(Set dst (LShiftVB src shift));
5415   match(Set dst (RShiftVB src shift));
5416   match(Set dst (URShiftVB src shift));
5417   effect(TEMP dst, USE src, USE shift, TEMP tmp, TEMP scratch);
5418   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
5419   ins_encode %{
5420     assert(UseSSE &gt; 3, &quot;required&quot;);
5421     int opcode = this-&gt;ideal_Opcode();
5422     __ vextendbw(opcode, $tmp$$XMMRegister, $src$$XMMRegister);
5423     __ vshiftw(opcode, $tmp$$XMMRegister, $shift$$XMMRegister);
5424     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
5425     __ pand($dst$$XMMRegister, $tmp$$XMMRegister);
5426     __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);
5427   %}
5428   ins_pipe( pipe_slow );
5429 %}
5430 
</pre>
<hr />
<pre>
5921   ins_encode %{
5922     __ pmaddwd($dst$$XMMRegister, $src1$$XMMRegister);
5923   %}
5924   ins_pipe( pipe_slow );
5925 %}
5926 
5927 instruct vmuladdS2I_reg_avx(vec dst, vec src1, vec src2) %{
5928   predicate(UseAVX &gt; 0);
5929   match(Set dst (MulAddVS2VI src1 src2));
5930   format %{ &quot;vpmaddwd $dst,$src1,$src2\t! muladd packedStoI&quot; %}
5931   ins_encode %{
5932     int vector_len = vector_length_encoding(this);
5933     __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5934   %}
5935   ins_pipe( pipe_slow );
5936 %}
5937 
5938 // --------------------------------- Vector Multiply Add Add ----------------------------------
5939 
5940 instruct vmuladdaddS2I_reg(vec dst, vec src1, vec src2) %{
<span class="line-modified">5941   predicate(VM_Version::supports_vnni());</span>
5942   match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
5943   format %{ &quot;evpdpwssd $dst,$src1,$src2\t! muladdadd packedStoI&quot; %}
5944   ins_encode %{
5945     assert(UseAVX &gt; 2, &quot;required&quot;);
5946     int vector_len = vector_length_encoding(this);
5947     __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5948   %}
5949   ins_pipe( pipe_slow );
5950   ins_cost(10);
5951 %}
5952 
5953 // --------------------------------- PopCount --------------------------------------
5954 
5955 instruct vpopcountI(vec dst, vec src) %{
5956   match(Set dst (PopCountVI src));
5957   format %{ &quot;vpopcntd  $dst,$src\t! vector popcount packedI&quot; %}
5958   ins_encode %{
5959     assert(UsePopCountInstruction, &quot;not enabled&quot;);
5960 
5961     int vector_len = vector_length_encoding(this);
</pre>
</td>
<td>
<hr />
<pre>
1245   static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }
1246 
1247 //=============================================================================
1248 const bool Matcher::match_rule_supported(int opcode) {
1249   if (!has_match_rule(opcode)) {
1250     return false; // no match rule present
1251   }
1252   switch (opcode) {
1253     case Op_AbsVL:
1254       if (UseAVX &lt; 3) {
1255         return false;
1256       }
1257       break;
1258     case Op_PopCountI:
1259     case Op_PopCountL:
1260       if (!UsePopCountInstruction) {
1261         return false;
1262       }
1263       break;
1264     case Op_PopCountVI:
<span class="line-modified">1265       if (!UsePopCountInstruction || !VM_Version::supports_avx512_vpopcntdq()) {</span>
1266         return false;
1267       }
1268       break;
1269     case Op_MulVI:
1270       if ((UseSSE &lt; 4) &amp;&amp; (UseAVX &lt; 1)) { // only with SSE4_1 or AVX
1271         return false;
1272       }
1273       break;
1274     case Op_MulVL:
1275     case Op_MulReductionVL:
1276       if (VM_Version::supports_avx512dq() == false) {
1277         return false;
1278       }
1279       break;
1280     case Op_AddReductionVL:
1281       if (UseAVX &lt; 3) { // only EVEX : vector connectivity becomes an issue here
1282         return false;
1283       }
1284       break;
1285     case Op_AbsVB:
1286     case Op_AbsVS:
1287     case Op_AbsVI:
1288     case Op_AddReductionVI:
1289       if (UseSSE &lt; 3 || !VM_Version::supports_ssse3()) { // requires at least SSSE3
1290         return false;
1291       }
1292       break;
1293     case Op_MulReductionVI:
1294       if (UseSSE &lt; 4) { // requires at least SSE4
1295         return false;
1296       }
1297       break;








1298     case Op_SqrtVD:
1299     case Op_SqrtVF:
1300       if (UseAVX &lt; 1) { // enabled for AVX only
1301         return false;
1302       }
1303       break;
1304     case Op_CompareAndSwapL:
1305 #ifdef _LP64
1306     case Op_CompareAndSwapP:
1307 #endif
1308       if (!VM_Version::supports_cx8()) {
1309         return false;
1310       }
1311       break;
1312     case Op_CMoveVF:
1313     case Op_CMoveVD:
1314       if (UseAVX &lt; 1 || UseAVX &gt; 2) {
1315         return false;
1316       }
1317       break;
1318     case Op_StrIndexOf:
1319       if (!UseSSE42Intrinsics) {
1320         return false;
1321       }
1322       break;
1323     case Op_StrIndexOfChar:
1324       if (!UseSSE42Intrinsics) {
1325         return false;
1326       }
1327       break;
1328     case Op_OnSpinWait:
1329       if (VM_Version::supports_on_spin_wait() == false) {
1330         return false;
1331       }
1332       break;








1333     case Op_MulVB:
1334     case Op_LShiftVB:
1335     case Op_RShiftVB:
1336     case Op_URShiftVB:
1337       if (UseSSE &lt; 4) {
1338         return false;
1339       }
1340       break;
1341 #ifdef _LP64
1342     case Op_MaxD:
1343     case Op_MaxF:
1344     case Op_MinD:
1345     case Op_MinF:
1346       if (UseAVX &lt; 1) { // enabled for AVX only
1347         return false;
1348       }
1349       break;
1350 #endif
1351     case Op_CacheWB:
1352     case Op_CacheWBPreSync:
1353     case Op_CacheWBPostSync:
1354       if (!VM_Version::supports_data_cache_line_flush()) {
1355         return false;
1356       }
1357       break;
1358     case Op_RoundDoubleMode:
1359       if (UseSSE &lt; 4) {
1360         return false;
1361       }
1362       break;
1363     case Op_RoundDoubleModeV:
1364       if (VM_Version::supports_avx() == false) {
1365         return false; // 128bit vroundpd is not available
1366       }
1367       break;
<span class="line-added">1368 #ifndef _LP64</span>
<span class="line-added">1369     case Op_AddReductionVF:</span>
<span class="line-added">1370     case Op_AddReductionVD:</span>
<span class="line-added">1371     case Op_MulReductionVF:</span>
<span class="line-added">1372     case Op_MulReductionVD:</span>
<span class="line-added">1373       if (UseSSE &lt; 1) { // requires at least SSE</span>
<span class="line-added">1374         return false;</span>
<span class="line-added">1375       }</span>
<span class="line-added">1376       break;</span>
<span class="line-added">1377     case Op_MulAddVS2VI:</span>
<span class="line-added">1378     case Op_RShiftVL:</span>
<span class="line-added">1379     case Op_AbsVD:</span>
<span class="line-added">1380     case Op_NegVD:</span>
<span class="line-added">1381       if (UseSSE &lt; 2) {</span>
<span class="line-added">1382         return false;</span>
<span class="line-added">1383       }</span>
<span class="line-added">1384       break;</span>
<span class="line-added">1385 #endif // !LP64</span>
1386   }
1387   return true;  // Match rules are supported by default.
1388 }
1389 
1390 //------------------------------------------------------------------------
1391 
1392 // Identify extra cases that we might want to provide match rules for vector nodes and
1393 // other intrinsics guarded with vector length (vlen) and element type (bt).
1394 const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {
1395   if (!match_rule_supported(opcode)) {
1396     return false;
1397   }
1398   // Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):
1399   //   * SSE2 supports 128bit vectors for all types;
1400   //   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;
1401   //   * AVX2 supports 256bit vectors for all types;
1402   //   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;
1403   //   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.
1404   // There&#39;s also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).
1405   // And MaxVectorSize is taken into account as well.
</pre>
<hr />
<pre>
3107   match(Set mem (StoreVector mem src));
3108   ins_cost(145);
3109   format %{ &quot;store_vector $mem,$src\n\t&quot; %}
3110   ins_encode %{
3111     switch (vector_length_in_bytes(this, $src)) {
3112       case  4: __ movdl    ($mem$$Address, $src$$XMMRegister); break;
3113       case  8: __ movq     ($mem$$Address, $src$$XMMRegister); break;
3114       case 16: __ movdqu   ($mem$$Address, $src$$XMMRegister); break;
3115       case 32: __ vmovdqu  ($mem$$Address, $src$$XMMRegister); break;
3116       case 64: __ evmovdqul($mem$$Address, $src$$XMMRegister, Assembler::AVX_512bit); break;
3117       default: ShouldNotReachHere();
3118     }
3119   %}
3120   ins_pipe( pipe_slow );
3121 %}
3122 
3123 // ====================REPLICATE=======================================
3124 
3125 // Replicate byte scalar to be vector
3126 instruct ReplB_reg(vec dst, rRegI src) %{


3127   match(Set dst (ReplicateB src));
3128   format %{ &quot;replicateB $dst,$src&quot; %}
3129   ins_encode %{
3130     uint vlen = vector_length(this);
3131     if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<span class="line-modified">3132       assert(VM_Version::supports_avx512bw(), &quot;required&quot;); // 512-bit byte vectors assume AVX512BW</span>
3133       int vlen_enc = vector_length_encoding(this);
3134       __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);
3135     } else {
3136       __ movdl($dst$$XMMRegister, $src$$Register);
3137       __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
3138       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3139       if (vlen &gt;= 16) {
3140         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3141         if (vlen &gt;= 32) {
<span class="line-modified">3142           assert(vlen == 32, &quot;sanity&quot;);</span>
3143           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3144         }
3145       }
3146     }
3147   %}
3148   ins_pipe( pipe_slow );
3149 %}
3150 
















3151 instruct ReplB_mem(vec dst, memory mem) %{
<span class="line-modified">3152   predicate(VM_Version::supports_avx2());</span>

3153   match(Set dst (ReplicateB (LoadB mem)));
3154   format %{ &quot;replicateB $dst,$mem&quot; %}
3155   ins_encode %{

3156     int vector_len = vector_length_encoding(this);
3157     __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
3158   %}
3159   ins_pipe( pipe_slow );
3160 %}
3161 
3162 instruct ReplB_imm(vec dst, immI con) %{


3163   match(Set dst (ReplicateB con));
3164   format %{ &quot;replicateB $dst,$con&quot; %}
3165   ins_encode %{
3166     uint vlen = vector_length(this);
3167     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 1));
3168     if (vlen == 4) {
3169       __ movdl($dst$$XMMRegister, const_addr);
3170     } else {
3171       __ movq($dst$$XMMRegister, const_addr);
3172       if (vlen &gt;= 16) {
<span class="line-modified">3173         if (VM_Version::supports_avx2()) {</span>
3174           int vlen_enc = vector_length_encoding(this);
<span class="line-modified">3175           __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
3176         } else {
<span class="line-added">3177           assert(vlen == 16, &quot;sanity&quot;);</span>
3178           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);




3179         }
3180       }
3181     }
3182   %}
3183   ins_pipe( pipe_slow );
3184 %}
3185 













3186 // Replicate byte scalar zero to be vector
3187 instruct ReplB_zero(vec dst, immI0 zero) %{
3188   match(Set dst (ReplicateB zero));
3189   format %{ &quot;replicateB $dst,$zero&quot; %}
3190   ins_encode %{
3191     uint vlen = vector_length(this);
3192     if (vlen &lt;= 16) {
3193       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3194     } else {
3195       // Use vpxor since AVX512F does not have 512bit vxorpd (requires AVX512DQ).
3196       int vlen_enc = vector_length_encoding(this);
3197       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3198     }
3199   %}
3200   ins_pipe( fpu_reg_reg );
3201 %}
3202 
3203 // ====================ReplicateS=======================================
3204 
3205 instruct ReplS_reg(vec dst, rRegI src) %{


3206   match(Set dst (ReplicateS src));
3207   format %{ &quot;replicateS $dst,$src&quot; %}
3208   ins_encode %{
3209     uint vlen = vector_length(this);
3210     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
<span class="line-modified">3211       assert(VM_Version::supports_avx512bw(), &quot;required&quot;); // 512-bit short vectors assume AVX512BW</span>
3212       int vlen_enc = vector_length_encoding(this);
3213       __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);
3214     } else {
3215       __ movdl($dst$$XMMRegister, $src$$Register);
3216       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3217       if (vlen &gt;= 8) {
3218         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3219         if (vlen &gt;= 16) {
<span class="line-modified">3220           assert(vlen == 16, &quot;sanity&quot;);</span>
3221           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3222         }
3223       }
3224     }
3225   %}
3226   ins_pipe( pipe_slow );
3227 %}
3228 














3229 instruct ReplS_mem(vec dst, memory mem) %{
<span class="line-modified">3230   predicate(VM_Version::supports_avx2());</span>


3231   match(Set dst (ReplicateS (LoadS mem)));
3232   format %{ &quot;replicateS $dst,$mem&quot; %}
3233   ins_encode %{
<span class="line-modified">3234     int vlen_enc = vector_length_encoding(this);</span>
<span class="line-modified">3235     __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);</span>


























3236   %}
3237   ins_pipe( pipe_slow );
3238 %}
3239 
3240 instruct ReplS_imm(vec dst, immI con) %{


3241   match(Set dst (ReplicateS con));
3242   format %{ &quot;replicateS $dst,$con&quot; %}
3243   ins_encode %{
3244     uint vlen = vector_length(this);
<span class="line-modified">3245     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 2));</span>
3246     if (vlen == 2) {
<span class="line-modified">3247       __ movdl($dst$$XMMRegister, const_addr);</span>
3248     } else {
<span class="line-modified">3249       __ movq($dst$$XMMRegister, const_addr);</span>
<span class="line-modified">3250       if (vlen &gt;= 8) {</span>
<span class="line-modified">3251         if (VM_Version::supports_avx2()) {</span>
<span class="line-modified">3252           int vlen_enc = vector_length_encoding(this);</span>
<span class="line-modified">3253           __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);</span>
<span class="line-modified">3254         } else {</span>
<span class="line-modified">3255           assert(vlen == 8, &quot;sanity&quot;);</span>
<span class="line-modified">3256           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>



3257         }
3258       }
3259     }
3260   %}
3261   ins_pipe( fpu_reg_reg );
3262 %}
3263 













3264 instruct ReplS_zero(vec dst, immI0 zero) %{
3265   match(Set dst (ReplicateS zero));
3266   format %{ &quot;replicateS $dst,$zero&quot; %}
3267   ins_encode %{
3268     uint vlen = vector_length(this);
3269     if (vlen &lt;= 8) {
3270       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3271     } else {
3272       int vlen_enc = vector_length_encoding(this);
3273       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3274     }
3275   %}
3276   ins_pipe( fpu_reg_reg );
3277 %}
3278 
3279 // ====================ReplicateI=======================================
3280 
3281 instruct ReplI_reg(vec dst, rRegI src) %{


3282   match(Set dst (ReplicateI src));
3283   format %{ &quot;replicateI $dst,$src&quot; %}
3284   ins_encode %{
3285     uint vlen = vector_length(this);
<span class="line-modified">3286     if (vlen == 16 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>
3287       int vlen_enc = vector_length_encoding(this);
3288       __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vlen_enc);
3289     } else {
3290       __ movdl($dst$$XMMRegister, $src$$Register);
3291       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3292       if (vlen &gt;= 8) {
<span class="line-modified">3293         assert(vlen == 8, &quot;sanity&quot;);</span>
3294         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3295       }
3296     }
3297   %}
3298   ins_pipe( pipe_slow );
3299 %}
3300 













3301 instruct ReplI_mem(vec dst, memory mem) %{


3302   match(Set dst (ReplicateI (LoadI mem)));
3303   format %{ &quot;replicateI $dst,$mem&quot; %}
3304   ins_encode %{
3305     uint vlen = vector_length(this);
3306     if (vlen &lt;= 4) {
<span class="line-modified">3307       __ movdl($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-modified">3308       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>
<span class="line-added">3309     } else {</span>
<span class="line-added">3310       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);</span>
3311       int vector_len = vector_length_encoding(this);
3312       __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);




3313     }
3314   %}
3315   ins_pipe( pipe_slow );
3316 %}
3317 












3318 instruct ReplI_imm(vec dst, immI con) %{


3319   match(Set dst (ReplicateI con));
3320   format %{ &quot;replicateI $dst,$con&quot; %}
3321   ins_encode %{
3322     uint vlen = vector_length(this);
<span class="line-modified">3323     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 4));</span>
<span class="line-modified">3324     if (vlen &lt;= 4) {</span>
<span class="line-modified">3325       __ movq($dst$$XMMRegister, const_addr);</span>
<span class="line-modified">3326       if (vlen == 4) {</span>
<span class="line-added">3327         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);</span>
<span class="line-added">3328       }</span>
<span class="line-added">3329     } else {</span>
<span class="line-added">3330       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);</span>
3331       int vector_len = vector_length_encoding(this);
<span class="line-modified">3332       __ movq($dst$$XMMRegister, const_addr);</span>
3333       __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);







3334     }
3335   %}
3336   ins_pipe( pipe_slow );
3337 %}
3338 













3339 // Replicate integer (4 byte) scalar zero to be vector
3340 instruct ReplI_zero(vec dst, immI0 zero) %{
3341   match(Set dst (ReplicateI zero));
3342   format %{ &quot;replicateI $dst,$zero&quot; %}
3343   ins_encode %{
3344     uint vlen = vector_length(this);
3345     if (vlen &lt;= 4) {
3346       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3347     } else {
3348       int vlen_enc = vector_length_encoding(this);
3349       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3350     }
3351   %}
3352   ins_pipe( fpu_reg_reg );
3353 %}
3354 
3355 // ====================ReplicateL=======================================
3356 
3357 #ifdef _LP64
3358 // Replicate long (8 byte) scalar to be vector
3359 instruct ReplL_reg(vec dst, rRegL src) %{


3360   match(Set dst (ReplicateL src));
3361   format %{ &quot;replicateL $dst,$src&quot; %}
3362   ins_encode %{
3363     uint vlen = vector_length(this);
3364     if (vlen == 2) {
3365       __ movdq($dst$$XMMRegister, $src$$Register);
3366       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3367     } else if (vlen == 8 || VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands</span>
3368       int vlen_enc = vector_length_encoding(this);
3369       __ evpbroadcastq($dst$$XMMRegister, $src$$Register, vlen_enc);
3370     } else {
<span class="line-modified">3371       assert(vlen == 4, &quot;sanity&quot;);</span>
3372       __ movdq($dst$$XMMRegister, $src$$Register);
3373       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3374       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3375     }
3376   %}
3377   ins_pipe( pipe_slow );
3378 %}













3379 #else // _LP64
3380 // Replicate long (8 byte) scalar to be vector
3381 instruct ReplL_reg(vec dst, eRegL src, vec tmp) %{
3382   predicate(n-&gt;as_Vector()-&gt;length() &lt;= 4);
3383   match(Set dst (ReplicateL src));
3384   effect(TEMP dst, USE src, TEMP tmp);
3385   format %{ &quot;replicateL $dst,$src&quot; %}
3386   ins_encode %{
3387     uint vlen = vector_length(this);
3388     if (vlen == 2) {
3389       __ movdl($dst$$XMMRegister, $src$$Register);
3390       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3391       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3392       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3393     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3394       int vector_len = Assembler::AVX_256bit;
3395       __ movdl($dst$$XMMRegister, $src$$Register);
3396       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3397       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3398       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
</pre>
<hr />
<pre>
3416     if (VM_Version::supports_avx512vl()) {
3417       __ movdl($dst$$XMMRegister, $src$$Register);
3418       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3419       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3420       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3421       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3422       __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3423     } else {
3424       int vector_len = Assembler::AVX_512bit;
3425       __ movdl($dst$$XMMRegister, $src$$Register);
3426       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3427       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3428       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3429     }
3430   %}
3431   ins_pipe( pipe_slow );
3432 %}
3433 #endif // _LP64
3434 
3435 instruct ReplL_mem(vec dst, memory mem) %{


3436   match(Set dst (ReplicateL (LoadL mem)));
3437   format %{ &quot;replicateL $dst,$mem&quot; %}
3438   ins_encode %{
3439     uint vlen = vector_length(this);
3440     if (vlen == 2) {
3441       __ movq($dst$$XMMRegister, $mem$$Address);
3442       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3443     } else {</span>
<span class="line-added">3444       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);</span>
3445       int vlen_enc = vector_length_encoding(this);
3446       __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);





3447     }
3448   %}
3449   ins_pipe( pipe_slow );
3450 %}
3451 













3452 // Replicate long (8 byte) scalar immediate to be vector by loading from const table.
3453 instruct ReplL_imm(vec dst, immL con) %{


3454   match(Set dst (ReplicateL con));
3455   format %{ &quot;replicateL $dst,$con&quot; %}
3456   ins_encode %{
3457     uint vlen = vector_length(this);
3458     InternalAddress const_addr = $constantaddress($con);
3459     if (vlen == 2) {
3460       __ movq($dst$$XMMRegister, const_addr);
3461       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
<span class="line-modified">3462     } else {</span>
<span class="line-added">3463       assert(VM_Version::supports_avx2(), &quot;sanity&quot;);</span>
3464       int vlen_enc = vector_length_encoding(this);
3465       __ movq($dst$$XMMRegister, const_addr);
3466       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);





3467     }
3468   %}
3469   ins_pipe( pipe_slow );
3470 %}
3471 













3472 instruct ReplL_zero(vec dst, immL0 zero) %{
3473   match(Set dst (ReplicateL zero));
3474   format %{ &quot;replicateL $dst,$zero&quot; %}
3475   ins_encode %{
3476     int vlen = vector_length(this);
3477     if (vlen == 2) {
3478       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3479     } else {
3480       int vlen_enc = vector_length_encoding(this);
3481       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3482     }
3483   %}
3484   ins_pipe( fpu_reg_reg );
3485 %}
3486 
3487 // ====================ReplicateF=======================================
3488 
3489 instruct ReplF_reg(vec dst, vlRegF src) %{


3490   match(Set dst (ReplicateF src));
3491   format %{ &quot;replicateF $dst,$src&quot; %}
3492   ins_encode %{
3493     uint vlen = vector_length(this);
3494     if (vlen &lt;= 4) {
3495       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
<span class="line-modified">3496    } else if (VM_Version::supports_avx2()) {</span>
3497       int vector_len = vector_length_encoding(this);
<span class="line-modified">3498       __ vbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2</span>
3499     } else {
<span class="line-modified">3500       assert(vlen == 8, &quot;sanity&quot;);</span>
3501       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3502       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3503     }
3504   %}
3505   ins_pipe( pipe_slow );
3506 %}
3507 












3508 instruct ReplF_mem(vec dst, memory mem) %{


3509   match(Set dst (ReplicateF (LoadF mem)));
3510   format %{ &quot;replicateF $dst,$mem&quot; %}
3511   ins_encode %{
3512     uint vlen = vector_length(this);
3513     if (vlen &lt;= 4) {
<span class="line-modified">3514       __ movdl($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-modified">3515       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);</span>


3516     } else {
<span class="line-modified">3517       assert(VM_Version::supports_avx(), &quot;sanity&quot;);</span>
<span class="line-modified">3518       int vector_len = vector_length_encoding(this);</span>
<span class="line-modified">3519       __ vbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);</span>
3520     }
3521   %}
3522   ins_pipe( pipe_slow );
3523 %}
3524 












3525 instruct ReplF_zero(vec dst, immF0 zero) %{
3526   match(Set dst (ReplicateF zero));
3527   format %{ &quot;replicateF $dst,$zero&quot; %}
3528   ins_encode %{
3529     uint vlen = vector_length(this);
3530     if (vlen &lt;= 4) {
3531       __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
3532     } else {
3533       int vlen_enc = vector_length_encoding(this);
3534       __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3535     }
3536   %}
3537   ins_pipe( fpu_reg_reg );
3538 %}
3539 
3540 // ====================ReplicateD=======================================
3541 
3542 // Replicate double (8 bytes) scalar to be vector
3543 instruct ReplD_reg(vec dst, vlRegD src) %{


3544   match(Set dst (ReplicateD src));
3545   format %{ &quot;replicateD $dst,$src&quot; %}
3546   ins_encode %{
3547     uint vlen = vector_length(this);
3548     if (vlen == 2) {
3549       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
<span class="line-modified">3550     } else if (VM_Version::supports_avx2()) {</span>
3551       int vector_len = vector_length_encoding(this);
<span class="line-modified">3552       __ vbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len); // reg-to-reg variant requires AVX2</span>
3553     } else {
<span class="line-modified">3554       assert(vlen == 4, &quot;sanity&quot;);</span>
3555       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3556       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3557     }
3558   %}
3559   ins_pipe( pipe_slow );
3560 %}
3561 












3562 instruct ReplD_mem(vec dst, memory mem) %{


3563   match(Set dst (ReplicateD (LoadD mem)));
3564   format %{ &quot;replicateD $dst,$mem&quot; %}
3565   ins_encode %{
3566     uint vlen = vector_length(this);
3567     if (vlen == 2) {
<span class="line-modified">3568       __ movq($dst$$XMMRegister, $mem$$Address);</span>
<span class="line-modified">3569       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x44);</span>


3570     } else {
<span class="line-modified">3571       assert(VM_Version::supports_avx(), &quot;sanity&quot;);</span>
<span class="line-modified">3572       int vector_len = vector_length_encoding(this);</span>
<span class="line-modified">3573       __ vbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);</span>
3574     }
3575   %}
3576   ins_pipe( pipe_slow );
3577 %}
3578 












3579 instruct ReplD_zero(vec dst, immD0 zero) %{
3580   match(Set dst (ReplicateD zero));
3581   format %{ &quot;replicateD $dst,$zero&quot; %}
3582   ins_encode %{
3583     uint vlen = vector_length(this);
3584     if (vlen == 2) {
3585       __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);
3586     } else {
3587       int vlen_enc = vector_length_encoding(this);
3588       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3589     }
3590   %}
3591   ins_pipe( fpu_reg_reg );
3592 %}
3593 
3594 // ====================REDUCTION ARITHMETIC=======================================
3595 
3596 // =======================AddReductionVI==========================================
3597 
3598 instruct vadd2I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
</pre>
<hr />
<pre>
5119     int vector_len = vector_length_encoding(this);
5120     __ vsqrtpd($dst$$XMMRegister, $mem$$Address, vector_len);
5121   %}
5122   ins_pipe( pipe_slow );
5123 %}
5124 
5125 // ------------------------------ Shift ---------------------------------------
5126 
5127 // Left and right shift count vectors are the same on x86
5128 // (only lowest bits of xmm reg are used for count).
5129 instruct vshiftcnt(vec dst, rRegI cnt) %{
5130   match(Set dst (LShiftCntV cnt));
5131   match(Set dst (RShiftCntV cnt));
5132   format %{ &quot;movdl    $dst,$cnt\t! load shift count&quot; %}
5133   ins_encode %{
5134     __ movdl($dst$$XMMRegister, $cnt$$Register);
5135   %}
5136   ins_pipe( pipe_slow );
5137 %}
5138 












5139 // Byte vector shift
5140 instruct vshiftB(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
5141   predicate(n-&gt;as_Vector()-&gt;length() &lt;= 8);
5142   match(Set dst (LShiftVB src shift));
5143   match(Set dst (RShiftVB src shift));
5144   match(Set dst (URShiftVB src shift));
5145   effect(TEMP dst, USE src, USE shift, TEMP tmp, TEMP scratch);
5146   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
5147   ins_encode %{
5148     assert(UseSSE &gt; 3, &quot;required&quot;);
5149     int opcode = this-&gt;ideal_Opcode();
5150     __ vextendbw(opcode, $tmp$$XMMRegister, $src$$XMMRegister);
5151     __ vshiftw(opcode, $tmp$$XMMRegister, $shift$$XMMRegister);
5152     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
5153     __ pand($dst$$XMMRegister, $tmp$$XMMRegister);
5154     __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);
5155   %}
5156   ins_pipe( pipe_slow );
5157 %}
5158 
</pre>
<hr />
<pre>
5649   ins_encode %{
5650     __ pmaddwd($dst$$XMMRegister, $src1$$XMMRegister);
5651   %}
5652   ins_pipe( pipe_slow );
5653 %}
5654 
5655 instruct vmuladdS2I_reg_avx(vec dst, vec src1, vec src2) %{
5656   predicate(UseAVX &gt; 0);
5657   match(Set dst (MulAddVS2VI src1 src2));
5658   format %{ &quot;vpmaddwd $dst,$src1,$src2\t! muladd packedStoI&quot; %}
5659   ins_encode %{
5660     int vector_len = vector_length_encoding(this);
5661     __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5662   %}
5663   ins_pipe( pipe_slow );
5664 %}
5665 
5666 // --------------------------------- Vector Multiply Add Add ----------------------------------
5667 
5668 instruct vmuladdaddS2I_reg(vec dst, vec src1, vec src2) %{
<span class="line-modified">5669   predicate(VM_Version::supports_avx512_vnni());</span>
5670   match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
5671   format %{ &quot;evpdpwssd $dst,$src1,$src2\t! muladdadd packedStoI&quot; %}
5672   ins_encode %{
5673     assert(UseAVX &gt; 2, &quot;required&quot;);
5674     int vector_len = vector_length_encoding(this);
5675     __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5676   %}
5677   ins_pipe( pipe_slow );
5678   ins_cost(10);
5679 %}
5680 
5681 // --------------------------------- PopCount --------------------------------------
5682 
5683 instruct vpopcountI(vec dst, vec src) %{
5684   match(Set dst (PopCountVI src));
5685   format %{ &quot;vpopcntd  $dst,$src\t! vector popcount packedI&quot; %}
5686   ins_encode %{
5687     assert(UsePopCountInstruction, &quot;not enabled&quot;);
5688 
5689     int vector_len = vector_length_encoding(this);
</pre>
</td>
</tr>
</table>
<center><a href="vtableStubs_x86_64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="x86_32.ad.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>