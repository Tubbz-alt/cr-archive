<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.amd64/src/org/graalvm/compiler/lir/amd64/AMD64StringLatin1InflateOp.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2018, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 
 25 package org.graalvm.compiler.lir.amd64;
 26 
 27 import static jdk.vm.ci.amd64.AMD64.k2;
 28 import static jdk.vm.ci.amd64.AMD64.rdi;
 29 import static jdk.vm.ci.amd64.AMD64.rdx;
 30 import static jdk.vm.ci.amd64.AMD64.rsi;
 31 import static jdk.vm.ci.code.ValueUtil.asRegister;
 32 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.REG;
 33 
 34 import java.util.EnumSet;
 35 
 36 import org.graalvm.compiler.asm.Label;
 37 import org.graalvm.compiler.asm.amd64.AMD64Address;
 38 import org.graalvm.compiler.asm.amd64.AMD64Assembler.ConditionFlag;
 39 import org.graalvm.compiler.asm.amd64.AMD64MacroAssembler;
 40 import org.graalvm.compiler.core.common.LIRKind;
 41 import org.graalvm.compiler.lir.LIRInstructionClass;
 42 import org.graalvm.compiler.lir.Opcode;
 43 import org.graalvm.compiler.lir.asm.CompilationResultBuilder;
 44 import org.graalvm.compiler.lir.gen.LIRGeneratorTool;
 45 
 46 import jdk.vm.ci.amd64.AMD64;
 47 import jdk.vm.ci.amd64.AMD64.CPUFeature;
 48 import jdk.vm.ci.amd64.AMD64Kind;
 49 import jdk.vm.ci.code.CodeUtil;
 50 import jdk.vm.ci.code.Register;
 51 import jdk.vm.ci.code.TargetDescription;
 52 import jdk.vm.ci.meta.Value;
 53 
 54 @Opcode(&quot;AMD64_STRING_INFLATE&quot;)
 55 public final class AMD64StringLatin1InflateOp extends AMD64LIRInstruction {
 56     public static final LIRInstructionClass&lt;AMD64StringLatin1InflateOp&gt; TYPE = LIRInstructionClass.create(AMD64StringLatin1InflateOp.class);
 57 
 58     private final int useAVX3Threshold;
 59 
 60     @Use({REG}) private Value rsrc;
 61     @Use({REG}) private Value rdst;
 62     @Use({REG}) private Value rlen;
 63 
 64     @Temp({REG}) private Value rsrcTemp;
 65     @Temp({REG}) private Value rdstTemp;
 66     @Temp({REG}) private Value rlenTemp;
 67 
 68     @Temp({REG}) private Value vtmp1;
 69     @Temp({REG}) private Value rtmp2;
 70 
 71     public AMD64StringLatin1InflateOp(LIRGeneratorTool tool, int useAVX3Threshold, Value src, Value dst, Value len) {
 72         super(TYPE);
 73 
 74         assert CodeUtil.isPowerOf2(useAVX3Threshold) : &quot;AVX3Threshold must be power of 2&quot;;
 75         this.useAVX3Threshold = useAVX3Threshold;
 76 
 77         assert asRegister(src).equals(rsi);
 78         assert asRegister(dst).equals(rdi);
 79         assert asRegister(len).equals(rdx);
 80 
 81         rsrcTemp = rsrc = src;
 82         rdstTemp = rdst = dst;
 83         rlenTemp = rlen = len;
 84 
 85         vtmp1 = useAVX512ForStringInflateCompress(tool.target()) ? tool.newVariable(LIRKind.value(AMD64Kind.V512_BYTE)) : tool.newVariable(LIRKind.value(AMD64Kind.V128_BYTE));
 86         rtmp2 = tool.newVariable(LIRKind.value(AMD64Kind.DWORD));
 87     }
 88 
 89     @Override
 90     public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 91         Register src = asRegister(rsrc);
 92         Register dst = asRegister(rdst);
 93         Register len = asRegister(rlen);
 94 
 95         Register tmp1 = asRegister(vtmp1);
 96         Register tmp2 = asRegister(rtmp2);
 97 
 98         byteArrayInflate(masm, src, dst, len, tmp1, tmp2);
 99     }
100 
101     public static boolean useAVX512ForStringInflateCompress(TargetDescription target) {
102         EnumSet&lt;CPUFeature&gt; features = ((AMD64) target.arch).getFeatures();
103         return features.contains(AMD64.CPUFeature.AVX512BW) &amp;&amp;
104                         features.contains(AMD64.CPUFeature.AVX512VL) &amp;&amp;
105                         features.contains(AMD64.CPUFeature.BMI2);
106     }
107 
108     /**
109      * Inflate a Latin1 string using a byte[] array representation into a UTF16 string using a
110      * char[] array representation.
111      *
112      * @param masm the assembler
113      * @param src (rsi) the start address of source byte[] to be inflated
114      * @param dst (rdi) the start address of destination char[] array
115      * @param len (rdx) the length
116      * @param tmp1 (xmm) temporary xmm register
117      * @param tmp2 (gpr) temporary gpr register
118      */
119     private void byteArrayInflate(AMD64MacroAssembler masm, Register src, Register dst, Register len, Register tmp1, Register tmp2) {
120         assert tmp1.getRegisterCategory().equals(AMD64.XMM);
121 
122         Label labelCopyCharsLoop = new Label();
123         Label labelDone = new Label();
124         Label labelBelowThreshold = new Label();
125         Label labelAVX3Threshold = new Label();
126 
127         // assert different registers
128         assert src.number != dst.number &amp;&amp; src.number != len.number &amp;&amp; src.number != tmp2.number;
129         assert dst.number != len.number &amp;&amp; dst.number != tmp2.number;
130         assert len.number != tmp2.number;
131 
132         masm.movl(tmp2, len);
133         if (useAVX512ForStringInflateCompress(masm.target)) {
134             Label labelCopy32Loop = new Label();
135             Label labelCopyTail = new Label();
136             Register tmp3Aliased = len;
137 
138             // If the length of the string is less than 16, we chose not to use the
139             // AVX512 instructions.
140             masm.testlAndJcc(len, -16, ConditionFlag.Zero, labelBelowThreshold, false);
141             masm.testlAndJcc(len, -1 * useAVX3Threshold, ConditionFlag.Zero, labelAVX3Threshold, false);
142 
143             // Test for suitable number chunks with respect to the size of the vector
144             // operation, mask off remaining number of chars (bytes) to inflate after
145             // committing to the vector loop.
146             // Adjust vector pointers to upper address bounds and inverse loop index.
147             // This will keep the loop condition simple.
148             //
149             // NOTE: The above idiom/pattern is used in all the loops below.
150 
151             masm.andl(tmp2, 32 - 1);  // The tail count (in chars).
152             // The vector count (in chars).
153             masm.andlAndJcc(len, -32, ConditionFlag.Zero, labelCopyTail, true);
154 
155             masm.leaq(src, new AMD64Address(src, len, AMD64Address.Scale.Times1));
156             masm.leaq(dst, new AMD64Address(dst, len, AMD64Address.Scale.Times2));
157             masm.negq(len);
158 
159             // Inflate 32 chars per iteration, reading 256-bit compact vectors
160             // and writing 512-bit inflated ditto.
161             masm.bind(labelCopy32Loop);
162             masm.evpmovzxbw(tmp1, new AMD64Address(src, len, AMD64Address.Scale.Times1));
163             masm.evmovdqu16(new AMD64Address(dst, len, AMD64Address.Scale.Times2), tmp1);
164             masm.addqAndJcc(len, 32, ConditionFlag.NotZero, labelCopy32Loop, false);
165 
166             masm.bind(labelCopyTail);
167             // All done if the tail count is zero.
168             masm.testlAndJcc(tmp2, tmp2, ConditionFlag.Zero, labelDone, false);
169 
170             // Compute (1 &lt;&lt; N) - 1 = ~(~0 &lt;&lt; N), where N is the remaining number
171             // of characters to process.
172             masm.movl(tmp3Aliased, -1);
173             masm.shlxl(tmp3Aliased, tmp3Aliased, tmp2);
174             masm.notl(tmp3Aliased);
175 
176             masm.kmovd(k2, tmp3Aliased);
177             masm.evpmovzxbw(tmp1, k2, new AMD64Address(src));
178             masm.evmovdqu16(new AMD64Address(dst), k2, tmp1);
179 
180             masm.jmp(labelDone);
181             masm.bind(labelAVX3Threshold);
182         }
183 
184         if (masm.supports(AMD64.CPUFeature.SSE4_2)) {
185             Label labelCopy16Loop = new Label();
186             Label labelCopy8Loop = new Label();
187             Label labelCopyBytes = new Label();
188             Label labelCopyNewTail = new Label();
189             Label labelCopyTail = new Label();
190 
191             if (masm.supports(AMD64.CPUFeature.AVX2)) {
192                 masm.andl(tmp2, 16 - 1);
193                 masm.andlAndJcc(len, -16, ConditionFlag.Zero, labelCopyNewTail, true);
194             } else {
195                 masm.andl(tmp2, 0x00000007);
196                 masm.andlAndJcc(len, 0xfffffff8, ConditionFlag.Zero, labelCopyTail, true);
197             }
198 
199             // vectored inflation
200             masm.leaq(src, new AMD64Address(src, len, AMD64Address.Scale.Times1));
201             masm.leaq(dst, new AMD64Address(dst, len, AMD64Address.Scale.Times2));
202             masm.negq(len);
203 
204             if (masm.supports(AMD64.CPUFeature.AVX2)) {
205                 masm.bind(labelCopy16Loop);
206                 masm.vpmovzxbw(tmp1, new AMD64Address(src, len, AMD64Address.Scale.Times1));
207                 masm.vmovdqu(new AMD64Address(dst, len, AMD64Address.Scale.Times2), tmp1);
208                 masm.addqAndJcc(len, 16, ConditionFlag.NotZero, labelCopy16Loop, false);
209 
210                 // The avx512 logic may branch here. We assume that avx2 is supported when we use
211                 // avx512 instructions.
212                 masm.bind(labelBelowThreshold);
213                 masm.bind(labelCopyNewTail);
214                 masm.movl(len, tmp2);
215                 masm.andl(tmp2, 0x00000007);
216                 masm.andlAndJcc(len, 0xfffffff8, ConditionFlag.Zero, labelCopyTail, true);
217 
218                 // Inflate another 8 bytes before final tail copy.
219                 masm.pmovzxbw(tmp1, new AMD64Address(src));
220                 masm.movdqu(new AMD64Address(dst), tmp1);
221                 masm.addq(src, 8);
222                 masm.addq(dst, 16);
223 
224                 masm.jmp(labelCopyTail);
225             }
226 
227             // Inflate 8 bytes (chars) per iteration, reading 64-bit compact vectors
228             // and writing 128-bit inflated ditto.
229             masm.bind(labelCopy8Loop);
230             masm.pmovzxbw(tmp1, new AMD64Address(src, len, AMD64Address.Scale.Times1));
231             masm.movdqu(new AMD64Address(dst, len, AMD64Address.Scale.Times2), tmp1);
232             masm.addqAndJcc(len, 8, ConditionFlag.NotZero, labelCopy8Loop, false);
233 
234             masm.bind(labelCopyTail);
235             masm.movl(len, tmp2);
236 
237             masm.cmplAndJcc(len, 4, ConditionFlag.Less, labelCopyBytes, true);
238 
239             masm.movdl(tmp1, new AMD64Address(src));
240             masm.pmovzxbw(tmp1, tmp1);
241             masm.movq(new AMD64Address(dst), tmp1);
242             masm.subq(len, 4);
243             masm.addq(src, 4);
244             masm.addq(dst, 8);
245 
246             masm.bind(labelCopyBytes);
247         } else {
248             // TODO this seems meaningless. And previously this recast does not contain this.
249             masm.bind(labelBelowThreshold);
250         }
251 
252         // Inflate any remaining characters (bytes) using a vanilla implementation.
253         masm.testlAndJcc(len, len, ConditionFlag.Zero, labelDone, true);
254         masm.leaq(src, new AMD64Address(src, len, AMD64Address.Scale.Times1));
255         masm.leaq(dst, new AMD64Address(dst, len, AMD64Address.Scale.Times2));
256         masm.negq(len);
257 
258         // Inflate a single byte (char) per iteration.
259         masm.bind(labelCopyCharsLoop);
260         masm.movzbl(tmp2, new AMD64Address(src, len, AMD64Address.Scale.Times1));
261         masm.movw(new AMD64Address(dst, len, AMD64Address.Scale.Times2), tmp2);
262         masm.incqAndJcc(len, ConditionFlag.NotZero, labelCopyCharsLoop, false);
263 
264         masm.bind(labelDone);
265     }
266 
267     @Override
268     public boolean needsClearUpperVectorRegisters() {
269         return true;
270     }
271 }
    </pre>
  </body>
</html>