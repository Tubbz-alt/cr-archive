<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.aarch64/src/org/graalvm/compiler/lir/aarch64/AArch64ArithmeticOp.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2013, 2019, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 
 25 package org.graalvm.compiler.lir.aarch64;
 26 
 27 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.REG;
 28 import static org.graalvm.compiler.lir.aarch64.AArch64ArithmeticOp.ARMv8ConstantCategory.ARITHMETIC;
 29 import static org.graalvm.compiler.lir.aarch64.AArch64ArithmeticOp.ARMv8ConstantCategory.LOGICAL;
 30 import static org.graalvm.compiler.lir.aarch64.AArch64ArithmeticOp.ARMv8ConstantCategory.NONE;
 31 import static org.graalvm.compiler.lir.aarch64.AArch64ArithmeticOp.ARMv8ConstantCategory.SHIFT;
 32 import static jdk.vm.ci.aarch64.AArch64.zr;
 33 import static jdk.vm.ci.code.ValueUtil.asRegister;
 34 
 35 import org.graalvm.compiler.asm.aarch64.AArch64Assembler;
 36 import org.graalvm.compiler.asm.aarch64.AArch64Assembler.ConditionFlag;
 37 import org.graalvm.compiler.debug.GraalError;
 38 import org.graalvm.compiler.asm.aarch64.AArch64MacroAssembler;
 39 import org.graalvm.compiler.lir.LIRInstructionClass;
 40 import org.graalvm.compiler.lir.Opcode;
 41 import org.graalvm.compiler.lir.asm.CompilationResultBuilder;
 42 
 43 import jdk.vm.ci.code.Register;
 44 import jdk.vm.ci.meta.AllocatableValue;
 45 import jdk.vm.ci.meta.JavaConstant;
 46 
 47 public enum AArch64ArithmeticOp {
 48     // TODO At least add and sub *can* be used with SP, so this should be supported
 49     NEG,
 50     NOT,
 51     ADD(ARITHMETIC),
 52     ADDS(ARITHMETIC),
 53     SUB(ARITHMETIC),
 54     SUBS(ARITHMETIC),
 55     MUL,
 56     MULVS,
 57     MNEG,
 58     DIV,
 59     SMULH,
 60     UMULH,
 61     SMULL,
 62     SMNEGL,
 63     MADD,
 64     MSUB,
 65     FMADD,
 66     SMADDL,
 67     SMSUBL,
 68     REM,
 69     UDIV,
 70     UREM,
 71     AND(LOGICAL),
 72     ANDS(LOGICAL),
 73     OR(LOGICAL),
 74     XOR(LOGICAL),
 75     SHL(SHIFT),
 76     LSHR(SHIFT),
 77     ASHR(SHIFT),
 78     ABS,
 79 
 80     FADD,
 81     FSUB,
 82     FMUL,
 83     FDIV,
 84     FREM,
 85     FNEG,
 86     FABS,
 87     FRINTM,
 88     FRINTN,
 89     FRINTP,
 90     SQRT;
 91 
 92     /**
 93      * Specifies what constants can be used directly without having to be loaded into a register
 94      * with the given instruction.
 95      */
 96     public enum ARMv8ConstantCategory {
 97         NONE,
 98         LOGICAL,
 99         ARITHMETIC,
100         SHIFT
101     }
102 
103     public final ARMv8ConstantCategory category;
104 
105     AArch64ArithmeticOp(ARMv8ConstantCategory category) {
106         this.category = category;
107     }
108 
109     AArch64ArithmeticOp() {
110         this(NONE);
111     }
112 
113     public static class UnaryOp extends AArch64LIRInstruction {
114         private static final LIRInstructionClass&lt;UnaryOp&gt; TYPE = LIRInstructionClass.create(UnaryOp.class);
115 
116         @Opcode private final AArch64ArithmeticOp opcode;
117         @Def({REG}) protected AllocatableValue result;
118         @Use({REG}) protected AllocatableValue x;
119 
120         public UnaryOp(AArch64ArithmeticOp opcode, AllocatableValue result, AllocatableValue x) {
121             super(TYPE);
122             this.opcode = opcode;
123             this.result = result;
124             this.x = x;
125         }
126 
127         @Override
128         public void emitCode(CompilationResultBuilder crb, AArch64MacroAssembler masm) {
129             Register dst = asRegister(result);
130             Register src = asRegister(x);
131             int size = result.getPlatformKind().getSizeInBytes() * Byte.SIZE;
132             switch (opcode) {
133                 case NEG:
134                     masm.sub(size, dst, zr, src);
135                     break;
136                 case FNEG:
137                     masm.fneg(size, dst, src);
138                     break;
139                 case NOT:
140                     masm.not(size, dst, src);
141                     break;
142                 case ABS:
143                     masm.cmp(size, src, 0);
144                     masm.csneg(size, dst, src, ConditionFlag.LT);
145                     break;
146                 case FABS:
147                     masm.fabs(size, dst, src);
148                     break;
149                 case FRINTM:
150                     masm.frintm(size, dst, src);
151                     break;
152                 case FRINTN:
153                     masm.frintn(size, dst, src);
154                     break;
155                 case FRINTP:
156                     masm.frintp(size, dst, src);
157                     break;
158                 case SQRT:
159                     masm.fsqrt(size, dst, src);
160                     break;
161                 default:
162                     throw GraalError.shouldNotReachHere(&quot;op=&quot; + opcode.name());
163             }
164         }
165     }
166 
167     public static class BinaryConstOp extends AArch64LIRInstruction {
168         private static final LIRInstructionClass&lt;BinaryConstOp&gt; TYPE = LIRInstructionClass.create(BinaryConstOp.class);
169 
170         @Opcode private final AArch64ArithmeticOp op;
171         @Def({REG}) protected AllocatableValue result;
172         @Use({REG}) protected AllocatableValue a;
173         private final JavaConstant b;
174 
175         public BinaryConstOp(AArch64ArithmeticOp op, AllocatableValue result, AllocatableValue a, JavaConstant b) {
176             super(TYPE);
177             this.op = op;
178             this.result = result;
179             this.a = a;
180             this.b = b;
181         }
182 
183         @Override
184         public void emitCode(CompilationResultBuilder crb, AArch64MacroAssembler masm) {
185             assert op.category != NONE;
186             Register dst = asRegister(result);
187             Register src = asRegister(a);
188             int size = result.getPlatformKind().getSizeInBytes() * Byte.SIZE;
189             switch (op) {
190                 case ADD:
191                     // Don&#39;t use asInt() here, since we can&#39;t use asInt on a long variable, even
192                     // if the constant easily fits as an int.
193                     assert AArch64MacroAssembler.isArithmeticImmediate(b.asLong());
194                     masm.add(size, dst, src, (int) b.asLong());
195                     break;
196                 case SUB:
197                     // Don&#39;t use asInt() here, since we can&#39;t use asInt on a long variable, even
198                     // if the constant easily fits as an int.
199                     assert AArch64MacroAssembler.isArithmeticImmediate(b.asLong());
200                     masm.sub(size, dst, src, (int) b.asLong());
201                     break;
202                 case ADDS:
203                     assert AArch64MacroAssembler.isArithmeticImmediate(b.asLong());
204                     masm.adds(size, dst, src, (int) b.asLong());
205                     break;
206                 case SUBS:
207                     assert AArch64MacroAssembler.isArithmeticImmediate(b.asLong());
208                     masm.subs(size, dst, src, (int) b.asLong());
209                     break;
210                 case AND:
211                     // XXX Should this be handled somewhere else?
212                     if (size == 32 &amp;&amp; b.asLong() == 0xFFFF_FFFFL) {
213                         masm.mov(size, dst, src);
214                     } else {
215                         masm.and(size, dst, src, b.asLong());
216                     }
217                     break;
218                 case ANDS:
219                     masm.ands(size, dst, src, b.asLong());
220                     break;
221                 case OR:
222                     masm.or(size, dst, src, b.asLong());
223                     break;
224                 case XOR:
225                     masm.eor(size, dst, src, b.asLong());
226                     break;
227                 case SHL:
228                     masm.shl(size, dst, src, b.asLong());
229                     break;
230                 case LSHR:
231                     masm.lshr(size, dst, src, b.asLong());
232                     break;
233                 case ASHR:
234                     masm.ashr(size, dst, src, b.asLong());
235                     break;
236                 default:
237                     throw GraalError.shouldNotReachHere(&quot;op=&quot; + op.name());
238             }
239         }
240     }
241 
242     public static class BinaryOp extends AArch64LIRInstruction {
243         private static final LIRInstructionClass&lt;BinaryOp&gt; TYPE = LIRInstructionClass.create(BinaryOp.class);
244 
245         @Opcode private final AArch64ArithmeticOp op;
246         @Def({REG}) protected AllocatableValue result;
247         @Use({REG}) protected AllocatableValue a;
248         @Use({REG}) protected AllocatableValue b;
249 
250         public BinaryOp(AArch64ArithmeticOp op, AllocatableValue result, AllocatableValue a, AllocatableValue b) {
251             super(TYPE);
252             this.op = op;
253             this.result = result;
254             this.a = a;
255             this.b = b;
256         }
257 
258         @Override
259         public void emitCode(CompilationResultBuilder crb, AArch64MacroAssembler masm) {
260             Register dst = asRegister(result);
261             Register src1 = asRegister(a);
262             Register src2 = asRegister(b);
263             int size = result.getPlatformKind().getSizeInBytes() * Byte.SIZE;
264             switch (op) {
265                 case ADD:
266                     masm.add(size, dst, src1, src2);
267                     break;
268                 case ADDS:
269                     masm.adds(size, dst, src1, src2);
270                     break;
271                 case SUB:
272                     masm.sub(size, dst, src1, src2);
273                     break;
274                 case SUBS:
275                     masm.subs(size, dst, src1, src2);
276                     break;
277                 case MUL:
278                     masm.mul(size, dst, src1, src2);
279                     break;
280                 case UMULH:
281                     masm.umulh(size, dst, src1, src2);
282                     break;
283                 case SMULH:
284                     masm.smulh(size, dst, src1, src2);
285                     break;
286                 case MNEG:
287                     masm.mneg(size, dst, src1, src2);
288                     break;
289                 case SMULL:
290                     masm.smull(size, dst, src1, src2);
291                     break;
292                 case SMNEGL:
293                     masm.smnegl(size, dst, src1, src2);
294                     break;
295                 case DIV:
296                     masm.sdiv(size, dst, src1, src2);
297                     break;
298                 case UDIV:
299                     masm.udiv(size, dst, src1, src2);
300                     break;
301                 case AND:
302                     masm.and(size, dst, src1, src2);
303                     break;
304                 case ANDS:
305                     masm.ands(size, dst, src1, src2);
306                     break;
307                 case OR:
308                     masm.or(size, dst, src1, src2);
309                     break;
310                 case XOR:
311                     masm.eor(size, dst, src1, src2);
312                     break;
313                 case SHL:
314                     masm.shl(size, dst, src1, src2);
315                     break;
316                 case LSHR:
317                     masm.lshr(size, dst, src1, src2);
318                     break;
319                 case ASHR:
320                     masm.ashr(size, dst, src1, src2);
321                     break;
322                 case FADD:
323                     masm.fadd(size, dst, src1, src2);
324                     break;
325                 case FSUB:
326                     masm.fsub(size, dst, src1, src2);
327                     break;
328                 case FMUL:
329                     masm.fmul(size, dst, src1, src2);
330                     break;
331                 case FDIV:
332                     masm.fdiv(size, dst, src1, src2);
333                     break;
334                 case MULVS:
335                     masm.mulvs(size, dst, src1, src2);
336                     break;
337                 default:
338                     throw GraalError.shouldNotReachHere(&quot;op=&quot; + op.name());
339             }
340         }
341     }
342 
343     /**
344      * Class used for instructions that have to reuse one of their arguments. This only applies to
345      * the remainder instructions at the moment, since we have to compute n % d using rem = n -
346      * TruncatingDivision(n, d) * d
347      *
348      * TODO (das) Replace the remainder nodes in the LIR.
349      */
350     public static class BinaryCompositeOp extends AArch64LIRInstruction {
351         private static final LIRInstructionClass&lt;BinaryCompositeOp&gt; TYPE = LIRInstructionClass.create(BinaryCompositeOp.class);
352         @Opcode private final AArch64ArithmeticOp op;
353         @Def({REG}) protected AllocatableValue result;
354         @Alive({REG}) protected AllocatableValue a;
355         @Alive({REG}) protected AllocatableValue b;
356 
357         public BinaryCompositeOp(AArch64ArithmeticOp op, AllocatableValue result, AllocatableValue a, AllocatableValue b) {
358             super(TYPE);
359             this.op = op;
360             this.result = result;
361             this.a = a;
362             this.b = b;
363         }
364 
365         @Override
366         public void emitCode(CompilationResultBuilder crb, AArch64MacroAssembler masm) {
367             Register dst = asRegister(result);
368             Register src1 = asRegister(a);
369             Register src2 = asRegister(b);
370             int size = result.getPlatformKind().getSizeInBytes() * Byte.SIZE;
371             switch (op) {
372                 case REM:
373                     masm.rem(size, dst, src1, src2);
374                     break;
375                 case UREM:
376                     masm.urem(size, dst, src1, src2);
377                     break;
378                 case FREM:
379                     masm.frem(size, dst, src1, src2);
380                     break;
381                 default:
382                     throw GraalError.shouldNotReachHere();
383             }
384         }
385     }
386 
387     public static class BinaryShiftOp extends AArch64LIRInstruction {
388         private static final LIRInstructionClass&lt;BinaryShiftOp&gt; TYPE = LIRInstructionClass.create(BinaryShiftOp.class);
389 
390         @Opcode private final AArch64ArithmeticOp op;
391         @Def(REG) protected AllocatableValue result;
392         @Use(REG) protected AllocatableValue src1;
393         @Use(REG) protected AllocatableValue src2;
394         private final AArch64MacroAssembler.ShiftType shiftType;
395         private final int shiftAmt;
396         private final boolean isShiftNot;
397 
398         /**
399          * If shiftNot: Computes &lt;code&gt;result = src1 &lt;op&gt; ~(src2 &lt;shiftType&gt; &lt;shiftAmt&gt;)&lt;/code&gt;
400          * (Only for logic ops). else: Computes
401          * &lt;code&gt;result = src1 &lt;op&gt; src2 &lt;shiftType&gt; &lt;shiftAmt&gt;&lt;/code&gt;.
402          */
403         public BinaryShiftOp(AArch64ArithmeticOp op, AllocatableValue result, AllocatableValue src1, AllocatableValue src2,
404                         AArch64MacroAssembler.ShiftType shiftType, int shiftAmt, boolean isShiftNot) {
405             super(TYPE);
406             assert op == ADD || op == SUB || op == AND || op == OR || op == XOR;
407             this.op = op;
408             this.result = result;
409             this.src1 = src1;
410             this.src2 = src2;
411             this.shiftType = shiftType;
412             this.shiftAmt = shiftAmt;
413             this.isShiftNot = isShiftNot;
414         }
415 
416         @Override
417         public void emitCode(CompilationResultBuilder crb, AArch64MacroAssembler masm) {
418             int size = result.getPlatformKind().getSizeInBytes() * Byte.SIZE;
419             switch (op) {
420                 case ADD:
421                     masm.add(size, asRegister(result), asRegister(src1), asRegister(src2), shiftType, shiftAmt);
422                     break;
423                 case SUB:
424                     masm.sub(size, asRegister(result), asRegister(src1), asRegister(src2), shiftType, shiftAmt);
425                     break;
426                 case AND:
427                     if (!isShiftNot) {
428                         masm.and(size, asRegister(result), asRegister(src1), asRegister(src2), shiftType, shiftAmt);
429                     } else {
430                         masm.bic(size, asRegister(result), asRegister(src1), asRegister(src2), shiftType, shiftAmt);
431                     }
432                     break;
433                 case OR:
434                     if (!isShiftNot) {
435                         masm.or(size, asRegister(result), asRegister(src1), asRegister(src2), shiftType, shiftAmt);
436                     } else {
437                         masm.orn(size, asRegister(result), asRegister(src1), asRegister(src2), shiftType, shiftAmt);
438                     }
439                     break;
440                 case XOR:
441                     if (!isShiftNot) {
442                         masm.eor(size, asRegister(result), asRegister(src1), asRegister(src2), shiftType, shiftAmt);
443                     } else {
444                         masm.eon(size, asRegister(result), asRegister(src1), asRegister(src2), shiftType, shiftAmt);
445                     }
446                     break;
447                 default:
448                     throw GraalError.shouldNotReachHere();
449             }
450         }
451     }
452 
453     public static class ExtendedAddShiftOp extends AArch64LIRInstruction {
454         private static final LIRInstructionClass&lt;ExtendedAddShiftOp&gt; TYPE = LIRInstructionClass.create(ExtendedAddShiftOp.class);
455         @Def(REG) protected AllocatableValue result;
456         @Use(REG) protected AllocatableValue src1;
457         @Use(REG) protected AllocatableValue src2;
458         private final AArch64Assembler.ExtendType extendType;
459         private final int shiftAmt;
460 
461         /**
462          * Computes &lt;code&gt;result = src1 + extendType(src2) &lt;&lt; shiftAmt&lt;/code&gt;.
463          *
464          * @param extendType defines how src2 is extended to the same size as src1.
465          * @param shiftAmt must be in range 0 to 4.
466          */
467         public ExtendedAddShiftOp(AllocatableValue result, AllocatableValue src1, AllocatableValue src2, AArch64Assembler.ExtendType extendType, int shiftAmt) {
468             super(TYPE);
469             this.result = result;
470             this.src1 = src1;
471             this.src2 = src2;
472             this.extendType = extendType;
473             this.shiftAmt = shiftAmt;
474         }
475 
476         @Override
477         public void emitCode(CompilationResultBuilder crb, AArch64MacroAssembler masm) {
478             int size = result.getPlatformKind().getSizeInBytes() * Byte.SIZE;
479             masm.add(size, asRegister(result), asRegister(src1), asRegister(src2), extendType, shiftAmt);
480         }
481     }
482 
483     public static class MultiplyAddSubOp extends AArch64LIRInstruction {
484         private static final LIRInstructionClass&lt;MultiplyAddSubOp&gt; TYPE = LIRInstructionClass.create(MultiplyAddSubOp.class);
485 
486         @Opcode private final AArch64ArithmeticOp op;
487         @Def(REG) protected AllocatableValue result;
488         @Use(REG) protected AllocatableValue src1;
489         @Use(REG) protected AllocatableValue src2;
490         @Use(REG) protected AllocatableValue src3;
491 
492         /**
493          * Computes &lt;code&gt;result = src3 +/- src1 * src2&lt;/code&gt;.
494          */
495         public MultiplyAddSubOp(AArch64ArithmeticOp op, AllocatableValue result, AllocatableValue src1, AllocatableValue src2, AllocatableValue src3) {
496             super(TYPE);
497             this.op = op;
498             this.result = result;
499             this.src1 = src1;
500             this.src2 = src2;
501             this.src3 = src3;
502         }
503 
504         @Override
505         public void emitCode(CompilationResultBuilder crb, AArch64MacroAssembler masm) {
506             int size = result.getPlatformKind().getSizeInBytes() * Byte.SIZE;
507             switch (op) {
508                 case MADD:
509                     masm.madd(size, asRegister(result), asRegister(src1), asRegister(src2), asRegister(src3));
510                     break;
511                 case MSUB:
512                     masm.msub(size, asRegister(result), asRegister(src1), asRegister(src2), asRegister(src3));
513                     break;
514                 case FMADD:
515                     masm.fmadd(size, asRegister(result), asRegister(src1), asRegister(src2), asRegister(src3));
516                     break;
517                 case SMADDL:
518                     assert size == 64;
519                     masm.smaddl(size, asRegister(result), asRegister(src1), asRegister(src2), asRegister(src3));
520                     break;
521                 case SMSUBL:
522                     assert size == 64;
523                     masm.smsubl(size, asRegister(result), asRegister(src1), asRegister(src2), asRegister(src3));
524                     break;
525                 default:
526                     throw GraalError.shouldNotReachHere();
527             }
528         }
529     }
530 
531 }
    </pre>
  </body>
</html>