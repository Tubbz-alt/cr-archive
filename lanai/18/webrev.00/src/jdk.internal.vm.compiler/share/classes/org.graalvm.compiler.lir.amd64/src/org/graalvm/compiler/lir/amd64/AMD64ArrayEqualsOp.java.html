<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.amd64/src/org/graalvm/compiler/lir/amd64/AMD64ArrayEqualsOp.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2013, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 
 25 package org.graalvm.compiler.lir.amd64;
 26 
 27 import static jdk.vm.ci.code.ValueUtil.asRegister;
 28 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.XOR;
 29 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.CONST;
 30 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.ILLEGAL;
 31 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.REG;
 32 
 33 import java.util.Objects;
 34 
 35 import org.graalvm.compiler.asm.Label;
 36 import org.graalvm.compiler.asm.amd64.AMD64Address;
 37 import org.graalvm.compiler.asm.amd64.AMD64Address.Scale;
 38 import org.graalvm.compiler.asm.amd64.AMD64Assembler;
 39 import org.graalvm.compiler.asm.amd64.AMD64Assembler.ConditionFlag;
 40 import org.graalvm.compiler.asm.amd64.AMD64Assembler.SSEOp;
 41 import org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize;
 42 import org.graalvm.compiler.asm.amd64.AMD64MacroAssembler;
 43 import org.graalvm.compiler.asm.amd64.AVXKind;
 44 import org.graalvm.compiler.core.common.LIRKind;
 45 import org.graalvm.compiler.debug.GraalError;
 46 import org.graalvm.compiler.lir.LIRInstructionClass;
 47 import org.graalvm.compiler.lir.LIRValueUtil;
 48 import org.graalvm.compiler.lir.Opcode;
 49 import org.graalvm.compiler.lir.asm.CompilationResultBuilder;
 50 import org.graalvm.compiler.lir.gen.LIRGeneratorTool;
 51 
 52 import jdk.vm.ci.amd64.AMD64;
 53 import jdk.vm.ci.amd64.AMD64.CPUFeature;
 54 import jdk.vm.ci.amd64.AMD64Kind;
 55 import jdk.vm.ci.code.Register;
 56 import jdk.vm.ci.code.TargetDescription;
 57 import jdk.vm.ci.meta.JavaKind;
 58 import jdk.vm.ci.meta.Value;
 59 
 60 /**
 61  * Emits code which compares two arrays of the same length. If the CPU supports any vector
 62  * instructions specialized code is emitted to leverage these instructions.
 63  *
 64  * This op can also compare arrays of different integer types (e.g. {@code byte[]} and
 65  * {@code char[]}) with on-the-fly sign- or zero-extension. If one of the given arrays is a
 66  * {@code char[]} array, the smaller elements are zero-extended, otherwise they are sign-extended.
 67  */
 68 @Opcode(&quot;ARRAY_EQUALS&quot;)
 69 public final class AMD64ArrayEqualsOp extends AMD64LIRInstruction {
 70     public static final LIRInstructionClass&lt;AMD64ArrayEqualsOp&gt; TYPE = LIRInstructionClass.create(AMD64ArrayEqualsOp.class);
 71 
 72     private final JavaKind kind1;
 73     private final JavaKind kind2;
 74     private final int arrayBaseOffset1;
 75     private final int arrayBaseOffset2;
 76     private final Scale arrayIndexScale1;
 77     private final Scale arrayIndexScale2;
 78     private final AVXKind.AVXSize vectorSize;
 79     private final boolean signExtend;
 80 
 81     @Def({REG}) private Value resultValue;
 82     @Alive({REG}) private Value array1Value;
 83     @Alive({REG}) private Value array2Value;
 84     @Alive({REG, CONST}) private Value lengthValue;
 85     @Temp({REG, ILLEGAL}) private Value temp1;
 86     @Temp({REG, ILLEGAL}) private Value temp2;
 87     @Temp({REG}) private Value temp3;
 88     @Temp({REG, ILLEGAL}) private Value temp4;
 89 
 90     @Temp({REG, ILLEGAL}) private Value temp5;
 91     @Temp({REG, ILLEGAL}) private Value tempXMM;
 92 
 93     @Temp({REG, ILLEGAL}) private Value vectorTemp1;
 94     @Temp({REG, ILLEGAL}) private Value vectorTemp2;
 95     @Temp({REG, ILLEGAL}) private Value vectorTemp3;
 96     @Temp({REG, ILLEGAL}) private Value vectorTemp4;
 97 
 98     public AMD64ArrayEqualsOp(LIRGeneratorTool tool, JavaKind kind1, JavaKind kind2, Value result, Value array1, Value array2, Value length,
 99                     boolean directPointers, int maxVectorSize) {
100         super(TYPE);
101         this.kind1 = kind1;
102         this.kind2 = kind2;
103         this.signExtend = kind1 != JavaKind.Char &amp;&amp; kind2 != JavaKind.Char;
104 
105         assert kind1.isNumericInteger() &amp;&amp; kind2.isNumericInteger() || kind1 == kind2;
106 
107         this.arrayBaseOffset1 = directPointers ? 0 : tool.getProviders().getMetaAccess().getArrayBaseOffset(kind1);
108         this.arrayBaseOffset2 = directPointers ? 0 : tool.getProviders().getMetaAccess().getArrayBaseOffset(kind2);
109         this.arrayIndexScale1 = Objects.requireNonNull(Scale.fromInt(tool.getProviders().getMetaAccess().getArrayIndexScale(kind1)));
110         this.arrayIndexScale2 = Objects.requireNonNull(Scale.fromInt(tool.getProviders().getMetaAccess().getArrayIndexScale(kind2)));
111         this.vectorSize = ((AMD64) tool.target().arch).getFeatures().contains(CPUFeature.AVX2) &amp;&amp; (maxVectorSize &lt; 0 || maxVectorSize &gt;= 32) ? AVXKind.AVXSize.YMM : AVXKind.AVXSize.XMM;
112 
113         this.resultValue = result;
114         this.array1Value = array1;
115         this.array2Value = array2;
116         this.lengthValue = length;
117 
118         // Allocate some temporaries.
119         if (supportsSSE41(tool.target()) &amp;&amp; canGenerateConstantLengthCompare(tool.target()) &amp;&amp; !constantLengthCompareNeedsTmpArrayPointers()) {
120             this.temp1 = Value.ILLEGAL;
121             this.temp2 = Value.ILLEGAL;
122         } else {
123             this.temp1 = tool.newVariable(LIRKind.unknownReference(tool.target().arch.getWordKind()));
124             this.temp2 = tool.newVariable(LIRKind.unknownReference(tool.target().arch.getWordKind()));
125         }
126         this.temp3 = tool.newVariable(LIRKind.value(tool.target().arch.getWordKind()));
127         if (supportsSSE41(tool.target()) &amp;&amp; canGenerateConstantLengthCompare(tool.target())) {
128             this.temp4 = Value.ILLEGAL;
129             this.temp5 = Value.ILLEGAL;
130         } else {
131             this.temp4 = tool.newVariable(LIRKind.value(tool.target().arch.getWordKind()));
132             this.temp5 = kind1.isNumericFloat() || kind1 != kind2 ? tool.newVariable(LIRKind.value(tool.target().arch.getWordKind())) : Value.ILLEGAL;
133         }
134 
135         if (kind1 == JavaKind.Float) {
136             this.tempXMM = tool.newVariable(LIRKind.value(AMD64Kind.SINGLE));
137         } else if (kind1 == JavaKind.Double) {
138             this.tempXMM = tool.newVariable(LIRKind.value(AMD64Kind.DOUBLE));
139         } else {
140             this.tempXMM = Value.ILLEGAL;
141         }
142 
143         // We only need the vector temporaries if we generate SSE code.
144         if (supportsSSE41(tool.target())) {
145             if (canGenerateConstantLengthCompare(tool.target())) {
146                 LIRKind lirKind = LIRKind.value(vectorSize == AVXKind.AVXSize.YMM ? AMD64Kind.V256_BYTE : AMD64Kind.V128_BYTE);
147                 this.vectorTemp1 = tool.newVariable(lirKind);
148                 this.vectorTemp2 = tool.newVariable(lirKind);
149                 this.vectorTemp3 = tool.newVariable(lirKind);
150                 this.vectorTemp4 = tool.newVariable(lirKind);
151             } else {
152                 this.vectorTemp1 = tool.newVariable(LIRKind.value(AMD64Kind.DOUBLE));
153                 this.vectorTemp2 = tool.newVariable(LIRKind.value(AMD64Kind.DOUBLE));
154                 this.vectorTemp3 = Value.ILLEGAL;
155                 this.vectorTemp4 = Value.ILLEGAL;
156             }
157         } else {
158             this.vectorTemp1 = Value.ILLEGAL;
159             this.vectorTemp2 = Value.ILLEGAL;
160             this.vectorTemp3 = Value.ILLEGAL;
161             this.vectorTemp4 = Value.ILLEGAL;
162         }
163     }
164 
165     private boolean canGenerateConstantLengthCompare(TargetDescription target) {
166         return LIRValueUtil.isJavaConstant(lengthValue) &amp;&amp; kind1.isNumericInteger() &amp;&amp; (kind1 == kind2 || getElementsPerVector(AVXKind.AVXSize.XMM) &lt;= constantLength()) &amp;&amp; supportsSSE41(target);
167     }
168 
169     private int constantLength() {
170         return LIRValueUtil.asJavaConstant(lengthValue).asInt();
171     }
172 
173     @Override
174     public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
175         Register result = asRegister(resultValue);
176 
177         Label trueLabel = new Label();
178         Label falseLabel = new Label();
179         Label done = new Label();
180 
181         if (canGenerateConstantLengthCompare(crb.target)) {
182             emitConstantLengthArrayCompareBytes(crb, masm, new Register[]{asRegister(vectorTemp1), asRegister(vectorTemp2), asRegister(vectorTemp3), asRegister(vectorTemp4)}, falseLabel);
183         } else {
184             Register array1 = asRegister(temp1);
185             Register array2 = asRegister(temp2);
186             // Load array base addresses.
187             masm.leaq(array1, new AMD64Address(asRegister(array1Value), arrayBaseOffset1));
188             masm.leaq(array2, new AMD64Address(asRegister(array2Value), arrayBaseOffset2));
189             Register length = asRegister(temp3);
190             // Get array length.
191             if (LIRValueUtil.isJavaConstant(lengthValue)) {
192                 masm.movl(length, constantLength());
193             } else {
194                 masm.movl(length, asRegister(lengthValue));
195             }
196             // copy
197             masm.movl(result, length);
198             emitArrayCompare(crb, masm, result, array1, array2, length, trueLabel, falseLabel);
199         }
200 
201         // Return true
202         masm.bind(trueLabel);
203         masm.movl(result, 1);
204         masm.jmpb(done);
205 
206         // Return false
207         masm.bind(falseLabel);
208         masm.xorl(result, result);
209 
210         // That&#39;s it
211         masm.bind(done);
212     }
213 
214     private void emitArrayCompare(CompilationResultBuilder crb, AMD64MacroAssembler masm,
215                     Register result, Register array1, Register array2, Register length,
216                     Label trueLabel, Label falseLabel) {
217         if (supportsSSE41(crb.target)) {
218             emitVectorCompare(crb, masm, result, array1, array2, length, trueLabel, falseLabel);
219         }
220         if (kind1 == kind2) {
221             emit8ByteCompare(crb, masm, result, array1, array2, length, trueLabel, falseLabel);
222             emitTailCompares(masm, result, array1, array2, length, trueLabel, falseLabel);
223         } else {
224             emitDifferentKindsElementWiseCompare(crb, masm, result, array1, array2, length, trueLabel, falseLabel);
225         }
226     }
227 
228     /**
229      * Returns if the underlying AMD64 architecture supports SSE 4.1 instructions.
230      *
231      * @param target target description of the underlying architecture
232      * @return true if the underlying architecture supports SSE 4.1
233      */
234     private static boolean supportsSSE41(TargetDescription target) {
235         AMD64 arch = (AMD64) target.arch;
236         return arch.getFeatures().contains(CPUFeature.SSE4_1);
237     }
238 
239     /**
240      * Emits code that uses SSE4.1/AVX1 128-bit (16-byte) or AVX2 256-bit (32-byte) vector compares.
241      */
242     private void emitVectorCompare(CompilationResultBuilder crb, AMD64MacroAssembler masm,
243                     Register result, Register array1, Register array2, Register length,
244                     Label trueLabel, Label falseLabel) {
245         assert supportsSSE41(crb.target);
246 
247         Register vector1 = asRegister(vectorTemp1);
248         Register vector2 = asRegister(vectorTemp2);
249 
250         int elementsPerVector = getElementsPerVector(vectorSize);
251 
252         Label loop = new Label();
253         Label compareTail = new Label();
254 
255         boolean requiresNaNCheck = kind1.isNumericFloat();
256         Label loopCheck = new Label();
257         Label nanCheck = new Label();
258 
259         // Compare 16-byte vectors
260         masm.andl(result, elementsPerVector - 1); // tail count
261         masm.andlAndJcc(length, ~(elementsPerVector - 1), ConditionFlag.Zero, compareTail, false);
262 
263         masm.leaq(array1, new AMD64Address(array1, length, arrayIndexScale1, 0));
264         masm.leaq(array2, new AMD64Address(array2, length, arrayIndexScale2, 0));
265         masm.negq(length);
266 
267         // Align the main loop
268         masm.align(crb.target.wordSize * 2);
269         masm.bind(loop);
270         emitVectorLoad1(masm, vector1, array1, length, 0, vectorSize);
271         emitVectorLoad2(masm, vector2, array2, length, 0, vectorSize);
272         emitVectorCmp(masm, vector1, vector2, vectorSize);
273         masm.jcc(ConditionFlag.NotZero, requiresNaNCheck ? nanCheck : falseLabel);
274 
275         masm.bind(loopCheck);
276         masm.addqAndJcc(length, elementsPerVector, ConditionFlag.NotZero, loop, false);
277 
278         masm.testlAndJcc(result, result, ConditionFlag.Zero, trueLabel, false);
279 
280         if (requiresNaNCheck) {
281             Label unalignedCheck = new Label();
282             masm.jmpb(unalignedCheck);
283             masm.bind(nanCheck);
284             emitFloatCompareWithinRange(crb, masm, array1, array2, length, 0, falseLabel, elementsPerVector);
285             masm.jmpb(loopCheck);
286             masm.bind(unalignedCheck);
287         }
288 
289         /*
290          * Compare the remaining bytes with an unaligned memory load aligned to the end of the
291          * array.
292          */
293         emitVectorLoad1(masm, vector1, array1, result, scaleDisplacement1(-vectorSize.getBytes()), vectorSize);
294         emitVectorLoad2(masm, vector2, array2, result, scaleDisplacement2(-vectorSize.getBytes()), vectorSize);
295         emitVectorCmp(masm, vector1, vector2, vectorSize);
296         if (requiresNaNCheck) {
297             masm.jcc(ConditionFlag.Zero, trueLabel);
298             emitFloatCompareWithinRange(crb, masm, array1, array2, result, -vectorSize.getBytes(), falseLabel, elementsPerVector);
299         } else {
300             masm.jcc(ConditionFlag.NotZero, falseLabel);
301         }
302         masm.jmp(trueLabel);
303 
304         masm.bind(compareTail);
305         masm.movl(length, result);
306     }
307 
308     private int getElementsPerVector(AVXKind.AVXSize vSize) {
309         return vSize.getBytes() &gt;&gt; Math.max(arrayIndexScale1.log2, arrayIndexScale2.log2);
310     }
311 
312     private void emitVectorLoad1(AMD64MacroAssembler asm, Register dst, Register src, int displacement, AVXKind.AVXSize size) {
313         emitVectorLoad1(asm, dst, src, Register.None, displacement, size);
314     }
315 
316     private void emitVectorLoad2(AMD64MacroAssembler asm, Register dst, Register src, int displacement, AVXKind.AVXSize size) {
317         emitVectorLoad2(asm, dst, src, Register.None, displacement, size);
318     }
319 
320     private void emitVectorLoad1(AMD64MacroAssembler asm, Register dst, Register src, Register index, int displacement, AVXKind.AVXSize size) {
321         emitVectorLoad(asm, dst, src, index, displacement, arrayIndexScale1, arrayIndexScale2, size);
322     }
323 
324     private void emitVectorLoad2(AMD64MacroAssembler asm, Register dst, Register src, Register index, int displacement, AVXKind.AVXSize size) {
325         emitVectorLoad(asm, dst, src, index, displacement, arrayIndexScale2, arrayIndexScale1, size);
326     }
327 
328     private void emitVectorLoad(AMD64MacroAssembler asm, Register dst, Register src, Register index, int displacement, Scale ownScale, Scale otherScale, AVXKind.AVXSize size) {
329         AMD64Address address = new AMD64Address(src, index, ownScale, displacement);
330         if (ownScale.value &lt; otherScale.value) {
331             if (size == AVXKind.AVXSize.YMM) {
332                 getAVX2LoadAndExtendOp(ownScale, otherScale, signExtend).emit(asm, size, dst, address);
333             } else {
334                 loadAndExtendSSE(asm, dst, address, ownScale, otherScale, signExtend);
335             }
336         } else {
337             if (size == AVXKind.AVXSize.YMM) {
338                 asm.vmovdqu(dst, address);
339             } else {
340                 asm.movdqu(dst, address);
341             }
342         }
343     }
344 
345     private int scaleDisplacement1(int displacement) {
346         return scaleDisplacement(displacement, arrayIndexScale1, arrayIndexScale2);
347     }
348 
349     private int scaleDisplacement2(int displacement) {
350         return scaleDisplacement(displacement, arrayIndexScale2, arrayIndexScale1);
351     }
352 
353     private static int scaleDisplacement(int displacement, Scale ownScale, Scale otherScale) {
354         if (ownScale.value &lt; otherScale.value) {
355             return displacement &gt;&gt; (otherScale.log2 - ownScale.log2);
356         }
357         return displacement;
358     }
359 
360     private static AMD64Assembler.VexRMOp getAVX2LoadAndExtendOp(Scale ownScale, Scale otherScale, boolean signExtend) {
361         switch (ownScale) {
362             case Times1:
363                 switch (otherScale) {
364                     case Times2:
365                         return signExtend ? AMD64Assembler.VexRMOp.VPMOVSXBW : AMD64Assembler.VexRMOp.VPMOVZXBW;
366                     case Times4:
367                         return signExtend ? AMD64Assembler.VexRMOp.VPMOVSXBD : AMD64Assembler.VexRMOp.VPMOVZXBD;
368                     case Times8:
369                         return signExtend ? AMD64Assembler.VexRMOp.VPMOVSXBQ : AMD64Assembler.VexRMOp.VPMOVZXBQ;
370                 }
371                 throw GraalError.shouldNotReachHere();
372             case Times2:
373                 switch (otherScale) {
374                     case Times4:
375                         return signExtend ? AMD64Assembler.VexRMOp.VPMOVSXWD : AMD64Assembler.VexRMOp.VPMOVZXWD;
376                     case Times8:
377                         return signExtend ? AMD64Assembler.VexRMOp.VPMOVSXWQ : AMD64Assembler.VexRMOp.VPMOVZXWQ;
378                 }
379                 throw GraalError.shouldNotReachHere();
380             case Times4:
381                 return signExtend ? AMD64Assembler.VexRMOp.VPMOVSXDQ : AMD64Assembler.VexRMOp.VPMOVZXDQ;
382         }
383         throw GraalError.shouldNotReachHere();
384     }
385 
386     private static void loadAndExtendSSE(AMD64MacroAssembler asm, Register dst, AMD64Address src, Scale ownScale, Scale otherScale, boolean signExtend) {
387         switch (ownScale) {
388             case Times1:
389                 switch (otherScale) {
390                     case Times2:
391                         if (signExtend) {
392                             asm.pmovsxbw(dst, src);
393                         } else {
394                             asm.pmovzxbw(dst, src);
395                         }
396                         return;
397                     case Times4:
398                         if (signExtend) {
399                             asm.pmovsxbd(dst, src);
400                         } else {
401                             asm.pmovzxbd(dst, src);
402                         }
403                         return;
404                     case Times8:
405                         if (signExtend) {
406                             asm.pmovsxbq(dst, src);
407                         } else {
408                             asm.pmovzxbq(dst, src);
409                         }
410                         return;
411                 }
412                 throw GraalError.shouldNotReachHere();
413             case Times2:
414                 switch (otherScale) {
415                     case Times4:
416                         if (signExtend) {
417                             asm.pmovsxwd(dst, src);
418                         } else {
419                             asm.pmovzxwd(dst, src);
420                         }
421                         return;
422                     case Times8:
423                         if (signExtend) {
424                             asm.pmovsxwq(dst, src);
425                         } else {
426                             asm.pmovzxwq(dst, src);
427                         }
428                         return;
429                 }
430                 throw GraalError.shouldNotReachHere();
431             case Times4:
432                 if (signExtend) {
433                     asm.pmovsxdq(dst, src);
434                 } else {
435                     asm.pmovzxdq(dst, src);
436                 }
437                 return;
438         }
439         throw GraalError.shouldNotReachHere();
440     }
441 
442     private static void emitVectorCmp(AMD64MacroAssembler masm, Register vector1, Register vector2, AVXKind.AVXSize size) {
443         emitVectorXor(masm, vector1, vector2, size);
444         emitVectorTest(masm, vector1, size);
445     }
446 
447     private static void emitVectorXor(AMD64MacroAssembler masm, Register vector1, Register vector2, AVXKind.AVXSize size) {
448         if (size == AVXKind.AVXSize.YMM) {
449             masm.vpxor(vector1, vector1, vector2);
450         } else {
451             masm.pxor(vector1, vector2);
452         }
453     }
454 
455     private static void emitVectorTest(AMD64MacroAssembler masm, Register vector1, AVXKind.AVXSize size) {
456         if (size == AVXKind.AVXSize.YMM) {
457             masm.vptest(vector1, vector1);
458         } else {
459             masm.ptest(vector1, vector1);
460         }
461     }
462 
463     /**
464      * Vector size used in {@link #emit8ByteCompare}.
465      */
466     private static final int VECTOR_SIZE = 8;
467 
468     /**
469      * Emits code that uses 8-byte vector compares.
470      */
471     private void emit8ByteCompare(CompilationResultBuilder crb, AMD64MacroAssembler masm,
472                     Register result, Register array1, Register array2, Register length, Label trueLabel, Label falseLabel) {
473         assert kind1 == kind2;
474         Label loop = new Label();
475         Label compareTail = new Label();
476 
477         int elementsPerVector = 8 &gt;&gt; arrayIndexScale1.log2;
478 
479         boolean requiresNaNCheck = kind1.isNumericFloat();
480         Label loopCheck = new Label();
481         Label nanCheck = new Label();
482 
483         Register temp = asRegister(temp4);
484 
485         masm.andl(result, elementsPerVector - 1); // tail count
486         masm.andlAndJcc(length, ~(elementsPerVector - 1), ConditionFlag.Zero, compareTail, false);
487 
488         masm.leaq(array1, new AMD64Address(array1, length, arrayIndexScale1, 0));
489         masm.leaq(array2, new AMD64Address(array2, length, arrayIndexScale2, 0));
490         masm.negq(length);
491 
492         // Align the main loop
493         masm.align(crb.target.wordSize * 2);
494         masm.bind(loop);
495         masm.movq(temp, new AMD64Address(array1, length, arrayIndexScale1, 0));
496         masm.cmpqAndJcc(temp, new AMD64Address(array2, length, arrayIndexScale2, 0), ConditionFlag.NotEqual, requiresNaNCheck ? nanCheck : falseLabel, false);
497 
498         masm.bind(loopCheck);
499         masm.addqAndJcc(length, elementsPerVector, ConditionFlag.NotZero, loop, true);
500 
501         masm.testlAndJcc(result, result, ConditionFlag.Zero, trueLabel, false);
502 
503         if (requiresNaNCheck) {
504             // NaN check is slow path and hence placed outside of the main loop.
505             Label unalignedCheck = new Label();
506             masm.jmpb(unalignedCheck);
507             masm.bind(nanCheck);
508             // At most two iterations, unroll in the emitted code.
509             for (int offset = 0; offset &lt; VECTOR_SIZE; offset += kind1.getByteCount()) {
510                 emitFloatCompare(masm, array1, array2, length, offset, falseLabel, kind1.getByteCount() == VECTOR_SIZE);
511             }
512             masm.jmpb(loopCheck);
513             masm.bind(unalignedCheck);
514         }
515 
516         /*
517          * Compare the remaining bytes with an unaligned memory load aligned to the end of the
518          * array.
519          */
520         masm.movq(temp, new AMD64Address(array1, result, arrayIndexScale1, -VECTOR_SIZE));
521         if (requiresNaNCheck) {
522             masm.cmpqAndJcc(temp, new AMD64Address(array2, result, arrayIndexScale2, -VECTOR_SIZE), ConditionFlag.Equal, trueLabel, false);
523             // At most two iterations, unroll in the emitted code.
524             for (int offset = 0; offset &lt; VECTOR_SIZE; offset += kind1.getByteCount()) {
525                 emitFloatCompare(masm, array1, array2, result, -VECTOR_SIZE + offset, falseLabel, kind1.getByteCount() == VECTOR_SIZE);
526             }
527         } else {
528             masm.cmpqAndJcc(temp, new AMD64Address(array2, result, arrayIndexScale2, -VECTOR_SIZE), ConditionFlag.NotEqual, falseLabel, true);
529         }
530         masm.jmpb(trueLabel);
531 
532         masm.bind(compareTail);
533         masm.movl(length, result);
534     }
535 
536     /**
537      * Emits code to compare the remaining 1 to 4 bytes.
538      */
539     private void emitTailCompares(AMD64MacroAssembler masm,
540                     Register result, Register array1, Register array2, Register length, Label trueLabel, Label falseLabel) {
541         assert kind1 == kind2;
542         Label compare2Bytes = new Label();
543         Label compare1Byte = new Label();
544 
545         Register temp = asRegister(temp4);
546 
547         if (kind1.getByteCount() &lt;= 4) {
548             // Compare trailing 4 bytes, if any.
549             masm.testlAndJcc(result, arrayIndexScale1.log2 == 0 ? 4 : 4 &gt;&gt; arrayIndexScale1.log2, ConditionFlag.Zero, compare2Bytes, true);
550             masm.movl(temp, new AMD64Address(array1, 0));
551             if (kind1 == JavaKind.Float) {
552                 masm.cmplAndJcc(temp, new AMD64Address(array2, 0), ConditionFlag.Equal, trueLabel, true);
553                 emitFloatCompare(masm, array1, array2, Register.None, 0, falseLabel, true);
554                 masm.jmpb(trueLabel);
555             } else {
556                 masm.cmplAndJcc(temp, new AMD64Address(array2, 0), ConditionFlag.NotEqual, falseLabel, true);
557             }
558             if (kind1.getByteCount() &lt;= 2) {
559                 // Move array pointers forward.
560                 masm.leaq(array1, new AMD64Address(array1, 4));
561                 masm.leaq(array2, new AMD64Address(array2, 4));
562 
563                 // Compare trailing 2 bytes, if any.
564                 masm.bind(compare2Bytes);
565                 masm.testlAndJcc(result, arrayIndexScale1.log2 == 0 ? 2 : 2 &gt;&gt; arrayIndexScale1.log2, ConditionFlag.Zero, compare1Byte, true);
566                 masm.movzwl(temp, new AMD64Address(array1, 0));
567                 masm.movzwl(length, new AMD64Address(array2, 0));
568                 masm.cmplAndJcc(temp, length, ConditionFlag.NotEqual, falseLabel, true);
569 
570                 // The one-byte tail compare is only required for boolean and byte arrays.
571                 if (kind1.getByteCount() &lt;= 1) {
572                     // Move array pointers forward before we compare the last trailing byte.
573                     masm.leaq(array1, new AMD64Address(array1, 2));
574                     masm.leaq(array2, new AMD64Address(array2, 2));
575 
576                     // Compare trailing byte, if any.
577                     // TODO (yz) this can be optimized, i.e., bind after padding
578                     masm.bind(compare1Byte);
579                     masm.testlAndJcc(result, 1, ConditionFlag.Zero, trueLabel, true);
580                     masm.movzbl(temp, new AMD64Address(array1, 0));
581                     masm.movzbl(length, new AMD64Address(array2, 0));
582                     masm.cmplAndJcc(temp, length, ConditionFlag.NotEqual, falseLabel, true);
583                 } else {
584                     masm.bind(compare1Byte);
585                 }
586             } else {
587                 masm.bind(compare2Bytes);
588             }
589         }
590     }
591 
592     private void emitDifferentKindsElementWiseCompare(CompilationResultBuilder crb, AMD64MacroAssembler masm,
593                     Register result, Register array1, Register array2, Register length, Label trueLabel, Label falseLabel) {
594         assert kind1 != kind2;
595         assert kind1.isNumericInteger() &amp;&amp; kind2.isNumericInteger();
596         Label loop = new Label();
597         Label compareTail = new Label();
598 
599         int elementsPerLoopIteration = 4;
600 
601         Register tmp1 = asRegister(temp4);
602         Register tmp2 = asRegister(temp5);
603 
604         masm.andl(result, elementsPerLoopIteration - 1); // tail count
605         masm.andlAndJcc(length, ~(elementsPerLoopIteration - 1), ConditionFlag.Zero, compareTail, false);
606 
607         masm.leaq(array1, new AMD64Address(array1, length, arrayIndexScale1, 0));
608         masm.leaq(array2, new AMD64Address(array2, length, arrayIndexScale2, 0));
609         masm.negq(length);
610 
611         // clear comparison registers because of the missing movzlq instruction
612         masm.xorq(tmp1, tmp1);
613         masm.xorq(tmp2, tmp2);
614 
615         // Align the main loop
616         masm.align(crb.target.wordSize * 2);
617         masm.bind(loop);
618         for (int i = 0; i &lt; elementsPerLoopIteration; i++) {
619             emitMovBytes(masm, tmp1, new AMD64Address(array1, length, arrayIndexScale1, i &lt;&lt; arrayIndexScale1.log2), kind1.getByteCount());
620             emitMovBytes(masm, tmp2, new AMD64Address(array2, length, arrayIndexScale2, i &lt;&lt; arrayIndexScale2.log2), kind2.getByteCount());
621             masm.cmpqAndJcc(tmp1, tmp2, ConditionFlag.NotEqual, falseLabel, false);
622         }
623         masm.addqAndJcc(length, elementsPerLoopIteration, ConditionFlag.NotZero, loop, true);
624 
625         masm.bind(compareTail);
626         masm.testlAndJcc(result, result, ConditionFlag.Zero, trueLabel, false);
627         for (int i = 0; i &lt; elementsPerLoopIteration - 1; i++) {
628             emitMovBytes(masm, tmp1, new AMD64Address(array1, length, arrayIndexScale1, 0), kind1.getByteCount());
629             emitMovBytes(masm, tmp2, new AMD64Address(array2, length, arrayIndexScale2, 0), kind2.getByteCount());
630             masm.cmpqAndJcc(tmp1, tmp2, ConditionFlag.NotEqual, falseLabel, false);
631             if (i &lt; elementsPerLoopIteration - 2) {
632                 masm.incrementq(length, 1);
633                 masm.decqAndJcc(result, ConditionFlag.Zero, trueLabel, false);
634             } else {
635                 masm.jmpb(trueLabel);
636             }
637         }
638     }
639 
640     /**
641      * Emits code to fall through if {@code src} is NaN, otherwise jump to {@code branchOrdered}.
642      */
643     private void emitNaNCheck(AMD64MacroAssembler masm, AMD64Address src, Label branchIfNonNaN) {
644         assert kind1.isNumericFloat();
645         Register tempXMMReg = asRegister(tempXMM);
646         if (kind1 == JavaKind.Float) {
647             masm.movflt(tempXMMReg, src);
648         } else {
649             masm.movdbl(tempXMMReg, src);
650         }
651         SSEOp.UCOMIS.emit(masm, kind1 == JavaKind.Float ? OperandSize.PS : OperandSize.PD, tempXMMReg, tempXMMReg);
652         masm.jcc(ConditionFlag.NoParity, branchIfNonNaN);
653     }
654 
655     /**
656      * Emits code to compare if two floats are bitwise equal or both NaN.
657      */
658     private void emitFloatCompare(AMD64MacroAssembler masm, Register base1, Register base2, Register index, int offset, Label falseLabel,
659                     boolean skipBitwiseCompare) {
660         AMD64Address address1 = new AMD64Address(base1, index, arrayIndexScale1, offset);
661         AMD64Address address2 = new AMD64Address(base2, index, arrayIndexScale2, offset);
662 
663         Label bitwiseEqual = new Label();
664 
665         if (!skipBitwiseCompare) {
666             // Bitwise compare
667             Register temp = asRegister(temp4);
668 
669             if (kind1 == JavaKind.Float) {
670                 masm.movl(temp, address1);
671                 masm.cmplAndJcc(temp, address2, ConditionFlag.Equal, bitwiseEqual, true);
672             } else {
673                 masm.movq(temp, address1);
674                 masm.cmpqAndJcc(temp, address2, ConditionFlag.Equal, bitwiseEqual, true);
675             }
676         }
677 
678         emitNaNCheck(masm, address1, falseLabel);
679         emitNaNCheck(masm, address2, falseLabel);
680 
681         masm.bind(bitwiseEqual);
682     }
683 
684     /**
685      * Emits code to compare float equality within a range.
686      */
687     private void emitFloatCompareWithinRange(CompilationResultBuilder crb, AMD64MacroAssembler masm,
688                     Register base1, Register base2, Register index, int offset, Label falseLabel, int range) {
689         assert kind1.isNumericFloat();
690         Label loop = new Label();
691         Register i = asRegister(temp5);
692 
693         masm.movq(i, range);
694         masm.negq(i);
695         // Align the main loop
696         masm.align(crb.target.wordSize * 2);
697         masm.bind(loop);
698         emitFloatCompare(masm, base1, base2, index, offset, falseLabel, range == 1);
699         masm.incrementq(index, 1);
700         masm.incqAndJcc(i, ConditionFlag.NotZero, loop, true);
701         // Floats within the range are equal, revert change to the register index
702         masm.subq(index, range);
703     }
704 
705     private boolean constantLengthCompareNeedsTmpArrayPointers() {
706         AVXKind.AVXSize vSize = vectorSize;
707         if (constantLength() &lt; getElementsPerVector(vectorSize)) {
708             vSize = AVXKind.AVXSize.XMM;
709         }
710         int vectorCount = constantLength() &amp; ~(2 * getElementsPerVector(vSize) - 1);
711         return vectorCount &gt; 0;
712     }
713 
714     /**
715      * Emits specialized assembly for checking equality of memory regions
716      * {@code arrayPtr1[0..nBytes]} and {@code arrayPtr2[0..nBytes]}. If they match, execution
717      * continues directly after the emitted code block, otherwise we jump to {@code noMatch}.
718      */
719     private void emitConstantLengthArrayCompareBytes(
720                     CompilationResultBuilder crb,
721                     AMD64MacroAssembler asm,
722                     Register[] tmpVectors,
723                     Label noMatch) {
724         if (constantLength() == 0) {
725             // do nothing
726             return;
727         }
728         Register arrayPtr1 = asRegister(array1Value);
729         Register arrayPtr2 = asRegister(array2Value);
730         Register tmp = asRegister(temp3);
731         AVXKind.AVXSize vSize = vectorSize;
732         if (constantLength() &lt; getElementsPerVector(vectorSize)) {
733             vSize = AVXKind.AVXSize.XMM;
734         }
735         int elementsPerVector = getElementsPerVector(vSize);
736         if (elementsPerVector &gt; constantLength()) {
737             assert kind1 == kind2;
738             int byteLength = constantLength() &lt;&lt; arrayIndexScale1.log2;
739             // array is shorter than any vector register, use regular XOR instructions
740             int movSize = (byteLength &lt; 2) ? 1 : ((byteLength &lt; 4) ? 2 : ((byteLength &lt; 8) ? 4 : 8));
741             emitMovBytes(asm, tmp, new AMD64Address(arrayPtr1, arrayBaseOffset1), movSize);
742             emitXorBytes(asm, tmp, new AMD64Address(arrayPtr2, arrayBaseOffset2), movSize);
743             asm.jccb(AMD64Assembler.ConditionFlag.NotZero, noMatch);
744             if (byteLength &gt; movSize) {
745                 emitMovBytes(asm, tmp, new AMD64Address(arrayPtr1, arrayBaseOffset1 + byteLength - movSize), movSize);
746                 emitXorBytes(asm, tmp, new AMD64Address(arrayPtr2, arrayBaseOffset2 + byteLength - movSize), movSize);
747                 asm.jccb(AMD64Assembler.ConditionFlag.NotZero, noMatch);
748             }
749         } else {
750             int elementsPerVectorLoop = 2 * elementsPerVector;
751             int tailCount = constantLength() &amp; (elementsPerVectorLoop - 1);
752             int vectorCount = constantLength() &amp; ~(elementsPerVectorLoop - 1);
753             int bytesPerVector = vSize.getBytes();
754             if (vectorCount &gt; 0) {
755                 Label loopBegin = new Label();
756                 Register tmpArrayPtr1 = asRegister(temp1);
757                 Register tmpArrayPtr2 = asRegister(temp2);
758                 asm.leaq(tmpArrayPtr1, new AMD64Address(arrayPtr1, vectorCount &lt;&lt; arrayIndexScale1.log2));
759                 asm.leaq(tmpArrayPtr2, new AMD64Address(arrayPtr2, vectorCount &lt;&lt; arrayIndexScale2.log2));
760                 arrayPtr1 = tmpArrayPtr1;
761                 arrayPtr2 = tmpArrayPtr2;
762                 asm.movq(tmp, -vectorCount);
763                 asm.align(crb.target.wordSize * 2);
764                 asm.bind(loopBegin);
765                 emitVectorLoad1(asm, tmpVectors[0], arrayPtr1, tmp, arrayBaseOffset1, vSize);
766                 emitVectorLoad2(asm, tmpVectors[1], arrayPtr2, tmp, arrayBaseOffset2, vSize);
767                 emitVectorLoad1(asm, tmpVectors[2], arrayPtr1, tmp, arrayBaseOffset1 + scaleDisplacement1(bytesPerVector), vSize);
768                 emitVectorLoad2(asm, tmpVectors[3], arrayPtr2, tmp, arrayBaseOffset2 + scaleDisplacement2(bytesPerVector), vSize);
769                 emitVectorXor(asm, tmpVectors[0], tmpVectors[1], vSize);
770                 emitVectorXor(asm, tmpVectors[2], tmpVectors[3], vSize);
771                 emitVectorTest(asm, tmpVectors[0], vSize);
772                 asm.jccb(AMD64Assembler.ConditionFlag.NotZero, noMatch);
773                 emitVectorTest(asm, tmpVectors[2], vSize);
774                 asm.jccb(AMD64Assembler.ConditionFlag.NotZero, noMatch);
775                 asm.addqAndJcc(tmp, elementsPerVectorLoop, AMD64Assembler.ConditionFlag.NotZero, loopBegin, true);
776             }
777             if (tailCount &gt; 0) {
778                 emitVectorLoad1(asm, tmpVectors[0], arrayPtr1, arrayBaseOffset1 + (tailCount &lt;&lt; arrayIndexScale1.log2) - scaleDisplacement1(bytesPerVector), vSize);
779                 emitVectorLoad2(asm, tmpVectors[1], arrayPtr2, arrayBaseOffset2 + (tailCount &lt;&lt; arrayIndexScale2.log2) - scaleDisplacement2(bytesPerVector), vSize);
780                 emitVectorXor(asm, tmpVectors[0], tmpVectors[1], vSize);
781                 if (tailCount &gt; elementsPerVector) {
782                     emitVectorLoad1(asm, tmpVectors[2], arrayPtr1, arrayBaseOffset1, vSize);
783                     emitVectorLoad2(asm, tmpVectors[3], arrayPtr2, arrayBaseOffset2, vSize);
784                     emitVectorXor(asm, tmpVectors[2], tmpVectors[3], vSize);
785                     emitVectorTest(asm, tmpVectors[2], vSize);
786                     asm.jccb(AMD64Assembler.ConditionFlag.NotZero, noMatch);
787                 }
788                 emitVectorTest(asm, tmpVectors[0], vSize);
789                 asm.jccb(AMD64Assembler.ConditionFlag.NotZero, noMatch);
790             }
791         }
792     }
793 
794     private void emitMovBytes(AMD64MacroAssembler asm, Register dst, AMD64Address src, int size) {
795         switch (size) {
796             case 1:
797                 if (signExtend) {
798                     asm.movsbq(dst, src);
799                 } else {
800                     asm.movzbq(dst, src);
801                 }
802                 break;
803             case 2:
804                 if (signExtend) {
805                     asm.movswq(dst, src);
806                 } else {
807                     asm.movzwq(dst, src);
808                 }
809                 break;
810             case 4:
811                 if (signExtend) {
812                     asm.movslq(dst, src);
813                 } else {
814                     // there is no movzlq
815                     asm.movl(dst, src);
816                 }
817                 break;
818             case 8:
819                 asm.movq(dst, src);
820                 break;
821             default:
822                 throw new IllegalStateException();
823         }
824     }
825 
826     private static void emitXorBytes(AMD64MacroAssembler asm, Register dst, AMD64Address src, int size) {
827         OperandSize opSize = getOperandSize(size);
828         XOR.getRMOpcode(opSize).emit(asm, opSize, dst, src);
829     }
830 
831     private static OperandSize getOperandSize(int size) {
832         switch (size) {
833             case 1:
834                 return OperandSize.BYTE;
835             case 2:
836                 return OperandSize.WORD;
837             case 4:
838                 return OperandSize.DWORD;
839             case 8:
840                 return OperandSize.QWORD;
841             default:
842                 throw new IllegalStateException();
843         }
844     }
845 
846     @Override
847     public boolean needsClearUpperVectorRegisters() {
848         return true;
849     }
850 }
    </pre>
  </body>
</html>