diff a/src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.amd64/src/org/graalvm/compiler/lir/amd64/AMD64ArrayIndexOfOp.java b/src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.amd64/src/org/graalvm/compiler/lir/amd64/AMD64ArrayIndexOfOp.java
--- a/src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.amd64/src/org/graalvm/compiler/lir/amd64/AMD64ArrayIndexOfOp.java
+++ b/src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.amd64/src/org/graalvm/compiler/lir/amd64/AMD64ArrayIndexOfOp.java
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 2018, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2018, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -37,10 +37,11 @@
 import org.graalvm.compiler.asm.Label;
 import org.graalvm.compiler.asm.amd64.AMD64Address;
 import org.graalvm.compiler.asm.amd64.AMD64Address.Scale;
 import org.graalvm.compiler.asm.amd64.AMD64Assembler;
 import org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64RMOp;
+import org.graalvm.compiler.asm.amd64.AMD64Assembler.ConditionFlag;
 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexMoveOp;
 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexRMIOp;
 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexRMOp;
 import org.graalvm.compiler.asm.amd64.AMD64Assembler.VexRVMOp;
 import org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize;
@@ -199,20 +200,18 @@
         // important: this must be the first register manipulation, since fromIndex is
         // annotated with @Use
         asm.leaq(index, new AMD64Address(fromIndex, vectorSize + (findTwoConsecutive ? 1 : 0)));
 
         // check if vector vector load is in bounds
-        asm.cmpq(index, arrayLength);
-        asm.jccb(AMD64Assembler.ConditionFlag.LessEqual, runVectorized);
+        asm.cmpqAndJcc(index, arrayLength, ConditionFlag.LessEqual, runVectorized, true);
 
         // search range is smaller than vector size, do element-wise comparison
 
         // index = fromIndex (+ 1 if findTwoConsecutive)
         asm.subq(index, vectorSize);
         // check if enough array slots remain
-        asm.cmpq(index, arrayLength);
-        asm.jccb(AMD64Assembler.ConditionFlag.GreaterEqual, elementWiseNotFound);
+        asm.cmpqAndJcc(index, arrayLength, ConditionFlag.GreaterEqual, elementWiseNotFound, true);
         // compare one-by-one
         asm.bind(elementWiseLoop);
         // check for match
         OperandSize cmpSize = getOpSize(getComparisonKind());
         // address = findTwoConsecutive ? array[index - 1] : array[index]
@@ -227,28 +226,29 @@
                 } else if (isStackSlot(searchValue[i])) {
                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getRMOpcode(cmpSize).emit(asm, cmpSize, cmpResult[0], (AMD64Address) crb.asAddress(searchValue[i]));
                 } else {
                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getRMOpcode(cmpSize).emit(asm, cmpSize, cmpResult[0], asRegister(searchValue[i]));
                 }
+                // TODO (yz) the preceding cmp instruction may be fused with the following jcc
                 asm.jccb(AMD64Assembler.ConditionFlag.Equal, elementWiseFound);
             }
         } else {
             for (int i = 0; i < nValues; i++) {
                 if (isConstant(searchValue[i])) {
                     int imm = asConstant(searchValue[i]).asInt();
                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getMIOpcode(cmpSize, NumUtil.isByte(imm)).emit(asm, cmpSize, arrayAddr, imm);
                 } else {
                     AMD64Assembler.AMD64BinaryArithmetic.CMP.getRMOpcode(cmpSize).emit(asm, cmpSize, asRegister(searchValue[i]), arrayAddr);
                 }
+                // TODO (yz) the preceding cmp instruction may be fused with the following jcc
                 asm.jccb(AMD64Assembler.ConditionFlag.Equal, elementWiseFound);
             }
         }
         // adjust index
         asm.incrementq(index, 1);
         // continue loop
-        asm.cmpq(index, arrayLength);
-        asm.jccb(AMD64Assembler.ConditionFlag.Less, elementWiseLoop);
+        asm.cmpqAndJcc(index, arrayLength, ConditionFlag.Less, elementWiseLoop, true);
 
         asm.bind(elementWiseNotFound);
         asm.xorq(index, index);
 
         if (findTwoConsecutive) {
@@ -283,22 +283,20 @@
         asm.subq(index, cmpResult[0]);
         // add bulk size
         asm.addq(index, bulkSize);
 
         // check if there are enough array slots remaining for the bulk loop
-        asm.cmpq(index, arrayLength);
-        asm.jccb(AMD64Assembler.ConditionFlag.Greater, skipBulkVectorLoop);
+        asm.cmpqAndJcc(index, arrayLength, ConditionFlag.Greater, skipBulkVectorLoop, true);
 
         emitAlign(crb, asm);
         asm.bind(bulkVectorLoop);
         // memory-aligned bulk comparison
         emitVectorCompare(asm, vectorCompareKind, nVectors, arrayPtr, index, vecCmp, vecArray, cmpResult, vectorFound, false, !findTwoConsecutive);
         // adjust index
         asm.addq(index, bulkSize);
         // check if there are enough array slots remaining for the bulk loop
-        asm.cmpq(index, arrayLength);
-        asm.jccb(AMD64Assembler.ConditionFlag.LessEqual, bulkVectorLoop);
+        asm.cmpqAndJcc(index, arrayLength, ConditionFlag.LessEqual, bulkVectorLoop, true);
 
         asm.bind(skipBulkVectorLoop);
         if ((findTwoConsecutive && nVectors == 2) || nVectors == 1) {
             // do last load from end of array
             asm.movq(index, arrayLength);
@@ -317,12 +315,11 @@
             // if load would be over bounds, set the load to the end of the array
             asm.cmovq(AMD64Assembler.ConditionFlag.Greater, index, arrayLength);
             // compare
             emitVectorCompare(asm, vectorCompareKind, findTwoConsecutive ? 2 : 1, arrayPtr, index, vecCmp, vecArray, cmpResult, vectorFound, true, false);
             // check if there are enough array slots remaining for the loop
-            asm.cmpq(index, arrayLength);
-            asm.jccb(AMD64Assembler.ConditionFlag.Less, singleVectorLoop);
+            asm.cmpqAndJcc(index, arrayLength, ConditionFlag.Less, singleVectorLoop, true);
         }
 
         asm.movl(index, -1);
         asm.jmpb(ret);
 
@@ -366,12 +363,11 @@
 
             asm.bind(minResult);
             // find offset 0
             asm.bsfq(cmpResult[1], cmpResult[1]);
             // check if second result is also a match
-            asm.testq(cmpResult[0], cmpResult[0]);
-            asm.jccb(AMD64Assembler.ConditionFlag.Zero, minResultDone);
+            asm.testqAndJcc(cmpResult[0], cmpResult[0], ConditionFlag.Zero, minResultDone, true);
             // find offset 1
             asm.bsfq(cmpResult[0], cmpResult[0]);
             asm.addq(cmpResult[0], valueKind.getByteCount());
             // if first result is greater than second, replace it with the second result
             asm.cmpq(cmpResult[1], cmpResult[0]);
@@ -536,33 +532,24 @@
                 }
                 if (nValues > 2) {
                     emitPOR(asm, getVectorSize(), vecArray[base], vecArray[base + 2]);
                 }
                 emitMOVMSK(asm, getVectorSize(), cmpResult[0], vecArray[base]);
-                emitJnz(asm, cmpResult[0], vectorFound[nVectors - (i + 1)], shortJmp);
+                asm.testlAndJcc(cmpResult[0], cmpResult[0], ConditionFlag.NotZero, vectorFound[nVectors - (i + 1)], shortJmp);
             }
         } else {
             for (int i = 0; i < nVectors; i += 2) {
                 emitVectorCompareInst(asm, kind, getVectorSize(), vecArray[i], vecCmp[0]);
                 emitVectorCompareInst(asm, kind, getVectorSize(), vecArray[i + 1], vecCmp[0]);
                 emitMOVMSK(asm, getVectorSize(), cmpResult[1], vecArray[i]);
                 emitMOVMSK(asm, getVectorSize(), cmpResult[0], vecArray[i + 1]);
-                emitJnz(asm, cmpResult[1], vectorFound[nVectors - (i + 1)], shortJmp);
-                emitJnz(asm, cmpResult[0], vectorFound[nVectors - (i + 2)], shortJmp);
+                asm.testlAndJcc(cmpResult[1], cmpResult[1], ConditionFlag.NotZero, vectorFound[nVectors - (i + 1)], shortJmp);
+                asm.testlAndJcc(cmpResult[0], cmpResult[0], ConditionFlag.NotZero, vectorFound[nVectors - (i + 2)], shortJmp);
             }
         }
     }
 
-    private static void emitJnz(AMD64MacroAssembler asm, Register cond, Label tgt, boolean shortJmp) {
-        asm.testl(cond, cond);
-        if (shortJmp) {
-            asm.jccb(AMD64Assembler.ConditionFlag.NotZero, tgt);
-        } else {
-            asm.jcc(AMD64Assembler.ConditionFlag.NotZero, tgt);
-        }
-    }
-
     private void emitArrayLoad(AMD64MacroAssembler asm, AVXKind.AVXSize vectorSize, Register vecDst, Register arrayPtr, Register index, int offset, boolean alignedLoad) {
         AMD64Address src = new AMD64Address(arrayPtr, index, arrayIndexScale, offset);
         if (asm.supports(CPUFeature.AVX)) {
             VexMoveOp loadOp = alignedLoad ? VexMoveOp.VMOVDQA32 : VexMoveOp.VMOVDQU32;
             loadOp.emit(asm, vectorSize, vecDst, src);
