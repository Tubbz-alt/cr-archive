<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.lir.amd64/src/org/graalvm/compiler/lir/amd64/AMD64ControlFlow.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2011, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  */
  23 
  24 
  25 package org.graalvm.compiler.lir.amd64;
  26 
  27 import static jdk.vm.ci.code.ValueUtil.asRegister;
  28 import static jdk.vm.ci.code.ValueUtil.isRegister;
  29 import static jdk.vm.ci.code.ValueUtil.isStackSlot;
  30 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.DWORD;
  31 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.QWORD;
  32 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.WORD;
  33 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.COMPOSITE;
  34 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.CONST;
  35 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.HINT;
  36 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.ILLEGAL;
  37 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.REG;
  38 import static org.graalvm.compiler.lir.LIRInstruction.OperandFlag.STACK;
  39 
  40 import java.util.function.IntConsumer;
  41 import java.util.function.Supplier;
  42 
  43 import org.graalvm.compiler.asm.Label;
  44 import org.graalvm.compiler.asm.amd64.AMD64Address;
  45 import org.graalvm.compiler.asm.amd64.AMD64Address.Scale;
  46 import org.graalvm.compiler.asm.amd64.AMD64Assembler.ConditionFlag;
  47 import org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize;
  48 import org.graalvm.compiler.asm.amd64.AMD64MacroAssembler;
  49 import org.graalvm.compiler.code.CompilationResult.JumpTable;
  50 import org.graalvm.compiler.core.common.NumUtil;
  51 import org.graalvm.compiler.core.common.calc.Condition;
  52 import org.graalvm.compiler.debug.GraalError;
  53 import org.graalvm.compiler.lir.LIRFrameState;
  54 import org.graalvm.compiler.lir.LIRInstructionClass;
  55 import org.graalvm.compiler.lir.LabelRef;
  56 import org.graalvm.compiler.lir.Opcode;
  57 import org.graalvm.compiler.lir.StandardOp;
  58 import org.graalvm.compiler.lir.StandardOp.BlockEndOp;
  59 import org.graalvm.compiler.lir.StandardOp.ImplicitNullCheck;
  60 import org.graalvm.compiler.lir.SwitchStrategy;
  61 import org.graalvm.compiler.lir.SwitchStrategy.BaseSwitchClosure;
  62 import org.graalvm.compiler.lir.Variable;
  63 import org.graalvm.compiler.lir.asm.CompilationResultBuilder;
  64 
  65 import jdk.vm.ci.amd64.AMD64;
  66 import jdk.vm.ci.amd64.AMD64.CPUFeature;
  67 import jdk.vm.ci.amd64.AMD64Kind;
  68 import jdk.vm.ci.code.Register;
  69 import jdk.vm.ci.meta.AllocatableValue;
  70 import jdk.vm.ci.meta.Constant;
  71 import jdk.vm.ci.meta.JavaConstant;
  72 import jdk.vm.ci.meta.VMConstant;
  73 import jdk.vm.ci.meta.Value;
  74 
  75 public class AMD64ControlFlow {
  76 
  77     public static final class ReturnOp extends AMD64BlockEndOp implements BlockEndOp {
  78         public static final LIRInstructionClass&lt;ReturnOp&gt; TYPE = LIRInstructionClass.create(ReturnOp.class);
  79         @Use({REG, ILLEGAL}) protected Value x;
  80 
  81         public ReturnOp(Value x) {
  82             super(TYPE);
  83             this.x = x;
  84         }
  85 
  86         @Override
  87         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
  88             crb.frameContext.leave(crb);
  89             /*
  90              * We potentially return to the interpreter, and that&#39;s an AVX-SSE transition. The only
  91              * live value at this point should be the return value in either rax, or in xmm0 with
  92              * the upper half of the register unused, so we don&#39;t destroy any value here.
  93              */
  94             if (masm.supports(CPUFeature.AVX)) {
  95                 masm.vzeroupper();
  96             }
  97             masm.ret(0);
  98         }
  99     }
 100 
 101     public static class BranchOp extends AMD64BlockEndOp implements StandardOp.BranchOp {
 102         public static final LIRInstructionClass&lt;BranchOp&gt; TYPE = LIRInstructionClass.create(BranchOp.class);
 103         protected final ConditionFlag condition;
 104         protected final LabelRef trueDestination;
 105         protected final LabelRef falseDestination;
 106 
 107         protected final double trueDestinationProbability;
 108 
 109         public BranchOp(Condition condition, LabelRef trueDestination, LabelRef falseDestination, double trueDestinationProbability) {
 110             this(intCond(condition), trueDestination, falseDestination, trueDestinationProbability);
 111         }
 112 
 113         public BranchOp(ConditionFlag condition, LabelRef trueDestination, LabelRef falseDestination, double trueDestinationProbability) {
 114             this(TYPE, condition, trueDestination, falseDestination, trueDestinationProbability);
 115         }
 116 
 117         protected BranchOp(LIRInstructionClass&lt;? extends BranchOp&gt; c, ConditionFlag condition, LabelRef trueDestination, LabelRef falseDestination, double trueDestinationProbability) {
 118             super(c);
 119             this.condition = condition;
 120             this.trueDestination = trueDestination;
 121             this.falseDestination = falseDestination;
 122             this.trueDestinationProbability = trueDestinationProbability;
 123         }
 124 
 125         @Override
 126         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 127             /*
 128              * The strategy for emitting jumps is: If either trueDestination or falseDestination is
 129              * the successor block, assume the block scheduler did the correct thing and jcc to the
 130              * other. Otherwise, we need a jcc followed by a jmp. Use the branch probability to make
 131              * sure it is more likely to branch on the jcc (= less likely to execute both the jcc
 132              * and the jmp instead of just the jcc). In the case of loops, that means the jcc is the
 133              * back-edge.
 134              */
 135             if (crb.isSuccessorEdge(trueDestination)) {
 136                 jcc(masm, true, falseDestination);
 137             } else if (crb.isSuccessorEdge(falseDestination)) {
 138                 jcc(masm, false, trueDestination);
 139             } else if (trueDestinationProbability &lt; 0.5) {
 140                 jcc(masm, true, falseDestination);
 141                 masm.jmp(trueDestination.label());
 142             } else {
 143                 jcc(masm, false, trueDestination);
 144                 masm.jmp(falseDestination.label());
 145             }
 146         }
 147 
 148         protected void jcc(AMD64MacroAssembler masm, boolean negate, LabelRef target) {
 149             masm.jcc(negate ? condition.negate() : condition, target.label());
 150         }
 151     }
 152 
 153     public static class TestByteBranchOp extends BranchOp {
 154 
 155         public static final LIRInstructionClass&lt;TestByteBranchOp&gt; TYPE = LIRInstructionClass.create(TestByteBranchOp.class);
 156 
 157         @Use({REG}) protected AllocatableValue x;
 158         @Use({REG, STACK}) protected AllocatableValue y;
 159 
 160         public TestByteBranchOp(AllocatableValue x, AllocatableValue y, Condition cond, LabelRef trueDestination, LabelRef falseDestination, double trueDestinationProbability) {
 161             super(TYPE, intCond(cond), trueDestination, falseDestination, trueDestinationProbability);
 162 
 163             this.x = x;
 164             this.y = y;
 165         }
 166 
 167         @Override
 168         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 169             if (isRegister(y)) {
 170                 if (crb.isSuccessorEdge(trueDestination)) {
 171                     masm.testbAndJcc(asRegister(x), asRegister(y), condition.negate(), falseDestination.label(), false);
 172                 } else if (crb.isSuccessorEdge(falseDestination)) {
 173                     masm.testbAndJcc(asRegister(x), asRegister(y), condition, trueDestination.label(), false);
 174                 } else if (trueDestinationProbability &lt; 0.5) {
 175                     masm.testbAndJcc(asRegister(x), asRegister(y), condition.negate(), falseDestination.label(), false);
 176                     masm.jmp(trueDestination.label());
 177                 } else {
 178                     masm.testbAndJcc(asRegister(x), asRegister(y), condition, trueDestination.label(), false);
 179                     masm.jmp(falseDestination.label());
 180                 }
 181             } else {
 182                 assert isStackSlot(y);
 183                 if (crb.isSuccessorEdge(trueDestination)) {
 184                     masm.testbAndJcc(asRegister(x), (AMD64Address) crb.asAddress(y), condition.negate(), falseDestination.label(), false);
 185                 } else if (crb.isSuccessorEdge(falseDestination)) {
 186                     masm.testbAndJcc(asRegister(x), (AMD64Address) crb.asAddress(y), condition, trueDestination.label(), false);
 187                 } else if (trueDestinationProbability &lt; 0.5) {
 188                     masm.testbAndJcc(asRegister(x), (AMD64Address) crb.asAddress(y), condition.negate(), falseDestination.label(), false);
 189                     masm.jmp(trueDestination.label());
 190                 } else {
 191                     masm.testbAndJcc(asRegister(x), (AMD64Address) crb.asAddress(y), condition, trueDestination.label(), false);
 192                     masm.jmp(falseDestination.label());
 193                 }
 194             }
 195         }
 196     }
 197 
 198     public static class TestBranchOp extends BranchOp implements ImplicitNullCheck {
 199 
 200         public static final LIRInstructionClass&lt;TestBranchOp&gt; TYPE = LIRInstructionClass.create(TestBranchOp.class);
 201 
 202         private final OperandSize size;
 203 
 204         @Use({REG}) protected AllocatableValue x;
 205         @Use({REG, STACK, COMPOSITE}) protected Value y;
 206 
 207         @State protected LIRFrameState state;
 208 
 209         public TestBranchOp(OperandSize size, AllocatableValue x, Value y, LIRFrameState state, Condition cond, LabelRef trueDestination, LabelRef falseDestination,
 210                         double trueDestinationProbability) {
 211             super(TYPE, intCond(cond), trueDestination, falseDestination, trueDestinationProbability);
 212             assert size == WORD || size == DWORD || size == QWORD;
 213             this.size = size;
 214 
 215             this.x = x;
 216             this.y = y;
 217 
 218             this.state = state;
 219         }
 220 
 221         @Override
 222         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 223             if (isRegister(y)) {
 224                 assert state == null;
 225                 if (crb.isSuccessorEdge(trueDestination)) {
 226                     masm.testAndJcc(size, asRegister(x), asRegister(y), condition.negate(), falseDestination.label(), false);
 227                 } else if (crb.isSuccessorEdge(falseDestination)) {
 228                     masm.testAndJcc(size, asRegister(x), asRegister(y), condition, trueDestination.label(), false);
 229                 } else if (trueDestinationProbability &lt; 0.5) {
 230                     masm.testAndJcc(size, asRegister(x), asRegister(y), condition.negate(), falseDestination.label(), false);
 231                     masm.jmp(trueDestination.label());
 232                 } else {
 233                     masm.testAndJcc(size, asRegister(x), asRegister(y), condition, trueDestination.label(), false);
 234                     masm.jmp(falseDestination.label());
 235                 }
 236                 return;
 237             }
 238             IntConsumer applyBeforeFusedPair = state == null ? null : pos -&gt; crb.recordImplicitException(pos, state);
 239             if (isStackSlot(y)) {
 240                 if (crb.isSuccessorEdge(trueDestination)) {
 241                     masm.testAndJcc(size, asRegister(x), (AMD64Address) crb.asAddress(y), condition.negate(), falseDestination.label(), false, applyBeforeFusedPair);
 242                 } else if (crb.isSuccessorEdge(falseDestination)) {
 243                     masm.testAndJcc(size, asRegister(x), (AMD64Address) crb.asAddress(y), condition, trueDestination.label(), false, applyBeforeFusedPair);
 244                 } else if (trueDestinationProbability &lt; 0.5) {
 245                     masm.testAndJcc(size, asRegister(x), (AMD64Address) crb.asAddress(y), condition.negate(), falseDestination.label(), false, applyBeforeFusedPair);
 246                     masm.jmp(trueDestination.label());
 247                 } else {
 248                     masm.testAndJcc(size, asRegister(x), (AMD64Address) crb.asAddress(y), condition, trueDestination.label(), false, applyBeforeFusedPair);
 249                     masm.jmp(falseDestination.label());
 250                 }
 251             } else {
 252                 AMD64AddressValue yAddress = (AMD64AddressValue) y;
 253                 if (crb.isSuccessorEdge(trueDestination)) {
 254                     masm.testAndJcc(size, asRegister(x), yAddress.toAddress(), condition.negate(), falseDestination.label(), false, applyBeforeFusedPair);
 255                 } else if (crb.isSuccessorEdge(falseDestination)) {
 256                     masm.testAndJcc(size, asRegister(x), yAddress.toAddress(), condition, trueDestination.label(), false, applyBeforeFusedPair);
 257                 } else if (trueDestinationProbability &lt; 0.5) {
 258                     masm.testAndJcc(size, asRegister(x), yAddress.toAddress(), condition.negate(), falseDestination.label(), false, applyBeforeFusedPair);
 259                     masm.jmp(trueDestination.label());
 260                 } else {
 261                     masm.testAndJcc(size, asRegister(x), yAddress.toAddress(), condition, trueDestination.label(), false, applyBeforeFusedPair);
 262                     masm.jmp(falseDestination.label());
 263                 }
 264             }
 265         }
 266 
 267         @Override
 268         public boolean makeNullCheckFor(Value value, LIRFrameState nullCheckState, int implicitNullCheckLimit) {
 269             if (state == null &amp;&amp; y instanceof AMD64AddressValue &amp;&amp; ((AMD64AddressValue) y).isValidImplicitNullCheckFor(value, implicitNullCheckLimit)) {
 270                 state = nullCheckState;
 271                 return true;
 272             }
 273             return false;
 274         }
 275     }
 276 
 277     public static class TestConstBranchOp extends BranchOp implements ImplicitNullCheck {
 278 
 279         public static final LIRInstructionClass&lt;TestConstBranchOp&gt; TYPE = LIRInstructionClass.create(TestConstBranchOp.class);
 280 
 281         private final OperandSize size;
 282 
 283         @Use({REG, STACK, COMPOSITE}) protected Value x;
 284         private final int y;
 285 
 286         @State protected LIRFrameState state;
 287 
 288         public TestConstBranchOp(OperandSize size, Value x, int y, LIRFrameState state, Condition cond, LabelRef trueDestination, LabelRef falseDestination, double trueDestinationProbability) {
 289             super(TYPE, intCond(cond), trueDestination, falseDestination, trueDestinationProbability);
 290             assert size == DWORD || size == QWORD;
 291             this.size = size;
 292 
 293             this.x = x;
 294             this.y = y;
 295 
 296             this.state = state;
 297         }
 298 
 299         @Override
 300         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 301             if (isRegister(x)) {
 302                 assert state == null;
 303                 if (crb.isSuccessorEdge(trueDestination)) {
 304                     masm.testAndJcc(size, asRegister(x), y, condition.negate(), falseDestination.label(), false);
 305                 } else if (crb.isSuccessorEdge(falseDestination)) {
 306                     masm.testAndJcc(size, asRegister(x), y, condition, trueDestination.label(), false);
 307                 } else if (trueDestinationProbability &lt; 0.5) {
 308                     masm.testAndJcc(size, asRegister(x), y, condition.negate(), falseDestination.label(), false);
 309                     masm.jmp(trueDestination.label());
 310                 } else {
 311                     masm.testAndJcc(size, asRegister(x), y, condition, trueDestination.label(), false);
 312                     masm.jmp(falseDestination.label());
 313                 }
 314                 return;
 315             }
 316             IntConsumer applyBeforeFusedPair = state == null ? null : pos -&gt; crb.recordImplicitException(pos, state);
 317             if (isStackSlot(x)) {
 318                 if (crb.isSuccessorEdge(trueDestination)) {
 319                     masm.testAndJcc(size, (AMD64Address) crb.asAddress(x), y, condition.negate(), falseDestination.label(), false, applyBeforeFusedPair);
 320                 } else if (crb.isSuccessorEdge(falseDestination)) {
 321                     masm.testAndJcc(size, (AMD64Address) crb.asAddress(x), y, condition, trueDestination.label(), false, applyBeforeFusedPair);
 322                 } else if (trueDestinationProbability &lt; 0.5) {
 323                     masm.testAndJcc(size, (AMD64Address) crb.asAddress(x), y, condition.negate(), falseDestination.label(), false, applyBeforeFusedPair);
 324                     masm.jmp(trueDestination.label());
 325                 } else {
 326                     masm.testAndJcc(size, (AMD64Address) crb.asAddress(x), y, condition, trueDestination.label(), false, applyBeforeFusedPair);
 327                     masm.jmp(falseDestination.label());
 328                 }
 329             } else {
 330                 AMD64AddressValue xAddress = (AMD64AddressValue) x;
 331                 if (crb.isSuccessorEdge(trueDestination)) {
 332                     masm.testAndJcc(size, xAddress.toAddress(), y, condition.negate(), falseDestination.label(), false, applyBeforeFusedPair);
 333                 } else if (crb.isSuccessorEdge(falseDestination)) {
 334                     masm.testAndJcc(size, xAddress.toAddress(), y, condition, trueDestination.label(), false, applyBeforeFusedPair);
 335                 } else if (trueDestinationProbability &lt; 0.5) {
 336                     masm.testAndJcc(size, xAddress.toAddress(), y, condition.negate(), falseDestination.label(), false, applyBeforeFusedPair);
 337                     masm.jmp(trueDestination.label());
 338                 } else {
 339                     masm.testAndJcc(size, xAddress.toAddress(), y, condition, trueDestination.label(), false, applyBeforeFusedPair);
 340                     masm.jmp(falseDestination.label());
 341                 }
 342             }
 343         }
 344 
 345         @Override
 346         public boolean makeNullCheckFor(Value value, LIRFrameState nullCheckState, int implicitNullCheckLimit) {
 347             if (state == null &amp;&amp; x instanceof AMD64AddressValue &amp;&amp; ((AMD64AddressValue) x).isValidImplicitNullCheckFor(value, implicitNullCheckLimit)) {
 348                 state = nullCheckState;
 349                 return true;
 350             }
 351             return false;
 352         }
 353     }
 354 
 355     public static class CmpBranchOp extends BranchOp implements ImplicitNullCheck {
 356 
 357         public static final LIRInstructionClass&lt;CmpBranchOp&gt; TYPE = LIRInstructionClass.create(CmpBranchOp.class);
 358 
 359         private final OperandSize size;
 360 
 361         @Use({REG}) protected AllocatableValue x;
 362         @Use({REG, STACK, COMPOSITE}) protected Value y;
 363 
 364         @State protected LIRFrameState state;
 365 
 366         public CmpBranchOp(OperandSize size, AllocatableValue x, Value y, LIRFrameState state, Condition cond, LabelRef trueDestination, LabelRef falseDestination, double trueDestinationProbability) {
 367             super(TYPE, intCond(cond), trueDestination, falseDestination, trueDestinationProbability);
 368             this.size = size;
 369 
 370             this.x = x;
 371             this.y = y;
 372 
 373             this.state = state;
 374         }
 375 
 376         @Override
 377         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 378             if (isRegister(y)) {
 379                 assert state == null;
 380                 if (crb.isSuccessorEdge(trueDestination)) {
 381                     masm.cmpAndJcc(size, asRegister(x), asRegister(y), condition.negate(), falseDestination.label(), false);
 382                 } else if (crb.isSuccessorEdge(falseDestination)) {
 383                     masm.cmpAndJcc(size, asRegister(x), asRegister(y), condition, trueDestination.label(), false);
 384                 } else if (trueDestinationProbability &lt; 0.5) {
 385                     masm.cmpAndJcc(size, asRegister(x), asRegister(y), condition.negate(), falseDestination.label(), false);
 386                     masm.jmp(trueDestination.label());
 387                 } else {
 388                     masm.cmpAndJcc(size, asRegister(x), asRegister(y), condition, trueDestination.label(), false);
 389                     masm.jmp(falseDestination.label());
 390                 }
 391                 return;
 392             }
 393             IntConsumer applyBeforeFusedPair = state == null ? null : pos -&gt; crb.recordImplicitException(pos, state);
 394             if (isStackSlot(y)) {
 395                 if (crb.isSuccessorEdge(trueDestination)) {
 396                     masm.cmpAndJcc(size, asRegister(x), (AMD64Address) crb.asAddress(y), condition.negate(), falseDestination.label(), false, applyBeforeFusedPair);
 397                 } else if (crb.isSuccessorEdge(falseDestination)) {
 398                     masm.cmpAndJcc(size, asRegister(x), (AMD64Address) crb.asAddress(y), condition, trueDestination.label(), false, applyBeforeFusedPair);
 399                 } else if (trueDestinationProbability &lt; 0.5) {
 400                     masm.cmpAndJcc(size, asRegister(x), (AMD64Address) crb.asAddress(y), condition.negate(), falseDestination.label(), false, applyBeforeFusedPair);
 401                     masm.jmp(trueDestination.label());
 402                 } else {
 403                     masm.cmpAndJcc(size, asRegister(x), (AMD64Address) crb.asAddress(y), condition, trueDestination.label(), false, applyBeforeFusedPair);
 404                     masm.jmp(falseDestination.label());
 405                 }
 406             } else {
 407                 AMD64AddressValue yAddress = (AMD64AddressValue) y;
 408                 if (crb.isSuccessorEdge(trueDestination)) {
 409                     masm.cmpAndJcc(size, asRegister(x), yAddress.toAddress(), condition.negate(), falseDestination.label(), false, applyBeforeFusedPair);
 410                 } else if (crb.isSuccessorEdge(falseDestination)) {
 411                     masm.cmpAndJcc(size, asRegister(x), yAddress.toAddress(), condition, trueDestination.label(), false, applyBeforeFusedPair);
 412                 } else if (trueDestinationProbability &lt; 0.5) {
 413                     masm.cmpAndJcc(size, asRegister(x), yAddress.toAddress(), condition.negate(), falseDestination.label(), false, applyBeforeFusedPair);
 414                     masm.jmp(trueDestination.label());
 415                 } else {
 416                     masm.cmpAndJcc(size, asRegister(x), yAddress.toAddress(), condition, trueDestination.label(), false, applyBeforeFusedPair);
 417                     masm.jmp(falseDestination.label());
 418                 }
 419             }
 420         }
 421 
 422         @Override
 423         public boolean makeNullCheckFor(Value value, LIRFrameState nullCheckState, int implicitNullCheckLimit) {
 424             if (state == null &amp;&amp; y instanceof AMD64AddressValue &amp;&amp; ((AMD64AddressValue) y).isValidImplicitNullCheckFor(value, implicitNullCheckLimit)) {
 425                 state = nullCheckState;
 426                 return true;
 427             }
 428             return false;
 429         }
 430     }
 431 
 432     public static class CmpConstBranchOp extends BranchOp {
 433 
 434         public static final LIRInstructionClass&lt;CmpConstBranchOp&gt; TYPE = LIRInstructionClass.create(CmpConstBranchOp.class);
 435 
 436         private final OperandSize size;
 437 
 438         @Use({REG, STACK, COMPOSITE}) protected Value x;
 439         private final int y;
 440         private final VMConstant inlinedY;
 441 
 442         @State protected LIRFrameState state;
 443 
 444         public CmpConstBranchOp(OperandSize size, Value x, int y, LIRFrameState state, Condition cond, LabelRef trueDestination, LabelRef falseDestination, double trueDestinationProbability) {
 445             super(TYPE, intCond(cond), trueDestination, falseDestination, trueDestinationProbability);
 446             this.size = size;
 447 
 448             this.x = x;
 449             this.y = y;
 450             this.inlinedY = null;
 451 
 452             this.state = state;
 453         }
 454 
 455         public CmpConstBranchOp(OperandSize size, Value x, VMConstant y, LIRFrameState state, Condition cond, LabelRef trueDestination, LabelRef falseDestination, double trueDestinationProbability) {
 456             super(TYPE, intCond(cond), trueDestination, falseDestination, trueDestinationProbability);
 457             this.size = size;
 458 
 459             this.x = x;
 460             this.y = 0xDEADDEAD;
 461             this.inlinedY = y;
 462 
 463             this.state = state;
 464         }
 465 
 466         @Override
 467         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 468             final boolean inlineDataInCode = inlinedY != null;
 469             IntConsumer applyBeforeFusedPair = null;
 470             if (inlineDataInCode) {
 471                 applyBeforeFusedPair = pos -&gt; crb.recordInlineDataInCode(inlinedY);
 472             }
 473             if (isRegister(x)) {
 474                 assert state == null;
 475                 if (crb.isSuccessorEdge(trueDestination)) {
 476                     masm.cmpAndJcc(size, asRegister(x), y, condition.negate(), falseDestination.label(), false, inlineDataInCode, applyBeforeFusedPair);
 477                 } else if (crb.isSuccessorEdge(falseDestination)) {
 478                     masm.cmpAndJcc(size, asRegister(x), y, condition, trueDestination.label(), false, inlineDataInCode, applyBeforeFusedPair);
 479                 } else if (trueDestinationProbability &lt; 0.5) {
 480                     masm.cmpAndJcc(size, asRegister(x), y, condition.negate(), falseDestination.label(), false, inlineDataInCode, applyBeforeFusedPair);
 481                     masm.jmp(trueDestination.label());
 482                 } else {
 483                     masm.cmpAndJcc(size, asRegister(x), y, condition, trueDestination.label(), false, inlineDataInCode, applyBeforeFusedPair);
 484                     masm.jmp(falseDestination.label());
 485                 }
 486                 return;
 487             }
 488             if (state != null) {
 489                 IntConsumer implicitException = pos -&gt; crb.recordImplicitException(pos, state);
 490                 applyBeforeFusedPair = applyBeforeFusedPair == null ? implicitException : applyBeforeFusedPair.andThen(implicitException);
 491             }
 492             if (isStackSlot(x)) {
 493                 if (crb.isSuccessorEdge(trueDestination)) {
 494                     masm.cmpAndJcc(size, (AMD64Address) crb.asAddress(x), y, condition.negate(), falseDestination.label(), false, inlineDataInCode, applyBeforeFusedPair);
 495                 } else if (crb.isSuccessorEdge(falseDestination)) {
 496                     masm.cmpAndJcc(size, (AMD64Address) crb.asAddress(x), y, condition, trueDestination.label(), false, inlineDataInCode, applyBeforeFusedPair);
 497                 } else if (trueDestinationProbability &lt; 0.5) {
 498                     masm.cmpAndJcc(size, (AMD64Address) crb.asAddress(x), y, condition.negate(), falseDestination.label(), false, inlineDataInCode, applyBeforeFusedPair);
 499                     masm.jmp(trueDestination.label());
 500                 } else {
 501                     masm.cmpAndJcc(size, (AMD64Address) crb.asAddress(x), y, condition, trueDestination.label(), false, inlineDataInCode, applyBeforeFusedPair);
 502                     masm.jmp(falseDestination.label());
 503                 }
 504             } else {
 505                 AMD64AddressValue xAddress = (AMD64AddressValue) x;
 506                 if (crb.isSuccessorEdge(trueDestination)) {
 507                     masm.cmpAndJcc(size, xAddress.toAddress(), y, condition.negate(), falseDestination.label(), false, inlineDataInCode, applyBeforeFusedPair);
 508                 } else if (crb.isSuccessorEdge(falseDestination)) {
 509                     masm.cmpAndJcc(size, xAddress.toAddress(), y, condition, trueDestination.label(), false, inlineDataInCode, applyBeforeFusedPair);
 510                 } else if (trueDestinationProbability &lt; 0.5) {
 511                     masm.cmpAndJcc(size, xAddress.toAddress(), y, condition.negate(), falseDestination.label(), false, inlineDataInCode, applyBeforeFusedPair);
 512                     masm.jmp(trueDestination.label());
 513                 } else {
 514                     masm.cmpAndJcc(size, xAddress.toAddress(), y, condition, trueDestination.label(), false, inlineDataInCode, applyBeforeFusedPair);
 515                     masm.jmp(falseDestination.label());
 516                 }
 517             }
 518         }
 519     }
 520 
 521     public static class CmpDataBranchOp extends BranchOp {
 522 
 523         public static final LIRInstructionClass&lt;CmpDataBranchOp&gt; TYPE = LIRInstructionClass.create(CmpDataBranchOp.class);
 524 
 525         private final OperandSize size;
 526 
 527         @Use({REG}) protected AllocatableValue x;
 528         private final Constant y;
 529 
 530         public CmpDataBranchOp(OperandSize size, AllocatableValue x, Constant y, Condition cond, LabelRef trueDestination, LabelRef falseDestination, double trueDestinationProbability) {
 531             super(TYPE, intCond(cond), trueDestination, falseDestination, trueDestinationProbability);
 532             this.size = size;
 533 
 534             this.x = x;
 535             this.y = y;
 536         }
 537 
 538         @Override
 539         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 540             Supplier&lt;AMD64Address&gt; applyBeforeConsumer = () -&gt; (AMD64Address) crb.recordDataReferenceInCode(y, size.getBytes());
 541             if (crb.isSuccessorEdge(trueDestination)) {
 542                 masm.cmpAndJcc(size, asRegister(x), applyBeforeConsumer, condition.negate(), falseDestination.label());
 543             } else if (crb.isSuccessorEdge(falseDestination)) {
 544                 masm.cmpAndJcc(size, asRegister(x), applyBeforeConsumer, condition, trueDestination.label());
 545             } else if (trueDestinationProbability &lt; 0.5) {
 546                 masm.cmpAndJcc(size, asRegister(x), applyBeforeConsumer, condition.negate(), falseDestination.label());
 547                 masm.jmp(trueDestination.label());
 548             } else {
 549                 masm.cmpAndJcc(size, asRegister(x), applyBeforeConsumer, condition, trueDestination.label());
 550                 masm.jmp(falseDestination.label());
 551             }
 552         }
 553     }
 554 
 555     public static final class FloatBranchOp extends BranchOp {
 556         public static final LIRInstructionClass&lt;FloatBranchOp&gt; TYPE = LIRInstructionClass.create(FloatBranchOp.class);
 557         protected boolean unorderedIsTrue;
 558 
 559         public FloatBranchOp(Condition condition, boolean unorderedIsTrue, LabelRef trueDestination, LabelRef falseDestination, double trueDestinationProbability) {
 560             super(TYPE, floatCond(condition), trueDestination, falseDestination, trueDestinationProbability);
 561             this.unorderedIsTrue = unorderedIsTrue;
 562         }
 563 
 564         @Override
 565         protected void jcc(AMD64MacroAssembler masm, boolean negate, LabelRef target) {
 566             ConditionFlag condition1 = negate ? condition.negate() : condition;
 567             boolean unorderedIsTrue1 = negate ? !unorderedIsTrue : unorderedIsTrue;
 568             Label label = target.label();
 569             Label endLabel = new Label();
 570             if (unorderedIsTrue1 &amp;&amp; !trueOnUnordered(condition1)) {
 571                 masm.jcc(ConditionFlag.Parity, label);
 572             } else if (!unorderedIsTrue1 &amp;&amp; trueOnUnordered(condition1)) {
 573                 masm.jccb(ConditionFlag.Parity, endLabel);
 574             }
 575             masm.jcc(condition1, label);
 576             masm.bind(endLabel);
 577         }
 578     }
 579 
 580     public static class StrategySwitchOp extends AMD64BlockEndOp {
 581         public static final LIRInstructionClass&lt;StrategySwitchOp&gt; TYPE = LIRInstructionClass.create(StrategySwitchOp.class);
 582         protected final Constant[] keyConstants;
 583         private final LabelRef[] keyTargets;
 584         private LabelRef defaultTarget;
 585         @Alive({REG}) protected Value key;
 586         @Temp({REG, ILLEGAL}) protected Value scratch;
 587         protected final SwitchStrategy strategy;
 588 
 589         public StrategySwitchOp(SwitchStrategy strategy, LabelRef[] keyTargets, LabelRef defaultTarget, Value key, Value scratch) {
 590             this(TYPE, strategy, keyTargets, defaultTarget, key, scratch);
 591         }
 592 
 593         protected StrategySwitchOp(LIRInstructionClass&lt;? extends StrategySwitchOp&gt; c, SwitchStrategy strategy, LabelRef[] keyTargets, LabelRef defaultTarget, Value key, Value scratch) {
 594             super(c);
 595             this.strategy = strategy;
 596             this.keyConstants = strategy.getKeyConstants();
 597             this.keyTargets = keyTargets;
 598             this.defaultTarget = defaultTarget;
 599             this.key = key;
 600             this.scratch = scratch;
 601             assert keyConstants.length == keyTargets.length;
 602             assert keyConstants.length == strategy.keyProbabilities.length;
 603         }
 604 
 605         @Override
 606         public void emitCode(final CompilationResultBuilder crb, final AMD64MacroAssembler masm) {
 607             strategy.run(new SwitchClosure(asRegister(key), crb, masm));
 608         }
 609 
 610         public class SwitchClosure extends BaseSwitchClosure {
 611 
 612             protected final Register keyRegister;
 613             protected final CompilationResultBuilder crb;
 614             protected final AMD64MacroAssembler masm;
 615 
 616             protected SwitchClosure(Register keyRegister, CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 617                 super(crb, masm, keyTargets, defaultTarget);
 618                 this.keyRegister = keyRegister;
 619                 this.crb = crb;
 620                 this.masm = masm;
 621             }
 622 
 623             protected void emitComparison(Constant c) {
 624                 JavaConstant jc = (JavaConstant) c;
 625                 switch (jc.getJavaKind()) {
 626                     case Int:
 627                         long lc = jc.asLong();
 628                         assert NumUtil.isInt(lc);
 629                         masm.cmpl(keyRegister, (int) lc);
 630                         break;
 631                     case Long:
 632                         masm.cmpq(keyRegister, (AMD64Address) crb.asLongConstRef(jc));
 633                         break;
 634                     case Object:
 635                         AMD64Move.const2reg(crb, masm, asRegister(scratch), jc, AMD64Kind.QWORD);
 636                         masm.cmpptr(keyRegister, asRegister(scratch));
 637                         break;
 638                     default:
 639                         throw new GraalError(&quot;switch only supported for int, long and object&quot;);
 640                 }
 641             }
 642 
 643             @Override
 644             protected void conditionalJump(int index, Condition condition, Label target) {
 645                 emitComparison(keyConstants[index]);
 646                 masm.jcc(intCond(condition), target);
 647             }
 648         }
 649     }
 650 
 651     public static final class TableSwitchOp extends AMD64BlockEndOp {
 652         public static final LIRInstructionClass&lt;TableSwitchOp&gt; TYPE = LIRInstructionClass.create(TableSwitchOp.class);
 653         private final int lowKey;
 654         private final LabelRef defaultTarget;
 655         private final LabelRef[] targets;
 656         @Use protected Value index;
 657         @Temp({REG, HINT}) protected Value idxScratch;
 658         @Temp protected Value scratch;
 659 
 660         public TableSwitchOp(final int lowKey, final LabelRef defaultTarget, final LabelRef[] targets, Value index, Variable scratch, Variable idxScratch) {
 661             super(TYPE);
 662             this.lowKey = lowKey;
 663             this.defaultTarget = defaultTarget;
 664             this.targets = targets;
 665             this.index = index;
 666             this.scratch = scratch;
 667             this.idxScratch = idxScratch;
 668         }
 669 
 670         @Override
 671         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 672             Register indexReg = asRegister(index, AMD64Kind.DWORD);
 673             Register idxScratchReg = asRegister(idxScratch, AMD64Kind.DWORD);
 674             Register scratchReg = asRegister(scratch, AMD64Kind.QWORD);
 675 
 676             if (!indexReg.equals(idxScratchReg)) {
 677                 masm.movl(idxScratchReg, indexReg);
 678             }
 679 
 680             // Compare index against jump table bounds
 681             int highKey = lowKey + targets.length - 1;
 682             if (lowKey != 0) {
 683                 // subtract the low value from the switch value
 684                 masm.subl(idxScratchReg, lowKey);
 685                 masm.cmpl(idxScratchReg, highKey - lowKey);
 686             } else {
 687                 masm.cmpl(idxScratchReg, highKey);
 688             }
 689 
 690             // Jump to default target if index is not within the jump table
 691             if (defaultTarget != null) {
 692                 masm.jcc(ConditionFlag.Above, defaultTarget.label());
 693             }
 694 
 695             // Set scratch to address of jump table
 696             masm.leaq(scratchReg, new AMD64Address(AMD64.rip, 0));
 697             final int afterLea = masm.position();
 698 
 699             // Load jump table entry into scratch and jump to it
 700             masm.movslq(idxScratchReg, new AMD64Address(scratchReg, idxScratchReg, Scale.Times4, 0));
 701             masm.addq(scratchReg, idxScratchReg);
 702             masm.jmp(scratchReg);
 703 
 704             // Inserting padding so that jump table address is 4-byte aligned
 705             masm.align(4);
 706 
 707             // Patch LEA instruction above now that we know the position of the jump table
 708             // this is ugly but there is no better way to do this given the assembler API
 709             final int jumpTablePos = masm.position();
 710             final int leaDisplacementPosition = afterLea - 4;
 711             masm.emitInt(jumpTablePos - afterLea, leaDisplacementPosition);
 712 
 713             // Emit jump table entries
 714             for (LabelRef target : targets) {
 715                 Label label = target.label();
 716                 int offsetToJumpTableBase = masm.position() - jumpTablePos;
 717                 if (label.isBound()) {
 718                     int imm32 = label.position() - jumpTablePos;
 719                     masm.emitInt(imm32);
 720                 } else {
 721                     label.addPatchAt(masm.position(), masm);
 722 
 723                     masm.emitByte(0); // pseudo-opcode for jump table entry
 724                     masm.emitShort(offsetToJumpTableBase);
 725                     masm.emitByte(0); // padding to make jump table entry 4 bytes wide
 726                 }
 727             }
 728 
 729             JumpTable jt = new JumpTable(jumpTablePos, lowKey, highKey, 4);
 730             crb.compilationResult.addAnnotation(jt);
 731         }
 732     }
 733 
 734     public static final class HashTableSwitchOp extends AMD64BlockEndOp {
 735         public static final LIRInstructionClass&lt;HashTableSwitchOp&gt; TYPE = LIRInstructionClass.create(HashTableSwitchOp.class);
 736         private final JavaConstant[] keys;
 737         private final LabelRef defaultTarget;
 738         private final LabelRef[] targets;
 739         @Alive protected Value value;
 740         @Alive protected Value hash;
 741         @Temp({REG}) protected Value entryScratch;
 742         @Temp({REG}) protected Value scratch;
 743 
 744         public HashTableSwitchOp(final JavaConstant[] keys, final LabelRef defaultTarget, LabelRef[] targets, Value value, Value hash, Variable scratch, Variable entryScratch) {
 745             super(TYPE);
 746             this.keys = keys;
 747             this.defaultTarget = defaultTarget;
 748             this.targets = targets;
 749             this.value = value;
 750             this.hash = hash;
 751             this.scratch = scratch;
 752             this.entryScratch = entryScratch;
 753         }
 754 
 755         @Override
 756         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 757             Register valueReg = asRegister(value, AMD64Kind.DWORD);
 758             Register indexReg = asRegister(hash, AMD64Kind.DWORD);
 759             Register scratchReg = asRegister(scratch, AMD64Kind.QWORD);
 760             Register entryScratchReg = asRegister(entryScratch, AMD64Kind.QWORD);
 761 
 762             // Set scratch to address of jump table
 763             masm.leaq(scratchReg, new AMD64Address(AMD64.rip, 0));
 764             final int afterLea = masm.position();
 765 
 766             // When the default target is set, the jump table contains entries with two DWORDS:
 767             // the original key before hashing and the label jump address
 768             if (defaultTarget != null) {
 769 
 770                 // Move the table entry (two DWORDs) into a QWORD
 771                 masm.movq(entryScratchReg, new AMD64Address(scratchReg, indexReg, Scale.Times8, 0));
 772 
 773                 // Jump to the default target if the first DWORD (original key) doesn&#39;t match the
 774                 // current key. Accounts for hash collisions with unknown keys
 775                 masm.cmplAndJcc(entryScratchReg, valueReg, ConditionFlag.NotEqual, defaultTarget.label(), false);
 776 
 777                 // Shift to the second DWORD
 778                 masm.sarq(entryScratchReg, 32);
 779             } else {
 780 
 781                 // The jump table has a single DWORD with the label address if there&#39;s no
 782                 // default target
 783                 masm.movslq(entryScratchReg, new AMD64Address(scratchReg, indexReg, Scale.Times4, 0));
 784             }
 785             masm.addq(scratchReg, entryScratchReg);
 786             masm.jmp(scratchReg);
 787 
 788             // Inserting padding so that jump the table address is aligned
 789             if (defaultTarget != null) {
 790                 masm.align(8);
 791             } else {
 792                 masm.align(4);
 793             }
 794 
 795             // Patch LEA instruction above now that we know the position of the jump table
 796             // this is ugly but there is no better way to do this given the assembler API
 797             final int jumpTablePos = masm.position();
 798             final int leaDisplacementPosition = afterLea - 4;
 799             masm.emitInt(jumpTablePos - afterLea, leaDisplacementPosition);
 800 
 801             // Emit jump table entries
 802             for (int i = 0; i &lt; targets.length; i++) {
 803 
 804                 Label label = targets[i].label();
 805 
 806                 if (defaultTarget != null) {
 807                     masm.emitInt(keys[i].asInt());
 808                 }
 809                 if (label.isBound()) {
 810                     int imm32 = label.position() - jumpTablePos;
 811                     masm.emitInt(imm32);
 812                 } else {
 813                     int offsetToJumpTableBase = masm.position() - jumpTablePos;
 814                     label.addPatchAt(masm.position(), masm);
 815                     masm.emitByte(0); // pseudo-opcode for jump table entry
 816                     masm.emitShort(offsetToJumpTableBase);
 817                     masm.emitByte(0); // padding to make jump table entry 4 bytes wide
 818                 }
 819             }
 820 
 821             JumpTable jt = new JumpTable(jumpTablePos, keys[0].asInt(), keys[keys.length - 1].asInt(), 4);
 822             crb.compilationResult.addAnnotation(jt);
 823         }
 824     }
 825 
 826     @Opcode(&quot;SETcc&quot;)
 827     public static final class CondSetOp extends AMD64LIRInstruction {
 828         public static final LIRInstructionClass&lt;CondSetOp&gt; TYPE = LIRInstructionClass.create(CondSetOp.class);
 829         @Def({REG, HINT}) protected Value result;
 830         private final ConditionFlag condition;
 831 
 832         public CondSetOp(Variable result, Condition condition) {
 833             super(TYPE);
 834             this.result = result;
 835             this.condition = intCond(condition);
 836         }
 837 
 838         @Override
 839         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 840             setcc(masm, result, condition);
 841         }
 842     }
 843 
 844     @Opcode(&quot;SETcc&quot;)
 845     public static final class FloatCondSetOp extends AMD64LIRInstruction {
 846         public static final LIRInstructionClass&lt;FloatCondSetOp&gt; TYPE = LIRInstructionClass.create(FloatCondSetOp.class);
 847         @Def({REG, HINT}) protected Value result;
 848         private final ConditionFlag condition;
 849 
 850         public FloatCondSetOp(Variable result, Condition condition) {
 851             super(TYPE);
 852             this.result = result;
 853             this.condition = floatCond(condition);
 854         }
 855 
 856         @Override
 857         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 858             setcc(masm, result, condition);
 859         }
 860     }
 861 
 862     @Opcode(&quot;CMOVE&quot;)
 863     public static final class CondMoveOp extends AMD64LIRInstruction {
 864         public static final LIRInstructionClass&lt;CondMoveOp&gt; TYPE = LIRInstructionClass.create(CondMoveOp.class);
 865         @Def({REG, HINT}) protected Value result;
 866         @Alive({REG}) protected Value trueValue;
 867         @Use({REG, STACK, CONST}) protected Value falseValue;
 868         private final ConditionFlag condition;
 869 
 870         public CondMoveOp(Variable result, Condition condition, AllocatableValue trueValue, Value falseValue) {
 871             super(TYPE);
 872             this.result = result;
 873             this.condition = intCond(condition);
 874             this.trueValue = trueValue;
 875             this.falseValue = falseValue;
 876         }
 877 
 878         @Override
 879         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 880             cmove(crb, masm, result, false, condition, false, trueValue, falseValue);
 881         }
 882     }
 883 
 884     @Opcode(&quot;CMOVE&quot;)
 885     public static final class FloatCondMoveOp extends AMD64LIRInstruction {
 886         public static final LIRInstructionClass&lt;FloatCondMoveOp&gt; TYPE = LIRInstructionClass.create(FloatCondMoveOp.class);
 887         @Def({REG}) protected Value result;
 888         @Alive({REG}) protected Value trueValue;
 889         @Alive({REG}) protected Value falseValue;
 890         private final ConditionFlag condition;
 891         private final boolean unorderedIsTrue;
 892 
 893         public FloatCondMoveOp(Variable result, Condition condition, boolean unorderedIsTrue, Variable trueValue, Variable falseValue) {
 894             super(TYPE);
 895             this.result = result;
 896             this.condition = floatCond(condition);
 897             this.unorderedIsTrue = unorderedIsTrue;
 898             this.trueValue = trueValue;
 899             this.falseValue = falseValue;
 900         }
 901 
 902         @Override
 903         public void emitCode(CompilationResultBuilder crb, AMD64MacroAssembler masm) {
 904             cmove(crb, masm, result, true, condition, unorderedIsTrue, trueValue, falseValue);
 905         }
 906     }
 907 
 908     private static void cmove(CompilationResultBuilder crb, AMD64MacroAssembler masm, Value result, boolean isFloat, ConditionFlag condition, boolean unorderedIsTrue, Value trueValue,
 909                     Value falseValue) {
 910         // check that we don&#39;t overwrite an input operand before it is used.
 911         assert !result.equals(trueValue);
 912 
 913         AMD64Move.move(crb, masm, result, falseValue);
 914         cmove(crb, masm, result, condition, trueValue);
 915 
 916         if (isFloat) {
 917             if (unorderedIsTrue &amp;&amp; !trueOnUnordered(condition)) {
 918                 cmove(crb, masm, result, ConditionFlag.Parity, trueValue);
 919             } else if (!unorderedIsTrue &amp;&amp; trueOnUnordered(condition)) {
 920                 cmove(crb, masm, result, ConditionFlag.Parity, falseValue);
 921             }
 922         }
 923     }
 924 
 925     private static void cmove(CompilationResultBuilder crb, AMD64MacroAssembler masm, Value result, ConditionFlag cond, Value other) {
 926         if (isRegister(other)) {
 927             assert !asRegister(other).equals(asRegister(result)) : &quot;other already overwritten by previous move&quot;;
 928             switch ((AMD64Kind) other.getPlatformKind()) {
 929                 case BYTE:
 930                 case WORD:
 931                 case DWORD:
 932                     masm.cmovl(cond, asRegister(result), asRegister(other));
 933                     break;
 934                 case QWORD:
 935                     masm.cmovq(cond, asRegister(result), asRegister(other));
 936                     break;
 937                 default:
 938                     throw GraalError.shouldNotReachHere();
 939             }
 940         } else {
 941             AMD64Address addr = (AMD64Address) crb.asAddress(other);
 942             switch ((AMD64Kind) other.getPlatformKind()) {
 943                 case BYTE:
 944                 case WORD:
 945                 case DWORD:
 946                     masm.cmovl(cond, asRegister(result), addr);
 947                     break;
 948                 case QWORD:
 949                     masm.cmovq(cond, asRegister(result), addr);
 950                     break;
 951                 default:
 952                     throw GraalError.shouldNotReachHere();
 953             }
 954         }
 955     }
 956 
 957     private static void setcc(AMD64MacroAssembler masm, Value result, ConditionFlag cond) {
 958         switch ((AMD64Kind) result.getPlatformKind()) {
 959             case BYTE:
 960             case WORD:
 961             case DWORD:
 962                 masm.setl(cond, asRegister(result));
 963                 break;
 964             case QWORD:
 965                 masm.setq(cond, asRegister(result));
 966                 break;
 967             default:
 968                 throw GraalError.shouldNotReachHere();
 969         }
 970     }
 971 
 972     private static ConditionFlag intCond(Condition cond) {
 973         switch (cond) {
 974             case EQ:
 975                 return ConditionFlag.Equal;
 976             case NE:
 977                 return ConditionFlag.NotEqual;
 978             case LT:
 979                 return ConditionFlag.Less;
 980             case LE:
 981                 return ConditionFlag.LessEqual;
 982             case GE:
 983                 return ConditionFlag.GreaterEqual;
 984             case GT:
 985                 return ConditionFlag.Greater;
 986             case BE:
 987                 return ConditionFlag.BelowEqual;
 988             case AE:
 989                 return ConditionFlag.AboveEqual;
 990             case AT:
 991                 return ConditionFlag.Above;
 992             case BT:
 993                 return ConditionFlag.Below;
 994             default:
 995                 throw GraalError.shouldNotReachHere();
 996         }
 997     }
 998 
 999     private static ConditionFlag floatCond(Condition cond) {
1000         switch (cond) {
1001             case EQ:
1002                 return ConditionFlag.Equal;
1003             case NE:
1004                 return ConditionFlag.NotEqual;
1005             case LT:
1006                 return ConditionFlag.Below;
1007             case LE:
1008                 return ConditionFlag.BelowEqual;
1009             case GE:
1010                 return ConditionFlag.AboveEqual;
1011             case GT:
1012                 return ConditionFlag.Above;
1013             default:
1014                 throw GraalError.shouldNotReachHere();
1015         }
1016     }
1017 
1018     public static boolean trueOnUnordered(Condition condition) {
1019         return trueOnUnordered(floatCond(condition));
1020     }
1021 
1022     private static boolean trueOnUnordered(ConditionFlag condition) {
1023         switch (condition) {
1024             case AboveEqual:
1025             case NotEqual:
1026             case Above:
1027             case Less:
1028             case Overflow:
1029                 return false;
1030             case Equal:
1031             case BelowEqual:
1032             case Below:
1033             case GreaterEqual:
1034             case NoOverflow:
1035                 return true;
1036             default:
1037                 throw GraalError.shouldNotReachHere();
1038         }
1039     }
1040 }
    </pre>
  </body>
</html>