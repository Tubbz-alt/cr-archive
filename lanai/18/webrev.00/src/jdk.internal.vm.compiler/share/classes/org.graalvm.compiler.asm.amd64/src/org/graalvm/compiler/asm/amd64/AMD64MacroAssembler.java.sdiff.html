<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.asm.amd64/src/org/graalvm/compiler/asm/amd64/AMD64MacroAssembler.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../style.css" />
  </head>
<body>
<center><a href="AMD64BaseAssembler.java.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../../../index.html" target="_top">index</a> <a href="AVXKind.java.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.asm.amd64/src/org/graalvm/compiler/asm/amd64/AMD64MacroAssembler.java</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2009, 2019, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 
 25 package org.graalvm.compiler.asm.amd64;
 26 
 27 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseIncDec;
 28 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseXmmLoadAndClearUpper;
 29 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseXmmRegToRegMoveAll;
<span class="line-modified"> 30 </span>













 31 import org.graalvm.compiler.asm.amd64.AVXKind.AVXSize;
 32 import org.graalvm.compiler.core.common.NumUtil;

 33 
 34 import jdk.vm.ci.amd64.AMD64;
 35 import jdk.vm.ci.amd64.AMD64Kind;
 36 import jdk.vm.ci.code.Register;
 37 import jdk.vm.ci.code.TargetDescription;
 38 
 39 /**
 40  * This class implements commonly used X86 code patterns.
 41  */
 42 public class AMD64MacroAssembler extends AMD64Assembler {
 43 
 44     public AMD64MacroAssembler(TargetDescription target) {
 45         super(target);
 46     }
 47 




 48     public final void decrementq(Register reg, int value) {
 49         if (value == Integer.MIN_VALUE) {
 50             subq(reg, value);
 51             return;
 52         }
 53         if (value &lt; 0) {
 54             incrementq(reg, -value);
 55             return;
 56         }
 57         if (value == 0) {
 58             return;
 59         }
 60         if (value == 1 &amp;&amp; UseIncDec) {
 61             decq(reg);
 62         } else {
 63             subq(reg, value);
 64         }
 65     }
 66 
 67     public final void decrementq(AMD64Address dst, int value) {
</pre>
<hr />
<pre>
354         ffree(0);
355         fincstp();
356     }
357 
358     private AMD64Address trigPrologue(Register value) {
359         assert value.getRegisterCategory().equals(AMD64.XMM);
360         AMD64Address tmp = new AMD64Address(AMD64.rsp);
361         subq(AMD64.rsp, AMD64Kind.DOUBLE.getSizeInBytes());
362         movdbl(tmp, value);
363         fldd(tmp);
364         return tmp;
365     }
366 
367     private void trigEpilogue(Register dest, AMD64Address tmp) {
368         assert dest.getRegisterCategory().equals(AMD64.XMM);
369         fstpd(tmp);
370         movdbl(dest, tmp);
371         addq(AMD64.rsp, AMD64Kind.DOUBLE.getSizeInBytes());
372     }
373 















































































































































































































































































374 }
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 2009, 2020, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  */
 23 
 24 
 25 package org.graalvm.compiler.asm.amd64;
 26 
 27 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseIncDec;
 28 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseXmmLoadAndClearUpper;
 29 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseXmmRegToRegMoveAll;
<span class="line-modified"> 30 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.ADD;</span>
<span class="line-added"> 31 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.AND;</span>
<span class="line-added"> 32 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.CMP;</span>
<span class="line-added"> 33 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.SUB;</span>
<span class="line-added"> 34 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MOp.DEC;</span>
<span class="line-added"> 35 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MOp.INC;</span>
<span class="line-added"> 36 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.DWORD;</span>
<span class="line-added"> 37 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.QWORD;</span>
<span class="line-added"> 38 import static org.graalvm.compiler.core.common.NumUtil.isByte;</span>
<span class="line-added"> 39 </span>
<span class="line-added"> 40 import java.util.function.IntConsumer;</span>
<span class="line-added"> 41 import java.util.function.Supplier;</span>
<span class="line-added"> 42 </span>
<span class="line-added"> 43 import org.graalvm.compiler.asm.Label;</span>
 44 import org.graalvm.compiler.asm.amd64.AVXKind.AVXSize;
 45 import org.graalvm.compiler.core.common.NumUtil;
<span class="line-added"> 46 import org.graalvm.compiler.options.OptionValues;</span>
 47 
 48 import jdk.vm.ci.amd64.AMD64;
 49 import jdk.vm.ci.amd64.AMD64Kind;
 50 import jdk.vm.ci.code.Register;
 51 import jdk.vm.ci.code.TargetDescription;
 52 
 53 /**
 54  * This class implements commonly used X86 code patterns.
 55  */
 56 public class AMD64MacroAssembler extends AMD64Assembler {
 57 
 58     public AMD64MacroAssembler(TargetDescription target) {
 59         super(target);
 60     }
 61 
<span class="line-added"> 62     public AMD64MacroAssembler(TargetDescription target, OptionValues optionValues) {</span>
<span class="line-added"> 63         super(target, optionValues);</span>
<span class="line-added"> 64     }</span>
<span class="line-added"> 65 </span>
 66     public final void decrementq(Register reg, int value) {
 67         if (value == Integer.MIN_VALUE) {
 68             subq(reg, value);
 69             return;
 70         }
 71         if (value &lt; 0) {
 72             incrementq(reg, -value);
 73             return;
 74         }
 75         if (value == 0) {
 76             return;
 77         }
 78         if (value == 1 &amp;&amp; UseIncDec) {
 79             decq(reg);
 80         } else {
 81             subq(reg, value);
 82         }
 83     }
 84 
 85     public final void decrementq(AMD64Address dst, int value) {
</pre>
<hr />
<pre>
372         ffree(0);
373         fincstp();
374     }
375 
376     private AMD64Address trigPrologue(Register value) {
377         assert value.getRegisterCategory().equals(AMD64.XMM);
378         AMD64Address tmp = new AMD64Address(AMD64.rsp);
379         subq(AMD64.rsp, AMD64Kind.DOUBLE.getSizeInBytes());
380         movdbl(tmp, value);
381         fldd(tmp);
382         return tmp;
383     }
384 
385     private void trigEpilogue(Register dest, AMD64Address tmp) {
386         assert dest.getRegisterCategory().equals(AMD64.XMM);
387         fstpd(tmp);
388         movdbl(dest, tmp);
389         addq(AMD64.rsp, AMD64Kind.DOUBLE.getSizeInBytes());
390     }
391 
<span class="line-added">392     /**</span>
<span class="line-added">393      * Emit a direct call to a fixed address, which will be patched later during code installation.</span>
<span class="line-added">394      *</span>
<span class="line-added">395      * @param align indicates whether the displacement bytes (offset by</span>
<span class="line-added">396      *            {@code callDisplacementOffset}) of this call instruction should be aligned to</span>
<span class="line-added">397      *            {@code wordSize}.</span>
<span class="line-added">398      * @return where the actual call instruction starts.</span>
<span class="line-added">399      */</span>
<span class="line-added">400     public final int directCall(boolean align, int callDisplacementOffset, int wordSize) {</span>
<span class="line-added">401         emitAlignmentForDirectCall(align, callDisplacementOffset, wordSize);</span>
<span class="line-added">402         testAndAlign(5);</span>
<span class="line-added">403         // After padding to mitigate JCC erratum, the displacement may be unaligned again. The</span>
<span class="line-added">404         // previous pass is essential because JCC erratum padding may not trigger without the</span>
<span class="line-added">405         // displacement alignment.</span>
<span class="line-added">406         emitAlignmentForDirectCall(align, callDisplacementOffset, wordSize);</span>
<span class="line-added">407         int beforeCall = position();</span>
<span class="line-added">408         call();</span>
<span class="line-added">409         return beforeCall;</span>
<span class="line-added">410     }</span>
<span class="line-added">411 </span>
<span class="line-added">412     private void emitAlignmentForDirectCall(boolean align, int callDisplacementOffset, int wordSize) {</span>
<span class="line-added">413         if (align) {</span>
<span class="line-added">414             // make sure that the displacement word of the call ends up word aligned</span>
<span class="line-added">415             int offset = position();</span>
<span class="line-added">416             offset += callDisplacementOffset;</span>
<span class="line-added">417             int modulus = wordSize;</span>
<span class="line-added">418             if (offset % modulus != 0) {</span>
<span class="line-added">419                 nop(modulus - offset % modulus);</span>
<span class="line-added">420             }</span>
<span class="line-added">421         }</span>
<span class="line-added">422     }</span>
<span class="line-added">423 </span>
<span class="line-added">424     public final int indirectCall(Register callReg) {</span>
<span class="line-added">425         int bytesToEmit = needsRex(callReg) ? 3 : 2;</span>
<span class="line-added">426         testAndAlign(bytesToEmit);</span>
<span class="line-added">427         int beforeCall = position();</span>
<span class="line-added">428         call(callReg);</span>
<span class="line-added">429         assert beforeCall + bytesToEmit == position();</span>
<span class="line-added">430         return beforeCall;</span>
<span class="line-added">431     }</span>
<span class="line-added">432 </span>
<span class="line-added">433     public final int directCall(long address, Register scratch) {</span>
<span class="line-added">434         int bytesToEmit = needsRex(scratch) ? 13 : 12;</span>
<span class="line-added">435         testAndAlign(bytesToEmit);</span>
<span class="line-added">436         int beforeCall = position();</span>
<span class="line-added">437         movq(scratch, address);</span>
<span class="line-added">438         call(scratch);</span>
<span class="line-added">439         assert beforeCall + bytesToEmit == position();</span>
<span class="line-added">440         return beforeCall;</span>
<span class="line-added">441     }</span>
<span class="line-added">442 </span>
<span class="line-added">443     public final int directJmp(long address, Register scratch) {</span>
<span class="line-added">444         int bytesToEmit = needsRex(scratch) ? 13 : 12;</span>
<span class="line-added">445         testAndAlign(bytesToEmit);</span>
<span class="line-added">446         int beforeJmp = position();</span>
<span class="line-added">447         movq(scratch, address);</span>
<span class="line-added">448         jmpWithoutAlignment(scratch);</span>
<span class="line-added">449         assert beforeJmp + bytesToEmit == position();</span>
<span class="line-added">450         return beforeJmp;</span>
<span class="line-added">451     }</span>
<span class="line-added">452 </span>
<span class="line-added">453     // This should guarantee that the alignment in AMD64Assembler.jcc methods will be not triggered.</span>
<span class="line-added">454     private void alignFusedPair(Label branchTarget, boolean isShortJmp, int prevOpInBytes) {</span>
<span class="line-added">455         assert prevOpInBytes &lt; 26 : &quot;Fused pair may be longer than 0x20 bytes.&quot;;</span>
<span class="line-added">456         if (branchTarget == null) {</span>
<span class="line-added">457             testAndAlign(prevOpInBytes + 6);</span>
<span class="line-added">458         } else if (isShortJmp) {</span>
<span class="line-added">459             testAndAlign(prevOpInBytes + 2);</span>
<span class="line-added">460         } else if (!branchTarget.isBound()) {</span>
<span class="line-added">461             testAndAlign(prevOpInBytes + 6);</span>
<span class="line-added">462         } else {</span>
<span class="line-added">463             long disp = branchTarget.position() - (position() + prevOpInBytes);</span>
<span class="line-added">464             // assuming short jump first</span>
<span class="line-added">465             if (isByte(disp - 2)) {</span>
<span class="line-added">466                 testAndAlign(prevOpInBytes + 2);</span>
<span class="line-added">467                 // After alignment, isByte(disp - shortSize) might not hold. Need to check</span>
<span class="line-added">468                 // again.</span>
<span class="line-added">469                 disp = branchTarget.position() - (position() + prevOpInBytes);</span>
<span class="line-added">470                 if (isByte(disp - 2)) {</span>
<span class="line-added">471                     return;</span>
<span class="line-added">472                 }</span>
<span class="line-added">473             }</span>
<span class="line-added">474             testAndAlign(prevOpInBytes + 6);</span>
<span class="line-added">475         }</span>
<span class="line-added">476     }</span>
<span class="line-added">477 </span>
<span class="line-added">478     private void applyMIOpAndJcc(AMD64MIOp op, OperandSize size, Register src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp, boolean annotateImm,</span>
<span class="line-added">479                     IntConsumer applyBeforeFusedPair) {</span>
<span class="line-added">480         final int bytesToEmit = getPrefixInBytes(size, src, op.srcIsByte) + OPCODE_IN_BYTES + MODRM_IN_BYTES + op.immediateSize(size);</span>
<span class="line-added">481         alignFusedPair(branchTarget, isShortJmp, bytesToEmit);</span>
<span class="line-added">482         final int beforeFusedPair = position();</span>
<span class="line-added">483         if (applyBeforeFusedPair != null) {</span>
<span class="line-added">484             applyBeforeFusedPair.accept(beforeFusedPair);</span>
<span class="line-added">485         }</span>
<span class="line-added">486         op.emit(this, size, src, imm32, annotateImm);</span>
<span class="line-added">487         assert beforeFusedPair + bytesToEmit == position();</span>
<span class="line-added">488         jcc(cc, branchTarget, isShortJmp);</span>
<span class="line-added">489         assert ensureWithinBoundary(beforeFusedPair);</span>
<span class="line-added">490     }</span>
<span class="line-added">491 </span>
<span class="line-added">492     private void applyMIOpAndJcc(AMD64MIOp op, OperandSize size, AMD64Address src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp, boolean annotateImm,</span>
<span class="line-added">493                     IntConsumer applyBeforeFusedPair) {</span>
<span class="line-added">494         final int bytesToEmit = getPrefixInBytes(size, src) + OPCODE_IN_BYTES + addressInBytes(src) + op.immediateSize(size);</span>
<span class="line-added">495         alignFusedPair(branchTarget, isShortJmp, bytesToEmit);</span>
<span class="line-added">496         final int beforeFusedPair = position();</span>
<span class="line-added">497         if (applyBeforeFusedPair != null) {</span>
<span class="line-added">498             applyBeforeFusedPair.accept(beforeFusedPair);</span>
<span class="line-added">499         }</span>
<span class="line-added">500         op.emit(this, size, src, imm32, annotateImm);</span>
<span class="line-added">501         assert beforeFusedPair + bytesToEmit == position();</span>
<span class="line-added">502         jcc(cc, branchTarget, isShortJmp);</span>
<span class="line-added">503         assert ensureWithinBoundary(beforeFusedPair);</span>
<span class="line-added">504     }</span>
<span class="line-added">505 </span>
<span class="line-added">506     private int applyRMOpAndJcc(AMD64RMOp op, OperandSize size, Register src1, Register src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">507         final int bytesToEmit = getPrefixInBytes(size, src1, op.dstIsByte, src2, op.srcIsByte) + OPCODE_IN_BYTES + MODRM_IN_BYTES;</span>
<span class="line-added">508         alignFusedPair(branchTarget, isShortJmp, bytesToEmit);</span>
<span class="line-added">509         final int beforeFusedPair = position();</span>
<span class="line-added">510         op.emit(this, size, src1, src2);</span>
<span class="line-added">511         final int beforeJcc = position();</span>
<span class="line-added">512         assert beforeFusedPair + bytesToEmit == beforeJcc;</span>
<span class="line-added">513         jcc(cc, branchTarget, isShortJmp);</span>
<span class="line-added">514         assert ensureWithinBoundary(beforeFusedPair);</span>
<span class="line-added">515         return beforeJcc;</span>
<span class="line-added">516     }</span>
<span class="line-added">517 </span>
<span class="line-added">518     private int applyRMOpAndJcc(AMD64RMOp op, OperandSize size, Register src1, AMD64Address src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp, IntConsumer applyBeforeFusedPair) {</span>
<span class="line-added">519         final int bytesToEmit = getPrefixInBytes(size, src1, op.dstIsByte, src2) + OPCODE_IN_BYTES + addressInBytes(src2);</span>
<span class="line-added">520         alignFusedPair(branchTarget, isShortJmp, bytesToEmit);</span>
<span class="line-added">521         final int beforeFusedPair = position();</span>
<span class="line-added">522         if (applyBeforeFusedPair != null) {</span>
<span class="line-added">523             applyBeforeFusedPair.accept(beforeFusedPair);</span>
<span class="line-added">524         }</span>
<span class="line-added">525         op.emit(this, size, src1, src2);</span>
<span class="line-added">526         final int beforeJcc = position();</span>
<span class="line-added">527         assert beforeFusedPair + bytesToEmit == beforeJcc;</span>
<span class="line-added">528         jcc(cc, branchTarget, isShortJmp);</span>
<span class="line-added">529         assert ensureWithinBoundary(beforeFusedPair);</span>
<span class="line-added">530         return beforeJcc;</span>
<span class="line-added">531     }</span>
<span class="line-added">532 </span>
<span class="line-added">533     public void applyMOpAndJcc(AMD64MOp op, OperandSize size, Register dst, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">534         final int bytesToEmit = getPrefixInBytes(size, dst, op.srcIsByte) + OPCODE_IN_BYTES + MODRM_IN_BYTES;</span>
<span class="line-added">535         alignFusedPair(branchTarget, isShortJmp, bytesToEmit);</span>
<span class="line-added">536         final int beforeFusedPair = position();</span>
<span class="line-added">537         op.emit(this, size, dst);</span>
<span class="line-added">538         assert beforeFusedPair + bytesToEmit == position();</span>
<span class="line-added">539         jcc(cc, branchTarget, isShortJmp);</span>
<span class="line-added">540         assert ensureWithinBoundary(beforeFusedPair);</span>
<span class="line-added">541     }</span>
<span class="line-added">542 </span>
<span class="line-added">543     public final void testAndJcc(OperandSize size, Register src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">544         applyMIOpAndJcc(AMD64MIOp.TEST, size, src, imm32, cc, branchTarget, isShortJmp, false, null);</span>
<span class="line-added">545     }</span>
<span class="line-added">546 </span>
<span class="line-added">547     public final void testlAndJcc(Register src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">548         applyMIOpAndJcc(AMD64MIOp.TEST, DWORD, src, imm32, cc, branchTarget, isShortJmp, false, null);</span>
<span class="line-added">549     }</span>
<span class="line-added">550 </span>
<span class="line-added">551     public final void testAndJcc(OperandSize size, AMD64Address src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp, IntConsumer applyBeforeFusedPair) {</span>
<span class="line-added">552         applyMIOpAndJcc(AMD64MIOp.TEST, size, src, imm32, cc, branchTarget, isShortJmp, false, applyBeforeFusedPair);</span>
<span class="line-added">553     }</span>
<span class="line-added">554 </span>
<span class="line-added">555     public final void testAndJcc(OperandSize size, Register src1, Register src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">556         applyRMOpAndJcc(AMD64RMOp.TEST, size, src1, src2, cc, branchTarget, isShortJmp);</span>
<span class="line-added">557     }</span>
<span class="line-added">558 </span>
<span class="line-added">559     public final void testlAndJcc(Register src1, Register src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">560         applyRMOpAndJcc(AMD64RMOp.TEST, DWORD, src1, src2, cc, branchTarget, isShortJmp);</span>
<span class="line-added">561     }</span>
<span class="line-added">562 </span>
<span class="line-added">563     public final int testqAndJcc(Register src1, Register src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">564         return applyRMOpAndJcc(AMD64RMOp.TEST, QWORD, src1, src2, cc, branchTarget, isShortJmp);</span>
<span class="line-added">565     }</span>
<span class="line-added">566 </span>
<span class="line-added">567     public final void testAndJcc(OperandSize size, Register src1, AMD64Address src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp, IntConsumer applyBeforeFusedPair) {</span>
<span class="line-added">568         applyRMOpAndJcc(AMD64RMOp.TEST, size, src1, src2, cc, branchTarget, isShortJmp, applyBeforeFusedPair);</span>
<span class="line-added">569     }</span>
<span class="line-added">570 </span>
<span class="line-added">571     public final void testbAndJcc(Register src1, Register src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">572         applyRMOpAndJcc(AMD64RMOp.TESTB, OperandSize.BYTE, src1, src2, cc, branchTarget, isShortJmp);</span>
<span class="line-added">573     }</span>
<span class="line-added">574 </span>
<span class="line-added">575     public final void testbAndJcc(Register src1, AMD64Address src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">576         applyRMOpAndJcc(AMD64RMOp.TESTB, OperandSize.BYTE, src1, src2, cc, branchTarget, isShortJmp, null);</span>
<span class="line-added">577     }</span>
<span class="line-added">578 </span>
<span class="line-added">579     public final void cmpAndJcc(OperandSize size, Register src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp, boolean annotateImm, IntConsumer applyBeforeFusedPair) {</span>
<span class="line-added">580         applyMIOpAndJcc(CMP.getMIOpcode(size, isByte(imm32)), size, src, imm32, cc, branchTarget, isShortJmp, annotateImm, applyBeforeFusedPair);</span>
<span class="line-added">581     }</span>
<span class="line-added">582 </span>
<span class="line-added">583     public final void cmplAndJcc(Register src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">584         applyMIOpAndJcc(CMP.getMIOpcode(DWORD, isByte(imm32)), DWORD, src, imm32, cc, branchTarget, isShortJmp, false, null);</span>
<span class="line-added">585     }</span>
<span class="line-added">586 </span>
<span class="line-added">587     public final void cmpqAndJcc(Register src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">588         applyMIOpAndJcc(CMP.getMIOpcode(QWORD, isByte(imm32)), QWORD, src, imm32, cc, branchTarget, isShortJmp, false, null);</span>
<span class="line-added">589     }</span>
<span class="line-added">590 </span>
<span class="line-added">591     public final void cmpAndJcc(OperandSize size, AMD64Address src, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp, boolean annotateImm, IntConsumer applyBeforeFusedPair) {</span>
<span class="line-added">592         applyMIOpAndJcc(CMP.getMIOpcode(size, NumUtil.isByte(imm32)), size, src, imm32, cc, branchTarget, isShortJmp, annotateImm, applyBeforeFusedPair);</span>
<span class="line-added">593     }</span>
<span class="line-added">594 </span>
<span class="line-added">595     public final void cmpAndJcc(OperandSize size, Register src1, Register src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">596         applyRMOpAndJcc(CMP.getRMOpcode(size), size, src1, src2, cc, branchTarget, isShortJmp);</span>
<span class="line-added">597     }</span>
<span class="line-added">598 </span>
<span class="line-added">599     public final void cmplAndJcc(Register src1, Register src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">600         applyRMOpAndJcc(CMP.getRMOpcode(DWORD), DWORD, src1, src2, cc, branchTarget, isShortJmp);</span>
<span class="line-added">601     }</span>
<span class="line-added">602 </span>
<span class="line-added">603     public final int cmpqAndJcc(Register src1, Register src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">604         return applyRMOpAndJcc(CMP.getRMOpcode(QWORD), QWORD, src1, src2, cc, branchTarget, isShortJmp);</span>
<span class="line-added">605     }</span>
<span class="line-added">606 </span>
<span class="line-added">607     public final void cmpAndJcc(OperandSize size, Register src1, AMD64Address src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp, IntConsumer applyBeforeFusedPair) {</span>
<span class="line-added">608         applyRMOpAndJcc(CMP.getRMOpcode(size), size, src1, src2, cc, branchTarget, isShortJmp, applyBeforeFusedPair);</span>
<span class="line-added">609     }</span>
<span class="line-added">610 </span>
<span class="line-added">611     public final void cmplAndJcc(Register src1, AMD64Address src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">612         applyRMOpAndJcc(CMP.getRMOpcode(DWORD), DWORD, src1, src2, cc, branchTarget, isShortJmp, null);</span>
<span class="line-added">613     }</span>
<span class="line-added">614 </span>
<span class="line-added">615     public final int cmpqAndJcc(Register src1, AMD64Address src2, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">616         return applyRMOpAndJcc(CMP.getRMOpcode(QWORD), QWORD, src1, src2, cc, branchTarget, isShortJmp, null);</span>
<span class="line-added">617     }</span>
<span class="line-added">618 </span>
<span class="line-added">619     public final void cmpAndJcc(OperandSize size, Register src1, Supplier&lt;AMD64Address&gt; src2, ConditionFlag cc, Label branchTarget) {</span>
<span class="line-added">620         AMD64Address placeHolder = getPlaceholder(position());</span>
<span class="line-added">621         final AMD64RMOp op = CMP.getRMOpcode(size);</span>
<span class="line-added">622         final int bytesToEmit = getPrefixInBytes(size, src1, op.dstIsByte, placeHolder) + OPCODE_IN_BYTES + addressInBytes(placeHolder);</span>
<span class="line-added">623         alignFusedPair(branchTarget, false, bytesToEmit);</span>
<span class="line-added">624         final int beforeFusedPair = position();</span>
<span class="line-added">625         AMD64Address src2AsAddress = src2.get();</span>
<span class="line-added">626         op.emit(this, size, src1, src2AsAddress);</span>
<span class="line-added">627         assert beforeFusedPair + bytesToEmit == position();</span>
<span class="line-added">628         jcc(cc, branchTarget, false);</span>
<span class="line-added">629         assert ensureWithinBoundary(beforeFusedPair);</span>
<span class="line-added">630     }</span>
<span class="line-added">631 </span>
<span class="line-added">632     public final void andlAndJcc(Register dst, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">633         applyMIOpAndJcc(AND.getMIOpcode(DWORD, isByte(imm32)), DWORD, dst, imm32, cc, branchTarget, isShortJmp, false, null);</span>
<span class="line-added">634     }</span>
<span class="line-added">635 </span>
<span class="line-added">636     public final void addqAndJcc(Register dst, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">637         applyMIOpAndJcc(ADD.getMIOpcode(QWORD, isByte(imm32)), QWORD, dst, imm32, cc, branchTarget, isShortJmp, false, null);</span>
<span class="line-added">638     }</span>
<span class="line-added">639 </span>
<span class="line-added">640     public final void sublAndJcc(Register dst, Register src, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">641         applyRMOpAndJcc(SUB.getRMOpcode(DWORD), DWORD, dst, src, cc, branchTarget, isShortJmp);</span>
<span class="line-added">642     }</span>
<span class="line-added">643 </span>
<span class="line-added">644     public final void subqAndJcc(Register dst, Register src, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">645         applyRMOpAndJcc(SUB.getRMOpcode(QWORD), QWORD, dst, src, cc, branchTarget, isShortJmp);</span>
<span class="line-added">646     }</span>
<span class="line-added">647 </span>
<span class="line-added">648     public final void sublAndJcc(Register dst, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">649         applyMIOpAndJcc(SUB.getMIOpcode(DWORD, isByte(imm32)), DWORD, dst, imm32, cc, branchTarget, isShortJmp, false, null);</span>
<span class="line-added">650     }</span>
<span class="line-added">651 </span>
<span class="line-added">652     public final void subqAndJcc(Register dst, int imm32, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">653         applyMIOpAndJcc(SUB.getMIOpcode(QWORD, isByte(imm32)), QWORD, dst, imm32, cc, branchTarget, isShortJmp, false, null);</span>
<span class="line-added">654     }</span>
<span class="line-added">655 </span>
<span class="line-added">656     public final void incqAndJcc(Register dst, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">657         applyMOpAndJcc(INC, QWORD, dst, cc, branchTarget, isShortJmp);</span>
<span class="line-added">658     }</span>
<span class="line-added">659 </span>
<span class="line-added">660     public final void decqAndJcc(Register dst, ConditionFlag cc, Label branchTarget, boolean isShortJmp) {</span>
<span class="line-added">661         applyMOpAndJcc(DEC, QWORD, dst, cc, branchTarget, isShortJmp);</span>
<span class="line-added">662     }</span>
663 }
</pre>
</td>
</tr>
</table>
<center><a href="AMD64BaseAssembler.java.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../../../../../../../index.html" target="_top">index</a> <a href="AVXKind.java.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>