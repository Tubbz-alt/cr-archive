<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/jdk.internal.vm.compiler/share/classes/org.graalvm.compiler.asm.amd64/src/org/graalvm/compiler/asm/amd64/AMD64Assembler.java</title>
    <link rel="stylesheet" href="../../../../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2009, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  */
  23 
  24 
  25 package org.graalvm.compiler.asm.amd64;
  26 
  27 import static jdk.vm.ci.amd64.AMD64.CPU;
  28 import static jdk.vm.ci.amd64.AMD64.MASK;
  29 import static jdk.vm.ci.amd64.AMD64.XMM;
  30 import static jdk.vm.ci.amd64.AMD64.r12;
  31 import static jdk.vm.ci.amd64.AMD64.r13;
  32 import static jdk.vm.ci.amd64.AMD64.rbp;
  33 import static jdk.vm.ci.amd64.AMD64.rsp;
  34 import static jdk.vm.ci.amd64.AMD64.CPUFeature.AVX512BW;
  35 import static jdk.vm.ci.amd64.AMD64.CPUFeature.AVX512CD;
  36 import static jdk.vm.ci.amd64.AMD64.CPUFeature.AVX512DQ;
  37 import static jdk.vm.ci.amd64.AMD64.CPUFeature.AVX512F;
  38 import static jdk.vm.ci.amd64.AMD64.CPUFeature.AVX512VL;
  39 import static jdk.vm.ci.code.MemoryBarriers.STORE_LOAD;
  40 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseAddressNop;
  41 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseIntelNops;
  42 import static org.graalvm.compiler.asm.amd64.AMD64AsmOptions.UseNormalNop;
  43 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.ADD;
  44 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.AND;
  45 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.CMP;
  46 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.OR;
  47 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.SBB;
  48 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.SUB;
  49 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64BinaryArithmetic.XOR;
  50 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MOp.DEC;
  51 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MOp.INC;
  52 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MOp.NEG;
  53 import static org.graalvm.compiler.asm.amd64.AMD64Assembler.AMD64MOp.NOT;
  54 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.EVEXPrefixConfig.B0;
  55 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.EVEXPrefixConfig.Z0;
  56 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.EVEXPrefixConfig.Z1;
  57 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.BYTE;
  58 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.DWORD;
  59 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.PD;
  60 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.PS;
  61 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.QWORD;
  62 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.SD;
  63 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.SS;
  64 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.OperandSize.WORD;
  65 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.L128;
  66 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.L256;
  67 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.L512;
  68 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.LZ;
  69 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.M_0F;
  70 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.M_0F38;
  71 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.M_0F3A;
  72 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.P_;
  73 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.P_66;
  74 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.P_F2;
  75 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.P_F3;
  76 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.W0;
  77 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.W1;
  78 import static org.graalvm.compiler.asm.amd64.AMD64BaseAssembler.VEXPrefixConfig.WIG;
  79 import static org.graalvm.compiler.core.common.NumUtil.isByte;
  80 import static org.graalvm.compiler.core.common.NumUtil.isInt;
  81 import static org.graalvm.compiler.core.common.NumUtil.isShiftCount;
  82 import static org.graalvm.compiler.core.common.NumUtil.isUByte;
  83 
  84 import java.util.EnumSet;
  85 
  86 import org.graalvm.compiler.asm.Label;
  87 import org.graalvm.compiler.asm.amd64.AMD64Address.Scale;
  88 import org.graalvm.compiler.asm.amd64.AVXKind.AVXSize;
  89 import org.graalvm.compiler.core.common.calc.Condition;
  90 import org.graalvm.compiler.debug.GraalError;
  91 import org.graalvm.compiler.options.Option;
  92 import org.graalvm.compiler.options.OptionKey;
  93 import org.graalvm.compiler.options.OptionType;
  94 import org.graalvm.compiler.options.OptionValues;
  95 
  96 import jdk.vm.ci.amd64.AMD64;
  97 import jdk.vm.ci.amd64.AMD64.CPUFeature;
  98 import jdk.vm.ci.code.Register;
  99 import jdk.vm.ci.code.Register.RegisterCategory;
 100 import jdk.vm.ci.code.TargetDescription;
 101 
 102 /**
 103  * This class implements an assembler that can encode most X86 instructions.
 104  */
 105 public class AMD64Assembler extends AMD64BaseAssembler {
 106 
 107     public static class Options {
 108         // @formatter:off
 109         @Option(help = &quot;Force branch instructions to align with 32-bytes boundary, to mitigate the jcc erratum. &quot; +
 110                 &quot;See https://www.intel.com/content/dam/support/us/en/documents/processors/mitigations-jump-conditional-code-erratum.pdf for more details.&quot;, type = OptionType.User)
 111         public static final OptionKey&lt;Boolean&gt; UseBranchesWithin32ByteBoundary = new OptionKey&lt;&gt;(false);
 112         // @formatter:on
 113     }
 114 
 115     private final boolean useBranchesWithin32ByteBoundary;
 116 
 117     public interface CodePatchShifter {
 118         void shift(int pos, int bytesToShift);
 119     }
 120 
 121     protected CodePatchShifter codePatchShifter = null;
 122 
 123     public AMD64Assembler(TargetDescription target) {
 124         super(target);
 125         useBranchesWithin32ByteBoundary = false;
 126     }
 127 
 128     /**
 129      * Constructs an assembler for the AMD64 architecture.
 130      */
 131     public AMD64Assembler(TargetDescription target, OptionValues optionValues) {
 132         super(target);
 133         useBranchesWithin32ByteBoundary = Options.UseBranchesWithin32ByteBoundary.getValue(optionValues);
 134     }
 135 
 136     public void setCodePatchShifter(CodePatchShifter codePatchShifter) {
 137         assert this.codePatchShifter == null : &quot;overwriting existing value&quot;;
 138         this.codePatchShifter = codePatchShifter;
 139     }
 140 
 141     /**
 142      * The x86 condition codes used for conditional jumps/moves.
 143      */
 144     public enum ConditionFlag {
 145         Zero(0x4, &quot;|zero|&quot;),
 146         NotZero(0x5, &quot;|nzero|&quot;),
 147         Equal(0x4, &quot;=&quot;),
 148         NotEqual(0x5, &quot;!=&quot;),
 149         Less(0xc, &quot;&lt;&quot;),
 150         LessEqual(0xe, &quot;&lt;=&quot;),
 151         Greater(0xf, &quot;&gt;&quot;),
 152         GreaterEqual(0xd, &quot;&gt;=&quot;),
 153         Below(0x2, &quot;|&lt;|&quot;),
 154         BelowEqual(0x6, &quot;|&lt;=|&quot;),
 155         Above(0x7, &quot;|&gt;|&quot;),
 156         AboveEqual(0x3, &quot;|&gt;=|&quot;),
 157         Overflow(0x0, &quot;|of|&quot;),
 158         NoOverflow(0x1, &quot;|nof|&quot;),
 159         CarrySet(0x2, &quot;|carry|&quot;),
 160         CarryClear(0x3, &quot;|ncarry|&quot;),
 161         Negative(0x8, &quot;|neg|&quot;),
 162         Positive(0x9, &quot;|pos|&quot;),
 163         Parity(0xa, &quot;|par|&quot;),
 164         NoParity(0xb, &quot;|npar|&quot;);
 165 
 166         private final int value;
 167         private final String operator;
 168 
 169         ConditionFlag(int value, String operator) {
 170             this.value = value;
 171             this.operator = operator;
 172         }
 173 
 174         public ConditionFlag negate() {
 175             switch (this) {
 176                 case Zero:
 177                     return NotZero;
 178                 case NotZero:
 179                     return Zero;
 180                 case Equal:
 181                     return NotEqual;
 182                 case NotEqual:
 183                     return Equal;
 184                 case Less:
 185                     return GreaterEqual;
 186                 case LessEqual:
 187                     return Greater;
 188                 case Greater:
 189                     return LessEqual;
 190                 case GreaterEqual:
 191                     return Less;
 192                 case Below:
 193                     return AboveEqual;
 194                 case BelowEqual:
 195                     return Above;
 196                 case Above:
 197                     return BelowEqual;
 198                 case AboveEqual:
 199                     return Below;
 200                 case Overflow:
 201                     return NoOverflow;
 202                 case NoOverflow:
 203                     return Overflow;
 204                 case CarrySet:
 205                     return CarryClear;
 206                 case CarryClear:
 207                     return CarrySet;
 208                 case Negative:
 209                     return Positive;
 210                 case Positive:
 211                     return Negative;
 212                 case Parity:
 213                     return NoParity;
 214                 case NoParity:
 215                     return Parity;
 216             }
 217             throw new IllegalArgumentException();
 218         }
 219 
 220         public int getValue() {
 221             return value;
 222         }
 223 
 224         @Override
 225         public String toString() {
 226             return operator;
 227         }
 228     }
 229 
 230     /**
 231      * Operand size and register type constraints.
 232      */
 233     private enum OpAssertion {
 234         ByteAssertion(CPU, CPU, BYTE),
 235         ByteOrLargerAssertion(CPU, CPU, BYTE, WORD, DWORD, QWORD),
 236         WordOrLargerAssertion(CPU, CPU, WORD, DWORD, QWORD),
 237         DwordOrLargerAssertion(CPU, CPU, DWORD, QWORD),
 238         WordOrDwordAssertion(CPU, CPU, WORD, QWORD),
 239         QwordAssertion(CPU, CPU, QWORD),
 240         FloatAssertion(XMM, XMM, SS, SD, PS, PD),
 241         PackedFloatAssertion(XMM, XMM, PS, PD),
 242         SingleAssertion(XMM, XMM, SS),
 243         DoubleAssertion(XMM, XMM, SD),
 244         PackedDoubleAssertion(XMM, XMM, PD),
 245         IntToFloatAssertion(XMM, CPU, DWORD, QWORD),
 246         FloatToIntAssertion(CPU, XMM, DWORD, QWORD);
 247 
 248         private final RegisterCategory resultCategory;
 249         private final RegisterCategory inputCategory;
 250         private final OperandSize[] allowedSizes;
 251 
 252         OpAssertion(RegisterCategory resultCategory, RegisterCategory inputCategory, OperandSize... allowedSizes) {
 253             this.resultCategory = resultCategory;
 254             this.inputCategory = inputCategory;
 255             this.allowedSizes = allowedSizes;
 256         }
 257 
 258         protected boolean checkOperands(AMD64Op op, OperandSize size, Register resultReg, Register inputReg) {
 259             assert resultReg == null || resultCategory.equals(resultReg.getRegisterCategory()) : &quot;invalid result register &quot; + resultReg + &quot; used in &quot; + op;
 260             assert inputReg == null || inputCategory.equals(inputReg.getRegisterCategory()) : &quot;invalid input register &quot; + inputReg + &quot; used in &quot; + op;
 261 
 262             for (OperandSize s : allowedSizes) {
 263                 if (size == s) {
 264                     return true;
 265                 }
 266             }
 267 
 268             assert false : &quot;invalid operand size &quot; + size + &quot; used in &quot; + op;
 269             return false;
 270         }
 271 
 272     }
 273 
 274     protected static final int P_0F = 0x0F;
 275     protected static final int P_0F38 = 0x380F;
 276     protected static final int P_0F3A = 0x3A0F;
 277 
 278     /**
 279      * Base class for AMD64 opcodes.
 280      */
 281     public static class AMD64Op {
 282 
 283         private final String opcode;
 284 
 285         protected final int prefix1;
 286         protected final int prefix2;
 287         protected final int op;
 288 
 289         final boolean dstIsByte;
 290         final boolean srcIsByte;
 291 
 292         private final OpAssertion assertion;
 293         private final CPUFeature feature;
 294 
 295         protected AMD64Op(String opcode, int prefix1, int prefix2, int op, OpAssertion assertion, CPUFeature feature) {
 296             this(opcode, prefix1, prefix2, op, assertion == OpAssertion.ByteAssertion, assertion == OpAssertion.ByteAssertion, assertion, feature);
 297         }
 298 
 299         protected AMD64Op(String opcode, int prefix1, int prefix2, int op, boolean dstIsByte, boolean srcIsByte, OpAssertion assertion, CPUFeature feature) {
 300             this.opcode = opcode;
 301             this.prefix1 = prefix1;
 302             this.prefix2 = prefix2;
 303             this.op = op;
 304 
 305             this.dstIsByte = dstIsByte;
 306             this.srcIsByte = srcIsByte;
 307 
 308             this.assertion = assertion;
 309             this.feature = feature;
 310         }
 311 
 312         protected final void emitOpcode(AMD64Assembler asm, OperandSize size, int rxb, int dstEnc, int srcEnc) {
 313             if (prefix1 != 0) {
 314                 asm.emitByte(prefix1);
 315             }
 316             if (size.getSizePrefix() != 0) {
 317                 asm.emitByte(size.getSizePrefix());
 318             }
 319             int rexPrefix = 0x40 | rxb;
 320             if (size == QWORD) {
 321                 rexPrefix |= 0x08;
 322             }
 323             if (rexPrefix != 0x40 || (dstIsByte &amp;&amp; dstEnc &gt;= 4) || (srcIsByte &amp;&amp; srcEnc &gt;= 4)) {
 324                 asm.emitByte(rexPrefix);
 325             }
 326             if (prefix2 &gt; 0xFF) {
 327                 asm.emitShort(prefix2);
 328             } else if (prefix2 &gt; 0) {
 329                 asm.emitByte(prefix2);
 330             }
 331             asm.emitByte(op);
 332         }
 333 
 334         protected final boolean verify(AMD64Assembler asm, OperandSize size, Register resultReg, Register inputReg) {
 335             assert feature == null || asm.supports(feature) : String.format(&quot;unsupported feature %s required for %s&quot;, feature, opcode);
 336             assert assertion.checkOperands(this, size, resultReg, inputReg);
 337             return true;
 338         }
 339 
 340         public OperandSize[] getAllowedSizes() {
 341             return assertion.allowedSizes;
 342         }
 343 
 344         protected final boolean isSSEInstruction() {
 345             if (feature == null) {
 346                 return false;
 347             }
 348             switch (feature) {
 349                 case SSE:
 350                 case SSE2:
 351                 case SSE3:
 352                 case SSSE3:
 353                 case SSE4A:
 354                 case SSE4_1:
 355                 case SSE4_2:
 356                     return true;
 357                 default:
 358                     return false;
 359             }
 360         }
 361 
 362         public final OpAssertion getAssertion() {
 363             return assertion;
 364         }
 365 
 366         @Override
 367         public String toString() {
 368             return opcode;
 369         }
 370     }
 371 
 372     /**
 373      * Base class for AMD64 opcodes with immediate operands.
 374      */
 375     public static class AMD64ImmOp extends AMD64Op {
 376 
 377         private final boolean immIsByte;
 378 
 379         protected AMD64ImmOp(String opcode, boolean immIsByte, int prefix, int op, OpAssertion assertion) {
 380             this(opcode, immIsByte, prefix, op, assertion, null);
 381         }
 382 
 383         protected AMD64ImmOp(String opcode, boolean immIsByte, int prefix, int op, OpAssertion assertion, CPUFeature feature) {
 384             super(opcode, 0, prefix, op, assertion, feature);
 385             this.immIsByte = immIsByte;
 386         }
 387 
 388         protected final void emitImmediate(AMD64Assembler asm, OperandSize size, int imm) {
 389             if (immIsByte) {
 390                 assert imm == (byte) imm;
 391                 asm.emitByte(imm);
 392             } else {
 393                 size.emitImmediate(asm, imm);
 394             }
 395         }
 396 
 397         public final int immediateSize(OperandSize size) {
 398             if (immIsByte) {
 399                 return 1;
 400             } else {
 401                 return size.immediateSize();
 402             }
 403         }
 404     }
 405 
 406     /**
 407      * Opcode with operand order of either RM or MR for 2 address forms.
 408      */
 409     public abstract static class AMD64RROp extends AMD64Op {
 410 
 411         protected AMD64RROp(String opcode, int prefix1, int prefix2, int op, OpAssertion assertion, CPUFeature feature) {
 412             super(opcode, prefix1, prefix2, op, assertion, feature);
 413         }
 414 
 415         protected AMD64RROp(String opcode, int prefix1, int prefix2, int op, boolean dstIsByte, boolean srcIsByte, OpAssertion assertion, CPUFeature feature) {
 416             super(opcode, prefix1, prefix2, op, dstIsByte, srcIsByte, assertion, feature);
 417         }
 418 
 419         public abstract void emit(AMD64Assembler asm, OperandSize size, Register dst, Register src);
 420     }
 421 
 422     /**
 423      * Opcode with operand order of RM.
 424      */
 425     public static class AMD64RMOp extends AMD64RROp {
 426         // @formatter:off
 427         public static final AMD64RMOp IMUL   = new AMD64RMOp(&quot;IMUL&quot;,         P_0F, 0xAF, OpAssertion.ByteOrLargerAssertion);
 428         public static final AMD64RMOp BSF    = new AMD64RMOp(&quot;BSF&quot;,          P_0F, 0xBC);
 429         public static final AMD64RMOp BSR    = new AMD64RMOp(&quot;BSR&quot;,          P_0F, 0xBD);
 430         // POPCNT, TZCNT, and LZCNT support word operation. However, the legacy size prefix should
 431         // be emitted before the mandatory prefix 0xF3. Since we are not emitting bit count for
 432         // 16-bit operands, here we simply use DwordOrLargerAssertion.
 433         public static final AMD64RMOp POPCNT = new AMD64RMOp(&quot;POPCNT&quot;, 0xF3, P_0F, 0xB8, OpAssertion.DwordOrLargerAssertion, CPUFeature.POPCNT);
 434         public static final AMD64RMOp TZCNT  = new AMD64RMOp(&quot;TZCNT&quot;,  0xF3, P_0F, 0xBC, OpAssertion.DwordOrLargerAssertion, CPUFeature.BMI1);
 435         public static final AMD64RMOp LZCNT  = new AMD64RMOp(&quot;LZCNT&quot;,  0xF3, P_0F, 0xBD, OpAssertion.DwordOrLargerAssertion, CPUFeature.LZCNT);
 436         public static final AMD64RMOp MOVZXB = new AMD64RMOp(&quot;MOVZXB&quot;,       P_0F, 0xB6, false, true, OpAssertion.WordOrLargerAssertion);
 437         public static final AMD64RMOp MOVZX  = new AMD64RMOp(&quot;MOVZX&quot;,        P_0F, 0xB7, OpAssertion.DwordOrLargerAssertion);
 438         public static final AMD64RMOp MOVSXB = new AMD64RMOp(&quot;MOVSXB&quot;,       P_0F, 0xBE, false, true, OpAssertion.WordOrLargerAssertion);
 439         public static final AMD64RMOp MOVSX  = new AMD64RMOp(&quot;MOVSX&quot;,        P_0F, 0xBF, OpAssertion.DwordOrLargerAssertion);
 440         public static final AMD64RMOp MOVSXD = new AMD64RMOp(&quot;MOVSXD&quot;,             0x63, OpAssertion.QwordAssertion);
 441         public static final AMD64RMOp MOVB   = new AMD64RMOp(&quot;MOVB&quot;,               0x8A, OpAssertion.ByteAssertion);
 442         public static final AMD64RMOp MOV    = new AMD64RMOp(&quot;MOV&quot;,                0x8B);
 443         public static final AMD64RMOp CMP    = new AMD64RMOp(&quot;CMP&quot;,                0x3B);
 444 
 445         // MOVD/MOVQ and MOVSS/MOVSD are the same opcode, just with different operand size prefix
 446         public static final AMD64RMOp MOVD   = new AMD64RMOp(&quot;MOVD&quot;,   0x66, P_0F, 0x6E, OpAssertion.IntToFloatAssertion, CPUFeature.SSE2);
 447         public static final AMD64RMOp MOVQ   = new AMD64RMOp(&quot;MOVQ&quot;,   0x66, P_0F, 0x6E, OpAssertion.IntToFloatAssertion, CPUFeature.SSE2);
 448         public static final AMD64RMOp MOVSS  = new AMD64RMOp(&quot;MOVSS&quot;,        P_0F, 0x10, OpAssertion.FloatAssertion, CPUFeature.SSE);
 449         public static final AMD64RMOp MOVSD  = new AMD64RMOp(&quot;MOVSD&quot;,        P_0F, 0x10, OpAssertion.FloatAssertion, CPUFeature.SSE);
 450 
 451         // TEST is documented as MR operation, but it&#39;s symmetric, and using it as RM operation is more convenient.
 452         public static final AMD64RMOp TESTB  = new AMD64RMOp(&quot;TEST&quot;,               0x84, OpAssertion.ByteAssertion);
 453         public static final AMD64RMOp TEST   = new AMD64RMOp(&quot;TEST&quot;,               0x85);
 454         // @formatter:on
 455 
 456         protected AMD64RMOp(String opcode, int op) {
 457             this(opcode, 0, op);
 458         }
 459 
 460         protected AMD64RMOp(String opcode, int op, OpAssertion assertion) {
 461             this(opcode, 0, op, assertion);
 462         }
 463 
 464         protected AMD64RMOp(String opcode, int prefix, int op) {
 465             this(opcode, 0, prefix, op, null);
 466         }
 467 
 468         protected AMD64RMOp(String opcode, int prefix, int op, OpAssertion assertion) {
 469             this(opcode, 0, prefix, op, assertion, null);
 470         }
 471 
 472         protected AMD64RMOp(String opcode, int prefix, int op, OpAssertion assertion, CPUFeature feature) {
 473             this(opcode, 0, prefix, op, assertion, feature);
 474         }
 475 
 476         protected AMD64RMOp(String opcode, int prefix, int op, boolean dstIsByte, boolean srcIsByte, OpAssertion assertion) {
 477             super(opcode, 0, prefix, op, dstIsByte, srcIsByte, assertion, null);
 478         }
 479 
 480         protected AMD64RMOp(String opcode, int prefix1, int prefix2, int op, CPUFeature feature) {
 481             this(opcode, prefix1, prefix2, op, OpAssertion.WordOrLargerAssertion, feature);
 482         }
 483 
 484         protected AMD64RMOp(String opcode, int prefix1, int prefix2, int op, OpAssertion assertion, CPUFeature feature) {
 485             super(opcode, prefix1, prefix2, op, assertion, feature);
 486         }
 487 
 488         @Override
 489         public final void emit(AMD64Assembler asm, OperandSize size, Register dst, Register src) {
 490             assert verify(asm, size, dst, src);
 491             if (isSSEInstruction()) {
 492                 Register nds = Register.None;
 493                 switch (op) {
 494                     case 0x10:
 495                     case 0x51:
 496                         if ((size == SS) || (size == SD)) {
 497                             nds = dst;
 498                         }
 499                         break;
 500                     case 0x2A:
 501                     case 0x54:
 502                     case 0x55:
 503                     case 0x56:
 504                     case 0x57:
 505                     case 0x58:
 506                     case 0x59:
 507                     case 0x5A:
 508                     case 0x5C:
 509                     case 0x5D:
 510                     case 0x5E:
 511                     case 0x5F:
 512                         nds = dst;
 513                         break;
 514                     default:
 515                         break;
 516                 }
 517                 asm.simdPrefix(dst, nds, src, size, prefix1, prefix2, size == QWORD);
 518                 asm.emitByte(op);
 519                 asm.emitModRM(dst, src);
 520             } else {
 521                 emitOpcode(asm, size, getRXB(dst, src), dst.encoding, src.encoding);
 522                 asm.emitModRM(dst, src);
 523             }
 524         }
 525 
 526         public final void emit(AMD64Assembler asm, OperandSize size, Register dst, AMD64Address src) {
 527             assert verify(asm, size, dst, null);
 528             if (isSSEInstruction()) {
 529                 Register nds = Register.None;
 530                 switch (op) {
 531                     case 0x51:
 532                         if ((size == SS) || (size == SD)) {
 533                             nds = dst;
 534                         }
 535                         break;
 536                     case 0x2A:
 537                     case 0x54:
 538                     case 0x55:
 539                     case 0x56:
 540                     case 0x57:
 541                     case 0x58:
 542                     case 0x59:
 543                     case 0x5A:
 544                     case 0x5C:
 545                     case 0x5D:
 546                     case 0x5E:
 547                     case 0x5F:
 548                         nds = dst;
 549                         break;
 550                     default:
 551                         break;
 552                 }
 553                 asm.simdPrefix(dst, nds, src, size, prefix1, prefix2, size == QWORD);
 554                 asm.emitByte(op);
 555                 asm.emitOperandHelper(dst, src, 0);
 556             } else {
 557                 emitOpcode(asm, size, getRXB(dst, src), dst.encoding, 0);
 558                 asm.emitOperandHelper(dst, src, 0);
 559             }
 560         }
 561     }
 562 
 563     /**
 564      * Opcode with operand order of MR.
 565      */
 566     public static class AMD64MROp extends AMD64RROp {
 567         // @formatter:off
 568         public static final AMD64MROp MOVB   = new AMD64MROp(&quot;MOVB&quot;,               0x88, OpAssertion.ByteAssertion);
 569         public static final AMD64MROp MOV    = new AMD64MROp(&quot;MOV&quot;,                0x89);
 570 
 571         // MOVD and MOVQ are the same opcode, just with different operand size prefix
 572         // Note that as MR opcodes, they have reverse operand order, so the IntToFloatingAssertion must be used.
 573         public static final AMD64MROp MOVD   = new AMD64MROp(&quot;MOVD&quot;,   0x66, P_0F, 0x7E, OpAssertion.IntToFloatAssertion, CPUFeature.SSE2);
 574         public static final AMD64MROp MOVQ   = new AMD64MROp(&quot;MOVQ&quot;,   0x66, P_0F, 0x7E, OpAssertion.IntToFloatAssertion, CPUFeature.SSE2);
 575 
 576         // MOVSS and MOVSD are the same opcode, just with different operand size prefix
 577         public static final AMD64MROp MOVSS  = new AMD64MROp(&quot;MOVSS&quot;,        P_0F, 0x11, OpAssertion.FloatAssertion, CPUFeature.SSE);
 578         public static final AMD64MROp MOVSD  = new AMD64MROp(&quot;MOVSD&quot;,        P_0F, 0x11, OpAssertion.FloatAssertion, CPUFeature.SSE);
 579         // @formatter:on
 580 
 581         protected AMD64MROp(String opcode, int op) {
 582             this(opcode, 0, op);
 583         }
 584 
 585         protected AMD64MROp(String opcode, int op, OpAssertion assertion) {
 586             this(opcode, 0, op, assertion);
 587         }
 588 
 589         protected AMD64MROp(String opcode, int prefix, int op) {
 590             this(opcode, prefix, op, OpAssertion.WordOrLargerAssertion);
 591         }
 592 
 593         protected AMD64MROp(String opcode, int prefix, int op, OpAssertion assertion) {
 594             this(opcode, prefix, op, assertion, null);
 595         }
 596 
 597         protected AMD64MROp(String opcode, int prefix, int op, OpAssertion assertion, CPUFeature feature) {
 598             this(opcode, 0, prefix, op, assertion, feature);
 599         }
 600 
 601         protected AMD64MROp(String opcode, int prefix1, int prefix2, int op, OpAssertion assertion, CPUFeature feature) {
 602             super(opcode, prefix1, prefix2, op, assertion, feature);
 603         }
 604 
 605         @Override
 606         public final void emit(AMD64Assembler asm, OperandSize size, Register dst, Register src) {
 607             assert verify(asm, size, src, dst);
 608             if (isSSEInstruction()) {
 609                 Register nds = Register.None;
 610                 switch (op) {
 611                     case 0x11:
 612                         if ((size == SS) || (size == SD)) {
 613                             nds = src;
 614                         }
 615                         break;
 616                     default:
 617                         break;
 618                 }
 619                 asm.simdPrefix(src, nds, dst, size, prefix1, prefix2, size == QWORD);
 620                 asm.emitByte(op);
 621                 asm.emitModRM(src, dst);
 622             } else {
 623                 emitOpcode(asm, size, getRXB(src, dst), src.encoding, dst.encoding);
 624                 asm.emitModRM(src, dst);
 625             }
 626         }
 627 
 628         public final void emit(AMD64Assembler asm, OperandSize size, AMD64Address dst, Register src) {
 629             assert verify(asm, size, src, null);
 630             if (isSSEInstruction()) {
 631                 asm.simdPrefix(src, Register.None, dst, size, prefix1, prefix2, size == QWORD);
 632                 asm.emitByte(op);
 633             } else {
 634                 emitOpcode(asm, size, getRXB(src, dst), src.encoding, 0);
 635             }
 636             asm.emitOperandHelper(src, dst, 0);
 637         }
 638     }
 639 
 640     /**
 641      * Opcodes with operand order of M.
 642      */
 643     public static final class AMD64MOp extends AMD64Op {
 644         // @formatter:off
 645         public static final AMD64MOp NOT  = new AMD64MOp(&quot;NOT&quot;,  0xF7, 2);
 646         public static final AMD64MOp NEG  = new AMD64MOp(&quot;NEG&quot;,  0xF7, 3);
 647         public static final AMD64MOp MUL  = new AMD64MOp(&quot;MUL&quot;,  0xF7, 4);
 648         public static final AMD64MOp IMUL = new AMD64MOp(&quot;IMUL&quot;, 0xF7, 5);
 649         public static final AMD64MOp DIV  = new AMD64MOp(&quot;DIV&quot;,  0xF7, 6);
 650         public static final AMD64MOp IDIV = new AMD64MOp(&quot;IDIV&quot;, 0xF7, 7);
 651         public static final AMD64MOp INC  = new AMD64MOp(&quot;INC&quot;,  0xFF, 0);
 652         public static final AMD64MOp DEC  = new AMD64MOp(&quot;DEC&quot;,  0xFF, 1);
 653         public static final AMD64MOp PUSH = new AMD64MOp(&quot;PUSH&quot;, 0xFF, 6);
 654         public static final AMD64MOp POP  = new AMD64MOp(&quot;POP&quot;,  0x8F, 0, OpAssertion.WordOrDwordAssertion);
 655         // @formatter:on
 656 
 657         private final int ext;
 658 
 659         protected AMD64MOp(String opcode, int op, int ext) {
 660             this(opcode, 0, op, ext, OpAssertion.WordOrLargerAssertion);
 661         }
 662 
 663         protected AMD64MOp(String opcode, int op, int ext, OpAssertion assertion) {
 664             this(opcode, 0, op, ext, assertion);
 665         }
 666 
 667         protected AMD64MOp(String opcode, int prefix, int op, int ext, OpAssertion assertion) {
 668             super(opcode, 0, prefix, op, assertion, null);
 669             this.ext = ext;
 670         }
 671 
 672         public void emit(AMD64Assembler asm, OperandSize size, Register dst) {
 673             assert verify(asm, size, dst, null);
 674             emitOpcode(asm, size, getRXB(null, dst), 0, dst.encoding);
 675             asm.emitModRM(ext, dst);
 676         }
 677 
 678         public void emit(AMD64Assembler asm, OperandSize size, AMD64Address dst) {
 679             assert verify(asm, size, null, null);
 680             emitOpcode(asm, size, getRXB(null, dst), 0, 0);
 681             asm.emitOperandHelper(ext, dst, 0);
 682         }
 683     }
 684 
 685     /**
 686      * Opcodes with operand order of MI.
 687      */
 688     public static class AMD64MIOp extends AMD64ImmOp {
 689         // @formatter:off
 690         public static final AMD64MIOp MOVB = new AMD64MIOp(&quot;MOVB&quot;, true,  0xC6, 0, OpAssertion.ByteAssertion);
 691         public static final AMD64MIOp MOV  = new AMD64MIOp(&quot;MOV&quot;,  false, 0xC7, 0);
 692         public static final AMD64MIOp TEST = new AMD64MIOp(&quot;TEST&quot;, false, 0xF7, 0);
 693         // @formatter:on
 694 
 695         private final int ext;
 696 
 697         protected AMD64MIOp(String opcode, boolean immIsByte, int op, int ext) {
 698             this(opcode, immIsByte, op, ext, OpAssertion.WordOrLargerAssertion);
 699         }
 700 
 701         protected AMD64MIOp(String opcode, boolean immIsByte, int op, int ext, OpAssertion assertion) {
 702             this(opcode, immIsByte, 0, op, ext, assertion);
 703         }
 704 
 705         protected AMD64MIOp(String opcode, boolean immIsByte, int prefix, int op, int ext, OpAssertion assertion) {
 706             super(opcode, immIsByte, prefix, op, assertion);
 707             this.ext = ext;
 708         }
 709 
 710         public final void emit(AMD64Assembler asm, OperandSize size, Register dst, int imm) {
 711             emit(asm, size, dst, imm, false);
 712         }
 713 
 714         public final void emit(AMD64Assembler asm, OperandSize size, Register dst, int imm, boolean annotateImm) {
 715             assert verify(asm, size, dst, null);
 716             int insnPos = asm.position();
 717             emitOpcode(asm, size, getRXB(null, dst), 0, dst.encoding);
 718             asm.emitModRM(ext, dst);
 719             int immPos = asm.position();
 720             emitImmediate(asm, size, imm);
 721             int nextInsnPos = asm.position();
 722             if (annotateImm &amp;&amp; asm.codePatchingAnnotationConsumer != null) {
 723                 asm.codePatchingAnnotationConsumer.accept(new OperandDataAnnotation(insnPos, immPos, nextInsnPos - immPos, nextInsnPos));
 724             }
 725         }
 726 
 727         public final void emit(AMD64Assembler asm, OperandSize size, AMD64Address dst, int imm) {
 728             emit(asm, size, dst, imm, false);
 729         }
 730 
 731         public final void emit(AMD64Assembler asm, OperandSize size, AMD64Address dst, int imm, boolean annotateImm) {
 732             assert verify(asm, size, null, null);
 733             int insnPos = asm.position();
 734             emitOpcode(asm, size, getRXB(null, dst), 0, 0);
 735             asm.emitOperandHelper(ext, dst, immediateSize(size));
 736             int immPos = asm.position();
 737             emitImmediate(asm, size, imm);
 738             int nextInsnPos = asm.position();
 739             if (annotateImm &amp;&amp; asm.codePatchingAnnotationConsumer != null) {
 740                 asm.codePatchingAnnotationConsumer.accept(new OperandDataAnnotation(insnPos, immPos, nextInsnPos - immPos, nextInsnPos));
 741             }
 742         }
 743     }
 744 
 745     /**
 746      * Opcodes with operand order of RMI.
 747      *
 748      * We only have one form of round as the operation is always treated with single variant input,
 749      * making its extension to 3 address forms redundant.
 750      */
 751     public static class AMD64RMIOp extends AMD64ImmOp {
 752         // @formatter:off
 753         public static final AMD64RMIOp IMUL    = new AMD64RMIOp(&quot;IMUL&quot;, false, 0x69);
 754         public static final AMD64RMIOp IMUL_SX = new AMD64RMIOp(&quot;IMUL&quot;, true,  0x6B);
 755         public static final AMD64RMIOp ROUNDSS = new AMD64RMIOp(&quot;ROUNDSS&quot;, true, P_0F3A, 0x0A, OpAssertion.PackedDoubleAssertion, CPUFeature.SSE4_1);
 756         public static final AMD64RMIOp ROUNDSD = new AMD64RMIOp(&quot;ROUNDSD&quot;, true, P_0F3A, 0x0B, OpAssertion.PackedDoubleAssertion, CPUFeature.SSE4_1);
 757         // @formatter:on
 758 
 759         protected AMD64RMIOp(String opcode, boolean immIsByte, int op) {
 760             this(opcode, immIsByte, 0, op, OpAssertion.WordOrLargerAssertion, null);
 761         }
 762 
 763         protected AMD64RMIOp(String opcode, boolean immIsByte, int prefix, int op, OpAssertion assertion, CPUFeature feature) {
 764             super(opcode, immIsByte, prefix, op, assertion, feature);
 765         }
 766 
 767         public final void emit(AMD64Assembler asm, OperandSize size, Register dst, Register src, int imm) {
 768             assert verify(asm, size, dst, src);
 769             if (isSSEInstruction()) {
 770                 Register nds = Register.None;
 771                 switch (op) {
 772                     case 0x0A:
 773                     case 0x0B:
 774                         nds = dst;
 775                         break;
 776                     default:
 777                         break;
 778                 }
 779                 asm.simdPrefix(dst, nds, src, size, prefix1, prefix2, false);
 780                 asm.emitByte(op);
 781                 asm.emitModRM(dst, src);
 782             } else {
 783                 emitOpcode(asm, size, getRXB(dst, src), dst.encoding, src.encoding);
 784                 asm.emitModRM(dst, src);
 785             }
 786             emitImmediate(asm, size, imm);
 787         }
 788 
 789         public final void emit(AMD64Assembler asm, OperandSize size, Register dst, AMD64Address src, int imm) {
 790             assert verify(asm, size, dst, null);
 791             if (isSSEInstruction()) {
 792                 Register nds = Register.None;
 793                 switch (op) {
 794                     case 0x0A:
 795                     case 0x0B:
 796                         nds = dst;
 797                         break;
 798                     default:
 799                         break;
 800                 }
 801                 asm.simdPrefix(dst, nds, src, size, prefix1, prefix2, false);
 802                 asm.emitByte(op);
 803             } else {
 804                 emitOpcode(asm, size, getRXB(dst, src), dst.encoding, 0);
 805             }
 806             asm.emitOperandHelper(dst, src, immediateSize(size));
 807             emitImmediate(asm, size, imm);
 808         }
 809     }
 810 
 811     public static class SSEOp extends AMD64RMOp {
 812         // @formatter:off
 813         public static final SSEOp CVTSI2SS  = new SSEOp(&quot;CVTSI2SS&quot;,  0xF3, P_0F, 0x2A, OpAssertion.IntToFloatAssertion);
 814         public static final SSEOp CVTSI2SD  = new SSEOp(&quot;CVTSI2SD&quot;,  0xF2, P_0F, 0x2A, OpAssertion.IntToFloatAssertion);
 815         public static final SSEOp CVTTSS2SI = new SSEOp(&quot;CVTTSS2SI&quot;, 0xF3, P_0F, 0x2C, OpAssertion.FloatToIntAssertion);
 816         public static final SSEOp CVTTSD2SI = new SSEOp(&quot;CVTTSD2SI&quot;, 0xF2, P_0F, 0x2C, OpAssertion.FloatToIntAssertion);
 817         public static final SSEOp UCOMIS    = new SSEOp(&quot;UCOMIS&quot;,          P_0F, 0x2E, OpAssertion.PackedFloatAssertion);
 818         public static final SSEOp SQRT      = new SSEOp(&quot;SQRT&quot;,            P_0F, 0x51);
 819         public static final SSEOp AND       = new SSEOp(&quot;AND&quot;,             P_0F, 0x54, OpAssertion.PackedFloatAssertion);
 820         public static final SSEOp ANDN      = new SSEOp(&quot;ANDN&quot;,            P_0F, 0x55, OpAssertion.PackedFloatAssertion);
 821         public static final SSEOp OR        = new SSEOp(&quot;OR&quot;,              P_0F, 0x56, OpAssertion.PackedFloatAssertion);
 822         public static final SSEOp XOR       = new SSEOp(&quot;XOR&quot;,             P_0F, 0x57, OpAssertion.PackedFloatAssertion);
 823         public static final SSEOp ADD       = new SSEOp(&quot;ADD&quot;,             P_0F, 0x58);
 824         public static final SSEOp MUL       = new SSEOp(&quot;MUL&quot;,             P_0F, 0x59);
 825         public static final SSEOp CVTSS2SD  = new SSEOp(&quot;CVTSS2SD&quot;,        P_0F, 0x5A, OpAssertion.SingleAssertion);
 826         public static final SSEOp CVTSD2SS  = new SSEOp(&quot;CVTSD2SS&quot;,        P_0F, 0x5A, OpAssertion.DoubleAssertion);
 827         public static final SSEOp SUB       = new SSEOp(&quot;SUB&quot;,             P_0F, 0x5C);
 828         public static final SSEOp MIN       = new SSEOp(&quot;MIN&quot;,             P_0F, 0x5D);
 829         public static final SSEOp DIV       = new SSEOp(&quot;DIV&quot;,             P_0F, 0x5E);
 830         public static final SSEOp MAX       = new SSEOp(&quot;MAX&quot;,             P_0F, 0x5F);
 831         // @formatter:on
 832 
 833         protected SSEOp(String opcode, int prefix, int op) {
 834             this(opcode, prefix, op, OpAssertion.FloatAssertion);
 835         }
 836 
 837         protected SSEOp(String opcode, int prefix, int op, OpAssertion assertion) {
 838             this(opcode, 0, prefix, op, assertion);
 839         }
 840 
 841         protected SSEOp(String opcode, int mandatoryPrefix, int prefix, int op, OpAssertion assertion) {
 842             super(opcode, mandatoryPrefix, prefix, op, assertion, CPUFeature.SSE2);
 843         }
 844     }
 845 
 846     /**
 847      * Arithmetic operation with operand order of RM, MR or MI.
 848      */
 849     public static final class AMD64BinaryArithmetic {
 850         // @formatter:off
 851         public static final AMD64BinaryArithmetic ADD = new AMD64BinaryArithmetic(&quot;ADD&quot;, 0);
 852         public static final AMD64BinaryArithmetic OR  = new AMD64BinaryArithmetic(&quot;OR&quot;,  1);
 853         public static final AMD64BinaryArithmetic ADC = new AMD64BinaryArithmetic(&quot;ADC&quot;, 2);
 854         public static final AMD64BinaryArithmetic SBB = new AMD64BinaryArithmetic(&quot;SBB&quot;, 3);
 855         public static final AMD64BinaryArithmetic AND = new AMD64BinaryArithmetic(&quot;AND&quot;, 4);
 856         public static final AMD64BinaryArithmetic SUB = new AMD64BinaryArithmetic(&quot;SUB&quot;, 5);
 857         public static final AMD64BinaryArithmetic XOR = new AMD64BinaryArithmetic(&quot;XOR&quot;, 6);
 858         public static final AMD64BinaryArithmetic CMP = new AMD64BinaryArithmetic(&quot;CMP&quot;, 7);
 859         // @formatter:on
 860 
 861         private final AMD64MIOp byteImmOp;
 862         private final AMD64MROp byteMrOp;
 863         private final AMD64RMOp byteRmOp;
 864 
 865         private final AMD64MIOp immOp;
 866         private final AMD64MIOp immSxOp;
 867         private final AMD64MROp mrOp;
 868         private final AMD64RMOp rmOp;
 869 
 870         private AMD64BinaryArithmetic(String opcode, int code) {
 871             int baseOp = code &lt;&lt; 3;
 872 
 873             byteImmOp = new AMD64MIOp(opcode, true, 0, 0x80, code, OpAssertion.ByteAssertion);
 874             byteMrOp = new AMD64MROp(opcode, 0, baseOp, OpAssertion.ByteAssertion);
 875             byteRmOp = new AMD64RMOp(opcode, 0, baseOp | 0x02, OpAssertion.ByteAssertion);
 876 
 877             immOp = new AMD64MIOp(opcode, false, 0, 0x81, code, OpAssertion.WordOrLargerAssertion);
 878             immSxOp = new AMD64MIOp(opcode, true, 0, 0x83, code, OpAssertion.WordOrLargerAssertion);
 879             mrOp = new AMD64MROp(opcode, 0, baseOp | 0x01, OpAssertion.WordOrLargerAssertion);
 880             rmOp = new AMD64RMOp(opcode, 0, baseOp | 0x03, OpAssertion.WordOrLargerAssertion);
 881         }
 882 
 883         public AMD64MIOp getMIOpcode(OperandSize size, boolean sx) {
 884             if (size == BYTE) {
 885                 return byteImmOp;
 886             } else if (sx) {
 887                 return immSxOp;
 888             } else {
 889                 return immOp;
 890             }
 891         }
 892 
 893         public AMD64MROp getMROpcode(OperandSize size) {
 894             if (size == BYTE) {
 895                 return byteMrOp;
 896             } else {
 897                 return mrOp;
 898             }
 899         }
 900 
 901         public AMD64RMOp getRMOpcode(OperandSize size) {
 902             if (size == BYTE) {
 903                 return byteRmOp;
 904             } else {
 905                 return rmOp;
 906             }
 907         }
 908     }
 909 
 910     /**
 911      * Shift operation with operand order of M1, MC or MI.
 912      */
 913     public static final class AMD64Shift {
 914         // @formatter:off
 915         public static final AMD64Shift ROL = new AMD64Shift(&quot;ROL&quot;, 0);
 916         public static final AMD64Shift ROR = new AMD64Shift(&quot;ROR&quot;, 1);
 917         public static final AMD64Shift RCL = new AMD64Shift(&quot;RCL&quot;, 2);
 918         public static final AMD64Shift RCR = new AMD64Shift(&quot;RCR&quot;, 3);
 919         public static final AMD64Shift SHL = new AMD64Shift(&quot;SHL&quot;, 4);
 920         public static final AMD64Shift SHR = new AMD64Shift(&quot;SHR&quot;, 5);
 921         public static final AMD64Shift SAR = new AMD64Shift(&quot;SAR&quot;, 7);
 922         // @formatter:on
 923 
 924         public final AMD64MOp m1Op;
 925         public final AMD64MOp mcOp;
 926         public final AMD64MIOp miOp;
 927 
 928         private AMD64Shift(String opcode, int code) {
 929             m1Op = new AMD64MOp(opcode, 0, 0xD1, code, OpAssertion.WordOrLargerAssertion);
 930             mcOp = new AMD64MOp(opcode, 0, 0xD3, code, OpAssertion.WordOrLargerAssertion);
 931             miOp = new AMD64MIOp(opcode, true, 0, 0xC1, code, OpAssertion.WordOrLargerAssertion);
 932         }
 933     }
 934 
 935     private enum EVEXFeatureAssertion {
 936         AVX512F_ALL(EnumSet.of(AVX512F), EnumSet.of(AVX512F), EnumSet.of(AVX512F)),
 937         AVX512F_128ONLY(EnumSet.of(AVX512F), null, null),
 938         AVX512F_VL(EnumSet.of(AVX512F, AVX512VL), EnumSet.of(AVX512F, AVX512VL), EnumSet.of(AVX512F)),
 939         AVX512CD_VL(EnumSet.of(AVX512F, AVX512CD, AVX512VL), EnumSet.of(AVX512F, AVX512CD, AVX512VL), EnumSet.of(AVX512F, AVX512CD)),
 940         AVX512DQ_VL(EnumSet.of(AVX512F, AVX512DQ, AVX512VL), EnumSet.of(AVX512F, AVX512DQ, AVX512VL), EnumSet.of(AVX512F, AVX512DQ)),
 941         AVX512BW_VL(EnumSet.of(AVX512F, AVX512BW, AVX512VL), EnumSet.of(AVX512F, AVX512BW, AVX512VL), EnumSet.of(AVX512F, AVX512BW));
 942 
 943         private final EnumSet&lt;CPUFeature&gt; l128features;
 944         private final EnumSet&lt;CPUFeature&gt; l256features;
 945         private final EnumSet&lt;CPUFeature&gt; l512features;
 946 
 947         EVEXFeatureAssertion(EnumSet&lt;CPUFeature&gt; l128features, EnumSet&lt;CPUFeature&gt; l256features, EnumSet&lt;CPUFeature&gt; l512features) {
 948             this.l128features = l128features;
 949             this.l256features = l256features;
 950             this.l512features = l512features;
 951         }
 952 
 953         public boolean check(AMD64 arch, int l) {
 954             switch (l) {
 955                 case L128:
 956                     assert l128features != null &amp;&amp; arch.getFeatures().containsAll(l128features) : &quot;emitting illegal 128 bit instruction&quot;;
 957                     break;
 958                 case L256:
 959                     assert l256features != null &amp;&amp; arch.getFeatures().containsAll(l256features) : &quot;emitting illegal 256 bit instruction&quot;;
 960                     break;
 961                 case L512:
 962                     assert l512features != null &amp;&amp; arch.getFeatures().containsAll(l512features) : &quot;emitting illegal 512 bit instruction&quot;;
 963                     break;
 964             }
 965             return true;
 966         }
 967 
 968         public boolean supports(EnumSet&lt;CPUFeature&gt; features, AVXSize avxSize) {
 969             switch (avxSize) {
 970                 case XMM:
 971                     return l128features != null &amp;&amp; features.containsAll(l128features);
 972                 case YMM:
 973                     return l256features != null &amp;&amp; features.containsAll(l256features);
 974                 case ZMM:
 975                     return l512features != null &amp;&amp; features.containsAll(l512features);
 976                 default:
 977                     throw GraalError.shouldNotReachHere();
 978             }
 979         }
 980     }
 981 
 982     private enum VEXOpAssertion {
 983         AVX1(CPUFeature.AVX, CPUFeature.AVX, null),
 984         AVX1_2(CPUFeature.AVX, CPUFeature.AVX2, null),
 985         AVX2(CPUFeature.AVX2, CPUFeature.AVX2, null),
 986         AVX1_128ONLY(CPUFeature.AVX, null, null),
 987         AVX1_256ONLY(null, CPUFeature.AVX, null),
 988         AVX2_256ONLY(null, CPUFeature.AVX2, null),
 989         XMM_CPU(CPUFeature.AVX, null, null, XMM, null, CPU, null),
 990         XMM_XMM_CPU(CPUFeature.AVX, null, null, XMM, XMM, CPU, null),
 991         CPU_XMM(CPUFeature.AVX, null, null, CPU, null, XMM, null),
 992         AVX1_2_CPU_XMM(CPUFeature.AVX, CPUFeature.AVX2, null, CPU, null, XMM, null),
 993         BMI1(CPUFeature.BMI1, null, null, CPU, CPU, CPU, null),
 994         BMI2(CPUFeature.BMI2, null, null, CPU, CPU, CPU, null),
 995         FMA(CPUFeature.FMA, null, null, XMM, XMM, XMM, null),
 996 
 997         XMM_CPU_AVX512F_128ONLY(CPUFeature.AVX, null, EVEXFeatureAssertion.AVX512F_128ONLY, XMM, null, CPU, null),
 998         AVX1_AVX512F_ALL(CPUFeature.AVX, CPUFeature.AVX, EVEXFeatureAssertion.AVX512F_ALL),
 999         AVX1_AVX512F_VL(CPUFeature.AVX, CPUFeature.AVX, EVEXFeatureAssertion.AVX512F_VL);
1000 
1001         private final CPUFeature l128feature;
1002         private final CPUFeature l256feature;
1003         private final EVEXFeatureAssertion l512features;
1004 
1005         private final RegisterCategory rCategory;
1006         private final RegisterCategory vCategory;
1007         private final RegisterCategory mCategory;
1008         private final RegisterCategory imm8Category;
1009 
1010         VEXOpAssertion(CPUFeature l128feature, CPUFeature l256feature, EVEXFeatureAssertion l512features) {
1011             this(l128feature, l256feature, l512features, XMM, XMM, XMM, XMM);
1012         }
1013 
1014         VEXOpAssertion(CPUFeature l128feature, CPUFeature l256feature, EVEXFeatureAssertion l512features, RegisterCategory rCategory, RegisterCategory vCategory, RegisterCategory mCategory,
1015                         RegisterCategory imm8Category) {
1016             this.l128feature = l128feature;
1017             this.l256feature = l256feature;
1018             this.l512features = l512features;
1019             this.rCategory = rCategory;
1020             this.vCategory = vCategory;
1021             this.mCategory = mCategory;
1022             this.imm8Category = imm8Category;
1023         }
1024 
1025         public boolean check(AMD64 arch, AVXSize size, Register r, Register v, Register m) {
1026             return check(arch, getLFlag(size), r, v, m, null);
1027         }
1028 
1029         public boolean check(AMD64 arch, AVXSize size, Register r, Register v, Register m, Register imm8) {
1030             return check(arch, getLFlag(size), r, v, m, imm8);
1031         }
1032 
1033         public boolean check(AMD64 arch, int l, Register r, Register v, Register m, Register imm8) {
1034             if (isAVX512Register(r) || isAVX512Register(v) || isAVX512Register(m) || l == L512) {
1035                 assert l512features != null &amp;&amp; l512features.check(arch, l);
1036             } else if (l == L128) {
1037                 assert l128feature != null &amp;&amp; arch.getFeatures().contains(l128feature) : &quot;emitting illegal 128 bit instruction&quot;;
1038             } else if (l == L256) {
1039                 assert l256feature != null &amp;&amp; arch.getFeatures().contains(l256feature) : &quot;emitting illegal 256 bit instruction&quot;;
1040             }
1041             if (r != null) {
1042                 assert r.getRegisterCategory().equals(rCategory);
1043             }
1044             if (v != null) {
1045                 assert v.getRegisterCategory().equals(vCategory);
1046             }
1047             if (m != null) {
1048                 assert m.getRegisterCategory().equals(mCategory);
1049             }
1050             if (imm8 != null) {
1051                 assert imm8.getRegisterCategory().equals(imm8Category);
1052             }
1053             return true;
1054         }
1055 
1056         public boolean supports(EnumSet&lt;CPUFeature&gt; features, AVXSize avxSize, boolean useZMMRegisters) {
1057             if (useZMMRegisters || avxSize == AVXSize.ZMM) {
1058                 return l512features != null &amp;&amp; l512features.supports(features, avxSize);
1059             } else if (avxSize == AVXSize.XMM) {
1060                 return l128feature != null &amp;&amp; features.contains(l128feature);
1061             } else if (avxSize == AVXSize.YMM) {
1062                 return l256feature != null &amp;&amp; features.contains(l256feature);
1063             }
1064             throw GraalError.shouldNotReachHere();
1065         }
1066     }
1067 
1068     /**
1069      * Base class for VEX-encoded instructions.
1070      */
1071     public static class VexOp {
1072         protected final int pp;
1073         protected final int mmmmm;
1074         protected final int w;
1075         protected final int op;
1076 
1077         private final String opcode;
1078         protected final VEXOpAssertion assertion;
1079 
1080         protected final EVEXTuple evexTuple;
1081         protected final int wEvex;
1082 
1083         protected VexOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion, EVEXTuple evexTuple, int wEvex) {
1084             this.pp = pp;
1085             this.mmmmm = mmmmm;
1086             this.w = w;
1087             this.op = op;
1088             this.opcode = opcode;
1089             this.assertion = assertion;
1090             this.evexTuple = evexTuple;
1091             this.wEvex = wEvex;
1092         }
1093 
1094         protected VexOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1095             this(opcode, pp, mmmmm, w, op, assertion, EVEXTuple.INVALID, WIG);
1096         }
1097 
1098         public final boolean isSupported(AMD64Assembler vasm, AVXSize size) {
1099             return isSupported(vasm, size, false);
1100         }
1101 
1102         public final boolean isSupported(AMD64Assembler vasm, AVXSize size, boolean useZMMRegisters) {
1103             return assertion.supports(((AMD64) vasm.target.arch).getFeatures(), size, useZMMRegisters);
1104         }
1105 
1106         @Override
1107         public String toString() {
1108             return opcode;
1109         }
1110 
1111         protected final int getDisp8Scale(boolean useEvex, AVXSize size) {
1112             return useEvex ? evexTuple.getDisp8ScalingFactor(size) : DEFAULT_DISP8_SCALE;
1113         }
1114 
1115     }
1116 
1117     /**
1118      * VEX-encoded instructions with an operand order of RM, but the M operand must be a register.
1119      */
1120     public static class VexRROp extends VexOp {
1121         // @formatter:off
1122         public static final VexRROp VMASKMOVDQU = new VexRROp(&quot;VMASKMOVDQU&quot;, P_66, M_0F, WIG, 0xF7, VEXOpAssertion.AVX1_128ONLY, EVEXTuple.INVALID, WIG);
1123         // @formatter:on
1124 
1125         protected VexRROp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion, EVEXTuple evexTuple, int wEvex) {
1126             super(opcode, pp, mmmmm, w, op, assertion, evexTuple, wEvex);
1127         }
1128 
1129         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src) {
1130             assert assertion.check((AMD64) asm.target.arch, size, dst, null, src);
1131             assert op != 0x1A || op != 0x5A;
1132             asm.vexPrefix(dst, Register.None, src, size, pp, mmmmm, w, wEvex, false);
1133             asm.emitByte(op);
1134             asm.emitModRM(dst, src);
1135         }
1136     }
1137 
1138     /**
1139      * VEX-encoded instructions with an operand order of RM.
1140      */
1141     public static class VexRMOp extends VexRROp {
1142         // @formatter:off
1143         public static final VexRMOp VCVTTSS2SI      = new VexRMOp(&quot;VCVTTSS2SI&quot;,      P_F3, M_0F,   W0,  0x2C, VEXOpAssertion.CPU_XMM);
1144         public static final VexRMOp VCVTTSS2SQ      = new VexRMOp(&quot;VCVTTSS2SQ&quot;,      P_F3, M_0F,   W1,  0x2C, VEXOpAssertion.CPU_XMM);
1145         public static final VexRMOp VCVTTSD2SI      = new VexRMOp(&quot;VCVTTSD2SI&quot;,      P_F2, M_0F,   W0,  0x2C, VEXOpAssertion.CPU_XMM);
1146         public static final VexRMOp VCVTTSD2SQ      = new VexRMOp(&quot;VCVTTSD2SQ&quot;,      P_F2, M_0F,   W1,  0x2C, VEXOpAssertion.CPU_XMM);
1147         public static final VexRMOp VCVTPS2PD       = new VexRMOp(&quot;VCVTPS2PD&quot;,       P_,   M_0F,   WIG, 0x5A);
1148         public static final VexRMOp VCVTPD2PS       = new VexRMOp(&quot;VCVTPD2PS&quot;,       P_66, M_0F,   WIG, 0x5A);
1149         public static final VexRMOp VCVTDQ2PS       = new VexRMOp(&quot;VCVTDQ2PS&quot;,       P_,   M_0F,   WIG, 0x5B);
1150         public static final VexRMOp VCVTTPS2DQ      = new VexRMOp(&quot;VCVTTPS2DQ&quot;,      P_F3, M_0F,   WIG, 0x5B);
1151         public static final VexRMOp VCVTTPD2DQ      = new VexRMOp(&quot;VCVTTPD2DQ&quot;,      P_66, M_0F,   WIG, 0xE6);
1152         public static final VexRMOp VCVTDQ2PD       = new VexRMOp(&quot;VCVTDQ2PD&quot;,       P_F3, M_0F,   WIG, 0xE6);
1153         public static final VexRMOp VBROADCASTSS    = new VexRMOp(&quot;VBROADCASTSS&quot;,    P_66, M_0F38, W0,  0x18);
1154         public static final VexRMOp VBROADCASTSD    = new VexRMOp(&quot;VBROADCASTSD&quot;,    P_66, M_0F38, W0,  0x19, VEXOpAssertion.AVX1_256ONLY);
1155         public static final VexRMOp VBROADCASTF128  = new VexRMOp(&quot;VBROADCASTF128&quot;,  P_66, M_0F38, W0,  0x1A, VEXOpAssertion.AVX1_256ONLY);
1156         public static final VexRMOp VPBROADCASTI128 = new VexRMOp(&quot;VPBROADCASTI128&quot;, P_66, M_0F38, W0,  0x5A, VEXOpAssertion.AVX2_256ONLY);
1157         public static final VexRMOp VPBROADCASTB    = new VexRMOp(&quot;VPBROADCASTB&quot;,    P_66, M_0F38, W0,  0x78, VEXOpAssertion.AVX2);
1158         public static final VexRMOp VPBROADCASTW    = new VexRMOp(&quot;VPBROADCASTW&quot;,    P_66, M_0F38, W0,  0x79, VEXOpAssertion.AVX2);
1159         public static final VexRMOp VPBROADCASTD    = new VexRMOp(&quot;VPBROADCASTD&quot;,    P_66, M_0F38, W0,  0x58, VEXOpAssertion.AVX2);
1160         public static final VexRMOp VPBROADCASTQ    = new VexRMOp(&quot;VPBROADCASTQ&quot;,    P_66, M_0F38, W0,  0x59, VEXOpAssertion.AVX2);
1161         public static final VexRMOp VPMOVMSKB       = new VexRMOp(&quot;VPMOVMSKB&quot;,       P_66, M_0F,   WIG, 0xD7, VEXOpAssertion.AVX1_2_CPU_XMM);
1162         public static final VexRMOp VPMOVSXBW       = new VexRMOp(&quot;VPMOVSXBW&quot;,       P_66, M_0F38, WIG, 0x20);
1163         public static final VexRMOp VPMOVSXBD       = new VexRMOp(&quot;VPMOVSXBD&quot;,       P_66, M_0F38, WIG, 0x21);
1164         public static final VexRMOp VPMOVSXBQ       = new VexRMOp(&quot;VPMOVSXBQ&quot;,       P_66, M_0F38, WIG, 0x22);
1165         public static final VexRMOp VPMOVSXWD       = new VexRMOp(&quot;VPMOVSXWD&quot;,       P_66, M_0F38, WIG, 0x23);
1166         public static final VexRMOp VPMOVSXWQ       = new VexRMOp(&quot;VPMOVSXWQ&quot;,       P_66, M_0F38, WIG, 0x24);
1167         public static final VexRMOp VPMOVSXDQ       = new VexRMOp(&quot;VPMOVSXDQ&quot;,       P_66, M_0F38, WIG, 0x25);
1168         public static final VexRMOp VPMOVZXBW       = new VexRMOp(&quot;VPMOVZXBW&quot;,       P_66, M_0F38, WIG, 0x30);
1169         public static final VexRMOp VPMOVZXBD       = new VexRMOp(&quot;VPMOVZXBD&quot;,       P_66, M_0F38, WIG, 0x31);
1170         public static final VexRMOp VPMOVZXBQ       = new VexRMOp(&quot;VPMOVZXBQ&quot;,       P_66, M_0F38, WIG, 0x32);
1171         public static final VexRMOp VPMOVZXWD       = new VexRMOp(&quot;VPMOVZXWD&quot;,       P_66, M_0F38, WIG, 0x33);
1172         public static final VexRMOp VPMOVZXWQ       = new VexRMOp(&quot;VPMOVZXWQ&quot;,       P_66, M_0F38, WIG, 0x34);
1173         public static final VexRMOp VPMOVZXDQ       = new VexRMOp(&quot;VPMOVZXDQ&quot;,       P_66, M_0F38, WIG, 0x35);
1174         public static final VexRMOp VPTEST          = new VexRMOp(&quot;VPTEST&quot;,          P_66, M_0F38, WIG, 0x17);
1175         public static final VexRMOp VSQRTPD         = new VexRMOp(&quot;VSQRTPD&quot;,         P_66, M_0F,   WIG, 0x51);
1176         public static final VexRMOp VSQRTPS         = new VexRMOp(&quot;VSQRTPS&quot;,         P_,   M_0F,   WIG, 0x51);
1177         public static final VexRMOp VSQRTSD         = new VexRMOp(&quot;VSQRTSD&quot;,         P_F2, M_0F,   WIG, 0x51);
1178         public static final VexRMOp VSQRTSS         = new VexRMOp(&quot;VSQRTSS&quot;,         P_F3, M_0F,   WIG, 0x51);
1179         public static final VexRMOp VUCOMISS        = new VexRMOp(&quot;VUCOMISS&quot;,        P_,   M_0F,   WIG, 0x2E);
1180         public static final VexRMOp VUCOMISD        = new VexRMOp(&quot;VUCOMISD&quot;,        P_66, M_0F,   WIG, 0x2E);
1181         // @formatter:on
1182 
1183         protected VexRMOp(String opcode, int pp, int mmmmm, int w, int op) {
1184             this(opcode, pp, mmmmm, w, op, VEXOpAssertion.AVX1, EVEXTuple.INVALID, WIG);
1185         }
1186 
1187         protected VexRMOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1188             this(opcode, pp, mmmmm, w, op, assertion, EVEXTuple.INVALID, WIG);
1189         }
1190 
1191         protected VexRMOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion, EVEXTuple evexTuple, int wEvex) {
1192             super(opcode, pp, mmmmm, w, op, assertion, evexTuple, wEvex);
1193         }
1194 
1195         public void emit(AMD64Assembler asm, AVXSize size, Register dst, AMD64Address src) {
1196             assert assertion.check((AMD64) asm.target.arch, size, dst, null, null);
1197             boolean useEvex = asm.vexPrefix(dst, Register.None, src, size, pp, mmmmm, w, wEvex, false);
1198             asm.emitByte(op);
1199             asm.emitOperandHelper(dst, src, 0, getDisp8Scale(useEvex, size));
1200         }
1201     }
1202 
1203     /**
1204      * VEX-encoded move instructions.
1205      * &lt;p&gt;
1206      * These instructions have two opcodes: op is the forward move instruction with an operand order
1207      * of RM, and opReverse is the reverse move instruction with an operand order of MR.
1208      */
1209     public static final class VexMoveOp extends VexRMOp {
1210         // @formatter:off
1211         public static final VexMoveOp VMOVDQA32 = new VexMoveOp(&quot;VMOVDQA32&quot;, P_66, M_0F, WIG, 0x6F, 0x7F, VEXOpAssertion.AVX1_AVX512F_VL,         EVEXTuple.FVM,       W0);
1212         public static final VexMoveOp VMOVDQA64 = new VexMoveOp(&quot;VMOVDQA64&quot;, P_66, M_0F, WIG, 0x6F, 0x7F, VEXOpAssertion.AVX1_AVX512F_VL,         EVEXTuple.FVM,       W1);
1213         public static final VexMoveOp VMOVDQU32 = new VexMoveOp(&quot;VMOVDQU32&quot;, P_F3, M_0F, WIG, 0x6F, 0x7F, VEXOpAssertion.AVX1_AVX512F_VL,         EVEXTuple.FVM,       W0);
1214         public static final VexMoveOp VMOVDQU64 = new VexMoveOp(&quot;VMOVDQU64&quot;, P_F3, M_0F, WIG, 0x6F, 0x7F, VEXOpAssertion.AVX1_AVX512F_VL,         EVEXTuple.FVM,       W1);
1215         public static final VexMoveOp VMOVAPS   = new VexMoveOp(&quot;VMOVAPS&quot;,   P_,   M_0F, WIG, 0x28, 0x29, VEXOpAssertion.AVX1_AVX512F_VL,         EVEXTuple.FVM,       W0);
1216         public static final VexMoveOp VMOVAPD   = new VexMoveOp(&quot;VMOVAPD&quot;,   P_66, M_0F, WIG, 0x28, 0x29, VEXOpAssertion.AVX1_AVX512F_VL,         EVEXTuple.FVM,       W1);
1217         public static final VexMoveOp VMOVUPS   = new VexMoveOp(&quot;VMOVUPS&quot;,   P_,   M_0F, WIG, 0x10, 0x11, VEXOpAssertion.AVX1_AVX512F_VL,         EVEXTuple.FVM,       W0);
1218         public static final VexMoveOp VMOVUPD   = new VexMoveOp(&quot;VMOVUPD&quot;,   P_66, M_0F, WIG, 0x10, 0x11, VEXOpAssertion.AVX1_AVX512F_VL,         EVEXTuple.FVM,       W1);
1219         public static final VexMoveOp VMOVSS    = new VexMoveOp(&quot;VMOVSS&quot;,    P_F3, M_0F, WIG, 0x10, 0x11, VEXOpAssertion.AVX1_AVX512F_ALL,        EVEXTuple.T1S_32BIT, W0);
1220         public static final VexMoveOp VMOVSD    = new VexMoveOp(&quot;VMOVSD&quot;,    P_F2, M_0F, WIG, 0x10, 0x11, VEXOpAssertion.AVX1_AVX512F_ALL,        EVEXTuple.T1S_64BIT, W1);
1221         public static final VexMoveOp VMOVD     = new VexMoveOp(&quot;VMOVD&quot;,     P_66, M_0F, W0,  0x6E, 0x7E, VEXOpAssertion.XMM_CPU_AVX512F_128ONLY, EVEXTuple.T1F_32BIT, W0);
1222         public static final VexMoveOp VMOVQ     = new VexMoveOp(&quot;VMOVQ&quot;,     P_66, M_0F, W1,  0x6E, 0x7E, VEXOpAssertion.XMM_CPU_AVX512F_128ONLY, EVEXTuple.T1S_64BIT, W1);
1223         // @formatter:on
1224 
1225         private final int opReverse;
1226 
1227         private VexMoveOp(String opcode, int pp, int mmmmm, int w, int op, int opReverse) {
1228             this(opcode, pp, mmmmm, w, op, opReverse, VEXOpAssertion.AVX1, EVEXTuple.INVALID, WIG);
1229         }
1230 
1231         private VexMoveOp(String opcode, int pp, int mmmmm, int w, int op, int opReverse, VEXOpAssertion assertion) {
1232             this(opcode, pp, mmmmm, w, op, opReverse, assertion, EVEXTuple.INVALID, WIG);
1233         }
1234 
1235         private VexMoveOp(String opcode, int pp, int mmmmm, int w, int op, int opReverse, VEXOpAssertion assertion, EVEXTuple evexTuple, int wEvex) {
1236             super(opcode, pp, mmmmm, w, op, assertion, evexTuple, wEvex);
1237             this.opReverse = opReverse;
1238         }
1239 
1240         public void emit(AMD64Assembler asm, AVXSize size, AMD64Address dst, Register src) {
1241             assert assertion.check((AMD64) asm.target.arch, size, src, null, null);
1242             boolean useEvex = asm.vexPrefix(src, Register.None, dst, size, pp, mmmmm, w, wEvex, false);
1243             asm.emitByte(opReverse);
1244             asm.emitOperandHelper(src, dst, 0, getDisp8Scale(useEvex, size));
1245         }
1246 
1247         public void emitReverse(AMD64Assembler asm, AVXSize size, Register dst, Register src) {
1248             assert assertion.check((AMD64) asm.target.arch, size, src, null, dst);
1249             asm.vexPrefix(src, Register.None, dst, size, pp, mmmmm, w, wEvex, false);
1250             asm.emitByte(opReverse);
1251             asm.emitModRM(src, dst);
1252         }
1253     }
1254 
1255     public interface VexRRIOp {
1256         void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src, int imm8);
1257     }
1258 
1259     /**
1260      * VEX-encoded instructions with an operand order of RMI.
1261      */
1262     public static final class VexRMIOp extends VexOp implements VexRRIOp {
1263         // @formatter:off
1264         public static final VexRMIOp VPERMQ   = new VexRMIOp(&quot;VPERMQ&quot;,   P_66, M_0F3A, W1,  0x00, VEXOpAssertion.AVX2_256ONLY);
1265         public static final VexRMIOp VPSHUFLW = new VexRMIOp(&quot;VPSHUFLW&quot;, P_F2, M_0F,   WIG, 0x70, VEXOpAssertion.AVX1_2);
1266         public static final VexRMIOp VPSHUFHW = new VexRMIOp(&quot;VPSHUFHW&quot;, P_F3, M_0F,   WIG, 0x70, VEXOpAssertion.AVX1_2);
1267         public static final VexRMIOp VPSHUFD  = new VexRMIOp(&quot;VPSHUFD&quot;,  P_66, M_0F,   WIG, 0x70, VEXOpAssertion.AVX1_2);
1268         // @formatter:on
1269 
1270         private VexRMIOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1271             super(opcode, pp, mmmmm, w, op, assertion);
1272         }
1273 
1274         @Override
1275         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src, int imm8) {
1276             assert assertion.check((AMD64) asm.target.arch, size, dst, null, src);
1277             asm.vexPrefix(dst, Register.None, src, size, pp, mmmmm, w, wEvex, false);
1278             asm.emitByte(op);
1279             asm.emitModRM(dst, src);
1280             asm.emitByte(imm8);
1281         }
1282 
1283         public void emit(AMD64Assembler asm, AVXSize size, Register dst, AMD64Address src, int imm8) {
1284             assert assertion.check((AMD64) asm.target.arch, size, dst, null, null);
1285             boolean useEvex = asm.vexPrefix(dst, Register.None, src, size, pp, mmmmm, w, wEvex, false);
1286             asm.emitByte(op);
1287             asm.emitOperandHelper(dst, src, 1, getDisp8Scale(useEvex, size));
1288             asm.emitByte(imm8);
1289         }
1290     }
1291 
1292     /**
1293      * VEX-encoded instructions with an operand order of MRI.
1294      */
1295     public static final class VexMRIOp extends VexOp implements VexRRIOp {
1296         // @formatter:off
1297         public static final VexMRIOp VEXTRACTF128 = new VexMRIOp(&quot;VEXTRACTF128&quot;, P_66, M_0F3A, W0, 0x19, VEXOpAssertion.AVX1_256ONLY);
1298         public static final VexMRIOp VEXTRACTI128 = new VexMRIOp(&quot;VEXTRACTI128&quot;, P_66, M_0F3A, W0, 0x39, VEXOpAssertion.AVX2_256ONLY);
1299         public static final VexMRIOp VPEXTRB      = new VexMRIOp(&quot;VPEXTRB&quot;,      P_66, M_0F3A, W0, 0x14, VEXOpAssertion.XMM_CPU);
1300         public static final VexMRIOp VPEXTRW      = new VexMRIOp(&quot;VPEXTRW&quot;,      P_66, M_0F3A, W0, 0x15, VEXOpAssertion.XMM_CPU);
1301         public static final VexMRIOp VPEXTRD      = new VexMRIOp(&quot;VPEXTRD&quot;,      P_66, M_0F3A, W0, 0x16, VEXOpAssertion.XMM_CPU);
1302         public static final VexMRIOp VPEXTRQ      = new VexMRIOp(&quot;VPEXTRQ&quot;,      P_66, M_0F3A, W1, 0x16, VEXOpAssertion.XMM_CPU);
1303         // @formatter:on
1304 
1305         private VexMRIOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1306             super(opcode, pp, mmmmm, w, op, assertion);
1307         }
1308 
1309         @Override
1310         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src, int imm8) {
1311             assert assertion.check((AMD64) asm.target.arch, size, src, null, dst);
1312             asm.vexPrefix(src, Register.None, dst, size, pp, mmmmm, w, wEvex, false);
1313             asm.emitByte(op);
1314             asm.emitModRM(src, dst);
1315             asm.emitByte(imm8);
1316         }
1317 
1318         public void emit(AMD64Assembler asm, AVXSize size, AMD64Address dst, Register src, int imm8) {
1319             assert assertion.check((AMD64) asm.target.arch, size, src, null, null);
1320             boolean useEvex = asm.vexPrefix(src, Register.None, dst, size, pp, mmmmm, w, wEvex, false);
1321             asm.emitByte(op);
1322             asm.emitOperandHelper(src, dst, 1, getDisp8Scale(useEvex, size));
1323             asm.emitByte(imm8);
1324         }
1325     }
1326 
1327     /**
1328      * VEX-encoded instructions with an operand order of RVMR.
1329      */
1330     public static class VexRVMROp extends VexOp {
1331         // @formatter:off
1332         public static final VexRVMROp VPBLENDVB = new VexRVMROp(&quot;VPBLENDVB&quot;, P_66, M_0F3A, W0, 0x4C, VEXOpAssertion.AVX1_2);
1333         public static final VexRVMROp VBLENDVPS = new VexRVMROp(&quot;VBLENDVPS&quot;, P_66, M_0F3A, W0, 0x4A, VEXOpAssertion.AVX1);
1334         public static final VexRVMROp VBLENDVPD = new VexRVMROp(&quot;VBLENDVPD&quot;, P_66, M_0F3A, W0, 0x4B, VEXOpAssertion.AVX1);
1335         // @formatter:on
1336 
1337         protected VexRVMROp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1338             super(opcode, pp, mmmmm, w, op, assertion);
1339         }
1340 
1341         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register mask, Register src1, Register src2) {
1342             assert assertion.check((AMD64) asm.target.arch, size, dst, mask, src1, src2);
1343             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, wEvex, false);
1344             asm.emitByte(op);
1345             asm.emitModRM(dst, src2);
1346             asm.emitByte(mask.encoding() &lt;&lt; 4);
1347         }
1348 
1349         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register mask, Register src1, AMD64Address src2) {
1350             assert assertion.check((AMD64) asm.target.arch, size, dst, mask, src1, null);
1351             boolean useEvex = asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, wEvex, false);
1352             asm.emitByte(op);
1353             asm.emitOperandHelper(dst, src2, 0, getDisp8Scale(useEvex, size));
1354             asm.emitByte(mask.encoding() &lt;&lt; 4);
1355         }
1356     }
1357 
1358     /**
1359      * VEX-encoded instructions with an operand order of RVM.
1360      */
1361     public static class VexRVMOp extends VexOp {
1362         // @formatter:off
1363         public static final VexRVMOp VANDPS    = new VexRVMOp(&quot;VANDPS&quot;,    P_,   M_0F,   WIG, 0x54);
1364         public static final VexRVMOp VANDPD    = new VexRVMOp(&quot;VANDPD&quot;,    P_66, M_0F,   WIG, 0x54);
1365         public static final VexRVMOp VANDNPS   = new VexRVMOp(&quot;VANDNPS&quot;,   P_,   M_0F,   WIG, 0x55);
1366         public static final VexRVMOp VANDNPD   = new VexRVMOp(&quot;VANDNPD&quot;,   P_66, M_0F,   WIG, 0x55);
1367         public static final VexRVMOp VORPS     = new VexRVMOp(&quot;VORPS&quot;,     P_,   M_0F,   WIG, 0x56);
1368         public static final VexRVMOp VORPD     = new VexRVMOp(&quot;VORPD&quot;,     P_66, M_0F,   WIG, 0x56);
1369         public static final VexRVMOp VXORPS    = new VexRVMOp(&quot;VXORPS&quot;,    P_,   M_0F,   WIG, 0x57);
1370         public static final VexRVMOp VXORPD    = new VexRVMOp(&quot;VXORPD&quot;,    P_66, M_0F,   WIG, 0x57);
1371         public static final VexRVMOp VADDPS    = new VexRVMOp(&quot;VADDPS&quot;,    P_,   M_0F,   WIG, 0x58);
1372         public static final VexRVMOp VADDPD    = new VexRVMOp(&quot;VADDPD&quot;,    P_66, M_0F,   WIG, 0x58);
1373         public static final VexRVMOp VADDSS    = new VexRVMOp(&quot;VADDSS&quot;,    P_F3, M_0F,   WIG, 0x58);
1374         public static final VexRVMOp VADDSD    = new VexRVMOp(&quot;VADDSD&quot;,    P_F2, M_0F,   WIG, 0x58);
1375         public static final VexRVMOp VMULPS    = new VexRVMOp(&quot;VMULPS&quot;,    P_,   M_0F,   WIG, 0x59);
1376         public static final VexRVMOp VMULPD    = new VexRVMOp(&quot;VMULPD&quot;,    P_66, M_0F,   WIG, 0x59);
1377         public static final VexRVMOp VMULSS    = new VexRVMOp(&quot;VMULSS&quot;,    P_F3, M_0F,   WIG, 0x59);
1378         public static final VexRVMOp VMULSD    = new VexRVMOp(&quot;VMULSD&quot;,    P_F2, M_0F,   WIG, 0x59);
1379         public static final VexRVMOp VSUBPS    = new VexRVMOp(&quot;VSUBPS&quot;,    P_,   M_0F,   WIG, 0x5C);
1380         public static final VexRVMOp VSUBPD    = new VexRVMOp(&quot;VSUBPD&quot;,    P_66, M_0F,   WIG, 0x5C);
1381         public static final VexRVMOp VSUBSS    = new VexRVMOp(&quot;VSUBSS&quot;,    P_F3, M_0F,   WIG, 0x5C);
1382         public static final VexRVMOp VSUBSD    = new VexRVMOp(&quot;VSUBSD&quot;,    P_F2, M_0F,   WIG, 0x5C);
1383         public static final VexRVMOp VMINPS    = new VexRVMOp(&quot;VMINPS&quot;,    P_,   M_0F,   WIG, 0x5D);
1384         public static final VexRVMOp VMINPD    = new VexRVMOp(&quot;VMINPD&quot;,    P_66, M_0F,   WIG, 0x5D);
1385         public static final VexRVMOp VMINSS    = new VexRVMOp(&quot;VMINSS&quot;,    P_F3, M_0F,   WIG, 0x5D);
1386         public static final VexRVMOp VMINSD    = new VexRVMOp(&quot;VMINSD&quot;,    P_F2, M_0F,   WIG, 0x5D);
1387         public static final VexRVMOp VDIVPS    = new VexRVMOp(&quot;VDIVPS&quot;,    P_,   M_0F,   WIG, 0x5E);
1388         public static final VexRVMOp VDIVPD    = new VexRVMOp(&quot;VDIVPD&quot;,    P_66, M_0F,   WIG, 0x5E);
1389         public static final VexRVMOp VDIVSS    = new VexRVMOp(&quot;VDIVPS&quot;,    P_F3, M_0F,   WIG, 0x5E);
1390         public static final VexRVMOp VDIVSD    = new VexRVMOp(&quot;VDIVPD&quot;,    P_F2, M_0F,   WIG, 0x5E);
1391         public static final VexRVMOp VMAXPS    = new VexRVMOp(&quot;VMAXPS&quot;,    P_,   M_0F,   WIG, 0x5F);
1392         public static final VexRVMOp VMAXPD    = new VexRVMOp(&quot;VMAXPD&quot;,    P_66, M_0F,   WIG, 0x5F);
1393         public static final VexRVMOp VMAXSS    = new VexRVMOp(&quot;VMAXSS&quot;,    P_F3, M_0F,   WIG, 0x5F);
1394         public static final VexRVMOp VMAXSD    = new VexRVMOp(&quot;VMAXSD&quot;,    P_F2, M_0F,   WIG, 0x5F);
1395         public static final VexRVMOp VADDSUBPS = new VexRVMOp(&quot;VADDSUBPS&quot;, P_F2, M_0F,   WIG, 0xD0);
1396         public static final VexRVMOp VADDSUBPD = new VexRVMOp(&quot;VADDSUBPD&quot;, P_66, M_0F,   WIG, 0xD0);
1397         public static final VexRVMOp VPAND     = new VexRVMOp(&quot;VPAND&quot;,     P_66, M_0F,   WIG, 0xDB, VEXOpAssertion.AVX1_2);
1398         public static final VexRVMOp VPOR      = new VexRVMOp(&quot;VPOR&quot;,      P_66, M_0F,   WIG, 0xEB, VEXOpAssertion.AVX1_2);
1399         public static final VexRVMOp VPXOR     = new VexRVMOp(&quot;VPXOR&quot;,     P_66, M_0F,   WIG, 0xEF, VEXOpAssertion.AVX1_2);
1400         public static final VexRVMOp VPADDB    = new VexRVMOp(&quot;VPADDB&quot;,    P_66, M_0F,   WIG, 0xFC, VEXOpAssertion.AVX1_2);
1401         public static final VexRVMOp VPADDW    = new VexRVMOp(&quot;VPADDW&quot;,    P_66, M_0F,   WIG, 0xFD, VEXOpAssertion.AVX1_2);
1402         public static final VexRVMOp VPADDD    = new VexRVMOp(&quot;VPADDD&quot;,    P_66, M_0F,   WIG, 0xFE, VEXOpAssertion.AVX1_2);
1403         public static final VexRVMOp VPADDQ    = new VexRVMOp(&quot;VPADDQ&quot;,    P_66, M_0F,   WIG, 0xD4, VEXOpAssertion.AVX1_2);
1404         public static final VexRVMOp VPMULHUW  = new VexRVMOp(&quot;VPMULHUW&quot;,  P_66, M_0F,   WIG, 0xE4, VEXOpAssertion.AVX1_2);
1405         public static final VexRVMOp VPMULHW   = new VexRVMOp(&quot;VPMULHW&quot;,   P_66, M_0F,   WIG, 0xE5, VEXOpAssertion.AVX1_2);
1406         public static final VexRVMOp VPMULLW   = new VexRVMOp(&quot;VPMULLW&quot;,   P_66, M_0F,   WIG, 0xD5, VEXOpAssertion.AVX1_2);
1407         public static final VexRVMOp VPMULLD   = new VexRVMOp(&quot;VPMULLD&quot;,   P_66, M_0F38, WIG, 0x40, VEXOpAssertion.AVX1_2);
1408         public static final VexRVMOp VPSUBB    = new VexRVMOp(&quot;VPSUBB&quot;,    P_66, M_0F,   WIG, 0xF8, VEXOpAssertion.AVX1_2);
1409         public static final VexRVMOp VPSUBW    = new VexRVMOp(&quot;VPSUBW&quot;,    P_66, M_0F,   WIG, 0xF9, VEXOpAssertion.AVX1_2);
1410         public static final VexRVMOp VPSUBD    = new VexRVMOp(&quot;VPSUBD&quot;,    P_66, M_0F,   WIG, 0xFA, VEXOpAssertion.AVX1_2);
1411         public static final VexRVMOp VPSUBQ    = new VexRVMOp(&quot;VPSUBQ&quot;,    P_66, M_0F,   WIG, 0xFB, VEXOpAssertion.AVX1_2);
1412         public static final VexRVMOp VPSHUFB   = new VexRVMOp(&quot;VPSHUFB&quot;,   P_66, M_0F38, WIG, 0x00, VEXOpAssertion.AVX1_2);
1413         public static final VexRVMOp VCVTSD2SS = new VexRVMOp(&quot;VCVTSD2SS&quot;, P_F2, M_0F,   WIG, 0x5A);
1414         public static final VexRVMOp VCVTSS2SD = new VexRVMOp(&quot;VCVTSS2SD&quot;, P_F3, M_0F,   WIG, 0x5A);
1415         public static final VexRVMOp VCVTSI2SD = new VexRVMOp(&quot;VCVTSI2SD&quot;, P_F2, M_0F,   W0,  0x2A, VEXOpAssertion.XMM_XMM_CPU);
1416         public static final VexRVMOp VCVTSQ2SD = new VexRVMOp(&quot;VCVTSQ2SD&quot;, P_F2, M_0F,   W1,  0x2A, VEXOpAssertion.XMM_XMM_CPU);
1417         public static final VexRVMOp VCVTSI2SS = new VexRVMOp(&quot;VCVTSI2SS&quot;, P_F3, M_0F,   W0,  0x2A, VEXOpAssertion.XMM_XMM_CPU);
1418         public static final VexRVMOp VCVTSQ2SS = new VexRVMOp(&quot;VCVTSQ2SS&quot;, P_F3, M_0F,   W1,  0x2A, VEXOpAssertion.XMM_XMM_CPU);
1419         public static final VexRVMOp VPCMPEQB  = new VexRVMOp(&quot;VPCMPEQB&quot;,  P_66, M_0F,   WIG, 0x74, VEXOpAssertion.AVX1_2);
1420         public static final VexRVMOp VPCMPEQW  = new VexRVMOp(&quot;VPCMPEQW&quot;,  P_66, M_0F,   WIG, 0x75, VEXOpAssertion.AVX1_2);
1421         public static final VexRVMOp VPCMPEQD  = new VexRVMOp(&quot;VPCMPEQD&quot;,  P_66, M_0F,   WIG, 0x76, VEXOpAssertion.AVX1_2);
1422         public static final VexRVMOp VPCMPEQQ  = new VexRVMOp(&quot;VPCMPEQQ&quot;,  P_66, M_0F38, WIG, 0x29, VEXOpAssertion.AVX1_2);
1423         public static final VexRVMOp VPCMPGTB  = new VexRVMOp(&quot;VPCMPGTB&quot;,  P_66, M_0F,   WIG, 0x64, VEXOpAssertion.AVX1_2);
1424         public static final VexRVMOp VPCMPGTW  = new VexRVMOp(&quot;VPCMPGTW&quot;,  P_66, M_0F,   WIG, 0x65, VEXOpAssertion.AVX1_2);
1425         public static final VexRVMOp VPCMPGTD  = new VexRVMOp(&quot;VPCMPGTD&quot;,  P_66, M_0F,   WIG, 0x66, VEXOpAssertion.AVX1_2);
1426         public static final VexRVMOp VPCMPGTQ  = new VexRVMOp(&quot;VPCMPGTQ&quot;,  P_66, M_0F38, WIG, 0x37, VEXOpAssertion.AVX1_2);
1427         public static final VexRVMOp VFMADD231SS = new VexRVMOp(&quot;VFMADD231SS&quot;, P_66, M_0F38, W0, 0xB9, VEXOpAssertion.FMA);
1428         public static final VexRVMOp VFMADD231SD = new VexRVMOp(&quot;VFMADD231SD&quot;, P_66, M_0F38, W1, 0xB9, VEXOpAssertion.FMA);
1429         // @formatter:on
1430 
1431         private VexRVMOp(String opcode, int pp, int mmmmm, int w, int op) {
1432             this(opcode, pp, mmmmm, w, op, VEXOpAssertion.AVX1);
1433         }
1434 
1435         protected VexRVMOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1436             super(opcode, pp, mmmmm, w, op, assertion);
1437         }
1438 
1439         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, Register src2) {
1440             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, src2);
1441             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, wEvex, false);
1442             asm.emitByte(op);
1443             asm.emitModRM(dst, src2);
1444         }
1445 
1446         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, AMD64Address src2) {
1447             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, null);
1448             boolean useEvex = asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, wEvex, false);
1449             asm.emitByte(op);
1450             asm.emitOperandHelper(dst, src2, 0, getDisp8Scale(useEvex, size));
1451         }
1452     }
1453 
1454     public static final class VexGeneralPurposeRVMOp extends VexRVMOp {
1455         // @formatter:off
1456         public static final VexGeneralPurposeRVMOp ANDN   = new VexGeneralPurposeRVMOp(&quot;ANDN&quot;,   P_,   M_0F38, WIG, 0xF2, VEXOpAssertion.BMI1);
1457         public static final VexGeneralPurposeRVMOp MULX   = new VexGeneralPurposeRVMOp(&quot;MULX&quot;,   P_F2, M_0F38, WIG, 0xF6, VEXOpAssertion.BMI2);
1458         public static final VexGeneralPurposeRVMOp PDEP   = new VexGeneralPurposeRVMOp(&quot;PDEP&quot;,   P_F2, M_0F38, WIG, 0xF5, VEXOpAssertion.BMI2);
1459         public static final VexGeneralPurposeRVMOp PEXT   = new VexGeneralPurposeRVMOp(&quot;PEXT&quot;,   P_F3, M_0F38, WIG, 0xF5, VEXOpAssertion.BMI2);
1460         // @formatter:on
1461 
1462         private VexGeneralPurposeRVMOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1463             super(opcode, pp, mmmmm, w, op, assertion);
1464         }
1465 
1466         @Override
1467         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, Register src2) {
1468             assert assertion.check((AMD64) asm.target.arch, LZ, dst, src1, src2, null);
1469             assert size == AVXSize.DWORD || size == AVXSize.QWORD;
1470             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, wEvex, false);
1471             asm.emitByte(op);
1472             asm.emitModRM(dst, src2);
1473         }
1474 
1475         @Override
1476         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, AMD64Address src2) {
1477             assert assertion.check((AMD64) asm.target.arch, LZ, dst, src1, null, null);
1478             assert size == AVXSize.DWORD || size == AVXSize.QWORD;
1479             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, wEvex, false);
1480             asm.emitByte(op);
1481             asm.emitOperandHelper(dst, src2, 0);
1482         }
1483     }
1484 
1485     public static final class VexGeneralPurposeRMVOp extends VexOp {
1486         // @formatter:off
1487         public static final VexGeneralPurposeRMVOp BEXTR  = new VexGeneralPurposeRMVOp(&quot;BEXTR&quot;,  P_,   M_0F38, WIG, 0xF7, VEXOpAssertion.BMI1);
1488         public static final VexGeneralPurposeRMVOp BZHI   = new VexGeneralPurposeRMVOp(&quot;BZHI&quot;,   P_,   M_0F38, WIG, 0xF5, VEXOpAssertion.BMI2);
1489         public static final VexGeneralPurposeRMVOp SARX   = new VexGeneralPurposeRMVOp(&quot;SARX&quot;,   P_F3, M_0F38, WIG, 0xF7, VEXOpAssertion.BMI2);
1490         public static final VexGeneralPurposeRMVOp SHRX   = new VexGeneralPurposeRMVOp(&quot;SHRX&quot;,   P_F2, M_0F38, WIG, 0xF7, VEXOpAssertion.BMI2);
1491         public static final VexGeneralPurposeRMVOp SHLX   = new VexGeneralPurposeRMVOp(&quot;SHLX&quot;,   P_66, M_0F38, WIG, 0xF7, VEXOpAssertion.BMI2);
1492         // @formatter:on
1493 
1494         private VexGeneralPurposeRMVOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1495             super(opcode, pp, mmmmm, w, op, assertion);
1496         }
1497 
1498         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, Register src2) {
1499             assert assertion.check((AMD64) asm.target.arch, LZ, dst, src2, src1, null);
1500             assert size == AVXSize.DWORD || size == AVXSize.QWORD;
1501             asm.vexPrefix(dst, src2, src1, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, wEvex, false);
1502             asm.emitByte(op);
1503             asm.emitModRM(dst, src1);
1504         }
1505 
1506         public void emit(AMD64Assembler asm, AVXSize size, Register dst, AMD64Address src1, Register src2) {
1507             assert assertion.check((AMD64) asm.target.arch, LZ, dst, src2, null, null);
1508             assert size == AVXSize.DWORD || size == AVXSize.QWORD;
1509             asm.vexPrefix(dst, src2, src1, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, wEvex, false);
1510             asm.emitByte(op);
1511             asm.emitOperandHelper(dst, src1, 0);
1512         }
1513     }
1514 
1515     public static final class VexGeneralPurposeRMOp extends VexRMOp {
1516         // @formatter:off
1517         public static final VexGeneralPurposeRMOp BLSI    = new VexGeneralPurposeRMOp(&quot;BLSI&quot;,   P_,    M_0F38, WIG, 0xF3, 3, VEXOpAssertion.BMI1);
1518         public static final VexGeneralPurposeRMOp BLSMSK  = new VexGeneralPurposeRMOp(&quot;BLSMSK&quot;, P_,    M_0F38, WIG, 0xF3, 2, VEXOpAssertion.BMI1);
1519         public static final VexGeneralPurposeRMOp BLSR    = new VexGeneralPurposeRMOp(&quot;BLSR&quot;,   P_,    M_0F38, WIG, 0xF3, 1, VEXOpAssertion.BMI1);
1520         // @formatter:on
1521         private final int ext;
1522 
1523         private VexGeneralPurposeRMOp(String opcode, int pp, int mmmmm, int w, int op, int ext, VEXOpAssertion assertion) {
1524             super(opcode, pp, mmmmm, w, op, assertion);
1525             this.ext = ext;
1526         }
1527 
1528         @Override
1529         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src) {
1530             assert assertion.check((AMD64) asm.target.arch, size, dst, null, null);
1531             asm.vexPrefix(AMD64.cpuRegisters[ext], dst, src, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, wEvex, false);
1532             asm.emitByte(op);
1533             asm.emitModRM(ext, src);
1534         }
1535 
1536         @Override
1537         public void emit(AMD64Assembler asm, AVXSize size, Register dst, AMD64Address src) {
1538             assert assertion.check((AMD64) asm.target.arch, size, dst, null, null);
1539             asm.vexPrefix(AMD64.cpuRegisters[ext], dst, src, size, pp, mmmmm, size == AVXSize.DWORD ? W0 : W1, wEvex, false);
1540             asm.emitByte(op);
1541             asm.emitOperandHelper(ext, src, 0);
1542         }
1543     }
1544 
1545     /**
1546      * VEX-encoded shift instructions with an operand order of either RVM or VMI.
1547      */
1548     public static final class VexShiftOp extends VexRVMOp implements VexRRIOp {
1549         // @formatter:off
1550         public static final VexShiftOp VPSRLW = new VexShiftOp(&quot;VPSRLW&quot;, P_66, M_0F, WIG, 0xD1, 0x71, 2);
1551         public static final VexShiftOp VPSRLD = new VexShiftOp(&quot;VPSRLD&quot;, P_66, M_0F, WIG, 0xD2, 0x72, 2);
1552         public static final VexShiftOp VPSRLQ = new VexShiftOp(&quot;VPSRLQ&quot;, P_66, M_0F, WIG, 0xD3, 0x73, 2);
1553         public static final VexShiftOp VPSRAW = new VexShiftOp(&quot;VPSRAW&quot;, P_66, M_0F, WIG, 0xE1, 0x71, 4);
1554         public static final VexShiftOp VPSRAD = new VexShiftOp(&quot;VPSRAD&quot;, P_66, M_0F, WIG, 0xE2, 0x72, 4);
1555         public static final VexShiftOp VPSLLW = new VexShiftOp(&quot;VPSLLW&quot;, P_66, M_0F, WIG, 0xF1, 0x71, 6);
1556         public static final VexShiftOp VPSLLD = new VexShiftOp(&quot;VPSLLD&quot;, P_66, M_0F, WIG, 0xF2, 0x72, 6);
1557         public static final VexShiftOp VPSLLQ = new VexShiftOp(&quot;VPSLLQ&quot;, P_66, M_0F, WIG, 0xF3, 0x73, 6);
1558         // @formatter:on
1559 
1560         private final int immOp;
1561         private final int r;
1562 
1563         private VexShiftOp(String opcode, int pp, int mmmmm, int w, int op, int immOp, int r) {
1564             super(opcode, pp, mmmmm, w, op, VEXOpAssertion.AVX1_2);
1565             this.immOp = immOp;
1566             this.r = r;
1567         }
1568 
1569         @Override
1570         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src, int imm8) {
1571             assert assertion.check((AMD64) asm.target.arch, size, null, dst, src);
1572             asm.vexPrefix(null, dst, src, size, pp, mmmmm, w, wEvex, false);
1573             asm.emitByte(immOp);
1574             asm.emitModRM(r, src);
1575             asm.emitByte(imm8);
1576         }
1577     }
1578 
1579     public static final class VexMaskMoveOp extends VexOp {
1580         // @formatter:off
1581         public static final VexMaskMoveOp VMASKMOVPS = new VexMaskMoveOp(&quot;VMASKMOVPS&quot;, P_66, M_0F38, W0, 0x2C, 0x2E);
1582         public static final VexMaskMoveOp VMASKMOVPD = new VexMaskMoveOp(&quot;VMASKMOVPD&quot;, P_66, M_0F38, W0, 0x2D, 0x2F);
1583         public static final VexMaskMoveOp VPMASKMOVD = new VexMaskMoveOp(&quot;VPMASKMOVD&quot;, P_66, M_0F38, W0, 0x8C, 0x8E, VEXOpAssertion.AVX2);
1584         public static final VexMaskMoveOp VPMASKMOVQ = new VexMaskMoveOp(&quot;VPMASKMOVQ&quot;, P_66, M_0F38, W1, 0x8C, 0x8E, VEXOpAssertion.AVX2);
1585         // @formatter:on
1586 
1587         private final int opReverse;
1588 
1589         private VexMaskMoveOp(String opcode, int pp, int mmmmm, int w, int op, int opReverse) {
1590             this(opcode, pp, mmmmm, w, op, opReverse, VEXOpAssertion.AVX1);
1591         }
1592 
1593         private VexMaskMoveOp(String opcode, int pp, int mmmmm, int w, int op, int opReverse, VEXOpAssertion assertion) {
1594             super(opcode, pp, mmmmm, w, op, assertion);
1595             this.opReverse = opReverse;
1596         }
1597 
1598         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register mask, AMD64Address src) {
1599             assert assertion.check((AMD64) asm.target.arch, size, dst, mask, null);
1600             asm.vexPrefix(dst, mask, src, size, pp, mmmmm, w, wEvex, false);
1601             asm.emitByte(op);
1602             asm.emitOperandHelper(dst, src, 0);
1603         }
1604 
1605         public void emit(AMD64Assembler asm, AVXSize size, AMD64Address dst, Register mask, Register src) {
1606             assert assertion.check((AMD64) asm.target.arch, size, src, mask, null);
1607             boolean useEvex = asm.vexPrefix(src, mask, dst, size, pp, mmmmm, w, wEvex, false);
1608             asm.emitByte(opReverse);
1609             asm.emitOperandHelper(src, dst, 0, getDisp8Scale(useEvex, size));
1610         }
1611     }
1612 
1613     /**
1614      * VEX-encoded instructions with an operand order of RVMI.
1615      */
1616     public static final class VexRVMIOp extends VexOp {
1617         // @formatter:off
1618         public static final VexRVMIOp VSHUFPS     = new VexRVMIOp(&quot;VSHUFPS&quot;,     P_,   M_0F,   WIG, 0xC6);
1619         public static final VexRVMIOp VSHUFPD     = new VexRVMIOp(&quot;VSHUFPD&quot;,     P_66, M_0F,   WIG, 0xC6);
1620         public static final VexRVMIOp VINSERTF128 = new VexRVMIOp(&quot;VINSERTF128&quot;, P_66, M_0F3A, W0,  0x18, VEXOpAssertion.AVX1_256ONLY);
1621         public static final VexRVMIOp VINSERTI128 = new VexRVMIOp(&quot;VINSERTI128&quot;, P_66, M_0F3A, W0,  0x38, VEXOpAssertion.AVX2_256ONLY);
1622         // @formatter:on
1623 
1624         private VexRVMIOp(String opcode, int pp, int mmmmm, int w, int op) {
1625             this(opcode, pp, mmmmm, w, op, VEXOpAssertion.AVX1);
1626         }
1627 
1628         private VexRVMIOp(String opcode, int pp, int mmmmm, int w, int op, VEXOpAssertion assertion) {
1629             super(opcode, pp, mmmmm, w, op, assertion);
1630         }
1631 
1632         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, Register src2, int imm8) {
1633             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, src2);
1634             assert (imm8 &amp; 0xFF) == imm8;
1635             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, wEvex, false);
1636             asm.emitByte(op);
1637             asm.emitModRM(dst, src2);
1638             asm.emitByte(imm8);
1639         }
1640 
1641         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, AMD64Address src2, int imm8) {
1642             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, null);
1643             assert (imm8 &amp; 0xFF) == imm8;
1644             boolean useEvex = asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, wEvex, false);
1645             asm.emitByte(op);
1646             asm.emitOperandHelper(dst, src2, 1, getDisp8Scale(useEvex, size));
1647             asm.emitByte(imm8);
1648         }
1649     }
1650 
1651     /**
1652      * VEX-encoded comparison operation with an operand order of RVMI. The immediate operand is a
1653      * comparison operator.
1654      */
1655     public static final class VexFloatCompareOp extends VexOp {
1656         // @formatter:off
1657         public static final VexFloatCompareOp VCMPPS = new VexFloatCompareOp(&quot;VCMPPS&quot;, P_,   M_0F, WIG, 0xC2);
1658         public static final VexFloatCompareOp VCMPPD = new VexFloatCompareOp(&quot;VCMPPD&quot;, P_66, M_0F, WIG, 0xC2);
1659         public static final VexFloatCompareOp VCMPSS = new VexFloatCompareOp(&quot;VCMPSS&quot;, P_F2, M_0F, WIG, 0xC2);
1660         public static final VexFloatCompareOp VCMPSD = new VexFloatCompareOp(&quot;VCMPSD&quot;, P_F2, M_0F, WIG, 0xC2);
1661         // @formatter:on
1662 
1663         public enum Predicate {
1664             EQ_OQ(0x00),
1665             LT_OS(0x01),
1666             LE_OS(0x02),
1667             UNORD_Q(0x03),
1668             NEQ_UQ(0x04),
1669             NLT_US(0x05),
1670             NLE_US(0x06),
1671             ORD_Q(0x07),
1672             EQ_UQ(0x08),
1673             NGE_US(0x09),
1674             NGT_US(0x0a),
1675             FALSE_OQ(0x0b),
1676             NEQ_OQ(0x0c),
1677             GE_OS(0x0d),
1678             GT_OS(0x0e),
1679             TRUE_UQ(0x0f),
1680             EQ_OS(0x10),
1681             LT_OQ(0x11),
1682             LE_OQ(0x12),
1683             UNORD_S(0x13),
1684             NEQ_US(0x14),
1685             NLT_UQ(0x15),
1686             NLE_UQ(0x16),
1687             ORD_S(0x17),
1688             EQ_US(0x18),
1689             NGE_UQ(0x19),
1690             NGT_UQ(0x1a),
1691             FALSE_OS(0x1b),
1692             NEQ_OS(0x1c),
1693             GE_OQ(0x1d),
1694             GT_OQ(0x1e),
1695             TRUE_US(0x1f);
1696 
1697             private int imm8;
1698 
1699             Predicate(int imm8) {
1700                 this.imm8 = imm8;
1701             }
1702 
1703             public static Predicate getPredicate(Condition condition, boolean unorderedIsTrue) {
1704                 if (unorderedIsTrue) {
1705                     switch (condition) {
1706                         case EQ:
1707                             return EQ_UQ;
1708                         case NE:
1709                             return NEQ_UQ;
1710                         case LT:
1711                             return NGE_UQ;
1712                         case LE:
1713                             return NGT_UQ;
1714                         case GT:
1715                             return NLE_UQ;
1716                         case GE:
1717                             return NLT_UQ;
1718                         default:
1719                             throw GraalError.shouldNotReachHere();
1720                     }
1721                 } else {
1722                     switch (condition) {
1723                         case EQ:
1724                             return EQ_OQ;
1725                         case NE:
1726                             return NEQ_OQ;
1727                         case LT:
1728                             return LT_OQ;
1729                         case LE:
1730                             return LE_OQ;
1731                         case GT:
1732                             return GT_OQ;
1733                         case GE:
1734                             return GE_OQ;
1735                         default:
1736                             throw GraalError.shouldNotReachHere();
1737                     }
1738                 }
1739             }
1740         }
1741 
1742         private VexFloatCompareOp(String opcode, int pp, int mmmmm, int w, int op) {
1743             super(opcode, pp, mmmmm, w, op, VEXOpAssertion.AVX1);
1744         }
1745 
1746         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, Register src2, Predicate p) {
1747             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, src2);
1748             asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, wEvex, false);
1749             asm.emitByte(op);
1750             asm.emitModRM(dst, src2);
1751             asm.emitByte(p.imm8);
1752         }
1753 
1754         public void emit(AMD64Assembler asm, AVXSize size, Register dst, Register src1, AMD64Address src2, Predicate p) {
1755             assert assertion.check((AMD64) asm.target.arch, size, dst, src1, null);
1756             boolean useEvex = asm.vexPrefix(dst, src1, src2, size, pp, mmmmm, w, wEvex, false);
1757             asm.emitByte(op);
1758             asm.emitOperandHelper(dst, src2, 1, getDisp8Scale(useEvex, size));
1759             asm.emitByte(p.imm8);
1760         }
1761     }
1762 
1763     public final void addl(AMD64Address dst, int imm32) {
1764         ADD.getMIOpcode(DWORD, isByte(imm32)).emit(this, DWORD, dst, imm32);
1765     }
1766 
1767     public final void addl(Register dst, int imm32) {
1768         ADD.getMIOpcode(DWORD, isByte(imm32)).emit(this, DWORD, dst, imm32);
1769     }
1770 
1771     public final void addl(Register dst, Register src) {
1772         ADD.rmOp.emit(this, DWORD, dst, src);
1773     }
1774 
1775     public final void addpd(Register dst, Register src) {
1776         SSEOp.ADD.emit(this, PD, dst, src);
1777     }
1778 
1779     public final void addpd(Register dst, AMD64Address src) {
1780         SSEOp.ADD.emit(this, PD, dst, src);
1781     }
1782 
1783     public final void addsd(Register dst, Register src) {
1784         SSEOp.ADD.emit(this, SD, dst, src);
1785     }
1786 
1787     public final void addsd(Register dst, AMD64Address src) {
1788         SSEOp.ADD.emit(this, SD, dst, src);
1789     }
1790 
1791     private void addrNop4() {
1792         // 4 bytes: NOP DWORD PTR [EAX+0]
1793         emitByte(0x0F);
1794         emitByte(0x1F);
1795         emitByte(0x40); // emitRm(cbuf, 0x1, EAXEnc, EAXEnc);
1796         emitByte(0); // 8-bits offset (1 byte)
1797     }
1798 
1799     private void addrNop5() {
1800         // 5 bytes: NOP DWORD PTR [EAX+EAX*0+0] 8-bits offset
1801         emitByte(0x0F);
1802         emitByte(0x1F);
1803         emitByte(0x44); // emitRm(cbuf, 0x1, EAXEnc, 0x4);
1804         emitByte(0x00); // emitRm(cbuf, 0x0, EAXEnc, EAXEnc);
1805         emitByte(0); // 8-bits offset (1 byte)
1806     }
1807 
1808     private void addrNop7() {
1809         // 7 bytes: NOP DWORD PTR [EAX+0] 32-bits offset
1810         emitByte(0x0F);
1811         emitByte(0x1F);
1812         emitByte(0x80); // emitRm(cbuf, 0x2, EAXEnc, EAXEnc);
1813         emitInt(0); // 32-bits offset (4 bytes)
1814     }
1815 
1816     private void addrNop8() {
1817         // 8 bytes: NOP DWORD PTR [EAX+EAX*0+0] 32-bits offset
1818         emitByte(0x0F);
1819         emitByte(0x1F);
1820         emitByte(0x84); // emitRm(cbuf, 0x2, EAXEnc, 0x4);
1821         emitByte(0x00); // emitRm(cbuf, 0x0, EAXEnc, EAXEnc);
1822         emitInt(0); // 32-bits offset (4 bytes)
1823     }
1824 
1825     public final void andl(Register dst, int imm32) {
1826         AND.getMIOpcode(DWORD, isByte(imm32)).emit(this, DWORD, dst, imm32);
1827     }
1828 
1829     public final void andl(Register dst, Register src) {
1830         AND.rmOp.emit(this, DWORD, dst, src);
1831     }
1832 
1833     public final void andpd(Register dst, Register src) {
1834         SSEOp.AND.emit(this, PD, dst, src);
1835     }
1836 
1837     public final void andpd(Register dst, AMD64Address src) {
1838         SSEOp.AND.emit(this, PD, dst, src);
1839     }
1840 
1841     public final void bsfq(Register dst, Register src) {
1842         prefixq(dst, src);
1843         emitByte(0x0F);
1844         emitByte(0xBC);
1845         emitModRM(dst, src);
1846     }
1847 
1848     public final void bsrl(Register dst, Register src) {
1849         prefix(dst, src);
1850         emitByte(0x0F);
1851         emitByte(0xBD);
1852         emitModRM(dst, src);
1853     }
1854 
1855     public final void bswapl(Register reg) {
1856         prefix(reg);
1857         emitByte(0x0F);
1858         emitModRM(1, reg);
1859     }
1860 
1861     public final void cdql() {
1862         emitByte(0x99);
1863     }
1864 
1865     public final void cmovl(ConditionFlag cc, Register dst, Register src) {
1866         prefix(dst, src);
1867         emitByte(0x0F);
1868         emitByte(0x40 | cc.getValue());
1869         emitModRM(dst, src);
1870     }
1871 
1872     public final void cmovl(ConditionFlag cc, Register dst, AMD64Address src) {
1873         prefix(src, dst);
1874         emitByte(0x0F);
1875         emitByte(0x40 | cc.getValue());
1876         emitOperandHelper(dst, src, 0);
1877     }
1878 
1879     public final void cmpb(Register dst, Register src) {
1880         CMP.byteRmOp.emit(this, BYTE, dst, src);
1881     }
1882 
1883     public final void cmpw(Register dst, Register src) {
1884         CMP.rmOp.emit(this, WORD, dst, src);
1885     }
1886 
1887     public final void cmpl(Register dst, int imm32) {
1888         CMP.getMIOpcode(DWORD, isByte(imm32)).emit(this, DWORD, dst, imm32);
1889     }
1890 
1891     public final void cmpl(Register dst, Register src) {
1892         CMP.rmOp.emit(this, DWORD, dst, src);
1893     }
1894 
1895     public final void cmpl(Register dst, AMD64Address src) {
1896         CMP.rmOp.emit(this, DWORD, dst, src);
1897     }
1898 
1899     public final void cmpl(AMD64Address dst, int imm32) {
1900         CMP.getMIOpcode(DWORD, isByte(imm32)).emit(this, DWORD, dst, imm32);
1901     }
1902 
1903     /**
1904      * The 8-bit cmpxchg compares the value at adr with the contents of X86.rax, and stores reg into
1905      * adr if so; otherwise, the value at adr is loaded into X86.rax,. The ZF is set if the compared
1906      * values were equal, and cleared otherwise.
1907      */
1908     public final void cmpxchgb(Register reg, AMD64Address adr) { // cmpxchg
1909         prefixb(adr, reg);
1910         emitByte(0x0F);
1911         emitByte(0xB0);
1912         emitOperandHelper(reg, adr, 0);
1913     }
1914 
1915     /**
1916      * The 16-bit cmpxchg compares the value at adr with the contents of X86.rax, and stores reg
1917      * into adr if so; otherwise, the value at adr is loaded into X86.rax,. The ZF is set if the
1918      * compared values were equal, and cleared otherwise.
1919      */
1920     public final void cmpxchgw(Register reg, AMD64Address adr) { // cmpxchg
1921         emitByte(0x66); // Switch to 16-bit mode.
1922         prefix(adr, reg);
1923         emitByte(0x0F);
1924         emitByte(0xB1);
1925         emitOperandHelper(reg, adr, 0);
1926     }
1927 
1928     /**
1929      * The 32-bit cmpxchg compares the value at adr with the contents of X86.rax, and stores reg
1930      * into adr if so; otherwise, the value at adr is loaded into X86.rax,. The ZF is set if the
1931      * compared values were equal, and cleared otherwise.
1932      */
1933     public final void cmpxchgl(Register reg, AMD64Address adr) { // cmpxchg
1934         prefix(adr, reg);
1935         emitByte(0x0F);
1936         emitByte(0xB1);
1937         emitOperandHelper(reg, adr, 0);
1938     }
1939 
1940     public final void cvtsi2sdl(Register dst, Register src) {
1941         SSEOp.CVTSI2SD.emit(this, DWORD, dst, src);
1942     }
1943 
1944     public final void cvttsd2sil(Register dst, Register src) {
1945         SSEOp.CVTTSD2SI.emit(this, DWORD, dst, src);
1946     }
1947 
1948     public final void decl(AMD64Address dst) {
1949         DEC.emit(this, DWORD, dst);
1950     }
1951 
1952     public final void divsd(Register dst, Register src) {
1953         SSEOp.DIV.emit(this, SD, dst, src);
1954     }
1955 
1956     public final void hlt() {
1957         emitByte(0xF4);
1958     }
1959 
1960     public final void imull(Register dst, Register src, int value) {
1961         if (isByte(value)) {
1962             AMD64RMIOp.IMUL_SX.emit(this, DWORD, dst, src, value);
1963         } else {
1964             AMD64RMIOp.IMUL.emit(this, DWORD, dst, src, value);
1965         }
1966     }
1967 
1968     public final void incl(AMD64Address dst) {
1969         INC.emit(this, DWORD, dst);
1970     }
1971 
1972     public static final int JCC_ERRATUM_MITIGATION_BOUNDARY = 0x20;
1973     public static final int OPCODE_IN_BYTES = 1;
1974     public static final int MODRM_IN_BYTES = 1;
1975 
1976     protected static int getPrefixInBytes(OperandSize size, Register dst, boolean dstIsByte) {
1977         boolean needsRex = needsRex(dst, dstIsByte);
1978         if (size == WORD) {
1979             return needsRex ? 2 : 1;
1980         }
1981         return size == QWORD || needsRex ? 1 : 0;
1982     }
1983 
1984     protected static int getPrefixInBytes(OperandSize size, AMD64Address src) {
1985         boolean needsRex = needsRex(src.getBase()) || needsRex(src.getIndex());
1986         if (size == WORD) {
1987             return needsRex ? 2 : 1;
1988         }
1989         return size == QWORD || needsRex ? 1 : 0;
1990     }
1991 
1992     protected static int getPrefixInBytes(OperandSize size, Register dst, boolean dstIsByte, Register src, boolean srcIsByte) {
1993         boolean needsRex = needsRex(dst, dstIsByte) || needsRex(src, srcIsByte);
1994         if (size == WORD) {
1995             return needsRex ? 2 : 1;
1996         }
1997         return size == QWORD || needsRex ? 1 : 0;
1998     }
1999 
2000     protected static int getPrefixInBytes(OperandSize size, Register dst, boolean dstIsByte, AMD64Address src) {
2001         boolean needsRex = needsRex(dst, dstIsByte) || needsRex(src.getBase()) || needsRex(src.getIndex());
2002         if (size == WORD) {
2003             return needsRex ? 2 : 1;
2004         }
2005         return size == QWORD || needsRex ? 1 : 0;
2006     }
2007 
2008     protected boolean mayCrossBoundary(int opStart, int opEnd) {
2009         return (opStart / JCC_ERRATUM_MITIGATION_BOUNDARY) != ((opEnd - 1) / JCC_ERRATUM_MITIGATION_BOUNDARY) || (opEnd % JCC_ERRATUM_MITIGATION_BOUNDARY) == 0;
2010     }
2011 
2012     private static int bytesUntilBoundary(int pos) {
2013         return JCC_ERRATUM_MITIGATION_BOUNDARY - (pos % JCC_ERRATUM_MITIGATION_BOUNDARY);
2014     }
2015 
2016     protected boolean ensureWithinBoundary(int opStart) {
2017         if (useBranchesWithin32ByteBoundary) {
2018             assert !mayCrossBoundary(opStart, position());
2019         }
2020         return true;
2021     }
2022 
2023     protected final void testAndAlign(int bytesToEmit) {
2024         if (useBranchesWithin32ByteBoundary) {
2025             int beforeNextOp = position();
2026             int afterNextOp = beforeNextOp + bytesToEmit;
2027             if (mayCrossBoundary(beforeNextOp, afterNextOp)) {
2028                 int bytesToShift = bytesUntilBoundary(beforeNextOp);
2029                 nop(bytesToShift);
2030                 if (codePatchShifter != null) {
2031                     codePatchShifter.shift(beforeNextOp, bytesToShift);
2032                 }
2033             }
2034         }
2035     }
2036 
2037     public void jcc(ConditionFlag cc, int jumpTarget, boolean forceDisp32) {
2038         final int shortSize = 2;
2039         final int longSize = 6;
2040 
2041         long disp = jumpTarget - position();
2042         if (!forceDisp32 &amp;&amp; isByte(disp - shortSize)) {
2043             testAndAlign(shortSize);
2044             // After alignment, isByte(disp - shortSize) might not hold. Need to check again.
2045             disp = jumpTarget - position();
2046             if (isByte(disp - shortSize)) {
2047                 // 0111 tttn #8-bit disp
2048                 emitByte(0x70 | cc.getValue());
2049                 emitByte((int) ((disp - shortSize) &amp; 0xFF));
2050                 return;
2051             }
2052         }
2053 
2054         // 0000 1111 1000 tttn #32-bit disp
2055         assert forceDisp32 || isInt(disp - longSize) : &quot;must be 32bit offset (call4)&quot;;
2056         testAndAlign(longSize);
2057         disp = jumpTarget - position();
2058         emitByte(0x0F);
2059         emitByte(0x80 | cc.getValue());
2060         emitInt((int) (disp - longSize));
2061     }
2062 
2063     public final void jcc(ConditionFlag cc, Label l) {
2064         assert (0 &lt;= cc.getValue()) &amp;&amp; (cc.getValue() &lt; 16) : &quot;illegal cc&quot;;
2065         if (l.isBound()) {
2066             jcc(cc, l.position(), false);
2067         } else {
2068             testAndAlign(6);
2069             // Note: could eliminate cond. jumps to this jump if condition
2070             // is the same however, seems to be rather unlikely case.
2071             // Note: use jccb() if label to be bound is very close to get
2072             // an 8-bit displacement
2073             l.addPatchAt(position(), this);
2074             emitByte(0x0F);
2075             emitByte(0x80 | cc.getValue());
2076             emitInt(0);
2077         }
2078     }
2079 
2080     public final void jccb(ConditionFlag cc, Label l) {
2081         final int shortSize = 2;
2082         testAndAlign(shortSize);
2083         if (l.isBound()) {
2084             int entry = l.position();
2085             assert isByte(entry - (position() + shortSize)) : &quot;Displacement too large for a short jmp&quot;;
2086             long disp = entry - position();
2087             // 0111 tttn #8-bit disp
2088             emitByte(0x70 | cc.getValue());
2089             emitByte((int) ((disp - shortSize) &amp; 0xFF));
2090         } else {
2091             l.addPatchAt(position(), this);
2092             emitByte(0x70 | cc.getValue());
2093             emitByte(0);
2094         }
2095     }
2096 
2097     public final void jcc(ConditionFlag cc, Label branchTarget, boolean isShortJmp) {
2098         if (branchTarget == null) {
2099             // jump to placeholder
2100             jcc(cc, 0, true);
2101         } else if (isShortJmp) {
2102             jccb(cc, branchTarget);
2103         } else {
2104             jcc(cc, branchTarget);
2105         }
2106     }
2107 
2108     /**
2109      * Emit a jmp instruction given a known target address.
2110      *
2111      * @return the position where the jmp instruction starts.
2112      */
2113     public final int jmp(int jumpTarget, boolean forceDisp32) {
2114         final int shortSize = 2;
2115         final int longSize = 5;
2116         // For long jmp, the jmp instruction will cross the jcc-erratum-mitigation-boundary when the
2117         // current position is between [0x1b, 0x1f]. For short jmp [0x1e, 0x1f], which is covered by
2118         // the long jmp triggering range.
2119         if (!forceDisp32) {
2120             // We first align the next jmp assuming it will be short jmp.
2121             testAndAlign(shortSize);
2122             int pos = position();
2123             long disp = jumpTarget - pos;
2124             if (isByte(disp - shortSize)) {
2125                 emitByte(0xEB);
2126                 emitByte((int) ((disp - shortSize) &amp; 0xFF));
2127                 return pos;
2128             }
2129         }
2130 
2131         testAndAlign(longSize);
2132         int pos = position();
2133         long disp = jumpTarget - pos;
2134         emitByte(0xE9);
2135         emitInt((int) (disp - longSize));
2136         return pos;
2137     }
2138 
2139     @Override
2140     public final void jmp(Label l) {
2141         if (l.isBound()) {
2142             jmp(l.position(), false);
2143         } else {
2144             // By default, forward jumps are always 32-bit displacements, since
2145             // we can&#39;t yet know where the label will be bound. If you&#39;re sure that
2146             // the forward jump will not run beyond 256 bytes, use jmpb to
2147             // force an 8-bit displacement.
2148             testAndAlign(5);
2149             l.addPatchAt(position(), this);
2150             emitByte(0xE9);
2151             emitInt(0);
2152         }
2153     }
2154 
2155     protected final void jmpWithoutAlignment(Register entry) {
2156         prefix(entry);
2157         emitByte(0xFF);
2158         emitModRM(4, entry);
2159     }
2160 
2161     public final void jmp(Register entry) {
2162         int bytesToEmit = needsRex(entry) ? 3 : 2;
2163         testAndAlign(bytesToEmit);
2164         int beforeJmp = position();
2165         jmpWithoutAlignment(entry);
2166         assert beforeJmp + bytesToEmit == position();
2167     }
2168 
2169     public final void jmp(AMD64Address adr) {
2170         int bytesToEmit = getPrefixInBytes(DWORD, adr) + OPCODE_IN_BYTES + addressInBytes(adr);
2171         testAndAlign(bytesToEmit);
2172         int beforeJmp = position();
2173         prefix(adr);
2174         emitByte(0xFF);
2175         emitOperandHelper(AMD64.rsp, adr, 0);
2176         assert beforeJmp + bytesToEmit == position();
2177     }
2178 
2179     /**
2180      * This method should be synchronized with
2181      * {@link AMD64BaseAssembler#emitOperandHelper(Register, AMD64Address, int)}}.
2182      */
2183     protected static int addressInBytes(AMD64Address addr) {
2184         Register base = addr.getBase();
2185         Register index = addr.getIndex();
2186         int disp = addr.getDisplacement();
2187 
2188         if (base.equals(AMD64.rip)) {
2189             return 5;
2190         } else if (base.isValid()) {
2191             final boolean isZeroDisplacement = disp == 0 &amp;&amp; !base.equals(rbp) &amp;&amp; !base.equals(r13);
2192             if (index.isValid()) {
2193                 if (isZeroDisplacement) {
2194                     return 2;
2195                 } else if (isByte(disp)) {
2196                     return 3;
2197                 } else {
2198                     return 6;
2199                 }
2200             } else if (base.equals(rsp) || base.equals(r12)) {
2201                 if (disp == 0) {
2202                     return 2;
2203                 } else if (isByte(disp)) {
2204                     return 3;
2205                 } else {
2206                     return 6;
2207                 }
2208             } else {
2209                 if (isZeroDisplacement) {
2210                     return 1;
2211                 } else if (isByte(disp)) {
2212                     return 2;
2213                 } else {
2214                     return 5;
2215                 }
2216             }
2217         } else {
2218             return 6;
2219         }
2220     }
2221 
2222     public final void jmpb(Label l) {
2223         final int shortSize = 2;
2224         testAndAlign(shortSize);
2225         if (l.isBound()) {
2226             // Displacement is relative to byte just after jmpb instruction
2227             int displacement = l.position() - position() - shortSize;
2228             GraalError.guarantee(isByte(displacement), &quot;Displacement too large to be encoded as a byte: %d&quot;, displacement);
2229             emitByte(0xEB);
2230             emitByte(displacement &amp; 0xFF);
2231         } else {
2232             l.addPatchAt(position(), this);
2233             emitByte(0xEB);
2234             emitByte(0);
2235         }
2236     }
2237 
2238     public final void lead(Register dst, AMD64Address src) {
2239         prefix(src, dst);
2240         emitByte(0x8D);
2241         emitOperandHelper(dst, src, 0);
2242     }
2243 
2244     public final void leaq(Register dst, AMD64Address src) {
2245         prefixq(src, dst);
2246         emitByte(0x8D);
2247         emitOperandHelper(dst, src, 0);
2248     }
2249 
2250     public final void leave() {
2251         emitByte(0xC9);
2252     }
2253 
2254     public final void lock() {
2255         emitByte(0xF0);
2256     }
2257 
2258     public final void movapd(Register dst, Register src) {
2259         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
2260         simdPrefix(dst, Register.None, src, PD, P_0F, false);
2261         emitByte(0x28);
2262         emitModRM(dst, src);
2263     }
2264 
2265     public final void movaps(Register dst, Register src) {
2266         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
2267         simdPrefix(dst, Register.None, src, PS, P_0F, false);
2268         emitByte(0x28);
2269         emitModRM(dst, src);
2270     }
2271 
2272     public final void movb(AMD64Address dst, int imm8) {
2273         prefix(dst);
2274         emitByte(0xC6);
2275         emitOperandHelper(0, dst, 1);
2276         emitByte(imm8);
2277     }
2278 
2279     public final void movb(AMD64Address dst, Register src) {
2280         assert inRC(CPU, src) : &quot;must have byte register&quot;;
2281         prefixb(dst, src);
2282         emitByte(0x88);
2283         emitOperandHelper(src, dst, 0);
2284     }
2285 
2286     public final void movl(Register dst, int imm32) {
2287         movl(dst, imm32, false);
2288     }
2289 
2290     public final void movl(Register dst, int imm32, boolean annotateImm) {
2291         int insnPos = position();
2292         prefix(dst);
2293         emitByte(0xB8 + encode(dst));
2294         int immPos = position();
2295         emitInt(imm32);
2296         int nextInsnPos = position();
2297         if (annotateImm &amp;&amp; codePatchingAnnotationConsumer != null) {
2298             codePatchingAnnotationConsumer.accept(new OperandDataAnnotation(insnPos, immPos, nextInsnPos - immPos, nextInsnPos));
2299         }
2300     }
2301 
2302     public final void movl(Register dst, Register src) {
2303         prefix(dst, src);
2304         emitByte(0x8B);
2305         emitModRM(dst, src);
2306     }
2307 
2308     public final void movl(Register dst, AMD64Address src) {
2309         prefix(src, dst);
2310         emitByte(0x8B);
2311         emitOperandHelper(dst, src, 0);
2312     }
2313 
2314     /**
2315      * @param wide use 4 byte encoding for displacements that would normally fit in a byte
2316      */
2317     public final void movl(Register dst, AMD64Address src, boolean wide) {
2318         prefix(src, dst);
2319         emitByte(0x8B);
2320         emitOperandHelper(dst, src, wide, 0);
2321     }
2322 
2323     public final void movl(AMD64Address dst, int imm32) {
2324         prefix(dst);
2325         emitByte(0xC7);
2326         emitOperandHelper(0, dst, 4);
2327         emitInt(imm32);
2328     }
2329 
2330     public final void movl(AMD64Address dst, Register src) {
2331         prefix(dst, src);
2332         emitByte(0x89);
2333         emitOperandHelper(src, dst, 0);
2334     }
2335 
2336     /**
2337      * New CPUs require use of movsd and movss to avoid partial register stall when loading from
2338      * memory. But for old Opteron use movlpd instead of movsd. The selection is done in
2339      * {@link AMD64MacroAssembler#movdbl(Register, AMD64Address)} and
2340      * {@link AMD64MacroAssembler#movflt(Register, Register)}.
2341      */
2342     public final void movlpd(Register dst, AMD64Address src) {
2343         assert inRC(XMM, dst);
2344         simdPrefix(dst, dst, src, PD, P_0F, false);
2345         emitByte(0x12);
2346         emitOperandHelper(dst, src, 0);
2347     }
2348 
2349     public final void movlhps(Register dst, Register src) {
2350         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
2351         simdPrefix(dst, src, src, PS, P_0F, false);
2352         emitByte(0x16);
2353         emitModRM(dst, src);
2354     }
2355 
2356     public final void movq(Register dst, AMD64Address src) {
2357         movq(dst, src, false);
2358     }
2359 
2360     public final void movq(Register dst, AMD64Address src, boolean force4BytesDisplacement) {
2361         if (inRC(XMM, dst)) {
2362             // Insn: MOVQ xmm, r/m64
2363             // Code: F3 0F 7E /r
2364             // An alternative instruction would be 66 REX.W 0F 6E /r. We prefer the REX.W free
2365             // format, because it would allow us to emit 2-bytes-prefixed vex-encoding instruction
2366             // when applicable.
2367             simdPrefix(dst, Register.None, src, SS, P_0F, false);
2368             emitByte(0x7E);
2369             emitOperandHelper(dst, src, force4BytesDisplacement, 0);
2370         } else {
2371             // gpr version of movq
2372             prefixq(src, dst);
2373             emitByte(0x8B);
2374             emitOperandHelper(dst, src, force4BytesDisplacement, 0);
2375         }
2376     }
2377 
2378     public final void movq(Register dst, Register src) {
2379         assert inRC(CPU, dst) &amp;&amp; inRC(CPU, src);
2380         prefixq(dst, src);
2381         emitByte(0x8B);
2382         emitModRM(dst, src);
2383     }
2384 
2385     public final void movq(AMD64Address dst, Register src) {
2386         if (inRC(XMM, src)) {
2387             // Insn: MOVQ r/m64, xmm
2388             // Code: 66 0F D6 /r
2389             // An alternative instruction would be 66 REX.W 0F 7E /r. We prefer the REX.W free
2390             // format, because it would allow us to emit 2-bytes-prefixed vex-encoding instruction
2391             // when applicable.
2392             simdPrefix(src, Register.None, dst, PD, P_0F, false);
2393             emitByte(0xD6);
2394             emitOperandHelper(src, dst, 0);
2395         } else {
2396             // gpr version of movq
2397             prefixq(dst, src);
2398             emitByte(0x89);
2399             emitOperandHelper(src, dst, 0);
2400         }
2401     }
2402 
2403     public final void movsbl(Register dst, AMD64Address src) {
2404         prefix(src, dst);
2405         emitByte(0x0F);
2406         emitByte(0xBE);
2407         emitOperandHelper(dst, src, 0);
2408     }
2409 
2410     public final void movsbl(Register dst, Register src) {
2411         prefix(dst, false, src, true);
2412         emitByte(0x0F);
2413         emitByte(0xBE);
2414         emitModRM(dst, src);
2415     }
2416 
2417     public final void movsbq(Register dst, AMD64Address src) {
2418         prefixq(src, dst);
2419         emitByte(0x0F);
2420         emitByte(0xBE);
2421         emitOperandHelper(dst, src, 0);
2422     }
2423 
2424     public final void movsbq(Register dst, Register src) {
2425         prefixq(dst, src);
2426         emitByte(0x0F);
2427         emitByte(0xBE);
2428         emitModRM(dst, src);
2429     }
2430 
2431     public final void movsd(Register dst, Register src) {
2432         AMD64RMOp.MOVSD.emit(this, SD, dst, src);
2433     }
2434 
2435     public final void movsd(Register dst, AMD64Address src) {
2436         AMD64RMOp.MOVSD.emit(this, SD, dst, src);
2437     }
2438 
2439     public final void movsd(AMD64Address dst, Register src) {
2440         AMD64MROp.MOVSD.emit(this, SD, dst, src);
2441     }
2442 
2443     public final void movss(Register dst, Register src) {
2444         AMD64RMOp.MOVSS.emit(this, SS, dst, src);
2445     }
2446 
2447     public final void movss(Register dst, AMD64Address src) {
2448         AMD64RMOp.MOVSS.emit(this, SS, dst, src);
2449     }
2450 
2451     public final void movss(AMD64Address dst, Register src) {
2452         AMD64MROp.MOVSS.emit(this, SS, dst, src);
2453     }
2454 
2455     public final void mulpd(Register dst, Register src) {
2456         SSEOp.MUL.emit(this, PD, dst, src);
2457     }
2458 
2459     public final void mulpd(Register dst, AMD64Address src) {
2460         SSEOp.MUL.emit(this, PD, dst, src);
2461     }
2462 
2463     public final void mulsd(Register dst, Register src) {
2464         SSEOp.MUL.emit(this, SD, dst, src);
2465     }
2466 
2467     public final void mulsd(Register dst, AMD64Address src) {
2468         SSEOp.MUL.emit(this, SD, dst, src);
2469     }
2470 
2471     public final void mulss(Register dst, Register src) {
2472         SSEOp.MUL.emit(this, SS, dst, src);
2473     }
2474 
2475     public final void movswl(Register dst, AMD64Address src) {
2476         AMD64RMOp.MOVSX.emit(this, DWORD, dst, src);
2477     }
2478 
2479     public final void movswq(Register dst, AMD64Address src) {
2480         AMD64RMOp.MOVSX.emit(this, QWORD, dst, src);
2481     }
2482 
2483     public final void movw(AMD64Address dst, int imm16) {
2484         emitByte(0x66); // switch to 16-bit mode
2485         prefix(dst);
2486         emitByte(0xC7);
2487         emitOperandHelper(0, dst, 2);
2488         emitShort(imm16);
2489     }
2490 
2491     public final void movw(AMD64Address dst, Register src) {
2492         emitByte(0x66);
2493         prefix(dst, src);
2494         emitByte(0x89);
2495         emitOperandHelper(src, dst, 0);
2496     }
2497 
2498     public final void movw(Register dst, AMD64Address src) {
2499         emitByte(0x66);
2500         prefix(src, dst);
2501         emitByte(0x8B);
2502         emitOperandHelper(dst, src, 0);
2503     }
2504 
2505     public final void movzbl(Register dst, AMD64Address src) {
2506         prefix(src, dst);
2507         emitByte(0x0F);
2508         emitByte(0xB6);
2509         emitOperandHelper(dst, src, 0);
2510     }
2511 
2512     public final void movzbl(Register dst, Register src) {
2513         AMD64RMOp.MOVZXB.emit(this, DWORD, dst, src);
2514     }
2515 
2516     public final void movzbq(Register dst, Register src) {
2517         AMD64RMOp.MOVZXB.emit(this, QWORD, dst, src);
2518     }
2519 
2520     public final void movzbq(Register dst, AMD64Address src) {
2521         AMD64RMOp.MOVZXB.emit(this, QWORD, dst, src);
2522     }
2523 
2524     public final void movzwl(Register dst, AMD64Address src) {
2525         AMD64RMOp.MOVZX.emit(this, DWORD, dst, src);
2526     }
2527 
2528     public final void movzwq(Register dst, AMD64Address src) {
2529         AMD64RMOp.MOVZX.emit(this, QWORD, dst, src);
2530     }
2531 
2532     public final void negl(Register dst) {
2533         NEG.emit(this, DWORD, dst);
2534     }
2535 
2536     public final void notl(Register dst) {
2537         NOT.emit(this, DWORD, dst);
2538     }
2539 
2540     public final void notq(Register dst) {
2541         NOT.emit(this, QWORD, dst);
2542     }
2543 
2544     @Override
2545     public final void ensureUniquePC() {
2546         nop();
2547     }
2548 
2549     public final void nop() {
2550         nop(1);
2551     }
2552 
2553     public void nop(int count) {
2554         int i = count;
2555         if (UseNormalNop) {
2556             assert i &gt; 0 : &quot; &quot;;
2557             // The fancy nops aren&#39;t currently recognized by debuggers making it a
2558             // pain to disassemble code while debugging. If assert are on clearly
2559             // speed is not an issue so simply use the single byte traditional nop
2560             // to do alignment.
2561 
2562             for (; i &gt; 0; i--) {
2563                 emitByte(0x90);
2564             }
2565             return;
2566         }
2567 
2568         if (UseAddressNop) {
2569             if (UseIntelNops) {
2570                 intelNops(i);
2571             } else {
2572                 amdNops(i);
2573             }
2574             return;
2575         }
2576 
2577         // Using nops with size prefixes &quot;0x66 0x90&quot;.
2578         // From AMD Optimization Guide:
2579         // 1: 0x90
2580         // 2: 0x66 0x90
2581         // 3: 0x66 0x66 0x90
2582         // 4: 0x66 0x66 0x66 0x90
2583         // 5: 0x66 0x66 0x90 0x66 0x90
2584         // 6: 0x66 0x66 0x90 0x66 0x66 0x90
2585         // 7: 0x66 0x66 0x66 0x90 0x66 0x66 0x90
2586         // 8: 0x66 0x66 0x66 0x90 0x66 0x66 0x66 0x90
2587         // 9: 0x66 0x66 0x90 0x66 0x66 0x90 0x66 0x66 0x90
2588         // 10: 0x66 0x66 0x66 0x90 0x66 0x66 0x90 0x66 0x66 0x90
2589         //
2590         while (i &gt; 12) {
2591             i -= 4;
2592             emitByte(0x66); // size prefix
2593             emitByte(0x66);
2594             emitByte(0x66);
2595             emitByte(0x90); // nop
2596         }
2597         // 1 - 12 nops
2598         if (i &gt; 8) {
2599             if (i &gt; 9) {
2600                 i -= 1;
2601                 emitByte(0x66);
2602             }
2603             i -= 3;
2604             emitByte(0x66);
2605             emitByte(0x66);
2606             emitByte(0x90);
2607         }
2608         // 1 - 8 nops
2609         if (i &gt; 4) {
2610             if (i &gt; 6) {
2611                 i -= 1;
2612                 emitByte(0x66);
2613             }
2614             i -= 3;
2615             emitByte(0x66);
2616             emitByte(0x66);
2617             emitByte(0x90);
2618         }
2619         switch (i) {
2620             case 4:
2621                 emitByte(0x66);
2622                 emitByte(0x66);
2623                 emitByte(0x66);
2624                 emitByte(0x90);
2625                 break;
2626             case 3:
2627                 emitByte(0x66);
2628                 emitByte(0x66);
2629                 emitByte(0x90);
2630                 break;
2631             case 2:
2632                 emitByte(0x66);
2633                 emitByte(0x90);
2634                 break;
2635             case 1:
2636                 emitByte(0x90);
2637                 break;
2638             default:
2639                 assert i == 0;
2640         }
2641     }
2642 
2643     private void amdNops(int count) {
2644         int i = count;
2645         //
2646         // Using multi-bytes nops &quot;0x0F 0x1F [Address]&quot; for AMD.
2647         // 1: 0x90
2648         // 2: 0x66 0x90
2649         // 3: 0x66 0x66 0x90 (don&#39;t use &quot;0x0F 0x1F 0x00&quot; - need patching safe padding)
2650         // 4: 0x0F 0x1F 0x40 0x00
2651         // 5: 0x0F 0x1F 0x44 0x00 0x00
2652         // 6: 0x66 0x0F 0x1F 0x44 0x00 0x00
2653         // 7: 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00
2654         // 8: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
2655         // 9: 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
2656         // 10: 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
2657         // 11: 0x66 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
2658 
2659         // The rest coding is AMD specific - use consecutive Address nops
2660 
2661         // 12: 0x66 0x0F 0x1F 0x44 0x00 0x00 0x66 0x0F 0x1F 0x44 0x00 0x00
2662         // 13: 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00 0x66 0x0F 0x1F 0x44 0x00 0x00
2663         // 14: 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00
2664         // 15: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00
2665         // 16: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
2666         // Size prefixes (0x66) are added for larger sizes
2667 
2668         while (i &gt;= 22) {
2669             i -= 11;
2670             emitByte(0x66); // size prefix
2671             emitByte(0x66); // size prefix
2672             emitByte(0x66); // size prefix
2673             addrNop8();
2674         }
2675         // Generate first nop for size between 21-12
2676         switch (i) {
2677             case 21:
2678                 i -= 11;
2679                 emitByte(0x66); // size prefix
2680                 emitByte(0x66); // size prefix
2681                 emitByte(0x66); // size prefix
2682                 addrNop8();
2683                 break;
2684             case 20:
2685             case 19:
2686                 i -= 10;
2687                 emitByte(0x66); // size prefix
2688                 emitByte(0x66); // size prefix
2689                 addrNop8();
2690                 break;
2691             case 18:
2692             case 17:
2693                 i -= 9;
2694                 emitByte(0x66); // size prefix
2695                 addrNop8();
2696                 break;
2697             case 16:
2698             case 15:
2699                 i -= 8;
2700                 addrNop8();
2701                 break;
2702             case 14:
2703             case 13:
2704                 i -= 7;
2705                 addrNop7();
2706                 break;
2707             case 12:
2708                 i -= 6;
2709                 emitByte(0x66); // size prefix
2710                 addrNop5();
2711                 break;
2712             default:
2713                 assert i &lt; 12;
2714         }
2715 
2716         // Generate second nop for size between 11-1
2717         switch (i) {
2718             case 11:
2719                 emitByte(0x66); // size prefix
2720                 emitByte(0x66); // size prefix
2721                 emitByte(0x66); // size prefix
2722                 addrNop8();
2723                 break;
2724             case 10:
2725                 emitByte(0x66); // size prefix
2726                 emitByte(0x66); // size prefix
2727                 addrNop8();
2728                 break;
2729             case 9:
2730                 emitByte(0x66); // size prefix
2731                 addrNop8();
2732                 break;
2733             case 8:
2734                 addrNop8();
2735                 break;
2736             case 7:
2737                 addrNop7();
2738                 break;
2739             case 6:
2740                 emitByte(0x66); // size prefix
2741                 addrNop5();
2742                 break;
2743             case 5:
2744                 addrNop5();
2745                 break;
2746             case 4:
2747                 addrNop4();
2748                 break;
2749             case 3:
2750                 // Don&#39;t use &quot;0x0F 0x1F 0x00&quot; - need patching safe padding
2751                 emitByte(0x66); // size prefix
2752                 emitByte(0x66); // size prefix
2753                 emitByte(0x90); // nop
2754                 break;
2755             case 2:
2756                 emitByte(0x66); // size prefix
2757                 emitByte(0x90); // nop
2758                 break;
2759             case 1:
2760                 emitByte(0x90); // nop
2761                 break;
2762             default:
2763                 assert i == 0;
2764         }
2765     }
2766 
2767     @SuppressWarnings(&quot;fallthrough&quot;)
2768     private void intelNops(int count) {
2769         //
2770         // Using multi-bytes nops &quot;0x0F 0x1F [address]&quot; for Intel
2771         // 1: 0x90
2772         // 2: 0x66 0x90
2773         // 3: 0x66 0x66 0x90 (don&#39;t use &quot;0x0F 0x1F 0x00&quot; - need patching safe padding)
2774         // 4: 0x0F 0x1F 0x40 0x00
2775         // 5: 0x0F 0x1F 0x44 0x00 0x00
2776         // 6: 0x66 0x0F 0x1F 0x44 0x00 0x00
2777         // 7: 0x0F 0x1F 0x80 0x00 0x00 0x00 0x00
2778         // 8: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
2779         // 9: 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
2780         // 10: 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
2781         // 11: 0x66 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00
2782 
2783         // The rest coding is Intel specific - don&#39;t use consecutive address nops
2784 
2785         // 12: 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x66 0x66 0x66 0x90
2786         // 13: 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x66 0x66 0x66 0x90
2787         // 14: 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x66 0x66 0x66 0x90
2788         // 15: 0x66 0x66 0x66 0x0F 0x1F 0x84 0x00 0x00 0x00 0x00 0x00 0x66 0x66 0x66 0x90
2789 
2790         int i = count;
2791         while (i &gt;= 15) {
2792             // For Intel don&#39;t generate consecutive addess nops (mix with regular nops)
2793             i -= 15;
2794             emitByte(0x66);   // size prefix
2795             emitByte(0x66);   // size prefix
2796             emitByte(0x66);   // size prefix
2797             addrNop8();
2798             emitByte(0x66);   // size prefix
2799             emitByte(0x66);   // size prefix
2800             emitByte(0x66);   // size prefix
2801             emitByte(0x90);
2802             // nop
2803         }
2804         switch (i) {
2805             case 14:
2806                 emitByte(0x66); // size prefix
2807                 // fall through
2808             case 13:
2809                 emitByte(0x66); // size prefix
2810                 // fall through
2811             case 12:
2812                 addrNop8();
2813                 emitByte(0x66); // size prefix
2814                 emitByte(0x66); // size prefix
2815                 emitByte(0x66); // size prefix
2816                 emitByte(0x90);
2817                 // nop
2818                 break;
2819             case 11:
2820                 emitByte(0x66); // size prefix
2821                 // fall through
2822             case 10:
2823                 emitByte(0x66); // size prefix
2824                 // fall through
2825             case 9:
2826                 emitByte(0x66); // size prefix
2827                 // fall through
2828             case 8:
2829                 addrNop8();
2830                 break;
2831             case 7:
2832                 addrNop7();
2833                 break;
2834             case 6:
2835                 emitByte(0x66); // size prefix
2836                 // fall through
2837             case 5:
2838                 addrNop5();
2839                 break;
2840             case 4:
2841                 addrNop4();
2842                 break;
2843             case 3:
2844                 // Don&#39;t use &quot;0x0F 0x1F 0x00&quot; - need patching safe padding
2845                 emitByte(0x66); // size prefix
2846                 // fall through
2847             case 2:
2848                 emitByte(0x66); // size prefix
2849                 // fall through
2850             case 1:
2851                 emitByte(0x90);
2852                 // nop
2853                 break;
2854             default:
2855                 assert i == 0;
2856         }
2857     }
2858 
2859     public final void orl(Register dst, Register src) {
2860         OR.rmOp.emit(this, DWORD, dst, src);
2861     }
2862 
2863     public final void orl(Register dst, int imm32) {
2864         OR.getMIOpcode(DWORD, isByte(imm32)).emit(this, DWORD, dst, imm32);
2865     }
2866 
2867     // Insn: VPACKUSWB xmm1, xmm2, xmm3/m128
2868     // -----
2869     // Insn: VPACKUSWB xmm1, xmm1, xmm2
2870 
2871     public final void packuswb(Register dst, Register src) {
2872         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
2873         // Code: VEX.NDS.128.66.0F.WIG 67 /r
2874         simdPrefix(dst, dst, src, PD, P_0F, false);
2875         emitByte(0x67);
2876         emitModRM(dst, src);
2877     }
2878 
2879     public final void pop(Register dst) {
2880         prefix(dst);
2881         emitByte(0x58 + encode(dst));
2882     }
2883 
2884     public void popfq() {
2885         emitByte(0x9D);
2886     }
2887 
2888     public final void ptest(Register dst, Register src) {
2889         assert supports(CPUFeature.SSE4_1);
2890         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
2891         simdPrefix(dst, Register.None, src, PD, P_0F38, false);
2892         emitByte(0x17);
2893         emitModRM(dst, src);
2894     }
2895 
2896     public final void pcmpeqb(Register dst, Register src) {
2897         assert supports(CPUFeature.SSE2);
2898         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
2899         simdPrefix(dst, dst, src, PD, P_0F, false);
2900         emitByte(0x74);
2901         emitModRM(dst, src);
2902     }
2903 
2904     public final void pcmpeqw(Register dst, Register src) {
2905         assert supports(CPUFeature.SSE2);
2906         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
2907         simdPrefix(dst, dst, src, PD, P_0F, false);
2908         emitByte(0x75);
2909         emitModRM(dst, src);
2910     }
2911 
2912     public final void pcmpeqd(Register dst, Register src) {
2913         assert supports(CPUFeature.SSE2);
2914         assert dst.getRegisterCategory().equals(XMM) &amp;&amp; src.getRegisterCategory().equals(XMM);
2915         simdPrefix(dst, dst, src, PD, P_0F, false);
2916         emitByte(0x76);
2917         emitModRM(dst, src);
2918     }
2919 
2920     public final void pcmpestri(Register dst, AMD64Address src, int imm8) {
2921         assert supports(CPUFeature.SSE4_2);
2922         assert inRC(XMM, dst);
2923         simdPrefix(dst, Register.None, src, PD, P_0F3A, false);
2924         emitByte(0x61);
2925         emitOperandHelper(dst, src, 0);
2926         emitByte(imm8);
2927     }
2928 
2929     public final void pcmpestri(Register dst, Register src, int imm8) {
2930         assert supports(CPUFeature.SSE4_2);
2931         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
2932         simdPrefix(dst, Register.None, src, PD, P_0F3A, false);
2933         emitByte(0x61);
2934         emitModRM(dst, src);
2935         emitByte(imm8);
2936     }
2937 
2938     public final void pmovmskb(Register dst, Register src) {
2939         assert supports(CPUFeature.SSE2);
2940         assert inRC(CPU, dst) &amp;&amp; inRC(XMM, src);
2941         simdPrefix(dst, Register.None, src, PD, P_0F, false);
2942         emitByte(0xD7);
2943         emitModRM(dst, src);
2944     }
2945 
2946     private void pmovSZx(Register dst, AMD64Address src, int op) {
2947         assert supports(CPUFeature.SSE4_1);
2948         assert inRC(XMM, dst);
2949         simdPrefix(dst, Register.None, src, PD, P_0F38, false);
2950         emitByte(op);
2951         emitOperandHelper(dst, src, 0);
2952     }
2953 
2954     public final void pmovsxbw(Register dst, AMD64Address src) {
2955         pmovSZx(dst, src, 0x20);
2956     }
2957 
2958     public final void pmovsxbd(Register dst, AMD64Address src) {
2959         pmovSZx(dst, src, 0x21);
2960     }
2961 
2962     public final void pmovsxbq(Register dst, AMD64Address src) {
2963         pmovSZx(dst, src, 0x22);
2964     }
2965 
2966     public final void pmovsxwd(Register dst, AMD64Address src) {
2967         pmovSZx(dst, src, 0x23);
2968     }
2969 
2970     public final void pmovsxwq(Register dst, AMD64Address src) {
2971         pmovSZx(dst, src, 0x24);
2972     }
2973 
2974     public final void pmovsxdq(Register dst, AMD64Address src) {
2975         pmovSZx(dst, src, 0x25);
2976     }
2977 
2978     // Insn: VPMOVZXBW xmm1, xmm2/m64
2979     public final void pmovzxbw(Register dst, AMD64Address src) {
2980         pmovSZx(dst, src, 0x30);
2981     }
2982 
2983     public final void pmovzxbd(Register dst, AMD64Address src) {
2984         pmovSZx(dst, src, 0x31);
2985     }
2986 
2987     public final void pmovzxbq(Register dst, AMD64Address src) {
2988         pmovSZx(dst, src, 0x32);
2989     }
2990 
2991     public final void pmovzxwd(Register dst, AMD64Address src) {
2992         pmovSZx(dst, src, 0x33);
2993     }
2994 
2995     public final void pmovzxwq(Register dst, AMD64Address src) {
2996         pmovSZx(dst, src, 0x34);
2997     }
2998 
2999     public final void pmovzxdq(Register dst, AMD64Address src) {
3000         pmovSZx(dst, src, 0x35);
3001     }
3002 
3003     public final void pmovzxbw(Register dst, Register src) {
3004         assert supports(CPUFeature.SSE4_1);
3005         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3006         simdPrefix(dst, Register.None, src, PD, P_0F38, false);
3007         emitByte(0x30);
3008         emitModRM(dst, src);
3009     }
3010 
3011     public final void push(Register src) {
3012         prefix(src);
3013         emitByte(0x50 + encode(src));
3014     }
3015 
3016     public void pushfq() {
3017         emitByte(0x9c);
3018     }
3019 
3020     public final void paddd(Register dst, Register src) {
3021         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3022         simdPrefix(dst, dst, src, PD, P_0F, false);
3023         emitByte(0xFE);
3024         emitModRM(dst, src);
3025     }
3026 
3027     public final void paddq(Register dst, Register src) {
3028         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3029         simdPrefix(dst, dst, src, PD, P_0F, false);
3030         emitByte(0xD4);
3031         emitModRM(dst, src);
3032     }
3033 
3034     public final void pextrw(Register dst, Register src, int imm8) {
3035         assert inRC(CPU, dst) &amp;&amp; inRC(XMM, src);
3036         simdPrefix(dst, Register.None, src, PD, P_0F, false);
3037         emitByte(0xC5);
3038         emitModRM(dst, src);
3039         emitByte(imm8);
3040     }
3041 
3042     public final void pinsrw(Register dst, Register src, int imm8) {
3043         assert inRC(XMM, dst) &amp;&amp; inRC(CPU, src);
3044         simdPrefix(dst, dst, src, PD, P_0F, false);
3045         emitByte(0xC4);
3046         emitModRM(dst, src);
3047         emitByte(imm8);
3048     }
3049 
3050     public final void por(Register dst, Register src) {
3051         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3052         simdPrefix(dst, dst, src, PD, P_0F, false);
3053         emitByte(0xEB);
3054         emitModRM(dst, src);
3055     }
3056 
3057     public final void pand(Register dst, Register src) {
3058         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3059         simdPrefix(dst, dst, src, PD, P_0F, false);
3060         emitByte(0xDB);
3061         emitModRM(dst, src);
3062     }
3063 
3064     public final void pxor(Register dst, Register src) {
3065         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3066         simdPrefix(dst, dst, src, PD, P_0F, false);
3067         emitByte(0xEF);
3068         emitModRM(dst, src);
3069     }
3070 
3071     public final void pslld(Register dst, int imm8) {
3072         assert isUByte(imm8) : &quot;invalid value&quot;;
3073         assert inRC(XMM, dst);
3074         // XMM6 is for /6 encoding: 66 0F 72 /6 ib
3075         simdPrefix(AMD64.xmm6, dst, dst, PD, P_0F, false);
3076         emitByte(0x72);
3077         emitModRM(6, dst);
3078         emitByte(imm8 &amp; 0xFF);
3079     }
3080 
3081     public final void psllq(Register dst, Register shift) {
3082         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, shift);
3083         simdPrefix(dst, dst, shift, PD, P_0F, false);
3084         emitByte(0xF3);
3085         emitModRM(dst, shift);
3086     }
3087 
3088     public final void psllq(Register dst, int imm8) {
3089         assert isUByte(imm8) : &quot;invalid value&quot;;
3090         assert inRC(XMM, dst);
3091         // XMM6 is for /6 encoding: 66 0F 73 /6 ib
3092         simdPrefix(AMD64.xmm6, dst, dst, PD, P_0F, false);
3093         emitByte(0x73);
3094         emitModRM(6, dst);
3095         emitByte(imm8);
3096     }
3097 
3098     public final void psrad(Register dst, int imm8) {
3099         assert isUByte(imm8) : &quot;invalid value&quot;;
3100         assert inRC(XMM, dst);
3101         // XMM4 is for /4 encoding: 66 0F 72 /4 ib
3102         simdPrefix(AMD64.xmm4, dst, dst, PD, P_0F, false);
3103         emitByte(0x72);
3104         emitModRM(4, dst);
3105         emitByte(imm8);
3106     }
3107 
3108     public final void psrld(Register dst, int imm8) {
3109         assert isUByte(imm8) : &quot;invalid value&quot;;
3110         assert inRC(XMM, dst);
3111         // XMM2 is for /2 encoding: 66 0F 72 /2 ib
3112         simdPrefix(AMD64.xmm2, dst, dst, PD, P_0F, false);
3113         emitByte(0x72);
3114         emitModRM(2, dst);
3115         emitByte(imm8);
3116     }
3117 
3118     public final void psrlq(Register dst, int imm8) {
3119         assert isUByte(imm8) : &quot;invalid value&quot;;
3120         assert inRC(XMM, dst);
3121         // XMM2 is for /2 encoding: 66 0F 73 /2 ib
3122         simdPrefix(AMD64.xmm2, dst, dst, PD, P_0F, false);
3123         emitByte(0x73);
3124         emitModRM(2, dst);
3125         emitByte(imm8);
3126     }
3127 
3128     public final void psrldq(Register dst, int imm8) {
3129         assert isUByte(imm8) : &quot;invalid value&quot;;
3130         assert inRC(XMM, dst);
3131         simdPrefix(AMD64.xmm3, dst, dst, PD, P_0F, false);
3132         emitByte(0x73);
3133         emitModRM(3, dst);
3134         emitByte(imm8);
3135     }
3136 
3137     public final void pshufb(Register dst, Register src) {
3138         assert supports(CPUFeature.SSSE3);
3139         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3140         simdPrefix(dst, dst, src, PD, P_0F38, false);
3141         emitByte(0x00);
3142         emitModRM(dst, src);
3143     }
3144 
3145     public final void pshuflw(Register dst, Register src, int imm8) {
3146         assert supports(CPUFeature.SSE2);
3147         assert isUByte(imm8) : &quot;invalid value&quot;;
3148         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3149         simdPrefix(dst, Register.None, src, SD, P_0F, false);
3150         emitByte(0x70);
3151         emitModRM(dst, src);
3152         emitByte(imm8);
3153     }
3154 
3155     public final void pshufd(Register dst, Register src, int imm8) {
3156         assert isUByte(imm8) : &quot;invalid value&quot;;
3157         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3158         simdPrefix(dst, Register.None, src, PD, P_0F, false);
3159         emitByte(0x70);
3160         emitModRM(dst, src);
3161         emitByte(imm8);
3162     }
3163 
3164     public final void psubd(Register dst, Register src) {
3165         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3166         simdPrefix(dst, dst, src, PD, P_0F, false);
3167         emitByte(0xFA);
3168         emitModRM(dst, src);
3169     }
3170 
3171     public final void punpcklbw(Register dst, Register src) {
3172         assert supports(CPUFeature.SSE2);
3173         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3174         simdPrefix(dst, dst, src, PD, P_0F, false);
3175         emitByte(0x60);
3176         emitModRM(dst, src);
3177     }
3178 
3179     public final void rcpps(Register dst, Register src) {
3180         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3181         simdPrefix(dst, Register.None, src, PS, P_0F, false);
3182         emitByte(0x53);
3183         emitModRM(dst, src);
3184     }
3185 
3186     public final void ret(int imm16) {
3187         if (imm16 == 0) {
3188             testAndAlign(1);
3189             emitByte(0xC3);
3190         } else {
3191             testAndAlign(3);
3192             emitByte(0xC2);
3193             emitShort(imm16);
3194         }
3195     }
3196 
3197     public final void sarl(Register dst, int imm8) {
3198         prefix(dst);
3199         assert isShiftCount(imm8 &gt;&gt; 1) : &quot;illegal shift count&quot;;
3200         if (imm8 == 1) {
3201             emitByte(0xD1);
3202             emitModRM(7, dst);
3203         } else {
3204             emitByte(0xC1);
3205             emitModRM(7, dst);
3206             emitByte(imm8);
3207         }
3208     }
3209 
3210     public final void shll(Register dst, int imm8) {
3211         assert isShiftCount(imm8 &gt;&gt; 1) : &quot;illegal shift count&quot;;
3212         prefix(dst);
3213         if (imm8 == 1) {
3214             emitByte(0xD1);
3215             emitModRM(4, dst);
3216         } else {
3217             emitByte(0xC1);
3218             emitModRM(4, dst);
3219             emitByte(imm8);
3220         }
3221     }
3222 
3223     public final void shll(Register dst) {
3224         // Multiply dst by 2, CL times.
3225         prefix(dst);
3226         emitByte(0xD3);
3227         emitModRM(4, dst);
3228     }
3229 
3230     // Insn: SHLX r32a, r/m32, r32b
3231 
3232     public final void shlxl(Register dst, Register src1, Register src2) {
3233         VexGeneralPurposeRMVOp.SHLX.emit(this, AVXSize.DWORD, dst, src1, src2);
3234     }
3235 
3236     public final void shrl(Register dst, int imm8) {
3237         assert isShiftCount(imm8 &gt;&gt; 1) : &quot;illegal shift count&quot;;
3238         prefix(dst);
3239         emitByte(0xC1);
3240         emitModRM(5, dst);
3241         emitByte(imm8);
3242     }
3243 
3244     public final void shrl(Register dst) {
3245         // Unsigned divide dst by 2, CL times.
3246         prefix(dst);
3247         emitByte(0xD3);
3248         emitModRM(5, dst);
3249     }
3250 
3251     public final void subl(AMD64Address dst, int imm32) {
3252         SUB.getMIOpcode(DWORD, isByte(imm32)).emit(this, DWORD, dst, imm32);
3253     }
3254 
3255     public final void subl(Register dst, int imm32) {
3256         SUB.getMIOpcode(DWORD, isByte(imm32)).emit(this, DWORD, dst, imm32);
3257     }
3258 
3259     public final void subl(Register dst, Register src) {
3260         SUB.rmOp.emit(this, DWORD, dst, src);
3261     }
3262 
3263     public final void subpd(Register dst, Register src) {
3264         SSEOp.SUB.emit(this, PD, dst, src);
3265     }
3266 
3267     public final void subsd(Register dst, Register src) {
3268         SSEOp.SUB.emit(this, SD, dst, src);
3269     }
3270 
3271     public final void subsd(Register dst, AMD64Address src) {
3272         SSEOp.SUB.emit(this, SD, dst, src);
3273     }
3274 
3275     public final void testl(Register dst, int imm32) {
3276         // not using emitArith because test
3277         // doesn&#39;t support sign-extension of
3278         // 8bit operands
3279         if (dst.encoding == 0) {
3280             emitByte(0xA9);
3281             emitInt(imm32);
3282         } else {
3283             AMD64MIOp.TEST.emit(this, DWORD, dst, imm32);
3284         }
3285     }
3286 
3287     public final void testl(Register dst, Register src) {
3288         AMD64RMOp.TEST.emit(this, DWORD, dst, src);
3289     }
3290 
3291     public final void testl(Register dst, AMD64Address src) {
3292         AMD64RMOp.TEST.emit(this, DWORD, dst, src);
3293     }
3294 
3295     public final void unpckhpd(Register dst, Register src) {
3296         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3297         simdPrefix(dst, dst, src, PD, P_0F, false);
3298         emitByte(0x15);
3299         emitModRM(dst, src);
3300     }
3301 
3302     public final void unpcklpd(Register dst, Register src) {
3303         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3304         simdPrefix(dst, dst, src, PD, P_0F, false);
3305         emitByte(0x14);
3306         emitModRM(dst, src);
3307     }
3308 
3309     public final void xorl(Register dst, Register src) {
3310         XOR.rmOp.emit(this, DWORD, dst, src);
3311     }
3312 
3313     public final void xorq(Register dst, Register src) {
3314         XOR.rmOp.emit(this, QWORD, dst, src);
3315     }
3316 
3317     public final void xorpd(Register dst, Register src) {
3318         SSEOp.XOR.emit(this, PD, dst, src);
3319     }
3320 
3321     public final void xorps(Register dst, Register src) {
3322         SSEOp.XOR.emit(this, PS, dst, src);
3323     }
3324 
3325     public final void decl(Register dst) {
3326         // Use two-byte form (one-byte form is a REX prefix in 64-bit mode)
3327         DEC.emit(this, DWORD, dst);
3328     }
3329 
3330     public final void incl(Register dst) {
3331         // Use two-byte form (one-byte from is a REX prefix in 64-bit mode)
3332         INC.emit(this, DWORD, dst);
3333     }
3334 
3335     public final void addq(Register dst, int imm32) {
3336         ADD.getMIOpcode(QWORD, isByte(imm32)).emit(this, QWORD, dst, imm32);
3337     }
3338 
3339     public final void addq(AMD64Address dst, int imm32) {
3340         ADD.getMIOpcode(QWORD, isByte(imm32)).emit(this, QWORD, dst, imm32);
3341     }
3342 
3343     public final void addq(Register dst, Register src) {
3344         ADD.rmOp.emit(this, QWORD, dst, src);
3345     }
3346 
3347     public final void addq(AMD64Address dst, Register src) {
3348         ADD.mrOp.emit(this, QWORD, dst, src);
3349     }
3350 
3351     public final void andq(Register dst, int imm32) {
3352         AND.getMIOpcode(QWORD, isByte(imm32)).emit(this, QWORD, dst, imm32);
3353     }
3354 
3355     public final void bsrq(Register dst, Register src) {
3356         prefixq(dst, src);
3357         emitByte(0x0F);
3358         emitByte(0xBD);
3359         emitModRM(dst, src);
3360     }
3361 
3362     public final void bswapq(Register reg) {
3363         prefixq(reg);
3364         emitByte(0x0F);
3365         emitByte(0xC8 + encode(reg));
3366     }
3367 
3368     public final void cdqq() {
3369         rexw();
3370         emitByte(0x99);
3371     }
3372 
3373     public final void repStosb() {
3374         emitByte(0xf3);
3375         rexw();
3376         emitByte(0xaa);
3377     }
3378 
3379     public final void repStosq() {
3380         emitByte(0xf3);
3381         rexw();
3382         emitByte(0xab);
3383     }
3384 
3385     public final void cmovq(ConditionFlag cc, Register dst, Register src) {
3386         prefixq(dst, src);
3387         emitByte(0x0F);
3388         emitByte(0x40 | cc.getValue());
3389         emitModRM(dst, src);
3390     }
3391 
3392     public final void setb(ConditionFlag cc, Register dst) {
3393         prefix(dst, true);
3394         emitByte(0x0F);
3395         emitByte(0x90 | cc.getValue());
3396         emitModRM(0, dst);
3397     }
3398 
3399     public final void cmovq(ConditionFlag cc, Register dst, AMD64Address src) {
3400         prefixq(src, dst);
3401         emitByte(0x0F);
3402         emitByte(0x40 | cc.getValue());
3403         emitOperandHelper(dst, src, 0);
3404     }
3405 
3406     public final void cmpq(Register dst, int imm32) {
3407         CMP.getMIOpcode(QWORD, isByte(imm32)).emit(this, QWORD, dst, imm32);
3408     }
3409 
3410     public final void cmpq(Register dst, Register src) {
3411         CMP.rmOp.emit(this, QWORD, dst, src);
3412     }
3413 
3414     public final void cmpq(Register dst, AMD64Address src) {
3415         CMP.rmOp.emit(this, QWORD, dst, src);
3416     }
3417 
3418     public final void cmpxchgq(Register reg, AMD64Address adr) {
3419         prefixq(adr, reg);
3420         emitByte(0x0F);
3421         emitByte(0xB1);
3422         emitOperandHelper(reg, adr, 0);
3423     }
3424 
3425     public final void cvtdq2pd(Register dst, Register src) {
3426         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3427         simdPrefix(dst, Register.None, src, SS, P_0F, false);
3428         emitByte(0xE6);
3429         emitModRM(dst, src);
3430     }
3431 
3432     public final void cvtsi2sdq(Register dst, Register src) {
3433         SSEOp.CVTSI2SD.emit(this, QWORD, dst, src);
3434     }
3435 
3436     public final void cvttsd2siq(Register dst, Register src) {
3437         SSEOp.CVTTSD2SI.emit(this, QWORD, dst, src);
3438     }
3439 
3440     public final void cvttpd2dq(Register dst, Register src) {
3441         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3442         simdPrefix(dst, Register.None, src, PD, P_0F, false);
3443         emitByte(0xE6);
3444         emitModRM(dst, src);
3445     }
3446 
3447     public final void decq(Register dst) {
3448         // Use two-byte form (one-byte form is a REX prefix in 64-bit mode)
3449         DEC.emit(this, QWORD, dst);
3450     }
3451 
3452     public final void decq(AMD64Address dst) {
3453         DEC.emit(this, QWORD, dst);
3454     }
3455 
3456     public final void imulq(Register dst, Register src) {
3457         prefixq(dst, src);
3458         emitByte(0x0F);
3459         emitByte(0xAF);
3460         emitModRM(dst, src);
3461     }
3462 
3463     public final void incq(Register dst) {
3464         // Don&#39;t use it directly. Use Macroincrementq() instead.
3465         // Use two-byte form (one-byte from is a REX prefix in 64-bit mode)
3466         INC.emit(this, QWORD, dst);
3467     }
3468 
3469     public final void incq(AMD64Address dst) {
3470         INC.emit(this, QWORD, dst);
3471     }
3472 
3473     public final void movq(Register dst, long imm64) {
3474         movq(dst, imm64, false);
3475     }
3476 
3477     public final void movq(Register dst, long imm64, boolean annotateImm) {
3478         int insnPos = position();
3479         prefixq(dst);
3480         emitByte(0xB8 + encode(dst));
3481         int immPos = position();
3482         emitLong(imm64);
3483         int nextInsnPos = position();
3484         if (annotateImm &amp;&amp; codePatchingAnnotationConsumer != null) {
3485             codePatchingAnnotationConsumer.accept(new OperandDataAnnotation(insnPos, immPos, nextInsnPos - immPos, nextInsnPos));
3486         }
3487     }
3488 
3489     public final void movslq(Register dst, int imm32) {
3490         prefixq(dst);
3491         emitByte(0xC7);
3492         emitModRM(0, dst);
3493         emitInt(imm32);
3494     }
3495 
3496     public final void movdq(Register dst, AMD64Address src) {
3497         AMD64RMOp.MOVQ.emit(this, QWORD, dst, src);
3498     }
3499 
3500     public final void movdq(AMD64Address dst, Register src) {
3501         AMD64MROp.MOVQ.emit(this, QWORD, dst, src);
3502     }
3503 
3504     public final void movdq(Register dst, Register src) {
3505         if (inRC(XMM, dst) &amp;&amp; inRC(CPU, src)) {
3506             AMD64RMOp.MOVQ.emit(this, QWORD, dst, src);
3507         } else if (inRC(XMM, src) &amp;&amp; inRC(CPU, dst)) {
3508             AMD64MROp.MOVQ.emit(this, QWORD, dst, src);
3509         } else {
3510             throw new InternalError(&quot;should not reach here&quot;);
3511         }
3512     }
3513 
3514     public final void movdl(Register dst, Register src) {
3515         if (inRC(XMM, dst) &amp;&amp; inRC(CPU, src)) {
3516             AMD64RMOp.MOVD.emit(this, DWORD, dst, src);
3517         } else if (inRC(XMM, src) &amp;&amp; inRC(CPU, dst)) {
3518             AMD64MROp.MOVD.emit(this, DWORD, dst, src);
3519         } else {
3520             throw new InternalError(&quot;should not reach here&quot;);
3521         }
3522     }
3523 
3524     public final void movdl(Register dst, AMD64Address src) {
3525         AMD64RMOp.MOVD.emit(this, DWORD, dst, src);
3526     }
3527 
3528     public final void movddup(Register dst, Register src) {
3529         assert supports(CPUFeature.SSE3);
3530         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3531         simdPrefix(dst, Register.None, src, SD, P_0F, false);
3532         emitByte(0x12);
3533         emitModRM(dst, src);
3534     }
3535 
3536     public final void movdqu(Register dst, AMD64Address src) {
3537         assert inRC(XMM, dst);
3538         simdPrefix(dst, Register.None, src, SS, P_0F, false);
3539         emitByte(0x6F);
3540         emitOperandHelper(dst, src, 0);
3541     }
3542 
3543     public final void movdqu(Register dst, Register src) {
3544         assert inRC(XMM, dst) &amp;&amp; inRC(XMM, src);
3545         simdPrefix(dst, Register.None, src, SS, P_0F, false);
3546         emitByte(0x6F);
3547         emitModRM(dst, src);
3548     }
3549 
3550     // Insn: VMOVDQU xmm2/m128, xmm1
3551 
3552     public final void movdqu(AMD64Address dst, Register src) {
3553         assert inRC(XMM, src);
3554         // Code: VEX.128.F3.0F.WIG 7F /r
3555         simdPrefix(src, Register.None, dst, SS, P_0F, false);
3556         emitByte(0x7F);
3557         emitOperandHelper(src, dst, 0);
3558     }
3559 
3560     public final void movslq(AMD64Address dst, int imm32) {
3561         prefixq(dst);
3562         emitByte(0xC7);
3563         emitOperandHelper(0, dst, 4);
3564         emitInt(imm32);
3565     }
3566 
3567     public final void movslq(Register dst, AMD64Address src) {
3568         prefixq(src, dst);
3569         emitByte(0x63);
3570         emitOperandHelper(dst, src, 0);
3571     }
3572 
3573     public final void movslq(Register dst, Register src) {
3574         prefixq(dst, src);
3575         emitByte(0x63);
3576         emitModRM(dst, src);
3577     }
3578 
3579     public final void negq(Register dst) {
3580         prefixq(dst);
3581         emitByte(0xF7);
3582         emitModRM(3, dst);
3583     }
3584 
3585     public final void orq(Register dst, Register src) {
3586         OR.rmOp.emit(this, QWORD, dst, src);
3587     }
3588 
3589     public final void shlq(Register dst, int imm8) {
3590         assert isShiftCount(imm8 &gt;&gt; 1) : &quot;illegal shift count&quot;;
3591         prefixq(dst);
3592         if (imm8 == 1) {
3593             emitByte(0xD1);
3594             emitModRM(4, dst);
3595         } else {
3596             emitByte(0xC1);
3597             emitModRM(4, dst);
3598             emitByte(imm8);
3599         }
3600     }
3601 
3602     public final void shlq(Register dst) {
3603         // Multiply dst by 2, CL times.
3604         prefixq(dst);
3605         emitByte(0xD3);
3606         emitModRM(4, dst);
3607     }
3608 
3609     public final void shrq(Register dst, int imm8) {
3610         assert isShiftCount(imm8 &gt;&gt; 1) : &quot;illegal shift count&quot;;
3611         prefixq(dst);
3612         if (imm8 == 1) {
3613             emitByte(0xD1);
3614             emitModRM(5, dst);
3615         } else {
3616             emitByte(0xC1);
3617             emitModRM(5, dst);
3618             emitByte(imm8);
3619         }
3620     }
3621 
3622     public final void shrq(Register dst) {
3623         prefixq(dst);
3624         emitByte(0xD3);
3625         // Unsigned divide dst by 2, CL times.
3626         emitModRM(5, dst);
3627     }
3628 
3629     public final void sarq(Register dst, int imm8) {
3630         assert isShiftCount(imm8 &gt;&gt; 1) : &quot;illegal shift count&quot;;
3631         prefixq(dst);
3632         if (imm8 == 1) {
3633             emitByte(0xD1);
3634             emitModRM(7, dst);
3635         } else {
3636             emitByte(0xC1);
3637             emitModRM(7, dst);
3638             emitByte(imm8);
3639         }
3640     }
3641 
3642     public final void sbbq(Register dst, Register src) {
3643         SBB.rmOp.emit(this, QWORD, dst, src);
3644     }
3645 
3646     public final void subq(Register dst, int imm32) {
3647         SUB.getMIOpcode(QWORD, isByte(imm32)).emit(this, QWORD, dst, imm32);
3648     }
3649 
3650     public final void subq(AMD64Address dst, int imm32) {
3651         SUB.getMIOpcode(QWORD, isByte(imm32)).emit(this, QWORD, dst, imm32);
3652     }
3653 
3654     public final void subqWide(Register dst, int imm32) {
3655         // don&#39;t use the sign-extending version, forcing a 32-bit immediate
3656         SUB.getMIOpcode(QWORD, false).emit(this, QWORD, dst, imm32);
3657     }
3658 
3659     public final void subq(Register dst, Register src) {
3660         SUB.rmOp.emit(this, QWORD, dst, src);
3661     }
3662 
3663     public final void testq(Register dst, Register src) {
3664         AMD64RMOp.TEST.emit(this, QWORD, dst, src);
3665     }
3666 
3667     public final void btrq(Register src, int imm8) {
3668         prefixq(src);
3669         emitByte(0x0F);
3670         emitByte(0xBA);
3671         emitModRM(6, src);
3672         emitByte(imm8);
3673     }
3674 
3675     public final void xaddb(AMD64Address dst, Register src) {
3676         prefixb(dst, src);
3677         emitByte(0x0F);
3678         emitByte(0xC0);
3679         emitOperandHelper(src, dst, 0);
3680     }
3681 
3682     public final void xaddw(AMD64Address dst, Register src) {
3683         emitByte(0x66); // Switch to 16-bit mode.
3684         prefix(dst, src);
3685         emitByte(0x0F);
3686         emitByte(0xC1);
3687         emitOperandHelper(src, dst, 0);
3688     }
3689 
3690     public final void xaddl(AMD64Address dst, Register src) {
3691         prefix(dst, src);
3692         emitByte(0x0F);
3693         emitByte(0xC1);
3694         emitOperandHelper(src, dst, 0);
3695     }
3696 
3697     public final void xaddq(AMD64Address dst, Register src) {
3698         prefixq(dst, src);
3699         emitByte(0x0F);
3700         emitByte(0xC1);
3701         emitOperandHelper(src, dst, 0);
3702     }
3703 
3704     public final void xchgb(Register dst, AMD64Address src) {
3705         prefixb(src, dst);
3706         emitByte(0x86);
3707         emitOperandHelper(dst, src, 0);
3708     }
3709 
3710     public final void xchgw(Register dst, AMD64Address src) {
3711         emitByte(0x66);
3712         prefix(src, dst);
3713         emitByte(0x87);
3714         emitOperandHelper(dst, src, 0);
3715     }
3716 
3717     public final void xchgl(Register dst, AMD64Address src) {
3718         prefix(src, dst);
3719         emitByte(0x87);
3720         emitOperandHelper(dst, src, 0);
3721     }
3722 
3723     public final void xchgq(Register dst, AMD64Address src) {
3724         prefixq(src, dst);
3725         emitByte(0x87);
3726         emitOperandHelper(dst, src, 0);
3727     }
3728 
3729     public final void membar(int barriers) {
3730         if (target.isMP) {
3731             // We only have to handle StoreLoad
3732             if ((barriers &amp; STORE_LOAD) != 0) {
3733                 // All usable chips support &quot;locked&quot; instructions which suffice
3734                 // as barriers, and are much faster than the alternative of
3735                 // using cpuid instruction. We use here a locked add [rsp],0.
3736                 // This is conveniently otherwise a no-op except for blowing
3737                 // flags.
3738                 // Any change to this code may need to revisit other places in
3739                 // the code where this idiom is used, in particular the
3740                 // orderAccess code.
3741                 lock();
3742                 addl(new AMD64Address(AMD64.rsp, 0), 0); // Assert the lock# signal here
3743             }
3744         }
3745     }
3746 
3747     @Override
3748     protected final void patchJumpTarget(int branch, int branchTarget) {
3749         int op = getByte(branch);
3750         assert op == 0xE8 // call
3751                         || op == 0x00 // jump table entry
3752                         || op == 0xE9 // jmp
3753                         || op == 0xEB // short jmp
3754                         || (op &amp; 0xF0) == 0x70 // short jcc
3755                         || op == 0x0F &amp;&amp; (getByte(branch + 1) &amp; 0xF0) == 0x80 // jcc
3756         : &quot;Invalid opcode at patch point branch=&quot; + branch + &quot;, branchTarget=&quot; + branchTarget + &quot;, op=&quot; + op;
3757 
3758         if (op == 0x00) {
3759             int offsetToJumpTableBase = getShort(branch + 1);
3760             int jumpTableBase = branch - offsetToJumpTableBase;
3761             int imm32 = branchTarget - jumpTableBase;
3762             emitInt(imm32, branch);
3763         } else if (op == 0xEB || (op &amp; 0xF0) == 0x70) {
3764 
3765             // short offset operators (jmp and jcc)
3766             final int imm8 = branchTarget - (branch + 2);
3767             /*
3768              * Since a wrongly patched short branch can potentially lead to working but really bad
3769              * behaving code we should always fail with an exception instead of having an assert.
3770              */
3771             GraalError.guarantee(isByte(imm8), &quot;Displacement too large to be encoded as a byte: %d&quot;, imm8);
3772             emitByte(imm8, branch + 1);
3773 
3774         } else {
3775 
3776             int off = 1;
3777             if (op == 0x0F) {
3778                 off = 2;
3779             }
3780 
3781             int imm32 = branchTarget - (branch + 4 + off);
3782             emitInt(imm32, branch + off);
3783         }
3784     }
3785 
3786     public void nullCheck(AMD64Address address) {
3787         testl(AMD64.rax, address);
3788     }
3789 
3790     @Override
3791     public void align(int modulus) {
3792         if (position() % modulus != 0) {
3793             nop(modulus - (position() % modulus));
3794         }
3795     }
3796 
3797     /**
3798      * Emits a direct call instruction. Note that the actual call target is not specified, because
3799      * all calls need patching anyway. Therefore, 0 is emitted as the call target, and the user is
3800      * responsible to add the call address to the appropriate patching tables.
3801      */
3802     public final void call() {
3803         annotatePatchingImmediate(1, 4);
3804         emitByte(0xE8);
3805         emitInt(0);
3806     }
3807 
3808     public final void call(Register src) {
3809         prefix(src);
3810         emitByte(0xFF);
3811         emitModRM(2, src);
3812     }
3813 
3814     public final void int3() {
3815         emitByte(0xCC);
3816     }
3817 
3818     public final void pause() {
3819         emitByte(0xF3);
3820         emitByte(0x90);
3821     }
3822 
3823     private void emitx87(int b1, int b2, int i) {
3824         assert 0 &lt;= i &amp;&amp; i &lt; 8 : &quot;illegal stack offset&quot;;
3825         emitByte(b1);
3826         emitByte(b2 + i);
3827     }
3828 
3829     public final void fldd(AMD64Address src) {
3830         emitByte(0xDD);
3831         emitOperandHelper(0, src, 0);
3832     }
3833 
3834     public final void flds(AMD64Address src) {
3835         emitByte(0xD9);
3836         emitOperandHelper(0, src, 0);
3837     }
3838 
3839     public final void fldln2() {
3840         emitByte(0xD9);
3841         emitByte(0xED);
3842     }
3843 
3844     public final void fldlg2() {
3845         emitByte(0xD9);
3846         emitByte(0xEC);
3847     }
3848 
3849     public final void fyl2x() {
3850         emitByte(0xD9);
3851         emitByte(0xF1);
3852     }
3853 
3854     public final void fstps(AMD64Address src) {
3855         emitByte(0xD9);
3856         emitOperandHelper(3, src, 0);
3857     }
3858 
3859     public final void fstpd(AMD64Address src) {
3860         emitByte(0xDD);
3861         emitOperandHelper(3, src, 0);
3862     }
3863 
3864     private void emitFPUArith(int b1, int b2, int i) {
3865         assert 0 &lt;= i &amp;&amp; i &lt; 8 : &quot;illegal FPU register: &quot; + i;
3866         emitByte(b1);
3867         emitByte(b2 + i);
3868     }
3869 
3870     public void ffree(int i) {
3871         emitFPUArith(0xDD, 0xC0, i);
3872     }
3873 
3874     public void fincstp() {
3875         emitByte(0xD9);
3876         emitByte(0xF7);
3877     }
3878 
3879     public void fxch(int i) {
3880         emitFPUArith(0xD9, 0xC8, i);
3881     }
3882 
3883     public void fnstswAX() {
3884         emitByte(0xDF);
3885         emitByte(0xE0);
3886     }
3887 
3888     public void fwait() {
3889         emitByte(0x9B);
3890     }
3891 
3892     public void fprem() {
3893         emitByte(0xD9);
3894         emitByte(0xF8);
3895     }
3896 
3897     public final void fsin() {
3898         emitByte(0xD9);
3899         emitByte(0xFE);
3900     }
3901 
3902     public final void fcos() {
3903         emitByte(0xD9);
3904         emitByte(0xFF);
3905     }
3906 
3907     public final void fptan() {
3908         emitByte(0xD9);
3909         emitByte(0xF2);
3910     }
3911 
3912     public final void fstp(int i) {
3913         emitx87(0xDD, 0xD8, i);
3914     }
3915 
3916     @Override
3917     public AMD64Address makeAddress(Register base, int displacement) {
3918         return new AMD64Address(base, displacement);
3919     }
3920 
3921     @Override
3922     public AMD64Address getPlaceholder(int instructionStartPosition) {
3923         return new AMD64Address(AMD64.rip, Register.None, Scale.Times1, 0, instructionStartPosition);
3924     }
3925 
3926     private void prefetchPrefix(AMD64Address src) {
3927         prefix(src);
3928         emitByte(0x0F);
3929     }
3930 
3931     public void prefetchnta(AMD64Address src) {
3932         prefetchPrefix(src);
3933         emitByte(0x18);
3934         emitOperandHelper(0, src, 0);
3935     }
3936 
3937     void prefetchr(AMD64Address src) {
3938         assert supports(CPUFeature.AMD_3DNOW_PREFETCH);
3939         prefetchPrefix(src);
3940         emitByte(0x0D);
3941         emitOperandHelper(0, src, 0);
3942     }
3943 
3944     public void prefetcht0(AMD64Address src) {
3945         assert supports(CPUFeature.SSE);
3946         prefetchPrefix(src);
3947         emitByte(0x18);
3948         emitOperandHelper(1, src, 0);
3949     }
3950 
3951     public void prefetcht1(AMD64Address src) {
3952         assert supports(CPUFeature.SSE);
3953         prefetchPrefix(src);
3954         emitByte(0x18);
3955         emitOperandHelper(2, src, 0);
3956     }
3957 
3958     public void prefetcht2(AMD64Address src) {
3959         assert supports(CPUFeature.SSE);
3960         prefix(src);
3961         emitByte(0x0f);
3962         emitByte(0x18);
3963         emitOperandHelper(3, src, 0);
3964     }
3965 
3966     public void prefetchw(AMD64Address src) {
3967         assert supports(CPUFeature.AMD_3DNOW_PREFETCH);
3968         prefix(src);
3969         emitByte(0x0f);
3970         emitByte(0x0D);
3971         emitOperandHelper(1, src, 0);
3972     }
3973 
3974     public void rdtsc() {
3975         emitByte(0x0F);
3976         emitByte(0x31);
3977     }
3978 
3979     /**
3980      * Emits an instruction which is considered to be illegal. This is used if we deliberately want
3981      * to crash the program (debugging etc.).
3982      */
3983     public void illegal() {
3984         emitByte(0x0f);
3985         emitByte(0x0b);
3986     }
3987 
3988     public void lfence() {
3989         emitByte(0x0f);
3990         emitByte(0xae);
3991         emitByte(0xe8);
3992     }
3993 
3994     public final void vptest(Register dst, Register src) {
3995         VexRMOp.VPTEST.emit(this, AVXSize.YMM, dst, src);
3996     }
3997 
3998     public final void vpxor(Register dst, Register nds, Register src) {
3999         VexRVMOp.VPXOR.emit(this, AVXSize.YMM, dst, nds, src);
4000     }
4001 
4002     public final void vpxor(Register dst, Register nds, AMD64Address src) {
4003         VexRVMOp.VPXOR.emit(this, AVXSize.YMM, dst, nds, src);
4004     }
4005 
4006     public final void vmovdqu(Register dst, AMD64Address src) {
4007         VexMoveOp.VMOVDQU32.emit(this, AVXSize.YMM, dst, src);
4008     }
4009 
4010     public final void vmovdqu(AMD64Address dst, Register src) {
4011         assert inRC(XMM, src);
4012         VexMoveOp.VMOVDQU32.emit(this, AVXSize.YMM, dst, src);
4013     }
4014 
4015     public final void vpmovzxbw(Register dst, AMD64Address src) {
4016         assert supports(CPUFeature.AVX2);
4017         VexRMOp.VPMOVZXBW.emit(this, AVXSize.YMM, dst, src);
4018     }
4019 
4020     public final void vzeroupper() {
4021         emitVEX(L128, P_, M_0F, W0, 0, 0, true);
4022         emitByte(0x77);
4023     }
4024 
4025     // Insn: KORTESTD k1, k2
4026 
4027     // This instruction produces ZF or CF flags
4028     public final void kortestd(Register src1, Register src2) {
4029         assert supports(CPUFeature.AVX512BW);
4030         assert inRC(MASK, src1) &amp;&amp; inRC(MASK, src2);
4031         // Code: VEX.L0.66.0F.W1 98 /r
4032         vexPrefix(src1, Register.None, src2, AVXSize.XMM, P_66, M_0F, W1, W1, true);
4033         emitByte(0x98);
4034         emitModRM(src1, src2);
4035     }
4036 
4037     // Insn: KORTESTQ k1, k2
4038 
4039     // This instruction produces ZF or CF flags
4040     public final void kortestq(Register src1, Register src2) {
4041         assert supports(CPUFeature.AVX512BW);
4042         assert inRC(MASK, src1) &amp;&amp; inRC(MASK, src2);
4043         // Code: VEX.L0.0F.W1 98 /r
4044         vexPrefix(src1, Register.None, src2, AVXSize.XMM, P_, M_0F, W1, W1, true);
4045         emitByte(0x98);
4046         emitModRM(src1, src2);
4047     }
4048 
4049     public final void kmovd(Register dst, Register src) {
4050         assert supports(CPUFeature.AVX512BW);
4051         assert inRC(MASK, dst) || inRC(CPU, dst);
4052         assert inRC(MASK, src) || inRC(CPU, src);
4053         assert !(inRC(CPU, dst) &amp;&amp; inRC(CPU, src));
4054 
4055         if (inRC(MASK, dst)) {
4056             if (inRC(MASK, src)) {
4057                 // kmovd(KRegister dst, KRegister src):
4058                 // Insn: KMOVD k1, k2/m32
4059                 // Code: VEX.L0.66.0F.W1 90 /r
4060                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_66, M_0F, W1, W1, true);
4061                 emitByte(0x90);
4062                 emitModRM(dst, src);
4063             } else {
4064                 // kmovd(KRegister dst, Register src)
4065                 // Insn: KMOVD k1, r32
4066                 // Code: VEX.L0.F2.0F.W0 92 /r
4067                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_F2, M_0F, W0, W0, true);
4068                 emitByte(0x92);
4069                 emitModRM(dst, src);
4070             }
4071         } else {
4072             if (inRC(MASK, src)) {
4073                 // kmovd(Register dst, KRegister src)
4074                 // Insn: KMOVD r32, k1
4075                 // Code: VEX.L0.F2.0F.W0 93 /r
4076                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_F2, M_0F, W0, W0, true);
4077                 emitByte(0x93);
4078                 emitModRM(dst, src);
4079             } else {
4080                 throw GraalError.shouldNotReachHere();
4081             }
4082         }
4083     }
4084 
4085     public final void kmovq(Register dst, Register src) {
4086         assert supports(CPUFeature.AVX512BW);
4087         assert inRC(MASK, dst) || inRC(CPU, dst);
4088         assert inRC(MASK, src) || inRC(CPU, src);
4089         assert !(inRC(CPU, dst) &amp;&amp; inRC(CPU, src));
4090 
4091         if (inRC(MASK, dst)) {
4092             if (inRC(MASK, src)) {
4093                 // kmovq(KRegister dst, KRegister src):
4094                 // Insn: KMOVQ k1, k2/m64
4095                 // Code: VEX.L0.0F.W1 90 /r
4096                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_, M_0F, W1, W1, true);
4097                 emitByte(0x90);
4098                 emitModRM(dst, src);
4099             } else {
4100                 // kmovq(KRegister dst, Register src)
4101                 // Insn: KMOVQ k1, r64
4102                 // Code: VEX.L0.F2.0F.W1 92 /r
4103                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_F2, M_0F, W1, W1, true);
4104                 emitByte(0x92);
4105                 emitModRM(dst, src);
4106             }
4107         } else {
4108             if (inRC(MASK, src)) {
4109                 // kmovq(Register dst, KRegister src)
4110                 // Insn: KMOVQ r64, k1
4111                 // Code: VEX.L0.F2.0F.W1 93 /r
4112                 vexPrefix(dst, Register.None, src, AVXSize.XMM, P_F2, M_0F, W1, W1, true);
4113                 emitByte(0x93);
4114                 emitModRM(dst, src);
4115             } else {
4116                 throw GraalError.shouldNotReachHere();
4117             }
4118         }
4119     }
4120 
4121     // Insn: KTESTD k1, k2
4122 
4123     public final void ktestd(Register src1, Register src2) {
4124         assert supports(CPUFeature.AVX512BW);
4125         assert inRC(MASK, src1) &amp;&amp; inRC(MASK, src2);
4126         // Code: VEX.L0.66.0F.W1 99 /r
4127         vexPrefix(src1, Register.None, src2, AVXSize.XMM, P_66, M_0F, W1, W1, true);
4128         emitByte(0x99);
4129         emitModRM(src1, src2);
4130     }
4131 
4132     public final void evmovdqu64(Register dst, AMD64Address src) {
4133         assert supports(CPUFeature.AVX512F);
4134         assert inRC(XMM, dst);
4135         evexPrefix(dst, Register.None, Register.None, src, AVXSize.ZMM, P_F3, M_0F, W1, Z0, B0);
4136         emitByte(0x6F);
4137         emitOperandHelper(dst, src, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));
4138     }
4139 
4140     public final void evmovdqu64(AMD64Address dst, Register src) {
4141         assert supports(CPUFeature.AVX512F);
4142         assert inRC(XMM, src);
4143         evexPrefix(src, Register.None, Register.None, dst, AVXSize.ZMM, P_F3, M_0F, W1, Z0, B0);
4144         emitByte(0x7F);
4145         emitOperandHelper(src, dst, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));
4146     }
4147 
4148     // Insn: VPMOVZXBW zmm1, m256
4149 
4150     public final void evpmovzxbw(Register dst, AMD64Address src) {
4151         assert supports(CPUFeature.AVX512BW);
4152         assert inRC(XMM, dst);
4153         // Code: EVEX.512.66.0F38.WIG 30 /r
4154         evexPrefix(dst, Register.None, Register.None, src, AVXSize.ZMM, P_66, M_0F38, WIG, Z0, B0);
4155         emitByte(0x30);
4156         emitOperandHelper(dst, src, 0, EVEXTuple.HVM.getDisp8ScalingFactor(AVXSize.ZMM));
4157     }
4158 
4159     public final void evpcmpeqb(Register kdst, Register nds, AMD64Address src) {
4160         assert supports(CPUFeature.AVX512BW);
4161         assert inRC(MASK, kdst) &amp;&amp; inRC(XMM, nds);
4162         evexPrefix(kdst, Register.None, nds, src, AVXSize.ZMM, P_66, M_0F, WIG, Z0, B0);
4163         emitByte(0x74);
4164         emitOperandHelper(kdst, src, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));
4165     }
4166 
4167     // Insn: VMOVDQU16 zmm1 {k1}{z}, zmm2/m512
4168     // -----
4169     // Insn: VMOVDQU16 zmm1, m512
4170 
4171     public final void evmovdqu16(Register dst, AMD64Address src) {
4172         assert supports(CPUFeature.AVX512BW);
4173         assert inRC(XMM, dst);
4174         // Code: EVEX.512.F2.0F.W1 6F /r
4175         evexPrefix(dst, Register.None, Register.None, src, AVXSize.ZMM, P_F2, M_0F, W1, Z0, B0);
4176         emitByte(0x6F);
4177         emitOperandHelper(dst, src, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));
4178     }
4179 
4180     // Insn: VMOVDQU16 zmm1, k1:z, m512
4181 
4182     public final void evmovdqu16(Register dst, Register mask, AMD64Address src) {
4183         assert supports(CPUFeature.AVX512BW);
4184         assert inRC(XMM, dst) &amp;&amp; inRC(MASK, mask);
4185         // Code: EVEX.512.F2.0F.W1 6F /r
4186         evexPrefix(dst, mask, Register.None, src, AVXSize.ZMM, P_F2, M_0F, W1, Z1, B0);
4187         emitByte(0x6F);
4188         emitOperandHelper(dst, src, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));
4189     }
4190 
4191     // Insn: VMOVDQU16 zmm2/m512 {k1}{z}, zmm1
4192     // -----
4193     // Insn: VMOVDQU16 m512, zmm1
4194 
4195     public final void evmovdqu16(AMD64Address dst, Register src) {
4196         assert supports(CPUFeature.AVX512BW);
4197         assert inRC(XMM, src);
4198         // Code: EVEX.512.F2.0F.W1 7F /r
4199         evexPrefix(src, Register.None, Register.None, dst, AVXSize.ZMM, P_F2, M_0F, W1, Z0, B0);
4200         emitByte(0x7F);
4201         emitOperandHelper(src, dst, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));
4202     }
4203 
4204     // Insn: VMOVDQU16 m512, k1, zmm1
4205 
4206     public final void evmovdqu16(AMD64Address dst, Register mask, Register src) {
4207         assert supports(CPUFeature.AVX512BW);
4208         assert inRC(MASK, mask) &amp;&amp; inRC(XMM, src);
4209         // Code: EVEX.512.F2.0F.W1 7F /r
4210         evexPrefix(src, mask, Register.None, dst, AVXSize.ZMM, P_F2, M_0F, W1, Z0, B0);
4211         emitByte(0x7F);
4212         emitOperandHelper(src, dst, 0, EVEXTuple.FVM.getDisp8ScalingFactor(AVXSize.ZMM));
4213     }
4214 
4215     // Insn: VPBROADCASTW zmm1 {k1}{z}, reg
4216     // -----
4217     // Insn: VPBROADCASTW zmm1, reg
4218 
4219     public final void evpbroadcastw(Register dst, Register src) {
4220         assert supports(CPUFeature.AVX512BW);
4221         assert inRC(XMM, dst) &amp;&amp; inRC(CPU, src);
4222         // Code: EVEX.512.66.0F38.W0 7B /r
4223         evexPrefix(dst, Register.None, Register.None, src, AVXSize.ZMM, P_66, M_0F38, W0, Z0, B0);
4224         emitByte(0x7B);
4225         emitModRM(dst, src);
4226     }
4227 
4228     // Insn: VPCMPUW k1 {k2}, zmm2, zmm3/m512, imm8
4229     // -----
4230     // Insn: VPCMPUW k1, zmm2, zmm3, imm8
4231 
4232     public final void evpcmpuw(Register kdst, Register nds, Register src, int vcc) {
4233         assert supports(CPUFeature.AVX512BW);
4234         assert inRC(MASK, kdst) &amp;&amp; inRC(XMM, nds) &amp;&amp; inRC(XMM, src);
4235         // Code: EVEX.NDS.512.66.0F3A.W1 3E /r ib
4236         evexPrefix(kdst, Register.None, nds, src, AVXSize.ZMM, P_66, M_0F3A, W1, Z0, B0);
4237         emitByte(0x3E);
4238         emitModRM(kdst, src);
4239         emitByte(vcc);
4240     }
4241 
4242     // Insn: VPCMPUW k1 {k2}, zmm2, zmm3/m512, imm8
4243     // -----
4244     // Insn: VPCMPUW k1, k2, zmm2, zmm3, imm8
4245 
4246     public final void evpcmpuw(Register kdst, Register mask, Register nds, Register src, int vcc) {
4247         assert supports(CPUFeature.AVX512BW);
4248         assert inRC(MASK, kdst) &amp;&amp; inRC(MASK, mask);
4249         assert inRC(XMM, nds) &amp;&amp; inRC(XMM, src);
4250         // Code: EVEX.NDS.512.66.0F3A.W1 3E /r ib
4251         evexPrefix(kdst, mask, nds, src, AVXSize.ZMM, P_66, M_0F3A, W1, Z0, B0);
4252         emitByte(0x3E);
4253         emitModRM(kdst, src);
4254         emitByte(vcc);
4255     }
4256 
4257     // Insn: VPMOVWB ymm1/m256 {k1}{z}, zmm2
4258     // -----
4259     // Insn: VPMOVWB m256, zmm2
4260 
4261     public final void evpmovwb(AMD64Address dst, Register src) {
4262         assert supports(CPUFeature.AVX512BW);
4263         assert inRC(XMM, src);
4264         // Code: EVEX.512.F3.0F38.W0 30 /r
4265         evexPrefix(src, Register.None, Register.None, dst, AVXSize.ZMM, P_F3, M_0F38, W0, Z0, B0);
4266         emitByte(0x30);
4267         emitOperandHelper(src, dst, 0, EVEXTuple.HVM.getDisp8ScalingFactor(AVXSize.ZMM));
4268     }
4269 
4270     // Insn: VPMOVWB m256, k1, zmm2
4271 
4272     public final void evpmovwb(AMD64Address dst, Register mask, Register src) {
4273         assert supports(CPUFeature.AVX512BW);
4274         assert inRC(MASK, mask) &amp;&amp; inRC(XMM, src);
4275         // Code: EVEX.512.F3.0F38.W0 30 /r
4276         evexPrefix(src, mask, Register.None, dst, AVXSize.ZMM, P_F3, M_0F38, W0, Z0, B0);
4277         emitByte(0x30);
4278         emitOperandHelper(src, dst, 0, EVEXTuple.HVM.getDisp8ScalingFactor(AVXSize.ZMM));
4279     }
4280 
4281     // Insn: VPMOVZXBW zmm1 {k1}{z}, ymm2/m256
4282     // -----
4283     // Insn: VPMOVZXBW zmm1, k1, m256
4284 
4285     public final void evpmovzxbw(Register dst, Register mask, AMD64Address src) {
4286         assert supports(CPUFeature.AVX512BW);
4287         assert inRC(MASK, mask) &amp;&amp; inRC(XMM, dst);
4288         // Code: EVEX.512.66.0F38.WIG 30 /r
4289         evexPrefix(dst, mask, Register.None, src, AVXSize.ZMM, P_66, M_0F38, WIG, Z0, B0);
4290         emitByte(0x30);
4291         emitOperandHelper(dst, src, 0, EVEXTuple.HVM.getDisp8ScalingFactor(AVXSize.ZMM));
4292     }
4293 
4294 }
    </pre>
  </body>
</html>