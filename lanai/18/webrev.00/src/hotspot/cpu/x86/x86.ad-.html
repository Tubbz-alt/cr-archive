<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/cpu/x86/x86.ad</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 //
   2 // Copyright (c) 2011, 2019, Oracle and/or its affiliates. All rights reserved.
   3 // DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4 //
   5 // This code is free software; you can redistribute it and/or modify it
   6 // under the terms of the GNU General Public License version 2 only, as
   7 // published by the Free Software Foundation.
   8 //
   9 // This code is distributed in the hope that it will be useful, but WITHOUT
  10 // ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11 // FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12 // version 2 for more details (a copy is included in the LICENSE file that
  13 // accompanied this code).
  14 //
  15 // You should have received a copy of the GNU General Public License version
  16 // 2 along with this work; if not, write to the Free Software Foundation,
  17 // Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18 //
  19 // Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20 // or visit www.oracle.com if you need additional information or have any
  21 // questions.
  22 //
  23 //
  24 
  25 // X86 Common Architecture Description File
  26 
  27 //----------REGISTER DEFINITION BLOCK------------------------------------------
  28 // This information is used by the matcher and the register allocator to
  29 // describe individual registers and classes of registers within the target
  30 // archtecture.
  31 
  32 register %{
  33 //----------Architecture Description Register Definitions----------------------
  34 // General Registers
  35 // &quot;reg_def&quot;  name ( register save type, C convention save type,
  36 //                   ideal register type, encoding );
  37 // Register Save Types:
  38 //
  39 // NS  = No-Save:       The register allocator assumes that these registers
  40 //                      can be used without saving upon entry to the method, &amp;
  41 //                      that they do not need to be saved at call sites.
  42 //
  43 // SOC = Save-On-Call:  The register allocator assumes that these registers
  44 //                      can be used without saving upon entry to the method,
  45 //                      but that they must be saved at call sites.
  46 //
  47 // SOE = Save-On-Entry: The register allocator assumes that these registers
  48 //                      must be saved before using them upon entry to the
  49 //                      method, but they do not need to be saved at call
  50 //                      sites.
  51 //
  52 // AS  = Always-Save:   The register allocator assumes that these registers
  53 //                      must be saved before using them upon entry to the
  54 //                      method, &amp; that they must be saved at call sites.
  55 //
  56 // Ideal Register Type is used to determine how to save &amp; restore a
  57 // register.  Op_RegI will get spilled with LoadI/StoreI, Op_RegP will get
  58 // spilled with LoadP/StoreP.  If the register supports both, use Op_RegI.
  59 //
  60 // The encoding number is the actual bit-pattern placed into the opcodes.
  61 
  62 // XMM registers.  512-bit registers or 8 words each, labeled (a)-p.
  63 // Word a in each register holds a Float, words ab hold a Double.
  64 // The whole registers are used in SSE4.2 version intrinsics,
  65 // array copy stubs and superword operations (see UseSSE42Intrinsics,
  66 // UseXMMForArrayCopy and UseSuperword flags).
  67 // For pre EVEX enabled architectures:
  68 //      XMM8-XMM15 must be encoded with REX (VEX for UseAVX)
  69 // For EVEX enabled architectures:
  70 //      XMM8-XMM31 must be encoded with REX (EVEX for UseAVX).
  71 //
  72 // Linux ABI:   No register preserved across function calls
  73 //              XMM0-XMM7 might hold parameters
  74 // Windows ABI: XMM6-XMM31 preserved across function calls
  75 //              XMM0-XMM3 might hold parameters
  76 
  77 reg_def XMM0 ( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg());
  78 reg_def XMM0b( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(1));
  79 reg_def XMM0c( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(2));
  80 reg_def XMM0d( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(3));
  81 reg_def XMM0e( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(4));
  82 reg_def XMM0f( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(5));
  83 reg_def XMM0g( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(6));
  84 reg_def XMM0h( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(7));
  85 reg_def XMM0i( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(8));
  86 reg_def XMM0j( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(9));
  87 reg_def XMM0k( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(10));
  88 reg_def XMM0l( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(11));
  89 reg_def XMM0m( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(12));
  90 reg_def XMM0n( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(13));
  91 reg_def XMM0o( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(14));
  92 reg_def XMM0p( SOC, SOC, Op_RegF, 0, xmm0-&gt;as_VMReg()-&gt;next(15));
  93 
  94 reg_def XMM1 ( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg());
  95 reg_def XMM1b( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(1));
  96 reg_def XMM1c( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(2));
  97 reg_def XMM1d( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(3));
  98 reg_def XMM1e( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(4));
  99 reg_def XMM1f( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(5));
 100 reg_def XMM1g( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(6));
 101 reg_def XMM1h( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(7));
 102 reg_def XMM1i( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(8));
 103 reg_def XMM1j( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(9));
 104 reg_def XMM1k( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(10));
 105 reg_def XMM1l( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(11));
 106 reg_def XMM1m( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(12));
 107 reg_def XMM1n( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(13));
 108 reg_def XMM1o( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(14));
 109 reg_def XMM1p( SOC, SOC, Op_RegF, 1, xmm1-&gt;as_VMReg()-&gt;next(15));
 110 
 111 reg_def XMM2 ( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg());
 112 reg_def XMM2b( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(1));
 113 reg_def XMM2c( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(2));
 114 reg_def XMM2d( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(3));
 115 reg_def XMM2e( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(4));
 116 reg_def XMM2f( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(5));
 117 reg_def XMM2g( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(6));
 118 reg_def XMM2h( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(7));
 119 reg_def XMM2i( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(8));
 120 reg_def XMM2j( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(9));
 121 reg_def XMM2k( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(10));
 122 reg_def XMM2l( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(11));
 123 reg_def XMM2m( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(12));
 124 reg_def XMM2n( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(13));
 125 reg_def XMM2o( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(14));
 126 reg_def XMM2p( SOC, SOC, Op_RegF, 2, xmm2-&gt;as_VMReg()-&gt;next(15));
 127 
 128 reg_def XMM3 ( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg());
 129 reg_def XMM3b( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(1));
 130 reg_def XMM3c( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(2));
 131 reg_def XMM3d( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(3));
 132 reg_def XMM3e( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(4));
 133 reg_def XMM3f( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(5));
 134 reg_def XMM3g( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(6));
 135 reg_def XMM3h( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(7));
 136 reg_def XMM3i( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(8));
 137 reg_def XMM3j( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(9));
 138 reg_def XMM3k( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(10));
 139 reg_def XMM3l( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(11));
 140 reg_def XMM3m( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(12));
 141 reg_def XMM3n( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(13));
 142 reg_def XMM3o( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(14));
 143 reg_def XMM3p( SOC, SOC, Op_RegF, 3, xmm3-&gt;as_VMReg()-&gt;next(15));
 144 
 145 reg_def XMM4 ( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg());
 146 reg_def XMM4b( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(1));
 147 reg_def XMM4c( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(2));
 148 reg_def XMM4d( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(3));
 149 reg_def XMM4e( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(4));
 150 reg_def XMM4f( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(5));
 151 reg_def XMM4g( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(6));
 152 reg_def XMM4h( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(7));
 153 reg_def XMM4i( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(8));
 154 reg_def XMM4j( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(9));
 155 reg_def XMM4k( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(10));
 156 reg_def XMM4l( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(11));
 157 reg_def XMM4m( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(12));
 158 reg_def XMM4n( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(13));
 159 reg_def XMM4o( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(14));
 160 reg_def XMM4p( SOC, SOC, Op_RegF, 4, xmm4-&gt;as_VMReg()-&gt;next(15));
 161 
 162 reg_def XMM5 ( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg());
 163 reg_def XMM5b( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(1));
 164 reg_def XMM5c( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(2));
 165 reg_def XMM5d( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(3));
 166 reg_def XMM5e( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(4));
 167 reg_def XMM5f( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(5));
 168 reg_def XMM5g( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(6));
 169 reg_def XMM5h( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(7));
 170 reg_def XMM5i( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(8));
 171 reg_def XMM5j( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(9));
 172 reg_def XMM5k( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(10));
 173 reg_def XMM5l( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(11));
 174 reg_def XMM5m( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(12));
 175 reg_def XMM5n( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(13));
 176 reg_def XMM5o( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(14));
 177 reg_def XMM5p( SOC, SOC, Op_RegF, 5, xmm5-&gt;as_VMReg()-&gt;next(15));
 178 
 179 reg_def XMM6 ( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg());
 180 reg_def XMM6b( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(1));
 181 reg_def XMM6c( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(2));
 182 reg_def XMM6d( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(3));
 183 reg_def XMM6e( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(4));
 184 reg_def XMM6f( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(5));
 185 reg_def XMM6g( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(6));
 186 reg_def XMM6h( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(7));
 187 reg_def XMM6i( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(8));
 188 reg_def XMM6j( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(9));
 189 reg_def XMM6k( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(10));
 190 reg_def XMM6l( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(11));
 191 reg_def XMM6m( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(12));
 192 reg_def XMM6n( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(13));
 193 reg_def XMM6o( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(14));
 194 reg_def XMM6p( SOC, SOC, Op_RegF, 6, xmm6-&gt;as_VMReg()-&gt;next(15));
 195 
 196 reg_def XMM7 ( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg());
 197 reg_def XMM7b( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(1));
 198 reg_def XMM7c( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(2));
 199 reg_def XMM7d( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(3));
 200 reg_def XMM7e( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(4));
 201 reg_def XMM7f( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(5));
 202 reg_def XMM7g( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(6));
 203 reg_def XMM7h( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(7));
 204 reg_def XMM7i( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(8));
 205 reg_def XMM7j( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(9));
 206 reg_def XMM7k( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(10));
 207 reg_def XMM7l( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(11));
 208 reg_def XMM7m( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(12));
 209 reg_def XMM7n( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(13));
 210 reg_def XMM7o( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(14));
 211 reg_def XMM7p( SOC, SOC, Op_RegF, 7, xmm7-&gt;as_VMReg()-&gt;next(15));
 212 
 213 #ifdef _LP64
 214 
 215 reg_def XMM8 ( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg());
 216 reg_def XMM8b( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(1));
 217 reg_def XMM8c( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(2));
 218 reg_def XMM8d( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(3));
 219 reg_def XMM8e( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(4));
 220 reg_def XMM8f( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(5));
 221 reg_def XMM8g( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(6));
 222 reg_def XMM8h( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(7));
 223 reg_def XMM8i( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(8));
 224 reg_def XMM8j( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(9));
 225 reg_def XMM8k( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(10));
 226 reg_def XMM8l( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(11));
 227 reg_def XMM8m( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(12));
 228 reg_def XMM8n( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(13));
 229 reg_def XMM8o( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(14));
 230 reg_def XMM8p( SOC, SOC, Op_RegF, 8, xmm8-&gt;as_VMReg()-&gt;next(15));
 231 
 232 reg_def XMM9 ( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg());
 233 reg_def XMM9b( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(1));
 234 reg_def XMM9c( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(2));
 235 reg_def XMM9d( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(3));
 236 reg_def XMM9e( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(4));
 237 reg_def XMM9f( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(5));
 238 reg_def XMM9g( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(6));
 239 reg_def XMM9h( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(7));
 240 reg_def XMM9i( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(8));
 241 reg_def XMM9j( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(9));
 242 reg_def XMM9k( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(10));
 243 reg_def XMM9l( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(11));
 244 reg_def XMM9m( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(12));
 245 reg_def XMM9n( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(13));
 246 reg_def XMM9o( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(14));
 247 reg_def XMM9p( SOC, SOC, Op_RegF, 9, xmm9-&gt;as_VMReg()-&gt;next(15));
 248 
 249 reg_def XMM10 ( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg());
 250 reg_def XMM10b( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(1));
 251 reg_def XMM10c( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(2));
 252 reg_def XMM10d( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(3));
 253 reg_def XMM10e( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(4));
 254 reg_def XMM10f( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(5));
 255 reg_def XMM10g( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(6));
 256 reg_def XMM10h( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(7));
 257 reg_def XMM10i( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(8));
 258 reg_def XMM10j( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(9));
 259 reg_def XMM10k( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(10));
 260 reg_def XMM10l( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(11));
 261 reg_def XMM10m( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(12));
 262 reg_def XMM10n( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(13));
 263 reg_def XMM10o( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(14));
 264 reg_def XMM10p( SOC, SOC, Op_RegF, 10, xmm10-&gt;as_VMReg()-&gt;next(15));
 265 
 266 reg_def XMM11 ( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg());
 267 reg_def XMM11b( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(1));
 268 reg_def XMM11c( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(2));
 269 reg_def XMM11d( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(3));
 270 reg_def XMM11e( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(4));
 271 reg_def XMM11f( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(5));
 272 reg_def XMM11g( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(6));
 273 reg_def XMM11h( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(7));
 274 reg_def XMM11i( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(8));
 275 reg_def XMM11j( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(9));
 276 reg_def XMM11k( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(10));
 277 reg_def XMM11l( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(11));
 278 reg_def XMM11m( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(12));
 279 reg_def XMM11n( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(13));
 280 reg_def XMM11o( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(14));
 281 reg_def XMM11p( SOC, SOC, Op_RegF, 11, xmm11-&gt;as_VMReg()-&gt;next(15));
 282 
 283 reg_def XMM12 ( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg());
 284 reg_def XMM12b( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(1));
 285 reg_def XMM12c( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(2));
 286 reg_def XMM12d( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(3));
 287 reg_def XMM12e( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(4));
 288 reg_def XMM12f( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(5));
 289 reg_def XMM12g( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(6));
 290 reg_def XMM12h( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(7));
 291 reg_def XMM12i( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(8));
 292 reg_def XMM12j( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(9));
 293 reg_def XMM12k( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(10));
 294 reg_def XMM12l( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(11));
 295 reg_def XMM12m( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(12));
 296 reg_def XMM12n( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(13));
 297 reg_def XMM12o( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(14));
 298 reg_def XMM12p( SOC, SOC, Op_RegF, 12, xmm12-&gt;as_VMReg()-&gt;next(15));
 299 
 300 reg_def XMM13 ( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg());
 301 reg_def XMM13b( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(1));
 302 reg_def XMM13c( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(2));
 303 reg_def XMM13d( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(3));
 304 reg_def XMM13e( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(4));
 305 reg_def XMM13f( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(5));
 306 reg_def XMM13g( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(6));
 307 reg_def XMM13h( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(7));
 308 reg_def XMM13i( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(8));
 309 reg_def XMM13j( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(9));
 310 reg_def XMM13k( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(10));
 311 reg_def XMM13l( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(11));
 312 reg_def XMM13m( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(12));
 313 reg_def XMM13n( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(13));
 314 reg_def XMM13o( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(14));
 315 reg_def XMM13p( SOC, SOC, Op_RegF, 13, xmm13-&gt;as_VMReg()-&gt;next(15));
 316 
 317 reg_def XMM14 ( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg());
 318 reg_def XMM14b( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(1));
 319 reg_def XMM14c( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(2));
 320 reg_def XMM14d( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(3));
 321 reg_def XMM14e( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(4));
 322 reg_def XMM14f( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(5));
 323 reg_def XMM14g( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(6));
 324 reg_def XMM14h( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(7));
 325 reg_def XMM14i( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(8));
 326 reg_def XMM14j( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(9));
 327 reg_def XMM14k( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(10));
 328 reg_def XMM14l( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(11));
 329 reg_def XMM14m( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(12));
 330 reg_def XMM14n( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(13));
 331 reg_def XMM14o( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(14));
 332 reg_def XMM14p( SOC, SOC, Op_RegF, 14, xmm14-&gt;as_VMReg()-&gt;next(15));
 333 
 334 reg_def XMM15 ( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg());
 335 reg_def XMM15b( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(1));
 336 reg_def XMM15c( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(2));
 337 reg_def XMM15d( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(3));
 338 reg_def XMM15e( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(4));
 339 reg_def XMM15f( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(5));
 340 reg_def XMM15g( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(6));
 341 reg_def XMM15h( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(7));
 342 reg_def XMM15i( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(8));
 343 reg_def XMM15j( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(9));
 344 reg_def XMM15k( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(10));
 345 reg_def XMM15l( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(11));
 346 reg_def XMM15m( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(12));
 347 reg_def XMM15n( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(13));
 348 reg_def XMM15o( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(14));
 349 reg_def XMM15p( SOC, SOC, Op_RegF, 15, xmm15-&gt;as_VMReg()-&gt;next(15));
 350 
 351 reg_def XMM16 ( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg());
 352 reg_def XMM16b( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(1));
 353 reg_def XMM16c( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(2));
 354 reg_def XMM16d( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(3));
 355 reg_def XMM16e( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(4));
 356 reg_def XMM16f( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(5));
 357 reg_def XMM16g( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(6));
 358 reg_def XMM16h( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(7));
 359 reg_def XMM16i( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(8));
 360 reg_def XMM16j( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(9));
 361 reg_def XMM16k( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(10));
 362 reg_def XMM16l( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(11));
 363 reg_def XMM16m( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(12));
 364 reg_def XMM16n( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(13));
 365 reg_def XMM16o( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(14));
 366 reg_def XMM16p( SOC, SOC, Op_RegF, 16, xmm16-&gt;as_VMReg()-&gt;next(15));
 367 
 368 reg_def XMM17 ( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg());
 369 reg_def XMM17b( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(1));
 370 reg_def XMM17c( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(2));
 371 reg_def XMM17d( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(3));
 372 reg_def XMM17e( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(4));
 373 reg_def XMM17f( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(5));
 374 reg_def XMM17g( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(6));
 375 reg_def XMM17h( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(7));
 376 reg_def XMM17i( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(8));
 377 reg_def XMM17j( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(9));
 378 reg_def XMM17k( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(10));
 379 reg_def XMM17l( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(11));
 380 reg_def XMM17m( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(12));
 381 reg_def XMM17n( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(13));
 382 reg_def XMM17o( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(14));
 383 reg_def XMM17p( SOC, SOC, Op_RegF, 17, xmm17-&gt;as_VMReg()-&gt;next(15));
 384 
 385 reg_def XMM18 ( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg());
 386 reg_def XMM18b( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(1));
 387 reg_def XMM18c( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(2));
 388 reg_def XMM18d( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(3));
 389 reg_def XMM18e( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(4));
 390 reg_def XMM18f( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(5));
 391 reg_def XMM18g( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(6));
 392 reg_def XMM18h( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(7));
 393 reg_def XMM18i( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(8));
 394 reg_def XMM18j( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(9));
 395 reg_def XMM18k( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(10));
 396 reg_def XMM18l( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(11));
 397 reg_def XMM18m( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(12));
 398 reg_def XMM18n( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(13));
 399 reg_def XMM18o( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(14));
 400 reg_def XMM18p( SOC, SOC, Op_RegF, 18, xmm18-&gt;as_VMReg()-&gt;next(15));
 401 
 402 reg_def XMM19 ( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg());
 403 reg_def XMM19b( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(1));
 404 reg_def XMM19c( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(2));
 405 reg_def XMM19d( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(3));
 406 reg_def XMM19e( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(4));
 407 reg_def XMM19f( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(5));
 408 reg_def XMM19g( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(6));
 409 reg_def XMM19h( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(7));
 410 reg_def XMM19i( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(8));
 411 reg_def XMM19j( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(9));
 412 reg_def XMM19k( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(10));
 413 reg_def XMM19l( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(11));
 414 reg_def XMM19m( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(12));
 415 reg_def XMM19n( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(13));
 416 reg_def XMM19o( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(14));
 417 reg_def XMM19p( SOC, SOC, Op_RegF, 19, xmm19-&gt;as_VMReg()-&gt;next(15));
 418 
 419 reg_def XMM20 ( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg());
 420 reg_def XMM20b( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(1));
 421 reg_def XMM20c( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(2));
 422 reg_def XMM20d( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(3));
 423 reg_def XMM20e( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(4));
 424 reg_def XMM20f( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(5));
 425 reg_def XMM20g( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(6));
 426 reg_def XMM20h( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(7));
 427 reg_def XMM20i( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(8));
 428 reg_def XMM20j( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(9));
 429 reg_def XMM20k( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(10));
 430 reg_def XMM20l( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(11));
 431 reg_def XMM20m( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(12));
 432 reg_def XMM20n( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(13));
 433 reg_def XMM20o( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(14));
 434 reg_def XMM20p( SOC, SOC, Op_RegF, 20, xmm20-&gt;as_VMReg()-&gt;next(15));
 435 
 436 reg_def XMM21 ( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg());
 437 reg_def XMM21b( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(1));
 438 reg_def XMM21c( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(2));
 439 reg_def XMM21d( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(3));
 440 reg_def XMM21e( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(4));
 441 reg_def XMM21f( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(5));
 442 reg_def XMM21g( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(6));
 443 reg_def XMM21h( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(7));
 444 reg_def XMM21i( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(8));
 445 reg_def XMM21j( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(9));
 446 reg_def XMM21k( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(10));
 447 reg_def XMM21l( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(11));
 448 reg_def XMM21m( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(12));
 449 reg_def XMM21n( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(13));
 450 reg_def XMM21o( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(14));
 451 reg_def XMM21p( SOC, SOC, Op_RegF, 21, xmm21-&gt;as_VMReg()-&gt;next(15));
 452 
 453 reg_def XMM22 ( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg());
 454 reg_def XMM22b( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(1));
 455 reg_def XMM22c( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(2));
 456 reg_def XMM22d( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(3));
 457 reg_def XMM22e( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(4));
 458 reg_def XMM22f( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(5));
 459 reg_def XMM22g( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(6));
 460 reg_def XMM22h( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(7));
 461 reg_def XMM22i( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(8));
 462 reg_def XMM22j( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(9));
 463 reg_def XMM22k( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(10));
 464 reg_def XMM22l( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(11));
 465 reg_def XMM22m( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(12));
 466 reg_def XMM22n( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(13));
 467 reg_def XMM22o( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(14));
 468 reg_def XMM22p( SOC, SOC, Op_RegF, 22, xmm22-&gt;as_VMReg()-&gt;next(15));
 469 
 470 reg_def XMM23 ( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg());
 471 reg_def XMM23b( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(1));
 472 reg_def XMM23c( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(2));
 473 reg_def XMM23d( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(3));
 474 reg_def XMM23e( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(4));
 475 reg_def XMM23f( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(5));
 476 reg_def XMM23g( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(6));
 477 reg_def XMM23h( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(7));
 478 reg_def XMM23i( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(8));
 479 reg_def XMM23j( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(9));
 480 reg_def XMM23k( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(10));
 481 reg_def XMM23l( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(11));
 482 reg_def XMM23m( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(12));
 483 reg_def XMM23n( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(13));
 484 reg_def XMM23o( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(14));
 485 reg_def XMM23p( SOC, SOC, Op_RegF, 23, xmm23-&gt;as_VMReg()-&gt;next(15));
 486 
 487 reg_def XMM24 ( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg());
 488 reg_def XMM24b( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(1));
 489 reg_def XMM24c( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(2));
 490 reg_def XMM24d( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(3));
 491 reg_def XMM24e( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(4));
 492 reg_def XMM24f( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(5));
 493 reg_def XMM24g( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(6));
 494 reg_def XMM24h( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(7));
 495 reg_def XMM24i( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(8));
 496 reg_def XMM24j( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(9));
 497 reg_def XMM24k( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(10));
 498 reg_def XMM24l( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(11));
 499 reg_def XMM24m( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(12));
 500 reg_def XMM24n( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(13));
 501 reg_def XMM24o( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(14));
 502 reg_def XMM24p( SOC, SOC, Op_RegF, 24, xmm24-&gt;as_VMReg()-&gt;next(15));
 503 
 504 reg_def XMM25 ( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg());
 505 reg_def XMM25b( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(1));
 506 reg_def XMM25c( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(2));
 507 reg_def XMM25d( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(3));
 508 reg_def XMM25e( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(4));
 509 reg_def XMM25f( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(5));
 510 reg_def XMM25g( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(6));
 511 reg_def XMM25h( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(7));
 512 reg_def XMM25i( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(8));
 513 reg_def XMM25j( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(9));
 514 reg_def XMM25k( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(10));
 515 reg_def XMM25l( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(11));
 516 reg_def XMM25m( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(12));
 517 reg_def XMM25n( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(13));
 518 reg_def XMM25o( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(14));
 519 reg_def XMM25p( SOC, SOC, Op_RegF, 25, xmm25-&gt;as_VMReg()-&gt;next(15));
 520 
 521 reg_def XMM26 ( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg());
 522 reg_def XMM26b( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(1));
 523 reg_def XMM26c( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(2));
 524 reg_def XMM26d( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(3));
 525 reg_def XMM26e( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(4));
 526 reg_def XMM26f( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(5));
 527 reg_def XMM26g( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(6));
 528 reg_def XMM26h( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(7));
 529 reg_def XMM26i( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(8));
 530 reg_def XMM26j( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(9));
 531 reg_def XMM26k( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(10));
 532 reg_def XMM26l( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(11));
 533 reg_def XMM26m( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(12));
 534 reg_def XMM26n( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(13));
 535 reg_def XMM26o( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(14));
 536 reg_def XMM26p( SOC, SOC, Op_RegF, 26, xmm26-&gt;as_VMReg()-&gt;next(15));
 537 
 538 reg_def XMM27 ( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg());
 539 reg_def XMM27b( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(1));
 540 reg_def XMM27c( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(2));
 541 reg_def XMM27d( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(3));
 542 reg_def XMM27e( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(4));
 543 reg_def XMM27f( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(5));
 544 reg_def XMM27g( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(6));
 545 reg_def XMM27h( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(7));
 546 reg_def XMM27i( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(8));
 547 reg_def XMM27j( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(9));
 548 reg_def XMM27k( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(10));
 549 reg_def XMM27l( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(11));
 550 reg_def XMM27m( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(12));
 551 reg_def XMM27n( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(13));
 552 reg_def XMM27o( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(14));
 553 reg_def XMM27p( SOC, SOC, Op_RegF, 27, xmm27-&gt;as_VMReg()-&gt;next(15));
 554 
 555 reg_def XMM28 ( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg());
 556 reg_def XMM28b( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(1));
 557 reg_def XMM28c( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(2));
 558 reg_def XMM28d( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(3));
 559 reg_def XMM28e( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(4));
 560 reg_def XMM28f( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(5));
 561 reg_def XMM28g( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(6));
 562 reg_def XMM28h( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(7));
 563 reg_def XMM28i( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(8));
 564 reg_def XMM28j( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(9));
 565 reg_def XMM28k( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(10));
 566 reg_def XMM28l( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(11));
 567 reg_def XMM28m( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(12));
 568 reg_def XMM28n( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(13));
 569 reg_def XMM28o( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(14));
 570 reg_def XMM28p( SOC, SOC, Op_RegF, 28, xmm28-&gt;as_VMReg()-&gt;next(15));
 571 
 572 reg_def XMM29 ( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg());
 573 reg_def XMM29b( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(1));
 574 reg_def XMM29c( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(2));
 575 reg_def XMM29d( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(3));
 576 reg_def XMM29e( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(4));
 577 reg_def XMM29f( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(5));
 578 reg_def XMM29g( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(6));
 579 reg_def XMM29h( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(7));
 580 reg_def XMM29i( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(8));
 581 reg_def XMM29j( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(9));
 582 reg_def XMM29k( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(10));
 583 reg_def XMM29l( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(11));
 584 reg_def XMM29m( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(12));
 585 reg_def XMM29n( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(13));
 586 reg_def XMM29o( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(14));
 587 reg_def XMM29p( SOC, SOC, Op_RegF, 29, xmm29-&gt;as_VMReg()-&gt;next(15));
 588 
 589 reg_def XMM30 ( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg());
 590 reg_def XMM30b( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(1));
 591 reg_def XMM30c( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(2));
 592 reg_def XMM30d( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(3));
 593 reg_def XMM30e( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(4));
 594 reg_def XMM30f( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(5));
 595 reg_def XMM30g( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(6));
 596 reg_def XMM30h( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(7));
 597 reg_def XMM30i( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(8));
 598 reg_def XMM30j( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(9));
 599 reg_def XMM30k( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(10));
 600 reg_def XMM30l( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(11));
 601 reg_def XMM30m( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(12));
 602 reg_def XMM30n( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(13));
 603 reg_def XMM30o( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(14));
 604 reg_def XMM30p( SOC, SOC, Op_RegF, 30, xmm30-&gt;as_VMReg()-&gt;next(15));
 605 
 606 reg_def XMM31 ( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg());
 607 reg_def XMM31b( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(1));
 608 reg_def XMM31c( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(2));
 609 reg_def XMM31d( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(3));
 610 reg_def XMM31e( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(4));
 611 reg_def XMM31f( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(5));
 612 reg_def XMM31g( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(6));
 613 reg_def XMM31h( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(7));
 614 reg_def XMM31i( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(8));
 615 reg_def XMM31j( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(9));
 616 reg_def XMM31k( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(10));
 617 reg_def XMM31l( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(11));
 618 reg_def XMM31m( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(12));
 619 reg_def XMM31n( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(13));
 620 reg_def XMM31o( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(14));
 621 reg_def XMM31p( SOC, SOC, Op_RegF, 31, xmm31-&gt;as_VMReg()-&gt;next(15));
 622 
 623 #endif // _LP64
 624 
 625 #ifdef _LP64
 626 reg_def RFLAGS(SOC, SOC, 0, 16, VMRegImpl::Bad());
 627 #else
 628 reg_def RFLAGS(SOC, SOC, 0, 8, VMRegImpl::Bad());
 629 #endif // _LP64
 630 
 631 alloc_class chunk1(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
 632                    XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
 633                    XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
 634                    XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
 635                    XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
 636                    XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
 637                    XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
 638                    XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p
 639 #ifdef _LP64
 640                   ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
 641                    XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
 642                    XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
 643                    XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
 644                    XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
 645                    XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
 646                    XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
 647                    XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p
 648                   ,XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,
 649                    XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h, XMM17i, XMM17j, XMM17k, XMM17l, XMM17m, XMM17n, XMM17o, XMM17p,
 650                    XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h, XMM18i, XMM18j, XMM18k, XMM18l, XMM18m, XMM18n, XMM18o, XMM18p,
 651                    XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h, XMM19i, XMM19j, XMM19k, XMM19l, XMM19m, XMM19n, XMM19o, XMM19p,
 652                    XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h, XMM20i, XMM20j, XMM20k, XMM20l, XMM20m, XMM20n, XMM20o, XMM20p,
 653                    XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h, XMM21i, XMM21j, XMM21k, XMM21l, XMM21m, XMM21n, XMM21o, XMM21p,
 654                    XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h, XMM22i, XMM22j, XMM22k, XMM22l, XMM22m, XMM22n, XMM22o, XMM22p,
 655                    XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h, XMM23i, XMM23j, XMM23k, XMM23l, XMM23m, XMM23n, XMM23o, XMM23p,
 656                    XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h, XMM24i, XMM24j, XMM24k, XMM24l, XMM24m, XMM24n, XMM24o, XMM24p,
 657                    XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h, XMM25i, XMM25j, XMM25k, XMM25l, XMM25m, XMM25n, XMM25o, XMM25p,
 658                    XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h, XMM26i, XMM26j, XMM26k, XMM26l, XMM26m, XMM26n, XMM26o, XMM26p,
 659                    XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h, XMM27i, XMM27j, XMM27k, XMM27l, XMM27m, XMM27n, XMM27o, XMM27p,
 660                    XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h, XMM28i, XMM28j, XMM28k, XMM28l, XMM28m, XMM28n, XMM28o, XMM28p,
 661                    XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h, XMM29i, XMM29j, XMM29k, XMM29l, XMM29m, XMM29n, XMM29o, XMM29p,
 662                    XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h, XMM30i, XMM30j, XMM30k, XMM30l, XMM30m, XMM30n, XMM30o, XMM30p,
 663                    XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p
 664 #endif
 665                       );
 666 
 667 // flags allocation class should be last.
 668 alloc_class chunk2(RFLAGS);
 669 
 670 // Singleton class for condition codes
 671 reg_class int_flags(RFLAGS);
 672 
 673 // Class for pre evex float registers
 674 reg_class float_reg_legacy(XMM0,
 675                     XMM1,
 676                     XMM2,
 677                     XMM3,
 678                     XMM4,
 679                     XMM5,
 680                     XMM6,
 681                     XMM7
 682 #ifdef _LP64
 683                    ,XMM8,
 684                     XMM9,
 685                     XMM10,
 686                     XMM11,
 687                     XMM12,
 688                     XMM13,
 689                     XMM14,
 690                     XMM15
 691 #endif
 692                     );
 693 
 694 // Class for evex float registers
 695 reg_class float_reg_evex(XMM0,
 696                     XMM1,
 697                     XMM2,
 698                     XMM3,
 699                     XMM4,
 700                     XMM5,
 701                     XMM6,
 702                     XMM7
 703 #ifdef _LP64
 704                    ,XMM8,
 705                     XMM9,
 706                     XMM10,
 707                     XMM11,
 708                     XMM12,
 709                     XMM13,
 710                     XMM14,
 711                     XMM15,
 712                     XMM16,
 713                     XMM17,
 714                     XMM18,
 715                     XMM19,
 716                     XMM20,
 717                     XMM21,
 718                     XMM22,
 719                     XMM23,
 720                     XMM24,
 721                     XMM25,
 722                     XMM26,
 723                     XMM27,
 724                     XMM28,
 725                     XMM29,
 726                     XMM30,
 727                     XMM31
 728 #endif
 729                     );
 730 
 731 reg_class_dynamic float_reg(float_reg_evex, float_reg_legacy, %{ VM_Version::supports_evex() %} );
 732 reg_class_dynamic float_reg_vl(float_reg_evex, float_reg_legacy, %{ VM_Version::supports_evex() &amp;&amp; VM_Version::supports_avx512vl() %} );
 733 
 734 // Class for pre evex double registers
 735 reg_class double_reg_legacy(XMM0,  XMM0b,
 736                      XMM1,  XMM1b,
 737                      XMM2,  XMM2b,
 738                      XMM3,  XMM3b,
 739                      XMM4,  XMM4b,
 740                      XMM5,  XMM5b,
 741                      XMM6,  XMM6b,
 742                      XMM7,  XMM7b
 743 #ifdef _LP64
 744                     ,XMM8,  XMM8b,
 745                      XMM9,  XMM9b,
 746                      XMM10, XMM10b,
 747                      XMM11, XMM11b,
 748                      XMM12, XMM12b,
 749                      XMM13, XMM13b,
 750                      XMM14, XMM14b,
 751                      XMM15, XMM15b
 752 #endif
 753                      );
 754 
 755 // Class for evex double registers
 756 reg_class double_reg_evex(XMM0,  XMM0b,
 757                      XMM1,  XMM1b,
 758                      XMM2,  XMM2b,
 759                      XMM3,  XMM3b,
 760                      XMM4,  XMM4b,
 761                      XMM5,  XMM5b,
 762                      XMM6,  XMM6b,
 763                      XMM7,  XMM7b
 764 #ifdef _LP64
 765                     ,XMM8,  XMM8b,
 766                      XMM9,  XMM9b,
 767                      XMM10, XMM10b,
 768                      XMM11, XMM11b,
 769                      XMM12, XMM12b,
 770                      XMM13, XMM13b,
 771                      XMM14, XMM14b,
 772                      XMM15, XMM15b,
 773                      XMM16, XMM16b,
 774                      XMM17, XMM17b,
 775                      XMM18, XMM18b,
 776                      XMM19, XMM19b,
 777                      XMM20, XMM20b,
 778                      XMM21, XMM21b,
 779                      XMM22, XMM22b,
 780                      XMM23, XMM23b,
 781                      XMM24, XMM24b,
 782                      XMM25, XMM25b,
 783                      XMM26, XMM26b,
 784                      XMM27, XMM27b,
 785                      XMM28, XMM28b,
 786                      XMM29, XMM29b,
 787                      XMM30, XMM30b,
 788                      XMM31, XMM31b
 789 #endif
 790                      );
 791 
 792 reg_class_dynamic double_reg(double_reg_evex, double_reg_legacy, %{ VM_Version::supports_evex() %} );
 793 reg_class_dynamic double_reg_vl(double_reg_evex, double_reg_legacy, %{ VM_Version::supports_evex() &amp;&amp; VM_Version::supports_avx512vl() %} );
 794 
 795 // Class for pre evex 32bit vector registers
 796 reg_class vectors_reg_legacy(XMM0,
 797                       XMM1,
 798                       XMM2,
 799                       XMM3,
 800                       XMM4,
 801                       XMM5,
 802                       XMM6,
 803                       XMM7
 804 #ifdef _LP64
 805                      ,XMM8,
 806                       XMM9,
 807                       XMM10,
 808                       XMM11,
 809                       XMM12,
 810                       XMM13,
 811                       XMM14,
 812                       XMM15
 813 #endif
 814                       );
 815 
 816 // Class for evex 32bit vector registers
 817 reg_class vectors_reg_evex(XMM0,
 818                       XMM1,
 819                       XMM2,
 820                       XMM3,
 821                       XMM4,
 822                       XMM5,
 823                       XMM6,
 824                       XMM7
 825 #ifdef _LP64
 826                      ,XMM8,
 827                       XMM9,
 828                       XMM10,
 829                       XMM11,
 830                       XMM12,
 831                       XMM13,
 832                       XMM14,
 833                       XMM15,
 834                       XMM16,
 835                       XMM17,
 836                       XMM18,
 837                       XMM19,
 838                       XMM20,
 839                       XMM21,
 840                       XMM22,
 841                       XMM23,
 842                       XMM24,
 843                       XMM25,
 844                       XMM26,
 845                       XMM27,
 846                       XMM28,
 847                       XMM29,
 848                       XMM30,
 849                       XMM31
 850 #endif
 851                       );
 852 
 853 reg_class_dynamic vectors_reg(vectors_reg_evex, vectors_reg_legacy, %{ VM_Version::supports_evex() %} );
 854 reg_class_dynamic vectors_reg_vlbwdq(vectors_reg_evex, vectors_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
 855 
 856 // Class for all 64bit vector registers
 857 reg_class vectord_reg_legacy(XMM0,  XMM0b,
 858                       XMM1,  XMM1b,
 859                       XMM2,  XMM2b,
 860                       XMM3,  XMM3b,
 861                       XMM4,  XMM4b,
 862                       XMM5,  XMM5b,
 863                       XMM6,  XMM6b,
 864                       XMM7,  XMM7b
 865 #ifdef _LP64
 866                      ,XMM8,  XMM8b,
 867                       XMM9,  XMM9b,
 868                       XMM10, XMM10b,
 869                       XMM11, XMM11b,
 870                       XMM12, XMM12b,
 871                       XMM13, XMM13b,
 872                       XMM14, XMM14b,
 873                       XMM15, XMM15b
 874 #endif
 875                       );
 876 
 877 // Class for all 64bit vector registers
 878 reg_class vectord_reg_evex(XMM0,  XMM0b,
 879                       XMM1,  XMM1b,
 880                       XMM2,  XMM2b,
 881                       XMM3,  XMM3b,
 882                       XMM4,  XMM4b,
 883                       XMM5,  XMM5b,
 884                       XMM6,  XMM6b,
 885                       XMM7,  XMM7b
 886 #ifdef _LP64
 887                      ,XMM8,  XMM8b,
 888                       XMM9,  XMM9b,
 889                       XMM10, XMM10b,
 890                       XMM11, XMM11b,
 891                       XMM12, XMM12b,
 892                       XMM13, XMM13b,
 893                       XMM14, XMM14b,
 894                       XMM15, XMM15b,
 895                       XMM16, XMM16b,
 896                       XMM17, XMM17b,
 897                       XMM18, XMM18b,
 898                       XMM19, XMM19b,
 899                       XMM20, XMM20b,
 900                       XMM21, XMM21b,
 901                       XMM22, XMM22b,
 902                       XMM23, XMM23b,
 903                       XMM24, XMM24b,
 904                       XMM25, XMM25b,
 905                       XMM26, XMM26b,
 906                       XMM27, XMM27b,
 907                       XMM28, XMM28b,
 908                       XMM29, XMM29b,
 909                       XMM30, XMM30b,
 910                       XMM31, XMM31b
 911 #endif
 912                       );
 913 
 914 reg_class_dynamic vectord_reg(vectord_reg_evex, vectord_reg_legacy, %{ VM_Version::supports_evex() %} );
 915 reg_class_dynamic vectord_reg_vlbwdq(vectord_reg_evex, vectord_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
 916 
 917 // Class for all 128bit vector registers
 918 reg_class vectorx_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,
 919                       XMM1,  XMM1b,  XMM1c,  XMM1d,
 920                       XMM2,  XMM2b,  XMM2c,  XMM2d,
 921                       XMM3,  XMM3b,  XMM3c,  XMM3d,
 922                       XMM4,  XMM4b,  XMM4c,  XMM4d,
 923                       XMM5,  XMM5b,  XMM5c,  XMM5d,
 924                       XMM6,  XMM6b,  XMM6c,  XMM6d,
 925                       XMM7,  XMM7b,  XMM7c,  XMM7d
 926 #ifdef _LP64
 927                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,
 928                       XMM9,  XMM9b,  XMM9c,  XMM9d,
 929                       XMM10, XMM10b, XMM10c, XMM10d,
 930                       XMM11, XMM11b, XMM11c, XMM11d,
 931                       XMM12, XMM12b, XMM12c, XMM12d,
 932                       XMM13, XMM13b, XMM13c, XMM13d,
 933                       XMM14, XMM14b, XMM14c, XMM14d,
 934                       XMM15, XMM15b, XMM15c, XMM15d
 935 #endif
 936                       );
 937 
 938 // Class for all 128bit vector registers
 939 reg_class vectorx_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,
 940                       XMM1,  XMM1b,  XMM1c,  XMM1d,
 941                       XMM2,  XMM2b,  XMM2c,  XMM2d,
 942                       XMM3,  XMM3b,  XMM3c,  XMM3d,
 943                       XMM4,  XMM4b,  XMM4c,  XMM4d,
 944                       XMM5,  XMM5b,  XMM5c,  XMM5d,
 945                       XMM6,  XMM6b,  XMM6c,  XMM6d,
 946                       XMM7,  XMM7b,  XMM7c,  XMM7d
 947 #ifdef _LP64
 948                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,
 949                       XMM9,  XMM9b,  XMM9c,  XMM9d,
 950                       XMM10, XMM10b, XMM10c, XMM10d,
 951                       XMM11, XMM11b, XMM11c, XMM11d,
 952                       XMM12, XMM12b, XMM12c, XMM12d,
 953                       XMM13, XMM13b, XMM13c, XMM13d,
 954                       XMM14, XMM14b, XMM14c, XMM14d,
 955                       XMM15, XMM15b, XMM15c, XMM15d,
 956                       XMM16, XMM16b, XMM16c, XMM16d,
 957                       XMM17, XMM17b, XMM17c, XMM17d,
 958                       XMM18, XMM18b, XMM18c, XMM18d,
 959                       XMM19, XMM19b, XMM19c, XMM19d,
 960                       XMM20, XMM20b, XMM20c, XMM20d,
 961                       XMM21, XMM21b, XMM21c, XMM21d,
 962                       XMM22, XMM22b, XMM22c, XMM22d,
 963                       XMM23, XMM23b, XMM23c, XMM23d,
 964                       XMM24, XMM24b, XMM24c, XMM24d,
 965                       XMM25, XMM25b, XMM25c, XMM25d,
 966                       XMM26, XMM26b, XMM26c, XMM26d,
 967                       XMM27, XMM27b, XMM27c, XMM27d,
 968                       XMM28, XMM28b, XMM28c, XMM28d,
 969                       XMM29, XMM29b, XMM29c, XMM29d,
 970                       XMM30, XMM30b, XMM30c, XMM30d,
 971                       XMM31, XMM31b, XMM31c, XMM31d
 972 #endif
 973                       );
 974 
 975 reg_class_dynamic vectorx_reg(vectorx_reg_evex, vectorx_reg_legacy, %{ VM_Version::supports_evex() %} );
 976 reg_class_dynamic vectorx_reg_vlbwdq(vectorx_reg_evex, vectorx_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
 977 
 978 // Class for all 256bit vector registers
 979 reg_class vectory_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,
 980                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,
 981                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,
 982                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,
 983                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,
 984                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,
 985                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,
 986                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h
 987 #ifdef _LP64
 988                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,
 989                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,
 990                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h,
 991                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h,
 992                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h,
 993                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h,
 994                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h,
 995                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h
 996 #endif
 997                       );
 998 
 999 // Class for all 256bit vector registers
1000 reg_class vectory_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,
1001                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,
1002                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,
1003                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,
1004                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,
1005                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,
1006                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,
1007                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h
1008 #ifdef _LP64
1009                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,
1010                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,
1011                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h,
1012                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h,
1013                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h,
1014                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h,
1015                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h,
1016                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h,
1017                       XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h,
1018                       XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h,
1019                       XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h,
1020                       XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h,
1021                       XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h,
1022                       XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h,
1023                       XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h,
1024                       XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h,
1025                       XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h,
1026                       XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h,
1027                       XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h,
1028                       XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h,
1029                       XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h,
1030                       XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h,
1031                       XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h,
1032                       XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h
1033 #endif
1034                       );
1035 
1036 reg_class_dynamic vectory_reg(vectory_reg_evex, vectory_reg_legacy, %{ VM_Version::supports_evex() %} );
1037 reg_class_dynamic vectory_reg_vlbwdq(vectory_reg_evex, vectory_reg_legacy, %{ VM_Version::supports_avx512vlbwdq() %} );
1038 
1039 // Class for all 512bit vector registers
1040 reg_class vectorz_reg_evex(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
1041                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
1042                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
1043                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
1044                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
1045                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
1046                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
1047                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p
1048 #ifdef _LP64
1049                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
1050                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
1051                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
1052                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
1053                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
1054                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
1055                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
1056                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p
1057                      ,XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,
1058                       XMM17, XMM17b, XMM17c, XMM17d, XMM17e, XMM17f, XMM17g, XMM17h, XMM17i, XMM17j, XMM17k, XMM17l, XMM17m, XMM17n, XMM17o, XMM17p,
1059                       XMM18, XMM18b, XMM18c, XMM18d, XMM18e, XMM18f, XMM18g, XMM18h, XMM18i, XMM18j, XMM18k, XMM18l, XMM18m, XMM18n, XMM18o, XMM18p,
1060                       XMM19, XMM19b, XMM19c, XMM19d, XMM19e, XMM19f, XMM19g, XMM19h, XMM19i, XMM19j, XMM19k, XMM19l, XMM19m, XMM19n, XMM19o, XMM19p,
1061                       XMM20, XMM20b, XMM20c, XMM20d, XMM20e, XMM20f, XMM20g, XMM20h, XMM20i, XMM20j, XMM20k, XMM20l, XMM20m, XMM20n, XMM20o, XMM20p,
1062                       XMM21, XMM21b, XMM21c, XMM21d, XMM21e, XMM21f, XMM21g, XMM21h, XMM21i, XMM21j, XMM21k, XMM21l, XMM21m, XMM21n, XMM21o, XMM21p,
1063                       XMM22, XMM22b, XMM22c, XMM22d, XMM22e, XMM22f, XMM22g, XMM22h, XMM22i, XMM22j, XMM22k, XMM22l, XMM22m, XMM22n, XMM22o, XMM22p,
1064                       XMM23, XMM23b, XMM23c, XMM23d, XMM23e, XMM23f, XMM23g, XMM23h, XMM23i, XMM23j, XMM23k, XMM23l, XMM23m, XMM23n, XMM23o, XMM23p,
1065                       XMM24, XMM24b, XMM24c, XMM24d, XMM24e, XMM24f, XMM24g, XMM24h, XMM24i, XMM24j, XMM24k, XMM24l, XMM24m, XMM24n, XMM24o, XMM24p,
1066                       XMM25, XMM25b, XMM25c, XMM25d, XMM25e, XMM25f, XMM25g, XMM25h, XMM25i, XMM25j, XMM25k, XMM25l, XMM25m, XMM25n, XMM25o, XMM25p,
1067                       XMM26, XMM26b, XMM26c, XMM26d, XMM26e, XMM26f, XMM26g, XMM26h, XMM26i, XMM26j, XMM26k, XMM26l, XMM26m, XMM26n, XMM26o, XMM26p,
1068                       XMM27, XMM27b, XMM27c, XMM27d, XMM27e, XMM27f, XMM27g, XMM27h, XMM27i, XMM27j, XMM27k, XMM27l, XMM27m, XMM27n, XMM27o, XMM27p,
1069                       XMM28, XMM28b, XMM28c, XMM28d, XMM28e, XMM28f, XMM28g, XMM28h, XMM28i, XMM28j, XMM28k, XMM28l, XMM28m, XMM28n, XMM28o, XMM28p,
1070                       XMM29, XMM29b, XMM29c, XMM29d, XMM29e, XMM29f, XMM29g, XMM29h, XMM29i, XMM29j, XMM29k, XMM29l, XMM29m, XMM29n, XMM29o, XMM29p,
1071                       XMM30, XMM30b, XMM30c, XMM30d, XMM30e, XMM30f, XMM30g, XMM30h, XMM30i, XMM30j, XMM30k, XMM30l, XMM30m, XMM30n, XMM30o, XMM30p,
1072                       XMM31, XMM31b, XMM31c, XMM31d, XMM31e, XMM31f, XMM31g, XMM31h, XMM31i, XMM31j, XMM31k, XMM31l, XMM31m, XMM31n, XMM31o, XMM31p
1073 #endif
1074                       );
1075 
1076 // Class for restricted 512bit vector registers
1077 reg_class vectorz_reg_legacy(XMM0,  XMM0b,  XMM0c,  XMM0d,  XMM0e,  XMM0f,  XMM0g,  XMM0h,  XMM0i,  XMM0j,  XMM0k,  XMM0l,  XMM0m,  XMM0n,  XMM0o,  XMM0p,
1078                       XMM1,  XMM1b,  XMM1c,  XMM1d,  XMM1e,  XMM1f,  XMM1g,  XMM1h,  XMM1i,  XMM1j,  XMM1k,  XMM1l,  XMM1m,  XMM1n,  XMM1o,  XMM1p,
1079                       XMM2,  XMM2b,  XMM2c,  XMM2d,  XMM2e,  XMM2f,  XMM2g,  XMM2h,  XMM2i,  XMM2j,  XMM2k,  XMM2l,  XMM2m,  XMM2n,  XMM2o,  XMM2p,
1080                       XMM3,  XMM3b,  XMM3c,  XMM3d,  XMM3e,  XMM3f,  XMM3g,  XMM3h,  XMM3i,  XMM3j,  XMM3k,  XMM3l,  XMM3m,  XMM3n,  XMM3o,  XMM3p,
1081                       XMM4,  XMM4b,  XMM4c,  XMM4d,  XMM4e,  XMM4f,  XMM4g,  XMM4h,  XMM4i,  XMM4j,  XMM4k,  XMM4l,  XMM4m,  XMM4n,  XMM4o,  XMM4p,
1082                       XMM5,  XMM5b,  XMM5c,  XMM5d,  XMM5e,  XMM5f,  XMM5g,  XMM5h,  XMM5i,  XMM5j,  XMM5k,  XMM5l,  XMM5m,  XMM5n,  XMM5o,  XMM5p,
1083                       XMM6,  XMM6b,  XMM6c,  XMM6d,  XMM6e,  XMM6f,  XMM6g,  XMM6h,  XMM6i,  XMM6j,  XMM6k,  XMM6l,  XMM6m,  XMM6n,  XMM6o,  XMM6p,
1084                       XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p
1085 #ifdef _LP64
1086                      ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,
1087                       XMM9,  XMM9b,  XMM9c,  XMM9d,  XMM9e,  XMM9f,  XMM9g,  XMM9h,  XMM9i,  XMM9j,  XMM9k,  XMM9l,  XMM9m,  XMM9n,  XMM9o,  XMM9p,
1088                       XMM10, XMM10b, XMM10c, XMM10d, XMM10e, XMM10f, XMM10g, XMM10h, XMM10i, XMM10j, XMM10k, XMM10l, XMM10m, XMM10n, XMM10o, XMM10p,
1089                       XMM11, XMM11b, XMM11c, XMM11d, XMM11e, XMM11f, XMM11g, XMM11h, XMM11i, XMM11j, XMM11k, XMM11l, XMM11m, XMM11n, XMM11o, XMM11p,
1090                       XMM12, XMM12b, XMM12c, XMM12d, XMM12e, XMM12f, XMM12g, XMM12h, XMM12i, XMM12j, XMM12k, XMM12l, XMM12m, XMM12n, XMM12o, XMM12p,
1091                       XMM13, XMM13b, XMM13c, XMM13d, XMM13e, XMM13f, XMM13g, XMM13h, XMM13i, XMM13j, XMM13k, XMM13l, XMM13m, XMM13n, XMM13o, XMM13p,
1092                       XMM14, XMM14b, XMM14c, XMM14d, XMM14e, XMM14f, XMM14g, XMM14h, XMM14i, XMM14j, XMM14k, XMM14l, XMM14m, XMM14n, XMM14o, XMM14p,
1093                       XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p
1094 #endif
1095                       );
1096 
1097 reg_class_dynamic vectorz_reg   (vectorz_reg_evex, vectorz_reg_legacy, %{ VM_Version::supports_evex() %} );
1098 reg_class_dynamic vectorz_reg_vl(vectorz_reg_evex, vectorz_reg_legacy, %{ VM_Version::supports_evex() &amp;&amp; VM_Version::supports_avx512vl() %} );
1099 
1100 %}
1101 
1102 
1103 //----------SOURCE BLOCK-------------------------------------------------------
1104 // This is a block of C++ code which provides values, functions, and
1105 // definitions necessary in the rest of the architecture description
1106 
1107 source_hpp %{
1108 // Header information of the source block.
1109 // Method declarations/definitions which are used outside
1110 // the ad-scope can conveniently be defined here.
1111 //
1112 // To keep related declarations/definitions/uses close together,
1113 // we switch between source %{ }% and source_hpp %{ }% freely as needed.
1114 
1115 class NativeJump;
1116 
1117 class CallStubImpl {
1118 
1119   //--------------------------------------------------------------
1120   //---&lt;  Used for optimization in Compile::shorten_branches  &gt;---
1121   //--------------------------------------------------------------
1122 
1123  public:
1124   // Size of call trampoline stub.
1125   static uint size_call_trampoline() {
1126     return 0; // no call trampolines on this platform
1127   }
1128 
1129   // number of relocations needed by a call trampoline stub
1130   static uint reloc_call_trampoline() {
1131     return 0; // no call trampolines on this platform
1132   }
1133 };
1134 
1135 class HandlerImpl {
1136 
1137  public:
1138 
1139   static int emit_exception_handler(CodeBuffer &amp;cbuf);
1140   static int emit_deopt_handler(CodeBuffer&amp; cbuf);
1141 
1142   static uint size_exception_handler() {
1143     // NativeCall instruction size is the same as NativeJump.
1144     // exception handler starts out as jump and can be patched to
1145     // a call be deoptimization.  (4932387)
1146     // Note that this value is also credited (in output.cpp) to
1147     // the size of the code section.
1148     return NativeJump::instruction_size;
1149   }
1150 
1151 #ifdef _LP64
1152   static uint size_deopt_handler() {
1153     // three 5 byte instructions plus one move for unreachable address.
1154     return 15+3;
1155   }
1156 #else
1157   static uint size_deopt_handler() {
1158     // NativeCall instruction size is the same as NativeJump.
1159     // exception handler starts out as jump and can be patched to
1160     // a call be deoptimization.  (4932387)
1161     // Note that this value is also credited (in output.cpp) to
1162     // the size of the code section.
1163     return 5 + NativeJump::instruction_size; // pushl(); jmp;
1164   }
1165 #endif
1166 };
1167 
1168 %} // end source_hpp
1169 
1170 source %{
1171 
1172 #include &quot;opto/addnode.hpp&quot;
1173 
1174 // Emit exception handler code.
1175 // Stuff framesize into a register and call a VM stub routine.
1176 int HandlerImpl::emit_exception_handler(CodeBuffer&amp; cbuf) {
1177 
1178   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1179   // That&#39;s why we must use the macroassembler to generate a handler.
1180   MacroAssembler _masm(&amp;cbuf);
1181   address base = __ start_a_stub(size_exception_handler());
1182   if (base == NULL) {
1183     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1184     return 0;  // CodeBuffer::expand failed
1185   }
1186   int offset = __ offset();
1187   __ jump(RuntimeAddress(OptoRuntime::exception_blob()-&gt;entry_point()));
1188   assert(__ offset() - offset &lt;= (int) size_exception_handler(), &quot;overflow&quot;);
1189   __ end_a_stub();
1190   return offset;
1191 }
1192 
1193 // Emit deopt handler code.
1194 int HandlerImpl::emit_deopt_handler(CodeBuffer&amp; cbuf) {
1195 
1196   // Note that the code buffer&#39;s insts_mark is always relative to insts.
1197   // That&#39;s why we must use the macroassembler to generate a handler.
1198   MacroAssembler _masm(&amp;cbuf);
1199   address base = __ start_a_stub(size_deopt_handler());
1200   if (base == NULL) {
1201     ciEnv::current()-&gt;record_failure(&quot;CodeCache is full&quot;);
1202     return 0;  // CodeBuffer::expand failed
1203   }
1204   int offset = __ offset();
1205 
1206 #ifdef _LP64
1207   address the_pc = (address) __ pc();
1208   Label next;
1209   // push a &quot;the_pc&quot; on the stack without destroying any registers
1210   // as they all may be live.
1211 
1212   // push address of &quot;next&quot;
1213   __ call(next, relocInfo::none); // reloc none is fine since it is a disp32
1214   __ bind(next);
1215   // adjust it so it matches &quot;the_pc&quot;
1216   __ subptr(Address(rsp, 0), __ offset() - offset);
1217 #else
1218   InternalAddress here(__ pc());
1219   __ pushptr(here.addr());
1220 #endif
1221 
1222   __ jump(RuntimeAddress(SharedRuntime::deopt_blob()-&gt;unpack()));
1223   assert(__ offset() - offset &lt;= (int) size_deopt_handler(), &quot;overflow %d&quot;, (__ offset() - offset));
1224   __ end_a_stub();
1225   return offset;
1226 }
1227 
1228 
1229 //=============================================================================
1230 
1231   // Float masks come from different places depending on platform.
1232 #ifdef _LP64
1233   static address float_signmask()  { return StubRoutines::x86::float_sign_mask(); }
1234   static address float_signflip()  { return StubRoutines::x86::float_sign_flip(); }
1235   static address double_signmask() { return StubRoutines::x86::double_sign_mask(); }
1236   static address double_signflip() { return StubRoutines::x86::double_sign_flip(); }
1237 #else
1238   static address float_signmask()  { return (address)float_signmask_pool; }
1239   static address float_signflip()  { return (address)float_signflip_pool; }
1240   static address double_signmask() { return (address)double_signmask_pool; }
1241   static address double_signflip() { return (address)double_signflip_pool; }
1242 #endif
1243   static address vector_short_to_byte_mask() { return StubRoutines::x86::vector_short_to_byte_mask(); }
1244   static address vector_byte_perm_mask() { return StubRoutines::x86::vector_byte_perm_mask(); }
1245   static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }
1246 
1247 //=============================================================================
1248 const bool Matcher::match_rule_supported(int opcode) {
1249   if (!has_match_rule(opcode)) {
1250     return false; // no match rule present
1251   }
1252   switch (opcode) {
1253     case Op_AbsVL:
1254       if (UseAVX &lt; 3) {
1255         return false;
1256       }
1257       break;
1258     case Op_PopCountI:
1259     case Op_PopCountL:
1260       if (!UsePopCountInstruction) {
1261         return false;
1262       }
1263       break;
1264     case Op_PopCountVI:
1265       if (!UsePopCountInstruction || !VM_Version::supports_vpopcntdq()) {
1266         return false;
1267       }
1268       break;
1269     case Op_MulVI:
1270       if ((UseSSE &lt; 4) &amp;&amp; (UseAVX &lt; 1)) { // only with SSE4_1 or AVX
1271         return false;
1272       }
1273       break;
1274     case Op_MulVL:
1275     case Op_MulReductionVL:
1276       if (VM_Version::supports_avx512dq() == false) {
1277         return false;
1278       }
1279       break;
1280     case Op_AddReductionVL:
1281       if (UseAVX &lt; 3) { // only EVEX : vector connectivity becomes an issue here
1282         return false;
1283       }
1284       break;
1285     case Op_AbsVB:
1286     case Op_AbsVS:
1287     case Op_AbsVI:
1288     case Op_AddReductionVI:
1289       if (UseSSE &lt; 3 || !VM_Version::supports_ssse3()) { // requires at least SSSE3
1290         return false;
1291       }
1292       break;
1293     case Op_MulReductionVI:
1294       if (UseSSE &lt; 4) { // requires at least SSE4
1295         return false;
1296       }
1297       break;
1298     case Op_AddReductionVF:
1299     case Op_AddReductionVD:
1300     case Op_MulReductionVF:
1301     case Op_MulReductionVD:
1302       if (UseSSE &lt; 1) { // requires at least SSE
1303         return false;
1304       }
1305       break;
1306     case Op_SqrtVD:
1307     case Op_SqrtVF:
1308       if (UseAVX &lt; 1) { // enabled for AVX only
1309         return false;
1310       }
1311       break;
1312     case Op_CompareAndSwapL:
1313 #ifdef _LP64
1314     case Op_CompareAndSwapP:
1315 #endif
1316       if (!VM_Version::supports_cx8()) {
1317         return false;
1318       }
1319       break;
1320     case Op_CMoveVF:
1321     case Op_CMoveVD:
1322       if (UseAVX &lt; 1 || UseAVX &gt; 2) {
1323         return false;
1324       }
1325       break;
1326     case Op_StrIndexOf:
1327       if (!UseSSE42Intrinsics) {
1328         return false;
1329       }
1330       break;
1331     case Op_StrIndexOfChar:
1332       if (!UseSSE42Intrinsics) {
1333         return false;
1334       }
1335       break;
1336     case Op_OnSpinWait:
1337       if (VM_Version::supports_on_spin_wait() == false) {
1338         return false;
1339       }
1340       break;
1341     case Op_MulAddVS2VI:
1342     case Op_RShiftVL:
1343     case Op_AbsVD:
1344     case Op_NegVD:
1345       if (UseSSE &lt; 2) {
1346         return false;
1347       }
1348       break;
1349     case Op_MulVB:
1350     case Op_LShiftVB:
1351     case Op_RShiftVB:
1352     case Op_URShiftVB:
1353       if (UseSSE &lt; 4) {
1354         return false;
1355       }
1356       break;
1357 #ifdef _LP64
1358     case Op_MaxD:
1359     case Op_MaxF:
1360     case Op_MinD:
1361     case Op_MinF:
1362       if (UseAVX &lt; 1) { // enabled for AVX only
1363         return false;
1364       }
1365       break;
1366 #endif
1367     case Op_CacheWB:
1368     case Op_CacheWBPreSync:
1369     case Op_CacheWBPostSync:
1370       if (!VM_Version::supports_data_cache_line_flush()) {
1371         return false;
1372       }
1373       break;
1374     case Op_RoundDoubleMode:
1375       if (UseSSE &lt; 4) {
1376         return false;
1377       }
1378       break;
1379     case Op_RoundDoubleModeV:
1380       if (VM_Version::supports_avx() == false) {
1381         return false; // 128bit vroundpd is not available
1382       }
1383       break;
1384   }
1385   return true;  // Match rules are supported by default.
1386 }
1387 
1388 //------------------------------------------------------------------------
1389 
1390 // Identify extra cases that we might want to provide match rules for vector nodes and
1391 // other intrinsics guarded with vector length (vlen) and element type (bt).
1392 const bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {
1393   if (!match_rule_supported(opcode)) {
1394     return false;
1395   }
1396   // Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):
1397   //   * SSE2 supports 128bit vectors for all types;
1398   //   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;
1399   //   * AVX2 supports 256bit vectors for all types;
1400   //   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;
1401   //   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.
1402   // There&#39;s also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).
1403   // And MaxVectorSize is taken into account as well.
1404   if (!vector_size_supported(bt, vlen)) {
1405     return false;
1406   }
1407   // Special cases which require vector length follow:
1408   //   * implementation limitations
1409   //   * some 512bit vector operations on FLOAT and DOUBLE types require AVX512DQ
1410   //   * 128bit vroundpd instruction is present only in AVX1
1411   switch (opcode) {
1412     case Op_AbsVF:
1413     case Op_NegVF:
1414       if ((vlen == 16) &amp;&amp; (VM_Version::supports_avx512dq() == false)) {
1415         return false; // 512bit vandps and vxorps are not available
1416       }
1417       break;
1418     case Op_AbsVD:
1419     case Op_NegVD:
1420       if ((vlen == 8) &amp;&amp; (VM_Version::supports_avx512dq() == false)) {
1421         return false; // 512bit vandpd and vxorpd are not available
1422       }
1423       break;
1424     case Op_CMoveVF:
1425       if (vlen != 8) {
1426         return false; // implementation limitation (only vcmov8F_reg is present)
1427       }
1428       break;
1429     case Op_CMoveVD:
1430       if (vlen != 4) {
1431         return false; // implementation limitation (only vcmov4D_reg is present)
1432       }
1433       break;
1434   }
1435   return true;  // Per default match rules are supported.
1436 }
1437 
1438 // x86 supports generic vector operands: vec and legVec.
1439 const bool Matcher::supports_generic_vector_operands = true;
1440 
1441 MachOper* Matcher::specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp) {
1442   assert(Matcher::is_generic_vector(generic_opnd), &quot;not generic&quot;);
1443   bool legacy = (generic_opnd-&gt;opcode() == LEGVEC);
1444   if (!VM_Version::supports_avx512vlbwdq() &amp;&amp; // KNL
1445       is_temp &amp;&amp; !legacy &amp;&amp; (ideal_reg == Op_VecZ)) {
1446     // Conservatively specialize 512bit vec TEMP operands to legVecZ (zmm0-15) on KNL.
1447     return new legVecZOper();
1448   }
1449   if (legacy) {
1450     switch (ideal_reg) {
1451       case Op_VecS: return new legVecSOper();
1452       case Op_VecD: return new legVecDOper();
1453       case Op_VecX: return new legVecXOper();
1454       case Op_VecY: return new legVecYOper();
1455       case Op_VecZ: return new legVecZOper();
1456     }
1457   } else {
1458     switch (ideal_reg) {
1459       case Op_VecS: return new vecSOper();
1460       case Op_VecD: return new vecDOper();
1461       case Op_VecX: return new vecXOper();
1462       case Op_VecY: return new vecYOper();
1463       case Op_VecZ: return new vecZOper();
1464     }
1465   }
1466   ShouldNotReachHere();
1467   return NULL;
1468 }
1469 
1470 bool Matcher::is_generic_reg2reg_move(MachNode* m) {
1471   switch (m-&gt;rule()) {
1472     case MoveVec2Leg_rule:
1473     case MoveLeg2Vec_rule:
1474       return true;
1475     default:
1476       return false;
1477   }
1478 }
1479 
1480 bool Matcher::is_generic_vector(MachOper* opnd) {
1481   switch (opnd-&gt;opcode()) {
1482     case VEC:
1483     case LEGVEC:
1484       return true;
1485     default:
1486       return false;
1487   }
1488 }
1489 
1490 //------------------------------------------------------------------------
1491 
1492 const bool Matcher::has_predicated_vectors(void) {
1493   bool ret_value = false;
1494   if (UseAVX &gt; 2) {
1495     ret_value = VM_Version::supports_avx512vl();
1496   }
1497 
1498   return ret_value;
1499 }
1500 
1501 const int Matcher::float_pressure(int default_pressure_threshold) {
1502   int float_pressure_threshold = default_pressure_threshold;
1503 #ifdef _LP64
1504   if (UseAVX &gt; 2) {
1505     // Increase pressure threshold on machines with AVX3 which have
1506     // 2x more XMM registers.
1507     float_pressure_threshold = default_pressure_threshold * 2;
1508   }
1509 #endif
1510   return float_pressure_threshold;
1511 }
1512 
1513 // Max vector size in bytes. 0 if not supported.
1514 const int Matcher::vector_width_in_bytes(BasicType bt) {
1515   assert(is_java_primitive(bt), &quot;only primitive type vectors&quot;);
1516   if (UseSSE &lt; 2) return 0;
1517   // SSE2 supports 128bit vectors for all types.
1518   // AVX2 supports 256bit vectors for all types.
1519   // AVX2/EVEX supports 512bit vectors for all types.
1520   int size = (UseAVX &gt; 1) ? (1 &lt;&lt; UseAVX) * 8 : 16;
1521   // AVX1 supports 256bit vectors only for FLOAT and DOUBLE.
1522   if (UseAVX &gt; 0 &amp;&amp; (bt == T_FLOAT || bt == T_DOUBLE))
1523     size = (UseAVX &gt; 2) ? 64 : 32;
1524   if (UseAVX &gt; 2 &amp;&amp; (bt == T_BYTE || bt == T_SHORT || bt == T_CHAR))
1525     size = (VM_Version::supports_avx512bw()) ? 64 : 32;
1526   // Use flag to limit vector size.
1527   size = MIN2(size,(int)MaxVectorSize);
1528   // Minimum 2 values in vector (or 4 for bytes).
1529   switch (bt) {
1530   case T_DOUBLE:
1531   case T_LONG:
1532     if (size &lt; 16) return 0;
1533     break;
1534   case T_FLOAT:
1535   case T_INT:
1536     if (size &lt; 8) return 0;
1537     break;
1538   case T_BOOLEAN:
1539     if (size &lt; 4) return 0;
1540     break;
1541   case T_CHAR:
1542     if (size &lt; 4) return 0;
1543     break;
1544   case T_BYTE:
1545     if (size &lt; 4) return 0;
1546     break;
1547   case T_SHORT:
1548     if (size &lt; 4) return 0;
1549     break;
1550   default:
1551     ShouldNotReachHere();
1552   }
1553   return size;
1554 }
1555 
1556 // Limits on vector size (number of elements) loaded into vector.
1557 const int Matcher::max_vector_size(const BasicType bt) {
1558   return vector_width_in_bytes(bt)/type2aelembytes(bt);
1559 }
1560 const int Matcher::min_vector_size(const BasicType bt) {
1561   int max_size = max_vector_size(bt);
1562   // Min size which can be loaded into vector is 4 bytes.
1563   int size = (type2aelembytes(bt) == 1) ? 4 : 2;
1564   return MIN2(size,max_size);
1565 }
1566 
1567 // Vector ideal reg corresponding to specified size in bytes
1568 const uint Matcher::vector_ideal_reg(int size) {
1569   assert(MaxVectorSize &gt;= size, &quot;&quot;);
1570   switch(size) {
1571     case  4: return Op_VecS;
1572     case  8: return Op_VecD;
1573     case 16: return Op_VecX;
1574     case 32: return Op_VecY;
1575     case 64: return Op_VecZ;
1576   }
1577   ShouldNotReachHere();
1578   return 0;
1579 }
1580 
1581 // Only lowest bits of xmm reg are used for vector shift count.
1582 const uint Matcher::vector_shift_count_ideal_reg(int size) {
1583   return Op_VecS;
1584 }
1585 
1586 // x86 supports misaligned vectors store/load.
1587 const bool Matcher::misaligned_vectors_ok() {
1588   return true;
1589 }
1590 
1591 // x86 AES instructions are compatible with SunJCE expanded
1592 // keys, hence we do not need to pass the original key to stubs
1593 const bool Matcher::pass_original_key_for_aes() {
1594   return false;
1595 }
1596 
1597 
1598 const bool Matcher::convi2l_type_required = true;
1599 
1600 // Check for shift by small constant as well
1601 static bool clone_shift(Node* shift, Matcher* matcher, Matcher::MStack&amp; mstack, VectorSet&amp; address_visited) {
1602   if (shift-&gt;Opcode() == Op_LShiftX &amp;&amp; shift-&gt;in(2)-&gt;is_Con() &amp;&amp;
1603       shift-&gt;in(2)-&gt;get_int() &lt;= 3 &amp;&amp;
1604       // Are there other uses besides address expressions?
1605       !matcher-&gt;is_visited(shift)) {
1606     address_visited.set(shift-&gt;_idx); // Flag as address_visited
1607     mstack.push(shift-&gt;in(2), Matcher::Visit);
1608     Node *conv = shift-&gt;in(1);
1609 #ifdef _LP64
1610     // Allow Matcher to match the rule which bypass
1611     // ConvI2L operation for an array index on LP64
1612     // if the index value is positive.
1613     if (conv-&gt;Opcode() == Op_ConvI2L &amp;&amp;
1614         conv-&gt;as_Type()-&gt;type()-&gt;is_long()-&gt;_lo &gt;= 0 &amp;&amp;
1615         // Are there other uses besides address expressions?
1616         !matcher-&gt;is_visited(conv)) {
1617       address_visited.set(conv-&gt;_idx); // Flag as address_visited
1618       mstack.push(conv-&gt;in(1), Matcher::Pre_Visit);
1619     } else
1620 #endif
1621       mstack.push(conv, Matcher::Pre_Visit);
1622     return true;
1623   }
1624   return false;
1625 }
1626 
1627 // Should the Matcher clone shifts on addressing modes, expecting them
1628 // to be subsumed into complex addressing expressions or compute them
1629 // into registers?
1630 bool Matcher::clone_address_expressions(AddPNode* m, Matcher::MStack&amp; mstack, VectorSet&amp; address_visited) {
1631   Node *off = m-&gt;in(AddPNode::Offset);
1632   if (off-&gt;is_Con()) {
1633     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
1634     Node *adr = m-&gt;in(AddPNode::Address);
1635 
1636     // Intel can handle 2 adds in addressing mode
1637     // AtomicAdd is not an addressing expression.
1638     // Cheap to find it by looking for screwy base.
1639     if (adr-&gt;is_AddP() &amp;&amp;
1640         !adr-&gt;in(AddPNode::Base)-&gt;is_top() &amp;&amp;
1641         LP64_ONLY( off-&gt;get_long() == (int) (off-&gt;get_long()) &amp;&amp; ) // immL32
1642         // Are there other uses besides address expressions?
1643         !is_visited(adr)) {
1644       address_visited.set(adr-&gt;_idx); // Flag as address_visited
1645       Node *shift = adr-&gt;in(AddPNode::Offset);
1646       if (!clone_shift(shift, this, mstack, address_visited)) {
1647         mstack.push(shift, Pre_Visit);
1648       }
1649       mstack.push(adr-&gt;in(AddPNode::Address), Pre_Visit);
1650       mstack.push(adr-&gt;in(AddPNode::Base), Pre_Visit);
1651     } else {
1652       mstack.push(adr, Pre_Visit);
1653     }
1654 
1655     // Clone X+offset as it also folds into most addressing expressions
1656     mstack.push(off, Visit);
1657     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1658     return true;
1659   } else if (clone_shift(off, this, mstack, address_visited)) {
1660     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
1661     mstack.push(m-&gt;in(AddPNode::Address), Pre_Visit);
1662     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
1663     return true;
1664   }
1665   return false;
1666 }
1667 
1668 void Compile::reshape_address(AddPNode* addp) {
1669 }
1670 
1671 static inline uint vector_length(const MachNode* n) {
1672   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1673   return vt-&gt;length();
1674 }
1675 
1676 static inline uint vector_length_in_bytes(const MachNode* n) {
1677   const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
1678   return vt-&gt;length_in_bytes();
1679 }
1680 
1681 static inline uint vector_length_in_bytes(const MachNode* use, MachOper* opnd) {
1682   uint def_idx = use-&gt;operand_index(opnd);
1683   Node* def = use-&gt;in(def_idx);
1684   return def-&gt;bottom_type()-&gt;is_vect()-&gt;length_in_bytes();
1685 }
1686 
1687 static inline Assembler::AvxVectorLen vector_length_encoding(const MachNode* n) {
1688   switch(vector_length_in_bytes(n)) {
1689     case  4: // fall-through
1690     case  8: // fall-through
1691     case 16: return Assembler::AVX_128bit;
1692     case 32: return Assembler::AVX_256bit;
1693     case 64: return Assembler::AVX_512bit;
1694 
1695     default: {
1696       ShouldNotReachHere();
1697       return Assembler::AVX_NoVec;
1698     }
1699   }
1700 }
1701 
1702 // Helper methods for MachSpillCopyNode::implementation().
1703 static int vec_mov_helper(CodeBuffer *cbuf, bool do_size, int src_lo, int dst_lo,
1704                           int src_hi, int dst_hi, uint ireg, outputStream* st) {
1705   // In 64-bit VM size calculation is very complex. Emitting instructions
1706   // into scratch buffer is used to get size in 64-bit VM.
1707   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1708   assert(ireg == Op_VecS || // 32bit vector
1709          (src_lo &amp; 1) == 0 &amp;&amp; (src_lo + 1) == src_hi &amp;&amp;
1710          (dst_lo &amp; 1) == 0 &amp;&amp; (dst_lo + 1) == dst_hi,
1711          &quot;no non-adjacent vector moves&quot; );
1712   if (cbuf) {
1713     MacroAssembler _masm(cbuf);
1714     int offset = __ offset();
1715     switch (ireg) {
1716     case Op_VecS: // copy whole register
1717     case Op_VecD:
1718     case Op_VecX:
1719 #ifndef _LP64
1720       __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1721 #else
1722       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1723         __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1724       } else {
1725         __ vextractf32x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);
1726      }
1727 #endif
1728       break;
1729     case Op_VecY:
1730 #ifndef _LP64
1731       __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1732 #else
1733       if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1734         __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));
1735       } else {
1736         __ vextractf64x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);
1737      }
1738 #endif
1739       break;
1740     case Op_VecZ:
1741       __ evmovdquq(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 2);
1742       break;
1743     default:
1744       ShouldNotReachHere();
1745     }
1746     int size = __ offset() - offset;
1747 #ifdef ASSERT
1748     // VEX_2bytes prefix is used if UseAVX &gt; 0, so it takes the same 2 bytes as SIMD prefix.
1749     assert(!do_size || size == 4, &quot;incorrect size calculattion&quot;);
1750 #endif
1751     return size;
1752 #ifndef PRODUCT
1753   } else if (!do_size) {
1754     switch (ireg) {
1755     case Op_VecS:
1756     case Op_VecD:
1757     case Op_VecX:
1758       st-&gt;print(&quot;movdqu  %s,%s\t# spill&quot;,Matcher::regName[dst_lo],Matcher::regName[src_lo]);
1759       break;
1760     case Op_VecY:
1761     case Op_VecZ:
1762       st-&gt;print(&quot;vmovdqu %s,%s\t# spill&quot;,Matcher::regName[dst_lo],Matcher::regName[src_lo]);
1763       break;
1764     default:
1765       ShouldNotReachHere();
1766     }
1767 #endif
1768   }
1769   // VEX_2bytes prefix is used if UseAVX &gt; 0, and it takes the same 2 bytes as SIMD prefix.
1770   return (UseAVX &gt; 2) ? 6 : 4;
1771 }
1772 
1773 int vec_spill_helper(CodeBuffer *cbuf, bool do_size, bool is_load,
1774                      int stack_offset, int reg, uint ireg, outputStream* st) {
1775   // In 64-bit VM size calculation is very complex. Emitting instructions
1776   // into scratch buffer is used to get size in 64-bit VM.
1777   LP64_ONLY( assert(!do_size, &quot;this method calculates size only for 32-bit VM&quot;); )
1778   if (cbuf) {
1779     MacroAssembler _masm(cbuf);
1780     int offset = __ offset();
1781     if (is_load) {
1782       switch (ireg) {
1783       case Op_VecS:
1784         __ movdl(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1785         break;
1786       case Op_VecD:
1787         __ movq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1788         break;
1789       case Op_VecX:
1790 #ifndef _LP64
1791         __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1792 #else
1793         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1794           __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1795         } else {
1796           __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);
1797           __ vinsertf32x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);
1798         }
1799 #endif
1800         break;
1801       case Op_VecY:
1802 #ifndef _LP64
1803         __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1804 #else
1805         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1806           __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));
1807         } else {
1808           __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);
1809           __ vinsertf64x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);
1810         }
1811 #endif
1812         break;
1813       case Op_VecZ:
1814         __ evmovdquq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset), 2);
1815         break;
1816       default:
1817         ShouldNotReachHere();
1818       }
1819     } else { // store
1820       switch (ireg) {
1821       case Op_VecS:
1822         __ movdl(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1823         break;
1824       case Op_VecD:
1825         __ movq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1826         break;
1827       case Op_VecX:
1828 #ifndef _LP64
1829         __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1830 #else
1831         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1832           __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1833         }
1834         else {
1835           __ vextractf32x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);
1836         }
1837 #endif
1838         break;
1839       case Op_VecY:
1840 #ifndef _LP64
1841         __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1842 #else
1843         if ((UseAVX &lt; 3) || VM_Version::supports_avx512vl()) {
1844           __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));
1845         }
1846         else {
1847           __ vextractf64x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);
1848         }
1849 #endif
1850         break;
1851       case Op_VecZ:
1852         __ evmovdquq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 2);
1853         break;
1854       default:
1855         ShouldNotReachHere();
1856       }
1857     }
1858     int size = __ offset() - offset;
1859 #ifdef ASSERT
1860     int offset_size = (stack_offset == 0) ? 0 : ((stack_offset &lt; 0x80) ? 1 : (UseAVX &gt; 2) ? 6 : 4);
1861     // VEX_2bytes prefix is used if UseAVX &gt; 0, so it takes the same 2 bytes as SIMD prefix.
1862     assert(!do_size || size == (5+offset_size), &quot;incorrect size calculattion&quot;);
1863 #endif
1864     return size;
1865 #ifndef PRODUCT
1866   } else if (!do_size) {
1867     if (is_load) {
1868       switch (ireg) {
1869       case Op_VecS:
1870         st-&gt;print(&quot;movd    %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
1871         break;
1872       case Op_VecD:
1873         st-&gt;print(&quot;movq    %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
1874         break;
1875        case Op_VecX:
1876         st-&gt;print(&quot;movdqu  %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
1877         break;
1878       case Op_VecY:
1879       case Op_VecZ:
1880         st-&gt;print(&quot;vmovdqu %s,[rsp + %d]\t# spill&quot;, Matcher::regName[reg], stack_offset);
1881         break;
1882       default:
1883         ShouldNotReachHere();
1884       }
1885     } else { // store
1886       switch (ireg) {
1887       case Op_VecS:
1888         st-&gt;print(&quot;movd    [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
1889         break;
1890       case Op_VecD:
1891         st-&gt;print(&quot;movq    [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
1892         break;
1893        case Op_VecX:
1894         st-&gt;print(&quot;movdqu  [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
1895         break;
1896       case Op_VecY:
1897       case Op_VecZ:
1898         st-&gt;print(&quot;vmovdqu [rsp + %d],%s\t# spill&quot;, stack_offset, Matcher::regName[reg]);
1899         break;
1900       default:
1901         ShouldNotReachHere();
1902       }
1903     }
1904 #endif
1905   }
1906   bool is_single_byte = false;
1907   int vec_len = 0;
1908   if ((UseAVX &gt; 2) &amp;&amp; (stack_offset != 0)) {
1909     int tuple_type = Assembler::EVEX_FVM;
1910     int input_size = Assembler::EVEX_32bit;
1911     switch (ireg) {
1912     case Op_VecS:
1913       tuple_type = Assembler::EVEX_T1S;
1914       break;
1915     case Op_VecD:
1916       tuple_type = Assembler::EVEX_T1S;
1917       input_size = Assembler::EVEX_64bit;
1918       break;
1919     case Op_VecX:
1920       break;
1921     case Op_VecY:
1922       vec_len = 1;
1923       break;
1924     case Op_VecZ:
1925       vec_len = 2;
1926       break;
1927     }
1928     is_single_byte = Assembler::query_compressed_disp_byte(stack_offset, true, vec_len, tuple_type, input_size, 0);
1929   }
1930   int offset_size = 0;
1931   int size = 5;
1932   if (UseAVX &gt; 2 ) {
1933     if (VM_Version::supports_avx512novl() &amp;&amp; (vec_len == 2)) {
1934       offset_size = (stack_offset == 0) ? 0 : ((is_single_byte) ? 1 : 4);
1935       size += 2; // Need an additional two bytes for EVEX encoding
1936     } else if (VM_Version::supports_avx512novl() &amp;&amp; (vec_len &lt; 2)) {
1937       offset_size = (stack_offset == 0) ? 0 : ((stack_offset &lt;= 127) ? 1 : 4);
1938     } else {
1939       offset_size = (stack_offset == 0) ? 0 : ((is_single_byte) ? 1 : 4);
1940       size += 2; // Need an additional two bytes for EVEX encodding
1941     }
1942   } else {
1943     offset_size = (stack_offset == 0) ? 0 : ((stack_offset &lt;= 127) ? 1 : 4);
1944   }
1945   // VEX_2bytes prefix is used if UseAVX &gt; 0, so it takes the same 2 bytes as SIMD prefix.
1946   return size+offset_size;
1947 }
1948 
1949 static inline jint replicate4_imm(int con, int width) {
1950   // Load a constant of &quot;width&quot; (in bytes) and replicate it to fill 32bit.
1951   assert(width == 1 || width == 2, &quot;only byte or short types here&quot;);
1952   int bit_width = width * 8;
1953   jint val = con;
1954   val &amp;= (1 &lt;&lt; bit_width) - 1;  // mask off sign bits
1955   while(bit_width &lt; 32) {
1956     val |= (val &lt;&lt; bit_width);
1957     bit_width &lt;&lt;= 1;
1958   }
1959   return val;
1960 }
1961 
1962 static inline jlong replicate8_imm(int con, int width) {
1963   // Load a constant of &quot;width&quot; (in bytes) and replicate it to fill 64bit.
1964   assert(width == 1 || width == 2 || width == 4, &quot;only byte, short or int types here&quot;);
1965   int bit_width = width * 8;
1966   jlong val = con;
1967   val &amp;= (((jlong) 1) &lt;&lt; bit_width) - 1;  // mask off sign bits
1968   while(bit_width &lt; 64) {
1969     val |= (val &lt;&lt; bit_width);
1970     bit_width &lt;&lt;= 1;
1971   }
1972   return val;
1973 }
1974 
1975 #ifndef PRODUCT
1976   void MachNopNode::format(PhaseRegAlloc*, outputStream* st) const {
1977     st-&gt;print(&quot;nop \t# %d bytes pad for loops and calls&quot;, _count);
1978   }
1979 #endif
1980 
1981   void MachNopNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc*) const {
1982     MacroAssembler _masm(&amp;cbuf);
1983     __ nop(_count);
1984   }
1985 
1986   uint MachNopNode::size(PhaseRegAlloc*) const {
1987     return _count;
1988   }
1989 
1990 #ifndef PRODUCT
1991   void MachBreakpointNode::format(PhaseRegAlloc*, outputStream* st) const {
1992     st-&gt;print(&quot;# breakpoint&quot;);
1993   }
1994 #endif
1995 
1996   void MachBreakpointNode::emit(CodeBuffer &amp;cbuf, PhaseRegAlloc* ra_) const {
1997     MacroAssembler _masm(&amp;cbuf);
1998     __ int3();
1999   }
2000 
2001   uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {
2002     return MachNode::size(ra_);
2003   }
2004 
2005 %}
2006 
2007 encode %{
2008 
2009   enc_class call_epilog %{
2010     if (VerifyStackAtCalls) {
2011       // Check that stack depth is unchanged: find majik cookie on stack
2012       int framesize = ra_-&gt;reg2offset_unchecked(OptoReg::add(ra_-&gt;_matcher._old_SP, -3*VMRegImpl::slots_per_word));
2013       MacroAssembler _masm(&amp;cbuf);
2014       Label L;
2015       __ cmpptr(Address(rsp, framesize), (int32_t)0xbadb100d);
2016       __ jccb(Assembler::equal, L);
2017       // Die if stack mismatch
2018       __ int3();
2019       __ bind(L);
2020     }
2021   %}
2022 
2023 %}
2024 
2025 
2026 //----------OPERANDS-----------------------------------------------------------
2027 // Operand definitions must precede instruction definitions for correct parsing
2028 // in the ADLC because operands constitute user defined types which are used in
2029 // instruction definitions.
2030 
2031 // Vectors
2032 
2033 // Dummy generic vector class. Should be used for all vector operands.
2034 // Replaced with vec[SDXYZ] during post-selection pass.
2035 operand vec() %{
2036   constraint(ALLOC_IN_RC(dynamic));
2037   match(VecX);
2038   match(VecY);
2039   match(VecZ);
2040   match(VecS);
2041   match(VecD);
2042 
2043   format %{ %}
2044   interface(REG_INTER);
2045 %}
2046 
2047 // Dummy generic legacy vector class. Should be used for all legacy vector operands.
2048 // Replaced with legVec[SDXYZ] during post-selection cleanup.
2049 // Note: legacy register class is used to avoid extra (unneeded in 32-bit VM)
2050 // runtime code generation via reg_class_dynamic.
2051 operand legVec() %{
2052   constraint(ALLOC_IN_RC(dynamic));
2053   match(VecX);
2054   match(VecY);
2055   match(VecZ);
2056   match(VecS);
2057   match(VecD);
2058 
2059   format %{ %}
2060   interface(REG_INTER);
2061 %}
2062 
2063 // Replaces vec during post-selection cleanup. See above.
2064 operand vecS() %{
2065   constraint(ALLOC_IN_RC(vectors_reg_vlbwdq));
2066   match(VecS);
2067 
2068   format %{ %}
2069   interface(REG_INTER);
2070 %}
2071 
2072 // Replaces legVec during post-selection cleanup. See above.
2073 operand legVecS() %{
2074   constraint(ALLOC_IN_RC(vectors_reg_legacy));
2075   match(VecS);
2076 
2077   format %{ %}
2078   interface(REG_INTER);
2079 %}
2080 
2081 // Replaces vec during post-selection cleanup. See above.
2082 operand vecD() %{
2083   constraint(ALLOC_IN_RC(vectord_reg_vlbwdq));
2084   match(VecD);
2085 
2086   format %{ %}
2087   interface(REG_INTER);
2088 %}
2089 
2090 // Replaces legVec during post-selection cleanup. See above.
2091 operand legVecD() %{
2092   constraint(ALLOC_IN_RC(vectord_reg_legacy));
2093   match(VecD);
2094 
2095   format %{ %}
2096   interface(REG_INTER);
2097 %}
2098 
2099 // Replaces vec during post-selection cleanup. See above.
2100 operand vecX() %{
2101   constraint(ALLOC_IN_RC(vectorx_reg_vlbwdq));
2102   match(VecX);
2103 
2104   format %{ %}
2105   interface(REG_INTER);
2106 %}
2107 
2108 // Replaces legVec during post-selection cleanup. See above.
2109 operand legVecX() %{
2110   constraint(ALLOC_IN_RC(vectorx_reg_legacy));
2111   match(VecX);
2112 
2113   format %{ %}
2114   interface(REG_INTER);
2115 %}
2116 
2117 // Replaces vec during post-selection cleanup. See above.
2118 operand vecY() %{
2119   constraint(ALLOC_IN_RC(vectory_reg_vlbwdq));
2120   match(VecY);
2121 
2122   format %{ %}
2123   interface(REG_INTER);
2124 %}
2125 
2126 // Replaces legVec during post-selection cleanup. See above.
2127 operand legVecY() %{
2128   constraint(ALLOC_IN_RC(vectory_reg_legacy));
2129   match(VecY);
2130 
2131   format %{ %}
2132   interface(REG_INTER);
2133 %}
2134 
2135 // Replaces vec during post-selection cleanup. See above.
2136 operand vecZ() %{
2137   constraint(ALLOC_IN_RC(vectorz_reg));
2138   match(VecZ);
2139 
2140   format %{ %}
2141   interface(REG_INTER);
2142 %}
2143 
2144 // Replaces legVec during post-selection cleanup. See above.
2145 operand legVecZ() %{
2146   constraint(ALLOC_IN_RC(vectorz_reg_legacy));
2147   match(VecZ);
2148 
2149   format %{ %}
2150   interface(REG_INTER);
2151 %}
2152 
2153 // Comparison Code for FP conditional move
2154 operand cmpOp_vcmppd() %{
2155   match(Bool);
2156 
2157   predicate(n-&gt;as_Bool()-&gt;_test._test != BoolTest::overflow &amp;&amp;
2158             n-&gt;as_Bool()-&gt;_test._test != BoolTest::no_overflow);
2159   format %{ &quot;&quot; %}
2160   interface(COND_INTER) %{
2161     equal        (0x0, &quot;eq&quot;);
2162     less         (0x1, &quot;lt&quot;);
2163     less_equal   (0x2, &quot;le&quot;);
2164     not_equal    (0xC, &quot;ne&quot;);
2165     greater_equal(0xD, &quot;ge&quot;);
2166     greater      (0xE, &quot;gt&quot;);
2167     //TODO cannot compile (adlc breaks) without two next lines with error:
2168     // x86_64.ad(13987) Syntax Error: :In operand cmpOp_vcmppd: Do not support this encode constant: &#39; %{
2169     // equal&#39; for overflow.
2170     overflow     (0x20, &quot;o&quot;);  // not really supported by the instruction
2171     no_overflow  (0x21, &quot;no&quot;); // not really supported by the instruction
2172   %}
2173 %}
2174 
2175 
2176 // INSTRUCTIONS -- Platform independent definitions (same for 32- and 64-bit)
2177 
2178 // ============================================================================
2179 
2180 instruct ShouldNotReachHere() %{
2181   match(Halt);
2182   format %{ &quot;ud2\t# ShouldNotReachHere&quot; %}
2183   ins_encode %{
2184     __ stop(_halt_reason);
2185   %}
2186   ins_pipe(pipe_slow);
2187 %}
2188 
2189 // =================================EVEX special===============================
2190 
2191 instruct setMask(rRegI dst, rRegI src) %{
2192   predicate(Matcher::has_predicated_vectors());
2193   match(Set dst (SetVectMaskI  src));
2194   effect(TEMP dst);
2195   format %{ &quot;setvectmask   $dst, $src&quot; %}
2196   ins_encode %{
2197     __ setvectmask($dst$$Register, $src$$Register);
2198   %}
2199   ins_pipe(pipe_slow);
2200 %}
2201 
2202 // ============================================================================
2203 
2204 instruct addF_reg(regF dst, regF src) %{
2205   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2206   match(Set dst (AddF dst src));
2207 
2208   format %{ &quot;addss   $dst, $src&quot; %}
2209   ins_cost(150);
2210   ins_encode %{
2211     __ addss($dst$$XMMRegister, $src$$XMMRegister);
2212   %}
2213   ins_pipe(pipe_slow);
2214 %}
2215 
2216 instruct addF_mem(regF dst, memory src) %{
2217   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2218   match(Set dst (AddF dst (LoadF src)));
2219 
2220   format %{ &quot;addss   $dst, $src&quot; %}
2221   ins_cost(150);
2222   ins_encode %{
2223     __ addss($dst$$XMMRegister, $src$$Address);
2224   %}
2225   ins_pipe(pipe_slow);
2226 %}
2227 
2228 instruct addF_imm(regF dst, immF con) %{
2229   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2230   match(Set dst (AddF dst con));
2231   format %{ &quot;addss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2232   ins_cost(150);
2233   ins_encode %{
2234     __ addss($dst$$XMMRegister, $constantaddress($con));
2235   %}
2236   ins_pipe(pipe_slow);
2237 %}
2238 
2239 instruct addF_reg_reg(regF dst, regF src1, regF src2) %{
2240   predicate(UseAVX &gt; 0);
2241   match(Set dst (AddF src1 src2));
2242 
2243   format %{ &quot;vaddss  $dst, $src1, $src2&quot; %}
2244   ins_cost(150);
2245   ins_encode %{
2246     __ vaddss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2247   %}
2248   ins_pipe(pipe_slow);
2249 %}
2250 
2251 instruct addF_reg_mem(regF dst, regF src1, memory src2) %{
2252   predicate(UseAVX &gt; 0);
2253   match(Set dst (AddF src1 (LoadF src2)));
2254 
2255   format %{ &quot;vaddss  $dst, $src1, $src2&quot; %}
2256   ins_cost(150);
2257   ins_encode %{
2258     __ vaddss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2259   %}
2260   ins_pipe(pipe_slow);
2261 %}
2262 
2263 instruct addF_reg_imm(regF dst, regF src, immF con) %{
2264   predicate(UseAVX &gt; 0);
2265   match(Set dst (AddF src con));
2266 
2267   format %{ &quot;vaddss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2268   ins_cost(150);
2269   ins_encode %{
2270     __ vaddss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2271   %}
2272   ins_pipe(pipe_slow);
2273 %}
2274 
2275 instruct addD_reg(regD dst, regD src) %{
2276   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2277   match(Set dst (AddD dst src));
2278 
2279   format %{ &quot;addsd   $dst, $src&quot; %}
2280   ins_cost(150);
2281   ins_encode %{
2282     __ addsd($dst$$XMMRegister, $src$$XMMRegister);
2283   %}
2284   ins_pipe(pipe_slow);
2285 %}
2286 
2287 instruct addD_mem(regD dst, memory src) %{
2288   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2289   match(Set dst (AddD dst (LoadD src)));
2290 
2291   format %{ &quot;addsd   $dst, $src&quot; %}
2292   ins_cost(150);
2293   ins_encode %{
2294     __ addsd($dst$$XMMRegister, $src$$Address);
2295   %}
2296   ins_pipe(pipe_slow);
2297 %}
2298 
2299 instruct addD_imm(regD dst, immD con) %{
2300   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2301   match(Set dst (AddD dst con));
2302   format %{ &quot;addsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2303   ins_cost(150);
2304   ins_encode %{
2305     __ addsd($dst$$XMMRegister, $constantaddress($con));
2306   %}
2307   ins_pipe(pipe_slow);
2308 %}
2309 
2310 instruct addD_reg_reg(regD dst, regD src1, regD src2) %{
2311   predicate(UseAVX &gt; 0);
2312   match(Set dst (AddD src1 src2));
2313 
2314   format %{ &quot;vaddsd  $dst, $src1, $src2&quot; %}
2315   ins_cost(150);
2316   ins_encode %{
2317     __ vaddsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2318   %}
2319   ins_pipe(pipe_slow);
2320 %}
2321 
2322 instruct addD_reg_mem(regD dst, regD src1, memory src2) %{
2323   predicate(UseAVX &gt; 0);
2324   match(Set dst (AddD src1 (LoadD src2)));
2325 
2326   format %{ &quot;vaddsd  $dst, $src1, $src2&quot; %}
2327   ins_cost(150);
2328   ins_encode %{
2329     __ vaddsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2330   %}
2331   ins_pipe(pipe_slow);
2332 %}
2333 
2334 instruct addD_reg_imm(regD dst, regD src, immD con) %{
2335   predicate(UseAVX &gt; 0);
2336   match(Set dst (AddD src con));
2337 
2338   format %{ &quot;vaddsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2339   ins_cost(150);
2340   ins_encode %{
2341     __ vaddsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2342   %}
2343   ins_pipe(pipe_slow);
2344 %}
2345 
2346 instruct subF_reg(regF dst, regF src) %{
2347   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2348   match(Set dst (SubF dst src));
2349 
2350   format %{ &quot;subss   $dst, $src&quot; %}
2351   ins_cost(150);
2352   ins_encode %{
2353     __ subss($dst$$XMMRegister, $src$$XMMRegister);
2354   %}
2355   ins_pipe(pipe_slow);
2356 %}
2357 
2358 instruct subF_mem(regF dst, memory src) %{
2359   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2360   match(Set dst (SubF dst (LoadF src)));
2361 
2362   format %{ &quot;subss   $dst, $src&quot; %}
2363   ins_cost(150);
2364   ins_encode %{
2365     __ subss($dst$$XMMRegister, $src$$Address);
2366   %}
2367   ins_pipe(pipe_slow);
2368 %}
2369 
2370 instruct subF_imm(regF dst, immF con) %{
2371   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2372   match(Set dst (SubF dst con));
2373   format %{ &quot;subss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2374   ins_cost(150);
2375   ins_encode %{
2376     __ subss($dst$$XMMRegister, $constantaddress($con));
2377   %}
2378   ins_pipe(pipe_slow);
2379 %}
2380 
2381 instruct subF_reg_reg(regF dst, regF src1, regF src2) %{
2382   predicate(UseAVX &gt; 0);
2383   match(Set dst (SubF src1 src2));
2384 
2385   format %{ &quot;vsubss  $dst, $src1, $src2&quot; %}
2386   ins_cost(150);
2387   ins_encode %{
2388     __ vsubss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2389   %}
2390   ins_pipe(pipe_slow);
2391 %}
2392 
2393 instruct subF_reg_mem(regF dst, regF src1, memory src2) %{
2394   predicate(UseAVX &gt; 0);
2395   match(Set dst (SubF src1 (LoadF src2)));
2396 
2397   format %{ &quot;vsubss  $dst, $src1, $src2&quot; %}
2398   ins_cost(150);
2399   ins_encode %{
2400     __ vsubss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2401   %}
2402   ins_pipe(pipe_slow);
2403 %}
2404 
2405 instruct subF_reg_imm(regF dst, regF src, immF con) %{
2406   predicate(UseAVX &gt; 0);
2407   match(Set dst (SubF src con));
2408 
2409   format %{ &quot;vsubss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2410   ins_cost(150);
2411   ins_encode %{
2412     __ vsubss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2413   %}
2414   ins_pipe(pipe_slow);
2415 %}
2416 
2417 instruct subD_reg(regD dst, regD src) %{
2418   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2419   match(Set dst (SubD dst src));
2420 
2421   format %{ &quot;subsd   $dst, $src&quot; %}
2422   ins_cost(150);
2423   ins_encode %{
2424     __ subsd($dst$$XMMRegister, $src$$XMMRegister);
2425   %}
2426   ins_pipe(pipe_slow);
2427 %}
2428 
2429 instruct subD_mem(regD dst, memory src) %{
2430   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2431   match(Set dst (SubD dst (LoadD src)));
2432 
2433   format %{ &quot;subsd   $dst, $src&quot; %}
2434   ins_cost(150);
2435   ins_encode %{
2436     __ subsd($dst$$XMMRegister, $src$$Address);
2437   %}
2438   ins_pipe(pipe_slow);
2439 %}
2440 
2441 instruct subD_imm(regD dst, immD con) %{
2442   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2443   match(Set dst (SubD dst con));
2444   format %{ &quot;subsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2445   ins_cost(150);
2446   ins_encode %{
2447     __ subsd($dst$$XMMRegister, $constantaddress($con));
2448   %}
2449   ins_pipe(pipe_slow);
2450 %}
2451 
2452 instruct subD_reg_reg(regD dst, regD src1, regD src2) %{
2453   predicate(UseAVX &gt; 0);
2454   match(Set dst (SubD src1 src2));
2455 
2456   format %{ &quot;vsubsd  $dst, $src1, $src2&quot; %}
2457   ins_cost(150);
2458   ins_encode %{
2459     __ vsubsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2460   %}
2461   ins_pipe(pipe_slow);
2462 %}
2463 
2464 instruct subD_reg_mem(regD dst, regD src1, memory src2) %{
2465   predicate(UseAVX &gt; 0);
2466   match(Set dst (SubD src1 (LoadD src2)));
2467 
2468   format %{ &quot;vsubsd  $dst, $src1, $src2&quot; %}
2469   ins_cost(150);
2470   ins_encode %{
2471     __ vsubsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2472   %}
2473   ins_pipe(pipe_slow);
2474 %}
2475 
2476 instruct subD_reg_imm(regD dst, regD src, immD con) %{
2477   predicate(UseAVX &gt; 0);
2478   match(Set dst (SubD src con));
2479 
2480   format %{ &quot;vsubsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2481   ins_cost(150);
2482   ins_encode %{
2483     __ vsubsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2484   %}
2485   ins_pipe(pipe_slow);
2486 %}
2487 
2488 instruct mulF_reg(regF dst, regF src) %{
2489   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2490   match(Set dst (MulF dst src));
2491 
2492   format %{ &quot;mulss   $dst, $src&quot; %}
2493   ins_cost(150);
2494   ins_encode %{
2495     __ mulss($dst$$XMMRegister, $src$$XMMRegister);
2496   %}
2497   ins_pipe(pipe_slow);
2498 %}
2499 
2500 instruct mulF_mem(regF dst, memory src) %{
2501   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2502   match(Set dst (MulF dst (LoadF src)));
2503 
2504   format %{ &quot;mulss   $dst, $src&quot; %}
2505   ins_cost(150);
2506   ins_encode %{
2507     __ mulss($dst$$XMMRegister, $src$$Address);
2508   %}
2509   ins_pipe(pipe_slow);
2510 %}
2511 
2512 instruct mulF_imm(regF dst, immF con) %{
2513   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2514   match(Set dst (MulF dst con));
2515   format %{ &quot;mulss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2516   ins_cost(150);
2517   ins_encode %{
2518     __ mulss($dst$$XMMRegister, $constantaddress($con));
2519   %}
2520   ins_pipe(pipe_slow);
2521 %}
2522 
2523 instruct mulF_reg_reg(regF dst, regF src1, regF src2) %{
2524   predicate(UseAVX &gt; 0);
2525   match(Set dst (MulF src1 src2));
2526 
2527   format %{ &quot;vmulss  $dst, $src1, $src2&quot; %}
2528   ins_cost(150);
2529   ins_encode %{
2530     __ vmulss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2531   %}
2532   ins_pipe(pipe_slow);
2533 %}
2534 
2535 instruct mulF_reg_mem(regF dst, regF src1, memory src2) %{
2536   predicate(UseAVX &gt; 0);
2537   match(Set dst (MulF src1 (LoadF src2)));
2538 
2539   format %{ &quot;vmulss  $dst, $src1, $src2&quot; %}
2540   ins_cost(150);
2541   ins_encode %{
2542     __ vmulss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2543   %}
2544   ins_pipe(pipe_slow);
2545 %}
2546 
2547 instruct mulF_reg_imm(regF dst, regF src, immF con) %{
2548   predicate(UseAVX &gt; 0);
2549   match(Set dst (MulF src con));
2550 
2551   format %{ &quot;vmulss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2552   ins_cost(150);
2553   ins_encode %{
2554     __ vmulss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2555   %}
2556   ins_pipe(pipe_slow);
2557 %}
2558 
2559 instruct mulD_reg(regD dst, regD src) %{
2560   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2561   match(Set dst (MulD dst src));
2562 
2563   format %{ &quot;mulsd   $dst, $src&quot; %}
2564   ins_cost(150);
2565   ins_encode %{
2566     __ mulsd($dst$$XMMRegister, $src$$XMMRegister);
2567   %}
2568   ins_pipe(pipe_slow);
2569 %}
2570 
2571 instruct mulD_mem(regD dst, memory src) %{
2572   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2573   match(Set dst (MulD dst (LoadD src)));
2574 
2575   format %{ &quot;mulsd   $dst, $src&quot; %}
2576   ins_cost(150);
2577   ins_encode %{
2578     __ mulsd($dst$$XMMRegister, $src$$Address);
2579   %}
2580   ins_pipe(pipe_slow);
2581 %}
2582 
2583 instruct mulD_imm(regD dst, immD con) %{
2584   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2585   match(Set dst (MulD dst con));
2586   format %{ &quot;mulsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2587   ins_cost(150);
2588   ins_encode %{
2589     __ mulsd($dst$$XMMRegister, $constantaddress($con));
2590   %}
2591   ins_pipe(pipe_slow);
2592 %}
2593 
2594 instruct mulD_reg_reg(regD dst, regD src1, regD src2) %{
2595   predicate(UseAVX &gt; 0);
2596   match(Set dst (MulD src1 src2));
2597 
2598   format %{ &quot;vmulsd  $dst, $src1, $src2&quot; %}
2599   ins_cost(150);
2600   ins_encode %{
2601     __ vmulsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2602   %}
2603   ins_pipe(pipe_slow);
2604 %}
2605 
2606 instruct mulD_reg_mem(regD dst, regD src1, memory src2) %{
2607   predicate(UseAVX &gt; 0);
2608   match(Set dst (MulD src1 (LoadD src2)));
2609 
2610   format %{ &quot;vmulsd  $dst, $src1, $src2&quot; %}
2611   ins_cost(150);
2612   ins_encode %{
2613     __ vmulsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2614   %}
2615   ins_pipe(pipe_slow);
2616 %}
2617 
2618 instruct mulD_reg_imm(regD dst, regD src, immD con) %{
2619   predicate(UseAVX &gt; 0);
2620   match(Set dst (MulD src con));
2621 
2622   format %{ &quot;vmulsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2623   ins_cost(150);
2624   ins_encode %{
2625     __ vmulsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2626   %}
2627   ins_pipe(pipe_slow);
2628 %}
2629 
2630 instruct divF_reg(regF dst, regF src) %{
2631   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2632   match(Set dst (DivF dst src));
2633 
2634   format %{ &quot;divss   $dst, $src&quot; %}
2635   ins_cost(150);
2636   ins_encode %{
2637     __ divss($dst$$XMMRegister, $src$$XMMRegister);
2638   %}
2639   ins_pipe(pipe_slow);
2640 %}
2641 
2642 instruct divF_mem(regF dst, memory src) %{
2643   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2644   match(Set dst (DivF dst (LoadF src)));
2645 
2646   format %{ &quot;divss   $dst, $src&quot; %}
2647   ins_cost(150);
2648   ins_encode %{
2649     __ divss($dst$$XMMRegister, $src$$Address);
2650   %}
2651   ins_pipe(pipe_slow);
2652 %}
2653 
2654 instruct divF_imm(regF dst, immF con) %{
2655   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2656   match(Set dst (DivF dst con));
2657   format %{ &quot;divss   $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2658   ins_cost(150);
2659   ins_encode %{
2660     __ divss($dst$$XMMRegister, $constantaddress($con));
2661   %}
2662   ins_pipe(pipe_slow);
2663 %}
2664 
2665 instruct divF_reg_reg(regF dst, regF src1, regF src2) %{
2666   predicate(UseAVX &gt; 0);
2667   match(Set dst (DivF src1 src2));
2668 
2669   format %{ &quot;vdivss  $dst, $src1, $src2&quot; %}
2670   ins_cost(150);
2671   ins_encode %{
2672     __ vdivss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2673   %}
2674   ins_pipe(pipe_slow);
2675 %}
2676 
2677 instruct divF_reg_mem(regF dst, regF src1, memory src2) %{
2678   predicate(UseAVX &gt; 0);
2679   match(Set dst (DivF src1 (LoadF src2)));
2680 
2681   format %{ &quot;vdivss  $dst, $src1, $src2&quot; %}
2682   ins_cost(150);
2683   ins_encode %{
2684     __ vdivss($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2685   %}
2686   ins_pipe(pipe_slow);
2687 %}
2688 
2689 instruct divF_reg_imm(regF dst, regF src, immF con) %{
2690   predicate(UseAVX &gt; 0);
2691   match(Set dst (DivF src con));
2692 
2693   format %{ &quot;vdivss  $dst, $src, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2694   ins_cost(150);
2695   ins_encode %{
2696     __ vdivss($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2697   %}
2698   ins_pipe(pipe_slow);
2699 %}
2700 
2701 instruct divD_reg(regD dst, regD src) %{
2702   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2703   match(Set dst (DivD dst src));
2704 
2705   format %{ &quot;divsd   $dst, $src&quot; %}
2706   ins_cost(150);
2707   ins_encode %{
2708     __ divsd($dst$$XMMRegister, $src$$XMMRegister);
2709   %}
2710   ins_pipe(pipe_slow);
2711 %}
2712 
2713 instruct divD_mem(regD dst, memory src) %{
2714   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2715   match(Set dst (DivD dst (LoadD src)));
2716 
2717   format %{ &quot;divsd   $dst, $src&quot; %}
2718   ins_cost(150);
2719   ins_encode %{
2720     __ divsd($dst$$XMMRegister, $src$$Address);
2721   %}
2722   ins_pipe(pipe_slow);
2723 %}
2724 
2725 instruct divD_imm(regD dst, immD con) %{
2726   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2727   match(Set dst (DivD dst con));
2728   format %{ &quot;divsd   $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2729   ins_cost(150);
2730   ins_encode %{
2731     __ divsd($dst$$XMMRegister, $constantaddress($con));
2732   %}
2733   ins_pipe(pipe_slow);
2734 %}
2735 
2736 instruct divD_reg_reg(regD dst, regD src1, regD src2) %{
2737   predicate(UseAVX &gt; 0);
2738   match(Set dst (DivD src1 src2));
2739 
2740   format %{ &quot;vdivsd  $dst, $src1, $src2&quot; %}
2741   ins_cost(150);
2742   ins_encode %{
2743     __ vdivsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister);
2744   %}
2745   ins_pipe(pipe_slow);
2746 %}
2747 
2748 instruct divD_reg_mem(regD dst, regD src1, memory src2) %{
2749   predicate(UseAVX &gt; 0);
2750   match(Set dst (DivD src1 (LoadD src2)));
2751 
2752   format %{ &quot;vdivsd  $dst, $src1, $src2&quot; %}
2753   ins_cost(150);
2754   ins_encode %{
2755     __ vdivsd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$Address);
2756   %}
2757   ins_pipe(pipe_slow);
2758 %}
2759 
2760 instruct divD_reg_imm(regD dst, regD src, immD con) %{
2761   predicate(UseAVX &gt; 0);
2762   match(Set dst (DivD src con));
2763 
2764   format %{ &quot;vdivsd  $dst, $src, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2765   ins_cost(150);
2766   ins_encode %{
2767     __ vdivsd($dst$$XMMRegister, $src$$XMMRegister, $constantaddress($con));
2768   %}
2769   ins_pipe(pipe_slow);
2770 %}
2771 
2772 instruct absF_reg(regF dst) %{
2773   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2774   match(Set dst (AbsF dst));
2775   ins_cost(150);
2776   format %{ &quot;andps   $dst, [0x7fffffff]\t# abs float by sign masking&quot; %}
2777   ins_encode %{
2778     __ andps($dst$$XMMRegister, ExternalAddress(float_signmask()));
2779   %}
2780   ins_pipe(pipe_slow);
2781 %}
2782 
2783 instruct absF_reg_reg(vlRegF dst, vlRegF src) %{
2784   predicate(UseAVX &gt; 0);
2785   match(Set dst (AbsF src));
2786   ins_cost(150);
2787   format %{ &quot;vandps  $dst, $src, [0x7fffffff]\t# abs float by sign masking&quot; %}
2788   ins_encode %{
2789     int vector_len = 0;
2790     __ vandps($dst$$XMMRegister, $src$$XMMRegister,
2791               ExternalAddress(float_signmask()), vector_len);
2792   %}
2793   ins_pipe(pipe_slow);
2794 %}
2795 
2796 instruct absD_reg(regD dst) %{
2797   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2798   match(Set dst (AbsD dst));
2799   ins_cost(150);
2800   format %{ &quot;andpd   $dst, [0x7fffffffffffffff]\t&quot;
2801             &quot;# abs double by sign masking&quot; %}
2802   ins_encode %{
2803     __ andpd($dst$$XMMRegister, ExternalAddress(double_signmask()));
2804   %}
2805   ins_pipe(pipe_slow);
2806 %}
2807 
2808 instruct absD_reg_reg(vlRegD dst, vlRegD src) %{
2809   predicate(UseAVX &gt; 0);
2810   match(Set dst (AbsD src));
2811   ins_cost(150);
2812   format %{ &quot;vandpd  $dst, $src, [0x7fffffffffffffff]\t&quot;
2813             &quot;# abs double by sign masking&quot; %}
2814   ins_encode %{
2815     int vector_len = 0;
2816     __ vandpd($dst$$XMMRegister, $src$$XMMRegister,
2817               ExternalAddress(double_signmask()), vector_len);
2818   %}
2819   ins_pipe(pipe_slow);
2820 %}
2821 
2822 instruct negF_reg(regF dst) %{
2823   predicate((UseSSE&gt;=1) &amp;&amp; (UseAVX == 0));
2824   match(Set dst (NegF dst));
2825   ins_cost(150);
2826   format %{ &quot;xorps   $dst, [0x80000000]\t# neg float by sign flipping&quot; %}
2827   ins_encode %{
2828     __ xorps($dst$$XMMRegister, ExternalAddress(float_signflip()));
2829   %}
2830   ins_pipe(pipe_slow);
2831 %}
2832 
2833 instruct negF_reg_reg(vlRegF dst, vlRegF src) %{
2834   predicate(UseAVX &gt; 0);
2835   match(Set dst (NegF src));
2836   ins_cost(150);
2837   format %{ &quot;vnegatess  $dst, $src, [0x80000000]\t# neg float by sign flipping&quot; %}
2838   ins_encode %{
2839     __ vnegatess($dst$$XMMRegister, $src$$XMMRegister,
2840                  ExternalAddress(float_signflip()));
2841   %}
2842   ins_pipe(pipe_slow);
2843 %}
2844 
2845 instruct negD_reg(regD dst) %{
2846   predicate((UseSSE&gt;=2) &amp;&amp; (UseAVX == 0));
2847   match(Set dst (NegD dst));
2848   ins_cost(150);
2849   format %{ &quot;xorpd   $dst, [0x8000000000000000]\t&quot;
2850             &quot;# neg double by sign flipping&quot; %}
2851   ins_encode %{
2852     __ xorpd($dst$$XMMRegister, ExternalAddress(double_signflip()));
2853   %}
2854   ins_pipe(pipe_slow);
2855 %}
2856 
2857 instruct negD_reg_reg(vlRegD dst, vlRegD src) %{
2858   predicate(UseAVX &gt; 0);
2859   match(Set dst (NegD src));
2860   ins_cost(150);
2861   format %{ &quot;vnegatesd  $dst, $src, [0x8000000000000000]\t&quot;
2862             &quot;# neg double by sign flipping&quot; %}
2863   ins_encode %{
2864     __ vnegatesd($dst$$XMMRegister, $src$$XMMRegister,
2865                  ExternalAddress(double_signflip()));
2866   %}
2867   ins_pipe(pipe_slow);
2868 %}
2869 
2870 instruct sqrtF_reg(regF dst, regF src) %{
2871   predicate(UseSSE&gt;=1);
2872   match(Set dst (SqrtF src));
2873 
2874   format %{ &quot;sqrtss  $dst, $src&quot; %}
2875   ins_cost(150);
2876   ins_encode %{
2877     __ sqrtss($dst$$XMMRegister, $src$$XMMRegister);
2878   %}
2879   ins_pipe(pipe_slow);
2880 %}
2881 
2882 instruct sqrtF_mem(regF dst, memory src) %{
2883   predicate(UseSSE&gt;=1);
2884   match(Set dst (SqrtF (LoadF src)));
2885 
2886   format %{ &quot;sqrtss  $dst, $src&quot; %}
2887   ins_cost(150);
2888   ins_encode %{
2889     __ sqrtss($dst$$XMMRegister, $src$$Address);
2890   %}
2891   ins_pipe(pipe_slow);
2892 %}
2893 
2894 instruct sqrtF_imm(regF dst, immF con) %{
2895   predicate(UseSSE&gt;=1);
2896   match(Set dst (SqrtF con));
2897 
2898   format %{ &quot;sqrtss  $dst, [$constantaddress]\t# load from constant table: float=$con&quot; %}
2899   ins_cost(150);
2900   ins_encode %{
2901     __ sqrtss($dst$$XMMRegister, $constantaddress($con));
2902   %}
2903   ins_pipe(pipe_slow);
2904 %}
2905 
2906 instruct sqrtD_reg(regD dst, regD src) %{
2907   predicate(UseSSE&gt;=2);
2908   match(Set dst (SqrtD src));
2909 
2910   format %{ &quot;sqrtsd  $dst, $src&quot; %}
2911   ins_cost(150);
2912   ins_encode %{
2913     __ sqrtsd($dst$$XMMRegister, $src$$XMMRegister);
2914   %}
2915   ins_pipe(pipe_slow);
2916 %}
2917 
2918 instruct sqrtD_mem(regD dst, memory src) %{
2919   predicate(UseSSE&gt;=2);
2920   match(Set dst (SqrtD (LoadD src)));
2921 
2922   format %{ &quot;sqrtsd  $dst, $src&quot; %}
2923   ins_cost(150);
2924   ins_encode %{
2925     __ sqrtsd($dst$$XMMRegister, $src$$Address);
2926   %}
2927   ins_pipe(pipe_slow);
2928 %}
2929 
2930 instruct sqrtD_imm(regD dst, immD con) %{
2931   predicate(UseSSE&gt;=2);
2932   match(Set dst (SqrtD con));
2933   format %{ &quot;sqrtsd  $dst, [$constantaddress]\t# load from constant table: double=$con&quot; %}
2934   ins_cost(150);
2935   ins_encode %{
2936     __ sqrtsd($dst$$XMMRegister, $constantaddress($con));
2937   %}
2938   ins_pipe(pipe_slow);
2939 %}
2940 
2941 
2942 #ifdef _LP64
2943 instruct roundD_reg(legRegD dst, legRegD src, immU8 rmode) %{
2944   match(Set dst (RoundDoubleMode src rmode));
2945   format %{ &quot;roundsd $dst,$src&quot; %}
2946   ins_cost(150);
2947   ins_encode %{
2948     assert(UseSSE &gt;= 4, &quot;required&quot;);
2949     __ roundsd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant);
2950   %}
2951   ins_pipe(pipe_slow);
2952 %}
2953 
2954 instruct roundD_mem(legRegD dst, memory src, immU8 rmode) %{
2955   match(Set dst (RoundDoubleMode (LoadD src) rmode));
2956   format %{ &quot;roundsd $dst,$src&quot; %}
2957   ins_cost(150);
2958   ins_encode %{
2959     assert(UseSSE &gt;= 4, &quot;required&quot;);
2960     __ roundsd($dst$$XMMRegister, $src$$Address, $rmode$$constant);
2961   %}
2962   ins_pipe(pipe_slow);
2963 %}
2964 
2965 instruct roundD_imm(legRegD dst, immD con, immU8 rmode, rRegI scratch_reg) %{
2966   match(Set dst (RoundDoubleMode con rmode));
2967   effect(TEMP scratch_reg);
2968   format %{ &quot;roundsd $dst,[$constantaddress]\t# load from constant table: double=$con&quot; %}
2969   ins_cost(150);
2970   ins_encode %{
2971     assert(UseSSE &gt;= 4, &quot;required&quot;);
2972     __ roundsd($dst$$XMMRegister, $constantaddress($con), $rmode$$constant, $scratch_reg$$Register);
2973   %}
2974   ins_pipe(pipe_slow);
2975 %}
2976 
2977 instruct vroundD_reg(legVec dst, legVec src, immU8 rmode) %{
2978   predicate(n-&gt;as_Vector()-&gt;length() &lt; 8);
2979   match(Set dst (RoundDoubleModeV src rmode));
2980   format %{ &quot;vroundpd $dst,$src,$rmode\t! round packedD&quot; %}
2981   ins_encode %{
2982     assert(UseAVX &gt; 0, &quot;required&quot;);
2983     int vector_len = vector_length_encoding(this);
2984     __ vroundpd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant, vector_len);
2985   %}
2986   ins_pipe( pipe_slow );
2987 %}
2988 
2989 instruct vround8D_reg(vec dst, vec src, immU8 rmode) %{
2990   predicate(n-&gt;as_Vector()-&gt;length() == 8);
2991   match(Set dst (RoundDoubleModeV src rmode));
2992   format %{ &quot;vrndscalepd $dst,$src,$rmode\t! round packed8D&quot; %}
2993   ins_encode %{
2994     assert(UseAVX &gt; 2, &quot;required&quot;);
2995     __ vrndscalepd($dst$$XMMRegister, $src$$XMMRegister, $rmode$$constant, Assembler::AVX_512bit);
2996   %}
2997   ins_pipe( pipe_slow );
2998 %}
2999 
3000 instruct vroundD_mem(legVec dst, memory mem, immU8 rmode) %{
3001   predicate(n-&gt;as_Vector()-&gt;length() &lt; 8);
3002   match(Set dst (RoundDoubleModeV (LoadVector mem) rmode));
3003   format %{ &quot;vroundpd $dst, $mem, $rmode\t! round packedD&quot; %}
3004   ins_encode %{
3005     assert(UseAVX &gt; 0, &quot;required&quot;);
3006     int vector_len = vector_length_encoding(this);
3007     __ vroundpd($dst$$XMMRegister, $mem$$Address, $rmode$$constant, vector_len);
3008   %}
3009   ins_pipe( pipe_slow );
3010 %}
3011 
3012 instruct vround8D_mem(vec dst, memory mem, immU8 rmode) %{
3013   predicate(n-&gt;as_Vector()-&gt;length() == 8);
3014   match(Set dst (RoundDoubleModeV (LoadVector mem) rmode));
3015   format %{ &quot;vrndscalepd $dst,$mem,$rmode\t! round packed8D&quot; %}
3016   ins_encode %{
3017     assert(UseAVX &gt; 2, &quot;required&quot;);
3018     __ vrndscalepd($dst$$XMMRegister, $mem$$Address, $rmode$$constant, Assembler::AVX_512bit);
3019   %}
3020   ins_pipe( pipe_slow );
3021 %}
3022 #endif // _LP64
3023 
3024 instruct onspinwait() %{
3025   match(OnSpinWait);
3026   ins_cost(200);
3027 
3028   format %{
3029     $$template
3030     $$emit$$&quot;pause\t! membar_onspinwait&quot;
3031   %}
3032   ins_encode %{
3033     __ pause();
3034   %}
3035   ins_pipe(pipe_slow);
3036 %}
3037 
3038 // a * b + c
3039 instruct fmaD_reg(regD a, regD b, regD c) %{
3040   predicate(UseFMA);
3041   match(Set c (FmaD  c (Binary a b)));
3042   format %{ &quot;fmasd $a,$b,$c\t# $c = $a * $b + $c&quot; %}
3043   ins_cost(150);
3044   ins_encode %{
3045     __ fmad($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister);
3046   %}
3047   ins_pipe( pipe_slow );
3048 %}
3049 
3050 // a * b + c
3051 instruct fmaF_reg(regF a, regF b, regF c) %{
3052   predicate(UseFMA);
3053   match(Set c (FmaF  c (Binary a b)));
3054   format %{ &quot;fmass $a,$b,$c\t# $c = $a * $b + $c&quot; %}
3055   ins_cost(150);
3056   ins_encode %{
3057     __ fmaf($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister);
3058   %}
3059   ins_pipe( pipe_slow );
3060 %}
3061 
3062 // ====================VECTOR INSTRUCTIONS=====================================
3063 
3064 // Dummy reg-to-reg vector moves. Removed during post-selection cleanup.
3065 instruct MoveVec2Leg(legVec dst, vec src) %{
3066   match(Set dst src);
3067   format %{ &quot;&quot; %}
3068   ins_encode %{
3069     ShouldNotReachHere();
3070   %}
3071   ins_pipe( fpu_reg_reg );
3072 %}
3073 
3074 instruct MoveLeg2Vec(vec dst, legVec src) %{
3075   match(Set dst src);
3076   format %{ &quot;&quot; %}
3077   ins_encode %{
3078     ShouldNotReachHere();
3079   %}
3080   ins_pipe( fpu_reg_reg );
3081 %}
3082 
3083 // ============================================================================
3084 
3085 // Load vectors
3086 instruct loadV(vec dst, memory mem) %{
3087   match(Set dst (LoadVector mem));
3088   ins_cost(125);
3089   format %{ &quot;load_vector $dst,$mem&quot; %}
3090   ins_encode %{
3091     switch (vector_length_in_bytes(this)) {
3092       case  4: __ movdl    ($dst$$XMMRegister, $mem$$Address); break;
3093       case  8: __ movq     ($dst$$XMMRegister, $mem$$Address); break;
3094       case 16: __ movdqu   ($dst$$XMMRegister, $mem$$Address); break;
3095       case 32: __ vmovdqu  ($dst$$XMMRegister, $mem$$Address); break;
3096       case 64: __ evmovdqul($dst$$XMMRegister, $mem$$Address, Assembler::AVX_512bit); break;
3097       default: ShouldNotReachHere();
3098     }
3099   %}
3100   ins_pipe( pipe_slow );
3101 %}
3102 
3103 // Store vectors generic operand pattern.
3104 instruct storeV(memory mem, vec src) %{
3105   match(Set mem (StoreVector mem src));
3106   ins_cost(145);
3107   format %{ &quot;store_vector $mem,$src\n\t&quot; %}
3108   ins_encode %{
3109     switch (vector_length_in_bytes(this, $src)) {
3110       case  4: __ movdl    ($mem$$Address, $src$$XMMRegister); break;
3111       case  8: __ movq     ($mem$$Address, $src$$XMMRegister); break;
3112       case 16: __ movdqu   ($mem$$Address, $src$$XMMRegister); break;
3113       case 32: __ vmovdqu  ($mem$$Address, $src$$XMMRegister); break;
3114       case 64: __ evmovdqul($mem$$Address, $src$$XMMRegister, Assembler::AVX_512bit); break;
3115       default: ShouldNotReachHere();
3116     }
3117   %}
3118   ins_pipe( pipe_slow );
3119 %}
3120 
3121 // ====================REPLICATE=======================================
3122 
3123 // Replicate byte scalar to be vector
3124 instruct ReplB_reg(vec dst, rRegI src) %{
3125   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32) ||
3126             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions
3127   match(Set dst (ReplicateB src));
3128   format %{ &quot;replicateB $dst,$src&quot; %}
3129   ins_encode %{
3130     uint vlen = vector_length(this);
3131     if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
3132       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);
3133       int vlen_enc = vector_length_encoding(this);
3134       __ evpbroadcastb($dst$$XMMRegister, $src$$Register, vlen_enc);
3135     } else {
3136       __ movdl($dst$$XMMRegister, $src$$Register);
3137       __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
3138       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3139       if (vlen &gt;= 16) {
3140         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3141         if (vlen &gt;= 32) {
3142           assert(vlen == 32, &quot;sanity&quot;); // vlen == 64 &amp;&amp; !AVX512BW is covered by ReplB_reg_leg
3143           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3144         }
3145       }
3146     }
3147   %}
3148   ins_pipe( pipe_slow );
3149 %}
3150 
3151 instruct ReplB_reg_leg(legVec dst, rRegI src) %{
3152   predicate(n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; !VM_Version::supports_avx512bw()); // AVX512BW for 512bit byte instructions
3153   match(Set dst (ReplicateB src));
3154   format %{ &quot;replicateB $dst,$src&quot; %}
3155   ins_encode %{
3156     assert(UseAVX &gt; 2, &quot;required&quot;);
3157     __ movdl($dst$$XMMRegister, $src$$Register);
3158     __ punpcklbw($dst$$XMMRegister, $dst$$XMMRegister);
3159     __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3160     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3161     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3162     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3163   %}
3164   ins_pipe( pipe_slow );
3165 %}
3166 
3167 instruct ReplB_mem(vec dst, memory mem) %{
3168   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32 &amp;&amp; VM_Version::supports_avx512vlbw()) || // AVX512VL for &lt;512bit operands
3169             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw()));    // AVX512BW for 512bit byte instructions
3170   match(Set dst (ReplicateB (LoadB mem)));
3171   format %{ &quot;replicateB $dst,$mem&quot; %}
3172   ins_encode %{
3173     assert(UseAVX &gt; 2, &quot;required&quot;);
3174     int vector_len = vector_length_encoding(this);
3175     __ vpbroadcastb($dst$$XMMRegister, $mem$$Address, vector_len);
3176   %}
3177   ins_pipe( pipe_slow );
3178 %}
3179 
3180 instruct ReplB_imm(vec dst, immI con) %{
3181   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 32) ||
3182             (n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit byte instructions
3183   match(Set dst (ReplicateB con));
3184   format %{ &quot;replicateB $dst,$con&quot; %}
3185   ins_encode %{
3186     uint vlen = vector_length(this);
3187     InternalAddress const_addr = $constantaddress(replicate8_imm($con$$constant, 1));
3188     if (vlen == 4) {
3189       __ movdl($dst$$XMMRegister, const_addr);
3190     } else {
3191       __ movq($dst$$XMMRegister, const_addr);
3192       if (vlen &gt;= 16) {
3193         if (vlen == 64 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
3194           int vlen_enc = vector_length_encoding(this);
3195           __ vpbroadcastb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3196         } else {
3197           __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3198           if (vlen &gt;= 32) {
3199              assert(vlen == 32, &quot;sanity&quot;);// vlen == 64 &amp;&amp; !AVX512BW is covered by ReplB_imm_leg
3200             __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3201           }
3202         }
3203       }
3204     }
3205   %}
3206   ins_pipe( pipe_slow );
3207 %}
3208 
3209 instruct ReplB_imm_leg(legVec dst, immI con) %{
3210   predicate(n-&gt;as_Vector()-&gt;length() == 64 &amp;&amp; !VM_Version::supports_avx512bw());
3211   match(Set dst (ReplicateB con));
3212   format %{ &quot;replicateB $dst,$con&quot; %}
3213   ins_encode %{
3214     __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 1)));
3215     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3216     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3217     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3218   %}
3219   ins_pipe( pipe_slow );
3220 %}
3221 
3222 // Replicate byte scalar zero to be vector
3223 instruct ReplB_zero(vec dst, immI0 zero) %{
3224   match(Set dst (ReplicateB zero));
3225   format %{ &quot;replicateB $dst,$zero&quot; %}
3226   ins_encode %{
3227     uint vlen = vector_length(this);
3228     if (vlen &lt;= 16) {
3229       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3230     } else {
3231       // Use vpxor since AVX512F does not have 512bit vxorpd (requires AVX512DQ).
3232       int vlen_enc = vector_length_encoding(this);
3233       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3234     }
3235   %}
3236   ins_pipe( fpu_reg_reg );
3237 %}
3238 
3239 // ====================ReplicateS=======================================
3240 
3241 instruct ReplS_reg(vec dst, rRegI src) %{
3242   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 16) ||
3243             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts
3244   match(Set dst (ReplicateS src));
3245   format %{ &quot;replicateS $dst,$src&quot; %}
3246   ins_encode %{
3247     uint vlen = vector_length(this);
3248     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
3249       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);
3250       int vlen_enc = vector_length_encoding(this);
3251       __ evpbroadcastw($dst$$XMMRegister, $src$$Register, vlen_enc);
3252     } else {
3253       __ movdl($dst$$XMMRegister, $src$$Register);
3254       __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3255       if (vlen &gt;= 8) {
3256         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3257         if (vlen &gt;= 16) {
3258           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_reg_leg
3259           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3260         }
3261       }
3262     }
3263   %}
3264   ins_pipe( pipe_slow );
3265 %}
3266 
3267 instruct ReplS_reg_leg(legVec dst, rRegI src) %{
3268   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());
3269   match(Set dst (ReplicateS src));
3270   format %{ &quot;replicateS $dst,$src&quot; %}
3271   ins_encode %{
3272     __ movdl($dst$$XMMRegister, $src$$Register);
3273     __ pshuflw($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3274     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3275     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3276     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3277   %}
3278   ins_pipe( pipe_slow );
3279 %}
3280 
3281 instruct ReplS_mem(vec dst, memory mem) %{
3282   predicate((n-&gt;as_Vector()-&gt;length() &gt;= 4  &amp;&amp;
3283              n-&gt;as_Vector()-&gt;length() &lt;= 16 &amp;&amp; VM_Version::supports_avx()) ||
3284             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts
3285   match(Set dst (ReplicateS (LoadS mem)));
3286   format %{ &quot;replicateS $dst,$mem&quot; %}
3287   ins_encode %{
3288     uint vlen = vector_length(this);
3289     if (vlen == 32 || VM_Version::supports_avx512vlbw()) { // AVX512VL for &lt;512bit operands
3290       assert(VM_Version::supports_avx512bw(), &quot;required&quot;);
3291       int vlen_enc = vector_length_encoding(this);
3292       __ vpbroadcastw($dst$$XMMRegister, $mem$$Address, vlen_enc);
3293     } else {
3294       __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);
3295       if (vlen &gt;= 8) {
3296         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3297         if (vlen &gt;= 16) {
3298           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_mem_leg
3299           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3300         }
3301       }
3302     }
3303   %}
3304   ins_pipe( pipe_slow );
3305 %}
3306 
3307 instruct ReplS_mem_leg(legVec dst, memory mem) %{
3308   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());
3309   match(Set dst (ReplicateS (LoadS mem)));
3310   format %{ &quot;replicateS $dst,$mem&quot; %}
3311   ins_encode %{
3312     __ pshuflw($dst$$XMMRegister, $mem$$Address, 0x00);
3313     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3314     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3315     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3316   %}
3317   ins_pipe( pipe_slow );
3318 %}
3319 
3320 instruct ReplS_imm(vec dst, immI con) %{
3321   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 16) ||
3322             (n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; VM_Version::supports_avx512bw())); // AVX512BW for 512bit instructions on shorts
3323   match(Set dst (ReplicateS con));
3324   format %{ &quot;replicateS $dst,$con&quot; %}
3325   ins_encode %{
3326     uint vlen = vector_length(this);
3327     InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 2));
3328     if (vlen == 2) {
3329       __ movdl($dst$$XMMRegister, constaddr);
3330     } else {
3331       __ movq($dst$$XMMRegister, constaddr);
3332       if (vlen == 32 || VM_Version::supports_avx512vlbw() ) { // AVX512VL for &lt;512bit operands
3333         assert(VM_Version::supports_avx512bw(), &quot;required&quot;);
3334         int vlen_enc = vector_length_encoding(this);
3335         __ vpbroadcastw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3336       } else {
3337         __ movq($dst$$XMMRegister, constaddr);
3338         __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3339         if (vlen &gt;= 16) {
3340           assert(vlen == 16, &quot;sanity&quot;); // vlen == 32 &amp;&amp; !AVX512BW is covered by ReplS_imm_leg
3341           __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3342         }
3343       }
3344     }
3345   %}
3346   ins_pipe( fpu_reg_reg );
3347 %}
3348 
3349 instruct ReplS_imm_leg(legVec dst, immI con) %{
3350   predicate(n-&gt;as_Vector()-&gt;length() == 32 &amp;&amp; !VM_Version::supports_avx512bw());
3351   match(Set dst (ReplicateS con));
3352   format %{ &quot;replicateS $dst,$con&quot; %}
3353   ins_encode %{
3354     __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 2)));
3355     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3356     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3357     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3358   %}
3359   ins_pipe( pipe_slow );
3360 %}
3361 
3362 instruct ReplS_zero(vec dst, immI0 zero) %{
3363   match(Set dst (ReplicateS zero));
3364   format %{ &quot;replicateS $dst,$zero&quot; %}
3365   ins_encode %{
3366     uint vlen = vector_length(this);
3367     if (vlen &lt;= 8) {
3368       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3369     } else {
3370       int vlen_enc = vector_length_encoding(this);
3371       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3372     }
3373   %}
3374   ins_pipe( fpu_reg_reg );
3375 %}
3376 
3377 // ====================ReplicateI=======================================
3378 
3379 instruct ReplI_reg(vec dst, rRegI src) %{
3380   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 8) ||
3381             (n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; VM_Version::supports_avx512vl()));
3382   match(Set dst (ReplicateI src));
3383   format %{ &quot;replicateI $dst,$src&quot; %}
3384   ins_encode %{
3385     uint vlen = vector_length(this);
3386     if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3387       int vlen_enc = vector_length_encoding(this);
3388       __ evpbroadcastd($dst$$XMMRegister, $src$$Register, vlen_enc);
3389     } else {
3390       __ movdl($dst$$XMMRegister, $src$$Register);
3391       __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3392       if (vlen &gt;= 8) {
3393         assert(vlen == 8, &quot;sanity&quot;); // vlen == 16 &amp;&amp; !AVX512VL is covered by ReplI_reg_leg
3394         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3395       }
3396     }
3397   %}
3398   ins_pipe( pipe_slow );
3399 %}
3400 
3401 instruct ReplI_reg_leg(legVec dst, rRegI src) %{
3402   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; !VM_Version::supports_avx512vl());
3403   match(Set dst (ReplicateI src));
3404   format %{ &quot;replicateI  $dst,$src&quot; %}
3405   ins_encode %{
3406     __ movdl($dst$$XMMRegister, $src$$Register);
3407     __ pshufd($dst$$XMMRegister, $dst$$XMMRegister, 0x00);
3408     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3409     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3410   %}
3411   ins_pipe( pipe_slow );
3412 %}
3413 
3414 instruct ReplI_mem(vec dst, memory mem) %{
3415   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 8  &amp;&amp; VM_Version::supports_avx()) ||
3416             (n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; VM_Version::supports_avx512vl()));
3417   match(Set dst (ReplicateI (LoadI mem)));
3418   format %{ &quot;replicateI $dst,$mem&quot; %}
3419   ins_encode %{
3420     uint vlen = vector_length(this);
3421     if (vlen &lt;= 4) {
3422       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
3423     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3424       int vector_len = vector_length_encoding(this);
3425       __ vpbroadcastd($dst$$XMMRegister, $mem$$Address, vector_len);
3426     } else {
3427       assert(vlen == 8, &quot;sanity&quot;); // vlen == 16 &amp;&amp; !AVX512VL is covered by ReplI_mem_leg
3428       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
3429       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3430     }
3431   %}
3432   ins_pipe( pipe_slow );
3433 %}
3434 
3435 instruct ReplI_mem_leg(legVec dst, memory mem) %{
3436   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; !VM_Version::supports_avx512vl());
3437   match(Set dst (ReplicateI (LoadI mem)));
3438   format %{ &quot;replicateI $dst,$mem&quot; %}
3439   ins_encode %{
3440     __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
3441     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3442     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3443   %}
3444   ins_pipe( pipe_slow );
3445 %}
3446 
3447 instruct ReplI_imm(vec dst, immI con) %{
3448   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 8) ||
3449             (n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; VM_Version::supports_avx512vl()));
3450   match(Set dst (ReplicateI con));
3451   format %{ &quot;replicateI $dst,$con&quot; %}
3452   ins_encode %{
3453     uint vlen = vector_length(this);
3454     InternalAddress constaddr = $constantaddress(replicate8_imm($con$$constant, 4));
3455     if (vlen == 2) {
3456       __ movq($dst$$XMMRegister, constaddr);
3457     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3458       int vector_len = vector_length_encoding(this);
3459       __ movq($dst$$XMMRegister, constaddr);
3460       __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3461     } else {
3462       __ movq($dst$$XMMRegister, constaddr);
3463       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3464       if (vlen &gt;= 8) {
3465         assert(vlen == 8, &quot;sanity&quot;);
3466         __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3467       }
3468     }
3469   %}
3470   ins_pipe( pipe_slow );
3471 %}
3472 
3473 instruct ReplI_imm_leg(legVec dst, immI con) %{
3474   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; !VM_Version::supports_avx512vl());
3475   match(Set dst (ReplicateI con));
3476   format %{ &quot;replicateI $dst,$con&quot; %}
3477   ins_encode %{
3478     __ movq($dst$$XMMRegister, $constantaddress(replicate8_imm($con$$constant, 4)));
3479     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3480     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3481     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3482   %}
3483   ins_pipe( pipe_slow );
3484 %}
3485 
3486 // Replicate integer (4 byte) scalar zero to be vector
3487 instruct ReplI_zero(vec dst, immI0 zero) %{
3488   match(Set dst (ReplicateI zero));
3489   format %{ &quot;replicateI $dst,$zero&quot; %}
3490   ins_encode %{
3491     uint vlen = vector_length(this);
3492     if (vlen &lt;= 4) {
3493       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3494     } else {
3495       int vlen_enc = vector_length_encoding(this);
3496       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3497     }
3498   %}
3499   ins_pipe( fpu_reg_reg );
3500 %}
3501 
3502 // ====================ReplicateL=======================================
3503 
3504 #ifdef _LP64
3505 // Replicate long (8 byte) scalar to be vector
3506 instruct ReplL_reg(vec dst, rRegL src) %{
3507   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 4) ||
3508             (n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; VM_Version::supports_avx512vl()));
3509   match(Set dst (ReplicateL src));
3510   format %{ &quot;replicateL $dst,$src&quot; %}
3511   ins_encode %{
3512     uint vlen = vector_length(this);
3513     if (vlen == 2) {
3514       __ movdq($dst$$XMMRegister, $src$$Register);
3515       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3516     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3517       int vlen_enc = vector_length_encoding(this);
3518       __ evpbroadcastq($dst$$XMMRegister, $src$$Register, vlen_enc);
3519     } else {
3520       assert(vlen == 4, &quot;sanity&quot;); // vlen == 8 &amp;&amp; !AVX512VL is covered by ReplL_reg_leg
3521       __ movdq($dst$$XMMRegister, $src$$Register);
3522       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3523       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3524     }
3525   %}
3526   ins_pipe( pipe_slow );
3527 %}
3528 
3529 instruct ReplL_reg_leg(legVec dst, rRegL src) %{
3530   predicate(n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; !VM_Version::supports_avx512vl());
3531   match(Set dst (ReplicateL src));
3532   format %{ &quot;replicateL $dst,$src&quot; %}
3533   ins_encode %{
3534     __ movdq($dst$$XMMRegister, $src$$Register);
3535     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3536     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3537     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3538   %}
3539   ins_pipe( pipe_slow );
3540 %}
3541 #else // _LP64
3542 // Replicate long (8 byte) scalar to be vector
3543 instruct ReplL_reg(vec dst, eRegL src, vec tmp) %{
3544   predicate(n-&gt;as_Vector()-&gt;length() &lt;= 4);
3545   match(Set dst (ReplicateL src));
3546   effect(TEMP dst, USE src, TEMP tmp);
3547   format %{ &quot;replicateL $dst,$src&quot; %}
3548   ins_encode %{
3549     uint vlen = vector_length(this);
3550     if (vlen == 2) {
3551       __ movdl($dst$$XMMRegister, $src$$Register);
3552       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3553       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3554       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3555     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3556       int vector_len = Assembler::AVX_256bit;
3557       __ movdl($dst$$XMMRegister, $src$$Register);
3558       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3559       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3560       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3561     } else {
3562       __ movdl($dst$$XMMRegister, $src$$Register);
3563       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3564       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3565       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3566       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3567     }
3568   %}
3569   ins_pipe( pipe_slow );
3570 %}
3571 
3572 instruct ReplL_reg_leg(legVec dst, eRegL src, legVec tmp) %{
3573   predicate(n-&gt;as_Vector()-&gt;length() == 8);
3574   match(Set dst (ReplicateL src));
3575   effect(TEMP dst, USE src, TEMP tmp);
3576   format %{ &quot;replicateL $dst,$src&quot; %}
3577   ins_encode %{
3578     if (VM_Version::supports_avx512vl()) {
3579       __ movdl($dst$$XMMRegister, $src$$Register);
3580       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3581       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3582       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3583       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3584       __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3585     } else {
3586       int vector_len = Assembler::AVX_512bit;
3587       __ movdl($dst$$XMMRegister, $src$$Register);
3588       __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));
3589       __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);
3590       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
3591     }
3592   %}
3593   ins_pipe( pipe_slow );
3594 %}
3595 #endif // _LP64
3596 
3597 instruct ReplL_mem(vec dst, memory mem) %{
3598   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 4) ||
3599             (n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; VM_Version::supports_avx512vl()));
3600   match(Set dst (ReplicateL (LoadL mem)));
3601   format %{ &quot;replicateL $dst,$mem&quot; %}
3602   ins_encode %{
3603     uint vlen = vector_length(this);
3604     if (vlen == 2) {
3605       __ movq($dst$$XMMRegister, $mem$$Address);
3606       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3607     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3608       int vlen_enc = vector_length_encoding(this);
3609       __ vpbroadcastq($dst$$XMMRegister, $mem$$Address, vlen_enc);
3610     } else {
3611       assert(vlen == 4, &quot;sanity&quot;); // vlen == 8 &amp;&amp; !AVX512VL is covered by ReplL_mem_leg
3612       __ movq($dst$$XMMRegister, $mem$$Address);
3613       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3614       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3615     }
3616   %}
3617   ins_pipe( pipe_slow );
3618 %}
3619 
3620 instruct ReplL_mem_leg(legVec dst, memory mem) %{
3621   predicate(n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; !VM_Version::supports_avx512vl());
3622   match(Set dst (ReplicateL (LoadL mem)));
3623   format %{ &quot;replicateL $dst,$mem&quot; %}
3624   ins_encode %{
3625     __ movq($dst$$XMMRegister, $mem$$Address);
3626     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3627     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3628     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3629   %}
3630   ins_pipe( pipe_slow );
3631 %}
3632 
3633 // Replicate long (8 byte) scalar immediate to be vector by loading from const table.
3634 instruct ReplL_imm(vec dst, immL con) %{
3635   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 4) ||
3636             (n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; VM_Version::supports_avx512vl()));
3637   match(Set dst (ReplicateL con));
3638   format %{ &quot;replicateL $dst,$con&quot; %}
3639   ins_encode %{
3640     uint vlen = vector_length(this);
3641     InternalAddress const_addr = $constantaddress($con);
3642     if (vlen == 2) {
3643       __ movq($dst$$XMMRegister, const_addr);
3644       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3645     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3646       int vlen_enc = vector_length_encoding(this);
3647       __ movq($dst$$XMMRegister, const_addr);
3648       __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3649     } else {
3650       assert(vlen == 4, &quot;sanity&quot;); // vlen == 8 &amp;&amp; !AVX512VL is covered by ReplL_imm_leg
3651       __ movq($dst$$XMMRegister, const_addr);
3652       __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3653       __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3654     }
3655   %}
3656   ins_pipe( pipe_slow );
3657 %}
3658 
3659 instruct ReplL_imm_leg(legVec dst, immL con) %{
3660   predicate(n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; !VM_Version::supports_avx512vl());
3661   match(Set dst (ReplicateL con));
3662   format %{ &quot;replicateL $dst,$con&quot; %}
3663   ins_encode %{
3664     __ movq($dst$$XMMRegister, $constantaddress($con));
3665     __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);
3666     __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);
3667     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3668   %}
3669   ins_pipe( pipe_slow );
3670 %}
3671 
3672 instruct ReplL_zero(vec dst, immL0 zero) %{
3673   match(Set dst (ReplicateL zero));
3674   format %{ &quot;replicateL $dst,$zero&quot; %}
3675   ins_encode %{
3676     int vlen = vector_length(this);
3677     if (vlen == 2) {
3678       __ pxor($dst$$XMMRegister, $dst$$XMMRegister);
3679     } else {
3680       int vlen_enc = vector_length_encoding(this);
3681       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);
3682     }
3683   %}
3684   ins_pipe( fpu_reg_reg );
3685 %}
3686 
3687 // ====================ReplicateF=======================================
3688 
3689 instruct ReplF_reg(vec dst, vlRegF src) %{
3690   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 8) ||
3691             (n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; VM_Version::supports_avx512vl()));
3692   match(Set dst (ReplicateF src));
3693   format %{ &quot;replicateF $dst,$src&quot; %}
3694   ins_encode %{
3695     uint vlen = vector_length(this);
3696     if (vlen &lt;= 4) {
3697       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3698     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3699       int vector_len = vector_length_encoding(this);
3700       __ vpbroadcastss($dst$$XMMRegister, $src$$XMMRegister, vector_len);
3701     } else {
3702       assert(vlen == 8, &quot;sanity&quot;); // vlen == 16 &amp;&amp; !AVX512VL is covered by ReplF_reg_leg
3703       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3704       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3705     }
3706   %}
3707   ins_pipe( pipe_slow );
3708 %}
3709 
3710 instruct ReplF_reg_leg(legVec dst, vlRegF src) %{
3711   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; !VM_Version::supports_avx512vl());
3712   match(Set dst (ReplicateF src));
3713   format %{ &quot;replicateF $dst,$src&quot; %}
3714   ins_encode %{
3715     __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x00);
3716     __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3717     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3718   %}
3719   ins_pipe( pipe_slow );
3720 %}
3721 
3722 instruct ReplF_mem(vec dst, memory mem) %{
3723   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 8  &amp;&amp; VM_Version::supports_avx()) ||
3724             (n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; VM_Version::supports_avx512vl()));
3725   match(Set dst (ReplicateF (LoadF mem)));
3726   format %{ &quot;replicateF $dst,$mem&quot; %}
3727   ins_encode %{
3728     uint vlen = vector_length(this);
3729     if (vlen &lt;= 4) {
3730       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
3731     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3732       int vector_len = vector_length_encoding(this);
3733       __ vpbroadcastss($dst$$XMMRegister, $mem$$Address, vector_len);
3734     } else {
3735       assert(vlen == 8, &quot;sanity&quot;); // vlen == 16 &amp;&amp; !AVX512VL is covered by ReplF_mem_leg
3736       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
3737       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3738     }
3739   %}
3740   ins_pipe( pipe_slow );
3741 %}
3742 
3743 instruct ReplF_mem_leg(legVec dst, memory mem) %{
3744   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; !VM_Version::supports_avx512vl());
3745   match(Set dst (ReplicateF (LoadF mem)));
3746   format %{ &quot;replicateF $dst,$mem&quot; %}
3747   ins_encode %{
3748     __ pshufd($dst$$XMMRegister, $mem$$Address, 0x00);
3749     __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3750     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3751   %}
3752   ins_pipe( pipe_slow );
3753 %}
3754 
3755 instruct ReplF_zero(vec dst, immF0 zero) %{
3756   match(Set dst (ReplicateF zero));
3757   format %{ &quot;replicateF $dst,$zero&quot; %}
3758   ins_encode %{
3759     uint vlen = vector_length(this);
3760     if (vlen &lt;= 4) {
3761       __ xorps($dst$$XMMRegister, $dst$$XMMRegister);
3762     } else {
3763       int vlen_enc = vector_length_encoding(this);
3764       __ vpxor($dst$$XMMRegister,$dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3765     }
3766   %}
3767   ins_pipe( fpu_reg_reg );
3768 %}
3769 
3770 // ====================ReplicateD=======================================
3771 
3772 // Replicate double (8 bytes) scalar to be vector
3773 instruct ReplD_reg(vec dst, vlRegD src) %{
3774   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 4) ||
3775             (n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; VM_Version::supports_avx512vl()));
3776   match(Set dst (ReplicateD src));
3777   format %{ &quot;replicateD $dst,$src&quot; %}
3778   ins_encode %{
3779     uint vlen = vector_length(this);
3780     if (vlen == 2) {
3781       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3782     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3783       int vector_len = vector_length_encoding(this);
3784       __ vpbroadcastsd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
3785     } else {
3786       assert(vlen == 4, &quot;sanity&quot;); // vlen == 8 &amp;&amp; !AVX512VL is covered by ReplD_reg_leg
3787       __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3788       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3789     }
3790   %}
3791   ins_pipe( pipe_slow );
3792 %}
3793 
3794 instruct ReplD_reg_leg(legVec dst, vlRegD src) %{
3795   predicate(n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; !VM_Version::supports_avx512vl());
3796   match(Set dst (ReplicateD src));
3797   format %{ &quot;replicateD $dst,$src&quot; %}
3798   ins_encode %{
3799     __ pshufd($dst$$XMMRegister, $src$$XMMRegister, 0x44);
3800     __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3801     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3802   %}
3803   ins_pipe( pipe_slow );
3804 %}
3805 
3806 instruct ReplD_mem(vec dst, memory mem) %{
3807   predicate((n-&gt;as_Vector()-&gt;length() &lt;= 4 &amp;&amp; VM_Version::supports_avx()) ||
3808             (n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; VM_Version::supports_avx512vl()));
3809   match(Set dst (ReplicateD (LoadD mem)));
3810   format %{ &quot;replicateD $dst,$mem&quot; %}
3811   ins_encode %{
3812     uint vlen = vector_length(this);
3813     if (vlen == 2) {
3814       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);
3815     } else if (VM_Version::supports_avx512vl()) { // AVX512VL for &lt;512bit operands
3816       int vector_len = vector_length_encoding(this);
3817       __ vpbroadcastsd($dst$$XMMRegister, $mem$$Address, vector_len);
3818     } else {
3819       assert(vlen == 4, &quot;sanity&quot;); // vlen == 8 &amp;&amp; !AVX512VL is covered by ReplD_mem_leg
3820       __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);
3821       __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3822     }
3823   %}
3824   ins_pipe( pipe_slow );
3825 %}
3826 
3827 instruct ReplD_mem_leg(legVec dst, memory mem) %{
3828   predicate(n-&gt;as_Vector()-&gt;length() == 8 &amp;&amp; !VM_Version::supports_avx512vl());
3829   match(Set dst (ReplicateD (LoadD mem)));
3830   format %{ &quot;replicateD $dst,$mem&quot; %}
3831   ins_encode %{
3832     __ pshufd($dst$$XMMRegister, $mem$$Address, 0x44);
3833     __ vinsertf128_high($dst$$XMMRegister, $dst$$XMMRegister);
3834     __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);
3835   %}
3836   ins_pipe( pipe_slow );
3837 %}
3838 
3839 instruct ReplD_zero(vec dst, immD0 zero) %{
3840   match(Set dst (ReplicateD zero));
3841   format %{ &quot;replicateD $dst,$zero&quot; %}
3842   ins_encode %{
3843     uint vlen = vector_length(this);
3844     if (vlen == 2) {
3845       __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);
3846     } else {
3847       int vlen_enc = vector_length_encoding(this);
3848       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc); // 512bit vxorps requires AVX512DQ
3849     }
3850   %}
3851   ins_pipe( fpu_reg_reg );
3852 %}
3853 
3854 // ====================REDUCTION ARITHMETIC=======================================
3855 
3856 // =======================AddReductionVI==========================================
3857 
3858 instruct vadd2I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
3859   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2
3860   match(Set dst (AddReductionVI src1 src2));
3861   effect(TEMP tmp, TEMP tmp2);
3862   format %{ &quot;vector_add2I_reduction $dst,$src1,$src2&quot; %}
3863   ins_encode %{
3864     if (UseAVX &gt; 2) {
3865       int vector_len = Assembler::AVX_128bit;
3866       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
3867       __ vpaddd($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3868       __ movdl($tmp2$$XMMRegister, $src1$$Register);
3869       __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3870       __ movdl($dst$$Register, $tmp2$$XMMRegister);
3871     } else if (VM_Version::supports_avxonly()) {
3872       int vector_len = Assembler::AVX_128bit;
3873       __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);
3874       __ movdl($tmp2$$XMMRegister, $src1$$Register);
3875       __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, vector_len);
3876       __ movdl($dst$$Register, $tmp2$$XMMRegister);
3877     } else {
3878       assert(UseSSE &gt; 2, &quot;required&quot;);
3879       __ movdqu($tmp2$$XMMRegister, $src2$$XMMRegister);
3880       __ phaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister);
3881       __ movdl($tmp$$XMMRegister, $src1$$Register);
3882       __ paddd($tmp$$XMMRegister, $tmp2$$XMMRegister);
3883       __ movdl($dst$$Register, $tmp$$XMMRegister);
3884     }
3885   %}
3886   ins_pipe( pipe_slow );
3887 %}
3888 
3889 instruct vadd4I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
3890   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4
3891   match(Set dst (AddReductionVI src1 src2));
3892   effect(TEMP tmp, TEMP tmp2);
3893   format %{ &quot;vector_add4I_reduction $dst,$src1,$src2&quot; %}
3894   ins_encode %{
3895     if (UseAVX &gt; 2) {
3896       int vector_len = Assembler::AVX_128bit;
3897       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
3898       __ vpaddd($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3899       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
3900       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3901       __ movdl($tmp2$$XMMRegister, $src1$$Register);
3902       __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3903       __ movdl($dst$$Register, $tmp2$$XMMRegister);
3904     } else if (VM_Version::supports_avxonly()) {
3905       int vector_len = Assembler::AVX_128bit;
3906       __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);
3907       __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp$$XMMRegister, vector_len);
3908       __ movdl($tmp2$$XMMRegister, $src1$$Register);
3909       __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, vector_len);
3910       __ movdl($dst$$Register, $tmp2$$XMMRegister);
3911     } else {
3912       assert(UseSSE &gt; 2, &quot;required&quot;);
3913       __ movdqu($tmp$$XMMRegister, $src2$$XMMRegister);
3914       __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);
3915       __ phaddd($tmp$$XMMRegister, $tmp$$XMMRegister);
3916       __ movdl($tmp2$$XMMRegister, $src1$$Register);
3917       __ paddd($tmp2$$XMMRegister, $tmp$$XMMRegister);
3918       __ movdl($dst$$Register, $tmp2$$XMMRegister);
3919     }
3920   %}
3921   ins_pipe( pipe_slow );
3922 %}
3923 
3924 instruct vadd8I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
3925   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8
3926   match(Set dst (AddReductionVI src1 src2));
3927   effect(TEMP tmp, TEMP tmp2);
3928   format %{ &quot;vector_add8I_reduction $dst,$src1,$src2&quot; %}
3929   ins_encode %{
3930     if (UseAVX &gt; 2) {
3931       int vector_len = Assembler::AVX_128bit;
3932       __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
3933       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, vector_len);
3934       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
3935       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3936       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
3937       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3938       __ movdl($tmp2$$XMMRegister, $src1$$Register);
3939       __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3940       __ movdl($dst$$Register, $tmp2$$XMMRegister);
3941     } else {
3942       assert(UseAVX &gt; 0, &quot;&quot;);
3943       int vector_len = Assembler::AVX_256bit;
3944       __ vphaddd($tmp$$XMMRegister, $src2$$XMMRegister, $src2$$XMMRegister, vector_len);
3945       __ vphaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
3946       __ vextracti128_high($tmp2$$XMMRegister, $tmp$$XMMRegister);
3947       __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
3948       __ movdl($tmp2$$XMMRegister, $src1$$Register);
3949       __ vpaddd($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
3950       __ movdl($dst$$Register, $tmp2$$XMMRegister);
3951     }
3952   %}
3953   ins_pipe( pipe_slow );
3954 %}
3955 
3956 instruct vadd16I_reduction_reg(rRegI dst, rRegI src1, legVec src2, legVec tmp, legVec tmp2, legVec tmp3) %{
3957   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16
3958   match(Set dst (AddReductionVI src1 src2));
3959   effect(TEMP tmp, TEMP tmp2, TEMP tmp3);
3960   format %{ &quot;vector_add16I_reduction $dst,$src1,$src2&quot; %}
3961   ins_encode %{
3962     __ vextracti64x4_high($tmp3$$XMMRegister, $src2$$XMMRegister);
3963     __ vpaddd($tmp3$$XMMRegister, $tmp3$$XMMRegister, $src2$$XMMRegister, 1);
3964     __ vextracti128_high($tmp$$XMMRegister, $tmp3$$XMMRegister);
3965     __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp3$$XMMRegister, 0);
3966     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
3967     __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
3968     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
3969     __ vpaddd($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
3970     __ movdl($tmp2$$XMMRegister, $src1$$Register);
3971     __ vpaddd($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
3972     __ movdl($dst$$Register, $tmp2$$XMMRegister);
3973   %}
3974   ins_pipe( pipe_slow );
3975 %}
3976 
3977 // =======================AddReductionVL==========================================
3978 
3979 #ifdef _LP64
3980 instruct vadd2L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{
3981   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2
3982   match(Set dst (AddReductionVL src1 src2));
3983   effect(TEMP tmp, TEMP tmp2);
3984   format %{ &quot;vector_add2L_reduction $dst,$src1,$src2&quot; %}
3985   ins_encode %{
3986     assert(UseAVX &gt; 2, &quot;required&quot;);
3987     __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
3988     __ vpaddq($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, 0);
3989     __ movdq($tmp2$$XMMRegister, $src1$$Register);
3990     __ vpaddq($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
3991     __ movdq($dst$$Register, $tmp2$$XMMRegister);
3992   %}
3993   ins_pipe( pipe_slow );
3994 %}
3995 
3996 instruct vadd4L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{
3997   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4
3998   match(Set dst (AddReductionVL src1 src2));
3999   effect(TEMP tmp, TEMP tmp2);
4000   format %{ &quot;vector_add4L_reduction $dst,$src1,$src2&quot; %}
4001   ins_encode %{
4002     assert(UseAVX &gt; 2, &quot;required&quot;);
4003     __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
4004     __ vpaddq($tmp2$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, 0);
4005     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4006     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
4007     __ movdq($tmp$$XMMRegister, $src1$$Register);
4008     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
4009     __ movdq($dst$$Register, $tmp2$$XMMRegister);
4010   %}
4011   ins_pipe( pipe_slow );
4012 %}
4013 
4014 instruct vadd8L_reduction_reg(rRegL dst, rRegL src1, legVec src2, legVec tmp, legVec tmp2) %{
4015   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8
4016   match(Set dst (AddReductionVL src1 src2));
4017   effect(TEMP tmp, TEMP tmp2);
4018   format %{ &quot;vector_addL_reduction $dst,$src1,$src2&quot; %}
4019   ins_encode %{
4020     assert(UseAVX &gt; 2, &quot;required&quot;);
4021     __ vextracti64x4_high($tmp2$$XMMRegister, $src2$$XMMRegister);
4022     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $src2$$XMMRegister, 1);
4023     __ vextracti128_high($tmp$$XMMRegister, $tmp2$$XMMRegister);
4024     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
4025     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4026     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
4027     __ movdq($tmp$$XMMRegister, $src1$$Register);
4028     __ vpaddq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
4029     __ movdq($dst$$Register, $tmp2$$XMMRegister);
4030   %}
4031   ins_pipe( pipe_slow );
4032 %}
4033 #endif // _LP64
4034 
4035 // =======================AddReductionVF==========================================
4036 
4037 instruct vadd2F_reduction_reg(regF dst, vec src2, vec tmp) %{
4038   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2
4039   match(Set dst (AddReductionVF dst src2));
4040   effect(TEMP dst, TEMP tmp);
4041   format %{ &quot;vector_add2F_reduction $dst,$dst,$src2&quot; %}
4042   ins_encode %{
4043     if (UseAVX &gt; 0) {
4044       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4045       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4046       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4047     } else {
4048       assert(UseSSE &gt; 0, &quot;required&quot;);
4049       __ addss($dst$$XMMRegister, $src2$$XMMRegister);
4050       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4051       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
4052     }
4053   %}
4054   ins_pipe( pipe_slow );
4055 %}
4056 
4057 instruct vadd4F_reduction_reg(regF dst, vec src2, vec tmp) %{
4058   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4
4059   match(Set dst (AddReductionVF dst src2));
4060   effect(TEMP dst, TEMP tmp);
4061   format %{ &quot;vector_add4F_reduction $dst,$dst,$src2&quot; %}
4062   ins_encode %{
4063     if (UseAVX &gt; 0) {
4064       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4065       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4066       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4067       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
4068       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4069       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
4070       __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4071     } else {
4072       assert(UseSSE &gt; 0, &quot;required&quot;);
4073       __ addss($dst$$XMMRegister, $src2$$XMMRegister);
4074       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4075       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
4076       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
4077       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
4078       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
4079       __ addss($dst$$XMMRegister, $tmp$$XMMRegister);
4080     }
4081   %}
4082   ins_pipe( pipe_slow );
4083 %}
4084 
4085 
4086 instruct vadd8F_reduction_reg(regF dst, vec src2, vec tmp, vec tmp2) %{
4087   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8
4088   match(Set dst (AddReductionVF dst src2));
4089   effect(TEMP tmp, TEMP dst, TEMP tmp2);
4090   format %{ &quot;vector_add8F_reduction $dst,$dst,$src2&quot; %}
4091   ins_encode %{
4092     assert(UseAVX &gt; 0, &quot;required&quot;);
4093     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4094     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4095     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4096     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
4097     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4098     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
4099     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4100     __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);
4101     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4102     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
4103     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4104     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
4105     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4106     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
4107     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4108   %}
4109   ins_pipe( pipe_slow );
4110 %}
4111 
4112 instruct vadd16F_reduction_reg(regF dst, legVec src2, legVec tmp, legVec tmp2) %{
4113   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16
4114   match(Set dst (AddReductionVF dst src2));
4115   effect(TEMP tmp, TEMP dst, TEMP tmp2);
4116   format %{ &quot;vector_add16F_reduction $dst,$dst,$src2&quot; %}
4117   ins_encode %{
4118     assert(UseAVX &gt; 2, &quot;required&quot;);
4119     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4120     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4121     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4122     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
4123     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4124     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
4125     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4126     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
4127     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4128     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
4129     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4130     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
4131     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4132     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
4133     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4134     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
4135     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4136     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
4137     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4138     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
4139     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4140     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
4141     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4142     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
4143     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4144     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
4145     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4146     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
4147     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4148     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
4149     __ vaddss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4150   %}
4151   ins_pipe( pipe_slow );
4152 %}
4153 
4154 // =======================AddReductionVD==========================================
4155 
4156 instruct vadd2D_reduction_reg(regD dst, vec src2, vec tmp) %{
4157   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2
4158   match(Set dst (AddReductionVD dst src2));
4159   effect(TEMP tmp, TEMP dst);
4160   format %{ &quot;vector_add2D_reduction  $dst,$src2&quot; %}
4161   ins_encode %{
4162     if (UseAVX &gt; 0) {
4163       __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4164       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
4165       __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4166     } else {
4167       assert(UseSSE &gt; 0, &quot;required&quot;);
4168       __ addsd($dst$$XMMRegister, $src2$$XMMRegister);
4169       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
4170       __ addsd($dst$$XMMRegister, $tmp$$XMMRegister);
4171     }
4172   %}
4173   ins_pipe( pipe_slow );
4174 %}
4175 
4176 instruct vadd4D_reduction_reg(regD dst, vec src2, vec tmp, vec tmp2) %{
4177   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4
4178   match(Set dst (AddReductionVD dst src2));
4179   effect(TEMP tmp, TEMP dst, TEMP tmp2);
4180   format %{ &quot;vector_add4D_reduction $dst,$dst,$src2&quot; %}
4181   ins_encode %{
4182     assert(UseAVX &gt; 0, &quot;required&quot;);
4183     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4184     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
4185     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4186     __ vextractf128($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
4187     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4188     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4189     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4190   %}
4191   ins_pipe( pipe_slow );
4192 %}
4193 
4194 instruct vadd8D_reduction_reg(regD dst, legVec src2, legVec tmp, legVec tmp2) %{
4195   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8
4196   match(Set dst (AddReductionVD dst src2));
4197   effect(TEMP tmp, TEMP dst, TEMP tmp2);
4198   format %{ &quot;vector_add8D_reduction $dst,$dst,$src2&quot; %}
4199   ins_encode %{
4200     assert(UseAVX &gt; 2, &quot;required&quot;);
4201     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4202     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
4203     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4204     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
4205     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4206     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4207     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4208     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
4209     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4210     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4211     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4212     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
4213     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4214     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4215     __ vaddsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4216   %}
4217   ins_pipe( pipe_slow );
4218 %}
4219 
4220 // =======================MulReductionVI==========================================
4221 
4222 instruct vmul2I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
4223   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2
4224   match(Set dst (MulReductionVI src1 src2));
4225   effect(TEMP tmp, TEMP tmp2);
4226   format %{ &quot;vector_mul2I_reduction $dst,$src1,$src2&quot; %}
4227   ins_encode %{
4228     if (UseAVX &gt; 0) {
4229       int vector_len = Assembler::AVX_128bit;
4230       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
4231       __ vpmulld($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4232       __ movdl($tmp2$$XMMRegister, $src1$$Register);
4233       __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4234       __ movdl($dst$$Register, $tmp2$$XMMRegister);
4235     } else {
4236       assert(UseSSE &gt; 3, &quot;required&quot;);
4237       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
4238       __ pmulld($tmp2$$XMMRegister, $src2$$XMMRegister);
4239       __ movdl($tmp$$XMMRegister, $src1$$Register);
4240       __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);
4241       __ movdl($dst$$Register, $tmp2$$XMMRegister);
4242     }
4243   %}
4244   ins_pipe( pipe_slow );
4245 %}
4246 
4247 instruct vmul4I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
4248   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4
4249   match(Set dst (MulReductionVI src1 src2));
4250   effect(TEMP tmp, TEMP tmp2);
4251   format %{ &quot;vector_mul4I_reduction $dst,$src1,$src2&quot; %}
4252   ins_encode %{
4253     if (UseAVX &gt; 0) {
4254       int vector_len = Assembler::AVX_128bit;
4255       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
4256       __ vpmulld($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4257       __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
4258       __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4259       __ movdl($tmp2$$XMMRegister, $src1$$Register);
4260       __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4261       __ movdl($dst$$Register, $tmp2$$XMMRegister);
4262     } else {
4263       assert(UseSSE &gt; 3, &quot;required&quot;);
4264       __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
4265       __ pmulld($tmp2$$XMMRegister, $src2$$XMMRegister);
4266       __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x1);
4267       __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);
4268       __ movdl($tmp$$XMMRegister, $src1$$Register);
4269       __ pmulld($tmp2$$XMMRegister, $tmp$$XMMRegister);
4270       __ movdl($dst$$Register, $tmp2$$XMMRegister);
4271     }
4272   %}
4273   ins_pipe( pipe_slow );
4274 %}
4275 
4276 instruct vmul8I_reduction_reg(rRegI dst, rRegI src1, vec src2, vec tmp, vec tmp2) %{
4277   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8
4278   match(Set dst (MulReductionVI src1 src2));
4279   effect(TEMP tmp, TEMP tmp2);
4280   format %{ &quot;vector_mul8I_reduction $dst,$src1,$src2&quot; %}
4281   ins_encode %{
4282     assert(UseAVX &gt; 1, &quot;required&quot;);
4283     int vector_len = Assembler::AVX_128bit;
4284     __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
4285     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, vector_len);
4286     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
4287     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4288     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
4289     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4290     __ movdl($tmp2$$XMMRegister, $src1$$Register);
4291     __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, vector_len);
4292     __ movdl($dst$$Register, $tmp2$$XMMRegister);
4293   %}
4294   ins_pipe( pipe_slow );
4295 %}
4296 
4297 instruct vmul16I_reduction_reg(rRegI dst, rRegI src1, legVec src2, legVec tmp, legVec tmp2, legVec tmp3) %{
4298   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16
4299   match(Set dst (MulReductionVI src1 src2));
4300   effect(TEMP tmp, TEMP tmp2, TEMP tmp3);
4301   format %{ &quot;vector_mul16I_reduction $dst,$src1,$src2&quot; %}
4302   ins_encode %{
4303     assert(UseAVX &gt; 2, &quot;required&quot;);
4304     __ vextracti64x4_high($tmp3$$XMMRegister, $src2$$XMMRegister);
4305     __ vpmulld($tmp3$$XMMRegister, $tmp3$$XMMRegister, $src2$$XMMRegister, 1);
4306     __ vextracti128_high($tmp$$XMMRegister, $tmp3$$XMMRegister);
4307     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp3$$XMMRegister, 0);
4308     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0xE);
4309     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
4310     __ pshufd($tmp2$$XMMRegister, $tmp$$XMMRegister, 0x1);
4311     __ vpmulld($tmp$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
4312     __ movdl($tmp2$$XMMRegister, $src1$$Register);
4313     __ vpmulld($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
4314     __ movdl($dst$$Register, $tmp2$$XMMRegister);
4315   %}
4316   ins_pipe( pipe_slow );
4317 %}
4318 
4319 // =======================MulReductionVL==========================================
4320 
4321 #ifdef _LP64
4322 instruct vmul2L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{
4323   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2
4324   match(Set dst (MulReductionVL src1 src2));
4325   effect(TEMP tmp, TEMP tmp2);
4326   format %{ &quot;vector_mul2L_reduction $dst,$src1,$src2&quot; %}
4327   ins_encode %{
4328     assert(VM_Version::supports_avx512dq(), &quot;required&quot;);
4329     __ pshufd($tmp2$$XMMRegister, $src2$$XMMRegister, 0xE);
4330     __ vpmullq($tmp$$XMMRegister, $src2$$XMMRegister, $tmp2$$XMMRegister, 0);
4331     __ movdq($tmp2$$XMMRegister, $src1$$Register);
4332     __ vpmullq($tmp2$$XMMRegister, $tmp$$XMMRegister, $tmp2$$XMMRegister, 0);
4333     __ movdq($dst$$Register, $tmp2$$XMMRegister);
4334   %}
4335   ins_pipe( pipe_slow );
4336 %}
4337 
4338 instruct vmul4L_reduction_reg(rRegL dst, rRegL src1, vec src2, vec tmp, vec tmp2) %{
4339   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4
4340   match(Set dst (MulReductionVL src1 src2));
4341   effect(TEMP tmp, TEMP tmp2);
4342   format %{ &quot;vector_mul4L_reduction $dst,$src1,$src2&quot; %}
4343   ins_encode %{
4344     assert(VM_Version::supports_avx512dq(), &quot;required&quot;);
4345     __ vextracti128_high($tmp$$XMMRegister, $src2$$XMMRegister);
4346     __ vpmullq($tmp2$$XMMRegister, $tmp$$XMMRegister, $src2$$XMMRegister, 0);
4347     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4348     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
4349     __ movdq($tmp$$XMMRegister, $src1$$Register);
4350     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
4351     __ movdq($dst$$Register, $tmp2$$XMMRegister);
4352   %}
4353   ins_pipe( pipe_slow );
4354 %}
4355 
4356 instruct vmul8L_reduction_reg(rRegL dst, rRegL src1, legVec src2, legVec tmp, legVec tmp2) %{
4357   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8
4358   match(Set dst (MulReductionVL src1 src2));
4359   effect(TEMP tmp, TEMP tmp2);
4360   format %{ &quot;vector_mul8L_reduction $dst,$src1,$src2&quot; %}
4361   ins_encode %{
4362     assert(VM_Version::supports_avx512dq(), &quot;required&quot;);
4363     __ vextracti64x4_high($tmp2$$XMMRegister, $src2$$XMMRegister);
4364     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $src2$$XMMRegister, 1);
4365     __ vextracti128_high($tmp$$XMMRegister, $tmp2$$XMMRegister);
4366     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
4367     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4368     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
4369     __ movdq($tmp$$XMMRegister, $src1$$Register);
4370     __ vpmullq($tmp2$$XMMRegister, $tmp2$$XMMRegister, $tmp$$XMMRegister, 0);
4371     __ movdq($dst$$Register, $tmp2$$XMMRegister);
4372   %}
4373   ins_pipe( pipe_slow );
4374 %}
4375 #endif
4376 
4377 // =======================MulReductionVF==========================================
4378 
4379 instruct vmul2F_reduction_reg(regF dst, vec src2, vec tmp) %{
4380   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2
4381   match(Set dst (MulReductionVF dst src2));
4382   effect(TEMP dst, TEMP tmp);
4383   format %{ &quot;vector_mul2F_reduction $dst,$dst,$src2&quot; %}
4384   ins_encode %{
4385     if (UseAVX &gt; 0) {
4386       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4387       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4388       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4389     } else {
4390       assert(UseSSE &gt; 0, &quot;required&quot;);
4391       __ mulss($dst$$XMMRegister, $src2$$XMMRegister);
4392       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4393       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
4394     }
4395   %}
4396   ins_pipe( pipe_slow );
4397 %}
4398 
4399 instruct vmul4F_reduction_reg(regF dst, vec src2, vec tmp) %{
4400   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 4
4401   match(Set dst (MulReductionVF dst src2));
4402   effect(TEMP dst, TEMP tmp);
4403   format %{ &quot;vector_mul4F_reduction $dst,$dst,$src2&quot; %}
4404   ins_encode %{
4405     if (UseAVX &gt; 0) {
4406       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4407       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4408       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4409       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
4410       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4411       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
4412       __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4413     } else {
4414       assert(UseSSE &gt; 0, &quot;required&quot;);
4415       __ mulss($dst$$XMMRegister, $src2$$XMMRegister);
4416       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4417       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
4418       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
4419       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
4420       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
4421       __ mulss($dst$$XMMRegister, $tmp$$XMMRegister);
4422     }
4423   %}
4424   ins_pipe( pipe_slow );
4425 %}
4426 
4427 instruct vmul8F_reduction_reg(regF dst, vec src2, vec tmp, vec tmp2) %{
4428   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 8
4429   match(Set dst (MulReductionVF dst src2));
4430   effect(TEMP tmp, TEMP dst, TEMP tmp2);
4431   format %{ &quot;vector_mul8F_reduction $dst,$dst,$src2&quot; %}
4432   ins_encode %{
4433     assert(UseAVX &gt; 0, &quot;required&quot;);
4434     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4435     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4436     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4437     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
4438     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4439     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
4440     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4441     __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);
4442     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4443     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
4444     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4445     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
4446     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4447     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
4448     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4449   %}
4450   ins_pipe( pipe_slow );
4451 %}
4452 
4453 instruct vmul16F_reduction_reg(regF dst, legVec src2, legVec tmp, legVec tmp2) %{
4454   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 16); // vector_length(src2) == 16
4455   match(Set dst (MulReductionVF dst src2));
4456   effect(TEMP tmp, TEMP dst, TEMP tmp2);
4457   format %{ &quot;vector_mul16F_reduction $dst,$dst,$src2&quot; %}
4458   ins_encode %{
4459     assert(UseAVX &gt; 2, &quot;required&quot;);
4460     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4461     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x01);
4462     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4463     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x02);
4464     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4465     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0x03);
4466     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4467     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
4468     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4469     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
4470     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4471     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
4472     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4473     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
4474     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4475     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
4476     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4477     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
4478     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4479     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
4480     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4481     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
4482     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4483     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
4484     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4485     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x01);
4486     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4487     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x02);
4488     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4489     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0x03);
4490     __ vmulss($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4491   %}
4492   ins_pipe( pipe_slow );
4493 %}
4494 
4495 // =======================MulReductionVD==========================================
4496 
4497 instruct vmul2D_reduction_reg(regD dst, vec src2, vec tmp) %{
4498   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 2); // vector_length(src2) == 2
4499   match(Set dst (MulReductionVD dst src2));
4500   effect(TEMP dst, TEMP tmp);
4501   format %{ &quot;vector_mul2D_reduction $dst,$dst,$src2&quot; %}
4502   ins_encode %{
4503     if (UseAVX &gt; 0) {
4504       __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4505       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
4506       __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4507     } else {
4508       assert(UseSSE &gt; 0, &quot;required&quot;);
4509       __ mulsd($dst$$XMMRegister, $src2$$XMMRegister);
4510       __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
4511       __ mulsd($dst$$XMMRegister, $tmp$$XMMRegister);
4512     }
4513   %}
4514   ins_pipe( pipe_slow );
4515 %}
4516 
4517 
4518 instruct vmul4D_reduction_reg(regD dst, vec src2, vec tmp, vec tmp2) %{
4519   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 4); // vector_length(src2) == 2
4520   match(Set dst (MulReductionVD dst src2));
4521   effect(TEMP tmp, TEMP dst, TEMP tmp2);
4522   format %{ &quot;vector_mul4D_reduction  $dst,$dst,$src2&quot; %}
4523   ins_encode %{
4524     assert(UseAVX &gt; 0, &quot;required&quot;);
4525     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4526     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
4527     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4528     __ vextractf128_high($tmp2$$XMMRegister, $src2$$XMMRegister);
4529     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4530     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4531     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4532   %}
4533   ins_pipe( pipe_slow );
4534 %}
4535 
4536 instruct vmul8D_reduction_reg(regD dst, legVec src2, legVec tmp, legVec tmp2) %{
4537   predicate(n-&gt;in(2)-&gt;bottom_type()-&gt;is_vect()-&gt;length() == 8); // vector_length(src2) == 2
4538   match(Set dst (MulReductionVD dst src2));
4539   effect(TEMP tmp, TEMP dst, TEMP tmp2);
4540   format %{ &quot;vector_mul8D_reduction $dst,$dst,$src2&quot; %}
4541   ins_encode %{
4542     assert(UseAVX &gt; 0, &quot;required&quot;);
4543     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $src2$$XMMRegister);
4544     __ pshufd($tmp$$XMMRegister, $src2$$XMMRegister, 0xE);
4545     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4546     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x1);
4547     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4548     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4549     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4550     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x2);
4551     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4552     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4553     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4554     __ vextractf32x4($tmp2$$XMMRegister, $src2$$XMMRegister, 0x3);
4555     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister);
4556     __ pshufd($tmp$$XMMRegister, $tmp2$$XMMRegister, 0xE);
4557     __ vmulsd($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister);
4558   %}
4559   ins_pipe( pipe_slow );
4560 %}
4561 
4562 // ====================VECTOR ARITHMETIC=======================================
4563 
4564 // --------------------------------- ADD --------------------------------------
4565 
4566 // Bytes vector add
4567 instruct vaddB(vec dst, vec src) %{
4568   predicate(UseAVX == 0);
4569   match(Set dst (AddVB dst src));
4570   format %{ &quot;paddb   $dst,$src\t! add packedB&quot; %}
4571   ins_encode %{
4572     __ paddb($dst$$XMMRegister, $src$$XMMRegister);
4573   %}
4574   ins_pipe( pipe_slow );
4575 %}
4576 
4577 instruct vaddB_reg(vec dst, vec src1, vec src2) %{
4578   predicate(UseAVX &gt; 0);
4579   match(Set dst (AddVB src1 src2));
4580   format %{ &quot;vpaddb  $dst,$src1,$src2\t! add packedB&quot; %}
4581   ins_encode %{
4582     int vector_len = vector_length_encoding(this);
4583     __ vpaddb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4584   %}
4585   ins_pipe( pipe_slow );
4586 %}
4587 
4588 instruct vaddB_mem(vec dst, vec src, memory mem) %{
4589   predicate(UseAVX &gt; 0);
4590   match(Set dst (AddVB src (LoadVector mem)));
4591   format %{ &quot;vpaddb  $dst,$src,$mem\t! add packedB&quot; %}
4592   ins_encode %{
4593     int vector_len = vector_length_encoding(this);
4594     __ vpaddb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4595   %}
4596   ins_pipe( pipe_slow );
4597 %}
4598 
4599 // Shorts/Chars vector add
4600 instruct vaddS(vec dst, vec src) %{
4601   predicate(UseAVX == 0);
4602   match(Set dst (AddVS dst src));
4603   format %{ &quot;paddw   $dst,$src\t! add packedS&quot; %}
4604   ins_encode %{
4605     __ paddw($dst$$XMMRegister, $src$$XMMRegister);
4606   %}
4607   ins_pipe( pipe_slow );
4608 %}
4609 
4610 instruct vaddS_reg(vec dst, vec src1, vec src2) %{
4611   predicate(UseAVX &gt; 0);
4612   match(Set dst (AddVS src1 src2));
4613   format %{ &quot;vpaddw  $dst,$src1,$src2\t! add packedS&quot; %}
4614   ins_encode %{
4615     int vector_len = vector_length_encoding(this);
4616     __ vpaddw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4617   %}
4618   ins_pipe( pipe_slow );
4619 %}
4620 
4621 instruct vaddS_mem(vec dst, vec src, memory mem) %{
4622   predicate(UseAVX &gt; 0);
4623   match(Set dst (AddVS src (LoadVector mem)));
4624   format %{ &quot;vpaddw  $dst,$src,$mem\t! add packedS&quot; %}
4625   ins_encode %{
4626     int vector_len = vector_length_encoding(this);
4627     __ vpaddw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4628   %}
4629   ins_pipe( pipe_slow );
4630 %}
4631 
4632 // Integers vector add
4633 instruct vaddI(vec dst, vec src) %{
4634   predicate(UseAVX == 0);
4635   match(Set dst (AddVI dst src));
4636   format %{ &quot;paddd   $dst,$src\t! add packedI&quot; %}
4637   ins_encode %{
4638     __ paddd($dst$$XMMRegister, $src$$XMMRegister);
4639   %}
4640   ins_pipe( pipe_slow );
4641 %}
4642 
4643 instruct vaddI_reg(vec dst, vec src1, vec src2) %{
4644   predicate(UseAVX &gt; 0);
4645   match(Set dst (AddVI src1 src2));
4646   format %{ &quot;vpaddd  $dst,$src1,$src2\t! add packedI&quot; %}
4647   ins_encode %{
4648     int vector_len = vector_length_encoding(this);
4649     __ vpaddd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4650   %}
4651   ins_pipe( pipe_slow );
4652 %}
4653 
4654 
4655 instruct vaddI_mem(vec dst, vec src, memory mem) %{
4656   predicate(UseAVX &gt; 0);
4657   match(Set dst (AddVI src (LoadVector mem)));
4658   format %{ &quot;vpaddd  $dst,$src,$mem\t! add packedI&quot; %}
4659   ins_encode %{
4660     int vector_len = vector_length_encoding(this);
4661     __ vpaddd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4662   %}
4663   ins_pipe( pipe_slow );
4664 %}
4665 
4666 // Longs vector add
4667 instruct vaddL(vec dst, vec src) %{
4668   predicate(UseAVX == 0);
4669   match(Set dst (AddVL dst src));
4670   format %{ &quot;paddq   $dst,$src\t! add packedL&quot; %}
4671   ins_encode %{
4672     __ paddq($dst$$XMMRegister, $src$$XMMRegister);
4673   %}
4674   ins_pipe( pipe_slow );
4675 %}
4676 
4677 instruct vaddL_reg(vec dst, vec src1, vec src2) %{
4678   predicate(UseAVX &gt; 0);
4679   match(Set dst (AddVL src1 src2));
4680   format %{ &quot;vpaddq  $dst,$src1,$src2\t! add packedL&quot; %}
4681   ins_encode %{
4682     int vector_len = vector_length_encoding(this);
4683     __ vpaddq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4684   %}
4685   ins_pipe( pipe_slow );
4686 %}
4687 
4688 instruct vaddL_mem(vec dst, vec src, memory mem) %{
4689   predicate(UseAVX &gt; 0);
4690   match(Set dst (AddVL src (LoadVector mem)));
4691   format %{ &quot;vpaddq  $dst,$src,$mem\t! add packedL&quot; %}
4692   ins_encode %{
4693     int vector_len = vector_length_encoding(this);
4694     __ vpaddq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4695   %}
4696   ins_pipe( pipe_slow );
4697 %}
4698 
4699 // Floats vector add
4700 instruct vaddF(vec dst, vec src) %{
4701   predicate(UseAVX == 0);
4702   match(Set dst (AddVF dst src));
4703   format %{ &quot;addps   $dst,$src\t! add packedF&quot; %}
4704   ins_encode %{
4705     __ addps($dst$$XMMRegister, $src$$XMMRegister);
4706   %}
4707   ins_pipe( pipe_slow );
4708 %}
4709 
4710 instruct vaddF_reg(vec dst, vec src1, vec src2) %{
4711   predicate(UseAVX &gt; 0);
4712   match(Set dst (AddVF src1 src2));
4713   format %{ &quot;vaddps  $dst,$src1,$src2\t! add packedF&quot; %}
4714   ins_encode %{
4715     int vector_len = vector_length_encoding(this);
4716     __ vaddps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4717   %}
4718   ins_pipe( pipe_slow );
4719 %}
4720 
4721 instruct vaddF_mem(vec dst, vec src, memory mem) %{
4722   predicate(UseAVX &gt; 0);
4723   match(Set dst (AddVF src (LoadVector mem)));
4724   format %{ &quot;vaddps  $dst,$src,$mem\t! add packedF&quot; %}
4725   ins_encode %{
4726     int vector_len = vector_length_encoding(this);
4727     __ vaddps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4728   %}
4729   ins_pipe( pipe_slow );
4730 %}
4731 
4732 // Doubles vector add
4733 instruct vaddD(vec dst, vec src) %{
4734   predicate(UseAVX == 0);
4735   match(Set dst (AddVD dst src));
4736   format %{ &quot;addpd   $dst,$src\t! add packedD&quot; %}
4737   ins_encode %{
4738     __ addpd($dst$$XMMRegister, $src$$XMMRegister);
4739   %}
4740   ins_pipe( pipe_slow );
4741 %}
4742 
4743 instruct vaddD_reg(vec dst, vec src1, vec src2) %{
4744   predicate(UseAVX &gt; 0);
4745   match(Set dst (AddVD src1 src2));
4746   format %{ &quot;vaddpd  $dst,$src1,$src2\t! add packedD&quot; %}
4747   ins_encode %{
4748     int vector_len = vector_length_encoding(this);
4749     __ vaddpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4750   %}
4751   ins_pipe( pipe_slow );
4752 %}
4753 
4754 instruct vaddD_mem(vec dst, vec src, memory mem) %{
4755   predicate(UseAVX &gt; 0);
4756   match(Set dst (AddVD src (LoadVector mem)));
4757   format %{ &quot;vaddpd  $dst,$src,$mem\t! add packedD&quot; %}
4758   ins_encode %{
4759     int vector_len = vector_length_encoding(this);
4760     __ vaddpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4761   %}
4762   ins_pipe( pipe_slow );
4763 %}
4764 
4765 // --------------------------------- SUB --------------------------------------
4766 
4767 // Bytes vector sub
4768 instruct vsubB(vec dst, vec src) %{
4769   predicate(UseAVX == 0);
4770   match(Set dst (SubVB dst src));
4771   format %{ &quot;psubb   $dst,$src\t! sub packedB&quot; %}
4772   ins_encode %{
4773     __ psubb($dst$$XMMRegister, $src$$XMMRegister);
4774   %}
4775   ins_pipe( pipe_slow );
4776 %}
4777 
4778 instruct vsubB_reg(vec dst, vec src1, vec src2) %{
4779   predicate(UseAVX &gt; 0);
4780   match(Set dst (SubVB src1 src2));
4781   format %{ &quot;vpsubb  $dst,$src1,$src2\t! sub packedB&quot; %}
4782   ins_encode %{
4783     int vector_len = vector_length_encoding(this);
4784     __ vpsubb($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4785   %}
4786   ins_pipe( pipe_slow );
4787 %}
4788 
4789 instruct vsubB_mem(vec dst, vec src, memory mem) %{
4790   predicate(UseAVX &gt; 0);
4791   match(Set dst (SubVB src (LoadVector mem)));
4792   format %{ &quot;vpsubb  $dst,$src,$mem\t! sub packedB&quot; %}
4793   ins_encode %{
4794     int vector_len = vector_length_encoding(this);
4795     __ vpsubb($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4796   %}
4797   ins_pipe( pipe_slow );
4798 %}
4799 
4800 // Shorts/Chars vector sub
4801 instruct vsubS(vec dst, vec src) %{
4802   predicate(UseAVX == 0);
4803   match(Set dst (SubVS dst src));
4804   format %{ &quot;psubw   $dst,$src\t! sub packedS&quot; %}
4805   ins_encode %{
4806     __ psubw($dst$$XMMRegister, $src$$XMMRegister);
4807   %}
4808   ins_pipe( pipe_slow );
4809 %}
4810 
4811 
4812 instruct vsubS_reg(vec dst, vec src1, vec src2) %{
4813   predicate(UseAVX &gt; 0);
4814   match(Set dst (SubVS src1 src2));
4815   format %{ &quot;vpsubw  $dst,$src1,$src2\t! sub packedS&quot; %}
4816   ins_encode %{
4817     int vector_len = vector_length_encoding(this);
4818     __ vpsubw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4819   %}
4820   ins_pipe( pipe_slow );
4821 %}
4822 
4823 instruct vsubS_mem(vec dst, vec src, memory mem) %{
4824   predicate(UseAVX &gt; 0);
4825   match(Set dst (SubVS src (LoadVector mem)));
4826   format %{ &quot;vpsubw  $dst,$src,$mem\t! sub packedS&quot; %}
4827   ins_encode %{
4828     int vector_len = vector_length_encoding(this);
4829     __ vpsubw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4830   %}
4831   ins_pipe( pipe_slow );
4832 %}
4833 
4834 // Integers vector sub
4835 instruct vsubI(vec dst, vec src) %{
4836   predicate(UseAVX == 0);
4837   match(Set dst (SubVI dst src));
4838   format %{ &quot;psubd   $dst,$src\t! sub packedI&quot; %}
4839   ins_encode %{
4840     __ psubd($dst$$XMMRegister, $src$$XMMRegister);
4841   %}
4842   ins_pipe( pipe_slow );
4843 %}
4844 
4845 instruct vsubI_reg(vec dst, vec src1, vec src2) %{
4846   predicate(UseAVX &gt; 0);
4847   match(Set dst (SubVI src1 src2));
4848   format %{ &quot;vpsubd  $dst,$src1,$src2\t! sub packedI&quot; %}
4849   ins_encode %{
4850     int vector_len = vector_length_encoding(this);
4851     __ vpsubd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4852   %}
4853   ins_pipe( pipe_slow );
4854 %}
4855 
4856 instruct vsubI_mem(vec dst, vec src, memory mem) %{
4857   predicate(UseAVX &gt; 0);
4858   match(Set dst (SubVI src (LoadVector mem)));
4859   format %{ &quot;vpsubd  $dst,$src,$mem\t! sub packedI&quot; %}
4860   ins_encode %{
4861     int vector_len = vector_length_encoding(this);
4862     __ vpsubd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4863   %}
4864   ins_pipe( pipe_slow );
4865 %}
4866 
4867 // Longs vector sub
4868 instruct vsubL(vec dst, vec src) %{
4869   predicate(UseAVX == 0);
4870   match(Set dst (SubVL dst src));
4871   format %{ &quot;psubq   $dst,$src\t! sub packedL&quot; %}
4872   ins_encode %{
4873     __ psubq($dst$$XMMRegister, $src$$XMMRegister);
4874   %}
4875   ins_pipe( pipe_slow );
4876 %}
4877 
4878 instruct vsubL_reg(vec dst, vec src1, vec src2) %{
4879   predicate(UseAVX &gt; 0);
4880   match(Set dst (SubVL src1 src2));
4881   format %{ &quot;vpsubq  $dst,$src1,$src2\t! sub packedL&quot; %}
4882   ins_encode %{
4883     int vector_len = vector_length_encoding(this);
4884     __ vpsubq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4885   %}
4886   ins_pipe( pipe_slow );
4887 %}
4888 
4889 
4890 instruct vsubL_mem(vec dst, vec src, memory mem) %{
4891   predicate(UseAVX &gt; 0);
4892   match(Set dst (SubVL src (LoadVector mem)));
4893   format %{ &quot;vpsubq  $dst,$src,$mem\t! sub packedL&quot; %}
4894   ins_encode %{
4895     int vector_len = vector_length_encoding(this);
4896     __ vpsubq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4897   %}
4898   ins_pipe( pipe_slow );
4899 %}
4900 
4901 // Floats vector sub
4902 instruct vsubF(vec dst, vec src) %{
4903   predicate(UseAVX == 0);
4904   match(Set dst (SubVF dst src));
4905   format %{ &quot;subps   $dst,$src\t! sub packedF&quot; %}
4906   ins_encode %{
4907     __ subps($dst$$XMMRegister, $src$$XMMRegister);
4908   %}
4909   ins_pipe( pipe_slow );
4910 %}
4911 
4912 instruct vsubF_reg(vec dst, vec src1, vec src2) %{
4913   predicate(UseAVX &gt; 0);
4914   match(Set dst (SubVF src1 src2));
4915   format %{ &quot;vsubps  $dst,$src1,$src2\t! sub packedF&quot; %}
4916   ins_encode %{
4917     int vector_len = vector_length_encoding(this);
4918     __ vsubps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4919   %}
4920   ins_pipe( pipe_slow );
4921 %}
4922 
4923 instruct vsubF_mem(vec dst, vec src, memory mem) %{
4924   predicate(UseAVX &gt; 0);
4925   match(Set dst (SubVF src (LoadVector mem)));
4926   format %{ &quot;vsubps  $dst,$src,$mem\t! sub packedF&quot; %}
4927   ins_encode %{
4928     int vector_len = vector_length_encoding(this);
4929     __ vsubps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4930   %}
4931   ins_pipe( pipe_slow );
4932 %}
4933 
4934 // Doubles vector sub
4935 instruct vsubD(vec dst, vec src) %{
4936   predicate(UseAVX == 0);
4937   match(Set dst (SubVD dst src));
4938   format %{ &quot;subpd   $dst,$src\t! sub packedD&quot; %}
4939   ins_encode %{
4940     __ subpd($dst$$XMMRegister, $src$$XMMRegister);
4941   %}
4942   ins_pipe( pipe_slow );
4943 %}
4944 
4945 instruct vsubD_reg(vec dst, vec src1, vec src2) %{
4946   predicate(UseAVX &gt; 0);
4947   match(Set dst (SubVD src1 src2));
4948   format %{ &quot;vsubpd  $dst,$src1,$src2\t! sub packedD&quot; %}
4949   ins_encode %{
4950     int vector_len = vector_length_encoding(this);
4951     __ vsubpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
4952   %}
4953   ins_pipe( pipe_slow );
4954 %}
4955 
4956 instruct vsubD_mem(vec dst, vec src, memory mem) %{
4957   predicate(UseAVX &gt; 0);
4958   match(Set dst (SubVD src (LoadVector mem)));
4959   format %{ &quot;vsubpd  $dst,$src,$mem\t! sub packedD&quot; %}
4960   ins_encode %{
4961     int vector_len = vector_length_encoding(this);
4962     __ vsubpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
4963   %}
4964   ins_pipe( pipe_slow );
4965 %}
4966 
4967 // --------------------------------- MUL --------------------------------------
4968 
4969 // Byte vector mul
4970 instruct mulB_reg(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{
4971   predicate(n-&gt;as_Vector()-&gt;length() == 4 ||
4972             n-&gt;as_Vector()-&gt;length() == 8);
4973   match(Set dst (MulVB src1 src2));
4974   effect(TEMP dst, TEMP tmp, TEMP scratch);
4975   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4976   ins_encode %{
4977     assert(UseSSE &gt; 3, &quot;required&quot;);
4978     __ pmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister);
4979     __ pmovsxbw($dst$$XMMRegister, $src2$$XMMRegister);
4980     __ pmullw($tmp$$XMMRegister, $dst$$XMMRegister);
4981     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
4982     __ pand($dst$$XMMRegister, $tmp$$XMMRegister);
4983     __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);
4984   %}
4985   ins_pipe( pipe_slow );
4986 %}
4987 
4988 instruct mul16B_reg(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
4989   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &lt;= 1);
4990   match(Set dst (MulVB src1 src2));
4991   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
4992   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
4993   ins_encode %{
4994     assert(UseSSE &gt; 3, &quot;required&quot;);
4995     __ pmovsxbw($tmp1$$XMMRegister, $src1$$XMMRegister);
4996     __ pmovsxbw($tmp2$$XMMRegister, $src2$$XMMRegister);
4997     __ pmullw($tmp1$$XMMRegister, $tmp2$$XMMRegister);
4998     __ pshufd($tmp2$$XMMRegister, $src1$$XMMRegister, 0xEE);
4999     __ pshufd($dst$$XMMRegister, $src2$$XMMRegister, 0xEE);
5000     __ pmovsxbw($tmp2$$XMMRegister, $tmp2$$XMMRegister);
5001     __ pmovsxbw($dst$$XMMRegister, $dst$$XMMRegister);
5002     __ pmullw($tmp2$$XMMRegister, $dst$$XMMRegister);
5003     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
5004     __ pand($tmp2$$XMMRegister, $dst$$XMMRegister);
5005     __ pand($dst$$XMMRegister, $tmp1$$XMMRegister);
5006     __ packuswb($dst$$XMMRegister, $tmp2$$XMMRegister);
5007   %}
5008   ins_pipe( pipe_slow );
5009 %}
5010 
5011 instruct vmul16B_reg_avx(vec dst, vec src1, vec src2, vec tmp, rRegI scratch) %{
5012   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &gt; 1);
5013   match(Set dst (MulVB src1 src2));
5014   effect(TEMP dst, TEMP tmp, TEMP scratch);
5015   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
5016   ins_encode %{
5017   int vector_len = Assembler::AVX_256bit;
5018     __ vpmovsxbw($tmp$$XMMRegister, $src1$$XMMRegister, vector_len);
5019     __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
5020     __ vpmullw($tmp$$XMMRegister, $tmp$$XMMRegister, $dst$$XMMRegister, vector_len);
5021     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
5022     __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
5023     __ vextracti128_high($tmp$$XMMRegister, $dst$$XMMRegister);
5024     __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, 0);
5025   %}
5026   ins_pipe( pipe_slow );
5027 %}
5028 
5029 instruct vmul32B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
5030   predicate(n-&gt;as_Vector()-&gt;length() == 32);
5031   match(Set dst (MulVB src1 src2));
5032   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
5033   format %{&quot;vector_mulB $dst,$src1,$src2&quot; %}
5034   ins_encode %{
5035     assert(UseAVX &gt; 1, &quot;required&quot;);
5036     int vector_len = Assembler::AVX_256bit;
5037     __ vextracti128_high($tmp1$$XMMRegister, $src1$$XMMRegister);
5038     __ vextracti128_high($dst$$XMMRegister, $src2$$XMMRegister);
5039     __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
5040     __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
5041     __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
5042     __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vector_len);
5043     __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
5044     __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
5045     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
5046     __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
5047     __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
5048     __ vpand($dst$$XMMRegister, $dst$$XMMRegister, $tmp2$$XMMRegister, vector_len);
5049     __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp1$$XMMRegister, vector_len);
5050     __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 0xD8, vector_len);
5051   %}
5052   ins_pipe( pipe_slow );
5053 %}
5054 
5055 instruct vmul64B_reg_avx(vec dst, vec src1, vec src2, vec tmp1, vec tmp2, rRegI scratch) %{
5056   predicate(n-&gt;as_Vector()-&gt;length() == 64);
5057   match(Set dst (MulVB src1 src2));
5058   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
5059   format %{&quot;vector_mulB $dst,$src1,$src2\n\t&quot; %}
5060   ins_encode %{
5061     assert(UseAVX &gt; 2, &quot;required&quot;);
5062     int vector_len = Assembler::AVX_512bit;
5063     __ vextracti64x4_high($tmp1$$XMMRegister, $src1$$XMMRegister);
5064     __ vextracti64x4_high($dst$$XMMRegister, $src2$$XMMRegister);
5065     __ vpmovsxbw($tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
5066     __ vpmovsxbw($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
5067     __ vpmullw($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
5068     __ vpmovsxbw($tmp2$$XMMRegister, $src1$$XMMRegister, vector_len);
5069     __ vpmovsxbw($dst$$XMMRegister, $src2$$XMMRegister, vector_len);
5070     __ vpmullw($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
5071     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
5072     __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
5073     __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
5074     __ vpand($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
5075     __ vpackuswb($dst$$XMMRegister, $tmp1$$XMMRegister, $tmp2$$XMMRegister, vector_len);
5076     __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vector_len, $scratch$$Register);
5077     __ vpermq($dst$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
5078   %}
5079   ins_pipe( pipe_slow );
5080 %}
5081 
5082 // Shorts/Chars vector mul
5083 instruct vmulS(vec dst, vec src) %{
5084   predicate(UseAVX == 0);
5085   match(Set dst (MulVS dst src));
5086   format %{ &quot;pmullw $dst,$src\t! mul packedS&quot; %}
5087   ins_encode %{
5088     __ pmullw($dst$$XMMRegister, $src$$XMMRegister);
5089   %}
5090   ins_pipe( pipe_slow );
5091 %}
5092 
5093 instruct vmulS_reg(vec dst, vec src1, vec src2) %{
5094   predicate(UseAVX &gt; 0);
5095   match(Set dst (MulVS src1 src2));
5096   format %{ &quot;vpmullw $dst,$src1,$src2\t! mul packedS&quot; %}
5097   ins_encode %{
5098     int vector_len = vector_length_encoding(this);
5099     __ vpmullw($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5100   %}
5101   ins_pipe( pipe_slow );
5102 %}
5103 
5104 instruct vmulS_mem(vec dst, vec src, memory mem) %{
5105   predicate(UseAVX &gt; 0);
5106   match(Set dst (MulVS src (LoadVector mem)));
5107   format %{ &quot;vpmullw $dst,$src,$mem\t! mul packedS&quot; %}
5108   ins_encode %{
5109     int vector_len = vector_length_encoding(this);
5110     __ vpmullw($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5111   %}
5112   ins_pipe( pipe_slow );
5113 %}
5114 
5115 // Integers vector mul
5116 instruct vmulI(vec dst, vec src) %{
5117   predicate(UseAVX == 0);
5118   match(Set dst (MulVI dst src));
5119   format %{ &quot;pmulld  $dst,$src\t! mul packedI&quot; %}
5120   ins_encode %{
5121     assert(UseSSE &gt; 3, &quot;required&quot;);
5122     __ pmulld($dst$$XMMRegister, $src$$XMMRegister);
5123   %}
5124   ins_pipe( pipe_slow );
5125 %}
5126 
5127 instruct vmulI_reg(vec dst, vec src1, vec src2) %{
5128   predicate(UseAVX &gt; 0);
5129   match(Set dst (MulVI src1 src2));
5130   format %{ &quot;vpmulld $dst,$src1,$src2\t! mul packedI&quot; %}
5131   ins_encode %{
5132     int vector_len = vector_length_encoding(this);
5133     __ vpmulld($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5134   %}
5135   ins_pipe( pipe_slow );
5136 %}
5137 
5138 instruct vmulI_mem(vec dst, vec src, memory mem) %{
5139   predicate(UseAVX &gt; 0);
5140   match(Set dst (MulVI src (LoadVector mem)));
5141   format %{ &quot;vpmulld $dst,$src,$mem\t! mul packedI&quot; %}
5142   ins_encode %{
5143     int vector_len = vector_length_encoding(this);
5144     __ vpmulld($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5145   %}
5146   ins_pipe( pipe_slow );
5147 %}
5148 
5149 // Longs vector mul
5150 instruct vmulL_reg(vec dst, vec src1, vec src2) %{
5151   match(Set dst (MulVL src1 src2));
5152   format %{ &quot;vpmullq $dst,$src1,$src2\t! mul packedL&quot; %}
5153   ins_encode %{
5154     assert(UseAVX &gt; 2, &quot;required&quot;);
5155     int vector_len = vector_length_encoding(this);
5156     __ vpmullq($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5157   %}
5158   ins_pipe( pipe_slow );
5159 %}
5160 
5161 instruct vmulL_mem(vec dst, vec src, memory mem) %{
5162   match(Set dst (MulVL src (LoadVector mem)));
5163   format %{ &quot;vpmullq $dst,$src,$mem\t! mul packedL&quot; %}
5164   ins_encode %{
5165     assert(UseAVX &gt; 2, &quot;required&quot;);
5166     int vector_len = vector_length_encoding(this);
5167     __ vpmullq($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5168   %}
5169   ins_pipe( pipe_slow );
5170 %}
5171 
5172 // Floats vector mul
5173 instruct vmulF(vec dst, vec src) %{
5174   predicate(UseAVX == 0);
5175   match(Set dst (MulVF dst src));
5176   format %{ &quot;mulps   $dst,$src\t! mul packedF&quot; %}
5177   ins_encode %{
5178     __ mulps($dst$$XMMRegister, $src$$XMMRegister);
5179   %}
5180   ins_pipe( pipe_slow );
5181 %}
5182 
5183 instruct vmulF_reg(vec dst, vec src1, vec src2) %{
5184   predicate(UseAVX &gt; 0);
5185   match(Set dst (MulVF src1 src2));
5186   format %{ &quot;vmulps  $dst,$src1,$src2\t! mul packedF&quot; %}
5187   ins_encode %{
5188     int vector_len = vector_length_encoding(this);
5189     __ vmulps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5190   %}
5191   ins_pipe( pipe_slow );
5192 %}
5193 
5194 instruct vmulF_mem(vec dst, vec src, memory mem) %{
5195   predicate(UseAVX &gt; 0);
5196   match(Set dst (MulVF src (LoadVector mem)));
5197   format %{ &quot;vmulps  $dst,$src,$mem\t! mul packedF&quot; %}
5198   ins_encode %{
5199     int vector_len = vector_length_encoding(this);
5200     __ vmulps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5201   %}
5202   ins_pipe( pipe_slow );
5203 %}
5204 
5205 // Doubles vector mul
5206 instruct vmulD(vec dst, vec src) %{
5207   predicate(UseAVX == 0);
5208   match(Set dst (MulVD dst src));
5209   format %{ &quot;mulpd   $dst,$src\t! mul packedD&quot; %}
5210   ins_encode %{
5211     __ mulpd($dst$$XMMRegister, $src$$XMMRegister);
5212   %}
5213   ins_pipe( pipe_slow );
5214 %}
5215 
5216 instruct vmulD_reg(vec dst, vec src1, vec src2) %{
5217   predicate(UseAVX &gt; 0);
5218   match(Set dst (MulVD src1 src2));
5219   format %{ &quot;vmulpd  $dst,$src1,$src2\t! mul packedD&quot; %}
5220   ins_encode %{
5221     int vector_len = vector_length_encoding(this);
5222     __ vmulpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5223   %}
5224   ins_pipe( pipe_slow );
5225 %}
5226 
5227 instruct vmulD_mem(vec dst, vec src, memory mem) %{
5228   predicate(UseAVX &gt; 0);
5229   match(Set dst (MulVD src (LoadVector mem)));
5230   format %{ &quot;vmulpd  $dst,$src,$mem\t! mul packedD&quot; %}
5231   ins_encode %{
5232     int vector_len = vector_length_encoding(this);
5233     __ vmulpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5234   %}
5235   ins_pipe( pipe_slow );
5236 %}
5237 
5238 instruct vcmov8F_reg(legVec dst, legVec src1, legVec src2, immI8 cop, cmpOp_vcmppd copnd) %{
5239   predicate(UseAVX &gt; 0 &amp;&amp; n-&gt;as_Vector()-&gt;length() == 8);
5240   match(Set dst (CMoveVF (Binary copnd cop) (Binary src1 src2)));
5241   effect(TEMP dst, USE src1, USE src2);
5242   format %{ &quot;cmpps.$copnd  $dst, $src1, $src2  ! vcmovevf, cond=$cop\n\t&quot;
5243             &quot;blendvps $dst,$src1,$src2,$dst ! vcmovevf\n\t&quot;
5244          %}
5245   ins_encode %{
5246     int vector_len = 1;
5247     int cond = (Assembler::Condition)($copnd$$cmpcode);
5248     __ cmpps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, cond, vector_len);
5249     __ blendvps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $dst$$XMMRegister, vector_len);
5250   %}
5251   ins_pipe( pipe_slow );
5252 %}
5253 
5254 instruct vcmov4D_reg(legVec dst, legVec src1, legVec src2, immI8 cop, cmpOp_vcmppd copnd) %{
5255   predicate(UseAVX &gt; 0 &amp;&amp; n-&gt;as_Vector()-&gt;length() == 4);
5256   match(Set dst (CMoveVD (Binary copnd cop) (Binary src1 src2)));
5257   effect(TEMP dst, USE src1, USE src2);
5258   format %{ &quot;cmppd.$copnd  $dst, $src1, $src2  ! vcmovevd, cond=$cop\n\t&quot;
5259             &quot;blendvpd $dst,$src1,$src2,$dst ! vcmovevd\n\t&quot;
5260          %}
5261   ins_encode %{
5262     int vector_len = 1;
5263     int cond = (Assembler::Condition)($copnd$$cmpcode);
5264     __ cmppd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, cond, vector_len);
5265     __ blendvpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, $dst$$XMMRegister, vector_len);
5266   %}
5267   ins_pipe( pipe_slow );
5268 %}
5269 
5270 // --------------------------------- DIV --------------------------------------
5271 
5272 // Floats vector div
5273 instruct vdivF(vec dst, vec src) %{
5274   predicate(UseAVX == 0);
5275   match(Set dst (DivVF dst src));
5276   format %{ &quot;divps   $dst,$src\t! div packedF&quot; %}
5277   ins_encode %{
5278     __ divps($dst$$XMMRegister, $src$$XMMRegister);
5279   %}
5280   ins_pipe( pipe_slow );
5281 %}
5282 
5283 instruct vdivF_reg(vec dst, vec src1, vec src2) %{
5284   predicate(UseAVX &gt; 0);
5285   match(Set dst (DivVF src1 src2));
5286   format %{ &quot;vdivps  $dst,$src1,$src2\t! div packedF&quot; %}
5287   ins_encode %{
5288     int vector_len = vector_length_encoding(this);
5289     __ vdivps($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5290   %}
5291   ins_pipe( pipe_slow );
5292 %}
5293 
5294 instruct vdivF_mem(vec dst, vec src, memory mem) %{
5295   predicate(UseAVX &gt; 0);
5296   match(Set dst (DivVF src (LoadVector mem)));
5297   format %{ &quot;vdivps  $dst,$src,$mem\t! div packedF&quot; %}
5298   ins_encode %{
5299     int vector_len = vector_length_encoding(this);
5300     __ vdivps($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5301   %}
5302   ins_pipe( pipe_slow );
5303 %}
5304 
5305 // Doubles vector div
5306 instruct vdivD(vec dst, vec src) %{
5307   predicate(UseAVX == 0);
5308   match(Set dst (DivVD dst src));
5309   format %{ &quot;divpd   $dst,$src\t! div packedD&quot; %}
5310   ins_encode %{
5311     __ divpd($dst$$XMMRegister, $src$$XMMRegister);
5312   %}
5313   ins_pipe( pipe_slow );
5314 %}
5315 
5316 instruct vdivD_reg(vec dst, vec src1, vec src2) %{
5317   predicate(UseAVX &gt; 0);
5318   match(Set dst (DivVD src1 src2));
5319   format %{ &quot;vdivpd  $dst,$src1,$src2\t! div packedD&quot; %}
5320   ins_encode %{
5321     int vector_len = vector_length_encoding(this);
5322     __ vdivpd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5323   %}
5324   ins_pipe( pipe_slow );
5325 %}
5326 
5327 instruct vdivD_mem(vec dst, vec src, memory mem) %{
5328   predicate(UseAVX &gt; 0);
5329   match(Set dst (DivVD src (LoadVector mem)));
5330   format %{ &quot;vdivpd  $dst,$src,$mem\t! div packedD&quot; %}
5331   ins_encode %{
5332     int vector_len = vector_length_encoding(this);
5333     __ vdivpd($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5334   %}
5335   ins_pipe( pipe_slow );
5336 %}
5337 
5338 // --------------------------------- Sqrt --------------------------------------
5339 
5340 instruct vsqrtF_reg(vec dst, vec src) %{
5341   match(Set dst (SqrtVF src));
5342   format %{ &quot;vsqrtps  $dst,$src\t! sqrt packedF&quot; %}
5343   ins_encode %{
5344     assert(UseAVX &gt; 0, &quot;required&quot;);
5345     int vector_len = vector_length_encoding(this);
5346     __ vsqrtps($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5347   %}
5348   ins_pipe( pipe_slow );
5349 %}
5350 
5351 instruct vsqrtF_mem(vec dst, memory mem) %{
5352   match(Set dst (SqrtVF (LoadVector mem)));
5353   format %{ &quot;vsqrtps  $dst,$mem\t! sqrt packedF&quot; %}
5354   ins_encode %{
5355     assert(UseAVX &gt; 0, &quot;required&quot;);
5356     int vector_len = vector_length_encoding(this);
5357     __ vsqrtps($dst$$XMMRegister, $mem$$Address, vector_len);
5358   %}
5359   ins_pipe( pipe_slow );
5360 %}
5361 
5362 // Floating point vector sqrt
5363 instruct vsqrtD_reg(vec dst, vec src) %{
5364   match(Set dst (SqrtVD src));
5365   format %{ &quot;vsqrtpd  $dst,$src\t! sqrt packedD&quot; %}
5366   ins_encode %{
5367     assert(UseAVX &gt; 0, &quot;required&quot;);
5368     int vector_len = vector_length_encoding(this);
5369     __ vsqrtpd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5370   %}
5371   ins_pipe( pipe_slow );
5372 %}
5373 
5374 instruct vsqrtD_mem(vec dst, memory mem) %{
5375   match(Set dst (SqrtVD (LoadVector mem)));
5376   format %{ &quot;vsqrtpd  $dst,$mem\t! sqrt packedD&quot; %}
5377   ins_encode %{
5378     assert(UseAVX &gt; 0, &quot;required&quot;);
5379     int vector_len = vector_length_encoding(this);
5380     __ vsqrtpd($dst$$XMMRegister, $mem$$Address, vector_len);
5381   %}
5382   ins_pipe( pipe_slow );
5383 %}
5384 
5385 // ------------------------------ Shift ---------------------------------------
5386 
5387 // Left and right shift count vectors are the same on x86
5388 // (only lowest bits of xmm reg are used for count).
5389 instruct vshiftcnt(vec dst, rRegI cnt) %{
5390   match(Set dst (LShiftCntV cnt));
5391   match(Set dst (RShiftCntV cnt));
5392   format %{ &quot;movdl    $dst,$cnt\t! load shift count&quot; %}
5393   ins_encode %{
5394     __ movdl($dst$$XMMRegister, $cnt$$Register);
5395   %}
5396   ins_pipe( pipe_slow );
5397 %}
5398 
5399 instruct vshiftcntimm(vec dst, immI8 cnt, rRegI tmp) %{
5400   match(Set dst cnt);
5401   effect(TEMP tmp);
5402   format %{ &quot;movl    $tmp,$cnt\t&quot;
5403             &quot;movdl   $dst,$tmp\t! load shift count&quot; %}
5404   ins_encode %{
5405     __ movl($tmp$$Register, $cnt$$constant);
5406     __ movdl($dst$$XMMRegister, $tmp$$Register);
5407   %}
5408   ins_pipe( pipe_slow );
5409 %}
5410 
5411 // Byte vector shift
5412 instruct vshiftB(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
5413   predicate(n-&gt;as_Vector()-&gt;length() &lt;= 8);
5414   match(Set dst (LShiftVB src shift));
5415   match(Set dst (RShiftVB src shift));
5416   match(Set dst (URShiftVB src shift));
5417   effect(TEMP dst, USE src, USE shift, TEMP tmp, TEMP scratch);
5418   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
5419   ins_encode %{
5420     assert(UseSSE &gt; 3, &quot;required&quot;);
5421     int opcode = this-&gt;ideal_Opcode();
5422     __ vextendbw(opcode, $tmp$$XMMRegister, $src$$XMMRegister);
5423     __ vshiftw(opcode, $tmp$$XMMRegister, $shift$$XMMRegister);
5424     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
5425     __ pand($dst$$XMMRegister, $tmp$$XMMRegister);
5426     __ packuswb($dst$$XMMRegister, $dst$$XMMRegister);
5427   %}
5428   ins_pipe( pipe_slow );
5429 %}
5430 
5431 instruct vshift16B(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{
5432   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &lt;= 1);
5433   match(Set dst (LShiftVB src shift));
5434   match(Set dst (RShiftVB src shift));
5435   match(Set dst (URShiftVB src shift));
5436   effect(TEMP dst, USE src, USE shift, TEMP tmp1, TEMP tmp2, TEMP scratch);
5437   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
5438   ins_encode %{
5439     assert(UseSSE &gt; 3, &quot;required&quot;);
5440     int opcode = this-&gt;ideal_Opcode();
5441 
5442     __ vextendbw(opcode, $tmp1$$XMMRegister, $src$$XMMRegister);
5443     __ vshiftw(opcode, $tmp1$$XMMRegister, $shift$$XMMRegister);
5444     __ pshufd($tmp2$$XMMRegister, $src$$XMMRegister, 0xE);
5445     __ vextendbw(opcode, $tmp2$$XMMRegister, $tmp2$$XMMRegister);
5446     __ vshiftw(opcode, $tmp2$$XMMRegister, $shift$$XMMRegister);
5447     __ movdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
5448     __ pand($tmp2$$XMMRegister, $dst$$XMMRegister);
5449     __ pand($dst$$XMMRegister, $tmp1$$XMMRegister);
5450     __ packuswb($dst$$XMMRegister, $tmp2$$XMMRegister);
5451   %}
5452   ins_pipe( pipe_slow );
5453 %}
5454 
5455 instruct vshift16B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
5456   predicate(n-&gt;as_Vector()-&gt;length() == 16 &amp;&amp; UseAVX &gt; 1);
5457   match(Set dst (LShiftVB src shift));
5458   match(Set dst (RShiftVB src shift));
5459   match(Set dst (URShiftVB src shift));
5460   effect(TEMP dst, TEMP tmp, TEMP scratch);
5461   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
5462   ins_encode %{
5463     int opcode = this-&gt;ideal_Opcode();
5464     int vector_len = Assembler::AVX_256bit;
5465     __ vextendbw(opcode, $tmp$$XMMRegister, $src$$XMMRegister, vector_len);
5466     __ vshiftw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
5467     __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
5468     __ vextracti128_high($dst$$XMMRegister, $tmp$$XMMRegister);
5469     __ vpackuswb($dst$$XMMRegister, $tmp$$XMMRegister, $dst$$XMMRegister, 0);
5470   %}
5471   ins_pipe( pipe_slow );
5472 %}
5473 
5474 instruct vshift32B_avx(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
5475   predicate(n-&gt;as_Vector()-&gt;length() == 32);
5476   match(Set dst (LShiftVB src shift));
5477   match(Set dst (RShiftVB src shift));
5478   match(Set dst (URShiftVB src shift));
5479   effect(TEMP dst, TEMP tmp, TEMP scratch);
5480   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
5481   ins_encode %{
5482     assert(UseAVX &gt; 1, &quot;required&quot;);
5483     int opcode = this-&gt;ideal_Opcode();
5484     int vector_len = Assembler::AVX_256bit;
5485     __ vextracti128_high($tmp$$XMMRegister, $src$$XMMRegister);
5486     __ vextendbw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, vector_len);
5487     __ vextendbw(opcode, $dst$$XMMRegister, $src$$XMMRegister, vector_len);
5488     __ vshiftw(opcode, $tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
5489     __ vshiftw(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $shift$$XMMRegister, vector_len);
5490     __ vpand($tmp$$XMMRegister, $tmp$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
5491     __ vpand($dst$$XMMRegister, $dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), vector_len, $scratch$$Register);
5492     __ vpackuswb($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
5493     __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 0xD8, vector_len);
5494   %}
5495   ins_pipe( pipe_slow );
5496 %}
5497 
5498 instruct vshift64B_avx(vec dst, vec src, vec shift, vec tmp1, vec tmp2, rRegI scratch) %{
5499   predicate(n-&gt;as_Vector()-&gt;length() == 64);
5500   match(Set dst (LShiftVB src shift));
5501   match(Set dst (RShiftVB src shift));
5502   match(Set dst (URShiftVB src shift));
5503   effect(TEMP dst, TEMP tmp1, TEMP tmp2, TEMP scratch);
5504   format %{&quot;vector_byte_shift $dst,$src,$shift&quot; %}
5505   ins_encode %{
5506     assert(UseAVX &gt; 2, &quot;required&quot;);
5507     int opcode = this-&gt;ideal_Opcode();
5508     int vector_len = Assembler::AVX_512bit;
5509     __ vextracti64x4($tmp1$$XMMRegister, $src$$XMMRegister, 1);
5510     __ vextendbw(opcode, $tmp1$$XMMRegister, $tmp1$$XMMRegister, vector_len);
5511     __ vextendbw(opcode, $tmp2$$XMMRegister, $src$$XMMRegister, vector_len);
5512     __ vshiftw(opcode, $tmp1$$XMMRegister, $tmp1$$XMMRegister, $shift$$XMMRegister, vector_len);
5513     __ vshiftw(opcode, $tmp2$$XMMRegister, $tmp2$$XMMRegister, $shift$$XMMRegister, vector_len);
5514     __ vmovdqu($dst$$XMMRegister, ExternalAddress(vector_short_to_byte_mask()), $scratch$$Register);
5515     __ vpbroadcastd($dst$$XMMRegister, $dst$$XMMRegister, vector_len);
5516     __ vpand($tmp1$$XMMRegister, $tmp1$$XMMRegister, $dst$$XMMRegister, vector_len);
5517     __ vpand($tmp2$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
5518     __ vpackuswb($dst$$XMMRegister, $tmp1$$XMMRegister, $tmp2$$XMMRegister, vector_len);
5519     __ evmovdquq($tmp2$$XMMRegister, ExternalAddress(vector_byte_perm_mask()), vector_len, $scratch$$Register);
5520     __ vpermq($dst$$XMMRegister, $tmp2$$XMMRegister, $dst$$XMMRegister, vector_len);
5521   %}
5522   ins_pipe( pipe_slow );
5523 %}
5524 
5525 // Shorts vector logical right shift produces incorrect Java result
5526 // for negative data because java code convert short value into int with
5527 // sign extension before a shift. But char vectors are fine since chars are
5528 // unsigned values.
5529 // Shorts/Chars vector left shift
5530 instruct vshiftS(vec dst, vec src, vec shift) %{
5531   match(Set dst (LShiftVS src shift));
5532   match(Set dst (RShiftVS src shift));
5533   match(Set dst (URShiftVS src shift));
5534   effect(TEMP dst, USE src, USE shift);
5535   format %{ &quot;vshiftw  $dst,$src,$shift\t! shift packedS&quot; %}
5536   ins_encode %{
5537     int opcode = this-&gt;ideal_Opcode();
5538     if (UseAVX &gt; 0) {
5539       int vlen_enc = vector_length_encoding(this);
5540       __ vshiftw(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vlen_enc);
5541     } else {
5542       int vlen = vector_length(this);
5543       if (vlen == 2) {
5544         __ movflt($dst$$XMMRegister, $src$$XMMRegister);
5545         __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5546       } else if (vlen == 4) {
5547         __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
5548         __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5549       } else {
5550         assert (vlen == 8, &quot;sanity&quot;);
5551         __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
5552         __ vshiftw(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5553       }
5554     }
5555   %}
5556   ins_pipe( pipe_slow );
5557 %}
5558 
5559 // Integers vector left shift
5560 instruct vshiftI(vec dst, vec src, vec shift) %{
5561   match(Set dst (LShiftVI src shift));
5562   match(Set dst (RShiftVI src shift));
5563   match(Set dst (URShiftVI src shift));
5564   effect(TEMP dst, USE src, USE shift);
5565   format %{ &quot;vshiftd  $dst,$src,$shift\t! shift packedI&quot; %}
5566   ins_encode %{
5567     int opcode = this-&gt;ideal_Opcode();
5568     if (UseAVX &gt; 0) {
5569       int vector_len = vector_length_encoding(this);
5570       __ vshiftd(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
5571     } else {
5572       int vlen = vector_length(this);
5573       if (vlen == 2) {
5574         __ movdbl($dst$$XMMRegister, $src$$XMMRegister);
5575         __ vshiftd(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5576       } else {
5577         assert(vlen == 4, &quot;sanity&quot;);
5578         __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
5579         __ vshiftd(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5580       }
5581     }
5582   %}
5583   ins_pipe( pipe_slow );
5584 %}
5585 
5586 // Longs vector shift
5587 instruct vshiftL(vec dst, vec src, vec shift) %{
5588   match(Set dst (LShiftVL src shift));
5589   match(Set dst (URShiftVL src shift));
5590   effect(TEMP dst, USE src, USE shift);
5591   format %{ &quot;vshiftq  $dst,$src,$shift\t! shift packedL&quot; %}
5592   ins_encode %{
5593     int opcode = this-&gt;ideal_Opcode();
5594     if (UseAVX &gt; 0) {
5595       int vector_len = vector_length_encoding(this);
5596       __ vshiftq(opcode, $dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
5597     } else {
5598       assert(vector_length(this) == 2, &quot;&quot;);
5599       __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
5600       __ vshiftq(opcode, $dst$$XMMRegister, $shift$$XMMRegister);
5601     }
5602   %}
5603   ins_pipe( pipe_slow );
5604 %}
5605 
5606 // -------------------ArithmeticRightShift -----------------------------------
5607 // Long vector arithmetic right shift
5608 instruct vshiftL_arith_reg(vec dst, vec src, vec shift, vec tmp, rRegI scratch) %{
5609   predicate(UseAVX &lt;= 2);
5610   match(Set dst (RShiftVL src shift));
5611   effect(TEMP dst, TEMP tmp, TEMP scratch);
5612   format %{ &quot;vshiftq $dst,$src,$shift&quot; %}
5613   ins_encode %{
5614     uint vlen = vector_length(this);
5615     if (vlen == 2) {
5616       assert(UseSSE &gt;= 2, &quot;required&quot;);
5617       __ movdqu($dst$$XMMRegister, $src$$XMMRegister);
5618       __ psrlq($dst$$XMMRegister, $shift$$XMMRegister);
5619       __ movdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);
5620       __ psrlq($tmp$$XMMRegister, $shift$$XMMRegister);
5621       __ pxor($dst$$XMMRegister, $tmp$$XMMRegister);
5622       __ psubq($dst$$XMMRegister, $tmp$$XMMRegister);
5623     } else {
5624       assert(vlen == 4, &quot;sanity&quot;);
5625       assert(UseAVX &gt; 1, &quot;required&quot;);
5626       int vector_len = Assembler::AVX_256bit;
5627       __ vpsrlq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
5628       __ vmovdqu($tmp$$XMMRegister, ExternalAddress(vector_long_sign_mask()), $scratch$$Register);
5629       __ vpsrlq($tmp$$XMMRegister, $tmp$$XMMRegister, $shift$$XMMRegister, vector_len);
5630       __ vpxor($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
5631       __ vpsubq($dst$$XMMRegister, $dst$$XMMRegister, $tmp$$XMMRegister, vector_len);
5632     }
5633   %}
5634   ins_pipe( pipe_slow );
5635 %}
5636 
5637 instruct vshiftL_arith_reg_evex(vec dst, vec src, vec shift) %{
5638   predicate(UseAVX &gt; 2);
5639   match(Set dst (RShiftVL src shift));
5640   format %{ &quot;vshiftq $dst,$src,$shift&quot; %}
5641   ins_encode %{
5642     int vector_len = vector_length_encoding(this);
5643     __ evpsraq($dst$$XMMRegister, $src$$XMMRegister, $shift$$XMMRegister, vector_len);
5644   %}
5645   ins_pipe( pipe_slow );
5646 %}
5647 
5648 // --------------------------------- AND --------------------------------------
5649 
5650 instruct vand(vec dst, vec src) %{
5651   predicate(UseAVX == 0);
5652   match(Set dst (AndV dst src));
5653   format %{ &quot;pand    $dst,$src\t! and vectors&quot; %}
5654   ins_encode %{
5655     __ pand($dst$$XMMRegister, $src$$XMMRegister);
5656   %}
5657   ins_pipe( pipe_slow );
5658 %}
5659 
5660 instruct vand_reg(vec dst, vec src1, vec src2) %{
5661   predicate(UseAVX &gt; 0);
5662   match(Set dst (AndV src1 src2));
5663   format %{ &quot;vpand   $dst,$src1,$src2\t! and vectors&quot; %}
5664   ins_encode %{
5665     int vector_len = vector_length_encoding(this);
5666     __ vpand($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5667   %}
5668   ins_pipe( pipe_slow );
5669 %}
5670 
5671 instruct vand_mem(vec dst, vec src, memory mem) %{
5672   predicate(UseAVX &gt; 0);
5673   match(Set dst (AndV src (LoadVector mem)));
5674   format %{ &quot;vpand   $dst,$src,$mem\t! and vectors&quot; %}
5675   ins_encode %{
5676     int vector_len = vector_length_encoding(this);
5677     __ vpand($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5678   %}
5679   ins_pipe( pipe_slow );
5680 %}
5681 
5682 // --------------------------------- OR ---------------------------------------
5683 
5684 instruct vor(vec dst, vec src) %{
5685   predicate(UseAVX == 0);
5686   match(Set dst (OrV dst src));
5687   format %{ &quot;por     $dst,$src\t! or vectors&quot; %}
5688   ins_encode %{
5689     __ por($dst$$XMMRegister, $src$$XMMRegister);
5690   %}
5691   ins_pipe( pipe_slow );
5692 %}
5693 
5694 instruct vor_reg(vec dst, vec src1, vec src2) %{
5695   predicate(UseAVX &gt; 0);
5696   match(Set dst (OrV src1 src2));
5697   format %{ &quot;vpor    $dst,$src1,$src2\t! or vectors&quot; %}
5698   ins_encode %{
5699     int vector_len = vector_length_encoding(this);
5700     __ vpor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5701   %}
5702   ins_pipe( pipe_slow );
5703 %}
5704 
5705 instruct vor_mem(vec dst, vec src, memory mem) %{
5706   predicate(UseAVX &gt; 0);
5707   match(Set dst (OrV src (LoadVector mem)));
5708   format %{ &quot;vpor    $dst,$src,$mem\t! or vectors&quot; %}
5709   ins_encode %{
5710     int vector_len = vector_length_encoding(this);
5711     __ vpor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5712   %}
5713   ins_pipe( pipe_slow );
5714 %}
5715 
5716 // --------------------------------- XOR --------------------------------------
5717 
5718 instruct vxor(vec dst, vec src) %{
5719   predicate(UseAVX == 0);
5720   match(Set dst (XorV dst src));
5721   format %{ &quot;pxor    $dst,$src\t! xor vectors&quot; %}
5722   ins_encode %{
5723     __ pxor($dst$$XMMRegister, $src$$XMMRegister);
5724   %}
5725   ins_pipe( pipe_slow );
5726 %}
5727 
5728 instruct vxor_reg(vec dst, vec src1, vec src2) %{
5729   predicate(UseAVX &gt; 0);
5730   match(Set dst (XorV src1 src2));
5731   format %{ &quot;vpxor   $dst,$src1,$src2\t! xor vectors&quot; %}
5732   ins_encode %{
5733     int vector_len = vector_length_encoding(this);
5734     __ vpxor($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5735   %}
5736   ins_pipe( pipe_slow );
5737 %}
5738 
5739 instruct vxor_mem(vec dst, vec src, memory mem) %{
5740   predicate(UseAVX &gt; 0);
5741   match(Set dst (XorV src (LoadVector mem)));
5742   format %{ &quot;vpxor   $dst,$src,$mem\t! xor vectors&quot; %}
5743   ins_encode %{
5744     int vector_len = vector_length_encoding(this);
5745     __ vpxor($dst$$XMMRegister, $src$$XMMRegister, $mem$$Address, vector_len);
5746   %}
5747   ins_pipe( pipe_slow );
5748 %}
5749 
5750 // --------------------------------- ABS --------------------------------------
5751 // a = |a|
5752 instruct vabsB_reg(vec dst, vec src) %{
5753   match(Set dst (AbsVB  src));
5754   format %{ &quot;vabsb $dst,$src\t# $dst = |$src| abs packedB&quot; %}
5755   ins_encode %{
5756     uint vlen = vector_length(this);
5757     if (vlen &lt;= 16) {
5758       __ pabsb($dst$$XMMRegister, $src$$XMMRegister);
5759     } else {
5760       int vlen_enc = vector_length_encoding(this);
5761       __ vpabsb($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
5762     }
5763   %}
5764   ins_pipe( pipe_slow );
5765 %}
5766 
5767 instruct vabsS_reg(vec dst, vec src) %{
5768   match(Set dst (AbsVS  src));
5769   format %{ &quot;vabsw $dst,$src\t# $dst = |$src| abs packedS&quot; %}
5770   ins_encode %{
5771     uint vlen = vector_length(this);
5772     if (vlen &lt;= 8) {
5773       __ pabsw($dst$$XMMRegister, $src$$XMMRegister);
5774     } else {
5775       int vlen_enc = vector_length_encoding(this);
5776       __ vpabsw($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
5777     }
5778   %}
5779   ins_pipe( pipe_slow );
5780 %}
5781 
5782 instruct vabsI_reg(vec dst, vec src) %{
5783   match(Set dst (AbsVI  src));
5784   format %{ &quot;pabsd $dst,$src\t# $dst = |$src| abs packedI&quot; %}
5785   ins_encode %{
5786     uint vlen = vector_length(this);
5787     if (vlen &lt;= 4) {
5788       __ pabsd($dst$$XMMRegister, $src$$XMMRegister);
5789     } else {
5790       int vlen_enc = vector_length_encoding(this);
5791       __ vpabsd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);
5792     }
5793   %}
5794   ins_pipe( pipe_slow );
5795 %}
5796 
5797 instruct vabsL_reg(vec dst, vec src) %{
5798   match(Set dst (AbsVL  src));
5799   format %{ &quot;evpabsq $dst,$src\t# $dst = |$src| abs packedL&quot; %}
5800   ins_encode %{
5801     assert(UseAVX &gt; 2, &quot;required&quot;);
5802     int vector_len = vector_length_encoding(this);
5803     __ evpabsq($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5804   %}
5805   ins_pipe( pipe_slow );
5806 %}
5807 
5808 // --------------------------------- ABSNEG --------------------------------------
5809 
5810 instruct vabsnegF(vec dst, vec src, rRegI scratch) %{
5811   predicate(n-&gt;as_Vector()-&gt;length() != 4); // handled by 1-operand instruction vabsneg4F
5812   match(Set dst (AbsVF src));
5813   match(Set dst (NegVF src));
5814   effect(TEMP scratch);
5815   format %{ &quot;vabsnegf $dst,$src,[mask]\t# absneg packedF&quot; %}
5816   ins_cost(150);
5817   ins_encode %{
5818     int opcode = this-&gt;ideal_Opcode();
5819     int vlen = vector_length(this);
5820     if (vlen == 2) {
5821       __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);
5822     } else {
5823       assert(vlen == 8 || vlen == 16, &quot;required&quot;);
5824       int vlen_enc = vector_length_encoding(this);
5825       __ vabsnegf(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);
5826     }
5827   %}
5828   ins_pipe( pipe_slow );
5829 %}
5830 
5831 instruct vabsneg4F(vec dst, rRegI scratch) %{
5832   predicate(n-&gt;as_Vector()-&gt;length() == 4);
5833   match(Set dst (AbsVF dst));
5834   match(Set dst (NegVF dst));
5835   effect(TEMP scratch);
5836   format %{ &quot;vabsnegf $dst,[mask]\t# absneg packed4F&quot; %}
5837   ins_cost(150);
5838   ins_encode %{
5839     int opcode = this-&gt;ideal_Opcode();
5840     __ vabsnegf(opcode, $dst$$XMMRegister, $dst$$XMMRegister, $scratch$$Register);
5841   %}
5842   ins_pipe( pipe_slow );
5843 %}
5844 
5845 instruct vabsnegD(vec dst, vec src, rRegI scratch) %{
5846   match(Set dst (AbsVD  src));
5847   match(Set dst (NegVD  src));
5848   effect(TEMP scratch);
5849   format %{ &quot;vabsnegd $dst,$src,[mask]\t# absneg packedD&quot; %}
5850   ins_encode %{
5851     int opcode = this-&gt;ideal_Opcode();
5852     uint vlen = vector_length(this);
5853     if (vlen == 2) {
5854       assert(UseSSE &gt;= 2, &quot;required&quot;);
5855       __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, $scratch$$Register);
5856     } else {
5857       int vlen_enc = vector_length_encoding(this);
5858       __ vabsnegd(opcode, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc, $scratch$$Register);
5859     }
5860   %}
5861   ins_pipe( pipe_slow );
5862 %}
5863 
5864 // --------------------------------- FMA --------------------------------------
5865 // a * b + c
5866 
5867 instruct vfmaF_reg(vec a, vec b, vec c) %{
5868   match(Set c (FmaVF  c (Binary a b)));
5869   format %{ &quot;fmaps $a,$b,$c\t# $c = $a * $b + $c fma packedF&quot; %}
5870   ins_cost(150);
5871   ins_encode %{
5872     assert(UseFMA, &quot;not enabled&quot;);
5873     int vector_len = vector_length_encoding(this);
5874     __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vector_len);
5875   %}
5876   ins_pipe( pipe_slow );
5877 %}
5878 
5879 instruct vfmaF_mem(vec a, memory b, vec c) %{
5880   match(Set c (FmaVF  c (Binary a (LoadVector b))));
5881   format %{ &quot;fmaps $a,$b,$c\t# $c = $a * $b + $c fma packedF&quot; %}
5882   ins_cost(150);
5883   ins_encode %{
5884     assert(UseFMA, &quot;not enabled&quot;);
5885     int vector_len = vector_length_encoding(this);
5886     __ vfmaf($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vector_len);
5887   %}
5888   ins_pipe( pipe_slow );
5889 %}
5890 
5891 instruct vfmaD_reg(vec a, vec b, vec c) %{
5892   match(Set c (FmaVD  c (Binary a b)));
5893   format %{ &quot;fmapd $a,$b,$c\t# $c = $a * $b + $c fma packedD&quot; %}
5894   ins_cost(150);
5895   ins_encode %{
5896     assert(UseFMA, &quot;not enabled&quot;);
5897     int vector_len = vector_length_encoding(this);
5898     __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $c$$XMMRegister, vector_len);
5899   %}
5900   ins_pipe( pipe_slow );
5901 %}
5902 
5903 instruct vfmaD_mem(vec a, memory b, vec c) %{
5904   match(Set c (FmaVD  c (Binary a (LoadVector b))));
5905   format %{ &quot;fmapd $a,$b,$c\t# $c = $a * $b + $c fma packedD&quot; %}
5906   ins_cost(150);
5907   ins_encode %{
5908     assert(UseFMA, &quot;not enabled&quot;);
5909     int vector_len = vector_length_encoding(this);
5910     __ vfmad($c$$XMMRegister, $a$$XMMRegister, $b$$Address, $c$$XMMRegister, vector_len);
5911   %}
5912   ins_pipe( pipe_slow );
5913 %}
5914 
5915 // --------------------------------- Vector Multiply Add --------------------------------------
5916 
5917 instruct vmuladdS2I_reg_sse(vec dst, vec src1) %{
5918   predicate(UseAVX == 0);
5919   match(Set dst (MulAddVS2VI dst src1));
5920   format %{ &quot;pmaddwd $dst,$dst,$src1\t! muladd packedStoI&quot; %}
5921   ins_encode %{
5922     __ pmaddwd($dst$$XMMRegister, $src1$$XMMRegister);
5923   %}
5924   ins_pipe( pipe_slow );
5925 %}
5926 
5927 instruct vmuladdS2I_reg_avx(vec dst, vec src1, vec src2) %{
5928   predicate(UseAVX &gt; 0);
5929   match(Set dst (MulAddVS2VI src1 src2));
5930   format %{ &quot;vpmaddwd $dst,$src1,$src2\t! muladd packedStoI&quot; %}
5931   ins_encode %{
5932     int vector_len = vector_length_encoding(this);
5933     __ vpmaddwd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5934   %}
5935   ins_pipe( pipe_slow );
5936 %}
5937 
5938 // --------------------------------- Vector Multiply Add Add ----------------------------------
5939 
5940 instruct vmuladdaddS2I_reg(vec dst, vec src1, vec src2) %{
5941   predicate(VM_Version::supports_vnni());
5942   match(Set dst (AddVI (MulAddVS2VI src1 src2) dst));
5943   format %{ &quot;evpdpwssd $dst,$src1,$src2\t! muladdadd packedStoI&quot; %}
5944   ins_encode %{
5945     assert(UseAVX &gt; 2, &quot;required&quot;);
5946     int vector_len = vector_length_encoding(this);
5947     __ evpdpwssd($dst$$XMMRegister, $src1$$XMMRegister, $src2$$XMMRegister, vector_len);
5948   %}
5949   ins_pipe( pipe_slow );
5950   ins_cost(10);
5951 %}
5952 
5953 // --------------------------------- PopCount --------------------------------------
5954 
5955 instruct vpopcountI(vec dst, vec src) %{
5956   match(Set dst (PopCountVI src));
5957   format %{ &quot;vpopcntd  $dst,$src\t! vector popcount packedI&quot; %}
5958   ins_encode %{
5959     assert(UsePopCountInstruction, &quot;not enabled&quot;);
5960 
5961     int vector_len = vector_length_encoding(this);
5962     __ vpopcntd($dst$$XMMRegister, $src$$XMMRegister, vector_len);
5963   %}
5964   ins_pipe( pipe_slow );
5965 %}
    </pre>
  </body>
</html>