<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &lt;sys/types.h&gt;
  27 
  28 #include &quot;precompiled.hpp&quot;
  29 #include &quot;jvm.h&quot;
  30 #include &quot;asm/assembler.hpp&quot;
  31 #include &quot;asm/assembler.inline.hpp&quot;
  32 #include &quot;gc/shared/barrierSet.hpp&quot;
  33 #include &quot;gc/shared/cardTable.hpp&quot;
  34 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  35 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
  36 #include &quot;interpreter/interpreter.hpp&quot;
  37 #include &quot;compiler/disassembler.hpp&quot;
  38 #include &quot;memory/resourceArea.hpp&quot;
  39 #include &quot;memory/universe.hpp&quot;
  40 #include &quot;nativeInst_aarch64.hpp&quot;
  41 #include &quot;oops/accessDecorators.hpp&quot;
  42 #include &quot;oops/compressedOops.inline.hpp&quot;
  43 #include &quot;oops/klass.inline.hpp&quot;
  44 #include &quot;runtime/biasedLocking.hpp&quot;
  45 #include &quot;runtime/icache.hpp&quot;
  46 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  47 #include &quot;runtime/jniHandles.inline.hpp&quot;
  48 #include &quot;runtime/sharedRuntime.hpp&quot;
  49 #include &quot;runtime/thread.hpp&quot;
<a name="1" id="anc1"></a><span class="line-added">  50 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  51 #ifdef COMPILER1
  52 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  53 #endif
  54 #ifdef COMPILER2
  55 #include &quot;oops/oop.hpp&quot;
  56 #include &quot;opto/compile.hpp&quot;
  57 #include &quot;opto/intrinsicnode.hpp&quot;
  58 #include &quot;opto/node.hpp&quot;
<a name="2" id="anc2"></a><span class="line-added">  59 #include &quot;opto/output.hpp&quot;</span>
  60 #endif
  61 
  62 #ifdef PRODUCT
  63 #define BLOCK_COMMENT(str) /* nothing */
  64 #define STOP(error) stop(error)
  65 #else
  66 #define BLOCK_COMMENT(str) block_comment(str)
  67 #define STOP(error) block_comment(error); stop(error)
  68 #endif
  69 
  70 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  71 
  72 // Patch any kind of instruction; there may be several instructions.
  73 // Return the total length (in bytes) of the instructions.
  74 int MacroAssembler::pd_patch_instruction_size(address branch, address target) {
  75   int instructions = 1;
  76   assert((uint64_t)target &lt; (1ul &lt;&lt; 48), &quot;48-bit overflow in address constant&quot;);
  77   long offset = (target - branch) &gt;&gt; 2;
  78   unsigned insn = *(unsigned*)branch;
  79   if ((Instruction_aarch64::extract(insn, 29, 24) &amp; 0b111011) == 0b011000) {
  80     // Load register (literal)
  81     Instruction_aarch64::spatch(branch, 23, 5, offset);
  82   } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {
  83     // Unconditional branch (immediate)
  84     Instruction_aarch64::spatch(branch, 25, 0, offset);
  85   } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {
  86     // Conditional branch (immediate)
  87     Instruction_aarch64::spatch(branch, 23, 5, offset);
  88   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {
  89     // Compare &amp; branch (immediate)
  90     Instruction_aarch64::spatch(branch, 23, 5, offset);
  91   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {
  92     // Test &amp; branch (immediate)
  93     Instruction_aarch64::spatch(branch, 18, 5, offset);
  94   } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {
  95     // PC-rel. addressing
  96     offset = target-branch;
  97     int shift = Instruction_aarch64::extract(insn, 31, 31);
  98     if (shift) {
  99       u_int64_t dest = (u_int64_t)target;
 100       uint64_t pc_page = (uint64_t)branch &gt;&gt; 12;
 101       uint64_t adr_page = (uint64_t)target &gt;&gt; 12;
 102       unsigned offset_lo = dest &amp; 0xfff;
 103       offset = adr_page - pc_page;
 104 
 105       // We handle 4 types of PC relative addressing
 106       //   1 - adrp    Rx, target_page
 107       //       ldr/str Ry, [Rx, #offset_in_page]
 108       //   2 - adrp    Rx, target_page
 109       //       add     Ry, Rx, #offset_in_page
 110       //   3 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 111       //       movk    Rx, #imm16&lt;&lt;32
 112       //   4 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 113       // In the first 3 cases we must check that Rx is the same in the adrp and the
 114       // subsequent ldr/str, add or movk instruction. Otherwise we could accidentally end
 115       // up treating a type 4 relocation as a type 1, 2 or 3 just because it happened
 116       // to be followed by a random unrelated ldr/str, add or movk instruction.
 117       //
 118       unsigned insn2 = ((unsigned*)branch)[1];
 119       if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &amp;&amp;
 120                 Instruction_aarch64::extract(insn, 4, 0) ==
 121                         Instruction_aarch64::extract(insn2, 9, 5)) {
 122         // Load/store register (unsigned immediate)
 123         unsigned size = Instruction_aarch64::extract(insn2, 31, 30);
 124         Instruction_aarch64::patch(branch + sizeof (unsigned),
 125                                     21, 10, offset_lo &gt;&gt; size);
 126         guarantee(((dest &gt;&gt; size) &lt;&lt; size) == dest, &quot;misaligned target&quot;);
 127         instructions = 2;
 128       } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &amp;&amp;
 129                 Instruction_aarch64::extract(insn, 4, 0) ==
 130                         Instruction_aarch64::extract(insn2, 4, 0)) {
 131         // add (immediate)
 132         Instruction_aarch64::patch(branch + sizeof (unsigned),
 133                                    21, 10, offset_lo);
 134         instructions = 2;
 135       } else if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110 &amp;&amp;
 136                    Instruction_aarch64::extract(insn, 4, 0) ==
 137                      Instruction_aarch64::extract(insn2, 4, 0)) {
 138         // movk #imm16&lt;&lt;32
 139         Instruction_aarch64::patch(branch + 4, 20, 5, (uint64_t)target &gt;&gt; 32);
 140         long dest = ((long)target &amp; 0xffffffffL) | ((long)branch &amp; 0xffff00000000L);
 141         long pc_page = (long)branch &gt;&gt; 12;
 142         long adr_page = (long)dest &gt;&gt; 12;
 143         offset = adr_page - pc_page;
 144         instructions = 2;
 145       }
 146     }
 147     int offset_lo = offset &amp; 3;
 148     offset &gt;&gt;= 2;
 149     Instruction_aarch64::spatch(branch, 23, 5, offset);
 150     Instruction_aarch64::patch(branch, 30, 29, offset_lo);
 151   } else if (Instruction_aarch64::extract(insn, 31, 21) == 0b11010010100) {
 152     u_int64_t dest = (u_int64_t)target;
 153     // Move wide constant
 154     assert(nativeInstruction_at(branch+4)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 155     assert(nativeInstruction_at(branch+8)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 156     Instruction_aarch64::patch(branch, 20, 5, dest &amp; 0xffff);
 157     Instruction_aarch64::patch(branch+4, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 158     Instruction_aarch64::patch(branch+8, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 159     assert(target_addr_for_insn(branch) == target, &quot;should be&quot;);
 160     instructions = 3;
 161   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 162              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 163     // nothing to do
 164     assert(target == 0, &quot;did not expect to relocate target for polling page load&quot;);
 165   } else {
 166     ShouldNotReachHere();
 167   }
 168   return instructions * NativeInstruction::instruction_size;
 169 }
 170 
 171 int MacroAssembler::patch_oop(address insn_addr, address o) {
 172   int instructions;
 173   unsigned insn = *(unsigned*)insn_addr;
 174   assert(nativeInstruction_at(insn_addr+4)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 175 
 176   // OOPs are either narrow (32 bits) or wide (48 bits).  We encode
 177   // narrow OOPs by setting the upper 16 bits in the first
 178   // instruction.
 179   if (Instruction_aarch64::extract(insn, 31, 21) == 0b11010010101) {
 180     // Move narrow OOP
 181     narrowOop n = CompressedOops::encode((oop)o);
 182     Instruction_aarch64::patch(insn_addr, 20, 5, n &gt;&gt; 16);
 183     Instruction_aarch64::patch(insn_addr+4, 20, 5, n &amp; 0xffff);
 184     instructions = 2;
 185   } else {
 186     // Move wide OOP
 187     assert(nativeInstruction_at(insn_addr+8)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 188     uintptr_t dest = (uintptr_t)o;
 189     Instruction_aarch64::patch(insn_addr, 20, 5, dest &amp; 0xffff);
 190     Instruction_aarch64::patch(insn_addr+4, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 191     Instruction_aarch64::patch(insn_addr+8, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 192     instructions = 3;
 193   }
 194   return instructions * NativeInstruction::instruction_size;
 195 }
 196 
 197 int MacroAssembler::patch_narrow_klass(address insn_addr, narrowKlass n) {
 198   // Metatdata pointers are either narrow (32 bits) or wide (48 bits).
 199   // We encode narrow ones by setting the upper 16 bits in the first
 200   // instruction.
 201   NativeInstruction *insn = nativeInstruction_at(insn_addr);
 202   assert(Instruction_aarch64::extract(insn-&gt;encoding(), 31, 21) == 0b11010010101 &amp;&amp;
 203          nativeInstruction_at(insn_addr+4)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 204 
 205   Instruction_aarch64::patch(insn_addr, 20, 5, n &gt;&gt; 16);
 206   Instruction_aarch64::patch(insn_addr+4, 20, 5, n &amp; 0xffff);
 207   return 2 * NativeInstruction::instruction_size;
 208 }
 209 
 210 address MacroAssembler::target_addr_for_insn(address insn_addr, unsigned insn) {
 211   long offset = 0;
 212   if ((Instruction_aarch64::extract(insn, 29, 24) &amp; 0b011011) == 0b00011000) {
 213     // Load register (literal)
 214     offset = Instruction_aarch64::sextract(insn, 23, 5);
 215     return address(((uint64_t)insn_addr + (offset &lt;&lt; 2)));
 216   } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {
 217     // Unconditional branch (immediate)
 218     offset = Instruction_aarch64::sextract(insn, 25, 0);
 219   } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {
 220     // Conditional branch (immediate)
 221     offset = Instruction_aarch64::sextract(insn, 23, 5);
 222   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {
 223     // Compare &amp; branch (immediate)
 224     offset = Instruction_aarch64::sextract(insn, 23, 5);
 225    } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {
 226     // Test &amp; branch (immediate)
 227     offset = Instruction_aarch64::sextract(insn, 18, 5);
 228   } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {
 229     // PC-rel. addressing
 230     offset = Instruction_aarch64::extract(insn, 30, 29);
 231     offset |= Instruction_aarch64::sextract(insn, 23, 5) &lt;&lt; 2;
 232     int shift = Instruction_aarch64::extract(insn, 31, 31) ? 12 : 0;
 233     if (shift) {
 234       offset &lt;&lt;= shift;
 235       uint64_t target_page = ((uint64_t)insn_addr) + offset;
 236       target_page &amp;= ((uint64_t)-1) &lt;&lt; shift;
 237       // Return the target address for the following sequences
 238       //   1 - adrp    Rx, target_page
 239       //       ldr/str Ry, [Rx, #offset_in_page]
 240       //   2 - adrp    Rx, target_page
 241       //       add     Ry, Rx, #offset_in_page
 242       //   3 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 243       //       movk    Rx, #imm12&lt;&lt;32
 244       //   4 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 245       //
 246       // In the first two cases  we check that the register is the same and
 247       // return the target_page + the offset within the page.
 248       // Otherwise we assume it is a page aligned relocation and return
 249       // the target page only.
 250       //
 251       unsigned insn2 = ((unsigned*)insn_addr)[1];
 252       if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &amp;&amp;
 253                 Instruction_aarch64::extract(insn, 4, 0) ==
 254                         Instruction_aarch64::extract(insn2, 9, 5)) {
 255         // Load/store register (unsigned immediate)
 256         unsigned int byte_offset = Instruction_aarch64::extract(insn2, 21, 10);
 257         unsigned int size = Instruction_aarch64::extract(insn2, 31, 30);
 258         return address(target_page + (byte_offset &lt;&lt; size));
 259       } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &amp;&amp;
 260                 Instruction_aarch64::extract(insn, 4, 0) ==
 261                         Instruction_aarch64::extract(insn2, 4, 0)) {
 262         // add (immediate)
 263         unsigned int byte_offset = Instruction_aarch64::extract(insn2, 21, 10);
 264         return address(target_page + byte_offset);
 265       } else {
 266         if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110  &amp;&amp;
 267                Instruction_aarch64::extract(insn, 4, 0) ==
 268                  Instruction_aarch64::extract(insn2, 4, 0)) {
 269           target_page = (target_page &amp; 0xffffffff) |
 270                          ((uint64_t)Instruction_aarch64::extract(insn2, 20, 5) &lt;&lt; 32);
 271         }
 272         return (address)target_page;
 273       }
 274     } else {
 275       ShouldNotReachHere();
 276     }
 277   } else if (Instruction_aarch64::extract(insn, 31, 23) == 0b110100101) {
 278     u_int32_t *insns = (u_int32_t *)insn_addr;
 279     // Move wide constant: movz, movk, movk.  See movptr().
 280     assert(nativeInstruction_at(insns+1)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 281     assert(nativeInstruction_at(insns+2)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 282     return address(u_int64_t(Instruction_aarch64::extract(insns[0], 20, 5))
 283                    + (u_int64_t(Instruction_aarch64::extract(insns[1], 20, 5)) &lt;&lt; 16)
 284                    + (u_int64_t(Instruction_aarch64::extract(insns[2], 20, 5)) &lt;&lt; 32));
 285   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 286              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 287     return 0;
 288   } else {
 289     ShouldNotReachHere();
 290   }
 291   return address(((uint64_t)insn_addr + (offset &lt;&lt; 2)));
 292 }
 293 
 294 void MacroAssembler::safepoint_poll(Label&amp; slow_path) {
 295   if (SafepointMechanism::uses_thread_local_poll()) {
 296     ldr(rscratch1, Address(rthread, Thread::polling_page_offset()));
 297     tbnz(rscratch1, exact_log2(SafepointMechanism::poll_bit()), slow_path);
 298   } else {
 299     unsigned long offset;
 300     adrp(rscratch1, ExternalAddress(SafepointSynchronize::address_of_state()), offset);
 301     ldrw(rscratch1, Address(rscratch1, offset));
 302     assert(SafepointSynchronize::_not_synchronized == 0, &quot;rewrite this code&quot;);
 303     cbnz(rscratch1, slow_path);
 304   }
 305 }
 306 
 307 // Just like safepoint_poll, but use an acquiring load for thread-
 308 // local polling.
 309 //
 310 // We need an acquire here to ensure that any subsequent load of the
 311 // global SafepointSynchronize::_state flag is ordered after this load
 312 // of the local Thread::_polling page.  We don&#39;t want this poll to
 313 // return false (i.e. not safepointing) and a later poll of the global
 314 // SafepointSynchronize::_state spuriously to return true.
 315 //
 316 // This is to avoid a race when we&#39;re in a native-&gt;Java transition
 317 // racing the code which wakes up from a safepoint.
 318 //
 319 void MacroAssembler::safepoint_poll_acquire(Label&amp; slow_path) {
 320   if (SafepointMechanism::uses_thread_local_poll()) {
 321     lea(rscratch1, Address(rthread, Thread::polling_page_offset()));
 322     ldar(rscratch1, rscratch1);
 323     tbnz(rscratch1, exact_log2(SafepointMechanism::poll_bit()), slow_path);
 324   } else {
 325     safepoint_poll(slow_path);
 326   }
 327 }
 328 
 329 void MacroAssembler::reset_last_Java_frame(bool clear_fp) {
 330   // we must set sp to zero to clear frame
 331   str(zr, Address(rthread, JavaThread::last_Java_sp_offset()));
 332 
 333   // must clear fp, so that compiled frames are not confused; it is
 334   // possible that we need it only for debugging
 335   if (clear_fp) {
 336     str(zr, Address(rthread, JavaThread::last_Java_fp_offset()));
 337   }
 338 
 339   // Always clear the pc because it could have been set by make_walkable()
 340   str(zr, Address(rthread, JavaThread::last_Java_pc_offset()));
 341 }
 342 
 343 // Calls to C land
 344 //
 345 // When entering C land, the rfp, &amp; resp of the last Java frame have to be recorded
 346 // in the (thread-local) JavaThread object. When leaving C land, the last Java fp
 347 // has to be reset to 0. This is required to allow proper stack traversal.
 348 void MacroAssembler::set_last_Java_frame(Register last_java_sp,
 349                                          Register last_java_fp,
 350                                          Register last_java_pc,
 351                                          Register scratch) {
 352 
 353   if (last_java_pc-&gt;is_valid()) {
 354       str(last_java_pc, Address(rthread,
 355                                 JavaThread::frame_anchor_offset()
 356                                 + JavaFrameAnchor::last_Java_pc_offset()));
 357     }
 358 
 359   // determine last_java_sp register
 360   if (last_java_sp == sp) {
 361     mov(scratch, sp);
 362     last_java_sp = scratch;
 363   } else if (!last_java_sp-&gt;is_valid()) {
 364     last_java_sp = esp;
 365   }
 366 
 367   str(last_java_sp, Address(rthread, JavaThread::last_Java_sp_offset()));
 368 
 369   // last_java_fp is optional
 370   if (last_java_fp-&gt;is_valid()) {
 371     str(last_java_fp, Address(rthread, JavaThread::last_Java_fp_offset()));
 372   }
 373 }
 374 
 375 void MacroAssembler::set_last_Java_frame(Register last_java_sp,
 376                                          Register last_java_fp,
 377                                          address  last_java_pc,
 378                                          Register scratch) {
 379   assert(last_java_pc != NULL, &quot;must provide a valid PC&quot;);
 380 
 381   adr(scratch, last_java_pc);
 382   str(scratch, Address(rthread,
 383                        JavaThread::frame_anchor_offset()
 384                        + JavaFrameAnchor::last_Java_pc_offset()));
 385 
 386   set_last_Java_frame(last_java_sp, last_java_fp, noreg, scratch);
 387 }
 388 
 389 void MacroAssembler::set_last_Java_frame(Register last_java_sp,
 390                                          Register last_java_fp,
 391                                          Label &amp;L,
 392                                          Register scratch) {
 393   if (L.is_bound()) {
 394     set_last_Java_frame(last_java_sp, last_java_fp, target(L), scratch);
 395   } else {
 396     InstructionMark im(this);
 397     L.add_patch_at(code(), locator());
 398     set_last_Java_frame(last_java_sp, last_java_fp, pc() /* Patched later */, scratch);
 399   }
 400 }
 401 
 402 void MacroAssembler::far_call(Address entry, CodeBuffer *cbuf, Register tmp) {
 403   assert(ReservedCodeCacheSize &lt; 4*G, &quot;branch out of range&quot;);
 404   assert(CodeCache::find_blob(entry.target()) != NULL,
 405          &quot;destination of far call not found in code cache&quot;);
 406   if (far_branches()) {
 407     unsigned long offset;
 408     // We can use ADRP here because we know that the total size of
 409     // the code cache cannot exceed 2Gb.
 410     adrp(tmp, entry, offset);
 411     add(tmp, tmp, offset);
 412     if (cbuf) cbuf-&gt;set_insts_mark();
 413     blr(tmp);
 414   } else {
 415     if (cbuf) cbuf-&gt;set_insts_mark();
 416     bl(entry);
 417   }
 418 }
 419 
 420 void MacroAssembler::far_jump(Address entry, CodeBuffer *cbuf, Register tmp) {
 421   assert(ReservedCodeCacheSize &lt; 4*G, &quot;branch out of range&quot;);
 422   assert(CodeCache::find_blob(entry.target()) != NULL,
 423          &quot;destination of far call not found in code cache&quot;);
 424   if (far_branches()) {
 425     unsigned long offset;
 426     // We can use ADRP here because we know that the total size of
 427     // the code cache cannot exceed 2Gb.
 428     adrp(tmp, entry, offset);
 429     add(tmp, tmp, offset);
 430     if (cbuf) cbuf-&gt;set_insts_mark();
 431     br(tmp);
 432   } else {
 433     if (cbuf) cbuf-&gt;set_insts_mark();
 434     b(entry);
 435   }
 436 }
 437 
 438 void MacroAssembler::reserved_stack_check() {
 439     // testing if reserved zone needs to be enabled
 440     Label no_reserved_zone_enabling;
 441 
 442     ldr(rscratch1, Address(rthread, JavaThread::reserved_stack_activation_offset()));
 443     cmp(sp, rscratch1);
 444     br(Assembler::LO, no_reserved_zone_enabling);
 445 
 446     enter();   // LR and FP are live.
 447     lea(rscratch1, CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone));
 448     mov(c_rarg0, rthread);
 449     blr(rscratch1);
 450     leave();
 451 
 452     // We have already removed our own frame.
 453     // throw_delayed_StackOverflowError will think that it&#39;s been
 454     // called by our caller.
 455     lea(rscratch1, RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()));
 456     br(rscratch1);
 457     should_not_reach_here();
 458 
 459     bind(no_reserved_zone_enabling);
 460 }
 461 
 462 int MacroAssembler::biased_locking_enter(Register lock_reg,
 463                                          Register obj_reg,
 464                                          Register swap_reg,
 465                                          Register tmp_reg,
 466                                          bool swap_reg_contains_mark,
 467                                          Label&amp; done,
 468                                          Label* slow_case,
 469                                          BiasedLockingCounters* counters) {
 470   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 471   assert_different_registers(lock_reg, obj_reg, swap_reg);
 472 
 473   if (PrintBiasedLockingStatistics &amp;&amp; counters == NULL)
 474     counters = BiasedLocking::counters();
 475 
 476   assert_different_registers(lock_reg, obj_reg, swap_reg, tmp_reg, rscratch1, rscratch2, noreg);
 477   assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits, &quot;biased locking makes assumptions about bit layout&quot;);
 478   Address mark_addr      (obj_reg, oopDesc::mark_offset_in_bytes());
 479   Address klass_addr     (obj_reg, oopDesc::klass_offset_in_bytes());
 480   Address saved_mark_addr(lock_reg, 0);
 481 
 482   // Biased locking
 483   // See whether the lock is currently biased toward our thread and
 484   // whether the epoch is still valid
 485   // Note that the runtime guarantees sufficient alignment of JavaThread
 486   // pointers to allow age to be placed into low bits
 487   // First check to see whether biasing is even enabled for this object
 488   Label cas_label;
 489   int null_check_offset = -1;
 490   if (!swap_reg_contains_mark) {
 491     null_check_offset = offset();
 492     ldr(swap_reg, mark_addr);
 493   }
 494   andr(tmp_reg, swap_reg, markWord::biased_lock_mask_in_place);
 495   cmp(tmp_reg, (u1)markWord::biased_lock_pattern);
 496   br(Assembler::NE, cas_label);
 497   // The bias pattern is present in the object&#39;s header. Need to check
 498   // whether the bias owner and the epoch are both still current.
 499   load_prototype_header(tmp_reg, obj_reg);
 500   orr(tmp_reg, tmp_reg, rthread);
 501   eor(tmp_reg, swap_reg, tmp_reg);
 502   andr(tmp_reg, tmp_reg, ~((int) markWord::age_mask_in_place));
 503   if (counters != NULL) {
 504     Label around;
 505     cbnz(tmp_reg, around);
 506     atomic_incw(Address((address)counters-&gt;biased_lock_entry_count_addr()), tmp_reg, rscratch1, rscratch2);
 507     b(done);
 508     bind(around);
 509   } else {
 510     cbz(tmp_reg, done);
 511   }
 512 
 513   Label try_revoke_bias;
 514   Label try_rebias;
 515 
 516   // At this point we know that the header has the bias pattern and
 517   // that we are not the bias owner in the current epoch. We need to
 518   // figure out more details about the state of the header in order to
 519   // know what operations can be legally performed on the object&#39;s
 520   // header.
 521 
 522   // If the low three bits in the xor result aren&#39;t clear, that means
 523   // the prototype header is no longer biased and we have to revoke
 524   // the bias on this object.
 525   andr(rscratch1, tmp_reg, markWord::biased_lock_mask_in_place);
 526   cbnz(rscratch1, try_revoke_bias);
 527 
 528   // Biasing is still enabled for this data type. See whether the
 529   // epoch of the current bias is still valid, meaning that the epoch
 530   // bits of the mark word are equal to the epoch bits of the
 531   // prototype header. (Note that the prototype header&#39;s epoch bits
 532   // only change at a safepoint.) If not, attempt to rebias the object
 533   // toward the current thread. Note that we must be absolutely sure
 534   // that the current epoch is invalid in order to do this because
 535   // otherwise the manipulations it performs on the mark word are
 536   // illegal.
 537   andr(rscratch1, tmp_reg, markWord::epoch_mask_in_place);
 538   cbnz(rscratch1, try_rebias);
 539 
 540   // The epoch of the current bias is still valid but we know nothing
 541   // about the owner; it might be set or it might be clear. Try to
 542   // acquire the bias of the object using an atomic operation. If this
 543   // fails we will go in to the runtime to revoke the object&#39;s bias.
 544   // Note that we first construct the presumed unbiased header so we
 545   // don&#39;t accidentally blow away another thread&#39;s valid bias.
 546   {
 547     Label here;
 548     mov(rscratch1, markWord::biased_lock_mask_in_place | markWord::age_mask_in_place | markWord::epoch_mask_in_place);
 549     andr(swap_reg, swap_reg, rscratch1);
 550     orr(tmp_reg, swap_reg, rthread);
 551     cmpxchg_obj_header(swap_reg, tmp_reg, obj_reg, rscratch1, here, slow_case);
 552     // If the biasing toward our thread failed, this means that
 553     // another thread succeeded in biasing it toward itself and we
 554     // need to revoke that bias. The revocation will occur in the
 555     // interpreter runtime in the slow case.
 556     bind(here);
 557     if (counters != NULL) {
 558       atomic_incw(Address((address)counters-&gt;anonymously_biased_lock_entry_count_addr()),
 559                   tmp_reg, rscratch1, rscratch2);
 560     }
 561   }
 562   b(done);
 563 
 564   bind(try_rebias);
 565   // At this point we know the epoch has expired, meaning that the
 566   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
 567   // circumstances _only_, we are allowed to use the current header&#39;s
 568   // value as the comparison value when doing the cas to acquire the
 569   // bias in the current epoch. In other words, we allow transfer of
 570   // the bias from one thread to another directly in this situation.
 571   //
 572   // FIXME: due to a lack of registers we currently blow away the age
 573   // bits in this situation. Should attempt to preserve them.
 574   {
 575     Label here;
 576     load_prototype_header(tmp_reg, obj_reg);
 577     orr(tmp_reg, rthread, tmp_reg);
 578     cmpxchg_obj_header(swap_reg, tmp_reg, obj_reg, rscratch1, here, slow_case);
 579     // If the biasing toward our thread failed, then another thread
 580     // succeeded in biasing it toward itself and we need to revoke that
 581     // bias. The revocation will occur in the runtime in the slow case.
 582     bind(here);
 583     if (counters != NULL) {
 584       atomic_incw(Address((address)counters-&gt;rebiased_lock_entry_count_addr()),
 585                   tmp_reg, rscratch1, rscratch2);
 586     }
 587   }
 588   b(done);
 589 
 590   bind(try_revoke_bias);
 591   // The prototype mark in the klass doesn&#39;t have the bias bit set any
 592   // more, indicating that objects of this data type are not supposed
 593   // to be biased any more. We are going to try to reset the mark of
 594   // this object to the prototype value and fall through to the
 595   // CAS-based locking scheme. Note that if our CAS fails, it means
 596   // that another thread raced us for the privilege of revoking the
 597   // bias of this particular object, so it&#39;s okay to continue in the
 598   // normal locking code.
 599   //
 600   // FIXME: due to a lack of registers we currently blow away the age
 601   // bits in this situation. Should attempt to preserve them.
 602   {
 603     Label here, nope;
 604     load_prototype_header(tmp_reg, obj_reg);
 605     cmpxchg_obj_header(swap_reg, tmp_reg, obj_reg, rscratch1, here, &amp;nope);
 606     bind(here);
 607 
 608     // Fall through to the normal CAS-based lock, because no matter what
 609     // the result of the above CAS, some thread must have succeeded in
 610     // removing the bias bit from the object&#39;s header.
 611     if (counters != NULL) {
 612       atomic_incw(Address((address)counters-&gt;revoked_lock_entry_count_addr()), tmp_reg,
 613                   rscratch1, rscratch2);
 614     }
 615     bind(nope);
 616   }
 617 
 618   bind(cas_label);
 619 
 620   return null_check_offset;
 621 }
 622 
 623 void MacroAssembler::biased_locking_exit(Register obj_reg, Register temp_reg, Label&amp; done) {
 624   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
 625 
 626   // Check for biased locking unlock case, which is a no-op
 627   // Note: we do not have to check the thread ID for two reasons.
 628   // First, the interpreter checks for IllegalMonitorStateException at
 629   // a higher level. Second, if the bias was revoked while we held the
 630   // lock, the object could not be rebiased toward another thread, so
 631   // the bias bit would be clear.
 632   ldr(temp_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
 633   andr(temp_reg, temp_reg, markWord::biased_lock_mask_in_place);
 634   cmp(temp_reg, (u1)markWord::biased_lock_pattern);
 635   br(Assembler::EQ, done);
 636 }
 637 
 638 static void pass_arg0(MacroAssembler* masm, Register arg) {
 639   if (c_rarg0 != arg ) {
 640     masm-&gt;mov(c_rarg0, arg);
 641   }
 642 }
 643 
 644 static void pass_arg1(MacroAssembler* masm, Register arg) {
 645   if (c_rarg1 != arg ) {
 646     masm-&gt;mov(c_rarg1, arg);
 647   }
 648 }
 649 
 650 static void pass_arg2(MacroAssembler* masm, Register arg) {
 651   if (c_rarg2 != arg ) {
 652     masm-&gt;mov(c_rarg2, arg);
 653   }
 654 }
 655 
 656 static void pass_arg3(MacroAssembler* masm, Register arg) {
 657   if (c_rarg3 != arg ) {
 658     masm-&gt;mov(c_rarg3, arg);
 659   }
 660 }
 661 
 662 void MacroAssembler::call_VM_base(Register oop_result,
 663                                   Register java_thread,
 664                                   Register last_java_sp,
 665                                   address  entry_point,
 666                                   int      number_of_arguments,
 667                                   bool     check_exceptions) {
 668    // determine java_thread register
 669   if (!java_thread-&gt;is_valid()) {
 670     java_thread = rthread;
 671   }
 672 
 673   // determine last_java_sp register
 674   if (!last_java_sp-&gt;is_valid()) {
 675     last_java_sp = esp;
 676   }
 677 
 678   // debugging support
 679   assert(number_of_arguments &gt;= 0   , &quot;cannot have negative number of arguments&quot;);
 680   assert(java_thread == rthread, &quot;unexpected register&quot;);
 681 #ifdef ASSERT
 682   // TraceBytecodes does not use r12 but saves it over the call, so don&#39;t verify
 683   // if ((UseCompressedOops || UseCompressedClassPointers) &amp;&amp; !TraceBytecodes) verify_heapbase(&quot;call_VM_base: heap base corrupted?&quot;);
 684 #endif // ASSERT
 685 
 686   assert(java_thread != oop_result  , &quot;cannot use the same register for java_thread &amp; oop_result&quot;);
 687   assert(java_thread != last_java_sp, &quot;cannot use the same register for java_thread &amp; last_java_sp&quot;);
 688 
 689   // push java thread (becomes first argument of C function)
 690 
 691   mov(c_rarg0, java_thread);
 692 
 693   // set last Java frame before call
 694   assert(last_java_sp != rfp, &quot;can&#39;t use rfp&quot;);
 695 
 696   Label l;
 697   set_last_Java_frame(last_java_sp, rfp, l, rscratch1);
 698 
 699   // do the call, remove parameters
 700   MacroAssembler::call_VM_leaf_base(entry_point, number_of_arguments, &amp;l);
 701 
 702   // reset last Java frame
 703   // Only interpreter should have to clear fp
 704   reset_last_Java_frame(true);
 705 
 706    // C++ interp handles this in the interpreter
 707   check_and_handle_popframe(java_thread);
 708   check_and_handle_earlyret(java_thread);
 709 
 710   if (check_exceptions) {
 711     // check for pending exceptions (java_thread is set upon return)
 712     ldr(rscratch1, Address(java_thread, in_bytes(Thread::pending_exception_offset())));
 713     Label ok;
 714     cbz(rscratch1, ok);
 715     lea(rscratch1, RuntimeAddress(StubRoutines::forward_exception_entry()));
 716     br(rscratch1);
 717     bind(ok);
 718   }
 719 
 720   // get oop result if there is one and reset the value in the thread
 721   if (oop_result-&gt;is_valid()) {
 722     get_vm_result(oop_result, java_thread);
 723   }
 724 }
 725 
 726 void MacroAssembler::call_VM_helper(Register oop_result, address entry_point, int number_of_arguments, bool check_exceptions) {
 727   call_VM_base(oop_result, noreg, noreg, entry_point, number_of_arguments, check_exceptions);
 728 }
 729 
 730 // Maybe emit a call via a trampoline.  If the code cache is small
 731 // trampolines won&#39;t be emitted.
 732 
 733 address MacroAssembler::trampoline_call(Address entry, CodeBuffer *cbuf) {
 734   assert(JavaThread::current()-&gt;is_Compiler_thread(), &quot;just checking&quot;);
 735   assert(entry.rspec().type() == relocInfo::runtime_call_type
 736          || entry.rspec().type() == relocInfo::opt_virtual_call_type
 737          || entry.rspec().type() == relocInfo::static_call_type
 738          || entry.rspec().type() == relocInfo::virtual_call_type, &quot;wrong reloc type&quot;);
 739 
 740   // We need a trampoline if branches are far.
 741   if (far_branches()) {
 742     bool in_scratch_emit_size = false;
 743 #ifdef COMPILER2
 744     // We don&#39;t want to emit a trampoline if C2 is generating dummy
 745     // code during its branch shortening phase.
 746     CompileTask* task = ciEnv::current()-&gt;task();
 747     in_scratch_emit_size =
 748       (task != NULL &amp;&amp; is_c2_compile(task-&gt;comp_level()) &amp;&amp;
<a name="3" id="anc3"></a><span class="line-modified"> 749        Compile::current()-&gt;output()-&gt;in_scratch_emit_size());</span>
 750 #endif
 751     if (!in_scratch_emit_size) {
 752       address stub = emit_trampoline_stub(offset(), entry.target());
 753       if (stub == NULL) {
 754         return NULL; // CodeCache is full
 755       }
 756     }
 757   }
 758 
 759   if (cbuf) cbuf-&gt;set_insts_mark();
 760   relocate(entry.rspec());
 761   if (!far_branches()) {
 762     bl(entry.target());
 763   } else {
 764     bl(pc());
 765   }
 766   // just need to return a non-null address
 767   return pc();
 768 }
 769 
 770 
 771 // Emit a trampoline stub for a call to a target which is too far away.
 772 //
 773 // code sequences:
 774 //
 775 // call-site:
 776 //   branch-and-link to &lt;destination&gt; or &lt;trampoline stub&gt;
 777 //
 778 // Related trampoline stub for this call site in the stub section:
 779 //   load the call target from the constant pool
 780 //   branch (LR still points to the call site above)
 781 
 782 address MacroAssembler::emit_trampoline_stub(int insts_call_instruction_offset,
 783                                              address dest) {
 784   // Max stub size: alignment nop, TrampolineStub.
 785   address stub = start_a_stub(NativeInstruction::instruction_size
 786                    + NativeCallTrampolineStub::instruction_size);
 787   if (stub == NULL) {
 788     return NULL;  // CodeBuffer::expand failed
 789   }
 790 
 791   // Create a trampoline stub relocation which relates this trampoline stub
 792   // with the call instruction at insts_call_instruction_offset in the
 793   // instructions code-section.
 794   align(wordSize);
 795   relocate(trampoline_stub_Relocation::spec(code()-&gt;insts()-&gt;start()
 796                                             + insts_call_instruction_offset));
 797   const int stub_start_offset = offset();
 798 
 799   // Now, create the trampoline stub&#39;s code:
 800   // - load the call
 801   // - call
 802   Label target;
 803   ldr(rscratch1, target);
 804   br(rscratch1);
 805   bind(target);
 806   assert(offset() - stub_start_offset == NativeCallTrampolineStub::data_offset,
 807          &quot;should be&quot;);
 808   emit_int64((int64_t)dest);
 809 
 810   const address stub_start_addr = addr_at(stub_start_offset);
 811 
 812   assert(is_NativeCallTrampolineStub_at(stub_start_addr), &quot;doesn&#39;t look like a trampoline&quot;);
 813 
 814   end_a_stub();
 815   return stub_start_addr;
 816 }
 817 
 818 void MacroAssembler::emit_static_call_stub() {
 819   // CompiledDirectStaticCall::set_to_interpreted knows the
 820   // exact layout of this stub.
 821 
 822   isb();
 823   mov_metadata(rmethod, (Metadata*)NULL);
 824 
 825   // Jump to the entry point of the i2c stub.
 826   movptr(rscratch1, 0);
 827   br(rscratch1);
 828 }
 829 
 830 void MacroAssembler::c2bool(Register x) {
 831   // implements x == 0 ? 0 : 1
 832   // note: must only look at least-significant byte of x
 833   //       since C-style booleans are stored in one byte
 834   //       only! (was bug)
 835   tst(x, 0xff);
 836   cset(x, Assembler::NE);
 837 }
 838 
 839 address MacroAssembler::ic_call(address entry, jint method_index) {
 840   RelocationHolder rh = virtual_call_Relocation::spec(pc(), method_index);
 841   // address const_ptr = long_constant((jlong)Universe::non_oop_word());
 842   // unsigned long offset;
 843   // ldr_constant(rscratch2, const_ptr);
 844   movptr(rscratch2, (uintptr_t)Universe::non_oop_word());
 845   return trampoline_call(Address(entry, rh));
 846 }
 847 
 848 // Implementation of call_VM versions
 849 
 850 void MacroAssembler::call_VM(Register oop_result,
 851                              address entry_point,
 852                              bool check_exceptions) {
 853   call_VM_helper(oop_result, entry_point, 0, check_exceptions);
 854 }
 855 
 856 void MacroAssembler::call_VM(Register oop_result,
 857                              address entry_point,
 858                              Register arg_1,
 859                              bool check_exceptions) {
 860   pass_arg1(this, arg_1);
 861   call_VM_helper(oop_result, entry_point, 1, check_exceptions);
 862 }
 863 
 864 void MacroAssembler::call_VM(Register oop_result,
 865                              address entry_point,
 866                              Register arg_1,
 867                              Register arg_2,
 868                              bool check_exceptions) {
 869   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
 870   pass_arg2(this, arg_2);
 871   pass_arg1(this, arg_1);
 872   call_VM_helper(oop_result, entry_point, 2, check_exceptions);
 873 }
 874 
 875 void MacroAssembler::call_VM(Register oop_result,
 876                              address entry_point,
 877                              Register arg_1,
 878                              Register arg_2,
 879                              Register arg_3,
 880                              bool check_exceptions) {
 881   assert(arg_1 != c_rarg3, &quot;smashed arg&quot;);
 882   assert(arg_2 != c_rarg3, &quot;smashed arg&quot;);
 883   pass_arg3(this, arg_3);
 884 
 885   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
 886   pass_arg2(this, arg_2);
 887 
 888   pass_arg1(this, arg_1);
 889   call_VM_helper(oop_result, entry_point, 3, check_exceptions);
 890 }
 891 
 892 void MacroAssembler::call_VM(Register oop_result,
 893                              Register last_java_sp,
 894                              address entry_point,
 895                              int number_of_arguments,
 896                              bool check_exceptions) {
 897   call_VM_base(oop_result, rthread, last_java_sp, entry_point, number_of_arguments, check_exceptions);
 898 }
 899 
 900 void MacroAssembler::call_VM(Register oop_result,
 901                              Register last_java_sp,
 902                              address entry_point,
 903                              Register arg_1,
 904                              bool check_exceptions) {
 905   pass_arg1(this, arg_1);
 906   call_VM(oop_result, last_java_sp, entry_point, 1, check_exceptions);
 907 }
 908 
 909 void MacroAssembler::call_VM(Register oop_result,
 910                              Register last_java_sp,
 911                              address entry_point,
 912                              Register arg_1,
 913                              Register arg_2,
 914                              bool check_exceptions) {
 915 
 916   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
 917   pass_arg2(this, arg_2);
 918   pass_arg1(this, arg_1);
 919   call_VM(oop_result, last_java_sp, entry_point, 2, check_exceptions);
 920 }
 921 
 922 void MacroAssembler::call_VM(Register oop_result,
 923                              Register last_java_sp,
 924                              address entry_point,
 925                              Register arg_1,
 926                              Register arg_2,
 927                              Register arg_3,
 928                              bool check_exceptions) {
 929   assert(arg_1 != c_rarg3, &quot;smashed arg&quot;);
 930   assert(arg_2 != c_rarg3, &quot;smashed arg&quot;);
 931   pass_arg3(this, arg_3);
 932   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
 933   pass_arg2(this, arg_2);
 934   pass_arg1(this, arg_1);
 935   call_VM(oop_result, last_java_sp, entry_point, 3, check_exceptions);
 936 }
 937 
 938 
 939 void MacroAssembler::get_vm_result(Register oop_result, Register java_thread) {
 940   ldr(oop_result, Address(java_thread, JavaThread::vm_result_offset()));
 941   str(zr, Address(java_thread, JavaThread::vm_result_offset()));
 942   verify_oop(oop_result, &quot;broken oop in call_VM_base&quot;);
 943 }
 944 
 945 void MacroAssembler::get_vm_result_2(Register metadata_result, Register java_thread) {
 946   ldr(metadata_result, Address(java_thread, JavaThread::vm_result_2_offset()));
 947   str(zr, Address(java_thread, JavaThread::vm_result_2_offset()));
 948 }
 949 
 950 void MacroAssembler::align(int modulus) {
 951   while (offset() % modulus != 0) nop();
 952 }
 953 
 954 // these are no-ops overridden by InterpreterMacroAssembler
 955 
 956 void MacroAssembler::check_and_handle_earlyret(Register java_thread) { }
 957 
 958 void MacroAssembler::check_and_handle_popframe(Register java_thread) { }
 959 
 960 
 961 RegisterOrConstant MacroAssembler::delayed_value_impl(intptr_t* delayed_value_addr,
 962                                                       Register tmp,
 963                                                       int offset) {
 964   intptr_t value = *delayed_value_addr;
 965   if (value != 0)
 966     return RegisterOrConstant(value + offset);
 967 
 968   // load indirectly to solve generation ordering problem
 969   ldr(tmp, ExternalAddress((address) delayed_value_addr));
 970 
 971   if (offset != 0)
 972     add(tmp, tmp, offset);
 973 
 974   return RegisterOrConstant(tmp);
 975 }
 976 
 977 // Look up the method for a megamorphic invokeinterface call.
 978 // The target method is determined by &lt;intf_klass, itable_index&gt;.
 979 // The receiver klass is in recv_klass.
 980 // On success, the result will be in method_result, and execution falls through.
 981 // On failure, execution transfers to the given label.
 982 void MacroAssembler::lookup_interface_method(Register recv_klass,
 983                                              Register intf_klass,
 984                                              RegisterOrConstant itable_index,
 985                                              Register method_result,
 986                                              Register scan_temp,
 987                                              Label&amp; L_no_such_interface,
 988                          bool return_method) {
 989   assert_different_registers(recv_klass, intf_klass, scan_temp);
 990   assert_different_registers(method_result, intf_klass, scan_temp);
 991   assert(recv_klass != method_result || !return_method,
 992      &quot;recv_klass can be destroyed when method isn&#39;t needed&quot;);
 993   assert(itable_index.is_constant() || itable_index.as_register() == method_result,
 994          &quot;caller must use same register for non-constant itable index as for method&quot;);
 995 
 996   // Compute start of first itableOffsetEntry (which is at the end of the vtable)
 997   int vtable_base = in_bytes(Klass::vtable_start_offset());
 998   int itentry_off = itableMethodEntry::method_offset_in_bytes();
 999   int scan_step   = itableOffsetEntry::size() * wordSize;
1000   int vte_size    = vtableEntry::size_in_bytes();
1001   assert(vte_size == wordSize, &quot;else adjust times_vte_scale&quot;);
1002 
1003   ldrw(scan_temp, Address(recv_klass, Klass::vtable_length_offset()));
1004 
1005   // %%% Could store the aligned, prescaled offset in the klassoop.
1006   // lea(scan_temp, Address(recv_klass, scan_temp, times_vte_scale, vtable_base));
1007   lea(scan_temp, Address(recv_klass, scan_temp, Address::lsl(3)));
1008   add(scan_temp, scan_temp, vtable_base);
1009 
1010   if (return_method) {
1011     // Adjust recv_klass by scaled itable_index, so we can free itable_index.
1012     assert(itableMethodEntry::size() * wordSize == wordSize, &quot;adjust the scaling in the code below&quot;);
1013     // lea(recv_klass, Address(recv_klass, itable_index, Address::times_ptr, itentry_off));
1014     lea(recv_klass, Address(recv_klass, itable_index, Address::lsl(3)));
1015     if (itentry_off)
1016       add(recv_klass, recv_klass, itentry_off);
1017   }
1018 
1019   // for (scan = klass-&gt;itable(); scan-&gt;interface() != NULL; scan += scan_step) {
1020   //   if (scan-&gt;interface() == intf) {
1021   //     result = (klass + scan-&gt;offset() + itable_index);
1022   //   }
1023   // }
1024   Label search, found_method;
1025 
1026   for (int peel = 1; peel &gt;= 0; peel--) {
1027     ldr(method_result, Address(scan_temp, itableOffsetEntry::interface_offset_in_bytes()));
1028     cmp(intf_klass, method_result);
1029 
1030     if (peel) {
1031       br(Assembler::EQ, found_method);
1032     } else {
1033       br(Assembler::NE, search);
1034       // (invert the test to fall through to found_method...)
1035     }
1036 
1037     if (!peel)  break;
1038 
1039     bind(search);
1040 
1041     // Check that the previous entry is non-null.  A null entry means that
1042     // the receiver class doesn&#39;t implement the interface, and wasn&#39;t the
1043     // same as when the caller was compiled.
1044     cbz(method_result, L_no_such_interface);
1045     add(scan_temp, scan_temp, scan_step);
1046   }
1047 
1048   bind(found_method);
1049 
1050   // Got a hit.
1051   if (return_method) {
1052     ldrw(scan_temp, Address(scan_temp, itableOffsetEntry::offset_offset_in_bytes()));
1053     ldr(method_result, Address(recv_klass, scan_temp, Address::uxtw(0)));
1054   }
1055 }
1056 
1057 // virtual method calling
1058 void MacroAssembler::lookup_virtual_method(Register recv_klass,
1059                                            RegisterOrConstant vtable_index,
1060                                            Register method_result) {
1061   const int base = in_bytes(Klass::vtable_start_offset());
1062   assert(vtableEntry::size() * wordSize == 8,
1063          &quot;adjust the scaling in the code below&quot;);
1064   int vtable_offset_in_bytes = base + vtableEntry::method_offset_in_bytes();
1065 
1066   if (vtable_index.is_register()) {
1067     lea(method_result, Address(recv_klass,
1068                                vtable_index.as_register(),
1069                                Address::lsl(LogBytesPerWord)));
1070     ldr(method_result, Address(method_result, vtable_offset_in_bytes));
1071   } else {
1072     vtable_offset_in_bytes += vtable_index.as_constant() * wordSize;
1073     ldr(method_result,
1074         form_address(rscratch1, recv_klass, vtable_offset_in_bytes, 0));
1075   }
1076 }
1077 
1078 void MacroAssembler::check_klass_subtype(Register sub_klass,
1079                            Register super_klass,
1080                            Register temp_reg,
1081                            Label&amp; L_success) {
1082   Label L_failure;
1083   check_klass_subtype_fast_path(sub_klass, super_klass, temp_reg,        &amp;L_success, &amp;L_failure, NULL);
1084   check_klass_subtype_slow_path(sub_klass, super_klass, temp_reg, noreg, &amp;L_success, NULL);
1085   bind(L_failure);
1086 }
1087 
1088 
1089 void MacroAssembler::check_klass_subtype_fast_path(Register sub_klass,
1090                                                    Register super_klass,
1091                                                    Register temp_reg,
1092                                                    Label* L_success,
1093                                                    Label* L_failure,
1094                                                    Label* L_slow_path,
1095                                         RegisterOrConstant super_check_offset) {
1096   assert_different_registers(sub_klass, super_klass, temp_reg);
1097   bool must_load_sco = (super_check_offset.constant_or_zero() == -1);
1098   if (super_check_offset.is_register()) {
1099     assert_different_registers(sub_klass, super_klass,
1100                                super_check_offset.as_register());
1101   } else if (must_load_sco) {
1102     assert(temp_reg != noreg, &quot;supply either a temp or a register offset&quot;);
1103   }
1104 
1105   Label L_fallthrough;
1106   int label_nulls = 0;
1107   if (L_success == NULL)   { L_success   = &amp;L_fallthrough; label_nulls++; }
1108   if (L_failure == NULL)   { L_failure   = &amp;L_fallthrough; label_nulls++; }
1109   if (L_slow_path == NULL) { L_slow_path = &amp;L_fallthrough; label_nulls++; }
1110   assert(label_nulls &lt;= 1, &quot;at most one NULL in the batch&quot;);
1111 
1112   int sc_offset = in_bytes(Klass::secondary_super_cache_offset());
1113   int sco_offset = in_bytes(Klass::super_check_offset_offset());
1114   Address super_check_offset_addr(super_klass, sco_offset);
1115 
1116   // Hacked jmp, which may only be used just before L_fallthrough.
1117 #define final_jmp(label)                                                \
1118   if (&amp;(label) == &amp;L_fallthrough) { /*do nothing*/ }                    \
1119   else                            b(label)                /*omit semi*/
1120 
1121   // If the pointers are equal, we are done (e.g., String[] elements).
1122   // This self-check enables sharing of secondary supertype arrays among
1123   // non-primary types such as array-of-interface.  Otherwise, each such
1124   // type would need its own customized SSA.
1125   // We move this check to the front of the fast path because many
1126   // type checks are in fact trivially successful in this manner,
1127   // so we get a nicely predicted branch right at the start of the check.
1128   cmp(sub_klass, super_klass);
1129   br(Assembler::EQ, *L_success);
1130 
1131   // Check the supertype display:
1132   if (must_load_sco) {
1133     ldrw(temp_reg, super_check_offset_addr);
1134     super_check_offset = RegisterOrConstant(temp_reg);
1135   }
1136   Address super_check_addr(sub_klass, super_check_offset);
1137   ldr(rscratch1, super_check_addr);
1138   cmp(super_klass, rscratch1); // load displayed supertype
1139 
1140   // This check has worked decisively for primary supers.
1141   // Secondary supers are sought in the super_cache (&#39;super_cache_addr&#39;).
1142   // (Secondary supers are interfaces and very deeply nested subtypes.)
1143   // This works in the same check above because of a tricky aliasing
1144   // between the super_cache and the primary super display elements.
1145   // (The &#39;super_check_addr&#39; can address either, as the case requires.)
1146   // Note that the cache is updated below if it does not help us find
1147   // what we need immediately.
1148   // So if it was a primary super, we can just fail immediately.
1149   // Otherwise, it&#39;s the slow path for us (no success at this point).
1150 
1151   if (super_check_offset.is_register()) {
1152     br(Assembler::EQ, *L_success);
1153     subs(zr, super_check_offset.as_register(), sc_offset);
1154     if (L_failure == &amp;L_fallthrough) {
1155       br(Assembler::EQ, *L_slow_path);
1156     } else {
1157       br(Assembler::NE, *L_failure);
1158       final_jmp(*L_slow_path);
1159     }
1160   } else if (super_check_offset.as_constant() == sc_offset) {
1161     // Need a slow path; fast failure is impossible.
1162     if (L_slow_path == &amp;L_fallthrough) {
1163       br(Assembler::EQ, *L_success);
1164     } else {
1165       br(Assembler::NE, *L_slow_path);
1166       final_jmp(*L_success);
1167     }
1168   } else {
1169     // No slow path; it&#39;s a fast decision.
1170     if (L_failure == &amp;L_fallthrough) {
1171       br(Assembler::EQ, *L_success);
1172     } else {
1173       br(Assembler::NE, *L_failure);
1174       final_jmp(*L_success);
1175     }
1176   }
1177 
1178   bind(L_fallthrough);
1179 
1180 #undef final_jmp
1181 }
1182 
1183 // These two are taken from x86, but they look generally useful
1184 
1185 // scans count pointer sized words at [addr] for occurence of value,
1186 // generic
1187 void MacroAssembler::repne_scan(Register addr, Register value, Register count,
1188                                 Register scratch) {
1189   Label Lloop, Lexit;
1190   cbz(count, Lexit);
1191   bind(Lloop);
1192   ldr(scratch, post(addr, wordSize));
1193   cmp(value, scratch);
1194   br(EQ, Lexit);
1195   sub(count, count, 1);
1196   cbnz(count, Lloop);
1197   bind(Lexit);
1198 }
1199 
1200 // scans count 4 byte words at [addr] for occurence of value,
1201 // generic
1202 void MacroAssembler::repne_scanw(Register addr, Register value, Register count,
1203                                 Register scratch) {
1204   Label Lloop, Lexit;
1205   cbz(count, Lexit);
1206   bind(Lloop);
1207   ldrw(scratch, post(addr, wordSize));
1208   cmpw(value, scratch);
1209   br(EQ, Lexit);
1210   sub(count, count, 1);
1211   cbnz(count, Lloop);
1212   bind(Lexit);
1213 }
1214 
1215 void MacroAssembler::check_klass_subtype_slow_path(Register sub_klass,
1216                                                    Register super_klass,
1217                                                    Register temp_reg,
1218                                                    Register temp2_reg,
1219                                                    Label* L_success,
1220                                                    Label* L_failure,
1221                                                    bool set_cond_codes) {
1222   assert_different_registers(sub_klass, super_klass, temp_reg);
1223   if (temp2_reg != noreg)
1224     assert_different_registers(sub_klass, super_klass, temp_reg, temp2_reg, rscratch1);
1225 #define IS_A_TEMP(reg) ((reg) == temp_reg || (reg) == temp2_reg)
1226 
1227   Label L_fallthrough;
1228   int label_nulls = 0;
1229   if (L_success == NULL)   { L_success   = &amp;L_fallthrough; label_nulls++; }
1230   if (L_failure == NULL)   { L_failure   = &amp;L_fallthrough; label_nulls++; }
1231   assert(label_nulls &lt;= 1, &quot;at most one NULL in the batch&quot;);
1232 
1233   // a couple of useful fields in sub_klass:
1234   int ss_offset = in_bytes(Klass::secondary_supers_offset());
1235   int sc_offset = in_bytes(Klass::secondary_super_cache_offset());
1236   Address secondary_supers_addr(sub_klass, ss_offset);
1237   Address super_cache_addr(     sub_klass, sc_offset);
1238 
1239   BLOCK_COMMENT(&quot;check_klass_subtype_slow_path&quot;);
1240 
1241   // Do a linear scan of the secondary super-klass chain.
1242   // This code is rarely used, so simplicity is a virtue here.
1243   // The repne_scan instruction uses fixed registers, which we must spill.
1244   // Don&#39;t worry too much about pre-existing connections with the input regs.
1245 
1246   assert(sub_klass != r0, &quot;killed reg&quot;); // killed by mov(r0, super)
1247   assert(sub_klass != r2, &quot;killed reg&quot;); // killed by lea(r2, &amp;pst_counter)
1248 
1249   RegSet pushed_registers;
1250   if (!IS_A_TEMP(r2))    pushed_registers += r2;
1251   if (!IS_A_TEMP(r5))    pushed_registers += r5;
1252 
1253   if (super_klass != r0 || UseCompressedOops) {
1254     if (!IS_A_TEMP(r0))   pushed_registers += r0;
1255   }
1256 
1257   push(pushed_registers, sp);
1258 
1259   // Get super_klass value into r0 (even if it was in r5 or r2).
1260   if (super_klass != r0) {
1261     mov(r0, super_klass);
1262   }
1263 
1264 #ifndef PRODUCT
1265   mov(rscratch2, (address)&amp;SharedRuntime::_partial_subtype_ctr);
1266   Address pst_counter_addr(rscratch2);
1267   ldr(rscratch1, pst_counter_addr);
1268   add(rscratch1, rscratch1, 1);
1269   str(rscratch1, pst_counter_addr);
1270 #endif //PRODUCT
1271 
1272   // We will consult the secondary-super array.
1273   ldr(r5, secondary_supers_addr);
1274   // Load the array length.
1275   ldrw(r2, Address(r5, Array&lt;Klass*&gt;::length_offset_in_bytes()));
1276   // Skip to start of data.
1277   add(r5, r5, Array&lt;Klass*&gt;::base_offset_in_bytes());
1278 
1279   cmp(sp, zr); // Clear Z flag; SP is never zero
1280   // Scan R2 words at [R5] for an occurrence of R0.
1281   // Set NZ/Z based on last compare.
1282   repne_scan(r5, r0, r2, rscratch1);
1283 
1284   // Unspill the temp. registers:
1285   pop(pushed_registers, sp);
1286 
1287   br(Assembler::NE, *L_failure);
1288 
1289   // Success.  Cache the super we found and proceed in triumph.
1290   str(super_klass, super_cache_addr);
1291 
1292   if (L_success != &amp;L_fallthrough) {
1293     b(*L_success);
1294   }
1295 
1296 #undef IS_A_TEMP
1297 
1298   bind(L_fallthrough);
1299 }
1300 
1301 void MacroAssembler::clinit_barrier(Register klass, Register scratch, Label* L_fast_path, Label* L_slow_path) {
1302   assert(L_fast_path != NULL || L_slow_path != NULL, &quot;at least one is required&quot;);
1303   assert_different_registers(klass, rthread, scratch);
1304 
1305   Label L_fallthrough, L_tmp;
1306   if (L_fast_path == NULL) {
1307     L_fast_path = &amp;L_fallthrough;
1308   } else if (L_slow_path == NULL) {
1309     L_slow_path = &amp;L_fallthrough;
1310   }
1311   // Fast path check: class is fully initialized
1312   ldrb(scratch, Address(klass, InstanceKlass::init_state_offset()));
1313   subs(zr, scratch, InstanceKlass::fully_initialized);
1314   br(Assembler::EQ, *L_fast_path);
1315 
1316   // Fast path check: current thread is initializer thread
1317   ldr(scratch, Address(klass, InstanceKlass::init_thread_offset()));
1318   cmp(rthread, scratch);
1319 
1320   if (L_slow_path == &amp;L_fallthrough) {
1321     br(Assembler::EQ, *L_fast_path);
1322     bind(*L_slow_path);
1323   } else if (L_fast_path == &amp;L_fallthrough) {
1324     br(Assembler::NE, *L_slow_path);
1325     bind(*L_fast_path);
1326   } else {
1327     Unimplemented();
1328   }
1329 }
1330 
1331 void MacroAssembler::verify_oop(Register reg, const char* s) {
1332   if (!VerifyOops) return;
1333 
1334   // Pass register number to verify_oop_subroutine
1335   const char* b = NULL;
1336   {
1337     ResourceMark rm;
1338     stringStream ss;
1339     ss.print(&quot;verify_oop: %s: %s&quot;, reg-&gt;name(), s);
1340     b = code_string(ss.as_string());
1341   }
1342   BLOCK_COMMENT(&quot;verify_oop {&quot;);
1343 
1344   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1345   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1346 
1347   mov(r0, reg);
1348   mov(rscratch1, (address)b);
1349 
1350   // call indirectly to solve generation ordering problem
1351   lea(rscratch2, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
1352   ldr(rscratch2, Address(rscratch2));
1353   blr(rscratch2);
1354 
1355   ldp(rscratch2, lr, Address(post(sp, 2 * wordSize)));
1356   ldp(r0, rscratch1, Address(post(sp, 2 * wordSize)));
1357 
1358   BLOCK_COMMENT(&quot;} verify_oop&quot;);
1359 }
1360 
1361 void MacroAssembler::verify_oop_addr(Address addr, const char* s) {
1362   if (!VerifyOops) return;
1363 
1364   const char* b = NULL;
1365   {
1366     ResourceMark rm;
1367     stringStream ss;
1368     ss.print(&quot;verify_oop_addr: %s&quot;, s);
1369     b = code_string(ss.as_string());
1370   }
1371   BLOCK_COMMENT(&quot;verify_oop_addr {&quot;);
1372 
1373   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1374   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1375 
1376   // addr may contain sp so we will have to adjust it based on the
1377   // pushes that we just did.
1378   if (addr.uses(sp)) {
1379     lea(r0, addr);
1380     ldr(r0, Address(r0, 4 * wordSize));
1381   } else {
1382     ldr(r0, addr);
1383   }
1384   mov(rscratch1, (address)b);
1385 
1386   // call indirectly to solve generation ordering problem
1387   lea(rscratch2, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
1388   ldr(rscratch2, Address(rscratch2));
1389   blr(rscratch2);
1390 
1391   ldp(rscratch2, lr, Address(post(sp, 2 * wordSize)));
1392   ldp(r0, rscratch1, Address(post(sp, 2 * wordSize)));
1393 
1394   BLOCK_COMMENT(&quot;} verify_oop_addr&quot;);
1395 }
1396 
1397 Address MacroAssembler::argument_address(RegisterOrConstant arg_slot,
1398                                          int extra_slot_offset) {
1399   // cf. TemplateTable::prepare_invoke(), if (load_receiver).
1400   int stackElementSize = Interpreter::stackElementSize;
1401   int offset = Interpreter::expr_offset_in_bytes(extra_slot_offset+0);
1402 #ifdef ASSERT
1403   int offset1 = Interpreter::expr_offset_in_bytes(extra_slot_offset+1);
1404   assert(offset1 - offset == stackElementSize, &quot;correct arithmetic&quot;);
1405 #endif
1406   if (arg_slot.is_constant()) {
1407     return Address(esp, arg_slot.as_constant() * stackElementSize
1408                    + offset);
1409   } else {
1410     add(rscratch1, esp, arg_slot.as_register(),
1411         ext::uxtx, exact_log2(stackElementSize));
1412     return Address(rscratch1, offset);
1413   }
1414 }
1415 
1416 void MacroAssembler::call_VM_leaf_base(address entry_point,
1417                                        int number_of_arguments,
1418                                        Label *retaddr) {
1419   Label E, L;
1420 
1421   stp(rscratch1, rmethod, Address(pre(sp, -2 * wordSize)));
1422 
1423   mov(rscratch1, entry_point);
1424   blr(rscratch1);
1425   if (retaddr)
1426     bind(*retaddr);
1427 
1428   ldp(rscratch1, rmethod, Address(post(sp, 2 * wordSize)));
1429   maybe_isb();
1430 }
1431 
1432 void MacroAssembler::call_VM_leaf(address entry_point, int number_of_arguments) {
1433   call_VM_leaf_base(entry_point, number_of_arguments);
1434 }
1435 
1436 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0) {
1437   pass_arg0(this, arg_0);
1438   call_VM_leaf_base(entry_point, 1);
1439 }
1440 
1441 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1442   pass_arg0(this, arg_0);
1443   pass_arg1(this, arg_1);
1444   call_VM_leaf_base(entry_point, 2);
1445 }
1446 
1447 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0,
1448                                   Register arg_1, Register arg_2) {
1449   pass_arg0(this, arg_0);
1450   pass_arg1(this, arg_1);
1451   pass_arg2(this, arg_2);
1452   call_VM_leaf_base(entry_point, 3);
1453 }
1454 
1455 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0) {
1456   pass_arg0(this, arg_0);
1457   MacroAssembler::call_VM_leaf_base(entry_point, 1);
1458 }
1459 
1460 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1461 
1462   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1463   pass_arg1(this, arg_1);
1464   pass_arg0(this, arg_0);
1465   MacroAssembler::call_VM_leaf_base(entry_point, 2);
1466 }
1467 
1468 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2) {
1469   assert(arg_0 != c_rarg2, &quot;smashed arg&quot;);
1470   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1471   pass_arg2(this, arg_2);
1472   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1473   pass_arg1(this, arg_1);
1474   pass_arg0(this, arg_0);
1475   MacroAssembler::call_VM_leaf_base(entry_point, 3);
1476 }
1477 
1478 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2, Register arg_3) {
1479   assert(arg_0 != c_rarg3, &quot;smashed arg&quot;);
1480   assert(arg_1 != c_rarg3, &quot;smashed arg&quot;);
1481   assert(arg_2 != c_rarg3, &quot;smashed arg&quot;);
1482   pass_arg3(this, arg_3);
1483   assert(arg_0 != c_rarg2, &quot;smashed arg&quot;);
1484   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1485   pass_arg2(this, arg_2);
1486   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1487   pass_arg1(this, arg_1);
1488   pass_arg0(this, arg_0);
1489   MacroAssembler::call_VM_leaf_base(entry_point, 4);
1490 }
1491 
1492 void MacroAssembler::null_check(Register reg, int offset) {
1493   if (needs_explicit_null_check(offset)) {
1494     // provoke OS NULL exception if reg = NULL by
1495     // accessing M[reg] w/o changing any registers
1496     // NOTE: this is plenty to provoke a segv
1497     ldr(zr, Address(reg));
1498   } else {
1499     // nothing to do, (later) access of M[reg + offset]
1500     // will provoke OS NULL exception if reg = NULL
1501   }
1502 }
1503 
1504 // MacroAssembler protected routines needed to implement
1505 // public methods
1506 
1507 void MacroAssembler::mov(Register r, Address dest) {
1508   code_section()-&gt;relocate(pc(), dest.rspec());
1509   u_int64_t imm64 = (u_int64_t)dest.target();
1510   movptr(r, imm64);
1511 }
1512 
1513 // Move a constant pointer into r.  In AArch64 mode the virtual
1514 // address space is 48 bits in size, so we only need three
1515 // instructions to create a patchable instruction sequence that can
1516 // reach anywhere.
1517 void MacroAssembler::movptr(Register r, uintptr_t imm64) {
1518 #ifndef PRODUCT
1519   {
1520     char buffer[64];
1521     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1522     block_comment(buffer);
1523   }
1524 #endif
1525   assert(imm64 &lt; (1ul &lt;&lt; 48), &quot;48-bit overflow in address constant&quot;);
1526   movz(r, imm64 &amp; 0xffff);
1527   imm64 &gt;&gt;= 16;
1528   movk(r, imm64 &amp; 0xffff, 16);
1529   imm64 &gt;&gt;= 16;
1530   movk(r, imm64 &amp; 0xffff, 32);
1531 }
1532 
1533 // Macro to mov replicated immediate to vector register.
1534 //  Vd will get the following values for different arrangements in T
1535 //   imm32 == hex 000000gh  T8B:  Vd = ghghghghghghghgh
1536 //   imm32 == hex 000000gh  T16B: Vd = ghghghghghghghghghghghghghghghgh
1537 //   imm32 == hex 0000efgh  T4H:  Vd = efghefghefghefgh
1538 //   imm32 == hex 0000efgh  T8H:  Vd = efghefghefghefghefghefghefghefgh
1539 //   imm32 == hex abcdefgh  T2S:  Vd = abcdefghabcdefgh
1540 //   imm32 == hex abcdefgh  T4S:  Vd = abcdefghabcdefghabcdefghabcdefgh
1541 //   T1D/T2D: invalid
1542 void MacroAssembler::mov(FloatRegister Vd, SIMD_Arrangement T, u_int32_t imm32) {
1543   assert(T != T1D &amp;&amp; T != T2D, &quot;invalid arrangement&quot;);
1544   if (T == T8B || T == T16B) {
1545     assert((imm32 &amp; ~0xff) == 0, &quot;extraneous bits in unsigned imm32 (T8B/T16B)&quot;);
1546     movi(Vd, T, imm32 &amp; 0xff, 0);
1547     return;
1548   }
1549   u_int32_t nimm32 = ~imm32;
1550   if (T == T4H || T == T8H) {
1551     assert((imm32  &amp; ~0xffff) == 0, &quot;extraneous bits in unsigned imm32 (T4H/T8H)&quot;);
1552     imm32 &amp;= 0xffff;
1553     nimm32 &amp;= 0xffff;
1554   }
1555   u_int32_t x = imm32;
1556   int movi_cnt = 0;
1557   int movn_cnt = 0;
1558   while (x) { if (x &amp; 0xff) movi_cnt++; x &gt;&gt;= 8; }
1559   x = nimm32;
1560   while (x) { if (x &amp; 0xff) movn_cnt++; x &gt;&gt;= 8; }
1561   if (movn_cnt &lt; movi_cnt) imm32 = nimm32;
1562   unsigned lsl = 0;
1563   while (imm32 &amp;&amp; (imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1564   if (movn_cnt &lt; movi_cnt)
1565     mvni(Vd, T, imm32 &amp; 0xff, lsl);
1566   else
1567     movi(Vd, T, imm32 &amp; 0xff, lsl);
1568   imm32 &gt;&gt;= 8; lsl += 8;
1569   while (imm32) {
1570     while ((imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1571     if (movn_cnt &lt; movi_cnt)
1572       bici(Vd, T, imm32 &amp; 0xff, lsl);
1573     else
1574       orri(Vd, T, imm32 &amp; 0xff, lsl);
1575     lsl += 8; imm32 &gt;&gt;= 8;
1576   }
1577 }
1578 
1579 void MacroAssembler::mov_immediate64(Register dst, u_int64_t imm64)
1580 {
1581 #ifndef PRODUCT
1582   {
1583     char buffer[64];
1584     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1585     block_comment(buffer);
1586   }
1587 #endif
1588   if (operand_valid_for_logical_immediate(false, imm64)) {
1589     orr(dst, zr, imm64);
1590   } else {
1591     // we can use a combination of MOVZ or MOVN with
1592     // MOVK to build up the constant
1593     u_int64_t imm_h[4];
1594     int zero_count = 0;
1595     int neg_count = 0;
1596     int i;
1597     for (i = 0; i &lt; 4; i++) {
1598       imm_h[i] = ((imm64 &gt;&gt; (i * 16)) &amp; 0xffffL);
1599       if (imm_h[i] == 0) {
1600         zero_count++;
1601       } else if (imm_h[i] == 0xffffL) {
1602         neg_count++;
1603       }
1604     }
1605     if (zero_count == 4) {
1606       // one MOVZ will do
1607       movz(dst, 0);
1608     } else if (neg_count == 4) {
1609       // one MOVN will do
1610       movn(dst, 0);
1611     } else if (zero_count == 3) {
1612       for (i = 0; i &lt; 4; i++) {
1613         if (imm_h[i] != 0L) {
1614           movz(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));
1615           break;
1616         }
1617       }
1618     } else if (neg_count == 3) {
1619       // one MOVN will do
1620       for (int i = 0; i &lt; 4; i++) {
1621         if (imm_h[i] != 0xffffL) {
1622           movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));
1623           break;
1624         }
1625       }
1626     } else if (zero_count == 2) {
1627       // one MOVZ and one MOVK will do
1628       for (i = 0; i &lt; 3; i++) {
1629         if (imm_h[i] != 0L) {
1630           movz(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));
1631           i++;
1632           break;
1633         }
1634       }
1635       for (;i &lt; 4; i++) {
1636         if (imm_h[i] != 0L) {
1637           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));
1638         }
1639       }
1640     } else if (neg_count == 2) {
1641       // one MOVN and one MOVK will do
1642       for (i = 0; i &lt; 4; i++) {
1643         if (imm_h[i] != 0xffffL) {
1644           movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));
1645           i++;
1646           break;
1647         }
1648       }
1649       for (;i &lt; 4; i++) {
1650         if (imm_h[i] != 0xffffL) {
1651           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));
1652         }
1653       }
1654     } else if (zero_count == 1) {
1655       // one MOVZ and two MOVKs will do
1656       for (i = 0; i &lt; 4; i++) {
1657         if (imm_h[i] != 0L) {
1658           movz(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));
1659           i++;
1660           break;
1661         }
1662       }
1663       for (;i &lt; 4; i++) {
1664         if (imm_h[i] != 0x0L) {
1665           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));
1666         }
1667       }
1668     } else if (neg_count == 1) {
1669       // one MOVN and two MOVKs will do
1670       for (i = 0; i &lt; 4; i++) {
1671         if (imm_h[i] != 0xffffL) {
1672           movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));
1673           i++;
1674           break;
1675         }
1676       }
1677       for (;i &lt; 4; i++) {
1678         if (imm_h[i] != 0xffffL) {
1679           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));
1680         }
1681       }
1682     } else {
1683       // use a MOVZ and 3 MOVKs (makes it easier to debug)
1684       movz(dst, (u_int32_t)imm_h[0], 0);
1685       for (i = 1; i &lt; 4; i++) {
1686         movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));
1687       }
1688     }
1689   }
1690 }
1691 
1692 void MacroAssembler::mov_immediate32(Register dst, u_int32_t imm32)
1693 {
1694 #ifndef PRODUCT
1695     {
1696       char buffer[64];
1697       snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX32, imm32);
1698       block_comment(buffer);
1699     }
1700 #endif
1701   if (operand_valid_for_logical_immediate(true, imm32)) {
1702     orrw(dst, zr, imm32);
1703   } else {
1704     // we can use MOVZ, MOVN or two calls to MOVK to build up the
1705     // constant
1706     u_int32_t imm_h[2];
1707     imm_h[0] = imm32 &amp; 0xffff;
1708     imm_h[1] = ((imm32 &gt;&gt; 16) &amp; 0xffff);
1709     if (imm_h[0] == 0) {
1710       movzw(dst, imm_h[1], 16);
1711     } else if (imm_h[0] == 0xffff) {
1712       movnw(dst, imm_h[1] ^ 0xffff, 16);
1713     } else if (imm_h[1] == 0) {
1714       movzw(dst, imm_h[0], 0);
1715     } else if (imm_h[1] == 0xffff) {
1716       movnw(dst, imm_h[0] ^ 0xffff, 0);
1717     } else {
1718       // use a MOVZ and MOVK (makes it easier to debug)
1719       movzw(dst, imm_h[0], 0);
1720       movkw(dst, imm_h[1], 16);
1721     }
1722   }
1723 }
1724 
1725 // Form an address from base + offset in Rd.  Rd may or may
1726 // not actually be used: you must use the Address that is returned.
1727 // It is up to you to ensure that the shift provided matches the size
1728 // of your data.
1729 Address MacroAssembler::form_address(Register Rd, Register base, long byte_offset, int shift) {
1730   if (Address::offset_ok_for_immed(byte_offset, shift))
1731     // It fits; no need for any heroics
1732     return Address(base, byte_offset);
1733 
1734   // Don&#39;t do anything clever with negative or misaligned offsets
1735   unsigned mask = (1 &lt;&lt; shift) - 1;
1736   if (byte_offset &lt; 0 || byte_offset &amp; mask) {
1737     mov(Rd, byte_offset);
1738     add(Rd, base, Rd);
1739     return Address(Rd);
1740   }
1741 
1742   // See if we can do this with two 12-bit offsets
1743   {
1744     unsigned long word_offset = byte_offset &gt;&gt; shift;
1745     unsigned long masked_offset = word_offset &amp; 0xfff000;
1746     if (Address::offset_ok_for_immed(word_offset - masked_offset, 0)
1747         &amp;&amp; Assembler::operand_valid_for_add_sub_immediate(masked_offset &lt;&lt; shift)) {
1748       add(Rd, base, masked_offset &lt;&lt; shift);
1749       word_offset -= masked_offset;
1750       return Address(Rd, word_offset &lt;&lt; shift);
1751     }
1752   }
1753 
1754   // Do it the hard way
1755   mov(Rd, byte_offset);
1756   add(Rd, base, Rd);
1757   return Address(Rd);
1758 }
1759 
1760 void MacroAssembler::atomic_incw(Register counter_addr, Register tmp, Register tmp2) {
1761   if (UseLSE) {
1762     mov(tmp, 1);
1763     ldadd(Assembler::word, tmp, zr, counter_addr);
1764     return;
1765   }
1766   Label retry_load;
1767   if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))
1768     prfm(Address(counter_addr), PSTL1STRM);
1769   bind(retry_load);
1770   // flush and load exclusive from the memory location
1771   ldxrw(tmp, counter_addr);
1772   addw(tmp, tmp, 1);
1773   // if we store+flush with no intervening write tmp wil be zero
1774   stxrw(tmp2, tmp, counter_addr);
1775   cbnzw(tmp2, retry_load);
1776 }
1777 
1778 
1779 int MacroAssembler::corrected_idivl(Register result, Register ra, Register rb,
1780                                     bool want_remainder, Register scratch)
1781 {
1782   // Full implementation of Java idiv and irem.  The function
1783   // returns the (pc) offset of the div instruction - may be needed
1784   // for implicit exceptions.
1785   //
1786   // constraint : ra/rb =/= scratch
1787   //         normal case
1788   //
1789   // input : ra: dividend
1790   //         rb: divisor
1791   //
1792   // result: either
1793   //         quotient  (= ra idiv rb)
1794   //         remainder (= ra irem rb)
1795 
1796   assert(ra != scratch &amp;&amp; rb != scratch, &quot;reg cannot be scratch&quot;);
1797 
1798   int idivl_offset = offset();
1799   if (! want_remainder) {
1800     sdivw(result, ra, rb);
1801   } else {
1802     sdivw(scratch, ra, rb);
1803     Assembler::msubw(result, scratch, rb, ra);
1804   }
1805 
1806   return idivl_offset;
1807 }
1808 
1809 int MacroAssembler::corrected_idivq(Register result, Register ra, Register rb,
1810                                     bool want_remainder, Register scratch)
1811 {
1812   // Full implementation of Java ldiv and lrem.  The function
1813   // returns the (pc) offset of the div instruction - may be needed
1814   // for implicit exceptions.
1815   //
1816   // constraint : ra/rb =/= scratch
1817   //         normal case
1818   //
1819   // input : ra: dividend
1820   //         rb: divisor
1821   //
1822   // result: either
1823   //         quotient  (= ra idiv rb)
1824   //         remainder (= ra irem rb)
1825 
1826   assert(ra != scratch &amp;&amp; rb != scratch, &quot;reg cannot be scratch&quot;);
1827 
1828   int idivq_offset = offset();
1829   if (! want_remainder) {
1830     sdiv(result, ra, rb);
1831   } else {
1832     sdiv(scratch, ra, rb);
1833     Assembler::msub(result, scratch, rb, ra);
1834   }
1835 
1836   return idivq_offset;
1837 }
1838 
1839 void MacroAssembler::membar(Membar_mask_bits order_constraint) {
1840   address prev = pc() - NativeMembar::instruction_size;
1841   address last = code()-&gt;last_insn();
1842   if (last != NULL &amp;&amp; nativeInstruction_at(last)-&gt;is_Membar() &amp;&amp; prev == last) {
1843     NativeMembar *bar = NativeMembar_at(prev);
1844     // We are merging two memory barrier instructions.  On AArch64 we
1845     // can do this simply by ORing them together.
1846     bar-&gt;set_kind(bar-&gt;get_kind() | order_constraint);
1847     BLOCK_COMMENT(&quot;merged membar&quot;);
1848   } else {
1849     code()-&gt;set_last_insn(pc());
1850     dmb(Assembler::barrier(order_constraint));
1851   }
1852 }
1853 
1854 bool MacroAssembler::try_merge_ldst(Register rt, const Address &amp;adr, size_t size_in_bytes, bool is_store) {
1855   if (ldst_can_merge(rt, adr, size_in_bytes, is_store)) {
1856     merge_ldst(rt, adr, size_in_bytes, is_store);
1857     code()-&gt;clear_last_insn();
1858     return true;
1859   } else {
1860     assert(size_in_bytes == 8 || size_in_bytes == 4, &quot;only 8 bytes or 4 bytes load/store is supported.&quot;);
1861     const unsigned mask = size_in_bytes - 1;
1862     if (adr.getMode() == Address::base_plus_offset &amp;&amp;
1863         (adr.offset() &amp; mask) == 0) { // only supports base_plus_offset.
1864       code()-&gt;set_last_insn(pc());
1865     }
1866     return false;
1867   }
1868 }
1869 
1870 void MacroAssembler::ldr(Register Rx, const Address &amp;adr) {
1871   // We always try to merge two adjacent loads into one ldp.
1872   if (!try_merge_ldst(Rx, adr, 8, false)) {
1873     Assembler::ldr(Rx, adr);
1874   }
1875 }
1876 
1877 void MacroAssembler::ldrw(Register Rw, const Address &amp;adr) {
1878   // We always try to merge two adjacent loads into one ldp.
1879   if (!try_merge_ldst(Rw, adr, 4, false)) {
1880     Assembler::ldrw(Rw, adr);
1881   }
1882 }
1883 
1884 void MacroAssembler::str(Register Rx, const Address &amp;adr) {
1885   // We always try to merge two adjacent stores into one stp.
1886   if (!try_merge_ldst(Rx, adr, 8, true)) {
1887     Assembler::str(Rx, adr);
1888   }
1889 }
1890 
1891 void MacroAssembler::strw(Register Rw, const Address &amp;adr) {
1892   // We always try to merge two adjacent stores into one stp.
1893   if (!try_merge_ldst(Rw, adr, 4, true)) {
1894     Assembler::strw(Rw, adr);
1895   }
1896 }
1897 
1898 // MacroAssembler routines found actually to be needed
1899 
1900 void MacroAssembler::push(Register src)
1901 {
1902   str(src, Address(pre(esp, -1 * wordSize)));
1903 }
1904 
1905 void MacroAssembler::pop(Register dst)
1906 {
1907   ldr(dst, Address(post(esp, 1 * wordSize)));
1908 }
1909 
1910 // Note: load_unsigned_short used to be called load_unsigned_word.
1911 int MacroAssembler::load_unsigned_short(Register dst, Address src) {
1912   int off = offset();
1913   ldrh(dst, src);
1914   return off;
1915 }
1916 
1917 int MacroAssembler::load_unsigned_byte(Register dst, Address src) {
1918   int off = offset();
1919   ldrb(dst, src);
1920   return off;
1921 }
1922 
1923 int MacroAssembler::load_signed_short(Register dst, Address src) {
1924   int off = offset();
1925   ldrsh(dst, src);
1926   return off;
1927 }
1928 
1929 int MacroAssembler::load_signed_byte(Register dst, Address src) {
1930   int off = offset();
1931   ldrsb(dst, src);
1932   return off;
1933 }
1934 
1935 int MacroAssembler::load_signed_short32(Register dst, Address src) {
1936   int off = offset();
1937   ldrshw(dst, src);
1938   return off;
1939 }
1940 
1941 int MacroAssembler::load_signed_byte32(Register dst, Address src) {
1942   int off = offset();
1943   ldrsbw(dst, src);
1944   return off;
1945 }
1946 
1947 void MacroAssembler::load_sized_value(Register dst, Address src, size_t size_in_bytes, bool is_signed, Register dst2) {
1948   switch (size_in_bytes) {
1949   case  8:  ldr(dst, src); break;
1950   case  4:  ldrw(dst, src); break;
1951   case  2:  is_signed ? load_signed_short(dst, src) : load_unsigned_short(dst, src); break;
1952   case  1:  is_signed ? load_signed_byte( dst, src) : load_unsigned_byte( dst, src); break;
1953   default:  ShouldNotReachHere();
1954   }
1955 }
1956 
1957 void MacroAssembler::store_sized_value(Address dst, Register src, size_t size_in_bytes, Register src2) {
1958   switch (size_in_bytes) {
1959   case  8:  str(src, dst); break;
1960   case  4:  strw(src, dst); break;
1961   case  2:  strh(src, dst); break;
1962   case  1:  strb(src, dst); break;
1963   default:  ShouldNotReachHere();
1964   }
1965 }
1966 
1967 void MacroAssembler::decrementw(Register reg, int value)
1968 {
1969   if (value &lt; 0)  { incrementw(reg, -value);      return; }
1970   if (value == 0) {                               return; }
1971   if (value &lt; (1 &lt;&lt; 12)) { subw(reg, reg, value); return; }
1972   /* else */ {
1973     guarantee(reg != rscratch2, &quot;invalid dst for register decrement&quot;);
1974     movw(rscratch2, (unsigned)value);
1975     subw(reg, reg, rscratch2);
1976   }
1977 }
1978 
1979 void MacroAssembler::decrement(Register reg, int value)
1980 {
1981   if (value &lt; 0)  { increment(reg, -value);      return; }
1982   if (value == 0) {                              return; }
1983   if (value &lt; (1 &lt;&lt; 12)) { sub(reg, reg, value); return; }
1984   /* else */ {
1985     assert(reg != rscratch2, &quot;invalid dst for register decrement&quot;);
1986     mov(rscratch2, (unsigned long)value);
1987     sub(reg, reg, rscratch2);
1988   }
1989 }
1990 
1991 void MacroAssembler::decrementw(Address dst, int value)
1992 {
1993   assert(!dst.uses(rscratch1), &quot;invalid dst for address decrement&quot;);
1994   if (dst.getMode() == Address::literal) {
1995     assert(abs(value) &lt; (1 &lt;&lt; 12), &quot;invalid value and address mode combination&quot;);
1996     lea(rscratch2, dst);
1997     dst = Address(rscratch2);
1998   }
1999   ldrw(rscratch1, dst);
2000   decrementw(rscratch1, value);
2001   strw(rscratch1, dst);
2002 }
2003 
2004 void MacroAssembler::decrement(Address dst, int value)
2005 {
2006   assert(!dst.uses(rscratch1), &quot;invalid address for decrement&quot;);
2007   if (dst.getMode() == Address::literal) {
2008     assert(abs(value) &lt; (1 &lt;&lt; 12), &quot;invalid value and address mode combination&quot;);
2009     lea(rscratch2, dst);
2010     dst = Address(rscratch2);
2011   }
2012   ldr(rscratch1, dst);
2013   decrement(rscratch1, value);
2014   str(rscratch1, dst);
2015 }
2016 
2017 void MacroAssembler::incrementw(Register reg, int value)
2018 {
2019   if (value &lt; 0)  { decrementw(reg, -value);      return; }
2020   if (value == 0) {                               return; }
2021   if (value &lt; (1 &lt;&lt; 12)) { addw(reg, reg, value); return; }
2022   /* else */ {
2023     assert(reg != rscratch2, &quot;invalid dst for register increment&quot;);
2024     movw(rscratch2, (unsigned)value);
2025     addw(reg, reg, rscratch2);
2026   }
2027 }
2028 
2029 void MacroAssembler::increment(Register reg, int value)
2030 {
2031   if (value &lt; 0)  { decrement(reg, -value);      return; }
2032   if (value == 0) {                              return; }
2033   if (value &lt; (1 &lt;&lt; 12)) { add(reg, reg, value); return; }
2034   /* else */ {
2035     assert(reg != rscratch2, &quot;invalid dst for register increment&quot;);
2036     movw(rscratch2, (unsigned)value);
2037     add(reg, reg, rscratch2);
2038   }
2039 }
2040 
2041 void MacroAssembler::incrementw(Address dst, int value)
2042 {
2043   assert(!dst.uses(rscratch1), &quot;invalid dst for address increment&quot;);
2044   if (dst.getMode() == Address::literal) {
2045     assert(abs(value) &lt; (1 &lt;&lt; 12), &quot;invalid value and address mode combination&quot;);
2046     lea(rscratch2, dst);
2047     dst = Address(rscratch2);
2048   }
2049   ldrw(rscratch1, dst);
2050   incrementw(rscratch1, value);
2051   strw(rscratch1, dst);
2052 }
2053 
2054 void MacroAssembler::increment(Address dst, int value)
2055 {
2056   assert(!dst.uses(rscratch1), &quot;invalid dst for address increment&quot;);
2057   if (dst.getMode() == Address::literal) {
2058     assert(abs(value) &lt; (1 &lt;&lt; 12), &quot;invalid value and address mode combination&quot;);
2059     lea(rscratch2, dst);
2060     dst = Address(rscratch2);
2061   }
2062   ldr(rscratch1, dst);
2063   increment(rscratch1, value);
2064   str(rscratch1, dst);
2065 }
2066 
2067 
2068 void MacroAssembler::pusha() {
2069   push(0x7fffffff, sp);
2070 }
2071 
2072 void MacroAssembler::popa() {
2073   pop(0x7fffffff, sp);
2074 }
2075 
2076 // Push lots of registers in the bit set supplied.  Don&#39;t push sp.
2077 // Return the number of words pushed
2078 int MacroAssembler::push(unsigned int bitset, Register stack) {
2079   int words_pushed = 0;
2080 
2081   // Scan bitset to accumulate register pairs
2082   unsigned char regs[32];
2083   int count = 0;
2084   for (int reg = 0; reg &lt;= 30; reg++) {
2085     if (1 &amp; bitset)
2086       regs[count++] = reg;
2087     bitset &gt;&gt;= 1;
2088   }
2089   regs[count++] = zr-&gt;encoding_nocheck();
2090   count &amp;= ~1;  // Only push an even nuber of regs
2091 
2092   if (count) {
2093     stp(as_Register(regs[0]), as_Register(regs[1]),
2094        Address(pre(stack, -count * wordSize)));
2095     words_pushed += 2;
2096   }
2097   for (int i = 2; i &lt; count; i += 2) {
2098     stp(as_Register(regs[i]), as_Register(regs[i+1]),
2099        Address(stack, i * wordSize));
2100     words_pushed += 2;
2101   }
2102 
2103   assert(words_pushed == count, &quot;oops, pushed != count&quot;);
2104 
2105   return count;
2106 }
2107 
2108 int MacroAssembler::pop(unsigned int bitset, Register stack) {
2109   int words_pushed = 0;
2110 
2111   // Scan bitset to accumulate register pairs
2112   unsigned char regs[32];
2113   int count = 0;
2114   for (int reg = 0; reg &lt;= 30; reg++) {
2115     if (1 &amp; bitset)
2116       regs[count++] = reg;
2117     bitset &gt;&gt;= 1;
2118   }
2119   regs[count++] = zr-&gt;encoding_nocheck();
2120   count &amp;= ~1;
2121 
2122   for (int i = 2; i &lt; count; i += 2) {
2123     ldp(as_Register(regs[i]), as_Register(regs[i+1]),
2124        Address(stack, i * wordSize));
2125     words_pushed += 2;
2126   }
2127   if (count) {
2128     ldp(as_Register(regs[0]), as_Register(regs[1]),
2129        Address(post(stack, count * wordSize)));
2130     words_pushed += 2;
2131   }
2132 
2133   assert(words_pushed == count, &quot;oops, pushed != count&quot;);
2134 
2135   return count;
2136 }
2137 
2138 // Push lots of registers in the bit set supplied.  Don&#39;t push sp.
2139 // Return the number of words pushed
2140 int MacroAssembler::push_fp(unsigned int bitset, Register stack) {
2141   int words_pushed = 0;
2142 
2143   // Scan bitset to accumulate register pairs
2144   unsigned char regs[32];
2145   int count = 0;
2146   for (int reg = 0; reg &lt;= 31; reg++) {
2147     if (1 &amp; bitset)
2148       regs[count++] = reg;
2149     bitset &gt;&gt;= 1;
2150   }
2151   regs[count++] = zr-&gt;encoding_nocheck();
2152   count &amp;= ~1;  // Only push an even number of regs
2153 
2154   // Always pushing full 128 bit registers.
2155   if (count) {
2156     stpq(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(pre(stack, -count * wordSize * 2)));
2157     words_pushed += 2;
2158   }
2159   for (int i = 2; i &lt; count; i += 2) {
2160     stpq(as_FloatRegister(regs[i]), as_FloatRegister(regs[i+1]), Address(stack, i * wordSize * 2));
2161     words_pushed += 2;
2162   }
2163 
2164   assert(words_pushed == count, &quot;oops, pushed != count&quot;);
2165   return count;
2166 }
2167 
2168 int MacroAssembler::pop_fp(unsigned int bitset, Register stack) {
2169   int words_pushed = 0;
2170 
2171   // Scan bitset to accumulate register pairs
2172   unsigned char regs[32];
2173   int count = 0;
2174   for (int reg = 0; reg &lt;= 31; reg++) {
2175     if (1 &amp; bitset)
2176       regs[count++] = reg;
2177     bitset &gt;&gt;= 1;
2178   }
2179   regs[count++] = zr-&gt;encoding_nocheck();
2180   count &amp;= ~1;
2181 
2182   for (int i = 2; i &lt; count; i += 2) {
2183     ldpq(as_FloatRegister(regs[i]), as_FloatRegister(regs[i+1]), Address(stack, i * wordSize * 2));
2184     words_pushed += 2;
2185   }
2186   if (count) {
2187     ldpq(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(post(stack, count * wordSize * 2)));
2188     words_pushed += 2;
2189   }
2190 
2191   assert(words_pushed == count, &quot;oops, pushed != count&quot;);
2192 
2193   return count;
2194 }
2195 
2196 #ifdef ASSERT
2197 void MacroAssembler::verify_heapbase(const char* msg) {
2198 #if 0
2199   assert (UseCompressedOops || UseCompressedClassPointers, &quot;should be compressed&quot;);
2200   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
2201   if (CheckCompressedOops) {
2202     Label ok;
2203     push(1 &lt;&lt; rscratch1-&gt;encoding(), sp); // cmpptr trashes rscratch1
2204     cmpptr(rheapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));
2205     br(Assembler::EQ, ok);
2206     stop(msg);
2207     bind(ok);
2208     pop(1 &lt;&lt; rscratch1-&gt;encoding(), sp);
2209   }
2210 #endif
2211 }
2212 #endif
2213 
2214 void MacroAssembler::resolve_jobject(Register value, Register thread, Register tmp) {
2215   Label done, not_weak;
2216   cbz(value, done);           // Use NULL as-is.
2217 
2218   STATIC_ASSERT(JNIHandles::weak_tag_mask == 1u);
2219   tbz(r0, 0, not_weak);    // Test for jweak tag.
2220 
2221   // Resolve jweak.
2222   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF, value,
2223                  Address(value, -JNIHandles::weak_tag_value), tmp, thread);
2224   verify_oop(value);
2225   b(done);
2226 
2227   bind(not_weak);
2228   // Resolve (untagged) jobject.
2229   access_load_at(T_OBJECT, IN_NATIVE, value, Address(value, 0), tmp, thread);
2230   verify_oop(value);
2231   bind(done);
2232 }
2233 
2234 void MacroAssembler::stop(const char* msg) {
2235   address ip = pc();
2236   pusha();
2237   mov(c_rarg0, (address)msg);
2238   mov(c_rarg1, (address)ip);
2239   mov(c_rarg2, sp);
2240   mov(c_rarg3, CAST_FROM_FN_PTR(address, MacroAssembler::debug64));
2241   blr(c_rarg3);
2242   hlt(0);
2243 }
2244 
2245 void MacroAssembler::warn(const char* msg) {
2246   pusha();
2247   mov(c_rarg0, (address)msg);
2248   mov(lr, CAST_FROM_FN_PTR(address, warning));
2249   blr(lr);
2250   popa();
2251 }
2252 
2253 void MacroAssembler::unimplemented(const char* what) {
2254   const char* buf = NULL;
2255   {
2256     ResourceMark rm;
2257     stringStream ss;
2258     ss.print(&quot;unimplemented: %s&quot;, what);
2259     buf = code_string(ss.as_string());
2260   }
2261   stop(buf);
2262 }
2263 
2264 // If a constant does not fit in an immediate field, generate some
2265 // number of MOV instructions and then perform the operation.
2266 void MacroAssembler::wrap_add_sub_imm_insn(Register Rd, Register Rn, unsigned imm,
2267                                            add_sub_imm_insn insn1,
2268                                            add_sub_reg_insn insn2) {
2269   assert(Rd != zr, &quot;Rd = zr and not setting flags?&quot;);
2270   if (operand_valid_for_add_sub_immediate((int)imm)) {
2271     (this-&gt;*insn1)(Rd, Rn, imm);
2272   } else {
2273     if (uabs(imm) &lt; (1 &lt;&lt; 24)) {
2274        (this-&gt;*insn1)(Rd, Rn, imm &amp; -(1 &lt;&lt; 12));
2275        (this-&gt;*insn1)(Rd, Rd, imm &amp; ((1 &lt;&lt; 12)-1));
2276     } else {
2277        assert_different_registers(Rd, Rn);
2278        mov(Rd, (uint64_t)imm);
2279        (this-&gt;*insn2)(Rd, Rn, Rd, LSL, 0);
2280     }
2281   }
2282 }
2283 
2284 // Seperate vsn which sets the flags. Optimisations are more restricted
2285 // because we must set the flags correctly.
2286 void MacroAssembler::wrap_adds_subs_imm_insn(Register Rd, Register Rn, unsigned imm,
2287                                            add_sub_imm_insn insn1,
2288                                            add_sub_reg_insn insn2) {
2289   if (operand_valid_for_add_sub_immediate((int)imm)) {
2290     (this-&gt;*insn1)(Rd, Rn, imm);
2291   } else {
2292     assert_different_registers(Rd, Rn);
2293     assert(Rd != zr, &quot;overflow in immediate operand&quot;);
2294     mov(Rd, (uint64_t)imm);
2295     (this-&gt;*insn2)(Rd, Rn, Rd, LSL, 0);
2296   }
2297 }
2298 
2299 
2300 void MacroAssembler::add(Register Rd, Register Rn, RegisterOrConstant increment) {
2301   if (increment.is_register()) {
2302     add(Rd, Rn, increment.as_register());
2303   } else {
2304     add(Rd, Rn, increment.as_constant());
2305   }
2306 }
2307 
2308 void MacroAssembler::addw(Register Rd, Register Rn, RegisterOrConstant increment) {
2309   if (increment.is_register()) {
2310     addw(Rd, Rn, increment.as_register());
2311   } else {
2312     addw(Rd, Rn, increment.as_constant());
2313   }
2314 }
2315 
2316 void MacroAssembler::sub(Register Rd, Register Rn, RegisterOrConstant decrement) {
2317   if (decrement.is_register()) {
2318     sub(Rd, Rn, decrement.as_register());
2319   } else {
2320     sub(Rd, Rn, decrement.as_constant());
2321   }
2322 }
2323 
2324 void MacroAssembler::subw(Register Rd, Register Rn, RegisterOrConstant decrement) {
2325   if (decrement.is_register()) {
2326     subw(Rd, Rn, decrement.as_register());
2327   } else {
2328     subw(Rd, Rn, decrement.as_constant());
2329   }
2330 }
2331 
2332 void MacroAssembler::reinit_heapbase()
2333 {
2334   if (UseCompressedOops) {
2335     if (Universe::is_fully_initialized()) {
2336       mov(rheapbase, CompressedOops::ptrs_base());
2337     } else {
2338       lea(rheapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()));
2339       ldr(rheapbase, Address(rheapbase));
2340     }
2341   }
2342 }
2343 
2344 // this simulates the behaviour of the x86 cmpxchg instruction using a
2345 // load linked/store conditional pair. we use the acquire/release
2346 // versions of these instructions so that we flush pending writes as
2347 // per Java semantics.
2348 
2349 // n.b the x86 version assumes the old value to be compared against is
2350 // in rax and updates rax with the value located in memory if the
2351 // cmpxchg fails. we supply a register for the old value explicitly
2352 
2353 // the aarch64 load linked/store conditional instructions do not
2354 // accept an offset. so, unlike x86, we must provide a plain register
2355 // to identify the memory word to be compared/exchanged rather than a
2356 // register+offset Address.
2357 
2358 void MacroAssembler::cmpxchgptr(Register oldv, Register newv, Register addr, Register tmp,
2359                                 Label &amp;succeed, Label *fail) {
2360   // oldv holds comparison value
2361   // newv holds value to write in exchange
2362   // addr identifies memory word to compare against/update
2363   if (UseLSE) {
2364     mov(tmp, oldv);
2365     casal(Assembler::xword, oldv, newv, addr);
2366     cmp(tmp, oldv);
2367     br(Assembler::EQ, succeed);
2368     membar(AnyAny);
2369   } else {
2370     Label retry_load, nope;
2371     if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))
2372       prfm(Address(addr), PSTL1STRM);
2373     bind(retry_load);
2374     // flush and load exclusive from the memory location
2375     // and fail if it is not what we expect
2376     ldaxr(tmp, addr);
2377     cmp(tmp, oldv);
2378     br(Assembler::NE, nope);
2379     // if we store+flush with no intervening write tmp wil be zero
2380     stlxr(tmp, newv, addr);
2381     cbzw(tmp, succeed);
2382     // retry so we only ever return after a load fails to compare
2383     // ensures we don&#39;t return a stale value after a failed write.
2384     b(retry_load);
2385     // if the memory word differs we return it in oldv and signal a fail
2386     bind(nope);
2387     membar(AnyAny);
2388     mov(oldv, tmp);
2389   }
2390   if (fail)
2391     b(*fail);
2392 }
2393 
2394 void MacroAssembler::cmpxchg_obj_header(Register oldv, Register newv, Register obj, Register tmp,
2395                                         Label &amp;succeed, Label *fail) {
2396   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;assumption&quot;);
2397   cmpxchgptr(oldv, newv, obj, tmp, succeed, fail);
2398 }
2399 
2400 void MacroAssembler::cmpxchgw(Register oldv, Register newv, Register addr, Register tmp,
2401                                 Label &amp;succeed, Label *fail) {
2402   // oldv holds comparison value
2403   // newv holds value to write in exchange
2404   // addr identifies memory word to compare against/update
2405   // tmp returns 0/1 for success/failure
2406   if (UseLSE) {
2407     mov(tmp, oldv);
2408     casal(Assembler::word, oldv, newv, addr);
2409     cmp(tmp, oldv);
2410     br(Assembler::EQ, succeed);
2411     membar(AnyAny);
2412   } else {
2413     Label retry_load, nope;
2414     if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))
2415       prfm(Address(addr), PSTL1STRM);
2416     bind(retry_load);
2417     // flush and load exclusive from the memory location
2418     // and fail if it is not what we expect
2419     ldaxrw(tmp, addr);
2420     cmp(tmp, oldv);
2421     br(Assembler::NE, nope);
2422     // if we store+flush with no intervening write tmp wil be zero
2423     stlxrw(tmp, newv, addr);
2424     cbzw(tmp, succeed);
2425     // retry so we only ever return after a load fails to compare
2426     // ensures we don&#39;t return a stale value after a failed write.
2427     b(retry_load);
2428     // if the memory word differs we return it in oldv and signal a fail
2429     bind(nope);
2430     membar(AnyAny);
2431     mov(oldv, tmp);
2432   }
2433   if (fail)
2434     b(*fail);
2435 }
2436 
2437 // A generic CAS; success or failure is in the EQ flag.  A weak CAS
2438 // doesn&#39;t retry and may fail spuriously.  If the oldval is wanted,
2439 // Pass a register for the result, otherwise pass noreg.
2440 
2441 // Clobbers rscratch1
2442 void MacroAssembler::cmpxchg(Register addr, Register expected,
2443                              Register new_val,
2444                              enum operand_size size,
2445                              bool acquire, bool release,
2446                              bool weak,
2447                              Register result) {
2448   if (result == noreg)  result = rscratch1;
2449   BLOCK_COMMENT(&quot;cmpxchg {&quot;);
2450   if (UseLSE) {
2451     mov(result, expected);
2452     lse_cas(result, new_val, addr, size, acquire, release, /*not_pair*/ true);
2453     compare_eq(result, expected, size);
2454   } else {
2455     Label retry_load, done;
2456     if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))
2457       prfm(Address(addr), PSTL1STRM);
2458     bind(retry_load);
2459     load_exclusive(result, addr, size, acquire);
2460     compare_eq(result, expected, size);
2461     br(Assembler::NE, done);
2462     store_exclusive(rscratch1, new_val, addr, size, release);
2463     if (weak) {
2464       cmpw(rscratch1, 0u);  // If the store fails, return NE to our caller.
2465     } else {
2466       cbnzw(rscratch1, retry_load);
2467     }
2468     bind(done);
2469   }
2470   BLOCK_COMMENT(&quot;} cmpxchg&quot;);
2471 }
2472 
2473 // A generic comparison. Only compares for equality, clobbers rscratch1.
2474 void MacroAssembler::compare_eq(Register rm, Register rn, enum operand_size size) {
2475   if (size == xword) {
2476     cmp(rm, rn);
2477   } else if (size == word) {
2478     cmpw(rm, rn);
2479   } else if (size == halfword) {
2480     eorw(rscratch1, rm, rn);
2481     ands(zr, rscratch1, 0xffff);
2482   } else if (size == byte) {
2483     eorw(rscratch1, rm, rn);
2484     ands(zr, rscratch1, 0xff);
2485   } else {
2486     ShouldNotReachHere();
2487   }
2488 }
2489 
2490 
2491 static bool different(Register a, RegisterOrConstant b, Register c) {
2492   if (b.is_constant())
2493     return a != c;
2494   else
2495     return a != b.as_register() &amp;&amp; a != c &amp;&amp; b.as_register() != c;
2496 }
2497 
2498 #define ATOMIC_OP(NAME, LDXR, OP, IOP, AOP, STXR, sz)                   \
2499 void MacroAssembler::atomic_##NAME(Register prev, RegisterOrConstant incr, Register addr) { \
2500   if (UseLSE) {                                                         \
2501     prev = prev-&gt;is_valid() ? prev : zr;                                \
2502     if (incr.is_register()) {                                           \
2503       AOP(sz, incr.as_register(), prev, addr);                          \
2504     } else {                                                            \
2505       mov(rscratch2, incr.as_constant());                               \
2506       AOP(sz, rscratch2, prev, addr);                                   \
2507     }                                                                   \
2508     return;                                                             \
2509   }                                                                     \
2510   Register result = rscratch2;                                          \
2511   if (prev-&gt;is_valid())                                                 \
2512     result = different(prev, incr, addr) ? prev : rscratch2;            \
2513                                                                         \
2514   Label retry_load;                                                     \
2515   if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))         \
2516     prfm(Address(addr), PSTL1STRM);                                     \
2517   bind(retry_load);                                                     \
2518   LDXR(result, addr);                                                   \
2519   OP(rscratch1, result, incr);                                          \
2520   STXR(rscratch2, rscratch1, addr);                                     \
2521   cbnzw(rscratch2, retry_load);                                         \
2522   if (prev-&gt;is_valid() &amp;&amp; prev != result) {                             \
2523     IOP(prev, rscratch1, incr);                                         \
2524   }                                                                     \
2525 }
2526 
2527 ATOMIC_OP(add, ldxr, add, sub, ldadd, stxr, Assembler::xword)
2528 ATOMIC_OP(addw, ldxrw, addw, subw, ldadd, stxrw, Assembler::word)
2529 ATOMIC_OP(addal, ldaxr, add, sub, ldaddal, stlxr, Assembler::xword)
2530 ATOMIC_OP(addalw, ldaxrw, addw, subw, ldaddal, stlxrw, Assembler::word)
2531 
2532 #undef ATOMIC_OP
2533 
2534 #define ATOMIC_XCHG(OP, AOP, LDXR, STXR, sz)                            \
2535 void MacroAssembler::atomic_##OP(Register prev, Register newv, Register addr) { \
2536   if (UseLSE) {                                                         \
2537     prev = prev-&gt;is_valid() ? prev : zr;                                \
2538     AOP(sz, newv, prev, addr);                                          \
2539     return;                                                             \
2540   }                                                                     \
2541   Register result = rscratch2;                                          \
2542   if (prev-&gt;is_valid())                                                 \
2543     result = different(prev, newv, addr) ? prev : rscratch2;            \
2544                                                                         \
2545   Label retry_load;                                                     \
2546   if ((VM_Version::features() &amp; VM_Version::CPU_STXR_PREFETCH))         \
2547     prfm(Address(addr), PSTL1STRM);                                     \
2548   bind(retry_load);                                                     \
2549   LDXR(result, addr);                                                   \
2550   STXR(rscratch1, newv, addr);                                          \
2551   cbnzw(rscratch1, retry_load);                                         \
2552   if (prev-&gt;is_valid() &amp;&amp; prev != result)                               \
2553     mov(prev, result);                                                  \
2554 }
2555 
2556 ATOMIC_XCHG(xchg, swp, ldxr, stxr, Assembler::xword)
2557 ATOMIC_XCHG(xchgw, swp, ldxrw, stxrw, Assembler::word)
2558 ATOMIC_XCHG(xchgal, swpal, ldaxr, stlxr, Assembler::xword)
2559 ATOMIC_XCHG(xchgalw, swpal, ldaxrw, stlxrw, Assembler::word)
2560 
2561 #undef ATOMIC_XCHG
2562 
2563 #ifndef PRODUCT
2564 extern &quot;C&quot; void findpc(intptr_t x);
2565 #endif
2566 
2567 void MacroAssembler::debug64(char* msg, int64_t pc, int64_t regs[])
2568 {
2569   // In order to get locks to work, we need to fake a in_VM state
2570   if (ShowMessageBoxOnError ) {
2571     JavaThread* thread = JavaThread::current();
2572     JavaThreadState saved_state = thread-&gt;thread_state();
2573     thread-&gt;set_thread_state(_thread_in_vm);
2574 #ifndef PRODUCT
2575     if (CountBytecodes || TraceBytecodes || StopInterpreterAt) {
2576       ttyLocker ttyl;
2577       BytecodeCounter::print();
2578     }
2579 #endif
2580     if (os::message_box(msg, &quot;Execution stopped, print registers?&quot;)) {
2581       ttyLocker ttyl;
2582       tty-&gt;print_cr(&quot; pc = 0x%016lx&quot;, pc);
2583 #ifndef PRODUCT
2584       tty-&gt;cr();
2585       findpc(pc);
2586       tty-&gt;cr();
2587 #endif
2588       tty-&gt;print_cr(&quot; r0 = 0x%016lx&quot;, regs[0]);
2589       tty-&gt;print_cr(&quot; r1 = 0x%016lx&quot;, regs[1]);
2590       tty-&gt;print_cr(&quot; r2 = 0x%016lx&quot;, regs[2]);
2591       tty-&gt;print_cr(&quot; r3 = 0x%016lx&quot;, regs[3]);
2592       tty-&gt;print_cr(&quot; r4 = 0x%016lx&quot;, regs[4]);
2593       tty-&gt;print_cr(&quot; r5 = 0x%016lx&quot;, regs[5]);
2594       tty-&gt;print_cr(&quot; r6 = 0x%016lx&quot;, regs[6]);
2595       tty-&gt;print_cr(&quot; r7 = 0x%016lx&quot;, regs[7]);
2596       tty-&gt;print_cr(&quot; r8 = 0x%016lx&quot;, regs[8]);
2597       tty-&gt;print_cr(&quot; r9 = 0x%016lx&quot;, regs[9]);
2598       tty-&gt;print_cr(&quot;r10 = 0x%016lx&quot;, regs[10]);
2599       tty-&gt;print_cr(&quot;r11 = 0x%016lx&quot;, regs[11]);
2600       tty-&gt;print_cr(&quot;r12 = 0x%016lx&quot;, regs[12]);
2601       tty-&gt;print_cr(&quot;r13 = 0x%016lx&quot;, regs[13]);
2602       tty-&gt;print_cr(&quot;r14 = 0x%016lx&quot;, regs[14]);
2603       tty-&gt;print_cr(&quot;r15 = 0x%016lx&quot;, regs[15]);
2604       tty-&gt;print_cr(&quot;r16 = 0x%016lx&quot;, regs[16]);
2605       tty-&gt;print_cr(&quot;r17 = 0x%016lx&quot;, regs[17]);
2606       tty-&gt;print_cr(&quot;r18 = 0x%016lx&quot;, regs[18]);
2607       tty-&gt;print_cr(&quot;r19 = 0x%016lx&quot;, regs[19]);
2608       tty-&gt;print_cr(&quot;r20 = 0x%016lx&quot;, regs[20]);
2609       tty-&gt;print_cr(&quot;r21 = 0x%016lx&quot;, regs[21]);
2610       tty-&gt;print_cr(&quot;r22 = 0x%016lx&quot;, regs[22]);
2611       tty-&gt;print_cr(&quot;r23 = 0x%016lx&quot;, regs[23]);
2612       tty-&gt;print_cr(&quot;r24 = 0x%016lx&quot;, regs[24]);
2613       tty-&gt;print_cr(&quot;r25 = 0x%016lx&quot;, regs[25]);
2614       tty-&gt;print_cr(&quot;r26 = 0x%016lx&quot;, regs[26]);
2615       tty-&gt;print_cr(&quot;r27 = 0x%016lx&quot;, regs[27]);
2616       tty-&gt;print_cr(&quot;r28 = 0x%016lx&quot;, regs[28]);
2617       tty-&gt;print_cr(&quot;r30 = 0x%016lx&quot;, regs[30]);
2618       tty-&gt;print_cr(&quot;r31 = 0x%016lx&quot;, regs[31]);
2619       BREAKPOINT;
2620     }
2621   }
2622   fatal(&quot;DEBUG MESSAGE: %s&quot;, msg);
2623 }
2624 
2625 void MacroAssembler::push_call_clobbered_registers() {
2626   int step = 4 * wordSize;
2627   push(RegSet::range(r0, r18) - RegSet::of(rscratch1, rscratch2), sp);
2628   sub(sp, sp, step);
2629   mov(rscratch1, -step);
2630   // Push v0-v7, v16-v31.
2631   for (int i = 31; i&gt;= 4; i -= 4) {
2632     if (i &lt;= v7-&gt;encoding() || i &gt;= v16-&gt;encoding())
2633       st1(as_FloatRegister(i-3), as_FloatRegister(i-2), as_FloatRegister(i-1),
2634           as_FloatRegister(i), T1D, Address(post(sp, rscratch1)));
2635   }
2636   st1(as_FloatRegister(0), as_FloatRegister(1), as_FloatRegister(2),
2637       as_FloatRegister(3), T1D, Address(sp));
2638 }
2639 
2640 void MacroAssembler::pop_call_clobbered_registers() {
2641   for (int i = 0; i &lt; 32; i += 4) {
2642     if (i &lt;= v7-&gt;encoding() || i &gt;= v16-&gt;encoding())
2643       ld1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
2644           as_FloatRegister(i+3), T1D, Address(post(sp, 4 * wordSize)));
2645   }
2646 
2647   pop(RegSet::range(r0, r18) - RegSet::of(rscratch1, rscratch2), sp);
2648 }
2649 
2650 void MacroAssembler::push_CPU_state(bool save_vectors) {
2651   int step = (save_vectors ? 8 : 4) * wordSize;
2652   push(0x3fffffff, sp);         // integer registers except lr &amp; sp
2653   mov(rscratch1, -step);
2654   sub(sp, sp, step);
2655   for (int i = 28; i &gt;= 4; i -= 4) {
2656     st1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
2657         as_FloatRegister(i+3), save_vectors ? T2D : T1D, Address(post(sp, rscratch1)));
2658   }
2659   st1(v0, v1, v2, v3, save_vectors ? T2D : T1D, sp);
2660 }
2661 
2662 void MacroAssembler::pop_CPU_state(bool restore_vectors) {
2663   int step = (restore_vectors ? 8 : 4) * wordSize;
2664   for (int i = 0; i &lt;= 28; i += 4)
2665     ld1(as_FloatRegister(i), as_FloatRegister(i+1), as_FloatRegister(i+2),
2666         as_FloatRegister(i+3), restore_vectors ? T2D : T1D, Address(post(sp, step)));
2667   pop(0x3fffffff, sp);         // integer registers except lr &amp; sp
2668 }
2669 
2670 /**
2671  * Helpers for multiply_to_len().
2672  */
2673 void MacroAssembler::add2_with_carry(Register final_dest_hi, Register dest_hi, Register dest_lo,
2674                                      Register src1, Register src2) {
2675   adds(dest_lo, dest_lo, src1);
2676   adc(dest_hi, dest_hi, zr);
2677   adds(dest_lo, dest_lo, src2);
2678   adc(final_dest_hi, dest_hi, zr);
2679 }
2680 
2681 // Generate an address from (r + r1 extend offset).  &quot;size&quot; is the
2682 // size of the operand.  The result may be in rscratch2.
2683 Address MacroAssembler::offsetted_address(Register r, Register r1,
2684                                           Address::extend ext, int offset, int size) {
2685   if (offset || (ext.shift() % size != 0)) {
2686     lea(rscratch2, Address(r, r1, ext));
2687     return Address(rscratch2, offset);
2688   } else {
2689     return Address(r, r1, ext);
2690   }
2691 }
2692 
2693 Address MacroAssembler::spill_address(int size, int offset, Register tmp)
2694 {
2695   assert(offset &gt;= 0, &quot;spill to negative address?&quot;);
2696   // Offset reachable ?
2697   //   Not aligned - 9 bits signed offset
2698   //   Aligned - 12 bits unsigned offset shifted
2699   Register base = sp;
2700   if ((offset &amp; (size-1)) &amp;&amp; offset &gt;= (1&lt;&lt;8)) {
2701     add(tmp, base, offset &amp; ((1&lt;&lt;12)-1));
2702     base = tmp;
2703     offset &amp;= -1u&lt;&lt;12;
2704   }
2705 
2706   if (offset &gt;= (1&lt;&lt;12) * size) {
2707     add(tmp, base, offset &amp; (((1&lt;&lt;12)-1)&lt;&lt;12));
2708     base = tmp;
2709     offset &amp;= ~(((1&lt;&lt;12)-1)&lt;&lt;12);
2710   }
2711 
2712   return Address(base, offset);
2713 }
2714 
2715 // Checks whether offset is aligned.
2716 // Returns true if it is, else false.
2717 bool MacroAssembler::merge_alignment_check(Register base,
2718                                            size_t size,
2719                                            long cur_offset,
2720                                            long prev_offset) const {
2721   if (AvoidUnalignedAccesses) {
2722     if (base == sp) {
2723       // Checks whether low offset if aligned to pair of registers.
2724       long pair_mask = size * 2 - 1;
2725       long offset = prev_offset &gt; cur_offset ? cur_offset : prev_offset;
2726       return (offset &amp; pair_mask) == 0;
2727     } else { // If base is not sp, we can&#39;t guarantee the access is aligned.
2728       return false;
2729     }
2730   } else {
2731     long mask = size - 1;
2732     // Load/store pair instruction only supports element size aligned offset.
2733     return (cur_offset &amp; mask) == 0 &amp;&amp; (prev_offset &amp; mask) == 0;
2734   }
2735 }
2736 
2737 // Checks whether current and previous loads/stores can be merged.
2738 // Returns true if it can be merged, else false.
2739 bool MacroAssembler::ldst_can_merge(Register rt,
2740                                     const Address &amp;adr,
2741                                     size_t cur_size_in_bytes,
2742                                     bool is_store) const {
2743   address prev = pc() - NativeInstruction::instruction_size;
2744   address last = code()-&gt;last_insn();
2745 
2746   if (last == NULL || !nativeInstruction_at(last)-&gt;is_Imm_LdSt()) {
2747     return false;
2748   }
2749 
2750   if (adr.getMode() != Address::base_plus_offset || prev != last) {
2751     return false;
2752   }
2753 
2754   NativeLdSt* prev_ldst = NativeLdSt_at(prev);
2755   size_t prev_size_in_bytes = prev_ldst-&gt;size_in_bytes();
2756 
2757   assert(prev_size_in_bytes == 4 || prev_size_in_bytes == 8, &quot;only supports 64/32bit merging.&quot;);
2758   assert(cur_size_in_bytes == 4 || cur_size_in_bytes == 8, &quot;only supports 64/32bit merging.&quot;);
2759 
2760   if (cur_size_in_bytes != prev_size_in_bytes || is_store != prev_ldst-&gt;is_store()) {
2761     return false;
2762   }
2763 
2764   long max_offset = 63 * prev_size_in_bytes;
2765   long min_offset = -64 * prev_size_in_bytes;
2766 
2767   assert(prev_ldst-&gt;is_not_pre_post_index(), &quot;pre-index or post-index is not supported to be merged.&quot;);
2768 
2769   // Only same base can be merged.
2770   if (adr.base() != prev_ldst-&gt;base()) {
2771     return false;
2772   }
2773 
2774   long cur_offset = adr.offset();
2775   long prev_offset = prev_ldst-&gt;offset();
2776   size_t diff = abs(cur_offset - prev_offset);
2777   if (diff != prev_size_in_bytes) {
2778     return false;
2779   }
2780 
2781   // Following cases can not be merged:
2782   // ldr x2, [x2, #8]
2783   // ldr x3, [x2, #16]
2784   // or:
2785   // ldr x2, [x3, #8]
2786   // ldr x2, [x3, #16]
2787   // If t1 and t2 is the same in &quot;ldp t1, t2, [xn, #imm]&quot;, we&#39;ll get SIGILL.
2788   if (!is_store &amp;&amp; (adr.base() == prev_ldst-&gt;target() || rt == prev_ldst-&gt;target())) {
2789     return false;
2790   }
2791 
2792   long low_offset = prev_offset &gt; cur_offset ? cur_offset : prev_offset;
2793   // Offset range must be in ldp/stp instruction&#39;s range.
2794   if (low_offset &gt; max_offset || low_offset &lt; min_offset) {
2795     return false;
2796   }
2797 
2798   if (merge_alignment_check(adr.base(), prev_size_in_bytes, cur_offset, prev_offset)) {
2799     return true;
2800   }
2801 
2802   return false;
2803 }
2804 
2805 // Merge current load/store with previous load/store into ldp/stp.
2806 void MacroAssembler::merge_ldst(Register rt,
2807                                 const Address &amp;adr,
2808                                 size_t cur_size_in_bytes,
2809                                 bool is_store) {
2810 
2811   assert(ldst_can_merge(rt, adr, cur_size_in_bytes, is_store) == true, &quot;cur and prev must be able to be merged.&quot;);
2812 
2813   Register rt_low, rt_high;
2814   address prev = pc() - NativeInstruction::instruction_size;
2815   NativeLdSt* prev_ldst = NativeLdSt_at(prev);
2816 
2817   long offset;
2818 
2819   if (adr.offset() &lt; prev_ldst-&gt;offset()) {
2820     offset = adr.offset();
2821     rt_low = rt;
2822     rt_high = prev_ldst-&gt;target();
2823   } else {
2824     offset = prev_ldst-&gt;offset();
2825     rt_low = prev_ldst-&gt;target();
2826     rt_high = rt;
2827   }
2828 
2829   Address adr_p = Address(prev_ldst-&gt;base(), offset);
2830   // Overwrite previous generated binary.
2831   code_section()-&gt;set_end(prev);
2832 
2833   const int sz = prev_ldst-&gt;size_in_bytes();
2834   assert(sz == 8 || sz == 4, &quot;only supports 64/32bit merging.&quot;);
2835   if (!is_store) {
2836     BLOCK_COMMENT(&quot;merged ldr pair&quot;);
2837     if (sz == 8) {
2838       ldp(rt_low, rt_high, adr_p);
2839     } else {
2840       ldpw(rt_low, rt_high, adr_p);
2841     }
2842   } else {
2843     BLOCK_COMMENT(&quot;merged str pair&quot;);
2844     if (sz == 8) {
2845       stp(rt_low, rt_high, adr_p);
2846     } else {
2847       stpw(rt_low, rt_high, adr_p);
2848     }
2849   }
2850 }
2851 
2852 /**
2853  * Multiply 64 bit by 64 bit first loop.
2854  */
2855 void MacroAssembler::multiply_64_x_64_loop(Register x, Register xstart, Register x_xstart,
2856                                            Register y, Register y_idx, Register z,
2857                                            Register carry, Register product,
2858                                            Register idx, Register kdx) {
2859   //
2860   //  jlong carry, x[], y[], z[];
2861   //  for (int idx=ystart, kdx=ystart+1+xstart; idx &gt;= 0; idx-, kdx--) {
2862   //    huge_128 product = y[idx] * x[xstart] + carry;
2863   //    z[kdx] = (jlong)product;
2864   //    carry  = (jlong)(product &gt;&gt;&gt; 64);
2865   //  }
2866   //  z[xstart] = carry;
2867   //
2868 
2869   Label L_first_loop, L_first_loop_exit;
2870   Label L_one_x, L_one_y, L_multiply;
2871 
2872   subsw(xstart, xstart, 1);
2873   br(Assembler::MI, L_one_x);
2874 
2875   lea(rscratch1, Address(x, xstart, Address::lsl(LogBytesPerInt)));
2876   ldr(x_xstart, Address(rscratch1));
2877   ror(x_xstart, x_xstart, 32); // convert big-endian to little-endian
2878 
2879   bind(L_first_loop);
2880   subsw(idx, idx, 1);
2881   br(Assembler::MI, L_first_loop_exit);
2882   subsw(idx, idx, 1);
2883   br(Assembler::MI, L_one_y);
2884   lea(rscratch1, Address(y, idx, Address::uxtw(LogBytesPerInt)));
2885   ldr(y_idx, Address(rscratch1));
2886   ror(y_idx, y_idx, 32); // convert big-endian to little-endian
2887   bind(L_multiply);
2888 
2889   // AArch64 has a multiply-accumulate instruction that we can&#39;t use
2890   // here because it has no way to process carries, so we have to use
2891   // separate add and adc instructions.  Bah.
2892   umulh(rscratch1, x_xstart, y_idx); // x_xstart * y_idx -&gt; rscratch1:product
2893   mul(product, x_xstart, y_idx);
2894   adds(product, product, carry);
2895   adc(carry, rscratch1, zr);   // x_xstart * y_idx + carry -&gt; carry:product
2896 
2897   subw(kdx, kdx, 2);
2898   ror(product, product, 32); // back to big-endian
2899   str(product, offsetted_address(z, kdx, Address::uxtw(LogBytesPerInt), 0, BytesPerLong));
2900 
2901   b(L_first_loop);
2902 
2903   bind(L_one_y);
2904   ldrw(y_idx, Address(y,  0));
2905   b(L_multiply);
2906 
2907   bind(L_one_x);
2908   ldrw(x_xstart, Address(x,  0));
2909   b(L_first_loop);
2910 
2911   bind(L_first_loop_exit);
2912 }
2913 
2914 /**
2915  * Multiply 128 bit by 128. Unrolled inner loop.
2916  *
2917  */
2918 void MacroAssembler::multiply_128_x_128_loop(Register y, Register z,
2919                                              Register carry, Register carry2,
2920                                              Register idx, Register jdx,
2921                                              Register yz_idx1, Register yz_idx2,
2922                                              Register tmp, Register tmp3, Register tmp4,
2923                                              Register tmp6, Register product_hi) {
2924 
2925   //   jlong carry, x[], y[], z[];
2926   //   int kdx = ystart+1;
2927   //   for (int idx=ystart-2; idx &gt;= 0; idx -= 2) { // Third loop
2928   //     huge_128 tmp3 = (y[idx+1] * product_hi) + z[kdx+idx+1] + carry;
2929   //     jlong carry2  = (jlong)(tmp3 &gt;&gt;&gt; 64);
2930   //     huge_128 tmp4 = (y[idx]   * product_hi) + z[kdx+idx] + carry2;
2931   //     carry  = (jlong)(tmp4 &gt;&gt;&gt; 64);
2932   //     z[kdx+idx+1] = (jlong)tmp3;
2933   //     z[kdx+idx] = (jlong)tmp4;
2934   //   }
2935   //   idx += 2;
2936   //   if (idx &gt; 0) {
2937   //     yz_idx1 = (y[idx] * product_hi) + z[kdx+idx] + carry;
2938   //     z[kdx+idx] = (jlong)yz_idx1;
2939   //     carry  = (jlong)(yz_idx1 &gt;&gt;&gt; 64);
2940   //   }
2941   //
2942 
2943   Label L_third_loop, L_third_loop_exit, L_post_third_loop_done;
2944 
2945   lsrw(jdx, idx, 2);
2946 
2947   bind(L_third_loop);
2948 
2949   subsw(jdx, jdx, 1);
2950   br(Assembler::MI, L_third_loop_exit);
2951   subw(idx, idx, 4);
2952 
2953   lea(rscratch1, Address(y, idx, Address::uxtw(LogBytesPerInt)));
2954 
2955   ldp(yz_idx2, yz_idx1, Address(rscratch1, 0));
2956 
2957   lea(tmp6, Address(z, idx, Address::uxtw(LogBytesPerInt)));
2958 
2959   ror(yz_idx1, yz_idx1, 32); // convert big-endian to little-endian
2960   ror(yz_idx2, yz_idx2, 32);
2961 
2962   ldp(rscratch2, rscratch1, Address(tmp6, 0));
2963 
2964   mul(tmp3, product_hi, yz_idx1);  //  yz_idx1 * product_hi -&gt; tmp4:tmp3
2965   umulh(tmp4, product_hi, yz_idx1);
2966 
2967   ror(rscratch1, rscratch1, 32); // convert big-endian to little-endian
2968   ror(rscratch2, rscratch2, 32);
2969 
2970   mul(tmp, product_hi, yz_idx2);   //  yz_idx2 * product_hi -&gt; carry2:tmp
2971   umulh(carry2, product_hi, yz_idx2);
2972 
2973   // propagate sum of both multiplications into carry:tmp4:tmp3
2974   adds(tmp3, tmp3, carry);
2975   adc(tmp4, tmp4, zr);
2976   adds(tmp3, tmp3, rscratch1);
2977   adcs(tmp4, tmp4, tmp);
2978   adc(carry, carry2, zr);
2979   adds(tmp4, tmp4, rscratch2);
2980   adc(carry, carry, zr);
2981 
2982   ror(tmp3, tmp3, 32); // convert little-endian to big-endian
2983   ror(tmp4, tmp4, 32);
2984   stp(tmp4, tmp3, Address(tmp6, 0));
2985 
2986   b(L_third_loop);
2987   bind (L_third_loop_exit);
2988 
2989   andw (idx, idx, 0x3);
2990   cbz(idx, L_post_third_loop_done);
2991 
2992   Label L_check_1;
2993   subsw(idx, idx, 2);
2994   br(Assembler::MI, L_check_1);
2995 
2996   lea(rscratch1, Address(y, idx, Address::uxtw(LogBytesPerInt)));
2997   ldr(yz_idx1, Address(rscratch1, 0));
2998   ror(yz_idx1, yz_idx1, 32);
2999   mul(tmp3, product_hi, yz_idx1);  //  yz_idx1 * product_hi -&gt; tmp4:tmp3
3000   umulh(tmp4, product_hi, yz_idx1);
3001   lea(rscratch1, Address(z, idx, Address::uxtw(LogBytesPerInt)));
3002   ldr(yz_idx2, Address(rscratch1, 0));
3003   ror(yz_idx2, yz_idx2, 32);
3004 
3005   add2_with_carry(carry, tmp4, tmp3, carry, yz_idx2);
3006 
3007   ror(tmp3, tmp3, 32);
3008   str(tmp3, Address(rscratch1, 0));
3009 
3010   bind (L_check_1);
3011 
3012   andw (idx, idx, 0x1);
3013   subsw(idx, idx, 1);
3014   br(Assembler::MI, L_post_third_loop_done);
3015   ldrw(tmp4, Address(y, idx, Address::uxtw(LogBytesPerInt)));
3016   mul(tmp3, tmp4, product_hi);  //  tmp4 * product_hi -&gt; carry2:tmp3
3017   umulh(carry2, tmp4, product_hi);
3018   ldrw(tmp4, Address(z, idx, Address::uxtw(LogBytesPerInt)));
3019 
3020   add2_with_carry(carry2, tmp3, tmp4, carry);
3021 
3022   strw(tmp3, Address(z, idx, Address::uxtw(LogBytesPerInt)));
3023   extr(carry, carry2, tmp3, 32);
3024 
3025   bind(L_post_third_loop_done);
3026 }
3027 
3028 /**
3029  * Code for BigInteger::multiplyToLen() instrinsic.
3030  *
3031  * r0: x
3032  * r1: xlen
3033  * r2: y
3034  * r3: ylen
3035  * r4:  z
3036  * r5: zlen
3037  * r10: tmp1
3038  * r11: tmp2
3039  * r12: tmp3
3040  * r13: tmp4
3041  * r14: tmp5
3042  * r15: tmp6
3043  * r16: tmp7
3044  *
3045  */
3046 void MacroAssembler::multiply_to_len(Register x, Register xlen, Register y, Register ylen,
3047                                      Register z, Register zlen,
3048                                      Register tmp1, Register tmp2, Register tmp3, Register tmp4,
3049                                      Register tmp5, Register tmp6, Register product_hi) {
3050 
3051   assert_different_registers(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6);
3052 
3053   const Register idx = tmp1;
3054   const Register kdx = tmp2;
3055   const Register xstart = tmp3;
3056 
3057   const Register y_idx = tmp4;
3058   const Register carry = tmp5;
3059   const Register product  = xlen;
3060   const Register x_xstart = zlen;  // reuse register
3061 
3062   // First Loop.
3063   //
3064   //  final static long LONG_MASK = 0xffffffffL;
3065   //  int xstart = xlen - 1;
3066   //  int ystart = ylen - 1;
3067   //  long carry = 0;
3068   //  for (int idx=ystart, kdx=ystart+1+xstart; idx &gt;= 0; idx-, kdx--) {
3069   //    long product = (y[idx] &amp; LONG_MASK) * (x[xstart] &amp; LONG_MASK) + carry;
3070   //    z[kdx] = (int)product;
3071   //    carry = product &gt;&gt;&gt; 32;
3072   //  }
3073   //  z[xstart] = (int)carry;
3074   //
3075 
3076   movw(idx, ylen);      // idx = ylen;
3077   movw(kdx, zlen);      // kdx = xlen+ylen;
3078   mov(carry, zr);       // carry = 0;
3079 
3080   Label L_done;
3081 
3082   movw(xstart, xlen);
3083   subsw(xstart, xstart, 1);
3084   br(Assembler::MI, L_done);
3085 
3086   multiply_64_x_64_loop(x, xstart, x_xstart, y, y_idx, z, carry, product, idx, kdx);
3087 
3088   Label L_second_loop;
3089   cbzw(kdx, L_second_loop);
3090 
3091   Label L_carry;
3092   subw(kdx, kdx, 1);
3093   cbzw(kdx, L_carry);
3094 
3095   strw(carry, Address(z, kdx, Address::uxtw(LogBytesPerInt)));
3096   lsr(carry, carry, 32);
3097   subw(kdx, kdx, 1);
3098 
3099   bind(L_carry);
3100   strw(carry, Address(z, kdx, Address::uxtw(LogBytesPerInt)));
3101 
3102   // Second and third (nested) loops.
3103   //
3104   // for (int i = xstart-1; i &gt;= 0; i--) { // Second loop
3105   //   carry = 0;
3106   //   for (int jdx=ystart, k=ystart+1+i; jdx &gt;= 0; jdx--, k--) { // Third loop
3107   //     long product = (y[jdx] &amp; LONG_MASK) * (x[i] &amp; LONG_MASK) +
3108   //                    (z[k] &amp; LONG_MASK) + carry;
3109   //     z[k] = (int)product;
3110   //     carry = product &gt;&gt;&gt; 32;
3111   //   }
3112   //   z[i] = (int)carry;
3113   // }
3114   //
3115   // i = xlen, j = tmp1, k = tmp2, carry = tmp5, x[i] = product_hi
3116 
3117   const Register jdx = tmp1;
3118 
3119   bind(L_second_loop);
3120   mov(carry, zr);                // carry = 0;
3121   movw(jdx, ylen);               // j = ystart+1
3122 
3123   subsw(xstart, xstart, 1);      // i = xstart-1;
3124   br(Assembler::MI, L_done);
3125 
3126   str(z, Address(pre(sp, -4 * wordSize)));
3127 
3128   Label L_last_x;
3129   lea(z, offsetted_address(z, xstart, Address::uxtw(LogBytesPerInt), 4, BytesPerInt)); // z = z + k - j
3130   subsw(xstart, xstart, 1);       // i = xstart-1;
3131   br(Assembler::MI, L_last_x);
3132 
3133   lea(rscratch1, Address(x, xstart, Address::uxtw(LogBytesPerInt)));
3134   ldr(product_hi, Address(rscratch1));
3135   ror(product_hi, product_hi, 32);  // convert big-endian to little-endian
3136 
3137   Label L_third_loop_prologue;
3138   bind(L_third_loop_prologue);
3139 
3140   str(ylen, Address(sp, wordSize));
3141   stp(x, xstart, Address(sp, 2 * wordSize));
3142   multiply_128_x_128_loop(y, z, carry, x, jdx, ylen, product,
3143                           tmp2, x_xstart, tmp3, tmp4, tmp6, product_hi);
3144   ldp(z, ylen, Address(post(sp, 2 * wordSize)));
3145   ldp(x, xlen, Address(post(sp, 2 * wordSize)));   // copy old xstart -&gt; xlen
3146 
3147   addw(tmp3, xlen, 1);
3148   strw(carry, Address(z, tmp3, Address::uxtw(LogBytesPerInt)));
3149   subsw(tmp3, tmp3, 1);
3150   br(Assembler::MI, L_done);
3151 
3152   lsr(carry, carry, 32);
3153   strw(carry, Address(z, tmp3, Address::uxtw(LogBytesPerInt)));
3154   b(L_second_loop);
3155 
3156   // Next infrequent code is moved outside loops.
3157   bind(L_last_x);
3158   ldrw(product_hi, Address(x,  0));
3159   b(L_third_loop_prologue);
3160 
3161   bind(L_done);
3162 }
3163 
3164 // Code for BigInteger::mulAdd instrinsic
3165 // out     = r0
3166 // in      = r1
3167 // offset  = r2  (already out.length-offset)
3168 // len     = r3
3169 // k       = r4
3170 //
3171 // pseudo code from java implementation:
3172 // carry = 0;
3173 // offset = out.length-offset - 1;
3174 // for (int j=len-1; j &gt;= 0; j--) {
3175 //     product = (in[j] &amp; LONG_MASK) * kLong + (out[offset] &amp; LONG_MASK) + carry;
3176 //     out[offset--] = (int)product;
3177 //     carry = product &gt;&gt;&gt; 32;
3178 // }
3179 // return (int)carry;
3180 void MacroAssembler::mul_add(Register out, Register in, Register offset,
3181       Register len, Register k) {
3182     Label LOOP, END;
3183     // pre-loop
3184     cmp(len, zr); // cmp, not cbz/cbnz: to use condition twice =&gt; less branches
3185     csel(out, zr, out, Assembler::EQ);
3186     br(Assembler::EQ, END);
3187     add(in, in, len, LSL, 2); // in[j+1] address
3188     add(offset, out, offset, LSL, 2); // out[offset + 1] address
3189     mov(out, zr); // used to keep carry now
3190     BIND(LOOP);
3191     ldrw(rscratch1, Address(pre(in, -4)));
3192     madd(rscratch1, rscratch1, k, out);
3193     ldrw(rscratch2, Address(pre(offset, -4)));
3194     add(rscratch1, rscratch1, rscratch2);
3195     strw(rscratch1, Address(offset));
3196     lsr(out, rscratch1, 32);
3197     subs(len, len, 1);
3198     br(Assembler::NE, LOOP);
3199     BIND(END);
3200 }
3201 
3202 /**
3203  * Emits code to update CRC-32 with a byte value according to constants in table
3204  *
3205  * @param [in,out]crc   Register containing the crc.
3206  * @param [in]val       Register containing the byte to fold into the CRC.
3207  * @param [in]table     Register containing the table of crc constants.
3208  *
3209  * uint32_t crc;
3210  * val = crc_table[(val ^ crc) &amp; 0xFF];
3211  * crc = val ^ (crc &gt;&gt; 8);
3212  *
3213  */
3214 void MacroAssembler::update_byte_crc32(Register crc, Register val, Register table) {
3215   eor(val, val, crc);
3216   andr(val, val, 0xff);
3217   ldrw(val, Address(table, val, Address::lsl(2)));
3218   eor(crc, val, crc, Assembler::LSR, 8);
3219 }
3220 
3221 /**
3222  * Emits code to update CRC-32 with a 32-bit value according to tables 0 to 3
3223  *
3224  * @param [in,out]crc   Register containing the crc.
3225  * @param [in]v         Register containing the 32-bit to fold into the CRC.
3226  * @param [in]table0    Register containing table 0 of crc constants.
3227  * @param [in]table1    Register containing table 1 of crc constants.
3228  * @param [in]table2    Register containing table 2 of crc constants.
3229  * @param [in]table3    Register containing table 3 of crc constants.
3230  *
3231  * uint32_t crc;
3232  *   v = crc ^ v
3233  *   crc = table3[v&amp;0xff]^table2[(v&gt;&gt;8)&amp;0xff]^table1[(v&gt;&gt;16)&amp;0xff]^table0[v&gt;&gt;24]
3234  *
3235  */
3236 void MacroAssembler::update_word_crc32(Register crc, Register v, Register tmp,
3237         Register table0, Register table1, Register table2, Register table3,
3238         bool upper) {
3239   eor(v, crc, v, upper ? LSR:LSL, upper ? 32:0);
3240   uxtb(tmp, v);
3241   ldrw(crc, Address(table3, tmp, Address::lsl(2)));
3242   ubfx(tmp, v, 8, 8);
3243   ldrw(tmp, Address(table2, tmp, Address::lsl(2)));
3244   eor(crc, crc, tmp);
3245   ubfx(tmp, v, 16, 8);
3246   ldrw(tmp, Address(table1, tmp, Address::lsl(2)));
3247   eor(crc, crc, tmp);
3248   ubfx(tmp, v, 24, 8);
3249   ldrw(tmp, Address(table0, tmp, Address::lsl(2)));
3250   eor(crc, crc, tmp);
3251 }
3252 
3253 void MacroAssembler::kernel_crc32_using_crc32(Register crc, Register buf,
3254         Register len, Register tmp0, Register tmp1, Register tmp2,
3255         Register tmp3) {
3256     Label CRC_by64_loop, CRC_by4_loop, CRC_by1_loop, CRC_less64, CRC_by64_pre, CRC_by32_loop, CRC_less32, L_exit;
3257     assert_different_registers(crc, buf, len, tmp0, tmp1, tmp2, tmp3);
3258 
3259     mvnw(crc, crc);
3260 
3261     subs(len, len, 128);
3262     br(Assembler::GE, CRC_by64_pre);
3263   BIND(CRC_less64);
3264     adds(len, len, 128-32);
3265     br(Assembler::GE, CRC_by32_loop);
3266   BIND(CRC_less32);
3267     adds(len, len, 32-4);
3268     br(Assembler::GE, CRC_by4_loop);
3269     adds(len, len, 4);
3270     br(Assembler::GT, CRC_by1_loop);
3271     b(L_exit);
3272 
3273   BIND(CRC_by32_loop);
3274     ldp(tmp0, tmp1, Address(post(buf, 16)));
3275     subs(len, len, 32);
3276     crc32x(crc, crc, tmp0);
3277     ldr(tmp2, Address(post(buf, 8)));
3278     crc32x(crc, crc, tmp1);
3279     ldr(tmp3, Address(post(buf, 8)));
3280     crc32x(crc, crc, tmp2);
3281     crc32x(crc, crc, tmp3);
3282     br(Assembler::GE, CRC_by32_loop);
3283     cmn(len, 32);
3284     br(Assembler::NE, CRC_less32);
3285     b(L_exit);
3286 
3287   BIND(CRC_by4_loop);
3288     ldrw(tmp0, Address(post(buf, 4)));
3289     subs(len, len, 4);
3290     crc32w(crc, crc, tmp0);
3291     br(Assembler::GE, CRC_by4_loop);
3292     adds(len, len, 4);
3293     br(Assembler::LE, L_exit);
3294   BIND(CRC_by1_loop);
3295     ldrb(tmp0, Address(post(buf, 1)));
3296     subs(len, len, 1);
3297     crc32b(crc, crc, tmp0);
3298     br(Assembler::GT, CRC_by1_loop);
3299     b(L_exit);
3300 
3301   BIND(CRC_by64_pre);
3302     sub(buf, buf, 8);
3303     ldp(tmp0, tmp1, Address(buf, 8));
3304     crc32x(crc, crc, tmp0);
3305     ldr(tmp2, Address(buf, 24));
3306     crc32x(crc, crc, tmp1);
3307     ldr(tmp3, Address(buf, 32));
3308     crc32x(crc, crc, tmp2);
3309     ldr(tmp0, Address(buf, 40));
3310     crc32x(crc, crc, tmp3);
3311     ldr(tmp1, Address(buf, 48));
3312     crc32x(crc, crc, tmp0);
3313     ldr(tmp2, Address(buf, 56));
3314     crc32x(crc, crc, tmp1);
3315     ldr(tmp3, Address(pre(buf, 64)));
3316 
3317     b(CRC_by64_loop);
3318 
3319     align(CodeEntryAlignment);
3320   BIND(CRC_by64_loop);
3321     subs(len, len, 64);
3322     crc32x(crc, crc, tmp2);
3323     ldr(tmp0, Address(buf, 8));
3324     crc32x(crc, crc, tmp3);
3325     ldr(tmp1, Address(buf, 16));
3326     crc32x(crc, crc, tmp0);
3327     ldr(tmp2, Address(buf, 24));
3328     crc32x(crc, crc, tmp1);
3329     ldr(tmp3, Address(buf, 32));
3330     crc32x(crc, crc, tmp2);
3331     ldr(tmp0, Address(buf, 40));
3332     crc32x(crc, crc, tmp3);
3333     ldr(tmp1, Address(buf, 48));
3334     crc32x(crc, crc, tmp0);
3335     ldr(tmp2, Address(buf, 56));
3336     crc32x(crc, crc, tmp1);
3337     ldr(tmp3, Address(pre(buf, 64)));
3338     br(Assembler::GE, CRC_by64_loop);
3339 
3340     // post-loop
3341     crc32x(crc, crc, tmp2);
3342     crc32x(crc, crc, tmp3);
3343 
3344     sub(len, len, 64);
3345     add(buf, buf, 8);
3346     cmn(len, 128);
3347     br(Assembler::NE, CRC_less64);
3348   BIND(L_exit);
3349     mvnw(crc, crc);
3350 }
3351 
3352 /**
3353  * @param crc   register containing existing CRC (32-bit)
3354  * @param buf   register pointing to input byte buffer (byte*)
3355  * @param len   register containing number of bytes
3356  * @param table register that will contain address of CRC table
3357  * @param tmp   scratch register
3358  */
3359 void MacroAssembler::kernel_crc32(Register crc, Register buf, Register len,
3360         Register table0, Register table1, Register table2, Register table3,
3361         Register tmp, Register tmp2, Register tmp3) {
3362   Label L_by16, L_by16_loop, L_by4, L_by4_loop, L_by1, L_by1_loop, L_exit;
3363   unsigned long offset;
3364 
3365   if (UseCRC32) {
3366       kernel_crc32_using_crc32(crc, buf, len, table0, table1, table2, table3);
3367       return;
3368   }
3369 
3370     mvnw(crc, crc);
3371 
3372     adrp(table0, ExternalAddress(StubRoutines::crc_table_addr()), offset);
3373     if (offset) add(table0, table0, offset);
3374     add(table1, table0, 1*256*sizeof(juint));
3375     add(table2, table0, 2*256*sizeof(juint));
3376     add(table3, table0, 3*256*sizeof(juint));
3377 
3378   if (UseNeon) {
3379       cmp(len, (u1)64);
3380       br(Assembler::LT, L_by16);
3381       eor(v16, T16B, v16, v16);
3382 
3383     Label L_fold;
3384 
3385       add(tmp, table0, 4*256*sizeof(juint)); // Point at the Neon constants
3386 
3387       ld1(v0, v1, T2D, post(buf, 32));
3388       ld1r(v4, T2D, post(tmp, 8));
3389       ld1r(v5, T2D, post(tmp, 8));
3390       ld1r(v6, T2D, post(tmp, 8));
3391       ld1r(v7, T2D, post(tmp, 8));
3392       mov(v16, T4S, 0, crc);
3393 
3394       eor(v0, T16B, v0, v16);
3395       sub(len, len, 64);
3396 
3397     BIND(L_fold);
3398       pmull(v22, T8H, v0, v5, T8B);
3399       pmull(v20, T8H, v0, v7, T8B);
3400       pmull(v23, T8H, v0, v4, T8B);
3401       pmull(v21, T8H, v0, v6, T8B);
3402 
3403       pmull2(v18, T8H, v0, v5, T16B);
3404       pmull2(v16, T8H, v0, v7, T16B);
3405       pmull2(v19, T8H, v0, v4, T16B);
3406       pmull2(v17, T8H, v0, v6, T16B);
3407 
3408       uzp1(v24, T8H, v20, v22);
3409       uzp2(v25, T8H, v20, v22);
3410       eor(v20, T16B, v24, v25);
3411 
3412       uzp1(v26, T8H, v16, v18);
3413       uzp2(v27, T8H, v16, v18);
3414       eor(v16, T16B, v26, v27);
3415 
3416       ushll2(v22, T4S, v20, T8H, 8);
3417       ushll(v20, T4S, v20, T4H, 8);
3418 
3419       ushll2(v18, T4S, v16, T8H, 8);
3420       ushll(v16, T4S, v16, T4H, 8);
3421 
3422       eor(v22, T16B, v23, v22);
3423       eor(v18, T16B, v19, v18);
3424       eor(v20, T16B, v21, v20);
3425       eor(v16, T16B, v17, v16);
3426 
3427       uzp1(v17, T2D, v16, v20);
3428       uzp2(v21, T2D, v16, v20);
3429       eor(v17, T16B, v17, v21);
3430 
3431       ushll2(v20, T2D, v17, T4S, 16);
3432       ushll(v16, T2D, v17, T2S, 16);
3433 
3434       eor(v20, T16B, v20, v22);
3435       eor(v16, T16B, v16, v18);
3436 
3437       uzp1(v17, T2D, v20, v16);
3438       uzp2(v21, T2D, v20, v16);
3439       eor(v28, T16B, v17, v21);
3440 
3441       pmull(v22, T8H, v1, v5, T8B);
3442       pmull(v20, T8H, v1, v7, T8B);
3443       pmull(v23, T8H, v1, v4, T8B);
3444       pmull(v21, T8H, v1, v6, T8B);
3445 
3446       pmull2(v18, T8H, v1, v5, T16B);
3447       pmull2(v16, T8H, v1, v7, T16B);
3448       pmull2(v19, T8H, v1, v4, T16B);
3449       pmull2(v17, T8H, v1, v6, T16B);
3450 
3451       ld1(v0, v1, T2D, post(buf, 32));
3452 
3453       uzp1(v24, T8H, v20, v22);
3454       uzp2(v25, T8H, v20, v22);
3455       eor(v20, T16B, v24, v25);
3456 
3457       uzp1(v26, T8H, v16, v18);
3458       uzp2(v27, T8H, v16, v18);
3459       eor(v16, T16B, v26, v27);
3460 
3461       ushll2(v22, T4S, v20, T8H, 8);
3462       ushll(v20, T4S, v20, T4H, 8);
3463 
3464       ushll2(v18, T4S, v16, T8H, 8);
3465       ushll(v16, T4S, v16, T4H, 8);
3466 
3467       eor(v22, T16B, v23, v22);
3468       eor(v18, T16B, v19, v18);
3469       eor(v20, T16B, v21, v20);
3470       eor(v16, T16B, v17, v16);
3471 
3472       uzp1(v17, T2D, v16, v20);
3473       uzp2(v21, T2D, v16, v20);
3474       eor(v16, T16B, v17, v21);
3475 
3476       ushll2(v20, T2D, v16, T4S, 16);
3477       ushll(v16, T2D, v16, T2S, 16);
3478 
3479       eor(v20, T16B, v22, v20);
3480       eor(v16, T16B, v16, v18);
3481 
3482       uzp1(v17, T2D, v20, v16);
3483       uzp2(v21, T2D, v20, v16);
3484       eor(v20, T16B, v17, v21);
3485 
3486       shl(v16, T2D, v28, 1);
3487       shl(v17, T2D, v20, 1);
3488 
3489       eor(v0, T16B, v0, v16);
3490       eor(v1, T16B, v1, v17);
3491 
3492       subs(len, len, 32);
3493       br(Assembler::GE, L_fold);
3494 
3495       mov(crc, 0);
3496       mov(tmp, v0, T1D, 0);
3497       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, false);
3498       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, true);
3499       mov(tmp, v0, T1D, 1);
3500       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, false);
3501       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, true);
3502       mov(tmp, v1, T1D, 0);
3503       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, false);
3504       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, true);
3505       mov(tmp, v1, T1D, 1);
3506       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, false);
3507       update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, true);
3508 
3509       add(len, len, 32);
3510   }
3511 
3512   BIND(L_by16);
3513     subs(len, len, 16);
3514     br(Assembler::GE, L_by16_loop);
3515     adds(len, len, 16-4);
3516     br(Assembler::GE, L_by4_loop);
3517     adds(len, len, 4);
3518     br(Assembler::GT, L_by1_loop);
3519     b(L_exit);
3520 
3521   BIND(L_by4_loop);
3522     ldrw(tmp, Address(post(buf, 4)));
3523     update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3);
3524     subs(len, len, 4);
3525     br(Assembler::GE, L_by4_loop);
3526     adds(len, len, 4);
3527     br(Assembler::LE, L_exit);
3528   BIND(L_by1_loop);
3529     subs(len, len, 1);
3530     ldrb(tmp, Address(post(buf, 1)));
3531     update_byte_crc32(crc, tmp, table0);
3532     br(Assembler::GT, L_by1_loop);
3533     b(L_exit);
3534 
3535     align(CodeEntryAlignment);
3536   BIND(L_by16_loop);
3537     subs(len, len, 16);
3538     ldp(tmp, tmp3, Address(post(buf, 16)));
3539     update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, false);
3540     update_word_crc32(crc, tmp, tmp2, table0, table1, table2, table3, true);
3541     update_word_crc32(crc, tmp3, tmp2, table0, table1, table2, table3, false);
3542     update_word_crc32(crc, tmp3, tmp2, table0, table1, table2, table3, true);
3543     br(Assembler::GE, L_by16_loop);
3544     adds(len, len, 16-4);
3545     br(Assembler::GE, L_by4_loop);
3546     adds(len, len, 4);
3547     br(Assembler::GT, L_by1_loop);
3548   BIND(L_exit);
3549     mvnw(crc, crc);
3550 }
3551 
3552 void MacroAssembler::kernel_crc32c_using_crc32c(Register crc, Register buf,
3553         Register len, Register tmp0, Register tmp1, Register tmp2,
3554         Register tmp3) {
3555     Label CRC_by64_loop, CRC_by4_loop, CRC_by1_loop, CRC_less64, CRC_by64_pre, CRC_by32_loop, CRC_less32, L_exit;
3556     assert_different_registers(crc, buf, len, tmp0, tmp1, tmp2, tmp3);
3557 
3558     subs(len, len, 128);
3559     br(Assembler::GE, CRC_by64_pre);
3560   BIND(CRC_less64);
3561     adds(len, len, 128-32);
3562     br(Assembler::GE, CRC_by32_loop);
3563   BIND(CRC_less32);
3564     adds(len, len, 32-4);
3565     br(Assembler::GE, CRC_by4_loop);
3566     adds(len, len, 4);
3567     br(Assembler::GT, CRC_by1_loop);
3568     b(L_exit);
3569 
3570   BIND(CRC_by32_loop);
3571     ldp(tmp0, tmp1, Address(post(buf, 16)));
3572     subs(len, len, 32);
3573     crc32cx(crc, crc, tmp0);
3574     ldr(tmp2, Address(post(buf, 8)));
3575     crc32cx(crc, crc, tmp1);
3576     ldr(tmp3, Address(post(buf, 8)));
3577     crc32cx(crc, crc, tmp2);
3578     crc32cx(crc, crc, tmp3);
3579     br(Assembler::GE, CRC_by32_loop);
3580     cmn(len, 32);
3581     br(Assembler::NE, CRC_less32);
3582     b(L_exit);
3583 
3584   BIND(CRC_by4_loop);
3585     ldrw(tmp0, Address(post(buf, 4)));
3586     subs(len, len, 4);
3587     crc32cw(crc, crc, tmp0);
3588     br(Assembler::GE, CRC_by4_loop);
3589     adds(len, len, 4);
3590     br(Assembler::LE, L_exit);
3591   BIND(CRC_by1_loop);
3592     ldrb(tmp0, Address(post(buf, 1)));
3593     subs(len, len, 1);
3594     crc32cb(crc, crc, tmp0);
3595     br(Assembler::GT, CRC_by1_loop);
3596     b(L_exit);
3597 
3598   BIND(CRC_by64_pre);
3599     sub(buf, buf, 8);
3600     ldp(tmp0, tmp1, Address(buf, 8));
3601     crc32cx(crc, crc, tmp0);
3602     ldr(tmp2, Address(buf, 24));
3603     crc32cx(crc, crc, tmp1);
3604     ldr(tmp3, Address(buf, 32));
3605     crc32cx(crc, crc, tmp2);
3606     ldr(tmp0, Address(buf, 40));
3607     crc32cx(crc, crc, tmp3);
3608     ldr(tmp1, Address(buf, 48));
3609     crc32cx(crc, crc, tmp0);
3610     ldr(tmp2, Address(buf, 56));
3611     crc32cx(crc, crc, tmp1);
3612     ldr(tmp3, Address(pre(buf, 64)));
3613 
3614     b(CRC_by64_loop);
3615 
3616     align(CodeEntryAlignment);
3617   BIND(CRC_by64_loop);
3618     subs(len, len, 64);
3619     crc32cx(crc, crc, tmp2);
3620     ldr(tmp0, Address(buf, 8));
3621     crc32cx(crc, crc, tmp3);
3622     ldr(tmp1, Address(buf, 16));
3623     crc32cx(crc, crc, tmp0);
3624     ldr(tmp2, Address(buf, 24));
3625     crc32cx(crc, crc, tmp1);
3626     ldr(tmp3, Address(buf, 32));
3627     crc32cx(crc, crc, tmp2);
3628     ldr(tmp0, Address(buf, 40));
3629     crc32cx(crc, crc, tmp3);
3630     ldr(tmp1, Address(buf, 48));
3631     crc32cx(crc, crc, tmp0);
3632     ldr(tmp2, Address(buf, 56));
3633     crc32cx(crc, crc, tmp1);
3634     ldr(tmp3, Address(pre(buf, 64)));
3635     br(Assembler::GE, CRC_by64_loop);
3636 
3637     // post-loop
3638     crc32cx(crc, crc, tmp2);
3639     crc32cx(crc, crc, tmp3);
3640 
3641     sub(len, len, 64);
3642     add(buf, buf, 8);
3643     cmn(len, 128);
3644     br(Assembler::NE, CRC_less64);
3645   BIND(L_exit);
3646 }
3647 
3648 /**
3649  * @param crc   register containing existing CRC (32-bit)
3650  * @param buf   register pointing to input byte buffer (byte*)
3651  * @param len   register containing number of bytes
3652  * @param table register that will contain address of CRC table
3653  * @param tmp   scratch register
3654  */
3655 void MacroAssembler::kernel_crc32c(Register crc, Register buf, Register len,
3656         Register table0, Register table1, Register table2, Register table3,
3657         Register tmp, Register tmp2, Register tmp3) {
3658   kernel_crc32c_using_crc32c(crc, buf, len, table0, table1, table2, table3);
3659 }
3660 
3661 
3662 SkipIfEqual::SkipIfEqual(
3663     MacroAssembler* masm, const bool* flag_addr, bool value) {
3664   _masm = masm;
3665   unsigned long offset;
3666   _masm-&gt;adrp(rscratch1, ExternalAddress((address)flag_addr), offset);
3667   _masm-&gt;ldrb(rscratch1, Address(rscratch1, offset));
3668   _masm-&gt;cbzw(rscratch1, _label);
3669 }
3670 
3671 SkipIfEqual::~SkipIfEqual() {
3672   _masm-&gt;bind(_label);
3673 }
3674 
3675 void MacroAssembler::addptr(const Address &amp;dst, int32_t src) {
3676   Address adr;
3677   switch(dst.getMode()) {
3678   case Address::base_plus_offset:
3679     // This is the expected mode, although we allow all the other
3680     // forms below.
3681     adr = form_address(rscratch2, dst.base(), dst.offset(), LogBytesPerWord);
3682     break;
3683   default:
3684     lea(rscratch2, dst);
3685     adr = Address(rscratch2);
3686     break;
3687   }
3688   ldr(rscratch1, adr);
3689   add(rscratch1, rscratch1, src);
3690   str(rscratch1, adr);
3691 }
3692 
3693 void MacroAssembler::cmpptr(Register src1, Address src2) {
3694   unsigned long offset;
3695   adrp(rscratch1, src2, offset);
3696   ldr(rscratch1, Address(rscratch1, offset));
3697   cmp(src1, rscratch1);
3698 }
3699 
3700 void MacroAssembler::cmpoop(Register obj1, Register obj2) {
3701   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
3702   bs-&gt;obj_equals(this, obj1, obj2);
3703 }
3704 
3705 void MacroAssembler::load_method_holder(Register holder, Register method) {
3706   ldr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
3707   ldr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
3708   ldr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
3709 }
3710 
3711 void MacroAssembler::load_klass(Register dst, Register src) {
3712   if (UseCompressedClassPointers) {
3713     ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3714     decode_klass_not_null(dst);
3715   } else {
3716     ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3717   }
3718 }
3719 
3720 // ((OopHandle)result).resolve();
3721 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
3722   // OopHandle::resolve is an indirection.
3723   access_load_at(T_OBJECT, IN_NATIVE, result, Address(result, 0), tmp, noreg);
3724 }
3725 
3726 void MacroAssembler::load_mirror(Register dst, Register method, Register tmp) {
3727   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
3728   ldr(dst, Address(rmethod, Method::const_offset()));
3729   ldr(dst, Address(dst, ConstMethod::constants_offset()));
3730   ldr(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));
3731   ldr(dst, Address(dst, mirror_offset));
3732   resolve_oop_handle(dst, tmp);
3733 }
3734 
3735 void MacroAssembler::cmp_klass(Register oop, Register trial_klass, Register tmp) {
3736   if (UseCompressedClassPointers) {
3737     ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3738     if (CompressedKlassPointers::base() == NULL) {
3739       cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());
3740       return;
3741     } else if (((uint64_t)CompressedKlassPointers::base() &amp; 0xffffffff) == 0
3742                &amp;&amp; CompressedKlassPointers::shift() == 0) {
3743       // Only the bottom 32 bits matter
3744       cmpw(trial_klass, tmp);
3745       return;
3746     }
3747     decode_klass_not_null(tmp);
3748   } else {
3749     ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3750   }
3751   cmp(trial_klass, tmp);
3752 }
3753 
3754 void MacroAssembler::load_prototype_header(Register dst, Register src) {
3755   load_klass(dst, src);
3756   ldr(dst, Address(dst, Klass::prototype_header_offset()));
3757 }
3758 
3759 void MacroAssembler::store_klass(Register dst, Register src) {
3760   // FIXME: Should this be a store release?  concurrent gcs assumes
3761   // klass length is valid if klass field is not null.
3762   if (UseCompressedClassPointers) {
3763     encode_klass_not_null(src);
3764     strw(src, Address(dst, oopDesc::klass_offset_in_bytes()));
3765   } else {
3766     str(src, Address(dst, oopDesc::klass_offset_in_bytes()));
3767   }
3768 }
3769 
3770 void MacroAssembler::store_klass_gap(Register dst, Register src) {
3771   if (UseCompressedClassPointers) {
3772     // Store to klass gap in destination
3773     strw(src, Address(dst, oopDesc::klass_gap_offset_in_bytes()));
3774   }
3775 }
3776 
3777 // Algorithm must match CompressedOops::encode.
3778 void MacroAssembler::encode_heap_oop(Register d, Register s) {
3779 #ifdef ASSERT
3780   verify_heapbase(&quot;MacroAssembler::encode_heap_oop: heap base corrupted?&quot;);
3781 #endif
3782   verify_oop(s, &quot;broken oop in encode_heap_oop&quot;);
3783   if (CompressedOops::base() == NULL) {
3784     if (CompressedOops::shift() != 0) {
3785       assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
3786       lsr(d, s, LogMinObjAlignmentInBytes);
3787     } else {
3788       mov(d, s);
3789     }
3790   } else {
3791     subs(d, s, rheapbase);
3792     csel(d, d, zr, Assembler::HS);
3793     lsr(d, d, LogMinObjAlignmentInBytes);
3794 
3795     /*  Old algorithm: is this any worse?
3796     Label nonnull;
3797     cbnz(r, nonnull);
3798     sub(r, r, rheapbase);
3799     bind(nonnull);
3800     lsr(r, r, LogMinObjAlignmentInBytes);
3801     */
3802   }
3803 }
3804 
3805 void MacroAssembler::encode_heap_oop_not_null(Register r) {
3806 #ifdef ASSERT
3807   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null: heap base corrupted?&quot;);
3808   if (CheckCompressedOops) {
3809     Label ok;
3810     cbnz(r, ok);
3811     stop(&quot;null oop passed to encode_heap_oop_not_null&quot;);
3812     bind(ok);
3813   }
3814 #endif
3815   verify_oop(r, &quot;broken oop in encode_heap_oop_not_null&quot;);
3816   if (CompressedOops::base() != NULL) {
3817     sub(r, r, rheapbase);
3818   }
3819   if (CompressedOops::shift() != 0) {
3820     assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
3821     lsr(r, r, LogMinObjAlignmentInBytes);
3822   }
3823 }
3824 
3825 void MacroAssembler::encode_heap_oop_not_null(Register dst, Register src) {
3826 #ifdef ASSERT
3827   verify_heapbase(&quot;MacroAssembler::encode_heap_oop_not_null2: heap base corrupted?&quot;);
3828   if (CheckCompressedOops) {
3829     Label ok;
3830     cbnz(src, ok);
3831     stop(&quot;null oop passed to encode_heap_oop_not_null2&quot;);
3832     bind(ok);
3833   }
3834 #endif
3835   verify_oop(src, &quot;broken oop in encode_heap_oop_not_null2&quot;);
3836 
3837   Register data = src;
3838   if (CompressedOops::base() != NULL) {
3839     sub(dst, src, rheapbase);
3840     data = dst;
3841   }
3842   if (CompressedOops::shift() != 0) {
3843     assert (LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
3844     lsr(dst, data, LogMinObjAlignmentInBytes);
3845     data = dst;
3846   }
3847   if (data == src)
3848     mov(dst, src);
3849 }
3850 
3851 void  MacroAssembler::decode_heap_oop(Register d, Register s) {
3852 #ifdef ASSERT
3853   verify_heapbase(&quot;MacroAssembler::decode_heap_oop: heap base corrupted?&quot;);
3854 #endif
3855   if (CompressedOops::base() == NULL) {
3856     if (CompressedOops::shift() != 0 || d != s) {
3857       lsl(d, s, CompressedOops::shift());
3858     }
3859   } else {
3860     Label done;
3861     if (d != s)
3862       mov(d, s);
3863     cbz(s, done);
3864     add(d, rheapbase, s, Assembler::LSL, LogMinObjAlignmentInBytes);
3865     bind(done);
3866   }
3867   verify_oop(d, &quot;broken oop in decode_heap_oop&quot;);
3868 }
3869 
3870 void  MacroAssembler::decode_heap_oop_not_null(Register r) {
3871   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
3872   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
3873   // Cannot assert, unverified entry point counts instructions (see .ad file)
3874   // vtableStubs also counts instructions in pd_code_size_limit.
3875   // Also do not verify_oop as this is called by verify_oop.
3876   if (CompressedOops::shift() != 0) {
3877     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
3878     if (CompressedOops::base() != NULL) {
3879       add(r, rheapbase, r, Assembler::LSL, LogMinObjAlignmentInBytes);
3880     } else {
3881       add(r, zr, r, Assembler::LSL, LogMinObjAlignmentInBytes);
3882     }
3883   } else {
3884     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);
3885   }
3886 }
3887 
3888 void  MacroAssembler::decode_heap_oop_not_null(Register dst, Register src) {
3889   assert (UseCompressedOops, &quot;should only be used for compressed headers&quot;);
3890   assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
3891   // Cannot assert, unverified entry point counts instructions (see .ad file)
3892   // vtableStubs also counts instructions in pd_code_size_limit.
3893   // Also do not verify_oop as this is called by verify_oop.
3894   if (CompressedOops::shift() != 0) {
3895     assert(LogMinObjAlignmentInBytes == CompressedOops::shift(), &quot;decode alg wrong&quot;);
3896     if (CompressedOops::base() != NULL) {
3897       add(dst, rheapbase, src, Assembler::LSL, LogMinObjAlignmentInBytes);
3898     } else {
3899       add(dst, zr, src, Assembler::LSL, LogMinObjAlignmentInBytes);
3900     }
3901   } else {
3902     assert (CompressedOops::base() == NULL, &quot;sanity&quot;);
3903     if (dst != src) {
3904       mov(dst, src);
3905     }
3906   }
3907 }
3908 
3909 MacroAssembler::KlassDecodeMode MacroAssembler::_klass_decode_mode(KlassDecodeNone);
3910 
3911 MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode() {
3912   assert(UseCompressedClassPointers, &quot;not using compressed class pointers&quot;);
3913   assert(Metaspace::initialized(), &quot;metaspace not initialized yet&quot;);
3914 
3915   if (_klass_decode_mode != KlassDecodeNone) {
3916     return _klass_decode_mode;
3917   }
3918 
3919   assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift()
3920          || 0 == CompressedKlassPointers::shift(), &quot;decode alg wrong&quot;);
3921 
3922   if (CompressedKlassPointers::base() == NULL) {
3923     return (_klass_decode_mode = KlassDecodeZero);
3924   }
3925 
3926   if (operand_valid_for_logical_immediate(
3927         /*is32*/false, (uint64_t)CompressedKlassPointers::base())) {
3928     const uint64_t range_mask =
3929       (1UL &lt;&lt; log2_intptr(CompressedKlassPointers::range())) - 1;
3930     if (((uint64_t)CompressedKlassPointers::base() &amp; range_mask) == 0) {
3931       return (_klass_decode_mode = KlassDecodeXor);
3932     }
3933   }
3934 
3935   const uint64_t shifted_base =
3936     (uint64_t)CompressedKlassPointers::base() &gt;&gt; CompressedKlassPointers::shift();
3937   guarantee((shifted_base &amp; 0xffff0000ffffffff) == 0,
3938             &quot;compressed class base bad alignment&quot;);
3939 
3940   return (_klass_decode_mode = KlassDecodeMovk);
3941 }
3942 
3943 void MacroAssembler::encode_klass_not_null(Register dst, Register src) {
3944   switch (klass_decode_mode()) {
3945   case KlassDecodeZero:
3946     if (CompressedKlassPointers::shift() != 0) {
3947       lsr(dst, src, LogKlassAlignmentInBytes);
3948     } else {
3949       if (dst != src) mov(dst, src);
3950     }
3951     break;
3952 
3953   case KlassDecodeXor:
3954     if (CompressedKlassPointers::shift() != 0) {
3955       eor(dst, src, (uint64_t)CompressedKlassPointers::base());
3956       lsr(dst, dst, LogKlassAlignmentInBytes);
3957     } else {
3958       eor(dst, src, (uint64_t)CompressedKlassPointers::base());
3959     }
3960     break;
3961 
3962   case KlassDecodeMovk:
3963     if (CompressedKlassPointers::shift() != 0) {
3964       ubfx(dst, src, LogKlassAlignmentInBytes, 32);
3965     } else {
3966       movw(dst, src);
3967     }
3968     break;
3969 
3970   case KlassDecodeNone:
3971     ShouldNotReachHere();
3972     break;
3973   }
3974 }
3975 
3976 void MacroAssembler::encode_klass_not_null(Register r) {
3977   encode_klass_not_null(r, r);
3978 }
3979 
3980 void  MacroAssembler::decode_klass_not_null(Register dst, Register src) {
3981   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
3982 
3983   switch (klass_decode_mode()) {
3984   case KlassDecodeZero:
3985     if (CompressedKlassPointers::shift() != 0) {
3986       lsl(dst, src, LogKlassAlignmentInBytes);
3987     } else {
3988       if (dst != src) mov(dst, src);
3989     }
3990     break;
3991 
3992   case KlassDecodeXor:
3993     if (CompressedKlassPointers::shift() != 0) {
3994       lsl(dst, src, LogKlassAlignmentInBytes);
3995       eor(dst, dst, (uint64_t)CompressedKlassPointers::base());
3996     } else {
3997       eor(dst, src, (uint64_t)CompressedKlassPointers::base());
3998     }
3999     break;
4000 
4001   case KlassDecodeMovk: {
4002     const uint64_t shifted_base =
4003       (uint64_t)CompressedKlassPointers::base() &gt;&gt; CompressedKlassPointers::shift();
4004 
4005     if (dst != src) movw(dst, src);
4006     movk(dst, shifted_base &gt;&gt; 32, 32);
4007 
4008     if (CompressedKlassPointers::shift() != 0) {
4009       lsl(dst, dst, LogKlassAlignmentInBytes);
4010     }
4011 
4012     break;
4013   }
4014 
4015   case KlassDecodeNone:
4016     ShouldNotReachHere();
4017     break;
4018   }
4019 }
4020 
4021 void  MacroAssembler::decode_klass_not_null(Register r) {
4022   decode_klass_not_null(r, r);
4023 }
4024 
4025 void  MacroAssembler::set_narrow_oop(Register dst, jobject obj) {
4026 #ifdef ASSERT
4027   {
4028     ThreadInVMfromUnknown tiv;
4029     assert (UseCompressedOops, &quot;should only be used for compressed oops&quot;);
4030     assert (Universe::heap() != NULL, &quot;java heap should be initialized&quot;);
4031     assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4032     assert(Universe::heap()-&gt;is_in(JNIHandles::resolve(obj)), &quot;should be real oop&quot;);
4033   }
4034 #endif
4035   int oop_index = oop_recorder()-&gt;find_index(obj);
4036   InstructionMark im(this);
4037   RelocationHolder rspec = oop_Relocation::spec(oop_index);
4038   code_section()-&gt;relocate(inst_mark(), rspec);
4039   movz(dst, 0xDEAD, 16);
4040   movk(dst, 0xBEEF);
4041 }
4042 
4043 void  MacroAssembler::set_narrow_klass(Register dst, Klass* k) {
4044   assert (UseCompressedClassPointers, &quot;should only be used for compressed headers&quot;);
4045   assert (oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4046   int index = oop_recorder()-&gt;find_index(k);
4047   assert(! Universe::heap()-&gt;is_in(k), &quot;should not be an oop&quot;);
4048 
4049   InstructionMark im(this);
4050   RelocationHolder rspec = metadata_Relocation::spec(index);
4051   code_section()-&gt;relocate(inst_mark(), rspec);
4052   narrowKlass nk = CompressedKlassPointers::encode(k);
4053   movz(dst, (nk &gt;&gt; 16), 16);
4054   movk(dst, nk &amp; 0xffff);
4055 }
4056 
4057 void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators,
4058                                     Register dst, Address src,
4059                                     Register tmp1, Register thread_tmp) {
4060   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4061   decorators = AccessInternal::decorator_fixup(decorators);
4062   bool as_raw = (decorators &amp; AS_RAW) != 0;
4063   if (as_raw) {
4064     bs-&gt;BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4065   } else {
4066     bs-&gt;load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4067   }
4068 }
4069 
4070 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators,
4071                                      Address dst, Register src,
4072                                      Register tmp1, Register thread_tmp) {
4073   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4074   decorators = AccessInternal::decorator_fixup(decorators);
4075   bool as_raw = (decorators &amp; AS_RAW) != 0;
4076   if (as_raw) {
4077     bs-&gt;BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4078   } else {
4079     bs-&gt;store_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4080   }
4081 }
4082 
4083 void MacroAssembler::resolve(DecoratorSet decorators, Register obj) {
4084   // Use stronger ACCESS_WRITE|ACCESS_READ by default.
4085   if ((decorators &amp; (ACCESS_READ | ACCESS_WRITE)) == 0) {
4086     decorators |= ACCESS_READ | ACCESS_WRITE;
4087   }
4088   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4089   return bs-&gt;resolve(this, decorators, obj);
4090 }
4091 
4092 void MacroAssembler::load_heap_oop(Register dst, Address src, Register tmp1,
4093                                    Register thread_tmp, DecoratorSet decorators) {
4094   access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);
4095 }
4096 
4097 void MacroAssembler::load_heap_oop_not_null(Register dst, Address src, Register tmp1,
4098                                             Register thread_tmp, DecoratorSet decorators) {
4099   access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);
4100 }
4101 
4102 void MacroAssembler::store_heap_oop(Address dst, Register src, Register tmp1,
4103                                     Register thread_tmp, DecoratorSet decorators) {
4104   access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);
4105 }
4106 
4107 // Used for storing NULLs.
4108 void MacroAssembler::store_heap_oop_null(Address dst) {
4109   access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);
4110 }
4111 
4112 Address MacroAssembler::allocate_metadata_address(Metadata* obj) {
4113   assert(oop_recorder() != NULL, &quot;this assembler needs a Recorder&quot;);
4114   int index = oop_recorder()-&gt;allocate_metadata_index(obj);
4115   RelocationHolder rspec = metadata_Relocation::spec(index);
4116   return Address((address)obj, rspec);
4117 }
4118 
4119 // Move an oop into a register.  immediate is true if we want
4120 // immediate instrcutions, i.e. we are not going to patch this
4121 // instruction while the code is being executed by another thread.  In
4122 // that case we can use move immediates rather than the constant pool.
4123 void MacroAssembler::movoop(Register dst, jobject obj, bool immediate) {
4124   int oop_index;
4125   if (obj == NULL) {
4126     oop_index = oop_recorder()-&gt;allocate_oop_index(obj);
4127   } else {
4128 #ifdef ASSERT
4129     {
4130       ThreadInVMfromUnknown tiv;
4131       assert(Universe::heap()-&gt;is_in(JNIHandles::resolve(obj)), &quot;should be real oop&quot;);
4132     }
4133 #endif
4134     oop_index = oop_recorder()-&gt;find_index(obj);
4135   }
4136   RelocationHolder rspec = oop_Relocation::spec(oop_index);
4137   if (! immediate) {
4138     address dummy = address(uintptr_t(pc()) &amp; -wordSize); // A nearby aligned address
4139     ldr_constant(dst, Address(dummy, rspec));
4140   } else
4141     mov(dst, Address((address)obj, rspec));
4142 }
4143 
4144 // Move a metadata address into a register.
4145 void MacroAssembler::mov_metadata(Register dst, Metadata* obj) {
4146   int oop_index;
4147   if (obj == NULL) {
4148     oop_index = oop_recorder()-&gt;allocate_metadata_index(obj);
4149   } else {
4150     oop_index = oop_recorder()-&gt;find_index(obj);
4151   }
4152   RelocationHolder rspec = metadata_Relocation::spec(oop_index);
4153   mov(dst, Address((address)obj, rspec));
4154 }
4155 
4156 Address MacroAssembler::constant_oop_address(jobject obj) {
4157 #ifdef ASSERT
4158   {
4159     ThreadInVMfromUnknown tiv;
4160     assert(oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
4161     assert(Universe::heap()-&gt;is_in(JNIHandles::resolve(obj)), &quot;not an oop&quot;);
4162   }
4163 #endif
4164   int oop_index = oop_recorder()-&gt;find_index(obj);
4165   return Address((address)obj, oop_Relocation::spec(oop_index));
4166 }
4167 
4168 // Defines obj, preserves var_size_in_bytes, okay for t2 == var_size_in_bytes.
4169 void MacroAssembler::tlab_allocate(Register obj,
4170                                    Register var_size_in_bytes,
4171                                    int con_size_in_bytes,
4172                                    Register t1,
4173                                    Register t2,
4174                                    Label&amp; slow_case) {
4175   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4176   bs-&gt;tlab_allocate(this, obj, var_size_in_bytes, con_size_in_bytes, t1, t2, slow_case);
4177 }
4178 
4179 // Defines obj, preserves var_size_in_bytes
4180 void MacroAssembler::eden_allocate(Register obj,
4181                                    Register var_size_in_bytes,
4182                                    int con_size_in_bytes,
4183                                    Register t1,
4184                                    Label&amp; slow_case) {
4185   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4186   bs-&gt;eden_allocate(this, obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);
4187 }
4188 
4189 // Zero words; len is in bytes
4190 // Destroys all registers except addr
4191 // len must be a nonzero multiple of wordSize
4192 void MacroAssembler::zero_memory(Register addr, Register len, Register t1) {
4193   assert_different_registers(addr, len, t1, rscratch1, rscratch2);
4194 
4195 #ifdef ASSERT
4196   { Label L;
4197     tst(len, BytesPerWord - 1);
4198     br(Assembler::EQ, L);
4199     stop(&quot;len is not a multiple of BytesPerWord&quot;);
4200     bind(L);
4201   }
4202 #endif
4203 
4204 #ifndef PRODUCT
4205   block_comment(&quot;zero memory&quot;);
4206 #endif
4207 
4208   Label loop;
4209   Label entry;
4210 
4211 //  Algorithm:
4212 //
4213 //    scratch1 = cnt &amp; 7;
4214 //    cnt -= scratch1;
4215 //    p += scratch1;
4216 //    switch (scratch1) {
4217 //      do {
4218 //        cnt -= 8;
4219 //          p[-8] = 0;
4220 //        case 7:
4221 //          p[-7] = 0;
4222 //        case 6:
4223 //          p[-6] = 0;
4224 //          // ...
4225 //        case 1:
4226 //          p[-1] = 0;
4227 //        case 0:
4228 //          p += 8;
4229 //      } while (cnt);
4230 //    }
4231 
4232   const int unroll = 8; // Number of str(zr) instructions we&#39;ll unroll
4233 
4234   lsr(len, len, LogBytesPerWord);
4235   andr(rscratch1, len, unroll - 1);  // tmp1 = cnt % unroll
4236   sub(len, len, rscratch1);      // cnt -= unroll
4237   // t1 always points to the end of the region we&#39;re about to zero
4238   add(t1, addr, rscratch1, Assembler::LSL, LogBytesPerWord);
4239   adr(rscratch2, entry);
4240   sub(rscratch2, rscratch2, rscratch1, Assembler::LSL, 2);
4241   br(rscratch2);
4242   bind(loop);
4243   sub(len, len, unroll);
4244   for (int i = -unroll; i &lt; 0; i++)
4245     Assembler::str(zr, Address(t1, i * wordSize));
4246   bind(entry);
4247   add(t1, t1, unroll * wordSize);
4248   cbnz(len, loop);
4249 }
4250 
4251 void MacroAssembler::verify_tlab() {
4252 #ifdef ASSERT
4253   if (UseTLAB &amp;&amp; VerifyOops) {
4254     Label next, ok;
4255 
4256     stp(rscratch2, rscratch1, Address(pre(sp, -16)));
4257 
4258     ldr(rscratch2, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));
4259     ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_start_offset())));
4260     cmp(rscratch2, rscratch1);
4261     br(Assembler::HS, next);
4262     STOP(&quot;assert(top &gt;= start)&quot;);
4263     should_not_reach_here();
4264 
4265     bind(next);
4266     ldr(rscratch2, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));
4267     ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));
4268     cmp(rscratch2, rscratch1);
4269     br(Assembler::HS, ok);
4270     STOP(&quot;assert(top &lt;= end)&quot;);
4271     should_not_reach_here();
4272 
4273     bind(ok);
4274     ldp(rscratch2, rscratch1, Address(post(sp, 16)));
4275   }
4276 #endif
4277 }
4278 
4279 // Writes to stack successive pages until offset reached to check for
4280 // stack overflow + shadow pages.  This clobbers tmp.
4281 void MacroAssembler::bang_stack_size(Register size, Register tmp) {
4282   assert_different_registers(tmp, size, rscratch1);
4283   mov(tmp, sp);
4284   // Bang stack for total size given plus shadow page size.
4285   // Bang one page at a time because large size can bang beyond yellow and
4286   // red zones.
4287   Label loop;
4288   mov(rscratch1, os::vm_page_size());
4289   bind(loop);
4290   lea(tmp, Address(tmp, -os::vm_page_size()));
4291   subsw(size, size, rscratch1);
4292   str(size, Address(tmp));
4293   br(Assembler::GT, loop);
4294 
4295   // Bang down shadow pages too.
4296   // At this point, (tmp-0) is the last address touched, so don&#39;t
4297   // touch it again.  (It was touched as (tmp-pagesize) but then tmp
4298   // was post-decremented.)  Skip this address by starting at i=1, and
4299   // touch a few more pages below.  N.B.  It is important to touch all
4300   // the way down to and including i=StackShadowPages.
4301   for (int i = 0; i &lt; (int)(JavaThread::stack_shadow_zone_size() / os::vm_page_size()) - 1; i++) {
4302     // this could be any sized move but this is can be a debugging crumb
4303     // so the bigger the better.
4304     lea(tmp, Address(tmp, -os::vm_page_size()));
4305     str(size, Address(tmp));
4306   }
4307 }
4308 
4309 
4310 // Move the address of the polling page into dest.
4311 void MacroAssembler::get_polling_page(Register dest, address page, relocInfo::relocType rtype) {
4312   if (SafepointMechanism::uses_thread_local_poll()) {
4313     ldr(dest, Address(rthread, Thread::polling_page_offset()));
4314   } else {
4315     unsigned long off;
4316     adrp(dest, Address(page, rtype), off);
4317     assert(off == 0, &quot;polling page must be page aligned&quot;);
4318   }
4319 }
4320 
4321 // Move the address of the polling page into r, then read the polling
4322 // page.
4323 address MacroAssembler::read_polling_page(Register r, address page, relocInfo::relocType rtype) {
4324   get_polling_page(r, page, rtype);
4325   return read_polling_page(r, rtype);
4326 }
4327 
4328 // Read the polling page.  The address of the polling page must
4329 // already be in r.
4330 address MacroAssembler::read_polling_page(Register r, relocInfo::relocType rtype) {
4331   InstructionMark im(this);
4332   code_section()-&gt;relocate(inst_mark(), rtype);
4333   ldrw(zr, Address(r, 0));
4334   return inst_mark();
4335 }
4336 
4337 void MacroAssembler::adrp(Register reg1, const Address &amp;dest, unsigned long &amp;byte_offset) {
4338   relocInfo::relocType rtype = dest.rspec().reloc()-&gt;type();
4339   unsigned long low_page = (unsigned long)CodeCache::low_bound() &gt;&gt; 12;
4340   unsigned long high_page = (unsigned long)(CodeCache::high_bound()-1) &gt;&gt; 12;
4341   unsigned long dest_page = (unsigned long)dest.target() &gt;&gt; 12;
4342   long offset_low = dest_page - low_page;
4343   long offset_high = dest_page - high_page;
4344 
4345   assert(is_valid_AArch64_address(dest.target()), &quot;bad address&quot;);
4346   assert(dest.getMode() == Address::literal, &quot;ADRP must be applied to a literal address&quot;);
4347 
4348   InstructionMark im(this);
4349   code_section()-&gt;relocate(inst_mark(), dest.rspec());
4350   // 8143067: Ensure that the adrp can reach the dest from anywhere within
4351   // the code cache so that if it is relocated we know it will still reach
4352   if (offset_high &gt;= -(1&lt;&lt;20) &amp;&amp; offset_low &lt; (1&lt;&lt;20)) {
4353     _adrp(reg1, dest.target());
4354   } else {
4355     unsigned long target = (unsigned long)dest.target();
4356     unsigned long adrp_target
4357       = (target &amp; 0xffffffffUL) | ((unsigned long)pc() &amp; 0xffff00000000UL);
4358 
4359     _adrp(reg1, (address)adrp_target);
4360     movk(reg1, target &gt;&gt; 32, 32);
4361   }
4362   byte_offset = (unsigned long)dest.target() &amp; 0xfff;
4363 }
4364 
4365 void MacroAssembler::load_byte_map_base(Register reg) {
4366   CardTable::CardValue* byte_map_base =
4367     ((CardTableBarrierSet*)(BarrierSet::barrier_set()))-&gt;card_table()-&gt;byte_map_base();
4368 
4369   if (is_valid_AArch64_address((address)byte_map_base)) {
4370     // Strictly speaking the byte_map_base isn&#39;t an address at all,
4371     // and it might even be negative.
4372     unsigned long offset;
4373     adrp(reg, ExternalAddress((address)byte_map_base), offset);
4374     // We expect offset to be zero with most collectors.
4375     if (offset != 0) {
4376       add(reg, reg, offset);
4377     }
4378   } else {
4379     mov(reg, (uint64_t)byte_map_base);
4380   }
4381 }
4382 
4383 void MacroAssembler::build_frame(int framesize) {
4384   assert(framesize &gt; 0, &quot;framesize must be &gt; 0&quot;);
4385   if (framesize &lt; ((1 &lt;&lt; 9) + 2 * wordSize)) {
4386     sub(sp, sp, framesize);
4387     stp(rfp, lr, Address(sp, framesize - 2 * wordSize));
4388     if (PreserveFramePointer) add(rfp, sp, framesize - 2 * wordSize);
4389   } else {
4390     stp(rfp, lr, Address(pre(sp, -2 * wordSize)));
4391     if (PreserveFramePointer) mov(rfp, sp);
4392     if (framesize &lt; ((1 &lt;&lt; 12) + 2 * wordSize))
4393       sub(sp, sp, framesize - 2 * wordSize);
4394     else {
4395       mov(rscratch1, framesize - 2 * wordSize);
4396       sub(sp, sp, rscratch1);
4397     }
4398   }
4399 }
4400 
4401 void MacroAssembler::remove_frame(int framesize) {
4402   assert(framesize &gt; 0, &quot;framesize must be &gt; 0&quot;);
4403   if (framesize &lt; ((1 &lt;&lt; 9) + 2 * wordSize)) {
4404     ldp(rfp, lr, Address(sp, framesize - 2 * wordSize));
4405     add(sp, sp, framesize);
4406   } else {
4407     if (framesize &lt; ((1 &lt;&lt; 12) + 2 * wordSize))
4408       add(sp, sp, framesize - 2 * wordSize);
4409     else {
4410       mov(rscratch1, framesize - 2 * wordSize);
4411       add(sp, sp, rscratch1);
4412     }
4413     ldp(rfp, lr, Address(post(sp, 2 * wordSize)));
4414   }
4415 }
4416 
4417 #ifdef COMPILER2
4418 typedef void (MacroAssembler::* chr_insn)(Register Rt, const Address &amp;adr);
4419 
4420 // Search for str1 in str2 and return index or -1
4421 void MacroAssembler::string_indexof(Register str2, Register str1,
4422                                     Register cnt2, Register cnt1,
4423                                     Register tmp1, Register tmp2,
4424                                     Register tmp3, Register tmp4,
4425                                     Register tmp5, Register tmp6,
4426                                     int icnt1, Register result, int ae) {
4427   // NOTE: tmp5, tmp6 can be zr depending on specific method version
4428   Label LINEARSEARCH, LINEARSTUB, LINEAR_MEDIUM, DONE, NOMATCH, MATCH;
4429 
4430   Register ch1 = rscratch1;
4431   Register ch2 = rscratch2;
4432   Register cnt1tmp = tmp1;
4433   Register cnt2tmp = tmp2;
4434   Register cnt1_neg = cnt1;
4435   Register cnt2_neg = cnt2;
4436   Register result_tmp = tmp4;
4437 
4438   bool isL = ae == StrIntrinsicNode::LL;
4439 
4440   bool str1_isL = ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UL;
4441   bool str2_isL = ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::LU;
4442   int str1_chr_shift = str1_isL ? 0:1;
4443   int str2_chr_shift = str2_isL ? 0:1;
4444   int str1_chr_size = str1_isL ? 1:2;
4445   int str2_chr_size = str2_isL ? 1:2;
4446   chr_insn str1_load_1chr = str1_isL ? (chr_insn)&amp;MacroAssembler::ldrb :
4447                                       (chr_insn)&amp;MacroAssembler::ldrh;
4448   chr_insn str2_load_1chr = str2_isL ? (chr_insn)&amp;MacroAssembler::ldrb :
4449                                       (chr_insn)&amp;MacroAssembler::ldrh;
4450   chr_insn load_2chr = isL ? (chr_insn)&amp;MacroAssembler::ldrh : (chr_insn)&amp;MacroAssembler::ldrw;
4451   chr_insn load_4chr = isL ? (chr_insn)&amp;MacroAssembler::ldrw : (chr_insn)&amp;MacroAssembler::ldr;
4452 
4453   // Note, inline_string_indexOf() generates checks:
4454   // if (substr.count &gt; string.count) return -1;
4455   // if (substr.count == 0) return 0;
4456 
4457   // We have two strings, a source string in str2, cnt2 and a pattern string
4458   // in str1, cnt1. Find the 1st occurence of pattern in source or return -1.
4459 
4460   // For larger pattern and source we use a simplified Boyer Moore algorithm.
4461   // With a small pattern and source we use linear scan.
4462 
4463   if (icnt1 == -1) {
4464     sub(result_tmp, cnt2, cnt1);
4465     cmp(cnt1, (u1)8);             // Use Linear Scan if cnt1 &lt; 8 || cnt1 &gt;= 256
4466     br(LT, LINEARSEARCH);
4467     dup(v0, T16B, cnt1); // done in separate FPU pipeline. Almost no penalty
4468     subs(zr, cnt1, 256);
4469     lsr(tmp1, cnt2, 2);
4470     ccmp(cnt1, tmp1, 0b0000, LT); // Source must be 4 * pattern for BM
4471     br(GE, LINEARSTUB);
4472   }
4473 
4474 // The Boyer Moore alogorithm is based on the description here:-
4475 //
4476 // http://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm
4477 //
4478 // This describes and algorithm with 2 shift rules. The &#39;Bad Character&#39; rule
4479 // and the &#39;Good Suffix&#39; rule.
4480 //
4481 // These rules are essentially heuristics for how far we can shift the
4482 // pattern along the search string.
4483 //
4484 // The implementation here uses the &#39;Bad Character&#39; rule only because of the
4485 // complexity of initialisation for the &#39;Good Suffix&#39; rule.
4486 //
4487 // This is also known as the Boyer-Moore-Horspool algorithm:-
4488 //
4489 // http://en.wikipedia.org/wiki/Boyer-Moore-Horspool_algorithm
4490 //
4491 // This particular implementation has few java-specific optimizations.
4492 //
4493 // #define ASIZE 256
4494 //
4495 //    int bm(unsigned char *x, int m, unsigned char *y, int n) {
4496 //       int i, j;
4497 //       unsigned c;
4498 //       unsigned char bc[ASIZE];
4499 //
4500 //       /* Preprocessing */
4501 //       for (i = 0; i &lt; ASIZE; ++i)
4502 //          bc[i] = m;
4503 //       for (i = 0; i &lt; m - 1; ) {
4504 //          c = x[i];
4505 //          ++i;
4506 //          // c &lt; 256 for Latin1 string, so, no need for branch
4507 //          #ifdef PATTERN_STRING_IS_LATIN1
4508 //          bc[c] = m - i;
4509 //          #else
4510 //          if (c &lt; ASIZE) bc[c] = m - i;
4511 //          #endif
4512 //       }
4513 //
4514 //       /* Searching */
4515 //       j = 0;
4516 //       while (j &lt;= n - m) {
4517 //          c = y[i+j];
4518 //          if (x[m-1] == c)
4519 //            for (i = m - 2; i &gt;= 0 &amp;&amp; x[i] == y[i + j]; --i);
4520 //          if (i &lt; 0) return j;
4521 //          // c &lt; 256 for Latin1 string, so, no need for branch
4522 //          #ifdef SOURCE_STRING_IS_LATIN1
4523 //          // LL case: (c&lt; 256) always true. Remove branch
4524 //          j += bc[y[j+m-1]];
4525 //          #endif
4526 //          #ifndef PATTERN_STRING_IS_UTF
4527 //          // UU case: need if (c&lt;ASIZE) check. Skip 1 character if not.
4528 //          if (c &lt; ASIZE)
4529 //            j += bc[y[j+m-1]];
4530 //          else
4531 //            j += 1
4532 //          #endif
4533 //          #ifdef PATTERN_IS_LATIN1_AND_SOURCE_IS_UTF
4534 //          // UL case: need if (c&lt;ASIZE) check. Skip &lt;pattern length&gt; if not.
4535 //          if (c &lt; ASIZE)
4536 //            j += bc[y[j+m-1]];
4537 //          else
4538 //            j += m
4539 //          #endif
4540 //       }
4541 //    }
4542 
4543   if (icnt1 == -1) {
4544     Label BCLOOP, BCSKIP, BMLOOPSTR2, BMLOOPSTR1, BMSKIP, BMADV, BMMATCH,
4545         BMLOOPSTR1_LASTCMP, BMLOOPSTR1_CMP, BMLOOPSTR1_AFTER_LOAD, BM_INIT_LOOP;
4546     Register cnt1end = tmp2;
4547     Register str2end = cnt2;
4548     Register skipch = tmp2;
4549 
4550     // str1 length is &gt;=8, so, we can read at least 1 register for cases when
4551     // UTF-&gt;Latin1 conversion is not needed(8 LL or 4UU) and half register for
4552     // UL case. We&#39;ll re-read last character in inner pre-loop code to have
4553     // single outer pre-loop load
4554     const int firstStep = isL ? 7 : 3;
4555 
4556     const int ASIZE = 256;
4557     const int STORED_BYTES = 32; // amount of bytes stored per instruction
4558     sub(sp, sp, ASIZE);
4559     mov(tmp5, ASIZE/STORED_BYTES); // loop iterations
4560     mov(ch1, sp);
4561     BIND(BM_INIT_LOOP);
4562       stpq(v0, v0, Address(post(ch1, STORED_BYTES)));
4563       subs(tmp5, tmp5, 1);
4564       br(GT, BM_INIT_LOOP);
4565 
4566       sub(cnt1tmp, cnt1, 1);
4567       mov(tmp5, str2);
4568       add(str2end, str2, result_tmp, LSL, str2_chr_shift);
4569       sub(ch2, cnt1, 1);
4570       mov(tmp3, str1);
4571     BIND(BCLOOP);
4572       (this-&gt;*str1_load_1chr)(ch1, Address(post(tmp3, str1_chr_size)));
4573       if (!str1_isL) {
4574         subs(zr, ch1, ASIZE);
4575         br(HS, BCSKIP);
4576       }
4577       strb(ch2, Address(sp, ch1));
4578     BIND(BCSKIP);
4579       subs(ch2, ch2, 1);
4580       br(GT, BCLOOP);
4581 
4582       add(tmp6, str1, cnt1, LSL, str1_chr_shift); // address after str1
4583       if (str1_isL == str2_isL) {
4584         // load last 8 bytes (8LL/4UU symbols)
4585         ldr(tmp6, Address(tmp6, -wordSize));
4586       } else {
4587         ldrw(tmp6, Address(tmp6, -wordSize/2)); // load last 4 bytes(4 symbols)
4588         // convert Latin1 to UTF. We&#39;ll have to wait until load completed, but
4589         // it&#39;s still faster than per-character loads+checks
4590         lsr(tmp3, tmp6, BitsPerByte * (wordSize/2 - str1_chr_size)); // str1[N-1]
4591         ubfx(ch1, tmp6, 8, 8); // str1[N-2]
4592         ubfx(ch2, tmp6, 16, 8); // str1[N-3]
4593         andr(tmp6, tmp6, 0xFF); // str1[N-4]
4594         orr(ch2, ch1, ch2, LSL, 16);
4595         orr(tmp6, tmp6, tmp3, LSL, 48);
4596         orr(tmp6, tmp6, ch2, LSL, 16);
4597       }
4598     BIND(BMLOOPSTR2);
4599       (this-&gt;*str2_load_1chr)(skipch, Address(str2, cnt1tmp, Address::lsl(str2_chr_shift)));
4600       sub(cnt1tmp, cnt1tmp, firstStep); // cnt1tmp is positive here, because cnt1 &gt;= 8
4601       if (str1_isL == str2_isL) {
4602         // re-init tmp3. It&#39;s for free because it&#39;s executed in parallel with
4603         // load above. Alternative is to initialize it before loop, but it&#39;ll
4604         // affect performance on in-order systems with 2 or more ld/st pipelines
4605         lsr(tmp3, tmp6, BitsPerByte * (wordSize - str1_chr_size));
4606       }
4607       if (!isL) { // UU/UL case
4608         lsl(ch2, cnt1tmp, 1); // offset in bytes
4609       }
4610       cmp(tmp3, skipch);
4611       br(NE, BMSKIP);
4612       ldr(ch2, Address(str2, isL ? cnt1tmp : ch2));
4613       mov(ch1, tmp6);
4614       if (isL) {
4615         b(BMLOOPSTR1_AFTER_LOAD);
4616       } else {
4617         sub(cnt1tmp, cnt1tmp, 1); // no need to branch for UU/UL case. cnt1 &gt;= 8
4618         b(BMLOOPSTR1_CMP);
4619       }
4620     BIND(BMLOOPSTR1);
4621       (this-&gt;*str1_load_1chr)(ch1, Address(str1, cnt1tmp, Address::lsl(str1_chr_shift)));
4622       (this-&gt;*str2_load_1chr)(ch2, Address(str2, cnt1tmp, Address::lsl(str2_chr_shift)));
4623     BIND(BMLOOPSTR1_AFTER_LOAD);
4624       subs(cnt1tmp, cnt1tmp, 1);
4625       br(LT, BMLOOPSTR1_LASTCMP);
4626     BIND(BMLOOPSTR1_CMP);
4627       cmp(ch1, ch2);
4628       br(EQ, BMLOOPSTR1);
4629     BIND(BMSKIP);
4630       if (!isL) {
4631         // if we&#39;ve met UTF symbol while searching Latin1 pattern, then we can
4632         // skip cnt1 symbols
4633         if (str1_isL != str2_isL) {
4634           mov(result_tmp, cnt1);
4635         } else {
4636           mov(result_tmp, 1);
4637         }
4638         subs(zr, skipch, ASIZE);
4639         br(HS, BMADV);
4640       }
4641       ldrb(result_tmp, Address(sp, skipch)); // load skip distance
4642     BIND(BMADV);
4643       sub(cnt1tmp, cnt1, 1);
4644       add(str2, str2, result_tmp, LSL, str2_chr_shift);
4645       cmp(str2, str2end);
4646       br(LE, BMLOOPSTR2);
4647       add(sp, sp, ASIZE);
4648       b(NOMATCH);
4649     BIND(BMLOOPSTR1_LASTCMP);
4650       cmp(ch1, ch2);
4651       br(NE, BMSKIP);
4652     BIND(BMMATCH);
4653       sub(result, str2, tmp5);
4654       if (!str2_isL) lsr(result, result, 1);
4655       add(sp, sp, ASIZE);
4656       b(DONE);
4657 
4658     BIND(LINEARSTUB);
4659     cmp(cnt1, (u1)16); // small patterns still should be handled by simple algorithm
4660     br(LT, LINEAR_MEDIUM);
4661     mov(result, zr);
4662     RuntimeAddress stub = NULL;
4663     if (isL) {
4664       stub = RuntimeAddress(StubRoutines::aarch64::string_indexof_linear_ll());
4665       assert(stub.target() != NULL, &quot;string_indexof_linear_ll stub has not been generated&quot;);
4666     } else if (str1_isL) {
4667       stub = RuntimeAddress(StubRoutines::aarch64::string_indexof_linear_ul());
4668        assert(stub.target() != NULL, &quot;string_indexof_linear_ul stub has not been generated&quot;);
4669     } else {
4670       stub = RuntimeAddress(StubRoutines::aarch64::string_indexof_linear_uu());
4671       assert(stub.target() != NULL, &quot;string_indexof_linear_uu stub has not been generated&quot;);
4672     }
4673     trampoline_call(stub);
4674     b(DONE);
4675   }
4676 
4677   BIND(LINEARSEARCH);
4678   {
4679     Label DO1, DO2, DO3;
4680 
4681     Register str2tmp = tmp2;
4682     Register first = tmp3;
4683 
4684     if (icnt1 == -1)
4685     {
4686         Label DOSHORT, FIRST_LOOP, STR2_NEXT, STR1_LOOP, STR1_NEXT;
4687 
4688         cmp(cnt1, u1(str1_isL == str2_isL ? 4 : 2));
4689         br(LT, DOSHORT);
4690       BIND(LINEAR_MEDIUM);
4691         (this-&gt;*str1_load_1chr)(first, Address(str1));
4692         lea(str1, Address(str1, cnt1, Address::lsl(str1_chr_shift)));
4693         sub(cnt1_neg, zr, cnt1, LSL, str1_chr_shift);
4694         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
4695         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
4696 
4697       BIND(FIRST_LOOP);
4698         (this-&gt;*str2_load_1chr)(ch2, Address(str2, cnt2_neg));
4699         cmp(first, ch2);
4700         br(EQ, STR1_LOOP);
4701       BIND(STR2_NEXT);
4702         adds(cnt2_neg, cnt2_neg, str2_chr_size);
4703         br(LE, FIRST_LOOP);
4704         b(NOMATCH);
4705 
4706       BIND(STR1_LOOP);
4707         adds(cnt1tmp, cnt1_neg, str1_chr_size);
4708         add(cnt2tmp, cnt2_neg, str2_chr_size);
4709         br(GE, MATCH);
4710 
4711       BIND(STR1_NEXT);
4712         (this-&gt;*str1_load_1chr)(ch1, Address(str1, cnt1tmp));
4713         (this-&gt;*str2_load_1chr)(ch2, Address(str2, cnt2tmp));
4714         cmp(ch1, ch2);
4715         br(NE, STR2_NEXT);
4716         adds(cnt1tmp, cnt1tmp, str1_chr_size);
4717         add(cnt2tmp, cnt2tmp, str2_chr_size);
4718         br(LT, STR1_NEXT);
4719         b(MATCH);
4720 
4721       BIND(DOSHORT);
4722       if (str1_isL == str2_isL) {
4723         cmp(cnt1, (u1)2);
4724         br(LT, DO1);
4725         br(GT, DO3);
4726       }
4727     }
4728 
4729     if (icnt1 == 4) {
4730       Label CH1_LOOP;
4731 
4732         (this-&gt;*load_4chr)(ch1, str1);
4733         sub(result_tmp, cnt2, 4);
4734         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
4735         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
4736 
4737       BIND(CH1_LOOP);
4738         (this-&gt;*load_4chr)(ch2, Address(str2, cnt2_neg));
4739         cmp(ch1, ch2);
4740         br(EQ, MATCH);
4741         adds(cnt2_neg, cnt2_neg, str2_chr_size);
4742         br(LE, CH1_LOOP);
4743         b(NOMATCH);
4744       }
4745 
4746     if ((icnt1 == -1 &amp;&amp; str1_isL == str2_isL) || icnt1 == 2) {
4747       Label CH1_LOOP;
4748 
4749       BIND(DO2);
4750         (this-&gt;*load_2chr)(ch1, str1);
4751         if (icnt1 == 2) {
4752           sub(result_tmp, cnt2, 2);
4753         }
4754         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
4755         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
4756       BIND(CH1_LOOP);
4757         (this-&gt;*load_2chr)(ch2, Address(str2, cnt2_neg));
4758         cmp(ch1, ch2);
4759         br(EQ, MATCH);
4760         adds(cnt2_neg, cnt2_neg, str2_chr_size);
4761         br(LE, CH1_LOOP);
4762         b(NOMATCH);
4763     }
4764 
4765     if ((icnt1 == -1 &amp;&amp; str1_isL == str2_isL) || icnt1 == 3) {
4766       Label FIRST_LOOP, STR2_NEXT, STR1_LOOP;
4767 
4768       BIND(DO3);
4769         (this-&gt;*load_2chr)(first, str1);
4770         (this-&gt;*str1_load_1chr)(ch1, Address(str1, 2*str1_chr_size));
4771         if (icnt1 == 3) {
4772           sub(result_tmp, cnt2, 3);
4773         }
4774         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
4775         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
4776       BIND(FIRST_LOOP);
4777         (this-&gt;*load_2chr)(ch2, Address(str2, cnt2_neg));
4778         cmpw(first, ch2);
4779         br(EQ, STR1_LOOP);
4780       BIND(STR2_NEXT);
4781         adds(cnt2_neg, cnt2_neg, str2_chr_size);
4782         br(LE, FIRST_LOOP);
4783         b(NOMATCH);
4784 
4785       BIND(STR1_LOOP);
4786         add(cnt2tmp, cnt2_neg, 2*str2_chr_size);
4787         (this-&gt;*str2_load_1chr)(ch2, Address(str2, cnt2tmp));
4788         cmp(ch1, ch2);
4789         br(NE, STR2_NEXT);
4790         b(MATCH);
4791     }
4792 
4793     if (icnt1 == -1 || icnt1 == 1) {
4794       Label CH1_LOOP, HAS_ZERO, DO1_SHORT, DO1_LOOP;
4795 
4796       BIND(DO1);
4797         (this-&gt;*str1_load_1chr)(ch1, str1);
4798         cmp(cnt2, (u1)8);
4799         br(LT, DO1_SHORT);
4800 
4801         sub(result_tmp, cnt2, 8/str2_chr_size);
4802         sub(cnt2_neg, zr, result_tmp, LSL, str2_chr_shift);
4803         mov(tmp3, str2_isL ? 0x0101010101010101 : 0x0001000100010001);
4804         lea(str2, Address(str2, result_tmp, Address::lsl(str2_chr_shift)));
4805 
4806         if (str2_isL) {
4807           orr(ch1, ch1, ch1, LSL, 8);
4808         }
4809         orr(ch1, ch1, ch1, LSL, 16);
4810         orr(ch1, ch1, ch1, LSL, 32);
4811       BIND(CH1_LOOP);
4812         ldr(ch2, Address(str2, cnt2_neg));
4813         eor(ch2, ch1, ch2);
4814         sub(tmp1, ch2, tmp3);
4815         orr(tmp2, ch2, str2_isL ? 0x7f7f7f7f7f7f7f7f : 0x7fff7fff7fff7fff);
4816         bics(tmp1, tmp1, tmp2);
4817         br(NE, HAS_ZERO);
4818         adds(cnt2_neg, cnt2_neg, 8);
4819         br(LT, CH1_LOOP);
4820 
4821         cmp(cnt2_neg, (u1)8);
4822         mov(cnt2_neg, 0);
4823         br(LT, CH1_LOOP);
4824         b(NOMATCH);
4825 
4826       BIND(HAS_ZERO);
4827         rev(tmp1, tmp1);
4828         clz(tmp1, tmp1);
4829         add(cnt2_neg, cnt2_neg, tmp1, LSR, 3);
4830         b(MATCH);
4831 
4832       BIND(DO1_SHORT);
4833         mov(result_tmp, cnt2);
4834         lea(str2, Address(str2, cnt2, Address::lsl(str2_chr_shift)));
4835         sub(cnt2_neg, zr, cnt2, LSL, str2_chr_shift);
4836       BIND(DO1_LOOP);
4837         (this-&gt;*str2_load_1chr)(ch2, Address(str2, cnt2_neg));
4838         cmpw(ch1, ch2);
4839         br(EQ, MATCH);
4840         adds(cnt2_neg, cnt2_neg, str2_chr_size);
4841         br(LT, DO1_LOOP);
4842     }
4843   }
4844   BIND(NOMATCH);
4845     mov(result, -1);
4846     b(DONE);
4847   BIND(MATCH);
4848     add(result, result_tmp, cnt2_neg, ASR, str2_chr_shift);
4849   BIND(DONE);
4850 }
4851 
4852 typedef void (MacroAssembler::* chr_insn)(Register Rt, const Address &amp;adr);
4853 typedef void (MacroAssembler::* uxt_insn)(Register Rd, Register Rn);
4854 
4855 void MacroAssembler::string_indexof_char(Register str1, Register cnt1,
4856                                          Register ch, Register result,
4857                                          Register tmp1, Register tmp2, Register tmp3)
4858 {
4859   Label CH1_LOOP, HAS_ZERO, DO1_SHORT, DO1_LOOP, MATCH, NOMATCH, DONE;
4860   Register cnt1_neg = cnt1;
4861   Register ch1 = rscratch1;
4862   Register result_tmp = rscratch2;
4863 
<a name="4" id="anc4"></a><span class="line-added">4864   cbz(cnt1, NOMATCH);</span>
<span class="line-added">4865 </span>
4866   cmp(cnt1, (u1)4);
4867   br(LT, DO1_SHORT);
4868 
4869   orr(ch, ch, ch, LSL, 16);
4870   orr(ch, ch, ch, LSL, 32);
4871 
4872   sub(cnt1, cnt1, 4);
4873   mov(result_tmp, cnt1);
4874   lea(str1, Address(str1, cnt1, Address::uxtw(1)));
4875   sub(cnt1_neg, zr, cnt1, LSL, 1);
4876 
4877   mov(tmp3, 0x0001000100010001);
4878 
4879   BIND(CH1_LOOP);
4880     ldr(ch1, Address(str1, cnt1_neg));
4881     eor(ch1, ch, ch1);
4882     sub(tmp1, ch1, tmp3);
4883     orr(tmp2, ch1, 0x7fff7fff7fff7fff);
4884     bics(tmp1, tmp1, tmp2);
4885     br(NE, HAS_ZERO);
4886     adds(cnt1_neg, cnt1_neg, 8);
4887     br(LT, CH1_LOOP);
4888 
4889     cmp(cnt1_neg, (u1)8);
4890     mov(cnt1_neg, 0);
4891     br(LT, CH1_LOOP);
4892     b(NOMATCH);
4893 
4894   BIND(HAS_ZERO);
4895     rev(tmp1, tmp1);
4896     clz(tmp1, tmp1);
4897     add(cnt1_neg, cnt1_neg, tmp1, LSR, 3);
4898     b(MATCH);
4899 
4900   BIND(DO1_SHORT);
4901     mov(result_tmp, cnt1);
4902     lea(str1, Address(str1, cnt1, Address::uxtw(1)));
4903     sub(cnt1_neg, zr, cnt1, LSL, 1);
4904   BIND(DO1_LOOP);
4905     ldrh(ch1, Address(str1, cnt1_neg));
4906     cmpw(ch, ch1);
4907     br(EQ, MATCH);
4908     adds(cnt1_neg, cnt1_neg, 2);
4909     br(LT, DO1_LOOP);
4910   BIND(NOMATCH);
4911     mov(result, -1);
4912     b(DONE);
4913   BIND(MATCH);
4914     add(result, result_tmp, cnt1_neg, ASR, 1);
4915   BIND(DONE);
4916 }
4917 
4918 // Compare strings.
4919 void MacroAssembler::string_compare(Register str1, Register str2,
4920     Register cnt1, Register cnt2, Register result, Register tmp1, Register tmp2,
4921     FloatRegister vtmp1, FloatRegister vtmp2, FloatRegister vtmp3, int ae) {
4922   Label DONE, SHORT_LOOP, SHORT_STRING, SHORT_LAST, TAIL, STUB,
4923       DIFFERENCE, NEXT_WORD, SHORT_LOOP_TAIL, SHORT_LAST2, SHORT_LAST_INIT,
4924       SHORT_LOOP_START, TAIL_CHECK;
4925 
4926   bool isLL = ae == StrIntrinsicNode::LL;
4927   bool isLU = ae == StrIntrinsicNode::LU;
4928   bool isUL = ae == StrIntrinsicNode::UL;
4929 
4930   // The stub threshold for LL strings is: 72 (64 + 8) chars
4931   // UU: 36 chars, or 72 bytes (valid for the 64-byte large loop with prefetch)
4932   // LU/UL: 24 chars, or 48 bytes (valid for the 16-character loop at least)
4933   const u1 stub_threshold = isLL ? 72 : ((isLU || isUL) ? 24 : 36);
4934 
4935   bool str1_isL = isLL || isLU;
4936   bool str2_isL = isLL || isUL;
4937 
4938   int str1_chr_shift = str1_isL ? 0 : 1;
4939   int str2_chr_shift = str2_isL ? 0 : 1;
4940   int str1_chr_size = str1_isL ? 1 : 2;
4941   int str2_chr_size = str2_isL ? 1 : 2;
4942   int minCharsInWord = isLL ? wordSize : wordSize/2;
4943 
4944   FloatRegister vtmpZ = vtmp1, vtmp = vtmp2;
4945   chr_insn str1_load_chr = str1_isL ? (chr_insn)&amp;MacroAssembler::ldrb :
4946                                       (chr_insn)&amp;MacroAssembler::ldrh;
4947   chr_insn str2_load_chr = str2_isL ? (chr_insn)&amp;MacroAssembler::ldrb :
4948                                       (chr_insn)&amp;MacroAssembler::ldrh;
4949   uxt_insn ext_chr = isLL ? (uxt_insn)&amp;MacroAssembler::uxtbw :
4950                             (uxt_insn)&amp;MacroAssembler::uxthw;
4951 
4952   BLOCK_COMMENT(&quot;string_compare {&quot;);
4953 
4954   // Bizzarely, the counts are passed in bytes, regardless of whether they
4955   // are L or U strings, however the result is always in characters.
4956   if (!str1_isL) asrw(cnt1, cnt1, 1);
4957   if (!str2_isL) asrw(cnt2, cnt2, 1);
4958 
4959   // Compute the minimum of the string lengths and save the difference.
4960   subsw(result, cnt1, cnt2);
4961   cselw(cnt2, cnt1, cnt2, Assembler::LE); // min
4962 
4963   // A very short string
4964   cmpw(cnt2, minCharsInWord);
4965   br(Assembler::LE, SHORT_STRING);
4966 
4967   // Compare longwords
4968   // load first parts of strings and finish initialization while loading
4969   {
4970     if (str1_isL == str2_isL) { // LL or UU
4971       ldr(tmp1, Address(str1));
4972       cmp(str1, str2);
4973       br(Assembler::EQ, DONE);
4974       ldr(tmp2, Address(str2));
4975       cmp(cnt2, stub_threshold);
4976       br(GE, STUB);
4977       subsw(cnt2, cnt2, minCharsInWord);
4978       br(EQ, TAIL_CHECK);
4979       lea(str2, Address(str2, cnt2, Address::uxtw(str2_chr_shift)));
4980       lea(str1, Address(str1, cnt2, Address::uxtw(str1_chr_shift)));
4981       sub(cnt2, zr, cnt2, LSL, str2_chr_shift);
4982     } else if (isLU) {
4983       ldrs(vtmp, Address(str1));
4984       ldr(tmp2, Address(str2));
4985       cmp(cnt2, stub_threshold);
4986       br(GE, STUB);
4987       subw(cnt2, cnt2, 4);
4988       eor(vtmpZ, T16B, vtmpZ, vtmpZ);
4989       lea(str1, Address(str1, cnt2, Address::uxtw(str1_chr_shift)));
4990       lea(str2, Address(str2, cnt2, Address::uxtw(str2_chr_shift)));
4991       zip1(vtmp, T8B, vtmp, vtmpZ);
4992       sub(cnt1, zr, cnt2, LSL, str1_chr_shift);
4993       sub(cnt2, zr, cnt2, LSL, str2_chr_shift);
4994       add(cnt1, cnt1, 4);
4995       fmovd(tmp1, vtmp);
4996     } else { // UL case
4997       ldr(tmp1, Address(str1));
4998       ldrs(vtmp, Address(str2));
4999       cmp(cnt2, stub_threshold);
5000       br(GE, STUB);
5001       subw(cnt2, cnt2, 4);
5002       lea(str1, Address(str1, cnt2, Address::uxtw(str1_chr_shift)));
5003       eor(vtmpZ, T16B, vtmpZ, vtmpZ);
5004       lea(str2, Address(str2, cnt2, Address::uxtw(str2_chr_shift)));
5005       sub(cnt1, zr, cnt2, LSL, str1_chr_shift);
5006       zip1(vtmp, T8B, vtmp, vtmpZ);
5007       sub(cnt2, zr, cnt2, LSL, str2_chr_shift);
5008       add(cnt1, cnt1, 8);
5009       fmovd(tmp2, vtmp);
5010     }
5011     adds(cnt2, cnt2, isUL ? 4 : 8);
5012     br(GE, TAIL);
5013     eor(rscratch2, tmp1, tmp2);
5014     cbnz(rscratch2, DIFFERENCE);
5015     // main loop
5016     bind(NEXT_WORD);
5017     if (str1_isL == str2_isL) {
5018       ldr(tmp1, Address(str1, cnt2));
5019       ldr(tmp2, Address(str2, cnt2));
5020       adds(cnt2, cnt2, 8);
5021     } else if (isLU) {
5022       ldrs(vtmp, Address(str1, cnt1));
5023       ldr(tmp2, Address(str2, cnt2));
5024       add(cnt1, cnt1, 4);
5025       zip1(vtmp, T8B, vtmp, vtmpZ);
5026       fmovd(tmp1, vtmp);
5027       adds(cnt2, cnt2, 8);
5028     } else { // UL
5029       ldrs(vtmp, Address(str2, cnt2));
5030       ldr(tmp1, Address(str1, cnt1));
5031       zip1(vtmp, T8B, vtmp, vtmpZ);
5032       add(cnt1, cnt1, 8);
5033       fmovd(tmp2, vtmp);
5034       adds(cnt2, cnt2, 4);
5035     }
5036     br(GE, TAIL);
5037 
5038     eor(rscratch2, tmp1, tmp2);
5039     cbz(rscratch2, NEXT_WORD);
5040     b(DIFFERENCE);
5041     bind(TAIL);
5042     eor(rscratch2, tmp1, tmp2);
5043     cbnz(rscratch2, DIFFERENCE);
5044     // Last longword.  In the case where length == 4 we compare the
5045     // same longword twice, but that&#39;s still faster than another
5046     // conditional branch.
5047     if (str1_isL == str2_isL) {
5048       ldr(tmp1, Address(str1));
5049       ldr(tmp2, Address(str2));
5050     } else if (isLU) {
5051       ldrs(vtmp, Address(str1));
5052       ldr(tmp2, Address(str2));
5053       zip1(vtmp, T8B, vtmp, vtmpZ);
5054       fmovd(tmp1, vtmp);
5055     } else { // UL
5056       ldrs(vtmp, Address(str2));
5057       ldr(tmp1, Address(str1));
5058       zip1(vtmp, T8B, vtmp, vtmpZ);
5059       fmovd(tmp2, vtmp);
5060     }
5061     bind(TAIL_CHECK);
5062     eor(rscratch2, tmp1, tmp2);
5063     cbz(rscratch2, DONE);
5064 
5065     // Find the first different characters in the longwords and
5066     // compute their difference.
5067     bind(DIFFERENCE);
5068     rev(rscratch2, rscratch2);
5069     clz(rscratch2, rscratch2);
5070     andr(rscratch2, rscratch2, isLL ? -8 : -16);
5071     lsrv(tmp1, tmp1, rscratch2);
5072     (this-&gt;*ext_chr)(tmp1, tmp1);
5073     lsrv(tmp2, tmp2, rscratch2);
5074     (this-&gt;*ext_chr)(tmp2, tmp2);
5075     subw(result, tmp1, tmp2);
5076     b(DONE);
5077   }
5078 
5079   bind(STUB);
5080     RuntimeAddress stub = NULL;
5081     switch(ae) {
5082       case StrIntrinsicNode::LL:
5083         stub = RuntimeAddress(StubRoutines::aarch64::compare_long_string_LL());
5084         break;
5085       case StrIntrinsicNode::UU:
5086         stub = RuntimeAddress(StubRoutines::aarch64::compare_long_string_UU());
5087         break;
5088       case StrIntrinsicNode::LU:
5089         stub = RuntimeAddress(StubRoutines::aarch64::compare_long_string_LU());
5090         break;
5091       case StrIntrinsicNode::UL:
5092         stub = RuntimeAddress(StubRoutines::aarch64::compare_long_string_UL());
5093         break;
5094       default:
5095         ShouldNotReachHere();
5096      }
5097     assert(stub.target() != NULL, &quot;compare_long_string stub has not been generated&quot;);
5098     trampoline_call(stub);
5099     b(DONE);
5100 
5101   bind(SHORT_STRING);
5102   // Is the minimum length zero?
5103   cbz(cnt2, DONE);
5104   // arrange code to do most branches while loading and loading next characters
5105   // while comparing previous
5106   (this-&gt;*str1_load_chr)(tmp1, Address(post(str1, str1_chr_size)));
5107   subs(cnt2, cnt2, 1);
5108   br(EQ, SHORT_LAST_INIT);
5109   (this-&gt;*str2_load_chr)(cnt1, Address(post(str2, str2_chr_size)));
5110   b(SHORT_LOOP_START);
5111   bind(SHORT_LOOP);
5112   subs(cnt2, cnt2, 1);
5113   br(EQ, SHORT_LAST);
5114   bind(SHORT_LOOP_START);
5115   (this-&gt;*str1_load_chr)(tmp2, Address(post(str1, str1_chr_size)));
5116   (this-&gt;*str2_load_chr)(rscratch1, Address(post(str2, str2_chr_size)));
5117   cmp(tmp1, cnt1);
5118   br(NE, SHORT_LOOP_TAIL);
5119   subs(cnt2, cnt2, 1);
5120   br(EQ, SHORT_LAST2);
5121   (this-&gt;*str1_load_chr)(tmp1, Address(post(str1, str1_chr_size)));
5122   (this-&gt;*str2_load_chr)(cnt1, Address(post(str2, str2_chr_size)));
5123   cmp(tmp2, rscratch1);
5124   br(EQ, SHORT_LOOP);
5125   sub(result, tmp2, rscratch1);
5126   b(DONE);
5127   bind(SHORT_LOOP_TAIL);
5128   sub(result, tmp1, cnt1);
5129   b(DONE);
5130   bind(SHORT_LAST2);
5131   cmp(tmp2, rscratch1);
5132   br(EQ, DONE);
5133   sub(result, tmp2, rscratch1);
5134 
5135   b(DONE);
5136   bind(SHORT_LAST_INIT);
5137   (this-&gt;*str2_load_chr)(cnt1, Address(post(str2, str2_chr_size)));
5138   bind(SHORT_LAST);
5139   cmp(tmp1, cnt1);
5140   br(EQ, DONE);
5141   sub(result, tmp1, cnt1);
5142 
5143   bind(DONE);
5144 
5145   BLOCK_COMMENT(&quot;} string_compare&quot;);
5146 }
5147 #endif // COMPILER2
5148 
5149 // This method checks if provided byte array contains byte with highest bit set.
5150 void MacroAssembler::has_negatives(Register ary1, Register len, Register result) {
5151     // Simple and most common case of aligned small array which is not at the
5152     // end of memory page is placed here. All other cases are in stub.
5153     Label LOOP, END, STUB, STUB_LONG, SET_RESULT, DONE;
5154     const uint64_t UPPER_BIT_MASK=0x8080808080808080;
5155     assert_different_registers(ary1, len, result);
5156 
5157     cmpw(len, 0);
5158     br(LE, SET_RESULT);
5159     cmpw(len, 4 * wordSize);
5160     br(GE, STUB_LONG); // size &gt; 32 then go to stub
5161 
5162     int shift = 64 - exact_log2(os::vm_page_size());
5163     lsl(rscratch1, ary1, shift);
5164     mov(rscratch2, (size_t)(4 * wordSize) &lt;&lt; shift);
5165     adds(rscratch2, rscratch1, rscratch2);  // At end of page?
5166     br(CS, STUB); // at the end of page then go to stub
5167     subs(len, len, wordSize);
5168     br(LT, END);
5169 
5170   BIND(LOOP);
5171     ldr(rscratch1, Address(post(ary1, wordSize)));
5172     tst(rscratch1, UPPER_BIT_MASK);
5173     br(NE, SET_RESULT);
5174     subs(len, len, wordSize);
5175     br(GE, LOOP);
5176     cmpw(len, -wordSize);
5177     br(EQ, SET_RESULT);
5178 
5179   BIND(END);
5180     ldr(result, Address(ary1));
5181     sub(len, zr, len, LSL, 3); // LSL 3 is to get bits from bytes
5182     lslv(result, result, len);
5183     tst(result, UPPER_BIT_MASK);
5184     b(SET_RESULT);
5185 
5186   BIND(STUB);
5187     RuntimeAddress has_neg =  RuntimeAddress(StubRoutines::aarch64::has_negatives());
5188     assert(has_neg.target() != NULL, &quot;has_negatives stub has not been generated&quot;);
5189     trampoline_call(has_neg);
5190     b(DONE);
5191 
5192   BIND(STUB_LONG);
5193     RuntimeAddress has_neg_long =  RuntimeAddress(
5194             StubRoutines::aarch64::has_negatives_long());
5195     assert(has_neg_long.target() != NULL, &quot;has_negatives stub has not been generated&quot;);
5196     trampoline_call(has_neg_long);
5197     b(DONE);
5198 
5199   BIND(SET_RESULT);
5200     cset(result, NE); // set true or false
5201 
5202   BIND(DONE);
5203 }
5204 
5205 void MacroAssembler::arrays_equals(Register a1, Register a2, Register tmp3,
5206                                    Register tmp4, Register tmp5, Register result,
5207                                    Register cnt1, int elem_size) {
5208   Label DONE, SAME;
5209   Register tmp1 = rscratch1;
5210   Register tmp2 = rscratch2;
5211   Register cnt2 = tmp2;  // cnt2 only used in array length compare
5212   int elem_per_word = wordSize/elem_size;
5213   int log_elem_size = exact_log2(elem_size);
5214   int length_offset = arrayOopDesc::length_offset_in_bytes();
5215   int base_offset
5216     = arrayOopDesc::base_offset_in_bytes(elem_size == 2 ? T_CHAR : T_BYTE);
5217   int stubBytesThreshold = 3 * 64 + (UseSIMDForArrayEquals ? 0 : 16);
5218 
5219   assert(elem_size == 1 || elem_size == 2, &quot;must be char or byte&quot;);
5220   assert_different_registers(a1, a2, result, cnt1, rscratch1, rscratch2);
5221 
5222 #ifndef PRODUCT
5223   {
5224     const char kind = (elem_size == 2) ? &#39;U&#39; : &#39;L&#39;;
5225     char comment[64];
5226     snprintf(comment, sizeof comment, &quot;array_equals%c{&quot;, kind);
5227     BLOCK_COMMENT(comment);
5228   }
5229 #endif
5230 
5231   // if (a1 == a2)
5232   //     return true;
5233   cmpoop(a1, a2); // May have read barriers for a1 and a2.
5234   br(EQ, SAME);
5235 
5236   if (UseSimpleArrayEquals) {
5237     Label NEXT_WORD, SHORT, TAIL03, TAIL01, A_MIGHT_BE_NULL, A_IS_NOT_NULL;
5238     // if (a1 == null || a2 == null)
5239     //     return false;
5240     // a1 &amp; a2 == 0 means (some-pointer is null) or
5241     // (very-rare-or-even-probably-impossible-pointer-values)
5242     // so, we can save one branch in most cases
5243     tst(a1, a2);
5244     mov(result, false);
5245     br(EQ, A_MIGHT_BE_NULL);
5246     // if (a1.length != a2.length)
5247     //      return false;
5248     bind(A_IS_NOT_NULL);
5249     ldrw(cnt1, Address(a1, length_offset));
5250     ldrw(cnt2, Address(a2, length_offset));
5251     eorw(tmp5, cnt1, cnt2);
5252     cbnzw(tmp5, DONE);
5253     lea(a1, Address(a1, base_offset));
5254     lea(a2, Address(a2, base_offset));
5255     // Check for short strings, i.e. smaller than wordSize.
5256     subs(cnt1, cnt1, elem_per_word);
5257     br(Assembler::LT, SHORT);
5258     // Main 8 byte comparison loop.
5259     bind(NEXT_WORD); {
5260       ldr(tmp1, Address(post(a1, wordSize)));
5261       ldr(tmp2, Address(post(a2, wordSize)));
5262       subs(cnt1, cnt1, elem_per_word);
5263       eor(tmp5, tmp1, tmp2);
5264       cbnz(tmp5, DONE);
5265     } br(GT, NEXT_WORD);
5266     // Last longword.  In the case where length == 4 we compare the
5267     // same longword twice, but that&#39;s still faster than another
5268     // conditional branch.
5269     // cnt1 could be 0, -1, -2, -3, -4 for chars; -4 only happens when
5270     // length == 4.
5271     if (log_elem_size &gt; 0)
5272       lsl(cnt1, cnt1, log_elem_size);
5273     ldr(tmp3, Address(a1, cnt1));
5274     ldr(tmp4, Address(a2, cnt1));
5275     eor(tmp5, tmp3, tmp4);
5276     cbnz(tmp5, DONE);
5277     b(SAME);
5278     bind(A_MIGHT_BE_NULL);
5279     // in case both a1 and a2 are not-null, proceed with loads
5280     cbz(a1, DONE);
5281     cbz(a2, DONE);
5282     b(A_IS_NOT_NULL);
5283     bind(SHORT);
5284 
5285     tbz(cnt1, 2 - log_elem_size, TAIL03); // 0-7 bytes left.
5286     {
5287       ldrw(tmp1, Address(post(a1, 4)));
5288       ldrw(tmp2, Address(post(a2, 4)));
5289       eorw(tmp5, tmp1, tmp2);
5290       cbnzw(tmp5, DONE);
5291     }
5292     bind(TAIL03);
5293     tbz(cnt1, 1 - log_elem_size, TAIL01); // 0-3 bytes left.
5294     {
5295       ldrh(tmp3, Address(post(a1, 2)));
5296       ldrh(tmp4, Address(post(a2, 2)));
5297       eorw(tmp5, tmp3, tmp4);
5298       cbnzw(tmp5, DONE);
5299     }
5300     bind(TAIL01);
5301     if (elem_size == 1) { // Only needed when comparing byte arrays.
5302       tbz(cnt1, 0, SAME); // 0-1 bytes left.
5303       {
5304         ldrb(tmp1, a1);
5305         ldrb(tmp2, a2);
5306         eorw(tmp5, tmp1, tmp2);
5307         cbnzw(tmp5, DONE);
5308       }
5309     }
5310   } else {
5311     Label NEXT_DWORD, SHORT, TAIL, TAIL2, STUB, EARLY_OUT,
5312         CSET_EQ, LAST_CHECK;
5313     mov(result, false);
5314     cbz(a1, DONE);
5315     ldrw(cnt1, Address(a1, length_offset));
5316     cbz(a2, DONE);
5317     ldrw(cnt2, Address(a2, length_offset));
5318     // on most CPUs a2 is still &quot;locked&quot;(surprisingly) in ldrw and it&#39;s
5319     // faster to perform another branch before comparing a1 and a2
5320     cmp(cnt1, (u1)elem_per_word);
5321     br(LE, SHORT); // short or same
5322     ldr(tmp3, Address(pre(a1, base_offset)));
5323     subs(zr, cnt1, stubBytesThreshold);
5324     br(GE, STUB);
5325     ldr(tmp4, Address(pre(a2, base_offset)));
5326     sub(tmp5, zr, cnt1, LSL, 3 + log_elem_size);
5327     cmp(cnt2, cnt1);
5328     br(NE, DONE);
5329 
5330     // Main 16 byte comparison loop with 2 exits
5331     bind(NEXT_DWORD); {
5332       ldr(tmp1, Address(pre(a1, wordSize)));
5333       ldr(tmp2, Address(pre(a2, wordSize)));
5334       subs(cnt1, cnt1, 2 * elem_per_word);
5335       br(LE, TAIL);
5336       eor(tmp4, tmp3, tmp4);
5337       cbnz(tmp4, DONE);
5338       ldr(tmp3, Address(pre(a1, wordSize)));
5339       ldr(tmp4, Address(pre(a2, wordSize)));
5340       cmp(cnt1, (u1)elem_per_word);
5341       br(LE, TAIL2);
5342       cmp(tmp1, tmp2);
5343     } br(EQ, NEXT_DWORD);
5344     b(DONE);
5345 
5346     bind(TAIL);
5347     eor(tmp4, tmp3, tmp4);
5348     eor(tmp2, tmp1, tmp2);
5349     lslv(tmp2, tmp2, tmp5);
5350     orr(tmp5, tmp4, tmp2);
5351     cmp(tmp5, zr);
5352     b(CSET_EQ);
5353 
5354     bind(TAIL2);
5355     eor(tmp2, tmp1, tmp2);
5356     cbnz(tmp2, DONE);
5357     b(LAST_CHECK);
5358 
5359     bind(STUB);
5360     ldr(tmp4, Address(pre(a2, base_offset)));
5361     cmp(cnt2, cnt1);
5362     br(NE, DONE);
5363     if (elem_size == 2) { // convert to byte counter
5364       lsl(cnt1, cnt1, 1);
5365     }
5366     eor(tmp5, tmp3, tmp4);
5367     cbnz(tmp5, DONE);
5368     RuntimeAddress stub = RuntimeAddress(StubRoutines::aarch64::large_array_equals());
5369     assert(stub.target() != NULL, &quot;array_equals_long stub has not been generated&quot;);
5370     trampoline_call(stub);
5371     b(DONE);
5372 
5373     bind(EARLY_OUT);
5374     // (a1 != null &amp;&amp; a2 == null) || (a1 != null &amp;&amp; a2 != null &amp;&amp; a1 == a2)
5375     // so, if a2 == null =&gt; return false(0), else return true, so we can return a2
5376     mov(result, a2);
5377     b(DONE);
5378     bind(SHORT);
5379     cmp(cnt2, cnt1);
5380     br(NE, DONE);
5381     cbz(cnt1, SAME);
5382     sub(tmp5, zr, cnt1, LSL, 3 + log_elem_size);
5383     ldr(tmp3, Address(a1, base_offset));
5384     ldr(tmp4, Address(a2, base_offset));
5385     bind(LAST_CHECK);
5386     eor(tmp4, tmp3, tmp4);
5387     lslv(tmp5, tmp4, tmp5);
5388     cmp(tmp5, zr);
5389     bind(CSET_EQ);
5390     cset(result, EQ);
5391     b(DONE);
5392   }
5393 
5394   bind(SAME);
5395   mov(result, true);
5396   // That&#39;s it.
5397   bind(DONE);
5398 
5399   BLOCK_COMMENT(&quot;} array_equals&quot;);
5400 }
5401 
5402 // Compare Strings
5403 
5404 // For Strings we&#39;re passed the address of the first characters in a1
5405 // and a2 and the length in cnt1.
5406 // elem_size is the element size in bytes: either 1 or 2.
5407 // There are two implementations.  For arrays &gt;= 8 bytes, all
5408 // comparisons (including the final one, which may overlap) are
5409 // performed 8 bytes at a time.  For strings &lt; 8 bytes, we compare a
5410 // halfword, then a short, and then a byte.
5411 
5412 void MacroAssembler::string_equals(Register a1, Register a2,
5413                                    Register result, Register cnt1, int elem_size)
5414 {
5415   Label SAME, DONE, SHORT, NEXT_WORD;
5416   Register tmp1 = rscratch1;
5417   Register tmp2 = rscratch2;
5418   Register cnt2 = tmp2;  // cnt2 only used in array length compare
5419 
5420   assert(elem_size == 1 || elem_size == 2, &quot;must be 2 or 1 byte&quot;);
5421   assert_different_registers(a1, a2, result, cnt1, rscratch1, rscratch2);
5422 
5423 #ifndef PRODUCT
5424   {
5425     const char kind = (elem_size == 2) ? &#39;U&#39; : &#39;L&#39;;
5426     char comment[64];
5427     snprintf(comment, sizeof comment, &quot;{string_equals%c&quot;, kind);
5428     BLOCK_COMMENT(comment);
5429   }
5430 #endif
5431 
5432   mov(result, false);
5433 
5434   // Check for short strings, i.e. smaller than wordSize.
5435   subs(cnt1, cnt1, wordSize);
5436   br(Assembler::LT, SHORT);
5437   // Main 8 byte comparison loop.
5438   bind(NEXT_WORD); {
5439     ldr(tmp1, Address(post(a1, wordSize)));
5440     ldr(tmp2, Address(post(a2, wordSize)));
5441     subs(cnt1, cnt1, wordSize);
5442     eor(tmp1, tmp1, tmp2);
5443     cbnz(tmp1, DONE);
5444   } br(GT, NEXT_WORD);
5445   // Last longword.  In the case where length == 4 we compare the
5446   // same longword twice, but that&#39;s still faster than another
5447   // conditional branch.
5448   // cnt1 could be 0, -1, -2, -3, -4 for chars; -4 only happens when
5449   // length == 4.
5450   ldr(tmp1, Address(a1, cnt1));
5451   ldr(tmp2, Address(a2, cnt1));
5452   eor(tmp2, tmp1, tmp2);
5453   cbnz(tmp2, DONE);
5454   b(SAME);
5455 
5456   bind(SHORT);
5457   Label TAIL03, TAIL01;
5458 
5459   tbz(cnt1, 2, TAIL03); // 0-7 bytes left.
5460   {
5461     ldrw(tmp1, Address(post(a1, 4)));
5462     ldrw(tmp2, Address(post(a2, 4)));
5463     eorw(tmp1, tmp1, tmp2);
5464     cbnzw(tmp1, DONE);
5465   }
5466   bind(TAIL03);
5467   tbz(cnt1, 1, TAIL01); // 0-3 bytes left.
5468   {
5469     ldrh(tmp1, Address(post(a1, 2)));
5470     ldrh(tmp2, Address(post(a2, 2)));
5471     eorw(tmp1, tmp1, tmp2);
5472     cbnzw(tmp1, DONE);
5473   }
5474   bind(TAIL01);
5475   if (elem_size == 1) { // Only needed when comparing 1-byte elements
5476     tbz(cnt1, 0, SAME); // 0-1 bytes left.
5477     {
5478       ldrb(tmp1, a1);
5479       ldrb(tmp2, a2);
5480       eorw(tmp1, tmp1, tmp2);
5481       cbnzw(tmp1, DONE);
5482     }
5483   }
5484   // Arrays are equal.
5485   bind(SAME);
5486   mov(result, true);
5487 
5488   // That&#39;s it.
5489   bind(DONE);
5490   BLOCK_COMMENT(&quot;} string_equals&quot;);
5491 }
5492 
5493 
5494 // The size of the blocks erased by the zero_blocks stub.  We must
5495 // handle anything smaller than this ourselves in zero_words().
5496 const int MacroAssembler::zero_words_block_size = 8;
5497 
5498 // zero_words() is used by C2 ClearArray patterns.  It is as small as
5499 // possible, handling small word counts locally and delegating
5500 // anything larger to the zero_blocks stub.  It is expanded many times
5501 // in compiled code, so it is important to keep it short.
5502 
5503 // ptr:   Address of a buffer to be zeroed.
5504 // cnt:   Count in HeapWords.
5505 //
5506 // ptr, cnt, rscratch1, and rscratch2 are clobbered.
5507 void MacroAssembler::zero_words(Register ptr, Register cnt)
5508 {
5509   assert(is_power_of_2(zero_words_block_size), &quot;adjust this&quot;);
5510   assert(ptr == r10 &amp;&amp; cnt == r11, &quot;mismatch in register usage&quot;);
5511 
5512   BLOCK_COMMENT(&quot;zero_words {&quot;);
5513   cmp(cnt, (u1)zero_words_block_size);
5514   Label around;
5515   br(LO, around);
5516   {
5517     RuntimeAddress zero_blocks =  RuntimeAddress(StubRoutines::aarch64::zero_blocks());
5518     assert(zero_blocks.target() != NULL, &quot;zero_blocks stub has not been generated&quot;);
5519     if (StubRoutines::aarch64::complete()) {
5520       trampoline_call(zero_blocks);
5521     } else {
5522       bl(zero_blocks);
5523     }
5524   }
5525   bind(around);
5526   for (int i = zero_words_block_size &gt;&gt; 1; i &gt; 1; i &gt;&gt;= 1) {
5527     Label l;
5528     tbz(cnt, exact_log2(i), l);
5529     for (int j = 0; j &lt; i; j += 2) {
5530       stp(zr, zr, post(ptr, 16));
5531     }
5532     bind(l);
5533   }
5534   {
5535     Label l;
5536     tbz(cnt, 0, l);
5537     str(zr, Address(ptr));
5538     bind(l);
5539   }
5540   BLOCK_COMMENT(&quot;} zero_words&quot;);
5541 }
5542 
5543 // base:         Address of a buffer to be zeroed, 8 bytes aligned.
5544 // cnt:          Immediate count in HeapWords.
5545 #define SmallArraySize (18 * BytesPerLong)
5546 void MacroAssembler::zero_words(Register base, u_int64_t cnt)
5547 {
5548   BLOCK_COMMENT(&quot;zero_words {&quot;);
5549   int i = cnt &amp; 1;  // store any odd word to start
5550   if (i) str(zr, Address(base));
5551 
5552   if (cnt &lt;= SmallArraySize / BytesPerLong) {
5553     for (; i &lt; (int)cnt; i += 2)
5554       stp(zr, zr, Address(base, i * wordSize));
5555   } else {
5556     const int unroll = 4; // Number of stp(zr, zr) instructions we&#39;ll unroll
5557     int remainder = cnt % (2 * unroll);
5558     for (; i &lt; remainder; i += 2)
5559       stp(zr, zr, Address(base, i * wordSize));
5560 
5561     Label loop;
5562     Register cnt_reg = rscratch1;
5563     Register loop_base = rscratch2;
5564     cnt = cnt - remainder;
5565     mov(cnt_reg, cnt);
5566     // adjust base and prebias by -2 * wordSize so we can pre-increment
5567     add(loop_base, base, (remainder - 2) * wordSize);
5568     bind(loop);
5569     sub(cnt_reg, cnt_reg, 2 * unroll);
5570     for (i = 1; i &lt; unroll; i++)
5571       stp(zr, zr, Address(loop_base, 2 * i * wordSize));
5572     stp(zr, zr, Address(pre(loop_base, 2 * unroll * wordSize)));
5573     cbnz(cnt_reg, loop);
5574   }
5575   BLOCK_COMMENT(&quot;} zero_words&quot;);
5576 }
5577 
5578 // Zero blocks of memory by using DC ZVA.
5579 //
5580 // Aligns the base address first sufficently for DC ZVA, then uses
5581 // DC ZVA repeatedly for every full block.  cnt is the size to be
5582 // zeroed in HeapWords.  Returns the count of words left to be zeroed
5583 // in cnt.
5584 //
5585 // NOTE: This is intended to be used in the zero_blocks() stub.  If
5586 // you want to use it elsewhere, note that cnt must be &gt;= 2*zva_length.
5587 void MacroAssembler::zero_dcache_blocks(Register base, Register cnt) {
5588   Register tmp = rscratch1;
5589   Register tmp2 = rscratch2;
5590   int zva_length = VM_Version::zva_length();
5591   Label initial_table_end, loop_zva;
5592   Label fini;
5593 
5594   // Base must be 16 byte aligned. If not just return and let caller handle it
5595   tst(base, 0x0f);
5596   br(Assembler::NE, fini);
5597   // Align base with ZVA length.
5598   neg(tmp, base);
5599   andr(tmp, tmp, zva_length - 1);
5600 
5601   // tmp: the number of bytes to be filled to align the base with ZVA length.
5602   add(base, base, tmp);
5603   sub(cnt, cnt, tmp, Assembler::ASR, 3);
5604   adr(tmp2, initial_table_end);
5605   sub(tmp2, tmp2, tmp, Assembler::LSR, 2);
5606   br(tmp2);
5607 
5608   for (int i = -zva_length + 16; i &lt; 0; i += 16)
5609     stp(zr, zr, Address(base, i));
5610   bind(initial_table_end);
5611 
5612   sub(cnt, cnt, zva_length &gt;&gt; 3);
5613   bind(loop_zva);
5614   dc(Assembler::ZVA, base);
5615   subs(cnt, cnt, zva_length &gt;&gt; 3);
5616   add(base, base, zva_length);
5617   br(Assembler::GE, loop_zva);
5618   add(cnt, cnt, zva_length &gt;&gt; 3); // count not zeroed by DC ZVA
5619   bind(fini);
5620 }
5621 
5622 // base:   Address of a buffer to be filled, 8 bytes aligned.
5623 // cnt:    Count in 8-byte unit.
5624 // value:  Value to be filled with.
5625 // base will point to the end of the buffer after filling.
5626 void MacroAssembler::fill_words(Register base, Register cnt, Register value)
5627 {
5628 //  Algorithm:
5629 //
5630 //    scratch1 = cnt &amp; 7;
5631 //    cnt -= scratch1;
5632 //    p += scratch1;
5633 //    switch (scratch1) {
5634 //      do {
5635 //        cnt -= 8;
5636 //          p[-8] = v;
5637 //        case 7:
5638 //          p[-7] = v;
5639 //        case 6:
5640 //          p[-6] = v;
5641 //          // ...
5642 //        case 1:
5643 //          p[-1] = v;
5644 //        case 0:
5645 //          p += 8;
5646 //      } while (cnt);
5647 //    }
5648 
5649   assert_different_registers(base, cnt, value, rscratch1, rscratch2);
5650 
5651   Label fini, skip, entry, loop;
5652   const int unroll = 8; // Number of stp instructions we&#39;ll unroll
5653 
5654   cbz(cnt, fini);
5655   tbz(base, 3, skip);
5656   str(value, Address(post(base, 8)));
5657   sub(cnt, cnt, 1);
5658   bind(skip);
5659 
5660   andr(rscratch1, cnt, (unroll-1) * 2);
5661   sub(cnt, cnt, rscratch1);
5662   add(base, base, rscratch1, Assembler::LSL, 3);
5663   adr(rscratch2, entry);
5664   sub(rscratch2, rscratch2, rscratch1, Assembler::LSL, 1);
5665   br(rscratch2);
5666 
5667   bind(loop);
5668   add(base, base, unroll * 16);
5669   for (int i = -unroll; i &lt; 0; i++)
5670     stp(value, value, Address(base, i * 16));
5671   bind(entry);
5672   subs(cnt, cnt, unroll * 2);
5673   br(Assembler::GE, loop);
5674 
5675   tbz(cnt, 0, fini);
5676   str(value, Address(post(base, 8)));
5677   bind(fini);
5678 }
5679 
5680 // Intrinsic for sun/nio/cs/ISO_8859_1$Encoder.implEncodeISOArray and
5681 // java/lang/StringUTF16.compress.
5682 void MacroAssembler::encode_iso_array(Register src, Register dst,
5683                       Register len, Register result,
5684                       FloatRegister Vtmp1, FloatRegister Vtmp2,
5685                       FloatRegister Vtmp3, FloatRegister Vtmp4)
5686 {
5687     Label DONE, SET_RESULT, NEXT_32, NEXT_32_PRFM, LOOP_8, NEXT_8, LOOP_1, NEXT_1,
5688         NEXT_32_START, NEXT_32_PRFM_START;
5689     Register tmp1 = rscratch1, tmp2 = rscratch2;
5690 
5691       mov(result, len); // Save initial len
5692 
5693       cmp(len, (u1)8); // handle shortest strings first
5694       br(LT, LOOP_1);
5695       cmp(len, (u1)32);
5696       br(LT, NEXT_8);
5697       // The following code uses the SIMD &#39;uzp1&#39; and &#39;uzp2&#39; instructions
5698       // to convert chars to bytes
5699       if (SoftwarePrefetchHintDistance &gt;= 0) {
5700         ld1(Vtmp1, Vtmp2, Vtmp3, Vtmp4, T8H, src);
5701         subs(tmp2, len, SoftwarePrefetchHintDistance/2 + 16);
5702         br(LE, NEXT_32_START);
5703         b(NEXT_32_PRFM_START);
5704         BIND(NEXT_32_PRFM);
5705           ld1(Vtmp1, Vtmp2, Vtmp3, Vtmp4, T8H, src);
5706         BIND(NEXT_32_PRFM_START);
5707           prfm(Address(src, SoftwarePrefetchHintDistance));
5708           orr(v4, T16B, Vtmp1, Vtmp2);
5709           orr(v5, T16B, Vtmp3, Vtmp4);
5710           uzp1(Vtmp1, T16B, Vtmp1, Vtmp2);
5711           uzp1(Vtmp3, T16B, Vtmp3, Vtmp4);
5712           uzp2(v5, T16B, v4, v5); // high bytes
5713           umov(tmp2, v5, D, 1);
5714           fmovd(tmp1, v5);
5715           orr(tmp1, tmp1, tmp2);
5716           cbnz(tmp1, LOOP_8);
5717           stpq(Vtmp1, Vtmp3, dst);
5718           sub(len, len, 32);
5719           add(dst, dst, 32);
5720           add(src, src, 64);
5721           subs(tmp2, len, SoftwarePrefetchHintDistance/2 + 16);
5722           br(GE, NEXT_32_PRFM);
5723           cmp(len, (u1)32);
5724           br(LT, LOOP_8);
5725         BIND(NEXT_32);
5726           ld1(Vtmp1, Vtmp2, Vtmp3, Vtmp4, T8H, src);
5727         BIND(NEXT_32_START);
5728       } else {
5729         BIND(NEXT_32);
5730           ld1(Vtmp1, Vtmp2, Vtmp3, Vtmp4, T8H, src);
5731       }
5732       prfm(Address(src, SoftwarePrefetchHintDistance));
5733       uzp1(v4, T16B, Vtmp1, Vtmp2);
5734       uzp1(v5, T16B, Vtmp3, Vtmp4);
5735       orr(Vtmp1, T16B, Vtmp1, Vtmp2);
5736       orr(Vtmp3, T16B, Vtmp3, Vtmp4);
5737       uzp2(Vtmp1, T16B, Vtmp1, Vtmp3); // high bytes
5738       umov(tmp2, Vtmp1, D, 1);
5739       fmovd(tmp1, Vtmp1);
5740       orr(tmp1, tmp1, tmp2);
5741       cbnz(tmp1, LOOP_8);
5742       stpq(v4, v5, dst);
5743       sub(len, len, 32);
5744       add(dst, dst, 32);
5745       add(src, src, 64);
5746       cmp(len, (u1)32);
5747       br(GE, NEXT_32);
5748       cbz(len, DONE);
5749 
5750     BIND(LOOP_8);
5751       cmp(len, (u1)8);
5752       br(LT, LOOP_1);
5753     BIND(NEXT_8);
5754       ld1(Vtmp1, T8H, src);
5755       uzp1(Vtmp2, T16B, Vtmp1, Vtmp1); // low bytes
5756       uzp2(Vtmp3, T16B, Vtmp1, Vtmp1); // high bytes
5757       fmovd(tmp1, Vtmp3);
5758       cbnz(tmp1, NEXT_1);
5759       strd(Vtmp2, dst);
5760 
5761       sub(len, len, 8);
5762       add(dst, dst, 8);
5763       add(src, src, 16);
5764       cmp(len, (u1)8);
5765       br(GE, NEXT_8);
5766 
5767     BIND(LOOP_1);
5768 
5769     cbz(len, DONE);
5770     BIND(NEXT_1);
5771       ldrh(tmp1, Address(post(src, 2)));
5772       tst(tmp1, 0xff00);
5773       br(NE, SET_RESULT);
5774       strb(tmp1, Address(post(dst, 1)));
5775       subs(len, len, 1);
5776       br(GT, NEXT_1);
5777 
5778     BIND(SET_RESULT);
5779       sub(result, result, len); // Return index where we stopped
5780                                 // Return len == 0 if we processed all
5781                                 // characters
5782     BIND(DONE);
5783 }
5784 
5785 
5786 // Inflate byte[] array to char[].
5787 void MacroAssembler::byte_array_inflate(Register src, Register dst, Register len,
5788                                         FloatRegister vtmp1, FloatRegister vtmp2, FloatRegister vtmp3,
5789                                         Register tmp4) {
5790   Label big, done, after_init, to_stub;
5791 
5792   assert_different_registers(src, dst, len, tmp4, rscratch1);
5793 
5794   fmovd(vtmp1, zr);
5795   lsrw(tmp4, len, 3);
5796   bind(after_init);
5797   cbnzw(tmp4, big);
5798   // Short string: less than 8 bytes.
5799   {
5800     Label loop, tiny;
5801 
5802     cmpw(len, 4);
5803     br(LT, tiny);
5804     // Use SIMD to do 4 bytes.
5805     ldrs(vtmp2, post(src, 4));
5806     zip1(vtmp3, T8B, vtmp2, vtmp1);
5807     subw(len, len, 4);
5808     strd(vtmp3, post(dst, 8));
5809 
5810     cbzw(len, done);
5811 
5812     // Do the remaining bytes by steam.
5813     bind(loop);
5814     ldrb(tmp4, post(src, 1));
5815     strh(tmp4, post(dst, 2));
5816     subw(len, len, 1);
5817 
5818     bind(tiny);
5819     cbnz(len, loop);
5820 
5821     b(done);
5822   }
5823 
5824   if (SoftwarePrefetchHintDistance &gt;= 0) {
5825     bind(to_stub);
5826       RuntimeAddress stub =  RuntimeAddress(StubRoutines::aarch64::large_byte_array_inflate());
5827       assert(stub.target() != NULL, &quot;large_byte_array_inflate stub has not been generated&quot;);
5828       trampoline_call(stub);
5829       b(after_init);
5830   }
5831 
5832   // Unpack the bytes 8 at a time.
5833   bind(big);
5834   {
5835     Label loop, around, loop_last, loop_start;
5836 
5837     if (SoftwarePrefetchHintDistance &gt;= 0) {
5838       const int large_loop_threshold = (64 + 16)/8;
5839       ldrd(vtmp2, post(src, 8));
5840       andw(len, len, 7);
5841       cmp(tmp4, (u1)large_loop_threshold);
5842       br(GE, to_stub);
5843       b(loop_start);
5844 
5845       bind(loop);
5846       ldrd(vtmp2, post(src, 8));
5847       bind(loop_start);
5848       subs(tmp4, tmp4, 1);
5849       br(EQ, loop_last);
5850       zip1(vtmp2, T16B, vtmp2, vtmp1);
5851       ldrd(vtmp3, post(src, 8));
5852       st1(vtmp2, T8H, post(dst, 16));
5853       subs(tmp4, tmp4, 1);
5854       zip1(vtmp3, T16B, vtmp3, vtmp1);
5855       st1(vtmp3, T8H, post(dst, 16));
5856       br(NE, loop);
5857       b(around);
5858       bind(loop_last);
5859       zip1(vtmp2, T16B, vtmp2, vtmp1);
5860       st1(vtmp2, T8H, post(dst, 16));
5861       bind(around);
5862       cbz(len, done);
5863     } else {
5864       andw(len, len, 7);
5865       bind(loop);
5866       ldrd(vtmp2, post(src, 8));
5867       sub(tmp4, tmp4, 1);
5868       zip1(vtmp3, T16B, vtmp2, vtmp1);
5869       st1(vtmp3, T8H, post(dst, 16));
5870       cbnz(tmp4, loop);
5871     }
5872   }
5873 
5874   // Do the tail of up to 8 bytes.
5875   add(src, src, len);
5876   ldrd(vtmp3, Address(src, -8));
5877   add(dst, dst, len, ext::uxtw, 1);
5878   zip1(vtmp3, T16B, vtmp3, vtmp1);
5879   strq(vtmp3, Address(dst, -16));
5880 
5881   bind(done);
5882 }
5883 
5884 // Compress char[] array to byte[].
5885 void MacroAssembler::char_array_compress(Register src, Register dst, Register len,
5886                                          FloatRegister tmp1Reg, FloatRegister tmp2Reg,
5887                                          FloatRegister tmp3Reg, FloatRegister tmp4Reg,
5888                                          Register result) {
5889   encode_iso_array(src, dst, len, result,
5890                    tmp1Reg, tmp2Reg, tmp3Reg, tmp4Reg);
5891   cmp(len, zr);
5892   csel(result, result, zr, EQ);
5893 }
5894 
5895 // get_thread() can be called anywhere inside generated code so we
5896 // need to save whatever non-callee save context might get clobbered
5897 // by the call to JavaThread::aarch64_get_thread_helper() or, indeed,
5898 // the call setup code.
5899 //
5900 // aarch64_get_thread_helper() clobbers only r0, r1, and flags.
5901 //
5902 void MacroAssembler::get_thread(Register dst) {
5903   RegSet saved_regs = RegSet::range(r0, r1) + lr - dst;
5904   push(saved_regs, sp);
5905 
5906   mov(lr, CAST_FROM_FN_PTR(address, JavaThread::aarch64_get_thread_helper));
5907   blr(lr);
5908   if (dst != c_rarg0) {
5909     mov(dst, c_rarg0);
5910   }
5911 
5912   pop(saved_regs, sp);
5913 }
5914 
5915 void MacroAssembler::cache_wb(Address line) {
5916   assert(line.getMode() == Address::base_plus_offset, &quot;mode should be base_plus_offset&quot;);
5917   assert(line.index() == noreg, &quot;index should be noreg&quot;);
5918   assert(line.offset() == 0, &quot;offset should be 0&quot;);
5919   // would like to assert this
5920   // assert(line._ext.shift == 0, &quot;shift should be zero&quot;);
5921   if (VM_Version::supports_dcpop()) {
5922     // writeback using clear virtual address to point of persistence
5923     dc(Assembler::CVAP, line.base());
5924   } else {
5925     // no need to generate anything as Unsafe.writebackMemory should
5926     // never invoke this stub
5927   }
5928 }
5929 
5930 void MacroAssembler::cache_wbsync(bool is_pre) {
5931   // we only need a barrier post sync
5932   if (!is_pre) {
5933     membar(Assembler::AnyAny);
5934   }
5935 }
<a name="5" id="anc5"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="5" type="hidden" />
</body>
</html>