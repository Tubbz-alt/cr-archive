<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/cpu/arm/c1_MacroAssembler_arm.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2008, 2018, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;c1/c1_MacroAssembler.hpp&quot;
 27 #include &quot;c1/c1_Runtime1.hpp&quot;
 28 #include &quot;classfile/systemDictionary.hpp&quot;
 29 #include &quot;gc/shared/collectedHeap.hpp&quot;
 30 #include &quot;interpreter/interpreter.hpp&quot;
 31 #include &quot;oops/arrayOop.hpp&quot;
 32 #include &quot;oops/markWord.hpp&quot;
 33 #include &quot;runtime/basicLock.hpp&quot;
 34 #include &quot;runtime/biasedLocking.hpp&quot;
 35 #include &quot;runtime/os.hpp&quot;
 36 #include &quot;runtime/sharedRuntime.hpp&quot;
 37 #include &quot;runtime/stubRoutines.hpp&quot;
 38 #include &quot;utilities/powerOfTwo.hpp&quot;
 39 
 40 // Note: Rtemp usage is this file should not impact C2 and should be
 41 // correct as long as it is not implicitly used in lower layers (the
 42 // arm [macro]assembler) and used with care in the other C1 specific
 43 // files.
 44 
 45 void C1_MacroAssembler::inline_cache_check(Register receiver, Register iCache) {
 46   Label verified;
 47   load_klass(Rtemp, receiver);
 48   cmp(Rtemp, iCache);
 49   b(verified, eq); // jump over alignment no-ops
 50   jump(SharedRuntime::get_ic_miss_stub(), relocInfo::runtime_call_type);
 51   align(CodeEntryAlignment);
 52   bind(verified);
 53 }
 54 
 55 void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {
 56   assert(bang_size_in_bytes &gt;= frame_size_in_bytes, &quot;stack bang size incorrect&quot;);
 57   assert((frame_size_in_bytes % StackAlignmentInBytes) == 0, &quot;frame size should be aligned&quot;);
 58 
 59 
 60   arm_stack_overflow_check(bang_size_in_bytes, Rtemp);
 61 
 62   // FP can no longer be used to memorize SP. It may be modified
 63   // if this method contains a methodHandle call site
 64   raw_push(FP, LR);
 65   sub_slow(SP, SP, frame_size_in_bytes);
 66 }
 67 
 68 void C1_MacroAssembler::remove_frame(int frame_size_in_bytes) {
 69   add_slow(SP, SP, frame_size_in_bytes);
 70   raw_pop(FP, LR);
 71 }
 72 
 73 void C1_MacroAssembler::verified_entry() {
 74   if (C1Breakpoint) {
 75     breakpoint();
 76   }
 77 }
 78 
 79 // Puts address of allocated object into register `obj` and end of allocated object into register `obj_end`.
 80 void C1_MacroAssembler::try_allocate(Register obj, Register obj_end, Register tmp1, Register tmp2,
 81                                      RegisterOrConstant size_expression, Label&amp; slow_case) {
 82   if (UseTLAB) {
 83     tlab_allocate(obj, obj_end, tmp1, size_expression, slow_case);
 84   } else {
 85     eden_allocate(obj, obj_end, tmp1, tmp2, size_expression, slow_case);
 86   }
 87 }
 88 
 89 
 90 void C1_MacroAssembler::initialize_header(Register obj, Register klass, Register len, Register tmp) {
 91   assert_different_registers(obj, klass, len, tmp);
 92 
 93   if(UseBiasedLocking &amp;&amp; !len-&gt;is_valid()) {
 94     ldr(tmp, Address(klass, Klass::prototype_header_offset()));
 95   } else {
 96     mov(tmp, (intptr_t)markWord::prototype().value());
 97   }
 98 
 99   str(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));
100   str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));
101 
102   if (len-&gt;is_valid()) {
103     str_32(len, Address(obj, arrayOopDesc::length_offset_in_bytes()));
104   }
105 }
106 
107 
108 // Cleans object body [base..obj_end]. Clobbers `base` and `tmp` registers.
109 void C1_MacroAssembler::initialize_body(Register base, Register obj_end, Register tmp) {
110   zero_memory(base, obj_end, tmp);
111 }
112 
113 
114 void C1_MacroAssembler::initialize_object(Register obj, Register obj_end, Register klass,
115                                           Register len, Register tmp1, Register tmp2,
116                                           RegisterOrConstant header_size, int obj_size_in_bytes,
117                                           bool is_tlab_allocated)
118 {
119   assert_different_registers(obj, obj_end, klass, len, tmp1, tmp2);
120   initialize_header(obj, klass, len, tmp1);
121 
122   const Register ptr = tmp2;
123 
124   if (!(UseTLAB &amp;&amp; ZeroTLAB &amp;&amp; is_tlab_allocated)) {
125     if (obj_size_in_bytes &gt;= 0 &amp;&amp; obj_size_in_bytes &lt;= 8 * BytesPerWord) {
126       mov(tmp1, 0);
127       const int base = instanceOopDesc::header_size() * HeapWordSize;
128       for (int i = base; i &lt; obj_size_in_bytes; i += wordSize) {
129         str(tmp1, Address(obj, i));
130       }
131     } else {
132       assert(header_size.is_constant() || header_size.as_register() == ptr, &quot;code assumption&quot;);
133       add(ptr, obj, header_size);
134       initialize_body(ptr, obj_end, tmp1);
135     }
136   }
137 
138   // StoreStore barrier required after complete initialization
139   // (headers + content zeroing), before the object may escape.
140   membar(MacroAssembler::StoreStore, tmp1);
141 }
142 
143 void C1_MacroAssembler::allocate_object(Register obj, Register tmp1, Register tmp2, Register tmp3,
144                                         int header_size, int object_size,
145                                         Register klass, Label&amp; slow_case) {
146   assert_different_registers(obj, tmp1, tmp2, tmp3, klass, Rtemp);
147   assert(header_size &gt;= 0 &amp;&amp; object_size &gt;= header_size, &quot;illegal sizes&quot;);
148   const int object_size_in_bytes = object_size * BytesPerWord;
149 
150   const Register obj_end = tmp1;
151   const Register len = noreg;
152 
153   if (Assembler::is_arith_imm_in_range(object_size_in_bytes)) {
154     try_allocate(obj, obj_end, tmp2, tmp3, object_size_in_bytes, slow_case);
155   } else {
156     // Rtemp should be free at c1 LIR level
157     mov_slow(Rtemp, object_size_in_bytes);
158     try_allocate(obj, obj_end, tmp2, tmp3, Rtemp, slow_case);
159   }
160   initialize_object(obj, obj_end, klass, len, tmp2, tmp3, instanceOopDesc::header_size() * HeapWordSize, object_size_in_bytes, /* is_tlab_allocated */ UseTLAB);
161 }
162 
163 void C1_MacroAssembler::allocate_array(Register obj, Register len,
164                                        Register tmp1, Register tmp2, Register tmp3,
165                                        int header_size, int element_size,
166                                        Register klass, Label&amp; slow_case) {
167   assert_different_registers(obj, len, tmp1, tmp2, tmp3, klass, Rtemp);
168   const int header_size_in_bytes = header_size * BytesPerWord;
169   const int scale_shift = exact_log2(element_size);
170   const Register obj_size = Rtemp; // Rtemp should be free at c1 LIR level
171 
172   cmp_32(len, max_array_allocation_length);
173   b(slow_case, hs);
174 
175   bool align_header = ((header_size_in_bytes | element_size) &amp; MinObjAlignmentInBytesMask) != 0;
176   assert(align_header || ((header_size_in_bytes &amp; MinObjAlignmentInBytesMask) == 0), &quot;must be&quot;);
177   assert(align_header || ((element_size &amp; MinObjAlignmentInBytesMask) == 0), &quot;must be&quot;);
178 
179   mov(obj_size, header_size_in_bytes + (align_header ? (MinObjAlignmentInBytes - 1) : 0));
180   add_ptr_scaled_int32(obj_size, obj_size, len, scale_shift);
181 
182   if (align_header) {
183     align_reg(obj_size, obj_size, MinObjAlignmentInBytes);
184   }
185 
186   try_allocate(obj, tmp1, tmp2, tmp3, obj_size, slow_case);
187   initialize_object(obj, tmp1, klass, len, tmp2, tmp3, header_size_in_bytes, -1, /* is_tlab_allocated */ UseTLAB);
188 }
189 
190 int C1_MacroAssembler::lock_object(Register hdr, Register obj,
191                                    Register disp_hdr, Register tmp1,
192                                    Label&amp; slow_case) {
193   Label done, fast_lock, fast_lock_done;
194   int null_check_offset = 0;
195 
196   const Register tmp2 = Rtemp; // Rtemp should be free at c1 LIR level
197   assert_different_registers(hdr, obj, disp_hdr, tmp1, tmp2);
198 
199   assert(BasicObjectLock::lock_offset_in_bytes() == 0, &quot;ajust this code&quot;);
200   const int obj_offset = BasicObjectLock::obj_offset_in_bytes();
201   const int mark_offset = BasicLock::displaced_header_offset_in_bytes();
202 
203   if (UseBiasedLocking) {
204     // load object
205     str(obj, Address(disp_hdr, obj_offset));
206     null_check_offset = biased_locking_enter(obj, hdr/*scratched*/, tmp1, false, tmp2, done, slow_case);
207   }
208 
209   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;Required by atomic instructions&quot;);
210 
211 
212   if (!UseBiasedLocking) {
213     null_check_offset = offset();
214   }
215 
216   // On MP platforms the next load could return a &#39;stale&#39; value if the memory location has been modified by another thread.
217   // That would be acceptable as ether CAS or slow case path is taken in that case.
218 
219   // Must be the first instruction here, because implicit null check relies on it
220   ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));
221 
222   str(obj, Address(disp_hdr, obj_offset));
223   tst(hdr, markWord::unlocked_value);
224   b(fast_lock, ne);
225 
226   // Check for recursive locking
227   // See comments in InterpreterMacroAssembler::lock_object for
228   // explanations on the fast recursive locking check.
229   // -1- test low 2 bits
230   movs(tmp2, AsmOperand(hdr, lsl, 30));
231   // -2- test (hdr - SP) if the low two bits are 0
232   sub(tmp2, hdr, SP, eq);
233   movs(tmp2, AsmOperand(tmp2, lsr, exact_log2(os::vm_page_size())), eq);
234   // If &#39;eq&#39; then OK for recursive fast locking: store 0 into a lock record.
235   str(tmp2, Address(disp_hdr, mark_offset), eq);
236   b(fast_lock_done, eq);
237   // else need slow case
238   b(slow_case);
239 
240 
241   bind(fast_lock);
242   // Save previous object header in BasicLock structure and update the header
243   str(hdr, Address(disp_hdr, mark_offset));
244 
245   cas_for_lock_acquire(hdr, disp_hdr, obj, tmp2, slow_case);
246 
247   bind(fast_lock_done);
248 
249 #ifndef PRODUCT
250   if (PrintBiasedLockingStatistics) {
251     cond_atomic_inc32(al, BiasedLocking::fast_path_entry_count_addr());
252   }
253 #endif // !PRODUCT
254 
255   bind(done);
256 
257   return null_check_offset;
258 }
259 
260 void C1_MacroAssembler::unlock_object(Register hdr, Register obj,
261                                       Register disp_hdr, Register tmp,
262                                       Label&amp; slow_case) {
263   // Note: this method is not using its &#39;tmp&#39; argument
264 
265   assert_different_registers(hdr, obj, disp_hdr, Rtemp);
266   Register tmp2 = Rtemp;
267 
268   assert(BasicObjectLock::lock_offset_in_bytes() == 0, &quot;ajust this code&quot;);
269   const int obj_offset = BasicObjectLock::obj_offset_in_bytes();
270   const int mark_offset = BasicLock::displaced_header_offset_in_bytes();
271 
272   Label done;
273   if (UseBiasedLocking) {
274     // load object
275     ldr(obj, Address(disp_hdr, obj_offset));
276     biased_locking_exit(obj, hdr, done);
277   }
278 
279   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;Required by atomic instructions&quot;);
280 
281   // Load displaced header and object from the lock
282   ldr(hdr, Address(disp_hdr, mark_offset));
283   // If hdr is NULL, we&#39;ve got recursive locking and there&#39;s nothing more to do
284   cbz(hdr, done);
285 
286   if(!UseBiasedLocking) {
287     // load object
288     ldr(obj, Address(disp_hdr, obj_offset));
289   }
290 
291   // Restore the object header
292   cas_for_lock_release(disp_hdr, hdr, obj, tmp2, slow_case);
293 
294   bind(done);
295 }
296 
297 
298 #ifndef PRODUCT
299 
300 void C1_MacroAssembler::verify_stack_oop(int stack_offset) {
301   if (!VerifyOops) return;
302   verify_oop_addr(Address(SP, stack_offset));
303 }
304 
305 void C1_MacroAssembler::verify_not_null_oop(Register r) {
306   Label not_null;
307   cbnz(r, not_null);
308   stop(&quot;non-null oop required&quot;);
309   bind(not_null);
310   if (!VerifyOops) return;
311   verify_oop(r);
312 }
313 
314 #endif // !PRODUCT
    </pre>
  </body>
</html>