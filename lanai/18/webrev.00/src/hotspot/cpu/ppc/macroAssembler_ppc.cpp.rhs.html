<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/ppc/macroAssembler_ppc.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2012, 2019, SAP SE. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;compiler/disassembler.hpp&quot;
  29 #include &quot;gc/shared/collectedHeap.inline.hpp&quot;
  30 #include &quot;gc/shared/barrierSet.hpp&quot;
  31 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  32 #include &quot;interpreter/interpreter.hpp&quot;
  33 #include &quot;memory/resourceArea.hpp&quot;
  34 #include &quot;nativeInst_ppc.hpp&quot;
  35 #include &quot;oops/klass.inline.hpp&quot;
  36 #include &quot;prims/methodHandles.hpp&quot;
  37 #include &quot;runtime/biasedLocking.hpp&quot;
  38 #include &quot;runtime/icache.hpp&quot;
  39 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  40 #include &quot;runtime/objectMonitor.hpp&quot;
  41 #include &quot;runtime/os.hpp&quot;
  42 #include &quot;runtime/safepoint.hpp&quot;
  43 #include &quot;runtime/safepointMechanism.hpp&quot;
  44 #include &quot;runtime/sharedRuntime.hpp&quot;
  45 #include &quot;runtime/stubRoutines.hpp&quot;
  46 #include &quot;utilities/macros.hpp&quot;
<a name="1" id="anc1"></a><span class="line-added">  47 #include &quot;utilities/powerOfTwo.hpp&quot;</span>
  48 #ifdef COMPILER2
  49 #include &quot;opto/intrinsicnode.hpp&quot;
  50 #endif
  51 
  52 #ifdef PRODUCT
  53 #define BLOCK_COMMENT(str) // nothing
  54 #else
  55 #define BLOCK_COMMENT(str) block_comment(str)
  56 #endif
  57 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  58 
  59 #ifdef ASSERT
  60 // On RISC, there&#39;s no benefit to verifying instruction boundaries.
  61 bool AbstractAssembler::pd_check_instruction_mark() { return false; }
  62 #endif
  63 
  64 void MacroAssembler::ld_largeoffset_unchecked(Register d, int si31, Register a, int emit_filler_nop) {
  65   assert(Assembler::is_simm(si31, 31) &amp;&amp; si31 &gt;= 0, &quot;si31 out of range&quot;);
  66   if (Assembler::is_simm(si31, 16)) {
  67     ld(d, si31, a);
  68     if (emit_filler_nop) nop();
  69   } else {
  70     const int hi = MacroAssembler::largeoffset_si16_si16_hi(si31);
  71     const int lo = MacroAssembler::largeoffset_si16_si16_lo(si31);
  72     addis(d, a, hi);
  73     ld(d, lo, d);
  74   }
  75 }
  76 
  77 void MacroAssembler::ld_largeoffset(Register d, int si31, Register a, int emit_filler_nop) {
  78   assert_different_registers(d, a);
  79   ld_largeoffset_unchecked(d, si31, a, emit_filler_nop);
  80 }
  81 
  82 void MacroAssembler::load_sized_value(Register dst, RegisterOrConstant offs, Register base,
  83                                       size_t size_in_bytes, bool is_signed) {
  84   switch (size_in_bytes) {
  85   case  8:              ld(dst, offs, base);                         break;
  86   case  4:  is_signed ? lwa(dst, offs, base) : lwz(dst, offs, base); break;
  87   case  2:  is_signed ? lha(dst, offs, base) : lhz(dst, offs, base); break;
  88   case  1:  lbz(dst, offs, base); if (is_signed) extsb(dst, dst);    break; // lba doesn&#39;t exist :(
  89   default:  ShouldNotReachHere();
  90   }
  91 }
  92 
  93 void MacroAssembler::store_sized_value(Register dst, RegisterOrConstant offs, Register base,
  94                                        size_t size_in_bytes) {
  95   switch (size_in_bytes) {
  96   case  8:  std(dst, offs, base); break;
  97   case  4:  stw(dst, offs, base); break;
  98   case  2:  sth(dst, offs, base); break;
  99   case  1:  stb(dst, offs, base); break;
 100   default:  ShouldNotReachHere();
 101   }
 102 }
 103 
 104 void MacroAssembler::align(int modulus, int max, int rem) {
 105   int padding = (rem + modulus - (offset() % modulus)) % modulus;
 106   if (padding &gt; max) return;
 107   for (int c = (padding &gt;&gt; 2); c &gt; 0; --c) { nop(); }
 108 }
 109 
 110 // Issue instructions that calculate given TOC from global TOC.
 111 void MacroAssembler::calculate_address_from_global_toc(Register dst, address addr, bool hi16, bool lo16,
 112                                                        bool add_relocation, bool emit_dummy_addr) {
 113   int offset = -1;
 114   if (emit_dummy_addr) {
 115     offset = -128; // dummy address
 116   } else if (addr != (address)(intptr_t)-1) {
 117     offset = MacroAssembler::offset_to_global_toc(addr);
 118   }
 119 
 120   if (hi16) {
 121     addis(dst, R29_TOC, MacroAssembler::largeoffset_si16_si16_hi(offset));
 122   }
 123   if (lo16) {
 124     if (add_relocation) {
 125       // Relocate at the addi to avoid confusion with a load from the method&#39;s TOC.
 126       relocate(internal_word_Relocation::spec(addr));
 127     }
 128     addi(dst, dst, MacroAssembler::largeoffset_si16_si16_lo(offset));
 129   }
 130 }
 131 
 132 address MacroAssembler::patch_calculate_address_from_global_toc_at(address a, address bound, address addr) {
 133   const int offset = MacroAssembler::offset_to_global_toc(addr);
 134 
 135   const address inst2_addr = a;
 136   const int inst2 = *(int *)inst2_addr;
 137 
 138   // The relocation points to the second instruction, the addi,
 139   // and the addi reads and writes the same register dst.
 140   const int dst = inv_rt_field(inst2);
 141   assert(is_addi(inst2) &amp;&amp; inv_ra_field(inst2) == dst, &quot;must be addi reading and writing dst&quot;);
 142 
 143   // Now, find the preceding addis which writes to dst.
 144   int inst1 = 0;
 145   address inst1_addr = inst2_addr - BytesPerInstWord;
 146   while (inst1_addr &gt;= bound) {
 147     inst1 = *(int *) inst1_addr;
 148     if (is_addis(inst1) &amp;&amp; inv_rt_field(inst1) == dst) {
 149       // Stop, found the addis which writes dst.
 150       break;
 151     }
 152     inst1_addr -= BytesPerInstWord;
 153   }
 154 
 155   assert(is_addis(inst1) &amp;&amp; inv_ra_field(inst1) == 29 /* R29 */, &quot;source must be global TOC&quot;);
 156   set_imm((int *)inst1_addr, MacroAssembler::largeoffset_si16_si16_hi(offset));
 157   set_imm((int *)inst2_addr, MacroAssembler::largeoffset_si16_si16_lo(offset));
 158   return inst1_addr;
 159 }
 160 
 161 address MacroAssembler::get_address_of_calculate_address_from_global_toc_at(address a, address bound) {
 162   const address inst2_addr = a;
 163   const int inst2 = *(int *)inst2_addr;
 164 
 165   // The relocation points to the second instruction, the addi,
 166   // and the addi reads and writes the same register dst.
 167   const int dst = inv_rt_field(inst2);
 168   assert(is_addi(inst2) &amp;&amp; inv_ra_field(inst2) == dst, &quot;must be addi reading and writing dst&quot;);
 169 
 170   // Now, find the preceding addis which writes to dst.
 171   int inst1 = 0;
 172   address inst1_addr = inst2_addr - BytesPerInstWord;
 173   while (inst1_addr &gt;= bound) {
 174     inst1 = *(int *) inst1_addr;
 175     if (is_addis(inst1) &amp;&amp; inv_rt_field(inst1) == dst) {
 176       // stop, found the addis which writes dst
 177       break;
 178     }
 179     inst1_addr -= BytesPerInstWord;
 180   }
 181 
 182   assert(is_addis(inst1) &amp;&amp; inv_ra_field(inst1) == 29 /* R29 */, &quot;source must be global TOC&quot;);
 183 
 184   int offset = (get_imm(inst1_addr, 0) &lt;&lt; 16) + get_imm(inst2_addr, 0);
 185   // -1 is a special case
 186   if (offset == -1) {
 187     return (address)(intptr_t)-1;
 188   } else {
 189     return global_toc() + offset;
 190   }
 191 }
 192 
 193 #ifdef _LP64
 194 // Patch compressed oops or klass constants.
 195 // Assembler sequence is
 196 // 1) compressed oops:
 197 //    lis  rx = const.hi
 198 //    ori rx = rx | const.lo
 199 // 2) compressed klass:
 200 //    lis  rx = const.hi
 201 //    clrldi rx = rx &amp; 0xFFFFffff // clearMS32b, optional
 202 //    ori rx = rx | const.lo
 203 // Clrldi will be passed by.
 204 address MacroAssembler::patch_set_narrow_oop(address a, address bound, narrowOop data) {
 205   assert(UseCompressedOops, &quot;Should only patch compressed oops&quot;);
 206 
 207   const address inst2_addr = a;
 208   const int inst2 = *(int *)inst2_addr;
 209 
 210   // The relocation points to the second instruction, the ori,
 211   // and the ori reads and writes the same register dst.
 212   const int dst = inv_rta_field(inst2);
 213   assert(is_ori(inst2) &amp;&amp; inv_rs_field(inst2) == dst, &quot;must be ori reading and writing dst&quot;);
 214   // Now, find the preceding addis which writes to dst.
 215   int inst1 = 0;
 216   address inst1_addr = inst2_addr - BytesPerInstWord;
 217   bool inst1_found = false;
 218   while (inst1_addr &gt;= bound) {
 219     inst1 = *(int *)inst1_addr;
 220     if (is_lis(inst1) &amp;&amp; inv_rs_field(inst1) == dst) { inst1_found = true; break; }
 221     inst1_addr -= BytesPerInstWord;
 222   }
 223   assert(inst1_found, &quot;inst is not lis&quot;);
 224 
 225   int xc = (data &gt;&gt; 16) &amp; 0xffff;
 226   int xd = (data &gt;&gt;  0) &amp; 0xffff;
 227 
 228   set_imm((int *)inst1_addr, (short)(xc)); // see enc_load_con_narrow_hi/_lo
 229   set_imm((int *)inst2_addr,        (xd)); // unsigned int
 230   return inst1_addr;
 231 }
 232 
 233 // Get compressed oop or klass constant.
 234 narrowOop MacroAssembler::get_narrow_oop(address a, address bound) {
 235   assert(UseCompressedOops, &quot;Should only patch compressed oops&quot;);
 236 
 237   const address inst2_addr = a;
 238   const int inst2 = *(int *)inst2_addr;
 239 
 240   // The relocation points to the second instruction, the ori,
 241   // and the ori reads and writes the same register dst.
 242   const int dst = inv_rta_field(inst2);
 243   assert(is_ori(inst2) &amp;&amp; inv_rs_field(inst2) == dst, &quot;must be ori reading and writing dst&quot;);
 244   // Now, find the preceding lis which writes to dst.
 245   int inst1 = 0;
 246   address inst1_addr = inst2_addr - BytesPerInstWord;
 247   bool inst1_found = false;
 248 
 249   while (inst1_addr &gt;= bound) {
 250     inst1 = *(int *) inst1_addr;
 251     if (is_lis(inst1) &amp;&amp; inv_rs_field(inst1) == dst) { inst1_found = true; break;}
 252     inst1_addr -= BytesPerInstWord;
 253   }
 254   assert(inst1_found, &quot;inst is not lis&quot;);
 255 
 256   uint xl = ((unsigned int) (get_imm(inst2_addr, 0) &amp; 0xffff));
 257   uint xh = (((get_imm(inst1_addr, 0)) &amp; 0xffff) &lt;&lt; 16);
 258 
 259   return (int) (xl | xh);
 260 }
 261 #endif // _LP64
 262 
 263 // Returns true if successful.
 264 bool MacroAssembler::load_const_from_method_toc(Register dst, AddressLiteral&amp; a,
 265                                                 Register toc, bool fixed_size) {
 266   int toc_offset = 0;
 267   // Use RelocationHolder::none for the constant pool entry, otherwise
 268   // we will end up with a failing NativeCall::verify(x) where x is
 269   // the address of the constant pool entry.
 270   // FIXME: We should insert relocation information for oops at the constant
 271   // pool entries instead of inserting it at the loads; patching of a constant
 272   // pool entry should be less expensive.
 273   address const_address = address_constant((address)a.value(), RelocationHolder::none);
 274   if (const_address == NULL) { return false; } // allocation failure
 275   // Relocate at the pc of the load.
 276   relocate(a.rspec());
 277   toc_offset = (int)(const_address - code()-&gt;consts()-&gt;start());
 278   ld_largeoffset_unchecked(dst, toc_offset, toc, fixed_size);
 279   return true;
 280 }
 281 
 282 bool MacroAssembler::is_load_const_from_method_toc_at(address a) {
 283   const address inst1_addr = a;
 284   const int inst1 = *(int *)inst1_addr;
 285 
 286    // The relocation points to the ld or the addis.
 287    return (is_ld(inst1)) ||
 288           (is_addis(inst1) &amp;&amp; inv_ra_field(inst1) != 0);
 289 }
 290 
 291 int MacroAssembler::get_offset_of_load_const_from_method_toc_at(address a) {
 292   assert(is_load_const_from_method_toc_at(a), &quot;must be load_const_from_method_toc&quot;);
 293 
 294   const address inst1_addr = a;
 295   const int inst1 = *(int *)inst1_addr;
 296 
 297   if (is_ld(inst1)) {
 298     return inv_d1_field(inst1);
 299   } else if (is_addis(inst1)) {
 300     const int dst = inv_rt_field(inst1);
 301 
 302     // Now, find the succeeding ld which reads and writes to dst.
 303     address inst2_addr = inst1_addr + BytesPerInstWord;
 304     int inst2 = 0;
 305     while (true) {
 306       inst2 = *(int *) inst2_addr;
 307       if (is_ld(inst2) &amp;&amp; inv_ra_field(inst2) == dst &amp;&amp; inv_rt_field(inst2) == dst) {
 308         // Stop, found the ld which reads and writes dst.
 309         break;
 310       }
 311       inst2_addr += BytesPerInstWord;
 312     }
 313     return (inv_d1_field(inst1) &lt;&lt; 16) + inv_d1_field(inst2);
 314   }
 315   ShouldNotReachHere();
 316   return 0;
 317 }
 318 
 319 // Get the constant from a `load_const&#39; sequence.
 320 long MacroAssembler::get_const(address a) {
 321   assert(is_load_const_at(a), &quot;not a load of a constant&quot;);
 322   const int *p = (const int*) a;
 323   unsigned long x = (((unsigned long) (get_imm(a,0) &amp; 0xffff)) &lt;&lt; 48);
 324   if (is_ori(*(p+1))) {
 325     x |= (((unsigned long) (get_imm(a,1) &amp; 0xffff)) &lt;&lt; 32);
 326     x |= (((unsigned long) (get_imm(a,3) &amp; 0xffff)) &lt;&lt; 16);
 327     x |= (((unsigned long) (get_imm(a,4) &amp; 0xffff)));
 328   } else if (is_lis(*(p+1))) {
 329     x |= (((unsigned long) (get_imm(a,2) &amp; 0xffff)) &lt;&lt; 32);
 330     x |= (((unsigned long) (get_imm(a,1) &amp; 0xffff)) &lt;&lt; 16);
 331     x |= (((unsigned long) (get_imm(a,3) &amp; 0xffff)));
 332   } else {
 333     ShouldNotReachHere();
 334     return (long) 0;
 335   }
 336   return (long) x;
 337 }
 338 
 339 // Patch the 64 bit constant of a `load_const&#39; sequence. This is a low
 340 // level procedure. It neither flushes the instruction cache nor is it
 341 // mt safe.
 342 void MacroAssembler::patch_const(address a, long x) {
 343   assert(is_load_const_at(a), &quot;not a load of a constant&quot;);
 344   int *p = (int*) a;
 345   if (is_ori(*(p+1))) {
 346     set_imm(0 + p, (x &gt;&gt; 48) &amp; 0xffff);
 347     set_imm(1 + p, (x &gt;&gt; 32) &amp; 0xffff);
 348     set_imm(3 + p, (x &gt;&gt; 16) &amp; 0xffff);
 349     set_imm(4 + p, x &amp; 0xffff);
 350   } else if (is_lis(*(p+1))) {
 351     set_imm(0 + p, (x &gt;&gt; 48) &amp; 0xffff);
 352     set_imm(2 + p, (x &gt;&gt; 32) &amp; 0xffff);
 353     set_imm(1 + p, (x &gt;&gt; 16) &amp; 0xffff);
 354     set_imm(3 + p, x &amp; 0xffff);
 355   } else {
 356     ShouldNotReachHere();
 357   }
 358 }
 359 
 360 AddressLiteral MacroAssembler::allocate_metadata_address(Metadata* obj) {
 361   assert(oop_recorder() != NULL, &quot;this assembler needs a Recorder&quot;);
 362   int index = oop_recorder()-&gt;allocate_metadata_index(obj);
 363   RelocationHolder rspec = metadata_Relocation::spec(index);
 364   return AddressLiteral((address)obj, rspec);
 365 }
 366 
 367 AddressLiteral MacroAssembler::constant_metadata_address(Metadata* obj) {
 368   assert(oop_recorder() != NULL, &quot;this assembler needs a Recorder&quot;);
 369   int index = oop_recorder()-&gt;find_index(obj);
 370   RelocationHolder rspec = metadata_Relocation::spec(index);
 371   return AddressLiteral((address)obj, rspec);
 372 }
 373 
 374 AddressLiteral MacroAssembler::allocate_oop_address(jobject obj) {
 375   assert(oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 376   int oop_index = oop_recorder()-&gt;allocate_oop_index(obj);
 377   return AddressLiteral(address(obj), oop_Relocation::spec(oop_index));
 378 }
 379 
 380 AddressLiteral MacroAssembler::constant_oop_address(jobject obj) {
 381   assert(oop_recorder() != NULL, &quot;this assembler needs an OopRecorder&quot;);
 382   int oop_index = oop_recorder()-&gt;find_index(obj);
 383   return AddressLiteral(address(obj), oop_Relocation::spec(oop_index));
 384 }
 385 
 386 RegisterOrConstant MacroAssembler::delayed_value_impl(intptr_t* delayed_value_addr,
 387                                                       Register tmp, int offset) {
 388   intptr_t value = *delayed_value_addr;
 389   if (value != 0) {
 390     return RegisterOrConstant(value + offset);
 391   }
 392 
 393   // Load indirectly to solve generation ordering problem.
 394   // static address, no relocation
 395   int simm16_offset = load_const_optimized(tmp, delayed_value_addr, noreg, true);
 396   ld(tmp, simm16_offset, tmp); // must be aligned ((xa &amp; 3) == 0)
 397 
 398   if (offset != 0) {
 399     addi(tmp, tmp, offset);
 400   }
 401 
 402   return RegisterOrConstant(tmp);
 403 }
 404 
 405 #ifndef PRODUCT
 406 void MacroAssembler::pd_print_patched_instruction(address branch) {
 407   Unimplemented(); // TODO: PPC port
 408 }
 409 #endif // ndef PRODUCT
 410 
 411 // Conditional far branch for destinations encodable in 24+2 bits.
 412 void MacroAssembler::bc_far(int boint, int biint, Label&amp; dest, int optimize) {
 413 
 414   // If requested by flag optimize, relocate the bc_far as a
 415   // runtime_call and prepare for optimizing it when the code gets
 416   // relocated.
 417   if (optimize == bc_far_optimize_on_relocate) {
 418     relocate(relocInfo::runtime_call_type);
 419   }
 420 
 421   // variant 2:
 422   //
 423   //    b!cxx SKIP
 424   //    bxx   DEST
 425   //  SKIP:
 426   //
 427 
 428   const int opposite_boint = add_bhint_to_boint(opposite_bhint(inv_boint_bhint(boint)),
 429                                                 opposite_bcond(inv_boint_bcond(boint)));
 430 
 431   // We emit two branches.
 432   // First, a conditional branch which jumps around the far branch.
 433   const address not_taken_pc = pc() + 2 * BytesPerInstWord;
 434   const address bc_pc        = pc();
 435   bc(opposite_boint, biint, not_taken_pc);
 436 
 437   const int bc_instr = *(int*)bc_pc;
 438   assert(not_taken_pc == (address)inv_bd_field(bc_instr, (intptr_t)bc_pc), &quot;postcondition&quot;);
 439   assert(opposite_boint == inv_bo_field(bc_instr), &quot;postcondition&quot;);
 440   assert(boint == add_bhint_to_boint(opposite_bhint(inv_boint_bhint(inv_bo_field(bc_instr))),
 441                                      opposite_bcond(inv_boint_bcond(inv_bo_field(bc_instr)))),
 442          &quot;postcondition&quot;);
 443   assert(biint == inv_bi_field(bc_instr), &quot;postcondition&quot;);
 444 
 445   // Second, an unconditional far branch which jumps to dest.
 446   // Note: target(dest) remembers the current pc (see CodeSection::target)
 447   //       and returns the current pc if the label is not bound yet; when
 448   //       the label gets bound, the unconditional far branch will be patched.
 449   const address target_pc = target(dest);
 450   const address b_pc  = pc();
 451   b(target_pc);
 452 
 453   assert(not_taken_pc == pc(),                     &quot;postcondition&quot;);
 454   assert(dest.is_bound() || target_pc == b_pc, &quot;postcondition&quot;);
 455 }
 456 
 457 // 1 or 2 instructions
 458 void MacroAssembler::bc_far_optimized(int boint, int biint, Label&amp; dest) {
 459   if (dest.is_bound() &amp;&amp; is_within_range_of_bcxx(target(dest), pc())) {
 460     bc(boint, biint, dest);
 461   } else {
 462     bc_far(boint, biint, dest, MacroAssembler::bc_far_optimize_on_relocate);
 463   }
 464 }
 465 
 466 bool MacroAssembler::is_bc_far_at(address instruction_addr) {
 467   return is_bc_far_variant1_at(instruction_addr) ||
 468          is_bc_far_variant2_at(instruction_addr) ||
 469          is_bc_far_variant3_at(instruction_addr);
 470 }
 471 
 472 address MacroAssembler::get_dest_of_bc_far_at(address instruction_addr) {
 473   if (is_bc_far_variant1_at(instruction_addr)) {
 474     const address instruction_1_addr = instruction_addr;
 475     const int instruction_1 = *(int*)instruction_1_addr;
 476     return (address)inv_bd_field(instruction_1, (intptr_t)instruction_1_addr);
 477   } else if (is_bc_far_variant2_at(instruction_addr)) {
 478     const address instruction_2_addr = instruction_addr + 4;
 479     return bxx_destination(instruction_2_addr);
 480   } else if (is_bc_far_variant3_at(instruction_addr)) {
 481     return instruction_addr + 8;
 482   }
 483   // variant 4 ???
 484   ShouldNotReachHere();
 485   return NULL;
 486 }
 487 void MacroAssembler::set_dest_of_bc_far_at(address instruction_addr, address dest) {
 488 
 489   if (is_bc_far_variant3_at(instruction_addr)) {
 490     // variant 3, far cond branch to the next instruction, already patched to nops:
 491     //
 492     //    nop
 493     //    endgroup
 494     //  SKIP/DEST:
 495     //
 496     return;
 497   }
 498 
 499   // first, extract boint and biint from the current branch
 500   int boint = 0;
 501   int biint = 0;
 502 
 503   ResourceMark rm;
 504   const int code_size = 2 * BytesPerInstWord;
 505   CodeBuffer buf(instruction_addr, code_size);
 506   MacroAssembler masm(&amp;buf);
 507   if (is_bc_far_variant2_at(instruction_addr) &amp;&amp; dest == instruction_addr + 8) {
 508     // Far branch to next instruction: Optimize it by patching nops (produce variant 3).
 509     masm.nop();
 510     masm.endgroup();
 511   } else {
 512     if (is_bc_far_variant1_at(instruction_addr)) {
 513       // variant 1, the 1st instruction contains the destination address:
 514       //
 515       //    bcxx  DEST
 516       //    nop
 517       //
 518       const int instruction_1 = *(int*)(instruction_addr);
 519       boint = inv_bo_field(instruction_1);
 520       biint = inv_bi_field(instruction_1);
 521     } else if (is_bc_far_variant2_at(instruction_addr)) {
 522       // variant 2, the 2nd instruction contains the destination address:
 523       //
 524       //    b!cxx SKIP
 525       //    bxx   DEST
 526       //  SKIP:
 527       //
 528       const int instruction_1 = *(int*)(instruction_addr);
 529       boint = add_bhint_to_boint(opposite_bhint(inv_boint_bhint(inv_bo_field(instruction_1))),
 530           opposite_bcond(inv_boint_bcond(inv_bo_field(instruction_1))));
 531       biint = inv_bi_field(instruction_1);
 532     } else {
 533       // variant 4???
 534       ShouldNotReachHere();
 535     }
 536 
 537     // second, set the new branch destination and optimize the code
 538     if (dest != instruction_addr + 4 &amp;&amp; // the bc_far is still unbound!
 539         masm.is_within_range_of_bcxx(dest, instruction_addr)) {
 540       // variant 1:
 541       //
 542       //    bcxx  DEST
 543       //    nop
 544       //
 545       masm.bc(boint, biint, dest);
 546       masm.nop();
 547     } else {
 548       // variant 2:
 549       //
 550       //    b!cxx SKIP
 551       //    bxx   DEST
 552       //  SKIP:
 553       //
 554       const int opposite_boint = add_bhint_to_boint(opposite_bhint(inv_boint_bhint(boint)),
 555                                                     opposite_bcond(inv_boint_bcond(boint)));
 556       const address not_taken_pc = masm.pc() + 2 * BytesPerInstWord;
 557       masm.bc(opposite_boint, biint, not_taken_pc);
 558       masm.b(dest);
 559     }
 560   }
 561   ICache::ppc64_flush_icache_bytes(instruction_addr, code_size);
 562 }
 563 
 564 // Emit a NOT mt-safe patchable 64 bit absolute call/jump.
 565 void MacroAssembler::bxx64_patchable(address dest, relocInfo::relocType rt, bool link) {
 566   // get current pc
 567   uint64_t start_pc = (uint64_t) pc();
 568 
 569   const address pc_of_bl = (address) (start_pc + (6*BytesPerInstWord)); // bl is last
 570   const address pc_of_b  = (address) (start_pc + (0*BytesPerInstWord)); // b is first
 571 
 572   // relocate here
 573   if (rt != relocInfo::none) {
 574     relocate(rt);
 575   }
 576 
 577   if ( ReoptimizeCallSequences &amp;&amp;
 578        (( link &amp;&amp; is_within_range_of_b(dest, pc_of_bl)) ||
 579         (!link &amp;&amp; is_within_range_of_b(dest, pc_of_b)))) {
 580     // variant 2:
 581     // Emit an optimized, pc-relative call/jump.
 582 
 583     if (link) {
 584       // some padding
 585       nop();
 586       nop();
 587       nop();
 588       nop();
 589       nop();
 590       nop();
 591 
 592       // do the call
 593       assert(pc() == pc_of_bl, &quot;just checking&quot;);
 594       bl(dest, relocInfo::none);
 595     } else {
 596       // do the jump
 597       assert(pc() == pc_of_b, &quot;just checking&quot;);
 598       b(dest, relocInfo::none);
 599 
 600       // some padding
 601       nop();
 602       nop();
 603       nop();
 604       nop();
 605       nop();
 606       nop();
 607     }
 608 
 609     // Assert that we can identify the emitted call/jump.
 610     assert(is_bxx64_patchable_variant2_at((address)start_pc, link),
 611            &quot;can&#39;t identify emitted call&quot;);
 612   } else {
 613     // variant 1:
 614     mr(R0, R11);  // spill R11 -&gt; R0.
 615 
 616     // Load the destination address into CTR,
 617     // calculate destination relative to global toc.
 618     calculate_address_from_global_toc(R11, dest, true, true, false);
 619 
 620     mtctr(R11);
 621     mr(R11, R0);  // spill R11 &lt;- R0.
 622     nop();
 623 
 624     // do the call/jump
 625     if (link) {
 626       bctrl();
 627     } else{
 628       bctr();
 629     }
 630     // Assert that we can identify the emitted call/jump.
 631     assert(is_bxx64_patchable_variant1b_at((address)start_pc, link),
 632            &quot;can&#39;t identify emitted call&quot;);
 633   }
 634 
 635   // Assert that we can identify the emitted call/jump.
 636   assert(is_bxx64_patchable_at((address)start_pc, link),
 637          &quot;can&#39;t identify emitted call&quot;);
 638   assert(get_dest_of_bxx64_patchable_at((address)start_pc, link) == dest,
 639          &quot;wrong encoding of dest address&quot;);
 640 }
 641 
 642 // Identify a bxx64_patchable instruction.
 643 bool MacroAssembler::is_bxx64_patchable_at(address instruction_addr, bool link) {
 644   return is_bxx64_patchable_variant1b_at(instruction_addr, link)
 645     //|| is_bxx64_patchable_variant1_at(instruction_addr, link)
 646       || is_bxx64_patchable_variant2_at(instruction_addr, link);
 647 }
 648 
 649 // Does the call64_patchable instruction use a pc-relative encoding of
 650 // the call destination?
 651 bool MacroAssembler::is_bxx64_patchable_pcrelative_at(address instruction_addr, bool link) {
 652   // variant 2 is pc-relative
 653   return is_bxx64_patchable_variant2_at(instruction_addr, link);
 654 }
 655 
 656 // Identify variant 1.
 657 bool MacroAssembler::is_bxx64_patchable_variant1_at(address instruction_addr, bool link) {
 658   unsigned int* instr = (unsigned int*) instruction_addr;
 659   return (link ? is_bctrl(instr[6]) : is_bctr(instr[6])) // bctr[l]
 660       &amp;&amp; is_mtctr(instr[5]) // mtctr
 661     &amp;&amp; is_load_const_at(instruction_addr);
 662 }
 663 
 664 // Identify variant 1b: load destination relative to global toc.
 665 bool MacroAssembler::is_bxx64_patchable_variant1b_at(address instruction_addr, bool link) {
 666   unsigned int* instr = (unsigned int*) instruction_addr;
 667   return (link ? is_bctrl(instr[6]) : is_bctr(instr[6])) // bctr[l]
 668     &amp;&amp; is_mtctr(instr[3]) // mtctr
 669     &amp;&amp; is_calculate_address_from_global_toc_at(instruction_addr + 2*BytesPerInstWord, instruction_addr);
 670 }
 671 
 672 // Identify variant 2.
 673 bool MacroAssembler::is_bxx64_patchable_variant2_at(address instruction_addr, bool link) {
 674   unsigned int* instr = (unsigned int*) instruction_addr;
 675   if (link) {
 676     return is_bl (instr[6])  // bl dest is last
 677       &amp;&amp; is_nop(instr[0])  // nop
 678       &amp;&amp; is_nop(instr[1])  // nop
 679       &amp;&amp; is_nop(instr[2])  // nop
 680       &amp;&amp; is_nop(instr[3])  // nop
 681       &amp;&amp; is_nop(instr[4])  // nop
 682       &amp;&amp; is_nop(instr[5]); // nop
 683   } else {
 684     return is_b  (instr[0])  // b  dest is first
 685       &amp;&amp; is_nop(instr[1])  // nop
 686       &amp;&amp; is_nop(instr[2])  // nop
 687       &amp;&amp; is_nop(instr[3])  // nop
 688       &amp;&amp; is_nop(instr[4])  // nop
 689       &amp;&amp; is_nop(instr[5])  // nop
 690       &amp;&amp; is_nop(instr[6]); // nop
 691   }
 692 }
 693 
 694 // Set dest address of a bxx64_patchable instruction.
 695 void MacroAssembler::set_dest_of_bxx64_patchable_at(address instruction_addr, address dest, bool link) {
 696   ResourceMark rm;
 697   int code_size = MacroAssembler::bxx64_patchable_size;
 698   CodeBuffer buf(instruction_addr, code_size);
 699   MacroAssembler masm(&amp;buf);
 700   masm.bxx64_patchable(dest, relocInfo::none, link);
 701   ICache::ppc64_flush_icache_bytes(instruction_addr, code_size);
 702 }
 703 
 704 // Get dest address of a bxx64_patchable instruction.
 705 address MacroAssembler::get_dest_of_bxx64_patchable_at(address instruction_addr, bool link) {
 706   if (is_bxx64_patchable_variant1_at(instruction_addr, link)) {
 707     return (address) (unsigned long) get_const(instruction_addr);
 708   } else if (is_bxx64_patchable_variant2_at(instruction_addr, link)) {
 709     unsigned int* instr = (unsigned int*) instruction_addr;
 710     if (link) {
 711       const int instr_idx = 6; // bl is last
 712       int branchoffset = branch_destination(instr[instr_idx], 0);
 713       return instruction_addr + branchoffset + instr_idx*BytesPerInstWord;
 714     } else {
 715       const int instr_idx = 0; // b is first
 716       int branchoffset = branch_destination(instr[instr_idx], 0);
 717       return instruction_addr + branchoffset + instr_idx*BytesPerInstWord;
 718     }
 719   // Load dest relative to global toc.
 720   } else if (is_bxx64_patchable_variant1b_at(instruction_addr, link)) {
 721     return get_address_of_calculate_address_from_global_toc_at(instruction_addr + 2*BytesPerInstWord,
 722                                                                instruction_addr);
 723   } else {
 724     ShouldNotReachHere();
 725     return NULL;
 726   }
 727 }
 728 
 729 // Uses ordering which corresponds to ABI:
 730 //    _savegpr0_14:  std  r14,-144(r1)
 731 //    _savegpr0_15:  std  r15,-136(r1)
 732 //    _savegpr0_16:  std  r16,-128(r1)
 733 void MacroAssembler::save_nonvolatile_gprs(Register dst, int offset) {
 734   std(R14, offset, dst);   offset += 8;
 735   std(R15, offset, dst);   offset += 8;
 736   std(R16, offset, dst);   offset += 8;
 737   std(R17, offset, dst);   offset += 8;
 738   std(R18, offset, dst);   offset += 8;
 739   std(R19, offset, dst);   offset += 8;
 740   std(R20, offset, dst);   offset += 8;
 741   std(R21, offset, dst);   offset += 8;
 742   std(R22, offset, dst);   offset += 8;
 743   std(R23, offset, dst);   offset += 8;
 744   std(R24, offset, dst);   offset += 8;
 745   std(R25, offset, dst);   offset += 8;
 746   std(R26, offset, dst);   offset += 8;
 747   std(R27, offset, dst);   offset += 8;
 748   std(R28, offset, dst);   offset += 8;
 749   std(R29, offset, dst);   offset += 8;
 750   std(R30, offset, dst);   offset += 8;
 751   std(R31, offset, dst);   offset += 8;
 752 
 753   stfd(F14, offset, dst);   offset += 8;
 754   stfd(F15, offset, dst);   offset += 8;
 755   stfd(F16, offset, dst);   offset += 8;
 756   stfd(F17, offset, dst);   offset += 8;
 757   stfd(F18, offset, dst);   offset += 8;
 758   stfd(F19, offset, dst);   offset += 8;
 759   stfd(F20, offset, dst);   offset += 8;
 760   stfd(F21, offset, dst);   offset += 8;
 761   stfd(F22, offset, dst);   offset += 8;
 762   stfd(F23, offset, dst);   offset += 8;
 763   stfd(F24, offset, dst);   offset += 8;
 764   stfd(F25, offset, dst);   offset += 8;
 765   stfd(F26, offset, dst);   offset += 8;
 766   stfd(F27, offset, dst);   offset += 8;
 767   stfd(F28, offset, dst);   offset += 8;
 768   stfd(F29, offset, dst);   offset += 8;
 769   stfd(F30, offset, dst);   offset += 8;
 770   stfd(F31, offset, dst);
 771 }
 772 
 773 // Uses ordering which corresponds to ABI:
 774 //    _restgpr0_14:  ld   r14,-144(r1)
 775 //    _restgpr0_15:  ld   r15,-136(r1)
 776 //    _restgpr0_16:  ld   r16,-128(r1)
 777 void MacroAssembler::restore_nonvolatile_gprs(Register src, int offset) {
 778   ld(R14, offset, src);   offset += 8;
 779   ld(R15, offset, src);   offset += 8;
 780   ld(R16, offset, src);   offset += 8;
 781   ld(R17, offset, src);   offset += 8;
 782   ld(R18, offset, src);   offset += 8;
 783   ld(R19, offset, src);   offset += 8;
 784   ld(R20, offset, src);   offset += 8;
 785   ld(R21, offset, src);   offset += 8;
 786   ld(R22, offset, src);   offset += 8;
 787   ld(R23, offset, src);   offset += 8;
 788   ld(R24, offset, src);   offset += 8;
 789   ld(R25, offset, src);   offset += 8;
 790   ld(R26, offset, src);   offset += 8;
 791   ld(R27, offset, src);   offset += 8;
 792   ld(R28, offset, src);   offset += 8;
 793   ld(R29, offset, src);   offset += 8;
 794   ld(R30, offset, src);   offset += 8;
 795   ld(R31, offset, src);   offset += 8;
 796 
 797   // FP registers
 798   lfd(F14, offset, src);   offset += 8;
 799   lfd(F15, offset, src);   offset += 8;
 800   lfd(F16, offset, src);   offset += 8;
 801   lfd(F17, offset, src);   offset += 8;
 802   lfd(F18, offset, src);   offset += 8;
 803   lfd(F19, offset, src);   offset += 8;
 804   lfd(F20, offset, src);   offset += 8;
 805   lfd(F21, offset, src);   offset += 8;
 806   lfd(F22, offset, src);   offset += 8;
 807   lfd(F23, offset, src);   offset += 8;
 808   lfd(F24, offset, src);   offset += 8;
 809   lfd(F25, offset, src);   offset += 8;
 810   lfd(F26, offset, src);   offset += 8;
 811   lfd(F27, offset, src);   offset += 8;
 812   lfd(F28, offset, src);   offset += 8;
 813   lfd(F29, offset, src);   offset += 8;
 814   lfd(F30, offset, src);   offset += 8;
 815   lfd(F31, offset, src);
 816 }
 817 
 818 // For verify_oops.
 819 void MacroAssembler::save_volatile_gprs(Register dst, int offset) {
 820   std(R2,  offset, dst);   offset += 8;
 821   std(R3,  offset, dst);   offset += 8;
 822   std(R4,  offset, dst);   offset += 8;
 823   std(R5,  offset, dst);   offset += 8;
 824   std(R6,  offset, dst);   offset += 8;
 825   std(R7,  offset, dst);   offset += 8;
 826   std(R8,  offset, dst);   offset += 8;
 827   std(R9,  offset, dst);   offset += 8;
 828   std(R10, offset, dst);   offset += 8;
 829   std(R11, offset, dst);   offset += 8;
 830   std(R12, offset, dst);   offset += 8;
 831 
 832   stfd(F0, offset, dst);   offset += 8;
 833   stfd(F1, offset, dst);   offset += 8;
 834   stfd(F2, offset, dst);   offset += 8;
 835   stfd(F3, offset, dst);   offset += 8;
 836   stfd(F4, offset, dst);   offset += 8;
 837   stfd(F5, offset, dst);   offset += 8;
 838   stfd(F6, offset, dst);   offset += 8;
 839   stfd(F7, offset, dst);   offset += 8;
 840   stfd(F8, offset, dst);   offset += 8;
 841   stfd(F9, offset, dst);   offset += 8;
 842   stfd(F10, offset, dst);  offset += 8;
 843   stfd(F11, offset, dst);  offset += 8;
 844   stfd(F12, offset, dst);  offset += 8;
 845   stfd(F13, offset, dst);
 846 }
 847 
 848 // For verify_oops.
 849 void MacroAssembler::restore_volatile_gprs(Register src, int offset) {
 850   ld(R2,  offset, src);   offset += 8;
 851   ld(R3,  offset, src);   offset += 8;
 852   ld(R4,  offset, src);   offset += 8;
 853   ld(R5,  offset, src);   offset += 8;
 854   ld(R6,  offset, src);   offset += 8;
 855   ld(R7,  offset, src);   offset += 8;
 856   ld(R8,  offset, src);   offset += 8;
 857   ld(R9,  offset, src);   offset += 8;
 858   ld(R10, offset, src);   offset += 8;
 859   ld(R11, offset, src);   offset += 8;
 860   ld(R12, offset, src);   offset += 8;
 861 
 862   lfd(F0, offset, src);   offset += 8;
 863   lfd(F1, offset, src);   offset += 8;
 864   lfd(F2, offset, src);   offset += 8;
 865   lfd(F3, offset, src);   offset += 8;
 866   lfd(F4, offset, src);   offset += 8;
 867   lfd(F5, offset, src);   offset += 8;
 868   lfd(F6, offset, src);   offset += 8;
 869   lfd(F7, offset, src);   offset += 8;
 870   lfd(F8, offset, src);   offset += 8;
 871   lfd(F9, offset, src);   offset += 8;
 872   lfd(F10, offset, src);  offset += 8;
 873   lfd(F11, offset, src);  offset += 8;
 874   lfd(F12, offset, src);  offset += 8;
 875   lfd(F13, offset, src);
 876 }
 877 
 878 void MacroAssembler::save_LR_CR(Register tmp) {
 879   mfcr(tmp);
 880   std(tmp, _abi(cr), R1_SP);
 881   mflr(tmp);
 882   std(tmp, _abi(lr), R1_SP);
 883   // Tmp must contain lr on exit! (see return_addr and prolog in ppc64.ad)
 884 }
 885 
 886 void MacroAssembler::restore_LR_CR(Register tmp) {
 887   assert(tmp != R1_SP, &quot;must be distinct&quot;);
 888   ld(tmp, _abi(lr), R1_SP);
 889   mtlr(tmp);
 890   ld(tmp, _abi(cr), R1_SP);
 891   mtcr(tmp);
 892 }
 893 
 894 address MacroAssembler::get_PC_trash_LR(Register result) {
 895   Label L;
 896   bl(L);
 897   bind(L);
 898   address lr_pc = pc();
 899   mflr(result);
 900   return lr_pc;
 901 }
 902 
 903 void MacroAssembler::resize_frame(Register offset, Register tmp) {
 904 #ifdef ASSERT
 905   assert_different_registers(offset, tmp, R1_SP);
 906   andi_(tmp, offset, frame::alignment_in_bytes-1);
 907   asm_assert_eq(&quot;resize_frame: unaligned&quot;, 0x204);
 908 #endif
 909 
 910   // tmp &lt;- *(SP)
 911   ld(tmp, _abi(callers_sp), R1_SP);
 912   // addr &lt;- SP + offset;
 913   // *(addr) &lt;- tmp;
 914   // SP &lt;- addr
 915   stdux(tmp, R1_SP, offset);
 916 }
 917 
 918 void MacroAssembler::resize_frame(int offset, Register tmp) {
 919   assert(is_simm(offset, 16), &quot;too big an offset&quot;);
 920   assert_different_registers(tmp, R1_SP);
 921   assert((offset &amp; (frame::alignment_in_bytes-1))==0, &quot;resize_frame: unaligned&quot;);
 922   // tmp &lt;- *(SP)
 923   ld(tmp, _abi(callers_sp), R1_SP);
 924   // addr &lt;- SP + offset;
 925   // *(addr) &lt;- tmp;
 926   // SP &lt;- addr
 927   stdu(tmp, offset, R1_SP);
 928 }
 929 
 930 void MacroAssembler::resize_frame_absolute(Register addr, Register tmp1, Register tmp2) {
 931   // (addr == tmp1) || (addr == tmp2) is allowed here!
 932   assert(tmp1 != tmp2, &quot;must be distinct&quot;);
 933 
 934   // compute offset w.r.t. current stack pointer
 935   // tmp_1 &lt;- addr - SP (!)
 936   subf(tmp1, R1_SP, addr);
 937 
 938   // atomically update SP keeping back link.
 939   resize_frame(tmp1/* offset */, tmp2/* tmp */);
 940 }
 941 
 942 void MacroAssembler::push_frame(Register bytes, Register tmp) {
 943 #ifdef ASSERT
 944   assert(bytes != R0, &quot;r0 not allowed here&quot;);
 945   andi_(R0, bytes, frame::alignment_in_bytes-1);
 946   asm_assert_eq(&quot;push_frame(Reg, Reg): unaligned&quot;, 0x203);
 947 #endif
 948   neg(tmp, bytes);
 949   stdux(R1_SP, R1_SP, tmp);
 950 }
 951 
 952 // Push a frame of size `bytes&#39;.
 953 void MacroAssembler::push_frame(unsigned int bytes, Register tmp) {
 954   long offset = align_addr(bytes, frame::alignment_in_bytes);
 955   if (is_simm(-offset, 16)) {
 956     stdu(R1_SP, -offset, R1_SP);
 957   } else {
 958     load_const_optimized(tmp, -offset);
 959     stdux(R1_SP, R1_SP, tmp);
 960   }
 961 }
 962 
 963 // Push a frame of size `bytes&#39; plus abi_reg_args on top.
 964 void MacroAssembler::push_frame_reg_args(unsigned int bytes, Register tmp) {
 965   push_frame(bytes + frame::abi_reg_args_size, tmp);
 966 }
 967 
 968 // Setup up a new C frame with a spill area for non-volatile GPRs and
 969 // additional space for local variables.
 970 void MacroAssembler::push_frame_reg_args_nonvolatiles(unsigned int bytes,
 971                                                       Register tmp) {
 972   push_frame(bytes + frame::abi_reg_args_size + frame::spill_nonvolatiles_size, tmp);
 973 }
 974 
 975 // Pop current C frame.
 976 void MacroAssembler::pop_frame() {
 977   ld(R1_SP, _abi(callers_sp), R1_SP);
 978 }
 979 
 980 #if defined(ABI_ELFv2)
 981 address MacroAssembler::branch_to(Register r_function_entry, bool and_link) {
 982   // TODO(asmundak): make sure the caller uses R12 as function descriptor
 983   // most of the times.
 984   if (R12 != r_function_entry) {
 985     mr(R12, r_function_entry);
 986   }
 987   mtctr(R12);
 988   // Do a call or a branch.
 989   if (and_link) {
 990     bctrl();
 991   } else {
 992     bctr();
 993   }
 994   _last_calls_return_pc = pc();
 995 
 996   return _last_calls_return_pc;
 997 }
 998 
 999 // Call a C function via a function descriptor and use full C
1000 // calling conventions. Updates and returns _last_calls_return_pc.
1001 address MacroAssembler::call_c(Register r_function_entry) {
1002   return branch_to(r_function_entry, /*and_link=*/true);
1003 }
1004 
1005 // For tail calls: only branch, don&#39;t link, so callee returns to caller of this function.
1006 address MacroAssembler::call_c_and_return_to_caller(Register r_function_entry) {
1007   return branch_to(r_function_entry, /*and_link=*/false);
1008 }
1009 
1010 address MacroAssembler::call_c(address function_entry, relocInfo::relocType rt) {
1011   load_const(R12, function_entry, R0);
1012   return branch_to(R12,  /*and_link=*/true);
1013 }
1014 
1015 #else
1016 // Generic version of a call to C function via a function descriptor
1017 // with variable support for C calling conventions (TOC, ENV, etc.).
1018 // Updates and returns _last_calls_return_pc.
1019 address MacroAssembler::branch_to(Register function_descriptor, bool and_link, bool save_toc_before_call,
1020                                   bool restore_toc_after_call, bool load_toc_of_callee, bool load_env_of_callee) {
1021   // we emit standard ptrgl glue code here
1022   assert((function_descriptor != R0), &quot;function_descriptor cannot be R0&quot;);
1023 
1024   // retrieve necessary entries from the function descriptor
1025   ld(R0, in_bytes(FunctionDescriptor::entry_offset()), function_descriptor);
1026   mtctr(R0);
1027 
1028   if (load_toc_of_callee) {
1029     ld(R2_TOC, in_bytes(FunctionDescriptor::toc_offset()), function_descriptor);
1030   }
1031   if (load_env_of_callee) {
1032     ld(R11, in_bytes(FunctionDescriptor::env_offset()), function_descriptor);
1033   } else if (load_toc_of_callee) {
1034     li(R11, 0);
1035   }
1036 
1037   // do a call or a branch
1038   if (and_link) {
1039     bctrl();
1040   } else {
1041     bctr();
1042   }
1043   _last_calls_return_pc = pc();
1044 
1045   return _last_calls_return_pc;
1046 }
1047 
1048 // Call a C function via a function descriptor and use full C calling
1049 // conventions.
1050 // We don&#39;t use the TOC in generated code, so there is no need to save
1051 // and restore its value.
1052 address MacroAssembler::call_c(Register fd) {
1053   return branch_to(fd, /*and_link=*/true,
1054                        /*save toc=*/false,
1055                        /*restore toc=*/false,
1056                        /*load toc=*/true,
1057                        /*load env=*/true);
1058 }
1059 
1060 address MacroAssembler::call_c_and_return_to_caller(Register fd) {
1061   return branch_to(fd, /*and_link=*/false,
1062                        /*save toc=*/false,
1063                        /*restore toc=*/false,
1064                        /*load toc=*/true,
1065                        /*load env=*/true);
1066 }
1067 
1068 address MacroAssembler::call_c(const FunctionDescriptor* fd, relocInfo::relocType rt) {
1069   if (rt != relocInfo::none) {
1070     // this call needs to be relocatable
1071     if (!ReoptimizeCallSequences
1072         || (rt != relocInfo::runtime_call_type &amp;&amp; rt != relocInfo::none)
1073         || fd == NULL   // support code-size estimation
1074         || !fd-&gt;is_friend_function()
1075         || fd-&gt;entry() == NULL) {
1076       // it&#39;s not a friend function as defined by class FunctionDescriptor,
1077       // so do a full call-c here.
1078       load_const(R11, (address)fd, R0);
1079 
1080       bool has_env = (fd != NULL &amp;&amp; fd-&gt;env() != NULL);
1081       return branch_to(R11, /*and_link=*/true,
1082                             /*save toc=*/false,
1083                             /*restore toc=*/false,
1084                             /*load toc=*/true,
1085                             /*load env=*/has_env);
1086     } else {
1087       // It&#39;s a friend function. Load the entry point and don&#39;t care about
1088       // toc and env. Use an optimizable call instruction, but ensure the
1089       // same code-size as in the case of a non-friend function.
1090       nop();
1091       nop();
1092       nop();
1093       bl64_patchable(fd-&gt;entry(), rt);
1094       _last_calls_return_pc = pc();
1095       return _last_calls_return_pc;
1096     }
1097   } else {
1098     // This call does not need to be relocatable, do more aggressive
1099     // optimizations.
1100     if (!ReoptimizeCallSequences
1101       || !fd-&gt;is_friend_function()) {
1102       // It&#39;s not a friend function as defined by class FunctionDescriptor,
1103       // so do a full call-c here.
1104       load_const(R11, (address)fd, R0);
1105       return branch_to(R11, /*and_link=*/true,
1106                             /*save toc=*/false,
1107                             /*restore toc=*/false,
1108                             /*load toc=*/true,
1109                             /*load env=*/true);
1110     } else {
1111       // it&#39;s a friend function, load the entry point and don&#39;t care about
1112       // toc and env.
1113       address dest = fd-&gt;entry();
1114       if (is_within_range_of_b(dest, pc())) {
1115         bl(dest);
1116       } else {
1117         bl64_patchable(dest, rt);
1118       }
1119       _last_calls_return_pc = pc();
1120       return _last_calls_return_pc;
1121     }
1122   }
1123 }
1124 
1125 // Call a C function.  All constants needed reside in TOC.
1126 //
1127 // Read the address to call from the TOC.
1128 // Read env from TOC, if fd specifies an env.
1129 // Read new TOC from TOC.
1130 address MacroAssembler::call_c_using_toc(const FunctionDescriptor* fd,
1131                                          relocInfo::relocType rt, Register toc) {
1132   if (!ReoptimizeCallSequences
1133     || (rt != relocInfo::runtime_call_type &amp;&amp; rt != relocInfo::none)
1134     || !fd-&gt;is_friend_function()) {
1135     // It&#39;s not a friend function as defined by class FunctionDescriptor,
1136     // so do a full call-c here.
1137     assert(fd-&gt;entry() != NULL, &quot;function must be linked&quot;);
1138 
1139     AddressLiteral fd_entry(fd-&gt;entry());
1140     bool success = load_const_from_method_toc(R11, fd_entry, toc, /*fixed_size*/ true);
1141     mtctr(R11);
1142     if (fd-&gt;env() == NULL) {
1143       li(R11, 0);
1144       nop();
1145     } else {
1146       AddressLiteral fd_env(fd-&gt;env());
1147       success = success &amp;&amp; load_const_from_method_toc(R11, fd_env, toc, /*fixed_size*/ true);
1148     }
1149     AddressLiteral fd_toc(fd-&gt;toc());
1150     // Set R2_TOC (load from toc)
1151     success = success &amp;&amp; load_const_from_method_toc(R2_TOC, fd_toc, toc, /*fixed_size*/ true);
1152     bctrl();
1153     _last_calls_return_pc = pc();
1154     if (!success) { return NULL; }
1155   } else {
1156     // It&#39;s a friend function, load the entry point and don&#39;t care about
1157     // toc and env. Use an optimizable call instruction, but ensure the
1158     // same code-size as in the case of a non-friend function.
1159     nop();
1160     bl64_patchable(fd-&gt;entry(), rt);
1161     _last_calls_return_pc = pc();
1162   }
1163   return _last_calls_return_pc;
1164 }
1165 #endif // ABI_ELFv2
1166 
1167 void MacroAssembler::call_VM_base(Register oop_result,
1168                                   Register last_java_sp,
1169                                   address  entry_point,
1170                                   bool     check_exceptions) {
1171   BLOCK_COMMENT(&quot;call_VM {&quot;);
1172   // Determine last_java_sp register.
1173   if (!last_java_sp-&gt;is_valid()) {
1174     last_java_sp = R1_SP;
1175   }
1176   set_top_ijava_frame_at_SP_as_last_Java_frame(last_java_sp, R11_scratch1);
1177 
1178   // ARG1 must hold thread address.
1179   mr(R3_ARG1, R16_thread);
1180 #if defined(ABI_ELFv2)
1181   address return_pc = call_c(entry_point, relocInfo::none);
1182 #else
1183   address return_pc = call_c((FunctionDescriptor*)entry_point, relocInfo::none);
1184 #endif
1185 
1186   reset_last_Java_frame();
1187 
1188   // Check for pending exceptions.
1189   if (check_exceptions) {
1190     // We don&#39;t check for exceptions here.
1191     ShouldNotReachHere();
1192   }
1193 
1194   // Get oop result if there is one and reset the value in the thread.
1195   if (oop_result-&gt;is_valid()) {
1196     get_vm_result(oop_result);
1197   }
1198 
1199   _last_calls_return_pc = return_pc;
1200   BLOCK_COMMENT(&quot;} call_VM&quot;);
1201 }
1202 
1203 void MacroAssembler::call_VM_leaf_base(address entry_point) {
1204   BLOCK_COMMENT(&quot;call_VM_leaf {&quot;);
1205 #if defined(ABI_ELFv2)
1206   call_c(entry_point, relocInfo::none);
1207 #else
1208   call_c(CAST_FROM_FN_PTR(FunctionDescriptor*, entry_point), relocInfo::none);
1209 #endif
1210   BLOCK_COMMENT(&quot;} call_VM_leaf&quot;);
1211 }
1212 
1213 void MacroAssembler::call_VM(Register oop_result, address entry_point, bool check_exceptions) {
1214   call_VM_base(oop_result, noreg, entry_point, check_exceptions);
1215 }
1216 
1217 void MacroAssembler::call_VM(Register oop_result, address entry_point, Register arg_1,
1218                              bool check_exceptions) {
1219   // R3_ARG1 is reserved for the thread.
1220   mr_if_needed(R4_ARG2, arg_1);
1221   call_VM(oop_result, entry_point, check_exceptions);
1222 }
1223 
1224 void MacroAssembler::call_VM(Register oop_result, address entry_point, Register arg_1, Register arg_2,
1225                              bool check_exceptions) {
1226   // R3_ARG1 is reserved for the thread
1227   mr_if_needed(R4_ARG2, arg_1);
1228   assert(arg_2 != R4_ARG2, &quot;smashed argument&quot;);
1229   mr_if_needed(R5_ARG3, arg_2);
1230   call_VM(oop_result, entry_point, check_exceptions);
1231 }
1232 
1233 void MacroAssembler::call_VM(Register oop_result, address entry_point, Register arg_1, Register arg_2, Register arg_3,
1234                              bool check_exceptions) {
1235   // R3_ARG1 is reserved for the thread
1236   mr_if_needed(R4_ARG2, arg_1);
1237   assert(arg_2 != R4_ARG2, &quot;smashed argument&quot;);
1238   mr_if_needed(R5_ARG3, arg_2);
1239   mr_if_needed(R6_ARG4, arg_3);
1240   call_VM(oop_result, entry_point, check_exceptions);
1241 }
1242 
1243 void MacroAssembler::call_VM_leaf(address entry_point) {
1244   call_VM_leaf_base(entry_point);
1245 }
1246 
1247 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_1) {
1248   mr_if_needed(R3_ARG1, arg_1);
1249   call_VM_leaf(entry_point);
1250 }
1251 
1252 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_1, Register arg_2) {
1253   mr_if_needed(R3_ARG1, arg_1);
1254   assert(arg_2 != R3_ARG1, &quot;smashed argument&quot;);
1255   mr_if_needed(R4_ARG2, arg_2);
1256   call_VM_leaf(entry_point);
1257 }
1258 
1259 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_1, Register arg_2, Register arg_3) {
1260   mr_if_needed(R3_ARG1, arg_1);
1261   assert(arg_2 != R3_ARG1, &quot;smashed argument&quot;);
1262   mr_if_needed(R4_ARG2, arg_2);
1263   assert(arg_3 != R3_ARG1 &amp;&amp; arg_3 != R4_ARG2, &quot;smashed argument&quot;);
1264   mr_if_needed(R5_ARG3, arg_3);
1265   call_VM_leaf(entry_point);
1266 }
1267 
1268 // Check whether instruction is a read access to the polling page
1269 // which was emitted by load_from_polling_page(..).
1270 bool MacroAssembler::is_load_from_polling_page(int instruction, void* ucontext,
1271                                                address* polling_address_ptr) {
1272   if (!is_ld(instruction))
1273     return false; // It&#39;s not a ld. Fail.
1274 
1275   int rt = inv_rt_field(instruction);
1276   int ra = inv_ra_field(instruction);
1277   int ds = inv_ds_field(instruction);
1278   if (!(ds == 0 &amp;&amp; ra != 0 &amp;&amp; rt == 0)) {
1279     return false; // It&#39;s not a ld(r0, X, ra). Fail.
1280   }
1281 
1282   if (!ucontext) {
1283     // Set polling address.
1284     if (polling_address_ptr != NULL) {
1285       *polling_address_ptr = NULL;
1286     }
1287     return true; // No ucontext given. Can&#39;t check value of ra. Assume true.
1288   }
1289 
1290 #ifdef LINUX
1291   // Ucontext given. Check that register ra contains the address of
1292   // the safepoing polling page.
1293   ucontext_t* uc = (ucontext_t*) ucontext;
1294   // Set polling address.
1295   address addr = (address)uc-&gt;uc_mcontext.regs-&gt;gpr[ra] + (ssize_t)ds;
1296   if (polling_address_ptr != NULL) {
1297     *polling_address_ptr = addr;
1298   }
1299   return os::is_poll_address(addr);
1300 #else
1301   // Not on Linux, ucontext must be NULL.
1302   ShouldNotReachHere();
1303   return false;
1304 #endif
1305 }
1306 
1307 void MacroAssembler::bang_stack_with_offset(int offset) {
1308   // When increasing the stack, the old stack pointer will be written
1309   // to the new top of stack according to the PPC64 abi.
1310   // Therefore, stack banging is not necessary when increasing
1311   // the stack by &lt;= os::vm_page_size() bytes.
1312   // When increasing the stack by a larger amount, this method is
1313   // called repeatedly to bang the intermediate pages.
1314 
1315   // Stack grows down, caller passes positive offset.
1316   assert(offset &gt; 0, &quot;must bang with positive offset&quot;);
1317 
1318   long stdoffset = -offset;
1319 
1320   if (is_simm(stdoffset, 16)) {
1321     // Signed 16 bit offset, a simple std is ok.
1322     if (UseLoadInstructionsForStackBangingPPC64) {
1323       ld(R0, (int)(signed short)stdoffset, R1_SP);
1324     } else {
1325       std(R0,(int)(signed short)stdoffset, R1_SP);
1326     }
1327   } else if (is_simm(stdoffset, 31)) {
1328     const int hi = MacroAssembler::largeoffset_si16_si16_hi(stdoffset);
1329     const int lo = MacroAssembler::largeoffset_si16_si16_lo(stdoffset);
1330 
1331     Register tmp = R11;
1332     addis(tmp, R1_SP, hi);
1333     if (UseLoadInstructionsForStackBangingPPC64) {
1334       ld(R0,  lo, tmp);
1335     } else {
1336       std(R0, lo, tmp);
1337     }
1338   } else {
1339     ShouldNotReachHere();
1340   }
1341 }
1342 
1343 // If instruction is a stack bang of the form
1344 //    std    R0,    x(Ry),       (see bang_stack_with_offset())
1345 //    stdu   R1_SP, x(R1_SP),    (see push_frame(), resize_frame())
1346 // or stdux  R1_SP, Rx, R1_SP    (see push_frame(), resize_frame())
1347 // return the banged address. Otherwise, return 0.
1348 address MacroAssembler::get_stack_bang_address(int instruction, void *ucontext) {
1349 #ifdef LINUX
1350   ucontext_t* uc = (ucontext_t*) ucontext;
1351   int rs = inv_rs_field(instruction);
1352   int ra = inv_ra_field(instruction);
1353   if (   (is_ld(instruction)   &amp;&amp; rs == 0 &amp;&amp;  UseLoadInstructionsForStackBangingPPC64)
1354       || (is_std(instruction)  &amp;&amp; rs == 0 &amp;&amp; !UseLoadInstructionsForStackBangingPPC64)
1355       || (is_stdu(instruction) &amp;&amp; rs == 1)) {
1356     int ds = inv_ds_field(instruction);
1357     // return banged address
1358     return ds+(address)uc-&gt;uc_mcontext.regs-&gt;gpr[ra];
1359   } else if (is_stdux(instruction) &amp;&amp; rs == 1) {
1360     int rb = inv_rb_field(instruction);
1361     address sp = (address)uc-&gt;uc_mcontext.regs-&gt;gpr[1];
1362     long rb_val = (long)uc-&gt;uc_mcontext.regs-&gt;gpr[rb];
1363     return ra != 1 || rb_val &gt;= 0 ? NULL         // not a stack bang
1364                                   : sp + rb_val; // banged address
1365   }
1366   return NULL; // not a stack bang
1367 #else
1368   // workaround not needed on !LINUX :-)
1369   ShouldNotCallThis();
1370   return NULL;
1371 #endif
1372 }
1373 
1374 void MacroAssembler::reserved_stack_check(Register return_pc) {
1375   // Test if reserved zone needs to be enabled.
1376   Label no_reserved_zone_enabling;
1377 
1378   ld_ptr(R0, JavaThread::reserved_stack_activation_offset(), R16_thread);
1379   cmpld(CCR0, R1_SP, R0);
1380   blt_predict_taken(CCR0, no_reserved_zone_enabling);
1381 
1382   // Enable reserved zone again, throw stack overflow exception.
1383   push_frame_reg_args(0, R0);
1384   call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone), R16_thread);
1385   pop_frame();
1386   mtlr(return_pc);
1387   load_const_optimized(R0, StubRoutines::throw_delayed_StackOverflowError_entry());
1388   mtctr(R0);
1389   bctr();
1390 
1391   should_not_reach_here();
1392 
1393   bind(no_reserved_zone_enabling);
1394 }
1395 
1396 void MacroAssembler::getandsetd(Register dest_current_value, Register exchange_value, Register addr_base,
1397                                 bool cmpxchgx_hint) {
1398   Label retry;
1399   bind(retry);
1400   ldarx(dest_current_value, addr_base, cmpxchgx_hint);
1401   stdcx_(exchange_value, addr_base);
1402   if (UseStaticBranchPredictionInCompareAndSwapPPC64) {
1403     bne_predict_not_taken(CCR0, retry); // StXcx_ sets CCR0.
1404   } else {
1405     bne(                  CCR0, retry); // StXcx_ sets CCR0.
1406   }
1407 }
1408 
1409 void MacroAssembler::getandaddd(Register dest_current_value, Register inc_value, Register addr_base,
1410                                 Register tmp, bool cmpxchgx_hint) {
1411   Label retry;
1412   bind(retry);
1413   ldarx(dest_current_value, addr_base, cmpxchgx_hint);
1414   add(tmp, dest_current_value, inc_value);
1415   stdcx_(tmp, addr_base);
1416   if (UseStaticBranchPredictionInCompareAndSwapPPC64) {
1417     bne_predict_not_taken(CCR0, retry); // StXcx_ sets CCR0.
1418   } else {
1419     bne(                  CCR0, retry); // StXcx_ sets CCR0.
1420   }
1421 }
1422 
1423 // Word/sub-word atomic helper functions
1424 
1425 // Temps and addr_base are killed if size &lt; 4 and processor does not support respective instructions.
1426 // Only signed types are supported with size &lt; 4.
1427 // Atomic add always kills tmp1.
1428 void MacroAssembler::atomic_get_and_modify_generic(Register dest_current_value, Register exchange_value,
1429                                                    Register addr_base, Register tmp1, Register tmp2, Register tmp3,
1430                                                    bool cmpxchgx_hint, bool is_add, int size) {
1431   // Sub-word instructions are available since Power 8.
1432   // For older processors, instruction_type != size holds, and we
1433   // emulate the sub-word instructions by constructing a 4-byte value
1434   // that leaves the other bytes unchanged.
1435   const int instruction_type = VM_Version::has_lqarx() ? size : 4;
1436 
1437   Label retry;
1438   Register shift_amount = noreg,
1439            val32 = dest_current_value,
1440            modval = is_add ? tmp1 : exchange_value;
1441 
1442   if (instruction_type != size) {
1443     assert_different_registers(tmp1, tmp2, tmp3, dest_current_value, exchange_value, addr_base);
1444     modval = tmp1;
1445     shift_amount = tmp2;
1446     val32 = tmp3;
1447     // Need some preperation: Compute shift amount, align address. Note: shorts must be 2 byte aligned.
1448 #ifdef VM_LITTLE_ENDIAN
1449     rldic(shift_amount, addr_base, 3, 64-5); // (dest &amp; 3) * 8;
1450     clrrdi(addr_base, addr_base, 2);
1451 #else
1452     xori(shift_amount, addr_base, (size == 1) ? 3 : 2);
1453     clrrdi(addr_base, addr_base, 2);
1454     rldic(shift_amount, shift_amount, 3, 64-5); // byte: ((3-dest) &amp; 3) * 8; short: ((1-dest/2) &amp; 1) * 16;
1455 #endif
1456   }
1457 
1458   // atomic emulation loop
1459   bind(retry);
1460 
1461   switch (instruction_type) {
1462     case 4: lwarx(val32, addr_base, cmpxchgx_hint); break;
1463     case 2: lharx(val32, addr_base, cmpxchgx_hint); break;
1464     case 1: lbarx(val32, addr_base, cmpxchgx_hint); break;
1465     default: ShouldNotReachHere();
1466   }
1467 
1468   if (instruction_type != size) {
1469     srw(dest_current_value, val32, shift_amount);
1470   }
1471 
1472   if (is_add) { add(modval, dest_current_value, exchange_value); }
1473 
1474   if (instruction_type != size) {
1475     // Transform exchange value such that the replacement can be done by one xor instruction.
1476     xorr(modval, dest_current_value, is_add ? modval : exchange_value);
1477     clrldi(modval, modval, (size == 1) ? 56 : 48);
1478     slw(modval, modval, shift_amount);
1479     xorr(modval, val32, modval);
1480   }
1481 
1482   switch (instruction_type) {
1483     case 4: stwcx_(modval, addr_base); break;
1484     case 2: sthcx_(modval, addr_base); break;
1485     case 1: stbcx_(modval, addr_base); break;
1486     default: ShouldNotReachHere();
1487   }
1488 
1489   if (UseStaticBranchPredictionInCompareAndSwapPPC64) {
1490     bne_predict_not_taken(CCR0, retry); // StXcx_ sets CCR0.
1491   } else {
1492     bne(                  CCR0, retry); // StXcx_ sets CCR0.
1493   }
1494 
1495   // l?arx zero-extends, but Java wants byte/short values sign-extended.
1496   if (size == 1) {
1497     extsb(dest_current_value, dest_current_value);
1498   } else if (size == 2) {
1499     extsh(dest_current_value, dest_current_value);
1500   };
1501 }
1502 
1503 // Temps, addr_base and exchange_value are killed if size &lt; 4 and processor does not support respective instructions.
1504 // Only signed types are supported with size &lt; 4.
1505 void MacroAssembler::cmpxchg_loop_body(ConditionRegister flag, Register dest_current_value,
1506                                        Register compare_value, Register exchange_value,
1507                                        Register addr_base, Register tmp1, Register tmp2,
1508                                        Label &amp;retry, Label &amp;failed, bool cmpxchgx_hint, int size) {
1509   // Sub-word instructions are available since Power 8.
1510   // For older processors, instruction_type != size holds, and we
1511   // emulate the sub-word instructions by constructing a 4-byte value
1512   // that leaves the other bytes unchanged.
1513   const int instruction_type = VM_Version::has_lqarx() ? size : 4;
1514 
1515   Register shift_amount = noreg,
1516            val32 = dest_current_value,
1517            modval = exchange_value;
1518 
1519   if (instruction_type != size) {
1520     assert_different_registers(tmp1, tmp2, dest_current_value, compare_value, exchange_value, addr_base);
1521     shift_amount = tmp1;
1522     val32 = tmp2;
1523     modval = tmp2;
1524     // Need some preperation: Compute shift amount, align address. Note: shorts must be 2 byte aligned.
1525 #ifdef VM_LITTLE_ENDIAN
1526     rldic(shift_amount, addr_base, 3, 64-5); // (dest &amp; 3) * 8;
1527     clrrdi(addr_base, addr_base, 2);
1528 #else
1529     xori(shift_amount, addr_base, (size == 1) ? 3 : 2);
1530     clrrdi(addr_base, addr_base, 2);
1531     rldic(shift_amount, shift_amount, 3, 64-5); // byte: ((3-dest) &amp; 3) * 8; short: ((1-dest/2) &amp; 1) * 16;
1532 #endif
1533     // Transform exchange value such that the replacement can be done by one xor instruction.
1534     xorr(exchange_value, compare_value, exchange_value);
1535     clrldi(exchange_value, exchange_value, (size == 1) ? 56 : 48);
1536     slw(exchange_value, exchange_value, shift_amount);
1537   }
1538 
1539   // atomic emulation loop
1540   bind(retry);
1541 
1542   switch (instruction_type) {
1543     case 4: lwarx(val32, addr_base, cmpxchgx_hint); break;
1544     case 2: lharx(val32, addr_base, cmpxchgx_hint); break;
1545     case 1: lbarx(val32, addr_base, cmpxchgx_hint); break;
1546     default: ShouldNotReachHere();
1547   }
1548 
1549   if (instruction_type != size) {
1550     srw(dest_current_value, val32, shift_amount);
1551   }
1552   if (size == 1) {
1553     extsb(dest_current_value, dest_current_value);
1554   } else if (size == 2) {
1555     extsh(dest_current_value, dest_current_value);
1556   };
1557 
1558   cmpw(flag, dest_current_value, compare_value);
1559   if (UseStaticBranchPredictionInCompareAndSwapPPC64) {
1560     bne_predict_not_taken(flag, failed);
1561   } else {
1562     bne(                  flag, failed);
1563   }
1564   // branch to done  =&gt; (flag == ne), (dest_current_value != compare_value)
1565   // fall through    =&gt; (flag == eq), (dest_current_value == compare_value)
1566 
1567   if (instruction_type != size) {
1568     xorr(modval, val32, exchange_value);
1569   }
1570 
1571   switch (instruction_type) {
1572     case 4: stwcx_(modval, addr_base); break;
1573     case 2: sthcx_(modval, addr_base); break;
1574     case 1: stbcx_(modval, addr_base); break;
1575     default: ShouldNotReachHere();
1576   }
1577 }
1578 
1579 // CmpxchgX sets condition register to cmpX(current, compare).
1580 void MacroAssembler::cmpxchg_generic(ConditionRegister flag, Register dest_current_value,
1581                                      Register compare_value, Register exchange_value,
1582                                      Register addr_base, Register tmp1, Register tmp2,
1583                                      int semantics, bool cmpxchgx_hint,
1584                                      Register int_flag_success, bool contention_hint, bool weak, int size) {
1585   Label retry;
1586   Label failed;
1587   Label done;
1588 
1589   // Save one branch if result is returned via register and
1590   // result register is different from the other ones.
1591   bool use_result_reg    = (int_flag_success != noreg);
1592   bool preset_result_reg = (int_flag_success != dest_current_value &amp;&amp; int_flag_success != compare_value &amp;&amp;
1593                             int_flag_success != exchange_value &amp;&amp; int_flag_success != addr_base &amp;&amp;
1594                             int_flag_success != tmp1 &amp;&amp; int_flag_success != tmp2);
1595   assert(!weak || flag == CCR0, &quot;weak only supported with CCR0&quot;);
1596   assert(size == 1 || size == 2 || size == 4, &quot;unsupported&quot;);
1597 
1598   if (use_result_reg &amp;&amp; preset_result_reg) {
1599     li(int_flag_success, 0); // preset (assume cas failed)
1600   }
1601 
1602   // Add simple guard in order to reduce risk of starving under high contention (recommended by IBM).
1603   if (contention_hint) { // Don&#39;t try to reserve if cmp fails.
1604     switch (size) {
1605       case 1: lbz(dest_current_value, 0, addr_base); extsb(dest_current_value, dest_current_value); break;
1606       case 2: lha(dest_current_value, 0, addr_base); break;
1607       case 4: lwz(dest_current_value, 0, addr_base); break;
1608       default: ShouldNotReachHere();
1609     }
1610     cmpw(flag, dest_current_value, compare_value);
1611     bne(flag, failed);
1612   }
1613 
1614   // release/fence semantics
1615   if (semantics &amp; MemBarRel) {
1616     release();
1617   }
1618 
1619   cmpxchg_loop_body(flag, dest_current_value, compare_value, exchange_value, addr_base, tmp1, tmp2,
1620                     retry, failed, cmpxchgx_hint, size);
1621   if (!weak || use_result_reg) {
1622     if (UseStaticBranchPredictionInCompareAndSwapPPC64) {
1623       bne_predict_not_taken(CCR0, weak ? failed : retry); // StXcx_ sets CCR0.
1624     } else {
1625       bne(                  CCR0, weak ? failed : retry); // StXcx_ sets CCR0.
1626     }
1627   }
1628   // fall through    =&gt; (flag == eq), (dest_current_value == compare_value), (swapped)
1629 
1630   // Result in register (must do this at the end because int_flag_success can be the
1631   // same register as one above).
1632   if (use_result_reg) {
1633     li(int_flag_success, 1);
1634   }
1635 
1636   if (semantics &amp; MemBarFenceAfter) {
1637     fence();
1638   } else if (semantics &amp; MemBarAcq) {
1639     isync();
1640   }
1641 
1642   if (use_result_reg &amp;&amp; !preset_result_reg) {
1643     b(done);
1644   }
1645 
1646   bind(failed);
1647   if (use_result_reg &amp;&amp; !preset_result_reg) {
1648     li(int_flag_success, 0);
1649   }
1650 
1651   bind(done);
1652   // (flag == ne) =&gt; (dest_current_value != compare_value), (!swapped)
1653   // (flag == eq) =&gt; (dest_current_value == compare_value), ( swapped)
1654 }
1655 
1656 // Preforms atomic compare exchange:
1657 //   if (compare_value == *addr_base)
1658 //     *addr_base = exchange_value
1659 //     int_flag_success = 1;
1660 //   else
1661 //     int_flag_success = 0;
1662 //
1663 // ConditionRegister flag       = cmp(compare_value, *addr_base)
1664 // Register dest_current_value  = *addr_base
1665 // Register compare_value       Used to compare with value in memory
1666 // Register exchange_value      Written to memory if compare_value == *addr_base
1667 // Register addr_base           The memory location to compareXChange
1668 // Register int_flag_success    Set to 1 if exchange_value was written to *addr_base
1669 //
1670 // To avoid the costly compare exchange the value is tested beforehand.
1671 // Several special cases exist to avoid that unnecessary information is generated.
1672 //
1673 void MacroAssembler::cmpxchgd(ConditionRegister flag,
1674                               Register dest_current_value, RegisterOrConstant compare_value, Register exchange_value,
1675                               Register addr_base, int semantics, bool cmpxchgx_hint,
1676                               Register int_flag_success, Label* failed_ext, bool contention_hint, bool weak) {
1677   Label retry;
1678   Label failed_int;
1679   Label&amp; failed = (failed_ext != NULL) ? *failed_ext : failed_int;
1680   Label done;
1681 
1682   // Save one branch if result is returned via register and result register is different from the other ones.
1683   bool use_result_reg    = (int_flag_success!=noreg);
1684   bool preset_result_reg = (int_flag_success!=dest_current_value &amp;&amp; int_flag_success!=compare_value.register_or_noreg() &amp;&amp;
1685                             int_flag_success!=exchange_value &amp;&amp; int_flag_success!=addr_base);
1686   assert(!weak || flag == CCR0, &quot;weak only supported with CCR0&quot;);
1687   assert(int_flag_success == noreg || failed_ext == NULL, &quot;cannot have both&quot;);
1688 
1689   if (use_result_reg &amp;&amp; preset_result_reg) {
1690     li(int_flag_success, 0); // preset (assume cas failed)
1691   }
1692 
1693   // Add simple guard in order to reduce risk of starving under high contention (recommended by IBM).
1694   if (contention_hint) { // Don&#39;t try to reserve if cmp fails.
1695     ld(dest_current_value, 0, addr_base);
1696     cmpd(flag, compare_value, dest_current_value);
1697     bne(flag, failed);
1698   }
1699 
1700   // release/fence semantics
1701   if (semantics &amp; MemBarRel) {
1702     release();
1703   }
1704 
1705   // atomic emulation loop
1706   bind(retry);
1707 
1708   ldarx(dest_current_value, addr_base, cmpxchgx_hint);
1709   cmpd(flag, compare_value, dest_current_value);
1710   if (UseStaticBranchPredictionInCompareAndSwapPPC64) {
1711     bne_predict_not_taken(flag, failed);
1712   } else {
1713     bne(                  flag, failed);
1714   }
1715 
1716   stdcx_(exchange_value, addr_base);
1717   if (!weak || use_result_reg || failed_ext) {
1718     if (UseStaticBranchPredictionInCompareAndSwapPPC64) {
1719       bne_predict_not_taken(CCR0, weak ? failed : retry); // stXcx_ sets CCR0
1720     } else {
1721       bne(                  CCR0, weak ? failed : retry); // stXcx_ sets CCR0
1722     }
1723   }
1724 
1725   // result in register (must do this at the end because int_flag_success can be the same register as one above)
1726   if (use_result_reg) {
1727     li(int_flag_success, 1);
1728   }
1729 
1730   if (semantics &amp; MemBarFenceAfter) {
1731     fence();
1732   } else if (semantics &amp; MemBarAcq) {
1733     isync();
1734   }
1735 
1736   if (use_result_reg &amp;&amp; !preset_result_reg) {
1737     b(done);
1738   }
1739 
1740   bind(failed_int);
1741   if (use_result_reg &amp;&amp; !preset_result_reg) {
1742     li(int_flag_success, 0);
1743   }
1744 
1745   bind(done);
1746   // (flag == ne) =&gt; (dest_current_value != compare_value), (!swapped)
1747   // (flag == eq) =&gt; (dest_current_value == compare_value), ( swapped)
1748 }
1749 
1750 // Look up the method for a megamorphic invokeinterface call.
1751 // The target method is determined by &lt;intf_klass, itable_index&gt;.
1752 // The receiver klass is in recv_klass.
1753 // On success, the result will be in method_result, and execution falls through.
1754 // On failure, execution transfers to the given label.
1755 void MacroAssembler::lookup_interface_method(Register recv_klass,
1756                                              Register intf_klass,
1757                                              RegisterOrConstant itable_index,
1758                                              Register method_result,
1759                                              Register scan_temp,
1760                                              Register temp2,
1761                                              Label&amp; L_no_such_interface,
1762                                              bool return_method) {
1763   assert_different_registers(recv_klass, intf_klass, method_result, scan_temp);
1764 
1765   // Compute start of first itableOffsetEntry (which is at the end of the vtable).
1766   int vtable_base = in_bytes(Klass::vtable_start_offset());
1767   int itentry_off = itableMethodEntry::method_offset_in_bytes();
1768   int logMEsize   = exact_log2(itableMethodEntry::size() * wordSize);
1769   int scan_step   = itableOffsetEntry::size() * wordSize;
1770   int log_vte_size= exact_log2(vtableEntry::size_in_bytes());
1771 
1772   lwz(scan_temp, in_bytes(Klass::vtable_length_offset()), recv_klass);
1773   // %%% We should store the aligned, prescaled offset in the klassoop.
1774   // Then the next several instructions would fold away.
1775 
1776   sldi(scan_temp, scan_temp, log_vte_size);
1777   addi(scan_temp, scan_temp, vtable_base);
1778   add(scan_temp, recv_klass, scan_temp);
1779 
1780   // Adjust recv_klass by scaled itable_index, so we can free itable_index.
1781   if (return_method) {
1782     if (itable_index.is_register()) {
1783       Register itable_offset = itable_index.as_register();
1784       sldi(method_result, itable_offset, logMEsize);
1785       if (itentry_off) { addi(method_result, method_result, itentry_off); }
1786       add(method_result, method_result, recv_klass);
1787     } else {
1788       long itable_offset = (long)itable_index.as_constant();
1789       // static address, no relocation
1790       add_const_optimized(method_result, recv_klass, (itable_offset &lt;&lt; logMEsize) + itentry_off, temp2);
1791     }
1792   }
1793 
1794   // for (scan = klass-&gt;itable(); scan-&gt;interface() != NULL; scan += scan_step) {
1795   //   if (scan-&gt;interface() == intf) {
1796   //     result = (klass + scan-&gt;offset() + itable_index);
1797   //   }
1798   // }
1799   Label search, found_method;
1800 
1801   for (int peel = 1; peel &gt;= 0; peel--) {
1802     // %%%% Could load both offset and interface in one ldx, if they were
1803     // in the opposite order. This would save a load.
1804     ld(temp2, itableOffsetEntry::interface_offset_in_bytes(), scan_temp);
1805 
1806     // Check that this entry is non-null. A null entry means that
1807     // the receiver class doesn&#39;t implement the interface, and wasn&#39;t the
1808     // same as when the caller was compiled.
1809     cmpd(CCR0, temp2, intf_klass);
1810 
1811     if (peel) {
1812       beq(CCR0, found_method);
1813     } else {
1814       bne(CCR0, search);
1815       // (invert the test to fall through to found_method...)
1816     }
1817 
1818     if (!peel) break;
1819 
1820     bind(search);
1821 
1822     cmpdi(CCR0, temp2, 0);
1823     beq(CCR0, L_no_such_interface);
1824     addi(scan_temp, scan_temp, scan_step);
1825   }
1826 
1827   bind(found_method);
1828 
1829   // Got a hit.
1830   if (return_method) {
1831     int ito_offset = itableOffsetEntry::offset_offset_in_bytes();
1832     lwz(scan_temp, ito_offset, scan_temp);
1833     ldx(method_result, scan_temp, method_result);
1834   }
1835 }
1836 
1837 // virtual method calling
1838 void MacroAssembler::lookup_virtual_method(Register recv_klass,
1839                                            RegisterOrConstant vtable_index,
1840                                            Register method_result) {
1841 
1842   assert_different_registers(recv_klass, method_result, vtable_index.register_or_noreg());
1843 
1844   const int base = in_bytes(Klass::vtable_start_offset());
1845   assert(vtableEntry::size() * wordSize == wordSize, &quot;adjust the scaling in the code below&quot;);
1846 
1847   if (vtable_index.is_register()) {
1848     sldi(vtable_index.as_register(), vtable_index.as_register(), LogBytesPerWord);
1849     add(recv_klass, vtable_index.as_register(), recv_klass);
1850   } else {
1851     addi(recv_klass, recv_klass, vtable_index.as_constant() &lt;&lt; LogBytesPerWord);
1852   }
1853   ld(R19_method, base + vtableEntry::method_offset_in_bytes(), recv_klass);
1854 }
1855 
1856 /////////////////////////////////////////// subtype checking ////////////////////////////////////////////
1857 void MacroAssembler::check_klass_subtype_fast_path(Register sub_klass,
1858                                                    Register super_klass,
1859                                                    Register temp1_reg,
1860                                                    Register temp2_reg,
1861                                                    Label* L_success,
1862                                                    Label* L_failure,
1863                                                    Label* L_slow_path,
1864                                                    RegisterOrConstant super_check_offset) {
1865 
1866   const Register check_cache_offset = temp1_reg;
1867   const Register cached_super       = temp2_reg;
1868 
1869   assert_different_registers(sub_klass, super_klass, check_cache_offset, cached_super);
1870 
1871   int sco_offset = in_bytes(Klass::super_check_offset_offset());
1872   int sc_offset  = in_bytes(Klass::secondary_super_cache_offset());
1873 
1874   bool must_load_sco = (super_check_offset.constant_or_zero() == -1);
1875   bool need_slow_path = (must_load_sco || super_check_offset.constant_or_zero() == sco_offset);
1876 
1877   Label L_fallthrough;
1878   int label_nulls = 0;
1879   if (L_success == NULL)   { L_success   = &amp;L_fallthrough; label_nulls++; }
1880   if (L_failure == NULL)   { L_failure   = &amp;L_fallthrough; label_nulls++; }
1881   if (L_slow_path == NULL) { L_slow_path = &amp;L_fallthrough; label_nulls++; }
1882   assert(label_nulls &lt;= 1 ||
1883          (L_slow_path == &amp;L_fallthrough &amp;&amp; label_nulls &lt;= 2 &amp;&amp; !need_slow_path),
1884          &quot;at most one NULL in the batch, usually&quot;);
1885 
1886   // If the pointers are equal, we are done (e.g., String[] elements).
1887   // This self-check enables sharing of secondary supertype arrays among
1888   // non-primary types such as array-of-interface. Otherwise, each such
1889   // type would need its own customized SSA.
1890   // We move this check to the front of the fast path because many
1891   // type checks are in fact trivially successful in this manner,
1892   // so we get a nicely predicted branch right at the start of the check.
1893   cmpd(CCR0, sub_klass, super_klass);
1894   beq(CCR0, *L_success);
1895 
1896   // Check the supertype display:
1897   if (must_load_sco) {
1898     // The super check offset is always positive...
1899     lwz(check_cache_offset, sco_offset, super_klass);
1900     super_check_offset = RegisterOrConstant(check_cache_offset);
1901     // super_check_offset is register.
1902     assert_different_registers(sub_klass, super_klass, cached_super, super_check_offset.as_register());
1903   }
1904   // The loaded value is the offset from KlassOopDesc.
1905 
1906   ld(cached_super, super_check_offset, sub_klass);
1907   cmpd(CCR0, cached_super, super_klass);
1908 
1909   // This check has worked decisively for primary supers.
1910   // Secondary supers are sought in the super_cache (&#39;super_cache_addr&#39;).
1911   // (Secondary supers are interfaces and very deeply nested subtypes.)
1912   // This works in the same check above because of a tricky aliasing
1913   // between the super_cache and the primary super display elements.
1914   // (The &#39;super_check_addr&#39; can address either, as the case requires.)
1915   // Note that the cache is updated below if it does not help us find
1916   // what we need immediately.
1917   // So if it was a primary super, we can just fail immediately.
1918   // Otherwise, it&#39;s the slow path for us (no success at this point).
1919 
1920 #define FINAL_JUMP(label) if (&amp;(label) != &amp;L_fallthrough) { b(label); }
1921 
1922   if (super_check_offset.is_register()) {
1923     beq(CCR0, *L_success);
1924     cmpwi(CCR0, super_check_offset.as_register(), sc_offset);
1925     if (L_failure == &amp;L_fallthrough) {
1926       beq(CCR0, *L_slow_path);
1927     } else {
1928       bne(CCR0, *L_failure);
1929       FINAL_JUMP(*L_slow_path);
1930     }
1931   } else {
1932     if (super_check_offset.as_constant() == sc_offset) {
1933       // Need a slow path; fast failure is impossible.
1934       if (L_slow_path == &amp;L_fallthrough) {
1935         beq(CCR0, *L_success);
1936       } else {
1937         bne(CCR0, *L_slow_path);
1938         FINAL_JUMP(*L_success);
1939       }
1940     } else {
1941       // No slow path; it&#39;s a fast decision.
1942       if (L_failure == &amp;L_fallthrough) {
1943         beq(CCR0, *L_success);
1944       } else {
1945         bne(CCR0, *L_failure);
1946         FINAL_JUMP(*L_success);
1947       }
1948     }
1949   }
1950 
1951   bind(L_fallthrough);
1952 #undef FINAL_JUMP
1953 }
1954 
1955 void MacroAssembler::check_klass_subtype_slow_path(Register sub_klass,
1956                                                    Register super_klass,
1957                                                    Register temp1_reg,
1958                                                    Register temp2_reg,
1959                                                    Label* L_success,
1960                                                    Register result_reg) {
1961   const Register array_ptr = temp1_reg; // current value from cache array
1962   const Register temp      = temp2_reg;
1963 
1964   assert_different_registers(sub_klass, super_klass, array_ptr, temp);
1965 
1966   int source_offset = in_bytes(Klass::secondary_supers_offset());
1967   int target_offset = in_bytes(Klass::secondary_super_cache_offset());
1968 
1969   int length_offset = Array&lt;Klass*&gt;::length_offset_in_bytes();
1970   int base_offset   = Array&lt;Klass*&gt;::base_offset_in_bytes();
1971 
1972   Label hit, loop, failure, fallthru;
1973 
1974   ld(array_ptr, source_offset, sub_klass);
1975 
1976   // TODO: PPC port: assert(4 == arrayOopDesc::length_length_in_bytes(), &quot;precondition violated.&quot;);
1977   lwz(temp, length_offset, array_ptr);
1978   cmpwi(CCR0, temp, 0);
1979   beq(CCR0, result_reg!=noreg ? failure : fallthru); // length 0
1980 
1981   mtctr(temp); // load ctr
1982 
1983   bind(loop);
1984   // Oops in table are NO MORE compressed.
1985   ld(temp, base_offset, array_ptr);
1986   cmpd(CCR0, temp, super_klass);
1987   beq(CCR0, hit);
1988   addi(array_ptr, array_ptr, BytesPerWord);
1989   bdnz(loop);
1990 
1991   bind(failure);
1992   if (result_reg!=noreg) li(result_reg, 1); // load non-zero result (indicates a miss)
1993   b(fallthru);
1994 
1995   bind(hit);
1996   std(super_klass, target_offset, sub_klass); // save result to cache
1997   if (result_reg != noreg) { li(result_reg, 0); } // load zero result (indicates a hit)
1998   if (L_success != NULL) { b(*L_success); }
1999   else if (result_reg == noreg) { blr(); } // return with CR0.eq if neither label nor result reg provided
2000 
2001   bind(fallthru);
2002 }
2003 
2004 // Try fast path, then go to slow one if not successful
2005 void MacroAssembler::check_klass_subtype(Register sub_klass,
2006                          Register super_klass,
2007                          Register temp1_reg,
2008                          Register temp2_reg,
2009                          Label&amp; L_success) {
2010   Label L_failure;
2011   check_klass_subtype_fast_path(sub_klass, super_klass, temp1_reg, temp2_reg, &amp;L_success, &amp;L_failure);
2012   check_klass_subtype_slow_path(sub_klass, super_klass, temp1_reg, temp2_reg, &amp;L_success);
2013   bind(L_failure); // Fallthru if not successful.
2014 }
2015 
2016 void MacroAssembler::clinit_barrier(Register klass, Register thread, Label* L_fast_path, Label* L_slow_path) {
2017   assert(L_fast_path != NULL || L_slow_path != NULL, &quot;at least one is required&quot;);
2018 
2019   Label L_fallthrough;
2020   if (L_fast_path == NULL) {
2021     L_fast_path = &amp;L_fallthrough;
2022   } else if (L_slow_path == NULL) {
2023     L_slow_path = &amp;L_fallthrough;
2024   }
2025 
2026   // Fast path check: class is fully initialized
2027   lbz(R0, in_bytes(InstanceKlass::init_state_offset()), klass);
2028   cmpwi(CCR0, R0, InstanceKlass::fully_initialized);
2029   beq(CCR0, *L_fast_path);
2030 
2031   // Fast path check: current thread is initializer thread
2032   ld(R0, in_bytes(InstanceKlass::init_thread_offset()), klass);
2033   cmpd(CCR0, thread, R0);
2034   if (L_slow_path == &amp;L_fallthrough) {
2035     beq(CCR0, *L_fast_path);
2036   } else if (L_fast_path == &amp;L_fallthrough) {
2037     bne(CCR0, *L_slow_path);
2038   } else {
2039     Unimplemented();
2040   }
2041 
2042   bind(L_fallthrough);
2043 }
2044 
2045 RegisterOrConstant MacroAssembler::argument_offset(RegisterOrConstant arg_slot,
2046                                                    Register temp_reg,
2047                                                    int extra_slot_offset) {
2048   // cf. TemplateTable::prepare_invoke(), if (load_receiver).
2049   int stackElementSize = Interpreter::stackElementSize;
2050   int offset = extra_slot_offset * stackElementSize;
2051   if (arg_slot.is_constant()) {
2052     offset += arg_slot.as_constant() * stackElementSize;
2053     return offset;
2054   } else {
2055     assert(temp_reg != noreg, &quot;must specify&quot;);
2056     sldi(temp_reg, arg_slot.as_register(), exact_log2(stackElementSize));
2057     if (offset != 0)
2058       addi(temp_reg, temp_reg, offset);
2059     return temp_reg;
2060   }
2061 }
2062 
2063 // Supports temp2_reg = R0.
2064 void MacroAssembler::biased_locking_enter(ConditionRegister cr_reg, Register obj_reg,
2065                                           Register mark_reg, Register temp_reg,
2066                                           Register temp2_reg, Label&amp; done, Label* slow_case) {
2067   assert(UseBiasedLocking, &quot;why call this otherwise?&quot;);
2068 
2069 #ifdef ASSERT
2070   assert_different_registers(obj_reg, mark_reg, temp_reg, temp2_reg);
2071 #endif
2072 
2073   Label cas_label;
2074 
2075   // Branch to done if fast path fails and no slow_case provided.
2076   Label *slow_case_int = (slow_case != NULL) ? slow_case : &amp;done;
2077 
2078   // Biased locking
2079   // See whether the lock is currently biased toward our thread and
2080   // whether the epoch is still valid
2081   // Note that the runtime guarantees sufficient alignment of JavaThread
2082   // pointers to allow age to be placed into low bits
2083   assert(markWord::age_shift == markWord::lock_bits + markWord::biased_lock_bits,
2084          &quot;biased locking makes assumptions about bit layout&quot;);
2085 
2086   if (PrintBiasedLockingStatistics) {
2087     load_const(temp2_reg, (address) BiasedLocking::total_entry_count_addr(), temp_reg);
2088     lwzx(temp_reg, temp2_reg);
2089     addi(temp_reg, temp_reg, 1);
2090     stwx(temp_reg, temp2_reg);
2091   }
2092 
2093   andi(temp_reg, mark_reg, markWord::biased_lock_mask_in_place);
2094   cmpwi(cr_reg, temp_reg, markWord::biased_lock_pattern);
2095   bne(cr_reg, cas_label);
2096 
2097   load_klass(temp_reg, obj_reg);
2098 
2099   load_const_optimized(temp2_reg, ~((int) markWord::age_mask_in_place));
2100   ld(temp_reg, in_bytes(Klass::prototype_header_offset()), temp_reg);
2101   orr(temp_reg, R16_thread, temp_reg);
2102   xorr(temp_reg, mark_reg, temp_reg);
2103   andr(temp_reg, temp_reg, temp2_reg);
2104   cmpdi(cr_reg, temp_reg, 0);
2105   if (PrintBiasedLockingStatistics) {
2106     Label l;
2107     bne(cr_reg, l);
2108     load_const(temp2_reg, (address) BiasedLocking::biased_lock_entry_count_addr());
2109     lwzx(mark_reg, temp2_reg);
2110     addi(mark_reg, mark_reg, 1);
2111     stwx(mark_reg, temp2_reg);
2112     // restore mark_reg
2113     ld(mark_reg, oopDesc::mark_offset_in_bytes(), obj_reg);
2114     bind(l);
2115   }
2116   beq(cr_reg, done);
2117 
2118   Label try_revoke_bias;
2119   Label try_rebias;
2120 
2121   // At this point we know that the header has the bias pattern and
2122   // that we are not the bias owner in the current epoch. We need to
2123   // figure out more details about the state of the header in order to
2124   // know what operations can be legally performed on the object&#39;s
2125   // header.
2126 
2127   // If the low three bits in the xor result aren&#39;t clear, that means
2128   // the prototype header is no longer biased and we have to revoke
2129   // the bias on this object.
2130   andi(temp2_reg, temp_reg, markWord::biased_lock_mask_in_place);
2131   cmpwi(cr_reg, temp2_reg, 0);
2132   bne(cr_reg, try_revoke_bias);
2133 
2134   // Biasing is still enabled for this data type. See whether the
2135   // epoch of the current bias is still valid, meaning that the epoch
2136   // bits of the mark word are equal to the epoch bits of the
2137   // prototype header. (Note that the prototype header&#39;s epoch bits
2138   // only change at a safepoint.) If not, attempt to rebias the object
2139   // toward the current thread. Note that we must be absolutely sure
2140   // that the current epoch is invalid in order to do this because
2141   // otherwise the manipulations it performs on the mark word are
2142   // illegal.
2143 
2144   int shift_amount = 64 - markWord::epoch_shift;
2145   // rotate epoch bits to right (little) end and set other bits to 0
2146   // [ big part | epoch | little part ] -&gt; [ 0..0 | epoch ]
2147   rldicl_(temp2_reg, temp_reg, shift_amount, 64 - markWord::epoch_bits);
2148   // branch if epoch bits are != 0, i.e. they differ, because the epoch has been incremented
2149   bne(CCR0, try_rebias);
2150 
2151   // The epoch of the current bias is still valid but we know nothing
2152   // about the owner; it might be set or it might be clear. Try to
2153   // acquire the bias of the object using an atomic operation. If this
2154   // fails we will go in to the runtime to revoke the object&#39;s bias.
2155   // Note that we first construct the presumed unbiased header so we
2156   // don&#39;t accidentally blow away another thread&#39;s valid bias.
2157   andi(mark_reg, mark_reg, (markWord::biased_lock_mask_in_place |
2158                                 markWord::age_mask_in_place |
2159                                 markWord::epoch_mask_in_place));
2160   orr(temp_reg, R16_thread, mark_reg);
2161 
2162   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
2163 
2164   // CmpxchgX sets cr_reg to cmpX(temp2_reg, mark_reg).
2165   cmpxchgd(/*flag=*/cr_reg, /*current_value=*/temp2_reg,
2166            /*compare_value=*/mark_reg, /*exchange_value=*/temp_reg,
2167            /*where=*/obj_reg,
2168            MacroAssembler::MemBarAcq,
2169            MacroAssembler::cmpxchgx_hint_acquire_lock(),
2170            noreg, slow_case_int); // bail out if failed
2171 
2172   // If the biasing toward our thread failed, this means that
2173   // another thread succeeded in biasing it toward itself and we
2174   // need to revoke that bias. The revocation will occur in the
2175   // interpreter runtime in the slow case.
2176   if (PrintBiasedLockingStatistics) {
2177     load_const(temp2_reg, (address) BiasedLocking::anonymously_biased_lock_entry_count_addr(), temp_reg);
2178     lwzx(temp_reg, temp2_reg);
2179     addi(temp_reg, temp_reg, 1);
2180     stwx(temp_reg, temp2_reg);
2181   }
2182   b(done);
2183 
2184   bind(try_rebias);
2185   // At this point we know the epoch has expired, meaning that the
2186   // current &quot;bias owner&quot;, if any, is actually invalid. Under these
2187   // circumstances _only_, we are allowed to use the current header&#39;s
2188   // value as the comparison value when doing the cas to acquire the
2189   // bias in the current epoch. In other words, we allow transfer of
2190   // the bias from one thread to another directly in this situation.
2191   load_klass(temp_reg, obj_reg);
2192   andi(temp2_reg, mark_reg, markWord::age_mask_in_place);
2193   orr(temp2_reg, R16_thread, temp2_reg);
2194   ld(temp_reg, in_bytes(Klass::prototype_header_offset()), temp_reg);
2195   orr(temp_reg, temp2_reg, temp_reg);
2196 
2197   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
2198 
2199   cmpxchgd(/*flag=*/cr_reg, /*current_value=*/temp2_reg,
2200                  /*compare_value=*/mark_reg, /*exchange_value=*/temp_reg,
2201                  /*where=*/obj_reg,
2202                  MacroAssembler::MemBarAcq,
2203                  MacroAssembler::cmpxchgx_hint_acquire_lock(),
2204                  noreg, slow_case_int); // bail out if failed
2205 
2206   // If the biasing toward our thread failed, this means that
2207   // another thread succeeded in biasing it toward itself and we
2208   // need to revoke that bias. The revocation will occur in the
2209   // interpreter runtime in the slow case.
2210   if (PrintBiasedLockingStatistics) {
2211     load_const(temp2_reg, (address) BiasedLocking::rebiased_lock_entry_count_addr(), temp_reg);
2212     lwzx(temp_reg, temp2_reg);
2213     addi(temp_reg, temp_reg, 1);
2214     stwx(temp_reg, temp2_reg);
2215   }
2216   b(done);
2217 
2218   bind(try_revoke_bias);
2219   // The prototype mark in the klass doesn&#39;t have the bias bit set any
2220   // more, indicating that objects of this data type are not supposed
2221   // to be biased any more. We are going to try to reset the mark of
2222   // this object to the prototype value and fall through to the
2223   // CAS-based locking scheme. Note that if our CAS fails, it means
2224   // that another thread raced us for the privilege of revoking the
2225   // bias of this particular object, so it&#39;s okay to continue in the
2226   // normal locking code.
2227   load_klass(temp_reg, obj_reg);
2228   ld(temp_reg, in_bytes(Klass::prototype_header_offset()), temp_reg);
2229   andi(temp2_reg, mark_reg, markWord::age_mask_in_place);
2230   orr(temp_reg, temp_reg, temp2_reg);
2231 
2232   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
2233 
2234   // CmpxchgX sets cr_reg to cmpX(temp2_reg, mark_reg).
2235   cmpxchgd(/*flag=*/cr_reg, /*current_value=*/temp2_reg,
2236                  /*compare_value=*/mark_reg, /*exchange_value=*/temp_reg,
2237                  /*where=*/obj_reg,
2238                  MacroAssembler::MemBarAcq,
2239                  MacroAssembler::cmpxchgx_hint_acquire_lock());
2240 
2241   // reload markWord in mark_reg before continuing with lightweight locking
2242   ld(mark_reg, oopDesc::mark_offset_in_bytes(), obj_reg);
2243 
2244   // Fall through to the normal CAS-based lock, because no matter what
2245   // the result of the above CAS, some thread must have succeeded in
2246   // removing the bias bit from the object&#39;s header.
2247   if (PrintBiasedLockingStatistics) {
2248     Label l;
2249     bne(cr_reg, l);
2250     load_const(temp2_reg, (address) BiasedLocking::revoked_lock_entry_count_addr(), temp_reg);
2251     lwzx(temp_reg, temp2_reg);
2252     addi(temp_reg, temp_reg, 1);
2253     stwx(temp_reg, temp2_reg);
2254     bind(l);
2255   }
2256 
2257   bind(cas_label);
2258 }
2259 
2260 void MacroAssembler::biased_locking_exit (ConditionRegister cr_reg, Register mark_addr, Register temp_reg, Label&amp; done) {
2261   // Check for biased locking unlock case, which is a no-op
2262   // Note: we do not have to check the thread ID for two reasons.
2263   // First, the interpreter checks for IllegalMonitorStateException at
2264   // a higher level. Second, if the bias was revoked while we held the
2265   // lock, the object could not be rebiased toward another thread, so
2266   // the bias bit would be clear.
2267 
2268   ld(temp_reg, 0, mark_addr);
2269   andi(temp_reg, temp_reg, markWord::biased_lock_mask_in_place);
2270 
2271   cmpwi(cr_reg, temp_reg, markWord::biased_lock_pattern);
2272   beq(cr_reg, done);
2273 }
2274 
2275 // allocation (for C1)
2276 void MacroAssembler::eden_allocate(
2277   Register obj,                      // result: pointer to object after successful allocation
2278   Register var_size_in_bytes,        // object size in bytes if unknown at compile time; invalid otherwise
2279   int      con_size_in_bytes,        // object size in bytes if   known at compile time
2280   Register t1,                       // temp register
2281   Register t2,                       // temp register
2282   Label&amp;   slow_case                 // continuation point if fast allocation fails
2283 ) {
2284   b(slow_case);
2285 }
2286 
2287 void MacroAssembler::tlab_allocate(
2288   Register obj,                      // result: pointer to object after successful allocation
2289   Register var_size_in_bytes,        // object size in bytes if unknown at compile time; invalid otherwise
2290   int      con_size_in_bytes,        // object size in bytes if   known at compile time
2291   Register t1,                       // temp register
2292   Label&amp;   slow_case                 // continuation point if fast allocation fails
2293 ) {
2294   // make sure arguments make sense
2295   assert_different_registers(obj, var_size_in_bytes, t1);
2296   assert(0 &lt;= con_size_in_bytes &amp;&amp; is_simm16(con_size_in_bytes), &quot;illegal object size&quot;);
2297   assert((con_size_in_bytes &amp; MinObjAlignmentInBytesMask) == 0, &quot;object size is not multiple of alignment&quot;);
2298 
2299   const Register new_top = t1;
2300   //verify_tlab(); not implemented
2301 
2302   ld(obj, in_bytes(JavaThread::tlab_top_offset()), R16_thread);
2303   ld(R0, in_bytes(JavaThread::tlab_end_offset()), R16_thread);
2304   if (var_size_in_bytes == noreg) {
2305     addi(new_top, obj, con_size_in_bytes);
2306   } else {
2307     add(new_top, obj, var_size_in_bytes);
2308   }
2309   cmpld(CCR0, new_top, R0);
2310   bc_far_optimized(Assembler::bcondCRbiIs1, bi0(CCR0, Assembler::greater), slow_case);
2311 
2312 #ifdef ASSERT
2313   // make sure new free pointer is properly aligned
2314   {
2315     Label L;
2316     andi_(R0, new_top, MinObjAlignmentInBytesMask);
2317     beq(CCR0, L);
2318     stop(&quot;updated TLAB free is not properly aligned&quot;, 0x934);
2319     bind(L);
2320   }
2321 #endif // ASSERT
2322 
2323   // update the tlab top pointer
2324   std(new_top, in_bytes(JavaThread::tlab_top_offset()), R16_thread);
2325   //verify_tlab(); not implemented
2326 }
2327 void MacroAssembler::incr_allocated_bytes(RegisterOrConstant size_in_bytes, Register t1, Register t2) {
2328   unimplemented(&quot;incr_allocated_bytes&quot;);
2329 }
2330 
2331 address MacroAssembler::emit_trampoline_stub(int destination_toc_offset,
2332                                              int insts_call_instruction_offset, Register Rtoc) {
2333   // Start the stub.
2334   address stub = start_a_stub(64);
2335   if (stub == NULL) { return NULL; } // CodeCache full: bail out
2336 
2337   // Create a trampoline stub relocation which relates this trampoline stub
2338   // with the call instruction at insts_call_instruction_offset in the
2339   // instructions code-section.
2340   relocate(trampoline_stub_Relocation::spec(code()-&gt;insts()-&gt;start() + insts_call_instruction_offset));
2341   const int stub_start_offset = offset();
2342 
2343   // For java_to_interp stubs we use R11_scratch1 as scratch register
2344   // and in call trampoline stubs we use R12_scratch2. This way we
2345   // can distinguish them (see is_NativeCallTrampolineStub_at()).
2346   Register reg_scratch = R12_scratch2;
2347 
2348   // Now, create the trampoline stub&#39;s code:
2349   // - load the TOC
2350   // - load the call target from the constant pool
2351   // - call
2352   if (Rtoc == noreg) {
2353     calculate_address_from_global_toc(reg_scratch, method_toc());
2354     Rtoc = reg_scratch;
2355   }
2356 
2357   ld_largeoffset_unchecked(reg_scratch, destination_toc_offset, Rtoc, false);
2358   mtctr(reg_scratch);
2359   bctr();
2360 
2361   const address stub_start_addr = addr_at(stub_start_offset);
2362 
2363   // Assert that the encoded destination_toc_offset can be identified and that it is correct.
2364   assert(destination_toc_offset == NativeCallTrampolineStub_at(stub_start_addr)-&gt;destination_toc_offset(),
2365          &quot;encoded offset into the constant pool must match&quot;);
2366   // Trampoline_stub_size should be good.
2367   assert((uint)(offset() - stub_start_offset) &lt;= trampoline_stub_size, &quot;should be good size&quot;);
2368   assert(is_NativeCallTrampolineStub_at(stub_start_addr), &quot;doesn&#39;t look like a trampoline&quot;);
2369 
2370   // End the stub.
2371   end_a_stub();
2372   return stub;
2373 }
2374 
2375 // TM on PPC64.
2376 void MacroAssembler::atomic_inc_ptr(Register addr, Register result, int simm16) {
2377   Label retry;
2378   bind(retry);
2379   ldarx(result, addr, /*hint*/ false);
2380   addi(result, result, simm16);
2381   stdcx_(result, addr);
2382   if (UseStaticBranchPredictionInCompareAndSwapPPC64) {
2383     bne_predict_not_taken(CCR0, retry); // stXcx_ sets CCR0
2384   } else {
2385     bne(                  CCR0, retry); // stXcx_ sets CCR0
2386   }
2387 }
2388 
2389 void MacroAssembler::atomic_ori_int(Register addr, Register result, int uimm16) {
2390   Label retry;
2391   bind(retry);
2392   lwarx(result, addr, /*hint*/ false);
2393   ori(result, result, uimm16);
2394   stwcx_(result, addr);
2395   if (UseStaticBranchPredictionInCompareAndSwapPPC64) {
2396     bne_predict_not_taken(CCR0, retry); // stXcx_ sets CCR0
2397   } else {
2398     bne(                  CCR0, retry); // stXcx_ sets CCR0
2399   }
2400 }
2401 
2402 #if INCLUDE_RTM_OPT
2403 
2404 // Update rtm_counters based on abort status
2405 // input: abort_status
2406 //        rtm_counters_Reg (RTMLockingCounters*)
2407 void MacroAssembler::rtm_counters_update(Register abort_status, Register rtm_counters_Reg) {
2408   // Mapping to keep PreciseRTMLockingStatistics similar to x86.
2409   // x86 ppc (! means inverted, ? means not the same)
2410   //  0   31  Set if abort caused by XABORT instruction.
2411   //  1  ! 7  If set, the transaction may succeed on a retry. This bit is always clear if bit 0 is set.
2412   //  2   13  Set if another logical processor conflicted with a memory address that was part of the transaction that aborted.
2413   //  3   10  Set if an internal buffer overflowed.
2414   //  4  ?12  Set if a debug breakpoint was hit.
2415   //  5  ?32  Set if an abort occurred during execution of a nested transaction.
2416   const int failure_bit[] = {tm_tabort, // Signal handler will set this too.
2417                              tm_failure_persistent,
2418                              tm_non_trans_cf,
2419                              tm_trans_cf,
2420                              tm_footprint_of,
2421                              tm_failure_code,
2422                              tm_transaction_level};
2423 
2424   const int num_failure_bits = sizeof(failure_bit) / sizeof(int);
2425   const int num_counters = RTMLockingCounters::ABORT_STATUS_LIMIT;
2426 
2427   const int bit2counter_map[][num_counters] =
2428   // 0 = no map; 1 = mapped, no inverted logic; -1 = mapped, inverted logic
2429   // Inverted logic means that if a bit is set don&#39;t count it, or vice-versa.
2430   // Care must be taken when mapping bits to counters as bits for a given
2431   // counter must be mutually exclusive. Otherwise, the counter will be
2432   // incremented more than once.
2433   // counters:
2434   // 0        1        2         3         4         5
2435   // abort  , persist, conflict, overflow, debug   , nested         bits:
2436   {{ 1      , 0      , 0       , 0       , 0       , 0      },   // abort
2437    { 0      , -1     , 0       , 0       , 0       , 0      },   // failure_persistent
2438    { 0      , 0      , 1       , 0       , 0       , 0      },   // non_trans_cf
2439    { 0      , 0      , 1       , 0       , 0       , 0      },   // trans_cf
2440    { 0      , 0      , 0       , 1       , 0       , 0      },   // footprint_of
2441    { 0      , 0      , 0       , 0       , -1      , 0      },   // failure_code = 0xD4
2442    { 0      , 0      , 0       , 0       , 0       , 1      }};  // transaction_level &gt; 1
2443   // ...
2444 
2445   // Move abort_status value to R0 and use abort_status register as a
2446   // temporary register because R0 as third operand in ld/std is treated
2447   // as base address zero (value). Likewise, R0 as second operand in addi
2448   // is problematic because it amounts to li.
2449   const Register temp_Reg = abort_status;
2450   const Register abort_status_R0 = R0;
2451   mr(abort_status_R0, abort_status);
2452 
2453   // Increment total abort counter.
2454   int counters_offs = RTMLockingCounters::abort_count_offset();
2455   ld(temp_Reg, counters_offs, rtm_counters_Reg);
2456   addi(temp_Reg, temp_Reg, 1);
2457   std(temp_Reg, counters_offs, rtm_counters_Reg);
2458 
2459   // Increment specific abort counters.
2460   if (PrintPreciseRTMLockingStatistics) {
2461 
2462     // #0 counter offset.
2463     int abortX_offs = RTMLockingCounters::abortX_count_offset();
2464 
2465     for (int nbit = 0; nbit &lt; num_failure_bits; nbit++) {
2466       for (int ncounter = 0; ncounter &lt; num_counters; ncounter++) {
2467         if (bit2counter_map[nbit][ncounter] != 0) {
2468           Label check_abort;
2469           int abort_counter_offs = abortX_offs + (ncounter &lt;&lt; 3);
2470 
2471           if (failure_bit[nbit] == tm_transaction_level) {
2472             // Don&#39;t check outer transaction, TL = 1 (bit 63). Hence only
2473             // 11 bits in the TL field are checked to find out if failure
2474             // occured in a nested transaction. This check also matches
2475             // the case when nesting_of = 1 (nesting overflow).
2476             rldicr_(temp_Reg, abort_status_R0, failure_bit[nbit], 10);
2477           } else if (failure_bit[nbit] == tm_failure_code) {
2478             // Check failure code for trap or illegal caught in TM.
2479             // Bits 0:7 are tested as bit 7 (persistent) is copied from
2480             // tabort or treclaim source operand.
2481             // On Linux: trap or illegal is TM_CAUSE_SIGNAL (0xD4).
2482             rldicl(temp_Reg, abort_status_R0, 8, 56);
2483             cmpdi(CCR0, temp_Reg, 0xD4);
2484           } else {
2485             rldicr_(temp_Reg, abort_status_R0, failure_bit[nbit], 0);
2486           }
2487 
2488           if (bit2counter_map[nbit][ncounter] == 1) {
2489             beq(CCR0, check_abort);
2490           } else {
2491             bne(CCR0, check_abort);
2492           }
2493 
2494           // We don&#39;t increment atomically.
2495           ld(temp_Reg, abort_counter_offs, rtm_counters_Reg);
2496           addi(temp_Reg, temp_Reg, 1);
2497           std(temp_Reg, abort_counter_offs, rtm_counters_Reg);
2498 
2499           bind(check_abort);
2500         }
2501       }
2502     }
2503   }
2504   // Restore abort_status.
2505   mr(abort_status, abort_status_R0);
2506 }
2507 
2508 // Branch if (random &amp; (count-1) != 0), count is 2^n
2509 // tmp and CR0 are killed
2510 void MacroAssembler::branch_on_random_using_tb(Register tmp, int count, Label&amp; brLabel) {
2511   mftb(tmp);
2512   andi_(tmp, tmp, count-1);
2513   bne(CCR0, brLabel);
2514 }
2515 
2516 // Perform abort ratio calculation, set no_rtm bit if high ratio.
2517 // input:  rtm_counters_Reg (RTMLockingCounters* address) - KILLED
2518 void MacroAssembler::rtm_abort_ratio_calculation(Register rtm_counters_Reg,
2519                                                  RTMLockingCounters* rtm_counters,
2520                                                  Metadata* method_data) {
2521   Label L_done, L_check_always_rtm1, L_check_always_rtm2;
2522 
2523   if (RTMLockingCalculationDelay &gt; 0) {
2524     // Delay calculation.
2525     ld(rtm_counters_Reg, (RegisterOrConstant)(intptr_t)RTMLockingCounters::rtm_calculation_flag_addr());
2526     cmpdi(CCR0, rtm_counters_Reg, 0);
2527     beq(CCR0, L_done);
2528     load_const_optimized(rtm_counters_Reg, (address)rtm_counters, R0); // reload
2529   }
2530   // Abort ratio calculation only if abort_count &gt; RTMAbortThreshold.
2531   //   Aborted transactions = abort_count * 100
2532   //   All transactions = total_count *  RTMTotalCountIncrRate
2533   //   Set no_rtm bit if (Aborted transactions &gt;= All transactions * RTMAbortRatio)
2534   ld(R0, RTMLockingCounters::abort_count_offset(), rtm_counters_Reg);
2535   if (is_simm(RTMAbortThreshold, 16)) {   // cmpdi can handle 16bit immediate only.
2536     cmpdi(CCR0, R0, RTMAbortThreshold);
2537     blt(CCR0, L_check_always_rtm2);  // reload of rtm_counters_Reg not necessary
2538   } else {
2539     load_const_optimized(rtm_counters_Reg, RTMAbortThreshold);
2540     cmpd(CCR0, R0, rtm_counters_Reg);
2541     blt(CCR0, L_check_always_rtm1);  // reload of rtm_counters_Reg required
2542   }
2543   mulli(R0, R0, 100);
2544 
2545   const Register tmpReg = rtm_counters_Reg;
2546   ld(tmpReg, RTMLockingCounters::total_count_offset(), rtm_counters_Reg);
2547   mulli(tmpReg, tmpReg, RTMTotalCountIncrRate); // allowable range: int16
2548   mulli(tmpReg, tmpReg, RTMAbortRatio);         // allowable range: int16
2549   cmpd(CCR0, R0, tmpReg);
2550   blt(CCR0, L_check_always_rtm1); // jump to reload
2551   if (method_data != NULL) {
2552     // Set rtm_state to &quot;no rtm&quot; in MDO.
2553     // Not using a metadata relocation. Method and Class Loader are kept alive anyway.
2554     // (See nmethod::metadata_do and CodeBuffer::finalize_oop_references.)
2555     load_const(R0, (address)method_data + MethodData::rtm_state_offset_in_bytes(), tmpReg);
2556     atomic_ori_int(R0, tmpReg, NoRTM);
2557   }
2558   b(L_done);
2559 
2560   bind(L_check_always_rtm1);
2561   load_const_optimized(rtm_counters_Reg, (address)rtm_counters, R0); // reload
2562   bind(L_check_always_rtm2);
2563   ld(tmpReg, RTMLockingCounters::total_count_offset(), rtm_counters_Reg);
2564   int64_t thresholdValue = RTMLockingThreshold / RTMTotalCountIncrRate;
2565   if (is_simm(thresholdValue, 16)) {   // cmpdi can handle 16bit immediate only.
2566     cmpdi(CCR0, tmpReg, thresholdValue);
2567   } else {
2568     load_const_optimized(R0, thresholdValue);
2569     cmpd(CCR0, tmpReg, R0);
2570   }
2571   blt(CCR0, L_done);
2572   if (method_data != NULL) {
2573     // Set rtm_state to &quot;always rtm&quot; in MDO.
2574     // Not using a metadata relocation. See above.
2575     load_const(R0, (address)method_data + MethodData::rtm_state_offset_in_bytes(), tmpReg);
2576     atomic_ori_int(R0, tmpReg, UseRTM);
2577   }
2578   bind(L_done);
2579 }
2580 
2581 // Update counters and perform abort ratio calculation.
2582 // input: abort_status_Reg
2583 void MacroAssembler::rtm_profiling(Register abort_status_Reg, Register temp_Reg,
2584                                    RTMLockingCounters* rtm_counters,
2585                                    Metadata* method_data,
2586                                    bool profile_rtm) {
2587 
2588   assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
2589   // Update rtm counters based on state at abort.
2590   // Reads abort_status_Reg, updates flags.
2591   assert_different_registers(abort_status_Reg, temp_Reg);
2592   load_const_optimized(temp_Reg, (address)rtm_counters, R0);
2593   rtm_counters_update(abort_status_Reg, temp_Reg);
2594   if (profile_rtm) {
2595     assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
2596     rtm_abort_ratio_calculation(temp_Reg, rtm_counters, method_data);
2597   }
2598 }
2599 
2600 // Retry on abort if abort&#39;s status indicates non-persistent failure.
2601 // inputs: retry_count_Reg
2602 //       : abort_status_Reg
2603 // output: retry_count_Reg decremented by 1
2604 void MacroAssembler::rtm_retry_lock_on_abort(Register retry_count_Reg, Register abort_status_Reg,
2605                                              Label&amp; retryLabel, Label* checkRetry) {
2606   Label doneRetry;
2607 
2608   // Don&#39;t retry if failure is persistent.
2609   // The persistent bit is set when a (A) Disallowed operation is performed in
2610   // transactional state, like for instance trying to write the TFHAR after a
2611   // transaction is started; or when there is (B) a Nesting Overflow (too many
2612   // nested transactions); or when (C) the Footprint overflows (too many
2613   // addressess touched in TM state so there is no more space in the footprint
2614   // area to track them); or in case of (D) a Self-Induced Conflict, i.e. a
2615   // store is performed to a given address in TM state, then once in suspended
2616   // state the same address is accessed. Failure (A) is very unlikely to occur
2617   // in the JVM. Failure (D) will never occur because Suspended state is never
2618   // used in the JVM. Thus mostly (B) a Nesting Overflow or (C) a Footprint
2619   // Overflow will set the persistent bit.
2620   rldicr_(R0, abort_status_Reg, tm_failure_persistent, 0);
2621   bne(CCR0, doneRetry);
2622 
2623   // Don&#39;t retry if transaction was deliberately aborted, i.e. caused by a
2624   // tabort instruction.
2625   rldicr_(R0, abort_status_Reg, tm_tabort, 0);
2626   bne(CCR0, doneRetry);
2627 
2628   // Retry if transaction aborted due to a conflict with another thread.
2629   if (checkRetry) { bind(*checkRetry); }
2630   addic_(retry_count_Reg, retry_count_Reg, -1);
2631   blt(CCR0, doneRetry);
2632   b(retryLabel);
2633   bind(doneRetry);
2634 }
2635 
2636 // Spin and retry if lock is busy.
2637 // inputs: owner_addr_Reg (monitor address)
2638 //       : retry_count_Reg
2639 // output: retry_count_Reg decremented by 1
2640 // CTR is killed
2641 void MacroAssembler::rtm_retry_lock_on_busy(Register retry_count_Reg, Register owner_addr_Reg, Label&amp; retryLabel) {
2642   Label SpinLoop, doneRetry, doRetry;
2643   addic_(retry_count_Reg, retry_count_Reg, -1);
2644   blt(CCR0, doneRetry);
2645 
2646   if (RTMSpinLoopCount &gt; 1) {
2647     li(R0, RTMSpinLoopCount);
2648     mtctr(R0);
2649   }
2650 
2651   // low thread priority
2652   smt_prio_low();
2653   bind(SpinLoop);
2654 
2655   if (RTMSpinLoopCount &gt; 1) {
2656     bdz(doRetry);
2657     ld(R0, 0, owner_addr_Reg);
2658     cmpdi(CCR0, R0, 0);
2659     bne(CCR0, SpinLoop);
2660   }
2661 
2662   bind(doRetry);
2663 
2664   // restore thread priority to default in userspace
2665 #ifdef LINUX
2666   smt_prio_medium_low();
2667 #else
2668   smt_prio_medium();
2669 #endif
2670 
2671   b(retryLabel);
2672 
2673   bind(doneRetry);
2674 }
2675 
2676 // Use RTM for normal stack locks.
2677 // Input: objReg (object to lock)
2678 void MacroAssembler::rtm_stack_locking(ConditionRegister flag,
2679                                        Register obj, Register mark_word, Register tmp,
2680                                        Register retry_on_abort_count_Reg,
2681                                        RTMLockingCounters* stack_rtm_counters,
2682                                        Metadata* method_data, bool profile_rtm,
2683                                        Label&amp; DONE_LABEL, Label&amp; IsInflated) {
2684   assert(UseRTMForStackLocks, &quot;why call this otherwise?&quot;);
2685   assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
2686   Label L_rtm_retry, L_decrement_retry, L_on_abort;
2687 
2688   if (RTMRetryCount &gt; 0) {
2689     load_const_optimized(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort
2690     bind(L_rtm_retry);
2691   }
2692   andi_(R0, mark_word, markWord::monitor_value);  // inflated vs stack-locked|neutral|biased
2693   bne(CCR0, IsInflated);
2694 
2695   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
2696     Label L_noincrement;
2697     if (RTMTotalCountIncrRate &gt; 1) {
2698       branch_on_random_using_tb(tmp, RTMTotalCountIncrRate, L_noincrement);
2699     }
2700     assert(stack_rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
2701     load_const_optimized(tmp, (address)stack_rtm_counters-&gt;total_count_addr(), R0);
2702     //atomic_inc_ptr(tmp, /*temp, will be reloaded*/mark_word); We don&#39;t increment atomically
2703     ldx(mark_word, tmp);
2704     addi(mark_word, mark_word, 1);
2705     stdx(mark_word, tmp);
2706     bind(L_noincrement);
2707   }
2708   tbegin_();
2709   beq(CCR0, L_on_abort);
2710   ld(mark_word, oopDesc::mark_offset_in_bytes(), obj);      // Reload in transaction, conflicts need to be tracked.
2711   andi(R0, mark_word, markWord::biased_lock_mask_in_place); // look at 3 lock bits
2712   cmpwi(flag, R0, markWord::unlocked_value);                // bits = 001 unlocked
2713   beq(flag, DONE_LABEL);                                    // all done if unlocked
2714 
2715   if (UseRTMXendForLockBusy) {
2716     tend_();
2717     b(L_decrement_retry);
2718   } else {
2719     tabort_();
2720   }
2721   bind(L_on_abort);
2722   const Register abort_status_Reg = tmp;
2723   mftexasr(abort_status_Reg);
2724   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
2725     rtm_profiling(abort_status_Reg, /*temp*/mark_word, stack_rtm_counters, method_data, profile_rtm);
2726   }
2727   ld(mark_word, oopDesc::mark_offset_in_bytes(), obj); // reload
2728   if (RTMRetryCount &gt; 0) {
2729     // Retry on lock abort if abort status is not permanent.
2730     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry, &amp;L_decrement_retry);
2731   } else {
2732     bind(L_decrement_retry);
2733   }
2734 }
2735 
2736 // Use RTM for inflating locks
2737 // inputs: obj       (object to lock)
2738 //         mark_word (current header - KILLED)
2739 //         boxReg    (on-stack box address (displaced header location) - KILLED)
2740 void MacroAssembler::rtm_inflated_locking(ConditionRegister flag,
2741                                           Register obj, Register mark_word, Register boxReg,
2742                                           Register retry_on_busy_count_Reg, Register retry_on_abort_count_Reg,
2743                                           RTMLockingCounters* rtm_counters,
2744                                           Metadata* method_data, bool profile_rtm,
2745                                           Label&amp; DONE_LABEL) {
2746   assert(UseRTMLocking, &quot;why call this otherwise?&quot;);
2747   Label L_rtm_retry, L_decrement_retry, L_on_abort;
2748   // Clean monitor_value bit to get valid pointer.
2749   int owner_offset = ObjectMonitor::owner_offset_in_bytes() - markWord::monitor_value;
2750 
2751   // Store non-null, using boxReg instead of (intptr_t)markWord::unused_mark().
2752   std(boxReg, BasicLock::displaced_header_offset_in_bytes(), boxReg);
2753   const Register tmpReg = boxReg;
2754   const Register owner_addr_Reg = mark_word;
2755   addi(owner_addr_Reg, mark_word, owner_offset);
2756 
2757   if (RTMRetryCount &gt; 0) {
2758     load_const_optimized(retry_on_busy_count_Reg, RTMRetryCount);  // Retry on lock busy.
2759     load_const_optimized(retry_on_abort_count_Reg, RTMRetryCount); // Retry on abort.
2760     bind(L_rtm_retry);
2761   }
2762   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
2763     Label L_noincrement;
2764     if (RTMTotalCountIncrRate &gt; 1) {
2765       branch_on_random_using_tb(R0, RTMTotalCountIncrRate, L_noincrement);
2766     }
2767     assert(rtm_counters != NULL, &quot;should not be NULL when profiling RTM&quot;);
2768     load_const(R0, (address)rtm_counters-&gt;total_count_addr(), tmpReg);
2769     //atomic_inc_ptr(R0, tmpReg); We don&#39;t increment atomically
2770     ldx(tmpReg, R0);
2771     addi(tmpReg, tmpReg, 1);
2772     stdx(tmpReg, R0);
2773     bind(L_noincrement);
2774   }
2775   tbegin_();
2776   beq(CCR0, L_on_abort);
2777   // We don&#39;t reload mark word. Will only be reset at safepoint.
2778   ld(R0, 0, owner_addr_Reg); // Load in transaction, conflicts need to be tracked.
2779   cmpdi(flag, R0, 0);
2780   beq(flag, DONE_LABEL);
2781 
2782   if (UseRTMXendForLockBusy) {
2783     tend_();
2784     b(L_decrement_retry);
2785   } else {
2786     tabort_();
2787   }
2788   bind(L_on_abort);
2789   const Register abort_status_Reg = tmpReg;
2790   mftexasr(abort_status_Reg);
2791   if (PrintPreciseRTMLockingStatistics || profile_rtm) {
2792     rtm_profiling(abort_status_Reg, /*temp*/ owner_addr_Reg, rtm_counters, method_data, profile_rtm);
2793     // Restore owner_addr_Reg
2794     ld(mark_word, oopDesc::mark_offset_in_bytes(), obj);
2795 #ifdef ASSERT
2796     andi_(R0, mark_word, markWord::monitor_value);
2797     asm_assert_ne(&quot;must be inflated&quot;, 0xa754); // Deflating only allowed at safepoint.
2798 #endif
2799     addi(owner_addr_Reg, mark_word, owner_offset);
2800   }
2801   if (RTMRetryCount &gt; 0) {
2802     // Retry on lock abort if abort status is not permanent.
2803     rtm_retry_lock_on_abort(retry_on_abort_count_Reg, abort_status_Reg, L_rtm_retry);
2804   }
2805 
2806   // Appears unlocked - try to swing _owner from null to non-null.
2807   cmpxchgd(flag, /*current val*/ R0, (intptr_t)0, /*new val*/ R16_thread, owner_addr_Reg,
2808            MacroAssembler::MemBarRel | MacroAssembler::MemBarAcq,
2809            MacroAssembler::cmpxchgx_hint_acquire_lock(), noreg, &amp;L_decrement_retry, true);
2810 
2811   if (RTMRetryCount &gt; 0) {
2812     // success done else retry
2813     b(DONE_LABEL);
2814     bind(L_decrement_retry);
2815     // Spin and retry if lock is busy.
2816     rtm_retry_lock_on_busy(retry_on_busy_count_Reg, owner_addr_Reg, L_rtm_retry);
2817   } else {
2818     bind(L_decrement_retry);
2819   }
2820 }
2821 
2822 #endif //  INCLUDE_RTM_OPT
2823 
2824 // &quot;The box&quot; is the space on the stack where we copy the object mark.
2825 void MacroAssembler::compiler_fast_lock_object(ConditionRegister flag, Register oop, Register box,
2826                                                Register temp, Register displaced_header, Register current_header,
2827                                                bool try_bias,
2828                                                RTMLockingCounters* rtm_counters,
2829                                                RTMLockingCounters* stack_rtm_counters,
2830                                                Metadata* method_data,
2831                                                bool use_rtm, bool profile_rtm) {
2832   assert_different_registers(oop, box, temp, displaced_header, current_header);
2833   assert(flag != CCR0, &quot;bad condition register&quot;);
2834   Label cont;
2835   Label object_has_monitor;
2836   Label cas_failed;
2837 
2838   // Load markWord from object into displaced_header.
2839   ld(displaced_header, oopDesc::mark_offset_in_bytes(), oop);
2840 
2841 
2842   if (try_bias) {
2843     biased_locking_enter(flag, oop, displaced_header, temp, current_header, cont);
2844   }
2845 
2846 #if INCLUDE_RTM_OPT
2847   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
2848     rtm_stack_locking(flag, oop, displaced_header, temp, /*temp*/ current_header,
2849                       stack_rtm_counters, method_data, profile_rtm,
2850                       cont, object_has_monitor);
2851   }
2852 #endif // INCLUDE_RTM_OPT
2853 
2854   // Handle existing monitor.
2855   // The object has an existing monitor iff (mark &amp; monitor_value) != 0.
2856   andi_(temp, displaced_header, markWord::monitor_value);
2857   bne(CCR0, object_has_monitor);
2858 
2859   // Set displaced_header to be (markWord of object | UNLOCK_VALUE).
2860   ori(displaced_header, displaced_header, markWord::unlocked_value);
2861 
2862   // Load Compare Value application register.
2863 
2864   // Initialize the box. (Must happen before we update the object mark!)
2865   std(displaced_header, BasicLock::displaced_header_offset_in_bytes(), box);
2866 
2867   // Must fence, otherwise, preceding store(s) may float below cmpxchg.
2868   // Compare object markWord with mark and if equal exchange scratch1 with object markWord.
2869   cmpxchgd(/*flag=*/flag,
2870            /*current_value=*/current_header,
2871            /*compare_value=*/displaced_header,
2872            /*exchange_value=*/box,
2873            /*where=*/oop,
2874            MacroAssembler::MemBarRel | MacroAssembler::MemBarAcq,
2875            MacroAssembler::cmpxchgx_hint_acquire_lock(),
2876            noreg,
2877            &amp;cas_failed,
2878            /*check without membar and ldarx first*/true);
2879   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
2880 
2881   // If the compare-and-exchange succeeded, then we found an unlocked
2882   // object and we have now locked it.
2883   b(cont);
2884 
2885   bind(cas_failed);
2886   // We did not see an unlocked object so try the fast recursive case.
2887 
2888   // Check if the owner is self by comparing the value in the markWord of object
2889   // (current_header) with the stack pointer.
2890   sub(current_header, current_header, R1_SP);
2891   load_const_optimized(temp, ~(os::vm_page_size()-1) | markWord::lock_mask_in_place);
2892 
2893   and_(R0/*==0?*/, current_header, temp);
2894   // If condition is true we are cont and hence we can store 0 as the
2895   // displaced header in the box, which indicates that it is a recursive lock.
2896   mcrf(flag,CCR0);
2897   std(R0/*==0, perhaps*/, BasicLock::displaced_header_offset_in_bytes(), box);
2898 
2899   // Handle existing monitor.
2900   b(cont);
2901 
2902   bind(object_has_monitor);
2903   // The object&#39;s monitor m is unlocked iff m-&gt;owner == NULL,
2904   // otherwise m-&gt;owner may contain a thread or a stack address.
2905 
2906 #if INCLUDE_RTM_OPT
2907   // Use the same RTM locking code in 32- and 64-bit VM.
2908   if (use_rtm) {
2909     rtm_inflated_locking(flag, oop, displaced_header, box, temp, /*temp*/ current_header,
2910                          rtm_counters, method_data, profile_rtm, cont);
2911   } else {
2912 #endif // INCLUDE_RTM_OPT
2913 
2914   // Try to CAS m-&gt;owner from NULL to current thread.
2915   addi(temp, displaced_header, ObjectMonitor::owner_offset_in_bytes()-markWord::monitor_value);
2916   cmpxchgd(/*flag=*/flag,
2917            /*current_value=*/current_header,
2918            /*compare_value=*/(intptr_t)0,
2919            /*exchange_value=*/R16_thread,
2920            /*where=*/temp,
2921            MacroAssembler::MemBarRel | MacroAssembler::MemBarAcq,
2922            MacroAssembler::cmpxchgx_hint_acquire_lock());
2923 
2924   // Store a non-null value into the box.
2925   std(box, BasicLock::displaced_header_offset_in_bytes(), box);
2926 
2927 # ifdef ASSERT
2928   bne(flag, cont);
2929   // We have acquired the monitor, check some invariants.
2930   addi(/*monitor=*/temp, temp, -ObjectMonitor::owner_offset_in_bytes());
2931   // Invariant 1: _recursions should be 0.
2932   //assert(ObjectMonitor::recursions_size_in_bytes() == 8, &quot;unexpected size&quot;);
2933   asm_assert_mem8_is_zero(ObjectMonitor::recursions_offset_in_bytes(), temp,
2934                             &quot;monitor-&gt;_recursions should be 0&quot;, -1);
2935 # endif
2936 
2937 #if INCLUDE_RTM_OPT
2938   } // use_rtm()
2939 #endif
2940 
2941   bind(cont);
2942   // flag == EQ indicates success
2943   // flag == NE indicates failure
2944 }
2945 
2946 void MacroAssembler::compiler_fast_unlock_object(ConditionRegister flag, Register oop, Register box,
2947                                                  Register temp, Register displaced_header, Register current_header,
2948                                                  bool try_bias, bool use_rtm) {
2949   assert_different_registers(oop, box, temp, displaced_header, current_header);
2950   assert(flag != CCR0, &quot;bad condition register&quot;);
2951   Label cont;
2952   Label object_has_monitor;
2953 
2954   if (try_bias) {
2955     biased_locking_exit(flag, oop, current_header, cont);
2956   }
2957 
2958 #if INCLUDE_RTM_OPT
2959   if (UseRTMForStackLocks &amp;&amp; use_rtm) {
2960     assert(!UseBiasedLocking, &quot;Biased locking is not supported with RTM locking&quot;);
2961     Label L_regular_unlock;
2962     ld(current_header, oopDesc::mark_offset_in_bytes(), oop);      // fetch markword
2963     andi(R0, current_header, markWord::biased_lock_mask_in_place); // look at 3 lock bits
2964     cmpwi(flag, R0, markWord::unlocked_value);                     // bits = 001 unlocked
2965     bne(flag, L_regular_unlock);                                   // else RegularLock
2966     tend_();                                                       // otherwise end...
2967     b(cont);                                                       // ... and we&#39;re done
2968     bind(L_regular_unlock);
2969   }
2970 #endif
2971 
2972   // Find the lock address and load the displaced header from the stack.
2973   ld(displaced_header, BasicLock::displaced_header_offset_in_bytes(), box);
2974 
2975   // If the displaced header is 0, we have a recursive unlock.
2976   cmpdi(flag, displaced_header, 0);
2977   beq(flag, cont);
2978 
2979   // Handle existing monitor.
2980   // The object has an existing monitor iff (mark &amp; monitor_value) != 0.
2981   RTM_OPT_ONLY( if (!(UseRTMForStackLocks &amp;&amp; use_rtm)) ) // skip load if already done
2982   ld(current_header, oopDesc::mark_offset_in_bytes(), oop);
2983   andi_(R0, current_header, markWord::monitor_value);
2984   bne(CCR0, object_has_monitor);
2985 
2986   // Check if it is still a light weight lock, this is is true if we see
2987   // the stack address of the basicLock in the markWord of the object.
2988   // Cmpxchg sets flag to cmpd(current_header, box).
2989   cmpxchgd(/*flag=*/flag,
2990            /*current_value=*/current_header,
2991            /*compare_value=*/box,
2992            /*exchange_value=*/displaced_header,
2993            /*where=*/oop,
2994            MacroAssembler::MemBarRel,
2995            MacroAssembler::cmpxchgx_hint_release_lock(),
2996            noreg,
2997            &amp;cont);
2998 
2999   assert(oopDesc::mark_offset_in_bytes() == 0, &quot;offset of _mark is not 0&quot;);
3000 
3001   // Handle existing monitor.
3002   b(cont);
3003 
3004   bind(object_has_monitor);
3005   STATIC_ASSERT(markWord::monitor_value &lt;= INT_MAX);
3006   addi(current_header, current_header, -(int)markWord::monitor_value); // monitor
3007   ld(temp,             ObjectMonitor::owner_offset_in_bytes(), current_header);
3008 
3009     // It&#39;s inflated.
3010 #if INCLUDE_RTM_OPT
3011   if (use_rtm) {
3012     Label L_regular_inflated_unlock;
3013     // Clean monitor_value bit to get valid pointer
3014     cmpdi(flag, temp, 0);
3015     bne(flag, L_regular_inflated_unlock);
3016     tend_();
3017     b(cont);
3018     bind(L_regular_inflated_unlock);
3019   }
3020 #endif
3021 
3022   ld(displaced_header, ObjectMonitor::recursions_offset_in_bytes(), current_header);
3023   xorr(temp, R16_thread, temp);      // Will be 0 if we are the owner.
3024   orr(temp, temp, displaced_header); // Will be 0 if there are 0 recursions.
3025   cmpdi(flag, temp, 0);
3026   bne(flag, cont);
3027 
3028   ld(temp,             ObjectMonitor::EntryList_offset_in_bytes(), current_header);
3029   ld(displaced_header, ObjectMonitor::cxq_offset_in_bytes(), current_header);
3030   orr(temp, temp, displaced_header); // Will be 0 if both are 0.
3031   cmpdi(flag, temp, 0);
3032   bne(flag, cont);
3033   release();
3034   std(temp, ObjectMonitor::owner_offset_in_bytes(), current_header);
3035 
3036   bind(cont);
3037   // flag == EQ indicates success
3038   // flag == NE indicates failure
3039 }
3040 
3041 void MacroAssembler::safepoint_poll(Label&amp; slow_path, Register temp_reg) {
3042   if (SafepointMechanism::uses_thread_local_poll()) {
3043     ld(temp_reg, in_bytes(Thread::polling_page_offset()), R16_thread);
3044     // Armed page has poll_bit set.
3045     andi_(temp_reg, temp_reg, SafepointMechanism::poll_bit());
3046   } else {
3047     lwz(temp_reg, (RegisterOrConstant)(intptr_t)SafepointSynchronize::address_of_state());
3048     cmpwi(CCR0, temp_reg, SafepointSynchronize::_not_synchronized);
3049   }
3050   bne(CCR0, slow_path);
3051 }
3052 
3053 void MacroAssembler::resolve_jobject(Register value, Register tmp1, Register tmp2, bool needs_frame) {
3054   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
3055   bs-&gt;resolve_jobject(this, value, tmp1, tmp2, needs_frame);
3056 }
3057 
3058 // Values for last_Java_pc, and last_Java_sp must comply to the rules
3059 // in frame_ppc.hpp.
3060 void MacroAssembler::set_last_Java_frame(Register last_Java_sp, Register last_Java_pc) {
3061   // Always set last_Java_pc and flags first because once last_Java_sp
3062   // is visible has_last_Java_frame is true and users will look at the
3063   // rest of the fields. (Note: flags should always be zero before we
3064   // get here so doesn&#39;t need to be set.)
3065 
3066   // Verify that last_Java_pc was zeroed on return to Java
3067   asm_assert_mem8_is_zero(in_bytes(JavaThread::last_Java_pc_offset()), R16_thread,
3068                           &quot;last_Java_pc not zeroed before leaving Java&quot;, 0x200);
3069 
3070   // When returning from calling out from Java mode the frame anchor&#39;s
3071   // last_Java_pc will always be set to NULL. It is set here so that
3072   // if we are doing a call to native (not VM) that we capture the
3073   // known pc and don&#39;t have to rely on the native call having a
3074   // standard frame linkage where we can find the pc.
3075   if (last_Java_pc != noreg)
3076     std(last_Java_pc, in_bytes(JavaThread::last_Java_pc_offset()), R16_thread);
3077 
3078   // Set last_Java_sp last.
3079   std(last_Java_sp, in_bytes(JavaThread::last_Java_sp_offset()), R16_thread);
3080 }
3081 
3082 void MacroAssembler::reset_last_Java_frame(void) {
3083   asm_assert_mem8_isnot_zero(in_bytes(JavaThread::last_Java_sp_offset()),
3084                              R16_thread, &quot;SP was not set, still zero&quot;, 0x202);
3085 
3086   BLOCK_COMMENT(&quot;reset_last_Java_frame {&quot;);
3087   li(R0, 0);
3088 
3089   // _last_Java_sp = 0
3090   std(R0, in_bytes(JavaThread::last_Java_sp_offset()), R16_thread);
3091 
3092   // _last_Java_pc = 0
3093   std(R0, in_bytes(JavaThread::last_Java_pc_offset()), R16_thread);
3094   BLOCK_COMMENT(&quot;} reset_last_Java_frame&quot;);
3095 }
3096 
3097 void MacroAssembler::set_top_ijava_frame_at_SP_as_last_Java_frame(Register sp, Register tmp1) {
3098   assert_different_registers(sp, tmp1);
3099 
3100   // sp points to a TOP_IJAVA_FRAME, retrieve frame&#39;s PC via
3101   // TOP_IJAVA_FRAME_ABI.
3102   // FIXME: assert that we really have a TOP_IJAVA_FRAME here!
3103   address entry = pc();
3104   load_const_optimized(tmp1, entry);
3105 
3106   set_last_Java_frame(/*sp=*/sp, /*pc=*/tmp1);
3107 }
3108 
3109 void MacroAssembler::get_vm_result(Register oop_result) {
3110   // Read:
3111   //   R16_thread
3112   //   R16_thread-&gt;in_bytes(JavaThread::vm_result_offset())
3113   //
3114   // Updated:
3115   //   oop_result
3116   //   R16_thread-&gt;in_bytes(JavaThread::vm_result_offset())
3117 
3118   verify_thread();
3119 
3120   ld(oop_result, in_bytes(JavaThread::vm_result_offset()), R16_thread);
3121   li(R0, 0);
3122   std(R0, in_bytes(JavaThread::vm_result_offset()), R16_thread);
3123 
3124   verify_oop(oop_result, FILE_AND_LINE);
3125 }
3126 
3127 void MacroAssembler::get_vm_result_2(Register metadata_result) {
3128   // Read:
3129   //   R16_thread
3130   //   R16_thread-&gt;in_bytes(JavaThread::vm_result_2_offset())
3131   //
3132   // Updated:
3133   //   metadata_result
3134   //   R16_thread-&gt;in_bytes(JavaThread::vm_result_2_offset())
3135 
3136   ld(metadata_result, in_bytes(JavaThread::vm_result_2_offset()), R16_thread);
3137   li(R0, 0);
3138   std(R0, in_bytes(JavaThread::vm_result_2_offset()), R16_thread);
3139 }
3140 
3141 Register MacroAssembler::encode_klass_not_null(Register dst, Register src) {
3142   Register current = (src != noreg) ? src : dst; // Klass is in dst if no src provided.
3143   if (CompressedKlassPointers::base() != 0) {
3144     // Use dst as temp if it is free.
3145     sub_const_optimized(dst, current, CompressedKlassPointers::base(), R0);
3146     current = dst;
3147   }
3148   if (CompressedKlassPointers::shift() != 0) {
3149     srdi(dst, current, CompressedKlassPointers::shift());
3150     current = dst;
3151   }
3152   return current;
3153 }
3154 
3155 void MacroAssembler::store_klass(Register dst_oop, Register klass, Register ck) {
3156   if (UseCompressedClassPointers) {
3157     Register compressedKlass = encode_klass_not_null(ck, klass);
3158     stw(compressedKlass, oopDesc::klass_offset_in_bytes(), dst_oop);
3159   } else {
3160     std(klass, oopDesc::klass_offset_in_bytes(), dst_oop);
3161   }
3162 }
3163 
3164 void MacroAssembler::store_klass_gap(Register dst_oop, Register val) {
3165   if (UseCompressedClassPointers) {
3166     if (val == noreg) {
3167       val = R0;
3168       li(val, 0);
3169     }
3170     stw(val, oopDesc::klass_gap_offset_in_bytes(), dst_oop); // klass gap if compressed
3171   }
3172 }
3173 
3174 int MacroAssembler::instr_size_for_decode_klass_not_null() {
3175   if (!UseCompressedClassPointers) return 0;
3176   int num_instrs = 1;  // shift or move
3177   if (CompressedKlassPointers::base() != 0) num_instrs = 7;  // shift + load const + add
3178   return num_instrs * BytesPerInstWord;
3179 }
3180 
3181 void MacroAssembler::decode_klass_not_null(Register dst, Register src) {
3182   assert(dst != R0, &quot;Dst reg may not be R0, as R0 is used here.&quot;);
3183   if (src == noreg) src = dst;
3184   Register shifted_src = src;
3185   if (CompressedKlassPointers::shift() != 0 ||
3186       CompressedKlassPointers::base() == 0 &amp;&amp; src != dst) {  // Move required.
3187     shifted_src = dst;
3188     sldi(shifted_src, src, CompressedKlassPointers::shift());
3189   }
3190   if (CompressedKlassPointers::base() != 0) {
3191     add_const_optimized(dst, shifted_src, CompressedKlassPointers::base(), R0);
3192   }
3193 }
3194 
3195 void MacroAssembler::load_klass(Register dst, Register src) {
3196   if (UseCompressedClassPointers) {
3197     lwz(dst, oopDesc::klass_offset_in_bytes(), src);
3198     // Attention: no null check here!
3199     decode_klass_not_null(dst, dst);
3200   } else {
3201     ld(dst, oopDesc::klass_offset_in_bytes(), src);
3202   }
3203 }
3204 
3205 // ((OopHandle)result).resolve();
3206 void MacroAssembler::resolve_oop_handle(Register result) {
3207   // OopHandle::resolve is an indirection.
3208   ld(result, 0, result);
3209 }
3210 
3211 void MacroAssembler::load_mirror_from_const_method(Register mirror, Register const_method) {
3212   ld(mirror, in_bytes(ConstMethod::constants_offset()), const_method);
3213   ld(mirror, ConstantPool::pool_holder_offset_in_bytes(), mirror);
3214   ld(mirror, in_bytes(Klass::java_mirror_offset()), mirror);
3215   resolve_oop_handle(mirror);
3216 }
3217 
3218 void MacroAssembler::load_method_holder(Register holder, Register method) {
3219   ld(holder, in_bytes(Method::const_offset()), method);
3220   ld(holder, in_bytes(ConstMethod::constants_offset()), holder);
3221   ld(holder, ConstantPool::pool_holder_offset_in_bytes(), holder);
3222 }
3223 
3224 // Clear Array
3225 // For very short arrays. tmp == R0 is allowed.
3226 void MacroAssembler::clear_memory_unrolled(Register base_ptr, int cnt_dwords, Register tmp, int offset) {
3227   if (cnt_dwords &gt; 0) { li(tmp, 0); }
3228   for (int i = 0; i &lt; cnt_dwords; ++i) { std(tmp, offset + i * 8, base_ptr); }
3229 }
3230 
3231 // Version for constant short array length. Kills base_ptr. tmp == R0 is allowed.
3232 void MacroAssembler::clear_memory_constlen(Register base_ptr, int cnt_dwords, Register tmp) {
3233   if (cnt_dwords &lt; 8) {
3234     clear_memory_unrolled(base_ptr, cnt_dwords, tmp);
3235     return;
3236   }
3237 
3238   Label loop;
3239   const long loopcnt   = cnt_dwords &gt;&gt; 1,
3240              remainder = cnt_dwords &amp; 1;
3241 
3242   li(tmp, loopcnt);
3243   mtctr(tmp);
3244   li(tmp, 0);
3245   bind(loop);
3246     std(tmp, 0, base_ptr);
3247     std(tmp, 8, base_ptr);
3248     addi(base_ptr, base_ptr, 16);
3249     bdnz(loop);
3250   if (remainder) { std(tmp, 0, base_ptr); }
3251 }
3252 
3253 // Kills both input registers. tmp == R0 is allowed.
3254 void MacroAssembler::clear_memory_doubleword(Register base_ptr, Register cnt_dwords, Register tmp, long const_cnt) {
3255   // Procedure for large arrays (uses data cache block zero instruction).
3256     Label startloop, fast, fastloop, small_rest, restloop, done;
3257     const int cl_size         = VM_Version::L1_data_cache_line_size(),
3258               cl_dwords       = cl_size &gt;&gt; 3,
3259               cl_dw_addr_bits = exact_log2(cl_dwords),
3260               dcbz_min        = 1,  // Min count of dcbz executions, needs to be &gt;0.
3261               min_cnt         = ((dcbz_min + 1) &lt;&lt; cl_dw_addr_bits) - 1;
3262 
3263   if (const_cnt &gt;= 0) {
3264     // Constant case.
3265     if (const_cnt &lt; min_cnt) {
3266       clear_memory_constlen(base_ptr, const_cnt, tmp);
3267       return;
3268     }
3269     load_const_optimized(cnt_dwords, const_cnt, tmp);
3270   } else {
3271     // cnt_dwords already loaded in register. Need to check size.
3272     cmpdi(CCR1, cnt_dwords, min_cnt); // Big enough? (ensure &gt;= dcbz_min lines included).
3273     blt(CCR1, small_rest);
3274   }
3275     rldicl_(tmp, base_ptr, 64-3, 64-cl_dw_addr_bits); // Extract dword offset within first cache line.
3276     beq(CCR0, fast);                                  // Already 128byte aligned.
3277 
3278     subfic(tmp, tmp, cl_dwords);
3279     mtctr(tmp);                        // Set ctr to hit 128byte boundary (0&lt;ctr&lt;cl_dwords).
3280     subf(cnt_dwords, tmp, cnt_dwords); // rest.
3281     li(tmp, 0);
3282 
3283   bind(startloop);                     // Clear at the beginning to reach 128byte boundary.
3284     std(tmp, 0, base_ptr);             // Clear 8byte aligned block.
3285     addi(base_ptr, base_ptr, 8);
3286     bdnz(startloop);
3287 
3288   bind(fast);                                  // Clear 128byte blocks.
3289     srdi(tmp, cnt_dwords, cl_dw_addr_bits);    // Loop count for 128byte loop (&gt;0).
3290     andi(cnt_dwords, cnt_dwords, cl_dwords-1); // Rest in dwords.
3291     mtctr(tmp);                                // Load counter.
3292 
3293   bind(fastloop);
3294     dcbz(base_ptr);                    // Clear 128byte aligned block.
3295     addi(base_ptr, base_ptr, cl_size);
3296     bdnz(fastloop);
3297 
3298   bind(small_rest);
3299     cmpdi(CCR0, cnt_dwords, 0);        // size 0?
3300     beq(CCR0, done);                   // rest == 0
3301     li(tmp, 0);
3302     mtctr(cnt_dwords);                 // Load counter.
3303 
3304   bind(restloop);                      // Clear rest.
3305     std(tmp, 0, base_ptr);             // Clear 8byte aligned block.
3306     addi(base_ptr, base_ptr, 8);
3307     bdnz(restloop);
3308 
3309   bind(done);
3310 }
3311 
3312 /////////////////////////////////////////// String intrinsics ////////////////////////////////////////////
3313 
3314 #ifdef COMPILER2
3315 // Intrinsics for CompactStrings
3316 
3317 // Compress char[] to byte[] by compressing 16 bytes at once.
3318 void MacroAssembler::string_compress_16(Register src, Register dst, Register cnt,
3319                                         Register tmp1, Register tmp2, Register tmp3, Register tmp4, Register tmp5,
3320                                         Label&amp; Lfailure) {
3321 
3322   const Register tmp0 = R0;
3323   assert_different_registers(src, dst, cnt, tmp0, tmp1, tmp2, tmp3, tmp4, tmp5);
3324   Label Lloop, Lslow;
3325 
3326   // Check if cnt &gt;= 8 (= 16 bytes)
3327   lis(tmp1, 0xFF);                // tmp1 = 0x00FF00FF00FF00FF
3328   srwi_(tmp2, cnt, 3);
3329   beq(CCR0, Lslow);
3330   ori(tmp1, tmp1, 0xFF);
3331   rldimi(tmp1, tmp1, 32, 0);
3332   mtctr(tmp2);
3333 
3334   // 2x unrolled loop
3335   bind(Lloop);
3336   ld(tmp2, 0, src);               // _0_1_2_3 (Big Endian)
3337   ld(tmp4, 8, src);               // _4_5_6_7
3338 
3339   orr(tmp0, tmp2, tmp4);
3340   rldicl(tmp3, tmp2, 6*8, 64-24); // _____1_2
3341   rldimi(tmp2, tmp2, 2*8, 2*8);   // _0_2_3_3
3342   rldicl(tmp5, tmp4, 6*8, 64-24); // _____5_6
3343   rldimi(tmp4, tmp4, 2*8, 2*8);   // _4_6_7_7
3344 
3345   andc_(tmp0, tmp0, tmp1);
3346   bne(CCR0, Lfailure);            // Not latin1.
3347   addi(src, src, 16);
3348 
3349   rlwimi(tmp3, tmp2, 0*8, 24, 31);// _____1_3
3350   srdi(tmp2, tmp2, 3*8);          // ____0_2_
3351   rlwimi(tmp5, tmp4, 0*8, 24, 31);// _____5_7
3352   srdi(tmp4, tmp4, 3*8);          // ____4_6_
3353 
3354   orr(tmp2, tmp2, tmp3);          // ____0123
3355   orr(tmp4, tmp4, tmp5);          // ____4567
3356 
3357   stw(tmp2, 0, dst);
3358   stw(tmp4, 4, dst);
3359   addi(dst, dst, 8);
3360   bdnz(Lloop);
3361 
3362   bind(Lslow);                    // Fallback to slow version
3363 }
3364 
3365 // Compress char[] to byte[]. cnt must be positive int.
3366 void MacroAssembler::string_compress(Register src, Register dst, Register cnt, Register tmp, Label&amp; Lfailure) {
3367   Label Lloop;
3368   mtctr(cnt);
3369 
3370   bind(Lloop);
3371   lhz(tmp, 0, src);
3372   cmplwi(CCR0, tmp, 0xff);
3373   bgt(CCR0, Lfailure);            // Not latin1.
3374   addi(src, src, 2);
3375   stb(tmp, 0, dst);
3376   addi(dst, dst, 1);
3377   bdnz(Lloop);
3378 }
3379 
3380 // Inflate byte[] to char[] by inflating 16 bytes at once.
3381 void MacroAssembler::string_inflate_16(Register src, Register dst, Register cnt,
3382                                        Register tmp1, Register tmp2, Register tmp3, Register tmp4, Register tmp5) {
3383   const Register tmp0 = R0;
3384   assert_different_registers(src, dst, cnt, tmp0, tmp1, tmp2, tmp3, tmp4, tmp5);
3385   Label Lloop, Lslow;
3386 
3387   // Check if cnt &gt;= 8
3388   srwi_(tmp2, cnt, 3);
3389   beq(CCR0, Lslow);
3390   lis(tmp1, 0xFF);                // tmp1 = 0x00FF00FF
3391   ori(tmp1, tmp1, 0xFF);
3392   mtctr(tmp2);
3393 
3394   // 2x unrolled loop
3395   bind(Lloop);
3396   lwz(tmp2, 0, src);              // ____0123 (Big Endian)
3397   lwz(tmp4, 4, src);              // ____4567
3398   addi(src, src, 8);
3399 
3400   rldicl(tmp3, tmp2, 7*8, 64-8);  // _______2
3401   rlwimi(tmp2, tmp2, 3*8, 16, 23);// ____0113
3402   rldicl(tmp5, tmp4, 7*8, 64-8);  // _______6
3403   rlwimi(tmp4, tmp4, 3*8, 16, 23);// ____4557
3404 
3405   andc(tmp0, tmp2, tmp1);         // ____0_1_
3406   rlwimi(tmp2, tmp3, 2*8, 0, 23); // _____2_3
3407   andc(tmp3, tmp4, tmp1);         // ____4_5_
3408   rlwimi(tmp4, tmp5, 2*8, 0, 23); // _____6_7
3409 
3410   rldimi(tmp2, tmp0, 3*8, 0*8);   // _0_1_2_3
3411   rldimi(tmp4, tmp3, 3*8, 0*8);   // _4_5_6_7
3412 
3413   std(tmp2, 0, dst);
3414   std(tmp4, 8, dst);
3415   addi(dst, dst, 16);
3416   bdnz(Lloop);
3417 
3418   bind(Lslow);                    // Fallback to slow version
3419 }
3420 
3421 // Inflate byte[] to char[]. cnt must be positive int.
3422 void MacroAssembler::string_inflate(Register src, Register dst, Register cnt, Register tmp) {
3423   Label Lloop;
3424   mtctr(cnt);
3425 
3426   bind(Lloop);
3427   lbz(tmp, 0, src);
3428   addi(src, src, 1);
3429   sth(tmp, 0, dst);
3430   addi(dst, dst, 2);
3431   bdnz(Lloop);
3432 }
3433 
3434 void MacroAssembler::string_compare(Register str1, Register str2,
3435                                     Register cnt1, Register cnt2,
3436                                     Register tmp1, Register result, int ae) {
3437   const Register tmp0 = R0,
3438                  diff = tmp1;
3439 
3440   assert_different_registers(str1, str2, cnt1, cnt2, tmp0, tmp1, result);
3441   Label Ldone, Lslow, Lloop, Lreturn_diff;
3442 
3443   // Note: Making use of the fact that compareTo(a, b) == -compareTo(b, a)
3444   // we interchange str1 and str2 in the UL case and negate the result.
3445   // Like this, str1 is always latin1 encoded, except for the UU case.
3446   // In addition, we need 0 (or sign which is 0) extend.
3447 
3448   if (ae == StrIntrinsicNode::UU) {
3449     srwi(cnt1, cnt1, 1);
3450   } else {
3451     clrldi(cnt1, cnt1, 32);
3452   }
3453 
3454   if (ae != StrIntrinsicNode::LL) {
3455     srwi(cnt2, cnt2, 1);
3456   } else {
3457     clrldi(cnt2, cnt2, 32);
3458   }
3459 
3460   // See if the lengths are different, and calculate min in cnt1.
3461   // Save diff in case we need it for a tie-breaker.
3462   subf_(diff, cnt2, cnt1); // diff = cnt1 - cnt2
3463   // if (diff &gt; 0) { cnt1 = cnt2; }
3464   if (VM_Version::has_isel()) {
3465     isel(cnt1, CCR0, Assembler::greater, /*invert*/ false, cnt2);
3466   } else {
3467     Label Lskip;
3468     blt(CCR0, Lskip);
3469     mr(cnt1, cnt2);
3470     bind(Lskip);
3471   }
3472 
3473   // Rename registers
3474   Register chr1 = result;
3475   Register chr2 = tmp0;
3476 
3477   // Compare multiple characters in fast loop (only implemented for same encoding).
3478   int stride1 = 8, stride2 = 8;
3479   if (ae == StrIntrinsicNode::LL || ae == StrIntrinsicNode::UU) {
3480     int log2_chars_per_iter = (ae == StrIntrinsicNode::LL) ? 3 : 2;
3481     Label Lfastloop, Lskipfast;
3482 
3483     srwi_(tmp0, cnt1, log2_chars_per_iter);
3484     beq(CCR0, Lskipfast);
3485     rldicl(cnt2, cnt1, 0, 64 - log2_chars_per_iter); // Remaining characters.
3486     li(cnt1, 1 &lt;&lt; log2_chars_per_iter); // Initialize for failure case: Rescan characters from current iteration.
3487     mtctr(tmp0);
3488 
3489     bind(Lfastloop);
3490     ld(chr1, 0, str1);
3491     ld(chr2, 0, str2);
3492     cmpd(CCR0, chr1, chr2);
3493     bne(CCR0, Lslow);
3494     addi(str1, str1, stride1);
3495     addi(str2, str2, stride2);
3496     bdnz(Lfastloop);
3497     mr(cnt1, cnt2); // Remaining characters.
3498     bind(Lskipfast);
3499   }
3500 
3501   // Loop which searches the first difference character by character.
3502   cmpwi(CCR0, cnt1, 0);
3503   beq(CCR0, Lreturn_diff);
3504   bind(Lslow);
3505   mtctr(cnt1);
3506 
3507   switch (ae) {
3508     case StrIntrinsicNode::LL: stride1 = 1; stride2 = 1; break;
3509     case StrIntrinsicNode::UL: // fallthru (see comment above)
3510     case StrIntrinsicNode::LU: stride1 = 1; stride2 = 2; break;
3511     case StrIntrinsicNode::UU: stride1 = 2; stride2 = 2; break;
3512     default: ShouldNotReachHere(); break;
3513   }
3514 
3515   bind(Lloop);
3516   if (stride1 == 1) { lbz(chr1, 0, str1); } else { lhz(chr1, 0, str1); }
3517   if (stride2 == 1) { lbz(chr2, 0, str2); } else { lhz(chr2, 0, str2); }
3518   subf_(result, chr2, chr1); // result = chr1 - chr2
3519   bne(CCR0, Ldone);
3520   addi(str1, str1, stride1);
3521   addi(str2, str2, stride2);
3522   bdnz(Lloop);
3523 
3524   // If strings are equal up to min length, return the length difference.
3525   bind(Lreturn_diff);
3526   mr(result, diff);
3527 
3528   // Otherwise, return the difference between the first mismatched chars.
3529   bind(Ldone);
3530   if (ae == StrIntrinsicNode::UL) {
3531     neg(result, result); // Negate result (see note above).
3532   }
3533 }
3534 
3535 void MacroAssembler::array_equals(bool is_array_equ, Register ary1, Register ary2,
3536                                   Register limit, Register tmp1, Register result, bool is_byte) {
3537   const Register tmp0 = R0;
3538   assert_different_registers(ary1, ary2, limit, tmp0, tmp1, result);
3539   Label Ldone, Lskiploop, Lloop, Lfastloop, Lskipfast;
3540   bool limit_needs_shift = false;
3541 
3542   if (is_array_equ) {
3543     const int length_offset = arrayOopDesc::length_offset_in_bytes();
3544     const int base_offset   = arrayOopDesc::base_offset_in_bytes(is_byte ? T_BYTE : T_CHAR);
3545 
3546     // Return true if the same array.
3547     cmpd(CCR0, ary1, ary2);
3548     beq(CCR0, Lskiploop);
3549 
3550     // Return false if one of them is NULL.
3551     cmpdi(CCR0, ary1, 0);
3552     cmpdi(CCR1, ary2, 0);
3553     li(result, 0);
3554     cror(CCR0, Assembler::equal, CCR1, Assembler::equal);
3555     beq(CCR0, Ldone);
3556 
3557     // Load the lengths of arrays.
3558     lwz(limit, length_offset, ary1);
3559     lwz(tmp0, length_offset, ary2);
3560 
3561     // Return false if the two arrays are not equal length.
3562     cmpw(CCR0, limit, tmp0);
3563     bne(CCR0, Ldone);
3564 
3565     // Load array addresses.
3566     addi(ary1, ary1, base_offset);
3567     addi(ary2, ary2, base_offset);
3568   } else {
3569     limit_needs_shift = !is_byte;
3570     li(result, 0); // Assume not equal.
3571   }
3572 
3573   // Rename registers
3574   Register chr1 = tmp0;
3575   Register chr2 = tmp1;
3576 
3577   // Compare 8 bytes per iteration in fast loop.
3578   const int log2_chars_per_iter = is_byte ? 3 : 2;
3579 
3580   srwi_(tmp0, limit, log2_chars_per_iter + (limit_needs_shift ? 1 : 0));
3581   beq(CCR0, Lskipfast);
3582   mtctr(tmp0);
3583 
3584   bind(Lfastloop);
3585   ld(chr1, 0, ary1);
3586   ld(chr2, 0, ary2);
3587   addi(ary1, ary1, 8);
3588   addi(ary2, ary2, 8);
3589   cmpd(CCR0, chr1, chr2);
3590   bne(CCR0, Ldone);
3591   bdnz(Lfastloop);
3592 
3593   bind(Lskipfast);
3594   rldicl_(limit, limit, limit_needs_shift ? 64 - 1 : 0, 64 - log2_chars_per_iter); // Remaining characters.
3595   beq(CCR0, Lskiploop);
3596   mtctr(limit);
3597 
3598   // Character by character.
3599   bind(Lloop);
3600   if (is_byte) {
3601     lbz(chr1, 0, ary1);
3602     lbz(chr2, 0, ary2);
3603     addi(ary1, ary1, 1);
3604     addi(ary2, ary2, 1);
3605   } else {
3606     lhz(chr1, 0, ary1);
3607     lhz(chr2, 0, ary2);
3608     addi(ary1, ary1, 2);
3609     addi(ary2, ary2, 2);
3610   }
3611   cmpw(CCR0, chr1, chr2);
3612   bne(CCR0, Ldone);
3613   bdnz(Lloop);
3614 
3615   bind(Lskiploop);
3616   li(result, 1); // All characters are equal.
3617   bind(Ldone);
3618 }
3619 
3620 void MacroAssembler::string_indexof(Register result, Register haystack, Register haycnt,
3621                                     Register needle, ciTypeArray* needle_values, Register needlecnt, int needlecntval,
3622                                     Register tmp1, Register tmp2, Register tmp3, Register tmp4, int ae) {
3623 
3624   // Ensure 0&lt;needlecnt&lt;=haycnt in ideal graph as prerequisite!
3625   Label L_TooShort, L_Found, L_NotFound, L_End;
3626   Register last_addr = haycnt, // Kill haycnt at the beginning.
3627   addr      = tmp1,
3628   n_start   = tmp2,
3629   ch1       = tmp3,
3630   ch2       = R0;
3631 
3632   assert(ae != StrIntrinsicNode::LU, &quot;Invalid encoding&quot;);
3633   const int h_csize = (ae == StrIntrinsicNode::LL) ? 1 : 2;
3634   const int n_csize = (ae == StrIntrinsicNode::UU) ? 2 : 1;
3635 
3636   // **************************************************************************************************
3637   // Prepare for main loop: optimized for needle count &gt;=2, bail out otherwise.
3638   // **************************************************************************************************
3639 
3640   // Compute last haystack addr to use if no match gets found.
3641   clrldi(haycnt, haycnt, 32);         // Ensure positive int is valid as 64 bit value.
3642   addi(addr, haystack, -h_csize);     // Accesses use pre-increment.
3643   if (needlecntval == 0) { // variable needlecnt
3644    cmpwi(CCR6, needlecnt, 2);
3645    clrldi(needlecnt, needlecnt, 32);  // Ensure positive int is valid as 64 bit value.
3646    blt(CCR6, L_TooShort);             // Variable needlecnt: handle short needle separately.
3647   }
3648 
3649   if (n_csize == 2) { lwz(n_start, 0, needle); } else { lhz(n_start, 0, needle); } // Load first 2 characters of needle.
3650 
3651   if (needlecntval == 0) { // variable needlecnt
3652    subf(ch1, needlecnt, haycnt);      // Last character index to compare is haycnt-needlecnt.
3653    addi(needlecnt, needlecnt, -2);    // Rest of needle.
3654   } else { // constant needlecnt
3655   guarantee(needlecntval != 1, &quot;IndexOf with single-character needle must be handled separately&quot;);
3656   assert((needlecntval &amp; 0x7fff) == needlecntval, &quot;wrong immediate&quot;);
3657    addi(ch1, haycnt, -needlecntval);  // Last character index to compare is haycnt-needlecnt.
3658    if (needlecntval &gt; 3) { li(needlecnt, needlecntval - 2); } // Rest of needle.
3659   }
3660 
3661   if (h_csize == 2) { slwi(ch1, ch1, 1); } // Scale to number of bytes.
3662 
3663   if (ae ==StrIntrinsicNode::UL) {
3664    srwi(tmp4, n_start, 1*8);          // ___0
3665    rlwimi(n_start, tmp4, 2*8, 0, 23); // _0_1
3666   }
3667 
3668   add(last_addr, haystack, ch1);      // Point to last address to compare (haystack+2*(haycnt-needlecnt)).
3669 
3670   // Main Loop (now we have at least 2 characters).
3671   Label L_OuterLoop, L_InnerLoop, L_FinalCheck, L_Comp1, L_Comp2;
3672   bind(L_OuterLoop); // Search for 1st 2 characters.
3673   Register addr_diff = tmp4;
3674    subf(addr_diff, addr, last_addr);  // Difference between already checked address and last address to check.
3675    addi(addr, addr, h_csize);         // This is the new address we want to use for comparing.
3676    srdi_(ch2, addr_diff, h_csize);
3677    beq(CCR0, L_FinalCheck);           // 2 characters left?
3678    mtctr(ch2);                        // num of characters / 2
3679   bind(L_InnerLoop);                  // Main work horse (2x unrolled search loop)
3680    if (h_csize == 2) {                // Load 2 characters of haystack (ignore alignment).
3681     lwz(ch1, 0, addr);
3682     lwz(ch2, 2, addr);
3683    } else {
3684     lhz(ch1, 0, addr);
3685     lhz(ch2, 1, addr);
3686    }
3687    cmpw(CCR0, ch1, n_start);          // Compare 2 characters (1 would be sufficient but try to reduce branches to CompLoop).
3688    cmpw(CCR1, ch2, n_start);
3689    beq(CCR0, L_Comp1);                // Did we find the needle start?
3690    beq(CCR1, L_Comp2);
3691    addi(addr, addr, 2 * h_csize);
3692    bdnz(L_InnerLoop);
3693   bind(L_FinalCheck);
3694    andi_(addr_diff, addr_diff, h_csize); // Remaining characters not covered by InnerLoop: (num of characters) &amp; 1.
3695    beq(CCR0, L_NotFound);
3696    if (h_csize == 2) { lwz(ch1, 0, addr); } else { lhz(ch1, 0, addr); } // One position left at which we have to compare.
3697    cmpw(CCR1, ch1, n_start);
3698    beq(CCR1, L_Comp1);
3699   bind(L_NotFound);
3700    li(result, -1);                    // not found
3701    b(L_End);
3702 
3703    // **************************************************************************************************
3704    // Special Case: unfortunately, the variable needle case can be called with needlecnt&lt;2
3705    // **************************************************************************************************
3706   if (needlecntval == 0) {           // We have to handle these cases separately.
3707   Label L_OneCharLoop;
3708   bind(L_TooShort);
3709    mtctr(haycnt);
3710    if (n_csize == 2) { lhz(n_start, 0, needle); } else { lbz(n_start, 0, needle); } // First character of needle
3711   bind(L_OneCharLoop);
3712    if (h_csize == 2) { lhzu(ch1, 2, addr); } else { lbzu(ch1, 1, addr); }
3713    cmpw(CCR1, ch1, n_start);
3714    beq(CCR1, L_Found);               // Did we find the one character needle?
3715    bdnz(L_OneCharLoop);
3716    li(result, -1);                   // Not found.
3717    b(L_End);
3718   }
3719 
3720   // **************************************************************************************************
3721   // Regular Case Part II: compare rest of needle (first 2 characters have been compared already)
3722   // **************************************************************************************************
3723 
3724   // Compare the rest
3725   bind(L_Comp2);
3726    addi(addr, addr, h_csize);        // First comparison has failed, 2nd one hit.
3727   bind(L_Comp1);                     // Addr points to possible needle start.
3728   if (needlecntval != 2) {           // Const needlecnt==2?
3729    if (needlecntval != 3) {
3730     if (needlecntval == 0) { beq(CCR6, L_Found); } // Variable needlecnt==2?
3731     Register n_ind = tmp4,
3732              h_ind = n_ind;
3733     li(n_ind, 2 * n_csize);          // First 2 characters are already compared, use index 2.
3734     mtctr(needlecnt);                // Decremented by 2, still &gt; 0.
3735    Label L_CompLoop;
3736    bind(L_CompLoop);
3737     if (ae ==StrIntrinsicNode::UL) {
3738       h_ind = ch1;
3739       sldi(h_ind, n_ind, 1);
3740     }
3741     if (n_csize == 2) { lhzx(ch2, needle, n_ind); } else { lbzx(ch2, needle, n_ind); }
3742     if (h_csize == 2) { lhzx(ch1, addr, h_ind); } else { lbzx(ch1, addr, h_ind); }
3743     cmpw(CCR1, ch1, ch2);
3744     bne(CCR1, L_OuterLoop);
3745     addi(n_ind, n_ind, n_csize);
3746     bdnz(L_CompLoop);
3747    } else { // No loop required if there&#39;s only one needle character left.
3748     if (n_csize == 2) { lhz(ch2, 2 * 2, needle); } else { lbz(ch2, 2 * 1, needle); }
3749     if (h_csize == 2) { lhz(ch1, 2 * 2, addr); } else { lbz(ch1, 2 * 1, addr); }
3750     cmpw(CCR1, ch1, ch2);
3751     bne(CCR1, L_OuterLoop);
3752    }
3753   }
3754   // Return index ...
3755   bind(L_Found);
3756    subf(result, haystack, addr);     // relative to haystack, ...
3757    if (h_csize == 2) { srdi(result, result, 1); } // in characters.
3758   bind(L_End);
3759 } // string_indexof
3760 
3761 void MacroAssembler::string_indexof_char(Register result, Register haystack, Register haycnt,
3762                                          Register needle, jchar needleChar, Register tmp1, Register tmp2, bool is_byte) {
3763   assert_different_registers(haystack, haycnt, needle, tmp1, tmp2);
3764 
3765   Label L_InnerLoop, L_FinalCheck, L_Found1, L_Found2, L_NotFound, L_End;
3766   Register addr = tmp1,
3767            ch1 = tmp2,
3768            ch2 = R0;
3769 
3770   const int h_csize = is_byte ? 1 : 2;
3771 
3772 //4:
3773    srwi_(tmp2, haycnt, 1);   // Shift right by exact_log2(UNROLL_FACTOR).
3774    mr(addr, haystack);
3775    beq(CCR0, L_FinalCheck);
3776    mtctr(tmp2);              // Move to count register.
3777 //8:
3778   bind(L_InnerLoop);         // Main work horse (2x unrolled search loop).
3779    if (!is_byte) {
3780     lhz(ch1, 0, addr);
3781     lhz(ch2, 2, addr);
3782    } else {
3783     lbz(ch1, 0, addr);
3784     lbz(ch2, 1, addr);
3785    }
3786    (needle != R0) ? cmpw(CCR0, ch1, needle) : cmplwi(CCR0, ch1, (unsigned int)needleChar);
3787    (needle != R0) ? cmpw(CCR1, ch2, needle) : cmplwi(CCR1, ch2, (unsigned int)needleChar);
3788    beq(CCR0, L_Found1);      // Did we find the needle?
3789    beq(CCR1, L_Found2);
3790    addi(addr, addr, 2 * h_csize);
3791    bdnz(L_InnerLoop);
3792 //16:
3793   bind(L_FinalCheck);
3794    andi_(R0, haycnt, 1);
3795    beq(CCR0, L_NotFound);
3796    if (!is_byte) { lhz(ch1, 0, addr); } else { lbz(ch1, 0, addr); } // One position left at which we have to compare.
3797    (needle != R0) ? cmpw(CCR1, ch1, needle) : cmplwi(CCR1, ch1, (unsigned int)needleChar);
3798    beq(CCR1, L_Found1);
3799 //21:
3800   bind(L_NotFound);
3801    li(result, -1);           // Not found.
3802    b(L_End);
3803 
3804   bind(L_Found2);
3805    addi(addr, addr, h_csize);
3806 //24:
3807   bind(L_Found1);            // Return index ...
3808    subf(result, haystack, addr); // relative to haystack, ...
3809    if (!is_byte) { srdi(result, result, 1); } // in characters.
3810   bind(L_End);
3811 } // string_indexof_char
3812 
3813 
3814 void MacroAssembler::has_negatives(Register src, Register cnt, Register result,
3815                                    Register tmp1, Register tmp2) {
3816   const Register tmp0 = R0;
3817   assert_different_registers(src, result, cnt, tmp0, tmp1, tmp2);
3818   Label Lfastloop, Lslow, Lloop, Lnoneg, Ldone;
3819 
3820   // Check if cnt &gt;= 8 (= 16 bytes)
3821   lis(tmp1, (int)(short)0x8080);  // tmp1 = 0x8080808080808080
3822   srwi_(tmp2, cnt, 4);
3823   li(result, 1);                  // Assume there&#39;s a negative byte.
3824   beq(CCR0, Lslow);
3825   ori(tmp1, tmp1, 0x8080);
3826   rldimi(tmp1, tmp1, 32, 0);
3827   mtctr(tmp2);
3828 
3829   // 2x unrolled loop
3830   bind(Lfastloop);
3831   ld(tmp2, 0, src);
3832   ld(tmp0, 8, src);
3833 
3834   orr(tmp0, tmp2, tmp0);
3835 
3836   and_(tmp0, tmp0, tmp1);
3837   bne(CCR0, Ldone);               // Found negative byte.
3838   addi(src, src, 16);
3839 
3840   bdnz(Lfastloop);
3841 
3842   bind(Lslow);                    // Fallback to slow version
3843   rldicl_(tmp0, cnt, 0, 64-4);
3844   beq(CCR0, Lnoneg);
3845   mtctr(tmp0);
3846   bind(Lloop);
3847   lbz(tmp0, 0, src);
3848   addi(src, src, 1);
3849   andi_(tmp0, tmp0, 0x80);
3850   bne(CCR0, Ldone);               // Found negative byte.
3851   bdnz(Lloop);
3852   bind(Lnoneg);
3853   li(result, 0);
3854 
3855   bind(Ldone);
3856 }
3857 
3858 #endif // Compiler2
3859 
3860 // Helpers for Intrinsic Emitters
3861 //
3862 // Revert the byte order of a 32bit value in a register
3863 //   src: 0x44556677
3864 //   dst: 0x77665544
3865 // Three steps to obtain the result:
3866 //  1) Rotate src (as doubleword) left 5 bytes. That puts the leftmost byte of the src word
3867 //     into the rightmost byte position. Afterwards, everything left of the rightmost byte is cleared.
3868 //     This value initializes dst.
3869 //  2) Rotate src (as word) left 3 bytes. That puts the rightmost byte of the src word into the leftmost
3870 //     byte position. Furthermore, byte 5 is rotated into byte 6 position where it is supposed to go.
3871 //     This value is mask inserted into dst with a [0..23] mask of 1s.
3872 //  3) Rotate src (as word) left 1 byte. That puts byte 6 into byte 5 position.
3873 //     This value is mask inserted into dst with a [8..15] mask of 1s.
3874 void MacroAssembler::load_reverse_32(Register dst, Register src) {
3875   assert_different_registers(dst, src);
3876 
3877   rldicl(dst, src, (4+1)*8, 56);       // Rotate byte 4 into position 7 (rightmost), clear all to the left.
3878   rlwimi(dst, src,     3*8,  0, 23);   // Insert byte 5 into position 6, 7 into 4, leave pos 7 alone.
3879   rlwimi(dst, src,     1*8,  8, 15);   // Insert byte 6 into position 5, leave the rest alone.
3880 }
3881 
3882 // Calculate the column addresses of the crc32 lookup table into distinct registers.
3883 // This loop-invariant calculation is moved out of the loop body, reducing the loop
3884 // body size from 20 to 16 instructions.
3885 // Returns the offset that was used to calculate the address of column tc3.
3886 // Due to register shortage, setting tc3 may overwrite table. With the return offset
3887 // at hand, the original table address can be easily reconstructed.
3888 int MacroAssembler::crc32_table_columns(Register table, Register tc0, Register tc1, Register tc2, Register tc3) {
3889   assert(!VM_Version::has_vpmsumb(), &quot;Vector version should be used instead!&quot;);
3890 
3891   // Point to 4 byte folding tables (byte-reversed version for Big Endian)
3892   // Layout: See StubRoutines::generate_crc_constants.
3893 #ifdef VM_LITTLE_ENDIAN
3894   const int ix0 = 3 * CRC32_TABLE_SIZE;
3895   const int ix1 = 2 * CRC32_TABLE_SIZE;
3896   const int ix2 = 1 * CRC32_TABLE_SIZE;
3897   const int ix3 = 0 * CRC32_TABLE_SIZE;
3898 #else
3899   const int ix0 = 1 * CRC32_TABLE_SIZE;
3900   const int ix1 = 2 * CRC32_TABLE_SIZE;
3901   const int ix2 = 3 * CRC32_TABLE_SIZE;
3902   const int ix3 = 4 * CRC32_TABLE_SIZE;
3903 #endif
3904   assert_different_registers(table, tc0, tc1, tc2);
3905   assert(table == tc3, &quot;must be!&quot;);
3906 
3907   addi(tc0, table, ix0);
3908   addi(tc1, table, ix1);
3909   addi(tc2, table, ix2);
3910   if (ix3 != 0) addi(tc3, table, ix3);
3911 
3912   return ix3;
3913 }
3914 
3915 /**
3916  * uint32_t crc;
3917  * table[crc &amp; 0xFF] ^ (crc &gt;&gt; 8);
3918  */
3919 void MacroAssembler::fold_byte_crc32(Register crc, Register val, Register table, Register tmp) {
3920   assert_different_registers(crc, table, tmp);
3921   assert_different_registers(val, table);
3922 
3923   if (crc == val) {                   // Must rotate first to use the unmodified value.
3924     rlwinm(tmp, val, 2, 24-2, 31-2);  // Insert (rightmost) byte 7 of val, shifted left by 2, into byte 6..7 of tmp, clear the rest.
3925                                       // As we use a word (4-byte) instruction, we have to adapt the mask bit positions.
3926     srwi(crc, crc, 8);                // Unsigned shift, clear leftmost 8 bits.
3927   } else {
3928     srwi(crc, crc, 8);                // Unsigned shift, clear leftmost 8 bits.
3929     rlwinm(tmp, val, 2, 24-2, 31-2);  // Insert (rightmost) byte 7 of val, shifted left by 2, into byte 6..7 of tmp, clear the rest.
3930   }
3931   lwzx(tmp, table, tmp);
3932   xorr(crc, crc, tmp);
3933 }
3934 
3935 /**
3936  * Emits code to update CRC-32 with a byte value according to constants in table.
3937  *
3938  * @param [in,out]crc   Register containing the crc.
3939  * @param [in]val       Register containing the byte to fold into the CRC.
3940  * @param [in]table     Register containing the table of crc constants.
3941  *
3942  * uint32_t crc;
3943  * val = crc_table[(val ^ crc) &amp; 0xFF];
3944  * crc = val ^ (crc &gt;&gt; 8);
3945  */
3946 void MacroAssembler::update_byte_crc32(Register crc, Register val, Register table) {
3947   BLOCK_COMMENT(&quot;update_byte_crc32:&quot;);
3948   xorr(val, val, crc);
3949   fold_byte_crc32(crc, val, table, val);
3950 }
3951 
3952 /**
3953  * @param crc   register containing existing CRC (32-bit)
3954  * @param buf   register pointing to input byte buffer (byte*)
3955  * @param len   register containing number of bytes
3956  * @param table register pointing to CRC table
3957  */
3958 void MacroAssembler::update_byteLoop_crc32(Register crc, Register buf, Register len, Register table,
3959                                            Register data, bool loopAlignment) {
3960   assert_different_registers(crc, buf, len, table, data);
3961 
3962   Label L_mainLoop, L_done;
3963   const int mainLoop_stepping  = 1;
3964   const int mainLoop_alignment = loopAlignment ? 32 : 4; // (InputForNewCode &gt; 4 ? InputForNewCode : 32) : 4;
3965 
3966   // Process all bytes in a single-byte loop.
3967   clrldi_(len, len, 32);                         // Enforce 32 bit. Anything to do?
3968   beq(CCR0, L_done);
3969 
3970   mtctr(len);
3971   align(mainLoop_alignment);
3972   BIND(L_mainLoop);
3973     lbz(data, 0, buf);                           // Byte from buffer, zero-extended.
3974     addi(buf, buf, mainLoop_stepping);           // Advance buffer position.
3975     update_byte_crc32(crc, data, table);
3976     bdnz(L_mainLoop);                            // Iterate.
3977 
3978   bind(L_done);
3979 }
3980 
3981 /**
3982  * Emits code to update CRC-32 with a 4-byte value according to constants in table
3983  * Implementation according to jdk/src/share/native/java/util/zip/zlib-1.2.8/crc32.c
3984  */
3985 // A note on the lookup table address(es):
3986 // The implementation uses 4 table columns (byte-reversed versions for Big Endian).
3987 // To save the effort of adding the column offset to the table address each time
3988 // a table element is looked up, it is possible to pass the pre-calculated
3989 // column addresses.
3990 // Uses R9..R12 as work register. Must be saved/restored by caller, if necessary.
3991 void MacroAssembler::update_1word_crc32(Register crc, Register buf, Register table, int bufDisp, int bufInc,
3992                                         Register t0,  Register t1,  Register t2,  Register t3,
3993                                         Register tc0, Register tc1, Register tc2, Register tc3) {
3994   assert_different_registers(crc, t3);
3995 
3996   // XOR crc with next four bytes of buffer.
3997   lwz(t3, bufDisp, buf);
3998   if (bufInc != 0) {
3999     addi(buf, buf, bufInc);
4000   }
4001   xorr(t3, t3, crc);
4002 
4003   // Chop crc into 4 single-byte pieces, shifted left 2 bits, to form the table indices.
4004   rlwinm(t0, t3,  2,         24-2, 31-2);  // ((t1 &gt;&gt;  0) &amp; 0xff) &lt;&lt; 2
4005   rlwinm(t1, t3,  32+(2- 8), 24-2, 31-2);  // ((t1 &gt;&gt;  8) &amp; 0xff) &lt;&lt; 2
4006   rlwinm(t2, t3,  32+(2-16), 24-2, 31-2);  // ((t1 &gt;&gt; 16) &amp; 0xff) &lt;&lt; 2
4007   rlwinm(t3, t3,  32+(2-24), 24-2, 31-2);  // ((t1 &gt;&gt; 24) &amp; 0xff) &lt;&lt; 2
4008 
4009   // Use the pre-calculated column addresses.
4010   // Load pre-calculated table values.
4011   lwzx(t0, tc0, t0);
4012   lwzx(t1, tc1, t1);
4013   lwzx(t2, tc2, t2);
4014   lwzx(t3, tc3, t3);
4015 
4016   // Calculate new crc from table values.
4017   xorr(t0,  t0, t1);
4018   xorr(t2,  t2, t3);
4019   xorr(crc, t0, t2);  // Now crc contains the final checksum value.
4020 }
4021 
4022 /**
4023  * @param crc   register containing existing CRC (32-bit)
4024  * @param buf   register pointing to input byte buffer (byte*)
4025  * @param len   register containing number of bytes
4026  * @param table register pointing to CRC table
4027  *
4028  * uses R9..R12 as work register. Must be saved/restored by caller!
4029  */
4030 void MacroAssembler::kernel_crc32_1word(Register crc, Register buf, Register len, Register table,
4031                                         Register t0,  Register t1,  Register t2,  Register t3,
4032                                         Register tc0, Register tc1, Register tc2, Register tc3,
4033                                         bool invertCRC) {
4034   assert_different_registers(crc, buf, len, table);
4035 
4036   Label L_mainLoop, L_tail;
4037   Register  tmp          = t0;
4038   Register  data         = t0;
4039   Register  tmp2         = t1;
4040   const int mainLoop_stepping  = 4;
4041   const int tailLoop_stepping  = 1;
4042   const int log_stepping       = exact_log2(mainLoop_stepping);
4043   const int mainLoop_alignment = 32; // InputForNewCode &gt; 4 ? InputForNewCode : 32;
4044   const int complexThreshold   = 2*mainLoop_stepping;
4045 
4046   // Don&#39;t test for len &lt;= 0 here. This pathological case should not occur anyway.
4047   // Optimizing for it by adding a test and a branch seems to be a waste of CPU cycles
4048   // for all well-behaved cases. The situation itself is detected and handled correctly
4049   // within update_byteLoop_crc32.
4050   assert(tailLoop_stepping == 1, &quot;check tailLoop_stepping!&quot;);
4051 
4052   BLOCK_COMMENT(&quot;kernel_crc32_1word {&quot;);
4053 
4054   if (invertCRC) {
4055     nand(crc, crc, crc);                      // 1s complement of crc
4056   }
4057 
4058   // Check for short (&lt;mainLoop_stepping) buffer.
4059   cmpdi(CCR0, len, complexThreshold);
4060   blt(CCR0, L_tail);
4061 
4062   // Pre-mainLoop alignment did show a slight (1%) positive effect on performance.
4063   // We leave the code in for reference. Maybe we need alignment when we exploit vector instructions.
4064   {
4065     // Align buf addr to mainLoop_stepping boundary.
4066     neg(tmp2, buf);                              // Calculate # preLoop iterations for alignment.
4067     rldicl(tmp2, tmp2, 0, 64-log_stepping);      // Rotate tmp2 0 bits, insert into tmp2, anding with mask with 1s from 62..63.
4068 
4069     if (complexThreshold &gt; mainLoop_stepping) {
4070       sub(len, len, tmp2);                       // Remaining bytes for main loop (&gt;=mainLoop_stepping is guaranteed).
4071     } else {
4072       sub(tmp, len, tmp2);                       // Remaining bytes for main loop.
4073       cmpdi(CCR0, tmp, mainLoop_stepping);
4074       blt(CCR0, L_tail);                         // For less than one mainloop_stepping left, do only tail processing
4075       mr(len, tmp);                              // remaining bytes for main loop (&gt;=mainLoop_stepping is guaranteed).
4076     }
4077     update_byteLoop_crc32(crc, buf, tmp2, table, data, false);
4078   }
4079 
4080   srdi(tmp2, len, log_stepping);                 // #iterations for mainLoop
4081   andi(len, len, mainLoop_stepping-1);           // remaining bytes for tailLoop
4082   mtctr(tmp2);
4083 
4084 #ifdef VM_LITTLE_ENDIAN
4085   Register crc_rv = crc;
4086 #else
4087   Register crc_rv = tmp;                         // Load_reverse needs separate registers to work on.
4088                                                  // Occupies tmp, but frees up crc.
4089   load_reverse_32(crc_rv, crc);                  // Revert byte order because we are dealing with big-endian data.
4090   tmp = crc;
4091 #endif
4092 
4093   int reconstructTableOffset = crc32_table_columns(table, tc0, tc1, tc2, tc3);
4094 
4095   align(mainLoop_alignment);                     // Octoword-aligned loop address. Shows 2% improvement.
4096   BIND(L_mainLoop);
4097     update_1word_crc32(crc_rv, buf, table, 0, mainLoop_stepping, crc_rv, t1, t2, t3, tc0, tc1, tc2, tc3);
4098     bdnz(L_mainLoop);
4099 
4100 #ifndef VM_LITTLE_ENDIAN
4101   load_reverse_32(crc, crc_rv);                  // Revert byte order because we are dealing with big-endian data.
4102   tmp = crc_rv;                                  // Tmp uses it&#39;s original register again.
4103 #endif
4104 
4105   // Restore original table address for tailLoop.
4106   if (reconstructTableOffset != 0) {
4107     addi(table, table, -reconstructTableOffset);
4108   }
4109 
4110   // Process last few (&lt;complexThreshold) bytes of buffer.
4111   BIND(L_tail);
4112   update_byteLoop_crc32(crc, buf, len, table, data, false);
4113 
4114   if (invertCRC) {
4115     nand(crc, crc, crc);                      // 1s complement of crc
4116   }
4117   BLOCK_COMMENT(&quot;} kernel_crc32_1word&quot;);
4118 }
4119 
4120 /**
4121  * @param crc             register containing existing CRC (32-bit)
4122  * @param buf             register pointing to input byte buffer (byte*)
4123  * @param len             register containing number of bytes
4124  * @param constants       register pointing to precomputed constants
4125  * @param t0-t6           temp registers
4126  */
4127 void MacroAssembler::kernel_crc32_vpmsum(Register crc, Register buf, Register len, Register constants,
4128                                          Register t0, Register t1, Register t2, Register t3,
4129                                          Register t4, Register t5, Register t6, bool invertCRC) {
4130   assert_different_registers(crc, buf, len, constants);
4131 
4132   Label L_tail;
4133 
4134   BLOCK_COMMENT(&quot;kernel_crc32_vpmsum {&quot;);
4135 
4136   if (invertCRC) {
4137     nand(crc, crc, crc);                      // 1s complement of crc
4138   }
4139 
4140   // Enforce 32 bit.
4141   clrldi(len, len, 32);
4142 
4143   // Align if we have enough bytes for the fast version.
4144   const int alignment = 16,
4145             threshold = 32;
4146   Register prealign = t0;
4147 
4148   neg(prealign, buf);
4149   addi(t1, len, -threshold);
4150   andi(prealign, prealign, alignment - 1);
4151   cmpw(CCR0, t1, prealign);
4152   blt(CCR0, L_tail); // len - prealign &lt; threshold?
4153 
4154   subf(len, prealign, len);
4155   update_byteLoop_crc32(crc, buf, prealign, constants, t2, false);
4156 
4157   // Calculate from first aligned address as far as possible.
4158   addi(constants, constants, CRC32_TABLE_SIZE); // Point to vector constants.
4159   kernel_crc32_vpmsum_aligned(crc, buf, len, constants, t0, t1, t2, t3, t4, t5, t6);
4160   addi(constants, constants, -CRC32_TABLE_SIZE); // Point to table again.
4161 
4162   // Remaining bytes.
4163   BIND(L_tail);
4164   update_byteLoop_crc32(crc, buf, len, constants, t2, false);
4165 
4166   if (invertCRC) {
4167     nand(crc, crc, crc);                      // 1s complement of crc
4168   }
4169 
4170   BLOCK_COMMENT(&quot;} kernel_crc32_vpmsum&quot;);
4171 }
4172 
4173 /**
4174  * @param crc             register containing existing CRC (32-bit)
4175  * @param buf             register pointing to input byte buffer (byte*)
4176  * @param len             register containing number of bytes (will get updated to remaining bytes)
4177  * @param constants       register pointing to CRC table for 128-bit aligned memory
4178  * @param t0-t6           temp registers
4179  */
4180 void MacroAssembler::kernel_crc32_vpmsum_aligned(Register crc, Register buf, Register len, Register constants,
4181     Register t0, Register t1, Register t2, Register t3, Register t4, Register t5, Register t6) {
4182 
4183   // Save non-volatile vector registers (frameless).
4184   Register offset = t1;
4185   int offsetInt = 0;
4186   offsetInt -= 16; li(offset, offsetInt); stvx(VR20, offset, R1_SP);
4187   offsetInt -= 16; li(offset, offsetInt); stvx(VR21, offset, R1_SP);
4188   offsetInt -= 16; li(offset, offsetInt); stvx(VR22, offset, R1_SP);
4189   offsetInt -= 16; li(offset, offsetInt); stvx(VR23, offset, R1_SP);
4190   offsetInt -= 16; li(offset, offsetInt); stvx(VR24, offset, R1_SP);
4191   offsetInt -= 16; li(offset, offsetInt); stvx(VR25, offset, R1_SP);
4192 #ifndef VM_LITTLE_ENDIAN
4193   offsetInt -= 16; li(offset, offsetInt); stvx(VR26, offset, R1_SP);
4194 #endif
4195   offsetInt -= 8; std(R14, offsetInt, R1_SP);
4196   offsetInt -= 8; std(R15, offsetInt, R1_SP);
4197 
4198   // Implementation uses an inner loop which uses between 256 and 16 * unroll_factor
4199   // bytes per iteration. The basic scheme is:
4200   // lvx: load vector (Big Endian needs reversal)
4201   // vpmsumw: carry-less 32 bit multiplications with constant representing a large CRC shift
4202   // vxor: xor partial results together to get unroll_factor2 vectors
4203 
4204   // Outer loop performs the CRC shifts needed to combine the unroll_factor2 vectors.
4205 
4206   // Using 16 * unroll_factor / unroll_factor_2 bytes for constants.
4207   const int unroll_factor = CRC32_UNROLL_FACTOR,
4208             unroll_factor2 = CRC32_UNROLL_FACTOR2;
4209 
4210   const int outer_consts_size = (unroll_factor2 - 1) * 16,
4211             inner_consts_size = (unroll_factor / unroll_factor2) * 16;
4212 
4213   // Support registers.
4214   Register offs[] = { noreg, t0, t1, t2, t3, t4, t5, t6 };
4215   Register num_bytes = R14,
4216            loop_count = R15,
4217            cur_const = crc; // will live in VCRC
4218   // Constant array for outer loop: unroll_factor2 - 1 registers,
4219   // Constant array for inner loop: unroll_factor / unroll_factor2 registers.
4220   VectorRegister consts0[] = { VR16, VR17, VR18, VR19, VR20, VR21, VR22 },
4221                  consts1[] = { VR23, VR24 };
4222   // Data register arrays: 2 arrays with unroll_factor2 registers.
4223   VectorRegister data0[] = { VR0, VR1, VR2, VR3, VR4, VR5, VR6, VR7 },
4224                  data1[] = { VR8, VR9, VR10, VR11, VR12, VR13, VR14, VR15 };
4225 
4226   VectorRegister VCRC = data0[0];
4227   VectorRegister Vc = VR25;
4228   VectorRegister swap_bytes = VR26; // Only for Big Endian.
4229 
4230   // We have at least 1 iteration (ensured by caller).
4231   Label L_outer_loop, L_inner_loop, L_last;
4232 
4233   // If supported set DSCR pre-fetch to deepest.
4234   if (VM_Version::has_mfdscr()) {
4235     load_const_optimized(t0, VM_Version::_dscr_val | 7);
4236     mtdscr(t0);
4237   }
4238 
4239   mtvrwz(VCRC, crc); // crc lives in VCRC, now
4240 
4241   for (int i = 1; i &lt; unroll_factor2; ++i) {
4242     li(offs[i], 16 * i);
4243   }
4244 
4245   // Load consts for outer loop
4246   lvx(consts0[0], constants);
4247   for (int i = 1; i &lt; unroll_factor2 - 1; ++i) {
4248     lvx(consts0[i], offs[i], constants);
4249   }
4250 
4251   load_const_optimized(num_bytes, 16 * unroll_factor);
4252 
4253   // Reuse data registers outside of the loop.
4254   VectorRegister Vtmp = data1[0];
4255   VectorRegister Vtmp2 = data1[1];
4256   VectorRegister zeroes = data1[2];
4257 
4258   vspltisb(Vtmp, 0);
4259   vsldoi(VCRC, Vtmp, VCRC, 8); // 96 bit zeroes, 32 bit CRC.
4260 
4261   // Load vector for vpermxor (to xor both 64 bit parts together)
4262   lvsl(Vtmp, buf);   // 000102030405060708090a0b0c0d0e0f
4263   vspltisb(Vc, 4);
4264   vsl(Vc, Vtmp, Vc); // 00102030405060708090a0b0c0d0e0f0
4265   xxspltd(Vc-&gt;to_vsr(), Vc-&gt;to_vsr(), 0);
4266   vor(Vc, Vtmp, Vc); // 001122334455667708192a3b4c5d6e7f
4267 
4268 #ifdef VM_LITTLE_ENDIAN
4269 #define BE_swap_bytes(x)
4270 #else
4271   vspltisb(Vtmp2, 0xf);
4272   vxor(swap_bytes, Vtmp, Vtmp2);
4273 #define BE_swap_bytes(x) vperm(x, x, x, swap_bytes)
4274 #endif
4275 
4276   cmpd(CCR0, len, num_bytes);
4277   blt(CCR0, L_last);
4278 
4279   addi(cur_const, constants, outer_consts_size); // Point to consts for inner loop
4280   load_const_optimized(loop_count, unroll_factor / (2 * unroll_factor2) - 1); // One double-iteration peeled off.
4281 
4282   // ********** Main loop start **********
4283   align(32);
4284   bind(L_outer_loop);
4285 
4286   // Begin of unrolled first iteration (no xor).
4287   lvx(data1[0], buf);
4288   for (int i = 1; i &lt; unroll_factor2 / 2; ++i) {
4289     lvx(data1[i], offs[i], buf);
4290   }
4291   vpermxor(VCRC, VCRC, VCRC, Vc); // xor both halves to 64 bit result.
4292   lvx(consts1[0], cur_const);
4293   mtctr(loop_count);
4294   for (int i = 0; i &lt; unroll_factor2 / 2; ++i) {
4295     BE_swap_bytes(data1[i]);
4296     if (i == 0) { vxor(data1[0], data1[0], VCRC); } // xor in previous CRC.
4297     lvx(data1[i + unroll_factor2 / 2], offs[i + unroll_factor2 / 2], buf);
4298     vpmsumw(data0[i], data1[i], consts1[0]);
4299   }
4300   addi(buf, buf, 16 * unroll_factor2);
4301   subf(len, num_bytes, len);
4302   lvx(consts1[1], offs[1], cur_const);
4303   addi(cur_const, cur_const, 32);
4304   // Begin of unrolled second iteration (head).
4305   for (int i = 0; i &lt; unroll_factor2 / 2; ++i) {
4306     BE_swap_bytes(data1[i + unroll_factor2 / 2]);
4307     if (i == 0) { lvx(data1[0], buf); } else { lvx(data1[i], offs[i], buf); }
4308     vpmsumw(data0[i + unroll_factor2 / 2], data1[i + unroll_factor2 / 2], consts1[0]);
4309   }
4310   for (int i = 0; i &lt; unroll_factor2 / 2; ++i) {
4311     BE_swap_bytes(data1[i]);
4312     lvx(data1[i + unroll_factor2 / 2], offs[i + unroll_factor2 / 2], buf);
4313     vpmsumw(data1[i], data1[i], consts1[1]);
4314   }
4315   addi(buf, buf, 16 * unroll_factor2);
4316 
4317   // Generate most performance relevant code. Loads + half of the vpmsumw have been generated.
4318   // Double-iteration allows using the 2 constant registers alternatingly.
4319   align(32);
4320   bind(L_inner_loop);
4321   for (int j = 1; j &lt; 3; ++j) { // j &lt; unroll_factor / unroll_factor2 - 1 for complete unrolling.
4322     if (j &amp; 1) {
4323       lvx(consts1[0], cur_const);
4324     } else {
4325       lvx(consts1[1], offs[1], cur_const);
4326       addi(cur_const, cur_const, 32);
4327     }
4328     for (int i = 0; i &lt; unroll_factor2; ++i) {
4329       int idx = i + unroll_factor2 / 2, inc = 0; // For modulo-scheduled input.
4330       if (idx &gt;= unroll_factor2) { idx -= unroll_factor2; inc = 1; }
4331       BE_swap_bytes(data1[idx]);
4332       vxor(data0[i], data0[i], data1[i]);
4333       if (i == 0) lvx(data1[0], buf); else lvx(data1[i], offs[i], buf);
4334       vpmsumw(data1[idx], data1[idx], consts1[(j + inc) &amp; 1]);
4335     }
4336     addi(buf, buf, 16 * unroll_factor2);
4337   }
4338   bdnz(L_inner_loop);
4339 
4340   addi(cur_const, constants, outer_consts_size); // Reset
4341 
4342   // Tail of last iteration (no loads).
4343   for (int i = 0; i &lt; unroll_factor2 / 2; ++i) {
4344     BE_swap_bytes(data1[i + unroll_factor2 / 2]);
4345     vxor(data0[i], data0[i], data1[i]);
4346     vpmsumw(data1[i + unroll_factor2 / 2], data1[i + unroll_factor2 / 2], consts1[1]);
4347   }
4348   for (int i = 0; i &lt; unroll_factor2 / 2; ++i) {
4349     vpmsumw(data0[i], data0[i], consts0[unroll_factor2 - 2 - i]); // First half of fixup shifts.
4350     vxor(data0[i + unroll_factor2 / 2], data0[i + unroll_factor2 / 2], data1[i + unroll_factor2 / 2]);
4351   }
4352 
4353   // Last data register is ok, other ones need fixup shift.
4354   for (int i = unroll_factor2 / 2; i &lt; unroll_factor2 - 1; ++i) {
4355     vpmsumw(data0[i], data0[i], consts0[unroll_factor2 - 2 - i]);
4356   }
4357 
4358   // Combine to 128 bit result vector VCRC = data0[0].
4359   for (int i = 1; i &lt; unroll_factor2; i&lt;&lt;=1) {
4360     for (int j = 0; j &lt;= unroll_factor2 - 2*i; j+=2*i) {
4361       vxor(data0[j], data0[j], data0[j+i]);
4362     }
4363   }
4364   cmpd(CCR0, len, num_bytes);
4365   bge(CCR0, L_outer_loop);
4366 
4367   // Last chance with lower num_bytes.
4368   bind(L_last);
4369   srdi(loop_count, len, exact_log2(16 * 2 * unroll_factor2)); // Use double-iterations.
4370   // Point behind last const for inner loop.
4371   add_const_optimized(cur_const, constants, outer_consts_size + inner_consts_size);
4372   sldi(R0, loop_count, exact_log2(16 * 2)); // Bytes of constants to be used.
4373   clrrdi(num_bytes, len, exact_log2(16 * 2 * unroll_factor2));
4374   subf(cur_const, R0, cur_const); // Point to constant to be used first.
4375 
4376   addic_(loop_count, loop_count, -1); // One double-iteration peeled off.
4377   bgt(CCR0, L_outer_loop);
4378   // ********** Main loop end **********
4379 
4380   // Restore DSCR pre-fetch value.
4381   if (VM_Version::has_mfdscr()) {
4382     load_const_optimized(t0, VM_Version::_dscr_val);
4383     mtdscr(t0);
4384   }
4385 
4386   // ********** Simple loop for remaining 16 byte blocks **********
4387   {
4388     Label L_loop, L_done;
4389 
4390     srdi_(t0, len, 4); // 16 bytes per iteration
4391     clrldi(len, len, 64-4);
4392     beq(CCR0, L_done);
4393 
4394     // Point to const (same as last const for inner loop).
4395     add_const_optimized(cur_const, constants, outer_consts_size + inner_consts_size - 16);
4396     mtctr(t0);
4397     lvx(Vtmp2, cur_const);
4398 
4399     align(32);
4400     bind(L_loop);
4401 
4402     lvx(Vtmp, buf);
4403     addi(buf, buf, 16);
4404     vpermxor(VCRC, VCRC, VCRC, Vc); // xor both halves to 64 bit result.
4405     BE_swap_bytes(Vtmp);
4406     vxor(VCRC, VCRC, Vtmp);
4407     vpmsumw(VCRC, VCRC, Vtmp2);
4408     bdnz(L_loop);
4409 
4410     bind(L_done);
4411   }
4412   // ********** Simple loop end **********
4413 #undef BE_swap_bytes
4414 
4415   // Point to Barrett constants
4416   add_const_optimized(cur_const, constants, outer_consts_size + inner_consts_size);
4417 
4418   vspltisb(zeroes, 0);
4419 
4420   // Combine to 64 bit result.
4421   vpermxor(VCRC, VCRC, VCRC, Vc); // xor both halves to 64 bit result.
4422 
4423   // Reduce to 32 bit CRC: Remainder by multiply-high.
4424   lvx(Vtmp, cur_const);
4425   vsldoi(Vtmp2, zeroes, VCRC, 12);  // Extract high 32 bit.
4426   vpmsumd(Vtmp2, Vtmp2, Vtmp);      // Multiply by inverse long poly.
4427   vsldoi(Vtmp2, zeroes, Vtmp2, 12); // Extract high 32 bit.
4428   vsldoi(Vtmp, zeroes, Vtmp, 8);
4429   vpmsumd(Vtmp2, Vtmp2, Vtmp);      // Multiply quotient by long poly.
4430   vxor(VCRC, VCRC, Vtmp2);          // Remainder fits into 32 bit.
4431 
4432   // Move result. len is already updated.
4433   vsldoi(VCRC, VCRC, zeroes, 8);
4434   mfvrd(crc, VCRC);
4435 
4436   // Restore non-volatile Vector registers (frameless).
4437   offsetInt = 0;
4438   offsetInt -= 16; li(offset, offsetInt); lvx(VR20, offset, R1_SP);
4439   offsetInt -= 16; li(offset, offsetInt); lvx(VR21, offset, R1_SP);
4440   offsetInt -= 16; li(offset, offsetInt); lvx(VR22, offset, R1_SP);
4441   offsetInt -= 16; li(offset, offsetInt); lvx(VR23, offset, R1_SP);
4442   offsetInt -= 16; li(offset, offsetInt); lvx(VR24, offset, R1_SP);
4443   offsetInt -= 16; li(offset, offsetInt); lvx(VR25, offset, R1_SP);
4444 #ifndef VM_LITTLE_ENDIAN
4445   offsetInt -= 16; li(offset, offsetInt); lvx(VR26, offset, R1_SP);
4446 #endif
4447   offsetInt -= 8;  ld(R14, offsetInt, R1_SP);
4448   offsetInt -= 8;  ld(R15, offsetInt, R1_SP);
4449 }
4450 
4451 void MacroAssembler::crc32(Register crc, Register buf, Register len, Register t0, Register t1, Register t2,
4452                            Register t3, Register t4, Register t5, Register t6, Register t7, bool is_crc32c) {
4453   load_const_optimized(t0, is_crc32c ? StubRoutines::crc32c_table_addr()
4454                                      : StubRoutines::crc_table_addr()   , R0);
4455 
4456   if (VM_Version::has_vpmsumb()) {
4457     kernel_crc32_vpmsum(crc, buf, len, t0, t1, t2, t3, t4, t5, t6, t7, !is_crc32c);
4458   } else {
4459     kernel_crc32_1word(crc, buf, len, t0, t1, t2, t3, t4, t5, t6, t7, t0, !is_crc32c);
4460   }
4461 }
4462 
4463 void MacroAssembler::kernel_crc32_singleByteReg(Register crc, Register val, Register table, bool invertCRC) {
4464   assert_different_registers(crc, val, table);
4465 
4466   BLOCK_COMMENT(&quot;kernel_crc32_singleByteReg:&quot;);
4467   if (invertCRC) {
4468     nand(crc, crc, crc);                // 1s complement of crc
4469   }
4470 
4471   update_byte_crc32(crc, val, table);
4472 
4473   if (invertCRC) {
4474     nand(crc, crc, crc);                // 1s complement of crc
4475   }
4476 }
4477 
4478 // dest_lo += src1 + src2
4479 // dest_hi += carry1 + carry2
4480 void MacroAssembler::add2_with_carry(Register dest_hi,
4481                                      Register dest_lo,
4482                                      Register src1, Register src2) {
4483   li(R0, 0);
4484   addc(dest_lo, dest_lo, src1);
4485   adde(dest_hi, dest_hi, R0);
4486   addc(dest_lo, dest_lo, src2);
4487   adde(dest_hi, dest_hi, R0);
4488 }
4489 
4490 // Multiply 64 bit by 64 bit first loop.
4491 void MacroAssembler::multiply_64_x_64_loop(Register x, Register xstart,
4492                                            Register x_xstart,
4493                                            Register y, Register y_idx,
4494                                            Register z,
4495                                            Register carry,
4496                                            Register product_high, Register product,
4497                                            Register idx, Register kdx,
4498                                            Register tmp) {
4499   //  jlong carry, x[], y[], z[];
4500   //  for (int idx=ystart, kdx=ystart+1+xstart; idx &gt;= 0; idx--, kdx--) {
4501   //    huge_128 product = y[idx] * x[xstart] + carry;
4502   //    z[kdx] = (jlong)product;
4503   //    carry  = (jlong)(product &gt;&gt;&gt; 64);
4504   //  }
4505   //  z[xstart] = carry;
4506 
4507   Label L_first_loop, L_first_loop_exit;
4508   Label L_one_x, L_one_y, L_multiply;
4509 
4510   addic_(xstart, xstart, -1);
4511   blt(CCR0, L_one_x);   // Special case: length of x is 1.
4512 
4513   // Load next two integers of x.
4514   sldi(tmp, xstart, LogBytesPerInt);
4515   ldx(x_xstart, x, tmp);
4516 #ifdef VM_LITTLE_ENDIAN
4517   rldicl(x_xstart, x_xstart, 32, 0);
4518 #endif
4519 
4520   align(32, 16);
4521   bind(L_first_loop);
4522 
4523   cmpdi(CCR0, idx, 1);
4524   blt(CCR0, L_first_loop_exit);
4525   addi(idx, idx, -2);
4526   beq(CCR0, L_one_y);
4527 
4528   // Load next two integers of y.
4529   sldi(tmp, idx, LogBytesPerInt);
4530   ldx(y_idx, y, tmp);
4531 #ifdef VM_LITTLE_ENDIAN
4532   rldicl(y_idx, y_idx, 32, 0);
4533 #endif
4534 
4535 
4536   bind(L_multiply);
4537   multiply64(product_high, product, x_xstart, y_idx);
4538 
4539   li(tmp, 0);
4540   addc(product, product, carry);         // Add carry to result.
4541   adde(product_high, product_high, tmp); // Add carry of the last addition.
4542   addi(kdx, kdx, -2);
4543 
4544   // Store result.
4545 #ifdef VM_LITTLE_ENDIAN
4546   rldicl(product, product, 32, 0);
4547 #endif
4548   sldi(tmp, kdx, LogBytesPerInt);
4549   stdx(product, z, tmp);
4550   mr_if_needed(carry, product_high);
4551   b(L_first_loop);
4552 
4553 
4554   bind(L_one_y); // Load one 32 bit portion of y as (0,value).
4555 
4556   lwz(y_idx, 0, y);
4557   b(L_multiply);
4558 
4559 
4560   bind(L_one_x); // Load one 32 bit portion of x as (0,value).
4561 
4562   lwz(x_xstart, 0, x);
4563   b(L_first_loop);
4564 
4565   bind(L_first_loop_exit);
4566 }
4567 
4568 // Multiply 64 bit by 64 bit and add 128 bit.
4569 void MacroAssembler::multiply_add_128_x_128(Register x_xstart, Register y,
4570                                             Register z, Register yz_idx,
4571                                             Register idx, Register carry,
4572                                             Register product_high, Register product,
4573                                             Register tmp, int offset) {
4574 
4575   //  huge_128 product = (y[idx] * x_xstart) + z[kdx] + carry;
4576   //  z[kdx] = (jlong)product;
4577 
4578   sldi(tmp, idx, LogBytesPerInt);
4579   if (offset) {
4580     addi(tmp, tmp, offset);
4581   }
4582   ldx(yz_idx, y, tmp);
4583 #ifdef VM_LITTLE_ENDIAN
4584   rldicl(yz_idx, yz_idx, 32, 0);
4585 #endif
4586 
4587   multiply64(product_high, product, x_xstart, yz_idx);
4588   ldx(yz_idx, z, tmp);
4589 #ifdef VM_LITTLE_ENDIAN
4590   rldicl(yz_idx, yz_idx, 32, 0);
4591 #endif
4592 
4593   add2_with_carry(product_high, product, carry, yz_idx);
4594 
4595   sldi(tmp, idx, LogBytesPerInt);
4596   if (offset) {
4597     addi(tmp, tmp, offset);
4598   }
4599 #ifdef VM_LITTLE_ENDIAN
4600   rldicl(product, product, 32, 0);
4601 #endif
4602   stdx(product, z, tmp);
4603 }
4604 
4605 // Multiply 128 bit by 128 bit. Unrolled inner loop.
4606 void MacroAssembler::multiply_128_x_128_loop(Register x_xstart,
4607                                              Register y, Register z,
4608                                              Register yz_idx, Register idx, Register carry,
4609                                              Register product_high, Register product,
4610                                              Register carry2, Register tmp) {
4611 
4612   //  jlong carry, x[], y[], z[];
4613   //  int kdx = ystart+1;
4614   //  for (int idx=ystart-2; idx &gt;= 0; idx -= 2) { // Third loop
4615   //    huge_128 product = (y[idx+1] * x_xstart) + z[kdx+idx+1] + carry;
4616   //    z[kdx+idx+1] = (jlong)product;
4617   //    jlong carry2 = (jlong)(product &gt;&gt;&gt; 64);
4618   //    product = (y[idx] * x_xstart) + z[kdx+idx] + carry2;
4619   //    z[kdx+idx] = (jlong)product;
4620   //    carry = (jlong)(product &gt;&gt;&gt; 64);
4621   //  }
4622   //  idx += 2;
4623   //  if (idx &gt; 0) {
4624   //    product = (y[idx] * x_xstart) + z[kdx+idx] + carry;
4625   //    z[kdx+idx] = (jlong)product;
4626   //    carry = (jlong)(product &gt;&gt;&gt; 64);
4627   //  }
4628 
4629   Label L_third_loop, L_third_loop_exit, L_post_third_loop_done;
4630   const Register jdx = R0;
4631 
4632   // Scale the index.
4633   srdi_(jdx, idx, 2);
4634   beq(CCR0, L_third_loop_exit);
4635   mtctr(jdx);
4636 
4637   align(32, 16);
4638   bind(L_third_loop);
4639 
4640   addi(idx, idx, -4);
4641 
4642   multiply_add_128_x_128(x_xstart, y, z, yz_idx, idx, carry, product_high, product, tmp, 8);
4643   mr_if_needed(carry2, product_high);
4644 
4645   multiply_add_128_x_128(x_xstart, y, z, yz_idx, idx, carry2, product_high, product, tmp, 0);
4646   mr_if_needed(carry, product_high);
4647   bdnz(L_third_loop);
4648 
4649   bind(L_third_loop_exit);  // Handle any left-over operand parts.
4650 
4651   andi_(idx, idx, 0x3);
4652   beq(CCR0, L_post_third_loop_done);
4653 
4654   Label L_check_1;
4655 
4656   addic_(idx, idx, -2);
4657   blt(CCR0, L_check_1);
4658 
4659   multiply_add_128_x_128(x_xstart, y, z, yz_idx, idx, carry, product_high, product, tmp, 0);
4660   mr_if_needed(carry, product_high);
4661 
4662   bind(L_check_1);
4663 
4664   addi(idx, idx, 0x2);
4665   andi_(idx, idx, 0x1);
4666   addic_(idx, idx, -1);
4667   blt(CCR0, L_post_third_loop_done);
4668 
4669   sldi(tmp, idx, LogBytesPerInt);
4670   lwzx(yz_idx, y, tmp);
4671   multiply64(product_high, product, x_xstart, yz_idx);
4672   lwzx(yz_idx, z, tmp);
4673 
4674   add2_with_carry(product_high, product, yz_idx, carry);
4675 
4676   sldi(tmp, idx, LogBytesPerInt);
4677   stwx(product, z, tmp);
4678   srdi(product, product, 32);
4679 
4680   sldi(product_high, product_high, 32);
4681   orr(product, product, product_high);
4682   mr_if_needed(carry, product);
4683 
4684   bind(L_post_third_loop_done);
4685 }   // multiply_128_x_128_loop
4686 
4687 void MacroAssembler::muladd(Register out, Register in,
4688                             Register offset, Register len, Register k,
4689                             Register tmp1, Register tmp2, Register carry) {
4690 
4691   // Labels
4692   Label LOOP, SKIP;
4693 
4694   // Make sure length is positive.
4695   cmpdi  (CCR0,    len,     0);
4696 
4697   // Prepare variables
4698   subi   (offset,  offset,  4);
4699   li     (carry,   0);
4700   ble    (CCR0,    SKIP);
4701 
4702   mtctr  (len);
4703   subi   (len,     len,     1    );
4704   sldi   (len,     len,     2    );
4705 
4706   // Main loop
4707   bind(LOOP);
4708   lwzx   (tmp1,    len,     in   );
4709   lwzx   (tmp2,    offset,  out  );
4710   mulld  (tmp1,    tmp1,    k    );
4711   add    (tmp2,    carry,   tmp2 );
4712   add    (tmp2,    tmp1,    tmp2 );
4713   stwx   (tmp2,    offset,  out  );
4714   srdi   (carry,   tmp2,    32   );
4715   subi   (offset,  offset,  4    );
4716   subi   (len,     len,     4    );
4717   bdnz   (LOOP);
4718   bind(SKIP);
4719 }
4720 
4721 void MacroAssembler::multiply_to_len(Register x, Register xlen,
4722                                      Register y, Register ylen,
4723                                      Register z, Register zlen,
4724                                      Register tmp1, Register tmp2,
4725                                      Register tmp3, Register tmp4,
4726                                      Register tmp5, Register tmp6,
4727                                      Register tmp7, Register tmp8,
4728                                      Register tmp9, Register tmp10,
4729                                      Register tmp11, Register tmp12,
4730                                      Register tmp13) {
4731 
4732   ShortBranchVerifier sbv(this);
4733 
4734   assert_different_registers(x, xlen, y, ylen, z, zlen,
4735                              tmp1, tmp2, tmp3, tmp4, tmp5, tmp6);
4736   assert_different_registers(x, xlen, y, ylen, z, zlen,
4737                              tmp1, tmp2, tmp3, tmp4, tmp5, tmp7);
4738   assert_different_registers(x, xlen, y, ylen, z, zlen,
4739                              tmp1, tmp2, tmp3, tmp4, tmp5, tmp8);
4740 
4741   const Register idx = tmp1;
4742   const Register kdx = tmp2;
4743   const Register xstart = tmp3;
4744 
4745   const Register y_idx = tmp4;
4746   const Register carry = tmp5;
4747   const Register product = tmp6;
4748   const Register product_high = tmp7;
4749   const Register x_xstart = tmp8;
4750   const Register tmp = tmp9;
4751 
4752   // First Loop.
4753   //
4754   //  final static long LONG_MASK = 0xffffffffL;
4755   //  int xstart = xlen - 1;
4756   //  int ystart = ylen - 1;
4757   //  long carry = 0;
4758   //  for (int idx=ystart, kdx=ystart+1+xstart; idx &gt;= 0; idx-, kdx--) {
4759   //    long product = (y[idx] &amp; LONG_MASK) * (x[xstart] &amp; LONG_MASK) + carry;
4760   //    z[kdx] = (int)product;
4761   //    carry = product &gt;&gt;&gt; 32;
4762   //  }
4763   //  z[xstart] = (int)carry;
4764 
4765   mr_if_needed(idx, ylen);        // idx = ylen
4766   mr_if_needed(kdx, zlen);        // kdx = xlen + ylen
4767   li(carry, 0);                   // carry = 0
4768 
4769   Label L_done;
4770 
4771   addic_(xstart, xlen, -1);
4772   blt(CCR0, L_done);
4773 
4774   multiply_64_x_64_loop(x, xstart, x_xstart, y, y_idx, z,
4775                         carry, product_high, product, idx, kdx, tmp);
4776 
4777   Label L_second_loop;
4778 
4779   cmpdi(CCR0, kdx, 0);
4780   beq(CCR0, L_second_loop);
4781 
4782   Label L_carry;
4783 
4784   addic_(kdx, kdx, -1);
4785   beq(CCR0, L_carry);
4786 
4787   // Store lower 32 bits of carry.
4788   sldi(tmp, kdx, LogBytesPerInt);
4789   stwx(carry, z, tmp);
4790   srdi(carry, carry, 32);
4791   addi(kdx, kdx, -1);
4792 
4793 
4794   bind(L_carry);
4795 
4796   // Store upper 32 bits of carry.
4797   sldi(tmp, kdx, LogBytesPerInt);
4798   stwx(carry, z, tmp);
4799 
4800   // Second and third (nested) loops.
4801   //
4802   //  for (int i = xstart-1; i &gt;= 0; i--) { // Second loop
4803   //    carry = 0;
4804   //    for (int jdx=ystart, k=ystart+1+i; jdx &gt;= 0; jdx--, k--) { // Third loop
4805   //      long product = (y[jdx] &amp; LONG_MASK) * (x[i] &amp; LONG_MASK) +
4806   //                     (z[k] &amp; LONG_MASK) + carry;
4807   //      z[k] = (int)product;
4808   //      carry = product &gt;&gt;&gt; 32;
4809   //    }
4810   //    z[i] = (int)carry;
4811   //  }
4812   //
4813   //  i = xlen, j = tmp1, k = tmp2, carry = tmp5, x[i] = rdx
4814 
4815   bind(L_second_loop);
4816 
4817   li(carry, 0);                   // carry = 0;
4818 
4819   addic_(xstart, xstart, -1);     // i = xstart-1;
4820   blt(CCR0, L_done);
4821 
4822   Register zsave = tmp10;
4823 
4824   mr(zsave, z);
4825 
4826 
4827   Label L_last_x;
4828 
4829   sldi(tmp, xstart, LogBytesPerInt);
4830   add(z, z, tmp);                 // z = z + k - j
4831   addi(z, z, 4);
4832   addic_(xstart, xstart, -1);     // i = xstart-1;
4833   blt(CCR0, L_last_x);
4834 
4835   sldi(tmp, xstart, LogBytesPerInt);
4836   ldx(x_xstart, x, tmp);
4837 #ifdef VM_LITTLE_ENDIAN
4838   rldicl(x_xstart, x_xstart, 32, 0);
4839 #endif
4840 
4841 
4842   Label L_third_loop_prologue;
4843 
4844   bind(L_third_loop_prologue);
4845 
4846   Register xsave = tmp11;
4847   Register xlensave = tmp12;
4848   Register ylensave = tmp13;
4849 
4850   mr(xsave, x);
4851   mr(xlensave, xstart);
4852   mr(ylensave, ylen);
4853 
4854 
4855   multiply_128_x_128_loop(x_xstart, y, z, y_idx, ylen,
4856                           carry, product_high, product, x, tmp);
4857 
4858   mr(z, zsave);
4859   mr(x, xsave);
4860   mr(xlen, xlensave);   // This is the decrement of the loop counter!
4861   mr(ylen, ylensave);
4862 
4863   addi(tmp3, xlen, 1);
4864   sldi(tmp, tmp3, LogBytesPerInt);
4865   stwx(carry, z, tmp);
4866   addic_(tmp3, tmp3, -1);
4867   blt(CCR0, L_done);
4868 
4869   srdi(carry, carry, 32);
4870   sldi(tmp, tmp3, LogBytesPerInt);
4871   stwx(carry, z, tmp);
4872   b(L_second_loop);
4873 
4874   // Next infrequent code is moved outside loops.
4875   bind(L_last_x);
4876 
4877   lwz(x_xstart, 0, x);
4878   b(L_third_loop_prologue);
4879 
4880   bind(L_done);
4881 }   // multiply_to_len
4882 
4883 void MacroAssembler::asm_assert(bool check_equal, const char *msg, int id) {
4884 #ifdef ASSERT
4885   Label ok;
4886   if (check_equal) {
4887     beq(CCR0, ok);
4888   } else {
4889     bne(CCR0, ok);
4890   }
4891   stop(msg, id);
4892   bind(ok);
4893 #endif
4894 }
4895 
4896 void MacroAssembler::asm_assert_mems_zero(bool check_equal, int size, int mem_offset,
4897                                           Register mem_base, const char* msg, int id) {
4898 #ifdef ASSERT
4899   switch (size) {
4900     case 4:
4901       lwz(R0, mem_offset, mem_base);
4902       cmpwi(CCR0, R0, 0);
4903       break;
4904     case 8:
4905       ld(R0, mem_offset, mem_base);
4906       cmpdi(CCR0, R0, 0);
4907       break;
4908     default:
4909       ShouldNotReachHere();
4910   }
4911   asm_assert(check_equal, msg, id);
4912 #endif // ASSERT
4913 }
4914 
4915 void MacroAssembler::verify_thread() {
4916   if (VerifyThread) {
4917     unimplemented(&quot;&#39;VerifyThread&#39; currently not implemented on PPC&quot;);
4918   }
4919 }
4920 
4921 void MacroAssembler::verify_coop(Register coop, const char* msg) {
4922   if (!VerifyOops) { return; }
4923   if (UseCompressedOops) { decode_heap_oop(coop); }
4924   verify_oop(coop, msg);
4925   if (UseCompressedOops) { encode_heap_oop(coop, coop); }
4926 }
4927 
4928 // READ: oop. KILL: R0. Volatile floats perhaps.
4929 void MacroAssembler::verify_oop(Register oop, const char* msg) {
4930   if (!VerifyOops) {
4931     return;
4932   }
4933 
4934   address/* FunctionDescriptor** */fd = StubRoutines::verify_oop_subroutine_entry_address();
4935   const Register tmp = R11; // Will be preserved.
4936   const int nbytes_save = MacroAssembler::num_volatile_regs * 8;
4937 
4938   BLOCK_COMMENT(&quot;verify_oop {&quot;);
4939 
4940   save_volatile_gprs(R1_SP, -nbytes_save); // except R0
4941 
4942   mr_if_needed(R4_ARG2, oop);
4943   save_LR_CR(tmp); // save in old frame
4944   push_frame_reg_args(nbytes_save, tmp);
4945   // load FunctionDescriptor** / entry_address *
4946   load_const_optimized(tmp, fd, R0);
4947   // load FunctionDescriptor* / entry_address
4948   ld(tmp, 0, tmp);
4949   load_const_optimized(R3_ARG1, (address)msg, R0);
4950   // Call destination for its side effect.
4951   call_c(tmp);
4952 
4953   pop_frame();
4954   restore_LR_CR(tmp);
4955   restore_volatile_gprs(R1_SP, -nbytes_save); // except R0
4956 
4957   BLOCK_COMMENT(&quot;} verify_oop&quot;);
4958 }
4959 
4960 void MacroAssembler::verify_oop_addr(RegisterOrConstant offs, Register base, const char* msg) {
4961   if (!VerifyOops) {
4962     return;
4963   }
4964 
4965   address/* FunctionDescriptor** */fd = StubRoutines::verify_oop_subroutine_entry_address();
4966   const Register tmp = R11; // Will be preserved.
4967   const int nbytes_save = MacroAssembler::num_volatile_regs * 8;
4968   save_volatile_gprs(R1_SP, -nbytes_save); // except R0
4969 
4970   ld(R4_ARG2, offs, base);
4971   save_LR_CR(tmp); // save in old frame
4972   push_frame_reg_args(nbytes_save, tmp);
4973   // load FunctionDescriptor** / entry_address *
4974   load_const_optimized(tmp, fd, R0);
4975   // load FunctionDescriptor* / entry_address
4976   ld(tmp, 0, tmp);
4977   load_const_optimized(R3_ARG1, (address)msg, R0);
4978   // Call destination for its side effect.
4979   call_c(tmp);
4980 
4981   pop_frame();
4982   restore_LR_CR(tmp);
4983   restore_volatile_gprs(R1_SP, -nbytes_save); // except R0
4984 }
4985 
4986 const char* stop_types[] = {
4987   &quot;stop&quot;,
4988   &quot;untested&quot;,
4989   &quot;unimplemented&quot;,
4990   &quot;shouldnotreachhere&quot;
4991 };
4992 
4993 static void stop_on_request(int tp, const char* msg) {
4994   tty-&gt;print(&quot;PPC assembly code requires stop: (%s) %s\n&quot;, stop_types[tp%/*stop_end*/4], msg);
4995   guarantee(false, &quot;PPC assembly code requires stop: %s&quot;, msg);
4996 }
4997 
4998 // Call a C-function that prints output.
4999 void MacroAssembler::stop(int type, const char* msg, int id) {
5000 #ifndef PRODUCT
5001   block_comment(err_msg(&quot;stop: %s %s {&quot;, stop_types[type%stop_end], msg));
5002 #else
5003   block_comment(&quot;stop {&quot;);
5004 #endif
5005 
5006   // setup arguments
5007   load_const_optimized(R3_ARG1, type);
5008   load_const_optimized(R4_ARG2, (void *)msg, /*tmp=*/R0);
5009   call_VM_leaf(CAST_FROM_FN_PTR(address, stop_on_request), R3_ARG1, R4_ARG2);
5010   illtrap();
5011   emit_int32(id);
5012   block_comment(&quot;} stop;&quot;);
5013 }
5014 
5015 #ifndef PRODUCT
5016 // Write pattern 0x0101010101010101 in memory region [low-before, high+after].
5017 // Val, addr are temp registers.
5018 // If low == addr, addr is killed.
5019 // High is preserved.
5020 void MacroAssembler::zap_from_to(Register low, int before, Register high, int after, Register val, Register addr) {
5021   if (!ZapMemory) return;
5022 
5023   assert_different_registers(low, val);
5024 
5025   BLOCK_COMMENT(&quot;zap memory region {&quot;);
5026   load_const_optimized(val, 0x0101010101010101);
5027   int size = before + after;
5028   if (low == high &amp;&amp; size &lt; 5 &amp;&amp; size &gt; 0) {
5029     int offset = -before*BytesPerWord;
5030     for (int i = 0; i &lt; size; ++i) {
5031       std(val, offset, low);
5032       offset += (1*BytesPerWord);
5033     }
5034   } else {
5035     addi(addr, low, -before*BytesPerWord);
5036     assert_different_registers(high, val);
5037     if (after) addi(high, high, after * BytesPerWord);
5038     Label loop;
5039     bind(loop);
5040     std(val, 0, addr);
5041     addi(addr, addr, 8);
5042     cmpd(CCR6, addr, high);
5043     ble(CCR6, loop);
5044     if (after) addi(high, high, -after * BytesPerWord);  // Correct back to old value.
5045   }
5046   BLOCK_COMMENT(&quot;} zap memory region&quot;);
5047 }
5048 
5049 #endif // !PRODUCT
5050 
5051 void SkipIfEqualZero::skip_to_label_if_equal_zero(MacroAssembler* masm, Register temp,
5052                                                   const bool* flag_addr, Label&amp; label) {
5053   int simm16_offset = masm-&gt;load_const_optimized(temp, (address)flag_addr, R0, true);
5054   assert(sizeof(bool) == 1, &quot;PowerPC ABI&quot;);
5055   masm-&gt;lbz(temp, simm16_offset, temp);
5056   masm-&gt;cmpwi(CCR0, temp, 0);
5057   masm-&gt;beq(CCR0, label);
5058 }
5059 
5060 SkipIfEqualZero::SkipIfEqualZero(MacroAssembler* masm, Register temp, const bool* flag_addr) : _masm(masm), _label() {
5061   skip_to_label_if_equal_zero(masm, temp, flag_addr, _label);
5062 }
5063 
5064 SkipIfEqualZero::~SkipIfEqualZero() {
5065   _masm-&gt;bind(_label);
5066 }
5067 
5068 void MacroAssembler::cache_wb(Address line) {
5069   assert(line.index() == noreg, &quot;index should be noreg&quot;);
5070   assert(line.disp() == 0, &quot;displacement should be 0&quot;);
5071   assert(VM_Version::supports_data_cache_line_flush(), &quot;CPU or OS does not support flush to persistent memory&quot;);
5072   // Data Cache Store, not really a flush, so it works like a sync of cache
5073   // line and persistent mem, i.e. copying the cache line to persistent whilst
5074   // not invalidating the cache line.
5075   dcbst(line.base());
5076 }
5077 
5078 void MacroAssembler::cache_wbsync(bool is_presync) {
5079   assert(VM_Version::supports_data_cache_line_flush(), &quot;CPU or OS does not support sync related to persistent memory&quot;);
5080   // We only need a post sync barrier. Post means _after_ a cache line flush or
5081   // store instruction, pre means a barrier emitted before such a instructions.
5082   if (!is_presync) {
5083     fence();
5084   }
5085 }
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="2" type="hidden" />
</body>
</html>