<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/matcher.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="macroArrayCopy.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="matcher.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/matcher.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1993 
1994 
1995 bool Matcher::is_bmi_pattern(Node *n, Node *m) {
1996   if (n != NULL &amp;&amp; m != NULL) {
1997     if (m-&gt;Opcode() == Op_LoadI) {
1998       FusedPatternMatcher&lt;TypeInt&gt; bmii(n, m, Op_ConI);
1999       return bmii.match(Op_AndI, -1, Op_SubI,  1,  0)  ||
2000              bmii.match(Op_AndI, -1, Op_AddI, -1, -1)  ||
2001              bmii.match(Op_XorI, -1, Op_AddI, -1, -1);
2002     } else if (m-&gt;Opcode() == Op_LoadL) {
2003       FusedPatternMatcher&lt;TypeLong&gt; bmil(n, m, Op_ConL);
2004       return bmil.match(Op_AndL, -1, Op_SubL,  1,  0) ||
2005              bmil.match(Op_AndL, -1, Op_AddL, -1, -1) ||
2006              bmil.match(Op_XorL, -1, Op_AddL, -1, -1);
2007     }
2008   }
2009   return false;
2010 }
2011 #endif // X86
2012 









2013 bool Matcher::clone_base_plus_offset_address(AddPNode* m, Matcher::MStack&amp; mstack, VectorSet&amp; address_visited) {
2014   Node *off = m-&gt;in(AddPNode::Offset);
2015   if (off-&gt;is_Con()) {
2016     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
2017     mstack.push(m-&gt;in(AddPNode::Address), Pre_Visit);
2018     // Clone X+offset as it also folds into most addressing expressions
2019     mstack.push(off, Visit);
2020     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
2021     return true;
2022   }
2023   return false;
2024 }
2025 
2026 // A method-klass-holder may be passed in the inline_cache_reg
2027 // and then expanded into the inline_cache_reg and a method_oop register
2028 //   defined in ad_&lt;arch&gt;.cpp
2029 
2030 //------------------------------find_shared------------------------------------
2031 // Set bits if Node is shared or otherwise a root
2032 void Matcher::find_shared( Node *n ) {
</pre>
<hr />
<pre>
2073         uint mop = m-&gt;Opcode();
2074 
2075         // Must clone all producers of flags, or we will not match correctly.
2076         // Suppose a compare setting int-flags is shared (e.g., a switch-tree)
2077         // then it will match into an ideal Op_RegFlags.  Alas, the fp-flags
2078         // are also there, so we may match a float-branch to int-flags and
2079         // expect the allocator to haul the flags from the int-side to the
2080         // fp-side.  No can do.
2081         if( _must_clone[mop] ) {
2082           mstack.push(m, Visit);
2083           continue; // for(int i = ...)
2084         }
2085 
2086         // if &#39;n&#39; and &#39;m&#39; are part of a graph for BMI instruction, clone this node.
2087 #ifdef X86
2088         if (UseBMI1Instructions &amp;&amp; is_bmi_pattern(n, m)) {
2089           mstack.push(m, Visit);
2090           continue;
2091         }
2092 #endif




2093 
2094         // Clone addressing expressions as they are &quot;free&quot; in memory access instructions
2095         if (mem_op &amp;&amp; i == mem_addr_idx &amp;&amp; mop == Op_AddP &amp;&amp;
2096             // When there are other uses besides address expressions
2097             // put it on stack and mark as shared.
2098             !is_visited(m)) {
2099           // Some inputs for address expression are not put on stack
2100           // to avoid marking them as shared and forcing them into register
2101           // if they are used only in address expressions.
2102           // But they should be marked as shared if there are other uses
2103           // besides address expressions.
2104 
2105           if (clone_address_expressions(m-&gt;as_AddP(), mstack, address_visited)) {
2106             continue;
2107           }
2108         }   // if( mem_op &amp;&amp;
2109         mstack.push(m, Pre_Visit);
2110       }     // for(int i = ...)
2111     }
2112     else if (nstate == Alt_Post_Visit) {
</pre>
<hr />
<pre>
2508   // Handle generic vector operand case
2509   if (Matcher::supports_generic_vector_operands &amp;&amp; t-&gt;isa_vect()) {
2510     specialize_mach_node(mspill);
2511   }
2512   return &amp;mspill-&gt;out_RegMask();
2513 }
2514 
2515 // Process Mach IR right after selection phase is over.
2516 void Matcher::do_postselect_cleanup() {
2517   if (supports_generic_vector_operands) {
2518     specialize_generic_vector_operands();
2519     if (C-&gt;failing())  return;
2520   }
2521 }
2522 
2523 //----------------------------------------------------------------------
2524 // Generic machine operands elision.
2525 //----------------------------------------------------------------------
2526 
2527 // Convert (leg)Vec to (leg)Vec[SDXYZ].
<span class="line-modified">2528 MachOper* Matcher::specialize_vector_operand_helper(MachNode* m, uint opnd_idx, const Type* t) {</span>
2529   MachOper* original_opnd = m-&gt;_opnds[opnd_idx];
<span class="line-modified">2530   uint ideal_reg = t-&gt;ideal_reg();</span>
2531   // Handle special cases.
<span class="line-modified">2532   if (t-&gt;isa_vect()) {</span>
<span class="line-modified">2533     // LShiftCntV/RShiftCntV report wide vector type, but Matcher::vector_shift_count_ideal_reg() as ideal register (see vectornode.hpp).</span>
<span class="line-modified">2534     // Look for shift count use sites as well (at vector shift nodes).</span>
<span class="line-modified">2535     int opc = m-&gt;ideal_Opcode();</span>
<span class="line-modified">2536     if ((VectorNode::is_shift_count(opc)  &amp;&amp; opnd_idx == 0) || // DEF operand of LShiftCntV/RShiftCntV</span>
<span class="line-modified">2537         (VectorNode::is_vector_shift(opc) &amp;&amp; opnd_idx == 2)) { // shift operand of a vector shift node</span>
<span class="line-removed">2538       ideal_reg = Matcher::vector_shift_count_ideal_reg(t-&gt;is_vect()-&gt;length_in_bytes());</span>
<span class="line-removed">2539     }</span>
<span class="line-removed">2540   } else {</span>
<span class="line-removed">2541     // Chain instructions which convert scalar to vector (e.g., vshiftcntimm on x86) don&#39;t have vector type.</span>
<span class="line-removed">2542     int size_in_bytes = 4 * type2size[t-&gt;basic_type()];</span>
<span class="line-removed">2543     ideal_reg = Matcher::vector_ideal_reg(size_in_bytes);</span>
2544   }
2545   return Matcher::specialize_generic_vector_operand(original_opnd, ideal_reg, false);
2546 }
2547 
2548 // Compute concrete vector operand for a generic TEMP vector mach node based on its user info.
2549 void Matcher::specialize_temp_node(MachTempNode* tmp, MachNode* use, uint idx) {
2550   assert(use-&gt;in(idx) == tmp, &quot;not a user&quot;);
2551   assert(!Matcher::is_generic_vector(use-&gt;_opnds[0]), &quot;use not processed yet&quot;);
2552 
2553   if ((uint)idx == use-&gt;two_adr()) { // DEF_TEMP case
2554     tmp-&gt;_opnds[0] = use-&gt;_opnds[0]-&gt;clone();
2555   } else {
2556     uint ideal_vreg = vector_ideal_reg(C-&gt;max_vector_size());
2557     tmp-&gt;_opnds[0] = specialize_generic_vector_operand(tmp-&gt;_opnds[0], ideal_vreg, true);
2558   }
2559 }
2560 
2561 // Compute concrete vector operand for a generic DEF/USE vector operand (of mach node m at index idx).
2562 MachOper* Matcher::specialize_vector_operand(MachNode* m, uint opnd_idx) {
2563   assert(Matcher::is_generic_vector(m-&gt;_opnds[opnd_idx]), &quot;repeated updates&quot;);
2564   Node* def = NULL;
2565   if (opnd_idx == 0) { // DEF
2566     def = m; // use mach node itself to compute vector operand type
2567   } else {
2568     int base_idx = m-&gt;operand_index(opnd_idx);
2569     def = m-&gt;in(base_idx);
2570     if (def-&gt;is_Mach()) {
2571       if (def-&gt;is_MachTemp() &amp;&amp; Matcher::is_generic_vector(def-&gt;as_Mach()-&gt;_opnds[0])) {
2572         specialize_temp_node(def-&gt;as_MachTemp(), m, base_idx); // MachTemp node use site
2573       } else if (is_generic_reg2reg_move(def-&gt;as_Mach())) {
2574         def = def-&gt;in(1); // skip over generic reg-to-reg moves
2575       }
2576     }
2577   }
<span class="line-modified">2578   return specialize_vector_operand_helper(m, opnd_idx, def-&gt;bottom_type());</span>
2579 }
2580 
2581 void Matcher::specialize_mach_node(MachNode* m) {
2582   assert(!m-&gt;is_MachTemp(), &quot;processed along with its user&quot;);
2583   // For generic use operands pull specific register class operands from
2584   // its def instruction&#39;s output operand (def operand).
2585   for (uint i = 0; i &lt; m-&gt;num_opnds(); i++) {
2586     if (Matcher::is_generic_vector(m-&gt;_opnds[i])) {
2587       m-&gt;_opnds[i] = specialize_vector_operand(m, i);
2588     }
2589   }
2590 }
2591 
2592 // Replace generic vector operands with concrete vector operands and eliminate generic reg-to-reg moves from the graph.
2593 void Matcher::specialize_generic_vector_operands() {
2594   assert(supports_generic_vector_operands, &quot;sanity&quot;);
2595   ResourceMark rm;
2596 
2597   if (C-&gt;max_vector_size() == 0) {
2598     return; // no vector instructions or operands
</pre>
</td>
<td>
<hr />
<pre>
1993 
1994 
1995 bool Matcher::is_bmi_pattern(Node *n, Node *m) {
1996   if (n != NULL &amp;&amp; m != NULL) {
1997     if (m-&gt;Opcode() == Op_LoadI) {
1998       FusedPatternMatcher&lt;TypeInt&gt; bmii(n, m, Op_ConI);
1999       return bmii.match(Op_AndI, -1, Op_SubI,  1,  0)  ||
2000              bmii.match(Op_AndI, -1, Op_AddI, -1, -1)  ||
2001              bmii.match(Op_XorI, -1, Op_AddI, -1, -1);
2002     } else if (m-&gt;Opcode() == Op_LoadL) {
2003       FusedPatternMatcher&lt;TypeLong&gt; bmil(n, m, Op_ConL);
2004       return bmil.match(Op_AndL, -1, Op_SubL,  1,  0) ||
2005              bmil.match(Op_AndL, -1, Op_AddL, -1, -1) ||
2006              bmil.match(Op_XorL, -1, Op_AddL, -1, -1);
2007     }
2008   }
2009   return false;
2010 }
2011 #endif // X86
2012 
<span class="line-added">2013 bool Matcher::is_vshift_con_pattern(Node *n, Node *m) {</span>
<span class="line-added">2014   if (n != NULL &amp;&amp; m != NULL) {</span>
<span class="line-added">2015     return VectorNode::is_vector_shift(n) &amp;&amp;</span>
<span class="line-added">2016            VectorNode::is_vector_shift_count(m) &amp;&amp; m-&gt;in(1)-&gt;is_Con();</span>
<span class="line-added">2017   }</span>
<span class="line-added">2018   return false;</span>
<span class="line-added">2019 }</span>
<span class="line-added">2020 </span>
<span class="line-added">2021 </span>
2022 bool Matcher::clone_base_plus_offset_address(AddPNode* m, Matcher::MStack&amp; mstack, VectorSet&amp; address_visited) {
2023   Node *off = m-&gt;in(AddPNode::Offset);
2024   if (off-&gt;is_Con()) {
2025     address_visited.test_set(m-&gt;_idx); // Flag as address_visited
2026     mstack.push(m-&gt;in(AddPNode::Address), Pre_Visit);
2027     // Clone X+offset as it also folds into most addressing expressions
2028     mstack.push(off, Visit);
2029     mstack.push(m-&gt;in(AddPNode::Base), Pre_Visit);
2030     return true;
2031   }
2032   return false;
2033 }
2034 
2035 // A method-klass-holder may be passed in the inline_cache_reg
2036 // and then expanded into the inline_cache_reg and a method_oop register
2037 //   defined in ad_&lt;arch&gt;.cpp
2038 
2039 //------------------------------find_shared------------------------------------
2040 // Set bits if Node is shared or otherwise a root
2041 void Matcher::find_shared( Node *n ) {
</pre>
<hr />
<pre>
2082         uint mop = m-&gt;Opcode();
2083 
2084         // Must clone all producers of flags, or we will not match correctly.
2085         // Suppose a compare setting int-flags is shared (e.g., a switch-tree)
2086         // then it will match into an ideal Op_RegFlags.  Alas, the fp-flags
2087         // are also there, so we may match a float-branch to int-flags and
2088         // expect the allocator to haul the flags from the int-side to the
2089         // fp-side.  No can do.
2090         if( _must_clone[mop] ) {
2091           mstack.push(m, Visit);
2092           continue; // for(int i = ...)
2093         }
2094 
2095         // if &#39;n&#39; and &#39;m&#39; are part of a graph for BMI instruction, clone this node.
2096 #ifdef X86
2097         if (UseBMI1Instructions &amp;&amp; is_bmi_pattern(n, m)) {
2098           mstack.push(m, Visit);
2099           continue;
2100         }
2101 #endif
<span class="line-added">2102         if (is_vshift_con_pattern(n, m)) {</span>
<span class="line-added">2103           mstack.push(m, Visit);</span>
<span class="line-added">2104           continue;</span>
<span class="line-added">2105         }</span>
2106 
2107         // Clone addressing expressions as they are &quot;free&quot; in memory access instructions
2108         if (mem_op &amp;&amp; i == mem_addr_idx &amp;&amp; mop == Op_AddP &amp;&amp;
2109             // When there are other uses besides address expressions
2110             // put it on stack and mark as shared.
2111             !is_visited(m)) {
2112           // Some inputs for address expression are not put on stack
2113           // to avoid marking them as shared and forcing them into register
2114           // if they are used only in address expressions.
2115           // But they should be marked as shared if there are other uses
2116           // besides address expressions.
2117 
2118           if (clone_address_expressions(m-&gt;as_AddP(), mstack, address_visited)) {
2119             continue;
2120           }
2121         }   // if( mem_op &amp;&amp;
2122         mstack.push(m, Pre_Visit);
2123       }     // for(int i = ...)
2124     }
2125     else if (nstate == Alt_Post_Visit) {
</pre>
<hr />
<pre>
2521   // Handle generic vector operand case
2522   if (Matcher::supports_generic_vector_operands &amp;&amp; t-&gt;isa_vect()) {
2523     specialize_mach_node(mspill);
2524   }
2525   return &amp;mspill-&gt;out_RegMask();
2526 }
2527 
2528 // Process Mach IR right after selection phase is over.
2529 void Matcher::do_postselect_cleanup() {
2530   if (supports_generic_vector_operands) {
2531     specialize_generic_vector_operands();
2532     if (C-&gt;failing())  return;
2533   }
2534 }
2535 
2536 //----------------------------------------------------------------------
2537 // Generic machine operands elision.
2538 //----------------------------------------------------------------------
2539 
2540 // Convert (leg)Vec to (leg)Vec[SDXYZ].
<span class="line-modified">2541 MachOper* Matcher::specialize_vector_operand_helper(MachNode* m, uint opnd_idx, const TypeVect* vt) {</span>
2542   MachOper* original_opnd = m-&gt;_opnds[opnd_idx];
<span class="line-modified">2543   uint ideal_reg = vt-&gt;ideal_reg();</span>
2544   // Handle special cases.
<span class="line-modified">2545   // LShiftCntV/RShiftCntV report wide vector type, but Matcher::vector_shift_count_ideal_reg() as ideal register (see vectornode.hpp).</span>
<span class="line-modified">2546   // Look for shift count use sites as well (at vector shift nodes).</span>
<span class="line-modified">2547   int opc = m-&gt;ideal_Opcode();</span>
<span class="line-modified">2548   if ((VectorNode::is_vector_shift_count(opc)  &amp;&amp; opnd_idx == 0) || // DEF operand of LShiftCntV/RShiftCntV</span>
<span class="line-modified">2549       (VectorNode::is_vector_shift(opc)        &amp;&amp; opnd_idx == 2)) { // shift operand of a vector shift node</span>
<span class="line-modified">2550     ideal_reg = Matcher::vector_shift_count_ideal_reg(vt-&gt;length_in_bytes());</span>






2551   }
2552   return Matcher::specialize_generic_vector_operand(original_opnd, ideal_reg, false);
2553 }
2554 
2555 // Compute concrete vector operand for a generic TEMP vector mach node based on its user info.
2556 void Matcher::specialize_temp_node(MachTempNode* tmp, MachNode* use, uint idx) {
2557   assert(use-&gt;in(idx) == tmp, &quot;not a user&quot;);
2558   assert(!Matcher::is_generic_vector(use-&gt;_opnds[0]), &quot;use not processed yet&quot;);
2559 
2560   if ((uint)idx == use-&gt;two_adr()) { // DEF_TEMP case
2561     tmp-&gt;_opnds[0] = use-&gt;_opnds[0]-&gt;clone();
2562   } else {
2563     uint ideal_vreg = vector_ideal_reg(C-&gt;max_vector_size());
2564     tmp-&gt;_opnds[0] = specialize_generic_vector_operand(tmp-&gt;_opnds[0], ideal_vreg, true);
2565   }
2566 }
2567 
2568 // Compute concrete vector operand for a generic DEF/USE vector operand (of mach node m at index idx).
2569 MachOper* Matcher::specialize_vector_operand(MachNode* m, uint opnd_idx) {
2570   assert(Matcher::is_generic_vector(m-&gt;_opnds[opnd_idx]), &quot;repeated updates&quot;);
2571   Node* def = NULL;
2572   if (opnd_idx == 0) { // DEF
2573     def = m; // use mach node itself to compute vector operand type
2574   } else {
2575     int base_idx = m-&gt;operand_index(opnd_idx);
2576     def = m-&gt;in(base_idx);
2577     if (def-&gt;is_Mach()) {
2578       if (def-&gt;is_MachTemp() &amp;&amp; Matcher::is_generic_vector(def-&gt;as_Mach()-&gt;_opnds[0])) {
2579         specialize_temp_node(def-&gt;as_MachTemp(), m, base_idx); // MachTemp node use site
2580       } else if (is_generic_reg2reg_move(def-&gt;as_Mach())) {
2581         def = def-&gt;in(1); // skip over generic reg-to-reg moves
2582       }
2583     }
2584   }
<span class="line-modified">2585   return specialize_vector_operand_helper(m, opnd_idx, def-&gt;bottom_type()-&gt;is_vect());</span>
2586 }
2587 
2588 void Matcher::specialize_mach_node(MachNode* m) {
2589   assert(!m-&gt;is_MachTemp(), &quot;processed along with its user&quot;);
2590   // For generic use operands pull specific register class operands from
2591   // its def instruction&#39;s output operand (def operand).
2592   for (uint i = 0; i &lt; m-&gt;num_opnds(); i++) {
2593     if (Matcher::is_generic_vector(m-&gt;_opnds[i])) {
2594       m-&gt;_opnds[i] = specialize_vector_operand(m, i);
2595     }
2596   }
2597 }
2598 
2599 // Replace generic vector operands with concrete vector operands and eliminate generic reg-to-reg moves from the graph.
2600 void Matcher::specialize_generic_vector_operands() {
2601   assert(supports_generic_vector_operands, &quot;sanity&quot;);
2602   ResourceMark rm;
2603 
2604   if (C-&gt;max_vector_size() == 0) {
2605     return; // no vector instructions or operands
</pre>
</td>
</tr>
</table>
<center><a href="macroArrayCopy.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="matcher.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>