<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/gc/serial/defNewGeneration.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
    <script type="text/javascript" src="../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 2001, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/serial/defNewGeneration.inline.hpp&quot;
 27 #include &quot;gc/serial/serialHeap.inline.hpp&quot;
 28 #include &quot;gc/serial/tenuredGeneration.hpp&quot;
 29 #include &quot;gc/shared/adaptiveSizePolicy.hpp&quot;
 30 #include &quot;gc/shared/ageTable.inline.hpp&quot;
 31 #include &quot;gc/shared/cardTableRS.hpp&quot;
 32 #include &quot;gc/shared/collectorCounters.hpp&quot;
 33 #include &quot;gc/shared/gcArguments.hpp&quot;
 34 #include &quot;gc/shared/gcHeapSummary.hpp&quot;
 35 #include &quot;gc/shared/gcLocker.hpp&quot;
 36 #include &quot;gc/shared/gcPolicyCounters.hpp&quot;
 37 #include &quot;gc/shared/gcTimer.hpp&quot;
 38 #include &quot;gc/shared/gcTrace.hpp&quot;
 39 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
 40 #include &quot;gc/shared/genOopClosures.inline.hpp&quot;
 41 #include &quot;gc/shared/generationSpec.hpp&quot;
 42 #include &quot;gc/shared/preservedMarks.inline.hpp&quot;
 43 #include &quot;gc/shared/referencePolicy.hpp&quot;
 44 #include &quot;gc/shared/referenceProcessorPhaseTimes.hpp&quot;
 45 #include &quot;gc/shared/space.inline.hpp&quot;
 46 #include &quot;gc/shared/spaceDecorator.inline.hpp&quot;
 47 #include &quot;gc/shared/strongRootsScope.hpp&quot;
 48 #include &quot;gc/shared/weakProcessor.hpp&quot;
 49 #include &quot;logging/log.hpp&quot;
 50 #include &quot;memory/iterator.inline.hpp&quot;
 51 #include &quot;memory/resourceArea.hpp&quot;
 52 #include &quot;oops/instanceRefKlass.hpp&quot;
 53 #include &quot;oops/oop.inline.hpp&quot;
 54 #include &quot;runtime/java.hpp&quot;
 55 #include &quot;runtime/prefetch.inline.hpp&quot;
 56 #include &quot;runtime/thread.inline.hpp&quot;
 57 #include &quot;utilities/align.hpp&quot;
 58 #include &quot;utilities/copy.hpp&quot;
 59 #include &quot;utilities/globalDefinitions.hpp&quot;
 60 #include &quot;utilities/stack.inline.hpp&quot;
 61 
 62 //
 63 // DefNewGeneration functions.
 64 
 65 // Methods of protected closure types.
 66 
 67 DefNewGeneration::IsAliveClosure::IsAliveClosure(Generation* young_gen) : _young_gen(young_gen) {
 68   assert(_young_gen-&gt;kind() == Generation::DefNew, &quot;Expected the young generation here&quot;);
 69 }
 70 
 71 bool DefNewGeneration::IsAliveClosure::do_object_b(oop p) {
 72   return cast_from_oop&lt;HeapWord*&gt;(p) &gt;= _young_gen-&gt;reserved().end() || p-&gt;is_forwarded();
 73 }
 74 
 75 DefNewGeneration::KeepAliveClosure::
 76 KeepAliveClosure(ScanWeakRefClosure* cl) : _cl(cl) {
 77   _rs = GenCollectedHeap::heap()-&gt;rem_set();
 78 }
 79 
 80 void DefNewGeneration::KeepAliveClosure::do_oop(oop* p)       { DefNewGeneration::KeepAliveClosure::do_oop_work(p); }
 81 void DefNewGeneration::KeepAliveClosure::do_oop(narrowOop* p) { DefNewGeneration::KeepAliveClosure::do_oop_work(p); }
 82 
 83 
 84 DefNewGeneration::FastKeepAliveClosure::
 85 FastKeepAliveClosure(DefNewGeneration* g, ScanWeakRefClosure* cl) :
 86   DefNewGeneration::KeepAliveClosure(cl) {
 87   _boundary = g-&gt;reserved().end();
 88 }
 89 
 90 void DefNewGeneration::FastKeepAliveClosure::do_oop(oop* p)       { DefNewGeneration::FastKeepAliveClosure::do_oop_work(p); }
 91 void DefNewGeneration::FastKeepAliveClosure::do_oop(narrowOop* p) { DefNewGeneration::FastKeepAliveClosure::do_oop_work(p); }
 92 
 93 DefNewGeneration::FastEvacuateFollowersClosure::
 94 FastEvacuateFollowersClosure(SerialHeap* heap,
 95                              FastScanClosure* cur,
 96                              FastScanClosure* older) :
 97   _heap(heap), _scan_cur_or_nonheap(cur), _scan_older(older)
 98 {
 99 }
100 
101 void DefNewGeneration::FastEvacuateFollowersClosure::do_void() {
102   do {
103     _heap-&gt;oop_since_save_marks_iterate(_scan_cur_or_nonheap, _scan_older);
104   } while (!_heap-&gt;no_allocs_since_save_marks());
105   guarantee(_heap-&gt;young_gen()-&gt;promo_failure_scan_is_complete(), &quot;Failed to finish scan&quot;);
106 }
107 
108 ScanClosure::ScanClosure(DefNewGeneration* g, bool gc_barrier) :
109     OopsInClassLoaderDataOrGenClosure(g), _g(g), _gc_barrier(gc_barrier)
110 {
111   _boundary = _g-&gt;reserved().end();
112 }
113 
114 FastScanClosure::FastScanClosure(DefNewGeneration* g, bool gc_barrier) :
115     OopsInClassLoaderDataOrGenClosure(g), _g(g), _gc_barrier(gc_barrier)
116 {
117   _boundary = _g-&gt;reserved().end();
118 }
119 
120 void CLDScanClosure::do_cld(ClassLoaderData* cld) {
121   NOT_PRODUCT(ResourceMark rm);
122   log_develop_trace(gc, scavenge)(&quot;CLDScanClosure::do_cld &quot; PTR_FORMAT &quot;, %s, dirty: %s&quot;,
123                                   p2i(cld),
124                                   cld-&gt;loader_name_and_id(),
125                                   cld-&gt;has_modified_oops() ? &quot;true&quot; : &quot;false&quot;);
126 
127   // If the cld has not been dirtied we know that there&#39;s
128   // no references into  the young gen and we can skip it.
129   if (cld-&gt;has_modified_oops()) {
130     if (_accumulate_modified_oops) {
131       cld-&gt;accumulate_modified_oops();
132     }
133 
134     // Tell the closure which CLD is being scanned so that it can be dirtied
135     // if oops are left pointing into the young gen.
136     _scavenge_closure-&gt;set_scanned_cld(cld);
137 
138     // Clean the cld since we&#39;re going to scavenge all the metadata.
139     cld-&gt;oops_do(_scavenge_closure, ClassLoaderData::_claim_none, /*clear_modified_oops*/true);
140 
141     _scavenge_closure-&gt;set_scanned_cld(NULL);
142   }
143 }
144 
145 ScanWeakRefClosure::ScanWeakRefClosure(DefNewGeneration* g) :
146   _g(g)
147 {
148   _boundary = _g-&gt;reserved().end();
149 }
150 
151 DefNewGeneration::DefNewGeneration(ReservedSpace rs,
152                                    size_t initial_size,
153                                    size_t min_size,
154                                    size_t max_size,
155                                    const char* policy)
156   : Generation(rs, initial_size),
157     _preserved_marks_set(false /* in_c_heap */),
158     _promo_failure_drain_in_progress(false),
159     _should_allocate_from_space(false)
160 {
161   MemRegion cmr((HeapWord*)_virtual_space.low(),
162                 (HeapWord*)_virtual_space.high());
163   GenCollectedHeap* gch = GenCollectedHeap::heap();
164 
165   gch-&gt;rem_set()-&gt;resize_covered_region(cmr);
166 
167   _eden_space = new ContiguousSpace();
168   _from_space = new ContiguousSpace();
169   _to_space   = new ContiguousSpace();
170 
<a name="1" id="anc1"></a>



171   // Compute the maximum eden and survivor space sizes. These sizes
172   // are computed assuming the entire reserved space is committed.
173   // These values are exported as performance counters.
174   uintx size = _virtual_space.reserved_size();
175   _max_survivor_size = compute_survivor_size(size, SpaceAlignment);
176   _max_eden_size = size - (2*_max_survivor_size);
177 
178   // allocate the performance counters
179 
180   // Generation counters -- generation 0, 3 subspaces
181   _gen_counters = new GenerationCounters(&quot;new&quot;, 0, 3,
182       min_size, max_size, &amp;_virtual_space);
183   _gc_counters = new CollectorCounters(policy, 0);
184 
185   _eden_counters = new CSpaceCounters(&quot;eden&quot;, 0, _max_eden_size, _eden_space,
186                                       _gen_counters);
187   _from_counters = new CSpaceCounters(&quot;s0&quot;, 1, _max_survivor_size, _from_space,
188                                       _gen_counters);
189   _to_counters = new CSpaceCounters(&quot;s1&quot;, 2, _max_survivor_size, _to_space,
190                                     _gen_counters);
191 
192   compute_space_boundaries(0, SpaceDecorator::Clear, SpaceDecorator::Mangle);
193   update_counters();
194   _old_gen = NULL;
195   _tenuring_threshold = MaxTenuringThreshold;
196   _pretenure_size_threshold_words = PretenureSizeThreshold &gt;&gt; LogHeapWordSize;
197 
198   _gc_timer = new (ResourceObj::C_HEAP, mtGC) STWGCTimer();
199 }
200 
201 void DefNewGeneration::compute_space_boundaries(uintx minimum_eden_size,
202                                                 bool clear_space,
203                                                 bool mangle_space) {
204   // If the spaces are being cleared (only done at heap initialization
205   // currently), the survivor spaces need not be empty.
206   // Otherwise, no care is taken for used areas in the survivor spaces
207   // so check.
208   assert(clear_space || (to()-&gt;is_empty() &amp;&amp; from()-&gt;is_empty()),
209     &quot;Initialization of the survivor spaces assumes these are empty&quot;);
210 
211   // Compute sizes
212   uintx size = _virtual_space.committed_size();
213   uintx survivor_size = compute_survivor_size(size, SpaceAlignment);
214   uintx eden_size = size - (2*survivor_size);
215   assert(eden_size &gt; 0 &amp;&amp; survivor_size &lt;= eden_size, &quot;just checking&quot;);
216 
217   if (eden_size &lt; minimum_eden_size) {
218     // May happen due to 64Kb rounding, if so adjust eden size back up
219     minimum_eden_size = align_up(minimum_eden_size, SpaceAlignment);
220     uintx maximum_survivor_size = (size - minimum_eden_size) / 2;
221     uintx unaligned_survivor_size =
222       align_down(maximum_survivor_size, SpaceAlignment);
223     survivor_size = MAX2(unaligned_survivor_size, SpaceAlignment);
224     eden_size = size - (2*survivor_size);
225     assert(eden_size &gt; 0 &amp;&amp; survivor_size &lt;= eden_size, &quot;just checking&quot;);
226     assert(eden_size &gt;= minimum_eden_size, &quot;just checking&quot;);
227   }
228 
229   char *eden_start = _virtual_space.low();
230   char *from_start = eden_start + eden_size;
231   char *to_start   = from_start + survivor_size;
232   char *to_end     = to_start   + survivor_size;
233 
234   assert(to_end == _virtual_space.high(), &quot;just checking&quot;);
235   assert(Space::is_aligned(eden_start), &quot;checking alignment&quot;);
236   assert(Space::is_aligned(from_start), &quot;checking alignment&quot;);
237   assert(Space::is_aligned(to_start),   &quot;checking alignment&quot;);
238 
239   MemRegion edenMR((HeapWord*)eden_start, (HeapWord*)from_start);
240   MemRegion fromMR((HeapWord*)from_start, (HeapWord*)to_start);
241   MemRegion toMR  ((HeapWord*)to_start, (HeapWord*)to_end);
242 
243   // A minimum eden size implies that there is a part of eden that
244   // is being used and that affects the initialization of any
245   // newly formed eden.
246   bool live_in_eden = minimum_eden_size &gt; 0;
247 
248   // If not clearing the spaces, do some checking to verify that
249   // the space are already mangled.
250   if (!clear_space) {
251     // Must check mangling before the spaces are reshaped.  Otherwise,
252     // the bottom or end of one space may have moved into another
253     // a failure of the check may not correctly indicate which space
254     // is not properly mangled.
255     if (ZapUnusedHeapArea) {
256       HeapWord* limit = (HeapWord*) _virtual_space.high();
257       eden()-&gt;check_mangled_unused_area(limit);
258       from()-&gt;check_mangled_unused_area(limit);
259         to()-&gt;check_mangled_unused_area(limit);
260     }
261   }
262 
263   // Reset the spaces for their new regions.
264   eden()-&gt;initialize(edenMR,
265                      clear_space &amp;&amp; !live_in_eden,
266                      SpaceDecorator::Mangle);
267   // If clear_space and live_in_eden, we will not have cleared any
268   // portion of eden above its top. This can cause newly
269   // expanded space not to be mangled if using ZapUnusedHeapArea.
270   // We explicitly do such mangling here.
271   if (ZapUnusedHeapArea &amp;&amp; clear_space &amp;&amp; live_in_eden &amp;&amp; mangle_space) {
272     eden()-&gt;mangle_unused_area();
273   }
274   from()-&gt;initialize(fromMR, clear_space, mangle_space);
275   to()-&gt;initialize(toMR, clear_space, mangle_space);
276 
277   // Set next compaction spaces.
278   eden()-&gt;set_next_compaction_space(from());
279   // The to-space is normally empty before a compaction so need
280   // not be considered.  The exception is during promotion
281   // failure handling when to-space can contain live objects.
282   from()-&gt;set_next_compaction_space(NULL);
283 }
284 
285 void DefNewGeneration::swap_spaces() {
286   ContiguousSpace* s = from();
287   _from_space        = to();
288   _to_space          = s;
289   eden()-&gt;set_next_compaction_space(from());
290   // The to-space is normally empty before a compaction so need
291   // not be considered.  The exception is during promotion
292   // failure handling when to-space can contain live objects.
293   from()-&gt;set_next_compaction_space(NULL);
294 
295   if (UsePerfData) {
296     CSpaceCounters* c = _from_counters;
297     _from_counters = _to_counters;
298     _to_counters = c;
299   }
300 }
301 
302 bool DefNewGeneration::expand(size_t bytes) {
303   MutexLocker x(ExpandHeap_lock);
304   HeapWord* prev_high = (HeapWord*) _virtual_space.high();
305   bool success = _virtual_space.expand_by(bytes);
306   if (success &amp;&amp; ZapUnusedHeapArea) {
307     // Mangle newly committed space immediately because it
308     // can be done here more simply that after the new
309     // spaces have been computed.
310     HeapWord* new_high = (HeapWord*) _virtual_space.high();
311     MemRegion mangle_region(prev_high, new_high);
312     SpaceMangler::mangle_region(mangle_region);
313   }
314 
315   // Do not attempt an expand-to-the reserve size.  The
316   // request should properly observe the maximum size of
317   // the generation so an expand-to-reserve should be
318   // unnecessary.  Also a second call to expand-to-reserve
319   // value potentially can cause an undue expansion.
320   // For example if the first expand fail for unknown reasons,
321   // but the second succeeds and expands the heap to its maximum
322   // value.
323   if (GCLocker::is_active()) {
324     log_debug(gc)(&quot;Garbage collection disabled, expanded heap instead&quot;);
325   }
326 
327   return success;
328 }
329 
330 size_t DefNewGeneration::adjust_for_thread_increase(size_t new_size_candidate,
331                                                     size_t new_size_before,
332                                                     size_t alignment) const {
333   size_t desired_new_size = new_size_before;
334 
335   if (NewSizeThreadIncrease &gt; 0) {
336     int threads_count;
337     size_t thread_increase_size = 0;
338 
339     // 1. Check an overflow at &#39;threads_count * NewSizeThreadIncrease&#39;.
340     threads_count = Threads::number_of_non_daemon_threads();
341     if (threads_count &gt; 0 &amp;&amp; NewSizeThreadIncrease &lt;= max_uintx / threads_count) {
342       thread_increase_size = threads_count * NewSizeThreadIncrease;
343 
344       // 2. Check an overflow at &#39;new_size_candidate + thread_increase_size&#39;.
345       if (new_size_candidate &lt;= max_uintx - thread_increase_size) {
346         new_size_candidate += thread_increase_size;
347 
348         // 3. Check an overflow at &#39;align_up&#39;.
349         size_t aligned_max = ((max_uintx - alignment) &amp; ~(alignment-1));
350         if (new_size_candidate &lt;= aligned_max) {
351           desired_new_size = align_up(new_size_candidate, alignment);
352         }
353       }
354     }
355   }
356 
357   return desired_new_size;
358 }
359 
360 void DefNewGeneration::compute_new_size() {
361   // This is called after a GC that includes the old generation, so from-space
362   // will normally be empty.
363   // Note that we check both spaces, since if scavenge failed they revert roles.
364   // If not we bail out (otherwise we would have to relocate the objects).
365   if (!from()-&gt;is_empty() || !to()-&gt;is_empty()) {
366     return;
367   }
368 
369   GenCollectedHeap* gch = GenCollectedHeap::heap();
370 
371   size_t old_size = gch-&gt;old_gen()-&gt;capacity();
372   size_t new_size_before = _virtual_space.committed_size();
373   size_t min_new_size = initial_size();
374   size_t max_new_size = reserved().byte_size();
375   assert(min_new_size &lt;= new_size_before &amp;&amp;
376          new_size_before &lt;= max_new_size,
377          &quot;just checking&quot;);
378   // All space sizes must be multiples of Generation::GenGrain.
379   size_t alignment = Generation::GenGrain;
380 
381   int threads_count = 0;
382   size_t thread_increase_size = 0;
383 
384   size_t new_size_candidate = old_size / NewRatio;
385   // Compute desired new generation size based on NewRatio and NewSizeThreadIncrease
386   // and reverts to previous value if any overflow happens
387   size_t desired_new_size = adjust_for_thread_increase(new_size_candidate, new_size_before, alignment);
388 
389   // Adjust new generation size
390   desired_new_size = clamp(desired_new_size, min_new_size, max_new_size);
391   assert(desired_new_size &lt;= max_new_size, &quot;just checking&quot;);
392 
393   bool changed = false;
394   if (desired_new_size &gt; new_size_before) {
395     size_t change = desired_new_size - new_size_before;
396     assert(change % alignment == 0, &quot;just checking&quot;);
397     if (expand(change)) {
398        changed = true;
399     }
400     // If the heap failed to expand to the desired size,
401     // &quot;changed&quot; will be false.  If the expansion failed
402     // (and at this point it was expected to succeed),
403     // ignore the failure (leaving &quot;changed&quot; as false).
404   }
405   if (desired_new_size &lt; new_size_before &amp;&amp; eden()-&gt;is_empty()) {
406     // bail out of shrinking if objects in eden
407     size_t change = new_size_before - desired_new_size;
408     assert(change % alignment == 0, &quot;just checking&quot;);
409     _virtual_space.shrink_by(change);
410     changed = true;
411   }
412   if (changed) {
413     // The spaces have already been mangled at this point but
414     // may not have been cleared (set top = bottom) and should be.
415     // Mangling was done when the heap was being expanded.
416     compute_space_boundaries(eden()-&gt;used(),
417                              SpaceDecorator::Clear,
418                              SpaceDecorator::DontMangle);
419     MemRegion cmr((HeapWord*)_virtual_space.low(),
420                   (HeapWord*)_virtual_space.high());
421     gch-&gt;rem_set()-&gt;resize_covered_region(cmr);
422 
423     log_debug(gc, ergo, heap)(
424         &quot;New generation size &quot; SIZE_FORMAT &quot;K-&gt;&quot; SIZE_FORMAT &quot;K [eden=&quot; SIZE_FORMAT &quot;K,survivor=&quot; SIZE_FORMAT &quot;K]&quot;,
425         new_size_before/K, _virtual_space.committed_size()/K,
426         eden()-&gt;capacity()/K, from()-&gt;capacity()/K);
427     log_trace(gc, ergo, heap)(
428         &quot;  [allowed &quot; SIZE_FORMAT &quot;K extra for %d threads]&quot;,
429           thread_increase_size/K, threads_count);
430       }
431 }
432 
433 void DefNewGeneration::younger_refs_iterate(OopsInGenClosure* cl, uint n_threads) {
434   assert(false, &quot;NYI -- are you sure you want to call this?&quot;);
435 }
436 
437 
438 size_t DefNewGeneration::capacity() const {
439   return eden()-&gt;capacity()
440        + from()-&gt;capacity();  // to() is only used during scavenge
441 }
442 
443 
444 size_t DefNewGeneration::used() const {
445   return eden()-&gt;used()
446        + from()-&gt;used();      // to() is only used during scavenge
447 }
448 
449 
450 size_t DefNewGeneration::free() const {
451   return eden()-&gt;free()
452        + from()-&gt;free();      // to() is only used during scavenge
453 }
454 
455 size_t DefNewGeneration::max_capacity() const {
456   const size_t reserved_bytes = reserved().byte_size();
457   return reserved_bytes - compute_survivor_size(reserved_bytes, SpaceAlignment);
458 }
459 
460 size_t DefNewGeneration::unsafe_max_alloc_nogc() const {
461   return eden()-&gt;free();
462 }
463 
464 size_t DefNewGeneration::capacity_before_gc() const {
465   return eden()-&gt;capacity();
466 }
467 
468 size_t DefNewGeneration::contiguous_available() const {
469   return eden()-&gt;free();
470 }
471 
472 
473 HeapWord* volatile* DefNewGeneration::top_addr() const { return eden()-&gt;top_addr(); }
474 HeapWord** DefNewGeneration::end_addr() const { return eden()-&gt;end_addr(); }
475 
476 void DefNewGeneration::object_iterate(ObjectClosure* blk) {
477   eden()-&gt;object_iterate(blk);
478   from()-&gt;object_iterate(blk);
479 }
480 
481 
482 void DefNewGeneration::space_iterate(SpaceClosure* blk,
483                                      bool usedOnly) {
484   blk-&gt;do_space(eden());
485   blk-&gt;do_space(from());
486   blk-&gt;do_space(to());
487 }
488 
489 // The last collection bailed out, we are running out of heap space,
490 // so we try to allocate the from-space, too.
491 HeapWord* DefNewGeneration::allocate_from_space(size_t size) {
492   bool should_try_alloc = should_allocate_from_space() || GCLocker::is_active_and_needs_gc();
493 
494   // If the Heap_lock is not locked by this thread, this will be called
495   // again later with the Heap_lock held.
496   bool do_alloc = should_try_alloc &amp;&amp; (Heap_lock-&gt;owned_by_self() || (SafepointSynchronize::is_at_safepoint() &amp;&amp; Thread::current()-&gt;is_VM_thread()));
497 
498   HeapWord* result = NULL;
499   if (do_alloc) {
500     result = from()-&gt;allocate(size);
501   }
502 
503   log_trace(gc, alloc)(&quot;DefNewGeneration::allocate_from_space(&quot; SIZE_FORMAT &quot;):  will_fail: %s  heap_lock: %s  free: &quot; SIZE_FORMAT &quot;%s%s returns %s&quot;,
504                         size,
505                         GenCollectedHeap::heap()-&gt;incremental_collection_will_fail(false /* don&#39;t consult_young */) ?
506                           &quot;true&quot; : &quot;false&quot;,
507                         Heap_lock-&gt;is_locked() ? &quot;locked&quot; : &quot;unlocked&quot;,
508                         from()-&gt;free(),
509                         should_try_alloc ? &quot;&quot; : &quot;  should_allocate_from_space: NOT&quot;,
510                         do_alloc ? &quot;  Heap_lock is not owned by self&quot; : &quot;&quot;,
511                         result == NULL ? &quot;NULL&quot; : &quot;object&quot;);
512 
513   return result;
514 }
515 
516 HeapWord* DefNewGeneration::expand_and_allocate(size_t size,
517                                                 bool   is_tlab,
518                                                 bool   parallel) {
519   // We don&#39;t attempt to expand the young generation (but perhaps we should.)
520   return allocate(size, is_tlab);
521 }
522 
523 void DefNewGeneration::adjust_desired_tenuring_threshold() {
524   // Set the desired survivor size to half the real survivor space
525   size_t const survivor_capacity = to()-&gt;capacity() / HeapWordSize;
526   size_t const desired_survivor_size = (size_t)((((double)survivor_capacity) * TargetSurvivorRatio) / 100);
527 
528   _tenuring_threshold = age_table()-&gt;compute_tenuring_threshold(desired_survivor_size);
529 
530   if (UsePerfData) {
531     GCPolicyCounters* gc_counters = GenCollectedHeap::heap()-&gt;counters();
532     gc_counters-&gt;tenuring_threshold()-&gt;set_value(_tenuring_threshold);
533     gc_counters-&gt;desired_survivor_size()-&gt;set_value(desired_survivor_size * oopSize);
534   }
535 
536   age_table()-&gt;print_age_table(_tenuring_threshold);
537 }
538 
539 void DefNewGeneration::collect(bool   full,
540                                bool   clear_all_soft_refs,
541                                size_t size,
542                                bool   is_tlab) {
543   assert(full || size &gt; 0, &quot;otherwise we don&#39;t want to collect&quot;);
544 
545   SerialHeap* heap = SerialHeap::heap();
546 
547   _gc_timer-&gt;register_gc_start();
548   DefNewTracer gc_tracer;
549   gc_tracer.report_gc_start(heap-&gt;gc_cause(), _gc_timer-&gt;gc_start());
550 
551   _old_gen = heap-&gt;old_gen();
552 
553   // If the next generation is too full to accommodate promotion
554   // from this generation, pass on collection; let the next generation
555   // do it.
556   if (!collection_attempt_is_safe()) {
557     log_trace(gc)(&quot;:: Collection attempt not safe ::&quot;);
558     heap-&gt;set_incremental_collection_failed(); // Slight lie: we did not even attempt one
559     return;
560   }
561   assert(to()-&gt;is_empty(), &quot;Else not collection_attempt_is_safe&quot;);
562 
563   init_assuming_no_promotion_failure();
564 
565   GCTraceTime(Trace, gc, phases) tm(&quot;DefNew&quot;, NULL, heap-&gt;gc_cause());
566 
567   heap-&gt;trace_heap_before_gc(&amp;gc_tracer);
568 
569   // These can be shared for all code paths
570   IsAliveClosure is_alive(this);
571   ScanWeakRefClosure scan_weak_ref(this);
572 
573   age_table()-&gt;clear();
574   to()-&gt;clear(SpaceDecorator::Mangle);
575   // The preserved marks should be empty at the start of the GC.
576   _preserved_marks_set.init(1);
577 
578   heap-&gt;rem_set()-&gt;prepare_for_younger_refs_iterate(false);
579 
580   assert(heap-&gt;no_allocs_since_save_marks(),
581          &quot;save marks have not been newly set.&quot;);
582 
583   FastScanClosure fsc_with_no_gc_barrier(this, false);
584   FastScanClosure fsc_with_gc_barrier(this, true);
585 
586   CLDScanClosure cld_scan_closure(&amp;fsc_with_no_gc_barrier,
587                                   heap-&gt;rem_set()-&gt;cld_rem_set()-&gt;accumulate_modified_oops());
588 
589   set_promo_failure_scan_stack_closure(&amp;fsc_with_no_gc_barrier);
590   FastEvacuateFollowersClosure evacuate_followers(heap,
591                                                   &amp;fsc_with_no_gc_barrier,
592                                                   &amp;fsc_with_gc_barrier);
593 
594   assert(heap-&gt;no_allocs_since_save_marks(),
595          &quot;save marks have not been newly set.&quot;);
596 
597   {
598     // DefNew needs to run with n_threads == 0, to make sure the serial
599     // version of the card table scanning code is used.
600     // See: CardTableRS::non_clean_card_iterate_possibly_parallel.
601     StrongRootsScope srs(0);
602 
603     heap-&gt;young_process_roots(&amp;srs,
604                               &amp;fsc_with_no_gc_barrier,
605                               &amp;fsc_with_gc_barrier,
606                               &amp;cld_scan_closure);
607   }
608 
609   // &quot;evacuate followers&quot;.
610   evacuate_followers.do_void();
611 
612   FastKeepAliveClosure keep_alive(this, &amp;scan_weak_ref);
613   ReferenceProcessor* rp = ref_processor();
614   rp-&gt;setup_policy(clear_all_soft_refs);
615   ReferenceProcessorPhaseTimes pt(_gc_timer, rp-&gt;max_num_queues());
616   const ReferenceProcessorStats&amp; stats =
617   rp-&gt;process_discovered_references(&amp;is_alive, &amp;keep_alive, &amp;evacuate_followers,
618                                     NULL, &amp;pt);
619   gc_tracer.report_gc_reference_stats(stats);
620   gc_tracer.report_tenuring_threshold(tenuring_threshold());
621   pt.print_all_references();
622 
623   assert(heap-&gt;no_allocs_since_save_marks(), &quot;save marks have not been newly set.&quot;);
624 
625   WeakProcessor::weak_oops_do(&amp;is_alive, &amp;keep_alive);
626 
627   // Verify that the usage of keep_alive didn&#39;t copy any objects.
628   assert(heap-&gt;no_allocs_since_save_marks(), &quot;save marks have not been newly set.&quot;);
629 
630   if (!_promotion_failed) {
631     // Swap the survivor spaces.
632     eden()-&gt;clear(SpaceDecorator::Mangle);
633     from()-&gt;clear(SpaceDecorator::Mangle);
634     if (ZapUnusedHeapArea) {
635       // This is now done here because of the piece-meal mangling which
636       // can check for valid mangling at intermediate points in the
637       // collection(s).  When a young collection fails to collect
638       // sufficient space resizing of the young generation can occur
639       // an redistribute the spaces in the young generation.  Mangle
640       // here so that unzapped regions don&#39;t get distributed to
641       // other spaces.
642       to()-&gt;mangle_unused_area();
643     }
644     swap_spaces();
645 
646     assert(to()-&gt;is_empty(), &quot;to space should be empty now&quot;);
647 
648     adjust_desired_tenuring_threshold();
649 
650     // A successful scavenge should restart the GC time limit count which is
651     // for full GC&#39;s.
652     AdaptiveSizePolicy* size_policy = heap-&gt;size_policy();
653     size_policy-&gt;reset_gc_overhead_limit_count();
654     assert(!heap-&gt;incremental_collection_failed(), &quot;Should be clear&quot;);
655   } else {
656     assert(_promo_failure_scan_stack.is_empty(), &quot;post condition&quot;);
657     _promo_failure_scan_stack.clear(true); // Clear cached segments.
658 
659     remove_forwarding_pointers();
660     log_info(gc, promotion)(&quot;Promotion failed&quot;);
661     // Add to-space to the list of space to compact
662     // when a promotion failure has occurred.  In that
663     // case there can be live objects in to-space
664     // as a result of a partial evacuation of eden
665     // and from-space.
666     swap_spaces();   // For uniformity wrt ParNewGeneration.
667     from()-&gt;set_next_compaction_space(to());
668     heap-&gt;set_incremental_collection_failed();
669 
670     // Inform the next generation that a promotion failure occurred.
671     _old_gen-&gt;promotion_failure_occurred();
672     gc_tracer.report_promotion_failed(_promotion_failed_info);
673 
674     // Reset the PromotionFailureALot counters.
675     NOT_PRODUCT(heap-&gt;reset_promotion_should_fail();)
676   }
677   // We should have processed and cleared all the preserved marks.
678   _preserved_marks_set.reclaim();
679   // set new iteration safe limit for the survivor spaces
680   from()-&gt;set_concurrent_iteration_safe_limit(from()-&gt;top());
681   to()-&gt;set_concurrent_iteration_safe_limit(to()-&gt;top());
682 
683   // We need to use a monotonically non-decreasing time in ms
684   // or we will see time-warp warnings and os::javaTimeMillis()
685   // does not guarantee monotonicity.
686   jlong now = os::javaTimeNanos() / NANOSECS_PER_MILLISEC;
687   update_time_of_last_gc(now);
688 
689   heap-&gt;trace_heap_after_gc(&amp;gc_tracer);
690 
691   _gc_timer-&gt;register_gc_end();
692 
693   gc_tracer.report_gc_end(_gc_timer-&gt;gc_end(), _gc_timer-&gt;time_partitions());
694 }
695 
696 void DefNewGeneration::init_assuming_no_promotion_failure() {
697   _promotion_failed = false;
698   _promotion_failed_info.reset();
699   from()-&gt;set_next_compaction_space(NULL);
700 }
701 
702 void DefNewGeneration::remove_forwarding_pointers() {
703   RemoveForwardedPointerClosure rspc;
704   eden()-&gt;object_iterate(&amp;rspc);
705   from()-&gt;object_iterate(&amp;rspc);
706   restore_preserved_marks();
707 }
708 
709 void DefNewGeneration::restore_preserved_marks() {
710   _preserved_marks_set.restore(NULL);
711 }
712 
713 void DefNewGeneration::handle_promotion_failure(oop old) {
714   log_debug(gc, promotion)(&quot;Promotion failure size = %d) &quot;, old-&gt;size());
715 
716   _promotion_failed = true;
717   _promotion_failed_info.register_copy_failure(old-&gt;size());
718   _preserved_marks_set.get()-&gt;push_if_necessary(old, old-&gt;mark_raw());
719   // forward to self
720   old-&gt;forward_to(old);
721 
722   _promo_failure_scan_stack.push(old);
723 
724   if (!_promo_failure_drain_in_progress) {
725     // prevent recursion in copy_to_survivor_space()
726     _promo_failure_drain_in_progress = true;
727     drain_promo_failure_scan_stack();
728     _promo_failure_drain_in_progress = false;
729   }
730 }
731 
732 oop DefNewGeneration::copy_to_survivor_space(oop old) {
733   assert(is_in_reserved(old) &amp;&amp; !old-&gt;is_forwarded(),
734          &quot;shouldn&#39;t be scavenging this oop&quot;);
735   size_t s = old-&gt;size();
736   oop obj = NULL;
737 
738   // Try allocating obj in to-space (unless too old)
739   if (old-&gt;age() &lt; tenuring_threshold()) {
740     obj = (oop) to()-&gt;allocate_aligned(s);
741   }
742 
743   // Otherwise try allocating obj tenured
744   if (obj == NULL) {
745     obj = _old_gen-&gt;promote(old, s);
746     if (obj == NULL) {
747       handle_promotion_failure(old);
748       return old;
749     }
750   } else {
751     // Prefetch beyond obj
752     const intx interval = PrefetchCopyIntervalInBytes;
753     Prefetch::write(obj, interval);
754 
755     // Copy obj
756     Copy::aligned_disjoint_words(cast_from_oop&lt;HeapWord*&gt;(old), cast_from_oop&lt;HeapWord*&gt;(obj), s);
757 
758     // Increment age if obj still in new generation
759     obj-&gt;incr_age();
760     age_table()-&gt;add(obj, s);
761   }
762 
763   // Done, insert forward pointer to obj in this header
764   old-&gt;forward_to(obj);
765 
766   return obj;
767 }
768 
769 void DefNewGeneration::drain_promo_failure_scan_stack() {
770   while (!_promo_failure_scan_stack.is_empty()) {
771      oop obj = _promo_failure_scan_stack.pop();
772      obj-&gt;oop_iterate(_promo_failure_scan_stack_closure);
773   }
774 }
775 
776 void DefNewGeneration::save_marks() {
777   eden()-&gt;set_saved_mark();
778   to()-&gt;set_saved_mark();
779   from()-&gt;set_saved_mark();
780 }
781 
782 
783 void DefNewGeneration::reset_saved_marks() {
784   eden()-&gt;reset_saved_mark();
785   to()-&gt;reset_saved_mark();
786   from()-&gt;reset_saved_mark();
787 }
788 
789 
790 bool DefNewGeneration::no_allocs_since_save_marks() {
791   assert(eden()-&gt;saved_mark_at_top(), &quot;Violated spec - alloc in eden&quot;);
792   assert(from()-&gt;saved_mark_at_top(), &quot;Violated spec - alloc in from&quot;);
793   return to()-&gt;saved_mark_at_top();
794 }
795 
796 void DefNewGeneration::contribute_scratch(ScratchBlock*&amp; list, Generation* requestor,
797                                          size_t max_alloc_words) {
798   if (requestor == this || _promotion_failed) {
799     return;
800   }
801   assert(GenCollectedHeap::heap()-&gt;is_old_gen(requestor), &quot;We should not call our own generation&quot;);
802 
803   /* $$$ Assert this?  &quot;trace&quot; is a &quot;MarkSweep&quot; function so that&#39;s not appropriate.
804   if (to_space-&gt;top() &gt; to_space-&gt;bottom()) {
805     trace(&quot;to_space not empty when contribute_scratch called&quot;);
806   }
807   */
808 
809   ContiguousSpace* to_space = to();
810   assert(to_space-&gt;end() &gt;= to_space-&gt;top(), &quot;pointers out of order&quot;);
811   size_t free_words = pointer_delta(to_space-&gt;end(), to_space-&gt;top());
812   if (free_words &gt;= MinFreeScratchWords) {
813     ScratchBlock* sb = (ScratchBlock*)to_space-&gt;top();
814     sb-&gt;num_words = free_words;
815     sb-&gt;next = list;
816     list = sb;
817   }
818 }
819 
820 void DefNewGeneration::reset_scratch() {
821   // If contributing scratch in to_space, mangle all of
822   // to_space if ZapUnusedHeapArea.  This is needed because
823   // top is not maintained while using to-space as scratch.
824   if (ZapUnusedHeapArea) {
825     to()-&gt;mangle_unused_area_complete();
826   }
827 }
828 
829 bool DefNewGeneration::collection_attempt_is_safe() {
830   if (!to()-&gt;is_empty()) {
831     log_trace(gc)(&quot;:: to is not empty ::&quot;);
832     return false;
833   }
834   if (_old_gen == NULL) {
835     GenCollectedHeap* gch = GenCollectedHeap::heap();
836     _old_gen = gch-&gt;old_gen();
837   }
838   return _old_gen-&gt;promotion_attempt_is_safe(used());
839 }
840 
841 void DefNewGeneration::gc_epilogue(bool full) {
842   DEBUG_ONLY(static bool seen_incremental_collection_failed = false;)
843 
844   assert(!GCLocker::is_active(), &quot;We should not be executing here&quot;);
845   // Check if the heap is approaching full after a collection has
846   // been done.  Generally the young generation is empty at
847   // a minimum at the end of a collection.  If it is not, then
848   // the heap is approaching full.
849   GenCollectedHeap* gch = GenCollectedHeap::heap();
850   if (full) {
851     DEBUG_ONLY(seen_incremental_collection_failed = false;)
852     if (!collection_attempt_is_safe() &amp;&amp; !_eden_space-&gt;is_empty()) {
853       log_trace(gc)(&quot;DefNewEpilogue: cause(%s), full, not safe, set_failed, set_alloc_from, clear_seen&quot;,
854                             GCCause::to_string(gch-&gt;gc_cause()));
855       gch-&gt;set_incremental_collection_failed(); // Slight lie: a full gc left us in that state
856       set_should_allocate_from_space(); // we seem to be running out of space
857     } else {
858       log_trace(gc)(&quot;DefNewEpilogue: cause(%s), full, safe, clear_failed, clear_alloc_from, clear_seen&quot;,
859                             GCCause::to_string(gch-&gt;gc_cause()));
860       gch-&gt;clear_incremental_collection_failed(); // We just did a full collection
861       clear_should_allocate_from_space(); // if set
862     }
863   } else {
864 #ifdef ASSERT
865     // It is possible that incremental_collection_failed() == true
866     // here, because an attempted scavenge did not succeed. The policy
867     // is normally expected to cause a full collection which should
868     // clear that condition, so we should not be here twice in a row
869     // with incremental_collection_failed() == true without having done
870     // a full collection in between.
871     if (!seen_incremental_collection_failed &amp;&amp;
872         gch-&gt;incremental_collection_failed()) {
873       log_trace(gc)(&quot;DefNewEpilogue: cause(%s), not full, not_seen_failed, failed, set_seen_failed&quot;,
874                             GCCause::to_string(gch-&gt;gc_cause()));
875       seen_incremental_collection_failed = true;
876     } else if (seen_incremental_collection_failed) {
877       log_trace(gc)(&quot;DefNewEpilogue: cause(%s), not full, seen_failed, will_clear_seen_failed&quot;,
878                             GCCause::to_string(gch-&gt;gc_cause()));
879       assert(gch-&gt;gc_cause() == GCCause::_scavenge_alot ||
880              !gch-&gt;incremental_collection_failed(),
881              &quot;Twice in a row&quot;);
882       seen_incremental_collection_failed = false;
883     }
884 #endif // ASSERT
885   }
886 
887   if (ZapUnusedHeapArea) {
888     eden()-&gt;check_mangled_unused_area_complete();
889     from()-&gt;check_mangled_unused_area_complete();
890     to()-&gt;check_mangled_unused_area_complete();
891   }
892 
893   if (!CleanChunkPoolAsync) {
894     Chunk::clean_chunk_pool();
895   }
896 
897   // update the generation and space performance counters
898   update_counters();
899   gch-&gt;counters()-&gt;update_counters();
900 }
901 
902 void DefNewGeneration::record_spaces_top() {
903   assert(ZapUnusedHeapArea, &quot;Not mangling unused space&quot;);
904   eden()-&gt;set_top_for_allocations();
905   to()-&gt;set_top_for_allocations();
906   from()-&gt;set_top_for_allocations();
907 }
908 
909 void DefNewGeneration::ref_processor_init() {
910   Generation::ref_processor_init();
911 }
912 
913 
914 void DefNewGeneration::update_counters() {
915   if (UsePerfData) {
916     _eden_counters-&gt;update_all();
917     _from_counters-&gt;update_all();
918     _to_counters-&gt;update_all();
919     _gen_counters-&gt;update_all();
920   }
921 }
922 
923 void DefNewGeneration::verify() {
924   eden()-&gt;verify();
925   from()-&gt;verify();
926     to()-&gt;verify();
927 }
928 
929 void DefNewGeneration::print_on(outputStream* st) const {
930   Generation::print_on(st);
931   st-&gt;print(&quot;  eden&quot;);
932   eden()-&gt;print_on(st);
933   st-&gt;print(&quot;  from&quot;);
934   from()-&gt;print_on(st);
935   st-&gt;print(&quot;  to  &quot;);
936   to()-&gt;print_on(st);
937 }
938 
939 
940 const char* DefNewGeneration::name() const {
941   return &quot;def new generation&quot;;
942 }
943 
944 // Moved from inline file as they are not called inline
945 CompactibleSpace* DefNewGeneration::first_compaction_space() const {
946   return eden();
947 }
948 
949 HeapWord* DefNewGeneration::allocate(size_t word_size, bool is_tlab) {
950   // This is the slow-path allocation for the DefNewGeneration.
951   // Most allocations are fast-path in compiled code.
952   // We try to allocate from the eden.  If that works, we are happy.
953   // Note that since DefNewGeneration supports lock-free allocation, we
954   // have to use it here, as well.
955   HeapWord* result = eden()-&gt;par_allocate(word_size);
956   if (result != NULL) {
957     if (_old_gen != NULL) {
958       _old_gen-&gt;sample_eden_chunk();
959     }
960   } else {
961     // If the eden is full and the last collection bailed out, we are running
962     // out of heap space, and we try to allocate the from-space, too.
963     // allocate_from_space can&#39;t be inlined because that would introduce a
964     // circular dependency at compile time.
965     result = allocate_from_space(word_size);
966   }
967   return result;
968 }
969 
970 HeapWord* DefNewGeneration::par_allocate(size_t word_size,
971                                          bool is_tlab) {
972   HeapWord* res = eden()-&gt;par_allocate(word_size);
973   if (_old_gen != NULL) {
974     _old_gen-&gt;sample_eden_chunk();
975   }
976   return res;
977 }
978 
979 size_t DefNewGeneration::tlab_capacity() const {
980   return eden()-&gt;capacity();
981 }
982 
983 size_t DefNewGeneration::tlab_used() const {
984   return eden()-&gt;used();
985 }
986 
987 size_t DefNewGeneration::unsafe_max_tlab_alloc() const {
988   return unsafe_max_alloc_nogc();
989 }
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="2" type="hidden" />
</body>
</html>