<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/gc/serial/defNewGeneration.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
    <script type="text/javascript" src="../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 2001, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/serial/defNewGeneration.inline.hpp&quot;
 27 #include &quot;gc/serial/serialHeap.inline.hpp&quot;
 28 #include &quot;gc/serial/tenuredGeneration.hpp&quot;
 29 #include &quot;gc/shared/adaptiveSizePolicy.hpp&quot;
 30 #include &quot;gc/shared/ageTable.inline.hpp&quot;
 31 #include &quot;gc/shared/cardTableRS.hpp&quot;
 32 #include &quot;gc/shared/collectorCounters.hpp&quot;
 33 #include &quot;gc/shared/gcArguments.hpp&quot;
 34 #include &quot;gc/shared/gcHeapSummary.hpp&quot;
 35 #include &quot;gc/shared/gcLocker.hpp&quot;
 36 #include &quot;gc/shared/gcPolicyCounters.hpp&quot;
 37 #include &quot;gc/shared/gcTimer.hpp&quot;
 38 #include &quot;gc/shared/gcTrace.hpp&quot;
 39 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
 40 #include &quot;gc/shared/genOopClosures.inline.hpp&quot;
 41 #include &quot;gc/shared/generationSpec.hpp&quot;
 42 #include &quot;gc/shared/preservedMarks.inline.hpp&quot;
 43 #include &quot;gc/shared/referencePolicy.hpp&quot;
 44 #include &quot;gc/shared/referenceProcessorPhaseTimes.hpp&quot;
 45 #include &quot;gc/shared/space.inline.hpp&quot;
 46 #include &quot;gc/shared/spaceDecorator.inline.hpp&quot;
 47 #include &quot;gc/shared/strongRootsScope.hpp&quot;
 48 #include &quot;gc/shared/weakProcessor.hpp&quot;
 49 #include &quot;logging/log.hpp&quot;
 50 #include &quot;memory/iterator.inline.hpp&quot;
 51 #include &quot;memory/resourceArea.hpp&quot;
 52 #include &quot;oops/instanceRefKlass.hpp&quot;
 53 #include &quot;oops/oop.inline.hpp&quot;
 54 #include &quot;runtime/java.hpp&quot;
 55 #include &quot;runtime/prefetch.inline.hpp&quot;
 56 #include &quot;runtime/thread.inline.hpp&quot;
 57 #include &quot;utilities/align.hpp&quot;
 58 #include &quot;utilities/copy.hpp&quot;
 59 #include &quot;utilities/globalDefinitions.hpp&quot;
 60 #include &quot;utilities/stack.inline.hpp&quot;
 61 
 62 //
 63 // DefNewGeneration functions.
 64 
 65 // Methods of protected closure types.
 66 
 67 DefNewGeneration::IsAliveClosure::IsAliveClosure(Generation* young_gen) : _young_gen(young_gen) {
 68   assert(_young_gen-&gt;kind() == Generation::DefNew, &quot;Expected the young generation here&quot;);
 69 }
 70 
 71 bool DefNewGeneration::IsAliveClosure::do_object_b(oop p) {
 72   return cast_from_oop&lt;HeapWord*&gt;(p) &gt;= _young_gen-&gt;reserved().end() || p-&gt;is_forwarded();
 73 }
 74 
 75 DefNewGeneration::KeepAliveClosure::
 76 KeepAliveClosure(ScanWeakRefClosure* cl) : _cl(cl) {
 77   _rs = GenCollectedHeap::heap()-&gt;rem_set();
 78 }
 79 
 80 void DefNewGeneration::KeepAliveClosure::do_oop(oop* p)       { DefNewGeneration::KeepAliveClosure::do_oop_work(p); }
 81 void DefNewGeneration::KeepAliveClosure::do_oop(narrowOop* p) { DefNewGeneration::KeepAliveClosure::do_oop_work(p); }
 82 
 83 
 84 DefNewGeneration::FastKeepAliveClosure::
 85 FastKeepAliveClosure(DefNewGeneration* g, ScanWeakRefClosure* cl) :
 86   DefNewGeneration::KeepAliveClosure(cl) {
 87   _boundary = g-&gt;reserved().end();
 88 }
 89 
 90 void DefNewGeneration::FastKeepAliveClosure::do_oop(oop* p)       { DefNewGeneration::FastKeepAliveClosure::do_oop_work(p); }
 91 void DefNewGeneration::FastKeepAliveClosure::do_oop(narrowOop* p) { DefNewGeneration::FastKeepAliveClosure::do_oop_work(p); }
 92 
 93 DefNewGeneration::FastEvacuateFollowersClosure::
 94 FastEvacuateFollowersClosure(SerialHeap* heap,
 95                              FastScanClosure* cur,
 96                              FastScanClosure* older) :
 97   _heap(heap), _scan_cur_or_nonheap(cur), _scan_older(older)
 98 {
 99 }
100 
101 void DefNewGeneration::FastEvacuateFollowersClosure::do_void() {
102   do {
103     _heap-&gt;oop_since_save_marks_iterate(_scan_cur_or_nonheap, _scan_older);
104   } while (!_heap-&gt;no_allocs_since_save_marks());
105   guarantee(_heap-&gt;young_gen()-&gt;promo_failure_scan_is_complete(), &quot;Failed to finish scan&quot;);
106 }
107 
108 ScanClosure::ScanClosure(DefNewGeneration* g, bool gc_barrier) :
109     OopsInClassLoaderDataOrGenClosure(g), _g(g), _gc_barrier(gc_barrier)
110 {
111   _boundary = _g-&gt;reserved().end();
112 }
113 
114 FastScanClosure::FastScanClosure(DefNewGeneration* g, bool gc_barrier) :
115     OopsInClassLoaderDataOrGenClosure(g), _g(g), _gc_barrier(gc_barrier)
116 {
117   _boundary = _g-&gt;reserved().end();
118 }
119 
120 void CLDScanClosure::do_cld(ClassLoaderData* cld) {
121   NOT_PRODUCT(ResourceMark rm);
122   log_develop_trace(gc, scavenge)(&quot;CLDScanClosure::do_cld &quot; PTR_FORMAT &quot;, %s, dirty: %s&quot;,
123                                   p2i(cld),
124                                   cld-&gt;loader_name_and_id(),
125                                   cld-&gt;has_modified_oops() ? &quot;true&quot; : &quot;false&quot;);
126 
127   // If the cld has not been dirtied we know that there&#39;s
128   // no references into  the young gen and we can skip it.
129   if (cld-&gt;has_modified_oops()) {
130     if (_accumulate_modified_oops) {
131       cld-&gt;accumulate_modified_oops();
132     }
133 
134     // Tell the closure which CLD is being scanned so that it can be dirtied
135     // if oops are left pointing into the young gen.
136     _scavenge_closure-&gt;set_scanned_cld(cld);
137 
138     // Clean the cld since we&#39;re going to scavenge all the metadata.
139     cld-&gt;oops_do(_scavenge_closure, ClassLoaderData::_claim_none, /*clear_modified_oops*/true);
140 
141     _scavenge_closure-&gt;set_scanned_cld(NULL);
142   }
143 }
144 
145 ScanWeakRefClosure::ScanWeakRefClosure(DefNewGeneration* g) :
146   _g(g)
147 {
148   _boundary = _g-&gt;reserved().end();
149 }
150 
151 DefNewGeneration::DefNewGeneration(ReservedSpace rs,
152                                    size_t initial_size,
153                                    size_t min_size,
154                                    size_t max_size,
155                                    const char* policy)
156   : Generation(rs, initial_size),
157     _preserved_marks_set(false /* in_c_heap */),
158     _promo_failure_drain_in_progress(false),
159     _should_allocate_from_space(false)
160 {
161   MemRegion cmr((HeapWord*)_virtual_space.low(),
162                 (HeapWord*)_virtual_space.high());
163   GenCollectedHeap* gch = GenCollectedHeap::heap();
164 
165   gch-&gt;rem_set()-&gt;resize_covered_region(cmr);
166 
167   _eden_space = new ContiguousSpace();
168   _from_space = new ContiguousSpace();
169   _to_space   = new ContiguousSpace();
170 
<a name="1" id="anc1"></a><span class="line-removed">171   if (_eden_space == NULL || _from_space == NULL || _to_space == NULL) {</span>
<span class="line-removed">172     vm_exit_during_initialization(&quot;Could not allocate a new gen space&quot;);</span>
<span class="line-removed">173   }</span>
<span class="line-removed">174 </span>
175   // Compute the maximum eden and survivor space sizes. These sizes
176   // are computed assuming the entire reserved space is committed.
177   // These values are exported as performance counters.
178   uintx size = _virtual_space.reserved_size();
179   _max_survivor_size = compute_survivor_size(size, SpaceAlignment);
180   _max_eden_size = size - (2*_max_survivor_size);
181 
182   // allocate the performance counters
183 
184   // Generation counters -- generation 0, 3 subspaces
185   _gen_counters = new GenerationCounters(&quot;new&quot;, 0, 3,
186       min_size, max_size, &amp;_virtual_space);
187   _gc_counters = new CollectorCounters(policy, 0);
188 
189   _eden_counters = new CSpaceCounters(&quot;eden&quot;, 0, _max_eden_size, _eden_space,
190                                       _gen_counters);
191   _from_counters = new CSpaceCounters(&quot;s0&quot;, 1, _max_survivor_size, _from_space,
192                                       _gen_counters);
193   _to_counters = new CSpaceCounters(&quot;s1&quot;, 2, _max_survivor_size, _to_space,
194                                     _gen_counters);
195 
196   compute_space_boundaries(0, SpaceDecorator::Clear, SpaceDecorator::Mangle);
197   update_counters();
198   _old_gen = NULL;
199   _tenuring_threshold = MaxTenuringThreshold;
200   _pretenure_size_threshold_words = PretenureSizeThreshold &gt;&gt; LogHeapWordSize;
201 
202   _gc_timer = new (ResourceObj::C_HEAP, mtGC) STWGCTimer();
203 }
204 
205 void DefNewGeneration::compute_space_boundaries(uintx minimum_eden_size,
206                                                 bool clear_space,
207                                                 bool mangle_space) {
208   // If the spaces are being cleared (only done at heap initialization
209   // currently), the survivor spaces need not be empty.
210   // Otherwise, no care is taken for used areas in the survivor spaces
211   // so check.
212   assert(clear_space || (to()-&gt;is_empty() &amp;&amp; from()-&gt;is_empty()),
213     &quot;Initialization of the survivor spaces assumes these are empty&quot;);
214 
215   // Compute sizes
216   uintx size = _virtual_space.committed_size();
217   uintx survivor_size = compute_survivor_size(size, SpaceAlignment);
218   uintx eden_size = size - (2*survivor_size);
219   assert(eden_size &gt; 0 &amp;&amp; survivor_size &lt;= eden_size, &quot;just checking&quot;);
220 
221   if (eden_size &lt; minimum_eden_size) {
222     // May happen due to 64Kb rounding, if so adjust eden size back up
223     minimum_eden_size = align_up(minimum_eden_size, SpaceAlignment);
224     uintx maximum_survivor_size = (size - minimum_eden_size) / 2;
225     uintx unaligned_survivor_size =
226       align_down(maximum_survivor_size, SpaceAlignment);
227     survivor_size = MAX2(unaligned_survivor_size, SpaceAlignment);
228     eden_size = size - (2*survivor_size);
229     assert(eden_size &gt; 0 &amp;&amp; survivor_size &lt;= eden_size, &quot;just checking&quot;);
230     assert(eden_size &gt;= minimum_eden_size, &quot;just checking&quot;);
231   }
232 
233   char *eden_start = _virtual_space.low();
234   char *from_start = eden_start + eden_size;
235   char *to_start   = from_start + survivor_size;
236   char *to_end     = to_start   + survivor_size;
237 
238   assert(to_end == _virtual_space.high(), &quot;just checking&quot;);
239   assert(Space::is_aligned(eden_start), &quot;checking alignment&quot;);
240   assert(Space::is_aligned(from_start), &quot;checking alignment&quot;);
241   assert(Space::is_aligned(to_start),   &quot;checking alignment&quot;);
242 
243   MemRegion edenMR((HeapWord*)eden_start, (HeapWord*)from_start);
244   MemRegion fromMR((HeapWord*)from_start, (HeapWord*)to_start);
245   MemRegion toMR  ((HeapWord*)to_start, (HeapWord*)to_end);
246 
247   // A minimum eden size implies that there is a part of eden that
248   // is being used and that affects the initialization of any
249   // newly formed eden.
250   bool live_in_eden = minimum_eden_size &gt; 0;
251 
252   // If not clearing the spaces, do some checking to verify that
253   // the space are already mangled.
254   if (!clear_space) {
255     // Must check mangling before the spaces are reshaped.  Otherwise,
256     // the bottom or end of one space may have moved into another
257     // a failure of the check may not correctly indicate which space
258     // is not properly mangled.
259     if (ZapUnusedHeapArea) {
260       HeapWord* limit = (HeapWord*) _virtual_space.high();
261       eden()-&gt;check_mangled_unused_area(limit);
262       from()-&gt;check_mangled_unused_area(limit);
263         to()-&gt;check_mangled_unused_area(limit);
264     }
265   }
266 
267   // Reset the spaces for their new regions.
268   eden()-&gt;initialize(edenMR,
269                      clear_space &amp;&amp; !live_in_eden,
270                      SpaceDecorator::Mangle);
271   // If clear_space and live_in_eden, we will not have cleared any
272   // portion of eden above its top. This can cause newly
273   // expanded space not to be mangled if using ZapUnusedHeapArea.
274   // We explicitly do such mangling here.
275   if (ZapUnusedHeapArea &amp;&amp; clear_space &amp;&amp; live_in_eden &amp;&amp; mangle_space) {
276     eden()-&gt;mangle_unused_area();
277   }
278   from()-&gt;initialize(fromMR, clear_space, mangle_space);
279   to()-&gt;initialize(toMR, clear_space, mangle_space);
280 
281   // Set next compaction spaces.
282   eden()-&gt;set_next_compaction_space(from());
283   // The to-space is normally empty before a compaction so need
284   // not be considered.  The exception is during promotion
285   // failure handling when to-space can contain live objects.
286   from()-&gt;set_next_compaction_space(NULL);
287 }
288 
289 void DefNewGeneration::swap_spaces() {
290   ContiguousSpace* s = from();
291   _from_space        = to();
292   _to_space          = s;
293   eden()-&gt;set_next_compaction_space(from());
294   // The to-space is normally empty before a compaction so need
295   // not be considered.  The exception is during promotion
296   // failure handling when to-space can contain live objects.
297   from()-&gt;set_next_compaction_space(NULL);
298 
299   if (UsePerfData) {
300     CSpaceCounters* c = _from_counters;
301     _from_counters = _to_counters;
302     _to_counters = c;
303   }
304 }
305 
306 bool DefNewGeneration::expand(size_t bytes) {
307   MutexLocker x(ExpandHeap_lock);
308   HeapWord* prev_high = (HeapWord*) _virtual_space.high();
309   bool success = _virtual_space.expand_by(bytes);
310   if (success &amp;&amp; ZapUnusedHeapArea) {
311     // Mangle newly committed space immediately because it
312     // can be done here more simply that after the new
313     // spaces have been computed.
314     HeapWord* new_high = (HeapWord*) _virtual_space.high();
315     MemRegion mangle_region(prev_high, new_high);
316     SpaceMangler::mangle_region(mangle_region);
317   }
318 
319   // Do not attempt an expand-to-the reserve size.  The
320   // request should properly observe the maximum size of
321   // the generation so an expand-to-reserve should be
322   // unnecessary.  Also a second call to expand-to-reserve
323   // value potentially can cause an undue expansion.
324   // For example if the first expand fail for unknown reasons,
325   // but the second succeeds and expands the heap to its maximum
326   // value.
327   if (GCLocker::is_active()) {
328     log_debug(gc)(&quot;Garbage collection disabled, expanded heap instead&quot;);
329   }
330 
331   return success;
332 }
333 
334 size_t DefNewGeneration::adjust_for_thread_increase(size_t new_size_candidate,
335                                                     size_t new_size_before,
336                                                     size_t alignment) const {
337   size_t desired_new_size = new_size_before;
338 
339   if (NewSizeThreadIncrease &gt; 0) {
340     int threads_count;
341     size_t thread_increase_size = 0;
342 
343     // 1. Check an overflow at &#39;threads_count * NewSizeThreadIncrease&#39;.
344     threads_count = Threads::number_of_non_daemon_threads();
345     if (threads_count &gt; 0 &amp;&amp; NewSizeThreadIncrease &lt;= max_uintx / threads_count) {
346       thread_increase_size = threads_count * NewSizeThreadIncrease;
347 
348       // 2. Check an overflow at &#39;new_size_candidate + thread_increase_size&#39;.
349       if (new_size_candidate &lt;= max_uintx - thread_increase_size) {
350         new_size_candidate += thread_increase_size;
351 
352         // 3. Check an overflow at &#39;align_up&#39;.
353         size_t aligned_max = ((max_uintx - alignment) &amp; ~(alignment-1));
354         if (new_size_candidate &lt;= aligned_max) {
355           desired_new_size = align_up(new_size_candidate, alignment);
356         }
357       }
358     }
359   }
360 
361   return desired_new_size;
362 }
363 
364 void DefNewGeneration::compute_new_size() {
365   // This is called after a GC that includes the old generation, so from-space
366   // will normally be empty.
367   // Note that we check both spaces, since if scavenge failed they revert roles.
368   // If not we bail out (otherwise we would have to relocate the objects).
369   if (!from()-&gt;is_empty() || !to()-&gt;is_empty()) {
370     return;
371   }
372 
373   GenCollectedHeap* gch = GenCollectedHeap::heap();
374 
375   size_t old_size = gch-&gt;old_gen()-&gt;capacity();
376   size_t new_size_before = _virtual_space.committed_size();
377   size_t min_new_size = initial_size();
378   size_t max_new_size = reserved().byte_size();
379   assert(min_new_size &lt;= new_size_before &amp;&amp;
380          new_size_before &lt;= max_new_size,
381          &quot;just checking&quot;);
382   // All space sizes must be multiples of Generation::GenGrain.
383   size_t alignment = Generation::GenGrain;
384 
385   int threads_count = 0;
386   size_t thread_increase_size = 0;
387 
388   size_t new_size_candidate = old_size / NewRatio;
389   // Compute desired new generation size based on NewRatio and NewSizeThreadIncrease
390   // and reverts to previous value if any overflow happens
391   size_t desired_new_size = adjust_for_thread_increase(new_size_candidate, new_size_before, alignment);
392 
393   // Adjust new generation size
394   desired_new_size = clamp(desired_new_size, min_new_size, max_new_size);
395   assert(desired_new_size &lt;= max_new_size, &quot;just checking&quot;);
396 
397   bool changed = false;
398   if (desired_new_size &gt; new_size_before) {
399     size_t change = desired_new_size - new_size_before;
400     assert(change % alignment == 0, &quot;just checking&quot;);
401     if (expand(change)) {
402        changed = true;
403     }
404     // If the heap failed to expand to the desired size,
405     // &quot;changed&quot; will be false.  If the expansion failed
406     // (and at this point it was expected to succeed),
407     // ignore the failure (leaving &quot;changed&quot; as false).
408   }
409   if (desired_new_size &lt; new_size_before &amp;&amp; eden()-&gt;is_empty()) {
410     // bail out of shrinking if objects in eden
411     size_t change = new_size_before - desired_new_size;
412     assert(change % alignment == 0, &quot;just checking&quot;);
413     _virtual_space.shrink_by(change);
414     changed = true;
415   }
416   if (changed) {
417     // The spaces have already been mangled at this point but
418     // may not have been cleared (set top = bottom) and should be.
419     // Mangling was done when the heap was being expanded.
420     compute_space_boundaries(eden()-&gt;used(),
421                              SpaceDecorator::Clear,
422                              SpaceDecorator::DontMangle);
423     MemRegion cmr((HeapWord*)_virtual_space.low(),
424                   (HeapWord*)_virtual_space.high());
425     gch-&gt;rem_set()-&gt;resize_covered_region(cmr);
426 
427     log_debug(gc, ergo, heap)(
428         &quot;New generation size &quot; SIZE_FORMAT &quot;K-&gt;&quot; SIZE_FORMAT &quot;K [eden=&quot; SIZE_FORMAT &quot;K,survivor=&quot; SIZE_FORMAT &quot;K]&quot;,
429         new_size_before/K, _virtual_space.committed_size()/K,
430         eden()-&gt;capacity()/K, from()-&gt;capacity()/K);
431     log_trace(gc, ergo, heap)(
432         &quot;  [allowed &quot; SIZE_FORMAT &quot;K extra for %d threads]&quot;,
433           thread_increase_size/K, threads_count);
434       }
435 }
436 
437 void DefNewGeneration::younger_refs_iterate(OopsInGenClosure* cl, uint n_threads) {
438   assert(false, &quot;NYI -- are you sure you want to call this?&quot;);
439 }
440 
441 
442 size_t DefNewGeneration::capacity() const {
443   return eden()-&gt;capacity()
444        + from()-&gt;capacity();  // to() is only used during scavenge
445 }
446 
447 
448 size_t DefNewGeneration::used() const {
449   return eden()-&gt;used()
450        + from()-&gt;used();      // to() is only used during scavenge
451 }
452 
453 
454 size_t DefNewGeneration::free() const {
455   return eden()-&gt;free()
456        + from()-&gt;free();      // to() is only used during scavenge
457 }
458 
459 size_t DefNewGeneration::max_capacity() const {
460   const size_t reserved_bytes = reserved().byte_size();
461   return reserved_bytes - compute_survivor_size(reserved_bytes, SpaceAlignment);
462 }
463 
464 size_t DefNewGeneration::unsafe_max_alloc_nogc() const {
465   return eden()-&gt;free();
466 }
467 
468 size_t DefNewGeneration::capacity_before_gc() const {
469   return eden()-&gt;capacity();
470 }
471 
472 size_t DefNewGeneration::contiguous_available() const {
473   return eden()-&gt;free();
474 }
475 
476 
477 HeapWord* volatile* DefNewGeneration::top_addr() const { return eden()-&gt;top_addr(); }
478 HeapWord** DefNewGeneration::end_addr() const { return eden()-&gt;end_addr(); }
479 
480 void DefNewGeneration::object_iterate(ObjectClosure* blk) {
481   eden()-&gt;object_iterate(blk);
482   from()-&gt;object_iterate(blk);
483 }
484 
485 
486 void DefNewGeneration::space_iterate(SpaceClosure* blk,
487                                      bool usedOnly) {
488   blk-&gt;do_space(eden());
489   blk-&gt;do_space(from());
490   blk-&gt;do_space(to());
491 }
492 
493 // The last collection bailed out, we are running out of heap space,
494 // so we try to allocate the from-space, too.
495 HeapWord* DefNewGeneration::allocate_from_space(size_t size) {
496   bool should_try_alloc = should_allocate_from_space() || GCLocker::is_active_and_needs_gc();
497 
498   // If the Heap_lock is not locked by this thread, this will be called
499   // again later with the Heap_lock held.
500   bool do_alloc = should_try_alloc &amp;&amp; (Heap_lock-&gt;owned_by_self() || (SafepointSynchronize::is_at_safepoint() &amp;&amp; Thread::current()-&gt;is_VM_thread()));
501 
502   HeapWord* result = NULL;
503   if (do_alloc) {
504     result = from()-&gt;allocate(size);
505   }
506 
507   log_trace(gc, alloc)(&quot;DefNewGeneration::allocate_from_space(&quot; SIZE_FORMAT &quot;):  will_fail: %s  heap_lock: %s  free: &quot; SIZE_FORMAT &quot;%s%s returns %s&quot;,
508                         size,
509                         GenCollectedHeap::heap()-&gt;incremental_collection_will_fail(false /* don&#39;t consult_young */) ?
510                           &quot;true&quot; : &quot;false&quot;,
511                         Heap_lock-&gt;is_locked() ? &quot;locked&quot; : &quot;unlocked&quot;,
512                         from()-&gt;free(),
513                         should_try_alloc ? &quot;&quot; : &quot;  should_allocate_from_space: NOT&quot;,
514                         do_alloc ? &quot;  Heap_lock is not owned by self&quot; : &quot;&quot;,
515                         result == NULL ? &quot;NULL&quot; : &quot;object&quot;);
516 
517   return result;
518 }
519 
520 HeapWord* DefNewGeneration::expand_and_allocate(size_t size,
521                                                 bool   is_tlab,
522                                                 bool   parallel) {
523   // We don&#39;t attempt to expand the young generation (but perhaps we should.)
524   return allocate(size, is_tlab);
525 }
526 
527 void DefNewGeneration::adjust_desired_tenuring_threshold() {
528   // Set the desired survivor size to half the real survivor space
529   size_t const survivor_capacity = to()-&gt;capacity() / HeapWordSize;
530   size_t const desired_survivor_size = (size_t)((((double)survivor_capacity) * TargetSurvivorRatio) / 100);
531 
532   _tenuring_threshold = age_table()-&gt;compute_tenuring_threshold(desired_survivor_size);
533 
534   if (UsePerfData) {
535     GCPolicyCounters* gc_counters = GenCollectedHeap::heap()-&gt;counters();
536     gc_counters-&gt;tenuring_threshold()-&gt;set_value(_tenuring_threshold);
537     gc_counters-&gt;desired_survivor_size()-&gt;set_value(desired_survivor_size * oopSize);
538   }
539 
540   age_table()-&gt;print_age_table(_tenuring_threshold);
541 }
542 
543 void DefNewGeneration::collect(bool   full,
544                                bool   clear_all_soft_refs,
545                                size_t size,
546                                bool   is_tlab) {
547   assert(full || size &gt; 0, &quot;otherwise we don&#39;t want to collect&quot;);
548 
549   SerialHeap* heap = SerialHeap::heap();
550 
551   _gc_timer-&gt;register_gc_start();
552   DefNewTracer gc_tracer;
553   gc_tracer.report_gc_start(heap-&gt;gc_cause(), _gc_timer-&gt;gc_start());
554 
555   _old_gen = heap-&gt;old_gen();
556 
557   // If the next generation is too full to accommodate promotion
558   // from this generation, pass on collection; let the next generation
559   // do it.
560   if (!collection_attempt_is_safe()) {
561     log_trace(gc)(&quot;:: Collection attempt not safe ::&quot;);
562     heap-&gt;set_incremental_collection_failed(); // Slight lie: we did not even attempt one
563     return;
564   }
565   assert(to()-&gt;is_empty(), &quot;Else not collection_attempt_is_safe&quot;);
566 
567   init_assuming_no_promotion_failure();
568 
569   GCTraceTime(Trace, gc, phases) tm(&quot;DefNew&quot;, NULL, heap-&gt;gc_cause());
570 
571   heap-&gt;trace_heap_before_gc(&amp;gc_tracer);
572 
573   // These can be shared for all code paths
574   IsAliveClosure is_alive(this);
575   ScanWeakRefClosure scan_weak_ref(this);
576 
577   age_table()-&gt;clear();
578   to()-&gt;clear(SpaceDecorator::Mangle);
579   // The preserved marks should be empty at the start of the GC.
580   _preserved_marks_set.init(1);
581 
582   heap-&gt;rem_set()-&gt;prepare_for_younger_refs_iterate(false);
583 
584   assert(heap-&gt;no_allocs_since_save_marks(),
585          &quot;save marks have not been newly set.&quot;);
586 
587   FastScanClosure fsc_with_no_gc_barrier(this, false);
588   FastScanClosure fsc_with_gc_barrier(this, true);
589 
590   CLDScanClosure cld_scan_closure(&amp;fsc_with_no_gc_barrier,
591                                   heap-&gt;rem_set()-&gt;cld_rem_set()-&gt;accumulate_modified_oops());
592 
593   set_promo_failure_scan_stack_closure(&amp;fsc_with_no_gc_barrier);
594   FastEvacuateFollowersClosure evacuate_followers(heap,
595                                                   &amp;fsc_with_no_gc_barrier,
596                                                   &amp;fsc_with_gc_barrier);
597 
598   assert(heap-&gt;no_allocs_since_save_marks(),
599          &quot;save marks have not been newly set.&quot;);
600 
601   {
602     // DefNew needs to run with n_threads == 0, to make sure the serial
603     // version of the card table scanning code is used.
604     // See: CardTableRS::non_clean_card_iterate_possibly_parallel.
605     StrongRootsScope srs(0);
606 
607     heap-&gt;young_process_roots(&amp;srs,
608                               &amp;fsc_with_no_gc_barrier,
609                               &amp;fsc_with_gc_barrier,
610                               &amp;cld_scan_closure);
611   }
612 
613   // &quot;evacuate followers&quot;.
614   evacuate_followers.do_void();
615 
616   FastKeepAliveClosure keep_alive(this, &amp;scan_weak_ref);
617   ReferenceProcessor* rp = ref_processor();
618   rp-&gt;setup_policy(clear_all_soft_refs);
619   ReferenceProcessorPhaseTimes pt(_gc_timer, rp-&gt;max_num_queues());
620   const ReferenceProcessorStats&amp; stats =
621   rp-&gt;process_discovered_references(&amp;is_alive, &amp;keep_alive, &amp;evacuate_followers,
622                                     NULL, &amp;pt);
623   gc_tracer.report_gc_reference_stats(stats);
624   gc_tracer.report_tenuring_threshold(tenuring_threshold());
625   pt.print_all_references();
626 
627   assert(heap-&gt;no_allocs_since_save_marks(), &quot;save marks have not been newly set.&quot;);
628 
629   WeakProcessor::weak_oops_do(&amp;is_alive, &amp;keep_alive);
630 
631   // Verify that the usage of keep_alive didn&#39;t copy any objects.
632   assert(heap-&gt;no_allocs_since_save_marks(), &quot;save marks have not been newly set.&quot;);
633 
634   if (!_promotion_failed) {
635     // Swap the survivor spaces.
636     eden()-&gt;clear(SpaceDecorator::Mangle);
637     from()-&gt;clear(SpaceDecorator::Mangle);
638     if (ZapUnusedHeapArea) {
639       // This is now done here because of the piece-meal mangling which
640       // can check for valid mangling at intermediate points in the
641       // collection(s).  When a young collection fails to collect
642       // sufficient space resizing of the young generation can occur
643       // an redistribute the spaces in the young generation.  Mangle
644       // here so that unzapped regions don&#39;t get distributed to
645       // other spaces.
646       to()-&gt;mangle_unused_area();
647     }
648     swap_spaces();
649 
650     assert(to()-&gt;is_empty(), &quot;to space should be empty now&quot;);
651 
652     adjust_desired_tenuring_threshold();
653 
654     // A successful scavenge should restart the GC time limit count which is
655     // for full GC&#39;s.
656     AdaptiveSizePolicy* size_policy = heap-&gt;size_policy();
657     size_policy-&gt;reset_gc_overhead_limit_count();
658     assert(!heap-&gt;incremental_collection_failed(), &quot;Should be clear&quot;);
659   } else {
660     assert(_promo_failure_scan_stack.is_empty(), &quot;post condition&quot;);
661     _promo_failure_scan_stack.clear(true); // Clear cached segments.
662 
663     remove_forwarding_pointers();
664     log_info(gc, promotion)(&quot;Promotion failed&quot;);
665     // Add to-space to the list of space to compact
666     // when a promotion failure has occurred.  In that
667     // case there can be live objects in to-space
668     // as a result of a partial evacuation of eden
669     // and from-space.
670     swap_spaces();   // For uniformity wrt ParNewGeneration.
671     from()-&gt;set_next_compaction_space(to());
672     heap-&gt;set_incremental_collection_failed();
673 
674     // Inform the next generation that a promotion failure occurred.
675     _old_gen-&gt;promotion_failure_occurred();
676     gc_tracer.report_promotion_failed(_promotion_failed_info);
677 
678     // Reset the PromotionFailureALot counters.
679     NOT_PRODUCT(heap-&gt;reset_promotion_should_fail();)
680   }
681   // We should have processed and cleared all the preserved marks.
682   _preserved_marks_set.reclaim();
683   // set new iteration safe limit for the survivor spaces
684   from()-&gt;set_concurrent_iteration_safe_limit(from()-&gt;top());
685   to()-&gt;set_concurrent_iteration_safe_limit(to()-&gt;top());
686 
687   // We need to use a monotonically non-decreasing time in ms
688   // or we will see time-warp warnings and os::javaTimeMillis()
689   // does not guarantee monotonicity.
690   jlong now = os::javaTimeNanos() / NANOSECS_PER_MILLISEC;
691   update_time_of_last_gc(now);
692 
693   heap-&gt;trace_heap_after_gc(&amp;gc_tracer);
694 
695   _gc_timer-&gt;register_gc_end();
696 
697   gc_tracer.report_gc_end(_gc_timer-&gt;gc_end(), _gc_timer-&gt;time_partitions());
698 }
699 
700 void DefNewGeneration::init_assuming_no_promotion_failure() {
701   _promotion_failed = false;
702   _promotion_failed_info.reset();
703   from()-&gt;set_next_compaction_space(NULL);
704 }
705 
706 void DefNewGeneration::remove_forwarding_pointers() {
707   RemoveForwardedPointerClosure rspc;
708   eden()-&gt;object_iterate(&amp;rspc);
709   from()-&gt;object_iterate(&amp;rspc);
710   restore_preserved_marks();
711 }
712 
713 void DefNewGeneration::restore_preserved_marks() {
714   _preserved_marks_set.restore(NULL);
715 }
716 
717 void DefNewGeneration::handle_promotion_failure(oop old) {
718   log_debug(gc, promotion)(&quot;Promotion failure size = %d) &quot;, old-&gt;size());
719 
720   _promotion_failed = true;
721   _promotion_failed_info.register_copy_failure(old-&gt;size());
722   _preserved_marks_set.get()-&gt;push_if_necessary(old, old-&gt;mark_raw());
723   // forward to self
724   old-&gt;forward_to(old);
725 
726   _promo_failure_scan_stack.push(old);
727 
728   if (!_promo_failure_drain_in_progress) {
729     // prevent recursion in copy_to_survivor_space()
730     _promo_failure_drain_in_progress = true;
731     drain_promo_failure_scan_stack();
732     _promo_failure_drain_in_progress = false;
733   }
734 }
735 
736 oop DefNewGeneration::copy_to_survivor_space(oop old) {
737   assert(is_in_reserved(old) &amp;&amp; !old-&gt;is_forwarded(),
738          &quot;shouldn&#39;t be scavenging this oop&quot;);
739   size_t s = old-&gt;size();
740   oop obj = NULL;
741 
742   // Try allocating obj in to-space (unless too old)
743   if (old-&gt;age() &lt; tenuring_threshold()) {
744     obj = (oop) to()-&gt;allocate_aligned(s);
745   }
746 
747   // Otherwise try allocating obj tenured
748   if (obj == NULL) {
749     obj = _old_gen-&gt;promote(old, s);
750     if (obj == NULL) {
751       handle_promotion_failure(old);
752       return old;
753     }
754   } else {
755     // Prefetch beyond obj
756     const intx interval = PrefetchCopyIntervalInBytes;
757     Prefetch::write(obj, interval);
758 
759     // Copy obj
760     Copy::aligned_disjoint_words(cast_from_oop&lt;HeapWord*&gt;(old), cast_from_oop&lt;HeapWord*&gt;(obj), s);
761 
762     // Increment age if obj still in new generation
763     obj-&gt;incr_age();
764     age_table()-&gt;add(obj, s);
765   }
766 
767   // Done, insert forward pointer to obj in this header
768   old-&gt;forward_to(obj);
769 
770   return obj;
771 }
772 
773 void DefNewGeneration::drain_promo_failure_scan_stack() {
774   while (!_promo_failure_scan_stack.is_empty()) {
775      oop obj = _promo_failure_scan_stack.pop();
776      obj-&gt;oop_iterate(_promo_failure_scan_stack_closure);
777   }
778 }
779 
780 void DefNewGeneration::save_marks() {
781   eden()-&gt;set_saved_mark();
782   to()-&gt;set_saved_mark();
783   from()-&gt;set_saved_mark();
784 }
785 
786 
787 void DefNewGeneration::reset_saved_marks() {
788   eden()-&gt;reset_saved_mark();
789   to()-&gt;reset_saved_mark();
790   from()-&gt;reset_saved_mark();
791 }
792 
793 
794 bool DefNewGeneration::no_allocs_since_save_marks() {
795   assert(eden()-&gt;saved_mark_at_top(), &quot;Violated spec - alloc in eden&quot;);
796   assert(from()-&gt;saved_mark_at_top(), &quot;Violated spec - alloc in from&quot;);
797   return to()-&gt;saved_mark_at_top();
798 }
799 
800 void DefNewGeneration::contribute_scratch(ScratchBlock*&amp; list, Generation* requestor,
801                                          size_t max_alloc_words) {
802   if (requestor == this || _promotion_failed) {
803     return;
804   }
805   assert(GenCollectedHeap::heap()-&gt;is_old_gen(requestor), &quot;We should not call our own generation&quot;);
806 
807   /* $$$ Assert this?  &quot;trace&quot; is a &quot;MarkSweep&quot; function so that&#39;s not appropriate.
808   if (to_space-&gt;top() &gt; to_space-&gt;bottom()) {
809     trace(&quot;to_space not empty when contribute_scratch called&quot;);
810   }
811   */
812 
813   ContiguousSpace* to_space = to();
814   assert(to_space-&gt;end() &gt;= to_space-&gt;top(), &quot;pointers out of order&quot;);
815   size_t free_words = pointer_delta(to_space-&gt;end(), to_space-&gt;top());
816   if (free_words &gt;= MinFreeScratchWords) {
817     ScratchBlock* sb = (ScratchBlock*)to_space-&gt;top();
818     sb-&gt;num_words = free_words;
819     sb-&gt;next = list;
820     list = sb;
821   }
822 }
823 
824 void DefNewGeneration::reset_scratch() {
825   // If contributing scratch in to_space, mangle all of
826   // to_space if ZapUnusedHeapArea.  This is needed because
827   // top is not maintained while using to-space as scratch.
828   if (ZapUnusedHeapArea) {
829     to()-&gt;mangle_unused_area_complete();
830   }
831 }
832 
833 bool DefNewGeneration::collection_attempt_is_safe() {
834   if (!to()-&gt;is_empty()) {
835     log_trace(gc)(&quot;:: to is not empty ::&quot;);
836     return false;
837   }
838   if (_old_gen == NULL) {
839     GenCollectedHeap* gch = GenCollectedHeap::heap();
840     _old_gen = gch-&gt;old_gen();
841   }
842   return _old_gen-&gt;promotion_attempt_is_safe(used());
843 }
844 
845 void DefNewGeneration::gc_epilogue(bool full) {
846   DEBUG_ONLY(static bool seen_incremental_collection_failed = false;)
847 
848   assert(!GCLocker::is_active(), &quot;We should not be executing here&quot;);
849   // Check if the heap is approaching full after a collection has
850   // been done.  Generally the young generation is empty at
851   // a minimum at the end of a collection.  If it is not, then
852   // the heap is approaching full.
853   GenCollectedHeap* gch = GenCollectedHeap::heap();
854   if (full) {
855     DEBUG_ONLY(seen_incremental_collection_failed = false;)
856     if (!collection_attempt_is_safe() &amp;&amp; !_eden_space-&gt;is_empty()) {
857       log_trace(gc)(&quot;DefNewEpilogue: cause(%s), full, not safe, set_failed, set_alloc_from, clear_seen&quot;,
858                             GCCause::to_string(gch-&gt;gc_cause()));
859       gch-&gt;set_incremental_collection_failed(); // Slight lie: a full gc left us in that state
860       set_should_allocate_from_space(); // we seem to be running out of space
861     } else {
862       log_trace(gc)(&quot;DefNewEpilogue: cause(%s), full, safe, clear_failed, clear_alloc_from, clear_seen&quot;,
863                             GCCause::to_string(gch-&gt;gc_cause()));
864       gch-&gt;clear_incremental_collection_failed(); // We just did a full collection
865       clear_should_allocate_from_space(); // if set
866     }
867   } else {
868 #ifdef ASSERT
869     // It is possible that incremental_collection_failed() == true
870     // here, because an attempted scavenge did not succeed. The policy
871     // is normally expected to cause a full collection which should
872     // clear that condition, so we should not be here twice in a row
873     // with incremental_collection_failed() == true without having done
874     // a full collection in between.
875     if (!seen_incremental_collection_failed &amp;&amp;
876         gch-&gt;incremental_collection_failed()) {
877       log_trace(gc)(&quot;DefNewEpilogue: cause(%s), not full, not_seen_failed, failed, set_seen_failed&quot;,
878                             GCCause::to_string(gch-&gt;gc_cause()));
879       seen_incremental_collection_failed = true;
880     } else if (seen_incremental_collection_failed) {
881       log_trace(gc)(&quot;DefNewEpilogue: cause(%s), not full, seen_failed, will_clear_seen_failed&quot;,
882                             GCCause::to_string(gch-&gt;gc_cause()));
883       assert(gch-&gt;gc_cause() == GCCause::_scavenge_alot ||
884              !gch-&gt;incremental_collection_failed(),
885              &quot;Twice in a row&quot;);
886       seen_incremental_collection_failed = false;
887     }
888 #endif // ASSERT
889   }
890 
891   if (ZapUnusedHeapArea) {
892     eden()-&gt;check_mangled_unused_area_complete();
893     from()-&gt;check_mangled_unused_area_complete();
894     to()-&gt;check_mangled_unused_area_complete();
895   }
896 
897   if (!CleanChunkPoolAsync) {
898     Chunk::clean_chunk_pool();
899   }
900 
901   // update the generation and space performance counters
902   update_counters();
903   gch-&gt;counters()-&gt;update_counters();
904 }
905 
906 void DefNewGeneration::record_spaces_top() {
907   assert(ZapUnusedHeapArea, &quot;Not mangling unused space&quot;);
908   eden()-&gt;set_top_for_allocations();
909   to()-&gt;set_top_for_allocations();
910   from()-&gt;set_top_for_allocations();
911 }
912 
913 void DefNewGeneration::ref_processor_init() {
914   Generation::ref_processor_init();
915 }
916 
917 
918 void DefNewGeneration::update_counters() {
919   if (UsePerfData) {
920     _eden_counters-&gt;update_all();
921     _from_counters-&gt;update_all();
922     _to_counters-&gt;update_all();
923     _gen_counters-&gt;update_all();
924   }
925 }
926 
927 void DefNewGeneration::verify() {
928   eden()-&gt;verify();
929   from()-&gt;verify();
930     to()-&gt;verify();
931 }
932 
933 void DefNewGeneration::print_on(outputStream* st) const {
934   Generation::print_on(st);
935   st-&gt;print(&quot;  eden&quot;);
936   eden()-&gt;print_on(st);
937   st-&gt;print(&quot;  from&quot;);
938   from()-&gt;print_on(st);
939   st-&gt;print(&quot;  to  &quot;);
940   to()-&gt;print_on(st);
941 }
942 
943 
944 const char* DefNewGeneration::name() const {
945   return &quot;def new generation&quot;;
946 }
947 
948 // Moved from inline file as they are not called inline
949 CompactibleSpace* DefNewGeneration::first_compaction_space() const {
950   return eden();
951 }
952 
953 HeapWord* DefNewGeneration::allocate(size_t word_size, bool is_tlab) {
954   // This is the slow-path allocation for the DefNewGeneration.
955   // Most allocations are fast-path in compiled code.
956   // We try to allocate from the eden.  If that works, we are happy.
957   // Note that since DefNewGeneration supports lock-free allocation, we
958   // have to use it here, as well.
959   HeapWord* result = eden()-&gt;par_allocate(word_size);
960   if (result != NULL) {
961     if (_old_gen != NULL) {
962       _old_gen-&gt;sample_eden_chunk();
963     }
964   } else {
965     // If the eden is full and the last collection bailed out, we are running
966     // out of heap space, and we try to allocate the from-space, too.
967     // allocate_from_space can&#39;t be inlined because that would introduce a
968     // circular dependency at compile time.
969     result = allocate_from_space(word_size);
970   }
971   return result;
972 }
973 
974 HeapWord* DefNewGeneration::par_allocate(size_t word_size,
975                                          bool is_tlab) {
976   HeapWord* res = eden()-&gt;par_allocate(word_size);
977   if (_old_gen != NULL) {
978     _old_gen-&gt;sample_eden_chunk();
979   }
980   return res;
981 }
982 
983 size_t DefNewGeneration::tlab_capacity() const {
984   return eden()-&gt;capacity();
985 }
986 
987 size_t DefNewGeneration::tlab_used() const {
988   return eden()-&gt;used();
989 }
990 
991 size_t DefNewGeneration::unsafe_max_tlab_alloc() const {
992   return unsafe_max_alloc_nogc();
993 }
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="2" type="hidden" />
</body>
</html>