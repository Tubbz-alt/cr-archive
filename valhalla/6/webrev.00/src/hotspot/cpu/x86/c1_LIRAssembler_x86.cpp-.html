<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/cpu/x86/c1_LIRAssembler_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;c1/c1_Compilation.hpp&quot;
  29 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  30 #include &quot;c1/c1_MacroAssembler.hpp&quot;
  31 #include &quot;c1/c1_Runtime1.hpp&quot;
  32 #include &quot;c1/c1_ValueStack.hpp&quot;
  33 #include &quot;ci/ciArrayKlass.hpp&quot;
  34 #include &quot;ci/ciInstance.hpp&quot;
  35 #include &quot;ci/ciValueKlass.hpp&quot;
  36 #include &quot;gc/shared/collectedHeap.hpp&quot;
  37 #include &quot;nativeInst_x86.hpp&quot;
  38 #include &quot;oops/oop.inline.hpp&quot;
  39 #include &quot;oops/objArrayKlass.hpp&quot;
  40 #include &quot;runtime/frame.inline.hpp&quot;
  41 #include &quot;runtime/safepointMechanism.hpp&quot;
  42 #include &quot;runtime/sharedRuntime.hpp&quot;
  43 #include &quot;utilities/powerOfTwo.hpp&quot;
  44 #include &quot;vmreg_x86.inline.hpp&quot;
  45 
  46 
  47 // These masks are used to provide 128-bit aligned bitmasks to the XMM
  48 // instructions, to allow sign-masking or sign-bit flipping.  They allow
  49 // fast versions of NegF/NegD and AbsF/AbsD.
  50 
  51 // Note: &#39;double&#39; and &#39;long long&#39; have 32-bits alignment on x86.
  52 static jlong* double_quadword(jlong *adr, jlong lo, jlong hi) {
  53   // Use the expression (adr)&amp;(~0xF) to provide 128-bits aligned address
  54   // of 128-bits operands for SSE instructions.
  55   jlong *operand = (jlong*)(((intptr_t)adr) &amp; ((intptr_t)(~0xF)));
  56   // Store the value to a 128-bits operand.
  57   operand[0] = lo;
  58   operand[1] = hi;
  59   return operand;
  60 }
  61 
  62 // Buffer for 128-bits masks used by SSE instructions.
  63 static jlong fp_signmask_pool[(4+1)*2]; // 4*128bits(data) + 128bits(alignment)
  64 
  65 // Static initialization during VM startup.
  66 static jlong *float_signmask_pool  = double_quadword(&amp;fp_signmask_pool[1*2],         CONST64(0x7FFFFFFF7FFFFFFF),         CONST64(0x7FFFFFFF7FFFFFFF));
  67 static jlong *double_signmask_pool = double_quadword(&amp;fp_signmask_pool[2*2],         CONST64(0x7FFFFFFFFFFFFFFF),         CONST64(0x7FFFFFFFFFFFFFFF));
  68 static jlong *float_signflip_pool  = double_quadword(&amp;fp_signmask_pool[3*2], (jlong)UCONST64(0x8000000080000000), (jlong)UCONST64(0x8000000080000000));
  69 static jlong *double_signflip_pool = double_quadword(&amp;fp_signmask_pool[4*2], (jlong)UCONST64(0x8000000000000000), (jlong)UCONST64(0x8000000000000000));
  70 
  71 
  72 NEEDS_CLEANUP // remove this definitions ?
  73 const Register IC_Klass    = rax;   // where the IC klass is cached
  74 const Register SYNC_header = rax;   // synchronization header
  75 const Register SHIFT_count = rcx;   // where count for shift operations must be
  76 
  77 #define __ _masm-&gt;
  78 
  79 
  80 static void select_different_registers(Register preserve,
  81                                        Register extra,
  82                                        Register &amp;tmp1,
  83                                        Register &amp;tmp2) {
  84   if (tmp1 == preserve) {
  85     assert_different_registers(tmp1, tmp2, extra);
  86     tmp1 = extra;
  87   } else if (tmp2 == preserve) {
  88     assert_different_registers(tmp1, tmp2, extra);
  89     tmp2 = extra;
  90   }
  91   assert_different_registers(preserve, tmp1, tmp2);
  92 }
  93 
  94 
  95 
  96 static void select_different_registers(Register preserve,
  97                                        Register extra,
  98                                        Register &amp;tmp1,
  99                                        Register &amp;tmp2,
 100                                        Register &amp;tmp3) {
 101   if (tmp1 == preserve) {
 102     assert_different_registers(tmp1, tmp2, tmp3, extra);
 103     tmp1 = extra;
 104   } else if (tmp2 == preserve) {
 105     assert_different_registers(tmp1, tmp2, tmp3, extra);
 106     tmp2 = extra;
 107   } else if (tmp3 == preserve) {
 108     assert_different_registers(tmp1, tmp2, tmp3, extra);
 109     tmp3 = extra;
 110   }
 111   assert_different_registers(preserve, tmp1, tmp2, tmp3);
 112 }
 113 
 114 
 115 
 116 bool LIR_Assembler::is_small_constant(LIR_Opr opr) {
 117   if (opr-&gt;is_constant()) {
 118     LIR_Const* constant = opr-&gt;as_constant_ptr();
 119     switch (constant-&gt;type()) {
 120       case T_INT: {
 121         return true;
 122       }
 123 
 124       default:
 125         return false;
 126     }
 127   }
 128   return false;
 129 }
 130 
 131 
 132 LIR_Opr LIR_Assembler::receiverOpr() {
 133   return FrameMap::receiver_opr;
 134 }
 135 
 136 LIR_Opr LIR_Assembler::osrBufferPointer() {
 137   return FrameMap::as_pointer_opr(receiverOpr()-&gt;as_register());
 138 }
 139 
 140 //--------------fpu register translations-----------------------
 141 
 142 
 143 address LIR_Assembler::float_constant(float f) {
 144   address const_addr = __ float_constant(f);
 145   if (const_addr == NULL) {
 146     bailout(&quot;const section overflow&quot;);
 147     return __ code()-&gt;consts()-&gt;start();
 148   } else {
 149     return const_addr;
 150   }
 151 }
 152 
 153 
 154 address LIR_Assembler::double_constant(double d) {
 155   address const_addr = __ double_constant(d);
 156   if (const_addr == NULL) {
 157     bailout(&quot;const section overflow&quot;);
 158     return __ code()-&gt;consts()-&gt;start();
 159   } else {
 160     return const_addr;
 161   }
 162 }
 163 
 164 #ifndef _LP64
 165 void LIR_Assembler::fpop() {
 166   __ fpop();
 167 }
 168 
 169 void LIR_Assembler::fxch(int i) {
 170   __ fxch(i);
 171 }
 172 
 173 void LIR_Assembler::fld(int i) {
 174   __ fld_s(i);
 175 }
 176 
 177 void LIR_Assembler::ffree(int i) {
 178   __ ffree(i);
 179 }
 180 #endif // !_LP64
 181 
 182 void LIR_Assembler::breakpoint() {
 183   __ int3();
 184 }
 185 
 186 void LIR_Assembler::push(LIR_Opr opr) {
 187   if (opr-&gt;is_single_cpu()) {
 188     __ push_reg(opr-&gt;as_register());
 189   } else if (opr-&gt;is_double_cpu()) {
 190     NOT_LP64(__ push_reg(opr-&gt;as_register_hi()));
 191     __ push_reg(opr-&gt;as_register_lo());
 192   } else if (opr-&gt;is_stack()) {
 193     __ push_addr(frame_map()-&gt;address_for_slot(opr-&gt;single_stack_ix()));
 194   } else if (opr-&gt;is_constant()) {
 195     LIR_Const* const_opr = opr-&gt;as_constant_ptr();
 196     if (const_opr-&gt;type() == T_OBJECT || const_opr-&gt;type() == T_VALUETYPE) {
 197       __ push_oop(const_opr-&gt;as_jobject());
 198     } else if (const_opr-&gt;type() == T_INT) {
 199       __ push_jint(const_opr-&gt;as_jint());
 200     } else {
 201       ShouldNotReachHere();
 202     }
 203 
 204   } else {
 205     ShouldNotReachHere();
 206   }
 207 }
 208 
 209 void LIR_Assembler::pop(LIR_Opr opr) {
 210   if (opr-&gt;is_single_cpu()) {
 211     __ pop_reg(opr-&gt;as_register());
 212   } else {
 213     ShouldNotReachHere();
 214   }
 215 }
 216 
 217 bool LIR_Assembler::is_literal_address(LIR_Address* addr) {
 218   return addr-&gt;base()-&gt;is_illegal() &amp;&amp; addr-&gt;index()-&gt;is_illegal();
 219 }
 220 
 221 //-------------------------------------------
 222 
 223 Address LIR_Assembler::as_Address(LIR_Address* addr) {
 224   return as_Address(addr, rscratch1);
 225 }
 226 
 227 Address LIR_Assembler::as_Address(LIR_Address* addr, Register tmp) {
 228   if (addr-&gt;base()-&gt;is_illegal()) {
 229     assert(addr-&gt;index()-&gt;is_illegal(), &quot;must be illegal too&quot;);
 230     AddressLiteral laddr((address)addr-&gt;disp(), relocInfo::none);
 231     if (! __ reachable(laddr)) {
 232       __ movptr(tmp, laddr.addr());
 233       Address res(tmp, 0);
 234       return res;
 235     } else {
 236       return __ as_Address(laddr);
 237     }
 238   }
 239 
 240   Register base = addr-&gt;base()-&gt;as_pointer_register();
 241 
 242   if (addr-&gt;index()-&gt;is_illegal()) {
 243     return Address( base, addr-&gt;disp());
 244   } else if (addr-&gt;index()-&gt;is_cpu_register()) {
 245     Register index = addr-&gt;index()-&gt;as_pointer_register();
 246     return Address(base, index, (Address::ScaleFactor) addr-&gt;scale(), addr-&gt;disp());
 247   } else if (addr-&gt;index()-&gt;is_constant()) {
 248     intptr_t addr_offset = (addr-&gt;index()-&gt;as_constant_ptr()-&gt;as_jint() &lt;&lt; addr-&gt;scale()) + addr-&gt;disp();
 249     assert(Assembler::is_simm32(addr_offset), &quot;must be&quot;);
 250 
 251     return Address(base, addr_offset);
 252   } else {
 253     Unimplemented();
 254     return Address();
 255   }
 256 }
 257 
 258 
 259 Address LIR_Assembler::as_Address_hi(LIR_Address* addr) {
 260   Address base = as_Address(addr);
 261   return Address(base._base, base._index, base._scale, base._disp + BytesPerWord);
 262 }
 263 
 264 
 265 Address LIR_Assembler::as_Address_lo(LIR_Address* addr) {
 266   return as_Address(addr);
 267 }
 268 
 269 
 270 void LIR_Assembler::osr_entry() {
 271   offsets()-&gt;set_value(CodeOffsets::OSR_Entry, code_offset());
 272   BlockBegin* osr_entry = compilation()-&gt;hir()-&gt;osr_entry();
 273   ValueStack* entry_state = osr_entry-&gt;state();
 274   int number_of_locks = entry_state-&gt;locks_size();
 275 
 276   // we jump here if osr happens with the interpreter
 277   // state set up to continue at the beginning of the
 278   // loop that triggered osr - in particular, we have
 279   // the following registers setup:
 280   //
 281   // rcx: osr buffer
 282   //
 283 
 284   // build frame
 285   ciMethod* m = compilation()-&gt;method();
 286   __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(),
 287                  needs_stack_repair(), NULL);
 288 
 289   // OSR buffer is
 290   //
 291   // locals[nlocals-1..0]
 292   // monitors[0..number_of_locks]
 293   //
 294   // locals is a direct copy of the interpreter frame so in the osr buffer
 295   // so first slot in the local array is the last local from the interpreter
 296   // and last slot is local[0] (receiver) from the interpreter
 297   //
 298   // Similarly with locks. The first lock slot in the osr buffer is the nth lock
 299   // from the interpreter frame, the nth lock slot in the osr buffer is 0th lock
 300   // in the interpreter frame (the method lock if a sync method)
 301 
 302   // Initialize monitors in the compiled activation.
 303   //   rcx: pointer to osr buffer
 304   //
 305   // All other registers are dead at this point and the locals will be
 306   // copied into place by code emitted in the IR.
 307 
 308   Register OSR_buf = osrBufferPointer()-&gt;as_pointer_register();
 309   { assert(frame::interpreter_frame_monitor_size() == BasicObjectLock::size(), &quot;adjust code below&quot;);
 310     int monitor_offset = BytesPerWord * method()-&gt;max_locals() +
 311       (BasicObjectLock::size() * BytesPerWord) * (number_of_locks - 1);
 312     // SharedRuntime::OSR_migration_begin() packs BasicObjectLocks in
 313     // the OSR buffer using 2 word entries: first the lock and then
 314     // the oop.
 315     for (int i = 0; i &lt; number_of_locks; i++) {
 316       int slot_offset = monitor_offset - ((i * 2) * BytesPerWord);
 317 #ifdef ASSERT
 318       // verify the interpreter&#39;s monitor has a non-null object
 319       {
 320         Label L;
 321         __ cmpptr(Address(OSR_buf, slot_offset + 1*BytesPerWord), (int32_t)NULL_WORD);
 322         __ jcc(Assembler::notZero, L);
 323         __ stop(&quot;locked object is NULL&quot;);
 324         __ bind(L);
 325       }
 326 #endif
 327       __ movptr(rbx, Address(OSR_buf, slot_offset + 0));
 328       __ movptr(frame_map()-&gt;address_for_monitor_lock(i), rbx);
 329       __ movptr(rbx, Address(OSR_buf, slot_offset + 1*BytesPerWord));
 330       __ movptr(frame_map()-&gt;address_for_monitor_object(i), rbx);
 331     }
 332   }
 333 }
 334 
 335 
 336 // inline cache check; done before the frame is built.
 337 int LIR_Assembler::check_icache() {
 338   Register receiver = FrameMap::receiver_opr-&gt;as_register();
 339   Register ic_klass = IC_Klass;
 340   const int ic_cmp_size = LP64_ONLY(10) NOT_LP64(9);
 341   const bool do_post_padding = VerifyOops || UseCompressedClassPointers;
 342   if (!do_post_padding) {
 343     // insert some nops so that the verified entry point is aligned on CodeEntryAlignment
 344     __ align(CodeEntryAlignment, __ offset() + ic_cmp_size);
 345   }
 346   int offset = __ offset();
 347   __ inline_cache_check(receiver, IC_Klass);
 348   assert(__ offset() % CodeEntryAlignment == 0 || do_post_padding, &quot;alignment must be correct&quot;);
 349   if (do_post_padding) {
 350     // force alignment after the cache check.
 351     // It&#39;s been verified to be aligned if !VerifyOops
 352     __ align(CodeEntryAlignment);
 353   }
 354   return offset;
 355 }
 356 
 357 void LIR_Assembler::clinit_barrier(ciMethod* method) {
 358   assert(VM_Version::supports_fast_class_init_checks(), &quot;sanity&quot;);
 359   assert(!method-&gt;holder()-&gt;is_not_initialized(), &quot;initialization should have been started&quot;);
 360 
 361   Label L_skip_barrier;
 362   Register klass = rscratch1;
 363   Register thread = LP64_ONLY( r15_thread ) NOT_LP64( noreg );
 364   assert(thread != noreg, &quot;x86_32 not implemented&quot;);
 365 
 366   __ mov_metadata(klass, method-&gt;holder()-&gt;constant_encoding());
 367   __ clinit_barrier(klass, thread, &amp;L_skip_barrier /*L_fast_path*/);
 368 
 369   __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));
 370 
 371   __ bind(L_skip_barrier);
 372 }
 373 
 374 void LIR_Assembler::jobject2reg_with_patching(Register reg, CodeEmitInfo* info) {
 375   jobject o = NULL;
 376   PatchingStub* patch = new PatchingStub(_masm, patching_id(info));
 377   __ movoop(reg, o);
 378   patching_epilog(patch, lir_patch_normal, reg, info);
 379 }
 380 
 381 void LIR_Assembler::klass2reg_with_patching(Register reg, CodeEmitInfo* info) {
 382   Metadata* o = NULL;
 383   PatchingStub* patch = new PatchingStub(_masm, PatchingStub::load_klass_id);
 384   __ mov_metadata(reg, o);
 385   patching_epilog(patch, lir_patch_normal, reg, info);
 386 }
 387 
 388 // This specifies the rsp decrement needed to build the frame
 389 int LIR_Assembler::initial_frame_size_in_bytes() const {
 390   // if rounding, must let FrameMap know!
 391 
 392   // The frame_map records size in slots (32bit word)
 393 
 394   // subtract two words to account for return address and link
 395   return (frame_map()-&gt;framesize() - (2*VMRegImpl::slots_per_word))  * VMRegImpl::stack_slot_size;
 396 }
 397 
 398 
 399 int LIR_Assembler::emit_exception_handler() {
 400   // if the last instruction is a call (typically to do a throw which
 401   // is coming at the end after block reordering) the return address
 402   // must still point into the code area in order to avoid assertion
 403   // failures when searching for the corresponding bci =&gt; add a nop
 404   // (was bug 5/14/1999 - gri)
 405   __ nop();
 406 
 407   // generate code for exception handler
 408   address handler_base = __ start_a_stub(exception_handler_size());
 409   if (handler_base == NULL) {
 410     // not enough space left for the handler
 411     bailout(&quot;exception handler overflow&quot;);
 412     return -1;
 413   }
 414 
 415   int offset = code_offset();
 416 
 417   // the exception oop and pc are in rax, and rdx
 418   // no other registers need to be preserved, so invalidate them
 419   __ invalidate_registers(false, true, true, false, true, true);
 420 
 421   // check that there is really an exception
 422   __ verify_not_null_oop(rax);
 423 
 424   // search an exception handler (rax: exception oop, rdx: throwing pc)
 425   __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::handle_exception_from_callee_id)));
 426   __ should_not_reach_here();
 427   guarantee(code_offset() - offset &lt;= exception_handler_size(), &quot;overflow&quot;);
 428   __ end_a_stub();
 429 
 430   return offset;
 431 }
 432 
 433 
 434 // Emit the code to remove the frame from the stack in the exception
 435 // unwind path.
 436 int LIR_Assembler::emit_unwind_handler() {
 437 #ifndef PRODUCT
 438   if (CommentedAssembly) {
 439     _masm-&gt;block_comment(&quot;Unwind handler&quot;);
 440   }
 441 #endif
 442 
 443   int offset = code_offset();
 444 
 445   // Fetch the exception from TLS and clear out exception related thread state
 446   Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);
 447   NOT_LP64(__ get_thread(rsi));
 448   __ movptr(rax, Address(thread, JavaThread::exception_oop_offset()));
 449   __ movptr(Address(thread, JavaThread::exception_oop_offset()), (intptr_t)NULL_WORD);
 450   __ movptr(Address(thread, JavaThread::exception_pc_offset()), (intptr_t)NULL_WORD);
 451 
 452   __ bind(_unwind_handler_entry);
 453   __ verify_not_null_oop(rax);
 454   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 455     __ mov(rbx, rax);  // Preserve the exception (rbx is always callee-saved)
 456   }
 457 
 458   // Preform needed unlocking
 459   MonitorExitStub* stub = NULL;
 460   if (method()-&gt;is_synchronized()) {
 461     monitor_address(0, FrameMap::rax_opr);
 462     stub = new MonitorExitStub(FrameMap::rax_opr, true, 0);
 463     __ unlock_object(rdi, rsi, rax, *stub-&gt;entry());
 464     __ bind(*stub-&gt;continuation());
 465   }
 466 
 467   if (compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 468 #ifdef _LP64
 469     __ mov(rdi, r15_thread);
 470     __ mov_metadata(rsi, method()-&gt;constant_encoding());
 471 #else
 472     __ get_thread(rax);
 473     __ movptr(Address(rsp, 0), rax);
 474     __ mov_metadata(Address(rsp, sizeof(void*)), method()-&gt;constant_encoding());
 475 #endif
 476     __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit)));
 477   }
 478 
 479   if (method()-&gt;is_synchronized() || compilation()-&gt;env()-&gt;dtrace_method_probes()) {
 480     __ mov(rax, rbx);  // Restore the exception
 481   }
 482 
 483   // remove the activation and dispatch to the unwind handler
 484   __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());
 485   __ jump(RuntimeAddress(Runtime1::entry_for(Runtime1::unwind_exception_id)));
 486 
 487   // Emit the slow path assembly
 488   if (stub != NULL) {
 489     stub-&gt;emit_code(this);
 490   }
 491 
 492   return offset;
 493 }
 494 
 495 
 496 int LIR_Assembler::emit_deopt_handler() {
 497   // if the last instruction is a call (typically to do a throw which
 498   // is coming at the end after block reordering) the return address
 499   // must still point into the code area in order to avoid assertion
 500   // failures when searching for the corresponding bci =&gt; add a nop
 501   // (was bug 5/14/1999 - gri)
 502   __ nop();
 503 
 504   // generate code for exception handler
 505   address handler_base = __ start_a_stub(deopt_handler_size());
 506   if (handler_base == NULL) {
 507     // not enough space left for the handler
 508     bailout(&quot;deopt handler overflow&quot;);
 509     return -1;
 510   }
 511 
 512   int offset = code_offset();
 513   InternalAddress here(__ pc());
 514 
 515   __ pushptr(here.addr());
 516   __ jump(RuntimeAddress(SharedRuntime::deopt_blob()-&gt;unpack()));
 517   guarantee(code_offset() - offset &lt;= deopt_handler_size(), &quot;overflow&quot;);
 518   __ end_a_stub();
 519 
 520   return offset;
 521 }
 522 
 523 
 524 void LIR_Assembler::return_op(LIR_Opr result) {
 525   assert(result-&gt;is_illegal() || !result-&gt;is_single_cpu() || result-&gt;as_register() == rax, &quot;word returns are in rax,&quot;);
 526   if (!result-&gt;is_illegal() &amp;&amp; result-&gt;is_float_kind() &amp;&amp; !result-&gt;is_xmm_register()) {
 527     assert(result-&gt;fpu() == 0, &quot;result must already be on TOS&quot;);
 528   }
 529 
 530   ciMethod* method = compilation()-&gt;method();
 531   if (ValueTypeReturnedAsFields &amp;&amp; method-&gt;signature()-&gt;returns_never_null()) {
 532     ciType* return_type = method-&gt;return_type();
 533     if (return_type-&gt;is_valuetype()) {
 534       ciValueKlass* vk = return_type-&gt;as_value_klass();
 535       if (vk-&gt;can_be_returned_as_fields()) {
 536 #ifndef _LP64
 537         Unimplemented();
 538 #else
 539         address unpack_handler = vk-&gt;unpack_handler();
 540         assert(unpack_handler != NULL, &quot;must be&quot;);
 541         __ call(RuntimeAddress(unpack_handler));
 542         // At this point, rax points to the value object (for interpreter or C1 caller).
 543         // The fields of the object are copied into registers (for C2 caller).
 544 #endif
 545       }
 546     }
 547   }
 548 
 549   // Pop the stack before the safepoint code
 550   __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());
 551 
 552   if (StackReservedPages &gt; 0 &amp;&amp; compilation()-&gt;has_reserved_stack_access()) {
 553     __ reserved_stack_check();
 554   }
 555 
 556   bool result_is_oop = result-&gt;is_valid() ? result-&gt;is_oop() : false;
 557 
 558   // Note: we do not need to round double result; float result has the right precision
 559   // the poll sets the condition code, but no data registers
 560 
 561   if (SafepointMechanism::uses_thread_local_poll()) {
 562 #ifdef _LP64
 563     const Register poll_addr = rscratch1;
 564     __ movptr(poll_addr, Address(r15_thread, Thread::polling_page_offset()));
 565 #else
 566     const Register poll_addr = rbx;
 567     assert(FrameMap::is_caller_save_register(poll_addr), &quot;will overwrite&quot;);
 568     __ get_thread(poll_addr);
 569     __ movptr(poll_addr, Address(poll_addr, Thread::polling_page_offset()));
 570 #endif
 571     __ relocate(relocInfo::poll_return_type);
 572     __ testl(rax, Address(poll_addr, 0));
 573   } else {
 574     AddressLiteral polling_page(os::get_polling_page(), relocInfo::poll_return_type);
 575 
 576     if (Assembler::is_polling_page_far()) {
 577       __ lea(rscratch1, polling_page);
 578       __ relocate(relocInfo::poll_return_type);
 579       __ testl(rax, Address(rscratch1, 0));
 580     } else {
 581       __ testl(rax, polling_page);
 582     }
 583   }
 584   __ ret(0);
 585 }
 586 
 587 
 588 int LIR_Assembler::store_value_type_fields_to_buf(ciValueKlass* vk) {
 589   return (__ store_value_type_fields_to_buf(vk, false));
 590 }
 591 
 592 int LIR_Assembler::safepoint_poll(LIR_Opr tmp, CodeEmitInfo* info) {
 593   guarantee(info != NULL, &quot;Shouldn&#39;t be NULL&quot;);
 594   int offset = __ offset();
 595   if (SafepointMechanism::uses_thread_local_poll()) {
 596 #ifdef _LP64
 597     const Register poll_addr = rscratch1;
 598     __ movptr(poll_addr, Address(r15_thread, Thread::polling_page_offset()));
 599 #else
 600     assert(tmp-&gt;is_cpu_register(), &quot;needed&quot;);
 601     const Register poll_addr = tmp-&gt;as_register();
 602     __ get_thread(poll_addr);
 603     __ movptr(poll_addr, Address(poll_addr, in_bytes(Thread::polling_page_offset())));
 604 #endif
 605     add_debug_info_for_branch(info);
 606     __ relocate(relocInfo::poll_type);
 607     address pre_pc = __ pc();
 608     __ testl(rax, Address(poll_addr, 0));
 609     address post_pc = __ pc();
 610     guarantee(pointer_delta(post_pc, pre_pc, 1) == 2 LP64_ONLY(+1), &quot;must be exact length&quot;);
 611   } else {
 612     AddressLiteral polling_page(os::get_polling_page(), relocInfo::poll_type);
 613     if (Assembler::is_polling_page_far()) {
 614       __ lea(rscratch1, polling_page);
 615       offset = __ offset();
 616       add_debug_info_for_branch(info);
 617       __ relocate(relocInfo::poll_type);
 618       __ testl(rax, Address(rscratch1, 0));
 619     } else {
 620       add_debug_info_for_branch(info);
 621       __ testl(rax, polling_page);
 622     }
 623   }
 624   return offset;
 625 }
 626 
 627 
 628 void LIR_Assembler::move_regs(Register from_reg, Register to_reg) {
 629   if (from_reg != to_reg) __ mov(to_reg, from_reg);
 630 }
 631 
 632 void LIR_Assembler::swap_reg(Register a, Register b) {
 633   __ xchgptr(a, b);
 634 }
 635 
 636 
 637 void LIR_Assembler::const2reg(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
 638   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 639   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 640   LIR_Const* c = src-&gt;as_constant_ptr();
 641 
 642   switch (c-&gt;type()) {
 643     case T_INT: {
 644       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 645       __ movl(dest-&gt;as_register(), c-&gt;as_jint());
 646       break;
 647     }
 648 
 649     case T_ADDRESS: {
 650       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 651       __ movptr(dest-&gt;as_register(), c-&gt;as_jint());
 652       break;
 653     }
 654 
 655     case T_LONG: {
 656       assert(patch_code == lir_patch_none, &quot;no patching handled here&quot;);
 657 #ifdef _LP64
 658       __ movptr(dest-&gt;as_register_lo(), (intptr_t)c-&gt;as_jlong());
 659 #else
 660       __ movptr(dest-&gt;as_register_lo(), c-&gt;as_jint_lo());
 661       __ movptr(dest-&gt;as_register_hi(), c-&gt;as_jint_hi());
 662 #endif // _LP64
 663       break;
 664     }
 665 
 666     case T_VALUETYPE: // Fall through
 667     case T_OBJECT: {
 668       if (patch_code != lir_patch_none) {
 669         jobject2reg_with_patching(dest-&gt;as_register(), info);
 670       } else {
 671         __ movoop(dest-&gt;as_register(), c-&gt;as_jobject());
 672       }
 673       break;
 674     }
 675 
 676     case T_METADATA: {
 677       if (patch_code != lir_patch_none) {
 678         klass2reg_with_patching(dest-&gt;as_register(), info);
 679       } else {
 680         __ mov_metadata(dest-&gt;as_register(), c-&gt;as_metadata());
 681       }
 682       break;
 683     }
 684 
 685     case T_FLOAT: {
 686       if (dest-&gt;is_single_xmm()) {
 687         if (LP64_ONLY(UseAVX &lt;= 2 &amp;&amp;) c-&gt;is_zero_float()) {
 688           __ xorps(dest-&gt;as_xmm_float_reg(), dest-&gt;as_xmm_float_reg());
 689         } else {
 690           __ movflt(dest-&gt;as_xmm_float_reg(),
 691                    InternalAddress(float_constant(c-&gt;as_jfloat())));
 692         }
 693       } else {
 694 #ifndef _LP64
 695         assert(dest-&gt;is_single_fpu(), &quot;must be&quot;);
 696         assert(dest-&gt;fpu_regnr() == 0, &quot;dest must be TOS&quot;);
 697         if (c-&gt;is_zero_float()) {
 698           __ fldz();
 699         } else if (c-&gt;is_one_float()) {
 700           __ fld1();
 701         } else {
 702           __ fld_s (InternalAddress(float_constant(c-&gt;as_jfloat())));
 703         }
 704 #else
 705         ShouldNotReachHere();
 706 #endif // !_LP64
 707       }
 708       break;
 709     }
 710 
 711     case T_DOUBLE: {
 712       if (dest-&gt;is_double_xmm()) {
 713         if (LP64_ONLY(UseAVX &lt;= 2 &amp;&amp;) c-&gt;is_zero_double()) {
 714           __ xorpd(dest-&gt;as_xmm_double_reg(), dest-&gt;as_xmm_double_reg());
 715         } else {
 716           __ movdbl(dest-&gt;as_xmm_double_reg(),
 717                     InternalAddress(double_constant(c-&gt;as_jdouble())));
 718         }
 719       } else {
 720 #ifndef _LP64
 721         assert(dest-&gt;is_double_fpu(), &quot;must be&quot;);
 722         assert(dest-&gt;fpu_regnrLo() == 0, &quot;dest must be TOS&quot;);
 723         if (c-&gt;is_zero_double()) {
 724           __ fldz();
 725         } else if (c-&gt;is_one_double()) {
 726           __ fld1();
 727         } else {
 728           __ fld_d (InternalAddress(double_constant(c-&gt;as_jdouble())));
 729         }
 730 #else
 731         ShouldNotReachHere();
 732 #endif // !_LP64
 733       }
 734       break;
 735     }
 736 
 737     default:
 738       ShouldNotReachHere();
 739   }
 740 }
 741 
 742 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
 743   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 744   assert(dest-&gt;is_stack(), &quot;should not call otherwise&quot;);
 745   LIR_Const* c = src-&gt;as_constant_ptr();
 746 
 747   switch (c-&gt;type()) {
 748     case T_INT:  // fall through
 749     case T_FLOAT:
 750       __ movl(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jint_bits());
 751       break;
 752 
 753     case T_ADDRESS:
 754       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jint_bits());
 755       break;
 756 
 757     case T_VALUETYPE: // Fall through
 758     case T_OBJECT:
 759       __ movoop(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), c-&gt;as_jobject());
 760       break;
 761 
 762     case T_LONG:  // fall through
 763     case T_DOUBLE:
 764 #ifdef _LP64
 765       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 766                                             lo_word_offset_in_bytes), (intptr_t)c-&gt;as_jlong_bits());
 767 #else
 768       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 769                                               lo_word_offset_in_bytes), c-&gt;as_jint_lo_bits());
 770       __ movptr(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(),
 771                                               hi_word_offset_in_bytes), c-&gt;as_jint_hi_bits());
 772 #endif // _LP64
 773       break;
 774 
 775     default:
 776       ShouldNotReachHere();
 777   }
 778 }
 779 
 780 void LIR_Assembler::const2mem(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info, bool wide) {
 781   assert(src-&gt;is_constant(), &quot;should not call otherwise&quot;);
 782   assert(dest-&gt;is_address(), &quot;should not call otherwise&quot;);
 783   LIR_Const* c = src-&gt;as_constant_ptr();
 784   LIR_Address* addr = dest-&gt;as_address_ptr();
 785 
 786   int null_check_here = code_offset();
 787   switch (type) {
 788     case T_INT:    // fall through
 789     case T_FLOAT:
 790       __ movl(as_Address(addr), c-&gt;as_jint_bits());
 791       break;
 792 
 793     case T_ADDRESS:
 794       __ movptr(as_Address(addr), c-&gt;as_jint_bits());
 795       break;
 796 
 797     case T_VALUETYPE: // fall through
 798     case T_OBJECT:  // fall through
 799     case T_ARRAY:
 800       if (c-&gt;as_jobject() == NULL) {
 801         if (UseCompressedOops &amp;&amp; !wide) {
 802           __ movl(as_Address(addr), (int32_t)NULL_WORD);
 803         } else {
 804 #ifdef _LP64
 805           __ xorptr(rscratch1, rscratch1);
 806           null_check_here = code_offset();
 807           __ movptr(as_Address(addr), rscratch1);
 808 #else
 809           __ movptr(as_Address(addr), NULL_WORD);
 810 #endif
 811         }
 812       } else {
 813         if (is_literal_address(addr)) {
 814           ShouldNotReachHere();
 815           __ movoop(as_Address(addr, noreg), c-&gt;as_jobject());
 816         } else {
 817 #ifdef _LP64
 818           __ movoop(rscratch1, c-&gt;as_jobject());
 819           if (UseCompressedOops &amp;&amp; !wide) {
 820             __ encode_heap_oop(rscratch1);
 821             null_check_here = code_offset();
 822             __ movl(as_Address_lo(addr), rscratch1);
 823           } else {
 824             null_check_here = code_offset();
 825             __ movptr(as_Address_lo(addr), rscratch1);
 826           }
 827 #else
 828           __ movoop(as_Address(addr), c-&gt;as_jobject());
 829 #endif
 830         }
 831       }
 832       break;
 833 
 834     case T_LONG:    // fall through
 835     case T_DOUBLE:
 836 #ifdef _LP64
 837       if (is_literal_address(addr)) {
 838         ShouldNotReachHere();
 839         __ movptr(as_Address(addr, r15_thread), (intptr_t)c-&gt;as_jlong_bits());
 840       } else {
 841         __ movptr(r10, (intptr_t)c-&gt;as_jlong_bits());
 842         null_check_here = code_offset();
 843         __ movptr(as_Address_lo(addr), r10);
 844       }
 845 #else
 846       // Always reachable in 32bit so this doesn&#39;t produce useless move literal
 847       __ movptr(as_Address_hi(addr), c-&gt;as_jint_hi_bits());
 848       __ movptr(as_Address_lo(addr), c-&gt;as_jint_lo_bits());
 849 #endif // _LP64
 850       break;
 851 
 852     case T_BOOLEAN: // fall through
 853     case T_BYTE:
 854       __ movb(as_Address(addr), c-&gt;as_jint() &amp; 0xFF);
 855       break;
 856 
 857     case T_CHAR:    // fall through
 858     case T_SHORT:
 859       __ movw(as_Address(addr), c-&gt;as_jint() &amp; 0xFFFF);
 860       break;
 861 
 862     default:
 863       ShouldNotReachHere();
 864   };
 865 
 866   if (info != NULL) {
 867     add_debug_info_for_null_check(null_check_here, info);
 868   }
 869 }
 870 
 871 
 872 void LIR_Assembler::reg2reg(LIR_Opr src, LIR_Opr dest) {
 873   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 874   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
 875 
 876   // move between cpu-registers
 877   if (dest-&gt;is_single_cpu()) {
 878 #ifdef _LP64
 879     if (src-&gt;type() == T_LONG) {
 880       // Can do LONG -&gt; OBJECT
 881       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
 882       return;
 883     }
 884 #endif
 885     assert(src-&gt;is_single_cpu(), &quot;must match&quot;);
 886     if (src-&gt;type() == T_OBJECT || src-&gt;type() == T_VALUETYPE) {
 887       __ verify_oop(src-&gt;as_register());
 888     }
 889     move_regs(src-&gt;as_register(), dest-&gt;as_register());
 890 
 891   } else if (dest-&gt;is_double_cpu()) {
 892 #ifdef _LP64
 893     if (is_reference_type(src-&gt;type())) {
 894       // Surprising to me but we can see move of a long to t_object
 895       __ verify_oop(src-&gt;as_register());
 896       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
 897       return;
 898     }
 899 #endif
 900     assert(src-&gt;is_double_cpu(), &quot;must match&quot;);
 901     Register f_lo = src-&gt;as_register_lo();
 902     Register f_hi = src-&gt;as_register_hi();
 903     Register t_lo = dest-&gt;as_register_lo();
 904     Register t_hi = dest-&gt;as_register_hi();
 905 #ifdef _LP64
 906     assert(f_hi == f_lo, &quot;must be same&quot;);
 907     assert(t_hi == t_lo, &quot;must be same&quot;);
 908     move_regs(f_lo, t_lo);
 909 #else
 910     assert(f_lo != f_hi &amp;&amp; t_lo != t_hi, &quot;invalid register allocation&quot;);
 911 
 912 
 913     if (f_lo == t_hi &amp;&amp; f_hi == t_lo) {
 914       swap_reg(f_lo, f_hi);
 915     } else if (f_hi == t_lo) {
 916       assert(f_lo != t_hi, &quot;overwriting register&quot;);
 917       move_regs(f_hi, t_hi);
 918       move_regs(f_lo, t_lo);
 919     } else {
 920       assert(f_hi != t_lo, &quot;overwriting register&quot;);
 921       move_regs(f_lo, t_lo);
 922       move_regs(f_hi, t_hi);
 923     }
 924 #endif // LP64
 925 
 926 #ifndef _LP64
 927     // special moves from fpu-register to xmm-register
 928     // necessary for method results
 929   } else if (src-&gt;is_single_xmm() &amp;&amp; !dest-&gt;is_single_xmm()) {
 930     __ movflt(Address(rsp, 0), src-&gt;as_xmm_float_reg());
 931     __ fld_s(Address(rsp, 0));
 932   } else if (src-&gt;is_double_xmm() &amp;&amp; !dest-&gt;is_double_xmm()) {
 933     __ movdbl(Address(rsp, 0), src-&gt;as_xmm_double_reg());
 934     __ fld_d(Address(rsp, 0));
 935   } else if (dest-&gt;is_single_xmm() &amp;&amp; !src-&gt;is_single_xmm()) {
 936     __ fstp_s(Address(rsp, 0));
 937     __ movflt(dest-&gt;as_xmm_float_reg(), Address(rsp, 0));
 938   } else if (dest-&gt;is_double_xmm() &amp;&amp; !src-&gt;is_double_xmm()) {
 939     __ fstp_d(Address(rsp, 0));
 940     __ movdbl(dest-&gt;as_xmm_double_reg(), Address(rsp, 0));
 941 #endif // !_LP64
 942 
 943     // move between xmm-registers
 944   } else if (dest-&gt;is_single_xmm()) {
 945     assert(src-&gt;is_single_xmm(), &quot;must match&quot;);
 946     __ movflt(dest-&gt;as_xmm_float_reg(), src-&gt;as_xmm_float_reg());
 947   } else if (dest-&gt;is_double_xmm()) {
 948     assert(src-&gt;is_double_xmm(), &quot;must match&quot;);
 949     __ movdbl(dest-&gt;as_xmm_double_reg(), src-&gt;as_xmm_double_reg());
 950 
 951 #ifndef _LP64
 952     // move between fpu-registers (no instruction necessary because of fpu-stack)
 953   } else if (dest-&gt;is_single_fpu() || dest-&gt;is_double_fpu()) {
 954     assert(src-&gt;is_single_fpu() || src-&gt;is_double_fpu(), &quot;must match&quot;);
 955     assert(src-&gt;fpu() == dest-&gt;fpu(), &quot;currently should be nothing to do&quot;);
 956 #endif // !_LP64
 957 
 958   } else {
 959     ShouldNotReachHere();
 960   }
 961 }
 962 
 963 void LIR_Assembler::reg2stack(LIR_Opr src, LIR_Opr dest, BasicType type, bool pop_fpu_stack) {
 964   assert(src-&gt;is_register(), &quot;should not call otherwise&quot;);
 965   assert(dest-&gt;is_stack(), &quot;should not call otherwise&quot;);
 966 
 967   if (src-&gt;is_single_cpu()) {
 968     Address dst = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 969     if (is_reference_type(type)) {
 970       __ verify_oop(src-&gt;as_register());
 971       __ movptr (dst, src-&gt;as_register());
 972     } else if (type == T_METADATA || type == T_ADDRESS) {
 973       __ movptr (dst, src-&gt;as_register());
 974     } else {
 975       __ movl (dst, src-&gt;as_register());
 976     }
 977 
 978   } else if (src-&gt;is_double_cpu()) {
 979     Address dstLO = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), lo_word_offset_in_bytes);
 980     Address dstHI = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), hi_word_offset_in_bytes);
 981     __ movptr (dstLO, src-&gt;as_register_lo());
 982     NOT_LP64(__ movptr (dstHI, src-&gt;as_register_hi()));
 983 
 984   } else if (src-&gt;is_single_xmm()) {
 985     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 986     __ movflt(dst_addr, src-&gt;as_xmm_float_reg());
 987 
 988   } else if (src-&gt;is_double_xmm()) {
 989     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
 990     __ movdbl(dst_addr, src-&gt;as_xmm_double_reg());
 991 
 992 #ifndef _LP64
 993   } else if (src-&gt;is_single_fpu()) {
 994     assert(src-&gt;fpu_regnr() == 0, &quot;argument must be on TOS&quot;);
 995     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix());
 996     if (pop_fpu_stack)     __ fstp_s (dst_addr);
 997     else                   __ fst_s  (dst_addr);
 998 
 999   } else if (src-&gt;is_double_fpu()) {
1000     assert(src-&gt;fpu_regnrLo() == 0, &quot;argument must be on TOS&quot;);
1001     Address dst_addr = frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix());
1002     if (pop_fpu_stack)     __ fstp_d (dst_addr);
1003     else                   __ fst_d  (dst_addr);
1004 #endif // !_LP64
1005 
1006   } else {
1007     ShouldNotReachHere();
1008   }
1009 }
1010 
1011 
1012 void LIR_Assembler::reg2mem(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool wide, bool /* unaligned */) {
1013   LIR_Address* to_addr = dest-&gt;as_address_ptr();
1014   PatchingStub* patch = NULL;
1015   Register compressed_src = rscratch1;
1016 
1017   if (is_reference_type(type)) {
1018     __ verify_oop(src-&gt;as_register());
1019 #ifdef _LP64
1020     if (UseCompressedOops &amp;&amp; !wide) {
1021       __ movptr(compressed_src, src-&gt;as_register());
1022       __ encode_heap_oop(compressed_src);
1023       if (patch_code != lir_patch_none) {
1024         info-&gt;oop_map()-&gt;set_narrowoop(compressed_src-&gt;as_VMReg());
1025       }
1026     }
1027 #endif
1028   }
1029 
1030   if (patch_code != lir_patch_none) {
1031     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
1032     Address toa = as_Address(to_addr);
1033     assert(toa.disp() != 0, &quot;must have&quot;);
1034   }
1035 
1036   int null_check_here = code_offset();
1037   switch (type) {
1038     case T_FLOAT: {
1039 #ifdef _LP64
1040       assert(src-&gt;is_single_xmm(), &quot;not a float&quot;);
1041       __ movflt(as_Address(to_addr), src-&gt;as_xmm_float_reg());
1042 #else
1043       if (src-&gt;is_single_xmm()) {
1044         __ movflt(as_Address(to_addr), src-&gt;as_xmm_float_reg());
1045       } else {
1046         assert(src-&gt;is_single_fpu(), &quot;must be&quot;);
1047         assert(src-&gt;fpu_regnr() == 0, &quot;argument must be on TOS&quot;);
1048         if (pop_fpu_stack)      __ fstp_s(as_Address(to_addr));
1049         else                    __ fst_s (as_Address(to_addr));
1050       }
1051 #endif // _LP64
1052       break;
1053     }
1054 
1055     case T_DOUBLE: {
1056 #ifdef _LP64
1057       assert(src-&gt;is_double_xmm(), &quot;not a double&quot;);
1058       __ movdbl(as_Address(to_addr), src-&gt;as_xmm_double_reg());
1059 #else
1060       if (src-&gt;is_double_xmm()) {
1061         __ movdbl(as_Address(to_addr), src-&gt;as_xmm_double_reg());
1062       } else {
1063         assert(src-&gt;is_double_fpu(), &quot;must be&quot;);
1064         assert(src-&gt;fpu_regnrLo() == 0, &quot;argument must be on TOS&quot;);
1065         if (pop_fpu_stack)      __ fstp_d(as_Address(to_addr));
1066         else                    __ fst_d (as_Address(to_addr));
1067       }
1068 #endif // _LP64
1069       break;
1070     }
1071 
1072     case T_VALUETYPE: // fall through
1073     case T_ARRAY:   // fall through
1074     case T_OBJECT:  // fall through
1075       if (UseCompressedOops &amp;&amp; !wide) {
1076         __ movl(as_Address(to_addr), compressed_src);
1077       } else {
1078         __ movptr(as_Address(to_addr), src-&gt;as_register());
1079       }
1080       break;
1081     case T_METADATA:
1082       // We get here to store a method pointer to the stack to pass to
1083       // a dtrace runtime call. This can&#39;t work on 64 bit with
1084       // compressed klass ptrs: T_METADATA can be a compressed klass
1085       // ptr or a 64 bit method pointer.
1086       LP64_ONLY(ShouldNotReachHere());
1087       __ movptr(as_Address(to_addr), src-&gt;as_register());
1088       break;
1089     case T_ADDRESS:
1090       __ movptr(as_Address(to_addr), src-&gt;as_register());
1091       break;
1092     case T_INT:
1093       __ movl(as_Address(to_addr), src-&gt;as_register());
1094       break;
1095 
1096     case T_LONG: {
1097       Register from_lo = src-&gt;as_register_lo();
1098       Register from_hi = src-&gt;as_register_hi();
1099 #ifdef _LP64
1100       __ movptr(as_Address_lo(to_addr), from_lo);
1101 #else
1102       Register base = to_addr-&gt;base()-&gt;as_register();
1103       Register index = noreg;
1104       if (to_addr-&gt;index()-&gt;is_register()) {
1105         index = to_addr-&gt;index()-&gt;as_register();
1106       }
1107       if (base == from_lo || index == from_lo) {
1108         assert(base != from_hi, &quot;can&#39;t be&quot;);
1109         assert(index == noreg || (index != base &amp;&amp; index != from_hi), &quot;can&#39;t handle this&quot;);
1110         __ movl(as_Address_hi(to_addr), from_hi);
1111         if (patch != NULL) {
1112           patching_epilog(patch, lir_patch_high, base, info);
1113           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
1114           patch_code = lir_patch_low;
1115         }
1116         __ movl(as_Address_lo(to_addr), from_lo);
1117       } else {
1118         assert(index == noreg || (index != base &amp;&amp; index != from_lo), &quot;can&#39;t handle this&quot;);
1119         __ movl(as_Address_lo(to_addr), from_lo);
1120         if (patch != NULL) {
1121           patching_epilog(patch, lir_patch_low, base, info);
1122           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
1123           patch_code = lir_patch_high;
1124         }
1125         __ movl(as_Address_hi(to_addr), from_hi);
1126       }
1127 #endif // _LP64
1128       break;
1129     }
1130 
1131     case T_BYTE:    // fall through
1132     case T_BOOLEAN: {
1133       Register src_reg = src-&gt;as_register();
1134       Address dst_addr = as_Address(to_addr);
1135       assert(VM_Version::is_P6() || src_reg-&gt;has_byte_register(), &quot;must use byte registers if not P6&quot;);
1136       __ movb(dst_addr, src_reg);
1137       break;
1138     }
1139 
1140     case T_CHAR:    // fall through
1141     case T_SHORT:
1142       __ movw(as_Address(to_addr), src-&gt;as_register());
1143       break;
1144 
1145     default:
1146       ShouldNotReachHere();
1147   }
1148   if (info != NULL) {
1149     add_debug_info_for_null_check(null_check_here, info);
1150   }
1151 
1152   if (patch_code != lir_patch_none) {
1153     patching_epilog(patch, patch_code, to_addr-&gt;base()-&gt;as_register(), info);
1154   }
1155 }
1156 
1157 
1158 void LIR_Assembler::stack2reg(LIR_Opr src, LIR_Opr dest, BasicType type) {
1159   assert(src-&gt;is_stack(), &quot;should not call otherwise&quot;);
1160   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
1161 
1162   if (dest-&gt;is_single_cpu()) {
1163     if (is_reference_type(type)) {
1164       __ movptr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
1165       __ verify_oop(dest-&gt;as_register());
1166     } else if (type == T_METADATA || type == T_ADDRESS) {
1167       __ movptr(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
1168     } else {
1169       __ movl(dest-&gt;as_register(), frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix()));
1170     }
1171 
1172   } else if (dest-&gt;is_double_cpu()) {
1173     Address src_addr_LO = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), lo_word_offset_in_bytes);
1174     Address src_addr_HI = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix(), hi_word_offset_in_bytes);
1175     __ movptr(dest-&gt;as_register_lo(), src_addr_LO);
1176     NOT_LP64(__ movptr(dest-&gt;as_register_hi(), src_addr_HI));
1177 
1178   } else if (dest-&gt;is_single_xmm()) {
1179     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
1180     __ movflt(dest-&gt;as_xmm_float_reg(), src_addr);
1181 
1182   } else if (dest-&gt;is_double_xmm()) {
1183     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
1184     __ movdbl(dest-&gt;as_xmm_double_reg(), src_addr);
1185 
1186 #ifndef _LP64
1187   } else if (dest-&gt;is_single_fpu()) {
1188     assert(dest-&gt;fpu_regnr() == 0, &quot;dest must be TOS&quot;);
1189     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;single_stack_ix());
1190     __ fld_s(src_addr);
1191 
1192   } else if (dest-&gt;is_double_fpu()) {
1193     assert(dest-&gt;fpu_regnrLo() == 0, &quot;dest must be TOS&quot;);
1194     Address src_addr = frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix());
1195     __ fld_d(src_addr);
1196 #endif // _LP64
1197 
1198   } else {
1199     ShouldNotReachHere();
1200   }
1201 }
1202 
1203 
1204 void LIR_Assembler::stack2stack(LIR_Opr src, LIR_Opr dest, BasicType type) {
1205   if (src-&gt;is_single_stack()) {
1206     if (is_reference_type(type)) {
1207       __ pushptr(frame_map()-&gt;address_for_slot(src -&gt;single_stack_ix()));
1208       __ popptr (frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
1209     } else {
1210 #ifndef _LP64
1211       __ pushl(frame_map()-&gt;address_for_slot(src -&gt;single_stack_ix()));
1212       __ popl (frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()));
1213 #else
1214       //no pushl on 64bits
1215       __ movl(rscratch1, frame_map()-&gt;address_for_slot(src -&gt;single_stack_ix()));
1216       __ movl(frame_map()-&gt;address_for_slot(dest-&gt;single_stack_ix()), rscratch1);
1217 #endif
1218     }
1219 
1220   } else if (src-&gt;is_double_stack()) {
1221 #ifdef _LP64
1222     __ pushptr(frame_map()-&gt;address_for_slot(src -&gt;double_stack_ix()));
1223     __ popptr (frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix()));
1224 #else
1225     __ pushl(frame_map()-&gt;address_for_slot(src -&gt;double_stack_ix(), 0));
1226     // push and pop the part at src + wordSize, adding wordSize for the previous push
1227     __ pushl(frame_map()-&gt;address_for_slot(src -&gt;double_stack_ix(), 2 * wordSize));
1228     __ popl (frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), 2 * wordSize));
1229     __ popl (frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix(), 0));
1230 #endif // _LP64
1231 
1232   } else {
1233     ShouldNotReachHere();
1234   }
1235 }
1236 
1237 
1238 void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide, bool /* unaligned */) {
1239   assert(src-&gt;is_address(), &quot;should not call otherwise&quot;);
1240   assert(dest-&gt;is_register(), &quot;should not call otherwise&quot;);
1241 
1242   LIR_Address* addr = src-&gt;as_address_ptr();
1243   Address from_addr = as_Address(addr);
1244 
1245   if (addr-&gt;base()-&gt;type() == T_OBJECT || addr-&gt;base()-&gt;type() == T_VALUETYPE) {
1246     __ verify_oop(addr-&gt;base()-&gt;as_pointer_register());
1247   }
1248 
1249   switch (type) {
1250     case T_BOOLEAN: // fall through
1251     case T_BYTE:    // fall through
1252     case T_CHAR:    // fall through
1253     case T_SHORT:
1254       if (!VM_Version::is_P6() &amp;&amp; !from_addr.uses(dest-&gt;as_register())) {
1255         // on pre P6 processors we may get partial register stalls
1256         // so blow away the value of to_rinfo before loading a
1257         // partial word into it.  Do it here so that it precedes
1258         // the potential patch point below.
1259         __ xorptr(dest-&gt;as_register(), dest-&gt;as_register());
1260       }
1261       break;
1262    default:
1263      break;
1264   }
1265 
1266   PatchingStub* patch = NULL;
1267   if (patch_code != lir_patch_none) {
1268     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
1269     assert(from_addr.disp() != 0, &quot;must have&quot;);
1270   }
1271   if (info != NULL) {
1272     add_debug_info_for_null_check_here(info);
1273   }
1274 
1275   switch (type) {
1276     case T_FLOAT: {
1277       if (dest-&gt;is_single_xmm()) {
1278         __ movflt(dest-&gt;as_xmm_float_reg(), from_addr);
1279       } else {
1280 #ifndef _LP64
1281         assert(dest-&gt;is_single_fpu(), &quot;must be&quot;);
1282         assert(dest-&gt;fpu_regnr() == 0, &quot;dest must be TOS&quot;);
1283         __ fld_s(from_addr);
1284 #else
1285         ShouldNotReachHere();
1286 #endif // !LP64
1287       }
1288       break;
1289     }
1290 
1291     case T_DOUBLE: {
1292       if (dest-&gt;is_double_xmm()) {
1293         __ movdbl(dest-&gt;as_xmm_double_reg(), from_addr);
1294       } else {
1295 #ifndef _LP64
1296         assert(dest-&gt;is_double_fpu(), &quot;must be&quot;);
1297         assert(dest-&gt;fpu_regnrLo() == 0, &quot;dest must be TOS&quot;);
1298         __ fld_d(from_addr);
1299 #else
1300         ShouldNotReachHere();
1301 #endif // !LP64
1302       }
1303       break;
1304     }
1305 
1306     case T_VALUETYPE: // fall through
1307     case T_OBJECT:  // fall through
1308     case T_ARRAY:   // fall through
1309       if (UseCompressedOops &amp;&amp; !wide) {
1310         __ movl(dest-&gt;as_register(), from_addr);
1311       } else {
1312         __ movptr(dest-&gt;as_register(), from_addr);
1313       }
1314       break;
1315 
1316     case T_ADDRESS:
1317       if (UseCompressedClassPointers &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1318         __ movl(dest-&gt;as_register(), from_addr);
1319       } else {
1320         __ movptr(dest-&gt;as_register(), from_addr);
1321       }
1322       break;
1323     case T_INT:
1324       __ movl(dest-&gt;as_register(), from_addr);
1325       break;
1326 
1327     case T_LONG: {
1328       Register to_lo = dest-&gt;as_register_lo();
1329       Register to_hi = dest-&gt;as_register_hi();
1330 #ifdef _LP64
1331       __ movptr(to_lo, as_Address_lo(addr));
1332 #else
1333       Register base = addr-&gt;base()-&gt;as_register();
1334       Register index = noreg;
1335       if (addr-&gt;index()-&gt;is_register()) {
1336         index = addr-&gt;index()-&gt;as_register();
1337       }
1338       if ((base == to_lo &amp;&amp; index == to_hi) ||
1339           (base == to_hi &amp;&amp; index == to_lo)) {
1340         // addresses with 2 registers are only formed as a result of
1341         // array access so this code will never have to deal with
1342         // patches or null checks.
1343         assert(info == NULL &amp;&amp; patch == NULL, &quot;must be&quot;);
1344         __ lea(to_hi, as_Address(addr));
1345         __ movl(to_lo, Address(to_hi, 0));
1346         __ movl(to_hi, Address(to_hi, BytesPerWord));
1347       } else if (base == to_lo || index == to_lo) {
1348         assert(base != to_hi, &quot;can&#39;t be&quot;);
1349         assert(index == noreg || (index != base &amp;&amp; index != to_hi), &quot;can&#39;t handle this&quot;);
1350         __ movl(to_hi, as_Address_hi(addr));
1351         if (patch != NULL) {
1352           patching_epilog(patch, lir_patch_high, base, info);
1353           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
1354           patch_code = lir_patch_low;
1355         }
1356         __ movl(to_lo, as_Address_lo(addr));
1357       } else {
1358         assert(index == noreg || (index != base &amp;&amp; index != to_lo), &quot;can&#39;t handle this&quot;);
1359         __ movl(to_lo, as_Address_lo(addr));
1360         if (patch != NULL) {
1361           patching_epilog(patch, lir_patch_low, base, info);
1362           patch = new PatchingStub(_masm, PatchingStub::access_field_id);
1363           patch_code = lir_patch_high;
1364         }
1365         __ movl(to_hi, as_Address_hi(addr));
1366       }
1367 #endif // _LP64
1368       break;
1369     }
1370 
1371     case T_BOOLEAN: // fall through
1372     case T_BYTE: {
1373       Register dest_reg = dest-&gt;as_register();
1374       assert(VM_Version::is_P6() || dest_reg-&gt;has_byte_register(), &quot;must use byte registers if not P6&quot;);
1375       if (VM_Version::is_P6() || from_addr.uses(dest_reg)) {
1376         __ movsbl(dest_reg, from_addr);
1377       } else {
1378         __ movb(dest_reg, from_addr);
1379         __ shll(dest_reg, 24);
1380         __ sarl(dest_reg, 24);
1381       }
1382       break;
1383     }
1384 
1385     case T_CHAR: {
1386       Register dest_reg = dest-&gt;as_register();
1387       assert(VM_Version::is_P6() || dest_reg-&gt;has_byte_register(), &quot;must use byte registers if not P6&quot;);
1388       if (VM_Version::is_P6() || from_addr.uses(dest_reg)) {
1389         __ movzwl(dest_reg, from_addr);
1390       } else {
1391         __ movw(dest_reg, from_addr);
1392       }
1393       break;
1394     }
1395 
1396     case T_SHORT: {
1397       Register dest_reg = dest-&gt;as_register();
1398       if (VM_Version::is_P6() || from_addr.uses(dest_reg)) {
1399         __ movswl(dest_reg, from_addr);
1400       } else {
1401         __ movw(dest_reg, from_addr);
1402         __ shll(dest_reg, 16);
1403         __ sarl(dest_reg, 16);
1404       }
1405       break;
1406     }
1407 
1408     default:
1409       ShouldNotReachHere();
1410   }
1411 
1412   if (patch != NULL) {
1413     patching_epilog(patch, patch_code, addr-&gt;base()-&gt;as_register(), info);
1414   }
1415 
1416   if (is_reference_type(type)) {
1417 #ifdef _LP64
1418     if (UseCompressedOops &amp;&amp; !wide) {
1419       __ decode_heap_oop(dest-&gt;as_register());
1420     }
1421 #endif
1422 
1423     // Load barrier has not yet been applied, so ZGC can&#39;t verify the oop here
1424     if (!UseZGC) {
1425       __ verify_oop(dest-&gt;as_register());
1426     }
1427   } else if (type == T_ADDRESS &amp;&amp; addr-&gt;disp() == oopDesc::klass_offset_in_bytes()) {
1428 #ifdef _LP64
1429     if (UseCompressedClassPointers) {
1430       __ andl(dest-&gt;as_register(), oopDesc::compressed_klass_mask());
1431       __ decode_klass_not_null(dest-&gt;as_register());
1432     } else {
1433       __ shlq(dest-&gt;as_register(), oopDesc::storage_props_nof_bits);
1434       __ shrq(dest-&gt;as_register(), oopDesc::storage_props_nof_bits);
1435     }
1436 #else
1437     __ andl(dest-&gt;as_register(), oopDesc::wide_klass_mask());
1438 #endif
1439   }
1440 }
1441 
1442 
1443 NEEDS_CLEANUP; // This could be static?
1444 Address::ScaleFactor LIR_Assembler::array_element_size(BasicType type) const {
1445   int elem_size = type2aelembytes(type);
1446   switch (elem_size) {
1447     case 1: return Address::times_1;
1448     case 2: return Address::times_2;
1449     case 4: return Address::times_4;
1450     case 8: return Address::times_8;
1451   }
1452   ShouldNotReachHere();
1453   return Address::no_scale;
1454 }
1455 
1456 
1457 void LIR_Assembler::emit_op3(LIR_Op3* op) {
1458   switch (op-&gt;code()) {
1459     case lir_idiv:
1460     case lir_irem:
1461       arithmetic_idiv(op-&gt;code(),
1462                       op-&gt;in_opr1(),
1463                       op-&gt;in_opr2(),
1464                       op-&gt;in_opr3(),
1465                       op-&gt;result_opr(),
1466                       op-&gt;info());
1467       break;
1468     case lir_fmad:
1469       __ fmad(op-&gt;result_opr()-&gt;as_xmm_double_reg(),
1470               op-&gt;in_opr1()-&gt;as_xmm_double_reg(),
1471               op-&gt;in_opr2()-&gt;as_xmm_double_reg(),
1472               op-&gt;in_opr3()-&gt;as_xmm_double_reg());
1473       break;
1474     case lir_fmaf:
1475       __ fmaf(op-&gt;result_opr()-&gt;as_xmm_float_reg(),
1476               op-&gt;in_opr1()-&gt;as_xmm_float_reg(),
1477               op-&gt;in_opr2()-&gt;as_xmm_float_reg(),
1478               op-&gt;in_opr3()-&gt;as_xmm_float_reg());
1479       break;
1480     default:      ShouldNotReachHere(); break;
1481   }
1482 }
1483 
1484 void LIR_Assembler::emit_opBranch(LIR_OpBranch* op) {
1485 #ifdef ASSERT
1486   assert(op-&gt;block() == NULL || op-&gt;block()-&gt;label() == op-&gt;label(), &quot;wrong label&quot;);
1487   if (op-&gt;block() != NULL)  _branch_target_blocks.append(op-&gt;block());
1488   if (op-&gt;ublock() != NULL) _branch_target_blocks.append(op-&gt;ublock());
1489 #endif
1490 
1491   if (op-&gt;cond() == lir_cond_always) {
1492     if (op-&gt;info() != NULL) add_debug_info_for_branch(op-&gt;info());
1493     __ jmp (*(op-&gt;label()));
1494   } else {
1495     Assembler::Condition acond = Assembler::zero;
1496     if (op-&gt;code() == lir_cond_float_branch) {
1497       assert(op-&gt;ublock() != NULL, &quot;must have unordered successor&quot;);
1498       __ jcc(Assembler::parity, *(op-&gt;ublock()-&gt;label()));
1499       switch(op-&gt;cond()) {
1500         case lir_cond_equal:        acond = Assembler::equal;      break;
1501         case lir_cond_notEqual:     acond = Assembler::notEqual;   break;
1502         case lir_cond_less:         acond = Assembler::below;      break;
1503         case lir_cond_lessEqual:    acond = Assembler::belowEqual; break;
1504         case lir_cond_greaterEqual: acond = Assembler::aboveEqual; break;
1505         case lir_cond_greater:      acond = Assembler::above;      break;
1506         default:                         ShouldNotReachHere();
1507       }
1508     } else {
1509       switch (op-&gt;cond()) {
1510         case lir_cond_equal:        acond = Assembler::equal;       break;
1511         case lir_cond_notEqual:     acond = Assembler::notEqual;    break;
1512         case lir_cond_less:         acond = Assembler::less;        break;
1513         case lir_cond_lessEqual:    acond = Assembler::lessEqual;   break;
1514         case lir_cond_greaterEqual: acond = Assembler::greaterEqual;break;
1515         case lir_cond_greater:      acond = Assembler::greater;     break;
1516         case lir_cond_belowEqual:   acond = Assembler::belowEqual;  break;
1517         case lir_cond_aboveEqual:   acond = Assembler::aboveEqual;  break;
1518         default:                         ShouldNotReachHere();
1519       }
1520     }
1521     __ jcc(acond,*(op-&gt;label()));
1522   }
1523 }
1524 
1525 void LIR_Assembler::emit_opConvert(LIR_OpConvert* op) {
1526   LIR_Opr src  = op-&gt;in_opr();
1527   LIR_Opr dest = op-&gt;result_opr();
1528 
1529   switch (op-&gt;bytecode()) {
1530     case Bytecodes::_i2l:
1531 #ifdef _LP64
1532       __ movl2ptr(dest-&gt;as_register_lo(), src-&gt;as_register());
1533 #else
1534       move_regs(src-&gt;as_register(), dest-&gt;as_register_lo());
1535       move_regs(src-&gt;as_register(), dest-&gt;as_register_hi());
1536       __ sarl(dest-&gt;as_register_hi(), 31);
1537 #endif // LP64
1538       break;
1539 
1540     case Bytecodes::_l2i:
1541 #ifdef _LP64
1542       __ movl(dest-&gt;as_register(), src-&gt;as_register_lo());
1543 #else
1544       move_regs(src-&gt;as_register_lo(), dest-&gt;as_register());
1545 #endif
1546       break;
1547 
1548     case Bytecodes::_i2b:
1549       move_regs(src-&gt;as_register(), dest-&gt;as_register());
1550       __ sign_extend_byte(dest-&gt;as_register());
1551       break;
1552 
1553     case Bytecodes::_i2c:
1554       move_regs(src-&gt;as_register(), dest-&gt;as_register());
1555       __ andl(dest-&gt;as_register(), 0xFFFF);
1556       break;
1557 
1558     case Bytecodes::_i2s:
1559       move_regs(src-&gt;as_register(), dest-&gt;as_register());
1560       __ sign_extend_short(dest-&gt;as_register());
1561       break;
1562 
1563 
1564 #ifdef _LP64
1565     case Bytecodes::_f2d:
1566       __ cvtss2sd(dest-&gt;as_xmm_double_reg(), src-&gt;as_xmm_float_reg());
1567       break;
1568 
1569     case Bytecodes::_d2f:
1570       __ cvtsd2ss(dest-&gt;as_xmm_float_reg(), src-&gt;as_xmm_double_reg());
1571       break;
1572 
1573     case Bytecodes::_i2f:
1574       __ cvtsi2ssl(dest-&gt;as_xmm_float_reg(), src-&gt;as_register());
1575       break;
1576 
1577     case Bytecodes::_i2d:
1578       __ cvtsi2sdl(dest-&gt;as_xmm_double_reg(), src-&gt;as_register());
1579       break;
1580 
1581     case Bytecodes::_l2f:
1582       __ cvtsi2ssq(dest-&gt;as_xmm_float_reg(), src-&gt;as_register_lo());
1583       break;
1584 
1585     case Bytecodes::_l2d:
1586       __ cvtsi2sdq(dest-&gt;as_xmm_double_reg(), src-&gt;as_register_lo());
1587       break;
1588 
1589     case Bytecodes::_f2i:
1590       __ convert_f2i(dest-&gt;as_register(), src-&gt;as_xmm_float_reg());
1591       break;
1592 
1593     case Bytecodes::_d2i:
1594       __ convert_d2i(dest-&gt;as_register(), src-&gt;as_xmm_double_reg());
1595       break;
1596 
1597     case Bytecodes::_f2l:
1598       __ convert_f2l(dest-&gt;as_register_lo(), src-&gt;as_xmm_float_reg());
1599       break;
1600 
1601     case Bytecodes::_d2l:
1602       __ convert_d2l(dest-&gt;as_register_lo(), src-&gt;as_xmm_double_reg());
1603       break;
1604 #else
1605     case Bytecodes::_f2d:
1606     case Bytecodes::_d2f:
1607       if (dest-&gt;is_single_xmm()) {
1608         __ cvtsd2ss(dest-&gt;as_xmm_float_reg(), src-&gt;as_xmm_double_reg());
1609       } else if (dest-&gt;is_double_xmm()) {
1610         __ cvtss2sd(dest-&gt;as_xmm_double_reg(), src-&gt;as_xmm_float_reg());
1611       } else {
1612         assert(src-&gt;fpu() == dest-&gt;fpu(), &quot;register must be equal&quot;);
1613         // do nothing (float result is rounded later through spilling)
1614       }
1615       break;
1616 
1617     case Bytecodes::_i2f:
1618     case Bytecodes::_i2d:
1619       if (dest-&gt;is_single_xmm()) {
1620         __ cvtsi2ssl(dest-&gt;as_xmm_float_reg(), src-&gt;as_register());
1621       } else if (dest-&gt;is_double_xmm()) {
1622         __ cvtsi2sdl(dest-&gt;as_xmm_double_reg(), src-&gt;as_register());
1623       } else {
1624         assert(dest-&gt;fpu() == 0, &quot;result must be on TOS&quot;);
1625         __ movl(Address(rsp, 0), src-&gt;as_register());
1626         __ fild_s(Address(rsp, 0));
1627       }
1628       break;
1629 
1630     case Bytecodes::_l2f:
1631     case Bytecodes::_l2d:
1632       assert(!dest-&gt;is_xmm_register(), &quot;result in xmm register not supported (no SSE instruction present)&quot;);
1633       assert(dest-&gt;fpu() == 0, &quot;result must be on TOS&quot;);
1634       __ movptr(Address(rsp, 0),          src-&gt;as_register_lo());
1635       __ movl(Address(rsp, BytesPerWord), src-&gt;as_register_hi());
1636       __ fild_d(Address(rsp, 0));
1637       // float result is rounded later through spilling
1638       break;
1639 
1640     case Bytecodes::_f2i:
1641     case Bytecodes::_d2i:
1642       if (src-&gt;is_single_xmm()) {
1643         __ cvttss2sil(dest-&gt;as_register(), src-&gt;as_xmm_float_reg());
1644       } else if (src-&gt;is_double_xmm()) {
1645         __ cvttsd2sil(dest-&gt;as_register(), src-&gt;as_xmm_double_reg());
1646       } else {
1647         assert(src-&gt;fpu() == 0, &quot;input must be on TOS&quot;);
1648         __ fldcw(ExternalAddress(StubRoutines::addr_fpu_cntrl_wrd_trunc()));
1649         __ fist_s(Address(rsp, 0));
1650         __ movl(dest-&gt;as_register(), Address(rsp, 0));
1651         __ fldcw(ExternalAddress(StubRoutines::addr_fpu_cntrl_wrd_std()));
1652       }
1653       // IA32 conversion instructions do not match JLS for overflow, underflow and NaN -&gt; fixup in stub
1654       assert(op-&gt;stub() != NULL, &quot;stub required&quot;);
1655       __ cmpl(dest-&gt;as_register(), 0x80000000);
1656       __ jcc(Assembler::equal, *op-&gt;stub()-&gt;entry());
1657       __ bind(*op-&gt;stub()-&gt;continuation());
1658       break;
1659 
1660     case Bytecodes::_f2l:
1661     case Bytecodes::_d2l:
1662       assert(!src-&gt;is_xmm_register(), &quot;input in xmm register not supported (no SSE instruction present)&quot;);
1663       assert(src-&gt;fpu() == 0, &quot;input must be on TOS&quot;);
1664       assert(dest == FrameMap::long0_opr, &quot;runtime stub places result in these registers&quot;);
1665 
1666       // instruction sequence too long to inline it here
1667       {
1668         __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::fpu2long_stub_id)));
1669       }
1670       break;
1671 #endif // _LP64
1672 
1673     default: ShouldNotReachHere();
1674   }
1675 }
1676 
1677 void LIR_Assembler::emit_alloc_obj(LIR_OpAllocObj* op) {
1678   if (op-&gt;init_check()) {
1679     add_debug_info_for_null_check_here(op-&gt;stub()-&gt;info());
1680     __ cmpb(Address(op-&gt;klass()-&gt;as_register(),
1681                     InstanceKlass::init_state_offset()),
1682                     InstanceKlass::fully_initialized);
1683     __ jcc(Assembler::notEqual, *op-&gt;stub()-&gt;entry());
1684   }
1685   __ allocate_object(op-&gt;obj()-&gt;as_register(),
1686                      op-&gt;tmp1()-&gt;as_register(),
1687                      op-&gt;tmp2()-&gt;as_register(),
1688                      op-&gt;header_size(),
1689                      op-&gt;object_size(),
1690                      op-&gt;klass()-&gt;as_register(),
1691                      *op-&gt;stub()-&gt;entry());
1692   __ bind(*op-&gt;stub()-&gt;continuation());
1693 }
1694 
1695 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
1696   Register len =  op-&gt;len()-&gt;as_register();
1697   LP64_ONLY( __ movslq(len, len); )
1698 
1699   if (UseSlowPath || op-&gt;type() == T_VALUETYPE ||
1700       (!UseFastNewObjectArray &amp;&amp; is_reference_type(op-&gt;type())) ||
1701       (!UseFastNewTypeArray   &amp;&amp; !is_reference_type(op-&gt;type()))) {
1702     __ jmp(*op-&gt;stub()-&gt;entry());
1703   } else {
1704     Register tmp1 = op-&gt;tmp1()-&gt;as_register();
1705     Register tmp2 = op-&gt;tmp2()-&gt;as_register();
1706     Register tmp3 = op-&gt;tmp3()-&gt;as_register();
1707     if (len == tmp1) {
1708       tmp1 = tmp3;
1709     } else if (len == tmp2) {
1710       tmp2 = tmp3;
1711     } else if (len == tmp3) {
1712       // everything is ok
1713     } else {
1714       __ mov(tmp3, len);
1715     }
1716     __ allocate_array(op-&gt;obj()-&gt;as_register(),
1717                       len,
1718                       tmp1,
1719                       tmp2,
1720                       arrayOopDesc::header_size(op-&gt;type()),
1721                       array_element_size(op-&gt;type()),
1722                       op-&gt;klass()-&gt;as_register(),
1723                       *op-&gt;stub()-&gt;entry());
1724   }
1725   __ bind(*op-&gt;stub()-&gt;continuation());
1726 }
1727 
1728 void LIR_Assembler::type_profile_helper(Register mdo,
1729                                         ciMethodData *md, ciProfileData *data,
1730                                         Register recv, Label* update_done) {
1731   for (uint i = 0; i &lt; ReceiverTypeData::row_limit(); i++) {
1732     Label next_test;
1733     // See if the receiver is receiver[n].
1734     __ cmpptr(recv, Address(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i))));
1735     __ jccb(Assembler::notEqual, next_test);
1736     Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i)));
1737     __ addptr(data_addr, DataLayout::counter_increment);
1738     __ jmp(*update_done);
1739     __ bind(next_test);
1740   }
1741 
1742   // Didn&#39;t find receiver; find next empty slot and fill it in
1743   for (uint i = 0; i &lt; ReceiverTypeData::row_limit(); i++) {
1744     Label next_test;
1745     Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_offset(i)));
1746     __ cmpptr(recv_addr, (intptr_t)NULL_WORD);
1747     __ jccb(Assembler::notEqual, next_test);
1748     __ movptr(recv_addr, recv);
1749     __ movptr(Address(mdo, md-&gt;byte_offset_of_slot(data, ReceiverTypeData::receiver_count_offset(i))), DataLayout::counter_increment);
1750     __ jmp(*update_done);
1751     __ bind(next_test);
1752   }
1753 }
1754 
1755 void LIR_Assembler::emit_typecheck_helper(LIR_OpTypeCheck *op, Label* success, Label* failure, Label* obj_is_null) {
1756   // we always need a stub for the failure case.
1757   CodeStub* stub = op-&gt;stub();
1758   Register obj = op-&gt;object()-&gt;as_register();
1759   Register k_RInfo = op-&gt;tmp1()-&gt;as_register();
1760   Register klass_RInfo = op-&gt;tmp2()-&gt;as_register();
1761   Register dst = op-&gt;result_opr()-&gt;as_register();
1762   ciKlass* k = op-&gt;klass();
1763   Register Rtmp1 = noreg;
1764 
1765   // check if it needs to be profiled
1766   ciMethodData* md = NULL;
1767   ciProfileData* data = NULL;
1768 
1769   if (op-&gt;should_profile()) {
1770     ciMethod* method = op-&gt;profiled_method();
1771     assert(method != NULL, &quot;Should have method&quot;);
1772     int bci = op-&gt;profiled_bci();
1773     md = method-&gt;method_data_or_null();
1774     assert(md != NULL, &quot;Sanity&quot;);
1775     data = md-&gt;bci_to_data(bci);
1776     assert(data != NULL,                &quot;need data for type check&quot;);
1777     assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
1778   }
1779   Label profile_cast_success, profile_cast_failure;
1780   Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : success;
1781   Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : failure;
1782 
1783   if (obj == k_RInfo) {
1784     k_RInfo = dst;
1785   } else if (obj == klass_RInfo) {
1786     klass_RInfo = dst;
1787   }
1788   if (k-&gt;is_loaded() &amp;&amp; !UseCompressedClassPointers) {
1789     select_different_registers(obj, dst, k_RInfo, klass_RInfo);
1790   } else {
1791     Rtmp1 = op-&gt;tmp3()-&gt;as_register();
1792     select_different_registers(obj, dst, k_RInfo, klass_RInfo, Rtmp1);
1793   }
1794 
1795   assert_different_registers(obj, k_RInfo, klass_RInfo);
1796 
1797   if (op-&gt;need_null_check()) {
1798     __ cmpptr(obj, (int32_t)NULL_WORD);
1799     if (op-&gt;should_profile()) {
1800       Label not_null;
1801       __ jccb(Assembler::notEqual, not_null);
1802       // Object is null; update MDO and exit
1803       Register mdo  = klass_RInfo;
1804       __ mov_metadata(mdo, md-&gt;constant_encoding());
1805       Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()));
1806       int header_bits = BitData::null_seen_byte_constant();
1807       __ orb(data_addr, header_bits);
1808       __ jmp(*obj_is_null);
1809       __ bind(not_null);
1810     } else {
1811       __ jcc(Assembler::equal, *obj_is_null);
1812     }
1813   }
1814 
1815   if (!k-&gt;is_loaded()) {
1816     klass2reg_with_patching(k_RInfo, op-&gt;info_for_patch());
1817   } else {
1818 #ifdef _LP64
1819     __ mov_metadata(k_RInfo, k-&gt;constant_encoding());
1820 #endif // _LP64
1821   }
1822   __ verify_oop(obj);
1823 
1824   if (op-&gt;fast_check()) {
1825     // get object class
1826     // not a safepoint as obj null check happens earlier
1827 #ifdef _LP64
1828     if (UseCompressedClassPointers) {
1829       __ load_klass(Rtmp1, obj);
1830       __ cmpptr(k_RInfo, Rtmp1);
1831     } else {
1832       __ cmpptr(k_RInfo, Address(obj, oopDesc::klass_offset_in_bytes()));
1833     }
1834 #else
1835     if (k-&gt;is_loaded()) {
1836       __ cmpklass(Address(obj, oopDesc::klass_offset_in_bytes()), k-&gt;constant_encoding());
1837     } else {
1838       __ cmpptr(k_RInfo, Address(obj, oopDesc::klass_offset_in_bytes()));
1839     }
1840 #endif
1841     __ jcc(Assembler::notEqual, *failure_target);
1842     // successful cast, fall through to profile or jump
1843   } else {
1844     // get object class
1845     // not a safepoint as obj null check happens earlier
1846     __ load_klass(klass_RInfo, obj);
1847     if (k-&gt;is_loaded()) {
1848       // See if we get an immediate positive hit
1849 #ifdef _LP64
1850       __ cmpptr(k_RInfo, Address(klass_RInfo, k-&gt;super_check_offset()));
1851 #else
1852       __ cmpklass(Address(klass_RInfo, k-&gt;super_check_offset()), k-&gt;constant_encoding());
1853 #endif // _LP64
1854       if ((juint)in_bytes(Klass::secondary_super_cache_offset()) != k-&gt;super_check_offset()) {
1855         __ jcc(Assembler::notEqual, *failure_target);
1856         // successful cast, fall through to profile or jump
1857       } else {
1858         // See if we get an immediate positive hit
1859         __ jcc(Assembler::equal, *success_target);
1860         // check for self
1861 #ifdef _LP64
1862         __ cmpptr(klass_RInfo, k_RInfo);
1863 #else
1864         __ cmpklass(klass_RInfo, k-&gt;constant_encoding());
1865 #endif // _LP64
1866         __ jcc(Assembler::equal, *success_target);
1867 
1868         __ push(klass_RInfo);
1869 #ifdef _LP64
1870         __ push(k_RInfo);
1871 #else
1872         __ pushklass(k-&gt;constant_encoding());
1873 #endif // _LP64
1874         __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
1875         __ pop(klass_RInfo);
1876         __ pop(klass_RInfo);
1877         // result is a boolean
1878         __ cmpl(klass_RInfo, 0);
1879         __ jcc(Assembler::equal, *failure_target);
1880         // successful cast, fall through to profile or jump
1881       }
1882     } else {
1883       // perform the fast part of the checking logic
1884       __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);
1885       // call out-of-line instance of __ check_klass_subtype_slow_path(...):
1886       __ push(klass_RInfo);
1887       __ push(k_RInfo);
1888       __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
1889       __ pop(klass_RInfo);
1890       __ pop(k_RInfo);
1891       // result is a boolean
1892       __ cmpl(k_RInfo, 0);
1893       __ jcc(Assembler::equal, *failure_target);
1894       // successful cast, fall through to profile or jump
1895     }
1896   }
1897   if (op-&gt;should_profile()) {
1898     Register mdo  = klass_RInfo, recv = k_RInfo;
1899     __ bind(profile_cast_success);
1900     __ mov_metadata(mdo, md-&gt;constant_encoding());
1901     __ load_klass(recv, obj);
1902     type_profile_helper(mdo, md, data, recv, success);
1903     __ jmp(*success);
1904 
1905     __ bind(profile_cast_failure);
1906     __ mov_metadata(mdo, md-&gt;constant_encoding());
1907     Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()));
1908     __ subptr(counter_addr, DataLayout::counter_increment);
1909     __ jmp(*failure);
1910   }
1911   __ jmp(*success);
1912 }
1913 
1914 
1915 void LIR_Assembler::emit_opTypeCheck(LIR_OpTypeCheck* op) {
1916   LIR_Code code = op-&gt;code();
1917   if (code == lir_store_check) {
1918     Register value = op-&gt;object()-&gt;as_register();
1919     Register array = op-&gt;array()-&gt;as_register();
1920     Register k_RInfo = op-&gt;tmp1()-&gt;as_register();
1921     Register klass_RInfo = op-&gt;tmp2()-&gt;as_register();
1922     Register Rtmp1 = op-&gt;tmp3()-&gt;as_register();
1923 
1924     CodeStub* stub = op-&gt;stub();
1925 
1926     // check if it needs to be profiled
1927     ciMethodData* md = NULL;
1928     ciProfileData* data = NULL;
1929 
1930     if (op-&gt;should_profile()) {
1931       ciMethod* method = op-&gt;profiled_method();
1932       assert(method != NULL, &quot;Should have method&quot;);
1933       int bci = op-&gt;profiled_bci();
1934       md = method-&gt;method_data_or_null();
1935       assert(md != NULL, &quot;Sanity&quot;);
1936       data = md-&gt;bci_to_data(bci);
1937       assert(data != NULL,                &quot;need data for type check&quot;);
1938       assert(data-&gt;is_ReceiverTypeData(), &quot;need ReceiverTypeData for type check&quot;);
1939     }
1940     Label profile_cast_success, profile_cast_failure, done;
1941     Label *success_target = op-&gt;should_profile() ? &amp;profile_cast_success : &amp;done;
1942     Label *failure_target = op-&gt;should_profile() ? &amp;profile_cast_failure : stub-&gt;entry();
1943 
1944     __ cmpptr(value, (int32_t)NULL_WORD);
1945     if (op-&gt;should_profile()) {
1946       Label not_null;
1947       __ jccb(Assembler::notEqual, not_null);
1948       // Object is null; update MDO and exit
1949       Register mdo  = klass_RInfo;
1950       __ mov_metadata(mdo, md-&gt;constant_encoding());
1951       Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, DataLayout::flags_offset()));
1952       int header_bits = BitData::null_seen_byte_constant();
1953       __ orb(data_addr, header_bits);
1954       __ jmp(done);
1955       __ bind(not_null);
1956     } else {
1957       __ jcc(Assembler::equal, done);
1958     }
1959 
1960     add_debug_info_for_null_check_here(op-&gt;info_for_exception());
1961     __ load_klass(k_RInfo, array);
1962     __ load_klass(klass_RInfo, value);
1963 
1964     // get instance klass (it&#39;s already uncompressed)
1965     __ movptr(k_RInfo, Address(k_RInfo, ObjArrayKlass::element_klass_offset()));
1966     // perform the fast part of the checking logic
1967     __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);
1968     // call out-of-line instance of __ check_klass_subtype_slow_path(...):
1969     __ push(klass_RInfo);
1970     __ push(k_RInfo);
1971     __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
1972     __ pop(klass_RInfo);
1973     __ pop(k_RInfo);
1974     // result is a boolean
1975     __ cmpl(k_RInfo, 0);
1976     __ jcc(Assembler::equal, *failure_target);
1977     // fall through to the success case
1978 
1979     if (op-&gt;should_profile()) {
1980       Register mdo  = klass_RInfo, recv = k_RInfo;
1981       __ bind(profile_cast_success);
1982       __ mov_metadata(mdo, md-&gt;constant_encoding());
1983       __ load_klass(recv, value);
1984       type_profile_helper(mdo, md, data, recv, &amp;done);
1985       __ jmpb(done);
1986 
1987       __ bind(profile_cast_failure);
1988       __ mov_metadata(mdo, md-&gt;constant_encoding());
1989       Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()));
1990       __ subptr(counter_addr, DataLayout::counter_increment);
1991       __ jmp(*stub-&gt;entry());
1992     }
1993 
1994     __ bind(done);
1995   } else
1996     if (code == lir_checkcast) {
1997       Register obj = op-&gt;object()-&gt;as_register();
1998       Register dst = op-&gt;result_opr()-&gt;as_register();
1999       Label success;
2000       emit_typecheck_helper(op, &amp;success, op-&gt;stub()-&gt;entry(), &amp;success);
2001       __ bind(success);
2002       if (dst != obj) {
2003         __ mov(dst, obj);
2004       }
2005     } else
2006       if (code == lir_instanceof) {
2007         Register obj = op-&gt;object()-&gt;as_register();
2008         Register dst = op-&gt;result_opr()-&gt;as_register();
2009         Label success, failure, done;
2010         emit_typecheck_helper(op, &amp;success, &amp;failure, &amp;failure);
2011         __ bind(failure);
2012         __ xorptr(dst, dst);
2013         __ jmpb(done);
2014         __ bind(success);
2015         __ movptr(dst, 1);
2016         __ bind(done);
2017       } else {
2018         ShouldNotReachHere();
2019       }
2020 
2021 }
2022 
2023 void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {
2024   // We are loading/storing an array that *may* be a flattened array (the declared type
2025   // Object[], interface[], or VT?[]). If this array is flattened, take slow path.
2026 
2027   __ load_storage_props(op-&gt;tmp()-&gt;as_register(), op-&gt;array()-&gt;as_register());
2028   __ testb(op-&gt;tmp()-&gt;as_register(), ArrayStorageProperties::flattened_value);
2029   __ jcc(Assembler::notZero, *op-&gt;stub()-&gt;entry());
2030   if (!op-&gt;value()-&gt;is_illegal()) {
2031     // We are storing into the array.
2032     Label skip;
2033     __ testb(op-&gt;tmp()-&gt;as_register(), ArrayStorageProperties::null_free_value);
2034     __ jcc(Assembler::zero, skip);
2035     // The array is not flattened, but it is null_free. If we are storing
2036     // a null, take the slow path (which will throw NPE).
2037     __ cmpptr(op-&gt;value()-&gt;as_register(), (int32_t)NULL_WORD);
2038     __ jcc(Assembler::zero, *op-&gt;stub()-&gt;entry());
2039     __ bind(skip);
2040   }
2041 }
2042 
2043 void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {
2044   // This is called when we use aastore into a an array declared as &quot;[LVT;&quot;,
2045   // where we know VT is not flattenable (due to ValueArrayElemMaxFlatOops, etc).
2046   // However, we need to do a NULL check if the actual array is a &quot;[QVT;&quot;.
2047 
2048   __ load_storage_props(op-&gt;tmp()-&gt;as_register(), op-&gt;array()-&gt;as_register());
2049   __ testb(op-&gt;tmp()-&gt;as_register(), ArrayStorageProperties::null_free_value);
2050 }
2051 
2052 void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {
2053   Label L_oops_equal;
2054   Label L_oops_not_equal;
2055   Label L_end;
2056 
2057   Register left  = op-&gt;left()-&gt;as_register();
2058   Register right = op-&gt;right()-&gt;as_register();
2059 
2060   __ cmpptr(left, right);
2061   __ jcc(Assembler::equal, L_oops_equal);
2062 
2063   // (1) Null check -- if one of the operands is null, the other must not be null (because
2064   //     the two references are not equal), so they are not substitutable,
2065   //     FIXME: do null check only if the operand is nullable
2066   {
2067     __ cmpptr(left, (int32_t)NULL_WORD);
2068     __ jcc(Assembler::equal, L_oops_not_equal);
2069 
2070     __ cmpptr(right, (int32_t)NULL_WORD);
2071     __ jcc(Assembler::equal, L_oops_not_equal);
2072   }
2073 
2074   ciKlass* left_klass = op-&gt;left_klass();
2075   ciKlass* right_klass = op-&gt;right_klass();
2076 
2077   // (2) Value object check -- if either of the operands is not a value object,
2078   //     they are not substitutable. We do this only if we are not sure that the
2079   //     operands are value objects
2080   if ((left_klass == NULL || right_klass == NULL) ||// The klass is still unloaded, or came from a Phi node.
2081       !left_klass-&gt;is_valuetype() || !right_klass-&gt;is_valuetype()) {
2082     Register tmp1  = op-&gt;tmp1()-&gt;as_register();
2083     __ movptr(tmp1, (intptr_t)markWord::always_locked_pattern);
2084     __ andl(tmp1, Address(left, oopDesc::mark_offset_in_bytes()));
2085     __ andl(tmp1, Address(right, oopDesc::mark_offset_in_bytes()));
2086     __ cmpptr(tmp1, (intptr_t)markWord::always_locked_pattern);
2087     __ jcc(Assembler::notEqual, L_oops_not_equal);
2088   }
2089 
2090   // (3) Same klass check: if the operands are of different klasses, they are not substitutable.
2091   if (left_klass != NULL &amp;&amp; left_klass-&gt;is_valuetype() &amp;&amp; left_klass == right_klass) {
2092     // No need to load klass -- the operands are statically known to be the same value klass.
2093     __ jmp(*op-&gt;stub()-&gt;entry());
2094   } else {
2095     Register left_klass_op = op-&gt;left_klass_op()-&gt;as_register();
2096     Register right_klass_op = op-&gt;right_klass_op()-&gt;as_register();
2097 
2098     if (UseCompressedOops) {
2099       __ movl(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));
2100       __ movl(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));
2101       __ cmpl(left_klass_op, right_klass_op);
2102     } else {
2103       __ movptr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));
2104       __ movptr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));
2105       __ cmpptr(left_klass_op, right_klass_op);
2106     }
2107 
2108     __ jcc(Assembler::equal, *op-&gt;stub()-&gt;entry()); // same klass -&gt; do slow check
2109     // fall through to L_oops_not_equal
2110   }
2111 
2112   __ bind(L_oops_not_equal);
2113   move(op-&gt;not_equal_result(), op-&gt;result_opr());
2114   __ jmp(L_end);
2115 
2116   __ bind(L_oops_equal);
2117   move(op-&gt;equal_result(), op-&gt;result_opr());
2118   __ jmp(L_end);
2119 
2120   // We&#39;ve returned from the stub. RAX contains 0x0 IFF the two
2121   // operands are not substitutable. (Don&#39;t compare against 0x1 in case the
2122   // C compiler is naughty)
2123   __ bind(*op-&gt;stub()-&gt;continuation());
2124   __ cmpl(rax, 0);
2125   __ jcc(Assembler::equal, L_oops_not_equal); // (call_stub() == 0x0) -&gt; not_equal
2126   move(op-&gt;equal_result(), op-&gt;result_opr()); // (call_stub() != 0x0) -&gt; equal
2127   // fall-through
2128   __ bind(L_end);
2129 }
2130 
2131 void LIR_Assembler::emit_compare_and_swap(LIR_OpCompareAndSwap* op) {
2132   if (LP64_ONLY(false &amp;&amp;) op-&gt;code() == lir_cas_long &amp;&amp; VM_Version::supports_cx8()) {
2133     assert(op-&gt;cmp_value()-&gt;as_register_lo() == rax, &quot;wrong register&quot;);
2134     assert(op-&gt;cmp_value()-&gt;as_register_hi() == rdx, &quot;wrong register&quot;);
2135     assert(op-&gt;new_value()-&gt;as_register_lo() == rbx, &quot;wrong register&quot;);
2136     assert(op-&gt;new_value()-&gt;as_register_hi() == rcx, &quot;wrong register&quot;);
2137     Register addr = op-&gt;addr()-&gt;as_register();
2138     __ lock();
2139     NOT_LP64(__ cmpxchg8(Address(addr, 0)));
2140 
2141   } else if (op-&gt;code() == lir_cas_int || op-&gt;code() == lir_cas_obj ) {
2142     NOT_LP64(assert(op-&gt;addr()-&gt;is_single_cpu(), &quot;must be single&quot;);)
2143     Register addr = (op-&gt;addr()-&gt;is_single_cpu() ? op-&gt;addr()-&gt;as_register() : op-&gt;addr()-&gt;as_register_lo());
2144     Register newval = op-&gt;new_value()-&gt;as_register();
2145     Register cmpval = op-&gt;cmp_value()-&gt;as_register();
2146     assert(cmpval == rax, &quot;wrong register&quot;);
2147     assert(newval != NULL, &quot;new val must be register&quot;);
2148     assert(cmpval != newval, &quot;cmp and new values must be in different registers&quot;);
2149     assert(cmpval != addr, &quot;cmp and addr must be in different registers&quot;);
2150     assert(newval != addr, &quot;new value and addr must be in different registers&quot;);
2151 
2152     if ( op-&gt;code() == lir_cas_obj) {
2153 #ifdef _LP64
2154       if (UseCompressedOops) {
2155         __ encode_heap_oop(cmpval);
2156         __ mov(rscratch1, newval);
2157         __ encode_heap_oop(rscratch1);
2158         __ lock();
2159         // cmpval (rax) is implicitly used by this instruction
2160         __ cmpxchgl(rscratch1, Address(addr, 0));
2161       } else
2162 #endif
2163       {
2164         __ lock();
2165         __ cmpxchgptr(newval, Address(addr, 0));
2166       }
2167     } else {
2168       assert(op-&gt;code() == lir_cas_int, &quot;lir_cas_int expected&quot;);
2169       __ lock();
2170       __ cmpxchgl(newval, Address(addr, 0));
2171     }
2172 #ifdef _LP64
2173   } else if (op-&gt;code() == lir_cas_long) {
2174     Register addr = (op-&gt;addr()-&gt;is_single_cpu() ? op-&gt;addr()-&gt;as_register() : op-&gt;addr()-&gt;as_register_lo());
2175     Register newval = op-&gt;new_value()-&gt;as_register_lo();
2176     Register cmpval = op-&gt;cmp_value()-&gt;as_register_lo();
2177     assert(cmpval == rax, &quot;wrong register&quot;);
2178     assert(newval != NULL, &quot;new val must be register&quot;);
2179     assert(cmpval != newval, &quot;cmp and new values must be in different registers&quot;);
2180     assert(cmpval != addr, &quot;cmp and addr must be in different registers&quot;);
2181     assert(newval != addr, &quot;new value and addr must be in different registers&quot;);
2182     __ lock();
2183     __ cmpxchgq(newval, Address(addr, 0));
2184 #endif // _LP64
2185   } else {
2186     Unimplemented();
2187   }
2188 }
2189 
2190 void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {
2191   assert(dst-&gt;is_cpu_register(), &quot;must be&quot;);
2192   assert(dst-&gt;type() == src-&gt;type(), &quot;must be&quot;);
2193 
2194   if (src-&gt;is_cpu_register()) {
2195     reg2reg(src, dst);
2196   } else if (src-&gt;is_stack()) {
2197     stack2reg(src, dst, dst-&gt;type());
2198   } else if (src-&gt;is_constant()) {
2199     const2reg(src, dst, lir_patch_none, NULL);
2200   } else {
2201     ShouldNotReachHere();
2202   }
2203 }
2204 
2205 void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {
2206   Assembler::Condition acond, ncond;
2207   switch (condition) {
2208     case lir_cond_equal:        acond = Assembler::equal;        ncond = Assembler::notEqual;     break;
2209     case lir_cond_notEqual:     acond = Assembler::notEqual;     ncond = Assembler::equal;        break;
2210     case lir_cond_less:         acond = Assembler::less;         ncond = Assembler::greaterEqual; break;
2211     case lir_cond_lessEqual:    acond = Assembler::lessEqual;    ncond = Assembler::greater;      break;
2212     case lir_cond_greaterEqual: acond = Assembler::greaterEqual; ncond = Assembler::less;         break;
2213     case lir_cond_greater:      acond = Assembler::greater;      ncond = Assembler::lessEqual;    break;
2214     case lir_cond_belowEqual:   acond = Assembler::belowEqual;   ncond = Assembler::above;        break;
2215     case lir_cond_aboveEqual:   acond = Assembler::aboveEqual;   ncond = Assembler::below;        break;
2216     default:                    acond = Assembler::equal;        ncond = Assembler::notEqual;
2217                                 ShouldNotReachHere();
2218   }
2219 
2220   if (opr1-&gt;is_cpu_register()) {
2221     reg2reg(opr1, result);
2222   } else if (opr1-&gt;is_stack()) {
2223     stack2reg(opr1, result, result-&gt;type());
2224   } else if (opr1-&gt;is_constant()) {
2225     const2reg(opr1, result, lir_patch_none, NULL);
2226   } else {
2227     ShouldNotReachHere();
2228   }
2229 
2230   if (VM_Version::supports_cmov() &amp;&amp; !opr2-&gt;is_constant()) {
2231     // optimized version that does not require a branch
2232     if (opr2-&gt;is_single_cpu()) {
2233       assert(opr2-&gt;cpu_regnr() != result-&gt;cpu_regnr(), &quot;opr2 already overwritten by previous move&quot;);
2234       __ cmov(ncond, result-&gt;as_register(), opr2-&gt;as_register());
2235     } else if (opr2-&gt;is_double_cpu()) {
2236       assert(opr2-&gt;cpu_regnrLo() != result-&gt;cpu_regnrLo() &amp;&amp; opr2-&gt;cpu_regnrLo() != result-&gt;cpu_regnrHi(), &quot;opr2 already overwritten by previous move&quot;);
2237       assert(opr2-&gt;cpu_regnrHi() != result-&gt;cpu_regnrLo() &amp;&amp; opr2-&gt;cpu_regnrHi() != result-&gt;cpu_regnrHi(), &quot;opr2 already overwritten by previous move&quot;);
2238       __ cmovptr(ncond, result-&gt;as_register_lo(), opr2-&gt;as_register_lo());
2239       NOT_LP64(__ cmovptr(ncond, result-&gt;as_register_hi(), opr2-&gt;as_register_hi());)
2240     } else if (opr2-&gt;is_single_stack()) {
2241       __ cmovl(ncond, result-&gt;as_register(), frame_map()-&gt;address_for_slot(opr2-&gt;single_stack_ix()));
2242     } else if (opr2-&gt;is_double_stack()) {
2243       __ cmovptr(ncond, result-&gt;as_register_lo(), frame_map()-&gt;address_for_slot(opr2-&gt;double_stack_ix(), lo_word_offset_in_bytes));
2244       NOT_LP64(__ cmovptr(ncond, result-&gt;as_register_hi(), frame_map()-&gt;address_for_slot(opr2-&gt;double_stack_ix(), hi_word_offset_in_bytes));)
2245     } else {
2246       ShouldNotReachHere();
2247     }
2248 
2249   } else {
2250     Label skip;
2251     __ jcc (acond, skip);
2252     if (opr2-&gt;is_cpu_register()) {
2253       reg2reg(opr2, result);
2254     } else if (opr2-&gt;is_stack()) {
2255       stack2reg(opr2, result, result-&gt;type());
2256     } else if (opr2-&gt;is_constant()) {
2257       const2reg(opr2, result, lir_patch_none, NULL);
2258     } else {
2259       ShouldNotReachHere();
2260     }
2261     __ bind(skip);
2262   }
2263 }
2264 
2265 
2266 void LIR_Assembler::arith_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dest, CodeEmitInfo* info, bool pop_fpu_stack) {
2267   assert(info == NULL, &quot;should never be used, idiv/irem and ldiv/lrem not handled by this method&quot;);
2268 
2269   if (left-&gt;is_single_cpu()) {
2270     assert(left == dest, &quot;left and dest must be equal&quot;);
2271     Register lreg = left-&gt;as_register();
2272 
2273     if (right-&gt;is_single_cpu()) {
2274       // cpu register - cpu register
2275       Register rreg = right-&gt;as_register();
2276       switch (code) {
2277         case lir_add: __ addl (lreg, rreg); break;
2278         case lir_sub: __ subl (lreg, rreg); break;
2279         case lir_mul: __ imull(lreg, rreg); break;
2280         default:      ShouldNotReachHere();
2281       }
2282 
2283     } else if (right-&gt;is_stack()) {
2284       // cpu register - stack
2285       Address raddr = frame_map()-&gt;address_for_slot(right-&gt;single_stack_ix());
2286       switch (code) {
2287         case lir_add: __ addl(lreg, raddr); break;
2288         case lir_sub: __ subl(lreg, raddr); break;
2289         default:      ShouldNotReachHere();
2290       }
2291 
2292     } else if (right-&gt;is_constant()) {
2293       // cpu register - constant
2294       jint c = right-&gt;as_constant_ptr()-&gt;as_jint();
2295       switch (code) {
2296         case lir_add: {
2297           __ incrementl(lreg, c);
2298           break;
2299         }
2300         case lir_sub: {
2301           __ decrementl(lreg, c);
2302           break;
2303         }
2304         default: ShouldNotReachHere();
2305       }
2306 
2307     } else {
2308       ShouldNotReachHere();
2309     }
2310 
2311   } else if (left-&gt;is_double_cpu()) {
2312     assert(left == dest, &quot;left and dest must be equal&quot;);
2313     Register lreg_lo = left-&gt;as_register_lo();
2314     Register lreg_hi = left-&gt;as_register_hi();
2315 
2316     if (right-&gt;is_double_cpu()) {
2317       // cpu register - cpu register
2318       Register rreg_lo = right-&gt;as_register_lo();
2319       Register rreg_hi = right-&gt;as_register_hi();
2320       NOT_LP64(assert_different_registers(lreg_lo, lreg_hi, rreg_lo, rreg_hi));
2321       LP64_ONLY(assert_different_registers(lreg_lo, rreg_lo));
2322       switch (code) {
2323         case lir_add:
2324           __ addptr(lreg_lo, rreg_lo);
2325           NOT_LP64(__ adcl(lreg_hi, rreg_hi));
2326           break;
2327         case lir_sub:
2328           __ subptr(lreg_lo, rreg_lo);
2329           NOT_LP64(__ sbbl(lreg_hi, rreg_hi));
2330           break;
2331         case lir_mul:
2332 #ifdef _LP64
2333           __ imulq(lreg_lo, rreg_lo);
2334 #else
2335           assert(lreg_lo == rax &amp;&amp; lreg_hi == rdx, &quot;must be&quot;);
2336           __ imull(lreg_hi, rreg_lo);
2337           __ imull(rreg_hi, lreg_lo);
2338           __ addl (rreg_hi, lreg_hi);
2339           __ mull (rreg_lo);
2340           __ addl (lreg_hi, rreg_hi);
2341 #endif // _LP64
2342           break;
2343         default:
2344           ShouldNotReachHere();
2345       }
2346 
2347     } else if (right-&gt;is_constant()) {
2348       // cpu register - constant
2349 #ifdef _LP64
2350       jlong c = right-&gt;as_constant_ptr()-&gt;as_jlong_bits();
2351       __ movptr(r10, (intptr_t) c);
2352       switch (code) {
2353         case lir_add:
2354           __ addptr(lreg_lo, r10);
2355           break;
2356         case lir_sub:
2357           __ subptr(lreg_lo, r10);
2358           break;
2359         default:
2360           ShouldNotReachHere();
2361       }
2362 #else
2363       jint c_lo = right-&gt;as_constant_ptr()-&gt;as_jint_lo();
2364       jint c_hi = right-&gt;as_constant_ptr()-&gt;as_jint_hi();
2365       switch (code) {
2366         case lir_add:
2367           __ addptr(lreg_lo, c_lo);
2368           __ adcl(lreg_hi, c_hi);
2369           break;
2370         case lir_sub:
2371           __ subptr(lreg_lo, c_lo);
2372           __ sbbl(lreg_hi, c_hi);
2373           break;
2374         default:
2375           ShouldNotReachHere();
2376       }
2377 #endif // _LP64
2378 
2379     } else {
2380       ShouldNotReachHere();
2381     }
2382 
2383   } else if (left-&gt;is_single_xmm()) {
2384     assert(left == dest, &quot;left and dest must be equal&quot;);
2385     XMMRegister lreg = left-&gt;as_xmm_float_reg();
2386 
2387     if (right-&gt;is_single_xmm()) {
2388       XMMRegister rreg = right-&gt;as_xmm_float_reg();
2389       switch (code) {
2390         case lir_add: __ addss(lreg, rreg);  break;
2391         case lir_sub: __ subss(lreg, rreg);  break;
2392         case lir_mul_strictfp: // fall through
2393         case lir_mul: __ mulss(lreg, rreg);  break;
2394         case lir_div_strictfp: // fall through
2395         case lir_div: __ divss(lreg, rreg);  break;
2396         default: ShouldNotReachHere();
2397       }
2398     } else {
2399       Address raddr;
2400       if (right-&gt;is_single_stack()) {
2401         raddr = frame_map()-&gt;address_for_slot(right-&gt;single_stack_ix());
2402       } else if (right-&gt;is_constant()) {
2403         // hack for now
2404         raddr = __ as_Address(InternalAddress(float_constant(right-&gt;as_jfloat())));
2405       } else {
2406         ShouldNotReachHere();
2407       }
2408       switch (code) {
2409         case lir_add: __ addss(lreg, raddr);  break;
2410         case lir_sub: __ subss(lreg, raddr);  break;
2411         case lir_mul_strictfp: // fall through
2412         case lir_mul: __ mulss(lreg, raddr);  break;
2413         case lir_div_strictfp: // fall through
2414         case lir_div: __ divss(lreg, raddr);  break;
2415         default: ShouldNotReachHere();
2416       }
2417     }
2418 
2419   } else if (left-&gt;is_double_xmm()) {
2420     assert(left == dest, &quot;left and dest must be equal&quot;);
2421 
2422     XMMRegister lreg = left-&gt;as_xmm_double_reg();
2423     if (right-&gt;is_double_xmm()) {
2424       XMMRegister rreg = right-&gt;as_xmm_double_reg();
2425       switch (code) {
2426         case lir_add: __ addsd(lreg, rreg);  break;
2427         case lir_sub: __ subsd(lreg, rreg);  break;
2428         case lir_mul_strictfp: // fall through
2429         case lir_mul: __ mulsd(lreg, rreg);  break;
2430         case lir_div_strictfp: // fall through
2431         case lir_div: __ divsd(lreg, rreg);  break;
2432         default: ShouldNotReachHere();
2433       }
2434     } else {
2435       Address raddr;
2436       if (right-&gt;is_double_stack()) {
2437         raddr = frame_map()-&gt;address_for_slot(right-&gt;double_stack_ix());
2438       } else if (right-&gt;is_constant()) {
2439         // hack for now
2440         raddr = __ as_Address(InternalAddress(double_constant(right-&gt;as_jdouble())));
2441       } else {
2442         ShouldNotReachHere();
2443       }
2444       switch (code) {
2445         case lir_add: __ addsd(lreg, raddr);  break;
2446         case lir_sub: __ subsd(lreg, raddr);  break;
2447         case lir_mul_strictfp: // fall through
2448         case lir_mul: __ mulsd(lreg, raddr);  break;
2449         case lir_div_strictfp: // fall through
2450         case lir_div: __ divsd(lreg, raddr);  break;
2451         default: ShouldNotReachHere();
2452       }
2453     }
2454 
2455 #ifndef _LP64
2456   } else if (left-&gt;is_single_fpu()) {
2457     assert(dest-&gt;is_single_fpu(),  &quot;fpu stack allocation required&quot;);
2458 
2459     if (right-&gt;is_single_fpu()) {
2460       arith_fpu_implementation(code, left-&gt;fpu_regnr(), right-&gt;fpu_regnr(), dest-&gt;fpu_regnr(), pop_fpu_stack);
2461 
2462     } else {
2463       assert(left-&gt;fpu_regnr() == 0, &quot;left must be on TOS&quot;);
2464       assert(dest-&gt;fpu_regnr() == 0, &quot;dest must be on TOS&quot;);
2465 
2466       Address raddr;
2467       if (right-&gt;is_single_stack()) {
2468         raddr = frame_map()-&gt;address_for_slot(right-&gt;single_stack_ix());
2469       } else if (right-&gt;is_constant()) {
2470         address const_addr = float_constant(right-&gt;as_jfloat());
2471         assert(const_addr != NULL, &quot;incorrect float/double constant maintainance&quot;);
2472         // hack for now
2473         raddr = __ as_Address(InternalAddress(const_addr));
2474       } else {
2475         ShouldNotReachHere();
2476       }
2477 
2478       switch (code) {
2479         case lir_add: __ fadd_s(raddr); break;
2480         case lir_sub: __ fsub_s(raddr); break;
2481         case lir_mul_strictfp: // fall through
2482         case lir_mul: __ fmul_s(raddr); break;
2483         case lir_div_strictfp: // fall through
2484         case lir_div: __ fdiv_s(raddr); break;
2485         default:      ShouldNotReachHere();
2486       }
2487     }
2488 
2489   } else if (left-&gt;is_double_fpu()) {
2490     assert(dest-&gt;is_double_fpu(),  &quot;fpu stack allocation required&quot;);
2491 
2492     if (code == lir_mul_strictfp || code == lir_div_strictfp) {
2493       // Double values require special handling for strictfp mul/div on x86
2494       __ fld_x(ExternalAddress(StubRoutines::addr_fpu_subnormal_bias1()));
2495       __ fmulp(left-&gt;fpu_regnrLo() + 1);
2496     }
2497 
2498     if (right-&gt;is_double_fpu()) {
2499       arith_fpu_implementation(code, left-&gt;fpu_regnrLo(), right-&gt;fpu_regnrLo(), dest-&gt;fpu_regnrLo(), pop_fpu_stack);
2500 
2501     } else {
2502       assert(left-&gt;fpu_regnrLo() == 0, &quot;left must be on TOS&quot;);
2503       assert(dest-&gt;fpu_regnrLo() == 0, &quot;dest must be on TOS&quot;);
2504 
2505       Address raddr;
2506       if (right-&gt;is_double_stack()) {
2507         raddr = frame_map()-&gt;address_for_slot(right-&gt;double_stack_ix());
2508       } else if (right-&gt;is_constant()) {
2509         // hack for now
2510         raddr = __ as_Address(InternalAddress(double_constant(right-&gt;as_jdouble())));
2511       } else {
2512         ShouldNotReachHere();
2513       }
2514 
2515       switch (code) {
2516         case lir_add: __ fadd_d(raddr); break;
2517         case lir_sub: __ fsub_d(raddr); break;
2518         case lir_mul_strictfp: // fall through
2519         case lir_mul: __ fmul_d(raddr); break;
2520         case lir_div_strictfp: // fall through
2521         case lir_div: __ fdiv_d(raddr); break;
2522         default: ShouldNotReachHere();
2523       }
2524     }
2525 
2526     if (code == lir_mul_strictfp || code == lir_div_strictfp) {
2527       // Double values require special handling for strictfp mul/div on x86
2528       __ fld_x(ExternalAddress(StubRoutines::addr_fpu_subnormal_bias2()));
2529       __ fmulp(dest-&gt;fpu_regnrLo() + 1);
2530     }
2531 #endif // !_LP64
2532 
2533   } else if (left-&gt;is_single_stack() || left-&gt;is_address()) {
2534     assert(left == dest, &quot;left and dest must be equal&quot;);
2535 
2536     Address laddr;
2537     if (left-&gt;is_single_stack()) {
2538       laddr = frame_map()-&gt;address_for_slot(left-&gt;single_stack_ix());
2539     } else if (left-&gt;is_address()) {
2540       laddr = as_Address(left-&gt;as_address_ptr());
2541     } else {
2542       ShouldNotReachHere();
2543     }
2544 
2545     if (right-&gt;is_single_cpu()) {
2546       Register rreg = right-&gt;as_register();
2547       switch (code) {
2548         case lir_add: __ addl(laddr, rreg); break;
2549         case lir_sub: __ subl(laddr, rreg); break;
2550         default:      ShouldNotReachHere();
2551       }
2552     } else if (right-&gt;is_constant()) {
2553       jint c = right-&gt;as_constant_ptr()-&gt;as_jint();
2554       switch (code) {
2555         case lir_add: {
2556           __ incrementl(laddr, c);
2557           break;
2558         }
2559         case lir_sub: {
2560           __ decrementl(laddr, c);
2561           break;
2562         }
2563         default: ShouldNotReachHere();
2564       }
2565     } else {
2566       ShouldNotReachHere();
2567     }
2568 
2569   } else {
2570     ShouldNotReachHere();
2571   }
2572 }
2573 
2574 #ifndef _LP64
2575 void LIR_Assembler::arith_fpu_implementation(LIR_Code code, int left_index, int right_index, int dest_index, bool pop_fpu_stack) {
2576   assert(pop_fpu_stack  || (left_index     == dest_index || right_index     == dest_index), &quot;invalid LIR&quot;);
2577   assert(!pop_fpu_stack || (left_index - 1 == dest_index || right_index - 1 == dest_index), &quot;invalid LIR&quot;);
2578   assert(left_index == 0 || right_index == 0, &quot;either must be on top of stack&quot;);
2579 
2580   bool left_is_tos = (left_index == 0);
2581   bool dest_is_tos = (dest_index == 0);
2582   int non_tos_index = (left_is_tos ? right_index : left_index);
2583 
2584   switch (code) {
2585     case lir_add:
2586       if (pop_fpu_stack)       __ faddp(non_tos_index);
2587       else if (dest_is_tos)    __ fadd (non_tos_index);
2588       else                     __ fadda(non_tos_index);
2589       break;
2590 
2591     case lir_sub:
2592       if (left_is_tos) {
2593         if (pop_fpu_stack)     __ fsubrp(non_tos_index);
2594         else if (dest_is_tos)  __ fsub  (non_tos_index);
2595         else                   __ fsubra(non_tos_index);
2596       } else {
2597         if (pop_fpu_stack)     __ fsubp (non_tos_index);
2598         else if (dest_is_tos)  __ fsubr (non_tos_index);
2599         else                   __ fsuba (non_tos_index);
2600       }
2601       break;
2602 
2603     case lir_mul_strictfp: // fall through
2604     case lir_mul:
2605       if (pop_fpu_stack)       __ fmulp(non_tos_index);
2606       else if (dest_is_tos)    __ fmul (non_tos_index);
2607       else                     __ fmula(non_tos_index);
2608       break;
2609 
2610     case lir_div_strictfp: // fall through
2611     case lir_div:
2612       if (left_is_tos) {
2613         if (pop_fpu_stack)     __ fdivrp(non_tos_index);
2614         else if (dest_is_tos)  __ fdiv  (non_tos_index);
2615         else                   __ fdivra(non_tos_index);
2616       } else {
2617         if (pop_fpu_stack)     __ fdivp (non_tos_index);
2618         else if (dest_is_tos)  __ fdivr (non_tos_index);
2619         else                   __ fdiva (non_tos_index);
2620       }
2621       break;
2622 
2623     case lir_rem:
2624       assert(left_is_tos &amp;&amp; dest_is_tos &amp;&amp; right_index == 1, &quot;must be guaranteed by FPU stack allocation&quot;);
2625       __ fremr(noreg);
2626       break;
2627 
2628     default:
2629       ShouldNotReachHere();
2630   }
2631 }
2632 #endif // _LP64
2633 
2634 
2635 void LIR_Assembler::intrinsic_op(LIR_Code code, LIR_Opr value, LIR_Opr tmp, LIR_Opr dest, LIR_Op* op) {
2636   if (value-&gt;is_double_xmm()) {
2637     switch(code) {
2638       case lir_abs :
2639         {
2640 #ifdef _LP64
2641           if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512vl()) {
2642             assert(tmp-&gt;is_valid(), &quot;need temporary&quot;);
2643             __ vpandn(dest-&gt;as_xmm_double_reg(), tmp-&gt;as_xmm_double_reg(), value-&gt;as_xmm_double_reg(), 2);
2644           } else
2645 #endif
2646           {
2647             if (dest-&gt;as_xmm_double_reg() != value-&gt;as_xmm_double_reg()) {
2648               __ movdbl(dest-&gt;as_xmm_double_reg(), value-&gt;as_xmm_double_reg());
2649             }
2650             assert(!tmp-&gt;is_valid(), &quot;do not need temporary&quot;);
2651             __ andpd(dest-&gt;as_xmm_double_reg(),
2652                      ExternalAddress((address)double_signmask_pool));
2653           }
2654         }
2655         break;
2656 
2657       case lir_sqrt: __ sqrtsd(dest-&gt;as_xmm_double_reg(), value-&gt;as_xmm_double_reg()); break;
2658       // all other intrinsics are not available in the SSE instruction set, so FPU is used
2659       default      : ShouldNotReachHere();
2660     }
2661 
2662 #ifndef _LP64
2663   } else if (value-&gt;is_double_fpu()) {
2664     assert(value-&gt;fpu_regnrLo() == 0 &amp;&amp; dest-&gt;fpu_regnrLo() == 0, &quot;both must be on TOS&quot;);
2665     switch(code) {
2666       case lir_abs   : __ fabs() ; break;
2667       case lir_sqrt  : __ fsqrt(); break;
2668       default      : ShouldNotReachHere();
2669     }
2670 #endif // !_LP64
2671   } else {
2672     Unimplemented();
2673   }
2674 }
2675 
2676 void LIR_Assembler::logic_op(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst) {
2677   // assert(left-&gt;destroys_register(), &quot;check&quot;);
2678   if (left-&gt;is_single_cpu()) {
2679     Register reg = left-&gt;as_register();
2680     if (right-&gt;is_constant()) {
2681       int val = right-&gt;as_constant_ptr()-&gt;as_jint();
2682       switch (code) {
2683         case lir_logic_and: __ andl (reg, val); break;
2684         case lir_logic_or:  __ orl  (reg, val); break;
2685         case lir_logic_xor: __ xorl (reg, val); break;
2686         default: ShouldNotReachHere();
2687       }
2688     } else if (right-&gt;is_stack()) {
2689       // added support for stack operands
2690       Address raddr = frame_map()-&gt;address_for_slot(right-&gt;single_stack_ix());
2691       switch (code) {
2692         case lir_logic_and: __ andl (reg, raddr); break;
2693         case lir_logic_or:  __ orl  (reg, raddr); break;
2694         case lir_logic_xor: __ xorl (reg, raddr); break;
2695         default: ShouldNotReachHere();
2696       }
2697     } else {
2698       Register rright = right-&gt;as_register();
2699       switch (code) {
2700         case lir_logic_and: __ andptr (reg, rright); break;
2701         case lir_logic_or : __ orptr  (reg, rright); break;
2702         case lir_logic_xor: __ xorptr (reg, rright); break;
2703         default: ShouldNotReachHere();
2704       }
2705     }
2706     move_regs(reg, dst-&gt;as_register());
2707   } else {
2708     Register l_lo = left-&gt;as_register_lo();
2709     Register l_hi = left-&gt;as_register_hi();
2710     if (right-&gt;is_constant()) {
2711 #ifdef _LP64
2712       __ mov64(rscratch1, right-&gt;as_constant_ptr()-&gt;as_jlong());
2713       switch (code) {
2714         case lir_logic_and:
2715           __ andq(l_lo, rscratch1);
2716           break;
2717         case lir_logic_or:
2718           __ orq(l_lo, rscratch1);
2719           break;
2720         case lir_logic_xor:
2721           __ xorq(l_lo, rscratch1);
2722           break;
2723         default: ShouldNotReachHere();
2724       }
2725 #else
2726       int r_lo = right-&gt;as_constant_ptr()-&gt;as_jint_lo();
2727       int r_hi = right-&gt;as_constant_ptr()-&gt;as_jint_hi();
2728       switch (code) {
2729         case lir_logic_and:
2730           __ andl(l_lo, r_lo);
2731           __ andl(l_hi, r_hi);
2732           break;
2733         case lir_logic_or:
2734           __ orl(l_lo, r_lo);
2735           __ orl(l_hi, r_hi);
2736           break;
2737         case lir_logic_xor:
2738           __ xorl(l_lo, r_lo);
2739           __ xorl(l_hi, r_hi);
2740           break;
2741         default: ShouldNotReachHere();
2742       }
2743 #endif // _LP64
2744     } else {
2745 #ifdef _LP64
2746       Register r_lo;
2747       if (is_reference_type(right-&gt;type())) {
2748         r_lo = right-&gt;as_register();
2749       } else {
2750         r_lo = right-&gt;as_register_lo();
2751       }
2752 #else
2753       Register r_lo = right-&gt;as_register_lo();
2754       Register r_hi = right-&gt;as_register_hi();
2755       assert(l_lo != r_hi, &quot;overwriting registers&quot;);
2756 #endif
2757       switch (code) {
2758         case lir_logic_and:
2759           __ andptr(l_lo, r_lo);
2760           NOT_LP64(__ andptr(l_hi, r_hi);)
2761           break;
2762         case lir_logic_or:
2763           __ orptr(l_lo, r_lo);
2764           NOT_LP64(__ orptr(l_hi, r_hi);)
2765           break;
2766         case lir_logic_xor:
2767           __ xorptr(l_lo, r_lo);
2768           NOT_LP64(__ xorptr(l_hi, r_hi);)
2769           break;
2770         default: ShouldNotReachHere();
2771       }
2772     }
2773 
2774     Register dst_lo = dst-&gt;as_register_lo();
2775     Register dst_hi = dst-&gt;as_register_hi();
2776 
2777 #ifdef _LP64
2778     move_regs(l_lo, dst_lo);
2779 #else
2780     if (dst_lo == l_hi) {
2781       assert(dst_hi != l_lo, &quot;overwriting registers&quot;);
2782       move_regs(l_hi, dst_hi);
2783       move_regs(l_lo, dst_lo);
2784     } else {
2785       assert(dst_lo != l_hi, &quot;overwriting registers&quot;);
2786       move_regs(l_lo, dst_lo);
2787       move_regs(l_hi, dst_hi);
2788     }
2789 #endif // _LP64
2790   }
2791 }
2792 
2793 
2794 // we assume that rax, and rdx can be overwritten
2795 void LIR_Assembler::arithmetic_idiv(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr temp, LIR_Opr result, CodeEmitInfo* info) {
2796 
2797   assert(left-&gt;is_single_cpu(),   &quot;left must be register&quot;);
2798   assert(right-&gt;is_single_cpu() || right-&gt;is_constant(),  &quot;right must be register or constant&quot;);
2799   assert(result-&gt;is_single_cpu(), &quot;result must be register&quot;);
2800 
2801   //  assert(left-&gt;destroys_register(), &quot;check&quot;);
2802   //  assert(right-&gt;destroys_register(), &quot;check&quot;);
2803 
2804   Register lreg = left-&gt;as_register();
2805   Register dreg = result-&gt;as_register();
2806 
2807   if (right-&gt;is_constant()) {
2808     jint divisor = right-&gt;as_constant_ptr()-&gt;as_jint();
2809     assert(divisor &gt; 0 &amp;&amp; is_power_of_2(divisor), &quot;must be&quot;);
2810     if (code == lir_idiv) {
2811       assert(lreg == rax, &quot;must be rax,&quot;);
2812       assert(temp-&gt;as_register() == rdx, &quot;tmp register must be rdx&quot;);
2813       __ cdql(); // sign extend into rdx:rax
2814       if (divisor == 2) {
2815         __ subl(lreg, rdx);
2816       } else {
2817         __ andl(rdx, divisor - 1);
2818         __ addl(lreg, rdx);
2819       }
2820       __ sarl(lreg, log2_jint(divisor));
2821       move_regs(lreg, dreg);
2822     } else if (code == lir_irem) {
2823       Label done;
2824       __ mov(dreg, lreg);
2825       __ andl(dreg, 0x80000000 | (divisor - 1));
2826       __ jcc(Assembler::positive, done);
2827       __ decrement(dreg);
2828       __ orl(dreg, ~(divisor - 1));
2829       __ increment(dreg);
2830       __ bind(done);
2831     } else {
2832       ShouldNotReachHere();
2833     }
2834   } else {
2835     Register rreg = right-&gt;as_register();
2836     assert(lreg == rax, &quot;left register must be rax,&quot;);
2837     assert(rreg != rdx, &quot;right register must not be rdx&quot;);
2838     assert(temp-&gt;as_register() == rdx, &quot;tmp register must be rdx&quot;);
2839 
2840     move_regs(lreg, rax);
2841 
2842     int idivl_offset = __ corrected_idivl(rreg);
2843     if (ImplicitDiv0Checks) {
2844       add_debug_info_for_div0(idivl_offset, info);
2845     }
2846     if (code == lir_irem) {
2847       move_regs(rdx, dreg); // result is in rdx
2848     } else {
2849       move_regs(rax, dreg);
2850     }
2851   }
2852 }
2853 
2854 
2855 void LIR_Assembler::comp_op(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Op2* op) {
2856   if (opr1-&gt;is_single_cpu()) {
2857     Register reg1 = opr1-&gt;as_register();
2858     if (opr2-&gt;is_single_cpu()) {
2859       // cpu register - cpu register
2860       if (is_reference_type(opr1-&gt;type())) {
2861         __ cmpoop(reg1, opr2-&gt;as_register());
2862       } else {
2863         assert(!is_reference_type(opr2-&gt;type()), &quot;cmp int, oop?&quot;);
2864         __ cmpl(reg1, opr2-&gt;as_register());
2865       }
2866     } else if (opr2-&gt;is_stack()) {
2867       // cpu register - stack
2868       if (is_reference_type(opr1-&gt;type())) {
2869         __ cmpoop(reg1, frame_map()-&gt;address_for_slot(opr2-&gt;single_stack_ix()));
2870       } else {
2871         __ cmpl(reg1, frame_map()-&gt;address_for_slot(opr2-&gt;single_stack_ix()));
2872       }
2873     } else if (opr2-&gt;is_constant()) {
2874       // cpu register - constant
2875       LIR_Const* c = opr2-&gt;as_constant_ptr();
2876       if (c-&gt;type() == T_INT) {
2877         __ cmpl(reg1, c-&gt;as_jint());
2878       } else if (c-&gt;type() == T_METADATA) {
2879         // All we need for now is a comparison with NULL for equality.
2880         assert(condition == lir_cond_equal || condition == lir_cond_notEqual, &quot;oops&quot;);
2881         Metadata* m = c-&gt;as_metadata();
2882         if (m == NULL) {
2883           __ cmpptr(reg1, (int32_t)0);
2884         } else {
2885           ShouldNotReachHere();
2886         }
2887       } else if (is_reference_type(c-&gt;type())) {
2888         // In 64bit oops are single register
2889         jobject o = c-&gt;as_jobject();
2890         if (o == NULL) {
2891           __ cmpptr(reg1, (int32_t)NULL_WORD);
2892         } else {
2893           __ cmpoop(reg1, o);
2894         }
2895       } else {
2896         fatal(&quot;unexpected type: %s&quot;, basictype_to_str(c-&gt;type()));
2897       }
2898       // cpu register - address
2899     } else if (opr2-&gt;is_address()) {
2900       if (op-&gt;info() != NULL) {
2901         add_debug_info_for_null_check_here(op-&gt;info());
2902       }
2903       __ cmpl(reg1, as_Address(opr2-&gt;as_address_ptr()));
2904     } else {
2905       ShouldNotReachHere();
2906     }
2907 
2908   } else if(opr1-&gt;is_double_cpu()) {
2909     Register xlo = opr1-&gt;as_register_lo();
2910     Register xhi = opr1-&gt;as_register_hi();
2911     if (opr2-&gt;is_double_cpu()) {
2912 #ifdef _LP64
2913       __ cmpptr(xlo, opr2-&gt;as_register_lo());
2914 #else
2915       // cpu register - cpu register
2916       Register ylo = opr2-&gt;as_register_lo();
2917       Register yhi = opr2-&gt;as_register_hi();
2918       __ subl(xlo, ylo);
2919       __ sbbl(xhi, yhi);
2920       if (condition == lir_cond_equal || condition == lir_cond_notEqual) {
2921         __ orl(xhi, xlo);
2922       }
2923 #endif // _LP64
2924     } else if (opr2-&gt;is_constant()) {
2925       // cpu register - constant 0
2926       assert(opr2-&gt;as_jlong() == (jlong)0, &quot;only handles zero&quot;);
2927 #ifdef _LP64
2928       __ cmpptr(xlo, (int32_t)opr2-&gt;as_jlong());
2929 #else
2930       assert(condition == lir_cond_equal || condition == lir_cond_notEqual, &quot;only handles equals case&quot;);
2931       __ orl(xhi, xlo);
2932 #endif // _LP64
2933     } else {
2934       ShouldNotReachHere();
2935     }
2936 
2937   } else if (opr1-&gt;is_single_xmm()) {
2938     XMMRegister reg1 = opr1-&gt;as_xmm_float_reg();
2939     if (opr2-&gt;is_single_xmm()) {
2940       // xmm register - xmm register
2941       __ ucomiss(reg1, opr2-&gt;as_xmm_float_reg());
2942     } else if (opr2-&gt;is_stack()) {
2943       // xmm register - stack
2944       __ ucomiss(reg1, frame_map()-&gt;address_for_slot(opr2-&gt;single_stack_ix()));
2945     } else if (opr2-&gt;is_constant()) {
2946       // xmm register - constant
2947       __ ucomiss(reg1, InternalAddress(float_constant(opr2-&gt;as_jfloat())));
2948     } else if (opr2-&gt;is_address()) {
2949       // xmm register - address
2950       if (op-&gt;info() != NULL) {
2951         add_debug_info_for_null_check_here(op-&gt;info());
2952       }
2953       __ ucomiss(reg1, as_Address(opr2-&gt;as_address_ptr()));
2954     } else {
2955       ShouldNotReachHere();
2956     }
2957 
2958   } else if (opr1-&gt;is_double_xmm()) {
2959     XMMRegister reg1 = opr1-&gt;as_xmm_double_reg();
2960     if (opr2-&gt;is_double_xmm()) {
2961       // xmm register - xmm register
2962       __ ucomisd(reg1, opr2-&gt;as_xmm_double_reg());
2963     } else if (opr2-&gt;is_stack()) {
2964       // xmm register - stack
2965       __ ucomisd(reg1, frame_map()-&gt;address_for_slot(opr2-&gt;double_stack_ix()));
2966     } else if (opr2-&gt;is_constant()) {
2967       // xmm register - constant
2968       __ ucomisd(reg1, InternalAddress(double_constant(opr2-&gt;as_jdouble())));
2969     } else if (opr2-&gt;is_address()) {
2970       // xmm register - address
2971       if (op-&gt;info() != NULL) {
2972         add_debug_info_for_null_check_here(op-&gt;info());
2973       }
2974       __ ucomisd(reg1, as_Address(opr2-&gt;pointer()-&gt;as_address()));
2975     } else {
2976       ShouldNotReachHere();
2977     }
2978 
2979 #ifndef _LP64
2980   } else if(opr1-&gt;is_single_fpu() || opr1-&gt;is_double_fpu()) {
2981     assert(opr1-&gt;is_fpu_register() &amp;&amp; opr1-&gt;fpu() == 0, &quot;currently left-hand side must be on TOS (relax this restriction)&quot;);
2982     assert(opr2-&gt;is_fpu_register(), &quot;both must be registers&quot;);
2983     __ fcmp(noreg, opr2-&gt;fpu(), op-&gt;fpu_pop_count() &gt; 0, op-&gt;fpu_pop_count() &gt; 1);
2984 #endif // LP64
2985 
2986   } else if (opr1-&gt;is_address() &amp;&amp; opr2-&gt;is_constant()) {
2987     LIR_Const* c = opr2-&gt;as_constant_ptr();
2988 #ifdef _LP64
2989     if (is_reference_type(c-&gt;type())) {
2990       assert(condition == lir_cond_equal || condition == lir_cond_notEqual, &quot;need to reverse&quot;);
2991       __ movoop(rscratch1, c-&gt;as_jobject());
2992     }
2993 #endif // LP64
2994     if (op-&gt;info() != NULL) {
2995       add_debug_info_for_null_check_here(op-&gt;info());
2996     }
2997     // special case: address - constant
2998     LIR_Address* addr = opr1-&gt;as_address_ptr();
2999     if (c-&gt;type() == T_INT) {
3000       __ cmpl(as_Address(addr), c-&gt;as_jint());
3001     } else if (is_reference_type(c-&gt;type())) {
3002 #ifdef _LP64
3003       // %%% Make this explode if addr isn&#39;t reachable until we figure out a
3004       // better strategy by giving noreg as the temp for as_Address
3005       __ cmpoop(rscratch1, as_Address(addr, noreg));
3006 #else
3007       __ cmpoop(as_Address(addr), c-&gt;as_jobject());
3008 #endif // _LP64
3009     } else {
3010       ShouldNotReachHere();
3011     }
3012 
3013   } else {
3014     ShouldNotReachHere();
3015   }
3016 }
3017 
3018 void LIR_Assembler::comp_fl2i(LIR_Code code, LIR_Opr left, LIR_Opr right, LIR_Opr dst, LIR_Op2* op) {
3019   if (code == lir_cmp_fd2i || code == lir_ucmp_fd2i) {
3020     if (left-&gt;is_single_xmm()) {
3021       assert(right-&gt;is_single_xmm(), &quot;must match&quot;);
3022       __ cmpss2int(left-&gt;as_xmm_float_reg(), right-&gt;as_xmm_float_reg(), dst-&gt;as_register(), code == lir_ucmp_fd2i);
3023     } else if (left-&gt;is_double_xmm()) {
3024       assert(right-&gt;is_double_xmm(), &quot;must match&quot;);
3025       __ cmpsd2int(left-&gt;as_xmm_double_reg(), right-&gt;as_xmm_double_reg(), dst-&gt;as_register(), code == lir_ucmp_fd2i);
3026 
3027     } else {
3028 #ifdef _LP64
3029       ShouldNotReachHere();
3030 #else
3031       assert(left-&gt;is_single_fpu() || left-&gt;is_double_fpu(), &quot;must be&quot;);
3032       assert(right-&gt;is_single_fpu() || right-&gt;is_double_fpu(), &quot;must match&quot;);
3033 
3034       assert(left-&gt;fpu() == 0, &quot;left must be on TOS&quot;);
3035       __ fcmp2int(dst-&gt;as_register(), code == lir_ucmp_fd2i, right-&gt;fpu(),
3036                   op-&gt;fpu_pop_count() &gt; 0, op-&gt;fpu_pop_count() &gt; 1);
3037 #endif // LP64
3038     }
3039   } else {
3040     assert(code == lir_cmp_l2i, &quot;check&quot;);
3041 #ifdef _LP64
3042     Label done;
3043     Register dest = dst-&gt;as_register();
3044     __ cmpptr(left-&gt;as_register_lo(), right-&gt;as_register_lo());
3045     __ movl(dest, -1);
3046     __ jccb(Assembler::less, done);
3047     __ set_byte_if_not_zero(dest);
3048     __ movzbl(dest, dest);
3049     __ bind(done);
3050 #else
3051     __ lcmp2int(left-&gt;as_register_hi(),
3052                 left-&gt;as_register_lo(),
3053                 right-&gt;as_register_hi(),
3054                 right-&gt;as_register_lo());
3055     move_regs(left-&gt;as_register_hi(), dst-&gt;as_register());
3056 #endif // _LP64
3057   }
3058 }
3059 
3060 
3061 void LIR_Assembler::align_call(LIR_Code code) {
3062   // make sure that the displacement word of the call ends up word aligned
3063   int offset = __ offset();
3064   switch (code) {
3065   case lir_static_call:
3066   case lir_optvirtual_call:
3067   case lir_dynamic_call:
3068     offset += NativeCall::displacement_offset;
3069     break;
3070   case lir_icvirtual_call:
3071     offset += NativeCall::displacement_offset + NativeMovConstReg::instruction_size;
3072     break;
3073   case lir_virtual_call:  // currently, sparc-specific for niagara
3074   default: ShouldNotReachHere();
3075   }
3076   __ align(BytesPerWord, offset);
3077 }
3078 
3079 
3080 void LIR_Assembler::call(LIR_OpJavaCall* op, relocInfo::relocType rtype) {
3081   assert((__ offset() + NativeCall::displacement_offset) % BytesPerWord == 0,
3082          &quot;must be aligned&quot;);
3083   __ call(AddressLiteral(op-&gt;addr(), rtype));
3084   add_call_info(code_offset(), op-&gt;info(), op-&gt;maybe_return_as_fields());
3085 }
3086 
3087 
3088 void LIR_Assembler::ic_call(LIR_OpJavaCall* op) {
3089   __ ic_call(op-&gt;addr());
3090   add_call_info(code_offset(), op-&gt;info(), op-&gt;maybe_return_as_fields());
3091   assert((__ offset() - NativeCall::instruction_size + NativeCall::displacement_offset) % BytesPerWord == 0,
3092          &quot;must be aligned&quot;);
3093 }
3094 
3095 
3096 /* Currently, vtable-dispatch is only enabled for sparc platforms */
3097 void LIR_Assembler::vtable_call(LIR_OpJavaCall* op) {
3098   ShouldNotReachHere();
3099 }
3100 
3101 
3102 void LIR_Assembler::emit_static_call_stub() {
3103   address call_pc = __ pc();
3104   address stub = __ start_a_stub(call_stub_size());
3105   if (stub == NULL) {
3106     bailout(&quot;static call stub overflow&quot;);
3107     return;
3108   }
3109 
3110   int start = __ offset();
3111 
3112   // make sure that the displacement word of the call ends up word aligned
3113   __ align(BytesPerWord, __ offset() + NativeMovConstReg::instruction_size + NativeCall::displacement_offset);
3114   __ relocate(static_stub_Relocation::spec(call_pc, false /* is_aot */));
3115   __ mov_metadata(rbx, (Metadata*)NULL);
3116   // must be set to -1 at code generation time
3117   assert(((__ offset() + 1) % BytesPerWord) == 0, &quot;must be aligned&quot;);
3118   // On 64bit this will die since it will take a movq &amp; jmp, must be only a jmp
3119   __ jump(RuntimeAddress(__ pc()));
3120 
3121   if (UseAOT) {
3122     // Trampoline to aot code
3123     __ relocate(static_stub_Relocation::spec(call_pc, true /* is_aot */));
3124 #ifdef _LP64
3125     __ mov64(rax, CONST64(0));  // address is zapped till fixup time.
3126 #else
3127     __ movl(rax, 0xdeadffff);  // address is zapped till fixup time.
3128 #endif
3129     __ jmp(rax);
3130   }
3131   assert(__ offset() - start &lt;= call_stub_size(), &quot;stub too big&quot;);
3132   __ end_a_stub();
3133 }
3134 
3135 
3136 void LIR_Assembler::throw_op(LIR_Opr exceptionPC, LIR_Opr exceptionOop, CodeEmitInfo* info) {
3137   assert(exceptionOop-&gt;as_register() == rax, &quot;must match&quot;);
3138   assert(exceptionPC-&gt;as_register() == rdx, &quot;must match&quot;);
3139 
3140   // exception object is not added to oop map by LinearScan
3141   // (LinearScan assumes that no oops are in fixed registers)
3142   info-&gt;add_register_oop(exceptionOop);
3143   Runtime1::StubID unwind_id;
3144 
3145   // get current pc information
3146   // pc is only needed if the method has an exception handler, the unwind code does not need it.
3147   int pc_for_athrow_offset = __ offset();
3148   InternalAddress pc_for_athrow(__ pc());
3149   __ lea(exceptionPC-&gt;as_register(), pc_for_athrow);
3150   add_call_info(pc_for_athrow_offset, info); // for exception handler
3151 
3152   __ verify_not_null_oop(rax);
3153   // search an exception handler (rax: exception oop, rdx: throwing pc)
3154   if (compilation()-&gt;has_fpu_code()) {
3155     unwind_id = Runtime1::handle_exception_id;
3156   } else {
3157     unwind_id = Runtime1::handle_exception_nofpu_id;
3158   }
3159   __ call(RuntimeAddress(Runtime1::entry_for(unwind_id)));
3160 
3161   // enough room for two byte trap
3162   __ nop();
3163 }
3164 
3165 
3166 void LIR_Assembler::unwind_op(LIR_Opr exceptionOop) {
3167   assert(exceptionOop-&gt;as_register() == rax, &quot;must match&quot;);
3168 
3169   __ jmp(_unwind_handler_entry);
3170 }
3171 
3172 
3173 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, LIR_Opr count, LIR_Opr dest, LIR_Opr tmp) {
3174 
3175   // optimized version for linear scan:
3176   // * count must be already in ECX (guaranteed by LinearScan)
3177   // * left and dest must be equal
3178   // * tmp must be unused
3179   assert(count-&gt;as_register() == SHIFT_count, &quot;count must be in ECX&quot;);
3180   assert(left == dest, &quot;left and dest must be equal&quot;);
3181   assert(tmp-&gt;is_illegal(), &quot;wasting a register if tmp is allocated&quot;);
3182 
3183   if (left-&gt;is_single_cpu()) {
3184     Register value = left-&gt;as_register();
3185     assert(value != SHIFT_count, &quot;left cannot be ECX&quot;);
3186 
3187     switch (code) {
3188       case lir_shl:  __ shll(value); break;
3189       case lir_shr:  __ sarl(value); break;
3190       case lir_ushr: __ shrl(value); break;
3191       default: ShouldNotReachHere();
3192     }
3193   } else if (left-&gt;is_double_cpu()) {
3194     Register lo = left-&gt;as_register_lo();
3195     Register hi = left-&gt;as_register_hi();
3196     assert(lo != SHIFT_count &amp;&amp; hi != SHIFT_count, &quot;left cannot be ECX&quot;);
3197 #ifdef _LP64
3198     switch (code) {
3199       case lir_shl:  __ shlptr(lo);        break;
3200       case lir_shr:  __ sarptr(lo);        break;
3201       case lir_ushr: __ shrptr(lo);        break;
3202       default: ShouldNotReachHere();
3203     }
3204 #else
3205 
3206     switch (code) {
3207       case lir_shl:  __ lshl(hi, lo);        break;
3208       case lir_shr:  __ lshr(hi, lo, true);  break;
3209       case lir_ushr: __ lshr(hi, lo, false); break;
3210       default: ShouldNotReachHere();
3211     }
3212 #endif // LP64
3213   } else {
3214     ShouldNotReachHere();
3215   }
3216 }
3217 
3218 
3219 void LIR_Assembler::shift_op(LIR_Code code, LIR_Opr left, jint count, LIR_Opr dest) {
3220   if (dest-&gt;is_single_cpu()) {
3221     // first move left into dest so that left is not destroyed by the shift
3222     Register value = dest-&gt;as_register();
3223     count = count &amp; 0x1F; // Java spec
3224 
3225     move_regs(left-&gt;as_register(), value);
3226     switch (code) {
3227       case lir_shl:  __ shll(value, count); break;
3228       case lir_shr:  __ sarl(value, count); break;
3229       case lir_ushr: __ shrl(value, count); break;
3230       default: ShouldNotReachHere();
3231     }
3232   } else if (dest-&gt;is_double_cpu()) {
3233 #ifndef _LP64
3234     Unimplemented();
3235 #else
3236     // first move left into dest so that left is not destroyed by the shift
3237     Register value = dest-&gt;as_register_lo();
3238     count = count &amp; 0x1F; // Java spec
3239 
3240     move_regs(left-&gt;as_register_lo(), value);
3241     switch (code) {
3242       case lir_shl:  __ shlptr(value, count); break;
3243       case lir_shr:  __ sarptr(value, count); break;
3244       case lir_ushr: __ shrptr(value, count); break;
3245       default: ShouldNotReachHere();
3246     }
3247 #endif // _LP64
3248   } else {
3249     ShouldNotReachHere();
3250   }
3251 }
3252 
3253 
3254 void LIR_Assembler::store_parameter(Register r, int offset_from_rsp_in_words) {
3255   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
3256   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
3257   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
3258   __ movptr (Address(rsp, offset_from_rsp_in_bytes), r);
3259 }
3260 
3261 
3262 void LIR_Assembler::store_parameter(jint c,     int offset_from_rsp_in_words) {
3263   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
3264   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
3265   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
3266   __ movptr (Address(rsp, offset_from_rsp_in_bytes), c);
3267 }
3268 
3269 
3270 void LIR_Assembler::store_parameter(jobject o,  int offset_from_rsp_in_words) {
3271   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
3272   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
3273   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
3274   __ movoop (Address(rsp, offset_from_rsp_in_bytes), o);
3275 }
3276 
3277 
3278 void LIR_Assembler::store_parameter(Metadata* m,  int offset_from_rsp_in_words) {
3279   assert(offset_from_rsp_in_words &gt;= 0, &quot;invalid offset from rsp&quot;);
3280   int offset_from_rsp_in_bytes = offset_from_rsp_in_words * BytesPerWord;
3281   assert(offset_from_rsp_in_bytes &lt; frame_map()-&gt;reserved_argument_area_size(), &quot;invalid offset&quot;);
3282   __ mov_metadata(Address(rsp, offset_from_rsp_in_bytes), m);
3283 }
3284 
3285 
3286 void LIR_Assembler::arraycopy_valuetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {
3287   if (null_check) {
3288     __ testptr(obj, obj);
3289     __ jcc(Assembler::zero, *slow_path-&gt;entry());
3290   }
3291   __ load_storage_props(tmp, obj);
3292   if (is_dest) {
3293     // We also take slow path if it&#39;s a null_free destination array, just in case the source array
3294     // contains NULLs.
3295     __ testb(tmp, ArrayStorageProperties::flattened_value | ArrayStorageProperties::null_free_value);
3296   } else {
3297     __ testb(tmp, ArrayStorageProperties::flattened_value);
3298   }
3299   __ jcc(Assembler::notEqual, *slow_path-&gt;entry());
3300 }
3301 
3302 
3303 // This code replaces a call to arraycopy; no exception may
3304 // be thrown in this code, they must be thrown in the System.arraycopy
3305 // activation frame; we could save some checks if this would not be the case
3306 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
3307   ciArrayKlass* default_type = op-&gt;expected_type();
3308   Register src = op-&gt;src()-&gt;as_register();
3309   Register dst = op-&gt;dst()-&gt;as_register();
3310   Register src_pos = op-&gt;src_pos()-&gt;as_register();
3311   Register dst_pos = op-&gt;dst_pos()-&gt;as_register();
3312   Register length  = op-&gt;length()-&gt;as_register();
3313   Register tmp = op-&gt;tmp()-&gt;as_register();
3314 
3315   __ resolve(ACCESS_READ, src);
3316   __ resolve(ACCESS_WRITE, dst);
3317 
3318   CodeStub* stub = op-&gt;stub();
3319   int flags = op-&gt;flags();
3320   BasicType basic_type = default_type != NULL ? default_type-&gt;element_type()-&gt;basic_type() : T_ILLEGAL;
3321   if (is_reference_type(basic_type)) basic_type = T_OBJECT;
3322 
3323   if (flags &amp; LIR_OpArrayCopy::always_slow_path) {
3324     __ jmp(*stub-&gt;entry());
3325     __ bind(*stub-&gt;continuation());
3326     return;
3327   }
3328 
3329   if (flags &amp; LIR_OpArrayCopy::src_valuetype_check) {
3330     arraycopy_valuetype_check(src, tmp, stub, false, (flags &amp; LIR_OpArrayCopy::src_null_check));
3331   }
3332 
3333   if (flags &amp; LIR_OpArrayCopy::dst_valuetype_check) {
3334     arraycopy_valuetype_check(dst, tmp, stub, true, (flags &amp; LIR_OpArrayCopy::dst_null_check));
3335   }
3336 
3337   // if we don&#39;t know anything, just go through the generic arraycopy
3338   if (default_type == NULL) {
3339     // save outgoing arguments on stack in case call to System.arraycopy is needed
3340     // HACK ALERT. This code used to push the parameters in a hardwired fashion
3341     // for interpreter calling conventions. Now we have to do it in new style conventions.
3342     // For the moment until C1 gets the new register allocator I just force all the
3343     // args to the right place (except the register args) and then on the back side
3344     // reload the register args properly if we go slow path. Yuck
3345 
3346     // These are proper for the calling convention
3347     store_parameter(length, 2);
3348     store_parameter(dst_pos, 1);
3349     store_parameter(dst, 0);
3350 
3351     // these are just temporary placements until we need to reload
3352     store_parameter(src_pos, 3);
3353     store_parameter(src, 4);
3354     NOT_LP64(assert(src == rcx &amp;&amp; src_pos == rdx, &quot;mismatch in calling convention&quot;);)
3355 
3356     address copyfunc_addr = StubRoutines::generic_arraycopy();
3357     assert(copyfunc_addr != NULL, &quot;generic arraycopy stub required&quot;);
3358 
3359     // pass arguments: may push as this is not a safepoint; SP must be fix at each safepoint
3360 #ifdef _LP64
3361     // The arguments are in java calling convention so we can trivially shift them to C
3362     // convention
3363     assert_different_registers(c_rarg0, j_rarg1, j_rarg2, j_rarg3, j_rarg4);
3364     __ mov(c_rarg0, j_rarg0);
3365     assert_different_registers(c_rarg1, j_rarg2, j_rarg3, j_rarg4);
3366     __ mov(c_rarg1, j_rarg1);
3367     assert_different_registers(c_rarg2, j_rarg3, j_rarg4);
3368     __ mov(c_rarg2, j_rarg2);
3369     assert_different_registers(c_rarg3, j_rarg4);
3370     __ mov(c_rarg3, j_rarg3);
3371 #ifdef _WIN64
3372     // Allocate abi space for args but be sure to keep stack aligned
3373     __ subptr(rsp, 6*wordSize);
3374     store_parameter(j_rarg4, 4);
3375 #ifndef PRODUCT
3376     if (PrintC1Statistics) {
3377       __ incrementl(ExternalAddress((address)&amp;Runtime1::_generic_arraycopystub_cnt));
3378     }
3379 #endif
3380     __ call(RuntimeAddress(copyfunc_addr));
3381     __ addptr(rsp, 6*wordSize);
3382 #else
3383     __ mov(c_rarg4, j_rarg4);
3384 #ifndef PRODUCT
3385     if (PrintC1Statistics) {
3386       __ incrementl(ExternalAddress((address)&amp;Runtime1::_generic_arraycopystub_cnt));
3387     }
3388 #endif
3389     __ call(RuntimeAddress(copyfunc_addr));
3390 #endif // _WIN64
3391 #else
3392     __ push(length);
3393     __ push(dst_pos);
3394     __ push(dst);
3395     __ push(src_pos);
3396     __ push(src);
3397 
3398 #ifndef PRODUCT
3399     if (PrintC1Statistics) {
3400       __ incrementl(ExternalAddress((address)&amp;Runtime1::_generic_arraycopystub_cnt));
3401     }
3402 #endif
3403     __ call_VM_leaf(copyfunc_addr, 5); // removes pushed parameter from the stack
3404 
3405 #endif // _LP64
3406 
3407     __ cmpl(rax, 0);
3408     __ jcc(Assembler::equal, *stub-&gt;continuation());
3409 
3410     __ mov(tmp, rax);
3411     __ xorl(tmp, -1);
3412 
3413     // Reload values from the stack so they are where the stub
3414     // expects them.
3415     __ movptr   (dst,     Address(rsp, 0*BytesPerWord));
3416     __ movptr   (dst_pos, Address(rsp, 1*BytesPerWord));
3417     __ movptr   (length,  Address(rsp, 2*BytesPerWord));
3418     __ movptr   (src_pos, Address(rsp, 3*BytesPerWord));
3419     __ movptr   (src,     Address(rsp, 4*BytesPerWord));
3420 
3421     __ subl(length, tmp);
3422     __ addl(src_pos, tmp);
3423     __ addl(dst_pos, tmp);
3424     __ jmp(*stub-&gt;entry());
3425 
3426     __ bind(*stub-&gt;continuation());
3427     return;
3428   }
3429 
3430   assert(default_type != NULL &amp;&amp; default_type-&gt;is_array_klass() &amp;&amp; default_type-&gt;is_loaded(), &quot;must be true at this point&quot;);
3431 
3432   int elem_size = type2aelembytes(basic_type);
3433   Address::ScaleFactor scale;
3434 
3435   switch (elem_size) {
3436     case 1 :
3437       scale = Address::times_1;
3438       break;
3439     case 2 :
3440       scale = Address::times_2;
3441       break;
3442     case 4 :
3443       scale = Address::times_4;
3444       break;
3445     case 8 :
3446       scale = Address::times_8;
3447       break;
3448     default:
3449       scale = Address::no_scale;
3450       ShouldNotReachHere();
3451   }
3452 
3453   Address src_length_addr = Address(src, arrayOopDesc::length_offset_in_bytes());
3454   Address dst_length_addr = Address(dst, arrayOopDesc::length_offset_in_bytes());
3455   Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());
3456   Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());
3457 
3458   // length and pos&#39;s are all sign extended at this point on 64bit
3459 
3460   // test for NULL
3461   if (flags &amp; LIR_OpArrayCopy::src_null_check) {
3462     __ testptr(src, src);
3463     __ jcc(Assembler::zero, *stub-&gt;entry());
3464   }
3465   if (flags &amp; LIR_OpArrayCopy::dst_null_check) {
3466     __ testptr(dst, dst);
3467     __ jcc(Assembler::zero, *stub-&gt;entry());
3468   }
3469 
3470   // If the compiler was not able to prove that exact type of the source or the destination
3471   // of the arraycopy is an array type, check at runtime if the source or the destination is
3472   // an instance type.
3473   if (flags &amp; LIR_OpArrayCopy::type_check) {
3474     if (!(flags &amp; LIR_OpArrayCopy::dst_objarray)) {
3475       __ load_klass(tmp, dst);
3476       __ cmpl(Address(tmp, in_bytes(Klass::layout_helper_offset())), Klass::_lh_neutral_value);
3477       __ jcc(Assembler::greaterEqual, *stub-&gt;entry());
3478     }
3479 
3480     if (!(flags &amp; LIR_OpArrayCopy::src_objarray)) {
3481       __ load_klass(tmp, src);
3482       __ cmpl(Address(tmp, in_bytes(Klass::layout_helper_offset())), Klass::_lh_neutral_value);
3483       __ jcc(Assembler::greaterEqual, *stub-&gt;entry());
3484     }
3485   }
3486 
3487   // check if negative
3488   if (flags &amp; LIR_OpArrayCopy::src_pos_positive_check) {
3489     __ testl(src_pos, src_pos);
3490     __ jcc(Assembler::less, *stub-&gt;entry());
3491   }
3492   if (flags &amp; LIR_OpArrayCopy::dst_pos_positive_check) {
3493     __ testl(dst_pos, dst_pos);
3494     __ jcc(Assembler::less, *stub-&gt;entry());
3495   }
3496 
3497   if (flags &amp; LIR_OpArrayCopy::src_range_check) {
3498     __ lea(tmp, Address(src_pos, length, Address::times_1, 0));
3499     __ cmpl(tmp, src_length_addr);
3500     __ jcc(Assembler::above, *stub-&gt;entry());
3501   }
3502   if (flags &amp; LIR_OpArrayCopy::dst_range_check) {
3503     __ lea(tmp, Address(dst_pos, length, Address::times_1, 0));
3504     __ cmpl(tmp, dst_length_addr);
3505     __ jcc(Assembler::above, *stub-&gt;entry());
3506   }
3507 
3508   if (flags &amp; LIR_OpArrayCopy::length_positive_check) {
3509     __ testl(length, length);
3510     __ jcc(Assembler::less, *stub-&gt;entry());
3511   }
3512 
3513 #ifdef _LP64
3514   __ movl2ptr(src_pos, src_pos); //higher 32bits must be null
3515   __ movl2ptr(dst_pos, dst_pos); //higher 32bits must be null
3516 #endif
3517 
3518   if (flags &amp; LIR_OpArrayCopy::type_check) {
3519     // We don&#39;t know the array types are compatible
3520     if (basic_type != T_OBJECT) {
3521       // Simple test for basic type arrays
3522       if (UseCompressedClassPointers) {
3523         __ movl(tmp, src_klass_addr);
3524         __ cmpl(tmp, dst_klass_addr);
3525       } else {
3526         __ movptr(tmp, src_klass_addr);
3527         __ cmpptr(tmp, dst_klass_addr);
3528       }
3529       __ jcc(Assembler::notEqual, *stub-&gt;entry());
3530     } else {
3531       // For object arrays, if src is a sub class of dst then we can
3532       // safely do the copy.
3533       Label cont, slow;
3534 
3535       __ push(src);
3536       __ push(dst);
3537 
3538       __ load_klass(src, src);
3539       __ load_klass(dst, dst);
3540 
3541       __ check_klass_subtype_fast_path(src, dst, tmp, &amp;cont, &amp;slow, NULL);
3542 
3543       __ push(src);
3544       __ push(dst);
3545       __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::slow_subtype_check_id)));
3546       __ pop(dst);
3547       __ pop(src);
3548 
3549       __ cmpl(src, 0);
3550       __ jcc(Assembler::notEqual, cont);
3551 
3552       __ bind(slow);
3553       __ pop(dst);
3554       __ pop(src);
3555 
3556       address copyfunc_addr = StubRoutines::checkcast_arraycopy();
3557       if (copyfunc_addr != NULL) { // use stub if available
3558         // src is not a sub class of dst so we have to do a
3559         // per-element check.
3560 
3561         int mask = LIR_OpArrayCopy::src_objarray|LIR_OpArrayCopy::dst_objarray;
3562         if ((flags &amp; mask) != mask) {
3563           // Check that at least both of them object arrays.
3564           assert(flags &amp; mask, &quot;one of the two should be known to be an object array&quot;);
3565 
3566           if (!(flags &amp; LIR_OpArrayCopy::src_objarray)) {
3567             __ load_klass(tmp, src);
3568           } else if (!(flags &amp; LIR_OpArrayCopy::dst_objarray)) {
3569             __ load_klass(tmp, dst);
3570           }
3571           int lh_offset = in_bytes(Klass::layout_helper_offset());
3572           Address klass_lh_addr(tmp, lh_offset);
3573           jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
3574           __ cmpl(klass_lh_addr, objArray_lh);
3575           __ jcc(Assembler::notEqual, *stub-&gt;entry());
3576         }
3577 
3578        // Spill because stubs can use any register they like and it&#39;s
3579        // easier to restore just those that we care about.
3580        store_parameter(dst, 0);
3581        store_parameter(dst_pos, 1);
3582        store_parameter(length, 2);
3583        store_parameter(src_pos, 3);
3584        store_parameter(src, 4);
3585 
3586 #ifndef _LP64
3587         __ movptr(tmp, dst_klass_addr);
3588         __ movptr(tmp, Address(tmp, ObjArrayKlass::element_klass_offset()));
3589         __ push(tmp);
3590         __ movl(tmp, Address(tmp, Klass::super_check_offset_offset()));
3591         __ push(tmp);
3592         __ push(length);
3593         __ lea(tmp, Address(dst, dst_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));
3594         __ push(tmp);
3595         __ lea(tmp, Address(src, src_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));
3596         __ push(tmp);
3597 
3598         __ call_VM_leaf(copyfunc_addr, 5);
3599 #else
3600         __ movl2ptr(length, length); //higher 32bits must be null
3601 
3602         __ lea(c_rarg0, Address(src, src_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));
3603         assert_different_registers(c_rarg0, dst, dst_pos, length);
3604         __ lea(c_rarg1, Address(dst, dst_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));
3605         assert_different_registers(c_rarg1, dst, length);
3606 
3607         __ mov(c_rarg2, length);
3608         assert_different_registers(c_rarg2, dst);
3609 
3610 #ifdef _WIN64
3611         // Allocate abi space for args but be sure to keep stack aligned
3612         __ subptr(rsp, 6*wordSize);
3613         __ load_klass(c_rarg3, dst);
3614         __ movptr(c_rarg3, Address(c_rarg3, ObjArrayKlass::element_klass_offset()));
3615         store_parameter(c_rarg3, 4);
3616         __ movl(c_rarg3, Address(c_rarg3, Klass::super_check_offset_offset()));
3617         __ call(RuntimeAddress(copyfunc_addr));
3618         __ addptr(rsp, 6*wordSize);
3619 #else
3620         __ load_klass(c_rarg4, dst);
3621         __ movptr(c_rarg4, Address(c_rarg4, ObjArrayKlass::element_klass_offset()));
3622         __ movl(c_rarg3, Address(c_rarg4, Klass::super_check_offset_offset()));
3623         __ call(RuntimeAddress(copyfunc_addr));
3624 #endif
3625 
3626 #endif
3627 
3628 #ifndef PRODUCT
3629         if (PrintC1Statistics) {
3630           Label failed;
3631           __ testl(rax, rax);
3632           __ jcc(Assembler::notZero, failed);
3633           __ incrementl(ExternalAddress((address)&amp;Runtime1::_arraycopy_checkcast_cnt));
3634           __ bind(failed);
3635         }
3636 #endif
3637 
3638         __ testl(rax, rax);
3639         __ jcc(Assembler::zero, *stub-&gt;continuation());
3640 
3641 #ifndef PRODUCT
3642         if (PrintC1Statistics) {
3643           __ incrementl(ExternalAddress((address)&amp;Runtime1::_arraycopy_checkcast_attempt_cnt));
3644         }
3645 #endif
3646 
3647         __ mov(tmp, rax);
3648 
3649         __ xorl(tmp, -1);
3650 
3651         // Restore previously spilled arguments
3652         __ movptr   (dst,     Address(rsp, 0*BytesPerWord));
3653         __ movptr   (dst_pos, Address(rsp, 1*BytesPerWord));
3654         __ movptr   (length,  Address(rsp, 2*BytesPerWord));
3655         __ movptr   (src_pos, Address(rsp, 3*BytesPerWord));
3656         __ movptr   (src,     Address(rsp, 4*BytesPerWord));
3657 
3658 
3659         __ subl(length, tmp);
3660         __ addl(src_pos, tmp);
3661         __ addl(dst_pos, tmp);
3662       }
3663 
3664       __ jmp(*stub-&gt;entry());
3665 
3666       __ bind(cont);
3667       __ pop(dst);
3668       __ pop(src);
3669     }
3670   }
3671 
3672 #ifdef ASSERT
3673   if (basic_type != T_OBJECT || !(flags &amp; LIR_OpArrayCopy::type_check)) {
3674     // Sanity check the known type with the incoming class.  For the
3675     // primitive case the types must match exactly with src.klass and
3676     // dst.klass each exactly matching the default type.  For the
3677     // object array case, if no type check is needed then either the
3678     // dst type is exactly the expected type and the src type is a
3679     // subtype which we can&#39;t check or src is the same array as dst
3680     // but not necessarily exactly of type default_type.
3681     Label known_ok, halt;
3682     __ mov_metadata(tmp, default_type-&gt;constant_encoding());
3683 #ifdef _LP64
3684     if (UseCompressedClassPointers) {
3685       __ encode_klass_not_null(tmp);
3686     }
3687 #endif
3688 
3689     if (basic_type != T_OBJECT) {
3690 
3691       if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);
3692       else                   __ cmpptr(tmp, dst_klass_addr);
3693       __ jcc(Assembler::notEqual, halt);
3694       if (UseCompressedClassPointers)          __ cmpl(tmp, src_klass_addr);
3695       else                   __ cmpptr(tmp, src_klass_addr);
3696       __ jcc(Assembler::equal, known_ok);
3697     } else {
3698       if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);
3699       else                   __ cmpptr(tmp, dst_klass_addr);
3700       __ jcc(Assembler::equal, known_ok);
3701       __ cmpptr(src, dst);
3702       __ jcc(Assembler::equal, known_ok);
3703     }
3704     __ bind(halt);
3705     __ stop(&quot;incorrect type information in arraycopy&quot;);
3706     __ bind(known_ok);
3707   }
3708 #endif
3709 
3710 #ifndef PRODUCT
3711   if (PrintC1Statistics) {
3712     __ incrementl(ExternalAddress(Runtime1::arraycopy_count_address(basic_type)));
3713   }
3714 #endif
3715 
3716 #ifdef _LP64
3717   assert_different_registers(c_rarg0, dst, dst_pos, length);
3718   __ lea(c_rarg0, Address(src, src_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));
3719   assert_different_registers(c_rarg1, length);
3720   __ lea(c_rarg1, Address(dst, dst_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));
3721   __ mov(c_rarg2, length);
3722 
3723 #else
3724   __ lea(tmp, Address(src, src_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));
3725   store_parameter(tmp, 0);
3726   __ lea(tmp, Address(dst, dst_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));
3727   store_parameter(tmp, 1);
3728   store_parameter(length, 2);
3729 #endif // _LP64
3730 
3731   bool disjoint = (flags &amp; LIR_OpArrayCopy::overlapping) == 0;
3732   bool aligned = (flags &amp; LIR_OpArrayCopy::unaligned) == 0;
3733   const char *name;
3734   address entry = StubRoutines::select_arraycopy_function(basic_type, aligned, disjoint, name, false);
3735   __ call_VM_leaf(entry, 0);
3736 
3737   __ bind(*stub-&gt;continuation());
3738 }
3739 
3740 void LIR_Assembler::emit_updatecrc32(LIR_OpUpdateCRC32* op) {
3741   assert(op-&gt;crc()-&gt;is_single_cpu(),  &quot;crc must be register&quot;);
3742   assert(op-&gt;val()-&gt;is_single_cpu(),  &quot;byte value must be register&quot;);
3743   assert(op-&gt;result_opr()-&gt;is_single_cpu(), &quot;result must be register&quot;);
3744   Register crc = op-&gt;crc()-&gt;as_register();
3745   Register val = op-&gt;val()-&gt;as_register();
3746   Register res = op-&gt;result_opr()-&gt;as_register();
3747 
3748   assert_different_registers(val, crc, res);
3749 
3750   __ lea(res, ExternalAddress(StubRoutines::crc_table_addr()));
3751   __ notl(crc); // ~crc
3752   __ update_byte_crc32(crc, val, res);
3753   __ notl(crc); // ~crc
3754   __ mov(res, crc);
3755 }
3756 
3757 void LIR_Assembler::emit_lock(LIR_OpLock* op) {
3758   Register obj = op-&gt;obj_opr()-&gt;as_register();  // may not be an oop
3759   Register hdr = op-&gt;hdr_opr()-&gt;as_register();
3760   Register lock = op-&gt;lock_opr()-&gt;as_register();
3761   if (!UseFastLocking) {
3762     __ jmp(*op-&gt;stub()-&gt;entry());
3763   } else if (op-&gt;code() == lir_lock) {
3764     Register scratch = noreg;
3765     if (UseBiasedLocking) {
3766       scratch = op-&gt;scratch_opr()-&gt;as_register();
3767     }
3768     assert(BasicLock::displaced_header_offset_in_bytes() == 0, &quot;lock_reg must point to the displaced header&quot;);
3769     __ resolve(ACCESS_READ | ACCESS_WRITE, obj);
3770     // add debug info for NullPointerException only if one is possible
3771     int null_check_offset = __ lock_object(hdr, obj, lock, scratch, *op-&gt;stub()-&gt;entry());
3772     if (op-&gt;info() != NULL) {
3773       add_debug_info_for_null_check(null_check_offset, op-&gt;info());
3774     }
3775     // done
3776   } else if (op-&gt;code() == lir_unlock) {
3777     assert(BasicLock::displaced_header_offset_in_bytes() == 0, &quot;lock_reg must point to the displaced header&quot;);
3778     __ unlock_object(hdr, obj, lock, *op-&gt;stub()-&gt;entry());
3779   } else {
3780     Unimplemented();
3781   }
3782   __ bind(*op-&gt;stub()-&gt;continuation());
3783 }
3784 
3785 
3786 void LIR_Assembler::emit_profile_call(LIR_OpProfileCall* op) {
3787   ciMethod* method = op-&gt;profiled_method();
3788   int bci          = op-&gt;profiled_bci();
3789   ciMethod* callee = op-&gt;profiled_callee();
3790 
3791   // Update counter for all call types
3792   ciMethodData* md = method-&gt;method_data_or_null();
3793   assert(md != NULL, &quot;Sanity&quot;);
3794   ciProfileData* data = md-&gt;bci_to_data(bci);
3795   assert(data != NULL &amp;&amp; data-&gt;is_CounterData(), &quot;need CounterData for calls&quot;);
3796   assert(op-&gt;mdo()-&gt;is_single_cpu(),  &quot;mdo must be allocated&quot;);
3797   Register mdo  = op-&gt;mdo()-&gt;as_register();
3798   __ mov_metadata(mdo, md-&gt;constant_encoding());
3799   Address counter_addr(mdo, md-&gt;byte_offset_of_slot(data, CounterData::count_offset()));
3800   // Perform additional virtual call profiling for invokevirtual and
3801   // invokeinterface bytecodes
3802   if (op-&gt;should_profile_receiver_type()) {
3803     assert(op-&gt;recv()-&gt;is_single_cpu(), &quot;recv must be allocated&quot;);
3804     Register recv = op-&gt;recv()-&gt;as_register();
3805     assert_different_registers(mdo, recv);
3806     assert(data-&gt;is_VirtualCallData(), &quot;need VirtualCallData for virtual calls&quot;);
3807     ciKlass* known_klass = op-&gt;known_holder();
3808     if (C1OptimizeVirtualCallProfiling &amp;&amp; known_klass != NULL) {
3809       // We know the type that will be seen at this call site; we can
3810       // statically update the MethodData* rather than needing to do
3811       // dynamic tests on the receiver type
3812 
3813       // NOTE: we should probably put a lock around this search to
3814       // avoid collisions by concurrent compilations
3815       ciVirtualCallData* vc_data = (ciVirtualCallData*) data;
3816       uint i;
3817       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
3818         ciKlass* receiver = vc_data-&gt;receiver(i);
3819         if (known_klass-&gt;equals(receiver)) {
3820           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)));
3821           __ addptr(data_addr, DataLayout::counter_increment);
3822           return;
3823         }
3824       }
3825 
3826       // Receiver type not found in profile data; select an empty slot
3827 
3828       // Note that this is less efficient than it should be because it
3829       // always does a write to the receiver part of the
3830       // VirtualCallData rather than just the first time
3831       for (i = 0; i &lt; VirtualCallData::row_limit(); i++) {
3832         ciKlass* receiver = vc_data-&gt;receiver(i);
3833         if (receiver == NULL) {
3834           Address recv_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_offset(i)));
3835           __ mov_metadata(recv_addr, known_klass-&gt;constant_encoding());
3836           Address data_addr(mdo, md-&gt;byte_offset_of_slot(data, VirtualCallData::receiver_count_offset(i)));
3837           __ addptr(data_addr, DataLayout::counter_increment);
3838           return;
3839         }
3840       }
3841     } else {
3842       __ load_klass(recv, recv);
3843       Label update_done;
3844       type_profile_helper(mdo, md, data, recv, &amp;update_done);
3845       // Receiver did not match any saved receiver and there is no empty row for it.
3846       // Increment total counter to indicate polymorphic case.
3847       __ addptr(counter_addr, DataLayout::counter_increment);
3848 
3849       __ bind(update_done);
3850     }
3851   } else {
3852     // Static call
3853     __ addptr(counter_addr, DataLayout::counter_increment);
3854   }
3855 }
3856 
3857 void LIR_Assembler::emit_profile_type(LIR_OpProfileType* op) {
3858   Register obj = op-&gt;obj()-&gt;as_register();
3859   Register tmp = op-&gt;tmp()-&gt;as_pointer_register();
3860   Address mdo_addr = as_Address(op-&gt;mdp()-&gt;as_address_ptr());
3861   ciKlass* exact_klass = op-&gt;exact_klass();
3862   intptr_t current_klass = op-&gt;current_klass();
3863   bool not_null = op-&gt;not_null();
3864   bool no_conflict = op-&gt;no_conflict();
3865 
3866   Label update, next, none;
3867 
3868   bool do_null = !not_null;
3869   bool exact_klass_set = exact_klass != NULL &amp;&amp; ciTypeEntries::valid_ciklass(current_klass) == exact_klass;
3870   bool do_update = !TypeEntries::is_type_unknown(current_klass) &amp;&amp; !exact_klass_set;
3871 
3872   assert(do_null || do_update, &quot;why are we here?&quot;);
3873   assert(!TypeEntries::was_null_seen(current_klass) || do_update, &quot;why are we here?&quot;);
3874 
3875   __ verify_oop(obj);
3876 
3877   if (tmp != obj) {
3878     __ mov(tmp, obj);
3879   }
3880   if (do_null) {
3881     __ testptr(tmp, tmp);
3882     __ jccb(Assembler::notZero, update);
3883     if (!TypeEntries::was_null_seen(current_klass)) {
3884       __ orptr(mdo_addr, TypeEntries::null_seen);
3885     }
3886     if (do_update) {
3887 #ifndef ASSERT
3888       __ jmpb(next);
3889     }
3890 #else
3891       __ jmp(next);
3892     }
3893   } else {
3894     __ testptr(tmp, tmp);
3895     __ jcc(Assembler::notZero, update);
3896     __ stop(&quot;unexpect null obj&quot;);
3897 #endif
3898   }
3899 
3900   __ bind(update);
3901 
3902   if (do_update) {
3903 #ifdef ASSERT
3904     if (exact_klass != NULL) {
3905       Label ok;
3906       __ load_klass(tmp, tmp);
3907       __ push(tmp);
3908       __ mov_metadata(tmp, exact_klass-&gt;constant_encoding());
3909       __ cmpptr(tmp, Address(rsp, 0));
3910       __ jcc(Assembler::equal, ok);
3911       __ stop(&quot;exact klass and actual klass differ&quot;);
3912       __ bind(ok);
3913       __ pop(tmp);
3914     }
3915 #endif
3916     if (!no_conflict) {
3917       if (exact_klass == NULL || TypeEntries::is_type_none(current_klass)) {
3918         if (exact_klass != NULL) {
3919           __ mov_metadata(tmp, exact_klass-&gt;constant_encoding());
3920         } else {
3921           __ load_klass(tmp, tmp);
3922         }
3923 
3924         __ xorptr(tmp, mdo_addr);
3925         __ testptr(tmp, TypeEntries::type_klass_mask);
3926         // klass seen before, nothing to do. The unknown bit may have been
3927         // set already but no need to check.
3928         __ jccb(Assembler::zero, next);
3929 
3930         __ testptr(tmp, TypeEntries::type_unknown);
3931         __ jccb(Assembler::notZero, next); // already unknown. Nothing to do anymore.
3932 
3933         if (TypeEntries::is_type_none(current_klass)) {
3934           __ cmpptr(mdo_addr, 0);
3935           __ jccb(Assembler::equal, none);
3936           __ cmpptr(mdo_addr, TypeEntries::null_seen);
3937           __ jccb(Assembler::equal, none);
3938           // There is a chance that the checks above (re-reading profiling
3939           // data from memory) fail if another thread has just set the
3940           // profiling to this obj&#39;s klass
3941           __ xorptr(tmp, mdo_addr);
3942           __ testptr(tmp, TypeEntries::type_klass_mask);
3943           __ jccb(Assembler::zero, next);
3944         }
3945       } else {
3946         assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &amp;&amp;
3947                ciTypeEntries::valid_ciklass(current_klass) != exact_klass, &quot;conflict only&quot;);
3948 
3949         __ movptr(tmp, mdo_addr);
3950         __ testptr(tmp, TypeEntries::type_unknown);
3951         __ jccb(Assembler::notZero, next); // already unknown. Nothing to do anymore.
3952       }
3953 
3954       // different than before. Cannot keep accurate profile.
3955       __ orptr(mdo_addr, TypeEntries::type_unknown);
3956 
3957       if (TypeEntries::is_type_none(current_klass)) {
3958         __ jmpb(next);
3959 
3960         __ bind(none);
3961         // first time here. Set profile type.
3962         __ movptr(mdo_addr, tmp);
3963       }
3964     } else {
3965       // There&#39;s a single possible klass at this profile point
3966       assert(exact_klass != NULL, &quot;should be&quot;);
3967       if (TypeEntries::is_type_none(current_klass)) {
3968         __ mov_metadata(tmp, exact_klass-&gt;constant_encoding());
3969         __ xorptr(tmp, mdo_addr);
3970         __ testptr(tmp, TypeEntries::type_klass_mask);
3971 #ifdef ASSERT
3972         __ jcc(Assembler::zero, next);
3973 
3974         {
3975           Label ok;
3976           __ push(tmp);
3977           __ cmpptr(mdo_addr, 0);
3978           __ jcc(Assembler::equal, ok);
3979           __ cmpptr(mdo_addr, TypeEntries::null_seen);
3980           __ jcc(Assembler::equal, ok);
3981           // may have been set by another thread
3982           __ mov_metadata(tmp, exact_klass-&gt;constant_encoding());
3983           __ xorptr(tmp, mdo_addr);
3984           __ testptr(tmp, TypeEntries::type_mask);
3985           __ jcc(Assembler::zero, ok);
3986 
3987           __ stop(&quot;unexpected profiling mismatch&quot;);
3988           __ bind(ok);
3989           __ pop(tmp);
3990         }
3991 #else
3992         __ jccb(Assembler::zero, next);
3993 #endif
3994         // first time here. Set profile type.
3995         __ movptr(mdo_addr, tmp);
3996       } else {
3997         assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &amp;&amp;
3998                ciTypeEntries::valid_ciklass(current_klass) != exact_klass, &quot;inconsistent&quot;);
3999 
4000         __ movptr(tmp, mdo_addr);
4001         __ testptr(tmp, TypeEntries::type_unknown);
4002         __ jccb(Assembler::notZero, next); // already unknown. Nothing to do anymore.
4003 
4004         __ orptr(mdo_addr, TypeEntries::type_unknown);
4005       }
4006     }
4007 
4008     __ bind(next);
4009   }
4010 }
4011 
4012 void LIR_Assembler::emit_delay(LIR_OpDelay*) {
4013   Unimplemented();
4014 }
4015 
4016 
4017 void LIR_Assembler::monitor_address(int monitor_no, LIR_Opr dst) {
4018   __ lea(dst-&gt;as_register(), frame_map()-&gt;address_for_monitor_lock(monitor_no));
4019 }
4020 
4021 
4022 void LIR_Assembler::align_backward_branch_target() {
4023   __ align(BytesPerWord);
4024 }
4025 
4026 
4027 void LIR_Assembler::negate(LIR_Opr left, LIR_Opr dest, LIR_Opr tmp) {
4028   if (left-&gt;is_single_cpu()) {
4029     __ negl(left-&gt;as_register());
4030     move_regs(left-&gt;as_register(), dest-&gt;as_register());
4031 
4032   } else if (left-&gt;is_double_cpu()) {
4033     Register lo = left-&gt;as_register_lo();
4034 #ifdef _LP64
4035     Register dst = dest-&gt;as_register_lo();
4036     __ movptr(dst, lo);
4037     __ negptr(dst);
4038 #else
4039     Register hi = left-&gt;as_register_hi();
4040     __ lneg(hi, lo);
4041     if (dest-&gt;as_register_lo() == hi) {
4042       assert(dest-&gt;as_register_hi() != lo, &quot;destroying register&quot;);
4043       move_regs(hi, dest-&gt;as_register_hi());
4044       move_regs(lo, dest-&gt;as_register_lo());
4045     } else {
4046       move_regs(lo, dest-&gt;as_register_lo());
4047       move_regs(hi, dest-&gt;as_register_hi());
4048     }
4049 #endif // _LP64
4050 
4051   } else if (dest-&gt;is_single_xmm()) {
4052 #ifdef _LP64
4053     if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512vl()) {
4054       assert(tmp-&gt;is_valid(), &quot;need temporary&quot;);
4055       assert_different_registers(left-&gt;as_xmm_float_reg(), tmp-&gt;as_xmm_float_reg());
4056       __ vpxor(dest-&gt;as_xmm_float_reg(), tmp-&gt;as_xmm_float_reg(), left-&gt;as_xmm_float_reg(), 2);
4057     }
4058     else
4059 #endif
4060     {
4061       assert(!tmp-&gt;is_valid(), &quot;do not need temporary&quot;);
4062       if (left-&gt;as_xmm_float_reg() != dest-&gt;as_xmm_float_reg()) {
4063         __ movflt(dest-&gt;as_xmm_float_reg(), left-&gt;as_xmm_float_reg());
4064       }
4065       __ xorps(dest-&gt;as_xmm_float_reg(),
4066                ExternalAddress((address)float_signflip_pool));
4067     }
4068   } else if (dest-&gt;is_double_xmm()) {
4069 #ifdef _LP64
4070     if (UseAVX &gt; 2 &amp;&amp; !VM_Version::supports_avx512vl()) {
4071       assert(tmp-&gt;is_valid(), &quot;need temporary&quot;);
4072       assert_different_registers(left-&gt;as_xmm_double_reg(), tmp-&gt;as_xmm_double_reg());
4073       __ vpxor(dest-&gt;as_xmm_double_reg(), tmp-&gt;as_xmm_double_reg(), left-&gt;as_xmm_double_reg(), 2);
4074     }
4075     else
4076 #endif
4077     {
4078       assert(!tmp-&gt;is_valid(), &quot;do not need temporary&quot;);
4079       if (left-&gt;as_xmm_double_reg() != dest-&gt;as_xmm_double_reg()) {
4080         __ movdbl(dest-&gt;as_xmm_double_reg(), left-&gt;as_xmm_double_reg());
4081       }
4082       __ xorpd(dest-&gt;as_xmm_double_reg(),
4083                ExternalAddress((address)double_signflip_pool));
4084     }
4085 #ifndef _LP64
4086   } else if (left-&gt;is_single_fpu() || left-&gt;is_double_fpu()) {
4087     assert(left-&gt;fpu() == 0, &quot;arg must be on TOS&quot;);
4088     assert(dest-&gt;fpu() == 0, &quot;dest must be TOS&quot;);
4089     __ fchs();
4090 #endif // !_LP64
4091 
4092   } else {
4093     ShouldNotReachHere();
4094   }
4095 }
4096 
4097 
4098 void LIR_Assembler::leal(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code, CodeEmitInfo* info) {
4099   assert(src-&gt;is_address(), &quot;must be an address&quot;);
4100   assert(dest-&gt;is_register(), &quot;must be a register&quot;);
4101 
4102   PatchingStub* patch = NULL;
4103   if (patch_code != lir_patch_none) {
4104     patch = new PatchingStub(_masm, PatchingStub::access_field_id);
4105   }
4106 
4107   Register reg = dest-&gt;as_pointer_register();
4108   LIR_Address* addr = src-&gt;as_address_ptr();
4109   __ lea(reg, as_Address(addr));
4110 
4111   if (patch != NULL) {
4112     patching_epilog(patch, patch_code, addr-&gt;base()-&gt;as_register(), info);
4113   }
4114 }
4115 
4116 
4117 
4118 void LIR_Assembler::rt_call(LIR_Opr result, address dest, const LIR_OprList* args, LIR_Opr tmp, CodeEmitInfo* info) {
4119   assert(!tmp-&gt;is_valid(), &quot;don&#39;t need temporary&quot;);
4120   __ call(RuntimeAddress(dest));
4121   if (info != NULL) {
4122     add_call_info_here(info);
4123   }
4124 }
4125 
4126 
4127 void LIR_Assembler::volatile_move_op(LIR_Opr src, LIR_Opr dest, BasicType type, CodeEmitInfo* info) {
4128   assert(type == T_LONG, &quot;only for volatile long fields&quot;);
4129 
4130   if (info != NULL) {
4131     add_debug_info_for_null_check_here(info);
4132   }
4133 
4134   if (src-&gt;is_double_xmm()) {
4135     if (dest-&gt;is_double_cpu()) {
4136 #ifdef _LP64
4137       __ movdq(dest-&gt;as_register_lo(), src-&gt;as_xmm_double_reg());
4138 #else
4139       __ movdl(dest-&gt;as_register_lo(), src-&gt;as_xmm_double_reg());
4140       __ psrlq(src-&gt;as_xmm_double_reg(), 32);
4141       __ movdl(dest-&gt;as_register_hi(), src-&gt;as_xmm_double_reg());
4142 #endif // _LP64
4143     } else if (dest-&gt;is_double_stack()) {
4144       __ movdbl(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix()), src-&gt;as_xmm_double_reg());
4145     } else if (dest-&gt;is_address()) {
4146       __ movdbl(as_Address(dest-&gt;as_address_ptr()), src-&gt;as_xmm_double_reg());
4147     } else {
4148       ShouldNotReachHere();
4149     }
4150 
4151   } else if (dest-&gt;is_double_xmm()) {
4152     if (src-&gt;is_double_stack()) {
4153       __ movdbl(dest-&gt;as_xmm_double_reg(), frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix()));
4154     } else if (src-&gt;is_address()) {
4155       __ movdbl(dest-&gt;as_xmm_double_reg(), as_Address(src-&gt;as_address_ptr()));
4156     } else {
4157       ShouldNotReachHere();
4158     }
4159 
4160 #ifndef _LP64
4161   } else if (src-&gt;is_double_fpu()) {
4162     assert(src-&gt;fpu_regnrLo() == 0, &quot;must be TOS&quot;);
4163     if (dest-&gt;is_double_stack()) {
4164       __ fistp_d(frame_map()-&gt;address_for_slot(dest-&gt;double_stack_ix()));
4165     } else if (dest-&gt;is_address()) {
4166       __ fistp_d(as_Address(dest-&gt;as_address_ptr()));
4167     } else {
4168       ShouldNotReachHere();
4169     }
4170 
4171   } else if (dest-&gt;is_double_fpu()) {
4172     assert(dest-&gt;fpu_regnrLo() == 0, &quot;must be TOS&quot;);
4173     if (src-&gt;is_double_stack()) {
4174       __ fild_d(frame_map()-&gt;address_for_slot(src-&gt;double_stack_ix()));
4175     } else if (src-&gt;is_address()) {
4176       __ fild_d(as_Address(src-&gt;as_address_ptr()));
4177     } else {
4178       ShouldNotReachHere();
4179     }
4180 #endif // !_LP64
4181 
4182   } else {
4183     ShouldNotReachHere();
4184   }
4185 }
4186 
4187 #ifdef ASSERT
4188 // emit run-time assertion
4189 void LIR_Assembler::emit_assert(LIR_OpAssert* op) {
4190   assert(op-&gt;code() == lir_assert, &quot;must be&quot;);
4191 
4192   if (op-&gt;in_opr1()-&gt;is_valid()) {
4193     assert(op-&gt;in_opr2()-&gt;is_valid(), &quot;both operands must be valid&quot;);
4194     comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
4195   } else {
4196     assert(op-&gt;in_opr2()-&gt;is_illegal(), &quot;both operands must be illegal&quot;);
4197     assert(op-&gt;condition() == lir_cond_always, &quot;no other conditions allowed&quot;);
4198   }
4199 
4200   Label ok;
4201   if (op-&gt;condition() != lir_cond_always) {
4202     Assembler::Condition acond = Assembler::zero;
4203     switch (op-&gt;condition()) {
4204       case lir_cond_equal:        acond = Assembler::equal;       break;
4205       case lir_cond_notEqual:     acond = Assembler::notEqual;    break;
4206       case lir_cond_less:         acond = Assembler::less;        break;
4207       case lir_cond_lessEqual:    acond = Assembler::lessEqual;   break;
4208       case lir_cond_greaterEqual: acond = Assembler::greaterEqual;break;
4209       case lir_cond_greater:      acond = Assembler::greater;     break;
4210       case lir_cond_belowEqual:   acond = Assembler::belowEqual;  break;
4211       case lir_cond_aboveEqual:   acond = Assembler::aboveEqual;  break;
4212       default:                    ShouldNotReachHere();
4213     }
4214     __ jcc(acond, ok);
4215   }
4216   if (op-&gt;halt()) {
4217     const char* str = __ code_string(op-&gt;msg());
4218     __ stop(str);
4219   } else {
4220     breakpoint();
4221   }
4222   __ bind(ok);
4223 }
4224 #endif
4225 
4226 void LIR_Assembler::membar() {
4227   // QQQ sparc TSO uses this,
4228   __ membar( Assembler::Membar_mask_bits(Assembler::StoreLoad));
4229 }
4230 
4231 void LIR_Assembler::membar_acquire() {
4232   // No x86 machines currently require load fences
4233 }
4234 
4235 void LIR_Assembler::membar_release() {
4236   // No x86 machines currently require store fences
4237 }
4238 
4239 void LIR_Assembler::membar_loadload() {
4240   // no-op
4241   //__ membar(Assembler::Membar_mask_bits(Assembler::loadload));
4242 }
4243 
4244 void LIR_Assembler::membar_storestore() {
4245   // no-op
4246   //__ membar(Assembler::Membar_mask_bits(Assembler::storestore));
4247 }
4248 
4249 void LIR_Assembler::membar_loadstore() {
4250   // no-op
4251   //__ membar(Assembler::Membar_mask_bits(Assembler::loadstore));
4252 }
4253 
4254 void LIR_Assembler::membar_storeload() {
4255   __ membar(Assembler::Membar_mask_bits(Assembler::StoreLoad));
4256 }
4257 
4258 void LIR_Assembler::on_spin_wait() {
4259   __ pause ();
4260 }
4261 
4262 void LIR_Assembler::get_thread(LIR_Opr result_reg) {
4263   assert(result_reg-&gt;is_register(), &quot;check&quot;);
4264 #ifdef _LP64
4265   // __ get_thread(result_reg-&gt;as_register_lo());
4266   __ mov(result_reg-&gt;as_register(), r15_thread);
4267 #else
4268   __ get_thread(result_reg-&gt;as_register());
4269 #endif // _LP64
4270 }
4271 
4272 
4273 void LIR_Assembler::peephole(LIR_List*) {
4274   // do nothing for now
4275 }
4276 
4277 void LIR_Assembler::atomic_op(LIR_Code code, LIR_Opr src, LIR_Opr data, LIR_Opr dest, LIR_Opr tmp) {
4278   assert(data == dest, &quot;xchg/xadd uses only 2 operands&quot;);
4279 
4280   if (data-&gt;type() == T_INT) {
4281     if (code == lir_xadd) {
4282       __ lock();
4283       __ xaddl(as_Address(src-&gt;as_address_ptr()), data-&gt;as_register());
4284     } else {
4285       __ xchgl(data-&gt;as_register(), as_Address(src-&gt;as_address_ptr()));
4286     }
4287   } else if (data-&gt;is_oop()) {
4288     assert (code == lir_xchg, &quot;xadd for oops&quot;);
4289     Register obj = data-&gt;as_register();
4290 #ifdef _LP64
4291     if (UseCompressedOops) {
4292       __ encode_heap_oop(obj);
4293       __ xchgl(obj, as_Address(src-&gt;as_address_ptr()));
4294       __ decode_heap_oop(obj);
4295     } else {
4296       __ xchgptr(obj, as_Address(src-&gt;as_address_ptr()));
4297     }
4298 #else
4299     __ xchgl(obj, as_Address(src-&gt;as_address_ptr()));
4300 #endif
4301   } else if (data-&gt;type() == T_LONG) {
4302 #ifdef _LP64
4303     assert(data-&gt;as_register_lo() == data-&gt;as_register_hi(), &quot;should be a single register&quot;);
4304     if (code == lir_xadd) {
4305       __ lock();
4306       __ xaddq(as_Address(src-&gt;as_address_ptr()), data-&gt;as_register_lo());
4307     } else {
4308       __ xchgq(data-&gt;as_register_lo(), as_Address(src-&gt;as_address_ptr()));
4309     }
4310 #else
4311     ShouldNotReachHere();
4312 #endif
4313   } else {
4314     ShouldNotReachHere();
4315   }
4316 }
4317 
4318 #undef __
    </pre>
  </body>
</html>