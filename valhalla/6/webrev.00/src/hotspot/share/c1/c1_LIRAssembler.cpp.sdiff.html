<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/c1/c1_LIRAssembler.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c1_LIR.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_LIRAssembler.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/c1/c1_LIRAssembler.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 618   bool reexecute = false;
 619   bool return_oop = false; // This flag will be ignored since it used only for C2 with escape analysis.
 620   bool rethrow_exception = false;
 621   bool is_method_handle_invoke = false;
 622   debug_info-&gt;describe_scope(pc_offset, methodHandle(), method(), 0, reexecute, rethrow_exception, is_method_handle_invoke, return_oop, false, locvals, expvals, monvals);
 623   debug_info-&gt;end_safepoint(pc_offset);
 624 }
 625 
 626 // The entries points of C1-compiled methods can have the following types:
 627 // (1) Methods with no value args
 628 // (2) Methods with value receiver but no value args
 629 //     VVEP_RO is the same as VVEP
 630 // (3) Methods with non-value receiver and some value args
 631 //     VVEP_RO is the same as VEP
 632 // (4) Methods with value receiver and other value args
 633 //     Separate VEP, VVEP and VVEP_RO
 634 //
 635 // (1)               (2)                 (3)                    (4)
 636 // UEP/UVEP:         VEP:                UEP:                   UEP:
 637 //   check_icache      pack receiver       check_icache           check_icache
<span class="line-modified"> 638 // VEP/VVEP/VVEP_RO  UEP/UVEP:           VEP/VVEP_RO:           VVEP_RO:</span>
<span class="line-modified"> 639 //   body              check_icache        pack value args        pack value args (except receiver)</span>

 640 //                   VVEP/VVEP_RO        UVEP:                  VEP:
 641 //                     body                check_icache           pack all value args
<span class="line-modified"> 642 //                                       VVEP:                  UVEP:</span>
<span class="line-modified"> 643 //                                         body                   check_icache</span>

 644 //                                                              VVEP:
 645 //                                                                body
<span class="line-removed"> 646 //</span>
<span class="line-removed"> 647 // Note: after packing, we jump to the method body.</span>
 648 void LIR_Assembler::emit_std_entries() {
 649   offsets()-&gt;set_value(CodeOffsets::OSR_Entry, _masm-&gt;offset());
 650 
<span class="line-removed"> 651   const CompiledEntrySignature* ces = compilation()-&gt;compiled_entry_signature();</span>
<span class="line-removed"> 652 </span>
 653   _masm-&gt;align(CodeEntryAlignment);
<span class="line-modified"> 654 </span>
 655   if (ces-&gt;has_scalarized_args()) {
 656     assert(ValueTypePassFieldsAsArgs &amp;&amp; method()-&gt;get_Method()-&gt;has_scalarized_args(), &quot;must be&quot;);
<span class="line-removed"> 657 </span>
 658     CodeOffsets::Entries ro_entry_type = ces-&gt;c1_value_ro_entry_type();
 659 

 660     if (ro_entry_type != CodeOffsets::Verified_Value_Entry) {
<span class="line-removed"> 661       // This is the UEP. It will fall-through to VEP or VVEP(RO)</span>
 662       offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());
<span class="line-modified"> 663       if (needs_icache(compilation()-&gt;method())) {</span>
 664         check_icache();
 665       }
 666     }
 667 

 668     if (ro_entry_type == CodeOffsets::Verified_Value_Entry_RO) {
<span class="line-modified"> 669       // VVEP(RO) = pack all value parameters, except the &lt;this&gt; object.</span>
<span class="line-removed"> 670       add_scalarized_entry_info(emit_std_entry(CodeOffsets::Verified_Value_Entry_RO, ces));</span>
 671     }
 672 
<span class="line-modified"> 673     // VEP = pack all value parameters</span>
 674     _masm-&gt;align(CodeEntryAlignment);
<span class="line-modified"> 675     add_scalarized_entry_info(emit_std_entry(CodeOffsets::Verified_Entry, ces));</span>
 676 

 677     _masm-&gt;align(CodeEntryAlignment);
<span class="line-removed"> 678     // This is the UVEP. It will fall-through to VVEP.</span>
 679     offsets()-&gt;set_value(CodeOffsets::Value_Entry, _masm-&gt;offset());
 680     if (ro_entry_type == CodeOffsets::Verified_Value_Entry) {
 681       // Special case if we have VVEP == VVEP(RO):
 682       // this means UVEP (called by C1) == UEP (called by C2).
 683       offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());
 684     }
<span class="line-modified"> 685 </span>
<span class="line-removed"> 686     if (needs_icache(compilation()-&gt;method())) {</span>
 687       check_icache();
 688     }
<span class="line-modified"> 689     // VVEP = all value parameters are passed as refs - no packing.</span>

 690     emit_std_entry(CodeOffsets::Verified_Value_Entry, NULL);
 691 
 692     if (ro_entry_type != CodeOffsets::Verified_Value_Entry_RO) {
 693       // The VVEP(RO) is the same as VEP or VVEP
 694       assert(ro_entry_type == CodeOffsets::Verified_Entry ||
 695              ro_entry_type == CodeOffsets::Verified_Value_Entry, &quot;must be&quot;);
 696       offsets()-&gt;set_value(CodeOffsets::Verified_Value_Entry_RO,
 697                            offsets()-&gt;value(ro_entry_type));
 698     }
 699   } else {
 700     // All 3 entries are the same (no value-type packing)
 701     offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());
 702     offsets()-&gt;set_value(CodeOffsets::Value_Entry, _masm-&gt;offset());
<span class="line-modified"> 703     if (needs_icache(compilation()-&gt;method())) {</span>
 704       check_icache();
 705     }
<span class="line-modified"> 706     int offset = emit_std_entry(CodeOffsets::Verified_Value_Entry, NULL);</span>
<span class="line-modified"> 707     offsets()-&gt;set_value(CodeOffsets::Verified_Entry, offset);</span>
<span class="line-modified"> 708     offsets()-&gt;set_value(CodeOffsets::Verified_Value_Entry_RO, offset);</span>
 709   }
 710 }
 711 
<span class="line-modified"> 712 int LIR_Assembler::emit_std_entry(CodeOffsets::Entries entry, const CompiledEntrySignature* ces) {</span>
 713   offsets()-&gt;set_value(entry, _masm-&gt;offset());
<span class="line-modified"> 714   int offset = _masm-&gt;offset();</span>
 715   switch (entry) {
<span class="line-modified"> 716   case CodeOffsets::Verified_Entry:</span>
<span class="line-modified"> 717     offset = _masm-&gt;verified_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), _verified_value_entry);</span>
<span class="line-modified"> 718     if (needs_clinit_barrier_on_entry(compilation()-&gt;method())) {</span>
<span class="line-removed"> 719       clinit_barrier(compilation()-&gt;method());</span>
 720     }
<span class="line-modified"> 721     return offset;</span>
<span class="line-modified"> 722   case CodeOffsets::Verified_Value_Entry_RO:</span>
<span class="line-modified"> 723     offset = _masm-&gt;verified_value_ro_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), _verified_value_entry);</span>
<span class="line-modified"> 724     if (needs_clinit_barrier_on_entry(compilation()-&gt;method())) {</span>
<span class="line-modified"> 725       clinit_barrier(compilation()-&gt;method());</span>








 726     }
<span class="line-modified"> 727     return offset;</span>



 728   default:
<span class="line-modified"> 729     {</span>
<span class="line-modified"> 730       assert(entry == CodeOffsets::Verified_Value_Entry, &quot;must be&quot;);</span>
<span class="line-removed"> 731       _masm-&gt;verified_value_entry();</span>
<span class="line-removed"> 732       if (needs_clinit_barrier_on_entry(compilation()-&gt;method())) {</span>
<span class="line-removed"> 733         clinit_barrier(compilation()-&gt;method());</span>
<span class="line-removed"> 734       }</span>
<span class="line-removed"> 735       build_frame();</span>
<span class="line-removed"> 736       offsets()-&gt;set_value(CodeOffsets::Frame_Complete, _masm-&gt;offset());</span>
<span class="line-removed"> 737       return offset;</span>
<span class="line-removed"> 738     }</span>
 739   }
 740 }
 741 
 742 void LIR_Assembler::emit_op0(LIR_Op0* op) {
 743   switch (op-&gt;code()) {
 744     case lir_word_align: {
 745       _masm-&gt;align(BytesPerWord);
 746       break;
 747     }
 748 
 749     case lir_nop:
 750       assert(op-&gt;info() == NULL, &quot;not supported&quot;);
 751       _masm-&gt;nop();
 752       break;
 753 
 754     case lir_label:
 755       Unimplemented();
 756       break;
 757 
 758     case lir_build_frame:
</pre>
<hr />
<pre>
 797     case lir_membar_storestore:
 798       membar_storestore();
 799       break;
 800 
 801     case lir_membar_loadstore:
 802       membar_loadstore();
 803       break;
 804 
 805     case lir_membar_storeload:
 806       membar_storeload();
 807       break;
 808 
 809     case lir_get_thread:
 810       get_thread(op-&gt;result_opr());
 811       break;
 812 
 813     case lir_on_spin_wait:
 814       on_spin_wait();
 815       break;
 816 




 817     default:
 818       ShouldNotReachHere();
 819       break;
 820   }
 821 }
 822 
 823 
 824 void LIR_Assembler::emit_op2(LIR_Op2* op) {
 825   switch (op-&gt;code()) {
 826     case lir_cmp:
 827       if (op-&gt;info() != NULL) {
 828         assert(op-&gt;in_opr1()-&gt;is_address() || op-&gt;in_opr2()-&gt;is_address(),
 829                &quot;shouldn&#39;t be codeemitinfo for non-address operands&quot;);
 830         add_debug_info_for_null_check_here(op-&gt;info()); // exception possible
 831       }
 832       comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
 833       break;
 834 
 835     case lir_cmp_l2i:
 836     case lir_cmp_fd2i:
</pre>
<hr />
<pre>
 890         op-&gt;result_opr());
 891       break;
 892 
 893     case lir_throw:
 894       throw_op(op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;info());
 895       break;
 896 
 897     case lir_xadd:
 898     case lir_xchg:
 899       atomic_op(op-&gt;code(), op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;result_opr(), op-&gt;tmp1_opr());
 900       break;
 901 
 902     default:
 903       Unimplemented();
 904       break;
 905   }
 906 }
 907 
 908 
 909 void LIR_Assembler::build_frame() {
<span class="line-modified"> 910   _masm-&gt;build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(),</span>
<span class="line-modified"> 911                      compilation()-&gt;needs_stack_repair(),</span>
 912                      &amp;_verified_value_entry);
 913 }
 914 
 915 
 916 void LIR_Assembler::roundfp_op(LIR_Opr src, LIR_Opr tmp, LIR_Opr dest, bool pop_fpu_stack) {
 917   assert(strict_fp_requires_explicit_rounding, &quot;not required&quot;);
 918   assert((src-&gt;is_single_fpu() &amp;&amp; dest-&gt;is_single_stack()) ||
 919          (src-&gt;is_double_fpu() &amp;&amp; dest-&gt;is_double_stack()),
 920          &quot;round_fp: rounds register -&gt; stack location&quot;);
 921 
 922   reg2stack (src, dest, src-&gt;type(), pop_fpu_stack);
 923 }
 924 
 925 
 926 void LIR_Assembler::move_op(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool unaligned, bool wide) {
 927   if (src-&gt;is_register()) {
 928     if (dest-&gt;is_register()) {
 929       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
 930       reg2reg(src,  dest);
 931     } else if (dest-&gt;is_stack()) {
</pre>
</td>
<td>
<hr />
<pre>
 618   bool reexecute = false;
 619   bool return_oop = false; // This flag will be ignored since it used only for C2 with escape analysis.
 620   bool rethrow_exception = false;
 621   bool is_method_handle_invoke = false;
 622   debug_info-&gt;describe_scope(pc_offset, methodHandle(), method(), 0, reexecute, rethrow_exception, is_method_handle_invoke, return_oop, false, locvals, expvals, monvals);
 623   debug_info-&gt;end_safepoint(pc_offset);
 624 }
 625 
 626 // The entries points of C1-compiled methods can have the following types:
 627 // (1) Methods with no value args
 628 // (2) Methods with value receiver but no value args
 629 //     VVEP_RO is the same as VVEP
 630 // (3) Methods with non-value receiver and some value args
 631 //     VVEP_RO is the same as VEP
 632 // (4) Methods with value receiver and other value args
 633 //     Separate VEP, VVEP and VVEP_RO
 634 //
 635 // (1)               (2)                 (3)                    (4)
 636 // UEP/UVEP:         VEP:                UEP:                   UEP:
 637 //   check_icache      pack receiver       check_icache           check_icache
<span class="line-modified"> 638 // VEP/VVEP/VVEP_RO    jump to VVEP      VEP/VVEP_RO:           VVEP_RO:</span>
<span class="line-modified"> 639 //   body            UEP/UVEP:             pack value args        pack value args (except receiver)</span>
<span class="line-added"> 640 //                     check_icache        jump to VVEP           jump to VVEP</span>
 641 //                   VVEP/VVEP_RO        UVEP:                  VEP:
 642 //                     body                check_icache           pack all value args
<span class="line-modified"> 643 //                                       VVEP:                    jump to VVEP</span>
<span class="line-modified"> 644 //                                         body                 UVEP:</span>
<span class="line-added"> 645 //                                                                check_icache</span>
 646 //                                                              VVEP:
 647 //                                                                body


 648 void LIR_Assembler::emit_std_entries() {
 649   offsets()-&gt;set_value(CodeOffsets::OSR_Entry, _masm-&gt;offset());
 650 


 651   _masm-&gt;align(CodeEntryAlignment);
<span class="line-modified"> 652   const CompiledEntrySignature* ces = compilation()-&gt;compiled_entry_signature();</span>
 653   if (ces-&gt;has_scalarized_args()) {
 654     assert(ValueTypePassFieldsAsArgs &amp;&amp; method()-&gt;get_Method()-&gt;has_scalarized_args(), &quot;must be&quot;);

 655     CodeOffsets::Entries ro_entry_type = ces-&gt;c1_value_ro_entry_type();
 656 
<span class="line-added"> 657     // UEP: check icache and fall-through</span>
 658     if (ro_entry_type != CodeOffsets::Verified_Value_Entry) {

 659       offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());
<span class="line-modified"> 660       if (needs_icache(method())) {</span>
 661         check_icache();
 662       }
 663     }
 664 
<span class="line-added"> 665     // VVEP_RO: pack all value parameters, except the receiver</span>
 666     if (ro_entry_type == CodeOffsets::Verified_Value_Entry_RO) {
<span class="line-modified"> 667       emit_std_entry(CodeOffsets::Verified_Value_Entry_RO, ces);</span>

 668     }
 669 
<span class="line-modified"> 670     // VEP: pack all value parameters</span>
 671     _masm-&gt;align(CodeEntryAlignment);
<span class="line-modified"> 672     emit_std_entry(CodeOffsets::Verified_Entry, ces);</span>
 673 
<span class="line-added"> 674     // UVEP: check icache and fall-through</span>
 675     _masm-&gt;align(CodeEntryAlignment);

 676     offsets()-&gt;set_value(CodeOffsets::Value_Entry, _masm-&gt;offset());
 677     if (ro_entry_type == CodeOffsets::Verified_Value_Entry) {
 678       // Special case if we have VVEP == VVEP(RO):
 679       // this means UVEP (called by C1) == UEP (called by C2).
 680       offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());
 681     }
<span class="line-modified"> 682     if (needs_icache(method())) {</span>

 683       check_icache();
 684     }
<span class="line-modified"> 685 </span>
<span class="line-added"> 686     // VVEP: all value parameters are passed as refs - no packing.</span>
 687     emit_std_entry(CodeOffsets::Verified_Value_Entry, NULL);
 688 
 689     if (ro_entry_type != CodeOffsets::Verified_Value_Entry_RO) {
 690       // The VVEP(RO) is the same as VEP or VVEP
 691       assert(ro_entry_type == CodeOffsets::Verified_Entry ||
 692              ro_entry_type == CodeOffsets::Verified_Value_Entry, &quot;must be&quot;);
 693       offsets()-&gt;set_value(CodeOffsets::Verified_Value_Entry_RO,
 694                            offsets()-&gt;value(ro_entry_type));
 695     }
 696   } else {
 697     // All 3 entries are the same (no value-type packing)
 698     offsets()-&gt;set_value(CodeOffsets::Entry, _masm-&gt;offset());
 699     offsets()-&gt;set_value(CodeOffsets::Value_Entry, _masm-&gt;offset());
<span class="line-modified"> 700     if (needs_icache(method())) {</span>
 701       check_icache();
 702     }
<span class="line-modified"> 703     emit_std_entry(CodeOffsets::Verified_Value_Entry, NULL);</span>
<span class="line-modified"> 704     offsets()-&gt;set_value(CodeOffsets::Verified_Entry, offsets()-&gt;value(CodeOffsets::Verified_Value_Entry));</span>
<span class="line-modified"> 705     offsets()-&gt;set_value(CodeOffsets::Verified_Value_Entry_RO, offsets()-&gt;value(CodeOffsets::Verified_Value_Entry));</span>
 706   }
 707 }
 708 
<span class="line-modified"> 709 void LIR_Assembler::emit_std_entry(CodeOffsets::Entries entry, const CompiledEntrySignature* ces) {</span>
 710   offsets()-&gt;set_value(entry, _masm-&gt;offset());
<span class="line-modified"> 711   _masm-&gt;verified_entry();</span>
 712   switch (entry) {
<span class="line-modified"> 713   case CodeOffsets::Verified_Entry: {</span>
<span class="line-modified"> 714     if (needs_clinit_barrier_on_entry(method())) {</span>
<span class="line-modified"> 715       clinit_barrier(method());</span>

 716     }
<span class="line-modified"> 717     int rt_call_offset = _masm-&gt;verified_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()-&gt;sp_offset_for_orig_pc()), _verified_value_entry);</span>
<span class="line-modified"> 718     add_scalarized_entry_info(rt_call_offset);</span>
<span class="line-modified"> 719     break;</span>
<span class="line-modified"> 720   }</span>
<span class="line-modified"> 721   case CodeOffsets::Verified_Value_Entry_RO: {</span>
<span class="line-added"> 722     assert(!needs_clinit_barrier_on_entry(method()), &quot;can&#39;t be static&quot;);</span>
<span class="line-added"> 723     int rt_call_offset = _masm-&gt;verified_value_ro_entry(ces, initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()-&gt;sp_offset_for_orig_pc()), _verified_value_entry);</span>
<span class="line-added"> 724     add_scalarized_entry_info(rt_call_offset);</span>
<span class="line-added"> 725     break;</span>
<span class="line-added"> 726   }</span>
<span class="line-added"> 727   case CodeOffsets::Verified_Value_Entry: {</span>
<span class="line-added"> 728     if (needs_clinit_barrier_on_entry(method())) {</span>
<span class="line-added"> 729       clinit_barrier(method());</span>
 730     }
<span class="line-modified"> 731     build_frame();</span>
<span class="line-added"> 732     offsets()-&gt;set_value(CodeOffsets::Frame_Complete, _masm-&gt;offset());</span>
<span class="line-added"> 733     break;</span>
<span class="line-added"> 734   }</span>
 735   default:
<span class="line-modified"> 736     ShouldNotReachHere();</span>
<span class="line-modified"> 737     break;</span>








 738   }
 739 }
 740 
 741 void LIR_Assembler::emit_op0(LIR_Op0* op) {
 742   switch (op-&gt;code()) {
 743     case lir_word_align: {
 744       _masm-&gt;align(BytesPerWord);
 745       break;
 746     }
 747 
 748     case lir_nop:
 749       assert(op-&gt;info() == NULL, &quot;not supported&quot;);
 750       _masm-&gt;nop();
 751       break;
 752 
 753     case lir_label:
 754       Unimplemented();
 755       break;
 756 
 757     case lir_build_frame:
</pre>
<hr />
<pre>
 796     case lir_membar_storestore:
 797       membar_storestore();
 798       break;
 799 
 800     case lir_membar_loadstore:
 801       membar_loadstore();
 802       break;
 803 
 804     case lir_membar_storeload:
 805       membar_storeload();
 806       break;
 807 
 808     case lir_get_thread:
 809       get_thread(op-&gt;result_opr());
 810       break;
 811 
 812     case lir_on_spin_wait:
 813       on_spin_wait();
 814       break;
 815 
<span class="line-added"> 816     case lir_check_orig_pc:</span>
<span class="line-added"> 817       check_orig_pc();</span>
<span class="line-added"> 818       break;</span>
<span class="line-added"> 819 </span>
 820     default:
 821       ShouldNotReachHere();
 822       break;
 823   }
 824 }
 825 
 826 
 827 void LIR_Assembler::emit_op2(LIR_Op2* op) {
 828   switch (op-&gt;code()) {
 829     case lir_cmp:
 830       if (op-&gt;info() != NULL) {
 831         assert(op-&gt;in_opr1()-&gt;is_address() || op-&gt;in_opr2()-&gt;is_address(),
 832                &quot;shouldn&#39;t be codeemitinfo for non-address operands&quot;);
 833         add_debug_info_for_null_check_here(op-&gt;info()); // exception possible
 834       }
 835       comp_op(op-&gt;condition(), op-&gt;in_opr1(), op-&gt;in_opr2(), op);
 836       break;
 837 
 838     case lir_cmp_l2i:
 839     case lir_cmp_fd2i:
</pre>
<hr />
<pre>
 893         op-&gt;result_opr());
 894       break;
 895 
 896     case lir_throw:
 897       throw_op(op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;info());
 898       break;
 899 
 900     case lir_xadd:
 901     case lir_xchg:
 902       atomic_op(op-&gt;code(), op-&gt;in_opr1(), op-&gt;in_opr2(), op-&gt;result_opr(), op-&gt;tmp1_opr());
 903       break;
 904 
 905     default:
 906       Unimplemented();
 907       break;
 908   }
 909 }
 910 
 911 
 912 void LIR_Assembler::build_frame() {
<span class="line-modified"> 913   _masm-&gt;build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), in_bytes(frame_map()-&gt;sp_offset_for_orig_pc()),</span>
<span class="line-modified"> 914                      compilation()-&gt;needs_stack_repair(), method()-&gt;has_scalarized_args(),</span>
 915                      &amp;_verified_value_entry);
 916 }
 917 
 918 
 919 void LIR_Assembler::roundfp_op(LIR_Opr src, LIR_Opr tmp, LIR_Opr dest, bool pop_fpu_stack) {
 920   assert(strict_fp_requires_explicit_rounding, &quot;not required&quot;);
 921   assert((src-&gt;is_single_fpu() &amp;&amp; dest-&gt;is_single_stack()) ||
 922          (src-&gt;is_double_fpu() &amp;&amp; dest-&gt;is_double_stack()),
 923          &quot;round_fp: rounds register -&gt; stack location&quot;);
 924 
 925   reg2stack (src, dest, src-&gt;type(), pop_fpu_stack);
 926 }
 927 
 928 
 929 void LIR_Assembler::move_op(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool pop_fpu_stack, bool unaligned, bool wide) {
 930   if (src-&gt;is_register()) {
 931     if (dest-&gt;is_register()) {
 932       assert(patch_code == lir_patch_none &amp;&amp; info == NULL, &quot;no patching and info allowed here&quot;);
 933       reg2reg(src,  dest);
 934     } else if (dest-&gt;is_stack()) {
</pre>
</td>
</tr>
</table>
<center><a href="c1_LIR.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="c1_LIRAssembler.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>