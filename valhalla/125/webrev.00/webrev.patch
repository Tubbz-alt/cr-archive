diff a/src/hotspot/cpu/aarch64/c1_LIRGenerator_aarch64.cpp b/src/hotspot/cpu/aarch64/c1_LIRGenerator_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/c1_LIRGenerator_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/c1_LIRGenerator_aarch64.cpp
@@ -1226,17 +1226,17 @@
 
   length.load_item_force(FrameMap::r19_opr);
   LIR_Opr len = length.result();
 
   ciKlass* obj = (ciKlass*) x->exact_type();
-  CodeStub* slow_path = new NewObjectArrayStub(klass_reg, len, reg, info, x->is_never_null());
+  CodeStub* slow_path = new NewObjectArrayStub(klass_reg, len, reg, info, x->is_null_free());
   if (obj == ciEnv::unloaded_ciobjarrayklass()) {
     BAILOUT("encountered unloaded_ciobjarrayklass due to out of memory error");
   }
 
   klass2reg_with_patching(klass_reg, obj, patching_info);
-  if (x->is_never_null()) {
+  if (x->is_null_free()) {
     __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_INLINE_TYPE, klass_reg, slow_path);
   } else {
     __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_OBJECT, klass_reg, slow_path);
   }
 
@@ -1312,11 +1312,11 @@
 
   // info for exceptions
   CodeEmitInfo* info_for_exception =
       (x->needs_exception_state() ? state_for(x) :
                                     state_for(x, x->state_before(), true /*ignore_xhandler*/));
-  if (x->is_never_null()) {
+  if (x->is_null_free()) {
     __ null_check(obj.result(), new CodeEmitInfo(info_for_exception));
   }
 
   CodeStub* stub;
   if (x->is_incompatible_class_change_check()) {
@@ -1338,11 +1338,11 @@
 
 
   __ checkcast(reg, obj.result(), x->klass(),
                new_register(objectType), new_register(objectType), tmp3,
                x->direct_compare(), info_for_exception, patching_info, stub,
-               x->profiled_method(), x->profiled_bci(), x->is_never_null());
+               x->profiled_method(), x->profiled_bci(), x->is_null_free());
 
 }
 
 void LIRGenerator::do_InstanceOf(InstanceOf* x) {
   LIRItem obj(x->obj(), this);
diff a/src/hotspot/cpu/x86/c1_LIRGenerator_x86.cpp b/src/hotspot/cpu/x86/c1_LIRGenerator_x86.cpp
--- a/src/hotspot/cpu/x86/c1_LIRGenerator_x86.cpp
+++ b/src/hotspot/cpu/x86/c1_LIRGenerator_x86.cpp
@@ -1358,16 +1358,16 @@
 
   length.load_item_force(FrameMap::rbx_opr);
   LIR_Opr len = length.result();
 
   ciKlass* obj = (ciKlass*) x->exact_type();
-  CodeStub* slow_path = new NewObjectArrayStub(klass_reg, len, reg, info, x->is_never_null());
+  CodeStub* slow_path = new NewObjectArrayStub(klass_reg, len, reg, info, x->is_null_free());
   if (obj == ciEnv::unloaded_ciobjarrayklass()) {
     BAILOUT("encountered unloaded_ciobjarrayklass due to out of memory error");
   }
   klass2reg_with_patching(klass_reg, obj, patching_info);
-  if (x->is_never_null()) {
+  if (x->is_null_free()) {
     __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_INLINE_TYPE, klass_reg, slow_path);
   } else {
     __ allocate_array(reg, len, tmp1, tmp2, tmp3, tmp4, T_OBJECT, klass_reg, slow_path);
   }
 
@@ -1446,11 +1446,11 @@
   // info for exceptions
   CodeEmitInfo* info_for_exception =
       (x->needs_exception_state() ? state_for(x) :
                                     state_for(x, x->state_before(), true /*ignore_xhandler*/));
 
-  if (x->is_never_null()) {
+  if (x->is_null_free()) {
     __ null_check(obj.result(), new CodeEmitInfo(info_for_exception));
   }
 
   CodeStub* stub;
   if (x->is_incompatible_class_change_check()) {
@@ -1468,11 +1468,11 @@
     tmp3 = new_register(objectType);
   }
   __ checkcast(reg, obj.result(), x->klass(),
                new_register(objectType), new_register(objectType), tmp3,
                x->direct_compare(), info_for_exception, patching_info, stub,
-               x->profiled_method(), x->profiled_bci(), x->is_never_null());
+               x->profiled_method(), x->profiled_bci(), x->is_null_free());
 }
 
 
 void LIRGenerator::do_InstanceOf(InstanceOf* x) {
   LIRItem obj(x->obj(), this);
diff a/src/hotspot/share/c1/c1_GraphBuilder.cpp b/src/hotspot/share/c1/c1_GraphBuilder.cpp
--- a/src/hotspot/share/c1/c1_GraphBuilder.cpp
+++ b/src/hotspot/share/c1/c1_GraphBuilder.cpp
@@ -2416,13 +2416,13 @@
 
 
 void GraphBuilder::new_object_array() {
   bool will_link;
   ciKlass* klass = stream()->get_klass(will_link);
-  bool never_null = stream()->is_inline_klass();
+  bool null_free = stream()->is_inline_klass();
   ValueStack* state_before = !klass->is_loaded() || PatchALot ? copy_state_before() : copy_state_exhandling();
-  NewArray* n = new NewObjectArray(klass, ipop(), state_before, never_null);
+  NewArray* n = new NewObjectArray(klass, ipop(), state_before, null_free);
   apush(append_split(n));
 }
 
 
 bool GraphBuilder::direct_compare(ciKlass* k) {
@@ -2443,13 +2443,13 @@
 
 
 void GraphBuilder::check_cast(int klass_index) {
   bool will_link;
   ciKlass* klass = stream()->get_klass(will_link);
-  bool never_null = stream()->is_inline_klass();
+  bool null_free = stream()->is_inline_klass();
   ValueStack* state_before = !klass->is_loaded() || PatchALot ? copy_state_before() : copy_state_for_exception();
-  CheckCast* c = new CheckCast(klass, apop(), state_before, never_null);
+  CheckCast* c = new CheckCast(klass, apop(), state_before, null_free);
   apush(append_split(c));
   c->set_direct_compare(direct_compare(klass));
 
   if (is_profiling()) {
     // Note that we'd collect profile data in this method if we wanted it.
@@ -3455,11 +3455,11 @@
   // Set up locals for receiver
   int idx = 0;
   if (!method()->is_static()) {
     // we should always see the receiver
     state->store_local(idx, new Local(method()->holder(), objectType, idx,
-             /*receiver*/ true, /*never_null*/ method()->holder()->is_flat_array_klass()));
+             /*receiver*/ true, /*null_free*/ method()->holder()->is_flat_array_klass()));
     idx = 1;
   }
 
   // Set up locals for incoming arguments
   ciSignature* sig = method()->signature();
diff a/src/hotspot/share/c1/c1_Instruction.cpp b/src/hotspot/share/c1/c1_Instruction.cpp
--- a/src/hotspot/share/c1/c1_Instruction.cpp
+++ b/src/hotspot/share/c1/c1_Instruction.cpp
@@ -421,22 +421,22 @@
 
 // Implementation of Invoke
 
 
 Invoke::Invoke(Bytecodes::Code code, ValueType* result_type, Value recv, Values* args,
-               int vtable_index, ciMethod* target, ValueStack* state_before, bool never_null)
+               int vtable_index, ciMethod* target, ValueStack* state_before, bool null_free)
   : StateSplit(result_type, state_before)
   , _code(code)
   , _recv(recv)
   , _args(args)
   , _vtable_index(vtable_index)
   , _target(target)
 {
   set_flag(TargetIsLoadedFlag,   target->is_loaded());
   set_flag(TargetIsFinalFlag,    target_is_loaded() && target->is_final_method());
   set_flag(TargetIsStrictfpFlag, target_is_loaded() && target->is_strict());
-  set_never_null(never_null);
+  set_null_free(null_free);
 
   assert(args != NULL, "args must exist");
 #ifdef ASSERT
   AssertValues assert_value;
   values_do(&assert_value);
diff a/src/hotspot/share/c1/c1_Instruction.hpp b/src/hotspot/share/c1/c1_Instruction.hpp
--- a/src/hotspot/share/c1/c1_Instruction.hpp
+++ b/src/hotspot/share/c1/c1_Instruction.hpp
@@ -470,12 +470,12 @@
   Instruction* subst()                           { return _subst == NULL ? this : _subst->subst(); }
   LIR_Opr operand() const                        { return _operand; }
 
   void set_needs_null_check(bool f)              { set_flag(NeedsNullCheckFlag, f); }
   bool needs_null_check() const                  { return check_flag(NeedsNullCheckFlag); }
-  void set_never_null(bool f)                    { set_flag(NeverNullFlag, f); }
-  bool is_never_null() const                     { return check_flag(NeverNullFlag); }
+  void set_null_free(bool f)                    { set_flag(NeverNullFlag, f); }
+  bool is_null_free() const                     { return check_flag(NeverNullFlag); }
   bool is_linked() const                         { return check_flag(IsLinkedInBlockFlag); }
   bool can_be_linked()                           { return as_Local() == NULL && as_Phi() == NULL; }
 
   bool is_null_obj()                             { return as_Constant() != NULL && type()->as_ObjectType()->constant_value()->is_null_object(); }
 
@@ -740,17 +740,17 @@
   int      _java_index;                          // the local index within the method to which the local belongs
   bool     _is_receiver;                         // if local variable holds the receiver: "this" for non-static methods
   ciType*  _declared_type;
  public:
   // creation
-  Local(ciType* declared, ValueType* type, int index, bool receiver, bool never_null)
+  Local(ciType* declared, ValueType* type, int index, bool receiver, bool null_free)
     : Instruction(type)
     , _java_index(index)
     , _is_receiver(receiver)
     , _declared_type(declared)
   {
-    set_never_null(never_null);
+    set_null_free(null_free);
     NOT_PRODUCT(set_printable_bci(-1));
   }
 
   // accessors
   int java_index() const                         { return _java_index; }
@@ -870,11 +870,11 @@
   LoadField(Value obj, int offset, ciField* field, bool is_static,
             ValueStack* state_before, bool needs_patching,
             ciInlineKlass* inline_klass = NULL, Value default_value = NULL )
   : AccessField(obj, offset, field, is_static, state_before, needs_patching)
   {
-    set_never_null(field->signature()->is_Q_signature());
+    set_null_free(field->signature()->is_Q_signature());
   }
 
   ciType* declared_type() const;
 
   // generic; cannot be eliminated if needs patching or if volatile.
@@ -1308,11 +1308,11 @@
   ciMethod*       _target;
 
  public:
   // creation
   Invoke(Bytecodes::Code code, ValueType* result_type, Value recv, Values* args,
-         int vtable_index, ciMethod* target, ValueStack* state_before, bool never_null);
+         int vtable_index, ciMethod* target, ValueStack* state_before, bool null_free);
 
   // accessors
   Bytecodes::Code code() const                   { return _code; }
   Value receiver() const                         { return _recv; }
   bool has_receiver() const                      { return receiver() != NULL; }
@@ -1390,11 +1390,11 @@
     if (depends_on == NULL) {
       _depends_on = this;
     } else {
       _depends_on = depends_on;
     }
-    set_never_null(true);
+    set_null_free(true);
   }
 
   // accessors
   bool is_unresolved() const                     { return _is_unresolved; }
   Value depends_on();
@@ -1475,13 +1475,13 @@
  private:
   ciKlass* _klass;
 
  public:
   // creation
-  NewObjectArray(ciKlass* klass, Value length, ValueStack* state_before, bool never_null)
+  NewObjectArray(ciKlass* klass, Value length, ValueStack* state_before, bool null_free)
   : NewArray(length, state_before), _klass(klass) {
-    set_never_null(never_null);
+    set_null_free(null_free);
   }
 
   // accessors
   ciKlass* klass() const                         { return _klass; }
   ciType* exact_type() const;
@@ -1574,13 +1574,13 @@
 
 
 LEAF(CheckCast, TypeCheck)
  public:
   // creation
-  CheckCast(ciKlass* klass, Value obj, ValueStack* state_before, bool never_null = false)
+  CheckCast(ciKlass* klass, Value obj, ValueStack* state_before, bool null_free = false)
   : TypeCheck(klass, obj, objectType, state_before) {
-    set_never_null(never_null);
+    set_null_free(null_free);
   }
 
   void set_incompatible_class_change_check() {
     set_flag(ThrowIncompatibleClassChangeErrorFlag, true);
   }
diff a/src/hotspot/share/c1/c1_LIR.cpp b/src/hotspot/share/c1/c1_LIR.cpp
--- a/src/hotspot/share/c1/c1_LIR.cpp
+++ b/src/hotspot/share/c1/c1_LIR.cpp
@@ -1504,14 +1504,14 @@
 
 
 void LIR_List::checkcast (LIR_Opr result, LIR_Opr object, ciKlass* klass,
                           LIR_Opr tmp1, LIR_Opr tmp2, LIR_Opr tmp3, bool fast_check,
                           CodeEmitInfo* info_for_exception, CodeEmitInfo* info_for_patch, CodeStub* stub,
-                          ciMethod* profiled_method, int profiled_bci, bool is_never_null) {
+                          ciMethod* profiled_method, int profiled_bci, bool is_null_free) {
   // If klass is non-nullable,  LIRGenerator::do_CheckCast has already performed null-check
   // on the object.
-  bool need_null_check = !is_never_null;
+  bool need_null_check = !is_null_free;
   LIR_OpTypeCheck* c = new LIR_OpTypeCheck(lir_checkcast, result, object, klass,
                                            tmp1, tmp2, tmp3, fast_check, info_for_exception, info_for_patch, stub,
                                            need_null_check);
   if (profiled_method != NULL) {
     c->set_profiled_method(profiled_method);
diff a/src/hotspot/share/c1/c1_LIR.hpp b/src/hotspot/share/c1/c1_LIR.hpp
--- a/src/hotspot/share/c1/c1_LIR.hpp
+++ b/src/hotspot/share/c1/c1_LIR.hpp
@@ -2339,11 +2339,11 @@
                               CodeEmitInfo* info, CodeStub* stub);
 
   void checkcast (LIR_Opr result, LIR_Opr object, ciKlass* klass,
                   LIR_Opr tmp1, LIR_Opr tmp2, LIR_Opr tmp3, bool fast_check,
                   CodeEmitInfo* info_for_exception, CodeEmitInfo* info_for_patch, CodeStub* stub,
-                  ciMethod* profiled_method, int profiled_bci, bool is_never_null);
+                  ciMethod* profiled_method, int profiled_bci, bool is_null_free);
   // MethodData* profiling
   void profile_call(ciMethod* method, int bci, ciMethod* callee, LIR_Opr mdo, LIR_Opr recv, LIR_Opr t1, ciKlass* cha_klass) {
     append(new LIR_OpProfileCall(method, bci, callee, mdo, recv, t1, cha_klass));
   }
   void profile_type(LIR_Address* mdp, LIR_Opr obj, ciKlass* exact_klass, intptr_t current_klass, LIR_Opr tmp, bool not_null, bool no_conflict) {
diff a/src/hotspot/share/c1/c1_LIRGenerator.cpp b/src/hotspot/share/c1/c1_LIRGenerator.cpp
--- a/src/hotspot/share/c1/c1_LIRGenerator.cpp
+++ b/src/hotspot/share/c1/c1_LIRGenerator.cpp
@@ -1779,11 +1779,11 @@
     CodeEmitInfo* store_check_info = new CodeEmitInfo(range_check_info);
     array_store_check(value.result(), array.result(), store_check_info, NULL, -1);
   }
 
   if (is_loaded_flattened_array) {
-    if (!x->value()->is_never_null()) {
+    if (!x->value()->is_null_free()) {
       __ null_check(value.result(), new CodeEmitInfo(range_check_info));
     }
     access_flattened_array(false, array, index, value);
   } else {
     StoreFlattenedArrayStub* slow_path = NULL;
diff a/src/hotspot/share/ci/ciFlatArrayKlass.cpp b/src/hotspot/share/ci/ciFlatArrayKlass.cpp
--- a/src/hotspot/share/ci/ciFlatArrayKlass.cpp
+++ b/src/hotspot/share/ci/ciFlatArrayKlass.cpp
@@ -24,10 +24,11 @@
 
 #include "precompiled.hpp"
 #include "ci/ciFlatArrayKlass.hpp"
 #include "ci/ciInlineKlass.hpp"
 #include "ci/ciInstanceKlass.hpp"
+#include "ci/ciObjArrayKlass.hpp"
 #include "ci/ciSymbol.hpp"
 #include "ci/ciUtilities.hpp"
 #include "ci/ciUtilities.inline.hpp"
 #include "oops/flatArrayKlass.hpp"
 
@@ -124,34 +125,33 @@
 
 // ------------------------------------------------------------------
 // ciFlatArrayKlass::make_impl
 //
 // Implementation of make.
-ciFlatArrayKlass* ciFlatArrayKlass::make_impl(ciKlass* element_klass) {
-  assert(UseFlatArray, "should only be used for arrays");
-  assert(element_klass->is_inlinetype(), "element type must be inline type");
-  assert(element_klass->is_loaded(), "unloaded Q klasses are represented by ciInstanceKlass");
+ciArrayKlass* ciFlatArrayKlass::make_impl(ciKlass* element_klass) {
+  assert(UseFlatArray, "should only be used for flat arrays");
+  assert(element_klass->is_inlinetype(), "element type must be an inline type");
+  assert(element_klass->is_loaded(), "unloaded inline klasses are represented by ciInstanceKlass");
   {
     EXCEPTION_CONTEXT;
-    // The element klass is loaded
     Klass* array = element_klass->get_Klass()->array_klass(THREAD);
     if (HAS_PENDING_EXCEPTION) {
       CLEAR_PENDING_EXCEPTION;
       CURRENT_THREAD_ENV->record_out_of_memory_failure();
-      // TODO handle this
-      guarantee(false, "out of memory");
-      return NULL;
+      // Use unloaded ciObjArrayKlass here because flatArrayKlasses are always loaded
+      // and since this is only used for OOM detection, the actual type does not matter.
+      return ciEnv::unloaded_ciobjarrayklass();
     }
     return CURRENT_THREAD_ENV->get_flat_array_klass(array);
   }
 }
 
 // ------------------------------------------------------------------
 // ciFlatArrayKlass::make
 //
 // Make an array klass corresponding to the specified primitive type.
-ciFlatArrayKlass* ciFlatArrayKlass::make(ciKlass* element_klass) {
+ciArrayKlass* ciFlatArrayKlass::make(ciKlass* element_klass) {
   GUARDED_VM_ENTRY(return make_impl(element_klass);)
 }
 
 ciKlass* ciFlatArrayKlass::exact_klass() {
   assert(element_klass()->is_loaded() && element_klass()->as_inline_klass()->exact_klass() != NULL, "must have exact klass");
diff a/src/hotspot/share/ci/ciFlatArrayKlass.hpp b/src/hotspot/share/ci/ciFlatArrayKlass.hpp
--- a/src/hotspot/share/ci/ciFlatArrayKlass.hpp
+++ b/src/hotspot/share/ci/ciFlatArrayKlass.hpp
@@ -47,11 +47,11 @@
 
   FlatArrayKlass* get_FlatArrayKlass() {
     return (FlatArrayKlass*)get_Klass();
   }
 
-  static ciFlatArrayKlass* make_impl(ciKlass* element_klass);
+  static ciArrayKlass* make_impl(ciKlass* element_klass);
   static ciSymbol* construct_array_name(ciSymbol* element_name,
                                         int       dimension);
 
   const char* type_string() { return "ciFlatArrayKlass"; }
 
@@ -75,11 +75,11 @@
   ciKlass* base_element_klass() { return _base_element_klass; }
 
   // What kind of ciObject is this?
   bool is_flat_array_klass() const { return true; }
 
-  static ciFlatArrayKlass* make(ciKlass* element_klass);
+  static ciArrayKlass* make(ciKlass* element_klass);
 
   virtual ciKlass* exact_klass();
 
   virtual bool can_be_inline_array_klass() {
     return true;
diff a/src/hotspot/share/ci/ciInstanceKlass.cpp b/src/hotspot/share/ci/ciInstanceKlass.cpp
--- a/src/hotspot/share/ci/ciInstanceKlass.cpp
+++ b/src/hotspot/share/ci/ciInstanceKlass.cpp
@@ -674,32 +674,18 @@
 
 bool ciInstanceKlass::can_be_inline_klass(bool is_exact) {
   if (!EnableValhalla) {
     return false;
   }
-  if (!is_loaded() ||   // Not loaded, might be an inline klass
-      is_inlinetype() || // Known to be an inline klass
-      // Non-exact j.l.Object or interface klass
-      ((is_java_lang_Object() || is_interface()) && !is_exact)) {
+  if (!is_loaded() || is_inlinetype()) {
+    // Not loaded or known to be an inline klass
     return true;
   }
-  if (is_abstract() && !is_exact && !has_nonstatic_fields()) {
-    // TODO Factor out and re-use similar code from the ClassFileParser
-    // An abstract class can only be implemented by an inline type if it has no instance
-    // fields, no synchronized instance methods and an empty, no-arg constructor.
+  if (!is_exact) {
+    // Not exact, check if this is a valid super for an inline klass
     VM_ENTRY_MARK;
-    Array<Method*>* methods = get_instanceKlass()->methods();
-    for (int i = 0; i < methods->length(); i++) {
-      Method* m = methods->at(i);
-      if ((m->is_synchronized() && !m->is_static()) ||
-          (m->is_object_constructor() &&
-           (m->signature() != vmSymbols::void_method_signature() ||
-            !m->is_vanilla_constructor()))) {
-        return false;
-      }
-    }
-    return true;
+    return !get_instanceKlass()->invalid_inline_super();
   }
   return false;
 }
 
 ciInstanceKlass* ciInstanceKlass::unsafe_anonymous_host() {
diff a/src/hotspot/share/opto/graphKit.cpp b/src/hotspot/share/opto/graphKit.cpp
--- a/src/hotspot/share/opto/graphKit.cpp
+++ b/src/hotspot/share/opto/graphKit.cpp
@@ -1397,10 +1397,11 @@
 
   return value;
 }
 
 Node* GraphKit::null2default(Node* value, ciInlineKlass* vk) {
+  assert(!vk->is_scalarizable(), "Should only be used for non scalarizable inline klasses");
   Node* null_ctl = top();
   value = null_check_oop(value, &null_ctl);
   if (!null_ctl->is_top()) {
     // Return default value if oop is null
     Node* region = new RegionNode(3);
@@ -1429,11 +1430,11 @@
   cast->init_req(0, control());
   cast = _gvn.transform( cast );
 
   if (t->is_inlinetypeptr() && t->inline_klass()->is_scalarizable()) {
     // Scalarize inline type now that we know it's non-null
-    cast = InlineTypeNode::make_from_oop(this, cast, t->inline_klass())->buffer(this, false);
+    cast = InlineTypeNode::make_from_oop(this, cast, t->inline_klass())->as_ptr(&gvn());
   }
 
   // Scan for instances of 'obj' in the current JVM mapping.
   // These instances are known to be not-null after the test.
   if (do_replace_in_map)
diff a/src/hotspot/share/opto/inlinetypenode.cpp b/src/hotspot/share/opto/inlinetypenode.cpp
--- a/src/hotspot/share/opto/inlinetypenode.cpp
+++ b/src/hotspot/share/opto/inlinetypenode.cpp
@@ -363,11 +363,11 @@
   // Check if inline type is already allocated
   Node* null_ctl = kit->top();
   Node* not_null_oop = kit->null_check_oop(get_oop(), &null_ctl);
   if (null_ctl->is_top()) {
     // Inline type is allocated
-    return kit->gvn().transform(new InlineTypePtrNode(this))->as_InlineTypePtr();
+    return as_ptr(&kit->gvn());
   }
   assert(!is_allocated(&kit->gvn()), "should not be allocated");
   RegionNode* region = new RegionNode(3);
 
   // Oop is non-NULL, use it
@@ -422,20 +422,27 @@
     kit->replace_in_map(this, vt);
   }
   // InlineTypeNode::remove_redundant_allocations piggybacks on split if.
   // Make sure it gets a chance to remove this allocation.
   kit->C->set_has_split_ifs(true);
-  assert(vt->is_allocated(&kit->gvn()), "must be allocated");
-  return kit->gvn().transform(new InlineTypePtrNode(vt))->as_InlineTypePtr();
+  return vt->as_ptr(&kit->gvn());
 }
 
 bool InlineTypeBaseNode::is_allocated(PhaseGVN* phase) const {
   Node* oop = get_oop();
   const Type* oop_type = (phase != NULL) ? phase->type(oop) : oop->bottom_type();
   return !oop_type->maybe_null();
 }
 
+InlineTypePtrNode* InlineTypeBaseNode::as_ptr(PhaseGVN* phase) const {
+  assert(is_allocated(phase), "must be allocated");
+  if (is_InlineTypePtr()) {
+    return as_InlineTypePtr();
+  }
+  return phase->transform(new InlineTypePtrNode(this))->as_InlineTypePtr();
+}
+
 // When a call returns multiple values, it has several result
 // projections, one per field. Replacing the result of the call by a
 // inline type node (after late inlining) requires that for each result
 // projection, we find the corresponding inline type field.
 void InlineTypeBaseNode::replace_call_results(GraphKit* kit, Node* call, Compile* C) {
@@ -501,31 +508,31 @@
   for (uint i = 0; i < vt->field_count(); ++i) {
     ciType* field_type = vt->field_type(i);
     Node* value = NULL;
     if (field_type->is_inlinetype()) {
       ciInlineKlass* field_klass = field_type->as_inline_klass();
-      if (field_klass->is_scalarizable() || vt->field_is_flattened(i)) {
+      if (field_klass->is_scalarizable()) {
         value = InlineTypeNode::make_default(gvn, field_klass);
       } else {
         value = default_oop(gvn, field_klass);
       }
     } else {
       value = gvn.zerocon(field_type->basic_type());
     }
     vt->set_field_value(i, value);
   }
   vt = gvn.transform(vt)->as_InlineType();
-  assert(vt->is_default(gvn), "must be the default inline type");
+  assert(vt->is_default(&gvn), "must be the default inline type");
   return vt;
 }
 
-bool InlineTypeNode::is_default(PhaseGVN& gvn) const {
+bool InlineTypeNode::is_default(PhaseGVN* gvn) const {
   for (uint i = 0; i < field_count(); ++i) {
     Node* value = field_value(i);
-    if (!gvn.type(value)->is_zero_type() &&
+    if (!gvn->type(value)->is_zero_type() &&
         !(value->is_InlineType() && value->as_InlineType()->is_default(gvn)) &&
-        !(field_type(i)->is_inlinetype() && value == default_oop(gvn, field_type(i)->as_inline_klass()))) {
+        !(field_type(i)->is_inlinetype() && value == default_oop(*gvn, field_type(i)->as_inline_klass()))) {
       return false;
     }
   }
   return true;
 }
@@ -774,15 +781,33 @@
     set_field_value(idx, parm);
     gvn.record_for_igvn(parm);
   }
 }
 
+// Replace a buffer allocation by a dominating allocation
+static void replace_allocation(PhaseIterGVN* igvn, Node* res, Node* dom) {
+  // Remove initializing stores
+  for (DUIterator_Fast imax, i = res->fast_outs(imax); i < imax; i++) {
+    AddPNode* addp = res->fast_out(i)->isa_AddP();
+    if (addp != NULL) {
+      for (DUIterator_Fast jmax, j = addp->fast_outs(jmax); j < jmax; j++) {
+        StoreNode* store = addp->fast_out(j)->isa_Store();
+        if (store != NULL) {
+          igvn->replace_in_uses(store, store->in(MemNode::Memory));
+        }
+      }
+    }
+  }
+  igvn->replace_node(res, dom);
+}
+
 Node* InlineTypeNode::Ideal(PhaseGVN* phase, bool can_reshape) {
   Node* oop = get_oop();
-  if (is_default(*phase) && (!oop->is_Con() || phase->type(oop)->is_zero_type())) {
+  if (is_default(phase) && (!oop->is_Con() || phase->type(oop)->is_zero_type())) {
     // Use the pre-allocated oop for default inline types
     set_oop(default_oop(*phase, inline_klass()));
+    assert(is_allocated(phase), "should now be allocated");
     return this;
   } else if (oop->isa_InlineTypePtr()) {
     // Can happen with late inlining
     InlineTypePtrNode* vtptr = oop->as_InlineTypePtr();
     set_oop(vtptr->get_oop());
@@ -804,55 +829,25 @@
   }
 
   if (can_reshape) {
     PhaseIterGVN* igvn = phase->is_IterGVN();
 
-    if (is_default(*phase)) {
-      // Search for users of the default inline type
+    if (is_allocated(phase)) {
+      // Search for and remove re-allocations of this inline type.
+      // This can happen with late inlining when we first allocate an inline type argument
+      // but later decide to inline the call after the callee code also triggered allocation.
       for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {
-        Node* user = fast_out(i);
-        AllocateNode* alloc = user->isa_Allocate();
+        AllocateNode* alloc = fast_out(i)->isa_Allocate();
         if (alloc != NULL && alloc->in(AllocateNode::InlineTypeNode) == this) {
-          // Found an allocation of the default inline type.
+          // Found a re-allocation
           Node* res = alloc->result_cast();
-          if (res != NULL) {
-            // If the code in StoreNode::Identity() that removes useless stores was not yet
-            // executed or ReduceFieldZeroing is disabled, there can still be initializing
-            // stores (only zero-type or default value stores, because inline types are immutable).
-            for (DUIterator_Fast jmax, j = res->fast_outs(jmax); j < jmax; j++) {
-              AddPNode* addp = res->fast_out(j)->isa_AddP();
-              if (addp != NULL) {
-                for (DUIterator_Fast kmax, k = addp->fast_outs(kmax); k < kmax; k++) {
-                  StoreNode* store = addp->fast_out(k)->isa_Store();
-                  if (store != NULL && store->outcnt() != 0) {
-                    // Remove the useless store
-                    igvn->replace_in_uses(store, store->in(MemNode::Memory));
-                  }
-                }
-              }
-            }
-            // Replace allocation by pre-allocated oop
-            igvn->replace_node(res, default_oop(*phase, inline_klass()));
+          if (res != NULL && res->is_CheckCastPP()) {
+            // Replace allocation by oop and unlink AllocateNode
+            replace_allocation(igvn, res, get_oop());
+            igvn->replace_input_of(alloc, AllocateNode::InlineTypeNode, igvn->C->top());
+            --i; --imax;
           }
-          // Unlink AllocateNode
-          igvn->replace_input_of(alloc, AllocateNode::InlineTypeNode, igvn->C->top());
-          --i; --imax;
-        } else if (user->is_InlineType()) {
-          // Add inline type user to worklist to give it a chance to get optimized as well
-          igvn->_worklist.push(user);
-        }
-      }
-    }
-
-    if (is_allocated(igvn)) {
-      // Inline type is heap allocated, search for safepoint uses
-      for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {
-        Node* out = fast_out(i);
-        if (out->is_SafePoint()) {
-          // Let SafePointNode::Ideal() take care of re-wiring the
-          // safepoint to the oop input instead of the inline type node.
-          igvn->rehash_node_delayed(out);
         }
       }
     }
   }
   return NULL;
@@ -864,37 +859,30 @@
 void InlineTypeNode::remove_redundant_allocations(PhaseIterGVN* igvn, PhaseIdealLoop* phase) {
   // Search for allocations of this inline type
   for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {
     AllocateNode* alloc = fast_out(i)->isa_Allocate();
     if (alloc != NULL && alloc->in(AllocateNode::InlineTypeNode) == this) {
-      assert(!is_default(*igvn), "default inline type allocation");
       Node* res = alloc->result_cast();
       if (res == NULL || !res->is_CheckCastPP()) {
         break; // No unique CheckCastPP
       }
+      assert(!is_default(igvn) && !is_allocated(igvn), "re-allocation should be removed by Ideal transformation");
+      // Search for a dominating allocation of the same inline type
       Node* res_dom = res;
-      if (is_allocated(igvn)) {
-        // The inline type is already allocated but still connected to an AllocateNode.
-        // This can happen with late inlining when we first allocate an inline type argument
-        // but later decide to inline the call with the callee code also allocating.
-        res_dom = get_oop();
-      } else {
-        // Search for a dominating allocation of the same inline type
-        for (DUIterator_Fast jmax, j = fast_outs(jmax); j < jmax; j++) {
-          AllocateNode* alloc_other = fast_out(j)->isa_Allocate();
-          if (alloc_other != NULL && alloc_other->in(AllocateNode::InlineTypeNode) == this) {
-            Node* res_other = alloc_other->result_cast();
-            if (res_other != NULL && res_other->is_CheckCastPP() && res_other != res_dom &&
-                phase->is_dominator(res_other->in(0), res_dom->in(0))) {
-              res_dom = res_other;
-            }
+      for (DUIterator_Fast jmax, j = fast_outs(jmax); j < jmax; j++) {
+        AllocateNode* alloc_other = fast_out(j)->isa_Allocate();
+        if (alloc_other != NULL && alloc_other->in(AllocateNode::InlineTypeNode) == this) {
+          Node* res_other = alloc_other->result_cast();
+          if (res_other != NULL && res_other->is_CheckCastPP() && res_other != res_dom &&
+              phase->is_dominator(res_other->in(0), res_dom->in(0))) {
+            res_dom = res_other;
           }
         }
       }
       if (res_dom != res) {
-        // Move users to dominating allocation
-        igvn->replace_node(res, res_dom);
+        // Replace allocation by dominating one.
+        replace_allocation(igvn, res, res_dom);
         // The result of the dominated allocation is now unused and will be removed
         // later in PhaseMacroExpand::eliminate_allocate_node to not confuse loop opts.
         igvn->record_for_igvn(alloc);
       }
     }
diff a/src/hotspot/share/opto/inlinetypenode.hpp b/src/hotspot/share/opto/inlinetypenode.hpp
--- a/src/hotspot/share/opto/inlinetypenode.hpp
+++ b/src/hotspot/share/opto/inlinetypenode.hpp
@@ -86,10 +86,11 @@
   void load(GraphKit* kit, Node* base, Node* ptr, ciInstanceKlass* holder, int holder_offset = 0, DecoratorSet decorators = IN_HEAP | MO_UNORDERED);
 
   // Allocates the inline type (if not yet allocated)
   InlineTypePtrNode* buffer(GraphKit* kit, bool safe_for_replace = true);
   bool is_allocated(PhaseGVN* phase) const;
+  InlineTypePtrNode* as_ptr(PhaseGVN* phase) const;
 
   void replace_call_results(GraphKit* kit, Node* call, Compile* C);
 
   // Allocate all non-flattened inline type fields
   Node* allocate_fields(GraphKit* kit);
@@ -109,11 +110,11 @@
 
   // Checks if the inline type is loaded from memory and if so returns the oop
   Node* is_loaded(PhaseGVN* phase, ciInlineKlass* vk = NULL, Node* base = NULL, int holder_offset = 0);
 
   // Checks if the inline type fields are all set to default values
-  bool is_default(PhaseGVN& gvn) const;
+  bool is_default(PhaseGVN* gvn) const;
 
   const TypeInstPtr* inline_ptr() const { return TypeInstPtr::make(TypePtr::BotPTR, inline_klass()); }
 
 public:
   // Create uninitialized
@@ -154,11 +155,11 @@
 class InlineTypePtrNode : public InlineTypeBaseNode {
   friend class InlineTypeBaseNode;
 private:
   const TypeInstPtr* inline_ptr() const { return type()->isa_instptr(); }
 
-  InlineTypePtrNode(InlineTypeBaseNode* vt)
+  InlineTypePtrNode(const InlineTypeBaseNode* vt)
     : InlineTypeBaseNode(TypeInstPtr::make(TypePtr::NotNull, vt->type()->inline_klass()), vt->req()) {
     init_class_id(Class_InlineTypePtr);
     init_req(Oop, vt->get_oop());
     for (uint i = Oop+1; i < vt->req(); i++) {
       init_req(i, vt->in(i));
diff a/src/hotspot/share/opto/loopopts.cpp b/src/hotspot/share/opto/loopopts.cpp
--- a/src/hotspot/share/opto/loopopts.cpp
+++ b/src/hotspot/share/opto/loopopts.cpp
@@ -1210,11 +1210,11 @@
   return out_le;
 }
 
 bool PhaseIdealLoop::flatten_array_element_type_check(Node *n) {
   // If the CmpP is a subtype check for a value that has just been
-  // loaded from an array, the subtype checks guarantees the value
+  // loaded from an array, the subtype check guarantees the value
   // can't be stored in a flattened array and the load of the value
   // happens with a flattened array check then: push the type check
   // through the phi of the flattened array check. This needs special
   // logic because the subtype check's input is not a phi but a
   // LoadKlass that must first be cloned through the phi.
@@ -1284,11 +1284,11 @@
     klassptr_clone->set_req(2, addr_clone);
     register_new_node(klassptr_clone, ctrl);
     _igvn.set_type(klassptr_clone, klassptr_clone->Value(&_igvn));
     if (klassptr != n->in(1)) {
       Node* decode = n->in(1);
-      assert(decode->is_DecodeNarrowPtr(), "inconcistent subgraph");
+      assert(decode->is_DecodeNarrowPtr(), "inconsistent subgraph");
       Node* decode_clone = decode->clone();
       decode_clone->set_req(1, klassptr_clone);
       register_new_node(decode_clone, ctrl);
       _igvn.set_type(decode_clone, decode_clone->Value(&_igvn));
       klassptr_clone = decode_clone;
diff a/src/hotspot/share/opto/parse1.cpp b/src/hotspot/share/opto/parse1.cpp
--- a/src/hotspot/share/opto/parse1.cpp
+++ b/src/hotspot/share/opto/parse1.cpp
@@ -873,11 +873,10 @@
       // field of the inline type. Build InlineTypeNodes from the inline type arguments.
       GraphKit kit(jvms, &gvn);
       kit.set_control(map->control());
       Node* old_mem = map->memory();
       // Use immutable memory for inline type loads and restore it below
-      // TODO make sure inline types are always loaded from immutable memory
       kit.set_all_memory(C->immutable_memory());
       parm = InlineTypeNode::make_from_multi(&kit, start, sig_cc, t->inline_klass(), j, true);
       map->set_control(kit.control());
       map->set_memory(old_mem);
     } else {
diff a/src/hotspot/share/opto/phaseX.cpp b/src/hotspot/share/opto/phaseX.cpp
--- a/src/hotspot/share/opto/phaseX.cpp
+++ b/src/hotspot/share/opto/phaseX.cpp
@@ -1604,10 +1604,19 @@
           }
         }
       }
     }
 
+    // Inline type nodes can have other inline types as users. If an input gets
+    // updated, make sure that inline type users get a chance for optimization.
+    if (use->is_InlineTypeBase()) {
+      for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {
+        Node* u = use->fast_out(i2);
+        if (u->is_InlineTypeBase())
+          _worklist.push(u);
+      }
+    }
     // If changed Cast input, check Phi users for simple cycles
     if (use->is_ConstraintCast()) {
       for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {
         Node* u = use->fast_out(i2);
         if (u->is_Phi())
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestArrays.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestArrays.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestArrays.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestArrays.java
@@ -28,10 +28,11 @@
 import java.lang.reflect.Method;
 import java.util.Arrays;
 
 /*
  * @test
+ * @key randomness
  * @summary Test inline type arrays
  * @library /testlibrary /test/lib /compiler/whitebox /
  * @requires (os.simpleArch == "x64" | os.simpleArch == "aarch64")
  * @compile TestArrays.java
  * @run driver ClassFileInstaller sun.hotspot.WhiteBox jdk.test.lib.Platform
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestBasicFunctionality.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestBasicFunctionality.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestBasicFunctionality.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestBasicFunctionality.java
@@ -25,10 +25,11 @@
 
 import jdk.test.lib.Asserts;
 
 /*
  * @test
+ * @key randomness
  * @summary Test the basic inline type implementation in C2
  * @library /testlibrary /test/lib /compiler/whitebox /
  * @requires os.simpleArch == "x64"
  * @compile TestBasicFunctionality.java
  * @run driver ClassFileInstaller sun.hotspot.WhiteBox jdk.test.lib.Platform
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestBimorphicInlining.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestBimorphicInlining.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestBimorphicInlining.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestBimorphicInlining.java
@@ -26,10 +26,11 @@
 import java.util.Random;
 import jdk.test.lib.Asserts;
 
 /**
  * @test
+ * @key randomness
  * @bug 8209009
  * @summary Test bimorphic inlining with inline type receivers.
  * @library /testlibrary /test/lib
  * @run main/othervm -Xbatch -XX:TypeProfileLevel=222
  *                   -XX:CompileCommand=compileonly,compiler.valhalla.inlinetypes.TestBimorphicInlining::test*
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestBufferTearing.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestBufferTearing.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestBufferTearing.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestBufferTearing.java
@@ -30,10 +30,11 @@
 import jdk.test.lib.Asserts;
 import jdk.internal.misc.Unsafe;
 
 /**
  * @test TestBufferTearing
+ * @key randomness
  * @summary Detect tearing on inline type buffer writes due to missing barriers.
  * @library /testlibrary /test/lib /compiler/whitebox /
  * @modules java.base/jdk.internal.misc
  * @run main/othervm -XX:InlineFieldMaxFlatSize=0 -XX:FlatArrayElementMaxSize=0
  *                   -XX:+UnlockDiagnosticVMOptions -XX:+StressGCM -XX:+StressLCM
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestC1.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestC1.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestC1.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestC1.java
@@ -31,10 +31,11 @@
 import jdk.experimental.value.MethodHandleBuilder;
 import jdk.test.lib.Asserts;
 
 /*
  * @test
+ * @key randomness
  * @summary Various tests that are specific for C1.
  * @modules java.base/jdk.experimental.value
  * @library /testlibrary /test/lib /compiler/whitebox /
  * @requires os.simpleArch == "x64"
  * @compile -XDallowWithFieldOperator TestC1.java
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestC2CCalls.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestC2CCalls.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestC2CCalls.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestC2CCalls.java
@@ -21,10 +21,11 @@
  * questions.
  */
 
 /**
  * @test
+ * @key randomness
  * @summary Test inline type calling convention with compiled to compiled calls.
  * @library /test/lib /test/lib /compiler/whitebox /
  * @compile TestC2CCalls.java
  * @run driver ClassFileInstaller sun.hotspot.WhiteBox
  * @run main/othervm -Xbootclasspath/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestCallingConvention.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestCallingConvention.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestCallingConvention.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestCallingConvention.java
@@ -28,10 +28,11 @@
 import java.lang.invoke.*;
 import java.lang.reflect.Method;
 
 /*
  * @test
+ * @key randomness
  * @summary Test inline type calling convention optimizations
  * @library /testlibrary /test/lib /compiler/whitebox /
  * @requires (os.simpleArch == "x64" | os.simpleArch == "aarch64")
  * @compile -XDallowEmptyValues TestCallingConvention.java
  * @run driver ClassFileInstaller sun.hotspot.WhiteBox jdk.test.lib.Platform
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestCallingConventionC1.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestCallingConventionC1.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestCallingConventionC1.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestCallingConventionC1.java
@@ -26,10 +26,11 @@
 import sun.hotspot.WhiteBox;
 import jdk.test.lib.Asserts;
 
 /*
  * @test
+ * @key randomness
  * @summary Test calls from {C1} to {C2, Interpreter}, and vice versa.
  * @library /testlibrary /test/lib /compiler/whitebox /
  * @requires (os.simpleArch == "x64" | os.simpleArch == "aarch64")
  * @compile TestCallingConventionC1.java
  * @run driver ClassFileInstaller sun.hotspot.WhiteBox jdk.test.lib.Platform
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestIntrinsics.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestIntrinsics.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestIntrinsics.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestIntrinsics.java
@@ -31,10 +31,11 @@
 import jdk.test.lib.Asserts;
 import jdk.internal.misc.Unsafe;
 
 /*
  * @test
+ * @key randomness
  * @summary Test intrinsic support for inline types
  * @library /testlibrary /test/lib /compiler/whitebox /
  * @modules java.base/jdk.internal.misc
  * @requires (os.simpleArch == "x64" | os.simpleArch == "aarch64")
  * @compile TestIntrinsics.java
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestJNICalls.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestJNICalls.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestJNICalls.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestJNICalls.java
@@ -27,10 +27,11 @@
 
 import java.lang.reflect.Method;
 
 /*
  * @test
+ * @key randomness
  * @summary Test calling native methods with inline type arguments from compiled code.
  * @library /testlibrary /test/lib /compiler/whitebox /
  * @requires (os.simpleArch == "x64" | os.simpleArch == "aarch64")
  * @compile TestJNICalls.java
  * @run driver ClassFileInstaller sun.hotspot.WhiteBox jdk.test.lib.Platform
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestLWorld.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestLWorld.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestLWorld.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestLWorld.java
@@ -30,10 +30,11 @@
 import jdk.experimental.value.MethodHandleBuilder;
 import jdk.test.lib.Asserts;
 
 /*
  * @test
+ * @key randomness
  * @summary Test inline types in LWorld.
  * @modules java.base/jdk.experimental.value
  * @library /testlibrary /test/lib /compiler/whitebox /
  * @requires (os.simpleArch == "x64" | os.simpleArch == "aarch64")
  * @compile -XDallowEmptyValues TestLWorld.java
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestLWorldProfiling.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestLWorldProfiling.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestLWorldProfiling.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestLWorldProfiling.java
@@ -26,10 +26,11 @@
 import jdk.test.lib.Asserts;
 import java.lang.reflect.Method;
 
 /*
  * @test
+ * @key randomness
  * @summary Test inline type specific profiling
  * @modules java.base/jdk.experimental.value
  * @library /testlibrary /test/lib /compiler/whitebox /
  * @requires (os.simpleArch == "x64")
  * @compile TestLWorldProfiling.java
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestMethodHandles.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestMethodHandles.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestMethodHandles.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestMethodHandles.java
@@ -28,10 +28,11 @@
 
 import jdk.test.lib.Asserts;
 
 /*
  * @test
+ * @key randomness
  * @summary Test method handle support for inline types
  * @library /testlibrary /test/lib /compiler/whitebox /
  * @requires os.simpleArch == "x64"
  * @compile TestMethodHandles.java
  * @run driver ClassFileInstaller sun.hotspot.WhiteBox jdk.test.lib.Platform
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestNullableArrays.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestNullableArrays.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestNullableArrays.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestNullableArrays.java
@@ -27,10 +27,11 @@
 import java.lang.reflect.Method;
 import java.util.Arrays;
 
 /*
  * @test
+ * @key randomness
  * @summary Test nullable inline type arrays
  * @library /testlibrary /test/lib /compiler/whitebox /
  * @requires (os.simpleArch == "x64" | os.simpleArch == "aarch64")
  * @compile TestNullableArrays.java
  * @run driver ClassFileInstaller sun.hotspot.WhiteBox jdk.test.lib.Platform
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestNullableInlineTypes.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestNullableInlineTypes.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestNullableInlineTypes.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestNullableInlineTypes.java
@@ -28,10 +28,11 @@
 
 import jdk.test.lib.Asserts;
 
 /*
  * @test
+ * @key randomness
  * @summary Test correct handling of nullable inline types.
  * @library /testlibrary /test/lib /compiler/whitebox /
  * @requires (os.simpleArch == "x64" | os.simpleArch == "aarch64")
  * @compile TestNullableInlineTypes.java
  * @run driver ClassFileInstaller sun.hotspot.WhiteBox jdk.test.lib.Platform
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestOnStackReplacement.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestOnStackReplacement.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestOnStackReplacement.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestOnStackReplacement.java
@@ -26,10 +26,11 @@
 import jdk.test.lib.Asserts;
 import java.lang.reflect.Method;
 
 /*
  * @test
+ * @key randomness
  * @summary Test on stack replacement (OSR) with inline types
  * @library /testlibrary /test/lib /compiler/whitebox /
  * @requires (os.simpleArch == "x64" | os.simpleArch == "aarch64")
  * @compile TestOnStackReplacement.java
  * @run driver ClassFileInstaller sun.hotspot.WhiteBox jdk.test.lib.Platform
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestUnloadedInlineTypeField.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestUnloadedInlineTypeField.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestUnloadedInlineTypeField.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestUnloadedInlineTypeField.java
@@ -24,10 +24,11 @@
 package compiler.valhalla.inlinetypes;
 import jdk.test.lib.Asserts;
 
 /**
  * @test
+ * @key randomness
  * @library /testlibrary /test/lib /compiler/whitebox /
  * @summary Test the handling of fields of unloaded inline classes.
  * @compile hack/GetUnresolvedInlineFieldWrongSignature.java
  * @compile TestUnloadedInlineTypeField.java
  * @run driver ClassFileInstaller sun.hotspot.WhiteBox jdk.test.lib.Platform
@@ -35,11 +36,11 @@
  *                               -XX:+UnlockExperimentalVMOptions -XX:+WhiteBoxAPI
  *                               compiler.valhalla.inlinetypes.InlineTypeTest
  *                               compiler.valhalla.inlinetypes.TestUnloadedInlineTypeField
  */
 
-public class TestUnloadedInlineTypeField extends compiler.valhalla.inlinetypes.InlineTypeTest {
+public class TestUnloadedInlineTypeField extends InlineTypeTest {
     public static void main(String[] args) throws Throwable {
         TestUnloadedInlineTypeField test = new TestUnloadedInlineTypeField();
         test.run(args);
     }
 
diff a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestWithfieldC1.java b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestWithfieldC1.java
--- a/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestWithfieldC1.java
+++ b/test/hotspot/jtreg/compiler/valhalla/inlinetypes/TestWithfieldC1.java
@@ -31,10 +31,11 @@
 import jdk.experimental.value.MethodHandleBuilder;
 import jdk.test.lib.Asserts;
 
 /*
  * @test
+ * @key randomness
  * @summary Verify that C1 performs escape analysis before optimizing withfield bytecode to putfield.
  * @modules java.base/jdk.experimental.value
  * @library /testlibrary /test/lib /compiler/whitebox /
  * @requires os.simpleArch == "x64"
  * @compile -XDallowWithFieldOperator TestWithfieldC1.java
