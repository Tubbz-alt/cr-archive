<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/cpu/x86/stubGenerator_x86_64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2003, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;ci/ciUtilities.hpp&quot;
  29 #include &quot;gc/shared/barrierSet.hpp&quot;
  30 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  31 #include &quot;gc/shared/barrierSetNMethod.hpp&quot;
  32 #include &quot;interpreter/interpreter.hpp&quot;
  33 #include &quot;memory/universe.hpp&quot;
  34 #include &quot;nativeInst_x86.hpp&quot;
  35 #include &quot;oops/instanceOop.hpp&quot;
  36 #include &quot;oops/method.hpp&quot;
  37 #include &quot;oops/objArrayKlass.hpp&quot;
  38 #include &quot;oops/oop.inline.hpp&quot;
  39 #include &quot;prims/methodHandles.hpp&quot;
  40 #include &quot;runtime/frame.inline.hpp&quot;
  41 #include &quot;runtime/handles.inline.hpp&quot;
  42 #include &quot;runtime/sharedRuntime.hpp&quot;
  43 #include &quot;runtime/stubCodeGenerator.hpp&quot;
  44 #include &quot;runtime/stubRoutines.hpp&quot;
  45 #include &quot;runtime/thread.inline.hpp&quot;
  46 #ifdef COMPILER2
  47 #include &quot;opto/runtime.hpp&quot;
  48 #endif
  49 #if INCLUDE_ZGC
  50 #include &quot;gc/z/zThreadLocalData.hpp&quot;
  51 #endif
  52 
  53 // Declaration and definition of StubGenerator (no .hpp file).
  54 // For a more detailed description of the stub routine structure
  55 // see the comment in stubRoutines.hpp
  56 
  57 #define __ _masm-&gt;
  58 #define TIMES_OOP (UseCompressedOops ? Address::times_4 : Address::times_8)
  59 #define a__ ((Assembler*)_masm)-&gt;
  60 
  61 #ifdef PRODUCT
  62 #define BLOCK_COMMENT(str) /* nothing */
  63 #else
  64 #define BLOCK_COMMENT(str) __ block_comment(str)
  65 #endif
  66 
  67 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  68 const int MXCSR_MASK = 0xFFC0;  // Mask out any pending exceptions
  69 
  70 // Stub Code definitions
  71 
  72 class StubGenerator: public StubCodeGenerator {
  73  private:
  74 
  75 #ifdef PRODUCT
  76 #define inc_counter_np(counter) ((void)0)
  77 #else
  78   void inc_counter_np_(int&amp; counter) {
  79     // This can destroy rscratch1 if counter is far from the code cache
  80     __ incrementl(ExternalAddress((address)&amp;counter));
  81   }
  82 #define inc_counter_np(counter) \
  83   BLOCK_COMMENT(&quot;inc_counter &quot; #counter); \
  84   inc_counter_np_(counter);
  85 #endif
  86 
  87   // Call stubs are used to call Java from C
  88   //
  89   // Linux Arguments:
  90   //    c_rarg0:   call wrapper address                   address
  91   //    c_rarg1:   result                                 address
  92   //    c_rarg2:   result type                            BasicType
  93   //    c_rarg3:   method                                 Method*
  94   //    c_rarg4:   (interpreter) entry point              address
  95   //    c_rarg5:   parameters                             intptr_t*
  96   //    16(rbp): parameter size (in words)              int
  97   //    24(rbp): thread                                 Thread*
  98   //
  99   //     [ return_from_Java     ] &lt;--- rsp
 100   //     [ argument word n      ]
 101   //      ...
 102   // -12 [ argument word 1      ]
 103   // -11 [ saved r15            ] &lt;--- rsp_after_call
 104   // -10 [ saved r14            ]
 105   //  -9 [ saved r13            ]
 106   //  -8 [ saved r12            ]
 107   //  -7 [ saved rbx            ]
 108   //  -6 [ call wrapper         ]
 109   //  -5 [ result               ]
 110   //  -4 [ result type          ]
 111   //  -3 [ method               ]
 112   //  -2 [ entry point          ]
 113   //  -1 [ parameters           ]
 114   //   0 [ saved rbp            ] &lt;--- rbp
 115   //   1 [ return address       ]
 116   //   2 [ parameter size       ]
 117   //   3 [ thread               ]
 118   //
 119   // Windows Arguments:
 120   //    c_rarg0:   call wrapper address                   address
 121   //    c_rarg1:   result                                 address
 122   //    c_rarg2:   result type                            BasicType
 123   //    c_rarg3:   method                                 Method*
 124   //    48(rbp): (interpreter) entry point              address
 125   //    56(rbp): parameters                             intptr_t*
 126   //    64(rbp): parameter size (in words)              int
 127   //    72(rbp): thread                                 Thread*
 128   //
 129   //     [ return_from_Java     ] &lt;--- rsp
 130   //     [ argument word n      ]
 131   //      ...
 132   // -60 [ argument word 1      ]
 133   // -59 [ saved xmm31          ] &lt;--- rsp after_call
 134   //     [ saved xmm16-xmm30    ] (EVEX enabled, else the space is blank)
 135   // -27 [ saved xmm15          ]
 136   //     [ saved xmm7-xmm14     ]
 137   //  -9 [ saved xmm6           ] (each xmm register takes 2 slots)
 138   //  -7 [ saved r15            ]
 139   //  -6 [ saved r14            ]
 140   //  -5 [ saved r13            ]
 141   //  -4 [ saved r12            ]
 142   //  -3 [ saved rdi            ]
 143   //  -2 [ saved rsi            ]
 144   //  -1 [ saved rbx            ]
 145   //   0 [ saved rbp            ] &lt;--- rbp
 146   //   1 [ return address       ]
 147   //   2 [ call wrapper         ]
 148   //   3 [ result               ]
 149   //   4 [ result type          ]
 150   //   5 [ method               ]
 151   //   6 [ entry point          ]
 152   //   7 [ parameters           ]
 153   //   8 [ parameter size       ]
 154   //   9 [ thread               ]
 155   //
 156   //    Windows reserves the callers stack space for arguments 1-4.
 157   //    We spill c_rarg0-c_rarg3 to this space.
 158 
 159   // Call stub stack layout word offsets from rbp
 160   enum call_stub_layout {
 161 #ifdef _WIN64
 162     xmm_save_first     = 6,  // save from xmm6
 163     xmm_save_last      = 31, // to xmm31
 164     xmm_save_base      = -9,
 165     rsp_after_call_off = xmm_save_base - 2 * (xmm_save_last - xmm_save_first), // -27
 166     r15_off            = -7,
 167     r14_off            = -6,
 168     r13_off            = -5,
 169     r12_off            = -4,
 170     rdi_off            = -3,
 171     rsi_off            = -2,
 172     rbx_off            = -1,
 173     rbp_off            =  0,
 174     retaddr_off        =  1,
 175     call_wrapper_off   =  2,
 176     result_off         =  3,
 177     result_type_off    =  4,
 178     method_off         =  5,
 179     entry_point_off    =  6,
 180     parameters_off     =  7,
 181     parameter_size_off =  8,
 182     thread_off         =  9
 183 #else
 184     rsp_after_call_off = -12,
 185     mxcsr_off          = rsp_after_call_off,
 186     r15_off            = -11,
 187     r14_off            = -10,
 188     r13_off            = -9,
 189     r12_off            = -8,
 190     rbx_off            = -7,
 191     call_wrapper_off   = -6,
 192     result_off         = -5,
 193     result_type_off    = -4,
 194     method_off         = -3,
 195     entry_point_off    = -2,
 196     parameters_off     = -1,
 197     rbp_off            =  0,
 198     retaddr_off        =  1,
 199     parameter_size_off =  2,
 200     thread_off         =  3
 201 #endif
 202   };
 203 
 204 #ifdef _WIN64
 205   Address xmm_save(int reg) {
 206     assert(reg &gt;= xmm_save_first &amp;&amp; reg &lt;= xmm_save_last, &quot;XMM register number out of range&quot;);
 207     return Address(rbp, (xmm_save_base - (reg - xmm_save_first) * 2) * wordSize);
 208   }
 209 #endif
 210 
 211   address generate_call_stub(address&amp; return_address) {
 212     assert((int)frame::entry_frame_after_call_words == -(int)rsp_after_call_off + 1 &amp;&amp;
 213            (int)frame::entry_frame_call_wrapper_offset == (int)call_wrapper_off,
 214            &quot;adjust this code&quot;);
 215     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;call_stub&quot;);
 216     address start = __ pc();
 217 
 218     // same as in generate_catch_exception()!
 219     const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);
 220 
 221     const Address call_wrapper  (rbp, call_wrapper_off   * wordSize);
 222     const Address result        (rbp, result_off         * wordSize);
 223     const Address result_type   (rbp, result_type_off    * wordSize);
 224     const Address method        (rbp, method_off         * wordSize);
 225     const Address entry_point   (rbp, entry_point_off    * wordSize);
 226     const Address parameters    (rbp, parameters_off     * wordSize);
 227     const Address parameter_size(rbp, parameter_size_off * wordSize);
 228 
 229     // same as in generate_catch_exception()!
 230     const Address thread        (rbp, thread_off         * wordSize);
 231 
 232     const Address r15_save(rbp, r15_off * wordSize);
 233     const Address r14_save(rbp, r14_off * wordSize);
 234     const Address r13_save(rbp, r13_off * wordSize);
 235     const Address r12_save(rbp, r12_off * wordSize);
 236     const Address rbx_save(rbp, rbx_off * wordSize);
 237 
 238     // stub code
 239     __ enter();
 240     __ subptr(rsp, -rsp_after_call_off * wordSize);
 241 
 242     // save register parameters
 243 #ifndef _WIN64
 244     __ movptr(parameters,   c_rarg5); // parameters
 245     __ movptr(entry_point,  c_rarg4); // entry_point
 246 #endif
 247 
 248     __ movptr(method,       c_rarg3); // method
 249     __ movl(result_type,  c_rarg2);   // result type
 250     __ movptr(result,       c_rarg1); // result
 251     __ movptr(call_wrapper, c_rarg0); // call wrapper
 252 
 253     // save regs belonging to calling function
 254     __ movptr(rbx_save, rbx);
 255     __ movptr(r12_save, r12);
 256     __ movptr(r13_save, r13);
 257     __ movptr(r14_save, r14);
 258     __ movptr(r15_save, r15);
 259 
 260 #ifdef _WIN64
 261     int last_reg = 15;
 262     if (UseAVX &gt; 2) {
 263       last_reg = 31;
 264     }
 265     if (VM_Version::supports_evex()) {
 266       for (int i = xmm_save_first; i &lt;= last_reg; i++) {
 267         __ vextractf32x4(xmm_save(i), as_XMMRegister(i), 0);
 268       }
 269     } else {
 270       for (int i = xmm_save_first; i &lt;= last_reg; i++) {
 271         __ movdqu(xmm_save(i), as_XMMRegister(i));
 272       }
 273     }
 274 
 275     const Address rdi_save(rbp, rdi_off * wordSize);
 276     const Address rsi_save(rbp, rsi_off * wordSize);
 277 
 278     __ movptr(rsi_save, rsi);
 279     __ movptr(rdi_save, rdi);
 280 #else
 281     const Address mxcsr_save(rbp, mxcsr_off * wordSize);
 282     {
 283       Label skip_ldmx;
 284       __ stmxcsr(mxcsr_save);
 285       __ movl(rax, mxcsr_save);
 286       __ andl(rax, MXCSR_MASK);    // Only check control and mask bits
 287       ExternalAddress mxcsr_std(StubRoutines::addr_mxcsr_std());
 288       __ cmp32(rax, mxcsr_std);
 289       __ jcc(Assembler::equal, skip_ldmx);
 290       __ ldmxcsr(mxcsr_std);
 291       __ bind(skip_ldmx);
 292     }
 293 #endif
 294 
 295     // Load up thread register
 296     __ movptr(r15_thread, thread);
 297     __ reinit_heapbase();
 298 
 299 #ifdef ASSERT
 300     // make sure we have no pending exceptions
 301     {
 302       Label L;
 303       __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);
 304       __ jcc(Assembler::equal, L);
 305       __ stop(&quot;StubRoutines::call_stub: entered with pending exception&quot;);
 306       __ bind(L);
 307     }
 308 #endif
 309 
 310     // pass parameters if any
 311     BLOCK_COMMENT(&quot;pass parameters if any&quot;);
 312     Label parameters_done;
 313     __ movl(c_rarg3, parameter_size);
 314     __ testl(c_rarg3, c_rarg3);
 315     __ jcc(Assembler::zero, parameters_done);
 316 
 317     Label loop;
 318     __ movptr(c_rarg2, parameters);       // parameter pointer
 319     __ movl(c_rarg1, c_rarg3);            // parameter counter is in c_rarg1
 320     __ BIND(loop);
 321     __ movptr(rax, Address(c_rarg2, 0));// get parameter
 322     __ addptr(c_rarg2, wordSize);       // advance to next parameter
 323     __ decrementl(c_rarg1);             // decrement counter
 324     __ push(rax);                       // pass parameter
 325     __ jcc(Assembler::notZero, loop);
 326 
 327     // call Java function
 328     __ BIND(parameters_done);
 329     __ movptr(rbx, method);             // get Method*
 330     __ movptr(c_rarg1, entry_point);    // get entry_point
 331     __ mov(r13, rsp);                   // set sender sp
 332     BLOCK_COMMENT(&quot;call Java function&quot;);
 333     __ call(c_rarg1);
 334 
 335     BLOCK_COMMENT(&quot;call_stub_return_address:&quot;);
 336     return_address = __ pc();
 337 
 338     // store result depending on type (everything that is not
 339     // T_OBJECT, T_VALUETYPE, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)
 340     __ movptr(r13, result);
 341     Label is_long, is_float, is_double, is_value, exit;
 342     __ movl(rbx, result_type);
 343     __ cmpl(rbx, T_OBJECT);
 344     __ jcc(Assembler::equal, is_long);
 345     __ cmpl(rbx, T_VALUETYPE);
 346     __ jcc(Assembler::equal, is_value);
 347     __ cmpl(rbx, T_LONG);
 348     __ jcc(Assembler::equal, is_long);
 349     __ cmpl(rbx, T_FLOAT);
 350     __ jcc(Assembler::equal, is_float);
 351     __ cmpl(rbx, T_DOUBLE);
 352     __ jcc(Assembler::equal, is_double);
 353 
 354     // handle T_INT case
 355     __ movl(Address(r13, 0), rax);
 356 
 357     __ BIND(exit);
 358 
 359     // pop parameters
 360     __ lea(rsp, rsp_after_call);
 361 
 362 #ifdef ASSERT
 363     // verify that threads correspond
 364     {
 365      Label L1, L2, L3;
 366       __ cmpptr(r15_thread, thread);
 367       __ jcc(Assembler::equal, L1);
 368       __ stop(&quot;StubRoutines::call_stub: r15_thread is corrupted&quot;);
 369       __ bind(L1);
 370       __ get_thread(rbx);
 371       __ cmpptr(r15_thread, thread);
 372       __ jcc(Assembler::equal, L2);
 373       __ stop(&quot;StubRoutines::call_stub: r15_thread is modified by call&quot;);
 374       __ bind(L2);
 375       __ cmpptr(r15_thread, rbx);
 376       __ jcc(Assembler::equal, L3);
 377       __ stop(&quot;StubRoutines::call_stub: threads must correspond&quot;);
 378       __ bind(L3);
 379     }
 380 #endif
 381 
 382     // restore regs belonging to calling function
 383 #ifdef _WIN64
 384     // emit the restores for xmm regs
 385     if (VM_Version::supports_evex()) {
 386       for (int i = xmm_save_first; i &lt;= last_reg; i++) {
 387         __ vinsertf32x4(as_XMMRegister(i), as_XMMRegister(i), xmm_save(i), 0);
 388       }
 389     } else {
 390       for (int i = xmm_save_first; i &lt;= last_reg; i++) {
 391         __ movdqu(as_XMMRegister(i), xmm_save(i));
 392       }
 393     }
 394 #endif
 395     __ movptr(r15, r15_save);
 396     __ movptr(r14, r14_save);
 397     __ movptr(r13, r13_save);
 398     __ movptr(r12, r12_save);
 399     __ movptr(rbx, rbx_save);
 400 
 401 #ifdef _WIN64
 402     __ movptr(rdi, rdi_save);
 403     __ movptr(rsi, rsi_save);
 404 #else
 405     __ ldmxcsr(mxcsr_save);
 406 #endif
 407 
 408     // restore rsp
 409     __ addptr(rsp, -rsp_after_call_off * wordSize);
 410 
 411     // return
 412     __ vzeroupper();
 413     __ pop(rbp);
 414     __ ret(0);
 415 
 416     // handle return types different from T_INT
 417     __ BIND(is_value);
 418     if (InlineTypeReturnedAsFields) {
 419       // Check for flattened return value
 420       __ testptr(rax, 1);
 421       __ jcc(Assembler::zero, is_long);
 422       // Load pack handler address
 423       __ andptr(rax, -2);
 424       __ movptr(rax, Address(rax, InstanceKlass::adr_valueklass_fixed_block_offset()));
 425       __ movptr(rbx, Address(rax, ValueKlass::pack_handler_jobject_offset()));
 426       // Call pack handler to initialize the buffer
 427       __ call(rbx);
 428       __ jmp(exit);
 429     }
 430     __ BIND(is_long);
 431     __ movq(Address(r13, 0), rax);
 432     __ jmp(exit);
 433 
 434     __ BIND(is_float);
 435     __ movflt(Address(r13, 0), xmm0);
 436     __ jmp(exit);
 437 
 438     __ BIND(is_double);
 439     __ movdbl(Address(r13, 0), xmm0);
 440     __ jmp(exit);
 441 
 442     return start;
 443   }
 444 
 445   // Return point for a Java call if there&#39;s an exception thrown in
 446   // Java code.  The exception is caught and transformed into a
 447   // pending exception stored in JavaThread that can be tested from
 448   // within the VM.
 449   //
 450   // Note: Usually the parameters are removed by the callee. In case
 451   // of an exception crossing an activation frame boundary, that is
 452   // not the case if the callee is compiled code =&gt; need to setup the
 453   // rsp.
 454   //
 455   // rax: exception oop
 456 
 457   address generate_catch_exception() {
 458     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;catch_exception&quot;);
 459     address start = __ pc();
 460 
 461     // same as in generate_call_stub():
 462     const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);
 463     const Address thread        (rbp, thread_off         * wordSize);
 464 
 465 #ifdef ASSERT
 466     // verify that threads correspond
 467     {
 468       Label L1, L2, L3;
 469       __ cmpptr(r15_thread, thread);
 470       __ jcc(Assembler::equal, L1);
 471       __ stop(&quot;StubRoutines::catch_exception: r15_thread is corrupted&quot;);
 472       __ bind(L1);
 473       __ get_thread(rbx);
 474       __ cmpptr(r15_thread, thread);
 475       __ jcc(Assembler::equal, L2);
 476       __ stop(&quot;StubRoutines::catch_exception: r15_thread is modified by call&quot;);
 477       __ bind(L2);
 478       __ cmpptr(r15_thread, rbx);
 479       __ jcc(Assembler::equal, L3);
 480       __ stop(&quot;StubRoutines::catch_exception: threads must correspond&quot;);
 481       __ bind(L3);
 482     }
 483 #endif
 484 
 485     // set pending exception
 486     __ verify_oop(rax);
 487 
 488     __ movptr(Address(r15_thread, Thread::pending_exception_offset()), rax);
 489     __ lea(rscratch1, ExternalAddress((address)__FILE__));
 490     __ movptr(Address(r15_thread, Thread::exception_file_offset()), rscratch1);
 491     __ movl(Address(r15_thread, Thread::exception_line_offset()), (int)  __LINE__);
 492 
 493     // complete return to VM
 494     assert(StubRoutines::_call_stub_return_address != NULL,
 495            &quot;_call_stub_return_address must have been generated before&quot;);
 496     __ jump(RuntimeAddress(StubRoutines::_call_stub_return_address));
 497 
 498     return start;
 499   }
 500 
 501   // Continuation point for runtime calls returning with a pending
 502   // exception.  The pending exception check happened in the runtime
 503   // or native call stub.  The pending exception in Thread is
 504   // converted into a Java-level exception.
 505   //
 506   // Contract with Java-level exception handlers:
 507   // rax: exception
 508   // rdx: throwing pc
 509   //
 510   // NOTE: At entry of this stub, exception-pc must be on stack !!
 511 
 512   address generate_forward_exception() {
 513     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;forward exception&quot;);
 514     address start = __ pc();
 515 
 516     // Upon entry, the sp points to the return address returning into
 517     // Java (interpreted or compiled) code; i.e., the return address
 518     // becomes the throwing pc.
 519     //
 520     // Arguments pushed before the runtime call are still on the stack
 521     // but the exception handler will reset the stack pointer -&gt;
 522     // ignore them.  A potential result in registers can be ignored as
 523     // well.
 524 
 525 #ifdef ASSERT
 526     // make sure this code is only executed if there is a pending exception
 527     {
 528       Label L;
 529       __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t) NULL);
 530       __ jcc(Assembler::notEqual, L);
 531       __ stop(&quot;StubRoutines::forward exception: no pending exception (1)&quot;);
 532       __ bind(L);
 533     }
 534 #endif
 535 
 536     // compute exception handler into rbx
 537     __ movptr(c_rarg0, Address(rsp, 0));
 538     BLOCK_COMMENT(&quot;call exception_handler_for_return_address&quot;);
 539     __ call_VM_leaf(CAST_FROM_FN_PTR(address,
 540                          SharedRuntime::exception_handler_for_return_address),
 541                     r15_thread, c_rarg0);
 542     __ mov(rbx, rax);
 543 
 544     // setup rax &amp; rdx, remove return address &amp; clear pending exception
 545     __ pop(rdx);
 546     __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));
 547     __ movptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);
 548 
 549 #ifdef ASSERT
 550     // make sure exception is set
 551     {
 552       Label L;
 553       __ testptr(rax, rax);
 554       __ jcc(Assembler::notEqual, L);
 555       __ stop(&quot;StubRoutines::forward exception: no pending exception (2)&quot;);
 556       __ bind(L);
 557     }
 558 #endif
 559 
 560     // continue at exception handler (return address removed)
 561     // rax: exception
 562     // rbx: exception handler
 563     // rdx: throwing pc
 564     __ verify_oop(rax);
 565     __ jmp(rbx);
 566 
 567     return start;
 568   }
 569 
 570   // Implementation of jint atomic_xchg(jint add_value, volatile jint* dest)
 571   // used by Atomic::xchg(volatile jint* dest, jint exchange_value)
 572   //
 573   // Arguments :
 574   //    c_rarg0: exchange_value
 575   //    c_rarg0: dest
 576   //
 577   // Result:
 578   //    *dest &lt;- ex, return (orig *dest)
 579   address generate_atomic_xchg() {
 580     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_xchg&quot;);
 581     address start = __ pc();
 582 
 583     __ movl(rax, c_rarg0); // Copy to eax we need a return value anyhow
 584     __ xchgl(rax, Address(c_rarg1, 0)); // automatic LOCK
 585     __ ret(0);
 586 
 587     return start;
 588   }
 589 
 590   // Implementation of intptr_t atomic_xchg(jlong add_value, volatile jlong* dest)
 591   // used by Atomic::xchg(volatile jlong* dest, jlong exchange_value)
 592   //
 593   // Arguments :
 594   //    c_rarg0: exchange_value
 595   //    c_rarg1: dest
 596   //
 597   // Result:
 598   //    *dest &lt;- ex, return (orig *dest)
 599   address generate_atomic_xchg_long() {
 600     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_xchg_long&quot;);
 601     address start = __ pc();
 602 
 603     __ movptr(rax, c_rarg0); // Copy to eax we need a return value anyhow
 604     __ xchgptr(rax, Address(c_rarg1, 0)); // automatic LOCK
 605     __ ret(0);
 606 
 607     return start;
 608   }
 609 
 610   // Support for jint atomic::atomic_cmpxchg(jint exchange_value, volatile jint* dest,
 611   //                                         jint compare_value)
 612   //
 613   // Arguments :
 614   //    c_rarg0: exchange_value
 615   //    c_rarg1: dest
 616   //    c_rarg2: compare_value
 617   //
 618   // Result:
 619   //    if ( compare_value == *dest ) {
 620   //       *dest = exchange_value
 621   //       return compare_value;
 622   //    else
 623   //       return *dest;
 624   address generate_atomic_cmpxchg() {
 625     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_cmpxchg&quot;);
 626     address start = __ pc();
 627 
 628     __ movl(rax, c_rarg2);
 629     __ lock();
 630     __ cmpxchgl(c_rarg0, Address(c_rarg1, 0));
 631     __ ret(0);
 632 
 633     return start;
 634   }
 635 
 636   // Support for int8_t atomic::atomic_cmpxchg(int8_t exchange_value, volatile int8_t* dest,
 637   //                                           int8_t compare_value)
 638   //
 639   // Arguments :
 640   //    c_rarg0: exchange_value
 641   //    c_rarg1: dest
 642   //    c_rarg2: compare_value
 643   //
 644   // Result:
 645   //    if ( compare_value == *dest ) {
 646   //       *dest = exchange_value
 647   //       return compare_value;
 648   //    else
 649   //       return *dest;
 650   address generate_atomic_cmpxchg_byte() {
 651     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_cmpxchg_byte&quot;);
 652     address start = __ pc();
 653 
 654     __ movsbq(rax, c_rarg2);
 655     __ lock();
 656     __ cmpxchgb(c_rarg0, Address(c_rarg1, 0));
 657     __ ret(0);
 658 
 659     return start;
 660   }
 661 
 662   // Support for int64_t atomic::atomic_cmpxchg(int64_t exchange_value,
 663   //                                            volatile int64_t* dest,
 664   //                                            int64_t compare_value)
 665   // Arguments :
 666   //    c_rarg0: exchange_value
 667   //    c_rarg1: dest
 668   //    c_rarg2: compare_value
 669   //
 670   // Result:
 671   //    if ( compare_value == *dest ) {
 672   //       *dest = exchange_value
 673   //       return compare_value;
 674   //    else
 675   //       return *dest;
 676   address generate_atomic_cmpxchg_long() {
 677     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_cmpxchg_long&quot;);
 678     address start = __ pc();
 679 
 680     __ movq(rax, c_rarg2);
 681     __ lock();
 682     __ cmpxchgq(c_rarg0, Address(c_rarg1, 0));
 683     __ ret(0);
 684 
 685     return start;
 686   }
 687 
 688   // Implementation of jint atomic_add(jint add_value, volatile jint* dest)
 689   // used by Atomic::add(volatile jint* dest, jint add_value)
 690   //
 691   // Arguments :
 692   //    c_rarg0: add_value
 693   //    c_rarg1: dest
 694   //
 695   // Result:
 696   //    *dest += add_value
 697   //    return *dest;
 698   address generate_atomic_add() {
 699     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_add&quot;);
 700     address start = __ pc();
 701 
 702     __ movl(rax, c_rarg0);
 703     __ lock();
 704     __ xaddl(Address(c_rarg1, 0), c_rarg0);
 705     __ addl(rax, c_rarg0);
 706     __ ret(0);
 707 
 708     return start;
 709   }
 710 
 711   // Implementation of intptr_t atomic_add(intptr_t add_value, volatile intptr_t* dest)
 712   // used by Atomic::add(volatile intptr_t* dest, intptr_t add_value)
 713   //
 714   // Arguments :
 715   //    c_rarg0: add_value
 716   //    c_rarg1: dest
 717   //
 718   // Result:
 719   //    *dest += add_value
 720   //    return *dest;
 721   address generate_atomic_add_long() {
 722     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_add_long&quot;);
 723     address start = __ pc();
 724 
 725     __ movptr(rax, c_rarg0); // Copy to eax we need a return value anyhow
 726     __ lock();
 727     __ xaddptr(Address(c_rarg1, 0), c_rarg0);
 728     __ addptr(rax, c_rarg0);
 729     __ ret(0);
 730 
 731     return start;
 732   }
 733 
 734   // Support for intptr_t OrderAccess::fence()
 735   //
 736   // Arguments :
 737   //
 738   // Result:
 739   address generate_orderaccess_fence() {
 740     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;orderaccess_fence&quot;);
 741     address start = __ pc();
 742     __ membar(Assembler::StoreLoad);
 743     __ ret(0);
 744 
 745     return start;
 746   }
 747 
 748   // Support for intptr_t get_previous_fp()
 749   //
 750   // This routine is used to find the previous frame pointer for the
 751   // caller (current_frame_guess). This is used as part of debugging
 752   // ps() is seemingly lost trying to find frames.
 753   // This code assumes that caller current_frame_guess) has a frame.
 754   address generate_get_previous_fp() {
 755     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;get_previous_fp&quot;);
 756     const Address old_fp(rbp, 0);
 757     const Address older_fp(rax, 0);
 758     address start = __ pc();
 759 
 760     __ enter();
 761     __ movptr(rax, old_fp); // callers fp
 762     __ movptr(rax, older_fp); // the frame for ps()
 763     __ pop(rbp);
 764     __ ret(0);
 765 
 766     return start;
 767   }
 768 
 769   // Support for intptr_t get_previous_sp()
 770   //
 771   // This routine is used to find the previous stack pointer for the
 772   // caller.
 773   address generate_get_previous_sp() {
 774     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;get_previous_sp&quot;);
 775     address start = __ pc();
 776 
 777     __ movptr(rax, rsp);
 778     __ addptr(rax, 8); // return address is at the top of the stack.
 779     __ ret(0);
 780 
 781     return start;
 782   }
 783 
 784   //----------------------------------------------------------------------------------------------------
 785   // Support for void verify_mxcsr()
 786   //
 787   // This routine is used with -Xcheck:jni to verify that native
 788   // JNI code does not return to Java code without restoring the
 789   // MXCSR register to our expected state.
 790 
 791   address generate_verify_mxcsr() {
 792     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;verify_mxcsr&quot;);
 793     address start = __ pc();
 794 
 795     const Address mxcsr_save(rsp, 0);
 796 
 797     if (CheckJNICalls) {
 798       Label ok_ret;
 799       ExternalAddress mxcsr_std(StubRoutines::addr_mxcsr_std());
 800       __ push(rax);
 801       __ subptr(rsp, wordSize);      // allocate a temp location
 802       __ stmxcsr(mxcsr_save);
 803       __ movl(rax, mxcsr_save);
 804       __ andl(rax, MXCSR_MASK);    // Only check control and mask bits
 805       __ cmp32(rax, mxcsr_std);
 806       __ jcc(Assembler::equal, ok_ret);
 807 
 808       __ warn(&quot;MXCSR changed by native JNI code, use -XX:+RestoreMXCSROnJNICall&quot;);
 809 
 810       __ ldmxcsr(mxcsr_std);
 811 
 812       __ bind(ok_ret);
 813       __ addptr(rsp, wordSize);
 814       __ pop(rax);
 815     }
 816 
 817     __ ret(0);
 818 
 819     return start;
 820   }
 821 
 822   address generate_f2i_fixup() {
 823     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;f2i_fixup&quot;);
 824     Address inout(rsp, 5 * wordSize); // return address + 4 saves
 825 
 826     address start = __ pc();
 827 
 828     Label L;
 829 
 830     __ push(rax);
 831     __ push(c_rarg3);
 832     __ push(c_rarg2);
 833     __ push(c_rarg1);
 834 
 835     __ movl(rax, 0x7f800000);
 836     __ xorl(c_rarg3, c_rarg3);
 837     __ movl(c_rarg2, inout);
 838     __ movl(c_rarg1, c_rarg2);
 839     __ andl(c_rarg1, 0x7fffffff);
 840     __ cmpl(rax, c_rarg1); // NaN? -&gt; 0
 841     __ jcc(Assembler::negative, L);
 842     __ testl(c_rarg2, c_rarg2); // signed ? min_jint : max_jint
 843     __ movl(c_rarg3, 0x80000000);
 844     __ movl(rax, 0x7fffffff);
 845     __ cmovl(Assembler::positive, c_rarg3, rax);
 846 
 847     __ bind(L);
 848     __ movptr(inout, c_rarg3);
 849 
 850     __ pop(c_rarg1);
 851     __ pop(c_rarg2);
 852     __ pop(c_rarg3);
 853     __ pop(rax);
 854 
 855     __ ret(0);
 856 
 857     return start;
 858   }
 859 
 860   address generate_f2l_fixup() {
 861     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;f2l_fixup&quot;);
 862     Address inout(rsp, 5 * wordSize); // return address + 4 saves
 863     address start = __ pc();
 864 
 865     Label L;
 866 
 867     __ push(rax);
 868     __ push(c_rarg3);
 869     __ push(c_rarg2);
 870     __ push(c_rarg1);
 871 
 872     __ movl(rax, 0x7f800000);
 873     __ xorl(c_rarg3, c_rarg3);
 874     __ movl(c_rarg2, inout);
 875     __ movl(c_rarg1, c_rarg2);
 876     __ andl(c_rarg1, 0x7fffffff);
 877     __ cmpl(rax, c_rarg1); // NaN? -&gt; 0
 878     __ jcc(Assembler::negative, L);
 879     __ testl(c_rarg2, c_rarg2); // signed ? min_jlong : max_jlong
 880     __ mov64(c_rarg3, 0x8000000000000000);
 881     __ mov64(rax, 0x7fffffffffffffff);
 882     __ cmov(Assembler::positive, c_rarg3, rax);
 883 
 884     __ bind(L);
 885     __ movptr(inout, c_rarg3);
 886 
 887     __ pop(c_rarg1);
 888     __ pop(c_rarg2);
 889     __ pop(c_rarg3);
 890     __ pop(rax);
 891 
 892     __ ret(0);
 893 
 894     return start;
 895   }
 896 
 897   address generate_d2i_fixup() {
 898     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;d2i_fixup&quot;);
 899     Address inout(rsp, 6 * wordSize); // return address + 5 saves
 900 
 901     address start = __ pc();
 902 
 903     Label L;
 904 
 905     __ push(rax);
 906     __ push(c_rarg3);
 907     __ push(c_rarg2);
 908     __ push(c_rarg1);
 909     __ push(c_rarg0);
 910 
 911     __ movl(rax, 0x7ff00000);
 912     __ movq(c_rarg2, inout);
 913     __ movl(c_rarg3, c_rarg2);
 914     __ mov(c_rarg1, c_rarg2);
 915     __ mov(c_rarg0, c_rarg2);
 916     __ negl(c_rarg3);
 917     __ shrptr(c_rarg1, 0x20);
 918     __ orl(c_rarg3, c_rarg2);
 919     __ andl(c_rarg1, 0x7fffffff);
 920     __ xorl(c_rarg2, c_rarg2);
 921     __ shrl(c_rarg3, 0x1f);
 922     __ orl(c_rarg1, c_rarg3);
 923     __ cmpl(rax, c_rarg1);
 924     __ jcc(Assembler::negative, L); // NaN -&gt; 0
 925     __ testptr(c_rarg0, c_rarg0); // signed ? min_jint : max_jint
 926     __ movl(c_rarg2, 0x80000000);
 927     __ movl(rax, 0x7fffffff);
 928     __ cmov(Assembler::positive, c_rarg2, rax);
 929 
 930     __ bind(L);
 931     __ movptr(inout, c_rarg2);
 932 
 933     __ pop(c_rarg0);
 934     __ pop(c_rarg1);
 935     __ pop(c_rarg2);
 936     __ pop(c_rarg3);
 937     __ pop(rax);
 938 
 939     __ ret(0);
 940 
 941     return start;
 942   }
 943 
 944   address generate_d2l_fixup() {
 945     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;d2l_fixup&quot;);
 946     Address inout(rsp, 6 * wordSize); // return address + 5 saves
 947 
 948     address start = __ pc();
 949 
 950     Label L;
 951 
 952     __ push(rax);
 953     __ push(c_rarg3);
 954     __ push(c_rarg2);
 955     __ push(c_rarg1);
 956     __ push(c_rarg0);
 957 
 958     __ movl(rax, 0x7ff00000);
 959     __ movq(c_rarg2, inout);
 960     __ movl(c_rarg3, c_rarg2);
 961     __ mov(c_rarg1, c_rarg2);
 962     __ mov(c_rarg0, c_rarg2);
 963     __ negl(c_rarg3);
 964     __ shrptr(c_rarg1, 0x20);
 965     __ orl(c_rarg3, c_rarg2);
 966     __ andl(c_rarg1, 0x7fffffff);
 967     __ xorl(c_rarg2, c_rarg2);
 968     __ shrl(c_rarg3, 0x1f);
 969     __ orl(c_rarg1, c_rarg3);
 970     __ cmpl(rax, c_rarg1);
 971     __ jcc(Assembler::negative, L); // NaN -&gt; 0
 972     __ testq(c_rarg0, c_rarg0); // signed ? min_jlong : max_jlong
 973     __ mov64(c_rarg2, 0x8000000000000000);
 974     __ mov64(rax, 0x7fffffffffffffff);
 975     __ cmovq(Assembler::positive, c_rarg2, rax);
 976 
 977     __ bind(L);
 978     __ movq(inout, c_rarg2);
 979 
 980     __ pop(c_rarg0);
 981     __ pop(c_rarg1);
 982     __ pop(c_rarg2);
 983     __ pop(c_rarg3);
 984     __ pop(rax);
 985 
 986     __ ret(0);
 987 
 988     return start;
 989   }
 990 
 991   address generate_fp_mask(const char *stub_name, int64_t mask) {
 992     __ align(CodeEntryAlignment);
 993     StubCodeMark mark(this, &quot;StubRoutines&quot;, stub_name);
 994     address start = __ pc();
 995 
 996     __ emit_data64( mask, relocInfo::none );
 997     __ emit_data64( mask, relocInfo::none );
 998 
 999     return start;
1000   }
1001 
1002   address generate_vector_mask(const char *stub_name, int64_t mask) {
1003     __ align(CodeEntryAlignment);
1004     StubCodeMark mark(this, &quot;StubRoutines&quot;, stub_name);
1005     address start = __ pc();
1006 
1007     __ emit_data64(mask, relocInfo::none);
1008     __ emit_data64(mask, relocInfo::none);
1009     __ emit_data64(mask, relocInfo::none);
1010     __ emit_data64(mask, relocInfo::none);
1011     __ emit_data64(mask, relocInfo::none);
1012     __ emit_data64(mask, relocInfo::none);
1013     __ emit_data64(mask, relocInfo::none);
1014     __ emit_data64(mask, relocInfo::none);
1015 
1016     return start;
1017   }
1018 
1019   address generate_vector_byte_perm_mask(const char *stub_name) {
1020     __ align(CodeEntryAlignment);
1021     StubCodeMark mark(this, &quot;StubRoutines&quot;, stub_name);
1022     address start = __ pc();
1023 
1024     __ emit_data64(0x0000000000000001, relocInfo::none);
1025     __ emit_data64(0x0000000000000003, relocInfo::none);
1026     __ emit_data64(0x0000000000000005, relocInfo::none);
1027     __ emit_data64(0x0000000000000007, relocInfo::none);
1028     __ emit_data64(0x0000000000000000, relocInfo::none);
1029     __ emit_data64(0x0000000000000002, relocInfo::none);
1030     __ emit_data64(0x0000000000000004, relocInfo::none);
1031     __ emit_data64(0x0000000000000006, relocInfo::none);
1032 
1033     return start;
1034   }
1035 
1036   // Non-destructive plausibility checks for oops
1037   //
1038   // Arguments:
1039   //    all args on stack!
1040   //
1041   // Stack after saving c_rarg3:
1042   //    [tos + 0]: saved c_rarg3
1043   //    [tos + 1]: saved c_rarg2
1044   //    [tos + 2]: saved r12 (several TemplateTable methods use it)
1045   //    [tos + 3]: saved flags
1046   //    [tos + 4]: return address
1047   //  * [tos + 5]: error message (char*)
1048   //  * [tos + 6]: object to verify (oop)
1049   //  * [tos + 7]: saved rax - saved by caller and bashed
1050   //  * [tos + 8]: saved r10 (rscratch1) - saved by caller
1051   //  * = popped on exit
1052   address generate_verify_oop() {
1053     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;verify_oop&quot;);
1054     address start = __ pc();
1055 
1056     Label exit, error;
1057 
1058     __ pushf();
1059     __ incrementl(ExternalAddress((address) StubRoutines::verify_oop_count_addr()));
1060 
1061     __ push(r12);
1062 
1063     // save c_rarg2 and c_rarg3
1064     __ push(c_rarg2);
1065     __ push(c_rarg3);
1066 
1067     enum {
1068            // After previous pushes.
1069            oop_to_verify = 6 * wordSize,
1070            saved_rax     = 7 * wordSize,
1071            saved_r10     = 8 * wordSize,
1072 
1073            // Before the call to MacroAssembler::debug(), see below.
1074            return_addr   = 16 * wordSize,
1075            error_msg     = 17 * wordSize
1076     };
1077 
1078     // get object
1079     __ movptr(rax, Address(rsp, oop_to_verify));
1080 
1081     // make sure object is &#39;reasonable&#39;
1082     __ testptr(rax, rax);
1083     __ jcc(Assembler::zero, exit); // if obj is NULL it is OK
1084 
1085 #if INCLUDE_ZGC
1086     if (UseZGC) {
1087       // Check if metadata bits indicate a bad oop
1088       __ testptr(rax, Address(r15_thread, ZThreadLocalData::address_bad_mask_offset()));
1089       __ jcc(Assembler::notZero, error);
1090     }
1091 #endif
1092 
1093     // Check if the oop is in the right area of memory
1094     __ movptr(c_rarg2, rax);
1095     __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_mask());
1096     __ andptr(c_rarg2, c_rarg3);
1097     __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_bits());
1098     __ cmpptr(c_rarg2, c_rarg3);
1099     __ jcc(Assembler::notZero, error);
1100 
1101     // set r12 to heapbase for load_klass()
1102     __ reinit_heapbase();
1103 
1104     // make sure klass is &#39;reasonable&#39;, which is not zero.
1105     __ load_klass(rax, rax);  // get klass
1106     __ testptr(rax, rax);
1107     __ jcc(Assembler::zero, error); // if klass is NULL it is broken
1108 
1109     // return if everything seems ok
1110     __ bind(exit);
1111     __ movptr(rax, Address(rsp, saved_rax));     // get saved rax back
1112     __ movptr(rscratch1, Address(rsp, saved_r10)); // get saved r10 back
1113     __ pop(c_rarg3);                             // restore c_rarg3
1114     __ pop(c_rarg2);                             // restore c_rarg2
1115     __ pop(r12);                                 // restore r12
1116     __ popf();                                   // restore flags
1117     __ ret(4 * wordSize);                        // pop caller saved stuff
1118 
1119     // handle errors
1120     __ bind(error);
1121     __ movptr(rax, Address(rsp, saved_rax));     // get saved rax back
1122     __ movptr(rscratch1, Address(rsp, saved_r10)); // get saved r10 back
1123     __ pop(c_rarg3);                             // get saved c_rarg3 back
1124     __ pop(c_rarg2);                             // get saved c_rarg2 back
1125     __ pop(r12);                                 // get saved r12 back
1126     __ popf();                                   // get saved flags off stack --
1127                                                  // will be ignored
1128 
1129     __ pusha();                                  // push registers
1130                                                  // (rip is already
1131                                                  // already pushed)
1132     // debug(char* msg, int64_t pc, int64_t regs[])
1133     // We&#39;ve popped the registers we&#39;d saved (c_rarg3, c_rarg2 and flags), and
1134     // pushed all the registers, so now the stack looks like:
1135     //     [tos +  0] 16 saved registers
1136     //     [tos + 16] return address
1137     //   * [tos + 17] error message (char*)
1138     //   * [tos + 18] object to verify (oop)
1139     //   * [tos + 19] saved rax - saved by caller and bashed
1140     //   * [tos + 20] saved r10 (rscratch1) - saved by caller
1141     //   * = popped on exit
1142 
1143     __ movptr(c_rarg0, Address(rsp, error_msg));    // pass address of error message
1144     __ movptr(c_rarg1, Address(rsp, return_addr));  // pass return address
1145     __ movq(c_rarg2, rsp);                          // pass address of regs on stack
1146     __ mov(r12, rsp);                               // remember rsp
1147     __ subptr(rsp, frame::arg_reg_save_area_bytes); // windows
1148     __ andptr(rsp, -16);                            // align stack as required by ABI
1149     BLOCK_COMMENT(&quot;call MacroAssembler::debug&quot;);
1150     __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug64)));
1151     __ hlt();
1152     return start;
1153   }
1154 
1155   //
1156   // Verify that a register contains clean 32-bits positive value
1157   // (high 32-bits are 0) so it could be used in 64-bits shifts.
1158   //
1159   //  Input:
1160   //    Rint  -  32-bits value
1161   //    Rtmp  -  scratch
1162   //
1163   void assert_clean_int(Register Rint, Register Rtmp) {
1164 #ifdef ASSERT
1165     Label L;
1166     assert_different_registers(Rtmp, Rint);
1167     __ movslq(Rtmp, Rint);
1168     __ cmpq(Rtmp, Rint);
1169     __ jcc(Assembler::equal, L);
1170     __ stop(&quot;high 32-bits of int value are not 0&quot;);
1171     __ bind(L);
1172 #endif
1173   }
1174 
1175   //  Generate overlap test for array copy stubs
1176   //
1177   //  Input:
1178   //     c_rarg0 - from
1179   //     c_rarg1 - to
1180   //     c_rarg2 - element count
1181   //
1182   //  Output:
1183   //     rax   - &amp;from[element count - 1]
1184   //
1185   void array_overlap_test(address no_overlap_target, Address::ScaleFactor sf) {
1186     assert(no_overlap_target != NULL, &quot;must be generated&quot;);
1187     array_overlap_test(no_overlap_target, NULL, sf);
1188   }
1189   void array_overlap_test(Label&amp; L_no_overlap, Address::ScaleFactor sf) {
1190     array_overlap_test(NULL, &amp;L_no_overlap, sf);
1191   }
1192   void array_overlap_test(address no_overlap_target, Label* NOLp, Address::ScaleFactor sf) {
1193     const Register from     = c_rarg0;
1194     const Register to       = c_rarg1;
1195     const Register count    = c_rarg2;
1196     const Register end_from = rax;
1197 
1198     __ cmpptr(to, from);
1199     __ lea(end_from, Address(from, count, sf, 0));
1200     if (NOLp == NULL) {
1201       ExternalAddress no_overlap(no_overlap_target);
1202       __ jump_cc(Assembler::belowEqual, no_overlap);
1203       __ cmpptr(to, end_from);
1204       __ jump_cc(Assembler::aboveEqual, no_overlap);
1205     } else {
1206       __ jcc(Assembler::belowEqual, (*NOLp));
1207       __ cmpptr(to, end_from);
1208       __ jcc(Assembler::aboveEqual, (*NOLp));
1209     }
1210   }
1211 
1212   // Shuffle first three arg regs on Windows into Linux/Solaris locations.
1213   //
1214   // Outputs:
1215   //    rdi - rcx
1216   //    rsi - rdx
1217   //    rdx - r8
1218   //    rcx - r9
1219   //
1220   // Registers r9 and r10 are used to save rdi and rsi on Windows, which latter
1221   // are non-volatile.  r9 and r10 should not be used by the caller.
1222   //
1223   DEBUG_ONLY(bool regs_in_thread;)
1224 
1225   void setup_arg_regs(int nargs = 3) {
1226     const Register saved_rdi = r9;
1227     const Register saved_rsi = r10;
1228     assert(nargs == 3 || nargs == 4, &quot;else fix&quot;);
1229 #ifdef _WIN64
1230     assert(c_rarg0 == rcx &amp;&amp; c_rarg1 == rdx &amp;&amp; c_rarg2 == r8 &amp;&amp; c_rarg3 == r9,
1231            &quot;unexpected argument registers&quot;);
1232     if (nargs &gt;= 4)
1233       __ mov(rax, r9);  // r9 is also saved_rdi
1234     __ movptr(saved_rdi, rdi);
1235     __ movptr(saved_rsi, rsi);
1236     __ mov(rdi, rcx); // c_rarg0
1237     __ mov(rsi, rdx); // c_rarg1
1238     __ mov(rdx, r8);  // c_rarg2
1239     if (nargs &gt;= 4)
1240       __ mov(rcx, rax); // c_rarg3 (via rax)
1241 #else
1242     assert(c_rarg0 == rdi &amp;&amp; c_rarg1 == rsi &amp;&amp; c_rarg2 == rdx &amp;&amp; c_rarg3 == rcx,
1243            &quot;unexpected argument registers&quot;);
1244 #endif
1245     DEBUG_ONLY(regs_in_thread = false;)
1246   }
1247 
1248   void restore_arg_regs() {
1249     assert(!regs_in_thread, &quot;wrong call to restore_arg_regs&quot;);
1250     const Register saved_rdi = r9;
1251     const Register saved_rsi = r10;
1252 #ifdef _WIN64
1253     __ movptr(rdi, saved_rdi);
1254     __ movptr(rsi, saved_rsi);
1255 #endif
1256   }
1257 
1258   // This is used in places where r10 is a scratch register, and can
1259   // be adapted if r9 is needed also.
1260   void setup_arg_regs_using_thread() {
1261     const Register saved_r15 = r9;
1262 #ifdef _WIN64
1263     __ mov(saved_r15, r15);  // r15 is callee saved and needs to be restored
1264     __ get_thread(r15_thread);
1265     assert(c_rarg0 == rcx &amp;&amp; c_rarg1 == rdx &amp;&amp; c_rarg2 == r8 &amp;&amp; c_rarg3 == r9,
1266            &quot;unexpected argument registers&quot;);
1267     __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())), rdi);
1268     __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())), rsi);
1269 
1270     __ mov(rdi, rcx); // c_rarg0
1271     __ mov(rsi, rdx); // c_rarg1
1272     __ mov(rdx, r8);  // c_rarg2
1273 #else
1274     assert(c_rarg0 == rdi &amp;&amp; c_rarg1 == rsi &amp;&amp; c_rarg2 == rdx &amp;&amp; c_rarg3 == rcx,
1275            &quot;unexpected argument registers&quot;);
1276 #endif
1277     DEBUG_ONLY(regs_in_thread = true;)
1278   }
1279 
1280   void restore_arg_regs_using_thread() {
1281     assert(regs_in_thread, &quot;wrong call to restore_arg_regs&quot;);
1282     const Register saved_r15 = r9;
1283 #ifdef _WIN64
1284     __ get_thread(r15_thread);
1285     __ movptr(rsi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())));
1286     __ movptr(rdi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())));
1287     __ mov(r15, saved_r15);  // r15 is callee saved and needs to be restored
1288 #endif
1289   }
1290 
1291   // Copy big chunks forward
1292   //
1293   // Inputs:
1294   //   end_from     - source arrays end address
1295   //   end_to       - destination array end address
1296   //   qword_count  - 64-bits element count, negative
1297   //   to           - scratch
1298   //   L_copy_bytes - entry label
1299   //   L_copy_8_bytes  - exit  label
1300   //
1301   void copy_bytes_forward(Register end_from, Register end_to,
1302                              Register qword_count, Register to,
1303                              Label&amp; L_copy_bytes, Label&amp; L_copy_8_bytes) {
1304     DEBUG_ONLY(__ stop(&quot;enter at entry label, not here&quot;));
1305     Label L_loop;
1306     __ align(OptoLoopAlignment);
1307     if (UseUnalignedLoadStores) {
1308       Label L_end;
1309       // Copy 64-bytes per iteration
1310       if (UseAVX &gt; 2) {
1311         Label L_loop_avx512, L_loop_avx2, L_32_byte_head, L_above_threshold, L_below_threshold;
1312 
1313         __ BIND(L_copy_bytes);
1314         __ cmpptr(qword_count, (-1 * AVX3Threshold / 8));
1315         __ jccb(Assembler::less, L_above_threshold);
1316         __ jmpb(L_below_threshold);
1317 
1318         __ bind(L_loop_avx512);
1319         __ evmovdqul(xmm0, Address(end_from, qword_count, Address::times_8, -56), Assembler::AVX_512bit);
1320         __ evmovdqul(Address(end_to, qword_count, Address::times_8, -56), xmm0, Assembler::AVX_512bit);
1321         __ bind(L_above_threshold);
1322         __ addptr(qword_count, 8);
1323         __ jcc(Assembler::lessEqual, L_loop_avx512);
1324         __ jmpb(L_32_byte_head);
1325 
1326         __ bind(L_loop_avx2);
1327         __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));
1328         __ vmovdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);
1329         __ vmovdqu(xmm1, Address(end_from, qword_count, Address::times_8, -24));
1330         __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm1);
1331         __ bind(L_below_threshold);
1332         __ addptr(qword_count, 8);
1333         __ jcc(Assembler::lessEqual, L_loop_avx2);
1334 
1335         __ bind(L_32_byte_head);
1336         __ subptr(qword_count, 4);  // sub(8) and add(4)
1337         __ jccb(Assembler::greater, L_end);
1338       } else {
1339         __ BIND(L_loop);
1340         if (UseAVX == 2) {
1341           __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));
1342           __ vmovdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);
1343           __ vmovdqu(xmm1, Address(end_from, qword_count, Address::times_8, -24));
1344           __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm1);
1345         } else {
1346           __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));
1347           __ movdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);
1348           __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, -40));
1349           __ movdqu(Address(end_to, qword_count, Address::times_8, -40), xmm1);
1350           __ movdqu(xmm2, Address(end_from, qword_count, Address::times_8, -24));
1351           __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm2);
1352           __ movdqu(xmm3, Address(end_from, qword_count, Address::times_8, - 8));
1353           __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm3);
1354         }
1355 
1356         __ BIND(L_copy_bytes);
1357         __ addptr(qword_count, 8);
1358         __ jcc(Assembler::lessEqual, L_loop);
1359         __ subptr(qword_count, 4);  // sub(8) and add(4)
1360         __ jccb(Assembler::greater, L_end);
1361       }
1362       // Copy trailing 32 bytes
1363       if (UseAVX &gt;= 2) {
1364         __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));
1365         __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);
1366       } else {
1367         __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));
1368         __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);
1369         __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, - 8));
1370         __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm1);
1371       }
1372       __ addptr(qword_count, 4);
1373       __ BIND(L_end);
1374       if (UseAVX &gt;= 2) {
1375         // clean upper bits of YMM registers
1376         __ vpxor(xmm0, xmm0);
1377         __ vpxor(xmm1, xmm1);
1378       }
1379     } else {
1380       // Copy 32-bytes per iteration
1381       __ BIND(L_loop);
1382       __ movq(to, Address(end_from, qword_count, Address::times_8, -24));
1383       __ movq(Address(end_to, qword_count, Address::times_8, -24), to);
1384       __ movq(to, Address(end_from, qword_count, Address::times_8, -16));
1385       __ movq(Address(end_to, qword_count, Address::times_8, -16), to);
1386       __ movq(to, Address(end_from, qword_count, Address::times_8, - 8));
1387       __ movq(Address(end_to, qword_count, Address::times_8, - 8), to);
1388       __ movq(to, Address(end_from, qword_count, Address::times_8, - 0));
1389       __ movq(Address(end_to, qword_count, Address::times_8, - 0), to);
1390 
1391       __ BIND(L_copy_bytes);
1392       __ addptr(qword_count, 4);
1393       __ jcc(Assembler::lessEqual, L_loop);
1394     }
1395     __ subptr(qword_count, 4);
1396     __ jcc(Assembler::less, L_copy_8_bytes); // Copy trailing qwords
1397   }
1398 
1399   // Copy big chunks backward
1400   //
1401   // Inputs:
1402   //   from         - source arrays address
1403   //   dest         - destination array address
1404   //   qword_count  - 64-bits element count
1405   //   to           - scratch
1406   //   L_copy_bytes - entry label
1407   //   L_copy_8_bytes  - exit  label
1408   //
1409   void copy_bytes_backward(Register from, Register dest,
1410                               Register qword_count, Register to,
1411                               Label&amp; L_copy_bytes, Label&amp; L_copy_8_bytes) {
1412     DEBUG_ONLY(__ stop(&quot;enter at entry label, not here&quot;));
1413     Label L_loop;
1414     __ align(OptoLoopAlignment);
1415     if (UseUnalignedLoadStores) {
1416       Label L_end;
1417       // Copy 64-bytes per iteration
1418       if (UseAVX &gt; 2) {
1419         Label L_loop_avx512, L_loop_avx2, L_32_byte_head, L_above_threshold, L_below_threshold;
1420 
1421         __ BIND(L_copy_bytes);
1422         __ cmpptr(qword_count, (AVX3Threshold / 8));
1423         __ jccb(Assembler::greater, L_above_threshold);
1424         __ jmpb(L_below_threshold);
1425 
1426         __ BIND(L_loop_avx512);
1427         __ evmovdqul(xmm0, Address(from, qword_count, Address::times_8, 0), Assembler::AVX_512bit);
1428         __ evmovdqul(Address(dest, qword_count, Address::times_8, 0), xmm0, Assembler::AVX_512bit);
1429         __ bind(L_above_threshold);
1430         __ subptr(qword_count, 8);
1431         __ jcc(Assembler::greaterEqual, L_loop_avx512);
1432         __ jmpb(L_32_byte_head);
1433 
1434         __ bind(L_loop_avx2);
1435         __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 32));
1436         __ vmovdqu(Address(dest, qword_count, Address::times_8, 32), xmm0);
1437         __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8, 0));
1438         __ vmovdqu(Address(dest, qword_count, Address::times_8, 0), xmm1);
1439         __ bind(L_below_threshold);
1440         __ subptr(qword_count, 8);
1441         __ jcc(Assembler::greaterEqual, L_loop_avx2);
1442 
1443         __ bind(L_32_byte_head);
1444         __ addptr(qword_count, 4);  // add(8) and sub(4)
1445         __ jccb(Assembler::less, L_end);
1446       } else {
1447         __ BIND(L_loop);
1448         if (UseAVX == 2) {
1449           __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 32));
1450           __ vmovdqu(Address(dest, qword_count, Address::times_8, 32), xmm0);
1451           __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8,  0));
1452           __ vmovdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);
1453         } else {
1454           __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 48));
1455           __ movdqu(Address(dest, qword_count, Address::times_8, 48), xmm0);
1456           __ movdqu(xmm1, Address(from, qword_count, Address::times_8, 32));
1457           __ movdqu(Address(dest, qword_count, Address::times_8, 32), xmm1);
1458           __ movdqu(xmm2, Address(from, qword_count, Address::times_8, 16));
1459           __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm2);
1460           __ movdqu(xmm3, Address(from, qword_count, Address::times_8,  0));
1461           __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm3);
1462         }
1463 
1464         __ BIND(L_copy_bytes);
1465         __ subptr(qword_count, 8);
1466         __ jcc(Assembler::greaterEqual, L_loop);
1467 
1468         __ addptr(qword_count, 4);  // add(8) and sub(4)
1469         __ jccb(Assembler::less, L_end);
1470       }
1471       // Copy trailing 32 bytes
1472       if (UseAVX &gt;= 2) {
1473         __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 0));
1474         __ vmovdqu(Address(dest, qword_count, Address::times_8, 0), xmm0);
1475       } else {
1476         __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 16));
1477         __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm0);
1478         __ movdqu(xmm1, Address(from, qword_count, Address::times_8,  0));
1479         __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);
1480       }
1481       __ subptr(qword_count, 4);
1482       __ BIND(L_end);
1483       if (UseAVX &gt;= 2) {
1484         // clean upper bits of YMM registers
1485         __ vpxor(xmm0, xmm0);
1486         __ vpxor(xmm1, xmm1);
1487       }
1488     } else {
1489       // Copy 32-bytes per iteration
1490       __ BIND(L_loop);
1491       __ movq(to, Address(from, qword_count, Address::times_8, 24));
1492       __ movq(Address(dest, qword_count, Address::times_8, 24), to);
1493       __ movq(to, Address(from, qword_count, Address::times_8, 16));
1494       __ movq(Address(dest, qword_count, Address::times_8, 16), to);
1495       __ movq(to, Address(from, qword_count, Address::times_8,  8));
1496       __ movq(Address(dest, qword_count, Address::times_8,  8), to);
1497       __ movq(to, Address(from, qword_count, Address::times_8,  0));
1498       __ movq(Address(dest, qword_count, Address::times_8,  0), to);
1499 
1500       __ BIND(L_copy_bytes);
1501       __ subptr(qword_count, 4);
1502       __ jcc(Assembler::greaterEqual, L_loop);
1503     }
1504     __ addptr(qword_count, 4);
1505     __ jcc(Assembler::greater, L_copy_8_bytes); // Copy trailing qwords
1506   }
1507 
1508   // Arguments:
1509   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1510   //             ignored
1511   //   name    - stub name string
1512   //
1513   // Inputs:
1514   //   c_rarg0   - source array address
1515   //   c_rarg1   - destination array address
1516   //   c_rarg2   - element count, treated as ssize_t, can be zero
1517   //
1518   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1519   // we let the hardware handle it.  The one to eight bytes within words,
1520   // dwords or qwords that span cache line boundaries will still be loaded
1521   // and stored atomically.
1522   //
1523   // Side Effects:
1524   //   disjoint_byte_copy_entry is set to the no-overlap entry point
1525   //   used by generate_conjoint_byte_copy().
1526   //
1527   address generate_disjoint_byte_copy(bool aligned, address* entry, const char *name) {
1528     __ align(CodeEntryAlignment);
1529     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1530     address start = __ pc();
1531 
1532     Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;
1533     Label L_copy_byte, L_exit;
1534     const Register from        = rdi;  // source array address
1535     const Register to          = rsi;  // destination array address
1536     const Register count       = rdx;  // elements count
1537     const Register byte_count  = rcx;
1538     const Register qword_count = count;
1539     const Register end_from    = from; // source array end address
1540     const Register end_to      = to;   // destination array end address
1541     // End pointers are inclusive, and if count is not zero they point
1542     // to the last unit copied:  end_to[0] := end_from[0]
1543 
1544     __ enter(); // required for proper stackwalking of RuntimeStub frame
1545     assert_clean_int(c_rarg2, rax);    // Make sure &#39;count&#39; is clean int.
1546 
1547     if (entry != NULL) {
1548       *entry = __ pc();
1549        // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1550       BLOCK_COMMENT(&quot;Entry:&quot;);
1551     }
1552 
1553     setup_arg_regs(); // from =&gt; rdi, to =&gt; rsi, count =&gt; rdx
1554                       // r9 and r10 may be used to save non-volatile registers
1555 
1556     {
1557       // UnsafeCopyMemory page error: continue after ucm
1558       UnsafeCopyMemoryMark ucmm(this, !aligned, true);
1559       // &#39;from&#39;, &#39;to&#39; and &#39;count&#39; are now valid
1560       __ movptr(byte_count, count);
1561       __ shrptr(count, 3); // count =&gt; qword_count
1562 
1563       // Copy from low to high addresses.  Use &#39;to&#39; as scratch.
1564       __ lea(end_from, Address(from, qword_count, Address::times_8, -8));
1565       __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));
1566       __ negptr(qword_count); // make the count negative
1567       __ jmp(L_copy_bytes);
1568 
1569       // Copy trailing qwords
1570     __ BIND(L_copy_8_bytes);
1571       __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));
1572       __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);
1573       __ increment(qword_count);
1574       __ jcc(Assembler::notZero, L_copy_8_bytes);
1575 
1576       // Check for and copy trailing dword
1577     __ BIND(L_copy_4_bytes);
1578       __ testl(byte_count, 4);
1579       __ jccb(Assembler::zero, L_copy_2_bytes);
1580       __ movl(rax, Address(end_from, 8));
1581       __ movl(Address(end_to, 8), rax);
1582 
1583       __ addptr(end_from, 4);
1584       __ addptr(end_to, 4);
1585 
1586       // Check for and copy trailing word
1587     __ BIND(L_copy_2_bytes);
1588       __ testl(byte_count, 2);
1589       __ jccb(Assembler::zero, L_copy_byte);
1590       __ movw(rax, Address(end_from, 8));
1591       __ movw(Address(end_to, 8), rax);
1592 
1593       __ addptr(end_from, 2);
1594       __ addptr(end_to, 2);
1595 
1596       // Check for and copy trailing byte
1597     __ BIND(L_copy_byte);
1598       __ testl(byte_count, 1);
1599       __ jccb(Assembler::zero, L_exit);
1600       __ movb(rax, Address(end_from, 8));
1601       __ movb(Address(end_to, 8), rax);
1602     }
1603   __ BIND(L_exit);
1604     address ucme_exit_pc = __ pc();
1605     restore_arg_regs();
1606     inc_counter_np(SharedRuntime::_jbyte_array_copy_ctr); // Update counter after rscratch1 is free
1607     __ xorptr(rax, rax); // return 0
1608     __ vzeroupper();
1609     __ leave(); // required for proper stackwalking of RuntimeStub frame
1610     __ ret(0);
1611 
1612     {
1613       UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);
1614       // Copy in multi-bytes chunks
1615       copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);
1616       __ jmp(L_copy_4_bytes);
1617     }
1618     return start;
1619   }
1620 
1621   // Arguments:
1622   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1623   //             ignored
1624   //   name    - stub name string
1625   //
1626   // Inputs:
1627   //   c_rarg0   - source array address
1628   //   c_rarg1   - destination array address
1629   //   c_rarg2   - element count, treated as ssize_t, can be zero
1630   //
1631   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1632   // we let the hardware handle it.  The one to eight bytes within words,
1633   // dwords or qwords that span cache line boundaries will still be loaded
1634   // and stored atomically.
1635   //
1636   address generate_conjoint_byte_copy(bool aligned, address nooverlap_target,
1637                                       address* entry, const char *name) {
1638     __ align(CodeEntryAlignment);
1639     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1640     address start = __ pc();
1641 
1642     Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;
1643     const Register from        = rdi;  // source array address
1644     const Register to          = rsi;  // destination array address
1645     const Register count       = rdx;  // elements count
1646     const Register byte_count  = rcx;
1647     const Register qword_count = count;
1648 
1649     __ enter(); // required for proper stackwalking of RuntimeStub frame
1650     assert_clean_int(c_rarg2, rax);    // Make sure &#39;count&#39; is clean int.
1651 
1652     if (entry != NULL) {
1653       *entry = __ pc();
1654       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1655       BLOCK_COMMENT(&quot;Entry:&quot;);
1656     }
1657 
1658     array_overlap_test(nooverlap_target, Address::times_1);
1659     setup_arg_regs(); // from =&gt; rdi, to =&gt; rsi, count =&gt; rdx
1660                       // r9 and r10 may be used to save non-volatile registers
1661 
1662     {
1663       // UnsafeCopyMemory page error: continue after ucm
1664       UnsafeCopyMemoryMark ucmm(this, !aligned, true);
1665       // &#39;from&#39;, &#39;to&#39; and &#39;count&#39; are now valid
1666       __ movptr(byte_count, count);
1667       __ shrptr(count, 3);   // count =&gt; qword_count
1668 
1669       // Copy from high to low addresses.
1670 
1671       // Check for and copy trailing byte
1672       __ testl(byte_count, 1);
1673       __ jcc(Assembler::zero, L_copy_2_bytes);
1674       __ movb(rax, Address(from, byte_count, Address::times_1, -1));
1675       __ movb(Address(to, byte_count, Address::times_1, -1), rax);
1676       __ decrement(byte_count); // Adjust for possible trailing word
1677 
1678       // Check for and copy trailing word
1679     __ BIND(L_copy_2_bytes);
1680       __ testl(byte_count, 2);
1681       __ jcc(Assembler::zero, L_copy_4_bytes);
1682       __ movw(rax, Address(from, byte_count, Address::times_1, -2));
1683       __ movw(Address(to, byte_count, Address::times_1, -2), rax);
1684 
1685       // Check for and copy trailing dword
1686     __ BIND(L_copy_4_bytes);
1687       __ testl(byte_count, 4);
1688       __ jcc(Assembler::zero, L_copy_bytes);
1689       __ movl(rax, Address(from, qword_count, Address::times_8));
1690       __ movl(Address(to, qword_count, Address::times_8), rax);
1691       __ jmp(L_copy_bytes);
1692 
1693       // Copy trailing qwords
1694     __ BIND(L_copy_8_bytes);
1695       __ movq(rax, Address(from, qword_count, Address::times_8, -8));
1696       __ movq(Address(to, qword_count, Address::times_8, -8), rax);
1697       __ decrement(qword_count);
1698       __ jcc(Assembler::notZero, L_copy_8_bytes);
1699     }
1700     restore_arg_regs();
1701     inc_counter_np(SharedRuntime::_jbyte_array_copy_ctr); // Update counter after rscratch1 is free
1702     __ xorptr(rax, rax); // return 0
1703     __ vzeroupper();
1704     __ leave(); // required for proper stackwalking of RuntimeStub frame
1705     __ ret(0);
1706 
1707     {
1708       // UnsafeCopyMemory page error: continue after ucm
1709       UnsafeCopyMemoryMark ucmm(this, !aligned, true);
1710       // Copy in multi-bytes chunks
1711       copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);
1712     }
1713     restore_arg_regs();
1714     inc_counter_np(SharedRuntime::_jbyte_array_copy_ctr); // Update counter after rscratch1 is free
1715     __ xorptr(rax, rax); // return 0
1716     __ vzeroupper();
1717     __ leave(); // required for proper stackwalking of RuntimeStub frame
1718     __ ret(0);
1719 
1720     return start;
1721   }
1722 
1723   // Arguments:
1724   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1725   //             ignored
1726   //   name    - stub name string
1727   //
1728   // Inputs:
1729   //   c_rarg0   - source array address
1730   //   c_rarg1   - destination array address
1731   //   c_rarg2   - element count, treated as ssize_t, can be zero
1732   //
1733   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4- or 2-byte boundaries, we
1734   // let the hardware handle it.  The two or four words within dwords
1735   // or qwords that span cache line boundaries will still be loaded
1736   // and stored atomically.
1737   //
1738   // Side Effects:
1739   //   disjoint_short_copy_entry is set to the no-overlap entry point
1740   //   used by generate_conjoint_short_copy().
1741   //
1742   address generate_disjoint_short_copy(bool aligned, address *entry, const char *name) {
1743     __ align(CodeEntryAlignment);
1744     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1745     address start = __ pc();
1746 
1747     Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes,L_copy_2_bytes,L_exit;
1748     const Register from        = rdi;  // source array address
1749     const Register to          = rsi;  // destination array address
1750     const Register count       = rdx;  // elements count
1751     const Register word_count  = rcx;
1752     const Register qword_count = count;
1753     const Register end_from    = from; // source array end address
1754     const Register end_to      = to;   // destination array end address
1755     // End pointers are inclusive, and if count is not zero they point
1756     // to the last unit copied:  end_to[0] := end_from[0]
1757 
1758     __ enter(); // required for proper stackwalking of RuntimeStub frame
1759     assert_clean_int(c_rarg2, rax);    // Make sure &#39;count&#39; is clean int.
1760 
1761     if (entry != NULL) {
1762       *entry = __ pc();
1763       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1764       BLOCK_COMMENT(&quot;Entry:&quot;);
1765     }
1766 
1767     setup_arg_regs(); // from =&gt; rdi, to =&gt; rsi, count =&gt; rdx
1768                       // r9 and r10 may be used to save non-volatile registers
1769 
1770     {
1771       // UnsafeCopyMemory page error: continue after ucm
1772       UnsafeCopyMemoryMark ucmm(this, !aligned, true);
1773       // &#39;from&#39;, &#39;to&#39; and &#39;count&#39; are now valid
1774       __ movptr(word_count, count);
1775       __ shrptr(count, 2); // count =&gt; qword_count
1776 
1777       // Copy from low to high addresses.  Use &#39;to&#39; as scratch.
1778       __ lea(end_from, Address(from, qword_count, Address::times_8, -8));
1779       __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));
1780       __ negptr(qword_count);
1781       __ jmp(L_copy_bytes);
1782 
1783       // Copy trailing qwords
1784     __ BIND(L_copy_8_bytes);
1785       __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));
1786       __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);
1787       __ increment(qword_count);
1788       __ jcc(Assembler::notZero, L_copy_8_bytes);
1789 
1790       // Original &#39;dest&#39; is trashed, so we can&#39;t use it as a
1791       // base register for a possible trailing word copy
1792 
1793       // Check for and copy trailing dword
1794     __ BIND(L_copy_4_bytes);
1795       __ testl(word_count, 2);
1796       __ jccb(Assembler::zero, L_copy_2_bytes);
1797       __ movl(rax, Address(end_from, 8));
1798       __ movl(Address(end_to, 8), rax);
1799 
1800       __ addptr(end_from, 4);
1801       __ addptr(end_to, 4);
1802 
1803       // Check for and copy trailing word
1804     __ BIND(L_copy_2_bytes);
1805       __ testl(word_count, 1);
1806       __ jccb(Assembler::zero, L_exit);
1807       __ movw(rax, Address(end_from, 8));
1808       __ movw(Address(end_to, 8), rax);
1809     }
1810   __ BIND(L_exit);
1811     address ucme_exit_pc = __ pc();
1812     restore_arg_regs();
1813     inc_counter_np(SharedRuntime::_jshort_array_copy_ctr); // Update counter after rscratch1 is free
1814     __ xorptr(rax, rax); // return 0
1815     __ vzeroupper();
1816     __ leave(); // required for proper stackwalking of RuntimeStub frame
1817     __ ret(0);
1818 
1819     {
1820       UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);
1821       // Copy in multi-bytes chunks
1822       copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);
1823       __ jmp(L_copy_4_bytes);
1824     }
1825 
1826     return start;
1827   }
1828 
1829   address generate_fill(BasicType t, bool aligned, const char *name) {
1830     __ align(CodeEntryAlignment);
1831     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1832     address start = __ pc();
1833 
1834     BLOCK_COMMENT(&quot;Entry:&quot;);
1835 
1836     const Register to       = c_rarg0;  // source array address
1837     const Register value    = c_rarg1;  // value
1838     const Register count    = c_rarg2;  // elements count
1839 
1840     __ enter(); // required for proper stackwalking of RuntimeStub frame
1841 
1842     __ generate_fill(t, aligned, to, value, count, rax, xmm0);
1843 
1844     __ vzeroupper();
1845     __ leave(); // required for proper stackwalking of RuntimeStub frame
1846     __ ret(0);
1847     return start;
1848   }
1849 
1850   // Arguments:
1851   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1852   //             ignored
1853   //   name    - stub name string
1854   //
1855   // Inputs:
1856   //   c_rarg0   - source array address
1857   //   c_rarg1   - destination array address
1858   //   c_rarg2   - element count, treated as ssize_t, can be zero
1859   //
1860   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4- or 2-byte boundaries, we
1861   // let the hardware handle it.  The two or four words within dwords
1862   // or qwords that span cache line boundaries will still be loaded
1863   // and stored atomically.
1864   //
1865   address generate_conjoint_short_copy(bool aligned, address nooverlap_target,
1866                                        address *entry, const char *name) {
1867     __ align(CodeEntryAlignment);
1868     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1869     address start = __ pc();
1870 
1871     Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes;
1872     const Register from        = rdi;  // source array address
1873     const Register to          = rsi;  // destination array address
1874     const Register count       = rdx;  // elements count
1875     const Register word_count  = rcx;
1876     const Register qword_count = count;
1877 
1878     __ enter(); // required for proper stackwalking of RuntimeStub frame
1879     assert_clean_int(c_rarg2, rax);    // Make sure &#39;count&#39; is clean int.
1880 
1881     if (entry != NULL) {
1882       *entry = __ pc();
1883       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1884       BLOCK_COMMENT(&quot;Entry:&quot;);
1885     }
1886 
1887     array_overlap_test(nooverlap_target, Address::times_2);
1888     setup_arg_regs(); // from =&gt; rdi, to =&gt; rsi, count =&gt; rdx
1889                       // r9 and r10 may be used to save non-volatile registers
1890 
1891     {
1892       // UnsafeCopyMemory page error: continue after ucm
1893       UnsafeCopyMemoryMark ucmm(this, !aligned, true);
1894       // &#39;from&#39;, &#39;to&#39; and &#39;count&#39; are now valid
1895       __ movptr(word_count, count);
1896       __ shrptr(count, 2); // count =&gt; qword_count
1897 
1898       // Copy from high to low addresses.  Use &#39;to&#39; as scratch.
1899 
1900       // Check for and copy trailing word
1901       __ testl(word_count, 1);
1902       __ jccb(Assembler::zero, L_copy_4_bytes);
1903       __ movw(rax, Address(from, word_count, Address::times_2, -2));
1904       __ movw(Address(to, word_count, Address::times_2, -2), rax);
1905 
1906      // Check for and copy trailing dword
1907     __ BIND(L_copy_4_bytes);
1908       __ testl(word_count, 2);
1909       __ jcc(Assembler::zero, L_copy_bytes);
1910       __ movl(rax, Address(from, qword_count, Address::times_8));
1911       __ movl(Address(to, qword_count, Address::times_8), rax);
1912       __ jmp(L_copy_bytes);
1913 
1914       // Copy trailing qwords
1915     __ BIND(L_copy_8_bytes);
1916       __ movq(rax, Address(from, qword_count, Address::times_8, -8));
1917       __ movq(Address(to, qword_count, Address::times_8, -8), rax);
1918       __ decrement(qword_count);
1919       __ jcc(Assembler::notZero, L_copy_8_bytes);
1920     }
1921     restore_arg_regs();
1922     inc_counter_np(SharedRuntime::_jshort_array_copy_ctr); // Update counter after rscratch1 is free
1923     __ xorptr(rax, rax); // return 0
1924     __ vzeroupper();
1925     __ leave(); // required for proper stackwalking of RuntimeStub frame
1926     __ ret(0);
1927 
1928     {
1929       // UnsafeCopyMemory page error: continue after ucm
1930       UnsafeCopyMemoryMark ucmm(this, !aligned, true);
1931       // Copy in multi-bytes chunks
1932       copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);
1933     }
1934     restore_arg_regs();
1935     inc_counter_np(SharedRuntime::_jshort_array_copy_ctr); // Update counter after rscratch1 is free
1936     __ xorptr(rax, rax); // return 0
1937     __ vzeroupper();
1938     __ leave(); // required for proper stackwalking of RuntimeStub frame
1939     __ ret(0);
1940 
1941     return start;
1942   }
1943 
1944   // Arguments:
1945   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1946   //             ignored
1947   //   is_oop  - true =&gt; oop array, so generate store check code
1948   //   name    - stub name string
1949   //
1950   // Inputs:
1951   //   c_rarg0   - source array address
1952   //   c_rarg1   - destination array address
1953   //   c_rarg2   - element count, treated as ssize_t, can be zero
1954   //
1955   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1956   // the hardware handle it.  The two dwords within qwords that span
1957   // cache line boundaries will still be loaded and stored atomicly.
1958   //
1959   // Side Effects:
1960   //   disjoint_int_copy_entry is set to the no-overlap entry point
1961   //   used by generate_conjoint_int_oop_copy().
1962   //
1963   address generate_disjoint_int_oop_copy(bool aligned, bool is_oop, address* entry,
1964                                          const char *name, bool dest_uninitialized = false) {
1965     __ align(CodeEntryAlignment);
1966     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1967     address start = __ pc();
1968 
1969     Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_exit;
1970     const Register from        = rdi;  // source array address
1971     const Register to          = rsi;  // destination array address
1972     const Register count       = rdx;  // elements count
1973     const Register dword_count = rcx;
1974     const Register qword_count = count;
1975     const Register end_from    = from; // source array end address
1976     const Register end_to      = to;   // destination array end address
1977     // End pointers are inclusive, and if count is not zero they point
1978     // to the last unit copied:  end_to[0] := end_from[0]
1979 
1980     __ enter(); // required for proper stackwalking of RuntimeStub frame
1981     assert_clean_int(c_rarg2, rax);    // Make sure &#39;count&#39; is clean int.
1982 
1983     if (entry != NULL) {
1984       *entry = __ pc();
1985       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1986       BLOCK_COMMENT(&quot;Entry:&quot;);
1987     }
1988 
1989     setup_arg_regs_using_thread(); // from =&gt; rdi, to =&gt; rsi, count =&gt; rdx
1990                                    // r9 is used to save r15_thread
1991 
1992     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;
1993     if (dest_uninitialized) {
1994       decorators |= IS_DEST_UNINITIALIZED;
1995     }
1996     if (aligned) {
1997       decorators |= ARRAYCOPY_ALIGNED;
1998     }
1999 
2000     BasicType type = is_oop ? T_OBJECT : T_INT;
2001     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
2002     bs-&gt;arraycopy_prologue(_masm, decorators, type, from, to, count);
2003 
2004     {
2005       // UnsafeCopyMemory page error: continue after ucm
2006       UnsafeCopyMemoryMark ucmm(this, !is_oop &amp;&amp; !aligned, true);
2007       // &#39;from&#39;, &#39;to&#39; and &#39;count&#39; are now valid
2008       __ movptr(dword_count, count);
2009       __ shrptr(count, 1); // count =&gt; qword_count
2010 
2011       // Copy from low to high addresses.  Use &#39;to&#39; as scratch.
2012       __ lea(end_from, Address(from, qword_count, Address::times_8, -8));
2013       __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));
2014       __ negptr(qword_count);
2015       __ jmp(L_copy_bytes);
2016 
2017       // Copy trailing qwords
2018     __ BIND(L_copy_8_bytes);
2019       __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));
2020       __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);
2021       __ increment(qword_count);
2022       __ jcc(Assembler::notZero, L_copy_8_bytes);
2023 
2024       // Check for and copy trailing dword
2025     __ BIND(L_copy_4_bytes);
2026       __ testl(dword_count, 1); // Only byte test since the value is 0 or 1
2027       __ jccb(Assembler::zero, L_exit);
2028       __ movl(rax, Address(end_from, 8));
2029       __ movl(Address(end_to, 8), rax);
2030     }
2031   __ BIND(L_exit);
2032     address ucme_exit_pc = __ pc();
2033     bs-&gt;arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);
2034     restore_arg_regs_using_thread();
2035     inc_counter_np(SharedRuntime::_jint_array_copy_ctr); // Update counter after rscratch1 is free
2036     __ vzeroupper();
2037     __ xorptr(rax, rax); // return 0
2038     __ leave(); // required for proper stackwalking of RuntimeStub frame
2039     __ ret(0);
2040 
2041     {
2042       UnsafeCopyMemoryMark ucmm(this, !is_oop &amp;&amp; !aligned, false, ucme_exit_pc);
2043       // Copy in multi-bytes chunks
2044       copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);
2045       __ jmp(L_copy_4_bytes);
2046     }
2047 
2048     return start;
2049   }
2050 
2051   // Arguments:
2052   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
2053   //             ignored
2054   //   is_oop  - true =&gt; oop array, so generate store check code
2055   //   name    - stub name string
2056   //
2057   // Inputs:
2058   //   c_rarg0   - source array address
2059   //   c_rarg1   - destination array address
2060   //   c_rarg2   - element count, treated as ssize_t, can be zero
2061   //
2062   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
2063   // the hardware handle it.  The two dwords within qwords that span
2064   // cache line boundaries will still be loaded and stored atomicly.
2065   //
2066   address generate_conjoint_int_oop_copy(bool aligned, bool is_oop, address nooverlap_target,
2067                                          address *entry, const char *name,
2068                                          bool dest_uninitialized = false) {
2069     __ align(CodeEntryAlignment);
2070     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2071     address start = __ pc();
2072 
2073     Label L_copy_bytes, L_copy_8_bytes, L_exit;
2074     const Register from        = rdi;  // source array address
2075     const Register to          = rsi;  // destination array address
2076     const Register count       = rdx;  // elements count
2077     const Register dword_count = rcx;
2078     const Register qword_count = count;
2079 
2080     __ enter(); // required for proper stackwalking of RuntimeStub frame
2081     assert_clean_int(c_rarg2, rax);    // Make sure &#39;count&#39; is clean int.
2082 
2083     if (entry != NULL) {
2084       *entry = __ pc();
2085        // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
2086       BLOCK_COMMENT(&quot;Entry:&quot;);
2087     }
2088 
2089     array_overlap_test(nooverlap_target, Address::times_4);
2090     setup_arg_regs_using_thread(); // from =&gt; rdi, to =&gt; rsi, count =&gt; rdx
2091                                    // r9 is used to save r15_thread
2092 
2093     DecoratorSet decorators = IN_HEAP | IS_ARRAY;
2094     if (dest_uninitialized) {
2095       decorators |= IS_DEST_UNINITIALIZED;
2096     }
2097     if (aligned) {
2098       decorators |= ARRAYCOPY_ALIGNED;
2099     }
2100 
2101     BasicType type = is_oop ? T_OBJECT : T_INT;
2102     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
2103     // no registers are destroyed by this call
2104     bs-&gt;arraycopy_prologue(_masm, decorators, type, from, to, count);
2105 
2106     assert_clean_int(count, rax); // Make sure &#39;count&#39; is clean int.
2107     {
2108       // UnsafeCopyMemory page error: continue after ucm
2109       UnsafeCopyMemoryMark ucmm(this, !is_oop &amp;&amp; !aligned, true);
2110       // &#39;from&#39;, &#39;to&#39; and &#39;count&#39; are now valid
2111       __ movptr(dword_count, count);
2112       __ shrptr(count, 1); // count =&gt; qword_count
2113 
2114       // Copy from high to low addresses.  Use &#39;to&#39; as scratch.
2115 
2116       // Check for and copy trailing dword
2117       __ testl(dword_count, 1);
2118       __ jcc(Assembler::zero, L_copy_bytes);
2119       __ movl(rax, Address(from, dword_count, Address::times_4, -4));
2120       __ movl(Address(to, dword_count, Address::times_4, -4), rax);
2121       __ jmp(L_copy_bytes);
2122 
2123       // Copy trailing qwords
2124     __ BIND(L_copy_8_bytes);
2125       __ movq(rax, Address(from, qword_count, Address::times_8, -8));
2126       __ movq(Address(to, qword_count, Address::times_8, -8), rax);
2127       __ decrement(qword_count);
2128       __ jcc(Assembler::notZero, L_copy_8_bytes);
2129     }
2130     if (is_oop) {
2131       __ jmp(L_exit);
2132     }
2133     restore_arg_regs_using_thread();
2134     inc_counter_np(SharedRuntime::_jint_array_copy_ctr); // Update counter after rscratch1 is free
2135     __ xorptr(rax, rax); // return 0
2136     __ vzeroupper();
2137     __ leave(); // required for proper stackwalking of RuntimeStub frame
2138     __ ret(0);
2139 
2140     {
2141       // UnsafeCopyMemory page error: continue after ucm
2142       UnsafeCopyMemoryMark ucmm(this, !is_oop &amp;&amp; !aligned, true);
2143       // Copy in multi-bytes chunks
2144       copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);
2145     }
2146 
2147   __ BIND(L_exit);
2148     bs-&gt;arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);
2149     restore_arg_regs_using_thread();
2150     inc_counter_np(SharedRuntime::_jint_array_copy_ctr); // Update counter after rscratch1 is free
2151     __ xorptr(rax, rax); // return 0
2152     __ vzeroupper();
2153     __ leave(); // required for proper stackwalking of RuntimeStub frame
2154     __ ret(0);
2155 
2156     return start;
2157   }
2158 
2159   // Arguments:
2160   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
2161   //             ignored
2162   //   is_oop  - true =&gt; oop array, so generate store check code
2163   //   name    - stub name string
2164   //
2165   // Inputs:
2166   //   c_rarg0   - source array address
2167   //   c_rarg1   - destination array address
2168   //   c_rarg2   - element count, treated as ssize_t, can be zero
2169   //
2170  // Side Effects:
2171   //   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the
2172   //   no-overlap entry point used by generate_conjoint_long_oop_copy().
2173   //
2174   address generate_disjoint_long_oop_copy(bool aligned, bool is_oop, address *entry,
2175                                           const char *name, bool dest_uninitialized = false) {
2176     __ align(CodeEntryAlignment);
2177     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2178     address start = __ pc();
2179 
2180     Label L_copy_bytes, L_copy_8_bytes, L_exit;
2181     const Register from        = rdi;  // source array address
2182     const Register to          = rsi;  // destination array address
2183     const Register qword_count = rdx;  // elements count
2184     const Register end_from    = from; // source array end address
2185     const Register end_to      = rcx;  // destination array end address
2186     const Register saved_count = r11;
2187     // End pointers are inclusive, and if count is not zero they point
2188     // to the last unit copied:  end_to[0] := end_from[0]
2189 
2190     __ enter(); // required for proper stackwalking of RuntimeStub frame
2191     // Save no-overlap entry point for generate_conjoint_long_oop_copy()
2192     assert_clean_int(c_rarg2, rax);    // Make sure &#39;count&#39; is clean int.
2193 
2194     if (entry != NULL) {
2195       *entry = __ pc();
2196       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
2197       BLOCK_COMMENT(&quot;Entry:&quot;);
2198     }
2199 
2200     setup_arg_regs_using_thread(); // from =&gt; rdi, to =&gt; rsi, count =&gt; rdx
2201                                      // r9 is used to save r15_thread
2202     // &#39;from&#39;, &#39;to&#39; and &#39;qword_count&#39; are now valid
2203 
2204     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;
2205     if (dest_uninitialized) {
2206       decorators |= IS_DEST_UNINITIALIZED;
2207     }
2208     if (aligned) {
2209       decorators |= ARRAYCOPY_ALIGNED;
2210     }
2211 
2212     BasicType type = is_oop ? T_OBJECT : T_LONG;
2213     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
2214     bs-&gt;arraycopy_prologue(_masm, decorators, type, from, to, qword_count);
2215     {
2216       // UnsafeCopyMemory page error: continue after ucm
2217       UnsafeCopyMemoryMark ucmm(this, !is_oop &amp;&amp; !aligned, true);
2218 
2219       // Copy from low to high addresses.  Use &#39;to&#39; as scratch.
2220       __ lea(end_from, Address(from, qword_count, Address::times_8, -8));
2221       __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));
2222       __ negptr(qword_count);
2223       __ jmp(L_copy_bytes);
2224 
2225       // Copy trailing qwords
2226     __ BIND(L_copy_8_bytes);
2227       __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));
2228       __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);
2229       __ increment(qword_count);
2230       __ jcc(Assembler::notZero, L_copy_8_bytes);
2231     }
2232     if (is_oop) {
2233       __ jmp(L_exit);
2234     } else {
2235       restore_arg_regs_using_thread();
2236       inc_counter_np(SharedRuntime::_jlong_array_copy_ctr); // Update counter after rscratch1 is free
2237       __ xorptr(rax, rax); // return 0
2238       __ vzeroupper();
2239       __ leave(); // required for proper stackwalking of RuntimeStub frame
2240       __ ret(0);
2241     }
2242 
2243     {
2244       // UnsafeCopyMemory page error: continue after ucm
2245       UnsafeCopyMemoryMark ucmm(this, !is_oop &amp;&amp; !aligned, true);
2246       // Copy in multi-bytes chunks
2247       copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);
2248     }
2249 
2250     __ BIND(L_exit);
2251     bs-&gt;arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);
2252     restore_arg_regs_using_thread();
2253     if (is_oop) {
2254       inc_counter_np(SharedRuntime::_oop_array_copy_ctr); // Update counter after rscratch1 is free
2255     } else {
2256       inc_counter_np(SharedRuntime::_jlong_array_copy_ctr); // Update counter after rscratch1 is free
2257     }
2258     __ vzeroupper();
2259     __ xorptr(rax, rax); // return 0
2260     __ leave(); // required for proper stackwalking of RuntimeStub frame
2261     __ ret(0);
2262 
2263     return start;
2264   }
2265 
2266   // Arguments:
2267   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
2268   //             ignored
2269   //   is_oop  - true =&gt; oop array, so generate store check code
2270   //   name    - stub name string
2271   //
2272   // Inputs:
2273   //   c_rarg0   - source array address
2274   //   c_rarg1   - destination array address
2275   //   c_rarg2   - element count, treated as ssize_t, can be zero
2276   //
2277   address generate_conjoint_long_oop_copy(bool aligned, bool is_oop,
2278                                           address nooverlap_target, address *entry,
2279                                           const char *name, bool dest_uninitialized = false) {
2280     __ align(CodeEntryAlignment);
2281     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2282     address start = __ pc();
2283 
2284     Label L_copy_bytes, L_copy_8_bytes, L_exit;
2285     const Register from        = rdi;  // source array address
2286     const Register to          = rsi;  // destination array address
2287     const Register qword_count = rdx;  // elements count
2288     const Register saved_count = rcx;
2289 
2290     __ enter(); // required for proper stackwalking of RuntimeStub frame
2291     assert_clean_int(c_rarg2, rax);    // Make sure &#39;count&#39; is clean int.
2292 
2293     if (entry != NULL) {
2294       *entry = __ pc();
2295       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
2296       BLOCK_COMMENT(&quot;Entry:&quot;);
2297     }
2298 
2299     array_overlap_test(nooverlap_target, Address::times_8);
2300     setup_arg_regs_using_thread(); // from =&gt; rdi, to =&gt; rsi, count =&gt; rdx
2301                                    // r9 is used to save r15_thread
2302     // &#39;from&#39;, &#39;to&#39; and &#39;qword_count&#39; are now valid
2303 
2304     DecoratorSet decorators = IN_HEAP | IS_ARRAY;
2305     if (dest_uninitialized) {
2306       decorators |= IS_DEST_UNINITIALIZED;
2307     }
2308     if (aligned) {
2309       decorators |= ARRAYCOPY_ALIGNED;
2310     }
2311 
2312     BasicType type = is_oop ? T_OBJECT : T_LONG;
2313     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
2314     bs-&gt;arraycopy_prologue(_masm, decorators, type, from, to, qword_count);
2315     {
2316       // UnsafeCopyMemory page error: continue after ucm
2317       UnsafeCopyMemoryMark ucmm(this, !is_oop &amp;&amp; !aligned, true);
2318 
2319       __ jmp(L_copy_bytes);
2320 
2321       // Copy trailing qwords
2322     __ BIND(L_copy_8_bytes);
2323       __ movq(rax, Address(from, qword_count, Address::times_8, -8));
2324       __ movq(Address(to, qword_count, Address::times_8, -8), rax);
2325       __ decrement(qword_count);
2326       __ jcc(Assembler::notZero, L_copy_8_bytes);
2327     }
2328     if (is_oop) {
2329       __ jmp(L_exit);
2330     } else {
2331       restore_arg_regs_using_thread();
2332       inc_counter_np(SharedRuntime::_jlong_array_copy_ctr); // Update counter after rscratch1 is free
2333       __ xorptr(rax, rax); // return 0
2334       __ vzeroupper();
2335       __ leave(); // required for proper stackwalking of RuntimeStub frame
2336       __ ret(0);
2337     }
2338     {
2339       // UnsafeCopyMemory page error: continue after ucm
2340       UnsafeCopyMemoryMark ucmm(this, !is_oop &amp;&amp; !aligned, true);
2341 
2342       // Copy in multi-bytes chunks
2343       copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);
2344     }
2345     __ BIND(L_exit);
2346     bs-&gt;arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);
2347     restore_arg_regs_using_thread();
2348     if (is_oop) {
2349       inc_counter_np(SharedRuntime::_oop_array_copy_ctr); // Update counter after rscratch1 is free
2350     } else {
2351       inc_counter_np(SharedRuntime::_jlong_array_copy_ctr); // Update counter after rscratch1 is free
2352     }
2353     __ vzeroupper();
2354     __ xorptr(rax, rax); // return 0
2355     __ leave(); // required for proper stackwalking of RuntimeStub frame
2356     __ ret(0);
2357 
2358     return start;
2359   }
2360 
2361 
2362   // Helper for generating a dynamic type check.
2363   // Smashes no registers.
2364   void generate_type_check(Register sub_klass,
2365                            Register super_check_offset,
2366                            Register super_klass,
2367                            Label&amp; L_success) {
2368     assert_different_registers(sub_klass, super_check_offset, super_klass);
2369 
2370     BLOCK_COMMENT(&quot;type_check:&quot;);
2371 
2372     Label L_miss;
2373 
2374     __ check_klass_subtype_fast_path(sub_klass, super_klass, noreg,        &amp;L_success, &amp;L_miss, NULL,
2375                                      super_check_offset);
2376     __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &amp;L_success, NULL);
2377 
2378     // Fall through on failure!
2379     __ BIND(L_miss);
2380   }
2381 
2382   //
2383   //  Generate checkcasting array copy stub
2384   //
2385   //  Input:
2386   //    c_rarg0   - source array address
2387   //    c_rarg1   - destination array address
2388   //    c_rarg2   - element count, treated as ssize_t, can be zero
2389   //    c_rarg3   - size_t ckoff (super_check_offset)
2390   // not Win64
2391   //    c_rarg4   - oop ckval (super_klass)
2392   // Win64
2393   //    rsp+40    - oop ckval (super_klass)
2394   //
2395   //  Output:
2396   //    rax ==  0  -  success
2397   //    rax == -1^K - failure, where K is partial transfer count
2398   //
2399   address generate_checkcast_copy(const char *name, address *entry,
2400                                   bool dest_uninitialized = false) {
2401 
2402     Label L_load_element, L_store_element, L_do_card_marks, L_done;
2403 
2404     // Input registers (after setup_arg_regs)
2405     const Register from        = rdi;   // source array address
2406     const Register to          = rsi;   // destination array address
2407     const Register length      = rdx;   // elements count
2408     const Register ckoff       = rcx;   // super_check_offset
2409     const Register ckval       = r8;    // super_klass
2410 
2411     // Registers used as temps (r13, r14 are save-on-entry)
2412     const Register end_from    = from;  // source array end address
2413     const Register end_to      = r13;   // destination array end address
2414     const Register count       = rdx;   // -(count_remaining)
2415     const Register r14_length  = r14;   // saved copy of length
2416     // End pointers are inclusive, and if length is not zero they point
2417     // to the last unit copied:  end_to[0] := end_from[0]
2418 
2419     const Register rax_oop    = rax;    // actual oop copied
2420     const Register r11_klass  = r11;    // oop._klass
2421 
2422     //---------------------------------------------------------------
2423     // Assembler stub will be used for this call to arraycopy
2424     // if the two arrays are subtypes of Object[] but the
2425     // destination array type is not equal to or a supertype
2426     // of the source type.  Each element must be separately
2427     // checked.
2428 
2429     __ align(CodeEntryAlignment);
2430     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2431     address start = __ pc();
2432 
2433     __ enter(); // required for proper stackwalking of RuntimeStub frame
2434 
2435 #ifdef ASSERT
2436     // caller guarantees that the arrays really are different
2437     // otherwise, we would have to make conjoint checks
2438     { Label L;
2439       array_overlap_test(L, TIMES_OOP);
2440       __ stop(&quot;checkcast_copy within a single array&quot;);
2441       __ bind(L);
2442     }
2443 #endif //ASSERT
2444 
2445     setup_arg_regs(4); // from =&gt; rdi, to =&gt; rsi, length =&gt; rdx
2446                        // ckoff =&gt; rcx, ckval =&gt; r8
2447                        // r9 and r10 may be used to save non-volatile registers
2448 #ifdef _WIN64
2449     // last argument (#4) is on stack on Win64
2450     __ movptr(ckval, Address(rsp, 6 * wordSize));
2451 #endif
2452 
2453     // Caller of this entry point must set up the argument registers.
2454     if (entry != NULL) {
2455       *entry = __ pc();
2456       BLOCK_COMMENT(&quot;Entry:&quot;);
2457     }
2458 
2459     // allocate spill slots for r13, r14
2460     enum {
2461       saved_r13_offset,
2462       saved_r14_offset,
2463       saved_r10_offset,
2464       saved_rbp_offset
2465     };
2466     __ subptr(rsp, saved_rbp_offset * wordSize);
2467     __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);
2468     __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);
2469     __ movptr(Address(rsp, saved_r10_offset * wordSize), r10);
2470 
2471 #ifdef ASSERT
2472       Label L2;
2473       __ get_thread(r14);
2474       __ cmpptr(r15_thread, r14);
2475       __ jcc(Assembler::equal, L2);
2476       __ stop(&quot;StubRoutines::call_stub: r15_thread is modified by call&quot;);
2477       __ bind(L2);
2478 #endif // ASSERT
2479 
2480     // check that int operands are properly extended to size_t
2481     assert_clean_int(length, rax);
2482     assert_clean_int(ckoff, rax);
2483 
2484 #ifdef ASSERT
2485     BLOCK_COMMENT(&quot;assert consistent ckoff/ckval&quot;);
2486     // The ckoff and ckval must be mutually consistent,
2487     // even though caller generates both.
2488     { Label L;
2489       int sco_offset = in_bytes(Klass::super_check_offset_offset());
2490       __ cmpl(ckoff, Address(ckval, sco_offset));
2491       __ jcc(Assembler::equal, L);
2492       __ stop(&quot;super_check_offset inconsistent&quot;);
2493       __ bind(L);
2494     }
2495 #endif //ASSERT
2496 
2497     // Loop-invariant addresses.  They are exclusive end pointers.
2498     Address end_from_addr(from, length, TIMES_OOP, 0);
2499     Address   end_to_addr(to,   length, TIMES_OOP, 0);
2500     // Loop-variant addresses.  They assume post-incremented count &lt; 0.
2501     Address from_element_addr(end_from, count, TIMES_OOP, 0);
2502     Address   to_element_addr(end_to,   count, TIMES_OOP, 0);
2503 
2504     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT;
2505     if (dest_uninitialized) {
2506       decorators |= IS_DEST_UNINITIALIZED;
2507     }
2508 
2509     BasicType type = T_OBJECT;
2510     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
2511     bs-&gt;arraycopy_prologue(_masm, decorators, type, from, to, count);
2512 
2513     // Copy from low to high addresses, indexed from the end of each array.
2514     __ lea(end_from, end_from_addr);
2515     __ lea(end_to,   end_to_addr);
2516     __ movptr(r14_length, length);        // save a copy of the length
2517     assert(length == count, &quot;&quot;);          // else fix next line:
2518     __ negptr(count);                     // negate and test the length
2519     __ jcc(Assembler::notZero, L_load_element);
2520 
2521     // Empty array:  Nothing to do.
2522     __ xorptr(rax, rax);                  // return 0 on (trivial) success
2523     __ jmp(L_done);
2524 
2525     // ======== begin loop ========
2526     // (Loop is rotated; its entry is L_load_element.)
2527     // Loop control:
2528     //   for (count = -count; count != 0; count++)
2529     // Base pointers src, dst are biased by 8*(count-1),to last element.
2530     __ align(OptoLoopAlignment);
2531 
2532     __ BIND(L_store_element);
2533     __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, noreg, AS_RAW);  // store the oop
2534     __ increment(count);               // increment the count toward zero
2535     __ jcc(Assembler::zero, L_do_card_marks);
2536 
2537     // ======== loop entry is here ========
2538     __ BIND(L_load_element);
2539     __ load_heap_oop(rax_oop, from_element_addr, noreg, noreg, AS_RAW); // load the oop
2540     __ testptr(rax_oop, rax_oop);
2541     __ jcc(Assembler::zero, L_store_element);
2542 
2543     __ load_klass(r11_klass, rax_oop);// query the object klass
2544     generate_type_check(r11_klass, ckoff, ckval, L_store_element);
2545     // ======== end loop ========
2546 
2547     // It was a real error; we must depend on the caller to finish the job.
2548     // Register rdx = -1 * number of *remaining* oops, r14 = *total* oops.
2549     // Emit GC store barriers for the oops we have copied (r14 + rdx),
2550     // and report their number to the caller.
2551     assert_different_registers(rax, r14_length, count, to, end_to, rcx, rscratch1);
2552     Label L_post_barrier;
2553     __ addptr(r14_length, count);     // K = (original - remaining) oops
2554     __ movptr(rax, r14_length);       // save the value
2555     __ notptr(rax);                   // report (-1^K) to caller (does not affect flags)
2556     __ jccb(Assembler::notZero, L_post_barrier);
2557     __ jmp(L_done); // K == 0, nothing was copied, skip post barrier
2558 
2559     // Come here on success only.
2560     __ BIND(L_do_card_marks);
2561     __ xorptr(rax, rax);              // return 0 on success
2562 
2563     __ BIND(L_post_barrier);
2564     bs-&gt;arraycopy_epilogue(_masm, decorators, type, from, to, r14_length);
2565 
2566     // Common exit point (success or failure).
2567     __ BIND(L_done);
2568     __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));
2569     __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));
2570     __ movptr(r10, Address(rsp, saved_r10_offset * wordSize));
2571     restore_arg_regs();
2572     inc_counter_np(SharedRuntime::_checkcast_array_copy_ctr); // Update counter after rscratch1 is free
2573     __ leave(); // required for proper stackwalking of RuntimeStub frame
2574     __ ret(0);
2575 
2576     return start;
2577   }
2578 
2579   //
2580   //  Generate &#39;unsafe&#39; array copy stub
2581   //  Though just as safe as the other stubs, it takes an unscaled
2582   //  size_t argument instead of an element count.
2583   //
2584   //  Input:
2585   //    c_rarg0   - source array address
2586   //    c_rarg1   - destination array address
2587   //    c_rarg2   - byte count, treated as ssize_t, can be zero
2588   //
2589   // Examines the alignment of the operands and dispatches
2590   // to a long, int, short, or byte copy loop.
2591   //
2592   address generate_unsafe_copy(const char *name,
2593                                address byte_copy_entry, address short_copy_entry,
2594                                address int_copy_entry, address long_copy_entry) {
2595 
2596     Label L_long_aligned, L_int_aligned, L_short_aligned;
2597 
2598     // Input registers (before setup_arg_regs)
2599     const Register from        = c_rarg0;  // source array address
2600     const Register to          = c_rarg1;  // destination array address
2601     const Register size        = c_rarg2;  // byte count (size_t)
2602 
2603     // Register used as a temp
2604     const Register bits        = rax;      // test copy of low bits
2605 
2606     __ align(CodeEntryAlignment);
2607     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2608     address start = __ pc();
2609 
2610     __ enter(); // required for proper stackwalking of RuntimeStub frame
2611 
2612     // bump this on entry, not on exit:
2613     inc_counter_np(SharedRuntime::_unsafe_array_copy_ctr);
2614 
2615     __ mov(bits, from);
2616     __ orptr(bits, to);
2617     __ orptr(bits, size);
2618 
2619     __ testb(bits, BytesPerLong-1);
2620     __ jccb(Assembler::zero, L_long_aligned);
2621 
2622     __ testb(bits, BytesPerInt-1);
2623     __ jccb(Assembler::zero, L_int_aligned);
2624 
2625     __ testb(bits, BytesPerShort-1);
2626     __ jump_cc(Assembler::notZero, RuntimeAddress(byte_copy_entry));
2627 
2628     __ BIND(L_short_aligned);
2629     __ shrptr(size, LogBytesPerShort); // size =&gt; short_count
2630     __ jump(RuntimeAddress(short_copy_entry));
2631 
2632     __ BIND(L_int_aligned);
2633     __ shrptr(size, LogBytesPerInt); // size =&gt; int_count
2634     __ jump(RuntimeAddress(int_copy_entry));
2635 
2636     __ BIND(L_long_aligned);
2637     __ shrptr(size, LogBytesPerLong); // size =&gt; qword_count
2638     __ jump(RuntimeAddress(long_copy_entry));
2639 
2640     return start;
2641   }
2642 
2643   // Perform range checks on the proposed arraycopy.
2644   // Kills temp, but nothing else.
2645   // Also, clean the sign bits of src_pos and dst_pos.
2646   void arraycopy_range_checks(Register src,     // source array oop (c_rarg0)
2647                               Register src_pos, // source position (c_rarg1)
2648                               Register dst,     // destination array oo (c_rarg2)
2649                               Register dst_pos, // destination position (c_rarg3)
2650                               Register length,
2651                               Register temp,
2652                               Label&amp; L_failed) {
2653     BLOCK_COMMENT(&quot;arraycopy_range_checks:&quot;);
2654 
2655     //  if (src_pos + length &gt; arrayOop(src)-&gt;length())  FAIL;
2656     __ movl(temp, length);
2657     __ addl(temp, src_pos);             // src_pos + length
2658     __ cmpl(temp, Address(src, arrayOopDesc::length_offset_in_bytes()));
2659     __ jcc(Assembler::above, L_failed);
2660 
2661     //  if (dst_pos + length &gt; arrayOop(dst)-&gt;length())  FAIL;
2662     __ movl(temp, length);
2663     __ addl(temp, dst_pos);             // dst_pos + length
2664     __ cmpl(temp, Address(dst, arrayOopDesc::length_offset_in_bytes()));
2665     __ jcc(Assembler::above, L_failed);
2666 
2667     // Have to clean up high 32-bits of &#39;src_pos&#39; and &#39;dst_pos&#39;.
2668     // Move with sign extension can be used since they are positive.
2669     __ movslq(src_pos, src_pos);
2670     __ movslq(dst_pos, dst_pos);
2671 
2672     BLOCK_COMMENT(&quot;arraycopy_range_checks done&quot;);
2673   }
2674 
2675   //
2676   //  Generate generic array copy stubs
2677   //
2678   //  Input:
2679   //    c_rarg0    -  src oop
2680   //    c_rarg1    -  src_pos (32-bits)
2681   //    c_rarg2    -  dst oop
2682   //    c_rarg3    -  dst_pos (32-bits)
2683   // not Win64
2684   //    c_rarg4    -  element count (32-bits)
2685   // Win64
2686   //    rsp+40     -  element count (32-bits)
2687   //
2688   //  Output:
2689   //    rax ==  0  -  success
2690   //    rax == -1^K - failure, where K is partial transfer count
2691   //
2692   address generate_generic_copy(const char *name,
2693                                 address byte_copy_entry, address short_copy_entry,
2694                                 address int_copy_entry, address oop_copy_entry,
2695                                 address long_copy_entry, address checkcast_copy_entry) {
2696 
2697     Label L_failed, L_failed_0, L_objArray;
2698     Label L_copy_bytes, L_copy_shorts, L_copy_ints, L_copy_longs;
2699 
2700     // Input registers
2701     const Register src        = c_rarg0;  // source array oop
2702     const Register src_pos    = c_rarg1;  // source position
2703     const Register dst        = c_rarg2;  // destination array oop
2704     const Register dst_pos    = c_rarg3;  // destination position
2705 #ifndef _WIN64
2706     const Register length     = c_rarg4;
2707 #else
2708     const Address  length(rsp, 6 * wordSize);  // elements count is on stack on Win64
2709 #endif
2710 
2711     { int modulus = CodeEntryAlignment;
2712       int target  = modulus - 5; // 5 = sizeof jmp(L_failed)
2713       int advance = target - (__ offset() % modulus);
2714       if (advance &lt; 0)  advance += modulus;
2715       if (advance &gt; 0)  __ nop(advance);
2716     }
2717     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2718 
2719     // Short-hop target to L_failed.  Makes for denser prologue code.
2720     __ BIND(L_failed_0);
2721     __ jmp(L_failed);
2722     assert(__ offset() % CodeEntryAlignment == 0, &quot;no further alignment needed&quot;);
2723 
2724     __ align(CodeEntryAlignment);
2725     address start = __ pc();
2726 
2727     __ enter(); // required for proper stackwalking of RuntimeStub frame
2728 
2729     // bump this on entry, not on exit:
2730     inc_counter_np(SharedRuntime::_generic_array_copy_ctr);
2731 
2732     //-----------------------------------------------------------------------
2733     // Assembler stub will be used for this call to arraycopy
2734     // if the following conditions are met:
2735     //
2736     // (1) src and dst must not be null.
2737     // (2) src_pos must not be negative.
2738     // (3) dst_pos must not be negative.
2739     // (4) length  must not be negative.
2740     // (5) src klass and dst klass should be the same and not NULL.
2741     // (6) src and dst should be arrays.
2742     // (7) src_pos + length must not exceed length of src.
2743     // (8) dst_pos + length must not exceed length of dst.
2744     //
2745 
2746     //  if (src == NULL) return -1;
2747     __ testptr(src, src);         // src oop
2748     size_t j1off = __ offset();
2749     __ jccb(Assembler::zero, L_failed_0);
2750 
2751     //  if (src_pos &lt; 0) return -1;
2752     __ testl(src_pos, src_pos); // src_pos (32-bits)
2753     __ jccb(Assembler::negative, L_failed_0);
2754 
2755     //  if (dst == NULL) return -1;
2756     __ testptr(dst, dst);         // dst oop
2757     __ jccb(Assembler::zero, L_failed_0);
2758 
2759     //  if (dst_pos &lt; 0) return -1;
2760     __ testl(dst_pos, dst_pos); // dst_pos (32-bits)
2761     size_t j4off = __ offset();
2762     __ jccb(Assembler::negative, L_failed_0);
2763 
2764     // The first four tests are very dense code,
2765     // but not quite dense enough to put four
2766     // jumps in a 16-byte instruction fetch buffer.
2767     // That&#39;s good, because some branch predicters
2768     // do not like jumps so close together.
2769     // Make sure of this.
2770     guarantee(((j1off ^ j4off) &amp; ~15) != 0, &quot;I$ line of 1st &amp; 4th jumps&quot;);
2771 
2772     // registers used as temp
2773     const Register r11_length    = r11; // elements count to copy
2774     const Register r10_src_klass = r10; // array klass
2775 
2776     //  if (length &lt; 0) return -1;
2777     __ movl(r11_length, length);        // length (elements count, 32-bits value)
2778     __ testl(r11_length, r11_length);
2779     __ jccb(Assembler::negative, L_failed_0);
2780 
2781     __ load_klass(r10_src_klass, src);
2782 #ifdef ASSERT
2783     //  assert(src-&gt;klass() != NULL);
2784     {
2785       BLOCK_COMMENT(&quot;assert klasses not null {&quot;);
2786       Label L1, L2;
2787       __ testptr(r10_src_klass, r10_src_klass);
2788       __ jcc(Assembler::notZero, L2);   // it is broken if klass is NULL
2789       __ bind(L1);
2790       __ stop(&quot;broken null klass&quot;);
2791       __ bind(L2);
2792       __ load_klass(rax, dst);
2793       __ cmpq(rax, 0);
2794       __ jcc(Assembler::equal, L1);     // this would be broken also
2795       BLOCK_COMMENT(&quot;} assert klasses not null done&quot;);
2796     }
2797 #endif
2798 
2799     // Load layout helper (32-bits)
2800     //
2801     //  |array_tag|     | header_size | element_type |     |log2_element_size|
2802     // 32        30    24            16              8     2                 0
2803     //
2804     //   array_tag: typeArray = 0x3, objArray = 0x2, non-array = 0x0
2805     //
2806 
2807     const int lh_offset = in_bytes(Klass::layout_helper_offset());
2808 
2809     // Handle objArrays completely differently...
2810     const jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
2811     __ cmpl(Address(r10_src_klass, lh_offset), objArray_lh);
2812     __ jcc(Assembler::equal, L_objArray);
2813 
2814     //  if (src-&gt;klass() != dst-&gt;klass()) return -1;
2815     __ load_klass(rax, dst);
2816     __ cmpq(r10_src_klass, rax);
2817     __ jcc(Assembler::notEqual, L_failed);
2818 
2819     const Register rax_lh = rax;  // layout helper
2820     __ movl(rax_lh, Address(r10_src_klass, lh_offset));
2821 
2822     //  if (!src-&gt;is_Array()) return -1;
2823     __ cmpl(rax_lh, Klass::_lh_neutral_value);
2824     __ jcc(Assembler::greaterEqual, L_failed);
2825 
2826     // At this point, it is known to be a typeArray (array_tag 0x3).
2827 #ifdef ASSERT
2828     {
2829       BLOCK_COMMENT(&quot;assert primitive array {&quot;);
2830       Label L;
2831       __ cmpl(rax_lh, (Klass::_lh_array_tag_type_value &lt;&lt; Klass::_lh_array_tag_shift));
2832       __ jcc(Assembler::greaterEqual, L);
2833       __ stop(&quot;must be a primitive array&quot;);
2834       __ bind(L);
2835       BLOCK_COMMENT(&quot;} assert primitive array done&quot;);
2836     }
2837 #endif
2838 
2839     arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,
2840                            r10, L_failed);
2841 
2842     // TypeArrayKlass
2843     //
2844     // src_addr = (src + array_header_in_bytes()) + (src_pos &lt;&lt; log2elemsize);
2845     // dst_addr = (dst + array_header_in_bytes()) + (dst_pos &lt;&lt; log2elemsize);
2846     //
2847 
2848     const Register r10_offset = r10;    // array offset
2849     const Register rax_elsize = rax_lh; // element size
2850 
2851     __ movl(r10_offset, rax_lh);
2852     __ shrl(r10_offset, Klass::_lh_header_size_shift);
2853     __ andptr(r10_offset, Klass::_lh_header_size_mask);   // array_offset
2854     __ addptr(src, r10_offset);           // src array offset
2855     __ addptr(dst, r10_offset);           // dst array offset
2856     BLOCK_COMMENT(&quot;choose copy loop based on element size&quot;);
2857     __ andl(rax_lh, Klass::_lh_log2_element_size_mask); // rax_lh -&gt; rax_elsize
2858 
2859     // next registers should be set before the jump to corresponding stub
2860     const Register from     = c_rarg0;  // source array address
2861     const Register to       = c_rarg1;  // destination array address
2862     const Register count    = c_rarg2;  // elements count
2863 
2864     // &#39;from&#39;, &#39;to&#39;, &#39;count&#39; registers should be set in such order
2865     // since they are the same as &#39;src&#39;, &#39;src_pos&#39;, &#39;dst&#39;.
2866 
2867   __ BIND(L_copy_bytes);
2868     __ cmpl(rax_elsize, 0);
2869     __ jccb(Assembler::notEqual, L_copy_shorts);
2870     __ lea(from, Address(src, src_pos, Address::times_1, 0));// src_addr
2871     __ lea(to,   Address(dst, dst_pos, Address::times_1, 0));// dst_addr
2872     __ movl2ptr(count, r11_length); // length
2873     __ jump(RuntimeAddress(byte_copy_entry));
2874 
2875   __ BIND(L_copy_shorts);
2876     __ cmpl(rax_elsize, LogBytesPerShort);
2877     __ jccb(Assembler::notEqual, L_copy_ints);
2878     __ lea(from, Address(src, src_pos, Address::times_2, 0));// src_addr
2879     __ lea(to,   Address(dst, dst_pos, Address::times_2, 0));// dst_addr
2880     __ movl2ptr(count, r11_length); // length
2881     __ jump(RuntimeAddress(short_copy_entry));
2882 
2883   __ BIND(L_copy_ints);
2884     __ cmpl(rax_elsize, LogBytesPerInt);
2885     __ jccb(Assembler::notEqual, L_copy_longs);
2886     __ lea(from, Address(src, src_pos, Address::times_4, 0));// src_addr
2887     __ lea(to,   Address(dst, dst_pos, Address::times_4, 0));// dst_addr
2888     __ movl2ptr(count, r11_length); // length
2889     __ jump(RuntimeAddress(int_copy_entry));
2890 
2891   __ BIND(L_copy_longs);
2892 #ifdef ASSERT
2893     {
2894       BLOCK_COMMENT(&quot;assert long copy {&quot;);
2895       Label L;
2896       __ cmpl(rax_elsize, LogBytesPerLong);
2897       __ jcc(Assembler::equal, L);
2898       __ stop(&quot;must be long copy, but elsize is wrong&quot;);
2899       __ bind(L);
2900       BLOCK_COMMENT(&quot;} assert long copy done&quot;);
2901     }
2902 #endif
2903     __ lea(from, Address(src, src_pos, Address::times_8, 0));// src_addr
2904     __ lea(to,   Address(dst, dst_pos, Address::times_8, 0));// dst_addr
2905     __ movl2ptr(count, r11_length); // length
2906     __ jump(RuntimeAddress(long_copy_entry));
2907 
2908     // ObjArrayKlass
2909   __ BIND(L_objArray);
2910     // live at this point:  r10_src_klass, r11_length, src[_pos], dst[_pos]
2911 
2912     Label L_plain_copy, L_checkcast_copy;
2913     //  test array classes for subtyping
2914     __ load_klass(rax, dst);
2915     __ cmpq(r10_src_klass, rax); // usual case is exact equality
2916     __ jcc(Assembler::notEqual, L_checkcast_copy);
2917 
2918     // Identically typed arrays can be copied without element-wise checks.
2919     arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,
2920                            r10, L_failed);
2921 
2922     __ lea(from, Address(src, src_pos, TIMES_OOP,
2923                  arrayOopDesc::base_offset_in_bytes(T_OBJECT))); // src_addr
2924     __ lea(to,   Address(dst, dst_pos, TIMES_OOP,
2925                  arrayOopDesc::base_offset_in_bytes(T_OBJECT))); // dst_addr
2926     __ movl2ptr(count, r11_length); // length
2927   __ BIND(L_plain_copy);
2928     __ jump(RuntimeAddress(oop_copy_entry));
2929 
2930   __ BIND(L_checkcast_copy);
2931     // live at this point:  r10_src_klass, r11_length, rax (dst_klass)
2932     {
2933       // Before looking at dst.length, make sure dst is also an objArray.
2934       __ cmpl(Address(rax, lh_offset), objArray_lh);
2935       __ jcc(Assembler::notEqual, L_failed);
2936 
2937       // It is safe to examine both src.length and dst.length.
2938       arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,
2939                              rax, L_failed);
2940 
2941       const Register r11_dst_klass = r11;
2942       __ load_klass(r11_dst_klass, dst); // reload
2943 
2944       // Marshal the base address arguments now, freeing registers.
2945       __ lea(from, Address(src, src_pos, TIMES_OOP,
2946                    arrayOopDesc::base_offset_in_bytes(T_OBJECT)));
2947       __ lea(to,   Address(dst, dst_pos, TIMES_OOP,
2948                    arrayOopDesc::base_offset_in_bytes(T_OBJECT)));
2949       __ movl(count, length);           // length (reloaded)
2950       Register sco_temp = c_rarg3;      // this register is free now
2951       assert_different_registers(from, to, count, sco_temp,
2952                                  r11_dst_klass, r10_src_klass);
2953       assert_clean_int(count, sco_temp);
2954 
2955       // Generate the type check.
2956       const int sco_offset = in_bytes(Klass::super_check_offset_offset());
2957       __ movl(sco_temp, Address(r11_dst_klass, sco_offset));
2958       assert_clean_int(sco_temp, rax);
2959       generate_type_check(r10_src_klass, sco_temp, r11_dst_klass, L_plain_copy);
2960 
2961       // Fetch destination element klass from the ObjArrayKlass header.
2962       int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());
2963       __ movptr(r11_dst_klass, Address(r11_dst_klass, ek_offset));
2964       __ movl(  sco_temp,      Address(r11_dst_klass, sco_offset));
2965       assert_clean_int(sco_temp, rax);
2966 
2967       // the checkcast_copy loop needs two extra arguments:
2968       assert(c_rarg3 == sco_temp, &quot;#3 already in place&quot;);
2969       // Set up arguments for checkcast_copy_entry.
2970       setup_arg_regs(4);
2971       __ movptr(r8, r11_dst_klass);  // dst.klass.element_klass, r8 is c_rarg4 on Linux/Solaris
2972       __ jump(RuntimeAddress(checkcast_copy_entry));
2973     }
2974 
2975   __ BIND(L_failed);
2976     __ xorptr(rax, rax);
2977     __ notptr(rax); // return -1
2978     __ leave();   // required for proper stackwalking of RuntimeStub frame
2979     __ ret(0);
2980 
2981     return start;
2982   }
2983 
2984   address generate_data_cache_writeback() {
2985     const Register src        = c_rarg0;  // source address
2986 
2987     __ align(CodeEntryAlignment);
2988 
2989     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_data_cache_writeback&quot;);
2990 
2991     address start = __ pc();
2992     __ enter();
2993     __ cache_wb(Address(src, 0));
2994     __ leave();
2995     __ ret(0);
2996 
2997     return start;
2998   }
2999 
3000   address generate_data_cache_writeback_sync() {
3001     const Register is_pre    = c_rarg0;  // pre or post sync
3002 
3003     __ align(CodeEntryAlignment);
3004 
3005     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_data_cache_writeback_sync&quot;);
3006 
3007     // pre wbsync is a no-op
3008     // post wbsync translates to an sfence
3009 
3010     Label skip;
3011     address start = __ pc();
3012     __ enter();
3013     __ cmpl(is_pre, 0);
3014     __ jcc(Assembler::notEqual, skip);
3015     __ cache_wbsync(false);
3016     __ bind(skip);
3017     __ leave();
3018     __ ret(0);
3019 
3020     return start;
3021   }
3022 
3023   void generate_arraycopy_stubs() {
3024     address entry;
3025     address entry_jbyte_arraycopy;
3026     address entry_jshort_arraycopy;
3027     address entry_jint_arraycopy;
3028     address entry_oop_arraycopy;
3029     address entry_jlong_arraycopy;
3030     address entry_checkcast_arraycopy;
3031 
3032     StubRoutines::_jbyte_disjoint_arraycopy  = generate_disjoint_byte_copy(false, &amp;entry,
3033                                                                            &quot;jbyte_disjoint_arraycopy&quot;);
3034     StubRoutines::_jbyte_arraycopy           = generate_conjoint_byte_copy(false, entry, &amp;entry_jbyte_arraycopy,
3035                                                                            &quot;jbyte_arraycopy&quot;);
3036 
3037     StubRoutines::_jshort_disjoint_arraycopy = generate_disjoint_short_copy(false, &amp;entry,
3038                                                                             &quot;jshort_disjoint_arraycopy&quot;);
3039     StubRoutines::_jshort_arraycopy          = generate_conjoint_short_copy(false, entry, &amp;entry_jshort_arraycopy,
3040                                                                             &quot;jshort_arraycopy&quot;);
3041 
3042     StubRoutines::_jint_disjoint_arraycopy   = generate_disjoint_int_oop_copy(false, false, &amp;entry,
3043                                                                               &quot;jint_disjoint_arraycopy&quot;);
3044     StubRoutines::_jint_arraycopy            = generate_conjoint_int_oop_copy(false, false, entry,
3045                                                                               &amp;entry_jint_arraycopy, &quot;jint_arraycopy&quot;);
3046 
3047     StubRoutines::_jlong_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, false, &amp;entry,
3048                                                                                &quot;jlong_disjoint_arraycopy&quot;);
3049     StubRoutines::_jlong_arraycopy           = generate_conjoint_long_oop_copy(false, false, entry,
3050                                                                                &amp;entry_jlong_arraycopy, &quot;jlong_arraycopy&quot;);
3051 
3052 
3053     if (UseCompressedOops) {
3054       StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_int_oop_copy(false, true, &amp;entry,
3055                                                                               &quot;oop_disjoint_arraycopy&quot;);
3056       StubRoutines::_oop_arraycopy           = generate_conjoint_int_oop_copy(false, true, entry,
3057                                                                               &amp;entry_oop_arraycopy, &quot;oop_arraycopy&quot;);
3058       StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_int_oop_copy(false, true, &amp;entry,
3059                                                                                      &quot;oop_disjoint_arraycopy_uninit&quot;,
3060                                                                                      /*dest_uninitialized*/true);
3061       StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_int_oop_copy(false, true, entry,
3062                                                                                      NULL, &quot;oop_arraycopy_uninit&quot;,
3063                                                                                      /*dest_uninitialized*/true);
3064     } else {
3065       StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, true, &amp;entry,
3066                                                                                &quot;oop_disjoint_arraycopy&quot;);
3067       StubRoutines::_oop_arraycopy           = generate_conjoint_long_oop_copy(false, true, entry,
3068                                                                                &amp;entry_oop_arraycopy, &quot;oop_arraycopy&quot;);
3069       StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_long_oop_copy(false, true, &amp;entry,
3070                                                                                       &quot;oop_disjoint_arraycopy_uninit&quot;,
3071                                                                                       /*dest_uninitialized*/true);
3072       StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_long_oop_copy(false, true, entry,
3073                                                                                       NULL, &quot;oop_arraycopy_uninit&quot;,
3074                                                                                       /*dest_uninitialized*/true);
3075     }
3076 
3077     StubRoutines::_checkcast_arraycopy        = generate_checkcast_copy(&quot;checkcast_arraycopy&quot;, &amp;entry_checkcast_arraycopy);
3078     StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(&quot;checkcast_arraycopy_uninit&quot;, NULL,
3079                                                                         /*dest_uninitialized*/true);
3080 
3081     StubRoutines::_unsafe_arraycopy    = generate_unsafe_copy(&quot;unsafe_arraycopy&quot;,
3082                                                               entry_jbyte_arraycopy,
3083                                                               entry_jshort_arraycopy,
3084                                                               entry_jint_arraycopy,
3085                                                               entry_jlong_arraycopy);
3086     StubRoutines::_generic_arraycopy   = generate_generic_copy(&quot;generic_arraycopy&quot;,
3087                                                                entry_jbyte_arraycopy,
3088                                                                entry_jshort_arraycopy,
3089                                                                entry_jint_arraycopy,
3090                                                                entry_oop_arraycopy,
3091                                                                entry_jlong_arraycopy,
3092                                                                entry_checkcast_arraycopy);
3093 
3094     StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, &quot;jbyte_fill&quot;);
3095     StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, &quot;jshort_fill&quot;);
3096     StubRoutines::_jint_fill = generate_fill(T_INT, false, &quot;jint_fill&quot;);
3097     StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, &quot;arrayof_jbyte_fill&quot;);
3098     StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, &quot;arrayof_jshort_fill&quot;);
3099     StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, &quot;arrayof_jint_fill&quot;);
3100 
3101     // We don&#39;t generate specialized code for HeapWord-aligned source
3102     // arrays, so just use the code we&#39;ve already generated
3103     StubRoutines::_arrayof_jbyte_disjoint_arraycopy  = StubRoutines::_jbyte_disjoint_arraycopy;
3104     StubRoutines::_arrayof_jbyte_arraycopy           = StubRoutines::_jbyte_arraycopy;
3105 
3106     StubRoutines::_arrayof_jshort_disjoint_arraycopy = StubRoutines::_jshort_disjoint_arraycopy;
3107     StubRoutines::_arrayof_jshort_arraycopy          = StubRoutines::_jshort_arraycopy;
3108 
3109     StubRoutines::_arrayof_jint_disjoint_arraycopy   = StubRoutines::_jint_disjoint_arraycopy;
3110     StubRoutines::_arrayof_jint_arraycopy            = StubRoutines::_jint_arraycopy;
3111 
3112     StubRoutines::_arrayof_jlong_disjoint_arraycopy  = StubRoutines::_jlong_disjoint_arraycopy;
3113     StubRoutines::_arrayof_jlong_arraycopy           = StubRoutines::_jlong_arraycopy;
3114 
3115     StubRoutines::_arrayof_oop_disjoint_arraycopy    = StubRoutines::_oop_disjoint_arraycopy;
3116     StubRoutines::_arrayof_oop_arraycopy             = StubRoutines::_oop_arraycopy;
3117 
3118     StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit    = StubRoutines::_oop_disjoint_arraycopy_uninit;
3119     StubRoutines::_arrayof_oop_arraycopy_uninit             = StubRoutines::_oop_arraycopy_uninit;
3120   }
3121 
3122   // AES intrinsic stubs
3123   enum {AESBlockSize = 16};
3124 
3125   address generate_key_shuffle_mask() {
3126     __ align(16);
3127     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;key_shuffle_mask&quot;);
3128     address start = __ pc();
3129     __ emit_data64( 0x0405060700010203, relocInfo::none );
3130     __ emit_data64( 0x0c0d0e0f08090a0b, relocInfo::none );
3131     return start;
3132   }
3133 
3134   address generate_counter_shuffle_mask() {
3135     __ align(16);
3136     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;counter_shuffle_mask&quot;);
3137     address start = __ pc();
3138     __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);
3139     __ emit_data64(0x0001020304050607, relocInfo::none);
3140     return start;
3141   }
3142 
3143   // Utility routine for loading a 128-bit key word in little endian format
3144   // can optionally specify that the shuffle mask is already in an xmmregister
3145   void load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask=NULL) {
3146     __ movdqu(xmmdst, Address(key, offset));
3147     if (xmm_shuf_mask != NULL) {
3148       __ pshufb(xmmdst, xmm_shuf_mask);
3149     } else {
3150       __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
3151     }
3152   }
3153 
3154   // Utility routine for increase 128bit counter (iv in CTR mode)
3155   void inc_counter(Register reg, XMMRegister xmmdst, int inc_delta, Label&amp; next_block) {
3156     __ pextrq(reg, xmmdst, 0x0);
3157     __ addq(reg, inc_delta);
3158     __ pinsrq(xmmdst, reg, 0x0);
3159     __ jcc(Assembler::carryClear, next_block); // jump if no carry
3160     __ pextrq(reg, xmmdst, 0x01); // Carry
3161     __ addq(reg, 0x01);
3162     __ pinsrq(xmmdst, reg, 0x01); //Carry end
3163     __ BIND(next_block);          // next instruction
3164   }
3165 
3166   // Arguments:
3167   //
3168   // Inputs:
3169   //   c_rarg0   - source byte array address
3170   //   c_rarg1   - destination byte array address
3171   //   c_rarg2   - K (key) in little endian int array
3172   //
3173   address generate_aescrypt_encryptBlock() {
3174     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
3175     __ align(CodeEntryAlignment);
3176     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;aescrypt_encryptBlock&quot;);
3177     Label L_doLast;
3178     address start = __ pc();
3179 
3180     const Register from        = c_rarg0;  // source array address
3181     const Register to          = c_rarg1;  // destination array address
3182     const Register key         = c_rarg2;  // key array address
3183     const Register keylen      = rax;
3184 
3185     const XMMRegister xmm_result = xmm0;
3186     const XMMRegister xmm_key_shuf_mask = xmm1;
3187     // On win64 xmm6-xmm15 must be preserved so don&#39;t use them.
3188     const XMMRegister xmm_temp1  = xmm2;
3189     const XMMRegister xmm_temp2  = xmm3;
3190     const XMMRegister xmm_temp3  = xmm4;
3191     const XMMRegister xmm_temp4  = xmm5;
3192 
3193     __ enter(); // required for proper stackwalking of RuntimeStub frame
3194 
3195     // keylen could be only {11, 13, 15} * 4 = {44, 52, 60}
3196     __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
3197 
3198     __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
3199     __ movdqu(xmm_result, Address(from, 0));  // get 16 bytes of input
3200 
3201     // For encryption, the java expanded key ordering is just what we need
3202     // we don&#39;t know if the key is aligned, hence not using load-execute form
3203 
3204     load_key(xmm_temp1, key, 0x00, xmm_key_shuf_mask);
3205     __ pxor(xmm_result, xmm_temp1);
3206 
3207     load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);
3208     load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);
3209     load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);
3210     load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);
3211 
3212     __ aesenc(xmm_result, xmm_temp1);
3213     __ aesenc(xmm_result, xmm_temp2);
3214     __ aesenc(xmm_result, xmm_temp3);
3215     __ aesenc(xmm_result, xmm_temp4);
3216 
3217     load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);
3218     load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);
3219     load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);
3220     load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);
3221 
3222     __ aesenc(xmm_result, xmm_temp1);
3223     __ aesenc(xmm_result, xmm_temp2);
3224     __ aesenc(xmm_result, xmm_temp3);
3225     __ aesenc(xmm_result, xmm_temp4);
3226 
3227     load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);
3228     load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);
3229 
3230     __ cmpl(keylen, 44);
3231     __ jccb(Assembler::equal, L_doLast);
3232 
3233     __ aesenc(xmm_result, xmm_temp1);
3234     __ aesenc(xmm_result, xmm_temp2);
3235 
3236     load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);
3237     load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);
3238 
3239     __ cmpl(keylen, 52);
3240     __ jccb(Assembler::equal, L_doLast);
3241 
3242     __ aesenc(xmm_result, xmm_temp1);
3243     __ aesenc(xmm_result, xmm_temp2);
3244 
3245     load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);
3246     load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);
3247 
3248     __ BIND(L_doLast);
3249     __ aesenc(xmm_result, xmm_temp1);
3250     __ aesenclast(xmm_result, xmm_temp2);
3251     __ movdqu(Address(to, 0), xmm_result);        // store the result
3252     __ xorptr(rax, rax); // return 0
3253     __ leave(); // required for proper stackwalking of RuntimeStub frame
3254     __ ret(0);
3255 
3256     return start;
3257   }
3258 
3259 
3260   // Arguments:
3261   //
3262   // Inputs:
3263   //   c_rarg0   - source byte array address
3264   //   c_rarg1   - destination byte array address
3265   //   c_rarg2   - K (key) in little endian int array
3266   //
3267   address generate_aescrypt_decryptBlock() {
3268     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
3269     __ align(CodeEntryAlignment);
3270     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;aescrypt_decryptBlock&quot;);
3271     Label L_doLast;
3272     address start = __ pc();
3273 
3274     const Register from        = c_rarg0;  // source array address
3275     const Register to          = c_rarg1;  // destination array address
3276     const Register key         = c_rarg2;  // key array address
3277     const Register keylen      = rax;
3278 
3279     const XMMRegister xmm_result = xmm0;
3280     const XMMRegister xmm_key_shuf_mask = xmm1;
3281     // On win64 xmm6-xmm15 must be preserved so don&#39;t use them.
3282     const XMMRegister xmm_temp1  = xmm2;
3283     const XMMRegister xmm_temp2  = xmm3;
3284     const XMMRegister xmm_temp3  = xmm4;
3285     const XMMRegister xmm_temp4  = xmm5;
3286 
3287     __ enter(); // required for proper stackwalking of RuntimeStub frame
3288 
3289     // keylen could be only {11, 13, 15} * 4 = {44, 52, 60}
3290     __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
3291 
3292     __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
3293     __ movdqu(xmm_result, Address(from, 0));
3294 
3295     // for decryption java expanded key ordering is rotated one position from what we want
3296     // so we start from 0x10 here and hit 0x00 last
3297     // we don&#39;t know if the key is aligned, hence not using load-execute form
3298     load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);
3299     load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);
3300     load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);
3301     load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);
3302 
3303     __ pxor  (xmm_result, xmm_temp1);
3304     __ aesdec(xmm_result, xmm_temp2);
3305     __ aesdec(xmm_result, xmm_temp3);
3306     __ aesdec(xmm_result, xmm_temp4);
3307 
3308     load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);
3309     load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);
3310     load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);
3311     load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);
3312 
3313     __ aesdec(xmm_result, xmm_temp1);
3314     __ aesdec(xmm_result, xmm_temp2);
3315     __ aesdec(xmm_result, xmm_temp3);
3316     __ aesdec(xmm_result, xmm_temp4);
3317 
3318     load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);
3319     load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);
3320     load_key(xmm_temp3, key, 0x00, xmm_key_shuf_mask);
3321 
3322     __ cmpl(keylen, 44);
3323     __ jccb(Assembler::equal, L_doLast);
3324 
3325     __ aesdec(xmm_result, xmm_temp1);
3326     __ aesdec(xmm_result, xmm_temp2);
3327 
3328     load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);
3329     load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);
3330 
3331     __ cmpl(keylen, 52);
3332     __ jccb(Assembler::equal, L_doLast);
3333 
3334     __ aesdec(xmm_result, xmm_temp1);
3335     __ aesdec(xmm_result, xmm_temp2);
3336 
3337     load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);
3338     load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);
3339 
3340     __ BIND(L_doLast);
3341     __ aesdec(xmm_result, xmm_temp1);
3342     __ aesdec(xmm_result, xmm_temp2);
3343 
3344     // for decryption the aesdeclast operation is always on key+0x00
3345     __ aesdeclast(xmm_result, xmm_temp3);
3346     __ movdqu(Address(to, 0), xmm_result);  // store the result
3347     __ xorptr(rax, rax); // return 0
3348     __ leave(); // required for proper stackwalking of RuntimeStub frame
3349     __ ret(0);
3350 
3351     return start;
3352   }
3353 
3354 
3355   // Arguments:
3356   //
3357   // Inputs:
3358   //   c_rarg0   - source byte array address
3359   //   c_rarg1   - destination byte array address
3360   //   c_rarg2   - K (key) in little endian int array
3361   //   c_rarg3   - r vector byte array address
3362   //   c_rarg4   - input length
3363   //
3364   // Output:
3365   //   rax       - input length
3366   //
3367   address generate_cipherBlockChaining_encryptAESCrypt() {
3368     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
3369     __ align(CodeEntryAlignment);
3370     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_encryptAESCrypt&quot;);
3371     address start = __ pc();
3372 
3373     Label L_exit, L_key_192_256, L_key_256, L_loopTop_128, L_loopTop_192, L_loopTop_256;
3374     const Register from        = c_rarg0;  // source array address
3375     const Register to          = c_rarg1;  // destination array address
3376     const Register key         = c_rarg2;  // key array address
3377     const Register rvec        = c_rarg3;  // r byte array initialized from initvector array address
3378                                            // and left with the results of the last encryption block
3379 #ifndef _WIN64
3380     const Register len_reg     = c_rarg4;  // src len (must be multiple of blocksize 16)
3381 #else
3382     const Address  len_mem(rbp, 6 * wordSize);  // length is on stack on Win64
3383     const Register len_reg     = r11;      // pick the volatile windows register
3384 #endif
3385     const Register pos         = rax;
3386 
3387     // xmm register assignments for the loops below
3388     const XMMRegister xmm_result = xmm0;
3389     const XMMRegister xmm_temp   = xmm1;
3390     // keys 0-10 preloaded into xmm2-xmm12
3391     const int XMM_REG_NUM_KEY_FIRST = 2;
3392     const int XMM_REG_NUM_KEY_LAST  = 15;
3393     const XMMRegister xmm_key0   = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);
3394     const XMMRegister xmm_key10  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+10);
3395     const XMMRegister xmm_key11  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+11);
3396     const XMMRegister xmm_key12  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+12);
3397     const XMMRegister xmm_key13  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+13);
3398 
3399     __ enter(); // required for proper stackwalking of RuntimeStub frame
3400 
3401 #ifdef _WIN64
3402     // on win64, fill len_reg from stack position
3403     __ movl(len_reg, len_mem);
3404 #else
3405     __ push(len_reg); // Save
3406 #endif
3407 
3408     const XMMRegister xmm_key_shuf_mask = xmm_temp;  // used temporarily to swap key bytes up front
3409     __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
3410     // load up xmm regs xmm2 thru xmm12 with key 0x00 - 0xa0
3411     for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x00; rnum &lt;= XMM_REG_NUM_KEY_FIRST+10; rnum++) {
3412       load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);
3413       offset += 0x10;
3414     }
3415     __ movdqu(xmm_result, Address(rvec, 0x00));   // initialize xmm_result with r vec
3416 
3417     // now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))
3418     __ movl(rax, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
3419     __ cmpl(rax, 44);
3420     __ jcc(Assembler::notEqual, L_key_192_256);
3421 
3422     // 128 bit code follows here
3423     __ movptr(pos, 0);
3424     __ align(OptoLoopAlignment);
3425 
3426     __ BIND(L_loopTop_128);
3427     __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   // get next 16 bytes of input
3428     __ pxor  (xmm_result, xmm_temp);               // xor with the current r vector
3429     __ pxor  (xmm_result, xmm_key0);               // do the aes rounds
3430     for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum &lt;= XMM_REG_NUM_KEY_FIRST + 9; rnum++) {
3431       __ aesenc(xmm_result, as_XMMRegister(rnum));
3432     }
3433     __ aesenclast(xmm_result, xmm_key10);
3434     __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     // store into the next 16 bytes of output
3435     // no need to store r to memory until we exit
3436     __ addptr(pos, AESBlockSize);
3437     __ subptr(len_reg, AESBlockSize);
3438     __ jcc(Assembler::notEqual, L_loopTop_128);
3439 
3440     __ BIND(L_exit);
3441     __ movdqu(Address(rvec, 0), xmm_result);     // final value of r stored in rvec of CipherBlockChaining object
3442 
3443 #ifdef _WIN64
3444     __ movl(rax, len_mem);
3445 #else
3446     __ pop(rax); // return length
3447 #endif
3448     __ leave(); // required for proper stackwalking of RuntimeStub frame
3449     __ ret(0);
3450 
3451     __ BIND(L_key_192_256);
3452     // here rax = len in ints of AESCrypt.KLE array (52=192, or 60=256)
3453     load_key(xmm_key11, key, 0xb0, xmm_key_shuf_mask);
3454     load_key(xmm_key12, key, 0xc0, xmm_key_shuf_mask);
3455     __ cmpl(rax, 52);
3456     __ jcc(Assembler::notEqual, L_key_256);
3457 
3458     // 192-bit code follows here (could be changed to use more xmm registers)
3459     __ movptr(pos, 0);
3460     __ align(OptoLoopAlignment);
3461 
3462     __ BIND(L_loopTop_192);
3463     __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   // get next 16 bytes of input
3464     __ pxor  (xmm_result, xmm_temp);               // xor with the current r vector
3465     __ pxor  (xmm_result, xmm_key0);               // do the aes rounds
3466     for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  &lt;= XMM_REG_NUM_KEY_FIRST + 11; rnum++) {
3467       __ aesenc(xmm_result, as_XMMRegister(rnum));
3468     }
3469     __ aesenclast(xmm_result, xmm_key12);
3470     __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     // store into the next 16 bytes of output
3471     // no need to store r to memory until we exit
3472     __ addptr(pos, AESBlockSize);
3473     __ subptr(len_reg, AESBlockSize);
3474     __ jcc(Assembler::notEqual, L_loopTop_192);
3475     __ jmp(L_exit);
3476 
3477     __ BIND(L_key_256);
3478     // 256-bit code follows here (could be changed to use more xmm registers)
3479     load_key(xmm_key13, key, 0xd0, xmm_key_shuf_mask);
3480     __ movptr(pos, 0);
3481     __ align(OptoLoopAlignment);
3482 
3483     __ BIND(L_loopTop_256);
3484     __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   // get next 16 bytes of input
3485     __ pxor  (xmm_result, xmm_temp);               // xor with the current r vector
3486     __ pxor  (xmm_result, xmm_key0);               // do the aes rounds
3487     for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  &lt;= XMM_REG_NUM_KEY_FIRST + 13; rnum++) {
3488       __ aesenc(xmm_result, as_XMMRegister(rnum));
3489     }
3490     load_key(xmm_temp, key, 0xe0);
3491     __ aesenclast(xmm_result, xmm_temp);
3492     __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     // store into the next 16 bytes of output
3493     // no need to store r to memory until we exit
3494     __ addptr(pos, AESBlockSize);
3495     __ subptr(len_reg, AESBlockSize);
3496     __ jcc(Assembler::notEqual, L_loopTop_256);
3497     __ jmp(L_exit);
3498 
3499     return start;
3500   }
3501 
3502   // Safefetch stubs.
3503   void generate_safefetch(const char* name, int size, address* entry,
3504                           address* fault_pc, address* continuation_pc) {
3505     // safefetch signatures:
3506     //   int      SafeFetch32(int*      adr, int      errValue);
3507     //   intptr_t SafeFetchN (intptr_t* adr, intptr_t errValue);
3508     //
3509     // arguments:
3510     //   c_rarg0 = adr
3511     //   c_rarg1 = errValue
3512     //
3513     // result:
3514     //   PPC_RET  = *adr or errValue
3515 
3516     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3517 
3518     // Entry point, pc or function descriptor.
3519     *entry = __ pc();
3520 
3521     // Load *adr into c_rarg1, may fault.
3522     *fault_pc = __ pc();
3523     switch (size) {
3524       case 4:
3525         // int32_t
3526         __ movl(c_rarg1, Address(c_rarg0, 0));
3527         break;
3528       case 8:
3529         // int64_t
3530         __ movq(c_rarg1, Address(c_rarg0, 0));
3531         break;
3532       default:
3533         ShouldNotReachHere();
3534     }
3535 
3536     // return errValue or *adr
3537     *continuation_pc = __ pc();
3538     __ movq(rax, c_rarg1);
3539     __ ret(0);
3540   }
3541 
3542   // This is a version of CBC/AES Decrypt which does 4 blocks in a loop at a time
3543   // to hide instruction latency
3544   //
3545   // Arguments:
3546   //
3547   // Inputs:
3548   //   c_rarg0   - source byte array address
3549   //   c_rarg1   - destination byte array address
3550   //   c_rarg2   - K (key) in little endian int array
3551   //   c_rarg3   - r vector byte array address
3552   //   c_rarg4   - input length
3553   //
3554   // Output:
3555   //   rax       - input length
3556   //
3557   address generate_cipherBlockChaining_decryptAESCrypt_Parallel() {
3558     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
3559     __ align(CodeEntryAlignment);
3560     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_decryptAESCrypt&quot;);
3561     address start = __ pc();
3562 
3563     const Register from        = c_rarg0;  // source array address
3564     const Register to          = c_rarg1;  // destination array address
3565     const Register key         = c_rarg2;  // key array address
3566     const Register rvec        = c_rarg3;  // r byte array initialized from initvector array address
3567                                            // and left with the results of the last encryption block
3568 #ifndef _WIN64
3569     const Register len_reg     = c_rarg4;  // src len (must be multiple of blocksize 16)
3570 #else
3571     const Address  len_mem(rbp, 6 * wordSize);  // length is on stack on Win64
3572     const Register len_reg     = r11;      // pick the volatile windows register
3573 #endif
3574     const Register pos         = rax;
3575 
3576     const int PARALLEL_FACTOR = 4;
3577     const int ROUNDS[3] = { 10, 12, 14 }; // aes rounds for key128, key192, key256
3578 
3579     Label L_exit;
3580     Label L_singleBlock_loopTopHead[3]; // 128, 192, 256
3581     Label L_singleBlock_loopTopHead2[3]; // 128, 192, 256
3582     Label L_singleBlock_loopTop[3]; // 128, 192, 256
3583     Label L_multiBlock_loopTopHead[3]; // 128, 192, 256
3584     Label L_multiBlock_loopTop[3]; // 128, 192, 256
3585 
3586     // keys 0-10 preloaded into xmm5-xmm15
3587     const int XMM_REG_NUM_KEY_FIRST = 5;
3588     const int XMM_REG_NUM_KEY_LAST  = 15;
3589     const XMMRegister xmm_key_first = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);
3590     const XMMRegister xmm_key_last  = as_XMMRegister(XMM_REG_NUM_KEY_LAST);
3591 
3592     __ enter(); // required for proper stackwalking of RuntimeStub frame
3593 
3594 #ifdef _WIN64
3595     // on win64, fill len_reg from stack position
3596     __ movl(len_reg, len_mem);
3597 #else
3598     __ push(len_reg); // Save
3599 #endif
3600     __ push(rbx);
3601     // the java expanded key ordering is rotated one position from what we want
3602     // so we start from 0x10 here and hit 0x00 last
3603     const XMMRegister xmm_key_shuf_mask = xmm1;  // used temporarily to swap key bytes up front
3604     __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
3605     // load up xmm regs 5 thru 15 with key 0x10 - 0xa0 - 0x00
3606     for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x10; rnum &lt; XMM_REG_NUM_KEY_LAST; rnum++) {
3607       load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);
3608       offset += 0x10;
3609     }
3610     load_key(xmm_key_last, key, 0x00, xmm_key_shuf_mask);
3611 
3612     const XMMRegister xmm_prev_block_cipher = xmm1;  // holds cipher of previous block
3613 
3614     // registers holding the four results in the parallelized loop
3615     const XMMRegister xmm_result0 = xmm0;
3616     const XMMRegister xmm_result1 = xmm2;
3617     const XMMRegister xmm_result2 = xmm3;
3618     const XMMRegister xmm_result3 = xmm4;
3619 
3620     __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));   // initialize with initial rvec
3621 
3622     __ xorptr(pos, pos);
3623 
3624     // now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))
3625     __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
3626     __ cmpl(rbx, 52);
3627     __ jcc(Assembler::equal, L_multiBlock_loopTopHead[1]);
3628     __ cmpl(rbx, 60);
3629     __ jcc(Assembler::equal, L_multiBlock_loopTopHead[2]);
3630 
3631 #define DoFour(opc, src_reg)           \
3632   __ opc(xmm_result0, src_reg);         \
3633   __ opc(xmm_result1, src_reg);         \
3634   __ opc(xmm_result2, src_reg);         \
3635   __ opc(xmm_result3, src_reg);         \
3636 
3637     for (int k = 0; k &lt; 3; ++k) {
3638       __ BIND(L_multiBlock_loopTopHead[k]);
3639       if (k != 0) {
3640         __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); // see if at least 4 blocks left
3641         __ jcc(Assembler::less, L_singleBlock_loopTopHead2[k]);
3642       }
3643       if (k == 1) {
3644         __ subptr(rsp, 6 * wordSize);
3645         __ movdqu(Address(rsp, 0), xmm15); //save last_key from xmm15
3646         load_key(xmm15, key, 0xb0); // 0xb0; 192-bit key goes up to 0xc0
3647         __ movdqu(Address(rsp, 2 * wordSize), xmm15);
3648         load_key(xmm1, key, 0xc0);  // 0xc0;
3649         __ movdqu(Address(rsp, 4 * wordSize), xmm1);
3650       } else if (k == 2) {
3651         __ subptr(rsp, 10 * wordSize);
3652         __ movdqu(Address(rsp, 0), xmm15); //save last_key from xmm15
3653         load_key(xmm15, key, 0xd0); // 0xd0; 256-bit key goes upto 0xe0
3654         __ movdqu(Address(rsp, 6 * wordSize), xmm15);
3655         load_key(xmm1, key, 0xe0);  // 0xe0;
3656         __ movdqu(Address(rsp, 8 * wordSize), xmm1);
3657         load_key(xmm15, key, 0xb0); // 0xb0;
3658         __ movdqu(Address(rsp, 2 * wordSize), xmm15);
3659         load_key(xmm1, key, 0xc0);  // 0xc0;
3660         __ movdqu(Address(rsp, 4 * wordSize), xmm1);
3661       }
3662       __ align(OptoLoopAlignment);
3663       __ BIND(L_multiBlock_loopTop[k]);
3664       __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); // see if at least 4 blocks left
3665       __ jcc(Assembler::less, L_singleBlock_loopTopHead[k]);
3666 
3667       if  (k != 0) {
3668         __ movdqu(xmm15, Address(rsp, 2 * wordSize));
3669         __ movdqu(xmm1, Address(rsp, 4 * wordSize));
3670       }
3671 
3672       __ movdqu(xmm_result0, Address(from, pos, Address::times_1, 0 * AESBlockSize)); // get next 4 blocks into xmmresult registers
3673       __ movdqu(xmm_result1, Address(from, pos, Address::times_1, 1 * AESBlockSize));
3674       __ movdqu(xmm_result2, Address(from, pos, Address::times_1, 2 * AESBlockSize));
3675       __ movdqu(xmm_result3, Address(from, pos, Address::times_1, 3 * AESBlockSize));
3676 
3677       DoFour(pxor, xmm_key_first);
3678       if (k == 0) {
3679         for (int rnum = 1; rnum &lt; ROUNDS[k]; rnum++) {
3680           DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));
3681         }
3682         DoFour(aesdeclast, xmm_key_last);
3683       } else if (k == 1) {
3684         for (int rnum = 1; rnum &lt;= ROUNDS[k]-2; rnum++) {
3685           DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));
3686         }
3687         __ movdqu(xmm_key_last, Address(rsp, 0)); // xmm15 needs to be loaded again.
3688         DoFour(aesdec, xmm1);  // key : 0xc0
3689         __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  // xmm1 needs to be loaded again
3690         DoFour(aesdeclast, xmm_key_last);
3691       } else if (k == 2) {
3692         for (int rnum = 1; rnum &lt;= ROUNDS[k] - 4; rnum++) {
3693           DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));
3694         }
3695         DoFour(aesdec, xmm1);  // key : 0xc0
3696         __ movdqu(xmm15, Address(rsp, 6 * wordSize));
3697         __ movdqu(xmm1, Address(rsp, 8 * wordSize));
3698         DoFour(aesdec, xmm15);  // key : 0xd0
3699         __ movdqu(xmm_key_last, Address(rsp, 0)); // xmm15 needs to be loaded again.
3700         DoFour(aesdec, xmm1);  // key : 0xe0
3701         __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  // xmm1 needs to be loaded again
3702         DoFour(aesdeclast, xmm_key_last);
3703       }
3704 
3705       // for each result, xor with the r vector of previous cipher block
3706       __ pxor(xmm_result0, xmm_prev_block_cipher);
3707       __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 0 * AESBlockSize));
3708       __ pxor(xmm_result1, xmm_prev_block_cipher);
3709       __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 1 * AESBlockSize));
3710       __ pxor(xmm_result2, xmm_prev_block_cipher);
3711       __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 2 * AESBlockSize));
3712       __ pxor(xmm_result3, xmm_prev_block_cipher);
3713       __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 3 * AESBlockSize));   // this will carry over to next set of blocks
3714       if (k != 0) {
3715         __ movdqu(Address(rvec, 0x00), xmm_prev_block_cipher);
3716       }
3717 
3718       __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);     // store 4 results into the next 64 bytes of output
3719       __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);
3720       __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);
3721       __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);
3722 
3723       __ addptr(pos, PARALLEL_FACTOR * AESBlockSize);
3724       __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize);
3725       __ jmp(L_multiBlock_loopTop[k]);
3726 
3727       // registers used in the non-parallelized loops
3728       // xmm register assignments for the loops below
3729       const XMMRegister xmm_result = xmm0;
3730       const XMMRegister xmm_prev_block_cipher_save = xmm2;
3731       const XMMRegister xmm_key11 = xmm3;
3732       const XMMRegister xmm_key12 = xmm4;
3733       const XMMRegister key_tmp = xmm4;
3734 
3735       __ BIND(L_singleBlock_loopTopHead[k]);
3736       if (k == 1) {
3737         __ addptr(rsp, 6 * wordSize);
3738       } else if (k == 2) {
3739         __ addptr(rsp, 10 * wordSize);
3740       }
3741       __ cmpptr(len_reg, 0); // any blocks left??
3742       __ jcc(Assembler::equal, L_exit);
3743       __ BIND(L_singleBlock_loopTopHead2[k]);
3744       if (k == 1) {
3745         load_key(xmm_key11, key, 0xb0); // 0xb0; 192-bit key goes upto 0xc0
3746         load_key(xmm_key12, key, 0xc0); // 0xc0; 192-bit key goes upto 0xc0
3747       }
3748       if (k == 2) {
3749         load_key(xmm_key11, key, 0xb0); // 0xb0; 256-bit key goes upto 0xe0
3750       }
3751       __ align(OptoLoopAlignment);
3752       __ BIND(L_singleBlock_loopTop[k]);
3753       __ movdqu(xmm_result, Address(from, pos, Address::times_1, 0)); // get next 16 bytes of cipher input
3754       __ movdqa(xmm_prev_block_cipher_save, xmm_result); // save for next r vector
3755       __ pxor(xmm_result, xmm_key_first); // do the aes dec rounds
3756       for (int rnum = 1; rnum &lt;= 9 ; rnum++) {
3757           __ aesdec(xmm_result, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));
3758       }
3759       if (k == 1) {
3760         __ aesdec(xmm_result, xmm_key11);
3761         __ aesdec(xmm_result, xmm_key12);
3762       }
3763       if (k == 2) {
3764         __ aesdec(xmm_result, xmm_key11);
3765         load_key(key_tmp, key, 0xc0);
3766         __ aesdec(xmm_result, key_tmp);
3767         load_key(key_tmp, key, 0xd0);
3768         __ aesdec(xmm_result, key_tmp);
3769         load_key(key_tmp, key, 0xe0);
3770         __ aesdec(xmm_result, key_tmp);
3771       }
3772 
3773       __ aesdeclast(xmm_result, xmm_key_last); // xmm15 always came from key+0
3774       __ pxor(xmm_result, xmm_prev_block_cipher); // xor with the current r vector
3775       __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result); // store into the next 16 bytes of output
3776       // no need to store r to memory until we exit
3777       __ movdqa(xmm_prev_block_cipher, xmm_prev_block_cipher_save); // set up next r vector with cipher input from this block
3778       __ addptr(pos, AESBlockSize);
3779       __ subptr(len_reg, AESBlockSize);
3780       __ jcc(Assembler::notEqual, L_singleBlock_loopTop[k]);
3781       if (k != 2) {
3782         __ jmp(L_exit);
3783       }
3784     } //for 128/192/256
3785 
3786     __ BIND(L_exit);
3787     __ movdqu(Address(rvec, 0), xmm_prev_block_cipher);     // final value of r stored in rvec of CipherBlockChaining object
3788     __ pop(rbx);
3789 #ifdef _WIN64
3790     __ movl(rax, len_mem);
3791 #else
3792     __ pop(rax); // return length
3793 #endif
3794     __ leave(); // required for proper stackwalking of RuntimeStub frame
3795     __ ret(0);
3796     return start;
3797 }
3798 
3799   address generate_electronicCodeBook_encryptAESCrypt() {
3800     __ align(CodeEntryAlignment);
3801     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;electronicCodeBook_encryptAESCrypt&quot;);
3802     address start = __ pc();
3803     const Register from = c_rarg0;  // source array address
3804     const Register to = c_rarg1;  // destination array address
3805     const Register key = c_rarg2;  // key array address
3806     const Register len = c_rarg3;  // src len (must be multiple of blocksize 16)
3807     __ enter(); // required for proper stackwalking of RuntimeStub frame
3808     __ aesecb_encrypt(from, to, key, len);
3809     __ leave(); // required for proper stackwalking of RuntimeStub frame
3810     __ ret(0);
3811     return start;
3812  }
3813 
3814   address generate_electronicCodeBook_decryptAESCrypt() {
3815     __ align(CodeEntryAlignment);
3816     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;electronicCodeBook_decryptAESCrypt&quot;);
3817     address start = __ pc();
3818     const Register from = c_rarg0;  // source array address
3819     const Register to = c_rarg1;  // destination array address
3820     const Register key = c_rarg2;  // key array address
3821     const Register len = c_rarg3;  // src len (must be multiple of blocksize 16)
3822     __ enter(); // required for proper stackwalking of RuntimeStub frame
3823     __ aesecb_decrypt(from, to, key, len);
3824     __ leave(); // required for proper stackwalking of RuntimeStub frame
3825     __ ret(0);
3826     return start;
3827   }
3828 
3829   address generate_upper_word_mask() {
3830     __ align(64);
3831     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;upper_word_mask&quot;);
3832     address start = __ pc();
3833     __ emit_data64(0x0000000000000000, relocInfo::none);
3834     __ emit_data64(0xFFFFFFFF00000000, relocInfo::none);
3835     return start;
3836   }
3837 
3838   address generate_shuffle_byte_flip_mask() {
3839     __ align(64);
3840     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;shuffle_byte_flip_mask&quot;);
3841     address start = __ pc();
3842     __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);
3843     __ emit_data64(0x0001020304050607, relocInfo::none);
3844     return start;
3845   }
3846 
3847   // ofs and limit are use for multi-block byte array.
3848   // int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)
3849   address generate_sha1_implCompress(bool multi_block, const char *name) {
3850     __ align(CodeEntryAlignment);
3851     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3852     address start = __ pc();
3853 
3854     Register buf = c_rarg0;
3855     Register state = c_rarg1;
3856     Register ofs = c_rarg2;
3857     Register limit = c_rarg3;
3858 
3859     const XMMRegister abcd = xmm0;
3860     const XMMRegister e0 = xmm1;
3861     const XMMRegister e1 = xmm2;
3862     const XMMRegister msg0 = xmm3;
3863 
3864     const XMMRegister msg1 = xmm4;
3865     const XMMRegister msg2 = xmm5;
3866     const XMMRegister msg3 = xmm6;
3867     const XMMRegister shuf_mask = xmm7;
3868 
3869     __ enter();
3870 
3871     __ subptr(rsp, 4 * wordSize);
3872 
3873     __ fast_sha1(abcd, e0, e1, msg0, msg1, msg2, msg3, shuf_mask,
3874       buf, state, ofs, limit, rsp, multi_block);
3875 
3876     __ addptr(rsp, 4 * wordSize);
3877 
3878     __ leave();
3879     __ ret(0);
3880     return start;
3881   }
3882 
3883   address generate_pshuffle_byte_flip_mask() {
3884     __ align(64);
3885     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;pshuffle_byte_flip_mask&quot;);
3886     address start = __ pc();
3887     __ emit_data64(0x0405060700010203, relocInfo::none);
3888     __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);
3889 
3890     if (VM_Version::supports_avx2()) {
3891       __ emit_data64(0x0405060700010203, relocInfo::none); // second copy
3892       __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);
3893       // _SHUF_00BA
3894       __ emit_data64(0x0b0a090803020100, relocInfo::none);
3895       __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);
3896       __ emit_data64(0x0b0a090803020100, relocInfo::none);
3897       __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);
3898       // _SHUF_DC00
3899       __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);
3900       __ emit_data64(0x0b0a090803020100, relocInfo::none);
3901       __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);
3902       __ emit_data64(0x0b0a090803020100, relocInfo::none);
3903     }
3904 
3905     return start;
3906   }
3907 
3908   //Mask for byte-swapping a couple of qwords in an XMM register using (v)pshufb.
3909   address generate_pshuffle_byte_flip_mask_sha512() {
3910     __ align(32);
3911     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;pshuffle_byte_flip_mask_sha512&quot;);
3912     address start = __ pc();
3913     if (VM_Version::supports_avx2()) {
3914       __ emit_data64(0x0001020304050607, relocInfo::none); // PSHUFFLE_BYTE_FLIP_MASK
3915       __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);
3916       __ emit_data64(0x1011121314151617, relocInfo::none);
3917       __ emit_data64(0x18191a1b1c1d1e1f, relocInfo::none);
3918       __ emit_data64(0x0000000000000000, relocInfo::none); //MASK_YMM_LO
3919       __ emit_data64(0x0000000000000000, relocInfo::none);
3920       __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);
3921       __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);
3922     }
3923 
3924     return start;
3925   }
3926 
3927 // ofs and limit are use for multi-block byte array.
3928 // int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)
3929   address generate_sha256_implCompress(bool multi_block, const char *name) {
3930     assert(VM_Version::supports_sha() || VM_Version::supports_avx2(), &quot;&quot;);
3931     __ align(CodeEntryAlignment);
3932     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3933     address start = __ pc();
3934 
3935     Register buf = c_rarg0;
3936     Register state = c_rarg1;
3937     Register ofs = c_rarg2;
3938     Register limit = c_rarg3;
3939 
3940     const XMMRegister msg = xmm0;
3941     const XMMRegister state0 = xmm1;
3942     const XMMRegister state1 = xmm2;
3943     const XMMRegister msgtmp0 = xmm3;
3944 
3945     const XMMRegister msgtmp1 = xmm4;
3946     const XMMRegister msgtmp2 = xmm5;
3947     const XMMRegister msgtmp3 = xmm6;
3948     const XMMRegister msgtmp4 = xmm7;
3949 
3950     const XMMRegister shuf_mask = xmm8;
3951 
3952     __ enter();
3953 
3954     __ subptr(rsp, 4 * wordSize);
3955 
3956     if (VM_Version::supports_sha()) {
3957       __ fast_sha256(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,
3958         buf, state, ofs, limit, rsp, multi_block, shuf_mask);
3959     } else if (VM_Version::supports_avx2()) {
3960       __ sha256_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,
3961         buf, state, ofs, limit, rsp, multi_block, shuf_mask);
3962     }
3963     __ addptr(rsp, 4 * wordSize);
3964     __ vzeroupper();
3965     __ leave();
3966     __ ret(0);
3967     return start;
3968   }
3969 
3970   address generate_sha512_implCompress(bool multi_block, const char *name) {
3971     assert(VM_Version::supports_avx2(), &quot;&quot;);
3972     assert(VM_Version::supports_bmi2(), &quot;&quot;);
3973     __ align(CodeEntryAlignment);
3974     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3975     address start = __ pc();
3976 
3977     Register buf = c_rarg0;
3978     Register state = c_rarg1;
3979     Register ofs = c_rarg2;
3980     Register limit = c_rarg3;
3981 
3982     const XMMRegister msg = xmm0;
3983     const XMMRegister state0 = xmm1;
3984     const XMMRegister state1 = xmm2;
3985     const XMMRegister msgtmp0 = xmm3;
3986     const XMMRegister msgtmp1 = xmm4;
3987     const XMMRegister msgtmp2 = xmm5;
3988     const XMMRegister msgtmp3 = xmm6;
3989     const XMMRegister msgtmp4 = xmm7;
3990 
3991     const XMMRegister shuf_mask = xmm8;
3992 
3993     __ enter();
3994 
3995     __ sha512_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,
3996     buf, state, ofs, limit, rsp, multi_block, shuf_mask);
3997 
3998     __ vzeroupper();
3999     __ leave();
4000     __ ret(0);
4001     return start;
4002   }
4003 
4004   // This mask is used for incrementing counter value(linc0, linc4, etc.)
4005   address counter_mask_addr() {
4006     __ align(64);
4007     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;counter_mask_addr&quot;);
4008     address start = __ pc();
4009     __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);//lbswapmask
4010     __ emit_data64(0x0001020304050607, relocInfo::none);
4011     __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);
4012     __ emit_data64(0x0001020304050607, relocInfo::none);
4013     __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);
4014     __ emit_data64(0x0001020304050607, relocInfo::none);
4015     __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);
4016     __ emit_data64(0x0001020304050607, relocInfo::none);
4017     __ emit_data64(0x0000000000000000, relocInfo::none);//linc0 = counter_mask_addr+64
4018     __ emit_data64(0x0000000000000000, relocInfo::none);
4019     __ emit_data64(0x0000000000000001, relocInfo::none);//counter_mask_addr() + 80
4020     __ emit_data64(0x0000000000000000, relocInfo::none);
4021     __ emit_data64(0x0000000000000002, relocInfo::none);
4022     __ emit_data64(0x0000000000000000, relocInfo::none);
4023     __ emit_data64(0x0000000000000003, relocInfo::none);
4024     __ emit_data64(0x0000000000000000, relocInfo::none);
4025     __ emit_data64(0x0000000000000004, relocInfo::none);//linc4 = counter_mask_addr() + 128
4026     __ emit_data64(0x0000000000000000, relocInfo::none);
4027     __ emit_data64(0x0000000000000004, relocInfo::none);
4028     __ emit_data64(0x0000000000000000, relocInfo::none);
4029     __ emit_data64(0x0000000000000004, relocInfo::none);
4030     __ emit_data64(0x0000000000000000, relocInfo::none);
4031     __ emit_data64(0x0000000000000004, relocInfo::none);
4032     __ emit_data64(0x0000000000000000, relocInfo::none);
4033     __ emit_data64(0x0000000000000008, relocInfo::none);//linc8 = counter_mask_addr() + 192
4034     __ emit_data64(0x0000000000000000, relocInfo::none);
4035     __ emit_data64(0x0000000000000008, relocInfo::none);
4036     __ emit_data64(0x0000000000000000, relocInfo::none);
4037     __ emit_data64(0x0000000000000008, relocInfo::none);
4038     __ emit_data64(0x0000000000000000, relocInfo::none);
4039     __ emit_data64(0x0000000000000008, relocInfo::none);
4040     __ emit_data64(0x0000000000000000, relocInfo::none);
4041     __ emit_data64(0x0000000000000020, relocInfo::none);//linc32 = counter_mask_addr() + 256
4042     __ emit_data64(0x0000000000000000, relocInfo::none);
4043     __ emit_data64(0x0000000000000020, relocInfo::none);
4044     __ emit_data64(0x0000000000000000, relocInfo::none);
4045     __ emit_data64(0x0000000000000020, relocInfo::none);
4046     __ emit_data64(0x0000000000000000, relocInfo::none);
4047     __ emit_data64(0x0000000000000020, relocInfo::none);
4048     __ emit_data64(0x0000000000000000, relocInfo::none);
4049     __ emit_data64(0x0000000000000010, relocInfo::none);//linc16 = counter_mask_addr() + 320
4050     __ emit_data64(0x0000000000000000, relocInfo::none);
4051     __ emit_data64(0x0000000000000010, relocInfo::none);
4052     __ emit_data64(0x0000000000000000, relocInfo::none);
4053     __ emit_data64(0x0000000000000010, relocInfo::none);
4054     __ emit_data64(0x0000000000000000, relocInfo::none);
4055     __ emit_data64(0x0000000000000010, relocInfo::none);
4056     __ emit_data64(0x0000000000000000, relocInfo::none);
4057     return start;
4058   }
4059 
4060  // Vector AES Counter implementation
4061   address generate_counterMode_VectorAESCrypt()  {
4062     __ align(CodeEntryAlignment);
4063     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;counterMode_AESCrypt&quot;);
4064     address start = __ pc();
4065     const Register from = c_rarg0; // source array address
4066     const Register to = c_rarg1; // destination array address
4067     const Register key = c_rarg2; // key array address r8
4068     const Register counter = c_rarg3; // counter byte array initialized from counter array address
4069     // and updated with the incremented counter in the end
4070 #ifndef _WIN64
4071     const Register len_reg = c_rarg4;
4072     const Register saved_encCounter_start = c_rarg5;
4073     const Register used_addr = r10;
4074     const Address  used_mem(rbp, 2 * wordSize);
4075     const Register used = r11;
4076 #else
4077     const Address len_mem(rbp, 6 * wordSize); // length is on stack on Win64
4078     const Address saved_encCounter_mem(rbp, 7 * wordSize); // saved encrypted counter is on stack on Win64
4079     const Address used_mem(rbp, 8 * wordSize); // used length is on stack on Win64
4080     const Register len_reg = r10; // pick the first volatile windows register
4081     const Register saved_encCounter_start = r11;
4082     const Register used_addr = r13;
4083     const Register used = r14;
4084 #endif
4085     __ enter();
4086    // Save state before entering routine
4087     __ push(r12);
4088     __ push(r13);
4089     __ push(r14);
4090     __ push(r15);
4091 #ifdef _WIN64
4092     // on win64, fill len_reg from stack position
4093     __ movl(len_reg, len_mem);
4094     __ movptr(saved_encCounter_start, saved_encCounter_mem);
4095     __ movptr(used_addr, used_mem);
4096     __ movl(used, Address(used_addr, 0));
4097 #else
4098     __ push(len_reg); // Save
4099     __ movptr(used_addr, used_mem);
4100     __ movl(used, Address(used_addr, 0));
4101 #endif
4102     __ push(rbx);
4103     __ aesctr_encrypt(from, to, key, counter, len_reg, used, used_addr, saved_encCounter_start);
4104     // Restore state before leaving routine
4105     __ pop(rbx);
4106 #ifdef _WIN64
4107     __ movl(rax, len_mem); // return length
4108 #else
4109     __ pop(rax); // return length
4110 #endif
4111     __ pop(r15);
4112     __ pop(r14);
4113     __ pop(r13);
4114     __ pop(r12);
4115 
4116     __ leave(); // required for proper stackwalking of RuntimeStub frame
4117     __ ret(0);
4118     return start;
4119   }
4120 
4121   // This is a version of CTR/AES crypt which does 6 blocks in a loop at a time
4122   // to hide instruction latency
4123   //
4124   // Arguments:
4125   //
4126   // Inputs:
4127   //   c_rarg0   - source byte array address
4128   //   c_rarg1   - destination byte array address
4129   //   c_rarg2   - K (key) in little endian int array
4130   //   c_rarg3   - counter vector byte array address
4131   //   Linux
4132   //     c_rarg4   -          input length
4133   //     c_rarg5   -          saved encryptedCounter start
4134   //     rbp + 6 * wordSize - saved used length
4135   //   Windows
4136   //     rbp + 6 * wordSize - input length
4137   //     rbp + 7 * wordSize - saved encryptedCounter start
4138   //     rbp + 8 * wordSize - saved used length
4139   //
4140   // Output:
4141   //   rax       - input length
4142   //
4143   address generate_counterMode_AESCrypt_Parallel() {
4144     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
4145     __ align(CodeEntryAlignment);
4146     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;counterMode_AESCrypt&quot;);
4147     address start = __ pc();
4148     const Register from = c_rarg0; // source array address
4149     const Register to = c_rarg1; // destination array address
4150     const Register key = c_rarg2; // key array address
4151     const Register counter = c_rarg3; // counter byte array initialized from counter array address
4152                                       // and updated with the incremented counter in the end
4153 #ifndef _WIN64
4154     const Register len_reg = c_rarg4;
4155     const Register saved_encCounter_start = c_rarg5;
4156     const Register used_addr = r10;
4157     const Address  used_mem(rbp, 2 * wordSize);
4158     const Register used = r11;
4159 #else
4160     const Address len_mem(rbp, 6 * wordSize); // length is on stack on Win64
4161     const Address saved_encCounter_mem(rbp, 7 * wordSize); // length is on stack on Win64
4162     const Address used_mem(rbp, 8 * wordSize); // length is on stack on Win64
4163     const Register len_reg = r10; // pick the first volatile windows register
4164     const Register saved_encCounter_start = r11;
4165     const Register used_addr = r13;
4166     const Register used = r14;
4167 #endif
4168     const Register pos = rax;
4169 
4170     const int PARALLEL_FACTOR = 6;
4171     const XMMRegister xmm_counter_shuf_mask = xmm0;
4172     const XMMRegister xmm_key_shuf_mask = xmm1; // used temporarily to swap key bytes up front
4173     const XMMRegister xmm_curr_counter = xmm2;
4174 
4175     const XMMRegister xmm_key_tmp0 = xmm3;
4176     const XMMRegister xmm_key_tmp1 = xmm4;
4177 
4178     // registers holding the four results in the parallelized loop
4179     const XMMRegister xmm_result0 = xmm5;
4180     const XMMRegister xmm_result1 = xmm6;
4181     const XMMRegister xmm_result2 = xmm7;
4182     const XMMRegister xmm_result3 = xmm8;
4183     const XMMRegister xmm_result4 = xmm9;
4184     const XMMRegister xmm_result5 = xmm10;
4185 
4186     const XMMRegister xmm_from0 = xmm11;
4187     const XMMRegister xmm_from1 = xmm12;
4188     const XMMRegister xmm_from2 = xmm13;
4189     const XMMRegister xmm_from3 = xmm14; //the last one is xmm14. we have to preserve it on WIN64.
4190     const XMMRegister xmm_from4 = xmm3; //reuse xmm3~4. Because xmm_key_tmp0~1 are useless when loading input text
4191     const XMMRegister xmm_from5 = xmm4;
4192 
4193     //for key_128, key_192, key_256
4194     const int rounds[3] = {10, 12, 14};
4195     Label L_exit_preLoop, L_preLoop_start;
4196     Label L_multiBlock_loopTop[3];
4197     Label L_singleBlockLoopTop[3];
4198     Label L__incCounter[3][6]; //for 6 blocks
4199     Label L__incCounter_single[3]; //for single block, key128, key192, key256
4200     Label L_processTail_insr[3], L_processTail_4_insr[3], L_processTail_2_insr[3], L_processTail_1_insr[3], L_processTail_exit_insr[3];
4201     Label L_processTail_4_extr[3], L_processTail_2_extr[3], L_processTail_1_extr[3], L_processTail_exit_extr[3];
4202 
4203     Label L_exit;
4204 
4205     __ enter(); // required for proper stackwalking of RuntimeStub frame
4206 
4207 #ifdef _WIN64
4208     // allocate spill slots for r13, r14
4209     enum {
4210         saved_r13_offset,
4211         saved_r14_offset
4212     };
4213     __ subptr(rsp, 2 * wordSize);
4214     __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);
4215     __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);
4216 
4217     // on win64, fill len_reg from stack position
4218     __ movl(len_reg, len_mem);
4219     __ movptr(saved_encCounter_start, saved_encCounter_mem);
4220     __ movptr(used_addr, used_mem);
4221     __ movl(used, Address(used_addr, 0));
4222 #else
4223     __ push(len_reg); // Save
4224     __ movptr(used_addr, used_mem);
4225     __ movl(used, Address(used_addr, 0));
4226 #endif
4227 
4228     __ push(rbx); // Save RBX
4229     __ movdqu(xmm_curr_counter, Address(counter, 0x00)); // initialize counter with initial counter
4230     __ movdqu(xmm_counter_shuf_mask, ExternalAddress(StubRoutines::x86::counter_shuffle_mask_addr()), pos); // pos as scratch
4231     __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); //counter is shuffled
4232     __ movptr(pos, 0);
4233 
4234     // Use the partially used encrpyted counter from last invocation
4235     __ BIND(L_preLoop_start);
4236     __ cmpptr(used, 16);
4237     __ jcc(Assembler::aboveEqual, L_exit_preLoop);
4238       __ cmpptr(len_reg, 0);
4239       __ jcc(Assembler::lessEqual, L_exit_preLoop);
4240       __ movb(rbx, Address(saved_encCounter_start, used));
4241       __ xorb(rbx, Address(from, pos));
4242       __ movb(Address(to, pos), rbx);
4243       __ addptr(pos, 1);
4244       __ addptr(used, 1);
4245       __ subptr(len_reg, 1);
4246 
4247     __ jmp(L_preLoop_start);
4248 
4249     __ BIND(L_exit_preLoop);
4250     __ movl(Address(used_addr, 0), used);
4251 
4252     // key length could be only {11, 13, 15} * 4 = {44, 52, 60}
4253     __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()), rbx); // rbx as scratch
4254     __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
4255     __ cmpl(rbx, 52);
4256     __ jcc(Assembler::equal, L_multiBlock_loopTop[1]);
4257     __ cmpl(rbx, 60);
4258     __ jcc(Assembler::equal, L_multiBlock_loopTop[2]);
4259 
4260 #define CTR_DoSix(opc, src_reg)                \
4261     __ opc(xmm_result0, src_reg);              \
4262     __ opc(xmm_result1, src_reg);              \
4263     __ opc(xmm_result2, src_reg);              \
4264     __ opc(xmm_result3, src_reg);              \
4265     __ opc(xmm_result4, src_reg);              \
4266     __ opc(xmm_result5, src_reg);
4267 
4268     // k == 0 :  generate code for key_128
4269     // k == 1 :  generate code for key_192
4270     // k == 2 :  generate code for key_256
4271     for (int k = 0; k &lt; 3; ++k) {
4272       //multi blocks starts here
4273       __ align(OptoLoopAlignment);
4274       __ BIND(L_multiBlock_loopTop[k]);
4275       __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); // see if at least PARALLEL_FACTOR blocks left
4276       __ jcc(Assembler::less, L_singleBlockLoopTop[k]);
4277       load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);
4278 
4279       //load, then increase counters
4280       CTR_DoSix(movdqa, xmm_curr_counter);
4281       inc_counter(rbx, xmm_result1, 0x01, L__incCounter[k][0]);
4282       inc_counter(rbx, xmm_result2, 0x02, L__incCounter[k][1]);
4283       inc_counter(rbx, xmm_result3, 0x03, L__incCounter[k][2]);
4284       inc_counter(rbx, xmm_result4, 0x04, L__incCounter[k][3]);
4285       inc_counter(rbx, xmm_result5,  0x05, L__incCounter[k][4]);
4286       inc_counter(rbx, xmm_curr_counter, 0x06, L__incCounter[k][5]);
4287       CTR_DoSix(pshufb, xmm_counter_shuf_mask); // after increased, shuffled counters back for PXOR
4288       CTR_DoSix(pxor, xmm_key_tmp0);   //PXOR with Round 0 key
4289 
4290       //load two ROUND_KEYs at a time
4291       for (int i = 1; i &lt; rounds[k]; ) {
4292         load_key(xmm_key_tmp1, key, (0x10 * i), xmm_key_shuf_mask);
4293         load_key(xmm_key_tmp0, key, (0x10 * (i+1)), xmm_key_shuf_mask);
4294         CTR_DoSix(aesenc, xmm_key_tmp1);
4295         i++;
4296         if (i != rounds[k]) {
4297           CTR_DoSix(aesenc, xmm_key_tmp0);
4298         } else {
4299           CTR_DoSix(aesenclast, xmm_key_tmp0);
4300         }
4301         i++;
4302       }
4303 
4304       // get next PARALLEL_FACTOR blocks into xmm_result registers
4305       __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));
4306       __ movdqu(xmm_from1, Address(from, pos, Address::times_1, 1 * AESBlockSize));
4307       __ movdqu(xmm_from2, Address(from, pos, Address::times_1, 2 * AESBlockSize));
4308       __ movdqu(xmm_from3, Address(from, pos, Address::times_1, 3 * AESBlockSize));
4309       __ movdqu(xmm_from4, Address(from, pos, Address::times_1, 4 * AESBlockSize));
4310       __ movdqu(xmm_from5, Address(from, pos, Address::times_1, 5 * AESBlockSize));
4311 
4312       __ pxor(xmm_result0, xmm_from0);
4313       __ pxor(xmm_result1, xmm_from1);
4314       __ pxor(xmm_result2, xmm_from2);
4315       __ pxor(xmm_result3, xmm_from3);
4316       __ pxor(xmm_result4, xmm_from4);
4317       __ pxor(xmm_result5, xmm_from5);
4318 
4319       // store 6 results into the next 64 bytes of output
4320       __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);
4321       __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);
4322       __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);
4323       __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);
4324       __ movdqu(Address(to, pos, Address::times_1, 4 * AESBlockSize), xmm_result4);
4325       __ movdqu(Address(to, pos, Address::times_1, 5 * AESBlockSize), xmm_result5);
4326 
4327       __ addptr(pos, PARALLEL_FACTOR * AESBlockSize); // increase the length of crypt text
4328       __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize); // decrease the remaining length
4329       __ jmp(L_multiBlock_loopTop[k]);
4330 
4331       // singleBlock starts here
4332       __ align(OptoLoopAlignment);
4333       __ BIND(L_singleBlockLoopTop[k]);
4334       __ cmpptr(len_reg, 0);
4335       __ jcc(Assembler::lessEqual, L_exit);
4336       load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);
4337       __ movdqa(xmm_result0, xmm_curr_counter);
4338       inc_counter(rbx, xmm_curr_counter, 0x01, L__incCounter_single[k]);
4339       __ pshufb(xmm_result0, xmm_counter_shuf_mask);
4340       __ pxor(xmm_result0, xmm_key_tmp0);
4341       for (int i = 1; i &lt; rounds[k]; i++) {
4342         load_key(xmm_key_tmp0, key, (0x10 * i), xmm_key_shuf_mask);
4343         __ aesenc(xmm_result0, xmm_key_tmp0);
4344       }
4345       load_key(xmm_key_tmp0, key, (rounds[k] * 0x10), xmm_key_shuf_mask);
4346       __ aesenclast(xmm_result0, xmm_key_tmp0);
4347       __ cmpptr(len_reg, AESBlockSize);
4348       __ jcc(Assembler::less, L_processTail_insr[k]);
4349         __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));
4350         __ pxor(xmm_result0, xmm_from0);
4351         __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);
4352         __ addptr(pos, AESBlockSize);
4353         __ subptr(len_reg, AESBlockSize);
4354         __ jmp(L_singleBlockLoopTop[k]);
4355       __ BIND(L_processTail_insr[k]);                               // Process the tail part of the input array
4356         __ addptr(pos, len_reg);                                    // 1. Insert bytes from src array into xmm_from0 register
4357         __ testptr(len_reg, 8);
4358         __ jcc(Assembler::zero, L_processTail_4_insr[k]);
4359           __ subptr(pos,8);
4360           __ pinsrq(xmm_from0, Address(from, pos), 0);
4361         __ BIND(L_processTail_4_insr[k]);
4362         __ testptr(len_reg, 4);
4363         __ jcc(Assembler::zero, L_processTail_2_insr[k]);
4364           __ subptr(pos,4);
4365           __ pslldq(xmm_from0, 4);
4366           __ pinsrd(xmm_from0, Address(from, pos), 0);
4367         __ BIND(L_processTail_2_insr[k]);
4368         __ testptr(len_reg, 2);
4369         __ jcc(Assembler::zero, L_processTail_1_insr[k]);
4370           __ subptr(pos, 2);
4371           __ pslldq(xmm_from0, 2);
4372           __ pinsrw(xmm_from0, Address(from, pos), 0);
4373         __ BIND(L_processTail_1_insr[k]);
4374         __ testptr(len_reg, 1);
4375         __ jcc(Assembler::zero, L_processTail_exit_insr[k]);
4376           __ subptr(pos, 1);
4377           __ pslldq(xmm_from0, 1);
4378           __ pinsrb(xmm_from0, Address(from, pos), 0);
4379         __ BIND(L_processTail_exit_insr[k]);
4380 
4381         __ movdqu(Address(saved_encCounter_start, 0), xmm_result0);  // 2. Perform pxor of the encrypted counter and plaintext Bytes.
4382         __ pxor(xmm_result0, xmm_from0);                             //    Also the encrypted counter is saved for next invocation.
4383 
4384         __ testptr(len_reg, 8);
4385         __ jcc(Assembler::zero, L_processTail_4_extr[k]);            // 3. Extract bytes from xmm_result0 into the dest. array
4386           __ pextrq(Address(to, pos), xmm_result0, 0);
4387           __ psrldq(xmm_result0, 8);
4388           __ addptr(pos, 8);
4389         __ BIND(L_processTail_4_extr[k]);
4390         __ testptr(len_reg, 4);
4391         __ jcc(Assembler::zero, L_processTail_2_extr[k]);
4392           __ pextrd(Address(to, pos), xmm_result0, 0);
4393           __ psrldq(xmm_result0, 4);
4394           __ addptr(pos, 4);
4395         __ BIND(L_processTail_2_extr[k]);
4396         __ testptr(len_reg, 2);
4397         __ jcc(Assembler::zero, L_processTail_1_extr[k]);
4398           __ pextrw(Address(to, pos), xmm_result0, 0);
4399           __ psrldq(xmm_result0, 2);
4400           __ addptr(pos, 2);
4401         __ BIND(L_processTail_1_extr[k]);
4402         __ testptr(len_reg, 1);
4403         __ jcc(Assembler::zero, L_processTail_exit_extr[k]);
4404           __ pextrb(Address(to, pos), xmm_result0, 0);
4405 
4406         __ BIND(L_processTail_exit_extr[k]);
4407         __ movl(Address(used_addr, 0), len_reg);
4408         __ jmp(L_exit);
4409 
4410     }
4411 
4412     __ BIND(L_exit);
4413     __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); //counter is shuffled back.
4414     __ movdqu(Address(counter, 0), xmm_curr_counter); //save counter back
4415     __ pop(rbx); // pop the saved RBX.
4416 #ifdef _WIN64
4417     __ movl(rax, len_mem);
4418     __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));
4419     __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));
4420     __ addptr(rsp, 2 * wordSize);
4421 #else
4422     __ pop(rax); // return &#39;len&#39;
4423 #endif
4424     __ leave(); // required for proper stackwalking of RuntimeStub frame
4425     __ ret(0);
4426     return start;
4427   }
4428 
4429 void roundDec(XMMRegister xmm_reg) {
4430   __ vaesdec(xmm1, xmm1, xmm_reg, Assembler::AVX_512bit);
4431   __ vaesdec(xmm2, xmm2, xmm_reg, Assembler::AVX_512bit);
4432   __ vaesdec(xmm3, xmm3, xmm_reg, Assembler::AVX_512bit);
4433   __ vaesdec(xmm4, xmm4, xmm_reg, Assembler::AVX_512bit);
4434   __ vaesdec(xmm5, xmm5, xmm_reg, Assembler::AVX_512bit);
4435   __ vaesdec(xmm6, xmm6, xmm_reg, Assembler::AVX_512bit);
4436   __ vaesdec(xmm7, xmm7, xmm_reg, Assembler::AVX_512bit);
4437   __ vaesdec(xmm8, xmm8, xmm_reg, Assembler::AVX_512bit);
4438 }
4439 
4440 void roundDeclast(XMMRegister xmm_reg) {
4441   __ vaesdeclast(xmm1, xmm1, xmm_reg, Assembler::AVX_512bit);
4442   __ vaesdeclast(xmm2, xmm2, xmm_reg, Assembler::AVX_512bit);
4443   __ vaesdeclast(xmm3, xmm3, xmm_reg, Assembler::AVX_512bit);
4444   __ vaesdeclast(xmm4, xmm4, xmm_reg, Assembler::AVX_512bit);
4445   __ vaesdeclast(xmm5, xmm5, xmm_reg, Assembler::AVX_512bit);
4446   __ vaesdeclast(xmm6, xmm6, xmm_reg, Assembler::AVX_512bit);
4447   __ vaesdeclast(xmm7, xmm7, xmm_reg, Assembler::AVX_512bit);
4448   __ vaesdeclast(xmm8, xmm8, xmm_reg, Assembler::AVX_512bit);
4449 }
4450 
4451   void ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = NULL) {
4452     __ movdqu(xmmdst, Address(key, offset));
4453     if (xmm_shuf_mask != NULL) {
4454       __ pshufb(xmmdst, xmm_shuf_mask);
4455     } else {
4456       __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
4457     }
4458     __ evshufi64x2(xmmdst, xmmdst, xmmdst, 0x0, Assembler::AVX_512bit);
4459 
4460   }
4461 
4462 address generate_cipherBlockChaining_decryptVectorAESCrypt() {
4463     assert(VM_Version::supports_avx512_vaes(), &quot;need AES instructions and misaligned SSE support&quot;);
4464     __ align(CodeEntryAlignment);
4465     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_decryptAESCrypt&quot;);
4466     address start = __ pc();
4467 
4468     const Register from = c_rarg0;  // source array address
4469     const Register to = c_rarg1;  // destination array address
4470     const Register key = c_rarg2;  // key array address
4471     const Register rvec = c_rarg3;  // r byte array initialized from initvector array address
4472     // and left with the results of the last encryption block
4473 #ifndef _WIN64
4474     const Register len_reg = c_rarg4;  // src len (must be multiple of blocksize 16)
4475 #else
4476     const Address  len_mem(rbp, 6 * wordSize);  // length is on stack on Win64
4477     const Register len_reg = r11;      // pick the volatile windows register
4478 #endif
4479 
4480     Label Loop, Loop1, L_128, L_256, L_192, KEY_192, KEY_256, Loop2, Lcbc_dec_rem_loop,
4481           Lcbc_dec_rem_last, Lcbc_dec_ret, Lcbc_dec_rem, Lcbc_exit;
4482 
4483     __ enter();
4484 
4485 #ifdef _WIN64
4486   // on win64, fill len_reg from stack position
4487     __ movl(len_reg, len_mem);
4488 #else
4489     __ push(len_reg); // Save
4490 #endif
4491     __ push(rbx);
4492     __ vzeroupper();
4493 
4494     // Temporary variable declaration for swapping key bytes
4495     const XMMRegister xmm_key_shuf_mask = xmm1;
4496     __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
4497 
4498     // Calculate number of rounds from key size: 44 for 10-rounds, 52 for 12-rounds, 60 for 14-rounds
4499     const Register rounds = rbx;
4500     __ movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
4501 
4502     const XMMRegister IV = xmm0;
4503     // Load IV and broadcast value to 512-bits
4504     __ evbroadcasti64x2(IV, Address(rvec, 0), Assembler::AVX_512bit);
4505 
4506     // Temporary variables for storing round keys
4507     const XMMRegister RK0 = xmm30;
4508     const XMMRegister RK1 = xmm9;
4509     const XMMRegister RK2 = xmm18;
4510     const XMMRegister RK3 = xmm19;
4511     const XMMRegister RK4 = xmm20;
4512     const XMMRegister RK5 = xmm21;
4513     const XMMRegister RK6 = xmm22;
4514     const XMMRegister RK7 = xmm23;
4515     const XMMRegister RK8 = xmm24;
4516     const XMMRegister RK9 = xmm25;
4517     const XMMRegister RK10 = xmm26;
4518 
4519      // Load and shuffle key
4520     // the java expanded key ordering is rotated one position from what we want
4521     // so we start from 1*16 here and hit 0*16 last
4522     ev_load_key(RK1, key, 1 * 16, xmm_key_shuf_mask);
4523     ev_load_key(RK2, key, 2 * 16, xmm_key_shuf_mask);
4524     ev_load_key(RK3, key, 3 * 16, xmm_key_shuf_mask);
4525     ev_load_key(RK4, key, 4 * 16, xmm_key_shuf_mask);
4526     ev_load_key(RK5, key, 5 * 16, xmm_key_shuf_mask);
4527     ev_load_key(RK6, key, 6 * 16, xmm_key_shuf_mask);
4528     ev_load_key(RK7, key, 7 * 16, xmm_key_shuf_mask);
4529     ev_load_key(RK8, key, 8 * 16, xmm_key_shuf_mask);
4530     ev_load_key(RK9, key, 9 * 16, xmm_key_shuf_mask);
4531     ev_load_key(RK10, key, 10 * 16, xmm_key_shuf_mask);
4532     ev_load_key(RK0, key, 0*16, xmm_key_shuf_mask);
4533 
4534     // Variables for storing source cipher text
4535     const XMMRegister S0 = xmm10;
4536     const XMMRegister S1 = xmm11;
4537     const XMMRegister S2 = xmm12;
4538     const XMMRegister S3 = xmm13;
4539     const XMMRegister S4 = xmm14;
4540     const XMMRegister S5 = xmm15;
4541     const XMMRegister S6 = xmm16;
4542     const XMMRegister S7 = xmm17;
4543 
4544     // Variables for storing decrypted text
4545     const XMMRegister B0 = xmm1;
4546     const XMMRegister B1 = xmm2;
4547     const XMMRegister B2 = xmm3;
4548     const XMMRegister B3 = xmm4;
4549     const XMMRegister B4 = xmm5;
4550     const XMMRegister B5 = xmm6;
4551     const XMMRegister B6 = xmm7;
4552     const XMMRegister B7 = xmm8;
4553 
4554     __ cmpl(rounds, 44);
4555     __ jcc(Assembler::greater, KEY_192);
4556     __ jmp(Loop);
4557 
4558     __ BIND(KEY_192);
4559     const XMMRegister RK11 = xmm27;
4560     const XMMRegister RK12 = xmm28;
4561     ev_load_key(RK11, key, 11*16, xmm_key_shuf_mask);
4562     ev_load_key(RK12, key, 12*16, xmm_key_shuf_mask);
4563 
4564     __ cmpl(rounds, 52);
4565     __ jcc(Assembler::greater, KEY_256);
4566     __ jmp(Loop);
4567 
4568     __ BIND(KEY_256);
4569     const XMMRegister RK13 = xmm29;
4570     const XMMRegister RK14 = xmm31;
4571     ev_load_key(RK13, key, 13*16, xmm_key_shuf_mask);
4572     ev_load_key(RK14, key, 14*16, xmm_key_shuf_mask);
4573 
4574     __ BIND(Loop);
4575     __ cmpl(len_reg, 512);
4576     __ jcc(Assembler::below, Lcbc_dec_rem);
4577     __ BIND(Loop1);
4578     __ subl(len_reg, 512);
4579     __ evmovdquq(S0, Address(from, 0 * 64), Assembler::AVX_512bit);
4580     __ evmovdquq(S1, Address(from, 1 * 64), Assembler::AVX_512bit);
4581     __ evmovdquq(S2, Address(from, 2 * 64), Assembler::AVX_512bit);
4582     __ evmovdquq(S3, Address(from, 3 * 64), Assembler::AVX_512bit);
4583     __ evmovdquq(S4, Address(from, 4 * 64), Assembler::AVX_512bit);
4584     __ evmovdquq(S5, Address(from, 5 * 64), Assembler::AVX_512bit);
4585     __ evmovdquq(S6, Address(from, 6 * 64), Assembler::AVX_512bit);
4586     __ evmovdquq(S7, Address(from, 7 * 64), Assembler::AVX_512bit);
4587     __ leaq(from, Address(from, 8 * 64));
4588 
4589     __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);
4590     __ evpxorq(B1, S1, RK1, Assembler::AVX_512bit);
4591     __ evpxorq(B2, S2, RK1, Assembler::AVX_512bit);
4592     __ evpxorq(B3, S3, RK1, Assembler::AVX_512bit);
4593     __ evpxorq(B4, S4, RK1, Assembler::AVX_512bit);
4594     __ evpxorq(B5, S5, RK1, Assembler::AVX_512bit);
4595     __ evpxorq(B6, S6, RK1, Assembler::AVX_512bit);
4596     __ evpxorq(B7, S7, RK1, Assembler::AVX_512bit);
4597 
4598     __ evalignq(IV, S0, IV, 0x06);
4599     __ evalignq(S0, S1, S0, 0x06);
4600     __ evalignq(S1, S2, S1, 0x06);
4601     __ evalignq(S2, S3, S2, 0x06);
4602     __ evalignq(S3, S4, S3, 0x06);
4603     __ evalignq(S4, S5, S4, 0x06);
4604     __ evalignq(S5, S6, S5, 0x06);
4605     __ evalignq(S6, S7, S6, 0x06);
4606 
4607     roundDec(RK2);
4608     roundDec(RK3);
4609     roundDec(RK4);
4610     roundDec(RK5);
4611     roundDec(RK6);
4612     roundDec(RK7);
4613     roundDec(RK8);
4614     roundDec(RK9);
4615     roundDec(RK10);
4616 
4617     __ cmpl(rounds, 44);
4618     __ jcc(Assembler::belowEqual, L_128);
4619     roundDec(RK11);
4620     roundDec(RK12);
4621 
4622     __ cmpl(rounds, 52);
4623     __ jcc(Assembler::belowEqual, L_192);
4624     roundDec(RK13);
4625     roundDec(RK14);
4626 
4627     __ BIND(L_256);
4628     roundDeclast(RK0);
4629     __ jmp(Loop2);
4630 
4631     __ BIND(L_128);
4632     roundDeclast(RK0);
4633     __ jmp(Loop2);
4634 
4635     __ BIND(L_192);
4636     roundDeclast(RK0);
4637 
4638     __ BIND(Loop2);
4639     __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);
4640     __ evpxorq(B1, B1, S0, Assembler::AVX_512bit);
4641     __ evpxorq(B2, B2, S1, Assembler::AVX_512bit);
4642     __ evpxorq(B3, B3, S2, Assembler::AVX_512bit);
4643     __ evpxorq(B4, B4, S3, Assembler::AVX_512bit);
4644     __ evpxorq(B5, B5, S4, Assembler::AVX_512bit);
4645     __ evpxorq(B6, B6, S5, Assembler::AVX_512bit);
4646     __ evpxorq(B7, B7, S6, Assembler::AVX_512bit);
4647     __ evmovdquq(IV, S7, Assembler::AVX_512bit);
4648 
4649     __ evmovdquq(Address(to, 0 * 64), B0, Assembler::AVX_512bit);
4650     __ evmovdquq(Address(to, 1 * 64), B1, Assembler::AVX_512bit);
4651     __ evmovdquq(Address(to, 2 * 64), B2, Assembler::AVX_512bit);
4652     __ evmovdquq(Address(to, 3 * 64), B3, Assembler::AVX_512bit);
4653     __ evmovdquq(Address(to, 4 * 64), B4, Assembler::AVX_512bit);
4654     __ evmovdquq(Address(to, 5 * 64), B5, Assembler::AVX_512bit);
4655     __ evmovdquq(Address(to, 6 * 64), B6, Assembler::AVX_512bit);
4656     __ evmovdquq(Address(to, 7 * 64), B7, Assembler::AVX_512bit);
4657     __ leaq(to, Address(to, 8 * 64));
4658     __ jmp(Loop);
4659 
4660     __ BIND(Lcbc_dec_rem);
4661     __ evshufi64x2(IV, IV, IV, 0x03, Assembler::AVX_512bit);
4662 
4663     __ BIND(Lcbc_dec_rem_loop);
4664     __ subl(len_reg, 16);
4665     __ jcc(Assembler::carrySet, Lcbc_dec_ret);
4666 
4667     __ movdqu(S0, Address(from, 0));
4668     __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);
4669     __ vaesdec(B0, B0, RK2, Assembler::AVX_512bit);
4670     __ vaesdec(B0, B0, RK3, Assembler::AVX_512bit);
4671     __ vaesdec(B0, B0, RK4, Assembler::AVX_512bit);
4672     __ vaesdec(B0, B0, RK5, Assembler::AVX_512bit);
4673     __ vaesdec(B0, B0, RK6, Assembler::AVX_512bit);
4674     __ vaesdec(B0, B0, RK7, Assembler::AVX_512bit);
4675     __ vaesdec(B0, B0, RK8, Assembler::AVX_512bit);
4676     __ vaesdec(B0, B0, RK9, Assembler::AVX_512bit);
4677     __ vaesdec(B0, B0, RK10, Assembler::AVX_512bit);
4678     __ cmpl(rounds, 44);
4679     __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);
4680 
4681     __ vaesdec(B0, B0, RK11, Assembler::AVX_512bit);
4682     __ vaesdec(B0, B0, RK12, Assembler::AVX_512bit);
4683     __ cmpl(rounds, 52);
4684     __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);
4685 
4686     __ vaesdec(B0, B0, RK13, Assembler::AVX_512bit);
4687     __ vaesdec(B0, B0, RK14, Assembler::AVX_512bit);
4688 
4689     __ BIND(Lcbc_dec_rem_last);
4690     __ vaesdeclast(B0, B0, RK0, Assembler::AVX_512bit);
4691 
4692     __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);
4693     __ evmovdquq(IV, S0, Assembler::AVX_512bit);
4694     __ movdqu(Address(to, 0), B0);
4695     __ leaq(from, Address(from, 16));
4696     __ leaq(to, Address(to, 16));
4697     __ jmp(Lcbc_dec_rem_loop);
4698 
4699     __ BIND(Lcbc_dec_ret);
4700     __ movdqu(Address(rvec, 0), IV);
4701 
4702     // Zero out the round keys
4703     __ evpxorq(RK0, RK0, RK0, Assembler::AVX_512bit);
4704     __ evpxorq(RK1, RK1, RK1, Assembler::AVX_512bit);
4705     __ evpxorq(RK2, RK2, RK2, Assembler::AVX_512bit);
4706     __ evpxorq(RK3, RK3, RK3, Assembler::AVX_512bit);
4707     __ evpxorq(RK4, RK4, RK4, Assembler::AVX_512bit);
4708     __ evpxorq(RK5, RK5, RK5, Assembler::AVX_512bit);
4709     __ evpxorq(RK6, RK6, RK6, Assembler::AVX_512bit);
4710     __ evpxorq(RK7, RK7, RK7, Assembler::AVX_512bit);
4711     __ evpxorq(RK8, RK8, RK8, Assembler::AVX_512bit);
4712     __ evpxorq(RK9, RK9, RK9, Assembler::AVX_512bit);
4713     __ evpxorq(RK10, RK10, RK10, Assembler::AVX_512bit);
4714     __ cmpl(rounds, 44);
4715     __ jcc(Assembler::belowEqual, Lcbc_exit);
4716     __ evpxorq(RK11, RK11, RK11, Assembler::AVX_512bit);
4717     __ evpxorq(RK12, RK12, RK12, Assembler::AVX_512bit);
4718     __ cmpl(rounds, 52);
4719     __ jcc(Assembler::belowEqual, Lcbc_exit);
4720     __ evpxorq(RK13, RK13, RK13, Assembler::AVX_512bit);
4721     __ evpxorq(RK14, RK14, RK14, Assembler::AVX_512bit);
4722 
4723     __ BIND(Lcbc_exit);
4724     __ pop(rbx);
4725 #ifdef _WIN64
4726     __ movl(rax, len_mem);
4727 #else
4728     __ pop(rax); // return length
4729 #endif
4730     __ leave(); // required for proper stackwalking of RuntimeStub frame
4731     __ ret(0);
4732     return start;
4733 }
4734 
4735 // Polynomial x^128+x^127+x^126+x^121+1
4736 address ghash_polynomial_addr() {
4737     __ align(CodeEntryAlignment);
4738     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_ghash_poly_addr&quot;);
4739     address start = __ pc();
4740     __ emit_data64(0x0000000000000001, relocInfo::none);
4741     __ emit_data64(0xc200000000000000, relocInfo::none);
4742     return start;
4743 }
4744 
4745 address ghash_shufflemask_addr() {
4746     __ align(CodeEntryAlignment);
4747     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_ghash_shuffmask_addr&quot;);
4748     address start = __ pc();
4749     __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);
4750     __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);
4751     return start;
4752 }
4753 
4754 // Ghash single and multi block operations using AVX instructions
4755 address generate_avx_ghash_processBlocks() {
4756     __ align(CodeEntryAlignment);
4757 
4758     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;ghash_processBlocks&quot;);
4759     address start = __ pc();
4760 
4761     // arguments
4762     const Register state = c_rarg0;
4763     const Register htbl = c_rarg1;
4764     const Register data = c_rarg2;
4765     const Register blocks = c_rarg3;
4766     __ enter();
4767    // Save state before entering routine
4768     __ avx_ghash(state, htbl, data, blocks);
4769     __ leave(); // required for proper stackwalking of RuntimeStub frame
4770     __ ret(0);
4771     return start;
4772 }
4773 
4774   // byte swap x86 long
4775   address generate_ghash_long_swap_mask() {
4776     __ align(CodeEntryAlignment);
4777     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;ghash_long_swap_mask&quot;);
4778     address start = __ pc();
4779     __ emit_data64(0x0f0e0d0c0b0a0908, relocInfo::none );
4780     __ emit_data64(0x0706050403020100, relocInfo::none );
4781   return start;
4782   }
4783 
4784   // byte swap x86 byte array
4785   address generate_ghash_byte_swap_mask() {
4786     __ align(CodeEntryAlignment);
4787     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;ghash_byte_swap_mask&quot;);
4788     address start = __ pc();
4789     __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none );
4790     __ emit_data64(0x0001020304050607, relocInfo::none );
4791   return start;
4792   }
4793 
4794   /* Single and multi-block ghash operations */
4795   address generate_ghash_processBlocks() {
4796     __ align(CodeEntryAlignment);
4797     Label L_ghash_loop, L_exit;
4798     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;ghash_processBlocks&quot;);
4799     address start = __ pc();
4800 
4801     const Register state        = c_rarg0;
4802     const Register subkeyH      = c_rarg1;
4803     const Register data         = c_rarg2;
4804     const Register blocks       = c_rarg3;
4805 
4806     const XMMRegister xmm_temp0 = xmm0;
4807     const XMMRegister xmm_temp1 = xmm1;
4808     const XMMRegister xmm_temp2 = xmm2;
4809     const XMMRegister xmm_temp3 = xmm3;
4810     const XMMRegister xmm_temp4 = xmm4;
4811     const XMMRegister xmm_temp5 = xmm5;
4812     const XMMRegister xmm_temp6 = xmm6;
4813     const XMMRegister xmm_temp7 = xmm7;
4814     const XMMRegister xmm_temp8 = xmm8;
4815     const XMMRegister xmm_temp9 = xmm9;
4816     const XMMRegister xmm_temp10 = xmm10;
4817 
4818     __ enter();
4819 
4820     __ movdqu(xmm_temp10, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));
4821 
4822     __ movdqu(xmm_temp0, Address(state, 0));
4823     __ pshufb(xmm_temp0, xmm_temp10);
4824 
4825 
4826     __ BIND(L_ghash_loop);
4827     __ movdqu(xmm_temp2, Address(data, 0));
4828     __ pshufb(xmm_temp2, ExternalAddress(StubRoutines::x86::ghash_byte_swap_mask_addr()));
4829 
4830     __ movdqu(xmm_temp1, Address(subkeyH, 0));
4831     __ pshufb(xmm_temp1, xmm_temp10);
4832 
4833     __ pxor(xmm_temp0, xmm_temp2);
4834 
4835     //
4836     // Multiply with the hash key
4837     //
4838     __ movdqu(xmm_temp3, xmm_temp0);
4839     __ pclmulqdq(xmm_temp3, xmm_temp1, 0);      // xmm3 holds a0*b0
4840     __ movdqu(xmm_temp4, xmm_temp0);
4841     __ pclmulqdq(xmm_temp4, xmm_temp1, 16);     // xmm4 holds a0*b1
4842 
4843     __ movdqu(xmm_temp5, xmm_temp0);
4844     __ pclmulqdq(xmm_temp5, xmm_temp1, 1);      // xmm5 holds a1*b0
4845     __ movdqu(xmm_temp6, xmm_temp0);
4846     __ pclmulqdq(xmm_temp6, xmm_temp1, 17);     // xmm6 holds a1*b1
4847 
4848     __ pxor(xmm_temp4, xmm_temp5);      // xmm4 holds a0*b1 + a1*b0
4849 
4850     __ movdqu(xmm_temp5, xmm_temp4);    // move the contents of xmm4 to xmm5
4851     __ psrldq(xmm_temp4, 8);    // shift by xmm4 64 bits to the right
4852     __ pslldq(xmm_temp5, 8);    // shift by xmm5 64 bits to the left
4853     __ pxor(xmm_temp3, xmm_temp5);
4854     __ pxor(xmm_temp6, xmm_temp4);      // Register pair &lt;xmm6:xmm3&gt; holds the result
4855                                         // of the carry-less multiplication of
4856                                         // xmm0 by xmm1.
4857 
4858     // We shift the result of the multiplication by one bit position
4859     // to the left to cope for the fact that the bits are reversed.
4860     __ movdqu(xmm_temp7, xmm_temp3);
4861     __ movdqu(xmm_temp8, xmm_temp6);
4862     __ pslld(xmm_temp3, 1);
4863     __ pslld(xmm_temp6, 1);
4864     __ psrld(xmm_temp7, 31);
4865     __ psrld(xmm_temp8, 31);
4866     __ movdqu(xmm_temp9, xmm_temp7);
4867     __ pslldq(xmm_temp8, 4);
4868     __ pslldq(xmm_temp7, 4);
4869     __ psrldq(xmm_temp9, 12);
4870     __ por(xmm_temp3, xmm_temp7);
4871     __ por(xmm_temp6, xmm_temp8);
4872     __ por(xmm_temp6, xmm_temp9);
4873 
4874     //
4875     // First phase of the reduction
4876     //
4877     // Move xmm3 into xmm7, xmm8, xmm9 in order to perform the shifts
4878     // independently.
4879     __ movdqu(xmm_temp7, xmm_temp3);
4880     __ movdqu(xmm_temp8, xmm_temp3);
4881     __ movdqu(xmm_temp9, xmm_temp3);
4882     __ pslld(xmm_temp7, 31);    // packed right shift shifting &lt;&lt; 31
4883     __ pslld(xmm_temp8, 30);    // packed right shift shifting &lt;&lt; 30
4884     __ pslld(xmm_temp9, 25);    // packed right shift shifting &lt;&lt; 25
4885     __ pxor(xmm_temp7, xmm_temp8);      // xor the shifted versions
4886     __ pxor(xmm_temp7, xmm_temp9);
4887     __ movdqu(xmm_temp8, xmm_temp7);
4888     __ pslldq(xmm_temp7, 12);
4889     __ psrldq(xmm_temp8, 4);
4890     __ pxor(xmm_temp3, xmm_temp7);      // first phase of the reduction complete
4891 
4892     //
4893     // Second phase of the reduction
4894     //
4895     // Make 3 copies of xmm3 in xmm2, xmm4, xmm5 for doing these
4896     // shift operations.
4897     __ movdqu(xmm_temp2, xmm_temp3);
4898     __ movdqu(xmm_temp4, xmm_temp3);
4899     __ movdqu(xmm_temp5, xmm_temp3);
4900     __ psrld(xmm_temp2, 1);     // packed left shifting &gt;&gt; 1
4901     __ psrld(xmm_temp4, 2);     // packed left shifting &gt;&gt; 2
4902     __ psrld(xmm_temp5, 7);     // packed left shifting &gt;&gt; 7
4903     __ pxor(xmm_temp2, xmm_temp4);      // xor the shifted versions
4904     __ pxor(xmm_temp2, xmm_temp5);
4905     __ pxor(xmm_temp2, xmm_temp8);
4906     __ pxor(xmm_temp3, xmm_temp2);
4907     __ pxor(xmm_temp6, xmm_temp3);      // the result is in xmm6
4908 
4909     __ decrement(blocks);
4910     __ jcc(Assembler::zero, L_exit);
4911     __ movdqu(xmm_temp0, xmm_temp6);
4912     __ addptr(data, 16);
4913     __ jmp(L_ghash_loop);
4914 
4915     __ BIND(L_exit);
4916     __ pshufb(xmm_temp6, xmm_temp10);          // Byte swap 16-byte result
4917     __ movdqu(Address(state, 0), xmm_temp6);   // store the result
4918     __ leave();
4919     __ ret(0);
4920     return start;
4921   }
4922 
4923   //base64 character set
4924   address base64_charset_addr() {
4925     __ align(CodeEntryAlignment);
4926     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;base64_charset&quot;);
4927     address start = __ pc();
4928     __ emit_data64(0x0000004200000041, relocInfo::none);
4929     __ emit_data64(0x0000004400000043, relocInfo::none);
4930     __ emit_data64(0x0000004600000045, relocInfo::none);
4931     __ emit_data64(0x0000004800000047, relocInfo::none);
4932     __ emit_data64(0x0000004a00000049, relocInfo::none);
4933     __ emit_data64(0x0000004c0000004b, relocInfo::none);
4934     __ emit_data64(0x0000004e0000004d, relocInfo::none);
4935     __ emit_data64(0x000000500000004f, relocInfo::none);
4936     __ emit_data64(0x0000005200000051, relocInfo::none);
4937     __ emit_data64(0x0000005400000053, relocInfo::none);
4938     __ emit_data64(0x0000005600000055, relocInfo::none);
4939     __ emit_data64(0x0000005800000057, relocInfo::none);
4940     __ emit_data64(0x0000005a00000059, relocInfo::none);
4941     __ emit_data64(0x0000006200000061, relocInfo::none);
4942     __ emit_data64(0x0000006400000063, relocInfo::none);
4943     __ emit_data64(0x0000006600000065, relocInfo::none);
4944     __ emit_data64(0x0000006800000067, relocInfo::none);
4945     __ emit_data64(0x0000006a00000069, relocInfo::none);
4946     __ emit_data64(0x0000006c0000006b, relocInfo::none);
4947     __ emit_data64(0x0000006e0000006d, relocInfo::none);
4948     __ emit_data64(0x000000700000006f, relocInfo::none);
4949     __ emit_data64(0x0000007200000071, relocInfo::none);
4950     __ emit_data64(0x0000007400000073, relocInfo::none);
4951     __ emit_data64(0x0000007600000075, relocInfo::none);
4952     __ emit_data64(0x0000007800000077, relocInfo::none);
4953     __ emit_data64(0x0000007a00000079, relocInfo::none);
4954     __ emit_data64(0x0000003100000030, relocInfo::none);
4955     __ emit_data64(0x0000003300000032, relocInfo::none);
4956     __ emit_data64(0x0000003500000034, relocInfo::none);
4957     __ emit_data64(0x0000003700000036, relocInfo::none);
4958     __ emit_data64(0x0000003900000038, relocInfo::none);
4959     __ emit_data64(0x0000002f0000002b, relocInfo::none);
4960     return start;
4961   }
4962 
4963   //base64 url character set
4964   address base64url_charset_addr() {
4965     __ align(CodeEntryAlignment);
4966     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;base64url_charset&quot;);
4967     address start = __ pc();
4968     __ emit_data64(0x0000004200000041, relocInfo::none);
4969     __ emit_data64(0x0000004400000043, relocInfo::none);
4970     __ emit_data64(0x0000004600000045, relocInfo::none);
4971     __ emit_data64(0x0000004800000047, relocInfo::none);
4972     __ emit_data64(0x0000004a00000049, relocInfo::none);
4973     __ emit_data64(0x0000004c0000004b, relocInfo::none);
4974     __ emit_data64(0x0000004e0000004d, relocInfo::none);
4975     __ emit_data64(0x000000500000004f, relocInfo::none);
4976     __ emit_data64(0x0000005200000051, relocInfo::none);
4977     __ emit_data64(0x0000005400000053, relocInfo::none);
4978     __ emit_data64(0x0000005600000055, relocInfo::none);
4979     __ emit_data64(0x0000005800000057, relocInfo::none);
4980     __ emit_data64(0x0000005a00000059, relocInfo::none);
4981     __ emit_data64(0x0000006200000061, relocInfo::none);
4982     __ emit_data64(0x0000006400000063, relocInfo::none);
4983     __ emit_data64(0x0000006600000065, relocInfo::none);
4984     __ emit_data64(0x0000006800000067, relocInfo::none);
4985     __ emit_data64(0x0000006a00000069, relocInfo::none);
4986     __ emit_data64(0x0000006c0000006b, relocInfo::none);
4987     __ emit_data64(0x0000006e0000006d, relocInfo::none);
4988     __ emit_data64(0x000000700000006f, relocInfo::none);
4989     __ emit_data64(0x0000007200000071, relocInfo::none);
4990     __ emit_data64(0x0000007400000073, relocInfo::none);
4991     __ emit_data64(0x0000007600000075, relocInfo::none);
4992     __ emit_data64(0x0000007800000077, relocInfo::none);
4993     __ emit_data64(0x0000007a00000079, relocInfo::none);
4994     __ emit_data64(0x0000003100000030, relocInfo::none);
4995     __ emit_data64(0x0000003300000032, relocInfo::none);
4996     __ emit_data64(0x0000003500000034, relocInfo::none);
4997     __ emit_data64(0x0000003700000036, relocInfo::none);
4998     __ emit_data64(0x0000003900000038, relocInfo::none);
4999     __ emit_data64(0x0000005f0000002d, relocInfo::none);
5000 
5001     return start;
5002   }
5003 
5004   address base64_bswap_mask_addr() {
5005     __ align(CodeEntryAlignment);
5006     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;bswap_mask_base64&quot;);
5007     address start = __ pc();
5008     __ emit_data64(0x0504038002010080, relocInfo::none);
5009     __ emit_data64(0x0b0a098008070680, relocInfo::none);
5010     __ emit_data64(0x0908078006050480, relocInfo::none);
5011     __ emit_data64(0x0f0e0d800c0b0a80, relocInfo::none);
5012     __ emit_data64(0x0605048003020180, relocInfo::none);
5013     __ emit_data64(0x0c0b0a8009080780, relocInfo::none);
5014     __ emit_data64(0x0504038002010080, relocInfo::none);
5015     __ emit_data64(0x0b0a098008070680, relocInfo::none);
5016 
5017     return start;
5018   }
5019 
5020   address base64_right_shift_mask_addr() {
5021     __ align(CodeEntryAlignment);
5022     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;right_shift_mask&quot;);
5023     address start = __ pc();
5024     __ emit_data64(0x0006000400020000, relocInfo::none);
5025     __ emit_data64(0x0006000400020000, relocInfo::none);
5026     __ emit_data64(0x0006000400020000, relocInfo::none);
5027     __ emit_data64(0x0006000400020000, relocInfo::none);
5028     __ emit_data64(0x0006000400020000, relocInfo::none);
5029     __ emit_data64(0x0006000400020000, relocInfo::none);
5030     __ emit_data64(0x0006000400020000, relocInfo::none);
5031     __ emit_data64(0x0006000400020000, relocInfo::none);
5032 
5033     return start;
5034   }
5035 
5036   address base64_left_shift_mask_addr() {
5037     __ align(CodeEntryAlignment);
5038     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;left_shift_mask&quot;);
5039     address start = __ pc();
5040     __ emit_data64(0x0000000200040000, relocInfo::none);
5041     __ emit_data64(0x0000000200040000, relocInfo::none);
5042     __ emit_data64(0x0000000200040000, relocInfo::none);
5043     __ emit_data64(0x0000000200040000, relocInfo::none);
5044     __ emit_data64(0x0000000200040000, relocInfo::none);
5045     __ emit_data64(0x0000000200040000, relocInfo::none);
5046     __ emit_data64(0x0000000200040000, relocInfo::none);
5047     __ emit_data64(0x0000000200040000, relocInfo::none);
5048 
5049     return start;
5050   }
5051 
5052   address base64_and_mask_addr() {
5053     __ align(CodeEntryAlignment);
5054     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;and_mask&quot;);
5055     address start = __ pc();
5056     __ emit_data64(0x3f003f003f000000, relocInfo::none);
5057     __ emit_data64(0x3f003f003f000000, relocInfo::none);
5058     __ emit_data64(0x3f003f003f000000, relocInfo::none);
5059     __ emit_data64(0x3f003f003f000000, relocInfo::none);
5060     __ emit_data64(0x3f003f003f000000, relocInfo::none);
5061     __ emit_data64(0x3f003f003f000000, relocInfo::none);
5062     __ emit_data64(0x3f003f003f000000, relocInfo::none);
5063     __ emit_data64(0x3f003f003f000000, relocInfo::none);
5064     return start;
5065   }
5066 
5067   address base64_gather_mask_addr() {
5068     __ align(CodeEntryAlignment);
5069     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;gather_mask&quot;);
5070     address start = __ pc();
5071     __ emit_data64(0xffffffffffffffff, relocInfo::none);
5072     return start;
5073   }
5074 
5075 // Code for generating Base64 encoding.
5076 // Intrinsic function prototype in Base64.java:
5077 // private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL) {
5078   address generate_base64_encodeBlock() {
5079     __ align(CodeEntryAlignment);
5080     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;implEncode&quot;);
5081     address start = __ pc();
5082     __ enter();
5083 
5084     // Save callee-saved registers before using them
5085     __ push(r12);
5086     __ push(r13);
5087     __ push(r14);
5088     __ push(r15);
5089 
5090     // arguments
5091     const Register source = c_rarg0; // Source Array
5092     const Register start_offset = c_rarg1; // start offset
5093     const Register end_offset = c_rarg2; // end offset
5094     const Register dest = c_rarg3; // destination array
5095 
5096 #ifndef _WIN64
5097     const Register dp = c_rarg4;  // Position for writing to dest array
5098     const Register isURL = c_rarg5;// Base64 or URL character set
5099 #else
5100     const Address  dp_mem(rbp, 6 * wordSize);  // length is on stack on Win64
5101     const Address isURL_mem(rbp, 7 * wordSize);
5102     const Register isURL = r10;      // pick the volatile windows register
5103     const Register dp = r12;
5104     __ movl(dp, dp_mem);
5105     __ movl(isURL, isURL_mem);
5106 #endif
5107 
5108     const Register length = r14;
5109     Label L_process80, L_process32, L_process3, L_exit, L_processdata;
5110 
5111     // calculate length from offsets
5112     __ movl(length, end_offset);
5113     __ subl(length, start_offset);
5114     __ cmpl(length, 0);
5115     __ jcc(Assembler::lessEqual, L_exit);
5116 
5117     __ lea(r11, ExternalAddress(StubRoutines::x86::base64_charset_addr()));
5118     // check if base64 charset(isURL=0) or base64 url charset(isURL=1) needs to be loaded
5119     __ cmpl(isURL, 0);
5120     __ jcc(Assembler::equal, L_processdata);
5121     __ lea(r11, ExternalAddress(StubRoutines::x86::base64url_charset_addr()));
5122 
5123     // load masks required for encoding data
5124     __ BIND(L_processdata);
5125     __ movdqu(xmm16, ExternalAddress(StubRoutines::x86::base64_gather_mask_addr()));
5126     // Set 64 bits of K register.
5127     __ evpcmpeqb(k3, xmm16, xmm16, Assembler::AVX_512bit);
5128     __ evmovdquq(xmm12, ExternalAddress(StubRoutines::x86::base64_bswap_mask_addr()), Assembler::AVX_256bit, r13);
5129     __ evmovdquq(xmm13, ExternalAddress(StubRoutines::x86::base64_right_shift_mask_addr()), Assembler::AVX_512bit, r13);
5130     __ evmovdquq(xmm14, ExternalAddress(StubRoutines::x86::base64_left_shift_mask_addr()), Assembler::AVX_512bit, r13);
5131     __ evmovdquq(xmm15, ExternalAddress(StubRoutines::x86::base64_and_mask_addr()), Assembler::AVX_512bit, r13);
5132 
5133     // Vector Base64 implementation, producing 96 bytes of encoded data
5134     __ BIND(L_process80);
5135     __ cmpl(length, 80);
5136     __ jcc(Assembler::below, L_process32);
5137     __ evmovdquq(xmm0, Address(source, start_offset, Address::times_1, 0), Assembler::AVX_256bit);
5138     __ evmovdquq(xmm1, Address(source, start_offset, Address::times_1, 24), Assembler::AVX_256bit);
5139     __ evmovdquq(xmm2, Address(source, start_offset, Address::times_1, 48), Assembler::AVX_256bit);
5140 
5141     //permute the input data in such a manner that we have continuity of the source
5142     __ vpermq(xmm3, xmm0, 148, Assembler::AVX_256bit);
5143     __ vpermq(xmm4, xmm1, 148, Assembler::AVX_256bit);
5144     __ vpermq(xmm5, xmm2, 148, Assembler::AVX_256bit);
5145 
5146     //shuffle input and group 3 bytes of data and to it add 0 as the 4th byte.
5147     //we can deal with 12 bytes at a time in a 128 bit register
5148     __ vpshufb(xmm3, xmm3, xmm12, Assembler::AVX_256bit);
5149     __ vpshufb(xmm4, xmm4, xmm12, Assembler::AVX_256bit);
5150     __ vpshufb(xmm5, xmm5, xmm12, Assembler::AVX_256bit);
5151 
5152     //convert byte to word. Each 128 bit register will have 6 bytes for processing
5153     __ vpmovzxbw(xmm3, xmm3, Assembler::AVX_512bit);
5154     __ vpmovzxbw(xmm4, xmm4, Assembler::AVX_512bit);
5155     __ vpmovzxbw(xmm5, xmm5, Assembler::AVX_512bit);
5156 
5157     // Extract bits in the following pattern 6, 4+2, 2+4, 6 to convert 3, 8 bit numbers to 4, 6 bit numbers
5158     __ evpsrlvw(xmm0, xmm3, xmm13,  Assembler::AVX_512bit);
5159     __ evpsrlvw(xmm1, xmm4, xmm13, Assembler::AVX_512bit);
5160     __ evpsrlvw(xmm2, xmm5, xmm13, Assembler::AVX_512bit);
5161 
5162     __ evpsllvw(xmm3, xmm3, xmm14, Assembler::AVX_512bit);
5163     __ evpsllvw(xmm4, xmm4, xmm14, Assembler::AVX_512bit);
5164     __ evpsllvw(xmm5, xmm5, xmm14, Assembler::AVX_512bit);
5165 
5166     __ vpsrlq(xmm0, xmm0, 8, Assembler::AVX_512bit);
5167     __ vpsrlq(xmm1, xmm1, 8, Assembler::AVX_512bit);
5168     __ vpsrlq(xmm2, xmm2, 8, Assembler::AVX_512bit);
5169 
5170     __ vpsllq(xmm3, xmm3, 8, Assembler::AVX_512bit);
5171     __ vpsllq(xmm4, xmm4, 8, Assembler::AVX_512bit);
5172     __ vpsllq(xmm5, xmm5, 8, Assembler::AVX_512bit);
5173 
5174     __ vpandq(xmm3, xmm3, xmm15, Assembler::AVX_512bit);
5175     __ vpandq(xmm4, xmm4, xmm15, Assembler::AVX_512bit);
5176     __ vpandq(xmm5, xmm5, xmm15, Assembler::AVX_512bit);
5177 
5178     // Get the final 4*6 bits base64 encoding
5179     __ vporq(xmm3, xmm3, xmm0, Assembler::AVX_512bit);
5180     __ vporq(xmm4, xmm4, xmm1, Assembler::AVX_512bit);
5181     __ vporq(xmm5, xmm5, xmm2, Assembler::AVX_512bit);
5182 
5183     // Shift
5184     __ vpsrlq(xmm3, xmm3, 8, Assembler::AVX_512bit);
5185     __ vpsrlq(xmm4, xmm4, 8, Assembler::AVX_512bit);
5186     __ vpsrlq(xmm5, xmm5, 8, Assembler::AVX_512bit);
5187 
5188     // look up 6 bits in the base64 character set to fetch the encoding
5189     // we are converting word to dword as gather instructions need dword indices for looking up encoding
5190     __ vextracti64x4(xmm6, xmm3, 0);
5191     __ vpmovzxwd(xmm0, xmm6, Assembler::AVX_512bit);
5192     __ vextracti64x4(xmm6, xmm3, 1);
5193     __ vpmovzxwd(xmm1, xmm6, Assembler::AVX_512bit);
5194 
5195     __ vextracti64x4(xmm6, xmm4, 0);
5196     __ vpmovzxwd(xmm2, xmm6, Assembler::AVX_512bit);
5197     __ vextracti64x4(xmm6, xmm4, 1);
5198     __ vpmovzxwd(xmm3, xmm6, Assembler::AVX_512bit);
5199 
5200     __ vextracti64x4(xmm4, xmm5, 0);
5201     __ vpmovzxwd(xmm6, xmm4, Assembler::AVX_512bit);
5202 
5203     __ vextracti64x4(xmm4, xmm5, 1);
5204     __ vpmovzxwd(xmm7, xmm4, Assembler::AVX_512bit);
5205 
5206     __ kmovql(k2, k3);
5207     __ evpgatherdd(xmm4, k2, Address(r11, xmm0, Address::times_4, 0), Assembler::AVX_512bit);
5208     __ kmovql(k2, k3);
5209     __ evpgatherdd(xmm5, k2, Address(r11, xmm1, Address::times_4, 0), Assembler::AVX_512bit);
5210     __ kmovql(k2, k3);
5211     __ evpgatherdd(xmm8, k2, Address(r11, xmm2, Address::times_4, 0), Assembler::AVX_512bit);
5212     __ kmovql(k2, k3);
5213     __ evpgatherdd(xmm9, k2, Address(r11, xmm3, Address::times_4, 0), Assembler::AVX_512bit);
5214     __ kmovql(k2, k3);
5215     __ evpgatherdd(xmm10, k2, Address(r11, xmm6, Address::times_4, 0), Assembler::AVX_512bit);
5216     __ kmovql(k2, k3);
5217     __ evpgatherdd(xmm11, k2, Address(r11, xmm7, Address::times_4, 0), Assembler::AVX_512bit);
5218 
5219     //Down convert dword to byte. Final output is 16*6 = 96 bytes long
5220     __ evpmovdb(Address(dest, dp, Address::times_1, 0), xmm4, Assembler::AVX_512bit);
5221     __ evpmovdb(Address(dest, dp, Address::times_1, 16), xmm5, Assembler::AVX_512bit);
5222     __ evpmovdb(Address(dest, dp, Address::times_1, 32), xmm8, Assembler::AVX_512bit);
5223     __ evpmovdb(Address(dest, dp, Address::times_1, 48), xmm9, Assembler::AVX_512bit);
5224     __ evpmovdb(Address(dest, dp, Address::times_1, 64), xmm10, Assembler::AVX_512bit);
5225     __ evpmovdb(Address(dest, dp, Address::times_1, 80), xmm11, Assembler::AVX_512bit);
5226 
5227     __ addq(dest, 96);
5228     __ addq(source, 72);
5229     __ subq(length, 72);
5230     __ jmp(L_process80);
5231 
5232     // Vector Base64 implementation generating 32 bytes of encoded data
5233     __ BIND(L_process32);
5234     __ cmpl(length, 32);
5235     __ jcc(Assembler::below, L_process3);
5236     __ evmovdquq(xmm0, Address(source, start_offset), Assembler::AVX_256bit);
5237     __ vpermq(xmm0, xmm0, 148, Assembler::AVX_256bit);
5238     __ vpshufb(xmm6, xmm0, xmm12, Assembler::AVX_256bit);
5239     __ vpmovzxbw(xmm6, xmm6, Assembler::AVX_512bit);
5240     __ evpsrlvw(xmm2, xmm6, xmm13, Assembler::AVX_512bit);
5241     __ evpsllvw(xmm3, xmm6, xmm14, Assembler::AVX_512bit);
5242 
5243     __ vpsrlq(xmm2, xmm2, 8, Assembler::AVX_512bit);
5244     __ vpsllq(xmm3, xmm3, 8, Assembler::AVX_512bit);
5245     __ vpandq(xmm3, xmm3, xmm15, Assembler::AVX_512bit);
5246     __ vporq(xmm1, xmm2, xmm3, Assembler::AVX_512bit);
5247     __ vpsrlq(xmm1, xmm1, 8, Assembler::AVX_512bit);
5248     __ vextracti64x4(xmm9, xmm1, 0);
5249     __ vpmovzxwd(xmm6, xmm9, Assembler::AVX_512bit);
5250     __ vextracti64x4(xmm9, xmm1, 1);
5251     __ vpmovzxwd(xmm5, xmm9,  Assembler::AVX_512bit);
5252     __ kmovql(k2, k3);
5253     __ evpgatherdd(xmm8, k2, Address(r11, xmm6, Address::times_4, 0), Assembler::AVX_512bit);
5254     __ kmovql(k2, k3);
5255     __ evpgatherdd(xmm10, k2, Address(r11, xmm5, Address::times_4, 0), Assembler::AVX_512bit);
5256     __ evpmovdb(Address(dest, dp, Address::times_1, 0), xmm8, Assembler::AVX_512bit);
5257     __ evpmovdb(Address(dest, dp, Address::times_1, 16), xmm10, Assembler::AVX_512bit);
5258     __ subq(length, 24);
5259     __ addq(dest, 32);
5260     __ addq(source, 24);
5261     __ jmp(L_process32);
5262 
5263     // Scalar data processing takes 3 bytes at a time and produces 4 bytes of encoded data
5264     /* This code corresponds to the scalar version of the following snippet in Base64.java
5265     ** int bits = (src[sp0++] &amp; 0xff) &lt;&lt; 16 |(src[sp0++] &amp; 0xff) &lt;&lt; 8 |(src[sp0++] &amp; 0xff);
5266     ** dst[dp0++] = (byte)base64[(bits &gt;&gt; &gt; 18) &amp; 0x3f];
5267     ** dst[dp0++] = (byte)base64[(bits &gt;&gt; &gt; 12) &amp; 0x3f];
5268     ** dst[dp0++] = (byte)base64[(bits &gt;&gt; &gt; 6) &amp; 0x3f];
5269     ** dst[dp0++] = (byte)base64[bits &amp; 0x3f];*/
5270     __ BIND(L_process3);
5271     __ cmpl(length, 3);
5272     __ jcc(Assembler::below, L_exit);
5273     // Read 1 byte at a time
5274     __ movzbl(rax, Address(source, start_offset));
5275     __ shll(rax, 0x10);
5276     __ movl(r15, rax);
5277     __ movzbl(rax, Address(source, start_offset, Address::times_1, 1));
5278     __ shll(rax, 0x8);
5279     __ movzwl(rax, rax);
5280     __ orl(r15, rax);
5281     __ movzbl(rax, Address(source, start_offset, Address::times_1, 2));
5282     __ orl(rax, r15);
5283     // Save 3 bytes read in r15
5284     __ movl(r15, rax);
5285     __ shrl(rax, 0x12);
5286     __ andl(rax, 0x3f);
5287     // rax contains the index, r11 contains base64 lookup table
5288     __ movb(rax, Address(r11, rax, Address::times_4));
5289     // Write the encoded byte to destination
5290     __ movb(Address(dest, dp, Address::times_1, 0), rax);
5291     __ movl(rax, r15);
5292     __ shrl(rax, 0xc);
5293     __ andl(rax, 0x3f);
5294     __ movb(rax, Address(r11, rax, Address::times_4));
5295     __ movb(Address(dest, dp, Address::times_1, 1), rax);
5296     __ movl(rax, r15);
5297     __ shrl(rax, 0x6);
5298     __ andl(rax, 0x3f);
5299     __ movb(rax, Address(r11, rax, Address::times_4));
5300     __ movb(Address(dest, dp, Address::times_1, 2), rax);
5301     __ movl(rax, r15);
5302     __ andl(rax, 0x3f);
5303     __ movb(rax, Address(r11, rax, Address::times_4));
5304     __ movb(Address(dest, dp, Address::times_1, 3), rax);
5305     __ subl(length, 3);
5306     __ addq(dest, 4);
5307     __ addq(source, 3);
5308     __ jmp(L_process3);
5309     __ BIND(L_exit);
5310     __ pop(r15);
5311     __ pop(r14);
5312     __ pop(r13);
5313     __ pop(r12);
5314     __ leave();
5315     __ ret(0);
5316     return start;
5317   }
5318 
5319   /**
5320    *  Arguments:
5321    *
5322    * Inputs:
5323    *   c_rarg0   - int crc
5324    *   c_rarg1   - byte* buf
5325    *   c_rarg2   - int length
5326    *
5327    * Ouput:
5328    *       rax   - int crc result
5329    */
5330   address generate_updateBytesCRC32() {
5331     assert(UseCRC32Intrinsics, &quot;need AVX and CLMUL instructions&quot;);
5332 
5333     __ align(CodeEntryAlignment);
5334     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesCRC32&quot;);
5335 
5336     address start = __ pc();
5337     // Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)
5338     // Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)
5339     // rscratch1: r10
5340     const Register crc   = c_rarg0;  // crc
5341     const Register buf   = c_rarg1;  // source java byte array address
5342     const Register len   = c_rarg2;  // length
5343     const Register table = c_rarg3;  // crc_table address (reuse register)
5344     const Register tmp   = r11;
5345     assert_different_registers(crc, buf, len, table, tmp, rax);
5346 
5347     BLOCK_COMMENT(&quot;Entry:&quot;);
5348     __ enter(); // required for proper stackwalking of RuntimeStub frame
5349 
5350     __ kernel_crc32(crc, buf, len, table, tmp);
5351 
5352     __ movl(rax, crc);
5353     __ vzeroupper();
5354     __ leave(); // required for proper stackwalking of RuntimeStub frame
5355     __ ret(0);
5356 
5357     return start;
5358   }
5359 
5360   /**
5361   *  Arguments:
5362   *
5363   * Inputs:
5364   *   c_rarg0   - int crc
5365   *   c_rarg1   - byte* buf
5366   *   c_rarg2   - long length
5367   *   c_rarg3   - table_start - optional (present only when doing a library_call,
5368   *              not used by x86 algorithm)
5369   *
5370   * Ouput:
5371   *       rax   - int crc result
5372   */
5373   address generate_updateBytesCRC32C(bool is_pclmulqdq_supported) {
5374       assert(UseCRC32CIntrinsics, &quot;need SSE4_2&quot;);
5375       __ align(CodeEntryAlignment);
5376       StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesCRC32C&quot;);
5377       address start = __ pc();
5378       //reg.arg        int#0        int#1        int#2        int#3        int#4        int#5        float regs
5379       //Windows        RCX          RDX          R8           R9           none         none         XMM0..XMM3
5380       //Lin / Sol      RDI          RSI          RDX          RCX          R8           R9           XMM0..XMM7
5381       const Register crc = c_rarg0;  // crc
5382       const Register buf = c_rarg1;  // source java byte array address
5383       const Register len = c_rarg2;  // length
5384       const Register a = rax;
5385       const Register j = r9;
5386       const Register k = r10;
5387       const Register l = r11;
5388 #ifdef _WIN64
5389       const Register y = rdi;
5390       const Register z = rsi;
5391 #else
5392       const Register y = rcx;
5393       const Register z = r8;
5394 #endif
5395       assert_different_registers(crc, buf, len, a, j, k, l, y, z);
5396 
5397       BLOCK_COMMENT(&quot;Entry:&quot;);
5398       __ enter(); // required for proper stackwalking of RuntimeStub frame
5399 #ifdef _WIN64
5400       __ push(y);
5401       __ push(z);
5402 #endif
5403       __ crc32c_ipl_alg2_alt2(crc, buf, len,
5404                               a, j, k,
5405                               l, y, z,
5406                               c_farg0, c_farg1, c_farg2,
5407                               is_pclmulqdq_supported);
5408       __ movl(rax, crc);
5409 #ifdef _WIN64
5410       __ pop(z);
5411       __ pop(y);
5412 #endif
5413       __ vzeroupper();
5414       __ leave(); // required for proper stackwalking of RuntimeStub frame
5415       __ ret(0);
5416 
5417       return start;
5418   }
5419 
5420   /**
5421    *  Arguments:
5422    *
5423    *  Input:
5424    *    c_rarg0   - x address
5425    *    c_rarg1   - x length
5426    *    c_rarg2   - y address
5427    *    c_rarg3   - y length
5428    * not Win64
5429    *    c_rarg4   - z address
5430    *    c_rarg5   - z length
5431    * Win64
5432    *    rsp+40    - z address
5433    *    rsp+48    - z length
5434    */
5435   address generate_multiplyToLen() {
5436     __ align(CodeEntryAlignment);
5437     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;multiplyToLen&quot;);
5438 
5439     address start = __ pc();
5440     // Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)
5441     // Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)
5442     const Register x     = rdi;
5443     const Register xlen  = rax;
5444     const Register y     = rsi;
5445     const Register ylen  = rcx;
5446     const Register z     = r8;
5447     const Register zlen  = r11;
5448 
5449     // Next registers will be saved on stack in multiply_to_len().
5450     const Register tmp1  = r12;
5451     const Register tmp2  = r13;
5452     const Register tmp3  = r14;
5453     const Register tmp4  = r15;
5454     const Register tmp5  = rbx;
5455 
5456     BLOCK_COMMENT(&quot;Entry:&quot;);
5457     __ enter(); // required for proper stackwalking of RuntimeStub frame
5458 
5459 #ifndef _WIN64
5460     __ movptr(zlen, r9); // Save r9 in r11 - zlen
5461 #endif
5462     setup_arg_regs(4); // x =&gt; rdi, xlen =&gt; rsi, y =&gt; rdx
5463                        // ylen =&gt; rcx, z =&gt; r8, zlen =&gt; r11
5464                        // r9 and r10 may be used to save non-volatile registers
5465 #ifdef _WIN64
5466     // last 2 arguments (#4, #5) are on stack on Win64
5467     __ movptr(z, Address(rsp, 6 * wordSize));
5468     __ movptr(zlen, Address(rsp, 7 * wordSize));
5469 #endif
5470 
5471     __ movptr(xlen, rsi);
5472     __ movptr(y,    rdx);
5473     __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5);
5474 
5475     restore_arg_regs();
5476 
5477     __ leave(); // required for proper stackwalking of RuntimeStub frame
5478     __ ret(0);
5479 
5480     return start;
5481   }
5482 
5483   /**
5484   *  Arguments:
5485   *
5486   *  Input:
5487   *    c_rarg0   - obja     address
5488   *    c_rarg1   - objb     address
5489   *    c_rarg3   - length   length
5490   *    c_rarg4   - scale    log2_array_indxscale
5491   *
5492   *  Output:
5493   *        rax   - int &gt;= mismatched index, &lt; 0 bitwise complement of tail
5494   */
5495   address generate_vectorizedMismatch() {
5496     __ align(CodeEntryAlignment);
5497     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;vectorizedMismatch&quot;);
5498     address start = __ pc();
5499 
5500     BLOCK_COMMENT(&quot;Entry:&quot;);
5501     __ enter();
5502 
5503 #ifdef _WIN64  // Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)
5504     const Register scale = c_rarg0;  //rcx, will exchange with r9
5505     const Register objb = c_rarg1;   //rdx
5506     const Register length = c_rarg2; //r8
5507     const Register obja = c_rarg3;   //r9
5508     __ xchgq(obja, scale);  //now obja and scale contains the correct contents
5509 
5510     const Register tmp1 = r10;
5511     const Register tmp2 = r11;
5512 #endif
5513 #ifndef _WIN64 // Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)
5514     const Register obja = c_rarg0;   //U:rdi
5515     const Register objb = c_rarg1;   //U:rsi
5516     const Register length = c_rarg2; //U:rdx
5517     const Register scale = c_rarg3;  //U:rcx
5518     const Register tmp1 = r8;
5519     const Register tmp2 = r9;
5520 #endif
5521     const Register result = rax; //return value
5522     const XMMRegister vec0 = xmm0;
5523     const XMMRegister vec1 = xmm1;
5524     const XMMRegister vec2 = xmm2;
5525 
5526     __ vectorized_mismatch(obja, objb, length, scale, result, tmp1, tmp2, vec0, vec1, vec2);
5527 
5528     __ vzeroupper();
5529     __ leave();
5530     __ ret(0);
5531 
5532     return start;
5533   }
5534 
5535 /**
5536    *  Arguments:
5537    *
5538   //  Input:
5539   //    c_rarg0   - x address
5540   //    c_rarg1   - x length
5541   //    c_rarg2   - z address
5542   //    c_rarg3   - z lenth
5543    *
5544    */
5545   address generate_squareToLen() {
5546 
5547     __ align(CodeEntryAlignment);
5548     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;squareToLen&quot;);
5549 
5550     address start = __ pc();
5551     // Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)
5552     // Unix:  rdi, rsi, rdx, rcx (c_rarg0, c_rarg1, ...)
5553     const Register x      = rdi;
5554     const Register len    = rsi;
5555     const Register z      = r8;
5556     const Register zlen   = rcx;
5557 
5558    const Register tmp1      = r12;
5559    const Register tmp2      = r13;
5560    const Register tmp3      = r14;
5561    const Register tmp4      = r15;
5562    const Register tmp5      = rbx;
5563 
5564     BLOCK_COMMENT(&quot;Entry:&quot;);
5565     __ enter(); // required for proper stackwalking of RuntimeStub frame
5566 
5567     setup_arg_regs(4); // x =&gt; rdi, len =&gt; rsi, z =&gt; rdx
5568                        // zlen =&gt; rcx
5569                        // r9 and r10 may be used to save non-volatile registers
5570     __ movptr(r8, rdx);
5571     __ square_to_len(x, len, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);
5572 
5573     restore_arg_regs();
5574 
5575     __ leave(); // required for proper stackwalking of RuntimeStub frame
5576     __ ret(0);
5577 
5578     return start;
5579   }
5580 
5581   address generate_method_entry_barrier() {
5582     __ align(CodeEntryAlignment);
5583     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;nmethod_entry_barrier&quot;);
5584 
5585     Label deoptimize_label;
5586 
5587     address start = __ pc();
5588 
5589     __ push(-1); // cookie, this is used for writing the new rsp when deoptimizing
5590 
5591     BLOCK_COMMENT(&quot;Entry:&quot;);
5592     __ enter(); // save rbp
5593 
5594     // save c_rarg0, because we want to use that value.
5595     // We could do without it but then we depend on the number of slots used by pusha
5596     __ push(c_rarg0);
5597 
5598     __ lea(c_rarg0, Address(rsp, wordSize * 3)); // 1 for cookie, 1 for rbp, 1 for c_rarg0 - this should be the return address
5599 
5600     __ pusha();
5601 
5602     // The method may have floats as arguments, and we must spill them before calling
5603     // the VM runtime.
5604     assert(Argument::n_float_register_parameters_j == 8, &quot;Assumption&quot;);
5605     const int xmm_size = wordSize * 2;
5606     const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;
5607     __ subptr(rsp, xmm_spill_size);
5608     __ movdqu(Address(rsp, xmm_size * 7), xmm7);
5609     __ movdqu(Address(rsp, xmm_size * 6), xmm6);
5610     __ movdqu(Address(rsp, xmm_size * 5), xmm5);
5611     __ movdqu(Address(rsp, xmm_size * 4), xmm4);
5612     __ movdqu(Address(rsp, xmm_size * 3), xmm3);
5613     __ movdqu(Address(rsp, xmm_size * 2), xmm2);
5614     __ movdqu(Address(rsp, xmm_size * 1), xmm1);
5615     __ movdqu(Address(rsp, xmm_size * 0), xmm0);
5616 
5617     __ call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast&lt;int (*)(address*)&gt;(BarrierSetNMethod::nmethod_stub_entry_barrier)), 1);
5618 
5619     __ movdqu(xmm0, Address(rsp, xmm_size * 0));
5620     __ movdqu(xmm1, Address(rsp, xmm_size * 1));
5621     __ movdqu(xmm2, Address(rsp, xmm_size * 2));
5622     __ movdqu(xmm3, Address(rsp, xmm_size * 3));
5623     __ movdqu(xmm4, Address(rsp, xmm_size * 4));
5624     __ movdqu(xmm5, Address(rsp, xmm_size * 5));
5625     __ movdqu(xmm6, Address(rsp, xmm_size * 6));
5626     __ movdqu(xmm7, Address(rsp, xmm_size * 7));
5627     __ addptr(rsp, xmm_spill_size);
5628 
5629     __ cmpl(rax, 1); // 1 means deoptimize
5630     __ jcc(Assembler::equal, deoptimize_label);
5631 
5632     __ popa();
5633     __ pop(c_rarg0);
5634 
5635     __ leave();
5636 
5637     __ addptr(rsp, 1 * wordSize); // cookie
5638     __ ret(0);
5639 
5640 
5641     __ BIND(deoptimize_label);
5642 
5643     __ popa();
5644     __ pop(c_rarg0);
5645 
5646     __ leave();
5647 
5648     // this can be taken out, but is good for verification purposes. getting a SIGSEGV
5649     // here while still having a correct stack is valuable
5650     __ testptr(rsp, Address(rsp, 0));
5651 
5652     __ movptr(rsp, Address(rsp, 0)); // new rsp was written in the barrier
5653     __ jmp(Address(rsp, -1 * wordSize)); // jmp target should be callers verified_entry_point
5654 
5655     return start;
5656   }
5657 
5658    /**
5659    *  Arguments:
5660    *
5661    *  Input:
5662    *    c_rarg0   - out address
5663    *    c_rarg1   - in address
5664    *    c_rarg2   - offset
5665    *    c_rarg3   - len
5666    * not Win64
5667    *    c_rarg4   - k
5668    * Win64
5669    *    rsp+40    - k
5670    */
5671   address generate_mulAdd() {
5672     __ align(CodeEntryAlignment);
5673     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;mulAdd&quot;);
5674 
5675     address start = __ pc();
5676     // Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)
5677     // Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)
5678     const Register out     = rdi;
5679     const Register in      = rsi;
5680     const Register offset  = r11;
5681     const Register len     = rcx;
5682     const Register k       = r8;
5683 
5684     // Next registers will be saved on stack in mul_add().
5685     const Register tmp1  = r12;
5686     const Register tmp2  = r13;
5687     const Register tmp3  = r14;
5688     const Register tmp4  = r15;
5689     const Register tmp5  = rbx;
5690 
5691     BLOCK_COMMENT(&quot;Entry:&quot;);
5692     __ enter(); // required for proper stackwalking of RuntimeStub frame
5693 
5694     setup_arg_regs(4); // out =&gt; rdi, in =&gt; rsi, offset =&gt; rdx
5695                        // len =&gt; rcx, k =&gt; r8
5696                        // r9 and r10 may be used to save non-volatile registers
5697 #ifdef _WIN64
5698     // last argument is on stack on Win64
5699     __ movl(k, Address(rsp, 6 * wordSize));
5700 #endif
5701     __ movptr(r11, rdx);  // move offset in rdx to offset(r11)
5702     __ mul_add(out, in, offset, len, k, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);
5703 
5704     restore_arg_regs();
5705 
5706     __ leave(); // required for proper stackwalking of RuntimeStub frame
5707     __ ret(0);
5708 
5709     return start;
5710   }
5711 
5712   address generate_bigIntegerRightShift() {
5713     __ align(CodeEntryAlignment);
5714     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;bigIntegerRightShiftWorker&quot;);
5715 
5716     address start = __ pc();
5717     Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;
5718     // For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.
5719     const Register newArr = rdi;
5720     const Register oldArr = rsi;
5721     const Register newIdx = rdx;
5722     const Register shiftCount = rcx;  // It was intentional to have shiftCount in rcx since it is used implicitly for shift.
5723     const Register totalNumIter = r8;
5724 
5725     // For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.
5726     // For everything else, we prefer using r9 and r10 since we do not have to save them before use.
5727     const Register tmp1 = r11;                    // Caller save.
5728     const Register tmp2 = rax;                    // Caller save.
5729     const Register tmp3 = WINDOWS_ONLY(r12) NOT_WINDOWS(r9);   // Windows: Callee save. Linux: Caller save.
5730     const Register tmp4 = WINDOWS_ONLY(r13) NOT_WINDOWS(r10);  // Windows: Callee save. Linux: Caller save.
5731     const Register tmp5 = r14;                    // Callee save.
5732     const Register tmp6 = r15;
5733 
5734     const XMMRegister x0 = xmm0;
5735     const XMMRegister x1 = xmm1;
5736     const XMMRegister x2 = xmm2;
5737 
5738     BLOCK_COMMENT(&quot;Entry:&quot;);
5739     __ enter(); // required for proper stackwalking of RuntimeStub frame
5740 
5741 #ifdef _WINDOWS
5742     setup_arg_regs(4);
5743     // For windows, since last argument is on stack, we need to move it to the appropriate register.
5744     __ movl(totalNumIter, Address(rsp, 6 * wordSize));
5745     // Save callee save registers.
5746     __ push(tmp3);
5747     __ push(tmp4);
5748 #endif
5749     __ push(tmp5);
5750 
5751     // Rename temps used throughout the code.
5752     const Register idx = tmp1;
5753     const Register nIdx = tmp2;
5754 
5755     __ xorl(idx, idx);
5756 
5757     // Start right shift from end of the array.
5758     // For example, if #iteration = 4 and newIdx = 1
5759     // then dest[4] = src[4] &gt;&gt; shiftCount  | src[3] &lt;&lt;&lt; (shiftCount - 32)
5760     // if #iteration = 4 and newIdx = 0
5761     // then dest[3] = src[4] &gt;&gt; shiftCount  | src[3] &lt;&lt;&lt; (shiftCount - 32)
5762     __ movl(idx, totalNumIter);
5763     __ movl(nIdx, idx);
5764     __ addl(nIdx, newIdx);
5765 
5766     // If vectorization is enabled, check if the number of iterations is at least 64
5767     // If not, then go to ShifTwo processing 2 iterations
5768     if (VM_Version::supports_avx512_vbmi2()) {
5769       __ cmpptr(totalNumIter, (AVX3Threshold/64));
5770       __ jcc(Assembler::less, ShiftTwo);
5771 
5772       if (AVX3Threshold &lt; 16 * 64) {
5773         __ cmpl(totalNumIter, 16);
5774         __ jcc(Assembler::less, ShiftTwo);
5775       }
5776       __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);
5777       __ subl(idx, 16);
5778       __ subl(nIdx, 16);
5779       __ BIND(Shift512Loop);
5780       __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 4), Assembler::AVX_512bit);
5781       __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);
5782       __ vpshrdvd(x2, x1, x0, Assembler::AVX_512bit);
5783       __ evmovdqul(Address(newArr, nIdx, Address::times_4), x2, Assembler::AVX_512bit);
5784       __ subl(nIdx, 16);
5785       __ subl(idx, 16);
5786       __ jcc(Assembler::greaterEqual, Shift512Loop);
5787       __ addl(idx, 16);
5788       __ addl(nIdx, 16);
5789     }
5790     __ BIND(ShiftTwo);
5791     __ cmpl(idx, 2);
5792     __ jcc(Assembler::less, ShiftOne);
5793     __ subl(idx, 2);
5794     __ subl(nIdx, 2);
5795     __ BIND(ShiftTwoLoop);
5796     __ movl(tmp5, Address(oldArr, idx, Address::times_4, 8));
5797     __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));
5798     __ movl(tmp3, Address(oldArr, idx, Address::times_4));
5799     __ shrdl(tmp5, tmp4);
5800     __ shrdl(tmp4, tmp3);
5801     __ movl(Address(newArr, nIdx, Address::times_4, 4), tmp5);
5802     __ movl(Address(newArr, nIdx, Address::times_4), tmp4);
5803     __ subl(nIdx, 2);
5804     __ subl(idx, 2);
5805     __ jcc(Assembler::greaterEqual, ShiftTwoLoop);
5806     __ addl(idx, 2);
5807     __ addl(nIdx, 2);
5808 
5809     // Do the last iteration
5810     __ BIND(ShiftOne);
5811     __ cmpl(idx, 1);
5812     __ jcc(Assembler::less, Exit);
5813     __ subl(idx, 1);
5814     __ subl(nIdx, 1);
5815     __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));
5816     __ movl(tmp3, Address(oldArr, idx, Address::times_4));
5817     __ shrdl(tmp4, tmp3);
5818     __ movl(Address(newArr, nIdx, Address::times_4), tmp4);
5819     __ BIND(Exit);
5820     // Restore callee save registers.
5821     __ pop(tmp5);
5822 #ifdef _WINDOWS
5823     __ pop(tmp4);
5824     __ pop(tmp3);
5825     restore_arg_regs();
5826 #endif
5827     __ leave(); // required for proper stackwalking of RuntimeStub frame
5828     __ ret(0);
5829     return start;
5830   }
5831 
5832    /**
5833    *  Arguments:
5834    *
5835    *  Input:
5836    *    c_rarg0   - newArr address
5837    *    c_rarg1   - oldArr address
5838    *    c_rarg2   - newIdx
5839    *    c_rarg3   - shiftCount
5840    * not Win64
5841    *    c_rarg4   - numIter
5842    * Win64
5843    *    rsp40    - numIter
5844    */
5845   address generate_bigIntegerLeftShift() {
5846     __ align(CodeEntryAlignment);
5847     StubCodeMark mark(this,  &quot;StubRoutines&quot;, &quot;bigIntegerLeftShiftWorker&quot;);
5848     address start = __ pc();
5849     Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;
5850     // For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.
5851     const Register newArr = rdi;
5852     const Register oldArr = rsi;
5853     const Register newIdx = rdx;
5854     const Register shiftCount = rcx;  // It was intentional to have shiftCount in rcx since it is used implicitly for shift.
5855     const Register totalNumIter = r8;
5856     // For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.
5857     // For everything else, we prefer using r9 and r10 since we do not have to save them before use.
5858     const Register tmp1 = r11;                    // Caller save.
5859     const Register tmp2 = rax;                    // Caller save.
5860     const Register tmp3 = WINDOWS_ONLY(r12) NOT_WINDOWS(r9);   // Windows: Callee save. Linux: Caller save.
5861     const Register tmp4 = WINDOWS_ONLY(r13) NOT_WINDOWS(r10);  // Windows: Callee save. Linux: Caller save.
5862     const Register tmp5 = r14;                    // Callee save.
5863 
5864     const XMMRegister x0 = xmm0;
5865     const XMMRegister x1 = xmm1;
5866     const XMMRegister x2 = xmm2;
5867     BLOCK_COMMENT(&quot;Entry:&quot;);
5868     __ enter(); // required for proper stackwalking of RuntimeStub frame
5869 
5870 #ifdef _WINDOWS
5871     setup_arg_regs(4);
5872     // For windows, since last argument is on stack, we need to move it to the appropriate register.
5873     __ movl(totalNumIter, Address(rsp, 6 * wordSize));
5874     // Save callee save registers.
5875     __ push(tmp3);
5876     __ push(tmp4);
5877 #endif
5878     __ push(tmp5);
5879 
5880     // Rename temps used throughout the code
5881     const Register idx = tmp1;
5882     const Register numIterTmp = tmp2;
5883 
5884     // Start idx from zero.
5885     __ xorl(idx, idx);
5886     // Compute interior pointer for new array. We do this so that we can use same index for both old and new arrays.
5887     __ lea(newArr, Address(newArr, newIdx, Address::times_4));
5888     __ movl(numIterTmp, totalNumIter);
5889 
5890     // If vectorization is enabled, check if the number of iterations is at least 64
5891     // If not, then go to ShiftTwo shifting two numbers at a time
5892     if (VM_Version::supports_avx512_vbmi2()) {
5893       __ cmpl(totalNumIter, (AVX3Threshold/64));
5894       __ jcc(Assembler::less, ShiftTwo);
5895 
5896       if (AVX3Threshold &lt; 16 * 64) {
5897         __ cmpl(totalNumIter, 16);
5898         __ jcc(Assembler::less, ShiftTwo);
5899       }
5900       __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);
5901       __ subl(numIterTmp, 16);
5902       __ BIND(Shift512Loop);
5903       __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);
5904       __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 0x4), Assembler::AVX_512bit);
5905       __ vpshldvd(x1, x2, x0, Assembler::AVX_512bit);
5906       __ evmovdqul(Address(newArr, idx, Address::times_4), x1, Assembler::AVX_512bit);
5907       __ addl(idx, 16);
5908       __ subl(numIterTmp, 16);
5909       __ jcc(Assembler::greaterEqual, Shift512Loop);
5910       __ addl(numIterTmp, 16);
5911     }
5912     __ BIND(ShiftTwo);
5913     __ cmpl(totalNumIter, 1);
5914     __ jcc(Assembler::less, Exit);
5915     __ movl(tmp3, Address(oldArr, idx, Address::times_4));
5916     __ subl(numIterTmp, 2);
5917     __ jcc(Assembler::less, ShiftOne);
5918 
5919     __ BIND(ShiftTwoLoop);
5920     __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));
5921     __ movl(tmp5, Address(oldArr, idx, Address::times_4, 0x8));
5922     __ shldl(tmp3, tmp4);
5923     __ shldl(tmp4, tmp5);
5924     __ movl(Address(newArr, idx, Address::times_4), tmp3);
5925     __ movl(Address(newArr, idx, Address::times_4, 0x4), tmp4);
5926     __ movl(tmp3, tmp5);
5927     __ addl(idx, 2);
5928     __ subl(numIterTmp, 2);
5929     __ jcc(Assembler::greaterEqual, ShiftTwoLoop);
5930 
5931     // Do the last iteration
5932     __ BIND(ShiftOne);
5933     __ addl(numIterTmp, 2);
5934     __ cmpl(numIterTmp, 1);
5935     __ jcc(Assembler::less, Exit);
5936     __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));
5937     __ shldl(tmp3, tmp4);
5938     __ movl(Address(newArr, idx, Address::times_4), tmp3);
5939 
5940     __ BIND(Exit);
5941     // Restore callee save registers.
5942     __ pop(tmp5);
5943 #ifdef _WINDOWS
5944     __ pop(tmp4);
5945     __ pop(tmp3);
5946     restore_arg_regs();
5947 #endif
5948     __ leave(); // required for proper stackwalking of RuntimeStub frame
5949     __ ret(0);
5950     return start;
5951   }
5952 
5953   address generate_libmExp() {
5954     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;libmExp&quot;);
5955 
5956     address start = __ pc();
5957 
5958     const XMMRegister x0  = xmm0;
5959     const XMMRegister x1  = xmm1;
5960     const XMMRegister x2  = xmm2;
5961     const XMMRegister x3  = xmm3;
5962 
5963     const XMMRegister x4  = xmm4;
5964     const XMMRegister x5  = xmm5;
5965     const XMMRegister x6  = xmm6;
5966     const XMMRegister x7  = xmm7;
5967 
5968     const Register tmp   = r11;
5969 
5970     BLOCK_COMMENT(&quot;Entry:&quot;);
5971     __ enter(); // required for proper stackwalking of RuntimeStub frame
5972 
5973     __ fast_exp(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp);
5974 
5975     __ leave(); // required for proper stackwalking of RuntimeStub frame
5976     __ ret(0);
5977 
5978     return start;
5979 
5980   }
5981 
5982   address generate_libmLog() {
5983     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;libmLog&quot;);
5984 
5985     address start = __ pc();
5986 
5987     const XMMRegister x0 = xmm0;
5988     const XMMRegister x1 = xmm1;
5989     const XMMRegister x2 = xmm2;
5990     const XMMRegister x3 = xmm3;
5991 
5992     const XMMRegister x4 = xmm4;
5993     const XMMRegister x5 = xmm5;
5994     const XMMRegister x6 = xmm6;
5995     const XMMRegister x7 = xmm7;
5996 
5997     const Register tmp1 = r11;
5998     const Register tmp2 = r8;
5999 
6000     BLOCK_COMMENT(&quot;Entry:&quot;);
6001     __ enter(); // required for proper stackwalking of RuntimeStub frame
6002 
6003     __ fast_log(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp1, tmp2);
6004 
6005     __ leave(); // required for proper stackwalking of RuntimeStub frame
6006     __ ret(0);
6007 
6008     return start;
6009 
6010   }
6011 
6012   address generate_libmLog10() {
6013     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;libmLog10&quot;);
6014 
6015     address start = __ pc();
6016 
6017     const XMMRegister x0 = xmm0;
6018     const XMMRegister x1 = xmm1;
6019     const XMMRegister x2 = xmm2;
6020     const XMMRegister x3 = xmm3;
6021 
6022     const XMMRegister x4 = xmm4;
6023     const XMMRegister x5 = xmm5;
6024     const XMMRegister x6 = xmm6;
6025     const XMMRegister x7 = xmm7;
6026 
6027     const Register tmp = r11;
6028 
6029     BLOCK_COMMENT(&quot;Entry:&quot;);
6030     __ enter(); // required for proper stackwalking of RuntimeStub frame
6031 
6032     __ fast_log10(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp);
6033 
6034     __ leave(); // required for proper stackwalking of RuntimeStub frame
6035     __ ret(0);
6036 
6037     return start;
6038 
6039   }
6040 
6041   address generate_libmPow() {
6042     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;libmPow&quot;);
6043 
6044     address start = __ pc();
6045 
6046     const XMMRegister x0 = xmm0;
6047     const XMMRegister x1 = xmm1;
6048     const XMMRegister x2 = xmm2;
6049     const XMMRegister x3 = xmm3;
6050 
6051     const XMMRegister x4 = xmm4;
6052     const XMMRegister x5 = xmm5;
6053     const XMMRegister x6 = xmm6;
6054     const XMMRegister x7 = xmm7;
6055 
6056     const Register tmp1 = r8;
6057     const Register tmp2 = r9;
6058     const Register tmp3 = r10;
6059     const Register tmp4 = r11;
6060 
6061     BLOCK_COMMENT(&quot;Entry:&quot;);
6062     __ enter(); // required for proper stackwalking of RuntimeStub frame
6063 
6064     __ fast_pow(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp1, tmp2, tmp3, tmp4);
6065 
6066     __ leave(); // required for proper stackwalking of RuntimeStub frame
6067     __ ret(0);
6068 
6069     return start;
6070 
6071   }
6072 
6073   address generate_libmSin() {
6074     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;libmSin&quot;);
6075 
6076     address start = __ pc();
6077 
6078     const XMMRegister x0 = xmm0;
6079     const XMMRegister x1 = xmm1;
6080     const XMMRegister x2 = xmm2;
6081     const XMMRegister x3 = xmm3;
6082 
6083     const XMMRegister x4 = xmm4;
6084     const XMMRegister x5 = xmm5;
6085     const XMMRegister x6 = xmm6;
6086     const XMMRegister x7 = xmm7;
6087 
6088     const Register tmp1 = r8;
6089     const Register tmp2 = r9;
6090     const Register tmp3 = r10;
6091     const Register tmp4 = r11;
6092 
6093     BLOCK_COMMENT(&quot;Entry:&quot;);
6094     __ enter(); // required for proper stackwalking of RuntimeStub frame
6095 
6096 #ifdef _WIN64
6097     __ push(rsi);
6098     __ push(rdi);
6099 #endif
6100     __ fast_sin(x0, x1, x2, x3, x4, x5, x6, x7, rax, rbx, rcx, rdx, tmp1, tmp2, tmp3, tmp4);
6101 
6102 #ifdef _WIN64
6103     __ pop(rdi);
6104     __ pop(rsi);
6105 #endif
6106 
6107     __ leave(); // required for proper stackwalking of RuntimeStub frame
6108     __ ret(0);
6109 
6110     return start;
6111 
6112   }
6113 
6114   address generate_libmCos() {
6115     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;libmCos&quot;);
6116 
6117     address start = __ pc();
6118 
6119     const XMMRegister x0 = xmm0;
6120     const XMMRegister x1 = xmm1;
6121     const XMMRegister x2 = xmm2;
6122     const XMMRegister x3 = xmm3;
6123 
6124     const XMMRegister x4 = xmm4;
6125     const XMMRegister x5 = xmm5;
6126     const XMMRegister x6 = xmm6;
6127     const XMMRegister x7 = xmm7;
6128 
6129     const Register tmp1 = r8;
6130     const Register tmp2 = r9;
6131     const Register tmp3 = r10;
6132     const Register tmp4 = r11;
6133 
6134     BLOCK_COMMENT(&quot;Entry:&quot;);
6135     __ enter(); // required for proper stackwalking of RuntimeStub frame
6136 
6137 #ifdef _WIN64
6138     __ push(rsi);
6139     __ push(rdi);
6140 #endif
6141     __ fast_cos(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp1, tmp2, tmp3, tmp4);
6142 
6143 #ifdef _WIN64
6144     __ pop(rdi);
6145     __ pop(rsi);
6146 #endif
6147 
6148     __ leave(); // required for proper stackwalking of RuntimeStub frame
6149     __ ret(0);
6150 
6151     return start;
6152 
6153   }
6154 
6155   address generate_libmTan() {
6156     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;libmTan&quot;);
6157 
6158     address start = __ pc();
6159 
6160     const XMMRegister x0 = xmm0;
6161     const XMMRegister x1 = xmm1;
6162     const XMMRegister x2 = xmm2;
6163     const XMMRegister x3 = xmm3;
6164 
6165     const XMMRegister x4 = xmm4;
6166     const XMMRegister x5 = xmm5;
6167     const XMMRegister x6 = xmm6;
6168     const XMMRegister x7 = xmm7;
6169 
6170     const Register tmp1 = r8;
6171     const Register tmp2 = r9;
6172     const Register tmp3 = r10;
6173     const Register tmp4 = r11;
6174 
6175     BLOCK_COMMENT(&quot;Entry:&quot;);
6176     __ enter(); // required for proper stackwalking of RuntimeStub frame
6177 
6178 #ifdef _WIN64
6179     __ push(rsi);
6180     __ push(rdi);
6181 #endif
6182     __ fast_tan(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp1, tmp2, tmp3, tmp4);
6183 
6184 #ifdef _WIN64
6185     __ pop(rdi);
6186     __ pop(rsi);
6187 #endif
6188 
6189     __ leave(); // required for proper stackwalking of RuntimeStub frame
6190     __ ret(0);
6191 
6192     return start;
6193 
6194   }
6195 
6196 #undef __
6197 #define __ masm-&gt;
6198 
6199   // Continuation point for throwing of implicit exceptions that are
6200   // not handled in the current activation. Fabricates an exception
6201   // oop and initiates normal exception dispatching in this
6202   // frame. Since we need to preserve callee-saved values (currently
6203   // only for C2, but done for C1 as well) we need a callee-saved oop
6204   // map and therefore have to make these stubs into RuntimeStubs
6205   // rather than BufferBlobs.  If the compiler needs all registers to
6206   // be preserved between the fault point and the exception handler
6207   // then it must assume responsibility for that in
6208   // AbstractCompiler::continuation_for_implicit_null_exception or
6209   // continuation_for_implicit_division_by_zero_exception. All other
6210   // implicit exceptions (e.g., NullPointerException or
6211   // AbstractMethodError on entry) are either at call sites or
6212   // otherwise assume that stack unwinding will be initiated, so
6213   // caller saved registers were assumed volatile in the compiler.
6214   address generate_throw_exception(const char* name,
6215                                    address runtime_entry,
6216                                    Register arg1 = noreg,
6217                                    Register arg2 = noreg) {
6218     // Information about frame layout at time of blocking runtime call.
6219     // Note that we only have to preserve callee-saved registers since
6220     // the compilers are responsible for supplying a continuation point
6221     // if they expect all registers to be preserved.
6222     enum layout {
6223       rbp_off = frame::arg_reg_save_area_bytes/BytesPerInt,
6224       rbp_off2,
6225       return_off,
6226       return_off2,
6227       framesize // inclusive of return address
6228     };
6229 
6230     int insts_size = 512;
6231     int locs_size  = 64;
6232 
6233     CodeBuffer code(name, insts_size, locs_size);
6234     OopMapSet* oop_maps  = new OopMapSet();
6235     MacroAssembler* masm = new MacroAssembler(&amp;code);
6236 
6237     address start = __ pc();
6238 
6239     // This is an inlined and slightly modified version of call_VM
6240     // which has the ability to fetch the return PC out of
6241     // thread-local storage and also sets up last_Java_sp slightly
6242     // differently than the real call_VM
6243 
6244     __ enter(); // required for proper stackwalking of RuntimeStub frame
6245 
6246     assert(is_even(framesize/2), &quot;sp not 16-byte aligned&quot;);
6247 
6248     // return address and rbp are already in place
6249     __ subptr(rsp, (framesize-4) &lt;&lt; LogBytesPerInt); // prolog
6250 
6251     int frame_complete = __ pc() - start;
6252 
6253     // Set up last_Java_sp and last_Java_fp
6254     address the_pc = __ pc();
6255     __ set_last_Java_frame(rsp, rbp, the_pc);
6256     __ andptr(rsp, -(StackAlignmentInBytes));    // Align stack
6257 
6258     // Call runtime
6259     if (arg1 != noreg) {
6260       assert(arg2 != c_rarg1, &quot;clobbered&quot;);
6261       __ movptr(c_rarg1, arg1);
6262     }
6263     if (arg2 != noreg) {
6264       __ movptr(c_rarg2, arg2);
6265     }
6266     __ movptr(c_rarg0, r15_thread);
6267     BLOCK_COMMENT(&quot;call runtime_entry&quot;);
6268     __ call(RuntimeAddress(runtime_entry));
6269 
6270     // Generate oop map
6271     OopMap* map = new OopMap(framesize, 0);
6272 
6273     oop_maps-&gt;add_gc_map(the_pc - start, map);
6274 
6275     __ reset_last_Java_frame(true);
6276 
6277     __ leave(); // required for proper stackwalking of RuntimeStub frame
6278 
6279     // check for pending exceptions
6280 #ifdef ASSERT
6281     Label L;
6282     __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()),
6283             (int32_t) NULL_WORD);
6284     __ jcc(Assembler::notEqual, L);
6285     __ should_not_reach_here();
6286     __ bind(L);
6287 #endif // ASSERT
6288     __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
6289 
6290 
6291     // codeBlob framesize is in words (not VMRegImpl::slot_size)
6292     RuntimeStub* stub =
6293       RuntimeStub::new_runtime_stub(name,
6294                                     &amp;code,
6295                                     frame_complete,
6296                                     (framesize &gt;&gt; (LogBytesPerWord - LogBytesPerInt)),
6297                                     oop_maps, false);
6298     return stub-&gt;entry_point();
6299   }
6300 
6301   void create_control_words() {
6302     // Round to nearest, 53-bit mode, exceptions masked
6303     StubRoutines::_fpu_cntrl_wrd_std   = 0x027F;
6304     // Round to zero, 53-bit mode, exception mased
6305     StubRoutines::_fpu_cntrl_wrd_trunc = 0x0D7F;
6306     // Round to nearest, 24-bit mode, exceptions masked
6307     StubRoutines::_fpu_cntrl_wrd_24    = 0x007F;
6308     // Round to nearest, 64-bit mode, exceptions masked
6309     StubRoutines::_mxcsr_std           = 0x1F80;
6310     // Note: the following two constants are 80-bit values
6311     //       layout is critical for correct loading by FPU.
6312     // Bias for strict fp multiply/divide
6313     StubRoutines::_fpu_subnormal_bias1[0]= 0x00000000; // 2^(-15360) == 0x03ff 8000 0000 0000 0000
6314     StubRoutines::_fpu_subnormal_bias1[1]= 0x80000000;
6315     StubRoutines::_fpu_subnormal_bias1[2]= 0x03ff;
6316     // Un-Bias for strict fp multiply/divide
6317     StubRoutines::_fpu_subnormal_bias2[0]= 0x00000000; // 2^(+15360) == 0x7bff 8000 0000 0000 0000
6318     StubRoutines::_fpu_subnormal_bias2[1]= 0x80000000;
6319     StubRoutines::_fpu_subnormal_bias2[2]= 0x7bff;
6320   }
6321 
6322   // Call here from the interpreter or compiled code to either load
6323   // multiple returned values from the value type instance being
6324   // returned to registers or to store returned values to a newly
6325   // allocated value type instance.
6326   address generate_return_value_stub(address destination, const char* name, bool has_res) {
6327     // We need to save all registers the calling convention may use so
6328     // the runtime calls read or update those registers. This needs to
6329     // be in sync with SharedRuntime::java_return_convention().
6330     enum layout {
6331       pad_off = frame::arg_reg_save_area_bytes/BytesPerInt, pad_off_2,
6332       rax_off, rax_off_2,
6333       j_rarg5_off, j_rarg5_2,
6334       j_rarg4_off, j_rarg4_2,
6335       j_rarg3_off, j_rarg3_2,
6336       j_rarg2_off, j_rarg2_2,
6337       j_rarg1_off, j_rarg1_2,
6338       j_rarg0_off, j_rarg0_2,
6339       j_farg0_off, j_farg0_2,
6340       j_farg1_off, j_farg1_2,
6341       j_farg2_off, j_farg2_2,
6342       j_farg3_off, j_farg3_2,
6343       j_farg4_off, j_farg4_2,
6344       j_farg5_off, j_farg5_2,
6345       j_farg6_off, j_farg6_2,
6346       j_farg7_off, j_farg7_2,
6347       rbp_off, rbp_off_2,
6348       return_off, return_off_2,
6349 
6350       framesize
6351     };
6352 
6353     CodeBuffer buffer(name, 1000, 512);
6354     MacroAssembler* masm = new MacroAssembler(&amp;buffer);
6355 
6356     int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);
6357     assert(frame_size_in_bytes == framesize*BytesPerInt, &quot;misaligned&quot;);
6358     int frame_size_in_slots = frame_size_in_bytes / BytesPerInt;
6359     int frame_size_in_words = frame_size_in_bytes / wordSize;
6360 
6361     OopMapSet *oop_maps = new OopMapSet();
6362     OopMap* map = new OopMap(frame_size_in_slots, 0);
6363 
6364     map-&gt;set_callee_saved(VMRegImpl::stack2reg(rax_off), rax-&gt;as_VMReg());
6365     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5-&gt;as_VMReg());
6366     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4-&gt;as_VMReg());
6367     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3-&gt;as_VMReg());
6368     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2-&gt;as_VMReg());
6369     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1-&gt;as_VMReg());
6370     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0-&gt;as_VMReg());
6371     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0-&gt;as_VMReg());
6372     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1-&gt;as_VMReg());
6373     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2-&gt;as_VMReg());
6374     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3-&gt;as_VMReg());
6375     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4-&gt;as_VMReg());
6376     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5-&gt;as_VMReg());
6377     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6-&gt;as_VMReg());
6378     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7-&gt;as_VMReg());
6379 
6380     int start = __ offset();
6381 
6382     __ subptr(rsp, frame_size_in_bytes - 8 /* return address*/);
6383 
6384     __ movptr(Address(rsp, rbp_off * BytesPerInt), rbp);
6385     __ movdbl(Address(rsp, j_farg7_off * BytesPerInt), j_farg7);
6386     __ movdbl(Address(rsp, j_farg6_off * BytesPerInt), j_farg6);
6387     __ movdbl(Address(rsp, j_farg5_off * BytesPerInt), j_farg5);
6388     __ movdbl(Address(rsp, j_farg4_off * BytesPerInt), j_farg4);
6389     __ movdbl(Address(rsp, j_farg3_off * BytesPerInt), j_farg3);
6390     __ movdbl(Address(rsp, j_farg2_off * BytesPerInt), j_farg2);
6391     __ movdbl(Address(rsp, j_farg1_off * BytesPerInt), j_farg1);
6392     __ movdbl(Address(rsp, j_farg0_off * BytesPerInt), j_farg0);
6393 
6394     __ movptr(Address(rsp, j_rarg0_off * BytesPerInt), j_rarg0);
6395     __ movptr(Address(rsp, j_rarg1_off * BytesPerInt), j_rarg1);
6396     __ movptr(Address(rsp, j_rarg2_off * BytesPerInt), j_rarg2);
6397     __ movptr(Address(rsp, j_rarg3_off * BytesPerInt), j_rarg3);
6398     __ movptr(Address(rsp, j_rarg4_off * BytesPerInt), j_rarg4);
6399     __ movptr(Address(rsp, j_rarg5_off * BytesPerInt), j_rarg5);
6400     __ movptr(Address(rsp, rax_off * BytesPerInt), rax);
6401 
6402     int frame_complete = __ offset();
6403 
6404     __ set_last_Java_frame(noreg, noreg, NULL);
6405 
6406     __ mov(c_rarg0, r15_thread);
6407     __ mov(c_rarg1, rax);
6408 
6409     __ call(RuntimeAddress(destination));
6410 
6411     // Set an oopmap for the call site.
6412 
6413     oop_maps-&gt;add_gc_map( __ offset() - start, map);
6414 
6415     // clear last_Java_sp
6416     __ reset_last_Java_frame(false);
6417 
6418     __ movptr(rbp, Address(rsp, rbp_off * BytesPerInt));
6419     __ movdbl(j_farg7, Address(rsp, j_farg7_off * BytesPerInt));
6420     __ movdbl(j_farg6, Address(rsp, j_farg6_off * BytesPerInt));
6421     __ movdbl(j_farg5, Address(rsp, j_farg5_off * BytesPerInt));
6422     __ movdbl(j_farg4, Address(rsp, j_farg4_off * BytesPerInt));
6423     __ movdbl(j_farg3, Address(rsp, j_farg3_off * BytesPerInt));
6424     __ movdbl(j_farg2, Address(rsp, j_farg2_off * BytesPerInt));
6425     __ movdbl(j_farg1, Address(rsp, j_farg1_off * BytesPerInt));
6426     __ movdbl(j_farg0, Address(rsp, j_farg0_off * BytesPerInt));
6427 
6428     __ movptr(j_rarg0, Address(rsp, j_rarg0_off * BytesPerInt));
6429     __ movptr(j_rarg1, Address(rsp, j_rarg1_off * BytesPerInt));
6430     __ movptr(j_rarg2, Address(rsp, j_rarg2_off * BytesPerInt));
6431     __ movptr(j_rarg3, Address(rsp, j_rarg3_off * BytesPerInt));
6432     __ movptr(j_rarg4, Address(rsp, j_rarg4_off * BytesPerInt));
6433     __ movptr(j_rarg5, Address(rsp, j_rarg5_off * BytesPerInt));
6434     __ movptr(rax, Address(rsp, rax_off * BytesPerInt));
6435 
6436     __ addptr(rsp, frame_size_in_bytes-8);
6437 
6438     // check for pending exceptions
6439     Label pending;
6440     __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);
6441     __ jcc(Assembler::notEqual, pending);
6442 
6443     if (has_res) {
6444       __ get_vm_result(rax, r15_thread);
6445     }
6446 
6447     __ ret(0);
6448 
6449     __ bind(pending);
6450 
6451     __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));
6452     __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
6453 
6454     // -------------
6455     // make sure all code is generated
6456     masm-&gt;flush();
6457 
6458     RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &amp;buffer, frame_complete, frame_size_in_words, oop_maps, false);
6459     return stub-&gt;entry_point();
6460   }
6461 
6462   // Initialization
6463   void generate_initial() {
6464     // Generates all stubs and initializes the entry points
6465 
6466     // This platform-specific settings are needed by generate_call_stub()
6467     create_control_words();
6468 
6469     // entry points that exist in all platforms Note: This is code
6470     // that could be shared among different platforms - however the
6471     // benefit seems to be smaller than the disadvantage of having a
6472     // much more complicated generator structure. See also comment in
6473     // stubRoutines.hpp.
6474 
6475     StubRoutines::_forward_exception_entry = generate_forward_exception();
6476 
6477     // Generate these first because they are called from other stubs
6478     StubRoutines::_load_value_type_fields_in_regs = generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_value_type_fields_in_regs), &quot;load_value_type_fields_in_regs&quot;, false);
6479     StubRoutines::_store_value_type_fields_to_buf = generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_value_type_fields_to_buf), &quot;store_value_type_fields_to_buf&quot;, true);
6480 
6481     StubRoutines::_call_stub_entry = generate_call_stub(StubRoutines::_call_stub_return_address);
6482 
6483     // is referenced by megamorphic call
6484     StubRoutines::_catch_exception_entry = generate_catch_exception();
6485 
6486     // atomic calls
6487     StubRoutines::_atomic_xchg_entry          = generate_atomic_xchg();
6488     StubRoutines::_atomic_xchg_long_entry     = generate_atomic_xchg_long();
6489     StubRoutines::_atomic_cmpxchg_entry       = generate_atomic_cmpxchg();
6490     StubRoutines::_atomic_cmpxchg_byte_entry  = generate_atomic_cmpxchg_byte();
6491     StubRoutines::_atomic_cmpxchg_long_entry  = generate_atomic_cmpxchg_long();
6492     StubRoutines::_atomic_add_entry           = generate_atomic_add();
6493     StubRoutines::_atomic_add_long_entry      = generate_atomic_add_long();
6494     StubRoutines::_fence_entry                = generate_orderaccess_fence();
6495 
6496     // platform dependent
6497     StubRoutines::x86::_get_previous_fp_entry = generate_get_previous_fp();
6498     StubRoutines::x86::_get_previous_sp_entry = generate_get_previous_sp();
6499 
6500     StubRoutines::x86::_verify_mxcsr_entry    = generate_verify_mxcsr();
6501 
6502     StubRoutines::x86::_f2i_fixup             = generate_f2i_fixup();
6503     StubRoutines::x86::_f2l_fixup             = generate_f2l_fixup();
6504     StubRoutines::x86::_d2i_fixup             = generate_d2i_fixup();
6505     StubRoutines::x86::_d2l_fixup             = generate_d2l_fixup();
6506 
6507     StubRoutines::x86::_float_sign_mask       = generate_fp_mask(&quot;float_sign_mask&quot;,  0x7FFFFFFF7FFFFFFF);
6508     StubRoutines::x86::_float_sign_flip       = generate_fp_mask(&quot;float_sign_flip&quot;,  0x8000000080000000);
6509     StubRoutines::x86::_double_sign_mask      = generate_fp_mask(&quot;double_sign_mask&quot;, 0x7FFFFFFFFFFFFFFF);
6510     StubRoutines::x86::_double_sign_flip      = generate_fp_mask(&quot;double_sign_flip&quot;, 0x8000000000000000);
6511 
6512     // Build this early so it&#39;s available for the interpreter.
6513     StubRoutines::_throw_StackOverflowError_entry =
6514       generate_throw_exception(&quot;StackOverflowError throw_exception&quot;,
6515                                CAST_FROM_FN_PTR(address,
6516                                                 SharedRuntime::
6517                                                 throw_StackOverflowError));
6518     StubRoutines::_throw_delayed_StackOverflowError_entry =
6519       generate_throw_exception(&quot;delayed StackOverflowError throw_exception&quot;,
6520                                CAST_FROM_FN_PTR(address,
6521                                                 SharedRuntime::
6522                                                 throw_delayed_StackOverflowError));
6523     if (UseCRC32Intrinsics) {
6524       // set table address before stub generation which use it
6525       StubRoutines::_crc_table_adr = (address)StubRoutines::x86::_crc_table;
6526       StubRoutines::_updateBytesCRC32 = generate_updateBytesCRC32();
6527     }
6528 
6529     if (UseCRC32CIntrinsics) {
6530       bool supports_clmul = VM_Version::supports_clmul();
6531       StubRoutines::x86::generate_CRC32C_table(supports_clmul);
6532       StubRoutines::_crc32c_table_addr = (address)StubRoutines::x86::_crc32c_table;
6533       StubRoutines::_updateBytesCRC32C = generate_updateBytesCRC32C(supports_clmul);
6534     }
6535     if (UseLibmIntrinsic &amp;&amp; InlineIntrinsics) {
6536       if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin) ||
6537           vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos) ||
6538           vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dtan)) {
6539         StubRoutines::x86::_ONEHALF_adr = (address)StubRoutines::x86::_ONEHALF;
6540         StubRoutines::x86::_P_2_adr = (address)StubRoutines::x86::_P_2;
6541         StubRoutines::x86::_SC_4_adr = (address)StubRoutines::x86::_SC_4;
6542         StubRoutines::x86::_Ctable_adr = (address)StubRoutines::x86::_Ctable;
6543         StubRoutines::x86::_SC_2_adr = (address)StubRoutines::x86::_SC_2;
6544         StubRoutines::x86::_SC_3_adr = (address)StubRoutines::x86::_SC_3;
6545         StubRoutines::x86::_SC_1_adr = (address)StubRoutines::x86::_SC_1;
6546         StubRoutines::x86::_PI_INV_TABLE_adr = (address)StubRoutines::x86::_PI_INV_TABLE;
6547         StubRoutines::x86::_PI_4_adr = (address)StubRoutines::x86::_PI_4;
6548         StubRoutines::x86::_PI32INV_adr = (address)StubRoutines::x86::_PI32INV;
6549         StubRoutines::x86::_SIGN_MASK_adr = (address)StubRoutines::x86::_SIGN_MASK;
6550         StubRoutines::x86::_P_1_adr = (address)StubRoutines::x86::_P_1;
6551         StubRoutines::x86::_P_3_adr = (address)StubRoutines::x86::_P_3;
6552         StubRoutines::x86::_NEG_ZERO_adr = (address)StubRoutines::x86::_NEG_ZERO;
6553       }
6554       if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dexp)) {
6555         StubRoutines::_dexp = generate_libmExp();
6556       }
6557       if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog)) {
6558         StubRoutines::_dlog = generate_libmLog();
6559       }
6560       if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog10)) {
6561         StubRoutines::_dlog10 = generate_libmLog10();
6562       }
6563       if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dpow)) {
6564         StubRoutines::_dpow = generate_libmPow();
6565       }
6566       if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin)) {
6567         StubRoutines::_dsin = generate_libmSin();
6568       }
6569       if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {
6570         StubRoutines::_dcos = generate_libmCos();
6571       }
6572       if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dtan)) {
6573         StubRoutines::_dtan = generate_libmTan();
6574       }
6575     }
6576   }
6577 
6578   void generate_all() {
6579     // Generates all stubs and initializes the entry points
6580 
6581     // These entry points require SharedInfo::stack0 to be set up in
6582     // non-core builds and need to be relocatable, so they each
6583     // fabricate a RuntimeStub internally.
6584     StubRoutines::_throw_AbstractMethodError_entry =
6585       generate_throw_exception(&quot;AbstractMethodError throw_exception&quot;,
6586                                CAST_FROM_FN_PTR(address,
6587                                                 SharedRuntime::
6588                                                 throw_AbstractMethodError));
6589 
6590     StubRoutines::_throw_IncompatibleClassChangeError_entry =
6591       generate_throw_exception(&quot;IncompatibleClassChangeError throw_exception&quot;,
6592                                CAST_FROM_FN_PTR(address,
6593                                                 SharedRuntime::
6594                                                 throw_IncompatibleClassChangeError));
6595 
6596     StubRoutines::_throw_NullPointerException_at_call_entry =
6597       generate_throw_exception(&quot;NullPointerException at call throw_exception&quot;,
6598                                CAST_FROM_FN_PTR(address,
6599                                                 SharedRuntime::
6600                                                 throw_NullPointerException_at_call));
6601 
6602     // entry points that are platform specific
6603     StubRoutines::x86::_vector_float_sign_mask = generate_vector_mask(&quot;vector_float_sign_mask&quot;, 0x7FFFFFFF7FFFFFFF);
6604     StubRoutines::x86::_vector_float_sign_flip = generate_vector_mask(&quot;vector_float_sign_flip&quot;, 0x8000000080000000);
6605     StubRoutines::x86::_vector_double_sign_mask = generate_vector_mask(&quot;vector_double_sign_mask&quot;, 0x7FFFFFFFFFFFFFFF);
6606     StubRoutines::x86::_vector_double_sign_flip = generate_vector_mask(&quot;vector_double_sign_flip&quot;, 0x8000000000000000);
6607     StubRoutines::x86::_vector_short_to_byte_mask = generate_vector_mask(&quot;vector_short_to_byte_mask&quot;, 0x00ff00ff00ff00ff);
6608     StubRoutines::x86::_vector_byte_perm_mask = generate_vector_byte_perm_mask(&quot;vector_byte_perm_mask&quot;);
6609     StubRoutines::x86::_vector_long_sign_mask = generate_vector_mask(&quot;vector_long_sign_mask&quot;, 0x8000000000000000);
6610 
6611     // support for verify_oop (must happen after universe_init)
6612     StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();
6613 
6614     // data cache line writeback
6615     StubRoutines::_data_cache_writeback = generate_data_cache_writeback();
6616     StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();
6617 
6618     // arraycopy stubs used by compilers
6619     generate_arraycopy_stubs();
6620 
6621     // don&#39;t bother generating these AES intrinsic stubs unless global flag is set
6622     if (UseAESIntrinsics) {
6623       StubRoutines::x86::_key_shuffle_mask_addr = generate_key_shuffle_mask();  // needed by the others
6624       StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();
6625       StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();
6626       StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();
6627       if (VM_Version::supports_avx512_vaes() &amp;&amp;  VM_Version::supports_avx512vl() &amp;&amp; VM_Version::supports_avx512dq() ) {
6628         StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptVectorAESCrypt();
6629         StubRoutines::_electronicCodeBook_encryptAESCrypt = generate_electronicCodeBook_encryptAESCrypt();
6630         StubRoutines::_electronicCodeBook_decryptAESCrypt = generate_electronicCodeBook_decryptAESCrypt();
6631       } else {
6632         StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt_Parallel();
6633       }
6634     }
6635     if (UseAESCTRIntrinsics) {
6636       if (VM_Version::supports_avx512_vaes() &amp;&amp; VM_Version::supports_avx512bw() &amp;&amp; VM_Version::supports_avx512vl()) {
6637         StubRoutines::x86::_counter_mask_addr = counter_mask_addr();
6638         StubRoutines::_counterMode_AESCrypt = generate_counterMode_VectorAESCrypt();
6639       } else {
6640         StubRoutines::x86::_counter_shuffle_mask_addr = generate_counter_shuffle_mask();
6641         StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt_Parallel();
6642       }
6643     }
6644 
6645     if (UseSHA1Intrinsics) {
6646       StubRoutines::x86::_upper_word_mask_addr = generate_upper_word_mask();
6647       StubRoutines::x86::_shuffle_byte_flip_mask_addr = generate_shuffle_byte_flip_mask();
6648       StubRoutines::_sha1_implCompress = generate_sha1_implCompress(false, &quot;sha1_implCompress&quot;);
6649       StubRoutines::_sha1_implCompressMB = generate_sha1_implCompress(true, &quot;sha1_implCompressMB&quot;);
6650     }
6651     if (UseSHA256Intrinsics) {
6652       StubRoutines::x86::_k256_adr = (address)StubRoutines::x86::_k256;
6653       char* dst = (char*)StubRoutines::x86::_k256_W;
6654       char* src = (char*)StubRoutines::x86::_k256;
6655       for (int ii = 0; ii &lt; 16; ++ii) {
6656         memcpy(dst + 32 * ii,      src + 16 * ii, 16);
6657         memcpy(dst + 32 * ii + 16, src + 16 * ii, 16);
6658       }
6659       StubRoutines::x86::_k256_W_adr = (address)StubRoutines::x86::_k256_W;
6660       StubRoutines::x86::_pshuffle_byte_flip_mask_addr = generate_pshuffle_byte_flip_mask();
6661       StubRoutines::_sha256_implCompress = generate_sha256_implCompress(false, &quot;sha256_implCompress&quot;);
6662       StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true, &quot;sha256_implCompressMB&quot;);
6663     }
6664     if (UseSHA512Intrinsics) {
6665       StubRoutines::x86::_k512_W_addr = (address)StubRoutines::x86::_k512_W;
6666       StubRoutines::x86::_pshuffle_byte_flip_mask_addr_sha512 = generate_pshuffle_byte_flip_mask_sha512();
6667       StubRoutines::_sha512_implCompress = generate_sha512_implCompress(false, &quot;sha512_implCompress&quot;);
6668       StubRoutines::_sha512_implCompressMB = generate_sha512_implCompress(true, &quot;sha512_implCompressMB&quot;);
6669     }
6670 
6671     // Generate GHASH intrinsics code
6672     if (UseGHASHIntrinsics) {
6673     StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();
6674     StubRoutines::x86::_ghash_byte_swap_mask_addr = generate_ghash_byte_swap_mask();
6675       if (VM_Version::supports_avx()) {
6676         StubRoutines::x86::_ghash_shuffmask_addr = ghash_shufflemask_addr();
6677         StubRoutines::x86::_ghash_poly_addr = ghash_polynomial_addr();
6678         StubRoutines::_ghash_processBlocks = generate_avx_ghash_processBlocks();
6679       } else {
6680         StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();
6681       }
6682     }
6683 
6684     if (UseBASE64Intrinsics) {
6685       StubRoutines::x86::_and_mask = base64_and_mask_addr();
6686       StubRoutines::x86::_bswap_mask = base64_bswap_mask_addr();
6687       StubRoutines::x86::_base64_charset = base64_charset_addr();
6688       StubRoutines::x86::_url_charset = base64url_charset_addr();
6689       StubRoutines::x86::_gather_mask = base64_gather_mask_addr();
6690       StubRoutines::x86::_left_shift_mask = base64_left_shift_mask_addr();
6691       StubRoutines::x86::_right_shift_mask = base64_right_shift_mask_addr();
6692       StubRoutines::_base64_encodeBlock = generate_base64_encodeBlock();
6693     }
6694 
6695     // Safefetch stubs.
6696     generate_safefetch(&quot;SafeFetch32&quot;, sizeof(int),     &amp;StubRoutines::_safefetch32_entry,
6697                                                        &amp;StubRoutines::_safefetch32_fault_pc,
6698                                                        &amp;StubRoutines::_safefetch32_continuation_pc);
6699     generate_safefetch(&quot;SafeFetchN&quot;, sizeof(intptr_t), &amp;StubRoutines::_safefetchN_entry,
6700                                                        &amp;StubRoutines::_safefetchN_fault_pc,
6701                                                        &amp;StubRoutines::_safefetchN_continuation_pc);
6702 
6703     BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();
6704     if (bs_nm != NULL) {
6705       StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();
6706     }
6707 #ifdef COMPILER2
6708     if (UseMultiplyToLenIntrinsic) {
6709       StubRoutines::_multiplyToLen = generate_multiplyToLen();
6710     }
6711     if (UseSquareToLenIntrinsic) {
6712       StubRoutines::_squareToLen = generate_squareToLen();
6713     }
6714     if (UseMulAddIntrinsic) {
6715       StubRoutines::_mulAdd = generate_mulAdd();
6716     }
6717     if (VM_Version::supports_avx512_vbmi2()) {
6718       StubRoutines::_bigIntegerRightShiftWorker = generate_bigIntegerRightShift();
6719       StubRoutines::_bigIntegerLeftShiftWorker = generate_bigIntegerLeftShift();
6720     }
6721 #ifndef _WINDOWS
6722     if (UseMontgomeryMultiplyIntrinsic) {
6723       StubRoutines::_montgomeryMultiply
6724         = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_multiply);
6725     }
6726     if (UseMontgomerySquareIntrinsic) {
6727       StubRoutines::_montgomerySquare
6728         = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_square);
6729     }
6730 #endif // WINDOWS
6731 #endif // COMPILER2
6732 
6733     if (UseVectorizedMismatchIntrinsic) {
6734       StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();
6735     }
6736   }
6737 
6738  public:
6739   StubGenerator(CodeBuffer* code, bool all) : StubCodeGenerator(code) {
6740     if (all) {
6741       generate_all();
6742     } else {
6743       generate_initial();
6744     }
6745   }
6746 }; // end class declaration
6747 
6748 #define UCM_TABLE_MAX_ENTRIES 16
6749 void StubGenerator_generate(CodeBuffer* code, bool all) {
6750   if (UnsafeCopyMemory::_table == NULL) {
6751     UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);
6752   }
6753   StubGenerator g(code, all);
6754 }
    </pre>
  </body>
</html>