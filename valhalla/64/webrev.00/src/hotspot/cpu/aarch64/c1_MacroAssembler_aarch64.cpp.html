<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/cpu/aarch64/c1_MacroAssembler_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 1999, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * Copyright (c) 2014, Red Hat Inc. All rights reserved.
  4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  5  *
  6  * This code is free software; you can redistribute it and/or modify it
  7  * under the terms of the GNU General Public License version 2 only, as
  8  * published by the Free Software Foundation.
  9  *
 10  * This code is distributed in the hope that it will be useful, but WITHOUT
 11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 13  * version 2 for more details (a copy is included in the LICENSE file that
 14  * accompanied this code).
 15  *
 16  * You should have received a copy of the GNU General Public License version
 17  * 2 along with this work; if not, write to the Free Software Foundation,
 18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 19  *
 20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 21  * or visit www.oracle.com if you need additional information or have any
 22  * questions.
 23  *
 24  */
 25 
 26 #include &quot;precompiled.hpp&quot;
 27 #include &quot;c1/c1_MacroAssembler.hpp&quot;
 28 #include &quot;c1/c1_Runtime1.hpp&quot;
 29 #include &quot;classfile/systemDictionary.hpp&quot;
 30 #include &quot;gc/shared/collectedHeap.hpp&quot;
 31 #include &quot;gc/shared/barrierSet.hpp&quot;
 32 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
 33 #include &quot;interpreter/interpreter.hpp&quot;
 34 #include &quot;oops/arrayOop.hpp&quot;
 35 #include &quot;oops/markWord.hpp&quot;
 36 #include &quot;runtime/basicLock.hpp&quot;
 37 #include &quot;runtime/biasedLocking.hpp&quot;
 38 #include &quot;runtime/os.hpp&quot;
 39 #include &quot;runtime/sharedRuntime.hpp&quot;
 40 #include &quot;runtime/stubRoutines.hpp&quot;
 41 
 42 void C1_MacroAssembler::float_cmp(bool is_float, int unordered_result,
 43                                   FloatRegister f0, FloatRegister f1,
 44                                   Register result)
 45 {
 46   Label done;
 47   if (is_float) {
 48     fcmps(f0, f1);
 49   } else {
 50     fcmpd(f0, f1);
 51   }
 52   if (unordered_result &lt; 0) {
 53     // we want -1 for unordered or less than, 0 for equal and 1 for
 54     // greater than.
 55     cset(result, NE);  // Not equal or unordered
 56     cneg(result, result, LT);  // Less than or unordered
 57   } else {
 58     // we want -1 for less than, 0 for equal and 1 for unordered or
 59     // greater than.
 60     cset(result, NE);  // Not equal or unordered
 61     cneg(result, result, LO);  // Less than
 62   }
 63 }
 64 
 65 int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Register scratch, Label&amp; slow_case) {
 66   const int aligned_mask = BytesPerWord -1;
 67   const int hdr_offset = oopDesc::mark_offset_in_bytes();
 68   assert(hdr != obj &amp;&amp; hdr != disp_hdr &amp;&amp; obj != disp_hdr, &quot;registers must be different&quot;);
 69   Label done;
 70   int null_check_offset = -1;
 71 
 72   verify_oop(obj);
 73 
 74   // save object being locked into the BasicObjectLock
 75   str(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));
 76 
 77   if (UseBiasedLocking) {
 78     assert(scratch != noreg, &quot;should have scratch register at this point&quot;);
 79     null_check_offset = biased_locking_enter(disp_hdr, obj, hdr, scratch, false, done, &amp;slow_case);
 80   } else {
 81     null_check_offset = offset();
 82   }
 83 
 84   // Load object header
 85   ldr(hdr, Address(obj, hdr_offset));
 86   // and mark it as unlocked
 87   orr(hdr, hdr, markWord::unlocked_value);
 88 
 89   if (EnableValhalla &amp;&amp; !UseBiasedLocking) {
 90     // Mask always_locked bit such that we go to the slow path if object is a value type
 91     andr(hdr, hdr, ~markWord::biased_lock_bit_in_place);
 92   }
 93 
 94   // save unlocked object header into the displaced header location on the stack
 95   str(hdr, Address(disp_hdr, 0));
 96   // test if object header is still the same (i.e. unlocked), and if so, store the
 97   // displaced header address in the object header - if it is not the same, get the
 98   // object header instead
 99   lea(rscratch2, Address(obj, hdr_offset));
100   cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, /*fallthough*/NULL);
101   // if the object header was the same, we&#39;re done
102   // if the object header was not the same, it is now in the hdr register
103   // =&gt; test if it is a stack pointer into the same stack (recursive locking), i.e.:
104   //
105   // 1) (hdr &amp; aligned_mask) == 0
106   // 2) sp &lt;= hdr
107   // 3) hdr &lt;= sp + page_size
108   //
109   // these 3 tests can be done by evaluating the following expression:
110   //
111   // (hdr - sp) &amp; (aligned_mask - page_size)
112   //
113   // assuming both the stack pointer and page_size have their least
114   // significant 2 bits cleared and page_size is a power of 2
115   mov(rscratch1, sp);
116   sub(hdr, hdr, rscratch1);
117   ands(hdr, hdr, aligned_mask - os::vm_page_size());
118   // for recursive locking, the result is zero =&gt; save it in the displaced header
119   // location (NULL in the displaced hdr location indicates recursive locking)
120   str(hdr, Address(disp_hdr, 0));
121   // otherwise we don&#39;t care about the result and handle locking via runtime call
122   cbnz(hdr, slow_case);
123   // done
124   bind(done);
125   if (PrintBiasedLockingStatistics) {
126     lea(rscratch2, ExternalAddress((address)BiasedLocking::fast_path_entry_count_addr()));
127     addmw(Address(rscratch2, 0), 1, rscratch1);
128   }
129   return null_check_offset;
130 }
131 
132 
133 void C1_MacroAssembler::unlock_object(Register hdr, Register obj, Register disp_hdr, Label&amp; slow_case) {
134   const int aligned_mask = BytesPerWord -1;
135   const int hdr_offset = oopDesc::mark_offset_in_bytes();
136   assert(hdr != obj &amp;&amp; hdr != disp_hdr &amp;&amp; obj != disp_hdr, &quot;registers must be different&quot;);
137   Label done;
138 
139   if (UseBiasedLocking) {
140     // load object
141     ldr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));
142     biased_locking_exit(obj, hdr, done);
143   }
144 
145   // load displaced header
146   ldr(hdr, Address(disp_hdr, 0));
147   // if the loaded hdr is NULL we had recursive locking
148   // if we had recursive locking, we are done
149   cbz(hdr, done);
150   if (!UseBiasedLocking) {
151     // load object
152     ldr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));
153   }
154   verify_oop(obj);
155   // test if object header is pointing to the displaced header, and if so, restore
156   // the displaced header in the object - if the object header is not pointing to
157   // the displaced header, get the object header instead
158   // if the object header was not pointing to the displaced header,
159   // we do unlocking via runtime call
160   if (hdr_offset) {
161     lea(rscratch1, Address(obj, hdr_offset));
162     cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &amp;slow_case);
163   } else {
164     cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &amp;slow_case);
165   }
166   // done
167   bind(done);
168 }
169 
170 
171 // Defines obj, preserves var_size_in_bytes
172 void C1_MacroAssembler::try_allocate(Register obj, Register var_size_in_bytes, int con_size_in_bytes, Register t1, Register t2, Label&amp; slow_case) {
173   if (UseTLAB) {
174     tlab_allocate(obj, var_size_in_bytes, con_size_in_bytes, t1, t2, slow_case);
175   } else {
176     eden_allocate(obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);
177   }
178 }
179 
180 void C1_MacroAssembler::initialize_header(Register obj, Register klass, Register len, Register t1, Register t2) {
181   assert_different_registers(obj, klass, len);
182   if (UseBiasedLocking &amp;&amp; !len-&gt;is_valid()) {
183     assert_different_registers(obj, klass, len, t1, t2);
184     ldr(t1, Address(klass, Klass::prototype_header_offset()));
185   } else {
186     // This assumes that all prototype bits fit in an int32_t
187     mov(t1, (int32_t)(intptr_t)markWord::prototype().value());
188   }
189   str(t1, Address(obj, oopDesc::mark_offset_in_bytes()));
190 
191   if (UseCompressedClassPointers) { // Take care not to kill klass
192     encode_klass_not_null(t1, klass);
193     strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));
194   } else {
195     str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));
196   }
197 
198   if (len-&gt;is_valid()) {
199     strw(len, Address(obj, arrayOopDesc::length_offset_in_bytes()));
200   } else if (UseCompressedClassPointers) {
201     store_klass_gap(obj, zr);
202   }
203 }
204 
205 // preserves obj, destroys len_in_bytes
206 void C1_MacroAssembler::initialize_body(Register obj, Register len_in_bytes, int hdr_size_in_bytes, Register t1) {
207   assert(hdr_size_in_bytes &gt;= 0, &quot;header size must be positive or 0&quot;);
208   Label done;
209 
210   // len_in_bytes is positive and ptr sized
211   subs(len_in_bytes, len_in_bytes, hdr_size_in_bytes);
212   br(Assembler::EQ, done);
213 
214   // Preserve obj
215   if (hdr_size_in_bytes)
216     add(obj, obj, hdr_size_in_bytes);
217   zero_memory(obj, len_in_bytes, t1);
218   if (hdr_size_in_bytes)
219     sub(obj, obj, hdr_size_in_bytes);
220 
221   bind(done);
222 }
223 
224 
225 void C1_MacroAssembler::allocate_object(Register obj, Register t1, Register t2, int header_size, int object_size, Register klass, Label&amp; slow_case) {
226   assert_different_registers(obj, t1, t2); // XXX really?
227   assert(header_size &gt;= 0 &amp;&amp; object_size &gt;= header_size, &quot;illegal sizes&quot;);
228 
229   try_allocate(obj, noreg, object_size * BytesPerWord, t1, t2, slow_case);
230 
231   initialize_object(obj, klass, noreg, object_size * HeapWordSize, t1, t2, UseTLAB);
232 }
233 
234 void C1_MacroAssembler::initialize_object(Register obj, Register klass, Register var_size_in_bytes, int con_size_in_bytes, Register t1, Register t2, bool is_tlab_allocated) {
235   assert((con_size_in_bytes &amp; MinObjAlignmentInBytesMask) == 0,
236          &quot;con_size_in_bytes is not multiple of alignment&quot;);
237   const int hdr_size_in_bytes = instanceOopDesc::header_size() * HeapWordSize;
238 
239   initialize_header(obj, klass, noreg, t1, t2);
240 
241   if (!(UseTLAB &amp;&amp; ZeroTLAB &amp;&amp; is_tlab_allocated)) {
242      // clear rest of allocated space
243      const Register index = t2;
244      const int threshold = 16 * BytesPerWord;   // approximate break even point for code size (see comments below)
245      if (var_size_in_bytes != noreg) {
246        mov(index, var_size_in_bytes);
247        initialize_body(obj, index, hdr_size_in_bytes, t1);
248      } else if (con_size_in_bytes &lt;= threshold) {
249        // use explicit null stores
250        int i = hdr_size_in_bytes;
251        if (i &lt; con_size_in_bytes &amp;&amp; (con_size_in_bytes % (2 * BytesPerWord))) {
252          str(zr, Address(obj, i));
253          i += BytesPerWord;
254        }
255        for (; i &lt; con_size_in_bytes; i += 2 * BytesPerWord)
256          stp(zr, zr, Address(obj, i));
257      } else if (con_size_in_bytes &gt; hdr_size_in_bytes) {
258        block_comment(&quot;zero memory&quot;);
259       // use loop to null out the fields
260 
261        int words = (con_size_in_bytes - hdr_size_in_bytes) / BytesPerWord;
262        mov(index,  words / 8);
263 
264        const int unroll = 8; // Number of str(zr) instructions we&#39;ll unroll
265        int remainder = words % unroll;
266        lea(rscratch1, Address(obj, hdr_size_in_bytes + remainder * BytesPerWord));
267 
268        Label entry_point, loop;
269        b(entry_point);
270 
271        bind(loop);
272        sub(index, index, 1);
273        for (int i = -unroll; i &lt; 0; i++) {
274          if (-i == remainder)
275            bind(entry_point);
276          str(zr, Address(rscratch1, i * wordSize));
277        }
278        if (remainder == 0)
279          bind(entry_point);
280        add(rscratch1, rscratch1, unroll * wordSize);
281        cbnz(index, loop);
282 
283      }
284   }
285 
286   membar(StoreStore);
287 
288   if (CURRENT_ENV-&gt;dtrace_alloc_probes()) {
289     assert(obj == r0, &quot;must be&quot;);
290     far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::dtrace_object_alloc_id)));
291   }
292 
293   verify_oop(obj);
294 }
295 void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, int f, Register klass, Label&amp; slow_case) {
296   assert_different_registers(obj, len, t1, t2, klass);
297 
298   // determine alignment mask
299   assert(!(BytesPerWord &amp; 1), &quot;must be a multiple of 2 for masking code to work&quot;);
300 
301   // check for negative or excessive length
302   mov(rscratch1, (int32_t)max_array_allocation_length);
303   cmp(len, rscratch1);
304   br(Assembler::HS, slow_case);
305 
306   const Register arr_size = t2; // okay to be the same
307   // align object end
308   mov(arr_size, (int32_t)header_size * BytesPerWord + MinObjAlignmentInBytesMask);
309   add(arr_size, arr_size, len, ext::uxtw, f);
310   andr(arr_size, arr_size, ~MinObjAlignmentInBytesMask);
311 
312   try_allocate(obj, arr_size, 0, t1, t2, slow_case);
313 
314   initialize_header(obj, klass, len, t1, t2);
315 
316   // clear rest of allocated space
317   const Register len_zero = len;
318   initialize_body(obj, arr_size, header_size * BytesPerWord, len_zero);
319 
320   membar(StoreStore);
321 
322   if (CURRENT_ENV-&gt;dtrace_alloc_probes()) {
323     assert(obj == r0, &quot;must be&quot;);
324     far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::dtrace_object_alloc_id)));
325   }
326 
327   verify_oop(obj);
328 }
329 
330 
331 void C1_MacroAssembler::inline_cache_check(Register receiver, Register iCache) {
332   verify_oop(receiver);
333   // explicit NULL check not needed since load from [klass_offset] causes a trap
334   // check against inline cache
335   assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), &quot;must add explicit null check&quot;);
336 
337   cmp_klass(receiver, iCache, rscratch1);
338 }
339 
340 
341 void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes, bool needs_stack_repair, Label* verified_value_entry_label) {
342   assert(bang_size_in_bytes &gt;= framesize, &quot;stack bang size incorrect&quot;);
343   // Make sure there is enough stack space for this method&#39;s activation.
344   // Note that we do this before doing an enter().
345   generate_stack_overflow_check(bang_size_in_bytes);
346 
347   guarantee(needs_stack_repair == false, &quot;Stack repair should not be true&quot;);
348   if (verified_value_entry_label != NULL) {
349     bind(*verified_value_entry_label);
350   }
351 
352   MacroAssembler::build_frame(framesize + 2 * wordSize);
353 }
354 
355 void C1_MacroAssembler::remove_frame(int framesize, bool needs_stack_repair) {
356 
357   guarantee(needs_stack_repair == false, &quot;Stack repair should not be true&quot;);
358 
359   MacroAssembler::remove_frame(framesize + 2 * wordSize);
360 }
361 
362 void C1_MacroAssembler::verified_value_entry() {
363   if (C1Breakpoint || VerifyFPU || !UseStackBanging) {
364     // Verified Entry first instruction should be 5 bytes long for correct
365     // patching by patch_verified_entry().
366     //
367     // C1Breakpoint and VerifyFPU have one byte first instruction.
368     // Also first instruction will be one byte &quot;push(rbp)&quot; if stack banging
369     // code is not generated (see build_frame() above).
370     // For all these cases generate long instruction first.
371     nop();
372   }
373 
374   nop();
375   // build frame
376   // verify_FPU(0, &quot;method_entry&quot;);
377 }
378 
379 int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature *ces, int frame_size_in_bytes, int bang_size_in_bytes, Label&amp; verified_value_entry_label, bool is_value_ro_entry) {
380   // This function required to support for InlineTypePassFieldsAsArgs
381   if (C1Breakpoint || VerifyFPU || !UseStackBanging) {
382     // Verified Entry first instruction should be 5 bytes long for correct
383     // patching by patch_verified_entry().
384     //
385     // C1Breakpoint and VerifyFPU have one byte first instruction.
386     // Also first instruction will be one byte &quot;push(rbp)&quot; if stack banging
387     // code is not generated (see build_frame() above).
388     // For all these cases generate long instruction first.
389     nop();
390   }
391 
392   nop();
393   // verify_FPU(0, &quot;method_entry&quot;);
394 
395   assert(InlineTypePassFieldsAsArgs, &quot;sanity&quot;);
396 
397   GrowableArray&lt;SigEntry&gt;* sig   = &amp;ces-&gt;sig();
398   GrowableArray&lt;SigEntry&gt;* sig_cc = is_value_ro_entry ? &amp;ces-&gt;sig_cc_ro() : &amp;ces-&gt;sig_cc();
399   VMRegPair* regs      = ces-&gt;regs();
400   VMRegPair* regs_cc   = is_value_ro_entry ? ces-&gt;regs_cc_ro() : ces-&gt;regs_cc();
401   int args_on_stack    = ces-&gt;args_on_stack();
402   int args_on_stack_cc = is_value_ro_entry ? ces-&gt;args_on_stack_cc_ro() : ces-&gt;args_on_stack_cc();
403 
404   assert(sig-&gt;length() &lt;= sig_cc-&gt;length(), &quot;Zero-sized value class not allowed!&quot;);
405   BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc-&gt;length());
406   int args_passed = sig-&gt;length();
407   int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);
408 
409   int extra_stack_offset = wordSize; // tos is return address.
410 
411   // Create a temp frame so we can call into runtime. It must be properly set up to accomodate GC.
412   int sp_inc = (args_on_stack - args_on_stack_cc) * VMRegImpl::stack_slot_size;
413   if (sp_inc &gt; 0) {
414     sp_inc = align_up(sp_inc, StackAlignmentInBytes);
415     sub(sp, sp, sp_inc);
416   } else {
417     sp_inc = 0;
418   }
419 
420   sub(sp, sp, frame_size_in_bytes);
421   if (sp_inc &gt; 0) {
422     int real_frame_size = frame_size_in_bytes +
423            + wordSize  // pushed rbp
424            + wordSize  // returned address pushed by the stack extension code
425            + sp_inc;   // stack extension
426     mov(rscratch1, real_frame_size);
427     str(rscratch1, Address(sp, frame_size_in_bytes - wordSize));
428   }
429 
430   // FIXME -- call runtime only if we cannot in-line allocate all the incoming value args.
431   mov(r1, (intptr_t) ces-&gt;method());
432   if (is_value_ro_entry) {
433     far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_value_args_no_receiver_id)));
434   } else {
435     far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_value_args_id)));
436   }
437   int rt_call_offset = offset();
438 
439   // Remove the temp frame
440   add(sp, sp, frame_size_in_bytes);
441 
442   int n = shuffle_value_args(true, is_value_ro_entry, extra_stack_offset, sig_bt, sig_cc,
443                              args_passed_cc, args_on_stack_cc, regs_cc, // from
444                              args_passed, args_on_stack, regs);         // to
445   assert(sp_inc == n, &quot;must be&quot;);
446 
447   if (sp_inc != 0) {
448     // Do the stack banging here, and skip over the stack repair code in the
449     // verified_value_entry (which has a different real_frame_size).
450     assert(sp_inc &gt; 0, &quot;stack should not shrink&quot;);
451     generate_stack_overflow_check(bang_size_in_bytes);
452     decrement(sp, frame_size_in_bytes);
453   }
454 
455   b(verified_value_entry_label);
456   return rt_call_offset;
457 }
458 
459 
460 void C1_MacroAssembler::load_parameter(int offset_in_words, Register reg) {
461   // rbp, + 0: link
462   //     + 1: return address
463   //     + 2: argument with offset 0
464   //     + 3: argument with offset 1
465   //     + 4: ...
466 
467   ldr(reg, Address(rfp, (offset_in_words + 2) * BytesPerWord));
468 }
469 
470 #ifndef PRODUCT
471 
472 void C1_MacroAssembler::verify_stack_oop(int stack_offset) {
473   if (!VerifyOops) return;
474   verify_oop_addr(Address(sp, stack_offset), &quot;oop&quot;);
475 }
476 
477 void C1_MacroAssembler::verify_not_null_oop(Register r) {
478   if (!VerifyOops) return;
479   Label not_null;
480   cbnz(r, not_null);
481   stop(&quot;non-null oop required&quot;);
482   bind(not_null);
483   verify_oop(r);
484 }
485 
486 void C1_MacroAssembler::invalidate_registers(bool inv_r0, bool inv_r19, bool inv_r2, bool inv_r3, bool inv_r4, bool inv_r5) {
487 #ifdef ASSERT
488   static int nn;
489   if (inv_r0) mov(r0, 0xDEAD);
490   if (inv_r19) mov(r19, 0xDEAD);
491   if (inv_r2) mov(r2, nn++);
492   if (inv_r3) mov(r3, 0xDEAD);
493   if (inv_r4) mov(r4, 0xDEAD);
494   if (inv_r5) mov(r5, 0xDEAD);
495 #endif
496 }
497 #endif // ifndef PRODUCT
    </pre>
  </body>
</html>