<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/aarch64/templateInterpreterGenerator_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
<a name="1" id="anc1"></a><span class="line-modified">   2  * Copyright (c) 2003, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   3  * Copyright (c) 2014, 2019, Red Hat Inc. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  29 #include &quot;interpreter/bytecodeHistogram.hpp&quot;
  30 #include &quot;interpreter/interpreter.hpp&quot;
  31 #include &quot;interpreter/interpreterRuntime.hpp&quot;
  32 #include &quot;interpreter/interp_masm.hpp&quot;
  33 #include &quot;interpreter/templateInterpreterGenerator.hpp&quot;
  34 #include &quot;interpreter/templateTable.hpp&quot;
  35 #include &quot;interpreter/bytecodeTracer.hpp&quot;
  36 #include &quot;memory/resourceArea.hpp&quot;
  37 #include &quot;oops/arrayOop.hpp&quot;
  38 #include &quot;oops/methodData.hpp&quot;
  39 #include &quot;oops/method.hpp&quot;
  40 #include &quot;oops/oop.inline.hpp&quot;
  41 #include &quot;oops/valueKlass.hpp&quot;
  42 #include &quot;prims/jvmtiExport.hpp&quot;
  43 #include &quot;prims/jvmtiThreadState.hpp&quot;
  44 #include &quot;runtime/arguments.hpp&quot;
  45 #include &quot;runtime/deoptimization.hpp&quot;
  46 #include &quot;runtime/frame.inline.hpp&quot;
  47 #include &quot;runtime/sharedRuntime.hpp&quot;
  48 #include &quot;runtime/stubRoutines.hpp&quot;
  49 #include &quot;runtime/synchronizer.hpp&quot;
  50 #include &quot;runtime/timer.hpp&quot;
  51 #include &quot;runtime/vframeArray.hpp&quot;
  52 #include &quot;utilities/debug.hpp&quot;
  53 #include &quot;utilities/powerOfTwo.hpp&quot;
  54 #include &lt;sys/types.h&gt;
  55 
  56 #ifndef PRODUCT
  57 #include &quot;oops/method.hpp&quot;
  58 #endif // !PRODUCT
  59 
  60 // Size of interpreter code.  Increase if too small.  Interpreter will
  61 // fail with a guarantee (&quot;not enough space for interpreter generation&quot;);
  62 // if too small.
  63 // Run with +PrintInterpreter to get the VM to print out the size.
  64 // Max size with JVMTI
  65 int TemplateInterpreter::InterpreterCodeSize = 200 * 1024;
  66 
  67 #define __ _masm-&gt;
  68 
  69 //-----------------------------------------------------------------------------
  70 
  71 extern &quot;C&quot; void entry(CodeBuffer*);
  72 
  73 //-----------------------------------------------------------------------------
  74 
  75 address TemplateInterpreterGenerator::generate_slow_signature_handler() {
  76   address entry = __ pc();
  77 
  78   __ andr(esp, esp, -16);
  79   __ mov(c_rarg3, esp);
  80   // rmethod
  81   // rlocals
  82   // c_rarg3: first stack arg - wordSize
  83 
  84   // adjust sp
  85   __ sub(sp, c_rarg3, 18 * wordSize);
  86   __ str(lr, Address(__ pre(sp, -2 * wordSize)));
  87   __ call_VM(noreg,
  88              CAST_FROM_FN_PTR(address,
  89                               InterpreterRuntime::slow_signature_handler),
  90              rmethod, rlocals, c_rarg3);
  91 
  92   // r0: result handler
  93 
  94   // Stack layout:
  95   // rsp: return address           &lt;- sp
  96   //      1 garbage
  97   //      8 integer args (if static first is unused)
  98   //      1 float/double identifiers
  99   //      8 double args
 100   //        stack args              &lt;- esp
 101   //        garbage
 102   //        expression stack bottom
 103   //        bcp (NULL)
 104   //        ...
 105 
 106   // Restore LR
 107   __ ldr(lr, Address(__ post(sp, 2 * wordSize)));
 108 
 109   // Do FP first so we can use c_rarg3 as temp
 110   __ ldrw(c_rarg3, Address(sp, 9 * wordSize)); // float/double identifiers
 111 
 112   for (int i = 0; i &lt; Argument::n_float_register_parameters_c; i++) {
 113     const FloatRegister r = as_FloatRegister(i);
 114 
 115     Label d, done;
 116 
 117     __ tbnz(c_rarg3, i, d);
 118     __ ldrs(r, Address(sp, (10 + i) * wordSize));
 119     __ b(done);
 120     __ bind(d);
 121     __ ldrd(r, Address(sp, (10 + i) * wordSize));
 122     __ bind(done);
 123   }
 124 
 125   // c_rarg0 contains the result from the call of
 126   // InterpreterRuntime::slow_signature_handler so we don&#39;t touch it
 127   // here.  It will be loaded with the JNIEnv* later.
 128   __ ldr(c_rarg1, Address(sp, 1 * wordSize));
 129   for (int i = c_rarg2-&gt;encoding(); i &lt;= c_rarg7-&gt;encoding(); i += 2) {
 130     Register rm = as_Register(i), rn = as_Register(i+1);
 131     __ ldp(rm, rn, Address(sp, i * wordSize));
 132   }
 133 
 134   __ add(sp, sp, 18 * wordSize);
 135   __ ret(lr);
 136 
 137   return entry;
 138 }
 139 
 140 
 141 //
 142 // Various method entries
 143 //
 144 
 145 address TemplateInterpreterGenerator::generate_math_entry(AbstractInterpreter::MethodKind kind) {
 146   // rmethod: Method*
 147   // r13: sender sp
 148   // esp: args
 149 
 150   if (!InlineIntrinsics) return NULL; // Generate a vanilla entry
 151 
 152   // These don&#39;t need a safepoint check because they aren&#39;t virtually
 153   // callable. We won&#39;t enter these intrinsics from compiled code.
 154   // If in the future we added an intrinsic which was virtually callable
 155   // we&#39;d have to worry about how to safepoint so that this code is used.
 156 
 157   // mathematical functions inlined by compiler
 158   // (interpreter must provide identical implementation
 159   // in order to avoid monotonicity bugs when switching
 160   // from interpreter to compiler in the middle of some
 161   // computation)
 162   //
 163   // stack:
 164   //        [ arg ] &lt;-- esp
 165   //        [ arg ]
 166   // retaddr in lr
 167 
 168   address entry_point = NULL;
 169   Register continuation = lr;
 170   switch (kind) {
 171   case Interpreter::java_lang_math_abs:
 172     entry_point = __ pc();
 173     __ ldrd(v0, Address(esp));
 174     __ fabsd(v0, v0);
 175     __ mov(sp, r13); // Restore caller&#39;s SP
 176     break;
 177   case Interpreter::java_lang_math_sqrt:
 178     entry_point = __ pc();
 179     __ ldrd(v0, Address(esp));
 180     __ fsqrtd(v0, v0);
 181     __ mov(sp, r13);
 182     break;
 183   case Interpreter::java_lang_math_sin :
 184   case Interpreter::java_lang_math_cos :
 185   case Interpreter::java_lang_math_tan :
 186   case Interpreter::java_lang_math_log :
 187   case Interpreter::java_lang_math_log10 :
 188   case Interpreter::java_lang_math_exp :
 189     entry_point = __ pc();
 190     __ ldrd(v0, Address(esp));
 191     __ mov(sp, r13);
 192     __ mov(r19, lr);
 193     continuation = r19;  // The first callee-saved register
 194     generate_transcendental_entry(kind, 1);
 195     break;
 196   case Interpreter::java_lang_math_pow :
 197     entry_point = __ pc();
 198     __ mov(r19, lr);
 199     continuation = r19;
 200     __ ldrd(v0, Address(esp, 2 * Interpreter::stackElementSize));
 201     __ ldrd(v1, Address(esp));
 202     __ mov(sp, r13);
 203     generate_transcendental_entry(kind, 2);
 204     break;
 205   case Interpreter::java_lang_math_fmaD :
 206     if (UseFMA) {
 207       entry_point = __ pc();
 208       __ ldrd(v0, Address(esp, 4 * Interpreter::stackElementSize));
 209       __ ldrd(v1, Address(esp, 2 * Interpreter::stackElementSize));
 210       __ ldrd(v2, Address(esp));
 211       __ fmaddd(v0, v0, v1, v2);
 212       __ mov(sp, r13); // Restore caller&#39;s SP
 213     }
 214     break;
 215   case Interpreter::java_lang_math_fmaF :
 216     if (UseFMA) {
 217       entry_point = __ pc();
 218       __ ldrs(v0, Address(esp, 2 * Interpreter::stackElementSize));
 219       __ ldrs(v1, Address(esp, Interpreter::stackElementSize));
 220       __ ldrs(v2, Address(esp));
 221       __ fmadds(v0, v0, v1, v2);
 222       __ mov(sp, r13); // Restore caller&#39;s SP
 223     }
 224     break;
 225   default:
 226     ;
 227   }
 228   if (entry_point) {
 229     __ br(continuation);
 230   }
 231 
 232   return entry_point;
 233 }
 234 
 235   // double trigonometrics and transcendentals
 236   // static jdouble dsin(jdouble x);
 237   // static jdouble dcos(jdouble x);
 238   // static jdouble dtan(jdouble x);
 239   // static jdouble dlog(jdouble x);
 240   // static jdouble dlog10(jdouble x);
 241   // static jdouble dexp(jdouble x);
 242   // static jdouble dpow(jdouble x, jdouble y);
 243 
 244 void TemplateInterpreterGenerator::generate_transcendental_entry(AbstractInterpreter::MethodKind kind, int fpargs) {
 245   address fn;
 246   switch (kind) {
 247   case Interpreter::java_lang_math_sin :
 248     if (StubRoutines::dsin() == NULL) {
 249       fn = CAST_FROM_FN_PTR(address, SharedRuntime::dsin);
 250     } else {
 251       fn = CAST_FROM_FN_PTR(address, StubRoutines::dsin());
 252     }
 253     break;
 254   case Interpreter::java_lang_math_cos :
 255     if (StubRoutines::dcos() == NULL) {
 256       fn = CAST_FROM_FN_PTR(address, SharedRuntime::dcos);
 257     } else {
 258       fn = CAST_FROM_FN_PTR(address, StubRoutines::dcos());
 259     }
 260     break;
 261   case Interpreter::java_lang_math_tan :
 262     if (StubRoutines::dtan() == NULL) {
 263       fn = CAST_FROM_FN_PTR(address, SharedRuntime::dtan);
 264     } else {
 265       fn = CAST_FROM_FN_PTR(address, StubRoutines::dtan());
 266     }
 267     break;
 268   case Interpreter::java_lang_math_log :
 269     if (StubRoutines::dlog() == NULL) {
 270       fn = CAST_FROM_FN_PTR(address, SharedRuntime::dlog);
 271     } else {
 272       fn = CAST_FROM_FN_PTR(address, StubRoutines::dlog());
 273     }
 274     break;
 275   case Interpreter::java_lang_math_log10 :
 276     if (StubRoutines::dlog10() == NULL) {
 277       fn = CAST_FROM_FN_PTR(address, SharedRuntime::dlog10);
 278     } else {
 279       fn = CAST_FROM_FN_PTR(address, StubRoutines::dlog10());
 280     }
 281     break;
 282   case Interpreter::java_lang_math_exp :
 283     if (StubRoutines::dexp() == NULL) {
 284       fn = CAST_FROM_FN_PTR(address, SharedRuntime::dexp);
 285     } else {
 286       fn = CAST_FROM_FN_PTR(address, StubRoutines::dexp());
 287     }
 288     break;
 289   case Interpreter::java_lang_math_pow :
 290     if (StubRoutines::dpow() == NULL) {
 291       fn = CAST_FROM_FN_PTR(address, SharedRuntime::dpow);
 292     } else {
 293       fn = CAST_FROM_FN_PTR(address, StubRoutines::dpow());
 294     }
 295     break;
 296   default:
 297     ShouldNotReachHere();
 298     fn = NULL;  // unreachable
 299   }
 300   __ mov(rscratch1, fn);
 301   __ blr(rscratch1);
 302 }
 303 
 304 // Abstract method entry
 305 // Attempt to execute abstract method. Throw exception
 306 address TemplateInterpreterGenerator::generate_abstract_entry(void) {
 307   // rmethod: Method*
 308   // r13: sender SP
 309 
 310   address entry_point = __ pc();
 311 
 312   // abstract method entry
 313 
 314   //  pop return address, reset last_sp to NULL
 315   __ empty_expression_stack();
 316   __ restore_bcp();      // bcp must be correct for exception handler   (was destroyed)
 317   __ restore_locals();   // make sure locals pointer is correct as well (was destroyed)
 318 
 319   // throw exception
 320   __ call_VM(noreg, CAST_FROM_FN_PTR(address,
 321                                      InterpreterRuntime::throw_AbstractMethodErrorWithMethod),
 322                                      rmethod);
 323   // the call_VM checks for exception, so we should never return here.
 324   __ should_not_reach_here();
 325 
 326   return entry_point;
 327 }
 328 
 329 address TemplateInterpreterGenerator::generate_StackOverflowError_handler() {
 330   address entry = __ pc();
 331 
 332 #ifdef ASSERT
 333   {
 334     Label L;
 335     __ ldr(rscratch1, Address(rfp,
 336                        frame::interpreter_frame_monitor_block_top_offset *
 337                        wordSize));
 338     __ mov(rscratch2, sp);
 339     __ cmp(rscratch1, rscratch2); // maximal rsp for current rfp (stack
 340                            // grows negative)
 341     __ br(Assembler::HS, L); // check if frame is complete
 342     __ stop (&quot;interpreter frame not set up&quot;);
 343     __ bind(L);
 344   }
 345 #endif // ASSERT
 346   // Restore bcp under the assumption that the current frame is still
 347   // interpreted
 348   __ restore_bcp();
 349 
 350   // expression stack must be empty before entering the VM if an
 351   // exception happened
 352   __ empty_expression_stack();
 353   // throw exception
 354   __ call_VM(noreg,
 355              CAST_FROM_FN_PTR(address,
 356                               InterpreterRuntime::throw_StackOverflowError));
 357   return entry;
 358 }
 359 
 360 address TemplateInterpreterGenerator::generate_ArrayIndexOutOfBounds_handler() {
 361   address entry = __ pc();
 362   // expression stack must be empty before entering the VM if an
 363   // exception happened
 364   __ empty_expression_stack();
 365   // setup parameters
 366 
 367   // ??? convention: expect aberrant index in register r1
 368   __ movw(c_rarg2, r1);
 369   // ??? convention: expect array in register r3
 370   __ mov(c_rarg1, r3);
 371   __ call_VM(noreg,
 372              CAST_FROM_FN_PTR(address,
 373                               InterpreterRuntime::
 374                               throw_ArrayIndexOutOfBoundsException),
 375              c_rarg1, c_rarg2);
 376   return entry;
 377 }
 378 
 379 address TemplateInterpreterGenerator::generate_ClassCastException_handler() {
 380   address entry = __ pc();
 381 
 382   // object is at TOS
 383   __ pop(c_rarg1);
 384 
 385   // expression stack must be empty before entering the VM if an
 386   // exception happened
 387   __ empty_expression_stack();
 388 
 389   __ call_VM(noreg,
 390              CAST_FROM_FN_PTR(address,
 391                               InterpreterRuntime::
 392                               throw_ClassCastException),
 393              c_rarg1);
 394   return entry;
 395 }
 396 
 397 address TemplateInterpreterGenerator::generate_exception_handler_common(
 398         const char* name, const char* message, bool pass_oop) {
 399   assert(!pass_oop || message == NULL, &quot;either oop or message but not both&quot;);
 400   address entry = __ pc();
 401   if (pass_oop) {
 402     // object is at TOS
 403     __ pop(c_rarg2);
 404   }
 405   // expression stack must be empty before entering the VM if an
 406   // exception happened
 407   __ empty_expression_stack();
 408   // setup parameters
 409   __ lea(c_rarg1, Address((address)name));
 410   if (pass_oop) {
 411     __ call_VM(r0, CAST_FROM_FN_PTR(address,
 412                                     InterpreterRuntime::
 413                                     create_klass_exception),
 414                c_rarg1, c_rarg2);
 415   } else {
 416     // kind of lame ExternalAddress can&#39;t take NULL because
 417     // external_word_Relocation will assert.
 418     if (message != NULL) {
 419       __ lea(c_rarg2, Address((address)message));
 420     } else {
 421       __ mov(c_rarg2, NULL_WORD);
 422     }
 423     __ call_VM(r0,
 424                CAST_FROM_FN_PTR(address, InterpreterRuntime::create_exception),
 425                c_rarg1, c_rarg2);
 426   }
 427   // throw exception
 428   __ b(address(Interpreter::throw_exception_entry()));
 429   return entry;
 430 }
 431 
 432 address TemplateInterpreterGenerator::generate_return_entry_for(TosState state, int step, size_t index_size) {
 433   address entry = __ pc();
 434 
 435   // Restore stack bottom in case i2c adjusted stack
 436   __ ldr(esp, Address(rfp, frame::interpreter_frame_last_sp_offset * wordSize));
 437   // and NULL it as marker that esp is now tos until next java call
 438   __ str(zr, Address(rfp, frame::interpreter_frame_last_sp_offset * wordSize));
 439 
<a name="2" id="anc2"></a><span class="line-modified"> 440   if (state == atos &amp;&amp; InlineTypeReturnedAsFields) {</span>
 441     __ store_value_type_fields_to_buf(NULL, true);
 442   }
 443 
 444   __ restore_bcp();
 445   __ restore_locals();
 446   __ restore_constant_pool_cache();
 447   __ get_method(rmethod);
 448 
 449   if (state == atos) {
 450     Register obj = r0;
 451     Register mdp = r1;
 452     Register tmp = r2;
 453     __ profile_return_type(mdp, obj, tmp);
 454   }
 455 
 456   // Pop N words from the stack
 457   __ get_cache_and_index_at_bcp(r1, r2, 1, index_size);
 458   __ ldr(r1, Address(r1, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()));
 459   __ andr(r1, r1, ConstantPoolCacheEntry::parameter_size_mask);
 460 
 461   __ add(esp, esp, r1, Assembler::LSL, 3);
 462 
 463   // Restore machine SP
 464   __ ldr(rscratch1, Address(rmethod, Method::const_offset()));
 465   __ ldrh(rscratch1, Address(rscratch1, ConstMethod::max_stack_offset()));
 466   __ add(rscratch1, rscratch1, frame::interpreter_frame_monitor_size() + 2);
 467   __ ldr(rscratch2,
 468          Address(rfp, frame::interpreter_frame_initial_sp_offset * wordSize));
 469   __ sub(rscratch1, rscratch2, rscratch1, ext::uxtw, 3);
 470   __ andr(sp, rscratch1, -16);
 471 
 472  __ check_and_handle_popframe(rthread);
 473  __ check_and_handle_earlyret(rthread);
 474 
 475   __ get_dispatch();
 476   __ dispatch_next(state, step);
 477 
 478   return entry;
 479 }
 480 
 481 address TemplateInterpreterGenerator::generate_deopt_entry_for(TosState state,
 482                                                                int step,
 483                                                                address continuation) {
 484   address entry = __ pc();
 485   __ restore_bcp();
 486   __ restore_locals();
 487   __ restore_constant_pool_cache();
 488   __ get_method(rmethod);
 489   __ get_dispatch();
 490 
 491   // Calculate stack limit
 492   __ ldr(rscratch1, Address(rmethod, Method::const_offset()));
 493   __ ldrh(rscratch1, Address(rscratch1, ConstMethod::max_stack_offset()));
 494   __ add(rscratch1, rscratch1, frame::interpreter_frame_monitor_size() + 2);
 495   __ ldr(rscratch2,
 496          Address(rfp, frame::interpreter_frame_initial_sp_offset * wordSize));
 497   __ sub(rscratch1, rscratch2, rscratch1, ext::uxtx, 3);
 498   __ andr(sp, rscratch1, -16);
 499 
 500   // Restore expression stack pointer
 501   __ ldr(esp, Address(rfp, frame::interpreter_frame_last_sp_offset * wordSize));
 502   // NULL last_sp until next java call
 503   __ str(zr, Address(rfp, frame::interpreter_frame_last_sp_offset * wordSize));
 504 
 505 #if INCLUDE_JVMCI
 506   // Check if we need to take lock at entry of synchronized method.  This can
 507   // only occur on method entry so emit it only for vtos with step 0.
 508   if ((EnableJVMCI || UseAOT) &amp;&amp; state == vtos &amp;&amp; step == 0) {
 509     Label L;
 510     __ ldrb(rscratch1, Address(rthread, JavaThread::pending_monitorenter_offset()));
 511     __ cbz(rscratch1, L);
 512     // Clear flag.
 513     __ strb(zr, Address(rthread, JavaThread::pending_monitorenter_offset()));
 514     // Take lock.
 515     lock_method();
 516     __ bind(L);
 517   } else {
 518 #ifdef ASSERT
 519     if (EnableJVMCI) {
 520       Label L;
 521       __ ldrb(rscratch1, Address(rthread, JavaThread::pending_monitorenter_offset()));
 522       __ cbz(rscratch1, L);
 523       __ stop(&quot;unexpected pending monitor in deopt entry&quot;);
 524       __ bind(L);
 525     }
 526 #endif
 527   }
 528 #endif
 529   // handle exceptions
 530   {
 531     Label L;
 532     __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));
 533     __ cbz(rscratch1, L);
 534     __ call_VM(noreg,
 535                CAST_FROM_FN_PTR(address,
 536                                 InterpreterRuntime::throw_pending_exception));
 537     __ should_not_reach_here();
 538     __ bind(L);
 539   }
 540 
 541   if (continuation == NULL) {
 542     __ dispatch_next(state, step);
 543   } else {
 544     __ jump_to_entry(continuation);
 545   }
 546   return entry;
 547 }
 548 
 549 address TemplateInterpreterGenerator::generate_result_handler_for(
 550         BasicType type) {
 551     address entry = __ pc();
 552   switch (type) {
 553   case T_BOOLEAN: __ c2bool(r0);         break;
 554   case T_CHAR   : __ uxth(r0, r0);       break;
 555   case T_BYTE   : __ sxtb(r0, r0);        break;
 556   case T_SHORT  : __ sxth(r0, r0);        break;
 557   case T_INT    : __ uxtw(r0, r0);        break;  // FIXME: We almost certainly don&#39;t need this
 558   case T_LONG   : /* nothing to do */        break;
 559   case T_VOID   : /* nothing to do */        break;
 560   case T_FLOAT  : /* nothing to do */        break;
 561   case T_DOUBLE : /* nothing to do */        break;
 562   case T_VALUETYPE: // fall through (value types are handled with oops)
 563   case T_OBJECT :
 564     // retrieve result from frame
 565     __ ldr(r0, Address(rfp, frame::interpreter_frame_oop_temp_offset*wordSize));
 566     // and verify it
 567     __ verify_oop(r0);
 568     break;
 569   default       : ShouldNotReachHere();
 570   }
 571   __ ret(lr);                                  // return from result handler
 572   return entry;
 573 }
 574 
 575 address TemplateInterpreterGenerator::generate_safept_entry_for(
 576         TosState state,
 577         address runtime_entry) {
 578   address entry = __ pc();
 579   __ push(state);
 580   __ call_VM(noreg, runtime_entry);
 581   __ membar(Assembler::AnyAny);
 582   __ dispatch_via(vtos, Interpreter::_normal_table.table_for(vtos));
 583   return entry;
 584 }
 585 
 586 // Helpers for commoning out cases in the various type of method entries.
 587 //
 588 
 589 
 590 // increment invocation count &amp; check for overflow
 591 //
 592 // Note: checking for negative value instead of overflow
 593 //       so we have a &#39;sticky&#39; overflow test
 594 //
 595 // rmethod: method
 596 //
 597 void TemplateInterpreterGenerator::generate_counter_incr(
 598         Label* overflow,
 599         Label* profile_method,
 600         Label* profile_method_continue) {
 601   Label done;
 602   // Note: In tiered we increment either counters in Method* or in MDO depending if we&#39;re profiling or not.
 603   if (TieredCompilation) {
 604     int increment = InvocationCounter::count_increment;
 605     Label no_mdo;
 606     if (ProfileInterpreter) {
 607       // Are we profiling?
 608       __ ldr(r0, Address(rmethod, Method::method_data_offset()));
 609       __ cbz(r0, no_mdo);
 610       // Increment counter in the MDO
 611       const Address mdo_invocation_counter(r0, in_bytes(MethodData::invocation_counter_offset()) +
 612                                                 in_bytes(InvocationCounter::counter_offset()));
 613       const Address mask(r0, in_bytes(MethodData::invoke_mask_offset()));
 614       __ increment_mask_and_jump(mdo_invocation_counter, increment, mask, rscratch1, rscratch2, false, Assembler::EQ, overflow);
 615       __ b(done);
 616     }
 617     __ bind(no_mdo);
 618     // Increment counter in MethodCounters
 619     const Address invocation_counter(rscratch2,
 620                   MethodCounters::invocation_counter_offset() +
 621                   InvocationCounter::counter_offset());
 622     __ get_method_counters(rmethod, rscratch2, done);
 623     const Address mask(rscratch2, in_bytes(MethodCounters::invoke_mask_offset()));
 624     __ increment_mask_and_jump(invocation_counter, increment, mask, rscratch1, r1, false, Assembler::EQ, overflow);
 625     __ bind(done);
 626   } else { // not TieredCompilation
 627     const Address backedge_counter(rscratch2,
 628                   MethodCounters::backedge_counter_offset() +
 629                   InvocationCounter::counter_offset());
 630     const Address invocation_counter(rscratch2,
 631                   MethodCounters::invocation_counter_offset() +
 632                   InvocationCounter::counter_offset());
 633 
 634     __ get_method_counters(rmethod, rscratch2, done);
 635 
 636     if (ProfileInterpreter) { // %%% Merge this into MethodData*
 637       __ ldrw(r1, Address(rscratch2, MethodCounters::interpreter_invocation_counter_offset()));
 638       __ addw(r1, r1, 1);
 639       __ strw(r1, Address(rscratch2, MethodCounters::interpreter_invocation_counter_offset()));
 640     }
 641     // Update standard invocation counters
 642     __ ldrw(r1, invocation_counter);
 643     __ ldrw(r0, backedge_counter);
 644 
 645     __ addw(r1, r1, InvocationCounter::count_increment);
 646     __ andw(r0, r0, InvocationCounter::count_mask_value);
 647 
 648     __ strw(r1, invocation_counter);
 649     __ addw(r0, r0, r1);                // add both counters
 650 
 651     // profile_method is non-null only for interpreted method so
 652     // profile_method != NULL == !native_call
 653 
 654     if (ProfileInterpreter &amp;&amp; profile_method != NULL) {
 655       // Test to see if we should create a method data oop
 656       __ ldr(rscratch2, Address(rmethod, Method::method_counters_offset()));
 657       __ ldrw(rscratch2, Address(rscratch2, in_bytes(MethodCounters::interpreter_profile_limit_offset())));
 658       __ cmpw(r0, rscratch2);
 659       __ br(Assembler::LT, *profile_method_continue);
 660 
 661       // if no method data exists, go to profile_method
 662       __ test_method_data_pointer(rscratch2, *profile_method);
 663     }
 664 
 665     {
 666       __ ldr(rscratch2, Address(rmethod, Method::method_counters_offset()));
 667       __ ldrw(rscratch2, Address(rscratch2, in_bytes(MethodCounters::interpreter_invocation_limit_offset())));
 668       __ cmpw(r0, rscratch2);
 669       __ br(Assembler::HS, *overflow);
 670     }
 671     __ bind(done);
 672   }
 673 }
 674 
 675 void TemplateInterpreterGenerator::generate_counter_overflow(Label&amp; do_continue) {
 676 
 677   // Asm interpreter on entry
 678   // On return (i.e. jump to entry_point) [ back to invocation of interpreter ]
 679   // Everything as it was on entry
 680 
 681   // InterpreterRuntime::frequency_counter_overflow takes two
 682   // arguments, the first (thread) is passed by call_VM, the second
 683   // indicates if the counter overflow occurs at a backwards branch
 684   // (NULL bcp).  We pass zero for it.  The call returns the address
 685   // of the verified entry point for the method or NULL if the
 686   // compilation did not complete (either went background or bailed
 687   // out).
 688   __ mov(c_rarg1, 0);
 689   __ call_VM(noreg,
 690              CAST_FROM_FN_PTR(address,
 691                               InterpreterRuntime::frequency_counter_overflow),
 692              c_rarg1);
 693 
 694   __ b(do_continue);
 695 }
 696 
 697 // See if we&#39;ve got enough room on the stack for locals plus overhead
 698 // below JavaThread::stack_overflow_limit(). If not, throw a StackOverflowError
 699 // without going through the signal handler, i.e., reserved and yellow zones
 700 // will not be made usable. The shadow zone must suffice to handle the
 701 // overflow.
 702 // The expression stack grows down incrementally, so the normal guard
 703 // page mechanism will work for that.
 704 //
 705 // NOTE: Since the additional locals are also always pushed (wasn&#39;t
 706 // obvious in generate_method_entry) so the guard should work for them
 707 // too.
 708 //
 709 // Args:
 710 //      r3: number of additional locals this frame needs (what we must check)
 711 //      rmethod: Method*
 712 //
 713 // Kills:
 714 //      r0
 715 void TemplateInterpreterGenerator::generate_stack_overflow_check(void) {
 716 
 717   // monitor entry size: see picture of stack set
 718   // (generate_method_entry) and frame_amd64.hpp
 719   const int entry_size = frame::interpreter_frame_monitor_size() * wordSize;
 720 
 721   // total overhead size: entry_size + (saved rbp through expr stack
 722   // bottom).  be sure to change this if you add/subtract anything
 723   // to/from the overhead area
 724   const int overhead_size =
 725     -(frame::interpreter_frame_initial_sp_offset * wordSize) + entry_size;
 726 
 727   const int page_size = os::vm_page_size();
 728 
 729   Label after_frame_check;
 730 
 731   // see if the frame is greater than one page in size. If so,
 732   // then we need to verify there is enough stack space remaining
 733   // for the additional locals.
 734   //
 735   // Note that we use SUBS rather than CMP here because the immediate
 736   // field of this instruction may overflow.  SUBS can cope with this
 737   // because it is a macro that will expand to some number of MOV
 738   // instructions and a register operation.
 739   __ subs(rscratch1, r3, (page_size - overhead_size) / Interpreter::stackElementSize);
 740   __ br(Assembler::LS, after_frame_check);
 741 
 742   // compute rsp as if this were going to be the last frame on
 743   // the stack before the red zone
 744 
 745   // locals + overhead, in bytes
 746   __ mov(r0, overhead_size);
 747   __ add(r0, r0, r3, Assembler::LSL, Interpreter::logStackElementSize);  // 2 slots per parameter.
 748 
 749   const Address stack_limit(rthread, JavaThread::stack_overflow_limit_offset());
 750   __ ldr(rscratch1, stack_limit);
 751 
 752 #ifdef ASSERT
 753   Label limit_okay;
 754   // Verify that thread stack limit is non-zero.
 755   __ cbnz(rscratch1, limit_okay);
 756   __ stop(&quot;stack overflow limit is zero&quot;);
 757   __ bind(limit_okay);
 758 #endif
 759 
 760   // Add stack limit to locals.
 761   __ add(r0, r0, rscratch1);
 762 
 763   // Check against the current stack bottom.
 764   __ cmp(sp, r0);
 765   __ br(Assembler::HI, after_frame_check);
 766 
 767   // Remove the incoming args, peeling the machine SP back to where it
 768   // was in the caller.  This is not strictly necessary, but unless we
 769   // do so the stack frame may have a garbage FP; this ensures a
 770   // correct call stack that we can always unwind.  The ANDR should be
 771   // unnecessary because the sender SP in r13 is always aligned, but
 772   // it doesn&#39;t hurt.
 773   __ andr(sp, r13, -16);
 774 
 775   // Note: the restored frame is not necessarily interpreted.
 776   // Use the shared runtime version of the StackOverflowError.
 777   assert(StubRoutines::throw_StackOverflowError_entry() != NULL, &quot;stub not yet generated&quot;);
 778   __ far_jump(RuntimeAddress(StubRoutines::throw_StackOverflowError_entry()));
 779 
 780   // all done with frame size check
 781   __ bind(after_frame_check);
 782 }
 783 
 784 // Allocate monitor and lock method (asm interpreter)
 785 //
 786 // Args:
 787 //      rmethod: Method*
 788 //      rlocals: locals
 789 //
 790 // Kills:
 791 //      r0
 792 //      c_rarg0, c_rarg1, c_rarg2, c_rarg3, ...(param regs)
 793 //      rscratch1, rscratch2 (scratch regs)
 794 void TemplateInterpreterGenerator::lock_method() {
 795   // synchronize method
 796   const Address access_flags(rmethod, Method::access_flags_offset());
 797   const Address monitor_block_top(
 798         rfp,
 799         frame::interpreter_frame_monitor_block_top_offset * wordSize);
 800   const int entry_size = frame::interpreter_frame_monitor_size() * wordSize;
 801 
 802 #ifdef ASSERT
 803   {
 804     Label L;
 805     __ ldrw(r0, access_flags);
 806     __ tst(r0, JVM_ACC_SYNCHRONIZED);
 807     __ br(Assembler::NE, L);
 808     __ stop(&quot;method doesn&#39;t need synchronization&quot;);
 809     __ bind(L);
 810   }
 811 #endif // ASSERT
 812 
 813   // get synchronization object
 814   {
 815     Label done;
 816     __ ldrw(r0, access_flags);
 817     __ tst(r0, JVM_ACC_STATIC);
 818     // get receiver (assume this is frequent case)
 819     __ ldr(r0, Address(rlocals, Interpreter::local_offset_in_bytes(0)));
 820     __ br(Assembler::EQ, done);
 821     __ load_mirror(r0, rmethod);
 822 
 823 #ifdef ASSERT
 824     {
 825       Label L;
 826       __ cbnz(r0, L);
 827       __ stop(&quot;synchronization object is NULL&quot;);
 828       __ bind(L);
 829     }
 830 #endif // ASSERT
 831 
 832     __ bind(done);
 833     __ resolve(IS_NOT_NULL, r0);
 834   }
 835 
 836   // add space for monitor &amp; lock
 837   __ sub(sp, sp, entry_size); // add space for a monitor entry
 838   __ sub(esp, esp, entry_size);
 839   __ mov(rscratch1, esp);
 840   __ str(rscratch1, monitor_block_top);  // set new monitor block top
 841   // store object
 842   __ str(r0, Address(esp, BasicObjectLock::obj_offset_in_bytes()));
 843   __ mov(c_rarg1, esp); // object address
 844   __ lock_object(c_rarg1);
 845 }
 846 
 847 // Generate a fixed interpreter frame. This is identical setup for
 848 // interpreted methods and for native methods hence the shared code.
 849 //
 850 // Args:
 851 //      lr: return address
 852 //      rmethod: Method*
 853 //      rlocals: pointer to locals
 854 //      rcpool: cp cache
 855 //      stack_pointer: previous sp
 856 void TemplateInterpreterGenerator::generate_fixed_frame(bool native_call) {
 857   // initialize fixed part of activation frame
 858   if (native_call) {
 859     __ sub(esp, sp, 14 *  wordSize);
 860     __ mov(rbcp, zr);
 861     __ stp(esp, zr, Address(__ pre(sp, -14 * wordSize)));
 862     // add 2 zero-initialized slots for native calls
 863     __ stp(zr, zr, Address(sp, 12 * wordSize));
 864   } else {
 865     __ sub(esp, sp, 12 *  wordSize);
 866     __ ldr(rscratch1, Address(rmethod, Method::const_offset()));      // get ConstMethod
 867     __ add(rbcp, rscratch1, in_bytes(ConstMethod::codes_offset())); // get codebase
 868     __ stp(esp, rbcp, Address(__ pre(sp, -12 * wordSize)));
 869   }
 870 
 871   if (ProfileInterpreter) {
 872     Label method_data_continue;
 873     __ ldr(rscratch1, Address(rmethod, Method::method_data_offset()));
 874     __ cbz(rscratch1, method_data_continue);
 875     __ lea(rscratch1, Address(rscratch1, in_bytes(MethodData::data_offset())));
 876     __ bind(method_data_continue);
 877     __ stp(rscratch1, rmethod, Address(sp, 6 * wordSize));  // save Method* and mdp (method data pointer)
 878   } else {
 879     __ stp(zr, rmethod, Address(sp, 6 * wordSize));        // save Method* (no mdp)
 880   }
 881 
 882   // Get mirror and store it in the frame as GC root for this Method*
 883   __ load_mirror(r10, rmethod);
 884   __ stp(r10, zr, Address(sp, 4 * wordSize));
 885 
 886   __ ldr(rcpool, Address(rmethod, Method::const_offset()));
 887   __ ldr(rcpool, Address(rcpool, ConstMethod::constants_offset()));
 888   __ ldr(rcpool, Address(rcpool, ConstantPool::cache_offset_in_bytes()));
 889   __ stp(rlocals, rcpool, Address(sp, 2 * wordSize));
 890 
 891   __ stp(rfp, lr, Address(sp, 10 * wordSize));
 892   __ lea(rfp, Address(sp, 10 * wordSize));
 893 
 894   // set sender sp
 895   // leave last_sp as null
 896   __ stp(zr, r13, Address(sp, 8 * wordSize));
 897 
 898   // Move SP out of the way
 899   if (! native_call) {
 900     __ ldr(rscratch1, Address(rmethod, Method::const_offset()));
 901     __ ldrh(rscratch1, Address(rscratch1, ConstMethod::max_stack_offset()));
 902     __ add(rscratch1, rscratch1, frame::interpreter_frame_monitor_size() + 2);
 903     __ sub(rscratch1, sp, rscratch1, ext::uxtw, 3);
 904     __ andr(sp, rscratch1, -16);
 905   }
 906 }
 907 
 908 // End of helpers
 909 
 910 // Various method entries
 911 //------------------------------------------------------------------------------------------------------------------------
 912 //
 913 //
 914 
 915 // Method entry for java.lang.ref.Reference.get.
 916 address TemplateInterpreterGenerator::generate_Reference_get_entry(void) {
 917   // Code: _aload_0, _getfield, _areturn
 918   // parameter size = 1
 919   //
 920   // The code that gets generated by this routine is split into 2 parts:
 921   //    1. The &quot;intrinsified&quot; code for G1 (or any SATB based GC),
 922   //    2. The slow path - which is an expansion of the regular method entry.
 923   //
 924   // Notes:-
 925   // * In the G1 code we do not check whether we need to block for
 926   //   a safepoint. If G1 is enabled then we must execute the specialized
 927   //   code for Reference.get (except when the Reference object is null)
 928   //   so that we can log the value in the referent field with an SATB
 929   //   update buffer.
 930   //   If the code for the getfield template is modified so that the
 931   //   G1 pre-barrier code is executed when the current method is
 932   //   Reference.get() then going through the normal method entry
 933   //   will be fine.
 934   // * The G1 code can, however, check the receiver object (the instance
 935   //   of java.lang.Reference) and jump to the slow path if null. If the
 936   //   Reference object is null then we obviously cannot fetch the referent
 937   //   and so we don&#39;t need to call the G1 pre-barrier. Thus we can use the
 938   //   regular method entry code to generate the NPE.
 939   //
 940   // This code is based on generate_accessor_entry.
 941   //
 942   // rmethod: Method*
 943   // r13: senderSP must preserve for slow path, set SP to it on fast path
 944 
 945   // LR is live.  It must be saved around calls.
 946 
 947   address entry = __ pc();
 948 
 949   const int referent_offset = java_lang_ref_Reference::referent_offset;
 950   guarantee(referent_offset &gt; 0, &quot;referent offset not initialized&quot;);
 951 
 952   Label slow_path;
 953   const Register local_0 = c_rarg0;
 954   // Check if local 0 != NULL
 955   // If the receiver is null then it is OK to jump to the slow path.
 956   __ ldr(local_0, Address(esp, 0));
 957   __ cbz(local_0, slow_path);
 958 
 959   __ mov(r19, r13);   // Move senderSP to a callee-saved register
 960 
 961   // Load the value of the referent field.
 962   const Address field_address(local_0, referent_offset);
 963   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 964   bs-&gt;load_at(_masm, IN_HEAP | ON_WEAK_OOP_REF, T_OBJECT, local_0, field_address, /*tmp1*/ rscratch2, /*tmp2*/ rscratch1);
 965 
 966   // areturn
 967   __ andr(sp, r19, -16);  // done with stack
 968   __ ret(lr);
 969 
 970   // generate a vanilla interpreter entry as the slow path
 971   __ bind(slow_path);
 972   __ jump_to_entry(Interpreter::entry_for_kind(Interpreter::zerolocals));
 973   return entry;
 974 
 975 }
 976 
 977 /**
 978  * Method entry for static native methods:
 979  *   int java.util.zip.CRC32.update(int crc, int b)
 980  */
 981 address TemplateInterpreterGenerator::generate_CRC32_update_entry() {
 982   if (UseCRC32Intrinsics) {
 983     address entry = __ pc();
 984 
 985     // rmethod: Method*
 986     // r13: senderSP must preserved for slow path
 987     // esp: args
 988 
 989     Label slow_path;
 990     // If we need a safepoint check, generate full interpreter entry.
 991     __ safepoint_poll(slow_path);
 992 
 993     // We don&#39;t generate local frame and don&#39;t align stack because
 994     // we call stub code and there is no safepoint on this path.
 995 
 996     // Load parameters
 997     const Register crc = c_rarg0;  // crc
 998     const Register val = c_rarg1;  // source java byte value
 999     const Register tbl = c_rarg2;  // scratch
1000 
1001     // Arguments are reversed on java expression stack
1002     __ ldrw(val, Address(esp, 0));              // byte value
1003     __ ldrw(crc, Address(esp, wordSize));       // Initial CRC
1004 
1005     unsigned long offset;
1006     __ adrp(tbl, ExternalAddress(StubRoutines::crc_table_addr()), offset);
1007     __ add(tbl, tbl, offset);
1008 
1009     __ mvnw(crc, crc); // ~crc
1010     __ update_byte_crc32(crc, val, tbl);
1011     __ mvnw(crc, crc); // ~crc
1012 
1013     // result in c_rarg0
1014 
1015     __ andr(sp, r13, -16);
1016     __ ret(lr);
1017 
1018     // generate a vanilla native entry as the slow path
1019     __ bind(slow_path);
1020     __ jump_to_entry(Interpreter::entry_for_kind(Interpreter::native));
1021     return entry;
1022   }
1023   return NULL;
1024 }
1025 
1026 /**
1027  * Method entry for static native methods:
1028  *   int java.util.zip.CRC32.updateBytes(int crc, byte[] b, int off, int len)
1029  *   int java.util.zip.CRC32.updateByteBuffer(int crc, long buf, int off, int len)
1030  */
1031 address TemplateInterpreterGenerator::generate_CRC32_updateBytes_entry(AbstractInterpreter::MethodKind kind) {
1032   if (UseCRC32Intrinsics) {
1033     address entry = __ pc();
1034 
1035     // rmethod,: Method*
1036     // r13: senderSP must preserved for slow path
1037 
1038     Label slow_path;
1039     // If we need a safepoint check, generate full interpreter entry.
1040     __ safepoint_poll(slow_path);
1041 
1042     // We don&#39;t generate local frame and don&#39;t align stack because
1043     // we call stub code and there is no safepoint on this path.
1044 
1045     // Load parameters
1046     const Register crc = c_rarg0;  // crc
1047     const Register buf = c_rarg1;  // source java byte array address
1048     const Register len = c_rarg2;  // length
1049     const Register off = len;      // offset (never overlaps with &#39;len&#39;)
1050 
1051     // Arguments are reversed on java expression stack
1052     // Calculate address of start element
1053     if (kind == Interpreter::java_util_zip_CRC32_updateByteBuffer) {
1054       __ ldr(buf, Address(esp, 2*wordSize)); // long buf
1055       __ ldrw(off, Address(esp, wordSize)); // offset
1056       __ add(buf, buf, off); // + offset
1057       __ ldrw(crc,   Address(esp, 4*wordSize)); // Initial CRC
1058     } else {
1059       __ ldr(buf, Address(esp, 2*wordSize)); // byte[] array
1060       __ resolve(IS_NOT_NULL | ACCESS_READ, buf);
1061       __ add(buf, buf, arrayOopDesc::base_offset_in_bytes(T_BYTE)); // + header size
1062       __ ldrw(off, Address(esp, wordSize)); // offset
1063       __ add(buf, buf, off); // + offset
1064       __ ldrw(crc,   Address(esp, 3*wordSize)); // Initial CRC
1065     }
1066     // Can now load &#39;len&#39; since we&#39;re finished with &#39;off&#39;
1067     __ ldrw(len, Address(esp, 0x0)); // Length
1068 
1069     __ andr(sp, r13, -16); // Restore the caller&#39;s SP
1070 
1071     // We are frameless so we can just jump to the stub.
1072     __ b(CAST_FROM_FN_PTR(address, StubRoutines::updateBytesCRC32()));
1073 
1074     // generate a vanilla native entry as the slow path
1075     __ bind(slow_path);
1076     __ jump_to_entry(Interpreter::entry_for_kind(Interpreter::native));
1077     return entry;
1078   }
1079   return NULL;
1080 }
1081 
1082 /**
1083  * Method entry for intrinsic-candidate (non-native) methods:
1084  *   int java.util.zip.CRC32C.updateBytes(int crc, byte[] b, int off, int end)
1085  *   int java.util.zip.CRC32C.updateDirectByteBuffer(int crc, long buf, int off, int end)
1086  * Unlike CRC32, CRC32C does not have any methods marked as native
1087  * CRC32C also uses an &quot;end&quot; variable instead of the length variable CRC32 uses
1088  */
1089 address TemplateInterpreterGenerator::generate_CRC32C_updateBytes_entry(AbstractInterpreter::MethodKind kind) {
1090   if (UseCRC32CIntrinsics) {
1091     address entry = __ pc();
1092 
1093     // Prepare jump to stub using parameters from the stack
1094     const Register crc = c_rarg0; // initial crc
1095     const Register buf = c_rarg1; // source java byte array address
1096     const Register len = c_rarg2; // len argument to the kernel
1097 
1098     const Register end = len; // index of last element to process
1099     const Register off = crc; // offset
1100 
1101     __ ldrw(end, Address(esp)); // int end
1102     __ ldrw(off, Address(esp, wordSize)); // int offset
1103     __ sub(len, end, off);
1104     __ ldr(buf, Address(esp, 2*wordSize)); // byte[] buf | long buf
1105     if (kind == Interpreter::java_util_zip_CRC32C_updateBytes) {
1106       __ resolve(IS_NOT_NULL | ACCESS_READ, buf);
1107     }
1108     __ add(buf, buf, off); // + offset
1109     if (kind == Interpreter::java_util_zip_CRC32C_updateDirectByteBuffer) {
1110       __ ldrw(crc, Address(esp, 4*wordSize)); // long crc
1111     } else {
1112       __ add(buf, buf, arrayOopDesc::base_offset_in_bytes(T_BYTE)); // + header size
1113       __ ldrw(crc, Address(esp, 3*wordSize)); // long crc
1114     }
1115 
1116     __ andr(sp, r13, -16); // Restore the caller&#39;s SP
1117 
1118     // Jump to the stub.
1119     __ b(CAST_FROM_FN_PTR(address, StubRoutines::updateBytesCRC32C()));
1120 
1121     return entry;
1122   }
1123   return NULL;
1124 }
1125 
1126 void TemplateInterpreterGenerator::bang_stack_shadow_pages(bool native_call) {
1127   // Bang each page in the shadow zone. We can&#39;t assume it&#39;s been done for
1128   // an interpreter frame with greater than a page of locals, so each page
1129   // needs to be checked.  Only true for non-native.
1130   if (UseStackBanging) {
1131     const int n_shadow_pages = JavaThread::stack_shadow_zone_size() / os::vm_page_size();
1132     const int start_page = native_call ? n_shadow_pages : 1;
1133     const int page_size = os::vm_page_size();
1134     for (int pages = start_page; pages &lt;= n_shadow_pages ; pages++) {
1135       __ sub(rscratch2, sp, pages*page_size);
1136       __ str(zr, Address(rscratch2));
1137     }
1138   }
1139 }
1140 
1141 
1142 // Interpreter stub for calling a native method. (asm interpreter)
1143 // This sets up a somewhat different looking stack for calling the
1144 // native method than the typical interpreter frame setup.
1145 address TemplateInterpreterGenerator::generate_native_entry(bool synchronized) {
1146   // determine code generation flags
1147   bool inc_counter  = UseCompiler || CountCompiledCalls || LogTouchedMethods;
1148 
1149   // r1: Method*
1150   // rscratch1: sender sp
1151 
1152   address entry_point = __ pc();
1153 
1154   const Address constMethod       (rmethod, Method::const_offset());
1155   const Address access_flags      (rmethod, Method::access_flags_offset());
1156   const Address size_of_parameters(r2, ConstMethod::
1157                                        size_of_parameters_offset());
1158 
1159   // get parameter size (always needed)
1160   __ ldr(r2, constMethod);
1161   __ load_unsigned_short(r2, size_of_parameters);
1162 
1163   // Native calls don&#39;t need the stack size check since they have no
1164   // expression stack and the arguments are already on the stack and
1165   // we only add a handful of words to the stack.
1166 
1167   // rmethod: Method*
1168   // r2: size of parameters
1169   // rscratch1: sender sp
1170 
1171   // for natives the size of locals is zero
1172 
1173   // compute beginning of parameters (rlocals)
1174   __ add(rlocals, esp, r2, ext::uxtx, 3);
1175   __ add(rlocals, rlocals, -wordSize);
1176 
1177   // Pull SP back to minimum size: this avoids holes in the stack
1178   __ andr(sp, esp, -16);
1179 
1180   // initialize fixed part of activation frame
1181   generate_fixed_frame(true);
1182 
1183   // make sure method is native &amp; not abstract
1184 #ifdef ASSERT
1185   __ ldrw(r0, access_flags);
1186   {
1187     Label L;
1188     __ tst(r0, JVM_ACC_NATIVE);
1189     __ br(Assembler::NE, L);
1190     __ stop(&quot;tried to execute non-native method as native&quot;);
1191     __ bind(L);
1192   }
1193   {
1194     Label L;
1195     __ tst(r0, JVM_ACC_ABSTRACT);
1196     __ br(Assembler::EQ, L);
1197     __ stop(&quot;tried to execute abstract method in interpreter&quot;);
1198     __ bind(L);
1199   }
1200 #endif
1201 
1202   // Since at this point in the method invocation the exception
1203   // handler would try to exit the monitor of synchronized methods
1204   // which hasn&#39;t been entered yet, we set the thread local variable
1205   // _do_not_unlock_if_synchronized to true. The remove_activation
1206   // will check this flag.
1207 
1208    const Address do_not_unlock_if_synchronized(rthread,
1209         in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()));
1210   __ mov(rscratch2, true);
1211   __ strb(rscratch2, do_not_unlock_if_synchronized);
1212 
1213   // increment invocation count &amp; check for overflow
1214   Label invocation_counter_overflow;
1215   if (inc_counter) {
1216     generate_counter_incr(&amp;invocation_counter_overflow, NULL, NULL);
1217   }
1218 
1219   Label continue_after_compile;
1220   __ bind(continue_after_compile);
1221 
1222   bang_stack_shadow_pages(true);
1223 
1224   // reset the _do_not_unlock_if_synchronized flag
1225   __ strb(zr, do_not_unlock_if_synchronized);
1226 
1227   // check for synchronized methods
1228   // Must happen AFTER invocation_counter check and stack overflow check,
1229   // so method is not locked if overflows.
1230   if (synchronized) {
1231     lock_method();
1232   } else {
1233     // no synchronization necessary
1234 #ifdef ASSERT
1235     {
1236       Label L;
1237       __ ldrw(r0, access_flags);
1238       __ tst(r0, JVM_ACC_SYNCHRONIZED);
1239       __ br(Assembler::EQ, L);
1240       __ stop(&quot;method needs synchronization&quot;);
1241       __ bind(L);
1242     }
1243 #endif
1244   }
1245 
1246   // start execution
1247 #ifdef ASSERT
1248   {
1249     Label L;
1250     const Address monitor_block_top(rfp,
1251                  frame::interpreter_frame_monitor_block_top_offset * wordSize);
1252     __ ldr(rscratch1, monitor_block_top);
1253     __ cmp(esp, rscratch1);
1254     __ br(Assembler::EQ, L);
1255     __ stop(&quot;broken stack frame setup in interpreter&quot;);
1256     __ bind(L);
1257   }
1258 #endif
1259 
1260   // jvmti support
1261   __ notify_method_entry();
1262 
1263   // work registers
1264   const Register t = r17;
1265   const Register result_handler = r19;
1266 
1267   // allocate space for parameters
1268   __ ldr(t, Address(rmethod, Method::const_offset()));
1269   __ load_unsigned_short(t, Address(t, ConstMethod::size_of_parameters_offset()));
1270 
1271   __ sub(rscratch1, esp, t, ext::uxtx, Interpreter::logStackElementSize);
1272   __ andr(sp, rscratch1, -16);
1273   __ mov(esp, rscratch1);
1274 
1275   // get signature handler
1276   {
1277     Label L;
1278     __ ldr(t, Address(rmethod, Method::signature_handler_offset()));
1279     __ cbnz(t, L);
1280     __ call_VM(noreg,
1281                CAST_FROM_FN_PTR(address,
1282                                 InterpreterRuntime::prepare_native_call),
1283                rmethod);
1284     __ ldr(t, Address(rmethod, Method::signature_handler_offset()));
1285     __ bind(L);
1286   }
1287 
1288   // call signature handler
1289   assert(InterpreterRuntime::SignatureHandlerGenerator::from() == rlocals,
1290          &quot;adjust this code&quot;);
1291   assert(InterpreterRuntime::SignatureHandlerGenerator::to() == sp,
1292          &quot;adjust this code&quot;);
1293   assert(InterpreterRuntime::SignatureHandlerGenerator::temp() == rscratch1,
1294           &quot;adjust this code&quot;);
1295 
1296   // The generated handlers do not touch rmethod (the method).
1297   // However, large signatures cannot be cached and are generated
1298   // each time here.  The slow-path generator can do a GC on return,
1299   // so we must reload it after the call.
1300   __ blr(t);
1301   __ get_method(rmethod);        // slow path can do a GC, reload rmethod
1302 
1303 
1304   // result handler is in r0
1305   // set result handler
1306   __ mov(result_handler, r0);
1307   // pass mirror handle if static call
1308   {
1309     Label L;
1310     __ ldrw(t, Address(rmethod, Method::access_flags_offset()));
1311     __ tbz(t, exact_log2(JVM_ACC_STATIC), L);
1312     // get mirror
1313     __ load_mirror(t, rmethod);
1314     // copy mirror into activation frame
1315     __ str(t, Address(rfp, frame::interpreter_frame_oop_temp_offset * wordSize));
1316     // pass handle to mirror
1317     __ add(c_rarg1, rfp, frame::interpreter_frame_oop_temp_offset * wordSize);
1318     __ bind(L);
1319   }
1320 
1321   // get native function entry point in r10
1322   {
1323     Label L;
1324     __ ldr(r10, Address(rmethod, Method::native_function_offset()));
1325     address unsatisfied = (SharedRuntime::native_method_throw_unsatisfied_link_error_entry());
1326     __ mov(rscratch2, unsatisfied);
1327     __ ldr(rscratch2, rscratch2);
1328     __ cmp(r10, rscratch2);
1329     __ br(Assembler::NE, L);
1330     __ call_VM(noreg,
1331                CAST_FROM_FN_PTR(address,
1332                                 InterpreterRuntime::prepare_native_call),
1333                rmethod);
1334     __ get_method(rmethod);
1335     __ ldr(r10, Address(rmethod, Method::native_function_offset()));
1336     __ bind(L);
1337   }
1338 
1339   // pass JNIEnv
1340   __ add(c_rarg0, rthread, in_bytes(JavaThread::jni_environment_offset()));
1341 
1342   // Set the last Java PC in the frame anchor to be the return address from
1343   // the call to the native method: this will allow the debugger to
1344   // generate an accurate stack trace.
1345   Label native_return;
1346   __ set_last_Java_frame(esp, rfp, native_return, rscratch1);
1347 
1348   // change thread state
1349 #ifdef ASSERT
1350   {
1351     Label L;
1352     __ ldrw(t, Address(rthread, JavaThread::thread_state_offset()));
1353     __ cmp(t, (u1)_thread_in_Java);
1354     __ br(Assembler::EQ, L);
1355     __ stop(&quot;Wrong thread state in native stub&quot;);
1356     __ bind(L);
1357   }
1358 #endif
1359 
1360   // Change state to native
1361   __ mov(rscratch1, _thread_in_native);
1362   __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));
1363   __ stlrw(rscratch1, rscratch2);
1364 
1365   // Call the native method.
1366   __ blr(r10);
1367   __ bind(native_return);
1368   __ maybe_isb();
1369   __ get_method(rmethod);
1370   // result potentially in r0 or v0
1371 
1372   // make room for the pushes we&#39;re about to do
1373   __ sub(rscratch1, esp, 4 * wordSize);
1374   __ andr(sp, rscratch1, -16);
1375 
1376   // NOTE: The order of these pushes is known to frame::interpreter_frame_result
1377   // in order to extract the result of a method call. If the order of these
1378   // pushes change or anything else is added to the stack then the code in
1379   // interpreter_frame_result must also change.
1380   __ push(dtos);
1381   __ push(ltos);
1382 
1383   // change thread state
1384   __ mov(rscratch1, _thread_in_native_trans);
1385   __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));
1386   __ stlrw(rscratch1, rscratch2);
1387 
1388   // Force this write out before the read below
1389   __ dmb(Assembler::ISH);
1390 
1391   // check for safepoint operation in progress and/or pending suspend requests
1392   {
1393     Label L, Continue;
1394     __ safepoint_poll_acquire(L);
1395     __ ldrw(rscratch2, Address(rthread, JavaThread::suspend_flags_offset()));
1396     __ cbz(rscratch2, Continue);
1397     __ bind(L);
1398 
1399     // Don&#39;t use call_VM as it will see a possible pending exception
1400     // and forward it and never return here preventing us from
1401     // clearing _last_native_pc down below. So we do a runtime call by
1402     // hand.
1403     //
1404     __ mov(c_rarg0, rthread);
1405     __ mov(rscratch2, CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans));
1406     __ blr(rscratch2);
1407     __ maybe_isb();
1408     __ get_method(rmethod);
1409     __ reinit_heapbase();
1410     __ bind(Continue);
1411   }
1412 
1413   // change thread state
1414   __ mov(rscratch1, _thread_in_Java);
1415   __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));
1416   __ stlrw(rscratch1, rscratch2);
1417 
1418   // reset_last_Java_frame
1419   __ reset_last_Java_frame(true);
1420 
1421   if (CheckJNICalls) {
1422     // clear_pending_jni_exception_check
1423     __ str(zr, Address(rthread, JavaThread::pending_jni_exception_check_fn_offset()));
1424   }
1425 
1426   // reset handle block
1427   __ ldr(t, Address(rthread, JavaThread::active_handles_offset()));
1428   __ str(zr, Address(t, JNIHandleBlock::top_offset_in_bytes()));
1429 
1430   // If result is an oop unbox and store it in frame where gc will see it
1431   // and result handler will pick it up
1432 
1433   {
1434     Label no_oop;
1435     __ adr(t, ExternalAddress(AbstractInterpreter::result_handler(T_OBJECT)));
1436     __ cmp(t, result_handler);
1437     __ br(Assembler::NE, no_oop);
1438     // Unbox oop result, e.g. JNIHandles::resolve result.
1439     __ pop(ltos);
1440     __ resolve_jobject(r0, rthread, t);
1441     __ str(r0, Address(rfp, frame::interpreter_frame_oop_temp_offset*wordSize));
1442     // keep stack depth as expected by pushing oop which will eventually be discarded
1443     __ push(ltos);
1444     __ bind(no_oop);
1445   }
1446 
1447   {
1448     Label no_reguard;
1449     __ lea(rscratch1, Address(rthread, in_bytes(JavaThread::stack_guard_state_offset())));
1450     __ ldrw(rscratch1, Address(rscratch1));
1451     __ cmp(rscratch1, (u1)JavaThread::stack_guard_yellow_reserved_disabled);
1452     __ br(Assembler::NE, no_reguard);
1453 
1454     __ pusha(); // XXX only save smashed registers
1455     __ mov(c_rarg0, rthread);
1456     __ mov(rscratch2, CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));
1457     __ blr(rscratch2);
1458     __ popa(); // XXX only restore smashed registers
1459     __ bind(no_reguard);
1460   }
1461 
1462   // The method register is junk from after the thread_in_native transition
1463   // until here.  Also can&#39;t call_VM until the bcp has been
1464   // restored.  Need bcp for throwing exception below so get it now.
1465   __ get_method(rmethod);
1466 
1467   // restore bcp to have legal interpreter frame, i.e., bci == 0 &lt;=&gt;
1468   // rbcp == code_base()
1469   __ ldr(rbcp, Address(rmethod, Method::const_offset()));   // get ConstMethod*
1470   __ add(rbcp, rbcp, in_bytes(ConstMethod::codes_offset()));          // get codebase
1471   // handle exceptions (exception handling will handle unlocking!)
1472   {
1473     Label L;
1474     __ ldr(rscratch1, Address(rthread, Thread::pending_exception_offset()));
1475     __ cbz(rscratch1, L);
1476     // Note: At some point we may want to unify this with the code
1477     // used in call_VM_base(); i.e., we should use the
1478     // StubRoutines::forward_exception code. For now this doesn&#39;t work
1479     // here because the rsp is not correctly set at this point.
1480     __ MacroAssembler::call_VM(noreg,
1481                                CAST_FROM_FN_PTR(address,
1482                                InterpreterRuntime::throw_pending_exception));
1483     __ should_not_reach_here();
1484     __ bind(L);
1485   }
1486 
1487   // do unlocking if necessary
1488   {
1489     Label L;
1490     __ ldrw(t, Address(rmethod, Method::access_flags_offset()));
1491     __ tbz(t, exact_log2(JVM_ACC_SYNCHRONIZED), L);
1492     // the code below should be shared with interpreter macro
1493     // assembler implementation
1494     {
1495       Label unlock;
1496       // BasicObjectLock will be first in list, since this is a
1497       // synchronized method. However, need to check that the object
1498       // has not been unlocked by an explicit monitorexit bytecode.
1499 
1500       // monitor expect in c_rarg1 for slow unlock path
1501       __ lea (c_rarg1, Address(rfp,   // address of first monitor
1502                                (intptr_t)(frame::interpreter_frame_initial_sp_offset *
1503                                           wordSize - sizeof(BasicObjectLock))));
1504 
1505       __ ldr(t, Address(c_rarg1, BasicObjectLock::obj_offset_in_bytes()));
1506       __ cbnz(t, unlock);
1507 
1508       // Entry already unlocked, need to throw exception
1509       __ MacroAssembler::call_VM(noreg,
1510                                  CAST_FROM_FN_PTR(address,
1511                    InterpreterRuntime::throw_illegal_monitor_state_exception));
1512       __ should_not_reach_here();
1513 
1514       __ bind(unlock);
1515       __ unlock_object(c_rarg1);
1516     }
1517     __ bind(L);
1518   }
1519 
1520   // jvmti support
1521   // Note: This must happen _after_ handling/throwing any exceptions since
1522   //       the exception handler code notifies the runtime of method exits
1523   //       too. If this happens before, method entry/exit notifications are
1524   //       not properly paired (was bug - gri 11/22/99).
1525   __ notify_method_exit(vtos, InterpreterMacroAssembler::NotifyJVMTI);
1526 
1527   // restore potential result in r0:d0, call result handler to
1528   // restore potential result in ST0 &amp; handle result
1529 
1530   __ pop(ltos);
1531   __ pop(dtos);
1532 
1533   __ blr(result_handler);
1534 
1535   // remove activation
1536   __ ldr(esp, Address(rfp,
1537                     frame::interpreter_frame_sender_sp_offset *
1538                     wordSize)); // get sender sp
1539   // remove frame anchor
1540   __ leave();
1541 
1542   // resture sender sp
1543   __ mov(sp, esp);
1544 
1545   __ ret(lr);
1546 
1547   if (inc_counter) {
1548     // Handle overflow of counter and compile method
1549     __ bind(invocation_counter_overflow);
1550     generate_counter_overflow(continue_after_compile);
1551   }
1552 
1553   return entry_point;
1554 }
1555 
1556 //
1557 // Generic interpreted method entry to (asm) interpreter
1558 //
1559 address TemplateInterpreterGenerator::generate_normal_entry(bool synchronized) {
1560   // determine code generation flags
1561   bool inc_counter  = UseCompiler || CountCompiledCalls || LogTouchedMethods;
1562 
1563   // rscratch1: sender sp
1564   address entry_point = __ pc();
1565 
1566   const Address constMethod(rmethod, Method::const_offset());
1567   const Address access_flags(rmethod, Method::access_flags_offset());
1568   const Address size_of_parameters(r3,
1569                                    ConstMethod::size_of_parameters_offset());
1570   const Address size_of_locals(r3, ConstMethod::size_of_locals_offset());
1571 
1572   // get parameter size (always needed)
1573   // need to load the const method first
1574   __ ldr(r3, constMethod);
1575   __ load_unsigned_short(r2, size_of_parameters);
1576 
1577   // r2: size of parameters
1578 
1579   __ load_unsigned_short(r3, size_of_locals); // get size of locals in words
1580   __ sub(r3, r3, r2); // r3 = no. of additional locals
1581 
1582   // see if we&#39;ve got enough room on the stack for locals plus overhead.
1583   generate_stack_overflow_check();
1584 
1585   // compute beginning of parameters (rlocals)
1586   __ add(rlocals, esp, r2, ext::uxtx, 3);
1587   __ sub(rlocals, rlocals, wordSize);
1588 
1589   // Make room for locals
1590   __ sub(rscratch1, esp, r3, ext::uxtx, 3);
1591   __ andr(sp, rscratch1, -16);
1592 
1593   // r3 - # of additional locals
1594   // allocate space for locals
1595   // explicitly initialize locals
1596   {
1597     Label exit, loop;
1598     __ ands(zr, r3, r3);
1599     __ br(Assembler::LE, exit); // do nothing if r3 &lt;= 0
1600     __ bind(loop);
1601     __ str(zr, Address(__ post(rscratch1, wordSize)));
1602     __ sub(r3, r3, 1); // until everything initialized
1603     __ cbnz(r3, loop);
1604     __ bind(exit);
1605   }
1606 
1607   // And the base dispatch table
1608   __ get_dispatch();
1609 
1610   // initialize fixed part of activation frame
1611   generate_fixed_frame(false);
1612 
1613   // make sure method is not native &amp; not abstract
1614 #ifdef ASSERT
1615   __ ldrw(r0, access_flags);
1616   {
1617     Label L;
1618     __ tst(r0, JVM_ACC_NATIVE);
1619     __ br(Assembler::EQ, L);
1620     __ stop(&quot;tried to execute native method as non-native&quot;);
1621     __ bind(L);
1622   }
1623  {
1624     Label L;
1625     __ tst(r0, JVM_ACC_ABSTRACT);
1626     __ br(Assembler::EQ, L);
1627     __ stop(&quot;tried to execute abstract method in interpreter&quot;);
1628     __ bind(L);
1629   }
1630 #endif
1631 
1632   // Since at this point in the method invocation the exception
1633   // handler would try to exit the monitor of synchronized methods
1634   // which hasn&#39;t been entered yet, we set the thread local variable
1635   // _do_not_unlock_if_synchronized to true. The remove_activation
1636   // will check this flag.
1637 
1638    const Address do_not_unlock_if_synchronized(rthread,
1639         in_bytes(JavaThread::do_not_unlock_if_synchronized_offset()));
1640   __ mov(rscratch2, true);
1641   __ strb(rscratch2, do_not_unlock_if_synchronized);
1642 
1643   Register mdp = r3;
1644   __ profile_parameters_type(mdp, r1, r2);
1645 
1646   // increment invocation count &amp; check for overflow
1647   Label invocation_counter_overflow;
1648   Label profile_method;
1649   Label profile_method_continue;
1650   if (inc_counter) {
1651     generate_counter_incr(&amp;invocation_counter_overflow,
1652                           &amp;profile_method,
1653                           &amp;profile_method_continue);
1654     if (ProfileInterpreter) {
1655       __ bind(profile_method_continue);
1656     }
1657   }
1658 
1659   Label continue_after_compile;
1660   __ bind(continue_after_compile);
1661 
1662   bang_stack_shadow_pages(false);
1663 
1664   // reset the _do_not_unlock_if_synchronized flag
1665   __ strb(zr, do_not_unlock_if_synchronized);
1666 
1667   // check for synchronized methods
1668   // Must happen AFTER invocation_counter check and stack overflow check,
1669   // so method is not locked if overflows.
1670   if (synchronized) {
1671     // Allocate monitor and lock method
1672     lock_method();
1673   } else {
1674     // no synchronization necessary
1675 #ifdef ASSERT
1676     {
1677       Label L;
1678       __ ldrw(r0, access_flags);
1679       __ tst(r0, JVM_ACC_SYNCHRONIZED);
1680       __ br(Assembler::EQ, L);
1681       __ stop(&quot;method needs synchronization&quot;);
1682       __ bind(L);
1683     }
1684 #endif
1685   }
1686 
1687   // start execution
1688 #ifdef ASSERT
1689   {
1690     Label L;
1691      const Address monitor_block_top (rfp,
1692                  frame::interpreter_frame_monitor_block_top_offset * wordSize);
1693     __ ldr(rscratch1, monitor_block_top);
1694     __ cmp(esp, rscratch1);
1695     __ br(Assembler::EQ, L);
1696     __ stop(&quot;broken stack frame setup in interpreter&quot;);
1697     __ bind(L);
1698   }
1699 #endif
1700 
1701   // jvmti support
1702   __ notify_method_entry();
1703 
1704   __ dispatch_next(vtos);
1705 
1706   // invocation counter overflow
1707   if (inc_counter) {
1708     if (ProfileInterpreter) {
1709       // We have decided to profile this method in the interpreter
1710       __ bind(profile_method);
1711       __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::profile_method));
1712       __ set_method_data_pointer_for_bcp();
1713       // don&#39;t think we need this
1714       __ get_method(r1);
1715       __ b(profile_method_continue);
1716     }
1717     // Handle overflow of counter and compile method
1718     __ bind(invocation_counter_overflow);
1719     generate_counter_overflow(continue_after_compile);
1720   }
1721 
1722   return entry_point;
1723 }
1724 
1725 //-----------------------------------------------------------------------------
1726 // Exceptions
1727 
1728 void TemplateInterpreterGenerator::generate_throw_exception() {
1729   // Entry point in previous activation (i.e., if the caller was
1730   // interpreted)
1731   Interpreter::_rethrow_exception_entry = __ pc();
1732   // Restore sp to interpreter_frame_last_sp even though we are going
1733   // to empty the expression stack for the exception processing.
1734   __ str(zr, Address(rfp, frame::interpreter_frame_last_sp_offset * wordSize));
1735   // r0: exception
1736   // r3: return address/pc that threw exception
1737   __ restore_bcp();    // rbcp points to call/send
1738   __ restore_locals();
1739   __ restore_constant_pool_cache();
1740   __ reinit_heapbase();  // restore rheapbase as heapbase.
1741   __ get_dispatch();
1742 
1743   // Entry point for exceptions thrown within interpreter code
1744   Interpreter::_throw_exception_entry = __ pc();
1745   // If we came here via a NullPointerException on the receiver of a
1746   // method, rmethod may be corrupt.
1747   __ get_method(rmethod);
1748   // expression stack is undefined here
1749   // r0: exception
1750   // rbcp: exception bcp
1751   __ verify_oop(r0);
1752   __ mov(c_rarg1, r0);
1753 
1754   // expression stack must be empty before entering the VM in case of
1755   // an exception
1756   __ empty_expression_stack();
1757   // find exception handler address and preserve exception oop
1758   __ call_VM(r3,
1759              CAST_FROM_FN_PTR(address,
1760                           InterpreterRuntime::exception_handler_for_exception),
1761              c_rarg1);
1762 
1763   // Calculate stack limit
1764   __ ldr(rscratch1, Address(rmethod, Method::const_offset()));
1765   __ ldrh(rscratch1, Address(rscratch1, ConstMethod::max_stack_offset()));
1766   __ add(rscratch1, rscratch1, frame::interpreter_frame_monitor_size() + 4);
1767   __ ldr(rscratch2,
1768          Address(rfp, frame::interpreter_frame_initial_sp_offset * wordSize));
1769   __ sub(rscratch1, rscratch2, rscratch1, ext::uxtx, 3);
1770   __ andr(sp, rscratch1, -16);
1771 
1772   // r0: exception handler entry point
1773   // r3: preserved exception oop
1774   // rbcp: bcp for exception handler
1775   __ push_ptr(r3); // push exception which is now the only value on the stack
1776   __ br(r0); // jump to exception handler (may be _remove_activation_entry!)
1777 
1778   // If the exception is not handled in the current frame the frame is
1779   // removed and the exception is rethrown (i.e. exception
1780   // continuation is _rethrow_exception).
1781   //
1782   // Note: At this point the bci is still the bxi for the instruction
1783   // which caused the exception and the expression stack is
1784   // empty. Thus, for any VM calls at this point, GC will find a legal
1785   // oop map (with empty expression stack).
1786 
1787   //
1788   // JVMTI PopFrame support
1789   //
1790 
1791   Interpreter::_remove_activation_preserving_args_entry = __ pc();
1792   __ empty_expression_stack();
1793   // Set the popframe_processing bit in pending_popframe_condition
1794   // indicating that we are currently handling popframe, so that
1795   // call_VMs that may happen later do not trigger new popframe
1796   // handling cycles.
1797   __ ldrw(r3, Address(rthread, JavaThread::popframe_condition_offset()));
1798   __ orr(r3, r3, JavaThread::popframe_processing_bit);
1799   __ strw(r3, Address(rthread, JavaThread::popframe_condition_offset()));
1800 
1801   {
1802     // Check to see whether we are returning to a deoptimized frame.
1803     // (The PopFrame call ensures that the caller of the popped frame is
1804     // either interpreted or compiled and deoptimizes it if compiled.)
1805     // In this case, we can&#39;t call dispatch_next() after the frame is
1806     // popped, but instead must save the incoming arguments and restore
1807     // them after deoptimization has occurred.
1808     //
1809     // Note that we don&#39;t compare the return PC against the
1810     // deoptimization blob&#39;s unpack entry because of the presence of
1811     // adapter frames in C2.
1812     Label caller_not_deoptimized;
1813     __ ldr(c_rarg1, Address(rfp, frame::return_addr_offset * wordSize));
1814     __ super_call_VM_leaf(CAST_FROM_FN_PTR(address,
1815                                InterpreterRuntime::interpreter_contains), c_rarg1);
1816     __ cbnz(r0, caller_not_deoptimized);
1817 
1818     // Compute size of arguments for saving when returning to
1819     // deoptimized caller
1820     __ get_method(r0);
1821     __ ldr(r0, Address(r0, Method::const_offset()));
1822     __ load_unsigned_short(r0, Address(r0, in_bytes(ConstMethod::
1823                                                     size_of_parameters_offset())));
1824     __ lsl(r0, r0, Interpreter::logStackElementSize);
1825     __ restore_locals(); // XXX do we need this?
1826     __ sub(rlocals, rlocals, r0);
1827     __ add(rlocals, rlocals, wordSize);
1828     // Save these arguments
1829     __ super_call_VM_leaf(CAST_FROM_FN_PTR(address,
1830                                            Deoptimization::
1831                                            popframe_preserve_args),
1832                           rthread, r0, rlocals);
1833 
1834     __ remove_activation(vtos,
1835                          /* throw_monitor_exception */ false,
1836                          /* install_monitor_exception */ false,
1837                          /* notify_jvmdi */ false);
1838 
1839     // Inform deoptimization that it is responsible for restoring
1840     // these arguments
1841     __ mov(rscratch1, JavaThread::popframe_force_deopt_reexecution_bit);
1842     __ strw(rscratch1, Address(rthread, JavaThread::popframe_condition_offset()));
1843 
1844     // Continue in deoptimization handler
1845     __ ret(lr);
1846 
1847     __ bind(caller_not_deoptimized);
1848   }
1849 
1850   __ remove_activation(vtos,
1851                        /* throw_monitor_exception */ false,
1852                        /* install_monitor_exception */ false,
1853                        /* notify_jvmdi */ false);
1854 
1855   // Restore the last_sp and null it out
1856   __ ldr(esp, Address(rfp, frame::interpreter_frame_last_sp_offset * wordSize));
1857   __ str(zr, Address(rfp, frame::interpreter_frame_last_sp_offset * wordSize));
1858 
1859   __ restore_bcp();
1860   __ restore_locals();
1861   __ restore_constant_pool_cache();
1862   __ get_method(rmethod);
1863   __ get_dispatch();
1864 
1865   // The method data pointer was incremented already during
1866   // call profiling. We have to restore the mdp for the current bcp.
1867   if (ProfileInterpreter) {
1868     __ set_method_data_pointer_for_bcp();
1869   }
1870 
1871   // Clear the popframe condition flag
1872   __ strw(zr, Address(rthread, JavaThread::popframe_condition_offset()));
1873   assert(JavaThread::popframe_inactive == 0, &quot;fix popframe_inactive&quot;);
1874 
1875 #if INCLUDE_JVMTI
1876   {
1877     Label L_done;
1878 
1879     __ ldrb(rscratch1, Address(rbcp, 0));
1880     __ cmpw(rscratch1, Bytecodes::_invokestatic);
1881     __ br(Assembler::NE, L_done);
1882 
1883     // The member name argument must be restored if _invokestatic is re-executed after a PopFrame call.
1884     // Detect such a case in the InterpreterRuntime function and return the member name argument, or NULL.
1885 
1886     __ ldr(c_rarg0, Address(rlocals, 0));
1887     __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::member_name_arg_or_null), c_rarg0, rmethod, rbcp);
1888 
1889     __ cbz(r0, L_done);
1890 
1891     __ str(r0, Address(esp, 0));
1892     __ bind(L_done);
1893   }
1894 #endif // INCLUDE_JVMTI
1895 
1896   // Restore machine SP
1897   __ ldr(rscratch1, Address(rmethod, Method::const_offset()));
1898   __ ldrh(rscratch1, Address(rscratch1, ConstMethod::max_stack_offset()));
1899   __ add(rscratch1, rscratch1, frame::interpreter_frame_monitor_size() + 4);
1900   __ ldr(rscratch2,
1901          Address(rfp, frame::interpreter_frame_initial_sp_offset * wordSize));
1902   __ sub(rscratch1, rscratch2, rscratch1, ext::uxtw, 3);
1903   __ andr(sp, rscratch1, -16);
1904 
1905   __ dispatch_next(vtos);
1906   // end of PopFrame support
1907 
1908   Interpreter::_remove_activation_entry = __ pc();
1909 
1910   // preserve exception over this code sequence
1911   __ pop_ptr(r0);
1912   __ str(r0, Address(rthread, JavaThread::vm_result_offset()));
1913   // remove the activation (without doing throws on illegalMonitorExceptions)
1914   __ remove_activation(vtos, false, true, false);
1915   // restore exception
1916   __ get_vm_result(r0, rthread);
1917 
1918   // In between activations - previous activation type unknown yet
1919   // compute continuation point - the continuation point expects the
1920   // following registers set up:
1921   //
1922   // r0: exception
1923   // lr: return address/pc that threw exception
1924   // esp: expression stack of caller
1925   // rfp: fp of caller
1926   __ stp(r0, lr, Address(__ pre(sp, -2 * wordSize)));  // save exception &amp; return address
1927   __ super_call_VM_leaf(CAST_FROM_FN_PTR(address,
1928                           SharedRuntime::exception_handler_for_return_address),
1929                         rthread, lr);
1930   __ mov(r1, r0);                               // save exception handler
1931   __ ldp(r0, lr, Address(__ post(sp, 2 * wordSize)));  // restore exception &amp; return address
1932   // We might be returning to a deopt handler that expects r3 to
1933   // contain the exception pc
1934   __ mov(r3, lr);
1935   // Note that an &quot;issuing PC&quot; is actually the next PC after the call
1936   __ br(r1);                                    // jump to exception
1937                                                 // handler of caller
1938 }
1939 
1940 
1941 //
1942 // JVMTI ForceEarlyReturn support
1943 //
1944 address TemplateInterpreterGenerator::generate_earlyret_entry_for(TosState state) {
1945   address entry = __ pc();
1946 
1947   __ restore_bcp();
1948   __ restore_locals();
1949   __ empty_expression_stack();
1950   __ load_earlyret_value(state);
1951 
1952   __ ldr(rscratch1, Address(rthread, JavaThread::jvmti_thread_state_offset()));
1953   Address cond_addr(rscratch1, JvmtiThreadState::earlyret_state_offset());
1954 
1955   // Clear the earlyret state
1956   assert(JvmtiThreadState::earlyret_inactive == 0, &quot;should be&quot;);
1957   __ str(zr, cond_addr);
1958 
1959   __ remove_activation(state,
1960                        false, /* throw_monitor_exception */
1961                        false, /* install_monitor_exception */
1962                        true); /* notify_jvmdi */
1963   __ ret(lr);
1964 
1965   return entry;
1966 } // end of ForceEarlyReturn support
1967 
1968 
1969 
1970 //-----------------------------------------------------------------------------
1971 // Helper for vtos entry point generation
1972 
1973 void TemplateInterpreterGenerator::set_vtos_entry_points(Template* t,
1974                                                          address&amp; bep,
1975                                                          address&amp; cep,
1976                                                          address&amp; sep,
1977                                                          address&amp; aep,
1978                                                          address&amp; iep,
1979                                                          address&amp; lep,
1980                                                          address&amp; fep,
1981                                                          address&amp; dep,
1982                                                          address&amp; vep) {
1983   assert(t-&gt;is_valid() &amp;&amp; t-&gt;tos_in() == vtos, &quot;illegal template&quot;);
1984   Label L;
1985   aep = __ pc();  __ push_ptr();  __ b(L);
1986   fep = __ pc();  __ push_f();    __ b(L);
1987   dep = __ pc();  __ push_d();    __ b(L);
1988   lep = __ pc();  __ push_l();    __ b(L);
1989   bep = cep = sep =
1990   iep = __ pc();  __ push_i();
1991   vep = __ pc();
1992   __ bind(L);
1993   generate_and_dispatch(t);
1994 }
1995 
1996 //-----------------------------------------------------------------------------
1997 
1998 // Non-product code
1999 #ifndef PRODUCT
2000 address TemplateInterpreterGenerator::generate_trace_code(TosState state) {
2001   address entry = __ pc();
2002 
2003   __ push(lr);
2004   __ push(state);
2005   __ push(RegSet::range(r0, r15), sp);
2006   __ mov(c_rarg2, r0);  // Pass itos
2007   __ call_VM(noreg,
2008              CAST_FROM_FN_PTR(address, InterpreterRuntime::trace_bytecode),
2009              c_rarg1, c_rarg2, c_rarg3);
2010   __ pop(RegSet::range(r0, r15), sp);
2011   __ pop(state);
2012   __ pop(lr);
2013   __ ret(lr);                                   // return from result handler
2014 
2015   return entry;
2016 }
2017 
2018 void TemplateInterpreterGenerator::count_bytecode() {
2019   Register rscratch3 = r0;
2020   __ push(rscratch1);
2021   __ push(rscratch2);
2022   __ push(rscratch3);
2023   __ mov(rscratch3, (address) &amp;BytecodeCounter::_counter_value);
2024   __ atomic_add(noreg, 1, rscratch3);
2025   __ pop(rscratch3);
2026   __ pop(rscratch2);
2027   __ pop(rscratch1);
2028 }
2029 
2030 void TemplateInterpreterGenerator::histogram_bytecode(Template* t) { ; }
2031 
2032 void TemplateInterpreterGenerator::histogram_bytecode_pair(Template* t) { ; }
2033 
2034 
2035 void TemplateInterpreterGenerator::trace_bytecode(Template* t) {
2036   // Call a little run-time stub to avoid blow-up for each bytecode.
2037   // The run-time runtime saves the right registers, depending on
2038   // the tosca in-state for the given template.
2039 
2040   assert(Interpreter::trace_code(t-&gt;tos_in()) != NULL,
2041          &quot;entry must have been generated&quot;);
2042   __ bl(Interpreter::trace_code(t-&gt;tos_in()));
2043   __ reinit_heapbase();
2044 }
2045 
2046 
2047 void TemplateInterpreterGenerator::stop_interpreter_at() {
2048   Label L;
2049   __ push(rscratch1);
2050   __ mov(rscratch1, (address) &amp;BytecodeCounter::_counter_value);
2051   __ ldr(rscratch1, Address(rscratch1));
2052   __ mov(rscratch2, StopInterpreterAt);
2053   __ cmpw(rscratch1, rscratch2);
2054   __ br(Assembler::NE, L);
2055   __ brk(0);
2056   __ bind(L);
2057   __ pop(rscratch1);
2058 }
2059 
2060 #endif // !PRODUCT
<a name="3" id="anc3"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="3" type="hidden" />
</body>
</html>