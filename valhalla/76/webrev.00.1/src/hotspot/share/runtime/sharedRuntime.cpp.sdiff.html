<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/runtime/sharedRuntime.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="safepoint.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="synchronizer.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/runtime/sharedRuntime.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  27 #include &quot;aot/aotLoader.hpp&quot;
  28 #include &quot;classfile/stringTable.hpp&quot;
  29 #include &quot;classfile/systemDictionary.hpp&quot;
  30 #include &quot;classfile/vmSymbols.hpp&quot;
  31 #include &quot;code/codeCache.hpp&quot;
  32 #include &quot;code/compiledIC.hpp&quot;
  33 #include &quot;code/icBuffer.hpp&quot;
  34 #include &quot;code/compiledMethod.inline.hpp&quot;
  35 #include &quot;code/scopeDesc.hpp&quot;
  36 #include &quot;code/vtableStubs.hpp&quot;
  37 #include &quot;compiler/abstractCompiler.hpp&quot;
  38 #include &quot;compiler/compileBroker.hpp&quot;
  39 #include &quot;compiler/disassembler.hpp&quot;
  40 #include &quot;gc/shared/barrierSet.hpp&quot;
  41 #include &quot;gc/shared/gcLocker.inline.hpp&quot;
  42 #include &quot;interpreter/interpreter.hpp&quot;
  43 #include &quot;interpreter/interpreterRuntime.hpp&quot;
  44 #include &quot;jfr/jfrEvents.hpp&quot;
  45 #include &quot;logging/log.hpp&quot;
  46 #include &quot;memory/metaspaceShared.hpp&quot;

  47 #include &quot;memory/resourceArea.hpp&quot;
  48 #include &quot;memory/universe.hpp&quot;


  49 #include &quot;oops/klass.hpp&quot;
  50 #include &quot;oops/method.inline.hpp&quot;
  51 #include &quot;oops/objArrayKlass.hpp&quot;

  52 #include &quot;oops/oop.inline.hpp&quot;

  53 #include &quot;prims/forte.hpp&quot;
  54 #include &quot;prims/jvmtiExport.hpp&quot;
  55 #include &quot;prims/methodHandles.hpp&quot;
  56 #include &quot;prims/nativeLookup.hpp&quot;
  57 #include &quot;runtime/arguments.hpp&quot;
  58 #include &quot;runtime/atomic.hpp&quot;
  59 #include &quot;runtime/biasedLocking.hpp&quot;
  60 #include &quot;runtime/frame.inline.hpp&quot;
  61 #include &quot;runtime/handles.inline.hpp&quot;
  62 #include &quot;runtime/init.hpp&quot;
  63 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  64 #include &quot;runtime/java.hpp&quot;
  65 #include &quot;runtime/javaCalls.hpp&quot;
  66 #include &quot;runtime/sharedRuntime.hpp&quot;
  67 #include &quot;runtime/stubRoutines.hpp&quot;
  68 #include &quot;runtime/synchronizer.hpp&quot;
  69 #include &quot;runtime/vframe.inline.hpp&quot;
  70 #include &quot;runtime/vframeArray.hpp&quot;
  71 #include &quot;utilities/copy.hpp&quot;
  72 #include &quot;utilities/dtrace.hpp&quot;
  73 #include &quot;utilities/events.hpp&quot;
  74 #include &quot;utilities/hashtable.inline.hpp&quot;
  75 #include &quot;utilities/macros.hpp&quot;
  76 #include &quot;utilities/xmlstream.hpp&quot;
  77 #ifdef COMPILER1
  78 #include &quot;c1/c1_Runtime1.hpp&quot;
  79 #endif
  80 
  81 // Shared stub locations
  82 RuntimeStub*        SharedRuntime::_wrong_method_blob;
  83 RuntimeStub*        SharedRuntime::_wrong_method_abstract_blob;
  84 RuntimeStub*        SharedRuntime::_ic_miss_blob;
  85 RuntimeStub*        SharedRuntime::_resolve_opt_virtual_call_blob;
  86 RuntimeStub*        SharedRuntime::_resolve_virtual_call_blob;
  87 RuntimeStub*        SharedRuntime::_resolve_static_call_blob;
<span class="line-removed">  88 address             SharedRuntime::_resolve_static_call_entry;</span>
  89 
  90 DeoptimizationBlob* SharedRuntime::_deopt_blob;
  91 SafepointBlob*      SharedRuntime::_polling_page_vectors_safepoint_handler_blob;
  92 SafepointBlob*      SharedRuntime::_polling_page_safepoint_handler_blob;
  93 SafepointBlob*      SharedRuntime::_polling_page_return_handler_blob;
  94 
  95 #ifdef COMPILER2
  96 UncommonTrapBlob*   SharedRuntime::_uncommon_trap_blob;
  97 #endif // COMPILER2
  98 
  99 
 100 //----------------------------generate_stubs-----------------------------------
 101 void SharedRuntime::generate_stubs() {
 102   _wrong_method_blob                   = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::handle_wrong_method),          &quot;wrong_method_stub&quot;);
 103   _wrong_method_abstract_blob          = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::handle_wrong_method_abstract), &quot;wrong_method_abstract_stub&quot;);
 104   _ic_miss_blob                        = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::handle_wrong_method_ic_miss),  &quot;ic_miss_stub&quot;);
 105   _resolve_opt_virtual_call_blob       = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::resolve_opt_virtual_call_C),   &quot;resolve_opt_virtual_call&quot;);
 106   _resolve_virtual_call_blob           = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::resolve_virtual_call_C),       &quot;resolve_virtual_call&quot;);
 107   _resolve_static_call_blob            = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::resolve_static_call_C),        &quot;resolve_static_call&quot;);
<span class="line-removed"> 108   _resolve_static_call_entry           = _resolve_static_call_blob-&gt;entry_point();</span>
 109 
 110 #if COMPILER2_OR_JVMCI
 111   // Vectors are generated only by C2 and JVMCI.
 112   bool support_wide = is_wide_vector(MaxVectorSize);
 113   if (support_wide) {
 114     _polling_page_vectors_safepoint_handler_blob = generate_handler_blob(CAST_FROM_FN_PTR(address, SafepointSynchronize::handle_polling_page_exception), POLL_AT_VECTOR_LOOP);
 115   }
 116 #endif // COMPILER2_OR_JVMCI
 117   _polling_page_safepoint_handler_blob = generate_handler_blob(CAST_FROM_FN_PTR(address, SafepointSynchronize::handle_polling_page_exception), POLL_AT_LOOP);
 118   _polling_page_return_handler_blob    = generate_handler_blob(CAST_FROM_FN_PTR(address, SafepointSynchronize::handle_polling_page_exception), POLL_AT_RETURN);
 119 
 120   generate_deopt_blob();
 121 
 122 #ifdef COMPILER2
 123   generate_uncommon_trap_blob();
 124 #endif // COMPILER2
 125 }
 126 
 127 #include &lt;math.h&gt;
 128 
</pre>
<hr />
<pre>
1035   }
1036   return NULL;
1037 }
1038 
1039 // Finds receiver, CallInfo (i.e. receiver method), and calling bytecode
1040 // for a call current in progress, i.e., arguments has been pushed on stack
1041 // but callee has not been invoked yet.  Caller frame must be compiled.
1042 Handle SharedRuntime::find_callee_info_helper(JavaThread* thread,
1043                                               vframeStream&amp; vfst,
1044                                               Bytecodes::Code&amp; bc,
1045                                               CallInfo&amp; callinfo, TRAPS) {
1046   Handle receiver;
1047   Handle nullHandle;  //create a handy null handle for exception returns
1048 
1049   assert(!vfst.at_end(), &quot;Java frame must exist&quot;);
1050 
1051   // Find caller and bci from vframe
1052   methodHandle caller(THREAD, vfst.method());
1053   int          bci   = vfst.bci();
1054 















1055   Bytecode_invoke bytecode(caller, bci);
1056   int bytecode_index = bytecode.index();
1057   bc = bytecode.invoke_code();
1058 
1059   methodHandle attached_method(THREAD, extract_attached_method(vfst));
1060   if (attached_method.not_null()) {
1061     Method* callee = bytecode.static_target(CHECK_NH);
1062     vmIntrinsics::ID id = callee-&gt;intrinsic_id();
1063     // When VM replaces MH.invokeBasic/linkTo* call with a direct/virtual call,
1064     // it attaches statically resolved method to the call site.
1065     if (MethodHandles::is_signature_polymorphic(id) &amp;&amp;
1066         MethodHandles::is_signature_polymorphic_intrinsic(id)) {
1067       bc = MethodHandles::signature_polymorphic_intrinsic_bytecode(id);
1068 
1069       // Adjust invocation mode according to the attached method.
1070       switch (bc) {
1071         case Bytecodes::_invokevirtual:
1072           if (attached_method-&gt;method_holder()-&gt;is_interface()) {
1073             bc = Bytecodes::_invokeinterface;
1074           }
1075           break;
1076         case Bytecodes::_invokeinterface:
1077           if (!attached_method-&gt;method_holder()-&gt;is_interface()) {
1078             bc = Bytecodes::_invokevirtual;
1079           }
1080           break;
1081         case Bytecodes::_invokehandle:
1082           if (!MethodHandles::is_signature_polymorphic_method(attached_method())) {
1083             bc = attached_method-&gt;is_static() ? Bytecodes::_invokestatic
1084                                               : Bytecodes::_invokevirtual;
1085           }
1086           break;
1087         default:
1088           break;
1089       }






1090     }
1091   }
1092 
1093   assert(bc != Bytecodes::_illegal, &quot;not initialized&quot;);
1094 
1095   bool has_receiver = bc != Bytecodes::_invokestatic &amp;&amp;
1096                       bc != Bytecodes::_invokedynamic &amp;&amp;
1097                       bc != Bytecodes::_invokehandle;

1098 
1099   // Find receiver for non-static call
1100   if (has_receiver) {
1101     // This register map must be update since we need to find the receiver for
1102     // compiled frames. The receiver might be in a register.
1103     RegisterMap reg_map2(thread);
1104     frame stubFrame   = thread-&gt;last_frame();
1105     // Caller-frame is a compiled frame
1106     frame callerFrame = stubFrame.sender(&amp;reg_map2);

1107 
<span class="line-modified">1108     if (attached_method.is_null()) {</span>
<span class="line-modified">1109       Method* callee = bytecode.static_target(CHECK_NH);</span>





1110       if (callee == NULL) {
1111         THROW_(vmSymbols::java_lang_NoSuchMethodException(), nullHandle);
1112       }
1113     }
<span class="line-modified">1114 </span>
<span class="line-modified">1115     // Retrieve from a compiled argument list</span>
<span class="line-modified">1116     receiver = Handle(THREAD, callerFrame.retrieve_receiver(&amp;reg_map2));</span>
<span class="line-modified">1117 </span>
<span class="line-modified">1118     if (receiver.is_null()) {</span>
<span class="line-modified">1119       THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);</span>








1120     }
1121   }
1122 
1123   // Resolve method
1124   if (attached_method.not_null()) {
1125     // Parameterized by attached method.
<span class="line-modified">1126     LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, CHECK_NH);</span>
1127   } else {
1128     // Parameterized by bytecode.
1129     constantPoolHandle constants(THREAD, caller-&gt;constants());
1130     LinkResolver::resolve_invoke(callinfo, receiver, constants, bytecode_index, bc, CHECK_NH);
1131   }
1132 
1133 #ifdef ASSERT
1134   // Check that the receiver klass is of the right subtype and that it is initialized for virtual calls
<span class="line-modified">1135   if (has_receiver) {</span>
1136     assert(receiver.not_null(), &quot;should have thrown exception&quot;);
1137     Klass* receiver_klass = receiver-&gt;klass();
1138     Klass* rk = NULL;
1139     if (attached_method.not_null()) {
1140       // In case there&#39;s resolved method attached, use its holder during the check.
1141       rk = attached_method-&gt;method_holder();
1142     } else {
1143       // Klass is already loaded.
1144       constantPoolHandle constants(THREAD, caller-&gt;constants());
1145       rk = constants-&gt;klass_ref_at(bytecode_index, CHECK_NH);
1146     }
1147     Klass* static_receiver_klass = rk;
1148     assert(receiver_klass-&gt;is_subtype_of(static_receiver_klass),
1149            &quot;actual receiver must be subclass of static receiver klass&quot;);
1150     if (receiver_klass-&gt;is_instance_klass()) {
1151       if (InstanceKlass::cast(receiver_klass)-&gt;is_not_initialized()) {
1152         tty-&gt;print_cr(&quot;ERROR: Klass not yet initialized!!&quot;);
1153         receiver_klass-&gt;print();
1154       }
1155       assert(!InstanceKlass::cast(receiver_klass)-&gt;is_not_initialized(), &quot;receiver_klass must be initialized&quot;);
</pre>
<hr />
<pre>
1174     RegisterMap reg_map(thread, false);
1175     frame fr = thread-&gt;last_frame();
1176     assert(fr.is_runtime_frame(), &quot;must be a runtimeStub&quot;);
1177     fr = fr.sender(&amp;reg_map);
1178     assert(fr.is_entry_frame(), &quot;must be&quot;);
1179     // fr is now pointing to the entry frame.
1180     callee_method = methodHandle(THREAD, fr.entry_frame_call_wrapper()-&gt;callee_method());
1181   } else {
1182     Bytecodes::Code bc;
1183     CallInfo callinfo;
1184     find_callee_info_helper(thread, vfst, bc, callinfo, CHECK_(methodHandle()));
1185     callee_method = methodHandle(THREAD, callinfo.selected_method());
1186   }
1187   assert(callee_method()-&gt;is_method(), &quot;must be&quot;);
1188   return callee_method;
1189 }
1190 
1191 // Resolves a call.
1192 methodHandle SharedRuntime::resolve_helper(JavaThread *thread,
1193                                            bool is_virtual,
<span class="line-modified">1194                                            bool is_optimized, TRAPS) {</span>

1195   methodHandle callee_method;
<span class="line-modified">1196   callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, THREAD);</span>
1197   if (JvmtiExport::can_hotswap_or_post_breakpoint()) {
1198     int retry_count = 0;
1199     while (!HAS_PENDING_EXCEPTION &amp;&amp; callee_method-&gt;is_old() &amp;&amp;
1200            callee_method-&gt;method_holder() != SystemDictionary::Object_klass()) {
1201       // If has a pending exception then there is no need to re-try to
1202       // resolve this method.
1203       // If the method has been redefined, we need to try again.
1204       // Hack: we have no way to update the vtables of arrays, so don&#39;t
1205       // require that java.lang.Object has been updated.
1206 
1207       // It is very unlikely that method is redefined more than 100 times
1208       // in the middle of resolve. If it is looping here more than 100 times
1209       // means then there could be a bug here.
1210       guarantee((retry_count++ &lt; 100),
1211                 &quot;Could not resolve to latest version of redefined method&quot;);
1212       // method is redefined in the middle of resolve so re-try.
<span class="line-modified">1213       callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, THREAD);</span>
1214     }
1215   }
1216   return callee_method;
1217 }
1218 
1219 // This fails if resolution required refilling of IC stubs
1220 bool SharedRuntime::resolve_sub_helper_internal(methodHandle callee_method, const frame&amp; caller_frame,
1221                                                 CompiledMethod* caller_nm, bool is_virtual, bool is_optimized,
1222                                                 Handle receiver, CallInfo&amp; call_info, Bytecodes::Code invoke_code, TRAPS) {
1223   StaticCallInfo static_call_info;
1224   CompiledICInfo virtual_call_info;
1225 
1226   // Make sure the callee nmethod does not get deoptimized and removed before
1227   // we are done patching the code.
1228   CompiledMethod* callee = callee_method-&gt;code();
1229 
1230   if (callee != NULL) {
1231     assert(callee-&gt;is_compiled(), &quot;must be nmethod for patching&quot;);
1232   }
1233 
1234   if (callee != NULL &amp;&amp; !callee-&gt;is_in_use()) {
1235     // Patch call site to C2I adapter if callee nmethod is deoptimized or unloaded.
1236     callee = NULL;
1237   }
1238   nmethodLocker nl_callee(callee);
1239 #ifdef ASSERT
1240   address dest_entry_point = callee == NULL ? 0 : callee-&gt;entry_point(); // used below
1241 #endif
1242 
1243   bool is_nmethod = caller_nm-&gt;is_nmethod();

1244 
1245   if (is_virtual) {
<span class="line-modified">1246     assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, &quot;sanity check&quot;);</span>







1247     bool static_bound = call_info.resolved_method()-&gt;can_be_statically_bound();
<span class="line-modified">1248     Klass* klass = invoke_code == Bytecodes::_invokehandle ? NULL : receiver-&gt;klass();</span>
<span class="line-modified">1249     CompiledIC::compute_monomorphic_entry(callee_method, klass,</span>
<span class="line-removed">1250                      is_optimized, static_bound, is_nmethod, virtual_call_info,</span>
1251                      CHECK_false);
1252   } else {
1253     // static call
<span class="line-modified">1254     CompiledStaticCall::compute_entry(callee_method, is_nmethod, static_call_info);</span>
1255   }
1256 
1257   // grab lock, check for deoptimization and potentially patch caller
1258   {
1259     CompiledICLocker ml(caller_nm);
1260 
1261     // Lock blocks for safepoint during which both nmethods can change state.
1262 
1263     // Now that we are ready to patch if the Method* was redefined then
1264     // don&#39;t update call site and let the caller retry.
1265     // Don&#39;t update call site if callee nmethod was unloaded or deoptimized.
1266     // Don&#39;t update call site if callee nmethod was replaced by an other nmethod
1267     // which may happen when multiply alive nmethod (tiered compilation)
1268     // will be supported.
1269     if (!callee_method-&gt;is_old() &amp;&amp;
1270         (callee == NULL || (callee-&gt;is_in_use() &amp;&amp; callee_method-&gt;code() == callee))) {
1271       NoSafepointVerifier nsv;
1272 #ifdef ASSERT
1273       // We must not try to patch to jump to an already unloaded method.
1274       if (dest_entry_point != 0) {
</pre>
<hr />
<pre>
1286         }
1287       } else {
1288         if (VM_Version::supports_fast_class_init_checks() &amp;&amp;
1289             invoke_code == Bytecodes::_invokestatic &amp;&amp;
1290             callee_method-&gt;needs_clinit_barrier() &amp;&amp;
1291             callee != NULL &amp;&amp; (callee-&gt;is_compiled_by_jvmci() || callee-&gt;is_aot())) {
1292           return true; // skip patching for JVMCI or AOT code
1293         }
1294         CompiledStaticCall* ssc = caller_nm-&gt;compiledStaticCall_before(caller_frame.pc());
1295         if (ssc-&gt;is_clean()) ssc-&gt;set(static_call_info);
1296       }
1297     }
1298   } // unlock CompiledICLocker
1299   return true;
1300 }
1301 
1302 // Resolves a call.  The compilers generate code for calls that go here
1303 // and are patched with the real destination of the call.
1304 methodHandle SharedRuntime::resolve_sub_helper(JavaThread *thread,
1305                                                bool is_virtual,
<span class="line-modified">1306                                                bool is_optimized, TRAPS) {</span>

1307 
1308   ResourceMark rm(thread);
1309   RegisterMap cbl_map(thread, false);
1310   frame caller_frame = thread-&gt;last_frame().sender(&amp;cbl_map);
1311 
1312   CodeBlob* caller_cb = caller_frame.cb();
1313   guarantee(caller_cb != NULL &amp;&amp; caller_cb-&gt;is_compiled(), &quot;must be called from compiled method&quot;);
1314   CompiledMethod* caller_nm = caller_cb-&gt;as_compiled_method_or_null();

1315 
1316   // make sure caller is not getting deoptimized
1317   // and removed before we are done with it.
1318   // CLEANUP - with lazy deopt shouldn&#39;t need this lock
1319   nmethodLocker caller_lock(caller_nm);
1320 
1321   // determine call info &amp; receiver
1322   // note: a) receiver is NULL for static calls
1323   //       b) an exception is thrown if receiver is NULL for non-static calls
1324   CallInfo call_info;
1325   Bytecodes::Code invoke_code = Bytecodes::_illegal;
1326   Handle receiver = find_callee_info(thread, invoke_code,
1327                                      call_info, CHECK_(methodHandle()));
1328   methodHandle callee_method(THREAD, call_info.selected_method());
1329 
1330   assert((!is_virtual &amp;&amp; invoke_code == Bytecodes::_invokestatic ) ||
1331          (!is_virtual &amp;&amp; invoke_code == Bytecodes::_invokespecial) ||
1332          (!is_virtual &amp;&amp; invoke_code == Bytecodes::_invokehandle ) ||
1333          (!is_virtual &amp;&amp; invoke_code == Bytecodes::_invokedynamic) ||
1334          ( is_virtual &amp;&amp; invoke_code != Bytecodes::_invokestatic ), &quot;inconsistent bytecode&quot;);
</pre>
<hr />
<pre>
1396       return callee_method;
1397     } else {
1398       InlineCacheBuffer::refill_ic_stubs();
1399     }
1400   }
1401 
1402 }
1403 
1404 
1405 // Inline caches exist only in compiled code
1406 JRT_BLOCK_ENTRY(address, SharedRuntime::handle_wrong_method_ic_miss(JavaThread* thread))
1407 #ifdef ASSERT
1408   RegisterMap reg_map(thread, false);
1409   frame stub_frame = thread-&gt;last_frame();
1410   assert(stub_frame.is_runtime_frame(), &quot;sanity check&quot;);
1411   frame caller_frame = stub_frame.sender(&amp;reg_map);
1412   assert(!caller_frame.is_interpreted_frame() &amp;&amp; !caller_frame.is_entry_frame(), &quot;unexpected frame&quot;);
1413 #endif /* ASSERT */
1414 
1415   methodHandle callee_method;


1416   JRT_BLOCK
<span class="line-modified">1417     callee_method = SharedRuntime::handle_ic_miss_helper(thread, CHECK_NULL);</span>
1418     // Return Method* through TLS
1419     thread-&gt;set_vm_result_2(callee_method());
1420   JRT_BLOCK_END
1421   // return compiled code entry point after potential safepoints
<span class="line-modified">1422   assert(callee_method-&gt;verified_code_entry() != NULL, &quot; Jump to zero!&quot;);</span>
<span class="line-removed">1423   return callee_method-&gt;verified_code_entry();</span>
1424 JRT_END
1425 
1426 
1427 // Handle call site that has been made non-entrant
1428 JRT_BLOCK_ENTRY(address, SharedRuntime::handle_wrong_method(JavaThread* thread))
1429   // 6243940 We might end up in here if the callee is deoptimized
1430   // as we race to call it.  We don&#39;t want to take a safepoint if
1431   // the caller was interpreted because the caller frame will look
1432   // interpreted to the stack walkers and arguments are now
1433   // &quot;compiled&quot; so it is much better to make this transition
1434   // invisible to the stack walking code. The i2c path will
1435   // place the callee method in the callee_target. It is stashed
1436   // there because if we try and find the callee by normal means a
1437   // safepoint is possible and have trouble gc&#39;ing the compiled args.
1438   RegisterMap reg_map(thread, false);
1439   frame stub_frame = thread-&gt;last_frame();
1440   assert(stub_frame.is_runtime_frame(), &quot;sanity check&quot;);
1441   frame caller_frame = stub_frame.sender(&amp;reg_map);
1442 
1443   if (caller_frame.is_interpreted_frame() ||
</pre>
<hr />
<pre>
1446     guarantee(callee != NULL &amp;&amp; callee-&gt;is_method(), &quot;bad handshake&quot;);
1447     thread-&gt;set_vm_result_2(callee);
1448     thread-&gt;set_callee_target(NULL);
1449     if (caller_frame.is_entry_frame() &amp;&amp; VM_Version::supports_fast_class_init_checks()) {
1450       // Bypass class initialization checks in c2i when caller is in native.
1451       // JNI calls to static methods don&#39;t have class initialization checks.
1452       // Fast class initialization checks are present in c2i adapters and call into
1453       // SharedRuntime::handle_wrong_method() on the slow path.
1454       //
1455       // JVM upcalls may land here as well, but there&#39;s a proper check present in
1456       // LinkResolver::resolve_static_call (called from JavaCalls::call_static),
1457       // so bypassing it in c2i adapter is benign.
1458       return callee-&gt;get_c2i_no_clinit_check_entry();
1459     } else {
1460       return callee-&gt;get_c2i_entry();
1461     }
1462   }
1463 
1464   // Must be compiled to compiled path which is safe to stackwalk
1465   methodHandle callee_method;



1466   JRT_BLOCK
1467     // Force resolving of caller (if we called from compiled frame)
<span class="line-modified">1468     callee_method = SharedRuntime::reresolve_call_site(thread, CHECK_NULL);</span>
1469     thread-&gt;set_vm_result_2(callee_method());
1470   JRT_BLOCK_END
1471   // return compiled code entry point after potential safepoints
<span class="line-modified">1472   assert(callee_method-&gt;verified_code_entry() != NULL, &quot; Jump to zero!&quot;);</span>
<span class="line-removed">1473   return callee_method-&gt;verified_code_entry();</span>
1474 JRT_END
1475 
1476 // Handle abstract method call
1477 JRT_BLOCK_ENTRY(address, SharedRuntime::handle_wrong_method_abstract(JavaThread* thread))
1478   // Verbose error message for AbstractMethodError.
1479   // Get the called method from the invoke bytecode.
1480   vframeStream vfst(thread, true);
1481   assert(!vfst.at_end(), &quot;Java frame must exist&quot;);
1482   methodHandle caller(thread, vfst.method());
1483   Bytecode_invoke invoke(caller, vfst.bci());
1484   DEBUG_ONLY( invoke.verify(); )
1485 
1486   // Find the compiled caller frame.
1487   RegisterMap reg_map(thread);
1488   frame stubFrame = thread-&gt;last_frame();
1489   assert(stubFrame.is_runtime_frame(), &quot;must be&quot;);
1490   frame callerFrame = stubFrame.sender(&amp;reg_map);
1491   assert(callerFrame.is_compiled_frame(), &quot;must be&quot;);
1492 
1493   // Install exception and return forward entry.
1494   address res = StubRoutines::throw_AbstractMethodError_entry();
1495   JRT_BLOCK
1496     methodHandle callee(thread, invoke.static_target(thread));
1497     if (!callee.is_null()) {
1498       oop recv = callerFrame.retrieve_receiver(&amp;reg_map);
1499       Klass *recv_klass = (recv != NULL) ? recv-&gt;klass() : NULL;
1500       LinkResolver::throw_abstract_method_error(callee, recv_klass, thread);
1501       res = StubRoutines::forward_exception_entry();
1502     }
1503   JRT_BLOCK_END
1504   return res;
1505 JRT_END
1506 
1507 
1508 // resolve a static call and patch code
1509 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_static_call_C(JavaThread *thread ))
1510   methodHandle callee_method;

1511   JRT_BLOCK
<span class="line-modified">1512     callee_method = SharedRuntime::resolve_helper(thread, false, false, CHECK_NULL);</span>
1513     thread-&gt;set_vm_result_2(callee_method());
1514   JRT_BLOCK_END
1515   // return compiled code entry point after potential safepoints
<span class="line-modified">1516   assert(callee_method-&gt;verified_code_entry() != NULL, &quot; Jump to zero!&quot;);</span>
<span class="line-modified">1517   return callee_method-&gt;verified_code_entry();</span>


1518 JRT_END
1519 
1520 
1521 // resolve virtual call and update inline cache to monomorphic
1522 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_virtual_call_C(JavaThread *thread ))
1523   methodHandle callee_method;

1524   JRT_BLOCK
<span class="line-modified">1525     callee_method = SharedRuntime::resolve_helper(thread, true, false, CHECK_NULL);</span>
1526     thread-&gt;set_vm_result_2(callee_method());
1527   JRT_BLOCK_END
1528   // return compiled code entry point after potential safepoints
<span class="line-modified">1529   assert(callee_method-&gt;verified_code_entry() != NULL, &quot; Jump to zero!&quot;);</span>
<span class="line-modified">1530   return callee_method-&gt;verified_code_entry();</span>


1531 JRT_END
1532 
1533 
1534 // Resolve a virtual call that can be statically bound (e.g., always
1535 // monomorphic, so it has no inline cache).  Patch code to resolved target.
1536 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_opt_virtual_call_C(JavaThread *thread))
1537   methodHandle callee_method;

1538   JRT_BLOCK
<span class="line-modified">1539     callee_method = SharedRuntime::resolve_helper(thread, true, true, CHECK_NULL);</span>
1540     thread-&gt;set_vm_result_2(callee_method());
1541   JRT_BLOCK_END
1542   // return compiled code entry point after potential safepoints
<span class="line-modified">1543   assert(callee_method-&gt;verified_code_entry() != NULL, &quot; Jump to zero!&quot;);</span>
<span class="line-modified">1544   return callee_method-&gt;verified_code_entry();</span>


1545 JRT_END
1546 
1547 // The handle_ic_miss_helper_internal function returns false if it failed due
1548 // to either running out of vtable stubs or ic stubs due to IC transitions
1549 // to transitional states. The needs_ic_stub_refill value will be set if
1550 // the failure was due to running out of IC stubs, in which case handle_ic_miss_helper
1551 // refills the IC stubs and tries again.
1552 bool SharedRuntime::handle_ic_miss_helper_internal(Handle receiver, CompiledMethod* caller_nm,
1553                                                    const frame&amp; caller_frame, methodHandle callee_method,
1554                                                    Bytecodes::Code bc, CallInfo&amp; call_info,
<span class="line-modified">1555                                                    bool&amp; needs_ic_stub_refill, TRAPS) {</span>
1556   CompiledICLocker ml(caller_nm);
1557   CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());
1558   bool should_be_mono = false;
1559   if (inline_cache-&gt;is_optimized()) {
1560     if (TraceCallFixup) {
1561       ResourceMark rm(THREAD);
1562       tty-&gt;print(&quot;OPTIMIZED IC miss (%s) call to&quot;, Bytecodes::name(bc));
1563       callee_method-&gt;print_short_name(tty);
1564       tty-&gt;print_cr(&quot; code: &quot; INTPTR_FORMAT, p2i(callee_method-&gt;code()));
1565     }

1566     should_be_mono = true;
1567   } else if (inline_cache-&gt;is_icholder_call()) {
1568     CompiledICHolder* ic_oop = inline_cache-&gt;cached_icholder();
1569     if (ic_oop != NULL) {
1570       if (!ic_oop-&gt;is_loader_alive()) {
1571         // Deferred IC cleaning due to concurrent class unloading
1572         if (!inline_cache-&gt;set_to_clean()) {
1573           needs_ic_stub_refill = true;
1574           return false;
1575         }
1576       } else if (receiver()-&gt;klass() == ic_oop-&gt;holder_klass()) {
1577         // This isn&#39;t a real miss. We must have seen that compiled code
1578         // is now available and we want the call site converted to a
1579         // monomorphic compiled call site.
1580         // We can&#39;t assert for callee_method-&gt;code() != NULL because it
1581         // could have been deoptimized in the meantime
1582         if (TraceCallFixup) {
1583           ResourceMark rm(THREAD);
1584           tty-&gt;print(&quot;FALSE IC miss (%s) converting to compiled call to&quot;, Bytecodes::name(bc));
1585           callee_method-&gt;print_short_name(tty);
1586           tty-&gt;print_cr(&quot; code: &quot; INTPTR_FORMAT, p2i(callee_method-&gt;code()));
1587         }
1588         should_be_mono = true;
1589       }
1590     }
1591   }
1592 
1593   if (should_be_mono) {
1594     // We have a path that was monomorphic but was going interpreted
1595     // and now we have (or had) a compiled entry. We correct the IC
1596     // by using a new icBuffer.
1597     CompiledICInfo info;
1598     Klass* receiver_klass = receiver()-&gt;klass();
1599     inline_cache-&gt;compute_monomorphic_entry(callee_method,
1600                                             receiver_klass,
1601                                             inline_cache-&gt;is_optimized(),
1602                                             false, caller_nm-&gt;is_nmethod(),

1603                                             info, CHECK_false);
1604     if (!inline_cache-&gt;set_to_monomorphic(info)) {
1605       needs_ic_stub_refill = true;
1606       return false;
1607     }
1608   } else if (!inline_cache-&gt;is_megamorphic() &amp;&amp; !inline_cache-&gt;is_clean()) {
1609     // Potential change to megamorphic
1610 
<span class="line-modified">1611     bool successful = inline_cache-&gt;set_to_megamorphic(&amp;call_info, bc, needs_ic_stub_refill, CHECK_false);</span>
1612     if (needs_ic_stub_refill) {
1613       return false;
1614     }
1615     if (!successful) {
1616       if (!inline_cache-&gt;set_to_clean()) {
1617         needs_ic_stub_refill = true;
1618         return false;
1619       }
1620     }
1621   } else {
1622     // Either clean or megamorphic
1623   }
1624   return true;
1625 }
1626 
<span class="line-modified">1627 methodHandle SharedRuntime::handle_ic_miss_helper(JavaThread *thread, TRAPS) {</span>
1628   ResourceMark rm(thread);
1629   CallInfo call_info;
1630   Bytecodes::Code bc;
1631 
1632   // receiver is NULL for static calls. An exception is thrown for NULL
1633   // receivers for non-static calls
1634   Handle receiver = find_callee_info(thread, bc, call_info,
1635                                      CHECK_(methodHandle()));
1636   // Compiler1 can produce virtual call sites that can actually be statically bound
1637   // If we fell thru to below we would think that the site was going megamorphic
1638   // when in fact the site can never miss. Worse because we&#39;d think it was megamorphic
1639   // we&#39;d try and do a vtable dispatch however methods that can be statically bound
1640   // don&#39;t have vtable entries (vtable_index &lt; 0) and we&#39;d blow up. So we force a
1641   // reresolution of the  call site (as if we did a handle_wrong_method and not an
1642   // plain ic_miss) and the site will be converted to an optimized virtual call site
1643   // never to miss again. I don&#39;t believe C2 will produce code like this but if it
1644   // did this would still be the correct thing to do for it too, hence no ifdef.
1645   //
1646   if (call_info.resolved_method()-&gt;can_be_statically_bound()) {
<span class="line-modified">1647     methodHandle callee_method = SharedRuntime::reresolve_call_site(thread, CHECK_(methodHandle()));</span>


1648     if (TraceCallFixup) {
1649       RegisterMap reg_map(thread, false);
1650       frame caller_frame = thread-&gt;last_frame().sender(&amp;reg_map);
1651       ResourceMark rm(thread);
1652       tty-&gt;print(&quot;converting IC miss to reresolve (%s) call to&quot;, Bytecodes::name(bc));
1653       callee_method-&gt;print_short_name(tty);
1654       tty-&gt;print_cr(&quot; from pc: &quot; INTPTR_FORMAT, p2i(caller_frame.pc()));
1655       tty-&gt;print_cr(&quot; code: &quot; INTPTR_FORMAT, p2i(callee_method-&gt;code()));
1656     }
1657     return callee_method;
1658   }
1659 
1660   methodHandle callee_method(thread, call_info.selected_method());
1661 
1662 #ifndef PRODUCT
1663   Atomic::inc(&amp;_ic_miss_ctr);
1664 
1665   // Statistics &amp; Tracing
1666   if (TraceCallFixup) {
1667     ResourceMark rm(thread);
</pre>
<hr />
<pre>
1677     // produce statistics under the lock
1678     trace_ic_miss(f.pc());
1679   }
1680 #endif
1681 
1682   // install an event collector so that when a vtable stub is created the
1683   // profiler can be notified via a DYNAMIC_CODE_GENERATED event. The
1684   // event can&#39;t be posted when the stub is created as locks are held
1685   // - instead the event will be deferred until the event collector goes
1686   // out of scope.
1687   JvmtiDynamicCodeEventCollector event_collector;
1688 
1689   // Update inline cache to megamorphic. Skip update if we are called from interpreted.
1690   // Transitioning IC caches may require transition stubs. If we run out
1691   // of transition stubs, we have to drop locks and perform a safepoint
1692   // that refills them.
1693   RegisterMap reg_map(thread, false);
1694   frame caller_frame = thread-&gt;last_frame().sender(&amp;reg_map);
1695   CodeBlob* cb = caller_frame.cb();
1696   CompiledMethod* caller_nm = cb-&gt;as_compiled_method();

1697 
1698   for (;;) {
1699     ICRefillVerifier ic_refill_verifier;
1700     bool needs_ic_stub_refill = false;
1701     bool successful = handle_ic_miss_helper_internal(receiver, caller_nm, caller_frame, callee_method,
<span class="line-modified">1702                                                      bc, call_info, needs_ic_stub_refill, CHECK_(methodHandle()));</span>
1703     if (successful || !needs_ic_stub_refill) {
1704       return callee_method;
1705     } else {
1706       InlineCacheBuffer::refill_ic_stubs();
1707     }
1708   }
1709 }
1710 
1711 static bool clear_ic_at_addr(CompiledMethod* caller_nm, address call_addr, bool is_static_call) {
1712   CompiledICLocker ml(caller_nm);
1713   if (is_static_call) {
1714     CompiledStaticCall* ssc = caller_nm-&gt;compiledStaticCall_at(call_addr);
1715     if (!ssc-&gt;is_clean()) {
1716       return ssc-&gt;set_to_clean();
1717     }
1718   } else {
1719     // compiled, dispatched call (which used to call an interpreted method)
1720     CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);
1721     if (!inline_cache-&gt;is_clean()) {
1722       return inline_cache-&gt;set_to_clean();
1723     }
1724   }
1725   return true;
1726 }
1727 
1728 //
1729 // Resets a call-site in compiled code so it will get resolved again.
1730 // This routines handles both virtual call sites, optimized virtual call
1731 // sites, and static call sites. Typically used to change a call sites
1732 // destination from compiled to interpreted.
1733 //
<span class="line-modified">1734 methodHandle SharedRuntime::reresolve_call_site(JavaThread *thread, TRAPS) {</span>
1735   ResourceMark rm(thread);
1736   RegisterMap reg_map(thread, false);
1737   frame stub_frame = thread-&gt;last_frame();
1738   assert(stub_frame.is_runtime_frame(), &quot;must be a runtimeStub&quot;);
1739   frame caller = stub_frame.sender(&amp;reg_map);
1740 
1741   // Do nothing if the frame isn&#39;t a live compiled frame.
1742   // nmethod could be deoptimized by the time we get here
1743   // so no update to the caller is needed.
1744 
1745   if (caller.is_compiled_frame() &amp;&amp; !caller.is_deoptimized_frame()) {
1746 
1747     address pc = caller.pc();
1748 
1749     // Check for static or virtual call
<span class="line-modified">1750     bool is_static_call = false;</span>
1751     CompiledMethod* caller_nm = CodeCache::find_compiled(pc);
1752 
1753     // Default call_addr is the location of the &quot;basic&quot; call.
1754     // Determine the address of the call we a reresolving. With
1755     // Inline Caches we will always find a recognizable call.
1756     // With Inline Caches disabled we may or may not find a
1757     // recognizable call. We will always find a call for static
1758     // calls and for optimized virtual calls. For vanilla virtual
1759     // calls it depends on the state of the UseInlineCaches switch.
1760     //
1761     // With Inline Caches disabled we can get here for a virtual call
1762     // for two reasons:
1763     //   1 - calling an abstract method. The vtable for abstract methods
1764     //       will run us thru handle_wrong_method and we will eventually
1765     //       end up in the interpreter to throw the ame.
1766     //   2 - a racing deoptimization. We could be doing a vanilla vtable
1767     //       call and between the time we fetch the entry address and
1768     //       we jump to it the target gets deoptimized. Similar to 1
1769     //       we will wind up in the interprter (thru a c2i with c2).
1770     //
</pre>
<hr />
<pre>
1775       CompiledICLocker ml(caller_nm);
1776       // Location of call instruction
1777       call_addr = caller_nm-&gt;call_instruction_address(pc);
1778     }
1779     // Make sure nmethod doesn&#39;t get deoptimized and removed until
1780     // this is done with it.
1781     // CLEANUP - with lazy deopt shouldn&#39;t need this lock
1782     nmethodLocker nmlock(caller_nm);
1783 
1784     if (call_addr != NULL) {
1785       RelocIterator iter(caller_nm, call_addr, call_addr+1);
1786       int ret = iter.next(); // Get item
1787       if (ret) {
1788         assert(iter.addr() == call_addr, &quot;must find call&quot;);
1789         if (iter.type() == relocInfo::static_call_type) {
1790           is_static_call = true;
1791         } else {
1792           assert(iter.type() == relocInfo::virtual_call_type ||
1793                  iter.type() == relocInfo::opt_virtual_call_type
1794                 , &quot;unexpected relocInfo. type&quot;);

1795         }
1796       } else {
1797         assert(!UseInlineCaches, &quot;relocation info. must exist for this address&quot;);
1798       }
1799 
1800       // Cleaning the inline cache will force a new resolve. This is more robust
1801       // than directly setting it to the new destination, since resolving of calls
1802       // is always done through the same code path. (experience shows that it
1803       // leads to very hard to track down bugs, if an inline cache gets updated
1804       // to a wrong method). It should not be performance critical, since the
1805       // resolve is only done once.
1806 
1807       for (;;) {
1808         ICRefillVerifier ic_refill_verifier;
1809         if (!clear_ic_at_addr(caller_nm, call_addr, is_static_call)) {
1810           InlineCacheBuffer::refill_ic_stubs();
1811         } else {
1812           break;
1813         }
1814       }
1815     }
1816   }
1817 
1818   methodHandle callee_method = find_callee_method(thread, CHECK_(methodHandle()));
1819 
<span class="line-removed">1820 </span>
1821 #ifndef PRODUCT
1822   Atomic::inc(&amp;_wrong_method_ctr);
1823 
1824   if (TraceCallFixup) {
1825     ResourceMark rm(thread);
1826     tty-&gt;print(&quot;handle_wrong_method reresolving call to&quot;);
1827     callee_method-&gt;print_short_name(tty);
1828     tty-&gt;print_cr(&quot; code: &quot; INTPTR_FORMAT, p2i(callee_method-&gt;code()));
1829   }
1830 #endif
1831 
1832   return callee_method;
1833 }
1834 
1835 address SharedRuntime::handle_unsafe_access(JavaThread* thread, address next_pc) {
1836   // The faulting unsafe accesses should be changed to throw the error
1837   // synchronously instead. Meanwhile the faulting instruction will be
1838   // skipped over (effectively turning it into a no-op) and an
1839   // asynchronous exception will be raised which the thread will
1840   // handle at a later point. If the instruction is a load it will
</pre>
<hr />
<pre>
1895     }
1896   } else {
1897     if (TraceCallFixup) {
1898       tty-&gt;print(&quot;already patched callsite at &quot; INTPTR_FORMAT &quot; to compiled code for&quot;, p2i(caller_pc));
1899       moop-&gt;print_short_name(tty);
1900       tty-&gt;print_cr(&quot; to &quot; INTPTR_FORMAT, p2i(entry_point));
1901     }
1902   }
1903   return false;
1904 }
1905 
1906 // ---------------------------------------------------------------------------
1907 // We are calling the interpreter via a c2i. Normally this would mean that
1908 // we were called by a compiled method. However we could have lost a race
1909 // where we went int -&gt; i2c -&gt; c2i and so the caller could in fact be
1910 // interpreted. If the caller is compiled we attempt to patch the caller
1911 // so he no longer calls into the interpreter.
1912 JRT_LEAF(void, SharedRuntime::fixup_callers_callsite(Method* method, address caller_pc))
1913   Method* moop(method);
1914 
<span class="line-removed">1915   address entry_point = moop-&gt;from_compiled_entry_no_trampoline();</span>
<span class="line-removed">1916 </span>
1917   // It&#39;s possible that deoptimization can occur at a call site which hasn&#39;t
1918   // been resolved yet, in which case this function will be called from
1919   // an nmethod that has been patched for deopt and we can ignore the
1920   // request for a fixup.
1921   // Also it is possible that we lost a race in that from_compiled_entry
1922   // is now back to the i2c in that case we don&#39;t need to patch and if
1923   // we did we&#39;d leap into space because the callsite needs to use
1924   // &quot;to interpreter&quot; stub in order to load up the Method*. Don&#39;t
1925   // ask me how I know this...
1926 
1927   CodeBlob* cb = CodeCache::find_blob(caller_pc);
<span class="line-modified">1928   if (cb == NULL || !cb-&gt;is_compiled() || entry_point == moop-&gt;get_c2i_entry()) {</span>




1929     return;
1930   }
1931 
1932   // The check above makes sure this is a nmethod.
1933   CompiledMethod* nm = cb-&gt;as_compiled_method_or_null();
1934   assert(nm, &quot;must be&quot;);
1935 
1936   // Get the return PC for the passed caller PC.
1937   address return_pc = caller_pc + frame::pc_return_offset;
1938 
1939   // There is a benign race here. We could be attempting to patch to a compiled
1940   // entry point at the same time the callee is being deoptimized. If that is
1941   // the case then entry_point may in fact point to a c2i and we&#39;d patch the
1942   // call site with the same old data. clear_code will set code() to NULL
1943   // at the end of it. If we happen to see that NULL then we can skip trying
1944   // to patch. If we hit the window where the callee has a c2i in the
1945   // from_compiled_entry and the NULL isn&#39;t present yet then we lose the race
1946   // and patch the code with the same old data. Asi es la vida.
1947 
1948   if (moop-&gt;code() == NULL) return;
</pre>
<hr />
<pre>
2269  private:
2270   enum {
2271     _basic_type_bits = 4,
2272     _basic_type_mask = right_n_bits(_basic_type_bits),
2273     _basic_types_per_int = BitsPerInt / _basic_type_bits,
2274     _compact_int_count = 3
2275   };
2276   // TO DO:  Consider integrating this with a more global scheme for compressing signatures.
2277   // For now, 4 bits per components (plus T_VOID gaps after double/long) is not excessive.
2278 
2279   union {
2280     int  _compact[_compact_int_count];
2281     int* _fingerprint;
2282   } _value;
2283   int _length; // A negative length indicates the fingerprint is in the compact form,
2284                // Otherwise _value._fingerprint is the array.
2285 
2286   // Remap BasicTypes that are handled equivalently by the adapters.
2287   // These are correct for the current system but someday it might be
2288   // necessary to make this mapping platform dependent.
<span class="line-modified">2289   static int adapter_encoding(BasicType in) {</span>
2290     switch (in) {
2291       case T_BOOLEAN:
2292       case T_BYTE:
2293       case T_SHORT:
<span class="line-modified">2294       case T_CHAR:</span>
<span class="line-modified">2295         // There are all promoted to T_INT in the calling convention</span>
<span class="line-modified">2296         return T_INT;</span>













2297 
2298       case T_OBJECT:
2299       case T_ARRAY:
2300         // In other words, we assume that any register good enough for
2301         // an int or long is good enough for a managed pointer.
2302 #ifdef _LP64
2303         return T_LONG;
2304 #else
2305         return T_INT;
2306 #endif
2307 
2308       case T_INT:
2309       case T_LONG:
2310       case T_FLOAT:
2311       case T_DOUBLE:
2312       case T_VOID:
2313         return in;
2314 
2315       default:
2316         ShouldNotReachHere();
2317         return T_CONFLICT;
2318     }
2319   }
2320 
2321  public:
<span class="line-modified">2322   AdapterFingerPrint(int total_args_passed, BasicType* sig_bt) {</span>
2323     // The fingerprint is based on the BasicType signature encoded
2324     // into an array of ints with eight entries per int.

2325     int* ptr;
2326     int len = (total_args_passed + (_basic_types_per_int-1)) / _basic_types_per_int;
2327     if (len &lt;= _compact_int_count) {
2328       assert(_compact_int_count == 3, &quot;else change next line&quot;);
2329       _value._compact[0] = _value._compact[1] = _value._compact[2] = 0;
2330       // Storing the signature encoded as signed chars hits about 98%
2331       // of the time.
2332       _length = -len;
2333       ptr = _value._compact;
2334     } else {
2335       _length = len;
2336       _value._fingerprint = NEW_C_HEAP_ARRAY(int, _length, mtCode);
2337       ptr = _value._fingerprint;
2338     }
2339 
2340     // Now pack the BasicTypes with 8 per int
2341     int sig_index = 0;


2342     for (int index = 0; index &lt; len; index++) {
2343       int value = 0;
2344       for (int byte = 0; byte &lt; _basic_types_per_int; byte++) {
<span class="line-modified">2345         int bt = ((sig_index &lt; total_args_passed)</span>
<span class="line-modified">2346                   ? adapter_encoding(sig_bt[sig_index++])</span>
<span class="line-modified">2347                   : 0);</span>

















2348         assert((bt &amp; _basic_type_mask) == bt, &quot;must fit in 4 bits&quot;);
2349         value = (value &lt;&lt; _basic_type_bits) | bt;
2350       }
2351       ptr[index] = value;
2352     }

2353   }
2354 
2355   ~AdapterFingerPrint() {
2356     if (_length &gt; 0) {
2357       FREE_C_HEAP_ARRAY(int, _value._fingerprint);
2358     }
2359   }
2360 
2361   int value(int index) {
2362     if (_length &lt; 0) {
2363       return _value._compact[index];
2364     }
2365     return _value._fingerprint[index];
2366   }
2367   int length() {
2368     if (_length &lt; 0) return -_length;
2369     return _length;
2370   }
2371 
2372   bool is_compact() {
</pre>
<hr />
<pre>
2418 
2419  private:
2420 
2421 #ifndef PRODUCT
2422   static int _lookups; // number of calls to lookup
2423   static int _buckets; // number of buckets checked
2424   static int _equals;  // number of buckets checked with matching hash
2425   static int _hits;    // number of successful lookups
2426   static int _compact; // number of equals calls with compact signature
2427 #endif
2428 
2429   AdapterHandlerEntry* bucket(int i) {
2430     return (AdapterHandlerEntry*)BasicHashtable&lt;mtCode&gt;::bucket(i);
2431   }
2432 
2433  public:
2434   AdapterHandlerTable()
2435     : BasicHashtable&lt;mtCode&gt;(293, (DumpSharedSpaces ? sizeof(CDSAdapterHandlerEntry) : sizeof(AdapterHandlerEntry))) { }
2436 
2437   // Create a new entry suitable for insertion in the table
<span class="line-modified">2438   AdapterHandlerEntry* new_entry(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry, address c2i_unverified_entry, address c2i_no_clinit_check_entry) {</span>


2439     AdapterHandlerEntry* entry = (AdapterHandlerEntry*)BasicHashtable&lt;mtCode&gt;::new_entry(fingerprint-&gt;compute_hash());
<span class="line-modified">2440     entry-&gt;init(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);</span>

2441     if (DumpSharedSpaces) {
2442       ((CDSAdapterHandlerEntry*)entry)-&gt;init();
2443     }
2444     return entry;
2445   }
2446 
2447   // Insert an entry into the table
2448   void add(AdapterHandlerEntry* entry) {
2449     int index = hash_to_index(entry-&gt;hash());
2450     add_entry(index, entry);
2451   }
2452 
2453   void free_entry(AdapterHandlerEntry* entry) {
2454     entry-&gt;deallocate();
2455     BasicHashtable&lt;mtCode&gt;::free_entry(entry);
2456   }
2457 
2458   // Find a entry with the same fingerprint if it exists
<span class="line-modified">2459   AdapterHandlerEntry* lookup(int total_args_passed, BasicType* sig_bt) {</span>
2460     NOT_PRODUCT(_lookups++);
<span class="line-modified">2461     AdapterFingerPrint fp(total_args_passed, sig_bt);</span>
2462     unsigned int hash = fp.compute_hash();
2463     int index = hash_to_index(hash);
2464     for (AdapterHandlerEntry* e = bucket(index); e != NULL; e = e-&gt;next()) {
2465       NOT_PRODUCT(_buckets++);
2466       if (e-&gt;hash() == hash) {
2467         NOT_PRODUCT(_equals++);
2468         if (fp.equals(e-&gt;fingerprint())) {
2469 #ifndef PRODUCT
2470           if (fp.is_compact()) _compact++;
2471           _hits++;
2472 #endif
2473           return e;
2474         }
2475       }
2476     }
2477     return NULL;
2478   }
2479 
2480 #ifndef PRODUCT
2481   void print_statistics() {
</pre>
<hr />
<pre>
2537   bool has_next() {
2538     return _current != NULL;
2539   }
2540   AdapterHandlerEntry* next() {
2541     if (_current != NULL) {
2542       AdapterHandlerEntry* result = _current;
2543       _current = _current-&gt;next();
2544       if (_current == NULL) scan();
2545       return result;
2546     } else {
2547       return NULL;
2548     }
2549   }
2550 };
2551 
2552 
2553 // ---------------------------------------------------------------------------
2554 // Implementation of AdapterHandlerLibrary
2555 AdapterHandlerTable* AdapterHandlerLibrary::_adapters = NULL;
2556 AdapterHandlerEntry* AdapterHandlerLibrary::_abstract_method_handler = NULL;
<span class="line-modified">2557 const int AdapterHandlerLibrary_size = 16*K;</span>
2558 BufferBlob* AdapterHandlerLibrary::_buffer = NULL;
2559 
2560 BufferBlob* AdapterHandlerLibrary::buffer_blob() {
2561   // Should be called only when AdapterHandlerLibrary_lock is active.
2562   if (_buffer == NULL) // Initialize lazily
2563       _buffer = BufferBlob::create(&quot;adapters&quot;, AdapterHandlerLibrary_size);
2564   return _buffer;
2565 }
2566 
2567 extern &quot;C&quot; void unexpected_adapter_call() {
2568   ShouldNotCallThis();
2569 }
2570 
2571 void AdapterHandlerLibrary::initialize() {
2572   if (_adapters != NULL) return;
2573   _adapters = new AdapterHandlerTable();
2574 
2575   // Create a special handler for abstract methods.  Abstract methods
2576   // are never compiled so an i2c entry is somewhat meaningless, but
2577   // throw AbstractMethodError just in case.
2578   // Pass wrong_method_abstract for the c2i transitions to return
2579   // AbstractMethodError for invalid invocations.
2580   address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();
<span class="line-modified">2581   _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(0, NULL),</span>
2582                                                               StubRoutines::throw_AbstractMethodError_entry(),

2583                                                               wrong_method_abstract, wrong_method_abstract);
2584 }
2585 
2586 AdapterHandlerEntry* AdapterHandlerLibrary::new_entry(AdapterFingerPrint* fingerprint,
2587                                                       address i2c_entry,
2588                                                       address c2i_entry,


2589                                                       address c2i_unverified_entry,

2590                                                       address c2i_no_clinit_check_entry) {
<span class="line-modified">2591   return _adapters-&gt;new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);</span>















2592 }
2593 
2594 AdapterHandlerEntry* AdapterHandlerLibrary::get_adapter(const methodHandle&amp; method) {
2595   AdapterHandlerEntry* entry = get_adapter0(method);
2596   if (entry != NULL &amp;&amp; method-&gt;is_shared()) {
2597     // See comments around Method::link_method()
2598     MutexLocker mu(AdapterHandlerLibrary_lock);
2599     if (method-&gt;adapter() == NULL) {
2600       method-&gt;update_adapter_trampoline(entry);
2601     }
<span class="line-modified">2602     address trampoline = method-&gt;from_compiled_entry();</span>
<span class="line-modified">2603     if (*(int*)trampoline == 0) {</span>
<span class="line-modified">2604       CodeBuffer buffer(trampoline, (int)SharedRuntime::trampoline_size());</span>
<span class="line-modified">2605       MacroAssembler _masm(&amp;buffer);</span>
<span class="line-modified">2606       SharedRuntime::generate_trampoline(&amp;_masm, entry-&gt;get_c2i_entry());</span>
<span class="line-modified">2607       assert(*(int*)trampoline != 0, &quot;Instruction(s) for trampoline must not be encoded as zeros.&quot;);</span>
<span class="line-modified">2608       _masm.flush();</span>

2609 
<span class="line-modified">2610       if (PrintInterpreter) {</span>
<span class="line-modified">2611         Disassembler::decode(buffer.insts_begin(), buffer.insts_end());</span>


























2612       }


2613     }
2614   }



2615 
<span class="line-modified">2616   return entry;</span>



























































































































































2617 }
2618 
2619 AdapterHandlerEntry* AdapterHandlerLibrary::get_adapter0(const methodHandle&amp; method) {
2620   // Use customized signature handler.  Need to lock around updates to
2621   // the AdapterHandlerTable (it is not safe for concurrent readers
2622   // and a single writer: this could be fixed if it becomes a
2623   // problem).
2624 
2625   ResourceMark rm;
2626 
<span class="line-modified">2627   NOT_PRODUCT(int insts_size);</span>
2628   AdapterBlob* new_adapter = NULL;
2629   AdapterHandlerEntry* entry = NULL;
2630   AdapterFingerPrint* fingerprint = NULL;

2631   {
2632     MutexLocker mu(AdapterHandlerLibrary_lock);
2633     // make sure data structure is initialized
2634     initialize();
2635 
<span class="line-modified">2636     if (method-&gt;is_abstract()) {</span>
<span class="line-modified">2637       return _abstract_method_handler;</span>


2638     }






2639 
<span class="line-modified">2640     // Fill in the signature array, for the calling-convention call.</span>
<span class="line-modified">2641     int total_args_passed = method-&gt;size_of_parameters(); // All args on stack</span>



2642 
<span class="line-modified">2643     BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_args_passed);</span>
<span class="line-modified">2644     VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);</span>
<span class="line-modified">2645     int i = 0;</span>
<span class="line-modified">2646     if (!method-&gt;is_static())  // Pass in receiver first</span>
<span class="line-modified">2647       sig_bt[i++] = T_OBJECT;</span>
<span class="line-modified">2648     for (SignatureStream ss(method-&gt;signature()); !ss.at_return_type(); ss.next()) {</span>
<span class="line-modified">2649       sig_bt[i++] = ss.type();  // Collect remaining bits of signature</span>
<span class="line-modified">2650       if (ss.type() == T_LONG || ss.type() == T_DOUBLE)</span>
<span class="line-modified">2651         sig_bt[i++] = T_VOID;   // Longs &amp; doubles take 2 Java slots</span>






2652     }
<span class="line-removed">2653     assert(i == total_args_passed, &quot;&quot;);</span>
2654 
2655     // Lookup method signature&#39;s fingerprint
<span class="line-modified">2656     entry = _adapters-&gt;lookup(total_args_passed, sig_bt);</span>
2657 
2658 #ifdef ASSERT
2659     AdapterHandlerEntry* shared_entry = NULL;
2660     // Start adapter sharing verification only after the VM is booted.
2661     if (VerifyAdapterSharing &amp;&amp; (entry != NULL)) {
2662       shared_entry = entry;
2663       entry = NULL;
2664     }
2665 #endif
2666 
2667     if (entry != NULL) {
2668       return entry;
2669     }
2670 
<span class="line-modified">2671     // Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage</span>
<span class="line-removed">2672     int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed, false);</span>
<span class="line-removed">2673 </span>
<span class="line-removed">2674     // Make a C heap allocated version of the fingerprint to store in the adapter</span>
2675     fingerprint = new AdapterFingerPrint(total_args_passed, sig_bt);
2676 
2677     // StubRoutines::code2() is initialized after this function can be called. As a result,
2678     // VerifyAdapterCalls and VerifyAdapterSharing can fail if we re-use code that generated
2679     // prior to StubRoutines::code2() being set. Checks refer to checks generated in an I2C
2680     // stub that ensure that an I2C stub is called from an interpreter frame.
2681     bool contains_all_checks = StubRoutines::code2() != NULL;
2682 
2683     // Create I2C &amp; C2I handlers
2684     BufferBlob* buf = buffer_blob(); // the temporary code buffer in CodeCache
2685     if (buf != NULL) {
2686       CodeBuffer buffer(buf);
2687       short buffer_locs[20];
2688       buffer.insts()-&gt;initialize_shared_locs((relocInfo*)buffer_locs,
2689                                              sizeof(buffer_locs)/sizeof(relocInfo));
2690 
2691       MacroAssembler _masm(&amp;buffer);
2692       entry = SharedRuntime::generate_i2c2i_adapters(&amp;_masm,
<span class="line-modified">2693                                                      total_args_passed,</span>
<span class="line-modified">2694                                                      comp_args_on_stack,</span>
<span class="line-removed">2695                                                      sig_bt,</span>
2696                                                      regs,
<span class="line-modified">2697                                                      fingerprint);</span>













2698 #ifdef ASSERT
2699       if (VerifyAdapterSharing) {
2700         if (shared_entry != NULL) {



2701           assert(shared_entry-&gt;compare_code(buf-&gt;code_begin(), buffer.insts_size()), &quot;code must match&quot;);
2702           // Release the one just created and return the original
2703           _adapters-&gt;free_entry(entry);
2704           return shared_entry;
2705         } else  {
2706           entry-&gt;save_code(buf-&gt;code_begin(), buffer.insts_size());
2707         }
2708       }
2709 #endif
2710 
<span class="line-removed">2711       new_adapter = AdapterBlob::create(&amp;buffer);</span>
2712       NOT_PRODUCT(insts_size = buffer.insts_size());
2713     }
2714     if (new_adapter == NULL) {
2715       // CodeCache is full, disable compilation
2716       // Ought to log this but compile log is only per compile thread
2717       // and we&#39;re some non descript Java thread.
2718       return NULL; // Out of CodeCache space
2719     }
2720     entry-&gt;relocate(new_adapter-&gt;content_begin());
2721 #ifndef PRODUCT
2722     // debugging suppport
2723     if (PrintAdapterHandlers || PrintStubCode) {
2724       ttyLocker ttyl;
2725       entry-&gt;print_adapter_on(tty);
2726       tty-&gt;print_cr(&quot;i2c argument handler #%d for: %s %s %s (%d bytes generated)&quot;,
2727                     _adapters-&gt;number_of_entries(), (method-&gt;is_static() ? &quot;static&quot; : &quot;receiver&quot;),
2728                     method-&gt;signature()-&gt;as_C_string(), fingerprint-&gt;as_string(), insts_size);
2729       tty-&gt;print_cr(&quot;c2i argument handler starts at %p&quot;, entry-&gt;get_c2i_entry());
2730       if (Verbose || PrintStubCode) {
2731         address first_pc = entry-&gt;base_address();
</pre>
<hr />
<pre>
2747     char blob_id[256];
2748     jio_snprintf(blob_id,
2749                  sizeof(blob_id),
2750                  &quot;%s(%s)@&quot; PTR_FORMAT,
2751                  new_adapter-&gt;name(),
2752                  fingerprint-&gt;as_string(),
2753                  new_adapter-&gt;content_begin());
2754     Forte::register_stub(blob_id, new_adapter-&gt;content_begin(), new_adapter-&gt;content_end());
2755 
2756     if (JvmtiExport::should_post_dynamic_code_generated()) {
2757       JvmtiExport::post_dynamic_code_generated(blob_id, new_adapter-&gt;content_begin(), new_adapter-&gt;content_end());
2758     }
2759   }
2760   return entry;
2761 }
2762 
2763 address AdapterHandlerEntry::base_address() {
2764   address base = _i2c_entry;
2765   if (base == NULL)  base = _c2i_entry;
2766   assert(base &lt;= _c2i_entry || _c2i_entry == NULL, &quot;&quot;);


2767   assert(base &lt;= _c2i_unverified_entry || _c2i_unverified_entry == NULL, &quot;&quot;);

2768   assert(base &lt;= _c2i_no_clinit_check_entry || _c2i_no_clinit_check_entry == NULL, &quot;&quot;);
2769   return base;
2770 }
2771 
2772 void AdapterHandlerEntry::relocate(address new_base) {
2773   address old_base = base_address();
2774   assert(old_base != NULL, &quot;&quot;);
2775   ptrdiff_t delta = new_base - old_base;
2776   if (_i2c_entry != NULL)
2777     _i2c_entry += delta;
2778   if (_c2i_entry != NULL)
2779     _c2i_entry += delta;




2780   if (_c2i_unverified_entry != NULL)
2781     _c2i_unverified_entry += delta;


2782   if (_c2i_no_clinit_check_entry != NULL)
2783     _c2i_no_clinit_check_entry += delta;
2784   assert(base_address() == new_base, &quot;&quot;);
2785 }
2786 
2787 
2788 void AdapterHandlerEntry::deallocate() {
2789   delete _fingerprint;



2790 #ifdef ASSERT
2791   FREE_C_HEAP_ARRAY(unsigned char, _saved_code);
2792 #endif
2793 }
2794 
2795 
2796 #ifdef ASSERT
2797 // Capture the code before relocation so that it can be compared
2798 // against other versions.  If the code is captured after relocation
2799 // then relative instructions won&#39;t be equivalent.
2800 void AdapterHandlerEntry::save_code(unsigned char* buffer, int length) {
2801   _saved_code = NEW_C_HEAP_ARRAY(unsigned char, length, mtCode);
2802   _saved_code_length = length;
2803   memcpy(_saved_code, buffer, length);
2804 }
2805 
2806 
2807 bool AdapterHandlerEntry::compare_code(unsigned char* buffer, int length) {
2808   if (length != _saved_code_length) {
2809     return false;
</pre>
<hr />
<pre>
2853       double locs_buf[20];
2854       buffer.insts()-&gt;initialize_shared_locs((relocInfo*)locs_buf, sizeof(locs_buf) / sizeof(relocInfo));
2855 #if defined(AARCH64)
2856       // On AArch64 with ZGC and nmethod entry barriers, we need all oops to be
2857       // in the constant pool to ensure ordering between the barrier and oops
2858       // accesses. For native_wrappers we need a constant.
2859       buffer.initialize_consts_size(8);
2860 #endif
2861       MacroAssembler _masm(&amp;buffer);
2862 
2863       // Fill in the signature array, for the calling-convention call.
2864       const int total_args_passed = method-&gt;size_of_parameters();
2865 
2866       BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_args_passed);
2867       VMRegPair*   regs = NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);
2868       int i=0;
2869       if (!method-&gt;is_static())  // Pass in receiver first
2870         sig_bt[i++] = T_OBJECT;
2871       SignatureStream ss(method-&gt;signature());
2872       for (; !ss.at_return_type(); ss.next()) {
<span class="line-modified">2873         sig_bt[i++] = ss.type();  // Collect remaining bits of signature</span>

2874         if (ss.type() == T_LONG || ss.type() == T_DOUBLE)
2875           sig_bt[i++] = T_VOID;   // Longs &amp; doubles take 2 Java slots
2876       }
2877       assert(i == total_args_passed, &quot;&quot;);
2878       BasicType ret_type = ss.type();
2879 
2880       // Now get the compiled-Java layout as input (or output) arguments.
2881       // NOTE: Stubs for compiled entry points of method handle intrinsics
2882       // are just trampolines so the argument registers must be outgoing ones.
2883       const bool is_outgoing = method-&gt;is_method_handle_intrinsic();
2884       int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed, is_outgoing);
2885 
2886       // Generate the compiled-to-native wrapper code
2887       nm = SharedRuntime::generate_native_wrapper(&amp;_masm, method, compile_id, sig_bt, regs, ret_type, critical_entry);
2888 
2889       if (nm != NULL) {
2890         {
2891           MutexLocker pl(CompiledMethod_lock, Mutex::_no_safepoint_check_flag);
2892           if (nm-&gt;make_in_use()) {
2893             method-&gt;set_code(method, nm);
</pre>
<hr />
<pre>
3106   AdapterHandlerTableIterator iter(_adapters);
3107   while (iter.has_next()) {
3108     AdapterHandlerEntry* a = iter.next();
3109     if (b == CodeCache::find_blob(a-&gt;get_i2c_entry())) {
3110       st-&gt;print(&quot;Adapter for signature: &quot;);
3111       a-&gt;print_adapter_on(tty);
3112       return;
3113     }
3114   }
3115   assert(false, &quot;Should have found handler&quot;);
3116 }
3117 
3118 void AdapterHandlerEntry::print_adapter_on(outputStream* st) const {
3119   st-&gt;print(&quot;AHE@&quot; INTPTR_FORMAT &quot;: %s&quot;, p2i(this), fingerprint()-&gt;as_string());
3120   if (get_i2c_entry() != NULL) {
3121     st-&gt;print(&quot; i2c: &quot; INTPTR_FORMAT, p2i(get_i2c_entry()));
3122   }
3123   if (get_c2i_entry() != NULL) {
3124     st-&gt;print(&quot; c2i: &quot; INTPTR_FORMAT, p2i(get_c2i_entry()));
3125   }






3126   if (get_c2i_unverified_entry() != NULL) {
<span class="line-modified">3127     st-&gt;print(&quot; c2iUV: &quot; INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));</span>



3128   }
3129   if (get_c2i_no_clinit_check_entry() != NULL) {
3130     st-&gt;print(&quot; c2iNCI: &quot; INTPTR_FORMAT, p2i(get_c2i_no_clinit_check_entry()));
3131   }
3132   st-&gt;cr();
3133 }
3134 
3135 #if INCLUDE_CDS
3136 
3137 void CDSAdapterHandlerEntry::init() {
3138   assert(DumpSharedSpaces, &quot;used during dump time only&quot;);
3139   _c2i_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());


3140   _adapter_trampoline = (AdapterHandlerEntry**)MetaspaceShared::misc_code_space_alloc(sizeof(AdapterHandlerEntry*));
3141 };
3142 
3143 #endif // INCLUDE_CDS
3144 
3145 
3146 #ifndef PRODUCT
3147 
3148 void AdapterHandlerLibrary::print_statistics() {
3149   _adapters-&gt;print_statistics();
3150 }
3151 
3152 #endif /* PRODUCT */
3153 
3154 JRT_LEAF(void, SharedRuntime::enable_stack_reserved_zone(JavaThread* thread))
3155   assert(thread-&gt;is_Java_thread(), &quot;Only Java threads have a stack reserved zone&quot;);
3156   if (thread-&gt;stack_reserved_zone_disabled()) {
3157   thread-&gt;enable_stack_reserved_zone();
3158   }
3159   thread-&gt;set_reserved_stack_activation(thread-&gt;stack_base());
</pre>
<hr />
<pre>
3205       break;
3206     } else {
3207       fr = fr.java_sender();
3208     }
3209   }
3210   return activation;
3211 }
3212 
3213 void SharedRuntime::on_slowpath_allocation_exit(JavaThread* thread) {
3214   // After any safepoint, just before going back to compiled code,
3215   // we inform the GC that we will be doing initializing writes to
3216   // this object in the future without emitting card-marks, so
3217   // GC may take any compensating steps.
3218 
3219   oop new_obj = thread-&gt;vm_result();
3220   if (new_obj == NULL) return;
3221 
3222   BarrierSet *bs = BarrierSet::barrier_set();
3223   bs-&gt;on_slowpath_allocation_exit(thread, new_obj);
3224 }














































































































































































































</pre>
</td>
<td>
<hr />
<pre>
  27 #include &quot;aot/aotLoader.hpp&quot;
  28 #include &quot;classfile/stringTable.hpp&quot;
  29 #include &quot;classfile/systemDictionary.hpp&quot;
  30 #include &quot;classfile/vmSymbols.hpp&quot;
  31 #include &quot;code/codeCache.hpp&quot;
  32 #include &quot;code/compiledIC.hpp&quot;
  33 #include &quot;code/icBuffer.hpp&quot;
  34 #include &quot;code/compiledMethod.inline.hpp&quot;
  35 #include &quot;code/scopeDesc.hpp&quot;
  36 #include &quot;code/vtableStubs.hpp&quot;
  37 #include &quot;compiler/abstractCompiler.hpp&quot;
  38 #include &quot;compiler/compileBroker.hpp&quot;
  39 #include &quot;compiler/disassembler.hpp&quot;
  40 #include &quot;gc/shared/barrierSet.hpp&quot;
  41 #include &quot;gc/shared/gcLocker.inline.hpp&quot;
  42 #include &quot;interpreter/interpreter.hpp&quot;
  43 #include &quot;interpreter/interpreterRuntime.hpp&quot;
  44 #include &quot;jfr/jfrEvents.hpp&quot;
  45 #include &quot;logging/log.hpp&quot;
  46 #include &quot;memory/metaspaceShared.hpp&quot;
<span class="line-added">  47 #include &quot;memory/oopFactory.hpp&quot;</span>
  48 #include &quot;memory/resourceArea.hpp&quot;
  49 #include &quot;memory/universe.hpp&quot;
<span class="line-added">  50 #include &quot;oops/access.hpp&quot;</span>
<span class="line-added">  51 #include &quot;oops/fieldStreams.inline.hpp&quot;</span>
  52 #include &quot;oops/klass.hpp&quot;
  53 #include &quot;oops/method.inline.hpp&quot;
  54 #include &quot;oops/objArrayKlass.hpp&quot;
<span class="line-added">  55 #include &quot;oops/objArrayOop.inline.hpp&quot;</span>
  56 #include &quot;oops/oop.inline.hpp&quot;
<span class="line-added">  57 #include &quot;oops/valueKlass.inline.hpp&quot;</span>
  58 #include &quot;prims/forte.hpp&quot;
  59 #include &quot;prims/jvmtiExport.hpp&quot;
  60 #include &quot;prims/methodHandles.hpp&quot;
  61 #include &quot;prims/nativeLookup.hpp&quot;
  62 #include &quot;runtime/arguments.hpp&quot;
  63 #include &quot;runtime/atomic.hpp&quot;
  64 #include &quot;runtime/biasedLocking.hpp&quot;
  65 #include &quot;runtime/frame.inline.hpp&quot;
  66 #include &quot;runtime/handles.inline.hpp&quot;
  67 #include &quot;runtime/init.hpp&quot;
  68 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  69 #include &quot;runtime/java.hpp&quot;
  70 #include &quot;runtime/javaCalls.hpp&quot;
  71 #include &quot;runtime/sharedRuntime.hpp&quot;
  72 #include &quot;runtime/stubRoutines.hpp&quot;
  73 #include &quot;runtime/synchronizer.hpp&quot;
  74 #include &quot;runtime/vframe.inline.hpp&quot;
  75 #include &quot;runtime/vframeArray.hpp&quot;
  76 #include &quot;utilities/copy.hpp&quot;
  77 #include &quot;utilities/dtrace.hpp&quot;
  78 #include &quot;utilities/events.hpp&quot;
  79 #include &quot;utilities/hashtable.inline.hpp&quot;
  80 #include &quot;utilities/macros.hpp&quot;
  81 #include &quot;utilities/xmlstream.hpp&quot;
  82 #ifdef COMPILER1
  83 #include &quot;c1/c1_Runtime1.hpp&quot;
  84 #endif
  85 
  86 // Shared stub locations
  87 RuntimeStub*        SharedRuntime::_wrong_method_blob;
  88 RuntimeStub*        SharedRuntime::_wrong_method_abstract_blob;
  89 RuntimeStub*        SharedRuntime::_ic_miss_blob;
  90 RuntimeStub*        SharedRuntime::_resolve_opt_virtual_call_blob;
  91 RuntimeStub*        SharedRuntime::_resolve_virtual_call_blob;
  92 RuntimeStub*        SharedRuntime::_resolve_static_call_blob;

  93 
  94 DeoptimizationBlob* SharedRuntime::_deopt_blob;
  95 SafepointBlob*      SharedRuntime::_polling_page_vectors_safepoint_handler_blob;
  96 SafepointBlob*      SharedRuntime::_polling_page_safepoint_handler_blob;
  97 SafepointBlob*      SharedRuntime::_polling_page_return_handler_blob;
  98 
  99 #ifdef COMPILER2
 100 UncommonTrapBlob*   SharedRuntime::_uncommon_trap_blob;
 101 #endif // COMPILER2
 102 
 103 
 104 //----------------------------generate_stubs-----------------------------------
 105 void SharedRuntime::generate_stubs() {
 106   _wrong_method_blob                   = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::handle_wrong_method),          &quot;wrong_method_stub&quot;);
 107   _wrong_method_abstract_blob          = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::handle_wrong_method_abstract), &quot;wrong_method_abstract_stub&quot;);
 108   _ic_miss_blob                        = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::handle_wrong_method_ic_miss),  &quot;ic_miss_stub&quot;);
 109   _resolve_opt_virtual_call_blob       = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::resolve_opt_virtual_call_C),   &quot;resolve_opt_virtual_call&quot;);
 110   _resolve_virtual_call_blob           = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::resolve_virtual_call_C),       &quot;resolve_virtual_call&quot;);
 111   _resolve_static_call_blob            = generate_resolve_blob(CAST_FROM_FN_PTR(address, SharedRuntime::resolve_static_call_C),        &quot;resolve_static_call&quot;);

 112 
 113 #if COMPILER2_OR_JVMCI
 114   // Vectors are generated only by C2 and JVMCI.
 115   bool support_wide = is_wide_vector(MaxVectorSize);
 116   if (support_wide) {
 117     _polling_page_vectors_safepoint_handler_blob = generate_handler_blob(CAST_FROM_FN_PTR(address, SafepointSynchronize::handle_polling_page_exception), POLL_AT_VECTOR_LOOP);
 118   }
 119 #endif // COMPILER2_OR_JVMCI
 120   _polling_page_safepoint_handler_blob = generate_handler_blob(CAST_FROM_FN_PTR(address, SafepointSynchronize::handle_polling_page_exception), POLL_AT_LOOP);
 121   _polling_page_return_handler_blob    = generate_handler_blob(CAST_FROM_FN_PTR(address, SafepointSynchronize::handle_polling_page_exception), POLL_AT_RETURN);
 122 
 123   generate_deopt_blob();
 124 
 125 #ifdef COMPILER2
 126   generate_uncommon_trap_blob();
 127 #endif // COMPILER2
 128 }
 129 
 130 #include &lt;math.h&gt;
 131 
</pre>
<hr />
<pre>
1038   }
1039   return NULL;
1040 }
1041 
1042 // Finds receiver, CallInfo (i.e. receiver method), and calling bytecode
1043 // for a call current in progress, i.e., arguments has been pushed on stack
1044 // but callee has not been invoked yet.  Caller frame must be compiled.
1045 Handle SharedRuntime::find_callee_info_helper(JavaThread* thread,
1046                                               vframeStream&amp; vfst,
1047                                               Bytecodes::Code&amp; bc,
1048                                               CallInfo&amp; callinfo, TRAPS) {
1049   Handle receiver;
1050   Handle nullHandle;  //create a handy null handle for exception returns
1051 
1052   assert(!vfst.at_end(), &quot;Java frame must exist&quot;);
1053 
1054   // Find caller and bci from vframe
1055   methodHandle caller(THREAD, vfst.method());
1056   int          bci   = vfst.bci();
1057 
<span class="line-added">1058   // Substitutability test implementation piggy backs on static call resolution</span>
<span class="line-added">1059   Bytecodes::Code code = caller-&gt;java_code_at(bci);</span>
<span class="line-added">1060   if (code == Bytecodes::_if_acmpeq || code == Bytecodes::_if_acmpne) {</span>
<span class="line-added">1061     bc = Bytecodes::_invokestatic;</span>
<span class="line-added">1062     methodHandle attached_method(THREAD, extract_attached_method(vfst));</span>
<span class="line-added">1063     assert(attached_method.not_null(), &quot;must have attached method&quot;);</span>
<span class="line-added">1064     SystemDictionary::ValueBootstrapMethods_klass()-&gt;initialize(CHECK_NH);</span>
<span class="line-added">1065     LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, false, CHECK_NH);</span>
<span class="line-added">1066 #ifdef ASSERT</span>
<span class="line-added">1067     Method* is_subst = SystemDictionary::ValueBootstrapMethods_klass()-&gt;find_method(vmSymbols::isSubstitutable_name(), vmSymbols::object_object_boolean_signature());</span>
<span class="line-added">1068     assert(callinfo.selected_method() == is_subst, &quot;must be isSubstitutable method&quot;);</span>
<span class="line-added">1069 #endif</span>
<span class="line-added">1070     return receiver;</span>
<span class="line-added">1071   }</span>
<span class="line-added">1072 </span>
1073   Bytecode_invoke bytecode(caller, bci);
1074   int bytecode_index = bytecode.index();
1075   bc = bytecode.invoke_code();
1076 
1077   methodHandle attached_method(THREAD, extract_attached_method(vfst));
1078   if (attached_method.not_null()) {
1079     Method* callee = bytecode.static_target(CHECK_NH);
1080     vmIntrinsics::ID id = callee-&gt;intrinsic_id();
1081     // When VM replaces MH.invokeBasic/linkTo* call with a direct/virtual call,
1082     // it attaches statically resolved method to the call site.
1083     if (MethodHandles::is_signature_polymorphic(id) &amp;&amp;
1084         MethodHandles::is_signature_polymorphic_intrinsic(id)) {
1085       bc = MethodHandles::signature_polymorphic_intrinsic_bytecode(id);
1086 
1087       // Adjust invocation mode according to the attached method.
1088       switch (bc) {
1089         case Bytecodes::_invokevirtual:
1090           if (attached_method-&gt;method_holder()-&gt;is_interface()) {
1091             bc = Bytecodes::_invokeinterface;
1092           }
1093           break;
1094         case Bytecodes::_invokeinterface:
1095           if (!attached_method-&gt;method_holder()-&gt;is_interface()) {
1096             bc = Bytecodes::_invokevirtual;
1097           }
1098           break;
1099         case Bytecodes::_invokehandle:
1100           if (!MethodHandles::is_signature_polymorphic_method(attached_method())) {
1101             bc = attached_method-&gt;is_static() ? Bytecodes::_invokestatic
1102                                               : Bytecodes::_invokevirtual;
1103           }
1104           break;
1105         default:
1106           break;
1107       }
<span class="line-added">1108     } else {</span>
<span class="line-added">1109       assert(attached_method-&gt;has_scalarized_args(), &quot;invalid use of attached method&quot;);</span>
<span class="line-added">1110       if (!attached_method-&gt;method_holder()-&gt;is_value()) {</span>
<span class="line-added">1111         // Ignore the attached method in this case to not confuse below code</span>
<span class="line-added">1112         attached_method = methodHandle(thread, NULL);</span>
<span class="line-added">1113       }</span>
1114     }
1115   }
1116 
1117   assert(bc != Bytecodes::_illegal, &quot;not initialized&quot;);
1118 
1119   bool has_receiver = bc != Bytecodes::_invokestatic &amp;&amp;
1120                       bc != Bytecodes::_invokedynamic &amp;&amp;
1121                       bc != Bytecodes::_invokehandle;
<span class="line-added">1122   bool check_null_and_abstract = true;</span>
1123 
1124   // Find receiver for non-static call
1125   if (has_receiver) {
1126     // This register map must be update since we need to find the receiver for
1127     // compiled frames. The receiver might be in a register.
1128     RegisterMap reg_map2(thread);
1129     frame stubFrame   = thread-&gt;last_frame();
1130     // Caller-frame is a compiled frame
1131     frame callerFrame = stubFrame.sender(&amp;reg_map2);
<span class="line-added">1132     bool caller_is_c1 = false;</span>
1133 
<span class="line-modified">1134     if (callerFrame.is_compiled_frame() &amp;&amp; !callerFrame.is_deoptimized_frame()) {</span>
<span class="line-modified">1135       caller_is_c1 = callerFrame.cb()-&gt;is_compiled_by_c1();</span>
<span class="line-added">1136     }</span>
<span class="line-added">1137 </span>
<span class="line-added">1138     Method* callee = attached_method();</span>
<span class="line-added">1139     if (callee == NULL) {</span>
<span class="line-added">1140       callee = bytecode.static_target(CHECK_NH);</span>
1141       if (callee == NULL) {
1142         THROW_(vmSymbols::java_lang_NoSuchMethodException(), nullHandle);
1143       }
1144     }
<span class="line-modified">1145     if (!caller_is_c1 &amp;&amp; callee-&gt;has_scalarized_args() &amp;&amp; callee-&gt;method_holder()-&gt;is_value()) {</span>
<span class="line-modified">1146       // If the receiver is a value type that is passed as fields, no oop is available.</span>
<span class="line-modified">1147       // Resolve the call without receiver null checking.</span>
<span class="line-modified">1148       assert(attached_method.not_null() &amp;&amp; !attached_method-&gt;is_abstract(), &quot;must have non-abstract attached method&quot;);</span>
<span class="line-modified">1149       if (bc == Bytecodes::_invokeinterface) {</span>
<span class="line-modified">1150         bc = Bytecodes::_invokevirtual; // C2 optimistically replaces interface calls by virtual calls</span>
<span class="line-added">1151       }</span>
<span class="line-added">1152       check_null_and_abstract = false;</span>
<span class="line-added">1153     } else {</span>
<span class="line-added">1154       // Retrieve from a compiled argument list</span>
<span class="line-added">1155       receiver = Handle(THREAD, callerFrame.retrieve_receiver(&amp;reg_map2));</span>
<span class="line-added">1156       if (receiver.is_null()) {</span>
<span class="line-added">1157         THROW_(vmSymbols::java_lang_NullPointerException(), nullHandle);</span>
<span class="line-added">1158       }</span>
1159     }
1160   }
1161 
1162   // Resolve method
1163   if (attached_method.not_null()) {
1164     // Parameterized by attached method.
<span class="line-modified">1165     LinkResolver::resolve_invoke(callinfo, receiver, attached_method, bc, check_null_and_abstract, CHECK_NH);</span>
1166   } else {
1167     // Parameterized by bytecode.
1168     constantPoolHandle constants(THREAD, caller-&gt;constants());
1169     LinkResolver::resolve_invoke(callinfo, receiver, constants, bytecode_index, bc, CHECK_NH);
1170   }
1171 
1172 #ifdef ASSERT
1173   // Check that the receiver klass is of the right subtype and that it is initialized for virtual calls
<span class="line-modified">1174   if (has_receiver &amp;&amp; check_null_and_abstract) {</span>
1175     assert(receiver.not_null(), &quot;should have thrown exception&quot;);
1176     Klass* receiver_klass = receiver-&gt;klass();
1177     Klass* rk = NULL;
1178     if (attached_method.not_null()) {
1179       // In case there&#39;s resolved method attached, use its holder during the check.
1180       rk = attached_method-&gt;method_holder();
1181     } else {
1182       // Klass is already loaded.
1183       constantPoolHandle constants(THREAD, caller-&gt;constants());
1184       rk = constants-&gt;klass_ref_at(bytecode_index, CHECK_NH);
1185     }
1186     Klass* static_receiver_klass = rk;
1187     assert(receiver_klass-&gt;is_subtype_of(static_receiver_klass),
1188            &quot;actual receiver must be subclass of static receiver klass&quot;);
1189     if (receiver_klass-&gt;is_instance_klass()) {
1190       if (InstanceKlass::cast(receiver_klass)-&gt;is_not_initialized()) {
1191         tty-&gt;print_cr(&quot;ERROR: Klass not yet initialized!!&quot;);
1192         receiver_klass-&gt;print();
1193       }
1194       assert(!InstanceKlass::cast(receiver_klass)-&gt;is_not_initialized(), &quot;receiver_klass must be initialized&quot;);
</pre>
<hr />
<pre>
1213     RegisterMap reg_map(thread, false);
1214     frame fr = thread-&gt;last_frame();
1215     assert(fr.is_runtime_frame(), &quot;must be a runtimeStub&quot;);
1216     fr = fr.sender(&amp;reg_map);
1217     assert(fr.is_entry_frame(), &quot;must be&quot;);
1218     // fr is now pointing to the entry frame.
1219     callee_method = methodHandle(THREAD, fr.entry_frame_call_wrapper()-&gt;callee_method());
1220   } else {
1221     Bytecodes::Code bc;
1222     CallInfo callinfo;
1223     find_callee_info_helper(thread, vfst, bc, callinfo, CHECK_(methodHandle()));
1224     callee_method = methodHandle(THREAD, callinfo.selected_method());
1225   }
1226   assert(callee_method()-&gt;is_method(), &quot;must be&quot;);
1227   return callee_method;
1228 }
1229 
1230 // Resolves a call.
1231 methodHandle SharedRuntime::resolve_helper(JavaThread *thread,
1232                                            bool is_virtual,
<span class="line-modified">1233                                            bool is_optimized,</span>
<span class="line-added">1234                                            bool* caller_is_c1, TRAPS) {</span>
1235   methodHandle callee_method;
<span class="line-modified">1236   callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, caller_is_c1, THREAD);</span>
1237   if (JvmtiExport::can_hotswap_or_post_breakpoint()) {
1238     int retry_count = 0;
1239     while (!HAS_PENDING_EXCEPTION &amp;&amp; callee_method-&gt;is_old() &amp;&amp;
1240            callee_method-&gt;method_holder() != SystemDictionary::Object_klass()) {
1241       // If has a pending exception then there is no need to re-try to
1242       // resolve this method.
1243       // If the method has been redefined, we need to try again.
1244       // Hack: we have no way to update the vtables of arrays, so don&#39;t
1245       // require that java.lang.Object has been updated.
1246 
1247       // It is very unlikely that method is redefined more than 100 times
1248       // in the middle of resolve. If it is looping here more than 100 times
1249       // means then there could be a bug here.
1250       guarantee((retry_count++ &lt; 100),
1251                 &quot;Could not resolve to latest version of redefined method&quot;);
1252       // method is redefined in the middle of resolve so re-try.
<span class="line-modified">1253       callee_method = resolve_sub_helper(thread, is_virtual, is_optimized, caller_is_c1, THREAD);</span>
1254     }
1255   }
1256   return callee_method;
1257 }
1258 
1259 // This fails if resolution required refilling of IC stubs
1260 bool SharedRuntime::resolve_sub_helper_internal(methodHandle callee_method, const frame&amp; caller_frame,
1261                                                 CompiledMethod* caller_nm, bool is_virtual, bool is_optimized,
1262                                                 Handle receiver, CallInfo&amp; call_info, Bytecodes::Code invoke_code, TRAPS) {
1263   StaticCallInfo static_call_info;
1264   CompiledICInfo virtual_call_info;
1265 
1266   // Make sure the callee nmethod does not get deoptimized and removed before
1267   // we are done patching the code.
1268   CompiledMethod* callee = callee_method-&gt;code();
1269 
1270   if (callee != NULL) {
1271     assert(callee-&gt;is_compiled(), &quot;must be nmethod for patching&quot;);
1272   }
1273 
1274   if (callee != NULL &amp;&amp; !callee-&gt;is_in_use()) {
1275     // Patch call site to C2I adapter if callee nmethod is deoptimized or unloaded.
1276     callee = NULL;
1277   }
1278   nmethodLocker nl_callee(callee);
1279 #ifdef ASSERT
1280   address dest_entry_point = callee == NULL ? 0 : callee-&gt;entry_point(); // used below
1281 #endif
1282 
1283   bool is_nmethod = caller_nm-&gt;is_nmethod();
<span class="line-added">1284   bool caller_is_c1 = caller_nm-&gt;is_compiled_by_c1();</span>
1285 
1286   if (is_virtual) {
<span class="line-modified">1287     Klass* receiver_klass = NULL;</span>
<span class="line-added">1288     if (InlineTypePassFieldsAsArgs &amp;&amp; !caller_is_c1 &amp;&amp; callee_method-&gt;method_holder()-&gt;is_value()) {</span>
<span class="line-added">1289       // If the receiver is an inline type that is passed as fields, no oop is available</span>
<span class="line-added">1290       receiver_klass = callee_method-&gt;method_holder();</span>
<span class="line-added">1291     } else {</span>
<span class="line-added">1292       assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, &quot;sanity check&quot;);</span>
<span class="line-added">1293       receiver_klass = invoke_code == Bytecodes::_invokehandle ? NULL : receiver-&gt;klass();</span>
<span class="line-added">1294     }</span>
1295     bool static_bound = call_info.resolved_method()-&gt;can_be_statically_bound();
<span class="line-modified">1296     CompiledIC::compute_monomorphic_entry(callee_method, receiver_klass,</span>
<span class="line-modified">1297                      is_optimized, static_bound, is_nmethod, caller_is_c1, virtual_call_info,</span>

1298                      CHECK_false);
1299   } else {
1300     // static call
<span class="line-modified">1301     CompiledStaticCall::compute_entry(callee_method, caller_nm, static_call_info);</span>
1302   }
1303 
1304   // grab lock, check for deoptimization and potentially patch caller
1305   {
1306     CompiledICLocker ml(caller_nm);
1307 
1308     // Lock blocks for safepoint during which both nmethods can change state.
1309 
1310     // Now that we are ready to patch if the Method* was redefined then
1311     // don&#39;t update call site and let the caller retry.
1312     // Don&#39;t update call site if callee nmethod was unloaded or deoptimized.
1313     // Don&#39;t update call site if callee nmethod was replaced by an other nmethod
1314     // which may happen when multiply alive nmethod (tiered compilation)
1315     // will be supported.
1316     if (!callee_method-&gt;is_old() &amp;&amp;
1317         (callee == NULL || (callee-&gt;is_in_use() &amp;&amp; callee_method-&gt;code() == callee))) {
1318       NoSafepointVerifier nsv;
1319 #ifdef ASSERT
1320       // We must not try to patch to jump to an already unloaded method.
1321       if (dest_entry_point != 0) {
</pre>
<hr />
<pre>
1333         }
1334       } else {
1335         if (VM_Version::supports_fast_class_init_checks() &amp;&amp;
1336             invoke_code == Bytecodes::_invokestatic &amp;&amp;
1337             callee_method-&gt;needs_clinit_barrier() &amp;&amp;
1338             callee != NULL &amp;&amp; (callee-&gt;is_compiled_by_jvmci() || callee-&gt;is_aot())) {
1339           return true; // skip patching for JVMCI or AOT code
1340         }
1341         CompiledStaticCall* ssc = caller_nm-&gt;compiledStaticCall_before(caller_frame.pc());
1342         if (ssc-&gt;is_clean()) ssc-&gt;set(static_call_info);
1343       }
1344     }
1345   } // unlock CompiledICLocker
1346   return true;
1347 }
1348 
1349 // Resolves a call.  The compilers generate code for calls that go here
1350 // and are patched with the real destination of the call.
1351 methodHandle SharedRuntime::resolve_sub_helper(JavaThread *thread,
1352                                                bool is_virtual,
<span class="line-modified">1353                                                bool is_optimized,</span>
<span class="line-added">1354                                                bool* caller_is_c1, TRAPS) {</span>
1355 
1356   ResourceMark rm(thread);
1357   RegisterMap cbl_map(thread, false);
1358   frame caller_frame = thread-&gt;last_frame().sender(&amp;cbl_map);
1359 
1360   CodeBlob* caller_cb = caller_frame.cb();
1361   guarantee(caller_cb != NULL &amp;&amp; caller_cb-&gt;is_compiled(), &quot;must be called from compiled method&quot;);
1362   CompiledMethod* caller_nm = caller_cb-&gt;as_compiled_method_or_null();
<span class="line-added">1363   *caller_is_c1 = caller_nm-&gt;is_compiled_by_c1();</span>
1364 
1365   // make sure caller is not getting deoptimized
1366   // and removed before we are done with it.
1367   // CLEANUP - with lazy deopt shouldn&#39;t need this lock
1368   nmethodLocker caller_lock(caller_nm);
1369 
1370   // determine call info &amp; receiver
1371   // note: a) receiver is NULL for static calls
1372   //       b) an exception is thrown if receiver is NULL for non-static calls
1373   CallInfo call_info;
1374   Bytecodes::Code invoke_code = Bytecodes::_illegal;
1375   Handle receiver = find_callee_info(thread, invoke_code,
1376                                      call_info, CHECK_(methodHandle()));
1377   methodHandle callee_method(THREAD, call_info.selected_method());
1378 
1379   assert((!is_virtual &amp;&amp; invoke_code == Bytecodes::_invokestatic ) ||
1380          (!is_virtual &amp;&amp; invoke_code == Bytecodes::_invokespecial) ||
1381          (!is_virtual &amp;&amp; invoke_code == Bytecodes::_invokehandle ) ||
1382          (!is_virtual &amp;&amp; invoke_code == Bytecodes::_invokedynamic) ||
1383          ( is_virtual &amp;&amp; invoke_code != Bytecodes::_invokestatic ), &quot;inconsistent bytecode&quot;);
</pre>
<hr />
<pre>
1445       return callee_method;
1446     } else {
1447       InlineCacheBuffer::refill_ic_stubs();
1448     }
1449   }
1450 
1451 }
1452 
1453 
1454 // Inline caches exist only in compiled code
1455 JRT_BLOCK_ENTRY(address, SharedRuntime::handle_wrong_method_ic_miss(JavaThread* thread))
1456 #ifdef ASSERT
1457   RegisterMap reg_map(thread, false);
1458   frame stub_frame = thread-&gt;last_frame();
1459   assert(stub_frame.is_runtime_frame(), &quot;sanity check&quot;);
1460   frame caller_frame = stub_frame.sender(&amp;reg_map);
1461   assert(!caller_frame.is_interpreted_frame() &amp;&amp; !caller_frame.is_entry_frame(), &quot;unexpected frame&quot;);
1462 #endif /* ASSERT */
1463 
1464   methodHandle callee_method;
<span class="line-added">1465   bool is_optimized = false;</span>
<span class="line-added">1466   bool caller_is_c1 = false;</span>
1467   JRT_BLOCK
<span class="line-modified">1468     callee_method = SharedRuntime::handle_ic_miss_helper(thread, is_optimized, caller_is_c1, CHECK_NULL);</span>
1469     // Return Method* through TLS
1470     thread-&gt;set_vm_result_2(callee_method());
1471   JRT_BLOCK_END
1472   // return compiled code entry point after potential safepoints
<span class="line-modified">1473   return entry_for_handle_wrong_method(callee_method, false, is_optimized, caller_is_c1);</span>

1474 JRT_END
1475 
1476 
1477 // Handle call site that has been made non-entrant
1478 JRT_BLOCK_ENTRY(address, SharedRuntime::handle_wrong_method(JavaThread* thread))
1479   // 6243940 We might end up in here if the callee is deoptimized
1480   // as we race to call it.  We don&#39;t want to take a safepoint if
1481   // the caller was interpreted because the caller frame will look
1482   // interpreted to the stack walkers and arguments are now
1483   // &quot;compiled&quot; so it is much better to make this transition
1484   // invisible to the stack walking code. The i2c path will
1485   // place the callee method in the callee_target. It is stashed
1486   // there because if we try and find the callee by normal means a
1487   // safepoint is possible and have trouble gc&#39;ing the compiled args.
1488   RegisterMap reg_map(thread, false);
1489   frame stub_frame = thread-&gt;last_frame();
1490   assert(stub_frame.is_runtime_frame(), &quot;sanity check&quot;);
1491   frame caller_frame = stub_frame.sender(&amp;reg_map);
1492 
1493   if (caller_frame.is_interpreted_frame() ||
</pre>
<hr />
<pre>
1496     guarantee(callee != NULL &amp;&amp; callee-&gt;is_method(), &quot;bad handshake&quot;);
1497     thread-&gt;set_vm_result_2(callee);
1498     thread-&gt;set_callee_target(NULL);
1499     if (caller_frame.is_entry_frame() &amp;&amp; VM_Version::supports_fast_class_init_checks()) {
1500       // Bypass class initialization checks in c2i when caller is in native.
1501       // JNI calls to static methods don&#39;t have class initialization checks.
1502       // Fast class initialization checks are present in c2i adapters and call into
1503       // SharedRuntime::handle_wrong_method() on the slow path.
1504       //
1505       // JVM upcalls may land here as well, but there&#39;s a proper check present in
1506       // LinkResolver::resolve_static_call (called from JavaCalls::call_static),
1507       // so bypassing it in c2i adapter is benign.
1508       return callee-&gt;get_c2i_no_clinit_check_entry();
1509     } else {
1510       return callee-&gt;get_c2i_entry();
1511     }
1512   }
1513 
1514   // Must be compiled to compiled path which is safe to stackwalk
1515   methodHandle callee_method;
<span class="line-added">1516   bool is_static_call = false;</span>
<span class="line-added">1517   bool is_optimized = false;</span>
<span class="line-added">1518   bool caller_is_c1 = false;</span>
1519   JRT_BLOCK
1520     // Force resolving of caller (if we called from compiled frame)
<span class="line-modified">1521     callee_method = SharedRuntime::reresolve_call_site(thread, is_static_call, is_optimized, caller_is_c1, CHECK_NULL);</span>
1522     thread-&gt;set_vm_result_2(callee_method());
1523   JRT_BLOCK_END
1524   // return compiled code entry point after potential safepoints
<span class="line-modified">1525   return entry_for_handle_wrong_method(callee_method, is_static_call, is_optimized, caller_is_c1);</span>

1526 JRT_END
1527 
1528 // Handle abstract method call
1529 JRT_BLOCK_ENTRY(address, SharedRuntime::handle_wrong_method_abstract(JavaThread* thread))
1530   // Verbose error message for AbstractMethodError.
1531   // Get the called method from the invoke bytecode.
1532   vframeStream vfst(thread, true);
1533   assert(!vfst.at_end(), &quot;Java frame must exist&quot;);
1534   methodHandle caller(thread, vfst.method());
1535   Bytecode_invoke invoke(caller, vfst.bci());
1536   DEBUG_ONLY( invoke.verify(); )
1537 
1538   // Find the compiled caller frame.
1539   RegisterMap reg_map(thread);
1540   frame stubFrame = thread-&gt;last_frame();
1541   assert(stubFrame.is_runtime_frame(), &quot;must be&quot;);
1542   frame callerFrame = stubFrame.sender(&amp;reg_map);
1543   assert(callerFrame.is_compiled_frame(), &quot;must be&quot;);
1544 
1545   // Install exception and return forward entry.
1546   address res = StubRoutines::throw_AbstractMethodError_entry();
1547   JRT_BLOCK
1548     methodHandle callee(thread, invoke.static_target(thread));
1549     if (!callee.is_null()) {
1550       oop recv = callerFrame.retrieve_receiver(&amp;reg_map);
1551       Klass *recv_klass = (recv != NULL) ? recv-&gt;klass() : NULL;
1552       LinkResolver::throw_abstract_method_error(callee, recv_klass, thread);
1553       res = StubRoutines::forward_exception_entry();
1554     }
1555   JRT_BLOCK_END
1556   return res;
1557 JRT_END
1558 
1559 
1560 // resolve a static call and patch code
1561 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_static_call_C(JavaThread *thread ))
1562   methodHandle callee_method;
<span class="line-added">1563   bool caller_is_c1;</span>
1564   JRT_BLOCK
<span class="line-modified">1565     callee_method = SharedRuntime::resolve_helper(thread, false, false, &amp;caller_is_c1, CHECK_NULL);</span>
1566     thread-&gt;set_vm_result_2(callee_method());
1567   JRT_BLOCK_END
1568   // return compiled code entry point after potential safepoints
<span class="line-modified">1569   address entry = caller_is_c1 ?</span>
<span class="line-modified">1570     callee_method-&gt;verified_value_code_entry() : callee_method-&gt;verified_code_entry();</span>
<span class="line-added">1571   assert(entry != NULL, &quot;Jump to zero!&quot;);</span>
<span class="line-added">1572   return entry;</span>
1573 JRT_END
1574 
1575 
1576 // resolve virtual call and update inline cache to monomorphic
1577 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_virtual_call_C(JavaThread *thread ))
1578   methodHandle callee_method;
<span class="line-added">1579   bool caller_is_c1;</span>
1580   JRT_BLOCK
<span class="line-modified">1581     callee_method = SharedRuntime::resolve_helper(thread, true, false, &amp;caller_is_c1, CHECK_NULL);</span>
1582     thread-&gt;set_vm_result_2(callee_method());
1583   JRT_BLOCK_END
1584   // return compiled code entry point after potential safepoints
<span class="line-modified">1585   address entry = caller_is_c1 ?</span>
<span class="line-modified">1586     callee_method-&gt;verified_value_code_entry() : callee_method-&gt;verified_value_ro_code_entry();</span>
<span class="line-added">1587   assert(entry != NULL, &quot;Jump to zero!&quot;);</span>
<span class="line-added">1588   return entry;</span>
1589 JRT_END
1590 
1591 
1592 // Resolve a virtual call that can be statically bound (e.g., always
1593 // monomorphic, so it has no inline cache).  Patch code to resolved target.
1594 JRT_BLOCK_ENTRY(address, SharedRuntime::resolve_opt_virtual_call_C(JavaThread *thread))
1595   methodHandle callee_method;
<span class="line-added">1596   bool caller_is_c1;</span>
1597   JRT_BLOCK
<span class="line-modified">1598     callee_method = SharedRuntime::resolve_helper(thread, true, true, &amp;caller_is_c1, CHECK_NULL);</span>
1599     thread-&gt;set_vm_result_2(callee_method());
1600   JRT_BLOCK_END
1601   // return compiled code entry point after potential safepoints
<span class="line-modified">1602   address entry = caller_is_c1 ?</span>
<span class="line-modified">1603     callee_method-&gt;verified_value_code_entry() : callee_method-&gt;verified_code_entry();</span>
<span class="line-added">1604   assert(entry != NULL, &quot;Jump to zero!&quot;);</span>
<span class="line-added">1605   return entry;</span>
1606 JRT_END
1607 
1608 // The handle_ic_miss_helper_internal function returns false if it failed due
1609 // to either running out of vtable stubs or ic stubs due to IC transitions
1610 // to transitional states. The needs_ic_stub_refill value will be set if
1611 // the failure was due to running out of IC stubs, in which case handle_ic_miss_helper
1612 // refills the IC stubs and tries again.
1613 bool SharedRuntime::handle_ic_miss_helper_internal(Handle receiver, CompiledMethod* caller_nm,
1614                                                    const frame&amp; caller_frame, methodHandle callee_method,
1615                                                    Bytecodes::Code bc, CallInfo&amp; call_info,
<span class="line-modified">1616                                                    bool&amp; needs_ic_stub_refill, bool&amp; is_optimized, bool caller_is_c1, TRAPS) {</span>
1617   CompiledICLocker ml(caller_nm);
1618   CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());
1619   bool should_be_mono = false;
1620   if (inline_cache-&gt;is_optimized()) {
1621     if (TraceCallFixup) {
1622       ResourceMark rm(THREAD);
1623       tty-&gt;print(&quot;OPTIMIZED IC miss (%s) call to&quot;, Bytecodes::name(bc));
1624       callee_method-&gt;print_short_name(tty);
1625       tty-&gt;print_cr(&quot; code: &quot; INTPTR_FORMAT, p2i(callee_method-&gt;code()));
1626     }
<span class="line-added">1627     is_optimized = true;</span>
1628     should_be_mono = true;
1629   } else if (inline_cache-&gt;is_icholder_call()) {
1630     CompiledICHolder* ic_oop = inline_cache-&gt;cached_icholder();
1631     if (ic_oop != NULL) {
1632       if (!ic_oop-&gt;is_loader_alive()) {
1633         // Deferred IC cleaning due to concurrent class unloading
1634         if (!inline_cache-&gt;set_to_clean()) {
1635           needs_ic_stub_refill = true;
1636           return false;
1637         }
1638       } else if (receiver()-&gt;klass() == ic_oop-&gt;holder_klass()) {
1639         // This isn&#39;t a real miss. We must have seen that compiled code
1640         // is now available and we want the call site converted to a
1641         // monomorphic compiled call site.
1642         // We can&#39;t assert for callee_method-&gt;code() != NULL because it
1643         // could have been deoptimized in the meantime
1644         if (TraceCallFixup) {
1645           ResourceMark rm(THREAD);
1646           tty-&gt;print(&quot;FALSE IC miss (%s) converting to compiled call to&quot;, Bytecodes::name(bc));
1647           callee_method-&gt;print_short_name(tty);
1648           tty-&gt;print_cr(&quot; code: &quot; INTPTR_FORMAT, p2i(callee_method-&gt;code()));
1649         }
1650         should_be_mono = true;
1651       }
1652     }
1653   }
1654 
1655   if (should_be_mono) {
1656     // We have a path that was monomorphic but was going interpreted
1657     // and now we have (or had) a compiled entry. We correct the IC
1658     // by using a new icBuffer.
1659     CompiledICInfo info;
1660     Klass* receiver_klass = receiver()-&gt;klass();
1661     inline_cache-&gt;compute_monomorphic_entry(callee_method,
1662                                             receiver_klass,
1663                                             inline_cache-&gt;is_optimized(),
1664                                             false, caller_nm-&gt;is_nmethod(),
<span class="line-added">1665                                             caller_nm-&gt;is_compiled_by_c1(),</span>
1666                                             info, CHECK_false);
1667     if (!inline_cache-&gt;set_to_monomorphic(info)) {
1668       needs_ic_stub_refill = true;
1669       return false;
1670     }
1671   } else if (!inline_cache-&gt;is_megamorphic() &amp;&amp; !inline_cache-&gt;is_clean()) {
1672     // Potential change to megamorphic
1673 
<span class="line-modified">1674     bool successful = inline_cache-&gt;set_to_megamorphic(&amp;call_info, bc, needs_ic_stub_refill, caller_is_c1, CHECK_false);</span>
1675     if (needs_ic_stub_refill) {
1676       return false;
1677     }
1678     if (!successful) {
1679       if (!inline_cache-&gt;set_to_clean()) {
1680         needs_ic_stub_refill = true;
1681         return false;
1682       }
1683     }
1684   } else {
1685     // Either clean or megamorphic
1686   }
1687   return true;
1688 }
1689 
<span class="line-modified">1690 methodHandle SharedRuntime::handle_ic_miss_helper(JavaThread *thread, bool&amp; is_optimized, bool&amp; caller_is_c1, TRAPS) {</span>
1691   ResourceMark rm(thread);
1692   CallInfo call_info;
1693   Bytecodes::Code bc;
1694 
1695   // receiver is NULL for static calls. An exception is thrown for NULL
1696   // receivers for non-static calls
1697   Handle receiver = find_callee_info(thread, bc, call_info,
1698                                      CHECK_(methodHandle()));
1699   // Compiler1 can produce virtual call sites that can actually be statically bound
1700   // If we fell thru to below we would think that the site was going megamorphic
1701   // when in fact the site can never miss. Worse because we&#39;d think it was megamorphic
1702   // we&#39;d try and do a vtable dispatch however methods that can be statically bound
1703   // don&#39;t have vtable entries (vtable_index &lt; 0) and we&#39;d blow up. So we force a
1704   // reresolution of the  call site (as if we did a handle_wrong_method and not an
1705   // plain ic_miss) and the site will be converted to an optimized virtual call site
1706   // never to miss again. I don&#39;t believe C2 will produce code like this but if it
1707   // did this would still be the correct thing to do for it too, hence no ifdef.
1708   //
1709   if (call_info.resolved_method()-&gt;can_be_statically_bound()) {
<span class="line-modified">1710     bool is_static_call = false;</span>
<span class="line-added">1711     methodHandle callee_method = SharedRuntime::reresolve_call_site(thread, is_static_call, is_optimized, caller_is_c1, CHECK_(methodHandle()));</span>
<span class="line-added">1712     assert(!is_static_call, &quot;IC miss at static call?&quot;);</span>
1713     if (TraceCallFixup) {
1714       RegisterMap reg_map(thread, false);
1715       frame caller_frame = thread-&gt;last_frame().sender(&amp;reg_map);
1716       ResourceMark rm(thread);
1717       tty-&gt;print(&quot;converting IC miss to reresolve (%s) call to&quot;, Bytecodes::name(bc));
1718       callee_method-&gt;print_short_name(tty);
1719       tty-&gt;print_cr(&quot; from pc: &quot; INTPTR_FORMAT, p2i(caller_frame.pc()));
1720       tty-&gt;print_cr(&quot; code: &quot; INTPTR_FORMAT, p2i(callee_method-&gt;code()));
1721     }
1722     return callee_method;
1723   }
1724 
1725   methodHandle callee_method(thread, call_info.selected_method());
1726 
1727 #ifndef PRODUCT
1728   Atomic::inc(&amp;_ic_miss_ctr);
1729 
1730   // Statistics &amp; Tracing
1731   if (TraceCallFixup) {
1732     ResourceMark rm(thread);
</pre>
<hr />
<pre>
1742     // produce statistics under the lock
1743     trace_ic_miss(f.pc());
1744   }
1745 #endif
1746 
1747   // install an event collector so that when a vtable stub is created the
1748   // profiler can be notified via a DYNAMIC_CODE_GENERATED event. The
1749   // event can&#39;t be posted when the stub is created as locks are held
1750   // - instead the event will be deferred until the event collector goes
1751   // out of scope.
1752   JvmtiDynamicCodeEventCollector event_collector;
1753 
1754   // Update inline cache to megamorphic. Skip update if we are called from interpreted.
1755   // Transitioning IC caches may require transition stubs. If we run out
1756   // of transition stubs, we have to drop locks and perform a safepoint
1757   // that refills them.
1758   RegisterMap reg_map(thread, false);
1759   frame caller_frame = thread-&gt;last_frame().sender(&amp;reg_map);
1760   CodeBlob* cb = caller_frame.cb();
1761   CompiledMethod* caller_nm = cb-&gt;as_compiled_method();
<span class="line-added">1762   caller_is_c1 = caller_nm-&gt;is_compiled_by_c1();</span>
1763 
1764   for (;;) {
1765     ICRefillVerifier ic_refill_verifier;
1766     bool needs_ic_stub_refill = false;
1767     bool successful = handle_ic_miss_helper_internal(receiver, caller_nm, caller_frame, callee_method,
<span class="line-modified">1768                                                      bc, call_info, needs_ic_stub_refill, is_optimized, caller_is_c1, CHECK_(methodHandle()));</span>
1769     if (successful || !needs_ic_stub_refill) {
1770       return callee_method;
1771     } else {
1772       InlineCacheBuffer::refill_ic_stubs();
1773     }
1774   }
1775 }
1776 
1777 static bool clear_ic_at_addr(CompiledMethod* caller_nm, address call_addr, bool is_static_call) {
1778   CompiledICLocker ml(caller_nm);
1779   if (is_static_call) {
1780     CompiledStaticCall* ssc = caller_nm-&gt;compiledStaticCall_at(call_addr);
1781     if (!ssc-&gt;is_clean()) {
1782       return ssc-&gt;set_to_clean();
1783     }
1784   } else {
1785     // compiled, dispatched call (which used to call an interpreted method)
1786     CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);
1787     if (!inline_cache-&gt;is_clean()) {
1788       return inline_cache-&gt;set_to_clean();
1789     }
1790   }
1791   return true;
1792 }
1793 
1794 //
1795 // Resets a call-site in compiled code so it will get resolved again.
1796 // This routines handles both virtual call sites, optimized virtual call
1797 // sites, and static call sites. Typically used to change a call sites
1798 // destination from compiled to interpreted.
1799 //
<span class="line-modified">1800 methodHandle SharedRuntime::reresolve_call_site(JavaThread *thread, bool&amp; is_static_call, bool&amp; is_optimized, bool&amp; caller_is_c1, TRAPS) {</span>
1801   ResourceMark rm(thread);
1802   RegisterMap reg_map(thread, false);
1803   frame stub_frame = thread-&gt;last_frame();
1804   assert(stub_frame.is_runtime_frame(), &quot;must be a runtimeStub&quot;);
1805   frame caller = stub_frame.sender(&amp;reg_map);
1806 
1807   // Do nothing if the frame isn&#39;t a live compiled frame.
1808   // nmethod could be deoptimized by the time we get here
1809   // so no update to the caller is needed.
1810 
1811   if (caller.is_compiled_frame() &amp;&amp; !caller.is_deoptimized_frame()) {
1812 
1813     address pc = caller.pc();
1814 
1815     // Check for static or virtual call
<span class="line-modified">1816     CompiledMethod* caller_nm = CodeCache::find_compiled(pc);</span>
1817     caller_is_c1 = caller_nm-&gt;is_compiled_by_c1();
1818 
1819     // Default call_addr is the location of the &quot;basic&quot; call.
1820     // Determine the address of the call we a reresolving. With
1821     // Inline Caches we will always find a recognizable call.
1822     // With Inline Caches disabled we may or may not find a
1823     // recognizable call. We will always find a call for static
1824     // calls and for optimized virtual calls. For vanilla virtual
1825     // calls it depends on the state of the UseInlineCaches switch.
1826     //
1827     // With Inline Caches disabled we can get here for a virtual call
1828     // for two reasons:
1829     //   1 - calling an abstract method. The vtable for abstract methods
1830     //       will run us thru handle_wrong_method and we will eventually
1831     //       end up in the interpreter to throw the ame.
1832     //   2 - a racing deoptimization. We could be doing a vanilla vtable
1833     //       call and between the time we fetch the entry address and
1834     //       we jump to it the target gets deoptimized. Similar to 1
1835     //       we will wind up in the interprter (thru a c2i with c2).
1836     //
</pre>
<hr />
<pre>
1841       CompiledICLocker ml(caller_nm);
1842       // Location of call instruction
1843       call_addr = caller_nm-&gt;call_instruction_address(pc);
1844     }
1845     // Make sure nmethod doesn&#39;t get deoptimized and removed until
1846     // this is done with it.
1847     // CLEANUP - with lazy deopt shouldn&#39;t need this lock
1848     nmethodLocker nmlock(caller_nm);
1849 
1850     if (call_addr != NULL) {
1851       RelocIterator iter(caller_nm, call_addr, call_addr+1);
1852       int ret = iter.next(); // Get item
1853       if (ret) {
1854         assert(iter.addr() == call_addr, &quot;must find call&quot;);
1855         if (iter.type() == relocInfo::static_call_type) {
1856           is_static_call = true;
1857         } else {
1858           assert(iter.type() == relocInfo::virtual_call_type ||
1859                  iter.type() == relocInfo::opt_virtual_call_type
1860                 , &quot;unexpected relocInfo. type&quot;);
<span class="line-added">1861           is_optimized = (iter.type() == relocInfo::opt_virtual_call_type);</span>
1862         }
1863       } else {
1864         assert(!UseInlineCaches, &quot;relocation info. must exist for this address&quot;);
1865       }
1866 
1867       // Cleaning the inline cache will force a new resolve. This is more robust
1868       // than directly setting it to the new destination, since resolving of calls
1869       // is always done through the same code path. (experience shows that it
1870       // leads to very hard to track down bugs, if an inline cache gets updated
1871       // to a wrong method). It should not be performance critical, since the
1872       // resolve is only done once.
1873 
1874       for (;;) {
1875         ICRefillVerifier ic_refill_verifier;
1876         if (!clear_ic_at_addr(caller_nm, call_addr, is_static_call)) {
1877           InlineCacheBuffer::refill_ic_stubs();
1878         } else {
1879           break;
1880         }
1881       }
1882     }
1883   }
1884 
1885   methodHandle callee_method = find_callee_method(thread, CHECK_(methodHandle()));
1886 

1887 #ifndef PRODUCT
1888   Atomic::inc(&amp;_wrong_method_ctr);
1889 
1890   if (TraceCallFixup) {
1891     ResourceMark rm(thread);
1892     tty-&gt;print(&quot;handle_wrong_method reresolving call to&quot;);
1893     callee_method-&gt;print_short_name(tty);
1894     tty-&gt;print_cr(&quot; code: &quot; INTPTR_FORMAT, p2i(callee_method-&gt;code()));
1895   }
1896 #endif
1897 
1898   return callee_method;
1899 }
1900 
1901 address SharedRuntime::handle_unsafe_access(JavaThread* thread, address next_pc) {
1902   // The faulting unsafe accesses should be changed to throw the error
1903   // synchronously instead. Meanwhile the faulting instruction will be
1904   // skipped over (effectively turning it into a no-op) and an
1905   // asynchronous exception will be raised which the thread will
1906   // handle at a later point. If the instruction is a load it will
</pre>
<hr />
<pre>
1961     }
1962   } else {
1963     if (TraceCallFixup) {
1964       tty-&gt;print(&quot;already patched callsite at &quot; INTPTR_FORMAT &quot; to compiled code for&quot;, p2i(caller_pc));
1965       moop-&gt;print_short_name(tty);
1966       tty-&gt;print_cr(&quot; to &quot; INTPTR_FORMAT, p2i(entry_point));
1967     }
1968   }
1969   return false;
1970 }
1971 
1972 // ---------------------------------------------------------------------------
1973 // We are calling the interpreter via a c2i. Normally this would mean that
1974 // we were called by a compiled method. However we could have lost a race
1975 // where we went int -&gt; i2c -&gt; c2i and so the caller could in fact be
1976 // interpreted. If the caller is compiled we attempt to patch the caller
1977 // so he no longer calls into the interpreter.
1978 JRT_LEAF(void, SharedRuntime::fixup_callers_callsite(Method* method, address caller_pc))
1979   Method* moop(method);
1980 


1981   // It&#39;s possible that deoptimization can occur at a call site which hasn&#39;t
1982   // been resolved yet, in which case this function will be called from
1983   // an nmethod that has been patched for deopt and we can ignore the
1984   // request for a fixup.
1985   // Also it is possible that we lost a race in that from_compiled_entry
1986   // is now back to the i2c in that case we don&#39;t need to patch and if
1987   // we did we&#39;d leap into space because the callsite needs to use
1988   // &quot;to interpreter&quot; stub in order to load up the Method*. Don&#39;t
1989   // ask me how I know this...
1990 
1991   CodeBlob* cb = CodeCache::find_blob(caller_pc);
<span class="line-modified">1992   if (cb == NULL || !cb-&gt;is_compiled()) {</span>
<span class="line-added">1993     return;</span>
<span class="line-added">1994   }</span>
<span class="line-added">1995   address entry_point = moop-&gt;from_compiled_entry_no_trampoline(cb-&gt;is_compiled_by_c1());</span>
<span class="line-added">1996   if (entry_point == moop-&gt;get_c2i_entry()) {</span>
1997     return;
1998   }
1999 
2000   // The check above makes sure this is a nmethod.
2001   CompiledMethod* nm = cb-&gt;as_compiled_method_or_null();
2002   assert(nm, &quot;must be&quot;);
2003 
2004   // Get the return PC for the passed caller PC.
2005   address return_pc = caller_pc + frame::pc_return_offset;
2006 
2007   // There is a benign race here. We could be attempting to patch to a compiled
2008   // entry point at the same time the callee is being deoptimized. If that is
2009   // the case then entry_point may in fact point to a c2i and we&#39;d patch the
2010   // call site with the same old data. clear_code will set code() to NULL
2011   // at the end of it. If we happen to see that NULL then we can skip trying
2012   // to patch. If we hit the window where the callee has a c2i in the
2013   // from_compiled_entry and the NULL isn&#39;t present yet then we lose the race
2014   // and patch the code with the same old data. Asi es la vida.
2015 
2016   if (moop-&gt;code() == NULL) return;
</pre>
<hr />
<pre>
2337  private:
2338   enum {
2339     _basic_type_bits = 4,
2340     _basic_type_mask = right_n_bits(_basic_type_bits),
2341     _basic_types_per_int = BitsPerInt / _basic_type_bits,
2342     _compact_int_count = 3
2343   };
2344   // TO DO:  Consider integrating this with a more global scheme for compressing signatures.
2345   // For now, 4 bits per components (plus T_VOID gaps after double/long) is not excessive.
2346 
2347   union {
2348     int  _compact[_compact_int_count];
2349     int* _fingerprint;
2350   } _value;
2351   int _length; // A negative length indicates the fingerprint is in the compact form,
2352                // Otherwise _value._fingerprint is the array.
2353 
2354   // Remap BasicTypes that are handled equivalently by the adapters.
2355   // These are correct for the current system but someday it might be
2356   // necessary to make this mapping platform dependent.
<span class="line-modified">2357   static int adapter_encoding(BasicType in, bool is_valuetype) {</span>
2358     switch (in) {
2359       case T_BOOLEAN:
2360       case T_BYTE:
2361       case T_SHORT:
<span class="line-modified">2362       case T_CHAR: {</span>
<span class="line-modified">2363         if (is_valuetype) {</span>
<span class="line-modified">2364           // Do not widen inline type field types</span>
<span class="line-added">2365           assert(InlineTypePassFieldsAsArgs, &quot;must be enabled&quot;);</span>
<span class="line-added">2366           return in;</span>
<span class="line-added">2367         } else {</span>
<span class="line-added">2368           // They are all promoted to T_INT in the calling convention</span>
<span class="line-added">2369           return T_INT;</span>
<span class="line-added">2370         }</span>
<span class="line-added">2371       }</span>
<span class="line-added">2372 </span>
<span class="line-added">2373       case T_VALUETYPE: {</span>
<span class="line-added">2374         // If inline types are passed as fields, return &#39;in&#39; to differentiate</span>
<span class="line-added">2375         // between a T_VALUETYPE and a T_OBJECT in the signature.</span>
<span class="line-added">2376         return InlineTypePassFieldsAsArgs ? in : adapter_encoding(T_OBJECT, false);</span>
<span class="line-added">2377       }</span>
2378 
2379       case T_OBJECT:
2380       case T_ARRAY:
2381         // In other words, we assume that any register good enough for
2382         // an int or long is good enough for a managed pointer.
2383 #ifdef _LP64
2384         return T_LONG;
2385 #else
2386         return T_INT;
2387 #endif
2388 
2389       case T_INT:
2390       case T_LONG:
2391       case T_FLOAT:
2392       case T_DOUBLE:
2393       case T_VOID:
2394         return in;
2395 
2396       default:
2397         ShouldNotReachHere();
2398         return T_CONFLICT;
2399     }
2400   }
2401 
2402  public:
<span class="line-modified">2403   AdapterFingerPrint(const GrowableArray&lt;SigEntry&gt;* sig, bool has_ro_adapter = false) {</span>
2404     // The fingerprint is based on the BasicType signature encoded
2405     // into an array of ints with eight entries per int.
<span class="line-added">2406     int total_args_passed = (sig != NULL) ? sig-&gt;length() : 0;</span>
2407     int* ptr;
2408     int len = (total_args_passed + (_basic_types_per_int-1)) / _basic_types_per_int;
2409     if (len &lt;= _compact_int_count) {
2410       assert(_compact_int_count == 3, &quot;else change next line&quot;);
2411       _value._compact[0] = _value._compact[1] = _value._compact[2] = 0;
2412       // Storing the signature encoded as signed chars hits about 98%
2413       // of the time.
2414       _length = -len;
2415       ptr = _value._compact;
2416     } else {
2417       _length = len;
2418       _value._fingerprint = NEW_C_HEAP_ARRAY(int, _length, mtCode);
2419       ptr = _value._fingerprint;
2420     }
2421 
2422     // Now pack the BasicTypes with 8 per int
2423     int sig_index = 0;
<span class="line-added">2424     BasicType prev_sbt = T_ILLEGAL;</span>
<span class="line-added">2425     int vt_count = 0;</span>
2426     for (int index = 0; index &lt; len; index++) {
2427       int value = 0;
2428       for (int byte = 0; byte &lt; _basic_types_per_int; byte++) {
<span class="line-modified">2429         int bt = 0;</span>
<span class="line-modified">2430         if (sig_index &lt; total_args_passed) {</span>
<span class="line-modified">2431           BasicType sbt = sig-&gt;at(sig_index++)._bt;</span>
<span class="line-added">2432           if (InlineTypePassFieldsAsArgs &amp;&amp; sbt == T_VALUETYPE) {</span>
<span class="line-added">2433             // Found start of inline type in signature</span>
<span class="line-added">2434             vt_count++;</span>
<span class="line-added">2435             if (sig_index == 1 &amp;&amp; has_ro_adapter) {</span>
<span class="line-added">2436               // With a ro_adapter, replace receiver value type delimiter by T_VOID to prevent matching</span>
<span class="line-added">2437               // with other adapters that have the same value type as first argument and no receiver.</span>
<span class="line-added">2438               sbt = T_VOID;</span>
<span class="line-added">2439             }</span>
<span class="line-added">2440           } else if (InlineTypePassFieldsAsArgs &amp;&amp; sbt == T_VOID &amp;&amp;</span>
<span class="line-added">2441                      prev_sbt != T_LONG &amp;&amp; prev_sbt != T_DOUBLE) {</span>
<span class="line-added">2442             // Found end of inline type in signature</span>
<span class="line-added">2443             vt_count--;</span>
<span class="line-added">2444             assert(vt_count &gt;= 0, &quot;invalid vt_count&quot;);</span>
<span class="line-added">2445           }</span>
<span class="line-added">2446           bt = adapter_encoding(sbt, vt_count &gt; 0);</span>
<span class="line-added">2447           prev_sbt = sbt;</span>
<span class="line-added">2448         }</span>
2449         assert((bt &amp; _basic_type_mask) == bt, &quot;must fit in 4 bits&quot;);
2450         value = (value &lt;&lt; _basic_type_bits) | bt;
2451       }
2452       ptr[index] = value;
2453     }
<span class="line-added">2454     assert(vt_count == 0, &quot;invalid vt_count&quot;);</span>
2455   }
2456 
2457   ~AdapterFingerPrint() {
2458     if (_length &gt; 0) {
2459       FREE_C_HEAP_ARRAY(int, _value._fingerprint);
2460     }
2461   }
2462 
2463   int value(int index) {
2464     if (_length &lt; 0) {
2465       return _value._compact[index];
2466     }
2467     return _value._fingerprint[index];
2468   }
2469   int length() {
2470     if (_length &lt; 0) return -_length;
2471     return _length;
2472   }
2473 
2474   bool is_compact() {
</pre>
<hr />
<pre>
2520 
2521  private:
2522 
2523 #ifndef PRODUCT
2524   static int _lookups; // number of calls to lookup
2525   static int _buckets; // number of buckets checked
2526   static int _equals;  // number of buckets checked with matching hash
2527   static int _hits;    // number of successful lookups
2528   static int _compact; // number of equals calls with compact signature
2529 #endif
2530 
2531   AdapterHandlerEntry* bucket(int i) {
2532     return (AdapterHandlerEntry*)BasicHashtable&lt;mtCode&gt;::bucket(i);
2533   }
2534 
2535  public:
2536   AdapterHandlerTable()
2537     : BasicHashtable&lt;mtCode&gt;(293, (DumpSharedSpaces ? sizeof(CDSAdapterHandlerEntry) : sizeof(AdapterHandlerEntry))) { }
2538 
2539   // Create a new entry suitable for insertion in the table
<span class="line-modified">2540   AdapterHandlerEntry* new_entry(AdapterFingerPrint* fingerprint, address i2c_entry, address c2i_entry,</span>
<span class="line-added">2541                                  address c2i_value_entry, address c2i_value_ro_entry,</span>
<span class="line-added">2542                                  address c2i_unverified_entry, address c2i_unverified_value_entry, address c2i_no_clinit_check_entry) {</span>
2543     AdapterHandlerEntry* entry = (AdapterHandlerEntry*)BasicHashtable&lt;mtCode&gt;::new_entry(fingerprint-&gt;compute_hash());
<span class="line-modified">2544     entry-&gt;init(fingerprint, i2c_entry, c2i_entry, c2i_value_entry, c2i_value_ro_entry,</span>
<span class="line-added">2545                 c2i_unverified_entry, c2i_unverified_value_entry, c2i_no_clinit_check_entry);</span>
2546     if (DumpSharedSpaces) {
2547       ((CDSAdapterHandlerEntry*)entry)-&gt;init();
2548     }
2549     return entry;
2550   }
2551 
2552   // Insert an entry into the table
2553   void add(AdapterHandlerEntry* entry) {
2554     int index = hash_to_index(entry-&gt;hash());
2555     add_entry(index, entry);
2556   }
2557 
2558   void free_entry(AdapterHandlerEntry* entry) {
2559     entry-&gt;deallocate();
2560     BasicHashtable&lt;mtCode&gt;::free_entry(entry);
2561   }
2562 
2563   // Find a entry with the same fingerprint if it exists
<span class="line-modified">2564   AdapterHandlerEntry* lookup(const GrowableArray&lt;SigEntry&gt;* sig, bool has_ro_adapter = false) {</span>
2565     NOT_PRODUCT(_lookups++);
<span class="line-modified">2566     AdapterFingerPrint fp(sig, has_ro_adapter);</span>
2567     unsigned int hash = fp.compute_hash();
2568     int index = hash_to_index(hash);
2569     for (AdapterHandlerEntry* e = bucket(index); e != NULL; e = e-&gt;next()) {
2570       NOT_PRODUCT(_buckets++);
2571       if (e-&gt;hash() == hash) {
2572         NOT_PRODUCT(_equals++);
2573         if (fp.equals(e-&gt;fingerprint())) {
2574 #ifndef PRODUCT
2575           if (fp.is_compact()) _compact++;
2576           _hits++;
2577 #endif
2578           return e;
2579         }
2580       }
2581     }
2582     return NULL;
2583   }
2584 
2585 #ifndef PRODUCT
2586   void print_statistics() {
</pre>
<hr />
<pre>
2642   bool has_next() {
2643     return _current != NULL;
2644   }
2645   AdapterHandlerEntry* next() {
2646     if (_current != NULL) {
2647       AdapterHandlerEntry* result = _current;
2648       _current = _current-&gt;next();
2649       if (_current == NULL) scan();
2650       return result;
2651     } else {
2652       return NULL;
2653     }
2654   }
2655 };
2656 
2657 
2658 // ---------------------------------------------------------------------------
2659 // Implementation of AdapterHandlerLibrary
2660 AdapterHandlerTable* AdapterHandlerLibrary::_adapters = NULL;
2661 AdapterHandlerEntry* AdapterHandlerLibrary::_abstract_method_handler = NULL;
<span class="line-modified">2662 const int AdapterHandlerLibrary_size = 32*K;</span>
2663 BufferBlob* AdapterHandlerLibrary::_buffer = NULL;
2664 
2665 BufferBlob* AdapterHandlerLibrary::buffer_blob() {
2666   // Should be called only when AdapterHandlerLibrary_lock is active.
2667   if (_buffer == NULL) // Initialize lazily
2668       _buffer = BufferBlob::create(&quot;adapters&quot;, AdapterHandlerLibrary_size);
2669   return _buffer;
2670 }
2671 
2672 extern &quot;C&quot; void unexpected_adapter_call() {
2673   ShouldNotCallThis();
2674 }
2675 
2676 void AdapterHandlerLibrary::initialize() {
2677   if (_adapters != NULL) return;
2678   _adapters = new AdapterHandlerTable();
2679 
2680   // Create a special handler for abstract methods.  Abstract methods
2681   // are never compiled so an i2c entry is somewhat meaningless, but
2682   // throw AbstractMethodError just in case.
2683   // Pass wrong_method_abstract for the c2i transitions to return
2684   // AbstractMethodError for invalid invocations.
2685   address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();
<span class="line-modified">2686   _abstract_method_handler = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(NULL),</span>
2687                                                               StubRoutines::throw_AbstractMethodError_entry(),
<span class="line-added">2688                                                               wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,</span>
2689                                                               wrong_method_abstract, wrong_method_abstract);
2690 }
2691 
2692 AdapterHandlerEntry* AdapterHandlerLibrary::new_entry(AdapterFingerPrint* fingerprint,
2693                                                       address i2c_entry,
2694                                                       address c2i_entry,
<span class="line-added">2695                                                       address c2i_value_entry,</span>
<span class="line-added">2696                                                       address c2i_value_ro_entry,</span>
2697                                                       address c2i_unverified_entry,
<span class="line-added">2698                                                       address c2i_unverified_value_entry,</span>
2699                                                       address c2i_no_clinit_check_entry) {
<span class="line-modified">2700   return _adapters-&gt;new_entry(fingerprint, i2c_entry, c2i_entry, c2i_value_entry, c2i_value_ro_entry, c2i_unverified_entry,</span>
<span class="line-added">2701                               c2i_unverified_value_entry, c2i_no_clinit_check_entry);</span>
<span class="line-added">2702 }</span>
<span class="line-added">2703 </span>
<span class="line-added">2704 static void generate_trampoline(address trampoline, address destination) {</span>
<span class="line-added">2705   if (*(int*)trampoline == 0) {</span>
<span class="line-added">2706     CodeBuffer buffer(trampoline, (int)SharedRuntime::trampoline_size());</span>
<span class="line-added">2707     MacroAssembler _masm(&amp;buffer);</span>
<span class="line-added">2708     SharedRuntime::generate_trampoline(&amp;_masm, destination);</span>
<span class="line-added">2709     assert(*(int*)trampoline != 0, &quot;Instruction(s) for trampoline must not be encoded as zeros.&quot;);</span>
<span class="line-added">2710       _masm.flush();</span>
<span class="line-added">2711 </span>
<span class="line-added">2712     if (PrintInterpreter) {</span>
<span class="line-added">2713       Disassembler::decode(buffer.insts_begin(), buffer.insts_end());</span>
<span class="line-added">2714     }</span>
<span class="line-added">2715   }</span>
2716 }
2717 
2718 AdapterHandlerEntry* AdapterHandlerLibrary::get_adapter(const methodHandle&amp; method) {
2719   AdapterHandlerEntry* entry = get_adapter0(method);
2720   if (entry != NULL &amp;&amp; method-&gt;is_shared()) {
2721     // See comments around Method::link_method()
2722     MutexLocker mu(AdapterHandlerLibrary_lock);
2723     if (method-&gt;adapter() == NULL) {
2724       method-&gt;update_adapter_trampoline(entry);
2725     }
<span class="line-modified">2726     generate_trampoline(method-&gt;from_compiled_entry(),          entry-&gt;get_c2i_entry());</span>
<span class="line-modified">2727     generate_trampoline(method-&gt;from_compiled_value_ro_entry(), entry-&gt;get_c2i_value_ro_entry());</span>
<span class="line-modified">2728     generate_trampoline(method-&gt;from_compiled_value_entry(),    entry-&gt;get_c2i_value_entry());</span>
<span class="line-modified">2729   }</span>
<span class="line-modified">2730 </span>
<span class="line-modified">2731   return entry;</span>
<span class="line-modified">2732 }</span>
<span class="line-added">2733 </span>
2734 
<span class="line-modified">2735 CompiledEntrySignature::CompiledEntrySignature(Method* method) :</span>
<span class="line-modified">2736   _method(method), _num_value_args(0), _has_value_recv(false),</span>
<span class="line-added">2737   _sig_cc(NULL), _sig_cc_ro(NULL), _regs(NULL), _regs_cc(NULL), _regs_cc_ro(NULL),</span>
<span class="line-added">2738   _args_on_stack(0), _args_on_stack_cc(0), _args_on_stack_cc_ro(0),</span>
<span class="line-added">2739   _c1_needs_stack_repair(false), _c2_needs_stack_repair(false), _has_scalarized_args(false) {</span>
<span class="line-added">2740   _has_reserved_entries = false;</span>
<span class="line-added">2741   _sig = new GrowableArray&lt;SigEntry&gt;(method-&gt;size_of_parameters());</span>
<span class="line-added">2742 </span>
<span class="line-added">2743 }</span>
<span class="line-added">2744 </span>
<span class="line-added">2745 int CompiledEntrySignature::compute_scalarized_cc(GrowableArray&lt;SigEntry&gt;*&amp; sig_cc, VMRegPair*&amp; regs_cc, bool scalar_receiver) {</span>
<span class="line-added">2746   InstanceKlass* holder = _method-&gt;method_holder();</span>
<span class="line-added">2747   sig_cc = new GrowableArray&lt;SigEntry&gt;(_method-&gt;size_of_parameters());</span>
<span class="line-added">2748   if (!_method-&gt;is_static()) {</span>
<span class="line-added">2749     if (holder-&gt;is_value() &amp;&amp; scalar_receiver &amp;&amp; ValueKlass::cast(holder)-&gt;is_scalarizable()) {</span>
<span class="line-added">2750       sig_cc-&gt;appendAll(ValueKlass::cast(holder)-&gt;extended_sig());</span>
<span class="line-added">2751     } else {</span>
<span class="line-added">2752       SigEntry::add_entry(sig_cc, T_OBJECT);</span>
<span class="line-added">2753     }</span>
<span class="line-added">2754   }</span>
<span class="line-added">2755   Thread* THREAD = Thread::current();</span>
<span class="line-added">2756   for (SignatureStream ss(_method-&gt;signature()); !ss.at_return_type(); ss.next()) {</span>
<span class="line-added">2757     if (ss.type() == T_VALUETYPE) {</span>
<span class="line-added">2758       ValueKlass* vk = ss.as_value_klass(holder);</span>
<span class="line-added">2759       if (vk-&gt;is_scalarizable()) {</span>
<span class="line-added">2760         sig_cc-&gt;appendAll(vk-&gt;extended_sig());</span>
<span class="line-added">2761       } else {</span>
<span class="line-added">2762         SigEntry::add_entry(sig_cc, T_OBJECT);</span>
2763       }
<span class="line-added">2764     } else {</span>
<span class="line-added">2765       SigEntry::add_entry(sig_cc, ss.type());</span>
2766     }
2767   }
<span class="line-added">2768   regs_cc = NEW_RESOURCE_ARRAY(VMRegPair, sig_cc-&gt;length() + 2);</span>
<span class="line-added">2769   return SharedRuntime::java_calling_convention(sig_cc, regs_cc);</span>
<span class="line-added">2770 }</span>
2771 
<span class="line-modified">2772 int CompiledEntrySignature::insert_reserved_entry(int ret_off) {</span>
<span class="line-added">2773   // Find index in signature that belongs to return address slot</span>
<span class="line-added">2774   BasicType bt = T_ILLEGAL;</span>
<span class="line-added">2775   int i = 0;</span>
<span class="line-added">2776   for (uint off = 0; i &lt; _sig_cc-&gt;length(); ++i) {</span>
<span class="line-added">2777     if (SigEntry::skip_value_delimiters(_sig_cc, i)) {</span>
<span class="line-added">2778       VMReg first = _regs_cc[off++].first();</span>
<span class="line-added">2779       if (first-&gt;is_valid() &amp;&amp; first-&gt;is_stack()) {</span>
<span class="line-added">2780         // Select a type for the reserved entry that will end up on the stack</span>
<span class="line-added">2781         bt = _sig_cc-&gt;at(i)._bt;</span>
<span class="line-added">2782         if (((int)first-&gt;reg2stack() + VMRegImpl::slots_per_word) == ret_off) {</span>
<span class="line-added">2783           break; // Index of the return address found</span>
<span class="line-added">2784         }</span>
<span class="line-added">2785       }</span>
<span class="line-added">2786     }</span>
<span class="line-added">2787   }</span>
<span class="line-added">2788   // Insert reserved entry and re-compute calling convention</span>
<span class="line-added">2789   SigEntry::insert_reserved_entry(_sig_cc, i, bt);</span>
<span class="line-added">2790   return SharedRuntime::java_calling_convention(_sig_cc, _regs_cc);</span>
<span class="line-added">2791 }</span>
<span class="line-added">2792 </span>
<span class="line-added">2793 // See if we can save space by sharing the same entry for VVEP and VVEP(RO),</span>
<span class="line-added">2794 // or the same entry for VEP and VVEP(RO).</span>
<span class="line-added">2795 CodeOffsets::Entries CompiledEntrySignature::c1_value_ro_entry_type() const {</span>
<span class="line-added">2796   if (!has_scalarized_args()) {</span>
<span class="line-added">2797     // VEP/VVEP/VVEP(RO) all share the same entry. There&#39;s no packing.</span>
<span class="line-added">2798     return CodeOffsets::Verified_Entry;</span>
<span class="line-added">2799   }</span>
<span class="line-added">2800   if (_method-&gt;is_static()) {</span>
<span class="line-added">2801     // Static methods don&#39;t need VVEP(RO)</span>
<span class="line-added">2802     return CodeOffsets::Verified_Entry;</span>
<span class="line-added">2803   }</span>
<span class="line-added">2804 </span>
<span class="line-added">2805   if (has_value_recv()) {</span>
<span class="line-added">2806     if (num_value_args() == 1) {</span>
<span class="line-added">2807       // Share same entry for VVEP and VVEP(RO).</span>
<span class="line-added">2808       // This is quite common: we have an instance method in a ValueKlass that has</span>
<span class="line-added">2809       // no value args other than &lt;this&gt;.</span>
<span class="line-added">2810       return CodeOffsets::Verified_Value_Entry;</span>
<span class="line-added">2811     } else {</span>
<span class="line-added">2812       assert(num_value_args() &gt; 1, &quot;must be&quot;);</span>
<span class="line-added">2813       // No sharing:</span>
<span class="line-added">2814       //   VVEP(RO) -- &lt;this&gt; is passed as object</span>
<span class="line-added">2815       //   VEP      -- &lt;this&gt; is passed as fields</span>
<span class="line-added">2816       return CodeOffsets::Verified_Value_Entry_RO;</span>
<span class="line-added">2817     }</span>
<span class="line-added">2818   }</span>
<span class="line-added">2819 </span>
<span class="line-added">2820   // Either a static method, or &lt;this&gt; is not a value type</span>
<span class="line-added">2821   if (args_on_stack_cc() != args_on_stack_cc_ro() || _has_reserved_entries) {</span>
<span class="line-added">2822     // No sharing:</span>
<span class="line-added">2823     // Some arguments are passed on the stack, and we have inserted reserved entries</span>
<span class="line-added">2824     // into the VEP, but we never insert reserved entries into the VVEP(RO).</span>
<span class="line-added">2825     return CodeOffsets::Verified_Value_Entry_RO;</span>
<span class="line-added">2826   } else {</span>
<span class="line-added">2827     // Share same entry for VEP and VVEP(RO).</span>
<span class="line-added">2828     return CodeOffsets::Verified_Entry;</span>
<span class="line-added">2829   }</span>
<span class="line-added">2830 }</span>
<span class="line-added">2831 </span>
<span class="line-added">2832 </span>
<span class="line-added">2833 void CompiledEntrySignature::compute_calling_conventions() {</span>
<span class="line-added">2834   // Get the (non-scalarized) signature and check for value type arguments</span>
<span class="line-added">2835   if (!_method-&gt;is_static()) {</span>
<span class="line-added">2836     if (_method-&gt;method_holder()-&gt;is_value() &amp;&amp; ValueKlass::cast(_method-&gt;method_holder())-&gt;is_scalarizable()) {</span>
<span class="line-added">2837       _has_value_recv = true;</span>
<span class="line-added">2838       _num_value_args++;</span>
<span class="line-added">2839     }</span>
<span class="line-added">2840     SigEntry::add_entry(_sig, T_OBJECT);</span>
<span class="line-added">2841   }</span>
<span class="line-added">2842   for (SignatureStream ss(_method-&gt;signature()); !ss.at_return_type(); ss.next()) {</span>
<span class="line-added">2843     BasicType bt = ss.type();</span>
<span class="line-added">2844     if (bt == T_VALUETYPE) {</span>
<span class="line-added">2845       if (ss.as_value_klass(_method-&gt;method_holder())-&gt;is_scalarizable()) {</span>
<span class="line-added">2846         _num_value_args++;</span>
<span class="line-added">2847       }</span>
<span class="line-added">2848       bt = T_OBJECT;</span>
<span class="line-added">2849     }</span>
<span class="line-added">2850     SigEntry::add_entry(_sig, bt);</span>
<span class="line-added">2851   }</span>
<span class="line-added">2852   if (_method-&gt;is_abstract() &amp;&amp; !(InlineTypePassFieldsAsArgs &amp;&amp; has_value_arg())) {</span>
<span class="line-added">2853     return;</span>
<span class="line-added">2854   }</span>
<span class="line-added">2855 </span>
<span class="line-added">2856   // Get a description of the compiled java calling convention and the largest used (VMReg) stack slot usage</span>
<span class="line-added">2857   _regs = NEW_RESOURCE_ARRAY(VMRegPair, _sig-&gt;length());</span>
<span class="line-added">2858   _args_on_stack = SharedRuntime::java_calling_convention(_sig, _regs);</span>
<span class="line-added">2859 </span>
<span class="line-added">2860   // Now compute the scalarized calling convention if there are value types in the signature</span>
<span class="line-added">2861   _sig_cc = _sig;</span>
<span class="line-added">2862   _sig_cc_ro = _sig;</span>
<span class="line-added">2863   _regs_cc = _regs;</span>
<span class="line-added">2864   _regs_cc_ro = _regs;</span>
<span class="line-added">2865   _args_on_stack_cc = _args_on_stack;</span>
<span class="line-added">2866   _args_on_stack_cc_ro = _args_on_stack;</span>
<span class="line-added">2867 </span>
<span class="line-added">2868   if (InlineTypePassFieldsAsArgs &amp;&amp; has_value_arg() &amp;&amp; !_method-&gt;is_native()) {</span>
<span class="line-added">2869     _args_on_stack_cc = compute_scalarized_cc(_sig_cc, _regs_cc, /* scalar_receiver = */ true);</span>
<span class="line-added">2870 </span>
<span class="line-added">2871     _sig_cc_ro = _sig_cc;</span>
<span class="line-added">2872     _regs_cc_ro = _regs_cc;</span>
<span class="line-added">2873     _args_on_stack_cc_ro = _args_on_stack_cc;</span>
<span class="line-added">2874     if (_has_value_recv || _args_on_stack_cc &gt; _args_on_stack) {</span>
<span class="line-added">2875       // For interface calls, we need another entry point / adapter to unpack the receiver</span>
<span class="line-added">2876       _args_on_stack_cc_ro = compute_scalarized_cc(_sig_cc_ro, _regs_cc_ro, /* scalar_receiver = */ false);</span>
<span class="line-added">2877     }</span>
<span class="line-added">2878 </span>
<span class="line-added">2879     // Compute the stack extension that is required to convert between the calling conventions.</span>
<span class="line-added">2880     // The stack slots at these offsets are occupied by the return address with the unscalarized</span>
<span class="line-added">2881     // calling convention. Don&#39;t use them for arguments with the scalarized calling convention.</span>
<span class="line-added">2882     int ret_off    = _args_on_stack_cc - _args_on_stack;</span>
<span class="line-added">2883     int ret_off_ro = _args_on_stack_cc - _args_on_stack_cc_ro;</span>
<span class="line-added">2884     assert(ret_off_ro &lt;= 0 || ret_off &gt; 0, &quot;receiver unpacking requires more stack space than expected&quot;);</span>
<span class="line-added">2885 </span>
<span class="line-added">2886     if (ret_off &gt; 0) {</span>
<span class="line-added">2887       // Make sure the stack of the scalarized calling convention with the reserved</span>
<span class="line-added">2888       // entries (2 slots each) remains 16-byte (4 slots) aligned after stack extension.</span>
<span class="line-added">2889       int alignment = StackAlignmentInBytes / VMRegImpl::stack_slot_size;</span>
<span class="line-added">2890       if (ret_off_ro != ret_off &amp;&amp; ret_off_ro &gt;= 0) {</span>
<span class="line-added">2891         ret_off    += 4; // Account for two reserved entries (4 slots)</span>
<span class="line-added">2892         ret_off_ro += 4;</span>
<span class="line-added">2893         ret_off     = align_up(ret_off, alignment);</span>
<span class="line-added">2894         ret_off_ro  = align_up(ret_off_ro, alignment);</span>
<span class="line-added">2895         // TODO can we avoid wasting a stack slot here?</span>
<span class="line-added">2896         //assert(ret_off != ret_off_ro, &quot;fail&quot;);</span>
<span class="line-added">2897         if (ret_off &gt; ret_off_ro) {</span>
<span class="line-added">2898           swap(ret_off, ret_off_ro); // Sort by offset</span>
<span class="line-added">2899         }</span>
<span class="line-added">2900         _args_on_stack_cc = insert_reserved_entry(ret_off);</span>
<span class="line-added">2901         _args_on_stack_cc = insert_reserved_entry(ret_off_ro);</span>
<span class="line-added">2902       } else {</span>
<span class="line-added">2903         ret_off += 2; // Account for one reserved entry (2 slots)</span>
<span class="line-added">2904         ret_off = align_up(ret_off, alignment);</span>
<span class="line-added">2905         _args_on_stack_cc = insert_reserved_entry(ret_off);</span>
<span class="line-added">2906       }</span>
<span class="line-added">2907 </span>
<span class="line-added">2908       _has_reserved_entries = true;</span>
<span class="line-added">2909     }</span>
<span class="line-added">2910 </span>
<span class="line-added">2911     // Upper bound on stack arguments to avoid hitting the argument limit and</span>
<span class="line-added">2912     // bailing out of compilation (&quot;unsupported incoming calling sequence&quot;).</span>
<span class="line-added">2913     // TODO we need a reasonable limit (flag?) here</span>
<span class="line-added">2914     if (_args_on_stack_cc &gt; 50) {</span>
<span class="line-added">2915       // Don&#39;t scalarize value type arguments</span>
<span class="line-added">2916       _sig_cc = _sig;</span>
<span class="line-added">2917       _sig_cc_ro = _sig;</span>
<span class="line-added">2918       _regs_cc = _regs;</span>
<span class="line-added">2919       _regs_cc_ro = _regs;</span>
<span class="line-added">2920       _args_on_stack_cc = _args_on_stack;</span>
<span class="line-added">2921       _args_on_stack_cc_ro = _args_on_stack;</span>
<span class="line-added">2922     } else {</span>
<span class="line-added">2923       _c1_needs_stack_repair = (_args_on_stack_cc &lt; _args_on_stack) || (_args_on_stack_cc_ro &lt; _args_on_stack);</span>
<span class="line-added">2924       _c2_needs_stack_repair = (_args_on_stack_cc &gt; _args_on_stack) || (_args_on_stack_cc &gt; _args_on_stack_cc_ro);</span>
<span class="line-added">2925       _has_scalarized_args = true;</span>
<span class="line-added">2926     }</span>
<span class="line-added">2927   }</span>
2928 }
2929 
2930 AdapterHandlerEntry* AdapterHandlerLibrary::get_adapter0(const methodHandle&amp; method) {
2931   // Use customized signature handler.  Need to lock around updates to
2932   // the AdapterHandlerTable (it is not safe for concurrent readers
2933   // and a single writer: this could be fixed if it becomes a
2934   // problem).
2935 
2936   ResourceMark rm;
2937 
<span class="line-modified">2938   NOT_PRODUCT(int insts_size = 0);</span>
2939   AdapterBlob* new_adapter = NULL;
2940   AdapterHandlerEntry* entry = NULL;
2941   AdapterFingerPrint* fingerprint = NULL;
<span class="line-added">2942 </span>
2943   {
2944     MutexLocker mu(AdapterHandlerLibrary_lock);
2945     // make sure data structure is initialized
2946     initialize();
2947 
<span class="line-modified">2948     CompiledEntrySignature ces(method());</span>
<span class="line-modified">2949     {</span>
<span class="line-added">2950        MutexUnlocker mul(AdapterHandlerLibrary_lock);</span>
<span class="line-added">2951        ces.compute_calling_conventions();</span>
2952     }
<span class="line-added">2953     GrowableArray&lt;SigEntry&gt;&amp; sig       = ces.sig();</span>
<span class="line-added">2954     GrowableArray&lt;SigEntry&gt;&amp; sig_cc    = ces.sig_cc();</span>
<span class="line-added">2955     GrowableArray&lt;SigEntry&gt;&amp; sig_cc_ro = ces.sig_cc_ro();</span>
<span class="line-added">2956     VMRegPair* regs         = ces.regs();</span>
<span class="line-added">2957     VMRegPair* regs_cc      = ces.regs_cc();</span>
<span class="line-added">2958     VMRegPair* regs_cc_ro   = ces.regs_cc_ro();</span>
2959 
<span class="line-modified">2960     if (ces.has_scalarized_args()) {</span>
<span class="line-modified">2961       method-&gt;set_has_scalarized_args(true);</span>
<span class="line-added">2962       method-&gt;set_c1_needs_stack_repair(ces.c1_needs_stack_repair());</span>
<span class="line-added">2963       method-&gt;set_c2_needs_stack_repair(ces.c2_needs_stack_repair());</span>
<span class="line-added">2964     }</span>
2965 
<span class="line-modified">2966     if (method-&gt;is_abstract()) {</span>
<span class="line-modified">2967       if (ces.has_scalarized_args()) {</span>
<span class="line-modified">2968         // Save a C heap allocated version of the signature for abstract methods with scalarized value type arguments</span>
<span class="line-modified">2969         address wrong_method_abstract = SharedRuntime::get_handle_wrong_method_abstract_stub();</span>
<span class="line-modified">2970         entry = AdapterHandlerLibrary::new_entry(new AdapterFingerPrint(NULL),</span>
<span class="line-modified">2971                                                  StubRoutines::throw_AbstractMethodError_entry(),</span>
<span class="line-modified">2972                                                  wrong_method_abstract, wrong_method_abstract, wrong_method_abstract,</span>
<span class="line-modified">2973                                                  wrong_method_abstract, wrong_method_abstract);</span>
<span class="line-modified">2974         GrowableArray&lt;SigEntry&gt;* heap_sig = new (ResourceObj::C_HEAP, mtInternal) GrowableArray&lt;SigEntry&gt;(sig_cc_ro.length(), true);</span>
<span class="line-added">2975         heap_sig-&gt;appendAll(&amp;sig_cc_ro);</span>
<span class="line-added">2976         entry-&gt;set_sig_cc(heap_sig);</span>
<span class="line-added">2977         return entry;</span>
<span class="line-added">2978       } else {</span>
<span class="line-added">2979         return _abstract_method_handler;</span>
<span class="line-added">2980       }</span>
2981     }

2982 
2983     // Lookup method signature&#39;s fingerprint
<span class="line-modified">2984     entry = _adapters-&gt;lookup(&amp;sig_cc, regs_cc != regs_cc_ro);</span>
2985 
2986 #ifdef ASSERT
2987     AdapterHandlerEntry* shared_entry = NULL;
2988     // Start adapter sharing verification only after the VM is booted.
2989     if (VerifyAdapterSharing &amp;&amp; (entry != NULL)) {
2990       shared_entry = entry;
2991       entry = NULL;
2992     }
2993 #endif
2994 
2995     if (entry != NULL) {
2996       return entry;
2997     }
2998 
<span class="line-modified">2999     // Make a C heap allocated version of the fingerprint to store in the adapter</span>



3000     fingerprint = new AdapterFingerPrint(&amp;sig_cc, regs_cc != regs_cc_ro);
3001 
3002     // StubRoutines::code2() is initialized after this function can be called. As a result,
3003     // VerifyAdapterCalls and VerifyAdapterSharing can fail if we re-use code that generated
3004     // prior to StubRoutines::code2() being set. Checks refer to checks generated in an I2C
3005     // stub that ensure that an I2C stub is called from an interpreter frame.
3006     bool contains_all_checks = StubRoutines::code2() != NULL;
3007 
3008     // Create I2C &amp; C2I handlers
3009     BufferBlob* buf = buffer_blob(); // the temporary code buffer in CodeCache
3010     if (buf != NULL) {
3011       CodeBuffer buffer(buf);
3012       short buffer_locs[20];
3013       buffer.insts()-&gt;initialize_shared_locs((relocInfo*)buffer_locs,
3014                                              sizeof(buffer_locs)/sizeof(relocInfo));
3015 
3016       MacroAssembler _masm(&amp;buffer);
3017       entry = SharedRuntime::generate_i2c2i_adapters(&amp;_masm,
<span class="line-modified">3018                                                      ces.args_on_stack(),</span>
<span class="line-modified">3019                                                      &amp;sig,</span>

3020                                                      regs,
<span class="line-modified">3021                                                      &amp;sig_cc,</span>
<span class="line-added">3022                                                      regs_cc,</span>
<span class="line-added">3023                                                      &amp;sig_cc_ro,</span>
<span class="line-added">3024                                                      regs_cc_ro,</span>
<span class="line-added">3025                                                      fingerprint,</span>
<span class="line-added">3026                                                      new_adapter);</span>
<span class="line-added">3027 </span>
<span class="line-added">3028       if (ces.has_scalarized_args()) {</span>
<span class="line-added">3029         // Save a C heap allocated version of the scalarized signature and store it in the adapter</span>
<span class="line-added">3030         GrowableArray&lt;SigEntry&gt;* heap_sig = new (ResourceObj::C_HEAP, mtInternal) GrowableArray&lt;SigEntry&gt;(sig_cc.length(), true);</span>
<span class="line-added">3031         heap_sig-&gt;appendAll(&amp;sig_cc);</span>
<span class="line-added">3032         entry-&gt;set_sig_cc(heap_sig);</span>
<span class="line-added">3033       }</span>
<span class="line-added">3034 </span>
3035 #ifdef ASSERT
3036       if (VerifyAdapterSharing) {
3037         if (shared_entry != NULL) {
<span class="line-added">3038           if (!shared_entry-&gt;compare_code(buf-&gt;code_begin(), buffer.insts_size())) {</span>
<span class="line-added">3039             method-&gt;print();</span>
<span class="line-added">3040           }</span>
3041           assert(shared_entry-&gt;compare_code(buf-&gt;code_begin(), buffer.insts_size()), &quot;code must match&quot;);
3042           // Release the one just created and return the original
3043           _adapters-&gt;free_entry(entry);
3044           return shared_entry;
3045         } else  {
3046           entry-&gt;save_code(buf-&gt;code_begin(), buffer.insts_size());
3047         }
3048       }
3049 #endif
3050 

3051       NOT_PRODUCT(insts_size = buffer.insts_size());
3052     }
3053     if (new_adapter == NULL) {
3054       // CodeCache is full, disable compilation
3055       // Ought to log this but compile log is only per compile thread
3056       // and we&#39;re some non descript Java thread.
3057       return NULL; // Out of CodeCache space
3058     }
3059     entry-&gt;relocate(new_adapter-&gt;content_begin());
3060 #ifndef PRODUCT
3061     // debugging suppport
3062     if (PrintAdapterHandlers || PrintStubCode) {
3063       ttyLocker ttyl;
3064       entry-&gt;print_adapter_on(tty);
3065       tty-&gt;print_cr(&quot;i2c argument handler #%d for: %s %s %s (%d bytes generated)&quot;,
3066                     _adapters-&gt;number_of_entries(), (method-&gt;is_static() ? &quot;static&quot; : &quot;receiver&quot;),
3067                     method-&gt;signature()-&gt;as_C_string(), fingerprint-&gt;as_string(), insts_size);
3068       tty-&gt;print_cr(&quot;c2i argument handler starts at %p&quot;, entry-&gt;get_c2i_entry());
3069       if (Verbose || PrintStubCode) {
3070         address first_pc = entry-&gt;base_address();
</pre>
<hr />
<pre>
3086     char blob_id[256];
3087     jio_snprintf(blob_id,
3088                  sizeof(blob_id),
3089                  &quot;%s(%s)@&quot; PTR_FORMAT,
3090                  new_adapter-&gt;name(),
3091                  fingerprint-&gt;as_string(),
3092                  new_adapter-&gt;content_begin());
3093     Forte::register_stub(blob_id, new_adapter-&gt;content_begin(), new_adapter-&gt;content_end());
3094 
3095     if (JvmtiExport::should_post_dynamic_code_generated()) {
3096       JvmtiExport::post_dynamic_code_generated(blob_id, new_adapter-&gt;content_begin(), new_adapter-&gt;content_end());
3097     }
3098   }
3099   return entry;
3100 }
3101 
3102 address AdapterHandlerEntry::base_address() {
3103   address base = _i2c_entry;
3104   if (base == NULL)  base = _c2i_entry;
3105   assert(base &lt;= _c2i_entry || _c2i_entry == NULL, &quot;&quot;);
<span class="line-added">3106   assert(base &lt;= _c2i_value_entry || _c2i_value_entry == NULL, &quot;&quot;);</span>
<span class="line-added">3107   assert(base &lt;= _c2i_value_ro_entry || _c2i_value_ro_entry == NULL, &quot;&quot;);</span>
3108   assert(base &lt;= _c2i_unverified_entry || _c2i_unverified_entry == NULL, &quot;&quot;);
<span class="line-added">3109   assert(base &lt;= _c2i_unverified_value_entry || _c2i_unverified_value_entry == NULL, &quot;&quot;);</span>
3110   assert(base &lt;= _c2i_no_clinit_check_entry || _c2i_no_clinit_check_entry == NULL, &quot;&quot;);
3111   return base;
3112 }
3113 
3114 void AdapterHandlerEntry::relocate(address new_base) {
3115   address old_base = base_address();
3116   assert(old_base != NULL, &quot;&quot;);
3117   ptrdiff_t delta = new_base - old_base;
3118   if (_i2c_entry != NULL)
3119     _i2c_entry += delta;
3120   if (_c2i_entry != NULL)
3121     _c2i_entry += delta;
<span class="line-added">3122   if (_c2i_value_entry != NULL)</span>
<span class="line-added">3123     _c2i_value_entry += delta;</span>
<span class="line-added">3124   if (_c2i_value_ro_entry != NULL)</span>
<span class="line-added">3125     _c2i_value_ro_entry += delta;</span>
3126   if (_c2i_unverified_entry != NULL)
3127     _c2i_unverified_entry += delta;
<span class="line-added">3128   if (_c2i_unverified_value_entry != NULL)</span>
<span class="line-added">3129     _c2i_unverified_value_entry += delta;</span>
3130   if (_c2i_no_clinit_check_entry != NULL)
3131     _c2i_no_clinit_check_entry += delta;
3132   assert(base_address() == new_base, &quot;&quot;);
3133 }
3134 
3135 
3136 void AdapterHandlerEntry::deallocate() {
3137   delete _fingerprint;
<span class="line-added">3138   if (_sig_cc != NULL) {</span>
<span class="line-added">3139     delete _sig_cc;</span>
<span class="line-added">3140   }</span>
3141 #ifdef ASSERT
3142   FREE_C_HEAP_ARRAY(unsigned char, _saved_code);
3143 #endif
3144 }
3145 
3146 
3147 #ifdef ASSERT
3148 // Capture the code before relocation so that it can be compared
3149 // against other versions.  If the code is captured after relocation
3150 // then relative instructions won&#39;t be equivalent.
3151 void AdapterHandlerEntry::save_code(unsigned char* buffer, int length) {
3152   _saved_code = NEW_C_HEAP_ARRAY(unsigned char, length, mtCode);
3153   _saved_code_length = length;
3154   memcpy(_saved_code, buffer, length);
3155 }
3156 
3157 
3158 bool AdapterHandlerEntry::compare_code(unsigned char* buffer, int length) {
3159   if (length != _saved_code_length) {
3160     return false;
</pre>
<hr />
<pre>
3204       double locs_buf[20];
3205       buffer.insts()-&gt;initialize_shared_locs((relocInfo*)locs_buf, sizeof(locs_buf) / sizeof(relocInfo));
3206 #if defined(AARCH64)
3207       // On AArch64 with ZGC and nmethod entry barriers, we need all oops to be
3208       // in the constant pool to ensure ordering between the barrier and oops
3209       // accesses. For native_wrappers we need a constant.
3210       buffer.initialize_consts_size(8);
3211 #endif
3212       MacroAssembler _masm(&amp;buffer);
3213 
3214       // Fill in the signature array, for the calling-convention call.
3215       const int total_args_passed = method-&gt;size_of_parameters();
3216 
3217       BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_args_passed);
3218       VMRegPair*   regs = NEW_RESOURCE_ARRAY(VMRegPair, total_args_passed);
3219       int i=0;
3220       if (!method-&gt;is_static())  // Pass in receiver first
3221         sig_bt[i++] = T_OBJECT;
3222       SignatureStream ss(method-&gt;signature());
3223       for (; !ss.at_return_type(); ss.next()) {
<span class="line-modified">3224         BasicType bt = ss.type();</span>
<span class="line-added">3225         sig_bt[i++] = bt;  // Collect remaining bits of signature</span>
3226         if (ss.type() == T_LONG || ss.type() == T_DOUBLE)
3227           sig_bt[i++] = T_VOID;   // Longs &amp; doubles take 2 Java slots
3228       }
3229       assert(i == total_args_passed, &quot;&quot;);
3230       BasicType ret_type = ss.type();
3231 
3232       // Now get the compiled-Java layout as input (or output) arguments.
3233       // NOTE: Stubs for compiled entry points of method handle intrinsics
3234       // are just trampolines so the argument registers must be outgoing ones.
3235       const bool is_outgoing = method-&gt;is_method_handle_intrinsic();
3236       int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed, is_outgoing);
3237 
3238       // Generate the compiled-to-native wrapper code
3239       nm = SharedRuntime::generate_native_wrapper(&amp;_masm, method, compile_id, sig_bt, regs, ret_type, critical_entry);
3240 
3241       if (nm != NULL) {
3242         {
3243           MutexLocker pl(CompiledMethod_lock, Mutex::_no_safepoint_check_flag);
3244           if (nm-&gt;make_in_use()) {
3245             method-&gt;set_code(method, nm);
</pre>
<hr />
<pre>
3458   AdapterHandlerTableIterator iter(_adapters);
3459   while (iter.has_next()) {
3460     AdapterHandlerEntry* a = iter.next();
3461     if (b == CodeCache::find_blob(a-&gt;get_i2c_entry())) {
3462       st-&gt;print(&quot;Adapter for signature: &quot;);
3463       a-&gt;print_adapter_on(tty);
3464       return;
3465     }
3466   }
3467   assert(false, &quot;Should have found handler&quot;);
3468 }
3469 
3470 void AdapterHandlerEntry::print_adapter_on(outputStream* st) const {
3471   st-&gt;print(&quot;AHE@&quot; INTPTR_FORMAT &quot;: %s&quot;, p2i(this), fingerprint()-&gt;as_string());
3472   if (get_i2c_entry() != NULL) {
3473     st-&gt;print(&quot; i2c: &quot; INTPTR_FORMAT, p2i(get_i2c_entry()));
3474   }
3475   if (get_c2i_entry() != NULL) {
3476     st-&gt;print(&quot; c2i: &quot; INTPTR_FORMAT, p2i(get_c2i_entry()));
3477   }
<span class="line-added">3478   if (get_c2i_entry() != NULL) {</span>
<span class="line-added">3479     st-&gt;print(&quot; c2iVE: &quot; INTPTR_FORMAT, p2i(get_c2i_value_entry()));</span>
<span class="line-added">3480   }</span>
<span class="line-added">3481   if (get_c2i_entry() != NULL) {</span>
<span class="line-added">3482     st-&gt;print(&quot; c2iVROE: &quot; INTPTR_FORMAT, p2i(get_c2i_value_ro_entry()));</span>
<span class="line-added">3483   }</span>
3484   if (get_c2i_unverified_entry() != NULL) {
<span class="line-modified">3485     st-&gt;print(&quot; c2iUE: &quot; INTPTR_FORMAT, p2i(get_c2i_unverified_entry()));</span>
<span class="line-added">3486   }</span>
<span class="line-added">3487   if (get_c2i_unverified_entry() != NULL) {</span>
<span class="line-added">3488     st-&gt;print(&quot; c2iUVE: &quot; INTPTR_FORMAT, p2i(get_c2i_unverified_value_entry()));</span>
3489   }
3490   if (get_c2i_no_clinit_check_entry() != NULL) {
3491     st-&gt;print(&quot; c2iNCI: &quot; INTPTR_FORMAT, p2i(get_c2i_no_clinit_check_entry()));
3492   }
3493   st-&gt;cr();
3494 }
3495 
3496 #if INCLUDE_CDS
3497 
3498 void CDSAdapterHandlerEntry::init() {
3499   assert(DumpSharedSpaces, &quot;used during dump time only&quot;);
3500   _c2i_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());
<span class="line-added">3501   _c2i_value_ro_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());</span>
<span class="line-added">3502   _c2i_value_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());</span>
3503   _adapter_trampoline = (AdapterHandlerEntry**)MetaspaceShared::misc_code_space_alloc(sizeof(AdapterHandlerEntry*));
3504 };
3505 
3506 #endif // INCLUDE_CDS
3507 
3508 
3509 #ifndef PRODUCT
3510 
3511 void AdapterHandlerLibrary::print_statistics() {
3512   _adapters-&gt;print_statistics();
3513 }
3514 
3515 #endif /* PRODUCT */
3516 
3517 JRT_LEAF(void, SharedRuntime::enable_stack_reserved_zone(JavaThread* thread))
3518   assert(thread-&gt;is_Java_thread(), &quot;Only Java threads have a stack reserved zone&quot;);
3519   if (thread-&gt;stack_reserved_zone_disabled()) {
3520   thread-&gt;enable_stack_reserved_zone();
3521   }
3522   thread-&gt;set_reserved_stack_activation(thread-&gt;stack_base());
</pre>
<hr />
<pre>
3568       break;
3569     } else {
3570       fr = fr.java_sender();
3571     }
3572   }
3573   return activation;
3574 }
3575 
3576 void SharedRuntime::on_slowpath_allocation_exit(JavaThread* thread) {
3577   // After any safepoint, just before going back to compiled code,
3578   // we inform the GC that we will be doing initializing writes to
3579   // this object in the future without emitting card-marks, so
3580   // GC may take any compensating steps.
3581 
3582   oop new_obj = thread-&gt;vm_result();
3583   if (new_obj == NULL) return;
3584 
3585   BarrierSet *bs = BarrierSet::barrier_set();
3586   bs-&gt;on_slowpath_allocation_exit(thread, new_obj);
3587 }
<span class="line-added">3588 </span>
<span class="line-added">3589 // We are at a compiled code to interpreter call. We need backing</span>
<span class="line-added">3590 // buffers for all value type arguments. Allocate an object array to</span>
<span class="line-added">3591 // hold them (convenient because once we&#39;re done with it we don&#39;t have</span>
<span class="line-added">3592 // to worry about freeing it).</span>
<span class="line-added">3593 oop SharedRuntime::allocate_value_types_impl(JavaThread* thread, methodHandle callee, bool allocate_receiver, TRAPS) {</span>
<span class="line-added">3594   assert(InlineTypePassFieldsAsArgs, &quot;no reason to call this&quot;);</span>
<span class="line-added">3595   ResourceMark rm;</span>
<span class="line-added">3596 </span>
<span class="line-added">3597   int nb_slots = 0;</span>
<span class="line-added">3598   InstanceKlass* holder = callee-&gt;method_holder();</span>
<span class="line-added">3599   allocate_receiver &amp;= !callee-&gt;is_static() &amp;&amp; holder-&gt;is_value();</span>
<span class="line-added">3600   if (allocate_receiver) {</span>
<span class="line-added">3601     nb_slots++;</span>
<span class="line-added">3602   }</span>
<span class="line-added">3603   for (SignatureStream ss(callee-&gt;signature()); !ss.at_return_type(); ss.next()) {</span>
<span class="line-added">3604     if (ss.type() == T_VALUETYPE) {</span>
<span class="line-added">3605       nb_slots++;</span>
<span class="line-added">3606     }</span>
<span class="line-added">3607   }</span>
<span class="line-added">3608   objArrayOop array_oop = oopFactory::new_objectArray(nb_slots, CHECK_NULL);</span>
<span class="line-added">3609   objArrayHandle array(THREAD, array_oop);</span>
<span class="line-added">3610   int i = 0;</span>
<span class="line-added">3611   if (allocate_receiver) {</span>
<span class="line-added">3612     ValueKlass* vk = ValueKlass::cast(holder);</span>
<span class="line-added">3613     oop res = vk-&gt;allocate_instance(CHECK_NULL);</span>
<span class="line-added">3614     array-&gt;obj_at_put(i, res);</span>
<span class="line-added">3615     i++;</span>
<span class="line-added">3616   }</span>
<span class="line-added">3617   for (SignatureStream ss(callee-&gt;signature()); !ss.at_return_type(); ss.next()) {</span>
<span class="line-added">3618     if (ss.type() == T_VALUETYPE) {</span>
<span class="line-added">3619       ValueKlass* vk = ss.as_value_klass(holder);</span>
<span class="line-added">3620       oop res = vk-&gt;allocate_instance(CHECK_NULL);</span>
<span class="line-added">3621       array-&gt;obj_at_put(i, res);</span>
<span class="line-added">3622       i++;</span>
<span class="line-added">3623     }</span>
<span class="line-added">3624   }</span>
<span class="line-added">3625   return array();</span>
<span class="line-added">3626 }</span>
<span class="line-added">3627 </span>
<span class="line-added">3628 JRT_ENTRY(void, SharedRuntime::allocate_value_types(JavaThread* thread, Method* callee_method, bool allocate_receiver))</span>
<span class="line-added">3629   methodHandle callee(thread, callee_method);</span>
<span class="line-added">3630   oop array = SharedRuntime::allocate_value_types_impl(thread, callee, allocate_receiver, CHECK);</span>
<span class="line-added">3631   thread-&gt;set_vm_result(array);</span>
<span class="line-added">3632   thread-&gt;set_vm_result_2(callee()); // TODO: required to keep callee live?</span>
<span class="line-added">3633 JRT_END</span>
<span class="line-added">3634 </span>
<span class="line-added">3635 // TODO remove this once the AARCH64 dependency is gone</span>
<span class="line-added">3636 // Iterate over the array of heap allocated value types and apply the GC post barrier to all reference fields.</span>
<span class="line-added">3637 // This is called from the C2I adapter after value type arguments are heap allocated and initialized.</span>
<span class="line-added">3638 JRT_LEAF(void, SharedRuntime::apply_post_barriers(JavaThread* thread, objArrayOopDesc* array))</span>
<span class="line-added">3639 {</span>
<span class="line-added">3640   assert(InlineTypePassFieldsAsArgs, &quot;no reason to call this&quot;);</span>
<span class="line-added">3641   assert(oopDesc::is_oop(array), &quot;should be oop&quot;);</span>
<span class="line-added">3642   for (int i = 0; i &lt; array-&gt;length(); ++i) {</span>
<span class="line-added">3643     instanceOop valueOop = (instanceOop)array-&gt;obj_at(i);</span>
<span class="line-added">3644     ValueKlass* vk = ValueKlass::cast(valueOop-&gt;klass());</span>
<span class="line-added">3645     if (vk-&gt;contains_oops()) {</span>
<span class="line-added">3646       const address dst_oop_addr = ((address) (void*) valueOop);</span>
<span class="line-added">3647       OopMapBlock* map = vk-&gt;start_of_nonstatic_oop_maps();</span>
<span class="line-added">3648       OopMapBlock* const end = map + vk-&gt;nonstatic_oop_map_count();</span>
<span class="line-added">3649       while (map != end) {</span>
<span class="line-added">3650         address doop_address = dst_oop_addr + map-&gt;offset();</span>
<span class="line-added">3651         barrier_set_cast&lt;ModRefBarrierSet&gt;(BarrierSet::barrier_set())-&gt;</span>
<span class="line-added">3652           write_ref_array((HeapWord*) doop_address, map-&gt;count());</span>
<span class="line-added">3653         map++;</span>
<span class="line-added">3654       }</span>
<span class="line-added">3655     }</span>
<span class="line-added">3656   }</span>
<span class="line-added">3657 }</span>
<span class="line-added">3658 JRT_END</span>
<span class="line-added">3659 </span>
<span class="line-added">3660 // We&#39;re returning from an interpreted method: load each field into a</span>
<span class="line-added">3661 // register following the calling convention</span>
<span class="line-added">3662 JRT_LEAF(void, SharedRuntime::load_value_type_fields_in_regs(JavaThread* thread, oopDesc* res))</span>
<span class="line-added">3663 {</span>
<span class="line-added">3664   assert(res-&gt;klass()-&gt;is_value(), &quot;only value types here&quot;);</span>
<span class="line-added">3665   ResourceMark rm;</span>
<span class="line-added">3666   RegisterMap reg_map(thread);</span>
<span class="line-added">3667   frame stubFrame = thread-&gt;last_frame();</span>
<span class="line-added">3668   frame callerFrame = stubFrame.sender(&amp;reg_map);</span>
<span class="line-added">3669   assert(callerFrame.is_interpreted_frame(), &quot;should be coming from interpreter&quot;);</span>
<span class="line-added">3670 </span>
<span class="line-added">3671   ValueKlass* vk = ValueKlass::cast(res-&gt;klass());</span>
<span class="line-added">3672 </span>
<span class="line-added">3673   const Array&lt;SigEntry&gt;* sig_vk = vk-&gt;extended_sig();</span>
<span class="line-added">3674   const Array&lt;VMRegPair&gt;* regs = vk-&gt;return_regs();</span>
<span class="line-added">3675 </span>
<span class="line-added">3676   if (regs == NULL) {</span>
<span class="line-added">3677     // The fields of the value klass don&#39;t fit in registers, bail out</span>
<span class="line-added">3678     return;</span>
<span class="line-added">3679   }</span>
<span class="line-added">3680 </span>
<span class="line-added">3681   int j = 1;</span>
<span class="line-added">3682   for (int i = 0; i &lt; sig_vk-&gt;length(); i++) {</span>
<span class="line-added">3683     BasicType bt = sig_vk-&gt;at(i)._bt;</span>
<span class="line-added">3684     if (bt == T_VALUETYPE) {</span>
<span class="line-added">3685       continue;</span>
<span class="line-added">3686     }</span>
<span class="line-added">3687     if (bt == T_VOID) {</span>
<span class="line-added">3688       if (sig_vk-&gt;at(i-1)._bt == T_LONG ||</span>
<span class="line-added">3689           sig_vk-&gt;at(i-1)._bt == T_DOUBLE) {</span>
<span class="line-added">3690         j++;</span>
<span class="line-added">3691       }</span>
<span class="line-added">3692       continue;</span>
<span class="line-added">3693     }</span>
<span class="line-added">3694     int off = sig_vk-&gt;at(i)._offset;</span>
<span class="line-added">3695     assert(off &gt; 0, &quot;offset in object should be positive&quot;);</span>
<span class="line-added">3696     VMRegPair pair = regs-&gt;at(j);</span>
<span class="line-added">3697     address loc = reg_map.location(pair.first());</span>
<span class="line-added">3698     switch(bt) {</span>
<span class="line-added">3699     case T_BOOLEAN:</span>
<span class="line-added">3700       *(jboolean*)loc = res-&gt;bool_field(off);</span>
<span class="line-added">3701       break;</span>
<span class="line-added">3702     case T_CHAR:</span>
<span class="line-added">3703       *(jchar*)loc = res-&gt;char_field(off);</span>
<span class="line-added">3704       break;</span>
<span class="line-added">3705     case T_BYTE:</span>
<span class="line-added">3706       *(jbyte*)loc = res-&gt;byte_field(off);</span>
<span class="line-added">3707       break;</span>
<span class="line-added">3708     case T_SHORT:</span>
<span class="line-added">3709       *(jshort*)loc = res-&gt;short_field(off);</span>
<span class="line-added">3710       break;</span>
<span class="line-added">3711     case T_INT: {</span>
<span class="line-added">3712       *(jint*)loc = res-&gt;int_field(off);</span>
<span class="line-added">3713       break;</span>
<span class="line-added">3714     }</span>
<span class="line-added">3715     case T_LONG:</span>
<span class="line-added">3716 #ifdef _LP64</span>
<span class="line-added">3717       *(intptr_t*)loc = res-&gt;long_field(off);</span>
<span class="line-added">3718 #else</span>
<span class="line-added">3719       Unimplemented();</span>
<span class="line-added">3720 #endif</span>
<span class="line-added">3721       break;</span>
<span class="line-added">3722     case T_OBJECT:</span>
<span class="line-added">3723     case T_ARRAY: {</span>
<span class="line-added">3724       *(oop*)loc = res-&gt;obj_field(off);</span>
<span class="line-added">3725       break;</span>
<span class="line-added">3726     }</span>
<span class="line-added">3727     case T_FLOAT:</span>
<span class="line-added">3728       *(jfloat*)loc = res-&gt;float_field(off);</span>
<span class="line-added">3729       break;</span>
<span class="line-added">3730     case T_DOUBLE:</span>
<span class="line-added">3731       *(jdouble*)loc = res-&gt;double_field(off);</span>
<span class="line-added">3732       break;</span>
<span class="line-added">3733     default:</span>
<span class="line-added">3734       ShouldNotReachHere();</span>
<span class="line-added">3735     }</span>
<span class="line-added">3736     j++;</span>
<span class="line-added">3737   }</span>
<span class="line-added">3738   assert(j == regs-&gt;length(), &quot;missed a field?&quot;);</span>
<span class="line-added">3739 </span>
<span class="line-added">3740 #ifdef ASSERT</span>
<span class="line-added">3741   VMRegPair pair = regs-&gt;at(0);</span>
<span class="line-added">3742   address loc = reg_map.location(pair.first());</span>
<span class="line-added">3743   assert(*(oopDesc**)loc == res, &quot;overwritten object&quot;);</span>
<span class="line-added">3744 #endif</span>
<span class="line-added">3745 </span>
<span class="line-added">3746   thread-&gt;set_vm_result(res);</span>
<span class="line-added">3747 }</span>
<span class="line-added">3748 JRT_END</span>
<span class="line-added">3749 </span>
<span class="line-added">3750 // We&#39;ve returned to an interpreted method, the interpreter needs a</span>
<span class="line-added">3751 // reference to a value type instance. Allocate it and initialize it</span>
<span class="line-added">3752 // from field&#39;s values in registers.</span>
<span class="line-added">3753 JRT_BLOCK_ENTRY(void, SharedRuntime::store_value_type_fields_to_buf(JavaThread* thread, intptr_t res))</span>
<span class="line-added">3754 {</span>
<span class="line-added">3755   ResourceMark rm;</span>
<span class="line-added">3756   RegisterMap reg_map(thread);</span>
<span class="line-added">3757   frame stubFrame = thread-&gt;last_frame();</span>
<span class="line-added">3758   frame callerFrame = stubFrame.sender(&amp;reg_map);</span>
<span class="line-added">3759 </span>
<span class="line-added">3760 #ifdef ASSERT</span>
<span class="line-added">3761   ValueKlass* verif_vk = ValueKlass::returned_value_klass(reg_map);</span>
<span class="line-added">3762 #endif</span>
<span class="line-added">3763 </span>
<span class="line-added">3764   if (!is_set_nth_bit(res, 0)) {</span>
<span class="line-added">3765     // We&#39;re not returning with value type fields in registers (the</span>
<span class="line-added">3766     // calling convention didn&#39;t allow it for this value klass)</span>
<span class="line-added">3767     assert(!Metaspace::contains((void*)res), &quot;should be oop or pointer in buffer area&quot;);</span>
<span class="line-added">3768     thread-&gt;set_vm_result((oopDesc*)res);</span>
<span class="line-added">3769     assert(verif_vk == NULL, &quot;broken calling convention&quot;);</span>
<span class="line-added">3770     return;</span>
<span class="line-added">3771   }</span>
<span class="line-added">3772 </span>
<span class="line-added">3773   clear_nth_bit(res, 0);</span>
<span class="line-added">3774   ValueKlass* vk = (ValueKlass*)res;</span>
<span class="line-added">3775   assert(verif_vk == vk, &quot;broken calling convention&quot;);</span>
<span class="line-added">3776   assert(Metaspace::contains((void*)res), &quot;should be klass&quot;);</span>
<span class="line-added">3777 </span>
<span class="line-added">3778   // Allocate handles for every oop field so they are safe in case of</span>
<span class="line-added">3779   // a safepoint when allocating</span>
<span class="line-added">3780   GrowableArray&lt;Handle&gt; handles;</span>
<span class="line-added">3781   vk-&gt;save_oop_fields(reg_map, handles);</span>
<span class="line-added">3782 </span>
<span class="line-added">3783   // It&#39;s unsafe to safepoint until we are here</span>
<span class="line-added">3784   JRT_BLOCK;</span>
<span class="line-added">3785   {</span>
<span class="line-added">3786     Thread* THREAD = thread;</span>
<span class="line-added">3787     oop vt = vk-&gt;realloc_result(reg_map, handles, CHECK);</span>
<span class="line-added">3788     thread-&gt;set_vm_result(vt);</span>
<span class="line-added">3789   }</span>
<span class="line-added">3790   JRT_BLOCK_END;</span>
<span class="line-added">3791 }</span>
<span class="line-added">3792 JRT_END</span>
<span class="line-added">3793 </span>
</pre>
</td>
</tr>
</table>
<center><a href="safepoint.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="synchronizer.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>