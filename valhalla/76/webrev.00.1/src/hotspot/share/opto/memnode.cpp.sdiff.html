<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/memnode.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="macro.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stringopts.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/memnode.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  28 #include &quot;gc/shared/barrierSet.hpp&quot;
  29 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  30 #include &quot;memory/allocation.inline.hpp&quot;
  31 #include &quot;memory/resourceArea.hpp&quot;
  32 #include &quot;oops/objArrayKlass.hpp&quot;
  33 #include &quot;opto/addnode.hpp&quot;
  34 #include &quot;opto/arraycopynode.hpp&quot;
  35 #include &quot;opto/cfgnode.hpp&quot;
  36 #include &quot;opto/compile.hpp&quot;
  37 #include &quot;opto/connode.hpp&quot;
  38 #include &quot;opto/convertnode.hpp&quot;
  39 #include &quot;opto/loopnode.hpp&quot;
  40 #include &quot;opto/machnode.hpp&quot;
  41 #include &quot;opto/matcher.hpp&quot;
  42 #include &quot;opto/memnode.hpp&quot;
  43 #include &quot;opto/mulnode.hpp&quot;
  44 #include &quot;opto/narrowptrnode.hpp&quot;
  45 #include &quot;opto/phaseX.hpp&quot;
  46 #include &quot;opto/regmask.hpp&quot;
  47 #include &quot;opto/rootnode.hpp&quot;

  48 #include &quot;utilities/align.hpp&quot;
  49 #include &quot;utilities/copy.hpp&quot;
  50 #include &quot;utilities/macros.hpp&quot;
  51 #include &quot;utilities/powerOfTwo.hpp&quot;
  52 #include &quot;utilities/vmError.hpp&quot;
  53 
  54 // Portions of code courtesy of Clifford Click
  55 
  56 // Optimization - Graph Style
  57 
  58 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st);
  59 
  60 //=============================================================================
  61 uint MemNode::size_of() const { return sizeof(*this); }
  62 
  63 const TypePtr *MemNode::adr_type() const {
  64   Node* adr = in(Address);
  65   if (adr == NULL)  return NULL; // node is dead
  66   const TypePtr* cross_check = NULL;
  67   DEBUG_ONLY(cross_check = _adr_type);
</pre>
<hr />
<pre>
 220       // clone the Phi with our address type
 221       result = mphi-&gt;split_out_instance(t_adr, igvn);
 222     } else {
 223       assert(phase-&gt;C-&gt;get_alias_index(t) == phase-&gt;C-&gt;get_alias_index(t_adr), &quot;correct memory chain&quot;);
 224     }
 225   }
 226   return result;
 227 }
 228 
 229 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st) {
 230   uint alias_idx = phase-&gt;C-&gt;get_alias_index(tp);
 231   Node *mem = mmem;
 232 #ifdef ASSERT
 233   {
 234     // Check that current type is consistent with the alias index used during graph construction
 235     assert(alias_idx &gt;= Compile::AliasIdxRaw, &quot;must not be a bad alias_idx&quot;);
 236     bool consistent =  adr_check == NULL || adr_check-&gt;empty() ||
 237                        phase-&gt;C-&gt;must_alias(adr_check, alias_idx );
 238     // Sometimes dead array references collapse to a[-1], a[-2], or a[-3]
 239     if( !consistent &amp;&amp; adr_check != NULL &amp;&amp; !adr_check-&gt;empty() &amp;&amp;
<span class="line-modified"> 240                tp-&gt;isa_aryptr() &amp;&amp;        tp-&gt;offset() == Type::OffsetBot &amp;&amp;</span>
 241         adr_check-&gt;isa_aryptr() &amp;&amp; adr_check-&gt;offset() != Type::OffsetBot &amp;&amp;
 242         ( adr_check-&gt;offset() == arrayOopDesc::length_offset_in_bytes() ||
 243           adr_check-&gt;offset() == oopDesc::klass_offset_in_bytes() ||
 244           adr_check-&gt;offset() == oopDesc::mark_offset_in_bytes() ) ) {
 245       // don&#39;t assert if it is dead code.
 246       consistent = true;
 247     }
 248     if( !consistent ) {
 249       st-&gt;print(&quot;alias_idx==%d, adr_check==&quot;, alias_idx);
 250       if( adr_check == NULL ) {
 251         st-&gt;print(&quot;NULL&quot;);
 252       } else {
 253         adr_check-&gt;dump();
 254       }
 255       st-&gt;cr();
 256       print_alias_types();
 257       assert(consistent, &quot;adr_check must match alias idx&quot;);
 258     }
 259   }
 260 #endif
</pre>
<hr />
<pre>
 815          &quot;use LoadKlassNode instead&quot;);
 816   assert(!(adr_type-&gt;isa_aryptr() &amp;&amp;
 817            adr_type-&gt;offset() == arrayOopDesc::length_offset_in_bytes()),
 818          &quot;use LoadRangeNode instead&quot;);
 819   // Check control edge of raw loads
 820   assert( ctl != NULL || C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
 821           // oop will be recorded in oop map if load crosses safepoint
 822           rt-&gt;isa_oopptr() || is_immutable_value(adr),
 823           &quot;raw memory operations should have control edge&quot;);
 824   LoadNode* load = NULL;
 825   switch (bt) {
 826   case T_BOOLEAN: load = new LoadUBNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 827   case T_BYTE:    load = new LoadBNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 828   case T_INT:     load = new LoadINode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 829   case T_CHAR:    load = new LoadUSNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 830   case T_SHORT:   load = new LoadSNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 831   case T_LONG:    load = new LoadLNode (ctl, mem, adr, adr_type, rt-&gt;is_long(), mo, control_dependency); break;
 832   case T_FLOAT:   load = new LoadFNode (ctl, mem, adr, adr_type, rt,            mo, control_dependency); break;
 833   case T_DOUBLE:  load = new LoadDNode (ctl, mem, adr, adr_type, rt,            mo, control_dependency); break;
 834   case T_ADDRESS: load = new LoadPNode (ctl, mem, adr, adr_type, rt-&gt;is_ptr(),  mo, control_dependency); break;

 835   case T_OBJECT:
 836 #ifdef _LP64
 837     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
 838       load = new LoadNNode(ctl, mem, adr, adr_type, rt-&gt;make_narrowoop(), mo, control_dependency);
 839     } else
 840 #endif
 841     {
 842       assert(!adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop() &amp;&amp; !adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass(), &quot;should have got back a narrow oop&quot;);
 843       load = new LoadPNode(ctl, mem, adr, adr_type, rt-&gt;is_ptr(), mo, control_dependency);
 844     }
 845     break;
 846   default:
 847     ShouldNotReachHere();
 848     break;
 849   }
 850   assert(load != NULL, &quot;LoadNode should have been created&quot;);
 851   if (unaligned) {
 852     load-&gt;set_unaligned_access();
 853   }
 854   if (mismatched) {
</pre>
<hr />
<pre>
 945     if (ac-&gt;as_ArrayCopy()-&gt;is_clonebasic()) {
 946       assert(ld_alloc != NULL, &quot;need an alloc&quot;);
 947       assert(addp-&gt;is_AddP(), &quot;address must be addp&quot;);
 948       BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 949       assert(bs-&gt;step_over_gc_barrier(addp-&gt;in(AddPNode::Base)) == bs-&gt;step_over_gc_barrier(ac-&gt;in(ArrayCopyNode::Dest)), &quot;strange pattern&quot;);
 950       assert(bs-&gt;step_over_gc_barrier(addp-&gt;in(AddPNode::Address)) == bs-&gt;step_over_gc_barrier(ac-&gt;in(ArrayCopyNode::Dest)), &quot;strange pattern&quot;);
 951       addp-&gt;set_req(AddPNode::Base, src);
 952       addp-&gt;set_req(AddPNode::Address, src);
 953     } else {
 954       assert(ac-&gt;as_ArrayCopy()-&gt;is_arraycopy_validated() ||
 955              ac-&gt;as_ArrayCopy()-&gt;is_copyof_validated() ||
 956              ac-&gt;as_ArrayCopy()-&gt;is_copyofrange_validated(), &quot;only supported cases&quot;);
 957       assert(addp-&gt;in(AddPNode::Base) == addp-&gt;in(AddPNode::Address), &quot;should be&quot;);
 958       addp-&gt;set_req(AddPNode::Base, src);
 959       addp-&gt;set_req(AddPNode::Address, src);
 960 
 961       const TypeAryPtr* ary_t = phase-&gt;type(in(MemNode::Address))-&gt;isa_aryptr();
 962       BasicType ary_elem  = ary_t-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
 963       uint header = arrayOopDesc::base_offset_in_bytes(ary_elem);
 964       uint shift  = exact_log2(type2aelembytes(ary_elem));




 965 
 966       Node* diff = phase-&gt;transform(new SubINode(ac-&gt;in(ArrayCopyNode::SrcPos), ac-&gt;in(ArrayCopyNode::DestPos)));
 967 #ifdef _LP64
 968       diff = phase-&gt;transform(new ConvI2LNode(diff));
 969 #endif
 970       diff = phase-&gt;transform(new LShiftXNode(diff, phase-&gt;intcon(shift)));
 971 
 972       Node* offset = phase-&gt;transform(new AddXNode(addp-&gt;in(AddPNode::Offset), diff));
 973       addp-&gt;set_req(AddPNode::Offset, offset);
 974     }
 975     addp = phase-&gt;transform(addp);
 976 #ifdef ASSERT
 977     const TypePtr* adr_type = phase-&gt;type(addp)-&gt;is_ptr();
 978     ld-&gt;_adr_type = adr_type;
 979 #endif
 980     ld-&gt;set_req(MemNode::Address, addp);
 981     ld-&gt;set_req(0, ctl);
 982     ld-&gt;set_req(MemNode::Memory, mem);
 983     // load depends on the tests that validate the arraycopy
 984     ld-&gt;_control_dependency = UnknownControl;
</pre>
<hr />
<pre>
1072         // the same pointer-and-offset that we stored to.
1073         // Casted version may carry a dependency and it is respected.
1074         // Thus, we are able to replace L by V.
1075       }
1076       // Now prove that we have a LoadQ matched to a StoreQ, for some Q.
1077       if (store_Opcode() != st-&gt;Opcode())
1078         return NULL;
1079       return st-&gt;in(MemNode::ValueIn);
1080     }
1081 
1082     // A load from a freshly-created object always returns zero.
1083     // (This can happen after LoadNode::Ideal resets the load&#39;s memory input
1084     // to find_captured_store, which returned InitializeNode::zero_memory.)
1085     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
1086         (st-&gt;in(0) == ld_alloc) &amp;&amp;
1087         (ld_off &gt;= st-&gt;in(0)-&gt;as_Allocate()-&gt;minimum_header_size())) {
1088       // return a zero value for the load&#39;s basic type
1089       // (This is one of the few places where a generic PhaseTransform
1090       // can create new nodes.  Think of it as lazily manifesting
1091       // virtually pre-existing constants.)






1092       return phase-&gt;zerocon(memory_type());
1093     }
1094 
1095     // A load from an initialization barrier can match a captured store.
1096     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Initialize()) {
1097       InitializeNode* init = st-&gt;in(0)-&gt;as_Initialize();
1098       AllocateNode* alloc = init-&gt;allocation();
1099       if ((alloc != NULL) &amp;&amp; (alloc == ld_alloc)) {
1100         // examine a captured store value
1101         st = init-&gt;find_captured_store(ld_off, memory_size(), phase);
1102         if (st != NULL) {
1103           continue;             // take one more trip around
1104         }
1105       }
1106     }
1107 
1108     // Load boxed value from result of valueOf() call is input parameter.
1109     if (this-&gt;is_Load() &amp;&amp; ld_adr-&gt;is_AddP() &amp;&amp;
1110         (tp != NULL) &amp;&amp; tp-&gt;is_ptr_to_boxed_value()) {
1111       intptr_t ignore = 0;
</pre>
<hr />
<pre>
1129 //----------------------is_instance_field_load_with_local_phi------------------
1130 bool LoadNode::is_instance_field_load_with_local_phi(Node* ctrl) {
1131   if( in(Memory)-&gt;is_Phi() &amp;&amp; in(Memory)-&gt;in(0) == ctrl &amp;&amp;
1132       in(Address)-&gt;is_AddP() ) {
1133     const TypeOopPtr* t_oop = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1134     // Only instances and boxed values.
1135     if( t_oop != NULL &amp;&amp;
1136         (t_oop-&gt;is_ptr_to_boxed_value() ||
1137          t_oop-&gt;is_known_instance_field()) &amp;&amp;
1138         t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
1139         t_oop-&gt;offset() != Type::OffsetTop) {
1140       return true;
1141     }
1142   }
1143   return false;
1144 }
1145 
1146 //------------------------------Identity---------------------------------------
1147 // Loads are identity if previous store is to same address
1148 Node* LoadNode::Identity(PhaseGVN* phase) {



























1149   // If the previous store-maker is the right kind of Store, and the store is
1150   // to the same address, then we are equal to the value stored.
1151   Node* mem = in(Memory);
1152   Node* value = can_see_stored_value(mem, phase);
1153   if( value ) {
1154     // byte, short &amp; char stores truncate naturally.
1155     // A load has to load the truncated value which requires
1156     // some sort of masking operation and that requires an
1157     // Ideal call instead of an Identity call.
1158     if (memory_size() &lt; BytesPerInt) {
1159       // If the input to the store does not fit with the load&#39;s result type,
1160       // it must be truncated via an Ideal call.
1161       if (!phase-&gt;type(value)-&gt;higher_equal(phase-&gt;type(this)))
1162         return this;
1163     }
1164     // (This works even when value is a Con, but LoadNode::Value
1165     // usually runs first, producing the singleton type of the Con.)
1166     return value;
1167   }
1168 
</pre>
<hr />
<pre>
1686   // fold up, do so.
1687   Node* prev_mem = find_previous_store(phase);
1688   if (prev_mem != NULL) {
1689     Node* value = can_see_arraycopy_value(prev_mem, phase);
1690     if (value != NULL) {
1691       return value;
1692     }
1693   }
1694   // Steps (a), (b):  Walk past independent stores to find an exact match.
1695   if (prev_mem != NULL &amp;&amp; prev_mem != in(MemNode::Memory)) {
1696     // (c) See if we can fold up on the spot, but don&#39;t fold up here.
1697     // Fold-up might require truncation (for LoadB/LoadS/LoadUS) or
1698     // just return a prior value, which is done by Identity calls.
1699     if (can_see_stored_value(prev_mem, phase)) {
1700       // Make ready for step (d):
1701       set_req(MemNode::Memory, prev_mem);
1702       return this;
1703     }
1704   }
1705 
<span class="line-modified">1706   AllocateNode* alloc = is_new_object_mark_load(phase);</span>
<span class="line-modified">1707   if (alloc != NULL &amp;&amp; alloc-&gt;Opcode() == Op_Allocate &amp;&amp; UseBiasedLocking) {</span>




1708     InitializeNode* init = alloc-&gt;initialization();
1709     Node* control = init-&gt;proj_out(0);
<span class="line-modified">1710     return alloc-&gt;make_ideal_mark(phase, address, control, mem);</span>
1711   }
1712 
1713   return progress ? this : NULL;
1714 }
1715 
1716 // Helper to recognize certain Klass fields which are invariant across
1717 // some group of array types (e.g., int[] or all T[] where T &lt; Object).
1718 const Type*
1719 LoadNode::load_array_final_field(const TypeKlassPtr *tkls,
1720                                  ciKlass* klass) const {
1721   if (tkls-&gt;offset() == in_bytes(Klass::modifier_flags_offset())) {
1722     // The field is Klass::_modifier_flags.  Return its (constant) value.
1723     // (Folds up the 2nd indirection in aClassConstant.getModifiers().)
1724     assert(this-&gt;Opcode() == Op_LoadI, &quot;must load an int from _modifier_flags&quot;);
1725     return TypeInt::make(klass-&gt;modifier_flags());
1726   }
1727   if (tkls-&gt;offset() == in_bytes(Klass::access_flags_offset())) {
1728     // The field is Klass::_access_flags.  Return its (constant) value.
1729     // (Folds up the 2nd indirection in Reflection.getClassAccessFlags(aClassConstant).)
1730     assert(this-&gt;Opcode() == Op_LoadI, &quot;must load an int from _access_flags&quot;);
</pre>
<hr />
<pre>
1782       }
1783     }
1784 
1785     // Don&#39;t do this for integer types. There is only potential profit if
1786     // the element type t is lower than _type; that is, for int types, if _type is
1787     // more restrictive than t.  This only happens here if one is short and the other
1788     // char (both 16 bits), and in those cases we&#39;ve made an intentional decision
1789     // to use one kind of load over the other. See AndINode::Ideal and 4965907.
1790     // Also, do not try to narrow the type for a LoadKlass, regardless of offset.
1791     //
1792     // Yes, it is possible to encounter an expression like (LoadKlass p1:(AddP x x 8))
1793     // where the _gvn.type of the AddP is wider than 8.  This occurs when an earlier
1794     // copy p0 of (AddP x x 8) has been proven equal to p1, and the p0 has been
1795     // subsumed by p1.  If p1 is on the worklist but has not yet been re-transformed,
1796     // it is possible that p1 will have a type like Foo*[int+]:NotNull*+any.
1797     // In fact, that could have been the original type of p1, and p1 could have
1798     // had an original form like p1:(AddP x x (LShiftL quux 3)), where the
1799     // expression (LShiftL quux 3) independently optimized to the constant 8.
1800     if ((t-&gt;isa_int() == NULL) &amp;&amp; (t-&gt;isa_long() == NULL)
1801         &amp;&amp; (_type-&gt;isa_vect() == NULL)

1802         &amp;&amp; Opcode() != Op_LoadKlass &amp;&amp; Opcode() != Op_LoadNKlass) {
1803       // t might actually be lower than _type, if _type is a unique
1804       // concrete subclass of abstract class t.
1805       if (off_beyond_header || off == Type::OffsetBot) {  // is the offset beyond the header?
1806         const Type* jt = t-&gt;join_speculative(_type);
1807         // In any case, do not allow the join, per se, to empty out the type.
1808         if (jt-&gt;empty() &amp;&amp; !t-&gt;empty()) {
1809           // This can happen if a interface-typed array narrows to a class type.
1810           jt = _type;
1811         }
1812 #ifdef ASSERT
1813         if (phase-&gt;C-&gt;eliminate_boxing() &amp;&amp; adr-&gt;is_AddP()) {
1814           // The pointers in the autobox arrays are always non-null
1815           Node* base = adr-&gt;in(AddPNode::Base);
1816           if ((base != NULL) &amp;&amp; base-&gt;is_DecodeN()) {
1817             // Get LoadN node which loads IntegerCache.cache field
1818             base = base-&gt;in(1);
1819           }
1820           if ((base != NULL) &amp;&amp; base-&gt;is_Con()) {
1821             const TypeAryPtr* base_type = base-&gt;bottom_type()-&gt;isa_aryptr();
1822             if ((base_type != NULL) &amp;&amp; base_type-&gt;is_autobox_cache()) {
1823               // It could be narrow oop
1824               assert(jt-&gt;make_ptr()-&gt;ptr() == TypePtr::NotNull,&quot;sanity&quot;);
1825             }
1826           }
1827         }
1828 #endif
1829         return jt;
1830       }
1831     }
1832   } else if (tp-&gt;base() == Type::InstPtr) {
1833     assert( off != Type::OffsetBot ||
1834             // arrays can be cast to Objects
1835             tp-&gt;is_oopptr()-&gt;klass()-&gt;is_java_lang_Object() ||

1836             // unsafe field access may not have a constant offset
1837             C-&gt;has_unsafe_access(),
1838             &quot;Field accesses must be precise&quot; );
1839     // For oop loads, we expect the _type to be precise.
1840 
<span class="line-modified">1841     // Optimize loads from constant fields.</span>


1842     const TypeInstPtr* tinst = tp-&gt;is_instptr();
1843     ciObject* const_oop = tinst-&gt;const_oop();
1844     if (!is_mismatched_access() &amp;&amp; off != Type::OffsetBot &amp;&amp; const_oop != NULL &amp;&amp; const_oop-&gt;is_instance()) {
<span class="line-modified">1845       const Type* con_type = Type::make_constant_from_field(const_oop-&gt;as_instance(), off, is_unsigned(), memory_type());</span>









1846       if (con_type != NULL) {
1847         return con_type;
1848       }
1849     }
1850   } else if (tp-&gt;base() == Type::KlassPtr) {
1851     assert( off != Type::OffsetBot ||
1852             // arrays can be cast to Objects

1853             tp-&gt;is_klassptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1854             // also allow array-loading from the primary supertype
1855             // array during subtype checks
1856             Opcode() == Op_LoadKlass,
1857             &quot;Field accesses must be precise&quot; );
1858     // For klass/static loads, we expect the _type to be precise
<span class="line-modified">1859   } else if (tp-&gt;base() == Type::RawPtr &amp;&amp; adr-&gt;is_Load() &amp;&amp; off == 0) {</span>
<span class="line-modified">1860     /* With mirrors being an indirect in the Klass*</span>
<span class="line-modified">1861      * the VM is now using two loads. LoadKlass(LoadP(LoadP(Klass, mirror_offset), zero_offset))</span>
<span class="line-modified">1862      * The LoadP from the Klass has a RawPtr type (see LibraryCallKit::load_mirror_from_klass).</span>
<span class="line-modified">1863      *</span>
<span class="line-modified">1864      * So check the type and klass of the node before the LoadP.</span>
<span class="line-modified">1865      */</span>
<span class="line-modified">1866     Node* adr2 = adr-&gt;in(MemNode::Address);</span>
<span class="line-modified">1867     const TypeKlassPtr* tkls = phase-&gt;type(adr2)-&gt;isa_klassptr();</span>
<span class="line-modified">1868     if (tkls != NULL &amp;&amp; !StressReflectiveCode) {</span>
<span class="line-modified">1869       ciKlass* klass = tkls-&gt;klass();</span>
<span class="line-modified">1870       if (klass-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp; tkls-&gt;offset() == in_bytes(Klass::java_mirror_offset())) {</span>
<span class="line-modified">1871         assert(adr-&gt;Opcode() == Op_LoadP, &quot;must load an oop from _java_mirror&quot;);</span>
<span class="line-modified">1872         assert(Opcode() == Op_LoadP, &quot;must load an oop from _java_mirror&quot;);</span>
<span class="line-modified">1873         return TypeInstPtr::make(klass-&gt;java_mirror());</span>
















1874       }
1875     }
1876   }
1877 
1878   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
1879   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
1880     ciKlass* klass = tkls-&gt;klass();
<span class="line-modified">1881     if (klass-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact()) {</span>
1882       // We are loading a field from a Klass metaobject whose identity
1883       // is known at compile time (the type is &quot;exact&quot; or &quot;precise&quot;).
1884       // Check for fields we know are maintained as constants by the VM.
1885       if (tkls-&gt;offset() == in_bytes(Klass::super_check_offset_offset())) {
1886         // The field is Klass::_super_check_offset.  Return its (constant) value.
1887         // (Folds up type checking code.)
1888         assert(Opcode() == Op_LoadI, &quot;must load an int from _super_check_offset&quot;);
1889         return TypeInt::make(klass-&gt;super_check_offset());
1890       }
1891       // Compute index into primary_supers array
1892       juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1893       // Check for overflowing; use unsigned compare to handle the negative case.
1894       if( depth &lt; ciKlass::primary_super_limit() ) {
1895         // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1896         // (Folds up type checking code.)
1897         assert(Opcode() == Op_LoadKlass, &quot;must load a klass from _primary_supers&quot;);
1898         ciKlass *ss = klass-&gt;super_of_depth(depth);
1899         return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1900       }
1901       const Type* aift = load_array_final_field(tkls, klass);
1902       if (aift != NULL)  return aift;
1903     }
1904 
1905     // We can still check if we are loading from the primary_supers array at a
1906     // shallow enough depth.  Even though the klass is not exact, entries less
1907     // than or equal to its super depth are correct.
<span class="line-modified">1908     if (klass-&gt;is_loaded() ) {</span>
1909       ciType *inner = klass;
1910       while( inner-&gt;is_obj_array_klass() )
1911         inner = inner-&gt;as_obj_array_klass()-&gt;base_element_type();
1912       if( inner-&gt;is_instance_klass() &amp;&amp;
1913           !inner-&gt;as_instance_klass()-&gt;flags().is_interface() ) {
1914         // Compute index into primary_supers array
1915         juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1916         // Check for overflowing; use unsigned compare to handle the negative case.
1917         if( depth &lt; ciKlass::primary_super_limit() &amp;&amp;
1918             depth &lt;= klass-&gt;super_depth() ) { // allow self-depth checks to handle self-check case
1919           // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1920           // (Folds up type checking code.)
1921           assert(Opcode() == Op_LoadKlass, &quot;must load a klass from _primary_supers&quot;);
1922           ciKlass *ss = klass-&gt;super_of_depth(depth);
1923           return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1924         }
1925       }
1926     }
1927 
1928     // If the type is enough to determine that the thing is not an array,
</pre>
<hr />
<pre>
2093   return LoadNode::Ideal(phase, can_reshape);
2094 }
2095 
2096 const Type* LoadSNode::Value(PhaseGVN* phase) const {
2097   Node* mem = in(MemNode::Memory);
2098   Node* value = can_see_stored_value(mem,phase);
2099   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
2100       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
2101     // If the input to the store does not fit with the load&#39;s result type,
2102     // it must be truncated. We can&#39;t delay until Ideal call since
2103     // a singleton Value is needed for split_thru_phi optimization.
2104     int con = value-&gt;get_int();
2105     return TypeInt::make((con &lt;&lt; 16) &gt;&gt; 16);
2106   }
2107   return LoadNode::Value(phase);
2108 }
2109 
2110 //=============================================================================
2111 //----------------------------LoadKlassNode::make------------------------------
2112 // Polymorphic factory method:
<span class="line-modified">2113 Node* LoadKlassNode::make(PhaseGVN&amp; gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at, const TypeKlassPtr* tk) {</span>

2114   // sanity check the alias category against the created node type
2115   const TypePtr *adr_type = adr-&gt;bottom_type()-&gt;isa_ptr();
2116   assert(adr_type != NULL, &quot;expecting TypeKlassPtr&quot;);
2117 #ifdef _LP64
2118   if (adr_type-&gt;is_ptr_to_narrowklass()) {
2119     assert(UseCompressedClassPointers, &quot;no compressed klasses&quot;);
2120     Node* load_klass = gvn.transform(new LoadNKlassNode(ctl, mem, adr, at, tk-&gt;make_narrowklass(), MemNode::unordered));
2121     return new DecodeNKlassNode(load_klass, load_klass-&gt;bottom_type()-&gt;make_ptr());
2122   }
2123 #endif
2124   assert(!adr_type-&gt;is_ptr_to_narrowklass() &amp;&amp; !adr_type-&gt;is_ptr_to_narrowoop(), &quot;should have got back a narrow oop&quot;);
2125   return new LoadKlassNode(ctl, mem, adr, at, tk, MemNode::unordered);
2126 }
2127 
2128 //------------------------------Value------------------------------------------
2129 const Type* LoadKlassNode::Value(PhaseGVN* phase) const {
2130   return klass_value_common(phase);
2131 }
2132 
2133 // In most cases, LoadKlassNode does not have the control input set. If the control
</pre>
<hr />
<pre>
2181     if( !ik-&gt;is_loaded() )
2182       return _type;             // Bail out if not loaded
2183     if (offset == oopDesc::klass_offset_in_bytes()) {
2184       if (tinst-&gt;klass_is_exact()) {
2185         return TypeKlassPtr::make(ik);
2186       }
2187       // See if we can become precise: no subklasses and no interface
2188       // (Note:  We need to support verified interfaces.)
2189       if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2190         //assert(!UseExactTypes, &quot;this code should be useless with exact types&quot;);
2191         // Add a dependence; if any subclass added we need to recompile
2192         if (!ik-&gt;is_final()) {
2193           // %%% should use stronger assert_unique_concrete_subtype instead
2194           phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2195         }
2196         // Return precise klass
2197         return TypeKlassPtr::make(ik);
2198       }
2199 
2200       // Return root of possible klass
<span class="line-modified">2201       return TypeKlassPtr::make(TypePtr::NotNull, ik, 0/*offset*/);</span>
2202     }
2203   }
2204 
2205   // Check for loading klass from an array
2206   const TypeAryPtr *tary = tp-&gt;isa_aryptr();
<span class="line-modified">2207   if( tary != NULL ) {</span>
2208     ciKlass *tary_klass = tary-&gt;klass();
2209     if (tary_klass != NULL   // can be NULL when at BOTTOM or TOP
2210         &amp;&amp; tary-&gt;offset() == oopDesc::klass_offset_in_bytes()) {
2211       if (tary-&gt;klass_is_exact()) {
2212         return TypeKlassPtr::make(tary_klass);
2213       }
<span class="line-modified">2214       ciArrayKlass *ak = tary-&gt;klass()-&gt;as_array_klass();</span>
2215       // If the klass is an object array, we defer the question to the
2216       // array component klass.
<span class="line-modified">2217       if( ak-&gt;is_obj_array_klass() ) {</span>
<span class="line-modified">2218         assert( ak-&gt;is_loaded(), &quot;&quot; );</span>
2219         ciKlass *base_k = ak-&gt;as_obj_array_klass()-&gt;base_element_klass();
<span class="line-modified">2220         if( base_k-&gt;is_loaded() &amp;&amp; base_k-&gt;is_instance_klass() ) {</span>
<span class="line-modified">2221           ciInstanceKlass* ik = base_k-&gt;as_instance_klass();</span>
2222           // See if we can become precise: no subklasses and no interface
2223           if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2224             //assert(!UseExactTypes, &quot;this code should be useless with exact types&quot;);
2225             // Add a dependence; if any subclass added we need to recompile
2226             if (!ik-&gt;is_final()) {
2227               phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2228             }
2229             // Return precise array klass
2230             return TypeKlassPtr::make(ak);
2231           }
2232         }
<span class="line-modified">2233         return TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);</span>
<span class="line-modified">2234       } else {                  // Found a type-array?</span>
2235         //assert(!UseExactTypes, &quot;this code should be useless with exact types&quot;);
<span class="line-removed">2236         assert( ak-&gt;is_type_array_klass(), &quot;&quot; );</span>
2237         return TypeKlassPtr::make(ak); // These are always precise
2238       }
2239     }
2240   }
2241 
2242   // Check for loading klass from an array klass
2243   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
2244   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
<span class="line-modified">2245     ciKlass* klass = tkls-&gt;klass();</span>
<span class="line-removed">2246     if( !klass-&gt;is_loaded() )</span>
2247       return _type;             // Bail out if not loaded


2248     if( klass-&gt;is_obj_array_klass() &amp;&amp;
2249         tkls-&gt;offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {
2250       ciKlass* elem = klass-&gt;as_obj_array_klass()-&gt;element_klass();
2251       // // Always returning precise element type is incorrect,
2252       // // e.g., element type could be object and array may contain strings
2253       // return TypeKlassPtr::make(TypePtr::Constant, elem, 0);
2254 
2255       // The array&#39;s TypeKlassPtr was declared &#39;precise&#39; or &#39;not precise&#39;
2256       // according to the element type&#39;s subclassing.
<span class="line-modified">2257       return TypeKlassPtr::make(tkls-&gt;ptr(), elem, 0/*offset*/);</span>




2258     }
2259     if( klass-&gt;is_instance_klass() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp;
2260         tkls-&gt;offset() == in_bytes(Klass::super_offset())) {
2261       ciKlass* sup = klass-&gt;as_instance_klass()-&gt;super();
2262       // The field is Klass::_super.  Return its (constant) value.
2263       // (Folds up the 2nd indirection in aClassConstant.getSuperClass().)
2264       return sup ? TypeKlassPtr::make(sup) : TypePtr::NULL_PTR;
2265     }
2266   }
2267 
2268   // Bailout case
2269   return LoadNode::Value(phase);
2270 }
2271 
2272 //------------------------------Identity---------------------------------------
2273 // To clean up reflective code, simplify k.java_mirror.as_klass to plain k.
2274 // Also feed through the klass in Allocate(...klass...)._klass.
2275 Node* LoadKlassNode::Identity(PhaseGVN* phase) {
2276   return klass_identity_common(phase);
2277 }
</pre>
<hr />
<pre>
2445 //=============================================================================
2446 //---------------------------StoreNode::make-----------------------------------
2447 // Polymorphic factory method:
2448 StoreNode* StoreNode::make(PhaseGVN&amp; gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, BasicType bt, MemOrd mo) {
2449   assert((mo == unordered || mo == release), &quot;unexpected&quot;);
2450   Compile* C = gvn.C;
2451   assert(C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
2452          ctl != NULL, &quot;raw memory operations should have control edge&quot;);
2453 
2454   switch (bt) {
2455   case T_BOOLEAN: val = gvn.transform(new AndINode(val, gvn.intcon(0x1))); // Fall through to T_BYTE case
2456   case T_BYTE:    return new StoreBNode(ctl, mem, adr, adr_type, val, mo);
2457   case T_INT:     return new StoreINode(ctl, mem, adr, adr_type, val, mo);
2458   case T_CHAR:
2459   case T_SHORT:   return new StoreCNode(ctl, mem, adr, adr_type, val, mo);
2460   case T_LONG:    return new StoreLNode(ctl, mem, adr, adr_type, val, mo);
2461   case T_FLOAT:   return new StoreFNode(ctl, mem, adr, adr_type, val, mo);
2462   case T_DOUBLE:  return new StoreDNode(ctl, mem, adr, adr_type, val, mo);
2463   case T_METADATA:
2464   case T_ADDRESS:

2465   case T_OBJECT:
2466 #ifdef _LP64
2467     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2468       val = gvn.transform(new EncodePNode(val, val-&gt;bottom_type()-&gt;make_narrowoop()));
2469       return new StoreNNode(ctl, mem, adr, adr_type, val, mo);
2470     } else if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass() ||
2471                (UseCompressedClassPointers &amp;&amp; val-&gt;bottom_type()-&gt;isa_klassptr() &amp;&amp;
2472                 adr-&gt;bottom_type()-&gt;isa_rawptr())) {
2473       val = gvn.transform(new EncodePKlassNode(val, val-&gt;bottom_type()-&gt;make_narrowklass()));
2474       return new StoreNKlassNode(ctl, mem, adr, adr_type, val, mo);
2475     }
2476 #endif
2477     {
2478       return new StorePNode(ctl, mem, adr, adr_type, val, mo);
2479     }
2480   default:
2481     ShouldNotReachHere();
2482     return (StoreNode*)NULL;
2483   }
2484 }
</pre>
<hr />
<pre>
2505   //return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address) + (uintptr_t)in(ValueIn);
2506 
2507   // Since they are not commoned, do not hash them:
2508   return NO_HASH;
2509 }
2510 
2511 //------------------------------Ideal------------------------------------------
2512 // Change back-to-back Store(, p, x) -&gt; Store(m, p, y) to Store(m, p, x).
2513 // When a store immediately follows a relevant allocation/initialization,
2514 // try to capture it into the initialization, or hoist it above.
2515 Node *StoreNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2516   Node* p = MemNode::Ideal_common(phase, can_reshape);
2517   if (p)  return (p == NodeSentinel) ? NULL : p;
2518 
2519   Node* mem     = in(MemNode::Memory);
2520   Node* address = in(MemNode::Address);
2521   // Back-to-back stores to same address?  Fold em up.  Generally
2522   // unsafe if I have intervening uses...  Also disallowed for StoreCM
2523   // since they must follow each StoreP operation.  Redundant StoreCMs
2524   // are eliminated just before matching in final_graph_reshape.
<span class="line-modified">2525   {</span>
2526     Node* st = mem;
2527     // If Store &#39;st&#39; has more than one use, we cannot fold &#39;st&#39; away.
2528     // For example, &#39;st&#39; might be the final state at a conditional
2529     // return.  Or, &#39;st&#39; might be used by some node which is live at
2530     // the same time &#39;st&#39; is live, which might be unschedulable.  So,
2531     // require exactly ONE user until such time as we clone &#39;mem&#39; for
2532     // each of &#39;mem&#39;s uses (thus making the exactly-1-user-rule hold
2533     // true).
2534     while (st-&gt;is_Store() &amp;&amp; st-&gt;outcnt() == 1 &amp;&amp; st-&gt;Opcode() != Op_StoreCM) {
2535       // Looking at a dead closed cycle of memory?
2536       assert(st != st-&gt;in(MemNode::Memory), &quot;dead loop in StoreNode::Ideal&quot;);
2537       assert(Opcode() == st-&gt;Opcode() ||
2538              st-&gt;Opcode() == Op_StoreVector ||
2539              Opcode() == Op_StoreVector ||
2540              phase-&gt;C-&gt;get_alias_index(adr_type()) == Compile::AliasIdxRaw ||
2541              (Opcode() == Op_StoreL &amp;&amp; st-&gt;Opcode() == Op_StoreI) || // expanded ClearArrayNode
2542              (Opcode() == Op_StoreI &amp;&amp; st-&gt;Opcode() == Op_StoreL) || // initialization by arraycopy

2543              (is_mismatched_access() || st-&gt;as_Store()-&gt;is_mismatched_access()),
2544              &quot;no mismatched stores, except on raw memory: %s %s&quot;, NodeClassNames[Opcode()], NodeClassNames[st-&gt;Opcode()]);
2545 
2546       if (st-&gt;in(MemNode::Address)-&gt;eqv_uncast(address) &amp;&amp;
2547           st-&gt;as_Store()-&gt;memory_size() &lt;= this-&gt;memory_size()) {
2548         Node* use = st-&gt;raw_out(0);
2549         phase-&gt;igvn_rehash_node_delayed(use);
2550         if (can_reshape) {
2551           use-&gt;set_req_X(MemNode::Memory, st-&gt;in(MemNode::Memory), phase-&gt;is_IterGVN());
2552         } else {
2553           // It&#39;s OK to do this in the parser, since DU info is always accurate,
2554           // and the parser always refers to nodes via SafePointNode maps.
2555           use-&gt;set_req(MemNode::Memory, st-&gt;in(MemNode::Memory));
2556         }
2557         return this;
2558       }
2559       st = st-&gt;in(MemNode::Memory);
2560     }
2561   }
2562 
</pre>
<hr />
<pre>
2608   // Load then Store?  Then the Store is useless
2609   if (val-&gt;is_Load() &amp;&amp;
2610       val-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2611       val-&gt;in(MemNode::Memory )-&gt;eqv_uncast(mem) &amp;&amp;
2612       val-&gt;as_Load()-&gt;store_Opcode() == Opcode()) {
2613     result = mem;
2614   }
2615 
2616   // Two stores in a row of the same value?
2617   if (result == this &amp;&amp;
2618       mem-&gt;is_Store() &amp;&amp;
2619       mem-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2620       mem-&gt;in(MemNode::ValueIn)-&gt;eqv_uncast(val) &amp;&amp;
2621       mem-&gt;Opcode() == Opcode()) {
2622     result = mem;
2623   }
2624 
2625   // Store of zero anywhere into a freshly-allocated object?
2626   // Then the store is useless.
2627   // (It must already have been captured by the InitializeNode.)
<span class="line-modified">2628   if (result == this &amp;&amp;</span>
<span class="line-removed">2629       ReduceFieldZeroing &amp;&amp; phase-&gt;type(val)-&gt;is_zero_type()) {</span>
2630     // a newly allocated object is already all-zeroes everywhere
<span class="line-modified">2631     if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Allocate()) {</span>


2632       result = mem;
2633     }
2634 
2635     if (result == this) {
2636       // the store may also apply to zero-bits in an earlier object
2637       Node* prev_mem = find_previous_store(phase);
2638       // Steps (a), (b):  Walk past independent stores to find an exact match.
2639       if (prev_mem != NULL) {
2640         Node* prev_val = can_see_stored_value(prev_mem, phase);
2641         if (prev_val != NULL &amp;&amp; phase-&gt;eqv(prev_val, val)) {
2642           // prev_val and val might differ by a cast; it would be good
2643           // to keep the more informative of the two.
<span class="line-modified">2644           result = mem;</span>








2645         }
2646       }
2647     }
2648   }
2649 
2650   if (result != this &amp;&amp; phase-&gt;is_IterGVN() != NULL) {
2651     MemBarNode* trailing = trailing_membar();
2652     if (trailing != NULL) {
2653 #ifdef ASSERT
2654       const TypeOopPtr* t_oop = phase-&gt;type(in(Address))-&gt;isa_oopptr();
2655       assert(t_oop == NULL || t_oop-&gt;is_known_instance_field(), &quot;only for non escaping objects&quot;);
2656 #endif
2657       PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
2658       trailing-&gt;remove(igvn);
2659     }
2660   }
2661 
2662   return result;
2663 }
2664 
</pre>
<hr />
<pre>
2933 // Clearing a short array is faster with stores
2934 Node *ClearArrayNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2935   // Already know this is a large node, do not try to ideal it
2936   if (!IdealizeClearArrayNode || _is_large) return NULL;
2937 
2938   const int unit = BytesPerLong;
2939   const TypeX* t = phase-&gt;type(in(2))-&gt;isa_intptr_t();
2940   if (!t)  return NULL;
2941   if (!t-&gt;is_con())  return NULL;
2942   intptr_t raw_count = t-&gt;get_con();
2943   intptr_t size = raw_count;
2944   if (!Matcher::init_array_count_is_in_bytes) size *= unit;
2945   // Clearing nothing uses the Identity call.
2946   // Negative clears are possible on dead ClearArrays
2947   // (see jck test stmt114.stmt11402.val).
2948   if (size &lt;= 0 || size % unit != 0)  return NULL;
2949   intptr_t count = size / unit;
2950   // Length too long; communicate this to matchers and assemblers.
2951   // Assemblers are responsible to produce fast hardware clears for it.
2952   if (size &gt; InitArrayShortSize) {
<span class="line-modified">2953     return new ClearArrayNode(in(0), in(1), in(2), in(3), true);</span>
2954   }
2955   Node *mem = in(1);
2956   if( phase-&gt;type(mem)==Type::TOP ) return NULL;
2957   Node *adr = in(3);
2958   const Type* at = phase-&gt;type(adr);
2959   if( at==Type::TOP ) return NULL;
2960   const TypePtr* atp = at-&gt;isa_ptr();
2961   // adjust atp to be the correct array element address type
2962   if (atp == NULL)  atp = TypePtr::BOTTOM;
2963   else              atp = atp-&gt;add_offset(Type::OffsetBot);
2964   // Get base for derived pointer purposes
2965   if( adr-&gt;Opcode() != Op_AddP ) Unimplemented();
2966   Node *base = adr-&gt;in(1);
2967 
<span class="line-modified">2968   Node *zero = phase-&gt;makecon(TypeLong::ZERO);</span>
2969   Node *off  = phase-&gt;MakeConX(BytesPerLong);
<span class="line-modified">2970   mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);</span>
2971   count--;
2972   while( count-- ) {
2973     mem = phase-&gt;transform(mem);
2974     adr = phase-&gt;transform(new AddPNode(base,adr,off));
<span class="line-modified">2975     mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);</span>
2976   }
2977   return mem;
2978 }
2979 
2980 //----------------------------step_through----------------------------------
2981 // Return allocation input memory edge if it is different instance
2982 // or itself if it is the one we are looking for.
2983 bool ClearArrayNode::step_through(Node** np, uint instance_id, PhaseTransform* phase) {
2984   Node* n = *np;
2985   assert(n-&gt;is_ClearArray(), &quot;sanity&quot;);
2986   intptr_t offset;
2987   AllocateNode* alloc = AllocateNode::Ideal_allocation(n-&gt;in(3), phase, offset);
2988   // This method is called only before Allocate nodes are expanded
2989   // during macro nodes expansion. Before that ClearArray nodes are
2990   // only generated in PhaseMacroExpand::generate_arraycopy() (before
2991   // Allocate nodes are expanded) which follows allocations.
2992   assert(alloc != NULL, &quot;should have allocation&quot;);
2993   if (alloc-&gt;_idx == instance_id) {
2994     // Can not bypass initialization of the instance we are looking for.
2995     return false;
2996   }
2997   // Otherwise skip it.
2998   InitializeNode* init = alloc-&gt;initialization();
2999   if (init != NULL)
3000     *np = init-&gt;in(TypeFunc::Memory);
3001   else
3002     *np = alloc-&gt;in(TypeFunc::Memory);
3003   return true;
3004 }
3005 
3006 //----------------------------clear_memory-------------------------------------
3007 // Generate code to initialize object storage to zero.
3008 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,


3009                                    intptr_t start_offset,
3010                                    Node* end_offset,
3011                                    PhaseGVN* phase) {
3012   intptr_t offset = start_offset;
3013 
3014   int unit = BytesPerLong;
3015   if ((offset % unit) != 0) {
3016     Node* adr = new AddPNode(dest, dest, phase-&gt;MakeConX(offset));
3017     adr = phase-&gt;transform(adr);
3018     const TypePtr* atp = TypeRawPtr::BOTTOM;
<span class="line-modified">3019     mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT, MemNode::unordered);</span>






3020     mem = phase-&gt;transform(mem);
3021     offset += BytesPerInt;
3022   }
3023   assert((offset % unit) == 0, &quot;&quot;);
3024 
3025   // Initialize the remaining stuff, if any, with a ClearArray.
<span class="line-modified">3026   return clear_memory(ctl, mem, dest, phase-&gt;MakeConX(offset), end_offset, phase);</span>
3027 }
3028 
3029 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,

3030                                    Node* start_offset,
3031                                    Node* end_offset,
3032                                    PhaseGVN* phase) {
3033   if (start_offset == end_offset) {
3034     // nothing to do
3035     return mem;
3036   }
3037 
3038   int unit = BytesPerLong;
3039   Node* zbase = start_offset;
3040   Node* zend  = end_offset;
3041 
3042   // Scale to the unit required by the CPU:
3043   if (!Matcher::init_array_count_is_in_bytes) {
3044     Node* shift = phase-&gt;intcon(exact_log2(unit));
3045     zbase = phase-&gt;transform(new URShiftXNode(zbase, shift) );
3046     zend  = phase-&gt;transform(new URShiftXNode(zend,  shift) );
3047   }
3048 
3049   // Bulk clear double-words
3050   Node* zsize = phase-&gt;transform(new SubXNode(zend, zbase) );
3051   Node* adr = phase-&gt;transform(new AddPNode(dest, dest, start_offset) );
<span class="line-modified">3052   mem = new ClearArrayNode(ctl, mem, zsize, adr, false);</span>



3053   return phase-&gt;transform(mem);
3054 }
3055 
3056 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,


3057                                    intptr_t start_offset,
3058                                    intptr_t end_offset,
3059                                    PhaseGVN* phase) {
3060   if (start_offset == end_offset) {
3061     // nothing to do
3062     return mem;
3063   }
3064 
3065   assert((end_offset % BytesPerInt) == 0, &quot;odd end offset&quot;);
3066   intptr_t done_offset = end_offset;
3067   if ((done_offset % BytesPerLong) != 0) {
3068     done_offset -= BytesPerInt;
3069   }
3070   if (done_offset &gt; start_offset) {
<span class="line-modified">3071     mem = clear_memory(ctl, mem, dest,</span>
3072                        start_offset, phase-&gt;MakeConX(done_offset), phase);
3073   }
3074   if (done_offset &lt; end_offset) { // emit the final 32-bit store
3075     Node* adr = new AddPNode(dest, dest, phase-&gt;MakeConX(done_offset));
3076     adr = phase-&gt;transform(adr);
3077     const TypePtr* atp = TypeRawPtr::BOTTOM;
<span class="line-modified">3078     mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT, MemNode::unordered);</span>






3079     mem = phase-&gt;transform(mem);
3080     done_offset += BytesPerInt;
3081   }
3082   assert(done_offset == end_offset, &quot;&quot;);
3083   return mem;
3084 }
3085 
3086 //=============================================================================
3087 MemBarNode::MemBarNode(Compile* C, int alias_idx, Node* precedent)
3088   : MultiNode(TypeFunc::Parms + (precedent == NULL? 0: 1)),
3089     _adr_type(C-&gt;get_adr_type(alias_idx)), _kind(Standalone)
3090 #ifdef ASSERT
3091   , _pair_idx(0)
3092 #endif
3093 {
3094   init_class_id(Class_MemBar);
3095   Node* top = C-&gt;top();
3096   init_req(TypeFunc::I_O,top);
3097   init_req(TypeFunc::FramePtr,top);
3098   init_req(TypeFunc::ReturnAdr,top);
</pre>
<hr />
<pre>
3197       PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3198       remove(igvn);
3199       // Must return either the original node (now dead) or a new node
3200       // (Do not return a top here, since that would break the uniqueness of top.)
3201       return new ConINode(TypeInt::ZERO);
3202     }
3203   }
3204   return progress ? this : NULL;
3205 }
3206 
3207 //------------------------------Value------------------------------------------
3208 const Type* MemBarNode::Value(PhaseGVN* phase) const {
3209   if( !in(0) ) return Type::TOP;
3210   if( phase-&gt;type(in(0)) == Type::TOP )
3211     return Type::TOP;
3212   return TypeTuple::MEMBAR;
3213 }
3214 
3215 //------------------------------match------------------------------------------
3216 // Construct projections for memory.
<span class="line-modified">3217 Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {</span>
3218   switch (proj-&gt;_con) {
3219   case TypeFunc::Control:
3220   case TypeFunc::Memory:
3221     return new MachProjNode(this,proj-&gt;_con,RegMask::Empty,MachProjNode::unmatched_proj);
3222   }
3223   ShouldNotReachHere();
3224   return NULL;
3225 }
3226 
3227 void MemBarNode::set_store_pair(MemBarNode* leading, MemBarNode* trailing) {
3228   trailing-&gt;_kind = TrailingStore;
3229   leading-&gt;_kind = LeadingStore;
3230 #ifdef ASSERT
3231   trailing-&gt;_pair_idx = leading-&gt;_idx;
3232   leading-&gt;_pair_idx = leading-&gt;_idx;
3233 #endif
3234 }
3235 
3236 void MemBarNode::set_load_store_pair(MemBarNode* leading, MemBarNode* trailing) {
3237   trailing-&gt;_kind = TrailingLoadStore;
</pre>
<hr />
<pre>
3483   return (req() &gt; RawStores);
3484 }
3485 
3486 void InitializeNode::set_complete(PhaseGVN* phase) {
3487   assert(!is_complete(), &quot;caller responsibility&quot;);
3488   _is_complete = Complete;
3489 
3490   // After this node is complete, it contains a bunch of
3491   // raw-memory initializations.  There is no need for
3492   // it to have anything to do with non-raw memory effects.
3493   // Therefore, tell all non-raw users to re-optimize themselves,
3494   // after skipping the memory effects of this initialization.
3495   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3496   if (igvn)  igvn-&gt;add_users_to_worklist(this);
3497 }
3498 
3499 // convenience function
3500 // return false if the init contains any stores already
3501 bool AllocateNode::maybe_set_complete(PhaseGVN* phase) {
3502   InitializeNode* init = initialization();
<span class="line-modified">3503   if (init == NULL || init-&gt;is_complete())  return false;</span>


3504   init-&gt;remove_extra_zeroes();
3505   // for now, if this allocation has already collected any inits, bail:
3506   if (init-&gt;is_non_zero())  return false;
3507   init-&gt;set_complete(phase);
3508   return true;
3509 }
3510 
3511 void InitializeNode::remove_extra_zeroes() {
3512   if (req() == RawStores)  return;
3513   Node* zmem = zero_memory();
3514   uint fill = RawStores;
3515   for (uint i = fill; i &lt; req(); i++) {
3516     Node* n = in(i);
3517     if (n-&gt;is_top() || n == zmem)  continue;  // skip
3518     if (fill &lt; i)  set_req(fill, n);          // compact
3519     ++fill;
3520   }
3521   // delete any empty spaces created:
3522   while (fill &lt; req()) {
3523     del_req(fill);
</pre>
<hr />
<pre>
4241         //   z&#39;s_done      12  16  16  16    12  16    12
4242         //   z&#39;s_needed    12  16  16  16    16  16    16
4243         //   zsize          0   0   0   0     4   0     4
4244         if (next_full_store &lt; 0) {
4245           // Conservative tack:  Zero to end of current word.
4246           zeroes_needed = align_up(zeroes_needed, BytesPerInt);
4247         } else {
4248           // Zero to beginning of next fully initialized word.
4249           // Or, don&#39;t zero at all, if we are already in that word.
4250           assert(next_full_store &gt;= zeroes_needed, &quot;must go forward&quot;);
4251           assert((next_full_store &amp; (BytesPerInt-1)) == 0, &quot;even boundary&quot;);
4252           zeroes_needed = next_full_store;
4253         }
4254       }
4255 
4256       if (zeroes_needed &gt; zeroes_done) {
4257         intptr_t zsize = zeroes_needed - zeroes_done;
4258         // Do some incremental zeroing on rawmem, in parallel with inits.
4259         zeroes_done = align_down(zeroes_done, BytesPerInt);
4260         rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,


4261                                               zeroes_done, zeroes_needed,
4262                                               phase);
4263         zeroes_done = zeroes_needed;
4264         if (zsize &gt; InitArrayShortSize &amp;&amp; ++big_init_gaps &gt; 2)
4265           do_zeroing = false;   // leave the hole, next time
4266       }
4267     }
4268 
4269     // Collect the store and move on:
4270     phase-&gt;replace_input_of(st, MemNode::Memory, inits);
4271     inits = st;                 // put it on the linearized chain
4272     set_req(i, zmem);           // unhook from previous position
4273 
4274     if (zeroes_done == st_off)
4275       zeroes_done = next_init_off;
4276 
4277     assert(!do_zeroing || zeroes_done &gt;= next_init_off, &quot;don&#39;t miss any&quot;);
4278 
4279     #ifdef ASSERT
4280     // Various order invariants.  Weaker than stores_are_sane because
</pre>
<hr />
<pre>
4300   remove_extra_zeroes();        // clear out all the zmems left over
4301   add_req(inits);
4302 
4303   if (!(UseTLAB &amp;&amp; ZeroTLAB)) {
4304     // If anything remains to be zeroed, zero it all now.
4305     zeroes_done = align_down(zeroes_done, BytesPerInt);
4306     // if it is the last unused 4 bytes of an instance, forget about it
4307     intptr_t size_limit = phase-&gt;find_intptr_t_con(size_in_bytes, max_jint);
4308     if (zeroes_done + BytesPerLong &gt;= size_limit) {
4309       AllocateNode* alloc = allocation();
4310       assert(alloc != NULL, &quot;must be present&quot;);
4311       if (alloc != NULL &amp;&amp; alloc-&gt;Opcode() == Op_Allocate) {
4312         Node* klass_node = alloc-&gt;in(AllocateNode::KlassNode);
4313         ciKlass* k = phase-&gt;type(klass_node)-&gt;is_klassptr()-&gt;klass();
4314         if (zeroes_done == k-&gt;layout_helper())
4315           zeroes_done = size_limit;
4316       }
4317     }
4318     if (zeroes_done &lt; size_limit) {
4319       rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,


4320                                             zeroes_done, size_in_bytes, phase);
4321     }
4322   }
4323 
4324   set_complete(phase);
4325   return rawmem;
4326 }
4327 
4328 
4329 #ifdef ASSERT
4330 bool InitializeNode::stores_are_sane(PhaseTransform* phase) {
4331   if (is_complete())
4332     return true;                // stores could be anything at this point
4333   assert(allocation() != NULL, &quot;must be present&quot;);
4334   intptr_t last_off = allocation()-&gt;minimum_header_size();
4335   for (uint i = InitializeNode::RawStores; i &lt; req(); i++) {
4336     Node* st = in(i);
4337     intptr_t st_off = get_store_offset(st, phase);
4338     if (st_off &lt; 0)  continue;  // ignore dead garbage
4339     if (last_off &gt; st_off) {
</pre>
</td>
<td>
<hr />
<pre>
  28 #include &quot;gc/shared/barrierSet.hpp&quot;
  29 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  30 #include &quot;memory/allocation.inline.hpp&quot;
  31 #include &quot;memory/resourceArea.hpp&quot;
  32 #include &quot;oops/objArrayKlass.hpp&quot;
  33 #include &quot;opto/addnode.hpp&quot;
  34 #include &quot;opto/arraycopynode.hpp&quot;
  35 #include &quot;opto/cfgnode.hpp&quot;
  36 #include &quot;opto/compile.hpp&quot;
  37 #include &quot;opto/connode.hpp&quot;
  38 #include &quot;opto/convertnode.hpp&quot;
  39 #include &quot;opto/loopnode.hpp&quot;
  40 #include &quot;opto/machnode.hpp&quot;
  41 #include &quot;opto/matcher.hpp&quot;
  42 #include &quot;opto/memnode.hpp&quot;
  43 #include &quot;opto/mulnode.hpp&quot;
  44 #include &quot;opto/narrowptrnode.hpp&quot;
  45 #include &quot;opto/phaseX.hpp&quot;
  46 #include &quot;opto/regmask.hpp&quot;
  47 #include &quot;opto/rootnode.hpp&quot;
<span class="line-added">  48 #include &quot;opto/valuetypenode.hpp&quot;</span>
  49 #include &quot;utilities/align.hpp&quot;
  50 #include &quot;utilities/copy.hpp&quot;
  51 #include &quot;utilities/macros.hpp&quot;
  52 #include &quot;utilities/powerOfTwo.hpp&quot;
  53 #include &quot;utilities/vmError.hpp&quot;
  54 
  55 // Portions of code courtesy of Clifford Click
  56 
  57 // Optimization - Graph Style
  58 
  59 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st);
  60 
  61 //=============================================================================
  62 uint MemNode::size_of() const { return sizeof(*this); }
  63 
  64 const TypePtr *MemNode::adr_type() const {
  65   Node* adr = in(Address);
  66   if (adr == NULL)  return NULL; // node is dead
  67   const TypePtr* cross_check = NULL;
  68   DEBUG_ONLY(cross_check = _adr_type);
</pre>
<hr />
<pre>
 221       // clone the Phi with our address type
 222       result = mphi-&gt;split_out_instance(t_adr, igvn);
 223     } else {
 224       assert(phase-&gt;C-&gt;get_alias_index(t) == phase-&gt;C-&gt;get_alias_index(t_adr), &quot;correct memory chain&quot;);
 225     }
 226   }
 227   return result;
 228 }
 229 
 230 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st) {
 231   uint alias_idx = phase-&gt;C-&gt;get_alias_index(tp);
 232   Node *mem = mmem;
 233 #ifdef ASSERT
 234   {
 235     // Check that current type is consistent with the alias index used during graph construction
 236     assert(alias_idx &gt;= Compile::AliasIdxRaw, &quot;must not be a bad alias_idx&quot;);
 237     bool consistent =  adr_check == NULL || adr_check-&gt;empty() ||
 238                        phase-&gt;C-&gt;must_alias(adr_check, alias_idx );
 239     // Sometimes dead array references collapse to a[-1], a[-2], or a[-3]
 240     if( !consistent &amp;&amp; adr_check != NULL &amp;&amp; !adr_check-&gt;empty() &amp;&amp;
<span class="line-modified"> 241         tp-&gt;isa_aryptr() &amp;&amp;        tp-&gt;offset() == Type::OffsetBot &amp;&amp;</span>
 242         adr_check-&gt;isa_aryptr() &amp;&amp; adr_check-&gt;offset() != Type::OffsetBot &amp;&amp;
 243         ( adr_check-&gt;offset() == arrayOopDesc::length_offset_in_bytes() ||
 244           adr_check-&gt;offset() == oopDesc::klass_offset_in_bytes() ||
 245           adr_check-&gt;offset() == oopDesc::mark_offset_in_bytes() ) ) {
 246       // don&#39;t assert if it is dead code.
 247       consistent = true;
 248     }
 249     if( !consistent ) {
 250       st-&gt;print(&quot;alias_idx==%d, adr_check==&quot;, alias_idx);
 251       if( adr_check == NULL ) {
 252         st-&gt;print(&quot;NULL&quot;);
 253       } else {
 254         adr_check-&gt;dump();
 255       }
 256       st-&gt;cr();
 257       print_alias_types();
 258       assert(consistent, &quot;adr_check must match alias idx&quot;);
 259     }
 260   }
 261 #endif
</pre>
<hr />
<pre>
 816          &quot;use LoadKlassNode instead&quot;);
 817   assert(!(adr_type-&gt;isa_aryptr() &amp;&amp;
 818            adr_type-&gt;offset() == arrayOopDesc::length_offset_in_bytes()),
 819          &quot;use LoadRangeNode instead&quot;);
 820   // Check control edge of raw loads
 821   assert( ctl != NULL || C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
 822           // oop will be recorded in oop map if load crosses safepoint
 823           rt-&gt;isa_oopptr() || is_immutable_value(adr),
 824           &quot;raw memory operations should have control edge&quot;);
 825   LoadNode* load = NULL;
 826   switch (bt) {
 827   case T_BOOLEAN: load = new LoadUBNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 828   case T_BYTE:    load = new LoadBNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 829   case T_INT:     load = new LoadINode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 830   case T_CHAR:    load = new LoadUSNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 831   case T_SHORT:   load = new LoadSNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 832   case T_LONG:    load = new LoadLNode (ctl, mem, adr, adr_type, rt-&gt;is_long(), mo, control_dependency); break;
 833   case T_FLOAT:   load = new LoadFNode (ctl, mem, adr, adr_type, rt,            mo, control_dependency); break;
 834   case T_DOUBLE:  load = new LoadDNode (ctl, mem, adr, adr_type, rt,            mo, control_dependency); break;
 835   case T_ADDRESS: load = new LoadPNode (ctl, mem, adr, adr_type, rt-&gt;is_ptr(),  mo, control_dependency); break;
<span class="line-added"> 836   case T_VALUETYPE:</span>
 837   case T_OBJECT:
 838 #ifdef _LP64
 839     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
 840       load = new LoadNNode(ctl, mem, adr, adr_type, rt-&gt;make_narrowoop(), mo, control_dependency);
 841     } else
 842 #endif
 843     {
 844       assert(!adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop() &amp;&amp; !adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass(), &quot;should have got back a narrow oop&quot;);
 845       load = new LoadPNode(ctl, mem, adr, adr_type, rt-&gt;is_ptr(), mo, control_dependency);
 846     }
 847     break;
 848   default:
 849     ShouldNotReachHere();
 850     break;
 851   }
 852   assert(load != NULL, &quot;LoadNode should have been created&quot;);
 853   if (unaligned) {
 854     load-&gt;set_unaligned_access();
 855   }
 856   if (mismatched) {
</pre>
<hr />
<pre>
 947     if (ac-&gt;as_ArrayCopy()-&gt;is_clonebasic()) {
 948       assert(ld_alloc != NULL, &quot;need an alloc&quot;);
 949       assert(addp-&gt;is_AddP(), &quot;address must be addp&quot;);
 950       BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 951       assert(bs-&gt;step_over_gc_barrier(addp-&gt;in(AddPNode::Base)) == bs-&gt;step_over_gc_barrier(ac-&gt;in(ArrayCopyNode::Dest)), &quot;strange pattern&quot;);
 952       assert(bs-&gt;step_over_gc_barrier(addp-&gt;in(AddPNode::Address)) == bs-&gt;step_over_gc_barrier(ac-&gt;in(ArrayCopyNode::Dest)), &quot;strange pattern&quot;);
 953       addp-&gt;set_req(AddPNode::Base, src);
 954       addp-&gt;set_req(AddPNode::Address, src);
 955     } else {
 956       assert(ac-&gt;as_ArrayCopy()-&gt;is_arraycopy_validated() ||
 957              ac-&gt;as_ArrayCopy()-&gt;is_copyof_validated() ||
 958              ac-&gt;as_ArrayCopy()-&gt;is_copyofrange_validated(), &quot;only supported cases&quot;);
 959       assert(addp-&gt;in(AddPNode::Base) == addp-&gt;in(AddPNode::Address), &quot;should be&quot;);
 960       addp-&gt;set_req(AddPNode::Base, src);
 961       addp-&gt;set_req(AddPNode::Address, src);
 962 
 963       const TypeAryPtr* ary_t = phase-&gt;type(in(MemNode::Address))-&gt;isa_aryptr();
 964       BasicType ary_elem  = ary_t-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
 965       uint header = arrayOopDesc::base_offset_in_bytes(ary_elem);
 966       uint shift  = exact_log2(type2aelembytes(ary_elem));
<span class="line-added"> 967       if (ary_t-&gt;klass()-&gt;is_value_array_klass()) {</span>
<span class="line-added"> 968         ciValueArrayKlass* vak = ary_t-&gt;klass()-&gt;as_value_array_klass();</span>
<span class="line-added"> 969         shift = vak-&gt;log2_element_size();</span>
<span class="line-added"> 970       }</span>
 971 
 972       Node* diff = phase-&gt;transform(new SubINode(ac-&gt;in(ArrayCopyNode::SrcPos), ac-&gt;in(ArrayCopyNode::DestPos)));
 973 #ifdef _LP64
 974       diff = phase-&gt;transform(new ConvI2LNode(diff));
 975 #endif
 976       diff = phase-&gt;transform(new LShiftXNode(diff, phase-&gt;intcon(shift)));
 977 
 978       Node* offset = phase-&gt;transform(new AddXNode(addp-&gt;in(AddPNode::Offset), diff));
 979       addp-&gt;set_req(AddPNode::Offset, offset);
 980     }
 981     addp = phase-&gt;transform(addp);
 982 #ifdef ASSERT
 983     const TypePtr* adr_type = phase-&gt;type(addp)-&gt;is_ptr();
 984     ld-&gt;_adr_type = adr_type;
 985 #endif
 986     ld-&gt;set_req(MemNode::Address, addp);
 987     ld-&gt;set_req(0, ctl);
 988     ld-&gt;set_req(MemNode::Memory, mem);
 989     // load depends on the tests that validate the arraycopy
 990     ld-&gt;_control_dependency = UnknownControl;
</pre>
<hr />
<pre>
1078         // the same pointer-and-offset that we stored to.
1079         // Casted version may carry a dependency and it is respected.
1080         // Thus, we are able to replace L by V.
1081       }
1082       // Now prove that we have a LoadQ matched to a StoreQ, for some Q.
1083       if (store_Opcode() != st-&gt;Opcode())
1084         return NULL;
1085       return st-&gt;in(MemNode::ValueIn);
1086     }
1087 
1088     // A load from a freshly-created object always returns zero.
1089     // (This can happen after LoadNode::Ideal resets the load&#39;s memory input
1090     // to find_captured_store, which returned InitializeNode::zero_memory.)
1091     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
1092         (st-&gt;in(0) == ld_alloc) &amp;&amp;
1093         (ld_off &gt;= st-&gt;in(0)-&gt;as_Allocate()-&gt;minimum_header_size())) {
1094       // return a zero value for the load&#39;s basic type
1095       // (This is one of the few places where a generic PhaseTransform
1096       // can create new nodes.  Think of it as lazily manifesting
1097       // virtually pre-existing constants.)
<span class="line-added">1098       assert(memory_type() != T_VALUETYPE, &quot;should not be used for value types&quot;);</span>
<span class="line-added">1099       Node* default_value = ld_alloc-&gt;in(AllocateNode::DefaultValue);</span>
<span class="line-added">1100       if (default_value != NULL) {</span>
<span class="line-added">1101         return default_value;</span>
<span class="line-added">1102       }</span>
<span class="line-added">1103       assert(ld_alloc-&gt;in(AllocateNode::RawDefaultValue) == NULL, &quot;default value may not be null&quot;);</span>
1104       return phase-&gt;zerocon(memory_type());
1105     }
1106 
1107     // A load from an initialization barrier can match a captured store.
1108     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Initialize()) {
1109       InitializeNode* init = st-&gt;in(0)-&gt;as_Initialize();
1110       AllocateNode* alloc = init-&gt;allocation();
1111       if ((alloc != NULL) &amp;&amp; (alloc == ld_alloc)) {
1112         // examine a captured store value
1113         st = init-&gt;find_captured_store(ld_off, memory_size(), phase);
1114         if (st != NULL) {
1115           continue;             // take one more trip around
1116         }
1117       }
1118     }
1119 
1120     // Load boxed value from result of valueOf() call is input parameter.
1121     if (this-&gt;is_Load() &amp;&amp; ld_adr-&gt;is_AddP() &amp;&amp;
1122         (tp != NULL) &amp;&amp; tp-&gt;is_ptr_to_boxed_value()) {
1123       intptr_t ignore = 0;
</pre>
<hr />
<pre>
1141 //----------------------is_instance_field_load_with_local_phi------------------
1142 bool LoadNode::is_instance_field_load_with_local_phi(Node* ctrl) {
1143   if( in(Memory)-&gt;is_Phi() &amp;&amp; in(Memory)-&gt;in(0) == ctrl &amp;&amp;
1144       in(Address)-&gt;is_AddP() ) {
1145     const TypeOopPtr* t_oop = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1146     // Only instances and boxed values.
1147     if( t_oop != NULL &amp;&amp;
1148         (t_oop-&gt;is_ptr_to_boxed_value() ||
1149          t_oop-&gt;is_known_instance_field()) &amp;&amp;
1150         t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
1151         t_oop-&gt;offset() != Type::OffsetTop) {
1152       return true;
1153     }
1154   }
1155   return false;
1156 }
1157 
1158 //------------------------------Identity---------------------------------------
1159 // Loads are identity if previous store is to same address
1160 Node* LoadNode::Identity(PhaseGVN* phase) {
<span class="line-added">1161   // Loading from a ValueTypePtr? The ValueTypePtr has the values of</span>
<span class="line-added">1162   // all fields as input. Look for the field with matching offset.</span>
<span class="line-added">1163   Node* addr = in(Address);</span>
<span class="line-added">1164   intptr_t offset;</span>
<span class="line-added">1165   Node* base = AddPNode::Ideal_base_and_offset(addr, phase, offset);</span>
<span class="line-added">1166   if (base != NULL &amp;&amp; base-&gt;is_ValueTypePtr() &amp;&amp; offset &gt; oopDesc::klass_offset_in_bytes()) {</span>
<span class="line-added">1167     Node* value = base-&gt;as_ValueTypePtr()-&gt;field_value_by_offset((int)offset, true);</span>
<span class="line-added">1168     if (value-&gt;is_ValueType()) {</span>
<span class="line-added">1169       // Non-flattened value type field</span>
<span class="line-added">1170       ValueTypeNode* vt = value-&gt;as_ValueType();</span>
<span class="line-added">1171       if (vt-&gt;is_allocated(phase)) {</span>
<span class="line-added">1172         value = vt-&gt;get_oop();</span>
<span class="line-added">1173       } else {</span>
<span class="line-added">1174         // Not yet allocated, bail out</span>
<span class="line-added">1175         value = NULL;</span>
<span class="line-added">1176       }</span>
<span class="line-added">1177     }</span>
<span class="line-added">1178     if (value != NULL) {</span>
<span class="line-added">1179       if (Opcode() == Op_LoadN) {</span>
<span class="line-added">1180         // Encode oop value if we are loading a narrow oop</span>
<span class="line-added">1181         assert(!phase-&gt;type(value)-&gt;isa_narrowoop(), &quot;should already be decoded&quot;);</span>
<span class="line-added">1182         value = phase-&gt;transform(new EncodePNode(value, bottom_type()));</span>
<span class="line-added">1183       }</span>
<span class="line-added">1184       return value;</span>
<span class="line-added">1185     }</span>
<span class="line-added">1186   }</span>
<span class="line-added">1187 </span>
1188   // If the previous store-maker is the right kind of Store, and the store is
1189   // to the same address, then we are equal to the value stored.
1190   Node* mem = in(Memory);
1191   Node* value = can_see_stored_value(mem, phase);
1192   if( value ) {
1193     // byte, short &amp; char stores truncate naturally.
1194     // A load has to load the truncated value which requires
1195     // some sort of masking operation and that requires an
1196     // Ideal call instead of an Identity call.
1197     if (memory_size() &lt; BytesPerInt) {
1198       // If the input to the store does not fit with the load&#39;s result type,
1199       // it must be truncated via an Ideal call.
1200       if (!phase-&gt;type(value)-&gt;higher_equal(phase-&gt;type(this)))
1201         return this;
1202     }
1203     // (This works even when value is a Con, but LoadNode::Value
1204     // usually runs first, producing the singleton type of the Con.)
1205     return value;
1206   }
1207 
</pre>
<hr />
<pre>
1725   // fold up, do so.
1726   Node* prev_mem = find_previous_store(phase);
1727   if (prev_mem != NULL) {
1728     Node* value = can_see_arraycopy_value(prev_mem, phase);
1729     if (value != NULL) {
1730       return value;
1731     }
1732   }
1733   // Steps (a), (b):  Walk past independent stores to find an exact match.
1734   if (prev_mem != NULL &amp;&amp; prev_mem != in(MemNode::Memory)) {
1735     // (c) See if we can fold up on the spot, but don&#39;t fold up here.
1736     // Fold-up might require truncation (for LoadB/LoadS/LoadUS) or
1737     // just return a prior value, which is done by Identity calls.
1738     if (can_see_stored_value(prev_mem, phase)) {
1739       // Make ready for step (d):
1740       set_req(MemNode::Memory, prev_mem);
1741       return this;
1742     }
1743   }
1744 
<span class="line-modified">1745   AllocateNode* alloc = AllocateNode::Ideal_allocation(address, phase);</span>
<span class="line-modified">1746   if (alloc != NULL &amp;&amp; mem-&gt;is_Proj() &amp;&amp;</span>
<span class="line-added">1747       mem-&gt;in(0) != NULL &amp;&amp;</span>
<span class="line-added">1748       mem-&gt;in(0) == alloc-&gt;initialization() &amp;&amp;</span>
<span class="line-added">1749       Opcode() == Op_LoadX &amp;&amp;</span>
<span class="line-added">1750       alloc-&gt;initialization()-&gt;proj_out_or_null(0) != NULL) {</span>
1751     InitializeNode* init = alloc-&gt;initialization();
1752     Node* control = init-&gt;proj_out(0);
<span class="line-modified">1753     return alloc-&gt;make_ideal_mark(phase, control, mem);</span>
1754   }
1755 
1756   return progress ? this : NULL;
1757 }
1758 
1759 // Helper to recognize certain Klass fields which are invariant across
1760 // some group of array types (e.g., int[] or all T[] where T &lt; Object).
1761 const Type*
1762 LoadNode::load_array_final_field(const TypeKlassPtr *tkls,
1763                                  ciKlass* klass) const {
1764   if (tkls-&gt;offset() == in_bytes(Klass::modifier_flags_offset())) {
1765     // The field is Klass::_modifier_flags.  Return its (constant) value.
1766     // (Folds up the 2nd indirection in aClassConstant.getModifiers().)
1767     assert(this-&gt;Opcode() == Op_LoadI, &quot;must load an int from _modifier_flags&quot;);
1768     return TypeInt::make(klass-&gt;modifier_flags());
1769   }
1770   if (tkls-&gt;offset() == in_bytes(Klass::access_flags_offset())) {
1771     // The field is Klass::_access_flags.  Return its (constant) value.
1772     // (Folds up the 2nd indirection in Reflection.getClassAccessFlags(aClassConstant).)
1773     assert(this-&gt;Opcode() == Op_LoadI, &quot;must load an int from _access_flags&quot;);
</pre>
<hr />
<pre>
1825       }
1826     }
1827 
1828     // Don&#39;t do this for integer types. There is only potential profit if
1829     // the element type t is lower than _type; that is, for int types, if _type is
1830     // more restrictive than t.  This only happens here if one is short and the other
1831     // char (both 16 bits), and in those cases we&#39;ve made an intentional decision
1832     // to use one kind of load over the other. See AndINode::Ideal and 4965907.
1833     // Also, do not try to narrow the type for a LoadKlass, regardless of offset.
1834     //
1835     // Yes, it is possible to encounter an expression like (LoadKlass p1:(AddP x x 8))
1836     // where the _gvn.type of the AddP is wider than 8.  This occurs when an earlier
1837     // copy p0 of (AddP x x 8) has been proven equal to p1, and the p0 has been
1838     // subsumed by p1.  If p1 is on the worklist but has not yet been re-transformed,
1839     // it is possible that p1 will have a type like Foo*[int+]:NotNull*+any.
1840     // In fact, that could have been the original type of p1, and p1 could have
1841     // had an original form like p1:(AddP x x (LShiftL quux 3)), where the
1842     // expression (LShiftL quux 3) independently optimized to the constant 8.
1843     if ((t-&gt;isa_int() == NULL) &amp;&amp; (t-&gt;isa_long() == NULL)
1844         &amp;&amp; (_type-&gt;isa_vect() == NULL)
<span class="line-added">1845         &amp;&amp; t-&gt;isa_valuetype() == NULL</span>
1846         &amp;&amp; Opcode() != Op_LoadKlass &amp;&amp; Opcode() != Op_LoadNKlass) {
1847       // t might actually be lower than _type, if _type is a unique
1848       // concrete subclass of abstract class t.
1849       if (off_beyond_header || off == Type::OffsetBot) {  // is the offset beyond the header?
1850         const Type* jt = t-&gt;join_speculative(_type);
1851         // In any case, do not allow the join, per se, to empty out the type.
1852         if (jt-&gt;empty() &amp;&amp; !t-&gt;empty()) {
1853           // This can happen if a interface-typed array narrows to a class type.
1854           jt = _type;
1855         }
1856 #ifdef ASSERT
1857         if (phase-&gt;C-&gt;eliminate_boxing() &amp;&amp; adr-&gt;is_AddP()) {
1858           // The pointers in the autobox arrays are always non-null
1859           Node* base = adr-&gt;in(AddPNode::Base);
1860           if ((base != NULL) &amp;&amp; base-&gt;is_DecodeN()) {
1861             // Get LoadN node which loads IntegerCache.cache field
1862             base = base-&gt;in(1);
1863           }
1864           if ((base != NULL) &amp;&amp; base-&gt;is_Con()) {
1865             const TypeAryPtr* base_type = base-&gt;bottom_type()-&gt;isa_aryptr();
1866             if ((base_type != NULL) &amp;&amp; base_type-&gt;is_autobox_cache()) {
1867               // It could be narrow oop
1868               assert(jt-&gt;make_ptr()-&gt;ptr() == TypePtr::NotNull,&quot;sanity&quot;);
1869             }
1870           }
1871         }
1872 #endif
1873         return jt;
1874       }
1875     }
1876   } else if (tp-&gt;base() == Type::InstPtr) {
1877     assert( off != Type::OffsetBot ||
1878             // arrays can be cast to Objects
1879             tp-&gt;is_oopptr()-&gt;klass()-&gt;is_java_lang_Object() ||
<span class="line-added">1880             tp-&gt;is_oopptr()-&gt;klass() == ciEnv::current()-&gt;Class_klass() ||</span>
1881             // unsafe field access may not have a constant offset
1882             C-&gt;has_unsafe_access(),
1883             &quot;Field accesses must be precise&quot; );
1884     // For oop loads, we expect the _type to be precise.
1885 
<span class="line-modified">1886     const TypeInstPtr* tinst = tp-&gt;is_instptr();</span>
<span class="line-added">1887     BasicType bt = memory_type();</span>
<span class="line-added">1888 </span>
1889     // Optimize loads from constant fields.
1890     ciObject* const_oop = tinst-&gt;const_oop();
1891     if (!is_mismatched_access() &amp;&amp; off != Type::OffsetBot &amp;&amp; const_oop != NULL &amp;&amp; const_oop-&gt;is_instance()) {
<span class="line-modified">1892       ciType* mirror_type = const_oop-&gt;as_instance()-&gt;java_mirror_type();</span>
<span class="line-added">1893       if (mirror_type != NULL &amp;&amp; mirror_type-&gt;is_valuetype()) {</span>
<span class="line-added">1894         ciValueKlass* vk = mirror_type-&gt;as_value_klass();</span>
<span class="line-added">1895         if (off == vk-&gt;default_value_offset()) {</span>
<span class="line-added">1896           // Loading a special hidden field that contains the oop of the default value type</span>
<span class="line-added">1897           const Type* const_oop = TypeInstPtr::make(vk-&gt;default_value_instance());</span>
<span class="line-added">1898           return (bt == T_NARROWOOP) ? const_oop-&gt;make_narrowoop() : const_oop;</span>
<span class="line-added">1899         }</span>
<span class="line-added">1900       }</span>
<span class="line-added">1901       const Type* con_type = Type::make_constant_from_field(const_oop-&gt;as_instance(), off, is_unsigned(), bt);</span>
1902       if (con_type != NULL) {
1903         return con_type;
1904       }
1905     }
1906   } else if (tp-&gt;base() == Type::KlassPtr) {
1907     assert( off != Type::OffsetBot ||
1908             // arrays can be cast to Objects
<span class="line-added">1909             tp-&gt;is_klassptr()-&gt;klass() == NULL ||</span>
1910             tp-&gt;is_klassptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1911             // also allow array-loading from the primary supertype
1912             // array during subtype checks
1913             Opcode() == Op_LoadKlass,
1914             &quot;Field accesses must be precise&quot; );
1915     // For klass/static loads, we expect the _type to be precise
<span class="line-modified">1916   } else if (tp-&gt;base() == Type::RawPtr &amp;&amp; !StressReflectiveCode) {</span>
<span class="line-modified">1917     if (adr-&gt;is_Load() &amp;&amp; off == 0) {</span>
<span class="line-modified">1918       /* With mirrors being an indirect in the Klass*</span>
<span class="line-modified">1919        * the VM is now using two loads. LoadKlass(LoadP(LoadP(Klass, mirror_offset), zero_offset))</span>
<span class="line-modified">1920        * The LoadP from the Klass has a RawPtr type (see LibraryCallKit::load_mirror_from_klass).</span>
<span class="line-modified">1921        *</span>
<span class="line-modified">1922        * So check the type and klass of the node before the LoadP.</span>
<span class="line-modified">1923        */</span>
<span class="line-modified">1924       Node* adr2 = adr-&gt;in(MemNode::Address);</span>
<span class="line-modified">1925       const TypeKlassPtr* tkls = phase-&gt;type(adr2)-&gt;isa_klassptr();</span>
<span class="line-modified">1926       if (tkls != NULL) {</span>
<span class="line-modified">1927         ciKlass* klass = tkls-&gt;klass();</span>
<span class="line-modified">1928         if (klass != NULL &amp;&amp; klass-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp; tkls-&gt;offset() == in_bytes(Klass::java_mirror_offset())) {</span>
<span class="line-modified">1929           assert(adr-&gt;Opcode() == Op_LoadP, &quot;must load an oop from _java_mirror&quot;);</span>
<span class="line-modified">1930           assert(Opcode() == Op_LoadP, &quot;must load an oop from _java_mirror&quot;);</span>
<span class="line-added">1931           return TypeInstPtr::make(klass-&gt;java_mirror());</span>
<span class="line-added">1932         }</span>
<span class="line-added">1933       }</span>
<span class="line-added">1934     } else {</span>
<span class="line-added">1935       // Check for a load of the default value offset from the ValueKlassFixedBlock:</span>
<span class="line-added">1936       // LoadI(LoadP(value_klass, adr_valueklass_fixed_block_offset), default_value_offset_offset)</span>
<span class="line-added">1937       intptr_t offset = 0;</span>
<span class="line-added">1938       Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);</span>
<span class="line-added">1939       if (base != NULL &amp;&amp; base-&gt;is_Load() &amp;&amp; offset == in_bytes(ValueKlass::default_value_offset_offset())) {</span>
<span class="line-added">1940         const TypeKlassPtr* tkls = phase-&gt;type(base-&gt;in(MemNode::Address))-&gt;isa_klassptr();</span>
<span class="line-added">1941         if (tkls != NULL &amp;&amp; tkls-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp; tkls-&gt;isa_valuetype() &amp;&amp;</span>
<span class="line-added">1942             tkls-&gt;offset() == in_bytes(InstanceKlass::adr_valueklass_fixed_block_offset())) {</span>
<span class="line-added">1943           assert(base-&gt;Opcode() == Op_LoadP, &quot;must load an oop from klass&quot;);</span>
<span class="line-added">1944           assert(Opcode() == Op_LoadI, &quot;must load an int from fixed block&quot;);</span>
<span class="line-added">1945           return TypeInt::make(tkls-&gt;klass()-&gt;as_value_klass()-&gt;default_value_offset());</span>
<span class="line-added">1946         }</span>
1947       }
1948     }
1949   }
1950 
1951   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
1952   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
1953     ciKlass* klass = tkls-&gt;klass();
<span class="line-modified">1954     if (tkls-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact()) {</span>
1955       // We are loading a field from a Klass metaobject whose identity
1956       // is known at compile time (the type is &quot;exact&quot; or &quot;precise&quot;).
1957       // Check for fields we know are maintained as constants by the VM.
1958       if (tkls-&gt;offset() == in_bytes(Klass::super_check_offset_offset())) {
1959         // The field is Klass::_super_check_offset.  Return its (constant) value.
1960         // (Folds up type checking code.)
1961         assert(Opcode() == Op_LoadI, &quot;must load an int from _super_check_offset&quot;);
1962         return TypeInt::make(klass-&gt;super_check_offset());
1963       }
1964       // Compute index into primary_supers array
1965       juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1966       // Check for overflowing; use unsigned compare to handle the negative case.
1967       if( depth &lt; ciKlass::primary_super_limit() ) {
1968         // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1969         // (Folds up type checking code.)
1970         assert(Opcode() == Op_LoadKlass, &quot;must load a klass from _primary_supers&quot;);
1971         ciKlass *ss = klass-&gt;super_of_depth(depth);
1972         return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1973       }
1974       const Type* aift = load_array_final_field(tkls, klass);
1975       if (aift != NULL)  return aift;
1976     }
1977 
1978     // We can still check if we are loading from the primary_supers array at a
1979     // shallow enough depth.  Even though the klass is not exact, entries less
1980     // than or equal to its super depth are correct.
<span class="line-modified">1981     if (tkls-&gt;is_loaded()) {</span>
1982       ciType *inner = klass;
1983       while( inner-&gt;is_obj_array_klass() )
1984         inner = inner-&gt;as_obj_array_klass()-&gt;base_element_type();
1985       if( inner-&gt;is_instance_klass() &amp;&amp;
1986           !inner-&gt;as_instance_klass()-&gt;flags().is_interface() ) {
1987         // Compute index into primary_supers array
1988         juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1989         // Check for overflowing; use unsigned compare to handle the negative case.
1990         if( depth &lt; ciKlass::primary_super_limit() &amp;&amp;
1991             depth &lt;= klass-&gt;super_depth() ) { // allow self-depth checks to handle self-check case
1992           // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1993           // (Folds up type checking code.)
1994           assert(Opcode() == Op_LoadKlass, &quot;must load a klass from _primary_supers&quot;);
1995           ciKlass *ss = klass-&gt;super_of_depth(depth);
1996           return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1997         }
1998       }
1999     }
2000 
2001     // If the type is enough to determine that the thing is not an array,
</pre>
<hr />
<pre>
2166   return LoadNode::Ideal(phase, can_reshape);
2167 }
2168 
2169 const Type* LoadSNode::Value(PhaseGVN* phase) const {
2170   Node* mem = in(MemNode::Memory);
2171   Node* value = can_see_stored_value(mem,phase);
2172   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
2173       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
2174     // If the input to the store does not fit with the load&#39;s result type,
2175     // it must be truncated. We can&#39;t delay until Ideal call since
2176     // a singleton Value is needed for split_thru_phi optimization.
2177     int con = value-&gt;get_int();
2178     return TypeInt::make((con &lt;&lt; 16) &gt;&gt; 16);
2179   }
2180   return LoadNode::Value(phase);
2181 }
2182 
2183 //=============================================================================
2184 //----------------------------LoadKlassNode::make------------------------------
2185 // Polymorphic factory method:
<span class="line-modified">2186 Node* LoadKlassNode::make(PhaseGVN&amp; gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at,</span>
<span class="line-added">2187                           const TypeKlassPtr* tk) {</span>
2188   // sanity check the alias category against the created node type
2189   const TypePtr *adr_type = adr-&gt;bottom_type()-&gt;isa_ptr();
2190   assert(adr_type != NULL, &quot;expecting TypeKlassPtr&quot;);
2191 #ifdef _LP64
2192   if (adr_type-&gt;is_ptr_to_narrowklass()) {
2193     assert(UseCompressedClassPointers, &quot;no compressed klasses&quot;);
2194     Node* load_klass = gvn.transform(new LoadNKlassNode(ctl, mem, adr, at, tk-&gt;make_narrowklass(), MemNode::unordered));
2195     return new DecodeNKlassNode(load_klass, load_klass-&gt;bottom_type()-&gt;make_ptr());
2196   }
2197 #endif
2198   assert(!adr_type-&gt;is_ptr_to_narrowklass() &amp;&amp; !adr_type-&gt;is_ptr_to_narrowoop(), &quot;should have got back a narrow oop&quot;);
2199   return new LoadKlassNode(ctl, mem, adr, at, tk, MemNode::unordered);
2200 }
2201 
2202 //------------------------------Value------------------------------------------
2203 const Type* LoadKlassNode::Value(PhaseGVN* phase) const {
2204   return klass_value_common(phase);
2205 }
2206 
2207 // In most cases, LoadKlassNode does not have the control input set. If the control
</pre>
<hr />
<pre>
2255     if( !ik-&gt;is_loaded() )
2256       return _type;             // Bail out if not loaded
2257     if (offset == oopDesc::klass_offset_in_bytes()) {
2258       if (tinst-&gt;klass_is_exact()) {
2259         return TypeKlassPtr::make(ik);
2260       }
2261       // See if we can become precise: no subklasses and no interface
2262       // (Note:  We need to support verified interfaces.)
2263       if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2264         //assert(!UseExactTypes, &quot;this code should be useless with exact types&quot;);
2265         // Add a dependence; if any subclass added we need to recompile
2266         if (!ik-&gt;is_final()) {
2267           // %%% should use stronger assert_unique_concrete_subtype instead
2268           phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2269         }
2270         // Return precise klass
2271         return TypeKlassPtr::make(ik);
2272       }
2273 
2274       // Return root of possible klass
<span class="line-modified">2275       return TypeKlassPtr::make(TypePtr::NotNull, ik, Type::Offset(0), tinst-&gt;flat_array());</span>
2276     }
2277   }
2278 
2279   // Check for loading klass from an array
2280   const TypeAryPtr *tary = tp-&gt;isa_aryptr();
<span class="line-modified">2281   if (tary != NULL) {</span>
2282     ciKlass *tary_klass = tary-&gt;klass();
2283     if (tary_klass != NULL   // can be NULL when at BOTTOM or TOP
2284         &amp;&amp; tary-&gt;offset() == oopDesc::klass_offset_in_bytes()) {
2285       if (tary-&gt;klass_is_exact()) {
2286         return TypeKlassPtr::make(tary_klass);
2287       }
<span class="line-modified">2288       ciArrayKlass* ak = tary_klass-&gt;as_array_klass();</span>
2289       // If the klass is an object array, we defer the question to the
2290       // array component klass.
<span class="line-modified">2291       if (ak-&gt;is_obj_array_klass()) {</span>
<span class="line-modified">2292         assert(ak-&gt;is_loaded(), &quot;&quot;);</span>
2293         ciKlass *base_k = ak-&gt;as_obj_array_klass()-&gt;base_element_klass();
<span class="line-modified">2294         if (base_k-&gt;is_loaded() &amp;&amp; base_k-&gt;is_instance_klass()) {</span>
<span class="line-modified">2295           ciInstanceKlass *ik = base_k-&gt;as_instance_klass();</span>
2296           // See if we can become precise: no subklasses and no interface
2297           if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2298             //assert(!UseExactTypes, &quot;this code should be useless with exact types&quot;);
2299             // Add a dependence; if any subclass added we need to recompile
2300             if (!ik-&gt;is_final()) {
2301               phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2302             }
2303             // Return precise array klass
2304             return TypeKlassPtr::make(ak);
2305           }
2306         }
<span class="line-modified">2307         return TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0), false);</span>
<span class="line-modified">2308       } else if (ak-&gt;is_type_array_klass()) {</span>
2309         //assert(!UseExactTypes, &quot;this code should be useless with exact types&quot;);

2310         return TypeKlassPtr::make(ak); // These are always precise
2311       }
2312     }
2313   }
2314 
2315   // Check for loading klass from an array klass
2316   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
2317   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
<span class="line-modified">2318     if (!tkls-&gt;is_loaded()) {</span>

2319       return _type;             // Bail out if not loaded
<span class="line-added">2320     }</span>
<span class="line-added">2321     ciKlass* klass = tkls-&gt;klass();</span>
2322     if( klass-&gt;is_obj_array_klass() &amp;&amp;
2323         tkls-&gt;offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {
2324       ciKlass* elem = klass-&gt;as_obj_array_klass()-&gt;element_klass();
2325       // // Always returning precise element type is incorrect,
2326       // // e.g., element type could be object and array may contain strings
2327       // return TypeKlassPtr::make(TypePtr::Constant, elem, 0);
2328 
2329       // The array&#39;s TypeKlassPtr was declared &#39;precise&#39; or &#39;not precise&#39;
2330       // according to the element type&#39;s subclassing.
<span class="line-modified">2331       return TypeKlassPtr::make(tkls-&gt;ptr(), elem, Type::Offset(0), elem-&gt;flatten_array());</span>
<span class="line-added">2332     } else if (klass-&gt;is_value_array_klass() &amp;&amp;</span>
<span class="line-added">2333                tkls-&gt;offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {</span>
<span class="line-added">2334       ciKlass* elem = klass-&gt;as_value_array_klass()-&gt;element_klass();</span>
<span class="line-added">2335       return TypeKlassPtr::make(tkls-&gt;ptr(), elem, Type::Offset(0), /* flat_array= */ true);</span>
2336     }
2337     if( klass-&gt;is_instance_klass() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp;
2338         tkls-&gt;offset() == in_bytes(Klass::super_offset())) {
2339       ciKlass* sup = klass-&gt;as_instance_klass()-&gt;super();
2340       // The field is Klass::_super.  Return its (constant) value.
2341       // (Folds up the 2nd indirection in aClassConstant.getSuperClass().)
2342       return sup ? TypeKlassPtr::make(sup) : TypePtr::NULL_PTR;
2343     }
2344   }
2345 
2346   // Bailout case
2347   return LoadNode::Value(phase);
2348 }
2349 
2350 //------------------------------Identity---------------------------------------
2351 // To clean up reflective code, simplify k.java_mirror.as_klass to plain k.
2352 // Also feed through the klass in Allocate(...klass...)._klass.
2353 Node* LoadKlassNode::Identity(PhaseGVN* phase) {
2354   return klass_identity_common(phase);
2355 }
</pre>
<hr />
<pre>
2523 //=============================================================================
2524 //---------------------------StoreNode::make-----------------------------------
2525 // Polymorphic factory method:
2526 StoreNode* StoreNode::make(PhaseGVN&amp; gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, BasicType bt, MemOrd mo) {
2527   assert((mo == unordered || mo == release), &quot;unexpected&quot;);
2528   Compile* C = gvn.C;
2529   assert(C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
2530          ctl != NULL, &quot;raw memory operations should have control edge&quot;);
2531 
2532   switch (bt) {
2533   case T_BOOLEAN: val = gvn.transform(new AndINode(val, gvn.intcon(0x1))); // Fall through to T_BYTE case
2534   case T_BYTE:    return new StoreBNode(ctl, mem, adr, adr_type, val, mo);
2535   case T_INT:     return new StoreINode(ctl, mem, adr, adr_type, val, mo);
2536   case T_CHAR:
2537   case T_SHORT:   return new StoreCNode(ctl, mem, adr, adr_type, val, mo);
2538   case T_LONG:    return new StoreLNode(ctl, mem, adr, adr_type, val, mo);
2539   case T_FLOAT:   return new StoreFNode(ctl, mem, adr, adr_type, val, mo);
2540   case T_DOUBLE:  return new StoreDNode(ctl, mem, adr, adr_type, val, mo);
2541   case T_METADATA:
2542   case T_ADDRESS:
<span class="line-added">2543   case T_VALUETYPE:</span>
2544   case T_OBJECT:
2545 #ifdef _LP64
2546     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2547       val = gvn.transform(new EncodePNode(val, val-&gt;bottom_type()-&gt;make_narrowoop()));
2548       return new StoreNNode(ctl, mem, adr, adr_type, val, mo);
2549     } else if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass() ||
2550                (UseCompressedClassPointers &amp;&amp; val-&gt;bottom_type()-&gt;isa_klassptr() &amp;&amp;
2551                 adr-&gt;bottom_type()-&gt;isa_rawptr())) {
2552       val = gvn.transform(new EncodePKlassNode(val, val-&gt;bottom_type()-&gt;make_narrowklass()));
2553       return new StoreNKlassNode(ctl, mem, adr, adr_type, val, mo);
2554     }
2555 #endif
2556     {
2557       return new StorePNode(ctl, mem, adr, adr_type, val, mo);
2558     }
2559   default:
2560     ShouldNotReachHere();
2561     return (StoreNode*)NULL;
2562   }
2563 }
</pre>
<hr />
<pre>
2584   //return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address) + (uintptr_t)in(ValueIn);
2585 
2586   // Since they are not commoned, do not hash them:
2587   return NO_HASH;
2588 }
2589 
2590 //------------------------------Ideal------------------------------------------
2591 // Change back-to-back Store(, p, x) -&gt; Store(m, p, y) to Store(m, p, x).
2592 // When a store immediately follows a relevant allocation/initialization,
2593 // try to capture it into the initialization, or hoist it above.
2594 Node *StoreNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2595   Node* p = MemNode::Ideal_common(phase, can_reshape);
2596   if (p)  return (p == NodeSentinel) ? NULL : p;
2597 
2598   Node* mem     = in(MemNode::Memory);
2599   Node* address = in(MemNode::Address);
2600   // Back-to-back stores to same address?  Fold em up.  Generally
2601   // unsafe if I have intervening uses...  Also disallowed for StoreCM
2602   // since they must follow each StoreP operation.  Redundant StoreCMs
2603   // are eliminated just before matching in final_graph_reshape.
<span class="line-modified">2604   if (phase-&gt;C-&gt;get_adr_type(phase-&gt;C-&gt;get_alias_index(adr_type())) != TypeAryPtr::VALUES) {</span>
2605     Node* st = mem;
2606     // If Store &#39;st&#39; has more than one use, we cannot fold &#39;st&#39; away.
2607     // For example, &#39;st&#39; might be the final state at a conditional
2608     // return.  Or, &#39;st&#39; might be used by some node which is live at
2609     // the same time &#39;st&#39; is live, which might be unschedulable.  So,
2610     // require exactly ONE user until such time as we clone &#39;mem&#39; for
2611     // each of &#39;mem&#39;s uses (thus making the exactly-1-user-rule hold
2612     // true).
2613     while (st-&gt;is_Store() &amp;&amp; st-&gt;outcnt() == 1 &amp;&amp; st-&gt;Opcode() != Op_StoreCM) {
2614       // Looking at a dead closed cycle of memory?
2615       assert(st != st-&gt;in(MemNode::Memory), &quot;dead loop in StoreNode::Ideal&quot;);
2616       assert(Opcode() == st-&gt;Opcode() ||
2617              st-&gt;Opcode() == Op_StoreVector ||
2618              Opcode() == Op_StoreVector ||
2619              phase-&gt;C-&gt;get_alias_index(adr_type()) == Compile::AliasIdxRaw ||
2620              (Opcode() == Op_StoreL &amp;&amp; st-&gt;Opcode() == Op_StoreI) || // expanded ClearArrayNode
2621              (Opcode() == Op_StoreI &amp;&amp; st-&gt;Opcode() == Op_StoreL) || // initialization by arraycopy
<span class="line-added">2622              (Opcode() == Op_StoreL &amp;&amp; st-&gt;Opcode() == Op_StoreN) ||</span>
2623              (is_mismatched_access() || st-&gt;as_Store()-&gt;is_mismatched_access()),
2624              &quot;no mismatched stores, except on raw memory: %s %s&quot;, NodeClassNames[Opcode()], NodeClassNames[st-&gt;Opcode()]);
2625 
2626       if (st-&gt;in(MemNode::Address)-&gt;eqv_uncast(address) &amp;&amp;
2627           st-&gt;as_Store()-&gt;memory_size() &lt;= this-&gt;memory_size()) {
2628         Node* use = st-&gt;raw_out(0);
2629         phase-&gt;igvn_rehash_node_delayed(use);
2630         if (can_reshape) {
2631           use-&gt;set_req_X(MemNode::Memory, st-&gt;in(MemNode::Memory), phase-&gt;is_IterGVN());
2632         } else {
2633           // It&#39;s OK to do this in the parser, since DU info is always accurate,
2634           // and the parser always refers to nodes via SafePointNode maps.
2635           use-&gt;set_req(MemNode::Memory, st-&gt;in(MemNode::Memory));
2636         }
2637         return this;
2638       }
2639       st = st-&gt;in(MemNode::Memory);
2640     }
2641   }
2642 
</pre>
<hr />
<pre>
2688   // Load then Store?  Then the Store is useless
2689   if (val-&gt;is_Load() &amp;&amp;
2690       val-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2691       val-&gt;in(MemNode::Memory )-&gt;eqv_uncast(mem) &amp;&amp;
2692       val-&gt;as_Load()-&gt;store_Opcode() == Opcode()) {
2693     result = mem;
2694   }
2695 
2696   // Two stores in a row of the same value?
2697   if (result == this &amp;&amp;
2698       mem-&gt;is_Store() &amp;&amp;
2699       mem-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2700       mem-&gt;in(MemNode::ValueIn)-&gt;eqv_uncast(val) &amp;&amp;
2701       mem-&gt;Opcode() == Opcode()) {
2702     result = mem;
2703   }
2704 
2705   // Store of zero anywhere into a freshly-allocated object?
2706   // Then the store is useless.
2707   // (It must already have been captured by the InitializeNode.)
<span class="line-modified">2708   if (result == this &amp;&amp; ReduceFieldZeroing) {</span>

2709     // a newly allocated object is already all-zeroes everywhere
<span class="line-modified">2710     if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Allocate() &amp;&amp;</span>
<span class="line-added">2711         (phase-&gt;type(val)-&gt;is_zero_type() || mem-&gt;in(0)-&gt;in(AllocateNode::DefaultValue) == val)) {</span>
<span class="line-added">2712       assert(!phase-&gt;type(val)-&gt;is_zero_type() || mem-&gt;in(0)-&gt;in(AllocateNode::DefaultValue) == NULL, &quot;storing null to value array is forbidden&quot;);</span>
2713       result = mem;
2714     }
2715 
2716     if (result == this) {
2717       // the store may also apply to zero-bits in an earlier object
2718       Node* prev_mem = find_previous_store(phase);
2719       // Steps (a), (b):  Walk past independent stores to find an exact match.
2720       if (prev_mem != NULL) {
2721         Node* prev_val = can_see_stored_value(prev_mem, phase);
2722         if (prev_val != NULL &amp;&amp; phase-&gt;eqv(prev_val, val)) {
2723           // prev_val and val might differ by a cast; it would be good
2724           // to keep the more informative of the two.
<span class="line-modified">2725           if (phase-&gt;type(val)-&gt;is_zero_type()) {</span>
<span class="line-added">2726             result = mem;</span>
<span class="line-added">2727           } else if (prev_mem-&gt;is_Proj() &amp;&amp; prev_mem-&gt;in(0)-&gt;is_Initialize()) {</span>
<span class="line-added">2728             InitializeNode* init = prev_mem-&gt;in(0)-&gt;as_Initialize();</span>
<span class="line-added">2729             AllocateNode* alloc = init-&gt;allocation();</span>
<span class="line-added">2730             if (alloc != NULL &amp;&amp; alloc-&gt;in(AllocateNode::DefaultValue) == val) {</span>
<span class="line-added">2731               result = mem;</span>
<span class="line-added">2732             }</span>
<span class="line-added">2733           }</span>
2734         }
2735       }
2736     }
2737   }
2738 
2739   if (result != this &amp;&amp; phase-&gt;is_IterGVN() != NULL) {
2740     MemBarNode* trailing = trailing_membar();
2741     if (trailing != NULL) {
2742 #ifdef ASSERT
2743       const TypeOopPtr* t_oop = phase-&gt;type(in(Address))-&gt;isa_oopptr();
2744       assert(t_oop == NULL || t_oop-&gt;is_known_instance_field(), &quot;only for non escaping objects&quot;);
2745 #endif
2746       PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
2747       trailing-&gt;remove(igvn);
2748     }
2749   }
2750 
2751   return result;
2752 }
2753 
</pre>
<hr />
<pre>
3022 // Clearing a short array is faster with stores
3023 Node *ClearArrayNode::Ideal(PhaseGVN *phase, bool can_reshape) {
3024   // Already know this is a large node, do not try to ideal it
3025   if (!IdealizeClearArrayNode || _is_large) return NULL;
3026 
3027   const int unit = BytesPerLong;
3028   const TypeX* t = phase-&gt;type(in(2))-&gt;isa_intptr_t();
3029   if (!t)  return NULL;
3030   if (!t-&gt;is_con())  return NULL;
3031   intptr_t raw_count = t-&gt;get_con();
3032   intptr_t size = raw_count;
3033   if (!Matcher::init_array_count_is_in_bytes) size *= unit;
3034   // Clearing nothing uses the Identity call.
3035   // Negative clears are possible on dead ClearArrays
3036   // (see jck test stmt114.stmt11402.val).
3037   if (size &lt;= 0 || size % unit != 0)  return NULL;
3038   intptr_t count = size / unit;
3039   // Length too long; communicate this to matchers and assemblers.
3040   // Assemblers are responsible to produce fast hardware clears for it.
3041   if (size &gt; InitArrayShortSize) {
<span class="line-modified">3042     return new ClearArrayNode(in(0), in(1), in(2), in(3), in(4), true);</span>
3043   }
3044   Node *mem = in(1);
3045   if( phase-&gt;type(mem)==Type::TOP ) return NULL;
3046   Node *adr = in(3);
3047   const Type* at = phase-&gt;type(adr);
3048   if( at==Type::TOP ) return NULL;
3049   const TypePtr* atp = at-&gt;isa_ptr();
3050   // adjust atp to be the correct array element address type
3051   if (atp == NULL)  atp = TypePtr::BOTTOM;
3052   else              atp = atp-&gt;add_offset(Type::OffsetBot);
3053   // Get base for derived pointer purposes
3054   if( adr-&gt;Opcode() != Op_AddP ) Unimplemented();
3055   Node *base = adr-&gt;in(1);
3056 
<span class="line-modified">3057   Node *val = in(4);</span>
3058   Node *off  = phase-&gt;MakeConX(BytesPerLong);
<span class="line-modified">3059   mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);</span>
3060   count--;
3061   while( count-- ) {
3062     mem = phase-&gt;transform(mem);
3063     adr = phase-&gt;transform(new AddPNode(base,adr,off));
<span class="line-modified">3064     mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);</span>
3065   }
3066   return mem;
3067 }
3068 
3069 //----------------------------step_through----------------------------------
3070 // Return allocation input memory edge if it is different instance
3071 // or itself if it is the one we are looking for.
3072 bool ClearArrayNode::step_through(Node** np, uint instance_id, PhaseTransform* phase) {
3073   Node* n = *np;
3074   assert(n-&gt;is_ClearArray(), &quot;sanity&quot;);
3075   intptr_t offset;
3076   AllocateNode* alloc = AllocateNode::Ideal_allocation(n-&gt;in(3), phase, offset);
3077   // This method is called only before Allocate nodes are expanded
3078   // during macro nodes expansion. Before that ClearArray nodes are
3079   // only generated in PhaseMacroExpand::generate_arraycopy() (before
3080   // Allocate nodes are expanded) which follows allocations.
3081   assert(alloc != NULL, &quot;should have allocation&quot;);
3082   if (alloc-&gt;_idx == instance_id) {
3083     // Can not bypass initialization of the instance we are looking for.
3084     return false;
3085   }
3086   // Otherwise skip it.
3087   InitializeNode* init = alloc-&gt;initialization();
3088   if (init != NULL)
3089     *np = init-&gt;in(TypeFunc::Memory);
3090   else
3091     *np = alloc-&gt;in(TypeFunc::Memory);
3092   return true;
3093 }
3094 
3095 //----------------------------clear_memory-------------------------------------
3096 // Generate code to initialize object storage to zero.
3097 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
<span class="line-added">3098                                    Node* val,</span>
<span class="line-added">3099                                    Node* raw_val,</span>
3100                                    intptr_t start_offset,
3101                                    Node* end_offset,
3102                                    PhaseGVN* phase) {
3103   intptr_t offset = start_offset;
3104 
3105   int unit = BytesPerLong;
3106   if ((offset % unit) != 0) {
3107     Node* adr = new AddPNode(dest, dest, phase-&gt;MakeConX(offset));
3108     adr = phase-&gt;transform(adr);
3109     const TypePtr* atp = TypeRawPtr::BOTTOM;
<span class="line-modified">3110     if (val != NULL) {</span>
<span class="line-added">3111       assert(phase-&gt;type(val)-&gt;isa_narrowoop(), &quot;should be narrow oop&quot;);</span>
<span class="line-added">3112       mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);</span>
<span class="line-added">3113     } else {</span>
<span class="line-added">3114       assert(raw_val == NULL, &quot;val may not be null&quot;);</span>
<span class="line-added">3115       mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT, MemNode::unordered);</span>
<span class="line-added">3116     }</span>
3117     mem = phase-&gt;transform(mem);
3118     offset += BytesPerInt;
3119   }
3120   assert((offset % unit) == 0, &quot;&quot;);
3121 
3122   // Initialize the remaining stuff, if any, with a ClearArray.
<span class="line-modified">3123   return clear_memory(ctl, mem, dest, raw_val, phase-&gt;MakeConX(offset), end_offset, phase);</span>
3124 }
3125 
3126 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
<span class="line-added">3127                                    Node* raw_val,</span>
3128                                    Node* start_offset,
3129                                    Node* end_offset,
3130                                    PhaseGVN* phase) {
3131   if (start_offset == end_offset) {
3132     // nothing to do
3133     return mem;
3134   }
3135 
3136   int unit = BytesPerLong;
3137   Node* zbase = start_offset;
3138   Node* zend  = end_offset;
3139 
3140   // Scale to the unit required by the CPU:
3141   if (!Matcher::init_array_count_is_in_bytes) {
3142     Node* shift = phase-&gt;intcon(exact_log2(unit));
3143     zbase = phase-&gt;transform(new URShiftXNode(zbase, shift) );
3144     zend  = phase-&gt;transform(new URShiftXNode(zend,  shift) );
3145   }
3146 
3147   // Bulk clear double-words
3148   Node* zsize = phase-&gt;transform(new SubXNode(zend, zbase) );
3149   Node* adr = phase-&gt;transform(new AddPNode(dest, dest, start_offset) );
<span class="line-modified">3150   if (raw_val == NULL) {</span>
<span class="line-added">3151     raw_val = phase-&gt;MakeConX(0);</span>
<span class="line-added">3152   }</span>
<span class="line-added">3153   mem = new ClearArrayNode(ctl, mem, zsize, adr, raw_val, false);</span>
3154   return phase-&gt;transform(mem);
3155 }
3156 
3157 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
<span class="line-added">3158                                    Node* val,</span>
<span class="line-added">3159                                    Node* raw_val,</span>
3160                                    intptr_t start_offset,
3161                                    intptr_t end_offset,
3162                                    PhaseGVN* phase) {
3163   if (start_offset == end_offset) {
3164     // nothing to do
3165     return mem;
3166   }
3167 
3168   assert((end_offset % BytesPerInt) == 0, &quot;odd end offset&quot;);
3169   intptr_t done_offset = end_offset;
3170   if ((done_offset % BytesPerLong) != 0) {
3171     done_offset -= BytesPerInt;
3172   }
3173   if (done_offset &gt; start_offset) {
<span class="line-modified">3174     mem = clear_memory(ctl, mem, dest, val, raw_val,</span>
3175                        start_offset, phase-&gt;MakeConX(done_offset), phase);
3176   }
3177   if (done_offset &lt; end_offset) { // emit the final 32-bit store
3178     Node* adr = new AddPNode(dest, dest, phase-&gt;MakeConX(done_offset));
3179     adr = phase-&gt;transform(adr);
3180     const TypePtr* atp = TypeRawPtr::BOTTOM;
<span class="line-modified">3181     if (val != NULL) {</span>
<span class="line-added">3182       assert(phase-&gt;type(val)-&gt;isa_narrowoop(), &quot;should be narrow oop&quot;);</span>
<span class="line-added">3183       mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);</span>
<span class="line-added">3184     } else {</span>
<span class="line-added">3185       assert(raw_val == NULL, &quot;val may not be null&quot;);</span>
<span class="line-added">3186       mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT, MemNode::unordered);</span>
<span class="line-added">3187     }</span>
3188     mem = phase-&gt;transform(mem);
3189     done_offset += BytesPerInt;
3190   }
3191   assert(done_offset == end_offset, &quot;&quot;);
3192   return mem;
3193 }
3194 
3195 //=============================================================================
3196 MemBarNode::MemBarNode(Compile* C, int alias_idx, Node* precedent)
3197   : MultiNode(TypeFunc::Parms + (precedent == NULL? 0: 1)),
3198     _adr_type(C-&gt;get_adr_type(alias_idx)), _kind(Standalone)
3199 #ifdef ASSERT
3200   , _pair_idx(0)
3201 #endif
3202 {
3203   init_class_id(Class_MemBar);
3204   Node* top = C-&gt;top();
3205   init_req(TypeFunc::I_O,top);
3206   init_req(TypeFunc::FramePtr,top);
3207   init_req(TypeFunc::ReturnAdr,top);
</pre>
<hr />
<pre>
3306       PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3307       remove(igvn);
3308       // Must return either the original node (now dead) or a new node
3309       // (Do not return a top here, since that would break the uniqueness of top.)
3310       return new ConINode(TypeInt::ZERO);
3311     }
3312   }
3313   return progress ? this : NULL;
3314 }
3315 
3316 //------------------------------Value------------------------------------------
3317 const Type* MemBarNode::Value(PhaseGVN* phase) const {
3318   if( !in(0) ) return Type::TOP;
3319   if( phase-&gt;type(in(0)) == Type::TOP )
3320     return Type::TOP;
3321   return TypeTuple::MEMBAR;
3322 }
3323 
3324 //------------------------------match------------------------------------------
3325 // Construct projections for memory.
<span class="line-modified">3326 Node *MemBarNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) {</span>
3327   switch (proj-&gt;_con) {
3328   case TypeFunc::Control:
3329   case TypeFunc::Memory:
3330     return new MachProjNode(this,proj-&gt;_con,RegMask::Empty,MachProjNode::unmatched_proj);
3331   }
3332   ShouldNotReachHere();
3333   return NULL;
3334 }
3335 
3336 void MemBarNode::set_store_pair(MemBarNode* leading, MemBarNode* trailing) {
3337   trailing-&gt;_kind = TrailingStore;
3338   leading-&gt;_kind = LeadingStore;
3339 #ifdef ASSERT
3340   trailing-&gt;_pair_idx = leading-&gt;_idx;
3341   leading-&gt;_pair_idx = leading-&gt;_idx;
3342 #endif
3343 }
3344 
3345 void MemBarNode::set_load_store_pair(MemBarNode* leading, MemBarNode* trailing) {
3346   trailing-&gt;_kind = TrailingLoadStore;
</pre>
<hr />
<pre>
3592   return (req() &gt; RawStores);
3593 }
3594 
3595 void InitializeNode::set_complete(PhaseGVN* phase) {
3596   assert(!is_complete(), &quot;caller responsibility&quot;);
3597   _is_complete = Complete;
3598 
3599   // After this node is complete, it contains a bunch of
3600   // raw-memory initializations.  There is no need for
3601   // it to have anything to do with non-raw memory effects.
3602   // Therefore, tell all non-raw users to re-optimize themselves,
3603   // after skipping the memory effects of this initialization.
3604   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3605   if (igvn)  igvn-&gt;add_users_to_worklist(this);
3606 }
3607 
3608 // convenience function
3609 // return false if the init contains any stores already
3610 bool AllocateNode::maybe_set_complete(PhaseGVN* phase) {
3611   InitializeNode* init = initialization();
<span class="line-modified">3612   if (init == NULL || init-&gt;is_complete()) {</span>
<span class="line-added">3613     return false;</span>
<span class="line-added">3614   }</span>
3615   init-&gt;remove_extra_zeroes();
3616   // for now, if this allocation has already collected any inits, bail:
3617   if (init-&gt;is_non_zero())  return false;
3618   init-&gt;set_complete(phase);
3619   return true;
3620 }
3621 
3622 void InitializeNode::remove_extra_zeroes() {
3623   if (req() == RawStores)  return;
3624   Node* zmem = zero_memory();
3625   uint fill = RawStores;
3626   for (uint i = fill; i &lt; req(); i++) {
3627     Node* n = in(i);
3628     if (n-&gt;is_top() || n == zmem)  continue;  // skip
3629     if (fill &lt; i)  set_req(fill, n);          // compact
3630     ++fill;
3631   }
3632   // delete any empty spaces created:
3633   while (fill &lt; req()) {
3634     del_req(fill);
</pre>
<hr />
<pre>
4352         //   z&#39;s_done      12  16  16  16    12  16    12
4353         //   z&#39;s_needed    12  16  16  16    16  16    16
4354         //   zsize          0   0   0   0     4   0     4
4355         if (next_full_store &lt; 0) {
4356           // Conservative tack:  Zero to end of current word.
4357           zeroes_needed = align_up(zeroes_needed, BytesPerInt);
4358         } else {
4359           // Zero to beginning of next fully initialized word.
4360           // Or, don&#39;t zero at all, if we are already in that word.
4361           assert(next_full_store &gt;= zeroes_needed, &quot;must go forward&quot;);
4362           assert((next_full_store &amp; (BytesPerInt-1)) == 0, &quot;even boundary&quot;);
4363           zeroes_needed = next_full_store;
4364         }
4365       }
4366 
4367       if (zeroes_needed &gt; zeroes_done) {
4368         intptr_t zsize = zeroes_needed - zeroes_done;
4369         // Do some incremental zeroing on rawmem, in parallel with inits.
4370         zeroes_done = align_down(zeroes_done, BytesPerInt);
4371         rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
<span class="line-added">4372                                               allocation()-&gt;in(AllocateNode::DefaultValue),</span>
<span class="line-added">4373                                               allocation()-&gt;in(AllocateNode::RawDefaultValue),</span>
4374                                               zeroes_done, zeroes_needed,
4375                                               phase);
4376         zeroes_done = zeroes_needed;
4377         if (zsize &gt; InitArrayShortSize &amp;&amp; ++big_init_gaps &gt; 2)
4378           do_zeroing = false;   // leave the hole, next time
4379       }
4380     }
4381 
4382     // Collect the store and move on:
4383     phase-&gt;replace_input_of(st, MemNode::Memory, inits);
4384     inits = st;                 // put it on the linearized chain
4385     set_req(i, zmem);           // unhook from previous position
4386 
4387     if (zeroes_done == st_off)
4388       zeroes_done = next_init_off;
4389 
4390     assert(!do_zeroing || zeroes_done &gt;= next_init_off, &quot;don&#39;t miss any&quot;);
4391 
4392     #ifdef ASSERT
4393     // Various order invariants.  Weaker than stores_are_sane because
</pre>
<hr />
<pre>
4413   remove_extra_zeroes();        // clear out all the zmems left over
4414   add_req(inits);
4415 
4416   if (!(UseTLAB &amp;&amp; ZeroTLAB)) {
4417     // If anything remains to be zeroed, zero it all now.
4418     zeroes_done = align_down(zeroes_done, BytesPerInt);
4419     // if it is the last unused 4 bytes of an instance, forget about it
4420     intptr_t size_limit = phase-&gt;find_intptr_t_con(size_in_bytes, max_jint);
4421     if (zeroes_done + BytesPerLong &gt;= size_limit) {
4422       AllocateNode* alloc = allocation();
4423       assert(alloc != NULL, &quot;must be present&quot;);
4424       if (alloc != NULL &amp;&amp; alloc-&gt;Opcode() == Op_Allocate) {
4425         Node* klass_node = alloc-&gt;in(AllocateNode::KlassNode);
4426         ciKlass* k = phase-&gt;type(klass_node)-&gt;is_klassptr()-&gt;klass();
4427         if (zeroes_done == k-&gt;layout_helper())
4428           zeroes_done = size_limit;
4429       }
4430     }
4431     if (zeroes_done &lt; size_limit) {
4432       rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
<span class="line-added">4433                                             allocation()-&gt;in(AllocateNode::DefaultValue),</span>
<span class="line-added">4434                                             allocation()-&gt;in(AllocateNode::RawDefaultValue),</span>
4435                                             zeroes_done, size_in_bytes, phase);
4436     }
4437   }
4438 
4439   set_complete(phase);
4440   return rawmem;
4441 }
4442 
4443 
4444 #ifdef ASSERT
4445 bool InitializeNode::stores_are_sane(PhaseTransform* phase) {
4446   if (is_complete())
4447     return true;                // stores could be anything at this point
4448   assert(allocation() != NULL, &quot;must be present&quot;);
4449   intptr_t last_off = allocation()-&gt;minimum_header_size();
4450   for (uint i = InitializeNode::RawStores; i &lt; req(); i++) {
4451     Node* st = in(i);
4452     intptr_t st_off = get_store_offset(st, phase);
4453     if (st_off &lt; 0)  continue;  // ignore dead garbage
4454     if (last_off &gt; st_off) {
</pre>
</td>
</tr>
</table>
<center><a href="macro.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stringopts.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>