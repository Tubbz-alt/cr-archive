<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/compile.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c2_globals.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="graphKit.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/compile.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  51 #include &quot;opto/divnode.hpp&quot;
  52 #include &quot;opto/escape.hpp&quot;
  53 #include &quot;opto/idealGraphPrinter.hpp&quot;
  54 #include &quot;opto/loopnode.hpp&quot;
  55 #include &quot;opto/machnode.hpp&quot;
  56 #include &quot;opto/macro.hpp&quot;
  57 #include &quot;opto/matcher.hpp&quot;
  58 #include &quot;opto/mathexactnode.hpp&quot;
  59 #include &quot;opto/memnode.hpp&quot;
  60 #include &quot;opto/mulnode.hpp&quot;
  61 #include &quot;opto/narrowptrnode.hpp&quot;
  62 #include &quot;opto/node.hpp&quot;
  63 #include &quot;opto/opcodes.hpp&quot;
  64 #include &quot;opto/output.hpp&quot;
  65 #include &quot;opto/parse.hpp&quot;
  66 #include &quot;opto/phaseX.hpp&quot;
  67 #include &quot;opto/rootnode.hpp&quot;
  68 #include &quot;opto/runtime.hpp&quot;
  69 #include &quot;opto/stringopts.hpp&quot;
  70 #include &quot;opto/type.hpp&quot;

  71 #include &quot;opto/vectornode.hpp&quot;
  72 #include &quot;runtime/arguments.hpp&quot;
  73 #include &quot;runtime/sharedRuntime.hpp&quot;
  74 #include &quot;runtime/signature.hpp&quot;
  75 #include &quot;runtime/stubRoutines.hpp&quot;
  76 #include &quot;runtime/timer.hpp&quot;
  77 #include &quot;utilities/align.hpp&quot;
  78 #include &quot;utilities/copy.hpp&quot;
  79 #include &quot;utilities/macros.hpp&quot;
  80 #include &quot;utilities/resourceHash.hpp&quot;
  81 
  82 
  83 // -------------------- Compile::mach_constant_base_node -----------------------
  84 // Constant table base node singleton.
  85 MachConstantBaseNode* Compile::mach_constant_base_node() {
  86   if (_mach_constant_base_node == NULL) {
  87     _mach_constant_base_node = new MachConstantBaseNode();
  88     _mach_constant_base_node-&gt;add_req(C-&gt;root());
  89   }
  90   return _mach_constant_base_node;
</pre>
<hr />
<pre>
 388   for (int i = range_check_cast_count() - 1; i &gt;= 0; i--) {
 389     Node* cast = range_check_cast_node(i);
 390     if (!useful.member(cast)) {
 391       remove_range_check_cast(cast);
 392     }
 393   }
 394   // Remove useless expensive nodes
 395   for (int i = C-&gt;expensive_count()-1; i &gt;= 0; i--) {
 396     Node* n = C-&gt;expensive_node(i);
 397     if (!useful.member(n)) {
 398       remove_expensive_node(n);
 399     }
 400   }
 401   // Remove useless Opaque4 nodes
 402   for (int i = opaque4_count() - 1; i &gt;= 0; i--) {
 403     Node* opaq = opaque4_node(i);
 404     if (!useful.member(opaq)) {
 405       remove_opaque4_node(opaq);
 406     }
 407   }




 408   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 409   bs-&gt;eliminate_useless_gc_barriers(useful, this);
 410   // clean up the late inline lists
 411   remove_useless_late_inlines(&amp;_string_late_inlines, useful);
 412   remove_useless_late_inlines(&amp;_boxing_late_inlines, useful);
 413   remove_useless_late_inlines(&amp;_late_inlines, useful);
 414   debug_only(verify_graph_edges(true/*check for no_dead_code*/);)
 415 }
 416 
 417 // ============================================================================
 418 //------------------------------CompileWrapper---------------------------------
 419 class CompileWrapper : public StackObj {
 420   Compile *const _compile;
 421  public:
 422   CompileWrapper(Compile* compile);
 423 
 424   ~CompileWrapper();
 425 };
 426 
 427 CompileWrapper::CompileWrapper(Compile* compile) : _compile(compile) {
</pre>
<hr />
<pre>
 620   // Node list that Iterative GVN will start with
 621   Unique_Node_List for_igvn(comp_arena());
 622   set_for_igvn(&amp;for_igvn);
 623 
 624   // GVN that will be run immediately on new nodes
 625   uint estimated_size = method()-&gt;code_size()*4+64;
 626   estimated_size = (estimated_size &lt; MINIMUM_NODE_HASH ? MINIMUM_NODE_HASH : estimated_size);
 627   PhaseGVN gvn(node_arena(), estimated_size);
 628   set_initial_gvn(&amp;gvn);
 629 
 630   print_inlining_init();
 631   { // Scope for timing the parser
 632     TracePhase tp(&quot;parse&quot;, &amp;timers[_t_parser]);
 633 
 634     // Put top into the hash table ASAP.
 635     initial_gvn()-&gt;transform_no_reclaim(top());
 636 
 637     // Set up tf(), start(), and find a CallGenerator.
 638     CallGenerator* cg = NULL;
 639     if (is_osr_compilation()) {
<span class="line-modified"> 640       const TypeTuple *domain = StartOSRNode::osr_domain();</span>
<span class="line-modified"> 641       const TypeTuple *range = TypeTuple::make_range(method()-&gt;signature());</span>
<span class="line-removed"> 642       init_tf(TypeFunc::make(domain, range));</span>
<span class="line-removed"> 643       StartNode* s = new StartOSRNode(root(), domain);</span>
 644       initial_gvn()-&gt;set_type_bottom(s);
 645       init_start(s);
 646       cg = CallGenerator::for_osr(method(), entry_bci());
 647     } else {
 648       // Normal case.
 649       init_tf(TypeFunc::make(method()));
<span class="line-modified"> 650       StartNode* s = new StartNode(root(), tf()-&gt;domain());</span>
 651       initial_gvn()-&gt;set_type_bottom(s);
 652       init_start(s);
 653       if (method()-&gt;intrinsic_id() == vmIntrinsics::_Reference_get) {
 654         // With java.lang.ref.reference.get() we must go through the
 655         // intrinsic - even when get() is the root
 656         // method of the compile - so that, if necessary, the value in
 657         // the referent field of the reference object gets recorded by
 658         // the pre-barrier code.
 659         cg = find_intrinsic(method(), false);
 660       }
 661       if (cg == NULL) {
 662         float past_uses = method()-&gt;interpreter_invocation_count();
 663         float expected_uses = past_uses;
 664         cg = CallGenerator::for_inline(method(), expected_uses);
 665       }
 666     }
 667     if (failing())  return;
 668     if (cg == NULL) {
 669       record_method_not_compilable(&quot;cannot parse method&quot;);
 670       return;
</pre>
<hr />
<pre>
 755     }
 756   }
 757 #endif
 758 
 759 #ifdef ASSERT
 760   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 761   bs-&gt;verify_gc_barriers(this, BarrierSetC2::BeforeCodeGen);
 762 #endif
 763 
 764   // Dump compilation data to replay it.
 765   if (directive-&gt;DumpReplayOption) {
 766     env()-&gt;dump_replay_data(_compile_id);
 767   }
 768   if (directive-&gt;DumpInlineOption &amp;&amp; (ilt() != NULL)) {
 769     env()-&gt;dump_inline_data(_compile_id);
 770   }
 771 
 772   // Now that we know the size of all the monitors we can add a fixed slot
 773   // for the original deopt pc.
 774   int next_slot = fixed_slots() + (sizeof(address) / VMRegImpl::stack_slot_size);




 775   set_fixed_slots(next_slot);
 776 
 777   // Compute when to use implicit null checks. Used by matching trap based
 778   // nodes and NullCheck optimization.
 779   set_allowed_deopt_reasons();
 780 
 781   // Now generate code
 782   Code_Gen();
 783 }
 784 
 785 //------------------------------Compile----------------------------------------
 786 // Compile a runtime stub
 787 Compile::Compile( ciEnv* ci_env,
 788                   TypeFunc_generator generator,
 789                   address stub_function,
 790                   const char *stub_name,
 791                   int is_fancy_jump,
 792                   bool pass_tls,
 793                   bool save_arg_registers,
 794                   bool return_pc,
</pre>
<hr />
<pre>
 910   // Create Debug Information Recorder to record scopes, oopmaps, etc.
 911   env()-&gt;set_oop_recorder(new OopRecorder(env()-&gt;arena()));
 912   env()-&gt;set_debug_info(new DebugInformationRecorder(env()-&gt;oop_recorder()));
 913   env()-&gt;set_dependencies(new Dependencies(env()));
 914 
 915   _fixed_slots = 0;
 916   set_has_split_ifs(false);
 917   set_has_loops(has_method() &amp;&amp; method()-&gt;has_loops()); // first approximation
 918   set_has_stringbuilder(false);
 919   set_has_boxed_value(false);
 920   _trap_can_recompile = false;  // no traps emitted yet
 921   _major_progress = true; // start out assuming good things will happen
 922   set_has_unsafe_access(false);
 923   set_max_vector_size(0);
 924   set_clear_upper_avx(false);  //false as default for clear upper bits of ymm registers
 925   Copy::zero_to_bytes(_trap_hist, sizeof(_trap_hist));
 926   set_decompile_count(0);
 927 
 928   set_do_freq_based_layout(_directive-&gt;BlockLayoutByFrequencyOption);
 929   _loop_opts_cnt = LoopOptsCount;



 930   set_do_inlining(Inline);
 931   set_max_inline_size(MaxInlineSize);
 932   set_freq_inline_size(FreqInlineSize);
 933   set_do_scheduling(OptoScheduling);
 934   set_do_count_invocations(false);
 935   set_do_method_data_update(false);
 936 
 937   set_do_vector_loop(false);
 938 
 939   if (AllowVectorizeOnDemand) {
 940     if (has_method() &amp;&amp; (_directive-&gt;VectorizeOption || _directive-&gt;VectorizeDebugOption)) {
 941       set_do_vector_loop(true);
 942       NOT_PRODUCT(if (do_vector_loop() &amp;&amp; Verbose) {tty-&gt;print(&quot;Compile::Init: do vectorized loops (SIMD like) for method %s\n&quot;,  method()-&gt;name()-&gt;as_quoted_ascii());})
 943     } else if (has_method() &amp;&amp; method()-&gt;name() != 0 &amp;&amp;
 944                method()-&gt;intrinsic_id() == vmIntrinsics::_forEachRemaining) {
 945       set_do_vector_loop(true);
 946     }
 947   }
 948   set_use_cmove(UseCMoveUnconditionally /* || do_vector_loop()*/); //TODO: consider do_vector_loop() mandate use_cmove unconditionally
 949   NOT_PRODUCT(if (use_cmove() &amp;&amp; Verbose &amp;&amp; has_method()) {tty-&gt;print(&quot;Compile::Init: use CMove without profitability tests for method %s\n&quot;,  method()-&gt;name()-&gt;as_quoted_ascii());})
</pre>
<hr />
<pre>
 993   Copy::zero_to_bytes(ats, sizeof(AliasType)*grow_ats);
 994   {
 995     for (int i = 0; i &lt; grow_ats; i++)  _alias_types[i] = &amp;ats[i];
 996   }
 997   // Initialize the first few types.
 998   _alias_types[AliasIdxTop]-&gt;Init(AliasIdxTop, NULL);
 999   _alias_types[AliasIdxBot]-&gt;Init(AliasIdxBot, TypePtr::BOTTOM);
1000   _alias_types[AliasIdxRaw]-&gt;Init(AliasIdxRaw, TypeRawPtr::BOTTOM);
1001   _num_alias_types = AliasIdxRaw+1;
1002   // Zero out the alias type cache.
1003   Copy::zero_to_bytes(_alias_cache, sizeof(_alias_cache));
1004   // A NULL adr_type hits in the cache right away.  Preload the right answer.
1005   probe_alias_cache(NULL)-&gt;_index = AliasIdxTop;
1006 
1007   _intrinsics = NULL;
1008   _macro_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1009   _predicate_opaqs = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1010   _expensive_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1011   _range_check_casts = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1012   _opaque4_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);

1013   register_library_intrinsics();
1014 #ifdef ASSERT
1015   _type_verify_symmetry = true;
1016 #endif
1017 }
1018 
1019 //---------------------------init_start----------------------------------------
1020 // Install the StartNode on this compile object.
1021 void Compile::init_start(StartNode* s) {
1022   if (failing())
1023     return; // already failing
1024   assert(s == start(), &quot;&quot;);
1025 }
1026 
1027 /**
1028  * Return the &#39;StartNode&#39;. We must not have a pending failure, since the ideal graph
1029  * can be in an inconsistent state, i.e., we can get segmentation faults when traversing
1030  * the ideal graph.
1031  */
1032 StartNode* Compile::start() const {
</pre>
<hr />
<pre>
1220 bool Compile::allow_range_check_smearing() const {
1221   // If this method has already thrown a range-check,
1222   // assume it was because we already tried range smearing
1223   // and it failed.
1224   uint already_trapped = trap_count(Deoptimization::Reason_range_check);
1225   return !already_trapped;
1226 }
1227 
1228 
1229 //------------------------------flatten_alias_type-----------------------------
1230 const TypePtr *Compile::flatten_alias_type( const TypePtr *tj ) const {
1231   int offset = tj-&gt;offset();
1232   TypePtr::PTR ptr = tj-&gt;ptr();
1233 
1234   // Known instance (scalarizable allocation) alias only with itself.
1235   bool is_known_inst = tj-&gt;isa_oopptr() != NULL &amp;&amp;
1236                        tj-&gt;is_oopptr()-&gt;is_known_instance();
1237 
1238   // Process weird unsafe references.
1239   if (offset == Type::OffsetBot &amp;&amp; (tj-&gt;isa_instptr() /*|| tj-&gt;isa_klassptr()*/)) {
<span class="line-modified">1240     assert(InlineUnsafeOps, &quot;indeterminate pointers come only from unsafe ops&quot;);</span>

1241     assert(!is_known_inst, &quot;scalarizable allocation should not have unsafe references&quot;);
1242     tj = TypeOopPtr::BOTTOM;
1243     ptr = tj-&gt;ptr();
1244     offset = tj-&gt;offset();
1245   }
1246 
1247   // Array pointers need some flattening
1248   const TypeAryPtr *ta = tj-&gt;isa_aryptr();
1249   if (ta &amp;&amp; ta-&gt;is_stable()) {
1250     // Erase stability property for alias analysis.
1251     tj = ta = ta-&gt;cast_to_stable(false);
1252   }









1253   if( ta &amp;&amp; is_known_inst ) {
1254     if ( offset != Type::OffsetBot &amp;&amp;
1255          offset &gt; arrayOopDesc::length_offset_in_bytes() ) {
1256       offset = Type::OffsetBot; // Flatten constant access into array body only
<span class="line-modified">1257       tj = ta = TypeAryPtr::make(ptr, ta-&gt;ary(), ta-&gt;klass(), true, offset, ta-&gt;instance_id());</span>
1258     }
1259   } else if( ta &amp;&amp; _AliasLevel &gt;= 2 ) {
1260     // For arrays indexed by constant indices, we flatten the alias
1261     // space to include all of the array body.  Only the header, klass
1262     // and array length can be accessed un-aliased.


1263     if( offset != Type::OffsetBot ) {
1264       if( ta-&gt;const_oop() ) { // MethodData* or Method*
1265         offset = Type::OffsetBot;   // Flatten constant access into array body
<span class="line-modified">1266         tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),ta-&gt;ary(),ta-&gt;klass(),false,offset);</span>
1267       } else if( offset == arrayOopDesc::length_offset_in_bytes() ) {
1268         // range is OK as-is.
1269         tj = ta = TypeAryPtr::RANGE;
1270       } else if( offset == oopDesc::klass_offset_in_bytes() ) {
1271         tj = TypeInstPtr::KLASS; // all klass loads look alike
1272         ta = TypeAryPtr::RANGE; // generic ignored junk
1273         ptr = TypePtr::BotPTR;
1274       } else if( offset == oopDesc::mark_offset_in_bytes() ) {
1275         tj = TypeInstPtr::MARK;
1276         ta = TypeAryPtr::RANGE; // generic ignored junk
1277         ptr = TypePtr::BotPTR;
1278       } else {                  // Random constant offset into array body
1279         offset = Type::OffsetBot;   // Flatten constant access into array body
<span class="line-modified">1280         tj = ta = TypeAryPtr::make(ptr,ta-&gt;ary(),ta-&gt;klass(),false,offset);</span>
1281       }
1282     }
1283     // Arrays of fixed size alias with arrays of unknown size.
1284     if (ta-&gt;size() != TypeInt::POS) {
1285       const TypeAry *tary = TypeAry::make(ta-&gt;elem(), TypeInt::POS);
<span class="line-modified">1286       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,ta-&gt;klass(),false,offset);</span>
1287     }
1288     // Arrays of known objects become arrays of unknown objects.
1289     if (ta-&gt;elem()-&gt;isa_narrowoop() &amp;&amp; ta-&gt;elem() != TypeNarrowOop::BOTTOM) {
1290       const TypeAry *tary = TypeAry::make(TypeNarrowOop::BOTTOM, ta-&gt;size());
<span class="line-modified">1291       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,offset);</span>
1292     }
1293     if (ta-&gt;elem()-&gt;isa_oopptr() &amp;&amp; ta-&gt;elem() != TypeInstPtr::BOTTOM) {
1294       const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta-&gt;size());
<span class="line-modified">1295       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,offset);</span>





1296     }
1297     // Arrays of bytes and of booleans both use &#39;bastore&#39; and &#39;baload&#39; so
1298     // cannot be distinguished by bytecode alone.
1299     if (ta-&gt;elem() == TypeInt::BOOL) {
1300       const TypeAry *tary = TypeAry::make(TypeInt::BYTE, ta-&gt;size());
1301       ciKlass* aklass = ciTypeArrayKlass::make(T_BYTE);
<span class="line-modified">1302       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,aklass,false,offset);</span>
1303     }
1304     // During the 2nd round of IterGVN, NotNull castings are removed.
1305     // Make sure the Bottom and NotNull variants alias the same.
1306     // Also, make sure exact and non-exact variants alias the same.
1307     if (ptr == TypePtr::NotNull || ta-&gt;klass_is_exact() || ta-&gt;speculative() != NULL) {
<span class="line-modified">1308       tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta-&gt;ary(),ta-&gt;klass(),false,offset);</span>
1309     }
1310   }
1311 
1312   // Oop pointers need some flattening
1313   const TypeInstPtr *to = tj-&gt;isa_instptr();
1314   if( to &amp;&amp; _AliasLevel &gt;= 2 &amp;&amp; to != TypeOopPtr::BOTTOM ) {
1315     ciInstanceKlass *k = to-&gt;klass()-&gt;as_instance_klass();
1316     if( ptr == TypePtr::Constant ) {
1317       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass() ||
1318           offset &lt; k-&gt;size_helper() * wordSize) {
1319         // No constant oop pointers (such as Strings); they alias with
1320         // unknown strings.
1321         assert(!is_known_inst, &quot;not scalarizable allocation&quot;);
<span class="line-modified">1322         tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,offset);</span>
1323       }
1324     } else if( is_known_inst ) {
1325       tj = to; // Keep NotNull and klass_is_exact for instance type
1326     } else if( ptr == TypePtr::NotNull || to-&gt;klass_is_exact() ) {
1327       // During the 2nd round of IterGVN, NotNull castings are removed.
1328       // Make sure the Bottom and NotNull variants alias the same.
1329       // Also, make sure exact and non-exact variants alias the same.
<span class="line-modified">1330       tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,offset);</span>
1331     }
1332     if (to-&gt;speculative() != NULL) {
<span class="line-modified">1333       tj = to = TypeInstPtr::make(to-&gt;ptr(),to-&gt;klass(),to-&gt;klass_is_exact(),to-&gt;const_oop(),to-&gt;offset(), to-&gt;instance_id());</span>
1334     }
1335     // Canonicalize the holder of this field
1336     if (offset &gt;= 0 &amp;&amp; offset &lt; instanceOopDesc::base_offset_in_bytes()) {
1337       // First handle header references such as a LoadKlassNode, even if the
1338       // object&#39;s klass is unloaded at compile time (4965979).
1339       if (!is_known_inst) { // Do it only for non-instance types
<span class="line-modified">1340         tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()-&gt;Object_klass(), false, NULL, offset);</span>
1341       }
1342     } else if (offset &lt; 0 || offset &gt;= k-&gt;size_helper() * wordSize) {
1343       // Static fields are in the space above the normal instance
1344       // fields in the java.lang.Class instance.
1345       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass()) {
1346         to = NULL;
1347         tj = TypeOopPtr::BOTTOM;
1348         offset = tj-&gt;offset();
1349       }
1350     } else {
1351       ciInstanceKlass *canonical_holder = k-&gt;get_canonical_holder(offset);
1352       if (!k-&gt;equals(canonical_holder) || tj-&gt;offset() != offset) {
1353         if( is_known_inst ) {
<span class="line-modified">1354           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, true, NULL, offset, to-&gt;instance_id());</span>
1355         } else {
<span class="line-modified">1356           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, false, NULL, offset);</span>
1357         }
1358       }
1359     }
1360   }
1361 
1362   // Klass pointers to object array klasses need some flattening
1363   const TypeKlassPtr *tk = tj-&gt;isa_klassptr();
1364   if( tk ) {
1365     // If we are referencing a field within a Klass, we need
1366     // to assume the worst case of an Object.  Both exact and
1367     // inexact types must flatten to the same alias class so
1368     // use NotNull as the PTR.
1369     if ( offset == Type::OffsetBot || (offset &gt;= 0 &amp;&amp; (size_t)offset &lt; sizeof(Klass)) ) {
1370 
1371       tj = tk = TypeKlassPtr::make(TypePtr::NotNull,
1372                                    TypeKlassPtr::OBJECT-&gt;klass(),
<span class="line-modified">1373                                    offset);</span>

1374     }
1375 
1376     ciKlass* klass = tk-&gt;klass();
<span class="line-modified">1377     if( klass-&gt;is_obj_array_klass() ) {</span>
1378       ciKlass* k = TypeAryPtr::OOPS-&gt;klass();
1379       if( !k || !k-&gt;is_loaded() )                  // Only fails for some -Xcomp runs
1380         k = TypeInstPtr::BOTTOM-&gt;klass();
<span class="line-modified">1381       tj = tk = TypeKlassPtr::make( TypePtr::NotNull, k, offset );</span>
1382     }
1383 
1384     // Check for precise loads from the primary supertype array and force them
1385     // to the supertype cache alias index.  Check for generic array loads from
1386     // the primary supertype array and also force them to the supertype cache
1387     // alias index.  Since the same load can reach both, we need to merge
1388     // these 2 disparate memories into the same alias class.  Since the
1389     // primary supertype array is read-only, there&#39;s no chance of confusion
1390     // where we bypass an array load and an array store.
1391     int primary_supers_offset = in_bytes(Klass::primary_supers_offset());
1392     if (offset == Type::OffsetBot ||
1393         (offset &gt;= primary_supers_offset &amp;&amp;
1394          offset &lt; (int)(primary_supers_offset + Klass::primary_super_limit() * wordSize)) ||
1395         offset == (int)in_bytes(Klass::secondary_super_cache_offset())) {
1396       offset = in_bytes(Klass::secondary_super_cache_offset());
<span class="line-modified">1397       tj = tk = TypeKlassPtr::make( TypePtr::NotNull, tk-&gt;klass(), offset );</span>
1398     }
1399   }
1400 
1401   // Flatten all Raw pointers together.
1402   if (tj-&gt;base() == Type::RawPtr)
1403     tj = TypeRawPtr::BOTTOM;
1404 
1405   if (tj-&gt;base() == Type::AnyPtr)
1406     tj = TypePtr::BOTTOM;      // An error, which the caller must check for.
1407 
1408   // Flatten all to bottom for now
1409   switch( _AliasLevel ) {
1410   case 0:
1411     tj = TypePtr::BOTTOM;
1412     break;
1413   case 1:                       // Flatten to: oop, static, field or array
1414     switch (tj-&gt;base()) {
1415     //case Type::AryPtr: tj = TypeAryPtr::RANGE;    break;
1416     case Type::RawPtr:   tj = TypeRawPtr::BOTTOM;   break;
1417     case Type::AryPtr:   // do not distinguish arrays at all
</pre>
<hr />
<pre>
1516   intptr_t key = (intptr_t) adr_type;
1517   key ^= key &gt;&gt; logAliasCacheSize;
1518   return &amp;_alias_cache[key &amp; right_n_bits(logAliasCacheSize)];
1519 }
1520 
1521 
1522 //-----------------------------grow_alias_types--------------------------------
1523 void Compile::grow_alias_types() {
1524   const int old_ats  = _max_alias_types; // how many before?
1525   const int new_ats  = old_ats;          // how many more?
1526   const int grow_ats = old_ats+new_ats;  // how many now?
1527   _max_alias_types = grow_ats;
1528   _alias_types =  REALLOC_ARENA_ARRAY(comp_arena(), AliasType*, _alias_types, old_ats, grow_ats);
1529   AliasType* ats =    NEW_ARENA_ARRAY(comp_arena(), AliasType, new_ats);
1530   Copy::zero_to_bytes(ats, sizeof(AliasType)*new_ats);
1531   for (int i = 0; i &lt; new_ats; i++)  _alias_types[old_ats+i] = &amp;ats[i];
1532 }
1533 
1534 
1535 //--------------------------------find_alias_type------------------------------
<span class="line-modified">1536 Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {</span>
1537   if (_AliasLevel == 0)
1538     return alias_type(AliasIdxBot);
1539 
<span class="line-modified">1540   AliasCacheEntry* ace = probe_alias_cache(adr_type);</span>
<span class="line-modified">1541   if (ace-&gt;_adr_type == adr_type) {</span>
<span class="line-modified">1542     return alias_type(ace-&gt;_index);</span>



1543   }
1544 
1545   // Handle special cases.
1546   if (adr_type == NULL)             return alias_type(AliasIdxTop);
1547   if (adr_type == TypePtr::BOTTOM)  return alias_type(AliasIdxBot);
1548 
1549   // Do it the slow way.
1550   const TypePtr* flat = flatten_alias_type(adr_type);
1551 
1552 #ifdef ASSERT
1553   {
1554     ResourceMark rm;
1555     assert(flat == flatten_alias_type(flat), &quot;not idempotent: adr_type = %s; flat = %s =&gt; %s&quot;,
1556            Type::str(adr_type), Type::str(flat), Type::str(flatten_alias_type(flat)));
1557     assert(flat != TypePtr::BOTTOM, &quot;cannot alias-analyze an untyped ptr: adr_type = %s&quot;,
1558            Type::str(adr_type));
1559     if (flat-&gt;isa_oopptr() &amp;&amp; !flat-&gt;isa_klassptr()) {
1560       const TypeOopPtr* foop = flat-&gt;is_oopptr();
1561       // Scalarizable allocations have exact klass always.
1562       bool exact = !foop-&gt;klass_is_exact() || foop-&gt;is_known_instance();
</pre>
<hr />
<pre>
1572     if (alias_type(i)-&gt;adr_type() == flat) {
1573       idx = i;
1574       break;
1575     }
1576   }
1577 
1578   if (idx == AliasIdxTop) {
1579     if (no_create)  return NULL;
1580     // Grow the array if necessary.
1581     if (_num_alias_types == _max_alias_types)  grow_alias_types();
1582     // Add a new alias type.
1583     idx = _num_alias_types++;
1584     _alias_types[idx]-&gt;Init(idx, flat);
1585     if (flat == TypeInstPtr::KLASS)  alias_type(idx)-&gt;set_rewritable(false);
1586     if (flat == TypeAryPtr::RANGE)   alias_type(idx)-&gt;set_rewritable(false);
1587     if (flat-&gt;isa_instptr()) {
1588       if (flat-&gt;offset() == java_lang_Class::klass_offset()
1589           &amp;&amp; flat-&gt;is_instptr()-&gt;klass() == env()-&gt;Class_klass())
1590         alias_type(idx)-&gt;set_rewritable(false);
1591     }

1592     if (flat-&gt;isa_aryptr()) {
1593 #ifdef ASSERT
1594       const int header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1595       // (T_BYTE has the weakest alignment and size restrictions...)
1596       assert(flat-&gt;offset() &lt; header_size_min, &quot;array body reference must be OffsetBot&quot;);
1597 #endif

1598       if (flat-&gt;offset() == TypePtr::OffsetBot) {
<span class="line-modified">1599         alias_type(idx)-&gt;set_element(flat-&gt;is_aryptr()-&gt;elem());</span>








1600       }
1601     }
1602     if (flat-&gt;isa_klassptr()) {
1603       if (flat-&gt;offset() == in_bytes(Klass::super_check_offset_offset()))
1604         alias_type(idx)-&gt;set_rewritable(false);
1605       if (flat-&gt;offset() == in_bytes(Klass::modifier_flags_offset()))
1606         alias_type(idx)-&gt;set_rewritable(false);
1607       if (flat-&gt;offset() == in_bytes(Klass::access_flags_offset()))
1608         alias_type(idx)-&gt;set_rewritable(false);
1609       if (flat-&gt;offset() == in_bytes(Klass::java_mirror_offset()))
1610         alias_type(idx)-&gt;set_rewritable(false);


1611       if (flat-&gt;offset() == in_bytes(Klass::secondary_super_cache_offset()))
1612         alias_type(idx)-&gt;set_rewritable(false);
1613     }
1614     // %%% (We would like to finalize JavaThread::threadObj_offset(),
1615     // but the base pointer type is not distinctive enough to identify
1616     // references into JavaThread.)
1617 
1618     // Check for final fields.
1619     const TypeInstPtr* tinst = flat-&gt;isa_instptr();
1620     if (tinst &amp;&amp; tinst-&gt;offset() &gt;= instanceOopDesc::base_offset_in_bytes()) {
<span class="line-removed">1621       ciField* field;</span>
1622       if (tinst-&gt;const_oop() != NULL &amp;&amp;
1623           tinst-&gt;klass() == ciEnv::current()-&gt;Class_klass() &amp;&amp;
1624           tinst-&gt;offset() &gt;= (tinst-&gt;klass()-&gt;as_instance_klass()-&gt;size_helper() * wordSize)) {
1625         // static field
1626         ciInstanceKlass* k = tinst-&gt;const_oop()-&gt;as_instance()-&gt;java_lang_Class_klass()-&gt;as_instance_klass();
1627         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), true);




1628       } else {
<span class="line-modified">1629         ciInstanceKlass *k = tinst-&gt;klass()-&gt;as_instance_klass();</span>
1630         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1631       }
<span class="line-modified">1632       assert(field == NULL ||</span>
<span class="line-modified">1633              original_field == NULL ||</span>
<span class="line-modified">1634              (field-&gt;holder() == original_field-&gt;holder() &amp;&amp;</span>
<span class="line-modified">1635               field-&gt;offset() == original_field-&gt;offset() &amp;&amp;</span>
<span class="line-modified">1636               field-&gt;is_static() == original_field-&gt;is_static()), &quot;wrong field?&quot;);</span>
<span class="line-modified">1637       // Set field() and is_rewritable() attributes.</span>
<span class="line-modified">1638       if (field != NULL)  alias_type(idx)-&gt;set_field(field);</span>







1639     }
1640   }
1641 
1642   // Fill the cache for next time.
<span class="line-modified">1643   ace-&gt;_adr_type = adr_type;</span>
<span class="line-modified">1644   ace-&gt;_index    = idx;</span>
<span class="line-modified">1645   assert(alias_type(adr_type) == alias_type(idx),  &quot;type must be installed&quot;);</span>

1646 
<span class="line-modified">1647   // Might as well try to fill the cache for the flattened version, too.</span>
<span class="line-modified">1648   AliasCacheEntry* face = probe_alias_cache(flat);</span>
<span class="line-modified">1649   if (face-&gt;_adr_type == NULL) {</span>
<span class="line-modified">1650     face-&gt;_adr_type = flat;</span>
<span class="line-modified">1651     face-&gt;_index    = idx;</span>
<span class="line-modified">1652     assert(alias_type(flat) == alias_type(idx), &quot;flat type must work too&quot;);</span>

1653   }
1654 
1655   return alias_type(idx);
1656 }
1657 
1658 
1659 Compile::AliasType* Compile::alias_type(ciField* field) {
1660   const TypeOopPtr* t;
1661   if (field-&gt;is_static())
1662     t = TypeInstPtr::make(field-&gt;holder()-&gt;java_mirror());
1663   else
1664     t = TypeOopPtr::make_from_klass_raw(field-&gt;holder());
1665   AliasType* atp = alias_type(t-&gt;add_offset(field-&gt;offset_in_bytes()), field);
1666   assert((field-&gt;is_final() || field-&gt;is_stable()) == !atp-&gt;is_rewritable(), &quot;must get the rewritable bits correct&quot;);
1667   return atp;
1668 }
1669 
1670 
1671 //------------------------------have_alias_type--------------------------------
1672 bool Compile::have_alias_type(const TypePtr* adr_type) {
</pre>
<hr />
<pre>
1794   }
1795   assert(range_check_cast_count() == 0, &quot;should be empty&quot;);
1796 }
1797 
1798 void Compile::add_opaque4_node(Node* n) {
1799   assert(n-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1800   assert(!_opaque4_nodes-&gt;contains(n), &quot;duplicate entry in Opaque4 list&quot;);
1801   _opaque4_nodes-&gt;append(n);
1802 }
1803 
1804 // Remove all Opaque4 nodes.
1805 void Compile::remove_opaque4_nodes(PhaseIterGVN &amp;igvn) {
1806   for (int i = opaque4_count(); i &gt; 0; i--) {
1807     Node* opaq = opaque4_node(i-1);
1808     assert(opaq-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1809     igvn.replace_node(opaq, opaq-&gt;in(2));
1810   }
1811   assert(opaque4_count() == 0, &quot;should be empty&quot;);
1812 }
1813 





















































































































































































































































































































































1814 // StringOpts and late inlining of string methods
1815 void Compile::inline_string_calls(bool parse_time) {
1816   {
1817     // remove useless nodes to make the usage analysis simpler
1818     ResourceMark rm;
1819     PhaseRemoveUseless pru(initial_gvn(), for_igvn());
1820   }
1821 
1822   {
1823     ResourceMark rm;
1824     print_method(PHASE_BEFORE_STRINGOPTS, 3);
1825     PhaseStringOpts pso(initial_gvn(), for_igvn());
1826     print_method(PHASE_AFTER_STRINGOPTS, 3);
1827   }
1828 
1829   // now inline anything that we skipped the first time around
1830   if (!parse_time) {
1831     _late_inlines_pos = _late_inlines.length();
1832   }
1833 
</pre>
<hr />
<pre>
2073   remove_speculative_types(igvn);
2074 
2075   // No more new expensive nodes will be added to the list from here
2076   // so keep only the actual candidates for optimizations.
2077   cleanup_expensive_nodes(igvn);
2078 
2079   if (!failing() &amp;&amp; RenumberLiveNodes &amp;&amp; live_nodes() + NodeLimitFudgeFactor &lt; unique()) {
2080     Compile::TracePhase tp(&quot;&quot;, &amp;timers[_t_renumberLive]);
2081     initial_gvn()-&gt;replace_with(&amp;igvn);
2082     for_igvn()-&gt;clear();
2083     Unique_Node_List new_worklist(C-&gt;comp_arena());
2084     {
2085       ResourceMark rm;
2086       PhaseRenumberLive prl = PhaseRenumberLive(initial_gvn(), for_igvn(), &amp;new_worklist);
2087     }
2088     set_for_igvn(&amp;new_worklist);
2089     igvn = PhaseIterGVN(initial_gvn());
2090     igvn.optimize();
2091   }
2092 







2093   // Perform escape analysis
2094   if (_do_escape_analysis &amp;&amp; ConnectionGraph::has_candidates(this)) {
2095     if (has_loops()) {
2096       // Cleanup graph (remove dead nodes).
2097       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2098       PhaseIdealLoop::optimize(igvn, LoopOptsMaxUnroll);
2099       if (major_progress()) print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);
2100       if (failing())  return;
2101     }
2102     ConnectionGraph::do_analysis(this, &amp;igvn);
2103 
2104     if (failing())  return;
2105 
2106     // Optimize out fields loads from scalar replaceable allocations.
2107     igvn.optimize();
2108     print_method(PHASE_ITER_GVN_AFTER_EA, 2);
2109 
2110     if (failing())  return;
2111 
2112     if (congraph() != NULL &amp;&amp; macro_count() &gt; 0) {
</pre>
<hr />
<pre>
2747             // Accumulate any precedence edges
2748             if (mem-&gt;in(i) != NULL) {
2749               n-&gt;add_prec(mem-&gt;in(i));
2750             }
2751           }
2752           // Everything above this point has been processed.
2753           done = true;
2754         }
2755         // Eliminate the previous StoreCM
2756         prev-&gt;set_req(MemNode::Memory, mem-&gt;in(MemNode::Memory));
2757         assert(mem-&gt;outcnt() == 0, &quot;should be dead&quot;);
2758         mem-&gt;disconnect_inputs(NULL, this);
2759       } else {
2760         prev = mem;
2761       }
2762       mem = prev-&gt;in(MemNode::Memory);
2763     }
2764   }
2765 }
2766 

2767 //------------------------------final_graph_reshaping_impl----------------------
2768 // Implement items 1-5 from final_graph_reshaping below.
2769 void Compile::final_graph_reshaping_impl( Node *n, Final_Reshape_Counts &amp;frc) {
2770 
2771   if ( n-&gt;outcnt() == 0 ) return; // dead node
2772   uint nop = n-&gt;Opcode();
2773 
2774   // Check for 2-input instruction with &quot;last use&quot; on right input.
2775   // Swap to left input.  Implements item (2).
2776   if( n-&gt;req() == 3 &amp;&amp;          // two-input instruction
2777       n-&gt;in(1)-&gt;outcnt() &gt; 1 &amp;&amp; // left use is NOT a last use
2778       (!n-&gt;in(1)-&gt;is_Phi() || n-&gt;in(1)-&gt;in(2) != n) &amp;&amp; // it is not data loop
2779       n-&gt;in(2)-&gt;outcnt() == 1 &amp;&amp;// right use IS a last use
2780       !n-&gt;in(2)-&gt;is_Con() ) {   // right use is not a constant
2781     // Check for commutative opcode
2782     switch( nop ) {
2783     case Op_AddI:  case Op_AddF:  case Op_AddD:  case Op_AddL:
2784     case Op_MaxI:  case Op_MinI:
2785     case Op_MulI:  case Op_MulF:  case Op_MulD:  case Op_MulL:
2786     case Op_AndL:  case Op_XorL:  case Op_OrL:
</pre>
<hr />
<pre>
3485           // Replace all nodes with identical edges as m with m
3486           k-&gt;subsume_by(m, this);
3487         }
3488       }
3489     }
3490     break;
3491   }
3492   case Op_CmpUL: {
3493     if (!Matcher::has_match_rule(Op_CmpUL)) {
3494       // No support for unsigned long comparisons
3495       ConINode* sign_pos = new ConINode(TypeInt::make(BitsPerLong - 1));
3496       Node* sign_bit_mask = new RShiftLNode(n-&gt;in(1), sign_pos);
3497       Node* orl = new OrLNode(n-&gt;in(1), sign_bit_mask);
3498       ConLNode* remove_sign_mask = new ConLNode(TypeLong::make(max_jlong));
3499       Node* andl = new AndLNode(orl, remove_sign_mask);
3500       Node* cmp = new CmpLNode(andl, n-&gt;in(2));
3501       n-&gt;subsume_by(cmp, this);
3502     }
3503     break;
3504   }








3505   default:
3506     assert(!n-&gt;is_Call(), &quot;&quot;);
3507     assert(!n-&gt;is_Mem(), &quot;&quot;);
3508     assert(nop != Op_ProfileBoolean, &quot;should be eliminated during IGVN&quot;);
3509     break;
3510   }
3511 }
3512 
3513 //------------------------------final_graph_reshaping_walk---------------------
3514 // Replacing Opaque nodes with their input in final_graph_reshaping_impl(),
3515 // requires that the walk visits a node&#39;s inputs before visiting the node.
3516 void Compile::final_graph_reshaping_walk( Node_Stack &amp;nstack, Node *root, Final_Reshape_Counts &amp;frc ) {
3517   ResourceArea *area = Thread::current()-&gt;resource_area();
3518   Unique_Node_List sfpt(area);
3519 
3520   frc._visited.set(root-&gt;_idx); // first, mark node as visited
3521   uint cnt = root-&gt;req();
3522   Node *n = root;
3523   uint  i = 0;
3524   while (true) {
</pre>
<hr />
<pre>
3833   }
3834 }
3835 
3836 bool Compile::needs_clinit_barrier(ciMethod* method, ciMethod* accessing_method) {
3837   return method-&gt;is_static() &amp;&amp; needs_clinit_barrier(method-&gt;holder(), accessing_method);
3838 }
3839 
3840 bool Compile::needs_clinit_barrier(ciField* field, ciMethod* accessing_method) {
3841   return field-&gt;is_static() &amp;&amp; needs_clinit_barrier(field-&gt;holder(), accessing_method);
3842 }
3843 
3844 bool Compile::needs_clinit_barrier(ciInstanceKlass* holder, ciMethod* accessing_method) {
3845   if (holder-&gt;is_initialized()) {
3846     return false;
3847   }
3848   if (holder-&gt;is_being_initialized()) {
3849     if (accessing_method-&gt;holder() == holder) {
3850       // Access inside a class. The barrier can be elided when access happens in &lt;clinit&gt;,
3851       // &lt;init&gt;, or a static method. In all those cases, there was an initialization
3852       // barrier on the holder klass passed.
<span class="line-modified">3853       if (accessing_method-&gt;is_static_initializer() ||</span>
<span class="line-modified">3854           accessing_method-&gt;is_object_initializer() ||</span>
3855           accessing_method-&gt;is_static()) {
3856         return false;
3857       }
3858     } else if (accessing_method-&gt;holder()-&gt;is_subclass_of(holder)) {
3859       // Access from a subclass. The barrier can be elided only when access happens in &lt;clinit&gt;.
3860       // In case of &lt;init&gt; or a static method, the barrier is on the subclass is not enough:
3861       // child class can become fully initialized while its parent class is still being initialized.
<span class="line-modified">3862       if (accessing_method-&gt;is_static_initializer()) {</span>
3863         return false;
3864       }
3865     }
3866     ciMethod* root = method(); // the root method of compilation
3867     if (root != accessing_method) {
3868       return needs_clinit_barrier(holder, root); // check access in the context of compilation root
3869     }
3870   }
3871   return true;
3872 }
3873 
3874 #ifndef PRODUCT
3875 //------------------------------verify_graph_edges---------------------------
3876 // Walk the Graph and verify that there is a one-to-one correspondence
3877 // between Use-Def edges and Def-Use edges in the graph.
3878 void Compile::verify_graph_edges(bool no_dead_code) {
3879   if (VerifyGraphEdges) {
3880     ResourceArea *area = Thread::current()-&gt;resource_area();
3881     Unique_Node_List visited(area);
3882     // Call recursive graph walk to check edges
</pre>
<hr />
<pre>
3964                   _phase_name, C-&gt;unique(), C-&gt;live_nodes(), C-&gt;count_live_nodes_by_graph_walk());
3965   }
3966 
3967   if (VerifyIdealNodeCount) {
3968     Compile::current()-&gt;print_missing_nodes();
3969   }
3970 #endif
3971 
3972   if (_log != NULL) {
3973     _log-&gt;done(&quot;phase name=&#39;%s&#39; nodes=&#39;%d&#39; live=&#39;%d&#39;&quot;, _phase_name, C-&gt;unique(), C-&gt;live_nodes());
3974   }
3975 }
3976 
3977 //----------------------------static_subtype_check-----------------------------
3978 // Shortcut important common cases when superklass is exact:
3979 // (0) superklass is java.lang.Object (can occur in reflective code)
3980 // (1) subklass is already limited to a subtype of superklass =&gt; always ok
3981 // (2) subklass does not overlap with superklass =&gt; always fail
3982 // (3) superklass has NO subtypes and we can check with a simple compare.
3983 int Compile::static_subtype_check(ciKlass* superk, ciKlass* subk) {
<span class="line-modified">3984   if (StressReflectiveCode) {</span>
3985     return SSC_full_test;       // Let caller generate the general case.
3986   }
3987 
3988   if (superk == env()-&gt;Object_klass()) {
3989     return SSC_always_true;     // (0) this test cannot fail
3990   }
3991 
3992   ciType* superelem = superk;
<span class="line-modified">3993   if (superelem-&gt;is_array_klass())</span>

3994     superelem = superelem-&gt;as_array_klass()-&gt;base_element_type();

3995 
3996   if (!subk-&gt;is_interface()) {  // cannot trust static interface types yet
3997     if (subk-&gt;is_subtype_of(superk)) {
3998       return SSC_always_true;   // (1) false path dead; no dynamic test needed
3999     }
4000     if (!(superelem-&gt;is_klass() &amp;&amp; superelem-&gt;as_klass()-&gt;is_interface()) &amp;&amp;
4001         !superk-&gt;is_subtype_of(subk)) {
4002       return SSC_always_false;
4003     }
4004   }
4005 
4006   // If casting to an instance klass, it must have no subtypes
4007   if (superk-&gt;is_interface()) {
4008     // Cannot trust interfaces yet.
4009     // %%% S.B. superk-&gt;nof_implementors() == 1
4010   } else if (superelem-&gt;is_instance_klass()) {
4011     ciInstanceKlass* ik = superelem-&gt;as_instance_klass();
4012     if (!ik-&gt;has_subklass() &amp;&amp; !ik-&gt;is_interface()) {
4013       if (!ik-&gt;is_final()) {
4014         // Add a dependency if there is a chance of a later subclass.
</pre>
<hr />
<pre>
4435     for (uint next = 0; next &lt; worklist.size(); ++next) {
4436       Node *n  = worklist.at(next);
4437       const Type* t = igvn.type_or_null(n);
4438       assert((t == NULL) || (t == t-&gt;remove_speculative()), &quot;no more speculative types&quot;);
4439       if (n-&gt;is_Type()) {
4440         t = n-&gt;as_Type()-&gt;type();
4441         assert(t == t-&gt;remove_speculative(), &quot;no more speculative types&quot;);
4442       }
4443       uint max = n-&gt;len();
4444       for( uint i = 0; i &lt; max; ++i ) {
4445         Node *m = n-&gt;in(i);
4446         if (not_a_node(m))  continue;
4447         worklist.push(m);
4448       }
4449     }
4450     igvn.check_no_speculative_types();
4451 #endif
4452   }
4453 }
4454 





















4455 // Auxiliary method to support randomized stressing/fuzzing.
4456 //
4457 // This method can be called the arbitrary number of times, with current count
4458 // as the argument. The logic allows selecting a single candidate from the
4459 // running list of candidates as follows:
4460 //    int count = 0;
4461 //    Cand* selected = null;
4462 //    while(cand = cand-&gt;next()) {
4463 //      if (randomized_select(++count)) {
4464 //        selected = cand;
4465 //      }
4466 //    }
4467 //
4468 // Including count equalizes the chances any candidate is &quot;selected&quot;.
4469 // This is useful when we don&#39;t have the complete list of candidates to choose
4470 // from uniformly. In this case, we need to adjust the randomicity of the
4471 // selection, or else we will end up biasing the selection towards the latter
4472 // candidates.
4473 //
4474 // Quick back-envelope calculation shows that for the list of n candidates
</pre>
</td>
<td>
<hr />
<pre>
  51 #include &quot;opto/divnode.hpp&quot;
  52 #include &quot;opto/escape.hpp&quot;
  53 #include &quot;opto/idealGraphPrinter.hpp&quot;
  54 #include &quot;opto/loopnode.hpp&quot;
  55 #include &quot;opto/machnode.hpp&quot;
  56 #include &quot;opto/macro.hpp&quot;
  57 #include &quot;opto/matcher.hpp&quot;
  58 #include &quot;opto/mathexactnode.hpp&quot;
  59 #include &quot;opto/memnode.hpp&quot;
  60 #include &quot;opto/mulnode.hpp&quot;
  61 #include &quot;opto/narrowptrnode.hpp&quot;
  62 #include &quot;opto/node.hpp&quot;
  63 #include &quot;opto/opcodes.hpp&quot;
  64 #include &quot;opto/output.hpp&quot;
  65 #include &quot;opto/parse.hpp&quot;
  66 #include &quot;opto/phaseX.hpp&quot;
  67 #include &quot;opto/rootnode.hpp&quot;
  68 #include &quot;opto/runtime.hpp&quot;
  69 #include &quot;opto/stringopts.hpp&quot;
  70 #include &quot;opto/type.hpp&quot;
<span class="line-added">  71 #include &quot;opto/valuetypenode.hpp&quot;</span>
  72 #include &quot;opto/vectornode.hpp&quot;
  73 #include &quot;runtime/arguments.hpp&quot;
  74 #include &quot;runtime/sharedRuntime.hpp&quot;
  75 #include &quot;runtime/signature.hpp&quot;
  76 #include &quot;runtime/stubRoutines.hpp&quot;
  77 #include &quot;runtime/timer.hpp&quot;
  78 #include &quot;utilities/align.hpp&quot;
  79 #include &quot;utilities/copy.hpp&quot;
  80 #include &quot;utilities/macros.hpp&quot;
  81 #include &quot;utilities/resourceHash.hpp&quot;
  82 
  83 
  84 // -------------------- Compile::mach_constant_base_node -----------------------
  85 // Constant table base node singleton.
  86 MachConstantBaseNode* Compile::mach_constant_base_node() {
  87   if (_mach_constant_base_node == NULL) {
  88     _mach_constant_base_node = new MachConstantBaseNode();
  89     _mach_constant_base_node-&gt;add_req(C-&gt;root());
  90   }
  91   return _mach_constant_base_node;
</pre>
<hr />
<pre>
 389   for (int i = range_check_cast_count() - 1; i &gt;= 0; i--) {
 390     Node* cast = range_check_cast_node(i);
 391     if (!useful.member(cast)) {
 392       remove_range_check_cast(cast);
 393     }
 394   }
 395   // Remove useless expensive nodes
 396   for (int i = C-&gt;expensive_count()-1; i &gt;= 0; i--) {
 397     Node* n = C-&gt;expensive_node(i);
 398     if (!useful.member(n)) {
 399       remove_expensive_node(n);
 400     }
 401   }
 402   // Remove useless Opaque4 nodes
 403   for (int i = opaque4_count() - 1; i &gt;= 0; i--) {
 404     Node* opaq = opaque4_node(i);
 405     if (!useful.member(opaq)) {
 406       remove_opaque4_node(opaq);
 407     }
 408   }
<span class="line-added"> 409   // Remove useless value type nodes</span>
<span class="line-added"> 410   if (_value_type_nodes != NULL) {</span>
<span class="line-added"> 411     _value_type_nodes-&gt;remove_useless_nodes(useful.member_set());</span>
<span class="line-added"> 412   }</span>
 413   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 414   bs-&gt;eliminate_useless_gc_barriers(useful, this);
 415   // clean up the late inline lists
 416   remove_useless_late_inlines(&amp;_string_late_inlines, useful);
 417   remove_useless_late_inlines(&amp;_boxing_late_inlines, useful);
 418   remove_useless_late_inlines(&amp;_late_inlines, useful);
 419   debug_only(verify_graph_edges(true/*check for no_dead_code*/);)
 420 }
 421 
 422 // ============================================================================
 423 //------------------------------CompileWrapper---------------------------------
 424 class CompileWrapper : public StackObj {
 425   Compile *const _compile;
 426  public:
 427   CompileWrapper(Compile* compile);
 428 
 429   ~CompileWrapper();
 430 };
 431 
 432 CompileWrapper::CompileWrapper(Compile* compile) : _compile(compile) {
</pre>
<hr />
<pre>
 625   // Node list that Iterative GVN will start with
 626   Unique_Node_List for_igvn(comp_arena());
 627   set_for_igvn(&amp;for_igvn);
 628 
 629   // GVN that will be run immediately on new nodes
 630   uint estimated_size = method()-&gt;code_size()*4+64;
 631   estimated_size = (estimated_size &lt; MINIMUM_NODE_HASH ? MINIMUM_NODE_HASH : estimated_size);
 632   PhaseGVN gvn(node_arena(), estimated_size);
 633   set_initial_gvn(&amp;gvn);
 634 
 635   print_inlining_init();
 636   { // Scope for timing the parser
 637     TracePhase tp(&quot;parse&quot;, &amp;timers[_t_parser]);
 638 
 639     // Put top into the hash table ASAP.
 640     initial_gvn()-&gt;transform_no_reclaim(top());
 641 
 642     // Set up tf(), start(), and find a CallGenerator.
 643     CallGenerator* cg = NULL;
 644     if (is_osr_compilation()) {
<span class="line-modified"> 645       init_tf(TypeFunc::make(method(), /* is_osr_compilation = */ true));</span>
<span class="line-modified"> 646       StartNode* s = new StartOSRNode(root(), tf()-&gt;domain_sig());</span>


 647       initial_gvn()-&gt;set_type_bottom(s);
 648       init_start(s);
 649       cg = CallGenerator::for_osr(method(), entry_bci());
 650     } else {
 651       // Normal case.
 652       init_tf(TypeFunc::make(method()));
<span class="line-modified"> 653       StartNode* s = new StartNode(root(), tf()-&gt;domain_cc());</span>
 654       initial_gvn()-&gt;set_type_bottom(s);
 655       init_start(s);
 656       if (method()-&gt;intrinsic_id() == vmIntrinsics::_Reference_get) {
 657         // With java.lang.ref.reference.get() we must go through the
 658         // intrinsic - even when get() is the root
 659         // method of the compile - so that, if necessary, the value in
 660         // the referent field of the reference object gets recorded by
 661         // the pre-barrier code.
 662         cg = find_intrinsic(method(), false);
 663       }
 664       if (cg == NULL) {
 665         float past_uses = method()-&gt;interpreter_invocation_count();
 666         float expected_uses = past_uses;
 667         cg = CallGenerator::for_inline(method(), expected_uses);
 668       }
 669     }
 670     if (failing())  return;
 671     if (cg == NULL) {
 672       record_method_not_compilable(&quot;cannot parse method&quot;);
 673       return;
</pre>
<hr />
<pre>
 758     }
 759   }
 760 #endif
 761 
 762 #ifdef ASSERT
 763   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 764   bs-&gt;verify_gc_barriers(this, BarrierSetC2::BeforeCodeGen);
 765 #endif
 766 
 767   // Dump compilation data to replay it.
 768   if (directive-&gt;DumpReplayOption) {
 769     env()-&gt;dump_replay_data(_compile_id);
 770   }
 771   if (directive-&gt;DumpInlineOption &amp;&amp; (ilt() != NULL)) {
 772     env()-&gt;dump_inline_data(_compile_id);
 773   }
 774 
 775   // Now that we know the size of all the monitors we can add a fixed slot
 776   // for the original deopt pc.
 777   int next_slot = fixed_slots() + (sizeof(address) / VMRegImpl::stack_slot_size);
<span class="line-added"> 778   if (needs_stack_repair()) {</span>
<span class="line-added"> 779     // One extra slot for the special stack increment value</span>
<span class="line-added"> 780     next_slot += 2;</span>
<span class="line-added"> 781   }</span>
 782   set_fixed_slots(next_slot);
 783 
 784   // Compute when to use implicit null checks. Used by matching trap based
 785   // nodes and NullCheck optimization.
 786   set_allowed_deopt_reasons();
 787 
 788   // Now generate code
 789   Code_Gen();
 790 }
 791 
 792 //------------------------------Compile----------------------------------------
 793 // Compile a runtime stub
 794 Compile::Compile( ciEnv* ci_env,
 795                   TypeFunc_generator generator,
 796                   address stub_function,
 797                   const char *stub_name,
 798                   int is_fancy_jump,
 799                   bool pass_tls,
 800                   bool save_arg_registers,
 801                   bool return_pc,
</pre>
<hr />
<pre>
 917   // Create Debug Information Recorder to record scopes, oopmaps, etc.
 918   env()-&gt;set_oop_recorder(new OopRecorder(env()-&gt;arena()));
 919   env()-&gt;set_debug_info(new DebugInformationRecorder(env()-&gt;oop_recorder()));
 920   env()-&gt;set_dependencies(new Dependencies(env()));
 921 
 922   _fixed_slots = 0;
 923   set_has_split_ifs(false);
 924   set_has_loops(has_method() &amp;&amp; method()-&gt;has_loops()); // first approximation
 925   set_has_stringbuilder(false);
 926   set_has_boxed_value(false);
 927   _trap_can_recompile = false;  // no traps emitted yet
 928   _major_progress = true; // start out assuming good things will happen
 929   set_has_unsafe_access(false);
 930   set_max_vector_size(0);
 931   set_clear_upper_avx(false);  //false as default for clear upper bits of ymm registers
 932   Copy::zero_to_bytes(_trap_hist, sizeof(_trap_hist));
 933   set_decompile_count(0);
 934 
 935   set_do_freq_based_layout(_directive-&gt;BlockLayoutByFrequencyOption);
 936   _loop_opts_cnt = LoopOptsCount;
<span class="line-added"> 937   _has_flattened_accesses = false;</span>
<span class="line-added"> 938   _flattened_accesses_share_alias = true;</span>
<span class="line-added"> 939 </span>
 940   set_do_inlining(Inline);
 941   set_max_inline_size(MaxInlineSize);
 942   set_freq_inline_size(FreqInlineSize);
 943   set_do_scheduling(OptoScheduling);
 944   set_do_count_invocations(false);
 945   set_do_method_data_update(false);
 946 
 947   set_do_vector_loop(false);
 948 
 949   if (AllowVectorizeOnDemand) {
 950     if (has_method() &amp;&amp; (_directive-&gt;VectorizeOption || _directive-&gt;VectorizeDebugOption)) {
 951       set_do_vector_loop(true);
 952       NOT_PRODUCT(if (do_vector_loop() &amp;&amp; Verbose) {tty-&gt;print(&quot;Compile::Init: do vectorized loops (SIMD like) for method %s\n&quot;,  method()-&gt;name()-&gt;as_quoted_ascii());})
 953     } else if (has_method() &amp;&amp; method()-&gt;name() != 0 &amp;&amp;
 954                method()-&gt;intrinsic_id() == vmIntrinsics::_forEachRemaining) {
 955       set_do_vector_loop(true);
 956     }
 957   }
 958   set_use_cmove(UseCMoveUnconditionally /* || do_vector_loop()*/); //TODO: consider do_vector_loop() mandate use_cmove unconditionally
 959   NOT_PRODUCT(if (use_cmove() &amp;&amp; Verbose &amp;&amp; has_method()) {tty-&gt;print(&quot;Compile::Init: use CMove without profitability tests for method %s\n&quot;,  method()-&gt;name()-&gt;as_quoted_ascii());})
</pre>
<hr />
<pre>
1003   Copy::zero_to_bytes(ats, sizeof(AliasType)*grow_ats);
1004   {
1005     for (int i = 0; i &lt; grow_ats; i++)  _alias_types[i] = &amp;ats[i];
1006   }
1007   // Initialize the first few types.
1008   _alias_types[AliasIdxTop]-&gt;Init(AliasIdxTop, NULL);
1009   _alias_types[AliasIdxBot]-&gt;Init(AliasIdxBot, TypePtr::BOTTOM);
1010   _alias_types[AliasIdxRaw]-&gt;Init(AliasIdxRaw, TypeRawPtr::BOTTOM);
1011   _num_alias_types = AliasIdxRaw+1;
1012   // Zero out the alias type cache.
1013   Copy::zero_to_bytes(_alias_cache, sizeof(_alias_cache));
1014   // A NULL adr_type hits in the cache right away.  Preload the right answer.
1015   probe_alias_cache(NULL)-&gt;_index = AliasIdxTop;
1016 
1017   _intrinsics = NULL;
1018   _macro_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1019   _predicate_opaqs = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1020   _expensive_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1021   _range_check_casts = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1022   _opaque4_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
<span class="line-added">1023   _value_type_nodes = new (comp_arena()) Unique_Node_List(comp_arena());</span>
1024   register_library_intrinsics();
1025 #ifdef ASSERT
1026   _type_verify_symmetry = true;
1027 #endif
1028 }
1029 
1030 //---------------------------init_start----------------------------------------
1031 // Install the StartNode on this compile object.
1032 void Compile::init_start(StartNode* s) {
1033   if (failing())
1034     return; // already failing
1035   assert(s == start(), &quot;&quot;);
1036 }
1037 
1038 /**
1039  * Return the &#39;StartNode&#39;. We must not have a pending failure, since the ideal graph
1040  * can be in an inconsistent state, i.e., we can get segmentation faults when traversing
1041  * the ideal graph.
1042  */
1043 StartNode* Compile::start() const {
</pre>
<hr />
<pre>
1231 bool Compile::allow_range_check_smearing() const {
1232   // If this method has already thrown a range-check,
1233   // assume it was because we already tried range smearing
1234   // and it failed.
1235   uint already_trapped = trap_count(Deoptimization::Reason_range_check);
1236   return !already_trapped;
1237 }
1238 
1239 
1240 //------------------------------flatten_alias_type-----------------------------
1241 const TypePtr *Compile::flatten_alias_type( const TypePtr *tj ) const {
1242   int offset = tj-&gt;offset();
1243   TypePtr::PTR ptr = tj-&gt;ptr();
1244 
1245   // Known instance (scalarizable allocation) alias only with itself.
1246   bool is_known_inst = tj-&gt;isa_oopptr() != NULL &amp;&amp;
1247                        tj-&gt;is_oopptr()-&gt;is_known_instance();
1248 
1249   // Process weird unsafe references.
1250   if (offset == Type::OffsetBot &amp;&amp; (tj-&gt;isa_instptr() /*|| tj-&gt;isa_klassptr()*/)) {
<span class="line-modified">1251     bool default_value_load = EnableValhalla &amp;&amp; tj-&gt;is_instptr()-&gt;klass() == ciEnv::current()-&gt;Class_klass();</span>
<span class="line-added">1252     assert(InlineUnsafeOps || default_value_load, &quot;indeterminate pointers come only from unsafe ops&quot;);</span>
1253     assert(!is_known_inst, &quot;scalarizable allocation should not have unsafe references&quot;);
1254     tj = TypeOopPtr::BOTTOM;
1255     ptr = tj-&gt;ptr();
1256     offset = tj-&gt;offset();
1257   }
1258 
1259   // Array pointers need some flattening
1260   const TypeAryPtr *ta = tj-&gt;isa_aryptr();
1261   if (ta &amp;&amp; ta-&gt;is_stable()) {
1262     // Erase stability property for alias analysis.
1263     tj = ta = ta-&gt;cast_to_stable(false);
1264   }
<span class="line-added">1265   if (ta &amp;&amp; ta-&gt;is_not_flat()) {</span>
<span class="line-added">1266     // Erase not flat property for alias analysis.</span>
<span class="line-added">1267     tj = ta = ta-&gt;cast_to_not_flat(false);</span>
<span class="line-added">1268   }</span>
<span class="line-added">1269   if (ta &amp;&amp; ta-&gt;is_not_null_free()) {</span>
<span class="line-added">1270     // Erase not null free property for alias analysis.</span>
<span class="line-added">1271     tj = ta = ta-&gt;cast_to_not_null_free(false);</span>
<span class="line-added">1272   }</span>
<span class="line-added">1273 </span>
1274   if( ta &amp;&amp; is_known_inst ) {
1275     if ( offset != Type::OffsetBot &amp;&amp;
1276          offset &gt; arrayOopDesc::length_offset_in_bytes() ) {
1277       offset = Type::OffsetBot; // Flatten constant access into array body only
<span class="line-modified">1278       tj = ta = TypeAryPtr::make(ptr, ta-&gt;ary(), ta-&gt;klass(), true, Type::Offset(offset), ta-&gt;field_offset(), ta-&gt;instance_id());</span>
1279     }
1280   } else if( ta &amp;&amp; _AliasLevel &gt;= 2 ) {
1281     // For arrays indexed by constant indices, we flatten the alias
1282     // space to include all of the array body.  Only the header, klass
1283     // and array length can be accessed un-aliased.
<span class="line-added">1284     // For flattened value type array, each field has its own slice so</span>
<span class="line-added">1285     // we must include the field offset.</span>
1286     if( offset != Type::OffsetBot ) {
1287       if( ta-&gt;const_oop() ) { // MethodData* or Method*
1288         offset = Type::OffsetBot;   // Flatten constant access into array body
<span class="line-modified">1289         tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());</span>
1290       } else if( offset == arrayOopDesc::length_offset_in_bytes() ) {
1291         // range is OK as-is.
1292         tj = ta = TypeAryPtr::RANGE;
1293       } else if( offset == oopDesc::klass_offset_in_bytes() ) {
1294         tj = TypeInstPtr::KLASS; // all klass loads look alike
1295         ta = TypeAryPtr::RANGE; // generic ignored junk
1296         ptr = TypePtr::BotPTR;
1297       } else if( offset == oopDesc::mark_offset_in_bytes() ) {
1298         tj = TypeInstPtr::MARK;
1299         ta = TypeAryPtr::RANGE; // generic ignored junk
1300         ptr = TypePtr::BotPTR;
1301       } else {                  // Random constant offset into array body
1302         offset = Type::OffsetBot;   // Flatten constant access into array body
<span class="line-modified">1303         tj = ta = TypeAryPtr::make(ptr,ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());</span>
1304       }
1305     }
1306     // Arrays of fixed size alias with arrays of unknown size.
1307     if (ta-&gt;size() != TypeInt::POS) {
1308       const TypeAry *tary = TypeAry::make(ta-&gt;elem(), TypeInt::POS);
<span class="line-modified">1309       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());</span>
1310     }
1311     // Arrays of known objects become arrays of unknown objects.
1312     if (ta-&gt;elem()-&gt;isa_narrowoop() &amp;&amp; ta-&gt;elem() != TypeNarrowOop::BOTTOM) {
1313       const TypeAry *tary = TypeAry::make(TypeNarrowOop::BOTTOM, ta-&gt;size());
<span class="line-modified">1314       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), ta-&gt;field_offset());</span>
1315     }
1316     if (ta-&gt;elem()-&gt;isa_oopptr() &amp;&amp; ta-&gt;elem() != TypeInstPtr::BOTTOM) {
1317       const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta-&gt;size());
<span class="line-modified">1318       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), ta-&gt;field_offset());</span>
<span class="line-added">1319     }</span>
<span class="line-added">1320     // Initially all flattened array accesses share a single slice</span>
<span class="line-added">1321     if (ta-&gt;elem()-&gt;isa_valuetype() &amp;&amp; ta-&gt;elem() != TypeValueType::BOTTOM &amp;&amp; _flattened_accesses_share_alias) {</span>
<span class="line-added">1322       const TypeAry *tary = TypeAry::make(TypeValueType::BOTTOM, ta-&gt;size());</span>
<span class="line-added">1323       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));</span>
1324     }
1325     // Arrays of bytes and of booleans both use &#39;bastore&#39; and &#39;baload&#39; so
1326     // cannot be distinguished by bytecode alone.
1327     if (ta-&gt;elem() == TypeInt::BOOL) {
1328       const TypeAry *tary = TypeAry::make(TypeInt::BYTE, ta-&gt;size());
1329       ciKlass* aklass = ciTypeArrayKlass::make(T_BYTE);
<span class="line-modified">1330       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,aklass,false,Type::Offset(offset), ta-&gt;field_offset());</span>
1331     }
1332     // During the 2nd round of IterGVN, NotNull castings are removed.
1333     // Make sure the Bottom and NotNull variants alias the same.
1334     // Also, make sure exact and non-exact variants alias the same.
1335     if (ptr == TypePtr::NotNull || ta-&gt;klass_is_exact() || ta-&gt;speculative() != NULL) {
<span class="line-modified">1336       tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());</span>
1337     }
1338   }
1339 
1340   // Oop pointers need some flattening
1341   const TypeInstPtr *to = tj-&gt;isa_instptr();
1342   if( to &amp;&amp; _AliasLevel &gt;= 2 &amp;&amp; to != TypeOopPtr::BOTTOM ) {
1343     ciInstanceKlass *k = to-&gt;klass()-&gt;as_instance_klass();
1344     if( ptr == TypePtr::Constant ) {
1345       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass() ||
1346           offset &lt; k-&gt;size_helper() * wordSize) {
1347         // No constant oop pointers (such as Strings); they alias with
1348         // unknown strings.
1349         assert(!is_known_inst, &quot;not scalarizable allocation&quot;);
<span class="line-modified">1350         tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,Type::Offset(offset), to-&gt;klass()-&gt;flatten_array());</span>
1351       }
1352     } else if( is_known_inst ) {
1353       tj = to; // Keep NotNull and klass_is_exact for instance type
1354     } else if( ptr == TypePtr::NotNull || to-&gt;klass_is_exact() ) {
1355       // During the 2nd round of IterGVN, NotNull castings are removed.
1356       // Make sure the Bottom and NotNull variants alias the same.
1357       // Also, make sure exact and non-exact variants alias the same.
<span class="line-modified">1358       tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,Type::Offset(offset), to-&gt;klass()-&gt;flatten_array());</span>
1359     }
1360     if (to-&gt;speculative() != NULL) {
<span class="line-modified">1361       tj = to = TypeInstPtr::make(to-&gt;ptr(),to-&gt;klass(),to-&gt;klass_is_exact(),to-&gt;const_oop(),Type::Offset(to-&gt;offset()), to-&gt;klass()-&gt;flatten_array(), to-&gt;instance_id());</span>
1362     }
1363     // Canonicalize the holder of this field
1364     if (offset &gt;= 0 &amp;&amp; offset &lt; instanceOopDesc::base_offset_in_bytes()) {
1365       // First handle header references such as a LoadKlassNode, even if the
1366       // object&#39;s klass is unloaded at compile time (4965979).
1367       if (!is_known_inst) { // Do it only for non-instance types
<span class="line-modified">1368         tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()-&gt;Object_klass(), false, NULL, Type::Offset(offset), false);</span>
1369       }
1370     } else if (offset &lt; 0 || offset &gt;= k-&gt;size_helper() * wordSize) {
1371       // Static fields are in the space above the normal instance
1372       // fields in the java.lang.Class instance.
1373       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass()) {
1374         to = NULL;
1375         tj = TypeOopPtr::BOTTOM;
1376         offset = tj-&gt;offset();
1377       }
1378     } else {
1379       ciInstanceKlass *canonical_holder = k-&gt;get_canonical_holder(offset);
1380       if (!k-&gt;equals(canonical_holder) || tj-&gt;offset() != offset) {
1381         if( is_known_inst ) {
<span class="line-modified">1382           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, true, NULL, Type::Offset(offset), canonical_holder-&gt;flatten_array(), to-&gt;instance_id());</span>
1383         } else {
<span class="line-modified">1384           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, false, NULL, Type::Offset(offset), canonical_holder-&gt;flatten_array());</span>
1385         }
1386       }
1387     }
1388   }
1389 
1390   // Klass pointers to object array klasses need some flattening
1391   const TypeKlassPtr *tk = tj-&gt;isa_klassptr();
1392   if( tk ) {
1393     // If we are referencing a field within a Klass, we need
1394     // to assume the worst case of an Object.  Both exact and
1395     // inexact types must flatten to the same alias class so
1396     // use NotNull as the PTR.
1397     if ( offset == Type::OffsetBot || (offset &gt;= 0 &amp;&amp; (size_t)offset &lt; sizeof(Klass)) ) {
1398 
1399       tj = tk = TypeKlassPtr::make(TypePtr::NotNull,
1400                                    TypeKlassPtr::OBJECT-&gt;klass(),
<span class="line-modified">1401                                    Type::Offset(offset),</span>
<span class="line-added">1402                                    false);</span>
1403     }
1404 
1405     ciKlass* klass = tk-&gt;klass();
<span class="line-modified">1406     if (klass != NULL &amp;&amp; klass-&gt;is_obj_array_klass()) {</span>
1407       ciKlass* k = TypeAryPtr::OOPS-&gt;klass();
1408       if( !k || !k-&gt;is_loaded() )                  // Only fails for some -Xcomp runs
1409         k = TypeInstPtr::BOTTOM-&gt;klass();
<span class="line-modified">1410       tj = tk = TypeKlassPtr::make(TypePtr::NotNull, k, Type::Offset(offset), false);</span>
1411     }
1412 
1413     // Check for precise loads from the primary supertype array and force them
1414     // to the supertype cache alias index.  Check for generic array loads from
1415     // the primary supertype array and also force them to the supertype cache
1416     // alias index.  Since the same load can reach both, we need to merge
1417     // these 2 disparate memories into the same alias class.  Since the
1418     // primary supertype array is read-only, there&#39;s no chance of confusion
1419     // where we bypass an array load and an array store.
1420     int primary_supers_offset = in_bytes(Klass::primary_supers_offset());
1421     if (offset == Type::OffsetBot ||
1422         (offset &gt;= primary_supers_offset &amp;&amp;
1423          offset &lt; (int)(primary_supers_offset + Klass::primary_super_limit() * wordSize)) ||
1424         offset == (int)in_bytes(Klass::secondary_super_cache_offset())) {
1425       offset = in_bytes(Klass::secondary_super_cache_offset());
<span class="line-modified">1426       tj = tk = TypeKlassPtr::make(TypePtr::NotNull, tk-&gt;klass(), Type::Offset(offset), tk-&gt;flat_array());</span>
1427     }
1428   }
1429 
1430   // Flatten all Raw pointers together.
1431   if (tj-&gt;base() == Type::RawPtr)
1432     tj = TypeRawPtr::BOTTOM;
1433 
1434   if (tj-&gt;base() == Type::AnyPtr)
1435     tj = TypePtr::BOTTOM;      // An error, which the caller must check for.
1436 
1437   // Flatten all to bottom for now
1438   switch( _AliasLevel ) {
1439   case 0:
1440     tj = TypePtr::BOTTOM;
1441     break;
1442   case 1:                       // Flatten to: oop, static, field or array
1443     switch (tj-&gt;base()) {
1444     //case Type::AryPtr: tj = TypeAryPtr::RANGE;    break;
1445     case Type::RawPtr:   tj = TypeRawPtr::BOTTOM;   break;
1446     case Type::AryPtr:   // do not distinguish arrays at all
</pre>
<hr />
<pre>
1545   intptr_t key = (intptr_t) adr_type;
1546   key ^= key &gt;&gt; logAliasCacheSize;
1547   return &amp;_alias_cache[key &amp; right_n_bits(logAliasCacheSize)];
1548 }
1549 
1550 
1551 //-----------------------------grow_alias_types--------------------------------
1552 void Compile::grow_alias_types() {
1553   const int old_ats  = _max_alias_types; // how many before?
1554   const int new_ats  = old_ats;          // how many more?
1555   const int grow_ats = old_ats+new_ats;  // how many now?
1556   _max_alias_types = grow_ats;
1557   _alias_types =  REALLOC_ARENA_ARRAY(comp_arena(), AliasType*, _alias_types, old_ats, grow_ats);
1558   AliasType* ats =    NEW_ARENA_ARRAY(comp_arena(), AliasType, new_ats);
1559   Copy::zero_to_bytes(ats, sizeof(AliasType)*new_ats);
1560   for (int i = 0; i &lt; new_ats; i++)  _alias_types[old_ats+i] = &amp;ats[i];
1561 }
1562 
1563 
1564 //--------------------------------find_alias_type------------------------------
<span class="line-modified">1565 Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {</span>
1566   if (_AliasLevel == 0)
1567     return alias_type(AliasIdxBot);
1568 
<span class="line-modified">1569   AliasCacheEntry* ace = NULL;</span>
<span class="line-modified">1570   if (!uncached) {</span>
<span class="line-modified">1571     ace = probe_alias_cache(adr_type);</span>
<span class="line-added">1572     if (ace-&gt;_adr_type == adr_type) {</span>
<span class="line-added">1573       return alias_type(ace-&gt;_index);</span>
<span class="line-added">1574     }</span>
1575   }
1576 
1577   // Handle special cases.
1578   if (adr_type == NULL)             return alias_type(AliasIdxTop);
1579   if (adr_type == TypePtr::BOTTOM)  return alias_type(AliasIdxBot);
1580 
1581   // Do it the slow way.
1582   const TypePtr* flat = flatten_alias_type(adr_type);
1583 
1584 #ifdef ASSERT
1585   {
1586     ResourceMark rm;
1587     assert(flat == flatten_alias_type(flat), &quot;not idempotent: adr_type = %s; flat = %s =&gt; %s&quot;,
1588            Type::str(adr_type), Type::str(flat), Type::str(flatten_alias_type(flat)));
1589     assert(flat != TypePtr::BOTTOM, &quot;cannot alias-analyze an untyped ptr: adr_type = %s&quot;,
1590            Type::str(adr_type));
1591     if (flat-&gt;isa_oopptr() &amp;&amp; !flat-&gt;isa_klassptr()) {
1592       const TypeOopPtr* foop = flat-&gt;is_oopptr();
1593       // Scalarizable allocations have exact klass always.
1594       bool exact = !foop-&gt;klass_is_exact() || foop-&gt;is_known_instance();
</pre>
<hr />
<pre>
1604     if (alias_type(i)-&gt;adr_type() == flat) {
1605       idx = i;
1606       break;
1607     }
1608   }
1609 
1610   if (idx == AliasIdxTop) {
1611     if (no_create)  return NULL;
1612     // Grow the array if necessary.
1613     if (_num_alias_types == _max_alias_types)  grow_alias_types();
1614     // Add a new alias type.
1615     idx = _num_alias_types++;
1616     _alias_types[idx]-&gt;Init(idx, flat);
1617     if (flat == TypeInstPtr::KLASS)  alias_type(idx)-&gt;set_rewritable(false);
1618     if (flat == TypeAryPtr::RANGE)   alias_type(idx)-&gt;set_rewritable(false);
1619     if (flat-&gt;isa_instptr()) {
1620       if (flat-&gt;offset() == java_lang_Class::klass_offset()
1621           &amp;&amp; flat-&gt;is_instptr()-&gt;klass() == env()-&gt;Class_klass())
1622         alias_type(idx)-&gt;set_rewritable(false);
1623     }
<span class="line-added">1624     ciField* field = NULL;</span>
1625     if (flat-&gt;isa_aryptr()) {
1626 #ifdef ASSERT
1627       const int header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1628       // (T_BYTE has the weakest alignment and size restrictions...)
1629       assert(flat-&gt;offset() &lt; header_size_min, &quot;array body reference must be OffsetBot&quot;);
1630 #endif
<span class="line-added">1631       const Type* elemtype = flat-&gt;is_aryptr()-&gt;elem();</span>
1632       if (flat-&gt;offset() == TypePtr::OffsetBot) {
<span class="line-modified">1633         alias_type(idx)-&gt;set_element(elemtype);</span>
<span class="line-added">1634       }</span>
<span class="line-added">1635       int field_offset = flat-&gt;is_aryptr()-&gt;field_offset().get();</span>
<span class="line-added">1636       if (elemtype-&gt;isa_valuetype() &amp;&amp;</span>
<span class="line-added">1637           elemtype-&gt;value_klass() != NULL &amp;&amp;</span>
<span class="line-added">1638           field_offset != Type::OffsetBot) {</span>
<span class="line-added">1639         ciValueKlass* vk = elemtype-&gt;value_klass();</span>
<span class="line-added">1640         field_offset += vk-&gt;first_field_offset();</span>
<span class="line-added">1641         field = vk-&gt;get_field_by_offset(field_offset, false);</span>
1642       }
1643     }
1644     if (flat-&gt;isa_klassptr()) {
1645       if (flat-&gt;offset() == in_bytes(Klass::super_check_offset_offset()))
1646         alias_type(idx)-&gt;set_rewritable(false);
1647       if (flat-&gt;offset() == in_bytes(Klass::modifier_flags_offset()))
1648         alias_type(idx)-&gt;set_rewritable(false);
1649       if (flat-&gt;offset() == in_bytes(Klass::access_flags_offset()))
1650         alias_type(idx)-&gt;set_rewritable(false);
1651       if (flat-&gt;offset() == in_bytes(Klass::java_mirror_offset()))
1652         alias_type(idx)-&gt;set_rewritable(false);
<span class="line-added">1653       if (flat-&gt;offset() == in_bytes(Klass::layout_helper_offset()))</span>
<span class="line-added">1654         alias_type(idx)-&gt;set_rewritable(false);</span>
1655       if (flat-&gt;offset() == in_bytes(Klass::secondary_super_cache_offset()))
1656         alias_type(idx)-&gt;set_rewritable(false);
1657     }
1658     // %%% (We would like to finalize JavaThread::threadObj_offset(),
1659     // but the base pointer type is not distinctive enough to identify
1660     // references into JavaThread.)
1661 
1662     // Check for final fields.
1663     const TypeInstPtr* tinst = flat-&gt;isa_instptr();
1664     if (tinst &amp;&amp; tinst-&gt;offset() &gt;= instanceOopDesc::base_offset_in_bytes()) {

1665       if (tinst-&gt;const_oop() != NULL &amp;&amp;
1666           tinst-&gt;klass() == ciEnv::current()-&gt;Class_klass() &amp;&amp;
1667           tinst-&gt;offset() &gt;= (tinst-&gt;klass()-&gt;as_instance_klass()-&gt;size_helper() * wordSize)) {
1668         // static field
1669         ciInstanceKlass* k = tinst-&gt;const_oop()-&gt;as_instance()-&gt;java_lang_Class_klass()-&gt;as_instance_klass();
1670         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), true);
<span class="line-added">1671       } else if (tinst-&gt;klass()-&gt;is_valuetype()) {</span>
<span class="line-added">1672         // Value type field</span>
<span class="line-added">1673         ciValueKlass* vk = tinst-&gt;value_klass();</span>
<span class="line-added">1674         field = vk-&gt;get_field_by_offset(tinst-&gt;offset(), false);</span>
1675       } else {
<span class="line-modified">1676         ciInstanceKlass* k = tinst-&gt;klass()-&gt;as_instance_klass();</span>
1677         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1678       }
<span class="line-modified">1679     }</span>
<span class="line-modified">1680     assert(field == NULL ||</span>
<span class="line-modified">1681            original_field == NULL ||</span>
<span class="line-modified">1682            (field-&gt;holder() == original_field-&gt;holder() &amp;&amp;</span>
<span class="line-modified">1683             field-&gt;offset() == original_field-&gt;offset() &amp;&amp;</span>
<span class="line-modified">1684             field-&gt;is_static() == original_field-&gt;is_static()), &quot;wrong field?&quot;);</span>
<span class="line-modified">1685     // Set field() and is_rewritable() attributes.</span>
<span class="line-added">1686     if (field != NULL) {</span>
<span class="line-added">1687       alias_type(idx)-&gt;set_field(field);</span>
<span class="line-added">1688       if (flat-&gt;isa_aryptr()) {</span>
<span class="line-added">1689         // Fields of flattened inline type arrays are rewritable although they are declared final</span>
<span class="line-added">1690         assert(flat-&gt;is_aryptr()-&gt;elem()-&gt;isa_valuetype(), &quot;must be a flattened value array&quot;);</span>
<span class="line-added">1691         alias_type(idx)-&gt;set_rewritable(true);</span>
<span class="line-added">1692       }</span>
1693     }
1694   }
1695 
1696   // Fill the cache for next time.
<span class="line-modified">1697   if (!uncached) {</span>
<span class="line-modified">1698     ace-&gt;_adr_type = adr_type;</span>
<span class="line-modified">1699     ace-&gt;_index    = idx;</span>
<span class="line-added">1700     assert(alias_type(adr_type) == alias_type(idx),  &quot;type must be installed&quot;);</span>
1701 
<span class="line-modified">1702     // Might as well try to fill the cache for the flattened version, too.</span>
<span class="line-modified">1703     AliasCacheEntry* face = probe_alias_cache(flat);</span>
<span class="line-modified">1704     if (face-&gt;_adr_type == NULL) {</span>
<span class="line-modified">1705       face-&gt;_adr_type = flat;</span>
<span class="line-modified">1706       face-&gt;_index    = idx;</span>
<span class="line-modified">1707       assert(alias_type(flat) == alias_type(idx), &quot;flat type must work too&quot;);</span>
<span class="line-added">1708     }</span>
1709   }
1710 
1711   return alias_type(idx);
1712 }
1713 
1714 
1715 Compile::AliasType* Compile::alias_type(ciField* field) {
1716   const TypeOopPtr* t;
1717   if (field-&gt;is_static())
1718     t = TypeInstPtr::make(field-&gt;holder()-&gt;java_mirror());
1719   else
1720     t = TypeOopPtr::make_from_klass_raw(field-&gt;holder());
1721   AliasType* atp = alias_type(t-&gt;add_offset(field-&gt;offset_in_bytes()), field);
1722   assert((field-&gt;is_final() || field-&gt;is_stable()) == !atp-&gt;is_rewritable(), &quot;must get the rewritable bits correct&quot;);
1723   return atp;
1724 }
1725 
1726 
1727 //------------------------------have_alias_type--------------------------------
1728 bool Compile::have_alias_type(const TypePtr* adr_type) {
</pre>
<hr />
<pre>
1850   }
1851   assert(range_check_cast_count() == 0, &quot;should be empty&quot;);
1852 }
1853 
1854 void Compile::add_opaque4_node(Node* n) {
1855   assert(n-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1856   assert(!_opaque4_nodes-&gt;contains(n), &quot;duplicate entry in Opaque4 list&quot;);
1857   _opaque4_nodes-&gt;append(n);
1858 }
1859 
1860 // Remove all Opaque4 nodes.
1861 void Compile::remove_opaque4_nodes(PhaseIterGVN &amp;igvn) {
1862   for (int i = opaque4_count(); i &gt; 0; i--) {
1863     Node* opaq = opaque4_node(i-1);
1864     assert(opaq-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1865     igvn.replace_node(opaq, opaq-&gt;in(2));
1866   }
1867   assert(opaque4_count() == 0, &quot;should be empty&quot;);
1868 }
1869 
<span class="line-added">1870 void Compile::add_value_type(Node* n) {</span>
<span class="line-added">1871   assert(n-&gt;is_ValueTypeBase(), &quot;unexpected node&quot;);</span>
<span class="line-added">1872   if (_value_type_nodes != NULL) {</span>
<span class="line-added">1873     _value_type_nodes-&gt;push(n);</span>
<span class="line-added">1874   }</span>
<span class="line-added">1875 }</span>
<span class="line-added">1876 </span>
<span class="line-added">1877 void Compile::remove_value_type(Node* n) {</span>
<span class="line-added">1878   assert(n-&gt;is_ValueTypeBase(), &quot;unexpected node&quot;);</span>
<span class="line-added">1879   if (_value_type_nodes != NULL) {</span>
<span class="line-added">1880     _value_type_nodes-&gt;remove(n);</span>
<span class="line-added">1881   }</span>
<span class="line-added">1882 }</span>
<span class="line-added">1883 </span>
<span class="line-added">1884 // Does the return value keep otherwise useless value type allocations</span>
<span class="line-added">1885 // alive?</span>
<span class="line-added">1886 static bool return_val_keeps_allocations_alive(Node* ret_val) {</span>
<span class="line-added">1887   ResourceMark rm;</span>
<span class="line-added">1888   Unique_Node_List wq;</span>
<span class="line-added">1889   wq.push(ret_val);</span>
<span class="line-added">1890   bool some_allocations = false;</span>
<span class="line-added">1891   for (uint i = 0; i &lt; wq.size(); i++) {</span>
<span class="line-added">1892     Node* n = wq.at(i);</span>
<span class="line-added">1893     assert(!n-&gt;is_ValueTypeBase(), &quot;chain of value type nodes&quot;);</span>
<span class="line-added">1894     if (n-&gt;outcnt() &gt; 1) {</span>
<span class="line-added">1895       // Some other use for the allocation</span>
<span class="line-added">1896       return false;</span>
<span class="line-added">1897     } else if (n-&gt;is_Phi()) {</span>
<span class="line-added">1898       for (uint j = 1; j &lt; n-&gt;req(); j++) {</span>
<span class="line-added">1899         wq.push(n-&gt;in(j));</span>
<span class="line-added">1900       }</span>
<span class="line-added">1901     } else if (n-&gt;is_CheckCastPP() &amp;&amp;</span>
<span class="line-added">1902                n-&gt;in(1)-&gt;is_Proj() &amp;&amp;</span>
<span class="line-added">1903                n-&gt;in(1)-&gt;in(0)-&gt;is_Allocate()) {</span>
<span class="line-added">1904       some_allocations = true;</span>
<span class="line-added">1905     }</span>
<span class="line-added">1906   }</span>
<span class="line-added">1907   return some_allocations;</span>
<span class="line-added">1908 }</span>
<span class="line-added">1909 </span>
<span class="line-added">1910 void Compile::process_value_types(PhaseIterGVN &amp;igvn) {</span>
<span class="line-added">1911   // Make value types scalar in safepoints</span>
<span class="line-added">1912   while (_value_type_nodes-&gt;size() != 0) {</span>
<span class="line-added">1913     ValueTypeBaseNode* vt = _value_type_nodes-&gt;pop()-&gt;as_ValueTypeBase();</span>
<span class="line-added">1914     vt-&gt;make_scalar_in_safepoints(&amp;igvn);</span>
<span class="line-added">1915     if (vt-&gt;is_ValueTypePtr()) {</span>
<span class="line-added">1916       igvn.replace_node(vt, vt-&gt;get_oop());</span>
<span class="line-added">1917     } else if (vt-&gt;outcnt() == 0) {</span>
<span class="line-added">1918       igvn.remove_dead_node(vt);</span>
<span class="line-added">1919     }</span>
<span class="line-added">1920   }</span>
<span class="line-added">1921   _value_type_nodes = NULL;</span>
<span class="line-added">1922   if (tf()-&gt;returns_value_type_as_fields()) {</span>
<span class="line-added">1923     Node* ret = NULL;</span>
<span class="line-added">1924     for (uint i = 1; i &lt; root()-&gt;req(); i++){</span>
<span class="line-added">1925       Node* in = root()-&gt;in(i);</span>
<span class="line-added">1926       if (in-&gt;Opcode() == Op_Return) {</span>
<span class="line-added">1927         assert(ret == NULL, &quot;only one return&quot;);</span>
<span class="line-added">1928         ret = in;</span>
<span class="line-added">1929       }</span>
<span class="line-added">1930     }</span>
<span class="line-added">1931     if (ret != NULL) {</span>
<span class="line-added">1932       Node* ret_val = ret-&gt;in(TypeFunc::Parms);</span>
<span class="line-added">1933       if (igvn.type(ret_val)-&gt;isa_oopptr() &amp;&amp;</span>
<span class="line-added">1934           return_val_keeps_allocations_alive(ret_val)) {</span>
<span class="line-added">1935         igvn.replace_input_of(ret, TypeFunc::Parms, ValueTypeNode::tagged_klass(igvn.type(ret_val)-&gt;value_klass(), igvn));</span>
<span class="line-added">1936         assert(ret_val-&gt;outcnt() == 0, &quot;should be dead now&quot;);</span>
<span class="line-added">1937         igvn.remove_dead_node(ret_val);</span>
<span class="line-added">1938       }</span>
<span class="line-added">1939     }</span>
<span class="line-added">1940   }</span>
<span class="line-added">1941   igvn.optimize();</span>
<span class="line-added">1942 }</span>
<span class="line-added">1943 </span>
<span class="line-added">1944 void Compile::adjust_flattened_array_access_aliases(PhaseIterGVN&amp; igvn) {</span>
<span class="line-added">1945   if (!_has_flattened_accesses) {</span>
<span class="line-added">1946     return;</span>
<span class="line-added">1947   }</span>
<span class="line-added">1948   // Initially, all flattened array accesses share the same slice to</span>
<span class="line-added">1949   // keep dependencies with Object[] array accesses (that could be</span>
<span class="line-added">1950   // to a flattened array) correct. We&#39;re done with parsing so we</span>
<span class="line-added">1951   // now know all flattened array accesses in this compile</span>
<span class="line-added">1952   // unit. Let&#39;s move flattened array accesses to their own slice,</span>
<span class="line-added">1953   // one per element field. This should help memory access</span>
<span class="line-added">1954   // optimizations.</span>
<span class="line-added">1955   ResourceMark rm;</span>
<span class="line-added">1956   Unique_Node_List wq;</span>
<span class="line-added">1957   wq.push(root());</span>
<span class="line-added">1958 </span>
<span class="line-added">1959   Node_List mergememnodes;</span>
<span class="line-added">1960   Node_List memnodes;</span>
<span class="line-added">1961 </span>
<span class="line-added">1962   // Alias index currently shared by all flattened memory accesses</span>
<span class="line-added">1963   int index = get_alias_index(TypeAryPtr::VALUES);</span>
<span class="line-added">1964 </span>
<span class="line-added">1965   // Find MergeMem nodes and flattened array accesses</span>
<span class="line-added">1966   for (uint i = 0; i &lt; wq.size(); i++) {</span>
<span class="line-added">1967     Node* n = wq.at(i);</span>
<span class="line-added">1968     if (n-&gt;is_Mem()) {</span>
<span class="line-added">1969       const TypePtr* adr_type = NULL;</span>
<span class="line-added">1970       if (n-&gt;Opcode() == Op_StoreCM) {</span>
<span class="line-added">1971         adr_type = get_adr_type(get_alias_index(n-&gt;in(MemNode::OopStore)-&gt;adr_type()));</span>
<span class="line-added">1972       } else {</span>
<span class="line-added">1973         adr_type = get_adr_type(get_alias_index(n-&gt;adr_type()));</span>
<span class="line-added">1974       }</span>
<span class="line-added">1975       if (adr_type == TypeAryPtr::VALUES) {</span>
<span class="line-added">1976         memnodes.push(n);</span>
<span class="line-added">1977       }</span>
<span class="line-added">1978     } else if (n-&gt;is_MergeMem()) {</span>
<span class="line-added">1979       MergeMemNode* mm = n-&gt;as_MergeMem();</span>
<span class="line-added">1980       if (mm-&gt;memory_at(index) != mm-&gt;base_memory()) {</span>
<span class="line-added">1981         mergememnodes.push(n);</span>
<span class="line-added">1982       }</span>
<span class="line-added">1983     }</span>
<span class="line-added">1984     for (uint j = 0; j &lt; n-&gt;req(); j++) {</span>
<span class="line-added">1985       Node* m = n-&gt;in(j);</span>
<span class="line-added">1986       if (m != NULL) {</span>
<span class="line-added">1987         wq.push(m);</span>
<span class="line-added">1988       }</span>
<span class="line-added">1989     }</span>
<span class="line-added">1990   }</span>
<span class="line-added">1991 </span>
<span class="line-added">1992   if (memnodes.size() &gt; 0) {</span>
<span class="line-added">1993     _flattened_accesses_share_alias = false;</span>
<span class="line-added">1994 </span>
<span class="line-added">1995     // We are going to change the slice for the flattened array</span>
<span class="line-added">1996     // accesses so we need to clear the cache entries that refer to</span>
<span class="line-added">1997     // them.</span>
<span class="line-added">1998     for (uint i = 0; i &lt; AliasCacheSize; i++) {</span>
<span class="line-added">1999       AliasCacheEntry* ace = &amp;_alias_cache[i];</span>
<span class="line-added">2000       if (ace-&gt;_adr_type != NULL &amp;&amp;</span>
<span class="line-added">2001           ace-&gt;_adr_type-&gt;isa_aryptr() &amp;&amp;</span>
<span class="line-added">2002           ace-&gt;_adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_valuetype()) {</span>
<span class="line-added">2003         ace-&gt;_adr_type = NULL;</span>
<span class="line-added">2004         ace-&gt;_index = (i != 0) ? 0 : AliasIdxTop; // Make sure the NULL adr_type resolves to AliasIdxTop</span>
<span class="line-added">2005       }</span>
<span class="line-added">2006     }</span>
<span class="line-added">2007 </span>
<span class="line-added">2008     // Find what aliases we are going to add</span>
<span class="line-added">2009     int start_alias = num_alias_types()-1;</span>
<span class="line-added">2010     int stop_alias = 0;</span>
<span class="line-added">2011 </span>
<span class="line-added">2012     for (uint i = 0; i &lt; memnodes.size(); i++) {</span>
<span class="line-added">2013       Node* m = memnodes.at(i);</span>
<span class="line-added">2014       const TypePtr* adr_type = NULL;</span>
<span class="line-added">2015       if (m-&gt;Opcode() == Op_StoreCM) {</span>
<span class="line-added">2016         adr_type = m-&gt;in(MemNode::OopStore)-&gt;adr_type();</span>
<span class="line-added">2017         Node* clone = new StoreCMNode(m-&gt;in(MemNode::Control), m-&gt;in(MemNode::Memory), m-&gt;in(MemNode::Address),</span>
<span class="line-added">2018                                       m-&gt;adr_type(), m-&gt;in(MemNode::ValueIn), m-&gt;in(MemNode::OopStore),</span>
<span class="line-added">2019                                       get_alias_index(adr_type));</span>
<span class="line-added">2020         igvn.register_new_node_with_optimizer(clone);</span>
<span class="line-added">2021         igvn.replace_node(m, clone);</span>
<span class="line-added">2022       } else {</span>
<span class="line-added">2023         adr_type = m-&gt;adr_type();</span>
<span class="line-added">2024 #ifdef ASSERT</span>
<span class="line-added">2025         m-&gt;as_Mem()-&gt;set_adr_type(adr_type);</span>
<span class="line-added">2026 #endif</span>
<span class="line-added">2027       }</span>
<span class="line-added">2028       int idx = get_alias_index(adr_type);</span>
<span class="line-added">2029       start_alias = MIN2(start_alias, idx);</span>
<span class="line-added">2030       stop_alias = MAX2(stop_alias, idx);</span>
<span class="line-added">2031     }</span>
<span class="line-added">2032 </span>
<span class="line-added">2033     assert(stop_alias &gt;= start_alias, &quot;should have expanded aliases&quot;);</span>
<span class="line-added">2034 </span>
<span class="line-added">2035     Node_Stack stack(0);</span>
<span class="line-added">2036 #ifdef ASSERT</span>
<span class="line-added">2037     VectorSet seen(Thread::current()-&gt;resource_area());</span>
<span class="line-added">2038 #endif</span>
<span class="line-added">2039     // Now let&#39;s fix the memory graph so each flattened array access</span>
<span class="line-added">2040     // is moved to the right slice. Start from the MergeMem nodes.</span>
<span class="line-added">2041     uint last = unique();</span>
<span class="line-added">2042     for (uint i = 0; i &lt; mergememnodes.size(); i++) {</span>
<span class="line-added">2043       MergeMemNode* current = mergememnodes.at(i)-&gt;as_MergeMem();</span>
<span class="line-added">2044       Node* n = current-&gt;memory_at(index);</span>
<span class="line-added">2045       MergeMemNode* mm = NULL;</span>
<span class="line-added">2046       do {</span>
<span class="line-added">2047         // Follow memory edges through memory accesses, phis and</span>
<span class="line-added">2048         // narrow membars and push nodes on the stack. Once we hit</span>
<span class="line-added">2049         // bottom memory, we pop element off the stack one at a</span>
<span class="line-added">2050         // time, in reverse order, and move them to the right slice</span>
<span class="line-added">2051         // by changing their memory edges.</span>
<span class="line-added">2052         if ((n-&gt;is_Phi() &amp;&amp; n-&gt;adr_type() != TypePtr::BOTTOM) || n-&gt;is_Mem() || n-&gt;adr_type() == TypeAryPtr::VALUES) {</span>
<span class="line-added">2053           assert(!seen.test_set(n-&gt;_idx), &quot;&quot;);</span>
<span class="line-added">2054           // Uses (a load for instance) will need to be moved to the</span>
<span class="line-added">2055           // right slice as well and will get a new memory state</span>
<span class="line-added">2056           // that we don&#39;t know yet. The use could also be the</span>
<span class="line-added">2057           // backedge of a loop. We put a place holder node between</span>
<span class="line-added">2058           // the memory node and its uses. We replace that place</span>
<span class="line-added">2059           // holder with the correct memory state once we know it,</span>
<span class="line-added">2060           // i.e. when nodes are popped off the stack. Using the</span>
<span class="line-added">2061           // place holder make the logic work in the presence of</span>
<span class="line-added">2062           // loops.</span>
<span class="line-added">2063           if (n-&gt;outcnt() &gt; 1) {</span>
<span class="line-added">2064             Node* place_holder = NULL;</span>
<span class="line-added">2065             assert(!n-&gt;has_out_with(Op_Node), &quot;&quot;);</span>
<span class="line-added">2066             for (DUIterator k = n-&gt;outs(); n-&gt;has_out(k); k++) {</span>
<span class="line-added">2067               Node* u = n-&gt;out(k);</span>
<span class="line-added">2068               if (u != current &amp;&amp; u-&gt;_idx &lt; last) {</span>
<span class="line-added">2069                 bool success = false;</span>
<span class="line-added">2070                 for (uint l = 0; l &lt; u-&gt;req(); l++) {</span>
<span class="line-added">2071                   if (!stack.is_empty() &amp;&amp; u == stack.node() &amp;&amp; l == stack.index()) {</span>
<span class="line-added">2072                     continue;</span>
<span class="line-added">2073                   }</span>
<span class="line-added">2074                   Node* in = u-&gt;in(l);</span>
<span class="line-added">2075                   if (in == n) {</span>
<span class="line-added">2076                     if (place_holder == NULL) {</span>
<span class="line-added">2077                       place_holder = new Node(1);</span>
<span class="line-added">2078                       place_holder-&gt;init_req(0, n);</span>
<span class="line-added">2079                     }</span>
<span class="line-added">2080                     igvn.replace_input_of(u, l, place_holder);</span>
<span class="line-added">2081                     success = true;</span>
<span class="line-added">2082                   }</span>
<span class="line-added">2083                 }</span>
<span class="line-added">2084                 if (success) {</span>
<span class="line-added">2085                   --k;</span>
<span class="line-added">2086                 }</span>
<span class="line-added">2087               }</span>
<span class="line-added">2088             }</span>
<span class="line-added">2089           }</span>
<span class="line-added">2090           if (n-&gt;is_Phi()) {</span>
<span class="line-added">2091             stack.push(n, 1);</span>
<span class="line-added">2092             n = n-&gt;in(1);</span>
<span class="line-added">2093           } else if (n-&gt;is_Mem()) {</span>
<span class="line-added">2094             stack.push(n, n-&gt;req());</span>
<span class="line-added">2095             n = n-&gt;in(MemNode::Memory);</span>
<span class="line-added">2096           } else {</span>
<span class="line-added">2097             assert(n-&gt;is_Proj() &amp;&amp; n-&gt;in(0)-&gt;Opcode() == Op_MemBarCPUOrder, &quot;&quot;);</span>
<span class="line-added">2098             stack.push(n, n-&gt;req());</span>
<span class="line-added">2099             n = n-&gt;in(0)-&gt;in(TypeFunc::Memory);</span>
<span class="line-added">2100           }</span>
<span class="line-added">2101         } else {</span>
<span class="line-added">2102           assert(n-&gt;adr_type() == TypePtr::BOTTOM || (n-&gt;Opcode() == Op_Node &amp;&amp; n-&gt;_idx &gt;= last) || (n-&gt;is_Proj() &amp;&amp; n-&gt;in(0)-&gt;is_Initialize()), &quot;&quot;);</span>
<span class="line-added">2103           // Build a new MergeMem node to carry the new memory state</span>
<span class="line-added">2104           // as we build it. IGVN should fold extraneous MergeMem</span>
<span class="line-added">2105           // nodes.</span>
<span class="line-added">2106           mm = MergeMemNode::make(n);</span>
<span class="line-added">2107           igvn.register_new_node_with_optimizer(mm);</span>
<span class="line-added">2108           while (stack.size() &gt; 0) {</span>
<span class="line-added">2109             Node* m = stack.node();</span>
<span class="line-added">2110             uint idx = stack.index();</span>
<span class="line-added">2111             if (m-&gt;is_Mem()) {</span>
<span class="line-added">2112               // Move memory node to its new slice</span>
<span class="line-added">2113               const TypePtr* adr_type = m-&gt;adr_type();</span>
<span class="line-added">2114               int alias = get_alias_index(adr_type);</span>
<span class="line-added">2115               Node* prev = mm-&gt;memory_at(alias);</span>
<span class="line-added">2116               igvn.replace_input_of(m, MemNode::Memory, prev);</span>
<span class="line-added">2117               mm-&gt;set_memory_at(alias, m);</span>
<span class="line-added">2118             } else if (m-&gt;is_Phi()) {</span>
<span class="line-added">2119               // We need as many new phis as there are new aliases</span>
<span class="line-added">2120               igvn.replace_input_of(m, idx, mm);</span>
<span class="line-added">2121               if (idx == m-&gt;req()-1) {</span>
<span class="line-added">2122                 Node* r = m-&gt;in(0);</span>
<span class="line-added">2123                 for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {</span>
<span class="line-added">2124                   const Type* adr_type = get_adr_type(j);</span>
<span class="line-added">2125                   if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_valuetype()) {</span>
<span class="line-added">2126                     continue;</span>
<span class="line-added">2127                   }</span>
<span class="line-added">2128                   Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));</span>
<span class="line-added">2129                   igvn.register_new_node_with_optimizer(phi);</span>
<span class="line-added">2130                   for (uint k = 1; k &lt; m-&gt;req(); k++) {</span>
<span class="line-added">2131                     phi-&gt;init_req(k, m-&gt;in(k)-&gt;as_MergeMem()-&gt;memory_at(j));</span>
<span class="line-added">2132                   }</span>
<span class="line-added">2133                   mm-&gt;set_memory_at(j, phi);</span>
<span class="line-added">2134                 }</span>
<span class="line-added">2135                 Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);</span>
<span class="line-added">2136                 igvn.register_new_node_with_optimizer(base_phi);</span>
<span class="line-added">2137                 for (uint k = 1; k &lt; m-&gt;req(); k++) {</span>
<span class="line-added">2138                   base_phi-&gt;init_req(k, m-&gt;in(k)-&gt;as_MergeMem()-&gt;base_memory());</span>
<span class="line-added">2139                 }</span>
<span class="line-added">2140                 mm-&gt;set_base_memory(base_phi);</span>
<span class="line-added">2141               }</span>
<span class="line-added">2142             } else {</span>
<span class="line-added">2143               // This is a MemBarCPUOrder node from</span>
<span class="line-added">2144               // Parse::array_load()/Parse::array_store(), in the</span>
<span class="line-added">2145               // branch that handles flattened arrays hidden under</span>
<span class="line-added">2146               // an Object[] array. We also need one new membar per</span>
<span class="line-added">2147               // new alias to keep the unknown access that the</span>
<span class="line-added">2148               // membars protect properly ordered with accesses to</span>
<span class="line-added">2149               // known flattened array.</span>
<span class="line-added">2150               assert(m-&gt;is_Proj(), &quot;projection expected&quot;);</span>
<span class="line-added">2151               Node* ctrl = m-&gt;in(0)-&gt;in(TypeFunc::Control);</span>
<span class="line-added">2152               igvn.replace_input_of(m-&gt;in(0), TypeFunc::Control, top());</span>
<span class="line-added">2153               for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {</span>
<span class="line-added">2154                 const Type* adr_type = get_adr_type(j);</span>
<span class="line-added">2155                 if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_valuetype()) {</span>
<span class="line-added">2156                   continue;</span>
<span class="line-added">2157                 }</span>
<span class="line-added">2158                 MemBarNode* mb = new MemBarCPUOrderNode(this, j, NULL);</span>
<span class="line-added">2159                 igvn.register_new_node_with_optimizer(mb);</span>
<span class="line-added">2160                 Node* mem = mm-&gt;memory_at(j);</span>
<span class="line-added">2161                 mb-&gt;init_req(TypeFunc::Control, ctrl);</span>
<span class="line-added">2162                 mb-&gt;init_req(TypeFunc::Memory, mem);</span>
<span class="line-added">2163                 ctrl = new ProjNode(mb, TypeFunc::Control);</span>
<span class="line-added">2164                 igvn.register_new_node_with_optimizer(ctrl);</span>
<span class="line-added">2165                 mem = new ProjNode(mb, TypeFunc::Memory);</span>
<span class="line-added">2166                 igvn.register_new_node_with_optimizer(mem);</span>
<span class="line-added">2167                 mm-&gt;set_memory_at(j, mem);</span>
<span class="line-added">2168               }</span>
<span class="line-added">2169               igvn.replace_node(m-&gt;in(0)-&gt;as_Multi()-&gt;proj_out(TypeFunc::Control), ctrl);</span>
<span class="line-added">2170             }</span>
<span class="line-added">2171             if (idx &lt; m-&gt;req()-1) {</span>
<span class="line-added">2172               idx += 1;</span>
<span class="line-added">2173               stack.set_index(idx);</span>
<span class="line-added">2174               n = m-&gt;in(idx);</span>
<span class="line-added">2175               break;</span>
<span class="line-added">2176             }</span>
<span class="line-added">2177             // Take care of place holder nodes</span>
<span class="line-added">2178             if (m-&gt;has_out_with(Op_Node)) {</span>
<span class="line-added">2179               Node* place_holder = m-&gt;find_out_with(Op_Node);</span>
<span class="line-added">2180               if (place_holder != NULL) {</span>
<span class="line-added">2181                 Node* mm_clone = mm-&gt;clone();</span>
<span class="line-added">2182                 igvn.register_new_node_with_optimizer(mm_clone);</span>
<span class="line-added">2183                 Node* hook = new Node(1);</span>
<span class="line-added">2184                 hook-&gt;init_req(0, mm);</span>
<span class="line-added">2185                 igvn.replace_node(place_holder, mm_clone);</span>
<span class="line-added">2186                 hook-&gt;destruct();</span>
<span class="line-added">2187               }</span>
<span class="line-added">2188               assert(!m-&gt;has_out_with(Op_Node), &quot;place holder should be gone now&quot;);</span>
<span class="line-added">2189             }</span>
<span class="line-added">2190             stack.pop();</span>
<span class="line-added">2191           }</span>
<span class="line-added">2192         }</span>
<span class="line-added">2193       } while(stack.size() &gt; 0);</span>
<span class="line-added">2194       // Fix the memory state at the MergeMem we started from</span>
<span class="line-added">2195       igvn.rehash_node_delayed(current);</span>
<span class="line-added">2196       for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {</span>
<span class="line-added">2197         const Type* adr_type = get_adr_type(j);</span>
<span class="line-added">2198         if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_valuetype()) {</span>
<span class="line-added">2199           continue;</span>
<span class="line-added">2200         }</span>
<span class="line-added">2201         current-&gt;set_memory_at(j, mm);</span>
<span class="line-added">2202       }</span>
<span class="line-added">2203       current-&gt;set_memory_at(index, current-&gt;base_memory());</span>
<span class="line-added">2204     }</span>
<span class="line-added">2205     igvn.optimize();</span>
<span class="line-added">2206   }</span>
<span class="line-added">2207   print_method(PHASE_SPLIT_VALUES_ARRAY, 2);</span>
<span class="line-added">2208 }</span>
<span class="line-added">2209 </span>
<span class="line-added">2210 </span>
2211 // StringOpts and late inlining of string methods
2212 void Compile::inline_string_calls(bool parse_time) {
2213   {
2214     // remove useless nodes to make the usage analysis simpler
2215     ResourceMark rm;
2216     PhaseRemoveUseless pru(initial_gvn(), for_igvn());
2217   }
2218 
2219   {
2220     ResourceMark rm;
2221     print_method(PHASE_BEFORE_STRINGOPTS, 3);
2222     PhaseStringOpts pso(initial_gvn(), for_igvn());
2223     print_method(PHASE_AFTER_STRINGOPTS, 3);
2224   }
2225 
2226   // now inline anything that we skipped the first time around
2227   if (!parse_time) {
2228     _late_inlines_pos = _late_inlines.length();
2229   }
2230 
</pre>
<hr />
<pre>
2470   remove_speculative_types(igvn);
2471 
2472   // No more new expensive nodes will be added to the list from here
2473   // so keep only the actual candidates for optimizations.
2474   cleanup_expensive_nodes(igvn);
2475 
2476   if (!failing() &amp;&amp; RenumberLiveNodes &amp;&amp; live_nodes() + NodeLimitFudgeFactor &lt; unique()) {
2477     Compile::TracePhase tp(&quot;&quot;, &amp;timers[_t_renumberLive]);
2478     initial_gvn()-&gt;replace_with(&amp;igvn);
2479     for_igvn()-&gt;clear();
2480     Unique_Node_List new_worklist(C-&gt;comp_arena());
2481     {
2482       ResourceMark rm;
2483       PhaseRenumberLive prl = PhaseRenumberLive(initial_gvn(), for_igvn(), &amp;new_worklist);
2484     }
2485     set_for_igvn(&amp;new_worklist);
2486     igvn = PhaseIterGVN(initial_gvn());
2487     igvn.optimize();
2488   }
2489 
<span class="line-added">2490   if (_value_type_nodes-&gt;size() &gt; 0) {</span>
<span class="line-added">2491     // Do this once all inlining is over to avoid getting inconsistent debug info</span>
<span class="line-added">2492     process_value_types(igvn);</span>
<span class="line-added">2493   }</span>
<span class="line-added">2494 </span>
<span class="line-added">2495   adjust_flattened_array_access_aliases(igvn);</span>
<span class="line-added">2496 </span>
2497   // Perform escape analysis
2498   if (_do_escape_analysis &amp;&amp; ConnectionGraph::has_candidates(this)) {
2499     if (has_loops()) {
2500       // Cleanup graph (remove dead nodes).
2501       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2502       PhaseIdealLoop::optimize(igvn, LoopOptsMaxUnroll);
2503       if (major_progress()) print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);
2504       if (failing())  return;
2505     }
2506     ConnectionGraph::do_analysis(this, &amp;igvn);
2507 
2508     if (failing())  return;
2509 
2510     // Optimize out fields loads from scalar replaceable allocations.
2511     igvn.optimize();
2512     print_method(PHASE_ITER_GVN_AFTER_EA, 2);
2513 
2514     if (failing())  return;
2515 
2516     if (congraph() != NULL &amp;&amp; macro_count() &gt; 0) {
</pre>
<hr />
<pre>
3151             // Accumulate any precedence edges
3152             if (mem-&gt;in(i) != NULL) {
3153               n-&gt;add_prec(mem-&gt;in(i));
3154             }
3155           }
3156           // Everything above this point has been processed.
3157           done = true;
3158         }
3159         // Eliminate the previous StoreCM
3160         prev-&gt;set_req(MemNode::Memory, mem-&gt;in(MemNode::Memory));
3161         assert(mem-&gt;outcnt() == 0, &quot;should be dead&quot;);
3162         mem-&gt;disconnect_inputs(NULL, this);
3163       } else {
3164         prev = mem;
3165       }
3166       mem = prev-&gt;in(MemNode::Memory);
3167     }
3168   }
3169 }
3170 
<span class="line-added">3171 </span>
3172 //------------------------------final_graph_reshaping_impl----------------------
3173 // Implement items 1-5 from final_graph_reshaping below.
3174 void Compile::final_graph_reshaping_impl( Node *n, Final_Reshape_Counts &amp;frc) {
3175 
3176   if ( n-&gt;outcnt() == 0 ) return; // dead node
3177   uint nop = n-&gt;Opcode();
3178 
3179   // Check for 2-input instruction with &quot;last use&quot; on right input.
3180   // Swap to left input.  Implements item (2).
3181   if( n-&gt;req() == 3 &amp;&amp;          // two-input instruction
3182       n-&gt;in(1)-&gt;outcnt() &gt; 1 &amp;&amp; // left use is NOT a last use
3183       (!n-&gt;in(1)-&gt;is_Phi() || n-&gt;in(1)-&gt;in(2) != n) &amp;&amp; // it is not data loop
3184       n-&gt;in(2)-&gt;outcnt() == 1 &amp;&amp;// right use IS a last use
3185       !n-&gt;in(2)-&gt;is_Con() ) {   // right use is not a constant
3186     // Check for commutative opcode
3187     switch( nop ) {
3188     case Op_AddI:  case Op_AddF:  case Op_AddD:  case Op_AddL:
3189     case Op_MaxI:  case Op_MinI:
3190     case Op_MulI:  case Op_MulF:  case Op_MulD:  case Op_MulL:
3191     case Op_AndL:  case Op_XorL:  case Op_OrL:
</pre>
<hr />
<pre>
3890           // Replace all nodes with identical edges as m with m
3891           k-&gt;subsume_by(m, this);
3892         }
3893       }
3894     }
3895     break;
3896   }
3897   case Op_CmpUL: {
3898     if (!Matcher::has_match_rule(Op_CmpUL)) {
3899       // No support for unsigned long comparisons
3900       ConINode* sign_pos = new ConINode(TypeInt::make(BitsPerLong - 1));
3901       Node* sign_bit_mask = new RShiftLNode(n-&gt;in(1), sign_pos);
3902       Node* orl = new OrLNode(n-&gt;in(1), sign_bit_mask);
3903       ConLNode* remove_sign_mask = new ConLNode(TypeLong::make(max_jlong));
3904       Node* andl = new AndLNode(orl, remove_sign_mask);
3905       Node* cmp = new CmpLNode(andl, n-&gt;in(2));
3906       n-&gt;subsume_by(cmp, this);
3907     }
3908     break;
3909   }
<span class="line-added">3910 #ifdef ASSERT</span>
<span class="line-added">3911   case Op_ValueTypePtr:</span>
<span class="line-added">3912   case Op_ValueType: {</span>
<span class="line-added">3913     n-&gt;dump(-1);</span>
<span class="line-added">3914     assert(false, &quot;value type node was not removed&quot;);</span>
<span class="line-added">3915     break;</span>
<span class="line-added">3916   }</span>
<span class="line-added">3917 #endif</span>
3918   default:
3919     assert(!n-&gt;is_Call(), &quot;&quot;);
3920     assert(!n-&gt;is_Mem(), &quot;&quot;);
3921     assert(nop != Op_ProfileBoolean, &quot;should be eliminated during IGVN&quot;);
3922     break;
3923   }
3924 }
3925 
3926 //------------------------------final_graph_reshaping_walk---------------------
3927 // Replacing Opaque nodes with their input in final_graph_reshaping_impl(),
3928 // requires that the walk visits a node&#39;s inputs before visiting the node.
3929 void Compile::final_graph_reshaping_walk( Node_Stack &amp;nstack, Node *root, Final_Reshape_Counts &amp;frc ) {
3930   ResourceArea *area = Thread::current()-&gt;resource_area();
3931   Unique_Node_List sfpt(area);
3932 
3933   frc._visited.set(root-&gt;_idx); // first, mark node as visited
3934   uint cnt = root-&gt;req();
3935   Node *n = root;
3936   uint  i = 0;
3937   while (true) {
</pre>
<hr />
<pre>
4246   }
4247 }
4248 
4249 bool Compile::needs_clinit_barrier(ciMethod* method, ciMethod* accessing_method) {
4250   return method-&gt;is_static() &amp;&amp; needs_clinit_barrier(method-&gt;holder(), accessing_method);
4251 }
4252 
4253 bool Compile::needs_clinit_barrier(ciField* field, ciMethod* accessing_method) {
4254   return field-&gt;is_static() &amp;&amp; needs_clinit_barrier(field-&gt;holder(), accessing_method);
4255 }
4256 
4257 bool Compile::needs_clinit_barrier(ciInstanceKlass* holder, ciMethod* accessing_method) {
4258   if (holder-&gt;is_initialized()) {
4259     return false;
4260   }
4261   if (holder-&gt;is_being_initialized()) {
4262     if (accessing_method-&gt;holder() == holder) {
4263       // Access inside a class. The barrier can be elided when access happens in &lt;clinit&gt;,
4264       // &lt;init&gt;, or a static method. In all those cases, there was an initialization
4265       // barrier on the holder klass passed.
<span class="line-modified">4266       if (accessing_method-&gt;is_class_initializer() ||</span>
<span class="line-modified">4267           accessing_method-&gt;is_object_constructor() ||</span>
4268           accessing_method-&gt;is_static()) {
4269         return false;
4270       }
4271     } else if (accessing_method-&gt;holder()-&gt;is_subclass_of(holder)) {
4272       // Access from a subclass. The barrier can be elided only when access happens in &lt;clinit&gt;.
4273       // In case of &lt;init&gt; or a static method, the barrier is on the subclass is not enough:
4274       // child class can become fully initialized while its parent class is still being initialized.
<span class="line-modified">4275       if (accessing_method-&gt;is_class_initializer()) {</span>
4276         return false;
4277       }
4278     }
4279     ciMethod* root = method(); // the root method of compilation
4280     if (root != accessing_method) {
4281       return needs_clinit_barrier(holder, root); // check access in the context of compilation root
4282     }
4283   }
4284   return true;
4285 }
4286 
4287 #ifndef PRODUCT
4288 //------------------------------verify_graph_edges---------------------------
4289 // Walk the Graph and verify that there is a one-to-one correspondence
4290 // between Use-Def edges and Def-Use edges in the graph.
4291 void Compile::verify_graph_edges(bool no_dead_code) {
4292   if (VerifyGraphEdges) {
4293     ResourceArea *area = Thread::current()-&gt;resource_area();
4294     Unique_Node_List visited(area);
4295     // Call recursive graph walk to check edges
</pre>
<hr />
<pre>
4377                   _phase_name, C-&gt;unique(), C-&gt;live_nodes(), C-&gt;count_live_nodes_by_graph_walk());
4378   }
4379 
4380   if (VerifyIdealNodeCount) {
4381     Compile::current()-&gt;print_missing_nodes();
4382   }
4383 #endif
4384 
4385   if (_log != NULL) {
4386     _log-&gt;done(&quot;phase name=&#39;%s&#39; nodes=&#39;%d&#39; live=&#39;%d&#39;&quot;, _phase_name, C-&gt;unique(), C-&gt;live_nodes());
4387   }
4388 }
4389 
4390 //----------------------------static_subtype_check-----------------------------
4391 // Shortcut important common cases when superklass is exact:
4392 // (0) superklass is java.lang.Object (can occur in reflective code)
4393 // (1) subklass is already limited to a subtype of superklass =&gt; always ok
4394 // (2) subklass does not overlap with superklass =&gt; always fail
4395 // (3) superklass has NO subtypes and we can check with a simple compare.
4396 int Compile::static_subtype_check(ciKlass* superk, ciKlass* subk) {
<span class="line-modified">4397   if (StressReflectiveCode || superk == NULL || subk == NULL) {</span>
4398     return SSC_full_test;       // Let caller generate the general case.
4399   }
4400 
4401   if (superk == env()-&gt;Object_klass()) {
4402     return SSC_always_true;     // (0) this test cannot fail
4403   }
4404 
4405   ciType* superelem = superk;
<span class="line-modified">4406   if (superelem-&gt;is_array_klass()) {</span>
<span class="line-added">4407     ciArrayKlass* ak = superelem-&gt;as_array_klass();</span>
4408     superelem = superelem-&gt;as_array_klass()-&gt;base_element_type();
<span class="line-added">4409   }</span>
4410 
4411   if (!subk-&gt;is_interface()) {  // cannot trust static interface types yet
4412     if (subk-&gt;is_subtype_of(superk)) {
4413       return SSC_always_true;   // (1) false path dead; no dynamic test needed
4414     }
4415     if (!(superelem-&gt;is_klass() &amp;&amp; superelem-&gt;as_klass()-&gt;is_interface()) &amp;&amp;
4416         !superk-&gt;is_subtype_of(subk)) {
4417       return SSC_always_false;
4418     }
4419   }
4420 
4421   // If casting to an instance klass, it must have no subtypes
4422   if (superk-&gt;is_interface()) {
4423     // Cannot trust interfaces yet.
4424     // %%% S.B. superk-&gt;nof_implementors() == 1
4425   } else if (superelem-&gt;is_instance_klass()) {
4426     ciInstanceKlass* ik = superelem-&gt;as_instance_klass();
4427     if (!ik-&gt;has_subklass() &amp;&amp; !ik-&gt;is_interface()) {
4428       if (!ik-&gt;is_final()) {
4429         // Add a dependency if there is a chance of a later subclass.
</pre>
<hr />
<pre>
4850     for (uint next = 0; next &lt; worklist.size(); ++next) {
4851       Node *n  = worklist.at(next);
4852       const Type* t = igvn.type_or_null(n);
4853       assert((t == NULL) || (t == t-&gt;remove_speculative()), &quot;no more speculative types&quot;);
4854       if (n-&gt;is_Type()) {
4855         t = n-&gt;as_Type()-&gt;type();
4856         assert(t == t-&gt;remove_speculative(), &quot;no more speculative types&quot;);
4857       }
4858       uint max = n-&gt;len();
4859       for( uint i = 0; i &lt; max; ++i ) {
4860         Node *m = n-&gt;in(i);
4861         if (not_a_node(m))  continue;
4862         worklist.push(m);
4863       }
4864     }
4865     igvn.check_no_speculative_types();
4866 #endif
4867   }
4868 }
4869 
<span class="line-added">4870 Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {</span>
<span class="line-added">4871   const TypeInstPtr* ta = phase-&gt;type(a)-&gt;isa_instptr();</span>
<span class="line-added">4872   const TypeInstPtr* tb = phase-&gt;type(b)-&gt;isa_instptr();</span>
<span class="line-added">4873   if (!EnableValhalla || ta == NULL || tb == NULL ||</span>
<span class="line-added">4874       ta-&gt;is_zero_type() || tb-&gt;is_zero_type() ||</span>
<span class="line-added">4875       !ta-&gt;can_be_value_type() || !tb-&gt;can_be_value_type()) {</span>
<span class="line-added">4876     // Use old acmp if one operand is null or not a value type</span>
<span class="line-added">4877     return new CmpPNode(a, b);</span>
<span class="line-added">4878   } else if (ta-&gt;is_valuetypeptr() || tb-&gt;is_valuetypeptr()) {</span>
<span class="line-added">4879     // We know that one operand is a value type. Therefore,</span>
<span class="line-added">4880     // new acmp will only return true if both operands are NULL.</span>
<span class="line-added">4881     // Check if both operands are null by or&#39;ing the oops.</span>
<span class="line-added">4882     a = phase-&gt;transform(new CastP2XNode(NULL, a));</span>
<span class="line-added">4883     b = phase-&gt;transform(new CastP2XNode(NULL, b));</span>
<span class="line-added">4884     a = phase-&gt;transform(new OrXNode(a, b));</span>
<span class="line-added">4885     return new CmpXNode(a, phase-&gt;MakeConX(0));</span>
<span class="line-added">4886   }</span>
<span class="line-added">4887   // Use new acmp</span>
<span class="line-added">4888   return NULL;</span>
<span class="line-added">4889 }</span>
<span class="line-added">4890 </span>
4891 // Auxiliary method to support randomized stressing/fuzzing.
4892 //
4893 // This method can be called the arbitrary number of times, with current count
4894 // as the argument. The logic allows selecting a single candidate from the
4895 // running list of candidates as follows:
4896 //    int count = 0;
4897 //    Cand* selected = null;
4898 //    while(cand = cand-&gt;next()) {
4899 //      if (randomized_select(++count)) {
4900 //        selected = cand;
4901 //      }
4902 //    }
4903 //
4904 // Including count equalizes the chances any candidate is &quot;selected&quot;.
4905 // This is useful when we don&#39;t have the complete list of candidates to choose
4906 // from uniformly. In this case, we need to adjust the randomicity of the
4907 // selection, or else we will end up biasing the selection towards the latter
4908 // candidates.
4909 //
4910 // Quick back-envelope calculation shows that for the list of n candidates
</pre>
</td>
</tr>
</table>
<center><a href="c2_globals.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="graphKit.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>